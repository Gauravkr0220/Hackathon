DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaptation
by Combining 3D GANs and Diffusion Priors
Biwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, Xuansong Xie
Alibaba Group
{biwen.lbw, jinmao.yk, mengyang.fmy, miaomiao.cmm }@alibaba-inc.com,
xingtong.xxs@taobao.com
3DDomainAdaptation
RealimagePixarGreekstatueJoker
SampledimagePixarGoldenstatueFox,PixarstyleBlueeyesPinkhair
HulkBatman
ObamaGirlinwhitedress
RealimageText-to-Avatar
LocalEditingStevenJobs
Figure 1. Some results of the proposed DiffusionGAN3D on different tasks.
Abstract
Text-guided domain adaptation and generation of 3D-
aware portraits find many applications in various fields.
However, due to the lack of training data and the challenges
in handling the high variety of geometry and appearance,
the existing methods for these tasks suffer from issues like
inflexibility, instability, and low fidelity. In this paper, we
propose a novel framework DiffusionGAN3D, which boosts
text-guided 3D domain adaptation and generation by com-
bining 3D GANs and diffusion priors. Specifically, we in-
tegrate the pre-trained 3D generative models (e.g., EG3D)
and text-to-image diffusion models. The former provides a
strong foundation for stable and high-quality avatar gen-
eration from text. And the diffusion models in turn offer
powerful priors and guide the 3D generator finetuning with
informative direction to achieve flexible and efficient text-
guided domain adaptation. To enhance the diversity in do-
main adaptation and the generation capability in text-to-
avatar, we introduce the relative distance loss and case-
specific learnable triplane respectively. Besides, we design
a progressive texture refinement module to improve the tex-ture quality for both tasks above. Extensive experiments
demonstrate that the proposed framework achieves excel-
lent results in both domain adaptation and text-to-avatar
tasks, outperforming existing methods in terms of gener-
ation quality and efficiency. The project homepage is at
https://younglbw.github.io/DiffusionGAN3D-homepage/.
1. Introduction
3D portrait generation and stylization find a vast range
of applications in many scenarios, such as games, adver-
tisements, and film production. While extensive works
[4, 7, 9, 17] yield impressive results on realistic portrait gen-
eration, the performance on generating stylized, artistic, and
text-guided 3D avatars is still unsatisfying due to the lack
of 3D training data and the difficulties in modeling highly
variable geometry and texture.
Some works [2, 25, 26, 47, 51, 53, 56] perform transfer
learning on a pre-trained 3D GAN generator to achieve 3D
stylization, which relies on a large number of stylized im-
ages and strictly aligned camera poses for training. [2, 47]
leverage existing 2D-GAN trained on a specific domain to
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10487
synthesize training data and implement finetuning with ad-
versarial loss. In contrast, [25, 26, 51] utilize text-to-image
diffusion models to generate training datasets in the target
domain. This enables more flexible style transferring but
also brings problems like pose bias, tedious data processing,
and heavy computation costs. Unlike these adversarial fine-
tuning based methods, StyleGAN-Fusion [48] adopts SDS
[37] loss as guidance of text-guided adaptation of 2D and
3D generators, which gives a simple yet effective way to
fulfill domain adaptation. However, it also suffers from lim-
ited diversity and suboptimal text-image correspondence.
The recently proposed Score Distillation Sampling
(SDS) algorithm [37] exhibits impressive performance in
text-guided 3D generation. Introducing diffusion priors
into the texture and geometry modeling notably reduces
the training cost and offers powerful 3D generation ability.
However, it also leads to issues like unrealistic appearance
and Janus (multi-face) problems. Following [37], massive
works [5, 21, 27, 30, 49, 50, 52] have been proposed to en-
hance the generation quality and stability. Nevertheless, the
robustness and visual quality of the generated model are still
far less than the current generated 2D images.
Based on the observations above, we propose a novel
two-stage framework DiffusionGAN3D to boost the perfor-
mance of 3D domain adaptation and text-to-avatar tasks by
combining 3D generative models and diffusion priors, as
shown in Fig. 2. For the text-guided 3D Domain Adapta-
tion task, we first leverage diffusion models and adopt SDS
loss to finetune a pre-trained EG3D-based model [4, 7, 9]
with random noise input and camera views. The relative
distance loss is introduced to deal with the loss of diver-
sity caused by the SDS technique. Additionally, we design
a diffusion-guided reconstruction loss to adapt the frame-
work to local editing scenarios. Then, we extend the frame-
work to Text-to-Avatar task by finetuning 3D GANs with
a fixed latent code that is obtained guided by CLIP [38]
model. During optimization, a case-specific learnable tri-
plane is introduced to strengthen the generation capability
of the network. To sum up, in our framework, the diffu-
sion models offer powerful text-image priors, which guide
the domain adaptation of the 3D generator with informative
direction in a flexible and efficient way. In turn, 3D GANs
provide a strong foundation for text-to-avatar, enabling sta-
ble and high-quality avatar generation. Last but not least,
taking advantage of the powerful 2D synthesis capability of
diffusion models, we propose a Progressive Texture Re-
finement module as the second stage for these two tasks
above, which significantly enhances the texture quality. Ex-
tensive experiments demonstrate that our method exhibits
excellent performance in terms of generation quality and
stability on 3D domain adaptation and text-to-avtar tasks,
as shown in Fig. 1.
Our main contributions are as follows:(A)We achieve text-guided 3D domain adaptation in high
quality and diversity by combining 3D GANs and diffusion
priors with the assistance of the relative distance loss.
(B)We adapt the framework to a local editing scenario by
designing a diffusion-guided reconstruction loss.
(C)We achieve high-quality text-to-avatar in superior per-
formance and stability by introducing the case-specific
learnable triplane.
(D) We propose a novel progressive texture refinement
stage, which fully exploits the image generation capabilities
of the diffusion models and greatly enhances the quality of
texture generated above.
2. Related Work
Domain Adaptation of 3D GANs. The advancements in
3D generative models [4, 6, 7, 9, 11, 13, 14, 29, 35, 45] have
enabled geometry-aware and pose-controlled image genera-
tion. Especially, EG3D [7] utilizes triplane as 3D represen-
tation and integrates StyleGAN2 [24] generator with neural
rendering [33] to achieve high-quality 3D shapes and view-
consistency image synthesis, which facilitates the down-
stream applications such as 3D stylization, GAN inversion
[28]. Several works [2, 22, 53, 56] achieve 3D domain
adaptation by utilizing stylized 2D generator to synthesize
training images or distilling knowledge from it. In con-
trast, [25, 26, 51] leverage the powerful diffusion models to
generate training datasets in the target domain and accom-
plish text-guided 3D domain adaptation with great perfor-
mance. Though achieving impressive results, these adver-
sarial learning based methods above suffer from issues such
as pose bias, tedious data processing, and heavy compu-
tation cost. Recently, non-adversarial finetuining methods
[3, 12, 48] also exhibit great promise in text-guided domain
adaptation. Especially, StyleGAN-Fusion [48] adopts SDS
loss as guidance for the adaptation of 2D generators and
3D generators. It achieves efficient and flexible text-guided
domain adaptation but also faces the problems of limited
diversity and suboptimal text-image correspondence.
Text-to-3D Generation. In recent years, text-guided 2D
image synthesis [10, 41â€“43, 55] achieve significant progress
and provide a foundation for 3D generation. Prior works,
including CLIP-forge [44], CLIP-Mesh [34], and Dream-
Fields [20], employ CLIP [38] as guidance to optimize 3D
representations such as meshes and NeRF [33]. DreamFu-
sion [37] first proposes score distillation sampling (SDS)
loss to utilize a pre-trained text-to-image diffusion model
to guide the training of NeRF. It is a pioneering work and
exhibits great promise in text-to-3d generation, but also
suffers from over-saturation, over-smoothing, and Janus
(multi-face) problem. Subsequently, extensive improve-
ments [30, 39, 49, 50] over DreamFusion have been in-
troduced to address these issues. ProlificDreamer [50]
proposes variational score distillation (VSD) and produces
10488
Decoder
TriplaneGenerator
TriplaneGenerator
ğ‘Šğ‘³ğ’…ğ’Šğ’”
T2IDiffusion
Pixar styleğ¿$%$Add Noise
ğ’šğ’„
TriplaneGenerator
LatentSearching
T2IDiffusion
ğ¿$%$ğ’šDecoder
Add Noiseğ’„ğ’šLinkinZeldaTask2:Text-to-AvatarStage1Stage2
â€¦
â€¦VolumnRenderingMarchingcube&UVunwrapping
Adaptiveblend
ğ’™ğŸğ’™ğŸğ’Œ$ğŸğ’™ğŸğ’Œğ’™ğŸğ’Œ&ğŸğ‘¼ğŸ
ğ‘¼ğŸğ‘¼ğŸğ‘¼ğŸğ’Œ+ğŸ
â€¦Adaptiveblend
ğ’“ğ’Š"(ğ‘–=0,â€¦,2ğ‘˜+1)
â€¦
ğ’š
ğ’“ğŸğ’“ğŸğ’“ğŸğ’Œ+ğŸğ’“ğŸ,ğ’“ğŸ,ğ’“ğŸğ’Œ+ğŸ,
â€¦â€¦Img2img+ControlNetsInpainting+ControlNetsInpainting+ControlNetsrotaterotateProgressiveTextureRefinement
CLIP
ğ’˜ğ’™ğ’›ğ’•
ğ’™ğ’›ğ’•ğ‘»
ğ‘»â€²
ğ’šğ’š
ğ‘»ğ¿.$/0
ğ’ğŸğ’ğŸğ’Œ+ğŸğ’˜,ğ’šğ’šTask1:DomainAdaptation
ğ’˜,ğº/1234
ğº516784
ğº/1234
ğ‘»ğ’Acuteboy,Pixarstyleğ’šğ‘´
ğ‘´Figure 2. Overview of the proposed two-stage framework DiffusionGAN3D.
high-fidelity texture results. Magic3D [30] adopts a coarse-
to-fine strategy and utilizes DMTET [46] as the 3D rep-
resentation to implement texture refinement through SDS
loss. Despite yielding impressive progress, the appearance
of their results is still unsatisfying, existing issues such as
noise [50], lack of details [30, 49], multi-view inconsistency
[8, 40]. Moreover, these methods still face the problem
of insufficient robustness and incorrect geometry. When it
comes to avatar generation, these shortcomings can be more
obvious and unacceptable.
Text-to-Avatar Generation. To handle 3D avatar genera-
tion from text, extensive approaches [5, 18, 19, 21, 27, 54]
have been proposed. Avatar-CLIP [18] sets the founda-
tion by initializing human geometry with a shape V AE
and employing CLIP to guide geometry and texture model-
ing. DreamAvatar [5] and AvatarCraft [21] fulfill robust 3D
avatar creation by integrating the human parametric model
SMPL [31] with pre-trained text-to-image diffusion mod-
els. DreamHuman [27] further introduces a camera zoom-
in strategy to refine the local details of 6 important body
regions. Recently, AvatarVerse [52] and a concurrent work
[36] employ DensePose-conditioned ControlNet [55] for
SDS guidance to realize more stable avatar creation and
pose control. Although these methods exhibit quite decent
results, weak SDS guidance still hampers their performance
in multi-view consistency and texture fidelity.
3. Methods
In this section, we present DiffusionGAN3D, which boosts
the performance of 3D domain adaptation and text-to-avatarby combining and taking advantage of 3D GANs and diffu-
sion priors. Fig. 2 illustrates the overview of our frame-
work. After introducing some preliminaries (Sec. 3.1), we
first elaborate our designs in diffusion-guided 3D domain
adaptation (Sec. 3.2) , where we propose a relative dis-
tance loss to resolve the problem of diversity loss caused by
SDS. Then we extend this architecture and introduce a case-
specific learnable triplane to fulfill 3D-GAN based text-to-
avatar (Sec. 3.3). Finally, we design a novel progressive
texture refinement stage (Sec. 3.4) to improve the detail and
authenticity of the texture generated above.
3.1. Preliminaries
EG3D [7] is a SOTA 3D generative model, which employ
triplane as 3D representation and integrate StyleGAN2 [24]
generator with neural rendering [33] to achieve high quality
3D shapes and pose-controlled image synthesis. It is com-
posed of (1) a mapping network that projects the input noise
to the latent space W, (2) a triplane generator that synthe-
sizes the triplane with the latent code as input, and (3) a
decoder that includes a triplane decoder, volume rendering
module and super-resolution module in sequence. Given a
triplane and camera poses as input, the decoder generates
high-resolution images with view consistency.
Score Distillation Sampling (SDS) , proposed by Dream-
Fusion [7], utilizes a pre-trained diffusion model ÏµÏ•as prior
for optimization of a 3D representation Î¸. Given an image
x=g(Î¸)that is rendered from a differentiable model g, we
add random noise Ïµonxat noise level tto obtain a noisy
image zt. The SDS loss then optimizes Î¸by minimizing
10489
the difference between the predicted noise ÏµÏ•(zt;y, t)and
the added noise Ïµ, which can be presented as:
âˆ‡Î¸LSDS (Ï•, gÎ¸) =Et,Ïµ
wt(ÏµÏ•(zt;y, t)âˆ’Ïµ)âˆ‚x
âˆ‚Î¸
,(1)
where yindicates the text prompt and wtdenotes a weight-
ing function that depends on the noise level t.
3.2. Diffusion-Guided 3D Domain Adaptation
Due to the difficulties in obtaining high-quality pose-aware
data and model training, adversarial learning methods for
3D domain adaptation mostly suffer from the issues of te-
dious data processing and mode collapse. To address that,
we leverage diffusion models and adopt the SDS loss to im-
plement transfer learning on an EG3D-based 3D GAN to
achieve efficient 3D domain adaptation, as shown in Fig. 2.
Given a style code wgenerated from noise zâˆ¼N(0,1)
through the fixed mapping network, we can obtain the tri-
planeTand the image xrendered in a view controlled by
the input camera parameters cusing the triplane generator
and decoder in sequence. Then SDS loss (Sec. 3.1) is ap-
plied on xto finetune the network. Different from Dream-
Fusion which optimizes a NeRF network to implement sin-
gle object generation, we shift the 3D generator with ran-
dom noise and camera pose to achieve domain adaptation
guided by text y. During optimization, all parameters of
the framework are frozen except the triplane generator. We
find that the gradient provided by SDS loss is unstable and
can be harmful to some other well-trained modules such as
the super-resolution module. Besides, freezing the mapping
network ensures that the latent code wlies in the same do-
main during training, which is a crucial feature that can be
utilized in the diversity preserving of the 3D generator.
Relative Distance Loss. The SDS loss provides diffusion
priors and achieves text-guided domain adaptation of 3D
GAN in an efficient way. However, it also brings the prob-
lem of diversity loss as illustrated in [48]. To deal with that,
[48] proposes the directional regularizer to regularize the
generator optimization process, which improves the diver-
sity to a certain extent. However, it also limits the domain
shifting, facing a trade-off between diversity and the degree
of style transfer. To address this, we propose a relative dis-
tance loss. As shown in Fig. 3, considering two style codes
wiandwjwhich are mapping from two different noise zi
andzj, we project them into the original triplane domain
(Tâ€²
i,Tâ€²
j) and the finetuned one ( Ti,Tj) using a frozen tri-
plane generator Gfrozen and the finetuned triplane genera-
torGtrain , respectively. Note that, since the mapping net-
work is frozen during training in our framework, TiandTâ€²
i
(same for TjandTâ€²
j) share the same latent code and ought
to be close in context. Thus, we model the relative distance
of these two samples in triplane space and formulate the
relative distance loss Ldisas:
OriginaltriplanespaceNewtriplanespaceğ‘¤!ğ‘¤"ğ‘‡!ğ‘‡"ğ‘‡"#ğ‘‡!#
ğº$%&'()ğº*%+!)transferFigure 3. An illustration of the relative distance loss.
Ldis=abs(||Tâ€²
iâˆ’Tâ€²
j||2
||Tiâˆ’Tj||2âˆ’1). (2)
In this function, guided by the original network, the sam-
ples in the triplane space are forced to maintain distance
from each other. This prevents the generator from collaps-
ing to a fixed output pattern. Note that it only regularizes
the relative distance between different samples while per-
forming no limitation to the transfer of the triplane domain
itself. Extensive experiments in Sec. 4 demonstrate that the
proposed relative distance loss effectively improves the gen-
eration diversity without impairing the degree of stylization.
Diffusion-guided Reconstruction Loss. Despite the com-
bination of SDS loss and the proposed relative distance loss
is adequate for most domain adaptation tasks, it still fails
to handle the local editing scenarios. A naive solution is
to perform reconstruction loss between the rendered image
and the one from the frozen network. However, it will also
inhibit translation of the target region. Accordingly, we
propose a diffusion-guided reconstruction loss especially
for local editing, which aims to preserve non-target regions
while performing 3D editing on the target region. We found
that the gradient of SDS loss has a certain correlation with
the target area, especially when the noise level tis large, as
shown in Fig. 4. To this end, we design a diffusion-guided
reconstruction loss Ldiff that can be presented as:
Î³=abs(wt(ÏµÏ•(zt;y, t)âˆ’Ïµ)), (3)
Ldiff =t||(xâˆ’xâ€²)âŠ™
Jâˆ’h(Î³
max (Î³))
||2,(4)
where Î³is the absolute value of the gradient item in Eq. 1, h
represents the averaging operation in the feature dimension,
Jis the matrix of ones having the same spatial dimensions
as the output of h,xâ€²denotes the output image of the frozen
network under the same noise and camera parameters x,âŠ™
indicates the Hadamard product. The latter item of the âŠ™
operation can be regarded as an adaptive mask indicating
the non-target region. Compared with ordinary reconstruc-
tion loss, the proposed diffusion-guided reconstruction loss
alleviates the transfer limitation of the target region. Al-
though the gradient of SDS loss in a single iteration contains
a lot of noise and is inadequate to serve as an accurate mask,
10490
Sampledimageğ‘¡=0.1ğ‘¡=0.5ğ‘¡=0.9AccumulationFigure 4. Visualizations of the gradient response of SDS loss at
different noise levels, given the text â€a man with green hairâ€.
it can also provide effective guidance for network learning
with the accumulation of iterations as shown in Fig. 4. The
ablation experiment in Sec. 4 also proves its effectiveness.
To sum up, we can form the loss functions for normal do-
main adaptation and local editing scenario as Ladaptation =
Lsds+Î»1LdisandLediting =Lsds+Î»2Ldiff, respectively,
where Î»1andÎ»2are the weighting coefficients.
3.3. 3D-GAN Based Text-to-Avatar
Due to the lack of 3D priors , most text-to-3D methods can-
not perform stable generation, suffering from issues such
as Janua (multi-face) problem. To this end, we extend the
framework proposed above and utilize the pre-trained 3D
GAN as a strong base generator to achieve robust text-
guided 3D avatar generation. As shown in Fig. 2, we first
implement latent searching to obtain the latent code that is
contextually (gender, appearance, etc.) close to the text in-
put. Specifically, we sample knoisez1, ...,zkand select
one single noise zithat best fits the text description accord-
ing to the CLIP loss between the corresponding images syn-
thesized by the 3D GAN and the prompt. The CLIP loss is
further used to finetune the mapping network individually
to obtain the optimized latent code wâ€²fromzi. Then, wâ€²is
fixed during the following optimization process.
Case-specific learnable triplane. One main challenge of
the text-to-avatar task is how to model the highly variable
geometry and texture. Introducing 3D GANs as the base
generator provides strong priors and greatly improves sta-
bility. However, it also loses the flexibility of the simple
NeRF network, showing limited generation capability. Ac-
cordingly, we introduce a case-specific learnable triplane Tl
to enlarge the capacity of the network, as shown in Fig. 2.
Initialized with the value of 0, Tlis directly added to Tas
the input of subsequent modules. Thus, the trainable part of
the network now includes the triplane generator Gtrain and
Tl. The former achieves stable transformation, while the
latter provides a more flexible 3D representation. Due to
the high degree of freedom of Tland the instability of SDS
loss, optimizing Tlwith SDS loss alone will bring a lot of
noise, resulting in unsmooth results. To this end, we adopt
the total variation loss [23] and expand it to a multi-scale
manner Lmstv to regularize Tland facilitate more smooth-
ing results. In general, the loss function for text-to-avatar
task can be presented as: Lavatar =Lsds+Î»3Lmstv.
Note that, the proposed framework is only suitable for
the generation of specific categories depending on the pre-
DifferentiableRendering
ğ‘³ğ’•ğ’—
ğ‘³ğ’ğ’”ğ’†â€¦Figure 5. The details of the proposed adaptive blend module.
trained 3D GAN, such as head (PanoHead [4]) and hu-
man body (AG3D [9]). Nevertheless, extensive experiments
show that our framework can well adapt to avatar genera-
tion with large domain gaps, benefiting from the strong 3D
generator and the case-specific learnable triplane.
3.4. Progressive Texture Refinement
The SDS exhibits great promise in geometry modeling but
also suffers from texture-related problems such as over-
saturation and over-smoothing. How can we leverage the
powerful 2D generation ability of diffusion models to im-
prove the 3D textures? In this section, we propose a pro-
gressive texture refinement stage, which significantly en-
hances the texture quality of the results above through ex-
plicit texture modeling, as shown in Fig. 2.
Adaptive Blend Module. Given the implicit fields obtained
from the first stage, we first implement volume rendering
under uniformly selected 2k+ 2azimuths and jelevations
(we set the following jto 1 for simplicity) to obtain multi-
view images xi, ...,x2k+1. Then the canny maps and depth
maps of these images are extracted for the following image
translation. Meanwhile, we perform marching cube [32]
and the UV unwrapping [1] algorithm to obtain the explicit
meshMand corresponding UV coordinates (in head gen-
eration, we utilize cylinder unwrapping for better visualiza-
tion). Furthermore, we design an adaptive blend module to
project the multi-view renderings back into a texture map
through differentiable rendering. Specifically, as shown in
Fig. 5, the multi-view reconstruction loss Lmse and total
variation loss Ltvare adopted to optimize the texture map
that is initialized with zeros. Compared to directly im-
plementing back-projection, the proposed adaptive blend-
ing module produces smoother and more natural textures
in spliced areas of different images without compromising
texture quality. This optimized UV texture U0serves as an
initialization for the following texture refinement stage.
Progressive Refinement. Since we have already obtained
the explicit mesh and the multi-view renderings, a natu-
ral idea is to perform image-to-image on the multi-view
renderings using diffusion models to optimize the texture.
However, it neglects that the diffusion model cannot guar-
antee the consistency of image translation between differ-
ent views, which may result in discontinuous texture. To
this end, we introduce a progressive inpainting strategy to
address this issue. Firstly, we employ a pre-trained text-
10491
to-image diffusion model and ControlNets [55] to imple-
ment image-to-image translation guided by the prompt y
on the front-view image r0that is rendered from Mand
U0. The canny and depth extracted above are introduced
to ensure the alignment between r0and the resulting image
râ€²
0. Then we can obtain the partially refined texture map
U1by projecting râ€²
0intoU0. Next, we rotate the mesh
coupled with T1(or change the camera view) and render
a new image r1, which is refined again with the diffusion
model to get râ€²
1andU2. Differently, instead of image-to-
image, we apply inpainting on r1with mask m1in this
translation, which maintains the refined region and thus im-
proves the texture consistency between the adjacent views.
Note that the masks m1, ...,m2k+1indicate the unrefined
regions and are dilated to facilitate smoother results in in-
painting. Through progressively performing rotation and
inpainting, we manage to obtain consistent multi-view im-
agesrâ€²
0, ...,râ€²
2k+1that are refined by the diffusion model.
Finally, we apply the adaptive blend module again on the
refined images to yield the final texture. By implementing
refinement on the explicit texture, the proposed stage sig-
nificantly improves the texture quality in an efficient way.
4. Experiments
4.1. Implementation Details
Our framework is built on an EG3D-based model in the first
stage. Specifically, we implement 3D domain adaptation on
PanoHead, EG3D-FFHQ, and EG3D-AFHQ for head, face,
and cat, respectively. For text-to-avatar tasks, PanoHead
and AG3D are adopted as the base generators for head and
body generation. We employ StableDiffusion v2.1 as our
pre-trained text-to-image model. In the texture refinement
stage, StableDiffusion v1.5 coupled with ControlNets are
utilized to implement image-to-image and inpainting. More
details about the parameters and training setting are speci-
fied in supplementary materials.
4.2. Qualitative Comparison
For 3D Domain adaptation, we evaluate our model with
two powerful baselines: StyleGAN-NADA* [12] and
StyleGAN-Fusion [48] for text-guided domain adaptation
of 3D GANs, where * indicates the extension of the method
to 3D models. For a fair comparison, we use the same
prompts as guidance for all the methods. Besides, the visu-
alization results of different methods are sampled from the
same random noise. As shown in Fig. 6, the naive exten-
sion of StyleGAN-NADA* for EG3D exhibits poor results
in terms of diversity and image quality. StyleGAN-Fusion
achieves decent 3D domain adaptation and exhibits a certain
diversity. However, the proposed regularizer of StyleGAN-
Fusion also hinders itself from large-gap domain transfer,
resulting in a trade-off between the degree of stylizationand diversity. As Fig. 6 shows that the generated faces of
StyleGAN-Fusion lack diversity and details, and the hair
and clothes suffer from inadequate stylization. In contrast,
our method exhibits superior performance in diversity, im-
age quality, and text-image correspondence.
For text-to-avatar task, We present qualitative compar-
isons with several general text-to-3D methods (DreamFu-
sion [37], ProlificDreamer [50], Magic-3D [30]) and avatar
generation methods (DreamAvatar [5], DreamHuman [27],
AvatarVerse [52]). The former three methods are imple-
mented using the official code and the results of the rest
methods are obtained directly from their project pages. As
shown in Fig. 7, DreamFusion shows inferior performance
in avatar generation, suffering from over-saturation, Janus
(multi-face) problem, and incorrect body parts. Prolific-
Dreamer and Magic-3D improve the texture fidelity to some
extent but still face the problem of inaccurate and unsmooth
geometry. Taking advantage of the human priors obtained
from the SMPL model or DensePose, these text-to-avatar
methods achieve stable and high-quality avatar generation.
However, due to that the SDS loss requires a high CFG [16]
value during optimization, the texture fidelity and authentic-
ity of their results are still unsatisfying. In comparison, the
proposed method achieves stable and high-fidelity avatar
generation simultaneously, making full use of the 3D GANs
and diffusion priors. Please refer to the supplementary ma-
terials for more comparisons.
4.3. Quantitative Comparison
We quantitatively evaluate the above baselines and our
method on 3D domain adaptation through FID [15] compar-
ison and user study. Specifically, all methods are employed
to conduct domain adaptation on EG3D-face and EG3D-
cat with both four text prompts, respectively. For each
text prompt, we utilize the text-to-image diffusion model
to generate 2000 images with different random seeds as the
ground truth for FID calculation. In the user study, 12 vol-
unteers were invited to rate each finetuned model from 1 to
5 based on three dimensions: text-image correspondence,
image quality, and diversity. As shown in Table 1, the pro-
posed methods achieve lower FID scores than other base-
lines, which indicates superior image fidelity. Meanwhile,
the user study demonstrates that our method outperforms
the other two methods, especially in terms of image quality
and diversity.
For text-to-avatar, we also conducted a user study for
quantitative comparison. Since AvatarVerse and DreamA-
vatar have not released their code yet, while DreamHu-
man provided extensive results on the project page. So
we compare our method with DreamHuman for full-body
generation. Besides, DreamFusion, ProlificDreamer, and
Magic3D are involved in the comparison of head (10
prompts) and full-body (10 prompts) generation both. We
10492
Pixar
Lego
(b)StyleGAN-Fusion(a)StyleGAN-NADA*(c)Ours
Figure 6. The qualitative comparisons on 3D domain adaptation (applied on EG3D-FFHQ [7]).
(d)DreamAvatar/DreamHuman
(a)DreamFusion(b)ProlificDreamer(c)AvatarVerse(d)Ours
(a)DreamFusion(b)ProlificDreamer(f)Ours(c)Magic-3D(e)AvatarVerse
'' Yoda in Star Wars Series '''' A bearded man in a black leather jacket'''' Spiderman'''' Aman wearing a white tanktopand shorts''
Figure 7. Visual comparisons on text-to-avatar task. The first two rows are the results of â€˜headâ€™ and the rest are the results of â€˜bodyâ€™.
request the 12 volunteers to vote for their favorite results
based on texture and geometry quality, where all the results
are presented as rendered rotating videos. The final rates
presented in Table 2 show that the proposed method per-
forms favorably against the other approaches.
Table 1. Quantitative comparison on 3D domain adaptation task.
MethodsMetric User Study
FIDâ†“ text-corr â†‘ quality â†‘ diversity â†‘
StyleGAN-NADA* 136.2 2.619 2.257 1.756
StyleGAN-Fusion 53.6 3.465 3.255 2.978
Ours 28.5 3.725 3.758 3.416Table 2. User preference on text-to-avatar generation.
DreamFusion ProlificDreamer Magic3D DreamHuman Ours
head 1.1% 11.7% 6.7% N.A. 80.5%
body 0.6% 8.3% 5.6% 18.9% 66.6%
4.4. Ablation Study
On progressive texture refinement. Since we utilize cylin-
der unwrapping for head texture refinement, a naive idea
is to conduct image-to-image on the UV texture directly
to refine it. However, the result in Fig. 8 (b) shows that
this method tends to yield misaligned texture, let alone be
applied to fragmented texture maps. We also attempt to
10493
(a)Stage1(b)Textureimg2img(c)w/oInpainting(e)w/oABM
(d)MSE
(f)OursFigure 8. Ablation study of the texture refinement.
(a)SourceDomain(b)w/oRelativedistanceloss(c)Ours
Figure 9. Ablation study of the relative distance loss.
replace all the inpainting operations with image-to-image
translation, and the results in Fig. 8 (c) show that this will
cause the discontinuity problem. The refining strategy pro-
posed in [49] is also compared, where texture is progres-
sively optimized using MSE loss between the randomly
rendered images and the corresponding image-to-image re-
sults. The results in Fig. 8 (d) show that it fails to generate
high-frequency details. The comparison between (e) and (f)
in Fig. 8 proves the effectiveness of the proposed adaptive
blend module (ABM) in smoothing the texture splicing re-
gion. By contrast, the proposed progressive texture refine-
ment strategy significantly improves the texture quality.
On relative distance loss. As shown in Fig. 9, if adopting
SDS loss alone for domain adaptation, the generator will
tend to collapse to a fixed output pattern, losing its original
diversity. In contrast, the proposed relative distance loss
effectively preserves the diversity of the generator without
sacrificing the stylization degree.
On diffusion-guided reconstruction loss. The results in
Fig 10 show that the SDS loss tends to perform global trans-
fer. Regular reconstruction loss helps maintain the whole
structure, but also stem the translation of the target area. By
contrast, the model trained with our diffusion-guided recon-
struction loss achieves proper editing.
On additional learnable triplane. To prove the necessity
of the proposed case-specific learnable triplane, we finetune
the network with SDS loss without adding it, given a chal-
lenging prompt: â€Link in Zeldaâ€. The results in the first row
of Fig. 11 reveal that the network is optimized in the right
direction but fails to reach the precise point. By contrast,
the network adding the learnable triplane exhibits accurate
(a)Sourceimage(b)w/oRecloss(c)MSEloss(d)OursFigure 10. Ablation study of the diffusion guided reconstruc-
tion loss. The ToRGB module in EG3D is trained together with
Gtrain . The input text is â€œa close-up of a woman with green hairâ€.
(a)w/oCase-specificlearnabletriplane
(b)w/oMulti-scaletotalvariationloss
(c)Ours
Figure 11. Ablation study toward the case-specific learnable tri-
plane and the multi-scale total variation loss.
generation (second row in Fig. 11). Furthermore, the intro-
duced multi-scale total variation loss Lmstv on the triplane
facilitates more smooth results.
4.5. Applications and Limitations
Due to the page limitation, we will introduce the application
of DiffusionGAN3D on real images and specify the limita-
tions of our methods in the supplementary materials.
5. Conclusion
In this paper, we propose a novel two-stage framework Dif-
fusionGAN3D, which boosts text-guided 3D domain adap-
tation and avatar generation by combining the 3D GANs
and diffusion priors. Specifically, we integrate the pre-
trained 3D generative models (e.g., EG3D) with the text-
to-image diffusion models. The former, in our framework,
set a strong foundation for text-to-avatar, enabling stable
and high-quality 3D avatar generation. In return, the lat-
ter provides informative direction for 3D GANs to evolve,
which facilitates the text-guided domain adaptation of 3D
GANs in an efficient way. Moreover, we introduce a pro-
gressive texture refinement stage, which significantly en-
hances the texture quality of the generation results. Ex-
tensive experiments demonstrate that the proposed frame-
work achieves excellent results in both domain adaptation
and text-to-avatar tasks, outperforming existing methods in
terms of generation quality and efficiency.
10494
References
[1] Jonathan young. xatlas, 2021. https : / /
triplegangers.com/ . 5
[2] Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai,
Aliaksandr Siarohin, Peter Wonka, and Sergey Tulyakov.
3davatargan: Bridging domains for personalized editable
avatars. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 4552â€“
4562, 2023. 1, 2
[3] Aibek Alanov, Vadim Titov, and Dmitry P Vetrov. Hyperdo-
mainnet: Universal domain adaptation for generative adver-
sarial networks. Advances in Neural Information Processing
Systems , 35:29414â€“29426, 2022. 2
[4] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y
Ogras, and Linjie Luo. Panohead: Geometry-aware 3d full-
head synthesis in 360deg. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 20950â€“20959, 2023. 1, 2, 5
[5] Yukang Cao, YanPei Cao, Kai Han, Ying Shan, and Kwan-
Yee K. Wong. Dreamavatar: Text-and-shape guided 3d hu-
man avatar generation via diffusion models. arXiv preprint
arXiv:2304.00916 , 2023. 2, 3, 6
[6] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5799â€“5809, 2021. 2
[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16123â€“16133, 2022. 1, 2, 3,
7
[8] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey
Tulyakov, and Matthias NieÃŸner. Text2tex: Text-driven
texture synthesis via diffusion models. arXiv preprint
arXiv:2303.11396 , 2023. 3
[9] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-
mar Hilliges, and Andreas Geiger. Ag3d: Learning to gen-
erate 3d avatars from 2d image collections. arXiv preprint
arXiv:2305.02312 , 2023. 1, 2, 5
[10] Aditya Ramesh et al. Hierarchical text-conditional image
generation with clip latents, 2022. 2
[11] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape
induction from 2d views of multiple objects. In 2017 In-
ternational Conference on 3D Vision (3DV) , pages 402â€“411.
IEEE, 2017. 2
[12] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik,
and Daniel Cohen-Or. Stylegan-nada: Clip-guided do-
main adaptation of image generators. arXiv preprint
arXiv:2108.00946 , 2021. 2, 6
[13] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian
Theobalt. Stylenerf: A style-based 3d-aware genera-
tor for high-resolution image synthesis. arXiv preprint
arXiv:2110.08985 , 2021. 2[14] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escap-
ing platoâ€™s cave: 3d shape from adversarial rendering. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 9984â€“9993, 2019. 2
[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 6
[16] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 6
[17] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and
Ziwei Liu. Eva3d: Compositional 3d human generation from
2d image collections. arXiv preprint arXiv:2210.04888 ,
2022. 1
[18] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang
Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-
driven generation and animation of 3d avatars. arXiv preprint
arXiv:2205.08535 , 2022. 3
[19] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao
Qi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz:
Make a scene with complex 3d animatable avatars. arXiv
preprint arXiv:2305.12529 , 2023. 3
[20] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter
Abbeel, and Ben Poole. Zero-shot text-guided object genera-
tion with dream fields. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
867â€“876, 2022. 2
[21] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,
Mingming He, Dongdong Chen, and Jing Liao. Avatar-
craft: Transforming text into neural human avatars with
parameterized shape and pose control. arXiv preprint
arXiv:2303.17606 , 2023. 2, 3
[22] Wonjoon Jin, Nuri Ryu, Geonung Kim, Seung-Hwan Baek,
and Sunghyun Cho. Dr.3d: Adapting 3d gans to artistic
drawings. In SIGGRAPH Asia 2022 Conference Papers ,
pages 1â€“8, 2022. 2
[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
European conference on computer vision , pages 694â€“711.
Springer, 2016. 5
[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8110â€“8119, 2020. 2, 3
[25] Gwanghyun Kim and Se Young Chun. Datid-3d: Diversity-
preserved domain adaptation using text-to-image diffusion
for 3d generative model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 14203â€“14213, 2023. 1, 2
[26] Gwanghyun Kim, Ji Ha Jang, and Se Young Chun. Podia-
3d: Domain adaptation of 3d generative model across large
domain gap using pose-preserved text-to-image diffusion. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 22603â€“22612, 2023. 1, 2
10495
[27] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-
uard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis-
escu. Dreamhuman: Animatable 3d avatars from text. arXiv
preprint arXiv:2306.09329 , 2023. 2, 3, 6
[28] Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, and
Bo Dai. Self-supervised geometry-aware encoder for style-
based 3d gan inversion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 20940â€“20949, 2023. 2
[29] Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas
Geiger. Towards unsupervised learning of generative mod-
els for 3d controllable image synthesis. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 5871â€“5880, 2020. 2
[30] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 300â€“309, 2023. 2, 3, 6
[31] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. In Seminal Graphics Papers: Pushing
the Boundaries, Volume 2 , pages 851â€“866. 2023. 3
[32] William E Lorensen and Harvey E Cline. Marching cubes:
A high resolution 3d surface construction algorithm. In Sem-
inal graphics: pioneering efforts that shaped the field , pages
347â€“353. 1998. 5
[33] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99â€“106, 2021. 2,
3
[34] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,
and Tiberiu Popa. Clip-mesh: Generating textured meshes
from text using pretrained image-text models. In SIGGRAPH
Asia 2022 conference papers , pages 1â€“8, 2022. 2
[35] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shecht-
man, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.
Stylesdf: High-resolution 3d-consistent image and geome-
try generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13503â€“
13513, 2022. 2
[36] Mohit Mendiratta Pan, Mohamed Elgharib, Kartik Teo-
tia, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski,
Christian Theobalt, et al. Avatarstudio: Text-driven edit-
ing of 3d dynamic human head avatars. arXiv preprint
arXiv:2306.00547 , 2023. 3
[37] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988 , 2022. 2, 6
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748â€“8763. PMLR, 2021. 2[39] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-
man, Michael Rubinstein, Jonathan Barron, et al. Dream-
booth3d: Subject-driven text-to-3d generation. arXiv
preprint arXiv:2303.13508 , 2023. 2
[40] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,
and Daniel Cohen-Or. Texture: Text-guided texturing of 3d
shapes. arXiv preprint arXiv:2302.01721 , 2023. 3
[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj Â¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684â€“10695, 2022. 2
[42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2023.
[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi,
Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding. 2022. 2
[44] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,
Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-
shan. Clip-forge: Towards zero-shot text-to-shape genera-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 18603â€“18613,
2022. 2
[45] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. Graf: Generative radiance fields for 3d-aware im-
age synthesis. Advances in Neural Information Processing
Systems , 33:20154â€“20166, 2020. 2
[46] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and
Sanja Fidler. Deep marching tetrahedra: a hybrid repre-
sentation for high-resolution 3d shape synthesis. Advances
in Neural Information Processing Systems , 34:6087â€“6101,
2021. 3
[47] Guoxian Song, Hongyi Xu, Jing Liu, Tiancheng Zhi, Yichun
Shi, Jianfeng Zhang, Zihang Jiang, Jiashi Feng, Shen Sang,
and Linjie Luo. Agilegan3d: Few-shot 3d portrait styl-
ization by augmented transfer learning. arXiv preprint
arXiv:2303.14297 , 2023. 1
[48] Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris
Metaxas, and Ahmed Elgammal. Diffusion guided do-
main adaptation of image generators. arXiv preprint
arXiv:2212.04473 , 2022. 2, 4, 6
[49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang
Zeng. Dreamgaussian: Generative gaussian splatting for effi-
cient 3d content creation. arXiv preprint arXiv:2309.16653 ,
2023. 2, 3, 8
[50] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and
diverse text-to-3d generation with variational score distilla-
tion. arXiv preprint arXiv:2305.16213 , 2023. 2, 3, 6
[51] Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang Yu,
Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, and Chun-
10496
hua Shen. Styleavatar3d: Leveraging image-text diffusion
models for high-fidelity 3d avatar generation. arXiv preprint
arXiv:2305.19012 , 2023. 1, 2
[52] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu
Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min
Zheng. Avatarverse: High-quality & stable 3d avatar cre-
ation from text and pose. arXiv preprint arXiv:2308.03610 ,
2023. 2, 3, 6
[53] Junzhe Zhang, Yushi Lan, Shuai Yang, Fangzhou Hong,
Quan Wang, Chai Kiat Yeo, Ziwei Liu, and Chen Change
Loy. Deformtoon3d: Deformable neural radiance fields for
3d toonification. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 9144â€“9154,
2023. 1, 2
[54] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,
Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and
Jingyi Yu. Dreamface: Progressive generation of ani-
matable 3d faces under text guidance. arXiv preprint
arXiv:2304.03117 , 2023. 3
[55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models, 2023.
2, 3, 6
[56] Peng Zhou, Lingxi Xie, Bingbing Ni, and Qi Tian.
Cips-3d: A 3d-aware generator of gans based on
conditionally-independent pixel synthesis. arXiv preprint
arXiv:2110.09788 , 2021. 1, 2
10497
