Text-Enhanced Data-free Approach for Federated Class-Incremental Learning
Minh-Tuan Tran1, Trung Le1, Xuan-May Le2, Mehrtash Harandi1, Dinh Phung1
1Monash University,2University of Melbourne
{tuan.tran7,trunglm,mehrtash.harandi,dinh.phung }@monash.edu
xuanmay.le@student.unimelb.edu.au
Abstract
Federated Class-Incremental Learning (FCIL) is an un-
derexplored yet pivotal issue, involving the dynamic addi-
tion of new classes in the context of federated learning.
In this field, Data-Free Knowledge Transfer (DFKT) plays
a crucial role in addressing catastrophic forgetting and
data privacy problems. However, prior approaches lack
the crucial synergy between DFKT and the model training
phases, causing DFKT to encounter difficulties in gener-
ating high-quality data from a non-anchored latent space
of the old task model. In this paper, we introduce LAN-
DER (Label Text Centered Data-Free Knowledge Trans-
fer) to address this issue by utilizing label text embeddings
(LTE) produced by pretrained language models. Specifi-
cally, during the model training phase, our approach treats
LTE as anchor points and constrains the feature embed-
dings of corresponding training samples around them, en-
riching the surrounding area with more meaningful infor-
mation. In the DFKT phase, by using these LTE anchors,
LANDER can synthesize more meaningful samples, thereby
effectively addressing the forgetting problem. Additionally,
instead of tightly constraining embeddings toward the an-
chor, the Bounding Loss is introduced to encourage sam-
ple embeddings to remain flexible within a defined radius.
This approach preserves the natural differences in sample
embeddings and mitigates the embedding overlap caused
by heterogeneous federated settings. Extensive experiments
conducted on CIFAR100, Tiny-ImageNet, and ImageNet
demonstrate that LANDER significantly outperforms previ-
ous methods and achieves state-of-the-art performance in
FCIL. The code is available at https://github.com/
tmtuan1307/lander .
1. Introduction
Federated Learning (FL) is a decentralized and privacy-
preserving technique enabling collaboration among diverse
entities, such as organizations or devices [23, 31, 39, 65]. In
FL, multiple users (clients) train a common (server) modelin coordination with a server without sharing personal data.
FL in has recently gained attention in various fields like
healthcare [58], IoT [42], and autonomous driving [14].
While conventional FL studies assume static data classes
and domains, the reality is that new classes can emerge, and
data domains can change over time [3,7,34,64]. For exam-
ple, [6] reveals shifting customer interests in an online store
with seasons, and [60] discusses the need for healthcare
models to adapt to detect new diseases. Handling continu-
ously emerging data classes through entirely new models is
impractical due to substantial computational resources. Al-
ternatively, transfer learning from pre-existing models may
be considered, but it faces the issue of catastrophic forget-
ting [24, 25], degrading performance on previous classes.
To tackle catastrophic forgetting in FL, recent studies
[9, 12, 36, 60] propose the concept of Federated Contin-
ual Learning (FCL) which incorporate Continual Learn-
ing (CL) principles [3, 11, 54, 64] into Federated Learn-
ing. In that, the most popular setting is Federated Class-
Incremental Learning (FCIL) [5, 12, 60, 63, 66], providing
the flexibility to add new classes at any time.
The key challenges in FCIL are to mitigate catastrophic
forgetting and ensure data privacy. To tackle these chal-
lenges, the common FCIL process unfolds in two main
phases: client/server (model) training and the Data-Free
Knowledge Transfer (DFKT) phase. In this context, un-
like classic knowledge distillation methods [18, 43, 44, 46],
DFKT [16, 33, 53, 59] has emerged as a pivotal technique
since it can transfer the knowledge from the last task model
(as a teacher) to the current model (as a student) to mitigate
the forgetting problem but without accessing raw training
data, thus ensuring privacy. The core idea behind DFKT is
to generate synthetic data with confident predictions from
the teacher. In other words, these synthetic data points re-
side in the high-confidence region of the teacherâ€™s predic-
tions. Subsequently, this synthetic data is utilized to train
the student model, effectively addressing the issue of for-
getting.
However, the utilization of DFKT in previous meth-
ods has frequently resulted in unsatisfactory outcomes
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23870
[5, 63, 66]. This can be attributed to the fact that exist-
ing methods lack anchors shared between the client and
teacher/server models to constrain high-confidence regions
of client and server models, making them more well-
organized and thereby facilitating the generation of syn-
thetic data. Consequently, due to the disorganized and non-
anchored high-confidence regions of the teacher models, to
cover all knowledge from the teacher, previous methods
need to generate a large amount of synthetic data in these
complex regions, including both high/low-quality images,
limiting the effectiveness of DFKT in mitigating catas-
trophic forgetting. For instance, numerous existing cat im-
ages could confidently be classified as belonging to the dog
class by any teacher model, as discussed in [8, 20, 37].
In this paper, we address the problem by introduc-
ing a novel method named LANDER (LA bel text ceN ter
Data-free knowledgE transfeR ). LANDER leverages the
label-text embedding (LTE) produced by pretrained lan-
guage models to reorganize the latent embedding of train-
ing data, facilitating the synthesis of high-quality samples
in DFKT. Specifically, our method treats the LTEs as the
anchors and optimizes the feature embeddings of training
samples around these LTEs area, ensuring this area contains
more semantically meaningful information. Importantly,
our method queries the LTEs from the language model only
once. This LTEs is stored in memory for subsequent pro-
cessing, and we do not involve the language model in the
training process. During the DFKT phase, LANDER capi-
talizes on these advantages by generating samples in prox-
imity to these LTEs, thereby creating the synthetic data
with more valuable features. This approach mitigates catas-
trophic forgetting, as these more meaningful samples help
the model retain knowledge from previous tasks. LANDER
departs from conventional approaches by using LTE as in-
put to the generator, shifting the source of randomness from
the input level to the layer level, resulting in faster and more
diverse sampling. Furthermore, we introduce the concept
of the Bounding Loss to encourage sample embeddings to
remain flexible within a defined radius, rather than attempt-
ing to make them as close as possible to the LTEs. This
method retains inherent differences in embeddings and al-
leviates overlap arising from heterogeneous federated set-
tings. Our contributions can be summarized as follows:
â€¢ We propose LANDER which leverage the power of
pretrained language models in FCIL. It utilizes la-
bel text embeddings as anchors to enhance knowledge
transfer from previous models to the current model.
â€¢ We propose preserving natural embedding differences
with the Bounding Loss to address overlap issues in
imbalanced federated settings.
â€¢ We enhance data privacy by introducing a learnable
data stats for the data-free generator, eliminating the
need for clients to disclose specific data information.â€¢ Extensive experiments on CIFAR100, Tiny-ImageNet,
and ImageNet show that LANDER outperforms previ-
ous methods, establishing itself as the state-of-the-art
(SOTA) solution for FCIL.
2. Related Work
Data-free Knowledge Transfer. DFKT or data-free knowl-
edge distillation is an approach that enables knowledge
transfer from a teacher model to a student model without
the need for training data. Recent techniques, such as those
introduced in [10, 40], utilize a generative model to synthe-
size images, guiding the teacherâ€™s predictions. The student
and generator undergo joint adversarial training, facilitat-
ing rapid exploration of synthetic distributions. Notably, the
use of a pretrained text encoder in DFKT, as demonstrated
in [53], has achieved SOTA results. DFKT is gaining pop-
ularity in CL [19, 33] and FL [35, 62] due to its ability to
preserve knowledge without relying on memory while ad-
dressing privacy concerns.
Continual Learning. Catastrophic forgetting, a significant
challenge in machine learning [25], occurs when a modelâ€™s
performance on previously learned data decreases as it is
trained on new examples. CL [61] addresses this issue by
enabling models to acquire new knowledge while retain-
ing existing knowledge. Strategies include regularization
terms [1, 2, 45], experience replay [4, 50, 51], generative
models [30, 52, 57], and isolating architectural parameters
[13,17,38]. DFKT methods [19,41,59] prove promising for
CL, especially in privacy-sensitive applications. CL encom-
passes various learning scenarios: task-incremental learn-
ing (TIL), domain-incremental learning (DIL), and class-
incremental learning (CIL) [56]. TIL involves separate
tasks, DIL maintains a consistent output space, and CIL
gradually introduces new tasks and classes.
Federated Continual Learning. In real-world scenarios,
local user data constantly evolves due to changing inter-
ests or data loss concerns. Federated continual learning
(FCL) addresses the challenge of updating global models
with evolving user data while retaining previous knowledge.
Key contributions include [60], focusing on TIL with dis-
tinct task IDs, and [36], which employs knowledge distilla-
tion. Additionally, [12] diverges by assuming clients have
sufficient memory for storing and sharing data. Other stud-
ies, like [22, 47, 55], explore FCL in diverse domains.
3. Federated Class-Incremental Learning
In our paper, our primary focus is on Federated Class-
Incremental Learning, which applies class-incremental
learning in a federated setting. FCIL framework comprises
a global server Sand multiple clients ( C1,Â·Â·Â·,Cm). In that,
each client is trained to address a sequence of distinct and
non-overlapping tasks without sharing their data with one
23871
radius
Server ğ“¢ğ’•
radius
Server ğ“¢ğ’•
radius
Client ğ“’ğ’ğ’•
Client ğ“’ğŸğ’•
 Server ğ“¢ğ’•âˆ’ğŸ
New Data High -Confidence Region LTE LTE Bounding Area Old Synthetic Data
Task 
ğ’•
 Task 
ğ’•
+1
Distilling
Aggregating
 Generating
new data trainingsynthetic data distilling
LTE LTELTE
 LTELTE LTE LTE LTEsynthetic data distillingnew data training
radius
Data -free Generation Client Training Server Aggregation Data -free GenerationEmbedding Space
Embedding spaceFigure 1. LANDERâ€™s motivation centers on using shared LTE as a key constraint for new task features and applying feature distillation
from the previous server to anchor and organize the latent space of the client/server model. Subsequently, generating samples around the
shared anchor LTE facilitates efficient data-free knowledge transfer from the previous to the current task.
another or with the central server to protect their data pri-
vacy. For task t, we denote Dt
k(1â‰¤kâ‰¤m) as the data set
of task tof the client k, consisting of Nt
kpairs of samples
and their labels {(xkt
i,ykt
i)}Nt
k
i=1. Labels yt
ibelong to non-
overlapping subsets of classes Yt
kâŠ‚ Y, where Yis the set of
all possible classes. To provide clarity, we denote the server
and clients model for task tasStand (Ct
1,Â·Â·Â·,Ct
m), respec-
tively. The primary objective of FCIL is to maintain a global
model performance in previous and current tasks. In our
privacy-conscious scenario, the task sequence is presented
in an undisclosed order, and each client Ct
kcan exclusively
access its local data Dt
kfor task tduring that taskâ€™s train-
ing period, with no further access allowed thereafter. Note
that the models are trained in a distributed setting, with each
party having access to only a subset of the classes Yt(i.e.,
non-IID).
FCIL has recently gained significant attention for its
challenging nature and its closer alignment with real-world
scenarios, especially within the context of federated learn-
ing. Itâ€™s worth noting that in most FL applications, task
IDs are not readily available, and the preferred approach
is to train a unified model that accommodates all observed
data. Several methods have been proposed recently to ad-
dress this problem. For example, FedCIL [66] suggests
local training of the discriminator and generator to lever-
age generative replay, effectively compensating for the ab-
sence of old data and mitigating forgetting. On the other
hand, MFCL [5] and TARGET [63] introduce a data-free
approach in which the generative model is trained by the
server. This approach reduces client training time and com-
putational requirements while still eliminating the need for
access to their private data.
It is clear that DFKT plays a pivotal role in mitigating the
issue of catastrophic forgetting and ensuring privacy preser-
vation in the majority of current FCIL methods. However,
existing methods including TARGET [63], MFCL [5] and
FedCIL [66] treat DFKT as standalone modules, without in-
tegrating it into the training phases. This isolated approach
hampers their effectiveness in mitigating the model forget-ting problem.
4. Our Proposed Method: LANDER
4.1. Motivations of LANDER
For data-free FCIL, we follow the framework at [5, 63].
For task t, we firstly perform DFKT over the server model
up to task tâˆ’1(i.e.,Stâˆ’1) to generate synthetic data
Mtâˆ’1. Afterward, the server must engage in communica-
tion with the clients for crounds, where each round com-
prises two phases: client-side training and server aggrega-
tion. In client-side training, given the client k, its model Ct
k
is trained using the new task data Dt
kand the old task syn-
thetic data Mtâˆ’1. In server aggregation, the client models
are then sent to the server side to aggregate the server model
(i.e.,St=1
mPm
k=1Ct
k).
However, this naive mechanism lacks synergy between
the clients and the server to constrain the complexity of
high-confidence regions of the server model Stâˆ’1in order
to facilitate DFKT and generate more qualified synthetic old
task images.
To organize the high-confidence regions of the client
models and server model more effectively, we propose im-
posing constraints during the training of the client models.
Specifically, given the client k, (i) for new data in Dt
k, the
feature vectors of (x,y)âˆˆ Dt
kat the penultimate layer
of the client model Ct
kmust center around the LTE of the
classy, and (ii) the feature vectors of old-task synthetic data
(Ë†x,Ë†y)âˆˆ Mtâˆ’1of the client model Ct
kmust distill those of
the server model Stâˆ’1. This approach aims to organize the
high-confident regions for classes of the client models more
coherently around meaningful LTEs.
Moreover, on the server side, we aggregate the client
models to obtain the server model. Hence, the well-
organized high-confident regions for classes of the client
models are inherited by the server model. Using LTE as
the anchor, this inherited property certainly facilitates the
server model to be data-freely distilled more effectively for
generating more qualified images for old tasks, thereby al-
23872
LTE Pool ğ’«
Noisy 
Layer
ğ’µ
Generator
ğ’¢
Server
ğ’®ğ‘¡âˆ’1
â€œA photo of a dog.â€ğ‘¦ğ·ğ‘œğ‘”
ğ‘’ğ·ğ‘œğ‘”áˆ˜ğ‘“ğ’®
Client
ğ’ğ’Šğ’•ğ‘’ğ·ğ‘œğ‘”
à·œğ‘¦ğ’®
Randomly 
reinitialize 
ğ’µ for each 
batch of 
synthetic 
data!
ğ‘“ğ¶áˆ˜ğ‘“ğ’
ğ‘¦ğ¶à·œğ‘¦ğ’
ğ‘’ğ¶ğ‘ğ‘¡
 LTE Pool ğ’«
CE LossOld task training New Task training Label Old Task Label Label New Task Label
Synthetized dataset 
for previous task
Real dataset for 
task t
â€œA photo of a cat.â€
Pseudo label 
for 
Old Task 
Real label 
for New Task 
Task k -1 server  model
Task k local modelB Loss
KL Loss MSE Loss
Server
ğ’®ğ‘¡âˆ’1
áˆ˜ğ‘“ğ’® à·œğ‘¦ğ’®
Real label
Save(a) Server -side: Data -free Generation
(b) Client -side: Continual Learning
Pseudo label
ğ‘¦ğ·ğ‘œğ‘”
ğ‘¦Cat ğ‘¦CatCE Loss B Loss
q
uery
q
ueryFigure 2. General Architecture of LANDER: (a) We utilize the
previous server model (trained on task tâˆ’1) to synthesize the data.
(b) Subsequently, we use this data to train the k-th task. We use
the LTE as the anchor to constrain the features in both the client
and generator, enhancing performance.
leviating catastrophic forgetting.
Figure 1 illustrates the motivations of our proposed
LANDER. In particular, the well-organized high-confident
regions for classes of the client models are inherited by the
server model through the aggregation operation, further fa-
cilitating the DFKT phase for generating high-quality im-
ages of the old tasks.
In the next section, we first discuss how to effectively
organize the high-confidence regions of the client/server
model (Section 4.2). Then, we delve into the details of the
two main components of our method: Client-Side Train-
ing (Section 4.3) and Server-Side Data Generation (Section
4.4). The overall architecture of our method is illustrated
in Figure 2, while Algorithm 1 provides a comprehensive
overview of the entire training process for LANDER.
4.2. How to Effectively Organize the High-
confidence Regions?
We consider a typical client model C(i.e.,Ckfor client
index k) and a new task data D(i.e.,Dt
kfor some task tand
client index k). Given a new data/label pair (x,y)âˆˆ D, we
extract the feature fCat the penultimate layer and prediction
yCforxof the client model C(i.e.,yC,fC=C(x)). We
now discuss how to establish constraints on fCto effectively
organize the high-confidence regions of ConD.
In previous works, the model Cis trained solely by min-
imizing the cross-entropy loss (CE) between the prediction
yCand the actual label y(ie, CE (yC,y)). While this ap-
proach places the training data xin the high-confidence re-
gions of C, it does not impose any constraints on the feature
fCofx, leading to unsatisfactory organization and scatter-
ing throughout the regions.
This raises the need for common anchors shared between
the client and server to both constrain the feature fandAlgorithm 1: LANDER
Input: E: local epoches, I: generation rounds, g
generator training steps, T: number of tasks, c:
number of communication rounds, synthetic data
M0={}, LTE pool P={}.
1foreach each task t= 1,Â·Â·Â·, Tdo
2 Store all embeddings ey=E(Yt
y)intoP;
3 iftÌ¸= 1thenMtâˆ’1=DataGeneration (Stâˆ’1,P);
4 forcrounds do
5 foreach each client i= 1,Â·Â·Â·, mdo
6 Ct
k=St;
7 Ct
k=ClientUpdate (Ct
k,Stâˆ’1,Mtâˆ’1,P);
8 St=Pm
k=1nk
nCt
k;
9ClientUpdate( Ct
k,Stâˆ’1,Mtâˆ’1,P)
10 forEepoches do
11 foreach batch ((x,y),Ë†x)âˆ¼ Dt
kâˆª Mtâˆ’1do
12 Query eyâˆ¼ P ;
13 ift= 1then
14 Update Ct
kby minimizing Eq. 4
15 else Update Ct
kby minimizing Eq. 6 ;
16 return Ct
k
17DataGeneration( Stâˆ’1,P)
18 Initialize G,Q, Âµ, Ïƒ,Mtâˆ’1={};
19 forIrounds do
20 Initializes noisy layer Zand pseudo label Ë†y;
21 Query eË†yâˆ¼ P ;
22 forgsteps do
23 Ë†x= (G(Z(eË†y))âˆ’Âµ)/Ïƒ;
24 Update G,Z, Âµ, Ïƒ by minimizing Eq. 16;
25 Mtâˆ’1=Mtâˆ’1âˆªË†x;
26 forbatch Ë†xâˆ¼ Mtâˆ’1do
27 Update Qby minimizing Eq. 12;
28 return Mtâˆ’1
facilitate the generation of synthetic data. To address this
requirement, we propose considering the label-text embed-
ding of yas anchors and optimizing the feature embeddings
of corresponding training samples around them. In the field
of text embedding, a common observation is that text with
similar meanings tends to exhibit closer embedding prox-
imity to one another [28]. Therefore, LTE has the ability
to encapsulate useful interclass information, making it an
ideal choice for this problem.
Inspired by [53], we initially prompt the label
textYyforyby adding the text "A photo of a
{class name}"where {class name}represents the
label of the class. For example, if the label is "dog" the
prompt will be "A photo of a dog." After that, the
LTE is calculated using the pretrained LM as follows:
ey=E(Yy),âˆ€yâˆˆ Y. (1)
Importantly, the embedding eyis generated once and then
23873
LTE LTE
radius
(b) CE + MSE Loss (a) Classic CE Loss (c) CE + Bounding LossHigh -Confidence Region Bounding Region New Data
 LTE
Figure 3. Server latent space when using only CE loss; CE with
MSE Loss; and CE with B Loss to constrain the feature embed-
ding. By using our B Loss, the latent features are organized around
but still remain flexible within a defined radius rof the LTE center,
mitigating the embedding overlap problem.
stored in the LTE pool P, remaining fixed throughout the
entire training process without any further fine-tuning of the
pretrained language model E. So, all that is required here is
the text of the label. Then, we can generate the embedding
of all labels eyglobally and send it to the client just once.
To consider the LTE as the anchor, a naive approach is to
minimize the Mean Squared Error (MSE) loss between the
alignment W(Â·)of the feature fCand the label embedding
ey.
MSE (fC,ey) =eyâˆ’ W (fC)2. (2)
Bounding Loss . However, in our experiments conducted
in Section 5.3, we found that striving to make the feature
fCas close as possible to the LTE eycan exacerbate the
data imbalance problem in federated learning. This issue
arises from the fact that if a client has too few data sam-
ples for a particular class, it becomes easy to make the fea-
tures of these data almost identical to the LTE. This results
in embedding overlap between data from different clients,
causing the embeddings to be too tight and lacking natural
differences for similar classes. To address this problem, we
introduce the concept of Bounding Loss (B Loss) (Eq. 3) to
mitigate imbalance issues within heterogeneous federated
settings. This approach encourages the embeddings of sam-
ples to remain flexible within a defined radius r, rather than
pushing them to be as close as possible to the anchor LTE.
Figure 3 illustrates the comparison of our B Loss and other
methods. Using B Loss, the LTE is intended to be an anchor
point, and each data point should naturally maintain some
distance from it due to their inherent differences. This mit-
igates the overlapping problem and improves performance
in heterogeneous federated settings.
B(fC,ey) = max 
0,eyâˆ’ W (fC)2âˆ’r
,(3)
where a linear projector W(Â·)is created to align feature di-
mensions effectively at a modest cost.
4.3. Client-Side: LT-Centered Training
On the client side of task t, the server sends the synthe-
sized data from the previous task, denoted as Mtâˆ’1, and
the previous server model Stâˆ’1to a specific client k. Then,we train the local model Ct
kfor task kusing both Mtâˆ’1and
their real training data Dt
ksimultaneously.
For the new task data, we utilize the CE loss to optimize
the client model with the real training data and establish
constraints on fCusing Bounding Loss at Eq. (3) to effec-
tively organize the high-confidence regions of Ct
konDt
k.
Lcur=CE(yC,y) +Î»ltcB(fC,ey), (4)
where yC,fCare from Ct
k(x), and (x,y)âˆˆ Dt
k.
For the synthetic data, we perform the knowledge distil-
lation on Mtâˆ’1to help the model remember the old tasks.
Lpre=KL(Ë†yC,Ë†yS) +MSE (Ë†fC,Ë†fS), (5)
where the old synthetic data/label pair (Ë†x,Ë†y)âˆˆ Mtâˆ’1,
Ë†yC,Ë†fCare from Ct
k(Ë†x)andË†yS,Ë†fSare from Stâˆ’1(Ë†x). The
KL (Kullback-Leibler divergence) term is utilized to distill
the logits of the previous model into the current model. Fur-
thermore, we propose using the MSE term to ensure that the
feature embedding of the current model remains consistent
with the previous one. This consistency helps organize the
high-confidence regions of Ct
k.
Combining the above losses, we obtain the loss of the
clientCt
kas follows:
LC=Î±t
curLcur+Î±t
preLpre. (6)
Inspired by [19], to address the difficulty of preserving pre-
vious knowledge grows as the ratio of previous classes to
new classes gets larger, the scale factor Î±t
curandÎ±t
preare
adaptively set as follows:
Î±t
cur=1 + 1 /Îº
Î´Î±cur;Î±t
pre=ÎºÎ´Î±pre, (7)
where Îº= log2(|Yt|
2+ 1),Î´=q
|Y1:tâˆ’1|
|Yt|,|Yt|is the num-
ber of classes in task t,Î±curandÎ±preare the base factors.
4.4. Global-Side: LT-Centered Data Generation
Data-free generation, a forefront tool in FCIL, models
global data distribution without compromising client pri-
vacy. Techniques like [12, 39, 63] utilize a pretrained server
model Stâˆ’1and random noise zâˆ¼ N (0, I)via a genera-
torGto craft synthetic images. However, a drawback is the
use of random noise lacking meaningful information, result-
ing in low-quality samples and limitations in addressing the
forgetting problem. Our approach, inspired by [53], lever-
ages meaningful LTE as input to capture valuable interclass
information, generating high-quality images swiftly. We in-
troduce a noisy layer at the layer level, incorporating ran-
dom noise to prevent overreliance on unchanging label in-
formation. Random reinitialization of the noisy layer with
each iteration enhances diversity in synthesized images, ef-
fectively mitigating the risk of overemphasizing label infor-
mation.
23874
First, we randomly sample the pseudo label Ë†yfrom a
categorical distribution, and then we consider the LTEs of
Ë†yquery from P(i.e.eË†yâˆ¼ P ) as the input for the noisy
layerZ. Subsequently, the output of Z(eË†y)is fed into the
generator Gto produce a batch of synthetic images Ë†x.
Ë†x=G(Z(eË†y)), (8)
where Zis designed as the combination of a BatchNorm
layer and a single Linear layer, follows to [53].
Learnable Data Stats . In DFKT, reusing the training data
stats as the normalization values for synthetic data is a com-
mon technique to generate high-quality images [15, 53, 63].
In these approaches, synthetic data Ë†xis normalized using
the mean and standard deviation calculated from the entire
training dataset. However, in a real-world federated set-
ting, computing these stats can be challenging since the data
are decentralized, and providing this information may raise
privacy concerns. To address this issue, we propose using
learnable data stats (LDS) with learnable mean Âµand stan-
dard deviation Ïƒ, which are then trained together with Z
andG. From that, the synthetic data is generated as follows:
Ë†x= (G(Z(eË†y))âˆ’Âµ)/Ïƒ . (9)
We also conducted experiments to demonstrate the benefits
of using LDS, as discussed in Section 5.3. Without requir-
ing the clientâ€™s data stats, LDS still achieves comparable
performance to the use of training data stats and signifi-
cantly outperforms methods using a random stats or those
without normalization technique.
Further, to effectively perform knowledge transfer, these
synthetic data need to provide three essential properties in-
cluding: Similarity, Stability andDiversity .
Similarity. The synthetic data Ë†xneeds to be similar to the
real training data. However, due to lack of access to the
client data, we achieve this by minimizing the logits of Stâˆ’1
and the pseudo label Ë†ythrough the following cross-entropy
(CE) loss.
Loh
G=CE(Ë†yStâˆ’1,Ë†y), (10)
where we denote Ë†yStâˆ’1as the prediction of Stâˆ’1onË†x.
Stability. To enhance the generatorâ€™s stability, we use the
common batch normalization regularization [15,59] to align
the mean and variance of features at the batch normalization
layer with their running counterparts.
Lbn
G=X
l 
âˆ¥Âµl(Ë†x)âˆ’Âµlâˆ¥+âˆ¥Ïƒ2
l(Ë†x)âˆ’Ïƒ2
lâˆ¥
, (11)
where Âµl(Ë†x)andÏƒ2
l(Ë†x)are the mean and variance of the
l-thBatchNorm layer of G, and ÂµlandÏƒ2
lare the mean
and variance of the l-thBatchNorm layer of Stâˆ’1.
Diversity. To avoid the generation of similar images, LAN-
DER incorporates an additional discriminator (student) net-
workQ. Specifically, for the synthetic images Ë†x,Qistrained to minimize the difference between its predictions
and those of the server (teacher) model.
LQ=KL(Ë†yQ,Ë†yStâˆ’1) +MSE (Ë†fQ,Ë†fStâˆ’1), (12)
where Ë†yQ,Ë†fQare from Q(Ë†x).
On the other hand, by minimizing the negative KL Loss
in Eq. 14, the generator is designed to produce images that
the student network has not learned before.
Ladv
G=âˆ’Ï‰KL(Ë†yQ,Ë†yStâˆ’1), (13)
Ï‰= 1(argmax (Ë†yStâˆ’1)Ì¸=argmax (Ë†yQ)),(14)
where 1(P)yields 1 if Pis true and 0 if Pis false. This
optimization process enables the generation of a diverse set
of images that cover the entire high-confidence space of the
previous server model.
Furthermore, to utilize the capabilities of the anchored
latent space as discussed in Section 4.3, we propose to use
the Bounding Loss (Eq. 3) to ensure that the synthetic im-
ages remain within the LTEs area. This ensures that these
data contain more meaningful and helpful information for
transferring knowledge from the previous server model.
Lltc
G=B(Ë†fStâˆ’1,eË†y). (15)
In summary, the final objective for the generator G, noisy
layerZ, learnable mean Âµand standard deviation Ïƒis pro-
vided as followed:
LG,Z,Âµ,Ïƒ=Ladv
G+Î»bnLbn
G+Î»ohLoh
G+Î»ltcLltc
G.(16)
Specifically, we set Î»bn= 1.0,Î»oh= 0.5, and Î»ltc= 5 in
all experiments.
5. Experiments
5.1. Experimental Setting
We conduct experiments on CIFAR-100 [26], Tiny-
ImageNet [29], and ImageNet [27] to evaluate our proposed
approach. Following to [5, 52, 63], we partition the dataset
classes into tasks, mimicking class continual learning with
5 and 10 tasks. ResNet18 [21] serves as the backbone for
all experiments.
The evaluation employs traditional CL metrics, includ-
ing average accuracy and a forgetting score [24, 25]. Fol-
lowing to [63], our approach is compared against four base-
line types in FCIL: 1) Finetune, which learns each task on
each client sequentially; 2) FedWeIT [60], a widely used
regularization-based method; 3) FedEWC [25] and FedLwF
[32], the application of common data-free CL techniques in
the federated scenario; 4) TARGET [63], a current SOTA
FCIL method. For details on task configuration, hyper-
parameters, additional results, and visualizations, please re-
fer to the Supplemental Material .
23875
Table 1. The average accuracy (%) and forgetting values over 3 trials for all learned tasks on CIFAR-100 are presented for different task
numbers (5, 10) under both IID and non-IID settings. NIID( Î²) indicate the Dirichlet parameter is set to Î², â€™Accâ€™ denotes average accuracy,
and â€™Fâ€™ signifies the forgetting measure [24,25]. The results of Finetune, FedEWC, FedWeIT, FedLwF and TARGET are from [63] and the
best results are highlighted in bold. Itâ€™s important to note that we exclude the results of MFCL [5] and Fed-CIL [66] due to their reporting
in a significantly different setting, and their source codes are unavailable.
Acc( â†‘) F(â†“)
Data partition IID NIID (1) NIID (0.5) NIID (0.1) IID NIID (1) NIID (0.5) NIID (0.1)
Tasks T=5 T=10 T=5 T=10 T=5 T=10 T=5 T=10 T=5 T=10 T=5 T=10 T=5 T=10 T=5 T=10
Finetune 16.12 7.83 16.33 8.45 15.49 7.64 - - 78.12 75.89 77.59 74.89 74.95 71.52 - -
FedEWC [25] 16.51 8.01 16.06 8.84 16.86 8.04 - - 71.12 65.06 68.02 62.14 62.40 65.23 - -
FedWeIT [60] 28.45 20.39 28.56 19.68 24.57 15.45 - - 52.12 43.18 49.84 45.82 45.96 48.54 - -
FedLwF [32] 30.61 23.27 30.94 21.16 27.59 17.98 - - 45.32 37.71 42.71 41.03 41.25 45.23 - -
TARGET [63] 36.31 24.76 34.89 22.85 33.33 20.71 28.32 19.25 32.23 35.45 34.48 38.25 39.23 42.23 38.23 45.23
LANDER (Ours) 52.60 40.21 51.78 37.21 48.23 33.35 43.42 29.29 18.03 25.56 18.92 28.92 30.61 32.86 15.20 28.69
0102030405060708090
10 20 30 40 50 60 70 80 90100Finetune FedEWC
FedIC FedLwF
TARGET LANDER
1525354555657585
20 40 60 80 100
Classes Classes
AccuracyAccuracy10 tasks 5 tasks
Figure 4. Average accuracy on incremental tasks.
5.2. Main Results
Experiments on CIFAR100. Following [63], we conduct
experiments on 5 and 10 tasks in both IID and non-IID sce-
narios. Table 1 shows the final average accuracy of the
server model and the forgetting measure for each experi-
ment. Notably: 1) LANDER outperforms prior methods
by two digits in both accuracy and forgetting score in all
settings; 2) Increasing tasks and lowering Dirichlet param-
eters substantially reduce accuracy for compared methods;
3) Even in highly skewed settings, LANDER consistently
outperforms others, highlighting its superior performance.
Figure 4 illustrates our modelâ€™s superior performance in all
incremental tasks. The graph depicts average accuracy on
current and previous tasks, emphasizing the modelâ€™s effec-
tiveness in enabling local clients to learn new classes in a
streaming manner while mitigating forgetting.
Experiments on Large-scale Datasets. To evaluate LAN-
DERâ€™s effectiveness, we conducted additional assessments
on the more challenging Tiny-ImageNet and ImageNet
datasets. For the Tiny-ImageNet dataset (Table 2), we
present final average accuracy and forgetting measures for
all tasks in both IID and non-IID settings with 5 tasks. The
results show our method consistently achieves an approxi-
mately 3% higher average accuracy than FedLwF and TAR-
GET. Additionally, our method exhibits significantly lower
forgetting measures than FedLwF in both settings, high-
lighting its effectiveness in mitigating catastrophic forget-
ting in the presence of extreme data distributions.
For ImageNet (Table 3), as most FCIL methods do not
report results on this dataset, our comparison primarily in-Table 2. The Average Accuracy (%) and Forgetting for all learned
tasks on Tiny-ImageNet for 5 tasks.
Data partition IID NIID(1) NIID(0.5) NIID(0.1) NIID(0.05)
Acc( â†‘)FedLwF [32] 24.32 22.56 21.76 18.78 18.59
TARGET [63] 26.25 24.12 23.95 21.15 20.95
LANDER (Ours) 30.29 28.21 27.98 25.27 25.02
F(â†“)FedLwF [32] 34.57 33.94 37.23 31.43 31.19
TARGET [63] 23.43 25.12 24.58 20.54 20.83
LANDER (Ours) 21.65 23.09 23.03 17.93 18.14
Table 3. The Average Accuracy (%) on ImageNet for 5 tasks.
Method Task 1 Task 2 Task 3 Task 4 Task 5
IIDTARGET [63] 77.16 55.32 45.67 36.19 31.83
LANDER (Ours) 77.32 65.42 56.34 48.82 43.24
NIID(1)TARGET [63] 76.78 54.74 42.67 31.19 29.83
LANDER (Ours) 76.91 63.35 54.25 45.35 41.75
volves the current SOTA FCIL method, TARGET [63]. For
a fair comparison, we re-conducted TARGETâ€™s experiments
to align with our settings. The results clearly demonstrate
LANDER outperforms other methods in accuracy, show-
casing its efficacy on a large-scale dataset.
5.3. Ablations Studies
Effectiveness of LT-Centered Generation . In Figure 5a,
we analyze the impact of removing LTE constraints in data-
free generation (woLTG) and the combination of a noisy
layer and LTE as input (woNL) on our methods. The results
indicate that these two components play important roles in
our LANDER.
Effectiveness of Bounding Loss. In Figure 5b, we as-
sess the impact of different radius rin the Bounding Loss
within a heterogeneous setting. The results demonstrate
that: 1) our method achieves the best results with a ra-
dius of r=0.015, which is approximately half of the mini-
mum class-wise L2 distance in CIFAR100; 2) with a higher
Dirichlet parameter, the method without using Bounding
Loss ( r=0) exhibits significantly lower accuracy compared
to the one using r=0.015, highlighting the benefits of our
Bounding Loss in addressing the imbalance problem in fed-
erated learning.
Effectiveness of Learnable Data Stats. In Figure 5c, we
23876
(b) Bounding Loss
AccuracyAccuracy
2530354045505560
r=0.030 r=0.020
r=0.015 r=0.010
r=0.005 r=0.000(c) Learnable  Data  Stats (a) LT-Centered Generation
0102030405060
T=5 T=10
LDS TDS RDS w/o Normalization0102030405060
T=5 T=10
LANDER woLTG woNL TARGETFigure 5. (a) Accuracies in 5 and 10 tasks of our method with
and without LT-centered generation. (b) Incremental Accuracy on
CIFAR-100 for different values of rin Bounding Loss. (c) Accu-
racies for different kinds of data normalization values.
compare the normalization using our LDS with that us-
ing training data stats (TDS) [63, 66], random data stats
(RDS), and without using normalization. The results show:
1) Without normalization or with random data stats, our
workâ€™s performance decreases significantly; 2) Our LDS
performs comparably or slightly better than the TDS for
data normalization, demonstrating its benefits in our work.
5.4. Further Analysis
Comparison with Different Prompting Engineering
Templates. We assess the influence of various prompt-
ing engineering techniques for generating label text.
We introduce three approaches to prompt label text:
P1: "a class of a {class name}", P2: "a
photo of a {class name}", P3: "a photo of
a{class index }". Table 4 highlights that P2 outper-
form the best in the comparison. Furthermore, even when
using only the label index, P3 maintains a performance ad-
vantage over the best baseline. This indicates the applica-
tion of using label indices in datasets with less meaningful
labels, highlighting the practical effectiveness of LANDER.
Table 4. Accuracies of different prompt engineering methods.
IID NIID(0.5)
Text Encoder SOTA P1 P2 P3 SOTA P1 P2 P3
Accuracy 36.31 52.45 52.60 50.12 33.33 48.09 48.23 45.39
Comparison with Different Text Encoder. We eval-
uate our LANDER across three common text encoders:
Doc2Vec [28], SBERT [49], and CLIP [48]. Table 5 re-
veals that LANDER performs well across diverse language
models, leveraging their capacity to capture label-text rela-
tions. When coupled with the label-index prompt engineer-
ing method discussed earlier, our approach adapts effec-
tively to different domains, even without meaningful label-
text. Furthermore, utilizing foundational models like CLIP
enhances our modelâ€™s performance marginally, this indicate
the benefit of multimodal models to our works. Finally, we
select CLIP as the text encoder for this paper.
Table 5. Accuracies of our LANDER with different LM.
IID NIID(0.5)
Text Encoder SOTA Doc2Vec SBERT CLIP SOTA Doc2Vec SBERT CLIP
Accuracy 36.31 52.45 52.53 52.60 33.33 48.21 48.18 48.23
Real Training data LANDERâ€™s synthetic data TARGETâ€™s synthetic data
(a) LANDER (b) TARGETFigure 6. Visualizing t-SNE on synthetic and real data in a ran-
domly chosen class in CIFAR-100. Our LANDER generates data
in latent space that closely resembles real data.
Figure 7. Real data (top) vs synthetic data generated by TARGET
(middle) and our LANDER (bottom) in CIFAR-100. Each column
shows samples from the same class.
Visualization . The t-SNE visualization in Figure 6 illus-
trates synthetic data generated by our LANDER and TAR-
GET. It is evident that, using the anchor LTE, LANDER
generates samples with embeddings more similar to real
data compared to TARGET [63]. This observation under-
scores the reason behind the improvement in our approach.
Visualization on Synthetic Data . Figure 7 compares real
and synthetic data from LANDER and TARGET. Our syn-
thetic samples intentionally differ from specific training ex-
amples, preserving privacy. Unlike TARGETâ€™s meaningless
images, our samples capture essential class knowledge and
effectively represent the entire class. Consequently, the in-
clusion of synthetic samples significantly mitigates the is-
sue of catastrophic forgetting.
6. Conclusion
In this paper, we propose LANDER to mitigate the for-
getting issue in federated learning. Specifically, we treat
LTEs as anchor points of data feature embeddings during
model training, enriching surrounding feature embeddings.
In the DFKT phase, LANDER leverages thse LTE anchors
to synthesize more meaningful samples, effectively ad-
dressing forgetting. Extensive experimental results demon-
strate that our method achieves SOTA performance in FCIL.
Acknowledgements
This work was supported by ARC DP23 grant
DP230101176 and by the Air Force Office of Scientific Re-
search under award number FA2386-23-1-4044.
23877
References
[1] Hongjoon Ahn, Sungmin Cha, Donggyu Lee, and Taesup
Moon. Uncertainty-based continual learning with adaptive
regularization. Advances in neural information processing
systems , 32, 2019. 2
[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings of
the European conference on computer vision (ECCV) , pages
139â€“154, 2018. 2
[3] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuyte-
laars. Task-free continual learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11254â€“11263, 2019. 1
[4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. Advances in neural information processing sys-
tems, 32, 2019. 2
[5] Sara Babakniya, Zalan Fabian, Chaoyang He, Mahdi
Soltanolkotabi, and Salman Avestimehr. A data-free ap-
proach to mitigate catastrophic forgetting in federated class
incremental learning for vision tasks. In Thirty-seventh Con-
ference on Neural Information Processing Systems , 2023. 1,
2, 3, 6, 7
[6] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha,
and Jonghyun Choi. Rainbow memory: Continual learn-
ing with a memory of diverse samples. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8218â€“8227, 2021. 1
[7] Eden Belouadah and Adrian Popescu. Il2m: Class in-
cremental learning with dual memory. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 583â€“592, 2019. 1
[8] Tuan Anh Bui, Trung Le, Quan Tran, He Zhao, and
Dinh Phung. A unified wasserstein distributional robust-
ness framework for adversarial training. arXiv preprint
arXiv:2202.13437 , 2022. 2
[9] Thang D Bui, Cuong V Nguyen, Siddharth Swaroop, and
Richard E Turner. Partitioned variational inference: A uni-
fied framework encompassing federated and continual learn-
ing. arXiv preprint arXiv:1811.11206 , 2018. 1
[10] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang,
Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi
Tian. Data-free learning of student networks. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 3514â€“3522, 2019. 2
[11] Khanh Doan, Quyen Tran, Tuan Nguyen, Dinh Phung, and
Trung Le. Class-prototype conditional diffusion model for
continual learning with generative replay. arXiv preprint
arXiv:2312.06710 , 2023. 1
[12] Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu,
Xiao Wang, and Qi Zhu. Federated class-incremental learn-
ing. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 10164â€“10173,
2022. 1, 2, 5
[13] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor
Darrell, and Marcus Rohrbach. Adversarial continual learn-ing. In Computer Visionâ€“ECCV 2020: 16th European Con-
ference, Glasgow, UK, August 23â€“28, 2020, Proceedings,
Part XI 16 , pages 386â€“402. Springer, 2020. 2
[14] Ahmet M Elbir, Burak Soner, Sinem C Â¸ Â¨oleri, Deniz G Â¨undÂ¨uz,
and Mehdi Bennis. Federated learning in vehicular networks.
In2022 IEEE International Mediterranean Conference on
Communications and Networking (MeditCom) , pages 72â€“77.
IEEE, 2022. 1
[15] Gongfan Fang, Kanya Mo, Xinchao Wang, Jie Song, Shitao
Bei, Haofei Zhang, and Mingli Song. Up to 100x faster data-
free knowledge distillation. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , volume 36, pages 6597â€“
6604, 2022. 6
[16] Gongfan Fang, Jie Song, Xinchao Wang, Chengchao Shen,
Xingen Wang, and Mingli Song. Contrastive model inver-
sion for data-free knowledge distillation. arXiv preprint
arXiv:2105.08584 , 2021. 1
[17] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori
Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and
Daan Wierstra. Pathnet: Evolution channels gradient descent
in super neural networks. arXiv preprint arXiv:1701.08734 ,
2017. 2
[18] Michael Fu, Van Nguyen, Chakkrit Kla Tantithamthavorn,
Trung Le, and Dinh Phung. Vulexplainer: A transformer-
based hierarchical distillation for explaining vulnerability
types. IEEE Transactions on Software Engineering , 2023.
1
[19] Qiankun Gao, Chen Zhao, Bernard Ghanem, and Jian Zhang.
R-dfcil: Relation-guided representation learning for data-
free class incremental learning. In European Conference on
Computer Vision , pages 423â€“439. Springer, 2022. 2, 5
[20] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572 , 2014. 2
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770â€“778, 2016. 6
[22] Ziyue Jiang, Yi Ren, Ming Lei, and Zhou Zhao. Fed-
speech: Federated text-to-speech with continual learning.
arXiv preprint arXiv:2110.07216 , 2021. 2
[23] Peter Kairouz, H Brendan McMahan, Brendan Avent,
AurÂ´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum-
mings, et al. Advances and open problems in federated learn-
ing. Foundations and TrendsÂ® in Machine Learning , 14(1â€“
2):1â€“210, 2021. 1
[24] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler
Hayes, and Christopher Kanan. Measuring catastrophic for-
getting in neural networks. In Proceedings of the AAAI con-
ference on artificial intelligence , volume 32, 2018. 1, 6, 7
[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the national academy of sci-
ences , 114(13):3521â€“3526, 2017. 1, 2, 6, 7
23878
[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 6
[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Advances in neural information processing systems ,
25, 2012. 6
[28] Quoc Le and Tomas Mikolov. Distributed representations
of sentences and documents. In International conference on
machine learning , pages 1188â€“1196. PMLR, 2014. 4, 8
[29] Ya Le and Xuan Yang. Tiny imagenet visual recognition
challenge. CS 231N , 7(7):3, 2015. 6
[30] Timoth Â´ee Lesort, Hugo Caselles-Dupr Â´e, Michael Garcia-
Ortiz, Andrei Stoian, and David Filliat. Generative models
from the perspective of continual learning. In 2019 Interna-
tional Joint Conference on Neural Networks (IJCNN) , pages
1â€“8. IEEE, 2019. 2
[31] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia
Smith. Federated learning: Challenges, methods, and future
directions. IEEE signal processing magazine , 37(3):50â€“60,
2020. 1
[32] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelli-
gence , 40(12):2935â€“2947, 2017. 6, 7
[33] Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner.
Data-free knowledge distillation for deep neural networks.
arXiv preprint arXiv:1710.07535 , 2017. 1, 2
[34] David Lopez-Paz and Marcâ€™Aurelio Ranzato. Gradient
episodic memory for continual learning. Advances in neu-
ral information processing systems , 30, 2017. 1
[35] Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi
Lan, and Ming Gao. DFRD: Data-free robustness distillation
for heterogeneous federated learning. In Thirty-seventh Con-
ference on Neural Information Processing Systems , 2023. 2
[36] Yuhang Ma, Zhongle Xie, Jue Wang, Ke Chen, and Lidan
Shou. Continual federated learning based on knowledge dis-
tillation. In Proceedings of the Thirty-First International
Joint Conference on Artificial Intelligence , volume 3, 2022.
1, 2
[37] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083 , 2017. 2
[38] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In Pro-
ceedings of the IEEE conference on Computer Vision and
Pattern Recognition , pages 7765â€“7773, 2018. 2
[39] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data.
InArtificial intelligence and statistics , pages 1273â€“1282.
PMLR, 2017. 1, 5
[40] Paul Micaelli and Amos J Storkey. Zero-shot knowledge
transfer via adversarial belief matching. Advances in Neu-
ral Information Processing Systems , 32, 2019. 2
[41] Alexander Mordvintsev, Christopher Olah, and Mike Tyka.
Inceptionism: Going deeper into neural networks. 2015. 2[42] Dinh C Nguyen, Ming Ding, Pubudu N Pathirana, Aruna
Seneviratne, Jun Li, and H Vincent Poor. Federated learning
for internet of things: A comprehensive survey. IEEE Com-
munications Surveys & Tutorials , 23(3):1622â€“1658, 2021. 1
[43] Tuan Nguyen, Van Nguyen, Trung Le, He Zhao, Quan Hung
Tran, and Dinh Phung. Cycle class consistency with dis-
tributional optimal transport and knowledge distillation for
unsupervised domain adaptation. In Uncertainty in Artificial
Intelligence , pages 1519â€“1529. PMLR, 2022. 1
[44] Thanh Nguyen-Duc, Trung Le, He Zhao, Jianfei Cai, and
Dinh Phung. Adversarial local distribution regularization
for knowledge distillation. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision ,
pages 4681â€“4690, 2023. 1
[45] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa
Eschenhagen, Richard Turner, and Mohammad Emtiyaz E
Khan. Continual deep learning by functional regularisation
of memorable past. Advances in Neural Information Pro-
cessing Systems , 33:4453â€“4464, 2020. 2
[46] Cuong Pham, Van-Anh Nguyen, Trung Le, Dinh Phung,
Gustavo Carneiro, and Thanh-Toan Do. Frequency at-
tention for knowledge distillation. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 2277â€“2286, 2024. 1
[47] Aman Priyanshu, Mudit Sinha, and Shreyans Mehta. Con-
tinual distributed learning for crisis management. arXiv
preprint arXiv:2104.12876 , 2021. 2
[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748â€“8763. PMLR, 2021. 8
[49] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence
embeddings using siamese bert-networks. arXiv preprint
arXiv:1908.10084 , 2019. 8
[50] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-
licrap, and Gregory Wayne. Experience replay for continual
learning. Advances in Neural Information Processing Sys-
tems, 32, 2019. 2
[51] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott San-
ner, Hyunwoo Kim, and Jongseong Jang. Online class-
incremental continual learning with adversarial shapley
value. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 35, pages 9630â€“9638, 2021. 2
[52] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. Advances in
neural information processing systems , 30, 2017. 2, 6
[53] Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Ha-
randi, Quan Hung Tran, and Dinh Phung. Nayer: Noisy layer
data generation for efficient and effective data-free knowl-
edge distillation. arXiv preprint arXiv:2310.00258 , 2023. 1,
2, 4, 5, 6
[54] Quyen Tran, Lam Tran, Khoat Than, Toan Tran, Dinh
Phung, and Trung Le. Koppa: Improving prompt-
based continual learning with key-query orthogonal pro-
jection and prototype-based one-versus-all. arXiv preprint
arXiv:2311.15414 , 2023. 1
23879
[55] Anastasiia Usmanova, Franc Â¸ois Portet, Philippe Lalanda,
and German Vega. A distillation-based approach integrat-
ing continual learning and federated learning for pervasive
services. arXiv preprint arXiv:2109.04197 , 2021. 2
[56] Gido M Van de Ven and Andreas S Tolias. Three scenar-
ios for continual learning. arXiv preprint arXiv:1904.07734 ,
2019. 2
[57] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, Zhengyou Zhang, and Yun Fu.
Incremental classifier learning with generative adversarial
networks. arXiv preprint arXiv:1802.00853 , 2018. 2
[58] Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker,
Jiang Bian, and Fei Wang. Federated learning for healthcare
informatics. Journal of Healthcare Informatics Research ,
5:1â€“19, 2021. 1
[59] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong
Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz.
Dreaming to distill: Data-free knowledge transfer via deep-
inversion. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8715â€“
8724, 2020. 1, 2, 6
[60] Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang,
and Sung Ju Hwang. Federated continual learning with
weighted inter-client transfer. In International Conference
on Machine Learning , pages 12073â€“12086. PMLR, 2021. 1,
2, 6, 7
[61] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
conference on machine learning , pages 3987â€“3995. PMLR,
2017. 2
[62] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu,
Shouhong Ding, Chunhua Shen, and Chao Wu. Dense: Data-
free one-shot federated learning. Advances in Neural Infor-
mation Processing Systems , 35:21414â€“21428, 2022. 2
[63] Jie Zhang, Chen Chen, Weiming Zhuang, and Lingjuan Lyu.
Target: Federated class-continual learning via exemplar-free
distillation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 4782â€“4793, 2023. 1,
2, 3, 5, 6, 7, 8
[64] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-
Tao Xia. Maintaining discrimination and fairness in class
incremental learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
13208â€“13217, 2020. 1
[65] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon
Civin, and Vikas Chandra. Federated learning with non-iid
data. arXiv preprint arXiv:1806.00582 , 2018. 1
[66] Martin Zong, Zengyu Qiu, Xinzhu Ma, Kunlin Yang,
Chunya Liu, Jun Hou, Shuai Yi, and Wanli Ouyang. Better
teacher better student: Dynamic prior knowledge for knowl-
edge distillation. In The Eleventh International Conference
on Learning Representations , 2023. 1, 2, 3, 7, 8
23880
