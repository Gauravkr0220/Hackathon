From Feature to Gaze: A Generalizable Replacement
of Linear Layer for Gaze Estimation
Yiwei Bao Feng Lu*
State Key Laboratory of VR Technology and Systems, School of CSE, Beihang University
{baoyiwei, lufeng }@buaa.edu.cn
Abstract
Deep-learning-based gaze estimation approaches often
suffer from notable performance degradation in unseen tar-
get domains. One of the primary reasons is that the Fully
Connected layer is highly prone to overfitting when map-
ping the high-dimensional image feature to 3D gaze. In this
paper, we propose Analytical Gaze Generalization frame-
work (AGG) to improve the generalization ability of gaze
estimation models without touching target domain data.
The AGG consists of two modules, the Geodesic Projection
Module (GPM) and the Sphere-Oriented Training (SOT).
GPM is a generalizable replacement of FC layer, which
projects high-dimensional image features to 3D space ana-
lytically to extract the principle components of gaze. Then,
we propose Sphere-Oriented Training (SOT) to incorpo-
rate the GPM into the training process and further improve
cross-domain performances. Experimental results demon-
strate that the AGG effectively alleviate the overfitting prob-
lem and consistently improves the cross-domain gaze esti-
mation accuracy in 12 cross-domain settings, without re-
quiring any target domain data. The insight from the Ana-
lytical Gaze Generalization framework has the potential to
benefit other regression tasks with physical meanings.
1. Introduction
Eye gaze reveals where human attention lands, which has
been widely applied in a variety of territories, such as
VR/AR systems [3, 15, 26], medical analysis [4, 13, 14] and
human-computer interaction[17, 27, 34]. Gaze estimation
methods can be classified into two categories: model-based
approaches and appearance-based approaches. Both ap-
proaches have their own strengths and weaknesses. Model-
based approaches estimate gaze by modeling the anatomical
structure of the eyeball. These methods achieve remarkable
accuracy in controlled environment. But they typically re-
quire dedicated hardware such as infrared cameras and light
sources. Appearance-based approaches use cost-effective
*Corresponding Author. This work was supported by the National Nat-
ural Science Foundation of China (NSFC) under Grant 62372019.
Generalization
FC Layer
Face Images
Gaze1500+ params
FeaturesConventional
512D 2D
Geodesic Projection Module ( GPM ) â€”10 params
Isometric Sphere
Mapping Alignment
ğœ“ğœ“
ğœƒğœƒLess Overfitting
Unseen
DomainsSource
DomainGeneralize
512D 3D 2D
Replaced byâ†’ Overfitted
Figure 1. Overview of the proposed AGG framework for general-
izing gaze estimation models to unseen target domains.
web cameras. They typically train Convolutional Neural
Networks (CNNs) in an end-to-end way, enabling them to
predict gaze direction from user face/eye images directly. In
recent years, appearance-based approaches have garnered
great interest due to their simplified hardware requirements
and the potential for widespread applications.
However, appearance-based methods suffer from severe
performance degradation in cross-domain settings. To im-
prove the cross-domain performance, various domain adap-
tation approaches have been proposed, i.e., adversarial
learning [12, 32], contrastive learning [33] and collaborative
learning [20]. However, these approaches require a num-
ber of target domain samples for adaptation, which is not
always attainable in real-world scenarios. More recently,
Cheng et al. proposed to generalize gaze estimation model
by purifying gaze feature during source domain training
[8]. The gaze generalization task is more practical yet more
challenging, because it does not have access to any target
domain data.
One of the significant reasons for the poor cross-domain
performances is the overfitting problem. Gaze estimation
CNNs are trained to extract high-dimensional image fea-
tures ( e.g. 512D) from input face images and map these
features to gaze (3D unit vector) by a Fully Connected layer
with thousands of parameters. The numerous parameters
of the FC layer easily overfit to gaze irrelevant factors
within the high-dimensional image features during the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1409
end-to-end training process. One possible solution is to
extract gaze-related information from the image features,
i.e. the Principle Component of Gaze (PCG), while exclud-
ing other irrelevant information.
In this paper, we introduce the Analytical Gaze Gener-
alization framework(AGG), a novel gaze generalization ap-
proach that connects the high-dimensional image features
to gaze analytically. The AGG consists of two modules,
the Geodesic Projection Module (GPM) and the Sphere-
Oriented Training (SOT) module. Given a pretrained gaze
estimation model, the GPM serves as a replacement of
the last FC layer, alleviating the overfitting issue by an-
alytical projection and alignment. Based on the observa-
tion that the geodesic distance between the image features
is proportional to the angular gaze difference between sam-
ples, the GPM projects the high-dimensional image fea-
tures to 3D space by the geodesic distance to extract the
Principle Component of Gaze. Then, we estimate gaze
from the projected features by the proposed Sphere Align-
ment algorithm using physical rotation and scaling with
only 10 learnable parameters. Next, we propose the Sphere-
Oriented Training to incorporate the GPM into the training
process to improve the generalization ability of the whole
network. Given source domain labels, the Sphere-Oriented
Training optimizes the gaze estimation network based on
the reverse process of the GPM.
Experiments show that, by only replacing the last FC
layer of the baseline model with GPM, both the accu-
racy and the stability in cross-domain testing have been
improved. After optimizing the model using the Sphere-
Oriented Training in the source domain, the performance
is further improved and outperforms SOTA gaze estimation
methods in multiple different cross-domain settings. The
primary contributions of this work are as follow:
â€¢ We propose the Geodesic Projection Module (GPM), a
novel method that predicts gaze from the geodesic dis-
tance between the image features analytically. As a novel
and explainable approach for gaze estimation, the insight
GPM presents may also inspire other regression tasks like
pose estimation.
â€¢ We propose the AGG framework for generalizable gaze
estimation. The AGG framework utilizes the Sphere-
Oriented Training module to optimize the gaze estima-
tion model based on the reverse process of the GPM for
better generalization ability.
â€¢ Experimental results illustrate that the proposed
AGG achieves consistent improvements in 12 different
cross-dataset settings. The AGG improves the gener-
alization ability of the baseline model up to 35.79%
without touching target domain data.
The subsequent sections are organized as follow: in
Sec. 3, we present the motivation and design principles of
the AGG through several validation experiments. In Sec. 4,we provide a detailed introduction to the AGG framework.
In Sec. 5, we assess the proposed method both quantita-
tively and qualitatively.
2. Related Work
2.1. Gaze Estimation
There are two mainstream gaze estimation approaches,
the model-based approaches and the appearance-based ap-
proaches. Model-based approaches estimate gaze by recon-
structing the anatomy structure of the eyeball [11]. These
methods achieve remarkable accuracy but also require per-
sonal calibration and dedicated devices such as depth sen-
sors [28, 35], infrared cameras [10, 29] and lights [10, 19].
Appearance-based approaches usually estimate gaze
from user images captured by a single web camera. Early
methods estimate gaze from eye images by traditional ma-
chine learning algorithms like manifold embedding [25] and
adaptive linear regression [21]. Lu et al. propose to esti-
mate eye rotation by measuring the geodesic distance be-
tween eye images [22]. Wang et al. propose to combine the
eye appearance with eye geometry by a Hierarchical Gen-
erative Model []. More recently, a number of gaze estima-
tion datasets have been collected [9, 12, 16, 38, 39]. These
datasets provide hundreds of thousands of user images with
gaze labels, which makes deep-learning-based gaze estima-
tion possible. Representative studies include gaze estima-
tion using convolutional neural networks (CNNs) [37] with
eye images [7, 37] or face images [1, 5, 6, 16, 38]. Some
previous studies also represent gaze features as low dimen-
sional manifolds for personalization [23] and unsupervised
learning [36]. But these methods still construct manifolds
by data-driven learning approach with supervision like gaze
redirection. Our method analytically connects the high-
dimensional image feature to gaze.
2.2. Cross-domain Gaze Estimation
One of the major problem of the deep-learning-based ap-
proaches is that the performance degrades severely when
testing on a different domain. To improve the cross-domain
performance, a number of unsupervised domain adaptation
methods have been proposed. Liu et al. propose to adapt
the model to target domain with the guidance of outliers
by collaborative learning [20]. Wang et al . utilize con-
trastive learning to pull features with close gaze labels to-
gether [33]. Bao et al. propose to improve the cross-dataset
accuracy by the rotation consistency of gaze [2]. Neverthe-
less, above methods require target domain images to train
domain specific models, which is infeasible in real world
settings, as target domain data is often inaccessible. Re-
cently, Cheng et al. propose to improve the generalization
ability of gaze estimation model by purifying gaze feature in
source domain[8]. The gaze generalization problem without
1410
ğœ¶ğœ¶Distance between Features
Gaze Difference ğœ¶ğœ¶(Â°)Spherical Dist.
L2 Dist.
Gaze
Spherical Distance âˆğœ¶ğœ¶Feature Space Physical Space
Proportional to Î±
Figure 2. Observation: The gaze differences between samples
can be linearly represented by the geodesic distances between ex-
tracted features. Such characteristic of the feature space is con-
sistent with the physics of gaze: the distance along the spherical
surface is proportional to the gaze differences.
access to target domain data is more challenging and yet to
be solved.
3. Analytical Gaze Estimation from Features
The aim of this paper is to design a replacement of the last
regression FC layer to alleviate the overfitting problem. In
this section, we try to answer the following key questions
by a series of validation experiments:
Question 1: How many dimensions does the Principle
Components of Gaze (PCG) in the high-dimensional image
features have?
Question 2: How to extract the PCG from the high-
dimensional image features for generalizable gaze estima-
tion?
3.1. The Overfitting Problem
To explore above questions, we initially pretrain a com-
monly used baseline model, i.e. ResNet-18 in the source
domain (ETH-XGaze [39]) for analysis.
Given the source domain Ds={xi,yi|N
i=1}where xi
is the face image and yi= (xi, yi, zi)is the ground truth
unit gaze direction vector, we pretrain the gaze estimation
model by L1loss function:
fi=FÎ¸1(xi),
arg min
Î¸1,Î¸2(L1(yi, LÎ¸2(fi))|N
i=1).(1)
where FÎ¸1(Â·)is the feature extractor CNN, fiis the 512D
high-dimensional image feature and LÎ¸2(Â·)is the last Fully
Connected layer which estimates gaze from fi.
Previous study [8] has proven that the feature extracted
by the pretraind baseline model fiencompasses not only
gaze information, but also other visual contents, including
appearance, illumination and head pose. Thus, the dimen-
sion of the PCG should be less than the dimension of fi
itself. Combined with the fact that gaze direction is a 3D
Top View
Side View
Gaze Label
Top View
Side View
Gaze Label
Top View
Side View
Principle Gaze Feature
Top View
Side View
Principle Gaze Feature 
Yaw within [âˆ’50Â°,50Â°] Pitch within [âˆ’50Â°,50Â°]
Figure 3. Following the observation in Fig. 2, we construct
PGF by projecting high-dimensional features to the 3D space us-
ing geodesic distance. The PGF shares the same spherical distri-
bution pattern as the gaze label. Data points are colored by gaze
yaw and pitch angles respectively.
Top View
Side View
Gaze Label
Top View
Side View
Isomap
Top View
Side View
T-SNE
Top View
Side View
LLE
Figure 4. Projecting features extracted by the pretrain gaze estima-
tion model into 3D space by varies distance metrics. Projections
by geodesic distance (Isomap) show identical spherical distribu-
tion pattern as the gaze label. Colored by gaze yaw angles.
unit vector with 2 degrees of freedom, for the answer of
Question 1, we hypothesis that the theoretical minimum
dimension of the PCG should be 2D, identical to the gaze
ground truth. The regression FC layer LÎ¸2is at a high risk
of overffiting since the number of parameters in LÎ¸2and
the dimension of fiare way beyond the minimum required
quantity.
3.2. Gaze on the 3D Sphere
To answer Question 2 , we examine the distribution of the
gaze ground truth. In the physical space, gaze distributes
across the surface of the 3D unit sphere. The distance alone
the data manifold i.e. the spherical surface is proportional
to the angular differences between gaze directions.
To verify if similar relationship also exists in the feature
space, we visualize the distance alone the data manifold in
1411
the feature space, i.e. the geodesic distance between fiin
Fig. 2. The result reveals an important observation:
Observation: the geodesic distance between fiis in a
strong direct proportion to the angular gaze differences be-
tween samples.
This observation leads to a possible answer of Question
2: The PCG within the high-dimensional image features
can be extracted by the geodesic distance. In the fol-
lowing step, we leverage the geodesic distance to extract
the Principle Gaze Feature (PGF) from fifor generalizable
gaze estimation.
3.3. Mapping Features to 3D Space Analytically
According to above observations, we utilize the geodesic
distance to extract the PGF from the high-dimensional im-
age features. Specifically, we project the high-dimensional
image features to the 3D space using geodesic distance,
i.e. Isometric Mapping (Isomap) [30]. Results in Fig. 3
demonstrate that the image features after projection (the
PGF) share similar distribution pattern as gaze: the
PGF distributes across the surface of a 3D sphere approxi-
mately. To better demonstrate the distribution pattern, we
color the PGFs within certain gaze range with pitch and
yaw angles respectively. It is obvious that gaze pitch and
yaw angle change monotonically alone the longitudinal and
latitudinal direction of the PGF Sphere. The Principle Gaze
Feature preserves gaze information while excluding unnec-
essary factors from fito alleviate the overffiting problem.
Generalizable gaze estimations could be made by simply
aligning the PGF Sphere with the unit sphere of gaze distri-
bution using the gaze label. This alignment process consists
of simple physical operation including rotation and scaling
with only 10 parameters, which is less unlikely to overfit.
We further utilize this idea to optimize the feature extrac-
tor CNN FÎ¸1for better generalization ability. The detailed
implementation will be introduced in Sec. 4.
3.4. Choice of Distance Metrics
In this section, we validate some other possible answers of
Question 2 . Specifically, we project the image features
fiinto the 3D space using two other dimension reduction
methods: the Local Linear Embedding (LLE) [24] and the
T-distributed Stochastic Neighbor Embedding (t-SNE) [31].
It is obvious in Fig. 4 that only the results of Isomap exhibit
similar distribution pattern to the gaze label. Although the
distributions of other dimension reduction methods also ex-
hibit some directional characteristics, their overall distribu-
tions do not show a clear geometric pattern like Isomap.
Above results prove that geodesic distance is the key to
extract the Principle Components of Gaze from the high-
dimensional image feature. In the next section, we explain
the specific implementation of the AGG framework4. Method
We propose the Analytical Gaze Generalization framework,
a domain generalization method for gaze estimation. The
AGG framework comprises two key modules: the Geodesic
Projection Module (GPM) module and the Sphere-Oriented
Training (SOT) module. The Geodesic Projection Mod-
ule predicts gaze analytically from the pretrained im-
age feature by constructing the Principle Gaze Feature us-
ing geodesic distance. Next, the Sphere-Oriented Train-
ing module optimizes the pretrained gaze estimation net-
work according to the reverse process of the GPM for better
generalization ability. The overview of the Analytical Gaze
Generalization framework is presented in Fig. 5.
4.1. Geodesic Projection Module
The Geodesic Projection Module mainly consists of two
steps. First, we project the image features extracted by
the pretrained gaze estimation model into 3D space us-
ing geodesic distance, i.e. Isomap. The projected feature
(named the Principle Gaze Feature) distributes alone the
surface of a sphere (named the PGF Sphere) approximately.
Then, we align the PGF Sphere with the unit sphere repre-
sents the gaze label distribution to predict gaze directions
analytically.
First, we pretrain a gaze estimation model consists of a
feature extractor CNN FÎ¸1and a Fully Connected layer LÎ¸2
for gaze regression in the source domain Ds={xi,yi|N
i=1}
usingL1loss function according to Eq. (1). Then, we
freeze the gaze estimation model and extract the high-
dimensional image features (512D in our experiments) by
{fi=FÎ¸1(xi)|Nâ€²
i=1}. The Principle Gaze Feature eiâˆˆR3
is constructed by projecting the image features fiinto the
3D space using Isomap algorithm:
{ei|Nâ€²
i=1}=Isomap ({fi|Nâ€²
i=1}). (2)
Since the Principle Gaze Feature distributes across the
surface of a 3D sphere approximately, the next step is to
predict gaze directions by aligning this PGF Sphere with
the unit gaze label sphere. First, we locate the center of the
PGF Sphere Ocand rotate it to align the orientation with
the unit gaze label sphere:
eâ€²
i=R(eiâˆ’Oc) = (xeâ€²
i, yeâ€²
i, zeâ€²
i)T, (3)
where Ris the rotation matrix. Then, we calculate the Euler
angles of eâ€²
iand predict gaze directions yi= (Î¸â€², Ïˆâ€²)by
simple linear fittings:
ï£±
ï£²
ï£³Î¸â€²
i=k1arctan (xeâ€²
i
zeâ€²
i) +b1,
Ïˆâ€²
i=k2arcsin (yeâ€²
i) +b2.(4)
The final gaze prediction yâ€²
i= (xâ€²
i, yâ€²
i, zâ€²
i)is obtained
through converting the Euler angle predictions (Î¸â€²
i, Ïˆâ€²
i,0)
to unit direction vectors. We formalize the process
1412
Dğ¬ğ¬
Dğ’•ğ’•
High-dimensional
Image Features
ğœ“ğœ“
ğœƒğœƒğ‘¥ğ‘¥ğ‘§ğ‘§ğ‘¦ğ‘¦
Sphere Fitting & 
Rotation Eq.(3)Euler Angle 
Fitting Eq.(4)Principle Gaze 
Feature (PGF) in 3D
Sphere Alignmentâ‘ GPM -Geodesic Projection Module (Sec. 4.1)
Gaze
Predictions
ï¼ˆğ‘¥ğ‘¥ ,ğ‘¦ğ‘¦,ğ‘§ğ‘§ï¼‰ï¼ˆğ‘¥ğ‘¥ ,ğ‘¦ğ‘¦,ğ‘§ğ‘§ï¼‰ï¼ˆğ‘¥ğ‘¥ ,ğ‘¦ğ‘¦,ğ‘§ğ‘§ï¼‰Gaze Label
Isometric
Propagator
PGFIdeal 
PositionActual 
Position
Sphere
Alignment
L1Loss
Source Domainâ‘¡Sphere- Oriented Training (Sec. 4.2)
UpdateIsometric
Mapping
â‘¢Inference â€”AGG (proposed) Inference (Conventional)
Face Images Gaze Predictions FC Layer512D
512D features
 Face Images Gaze Predictions
â‘ GPM10 params 1500+ params Dğ¬ğ¬
Dğ’•ğ’•
2DFigure 5. Overview of the proposed AGG framework. We propose two modules, the Geodesic Projection Module (GPM) and the Sphere-
Oriented Training module. We replace the last Linear layer of the pretrained baseline model with GPM to estimate gaze from the
high-dimensional image feature analytically. The Sphere-Oriented Training optimizes the gaze estimation model according to the reverse
process of the GPM for better generalization ability.
from the Principle Gaze Feature eito gaze prediction yâ€²
i
as the Sphere Alignment algorithm: yâ€²
i=SAÎ¸s(ei),
where Î¸sis the set of 10 learnable parameters Î¸s=
{Oc,R, k1, k2, b1, b2}. These parameters are obtained by
minimizing the angular difference between gaze prediction
yâ€²
iand the source domian gaze label yi:
arg min
Î¸s(Angular (yi, SA Î¸s(ei))|N
i=1). (5)
Since the GPM only contains 10 learnable parameters,
we only randomly choose 2000 source domain samples to
optimize Î¸sin our experiments. In the test time, features
of target domain samples are concatenated to the geodesic
distance map built in the source domain for Isometric Map-
ping. The parameters of the SA algorithm remain fixed in
the Sphere-Oriented Training and test time.
4.2. Sphere-Oriented Training
The Geodesic Projection Module predicts gaze from the
image feature analytically, thus the performance will be af-
fected by the quality of the image feature. To extract gener-
alizable image features, we propose Sphere-Oriented Train-
ing to optimize the pretrained feature extractor CNN FÎ¸1
with the reverse process of the GPM in the source domain.
Given a source domain sample {xi,yi}, we could re-
versely calculate the ideal position of the corresponding
Principle Gaze Feature since the Sphere Alignment algo-
rithm is totally analytical: Ë†ei=SAâˆ’1(yi).Theoretically,the pretrained CNN could be optimized by minimizing the
distance between the ideal position and actual position of
the Principle Gaze Feature on the PGF Sphere:
arg min
Î¸1(L1(Ë†ei, Isomap (FÎ¸1(xi)))|N
i=1). (6)
Unfortunately, it is difficult to integrate the Isomap
into the back propagation process because it is both time
and space consuming. The time complexity of Isomap is
O(N2logN)and the space demand is O(N2), where N
is the number of samples. To solve this issue, we pro-
pose the Isometric Propagator IPÎ¸3(Â·)to parameterize the
Isomap algorithm. Isometric Propagator is a three layer
MLP trained to simulate the Isomap function at the begin-
ning of Sphere-Oriented Training. We freeze the parameter
of the pretrained CNN FÎ¸1and train the Isometric Propaga-
tor as follow:
arg min
Î¸3(L1(Isomap (fi), IPÎ¸3(fi))|N
i=1). (7)
After the training of the Isometric Propagator, we freeze
its parameters and replace the Isomap with it to train the fea-
ture extractor CNN. The actual Sphere-Oriented Training is
formalized as:
arg min
Î¸1(L1(Ë†ei, IPÎ¸3(FÎ¸1(xi)))|N
i=1). (8)
Note that the Isometric Propagator is only used during
the source domain training. At test time, we predict gaze
1413
by the proposed GPM with Isomap for better generalization
ability. Parameters of the GPM are determined before the
Sphere-Oriented Training and remain fixed.
Owing to the advantage that the GPM suffers less from
the overfitting problem than the FC layer, the purpose of
the SOT is to utilize this advantage to optimize the gaze
estimation model for better generalization performance by
incorporating the GPM into the training process.
4.3. Implementation Details
We employ the AGG by PyTorch. For the training of the
pretrain model, IP and Sphere-Oriented Training, we use
the Adam optimizer with a learning rate of 10âˆ’4. The model
is pretrained for 10 epochs. We choose the last epoch as the
baseline model. The Sphere-Oriented Training is also 10
epochs, while the IP is trained for 100 epochs on 2000 ran-
domly selected samples. Batch sizes are set to 512. For
Isomap, we use the implementation of Scikit-learn and the
number of neighbor is set to 300. Pixel values are normal-
ized to [0,1], and no data augmentation is employed.
5. Experiments
5.1. Data Preparation
We conduct experiments on four commonly used gaze es-
timation datasets: ETH-XGaze ( DE) [39], Gaze360 ( DG)
[12], MPIIFaceGaze ( DM) [38] and EyeDiap ( DD) [9]. We
normalize the data following the techniques in [38]. ETH-
XGaze: 756kimages captured by high resolution cameras
in laboratory environment with large gaze range. We divide
the last 5 subjects as test set. Gaze360: 101kimages cap-
tured by a 360â—¦camera on streets with large gaze range.
We only use images with frontal faces in our experiments.
MPIIFaceGaze: 45kimages (standard test set) captured by
web camera during daily usage of laptop computers. The
gaze range of DMis less than half the range of DEandDG.
Thus, we only use DMas target domain. EyeDiap: 16k
images captured under laboratory environment with screen
and floating targets. As the number of images is signifi-
cantly less than other datasets, we only use DDas target
domain.
In addition, the cross-domain error between DEand
DGis extremely large (around 20â—¦). Thus, we exclude the
DEâ†’DGandDGâ†’DEsettings in our experiments, which
is also excluded in previous studies [2, 8, 20, 33].
5.2. Quantitative Evaluation
5.2.1 Evaluation of the GPM
We first evaluate the proposed GPM by replacing the last
FC layer with the GPM without changing other parameters
of the baseline model. The mean and the standard devi-
ation (std) of the estimation error from the last 5 epochsTable 1. Results of simply replacing the last FC layer with the
proposed GPM in inference. Results are the mean and std for the
final 5 epochs. Note that the modest reduction in within-dataset
accuracy is reasonable, since GPM is designed for generalization.
DEâ†’DMDEâ†’DD within DE
ResNet-18 8.66Â±0.53 7.76Â±0.29 5.37Â±0.24
ResNet-18 + GPM 7.87Â±0.23 7.72Â±0.33 5.74Â±0.08
ResNet-50 6.92Â±0.86 8.61Â±0.88 5.27Â±0.56
ResNet-50 + GPM 6.56Â±0.41 8.10Â±0.57 5.29Â±0.07
DGâ†’DMDGâ†’DD within DG
ResNet-18 8.59Â±0.57 10.87Â±1.52 12.59Â±0.14
ResNet-18 + GPM 8.57Â±0.41 10.94Â±0.85 12.64Â±0.10
ResNet-50 8.48Â±1.01 10.76Â±0.78 11.97Â±0.30
ResNet-50 + GPM 8.14Â±0.47 9.77Â±1.00 12.07Â±0.18
are shown in Tab. 1. The proposed GPM achieves better
performance in 7 out of 8 cross-domain experiments. In
addition, the GPM also performs more stably across differ-
ent epochs. These results demonstrate the advantage of the
proposed GPM over the traditional FC layer. The within-
dataset estimation errors of the GPM are slightly higher.
It is reasonable since GPM is designed for generalizing to
unseen domains. The higher within-dataset accuracy of the
FC layer is highly likely achieved by overfitting since it per-
forms worse in cross-domain tests.
5.2.2 Evaluation of the AGG Framework
In this section, we evaluate the effectiveness of the
AGG framework, which optimizes the gaze estimation
model to further improve generalization ability. We con-
duct experiments in 4 cross domain settings with 3 baseline
models, as shown in Tab. 2. The performances of baseline
models is quite different, due to their architectures and the
different characteristics of each domain. Nevertheless, the
proposed AGG framework achieves stable improvements in
all 12 cross-domain settings, proves that the AGG frame-
work is robust to different baseline models and source do-
mains. The AGG framework achieves improvements as
large as 35.79% without target domain data. We also re-
port the within dataset performance after generalization for
reference. As expected, the within dataset performance de-
creases mildly since the model is optimized for domain gen-
eralization. Above results demonstrate the effectiveness of
the proposed Sphere-Oriented Training, which improves the
generalization performance of varies baseline models sig-
nificantly.
5.2.3 Comparison with SOTA Methods
In Tab. 3, we compare the AGG framework with SOTA
gaze estimation methods [6, 12, 38] and gaze general-
1414
Table 2. Performance of the proposed AGG framework. Results are gaze estimation error in degrees. The proposed AGG achieves stable
improvements up to 35.79 %in all 12 cross-domain settings without using any target domain data. The symbolâˆ—indicates within-dataset
experiments for reference. Note that the modest reduction in within-dataset accuracy is to be expected for domain generalization methods.
Method DEâ†’DM DEâ†’DD DGâ†’DM DGâ†’DD within DEâˆ—within DGâˆ—
ResNet-18 8.64 7.83 8.68 12.35 5.08 12.73
ResNet-18+AGG 7.10â–¼17.82% 7.07â–¼9.71% 7.87â–¼9.33% 7.93â–¼35.79% 5.56â–²9.45% 13.03 â–²2.36%
ResNet-50 6.04 7.47 10.14 11.76 5.35 12.37
ResNet-50+AGG 5.91â–¼2.15% 6.75â–¼9.64% 9.2â–¼9.27% 11.36 â–¼3.40% 6.29â–²17.57% 15.63 â–²26.35%
VGG16 9.5 19.14 14.61 19.94 5.12 12.15
VGG16+AGG 9.13â–¼3.89% 17.2â–¼10.14% 11.3â–¼22.66% 13.97 â–¼29.94% 5.78â–²12.89% 13.13 â–²8.07%
Table 3. Cross domain gaze estimation error in degrees. âˆ—indi-
cates methods with ResNet-50 backbone. Overall, the proposed
AGG achieves better generalization ability than SOTA gaze esti-
mation methods.
Method DEâ†’DMDEâ†’DDDGâ†’DMDGâ†’DD
Full-Face[38] 12.35 30.15 11.13 14.42
ADL[12] 7.23 8.02 11.36 11.86
CA-Net[6] - - 27.13 31.41
LatentGaze[18] 7.98 9.81 - -
PureGaze[8] 7.08* 7.48* 9.28 9.32
ResNet18+AGG 7.10 7.07 7.87 7.93
ResNet50+AGG 5.91* 6.75* 9.20* 11.36*
ization methods [8, 18]. Results demonstrate that the
AGG outperforms other SOTA methods. The AGG with
ResNet-18 baseline achieves the best overall performances,
it outperforms SOTA methods in DEâ†’DD,DGâ†’DMand
DGâ†’DDsettings, while achieving performance compara-
ble to the PureGaze in the DEâ†’DMsetting. The AGG with
ResNet-50 baseline also surpasses SOTA methods in 3 out
of 4 cross-domain settings. It performs exceptionally well
when trained in the DEdomain. Overall, above experi-
ments prove that the AGG achieves better generalization
ability than SOTA gaze estimation methods.
5.3. Verification of the AGG
5.3.1 Verification of the Core Idea
The proposed Analytical Gaze Generalization framework is
designed based on the observation that the geodesic distance
between image features is proportional to the angular gaze
differences between input samples. To explore whether this
observation holds true in different domains, we verify it in
DE,DG,DMandDDrespectively. We train a baseline
ResNet-18 model according to Eq. (1) in each domain re-
spectively to extract the image feature, and visualize the L2
and Geodesic distance with respect to the angular gaze dif-
ferences. As shown in Fig. 6, the linear relationship holds
true for all sample pairs in DE, thanks to the high image
quality and controlled laboratory environment. For DG,
the pattern is evident at the beginning but becomes random
Gaze Differences / Â°Gaze Differences / Â°Gaze Differences / Â°Gaze Differences / Â°Distance between Feature
ETH-XGaze Gaze360 MPIIFaceGaze EyeDiap
Figure 6. The L2 and Geodesic distances between image features
with respect to the angular differences between samples.
when gaze differences surpass 140â—¦. We randomly visu-
alize 4 samples from the random section at the top of the
figure. Since the original DGdataset includes subjects fac-
ing away from the camera, the quality of samples appears
to deteriorate when the head pose approaches Â±90â—¦. For
DMandDD, the geodesic distance is also exhibits a more
direct proportionality to gaze differences. However, the dis-
parity between geodesic distance and L2 distance is less ob-
vious compared to what was observed in the DEandDG. It
is reasonable since the gaze ranges in DMandDDare sig-
nificantly smaller. The geodesic distance converges toward
the L2 distance when the features are in close proximity.
We further visualize the Principle Gaze Feature from
these four datasets in Fig. 7 for a more intuitive understand-
ing. The Principle Gaze Feature from all 4 datasets con-
sistently demonstrate identical distribution pattern with the
gaze label. Above results confirm that the the proportional
relationship between the geodesic distance and the gaze dif-
ferences remains consistent across different domains, even
though the image quality, gaze range, and head pose range
exhibit significant variations among these domains. Hence,
the proportional relationship can be employed for domain
generalization, given that it is domain-independent.
5.3.2 Verification of the Sphere-Oriented Training
In this section, we assess the efficacy of the Sphere-Oriented
Training, i.e. whether Sphere-Oriented Training optimizes
the model to extract features that better conform to the pro-
1415
Top View
Side View
Top View
Side View
Top View
Side View
Top View
Side View
Top View
Side View
Top View
Side View
Dğ¸ğ¸ Dğºğº Dğ‘€ğ‘€PGF Gaze LabelDğ¸ğ¸ Dğºğº Dğ‘€ğ‘€
Top View
Side View
Top View
Side View
Dğ·ğ·
Dğ·ğ·Figure 7. Visualization of the Principle Gaze Feature (PGF) and
gaze label from DE,DG,DMandDD. PGF from all 4 datasets
share the same distribution pattern with gaze label.
4.45
2.606.577.73
2.21
1.67Sphere Error / %
Dğ¸ğ¸â†’Dğ·ğ· Dğ¸ğ¸â†’DM Within Dğ¸ğ¸4.46Sphere Error / %
Dğºğºâ†’DD Dğºğºâ†’DM Within Dğºğº3.394.14
3.3012.70
7.15
Figure 8. The Sphere Error of the PGF before (ResNet-18) and
after Sphere-Oriented Training (Ours). Smaller sphere error in-
dicates that the geodesic distance between extracted features are
more proportional to the gaze differences.
portional relationship. To do so, we measure the Sphere
Error, defined as the ratio of distance between eiand the
sphere surface to the radius of the sphere. The quantitative
results presented in Fig. 8 demonstrate that the Sphere Error
after the Sphere-Oriented Training reduces in both within-
domain and cross-domain settings. Fig. 9 provides a more
intuitive view. The Sphere Error is significantly reduced
in the central region after the Sphere-Oriented Training and
the distribution of the PGF becomes more spherical. Above
results validates the effectiveness of the propsoed Sphere-
Oriented Training.
6. Limitations and Discussions
Q1: Does the observation in Fig. 2 apply to features ex-
tracted by different gaze estimation models? We have
proven that the observation holds true for different model
architectures in Tabs. 1 and 2 and different datasets Fig. 7.
Here we further investigate the influence of two loss func-
tions: L1,L2loss, and two gaze representations: 3D unit
vector (x, y, z ), 2D Euler angle (yaw, pitch ). In Fig. 10,
we train ResNet-18 models under above conditions and
project the extracted features to 3D space using geodesic
distance. Results show that the AGG is robust to different
loss functions. When gaze is represented by 2D Euler an-
gles, the projected features no longer distributes across the
sphere surface, they approximately distribute on the surface
of a 2D plane, similar to the gaze label. The Sphere Align-
ment algorithm needs to be altered to adapt different gaze
representations, which we have left for future work.
Top View
Side View
PGF of Baseline
Top View
Side View
PGF of AGG
SmallLargeSphere Error
Figure 9. Visualization of the Sphere Error before and after
Sphere-Oriented Training in DE. In the side view, the bottom area
of the PGF of AGG is more spherical.
Top View
Side View
Top View
Side View
Top View
Side View
Top View
Side View
Top View
Side View
Top View
Side View
ğ‘¥ğ‘¥,ğ‘¦ğ‘¦,ğ‘§ğ‘§ (ğ‘¦ğ‘¦ğ‘¦ğ‘¦ğ‘¦ğ‘¦ ,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ )Gaze Label
 AGG -L1 Loss AGG -L2 Loss
Gaze Label AGG -L1 Loss AGG -L2 Loss
Figure 10. Projecting features extracted by ResNet-18 trained with
different strategy in DEinto 3D space by geodesic distance.
Q2: Does the Isometric Propagator (IP) suffer from
the same overfitting issue as the last FC layer since it is
implemented by MLP? Although we completely replace
the last FC layer with the GPM in the test time, the MLP
based IP is still used in source domain training. The use
of IP is an unavoidable compromise, because in the cur-
rent deep-learning community, there has not been a perfect
solution for integrating Isomap into the back-propagation
process. But the IP differs from the original regression FC
layer, since IP is only used in the training time, as a tool
for back-propagation with fixed parameters. Still, the im-
plementation of IP is a limitation to our method. The per-
formance of the AGG might be further improved if there is
a better way to integrate Isomap into the training process.
7. Conclusion
In this paper we propose the Analytical Gaze Generaliza-
tion framework for generalizing gaze estimation models to
unseen domains. Based on the observation that the geodesic
distance between extracted image features is proportional to
the angular gaze differences, we propose the Geodesic Pro-
jection Module that estimates gaze from the image feature
analytically and incorporate it into the source domain train-
ing by the proposed Sphere-Oriented Training. Extensive
experiments show that the GPM achieves better generaliza-
tion ability than the conventional FC layer, and the AGG im-
proves the cross-domain accuracy significantly, outperform-
ing SOTA methods. The concept of the AGG may inspire
method designs in other physical regression tasks, e.g. pose
estimation.
1416
References
[1] Yiwei Bao, Yihua Cheng, Yunfei Liu, and Feng Lu. Adaptive
feature fusion network for gaze tracking in mobile tablets. In
2020 25th International Conference on Pattern Recognition
(ICPR) , pages 9936â€“9943. IEEE, 2021. 2
[2] Yiwei Bao, Yunfei Liu, Haofei Wang, and Feng Lu. Gen-
eralizing gaze estimation with rotation consistency. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 4207â€“4216, 2022. 2, 6
[3] Alisa Burova, John M Â¨akelÂ¨a, Jaakko Hakulinen, Tuuli Keski-
nen, Hanna Heinonen, Sanni Siltanen, and Markku Turunen.
Utilizing vr and gaze tracking to develop ar solutions for in-
dustrial maintenance. In Proceedings of the 2020 CHI Con-
ference on Human Factors in Computing Systems , pages 1â€“
13, 2020. 1
[4] Nora Castner, Thomas C Kuebler, Katharina Scheiter, Ju-
liane Richter, Th Â´erÂ´ese Eder, Fabian H Â¨uttig, Constanze Keu-
tel, and Enkelejda Kasneci. Deep semantic gaze embedding
and scanpath comparison for expertise classification during
opt viewing. In ACM Symposium on Eye Tracking Research
and Applications , pages 1â€“10, 2020. 1
[5] Zhaokang Chen and Bertram E Shi. Appearance-based gaze
estimation using dilated-convolutions. In Asian Conference
on Computer Vision , pages 309â€“324. Springer, 2018. 2
[6] Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and
Feng Lu. A coarse-to-fine adaptive network for appearance-
based gaze estimation. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , pages 10623â€“10630, 2020. 2,
6, 7
[7] Yihua Cheng, Xucong Zhang, Feng Lu, and Yoichi Sato.
Gaze estimation by exploring two-eye asymmetry. IEEE
Transactions on Image Processing , 29:5259â€“5272, 2020. 2
[8] Yihua Cheng, Yiwei Bao, and Feng Lu. Puregaze: Purifying
gaze feature for generalizable gaze estimation. In Proceed-
ings of the AAAI Conference on Artificial Intelligence , pages
436â€“443, 2022. 1, 2, 3, 6, 7
[9] Kenneth Alberto Funes Mora, Florent Monay, and Jean-
Marc Odobez. Eyediap: A database for the development and
evaluation of gaze estimation algorithms from rgb and rgb-d
cameras. In Proceedings of the Symposium on Eye Tracking
Research and Applications , pages 255â€“258, 2014. 2, 6
[10] Elias Daniel Guestrin and Moshe Eizenman. General theory
of remote gaze estimation using the pupil center and corneal
reflections. IEEE Transactions on biomedical engineering ,
53(6):1124â€“1133, 2006. 2
[11] Dan Witzner Hansen and Qiang Ji. In the eye of the beholder:
A survey of models for eyes and gaze. IEEE transactions on
pattern analysis and machine intelligence , 32(3):478â€“500,
2009. 2
[12] Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-
tusik, and Antonio Torralba. Gaze360: Physically uncon-
strained gaze estimation in the wild. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 6912â€“6921, 2019. 1, 2, 6, 7
[13] Jess Kerr-Gaffney, Amy Harrison, and Kate Tchanturia. Eye-
tracking research in eating disorders: A systematic review.International Journal of Eating Disorders , 52(1):3â€“27, 2019.
1
[14] Andrew J King, Gregory F Cooper, Gilles Clermont, Harry
Hochheiser, Milos Hauskrecht, Dean F Sittig, and Shyam
Visweswaran. Leveraging eye tracking to prioritize relevant
medical record data: comparative machine learning study.
Journal of medical Internet research , 22(4):e15876, 2020. 1
[15] Robert Konrad, Anastasios Angelopoulos, and Gordon Wet-
zstein. Gaze-contingent ocular parallax rendering for virtual
reality. ACM Transactions on Graphics (TOG) , 39(2):1â€“12,
2020. 1
[16] Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kan-
nan, Suchendra Bhandarkar, Wojciech Matusik, and Anto-
nio Torralba. Eye tracking for everyone. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 2176â€“2184, 2016. 2
[17] Mikko Kyt Â¨o, Barrett Ens, Thammathip Piumsomboon,
Gun A Lee, and Mark Billinghurst. Pinpointing: Precise
head-and eye-based target selection for augmented reality. In
Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems , pages 1â€“14, 2018. 1
[18] Isack Lee, Jun-Seok Yun, Hee Hyeon Kim, Youngju Na, and
Seok Bong Yoo. Latentgaze: Cross-domain gaze estimation
through gaze-aware analytic latent code manipulation. In
Proceedings of the Asian Conference on Computer Vision ,
pages 3379â€“3395, 2022. 7
[19] Jiahui Liu, Jiannan Chi, and Shuo Fan. A method for ac-
curate 3d gaze estimation with a single camera and two
collinear light sources. IEEE Transactions on Instrumenta-
tion and Measurement , 2022. 2
[20] Yunfei Liu, Ruicong Liu, Haofei Wang, and Feng Lu. Gen-
eralizing gaze estimation with outlier-guided collaborative
adaptation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 3835â€“3844, 2021. 1,
2, 6
[21] Feng Lu, Yusuke Sugano, Takahiro Okabe, and Yoichi Sato.
Adaptive linear regression for appearance-based gaze esti-
mation. IEEE transactions on pattern analysis and machine
intelligence , 36(10):2033â€“2046, 2014. 2
[22] Feng Lu, Xiaowu Chen, and Yoichi Sato. Appearance-based
gaze estimation via uncalibrated gaze pattern recovery. IEEE
Transactions on Image Processing , 26(4):1543â€“1553, 2017.
2
[23] Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar
Iqbal, Otmar Hilliges, and Jan Kautz. Few-shot adaptive
gaze estimation. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 9368â€“9377,
2019. 2
[24] Sam T Roweis and Lawrence K Saul. Nonlinear dimension-
ality reduction by locally linear embedding. science , 290
(5500):2323â€“2326, 2000. 4
[25] Timo Schneider, Boris Schauerte, and Rainer Stiefelhagen.
Manifold alignment for person independent appearance-
based gaze estimation. In 2014 22nd international confer-
ence on pattern recognition , pages 1167â€“1172. IEEE, 2014.
2
1417
[26] Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh
Agrawala, Diego Gutierrez, Belen Masia, and Gordon Wet-
zstein. Saliency in vr: How do people explore virtual envi-
ronments? IEEE transactions on visualization and computer
graphics , 24(4):1633â€“1642, 2018. 1
[27] Sophie Stellmach, Sebastian Stober, Andreas N Â¨urnberger,
and Raimund Dachselt. Designing gaze-supported multi-
modal interactions for the exploration of large image collec-
tions. In Proceedings of the 1st Conference on Novel Gaze-
Controlled Applications , New York, NY , USA, 2011. Asso-
ciation for Computing Machinery. 1
[28] Li Sun, Zicheng Liu, and Ming-Ting Sun. Real time gaze
estimation with a consumer depth camera. Information Sci-
ences , 320:346â€“360, 2015. 2
[29] Kentaro Takemura and Kenta Yamagishi. A hybrid eye-
tracking method using a multispectral camera. In 2017 IEEE
International Conference on Systems, Man, and Cybernetics
(SMC) , pages 1529â€“1534. IEEE, 2017. 2
[30] Joshua B Tenenbaum, Vin de Silva, and John C Langford.
A global geometric framework for nonlinear dimensionality
reduction. science , 290(5500):2319â€“2323, 2000. 4
[31] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of machine learning research , 9
(11), 2008. 4
[32] Kang Wang, Rui Zhao, Hui Su, and Qiang Ji. Generalizing
eye tracking with bayesian adversarial learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11907â€“11916, 2019. 1
[33] Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wen-
rui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Con-
trastive regression for domain adaptation on gaze estimation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 19376â€“19385, 2022.
1, 2, 6
[34] Zhimin Wang, Huangyue Yu, Haofei Wang, Zongji Wang,
and Feng Lu. Comparing single-modal and multimodal in-
teraction in an augmented reality system. In 2020 IEEE In-
ternational Symposium on Mixed and Augmented Reality Ad-
junct, ISMAR 2020 Adjunct, Recife, Brazil, November 9-13,
2020 , pages 165â€“166. IEEE, 2020. 1
[35] Xuehan Xiong, Zicheng Liu, Qin Cai, and Zhengyou Zhang.
Eye gaze tracking using an rgbd camera: A comparison with
a rgb solution. In Proceedings of the 2014 ACM Interna-
tional Joint Conference on Pervasive and Ubiquitous Com-
puting: Adjunct Publication , pages 1113â€“1121, 2014. 2
[36] Yu Yu and Jean-Marc Odobez. Unsupervised representa-
tion learning for gaze estimation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7314â€“7324, 2020. 2
[37] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas
Bulling. Appearance-based gaze estimation in the wild. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 4511â€“4520, 2015. 2
[38] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas
Bulling. Itâ€™s written all over your face: Full-face appearance-
based gaze estimation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops , pages 51â€“60, 2017. 2, 6, 7[39] Xucong Zhang, Seonwook Park, Thabo Beeler, Derek
Bradley, Siyu Tang, and Otmar Hilliges. Eth-xgaze: A large
scale dataset for gaze estimation under extreme head pose
and gaze variation. In European Conference on Computer
Vision , pages 365â€“381. Springer, 2020. 2, 3, 6
1418
