Learning Transferable Negative Prompts for Out-of-Distribution Detection
Tianqi Li1, Guansong Pang*2, Xiao Bai*1, Wenjun Miao1, and Jin Zheng1
1School of Computer Science and Engineering, State Key Laboratory of Complex & Critical Software
Environment, Jiangxi Research Institute, Beihang University, China
2School of Computing and Information Systems, Singapore Management University
Abstract
Existing prompt learning methods have shown certain
capabilities in Out-of-Distribution (OOD) detection, but the
lack of OOD images in the target dataset in their train-
ing can lead to mismatches between OOD images and In-
Distribution (ID) categories, resulting in a high false posi-
tive rate. To address this issue, we introduce a novel OOD
detection method, named ‚ÄòNegPrompt‚Äô, to learn a set of
negative prompts, each representing a negative connotation
of a given class label, for delineating the boundaries be-
tween ID and OOD images. It learns such negative prompts
with ID data only, without any reliance on external out-
lier data. Further, current methods assume the availabil-
ity of samples of all ID classes, rendering them ineffective
in open-vocabulary learning scenarios where the inference
stage can contain novel ID classes not present during train-
ing. In contrast, our learned negative prompts are trans-
ferable to novel class labels. Experiments on various Im-
ageNet benchmarks show that NegPrompt surpasses state-
of-the-art prompt-learning-based OOD detection methods
and maintains a consistent lead in hard OOD detection in
closed- and open-vocabulary classification scenarios. Code
is available at https://github.com/mala-lab/negprompt.
1. Introduction
Since the advent of deep learning, numerous image recog-
nition models [5, 9, 29] have relied solely on image features
for classification. However, in recent years large pre-trained
vision-language models (VLMs), such as CLIP [37], inte-
grated natural language processing into computer vision,
enhancing the semantic understanding capabilities of com-
puter vision models. It has been observed that these VLMs
excel in image classification, particularly in zero-shot sce-
narios. This is attributed to their extensive self-supervised
pre-training on web-scale image-text data, which has en-
dowed them with robust semantic transfer abilities.
*Corresponding author: G. Pang (gspang@smu.edu.sg) and X. Bai
(baixiao@buaa.edu.cn)
elephantbirdbeehorsePrompt Embedding Space
dogcatcat
cat
cat
dog
dog
dog
<latexit sha1_base64="1mgy36mbSmkRdxUbNESh1s8PNiI=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBoqVSEyrHdahVx47sG0QV+a+x8A/Y2FkYQIiVBactUChXsvX5nHtl+0QpZxo879GZmJyanpmdmy8tLC4tr5RX1+paZorQGpFcqkaENeVM0Bow4LSRKoqTiNPLqHtS+Jc3VGkmxQX0Uhom+FqwmBEMVmqVGwHQW4jivGmCBEPH0k3LNzs/h10TkLaEke3bEuZrOjRX+RenUhtTapUrXtXrlzsO/hAqaFhnrfJD0JYkS6gAwrHWTd9LIcyxAkY4NaUg0zTFpIuvadOiwAnVYd5PwLhbVmm7sVR2CXD76uhEjhOte0lkO4vX679eIf7nNTOID8OciTQDKsjgojjjLki3iNNtM0UJ8J4FTBSzb3VJBytMwIZehOD//fI41Her/n7VP9+rHB0P45hDG2gTbSMfHaAjdIrOUA0RdIee0At6de6dZ+fNeR+0TjjDmXX0q5yPT+FMtA4=</latexit>[v1,v2¬∑¬∑¬∑vn]pos
<latexit sha1_base64="1mgy36mbSmkRdxUbNESh1s8PNiI=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBoqVSEyrHdahVx47sG0QV+a+x8A/Y2FkYQIiVBactUChXsvX5nHtl+0QpZxo879GZmJyanpmdmy8tLC4tr5RX1+paZorQGpFcqkaENeVM0Bow4LSRKoqTiNPLqHtS+Jc3VGkmxQX0Uhom+FqwmBEMVmqVGwHQW4jivGmCBEPH0k3LNzs/h10TkLaEke3bEuZrOjRX+RenUhtTapUrXtXrlzsO/hAqaFhnrfJD0JYkS6gAwrHWTd9LIcyxAkY4NaUg0zTFpIuvadOiwAnVYd5PwLhbVmm7sVR2CXD76uhEjhOte0lkO4vX679eIf7nNTOID8OciTQDKsjgojjjLki3iNNtM0UJ8J4FTBSzb3VJBytMwIZehOD//fI41Her/n7VP9+rHB0P45hDG2gTbSMfHaAjdIrOUA0RdIee0At6de6dZ+fNeR+0TjjDmXX0q5yPT+FMtA4=</latexit>[v1,v2¬∑¬∑¬∑vn]pos
<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2¬∑¬∑¬∑vn]neg<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2¬∑¬∑¬∑vn]neg
<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2¬∑¬∑¬∑vn]neg<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2¬∑¬∑¬∑vn]neg
<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2¬∑¬∑¬∑vn]neg<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2¬∑¬∑¬∑vn]negFigure 1. Illustration of the key intuition of NegPrompt. For each
ID class, NegPrompt trains a small set of learnable prompts that
have negative semantics to the learned positive prompt of the given
class. As a result, OOD samples exhibit higher similarity to the
negative prompts than the positive prompts.
Despite the strong zero-shot classification capabilities of
VLMs, numerous research efforts are put to unleash their
potential, e.g., by investigating whether models like CLIP
can achieve enhanced performance with training on down-
stream target datasets. This has led to the development of
various techniques to fine-tune CLIP [8, 18], among which
prompt learning has sparked widespread interest. Prompt
learning (or prompt tuning) [19, 39, 50], a methodology
originating from natural language processing, focuses on
learning the prompt inputs into a large-scale pre-trained net-
work, rather than learning or fine-tuning the parameters of
the network. In CLIP, a common prompt is ‚Äò a photo of
a [class name] ‚Äô. The aim of prompt learning, e.g., in
approaches like CoOp [54], is to learn a soft/differentiable
context vector to replace the fixed text prompt like ‚Äò a
photo of a ‚Äô, thereby leveraging CLIP‚Äôs powerful gen-
eralization in semantic understanding while also fine-tuning
for specific target datasets [53, 54].
However, even though prompt learning enhances the tar-
get dataset perception capabilities of VLMs, it struggles
with Out-Of-Distribution (OOD) detection [10, 26]. Un-
der the OOD detection task, the test set comprises images
from both the training classes ‚Äì in-distribution (ID) data ‚Äì
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17584
and images from other unknown categories (OOD images).
VLM-based classification is typically performed by first re-
placing the ‚Äò [class name] ‚Äô with the class label of each
category in a prompt, which is then processed by the text
encoder of CLIP to obtain the class embedding and assign
the class label that has the highest cosine similarity with the
embedding of a test image from the image encoder of CLIP.
However in OOD detection tasks, the models do not have
access to the class names of OOD images, thereby lacking
the knowledge about OOD data.
This issue, exacerbated by the models‚Äô tendency towards
overconfidence that often predicts OOD data as ID images
with high confidence [34], undermines their ability to effec-
tively detect OOD images.
There have been a number of CLIP-based methods de-
signed specifically for VLM-driven OOD detection meth-
ods, but most of them [6, 32, 46] focused on a zero-shot set-
ting where no training data of the target dataset is available.
Due to the lack of adaptation to the target dataset, they tend
to detect unusual ID images as OOD data. Among them,
CLIPN [46], which trains an additional ‚Äòno‚Äô text encoder to
provide prompt embeddings of not having specific classes,
is the best detector, but it relies on a large-scale auxiliary
dataset to train such a text encoder and it is computationally
expensive.
The most related work is a very recent approach Lo-
CoOp [33] that utilizes the training ID data to tune CLIP
to capture local features of the ID classes in the prompt.
It shows substantially improved performance compared to
zero-shot methods [32], but it may compromise the ID
classification accuracy due to an overemphasis on model-
ing local features. LoCoOp also lacks knowledge about
OOD samples, making it difficult to differentiate boundary
ID/OOD images.
In this work, we propose a novel CLIP-based OOD de-
tection method, named NegPrompt . Inspired by [25], Neg-
prompt is designed to learn a set of negative prompts , each
representing a negative connotation of a given ID class la-
bel, to delineate the boundaries between ID and OOD im-
ages, as shown in Fig. 1. The negative prompts represent
specialized ID-class-dependent concepts, guiding the mod-
els to pay attention to the characteristics that are contrary to
or disjoint from the ID classes. NegPrompt aims to utilize
ID training data and positive prompts (i.e., the text prompt
embeddings of ID classes) to learn such negative prompts in
a way to which OOD images exhibits higher similarity than
ID images.
Essentially, the learned negative prompts have similar se-
mantics to the text prompts generated from the ‚Äòno‚Äô text en-
coder in CLIPN, but NegPrompt presents a fundamentally
different approach: it capitalizes on the generalization abil-
ity of the CLIP model and learns the negative prompts with
training ID data only, eliminating the reliance on externaldata and the extensive computation overhead as in CLIPN.
Furthermore, benefiting from the superior generalization
ability of CLIP, our method does not require the exposure
to all ID classes during training. In other words, the model
learns transferable negative prompts by using only a small
subset of the ID classes, after which we can obtain the nega-
tive prompts for the other ID classes by simply replacing the
‚Äò[class name] ‚Äô in the prompts with the name of those
unexposed classes. This allows our model to work in open-
vocabulary [7, 31] learning settings, where the models are
required to classify images of novel classes that are not seen
during training, in addition to a set of training base classes.
Our main contributions can be summarized as follows:
‚Ä¢ We propose a prompt learning-based OOD detection ap-
proach NegPrompt, which is able to learn negative se-
mantics relative to specific ID classes, thereby enhanc-
ing the VLMs‚Äô sensitivity to unknown samples. It is a
lightweight method that does not require training extra
encoders on external data as in related methods [46].
‚Ä¢ NegPrompt possesses an open-vocabulary capability due
to the transferability of its negative prompts. This means
that with training images from just a small subset of ID
classes and the class names of all IDs, we can achieve
OOD detection on test data with all these ID classes. To
the best of our knowledge, there have been no previous
fine-tuning methods exploring such a capability.
‚Ä¢ Extensive experiments on multiple ImageNet-based
benchmarks show that NegPrompt consistently outper-
forms current state-of-the-art methods in both conven-
tional and hard OOD Detection settings.
2. Related Work
2.1. Pre-trained Vision-Language Models
Understanding the semantic information of images remains
a significant challenge in the field of computer vision. With
the advent of Transformer [44] in computer vision tasks [5],
CLIP [37] has been introduced as one of the most advanced
pre-trained VLMs. Utilizing contrastive learning [21],
large-scale models and datasets [38], CLIP employs image-
text pairs as the training data for self-supervised learning.
This approach has successfully trained the model to align
visual and text signals in a latent space. Concurrently, other
researchers [1, 24, 49] have also shown the remarkable gen-
eralization capabilities of CLIP and similar vision-language
models [17] in various downstream tasks [7, 20, 56].
2.2. Prompt Learning
The concept of prompt learning was initially focused on au-
tomating the creation of templates/prompts for extracting
knowledge from Bert [4] or GPT [36]. To bypass man-
ual creation, prompt learning advocates the use of super-
vised learning to automate the prompt development. [39]
proposed a gradient-based approach for identifying the
17585
best prompt, establishing the foundation of prompt learn-
ing. Later, CoOp [54] integrated prompt learning into
computer vision. CoOp learns a segment of the context
before it is fed into the text encoder of CLIP, thus tai-
loring the learned prompt to the specific target dataset.
Many other prompt learning methods for different vision
tasks [13, 18, 20, 40, 47, 53, 55] are subsequently intro-
duced. However, they are not designed for OOD detection,
so they struggle with dealing with the unknown OOD sam-
ples during inference.
2.3. Out-of-Distribution Detection
OOD detection is committed to identifying images in im-
age classification tasks that belong to categories not present
in the training dataset, typically originating from a differ-
ent distribution. While traditional OOD detection meth-
ods often tackle the problem by either exploiting the pre-
diction logits to define OOD scores [10, 12, 15, 26, 27] or
focusing on the class-agnostic information in feature space
that is not recoverable from logits [41, 45], recent meth-
ods [11, 14, 22, 28, 30, 42, 52] introduce extra or synthetic
OOD data, employing fine-tuning to elevate their model‚Äôs
sensitivity towards unknown classes.
With the introduction of large pre-trained VLMs, OOD
detection has embarked on a new trajectory driven by
VLMs. MCM [32] aims to integrate the idea of maxi-
mum softmax probability [10] into the inference process
of CLIP, while ZOC [6] enhances OOD detection in a
zero-shot setting by learning an additional image inter-
preter and guessing the category of images. CLIPN [46]
and LoCoOp [33], the most related methods to ours, are
based on text prompts. However, CLIPN, during its pre-
training phase, trains an additional negative text encoder us-
ing a large external dataset to improve its negative seman-
tic prompt, which increases network parameters and devi-
ates from prompt learning that is focused on tuning the tar-
get data. LoCoOp, on the other hand, uses prompt learn-
ing for matching text and image local features, which can
compromise the global perception capability of CLIP and
reduce classification accuracy for in-distribution (ID) sam-
ples. Also, its model lacks knowledge about OOD samples,
which can often lead to high detection errors.
3. Method
In this paper, we propose an approach named NegPropmt
that leverages pre-trained VLMs, specifically CLIP, to learn
negative prompts relative to ID classes for the purpose of
OOD detection. The negative prompts are learned in the
CLIP‚Äôs text-image-aligned embedding space with the sup-
port of training ID data and their positive prompts ( i.e.,
prompt embeddings of ID classes); no external outlier data
is required. Due to its general effectiveness, the popular
prompt learning method CoOp [54] is used by default to
provide the positive prompts for training NegPrompt.3.1. Preliminaries
Problem Statement. Formally, we assume that we have
two datasets, namely ID dataset denoted as Dinand OOD
dataset denoted as Dout. The ID dataset consists of image-
label pairs (xin, yin), where yin‚ààYin={0,1,2,3...k}
belong to the ID class set. Similarly, the OOD dataset con-
tains image-label pairs (xout, yout), but all yout‚ààYout=
{k+ 1, k+ 2, ...}belong to the OOD class set. It is im-
portant to note that these two sets do not intersect, meaning
thatYin‚à©Yout=‚àÖ. As we have a test set Xtestconsists
of images from ID and OOD, the goal of OOD detection
is to train a classifier œï(x)that takes an image xas input
and returns whether the image belongs to OOD. Unlike ex-
isting studies using full-/zero-shot ID training samples, we
only use a few samples (16 samples per class) for Din
train
like [33] does. In the open-vocabulary detection setting, we
only use a small part (10%) but not all the classes of Din
train .
CLIP and CoOp. CLIP [37] is currently one of the most
popular image-text models. During the pre-training phase,
it uses large-scale image-text pairs for self-supervised
contrastive learning, aligning images and texts into the
same latent space. The main components of CLIP are
an image encoder Encoderimage(I)and a text encoder
Encodertext(T), which respectively accept image and text
inputs. In zero-shot image classification tasks, assume
we have kclass labels for classification, such as ‚Äúcat‚Äù,
‚Äúdog‚Äù, etc., CLIP first incorporates the class labels into
pre-designed hard/unlearnable text prompts, such as ‚Äú a
photo of a [class name] ‚Äù, forming a prompt in-
put set of ‚Äú a photo of a cat ‚Äù, ‚Äúa photo of a
dog‚Äù and so on. These prompts are then individually fed
into the text encoder Encodertext(T)to obtain ktext fea-
tures Tf. The testing image is then input into the image
encoder to obtain an image feature If. The cosine similar-
ity is calculated between the normalized image feature and
all text features, formally, Sim(Tf, If) =Tf¬∑If, and the
text feature Tfwith the highest similarity to Ifis consid-
ered to be the category to which the image belongs.
Besides zero-shot classification, many have explored
ways to improve CLIP‚Äôs performance when the target data
is accessible. CoOp [54] introduces prompt learning with
CLIP by freezing its encoders and using backpropagation
to learn dataset-specific soft/learnable prompts. The text
prompts are represented as ti={œâpos
1, œâpos
2, ..., œâpos
n, ci},
where ciis the word embedding of the class name and œâposs
are learnable vectors that have positive semantics w.r.t. the
class i. The goal is to optimize these œâpositives. Specifi-
cally, tiis processed by Encodertextto yield Tf,pos
i as a
positive prompt embedding of the class i, and then the pre-
diction probability is computed as:
p(y=i|x) =exp(sim(Tf,pos
i, If)/œÑ)
Pk
j=1exp(sim(Tf,pos
j, If)/œÑ),(1)
17586
Text
Encoder[Class Label]dogcar<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>¬∑¬∑¬∑busTokenizer<latexit sha1_base64="l9yZhi4EFkgrrt1ORlYy+R3Qlhw=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSIIQklE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+Oyura+sbm4Wt4vbO7t5+6eCwqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR3dRvPaHSPJYPZpygH9GB5CFn1Fipft4rld2KOwNZJl5OypCj1it9dfsxSyOUhgmqdcdzE+NnVBnOBE6K3VRjQtmIDrBjqaQRaj+bHTohp1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1IQ3fsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2RRtCN7iy8ukeVHxripe/bJcvc3jKMAxnMAZeHANVbiHGjSAAcIzvMKb8+i8OO/Ox7x1xclnjuAPnM8fc32MtQ==</latexit>+
<latexit sha1_base64="98GFsDGNARcto84kAzgcKolUg7s=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi94q2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbqZ+64lrI2L1iOOE+xEdKBEKRtFKD3e9sFeuuFV3BrJMvJxUIEe9V/7q9mOWRlwhk9SYjucm6GdUo2CST0rd1PCEshEd8I6likbc+Nns1Ak5sUqfhLG2pZDM1N8TGY2MGUeB7YwoDs2iNxX/8zophld+JlSSIldsvihMJcGYTP8mfaE5Qzm2hDIt7K2EDammDG06JRuCt/jyMmmeVb2Lqnd/Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OdF6cd+dj3lpw8plD+APn8wcWYo2s</latexit>If<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>¬∑¬∑¬∑
image
featureslearned positive prompt

Enforce positive and negative
text features not too far<latexit sha1_base64="os1LA9GpKIWQzKWa3ZUY5OmdQUI=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUlE1GVRFy5EKtgHtCFMptN26GQSZiZiCfkVNy4UceuPuPNvnLRZaPXAwOGce7lnThBzprTjfFmlpeWV1bXyemVjc2t7x96ttlWUSEJbJOKR7AZYUc4EbWmmOe3GkuIw4LQTTC5zv/NApWKRuNfTmHohHgk2ZARrI/l2FfVDrMcE8/Qm89Pb5lXm2zWn7syA/hK3IDUo0PTtz/4gIklIhSYcK9VznVh7KZaaEU6zSj9RNMZkgke0Z6jAIVVeOsueoUOjDNAwkuYJjWbqz40Uh0pNw8BM5kHVopeL/3m9RA/PvZSJONFUkPmhYcKRjlBeBBowSYnmU0MwkcxkRWSMJSba1FUxJbiLX/5L2sd197Tu3p3UGhdFHWXYhwM4AhfOoAHX0IQWEHiEJ3iBVyuznq03630+WrKKnT34BevjG6/BlDg=</latexit>LNPD
Distribute the probability
distribution evenly<latexit sha1_base64="OHOj4AtWE0KT+4t9maEsWkaj27s=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUlE1GXRjYJIRfuANoTJdNIOnTyYmYgl5FfcuFDErT/izr9x0mahrQcGDufcyz1zvJgzqSzr2ygtLa+srpXXKxubW9s75m61LaNEENoiEY9E18OSchbSlmKK024sKA48Tjve+DL3O49USBaFD2oSUyfAw5D5jGClJdeson6A1Yhgnt5kbnp7fZ+5Zs2qW1OgRWIXpAYFmq751R9EJAloqAjHUvZsK1ZOioVihNOs0k8kjTEZ4yHtaRrigEonnWbP0KFWBsiPhH6hQlP190aKAykngacn86By3svF/7xeovxzJ2VhnCgaktkhP+FIRSgvAg2YoETxiSaYCKazIjLCAhOl66roEuz5Ly+S9nHdPq3bdye1xkVRRxn24QCOwIYzaMAVNKEFBJ7gGV7hzciMF+Pd+JiNloxiZw/+wPj8AbvilEA=</latexit>LNIS
‚ùÑ Frozen Parameters
üî• Learnable Parameters
<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>¬∑¬∑¬∑
üî•<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>¬∑¬∑¬∑
<latexit sha1_base64="S9thO0d7WpslZZKs2eluMwHd3Yk=">AAAB9XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyCbOT2WTIPJaZWSUs+Q8vHhTx6r9482+cJHvQxIKGoqqb7q4o4cxY3//2VlbX1jc2C1vF7Z3dvf3SwWHDqFQTWieKK92KsKGcSVq3zHLaSjTFIuK0GY1up37zkWrDlHyw44SGAg8kixnB1kndjhJ0gHtBN0uUmfRKZb/iz4CWSZCTMuSo9Upfnb4iqaDSEo6NaQd+YsMMa8sIp5NiJzU0wWSEB7TtqMSCmjCbXT1Bp07po1hpV9Kimfp7IsPCmLGIXKfAdmgWvan4n9dObXwdZkwmqaWSzBfFKUdWoWkEqM80JZaPHcFEM3crIkOsMbEuqKILIVh8eZk0zivBZSW4vyhXb/I4CnAMJ3AGAVxBFe6gBnUgoOEZXuHNe/JevHfvY9664uUzR/AH3ucPuFWSqQ==</latexit>!pos1<latexit sha1_base64="dDvjCW8HYg8XZ3hJRhyid2vpWHw=">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKewGUY9BLx4jmAckmzA7mSRD5rHMzCphyX948aCIV//Fm3/jJNmDJhY0FFXddHdFMWfG+v63l1tb39jcym8Xdnb39g+Kh0cNoxJNaJ0ornQrwoZyJmndMstpK9YUi4jTZjS+nfnNR6oNU/LBTmIaCjyUbMAItk7qdpSgQ9yrdNNYmWmvWPLL/hxolQQZKUGGWq/41ekrkggqLeHYmHbgxzZMsbaMcDotdBJDY0zGeEjbjkosqAnT+dVTdOaUPhoo7UpaNFd/T6RYGDMRkesU2I7MsjcT//PaiR1chymTcWKpJItFg4Qjq9AsAtRnmhLLJ45gopm7FZER1phYF1TBhRAsv7xKGpVycFkO7i9K1ZssjjycwCmcQwBXUIU7qEEdCGh4hld48568F+/d+1i05rxs5hj+wPv8Abnfkqo=</latexit>!pos2<latexit sha1_base64="Y13Ng0C5G5XLuKm3sVrm6UzW8uo=">AAAB9XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyCbOT2WTIPJaZWSUs+Q8vHhTx6r9482+cJHvQxIKGoqqb7q4o4cxY3//2VlbX1jc2C1vF7Z3dvf3SwWHDqFQTWieKK92KsKGcSVq3zHLaSjTFIuK0GY1up37zkWrDlHyw44SGAg8kixnB1kndjhJ0gHuymyXKTHqlsl/xZ0DLJMhJGXLUeqWvTl+RVFBpCcfGtAM/sWGGtWWE00mxkxqaYDLCA9p2VGJBTZjNrp6gU6f0Uay0K2nRTP09kWFhzFhErlNgOzSL3lT8z2unNr4OMyaT1FJJ5ovilCOr0DQC1GeaEsvHjmCimbsVkSHWmFgXVNGFECy+vEwa55XgshLcX5SrN3kcBTiGEziDAK6gCndQgzoQ0PAMr/DmPXkv3rv3MW9d8fKZI/gD7/MHFkaS5g==</latexit>!posn
‚ùÑ
Image
Encoder
‚ùÑ
‚ùÑ<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>¬∑¬∑¬∑
‚ùÑ
üî•
<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>¬∑¬∑¬∑
<latexit sha1_base64="X8WVPY0U3GQk+wDeysSOXdw59Fc=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16CRbBg5RERD0WvXisYD+gjWGznbRLN7thdyOWkL/ixYMiXv0j3vw3btsctPXBwOO9GWbmhQmjSrvut1VaWV1b3yhvVra2d3b37P1qW4lUEmgRwYTshlgBoxxammoG3UQCjkMGnXB8M/U7jyAVFfxeTxLwYzzkNKIEayMFdrUvYhjih4zDMA8y79TLA7vm1t0ZnGXiFaSGCjQD+6s/ECSNgWvCsFI9z020n2GpKWGQV/qpggSTMR5Cz1COY1B+Nrs9d46NMnAiIU1x7czU3xMZjpWaxKHpjLEeqUVvKv7n9VIdXfkZ5UmqgZP5oihljhbONAhnQCUQzSaGYCKpudUhIywx0SauignBW3x5mbTP6t5F3bs7rzWuizjK6BAdoRPkoUvUQLeoiVqIoCf0jF7Rm5VbL9a79TFvLVnFzAH6A+vzB77+lD8=</latexit>!neg1,1<latexit sha1_base64="xb5lE+MSr79TVSD4bs90JnwXGgk=">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwICUpoh6LXjxWsB/QxrDZTtqlm03Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5QcKZ0o7zba2srq1vbJa2yts7u3v79kGlreJUUmjRmMeyGxAFnAloaaY5dBMJJAo4dILxzdTvPIJULBb3epKAF5GhYCGjRBvJtyv9OIIhecgEDHM/c8/quW9XnZozA14mbkGqqEDTt7/6g5imEQhNOVGq5zqJ9jIiNaMc8nI/VZAQOiZD6BkqSATKy2a35/jEKAMcxtKU0Him/p7ISKTUJApMZ0T0SC16U/E/r5fq8MrLmEhSDYLOF4UpxzrG0yDwgEmgmk8MIVQycyumIyIJ1SausgnBXXx5mbTrNfei5t6dVxvXRRwldISO0Sly0SVqoFvURC1E0RN6Rq/ozcqtF+vd+pi3rljFzCH6A+vzB8CDlEA=</latexit>!neg1,2<latexit sha1_base64="B8mfKLODnnqvOd24K9+UBBPLyLw=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16CRbBg5RERD0WvXisYD+gjWGznbRLN7thdyOWkL/ixYMiXv0j3vw3btsctPXBwOO9GWbmhQmjSrvut1VaWV1b3yhvVra2d3b37P1qW4lUEmgRwYTshlgBoxxammoG3UQCjkMGnXB8M/U7jyAVFfxeTxLwYzzkNKIEayMFdrUvYhjih4zDMA8y75TngV1z6+4MzjLxClJDBZqB/dUfCJLGwDVhWKme5ybaz7DUlDDIK/1UQYLJGA+hZyjHMSg/m92eO8dGGTiRkKa4dmbq74kMx0pN4tB0xliP1KI3Ff/zeqmOrvyM8iTVwMl8UZQyRwtnGoQzoBKIZhNDMJHU3OqQEZaYaBNXxYTgLb68TNpnde+i7t2d1xrXRRxldIiO0Any0CVqoFvURC1E0BN6Rq/ozcqtF+vd+pi3lqxi5gD9gfX5Axu+lHw=</latexit>!neg1,n
<latexit sha1_base64="cYoVl5MAHDFkxuzHA3zwhWWbNzY=">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwICUpoh6LXjxWsB/QxrDZTtqlm03Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5QcKZ0o7zba2srq1vbJa2yts7u3v79kGlreJUUmjRmMeyGxAFnAloaaY5dBMJJAo4dILxzdTvPIJULBb3epKAF5GhYCGjRBvJtyv9OIIhecgEDHM/q5+J3LerTs2ZAS8TtyBVVKDp21/9QUzTCISmnCjVc51EexmRmlEOebmfKkgIHZMh9AwVJALlZbPbc3xilAEOY2lKaDxTf09kJFJqEgWmMyJ6pBa9qfif10t1eOVlTCSpBkHni8KUYx3jaRB4wCRQzSeGECqZuRXTEZGEahNX2YTgLr68TNr1mntRc+/Oq43rIo4SOkLH6BS56BI10C1qohai6Ak9o1f0ZuXWi/VufcxbV6xi5hD9gfX5Ax1FlH0=</latexit>!neg2,n
<latexit sha1_base64="12fowCcFz2rwzN1Qw0nTG4VFIZc=">AAAB+3icbVBNS8NAEN34WetXrEcvwSJ4kJIUUY9FLx4r2A9oY9hsJ+3S3U3Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5YcKo0q77ba2srq1vbJa2yts7u3v79kGlreJUEmiRmMWyG2IFjApoaaoZdBMJmIcMOuH4Zup3HkEqGot7PUnA53goaEQJ1kYK7Eo/5jDED5mAYR5k9bN6HthVt+bO4CwTryBVVKAZ2F/9QUxSDkIThpXqeW6i/QxLTQmDvNxPFSSYjPEQeoYKzEH52ez23DkxysCJYmlKaGem/p7IMFdqwkPTybEeqUVvKv7n9VIdXfkZFUmqQZD5oihljo6daRDOgEogmk0MwURSc6tDRlhiok1cZROCt/jyMmnXa95Fzbs7rzauizhK6Agdo1PkoUvUQLeoiVqIoCf0jF7Rm5VbL9a79TFvXbGKmUP0B9bnD8IKlEE=</latexit>!neg2,2
<latexit sha1_base64="edBl8JdrPKYHCiQEEACUNRXYh7A=">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwICUpoh6LXjxWsB/QxrDZTtqlm03Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5QcKZ0o7zba2srq1vbJa2yts7u3v79kGlreJUUmjRmMeyGxAFnAloaaY5dBMJJAo4dILxzdTvPIJULBb3epKAF5GhYCGjRBvJtyv9OIIhecgEDHM/q5+5uW9XnZozA14mbkGqqEDTt7/6g5imEQhNOVGq5zqJ9jIiNaMc8nI/VZAQOiZD6BkqSATKy2a35/jEKAMcxtKU0Him/p7ISKTUJApMZ0T0SC16U/E/r5fq8MrLmEhSDYLOF4UpxzrG0yDwgEmgmk8MIVQycyumIyIJ1SausgnBXXx5mbTrNfei5t6dVxvXRRwldISO0Sly0SVqoFvURC1E0RN6Rq/ozcqtF+vd+pi3rljFzCH6A+vzB8CFlEA=</latexit>!neg2,1<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>¬∑¬∑¬∑learnable negative prompts

Learn diverse, non-overlapping
 negative prompts<latexit sha1_base64="ysNe6n4E+he89hOLYgCD5XfHE3k=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUlE1GVRFy6kVLAPaEOYTCft0MmDmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzx4s5k8qyvo3Syura+kZ5s7K1vbO7Z+5XOzJKBKFtEvFI9DwsKWchbSumOO3FguLA47TrTa5zv/tIhWRR+KCmMXUCPAqZzwhWWnLNKhoEWI0J5uld5qbN5k3mmjWrbs2AloldkBoUaLnm12AYkSSgoSIcS9m3rVg5KRaKEU6zyiCRNMZkgke0r2mIAyqddJY9Q8daGSI/EvqFCs3U3xspDqScBp6ezIPKRS8X//P6ifIvnZSFcaJoSOaH/IQjFaG8CDRkghLFp5pgIpjOisgYC0yUrquiS7AXv7xMOqd1+7xu35/VGldFHWU4hCM4ARsuoAG30II2EHiCZ3iFNyMzXox342M+WjKKnQP4A+PzB6y1lDY=</latexit>LNND
<latexit sha1_base64="qnaZBL5VQEk10m0NMw/pplbC56o=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRbBVUlE1GXRjcsKfUEbw2QyaYdOMmHmRqwh+CtuXCji1v9w5984bbPQ1gMXDufcy733+AlnCmz72ygtLa+srpXXKxubW9s75u5eW4lUEtoiggvZ9bGinMW0BQw47SaS4sjntOOPrid+555KxUTchHFC3QgPYhYygkFLnnnQvMsSoXIv6wN9gCwQgzz3zKpds6ewFolTkCoq0PDMr34gSBrRGAjHSvUcOwE3wxIY4TSv9FNFE0xGeEB7msY4osrNptfn1rFWAisUUlcM1lT9PZHhSKlx5OvOCMNQzXsT8T+vl0J46WYsTlKgMZktClNugbAmUVgBk5QAH2uCiWT6VosMscQEdGAVHYIz//IiaZ/WnPOac3tWrV8VcZTRITpCJ8hBF6iOblADtRBBj+gZvaI348l4Md6Nj1lryShm9tEfGJ8/1bKWHw==</latexit>Tposdog<latexit sha1_base64="+BAXpZgUU4Ay0ChMV5OLc/2Xf84=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRbBVUlE1GXRjcsKfUEbw2Q6aYdOZsLMjVhD8FfcuFDErf/hzr9x+lho64ELh3Pu5d57woQzDa77bRWWlldW14rrpY3Nre0de3evqWWqCG0QyaVqh1hTzgRtAANO24miOA45bYXD67HfuqdKMynqMEqoH+O+YBEjGIwU2Af1uyyROg+yLtAHyAhWeR7YZbfiTuAsEm9GymiGWmB/dXuSpDEVQDjWuuO5CfgZVsAIp3mpm2qaYDLEfdoxVOCYaj+bXJ87x0bpOZFUpgQ4E/X3RIZjrUdxaDpjDAM9743F/7xOCtGlnzGRpEAFmS6KUu6AdMZROD2mKAE+MgQTxcytDhlghQmYwEomBG/+5UXSPK145xXv9qxcvZrFUUSH6AidIA9doCq6QTXUQAQ9omf0it6sJ+vFerc+pq0Fazazj/7A+vwBz4qWGw==</latexit>Tposcar<latexit sha1_base64="XNXsmM2l/TFdEfo3KuB3ZeYxu0Q=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRbBVUlE1GXRjcsKfUEbw2Q6aYdOZsLMjVhD8FfcuFDErf/hzr9x+lho64ELh3Pu5d57woQzDa77bRWWlldW14rrpY3Nre0de3evqWWqCG0QyaVqh1hTzgRtAANO24miOA45bYXD67HfuqdKMynqMEqoH+O+YBEjGIwU2Af1uyyROg+yLtAHyMJU53lgl92KO4GzSLwZKaMZaoH91e1JksZUAOFY647nJuBnWAEjnOalbqppgskQ92nHUIFjqv1scn3uHBul50RSmRLgTNTfExmOtR7FoemMMQz0vDcW//M6KUSXfsZEkgIVZLooSrkD0hlH4fSYogT4yBBMFDO3OmSAFSZgAiuZELz5lxdJ87TinVe827Ny9WoWRxEdoiN0gjx0garoBtVQAxH0iJ7RK3qznqwX6936mLYWrNnMPvoD6/MH7hSWLw==</latexit>Tposbus<latexit sha1_base64="PU0xpDgPs8E6XXLNiizLTma4m5Y=">AAAB/3icbVBNS8NAEN34WetXVPDiJVgED1ISEfVY9OKxQr+gjWWznbZLN5uwOxFLzMG/4sWDIl79G978N24/Dtr6YODx3gwz84JYcI2u+20tLC4tr6zm1vLrG5tb2/bObk1HiWJQZZGIVCOgGgSXUEWOAhqxAhoGAurB4Hrk1+9BaR7JCg5j8EPak7zLGUUjte39yl0qoZe1U++khfCAaZDoLGvbBbfojuHME29KCmSKctv+anUiloQgkQmqddNzY/RTqpAzAVm+lWiIKRvQHjQNlTQE7afj+zPnyCgdpxspUxKdsfp7IqWh1sMwMJ0hxb6e9Ubif14zwe6ln3IZJwiSTRZ1E+Fg5IzCcDpcAUMxNIQyxc2tDutTRRmayPImBG/25XlSOy1650Xv9qxQuprGkSMH5JAcE49ckBK5IWVSJYw8kmfySt6sJ+vFerc+Jq0L1nRmj/yB9fkDq1mWiA==</latexit>Tneg1,bus<latexit sha1_base64="z3VHxIiqj/VlehU5OHxjEBknDCM=">AAAB/3icbVDLSgNBEJyNrxhfq4IXL4tB8CBhN4h6DHrxGCEvSGKYnXSSIbOzy0yvGNY9+CtePCji1d/w5t84eRw0saChqOqmu8uPBNfout9WZml5ZXUtu57b2Nza3rF392o6jBWDKgtFqBo+1SC4hCpyFNCIFNDAF1D3h9djv34PSvNQVnAUQTugfcl7nFE0Usc+qNwlEvppJymethAeMPFjnaYdO+8W3AmcReLNSJ7MUO7YX61uyOIAJDJBtW56boTthCrkTECaa8UaIsqGtA9NQyUNQLeTyf2pc2yUrtMLlSmJzkT9PZHQQOtR4JvOgOJAz3tj8T+vGWPvsp1wGcUIkk0X9WLhYOiMw3C6XAFDMTKEMsXNrQ4bUEUZmshyJgRv/uVFUisWvPOCd3uWL13N4siSQ3JETohHLkiJ3JAyqRJGHskzeSVv1pP1Yr1bH9PWjDWb2Sd/YH3+AKzplok=</latexit>Tneg2,bus
<latexit sha1_base64="wm81vzrjK1KIGltNcM8PQa7VGUw=">AAAB/3icbVBNS8NAEN34WetXVfDiJVgED1ISEfVY9OKxQr+grWWznaRLN5uwOxFLzMG/4sWDIl79G978N24/Dtr6YODx3gwz87xYcI2O820tLC4tr6zm1vLrG5tb24Wd3bqOEsWgxiIRqaZHNQguoYYcBTRjBTT0BDS8wfXIb9yD0jySVRzG0AlpILnPGUUjdQv71btUQpB1U/ekjfCAaS8KsqxbKDolZwx7nrhTUiRTVLqFr3YvYkkIEpmgWrdcJ8ZOShVyJiDLtxMNMWUDGkDLUElD0J10fH9mHxmlZ/uRMiXRHqu/J1Iaaj0MPdMZUuzrWW8k/ue1EvQvOymXcYIg2WSRnwgbI3sUht3jChiKoSGUKW5utVmfKsrQRJY3IbizL8+T+mnJPS+5t2fF8tU0jhw5IIfkmLjkgpTJDamQGmHkkTyTV/JmPVkv1rv1MWldsKYze+QPrM8fkveWeA==</latexit>Tneg1,dog<latexit sha1_base64="XEVpJ/rTdyOrL6/Y7b7QH9syl2E=">AAAB/3icbVDLSgNBEJyNrxhfq4IXL4tB8CBhN4h6DHrxGCEvSGKYnXSSIbOzy0yvGNY9+CtePCji1d/w5t84eRw0saChqOqmu8uPBNfout9WZml5ZXUtu57b2Nza3rF392o6jBWDKgtFqBo+1SC4hCpyFNCIFNDAF1D3h9djv34PSvNQVnAUQTugfcl7nFE0Usc+qNwlEvppJymethAeMOmG/TTt2Hm34E7gLBJvRvJkhnLH/mp1QxYHIJEJqnXTcyNsJ1QhZwLSXCvWEFE2pH1oGippALqdTO5PnWOjdJ1eqExJdCbq74mEBlqPAt90BhQHet4bi/95zRh7l+2EyyhGkGy6qBcLB0NnHIbT5QoYipEhlClubnXYgCrK0ESWMyF48y8vklqx4J0XvNuzfOlqFkeWHJIjckI8ckFK5IaUSZUw8kieySt5s56sF+vd+pi2ZqzZzD75A+vzB5SHlnk=</latexit>Tneg2,dog
<latexit sha1_base64="MAcQeGQh5g2HfnovlVtkaZrGk3o=">AAAB/3icbVBNS8NAEN34WetXVPDiJVgED1ISEfVY9OKxQr+gjWWznbZLN5uwOxFLzMG/4sWDIl79G978N24/Dtr6YODx3gwz84JYcI2u+20tLC4tr6zm1vLrG5tb2/bObk1HiWJQZZGIVCOgGgSXUEWOAhqxAhoGAurB4Hrk1+9BaR7JCg5j8EPak7zLGUUjte39yl0qoZe1U++khfCAKaMqy9p2wS26YzjzxJuSApmi3La/Wp2IJSFIZIJq3fTcGP2UKuRMQJZvJRpiyga0B01DJQ1B++n4/sw5MkrH6UbKlERnrP6eSGmo9TAMTGdIsa9nvZH4n9dMsHvpp1zGCYJkk0XdRDgYOaMwnA5XwFAMDaFMcXOrw/pUUYYmsrwJwZt9eZ7UToveedG7PSuUrqZx5MgBOSTHxCMXpERuSJlUCSOP5Jm8kjfryXqx3q2PSeuCNZ3ZI39gff4AjM+WdA==</latexit>Tneg1,car<latexit sha1_base64="lAJtD1qwdRUyzWCHnZmwdEOs+Nk=">AAAB/3icbVDLSgNBEJyNrxhfq4IXL4tB8CBhN4h6DHrxGCEvSGKYnXSSIbOzy0yvGNY9+CtePCji1d/w5t84eRw0saChqOqmu8uPBNfout9WZml5ZXUtu57b2Nza3rF392o6jBWDKgtFqBo+1SC4hCpyFNCIFNDAF1D3h9djv34PSvNQVnAUQTugfcl7nFE0Usc+qNwlEvppJymethAeMGFUpWnHzrsFdwJnkXgzkiczlDv2V6sbsjgAiUxQrZueG2E7oQo5E5DmWrGGiLIh7UPTUEkD0O1kcn/qHBul6/RCZUqiM1F/TyQ00HoU+KYzoDjQ895Y/M9rxti7bCdcRjGCZNNFvVg4GDrjMJwuV8BQjAyhTHFzq8MGVFGGJrKcCcGbf3mR1IoF77zg3Z7lS1ezOLLkkByRE+KRC1IiN6RMqoSRR/JMXsmb9WS9WO/Wx7Q1Y81m9skfWJ8/jl+WdQ==</latexit>Tneg2,carpositive
text featuresnegative
text featuresFigure 2. Overview of NegPropmt. Given the CLIP model and positive prompts learned by existing prompt learning methods such as CoOp
[54], NegPrompt learns a set of negative prompts relative to different ID class labels via three loss functions that enforce the separation
between negative prompts and ID images, and between negative and positive prompts, as well as the diversity of the negative prompts.
where If=Encoderimage(x)andœÑis a temperature pa-
rameter. Next, cross-entropy loss is used to maximize simi-
larity between ID class text embeddings and images.
Lpositive =Exin‚àºDin
train[‚àílog(p(y=i|x)]. (2)
This method largely improves CLIP‚Äôs classification perfor-
mance by adapting the learnable prompts to the target data.
The resulting prompt embeddings of ID classes are used as
positive prompts to support the accurate learning of nega-
tive prompts in our method below.
3.2. Proposed Approach
NegPrompt aims to learn a set of negative prompts, each
representing a negative connotation of an ID class. As
shown in Fig. 2, it involves generating a series of prompts
that resemble the positive prompts obtained through CoOp,
but with a negative class semantic, such as ‚ÄúA photo of not
a [class label]‚Äù. These negative prompts, combined with
class labels, can generate a range of negative text features
around the positive prompts, so that OOD images exhibit
higher similarity to the negative prompts than ID images.
3.2.1 Learning Negative Prompts
To acquire accurate negative prompts representing nega-
tive class semantics, we first utilize CoOp to learn positive
prompts œâposs, after which we consider positive prompts
to accurately capture the class semantics of samples in the
ID dataset. Thereafter, we freeze the positive prompts and
focus solely on learning the negative prompts. A negative
prompt is denoted as {œâneg
1, œâneg
2, ..., œâneg
n}, where nrep-
resents the number of context vectors we aim to learn. Byutilizing the frozen CLIP text encoder, we can obtain neg-
ative prompt embeddings Tf,neg
l,i=Encodertext(tneg
l,i),
where tneg
l,i={œâneg
l,1, œâneg
l,2, ..., œâneg
l,n, ci}is the l-th negative
prompt relative the ID class i. Thus, due to the presence
of the negative prompts, we turn the prediction probability
into the following form:
p(y=i|x) =exp(Sf,pos
i)
Pk
j=1exp(Sf,pos
j) +Pp
l=1Pk
j=1exp(Sf,neg
l,j)
(3)
where Sf,pos
j = sim(Tf,pos
j, If)/œÑ,Sf,neg
l,j=
sim(Tf,neg
l,j, If)/œÑandpis the number of negative
prompts we aim to learn for each ID class. The objective
is to learn negative text prompt embeddings that can effec-
tively separate ID and OOD samples around the positive
text features. To achieve this, we introduce the following
three loss functions:
Negative-Image Separation Loss. One of our objectives
is to have the negative text prompt embeddings serve as the
closest text features to OOD images. However, we only
have images from the ID dataset; no OOD images are avail-
able. To remedy this problem, we take an alternative ap-
proach that aims to push the negative text features away
from the ID images. In this regard, we draw inspiration
from OE [11]. Contrary to the approach in OE, where
the probability distribution of outlier images is evenly dis-
tributed among all ID labels, we distribute the probability
distribution obtained from ID images evenly among all neg-
ative prompts. Since the network parameters are frozen, this
drives the negative text features to move away from the ID
17587
images, resulting in the learning of prompts with a negative
connotation. The formulation is as follows:
LNIS=Exin‚àºDin
train[H(u;F(x))], (4)
where F(x)is the probability vector computed as
Softmax (Sf,neg), anduis a uniform distribution and H
is the cross entropy loss.
Negative-Positive Distance Loss. To avoid learning trivial
negative prompts that are distant from both ID and OOD
images, we need to control the negative text feature within
a certain range between the ID and OOD images. To this
end, we devise a constraint, enforcing that the negative text
feature does not deviate too far from the positive text fea-
ture. Therefore, we introduce the following loss function to
guarantee a certain level of similarity between the negative
and positive text feature in the latent space:
LNPD =‚àí1
k‚àópkX
j=1pX
i=1sim(Tf,neg
i,j, Tf,pos
j).(5)
Negative-Negative Distance Loss. Furthermore, to ensure
that we are effectively learning diverse, non-overlapping
negative prompts, we extend the distances between differ-
ent negative text features within the same label via the fol-
lowing loss function:
LNND =1
k‚àóp‚àó(p‚àí1)kX
j=1pX
i=1X
lÃ∏=isim(Tf,neg
i,j, Tf,neg
l,j).
(6)
Overall, our NegPrompt objective consists of the above
three losses. During training, we are able to obtain a diverse
set of non-trivial prompts that effectively convey negative
meanings relative to the ID class labels by minimizing the
following overall loss:
LNegativePrompts =LNIS+Œ≤‚àóLNPD +Œ≥‚àóLNND,(7)
where Œ≤andŒ≥are hyper-parameters to balance the losses.
3.2.2 Open-Vocabulary Capability
Since the negative prompts we learn do not depend on spe-
cific class labels but are instead generic templates represent-
ing negative semantics of any given class labels, it is pos-
sible to utilize the generalization ability of CLIP to learn a
set of transferable negative prompts. Specifically, instead of
utilizing all of the ID classes Din
train , NegPrompt may only
employ its small subset Din,sub
train for training the negative
prompts œâneg, where Din,sub
train ={(x, yin
sub)|yin
sub‚äÇyin}.
After obtaining the trained negative prompts, we combine
them with the remaining ID class names, i.e., replace ciin
tneg
l,iwith the unseen ID class names, to obtain the corre-
sponding negative prompts for the novel ID classes that areID OOD
Split-1 All dog classes Non-animal classes
Split-2 Half of hunting dog classes Other 4-legged animal classes
Split-3 Mix of common classes Mix of common classes
Table 1. Three ImageNet-1K splits for hard OOD detection.
unseen during training. This approach, which achieves out-
of-distribution detection by only exposing with a small por-
tion of ID images, is unprecedented in prior research. We
refer this as to be open-vocabulary OOD detection.
3.2.3 Inference
During inference, we employ the MCM [32] scoring ap-
proach for OOD detection, but with the addition of our neg-
ative prompts into the softmax function. Particularly, MCM
uses the inverse of the maximum softmax score in Eq. 1 as
the OOD score. Our OOD scoring extends MCM and de-
fines it as: s(x) = max( p(y=i|x)), where p(y=i|x)
is defined in Eq. 3 that also includes the similarities of the
test image to the negative prompts, in addition to the sim-
ilarities to the positive prompts. The rationale behind this
is that for ID images, they will be matched to one of the
positive text features, leading to a higher Sf,posbut lower
Sf,neg, and thereby a higher maximum softmax score ( i.e.,
a lower OOD score). Conversely, for OOD data, it will be
matched to one of the negative text features, resulting in a
lower maximum softmax score ( i.e., a higher OOD score).
4. Experiment
4.1. Experimental Details
Datasets. For conventional OOD detection, we use a popu-
lar benchmark in which ImageNet-1K [3] with 1,000 classes
is used as the ID dataset, and the same OOD datasets as in
[32] are used, including subsets of Texture [2], iNatural-
ist [43], Places [51] and SUN [48]. In addressing the more
challenging OOD scenarios, we partitioned the ImageNet1k
dataset into two segments: one segment of the data serves
as the ID, while the other serves as OOD. As shown in Ta-
ble 1, three different splits are derived, following from [35].
We further create another ImageNet split, Split-4, in which
the first 100 classes are used as ID data and the subsequent
900 classes are used as OOD samples. Following CoOp and
LoCoOp [33, 54], during our training process, we utilized
only few-shot training data for each category. Particularly,
we only train the model with 16 images per ID class and
without any exposure to OOD images. During testing, we
employ the entire ID and OOD test set for evaluation.
Implementation Details. Following existing studies [46],
we use CLIP based on CLIP-B/16 which is pre-trained from
OpenCLIP [16]. NegPrompt is trained using 16-shot im-
ages of all ID classes under the normal OOD detection set-
ting. For open-vocabulary OOD detection, we train our
model using only the images of the first 10% classes from
the ID dataset, withholding 90% ID classes that only appear
17588
MethodTexture iNaturalist Places SUN Avg
AUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚Üì
Zero-shot methods
MCM [32]‚Ä†86.11 57.77 94.61 30.91 89.77 44.69 92.57 34.59 90.76 42.74
CLIPN [46]‚Ä†90.93 40.83 95.27 23.94 92.28 33.45 93.92 26.17 93.10 31.10
CLIP-based posthoc methods
MSP [10]‚Ä†74.84 73.66 77.74 74.57 72.18 79.12 73.97 76.95 74.98 76.22
MaxLogit [12]‚Ä†88.63 48.72 88.03 60.88 87.45 55.54 91.16 44.83 88.82 52.49
Energy [27]‚Ä†88.22 50.39 87.18 64.98 87.33 57.40 91.17 46.42 88.48 54.80
ReAct [41]‚Ä†88.13 49.88 86.87 65.57 87.42 56.85 91.04 46.17 88.37 54.62
ODIN [26]‚Ä†87.85 51.67 94.65 30.22 85.54 55.06 87.17 54.04 88.80 47.75
Prompt learning methods
CoOp [54] 89.47 45.00 93.77 29.81 90.58 40.11 93.29 40.83 91.78 51.68
LoCoOp [33]‚Ä†90.19 42.28 96.86 16.05 91.98 32.87 95.07 23.44 93.52 28.66
NegPrompt (Ours) 91.60 35.21 98.73 6.32 93.34 27.60 95.55 22.89 94.81 23.01
Open-vocabulary OOD detection
CoOp (10%) 87.58 50.55 91.08 42.53 89.56 46.12 91.52 41.92 89.94 45.28
LoCoOp (10%) 88.21 47.32 94.47 34.90 91.64 39.85 92.54 26.30 90.15 37.09
NegPrompt (Ours) (10%) 90.30 39.31 98.39 7.48 92.68 29.75 93.70 26.92 93.76 25.86
Table 2. Conventional OOD detection results. We trained using ImageNet1k as the ID and CLIP-B/16 as the CLIP backbone. The boldfaced
results indicate the best performance. Results marked with ‚Ä†are taken from [46] and [33]. ‚ÄòMETHOD‚Äô (10%) in open-vocabulary OOD
detection is to evaluate the performance of the ‚ÄòMETHOD‚Äô when only images from 10% ID classes are accessible during training.
together with OOD data during inference. We train a shared
positive prompt and two shared negative prompts w.r.t. each
training ID class. The hyperparameters Œ≤andŒ≥are set to
0.1 and 0.05, respectively (see Appendix A for detail). In
the first stage, CoOp is trained for 100 epochs to obtain the
positive prompts. In the second stage, the positive prompts
are frozen, and our model is trained for 10 epochs to learn
the negative prompts. For all experiments, we report the av-
eraged results over three runs with different random seeds.
Comparison Methods. To substantiate the effectiveness
of NegPrompts, we conduct an empirical analysis of three
distinct categories of methodologies employed for OOD de-
tection utilizing Vision-language models. These categories
encompass zero-shot pretraining approaches, the methods
that combine the CLIP image encoder with classical ap-
proaches, and the methods grounded in prompt learning.
In the context of zero-shot methods, we opted for the two
recent methods, MCM [32] and CLIPN [46]. MCM em-
ploys the original CLIP, utilizing the maximum softmax
probability operation on the similarities for detection, and
CLIPN involves an additional training phase during pre-
training, specifically training a negative text encoder us-
ing large external data. For the second group of methods,
we adapt previous logits-based methodologies to the use of
the CLIP image encoder, including MSP [10], Energy [27],
MaxLogit [12], ReAct [41] and ODIN [26], to serve as the
CLIP-adapted methods. For the prompt learning methods,
NegPrompt is compared with CoOp [54] and LoCoOp [33].
Evaluation Metrics. Two OOD detection metrics are used.
The first metric is the False Positive Rate at a 95% True
Negative Rate (FPR95), which denotes the rate of falsely
identified OOD instances when the true negative rate is
maintained at 95%. The second metric is the Area Under
the Receiver Operating Characteristic curve (AUROC), rep-resenting the measure of OOD ranking across various clas-
sification thresholds. We also check the classification accu-
racy of the ID data to evaluate how the OOD detectors affect
the ID classification.
4.2. Comparison to State-of-the-art Models
Conventional OOD Detection. The results of conven-
tional OOD detection are reported in Table 2. It is clear
that our proposed NegPrompt achieves consistently supe-
rior performance in both individual OOD datasets and the
averaged results. When compared with the zero-shot meth-
ods, on average, our approach surpasses the best competing
method CLIPN by more than 1.5% in AUC and around 8%
in FPR95, despite the fact that CLIPN requires the use of an
additional large external dataset to train an additional neg-
ative text encoder. In other words, although NegPrompt is
significantly more lightweight than CLIPN in model size,
it can substantially and consistently outperform CLIPN in
both metrics across all OOD datasets. The adapted post-hoc
methods generally do not leverage the CLIP‚Äôs capabilities
well and thus perform less effectively.
NegPrompt also substantially surpasses both prompt
learning-based methods, reducing the FPR95 by about 28%
(CoOp) and 5% (LoCoOp). This indicates that the learned
negative prompts provide informed knowledge about OOD
data, which is lacking in the competing methods, helping
largely reduce detection errors.
Hard OOD Detection. Hard OOD detection presents
unique challenges as the OOD samples often exhibit some
similar features as the ID samples. The results on the four
hard OOD datasets derived from ImageNet-1K are shown
in Table 3. Similar empirical observations can be derived.
Our method NegPropmt is consistently the best performer
in the average performance, showcasing its general effec-
17589
MethodSplit-1 Split-2 Split-3 Split-4 Avg
AUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚Üì
Zero-shot methods
MCM 97.93 9.17 88.10 56.40 90.34 33.05 98.72 4.73 93.77 25.83
CLIPN 99.38 2.07 97.77 10.55 90.03 36.85 98.83 4.68 96.50 13.53
CLIP-based posthoc methods
MSP 77.85 63.60 68.73 83.63 79.10 70.55 82.40 65.52 77.02 70.83
MaxLogit 99.87 0.49 98.06 8.69 90.96 34.34 99.35 2.66 97.06 11.55
Energy 99.88 0.46 98.18 8.40 90.65 35.02 99.36 2.83 97.02 11.68
ReAct 99.34 0.72 97.91 9.33 90.72 35.65 99.12 2.94 96.77 12.16
ODIN 98.78 1.12 98.23 8.18 89.92 37.20 98.76 13.20 96.42 14.92
Prompt learning methods
CoOp 98.53 6.78 88.25 50.76 90.64 33.89 98.54 5.11 93.99 24.14
LoCoOp 98.64 6.29 84.63 61.09 91.30 27.79 98.83 41.44 93.35 34.15
NegPrompt (Ours) 99.85 0.62 98.54 7.60 93.89 22.89 99.57 1.60 97.96 8.18
Open-vocabulary OOD detection
CoOp (10%) 97.97 12.217 80.11 74.62 87.92 46.00 96.59 16.60 90.65 37.36
LoCoOp (10%) 98.00 9.23 87.02 52.18 80.51 59.93 82.41 48.72 86.99 42.52
NegPrompt (Ours) (10%) 99.66 1.36 96.30 19.89 91.75 26.92 98.14 5.24 96.46 13.36
Table 3. Hard OOD detection results. We use the same notations here as those used in Table 2.
tiveness across different dataset splits. The superiority of
NegPrompt over the zero-shot and prompt learning-based
methods is similar to that in Table 2. Although it is slightly
less effective than Energy under the Split-1 setting, it out-
performs Energy in all other metrics, achieving maximally
over 3% AUC and 12% FPR95 improvement among the
performance on the other OOD datasets.
Note that the results in Table 3 are generally more
promising than that in Table 2. This is mainly because the
OOD detection difficulty in all four ImageNet-1K splits is
largely reduced since the number of their ID classes is sig-
nificantly less than that in the full ImageNet-1K data.
Open-Vocabulary OOD Detection. The open-vocabulary
OOD detection results are reported in both Tables 2 and 3.
Impressively, even when training on only 10% ID classes,
our approach can still perform better than the competing
methods using the full ID classes, e.g., the average AUC
and FPR95 in Table 2. In this open-vocabulary setting, in
general, both LoCoOp and CoOp exhibit a much larger per-
formance decline than NegPrompt, especially on the results
in Table 3, in which CoOp has over 3% AUC drop and Lo-
CoOp has over 6% AUC drop while our method has only
about 1.5% AUC drop. These results demonstrate that the
negative prompts in NegPrompt have much better transfer-
ability than those in the two competing methods.
4.2.1 Classification Accuracy on ID Data
We also evaluate the classification accuracy on the ID data
when using NegPrompt for OOD detection, with CoOp, Lo-
CoOp, MCM and CLIPN as the baselines. The classifica-
tion accuracy results on the full ImageNet-1K test data are
shown in Table 4. When using the full ImageNet-1K train-
ing ID class data, our method NegPrompt can maintain the
same classification accuracy as CoOp.
Our accuracy is slightly compromised when using only
10% ID classes in our training. On the other hand, the OODMethod Top-1 Accuracy
CoOp 72.1
LoCoOp‚Ä†71.7
CLIPN & MCM 67.0
NegPrompt(Ours)(10%) 71.9
NegPrompt(Ours)(Full) 72.1
Table 4. Top-1 Accuracy. Results with ‚Ä†are taken from [33].
ID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281
0.241
0.2190.253T exture
ID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281
0.241
0.2340.274iNaturalist
ID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281
0.241
0.2240.265Places365
ID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281
0.241
0.2230.247SUN
Figure 3. Similarity of ID/OOD and Positive/Negative Prompts.
detection in LoCoOp compromises the ID classification ac-
curacy, dropped from 72.1% in CoOp to 71.7%. This may
be attributed to its focus on the localized regions within the
background rather than the primary object of interest, re-
sulting in the missing of some discriminative features for
ID data classification. CLIPN and MCM, being zero-shot
methods, have not been exposed to the target ID data, lead-
ing to a much lower accuracy than the other methods.
17590
Ablation StudyTexture iNaturalist Places SUN Avg
AUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìAUC‚ÜëFPR95 ‚ÜìACC‚Üë
Backbones
ResNet-50 79.44 72.73 90.97 44.10 84.35 61.37 88.04 50.55 85.70 57.19 68.2
ResNet-101 82.97 70.83 93.96 31.3 86.41 51.66 88.81 47.61 88.04 50.35 70.3
ViT-B-32 90.43 38.79 97.40 12.64 92.83 32.79 92.82 26.03 93.37 27.56 72.6
# Negative Prompts
1 90.04 40.67 98.24 9.23 90.37 32.62 92.26 27.33 92.73 27.46 72.1
Training Process
One-stage 80.25 72.87 77.02 95.74 82.53 95.94 81.13 95.72 80.23 90.07 62.8
ViT-B-16 & 2 & Two-stage 90.30 39.31 98.39 7.48 92.68 29.75 93.70 26.92 93.76 25.86 72.1
Table 5. Results of our ablation experiments.
Positive text featuresNegative text featuresOOD imagesID images
Figure 4. T-SNE visualization of NegPrompt, utilizing a subset of
ImageNet - TinyImageNet as the dataset.
4.3. Analysis of NegPrompt
4.3.1 Why Does NegPrompt Work?
To better understand the effectiveness of NegPrompt,
we summarize the average maximum similarity between
ID/OOD images and positive/negative prompts, as shown
in Fig. 3. Across all four OOD datasets, ID images have the
highest similarity with positive prompts, and OOD images
with negative prompts. This suggests positive prompts are
closer to ID images and negative prompts to OOD images in
latent space, ensuring OOD images receive lower softmax
scores ( i.e., higher OOD scores) than ID images in Eq. 3.
Using TinyImageNet [23], a subset of ImageNet, we fur-
ther visualized the learned negative text features with its
test data [6]. Learning three negative prompts per ID class,
the results in Fig. 4 demonstrate that positive text features
from positive prompts align closely with ID images in latent
space. Conversely, the negative text features encoded from
negative prompts lie outside of the ID data, with OOD im-
ages interspersed among them. This suggests negative text
features effectively act as a fence aligning much better to the
OOD images than the ID images, well supporting the OOD
detection while preserving the ID classification accuracy.
4.3.2 Ablation Study
Table 5 shows our ablation study results on the backbone,
the number of negative prompts, and the training process.
Backbones. We experiment with diverse CLIP back-
bones. The results reveal that for CNN-based backboneslike ResNet50 and ResNet101, the OOF detection perfor-
mance is not as proficient as the ViT-based backbones. Re-
garding ViT-B-32 and ViT-B-16, their performance is found
to be uneven. Overall, the performance of OOD detection
tends to increase with more advanced backbones.
The Number of Negative Prompts. The number of nega-
tive prompts also influences the results. It is observed that
the OOD detection performance improves when increasing
the number of negative prompts from one to two. There-
fore, it is suggested to further increase the number of neg-
ative prompts for better detection accuracy. However, note
that with an increase in the number of negative prompts, the
computational cost also rises rapidly. A good balance be-
tween computational cost and detection accuracy is needed
when determining the number of negative prompts.
Training Process. The training process is also important.
As discussed before, due to the necessity of anchoring the
positive prompts, a two-stage training process is used in our
model. This process involves training the positive prompts
in the first stage and freezing them before proceeding to
train the negative prompts in the second stage. When si-
multaneously training both positive and negative prompts in
a unified step, the model‚Äôs ability to effectively learn posi-
tive prompts is significantly undermined, and consequently
we obtain unstable negative prompts, leading to the largely
decreased OOD detection performance.
5. Conclusion
We present NegPrompt, a novel approach for prompt
learning-based OOD detection. It utilizes VLMs to learn
a small set of negative prompts for conveying negative se-
mantics relative to ID classes. Our empirical results reveal
that NegPrompt 1) achieves superior OOD detection perfor-
mance compared to the SOTA models across various OOD
datasets in both conventional and hard OOD detection sce-
narios, and 2) learns transfer negative prompts that enable
excellent open-vocabulary OOD detection performance.
6. Acknowledgement
This work was supported by the National Natural Science
Foundation of China 62276016, 62372029.
17591
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
Neural Information Processing Systems , 35:23716‚Äì23736,
2022. 2
[2] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing textures in the
wild. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 3606‚Äì3613, 2014. 5
[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248‚Äì255. Ieee, 2009. 5
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 2
[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1, 2
[6] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei
Shu. Zero-shot out-of-distribution detection based on the
pre-trained model clip. In Proceedings of the AAAI confer-
ence on artificial intelligence , pages 6568‚Äì6576, 2022. 2, 3,
8
[7] Ruohuan Fang, Guansong Pang, and Xiao Bai. Simple
image-level classification improves open-vocabulary object
detection. arXiv preprint arXiv:2312.10439 , 2023. 2
[8] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
Clip-adapter: Better vision-language models with feature
adapters. International Journal of Computer Vision , pages
1‚Äì15, 2023. 1
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770‚Äì778, 2016. 1
[10] Dan Hendrycks and Kevin Gimpel. A baseline for detect-
ing misclassified and out-of-distribution examples in neural
networks. In International Conference on Learning Repre-
sentations , 2017. 1, 3, 6
[11] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.
Deep anomaly detection with outlier exposure. arXiv
preprint arXiv:1812.04606 , 2018. 3, 4
[12] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou,
Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and
Dawn Song. Scaling out-of-distribution detection for real-
world settings. arXiv preprint arXiv:1911.11132 , 2019. 3,
6
[13] Ping Hu, Ximeng Sun, Stan Sclaroff, and Kate Saenko. Du-
alcoop++: Fast and effective adaptation to multi-label recog-nition with limited annotations. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 2023. 3
[14] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-
distribution detection for large semantic space. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 8710‚Äì8719, 2021. 3
[15] Rui Huang, Andrew Geng, and Yixuan Li. On the impor-
tance of gradients for detecting distributional shifts in the
wild. Advances in Neural Information Processing Systems ,
34:677‚Äì689, 2021. 3
[16] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. 5
[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904‚Äì4916, 2021. 2
[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In European Conference on Computer
Vision , pages 709‚Äì727. Springer, 2022. 1, 3
[19] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neu-
big. How can we know what language models know? Trans-
actions of the Association for Computational Linguistics , 8:
423‚Äì438, 2020. 1
[20] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad
Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple:
Multi-modal prompt learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19113‚Äì19122, 2023. 2, 3
[21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. Advances
in neural information processing systems , 33:18661‚Äì18673,
2020. 2
[22] Shu Kong and Deva Ramanan. Opengan: Open-set recog-
nition via open data generation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 813‚Äì822, 2021. 3
[23] Ya Le and Xuan Yang. Tiny imagenet visual recognition
challenge. CS 231N , 7(7):3, 2015. 8
[24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation. In In-
ternational Conference on Machine Learning , pages 12888‚Äì
12900. PMLR, 2022. 2
[25] Tianqi Li, Guansong Pang, Xiao Bai, Jin Zheng, Lei Zhou,
and Xin Ning. Learning adversarial semantic embeddings for
zero-shot recognition in open worlds. Pattern Recognition ,
149:110258, 2024. 2
[26] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the re-
liability of out-of-distribution image detection in neural net-
works. In International Conference on Learning Represen-
tations , 2018. 1, 3, 6
17592
[27] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan
Li. Energy-based out-of-distribution detection. Advances
in neural information processing systems , 33:21464‚Äì21475,
2020. 3, 6
[28] Yuyuan Liu, Choubo Ding, Yu Tian, Guansong Pang,
Vasileios Belagiannis, Ian Reid, and Gustavo Carneiro.
Residual pattern learning for pixel-wise out-of-distribution
detection in semantic segmentation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 1151‚Äì1161, 2023. 3
[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012‚Äì10022, 2021. 1
[30] Wenjun Miao, Guansong Pang, Tianqi Li, Xiao Bai, and Jin
Zheng. Out-of-distribution detection in long-tailed recog-
nition with calibrated outlier class learning. arXiv preprint
arXiv:2312.10686 , 2023. 3
[31] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim
Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh
Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran
Shen, et al. Simple open-vocabulary object detection. In
European Conference on Computer Vision , pages 728‚Äì755.
Springer, 2022. 2
[32] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li,
and Yixuan Li. Delving into out-of-distribution detection
with vision-language representations. Advances in Neural
Information Processing Systems , 35:35087‚Äì35102, 2022. 2,
3, 5, 6
[33] Atsuyuki Miyai, Qing Yu, Go Irie, and Kiyoharu Aizawa.
Locoop: Few-shot out-of-distribution detection via prompt
learning. In Thirty-Seventh Conference on Neural Informa-
tion Processing Systems , 2023. 2, 3, 5, 6, 7
[34] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural
networks are easily fooled: High confidence predictions for
unrecognizable images. In CVPR , pages 427‚Äì436, 2015. 2
[35] Andres Palechor, Annesha Bhoumik, and Manuel G ¬®unther.
Large-scale open-set classification protocols for imagenet. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 42‚Äì51, 2023. 5
[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. 2018. 2
[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision, 2021. 1, 2, 3
[38] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278‚Äì25294, 2022. 2
[39] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric
Wallace, and Sameer Singh. Autoprompt: Eliciting knowl-edge from language models with automatically generated
prompts. arXiv preprint arXiv:2010.15980 , 2020. 1, 2
[40] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast
adaptation to multi-label recognition with limited annota-
tions. Advances in Neural Information Processing Systems ,
35:30569‚Äì30582, 2022. 3
[41] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-
distribution detection with rectified activations. Advances in
Neural Information Processing Systems , 34:144‚Äì157, 2021.
3, 6
[42] Yu Tian, Yuyuan Liu, Guansong Pang, Fengbei Liu, Yuan-
hong Chen, and Gustavo Carneiro. Pixel-wise energy-biased
abstention learning for anomaly segmentation on complex
urban driving scenes. In European Conference on Computer
Vision , pages 246‚Äì263. Springer, 2022. 3
[43] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and
Serge Belongie. The inaturalist species classification and de-
tection dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 8769‚Äì8778,
2018. 5
[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2
[45] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang.
Vim: Out-of-distribution with virtual-logit matching. In
2022 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 4911‚Äì4920, 2022. 3
[46] Hualiang Wang, Yi Li, Huifeng Yao, and Xiaomeng Li.
Clipn for zero-shot ood detection: Teaching clip to say no.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 1802‚Äì1812, 2023. 2, 3, 5, 6
[47] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou,
Qingsen Yan, Peng Wang, and Yanning Zhang. Vadclip:
Adapting vision-language models for weakly supervised
video anomaly detection. arXiv preprint arXiv:2308.11681 ,
2023. 3
[48] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In 2010 IEEE computer so-
ciety conference on computer vision and pattern recognition ,
pages 3485‚Äì3492. IEEE, 2010. 5
[49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836‚Äì3847, 2023. 2
[50] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual
probing is [mask]: Learning vs. learning to recall. arXiv
preprint arXiv:2104.05240 , 2021. 1
[51] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE transactions on pattern analysis
and machine intelligence , 40(6):1452‚Äì1464, 2017. 5
[52] Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Learning
placeholders for open-set recognition. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4401‚Äì4410, 2021. 3
17593
[53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Conditional prompt learning for vision-language mod-
els. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 16816‚Äì16825,
2022. 1, 3
[54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. In-
ternational Journal of Computer Vision , 130(9):2337‚Äì2348,
2022. 1, 3, 4, 5, 6
[55] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and
Jiming Chen. Anomalyclip: Object-agnostic prompt learn-
ing for zero-shot anomaly detection. arXiv preprint
arXiv:2310.18961 , 2023. 3
[56] Jiawen Zhu and Guansong Pang. Toward generalist anomaly
detection via in-context residual learning with few-shot sam-
ple prompts. arXiv preprint arXiv:2403.06495 , 2024. 2
17594
