MindBridge: A Cross-Subject Brain Decoding Framework
Shizun Wang Songhua Liu Zhenxiong Tan Xinchao Wang‚Ä†
National University of Singapore
{shizun.wang, songhua.liu, zhenxiong }@u.nus.edu, xinchao@nus.edu.sg
New Subject Adaptation500/8859Data Points1500/8859Data Points4000/8859Data Points
Image StimulusVanillaMindBridge(Ours)Image StimulusVanillaMindBridge(Ours)Image StimulusVanillaMindBridge(Ours)
Figure 1. Image stimuli and images reconstructed from captured brain signals. Given the limited training data from a new subject
(subj07 from the NSD dataset[1]), our proposed MindBridge can faithfully reconstruct natural images using less data, benefiting from
pretrained cross-subject knowledge. In contrast, the Vanilla method, which represents current methods following a per-subject-per-
model paradigm, fails to learn effectively from limited data.
Abstract
Brain decoding, a pivotal field in neuroscience, aims
to reconstruct stimuli from acquired brain signals, primar-
ily utilizing functional magnetic resonance imaging (fMRI).
Currently, brain decoding is confined to a per-subject-per-
model paradigm, limiting its applicability to the same in-
dividual for whom the decoding model is trained. This
constraint stems from three key challenges: 1) the inherent
variability in input dimensions across subjects due to differ-
ences in brain size; 2) the unique intrinsic neural patterns,
influencing how different individuals perceive and process
sensory information; 3) limited data availability for new
subjects in real-world scenarios hampers the performance
of decoding models.
In this paper, we present a novel approach, MindBridge ,
that achieves cross-subject brain decoding by employing
only one model. Our proposed framework establishes a
generic paradigm capable of addressing these challenges
by introducing biological-inspired aggregation function and
novel cyclic fMRI reconstruction mechanism for subject-
invariant representation learning. Notably, by cycle re-
‚Ä†Corresponding author.construction of fMRI, MindBridge can enable novel fMRI
synthesis, which also can serve as pseudo data augmenta-
tion. Within the framework, we also devise a novel reset-
tuning method for adapting a pretrained model to a new
subject. Experimental results demonstrate MindBridge‚Äôs
ability to reconstruct images for multiple subjects, which
is competitive with dedicated subject-specific models. Fur-
thermore, with limited data for a new subject, we achieve a
high level of decoding accuracy, surpassing that of subject-
specific models. This advancement in cross-subject brain
decoding suggests promising directions for wider applica-
tions in neuroscience and indicates potential for more ef-
ficient utilization of limited fMRI data in real-world sce-
narios. Project page: https://littlepure2333.
github.io/MindBridge
1. Introduction
The human brain, an intricate web of neurons, possesses
the remarkable ability to encode the sensory stimuli that
we encounter every day, making sense of our perceptual
world. While the reverse process, known as brain decod-
ing, aims to reconstructs image stimulus from brain signals,
which are primarily captured by functional magnetic reso-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11333
nance imaging (fMRI). Brain decoding has been a subject
of intense interest, as it offers the tantalizing prospect of un-
raveling the secrets of cognition and perception, and present
a potential advancement for brain-computer interface (BCI)
[7] and beyond. From GANs [24, 35] to diffusion mod-
els [24, 33, 38], the use of increasingly powerful generative
models has enabled brain decoding to reconstruct more re-
alistic and faithful images. However, nowadays brain de-
coding is confronted with significant challenges that hinder
its application on a broader scale.
Specifically, the current practice of brain decoding is
confined to subject-specific applications [12, 21, 23, 33, 38,
45]. In other words, a decoding model trained on a one sub-
ject‚Äôs brain can only be effectively applied to that same sub-
ject, and can not be applied to other subjects, which results
in high expense of model storage and training. This lim-
itation motivates us to move towards cross-subject brain
decoding , which is capable of using one model to decode
brain signals from multiple subjects and adapting to new
subjects. Such a paradigm thereby can expand the utility
of brain decoding in a more general way and bring more
applicability in real scenarios.
However, it requires adequately addressing various sub-
stantial challenges in pursuit of this beautiful vision: 1) Size
Variability : fMRI signals exhibit substantial size differ-
ences across subjects , largely due to the inherent variabil-
ity in brain size and structure, which necessitates a flexible
approach to handle this variability. 2) Diverse Neural Re-
sponses : The intricacies of the brain extend beyond struc-
tural variations. The way each subject‚Äôs brain processes
stimuli is uniquely shaped by their experiences, biases, and
cognitive patterns, posing a challenge in unifying the inter-
pretation of brain signals. 3) Data Scarcity for New Sub-
jects : In real-world applications, it is highly cumbersome to
acquire extensive fMRI data for new subjects if not infeasi-
ble at all. The high costs in both resources and time signifi-
cantly hinder the training and adaptation of brain-decoding
models for new subjects.
To address these challenges, we devise ‚ÄúMindBridge‚Äù ,
a novel framework designed to achieve cross-subject brain
decoding. MindBridge employs innovative strategies to
tackle each of the identified obstacles: 1) Adaptive Sig-
nal Aggregation : Inspired by neural-science findings that
the brain activation is sparse and only neurons exceeding a
certain threshold activate, we propose to use an aggregation
function based on adaptive max pooling to extract most use-
ful information , and unify the input dimension of fMRI sig-
nals across different subjects. 2) Subject-Invariant Rep-
resentation : We extract subject-invariant semantic embed-
dings from disparate subjects‚Äô fMRI signals by utilizing
a novel cycle reconstruction mechanism. These embed-
dings are then translated and aligned within a consistent
CLIP embedding space, facilitating a standardized interpre-tation across varying neural responses. 3) Efficient Adap-
tation Strategy : To mitigate the data scarcity issue for new
subjects, we introduce a novel finetuning method, reset-
tuning. Because transferable knowledge from cross-subject
pretraining is held in the deep layers, while the shallow lay-
ers are responsible for projecting diverse subjects‚Äô fMRI
signals into subject-invariant embeddings. Reset-tuning re-
set the shallow layers but reuse the deep layers.
Furthermore, MindBridge incorporates additional en-
hancements to improve semantic accuracy and expand its
application. Utilizing a multi-modal versatile diffusion
(VD) model [46], we can incorporate not only image stimuli
but also the text caption as training data, then predict cor-
responding image and text embeddings to reconstruct more
semantically faithful images. Moreover, MindBridge opens
new possibility to synthesize new brain signals using data
from other subjects while preserving same semantic mean-
ing by cyclic fMRI reconstruction. Therefore, MindBridge
not only bridges the gap among different brains but also po-
tentially augments the volume of available data.
To verify our approach, we conducted experiments on
the publicly available NSD dataset [1]. Notably, the ab-
sence of common images across different subjects in the
training set poses an additional challenge to cross-subject
brain decoding. Surprisingly, MindBridge, employing only
one model, achieves performance comparable to subject-
specific methods, which require multiple models. Addition-
ally, experiments on new subject adaptation validate that
our method surpasses methods trained from scratch, show-
casing the benefits of transferable knowledge from cross-
subject pretraining and our proposed reset-tuning.
In summary, our contributions are as follows:
‚Ä¢ To the best of our knowledge, we are the first to ef-
fectively addresses the challenge of cross-subject brain
decoding. We design a novel framework, MindBridge,
equipped with an adaptive signal aggregation function
and novel cyclic fMRI reconstruction mechanism for
subject-invariant representations learning.
‚Ä¢ We introduce a novel ‚Äúreset-tuning‚Äù strategy, which ef-
ficiently adapts the MindBridge model to new subjects,
and effectively overcoming the limitations posed by data
scarcity for new subjects.
‚Ä¢ MindBridge enables new capability for the synthesis of
brain signals, leveraging data across various subjects
while maintaining consistent semantic interpretation.
‚Ä¢ Extensive experiments demonstrate MindBridge‚Äôs effi-
cacy and adaptability, showcasing its potential to signifi-
cantly advance the field of brain decoding.
2. Related Work
2.1. Brain Decoding
The evolution of brain decoding has been marked by the in-
tegration of advanced modeling approaches. Earlier work
11334
[15] applies sparse linear regression on fMRI data to pre-
dict features from early convolutional layers of pretrained
CNNs. With the introduction of generative adversarial net-
works (GANs) [11], there has been a shift towards visual
decoding techniques that map brain signals to the latent
spaces of GANs, facilitating the reconstruction of hand-
written digits [31], human faces [40], and natural scenes
[12, 24, 34].
The advent of high-resolution image synthesis with La-
tent Diffusion Models [28] and multi-modal contrastive
models like CLIP [25], along with extensive fMRI datasets
[1], has propelled researchers to map fMRI signals into the
CLIP embedding space. This mapping guides latent diffu-
sion models for image reconstruction [38], with efforts fo-
cusing on improved mapping through self-supervision [3],
masked modeling [5], and contrastive learning [33]. Addi-
tional explorations involve advanced diffusion models [21]
and conditional control [19, 45]. Unlike almost all brain
decoding research that requires training multiple models
for different subjects, MindBridge stands out by aiming to
achieve cross-subject brain decoding with a single model.
2.2. Diffusion Models
Diffusion models (DMs) [6, 8, 13, 20, 36, 44, 48] have re-
cently emerged as a focal point in deep generative model re-
search, known for their ability to generate high-quality im-
ages. DMs utilize iterative denoising to recover a sampled
variable from Gaussian noise and transform it into a sam-
ple conforming to the learned data distribution. With large-
scale image-text pair datasets [32], DMs have demonstrated
superior performance in the task of text-to-image genera-
tion [9, 26, 30, 42, 47] and achieves unprecedented image
quality. Building on this, latent diffusion models (LDMs),
also known as Stable Diffusion (SD), have furthered DMs
by reducing computational demands through denoising in
a latent space produced by autoencoders. An advanced
form of LDMs, the Versatile Diffusion (VD) model [46],
demonstrates the capability to produce high-quality images,
guided by both image and text inputs. Consequently, we
have adopted the VD model for its dual-input capacity,
leveraging its enhanced image generation potential.
3. MindBridge
3.1. Data Elaboration
To better understand the task at hand, we illustrate the
data we used in ahead. We have chosen the widely-used
Natural Scenes Dataset (NSD) [1] for our brain decod-
ing research. This dataset consists of high-resolution 7-
Tesla fMRI scans collected from 8 healthy adult subjects,
who were instructed to view thousands of natural images
from MS-COCO dataset [18]. Following common practices
[12, 21, 23, 33, 38, 45], our research mainly use data from4 subjects (subj01, 02, 05, 07), who completed all the scan
sessions. Notably, only a subset of data, 982 images, were
commonly viewed by all four subjects. Those data were
used as the test set . While the remaining data, each of 8,859
distinct images viewed by each subject were used as the
training set . Following prior work [33], we use prepro-
cessed fMRI voxels from ‚ÄúNSDGeneral‚Äù regions of inter-
est (ROI). Due to the inherent variablity in brain size and
structure, the fMRI signals within the ROI exhibit different
sizes (about 13,000 to 16,000 voxels per subject), which is
the first challenge we need to tackle in cross-subject brain
decoding. The original acquired fMRI data is 4D (3D+t),
which is firstly averaged among time dimension, and then
is flattened from 3D to 1D. ROIs serves as masks on the 1D
vector. So the dimensionality of input fMRI voxels is 1D.
3.2. Cross-Subject Brain Decoding
Current brain decoding pipeline can be summarized in two
steps: mapping fMRI voxels to CLIP embeddings and then
using these embeddings to guide diffusion models in gener-
ating reconstructed images. While previous brain decoding
methods all fall in a per-subject-per-model fashion. Here
we argue the key insight for achieving cross-subject brain
decoding lies in establishing a shared common representa-
tion space that is subject-invariant. However, there are two
main barriers to realizing this objective. The first is the vari-
ation in the size of fMRI signals among different subjects,
as explained in Sec. 3.1. The second challenge is how to
effectively model subject-invariant representation learning.
To address these challenges, we propose MindBridge,
a novel framework for cross-subject brain decoding. For-
mally, we denote the 1D fMRI voxels from subject sas
Vs‚ààRF, where Frepresents fMRI voxels‚Äô size. The cor-
responding image stimulus Iand image caption Tcan be
extracted by a pretrained CLIP model as image embedding
eIand text embedding eT.
Pipeline. MindBridge first adaptively unifies the fMRI
voxels Vsto a unified size vs=f(Vs)using a biologically-
inspired aggregation function f. Unlike previous methods
that directly learn the projection between fMRI voxels and
corresponding CLIP embeddings, MindBridge projects dif-
ferent subjects‚Äô aggregated fMRI voxels vsto an intermedi-
ate semantic embedding es=Es(vs)using a subject-wise
brain embedder Es. To ensure that semantic embeddings
from different subjects reside in a common shared space,
we propose a novel cyclic fMRI reconstruction mecha-
nism. This mechanism relys on an additional subject-wise
brain builder Bsto reconstruct the unified fMRI voxels
ÀÜvs=Bs(es). Once the semantic embeddings are obtained,
a brain translator Ttranslates them into two embeddings,
(ÀÜeI,ÀÜeT) =T(es), representing the predicted CLIP image
and text embeddings. The brain embedder, brain builder
and brain translator are all MLP-like networks.
11335
BrainSignal ùëâ!MindBridge257√ó76877√ó768CLIP Image Embedding ùëí"CLIP text Embedding ùëí#Image ùêº257√ó76877√ó768Pred Image Embedding ÃÇùëí"Pred text Embedding ÃÇùëí#Caption ùëáImageClipperTextClipper‚Ñí$%&'(
‚Ñí)(*)
Subject-wiseCross-subjectFrozen
TrainableRecon.ImageVersatileDiffusion
Brain Signal ùëâ!
2048Recon. Brain (ùë£!Varied size: 13,000~16,000‚Ñí+(,
Brain TranslatorBrainEmbedder8192Aggre. Brain ùë£!Aggregation FunctionBrainBuilder 
8192
‚Ä¶‚Ä¶‚Ñí,-,2048
Recon. Brain ùë£.BrainEmbedder8192Aggre. Brain ùë£&
BrainBuilder
8192
‚Ñí,-,Semantic Embedding ùëí!Semantic Embedding ùëí&
2048BrainEmbedder
Semantic Embedding ùëí.TrainInferenceMindBridgeCycle Loss
Subject A (for example)Subject B (for example)
‚ÄúA bowl of cooked carrots and broccoli.‚Äù
Figure 2. Overview of MindBridge. MindBridge is a cross-subject brain decoding framework capable of handling fMRI signals from
different subjects. Initially, an aggregation function unifies the size of fMRI signals. Subsequently, subject-wise brain embedders and brain
builders are trained to obtain subject-invariant semantic embeddings. The Brain Translator then generates text and image embeddings,
which are utilized to reconstruct images through versatile diffusion model. The dimension of data is denoted within the box.
Diffusion Model. Due to the limited volume of brain
data, a generative model trained on a large-scale dataset is
necessary to aid the image reconstruction process. Previ-
ous methods [5, 33, 38] have demonstrated the superiority
of using diffusion models as an interface for image genera-
tion. In this work, we have chosen to employ the versatile
diffusion (VD) [46] model, a multimodal latent diffusion
model that is guided by image and text CLIP embeddings
and achieves state-of-the-art performance in image genera-
tion. Its exceptional capabilities give us an opportunity to
utilize both visual and semantic information, as represented
by CLIP image and text embeddings predicted by Mind-
Bridge, to reconstruct images at inference time.
Adaptive fMRI Aggregation. Modern neural-science
research [22, 27, 41] reveals that visual stimuli are encoded
sparsely in the primary visual cortex, activating only a few
neurons for most natural images. Also, neurons require a
certain level of threshold, to become active and fire an ac-
tion potential [14, 16]. The activation functions of artifi-
cial neural networks such as Sigmoid or ReLU also echo
this fundamental principle in neurophysiology [10, 29]. In-
spired by these findings, we posit that brain signals can be
aggregated sparsely, with higher values tending to be more
valuable. Consequently, we propose employing ‚ÄúAdaptiveMax Pooling‚Äù1as the aggregation function. This function
unifies the size of input fMRI signals by dynamically ad-
justing its pooling size to produce a fixed output size.
Learning Objectives. MindBridge learns CLIP image
and text embeddings through two types of losses. One is
the SoftCLIP loss, introduced in [33], which has proven ef-
fective in aligning the fMRI modality with the embedding
space of the pretrained CLIP model. This loss facilitates
contrastive learning by maximizing the similarity of posi-
tive pairs while minimizing the similarity of negative pairs.
Positive pairs are defined as the soft labels produced by the
dot product of embeddings within a batch, and the loss con-
siders both CLIP-CLIP and Brain-CLIP scenarios.
LSoftCLIP (p, t) =‚àíPN
i=1PN
j=1
"
exp (ti¬∑tj/œÑ)PN
m=1exp ti¬∑tm
œÑ¬∑log 
exp (pi¬∑tj/œÑ)PN
m=1exp pi¬∑tm
œÑ!#
(1)
Where p,tare the predicted CLIP embedding and target
CLIP embedding in a batch of size N, respectively. œÑis a
temperature hyperparameter.
1PyTorch implementation: https://pytorch.org/docs/
stable/generated/torch.nn.AdaptiveMaxPool1d.html
11336
During our exploratory experiments, however, we ob-
served that reconstructed images still exhibited some ar-
tifacts when using only the SoftCLIP loss. We hypothe-
size that this may be due to the SoftCLIP loss‚Äôs inability to
guarantee the authenticity of the learned CLIP embeddings.
Therefore, we introduced the second loss, the MSE loss, to
ensure a more accurate prediction of CLIP embeddings.
LMSE(p, t) =1
NPN
i=1(pi‚àíti)2(2)
Incorporating these two losses ensures a more natural
image reconstruction. The complete set of losses for pre-
dicting image and text CLIP embeddings includes:
Limage =LSoftCLIP (ÀÜeI, eI) +LMSE(ÀÜeI, eI) (3)
Ltext=LSoftCLIP (ÀÜeT, eT) +LMSE(ÀÜeT, eT)(4)
Where eIandeTare CLIP image and text embeddings
of image stimuli Iand captions T.
Cyclic fMRI Reconstruction. To facilitate subject-
invariant representation learning, the simplest way is to di-
rectly minimize the distance between the semantic embed-
dings esfrom two subjects when they are viewing the same
images. Nevertheless, as described in Sec. 3.1, there is no
common-viewd image across different subjects in the train-
ing set. Therefore, we turn to design a mechanism, wishing
to synthesize the fMRI signals even the subject does not ac-
tually see the image stimulus. In this way, we can mimic
the scenario that two subjects are viewing the same images.
To realize that, we first introduce a brain builder Bsto
reconstruct fMRI signal ÀÜvs=Bs(Es(vs))in a AutoEncoder
[2] manner. The reconstruction loss is:
Lrec=1
NPN
i=1(ÀÜvs‚àívs)2(5)
We then randomly select two subjects aandbfrom all
training subjects in every training iteration. Through a
cyclic fMRI reconstruction, we can transform subject a‚Äôs
fMRI signal vainto subject b‚Äôsvb=Bb(Ea(va))like they
are viewing the same image. This cycle is tenable only
when the involved semantic embeddings ea=Ea(va), eb=
Eb(vb)are really subject-invariant, that is, the same when
viewing the same image. So a cycle loss is employed to
ensure consistency in this cycle:
Lcyc=1
NPN
i=1(eb‚àíea)2(6)
MindBridge is trained end-to-end by incorporating all
these losses to achieve cross-subject brain decoding.
Ltotal=Limage +Ltext+Lrec+Lcyc (7)
3.3. New-Subject Adaptation
The scope of ‚Äúcross-subject‚Äù is not limited to previously
trained subjects. With MindBridge‚Äôs ability to handle dif-
ferent subjects within a single model, adapting the modelto a new subject is now feasible. This scenario is ubiqui-
tous in real-world applications, such as diagnosing a new
patient. However, in practice, acquiring brain signals for a
new subject can be extremely costly and time-consuming.
For example, the authors of NSD dataset [1] spent an en-
tire year completing all fMRI scan sessions. To address the
challenge of limited data for a new subject, we adopt the
classic ‚Äúpretrain-then-finetune‚Äù paradigm and propose two
techniques: reset-tuning and pseudo data augmentation, to
enhance the performance of new subject adaptation.
Reset-Tuning. Contrary to traditional fine-tuning in
computer vision, which often freezes shallow layers to
leverage generic features [49], MindBridge adopts an in-
verse manner. The transferable knowledge within Mind-
Bridge resides in the deep layers, brain translator T. While
the shallow layers, brain embedder Esand builder Bs, are
subject-specific due to human brain diversity. Hence, we
propose reset-tuning strategy: training the brain embedder
and builder from reset parameters while freezing the brain
translator to retain the pretrained cross-subject knowledge.
Pseudo Data Augmentation. A straightforward ap-
proach to mitigating the data scarcity problem is through
data augmentation. However, suitable data augmentation
method for brain signals is currently unavailable. Never-
theless, the cycle reconstruction mechanism of fMRI can
serve as a form of pseudo data augmentation. During the
adaptation process, fMRI signals from all previously trained
subjects can be utilized to augment new subject‚Äôs data: con-
verted into the fMRI signals of the new subject through cy-
cle reconstruction, regulated by LrecandLcyctoo.
4. Experiments and Analysis
Evaluation Metrics. To quantitatively compare with other
methods, we adopt eight image quality evaluation met-
rics following [23]. PixCorr, SSIM [43], AlexNet(2), and
AlexNet(5) [17] are used to evaluate low-level properties.
Inception [37], CLIP [25], EffNet-B [39], and SwA V [4]
are considered for evaluating higher-level properties.
4.1. Cross-Subject Brain Decoding
MindBridge can perform brain decoding for multiple sub-
jects using one single model, whereas other methods re-
quire training separate models for each subject. To vali-
date MindBridge‚Äôs effectiveness, we compare its average
image reconstruction performance across all four subjects
with that of state-of-the-art methods: Takagi et al. [38],
Brain-Diffuser [23], and MindEye [33]. We also train a per-
subject-per-model MindBridge for fair comparison, which
is denoted as ‚ÄúMindBridge (Single)‚Äù. The quantitative and
qualitative results for all methods are presented in Tab. 1
and Fig. 3 respectively. Through subject-invariant represen-
tation learning, we achieve comparable performance against
11337
Cross-subject brain decoding
36MindEye
4835eyecoco73ktakagi347
Tagakiet al.
MindBridge(Ours)
Subject 1
Stimulus
Subject 2Subject 5Subject 7Subject 1Subject 1
Figure 3. Brain decoding results with only one model. Unlike previous methods, which confine one model to a specific subject, our
proposed cross-subject brain decoding framework, MindBridge, can reconstruct images from multiple subjects using just one model.
Method # ModelsLow-Level High-Level
PixCorr ‚ÜëSSIM‚ÜëAlex(2) ‚ÜëAlex(5) ‚ÜëIncep‚ÜëCLIP‚ÜëEffNet-B ‚ÜìSwA V ‚Üì
Takagi et al. [38] 4 ‚Äì ‚Äì 83.0% 83.0% 76.0% 77.0% ‚Äì ‚Äì
Brain-Diffuser [23] 4 .254 .356 94.2% 96.2% 87.2% 91.5% .775 .423
MindEye [33] 4 .309 .323 94.7% 97.8% 93.8% 94.1% .645 .367
MindBridge (Single) 4 .148 .259 86.9% 95.3% 92.2% 94.3% .713 .413
MindBridge (Ours) 1 .151 .263 87.7% 95.5% 92.4% 94.7% .712 .418
Table 1. Quantitative comparison of brain decoding between MindBridge and other methods. Our MindBridge is the first effective
cross-subject brain decoding approach that only employs one model to reconstruct images from multiple subjects‚Äô fMRI signals. While
other methods follows a per-subject-per-model fashion. All metrics are calculated as the average across 4 subjects.
state-of-the-art methods while maintaining just one model,
demonstrating our success on cross-subject brain decoding.
4.2. New Subject Adaptation
MindBridge also possesses a capability to transfer its pre-
trained knowledge for adapting to new subjects, which is
valuable in practical applications where collecting brain
signals for new subjects is resource-intensive and time-
consuming. To simulate scenarios with limited data, we
tested our method using subsets of the total 8859 train-
ing data ‚Äì specifically, 500, 1500, and 4000 data points
‚Äì for new subject adaptation. We selected three subjects
for pretraining (source subjects) and one additional sub-
ject for adaptation (target subject). We choose to com-
pare our method with the ‚Äúvanilla‚Äù approach, which in-
volves training MindBridge from ‚Äúscratch‚Äù on the same tar-get data in a per-subject-per-model fashion. In Fig. 1, we
present a qualitative comparison of our method with vanilla
method. The vanilla method struggles to reconstruct rea-
sonable images, which can be attributed to two main fac-
tors. Firstly, our pretrained brain translator serves as a ro-
bust prior backbone, transferring highly useful knowledge
that significantly enhances our method‚Äôs performance. Sec-
ondly, the full-parameter model used in the vanilla approach
tends to overfit when data is limited. In contrast, our ap-
proach employs reset-tuning, updating only the parameters
within the lightweight brain embedder and brain builder, ef-
fectively preventing overfitting. The quantitative compar-
ison shown in Tab. 2 demonstrates that our method not
only significantly outperforms traditional approaches but
also highlights the feasibility of reliable brain decoding with
substantially less data. This advancement opens up exciting
11338
Method # DataLow-Level High-Level
PixCorr ‚ÜëSSIM‚ÜëAlex(2) ‚ÜëAlex(5) ‚ÜëIncep‚ÜëCLIP‚ÜëEffNet-B ‚ÜìSwA V ‚Üì
Vanilla 500 .079 .171 73.5% 83.3% 74.4% 80.1% .894 .587
MindBridge (Ours) 500 .112 .229 79.6% 89.0% 82.3% 86.7% .840 .521
Vanilla 1500 .107 .206 79.4% 90.0% 82.4% 87.2% .844 .523
MindBridge (Ours) 1500 .140 .250 84.6% 92.6% 85.8% 91.0% .796 .485
Vanilla 4000 .114 .232 81.4% 92.2% 85.3% 89.8% .815 .491
MindBridge (Ours) 4000 .156 .258 85.7% 94.1% 88.9% 92.5% .765 .458
Table 2. Reseults of new subject adaptation in limited data scenario. Here we report results from models that were trained on subsets
of 500, 1500, and 4000 data points, selected from a total of 8859 training data points for subject 7. MindBridge (Ours) is fine-tuned
using reset-tuning from the pretrained MindBridge model in subjects 1, 2, and 5. Compared to vanilla methods, which trains per-subject-
per-model MindBridge from scratch, our MindBridge achieves superior brain decoding performance, benefiting from its use of pretrained
cross-subject knowledge. For adaptation of other subjects, please refer to supplementary materials.
Aggregation FunctionLow-Level High-Level
PixCorr ‚ÜëSSIM‚ÜëAlex(2) ‚ÜëAlex(5) ‚ÜëIncep‚ÜëCLIP‚ÜëEffNet-B ‚ÜìSwA V ‚Üì
Interpolation .151 .260 87.1% 95.4% 92.1% 94.4% .712 .413
AdaAvgPool .163 .274 87.4% 95.7% 92.8% 94.5% .707 .405
AdaMaxPool (Ours) .165 .284 88.7% 96.2% 93.7% 95.0% .697 .400
Table 3. Ablation of different aggregation functions. Models are trained and evaluated on subject 1.
Synthetic brain data
34
30
Subj2 on subj1
1738020
Subject 5‚Äôs Image StimuliMindBridge
Subject 1‚Äôs Reconstruction
Subject 2‚Äôs Image StimuliSubject 1‚Äôs ReconstructionMindBridge
Figure 4. Novel fMRI synthesis within MindBridge pretrained
on subject 1, 2, 5 . The fMRI signals of subjects 5 and 2 are con-
verted into subject 1‚Äôs fMRI signals through cycle reconstruction,
then subject 1‚Äôs brain embedder are utilized for brain decoding.
prospects for considerably reducing scan times in practi-
cal applications, paving the way for more cost-efficient and
generalizable brain decoding strategies.
4.3. Novel fMRI Synthesis
Utilizing our cycle reconstruction mechanism, we have en-
abled a new task: novel fMRI synthesis. This process can
transform one‚Äôs fMRI signal into another‚Äôs, while preserv-ing the same semantic content as the original stimuli. By
employing the pretrained MindBridge model on subject 1,2,
and 5, we converted fMRI signals of subject 5 and 2 into
those of subjects 1 using their respective brain embedders
and brain builders. To validate the quality of these novel
fMRI signals, we display the reconstructed images from
the synthesized novel fMRI signals in Fig. 4. Notably,
the stimuli corresponding to these novel fMRI signals have
never been viewed by subject 1. Yet, they can still be re-
constructed faithfully, demonstrating the effectiveness of
our proposed cycle reconstruction mechanism in synthesize
fMRI as well as facilitating subject-invariant representation
learning.
4.4. Ablation Study
Ablation on Aggregation Functions. A key component
in cross-subject brain decoding is the aggregation function.
The ability of this function to retain valuable information
while unifying the dimensions of brain signals is crucial.
The more effectively it preserves useful information, the
more accurate the results will be during the brain decod-
ing process. We show ablation comparison with other func-
tions in Tab. 3. Compared to adaptive average pooling and
interpolation functions, our chosen function, adaptive max
pooling, not only offers better biological interpretability but
also achieves superior performance.
Ablation on Pretraining Losses. We present the re-
11339
Pretrain LossLow-Level High-Level
PixCorr ‚ÜëSSIM‚ÜëAlex(2) ‚ÜëAlex(5) ‚ÜëIncep‚ÜëCLIP‚ÜëEffNet-B ‚ÜìSwA V ‚Üì
SoftCLIP loss .085 .336 76.9% 83.1% 79.1% 80.6% .877 .542
+ MSE loss .158 .272 88.3% 95.7% 92.2% 94.5% .720 .418
+ Recon + Cycle Loss (Ours) .168 .277 88.7% 96.1% 92.7% 94.9% .707 .410
Table 4. Ablation of different losses at pretraining stage. Models are trained on subject 1,2 and 5, then evaluated on subject 1.
Finetune StrategyLow-Level High-Level
PixCorr ‚ÜëSSIM‚ÜëAlex(2) ‚ÜëAlex(5) ‚ÜëIncep‚ÜëCLIP‚ÜëEffNet-B ‚ÜìSwA V ‚Üì
Full-tuning + Limage +Ltext .110 .232 79.5% 88.1% 79.6% 86.4% .847 .526
Full-tuning + Ltotal .100 .220 78.6% 88.0% 79.8% 86.0% .851 .529
Reset-tuning + Limage +Ltext .116 .227 79.7% 88.9% 80.5% 86.1% .851 .525
Reset-tuning + Ltotal (Ours) .112 .229 79.6% 89.0% 82.3% 86.7% .840 .520
Table 5. Ablation of different finetuning strategies. Models are trained on subject 1,2 and 5, then finetuned and evaluated on subject 7.
sults of involving different losses at the pretraining stage
in Tab. 4. When only the SoftCLIP loss is applied, the
model struggles to fully learn the reasonable CLIP embed-
dings and can only achieve a resemblance to the target CLIP
embeddings. The inclusion of MSE loss enhances the nat-
uralness of the reconstructed images. Finally, the addition
of both reconstruction loss and cycle loss improves the in-
tegrity of subject-invariant representation learning, thereby
enhancing cross-subject brain decoding performance.
Ablation on Finetuning Strategies. Once we have ob-
tained the pretrained model, aside from our proposed reset-
tuning strategy, we have several options for fine-tuning it
to adapt to a new subject. Full-tuning involves fine-tuning
the brain translator, while reset-tuning entails keeping the
brain translator frozen. Both fine-tuning methods involve
training a new brain embedder and brain builder, if applica-
ble. We also conducted an ablation study of losses during
fine-tuning to assess the benefits of using pseudo data aug-
mentation, which corresponds to the application of losses
related to cycle reconstruction. Tab. 5 presents a quantita-
tive comparison among these fine-tuning strategies. The re-
sults indicate that the strategy combining reset-tuning with
pseudo data augmentation yields the most satisfactory re-
sults. This outcome suggests that reset-tuning, which only
establishes a projection between brain signals and semantic
embeddings, can already sufficiently adapt to a new sub-
ject. Moreover, the incorporation of our novel pseudo data
augmentation can further imporove performance.
5. Discussion
Currently, due to the limited availability of high-quality
fMRI data, one limitation of our paper is the evaluation is
only restricted to a small dataset NSD. As a cross-subjectframework, the generalizability could be further validated
on a more diverse and large-scale dataset in the future.
While considering the high cost of acquiring fMRI data, our
method also offers a potential solution to reduce scan time.
Another limitation is that the fMRI signals are serialized as
1D vectors, which may ruin the original spatial relationship.
Although brain decoding holds promise for assisting vi-
sual impaired people, ethical concerns arise regarding mis-
use for malicious or immoral purposes. Thus, a consented
data privacy protocol and a responsible research code of
conduct must be established with broader considerations.
6. Conclusion
In this paper, we introduced ‚ÄúMindBridge‚Äù, a novel cross-
subject brain decoding framework that successfully chal-
lenges the conventional per-subject-per-model paradigm in
brain decoding. By innovatively addressing the critical
issues of size variability, diverse neural responses, and
data scarcity for new subjects, MindBridge demonstrates
significant advancements in cross-subject brain decoding.
Our approach, characterized by adaptive signal aggrega-
tion, cyclic fMRI reconstruction for subject-invariant repre-
sentation, and reset-tuning for new subject adaptation, has
proven effective in our experiments with the NSD dataset.
These achievements not only enhance the decoding accu-
racy across multiple subjects but also open new avenues for
fMRI synthesis and practical applications in neuroscience.
7. Acknowledgement
This project is supported by the Ministry of Education Sin-
gapore, under its Academic Research Fund Tier 2 (Award
Number: MOE-T2EP20122-0006).
11340
References
[1] Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L
Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau,
Brad Caron, Franco Pestilli, Ian Charest, et al. A massive
7t fmri dataset to bridge cognitive neuroscience and artificial
intelligence. Nature neuroscience, 25(1):116‚Äì126, 2022. 1,
2, 3, 5
[2] Dor Bank, Noam Koenigstein, and Raja Giryes. Autoen-
coders. Machine learning fordata science handbook: data
mining andknowledge discovery handbook, pages 353‚Äì374,
2023. 5
[3] Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini,
Tal Golan, and Michal Irani. From voxels to pixels and
back: Self-supervision in natural-image reconstruction from
fmri. Advances inNeural Information Processing Systems,
32, 2019. 3
[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learn-
ing of visual features by contrasting cluster assignments.
Advances inneural information processing systems, 33:
9912‚Äì9924, 2020. 5
[5] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and
Juan Helen Zhou. Seeing beyond the brain: Conditional dif-
fusion model with sparse masked modeling for vision de-
coding. In Proceedings oftheIEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 22710‚Äì
22720, 2023. 3, 4
[6] Prafulla Dhariwal and Alexander Nichol. Diffusion mod-
els beat gans on image synthesis. Advances inneural
information processing systems, 34:8780‚Äì8794, 2021. 3
[7] Bing Du, Xiaomu Cheng, Yiping Duan, and Huansheng
Ning. fmri brain decoding and its applications in brain‚Äì
computer interface: A survey. Brain Sciences, 12(2):228,
2022. 2
[8] Chengbin Du, Yanxi Li, Zhongwei Qiu, and Chang Xu. Sta-
ble diffusion is unstable. Advances inNeural Information
Processing Systems, 36, 2023. 3
[9] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Struc-
tural pruning for diffusion models. In Advances inNeural
Information Processing Systems, 2023. 3
[10] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep
learning. MIT press, 2016. 4
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial networks.
Communications oftheACM, 63(11):139‚Äì144, 2020. 3
[12] Zijin Gu, Keith Jamison, Amy Kuceyeski, and Mert
Sabuncu. Decoding natural image stimuli from fmri data
with a surface-based convolutional network. arXiv preprint
arXiv:2212.02409, 2022. 2, 3
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances inneural information
processing systems, 33:6840‚Äì6851, 2020. 3
[14] Alan L Hodgkin and Andrew F Huxley. A quantitative de-
scription of membrane current and its application to conduc-
tion and excitation in nerve. TheJournal ofphysiology, 117
(4):500, 1952. 4[15] Tomoyasu Horikawa and Yukiyasu Kamitani. Generic de-
coding of seen and imagined objects using hierarchical vi-
sual features. Nature communications, 8(1):15037, 2017. 3
[16] Eric R Kandel, James H Schwartz, Thomas M Jessell,
Steven Siegelbaum, A James Hudspeth, Sarah Mack, et al.
Principles ofneural science. McGraw-hill New York, 2000.
4
[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Advances inneural information processing systems,
25, 2012. 5
[18] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision‚ÄìECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
PartV13, pages 740‚Äì755. Springer, 2014. 3
[19] Yizhuo Lu, Changde Du, Qiongyi Zhou, Dianpeng Wang,
and Huiguang He. Minddiffuser: Controlled image recon-
struction from human brain activity with semantic and struc-
tural diffusion. In Proceedings ofthe31stACM International
Conference onMultimedia, pages 5899‚Äì5908, 2023. 3
[20] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache:
Accelerating diffusion models for free. In The IEEE/CVF
Conference onComputer Vision andPattern Recognition,
2024. 3
[21] Weijian Mai and Zhijun Zhang. Unibrain: Unify image
reconstruction and captioning all in one diffusion model
from human brain activity. arXiv preprint arXiv:2308.07428,
2023. 2, 3
[22] Bruno A Olshausen and David J Field. Emergence of simple-
cell receptive field properties by learning a sparse code for
natural images. Nature, 381(6583):607‚Äì609, 1996. 4
[23] Furkan Ozcelik and Rufin VanRullen. Brain-diffuser: Nat-
ural scene reconstruction from fmri signals using generative
latent diffusion. arXiv preprint arXiv:2303.05334, 2023. 2,
3, 5, 6
[24] Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila
Reddy, and Rufin VanRullen. Reconstruction of perceived
images from fmri patterns and semantic brain exploration
using instance-conditioned gans. In 2022 International Joint
Conference onNeural Networks (IJCNN), pages 1‚Äì8. IEEE,
2022. 2, 3
[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference onmachine learning, pages
8748‚Äì8763. PMLR, 2021. 3, 5
[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125, 1
(2):3, 2022. 3
[27] Rajesh PN Rao and Dana H Ballard. Predictive coding in
the visual cortex: a functional interpretation of some extra-
classical receptive-field effects. Nature neuroscience, 2(1):
79‚Äì87, 1999. 4
11341
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
theIEEE/CVF conference oncomputer vision andpattern
recognition, pages 10684‚Äì10695, 2022. 3
[29] David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. Learning representations by back-propagating er-
rors. nature, 323(6088):533‚Äì536, 1986. 4
[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances inNeural Information
Processing Systems, 35:36479‚Äì36494, 2022. 3
[31] Sanne Schoenmakers, Markus Barth, Tom Heskes, and Mar-
cel Van Gerven. Linear reconstruction of perceived images
from human brain activity. NeuroImage, 83:951‚Äì961, 2013.
3
[32] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for train-
ing next generation image-text models. Advances inNeural
Information Processing Systems, 35:25278‚Äì25294, 2022. 3
[33] Paul S Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan
Shabalin, Alex Nguyen, Ethan Cohen, Aidan J Demp-
ster, Nathalie Verlinde, Elad Yundler, David Weisberg,
et al. Reconstructing the mind‚Äôs eye: fmri-to-image with
contrastive learning and diffusion priors. arXiv preprint
arXiv:2305.18274, 2023. 2, 3, 4, 5, 6
[34] Katja Seeliger, Umut G ¬®uc ¬∏l¬®u, Luca Ambrogioni, Yagmur
G¬®uc ¬∏l¬®ut¬®urk, and Marcel AJ van Gerven. Generative adver-
sarial networks for reconstructing natural images from brain
activity. NeuroImage, 181:775‚Äì785, 2018. 3
[35] Guohua Shen, Tomoyasu Horikawa, Kei Majima, and
Yukiyasu Kamitani. Deep image reconstruction from human
brain activity. PLoS computational biology, 15(1):e1006633,
2019. 2
[36] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502, 2020. 3
[37] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In Proceedings oftheIEEE
conference oncomputer vision and pattern recognition,
pages 2818‚Äì2826, 2016. 5
[38] Yu Takagi and Shinji Nishimoto. High-resolution image re-
construction with latent diffusion models from human brain
activity. biorxiv. 2022. 2, 3, 4, 5, 6
[39] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In International
conference onmachine learning, pages 6105‚Äì6114. PMLR,
2019. 5
[40] Rufin VanRullen and Leila Reddy. Reconstructing faces
from fmri patterns using deep generative neural networks.
Communications biology, 2(1):193, 2019. 3[41] William E Vinje and Jack L Gallant. Sparse coding and
decorrelation in primary visual cortex during natural vision.
Science, 287(5456):1273‚Äì1276, 2000. 4
[42] Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai
Han, et al. Pangu- œÄ: Enhancing language model architec-
tures via nonlinearity compensation. In arXiv:2312.17276,
2023. 3
[43] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions onimage processing,
13(4):600‚Äì612, 2004. 5
[44] Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao
Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, and
Mingyuan Zhou. Patch diffusion: Faster and more data-
efficient training of diffusion models. Advances inNeural
Information Processing Systems, 36, 2024. 3
[45] Weihao Xia, Raoul de Charette, Cengiz ¬®Oztireli, and Jing-
Hao Xue. Dream: Visual decoding from reversing human
visual system. arXiv preprint arXiv:2310.02265, 2023. 2, 3
[46] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang,
and Humphrey Shi. Versatile diffusion: Text, images and
variations all in one diffusion model. In Proceedings ofthe
IEEE/CVF International Conference onComputer Vision,
pages 7754‚Äì7765, 2023. 2, 3, 4
[47] Xingyi Yang and Xinchao Wang. Diffusion model as repre-
sentation learner. In IEEE/CVF International Conference on
Computer Vision, 2023. 3
[48] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
Diffusion probabilistic model made slim. In TheIEEE/CVF
Conference onComputer Vision andPattern Recognition,
2023. 3
[49] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lip-
son. How transferable are features in deep neural net-
works? Advances inneural information processing systems,
27, 2014. 5
11342
