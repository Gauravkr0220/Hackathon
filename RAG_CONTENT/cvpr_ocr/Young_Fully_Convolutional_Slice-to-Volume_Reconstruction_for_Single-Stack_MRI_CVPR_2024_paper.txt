 
 Abstract  In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR) refers to computational reconstruction of an unknown 3D magnetic resonance volume from stacks of 2D slices corrupted by motion. While promising, current SVR methods require multiple slice stacks for accurate 3D reconstruction, leading to long scans and limiting their use in time-sensitive applications such as fetal fMRI. Here, we propose a SVR method that overcomes the shortcomings of previous work and produces state-of-the-art reconstructions in the presence of extreme inter-slice motion. Inspired by the recent success of single-view depth estimation methods, we formulate SVR as a single-stack motion estimation task and train a fully convolutional network to predict a motion stack for a given slice stack, producing a 3D reconstruction as a byproduct of the predicted motion. Extensive experiments on the SVR of adult and fetal brains demonstrate that our fully convolutional method is twice as accurate as previous SVR methods. Our code is available at github.com/seannz/svr. 1. Introduction  For non-invasive imaging of the brain where the subject may potentially exhibit severe uncontrollable motionâ€”such as in the case of the fetal populationâ€”two-dimensional (2D) magnetic resonance imaging (MRI) techniques are typically used to acquire a stack of 2D brain slices and assemble them into a 3D volume, a computational procedure often referred to as slice-to-volume reconstruction (SVR). Unlike 3D MRI acquisitions where the entire ğ‘˜-space (Fourier) coeï¬ƒcients can become corrupted even from a single motion event, 2D techniques can restrict this corruption to within a slice while leaving the ğ‘˜-space data for the other slices intact. Le SVR procedure can then be applied on the non-corrupted slices to enable artifact-free imaging of the brain and facilitate other downstream tasks, such as brain morphometry [1â€“3], whole brain segmentation [4â€“6] and atlas creation [7â€“9].  A large number of classical SVR techniques [10â€“18] are descended from the work of Rousseau et al. [19], where the SVR task is posed as an optimization problem to recover an unknown 3D volume from observed 2D slices. Such inverse methods do not have the capability to learn the latent space of reconstructions, and as a result, typically require multiple orthogonal slice stacks to cross-regularize the estimation of the motion of each stack, adding a computational burden to an already iterative numerical method. More recent methods [20â€“25] overcome the burden of numerical optimization by directly predicting the position parameters of 2D slices with neural network models. However, these approaches process slices independently of each other and cannot constrain the reconstruction in 3D space [20â€“22] or require multiple slice stacks for accurate reconstruction [25], precluding their use in time-sensitive applications such as fetal fMRI [26, 27].  In the closely related tasks of pairwise image registration [28â€“32], motion [33â€“39] and depth estimation [40â€“43], fully convolutional network (FCN) architectures have been used Figure 1: Method overview. Inspired by monocular disparity estimation (a), we tackle slice-to-volume reconstruction (b) by predicting slice motion from a single stack of 2D slices using a fully convolutional network model. ?e intensities of the 2D slices are splatted using the slice motion predicted by our network and the splatting is then interpolated to produce an artifact-free 3D MR reconstruction. 
 (a) Monocular Depth (Disparity) Estimation [38â€“41] (b) Single-Stack Slice-to-Volume Reconstruction (Proposed) 
Splat and  Interpolate 
Standard U-Net 
Splatâ€“Slice U-Net 
 Single View Disparity Map Single Slice Stack Motion Stack 3D Reconstruction  Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI  Sean I. Young* YaÃ«l Balbastre*  Harvard Medical School Harvard Medical School  siyoung@mit.edu ybalbastre@mgh.harvard.edu  Bruce Fischl Polina Golland Juan Eugenio Iglesias  Harvard Medical School MIT Harvard Medical School  fischl@nmr.mgh.harvard.edu polina@csail.mit.edu jei@mit.edu    
*?ese authors contributed equally to this work. 
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11535
 
 to enable accurate non-iterative prediction of an underlying deformation, motion and disparity from a pair of images. By formulating the SVR task as a supervised motion estimation problem, we can reduce the task at hand to a problem that is more readily tackled using a convolutional framework. Lis convolutional approach to SVR bears a certain resemblance to monocular depth estimation, where disparity is predicted from a single view [42]. Lis is illustrated in Figure 1.  In this work, we propose a fully convolutional method for SVR with application to single-stack MRI. We pose the SVR task as a supervised image registration problem and train a convolutional network on paired (slice stack, motion stack) data to predict slice motion directly, producing a 3D volume reconstruction as a result (Figure 2). Our approach requires only a single slice stack for reconstruction, is many orders of magnitude faster than optimization approaches [10â€“18], and reduces error in the predicted motion by 2Ã— compared to the recent transformer-based model [25]. Our contributions are: â€¢ Splatâ€“Slice U-Net Model. We interpret the iterations of classical SVR as a series of splat and slice operations to design a 3D FCN model for fast slice motion prediction. â€¢ Supervised Learning. We formulate a parameter-free loss function to facilitate training with true slice motion for slice motion prediction with sub-voxel accuracy. â€¢ Artifact-Free Reconstruction. We obtain artifact-free 3D reconstructions by splatting slice data and imputing missing intensities via supervised interpolation. 2. Related Work  In fetal imaging, slice-to-volume reconstruction (SVR) is an important inverse problem with reconstruction methods ranging from traditional optimization-based ones [10â€“19] to recent learning-based ones [20â€“25], which we now review.  2.1. Optimization Approaches  Optimization-based SVR [10â€“18] traces its way back to the seminal work of Rousseau et al. [19], who predicted the underlying 3D MRI volume by alternatingly estimating the slice motion and volume from three orthogonal slice stacks and super-resolving the ï¬nal predicted volume. Le method of Jiang et al. [10] is closely related to [19] but adopts a grid search to estimate the slice motion parameters to overcome the non-convexities of the optimization problem. Gholipour et al. [11] and Kuklisova-Murgasova et al. [13] attribute the cause of poor reconstructions to inaccurate slice alignments and propose reconstruction strategies to combat misaligned slices post-hoc. Uus et al. [18] extend [13] to accommodate deformable motion models as would be required, e.g., in the reconstruction of the placenta. While the focus of our work is on brain reconstruction, our supervised learning approach similarly accommodates diï¬€erent motion models (rigid and deformable) by encoding regularity of motion directly into the training motion ï¬elds without additional regularization.   Several SVR approaches extend [19] by reparametrizing slice motion for a more favorable computational complexity or ï¬‚exibility of the slice motion. Kim et al. [12] propose to lower the cost of motion estimation by aligning 1D lines of intersection between 2D slices. Alansary et al. [15] propose to handle deformable slice motion by partitioning slices into square patches and estimating a separate set of rigid motion parameters for each patch. However, this strategy produces a patch-wise linear approximation of the underlying motion and leads to block artifacts in the reconstructed volume. In our work, we represent slice motion as dense motion ï¬elds to enable straight-forward artifact-free 3D reconstruction.  In optimization-based SVR methods, good initialization of the 3D volume and the slice motion can help mitigate the deleterious eï¬€ects of local minima in the objective function on the reconstruction. Kainz et al. [14] propose techniques to automatically select a reference slice stack to improve the initialization of the 3D volume. Tourbier et al. [16] initialize the 3D volume with an age-matched template image, which acts as prior knowledge in the reconstruction framework. By contrast, we task our fully convolutional network model with robustly determining slice motion in the presence of image non-convexities, allowing it to dispense with templates and produce subject- and pathology-speciï¬c reconstructions. 2.2. Learning-Based Approaches  Currently, the majority of machine-learning-based SVR methods [20â€“25] predict the position parameters of slices (9 
Figure 2: Supervised learning. We train our splatâ€“slice U-Net (a) on paired (slice stack, motion stack) examples, which are generated by slicing 3D volumes with randomized slice-wise motion (motion stack). We additionally train a fully convolutional interpolation network (b) on paired (splat volume, 3D volume) examples, where splat volumes are generated by splatting slice stacks with ground truth motion. 
 
 (a) Single-Stack Motion Estimation (b) Supervised Interpolation 
 
Slice Stack 
Splat Volume 
3D Volume 
ğ² 
â„’ 
Interpolator 
ğ± 
 
ğ² 
â„’ 
ğ± 
3D Volume 
Slice Stack 
Motion Stack 
Splatâ€“Slice U-Net 
Slice 
Splat 
11536
 
 numbers) within the underlying MRI volume by using a 2D convolutional neural network (CNN). Le predicted position parameters are then used to warp the slices and recover the underlying MRI volume. Le CNN model of Hou et al. [20] consists of the convolutional layers of VGG-16 pre-trained on ImageNet [44] and a dense connected head to predict the anchor points (AP) of the individual slices. Yeung et al. [24] extend [20] and predict the position parameters of all slices jointly, introducing an inter-slice attention layer between the extracted features and the output to conditionally predict the motion for each slice given the features of other slices.  More recently, a non-convolutional model for SVR was proposed by Xu et al. [25] in an unsupervised setting using a vision transformer model [45â€“47]. Although this produces promising results, it scales quadratically with the number of slices due to inter-slice attention computations and requires hyperparameter tuning to balance diï¬€erent terms in the loss function. By posing the SVR task as a supervised registration problem within a convolutional framework, we signiï¬cantly lessen the complexity of modeling pairwise slice interaction through convolutional weight sharing and obviate the need for hyperparameter optimization in the loss function.  Le problem of regressing slice position (or even motion) against intensity data is highly non-convex due to the image landscape [19]. While there have been eï¬€orts to overcome image non-convexities in learning-based image registration [31, 32], these concerns are not addressed in learning-based pose prediction works [24, 25] and are exacerbated further by separate processing of the individual slices [20]. Posing SVR as image registration allows us to appeal to e.g. [31, 32] to address the fundamentally non-convex nature of SVR. 2.3. Pre- and Post-Processing  SVR techniques typically involve slice pre-processing as a ï¬rst step to reject slices exhibiting considerable intra-slice motion and segment relevant, e.g., the brain, structures from the scan. Anquez et al. [48] propose a method to skull-strip fetal volumes, which can be used also to mask out non-brain structures in MR slices for small inter-slice motion. Tourbier et al. [16] propose to localize brain regions in MR slices by block-matching an age-matched template to the slices then registering the matched regions back to the template. Kainz et al. [49] localize brain regions based on rotation-invariant descriptors for labeling brain. Keraudren et al. [50, 51] use a dense SIFT and random forest classiï¬cation for voxel-level segmentation. Some shortcomings of these methods include high computational complexity [16, 48] as well as the need to handcraft features [50, 51] for segmentation.   Learning-based approaches to fetal segmentation include the methods of Rajchl et al. [52] and Salehi et al. [53], both of which use FCN models [54] to predict foreground masks directly on slices. For post-processing of the reconstructed 3D volume, Xu et al. [55] propose neural volume rendering similar to NeRF [56] to super-resolve reconstructions at an arbitrary resolution. In this work, we additionally train an interpolation network to postprocess the raw reconstructions and interpolate over regions of missing intensities that may appear in the single-stack reconstruction case, producing an artifact-free reconstruction for other downstream tasks. 3. Mathematical Preliminaries 3.1. Slice Acquisition Model  In high-resolution 3D MRI of subjects exhibiting severe uncontrollable motion, fast slice acquisition sequences such as single shot fast spin echo are used to â€œfreezeâ€ the motion in-plane, substantially mitigating motion artifacts compared to multi-shot methods [57]. SVR procedures aim to align an acquired stack of slices thereby removing the remaining (in-plane and through-plane) motion across slices.  Let us denote the coordinates inside the MRI scanner by (ğ‘¥,ğ‘¦,ğ‘§) and assume a stack ğ‘“ of 2D slices is acquired along the ğ‘§ axis with spacing ğ‘  from an underlying volume ğ‘£. Le model for the acquisition of the ğ‘˜th MR slice is [11, 19]:  ğ‘“ğ‘˜(ğ‘¥,ğ‘¦)=ğœ€+âˆ«(ğ‘£âˆ˜ğ‘¢ğ‘˜)(ğ‘¥,ğ‘¦,ğ‘§)â„(ğ‘ ğ‘˜âˆ’ğ‘§) dğ‘§âˆâˆ’âˆ  (1) for ğ‘˜=1,...,ğ¾, where ğ‘¢ğ‘˜â€†:(ğ‘¥,ğ‘¦,ğ‘§)â†¦(ğ‘¥,ğ‘¦,ğ‘§)â€² denotes an unobserved slice-varying rigid spatial subject motion, â„ denotes the point spread function of slice acquisition, and ğœ€ captures all MR-induced errors such as noise, bias, etc.  To discretize model (1), suppose ğ‘£ is sampled on a cubic lattice of ğ‘3 points and ğ‘“ğ‘˜ on a 2D lattice of ğ‘2 points. We can write the discretized model (up to random noise) as  ğŸ=[ğŸ1;...;ğŸğ¾]âˆˆâ„ğ‘2ğ¾,   ğŸğ‘˜=ğ‡ğ‘˜ğ”ğ‘˜ğ¯âˆˆâ„ğ‘2, (2) in which ğ”ğ‘˜ and ğ‡ğ‘˜ represent discretizations of (âˆ˜ğ‘¢ğ‘˜) and â„(ğ‘ ğ‘˜âˆ’ â‹…), respectively. In the small motion regime, we can further appeal to Taylorâ€™s approximation and deï¬ne  ğ”ğ‘˜ğ¯=ğ¯+[ğğ¢ğšğ (ğ¯ğ‘¥),ğğ¢ğšğ (ğ¯ğ‘¦),ğğ¢ğšğ (ğ¯ğ‘§)]ğ’ğ®ğ‘˜, (3) in which ğ’ğ®ğ‘˜ represents the expansion of the coeï¬ƒcients of motion ğ®ğ‘˜ in a basis ğ’ and ğ¯{ğ‘¥,ğ‘¦,ğ‘§} are the respective spatial derivatives of the 3D volume. We accommodate deformable motion models by setting ğ’ to the identity matrix, in which case, ğ®ğ‘˜ denotes the vectorization of a 3D motion ï¬eld. 3.2. Classical SVR Methods  For brevity, let us write ğ”=[ğ‡1ğ”1;...;ğ‡ğ¾ğ”ğ¾] and the associated motion parameters as ğ®=[ğ®1;...;ğ®ğ¾]. We can write the SVR problem with three orthogonal stacks as  minimize   ğ·(ğ®{1,2,3},ğ¯{1,2,3})=â€–ğ”1ğ¯1âˆ’ğŸ1â€–22            +â€–ğ”2ğ¯2âˆ’ğŸ2â€–22+â€–ğ”3ğ¯3âˆ’ğŸ3â€–22 subject to  ğ¯1âˆ’ğ¯2â€Š=â€Šğ¯2âˆ’ğ¯3â€Š=â€Šğ¯3âˆ’ğ¯1â€Š=â€ŠğŸ (4) in which ğ®{1,2,3} denote the parameters of motion in each of  the stacks. Contrast-invariant loss functions such as mutual 
11537
 
 information loss [58] are also popularly used in place of the quadratic one shown here for simplicity. Here, we introduce optimization variables ğ¯{1,2,3} with equality constraints as opposed to a single variable ğ¯ to facilitate solving (4) using alternating optimization strategies similarly to [17, 19].  A simple alternating optimization strategy which does not introduce dual variables is to relax the equality constraints to the quadratic penalty  ğ‘…(ğ¯{1,2,3})=â€–[ğ¯1,ğ¯2,ğ¯3]âˆ’[ğ¯2,ğ¯3,ğ¯1]â€–22 (5) and optimize the relaxation  ğ½(â‹…)=ğ·(ğ®{1,2,3},ğ¯{1,2,3})+ğœ†ğ‘…(ğ¯{1,2,3}), (6) in which ğœ† is a parameter controlling the relative weights of the data ï¬delity term ğ· and the coupling term ğ‘…. Objective (6) in fact corresponds to the objective implicitly optimized by Rousseau et al. [19] when the mutual information loss is used in the data ï¬delity term ğ·. Optimizing (6) using block coordinate descent yields updates for ğ¯1,ğ®1,ğ¯2,ğ®2, ğ¯3 and ğ®3, where the 3D reconstructions (ğ‘£-update) and the motion parameters (ğ‘¢-update) of each stack are updated in turn with the other two reconstructions ï¬xed at each turn.  In the ğ‘›th iteration of the block coordinate descent, the ğ‘£-update step for a given stack amounts to computing  ğ¯ğ‘›â†(ğ”ğ‘›âˆ—ğ”ğ‘›+2ğœ†ğˆ)âˆ’1 (ğ”ğ‘›âˆ—ğŸğ‘›+2ğœ†ğ¯Ì…Ì…Ì…Ì…ğ‘›), (7) in which ğ¯ğ‘› and ğŸğ‘› denote the reconstruction and 2D slices of the ğ‘›%3rd stack, respectively, and ğ¯Ì…Ì…Ì…Ì…ğ‘› denotes the average reconstruction from the two stacks orthogonal to ğ¯ğ‘›. We can interpret this update as a reconstruction of a new 3D volume from a weighted sum of the last average 3D reconstruction and warped slices, followed by Wiener-deconvolution. Lis interpretation of the ğ‘£-update will guide our construction of a fully convolutional network later in Sec 3.3.   Algebraically, (ğ”,ğ”âˆ—) deï¬ne an adjoint pair of warping operators, which we will refer to as â€œslicingâ€ and â€œsplattingâ€ respectively [59]. In the case where there is injective motion relating a slice stack ğŸğ‘› to a 3D volume ğ¯, we can obtain ğŸğ‘› by â€œslicingâ€ ğ¯ with ğ”ğ‘› and obtain ğ¯ by â€œsplattingâ€ ğŸğ‘› with ğ”ğ‘›âˆ—. Figure 3 illustrates the two adjoint warping operators in the discrete case when multi-linear interpolation weights are used for fractional slicing and splatting.  Le ğ‘¢-update for the motion of the ğ‘˜th slice in a stack is  ğ®ğ‘˜ğ‘›â†(ğ’âˆ—ğ•ğ‘˜ğ‘›âˆ—ğ•ğ‘˜ğ‘›ğ’)âˆ’1ğ’âˆ—ğ•ğ‘˜ğ‘›âˆ—(ğ‡ğ‘˜ğ¯ğ‘›âˆ’ğŸğ‘˜ğ‘›), (8) in which ğ•ğ‘˜ğ‘›=ğ‡ğ‘˜[ğğ¢ğšğ (ğ¯ğ‘¥ğ‘›),ğğ¢ğšğ (ğ¯ğ‘¦ğ‘›),ğğ¢ğšğ (ğ¯ğ‘§ğ‘›)], with the ğ¯{ğ‘¥,ğ‘¦,ğ‘§}ğ‘› denoting the respective spatial derivatives of the given 3D volume reconstructed in the (ğ‘›âˆ’1)th iteration.  In practice, subject motion in an MR slice likely exceeds one voxel in magnitude but the linear model of warping (3) stays valid only if the slice is ï¬rst blurred at the scale of the motion. Due to this, motion is often estimated on a pyramid in a coarse-to-ï¬ne manner, using the motion ï¬eld estimated at coarser levels to initialize motion at ï¬ner ones [60]. MRI acquisitions also suï¬€er from a bias ï¬eld due to non-uniform main magnetic ï¬eld and radiofrequency coils, introducing a global intensity shift between the unknown volume and the slices. A key challenge in optimization-based SVR methods is determining, at each scale, motion in the presence of bias and extreme slice artifacts. Mistaking either for motion can quickly lead to pitfalls of local minima in the optimization objective especially at coarse scales. However, it is diï¬ƒcult to hand-craft a reconstruction pipeline which can overcome image non-convexities due to sheer numbers of acquisition-speciï¬c design choices to be made (e.g., non-linear ï¬lters). 4. Fully Convolutional SVR  Using a FCN model for SVR enables us to bypass hand-crafted acquisition-speciï¬c reconstruction pipelines as well as numerical optimization. Rather than formulating SVR as the prediction of absolute slice coordinates in 3D space as is typically done in other learning-based methods [28â€“32], we cast SVR as the registration of some unobserved 3D volume to an observed stack of 2D slices. We train a FCN model to predict motion relating the slices to a 3D volume given only the slices as input, producing a 3D volume as a by-product of registration. Conceptually, this is similar to the problem of monocular depth estimation [40â€“43], where oneâ€™s goal is to predict disparity (motion along the epipolar line) relating a single 2D image to the depth of the underlying 3D scene. 4.1. Neural Network Architecture  CNN architectures such as those used in semantic image segmentation can produce suboptimal outcomes for motion estimation tasks if the motion is large or images exhibit ï¬ne texture [33â€“39]. While fully convolutional networks such as the U-Net [54] are endowed with a multi-scale architecture and could theoretically estimate motion in a coarse-to-ï¬ne manner, using convolution kernels to parameterize warping 
Figure 3: Slicing and splatting. In slicing (top row), the endpoints of motion vectors deï¬ne coordinate locations in a moving image to pull data from and construct an image. In splatting (bottom), the same motion vector endpoints deï¬ne coordinate locations in a ï¬xed image to push data to. Multi-linear weights are used for fractional splatting and slicing, guaranteeing diï¬€erentiability w.r.t grid data. 
 Moving Image Motion Field Slicing Sliced Image  
Â¼  
Â½   
Â¾  
0 
0 
1/
0 
Â¼ s 
Â¼  
Â¼  
Â¼  
Â½  
Â¼  
Â¾    
Â¼  
Â¼ s 
Â¼  
Â¼  
Â¼  
Â¼  
Â¼  
Â¼  
Â½   
Â¼  
Â¼  
Â¾  
1Â¼  
1Â½   
Â¼  
Â¼  
 Fixed Image Motion Field Splatting Splat Image  
Slice (Warp) 
Splat (WarpT) 
11538
 
 (space-varying transforms) is inherently ineï¬ƒcient, both in terms of parametrization and computation; see, e.g., [33, 39].  Our network model (Figure 4) bears similarities to those used in pairwise image registration [28â€“32], which predict image motion by processing the image pair across multiple levels of convolutions. Lese network models are typically equipped with warping layers between adjacent levels such that only the residual motion needs to be estimated at every level. Notionally, the slice stack assumes the role of a ï¬xed image while the 3D volume, which we seek to estimate, can be seen as a moving image. As discussed in Sec. 3.2, we can obtain the features of the 2D slices (ï¬xed image) by slicing the features of the 3D volume with the estimated motion. To obtain the features of the 3D volume, we splat the features of the slice stack using the same motion estimate.  Note that in our case, only the ï¬xed image (slice stack) is given, so the downward transform needs to process only the ï¬xed image. Le upward transform synthesizes ï¬xed image features by unpooling lower-resolution ones, concatenating them with the skip features and processing them via a series of convolutions. Le moving image (or 3D volume) features are synthesized in parallel by unpooling their low-resolution counterparts, concatenating them with the splattings of skip features and processing them via convolutions. Le moving image features are ï¬nally sliced, concatenated with the ï¬xed image features and processed using a series of convolutions to output residual slice motion, which is added to the lower-resolution motion that was used during splatting and slicing.  In terms of implementation, both the slice stack and the 3D volume features are seen as (B,C,D,H,W) tensors, where the D dimension acts as the slicing dimension in the case of slice stack features. 3D volume features are processed using convolution and pooling kernels of shape (3,3,3) and (2,2,2) respectively, while slice features are processed using kernels of shapes (1,3,3) and (1,2,2), respectively, so that each slice can be processed independently. Within the residual motion extraction stages, the ï¬rst convolutions are performed with kernels of shape (4,3,3) and strides of (4,1,1) since slices are internally represented as 3D slabs with thickness of 4 voxels and the motion needs to be regularized across each slab. We handle diï¬€erent slice spacings in real data by scaling the 2D input slices to a 4:1 slab thickness to voxel spacing ratio.  4.2. Rigid Motion-Compensating Loss  Inexact positioning of subjects in MRI scanners typically introduces a global subject oï¬€set (rotations and translations) in the acquired slices. For reconstruction, however, we need only recover the motion of the slices relatively to each other ignoring global shifts. To ensure that we penalize prediction errors only in the relative slice motions, we compensate for any global rigid motion shift that may exist in our predicted motion stack before computing the training loss against the prediction target. Assuming an MSE training objective, we can express our motion-compensating training loss between the predicted ğ® and ground truth ğ²âˆˆâ„ğ‘Ã—3 motion ï¬elds as  ğ“›(ğ®,ğ²)=min[ğ‘,ğ­]â€–ğ®+ğ©âˆ’(ğ²+ğ©)ğ‘âˆ’ğ­â€–ğ¹2, (9) in which ğ‘ represents a 3Ã—3 rotation matrix, ğ­ represents a 1Ã—3 translation vector, and ğ©âˆˆâ„ğ‘Ã—3 is a matrix of voxel coordinates. In words, this loss ï¬rst performs a global rigid alignment between the point clouds ğ®+ğ© and ğ²+ğ©, such that only the non-rigid motion component between the two point clouds is penalized. Loss ğ“› reduces to the usual MSE loss in the absence of global rotation ğ‘ and translation ğ­.  To solve the optimization problem (9), we ï¬rst minimize the unconstrained version of problem (9) over the space of all [ğ‘,ğ­]âˆˆâ„3Ã—4 to obtain a minimizer [ğ‘â€²,ğ­â‹†], then factor ğ‘â€² into its rotation and shear matrices as ğ‘â€²=ğ‘â‹†ğ’â‹†. From the singular value decomposition ğ‘â€²=ğ”ğšºğ•âˆ—, we see that  ğ‘â‹†=ğ”sign(ğšº)ğ•âˆ—,ğ’â‹†=ğ•abs(ğšº)ğ•âˆ—, (10) a factorization known as the polar decomposition [61]. Le resulting rotation matrix ğ‘â‹† is the minimum mean squared- error approximation of ğ‘â€², and consequently, the solution of the original rigid motion-constrained problem (9) is given by the pair [ğ‘â‹†,ğ­â‹†]. Le loss ğ“›(ğ®,ğ²) is then backpropagated and the weights of our SVR network model updated. Figure 
Figure 4: Splatâ€“Slice U-Net. ?e stack of 2D slices goes through 2D feature extraction (downward path) and reconstruction (upward path) with 2D skip features. 3D volume features are reconstructed by splatting 2D skip features with previous motion to form 3D skip features. At each level, the sliced 3D feature volume and the slice feature stack are convolved jointly to extract residual slice motion. 
 
Splat-Concat 
Concat 
1 
24 
32 
32 
48 
48 
64 
96 
48 
64 
32 
48 
24 
3 
3 
64 
3 
Splat-Concat 
Concat 
3 
Ã… 
Ã… 
Ã… 
 
 
2D Feat 
 
3D Feat 
 
Motion  
Conv 
Add 
Ã… 
Slice-Conv  
Figure 5: Rigid motion-compensated loss. V alidation MSE and EPE (end-point error) [33] of the predicted motion shown. Our loss  (blue), which compensates for oï¬€sets in global rigid motion, trains faster and attains higher ï¬nal prediction accuracy than the MSE loss (orange) and the loss that compensates for translations only (gray).  
    Rigid-Comp     No Comp    Trans-Comp 
Motion MSE (voxels) 
Motion EPE (voxels) 
    Rigid-Comp     No Comp    Trans-Comp 
Epochs 
11539
 
 5 plots the validation curves for our network model trained using motion-compensated and regular losses, showing that our rigid compensating loss is key to improving prediction. 4.3. Interpolating Reconstructions  In general, a 3D volume reconstructed from a single slice stack can contain holes if regions of the underlying volume are missed in all slice acquisitions due to subject motion. In comparison with natural images, MR scans typically exhibit a limited diversity in imagery and have a strong prior on the distribution of underlying 3D volumes. Lis justiï¬es the use of interpolation techniques to ï¬ll holes in reconstructions to satisfy regularity constraints (e.g., on pathology). We turn to a fully convolutional interpolation model trained in advance on (true reconstruction, volume) pairs. True reconstructions (with holes) can be obtained readily at train time by warping slice stacks with ground truth slice motion. While the search for the best interpolation network model is not the focus of this work, we ï¬nd that a standard U-Net model trained with true reconstructions as the target and using the mean squared error loss produces good results. Figure 2 (right) shows the training procedure for our fully convolutional interpolator.  5. Experimental Results  We run extensive experiments on both adult MRI stacks (for which ground truth MR volumes are available) and fetal ones, for which only reference volumes are given. For each experiment, we train a SVR network in a supervised fashion on paired (slice stack, motion stack) data and an interpolator network on paired (sliceâ€“splat volume, underlying volume) data (Sec. 4.3). Le data augmentation hyperparameters and training details are given in Appendix A. We use the mean squared error (MSE) and the average end-point error (EPE) [33] to measure prediction error; see Appendix A. 5.1. Single-Stack SVR of Adult Brains  SVR experiments on synthetic sliced MRI volumes can reveal qualitative and quantitative characteristics of a SVR method by facilitating comparison across the reconstructed and the ground truth 3D MR volumes. We curate 1100 adult brain MR scans from ABIDE, -2 [62], ADHD [63], COBRE [64], GSP [65], MCIC [66], OASIS [67], PPMI [68], UKB [69] and B39 [70], and split the scans into 1000 training and 100 validation examples. All scans have a 2563 volume with 1mm3 resolution. Le scans are aligned to the Talairach atlas [71] to facilitate slicing along the sagittal, axial and coronal directions; see Appendix A for details on training and sample generation.  Slicing Directions and Reconstruction. Single-stack SVR can achieve diï¬€erent reconstruction accuracy depending on the direction of slicing (sagittal, axial and coronal). To study the eï¬€ect of slice direction on the accuracy of the predicted motion, we train a separate network for each slice direction and plot in Figure 6, the validation MSE and Endpoint Error of the motion predicted by each network. Le ï¬nal validation error attained is the lowest for sagittal stacks at 0.89 (MSE) and the highest for axial stacks at 1.33 (MSE). Such a slight increase in the prediction error for axial stacks most likely stems from fewer â€œcornerâ€ features to track and align in axial slices, leading to a less precise alignment. Motion Prediction Error. We plot distributions of the per- subject errors of the predicted motion stacks across 500 held-out scans in Figure 7. As expected from the validation error curves seen in Figure 6, sagittal predictions have the lowest median motion MSE and the tightest interquartile range. On average, our predicted motion is accurate to less than a voxel although the PPMI dataset exhibits slightly higher errors. Reconstruction and Interpolation. Figure 8 visualizes the 3D reconstructions obtained using the predicted motion. We align all reconstructions back to the Talairach atlas [71] for comparison across diï¬€erent reconstructions. We also include reconstructions with the missing intensity data interpolated using our interpolator described in Sec. 4.3. Le splat results show that our network predicts linear motion ï¬elds without the need for additional linear projections. Le interpolation removes holes as well as slicing artifacts that interfere with downstream tasks such as brain morphometry. Larger holes are seen when splattings are viewed along the slicing axis. 
Figure 6: Accuracy of predicted motion. Slicing direction has an impact on the accuracy of predicted motion. We plot the validation MSE (left), and EPE (end-point error, right) of the predicted motion on adult brain slices for sagittal, axial, and coronal acquisitions. 
    Sagittal     Axial    Coronal 
Motion MSE (voxels) 
Motion EPE (voxels) 
    Sagittal     Axial    Coronal 
Epochs 
Motion MSE (voxels) 
 All AB2 AB ADH B39 CO GSP MC OAS PP UKB 
Figure 7: Motion accuracy across datasets. We plot the MSE of the motion stacks predicted on 500 held-out slice stacks (1mm3, see text). Motion prediction is accurate to < 1mm on average. Sagittal and coronal motion predictions tend to be more accurate than axial. 
 
 
Sagittal 
 
Axial 
 
Coronal 
11540
 
 5.2. Single-Stack SVR of Fetal Brains  Here, we curate 98 T2w fetal brain atlases and reference reconstructions across the CRL Fetal Atlases [72] and FeTA [73] (0.8mm3, aligned to age-matched CRL atlases) and split them into 86 training and 12 validation FeTA examples. We use two real T2w slice stacks from MIAL [16] for test. In the case of fetal SVR, we train a single splatâ€“slice network on all possible slicing directions by applying a random rotation (Euler angles between Â±180Âº) to our training examples. We compare our method with state-of-the-art SVRnet [20] and SV oRT [25]; see Appendix A. Optimization-based methods do not work on single stacks and are not compared against. 3D Reconstruction and Interpolation. Figure 9 visualizes the reconstructions obtained using our predicted motion. We align FeTA reconstructions to their gestational age-matched atlas [72] for comparison across diï¬€erent methods. SVRnet [20] completely fails to align slices in most of the cases and SV oRT (v2) introduces large slice misalignments, leading to suboptimal 3D reconstruction with apparent distortion. (Le SV oRT and SVRnet reconstructions are processed using our interpolation.) Our reconstructions are more faithful to the underlying 3D volumes up to ambiguities from slicing. Le last two rows visualize reconstruction of the two real MIAL acquisitions for which no ground truth exists and motion is smaller. Unlike SVRnet, the proposed method generalizes to real acquisitions even when the network model is trained on synthetic slice stacks with diï¬€erent motion and acquisition parameters, e.g., PSF. See Appendix B for more examples. Quantitative Results. Table 1 lists the prediction accuracy of diï¬€erent methods on validation subjects with 4 folds. In addition to motion EPE, we report the slice PSNR, obtained by slicing the true volume with the predicted motion, and the PSNR of the ï¬nal reconstruction. Le three-stack results are from [25], which lists the anchor point errors (APE). In the single-stack case, our method reduces error in the predicted motion by 77.6% and 44.6% relative to the SVRnet [20] and SV oRT (v2) predictions, respectively. Le slice and volume PSNR have also improved but unlike the EPE, these metrics depend on the PSF of acquisition and reconstruction and do not deï¬nitively measure alignment accuracy. ADHD200 8628223       ABIDE 50975       MCIC A00036476       Buckner39 990921        (a) Slice Stack (b) Splat (Ours, True Motion) (c) Interpolated (Ours, True Motion) (d) True Volume Figure 8: SVR of adult brain scans. We visualize our SVR results on sagittal (rows 1â€“2), axial (row 3), and coronal (last row) slice stacks synthesized using random slice motion (a). Using the motion stack predicted by our network, we splat slice data to reconstruct the underlying 3D volume (b). Using our pre-trained interpolator, we then interpolate the missing intensities (holes) in our reconstruction (c). ?e result is similar to the true 3D volume (d). We additionally visualize in (b) and (c) the splat and interpolated results when the true motion is used.  
 Sagittal Acquisition EPE: 1.66mm EPE: 0.00mm  Sagittal View 
 Axial Acquisition EPE: 2.20mm EPE: 0.00mm  Coronal View   
 Sagittal Acquisition EPE: 2.04mm EPE: 0.00mm  Axial View  
 Coronal Acquisition EPE: 3.36mm EPE: 0.00mm  Sagittal View   
11541
 
 6. Discussion Limitations. Our fetal SVR network is currently trained on automatically segmented brain slices. Training with original unsegmented slice stacks can improve the robustness of our approach against imperfectly generated segmentations. Future Work. Given that our approach predicts dense slice motion for reconstruction, we plan to extend our approach to problems where the acquired slices can undergo deformable motionâ€”e.g. in fetal torso or placental reconstruction [18]â€”and cannot be tackled using rigid motion SVR. Also, better interpolation networks, such as those that are equivariant to rigid motion, can further improve reconstruction; see [74].  7. Conclusion In brain imaging, slice-to-volume reconstruction (SVR) is an important computational technique for the imaging of subjects whose motion cannot easily be controlled. Yet, the majority of SVR techniques previously proposed do not reap the beneï¬ts of FCN models that have gradually become the mainstay in closely related tasks, such as image registration and segmentation. Le few learning-based SVR techniques resort to pretrained ImageNet classiï¬cation and even vision transformer models. Not only does our work ï¬ll the gap in the SVR method development timeline but it also shows that a FCN model can produce state-of-the-art SVR outcomes. Acknowledgments. SIY thanks Margherita Firenze for her thought-provoking questions. Primary support given by the NIH grants K99AG081493 and RF1MH123195. Supported by NIH R01AG064027, R21AG082082 and P41EB030006. FeTA Sub 073       FeTA Sub 053       MIAL001 Run 2       MIAL001 Run 1        (a) Slice Stack (b) SVRnetâ€  [20] (c) SVoRTv2 [25] (c) Splat (Ours) (d) Interp (Ours) (f) True Volume  Figure 9: Single-stack fetal SVR. We visualize the SVR results on validation subjects from the FeTA dataset [73] and two real acquisitions  from MIAL [16]. Zoomed in 4x for better visibility. Our results closely resemble the ground truth volumes while SV oRTv2 and SVRnet reconstructions (with our interpolation) exhibit spatial distortion from inaccurate slice alignment. â€ Our implementation; see Appendix A.  
 Random Acquisition EPE: 8.69mm EPE: 3.70mm EPE: 1.49mm Pathological 
 Sagittal Acquisition EPE: Unknown  EPE: Unknown EPE: Unknown  
 Random Acquisition EPE: 5.72mm EPE: 2.61mm EPE: 1.45mm Neurotypical 
 Sagittal Acquisition EPE: Unknown  EPE: Unknown EPE: Unknown   
Unknown 
Unknown 
 Method APE/Motion  EPE (mm) Slice  PSNR (dB) Vo l u m e  PSNR (dB) Time (sec) 3 Stacks SVRnet* [20] 12.82Â±5.69 20.53Â±1.62 19.54Â±1.52 â€“ PlaneInV ol* [24] 12.49Â±6.73 19.96Â±1.73 18.98Â±1.62 â€“ SV oRT* [25] 4.35Â±0.90 25.26Â±1.86 23.32Â±1.42 â€“ 1 Stack SVRnetâ€  [20] 8.08Â±2.35 13.46Â±1.56 16.03Â±1.28 0.011s SV oRT (v2) [25] 3.27Â±0.71 20.77Â±1.28 18.49Â±1.63 0.142s Proposed (Ours) 1.81Â±0.40 23.69Â±1.39 23.43Â±1.40 0.224s Table 1: Validation accuracy. We list the motion and 3D volume reconstruction accuracy of diï¬€erent methods. *Results taken from [25]. Timed on RTX8000. â€ Our implementation; see Appendix A. 
11542
 
 References  [1] Mitsuhiro Nishida et al. Detailed semiautomated MRI based morphometry of the neonatal brain: Preliminary results. NeuroImage, 32(3):1041â€“1049, 2006.  [2] Chao J. Liu et al. Quantiï¬cation of volumetric morphometry and optical property in the cortex of human cerebellum at micrometer resolution. NeuroImage, 244:118627, 2021.  [3] Maxwell L. Elliott et al. Brain morphometry in older adults with and without dementia using extremely rapid structural scans. NeuroImage, 276:120173, 2023.  [4] Bruce Fischl. FreeSurfer. NeuroImage, 62(2):774â€“781, 2012.  [5] Amy Zhao, Guha Balakrishnan, Fredo Durand, John V . Guttag, and Adrian V . Dalca. Data augmentation using learned transformations for one-shot medical image segmentation. In Proc. CVPR, 2019.  [6] Lilla ZÃ¶llei, Juan Eugenio Iglesias, Yangming Ou, P. Ellen Grant, and Bruce Fischl. Infant FreeSurfer: An automated segmentation and surface extraction pipeline for T1-weighted neuroimaging data of infants 0â€“2 years. NeuroImage, 218:116946, 2020.  [7] Alan C. Evans, Andrew L. Janke, D. Louis Collins, and Sylvain Baillet. Brain templates and atlases. NeuroImage, 62(2):911â€“922, 2012.  [8] Shadab Khan et al. Fetal brain growth portrayed by a spatiotemporal diï¬€usion tensor MRI atlas computed from in utero images. NeuroImage, 185:593â€“608, 2019.  [9] AdriÃ  Casamitjana, and Juan Eugenio Iglesias. High-resolution atlasing and segmentation of the subcortex: Review and perspective on challenges and opportunities created by machine learning. NeuroImage, 263:119616, 2022.  [10] Shuzhou Jiang, Hui Xue, Alan Glover, Mary Rutherford, Daniel Rueckert, and Joseph V . Hajnal. MRI of moving subjects using multislice snapshot images with volume reconstruction (SVR): Application to fetal, neonatal, and adult brain studies. IEEE Trans. Med. Imaging, 26(7):967â€“980, 2007.  [11] Ali Gholipour, Judy A. Estroï¬€, and Simon K. Warï¬eld. Robust super-resolution volume reconstruction from slice acquisitions: Application to fetal brain MRI. IEEE Trans. Med. Imaging, 29(10):1739â€“1758, 2010.  [12] Kio Kim, Piotr A. Habas, Francois Rousseau, Orit A. Glenn, Anthony J. Barkovich, and Colin Studholme. Intersection based motion correction of multislice MRI for 3-D in utero fetal brain image formation. IEEE Trans. Med. Imaging, 29(1):146â€“158, 2010.  [13] Maria Kuklisova-Murgasova, Gerardine Quaghebeur, Mary A. Rutherford, Joseph V . Hajnal, and Julia A. Schnabel. Reconstruction of fetal brain MRI with intensity matching and complete outlier removal. Med. Image Anal., 16(8):1550â€“1564, 2012.  [14] Bernhard Kainz et al. Fast volume reconstruction from motion corrupted stacks of 2D slices. IEEE Trans. Med. Imaging, 34(9):1901â€“1913, 2015.  [15] Amir Alansary et al. PVR: Patch-to-Vo l u m e  R e c o n s t r u c t i o n  for large area motion correction of fetal MRI. IEEE Trans. Med. Imaging, 36(10):2031â€“2044, 2017.  [16] SÃ©bastien Tourbier et al. Automated template-based brain localization and extraction for fetal brain MRI reconstruction. NeuroImage, 155:460â€“472, 2017.  [17] Michael Ebner et al. An automated framework for localization, segmentation and super-resolution reconstruction of fetal brain MRI. NeuroImage, 206:116324, 2020.  [18] Alena Uus, Tong Zhang, Laurence H. Jackson, ?omas A. Roberts, Mary A. Rutherford, Joseph V . Hajnal, and Maria Deprez. Deformable slice-to-volume registration for motion correction of fetal body and placenta MRI. IEEE Trans. Med. Imaging, 39(9):2750â€“2759, 2020.  [19] Francois Rousseau, Orit A. Glenn, Bistra Iordanova, Claudia Rodriguez-Carranza, Daniel B. Vigneron, James A. Barkovich, and Colin Studholme. Registration-based approach for reconstruction of high-resolution in utero fetal MR brain images. Acad. Radiol., 13(9):1072â€“1081, 2006.  [20] Benjamin Hou et al. 3-D Reconstruction in canonical co-ordinate space from arbitrarily oriented 2-D Images. IEEE Trans. Med. Imaging, 37(8):1737â€“1750, 2018.  [21] Benjamin Hou et al. Computing CNN loss and gradients for pose estimation with Riemannian geometry. In Proc. MICCAI, 2018.  [22] Seyed Sadegh Mohseni Salehi, Shadab Khan, Deniz Erdogmus, and Ali Gholipour. Real-time deep pose estimation with geodesic loss for image-to-template rigid registration. IEEE Trans. Med. Imaging, 38(2):470â€“481, 2019.  [23] Yuchen Pei, Lisheng Wang, Fenqiang Zhao, Tao Zhong, Lufan Liao, Dinggang Shen, and Gang Li. Anatomy-guided convolutional neural network for motion correction in fetal brain MRI. In Proc. MLMI, 2020.  [24] Pak-Hei Yeung, Moska Aliasi, Aris T. Papageorghiou, Monique Haak, Weidi Xie, and Ana I. L. Namburete. Learning to map 2D ultrasound images into 3D space with minimal human annotation. Med. Image Anal., 70:101998, 2021.  [25] Junshen Xu, Daniel Moyer, P. Ellen Grant, Polina Golland, Juan Eugenio Iglesias, and Elfar Adalsteinsson. SV oRT: Iterative transformer for slice-to-volume registration in fetal brain MRI. In Proc. MICCAI, 2022.  [26] Elise Turk et al. Functional connectome of the fetal brain. J. Neurosci., 39(49):9716â€“9724, 2019.  [27] Daniel Sobotka et al. Motion correction and volumetric reconstruction for fetal functional magnetic resonance imaging data. NeuroImage, 255:119213, 2022.  [28] Xiaohuan Cao, Jianhua Yang, Jun Zhang, Dong Nie, Minjeong Kim, Qian Wang, and Dinggang Shen. Deformable image registration based on similarity-steered CNN regression. In Proc. MICCAI, 2017.  [29] Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, and Adrian V . Dalca. An unsupervised learning model for deformable medical image registration. In Proc. CVPR, 2018.  [30] Tony C. W. Mok, and Albert C. S. Chung. Fast symmetric diï¬€eomorphic image registration with convolutional neural networks. In Proc. CVPR, 2020.  [31] Sean I. Young, YaÃ«l Balbastre, Adrian V . Dalca, William M. 
11543
 
 Wells, Juan Eugenio Iglesias, and Bruce Fischl. SuperWarp: Supervised learning and warping on U-Net for invariant subvoxel-precise registration. In Proc. WBIR, 2022.  [32] Miao Kang, Xiaojun Hu, Weilin Huang, Matthew R. Scott, and Mauricio Reyes. Dual-stream pyramid registration network. Med. Image Anal., 78:102379, 2022.  [33] Alexey Dosovitskiy et al. FlowNet: Learning Optical Flow With Convolutional Networks. In Proc. ICCV, 2015.  [34] Jason J. Yu, Adam W. Harley, and Konstantinos G. Derpanis. Back to basics: Unsupervised learning of optical ï¬‚ow via brightness constancy and motion smoothness. In Proc. ECCV, 2016.  [35] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and ?omas Brox. FlowNet 2.0: Evolution of optical ï¬‚ow estimation with deep networks. In Proc. CVPR, 2017.  [36] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: CNNs for optical ï¬‚ow using pyramid, warping, and cost volume. In Proc. CVPR, 2018.  [37] Anurag Ranjan, and Michael J. Black. Optical ï¬‚ow estimation using a spatial pyramid network. In Proc. CVPR, 2017.  [38] Pengpeng Liu, Michael Lyu, Irwin King, and Jia Xu. SelFlow: Self-supervised learning of optical ï¬‚ow. In Proc. CVPR, 2019.  [39] Junhwa Hur, and Stefan Roth. Iterative residual reï¬nement for joint optical ï¬‚ow and occlusion estimation. In Proc. CVPR, 2019.  [40] Fayao Liu, Chunhua Shen, and Guosheng Lin. Deep convolutional neural ï¬elds for depth estimation from a single image. In Proc. CVPR, 2015.  [41] Jae-Han Lee, and Chang-Su Kim. Monocular depth estimation using relative depth maps. In Proc. CVPR, 2019.  [42] Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. Digging into self-supervised monocular depth estimation. In Proc. ICCV, 2019.  [43] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive bins. In Proc. CVPR, 2021.  [44] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: a large-scale hierarchical image database. In Proc. CVPR, 2009.  [45] Alexey Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proc. ICLR, 2020.  [46] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario LuÄiÄ‡, and Cordelia Schmid. ViViT: A video vision transformer. In Proc. ICCV, 2021.  [47] Ze Liu et al. Swin Transformer: Hierarchical vision transformer using shifted windows. In Proc. ICCV, 2021.  [48] Jeremie Anquez, Elsa D. Angelini, and Isabelle Bloch. Automatic segmentation of head structures on fetal MRI. In Proc. ISBI, 2009.  [49] Bernhard Kainz, Kevin Keraudren, Vanessa Kyriakopoulou, Mary Rutherford, Joseph V . Hajnal, and Daniel Rueckert. Fast fully automatic brain detection in fetal MRI using dense rotation invariant image descriptors. In Proc. ISBI, 2014.  [50] Kevin Keraudren, Vanessa Kyriakopoulou, Mary Rutherford, Joseph V . Hajnal, and Daniel Rueckert. Localisation of the brain in fetal MRI using bundled SIFT features. In Proc. MICCAI, 2013.  [51] Kevin Keraudren et al. Automated fetal brain segmentation from 2D MRI slices for motion correction. NeuroImage, 101:633â€“643, 2014.  [52] Martin Rajchl et al. Learning under distributed weak supervision, http://arxiv.org/abs/1606.01100, 2016.  [53] Seyed Sadegh Mohseni Salehi et al. Real-time automatic fetal brain extraction in fetal MRI by deep learning. In Proc. ISBI, 2018.  [54] Olaf Ronneberger, Philipp Fischer, and ?omas Brox. U-Net: convolutional networks for biomedical image segmentation. In Proc. MICCAI, 2015.  [55] Junshen Xu, Daniel Moyer, Borjan Gagoski, Juan Eugenio Iglesias, P. Ellen Grant, Polina Golland, and Elfar Adalsteinsson. NeSV oR: Implicit neural representation for slice-to-volume reconstruction in MRI. IEEE Trans. Med. Imaging, 42(6):1707â€“1719, 2023.  [56] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance ï¬elds for view synthesis, In Proc. ECCV, 2020.  [57] Sahar N. Saleem. Fetal MRI: An approach to practice: A review. J. Adv. Res., 5(5):507â€“523, 2014.  [58] Hua-mei Chen, and P.K. Varshney. Mutual information-based CT-MR brain image registration using generalized partial volume joint histogram estimation. IEEE Trans. Med. Imaging, 22(9):1111â€“1119, 2003.  [59] Andrew Adams, Jongmin Baek, and Myers Abraham Davis. Fast high-dimensional ï¬ltering using the permutohedral lattice. Comput. Graph. Forum, 29(2):753â€“762, 2010.  [60] Nils Papenberg, AndrÃ©s Bruhn, ?omas Brox, Stephan Didas, and Joachim Weickert. Highly accurate optic ï¬‚ow computation with theoretically justiï¬ed warping. Int. J. Comput. Vis., 67(2):141â€“158, 2006.  [61] Nicholas J. Higham. Computing the polar decompositionâ€”with applications. SIAM J. Sci. Stat. Comput., 7(4):1160â€“1174, 1986.  [62] A. Di Martino et al. ?e autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. Mol. Psychiatry, 19(6):659â€“667, 2014.  [63] Michael Milham, Damien Fair, Maarten Mennes, and Stewart Mostofsky. ?e ADHD-200 consortium: a model to advance the translational potential of neuroimaging in clinical neuroscience. Front. Syst. Neurosci., 6, 2012.  [64] Vince Calhoun, Jing Sui, Kent Kiehl, Jessica Turner, Elena Allen, and Godfrey Pearlson. Exploring the psychosis functional connectome: aberrant intrinsic networks in schizophrenia and bipolar disorder. Front. Psychiatry, 2, 2012.  [65] Avram J. Holmes et al. Brain Genomics Superstruct Project initial data release with structural, functional, and behavioral measures. Sci. Data, 2(1):150031, 2015.  [66] Randy L. Gollub et al. ?e MCIC Collection: a shared repository of multi-modal, multi-site brain image data from a clinical investigation of schizophrenia. Neuroinformatics, 
11544
 
 11(3):367â€“388, 2013.  [67] Daniel S. Marcus, Tracy H. Wang, Jamie Parker, John G. Csernansky, John C. Morris, and Randy L. Buckner. Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI data in young, middle aged, nondemented, and demented older adults. J. Cogn. Neurosci., 19(9):1498â€“1507, 2007.  [68] Kenneth Marek et al. ?e Parkinson Progression Marker Initiative (PPMI). Prog. Neurobiol., 95(4):629â€“635, 2011.  [69] Cathie Sudlow et al. UK Biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age. PLOS Med., 12(3):e1001779, 2015.  [70] Bruce Fischl et al. Whole brain segmentation: Automated labeling of neuroanatomical structures in the human brain. Neuron, 33(3):341â€“355, 2002.  [71] J. Talairach, and Pierre Tournoux. Co-planar stereotaxic atlas of the human brain: 3-dimensional proportional systemâ€‰: an approach to cerebral imaging. G. ?iemeâ€‰; ?ieme Medical Publishers, Stuttgart, New York 1988.  [72] Ali Gholipour et al. A normative spatiotemporal MRI atlas of the fetal brain for automatic segmentation and analysis of early brain growth. Sci. Rep., 7(1):476, 2017.  [73] Kelly Payette et al. An automatic multi-tissue human fetal brain segmentation benchmark using the Fetal Tissue Annotation Dataset. Sci. Data, 8(1):167, 2021. [74]  Dongdong Chen, Mike Davies, Matthias J. Ehrhardt, Carola-Bibiane SchÃ¶nlieb, Ferdia Sherry, and JuliÃ¡n Tachella. Imaging with equivarient deep learning: from unrolled network design to fully unsupervised learning. IEEE Signal Process. Mag., 40(1):134â€“147, 2023.   
11545
