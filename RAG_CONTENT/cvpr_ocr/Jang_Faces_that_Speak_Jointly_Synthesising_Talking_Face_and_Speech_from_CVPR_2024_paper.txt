Faces that Speak:
Jointly Synthesising Talking Face and Speech from Text
Youngjoon Jang1âˆ—Ji-Hoon Kim1âˆ—Junseok Ahn1Doyeop Kwak1
Hong-Sun Yang2Yoon-Cheol Ju2Il-Hwan Kim2Byeong-Yeol Kim2Joon Son Chung1
1Korea Advanced Institute of Science and Technology,242dot Inc., Republic of Korea
Text-drivenTFG
TextFace-stylisedTTSUnified	model
Motion	conditionSpeakercondition
Figure 1. Our framework integrates Talking Face Generation (TFG) and Text-to-Speech (TTS) systems, generating synchronised natural
speech and a talking face video from a single portrait and text input. Our model is capable of variational motion generation by conditioning
the TFG model with the intermediate representations of the TTS model. The speech is conditioned using the identity features extracted in
the TFG model to align with the input identity.
Abstract
The goal of this work is to simultaneously generate nat-
ural talking faces and speech outputs from text. We achieve
this by integrating Talking Face Generation (TFG) and
Text-to-Speech (TTS) systems into a unified framework. We
address the main challenges of each task: (1) generating
a range of head poses representative of real-world scenar-
ios, and (2) ensuring voice consistency despite variations
in facial motion for the same identity. To tackle these is-
sues, we introduce a motion sampler based on conditional
flow matching, which is capable of high-quality motion code
generation in an efficient way. Moreover, we introduce
a novel conditioning method for the TTS system, which
utilises motion-removed features from the TFG model to
yield uniform speech outputs. Our extensive experiments
demonstrate that our method effectively creates natural-
looking talking faces and speech that accurately match the
input text. To our knowledge, this is the first effort to build a
multimodal synthesis system that can generalise to unseen
identities.Project page with demo: https://mm.kaist.ac.kr/projects/faces-that-speak
âˆ—Equal contribution.1. Introduction
In recent years, the field of talking face synthesis has at-
tracted growing interest, driven by the advancements in
deep learning techniques and the development of services
within the metaverse. This versatile technology has diverse
applications in movie and TV production, virtual assistants,
video conferencing, and dubbing, with the goal of creating
animated faces that are synchronised with audio to enable
natural and immersive human-machine interactions.
Previous studies in deep learning-based talking face syn-
thesis have focused on enhancing the controllability of fa-
cial movements and achieving precise lip synchronisation.
Some notable works [5, 14, 15, 27, 39, 57, 60, 70] incor-
porate 2D or 3D structural information to improve motion
representations. From this, recent research has naturally
diverged into two primary strands along the target appli-
cations of TFG: one strand [42, 65, 73, 76] concentrates
on generating expressive facial movements only from audio
conditions. Meanwhile, the other strand [6, 24, 25, 36, 62,
75] aims to enhance the controllability of talking faces by
introducing a target video as an additional condition. De-
spite these advancements, the audio-driven TFG methods
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8818
exhibit limitations, especially in scenarios like video pro-
duction and AI chatbots, where video and speech must be
generated simultaneously.
An emerging area of research is text-driven TFG, which
is relatively under-explored compared to audio-driven TFG.
Several studies [68, 69, 72] have attempted to merge TTS
systems with TFG using a cascade approach, but suffered
from issues like error accumulation or computational bot-
tleneck. A very recent work [43] uses latent features from
TTS systems for face keypoint generation, yet still requires
an additional stage for RGB video production. It highlights
the challenges and complexities in integrating TFG and TTS
systems into a cohesive and unified framework.
In this paper, we propose a unified framework, named
Text-to-Speaking Face ( TTSF ), which integrates text-
driven TFG and face-stylised TTS. The key to our method
lies in analysing mutually complementary elements across
distinct tasks and leveraging this analysis to construct an
improved framework. As illustrated in Fig. 1, our frame-
work is capable of simultaneously generating talking face
videos and natural speeches given text and a face portrait.
To combine the different tasks in a single model, we tackle
the primary challenges inherent in each task, TFG and TTS.
Firstly, our approach enables the generation of a range of
head poses that reflect real-world scenarios. To encompass
dynamic and authentic facial movements, we propose a mo-
tion sampler based on Optimal-Transport Conditional Flow
Matching (OT-CFM). This approach learns Ordinary Dif-
ferential Equations (ODEs) to extract precise motion codes
from a sophisticated distribution. Nonetheless, considera-
tions need to be taken into account to apply OT-CFM to the
motion sampling process. Direct prediction of target mo-
tion by OT-CFM results in the generation of unsteady facial
motions. To address this issue, we employ an auto-encoder-
based noise reducer to mitigate feature noise through com-
pression and reconstruction of latent features. The com-
pressed features serve as the target motions for our motion
sampler. This demonstrates an enhanced quality of the gen-
erated motion, particularly in terms of temporal consistency.
Secondly, we focus on the challenge of producing con-
sistent voices, specifically when the input identity remains
the same but facial motions differ. This problem arises
from a fundamental inquiry in face-stylised TTS: How can
we extract more refined speaker representations, influenc-
ing prosody, timbre, and accent, from a portrait image?
We observe that facial motion in the source image affects
the ability to identify the characteristics of the target voice.
Nevertheless, this issue has been overlooked in all previous
works [20, 33, 63], as they commonly omit a facial motion
disentanglement module, a crucial component in the TFG
system. With the benefit of integrating the TFG and TTS
models into a system, we present a straightforward yet ef-
fective approach to condition the face-stylised TTS model.By eliminating motion features from the input portrait, our
framework can generate speeches with the consistency of
speaker identity.
In addition to the previously mentioned advantages of
our framework, there are further benefits compared to cas-
cade text-driven TFG systems: (1) our framework does not
require an additional audio encoder, as it can be substituted
with the text encoder in our system, and (2) the joint training
eliminates the need for the fine-tuning process and yields
better-synchronised lip motions in the generated outcomes.
Our contributions can be summarised as follows:
â€¢ To our knowledge, we are the first to propose a unified
text-driven multimodal synthesis system with robust gen-
eralisation to unseen identities.
â€¢ We design the motion sampler based on OT-CFM that is
combined with the auto-encoder-based noise reducer, by
considering the characteristics of motion features.
â€¢ Our method preserves crucial speaker characteristics such
as prosody, timbre, and accent by removing the motion
factors in the source image,
â€¢ With the comprehensive experiments, we demonstrate
the proposed method surpasses the cascade-based talking
face generation methods while producing speeches from
the given text.
2. Related Works
Audio-driven Talking Face Generation. Audio-driven
Talking Face Generation (TFG) technology has captured
considerable attention in the fields of computer vision and
graphics due to its broad range of applications [8, 77]. In
the early works [16, 17], the focus is on situations with in-
dividual speakers, where a single model generates various
talking faces based on a single identity. Recently, advance-
ments in deep learning have facilitated the creation of more
versatile TFG models [7, 12, 31, 44, 47, 58, 74]. These
models can generate talking faces by incorporating identity
conditions as input. However, these studies overlook head
movements, grappling with the difficulty of disentangling
head poses from facial characteristics linked to identity. To
enhance natural facial movements, some studies integrate
landmarks and mesh [14, 57, 60, 70] or leverage 3D infor-
mation [5, 15, 27, 39]. Despite these efforts, performance
degradation occurs, especially in wild scenarios with low
landmark accuracy. Recent research branches [42, 65, 73,
76] focus on generating vivid facial movements only from
audio conditions. Another branch [6, 24, 25, 36, 62, 75]
demonstrates improved controllability by introducing a tar-
get video as an additional condition. These studies show-
case the creation of realistic talking faces with various facial
movements, encompassing head, eyes, and lip movements.
However, these approaches rely on audio sources for TFG,
limiting their applicability in multimedia scenarios lacking
8819
ð‘¬ð’•
Duration
PredictorText
ð‘¬ð’—ð‘¬ð’—
Shared parametersDTTS
Motion
FusionMel
Audio
Mapper
Prior
NetworkGð’©(ð±;0,ðˆ)
Motion
Extractor (ð¸!)ð‘“!"ð‘“#!$
ð¼%
ð¼"
ð‘“&ð‘“'#ð‘“&ð‘“($ð‘“(
#ð¼"
ðð‘“),+
Training only 
Inference only 
Motion
Sampler
Motion Normaliserð‘“%
ð‘“"EMBð’†(Figure 2. Overall architecture of our framework. The TTS model receives identity representations from the TFG model, while the
TFG model takes conditions for natural motion generation from the TTS model. These complementary elements enhance our modelâ€™s
capabilities in generating both speech and talking faces. The EMB block denotes an embedding operation. The grey dashed arrow
represents a path used only during the training process, and the red arrows represent paths used only during the inference process.
an audio source.
Text-driven Talking Face Generation. Text-driven TFG
is relatively less explored compared to the field of audio-
driven TFG. Most previous works [18, 31, 32, 38, 59] pri-
marily focus on generating lip regions for text-based redub-
bing or video-based translation tasks. Recent works [68,
69, 72] have tried to incorporate Text-to-Speech (TTS) tech-
nology into the process of TFG through a cascade method.
However, itâ€™s worth noting that the cascade method encoun-
ters bottlenecks in terms of both performance and inference
time [10]. To tackle this issue, the latest study [43] has
delved into the latent features of TTS to generate keypoints
for talking faces. This exploration provides evidence that
leveraging the latent features of a TTS model is advanta-
geous in substituting the latent of an audio encoder for TFG.
In this paper, we unify TTS and TFG tasks to generate
speech and talking face videos concurrently. Furthermore,
we extend the application of TTS in TFG by conditioning
the target voice with the input identity image. As a re-
sult, our model can generate a diverse range of talking face
videos using only a static face image and text as input.
Text-to-Speech. Text-to-Speech (TTS) systems aim to gen-
erate natural speech from text inputs, evolving from early
approaches to recent end-to-end methods [4, 28, 35, 40,
46, 49, 52]. Despite their success, unseen-speaker TTS
systems face a challenge in requiring substantial enroll-
ment data for accurate voice reproduction. While prior
works [9, 22, 26, 34, 41] extract speaker representations
from speech data, obtaining sufficient high-quality utter-
ances is challenging. Recent studies have incorporated
face images for speaker representation [20, 33, 63], aiming
to capture correlations between visual and audio features.
However, these models often neglect motion-related factors
in face images, leading to challenges in generating consis-
tent desired voices when the input identity remains constant
but the motion varies.In this paper, to tackle this issue, we leverage the motion
extractor of TFG to eliminate the motion features from the
source image. The motion-normalised feature is then fed
into the TTS system as a conditioning factor, aiding the TTS
model in producing consistent voices.
3. Method
In Fig. 2, we propose a unified architecture, named TTSF ,
which integrates TFG and TTS pipelines. In the TTS model,
the text input is embedded as etby an embedding layer.
The text encoder Etmaps this embedding to the text fea-
tureftâˆˆRltÃ—d, where ltandddenote the token length
and hidden dimension, respectively. The duration predic-
tor then upsamples fttoËœftâˆˆRlmÃ—dto align with the tar-
get mel-spectrogramâ€™s length lm. The Ëœftis subsequently
passed into the TTS decoder DTTS to predict the target
mel-spectrogram. Both EtandDTTS are conditioned with
the identity feature fidfrom the TFG model to incorporate
the characteristics of the target speaker. In the TFG model,
the source image Isand driving frames IdâˆˆRtÃ—cÃ—hÃ—w
pass through the shared visual encoder Ev, yielding visual
features fsandfdfor the source and target, respectively.
The motion extractor encodes motion features from the in-
put, obtaining the identity feature fidby subtracting the mo-
tion feature from fs. The target motion feature is denoted as
fm. With the motion fusion module, fid,fm, and the audio
mapper output flipare aggregated and then, input into the
TFG generator Gto generate videos Ë†Idwith desired mo-
tions. To produce variational facial movements during in-
ference, we propose a conditional flow matching-based mo-
tion sampler. Additionally, we introduce an auto-encoder-
based motion normaliser aimed at reducing the noise in the
sampled motions. The feature fc, compressed by the nor-
maliser, serves as the motion samplerâ€™s target during train-
ing. Consequently, our framework synthesises natural talk-
ing faces and speeches from a single portrait image and text
8820
condition.
3.1. Baseline for Talking Face Generation
Motion Extractor. Previous research in the fields of mo-
tion transfer [53, 54, 67] and TFG [25, 66] has identified
the presence of a reference space that only contains individ-
ual identities. Formally, we can express this as Ev(I) =
fid+fm, where Iis the input image, Evis the visual en-
coder, fidis an identity feature, and fmis a motion fea-
ture. In our framework, the motion extractor Emlearns the
subtraction of identity feature fidfrom the visual feature:
Em(Ev(I)) =fm=fâˆ’fid.Our motion extractor follows
the architecture of LIA [67], featuring a 5-layer MLP and
trainable motion codes under an orthogonality constraint.
This constraint facilitates the representation of diverse mo-
tions with compact channel sizes. Unlike LIA, which com-
putes relative motion between source and target images, our
motion extractor independently extracts identity and motion
features. This distinction is crucial for integrating TFG and
TTS models, where the identity feature conditions TTS to
generate consistent voice styles robust to facial motions.
Motion Fusion and Generator. To establish a baseline for
generating both talking faces and speeches, we consider two
key aspects in designing the TFG generator G: (1) mem-
ory efficiency and (2) resilience to unseen identity gener-
ation. To reflect these, we avoid using an inversion net-
work, known for its computational heaviness, and opt for
a flow-based generator that focuses on learning coordinate
mapping. For our generator, we choose LIAâ€™s one, which
employs a StyleGAN [1]-styled generator as a baseline.
However, LIA is explicitly tailored for face-to-face mo-
tion transfer and does not account for generating lip move-
ments synchronised with an audio source. To apply LIA
to TFG, specific considerations are needed. In the training
process, the lack of augmentation for target frames leads
to the model replicating lip motions from the target frames
rather than from audio sources. In response to this, inspired
by FC-TFG [25], we regulate lip motions by incorporating
audio features into specific n-th layers of the decoder. The
fusion process involves a straightforward linear operation:
fz,n=fid+fmiâˆˆ {non-lip motion layers }
fid+flip iâˆˆ {lip motion layers },(1)
where, fmdenotes the target motions extracted from tar-
get frames and flipdenotes the output of the audio mapper,
representing lip motion features. In the end, we generate
the final videos Ë†Idby inputting the style feature fz,ninto
the TFG generator G.
Audio Mapper. Unlike the cascade text-driven TFG, our
framework does not require extracting acoustic features us-
ing an audio encoder. Instead, we utilise the intermediate
representations of the TTS system, serving a definite pur-
Residual	BlockResidual	BlockResidual	Block!"#$"%:!!'(%)*(+$:[-!,$!]Leaky	ReLUConv1dMeanInterpolate56789:967Figure 3. The architecture of the audio mapper. The condition
denotes the concatenated feature of text embedding et, upsampled
text feature Ëœft, and energy, which is a norm of Ëœft.
pose: Generating natural lip motion with the TFG genera-
tor. This feature is crafted by aggregating the concatenated
features of text embedding et, the upsampled text feature
Ëœft, and energy which is an average from Ëœftalong the chan-
nel axis. The text embedding enables the TFG model to
grasp phoneme-level lip representation, while the upsam-
pled text feature and energy contribute to capturing intri-
cate lip shapes aligned with the generated speech sound. To
aggregate these different types of features, we use Multi-
Receptive field Fusion (MRF) module [30]. As illustrated
in Fig. 3, the MRF module comprises multiple residual
blocks, each characterised by 1D convolutions with dis-
tinct kernel sizes and dilations. This diverse configura-
tion enables the module to observe both fine and coarse
details in the input along the time axis. To avoid poten-
tial artifacts at the boundaries of motion features caused
by temporal padding operations, we intentionally remove
the padding operation and introduce temporal interpolation.
Consequently, our framework achieves well-synchronised
lip movements while effectively capturing the characteris-
tics of the generated speech.
Training Objectives. We use a non-saturating loss [19] in
adversarial training:
LGAN = min
Gmax
D
EId[log(D(Id))]
+Efz,n[log(1 âˆ’D(G(fz,n))]
.(2)
For pixel-level supervision, we use L1reconstruction loss
and Learned Perceptual Image Patch Similarity (LPIPS)
loss [71]. The reconstruction loss Lrecis formulated as:
Lrec=âˆ¥Ë†Idâˆ’Idâˆ¥1+1
NfNfX
i=1âˆ¥Ï•(Ë†Id)iâˆ’Ï•(Id)iâˆ¥2, (3)
where Ï•is a pretrained VGG19 [55] network, and Nfis
the number of feature maps. To preserve facial identity af-
ter motion transformation, we apply an identity-based sim-
ilarity loss [50] using a pretrained face recognition network
Lid= 1âˆ’cos
Eid
Ë†Id
, Eid(Id)
.Finally, To generate
well-synchronised videos according to the input audio con-
ditions, We use the modified SyncNet introduced in [25] to
enhance our modelâ€™s lip representations. We minimise the
8821
following sync loss: Lsync= 1âˆ’cos
Sv
Ë†Id
, Sa(As)
,
where Sa,Sv, and Asdenote the audio encoder, video en-
coder of SyncNet, and input audio source.
3.2. Variational Motion Sampling
Preliminary: Conditional Flow Matching. In this sub-
section, we present an outline of Optimal-Transport Condi-
tional Flow Matching (OT-CFM). Our exposition primary
adheres to the notation and definitions in [37, 40].
LetxâˆˆRdbe the data sample from the target distribu-
tionq(x), and p0(x)be tractable prior distribution. Flow
matching generative models aim to map x0âˆ¼p0(x)tox1
by constructing a probability density path pt: [0,1]Ã—Rdâ†’
R>0, such that p1(x)approximates q(x). Consider an arbi-
trary Ordinary Differential Equation (ODE):
d
dtÏ•t(x) =vt(Ï•t(x)), Ï•0(x) =x, (4)
where the vector field vt: [0,1]Ã—Rdâ†’Rdgenerates the
flowÏ•t: [0,1]Ã—Rdâ†’Rd. This ODE is associated with
pt, and it is sufficient to produce realistic data if a neural
network can predict an accurate vector field vt.
Suppose there exists the optimal vector field utthat can
generate accurate pt, then the neural network vt(x;Î¸)can
be trained to estimate the vector field ut. However, in prac-
tice, it is non-trivial to find the optimal vector field utand
the target probability pt. To address this, [37] leverages the
fact that estimation of conditional vector field is equivalent
to estimation of the unconditional one, i.e.,
min
Î¸Et,pt(x)âˆ¥ut(x)âˆ’vt(x;Î¸)âˆ¥2
â‰¡min
Î¸Et,q(x1),pt(x|x1)âˆ¥ut(x|x1)âˆ’vt(x;Î¸)âˆ¥2(5)
with boundary condition p0(x|x1) = p0(x)and
p1(x|x1) =N(x|x1, Ïƒ2I)for sufficiently small Ïƒ.
Meanwhile, [37] further generalise this technique with
noise condition x0âˆ¼ N(0,1), and define OT-CFM loss as:
LOTâˆ’CFM(Î¸) =Et,q(x1),p0(x0)âˆ¥uOT
t(Ï•OT
t(x0)|x1)
âˆ’vt(Ï•OT
t(x0)|Âµ;Î¸)âˆ¥2,(6)
where Âµis the predicted frame-wise mean of x1and
Ï•OT
t(x0) = (1 âˆ’(1âˆ’Ïƒmin)t)x0+tx1is the flow from
x0tox1. The target conditional vector field become
uOT
t(Ï•OT
t(x0)|x1) =x1âˆ’(1âˆ’Ïƒmin)x0, which enables
the improved performance with its inherent linearity. In our
work, we use fixed value of Ïƒmin= 10âˆ’4.
Prior Network. The prior serves as the initial condition
for OT-CFM, facilitating the identification of the optimal
path to x1. During training, our prior network takes the first
motion fm,0of target motion sequence fmand the acousticfeature flipas inputs. We structure the prior network with
a 4-layer conformer [21], where the input is formed by the
summation of fm,0andflip. Note that the first motion is
replaced as the source imageâ€™s motion in inference.
OT-CFM Motion Sampler. The objective of our motion
sampler is to sample a sequence of natural motion codes
from the prior Âµ. During training, this module aims to pre-
dict target motions fm. However, in our experiments, we
observed that directly regressing fm(equivalent to setting
x1asfm) leads to producing shaky motions during infer-
ence. We expect that this is due to the characteristics of
the StyleGAN-styled decoder. Each channel of the decoder
plays a semantically meaningful role in generating detailed
facial attributes. Therefore, when the motion sampler fails
to successfully estimate the vector field, it directly impacts
the final outcomes. To address this issue, we introduce an
auto-encoder-based motion normaliser that compresses fea-
ture and reconstructs them into the target motion fm. The
compressed motion features fcserve as x1in OT-CFM.
Training Objectives. The reconstruction loss for train-
ing our motion normaliser is defined as Mean Square Error
(MSE) loss between the target motion fmand the recon-
structed motion Ë†fmas follows: LAE=âˆ¥Ë†fmâˆ’fmâˆ¥2.
Moreover, as motion decoding commences from random
noiseN(Âµ, I)at inference, our objective is to minimise the
distance between the prior Âµand compressed target motion
fc. Considering the output of prior network Âµas parameter-
ising the input noise for the decoder, it is natural to view the
encoder output Âµas a normal distribution N(Âµ, I). Follow-
ing [46], we compute a negative log-likelihood prior loss:
Lprior =âˆ’TX
j=1logÏ†(fc,j;Âµj, I), (7)
where Ï†(Â·;Âµi, I)represents the probability density function
ofN(Âµi, I), andTdenotes the temporal length of motions.
3.3. Text-to-Speech Synthesis
Our TTS system aims to produce well-stylised speech from
a single portrait, acquired in in-the-wild setting. In this con-
text, we define the in-the-wild environment as follows: (1)
The model is exposed to previously unseen facial data, and
(2) the facial images exhibit various facial poses. First,
since we cannot access to the identity labels to unseen
speakers, we condition our model with image embedding.
Second, our emphasis is on the advantages of our frame-
work. By integrating TFG and TTS systems, we can utilise
the identity feature fid, a motion-removed feature, obtained
from the TFG model. Consequently, our TTS model is ca-
pable of generating speeches robust to various facial mo-
tions in image, maintaining consistency in the style of voice.
Our system is based on Matcha-TTS [40], an OT-CFM-
based TTS model known for synthesising high-quality
8822
SadTalkerAudio2HeadMakeItTalk
SadTalkerAudio2HeadMakeItTalkIdentityIdentity
TTSF(Ours)TTSFw/o energyTTSFw/o(energy & !"!)/É™Ëˆ// b// a// ÊŠ// t// Êƒ// e// Éª// p// s/
SynthesisedSpeechSynthesisedSpeech
TTSF(Ours)TTSFw/o energyTTSFw/o(energy & !"!)
Figure 4. Qualitative Results. We compare our method with several baselines listed in Table 1. Our approach outperforms all the baselines
in terms of generating natural facial motions, encompassing lip shape and head pose. MakeItTalk and SadTalker exhibit smaller variance
in head poses, while Audio2Head fails to preserve the source identity. We emphasis that our TTSF system can generate sophisticated lip
shapes, reflecting both linguistic and acoustic information from our TTS model.
speeches in a few synthesis steps. We input the identity
feature fidto both encoder and decoder. With this mini-
mal variation, our model is trained with prior, duration, and
OT-CFM losses, as outlined in [40]. These losses are collec-
tively denoted as LTTS. Finally, we convert the generated
mel-spectrogram by using a pretrained vocoder [30].
Final Loss. The final loss is calculated as the sum of the
aforementioned losses, represented as follows:
Ltotal=Î»1LGAN +Î»2Lrec+Î»3Lid+Î»4Lsync
+Î»5LOTâˆ’CFM +Î»6Lae+Î»7Lprior +Î»8Ltts,(8)
where hyperparameters Î»are introduced to balance the
scale of each loss. Each Î»controls the relative importance
of its corresponding loss term. Empirically, the values of
Î»1,Î»2,Î»3,Î»4,Î»5,Î»6,Î»7, and Î»8are set to 0.1, 1, 0.3, 0.1,
0.1, 1, 0.1, and 1.
4. Experiments
4.1. Experimental Setup
Dataset. Our framework is trained on the LRS3 [3] dataset,
which consists of both talking face videos and transcrip-
tion labels. LRS3 consists of videos captured during indoor
shows of TED or TEDx. We evaluate our model on V ox-
Celeb2 [13] and LRS2 [2] datasets, which contain morechallenging examples than LRS3 since many videos are
shot outdoors. We randomly select a subset of videos from
each dataset to evaluate the performance of our framework.
Implementation Details. First of all, we pretrain a Matcha-
TTS [40] model on the LRS3 dataset for 2,000 epochs and
then jointly train with the talking face generation model for
40 epochs. Our focus is on the manipulation of seven spe-
cific layers within the generator, namely layers 1 to 7. Fur-
thermore, we exclusively input the audio feature into two
specific layers, namely layers 6 and 7. ur motion sampler
is trained with 32-frame videos and then inferences with all
frames of each video. Audio data is sampled to 16kHz, and
converted to mel-spectrogram with a window size of 640, a
hop length of 160, and 80 mel bins. To update our model,
we employ the Adam optimiser [29] with a learning rate set
at1eâˆ’4. The entire framework is implemented using Py-
Torch [45] and is trained across eight 48GB A6000 GPUs.
Evaluation Metrics. In our quantitative assessments for
TFG, we employ a range of evaluation metrics introduced
in previous works. To assess the visual quality of the gen-
erated videos, we employ the Fr Â´echet Inception Distance
(FID) score and ID Similarity (ID-SIM) score using a pre-
trained face recognition model [23]. To measure the accu-
racy of mouth shapes and lip sync, we utilise the Lip Sync
Error Confidence (LSE-C), a metric introduced in [11]. For
the diversity of the generated head motions, we calculate the
8823
Models AudioVideo Quality Synchonisation Diversity
FIDâ†“ ID-SIM â†‘ LSE-C â†‘ DIVâ†‘
Ground Truth - 0.00 1.00 5.360 0.168
MakeItTalk [76] GT 24.213 0.845 2.674 0.102
MakeItTalk TTS 25.168 0.850 3.487 0.095
Audio2Head [64] GT 41.721 0.217 4.607 0.149
Audio2Head TTS 42.262 0.225 5.478 0.145
SadTalker [73] GT 20.771 0.854 4.978 0.109
SadTalker TTS 20.729 0.859 6.256 0.111
TTSF (Ours) - 18.348 0.864 5.686 0.143
Table 1. Comparison with the state-of-the-art methods on LRS2
in the one-shot setting. The Audio column refers to the speech
source for generation (GT: ground truth, TTS: synthesised audio.)
ModelsVideo Quality Synchronisation Diversity
ID-SIM â†‘ LSE-C â†‘ DIVâ†‘
MakeItTalk [76] 0.841 3.529 0.094
Audio2Head [64] 0.151 5.738 0.146
SadTalker [73] 0.855 6.310 0.111
TTSF (Ours) 0.876 5.721 0.143
Table 2. Comparison with the state-of-the-art methods on V ox-
Celeb2 in the one-shot setting. The previous audio-driven TFG
models are cascaded with our TTS model to generate talking faces
from text.
standard deviation of the head motion feature embeddings
extracted from the generated frames using Hopenet [51],
following the approach introduced in [73].
For the evaluation of TTS performance, we compute
Word Error Rate (WER), Mel Cepstral Distortion (MCD),
the cosine similarity (C-SIM) between x-vectors [56] of the
target and synthesised speech, as well as the Root Mean
Square Error (RMSE) for F0. WER and MCD represent
the intelligibility and naturalness of speech, respectively.
C-SIM and RMSE measure the voice similarity to the tar-
get speaker. For WER, we use a publicly available speech
recognition model of [48].
4.2. Comparison with State-of-the-art Methods
Text-driven Talking Face Generation. We compare
several state-of-the-art methods (MakeitTalk [76], Au-
dio2Head [64], and SadTalker [73]) for the text-driven talk-
ing head video generations by attaching our TTS model
to the previous audio-driven TFG models in the cascade
method. To simulate a one-shot talking face generation sce-
nario, we evaluate the baselines on the in-the-wild datasets,
LRS2 and V oxCeleb2. As shown in Table 1, the proposed
model outperforms every audio- and cascade text-driven
method in terms of video quality (FID, ID-SIM) on LRS2.
Additionally, we present experimental results on the V ox-
Celeb2 dataset in Table 5. Since this dataset does not con-
tain text transcription according to the speech in the video,
our framework generates both speech and a talking faceModelsIntel. Nat. Voice similarity
WERâ†“MCDâ†“C-SIM â†‘RMSE â†“
Ground Truth 6.35 â€“ â€“ â€“
Face-TTS [33] 18.02 6.85 0.272 52.33
Ours (w/ motion) 15.68 7.43 0.451 50.67
Ours (w/o motion) 14.56 7.23 0.593 48.52
Table 3. Quantitative results of synthesised speech. Intel. and Nat.
denote intelligibility and naturalness of audio, respectively.
by inputting a single frame from a V oxCeleb2 video and
a randomly selected transcription from LRS3. Similar to
the experimental results on LRS2, our framework exhibits
superior performance in ID-SIM score. On the other hand,
the proposed model records a lower synchronisation score
compared to SadTalker using the LSE-C metric. However,
given that the LSE-C metric relies significantly on a pre-
trained model, a more useful evaluation of lip synchronisa-
tion can be achieved through perceptual judgement by hu-
mans, as assessed in user studies. The qualitative assess-
ment in Section 4.3 shows that our method produces per-
ceptually better synchronised output compared to the base-
line. Although Audio2Head shows the best diversity score,
it records the lowest scores in video quality metrics. We
also observe that Audio2Head completely fails to generate
a natural video when the input source image is not located
in the centre of the screen. On the other hand, our pro-
posed method achieves high scores in both video quality
and diversity metrics. Considering the aforementioned is-
sues, our framework demonstrates robust generalisation to
unseen data when conducting multimodal synthesis encom-
passing both video and speech.
Face-stylised Text-to-Speech. To evaluate the generalis-
ability of our TTS system, we compare our model to Face-
TTS [33], which is a state-of-the-art method of face-stylised
TTS. For the evaluation, we simulate two scenarios on
LRS2 dataset: (1) w/ motion , where the TTS model is condi-
tioned with source image embedding, i.e., fid+fm; (2) w/o
motion , where the model is conditioned with only identity
feature fid. The results are shown in Table 1. While the pro-
posed model shows slight deviance in MCD, it clearly out-
performs the baseline in WER, C-SIM, and RMSE, demon-
strating its superiority in intelligibility and voice similarity.
More importantly, when we consider motion features to-
gether as our speaker condition, the generation performance
is significantly degraded, especially in voice similarity. It
indicates the benefits of unifying TFG and TTS systems,
highlighting the advantages of their integration.
4.3. Qualitative Evaluation
User Study. We evaluate the synthesised videos through a
user study involving 40 participants, each providing opin-
8824
ModelsLip Sync Motion Video
Accuracy Naturalness Realness
MakeItTalk [76] 2.44Â±0.07 2.79Â±0.09 2.79Â±0.08
Audio2Head [64] 2.73Â±0.08 2.92Â±0.09 2.48Â±0.09
SadTalker [73] 2.78Â±0.08 2.80Â±0.09 2.90Â±0.09
TTSF(Ours) 4.09Â±0.07 3.85Â±0.07 3.87Â±0.07
Table 4. MOS evaluation results. MOS is presented with 95% con-
fidence intervals. Note that the previous audio-driven TFG models
are cascaded with our TTS model.
ions on 20 videos. Reference images and texts were ran-
domly selected from the LRS2 test split to create videos
using MakeItTalk [76], Audio2Head [64], SadTalker [73],
and our proposed method. Mean Opinion Scores (MOS) are
used for evaluation, following the approach in [25, 36, 75].
Participants rate each video on a scale from 1 to 5, consid-
ering lip sync quality, video realness, and head movement
naturalness. The order of methods within each video clip is
randomly shuffled. The results in Table 4 indicate that our
method outperforms existing methods in generating talking
face videos with higher lip synchronisation and natural head
movement.
Analysis on Qualitative Results. We visually present our
qualitative results in Fig. 4. MakeItTalk fails to produce
precise lip motions aligned with the synthesised speech,
and Audio2Head struggles to preserve identity information.
SadTalker can generate well-synchronised lip motions but
is limited in facial movements. In contrast, our approach
exhibits more dynamic facial movements and can generate
vivid lip motions that reflect both linguistic and acoustic in-
formation. For instance, it can be seen that our modelâ€™s lip
motions are precisely aligned to the pronunciation of the
speeches (refer to the yellow arrows). The accuracy and the
details demonstrate that our method can generate realistic
and expressive talking faces.
The Effectiveness of Identity Features. To verify the ef-
fectiveness of identity feature-based conditioning, we visu-
alise the feature space of synthesised audio. Fig. 5 shows t-
SNE [61] plots of x-vectors from Face-TTS and our method.
As shown in Fig. 5a, Face-TTS fails to cluster features de-
rived from the same speaker. This implies the potential fail-
ure to generate the target voice with different styles. In con-
trast, as depicted in Fig. 5b, the proposed TTS system ef-
fectively clusters features derived from the same speaker de-
spite the variety in head motions. This demonstrates that our
method is capable of synthesising consistent voices, even in
the presence of varying motions.
4.4. Ablation Studies
Analysis on Feature Aggregation in Audio Mapper. We
perform an ablation study on the feature aggregation in ourModelsVideo Quality Synchronisation Diversity
ID-SIM â†‘ LSE-C â†‘ DIVâ†‘
TTSF (Ours) 0.876 5.721 0.143
w/o energy 0.874 5.555 0.143
w/o (energy & Ëœft) 0.872 3.935 0.139
Table 5. Ablation study on feature aggregation in audio mapper.
(a) Face-TTS [33]
 (b) Ours
Figure 5. Speaker representation space of (a) Face-TTS and (b)
Ours. Each colour represents a different speaker.
audio mapper. w/o (energy & Ëœft) indicates the TFG model
conditioned with text embedding etfrom the audio mapper.
In this case, the TFG model can incorporate only linguistic
information and it leads to our model failing to generate pre-
cise lip motions. When we additionally input the upsampled
text feature Ëœftto our TFG model, the synchronisation score
improves significantly. This is because our TTS model is
optimised by reducing the prior loss between Ëœftand the
target mel-spectrogram. This indicates that the Ëœftfeature
contains acoustic information. Finally, when we add the
energy feature to the previous condition, our model exhibits
the best performance across all metrics. This indicates that
the energy of speech significantly impacts generation of de-
tailed lip movements.
5. Conclusion
Our work introduces a unified text-driven multimodal syn-
thesis system that exhibits robust generalisation to unseen
identities. The proposed OT-CFM-based motion sampler,
coupled with an auto-encoder-based noise reducer, pro-
duces realistic facial poses. Notably, our method excels in
preserving essential speaker characteristics such as prosody,
timbre, and accent by effectively removing motion fac-
tors from the source image. Our experiments demonstrate
the superiority of our proposed method over cascade-based
talking face generation approaches, underscoring the effec-
tiveness of our unified framework in multimodal speech
synthesis.
8825
References
[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
age2styleGAN++: How to edit the embedded images? In
Proc. CVPR , 2020. 4
[2] Triantafyllos Afouras, Joon Son Chung, Andrew Senior,
Oriol Vinyals, and Andrew Zisserman. Deep audio-visual
speech recognition. IEEE Trans. on Pattern Analysis and
Machine Intelligence , 2018. 6
[3] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisser-
man. LRS3-TED: A large-scale dataset for visual speech
recognition. arXiv preprint arXiv:1809.00496 , 2018. 6
[4] Alan W Black, Heiga Zen, and Keiichi Tokuda. Statistical
parametric speech synthesis. In Proc. ICASSP , 2007. 3
[5] Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2d & 3d face alignment problem? (and
a dataset of 230,000 3d facial landmarks). In Proc. ICCV ,
2017. 1, 2
[6] Egor Burkov, Igor Pasechnik, Artur Grigorev, and Victor
Lempitsky. Neural head reenactment with latent pose de-
scriptors. In Proc. CVPR , 2020. 1, 2
[7] Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang
Xu. Hierarchical cross-modal talking face generation with
dynamic pixel-wise loss. In Proc. CVPR , 2019. 2
[8] Lele Chen, Guofeng Cui, Ziyi Kou, Haitian Zheng, and
Chenliang Xu. What comprises a good talking-head video
generation?: A survey and benchmark. arXiv preprint
arXiv:2005.03201 , 2020. 2
[9] Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin,
Sheng Zhao, and Tie-Yan Liu. Adaspeech: Adaptive text to
speech for custom voice. In Proc. ICLR , 2021. 3
[10] Jeongsoo Choi, Minsu Kim, Se Jin Park, and Yong Man
Ro. Reprogramming audio-driven talking face synthesis into
text-driven. In Proc. ICASSP , 2024. 3
[11] Joon Son Chung and Andrew Zisserman. Out of time: auto-
mated lip sync in the wild. In Proc. ACCV , 2017. 6
[12] Joon Son Chung, Amir Jamaludin, and Andrew Zisserman.
You said that? In Proc. BMVC. , 2017. 2
[13] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.
V oxceleb2: Deep speaker recognition. In Proc. Interspeech ,
2018. 6
[14] Dipanjan Das, Sandika Biswas, Sanjana Sinha, and Brojesh-
war Bhowmick. Speech-driven facial animation using cas-
caded GANs for learning of motion and texture. In Proc.
ECCV , 2020. 1, 2
[15] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde
Jia, and Xin Tong. Accurate 3d face reconstruction with
weakly-supervised learning: From single image to image set.
InProc. CVPR , 2019. 1, 2
[16] Bo Fan, Lijuan Wang, Frank K Soong, and Lei Xie. Photo-
real talking head with deep bidirectional LSTM. In Proc.
ICASSP , 2015. 2
[17] Bo Fan, Lei Xie, Shan Yang, Lijuan Wang, and Frank K
Soong. A deep bidirectional LSTM approach for video-
realistic talking head. Multimedia Tools and Applications ,
2016. 2
[18] Ohad Fried, Ayush Tewari, Michael Zollh Â¨ofer, Adam Finkel-
stein, Eli Shechtman, Dan B Goldman, Kyle Genova, ZeyuJin, Christian Theobalt, and Maneesh Agrawala. Text-based
editing of talking-head video. ACM Transactions on Graph-
ics, 2019. 3
[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 2020. 4
[20] Shunsuke Goto, Kotaro Onishi, Yuki Saito, Kentaro
Tachibana, and Koichiro Mori. Face2speech: Towards multi-
speaker text-to-speech synthesis using an embedding vector
predicted from a face image. In Proc. Interspeech , 2020. 2,
3
[21] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-
mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-
dong Zhang, Yonghui Wu, et al. Conformer: Convolution-
augmented transformer for speech recognition. In Proc. In-
terspeech , 2020. 5
[22] Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou
Zhao. Generspeech: Towards style transfer for generalizable
out-of-domain text-to-speech synthesis. In Proc. NeurIPS ,
2022. 3
[23] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu,
Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang.
Curricularface: Adaptive curriculum learning loss for deep
face recognition. In Proc. CVPR , 2020. 6
[24] Geumbyeol Hwang, Sunwon Hong, Seunghyun Lee, Sung-
woo Park, and Gyeongsu Chae. Discohead: Audio-and-
video-driven talking head generation by disentangled control
of head pose and facial expressions. In Proc. ICASSP , 2023.
1, 2
[25] Youngjoon Jang, Kyeongha Rho, Jongbin Woo, Hyeongkeun
Lee, Jihwan Park, Youshin Lim, Byeong-Yeol Kim, and
Joon Son Chung. Thatâ€™s what i said: Fully-controllable talk-
ing face generation. In Proc. ACM MM , 2023. 1, 2, 4, 8
[26] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan
Shen, Fei Ren, Patrick Nguyen, Ruoming Pang, Ignacio
Lopez Moreno, Yonghui Wu, et al. Transfer learning from
speaker verification to multispeaker text-to-speech synthesis.
InProc. NeurIPS , 2018. 3
[27] Zi-Hang Jiang, Qianyi Wu, Keyu Chen, and Juyong Zhang.
Disentangled representation learning for 3D face shape. In
Proc. CVPR , 2019. 1, 2
[28] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh
Yoon. Glow-TTS: A generative flow for text-to-speech via
monotonic alignment search. In Proc. NeurIPS , 2020. 3
[29] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. Proc. ICLR , 2014. 6
[30] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-GAN:
Generative adversarial networks for efficient and high fi-
delity speech synthesis. In Proc. NeurIPS , 2020. 4, 6
[31] Prajwal KR, Rudrabha Mukhopadhyay, Jerin Philip, Ab-
hishek Jha, Vinay Namboodiri, and CV Jawahar. Towards
automatic face-to-face translation. In Proc. ACM MM , 2019.
2, 3
[32] Rithesh Kumar, Jose Sotelo, Kundan Kumar, Alexan-
dre De Brebisson, and Yoshua Bengio. Obamanet:
Photo-realistic lip-sync from text. arXiv preprint
arXiv:1801.01442 , 2017. 3
8826
[33] Jiyoung Lee, Joon Son Chung, and Soo-Whan Chung. Imag-
inary voice: Face-styled diffusion model for text-to-speech.
InProc. ICASSP , 2023. 2, 3, 7, 8
[34] Ji-Hyun Lee, Sang-Hoon Lee, Ji-Hoon Kim, and Seong-
Whan Lee. PV AE-TTS: Adaptive text-to-speech via progres-
sive style adaptation. In Proc. ICASSP , 2022. 3
[35] Sang-Hoon Lee, Hyun-Wook Yoon, Hyeong-Rae Noh, Ji-
Hoon Kim, and Seong-Whan Lee. Multi-spectroGAN: High-
diversity and high-fidelity spectrogram generation with ad-
versarial style combination for speech synthesis. In Proc.
AAAI , 2021. 3
[36] Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, Zhibin
Hong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding,
and Jingdong Wang. Expressive talking head generation with
granular audio-visual control. In Proc. CVPR , 2022. 1, 2, 8
[37] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-
ian Nickel, and Matt Le. Flow matching for generative mod-
eling. In Proc. ICLR , 2023. 5
[38] Jinglin Liu, Zhiying Zhu, Yi Ren, Wencan Huang, Baoxing
Huai, Nicholas Yuan, and Zhou Zhao. Parallel and high-
fidelity text-to-lip generation. In Proc. AAAI , 2022. 3
[39] Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan,
Tangjie Lv, Yu Ding, Zhidong Deng, and Xin Yu. Styletalk:
One-shot talking head generation with controllable speaking
styles. In Proc. AAAI , 2023. 1, 2
[40] Shivam Mehta, Ruibo Tu, Jonas Beskow, Â´Eva Sz Â´ekely, and
Gustav Eje Henter. Matcha-TTS: A fast TTS architecture
with conditional flow matching. In Proc. ICASSP , 2024. 3,
5, 6
[41] Dongchan Min, Dong Bok Lee, Eunho Yang, and Sung Ju
Hwang. Meta-stylespeech: Multi-speaker adaptive text-to-
speech generation. In Proc. ICML , 2021. 3
[42] Dongchan Min, Minyoung Song, and Sung Ju Hwang.
Styletalker: One-shot style-based audio-driven talking head
video generation. arXiv preprint arXiv:2208.10922 , 2022. 1,
2
[43] Kentaro Mitsui, Yukiya Hono, and Kei Sawada. Uniflg: Uni-
fied facial landmark generator from text or speech. In Proc.
Interspeech , 2023. 2, 3
[44] Se Jin Park, Minsu Kim, Joanna Hong, Jeongsoo Choi, and
Yong Man Ro. Synctalkface: Talking face generation with
precise lip-syncing via audio-lip memory. In Proc. AAAI ,
2022. 2
[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library, 2019.
6
[46] Vadim Popov, Ivan V ovk, Vladimir Gogoryan, Tasnima
Sadekova, and Mikhail Kudinov. Grad-TTS: A diffusion
probabilistic model for text-to-speech. In Proc. ICML , 2021.
3, 5
[47] KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Nambood-
iri, and CV Jawahar. A lip sync expert is all you need for
speech to lip generation in the wild. In Proc. ACM MM ,
2020. 2[48] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,
Christine McLeavey, and Ilya Sutskever. Robust speech
recognition via large-scale weak supervision. In Proc. ICML ,
2023. 7
[49] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou
Zhao, and Tie-Yan Liu. Fastspeech: Fast, robust and control-
lable text to speech. In Proc. NeurIPS , 2019. 3
[50] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,
Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding
in style: A stylegan encoder for image-to-image translation.
InProc. CVPR , 2021. 4
[51] Nataniel Ruiz, Eunji Chong, and James M. Rehg. Fine-
grained head pose estimation without keypoints. In Proc.
CVPR , 2018. 7
[52] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,
Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,
Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural TTS synthesis
by conditioning wavenet on mel spectrogram predictions. In
Proc. ICASSP , 2018. 3
[53] Aliaksandr Siarohin, St Â´ephane Lathuili `ere, Sergey Tulyakov,
Elisa Ricci, and Nicu Sebe. First order motion model for
image animation. In Proc. NeurIPS , 2019. 4
[54] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei
Chai, and Sergey Tulyakov. Motion representations for ar-
ticulated animation. In Proc. CVPR , 2021. 4
[55] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In Proc.
ICLR , 2015. 4
[56] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel
Povey, and Sanjeev Khudanpur. X-vectors: Robust dnn em-
beddings for speaker recognition. In Proc. ICASSP , 2018.
7
[57] Linsen Song, Wayne Wu, Chen Qian, Ran He, and
Chen Change Loy. Everybodyâ€™s talkinâ€™: Let me talk as you
want. IEEE Transactions on Information Forensics and Se-
curity , 2022. 1, 2
[58] Yang Song, Jingwen Zhu, Dawei Li, Xiaolong Wang, and
Hairong Qi. Talking face generation by conditional recurrent
adversarial network. Proc. IJCAI , 2019. 2
[59] Sarah L Taylor, Moshe Mahler, Barry-John Theobald, and
Iain Matthews. Dynamic units of visual speech. In Proc.
ACM SIGGRAPH , 2012. 3
[60] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian
Theobalt, and Matthias NieÃŸner. Neural voice puppetry:
Audio-driven facial reenactment. In Proc. ECCV , 2020. 1, 2
[61] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-SNE. Journal of machine learning research ,
2008. 8
[62] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum,
and Baoyuan Wang. Progressive disentangled representation
learning for fine-grained controllable talking head synthesis.
InProc. CVPR , 2022. 1, 2
[63] Jianrong Wang, Zixuan Wang, Xiaosheng Hu, Xuewei Li,
Qiang Fang, and Li Liu. Residual-guided personalized
speech synthesis based on face image. In Proc. ICASSP ,
2022. 2, 3
8827
[64] Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, and Xin
Yu. Audio2head: Audio-driven one-shot talking-head gen-
eration with natural head motion. In Proc. IJCAI , 2021. 7,
8
[65] Suzhen Wang, Lincheng Li, Yu Ding, and Xin Yu. One-
shot talking face generation from single-speaker audio-visual
correlation learning. In Proc. AAAI , 2022. 1, 2
[66] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot
free-view neural talking-head synthesis for video conferenc-
ing. In Proc. CVPR , 2021. 4
[67] Yaohui Wang, Di Yang, Francois Bremond, and Antitza
Dantcheva. Latent image animator: Learning to animate im-
ages via latent space navigation. In Proc. ICLR , 2022. 4
[68] Zhichao Wang, Mengyu Dai, and Keld Lundgaard. Text-
to-video: A two-stage framework for zero-shot identity-
agnostic talking-head generation. In Proc. KDD , 2023. 2,
3
[69] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Chen Zhang,
Xiang Yin, Zejun Ma, and Zhou Zhao. Ada-TTA: Towards
adaptive high-quality text-to-talking avatar synthesis. In
Proc. ICMLW , 2023. 2, 3
[70] Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-
Jin Liu. Audio-driven talking face video generation with
learning-based personalized head pose. IEEE Trans. on Mul-
timedia , 2020. 1, 2
[71] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In Proc. CVPR , 2018. 4
[72] Sibo Zhang, Jiahong Yuan, Miao Liao, and Liangjun Zhang.
Text2video: Text-driven talking-head video synthesis with
personalized phoneme-pose dictionary. In Proc. ICASSP ,
2022. 2, 3
[73] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,
Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker:
Learning realistic 3D motion coefficients for stylized audio-
driven single image talking face animation. In Proc. CVPR ,
2023. 1, 2, 7, 8
[74] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang
Wang. Talking face generation by adversarially disentangled
audio-visual representation. In Proc. AAAI , 2019. 2
[75] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy,
Xiaogang Wang, and Ziwei Liu. Pose-controllable talking
face generation by implicitly modularized audio-visual rep-
resentation. In Proc. CVPR , 2021. 1, 2, 8
[76] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevar-
ria, Evangelos Kalogerakis, and Dingzeyu Li. Makeittalk:
speaker-aware talking-head animation. ACM Transactions
on Graphics , 2020. 1, 2, 7, 8
[77] Hao Zhu, Man-Di Luo, Rui Wang, Ai-Hua Zheng, and Ran
He. Deep audio-visual learning: A survey. International
Journal of Automation and Computing , 2021. 2
8828
