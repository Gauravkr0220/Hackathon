Extend Your Own Correspondences: Unsupervised Distant Point Cloud
Registration by Progressive Distance Extension
Quan Liu1Hongzi Zhu1*Zhenxi Wang1Yunsong Zhou1Shan Chang2Minyi Guo1
1Shanghai Jiao Tong University2Donghua University
https://github.com/liuQuan98/EYOC
Abstract
Registration of point clouds collected from a pair of dis-
tant vehicles provides a comprehensive and accurate 3D
view of the driving scenario, which is vital for driving safety
related applications, yet existing literature suffers from the
expensive pose label acquisition and the deficiency to gen-
eralize to new data distributions. In this paper, we pro-
pose EYOC, an unsupervised distant point cloud registra-
tion method that adapts to new point cloud distributions
on the fly, requiring no global pose labels. The core idea
of EYOC is to train a feature extractor in a progressive
fashion, where in each round, the feature extractor, trained
with near point cloud pairs, can label slightly farther point
cloud pairs, enabling self-supervision on such far point
cloud pairs. This process continues until the derived ex-
tractor can be used to register distant point clouds. Par-
ticularly, to enable high-fidelity correspondence label gen-
eration, we devise an effective spatial filtering scheme to
select the most representative correspondences to register a
point cloud pair, and then utilize the aligned point clouds
to discover more correct correspondences. Experiments
show that EYOC can achieve comparable performance with
state-of-the-art supervised methods at a lower training cost.
Moreover, it outwits supervised methods regarding general-
ization performance on new data distributions.
1. Introduction
Registering point clouds obtained on distant vehicles of 5
meters to 50 meters apart [28, 29] can greatly benefit a
rich set of self-driving vision tasks, ranging from detection
[55, 58, 61] and segmentation [42, 50] to birds‚Äô eye view
(BEV) representation [27, 38] and SLAM [33, 34], and ul-
timately improve the overall driving safety. Traditional su-
pervised registration methods not only heavily rely on accu-
ratepose labels during training [6, 21, 43] but cannot attain
expected performance on new data distributions as they do
*Corresponding author
2D feat. 3D feat.Est. Corr.Transfer
Supervision
LiDAR3D feat.GT Corr.
Supervise
(a) Supervised (b) BYOCRGB -D
Est. Corr.Transfer
Supervision
LiDAR3D labeler feat. 3D student feat.EMA
Extending point
cloud distance (c) EYOC (ours)Filtering +
Registration +
NN-Search[12]
Figure 1. (a) Supervised registration require ground-truth (GT)
pose, and (b) BYOC requires RGB-D images for supervision
[12]. (c) In contrast, EYOC acquires supervision from LiDAR
sequences directly, enabling single-modal unsupervised training.
on existing datasets [9, 20], making them infeasible to use
in real-world driving scenarios. In light of the ever-growing
LiDAR-equipped vehicles and the tremendous amount of
sequential unlabelled point cloud data, can we finetune a
registration network on a new point cloud distribution with
no pose labels so that distant point clouds on the new dis-
tribution can be accurately registered on the fly?
In the literature, a rich set of supervised indoor [20, 24,
26, 37, 53, 56] or synthetic [1, 15, 48, 52] low-overlap regis-
tration methods have been proposed. Most of these methods
simply fail on outdoor distant point clouds due to the patch-
similarity assumption [31, 56] or structural prior such as
optimal-transport [37, 54] no longer hold. While simpler
networks ( e.g., CNNs) showed better robustness on distant
point clouds [9, 20], they still need expensive ground-truth
poses for training, as depicted in Fig. 1(a). As pointed out
by Banani and Johnson [12], unsupervised registration is
all about establishing correspondences. BYOC [12], Unsu-
pervised R&R [13], and UDPReg [32] have bypassed cor-
respondence acquisition in the indoor setting by borrowing
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20816
correspondences from RGB channel or GMM matching, as
depicted in Fig. 1(b), but they suffer from the discrepancy
between dense surround images and a sparse point cloud
in outdoor settings. As a result, there is no successful so-
lution, to the best of our knowledge, to the unsupervised
distant point cloud registration problem.
In this paper, we propose Extend Your Own Correspon-
dences (EYOC), a fully unsupervised outdoor distant point
cloud registration method requiring neither pose labels nor
any input of other modality . As depicted in Fig. 1(c), our
core idea is to adopt a progressive self-labeling scheme to
train a feature extractor in multiple rounds. Specifically, in
each round, a labeler model trained with near point cloud
pairs can generate correspondence labels for farther-apart
point clouds, which are used to train a student model. Par-
ticularly, the Siamese labeler-student models are synchro-
nized using the exponential moving average (EMA). This
process repeats until a full-fledged student model, capable
of extracting effective features for distant point cloud regis-
tration, is obtained. Two main challenges are encountered
in the design of EYOC as follows.
First, it is extremely challenging to prevent the self-
labelling process from diverging, given the extreme low-
overlap and density-variation of a distant point cloud pair,
as witnessed even in supervised training [28]. To deal with
this challenge, we take a gradual learning methodology by
breaking the hard learning problem into a series of learn-
ing steps with increasing learning difficulties. Specifically,
in the first step, considering the spatial locality of two con-
secutive frames in a LiDAR point cloud sequence, we as-
sume that two consecutive frames approximately have no
transformation, which can be used as supervision to train
a basic model. After the model first converges to a decent
set of weights, we enable the labeler-student self-labelling
process and gradually extend the interval of training frames
in each learning step. As a result, the student model can
converge smoothly.
Second, it is nontrivial for the labeler in one learning step
to generate sufficient correspondence labels of high quality
for the next harder learning step. We observe the near-far
diversity phenomenon of LiDAR point clouds, i.e., when the
observation distance changes, the point density variation of
near objects is larger than that of far objects. This means
that features extracted from low-density (far-from-LiDAR)
regions are more stable along with distance changes. In-
spired by this insight, we develop a spatial filtering tech-
nique to effectively discover a set of initial quality corre-
spondences in low-density regions. Furthermore, to obtain
more widespread correspondences, we perform a live reg-
istration using the initial correspondences followed by an-
other round of nearest-neighbor search (NN-Search) to fur-
ther dig out and amplify correct correspondences, readied
for supervision of the student.We evaluate EYOC design with trace-driven experi-
ments on three major self-driving datasets, i.e., KITTI [16],
nuScenes [6], and WOD [43]. EYOC reaps comparable
performance with state-of-the-art (SOTA) fully supervised
registration methods while outwitting them by 17.4%mean
registration recall in an out-of-domain unlabelled setting.
To summarize, our contributions are listed as follows:
‚Ä¢ We analyzed the near-far diversity of point clouds, where
low-density regions of a point cloud produce consistent
feature correspondences during a distance extension step.
‚Ä¢ We propose an unsupervised distant point cloud registra-
tion method that can effectively adapt to new data distri-
butions without pose labels or other input modalities.
‚Ä¢ The performance and applicability of EYOC are validated
with extensive experiments on three self-driving datasets.
2. Related Work
2.1. Supervised Registration
Recent registration techniques are highly monopolized by
learning based methods [2‚Äì4, 9, 11, 19, 20, 24, 28, 29, 31,
35, 37, 53, 54, 57], due to both superior accuracy and faster
inference speed compared with traditional extractors [22,
41, 46] or pose estimators such as RANSAC [14].
Local feature extractors. Correspondence-based local
feature extractors have long been diverged into patch-based
methods [2, 11, 19, 35, 57] and fully-convolutional methods
[4, 9, 20, 28, 29]. 3DMatch [57] initiated the patch-based
genre, while PointNet [36], smoothed density value and re-
construction were later introduced by PPF-Net [11], Per-
fectMatch [19], and DIP [35], respectively. The recent pin-
nacle SpinNet [2] and BUFFER [3] combine SO(2) equiva-
lent cylindrical features with fully convolutional backbones.
On the other hand, following FCGF [9], fully convolu-
tional methods process the point cloud as a whole. KP-
Conv [45] backbones are equipped with keypoint detec-
tion in D3Feat [4] and overlap attention in Predator [20].
APR [28] and GCL [29] further enhanced outdoor distant
low-overlap registration with reconstruction and group-wise
contrastive learning. We build our method upon fully con-
volutional methods because they are deemed most suitable
for fast and generalizable outdoor registration.
Pose estimators. Pose estimators [5, 7, 10, 14, 25, 59, 60]
take in feature maps and output the most probable pose esti-
mation, where RANSAC [14] is a common time-consuming
baseline. While DGR [10], PointDSC [5] and DHVR
[25] opted for learned correspondence weight with FCNs,
Non-local Module, and Hough V oting, respectively, non-
parametric methods such as SC2-PCR [7] and MAC [59]
hit higher marks through the second order compatibility or
maximal clique search.
20817
Correspondence 
Filtering
Feature mapStudent Labeler
EMADirty 
Corr.Lowe 
Filtering
(optional)Spatial 
Filtering
SC2-PCRSpeculative 
Registration
‡∑†ùëÖ&∆∏ùë° NN-SearchClean
Corr.Correspondence
Rediscovery
Training
Correspondence
based LossInference
(student only)
ùëÖ&ùë°Point Clouds
ùëá
ùëÜ
SC2-PCR
Interval ùêà‚ààùüè,ùêÅ,ùêÅ‚àùùêûùê©ùê®ùêúùê°
LiDAR sweepsùëá
ùëÜ
Figure 2. Overview of Extend Your Own Correspondences (EYOC) . It exhibits a two-branch student-labeler structure with periodic
synchronization, where the labeler generates correspondences for the student. Point cloud pairs are selected at random frame interval,
whose range extends with time. Labeler dirty correspondences are filtered before the speculative registration which outputs an estimated
pose. Finally, correspondence rediscovery with NN-search on re-aligned input point clouds recovers clean correspondence labels.
Keypoint-free registration. Keypoint-free methods bor-
rowed the idea of superpixels [17] from image matching to
match heavily downsampled points ( i.e., superpoints), each
representing a local patch [24, 31, 37, 53, 54, 56]. HReg-
Net [31] proposed to refine global pose with different stages
of downsampling. CoFiNet [54], GeoTransformer [37], and
PEAL [56] treat superpoints as seeds and match promising
seed patches only. Another line of work, DeepPRO [24] and
REGTR [53], regress correspondences directly without fea-
ture matching. However, their assumption that superpoint
patches should share high overlap no longer holds consid-
ering extreme density-variation and low-overlap.
2.2. Unsupervised Registration
Compared with supervised methods, unsupervised registra-
tion is less explored especially for the outdoor scenario.
BYOC [12] highlighted that random 2D CNNs could gen-
erate image correspondences good enough to supervise a
3D network, therefore indoor RGB-D images are used for
self-supervision. UnsuperisedR&R [13] in turn seeked help
from differentiable rendering of RGB-D images as mutual
supervision after differentiable registration. UDPReg [32]
enforced multiple losses on GMM matching to generate
correspondences for indoor point clouds. However, outdoor
unsupervised registration remain an exciting yet unexplored
field of research, calling for more work on this area.
3. Problem Definition
Given two point clouds S ‚ààRn√ó3,T ‚ààRn√ó3, point cloud
registration aims to uncover their relative transformation
R‚ààSO(3), t‚ààR3so that SRT+tTaligns with T. When
the LiDARs are placed on two distant vehicles separated at
a distance of d‚àà[5m,50m], the sub-problem is referred to
asdistant point cloud registration [28]. Contrary to previoussettings [20, 24, 53], distant point clouds share extreme low-
overlap and density-variation leading to network divergence
when directly applied to training. This is usually mitigated
through a staged training strategy with pretraining on high-
overlap pairs and finetuning on low-overlap pairs [28].
4. Method
The overview of EYOC is illustrated in Fig. 2, which com-
poses of a siamese student-labeler network structure fol-
lowed by correspondence filtering, speculative registration,
and correspondence rediscovery. During training, two dis-
tant point clouds, S,T, are fed into the student and labeler
networks to extract point-wise features Fstu
S, Flab
S‚ààRn√ók
andFstu
T, Flab
T‚ààRm√ók. The labeler features are then pro-
cessed by correspondence filtering to obtain a decent cor-
respondence set Clab={(i, j)|pi‚àà S, qj‚àà T } . It is
later fed into speculative registration to decide an optimal
transformation ÀÜR‚ààSO(3),ÀÜt‚ààR3between SandT. The
high-fidelity estimated transformation is used to re-align in-
put point clouds, which allows us to rediscover correspon-
dences using NN-Search for supervision of the student.
4.1. Extension of Point Cloud Distance
Unlike the supervised setting, it is impossible to calcu-
late the accurate distance between LiDARs in the unsu-
pervised setting. However, leveraging the spatial locality
of LiDAR sequences, we can limit the translational upper
bound by limiting the frame interval Ibetween two frames
in a sequence. Improving upon the staged training strat-
egy [28], we propose to randomly select the frame interval
I‚ààN+, I‚àà[1, B]for every pair, where Bgrows from 1
to 30 during the course of training, forming 30 tiny steps.
When B= 1, we assume identity transformation and ap-
ply supervised training. Our progressive distance extension
20818
Figure 3. The dirty correspondence labels generated by closer-
range labeler (Left: B= 1; Right: B= 10 ) on farther-apart
point clouds (Left: d= 10 m; Right: d= 30 m) in KITTI
[16] before spatial filter. Correct ones are colored green and false
ones red. Close-to-LiDAR features are less generalizable to farther
pairs than far-from-LiDAR features.
strategy increases the problem difficulty gradually to facili-
tate smooth convergence.
4.2. Labeler-Student Feature Extraction
Given a pair of distant point clouds, we pass them through
two homogeneous 3D sparse convolutional backbones pa-
rameterized by WlabandWstu, to obtain point-wise feature
maps. The student is periodically updated to the labeler in
a gradual manner of exponential moving average (EMA),
which keeps the labeler both stable and up-to-date, facili-
tating consistent label generation. Specifically, we update
the labeler weights as in Eq. (1) after every epoch, where
Œª‚àà[0,1)is a decay factor:
Wlab
t+1‚Üê‚àíŒªWlab
t+ (1‚àíŒª)Wstu
t (1)
4.3. Correspondence Filtering
The correspondence filtering module aims to maximize the
portion of correct correspondences produced by the labeler
to enable unsupervised label generation. Different from
BYOC, random 3D CNNs cast much worse correspon-
dences than random 2D CNNs [12, 40, 47], so the dirty
correspondences obtained by matching 3D labeler features
Flab
SandFlab
Tis likely to be rife with different fault patterns
from RGB-D images. With that in mind, we investigate two
types of filtering techniques on both feature space and Eu-
clidean space based on data-centric observations.
Lowe filtering. Previous literature [12, 13] have found
Lowe‚Äôs Ratio [30] a good match for rating the most unique
correspondences on indoor RGB-D point clouds. Specifi-
cally, given two corresponding features fi
S‚ààFlab
S, fj
T‚àà
Flab
T, the significance is calculated according to Eq. (2),
where D(¬∑,¬∑)denotes the cosine similarity. Contrary to pre-
vious literature, we find Lowe filtering to deteriorate corre-
spondence quality drastically as discussed in Sec. 5.3.
œâi,j= 1‚àíD(fi
S, fj
T)
minfk
T‚ààFlab
T,kÃ∏=jD(fi
S, fk
T)(2)
(b)
(c)
 (d)
(a)
ùëë ùëë‚Ä≤ùë•Œîùúéùëù
ùë•ŒîùúéùëûFigure 4. Visual groundings for our hypothesis on KITTI [16].
(a) Density of close-to-LiDAR points are more sensitive to move-
ment than far-from-LiDAR points. (b-d) Cosine similarity of cor-
respondences with its distance to two LiDARs, d1, d2, under (b)
I‚àà[1,1], (c)I‚àà[1,15], and (d) I‚àà[1,30].
Spatial characteristic of labeler correspondences. In
response to the failure of Lowe filtering, we conduct a label-
driven investigation based on the the near-far diversity phe-
nomenon , where far objects should have more consistent
densities when the viewpoint undergoes displacements. We
hereby examine the quality of raw feature correspondences
for a labeler model on farther-apart point clouds than those
in the labeler‚Äôs training set, as depicted in Fig. 3, and pro-
pose the following hypothesis:
Hypothesis 4.1 Correct correspondences are more likely
to be clustered in low-density regions far from the LiDARs
during the distance extension.
Proof. We provide the rationale of a simplified case here
based on the LiDAR sensor model [23]. A LiDAR can
be modeled as a light source emitting light uniformly in
all directions, and the probability density of a point be-
ing scanned is proportional to its energy absorption rate.
Specifically, given two points in the world coordinate p=
(d,0,0)T, q= (d‚Ä≤,0,0)T,0< d < d‚Ä≤and the current
LiDAR center O= (0 ,0,0)T, their respective densities
areœÉp=Œ±
d2, œÉq=Œ±
d‚Ä≤2, where Œ±is an unknown con-
stant depending on the LiDAR resolution and incident an-
gle, which we assume are the same for pandq. Sup-
pose the LiDAR center now moves to O‚Ä≤= (x,0,0)T
where 0< x < d < d‚Ä≤, the delta densities are ‚àÜœÉp=
Œ±
(d‚àíx)2‚àíŒ±
d2,‚àÜœÉq=Œ±
(d‚Ä≤‚àíx)2‚àíŒ±
d‚Ä≤2. It is easy to prove
that‚àÜœÉp>‚àÜœÉq, as illustrated in Fig. 4(a). As widely
acknowledged, CNN features are sensitive to density varia-
tion [20, 37, 44, 49, 51], therefore making close-range point
features less robust under vehicle translation.
20819
Spatial filtering design. Based on the findings, we quan-
titatively explore the relationship between distance from a
correspondence to two LiDARs denoted by d1, d2, and the
cosine similarity of that feature correspondence, as depicted
in Fig. 4(b-d). We refer readers to Appendix Sec. 11.1 for
similar results on other datasets. We confirm that close-to-
LiDAR regions contain most correspondences, but are con-
sistently under-performing and, therefore, could be purged
to improve supervision quality. We hereby propose two sets
of spatial filtering strategies:
‚Ä¢Hard : Discard points where min(d1, d2)< dthresh , re-
gardless of training progression;
‚Ä¢Adaptive : Discard regions with ‚â§sthresh similarity in
Fig. 4, where the decision boundary at sthresh = 0.6
is highlighted in cyan. The similarities are exhaustively
recorded from the pretraining dataset.
Emperically, both methods suffice to cut over 70% of
the false correspondences while only 9% correct correspon-
dences are discarded.
4.4. Speculative Registration
After the correspondence filtering, we adopt a SOTA reg-
istration algorithm SC2-PCR [7] for accurate real-time reg-
istration to amplify the most promising set of correspon-
dences. Although the correspondences have been heavily
cleansed down to several hundred pairs, only 20% among
which are correct on average, which is below the bar for
direct supervision as discussed in Sec. 5.3; However, lit-
erature has shown that this ratio is high enough for a suc-
cessful registration [7, 59]. Intuitively, if the input point
clouds could be correctly registered, we could imitate fully-
supervised training where correspondences are obtained di-
rectly from aligned input point clouds instead of matched
features. Moreover, the searched nearest neighbors can
vastly outnumber the heavily filtered labeler correspon-
dences, making the training process more data-efficient.
Therefore, we propose to obtain an estimated pose ÀÜR‚àà
SO(3),ÀÜt‚ààR3between the input point clouds S,Ton the
fly with real-time registration algorithms.
4.5. Correspondence Rediscovery
With the input point clouds S, T and the estimated trans-
formation ÀÜR‚ààSO(3),ÀÜt‚ààR3, we could simply follow
supervised training to search correspondences for dense su-
pervision. Specifically, we transform S‚Ä≤=RTS+tT, and
obtain the nearest neighbors according to Eq. (3), where
Œ≤inlier = 2mis a loosened match threshold to tolerate mi-
nor pose errors.
CST=
(i, j)pi
S‚àà S‚Ä≤, j= arg min
pj
T‚ààT||pi
S‚àípj
T||,

s.t.||pi
S‚Ä≤‚àípj
T||< Œ≤inlier(3)4.6. Loss Design
We adopt the widely-used Hardest-Contrastive Loss [9] as
the training loss for the student. As nearest-neighbor search
is not differentiable, we only back-propagate gradients to
the student but not the labeler. Specifically, the loss is for-
mulated as Eq. (4):
L=1
|CST|X
(i,j)‚ààCST
m+P(fi
S, fj
T)‚àímin
jÃ∏=k‚ààNP(fi
S, fk
T)
+
+1
|CT S|X
(j,i)‚ààCT S
m+P(fj
T, fi
S)‚àímin
iÃ∏=k‚ààNP(fj
T, fk
S)
+
(4)
Where Nis a subset of feature indices, mis the positive
margin, [¬∑]+rounds negative values to 0, P(¬∑,¬∑)denotes the
squared distance between two vectors. CT Sfollows Eq. (3)
but is calculated in the reverse direction from TtoS.
5. Results
We demonstrate the superiority of EYOC against state-of-
the-art methods on three major self-driving datasets, KITTI
[16], nuScenes [6], and WOD [43]. We then provide an
ablation study, finetuning strategies, and time analysis. Vi-
sualizations for the labeler are available in Fig. 6.
5.1. Experiment Setup
Datasets. Apart from our progressive dataset extension
strategy, shorthanded as progressive dataset , we also follow
existing literature [28, 29] to prepare the point cloud pairs
based on the distance between two LiDARs, referred to as
traditional dataset . The latter works under supervised set-
tings, where the point cloud pairs have a random Euclidean
distance between two LiDARs, denoted with d‚àà[M, N ]
in meters. The traditional datasets are also used during
all test sections. On the other hand, progressive datasets
work for either supervised or unsupervised training, where
point cloud pairs are selected with a random frame interval
I‚àà[1, B]due to the absence of pose labels. We set the ini-
tial value to B= 1which grows linearly to B= 30 during
200 epochs. All datasets are cut into train-val-test splits by
official recommendations.
Training. For supervised comparison methods, we fol-
low common practice [28] to train the model on tradi-
tional datasets with d‚àà[5,20]and further finetune on
d‚àà[5,50]. The strategy applies to all baselines, while pre-
trained weights will be used for those whose training does
not converge (denoted with *). On the other hand, EYOC
needs only one course of training thanks to the progressive
dataset. When a labelled pretraining dataset is available, the
parameters of adaptive spatial filtering are acquired with the
20820
TestNo. MethodPretrain FinetuneSupervisedProgressivemRRRR @ d‚àà
Set Dataset Dataset Dataset [5,10] [10,20] [20,30] [30,40] [40,50]
KITTIaFCGF [9] WOD - ‚úì - 71.8 98.0 92.5 85.0 52.6 30.7
Predator [20] WOD - ‚úì - 72.3 99.5 98.9 90.9 56.8 15.3
bFCGF [9] - KITTI ‚úì - 77.4 98.4 95.3 86.8 69.7 36.9
FCGF + C - KITTI ‚úì ‚úì 84.6 100.0 97.5 90.1 79.1 56.3
Predator [20] - KITTI ‚úì - 87.9 100.0 98.6 97.1 80.6 63.1
SpinNet* [2] - KITTI ‚úì - 39.1 99.1 82.5 13.7 0.0 0.0
D3Feat* [4] - KITTI ‚úì - 66.4 99.8 98.2 90.7 38.6 4.5
CoFiNet [54] - KITTI ‚úì - 82.1 99.9 99.1 94.1 78.6 38.7
GeoTrans.* [37] - KITTI ‚úì - 42.2 100.0 93.9 16.6 0.7 0.0
c EYOC (ours)- KITTI - ‚úì 83.2 99.5 96.6 89.1 78.6 52.3
WOD KITTI - ‚úì 80.6 99.5 95.6 89.1 75.1 43.7WODdFCGF [9] KITTI - ‚úì - 69.9 97.1 87.9 61.8 59.0 43.9
Predator [20] KITTI - ‚úì - 70.7 98.1 97.6 81.2 53.2 23.6
eFCGF [9] - WOD ‚úì - 89.5 100.0 98.6 91.2 83.5 74.0
FCGF + C - WOD ‚úì ‚úì 77.2 98.1 89.9 75.8 64.7 57.7
Predator [20] - WOD ‚úì - 86.4 100.0 100.0 95.3 79.1 57.7
f EYOC (ours)- WOD - ‚úì 78.4 97.6 91.3 78.2 65.5 59.3
KITTI WOD - ‚úì 77.3 97.1 90.3 75.8 65.5 57.7nuScenesgFCGF [9] WOD - ‚úì - 67.1 98.9 93.9 73.6 42.6 26.3
Predator [20] WOD - ‚úì - 34.5 93.0 55.2 11.8 6.0 6.7
hFCGF [9] - nuScenes ‚úì - 39.5 87.9 63.9 23.6 11.8 10.2
FCGF + C - nuScenes ‚úì ‚úì 59.3 96.2 85.1 59.6 35.8 20.0
Predator [20] - nuScenes ‚úì - 51.0 99.7 72.2 52.8 16.2 14.3
i EYOC (ours)- nuScenes - ‚úì 61.7 96.7 85.6 61.8 37.5 26.9
WOD nuScenes - ‚úì 68.4 98.9 91.7 73.3 44.3 33.7
Table 1. Comparison of mRR(%) and RR (%) between SOTA methods and EYOC over five test sets with d‚àà[b1, b2]on KITTI [16],
WOD [43], and nuScenes [6], respectively , with increasing point cloud distance and registration difficulty. We group the tests denoted
by letters a-i,where c,f,idenotes EYOC, a,d,g are the fair generalization results of supervised methods and b,e,h mark the oracle
supervised performance with labels on the new dataset . EYOC is the only unsupervised method. We use ‚ÄòFCGF + C‚Äô to denote FCGF
trained with progressive datasets, which is a theoretical upper bound for EYOC. All features are registered using RANSAC.
help of pose labels, presumably from KITTI or WOD; Oth-
erwise, we use hard spatial filtering. The complete training
of EYOC consists of 200 epochs with 0.001 learning rate
and1√ó10‚àí4weight decay, same as FCGF, implemented
with MinkowskiEngine [8] and Pytorch3D [39].
Inference. When conducting a comparison with previous
methods, we apply RANSAC [14] on all methods including
EYOC for fairness. Otherwise, we default EYOC inference
to SC2-PCR [7] for speed and performance.
Metrics. We report 5 metrics according to existing litera-
ture [9, 18, 28]: Registration Recall (RR), Relative Rotation
Error (RRE), Relative Translation Error (RTE), Mean RR
(mRR), and Inlier Ratio (IR), the formal definition of which
can be found in Appendix Sec. 7.2. We apply IR on the
generated labeler correspondences to indicate their quality
during training.5.2. Overall Performance
We compare both a generalization setting ( a,d,g ) and fine-
tuning setting ( b,e,h ) for SOTA supervised methods, against
the unsupervised EYOC ( c,f,i) on three datasets, KITTI
[16], WOD [43], and nuScenes [6], respectively in Tab. 1.
We first notice that supervised methods do fail to gen-
eralize to different datasets, according to a-b,d-e, andg-h
in Tab. 1. Generalizing from WOD to KITTI, which are
both 64-line datasets with small domain shift, supervised
methods suffer 5.6%and15.6%mRR drop respectively for
FCGF [9] and Predator [20], when compared with mod-
els trained on KITTI directly (rows aandb). Similar re-
sults are seen generalizing from KITTI to WOD as well
(rows dande), with 19.6%and15.7%mRR drop for FCGF
and Predator, respectively. On the other hand, a harder
dataset, nuScenes with only a 32-laser LiDAR, struggles
to support supervised training. We witness worse super-
vised performance than generalization scores from WOD
20821
1st Epoch [40,50]
No. LF SF-h SF-a SR+CR PD Labeler IR mRR RR RRE RTE
a - - - - ‚úì 5.1
N/Cb‚úì - - - ‚úì 1.5
c‚úì - - ‚úì ‚úì 0.6
d‚úì -‚úì - ‚úì 5.9
e‚úì ‚úì - - ‚úì 5.9
f - - ‚úì ‚úì - 0.0
g‚úì -‚úì ‚úì ‚úì 7.8 87.5 66.8 1.3 29.7
h - - - ‚úì ‚úì 18.4 84.6 60.3 1.4 33.9
i - - ‚úì ‚úì ‚úì 43.3 88.0 68.8 1.3 31.8
j - ‚úì - ‚úì ‚úì 53.2 87.6 67.8 1.31 32.2Œª [40,50]
0.0 71.9
0.1 70.4
0.2 73.9
0.3 71.4
0.4 69.3
0.5 71.9
0.6 69.8
0.7 72.9
0.8 67.3
0.85 58.8
0.9 N/C
0.99 N/C1st Epoch
dthresh Labeler IR
0 18.4
5 18.5
10 25.2
15 31.1
20 31.4
25 29.4
30 45.1
35 49.0
40 53.2
45 43.31st Epoch
sthresh Labeler IR
0.0 18.4
0.1 25.4
0.2 30.7
0.3 31.5
0.4 31.1
0.5 34.9
0.6 43.3
0.7 N/C
0.8 N/C
0.9 N/C
Table 2. Ablation study of EYOC. Labeler IR (%), mRR (%), RR@ [40,50](%), RRE (‚ó¶), and RTE (cm) on KITTI val set are presented.
Lowe Filtering (LF), Spatial Filtering of hard (SF-h) or adaptive (SF-a) strategies, Speculative Registration and Correspondence Redis-
covery (SR+CR), progressive Dataset (PD), EMA decay factor Œª, and two parameters of Spatial Filtering, dthresh , sthresh , are ablated.
for FCGF when comparing the rows gandh. Nonetheless,
their generalization scores are also subpar, hitting merely
67.1%and34.5%mRR with FCGF and Predator, respec-
tively on nuScenes in row g. Additionally, contrary to the
common belief, Predator performs much worse than FCGF
on an out-of-domain dataset, nuScenes, in row g.
Does unsupervised finetuning improve upon supervised
methods on out-of-domain unlabelled data? By com-
paring a-c,d-f, and g-iin Tab. 1, we confirm that EYOC
improves upon fixed supervised models by a consider-
able margin through unsupervised finetuning. On KITTI,
EYOC surpasses raw FCGF, achieving 83.2%(+11 .4%)
and80.6%(+8 .8%) mRR by training from scratch and fine-
tuning, respectively. On WOD and nuScenes, the respective
figures are 78.4%(+8 .5%) and77.3%(+7 .4%) on WOD,
61.7%(‚àí5.3%) and68.4%(+1 .3%) on nuScenes compared
to FCGF. We conclude that, given a pretrained model and
an incoming unlabelled dataset, applying EYOC for unsu-
pervised training/finetuning provides a considerable perfor-
mance boost on the new dataset.
Is EYOC comparable to supervised methods on labelled
data? Unsupervised methods have to perform similarly
to supervised ones in order to be considered valuable.
Through comparing b-c,e-f, and h-iin Tab. 1, we find that
EYOC exhibits comparable performance with SOTA fully-
supervised methods when trained on the same dataset. On
KITTI, mRR of EYOC is only 4.7%and1.4%lower than
that of the best-performing Predator and FCGF+C, respec-
tively. Other low-overlap registration methods, excluding
CoFiNet [54], are less suitable for outdoor scenarios, as
SpinNet [2], D3Feat [4], and Geotransformer [37] suffer
from divergence. In the meantime, different results are re-
ported on WOD where EYOC is 10.9%behind FCGF but
1.2%ahead of FCGF+C, indicating that FCGF+C is not al-
ways effective on all datasets. EYOC echibits stronger re-
sults on nuScenes, surpassing FCGF+C by 9.9%instead.We conclude that EYOC does perform similarly to fully su-
pervised methods while requiring no pose labels at all.
5.3. Ablation
Structural components. We first ablate supporting struc-
tures of EYOC in Tab. 2, including Lowe Filtering (LF),
Spatial Filtering with both hard (SF-h) and adaptive (SF-
a) strategies, Speculative Registration + Correspondence
Rediscovery (SR+CR), and the Progressive Dataset (PD).
Judging from a-bandg-i, Lowe‚Äôs filter deteriorates IR by
3.60% and35.5%, respectively, contrary to previous find-
ings on indoor RGB-D images. We keep Lowe filtering as
an option in case of other datasets. Also, lone Spatial Fil-
tering or speculative registration both fail to support train-
ing according to c,d,e . The best-performing setup ( i) fails
completely without Progressive Dataset ( f) at0.0%IR, in-
dicating the importance of the Progressive Dataset strat-
egy. On the other hand, converged setups reveal consistently
higher IR up to 53.2%. SF-h ( i) and SF-s ( j) achieve 88.0%
and87.6%mRR, respectively, slightly better than not using
Spatial Filtering ( h) which achieves 84.6%mRR. Similar
trends are observed with respective performance on long-
range pairs as well, where iandjoutperforms hby8.5%
and7.5%RR, respectively. We default EYOC structure to
SF-a, SR+CR, and PD ( i) for the highest performance.
Parameter choices. Three parameter choices, Œª,dthresh ,
andsthresh , are discussed in Tab. 2 as well. For the EMA
decay factor Œª, any value less than 0.7achieves similar
results averaging at 71.4%, while larger Œªquickly drains
the performance. On the other hand, similar to our previ-
ous findings Sec. 4.3, IR marks better scores with stricter
thresholds of dthresh andsthresh (i.e., using regions farther
from the LiDAR), but the number of correspondences could
shrink to the point of causing divergence under an extreme
threshold. In light of this phenomenon, we choose Œª= 0.2,
dthresh = 40 m, and sthresh = 0.6are default parame-
ters for the best performance just before the divergence line.
Should a divergence occur on new datasets, these thresholds
20822
/uni00000018/uni00000008/uni0000000f/uni00000015/uni00000011/uni00000013/uni0000004e/uni00000050 /uni00000014/uni00000013/uni00000008/uni0000000f/uni00000016/uni00000011/uni0000001c/uni0000004e/uni00000050 /uni00000015/uni00000013/uni00000008/uni0000000f/uni0000001a/uni00000011/uni0000001b/uni0000004e/uni00000050 /uni00000018/uni00000013/uni00000008/uni0000000f/uni00000014/uni0000001c/uni00000011/uni00000019/uni0000004e/uni00000050 /uni00000014/uni00000013/uni00000013/uni00000008/uni0000000f/uni00000016/uni0000001c/uni00000011/uni00000015/uni0000004e/uni00000050
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057/uni00000003/uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000035/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000050/uni00000035/uni00000035
/uni00000035/uni00000035/uni00000023/uni0000003e/uni00000017/uni00000013/uni0000000f/uni00000018/uni00000013/uni00000040
/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni00000048
/uni00000036/uni00000046/uni00000055/uni00000044/uni00000057/uni00000046/uni0000004bFigure 5. Comparison between finetuning from WOD and
training from scratch for EYOC , with the first 5% to 100% of
unlabelled KITTI, where both RR on d‚àà[40,50]and mRR are
displayed. The horizontal axis is in log scale. Finetuning exhibits
more stability before 20%, while training from scratch performs
better after 50%.
Training (one pass) # Training
Data NN-S Feat. Label Gen. Loss Total Required
FCGF [9] 692 - 128 - 356 1176 √ó2
FCGF* 17 33 152 - 301 503 √ó2
EYOC 18 - 170 381 296 865 √ó1
Table 3. Time analysis of EYOC, FCGF [9], and FCGF with GPU-
accelerated NN-Search (denoted with *) in milliseconds. The
number of complete training routines required for a network is
listed in the last column.
could be lowered to cater to new data distributions.
5.4. Finetuning versus From Scratch
We further compare the finetuning and training-from-
scratch strategies for EYOC with different portions of the
new dataset KITTI, while assuming a pretrained model on
WOD is available. Metrics including RR @ d‚àà[40,50],
mRR, and driving distance (km) on KITTI are displayed
with the first 5%to100% of KITTI, as illustrated in Fig. 5.
Overall, performance of both methods increase with the
amount of training data; However, finetuning grants more
stability by inheriting knowledge from the previous dataset,
therefore performing better with smaller datasets then 20%
(7.8km) where the mRR stablizes around 75%. On the
other hand, training from scratch achieves better results af-
ter50% (19.6km), peaking at the full dataset with 89.1%
mRR and 72.2%RR @ d‚àà[40,50], respectively. We con-
clude that finetuning is better for datasets roughly shorter
than 10km, while training from scratch would be a better
choice for larger datasets.
5.5. Time Analysis
We break down the training time for FCGF [9] and EYOC
in Tab. 3. Because the NN-search in Correspondence Redis-
covery of EYOC is accelerated with GPU using Pytorch3D
Figure 6. Visualization of clean correspondence labels on
KITTI (top row), nuScenes (middle row), and WOD (bottom
row) , where correspondences with ‚â§1mlocation error are col-
ored green and otherwise red. Even when Speculative Registration
fails, most of the false correspondences are in parallel to correct
ones, they are just not precise but still informative.
[39], it is necessary to apply the same trick to the base-
line FCGF for fair comparison, which is termed ‚ÄòFCGF*‚Äô.
While EYOC needs an additional 381ms for label gener-
ation, it completes training once and for all, resulting in
the lowest total training time. On the other hand, FCGF*
is trained twice to prevent divergence [28] as detailed in
Sec. 5.1. In comparison, vanilla FCGF ranks the slowest
due to a prolonged data loading time of 692ms. We con-
clude that EYOC enjoys a lower training cost than its su-
pervised counterpart.
6. Conclusion
We have proposed EYOC, an unsupervised distant point
cloud registration technique that requires nothing more than
consecutive LiDAR sweeps, which is easily acquired on-
the-fly with self-driving vehicles. With the correspondence
filtering pipeline built upon our investigations, EYOC al-
lows a 3D feature extractor to generate labels for itself,
enabling fully unsupervised training. Extensive experi-
ments demonstrate that, while enjoying comparable perfor-
mance to supervised methods, EYOC also has a lower train-
ing cost, thus being preferable compared to the traditional
‚Äòmanual labelling + supervised training‚Äô paradigm. EYOC‚Äôs
unrivalled capability of finetuning on new data distributions
marks a step towards the mass deployment of collaborative
sensing on SDVs.
Acknowledgement. This work was supported in part
by the Natural Science Foundation of Shanghai (Grant
No.22ZR1400200) and the Fundamental Research Funds
for the Central Universities (No. 2232023Y-01).
20823
References
[1] Sk Aziz Ali, Kerem Kahraman, Gerd Reis, and Didier
Stricker. RPSRNet: End-to-end trainable rigid point set reg-
istration network using Barnes-Hut 2D-Tree representation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 13100‚Äì13110, 2021.
1
[2] Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham,
and Yulan Guo. SpinNet: Learning a general surface de-
scriptor for 3D point cloud registration. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11753‚Äì11762, 2021. 2, 6, 7, 1
[3] Sheng Ao, Qingyong Hu, Hanyun Wang, Kai Xu, and Yu-
lan Guo. BUFFER: Balancing accuracy, efficiency, and gen-
eralizability in point cloud registration. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1255‚Äì1264, 2023. 2
[4] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan,
and Chiew-Lan Tai. D3Feat: Joint learning of dense detec-
tion and description of 3D local features. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6359‚Äì6367, 2020. 2, 6, 7, 1
[5] Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li,
Zeyu Hu, Hongbo Fu, and Chiew-Lan Tai. PointDSC: Ro-
bust point cloud registration using deep spatial consistency.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 15859‚Äì15869, 2021.
2
[6] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuScenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2020. 1, 2, 5, 6, 3
[7] Zhi Chen, Kun Sun, Fan Yang, and Wenbing Tao. SC2-PCR:
A second order spatial compatibility for efficient and robust
point cloud registration. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 13221‚Äì13231, 2022. 2, 5, 6, 1
[8] Christopher Choy, JunYoung Gwak, and Silvio Savarese.
4d spatio-temporal convnets: Minkowski convolutional neu-
ral networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3075‚Äì
3084, 2019. 6
[9] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully
convolutional geometric features. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
2019. 1, 2, 5, 6, 8
[10] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep
global registration. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2514‚Äì2523, 2020. 2
[11] Haowen Deng, Tolga Birdal, and Slobodan Ilic. PPFNet:
Global context aware local features for robust 3D point
matching. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 195‚Äì205,
2018. 2[12] Mohamed El Banani and Justin Johnson. Bootstrap your own
correspondences. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 6433‚Äì6442,
2021. 1, 3, 4
[13] Mohamed El Banani, Luya Gao, and Justin Johnson. Unsu-
pervisedR&R: Unsupervised point cloud registration via dif-
ferentiable rendering. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7129‚Äì7139, 2021. 1, 3, 4
[14] Martin A Fischler and Robert C Bolles. Random sample
consensus: a paradigm for model fitting with applications to
image analysis and automated cartography. Communications
of the ACM , 24(6):381‚Äì395, 1981. 2, 6
[15] Kexue Fu, Shaolei Liu, Xiaoyuan Luo, and Manning Wang.
Robust point cloud registration framework based on deep
graph matching. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 8893‚Äì
8902, 2021. 1
[16] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the KITTI vision benchmark
suite. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , 2012. 2, 4, 5, 6
[17] R ¬¥emi Giraud, Vinh-Thong Ta, Aur ¬¥elie Bugeau, Pierrick
Coup ¬¥e, and Nicolas Papadakis. SuperPatchMatch: An algo-
rithm for robust correspondences using superpixel patches.
IEEE Transactions on Image Processing , 26(8):4068‚Äì4078,
2017. 3
[18] Zan Gojcic, Caifa Zhou, and Andreas Wieser. Learned com-
pact local feature descriptor for tls-based geodetic monitor-
ing of natural outdoor scenes. International Archives of the
Photogrammetry, Remote Sensing and Spatial Information
Sciences , 4:113‚Äì120, 2018. 6
[19] Zan Gojcic, Caifa Zhou, Jan D Wegner, and Andreas Wieser.
The perfect match: 3D point cloud matching with smoothed
densities. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5545‚Äì
5554, 2019. 2
[20] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas
Wieser, and Konrad Schindler. PREDATOR: Registration
of 3D point clouds with low overlap. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4267‚Äì4276, 2021. 1, 2, 3, 4, 6
[21] Behley Jens, Garbade Martin, Milioto Andres, Quenzel
Jan, Behnke Sven, Stachniss Cyrill, and Gall Jurgen. Se-
manticKITTI: A dataset for semantic scene understanding of
LiDAR sequences. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , 2019. 1
[22] Andrew E Johnson and Martial Hebert. Using spin images
for efficient object recognition in cluttered 3D scenes. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
21(5):433‚Äì449, 1999. 2
[23] Felix J ¬®aremo Lawin, Martin Danelljan, Fahad Shahbaz
Khan, Per-Erik Forss ¬¥en, and Michael Felsberg. Den-
sity adaptive point set registration. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3829‚Äì3837, 2018. 4
[24] Donghoon Lee, Onur C Hamsici, Steven Feng, Prachee
Sharma, and Thorsten Gernoth. DeepPRO: Deep partial
20824
point cloud registration of objects. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5683‚Äì5692, 2021. 1, 2, 3
[25] Junha Lee, Seungwook Kim, Minsu Cho, and Jaesik Park.
Deep hough voting for robust global registration. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 15994‚Äì16003, 2021. 2
[26] Yang Li and Tatsuya Harada. Lepard: Learning partial point
cloud matching in rigid and deformable scenes. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5554‚Äì5564, 2022. 1
[27] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. BEVFormer:
Learning bird‚Äôs-eye-view representation from multi-camera
images via spatiotemporal transformers. In European Con-
ference on Computer Vision , pages 1‚Äì18. Springer, 2022. 1
[28] Quan Liu, Yunsong Zhou, Hongzi Zhu, Shan Chang, and
Minyi Guo. APR: Online distant point cloud registration
through aggregated point cloud reconstruction. In Proceed-
ings of the International Joint Conference on Artificial Intel-
ligence , pages 1204‚Äì1212. International Joint Conferences
on Artificial Intelligence Organization, 2023. Main Track.
1, 2, 3, 5, 6, 8
[29] Quan Liu, Hongzi Zhu, Yunsong Zhou, Hongyang Li, Shan
Chang, and Minyi Guo. Density-invariant features for distant
point cloud registration. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 18215‚Äì
18225, 2023. 1, 2, 5, 3
[30] H Christopher Longuet-Higgins. A computer algorithm for
reconstructing a scene from two projections. Nature , 293
(5828):133‚Äì135, 1981. 4
[31] Fan Lu, Guang Chen, Yinlong Liu, Lijun Zhang, Sanqing
Qu, Shu Liu, and Rongqi Gu. HRegNet: A hierarchical net-
work for large-scale outdoor LiDAR point cloud registration.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 16014‚Äì16023, 2021. 1, 2, 3
[32] Guofeng Mei, Hao Tang, Xiaoshui Huang, Weijie Wang,
Juan Liu, Jian Zhang, Luc Van Gool, and Qiang Wu. Unsu-
pervised deep probabilistic approach for partial point cloud
registration. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13611‚Äì
13620, 2023. 1, 3
[33] Michael Montemerlo, Sebastian Thrun, Daphne Koller, Ben
Wegbreit, et al. FastSLAM: A factored solution to the simul-
taneous localization and mapping problem. Association for
the Advancement of Artificial Intelligence / Innovative Appli-
cations of Artificial Intelligence Conference , 593598, 2002.
1
[34] Raul Mur-Artal and Juan D Tard ¬¥os. Orb-slam2: An open-
source slam system for monocular, stereo, and RGB-D cam-
eras. IEEE Transactions on Robotics , 33(5):1255‚Äì1262,
2017. 1
[35] Fabio Poiesi and Davide Boscaini. Distinctive 3D local deep
descriptors. In Proceedings of the International Conference
on Pattern Recognition , pages 5720‚Äì5727. IEEE, 2021. 2
[36] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
PointNet: Deep learning on point sets for 3D classificationand segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
652‚Äì660, 2017. 2
[37] Zheng Qin, Hao Yu, Changjian Wang, Yulan Guo, Yuxing
Peng, and Kai Xu. Geometric transformer for fast and robust
point cloud registration. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 11143‚Äì11152, 2022. 1, 2, 3, 4, 6, 7
[38] Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen, and
Xi Li. UniFusion: Unified multi-view fusion transformer
for spatial-temporal representation in bird‚Äôs-eye-view. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8690‚Äì8699, 2023. 1
[39] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-
lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia
Gkioxari. Accelerating 3D deep learning with PyTorch3D.
arXiv:2007.08501 , 2020. 6, 8
[40] Amir Rosenfeld and John K Tsotsos. Intriguing properties
of randomly weighted networks: Generalizing while learn-
ing next to nothing. In Proceedings of the Conference on
Computer and Robot Vision , pages 9‚Äì16. IEEE, 2019. 4
[41] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast
point feature histograms (FPFH) for 3D registration. In Pro-
ceedings of the IEEE International Conference on Robotics
and Automation , pages 3212‚Äì3217. IEEE, 2009. 2
[42] Martin Simon, Karl Amende, Andrea Kraus, Jens Honer,
Timo Samann, Hauke Kaulbersch, Stefan Milz, and Horst
Michael Gross. Complexer-YOLO: Real-time 3D object de-
tection and tracking on semantic point clouds. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops , pages 0‚Äì0, 2019. 1
[43] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,
Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Et-
tinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang,
Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov.
Scalability in perception for autonomous driving: Waymo
Open Dataset. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , 2020.
1, 2, 5, 6, 3
[44] Gusi Te, Wei Hu, Amin Zheng, and Zongming Guo.
RGCNN: Regularized graph cnn for point cloud segmenta-
tion. In Proceedings of the ACM international conference on
Multimedia , pages 746‚Äì754, 2018. 4
[45] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc ¬∏ois Goulette, and Leonidas J
Guibas. KPConv: Flexible and deformable convolution for
point clouds. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 6411‚Äì6420, 2019. 2
[46] Federico Tombari, Samuele Salti, and Luigi Di Stefano.
Unique signatures of histograms for local surface descrip-
tion. In Proceedings of the European Conference on Com-
puter Vision , pages 356‚Äì369. Springer, 2010. 2
[47] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
Deep image prior. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 9446‚Äì
9454, 2018. 4
20825
[48] Yue Wang and Justin M Solomon. PRNet: Self-supervised
learning for partial-to-partial registration. Advances in Neu-
ral Information Processing Systems , 32, 2019. 1
[49] Wenxuan Wu, Zhongang Qi, and Li Fuxin. PointConv: Deep
convolutional networks on 3D point clouds. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 9621‚Äì9630, 2019. 4
[50] Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter
Vajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeeze-
SegV3: Spatially-adaptive convolution for efficient point-
cloud segmentation. In Proceedings of the European Con-
ference on Computer Vision , pages 1‚Äì19. Springer, 2020. 1
[51] Zi Jian Yew and Gim Hee Lee. 3DFeat-Net: Weakly super-
vised local 3D features for point cloud registration. In Pro-
ceedings of the European Conference on Computer Vision ,
pages 607‚Äì623, 2018. 4
[52] Zi Jian Yew and Gim Hee Lee. RPM-Net: Robust point
matching using learned features. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11824‚Äì11833, 2020. 1
[53] Zi Jian Yew and Gim Hee Lee. REGTR: End-to-end point
cloud correspondences with transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6677‚Äì6686, 2022. 1, 2, 3
[54] Hao Yu, Fu Li, Mahdi Saleh, Benjamin Busam, and Slobo-
dan Ilic. CoFiNet: Reliable coarse-to-fine correspondences
for robust pointcloud registration. Advances in Neural Infor-
mation Processing Systems , 34:23872‚Äì23884, 2021. 1, 2, 3,
6, 7
[55] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang,
Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui
Yuan, et al. DAIR-V2X: A large-scale dataset for vehicle-
infrastructure cooperative 3D object detection. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 21361‚Äì21370, 2022. 1
[56] Junle Yu, Luwei Ren, Yu Zhang, Wenhui Zhou, Lili Lin, and
Guojun Dai. PEAL: Prior-embedded explicit attention learn-
ing for low-overlap point cloud registration. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 17702‚Äì17711, 2023. 1, 3
[57] Andy Zeng, Shuran Song, Matthias Nie√üner, Matthew
Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3DMatch:
Learning local geometric descriptors from RGB-D recon-
structions. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1802‚Äì
1811, 2017. 2
[58] Xumiao Zhang, Anlan Zhang, Jiachen Sun, Xiao Zhu,
Y Ethan Guo, Feng Qian, and Z Morley Mao. EMP: Edge-
assisted multi-vehicle perception. In Proceedings of the 27th
Annual International Conference on Mobile Computing and
Networking , pages 545‚Äì558, 2021. 1
[59] Xiyu Zhang, Jiaqi Yang, Shikun Zhang, and Yanning Zhang.
3D registration with maximal cliques. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17745‚Äì17754, 2023. 2, 5
[60] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global
registration. In Proceedings of the European Conference on
Computer Vision , pages 766‚Äì782. Springer, 2016. 2[61] Hanqi Zhu, Jiajun Deng, Yu Zhang, Jianmin Ji, Qiuyu Mao,
Houqiang Li, and Yanyong Zhang. VPFNet: Improving 3D
object detection with virtual point based LiDAR and stereo
data fusion. IEEE Transactions on Multimedia , 2022. 1
20826
