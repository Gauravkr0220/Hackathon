Improving Spectral Snapshot Reconstruction with Spectral-Spatial Rectification
Jiancheng Zhang1, Haijin Zeng2, Yongyong Chen3, Dengxiu Yu1, Yin-Ping Zhao1,*
1Northwestern Polytechnical University,2IMEC-UGent,3Harbin Institute of Technology (Shenzhen)
Abstract
How to effectively utilize the spectral and spatial char-
acteristics of Hyperspectral Image (HSI) is always a key
problem in spectral snapshot reconstruction. Recently, the
spectra-wise transformer has shown great potential in cap-
turing inter-spectra similarities of HSI, but the classic de-
sign of the transformer, i.e., multi-head division in the spec-
tral (channel) dimension hinders the modeling of global
spectral information and results in mean effect. In addition,
previous methods adopt the normal spatial priors without
taking imaging processes into account and fail to address
the unique spatial degradation in snapshot spectral recon-
struction. In this paper, we analyze the influence of multi-
head division and propose a novel Spectral-Spatial Recti-
fication (SSR) method to enhance the utilization of spec-
tral information and improve spatial degradation. Specifi-
cally, SSR includes two core parts: Window-based Spectra-
wise Self-Attention (WSSA) and spAtial Rectification Block
(ARB). WSSA is proposed to capture global spectral in-
formation and account for local differences, whereas ARB
aims to mitigate the spatial degradation using a spatial
alignment strategy. The experimental results on simula-
tion and real scenes demonstrate the effectiveness of the
proposed modules, and we also provide models at multi-
ple scales to demonstrate the superiority of our approach.
https://github.com/ZhangJC-2k/SSR
1. Introduction
The advent of compressed sensing introduced the coded
aperture snapshot spectral compressive imaging (CASSI)
system [13, 29, 36], addressing drawbacks in tradi-
tional hyperspectral cameras regarding efficiency and cost-
effectiveness, gaining significant attention. This system
offers the promise of swift and cost-efficient capture of
Hyperspectral Images (HSIs). However, the inherent ill-
posedness of decoding 3D HSIs from 2D measurements
poses a significant challenge for reconstruction algorithm
design. Consequently, various model-based [1, 23, 24, 33,
*Corresponding Author
Figure 1. Comparison of PSNR-FLOPs between our SSR and pre-
vious reconstruction methods. Our SSR significantly outperforms
other methods over 2dB at the same FLOPs when surpassing the
state-of-the-art method over 1dB with fewer FLOPs.
38, 40] and learning-based [2‚Äì6, 17, 18, 28, 37, 42] ap-
proaches have emerged to tackle this challenge. Within the
realm of reconstruction algorithms, the effective utilization
of diverse priors remains a pivotal research challenge.
Model-based methods initially adopt total variation [20,
40], low-rank [24], sparsity [21, 34] and more traditional
hand-craft priors [1, 23, 24, 33, 38] to construct the op-
timization model, which preliminarily shows the practical
feasibility of snapshot spectral reconstruction. With the ad-
vancement of deep learning, both convolutional neural net-
works (CNNs) [6, 17, 18, 28, 37, 42] and transformers [3‚Äì
5, 11] have been successively introduced for learning the
spatial and spectral characteristics of HSIs. They serve as
end-to-end denoisers [3, 4, 6, 17, 28] or deep image pri-
ors [5, 11, 18, 37, 42] in learning-based methods. These
advancements have led to significant progress in both real-
world and simulated scenario reconstructions. In addition,
the effective utilization of spectral and spatial characteris-
tics has also naturally become a research hotspot.
Previous CNN methods such as TSA-Net [28], HDNet
[17], DGSMP [18], and HerosNet [42], along with spatial
transformers like Swin Transformer [25] and DAUHST [5],
excel in learning spatial representations but do not effec-
tively utilize the distinctive spectral characteristics inher-
ent in HSIs. Conversely, spectra-wise transformers such as
MST [3], MST++ [4], and RDLUF [11] show promising
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25817
potential in snapshot spectral reconstruction by exploiting
inter-spectral similarities. However, their implementations
follow the conventional transformer architecture, involving
multi-head division in the spectral dimension, which lim-
its the modeling of global spectral similarities and results
in mean effect. Furthermore, in spectral snapshot imag-
ing, some spatial degradation occurs in certain bands due to
masking, shifting, and compression factors. Unfortunately,
existing spatial and spectral networks lack tailored designs
to mitigate such degradation effectively.
To optimize the utilization of spectral information and
mitigate distinctive spatial degradation, we introduce the
Spectral-Spatial Rectification (SSR) method into snapshot
spectral reconstruction. Specifically, SSR comprises two
key components: Window-based Spectral Self-Attention
(WSSA) and spAtial Rectification Blocks (ARB). WSSA
divides the features into multiple local windows in the
spatial dimension, facilitating global spectral self-attention
computations and the local difference consideration. Ad-
dressing the interaction gap between windows of WSSA,
ARB leverages a Convolution Modulated Block (CMB).
This block employs sliding large-kernel convolutions to
interact with features between windows and learn effec-
tive spatial representation. Subsequently, to mitigate spa-
tial degradation, ARB integrates a novel spatial alignment
strategy. It assists low-quality bands in leveraging infor-
mation from high-quality bands through learning a unified
spatial representation and spectral weights, which further
optimizes the utilization of spatial information. Overall, our
contributions are summarized as follows:
‚Ä¢ We analyze the influence of multi-head division on pre-
vious spectra-wise transformers and propose Window-
based Spectra-wise Self-Attention to model global spec-
tral information while accounting for local differences.
‚Ä¢ A Spatial Rectification Block is specially designed to
enhance spatial representation and mitigate the spatial
degradation in low-quality bands through large-kernel
convolutions and a novel spatial alignment strategy.
‚Ä¢ The qualitative and quantitative results of real and simu-
lation experiments demonstrate that our SSR method sig-
nificantly improves spectral snapshot reconstruction.
2. Related Work
The model-based methods [1, 20, 21, 23, 24, 33, 34, 38, 40]
construct convex optimization models with some specific
priors such as total variation [20, 40], low-rankness [24],
and sparsity [21, 34] according to the physical schema of
CASSI then solve the problem in an iterative manner to ob-
tain reconstructed images. They have good interpretabil-
ity and low cost, yet lack performance guarantee. Subse-
quently, the rise of deep learning [14, 15, 22] has facil-
itated the rapid development of spectral snapshot recon-
struction. To improve reconstruction quality, Plug-and-Playmethods [41, 44] plug pre-trained deep networks as priors,
which make great progress in performance but are still lim-
ited by slow reconstruction speed. Along with faster infer-
ence and better results, the CNN-based end-to-end denois-
ers [17, 28, 31] quickly achieved better results by establish-
ing a mapping between measurements and HSIs, but also
brought huge memory and computation costs.
Recently, the transformer-based end-to-end denoisers
[2‚Äì4] which capture data similarity and long-range depen-
dence have shown impressive performance in both simula-
tion and real scenes. These methods adopt self-attention to
learn effective spatial and spectral representation, achieving
very high parameter and computational efficiency. Follow-
ing this, the unfolding methods [5, 11] regard transformers
as deep priors for optimization formulas and improve iter-
ative frameworks through end-to-end training. In these ap-
proaches, CNNs and transformers play a huge role either
as end-to-end denoisers or as deep priors. While previous
CNNs and spatial transformers fail to effectively utilize the
spectral characteristics inherent in HSIs, spectra-wise trans-
former [3, 4, 11] has shown great potential in spectral tasks
and attracted wide attention. However, current spectra-wise
transformers follow classic multi-head design [12, 25, 35]
and suffer from the loss of global spectral information and
the limitation of ignoring local differences.
3. Spectral Snapshot Imaging Model
Mathematically, we assume a spectral image patch with Œõ
bands{FŒª}Œõ
Œª=1‚ààRH√óW, where HandWrepresents the
HSI‚Äôs height and weight. Image frame FŒªis modulated by
a physical mask with pattern M‚ààRH√óWto get modulated
image frame F‚Ä≤
Œª:
F‚Ä≤
Œª=M‚ó¶FŒª, (1)
where‚ó¶denotes Hadamard‚Äôs (element-wise) product. Then
the modulated image frames of different wavelengthsn
F‚Ä≤
ŒªoŒõ
Œª=1are separated to different positions by the dis-
perser. After that,n
F‚Ä≤
ŒªoŒõ
Œª=1distributed across differ-
ent bands are shifted spatially and summed element-wise.
Thus,n
F‚Ä≤
ŒªoŒõ
Œª=1are compressed to a coded measurement:
G(m, n) =ŒõX
i=1F‚Ä≤
Œª(m, n+D(Œª)), (2)
where mandnindex the spatial coordinates, D(Œª) =
d(Œª‚àí1),drepresents pixels shift between adjacent bands.
Note Eq. (2) assumes a dispersion along the vertical di-
mension, and the derivation is also applicable for horizontal
dispersion. The imaging model in Eq. (2) can be rewritten
in the matrix-vector form as follows:
g= Œ¶f, (3)
25818
LayerNormWSSAFFN
GroupNorm
LayerNormCMBSAB
GroupNorm
Positional
Embedding InputOutput
SSRUnet
LMPARB
√óN
Initialize(a) SSR
(c) SSRB(c2) ARB (c1) ERB
(b) SSRUnet
SSRB Mask Embedding Down\ Up Sample
Conv1 √ó1 Conv3 √ó3 Skip ConnectionInput Output
(d) WSSAH√óW√óCH√óW√óC H √ó W√óC H √ó W√óCHW/M2 √ó C √óC
HW/M2 √ó M2 √ó CHW/M2 √ó M2 √ó C
Conv1 √ó1 Conv1 √ó1Split&ReshapeMatrix MultMatrix MultReshape&Merge
HW/M2 √ó M2 √ó CConv1 √ó1
H√óW√óCH√óW√óC
HW/M2 √ó M2 √ó C
Split&Reshape Split&Reshape
Conv1 √ó1maskgFigure 2. (a)-(c) The overall architecture of SSR, SSRUnet, and SSRB. LMP is achieved via a formula. (d) Details of WSSA.
where g‚ààRHW,f‚ààRHWŒõare the vectorized represen-
tation of the compressed image Gand the original spectral
image Frespectively, and Œ¶‚ààRHW√óHWŒõis the sensing
matrix that describes the system imaging model. In spectral
snapshot reconstruction, what we need to do is recover 3D
HSIffrom the compressed 2D measurement g.
4. Method
Existing spectral and spatial network designs [2‚Äì5, 11] are
limited by the direct application of traditional multi-head
division and unique spatial degradation of snapshot spec-
tral imaging. To address the limitations, we propose the
Spectral-Spatial Rectification (SSR) method to better model
the spectral information, enhance spatial representation,
and improve spatial degradation.
4.1. Overall Architecture
An overview of SSR is presented in Fig. 2 (a), we adopt
a multistage network including initialization and Ncas-
caded stages, and each stage consists of Linear Manifold
Projection (LMP), Spectral-Spatial Rectification Unet (SS-
RUnet), and spAtial Rectification Block (ARB). First, the
compressed measurement gis reversed to the initial shape
[3] and the 2D mask is repeated in channel dimension to ob-
tain the 3D mask. Then the two are inputted to a 1√ó1con-
volution kernel ( conv1√ó1) to get the initialization. In each
stage, LMP is adopted to assist the reconstruction based on
the imaging model. Then the SSRUnet is employed to refine
the input through novel spatial and spectral design. Finally,
ARB is used to further improve spatial degradation in re-
construction. For the core module SSRUnet, as illustrated
in Fig. 2 (b), the conv3√ó3s with residual are utilized to
embed mask information into input Xas follows:
XME=Conv 3√ó3(X)‚ó¶(TI+Conv 3√ó3(Mask )),
(4)
where XMEis the features embedded with the mask, TIis
an all-one tensor and Mask is the 3D mask obtained in ini-tialization. Then SSRUnet with three layers and conv3√ó3
are employed to extract the deep feature which is combined
with the input as a residual to produce a refined output. The
SSRUnet consists of downsampling modules ( conv4√ó4),
Spectral-Spatial Rectification Block (SSRB), upsampling
modules ( deconv 2√ó2), and Fusion modules ( conv1√ó1).
Spectral-Spatial Rectification Block (SSRB). SSRB is the
basic module of SSRUnet. As shown in Fig. 2 (c), the im-
plicit positional information [10] is firstly embedded in the
input for applying self-attention later in SSRB. Each SSRB
is the sequential combination of the spEctral Rectification
Block (ERB) and ARB. Fig. 2 (c) illustrates the components
of ERB and ARB, i.e., a Feed Forward Network(FFN) and a
Window-based Spectral Self-Attention (WSSA) for ERB, a
Convolution Moudulated Block (CMB) and a Spatial Align-
ment Block (SAB) for ARB, and same Layer Normalization
(LN) and Group Normalization (GN) for ERB and ARB.
WSSA is detailed in Fig. 2 (d) and FFN consists of two
linear layers sandwiched with depth-wise conv3√ó3.
4.2. Spectral Rectification Block
Spectral self-attention is a promising approach to utilize
spectral characteristics for HSI tasks. However, we find
that the previous spectra-wise transformer [3, 4, 11] directly
considers a band as a token and follows traditional spatial
self-attention design, i.e., multi-head division in spectral
(channel) dimension, which ignores the local differences
in spectral distribution and results in mean effect and non-
global spectral utilization. To solve the problem, we pro-
pose the ERB based on WSSA to consider local differences
and model global spectral information.
The Influence of Multi-Head Division in Spectral (Chan-
nel) Dimension. In the classic transformer design [12, 25,
35], the use of multi-head division in the channel dimen-
sion is common and brings some gains. Possibly affected
by this, the previous spectra-wise transformer in spectral
snapshot reconstruction also divides multi-head in spectral
dimension as shown in Fig. 3 (a), which results in the sep-
25819
Local attention
(a) Multi -head Division in Spectral  DimensionHead 1 Head 2Spatial
Spectral
(c) Multi -head Division 
    in Spatial  Dimension(b) Mean Effect0.60.3
0.3
0.20.60.4 0.7
0.8
0.5 0.5
0.3 0.4
0.35 0.45 0.6
(d) Window -based AttentionGlobal attentionFigure 3. (a)-(c) The influence of multi-head attention and mean effect, (d) The illustration of WSSA.
aration of spectral information and failure to model global
spectral information. It‚Äôs very easy to get confused because
the spectral transformer is in contrast to the spatial trans-
former, where the characteristic dimension of the former is
the spatial dimension, and the characteristic dimension of
the latter is the spectral (channel) dimension.
The Influence of Mean Effect. In addition, considering
that HSIs depict the spectral distribution of scenes, the sim-
ilarities of different regions should be different but previous
multi-head division results in all regions sharing the same
similarity. When treating the whole band as a token to cal-
culate spectral self-attention, we obtain the mean of all re-
gion similarities in the band and the local differences be-
tween regions are lost, which we call the ‚ÄôMean Effect‚Äô. De-
fineŒõfeatures {fŒª}Œõ
Œª=1where feature dimension consists
ofpdifferent patterns (regions), i.e., fŒª= [f1
Œª, f2
Œª, . . . , fp
Œª],
the influence of mean effect on the similarity calculation can
be simply described as follows:
sim(fk
i, fk
j)‚Üí1
ppX
m=1sim(fm
i, fm
j), (5)
where the left part denotes the expected similarity of the kth
pattern between fk
iandfk
j, and the right part represents the
obtained similarity in previous approaches. We also present
an example in Fig. 3 (b), the average similarity and local
region similarity between the red band and other bands are
marked with red numbers and black numbers respectively.
To demonstrate the validity of our theory, we carried out
ablation experiments of different region sizes, i.e., window
size under the same conditions later. In addition, the mean
effect may explain why multi-head attention is more effec-
tive than single-head attention.
Multi-Head Division in Spatial Dimension. When realiz-
ing appropriate feature dimension and mean effect, it was
natural to treat the entire band frame as a token and then
directly divide multi-head [35] in the merged spatial dimen-
sion to calculate self-attention, which we call MSSA later
for short. However, as shown in Fig. 3 (c), this would
split the entire image into (incomplete) strip shapes on each
head, which mitigates the mean effect to some extent but
breaks the spatial correlation of features. That is, adjacent
areas are divided into different heads, while areas that are
far apart in space are divided into the same head.Window-based Spectral Self-Attention (WSSA). To cap-
ture the global spectral similarity, solve the mean effect, and
maintain spatial correlation, WSSA divides features into a
number of windows spatial-wisely, and then the spectra-
wise self-attention is calculated within windows to avoid
the interference of each other, which is shown in Fig. 3
(d). Specifically, define input X‚ààRH√óW√óC, as shown in
Fig. 2 (d), Xis first linearly projected to 3Cchannels ten-
sor via a 1√ó1convolution then uniformly divides the ten-
sor channel-wisely into the query ( Q), key ( K), value( V).
Subsequently, Q,K,Vare split spatial-wise into HW/M2
windows with size of RM√óM√óCseparately:
{Qi, Ki, Vi}HW/M2
i=1 =Q, K, V , (6)
where Qi, Ki, Vi‚ààRM√óM√óCand are then reshaped into
shapeRM2√óCas tokens to calculate attention as follows:
Attention i=SoftMax (QT
iKi/M)Vi, (7)
where M is used as the scale factor before applying the
softmax function. Then the outputs of HW/M2attention
are reshape in RM√óM√óCand merge together in the original
arrangement to undergo a conv1√ó1projection:
X‚Ä≤={Attention i}HW/M2
i=1 W1, (8)
where W1‚ààRC√óCare weight matrices of a conv1√ó1and
X‚Ä≤is the final output ‚ààRH√óW√óC. Different from multi-
head self-attention which implements token embedding and
then divides heads, WSSA first implements windows split
then reshaping into tokens, which preserves the spatial cor-
relation of each token. Regarding WSSA as a special multi-
head self-attention, the number of WSSA heads is more,
often hundreds of thousands, far more than the number of
multi-head attention.
Computational Complexity. The computational complex-
ity of WSSA is displayed as follows:
O(WSSA ) = 4HWC2+ 2M2C2HW
M2= 6HWC2.(9)
The computational complexity of WSSA is linear to the spa-
tial size HW and window size-independent, meaning that
there is no additional computation cost when we improve
spectral utilization.
25820
(d) SABH √ó W√óCH √ó W√ó1
H √ó W√ó1
H √ó W√óCConv1 √ó1Conv3 √ó3 DConv3 √ó3GELU SigmoidElement -wise ProdConv1 √ó1
H √ó W√óCH √ó W√óCH √ó W√óC
(b) CMBH √ó W√óCH √ó W√óCConv1 √ó1 Conv1 √ó1Element -wise ProdConv1 √ó1
H √ó W√óC
H √ó W√óCH √ó W√óC
H √ó W√óC
(c) Inter -window interactionWindow SizeKernal Size
Shift CompressPartial Imaging Process
(e) Spatial alignment strategy
Estimator
Spectral
Weightdùëöùëöùëöùëöùëöùëö
ÔøΩd‚Ñé
Prediction(a)Spatial degradation in
spectra -wise transformer sw1d‚Ñé
w2d‚Ñé
w3d‚Ñéd‚Ñé
{w}ùëñùëñ ùê∂ùê∂
XA
V‚ÄôZ
FT
SWY
DConv11 √ó11Figure 4. (a) Display of spatial degradation in spectra-wise transformers. (b) The details of CMB. (c) The effect of large kernel convolution
in CMB. (d) The details of SAB. (e) Spatial alignment strategy of SAB.
4.3. Spatial Rectification Block
The utilization of WSSA enhances the capability of spec-
tral information capture. However, a limitation arises from
the absence of interaction between adjacent windows within
WSSA, potentially leading to discontinuous spatial repre-
sentation and the occurrence of blocking artifacts, as high-
lighted by [7]. Additionally, unique spatial degradation oc-
curs due to mask, shift, and compression in spectral snap-
shot imaging. To address these concerns, we propose a spA-
tial Rectification Block (ARB) comprising a Convolution
Modulated Block (CMB) and a Spatial Alignment Block
(SAB). This ARB aims to foster interaction between adja-
cent windows and alleviate spatial degradation in the HSIs.
Convolution Modulated Block (CMB). Here, we try to
solve two problems: (1) When different windows cannot
interfere with each other, WSSA also fails to exchange in-
formation, which leads to discrete spatial expression and
may lead to blocking artifacts [7]. (2) As shown in the top
two rows of Fig .4 (a), over-smoothing demonstrates that
the spatial representation ability of a single spectra-wise
transformer is not sufficient. To solve these two problems
and inspired by the development of large kernel convolu-
tion in high-level tasks, we adopted a Convolution Mod-
ulated Block (CMB) proposed in [16] to modulate spatial
information and make adjacent windows interact with each
other through large kernel convolution sliding as shown in
Fig. 4 (b). CMB modulates spatial information in a similar
manner to self-attention but much more efficiently. Specif-
ically, given the input tokens X‚ààRH√óW√óC, we use a
simple depth-wise convolution with kernel size 11√ó11to
calculate the modulated attention Aas follows:
A=DConv 11√ó11(XW 2), (10)
where W2‚ààRH√óW√óCare weight matrices of conv1√ó1s,
andDConv 11√ó11denotes a depth-wise convolution withkernal size 11√ó11. Then we adopt the element-wise prod-
uct and attention Ato modulate the project feature V‚Ä≤to get
the output Z:
Z= (A‚ó¶V‚Ä≤)W4, V‚Ä≤=XW 3, (11)
where W3andW4‚ààRC√óCare weight matrices of conv1√ó
1s. On the one hand, CMB could effectively improve the
spatial representation of features through modulation oper-
ation. On the other hand, CMB enables the pixels between
adjacent windows of WSSA to interact spatial-wisely when
depth-wise convolution kernel size 11√ó11is greater than
patch size 8√ó8, as demonstrated in Fig. 4 (c).
Spatial Alignment Block (SAB). As shown in the bottom
two rows of Fig. 4 (a), we observe that the reconstruction
results of some bands have more spatial degradation such as
distortion and deformation, which is not common in other
HSI tasks. Thus, we think this may be related to the unique
imaging process. As shown in Fig. 4 (e), shift and compres-
sion in the imaging process mix information from different
spatial locations, and spatial texture recovery in bands with
low spectral density may be difficult, which is neglected by
previous methods. Considering that the low-density bands
can be obtained by multiplying the high-density bands by
weights, we can utilize the high-quality bands to improve
the spatial texture of the low-quality bands through a novel
spatial alignment strategy, as shown in Fig. 4 (e). To
achieve this, a Spatial Alignment Block (SAB) is proposed
to improve spatial degradation. As shown in 4 (d), we adopt
different convolution kernels to estimate spatial texture dis-
tribution which corresponds to the high-density part, and
learn spectral weights in SAB when two linear layers are
used to mix channel information. Specifically, given the
input feature F‚ààRH√óW√óC, we use a convolution with
kernel size 3√ó3andGELU activation function to learn
the spatial texture Tas follows:
T=œà(Conv 3√ó3(FW 5)), (12)
25821
Table 1. The PSNR (upper entry in each cell) in dB and SSIM (lower entry in each cell) results of the test methods on 10 scenes.
Algorithms Reference Params GFLOPs Scene1 Scene2 Scene3 Scene4 Scene5 Scene6 Scene7 Scene8 Scene9 Scene10 Avg
DIP-HSI [30] ICCV 2021 33.85 64.4232.68
0.89227.26
0.85831.30
0.91540.54
0.95329.79
0.88430.39
0.90828.18
0.87829.44
0.88834.51
0.89028.51
0.87431.26
0.894
TSA-Net [28] ECCV 2020 44.25 110.0632.03
0.89231.00
0.85832.25
0.91539.19
0.95329.39
0.88431.44
0.90830.32
0.87829.35
0.88830.01
0.89029.59
0.87431.46
0.894
DGSMP [18] CVPR 2021 3.58 84.7733.26
0.91532.09
0.89833.06
0.92540.54
0.96428.86
0.88233.08
0.93730.74
0.88631.55
0.92331.66
0.91131.44
0.92532.63
0.917
GAP-Net [27] IJCV 2023 4.27 78.5833.74
0.91133.26
0.90034.28
0.92941.03
0.96731.44
0.91932.40
0.92532.27
0.90230.46
0.90533.51
0.91530.24
0.89533.26
0.917
HerosNet [42] CVPR 2022 11.27 414.7635.69
0.97335.01
0.96834.82
0.96738.07
0.98533.18
0.96934.94
0.97633.58
0.96233.19
0.96833.04
0.96433.01
0.96534.45
0.970
HDNet [17] CVPR 2022 2.37 154.7635.14
0.93535.67
0.94036.03
0.94342.30
0.96932.69
0.94634.46
0.95233.67
0.92632.48
0.94134.89
0.94232.38
0.93734.97
0.943
MST [3] CVPR 2022 2.03 28.1535.40
0.94135.87
0.94436.51
0.95342.27
0.97332.77
0.94734.80
0.95533.66
0.92532.67
0.94835.39
0.94932.50
0.94135.18
0.948
CST [2] ECCV 2022 3.0 40.1035.96
0.94936.84
0.95538.16
0.96242.44
0.97533.25
0.95535.72
0.96334.86
0.94434.34
0.96136.51
0.95733.09
0.94536.12
0.957
BIRNAT [8] TPAMI 2023 4.40 2122.6636.79
0.95137.89
0.95740.61
0.97146.94
0.98535.42
0.96435.30
0.95936.58
0.95533.96
0.95639.47
0.97032.80
0.93837.58
0.960
DAUHST [5] NeurIPS 2022 6.15 79.5037.25
0.95839.02
0.96741.05
0.97146.15
0.98335.80
0.96937.08
0.97037.57
0.96335.10
0.96640.02
0.97034.59
0.95638.36
0.967
RDLUF [11] CVPR 2023 1.81 115.3437.94
0.96640.95
0.97743.25
0.97947.83
0.99037.11
0.97637.47
0.97538.58
0.96935.50
0.97041.83
0.97835.23
0.96239.57
0.974
SSR-S Ours 1.73 26.3738.22
0.96340.05
0.97242.57
0.97547.46
0.98636.50
0.97237.33
0.97137.60
0.96435.70
0.96841.37
0.97535.06
0.95939.19
0.971
SSR-M Ours 3.45 52.6538.61
0.96741.35
0.97843.94
0.97948.32
0.98837.80
0.97738.07
0.97538.54
0.96936.82
0.97442.72
0.98035.79
0.96540.20
0.975
SSR-L Ours 5.18 78.9339.07
0.97042.04
0.98144.49
0.98048.80
0.99038.64
0.98038.50
0.97839.16
0.97136.96
0.97643.12
0.98236.08
0.96840.69
0.978
SSR-L* Ours 1.73 78.9338.81
0.96841.51
0.97943.76
0.97948.62
0.98838.32
0.97937.85
0.97538.50
0.96936.85
0.97442.64
0.98035.82
0.96540.27
0.976
where T‚ààRH√óW√ó1,œàisGELU activation function and
W5‚ààRC√óCare weight matrices of the linear layer. Mean-
while, we use a depth-wise convolution with kernel size
3√ó3andSigmoid activation function to learn spectral
element-wise weight as follows:
SW=œÉ(DConv 3√ó3(FW 5)), (13)
where SW‚ààRH√óW√óC,œÉisSigmoid activation func-
tion. Finally, we use multiple element-wise products and a
conv1√ó1projection W6to calculate the output Y as fol-
lows:
Y={T‚ó¶SWi}C
i=1W6, (14)
where Y‚ààRH√óW√óCandSWi‚ààRH√óW√ó1.
4.4. Linear Manifold Projection (LMP)
Eq. (3) is a key constraint in spectral compression recon-
struction. To take advantage of this prior, we introduce a
Linear Manifold Projection (LMP) proposed in GAP-Net
[27] to assist the reconstruction. We add an additional pa-
rameter to control projection intensity and perform the LMP
as follows:
f‚Ä≤=f+œÅŒ¶T[(g‚àíŒ¶f)./(Œ¶Œ¶T)], (15)
where œÅis the parameter that is estimated through a simple
network similar to [5].
4.5. Implementation Details
We change the stage numbers Nto establish a series of
SSR models with small, medium, and large scales: SSR-S(N= 3), SSR-M ( N= 6), and SSR-L ( N= 9). The pro-
posed SSR is implemented by PyTorch and the window size
in WSSA is empirically set to 8√ó8. Adam [19] optimizer
(Œ≤1= 0.9 and Œ≤2= 0.999) and Cosine Annealing [26] sched-
uler are adopted to train SSR on a single RTX 3090 GPU.
Training samples are patches with spatial sizes of 256 √ó256
and 384 √ó384 randomly cropped from 3D HSI data cubes
for simulation and real experiments separately. The shifting
stepdof the imaging model is 2. The channels of SSRUnet
layers are set to Œõ,2Œõ,4Œõin sequence and basic bands
Œõ = 28 . The training loss function is the root mean square
error (RMSE) between reconstructed and ground-truth HSIs
and adopts the multi-stage loss setting proposed in [43].
5. Experiment
5.1. Experimental Settings
Following [5, 11, 17, 28, 42], we select 28 wavelengths
from 450nm to 650nm by using spectral interpolation ma-
nipulation to derive HSIs. We conduct experiments on sim-
ulation and real datasets.
Simulation and Real Data. Two simulation datasets,
CA VE [32] and KAIST [9], and five real HSIs captured
by the CASSI system developed in [28] are adopted. The
CA VE dataset provides 32 HSIs with a spatial size of
512√ó512. The KAIST dataset includes 30 HSIs with a spa-
tial size of 2704 √ó3376. We use CA VE for simulation train-
ing and select 10 scenes from KAIST for simulation testing.
Evaluation Metrics. We adopt two image quality indexes
including peak signal-to-noise ratio (PSNR), and structure
25822
RGB Image Measurement
Ground Truth HDNet SSR-L DGSMP HerosNet MST BIRNAT CST RDLUF DAUHST
462.0nm
551.5nm
594.5nm
636.5nm(a)
(b)Density
(a)Density450 500 65000.10.20.30.40.50.60.70.80.91
 DGSMP, corr: 0.8776
 HerosNet, corr: 0.8901
 HDNet, corr: 0.878
 MST, corr: 0.9226
 CST, corr: 0.8773
 BIRNAT, corr: 0.9833
 DAUHST, corr: 0.9627
 RDLUF, corr: 0.9768
 SSR, corr: 0.9971
 Ground Truth
450 500 550 600 650
(b)  Wavelen gth(nm)00.10.20.30.40.50.60.70.80.91
 DGSMP, corr: 0.736
 HerosNet, corr: 0.7691
 HDNet, corr: 0.8188
 MST, corr: 0.8561
 CST, corr: 0.8785
 BIRNAT, corr: 0.9887
 DAUHST, corr: 0.9696
 RDLUF, corr: 0.9806
 SSR, corr: 0.9911
 Ground TruthFigure 5. Reconstructed images of simulation scene 1with 4out of 28spectral channels by the state-of-the-art methods. Two regions in
scene1 are selected for analyzing the spectra of the reconstructed results. The figure is better viewed in a zoomed-in PDF.
similarity (SSIM) [39] for quantitative evaluation. Specifi-
cally, PSNR measures the visual quality, while SSIM mea-
sures the structure similarity. Generally, higher values of
PSNR and SSIM mean better reconstruction results.
5.2. Simulation Scene
Quantitative results. Table. 1 and Fig. 1show the
quantitative comparison of our SSR and the state-of-the-art
(SOTA) methods: DIP-HSI [30], TSA-Net [28], DGSMP
[18], GAPnet [27], HerosNet [42], HDNet [17], MST [3],
CST [2], BIRNAT [8], DAUHST [5], RDLUF [11]. We can
see that SSR significantly outperforms the other methods
by over 2dB at the same FLOPs and exceeds the leading
method RDLUF by more than 1dB with reduced compu-
tational demands. Concretely, our best model, SSR-L sur-
passes SOTA methods RDLUF, DAUHST, and DAUHST
by 1.12, 2.33, and 3.11dB while less computational ef-
fort is required. Surprisingly, our small model SSR-S sur-
passes most methods when requiring the least parameters
and FLOPs. Moreover, our middle model outperforms the
best method RDLUF by 0.63dB when less than 1/2FLOPs
are required. It is worth noting that RDLUF adopts a stage
parameter-sharing strategy and we take the same strategy to
establish our SSR-L‚àó, which brings performance degrada-
tion but still achieves a clear advantage over other methods
with the least parameters. We found that the degradation
comes from the stability of the method, that is, as the num-
ber of parameters increases, the performance of our SSR
steadily increases and RDLUF decreases.
Visual comparison. We provide the visual comparison of
simulation scene7 with4out of 28spectral channels in Fig.
5. SSR-L successfully recovers the clear pattern and sharp
edge on the cup when the other methods all suffer from
blurred or distorted effects. In addition, we plot the spec-
tral curves of two regions in scene1 in the bottom-left of
Fig. 5. It‚Äôs intuitive that SSR-L has a prominent higher cor-
relation with the reference spectra, which demonstrates the
effectiveness of our spectra-wise attention.Table 2. Ablation study of WSSA and ARB.
Baseline-1 WSSA ARB PSNR SSIM Params (M) FLOPs (G)
‚úì 36.60 0.956 1.06 14.43
‚úì ‚úì 37.76 0.965 1.30 18.82
‚úì ‚úì ‚úì 39.19 0.971 1.73 26.37
5.3. Real Scene Results
To verify the effect of the proposed method on the real
scenes, five measurements captured by the CASSI system
are utilized for testing and the ground truths of the scenes
are unavailable. For fair comparisons, all methods are
trained on the CA VE and KAIST datasets jointly using the
fixed real mask with 11-bit shot noise injected. The bot-
tom two rows of Fig. 6plot the visual comparisons of
the proposed SSR-S and the existing SOTA methods while
the top two rows plot the visual comparisons of different
spectra-wise transformers [3, 4]. It is intuitive to see that our
SSR-S restores clearer spatial textures and sharper edges
than previous spectra-wise transformers in the top two rows,
which shows the effect of spatial modulation. Compared
with other methods in the bottom two rows, SSR-S obtains
clearer results in these two bands when other methods fail
to recover these two bands, demonstrating the effectiveness
of our spatial alignment strategy.
5.4. Ablation Experiment
In this part, we adopt the CA VE and KAIST datasets to con-
duct ablation studies.
Effectiveness of WSSA and ARB. We first conduct a
break-down ablation experiment to investigate the effect
of each component towards higher performance. The re-
sults are listed in Table. 2. The baseline-1 model is de-
rived by removing our WSSA and ARB from SSR-S and
yields 36.60dB. When we successively apply our WSSA
and ARB, the model continuously achieves 1.16dB and
1.43dB improvements. These results suggest the effective-
ness of WSSA and ARB.Wavelen gth(nm)550 600
25823
SSR-S DAUHST MST TSA-Net DGSMP HDNet RDLUF516.2nm575.3nm604.2nm648.1nm
CST
MeasurementRGB Image
MST MST++ SSR-S
RGB Image Measurement
MST MST++ SSR-SFigure 6. Reconstructed images of real scene 2and scene 4with 2and 4out of 28spectral channels separately by the state-of-the-art
methods. Compared with other competing methods, our SSR recovers more details and clear content.
Table 3. Ablation study of different self-attention schemes.
Metric Baseline- 2S-MSA [3] Swin-MSA [25] HS-MSA [5] WSSA* WSSA
PSNR 32.01 32.85 33.01 33.09 33.19 33.30
SSIM 0.908 0.916 0.918 0.919 0.922 0.924
Params (M) 1.03 1.29 1.29 1.29 1.29 1.29
FLOPs (G) 13.84 18.15 19.04 19.04 18.15 18.65
Self-Attention Scheme Comparison. We compare WSSA
with other self-attentions and report the results in Table. 3.
We adopt the half operation [5] in Swin-MSA [25] to keep
the computation almost the same and use multi-head atten-
tion in the spectral dimension of WSSA* to align with S-
MSA[3]. Baseline-2 is SSR-S that retains only the ERB
and removes attention and yields 32.01dB. WSSA yields the
most significant improvement of 1.29dB, which is 0.45dB,
0.29dB, and 0.21dB higher than S-MSA [3], Swin-MSA
[25], and HS-MSA [5], which shows the effectiveness of
WSSA. In addition, WSSA* and WSSA achieve 0.34dB
and 0.45dB gain than S-MSA respectively, which demon-
strates the role of considering local differences and model-
ing global spectral information.
Influence of Multi-head attention and Mean effect. We
demonstrate the influence of multi-head attention and mean
effect on the spectra-wise self-attention through further ex-
periments on the relationship between performance and to-
ken dim (window size) in WSSA and MSSA, which is
shown in Fig. 7. Intuitively, the large window size would
suffer from limited performance, which illustrates the influ-
ence of the mean effect. Performance degradation also oc-
curs when the window size is too small to retain complete
feature information. In our experiments, 8√ó8window size
optimally balances retaining feature information while min-
imizing the mean effect, which produces the highest per-
formance of 33.3dB. When the token dim is small, WSSA
Figure 7. Influence of token dim in spectra-wise attention.
clearly performs better than MSSA, which shows the role
of maintaining spatial correlation. When the token dim is
large, MSSA is slightly better than WSSA, which may be
related to the shift in the imaging process and the strip pat-
tern can contain some shift information.
6. Conclusion
In this paper, we analyze the influence of multi-head atten-
tion and mean effect on the spectra-wise transformer, and a
novel SSR method is proposed to improve spectral snapshot
reconstruction. To model the global spectral information,
consider the local difference, and maintain spatial correla-
tion, WSSA is proposed to better utilize spectral similarity.
ARB leverages CMB to address the interaction between ad-
jacent windows of WSSA and learn spatial representation
when SAB is specially designed to mitigate spatial degra-
dation in low-quality bands through a novel spatial align-
ment strategy. Extensive experiments on simulation and
real scenes show the effectiveness of the proposed modules.
Our SSR at different scales also significantly outperforms
the state-of-the-art methods with less cost.
Acknowledgements: This work was supported by the Na-
tional Natural Science Foundation of China under Grant
62302394 and 62106063, the China Postdoctoral Science
Foundation under Grant 317751, and the Natural Science
Foundation of Shaanxi under Grant 2023-JC-QN-0757.
25824
References
[1] Jos ¬¥e M. Bioucas-Dias and M ¬¥ario A. T. Figueiredo. A new
twist: Two-step iterative shrinkage/thresholding algorithms
for image restoration. IEEE Transactions on Image Process-
ing, 16:2992‚Äì3004, 2007. 1, 2
[2] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Coarse-to-fine sparse transformer for hyperspectral image re-
construction. In Computer Vision‚ÄìECCV 2022: 17th Euro-
pean Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Pro-
ceedings, Part XVII , pages 686‚Äì704. Springer, 2022. 1, 2, 3,
6, 7
[3] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Mask-guided spectral-wise transformer for efficient hyper-
spectral image reconstruction. In 2022 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
17481‚Äì17490, 2022. 1, 2, 3, 6, 7, 8
[4] Yuanhao Cai, Jing Lin, Zudi Lin, Haoqian Wang, Yulun
Zhang, Hanspeter Pfister, Radu Timofte, and Luc Van Gool.
Mst++: Multi-stage spectral-wise transformer for efficient
spectral reconstruction. In CVPRW , 2022. 1, 2, 3, 7
[5] Yuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Henghui
Ding, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Degradation-aware unfolding half-shuffle transformer for
spectral compressive imaging. In Advances in Neural In-
formation Processing Systems , 2022. 1, 2, 3, 6, 7, 8
[6] Yuanhao Cai, Yuxin Zheng, Jing Lin, Xin Yuan, Yulun
Zhang, and Haoqian Wang. Binarized spectral compressive
imaging. In Proc. Conf. Neural Inf. Process. Syst. , 2023. 1
[7] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao,
and Chao Dong. Activating more pixels in image super-
resolution transformer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 22367‚Äì22377, 2023. 5
[8] Ziheng Cheng, Bo Chen, Ruiying Lu, Zhengjue Wang, Hao
Zhang, Ziyi Meng, and Xin Yuan. Recurrent neural net-
works for snapshot compressive imaging. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 45(2):2264‚Äì
2281, 2023. 6, 7
[9] Inchang Choi, Daniel S. Jeon, Giljoo Nam, Diego Gutierrez,
and Min H. Kim. High-quality hyperspectral reconstruction
using a spectral prior. ACM Transactions on Graphics , 36:
1‚Äì13, 2017. 6
[10] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and
Chunhua Shen. Conditional positional encodings for vision
transformers. In The Eleventh International Conference on
Learning Representations , 2022. 3
[11] Yubo Dong, Dahua Gao, Tian Qiu, Yuyan Li, Minxi Yang,
and Guangming Shi. Residual degradation learning unfold-
ing framework with mixing priors across spectral and spa-
tial for compressive spectral imaging. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 22262‚Äì22271, 2023. 1, 2, 3, 6,
7
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 3
[13] Michael E Gehm, Renu John, David J Brady, Rebecca M
Willett, and Timothy J Schulz. Single-shot compressive
spectral imaging with a dual-disperser architecture. Optics
express , 15(21):14013‚Äì14027, 2007. 1
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770‚Äì778, 2016. 2
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In Computer
Vision‚ÄìECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11‚Äì14, 2016, Proceedings,
Part IV 14 , pages 630‚Äì645. Springer, 2016. 2
[16] Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, and Jiashi
Feng. Conv2former: A simple transformer-style convnet for
visual recognition. arXiv preprint arXiv:2211.11943 , 2022.
5
[17] Xiaowan Hu, Yuanhao Cai, Jing Lin, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Hd-
net: High-resolution dual-domain learning for spectral com-
pressive imaging. In 2022 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 17521‚Äì17530,
2022. 1, 2, 6, 7
[18] Tao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, and
Guangming Shi. Deep gaussian scale mixture prior for spec-
tral compressive imaging. In 2021 IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16211‚Äì
16220, 2021. 1, 6, 7
[19] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[20] David Kittle, Kerkil Choi, Ashwin Wagadarikar, and David J
Brady. Multiframe image estimation for coded aperture
snapshot spectral imagers. Applied optics , 49(36):6824‚Äì
6833, 2010. 1, 2
[21] David Kittle, Kerkil Choi, Ashwin Wagadarikar, and David J
Brady. Multiframe image estimation for coded aperture
snapshot spectral imagers. Applied optics , 49(36):6824‚Äì
6833, 2010. 1, 2
[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Advances in neural information processing systems ,
25, 2012. 2
[23] Xing Lin, Yebin Liu, Jiamin Wu, and Qionghai Dai. Spatial-
spectral encoded compressive hyperspectral imaging. ACM
Transactions on Graphics (TOG) , 33:1 ‚Äì 11, 2014. 1, 2
[24] Yang Liu, Xin Yuan, Jinli Suo, David J. Brady, and Qionghai
Dai. Rank minimization for snapshot compressive imaging.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 41:2990‚Äì3006, 2019. 1, 2
[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
25825
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012‚Äì10022, 2021. 1, 2, 3, 8
[26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-
tic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983 , 2016. 6
[27] Ziyi Meng, Shirin Jalali, and Xin Yuan. Gap-net for snapshot
compressive imaging. arXiv: Image and Video Processing ,
2020. 6, 7
[28] Ziyi Meng, Jiawei Ma, and Xin Yuan. End-to-end low
cost compressive spectral imaging with spatial-spectral self-
attention. In ECCV , 2020. 1, 2, 6, 7
[29] Ziyi Meng, Mu Qiao, Jiawei Ma, Zhenming Yu, Kun Xu, and
Xin Yuan. Snapshot multispectral endomicroscopy. Optics
Letters , 45(14):3897‚Äì3900, 2020. 1
[30] Ziyi Meng, Zhenming Yu, Kun Xu, and Xin Yuan. Self-
supervised neural networks for spectral snapshot compres-
sive imaging. In 2021 IEEE/CVF International Conference
on Computer Vision , pages 2602‚Äì2611, 2021. 6, 7
[31] X. Miao, X. Yuan, Y . Pu, and V . Athitsos. lambda-net:
Reconstruct hyperspectral images from a snapshot measure-
ment. In 2019 IEEE/CVF International Conference on Com-
puter Vision (ICCV) , 2019. 2
[32] J. Park, M. Lee, M. D. Grossberg, and S. K. Nayar. Multi-
spectral Imaging Using Multiplexed Illumination. In IEEE
International Conference on Computer Vision , 2007. 6
[33] Jin Tan, Yanting Ma, Hoover F. Rueda, Dror Baron, and Gon-
zalo R. Arce. Compressive hyperspectral imaging via ap-
proximate message passing. IEEE Journal of Selected Topics
in Signal Processing , 10:389‚Äì401, 2016. 1, 2
[34] Jin Tan, Yanting Ma, Hoover F. Rueda, Dror Baron, and Gon-
zalo R. Arce. Compressive hyperspectral imaging via ap-
proximate message passing. IEEE Journal of Selected Topics
in Signal Processing , 10:389‚Äì401, 2016. 1, 2
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2, 3, 4
[36] Ashwin A Wagadarikar, Nikos P Pitsianis, Xiaobai Sun, and
David J Brady. Video rate spectral imaging using a coded
aperture snapshot spectral imager. Optics express , 17(8):
6368‚Äì6388, 2009. 1
[37] Lizhi Wang, Chen Sun, Maoqing Zhang, Ying Fu, and Hua
Huang. Dnu: Deep non-local unrolling for computational
spectral imaging. In 2020 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 1658‚Äì1668,
2020. 1
[38] Minghua Wang, Qiang Wang, and Jocelyn Chanussot. Ten-
sor low-rank constraint and $l 0$ total variation for hyper-
spectral image mixed noise removal. IEEE Journal of Se-
lected Topics in Signal Processing , 15:718‚Äì733, 2021. 1, 2
[39] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE Transactions on Image Processing , 13(4):
600‚Äì612, 2004. 7
[40] Xin Yuan. Generalized alternating projection based total
variation minimization for compressive sensing. 2016 IEEE
International Conference on Image Processing (ICIP) , pages
2539‚Äì2543, 2016. 1, 2[41] Xin Yuan, Yang Liu, Jinli Suo, and Qionghai Dai. Plug-and-
play algorithms for large-scale snapshot compressive imag-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 1447 ‚Äì 1457,
2020. 2
[42] Xuanyu Zhang, Yongbing Zhang, Ruiqin Xiong, Qilin Sun,
and Jian Zhang. Herosnet: Hyperspectral explicable re-
construction and optimal sampling deep network for snap-
shot compressive imaging. In 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 17511‚Äì
17520, 2022. 1, 6, 7
[43] Yin-Ping Zhao, Jiancheng Zhang, Yongyong Chen, Zhen
Wang, and Xuelong Li. Rcump: Residual completion un-
rolling with mixed priors for snapshot compressive imag-
ing. IEEE Transactions on Image Processing , 33:2347‚Äì
2360, 2024. 6
[44] Siming Zheng, Yang Liu, Ziyi Meng, Mu Qiao, Zhishen
Tong, Xiaoyu Yang, Shensheng Han, and Xin Yuan.
Deep plug-and-play priors for spectral snapshot compressive
imaging. Photonics Research , 9(2):B18‚ÄìB29, 2021. 2
25826
