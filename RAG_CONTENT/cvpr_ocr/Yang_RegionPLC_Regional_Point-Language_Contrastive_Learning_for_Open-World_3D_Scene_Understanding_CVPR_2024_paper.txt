RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D
Scene Understanding
Jihan Yang1*Runyu Ding1‚àóWeipeng Deng1Zhe Wang2Xiaojuan Qi1
1The University of Hong Kong2SenseTime Research
https://jihanyang.github.io/projects/RegionPLC
Abstract
We propose a lightweight and scalable Region alPoint-
Language Contrastive learning framework, namely Re-
gionPLC , for open-world 3D scene understanding, aiming
to identify and recognize open-set objects and categories.
Specifically, based on our empirical studies, we introduce a
3D-aware SFusion strategy that fuses 3D vision-language
pairs derived from multiple 2D foundation models, yield-
ing high-quality, dense region-level language descriptions
without human 3D annotations. Subsequently, we devise a
region-aware point-discriminative contrastive learning ob-
jective to enable robust and effective 3D learning from
dense regional language supervision. We carry out exten-
sive experiments on ScanNet, ScanNet200, and nuScenes
datasets, and our model outperforms prior 3D open-world
scene understanding approaches by an average of 17.2%
and 9.1% for semantic and instance segmentation, respec-
tively, while maintaining greater scalability and lower re-
source demands. Furthermore, our method has the flexi-
bility to be effortlessly integrated with language models to
enable open-ended grounded 3D reasoning without extra
task-specific training. Code will be released at github.
1. Introduction
Open-world 3D scene understanding aims to equip models
with the ability to accurately perceive and identify open-set
objects and categories from 3D data, such as point clouds.
This ability is crucial for real-world applications where
objects from open-set categories are prevalent [2, 39].
However, this task poses significant challenges due to the
scarcity of dense 3D semantic annotations, which are diffi-
cult to gather and scale to a large vocabulary space.
Fortunately, the abundance of paired image and text
data from the Internet, featuring a vast semantic vocab-
ulary, has enabled 2D vision-language models to exhibit
exceptional open-world image comprehension capabilities.
These abilities span various tasks, such as image captioning
[1, 31], grounding [24, 32], and dense semantic prediction[19, 20, 44]. Consequently, recent research has been in-
spired to leverage these models to generate pseudo supervi-
sion such as dense semantic features [23, 40] and language
descriptions [7, 8] for training 3D models, thereby enabling
open-world inference without relying on image modalities.
Despite advancements, existing solutions still exhibit
limitations. For instance, feature distillation-based meth-
ods [23, 40]‚Äì despite harvesting dense supervision‚Äì suf-
fer from the constraints of 2D feature qualities and require
resource-intensive feature extraction, fusion, and storage
processes, preventing them from being scaled up with more
advanced 3D architectures and larger 3D datasets. Addi-
tionally, while [7] utilizes pseudo 3D-language pairs to en-
able direct learning from large-vocabulary language super-
vision, it suffers from sparse supervision provided by im-
age captioning models. Considering the recent success of
2D foundation models in image- and region-level vision-
language learning, we explore combining their strengths to
enrich vocabulary and construct high-quality region-level
3D-language associations. By doing so, our method can
yield denser 3D-language supervision and circumvent the
knowledge limitations of a single foundation model, facili-
tating resource-efficient and large-vocabulary 3D learning.
To this end, we propose a holistic Region alPoint
Language Contrastive learning framework, named Region-
PLC . This framework generates and fuses diverse region-
level captions from powerful 2D vision-language models,
which are subsequently mapped to 3D for constructing
region-level 3D and language pairs. These paired data are
then incorporated into a region-aware point-discriminative
contrastive learning framework, enabling 3D open-world
learning from dense language supervision.
Specifically, we begin by conducting a comprehensive
examination of various 2D foundation models (e.g., image
captioning [31], dense captioning [24, 32], and detection
models [44]) along with visual prompting techniques for
their capability to generate region-level 3D-language pairs.
Based on our examination, we propose a supplementary-
oriented fusion strategy that leverages the geometric rela-
tionship of regions in 3D space to alleviate ambiguities and
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19823
conflicts encountered when combining paired 3D-language
data from multiple 2D models, ultimately delivering high-
quality dense region-level 3D-language pairs. Furthermore,
with region-level language data, we introduce a region-
aware point-discriminative contrastive loss that prevents
the optimization of point-wise embeddings from being dis-
turbed by nearby points from unrelated semantic categories,
enhancing the discriminativeness of learned point-wise em-
beddings. The region-aware design further normalizes the
contribution of multiple region-level 3D-language pairs, re-
gardless of their region sizes, making feature learning more
robust. Finally, by harvesting the 3D-language associations,
our RegionPLC can be effortlessly integrated with language
models to enable open-ended 3D reasoning with grounding
abilities without requiring task-specific data for training.
We conduct extensive experiments on ScanNet [6], Scan-
Net200 [26], and nuScenes [3] datasets, covering both 3D
indoor and outdoor scenarios. Our method significantly out-
performs existing open-world scene understanding meth-
ods, achieving an average of 17.2%gains in terms of unseen
category mIoU for semantic segmentation and an average
of9.1%gains in terms of unseen category mAP 50for in-
stance segmentation. RegionPLC demonstrates promising
zero-shot segmentation performance, attaining 40.5%and
1.8%higher foreground mIoU compared to PLA [7] and
OpenScene [23], respectively. Notably, it achieves this per-
formance while consuming only 17% of OpenScene‚Äôs [23]
training cost and 5%of its storage requirements. Further-
more, RegionPLC can also be combined with OpenScene
to deliver 5.8%and10.0%gains in foreground mIoU and
mAcc, respectively.
2. Related Work
3D Scene Understanding. 3D semantic and instance
segmentation are two fundamental tasks for scene under-
standing, which predict each point‚Äôs semantic meaning
(and instance IDs) in a 3D point cloud. For semantic
feature extraction and prediction, existing approaches de-
sign customized point convolutions applied on raw point
clouds [28, 33, 35] or employ sparse convolution [10] to
develop voxel-based networks [5, 11] or transformers [18]
based on 3D grids. For instance-level prediction, represen-
tative approaches often use a bottom-up strategy that groups
points to form object proposals [16, 29, 30], or first predicts
3D bounding boxes and then refines the object masks us-
ing a top-down solution [17, 36, 38]. Though achieving
outstanding results on close-set benchmark datasets, they
always struggle with open-world recognition.
Open-world 3D Understanding. Open-world 3D under-
standing [7, 21, 23, 41] aims to recognize novel categories
that are unseen during training. Most recently, the high
open-world capability of 2D foundation models [1, 25]
trained on massive multi-modality data has inspired recent
approaches to leverage them for 3D open-world understand-ing. One line of work [13‚Äì15, 23, 27, 42] focuses on in-
corporating these 2D foundation models in the inference
stage for open-world recognition , which mainly conducts
open-world semantic prediction on the image modality us-
ing vision-language models [19, 25, 37] and fuses 2D pre-
diction results into 3D if required. Though promising, they
suffer from significant computation and storage overheads
during inference and can be sub-optimal to address 3D un-
derstanding without learning from 3D geometries.
This paper focuses on another open-world research di-
rection that concentrates on open-world point cloud learn-
ing. It requires training 3D backbones to enable their open-
world capabilities without the image modality dependence
during inference and thus have more applicability poten-
tial. Along this line of research, some [23, 40, 41] have
attempted to distill 2D dense features [19, 37] into 3D back-
bones for 3D feature learning. However, they still incur
high training costs and might inherit 2D prediction failure
modes. In addition, Ding et al. [7, 8] obtain point-language
paired data through image captioning by VL foundation
models for training 3D backbones. These methods are scal-
able toward a large vocabulary space and can be easily in-
tegrated with advanced 3D backbones. Despite the advan-
tages, they still suffer from the coarse language supervision.
3. RegionPLC
3.1. Overview
We focus on 3D open-world scene understanding at both
semantic and instance levels. During training, given a point
cloud of a scene P={p}, the model can utilize human an-
notations Yfor base categories CB, but cannot access anno-
tations for novel categories CN. During the inference phase,
the trained model needs to classify and localize points asso-
ciated with both base and novel categories ( CB‚à™ CN).
To achieve open-world understanding, apart from the
common 3D encoder F 3D, we follow [7] to replace the clas-
sification layer weights with category embeddings flex-
tracted from a pretrained text encoder F textof CLIP [25]
(See Figure 1: Upper). Hence, the prediction process is
shown as follows:
fp=FŒ∏(F3D(p)),s=œÉ(fl¬∑fp),o=Floc(F3D(p),s),(1)
where F Œ∏is the vision-language (VL) adapter to align the
feature dimension of the 3D point-wise features fpand cat-
egory embeddings fl,sis the semantic classification score,
œÉis the softmax function, ois the instance proposal out-
put, and F locis the localization network [30] for instance
segmentation. With these modifications, the model can pre-
dict any desired categories by computing similarity between
point-wise features and queried category embeddings for
open-world inference.
The goal of our RegionPLC is to train such an open-
world 3D backbone via dense region-level 3D-language su-
pervision leveraging powerful and diverse 2D foundation
19824
3DEncoderF!"VL-AdapterF#TextEncoderF$%&$
AsmallblackcellphoneAlargewoodencabinet‚Ä¶‚Ä¶
1111
AremotecontrolAcurtain
SFusion
AsmallblackcellphoneAlargewoodencabinetA chair sitting in front of a wooden deskA green curtain hanging on a wall
ùê≠!"ùê≠#$!ùê≠%&'('Point-discriminativeloss‚àáùêü)‚Ñí"#$‚àáùêü)‚Ñí%"#$‚àáùêü)‚Ñí"#$‚àáùêü)‚Ñí&
Region-awarepoint-discriminativelossCaptionregionGradientsumGradient
Right/WrongGradientredundantA chair sitting in front of a wooden deskA green curtain is hanging on a wallAsmallblackcellphoneAlargewoodencabinetRegional3D-LanguageAssociation(Sec.3.2)Region-awarePoint-discriminativeContrastiveLearning(Sec.3.5)
<p,t*>Region-awarePoint-discriminativeContrastiveLearningRegional3D-LanguageAssociation
conflictFigure 1. Overview of our regional point-language contrastive learning framework. For regional 3D-language association, We develop
a 3D-aware SFusion strategy effectively combining 3D vision-language pairs obtained from multiple 2D foundation models (refer to
Sec. 3.2). Upon these 3D-language data, we propose region-aware point-discriminative contrastive learning to facilitate more distinctive
and robust representation learning (detailed in Sec. 3.5). Different point & box colors in the bottom-right indicate various 3D-caption pairs.
models, as shown in Figure 1. we first obtain region-level
2D-language pairs through three streams of 2D VL mod-
els (i.e. image captioning [31], object detection [44] and
dense captioning [24, 32]) and then associate them to 3D
points (see Sec. 3.2). Then, we comprehensively bench-
mark and examine these 3D-language pairs from differ-
ent sources, deriving their merits and shortcomings for 3D
learning (see Sec. 3.3). Based on our study, we propose
a simple Supplementary-oriented Fusion (SFusion ) strat-
egy leveraging their 3D relationships to alleviate redun-
dancies and conflicts in Sec. 3.4, obtaining vocabulary-
enriched and denser region-level 3D-language paired data.
Finally, upon the 3D-language data, we design a region-
aware point-discriminative contrastive learning objective to
replace CLIP-style loss for more robust and discriminative
feature learning from language supervisions in Sec. 3.5.
3.2. Regional 3D-Language Association from 2D
Foundation Models
Here, we first introduce three streams of methods along with
two types of visual prompts to extract regional language de-
scriptions from 2D vision-language foundation models: i)
object detector with language template; ii) explicit visual
prompted image captioning; iii) dense captioning.
Object Detector with language template Gdet.The most
straightforward manner to obtain regional language super-
vision is to leverage the category prediction from 2D object
detector Detic [44] and then fill the category into a language
template as CLIP [25], as illustrated in Figure 2. Thanks to
the multi-scale training strategy, object detectors can cap-
ture remote and small objects. We denote such regional
captions as tdet-t.
Explicit visual prompted image captioning Gprompt.An-
other intuitive paradigm is first to generate explicit visual
A tall building with a street light next to itA large buildingwith a skybackground and treesA streetwith a barrieron the side of itAcitystreetfilledwithlotsoftrafficA large truckparked on the side of the roadAstreetwithafenceandabuildinginthebackgroundAnimageofatruckAnimageofacarAnimageofapersonAnimageofabenchAnimageofawheelAnimageofatrafficlightAlarge truckSeveral carsTall buildingsA few peopleOrangesafetybarriers
ùí¢!"#$!%ùí¢&'%ùí¢()!Figure 2. Comparisons of different advanced manners for extract-
ing regional language descriptions with 2D foundation models.
prompts such as boxes and then caption these image patches
via image captioning model OFA [31] (refer to Figure 2).
As for obtaining explicit visual prompts, we attempt two
types: sliding windows and object proposals. Sliding-
window-cropped image patches cover all potential semantic
regions without being constrained by pre-defined vocabu-
lary space, benefiting open-world tasks but sacrificing pre-
cise localization. In contrast, 2D object proposals from de-
tectors provide more accurate object localization but suf-
fer from the limited vocabulary space with pre-defined la-
bel space such as LVIS [12]. The obtained dense region-
level captions through sliding-window prompts and detector
prompts are denoted as tswandtdet-c, respectively.
Dense Captioning Gcap.Apart from the powerful image
detectors and image captioning models, recent advances in
dense captions and grounding models such as GRiT [32]
and Kosmos-2 [24] are trained on the large-scale 2D box
and box description pairs. As shown in Figure 2, dense cap-
tioners offer precise object localization and rich vocabulary
19825
MethodScanNet B12/N7 ScanNet annotation-free
25K 125K 25K 125K
tdet-t63.4 / 70.3 / 57.7 67.4 / 70.5 / 64.6 37.7 (59.7) 41.9 (64.5)
tsw65.9 / 70.2 / 62.1 66.0 / 70.2 / 62.3 48.1 (69.2) 47.8 (69.2)
tdet-c64.4 / 69.9 / 59.7 65.6 / 70.7 / 61.2 41.4 (64.1) 43.8 (65.6)
tgrit62.7 / 70.3 / 56.6 64.9 / 70.8 / 59.9 50.5 (72.2)51.3 ( 74.2)
tkos64.4 / 70.3 / 59.4 64.6 / 69.8 / 60.2 50.1 (70.7) 51.7 (72.7)
tkos‚à™tdet-t64.6 / 69.8 / 60.2 67.0 / 69.9 / 64.4 44.4 (66.3) 54.0 (74.5)
tkos‚à™tsw65.9 / 69.9 / 62.4 65.6 / 70.9 / 61.0 53.5 (72.9) 53.1 (73.9)
Ltkos+Ltdet-t65.0 / 70.2 / 60.5 64.4 / 69.1 / 60.2 51.3 (70.7) 51.2 (72.4)
Ltkos+Ltsw65.4 / 70.0 / 61.4 64.6 / 70.2 / 59.8 52.9 ( 73.6)52.7 (73.8)
Table 1. Results of regional caption fusion on base-annotated
(hIoU / mIoUB/ mIoUN) and annotation-free (mIoU‚Ä†(mAcc‚Ä†),
tested on foreground classes only) 3D ScanNet semantic segmen-
tation. tkos‚à™tdet-tandLtkos+Ltdet-tindicate data-level and multi-
loss fusion, respectively. Best results are presented in bold .
spaces but tend to focus on only salient objects and ignore
small and distant objects. We denote captions generated
through GRiT [32], Kosmos-2 [24] and Detic [44] with a
caption template as tgrit, andtkosandtdet-t, respectively.
Associate Points to Dense Captions. Upon above 5 types
of regional captions tr={tsw,tdet-c,tdet-t,tgrit,tkos}, we
associate them to partial point sets through 3D geometry,
similar to [7, 23], to pair points and language. Specifically,
we begin by projecting the 3D scenes onto 2D images to
align points with pixels. Then by connecting the points ÀÜ p
within each 2D region to their respective captions, we ob-
tain the regional 3D-language pairs ‚ü®ÀÜ p,tr‚ü©.
3.3. Benchmark and Analysis on Regional 3D-
Language Pairs
With the constructed five types of regional 3D-language
pairstr={tsw,tdet-c,tdet-t,tgrit,tkos}, the follow-up ques-
tion is which delivers the best performance on learning
3D open-world representation and how to combine them to
obtain enriched vocabulary space and denser regional 3D-
language association. Hence, we benchmark them on Scan-
Net [6] semantic segmentation tasks with different novel
categories and 2D image quantities (25K vs. 125K). Our
benchmark encompasses two settings: i) the B12/N7 setting
including 12 annotated base categories and 7 unannotated
novel categories, which requires a strong comprehension of
a large vocabulary corpus; ii) the annotation-free setting,
wherein all categories are novel ones, and thus necessitates
both open-vocabulary recognition and precise object local-
ization with only sparse 3D-language pairs.
Complementary cues. As shown in the upper of Table 1,
no single type of 3D-language source consistently outper-
forms others in all settings, and each association has its own
merits. For example, tdet-tinherits the advanced small ob-
ject localization capabilities (refer to Figure 2 middle for
‚Äútraffic light‚Äù and ‚Äúwheel‚Äù descriptions.), excelling others
in the ScanNet B12/N7 (125K). However, it suffers from
the limited pre-defined vocabulary space and obtains the
worst performance in the annotation-free setting with 17
novel categories. In contrast, dense captioners tkosandtgritoffer salient object localization with semantic-rich vocabu-
lary (refer to Figure 2 right for attribute descriptions), ex-
hibiting superior results on the annotation-free setting. This
suggests that different VL models and visual prompts offer
various merits and might complement each other.
End-to-end manners scale better . When comparing the
performance of utilizing 25K and 125K images, we find
that end-to-end trained dense captioners and detectors ( i.e.
tkos,tgritandtdet-t) scale better than the two-stage image
captioning manners with visual prompts. The reason might
be that end-to-end trained dense caption sources are more
consistent on different views and thus yield fewer semantic
conflicts when scaling up to more views.
Common combinations are not always effective. As above-
mentioned, different 3D-language pairs can offer comple-
mentary cues. Hence, we examine their synergy effect for
better performance. As shown in the bottom of Table 1, we
attempt to combine the representatives from three streams
of regional caption generation manners tkos,tdet-tandtsw
via data-level and multi-loss fusion. Nevertheless, the per-
formance lift across different settings is not consistent or
only shows incremental increases, which suggests the need
for a more dedicated fusion strategy to accommodate exten-
sive dense language supervision from multiple sources.
3.4. Boost Synergy of Diverse 3D-language Sources
Motivated by the observations of complementary merits
of individual 3D-language sources and their unsatisfactory
synergy results, we further study how to combine these
varied 3D-language sources effectively and efficiently. In
this regard, we propose a Supplementary-orientated Fusion
(SFusion) strategy to integrate the most diverse semantic
clues while filtering out potential conflicts from different
caption sources. As data-level mixing delivers better per-
formance than loss-level combination, we focus on tack-
ling the bottleneck of data-level 3D-language pairs fusion
here. When training 3D models on data-level mixed 3D-
language pairs, they are learning from a more informative
language description, but suffer from sub-optimal perfor-
mance. This suggests that the main challenges in straight-
forward data-level mixing are the redundancy and conflicts
from different caption sources, especially for highly over-
lapped point cloud regions (see Figure 1). For those highly
overlapped 3D regions with multiple language sources, mu-
tually conflicting descriptions will confuse models, and the
overabundance of repetitive language descriptions tends to
overwhelm optimization toward easily identifiable areas,
leading to sub-optimal performance.
Hence, our SFusion addresses these problems by fusing
3D-language pairs with low 3D overlaps to alleviate poten-
tial conflicts in overlapped areas and obtain spatially sup-
plementary 3D-language pairs. Specifically, we first select
the most reliable caption source that performs best as the
primary 3D-language source tpri. Then, we compute the
19826
overlap ratio œÑof point sets between the primary source and
candidate caption sources tcanoni-th 3D scene as follows,
œÑjk=overlap (ÀÜ ppri
ij,ÀÜ pcan
ik),ÀÜœÑk= max
jœÑjk, (2)
where overlap measures the intersection over union (IoU)
between two point sets, ÀÜ ppri
ijandÀÜ pcan
ikare the j-th and k-th
point set in the i-th scene from the primary source and can-
didate source, respectively. Then, we define thresholds Tl
andThto filter out 3D-language pairs with high overlap ra-
tios from tcan, which might result in redundant or conflict
supervision to the primary source. Hence, only candidate
3D-language pairs ‚ü®ÀÜ pcan
ik,tcan
ik‚ü©withTl<ÀÜœÑk< Th(Tlset to
zero) are fused with the primary source. This procedure can
be iteratively applied across all candidate caption sources to
obtain a collection of 3D-language pairs with low geomet-
rical overlaps. This refined set will serve as the supervision
for the follow-up contrastive training. Notice that we also
introduce a hyper-parameter œµ‚àà[0,1]to control the ratio
of the primary source and candidate source during fusion,
as maintaining the majority of primary sources is benefi-
cial during training with multi-source 3D-language pairs.
Our experimental results in Table 7 verify our above claims
and demonstrate that our SFusion strategy can significantly
boost the combination of multiple language sources.
3.5. Region-aware Point-discriminative Contrastive
Learning
After obtaining 3D-language pairs ‚ü®ÀÜ p,t‚ü©for supervision,
we proceed to train F3DandFŒ∏to align 3D features with
language features for open-world learning. We introduce
region-aware point-discriminative contrastive loss as below.
CLIP-style Contrastive Loss. We can pull paired 3D fea-
tures and language features closer while pushing away the
unmatched ones through CLIP-style [25] contrastive loss
(refer to Figure 1 top right). It can be formulated as:
fÀÜp=Pool(ÀÜ p,fp),ÀÜ z=fÀÜp¬∑Ft,ÀÜ s=œÉ(ÀÜ z), (3)
Lc=‚àíyt¬∑lnÀÜ s, (4)
where fÀÜpis the average-pooled region feature, Pool( ÀÜ p,fÀÜp) is
our custom CUDA operator to gather features fÀÜpover point
setÀÜ p,Ft= [ft
1,ft
2,¬∑¬∑¬∑ft
nt]concatenates all caption embed-
dings in a scene, ÀÜ zandÀÜ smeasure the similarity and the
score probability between a 3D region and all captions, œÉ
is sigmoid function and ytis the one-hot label highlighting
the position paired with ÀÜ p. While CLIP [25] targets learn-
ing a global image-level feature for the classification task,
it neglects the demand of learning point-wise discrimina-
tive features for dense prediction tasks. As shown in Fig-
ure 1, the pooling operation will average the point-wise fea-
tures and make all points in the same region ÀÜ poptimized in
the same direction, preventing the learning of discrimina-
1https://kaldir.vc.in.tum.de/scannet_benchmark/
documentationtive representations for dense prediction tasks. We present
more analysis on this undesired effect in the suppl..
Point-discriminative Contrastive Loss. Considering
the limitation of CLIP-style loss, we propose a point-
discriminative contrastive loss Lpdcto make the learning of
point embedding discriminative. Specifically, for each re-
gional 3D-language pair, instead of aggregating point fea-
tures into an averaged region-level feature, our Lpdcdi-
rectly computes the similarity between point-wise embed-
dings and caption embeddings. We then pool the logarithm
of predicted point-wise probability within ÀÜ pto compute the
cross-entropy loss regarding one-hot label ytas follows,
z=fp¬∑Ft,s=œÉ(z),Lpdc=‚àíyt¬∑Pool(ÀÜ p,lns),(5)
where zandsindicate the similarity and probability matrix
between point-wise features and all caption embeddings.
By doing so, the optimization direction of each point will
be adapted to its own point embeddings and thus make them
discriminative (refer to Figure 1). More details are included
in the supplementary materials.
Region-aware Normalization. Though discriminative, the
Lpdcwill back-propagate smaller gradients to points in large
regions due to the pooling operation, leading to an implicit
bias towards region size which can be harmful to represen-
tation learning. To alleviate this issue, we propose a region-
aware factor to normalize Lpdcby the region size, to ensure
an equivalent gradient scale on points in each region regard-
less of its size.Obtained region-aware loss Lrpdcas follows,
Lrpdc=‚àíŒ±rLpdc, Œ±r=nt¬∑card(ÀÜ p)Pnt
icard(ÀÜ pi), (6)
where Œ±ris the region-aware normalization factor, ntis the
number of 3D-language pairs each scene.
Analysis. With point-discriminative and region-aware
properties, our Lrpdcfacilitates more superior and robust
representation learning. It allows each point to grasp its
unique semantics without disruptions from other unrelated
points (refer to Figure 1 right). This is especially vital for
annotation-free dense prediction to segment object bound-
aries without any annotation (see Table 6 for verification).
Moreover, the region-aware factor in Lrpdcprovides a more
robust optimization procedure. As depicted in the right sec-
tion of Figure 1, points associated with multiple captions
are normalized to a similar gradient scale. When multiple
captions reach a consensus, this leads to consistent gradient
directions, thereby encouraging them to be optimized in a
unified direction. Conversely, when multiple captions con-
flict, this leads to inconsistent gradient directions and thus
discourages the noisy optimization.
4. Experiments
4.1. Basic Setups
Datasets and Validation Settings. To test the effectiveness
of RegionPLC, we evaluate it on three popular datasets:
19827
MethodScanNet [6] nuScenes [3] ScanNet200 [26]
B15/N4 B12/N7 B10/N9 B12/N3 B10/N5 B170/N30 B150/N50
3DGenZ [22] 20.6 / 56.0 / 12.6 19.8 / 35.5 / 13.3 12.0 / 63.6 / 6.6 1.6 / 53.3 / 0.8 1.9 / 44.6 / 1.0 2.6 / 15.8 / 1.4 3.3 / 14.1 / 1.9
3DTZSL [4] 10.5 / 36.7 / 6.1 3.8 / 36.6 / 2.0 7.8 / 55.5 / 4.2 1.2 / 21.0 / 0.6 6.4 / 17.1 / 3.9 0.9 / 4.0 / 0.5 0.7 / 3.8 / 0.4
OVSeg-3D [7] 0.0 / 64.4 / 0.0 0.9 / 55.7 / 0.1 1.8 / 68.4 / 0.9 0.6 / 74.4 / 0.3 0.0 / 71.5 / 0.0 1.5 / 21.1 / 0.8 3.0 / 20.6 / 1.6
PLA [7] 65.3 / 68.3 / 62.4 55.3 / 69.5 / 45.9 53.1 / 76.2 / 40.8 47.7 / 73.4 / 35.4 24.3 / 73.1 / 14.5 11.4 / 20.9 / 7.8 10.1 / 20.9 / 6.6
RegionPLC 69.4 / 68.2 / 70.7 68.2 /69.9 /66.6 64.3 /76.3 /55.6 64.4 /75.8 /56.0 49.0 /75.8 /36.3 16.6 /21.6 /13.9 14.6 /22.4 /10.8
Fully-Sup. 73.3 / 68.4 / 79.1 70.6 / 70.0 / 71.8 69.9 / 75.8 / 64.9 73.7 / 76.6 / 71.1 74.8 / 76.8 / 72.8 20.9 / 21.7 / 20.1 20.6 / 22.0 / 19.4
Table 2. Results for open-world 3D semantic segmentation on ScanNet, nuScenes and ScanNet200 in terms of hIoU / mIoUB/ mIoUN.
Best open-world results are presented in bold .
ScanNet [6], ScanNet200 [26] and nuScenes [3], cover-
ing indoor and outdoor scenarios. We validate the open-
world capability of our method with different numbers
of annotated categories, including base-annotated open
world (i.e. part of categories annotated) and annotation-
free open world (i.e. no category annotated). We evaluate
our method‚Äôs performance on both semantic segmentation
and instance segmentation tasks.
Category Partition. We split categories into base and novel
on ScanNet [6] following PLA [7]. For nuScenes [3], we
ignore the ‚Äúotherflat‚Äù class and randomly divide the rest
classes into B12/N3 ( i.e. 12 base and 3 novel categories)
and B10/N5. For ScanNet200 [26], we randomly split 200
classes to B170/N30 and B150/N50. See Suppl. for details.
Evaluation Metrics. For semantic segmentation, we follow
[7, 34] to employ mIoUB, mIoUNand harmonic mean IoU
(hIoU) for evaluating base, novel categories and their har-
monic mean separately. Similarly, for instance segmenta-
tion, we employ mAPB
50, mAPN
50and hAP 50.For annotation-
free semantic segmentation, we use mean IoU and mean
accuracy on foreground classes ( i.e. mIoU‚Ä†and mAcc‚Ä†) ex-
cluding ‚Äúwall‚Äù, ‚Äúfloor‚Äù and ‚Äúceiling‚Äù for evaluation.
Implementation Details. We adopt the sparse-convolution-
based UNet [11] as the 3D encoder with CLIP [25] text en-
coder as the final classifier for 3D semantic segmentation,
and SoftGroup [30] for instance segmentation as [7]. We
use category prompts to replace ambiguous category names
such as ‚Äúmanmade‚Äù and ‚Äúdrivable surface‚Äù with a list of
concrete category names when encoding category embed-
dings. We run all experiments with a batch size of 32 on 8
NVIDIA V100 or A100 (see Suppl. for more details).
4.2. Base-annotated Open World
Comparison Methods. We compare RegionPLC to pre-
vious open-world or zero-shot works. 3DGenZ [22] and
3DTZSL [4] are early works for 3D zero-shot learning re-
produced by [7]. OVSeg-3D extends LSeg to 3D [19], re-
ported by [7]. PLA [7] is the previous cutting-edge method.
3D Semantic Segmentation. As shown in Table 2, com-
pared to the previous state-of-the-art method PLA [7],
our method largely lifts the mIoU of unseen categories
by8.3%‚àº21.8%among various partitions on Scan-
Net and nuScenes. Furthermore, when compared to base-
lines without language supervision, i.e. 3DGenZ [22] and
3DTZSL [4], our method even obtains 30.6%‚àº42.7%per-formance gains regarding mIoU on novel categories among
different partitions and datasets. These significant and con-
sistent improvements across indoor and outdoor scenarios
show the effectiveness of our RegionPLC framework.
Furthermore, when facing more long-tail dataset Scan-
Net200 [26], our method still obtains notable mIoUNgains
ranging from 4.2%to5.1%compared to PLA [7] as shown
in Table 2. In this regard, our proposed region-level lan-
guage supervision and region-aware point-discriminative
contrastive loss show its potential to address 3D open-world
understanding in complex and long-tail scenarios.
Method ScanNet
B13/N4 B10/N7 B8/N9
OVSeg-3D [19] 5.1 / 57.9 / 2.6 2.0 / 50.7 / 1.0 2.4 / 59.4 / 1.2
PLA [7] 55.5 / 58.5 / 52.9 31.2 / 54.6 / 21.9 35.9 / 63.1 / 25.1
RegionPLC 58.2 /59.2 /57.2 40.6 / 53.9 / 32.5 46.8 / 62.5 / 37.4
Fully-Sup. 64.5 / 59.4 / 70.5 62.5 / 57.6 / 62.0 62.0 / 65.1 / 62.0
Table 3. Results for open-world 3D instance segmentation on
ScanNet in terms of hAP 50/ mAPB
50/ mAPN
50.
3D Instance Segmentation. As our pipeline provides lo-
cal language descriptions to fine-grained point sets and en-
courage points to learn discriminative features, it also ben-
efits instance-level localization task. As shown in Table 3,
our method consistently brings 4.3%‚àº12.3%gains com-
pared to the state-of-the-art PLA [7] across three partitions
on ScanNet. It is noteworthy that our method obtains more
obvious improvements for partitions with fewer base cate-
gories ( i.e. B10/N7 and B8/N9), demonstrating the effec-
tiveness of our RegionPLC in enabling the model to distin-
guish unseen instances without human annotations.
4.3. Annotation-free Open World
Comparison Methods. As shown in Table 4, we compare
two streams of methods: i) Training-free methods using
multi-view images for inference [23, 43]. ii) Methods lever-
aging 2D vision-language models during training [7, 23].
3D Semantic Segmentation. As shown in Table 4, our Re-
gionPLC with SparseUNet32 [11] backbone significantly
outperforms all other competitive methods by 1.8%‚àº
57.5%mIoU‚Ä†and7.2%‚àº72% mAcc‚Ä†. This is the first
time that a 3D open-world model achieves state-of-the-art
performance without any 3D annotation or 2D pixel-aligned
image features but only sparse language supervision for
learning. Moreover, our RegionPLC can scale up by scal-
ing the 3D backbone from SparseUNet16 to SparseUNet32,
obtaining 2.6% mIoU‚Ä†gains, which shows the advantage of
19828
Method Network mIoU‚Ä†mAcc‚Ä†Multi-view Infer GT Instance Mask Train Hours Extra Storage Latency
MaskCLIP‚Ä°[43] CLIP [25] 23.1 40.9 ‚úì √ó - - 1.7 s
OpenScene-2D [23] LSeg [19] 58.0 68.5 ‚úì √ó - - 106.1s
OpenScene-3D‚Ä°[23] SparseUNet16 [11] 57.2 69.9 √ó √ó 24.7 h 117.3 G 0.08 s
OpenScene-3D‚Ä°[23] SparseUNet32 [11] 57.8 70.3 √ó √ó 25.3 h 117.3 G 0.10 s
PLA‚Ä°[7] SparseUNet16 [11] 17.7 33.5 √ó √ó 11.5 h 1.1 G 0.08 s
PLA‚Ä°[7] SparseUNet32 [11] 19.1 41.5 √ó √ó 12.0 h 1.1 G 0.10 s
RegionPLC SparseUNet16 [11] 56.9 75.6 √ó √ó 12.5 h 5.5 G 0.08 s
RegionPLC SparseUNet32 [11] 59.6 77.5 √ó √ó 13.0 h 5.5 G 0.10 s
RegionPLC + OpenScene-3D‚Ä°SparseUNet16 [11] 60.1 74.4 √ó √ó 25.9 h 122.8 G 0.08 s
RegionPLC + OpenScene-3D‚Ä°SparseUNet32 [11] 63.6 80.3 √ó √ó 26.4 h 122.8 G 0.10 s
Fully-Sup. SparseUNet16 [11] 75.9 84.8 √ó √ó 9.6 h - 0.08 s
Fully-Sup. SparseUNet32 [11] 77.9 86.2 √ó √ó 10.5 h - 0.10 s
Table 4. Annotation-free 3D semantic segmentation on ScanNet.‚Ä°and‚ôØmean results reproduced by us and Uni3D, independently.
learning from sparse language supervision instead of pixel-
aligned feature distillation from 2D encoders [23]. It is also
noteworthy that RegionPLC can function as a lightweight
plug-and-play module and thus be integrated with other
methods such as OpenScene [23] to further boost about 4%
mIoU‚Ä†. Notably, our method is training-efficient, requiring
less disk storage and training time compared to OpenScene.
[23] [7] RegionPLC RegionPLC + [23] Fully-Sup.
5.9 (10.2) 1.8 (3.1) 9.1(17.3) 9.6(17.8) 23.9 (32.9)
Table 5. Annotation-free open-world semantic segmentation on
ScanNet200 [26] in terms of mIoU‚Ä†(mAcc‚Ä†).
Long-tail Scenario. As shown in Table 5, we set up com-
parisons on the more challenging long-tail dataset Scan-
Net200 [26]. Notably, our RegionPLC surpasses other
counterparts by 3.2%‚àº7.4%mIoU‚Ä†and7.1%‚àº14.2%
mAcc‚Ä†. Specifically, OpenScene is less effective on Scan-
Net200 with a large number of fine-grained categories as
it inherits the shortcomings or bias of the 2D segmentation
model that forgets a large number of concepts during fine-
tuning, as verified in [9, 15]. In contrast, our RegionPLC
directly learns in a rich vocabulary space with dense and di-
verse captions which is closer to real open-world scenarios.
4.4. Qualitative Studies
To demonstrate the open-world capability of our Region-
PLC, we provide compelling qualitative results showcasing
its capability in recognizing and localizing novel categories.
As illustrated in Figure 3 (a), RegionPLC successfully iden-
tifies numerous categories without any human annotation,
demonstrating the quality and richness of our region-level
captions and the effectiveness of our region-aware point-
discriminative learning objective. For base-annotated cases,
our model can recognize challenging tail classes such ‚Äúkey-
board‚Äù and ‚Äúladdar‚Äù with precise segmentation in indoor
scenarios (see Figure 3 (b)) and small-scale objects with
only a few points such as ‚Äúmotorcycle‚Äù in outdoor scenarios
(see Figure 3 (c)). Moreover, RegionPLC shows a strong lo-
calization ability in open-world instance segmentation, ac-
curately grouping novel objects as shown in Figure 3 (d).
5. Ablation Study
In this section, we examine key components of our frame-
work through in-depth ablation studies. Results for base-annotated and annotation-free experiments are measured in
hIoU / mIoUB/ mIoUNand mIoU‚Ä†(mAcc‚Ä†) separately.
Components ScanNet ScanNet
tv+etrLpdcLrpdcSFusion B0/N17 B12/N7
0.3 (5.3) 24.5 / 70.0 / 14.8
‚úì 17.7 (33.5) 55.3 / 69.5 / 45.9
‚úì 21.7 (37.1) 62.6 / 69.9 / 56.7
‚úì‚úì 50.6 (71.1) 63.6 / 70.6 / 57.9
‚úì ‚úì 51.7 (72.7) 67.4 / 70.5 / 64.6
‚úì ‚úì ‚úì 56.9 (75.5)68.2 / 69.9 / 66.6
Table 6. Component analysis on ScanNet. tv+eand trdenotes
the combination of view and entity language supervision [7] and
best region-level language supervision, respectively.
Component Analysis. Here, we study the effectiveness
of our proposed regional captions tr, the SFusion strategy
for caption integration, point-discriminative contrastive loss
Lpdcand its region-aware variants Lrpdc. As shown in Ta-
ble 6, when compared to view- and entity-level captions
used in PLA [7], our region-level language supervision de-
livers consistent boosts about 4%‚àº10.8%across differ-
ent category partitions. Additionally, Lpdcachieves con-
siderable gains when paired with tr. Particularly, it brings
28.9%mIoU‚Ä†gains in the annotation-free setting, illustrat-
ing its superiority in learning point-discriminative features
for dense parsing tasks. When combined with the region-
aware factor, Lrpdcsurpasses Lpdcby1.1%‚àº3.8%mIoU‚Ä†.
Lastly, 2%‚àº5.2%improvements yielded from SFusion
strategy confirm its effectiveness in eliminating redundancy
and conflicts from multiple captions for training.
Caption source[Tl,Th]
[0.0, 1.0]‚àó[0.5, 1.0] [0.0, 0.5] [0.0, 0.2]
tkos,tsw53.1 (73.9) 54.6 (75.3) 54.3 (74.4) 56.6 (74.7)
tkos,tdet-t54.0 (74.5) 54.1 (73.7) 54.3 (73.9) 55.9 (76.1)
tkos,tsw,tdet-t54.9 (74.2) 55.2 (73.7) 55.4 (75.3) 56.9 (75.5)
(a) Ablation of various caption sources and caption overlap thresholds.‚àó
equals to the data-mixing baseline. We report mIoU (mAcc) here.
[Tl,Th] [0.5, 1.0] [0.0, 0.5] [0.0, 0.5] [0.0, 0.2]
Ratio ( œµ) 0.75‚äï0.34‚äï0.75 0.72‚äï
mIoU (mAcc) 54.6 (75.3) 54.3 (74.4) 55.4 (75.6) 56.6 (74.7)
(b) Ablation of caption overlap thresholds and their ratios when fusing tkos
andtsw.‚äïmeans the raw ratio for tkosandtkos+tswwith no œµapplied.
Table 7. SFusion results for zero-shot semantic segmentation con-
sidering caption sources, overlap thresholds, and ratios.
SFusion. We also study the effectiveness of our SFusion
strategy. As shown in Table 7 (b), with a similar ratio œµ
19829
(a)Annotation-free(ScanNet)(b)Base-annotated(ScanNet200)
pillowkeyboardbackpackladderdeskbedchairpicture toiletsinkbathtubshowercurtaincurtaindoor
(c)Base-annotated(nuScenes)(d)Base-annotated(ScanNet)
pedestrianmotorcycleSemanticsegmentationInstancesegmentation
Figure 3. Qualitative results of our RegionPLC. The examples above show annotation-free open-world scene parsing where no human
annotation is available (see (a)), and base-annotated open-world learning where a limited number of base classes are annotated (see (b),
(c), (d)) for semantic and instance segmentation covering both indoor and outdoor scenarios. Unseen categories are highlighted in colors.
Pleaseprovideadetaileddescriptionfortheredboxregion.
SummarizationWhen I was heading home, it started raining outside. I ended up getting splashed with dirty water on my face. What should I do?
TaskPlanning
1. Find a bathroomor sinknearby.2. Turn on the faucetand adjust the water temperature.3. Wet your hands and splash water on your face to rinse off the dirt.4. If available, use soapto wash your face thoroughly.5. Rinse off the soapor cleanser with water.6. Pat your face dry with a clean towel.
In the room, there is a bed with various blanketsand pillows. Pictureshang on the wall, and a potted plantsits nearby. And a teddy bear and a catare on the bed.
Reasoning
To keep hardcopies of your thesis, you can use the printeron the tablein this room.I need to keep some hardcopy of my thesis. Please help me locate a machine that can achieve my goal in this room.
(b)RegionGRwithvarioususerqueries(a)RegionGR
I wantto host anafternoon tea party.Ishouldprepare somebeveragesandfoods for myfriends.
Takesomecupsanduse the coffee machine to make coffee or tea. Arrange some snackson the black desk and the counter top.Take out the pies from the refrigeratorandwarm up themin the microwave. ‚Ä¶...TaskPlanning
lowhigh
‚Ä¢A black coffee machine‚Ä¢A variety of coffee cups‚Ä¢A black refrigerator with a white tag‚Ä¢A microwave oven sitting on top of a counter¬∑¬∑¬∑¬∑¬∑RegionPLCLLM3D-LanguagePairs
Figure 4. (a) Visualizations of RegionGR that integrates LLM for
open-ended grounded 3D reasoning. (b) Demonstrating the versa-
tility of RegionGR via more examples of answering user queries.
of the main caption source relative to all merged captions,
fusing 3D-language pairs that have a low spatial overlap ra-
tio (i.e., less than 0.5) yields superior results compared to
fusing pairs that are highly overlapped (i.e., greater than
0.5). Besides, as shown in Table 7 (a), our SFusion largely
outperforms the naive data-mixing strategy ([ Tl,Th]=[0.0,
1.0]) with 1.9%‚àº3.5%gains. These experimental results
affirm that potential conflicts and redundancies introduced
by regions with a high degree of overlap restrict the benefits
derived from multiple language sources. Fortunately, our
SFusion method effectively tackles this challenge.
6. Open-ended Grounded 3D Reasoning
Recently, there has been a growing interest in employ-
ing language as an interface for connecting human inten-
tions with visual understanding, which facilitates high-level
reasoning and planning in the development of embodied
agents. Without specific design, RegionPLC can be seam-
lessly integrated with large language models to enable open-ended grounded 3D reasoning, referred to as RegionGR. As
depicted in Figure 4 (a), RegionGR integrates large lan-
guage models (LLM) for 3D reasoning with regional 3D-
language pairs as a knowledge base and utilizes RegionPLC
to coarsely locate and identify corresponding objects within
the 3D scene for grounded reasoning. Moreover, Figure 4
(b) further exhibits the versatility of RegionGR in respond-
ing to user intentions, from summarization to reasoning and
planning, particularly within user-specified regions of inter-
est (as shown in the summarization example).
Specifically, RegionGR runs in three steps: ( i) We
first initialize ‚Äúenvironment context‚Äù with our regional 3D-
language pairs , which enables LLM to understand a given
scene. If a user specifies an interested 3D region (see Fig-
ure 4 (b) left), highly overlapped 3D-language pairs are
kept. ( ii) We then feed our prompt in Sec. S4 of suppl.
along with ‚Äúuser query‚Äù and ‚Äúenvironment context‚Äù into
LLM to generate answers. ( iii) Finally, we parse objects
from LLM‚Äôs response and ground them with RegionPLC.
7. Conclusion
We present RegionPLC, a holistic regional point-language
contrastive learning framework to recognize and localize
unseen categories in open-world 3D scene understanding.
By leveraging advanced VL models and our SFusion strat-
egy, RegionPLC effectively builds comprehensive regional
point-language pairs. Furthermore, our region-aware point-
discriminative contrastive loss aids in learning distinctive
and robust features from regional captions. Extensive ex-
periments demonstrate that RegionPLC remarkably outper-
forms prior open-world methods in both indoor and outdoor
scenarios and excels in challenging long-tail or annotation-
free scenarios. Besides, RegionPLC can be effortlessly in-
tegrated with LLM for grounded 3D visual reasoning.
Acknowledgement This work has been supported by Hong
Kong Research Grant Council - Early Career Scheme
(Grant No. 27209621), General Research Fund Scheme
(Grant No. 17202422), and RGC Matching Fund Scheme
(RMGS). Part of the described research work is conducted
in the JC STEM Lab of Robotics for Soft Materials funded
by The Hong Kong Jockey Club Charities Trust.
19830
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katie Millican, Malcolm Reynolds, et al. Flamingo: a vi-
sual language model for few-shot learning. arXiv preprint
arXiv:2204.14198 , 2022. 1, 2
[2] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski,
Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D
Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al.
End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 , 2016. 1
[3] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11621‚Äì11631, 2020. 2, 6
[4] Ali Cheraghian, Shafin Rahman, Dylan Campbell, and Lars
Petersson. Transductive zero-shot learning for 3d point cloud
classification. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision , pages 923‚Äì933,
2020. 6
[5] Christopher Choy, JunYoung Gwak, and Silvio Savarese.
4d spatio-temporal convnets: Minkowski convolutional neu-
ral networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3075‚Äì
3084, 2019. 2
[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nie√üner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5828‚Äì5839, 2017. 2, 4, 6
[7] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,
Song Bai, and Xiaojuan Qi. Language-driven open-
vocabulary 3d scene understanding. arXiv preprint
arXiv:2211.16312 , 2022. 1, 2, 4, 6, 7
[8] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,
Song Bai, and Xiaojuan Qi. Lowis3d: Language-driven
open-world instance-level 3d scene understanding. arXiv
preprint arXiv:2308.00353 , 2023. 1, 2
[9] Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang,
and Haoxuan Ding. Don‚Äôt stop learning: Towards
continual learning for the clip model. arXiv preprint
arXiv:2207.09248 , 2022. 7
[10] Benjamin Graham and Laurens van der Maaten. Sub-
manifold sparse convolutional networks. arXiv preprint
arXiv:1706.01307 , 2017. 2
[11] Benjamin Graham, Martin Engelcke, and Laurens Van
Der Maaten. 3d semantic segmentation with submani-
fold sparse convolutional networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 9224‚Äì9232, 2018. 2, 6, 7
[12] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A
dataset for large vocabulary instance segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5356‚Äì5364, 2019. 3
[13] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui
Huang, Rynson WH Lau, Wanli Ouyang, and WangmengZuo. Clip2point: Transfer clip to point cloud classi-
fication with image-depth pre-training. arXiv preprint
arXiv:2210.01055 , 2022. 2
[14] Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao,
Lei Zhu, and Joan Lasenby. Openins3d: Snap and lookup for
3d open-vocabulary instance segmentation. arXiv preprint
arXiv:2309.00616 , 2023.
[15] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala,
Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh
Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al.
Conceptfusion: Open-set multimodal 3d mapping. arXiv
preprint arXiv:2302.07241 , 2023. 2, 7
[16] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-
Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point group-
ing for 3d instance segmentation. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2020. 2
[17] Maksim Kolodiazhnyi, Danila Rukhovich, Anna V orontsova,
and Anton Konushin. Top-down beats bottom-up in 3d
instance segmentation. arXiv preprint arXiv:2302.02871 ,
2023. 2
[18] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang
Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-
former for 3d point cloud segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8500‚Äì8509, 2022. 2
[19] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-
mentation. In International Conference on Learning Rep-
resentations , 2022. 1, 2, 6, 7
[20] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
language-image pre-training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10965‚Äì10975, 2022. 1
[21] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu,
Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and
Hao Su. Openshape: Scaling up 3d shape representa-
tion towards open-world understanding. arXiv preprint
arXiv:2305.10764 , 2023. 2
[22] Bj ¬®orn Michele, Alexandre Boulch, Gilles Puy, Maxime
Bucher, and Renaud Marlet. Generative zero-shot learning
for semantic segmentation of 3d point clouds. In 2021 Inter-
national Conference on 3D Vision (3DV) , pages 992‚Äì1002.
IEEE, 2021. 6
[23] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea
Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.
Openscene: 3d scene understanding with open vocabularies.
arXiv preprint arXiv:2211.15654 , 2022. 1, 2, 4, 6, 7
[24] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. arXiv
preprint arXiv:2306.14824 , 2023. 1, 3, 4
[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
19831
vision. In International Conference on Machine Learning ,
pages 8748‚Äì8763. PMLR, 2021. 2, 3, 5, 6, 7
[26] David Rozenberszki, Or Litany, and Angela Dai. Language-
grounded indoor 3d semantic segmentation in the wild. In
Computer Vision‚ÄìECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part
XXXIII , pages 125‚Äì141. Springer, 2022. 2, 6, 7
[27] Ayc ¬∏a Takmaz, Elisabetta Fedele, Robert W Sumner, Marc
Pollefeys, Federico Tombari, and Francis Engelmann. Open-
mask3d: Open-vocabulary 3d instance segmentation. arXiv
preprint arXiv:2306.13631 , 2023. 2
[28] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc ¬∏ois Goulette, and Leonidas J.
Guibas. Kpconv: Flexible and deformable convolution for
point clouds. Proceedings of the IEEE International Confer-
ence on Computer Vision , 2019. 2
[29] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, Jun-
yeong Kim, and Chang D Yoo. Softgroup++: Scalable 3d
instance segmentation with octree pyramid grouping. arXiv
preprint arXiv:2209.08263 , 2022. 2
[30] Thang Vu, Kookhoi Kim, Tung M. Luu, Xuan Thanh
Nguyen, and Chang D. Yoo. Softgroup for 3d instance seg-
mentation on 3d point clouds. In CVPR , 2022. 2, 6
[31] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. CoRR , abs/2202.03052, 2022. 1, 3
[32] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,
Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-
erative region-to-text transformer for object understanding.
arXiv preprint arXiv:2212.00280 , 2022. 1, 3, 4
[33] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep
convolutional networks on 3d point clouds. In Proceedings
of the IEEE/CVF Conference on computer vision and pattern
recognition , pages 9621‚Äì9630, 2019. 2
[34] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt
Schiele, and Zeynep Akata. Semantic projection network
for zero-and few-label semantic segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 8256‚Äì8265, 2019. 6
[35] Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiao-
juan Qi. Paconv: Position adaptive convolution with dy-
namic kernel assembling on point clouds. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3173‚Äì3182, 2021. 2
[36] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen
Wang, Andrew Markham, and Niki Trigoni. Learning ob-
ject bounding boxes for 3d instance segmentation on point
clouds. In Advances in Neural Information Processing Sys-
tems, pages 6737‚Äì6746, 2019. 2
[37] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang,
Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang
Xu. Detclip: Dictionary-enriched visual-concept paral-
leled pre-training for open-world detection. arXiv preprint
arXiv:2209.09407 , 2022. 2
[38] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J
Guibas. Gspn: Generative shape proposal network for 3dinstance segmentation in point cloud. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3947‚Äì3956, 2019. 2
[39] Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Al-
berto Rodriguez, and Thomas Funkhouser. Learning syn-
ergies between pushing and grasping with self-supervised
deep reinforcement learning. In 2018 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS) ,
pages 4238‚Äì4245. IEEE, 2018. 1
[40] Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han,
Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang,
Xiaodan Liang, and Hang Xu. Clip2: Contrastive language-
image-point pretraining from real-world point cloud data. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 15244‚Äì15253, 2023. 1,
2
[41] Junbo Zhang, Runpei Dong, and Kaisheng Ma. Clip-fo3d:
Learning free open-world 3d scene representations from 2d
dense clip. arXiv preprint arXiv:2303.04748 , 2023. 2
[42] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
peng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng
Li. Pointclip: Point cloud understanding by clip. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 8552‚Äì8562, 2022. 2
[43] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free
dense labels from clip. In European Conference on Com-
puter Vision (ECCV) , 2022. 6, 7
[44] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kr¬®ahenb ¬®uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. In ECCV , 2022. 1, 3,
4
19832
