Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks
Shinâ€™ya Yamaguchi*
NTT, Kyoto UniversitySekitoshi Kanai
NTTKazuki Adachi
NTTDaiki Chijiwa
NTT
Abstract
While fine-tuning is a de facto standard method for train-
ing deep neural networks, it still suffers from overfitting
when using small target datasets. Previous methods improve
fine-tuning performance by maintaining knowledge of the
source datasets or introducing regularization terms such as
contrastive loss. However, these methods require auxiliary
source information (e.g., source labels or datasets) or heavy
additional computations. In this paper, we propose a sim-
ple method called adaptive random feature regularization
(AdaRand). AdaRand helps the feature extractors of train-
ing models to adaptively change the distribution of feature
vectors for downstream classification tasks without auxil-
iary source information and with reasonable computation
costs. To this end, AdaRand minimizes the gap between fea-
ture vectors and random reference vectors that are sampled
from class conditional Gaussian distributions. Furthermore,
AdaRand dynamically updates the conditional distribution to
follow the currently updated feature extractors and balance
the distance between classes in feature spaces. Our experi-
ments show that AdaRand outperforms the other fine-tuning
regularization requiring auxiliary source information and
heavy computation costs.
1. Introduction
Fine-tuning is a standard technique for training deep neural
network models. In fine-tuning, we pre-train a model on
large-scale source tasks (e.g., ImageNet [ 28] and WebIm-
ageText for CLIP [ 27]) before training it on target tasks.
Fine-tuning can improve the performance and efficiency of
training on the target task [ 12]. However, fine-tuning deep
neural networks still suffers from overfitting when small
target datasets are used [18].
To alleviate the overfitting, previous studies have pro-
posed regularization terms so that models maintain source
knowledge [ 18â€“20,33]. The early methods simply min-
imize the gap between source and target models on the
*Corresponding author. shinya.yamaguchi@ntt.commodel parameters [ 18] or the intermediate outputs [ 19].
The recent methods have evolved to utilize auxiliary source
information such as source classification labels [ 33] and
source datasets [ 20] to improve the performance. However,
the source information is not always available because pre-
training methods are not limited to supervised classification
and source datasets are often private. For instance, multi-
modal pre-training by CLIP [ 27] provides only pre-trained
parameters but not the source dataset (WebImageText). Thus,
we cannot directly apply the regularization methods using
the auxiliary source information to CLIP pre-trained models.
An alternative regularization strategy that does not re-
quire source information is to modify feature vectors or task
heads to be desirable for solving downstream classification
tasks [ 10,36â€“38]. These methods penalize the feature ex-
tractors of fine-tuning models by â„“1/â„“2loss for selecting
features to train effectively [ 10,37] or contrastive loss for
obtaining instance discriminative features [ 36]. More re-
cently, Zhou et al. [38] have shown a regularization method
that enforces linear classification head parameters to classify
the features extracted from fixed source models. However,
this method requires additional memory to store pre-trained
features and compute their penalty term. This increases
the memory and computation costs of fine-tuning and the
complexity of implementation.
To achieve high performance without auxiliary source
information and non-negligible computation costs, we spot-
light the method proposed by Zhong and Maki [37], which
we call random feature regularization (RandReg). RandReg
penalizes the feature extractor by the â„“2distance between
feature vectors and random reference vectors. The reference
vectors are drawn from a class-agnostic prior distribution
that is independent of the target task, e.g., uniform distribu-
tion. By perturbing features with randomness in addition to
penalizing the feature norms, RandReg boosts fine-tuning
performance without auxiliary source information and heavy
additional computation costs.
Although RandReg boosts the performance, we empiri-
cally found that (a) the performance gain of RandReg de-
pends on the choice of the prior distribution because the
feature norms and scales of pre-trained models vary widely
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23481
RandRegAdaRand
ğ‘(ğ‘§)Regularizing features according to noises from a fixedclass-agnosticpriorRegularizing features according to noises from adaptiveclass-conditionalpriorsğ‘ğ‘§				)ğ‘ğ‘§				)ğ‘ğ‘§				)Figure 1. Intuitive comparison of RandReg and AdaRand (proposed method). RandReg regularizes a feature extractor by minimizing the
gap between input features and noises generated from a fixed prior distribution. Although RandReg is very simple, it tends to concentrate
the features in local regions due to the prior being fixed and class-agnostic, preventing separate classes. In contrast, AdaRand adopts class
conditional priors and dynamically updates them with the running feature statistics of training models and the maximization distances
between each pair of class conditional priors. This helps models to obtain more separable features and improve accuracy.
depending on the pre-training methods (Table 1), (b) in some
cases, RandReg is unexpectedly inferior to the â„“2feature
regularization without randomness, (c) RandReg has an un-
expected effect of reducing the feature norm and entropy due
to the single class-agnostic prior, which leads to limiting gra-
dients of cross-entropy loss and mutual information between
features and target labels. Therefore, naÃ¯vely introducing
RandReg with a simple prior may limit the performance of
target models.
To address the challenges of RandReg, we propose adap-
tive random feature regularization (AdaRand), which ex-
tends RandReg to be effective for arbitrary pre-training meth-
ods including self-supervised learning and CLIP. AdaRand
uses a parametric class conditional Gaussian prior that is
dynamically updated during fine-tuning instead of a fixed
class-agnostic prior. By initializing the prior distribution
with the statistics of feature vectors computed on pre-trained
models for each target class, AdaRand performs regulariza-
tion stably without suffering from the differences in features
due to the choice of pre-training methods. Whereas Ran-
dReg causes small feature norms and entropy, AdaRand
prevents them by dynamically updating the prior parameters
of each class according to the fine-tuning process. The ob-
jective function consists of (i) fitting the mean parameters to
the class-wise running mean of feature vectors during fine-
tuning and (ii) penalizing them so that they are not similar
to any other class. That is, the prior distributions are moved
toward the distribution of the current feature vectors while
maintaining a margin between classes. This improves the
mutual information between features and target labels, result-
ing in separable clusters of feature vectors that are suitable
for the target classification task (Figure 1).
We conduct experiments to evaluate AdaRand with vari-
ous (classification, self-supervised, and CLIP) pre-trainingmethods on multiple datasets. The experiments show that
AdaRand outperforms RandReg and existing fine-tuning
methods depending on auxiliary source information and non-
negligible computation costs, even though AdaRand does
not require either.
2. Related Work
Many regularization methods for fine-tuning deep neural net-
works are based on the assumption that maintaining source
knowledge is beneficial for solving target tasks. According
to this assumption, Li et al. [18] have presented a simple
regularization called L2SP, which minimizes the parame-
ters between source and target models during fine-tuning.
A subsequent study [ 19] has shown that learning to max-
imize the similarity between the output feature vectors of
source and target models can outperform L2SP. To prevent
catastrophic forgetting and negative transfer, batch spectral
shrinking (BSS, [ 5]) penalizes smaller singular values of the
batch feature matrices. Although these methods are simple
and flexible for arbitrary pre-training methods, the perfor-
mance improvements are limited. To achieve more practical
performance, Co-tuning [ 33] leverages source knowledge
contained in the source task-specific layers on the head of
pre-trained models, which are often discarded during fine-
tuning. Specifically, in addition to target tasks, Co-tuning
simultaneously solves a pseudo-source task that is defined
by soft-source labels corresponding to each target label.
BTfW [ 9] and UOT [ 21] search target-related subsets of
the source dataset through the selection algorithm and train
a model on both the target and target-related source subset to
directly transfer source knowledge. By leveraging auxiliary
source information (i.e., source class labels and datasets),
Co-tuning and UOT have achieved impressive fine-tuning
performance on target tasks. However, since recent powerful
23482
pre-training models such as CLIP [ 27] are often not trained
on classification or publicly available datasets, we cannot
apply the previous methods to them.
On the other hand, there are regularization methods that
refine parameters or features without explicitly maintaining
source knowledge. Takada and Fujisawa [29] have shown
that the â„“1regularization on training parameters helps to
select the parameters to be updated and improve fine-tuning
performance. Hariharan and Girshick [10] have proposed
theâ„“1/â„“2regularization methods on feature vectors called
feature norm penalty (FNP), which restrict feature activation
to extract only useful information with limited volume target
data. Subsequently, Zhong and Maki [37] have proposed
RandReg, which minimizes the gap between feature vectors
and the random reference vectors from a uniform prior dis-
tribution. RandReg can be regarded as an advanced method
of FNP because the definition of RandReg is decomposed by
theâ„“1/â„“2regularization term and perturbation term, which
is designed to help models not to be trapped in local min-
ima [ 37]. Our work is positioned as one of the regularization
methods without the assumption of auxiliary source informa-
tion and improves RandReg by introducing class conditional
Gaussian priors that are dynamically updated to prevent the
small norm and low entropy features.
The assumption of conditional Gaussian (mixture) distri-
butions over the feature spaces is often used in the context of
adversarial robustness [ 25,30]. In contrast, our method aims
to improve the performance of fine-tuning by regularizing
the feature extractor with the reference vectors sampled from
the conditional distributions.
3. Preliminary
3.1. Problem Setting
In this paper, we consider a standard problem of fine-tuning
deep neural networks on a Kclass classification task. We
train a neural network model fÎ¸:X â†’ Y on a labeled target
dataset D={(xi, yi)âˆˆ X Ã— Y}N
i=1, where XandYare
the input and output label spaces, respectively. fÎ¸is defined
by a composition of a feature extractor gÏ•:X â†’Rdand
a weight matrix for linear classification WâˆˆRdÃ—K, i.e.,
fÎ¸=WâŠ¤gÏ•andÎ¸= [Ï•, W ]. Here, Î¸is initialized by Î¸s=
[Ï•s, Ws], which is pre-trained on large-scale source datasets
through arbitrary pre-training methods such as supervised
training [ 32], self-supervised contrastive training [ 4,13], and
multimodal training [27].
3.2. Random Feature Regularization
We recall the principle of random feature regularization (Ran-
dReg) [ 37]. RandReg is a regularization method for fine-
tuning deep neural networks that uses a prior distribution
p(z)of the random reference vector zâˆˆRd, i.e., zâˆ¼p(z).The objective function is defined as follows:
min
Î¸=[Ï•,W]Lcls(Î¸) +Î»Lreg(Ï•), (1)
Lcls(Î¸) =E(x,y)âˆˆDâ„“CE(fÎ¸(x), y), (2)
Lreg(Ï•) =ExâˆˆDâˆ¥gÏ•(x)âˆ’zâˆ¥2
2, (3)
where â„“CEis cross-entropy loss. Intuitively, by minimizing
the gap between gÏ•(x)andz,Lreg(Ï•)makes the feature
extractor gÏ•forms the output feature vectors according to the
prior distribution p(z). The original paper [ 37] explains that
RandReg improves classifiers because the reference vector z
enlarges the variance of the gradient and prevents the model
from overfitting. However, we found that the performance
gain of RandReg largely depends on the combinations of
pre-training methods and priors, and the naÃ¯ve randomness
by RandReg is not effective in some cases.
4. Observations of RandReg
In this section, we analyze RandReg through preliminary
experiments from the perspective of pre-training methods
and priors. We evaluated RandReg with pre-trained ResNet-
50 [11] models. We provide more detailed training settings
in Sec. 6.1. In summary, we obtained three observations.
â€¢Effective prior distribution depends on pre-training meth-
ods and the features of the pre-trained models.
â€¢RandReg underperforms the â„“2feature regularization with-
out randomness (i.e., FNP [ 10]), indicating that there are
cases where the naÃ¯ve randomness is not effective.
â€¢RandReg makes training models generate features with
small norms and less diversity.
Consequently, based on these observations, we discuss the
challenges of RandReg in terms of fixed prior distributions
and features with small norms and less diversity.
4.1. Effects of Prior Distribution
We examine the performance when varying prior distribu-
tions in RandReg. We tried three priors for training target
models: uniform distribution U(0,1),standard Gaussian
distribution N(0, I), and Gaussian distribution with pre-
computed statistics N(Âµs, Ïƒ2
sI), which is parameterized
by feature mean ÂµsâˆˆRdand variance Ïƒ2
sâˆˆRdcomputed
with pre-trained weights on the target dataset. To assess the
importance of the randomness in RandReg, we also tested
theâ„“2feature norm regularization without randomness (FNP,
[10]), which is equivalent to the case where zis 0 in Eq. (3).
We evaluated these priors on multiple pre-trained models
with four pre-training methods including classification [ 1],
SimCLR [ 4], Barlow Twins [ 34], and CLIP [ 27]. Table 1
shows the top-1 test accuracy on the Cars [ 15] dataset. We
observed that RandReg improves the fine-tuning baselines
for all pre-trained models, but the best prior depends on
pre-trained methods. Table 1 also lists the averaged feature
23483
Table 1. Analysis of random feature regularization (RandReg [ 37]) in top-1 test accuracy on various pre-training methods (Cars, ResNet-50).
We also report the statistics of target data computed on a pre-trained model: the averaged feature norms ( âˆ¥gÏ•s(x)âˆ¥) and the dimension-wise
averaged mean and variance ( (Â¯Âµs=Pd
i=1Âµs[i],Â¯Ïƒ2
s=Pd
i=1Ïƒ2
s[i]))). The performance depends on the combinations of pre-training
methods and prior. Moreover, there is a case that the accuracy of RandReg degrades from simple â„“2feature regularization (i.e., FNP [ 10]).
Pre-trained Method âˆ¥gÏ•s(x)âˆ¥2
2 (Â¯Âµs,Â¯Ïƒ2
s) Fine-tuning FNP [10] RandReg- U(0,1) RandReg- N(0,1) RandReg- N(Âµs, Ïƒ2
s)
ImageNet Classification 19.58 (4.18 Ã—10âˆ’1, 2.69Ã—10âˆ’1) 89.14Â±.4290.27Â±.1090.59Â±.2490.42Â±.1290.61Â±.24
ImageNet SimCLR [4] 1.34 (2.60 Ã—10âˆ’2, 3.97Ã—10âˆ’2) 83.73Â±.7384.53Â±.3284.08Â±.2184.03Â±.0783.91Â±.04
ImageNet Barlow Twins [34] 3.67 (5.53 Ã—10âˆ’2, 6.94Ã—10âˆ’2) 86.98Â±.1687.44Â±.1587.24Â±.3087.74Â±.3387.65Â±.48
CLIP [27] 11.70 (5.92 Ã—10âˆ’4, 3.12Ã—10âˆ’2) 88.72Â±.2489.96Â±.0590.19Â±.4090.59Â±.2490.78Â±.07
norms ( âˆ¥gÏ•s(x)âˆ¥) and the dimension-wise averaged mean
and variance ( Â¯Âµs=Pd
i=1Âµs[i],Â¯Ïƒ2
s=Pd
i=1Ïƒ2
s[i]) that are
computed on each pre-trained feature extractor by forward-
ing target data before fine-tuning. Uniform and standard
Gaussian distributions are effective when the feature scale is
small (e.g., Barlow Twins), but when the scale is large (e.g.,
ImageNet Classification), Gaussian distributions with pre-
computed statistics, i.e., N(Âµs, Ïƒ2
sI), are the best. This im-
plies that considering a gap between the feature distributions
of pre-trained models and prior distributions is important in
selecting priors. However, RandReg is inferior to FNP in
the case of SimCLR, where the feature norm and scale are
quite small, even when we use pre-computed statistics. This
means that the naÃ¯ve randomness introduced by RandReg is
not effective for fine-tuning in some cases.
4.2. Effects on Feature Norm and Diversity
Next, we investigate the effects on features caused by Ran-
dReg. Here, we focus on the feature norm and diversity. This
is because the regularization term of RandReg in Eq. (3) di-
rectly affects the feature norm âˆ¥gÏ•(x)âˆ¥2
2by the squared loss,
and it can restrict the diversity of the features by the prior
p(z). We interpret the feature diversity as the feature en-
tropy H(gÏ•(x)), and estimate H(gÏ•(x))by the differential
entropy estimator with the assumption that the probabilistic
density of gÏ•(x)is constant in an Ïµ-ball around a feature
gÏ•(xi)for a randomly sampled xi[8]:
H(gÏ•(x))â‰ˆd
N(Nâˆ’1)X
iÌ¸=jlogâˆ¥gÏ•(xi)âˆ’gÏ•(xj)âˆ¥2
2,(4)
where dis the dimension of a feature vector and Nis the
dataset size. Figure 2 plots the feature norm âˆ¥gÏ•(x)âˆ¥2
2and
entropy H(gÏ•(x))for each epoch in training; we calculated
âˆ¥gÏ•(x)âˆ¥2
2andH(gÏ•(x))on training samples. While fine-
tuning slightly increases âˆ¥gÏ•(x)âˆ¥2
2andH(gÏ•(x)), RandReg
gradually decreases both of them, which means RandReg
produces small feature vectors with less diversity.
4.3. Challenges of RandReg
Fixed prior distribution. From the results in Sec. 4.1,
although RandReg can improve fine-tuning regardless of
pre-training methods, we should select an effective priordistribution to adjust the reference vectors to the pre-trained
feature extractor and obtain the best performance. This is
challenging in practice because it is not obvious which prior
distribution is the best choice for the pre-training method
and the manual search is costly; naÃ¯vely using pre-computed
statistics ÂµsandÏƒ2
sdid not always achieve the best perfor-
mance in Table 1. If the prior is not effective, there is a risk
that the randomness will not work effectively as in the case
of SimCLR in Table 1. Thus, we need to efficiently search
for appropriate prior distributions in fine-tuning.
Small norms and less diversity. The experiments in
Sec. 4.2 show that RandReg degenerates the feature norm
âˆ¥gÏ•(x)âˆ¥2
2and entropy H(gÏ•(x)). This poses potential chal-
lenges for fine-tuning. First, the small feature norm may
vanish the gradient of target loss functions. In a classifica-
tion task, the gradient of the cross-entropy loss with respect
toWof the classifier is formulated as follows [10].
âˆ¥âˆ‡Wâ„“CE(fÎ¸(x), y)âˆ¥2
2=KX
k=1(fÎ¸(x)[k]âˆ’Î´yk)2âˆ¥gÏ•(x)âˆ¥2
2,(5)
where fÎ¸(x)[k] =WâŠ¤gÏ•(x)[k]is the output of the classifier
for the k-th class that is normalized by softmax function,
andÎ´ykis1ify=kotherwise 0. Since Eq. (5) contains
the product of âˆ¥gÏ•(x)âˆ¥2
2, degenerating âˆ¥gÏ•(x)âˆ¥2
2leads to
vanishing âˆ¥âˆ‡Wâ„“CEâˆ¥2
2, resulting in stagnation of fine-tuning.
Second, the low entropy limits the modelâ€™s ability to learn
effective feature representations for solving the target task
in terms of mutual information, which is strongly related to
model performance [ 14]. Mutual information I(gÏ•(x);y)
measures the amount of shared information (i.e., correlation)
between the feature vector gÏ•(x)and label yas defined by
I(gÏ•(x);y) =H(gÏ•(x))âˆ’H(gÏ•(x)|y). (6)
The first term on the right-hand side H(gÏ•(x))can be in-
terpreted as the â€œdiversityâ€ of features and the second term
âˆ’H(gÏ•(x)|y)as the â€œtightnessâ€ for each class [ 3,35]. In
this sense, RandReg limits the mutual information of the
feature representations by decreasing the diversity term
H(gÏ•(x)). Although RandReg can increase the mutual in-
formation than the baseline as shown in Fig. 2d, there is a
potential to learn a more effective feature representation if
we resolve the decrease of H(gÏ•(x)).
23484
0 50 100 150 200
epoch101520||g(x)||2
2
FT
RandReg-U(0,1)
AdaRand(a) Feature Norm
0 50 100 150 200
Epoch0.00.10.20.30.40.5Train LossFT Train Loss
RandReg Train Loss
AdaRand Train Loss
FT Grad Norm
RandReg Grad Norm
AdaRand Grad Norm
0.51.01.52.02.53.0
Grad Norm(b) Gradient Norm
0 50 100 150 200
epoch55006000650070007500H(g(x))
FT
RandReg-U(0,1)
AdaRand (c) Feature Entropy
0 50 100 150 200
Epoch0.00.51.01.52.02.53.0Validation LossFT Val Loss
RandReg Val Loss
AdaRand Val Loss
FT Mutual Info.
RandReg Mutual Info.
AdaRand Mutual Info.
200300400500600700
Mutual Information (d) Mutual Information
Figure 2. Statistics on feature vectors during training (Cars, ResNet-50 pre-trained with ImageNet classification). RandReg tends to decrease
the feature norm ||gÏ•(x)||2
2and entropy H(gÏ•(x)). This implicitly limits the gradient norm of cross-entropy loss âˆ¥âˆ‡Wâ„“CE(fÎ¸(x), y)âˆ¥2
2
and the discriminability of features represented by mutual information I(gÏ•(x);y). In contrast, our AdaRand prevents the models from
decreasing ||gÏ•(x)||2
2andH(gÏ•(x)), and thus achieves larger âˆ¥âˆ‡Wâ„“CE(fÎ¸(x), y)âˆ¥2
2andI(gÏ•(x);y)than RandReg.
Algorithm 1 AdaRand
Require: Training dataset D, target model fÎ¸, training batchsize
B, step size Î·andÎ¾, trade-off parameter Î»
Ensure: Trained classifier fÎ¸
1:Initialize ÂµandÏƒ2for conditional priors by Eq. (9) and (10).
2:Â¯Âµâ†Âµ
3:while not converged do
4:{(xi, yi)}B
i=1âˆ¼ D
5:{zi|ziâˆ¼ N(Âµyi, Ïƒ2
yi)}B
i=1
6: // Updating fÎ¸=WâŠ¤gÏ•
7: Î¸â†Î¸âˆ’Î·
BPB
i=1âˆ‡Î¸(â„“CE(fÎ¸(xi), yi)+Î»âˆ¥gÏ•(xi)âˆ’ziâˆ¥2
2)
8: // Updating conditional priors
9: Â¯Âµâ†ema_update( {(gÏ•(xi), yi)}B
i=1)// Eq. (14),(15)
10: Âµâ†Âµâˆ’Î¾âˆ‡Âµ(â„“intra(Âµ,Â¯Âµ)+â„“inter(Âµ))// Eq. (11)-(13)
11:end while
5. Proposed Method
We propose adaptive random feature regularization
(AdaRand) by extending RandReg in terms of prior dis-
tributions. AdaRand adopts adaptive class conditional priors,
which automatically search for prior parameters and pre-
vent feature vectors from degenerating the norm and entropy
(Fig. 1). The overall procedure of AdaRand is described in
Algorithm 1. Before fine-tuning, we initialize the conditional
priors by computing the mean and variance of feature vectors
of target data on pre-trained models. During fine-tuning, we
regularize target models by the reference vectors from the
conditional priors and then update the mean parameters of
the prior by approaching them to the running feature mean
on training models ( â„“intra) and maximizing the distances
between classes ( â„“inter).
5.1. Conditional Prior
Overview. To overcome the challenge of the less diverse
feature vectors in RandReg, we introduce conditional prior
distributions for generating the reference vectors. As illus-
trated in Fig. 1, RandReg uses class-agnostic prior such as a
uniform distribution. This prevents models from naturallyenlarging the marginal entropy H(gÏ•(x))to classify labels
of input data. In order to achieve high performance in target
classification tasks, the clusters of feature vectors should be
separated for each class. In this sense, the reference vector
for the regularization term should be conditioned by class
labels of input data. Then, the prior of AdaRand can be
regarded as the marginal distribution over class labels.
Definition. The prior distribution is defined as follows.
p(z) =KX
k=1p(z|yk)p(yk), (7)
where p(z|yk) =N(Âµk, Ïƒ2
kI). We formalize p(z)as a mix-
ture of diagonal Gaussian distributions because the feature
vectors of softmax neural classifiers are known to follow
class conditional Gaussian distributions [ 2,17]. When sam-
pling the reference vectors, we generate a random reference
vector zifrom the class label yiof the input xi, i.e.,
ziâˆ¼ N(Âµyi, Ïƒ2
yiI). (8)
Using these conditional reference vectors in Eq. (3), we can
expect that the regularization effect of random noise can be
enjoyed while avoiding the decrease of H(gÏ•(x))due to
class-agnostic prior distributions.
Initialization. For the prior distribution, we initialize Âµk
andÏƒ2
kto the statistics of the pre-trained model and update
them adaptively. As discussed in Sec. 4.3, the hyperparame-
ter searches for the initial parameters are challenging. There-
fore, we initialize ÂµkandÏƒ2
kwith the class-wise mean and
variance of feature vectors computed on target data through
pre-trained models as
Âµk=1
NkPNk
i=1gÏ•s(xi), (9)
Ïƒ2
k=1
NkPNk
i=1(gÏ•s(xi)âˆ’Âµk)2, (10)
where Nkis the number of samples labeled as a class kand
gÏ•sis the pre-trained feature extractor before fine-tuning.
For simple notation in the latter sections, we denote the
sets of mean and variance parameters as Âµ={Âµk}K
k=1and
Ïƒ2={Ïƒ2
k}K
k=1.
23485
5.2. Adaptive Prior Update
Overview. During fine-tuning, we adaptively optimize the
prior parameter set Âµto adjust the reference vectors ac-
cording to the training progress of the feature extractors.
This addresses the challenges of fixed prior distributions and
small feature norms of RandReg. That is, we aim to avoid
producing small feature norms without manually searching
for prior parameters. Furthermore, we penalize Âµso that
eachÂµkis independent to the other class parameters {Âµl}lÌ¸=k.
This facilitates models to form tight class conditional feature
clusters, corresponding to maximizing the tightness term
âˆ’H(gÏ•(x)|y)of the mutual information given by Eq. (6).
Objective Function. The objective function for updating
Âµis defined as follows.
Lada=â„“intra(Âµ,Â¯Âµ) +â„“inter(Âµ), (11)
â„“intra(Âµ,Â¯Âµ) =1
KPK
k=1D(Âµk,Â¯Âµk), (12)
â„“inter(Âµ) =âˆ’1
K(Kâˆ’1)PK
k=1PK
lÌ¸=kD(Âµk, Âµl),(13)
where, Â¯Âµis the running mean vectors of gÏ•(x)in training,
D(Â·)is a distance function such as cosine distance, i.e.,
1âˆ’ÂµkÂ·Â¯Âµk
âˆ¥Âµkâˆ¥2âˆ¥Â¯Âµkâˆ¥2. We update Â¯Âµby exponential moving average
(EMA) with training samples in batch for each step as
Â¯Âµkâ†Î±Â¯Âµk+ (1âˆ’Î±)Ë†Âµk, (14)
Ë†Âµk=1
BkPBk
i=1gÏ•(xi), (15)
where Î±is a parameter for controlling the decay of the past
information. We fix Î±= 0.5throughout this paper; we
evaluate the effect of Î±in the supplement. If there are no
samples belonging to a class kin the batch, the EMA update
will be skipped. For D(Â·), we used the cosine distance in
our experiments because it achieved the best performance
empirically. Intuitively, â„“intra makes Âµkapproach the feature
region that the model is currently learning by fine-tuning,
and encourages the model to focus on learning the current
region through the reference random vectors. Meanwhile,
â„“inter corresponds to maximizing H(gÏ•(x))because the
k-th class mean parameter Âµkis penalized for moving away
from other class parameters {Âµl}lÌ¸=k. Then, the class-wise
reference vectors help to gather features for each class (i.e.,
maximizing âˆ’H(gÏ•(x)|y)).
6. Experiments
We evaluate AdaRand on the combinations of six visual
classification tasks, four pre-training methods, and three
neural network architectures. Furthermore, we conduct qual-
itative and quantitative experiments to assess the feature
space through PCA visualization, feature norms, loss gra-
dients, and mutual information. We also show the other
experiments, including fine-tuning CLIP on ImageNet, in
the supplementary.6.1. Settings
Baselines w/o source information. We compare AdaRand
with the following baselines. Fine-tuning (FT) : training fÎ¸
with pre-trained weight Î¸s.FNP [10]: fine-tuning â„“2penalty
ongÏ•(x), i.e.,âˆ¥gÏ•(x)âˆ¥2
2.L2SP [18]: fine-tuning fÎ¸with
anâ„“2penalty on Î¸not to diverge from Î¸s, i.e.,âˆ¥Î¸âˆ’Î¸sâˆ¥2
2.
DELTA [19]: fine-tuning fÎ¸with a penalty on the intermedi-
ate output not to diverge from one of Î¸s.BSS [5]: fine-tuning
fÎ¸by penalizing the singular values of the batch feature ma-
trices. RandReg [37]: fine-tuning fÎ¸with a penalty term to
minimize the gap between feature vectors gÏ•(x)and refer-
ence vectors zâˆ¼p(z), i.e.,âˆ¥gÏ•(x)âˆ’zâˆ¥2
2.Core-tuning [36]:
fine-tuning fÎ¸with supervised focal contrastive loss. DR-
Tune [38]: fine-tuning fÎ¸by penalizing Wto classify the
features extracted from source models into target classes.
Baselines w/ source information. To assess the prac-
ticality of AdaRand, we additionally used the following
baseline methods requiring auxiliary source information.
Co-Tuning [33]: fine-tuning fÎ¸with simultaneous pseudo
source tasks defined by mapping target and source class la-
bels. UOT [21]: fine-tuning fÎ¸with simultaneous partial
source tasks by extracting source samples related to target
tasks with optimal-transport-based mapping algorithm.
Datasets. We used six image datasets for classification
tasks in various domains: Aircraft [23],Birds [31],
Cars [15],DTD [6],Flowers [24], and Pets [26]. Fur-
thermore, we reduced the Cars dataset by {10,25,50}%in
volume on a fixed random seed to evaluate smaller dataset
cases. We randomly split a dataset into 9 : 1 and used the
former as the training set and the latter as the validation set.
Architectures. We basically used the ResNet-50 architec-
ture [ 11]. To confirm the flexibility among architectures, we
also evaluated our method with ViT-B [7].
Training. As the pre-training methods, we used supervised
classification [ 32] and self-supervised pre-training (Sim-
CLR [ 4] and Barlow Twins [ 34]) on ImageNet. Further,
we used the pre-trained weights of CLIP [ 27]. For train-
ing on downstream classification tasks with ResNet-50, we
trained fÎ¸by the Nesterov momentum SGD for 200 epochs
with a momentum of 0.9, and an initial learning rate of 0.01;
we decayed the learning rate by 0.1 at 60, 120, and 160
epochs. For ViT-B, we used the AdamW [ 22] optimizer
with the initial learning rate of 3.0 Ã—10âˆ’5that decayed by
cosine annealing. We used mini-batch sizes of 64. The input
samples were resized into a resolution of 224Ã—224. We
usedÎ»of1.0; we discuss the effect of Î»in the supplement.
We selected the final model by checking the validation ac-
curacy for each epoch. We implemented the training and
evaluation with PyTorch-1.11. We ran the experiments three
times on a 24-core Intel Xeon CPU with an NVIDIA A100
23486
Table 2. Top-1 test accuracy (%) of various combinations of pre-training methods and neural network architectures (Cars).
Pre-training Method Architecture Fine-tuning FNP [10] DR-Tune [38] RandReg-Best AdaRand (Ours)
ImageNet Classification RN-50 89.14Â±.4290.27Â±.1090.38Â±.5990.61Â±.2491.17Â±.13
ImageNet Classification ViT-B/32 78.56Â±1.381.77Â±.2179.49Â±.5182.46Â±.2083.84Â±.13
ImageNet Classification ViT-B/16 87.35Â±.5388.75Â±.4488.19Â±.2688.88Â±.2689.54Â±.17
ImageNet SimCLR [4] RN-50 83.73Â±.7384.53Â±.3284.05Â±.1784.08Â±.2185.51Â±.05
ImageNet Barlow Twins [34] RN-50 86.98Â±.1687.44Â±.1586.69Â±.2387.74Â±.3388.23Â±.39
CLIP [27] RN-50 88.72Â±.2490.19Â±.4090.16Â±.2290.78Â±.0791.25Â±.63
CLIP [27] ViT-B/32 83.56Â±.6085.79Â±.5285.71Â±.5986.83Â±.3487.40Â±.48
CLIP [27] ViT-B/16 90.35Â±.2391.24Â±.0390.47Â±.2691.33Â±.4492.84Â±.48
Table 3. Top-1 test accuracy (%) on multiple datasets (ResNet-50
pre-trained with ImageNet classification).
Method / Dataset Aircraft Birds Cars DTD Flower Pets
Fine-tuning 67.78Â±.0974.80Â±.5688.29Â±.1263.88Â±.3194.58Â±.2189.45Â±.18
FNP [10] 70.27Â±.4279.57Â±.4190.27Â±.1072.13Â±.0495.07Â±.0691.21Â±.12
DR-Tune [38] 73.89Â±.1076.63Â±.3190.38Â±.5970.55Â±.2893.43Â±.1793.24Â±.11
RandReg- U(0,1) 71.65Â±.5979.82Â±.2190.42Â±.2472.32Â±.4196.02Â±.0591.72Â±.11
RandReg- N(0,1) 73.11Â±.8880.48Â±.0690.32Â±.4172.57Â±.2695.01Â±.1191.80Â±.12
RandReg- N(Âµs, Ïƒ2
s)72.08Â±.5080.20Â±.0190.61Â±.2471.77Â±.7895.16Â±.1591.81Â±.11
RandReg-CP 72.10Â±.2080.02Â±.2090.55Â±.1772.34Â±.2795.86Â±.3091.76Â±.61
AdaRand (Ours) 74.60Â±.1081.27Â±.2691.17Â±.1374.86Â±.2296.68Â±.1492.34Â±.26
GPU with 40GB VRAM and recorded average test accuracy
with standard deviation evaluated on the final models.
6.2. Evaluation on Multiple Pre-trained Models
One of our motivations is to develop a method that achieves
high performance on arbitrary pre-training methods without
relying on auxiliary source information and additional heavy
computation. Here, to verify that this motivational goal is
achieved, we evaluate AdaRand with multiple combinations
of pre-training methods and architectures such as CLIP and
ViT-B. In this experiment, we compare our method with
RandReg, FNP, and DR-Tune, which are available without
auxiliary source information. Table 2 shows that AdaRand
stably outperforms RandReg and the other baselines for
all combinations of pre-training methods and architectures.
In particular, AdaRand overcomes the negative effects on
SimCLR observed in Sec. 4. These results indicate that the
adaptive prior update of AdaRand is widely effective for
many pre-trained models.
6.3. Evaluation on Multiple Datasets
We evaluate AdaRand on multiple different datasets to
demonstrate its generality across the datasets. Table 3 shows
the results. Except for the Pets dataset, AdaRand achieved
the best results for all of the datasets. While the perfor-
mance of RandReg depends on the hyperparameters of priors,
AdaRand stably performs by the adaptive prior update.
6.4. Evaluation on Small Datasets
We evaluate AdaRand by reducing the data volume of Cars
into{10,25,50}%. Table 4 shows that AdaRand still out-Table 4. Top-1 test accuracy (%) on small training datasets (Cars,
ResNet-50 pre-trained with ImageNet classification).
Method / Dataset Size (%) 10 % 25 % 50%
Fine-tuning 19.58Â±.0753.10Â±.0877.86Â±.09
FNP [10] 23.68Â±.3457.07Â±.3479.93Â±.21
DR-Tune [38] 23.68Â±.3457.07Â±.3479.93Â±.21
RandReg- U(0,1) 25.35Â±.1458.33Â±.4580.91Â±.51
RandReg- N(0,1) 26.15Â±.2159.94Â±.5881.74Â±.41
RandReg- N(Âµ0, Ïƒ2
0) 24.95Â±.0559.66Â±.2481.31Â±.17
RandReg-CP 26.45Â±.2659.37Â±.4681.13Â±.23
AdaRand (Ours) 27.55Â±.1061.09Â±.5482.19Â±.19
Table 5. Comparison among fine-tuning regularization methods
(Cars, ResNet-50 pre-trained with ImageNet classification). We
measure top-1 test accuracy, averaged training time per epoch, and
GPU memory usage. Our method (AdaRand) outperforms the other
baselines in accuracy while maintaining reasonable computation
time and GPU memory consumption.
Method Test Accuracy (%) Time / Epoch (sec.) GPU Mem. (MiB)
Fine-tuning 88.29Â±.129.737 8,073
FNP [10] 90.27Â±.069.916 8,073
L2SP [18] 88.50Â±.2512.405 8,153
DELTA [19] 89.00Â±.2414.733 9,211
BSS [5] 89.70Â±.0611.403 8,227
Core-tuning [36] 90.01Â±.1316.872 24,223
DR-Tune [38] 90.38Â±.5925.475 8,845
Co-Tuning [33] 90.66Â±.3411.546 8,125
UOT [21] 90.82Â±.1912.405 14,293
RandReg- U(0,1) 90.42Â±.249.988 8,075
RandReg- N(0,1) 90.32Â±.419.971 8,075
RandReg- N(Âµs, Ïƒ2
s) 90.61Â±.249.963 8,075
RandReg-CP 90.55Â±.1711.552 8,075
AdaRand w/o â„“intra 90.81Â±.2112.410 8,585
AdaRand w/o â„“inter 90.99Â±.0612.841 8,585
AdaRand (Ours) 91.17Â±.1313.824 8,585
performs the baselines even with a limited dataset of less
than a few training samples per class (i.e., 10%). The re-
sults demonstrate that AdaRand performs well in data-scarce
scenarios.
6.5. Evaluation on Comparison to SoTA Methods
We demonstrate the efficacy of AdaRand by comparing it
with the SoTA baselines. Table 5 shows the results on the
Cars dataset with the ResNet-50 architecture pre-trained
23487
20
 0 20 40 60 8010
5
05101520(a) Fine-tuning
2
 1
 0 1 22
1
012 (b) RandReg- U(0,1)
1
 0 11
012 (c) RandReg- N(0,1)
1
 0 1 22
1
012
(d) RandReg- N(Âµs, Ïƒ2
s)
4
 2
 0 2 4 64
2
024 (e) RandReg-CP
20
 15
 10
 5
 0 5 1010
5
0510 (f) AdaRand (Ours)
Figure 3. PCA visualization of feature spaces of trained models (CIFAR-10, ResNet-50). The colors in the sample plot correspond to that
class. AdaRand clearly forms well-separated clusters, which can be useful for solving downstream classification tasks.
on the ImageNet classification task. We confirm that our
AdaRand achieved better performance than the methods
leveraging auxiliary source information (i.e., Co-Tuning and
UOT) or additional heavy computation costs (i.e., Core-
tuning and DR-Tune). The bottom part of Table 5 also
shows an ablation study of AdaRand, where RandReg-CP
is a method using the conditional prior defined in Eq. (7)
without updating them. AdaRand significantly improved
the performance of RandReg-CP and the losses of â„“inter
andâ„“intra complementarily contributed to the improvements.
This indicates that naÃ¯vely introducing the conditional prior
is not sufficient to solve the challenges of RandReg and adap-
tively updating priors is effective in terms of performance.
6.6. Analysis and Discussion
Through the experiments in the previous sections, we vali-
date that AdaRand stably improves the accuracy of the down-
stream classification tasks in various settings and overcomes
the RandRegâ€™s challenge caused by using fixed priors dis-
cussed in the former paragraph of Sec. 4.3. Here, we provide
the analysis of AdaRand to confirm whether it overcomes the
rest challenges of RandReg, i.e., decreasing feature norms
âˆ¥gÏ•(x)âˆ¥2
2and entropy H(gÏ•(x)). Figure 2 demonstrates that
AdaRand succeeds in preventing the decreasing âˆ¥gÏ•(x)âˆ¥2
2
andH(gÏ•(x)). As a result, AdaRand achieves better the gra-
dient norm of cross-entropy loss âˆ¥âˆ‡Wâ„“CE(fÎ¸(x), y)âˆ¥2
2and
mutual information I(gÏ•(x);y)than RandReg. This also
can be an explanation for the reason why AdaRand stably
outperforms RandReg in the previous sections.
Finally, we discuss the effects of AdaRand on featurespaces by visualizing feature vectors from trained models
with the PCA dimension reduction. As the dataset, we used
CIFAR-10 [ 16]. We trained ResNet-50 by each method and
reduced the dimensions of the output of gÏ•(x)by PCA. We
randomly selected 1,024 samples from the test set for the
input. The visualization results are illustrated in Figure 3.
While the baseline method forms less separated feature clus-
ters, AdaRand clearly forms independent and dense feature
clusters for each class. This indicates that the adaptive prior
update of AdaRand helps models learn useful representations
for solving downstream classification tasks.
7. Conclusion
This paper presents a novel regularization method for fine-
tuning deep neural networks called AdaRand. AdaRand
penalizes feature vectors of deep models by guiding to the
random reference vectors that are generated from the class
conditional prior distributions. To encourage the fine-tuning
models to generate useful features for target classification
tasks, AdaRand adaptively updates the conditional prior
distributions so that the prior distributions are close to the
current feature distribution and balance the distance between
classes. Through this simple method, The fine-tuned model
increases the amount of mutual information between features
and labels, resulting in a significant improvement in final
test accuracy without either auxiliary source information or
additional heavy computation costs. An important future
step is to extend AdaRand beyond discriminative tasks such
as generative modeling by diffusion models.
23488
References
[1]Pulkit Agrawal, Ross Girshick, and Jitendra Malik. Analyzing
the performance of multilayer neural networks for object
recognition. In European conference on computer vision ,
2014. 3
[2]Christopher M Bishop. Neural networks for pattern recogni-
tion. Oxford university press, 1995. 5
[3]Malik Boudiaf, JÃ©rÃ´me Rony, Imtiaz Masud Ziko, Eric
Granger, Marco Pedersoli, Pablo Piantanida, and Ismail Ben
Ayed. A unifying mutual information view of metric learning:
cross-entropy vs. pairwise losses. In European conference on
computer vision , pages 548â€“564, 2020. 4
[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on
machine learning , 2020. 3, 4, 6, 7
[5]Xinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, and
Jianmin Wang. Catastrophic forgetting meets negative trans-
fer: Batch spectral shrinkage for safe transfer learning. In
Advances in Neural Information Processing Systems , 2019. 2,
6, 7
[6]M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.
Vedaldi. Describing textures in the wild. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2014. 6
[7]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions , 2021. 6
[8]Lev Faivishevsky and Jacob Goldberger. Ica based on a
smooth estimation of the differential entropy. In Advances in
neural information processing systems , 2008. 4
[9]Weifeng Ge and Yizhou Yu. Borrowing treasures from the
wealthy: Deep transfer learning through selective joint fine-
tuning. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 1086â€“1095, 2017. 2
[10] Bharath Hariharan and Ross Girshick. Low-shot visual recog-
nition by shrinking and hallucinating features. In Proceedings
of the IEEE International Conference on Computer Vision ,
pages 3018â€“3027, 2017. 1, 3, 4, 6, 7
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , 2016. 3, 6
[12] Kaiming He, Ross Girshick, and Piotr DollÃ¡r. Rethinking ima-
genet pre-training. In International Conference on Computer
Vision , pages 4918â€“4927, 2019. 1
[13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , 2020.
3
[14] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,
Karan Grewal, Phil Bachman, Adam Trischler, and YoshuaBengio. Learning deep representations by mutual information
estimation and maximization. In International Conference on
Learning Representations , 2019. 4
[15] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
4th International IEEE Workshop on 3D Representation and
Recognition , Sydney, Australia, 2013. 3, 6
[16] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Citeseer,
2009. 8
[17] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A
simple unified framework for detecting out-of-distribution
samples and adversarial attacks. In Advances in neural infor-
mation processing systems , 2018. 5
[18] Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit
inductive bias for transfer learning with convolutional net-
works. In International Conference on Machine Learning ,
2018. 1, 2, 6, 7
[19] Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Lip-
ing Liu, Zeyu Chen, and Jun Huan. Delta: Deep learning
transfer using feature map with attention for convolutional
networks. In International Conference on Learning Repre-
sentations , 2019. 1, 2, 6, 7
[20] Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Xi-
angyang Ji, Antoni Chan, and Rong Jin. Improved fine-tuning
by better leveraging pre-training data. 2022. 1
[21] Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Xi-
angyang Ji, Antoni B. Chan, and Rong Jin. Improved fine-
tuning by better leveraging pre-training data. In Advances in
Neural Information Processing Systems , 2022. 2, 6, 7
[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization, 2019. 6
[23] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
Fine-grained visual classification of aircraft. arXiv , 2013. 6
[24] M-E. Nilsback and A. Zisserman. Automated flower classifi-
cation over a large number of classes. In Proceedings of the
Indian Conference on Computer Vision, Graphics and Image
Processing , 2008. 6
[25] Tianyu Pang, Chao Du, and Jun Zhu. Max-mahalanobis linear
discriminant analysis networks. In International Conference
on Machine Learning , pages 4016â€“4025. PMLR, 2018. 3
[26] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and
C. V . Jawahar. Cats and dogs. In IEEE Conference on Com-
puter Vision and Pattern Recognition , 2012. 6
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
InInternational conference on machine learning , 2021. 1, 3,
4, 6, 7
[28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision , 115(3), 2015. 1
[29] Masaaki Takada and Hironori Fujisawa. Transfer learning
viaâ„“1regularization. In Advances in Neural Information
Processing Systems , 2020. 3
23489
[30] Weitao Wan, Cheng Yu, Jiansheng Chen, Tong Wu, Yuanyi
Zhong, and Ming-Hsuan Yang. Shaping deep feature space
towards gaussian mixture for visual classification. IEEE
transactions on pattern analysis and machine intelligence , 45
(2):2430â€“2444, 2022. 3
[31] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-UCSD Birds 200. Technical
report, California Institute of Technology, 2010. 6
[32] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
Advances in Neural Information Processing Systems , 2014. 3,
6
[33] Kaichao You, Zhi Kou, Mingsheng Long, and Jianmin Wang.
Co-tuning for transfer learning. Advances in Neural Informa-
tion Processing Systems , 2020. 1, 2, 6, 7
[34] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and StÃ©phane
Deny. Barlow twins: Self-supervised learning via redundancy
reduction. In International Conference on Machine Learning ,
2021. 3, 4, 6, 7
[35] Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng,
and Angela Yao. Improving deep regression with ordinal
entropy, 2023. 4
[36] Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, and Jiashi
Feng. Unleashing the power of contrastive self-supervised
visual models via contrast-regularized fine-tuning. Advances
in Neural Information Processing Systems , 2021. 1, 6, 7
[37] Yang Zhong and Atsuto Maki. Regularizing cnn transfer
learning with randomised regression. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13637â€“13646, 2020. 1, 3, 4, 6
[38] Nan Zhou, Jiaxin Chen, and Di Huang. Dr-tune: Improv-
ing fine-tuning of pretrained visual models by distribution
regularization with semantic calibration. In Proceedings of
the IEEE/CVF International Conference on Computer Vision ,
2023. 1, 6, 7
23490
