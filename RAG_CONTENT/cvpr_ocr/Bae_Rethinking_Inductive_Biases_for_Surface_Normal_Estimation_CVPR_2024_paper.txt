Rethinking Inductive Biases for Surface Normal Estimation
Gwangbin Bae Andrew J. Davison
Dyson Robotics Lab, Imperial College London
{g.bae, a.davison }@imperial.ac.uk
Figure 1. Examples of challenging in-the-wild images and their surface normals predicted by our method.
Abstract
Despite the growing demand for accurate surface nor-
mal estimation models, existing methods use general-
purpose dense prediction models, adopting the same induc-
tive biases as other tasks. In this paper, we discuss the
inductive biases needed for surface normal estimation and
propose to (1) utilize the per-pixel ray direction and (2) en-
code the relationship between neighboring surface normals
by learning their relative rotation. The proposed method
can generate crisp ‚Äî yet, piecewise smooth ‚Äî predictions
for challenging in-the-wild images of arbitrary resolution
and aspect ratio. Compared to a recent ViT-based state-
of-the-art model, our method shows a stronger generaliza-
tion ability, despite being trained on an orders of magni-
tude smaller dataset. The code is available at https:
//github.com/baegwangbin/DSINE .
1. Introduction
We address the problem of estimating per-pixel surface nor-
mal from a single RGB image. This task, unlike monocu-
lar depth estimation, is not affected by scale ambiguity and
has a compact output space (a unit sphere vs. positive real
value), making it feasible to collect data that densely cov-ers the output space. As a result, learning-based surface
normal estimation methods show strong generalization ca-
pability for out-of-distribution images, despite being trained
on relatively small datasets [2].
Despite their essentially local property, predicted sur-
face normals contain rich information about scene geom-
etry. In recent years, their usefulness has been demon-
strated for various computer vision tasks, including im-
age generation [62], object grasping [60], multi-task learn-
ing [36], depth estimation [3, 41], simultaneous localization
and mapping [63], human body shape estimation [5, 54, 55],
and CAD model alignment [33]. However, despite the
growing demand for accurate surface normal estimation
models, there has been little discussion on the right induc-
tive biases needed for the task.
State-of-the-art surface normal estimation methods [2,
14, 29, 56] use general-purpose dense prediction models,
adopting the same inductive biases as other tasks (e.g.
depth estimation and semantic segmentation). For example,
CNN-based models [2, 14] assume translation equivariance
and use the same set of weights for different parts of the
image. While such weight-sharing can improve sample ef-
ficiency [34], it is sub-optimal for surface normal estima-
tion as a pixel‚Äôs ray direction provides important cues and
constraints for its surface normal. This has limited the accu-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9535
racy of the prediction and the ability to generalize to images
taken with out-of-distribution cameras.
Another important aspect of surface normal estimation
overlooked by existing methods is that there are common
typical relationships between the normals at nearby im-
age pixels. Many 3D objects in a scene are piecewise
smooth [24] and neighboring normals often have similar
values. There is also a very frequently occurring relation-
ship between groups of nearby pixels on two surfaces in
contact, or between groups of pixels on a continuously curv-
ing surface: their normals are related by a rotation through
a certain angle, about an axis lying within the surface and
which is sometimes visible in the image as an edge.
In this paper, we provide a thorough discussion of the in-
ductive biases needed for deep learning-based surface nor-
mal estimation and propose three architectural changes to
incorporate such biases:
‚Ä¢ We supply dense pixel-wise ray direction as input to the
network to enable camera intrinsics-aware inference and
hence improve the generalization ability.
‚Ä¢ We propose a ray direction-based activation function to
ensure the visibility of the prediction.
‚Ä¢ We recast surface normal estimation as rotation estima-
tion, where the relative rotation with respect to the neigh-
boring pixels is estimated in the form of axis-angle rep-
resentation. This allows the model to generate piecewise
smooth predictions that are crisp at surface boundaries.
The proposed method shows strong generalization abil-
ity. It can generate highly detailed predictions even for chal-
lenging in-the-wild images of arbitrary resolution and as-
pect ratio (see Fig. 1). We outperform a recent ViT-based
state-of-the-art method [14, 29] ‚Äî both quantitatively and
qualitatively ‚Äî despite being trained on an orders of mag-
nitude smaller dataset.
2. Related work
Hoiem et al. [22, 23] were among the first to propose a
learning-based approach for monocular surface normal es-
timation. The output space was discretized and handcrafted
features were extracted to classify the normals. Fouhey et
al. [18] took a different approach and tried to detect geomet-
rically informative primitives from data. For detected primi-
tives, the normal maps of the corresponding training patches
were aligned to recover a dense prediction. Another com-
mon approach was to assume a Manhattan World [10] to
adjust the initial prediction [18] or generate candidate nor-
mals from pairs of vanishing points [19].
Following the success of deep convolutional neural net-
works in image classification [32], many deep learning-
based methods [4, 15, 53] were introduced. Since then,
notable contributions have been made by exploiting the sur-
face normals computed from Manhattan lines [51], intro-
ducing a spatial rectifier to handle tilted images [12], andestimating the aleatoric uncertainty to improve the perfor-
mance on small structures and near object boundaries [2].
Eftekhar et al. [14] trained a U-Net [45] on more than 12
million images covering diverse scenes and camera intrin-
sics. They recently released an updated model by training
a transformer-based model [42] with sophisticated 3D data
augmentation [29] and cross-task consistency [59]. This
model is the current state-of-the-art in surface normal es-
timation and will be the main comparison for our method.
3. Inductive bias for surface normal estimation
In this section, we discuss the inductive biases needed for
surface normal estimation. Throughout the rest of this pa-
per, we use the right-hand convention for camera-centered
coordinates, where the X,Y, andZaxes point right, down,
and front, respectively.
3.1. Encoding per-pixel ray direction
Under perspective projection, each pixel is associated with
a ray that passes through the camera center and intersects
the image plane at the pixel. Assuming a pinhole camera, a
ray of unit depth for a pixel at (u, v)can be written as
r(u, v) =u‚àícu
fuv‚àícv
fv1‚ä∫, (1)
where fuandfvare the focal lengths and (cu, cv)are the
pixel coordinates of the principal point.
Per-pixel ray direction is essential for surface normal es-
timation. For rectangular structures (e.g. buildings), we can
identify sets of parallel lines and their respective vanishing
points. The ray direction at the vanishing point then gives
us the 3D orientation of the lines and hence the surface nor-
mals [21]. Early works on single-image 3D reconstruction
[19, 25, 31, 35] made explicit use of such cues.
Now consider an occluding boundary created by a
smooth (i.e. infinitely differentiable) surface. As Marr [38]
pointed out, the surface normals at an occluding boundary
can be determined uniquely by forming a generalized cone
(whose apex is at the camera center) that intersects the im-
age plane at the boundary. In other words, the normals at
the boundary should be perpendicular to the ray direction
(see Fig. 2-a). Such insights have been widely adopted for
under-constrained 3D reconstruction tasks such as single-
image shape from shading [28].
Lastly, the ray direction decides the range of normals that
would be visible in that pixel, effectively halving the output
space (see Fig. 2-b). This is analogous to the case of depth
estimation, where the output should be positive. Such an
inductive bias is often adopted by interpreting the network
output as log depth [16] or by using a ReLU activation [61].
Despite the aforementioned usefulness, state-of-the-art
methods [2, 14] do not encode the ray direction and use
CNNs with translational weight sharing, preventing the
9536
(a) (b) (c) (d)Figure 2. Motivation. In this paper, we propose to utilize the per-pixel ray direction and estimate the surface normals by learning the
relative rotation between nearby pixels .(a)Ray direction serves as a useful cue for pixels near occluding boundaries as the normal should
be perpendicular to the ray. (b)It also gives us the range of normals that would be visible, effectively halving the output space. (c)The
surface normals of certain scene elements (in this case, the floor) may be difficult to estimate due to the lack of visual cues. Nonetheless,
we can infer their normals by learning the pairwise relationship between nearby normals (e.g. which surfaces should be perpendicular).
(d)The relative angle between the normals of the yellow pixels can be inferred from that of the red pixels by assuming circular symmetry.
model from learning ray direction-aware inference. While
recent transformer-based models [29, 56] have the capabil-
ity of encoding the ray direction in the form of learned po-
sitional encoding, it is not trivial to inter/extrapolate the po-
sitional encoding when testing the model on images taken
with out-of-distribution intrinsics.
3.2. Modeling inter-pixel constraints
Consider a pixel iand its neighboring pixel j. Since their
surface normals have unit length and share the same origin
(camera center), they are related by a 3D rotation matrix
R‚ààSO(3). While there are different ways to parameter-
izeR, we choose the axis-angle representation, Œ∏=Œ∏e,
where a unit vector erepresents the axis of rotation and
Œ∏is the angle of rotation. Then, the exponential map
exp : so(3)‚ÜíSO(3)‚Äî which is readily available in mod-
ern deep learning libraries [1, 43] ‚Äî can map Œ∏back to R.
Within flat surfaces (which are prevalent in man-made
scenes/objects), Œ∏would be zero and Rwould simply be the
identity. In a typical indoor scene, the surfaces of objects are
often perpendicular or parallel to the ground plane, creating
lines across which the normals should rotate by 90‚ó¶(see
Fig. 2-c). For a curved surface, the relative angle between
the pixels can be inferred from the occluding boundaries by
assuming a certain level of symmetry (see Fig. 2-d).
But why should we learn Rinstead of directly estimating
the normals? Firstly, learning the relative rotation is much
easier, as the angle between the normals, unlike the normals
themselves, is independent of the viewing direction (it is
also zero ‚Äî or close to zero ‚Äî for most pixel pairs). Find-
ing the axis of rotation is also straightforward. When two
(locally) flat surfaces intersect at a line, the normals rotate
around that intersection. As the image intensity generally
changes sharply near such intersections, the task can be as
simple as edge detection.Secondly, the estimated rotation can help improve the
accuracy for surfaces with limited visual cues. For instance,
while it is difficult to estimate the normal of a textureless
surface, the objects that are in contact with the surface can
provide evidence for its normal (see Fig. 2-c).
Lastly, as long as the relative rotations between the nor-
mals are captured correctly, any misalignment between the
prediction and the ground truth can be resolved via a single
global rotation. For example, in the case of a flat surface,
estimating inaccurate but constant normals is better than es-
timating accurate but noisy normals, as the orientation can
easily be corrected (e.g. via sparse depth measurements or
visual odometry). This is again analogous to depth estima-
tion where a relative depth map is easier to learn and can be
aligned via a global scaling factor.
4. Our approach
From Sec. 4.1 to 4.3, we explain how a dense prediction
network can be modified to encode the inductive biases dis-
cussed in Sec. 3. We then explain the network architecture
and our training dataset in Sec. 4.4 and 4.5.
4.1. Ray direction encoding
To avoid having to learn ray direction-aware prediction, one
can crop and zero-pad the images such that the principal
point is at the center and the field of view is always Œ∏‚ó¶,
where Œ∏is set to some high value. Then, a pixel at (u, v)
will always have the same ray direction, allowing the net-
work to encode it, e.g., in the form of positional encoding.
However, such an approach (1) wastes the compute for the
zero-padded regions, (2) loses high-frequency details from
downsampling, and (3) cannot be applied to images with
wider field of view. Instead, we compute the focal length-
normalized image coordinates ‚Äî i.e. Eq. 1 ‚Äî and provide
9537
Target FoV:90¬∞Input FoV: 60¬∞
ùë¢‚àíùëê!
ùëì!
ùë£‚àíùëê"
ùëì"Figure 3. Encoding camera intrinsics. (left) To avoid having
to learn camera intrinsics-aware prediction, one can zero-pad or
crop the images such that they always have the same intrinsics.
(right) Instead, we compute the focal length-normalized image
coordinates and provide them as additional input to the network.
this as an additional input to the intermediate layers of the
network (see Fig. 3). This encoding is similar to that of
CAM-Convs [17] which was designed for depth estimation.
Unlike [17], we do not encode the image coordinates them-
selves and only encode the ray direction.
4.2. Ray direction-based activation
A surface that is facing away from the camera would sim-
ply not be visible in the image. An important constraint
for surface normal estimation should thus be that the angle
between the ray direction and the estimated normal vector
must be greater than 90‚ó¶. To incorporate such a bias, we
propose a ray direction-based activation function analogous
to ReLU. Given the estimated normal nand ray direction r
(both are normalized), the activation can be written as
œÉray(n,r) :=n+ (min(0 ,n¬∑r)‚àín¬∑r)r
‚à•n+ (min(0 ,n¬∑r)‚àín¬∑r)r‚à•.(2)
Eq. 2 ensures that n¬∑r= cos Œ∏(i.e. the magnitude of n
along r) is less than or equal to zero. The rectified normal
is then re-normalized to have a unit length (see Fig. 4).
4.3. Learning rotation between nearby pixels
For pixel i, we can define its local neighborhood Ni={j:
|ui‚àíuj| ‚â§Œ≤and|vi‚àívj| ‚â§Œ≤}. We can then learn the
pairwise relationship between the surface normals niand
njin the form of a rotation matrix Rij.
For each pair of pixels, three quantities should be esti-
mated: First is the angle Œ∏ijbetween the two normals. This
is easy to learn as Œ∏ijis independent of the viewing direc-
tion and is 0‚ó¶or90‚ó¶for many pixel pairs. Secondly, we
need to estimate the axis of rotation eij(i.e. a 3D unit vec-
tor around which the normals rotate). While directly learn-
ingeijrequires complicated 3D reasoning, we propose a
simpler approach that only requires 2D information.
Ray 
ReLU
Ray 
ReLUFigure 4. Ray ReLU activation. An important constraint for sur-
face normal estimation is that the predicted normal should be vis-
ible. We achieve this by zeroing out the component that is in the
direction of the ray.
Let‚Äôs first consider the case where niandnjare on the
same smooth surface. As the surface can locally be approx-
imated as a plane, the angle should be close to zero. In such
a case, finding the axis is less important as the rotation ma-
trix will be close to the identity.
Another possibility is that the two points are on different
smooth surfaces that are intersecting with each other. In
this case, we only need to estimate the 2D projection ofeij.
Suppose that this 2D projection is a vector whose endpoints
are(uj, vj)and(uj+Œ¥uij, vj+Œ¥vij). The 3D vector eij
should then lie on a plane that passes through the camera
center and the two endpoints (see Fig 5-right). Formally,
this can be written as,
eij=Xc(uj+Œ¥uij, vj+Œ¥vij)‚àíXc(uj, vj)
=Ô£Æ
Ô£ØÔ£∞(zj+Œ¥zij)¬∑uj+Œ¥uij‚àícu
fu‚àízj¬∑uj‚àícu
fu
(zj+Œ¥zij)¬∑vj+Œ¥vij‚àícv
fv‚àízj¬∑vj‚àícv
fv
Œ¥zijÔ£π
Ô£∫Ô£ª,(3)
where Xc(¬∑,¬∑)are the camera-centered coordinates corre-
sponding to the pixel and Œ¥zrepresents the change in depth.
There are two unknowns in Eq. 3: zandŒ¥z. The first con-
straint for solving Eq. 3 is that ‚à•eij‚à•= 1. We can then
assume that the surface normal njis perpendicular to eij
(i.e.nj¬∑eij= 0), which is true for pixels near the intersec-
tion between two (locally) flat surfaces. Such an approach
is appealing for 2D CNNs ‚Äî whose initial layers are known
to be oriented edge filters ‚Äî as the image intensity tends to
change sharply near the intersection between two surfaces.
The final remaining possibility for niandnjis that they
are on two disconnected surfaces or on non-smooth sur-
faces. As estimating Rijin such a case is significantly
more challenging than the other two scenarios, we choose
to downweight them with a set of weights {wij}, which is
the last quantity we learn. The updated normal of pixel i
can be written as
9538
Rotation Angle
 Fusion Weight
Input Image (H, W, 3)
ConvGRU
Hidden state
Hidden state
 Context Feature
 Initial Normal
Rotation Axis
Rotation Matrices
Rotated Normals
Updated Normal
Upsampled Normal (H, W, 3)
Convex UpsamplingConvolutional encoder-decoder
(EfÔ¨ÅcientNet-B5 backbone)
Hidden state update
(can be replaced with a different RNN)
Learned upsamplingUpdate surface normals
by rotating 
neighboring pixels‚Äô normals√ó N
√ó NCNN
Figure 5. Network architecture. A lightweight CNN extracts a low-resolution feature map, from which the initial normal, hidden state
and context feature are obtained. The hidden state is then recurrently updated using a ConvGRU [9] unit. From the updated hidden state,
we estimate three quantities: rotation angle and axis to define a pairwise rotation matrix for each neighboring pixel; and a set of weights
that will be used to fuse the rotated normals.
nt+1
i=P
jwijœÉray(Rijnt
j,ri)
‚à•P
jwijœÉray(Rijnt
j,ri)‚à•
Rij= exp ( Œ∏ij[eij]√ó).(4)
where the ray-ReLU activation, introduced in Sec. 4.2, is
used to ensure that the rotated normals are in the visible
range for the target pixel i. We also added a superscript for
the normals to represent an iterative update.
To summarize, given some initial surface normal pre-
diction nt, the network should estimate the following three
quantities ‚Äî for each pixel i‚Äî in order to obtain the up-
dated normal map nt+1:
‚Ä¢ The rotation angles {Œ∏ij}for the neighboring pixels. For
the output, we use a sigmoid activation followed by a mul-
tiplication of œÄ.
‚Ä¢ 2D unit vectors {(Œ¥uij, Œ¥vij)}whose orientation repre-
sents the 2D projection of the rotation axes {eij}. We
use two output channels followed by an L2 normalization.
This, combined with {nt
j}gives us the axes of rotation.
‚Ä¢ The weights {wij}to fuse the rotated normals. We use a
sigmoid activation.
The process is then repeated for Nitertimes. In the fol-
lowing section, we explain how such inference can be done
in a convolutional recurrent neural network.
4.4. Network architecture
The components described in Sec. 4.1-4.3 are general and
can be adopted by most dense prediction neural networks
with minimal architectural changes. We use a light-weightCNN with a bottleneck recurrent unit (see Fig. 5). The ar-
chitecture is the same as that of [3] except for the quantities
that are estimated from the updated hidden state.
The initial prediction and the hidden state have the res-
olution of ( H/8√óW/8), where HandWare the input
height and width. Updating the normals in a coarse resolu-
tion allows us to model long-range relationships with small
compute. We set the neighborhood size Œ≤(mentioned in
Sec. 4.3) to 2 (i.e. 5√ó5neighborhood). The number of
surface normal updates Niteris set to 5, as it gave a good
balance between accuracy and computational efficiency. As
a result, each forward pass returns Niter+ 1 predictions
(initial prediction obtained via direct regression + Niterup-
dates). We then apply convex upsampling [50] to recover
full-resolution outputs (more details regarding the network
architecture are provided in the supplementary material).
The network is trained by minimizing the weighted sum of
their angular losses. The loss for pixel ican be written as
Li=NiterX
t=0Œ≥Niter‚àítcos‚àí1(ngt
i¬∑nt
i) (5)
where 0< Œ≥ < 1puts a bigger emphasis on the final pre-
diction. We set Œ≥= 0.8following RAFT [50].
4.5. Dataset
The proposed model is designed to have high sample effi-
ciency. Firstly, we use a fully convolutional design to allow
translational weight sharing, which is known to improve
the sample efficiency [13]. Secondly, we estimate the ro-
tation matrices by decomposing them into angles andaxes.
9539
DatasetTrain Val
# scenes # imgs # scenes # imgs
Cleargrasp [46] 9 900 9 45
3D Ken Burns [39] 23 4600 23 230
Hypersim [44] 407 38744 407 2035
SAIL-VOS 3D [26] 170 16262 170 850
TartanAir [52] 16 3200 16 160
MVS-Synth [27] 120 11400 120 600
BlendedMVG [57] 495 44070 7 35
Taskonomy‚ãÜ[58] 375 37500 73 365
Replica‚ãÜ[49] 10 1000 4 20
Replica + GSO‚ãÜ[14, 49] 30 3000 12 60
Total 1655 160676 841 4400
Table 1. Dataset statistics. We created a small meta-dataset that
covers diverse scenes (‚ãÜ: downloaded from Omnidata [14]).
The angle between two normals is unaffected by the camera
pose. While the axis of rotation does change with the cam-
era pose, we estimate the 2D orientation of its projection
on the image plane, which can be as simple as edge detec-
tion (its 3D orientation is then recovered from the surface
normal). For such reasons, we do not need a large number
of images from the same scene. Rendering synthetic scenes
with diverse camera intrinsics is also unnecessary as the in-
trinsics are explicitly encoded in the input.
To this end, we created a small meta-dataset consisting
of images extracted from 10 RGB-D datasets (see Tab. 1 for
dataset statistics). Our dataset, compared to Omnidata [14],
has a similar number of scenes (1655 vs. 1905) but a signif-
icantly smaller number of images (160K vs. 12M).
5. Experiments
After providing the experimental setup (Sec. 5.1), we com-
pare the generalization capability of our method to that of
the state-of-the-art methods (Sec. 5.2) and perform an abla-
tion study to demonstrate the effectiveness of the proposed
usage of additional inductive biases (Sec. 5.3).
5.1. Experimental setup
Evaluation protocol. We measure the angular error for the
pixels with ground truth and report the mean and median
(lower the better). We also report the percentage of pixels
with an error below t‚àà[5.0‚ó¶,7.5‚ó¶,11.25‚ó¶,22.5‚ó¶,30.0‚ó¶]
(higher the better).
Data preprocessing. The training images are randomly re-
sized and cropped with a random aspect ratio to facilitate
the learning of ray direction-aware prediction. As many
of our training images are synthetic, we also add image
appearance augmentation ‚Äî e.g., Gaussian blur, Gaussian
noise, motion blur, and color ‚Äî to reduce the domain gap.
More details regarding data preprocessing are provided in
the supplementary material.Implementation details. Our model is implemented in Py-
Torch [40]. In all our experiments, the network and its
variants are trained on our meta-dataset (Sec. 4.5) for five
epochs. We use the AdamW optimizer [37] and sched-
ule the learning rate using 1cycle policy [48] with lrmax=
3.5√ó10‚àí4. The batch size is set to 4 and the gradients are
accumulated every 4 batches. The training approximately
takes 12 hours on a single NVIDIA 4090 GPU.
5.2. Comparison to the state-of-the-art
We select six datasets to compare our method‚Äôs gener-
alization capability against the state-of-the-art methods.
NYUv2 [47], ScanNet [11], and iBims-1 [30] all contain
images of real indoor scenes captured with cameras of stan-
dard intrinsics (480 √ó640 resolution and approximately
60‚ó¶field of view). Generalizing to such datasets is straight-
forward as the scenes and the cameras are similar to those
of commonly-used training datasets (e.g. taskonomy [58]).
While our method outperforms other methods on most met-
rics, the improvement is relatively small for this reason.
On the contrary, Sintel [6] and Virtual KITTI [20] con-
tain dynamic outdoor scenes and have less common fields
of view (e.g. 18‚ó¶to83‚ó¶for Sintel). The aspect ratios ‚Äî
436√ó1024 and375√ó1242 , respectively ‚Äî are also out of
distribution. For such datasets, our approach significantly
outperforms the other methods across all metrics. This is
mainly due to the explicit encoding of the camera intrinsics.
Lastly, we evaluate the methods on the validation set of
OASIS [8], which contains 10,000 in-the-wild images col-
lected from the internet. Two things should be noted for this
dataset. Firstly, the ground truth surface normals only exist
for small patches of the images and are annotated by hu-
mans . Plus, the ground truth is generally available only for
large flat regions. The accuracy metrics thus do not faith-
fully represent the performance of the methods. Secondly,
unlike most RGB-D datasets, the camera intrinsics are not
available for the input images. We thus approximated the
intrinsics by using the focal length recorded in the image
metadata, and assuming that the principal point is at the cen-
ter and that there is zero distortion. Despite such approxi-
mation, our method performs on par with the other methods.
For OASIS, we provide a qualitative comparison against
Omnidata v2 [29] in Fig. 6. While the quantitative accu-
racy was better for [29], the predictions made by our method
show a significantly higher level of detail.
One notable advantage of our method over ViT-based
models (e.g. [29]) lies in the simplicity and efficiency
of network training. For example, Omnidata v2 [29] was
trained for 2 weeks on four NVIDIA V100 GPUs. A set
of sophisticated 3D data augmentation functions [29] were
used to improve the generalization performance and cross-
task consistency [59] was enforced by utilizing other ground
truth labels. On the contrary, our model can be trained in
9540
MethodNYUv2 [47] ScanNet [11] iBims-1 [30]
mean med 5.0‚ó¶7.5‚ó¶11.25‚ó¶22.5‚ó¶30‚ó¶mean med 5.0‚ó¶7.5‚ó¶11.25‚ó¶22.5‚ó¶30‚ó¶mean med 5.0‚ó¶7.5‚ó¶11.25‚ó¶22.5‚ó¶30‚ó¶
OASIS (Hourglass) [8]29.2 23.4 7.5 14.0 23.8 48.4 60.7 32.8 28.5 3.9 8.0 15.4 38.5 52.6 32.6 24.6 7.6 13.8 23.5 46.6 57.4
EESNU [2] 16.2 8.532.8 46.0 58.6 77.2 83.5 11.8 5.7 45.2 59.7 71.3 85.5 89.9 20.0 8.4 32.0 46.1 58.5 73.4 78.2
Omnidata v1 [14] 23.1 12.9 21.6 33.4 45.8 66.3 73.6 22.9 12.3 21.5 34.5 47.4 66.1 73.2 19.0 7.5 37.2 50.0 62.1 76.1 80.1
Omnidata v2 [29] 17.2 9.7 25.3 40.2 55.5 76.5 83.0 16.2 8.5 29.1 44.9 60.2 79.5 84.7 18.2 7.0 38.9 52.2 63.9 77.4 81.1
Ours 16.4 8.432.8 46.3 59.6 77.7 83.5 16.2 8.329.8 45.9 61.0 78.7 84.4 17.1 6.143.6 56.5 67.4 79.0 82.3
MethodSintel [6] Virtual KITTI [20] OASIS [8]
mean med 5.0‚ó¶7.5‚ó¶11.25‚ó¶22.5‚ó¶30‚ó¶mean med 5.0‚ó¶7.5‚ó¶11.25‚ó¶22.5‚ó¶30‚ó¶mean med 5.0‚ó¶7.5‚ó¶11.25‚ó¶22.5‚ó¶30‚ó¶
OASIS (Hourglass) [8]43.1 39.5 1.4 3.1 7.0 24.1 35.7 41.8 34.6 2.7 10.1 23.6 40.8 46.7 23.9 18.2 - - 31.2 59.5 71.8
EESNU [2] 42.1 36.5 3.0 6.1 11.5 29.8 41.2 51.9 53.3 1.3 4.5 14.9 29.1 34.0 27.7 21.0 - - 24.0 53.2 66.6
Omnidata v1 [14] 41.5 35.7 3.0 5.8 11.4 30.4 42.0 41.2 34.0 21.5 29.3 34.7 43.0 47.6 24.9 18.0 - - 31.0 59.5 71.4
Omnidata v2 [29] 40.5 35.1 4.6 7.9 14.7 33.0 43.5 37.5 27.4 30.7 36.1 39.7 47.1 51.5 24.2 18.2 - - 27.7 61.0 74.2
Ours 34.9 28.1 8.914.1 21.5 41.5 52.7 28.9 9.943.7 47.5 51.3 59.2 63.2 24.4 18.8 10.5 17.5 28.8 58.5 72.0
Table 2. Quantitative evaluation of the generalization capabilities possessed by different methods. Best results are highlighted in
green. For evaluation on [6, 11, 20, 30, 47], we used the official code and model weights to generate the predictions. For methods that
assume a specific aspect ratio and resolution, the images were zero-padded and resized accordingly to match the requirements. The numbers
in red mean that the method was trained on the same dataset. We excluded such methods in ranking to ensure a fair comparison.
Figure 6. Comparison to Omnidata v2 [14] (DPT [42] model trained on 12 million images using 3D data augmentation [29] and
cross-task consistency [59]). Our method shows a stronger generalization capability for challenging in-the-wild objects. For textureless
regions (e.g. sky in the fourth column), our model resolves any inconsistency in the prediction and outputs a flat surface, while preserving
sharp boundaries around other objects.
just 12 hours on a single NVIDIA 4090 GPU, does not re-
quire geometry-aware 3D augmentations, and does not re-
quire any additional supervisory signal. Our model also has
40% fewer parameters compared to [29] (72M vs 123M).
5.3. Ablation study
We now run an ablation study to examine the effectiveness
of the proposed usage of two new inductive biases ‚Äî uti-
lizing dense per-pixel ray direction and modeling the pair-
wise relative rotation between nearby pixels. As can beseen from Tab. 3, encoding the ray direction helps improve
the accuracy, especially for out-of-distribution camera in-
trinsics (Sintel and Virtual KITTI). This is in line with our
observations in Tab. 2.
On the other hand, the improvement coming from the
rotation estimation is not big. As the accuracy metrics are
dominated by the pixels belonging to large planar surfaces,
they do not convey the improvements near surface bound-
aries. The metrics also do not penalize the inconsistencies
within piecewise smooth surfaces. Qualitative comparison
9541
MethodNYUv2 [47] ScanNet [11] iBims-1 [30] Sintel [6] Virtual KITTI [20]
mean 11.25‚ó¶22.5‚ó¶30‚ó¶mean 11.25‚ó¶22.5‚ó¶30‚ó¶mean 11.25‚ó¶22.5‚ó¶30‚ó¶mean 11.25‚ó¶22.5‚ó¶30‚ó¶mean 11.25‚ó¶22.5‚ó¶30‚ó¶
base 16.6 59.1 76.8 82.9 16.5 60.5 77.7 83.5 18.0 66.0 77.7 81.2 36.6 18.4 38.3 50.0 30.5 48.0 55.9 60.6
base+ray 16.6 59.2 77.0 82.9 16.4 61.2 77.9 83.6 17.6 66.4 78.1 81.4 36.0 19.8 38.8 50.4 29.4 50.6 58.4 62.4
base+ray+rot (Ours) 16.4 59.6 77.7 83.5 16.2 61.0 78.7 84.4 17.1 67.4 79.0 82.3 34.9 21.5 41.5 52.7 28.9 51.3 59.2 63.2
Ours w/o ray ReLU 16.6 58.8 77.2 83.2 16.1 60.6 78.5 84.2 17.2 67.4 78.8 82.1 34.9 20.8 40.8 52.7 29.6 50.1 58.0 62.0
Table 3. Ablation study. The baseline model (‚Äùbase‚Äù) applies convex upsampling straight after the initial normal prediction and does
not make use of the ray direction. Adding per-pixel ray direction as input (‚Äùray‚Äù) and updating the initial prediction via iterative rotation
estimation (‚Äùrot‚Äù) both lead to an overall improvement in the metrics. The benefit of using ray direction is clearer for datasets with out-of-
distribution camera intrinsics (Sintel and Virtual KITTI). Removing the proposed ray ReLU activation leads to performance degradation.
Figure 7. Ablation study. Here we compare the predictions made
by two models with and without the iterative rotation estimation
(both are using per-pixel ray information). When the model is
trained to directly estimate the normals, the prediction is often in-
consistent within smooth surfaces, leading to bleeding artifacts.
The proposed refinement via rotation estimation leads to piece-
wise smooth surfaces that are crisp near surface boundaries.
in Fig. 7 clearly shows that the proposed refinement via ro-
tation estimation improves the piecewise consistency and
sharpness of the prediction near surface boundaries.
Since the predictions are refined by rotating the normals
of the neighboring pixels, and since the rotation axes are
also inferred from those normals, it is important to correct
theimpossible normals that are facing away from the cam-
era. Tab. 3 shows that removing the proposed ray ReLU
activation results in worse performance.
6. Conclusion
In this paper, we discussed the inductive biases needed for
surface normal estimation and introduced how per-pixel ray
direction and the relative rotational relationship between
neighboring pixels can be encoded in the output. Per-pixel
ray direction allows camera intrinsics-aware inference and
thus improves the generalization ability, especially when
tested on images taken with out-of-distribution cameras.Explicit modeling of inter-pixel constraints ‚Äî implemented
in the form of rotation estimation ‚Äî leads to piece-wise
smooth predictions that are crisp near surface boundaries.
Compared to a recent transformer-based state-of-the-art
method, our method shows stronger generalization capa-
bility and a significantly higher level of detail in the pre-
diction, despite being trained on an orders of magnitude
smaller dataset. Thanks to its fully convolutional architec-
ture, our model can be applied to images of arbitrary reso-
lution and aspect ratio, without the need for image resizing
or positional encoding inter/extrapolation. We believe that
the domain- and camera-agnostic generalization capability
of our method makes it a strong front-end perception that
can benefit many downstream 3D computer vision tasks.
7. Limitation and future work
Surface normal estimation is an inherently ambiguous task
when the camera intrinsics are not known. This was why we
proposed to encode the intrinsics in the form of dense per-
pixel ray direction. While this helped us push the limits of
single-image surface normal estimation, it also means that
the model requires prior knowledge about the camera.
Note, however, that most RGB-D datasets already pro-
vide pre-calibrated camera parameters, and that monocular
cameras can be calibrated easily using patterns with known
relative coordinates. For in-the-wild images, we demon-
strated in Sec. 5.2 that the intrinsics can be approximated
using the image metadata. If no information is available, we
can attempt to estimate the camera intrinsics from a single
image, e.g., by using vanishing points with known relative
angles [7]. As our model is designed to learn the relative
angle between surfaces, it can in turn be used for camera
calibration. This will be explored in our future work.
8. Acknowledgement
Research presented in this paper was supported by Dyson
Technology Ltd. The authors would like to thank Shikun
Liu, Eric Dexheimer, Callum Rhodes, Aalok Patwardhan,
Riku Murai, Hidenobu Matsuki, and other members of
the Dyson Robotics Lab for feedback and discussions.
9542
References
[1] Mart ¬¥ƒ±n Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian
Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Man ¬¥e, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-
nanda Vi ¬¥egas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorflow.org. 3
[2] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Es-
timating and exploiting the aleatoric uncertainty in surface
normal estimation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision (ICCV) , 2021. 1,
2, 7
[3] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Iron-
depth: Iterative refinement of single-view depth using sur-
face normal and its uncertainty. In Proceedings of the British
Machine Vision Conference (BMVC) , 2022. 1, 5
[4] Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr
revisited: 2d-3d alignment via surface normal prediction. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2016. 2
[5] Oliver Boyne, Gwangbin Bae, James Charles, and Roberto
Cipolla. Found: Foot optimization with uncertain normals
for surface deformation using synthetic data. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV) , 2024. 1
[6] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for optical
flow evaluation. In Proceedings of the European Conference
on Computer Vision (ECCV) , 2012. 6, 7, 8
[7] Bruno Caprile and Vincent Torre. Using vanishing points
for camera calibration. International Journal of Computer
Vision (IJCV) , 4(2):127‚Äì139, 1990. 8
[8] Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Ko-
jima, Max Hamilton, and Jia Deng. Oasis: A large-scale
dataset for single image 3d in the wild. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 6, 7
[9] Kyunghyun Cho, Bart Van Merri ¬®enboer, Dzmitry Bahdanau,
and Yoshua Bengio. On the properties of neural machine
translation: Encoder-decoder approaches. In Eighth Work-
shop on Syntax, Semantics and Structure in Statistical Trans-
lation (SSST-8) , 2014. 5
[10] James Coughlan and Alan L Yuille. The manhattan world
assumption: Regularities in scene statistics which enable
bayesian inference. In Advances in Neural Information Pro-
cessing Systems (NeurIPS) , 2000. 2
[11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nie√üner. Scannet:Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2017. 6, 7, 8
[12] Tien Do, Khiem Vuong, Stergios I Roumeliotis, and
Hyun Soo Park. Surface normal estimation of tilted images
via spatial rectifier. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), Part IV , 2020. 2
[13] St ¬¥ephane d‚ÄôAscoli, Hugo Touvron, Matthew L Leavitt, Ari S
Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving
vision transformers with soft convolutional inductive biases.
InInternational Conference on Machine Learning (ICML) ,
2021. 5
[14] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir
Zamir. Omnidata: A scalable pipeline for making multi-task
mid-level vision datasets from 3d scans. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , 2021. 1, 2, 6, 7
[15] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale convo-
lutional architecture. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , 2015.
2
[16] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In Advances in Neural Information Processing Sys-
tems (NeurIPS) , 2014. 2
[17] Jose M Facil, Benjamin Ummenhofer, Huizhong Zhou,
Luis Montesano, Thomas Brox, and Javier Civera. Cam-
convs: Camera-aware multi-scale convolutions for single-
view depth. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , 2019.
4
[18] David F Fouhey, Abhinav Gupta, and Martial Hebert. Data-
driven 3d primitives for single image understanding. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2013. 2
[19] David Ford Fouhey, Abhinav Gupta, and Martial Hebert. Un-
folding an indoor origami world. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV) , 2014. 2
[20] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora
Vig. Virtual worlds as proxy for multi-object tracking anal-
ysis. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2016. 6, 7,
8
[21] Richard Hartley and Andrew Zisserman. Multiple View Ge-
ometry in Computer Vision . Cambridge University Press,
2003. 2
[22] Derek Hoiem, Alexei A Efros, and Martial Hebert. Auto-
matic photo pop-up. In ACM SIGGRAPH , 2005. 2
[23] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recov-
ering surface layout from an image. International Journal of
Computer Vision (IJCV) , 75:151‚Äì172, 2007. 2
[24] Hugues Hoppe, Tony DeRose, Tom Duchamp, Mark Hal-
stead, Hubert Jin, John McDonald, Jean Schweitzer, and
Werner Stuetzle. Piecewise smooth surface reconstruction.
InProceedings of the 21st annual conference on Computer
graphics and interactive techniques , pages 295‚Äì302, 1994. 2
9543
[25] Youichi Horry, Ken-Ichi Anjyo, and Kiyoshi Arai. Tour into
the picture: using a spidery mesh interface to make anima-
tion from a single image. In Proceedings of the 24th an-
nual conference on Computer graphics and interactive tech-
niques , pages 225‚Äì232, 1997. 2
[26] Yuan-Ting Hu, Jiahong Wang, Raymond A Yeh, and Alexan-
der G Schwing. Sail-vos 3d: A synthetic dataset and base-
lines for object detection and 3d mesh reconstruction from
video data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2021. 6
[27] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra
Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view
stereopsis. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2018. 6
[28] Katsushi Ikeuchi and Berthold KP Horn. Numerical shape
from shading and occluding boundaries. Artificial Intelli-
gence , 17(1-3):141‚Äì184, 1981. 2
[29] O Àòguzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir
Zamir. 3d common corruptions and data augmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2022. 1, 2, 3, 6, 7
[30] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and
Marco Korner. Evaluation of cnn-based single-image depth
estimation methods. In Proceedings of the European Con-
ference on Computer Vision Workshops , 2018. 6, 7, 8
[31] Jana Ko Àáseck¬¥a and Wei Zhang. Extraction, matching, and
pose recovery based on dominant rectangular structures.
Computer Vision and Image Understanding , 100(3):274‚Äì
293, 2005. 2
[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems (NeurIPS) , 2012. 2
[33] Florian Langer, Gwangbin Bae, Ignas Budvytis, and Roberto
Cipolla. Sparc: Sparse render-and-compare for cad model
alignment in a single rgb image. In Proceedings of the British
Machine Vision Conference (BMVC) , 2022. 1
[34] Yann LeCun, L ¬¥eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998.
1
[35] David C Lee, Martial Hebert, and Takeo Kanade. Geometric
reasoning for single image structure recovery. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , 2009. 2
[36] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei
Xiao, and Anima Anandkumar. Prismer: A vision-language
model with multi-task experts. Transactions on Machine
Learning Research , 2024. 1
[37] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations (ICLR) , 2019. 6
[38] David Marr. Analysis of occluding contour. Proceedings of
the Royal Society of London. Series B. Biological Sciences ,
197(1129):441‚Äì475, 1977. 2
[39] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d
ken burns effect from a single image. ACM Transactions on
Graphics (TOG) , 38(6):1‚Äì15, 2019. 6[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:
An imperative style, high-performance deep learning li-
brary. In Advances in Neural Information Processing Sys-
tems (NeurIPS) , 2019. 6
[41] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun,
and Jiaya Jia. Geonet: Geometric neural network for joint
depth and surface normal estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2018. 1
[42] Ren ¬¥e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , 2021. 2, 7
[43] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-
lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia
Gkioxari. Accelerating 3d deep learning with pytorch3d.
arXiv preprint arXiv:2007.08501 , 2020. 3
[44] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,
and Joshua M. Susskind. Hypersim: A photorealistic syn-
thetic dataset for holistic indoor scene understanding. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2021. 6
[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InInternational Conference on Medical Image Computing
and Computer Assisted Intervention (MICCAI) , 2015. 2
[46] Shreeyak Sajjan, Matthew Moore, Mike Pan, Ganesh Na-
garaja, Johnny Lee, Andy Zeng, and Shuran Song. Clear
grasp: 3d shape estimation of transparent objects for manip-
ulation. In IEEE International Conference on Robotics and
Automation (ICRA) , 2020. 6
[47] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In Proceedings of the European Conference
on Computer Vision (ECCV) , 2012. 6, 7, 8
[48] Leslie N Smith and Nicholay Topin. Super-convergence:
Very fast training of residual networks using large learning
rates. arXiv preprint arXiv:1708.07120 , 2018. 6
[49] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik
Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl
Ren, Shobhit Verma, et al. The replica dataset: A digital
replica of indoor spaces. arXiv preprint arXiv:1906.05797 ,
2019. 6
[50] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In Proceedings of the European
Conference on Computer Vision (ECCV) , 2020. 5
[51] Rui Wang, David Geraghty, Kevin Matzen, Richard Szeliski,
and Jan-Michael Frahm. Vplnet: Deep single view normal
estimation with vanishing points and lines. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 2
[52] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,
Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Se-
bastian Scherer. Tartanair: A dataset to push the limits of
9544
visual slam. In IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) , 2020. 6
[53] Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design-
ing deep networks for surface normal estimation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , 2015. 2
[54] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J
Black. Icon: Implicit clothed humans obtained from nor-
mals. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2022. 1
[55] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and
Michael J Black. Econ: Explicit clothed humans optimized
via normal integration. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2023. 1
[56] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and
Elisa Ricci. Transformer-based attention networks for
continuous pixel-wise prediction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , 2021. 1, 3
[57] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,
Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-
scale dataset for generalized multi-view stereo networks. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2020. 6
[58] Amir R Zamir, Alexander Sax, William Shen, Leonidas J
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentangling task transfer learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2018. 6
[59] Amir R Zamir, Alexander Sax, Nikhil Cheerla, Rohan Suri,
Zhangjie Cao, Jitendra Malik, and Leonidas J Guibas. Ro-
bust learning through cross-task consistency. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2020. 2, 6, 7
[60] Guangyao Zhai, Dianye Huang, Shun-Cheng Wu, HyunJun
Jung, Yan Di, Fabian Manhardt, Federico Tombari, Nassir
Navab, and Benjamin Busam. Monograspnet: 6-dof grasping
with a single rgb image. In IEEE International Conference
on Robotics and Automation (ICRA) , 2023. 1
[61] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera,
Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised learn-
ing of monocular depth estimation and visual odometry with
deep feature reconstruction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2018. 2
[62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2023. 1
[63] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui,
Martin R Oswald, Andreas Geiger, and Marc Pollefeys.
Nicer-slam: Neural implicit scene encoding for rgb slam. In
International Conference on 3D Vision (3DV) , 2024. 1
9545
