FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning
Zihui Wangâˆ—
wangziwei@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaZheng Wangâˆ—
zwang@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaLingjuan Lyu
lingjuan.lv@sony.com
Sony AI
Zurich, Switzerland
Zhaopeng Peng
pengzhaopeng@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaZhicheng Yang
zcyang@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaChenglu Wen
clwen@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, China
Rongshan Yu
rsyu@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaCheng Wang
cwang@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaXiaoliang Fanâ€ 
fanxiaoliang@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, China
ABSTRACT
Collaborative fairness stands as an essential element in federated
learning to encourage client participation by equitably distribut-
ing rewards based on individual contributions. Existing methods
primarily focus on adjusting gradient allocations among clients to
achieve collaborative fairness. However, they frequently overlook
crucial factors such as maintaining consistency across local mod-
els and catering to the diverse requirements of high-contributing
clients. This oversight inevitably decreases both fairness and model
accuracy in practice. To address these issues, we propose FedSAC,
a novel Federated learning framework with dynamic Submodel
Allocation for Collaborative fairness, backed by a theoretical con-
vergence guarantee. First, we present the concept of "bounded collab-
orative fairness (BCF) ", which ensures fairness by tailoring rewards
to individual clients based on their contributions. Second, to im-
plement the BCF, we design a submodel allocation module with a
theoretical guarantee of fairness. This module incentivizes high-
contributing clients with high-performance submodels containing
a diverse range of crucial neurons, thereby preserving consistency
âˆ—Both authors contributed equally to this research
â€ Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671748across local models. Third, we further develop a dynamic aggre-
gation module to adaptively aggregate submodels, ensuring the
equitable treatment of low-frequency neurons and consequently en-
hancing overall model accuracy. Extensive experiments conducted
on three public benchmarks demonstrate that FedSAC outperforms
all baseline methods in both fairness and model accuracy. We see
this work as a significant step towards incentivizing broader client
participation in federated learning. The source code is available at
https://github.com/wangzihuixmu/FedSAC.
CCS CONCEPTS
â€¢Security and privacy; â€¢Information systems â†’Information
systems applications;
KEYWORDS
federated Learning, collaborative fairness, privacy
ACM Reference Format:
Zihui Wang, Zheng Wang, Lingjuan Lyu, Zhaopeng Peng, Zhicheng Yang,
Chenglu Wen, Rongshan Yu, Cheng Wang, and Xiaoliang Fan. 2024. Fed-
SAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated
Learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671748
1 INTRODUCTION
Federated Learning (FL) empowers multiple data owners to collec-
tively train a global model while preserving the privacy of their
individual training data [ 19,34,35]. Early FL frameworks [ 3,12,27]
usually distributed the same model to all clients without consider-
ing their distinct contributions to the model performance, resulting
3299
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zihui Wang et al.
in unfairness to high-contributing clients. Collaborative fairness
(CF) [ 18] stands as an essential element in federated learning to mo-
tivate client engagement by ensuring impartial reward distribution
tied directly to individual contributions.
More recently, several gradient-based methods were proposed
to enhance CF [ 18,31â€“33] (i.e., rewarding clients with correspond-
ing model quality according to their contributions) in FL. They dis-
tribute a larger quantity of gradients to higher-contributing clients
than the lower ones as rewards and quantify the degree of fairness
by Pearson Correlation Coefficient ğœŒ. However, for achieving CF,
existing gradient-based methods have two major limitations. On
one hand, the conventional definition of CF doesnâ€™t adequately
distinguish in reward distribution among clients, resulting in a
persistent unfairness for high-contributing clients. In Figure 1 (a),
suppose the contributions of three clients are ğ‘=[1,9,11], and their
rewards are ğœƒâˆ—=[99,99.2,99.3]corresponding. Through the defi-
nition of CF by CGSV [ 33], the fairness is calculated as ğ›¾=98.97, but
there exists an underlying unfairness towards ğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 2andğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 3
becauseğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 1with an inferior contribution is over-rewarded. On
the other hand, conventional gradient-based methods [ 18,31â€“33]
are ineffective because the inconsistency of local models updated by
variable gradients might lead to significant degradation of overall
model performance. In Figure 1 (a), the local models of clients in
roundğ‘¡(ğœƒ1,ğ‘¡,ğœƒ2,ğ‘¡, andğœƒ3,ğ‘¡) exhibit notable differences (i.e., the larger
the circle, the higher the accuracy). Consequently, the gradients
uploaded by individual clients may not be the optimal for others,
creating a misalignment between obtained rewards Î”ğœƒğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘
ğ‘–,ğ‘¡(i.e.,
the rewards ultimately obtained by the clients) and expected re-
wards Î”ğœƒğ‘’ğ‘¥ğ‘ğ‘’ğ‘ğ‘¡
ğ‘–,ğ‘¡(i.e., the rewards that the clients ultimately expected)
for each client.
To address the aforementioned challenges, we propose a novel
Federated learning framework with dynamic Submodel Allocation
for bounded Collaborative fairness (FedSAC ), supported by a the-
ory of convergence while achieving competitive model accuracy.
First, our approach introduces the concept of "bounded collabora-
tive fairness (BCF) (refer to ğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› 1)", which ensures fairness
by integrating a differentiated range of rewards allocated to each
client. Second, the submodel allocation module with a theoretical
fairness guarantee, is designed to assign relevant submodels (i.e.,
results of the aggregated model dropout) to individual clients based
on their contributions. Specifically, these submodels encompass
a diverse array of essential neurons for effective training. Third,
the dynamic aggregation module is implemented as a weight re-
alignment mechanism by treating low-frequency neurons equally,
which further improves the overall performance of the global model.
Extensive experiments on three public benchmarks show that the
proposed FedSAC outperforms all baseline methods in terms of
collaborative fairness and model accuracy.
The contributions of this work are summarized:
â€¢We propose FedSAC, a novel federated learning framework
with a convergence guarantee, introducing a new concept
ofbounded collaborative fairness (BCF). To the best of our
knowledge, this is the first approach that allocates submodels
equitably for collaborative fairness in FL.
â€¢We implement the concept of BCF through two modules.
First, submodel allocation module prioritizes high-contributing
Client1Client2 Client3
â‘ âˆ†ğœƒ2,ğ‘¡ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›:localmodelupdate â‘¡âˆ†ğœƒ2,ğ‘¡ğ‘–ğ‘‘ğ‘’ğ‘ğ‘™: ideal model
â‘¢âˆ†ğœƒ2,ğ‘¡ğ‘’ğ‘¥ğ‘ğ‘’ğ‘ğ‘¡ :excepted rewards                         â‘£âˆ†ğœƒ2,ğ‘¡ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ : actual rewards    
Obtained model                  Contribution           Expected model
(a) Gradient -based unfair method         (b) Submodel -based fair method ( FedSAC )ğœƒ1,ğ‘¡ğœƒ2,ğ‘¡ğœƒ3,ğ‘¡ğœƒ1âˆ—ğœƒ2âˆ—ğœƒ3âˆ—
â‘ 
Client1Client2 Client3ğœƒ1,ğ‘¡ğœƒ2,ğ‘¡ğœƒ3,ğ‘¡ğœƒ1âˆ—ğœƒ2âˆ—ğœƒ3âˆ—
â‘ â‘¡ â‘¡â‘¢ â‘¢â‘£c1c2c3 c1c2c3Training stepFigure 1: Problem illustration of collaborative fairness in
FL. (a) Conventional gradients-based methods will result in
poor fairness and model accuracy. For example, it is unfair
that obtained models of ğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 2andğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 1are equivalent
neglecting the inferior contribution ( ğ‘1) ofğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 1. Plus, the
inconsistency in local models results in that obtained models
ofğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 2andğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 3are worse than expected ( ğœƒâˆ—
ğ‘–). (b) Our pro-
posed FedSAC allocates sufficient submodels to each client
by ensuring a comprehensive balance between fairness and
model accuracy. For example, FedSAC ensures that obtained
models of all clients (i.e., ğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 1,ğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 2andğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 3) are in
accordance with their contributions respectively. In addition,
FedSAC guarantees the alignment of â‘¢andâ‘£ofğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 2dur-
ing the training process, thereby enabling all three clients to
obtain their expected models ( ğœƒâˆ—
ğ‘–).
clients by rewarding them with high-performance submodels
under a theoretical guarantee. Second, dynamic aggregation
module merges submodels by paying equitable attention to
low-frequency neurons to be aggregated.
â€¢We conduct extensive experiments on three benchmarks
with various settings and demonstrate that FedSAC ourper-
forms all baselines in both fairness and model accuracy.
2 RELATED WORKS
Recent research has shown that distributing different rewards based
on clientsâ€™ contributions can significantly impact the FL systems [ 17,
23,36]. The incentive mechanisms can motivate clients to contribute
high-quality data and promote collaboration [ 33]. We outline three
types of rewards that can be adopted to achieve CF in FL.
Money-based reward. Several studies focus on the mechanism
that rewards clients monetary based on their contributions. [ 43]
proposes a reputation-based and reverse auction theory mechanism
to reward clients with a limited budget. [ 37] shows a scheme that
dynamically allocates budgets to clients in a context-aware man-
ner by jointly maximizing the collective utility. While monetary
rewards can be a natural and effective way to incentivize clients
in FL, there exist challenges in maintaining a balance between the
value of model quality and money [1, 42].
Data-based reward. Early studies have explored the fairness
of rewarding different data sizes based on their contributions. [ 25]
evaluates clientsâ€™ contributions by aggregating the training data,
3300FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
and reward them with the corresponding models. [ 28] trains a
generative model through the local data of all clients and provides
more synthetic data to those datasets closely aligned with the real
data distribution. However, most of existing data-based reward
methods rely on the centralized aggregation of all the data during
training, making them difficult to be applied in the FL scenarios.
Gradient-based reward. Recent collaborative fairness (CF)
works aim to reward high-contributing clients with optimal mod-
els. CFFL [ 18] allocates different gradient numbers based on lo-
cal accuracy in the validation set and data sizes. CGSV [ 33] re-
wards more gradients to clients whose local gradients are more
similar to the global gradients. FedAVE [ 31] assigns more gradi-
ents to clients whose data distribution information is more sim-
ilar to the ideal dataset. However, existing rewards systems lack
sufficient differentiation, resulting in an ongoing unfairness for
high-contributing clients, which might degrade the fairness and
model accuracy. Different from these methods, we propose a novel
framework to achieve BCF (refer to ğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› 1) by allocating a
differential range of rewards to clients.
3 PRELIMINARY
FL system consists of a server and multiple clients, aiming to mini-
mize the weighted average of all clientsâ€™ local objectives by optimiz-
ing a global model [ 11,19]. In this setup, the goal of FL framework
is defined as:
ğ‘šğ‘–ğ‘›
ğœƒğ¹(ğœƒ):=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–ğ¹ğ‘–(ğœƒ), (1)
whereğœƒdenotes the global model, ğ‘represents the number of
clients, and ğ‘ğ‘–=ğ‘›ğ‘–
ğ‘›,ğ‘›=Ãğ‘
ğ‘˜=1ğ‘›ğ‘˜.ğ¹ğ‘–(ğœƒ)is the loss on client ğ‘–
using model parameters ğœƒ, i.e.,ğ¹ğ‘–(ğœƒ)=1
ğ‘›ğ‘–Ã
ğœ‰ğ‘–âˆ¼ğ·ğ‘–ğ‘™(ğœƒ,ğœ‰ğ‘–), whereğ·ğ‘–
represents the local dataset of client ğ‘–, andğ‘›ğ‘–denotes the data size
ofğ·ğ‘–. To achieve this goal as effectively as possible, FedAvg [ 19]
samples a subset ğ‘†ğ‘¡ofğ‘–clients uniformly, 0<ğ‘–â‰¤ğ‘, to train the
global model and aggregate the locally trained models by utilizing
the data size ratio ğ‘ğ‘–as the weight of client ğ‘–. Although FedAvg is
proven to be effective in minimizing the objective successfully, it
may be unfair to high-quality clients since the system distributes the
same rewards to all clients regardless of their contributions [ 18,33].
3.1 Problem Formulation
The standard FL framework allocates the same model to all clients
regardless of their contributions [ 7,19], dampening the motivation
of high-quality clients to join FL [ 25,30]. Collaborative fairness
in FL aims to reward high-contributing clients with high-quality
models. The existing works [ 18,31,33] assess the fairness with the
Pearson Correlation Coefficient, ğœŒ(ğ‘;ğœƒâˆ—), whereğ‘andğœƒâˆ—represent
the contributions and rewards of clients, respectively. However,
the definition simply considers the relationship between the con-
tributions and rewards of clients, which may lead to insufficient
incentives for high-contributing clients. For example in Figure 1
(a), suppose the contributions of ğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 1,ğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 2, andğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 3are
ğ‘ğ‘–=[1,9,11]and their rewards are ğœƒâˆ—
ğ‘–=[99,99.2,99.3]correspond-
ingly. Through the definition of CF, the fairness is calculated as
ğ›¾=98.97, but there exists an underlying unfairness towards ğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 2andğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 3becauseğ¶ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ 1with an inferior contribution is over-
rewarded.
To address this issue, we propose Bounded Collaborative Fairness
(BCF) to tackle the issue of insufficient incentives for the high-
contributing client. BCF could ensure ğ‘1<ğœƒâˆ—
1<(ğ‘1+ğœƒâˆ—
3)
2and then
quantitative fairness with ğœŒ(ğ‘;ğœƒâˆ—). The rationale behind the formula
inğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› 1 aims to amplify significant distinctions in rewards.
The formulaâ€™s left side ensures that clientsâ€™ rewards exceed their
contributions, while the right side prevents excessive rewards for
clients with low contributions.
Definition 1 (Bounded Collaborative Fairness). The
contributions ( ğ‘) and the rewards ( ğœƒâˆ—) of clients are calculated by the
performance of their standalone models (train without collaboration)
and the final models obtained after collaboration, respectively. Based
on clientâ€™s obtained rewards ğ‘ğ‘–<ğœƒâˆ—
ğ‘–<(ğ‘ğ‘–+ğ‘šğ‘ğ‘¥(ğœƒâˆ—))
2, the quantitative
fairness can be computed by ğ›¾:= 100Ã—ğœŒ(ğ‘,ğœƒâˆ—) whereğœŒ() is the
Pearson Correlation Coefficient. The larger ğ›¾, the better the fairness
of the framework.
4 THE PROPOSED FEDSAC
In this section, we will introduce the details of proposed FedSAC, a
method that ensures both BCF and consistency in local models for
each client. The architecture of FedSAC is shown in Figure 2. The
pseudo codes for FedSAC are provided in Algorithm 1. First, we
introduce the submodel allocation module in Section 4.1. Second,
we present the dynamic aggregation module in Section 4.2. Third,
we proposed the fairness guarantee theory in Section 4.3 to prove
that this submodel allocation strategy can achieve collaborative
fairness. Fourth, we conducted a convergence analysis on FedSAC
and demonstrated its convergence in Section 4.5. In addition, we
analyzed the time complexity and communication costs of FedSAC
in Section 4.5. Finally, we discussed limitations in Section 4.6.
4.1 Submodel Allocation Module
A naive approach achieving bounded collaborative fairness involves
allocating distinct submodels to each client based on their respec-
tive contributions [ 5]. Unlike previous works such as [ 6,24], there
are two primary motivations behind achieving BCF through sub-
model allocation. First, submodels with appropriate pruning may
not match the performance of the global model, enabling clients to
receive diverse submodels according to their contributions. Second,
despite being subsets of the global model, these submodels exhibit
strong mutual validity, meaning that the submodels uploaded by
one client are effective for others, facilitating the training of the
global model. However, it is still challenging to achieve BCF through
submodel-based methods. For one thing, it is crucial to ensure that
the majority of neurons are adequately trained to guarantee the
optimal performance of the global model. For another, the perfor-
mance of allocated submodels should align with their respective
contributions.
To address the aforementioned two challenges, we design a
two-step approach for submodel allocation module. First, we evalu-
ate the importance of each neuron within the model to determine
their contributions respectively (neuron importance evaluation in
Section 4.1.1). Second, we construct submodels for each client with
3301KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zihui Wang et al.
â‘ Neuron Importance EvaluationRound tServer
â‘¡Stage 1: Submodel Allocation Stage 2: Dynamic Aggregation
â‘¢Aggregate
â‘£Round t+1
Valid neuron               Invalid neuron                              Neuron importance                             Number o f aggregations 1     5      10 1     5      10ğ‘«ğ‘¨
ğ‘«ğ‘©
5.9 7.8
2.3
3.25.9
1.2 3.5 1.3
8.4 5.3 6.7 6.1
ğŸ.ğŸ‘+ğŸ–.ğŸ’
ğŸğŸ“.ğŸ—+ğŸ“.ğŸ‘
ğŸğŸ‘.ğŸ“
ğŸğŸ.ğŸ‘
ğŸ
â€¦â‘ 
â‘¡Submodel Constructionğ‘ªğ’ğ’Šğ’†ğ’ğ’• ğ‘¨
ğ‘ªğ’ğ’Šğ’†ğ’ğ’• ğ‘©Validation
Validation
Figure 2: The overall framework of FedSAC that achieves bounded collaborative fairness by maintaining consistency across
local models. FedSAC consists of two module: 1) submodel allocation module conducts neuron importance evaluation and
submodel construction to reward high-contributing clients with high-performance submodels, thus ensuring consistency in
local models; 2) dynamic aggregation module treats those low-frequency neurons equally, which further refines the performance
of the global model.
varying performances based on their contributions, ensuring a di-
verse array of important neurons is included within each submodel
(submodel construction in Section 4.1.2).
4.1.1 Neuron Importance Evaluation. Each neuron within the
model holds a unique contribution [ 16,20,39]. Our intuition is that
the constructed submodels can yield varied performances. Inspired
by Taylor-FO [ 20], we calculate the neuron importance in the model
by measuring the change in loss upon their removal. For instance,
a greater increase in loss indicates a more significant contribution
by the removed neuron to the model. In Figure 2, neurons depicted
in a redder shade represent a higher contribution to the model.
Essentially, the training objective is to minimize the cross-entropy
lossğ¿ğ‘ğ‘’:
min
ğœƒğ‘âˆ‘ï¸
ğ‘–=1ğ¿ğ‘ğ‘’(ğ‘¥ğ‘–,ğœƒ), (2)
whereğ‘¥ğ‘–denotes the sample, ğœƒrepresents the model, and ğ¿ğ‘ğ‘’(ğ‘¥ğ‘–,ğœƒ)
is the loss function of the classification tasks.
The neurons in the model have a multitude of model parameters,
each of which contributes to the overall performance of the model.
The importance of a neuron ğ¼ğ‘›ğ‘–of the model can be calculated
through the loss increased by removing it:
ğ¼ğ‘›ğ‘–=ğ¿ğ‘ğ‘’(ğ‘‰,ğœƒ|ğœƒğ‘§ğ‘’ğ‘Ÿğ‘œ
ğ‘›ğ‘–=0)âˆ’ğ¿ğ‘ğ‘’(ğ‘‰,ğœƒ), (3)
whereğ‘‰denotes the validation set, which is constructed by evenly
selecting 10% of the data from the original training samples [ 18],
ğœƒğ‘§ğ‘’ğ‘Ÿğ‘œğ‘›ğ‘–represents that the parameters of the ğ‘–-th neuron in the model
are all set to 0.
To simplify the construction of submodels, we normalize the
sum of neuron scores, which represent their importance in themodel presented by percentage:
ğ¼ğ‘›ğ‘–=ğ¼ğ‘›ğ‘–Ã
ğ‘›ğ‘–âˆˆğ‘†ğ¼ğ‘›ğ‘–âˆ—100, (4)
whereğ‘†represents all neurons of the model. To reduce the training
time of the framework, we measure the importance of neurons in
the model by Eq. (3) and Eq. (4) every 10 epochs. All these operations
allow us to efficiently assess the importance of each neuron while
limiting excessive computation demands.
4.1.2 Submodel Construction. In pursuit of fairness, we employ
a dynamic allocation system for submodels with varying perfor-
mances, leveraging clientsâ€™ reputations derived from their contribu-
tions. Our approach incorporates a pruning mechanism tailored to
clientsâ€™ contributions, simplifying the extraction of submodels with
different performance levels from the global model. In this scheme,
the clientğ‘–â€™s reputation ğ‘Ÿğ‘–is expressed as:
ğ‘Ÿğ‘–=ğ‘’ğ‘ğ‘–âˆ—ğ›½, (5)
ğ‘Ÿğ‘–=ğ‘Ÿğ‘–
ğ‘šğ‘ğ‘¥(ğ‘Ÿ)âˆ—100, (6)
whereğ‘ğ‘–represents client ğ‘–â€™s contribution, ğ›½is a hyper-parameter.
The reputations of clients are directly proportional to their contri-
butions. The design rationale for Eq. (5) and Eq. (6) is to calculate
the clientsâ€™ reputations ( ğ‘Ÿ), which facilitates the allocation of their
submodels fairly. More specifically, our pruning method begins
with the most important neuron, ensuring that submodels for low-
contribution clients possess a higher parameter count, which is
beneficial for training the global model. These actions serve a dual
purpose: promoting collaborative fairness while maximizing the
overall performance of the global model. Submodel ğœƒğ‘–is constructed
3302FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
by neurons with different reputations:
ğœƒğ‘–=ğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦(ğ‘Ÿğ‘–,âˆ‘ï¸
ğ‘›ğ‘–âˆˆğ‘†ğ¼ğ‘›ğ‘–), (7)
whereğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦(ğ‘Ÿğ‘–,Ã
ğ‘›ğ‘–âˆˆğ‘†ğ¼ğ‘›ğ‘–)represents the submodel ğœƒğ‘–when
ğ‘Ÿğ‘–=Ã
ğ‘›ğ‘–âˆˆğ‘†ğ¼ğ‘›ğ‘–,ğ‘Ÿğ‘–denotes client ğ‘–â€™s reputation,Ã
ğ‘›ğ‘–âˆˆğ‘†ğ¼ğ‘›ğ‘–denotes
the set importance for different neurons. ğ‘†represents the positions
of all neurons in the model, arranged in ascending order from the
least to the most important. This design choice aims to maximize the
inclusion of neurons in each submodel, thereby enhancing the per-
formance of the corresponding local model updates. Subsequently,
this quantity will be utilized in Eq. (8) to generate the submodelâ€™s
mask.
4.2 Dynamic Aggregation Module
Next, the server aggregates the locally trained submodels and allo-
cates distinct submodels to clients in the subsequent round. Recent
submodel-based methods [ 6,24] have aimed to allocate varied sub-
models containing numerous neurons to clients. However, these
approaches might pose a potential risk of compromising overall
model performance when integrating low-frequency neurons into
the global model. Consequently, employing a direct aggregation
method such as FedAvg [19] for all neurons becomes inequitable.
Instead of simply averaging the uploaded submodels, our objec-
tive is to optimize the utilization of all neurons within the model.
With the sizes of submodels varying across clients, it becomes es-
sential to treat the contribution of each neuron individually during
aggregation. To ensure fair treatment of low-frequency neurons,
we integrate the frequency of submodel parameter aggregations as
weights to dynamically aggregating local models:
ğ‘šğ‘ğ‘ ğ‘˜ğ‘¡
ğ‘–=ğ‘šğ‘ğ‘ ğ‘˜(ğœƒğ‘¡
ğ‘–,ğœƒğ‘¡âˆ’1
ğ‘”), (8)
ğœƒğ‘¡+1
ğ‘”=Ã
ğ‘–âˆˆğ‘ğœƒğ‘¡
ğ‘–Ã
ğ‘–âˆˆğ‘ğ‘šğ‘ğ‘ ğ‘˜ğ‘¡
ğ‘–, (9)
whereğ‘šğ‘ğ‘ ğ‘˜(ğœƒğ‘–,ğœƒğ‘”)denotes submodel ğœƒğ‘–â€™s mask (same shape as
the submodel ğœƒğ‘–),ğœƒğ‘”denotes the aggregated model, ğ‘denotes the
total number of clients. It sets the components of both ğœƒğ‘–andğœƒğ‘”
at the same position to 1 and 0 for the rest. The role of the mask
functionğ‘šğ‘ğ‘ ğ‘˜(ğœƒğ‘–,ğœƒğ‘”)is to calculate the frequency of each model
parameterğœƒğ‘–selected by the global model ğœƒğ‘”in roundğ‘¡. Later,
the mask function ğ‘šğ‘ğ‘ ğ‘˜(ğœƒğ‘–,ğœƒğ‘”)will be utilized in Eq. (9) to treat
those low-frequency parameters equally by suppressing the weight
of high-frequency parameters in the aggregation, which makes
each parameter play a fair role during the aggregation phase. For
example, the more the frequency of a selected parameter in round ğ‘¡,
the smaller the weight of the parameter to be aggregated in round
ğ‘¡+1.
4.3 Fairness Guarantee
In Section 4.1, we delved into the fundamental concept underpin-
ning our definition of fairness. This concept centers on rewarding
high-contributing clients with high-performance submodels, where
a submodelâ€™s improved performance correlates with the number of
neurons it contains. Consequently, this approach leads to a training
loss (i.e., model accuracy) that more closely aligns with the aggre-
gated model. Itâ€™s important to note that the submodel ğœƒğ‘–acquired
by clientğ‘–is determined based on its reputation ğ‘Ÿğ‘–across the entireAlgorithm 1 FedSAC
Input: The global model ğœƒğ‘”, the local submodel ğœƒğ‘–, neuronsğ‘–â€™s
importance ğ¼ğ‘›ğ‘–, the number of local update steps ğ¸, learning
rateğœ‚ğ‘¡, number of clients ğ‘, hyper-parameter ğ›½, clientâ€™s con-
tributionğ‘
1:Initialize the global model parameters ğœƒ0ğ‘”
2:forroundğ‘¡=0,1,...,ğ‘‡âˆ’1do
3: Computeğ¼ğ‘¡ğ‘›ğ‘–((3) and (4)) of ğœƒğ‘¡ğ‘”
4: Calculate the reputation ğ‘Ÿğ‘–of clientğ‘˜:ğ‘Ÿğ‘–=ğ‘’ğ‘ğ‘–âˆ—ğ›½
ğ‘šğ‘ğ‘¥(ğ‘’ğ‘âˆ—ğ›½)âˆ—100
5: Calculate Submodels ğœƒğ‘¡
ğ‘–of clientsğ‘–in roundğ‘¡:ğœƒğ‘¡
ğ‘–=
ğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦(ğ‘Ÿğ‘–,Ã
ğ‘›ğ‘–âˆˆğ‘†ğ¼ğ‘¡ğ‘›ğ‘–)
6: foreach clientğ‘–âˆˆğ‘do
7: foreach iteration ğ‘—=0,1,...,ğ¸âˆ’1do
8: ğœƒğ‘¡
ğ‘–,ğ‘—+1â†ğœƒğ‘¡
ğ‘–,ğ‘—âˆ’ğœ‚ğ‘¡âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğ‘—)
9: end for
10: end for
11: Submodelğ‘–â€™s mask in round ğ‘¡:ğ‘šğ‘ğ‘ ğ‘˜ğ‘¡
ğ‘–=ğ‘šğ‘ğ‘ ğ‘˜(ğœƒğ‘¡
ğ‘–,ğœƒğ‘¡âˆ’1ğ‘”)
12: The server aggregates the received submodels: ğœƒğ‘¡+1ğ‘”=Ã
ğ‘–âˆˆğ‘ğœƒğ‘¡
ğ‘– Ã
ğ‘–âˆˆğ‘ğ‘šğ‘ğ‘ ğ‘˜ğ‘¡
ğ‘–
13:end for
training process up to iteration ğ‘¡.
Our primary result ensures a notion of fairness under specific
conditions concerning the loss function ğ¹. If clientğ‘–holds a higher
reputation than client ğ‘—(ğ‘Ÿğ‘–â‰¥ğ‘Ÿğ‘—), and the submodel ğœƒğ‘¡
ğ‘–obtained by
clientğ‘–encompasses the submodel ğœƒğ‘¡
ğ‘—obtained by client ğ‘—(ğœƒğ‘¡
ğ‘—âˆˆğœƒğ‘¡
ğ‘–âˆˆ
ğœƒğ‘¡ğ‘”). Then, the submodel ğœƒğ‘¡
ğ‘–obtained by client ğ‘–will exhibit closer
alignment with the aggregated model ğœƒğ‘¡ğ‘”in roundğ‘¡. Lettingğ›¿ğ‘¡
ğ‘–:=
||ğœƒğ‘¡ğ‘”âˆ’ğœƒğ‘¡
ğ‘–||, itâ€™s evident that ğ›¿ğ‘¡
ğ‘–â©½ğ›¿ğ‘¡
ğ‘—. Consequently, the submodel ğœƒğ‘¡
ğ‘–
obtained by client ğ‘–will yield a smaller loss function ğ¹(ğœƒ)compared
to clientğ‘—in roundğ‘¡.
Assumption 1 ( ğ¿-smooth F). Ifğ¹isğ¿-smooth, thenâˆ€ğœƒğ‘–,ğœƒğ‘—âˆˆğœƒ,
ğ¹(ğœƒğ‘–)â©½ğ¹(ğœƒğ‘—)+âˆ‡ğ¹(ğœƒğ‘—)ğ‘‡(ğœƒğ‘–âˆ’ğœƒğ‘—)+ğ¿
2||ğœƒğ‘–âˆ’ğœƒğ‘—||2. (10)
Assumption 2 ( ğœ‡-strongly convex F). Ifğ¹isğœ‡-strongly convex,
thenâˆ€ğœƒğ‘–,ğœƒğ‘—âˆˆğœƒ,
ğ¹(ğœƒğ‘–)â‰¥ğ¹(ğœƒğ‘—)+âˆ‡ğ¹(ğœƒğ‘—)ğ‘‡(ğœƒğ‘–âˆ’ğœƒğ‘—)+ğœ‡
2||ğœƒğ‘–âˆ’ğœƒğ‘—||2. (11)
Theorem 1 (Fairness in Training Loss). Assume Assumptions 1
and 2 hold, FedSAC can guarantee collaborative fairness by rewarding
high-contributing clients obtaining high-performance models. For-
mally speaking, let ğ›¿ğ‘¡
ğ‘–:=||ğœƒğ‘¡ğ‘”âˆ’ğœƒğ‘¡
ğ‘–||. Suppose that ğœƒğ‘¡is close to a
stationary point of ğ¹forğ‘¡â‰¥ğ‘‡âˆˆğ‘+, andğ¹()is bothğ¿-smooth and
ğœ‡-strongly convex with ğ¿â©½ğœ‡. For allğ‘–,ğ‘—âˆˆğ‘in round t, if ğ‘Ÿğ‘–â‰¥ğ‘Ÿğ‘—,
it follows that ğœƒğ‘¡
ğ‘—âˆˆğœƒğ‘¡
ğ‘–âˆˆğœƒğ‘¡ğ‘”,ğ›¿ğ‘¡
ğ‘–â©½ğ›¿ğ‘¡
ğ‘—, and therefore ğ¹(ğœƒğ‘¡
ğ‘–)â©½ğ¹(ğœƒğ‘¡
ğ‘—).
The proof process is as follows:
Fromğ¿-smoothness (ASSUMPTION 1), we have
ğ¹(ğœƒğ‘¡
ğ‘–)â©½ğ¹(ğœƒğ‘¡
ğ‘)+âˆ‡ğ¹(ğœƒğ‘¡
ğ‘)ğ‘‡(ğœƒğ‘¡
ğ‘–âˆ’ğœƒğ‘¡
ğ‘)+ğ¿
2ğ›¿2
ğ‘–,ğ‘¡
|                                          {z                                          }
ğ‘…ğ¿. (12)
Fromğœ‡-strongly convex (ASSUMPTION 2), we have
3303KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zihui Wang et al.
ğ¹(ğœƒğ‘¡
ğ‘—)â‰¥ğ¹(ğœƒğ‘¡
ğ‘)+âˆ‡ğ¹(ğœƒğ‘¡
ğ‘)ğ‘‡(ğœƒğ‘¡
ğ‘—âˆ’ğœƒğ‘¡
ğ‘)+ğœ‡
2ğ›¿2
ğ‘—,ğ‘¡
|                                           {z                                           }
ğ‘…ğœ‡. (13)
In order to prove ğ¹(ğœƒğ‘¡
ğ‘–)â©½ğ¹(ğœƒğ‘¡
ğ‘—), it suffices to prove ğ‘…ğ¿â©½ğ‘…ğœ‡or
equivalently ğ‘…ğ¿âˆ’ğ‘…ğœ‡â©½0.
ğ‘…ğ¿âˆ’ğ‘…ğœ‡=âˆ‡ğ¹(ğœƒğ‘¡
ğ‘)ğ‘‡(ğœƒğ‘¡
ğ‘–âˆ’ğœƒğ‘¡
ğ‘—)
|                  {z                  }
ğ‘…1+1
2(ğ¿ğ›¿2
ğ‘–,ğ‘¡âˆ’ğœ‡ğ›¿2
ğ‘—,ğ‘¡)
|              {z              }
ğ‘…2. (14)
Withğ¿â©½ğœ‡andğ›¿ğ‘–,ğ‘¡â©½ğ›¿ğ‘—,ğ‘¡, we have
ğ‘…2=1
2(ğ¿ğ›¿2
ğ‘–,ğ‘¡âˆ’ğœ‡ğ›¿2
ğ‘—,ğ‘¡)â©½ğ¿
2(ğ›¿2
ğ‘–,ğ‘¡âˆ’ğ›¿2
ğ‘—,ğ‘¡)â©½0. (15)
We defineğœƒğ‘¡
ğ‘being close to a stationary point of F by establishing
an upper limit on the gradient:
||âˆ‡ğ¹(ğœƒğ‘¡
ğ‘)||â©½ğ¿|ğ›¿2
ğ‘–,ğ‘¡âˆ’ğ›¿2
ğ‘—,ğ‘¡|
2||ğœƒğ‘¡
ğ‘–âˆ’ğœƒğ‘¡
ğ‘—||. (16)
We have the following:
|ğ‘…1|â‰œ|âˆ‡ğ¹(ğœƒğ‘¡
ğ‘)ğ‘‡(ğœƒğ‘¡
ğ‘–âˆ’ğœƒğ‘¡
ğ‘—)|
â©½||âˆ‡ğ¹(ğœƒğ‘¡
ğ‘)||Ã—||(ğœƒğ‘¡
ğ‘–âˆ’ğœƒğ‘¡
ğ‘—)||
â©½ğ¿|ğ›¿2
ğ‘–,ğ‘¡âˆ’ğ›¿2
ğ‘—,ğ‘¡|
2
â©½|ğ‘…2|,(17)
where the first inequality is derived from the Cauchy-Schwarz, the
second inequality is by substituting the aforementioned upper limit
(refer to Eq. (16)), and the last inequality (line 1209) emerges from
taking the absolute values of two negative values (refer to Eq. (15)).
Finally, given that|ğ‘…1|â©½|ğ‘…2|andğ‘…2â©½0, we derive ğ‘…1+ğ‘…2â©½0.
Therefore, it follows that ğ‘…ğ¿âˆ’ğ‘…ğœ‡â‰œğ‘…1+ğ‘…2â©½0, which subsequently
impliesğ¹(ğœƒğ‘¡
ğ‘–)â©½ğ¹(ğœƒğ‘¡
ğ‘—).
4.4 Convergence Analysis
In this section, we delve into the convergence analysis of the pro-
posed FedSAC. To guarantee convergence to the global optimum,
we make the assumption that each neuron in the aggregated model
is equally allocated over ğ‘‡rounds. Consequently, the anticipated
weight of the allocated submodel ğœƒğ‘–contracts towards the aggre-
gate model ğœƒğ‘”, i.e.,ğœƒğ‘¡+1
ğ‘–=ğ‘ğ‘–ğœƒğ‘¡ğ‘”. Here,ğ‘ğ‘–(0â©½ğ‘ğ‘–â©½1) denotes the
long-term expectation of the size ratio between the submodel ğ‘–
and the aggregate model obtained in multiple iterations. At this
stage, Eq. (9) can be expressed as the aggregation of each submodel
ğœƒğ‘–divided by its respective ğ‘ğ‘–, i.e.,ğœƒğ‘¡+1ğ‘”=Ãğ‘
ğ‘–=1ğœƒğ‘¡+1
ğ‘–
ğ‘ğ‘–. We present
THEOREM 2 below, which demonstrates that FedSAC enables the
convergence of the aggregation model. Assumptions 3 and 4 are
derived from the works [26, 38, 44].
Assumption 3. Letğœ‰ğ‘¡
ğ‘–denote samples uniformly from the local data
of theğ‘–-th device at random. It is asserted that the variance of sto-
chastic gradients within each device remains constrained:
ğ¸âˆ¥âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ’âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–)âˆ¥â©½ğœ2
ğ‘–, (18)Assumption 4. The expected squared norm of stochastic gradients is
uniformly constrained:
ğ¸âˆ¥âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ¥â©½ğº2, (19)
whereğ‘–âˆˆ{1,2,...,N}andğ‘¡âˆˆ{1,2,...,ğ‘‡âˆ’1}.
Assumption 5. Each neuron in the aggregation model is assigned the
same number of times after ğ‘‡rounds. Therefore, the expected weight
of the allocated submodel ğœƒğ‘–is a contraction of the aggregate model
ğœƒğ‘”, i.e.,ğœƒğ‘¡+1
ğ‘–=ğ‘ğ‘–ğœƒğ‘¡ğ‘”. Here,ğ‘ğ‘–(0â©½ğ‘ğ‘–â©½1) denotes the long-term
expectation of the size ratio between the submodel ğ‘–and the aggregate
model obtained in multiple iterations.
Theorem 2 (Asymptotic convergence). Given that Assump-
tions 1 to 5 hold and ğ¿,ğœ‡,ğœğ‘–,ğº,ğ‘be defined therein. Choose ğœ…=2
ğœ‡,
ğ›¾=ğ‘šğ‘ğ‘¥{8ğ¿
ğœ‡,ğ¸}âˆ’1and the learning rate ğœ‚ğ‘¡=2
ğœ‡(ğ›¾+ğ‘¡). Then FedSAC
satisfiesğ¸[ğ¹(Â¯ğœƒğ‘‡)]âˆ’ğ¹âˆ—â©½[ğ¿
ğ›¾+ğ‘‡(2ğµ
ğœ‡2+ğ›¾+1
2â–³1)].
The proof is shown in Appendix A.
4.5 Complexity and Communication Cost
Analysis
We further analyze the time complexity and communication costs
of FedSAC as follows.
For the time complexity, the primary computational demand
in FedSAC stems from evaluating neuron importance, as defined
in Eq. (3) and Eq. (4)). The time complexity for this evaluation is
ğ‘‚(ğ‘€), whereğ‘€denotes the total number of neurons across the
hidden layers of the global model.
For the communication cost, FedSAC mitigates the introduc-
tion of additional communication overhead by conducting neuron
importance evaluation solely on the server. This approach effec-
tively eliminates the necessity for client-server communication,
thereby enhancing overall efficiency. Moreover, it displays a com-
munication complexity of O(d*m) per round, as outlined in [ 6],
where m â©½1 denotes the average ratio of submodel parameters to
the global model. As a result, FedSAC showcases lower communi-
cation complexity compared to all baseline methods in cross-silo
FL scenarios [ 22]. More detailed results about the communication
cost analysis are put in Appendix1E.
4.6 Limitations
In Table 1 and Table 2, we conduct extensive experiments on var-
ious datasets and observe that FedSAC could exhibit a distinct
advantage over all baseline methods in terms of both fairness and
model accuracy. Nevertheless, the sufficient evaluation of neuron
importance (Section 4.1.1) within the submodel allocation module
imposes an additional computational burden. This problem may be
amplified for large models. Despite this challenge, we hold a strong
conviction that the substantial enhancements in both fairness and
accuracy achieved through by FedSAC clearly affirm its superiority
over baseline methods.
1https://arxiv.org/abs/2405.18291
3304FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
5 EXPERIMENTS
In this section, we conduct comprehensive experiments to answer
the following research questions:
RQ1. How does the fairness of our FedSAC compare to various
state-of-the-art methods?
RQ2. How does the predictive model performance achieved by
our proposed method compare with the state-of-the-art methods
on different datasets?
RQ3. How do different components (i.e., submodel allocation
module and dynamic aggregation module) affect the fairness and
the predictive model performance?
5.1 Experimental Settings
Datasets and Models. We evaluate the performance of FedSAC
on three commonly used public datasets in collaborative fairness,
including Fashion MNIST [ 9], CIFAR10 [ 8], and SVHN [ 21]. Follow-
ing [10,15,29], we employ a feedforward neural network with two
hidden layers for all datasets.
Data splits. We construct five heterogeneous scenarios by vary-
ing the size and the class numbers of the dataset. For imbalanced
dataset sizes (POW) [ 18,33], we randomly divide the total dataset
into various data sizes for each client by using a power law. For
CIFAR10, we partition the data set of size 20000 among 10 clients.
The clients with more extensive data sizes are expected to achieve
better prediction performance. For imbalanced class numbers
(CLA) [ 18,33], we change the number of classes and keep them
have the same amount of data. For CIFAR10 with 5 clients, clients 1,
2, 3, 4, 5 own local training data with 1, 3, 5, 7, 10 classes respectively.
Forimbalanced data size and class numbers (DIR), we provide
clients with various data sizes and classes by the Dirichlet distri-
bution function [ 2,4,40,41]. Specifically, we sample ğ‘ğ‘™
ğ‘–âˆ¼ğ·ğ¼ğ‘…(ğ›¼)
and assign a ğ‘ğ‘™
ğ‘–percentage of the data of class ğ‘™to clientğ‘–, where
ğ·ğ¼ğ‘…(ğ›¼)is the Dirichlet distribution with a parameter ğ›¼.
Baselines. We compare FedSAC with the following methods:
(1) FedAvg [ 19] distributes the same model to all clients in each
FL iteration. In this case, Pearson Correlation Coefficient ğœŒ()in
Section 3.1 is uncomputable. To address this and create a person-
alized model for each client, we follow CFFL [ 18] and CGSV [ 33],
which enables clients to train for an additional epoch at the end of
FL algorithm. (2) q-FFL[ 13] enables the reweighting of loss across
different clients by the q-parameterized weights, thus reducing the
variance in the accuracy distribution and achieving a fairer distri-
bution of accuracy. (3) CFFL [ 18] allocates more gradients to higher
reputation client, and the reputation is calculated by the local accu-
racy and data sizes (or label diversity). (4) CGSV [ 33] assigns more
gradients to clients whose local model gradients is more similar
to the global gradients. (5) FedAVE [ 31] assigns more gradients to
clients whose data distribution information is more similar to the
ideal dataset. (6) Standalone [ 18] trains local models alone without
collaboration. Particularly, to evaluate more fairly, we make all
algorithms distribute rewards based on client contributions rather
than the calculated reputations.
Hyper-Parameters. We tune all hyper-parameters in datasets
by using grid search with FedAvg [ 19] and subsequently apply the
optimal parameters obtained from the validation dataset. The batch
size isğµ= 64 for SVHN and ğµ= 32 for both FashionMNIST andCIFAR10. The optimal parameters for Cifar10, SVHN, and Fashion-
MNIST of six scenarios are ğ¸= {15, 20},ğœ‚= {0.03, 0.05}, ğ›½= {2, 3,
5, 8};ğ¸= 20,ğœ‚= {0.03, 0.05}, ğ›½= {3, 5, 10}, and ğ¸= {15, 20},ğœ‚= {
0.05, 0.1},ğ›½= {1, 10, 20, 25}, respectively. For comparison, we select
the best fairness achieved by each method. The effects of hyper-
parameterğ›½on FedSAC are showed in Appendix D (the smaller ğ›½
is, the higher the accuracy achieved by FedSAC). More details about
the hyper-parameters results ( ğœ‚,ğ¸, andğ›½) are put in Appendix1F.
Implementation. All experiments are run on a 64 GB-RAM
Ubuntu 18.04.6 server with Intel(R) Xeon(R) CPU E5-2630 v4 @
2.20GHz and 1 NVidia(R) 2080Ti GPUs.
5.2 Experimental Results
Fairness (RQ1). To evaluate the FedSAC fairness, we compared
it with a few baselines on three datasets. Table 1 shows the fair-
ness metrics according to ğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› 1. Standalone [ 18] trains local
models alone without collaboration, which represents the clientsâ€™
contributions. Table 1 indicates that our proposed dynamic sub-
model allocation mechanism achieves a fairness score above 95.73%
on all datasets, while the FedAvg performs poorly with the lowest
fairness score of -19.83%. On three datasets, the fairness of algo-
rithms (i.e., CFFL, CGSV, and FedAVE) exceeds 73.65% for the POW
and CLA scene. In these scenarios, the clientâ€™s contribution varies
greatly and is mainly related to the amount of data or diverse labels
of data. For the DIR scene, the data distribution among clients is sig-
nificantly uneven, resulting in a high degree of non-iid settings and
clients with relatively similar contributions. Consequently, CFFL,
CGSV, and FedAVE show low fairness, as the rewards received by
clients tend to be indistinguishable. In particular, in DIR (1.0) of
CIFAR10, our method outperforms CFFL, CGSV and FedAVE by
69.24%, 35.85%, and 40.71%, respectively.
Table 1 demonstrates that the proposed FedSAC outperforms
the state-of-the-art approaches in fairness, and validated the ef-
fectiveness of our method: high-contributing clients obtain high-
performance models. Figure 3 shows the comparison results of
overall performance to achieve bounded collaborative fairness with
state-of-the-art methods in CIFAR10 (left), SVHN (middle), and
Fashion MNIST (right). Obviously, FedSAC outperforms all base-
lines in terms of fairness.
Predictive performance (RQ2). To effectively assess the pre-
dictive performance of algorithms, we present our highest test ac-
curacies in comparison with all baseline methods in Table 2. These
results demonstrate the ability of the algorithms to reward high-
contributing clients with high-performance. First, comparing the
accuracy of FedSAC with Standalone (i.e., contribution) reveals that
FedSAC significantly outperforms Standalone. Second, among the
POW scene, FedSAC achieves the highest performance in CIFAR10,
SVHN and Fashion MNIST with accuracies of 48.61%, 74.84%, and
87.88%, respectively. Third, for the CLA scene on three datasets, the
highest accuracy is obtained by FedSAC, surpassing FedAvg by at
least 0.19%. In addition, in the extremely non-iid setting (e.g., DIR
(1.0) of SVHN), our method outperforms CFFL, CGSV, and FedAVE
by 2.17%, 5.06%, and 4.62%, respectively. Finally, for the DIR(2.0),
and DIR (3.0) scenes, FedSAC achieves comparable performance
to baseline methods in terms of accuracy. Specially, the notably
poor accuracy of q-FFL appears attributed to its mechanism that of-
fers the same reward to all clients, without adapting these rewards
3305KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zihui Wang et al.
Dataset CIF
AR10 SVHN Fashion
MNIST
No
. Clients 10 10 10
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Fe
dAvg[19] 14.55Â±25.4
84.68Â±3.5 10.77Â±6.3 15.06Â±21.2 68.93Â±8.1 -19.24Â±26.7
71.42Â±13.0 58.56Â±4.0 50.95Â±13.5 31.43Â±19.0 -19.83Â±24.1
78.44Â±4.3 17.94Â±7.3 60.36Â±16.3 76.09Â±29.5
q-FFL[13] 34.91Â±15.1
98.74Â±0.5
90.89Â±1.5
89.01Â±1.5
73.02Â±2.5 67.97Â±11.6
93.05Â±1.0 82.11Â±1.8 86.87Â±4.7
89.41Â±2.1 42.69Â±9.8
88.99Â±0.3
73.19Â±6.0 83.93Â±0.7 79.99Â±7.3
CFFL[18] 93.55Â±1.3
89.99Â±0.8 29.90Â±3.6 81.82Â±0.8 59.86Â±3.0 96.38Â±1.5
95.63Â±0.4
46.91Â±5.7 38.58Â±2.5 31.35Â±8.3 90.94Â±0.5
86.50Â±0.7 85.90Â±0.9
85.09Â±1.3 71.10Â±0.3
CGSV[33] 90.78Â±0.6
91.04Â±0.8 63.29Â±3.6 84.59Â±1.6 84.75Â±0.2 90.99Â±0.4
87.22Â±0.6 72.09Â±0.2 72.19Â±0.2 76.31Â±0.2 95.34Â±0.3
73.65Â±2.6 82.91Â±3.6 82.91 Â±3.6 84.95Â±1.3
Fe
dAVE[31] 85.50Â±0.8
92.80Â±1.2 58.43Â±0.9 85.82Â±0.7 88.61 Â±1.3 92.68Â±0.4
92.77Â±0.8 82.42Â±1.2
64.83Â±1.0 79.38Â±0.8 86.98Â±0.5
86.36Â±1.1 79.51Â±1.3 87.99 Â±0.5
67.62Â±0.3
Ours 98.80Â±0.2 99.06Â±0.3 99.14Â±0.6 95.73Â±0.5 97.01Â±0.7 99.44Â±0.3 99.74Â±0.1 96.09Â±0.3 96.48Â±0.2 98.32Â±0.8 96.35Â±0.2 98.93Â±0.7 99.23Â±0.3 97.71Â±0.2 98.62Â±0.3
Table 1: Comparison results of fairness ğœŒâˆˆ[âˆ’ 100,100]with state-of-the-art methods on three datasets. The reported results are
averaged over 5 runs with different random seeds. (A higher value indicates better fairness. The best average result is marked
in bold. The second-best result is underlined. These notes are the same to others.)
Dataset CIF
AR10 SVHN Fashion
MNIST
No
. Clients 10 10 10
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Standalone 41.23Â±0.1
37.49Â±0.2 33.78Â±0.2 33.54Â±0.1 31.89Â±0.1 60.02Â±0.2
52.05Â±0.3 41.41Â±0.2 58.07Â±0.2 61.11Â±0.2 84.36Â±0.2
82.52Â±0.3 64.19Â±0.3 67.39Â±0.1 74.29Â±0.2
FedAvg[19] 48.36Â±0.2
42.64Â±0.5
48.84Â±0.1
49.49Â±0.5
49.72Â±0.5 74.16Â±0.2
68.25Â±0.1 77.75Â±0.2
78.17Â±0.1
81.35Â±0.1 87.64Â±0.2
85.42Â±0.0
87.32Â±0.1
87.27Â±0.2
88.25Â±0.2
q-FFL[13] 46.22Â±1.5
41.40Â±0.2 35.00Â±0.9 37.41Â±1.1 37.81Â±0.4 69.61Â±0.6
54.71Â±1.52 34.15Â±0.6 44.69Â±1.3 56.92Â±1.1 85.44Â±0.2
82.93Â±0.5 65.51Â±2.4 69.61Â±2.1 78.96Â±0.8
CFFL[18] 47.94Â±0.6
42.12Â±0.3 44.44Â±0.9 48.44Â±0.3 47.56Â±1.2 72.68Â±0.2
69.66Â±0.7
75.89Â±0.9 75.21Â±0.3 77.01Â±0.9 87.40Â±0.4
85.29Â±0.4 86.16Â±1.0 87.23Â±0.2 87.11Â±0.8
CGSV[33] 34.60Â±0.7
39.33Â±0.5 47.22Â±0.6 44.12Â±0.5 39.31Â±0.7 65.92Â±0.6
65.96Â±0.3 73.00Â±1.9 75.16Â±0.2 77.42Â±0.5 83.16Â±0.3
82.41Â±1.3 85.03Â±2.9 84.73Â±3.2 76.51Â±4.2
FedAVE[31] 46.51Â±0.2
35.18Â±1.5 46.60Â±0.6 39.20Â±1.2 40.60Â±1.8 70.80Â±0.5
65.10Â±0.6 73.44Â±0.6 73.43Â±0.3 75.48Â±0.7 86.18Â±0.6
79.86Â±1.2 76.66Â±0.8 80.90Â±1.3 67.74Â±0.8
Ours 48.61Â±0.2 44.16Â±0.2 49.06Â±0.6 50.01Â±0.2 49.85Â±0.3 74.84Â±0.2 70.51Â±0.8 78.06Â±0.1 78.55Â±0.1 81.95Â±0.4 87.88Â±0.2 85.61Â±0.3 87.85Â±0.1 87.54Â±0.4 88.38Â±0.4
Table 2: Comparison results of the maximum test accuracy (%) with state-of-the-art methods on three datasets. The reported
results are averaged over 5 runs with different random seeds. (A higher value indicates better accuracy.)
20 40 60 80 100
Fairness35.037.540.042.545.047.550.0The maximum test accuracyCIFAR10
FedSAC
20
 0 20 40 60 80 100
Fairness4050607080The maximum test accuracySVHN
Frameworks
FedAvg
q-FFL
CFFL
CGSV
FedAVE
FedSAC
20
 0 20 40 60 80 100
Fairness6570758085The maximum test accuracyFashion MNIST
Frameworks
FedAvg
q-FFL
CFFL
CGSV
FedAVE
FedSACFrameworks
FedAvg
q-FFL
CFFL
CGSV
FedAVE
FedSACScenes
POW
CLA
Dir(1.0)
Dir(2.0)
Dir(3.0)Scenes
POW
CLA
Dir(1.0)
Dir(2.0)
Dir(3.0)Scenes
POW
CLA
Dir(1.0)
Dir(2.0)
Dir(3.0)
Figure 3: Comparison results of overall performance to achieve bounded collaborative fairness with state-of-the-art methods
in CIFAR10 (left), SVHN (middle), and Fashion MNIST (right). (The closer the point is to the upper-right corner, the better the
performance.)
based on individual client contributions. Figure 4 illustrates the dis-
tributions of contributions and allocated rewards under scenes (i.e.,
DIR (1.0)) in CIFAR10 comparing FedSAC against baseline meth-
ods. It demonstrates that FedSAC not only guarantees bounded
collaborative fairness but also enables clients to receive rewards
that exceed their contributions (i.e., Standalone). More results un-
der different scenarios (i.e., POW and CLA scene) on CIFAR10 and
SVHN are presented in Appendix B. In short, FedSAC outperforms
all baselines in terms of accuracy.
Figure 5 illustrates the changes in clientsâ€™ test accuracy as the
number of communication rounds increases in the POW, and CLA
data partition of CIFAR10 and SVHN. Owing to the varying datasizes and diversity of labels owned by clients in FL, their contri-
butions to the system exhibit significant differences. As shown
in Figure 5, our proposed FedSAC, underpinned by a theoretical
guarantee, aims to reward high-contributing clients with high-
performance submodels by maintaining consistency in local mod-
els. As a result, each client will converge to a different model and
achieve varying levels of performance.
Ablation study (RQ3). To evaluate the effectiveness of two
proposed modules in FedSAC, a series of ablation experiments are
carried out on three public benchmarks with 10 clients, as shown
in Table 3 and 4. The operation of eliminating neuron importance,
denoted asğ‘¤/ğ‘œ ğ‘ğ‘™ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› , aims to treat all neurons equally and
3306FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Dataset CIF
AR10 SVHN Fashion
MNIST
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
ğ‘¤/ğ‘œ
ğ‘ğ‘™ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 79.99
96.47 10.93 43.81 -14.61 96.88
94.88 31.86 64.94 82.58 91.05
96.09 84.57 79.14 65.53
ğ‘¤/ğ‘œğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 98.64
98.13 99.22 84.81 83.59 99.00
99.20 95.66 96.05 87.65 93.72
97.45 97.32 27.40 91.43
ğ¹ğ‘’ğ‘‘ğ‘†ğ´ğ¶ 98.80
99.06 99.14 95.73 97.01 99.44
99.74 96.09 96.48 98.32 96.35
98.93 99.23 97.71 98.62
Table 3: Ablation studies on FedSAC for fairness ğœŒâˆˆ[âˆ’ 100,100]on three public benchmarks. A higher ğœŒdenotes better fairness.
Dataset CIF
AR10 SVHN Fashion
MNIST
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
ğ‘¤/ğ‘œ
ğ‘ğ‘™ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 47.68
42.46 40.02 47.00 44.09 69.98
66.95 66.25 68.14 74.02 87.48
85.00 70.99 73.13 83.43
ğ‘¤/ğ‘œğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 48.03
43.62 48.23 49.46 48.61 73.81
68.91 77.35 72.89 80.46 87.74
85.95 86.90 85.72 86.74
ğ¹ğ‘’ğ‘‘ğ‘†ğ´ğ¶ 48.61
44.16 49.06 50.01 49.85 74.84
70.51 78.06 78.55 81.95 87.88
85.61 87.85 87.54 88.38
Table 4: Ablation studies on FedSAC for the maximum test accuracy (%) on three public benchmarks.
00.20.40.6
1 2 3 4 5 6 7 8 9 10AccuracyCIFAR10
Contribution CFFL q-FFL CGSV FedAVE FedSAC
00.20.40.60.8
1 2 3 4 5 6 7 8 9 10AccuracySVHN
Contribution CFFL q-FFL CGSV FedAVE FedSAC00.20.40.6
1 2 3 4 5 6 7 8 9 10AccuracyCIFAR10
Contribution q-FFL CFFL CGSV FedAVE FedSAC
00.20.40.60.8
1 2 3 4 5 6 7 8 9 10AccuracySVHN
Contribution q-FFL CFFL CGSV FedAVE FedSAC
Figure 4: Comparison results of test accuracy using the data
partition of DIR (1.0) with state-of-the-art methods in CI-
FAR10 (up) and SVHN (down). Results of other scenes (i.e.,
POW and CLA) are in Appendix B.
allocate submodels based on their contributions. ğ‘¤/ğ‘œğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
denotes removing the dynamic aggregation module, which uses
the traditional FedAvg aggregation method to train. The effective-
ness of the submodel allocation module is demonstrated in Table 3,
indicating that this module can reward high-contribution clients
to obtain high-performance models. In particular, our method has
significantly improved the fairness measure by 88.21% on the DIR
(1.0) scene of the CIFAR10 dataset. Table 4 shows the results of
the proposed dynamic aggregation module, which implies that this
module can effectively aggregate submodels with different sizes,
thereby further improving the overall performance of the local
0.20.30.40.5
0 50 100 150 200a. CIFAR10 -POW
client 1 client 2 client 3 client 4 client 5
client 6 client 7 client 8 client 9 client 10
0.10.20.30.40.5
0 50 100 150 200c. CIFAR10 -CLA
client 1 client 2 client 3 client 4 client 5
client 6 client 7 client 8 client 9 client 10
0.10.20.30.40.5
0 50 100 150 200e. CIFAR10 -DIR(2.0)
client 1 client 2 client 3 client 4 client 5
client 6 client 7 client 8 client 9 client 1000.20.40.60.8
0 100 200 300 400 500b. SVHN -POW
client1 client2 client3 client4 client5
client6 client7 client8 client9 client10
00.20.40.60.8
0 100 200 300 400 500d. SVHN -CLA
client1 client2 client3 client4 client5
client6 client7 client8 client9 client10Accuracy
Round sAccuracy
Round sAccuracy
Round s
Accuracy
Round sAccuracy
Round sAccuracy
Round s00.20.40.60.8
0 100 200 300 400 500f. SVHN -DIR(2.0)
client1 client2 client3 client4 client5
client6 client7 client8 client9 client10Figure 5: The test accuracy achieved by clients during train-
ing for CIFAR10 (left) and SVHN (right) in each round, under
the setting of POW and CLA.
models. Thus, the ablation study demonstrates that the two de-
signed modules in FedSAC are crucial and significant in enhancing
bounded collaborative fairness.
In summary, all experimental results show that both fairness
and model accuracy are of significance for bounded collaborative
fairness, and our FedSAC outperforms all baseline methods in both
fairness and model accuracy. More detailed results on the varying
numbers of clients (i.e., 20, 40, and 60) used are put in Appendix C.
6 CONCLUSION
In this work, we introduce a novel FL framework named FedSAC
that allocates submodels based on their contributions, thereby en-
suring bounded collaborative fairness and attaining superior local
accuracy while maintaining the consistency in local models. Our
method ensures that high-contributing clients can be rewarded
with high-performance submodels, which in turn enhances the
overall model accuracy. The experiments on three datasets show
that FedSAC exhibits a distinct advantage over baseline methods in
terms of fairness and accuracy. In the future, we aim to investigate
the implementation of FedSAC on large models.
7 ACKNOWLEDGMENTS
The research was supported by Natural Science Foundation of China
(62272403).
3307KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zihui Wang et al.
REFERENCES
[1]Anish Agarwal et al .2019. A marketplace for data: An algorithmic solution. In
Proceedings of the 2019 ACM Conference on Economics and Computation. 701â€“726.
[2]Chen Chen et al .2022. Gear: a margin-based federated adversarial training ap-
proach. In International Workshop on Trustable, Verifiable, and Auditable Federated
Learning in Conjunction with AAAI, Vol. 2022.
[3]Dengsheng Chen, Jie Hu, Vince Junkai Tan, Xiaoming Wei, and Enhua Wu. 2023.
Elastic Aggregation for Federated Optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). 12187â€“12197.
[4]Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu.
2022. Feddc: Federated learning with non-iid data via local drift decoupling and
correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 10112â€“10121.
[5]Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. 2022. Efficient
Split-Mix Federated Learning for On-Demand and In-Situ Customization. In In
Proceedings of the International Conference on Learning Representations.
[6]Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos
Venieris, and Nicholas Lane. 2021. Fjord: Fair and accurate federated learning un-
der heterogeneous targets with ordered dropout. Advances in Neural Information
Processing Systems 34 (2021), 12876â€“12889.
[7]Wenke Huang, Mang Ye, and Bo Du. 2022. Learn From Others and Be Yourself in
Heterogeneous Federated Learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 10143â€“10153.
[8]Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. Masterâ€™s thesis, Department of Computer Science, University of
Toronto, 2009. (2009).
[9]Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278â€“
2324.
[10] Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. 2021. Fed-
mask: Joint computation and communication-efficient personalized federated
learning via heterogeneous masking. In Proceedings of the 19th ACM Conference
on Embedded Networked Sensor Systems. 42â€“55.
[11] Bo Li, Mikkel N. Schmidt, Tommy S. AlstrÃ¸m, and Sebastian U. Stich. 2023. On
the Effectiveness of Partial Variance Reduction in Federated Learning With
Heterogeneous Data. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). 3964â€“3973.
[12] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of Machine learning and systems 2 (2020), 429â€“450.
[13] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. 2019. Fair Re-
source Allocation in Federated Learning. In International Conference on Learning
Representations.
[14] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2019.
On the Convergence of FedAvg on Non-IID Data. In International Conference on
Learning Representations.
[15] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020. Ensemble
distillation for robust model fusion in federated learning. Advances in Neural
Information Processing Systems 33 (2020), 2351â€“2363.
[16] Jian-Hao Luo et al .2017. Thinet: A filter level pruning method for deep neural
network compression. In Proceedings of the IEEE international conference on
computer vision. 5058â€“5066.
[17] Lingjuan Lyu, Yitong Li, Karthik Nandakumar, Jiangshan Yu, and Xingjun Ma.
2020. How to democratise and protect AI: Fair and differentially private decen-
tralised deep learning. IEEE Transactions on Dependable and Secure Computing
(2020).
[18] Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. 2020. Collaborative fairness in
federated learning. In Federated Learning. Springer, 189â€“204.
[19] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial Intelligence and Statistics . PMLR,
1273â€“1282.
[20] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. 2019.
Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 11264â€“11272.
[21] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-
drew Y Ng. 2011. Reading digits in natural images with unsupervised feature
learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning
2011 (2011).
[22] Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,
Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Mar-
foq, Erum Mushtaq, et al .2022. FLamby: Datasets and Benchmarks for Cross-Silo
Federated Learning in Realistic Healthcare Settings. Advances in Neural Informa-
tion Processing Systems 35 (2022), 5315â€“5334.
[23] Adam Richardson et al .2020. Budget-bounded incentives for federated learning.
InFederated Learning. Springer, 176â€“188.[24] Yuxin Shi, Han Yu, and Cyril Leung. 2023. Towards fairness-aware federated
learning. IEEE Transactions on Neural Networks and Learning Systems (2023).
[25] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan
Kian Hsiang Low. 2020. Collaborative machine learning with incentive-aware
model rewards. In International Conference on Machine Learning. PMLR, 8927â€“
8936.
[26] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. 2018. Sparsified
SGD with memory. Advances in Neural Information Processing Systems 31 (2018).
[27] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang.
2023. Federated learning on non-iid graphs via structural knowledge sharing. In
Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 9953â€“9961.
[28] Sebastian Shenghong Tay, Xinyi Xu, Chuan Sheng Foo, and Bryan Kian Hsiang
Low. 2022. Incentivizing collaboration in machine learning via synthetic data
rewards. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.
9448â€“9456.
[29] Md Palash Uddin, Yong Xiang, Xuequan Lu, John Yearwood, and Longxiang Gao.
2020. Mutual information driven federated learning. IEEE Transactions on Parallel
and Distributed Systems 32, 7 (2020), 1526â€“1538.
[30] Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. 2020. A
principled approach to data valuation for federated learning. Federated Learning:
Privacy and Incentive (2020), 153â€“167.
[31] Zihui Wang, Zhaopeng Peng, Xiaoliang Fan, Zheng Wang, Shangbin Wu, Rong-
shan Yu, Peizhen Yang, Chuanpan Zheng, and Cheng Wang. 2024. FedAVE:
Adaptive data value evaluation framework for collaborative fairness in federated
learning. Neurocomputing (2024), 127227.
[32] Xinyi Xu and Lingjuan Lyu. 2020. A reputation mechanism is all you need:
Collaborative fairness and adversarial robustness in federated learning. arXiv
preprint arXiv:2011.10464 (2020).
[33] Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, and
Bryan Kian Hsiang Low. 2021. Gradient driven rewards to guarantee fairness
in collaborative machine learning. Advances in Neural Information Processing
Systems 34 (2021), 16104â€“16117.
[34] Yuan-Yi Xu, Ci-Siang Lin, and Yu-Chiang Frank Wang. 2023. Bias-Eliminating
Augmentation Learning for Debiased Federated Learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 20442â€“
20452.
[35] Gang Yan, Hao Wang, Xu Yuan, and Jian Li. 2023. Criticalfl: A critical learning
periods augmented client selection framework for efficient federated learning.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2898â€“2907.
[36] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine
learning: Concept and applications. ACM Transactions on Intelligent Systems and
Technology (TIST) 10, 2 (2019), 1â€“19.
[37] Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit
Niyato, and Qiang Yang. 2020. A fairness-aware incentive scheme for federated
learning. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.
393â€“399.
[38] Hao Yu, Sen Yang, and Shenghuo Zhu. 2019. Parallel restarted SGD with faster
convergence and less communication: Demystifying why model averaging works
for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 5693â€“5700.
[39] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han,
Mingfei Gao, Ching-Yung Lin, and Larry S Davis. 2018. Nisp: Pruning networks
using neuron importance score propagation. In Proceedings of the IEEE conference
on computer vision and pattern recognition. 9194â€“9203.
[40] Yaodong Yu, Alexander Wei, Sai Praneeth Karimireddy, Yi Ma, and Michael
Jordan. 2022. TCT: Convexifying federated learning using bootstrapped neural
tangent kernels. Advances in Neural Information Processing Systems 35 (2022),
30882â€“30897.
[41] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald,
Nghia Hoang, and Yasaman Khazaeni. 2019. Bayesian nonparametric federated
learning of neural networks. In International Conference on Machine Learning.
PMLR, 7252â€“7261.
[42] Yufeng Zhan, Jie Zhang, Zicong Hong, Leijie Wu, Peng Li, and Song Guo. 2021. A
survey of incentive mechanism design for federated learning. IEEE Transactions
on Emerging Topics in Computing (2021).
[43] Jingwen Zhang, Yuezhou Wu, and Rong Pan. 2021. Incentive mechanism for hori-
zontal federated learning based on reputation and reverse auction. In Proceedings
of the Web Conference 2021. 947â€“956.
[44] Yuchen Zhang, Martin J Wainwright, and John C Duchi. 2012. Communication-
efficient algorithms for statistical optimization. Advances in neural information
processing systems 25 (2012).
3308FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A. PROOF OF THEOREM 2
Letğ¼ğ¸be the set of global synchronization steps, i.e., ğ¼ğ¸={ğ‘›ğ¸|ğ‘›=
1,2,...}. For convenience, We define ğ‘£ğ‘¡+1
ğ‘–as the immediate result of
one step SGD update from ğœƒğ‘¡
ğ‘–, i.e.,ğ‘£ğ‘¡+1
ğ‘–=ğœƒğ‘¡
ğ‘–âˆ’ğœ‚ğ‘¡âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–).Â¯ğ‘”ğ‘¡=
Ãğ‘
ğ‘–=1âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–)
ğ‘ğ‘–andğ‘”ğ‘¡=Ãğ‘
ğ‘–=1âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)
ğ‘ğ‘–. Therefore, Â¯ğ‘£ğ‘¡+1=Â¯ğœƒğ‘¡âˆ’ğœ‚ğ‘¡ğ‘”ğ‘¡
andğ¸ğ‘”ğ‘¡=Â¯ğ‘”ğ‘¡.
Lemma 3. (Result of one step SGD). Assume ASSUMPTION 1 and
ASSUMPTION 2. If ğœ‚ğ‘¡â©½1
4ğ¿, we have
ğ¸âˆ¥Â¯ğ‘£ğ‘¡+1âˆ’ğœƒâˆ—âˆ¥2â©½(1âˆ’ğœ‚ğ‘¡ğœ‡)ğ¸âˆ¥Â¯ğœƒğ‘¡âˆ’ğœƒâˆ—âˆ¥2+ğœ‚2
ğ‘¡ğ¸âˆ¥ğ‘”ğ‘¡âˆ’Â¯ğ‘”ğ‘¡âˆ¥2
+6ğ¿ğœ‚2
ğ‘¡Î“+2ğ¸ğ‘âˆ‘ï¸
ğ‘–=1âˆ¥Â¯ğœƒğ‘¡âˆ’ğœƒğ‘¡
ğ‘–âˆ¥2
ğ‘ğ‘–,(20)
where Î“=ğ¹âˆ—âˆ’Ãğ‘
ğ‘–=1ğ¹âˆ—
ğ‘–
ğ‘ğ‘–. LAMMA 3 has been made by [14].
For Assumption 3, the variance of stochastic gradients within
deviceğ‘–is constrained by ğœ2
ğ‘–. Consequently,
ğ¸âˆ¥ğ‘”ğ‘¡âˆ’Â¯ğ‘”ğ‘¡âˆ¥2=ğ¸âˆ¥ğ‘âˆ‘ï¸
ğ‘–=11
ğ‘ğ‘–(âˆ‡ğ¹ğ‘˜(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ’âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–))âˆ¥2
=ğ‘âˆ‘ï¸
ğ‘–=11
ğ‘2
ğ‘–ğ¸âˆ¥âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ’âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–)âˆ¥2
â©½ğ‘âˆ‘ï¸
ğ‘–=11
ğ‘2
ğ‘–ğœ2
ğ‘–.(21)
As FedSAC requires communication each ğ¸steps. We let ğœ‚ğ‘¡â©½
2ğœ‚ğ‘¡+ğ¸. Therefore, for any ğ‘¡â‰¥0, there exists a ğ‘¡0â©½ğ‘¡, such that
ğ‘¡âˆ’ğ‘¡0â©½ğ¸âˆ’1andğœƒğ‘¡0
ğ‘–=Â¯ğœƒğ‘¡0for allğ‘˜=1,2,...,ğ‘ . Then
ğ¸ğ‘âˆ‘ï¸
ğ‘–=11
ğ‘ğ‘–âˆ¥Â¯ğœƒğ‘¡âˆ’ğœƒğ‘¡
ğ‘–âˆ¥2=ğ¸ğ‘âˆ‘ï¸
ğ‘–=11
ğ‘ğ‘–âˆ¥(ğœƒğ‘¡
ğ‘–âˆ’Â¯ğœƒğ‘¡0)âˆ’( Â¯ğœƒğ‘¡âˆ’Â¯ğœƒğ‘¡0)âˆ¥2
â©½ğ¸ğ‘âˆ‘ï¸
ğ‘–=11
ğ‘ğ‘–âˆ¥ğœƒğ‘¡
ğ‘–âˆ’Â¯ğœƒğ‘¡0âˆ¥2
â©½ğ¸ğ‘¡âˆ’1âˆ‘ï¸
ğ‘¡=ğ‘¡0(ğ¸âˆ’1)ğœ‚2
ğ‘¡âˆ¥âˆ‡ğ¹ğ‘˜(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ¥2
â©½ğ‘¡âˆ’1âˆ‘ï¸
ğ‘¡=ğ‘¡0(ğ¸âˆ’1)ğœ‚2
ğ‘¡0ğº2
â©½ğœ‚2
ğ‘¡0(ğ¸âˆ’1)2ğº2
â©½4ğœ‚2
ğ‘¡(ğ¸âˆ’1)2ğº2.(22)
Here in lines 1252-1256, we use ğ¸âˆ¥ğ‘‹âˆ’ğ¸ğ‘‹âˆ¥2â©½ğ¸âˆ¥ğ‘‹âˆ¥2where
ğ‘‹=ğœƒğ‘¡
ğ‘–âˆ’Â¯ğœƒğ‘¡0with probability1
ğ‘ğ‘–. In the lines 1256-1259, we use
Jensen inequality:
âˆ¥ğœƒğ‘¡
ğ‘–âˆ’Â¯ğœƒğ‘¡0âˆ¥=âˆ¥ğ‘¡âˆ’1âˆ‘ï¸
ğ‘¡=ğ‘¡0ğœ‚ğ‘¡âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ¥2
â©½(ğ‘¡âˆ’ğ‘¡0)âˆ‘ï¸
ğ‘¡âˆ’ğ‘¡0ğ‘¡âˆ’1âˆ‘ï¸
ğ‘¡âˆ’ğ‘¡0ğœ‚2
ğ‘¡âˆ¥âˆ‡ğ¹ğ‘–(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ¥2.(23)In lines 1259-1262, we utilize ğœ‚ğ‘¡â©½ğœ‚ğ‘¡0forğ‘¡â‰¥ğ‘¡0andğ¸âˆ¥âˆ‡ğ¹ğ‘˜(ğœƒğ‘¡
ğ‘–,ğœ‰ğ‘¡
ğ‘–)âˆ¥2
â©½ğº2forğ‘–=1,2,...,ğ‘ . In the lines 1263-1265, we use ğœ‚ğ‘¡0â©½
2ğœ‚ğ‘¡0+ğ¸â©½2ğœ‚ğ‘¡forğ‘¡0â©½ğ‘¡â©½ğ‘¡0+ğ¸.
Letâ–³ğ‘¡=ğ¸âˆ¥Â¯ğœƒğ‘¡âˆ’ğœƒâˆ—âˆ¥. From Eq.(20), Eq. (21), and Eq. (22), it
follows that
â–³ğ‘¡+1â©½(1âˆ’ğœ‚ğ‘¡ğœ‡)â–³ğ‘¡+ğœ‚2
ğ‘¡ğ‘âˆ‘ï¸
ğ‘–=1ğœ2
ğ‘2
ğ‘–+6ğ¿ğœ‚2
ğ‘¡Î“+8ğœ‚2
ğ‘¡(ğ¸âˆ’1)2ğº2
â©½(1âˆ’ğœ‚ğ‘¡ğœ‡)â–³ğ‘¡+ğœ‚2
ğ‘¡(ğ‘âˆ‘ï¸
ğ‘–=1ğœ2
ğ‘2
ğ‘–+6ğ¿Î“+8(ğ¸âˆ’1)2ğº2)
|                                  {z                                  }
ğµ(24)
For a diminishing stepsize, ğœ‚ğ‘¡=ğœ…
ğ‘¡+ğ›¾for someğœ…>1
ğœ‡andğ›¾>0
such thatğœ‚1â©½ğ‘šğ‘–ğ‘›{1
ğœ‡,1
4ğ¿}=1
4ğ¿andğœ‚ğ‘¡â©½2ğœ‚ğ‘¡+ğ¸. We will prove
â–³â©½ğ‘£
ğ›¾+ğ‘¡by induction, where ğ‘£=ğ‘šğ‘ğ‘¥{ğœ…2ğµ
ğœ…ğœ‡âˆ’1,(ğ›¾+1)â–³1}. Firstly,
the definition of ğ‘£guarantees its applicability for ğ‘¡=1. Assuming
the conclusion holds for some ğ‘¡, it follows that
â–³ğ‘¡+1â©½(1âˆ’ğœ‚ğ‘¡ğœ‡)â–³ğ‘¡+ğœ‚2
ğ‘¡ğµ
â©½(1âˆ’ğœ…ğœ‡
ğ‘¡+ğ›¾)ğ‘£
ğ‘¡+ğ›¾+ğœ…2ğµ
(ğ‘¡+ğ›¾)2
=ğ‘¡+ğ›¾âˆ’1
(ğ‘¡+ğ›¾)2ğ‘£+[ğœ…2ğµ
(ğ‘¡+ğ›¾)2âˆ’ğœ…ğœ‡âˆ’1
(ğ‘¡+ğ›¾)2ğ‘£]
â©½ğ‘¡+ğ›¾âˆ’1
(ğ‘¡+ğ›¾)2ğ‘£+ğœ…2ğµ
(ğ‘¡+ğ›¾)2âˆ’ğœ…2ğµ
(ğ‘¡+ğ›¾)2âˆ’ğœ…ğœ‡âˆ’1
(ğ‘¡+ğ›¾)2(ğ›¾+1)â–³1|                   {z                   }
â©½0
â©½ğ‘£
ğ‘¡+ğ›¾âˆ’1(25)
Then by the ğ¿-smoothness (ASSUMPTION 1) of ğ¹,
ğ¸[ğ¹(Â¯ğœƒğ‘‡)]âˆ’ğ¹âˆ—â©½(Â¯ğœƒğ‘‡âˆ’ğœƒâˆ—)ğ‘‡âˆ‡ğ¹ğ‘–(ğœƒâˆ—)
|   {z   }
=0+ğ¿
2âˆ¥Â¯ğœƒğ‘‡âˆ’ğœƒâˆ—âˆ¥2
2
=ğ¿
2â–³ğ‘‡
â©½ğ¿
2ğ‘£
ğ›¾+ğ‘‡(26)
We letğœ…=2
ğœ‡,ğ›¾=ğ‘šğ‘ğ‘¥{8ğ¿
ğœ‡,ğ¸}âˆ’1. In the lines 1293, we have
ğ‘£=ğ‘šğ‘ğ‘¥{ğœ…2ğµ
ğœ…ğœ‡âˆ’1,(ğ›¾+1)â–³1}
â©½ğœ…2ğµ
ğœ…ğœ‡âˆ’1+(ğ›¾+1)â–³1
â©½4ğµ
ğœ‡2+(ğ›¾+1)â–³1(27)
Substituting Eq. 27 into Eq. 26, we obtain
0â©½lim
ğ‘‡â†’âˆğ¸[ğ¹(Â¯ğœƒğ‘‡)]âˆ’ğ¹âˆ—â©½lim
ğ‘‡â†’âˆ[ğ¿
ğ›¾+ğ‘‡(2ğµ
ğœ‡2+ğ›¾+1
2â–³1)]=0
(28)
Therefore, lim
ğ‘‡â†’âˆğ¸[ğ¹(Â¯ğœƒğ‘‡)]âˆ’ğ¹âˆ—=0.
3309KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zihui Wang et al.
Dataset CIF
AR10 Fashion
MNIST
No
. Clients 20 40 60 20
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) DIR(1.0)
DIR(2.0) DIR(1.0)
DIR(2.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Fe
dAvg[19] -40.52Â±3.3
83.57Â±1.4 18.41Â±16.3 77.52Â±1.5 44.13Â±16.0 79.53Â±13.9
54.37Â±11.1 51.64Â±17.8
55.22Â±6.8 -34.64Â±5.6
84.37Â±3.3 22.95Â±2.6 30.39Â±6.7 56.37Â±3.5
q-FFL[13] 14.10Â±2.8
98.09Â±0.2
83.98Â±2.0 86.14Â±2.9
90.00Â±0.5 87.81Â±1.6
80.46Â±1.1 80.65Â±2.9
85.76Â±1.2 29.24Â±5.3
98.44Â±2.6
81.75Â±4.8 77.45Â±5.6 72.89Â±3.9
CFFL[18] 81.45Â±1.0
95.93Â±0.8 76.72Â±2.1 76.09Â±1.3 63.79 Â±0.5 50.33Â±1.2
49.61Â±0.6 86.59Â±1.3
87.52Â±0.5 88.02Â±0.4
92.29Â±2.5 72.74Â±2.4 78.40Â±1.2 75.36 Â±1.6
CGSV[33] 83.30Â±1.8
96.80Â±0.1 79.85Â±0.9 79.73Â±1.3 85.72 Â±0.4 82.90Â±0.4
80.47Â±0.7 85.91Â±1.2
77.91Â±0.8 83.85Â±0.4
94.04Â±0.8 88.32Â±2.5
83.68Â±1.1
74.15Â±1.7
FedAVE[31] 88.46Â±1.5
97.18Â±0.6 87.59Â±1.1
78.43Â±0.4 68.70Â±1.3 45.69Â±2.6
65.60Â±1.2 38.92Â±1.1
60.61Â±1.3 87.14Â±0.6
93.97Â±1.2 78.87Â±2.1 81.20Â±1.5 72.73Â±0.7
Ours 99.62Â±0.2 98.52Â±0.1 96.29Â±0.4 98.06Â±0.5 95.99Â±0.4 99.37Â±0.5 96.57Â±0.2 97.69Â±0.6 95.12Â±0.3 97.40Â±0.4 98.49Â±0.3 96.52Â±0.8 97.65Â±0.6 96.47Â±0.7
Table 5: Comparison results of fairness ğœŒâˆˆ[âˆ’ 100,100]with state-of-the-art methods on CIFAR10 and Fashion MNIST. The
reported results are averaged over 5 runs with different random seeds. (A higher value indicates better fairness. The best
average result is marked in bold. The second-best result is underlined. These notes are the same to others.)
Dataset CIF
AR10 Fashion
MNIST
No
. Clients 20 40 60 20
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) DIR(1.0)
DIR(2.0) DIR(1.0)
DIR(2.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Standalone 37.15Â±0.0
35.25Â±0.3 27.82Â±0.0 31.83Â±0.1 33.56Â±0.1 32.13Â±0.1
28.86Â±0.2 28.58Â±0.2
27.84Â±0.1 82.36Â±0.1
81.17Â±0.2 66.71Â±0.2 68.00Â±0.2 73.63Â±0.3
FedAvg[19] 46.94Â±0.2
41.14Â±0.6 48.31Â±0.5
49.00Â±0.5
50.13Â±0.3 48.19Â±0.3
49.57Â±0.3 49.06Â±0.1
49.30Â±0.3 87.29Â±0.3
84.67Â±0.2 87.46Â±0.0
88.06Â±0.1
88.04Â±0.1
q-FFL[13] 46.87Â±0.2
41.56Â±0.3
33.77Â±0.1 38.17Â±0.9 43.91Â±0.6 38.64Â±1.5
39.37Â±0.4 34.01Â±0.1
36.77Â±0.5 85.79Â±0.2
81.10Â±0.1 68.30Â±1.7 78.59Â±1.8 77.69Â±1.1
CFFL[18] 46.06Â±0.0
39.43Â±0.3 45.76Â±0.3 48.57Â±0.2 48.42Â±0.3 39.54Â±0.3
39.16Â±0.6 42.33Â±0.4
41.06Â±0.6 85.88Â±1.8
81.75Â±2.1 81.69Â±0.6 84.16Â±0.4 87.34Â±0.4
CGSV[33] 46.29Â±0.2
37.75Â±1.4 46.72Â±1.0 48.45Â±0.3 49.24Â±0.2 46.75Â±0.2
48.32Â±0.1 49.11Â±0.7
48.58Â±0.3 87.21Â±0.2
84.25Â±0.2 85.17Â±0.7 86.85Â±0.3 88.07 Â±0.2
Fe
dAVE[31] 46.43Â±0.6
40.99Â±0.2 46.64Â±0.6 48.34Â±0.5 48.13Â±0.6 46.58Â±0.4
46.73Â±0.3 47.29Â±0.8
49.21Â±0.6 87.44Â±0.1
84.79Â±0.5
79.53Â±0.9 84.47Â±0.4 86.83Â±0.1
Ours 48.60Â±0.7 43.39Â±0.2 49.41Â±0.1 49.09Â±0.1 50.48Â±0.1 48.64Â±0.3 49.68Â±0.4 49.23Â±0.1 49.34Â±0.2 87.60Â±0.1 84.99Â±0.3 87.57Â±0.6 88.08Â±0.3 88.17Â±0.3
Table 6: Comparison results of the maximum test accuracy (%) with state-of-the-art methods on CIFAR10 and Fashion MNIST.
The reported results are averaged over 5 runs with different random seeds. (A higher value indicates better accuracy.)
00.20.40.6
12345678910Accuracya. CIFAR10-POW
Contributionq-FFLCFFLCGSVFedAVEFedSAC
00.20.40.6
12345678910Accuracye. CIFAR10-DIR(2.0)
Contributionq-FFLCFFLCGSVFedSAEFedSAC00.20.40.60.8
12345678910Accuracyb. SVHN-POW
Contributionq-FFLCFFLCGSVFedAVEFedSAC
00.20.40.60.8
12345678910Accuracyb. SVHN-CLA
Contribution q-FFL CFFL CGSV FedAVE FedSAC
00.20.40.60.8
1 2 3 4 5 6 7 8 9 10Accuracyb. SVHN -DIR(2.0)
Contribution q-FFL CFFL CGSV FedAVE FedSAC00.20.40.6
1 2 3 4 5 6 7 8 9 10Accuracyc. CIFAR10 -CLA
Contribution q-FFL CFFL CGSV FedAVE FedSAC
Figure 6: Comparison results of test accuracy using the scene
of POW and CLA with state-of-the-art methods in CIFAR10
(left) and SVHN (right). FedSAC exhibits the highest level of
consistency with the contribution.
B. FINAL REWARDS OF CLIENTS
Figure 6 shows the distributions of contributions and the final
rewards of clients under different scenes (i.e., POW and CLA in CI-
FAR10 and SVHN) by FedSAC and the compared methods. FedSAC
effectively differentiates the rewards it received, thereby ensuring
the collaborative fairness in FL.
C. VARYING NUMBERS OF CLIENTS
To verify the effectiveness of FedSAC in scenarios with varying
numbers of clients, we conduct experiments by increasing the num-
ber of local clients to 20, 40, and 60, respectively. Table 5 presentsScene PO
W DIR(1.0) DIR(2.0) DIR(3.0)
ğ¹
ğ‘’ğ‘‘ğ‘†ğ´ğ¶(ğ›½=2) 49.23(44.77)
49.66(39.53) 50.60(37.87) 51.37(43.19)
ğ¹ğ‘’ğ‘‘ğ‘†ğ´ğ¶(ğ›½=5) 48.43(37.73)
49.10(35.76) 50.01(30.91) 49.85(38.46)
ğ¹ğ‘’ğ‘‘ğ‘†ğ´ğ¶(ğ›½=10) 48.61(30.91)
49.06(31.36) 47.08(23.72) 49.04(28.54)
ğ¹ğ‘’ğ‘‘ğ‘†ğ´ğ¶(ğ›½=20) 46.53(24.10)
46.98(28.41) 43.50(20.17) 46.03(25.70)
Table 7: The maximum test accuracy (%) achieved by FedSAC
across different ğ›½, given a fairness threshold of ğœŒ>95%,
on CIFAR10. Values in the middle brackets represent the
minimum test accuracy (%) among 10 clients.
the fairness results achieved by FedSAC and the compared meth-
ods, while Table 6 shows the maximum local model performance
achieved by these methods. In the large client number settings (No.
Clients = 20, 40, and 60), FedSAC outperforms all baseline methods
in terms of fairness (refer to Table 5) and accuracy (refer to Table 6).
The results demonstrate that FedSAC can effectively implement
BCF in scenarios with varying numbers of clients.
D. THE IMPACT OF ğ›½ON THE EXPERIMENT
In Table 7, we present the performance of FedSAC with different
values ofğ›½on each scene of CIFAR10. The experiments demon-
strate that as ğ›½increases, the maximum test accuracy will gradually
decrease. This is because the size of the submodels downloaded by
clients increases as ğ›½decreases. When ğ›½is small, the submodels of
low-contribution clients contain more neurons, enabling effective
training to enhance all local model performance.
3310