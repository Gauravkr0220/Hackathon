Causal Inference with Latent Variables: Recent Advances
and Future Prospectives
Yaochen Zhu
University of Virginia
Charlottesville, VA, USA
uqp4qh@virginia.eduYinhan He
University of Virginia
Charlottesville, VA, USA
nee7ne@virginia.eduJing Ma
Case Western Reserve University
Cleveland, OH, USA
jing.ma@case.edu
Mengxuan Hu
University of Virginia
Charlottesville, VA, USA
qtq7su@virginia.eduSheng Li
University of Virginia
Charlottesville, VA, USA
shengli@virginia.eduJundong Li
University of Virginia
Charlottesville, VA, USA
jundong@virginia.edu
Abstract
Causality lays the foundation for the trajectory of our world. Causal
inference (CI), which aims to infer intrinsic causal relations among
variables of interest, has emerged as a crucial research topic. Nev-
ertheless, the lack of observation of important variables (e.g., con-
founders, mediators, exogenous variables, etc.) severely compro-
mises the reliability of CI methods. The issue may arise from the
inherent difficulty in measuring the variables. Additionally, in ob-
servational studies where variables are passively recorded, certain
covariates might be inadvertently omitted by the experimenter.
Depending on the type of unobserved variables and the specific
CI task, various consequences can be incurred if these latent vari-
ables are carelessly handled, such as biased estimation of causal
effects, incomplete understanding of causal mechanisms, lack of
individual-level causal consideration, etc. In this survey, we provide
a comprehensive review of recent developments in CI with latent
variables. We start by discussing traditional CI techniques when
variables of interest are assumed to be fully observed. Afterward,
under the taxonomy of circumvention and inference-based meth-
ods, we provide an in-depth discussion of various CI strategies
to handle latent variables, covering the tasks of causal effect esti-
mation, mediation analysis, counterfactual reasoning, and causal
discovery. Furthermore, we generalize the discussion to graph data
where interference among units may exist. Finally, we offer fresh as-
pects for further advancement of CI with latent variables, especially
new opportunities in the era of large language models (LLMs).
CCS Concepts
â€¢Mathematics of computing â†’Causal networks.
Keywords
Causal inference; latent variable models; confounding analysis
ACM Reference Format:
Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, and Jundong
Li. 2024. Causal Inference with Latent Variables: Recent Advances and
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671450Future Prospectives. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671450
1 Introduction
Our world is a woven web of causes and effects, where everything
that occurs is the consequence of some prior actions [ 6,159]. For
example, my headache disappeared because of the aspirin I took
this afternoon, and I gained muscle because I worked out regularly
every day. From the levity of language, we may be under the illusion
that reasoning with causality from experiences can be simple and
straightforward. However, formal causal inference did not emerge
until decades ago, which enabled rigorously derivation of causal
relationships of interest from the observational data [109].
In hindsight, what prevented the emergence of formal causal
inference (CI) is the lack of mathematical language to describe
causality [ 100]. One tempting choice is to use conditional dis-
tributions from probability theory for causal reasoning [ 15]. For
example, if an event ğ‘‡causes another event ğ‘Œ(whereğ‘‡,ğ‘Œ=1
means that the event happened and 0 otherwise), we usually have
ğ‘(ğ‘Œ=1|ğ‘‡=1)>ğ‘(ğ‘Œ=1|ğ‘‡=0). However, if we use the converse,
i.e., the increase of probability, to denote causality, correlation can
be easily mistaken for causation. For example, we can observe that
people eat more ice cream when they wear fewer clothes. However,
the former is clearly not a cause for the latter, as both are caused
by a third variable: hot weather. Here, the issue lies in the fact
thatğ‘‡=1in the conditional distribution means that ğ‘‡is passively
observed, but what makes the relation between ğ‘‡andğ‘Œcausal is
thatğ‘Œwill happen if we makeğ‘‡happen. That is why Rubin claimed
that "there is no causality without intervention" and introduced
the potential outcome ğ‘Œ(ğ‘‡=1)to describe the event ğ‘Œifğ‘‡=1is
made to happen for all population [ 55]. Similarly, Pearl introduced
thedo-operator, where ğ‘(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡))denotes the distribution of ğ‘Œif
we make the event ğ‘‡happen instead of observing it passively [ 96].
With new symbols defined to facilitate causal reasoning, various
causal questions can be formed in a rigorous manner. One common
CI task is average treatment effect (ATE) estimation [ 4], which
aims to estimate the expected influence of an event ( ğ‘‡) on another
(ğ‘Œ), e.g., the change of recovery rate ğ‘Œif drugğ‘‡is prescribed to
all patients. Since ATE compares the outcomes of two interven-
tions, i.e., treatment/no treatment, it can be directly formulated as
6677
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yaochen Zhu et al.
E[ğ‘Œ(ğ‘‡=1)]âˆ’E[ğ‘Œ(ğ‘‡=0)]orE[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡=1)]âˆ’E[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡=0)].
In addition, causal mediation analysis [ 53] is also feasible via the
new symbols, which aims to determine the fine-grained causal ef-
fect ofğ‘‡onğ‘Œmediated by other factors. For example, if we know
that drugğ‘‡cures the disease by reducing the blood pressure ğ‘
but it also thickens the blood vessel wall, we can define the causal
effects ofğ‘‡onğ‘Œmediated by ğ‘as the effect as if the drug has
no side effect. Furthermore, the individual level causal effect also
becomes tractable [ 67]. For example, for Alice, who has received
the treatment and survived, we can formulate the question "would
she also survive if no treatment had been provided?". Finally, we
can even formulate causal discovery with the new symbols [ 119],
where causal relations among variables of interest (e.g., treatment,
mediators) can be automatically discovered from data.
Nevertheless, representing causal questions with new symbols is
not enough. After all, directly obtaining the causal estimand ğ‘Œ(ğ‘‡)
orğ‘(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡))requires intervention upon ğ‘‡, which is not always
feasible. One strategy is to simulate interventions with randomized
experiment (RE) [ 39], where the randomization ensures that treat-
mentğ‘‡is the only contributor to the variation of ğ‘Œ. However, RE
can be expensive and unethical (e.g., we cannot randomly decide
whether or not to give drugs to patients). Therefore, CI with obser-
vational studies gains more attention, where the experimenter has
no manipulation over the treatment assignment. The aim is to show
that if certain assumptions hold for the data (i.e., identification cri-
teria), causal estimand with causal symbols can still be calculated
with conditional relations measurable in the collected data. For
example, if all confounders (i.e., factors that simultaneously affect
causeğ‘‡and effectğ‘Œ) are observed and recorded, backdoor adjust-
ment [ 127] and propensity score weighting [ 108] can be used to
estimate ATE. In addition, if the mediator of interest ğ‘€is measured,
the path-specific effect of ğ‘‡onğ‘Œmediated by ğ‘€can be obtained un-
der certain identification criteria [ 54]. If exogenous variables (e.g.,
individual factors not considered as the main variables of interest)
are known, individual-level counterfactuals can be calculated [ 96].
Finally, if all variables of interest are known, mature algorithms
such as the PC algorithm [ 118] are off-the-shelf for causal discovery.
However, important variables for CI can be latent, which hin-
ders the reliability of existing CI techniques [ 120]. The issue lies
primarily in two folds: (i)First, certain variables can be intrinsi-
cally difficult to measure, e.g., the socioeconomic status of a patient,
which is a crucial confounder for drug effect evaluation [ 72].(ii)
In addition, in the observational study, important covariates for CI
may not be recorded in the collected data [ 91]. The consequences
of carelessly handling latent variables for CI can be multi-faceted.
First, unobserved confounders can lead to bias in ATE estimation
[21]; for example, if the severity of disease is not considered, we
may erroneously conclude that an effective drug lowers the recov-
ery rate, as more severe patients tend to be treated with the drug.
In addition, missing important mediators could result in an incom-
plete understanding of the causal mechanism [ 87]. For example,
the debate over the causal relation between tobacco smoking and
lung cancer was not resolved until the mediator Tar deposit was
determined to cause lung cancer for smokers [ 111]. Exogenous vari-
ables are usually considered as noise and are not explicitly included
in the observational data [ 67]. However, without them, individual
differences in treatment effects cannot be estimated, which hinderspersonalized counterfactual analysis. Finally, if not all variables of
interest are available, causal discovery would be impossible [16].
Recent years have witnessed a plethora of works on causal in-
ference with latent variables [ 72]. Generally, the methods can be
categorized into two classes: (i)Circumvention-based Methods and
(ii)Inference-based Methods. Circumvention-based strategies es-
chew direct modeling of latent variables; instead, they show that
under certain stringent assumptions/conditions, latent variables
can be avoided while the causal estimand can still be identified
with observational data. However, there is no free lunch, and the
price being paid could be the requirement to measure more vari-
ables (where errors could be introduced) [ 36] and an increase in
estimation variance [ 9]. Inference-based methods, in contrast, ex-
plicitly model the latent variables based on the observations. This
usually includes proxy of the latent variables (e.g., their noisy ob-
servations). However, latent variables may not be identifiable given
the observed data, where bias can still remain in the causal estima-
tions [ 61]. In addition, the proxy of latent variables may contain
undesirable components, and carelessly ignoring them can ruin the
estimation results [ 84]. Both strategies on the main CI tasks, as well
as their generalization to graph data where interference exists, will
be thoroughly discussed. Our contribution can be summarized as:
â€¢Timely Topic. CI with latent variable is an important topic
while scattered in different CI areas. This survey provides a
timely and comprehensive review of the state-of-the-art.
â€¢Novel Taxonomy. We provide novel taxonomy on existing CI
methods to address latent variables, where two main categories
of methods on four CI tasks are thoroughly discussed.
â€¢New Hope. Based on existing techniques, we provide insights
into the future advancement of CI with latent variables, espe-
cially the new opportunities with large language models (LLM).
2 Preliminaries
2.1 Symbol System
For most CI tasks, there are two main variables of interest, i.e.,
treatmentğ‘‡and outcome ğ‘Œ, on which the causal relation is scruti-
nized. We consider ğ‘‡as a binary variable by default, but the cases
of continuous/multiple/high-dimensional treatments will also be
covered in detail. The outcome ğ‘Œcan be arbitrary results of interest
under the potential causal influence of ğ‘‡. In addition, we use ğ‘‹to
denote other observed covariates in the system, which may have
certain causal relations with ğ‘‡andğ‘Œdepending on the context.
2.2 Rubinâ€™s Causal Model
To study the causal relation between treatment ğ‘‡and outcome ğ‘Œ,
Rubinâ€™s causal model (SCM) starts by comparing individual-level
counterfactuals, i.e., for unit ğ‘–, what the outcome ğ‘Œis if the unit is
treated (ğ‘‡=1) or is not treated ( ğ‘‡=0). Although the two results
cannot be observed for the same unit ğ‘–simultaneously, we can still
hypothetically define them as potential outcomes as follows:
Definition 2.1. (Potential Outcome). We use the notations {ğ‘Œğ‘–(ğ‘‡=
1),ğ‘Œğ‘–(ğ‘‡=0)}(which are shortened as ğ‘Œğ‘–(1),ğ‘Œğ‘–(0)if the treatment
is clear from the context) to denote the potential outcomes (PO) of
ğ‘Œfor unitğ‘–if the treatment ğ‘‡=1or0is imposed on the unit.
6678Causal Inference with Latent Variables: Recent Advances
and Future Prospectives KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a)Chaincausalspurious(b)Forkspurious(c)V-structureTMYTYCTYVUTUMUY
Figure 1: Atomic structures of SCM, where mutually inde-
pendent exogenous variables Uare omitted for (b) and (c).
Accordingly, R.V.ğ‘Œ(ğ‘‡=1),ğ‘Œ(ğ‘‡=0)reason with the distribu-
tion of the POs if all units are uniformly treated or non-treated (i.e.,
interventions). However, ğ‘Œ(ğ‘‡=1),ğ‘Œ(ğ‘‡=0)cannot be obtained
due to lack of individual counterfactuals. To estimate ğ‘Œ(ğ‘‡=1),
ğ‘Œ(ğ‘‡=0), most strategies need to collect the outcomes of two
groups of treated and non-treated units. Here, we use conditional
R.V.ğ‘Œ|ğ‘‡=1andğ‘Œ|ğ‘‡=0to denote the distribution of ğ‘Œfor the
two groups. Only in rare cases, e.g., randomized experiments, can
ğ‘Œ|ğ‘‡=ğ‘¡provide an unbiased estimate for ğ‘Œ(ğ‘‡=ğ‘¡). In other cases,
the purpose of RCM is to show that under certain assumptions,
causal estimand with PO can be reduced to conditional relations
measurable in the data (usually involving other covariates ğ‘‹).
2.3 Structural Causal Model
Pearlâ€™s structural causal model (SCM), in contrast, reasons with
causality via a pre-defined direct acyclic causal graph Gthat en-
codes the belief of causal relations among variables of interest [ 96].
Based on the causal graph, SCM can be formally defined as follows:
Definition 2.2. (SCM). Structural causal model (SCM) can be
defined as a triplet of sets (U,V,F), whereUis the set of latent
exogenous variables, Vis a set of observed endogenous variables,
andFis a set of structural equations. For an endogenous variable
ğ‘‰âˆˆV, we haveğ‘‰=ğ‘“ğ‘‰(Ağ‘ˆ(ğ‘‰),Ağ‘‰(ğ‘‰)), whereAğ‘ˆ(ğ‘‰),Ağ‘‰(ğ‘‰)
are the exogenous, endogenous parents of ğ‘‰inG, respectively.
In SCM, each unit ğ‘–is associated with a set of exogenous variables
U=Uğ‘–that causally determines the endogenous variables, e.g., ğ‘‡,
ğ‘Œ,ğ‘‹. The prior forUisğ‘(U). Mutually-independent exogenous
variables are usually ignored when average causal effects are con-
sidered, but they are vital for counterfactual reasoning since they
represent unit variations. Three atomic structures exist in a causal
graph (Fig. 1): (i)chainsğ‘‡â†’ğ‘€â†’ğ‘Œ,(ii)forksğ‘‡â†ğ¶â†’ğ‘Œ,
and(iii) V-structure ğ‘‡â†’ğ‘‰â†ğ‘Œ.ğ‘‡andğ‘Œare correlated if me-
diatorğ‘€is not unobserved for chains (causal), confounder ğ¶is
not observed for forks (not causal), and colliderğ‘‰is observed for
V-structures (not causal). Therefore, to distinguish causation from
correlation, Pearl introduces the do-operator, where ğ‘(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡=ğ‘¡))
means that we set ğ‘‡=ğ‘¡as an intervention and calculate ğ‘Œvia
ğ‘“ğ‘Œ(ğ‘‡=ğ‘¡,Ağ‘ˆ(ğ‘Œ),Ağ‘‰(ğ‘Œ)/ğ‘‡), regardless of observed parents of ğ‘‡.
2.4 Connections between SCM and RCM
If an SCM is correctly specified, potential outcome ğ‘Œğ‘–(ğ‘‡=ğ‘¡)can
be derived by (i)replacing the structural equation ğ‘“ğ‘‡inFwith
ğ‘“ğ‘‘ğ‘œ
ğ‘‡=ğ‘¡(i.e., intervention), which results in a new set of structural
equationsFğ‘‘ğ‘œ,(ii)setting the exogenous variables U=Uğ‘–(i.e.,
the individual factors for unit ğ‘–), and (iii)calculating the outcome ğ‘Œ
based onUğ‘–and the new structural equations Fğ‘‘ğ‘œ. R.V.ğ‘Œ(ğ‘‡=ğ‘¡)can be similarly derived by using the prior of U, i.e.,ğ‘(U), instead
ofUğ‘–. Therefore, the two frameworks are fundamentally equivalent.
2.5 Overview of Causal Inference Tasks
2.5.1 Treatment Effect Estimation. Treatment effect estimation
aims to quantitatively measure the causal influence of treatment ğ‘‡
(e.g., drug) on outcome ğ‘Œ(e.g., survival rate). The most commonly
used metric is the average treatment effect (ATE) [4], which is
the expected causal effect of ğ‘‡onğ‘Œfor the entire population. ATE
can be formulated via the two frameworks as follows:
ğ´ğ‘‡ğ¸=E[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡=1)]âˆ’E[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡=0)]=E[ğ‘Œ(ğ‘‡=1)âˆ’ğ‘Œ(ğ‘‡=0)].
(1)
For a pretreatment variable ğ‘‹(e.g., age), we can also define the
conditional average treatment effect (CATE) [2] as follows:
ğ¶ğ´ğ‘‡ğ¸(ğ‘‹)=E[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡=1),ğ‘‹]âˆ’E[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡=0),ğ‘‹], (2)
which is important when the treatment effect is heterogeneous, i.e.,
different sub-populations ğ‘‹have different responses to treatment.
2.5.2 Causal Mediation Analysis. Causal mediation analysis
aims to quantitatively study the fine-grained causal relationship
between treatment ğ‘‡(e.g., drug) and outcome ğ‘Œ(e.g., survival rate)
mediated by certain factors ğ‘€(e.g., blood pressure) [ 53]. When
there is only one mediator, the most common metric is the natural
indirect effect (NIE), which can be formulated as follows [99]:
ğ‘ğ¼ğ¸=E[ğ‘Œ(ğ‘€(ğ‘‡=1),ğ‘‡=0))]âˆ’ E[ğ‘Œ(ğ‘€(ğ‘‡=0),ğ‘‡=0)].(3)
Here,ğ‘Œ(ğ‘€(ğ‘‡=ğ‘¡),ğ‘‡=0))is a nested potential outcome (NPO)
denoting three interventions: (i)ğ‘‡â†ğ‘¡along the path ğ‘€â†ğ‘‡,
(ii)ğ‘‡â†0along the path ğ‘Œâ†ğ‘‡, and (iii)ğ‘€â†ğ‘€(ğ‘‡=ğ‘¡)along
pathğ‘Œâ†ğ‘€. NIE excludes the direct effect of ğ‘‡along pathğ‘‡â†’ğ‘Œ,
while enabling the indirect effect of ğ‘‡mediated by ğ‘€. Furthermore,
NIE can be generalized to an arbitrary causal path, i.e., path-specific
causal effect, which can be defined in a similar way via NPO [53].
2.5.3 Counterfactual Reasoning. Counterfactuals can be broadly
defined as causal estimands (represented by ğ‘‘ğ‘œ-operator or poten-
tial outcomes) that contradict the factual observations (represented
by conditional distributions). For example, the average treatment
effect on the treated (ATT) is defined as follows:
ğ´ğ‘‡ğ‘‡=E[ğ‘Œ(1)|ğ‘‡=1]âˆ’E[ğ‘Œ(0)|ğ‘‡=1]1. (4)
Here, E[ğ‘Œ(0)|ğ‘‡=1]in Eq. (4) denotes the expected outcome ğ‘Œ
in a counterfactual world where the treated units (denoted by the
conditionğ‘‡=1) had not been treated (denoted by ğ‘Œ(ğ‘‡=0)).
2.5.4 Causal Discovery. Given variables of interest, causal dis-
covery aims to recover the causal graph Ggiven the observed data,
such that the parent nodes are the direct cause of the child [119].
3 Treatment Effect Estimation
In this section, we discuss the treatment effect estimation with latent
variables. Specifically, we mainly focus on the latent confounders,
which can systematically bias the estimation if handled carelessly.
We first introduce traditional methods where confounders are as-
sumed to be fully observed. We then discuss the circumvention -
based and inference -based strategies to handle latent confounders.
1Here, please note consistency is always assumed, i.e., E[ğ‘Œ(1)|ğ‘‡=1]=E[ğ‘Œ|ğ‘‡=1].
6679KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yaochen Zhu et al.
3.1 Brief Review of Traditional Methods
Traditional treatment effect estimation methods assume away the
latent confounders via the following ignorability assumption:
Assumption 1. (Ignorability). ğ‘Œ(ğ‘‡=0,1)âŠ¥âŠ¥ğ‘‡|ğ‘‹.
Combined with other common assumptions for CI (e.g., positivity,
non-interference, etc. see [ 55]), ATE and CATE can be identified
from observational data by controlling ğ‘‹as follows:
ğ¶ğ´ğ‘‡ğ¸(ğ‘‹)=E[ğ‘Œ|ğ‘‡=1,ğ‘‹]âˆ’E[ğ‘Œ|ğ‘‡=0,ğ‘‹],ğ´ğ‘‡ğ¸ =Eğ‘(ğ‘‹)[ğ¶ğ´ğ‘‡ğ¸(ğ‘‹)]
(5)
From the SCMâ€™s perspective, ğ‘‹blocks all backdoor paths that lead
to spurious correlations between ğ‘‡andğ‘Œ(see Section 2.3), such
that in each stratum of ğ‘‹=ğ‘¥, the correlation between ğ‘‡andğ‘Œ
is causal. Based on Eq. (5), adjustment-based methods use non-
parametric methods [ 8] or fit parametric models ğ‘“(ğ‘¡,ğ‘‹)(which
will be denoted as ğ‘“ğ‘¡(ğ‘‹)if different models are used for different ğ‘‡)
to estimate E[ğ‘Œ|ğ‘‡=ğ‘¡,ğ‘‹], including linear models [ 55], tree-based
methods [ 133], and deep neural networks (DNN) [ 114]. Another line
of methods reweights samples via inverse propensity score E[ğ‘‡=
ğ‘¡|ğ‘‹], such that they can be viewed as pseudo-random samples.
3.2 Circumvention-based Methods
However, if important confounders ğ¶are missing from the observed
covariatesğ‘‹, Assumption 1 failed, and Eq. (5) is a biased estimation
for C/ATE. To address the latent confounding bias, circumvention-
based methods show that, under certain stringent conditions, causal
effects can still be unbiasedly estimated without the direct or indi-
rect measurement of latent confounders or their proxies.
3.2.1 Small Randomized Data. If a small amount of randomized
data is available (which cannot be directly used to estimate CATE
due to high variance), we can use them to correct the bias in large-
scale observational data with latent confounders [ 24,101,124,129].
One exemplar work is [ 60], which first fits a biased CATE estimator
ğ‘“ğ‘œğ‘ğ‘ 
ğ‘¡(ğ‘‹)on the observational data as Eq. (5) and correct the bias
with another estimator ğ‘’ğ‘’ğ‘¥ğ‘
ğ‘¡(ğ‘‹)fitting on the error of ğ‘“ğ‘œğ‘ğ‘ 
ğ‘¡(ğ‘‹)
evaluated on the randomized data. Since the value of the bias is
usually smaller than the CATE, the estimation variance can be
reduced compared to directly fitting the CATE estimator on the
small-scale randomized data. In contrast, Yang et al . [149] directly
tackles confounding bias in the observational data. They define the
latent confounding bias with the confounding function as follows:
ğœ†(ğ‘‹)=Eğ‘œğ‘ğ‘ [ğ‘Œ(0)|ğ‘‡=1,ğ‘‹]âˆ’Eğ‘œğ‘ğ‘ [ğ‘Œ(0)|ğ‘‡=0,ğ‘‹], (6)
which measures the systematic difference of the expected baseline
POğ‘Œ(0)between the treatment/non-treatment group in the obser-
vational data. They further show that under the transportability
assumption, i.e., the treatment effect is the same between the ran-
domized samples and the treated samples in the observational data,
the confounding bias ğœ†(ğ‘‹)can be estimated as follows2:
ğœ†(ğ‘‹)=(Eğ‘œğ‘ğ‘ [ğ‘Œ|ğ‘‡=1,ğ‘‹]âˆ’Eğ‘œğ‘ğ‘ [ğ‘Œ|ğ‘‡=0,ğ‘‹])âˆ’
(Eğ‘’ğ‘¥ğ‘[ğ‘Œ|ğ‘‡=1,ğ‘‹]âˆ’Eğ‘’ğ‘¥ğ‘[ğ‘Œ|ğ‘‡=0,ğ‘‹]).(7)
Correction of the above bias upon the biased CATE estimator fit
on the observational data leads to an unbiased CATE estimator
2Proof is straightforward with consistency and transportability assumptions.
(a)InstrumentalVariableITYC(b)Front-doorAdjustmentTMYCFigure 2: SCM for IV methods and front-door adjustment
with variance lower than direct estimation with randomized data.
Recently, Wu and Yang [143] improved over [ 149] by adopting the
R-learner [ 92] to model the confounding function, which allows
flexible ML models such as trees and DNNs as the estimator.
The advantage of using randomized data to tackle latent con-
founding is that the randomized data are guaranteed to be unbiased
(but with high variance due to small scale). However, these methods
fail in the case where even a small number of randomized samples
cannot be obtained, e.g., when the dataset was collected in the past.
3.2.2 Instrumental Variable. If randomized data are not pos-
sible, we can use instrumental variables (IV) to "extract" pseudo-
randomized data embedded inside the observational dataset to un-
biasedly estimate ATE/CATE. Formally, IV is defined as follows:
Definition 3.1. (Instrumental Variable, IV) A variable that (i)
has no confounding with the outcome ğ‘Œ,(ii)affects the treatment ğ‘‡
(relevance), (iii) affects the outcome ğ‘Œonly through ğ‘‡(restriction).
For a binary IV, ATE can be unbiasedly estimated via [40]:
Ë†ğ´ğ‘‡ğ¸=(E[ğ‘Œ|ğ¼=1]âˆ’E[ğ‘Œ|ğ¼=0])/(E[ğ‘‡|ğ¼=1]âˆ’E[ğ‘‡|ğ¼=0]),(8)
if we viewğ¼as the assigned treatment and ğ‘‡as the treatment re-
ceived, the numerator can be viewed as the intention-to-treat effect
of the treatment assignment ( ğ¼) on outcome ( ğ‘Œ), and the denomina-
tor as the compliance with the assigned treatment. General IV-based
methods follow a similar two-stage procedure. Assuming linear
causal relations, the two-stage least squares algorithm (2SLS) (i)
first calculates the conditional mean of the treatment ğ‘‡given the
IVğ¼, i.e., Ë†ğ‘‡=E[ğ‘‡|ğ¼], and (ii)regressesğ‘ŒonË†ğ‘‡, where the coef-
ficient gives the causal relation between ğ‘Œandğ‘‡[5]. Afterward,
efforts have been devoted to generalizing 2SLS to nonlinear cases
[30,85,116]. For example, Deep IV [ 48] estimates the conditional
density in Ë†ğ‘‡=E[ğ‘‡|ğ¼,ğ‘‹]from stage (i)with categorical distribution
(for discrete ğ‘‡) or mixture of Gaussian distribution (for continuous
ğ‘‡) parameterized by DNN, and predicts the outcome ğ‘Œin stage (ii)
via another DNN Ë†ğ‘Œ=ğ‘“ğ‘›ğ‘›(Ë†ğ‘‡,ğ‘‹). However, to make the objective
optimizable, Deep IV assumes simple distributions, which fail when
the treatment ğ‘‡is high dimensional. To address this issue, Bennett
et al. [13] proposed to use a generalized method of moments to
allow more flexible DNNs as treatment/outcome networks [47].
However, finding suitable IVs is still difficult. Recently, Yuan et al .
[151] proposed the Auto IV, which finds IVs Ë†ğ¼from candidates ğ‘‹that
satisfy Definition 3.1 by maximizing the mutual information (MI)
between Ë†ğ¼andğ‘‡to ensure relevance, and minimizing the conditional
MI between Ë†ğ¼,ğ‘Œgivenğ‘‡to ensure the restriction criteria.
The advantages of IV-based methods are that (i)no randomized
data are required to address latent confounding, and (ii)mature
methods exist with good theoretical properties. However, it is dif-
ficult to find IV that satisfies the Definition 3.1. In addition, if the
IV is weak, i.e., has mild influences on the received treatments, the
estimation will have a high variance even with large data.
6680Causal Inference with Latent Variables: Recent Advances
and Future Prospectives KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
3.2.3 Front-door Adjustment. In addition, if the causal mech-
anism between the treatment ğ‘‡and outcome ğ‘Œis known, i.e., all
mediatorsğ‘€are observable and unconfounded with ğ‘‡andğ‘Œ, front-
door adjustment can be used to address latent confounders [ 95].
Specifically, based on the probability theory, we have
E[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡)]=Eğ‘(ğ‘€|ğ‘‘ğ‘œ(ğ‘‡))[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘€)]. (9)
Since no backdoor path exists between ğ‘‡andğ‘€,ğ‘(ğ‘€|ğ‘‘ğ‘œ(ğ‘‡))=
ğ‘(ğ‘€|ğ‘‡). In addition, since ğ‘‡blocks the backdoor path between ğ‘€
andğ‘Œ,ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘€))=Eğ‘ƒ(ğ‘‡)[ğ‘ƒ(ğ‘Œ|ğ‘€,ğ‘‡)], where Eq. (9) is reduced
to conditional relations measurable from the data. However, similar
to IV-based methods, mediators that satisfy the front-door criterion
are difficult to find. Therefore, Xu et al . [147] proposed to infer
latent mediators that satisfy the front criterion from the covariate
ğ‘‹with the identifiable variational auto-encoder (iVAE) [61].
3.2.4 Multiple Causes. Finally, we consider the case of multiple
treatments, where we are interested in estimating the combined
causal effects of all the treatments in ğ‘‡(e.g., prescribing bundled
drugs) onğ‘Œ. If we can determine that the latent confounders are
shared among different treatments (i.e., single-cause ignorability
[139]), various methods can be used to address the confounding
bias. The deconfounder-based methods prove that if latent vari-
ablesğ‘can be found that render different treatments conditional
independent, controlling ğ‘adjusts for the confounding bias due to
multi-cause confounders ğ¶ğ‘š[139]. The proof is simple and elegant:
ifğ¶ğ‘šare still active after conditioning on ğ‘, they will render the
treatments dependent (see Section 2.3), which results in contra-
diction. Linear models [ 140] and DNNs [ 110,163,164] are used to
estimateğ‘fromğ‘‡. Recently, Ma et al . [78] proposed to learn latent ğ‘
with latent clustering, which can well accommodate new treatments.
Observing that under single-cause ignorability assumption, the data
is unconfounded for every single cause, Qian et al . [104] proposed
to learn a single cause interventional model for each cause, and
perturb the cause to generate counterfactually-augmented datasets,
which they show are beneficial to learn multi-cause models.
3.3 Inference-based Methods
In this subsection, we introduce inference-based methods, which
assume that even if confounders ğ¶cannot be directly observed,
we can observe their proxies ğ‘Š, which could be conducive to the
inference of latent confounders to address confounding bias [ 126].
3.3.1 Proxy-based Methods. Statistical methods generally as-
sume simple forms of ğ¶and its causal relations with observed prox-
iesğ‘Š[98]. However, even for the simplest relation, i.e., ğ¶â†’ğ‘Š,
directly controlling ğ‘Šleaves the backdoor path ğ‘‡â†ğ¶â†’ğ‘Œopen,
which cannot adjust for all the confounding bias. To address this,
Kuroki and Pearl [66] assumed that ğ‘Šcontains two independent
views ofğ¶to recoverğ‘(ğ‘Š|ğ¶), such thatğ‘(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‡))can be identi-
fied fromğ‘(ğ‘Š|ğ¶)and other observable relations. Miao et al . [83]
relax the assumption, allowing for an unbiased estimation without
recovering the confounder measurement error mechanism ğ‘(ğ‘Š|ğ¶).
However, CI usually faces high-dimensional confounders and
proxiesğ‘Š, where the statistical methods may fail to scale up to.
Observing that even if ğ¶is complex, only a small part of the infor-
mation inğ¶is necessary to adjust for confounding (e.g., if ğ‘“(ğ¶)
preserves the propensity score, i.e., E[ğ‘‡|ğ¶]=E[ğ‘‡|ğ‘“(ğ¶)], adjustingforğ‘“(ğ¶)in Eq. (5) still gives an unbiased estimate of ATE/CATE
[108]), Kallus et al . [59] proposed to use matrix factorization (MF)
to obtain low-rank components of ğ‘Š, which they show are better
approximations of true confounders ğ¶. Louizos et al . [72] proposed
the causal effect variational auto-encoder (CEVAE), which uses the
VAE [ 64] to recover the joint distribution ğ‘(ğ¶,ğ‘‡,ğ‘Š,ğ‘Œ)and infer
latent confounders ğ¶from the observations {ğ‘‡,ğ‘Š,ğ‘Œ}.
In addition to unbiasedness, other aspects need to be carefully
considered as well. To address high variance due to non-overlapping
covariateğ‘Š(i.e., certain values of ğ‘Šappear only when ğ‘‡=0orğ‘‡=
1, which is common if ğ‘Šis high-dimensional), Wu and Fukumizu
[144] proposed to map ğ‘Što low dimensional space with better
overlapping w.r.t. the prognostic score [ 46] based on the iVAE [ 61],
which is sufficient for the identification of ATE/CATE. Furthermore,
in certain cases, we cannot identify latent variables ğ‘that lead to
an unbiased estimation of ATE/CATE. Therefore, Hu et al . [51]
proposed an adversarial learning [ 38]-based method to bound the
error by finding the max/min values of the possible ATE via a
generator and ensures that the distribution parameterized by the
generator is faithful to the observations via another discriminator.
3.3.2 Covariate Disentanglement. If the proxy ğ‘Šscrambles
variables other than latent confounders ğ¶, various issues could be in-
curred if naively using the above-introduced proxy-based methods.
To address this issue, covariate disentanglement (CD) methods are
proposed to further scrutinize the latent variables ğ‘that generate
the proxyğ‘Š. Most methods assume that ğ‘Šis generated from three
types of latent variables: IVs ğ¼(see Definition 3.1), confounders
ğ¶, and adjusters ğ´, i.e., variables that causally influence only the
outcomeğ‘Œ. Previous work has proven that controlling ğ¶eliminates
confounding bias, controlling ğ´could reduce estimation variance,
while controlling IVs ğ¼could increase the variance [50, 88].
To address the issue, most methods rely on the statistical property
between{ğ¼,ğ¶,ğ´}and{ğ‘‡,ğ‘Œ}: IVsğ¼are correlated with only the
treatmentğ‘‡, adjustersğ´are correlated only with the outcome ğ‘Œ,
while confounders ğ¶are correlated with bothğ‘‡andğ‘Œ. To leverage
this property, DR-CFR [ 49] designs three encoders to infer three
sets of latent variables Ë†ğ¼,Ë†ğ¶,Ë†ğ´fromğ‘Š, which are learned by making
Ë†ğ¼,Ë†ğ¶predictive for ğ‘‡(i.e., maximizing ğ‘(ğ‘‡|Ë†ğ¼,Ë†ğ¶)) and Ë†ğ¶,Ë†ğ´predictive
forğ‘Œ(i.e., maximizing ğ‘(ğ‘Œ|Ë†ğ´,Ë†ğ¶)). Similarly, TEDVAE [ 155] splits
the encoder in CEVAE into three parts for Ë†ğ¼,Ë†ğ¶,Ë†ğ´, respectively, and
maximizesğ‘(ğ‘‡|Ë†ğ¼,Ë†ğ¶),ğ‘(ğ‘Œ|ğ‘‡,Ë†ğ´,Ë†ğ¶)to achieve disentanglement.
The disentanglement can also be achieved by relying on other
properties. For example, assuming ğ¼,ğ¶,ğ´ are separable in the ob-
served proxy ğ‘Š, AFS [ 136] shows that ğ‘={ğ¶,ğ´}, provided that
the efficient influence curve of ğ‘,ğ·ğ‘’ğ‘“ ğ‘“(ğ‘)[130] is minimized. The
empirical estimate of ğ·ğ‘’ğ‘“ ğ‘“(ğ‘)is then used as the reward to learn a
mask to select ğ¶,ğ´ fromğ‘Š. In addition, NICE [ 115] uses invariant
risk minimization (IRM) [ 7] to find all causal parents of ğ‘Œ(including
ğ¶,ğ´), which can effectively exclude IVs from the control set.
However, the above methods fail when latent post-treatment
variablesğ‘€â€²are scrambled in the proxy ğ‘Š, as similar to ğ¶, post-
treatment variables ğ‘€â€²can be correlated with both the treatment
ğ‘‡and the outcome ğ‘Œ, and can be the causal parents of ğ‘Œ. Recently,
CiVAE [ 161] was proposed to disentangle ğ¶fromğ‘€â€². Specifically,
after individually identifying latent variables Ë†ğ‘that generate ğ‘Š, in-
dependence tests are conducted for each pair of Ë†ğ‘ğ‘–,Ë†ğ‘ğ‘—, and the pairs
6681KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yaochen Zhu et al.
(a)LatentConfounderinCMAMY(b)LatentMediatorinCMATMYTCX
Figure 3: SCM for latent variables in CMA
with increased correlation after conditioning on ğ‘‡are selected as
confounders. In contrast, if both Ë†ğ‘ğ‘–,Ë†ğ‘ğ‘—are post-treatment variables
or one is a post-treatment variable and another is a confounder, the
correlation will decrease after conditioning on ğ‘‡(see [96]).
4 Causal Mediation Analysis
Causal mediation analysis (CMA) [ 103] aims to understand the
fine-grained causal mechanism between treatment ğ‘‡and outcome
ğ‘Œby identifying the effect mediated by another factor ğ‘€, which
mediates the causal effect from the treatment to the outcome.
4.1 Brief Review of Traditional Methods
Traditional CMA identifies the causal mediation effect based on
the assumptions of measurable confounders and mediators. Specifi-
cally, unobserved confounders are assumed away via the Sequential
Ignorability assumption defined as follows:
Assumption 2. (Sequential Ignorability, SI). (i) ğ‘€(ğ‘¡),ğ‘Œ(ğ‘¡,ğ‘š)
âŠ¥âŠ¥ğ‘‡|ğ‘‹;(ii)ğ‘Œ(ğ‘¡,ğ‘š)âŠ¥âŠ¥ğ‘€(ğ‘¡)|ğ‘‡,ğ‘‹ for all t, m.
Intuitively, SI assumption states no unobserved confounding be-
tween (i)the treatment ğ‘‡and the mediator ğ‘€,(ii)the mediator ğ‘€
and the outcome ğ‘Œ, and (iii) the treatment ğ‘‡and the outcome ğ‘Œ.
In addition, since ğ‘€is the factor of which the mediated effect is
interested in, it should be observed and measurable.
Assumption 3. (Measurable Mediator). ğ‘€is observed.
With Assumptions 2, 3, the causal effect mediated by ğ‘€in the form
of natural indirect effect ( ğ‘ğ¼ğ¸, see Eq. (3)) can be calculated as
ğ‘ğ¼ğ¸=Eğ‘(ğ‘‹)
Eğ‘(ğ‘€|ğ‘‡=1,ğ‘‹)[ğ‘Œ|ğ‘‡=0,ğ‘‹,ğ‘€]âˆ’
Eğ‘(ğ‘€|ğ‘‡=0,ğ‘‹)[ğ‘Œ|ğ‘‡=0,ğ‘‹,ğ‘€]
,(10)
which holds ğ‘‡=0fixed on the direct path ğ‘‡â†’ğ‘Œ, and change ğ‘‡
from 0to1on the indirect path ğ‘‡â†’ğ‘€â†’ğ‘Œ. The natural direct
effect (NDE) of ğ‘‡onğ‘Œcan be calculated as ğ´ğ‘‡ğ¸âˆ’ğ‘ğ¼ğ¸. In practice,
the conditional distributions required by ğ‘ğ¼ğ¸ andğ‘ğ·ğ¸ can be
estimated using various methods, such as linear regression [ 12],
logistic regression [ 82], or machine learning techniques [ 53], such
as decision trees and deep neural networks [34, 53].
4.2 Latent Confounders in CMA
If latent confounders ğ¶exist and are not included in ğ‘‹, the sequen-
tial ignorability assumption breaks and traditional methods that
rely on Eq. (10) to estimate ğ‘ğ¼ğ¸ will give biased results. To address
the issue, various proxy-based methods are proposed. Here, an ex-
emplar work is causal mediation analysis variational auto-encoder
(CMAVAE) [ 17], which assumes that the latent confounder ğ¶con-
founds all pair-wise relations among ğ‘‡,ğ‘Œ,ğ‘€ , of which a noisy proxy
ğ‘Šcan be observed (definition of proxy see Section 3.3.1). Inspired
(a)Counterfactualdo(t)TY(b)Path-specificCounterfactualdo(tâ€™)MYUMUYUMUYdo(t)Figure 4: SCM for (path-specific) counterfactuals
by CEVAE [ 72], they prove that the ğ‘ğ¼ğ¸ can be identified by estimat-
ing the joint distribution ğ‘(ğ¶,ğ‘Š,ğ‘€,ğ‘‡,ğ‘Œ), which are parameterized
with DNNs, where the posterior distribution ğ‘(ğ¶|ğ‘Š,ğ‘€,ğ‘‡,ğ‘Œ)is ob-
tained via variational inference [ 64]. Finally, they sample latent
confounders ğ¶fromğ‘(ğ¶|ğ‘Š,ğ‘€,ğ‘‡,ğ‘Œ)to unbiasedly estimate ğ‘ğ¼ğ¸.
4.3 Latent Mediation Analysis
In this part, we discuss CMA with latent mediators, where Assump-
tion 3 fails, and Eq. (10) cannot be used directly for estimation.
4.3.1 Circumvention-based Methods. Circumvention-based
methods are difficult in CMA with latent mediators. Nevertheless,
Derkach at. al. [ 29] proposed a method without any utilization of
observable proxies of latent mediators. They made a strong assump-
tion that the distribution of ğ‘Œbelongs to an exponential family
ğ‘“(ğ‘Œ;ğœ‰,ğœ™ğ‘Œ)=ğ‘’ğ‘¥ğ‘[ğ‘Œğœ‰ğ‘Œâˆ’ğ‘(ğœ‰ğ‘Œ)]/ğ‘(ğœ™ğ‘Œ)+ğ‘(ğ‘Œ,ğœ™ğ‘Œ), whereğœ‰ğ‘Œand
ğœ™ğ‘Œare modeled as functions of the latent mediator. They observe
thatğ‘ğ¼ğ¸ can be represented by the parameters of the distribu-
tion. Therefore, to estimate the ğ‘ğ¼ğ¸, it is sufficient to estimate
the parameters of ğ‘“(ğ‘Œ;ğœ‰,ğœ™ğ‘Œ), which is solved via an expectation-
maximization (EM) algorithm: In each iteration of the algorithm, the
expected latent mediators are first calculated, then the distribution
parameters are updated via likelihood maximization.
4.3.2 Proxy-based Methods. If the mediator of interest ğ‘€is not
directly observed, utilizing its observed proxies ğ‘Šis an effective
method for ğ‘ğ¼ğ¸ estimation. Kuroki et al. [ 65,66] showed that the
ğ‘ğ¼ğ¸ of treatment ğ‘‡on an outcome ğ‘Œcould be identified in linear
models given two independent proxies of an unobserved mediator.
In addition, Albert et al. [ 3] proposed a maximum likelihood-based
approach to estimate causal mediation effects with a continuous la-
tent mediator measured by multiple observed proxies. Their method
is based on fitting a generalized structural equation model (GSEM)
[86] using an approximate Monte Carlo EM algorithm. The fit-
ted GSEM is then used to estimate natural direct and indirect ef-
fects [ 97]. In addition to the latent mediator, this approach also
accommodates mediator-outcome confounding and mixed contin-
uous and categorical outcomes. However, it relies on parametric
modeling assumptions and may be computationally intensive. Re-
cently, Sun et al. [ 123] proposed a joint modeling approach that
incorporates multiple latent mediators and a survival outcome.
Specifically, a Bayesian approach with a Markov chain Monte Carlo
algorithm is developed to perform an efficient estimation of ğ‘ğ¼ğ¸.
5 Counterfactual Analysis
In this section, we focus on counterfactual reasoning, which aims to
explain the outcomes of a specific individual if a different treatment
was taken in the past. According to SCM, exogenous variables ğ‘ˆ
contain individual varieties (see Section 2.3). Therefore, counter-
factual reasoning under SCM is naturally a latent variable problem
6682Causal Inference with Latent Variables: Recent Advances
and Future Prospectives KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[96]. Please note that throughout this section, we assume Sequen-
tial Ignorability (see Assumption 2) holds, so that we can devote
the main discussions to the latent exogenous variables.
5.1 Overview
With a pre-defined SCM G, counterfactuals generally have the
following form: E[ğ‘Œğ‘ˆ(ğ‘‡=ğ‘¡â€²)|ğ‘Š,ğ‘‡ =ğ‘¡], whereğ‘Šis the evidence
(observed values for variables in G) andğ‘‡=ğ‘¡is the observed
treatment. Here, we use the subscript ğ‘ˆinğ‘Œğ‘ˆ(ğ‘‡=ğ‘¡â€²)to denote
the dependence of the PO ğ‘Œ(ğ‘‡)on exogenous variables ğ‘ˆ. The
counterfactual inference involves three steps as follows:
â€¢(abduction) The prior of ğ‘ˆ, i.e.,ğ‘(ğ‘ˆ), is updated into posterior
ğ‘(ğ‘ˆ|ğ‘Š,ğ‘‡)based on the observed ğ‘Šandğ‘‡=ğ‘¡.
â€¢(action) Structural equation ğ‘“ğ‘‡is substituted with ğ‘“ğ‘‘ğ‘œ(ğ‘‡)=ğ‘¡â€².
â€¢(prediction) The outcome ğ‘Œis computed with ğ‘(ğ‘ˆ|ğ‘Š,ğ‘‡ =ğ‘¡â€²),
ğ‘“ğ‘‘ğ‘œ(ğ‘‡)=ğ‘¡â€², and other structural equations in F.
The key to counterfactual inference lies in the abduction of latent
exogenous variables ğ‘ˆ, as the other two steps are straightforward.
5.2 Circumvention-based Methods
We can circumvent latent exogenous variables ğ‘ˆif the counterfac-
tuals of interest are not required to be qualitatively determined. For
example, when studying counterfactual fairness of ML models, we
only need to judge whether two counterfactuals are the same:
E[Ë†ğ‘Œğ‘ˆ(ğ‘‡=ğ‘¡â€²)|ğ‘Š=ğ‘¤,ğ‘‡=ğ‘¡]?=E[Ë†ğ‘Œğ‘ˆ(ğ‘‡=ğ‘¡)|ğ‘Š=ğ‘¤,ğ‘‡=ğ‘¡].(11)
Intuitively, Eq. (11) asks that given evidence ğ‘Š=ğ‘¤andğ‘‡=ğ‘¡
(whereğ‘‡could be the sensitive features such as race, gender, etc.),
whether the prediction Ë†ğ‘Œwould be the same for a unit ğ‘ˆifğ‘‡is set to
another value ğ‘¡â€². Kusner et al . [67] showed that Eq. (11) holds when
predictor Ë†ğ‘Œdoes not use any descendant of ğ‘‡, which precludes
the dependence of Ë†ğ‘Œonğ‘‡. Afterward, Chiappa [18] proposed path-
specific counterfactual fairness (PSCF), which allows the causal
influence of ğ‘‡onË†ğ‘Œalong certain causal paths. For example, in the
single mediator case, we may allow ğ‘‡â†’ğ‘€â†’Ë†ğ‘Œwhile forbid
ğ‘‡â†’Ë†ğ‘Œ, whereğ‘€is called a resolving variable [ 63]. In this case, the
counterfactual question can be formulated via NPOs as follows:
E[Ë†ğ‘Œğ‘ˆ(ğ‘€(ğ‘¡),ğ‘‡=ğ‘¡â€²)|ğ‘Š=ğ‘¤,ğ‘‡=ğ‘¡]?=E[Ë†ğ‘Œğ‘ˆ(ğ‘€(ğ‘¡),ğ‘‡=ğ‘¡)|ğ‘Š=ğ‘¤,ğ‘‡=ğ‘¡].
(12)
If Eq. (12) holds, the predictor Ë†ğ‘Œis precluded from using the descen-
dants ofğ‘‡along the unfair paths ( ğ‘‡itself included) [89].
5.3 Inference-based Methods
In other cases, when counterfactuals need to be calculated or bounded,
inference-based methods become more useful [ 160]. Observing that
exogenous variables ğ‘ˆsatisfy exactly the non-descendant require-
ment ofğ‘‡while containing all individual information, Kusner et al .
[67] assumed linear structural equations and fitted linear additive
models on the observed data, where the error terms are viewed as
the estimand of ğ‘ˆand used for fair predictions. Zuo et al . [165] fur-
ther generalized [ 67] to the case of partially observed SCMs, under
the assumption that ğ‘‡has no endogenous ancestor. Wu et al . [146]
proposed to bound Eq. (12) by dividing the ğ‘ˆspace into equivalent
regions via response functions [ 10], and search the upper and lower
limit of Eq. (12) while making the response functions compatiblewith the observed ğ‘Šandğ‘‡. To achieve PSCF, Chiappa [18] pro-
posed to use VAE to infer ğ‘ˆand use it to correct the dataset by
settingğ‘‡of all samples to the baseline value along the unfair paths.
6 Causal Discovery
Previous sections primarily focus on CI with a pre-defined causal
graph. However, when accurate causal relations cannot be obtained
(e.g., lack of domain knowledge), it becomes imperative to automat-
ically discover the causal relations from data via causal discovery
(CD). In this section, we first introduce traditional CD methods, in-
cluding constraint-based and score-based methods. We then discuss
CD strategies when unobserved confounders exist.
6.1 Brief Review of Traditional Methods
Causal discovery (CD) aims to infer causal relations among vari-
ables of interestVfrom the observational dataset, with the goal of
constructing a causal graph G=(V,E). Most traditional CD meth-
ods rely on the assumptions of faithfulness and causal sufficiency
(which assume away unobserved confounders) as follows:
Assumption 4. (Faithfulness). If two disjoint sets of variables M
andNare independent in the distribution ğ‘ƒwhen conditioning on
Z, then it implies that MandNare d-separated [ 118] in the graph
Gconditioning onZ, denoted as:MâŠ¥âŠ¥ ğ‘ƒN|Z =â‡’MâŠ¥âŠ¥ GN|Z .
Assumption 5. (Causal Sufficiency). For any two observed vari-
ablesğ‘‰ğ‘–andğ‘‰ğ‘—in the data, all common causes must also be observed.
Generally, CD methods can be categorized into two classes: (i)
constraint-based and (ii)score-based methods [ 93]. Constraint-
based methods use conditional independence tests to identify edges
in the graph based on the faithfulness assumption. For example, the
Peter-Clark (PC) algorithm [ 118] and its variants [ 14,27,68,137]
first identify an undirected causal graph (i.e., skeleton ) by removing
edges from a complete causal graph with conditional independence
tests, and then determine the edge direction by a set of orientation
propagation rules with V-structures and acyclicity property [ 118].
For example, consider a path in the skeleton ğ´âˆ’ğµâˆ’ğ¶, whereğ´and
ğ¶are not adjacent. If ğ´andğ¶became dependent conditioning on ğµ,
then the PC algorithm orients the edges as ğ´â†’ğµâ†ğ¶based on the
property of V-structures (see Section 2.3). In contrast, score-based
algorithms [ 19,106,141] aim to identify the best candidate graph
by maximizing a fitness score, such as the Bayesian Information
Criterion (BIC), to discover the causal graph from the data.
6.2 Proxy-based Methods
There are a few proxy-based methods for CD with unobserved
confounders. Liu et al . [71] studied causal discovery between two
variablesğ‘‡,ğ‘Œwith a latent confounder ğ‘ˆ. Assuming a proxy ğ‘Š,
i.e., a causal descendant of ğ‘ˆ, can be observed, they discretize ğ‘Š
and use [ 83] introduced in Section 3.3.1 to estimate the causal effect
and judge whether an edge exists between ğ‘‡andğ‘Œ. However, such
proxies may not exist in reality. Recently, [ 70] introduced time
series data to address the issue, where each variable is assumed to
be its causal parent in the next time step, serving as the proxy ğ‘Š.
6683KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yaochen Zhu et al.
6.3 Circumvention-based Methods
6.3.1 Constraint-based Methods. If unobserved confounders
exist, the causal sufficiency assumption will not hold, and naive
independence tests (i.e., correlation) cannot indicate causal relations
among variables of interest. To address this issue, Spirtes et al . [120]
proposed the FCI algorithm, which extends the PC algorithm by
introducing three more relations (in addition to ğ‘‹â†’ğ‘Œ) to model
the uncertainty regarding confounders: (i)ğ‘‹â†”ğ‘Œindicates the
presence of unmeasured confounders; (ii)ğ‘‹â—¦ â†’ğ‘Œrepresents
that eitherğ‘‹causesğ‘Œor there are unmeasured confounders; (iii)
ğ‘‹â—¦âˆ’â—¦ğ‘Œcan represent any of the following scenarios: (1) ğ‘‹causes
ğ‘Œ, (2)ğ‘Œcausesğ‘‹, or (3) there are unmeasured confounders, where
a new orientation rule [118, 152] is used to orient edges.
Subsequent research has been proposed to extend FCI from var-
ious perspectives, such as with enhanced efficiency [ 25,26,117],
tailored for sparse causal graphs [ 121], improved scalability [ 105],
and incorporating different conditional independence tests [56].
6.3.2 Score-based Methods. Score-based algorithms, e.g., GES
and Fast GES (FGES) [ 26], find optimal causal graph by greedily
adding and deleting edges based on predefined scores measuring
the fitness of a graph on observational data. However, they face
challenges when latent confounders exist (i.e., causal insufficiency).
To address the issue, the recent trend is to use confounders-robust
constraint-based methods such as FCI to correct the bias. However,
it underperforms when the sample sizes are small due to an inaccu-
rate estimation of the independence relations [ 120]. The Greedy FCI
(GFCI) algorithm [ 94] combines the strengths of both approaches.
It uses GES to identify a supergraph of the skeleton, then employs
FCI to prune the supergraph and determine the orientations to
handle unmeasured confounders. This integration enhances per-
formance while maintaining asymptotic correctness under causal
insufficiency. However, GFCIâ€™s scoring function cannot be applied
to mixed variables, which is addressed by the Bayesian Constraint-
Based Causal Discovery (BCCD) algorithm [ 22] via utilizing a hy-
brid constraint and score-based approach for causal search.
7 Future Directions
In this section, we discuss several promising directions to further
advance causal inference studies with latent variables and discuss
new opportunities in the era of large language models (LLM).
7.1 On Theories and Model Design
Firstly, there has been a growing interest in causal representation
learning [ 112], which aims to develop models capable of automat-
ically extracting and representing causal concepts and relations
from data. An in-depth study of causal representation learning with
latent variables would be interesting in real-world applications.
Additionally, integrating multi-modal information of the unit,
such as textual [ 132,158], visual [ 148], and sensor data [ 128], offers
opportunities to compensate for the absence of observation in a sin-
gle modality, which also increases the chance of finding applicable
circumvention or inference methods to address the latent variable.
Furthermore, improving the interpretation [ 20,33,42] (especially
towards latent variables) is essential to foster trust and transparency
in causal learning systems with latent variables, allowing users to
comprehend and validate causal conclusions effectively.Finally, exploring uncertainty quantification techniques [ 1], such
as conformal prediction [ 113], can provide valuable information on
the reliability and robustness of CI under latent variables, facilitate
more informed decision-making, and provide pessimistic/optimistic
bounds when exact causal effects cannot be identified.
7.2 Opportunities in the LLM Era
Recently, large language models (LLM) exhibit remarkable in-context
learning and reasoning capabilities [ 107,134,138,156,162]. Al-
though LLM itself is nowhere causal [ 153] (after all, it still fits con-
ditional distributions parameterized by transformer networks on
corpora [ 131]), recent research has shown some promising results
of LLMs to facilitate CI, e.g., causal reasoning [ 58], counterfactual
analysis [ 154], and causal discovery [ 11,23,62]. For example, Jin
et al. [58] showed that when provided with few-shot examples with
chain-of-thought (CoT) [ 142] causal reasoning steps in the prompts,
LLMs can construct causal graphs, formulate causal questions with
the two frameworks and manage to solve it with observational data.
Based on the above examples, we speculate that LLMs can also
provide opportunities to advance CI with latent variables. Here,
we provide the following interesting future perspectives. (i)First,
it is promising to see LLM facilitate the automatic identification
of important latent variables that could be neglected by human
beings. As such, issues of neglecting important variables can be
prevented in advance. (ii)In addition, if the absence of important
variables is inevitable, LLM may have the potential to reason with
new strategies to circumvent or infer the variables from proxy based
on the reasoning ability to the causal relation of latent variables
and observed variables at hand (i.e., automatic causal discovery).
(iii) Furthermore, LLM may provide a usable and user-friendly
interpretation of latent variable models for CI [ 145], as well as how
biases are generated and eliminated. (iv)Finally, recent advances in
multi-modal LLM [ 32] are also promising to systematically consider
multi-modal features of a unit, where the more comprehensive
causal graph can be established by the LLM to increase the chance
of finding good solutions to address the latent variables.
8 Conclusions
In this survey, we review recent advances in causal inference (CI)
with latent variables, covering four main CI tasks, i.e., causal effect
estimation, causal mediation analysis, counterfactual reasoning,
and causal discovery. We start by briefly reviewing CI methods
where important variables are assumed to be observed. Then, under
the new taxonomy of inference-based and circumvention-based
methods, we introduce methods that account for the absence of
crucial variables. Furthermore, we generalize the above method to
graphs, an important area for machine learning. Finally, we discuss
future perspectives, especially the new opportunity in the LLM era.
Acknowledgment
This work was supported in part by the National Science Foun-
dation (NSF) under grants IIS-2006844, IIS-2144209, IIS-2223769,
IIS-2316306, CNS-2154962, and BCS-2228534; the Commonwealth
Cyber Initiative Awards under grants VV-1Q23-007, HV-2Q23-003,
and VV-1Q24-011; the JP Morgan Chase Faculty Research Award;
the Cisco Faculty Research Award; and Snap gift funding.
6684Causal Inference with Latent Variables: Recent Advances
and Future Prospectives KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Appendix
A Generalization to Graph Data
Causal inference on graph data (e.g., social networks) naturally
faces unique challenges compared with traditional tabular data
due to the intrinsic interconnection and interactions among units
under study. In the last few decades, there have been substantial
efforts in marrying causal inference with graph mining [ 28,75,
79], where latent variables still severely impede the robustness
and trustworthiness of causal conclusions. Here, we extend the
methodology introduced in the main paper to graph data.
Treatment Effect Estimation. Estimating treatment effect on
graphs inevitably requires particular method to handle the chal-
lenges brought by the graph structure. Studies in this area mainly
include the following branches: (i)Proxies including graph struc-
ture: although latent confounders on graphs are easily neglected by
regular methods, fortunately, the graph structure itself can serve
as proxies for the latent confounders in many cases [ 43,44,74].(ii)
Circumvention-Based Methods : Under certain circumstances, graph
structure affects the treatment assignments and plays the role of
an instrumental variable [ 73]. Therefore, IV-based causal effect es-
timation approaches can be applied. (iii) Interference : one major
issue of treatment effect estimation on graphs is that there often
exists interference between connected units (graph nodes), i.e., the
treatment of one unit may causally influence the outcome of other
units. This, however, violates the SUTVA assumption [ 96] in tra-
ditional causal inference. There have been numerous explorations
[35, 52, 80, 81] in this problem, covering different types of graphs.
Counterfactual Analysis. On graphs, counterfactual reasoning
targets on generating a different graph under certain circumstances
different from the factual one. As the graph structure is involved,
counterfactual analysis on graphs often involves additional consid-
erations regarding the causal relations among nodes, as well as the
discrete and unorganized structural space. Various investigations
have been conducted for this problem, including different goals
such as generalization [ 69,122], explanation [ 41,57,76,102,125],
and fairness [31, 45, 77] in many important applications.
Causal Discovery. The nature of graphs makes them closely as-
sociated with causal relations. Related causal discovery work in
this area mainly includes (i)methods based on classical graphi-
cal models [ 37], which rely on causal graphical models and have
been the mainstream of causal discovery; (ii)methods based on
learnable graph adjacency matrices in neural networks [ 148,157],
which discover the causal relations inside data by learning an ğ‘Ã—ğ‘
adjacency matrix for a causal graph with ğ‘variables; (iii) meth-
ods based on graph neural networks (GNNs) [ 90,135,150], which
explicitly leverage GNN techniques to facilitate causal discovery.
References
[1]Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu,
Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Ra-
jendra Acharya, et al .2021. A review of uncertainty quantification in deep
learning: Techniques, applications and challenges. Information Fusion 76 (2021),
243â€“297.
[2]Jason Abrevaya, Yu-Chin Hsu, and Robert P Lieli. 2015. Estimating conditional
average treatment effects. JBES 33, 4 (2015), 485â€“505.
[3]Jeffrey M Albert, Cuiyu Geng, and Suchitra Nelson. 2016. Causal mediation
analysis with a latent mediator. Biom. J. (2016).
[4]Joshua Angrist and Guido Imbens. 1994. Identification and estimation of local
average treatment effects. Econometrica 62, 2 (1994), 467â€“475.[5]Joshua D Angrist, Guido W Imbens, and Donald B Rubin. 1996. Identification
of causal effects using instrumental variables. JASA (1996).
[6]Elizabeth Anscombe. 2018. Causality and determination. In Agency and Respon-
siblity.
[7]Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv (2019).
[8]Susan Athey and Guido Imbens. 2016. Recursive partitioning for heterogeneous
causal effects. Proc. Natl. Acad. Sci. U.S.A (2016).
[9]Michael Baiocchi, Jing Cheng, and Dylan S Small. 2014. Instrumental variable
methods for causal inference. Stat. Med. (2014).
[10] Alexander Balke and Judea Pearl. 1994. Counterfactual probabilities: Computa-
tional methods, bounds and applications. In UAI. 46â€“54.
[11] Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, and Huanhuan Chen. 2023. From
query tools to causal architects: Harnessing large language models for advanced
causal discovery from data. arXiv (2023).
[12] Reuben M Baron and David A Kenny. 1986. The moderatorâ€“mediator variable
distinction in social psychological research: Conceptual, strategic, and statistical
considerations. J. Pers. Soc. Psychol. (1986).
[13] Andrew Bennett, Nathan Kallus, and Tobias Schnabel. 2019. Deep generalized
method of moments for instrumental variable analysis. In NeurIPS.
[14] Konstantina Biza, Ioannis Tsamardinos, and Sofia Triantafillou. 2020. Tuning
causal discovery algorithms. In ICPGM. 17â€“28.
[15] Leo Breiman. 1992. Probability. SIAM.
[16] Ruichu Cai, Jie Qiao, Kun Zhang, Zhenjie Zhang, and Zhifeng Hao. 2019. Causal
discovery with cascade nonlinear additive noise models. arXiv (2019).
[17] Lu Cheng, Ruocheng Guo, and Huan Liu. 2022. Causal mediation analysis with
hidden confounders. In WWW.
[18] Silvia Chiappa. 2019. Path-specific counterfactual fairness. In AAAI, Vol. 33.
7801â€“7808.
[19] David Maxwell Chickering. 2002. Optimal structure identification with greedy
search. JMLR 3, Nov (2002), 507â€“554.
[20] Zhixuan Chu, Mengxuan Hu, Qing Cui, Longfei Li, and Sheng Li. 2024. Task-
driven causal feature distillation: Towards trustworthy risk prediction. In AAAI,
Vol. 38. 11642â€“11650.
[21] Zhixuan Chu and Sheng Li. 2023. Causal Effect Estimation: Recent Progress,
Challenges, and Opportunities. Machine Learning for Causal Inference (2023).
[22] Tom Claassen and Tom Heskes. 2012. A Bayesian approach to constraint based
causal inference. arXiv (2012).
[23] Kai-Hendrik Cohrs, Emiliano Diaz, Vasileios Sitokonstantinou, Gherardo
Varando, and Gustau Camps-Valls. 2023. Large Language Models for
Constrained-Based Causal Discovery. In AAAI Workshop.
[24] BÃ©nÃ©dicte Colnet, Imke Mayer, Guanhua Chen, Awa Dieng, Ruohong Li, GaÃ«l
Varoquaux, Jean-Philippe Vert, Julie Josse, and Shu Yang. 2024. Causal inference
methods for combining randomized trials and observational studies: A review.
Stat. Sci. (2024).
[25] Diego Colombo, Marloes H Maathuis, et al .2014. Order-independent constraint-
based causal structure learning. JMLR 15, 1 (2014), 3741â€“3782.
[26] Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richard-
son. 2012. Learning high-dimensional directed acyclic graphs with latent and
selection variables. Ann. Stat. (2012).
[27] Ruifei Cui, Perry Groot, and Tom Heskes. 2016. Copula PC algorithm for causal
discovery from mixed data. In ECML PKDD. 377â€“392.
[28] Haixing Dai, Mengxuan Hu, Qing Li, Lu Zhang, Lin Zhao, Dajiang Zhu, Diez,
et al.2023. Graph-based counterfactual causal inference modeling for neu-
roimaging analysis. In MICCAI. 205â€“213.
[29] Andriy Derkach, Ruth M Pfeiffer, Ting-Huei Chen, and Joshua N Sampson. 2019.
High dimensional mediation analysis with latent variables. Biometrics 75, 3
(2019), 745â€“756.
[30] Nishanth Dikkala, Greg Lewis, Lester Mackey, and Vasilis Syrgkanis. 2020.
Minimax estimation of conditional moment models. In NeurIPS.
[31] Yushun Dong, Jing Ma, Chen Chen, and Jundong Li. 2022. Fairness in Graph
Mining: A Survey. arXiv (2022).
[32] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al .
2023. PaLM-E: An Embodied Multimodal Language Model. In ICLR. 8469â€“8488.
[33] Mengnan Du, Ninghao Liu, and Xia Hu. 2019. Techniques for interpretable
machine learning. Commun. ACM 63, 1 (2019), 68â€“77.
[34] Helmut Farbmacher, Martin Huber, LukÃ¡Å¡ LaffÃ©rs, Henrika Langen, and Martin
Spindler. 2022. Causal mediation analysis with double machine learning. The
Econometrics Journal (2022).
[35] Zahra Fatemi and Elena Zheleva. 2020. Minimizing interference and selection
bias in network experiment design. In AAAI.
[36] Isabel R Fulcher, Ilya Shpitser, Stella Marealle, and Eric J Tchetgen Tchetgen.
2020. Robust inference on population indirect causal effects: The generalized
front door criterion. J. R. Stat. (2020).
[37] Clark Glymour and Kun Zhang. 2019. Review of causal discovery methods
based on graphical models. Front. Genet. (2019).
6685KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yaochen Zhu et al.
[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In NeurIPS.
[39] Sander Greenland. 1990. Randomization, statistics, and causal inference. Epi-
demiology (1990).
[40] Sander Greenland. 2000. An introduction to instrumental variables for epidemi-
ologists. International Journal of Epidemiology 29, 4 (2000), 722â€“729.
[41] Zihan Guan, Mengnan Du, and Ninghao Liu. 2023. XGBD: Explanation-Guided
Graph Backdoor Detection. arXiv:2308.04406
[42] Zihan Guan, Mengxuan Hu, Sheng Li, and Anil Vullikanti. 2024. Ufid: A unified
framework for input-level backdoor detection on diffusion models. arXiv (2024).
[43] Ruocheng Guo, Jundong Li, Yichuan Li, K SelÃ§uk Candan, Adrienne Raglin, and
Huan Liu. 2020. IGNITE: A minimax game toward learning individual treatment
effects from networked observational data. In IJCAI.
[44] Ruocheng Guo, Jundong Li, and Huan Liu. 2020. Learning individual causal
effects from networked observational data. In WSDM.
[45] Zhimeng Guo, Jialiang Li, Teng Xiao, Yao Ma, and Suhang Wang. 2023. Towards
fair graph neural networks via graph counterfactual. In CIKM. 669â€“678.
[46] Ben B Hansen. 2008. The prognostic analogue of the propensity score. Biometrika
95, 2 (2008), 481â€“488.
[47] Lars Peter Hansen. 1982. Large sample properties of generalized method of
moments estimators. Econometrica (1982).
[48] Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2017. Deep
IV: A flexible approach for counterfactual prediction. In ICML.
[49] Negar Hassanpour and Russell Greiner. 2020. Learning disentangled represen-
tations for counterfactual regression. In ICLR.
[50] Mengxuan Hu, Zhixuan Chu, and Sheng Li. 2023. DBRNet: Advancing Individual-
Level Continuous Treatment Estimation through Disentangled and Balanced
Representation. (2023).
[51] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. 2021. A generative adversar-
ial framework for bounding confounded causal effects. In AAAI. 12104â€“12112.
[52] Qiang Huang, Jing Ma, Jundong Li, Ruocheng Guo, Huiyan Sun, and Yi Chang.
2023. Modeling interference for individual treatment effect estimation from
networked observational data. TKDD 18, 3 (2023), 1â€“21.
[53] Kosuke Imai, Luke Keele, and Dustin Tingley. 2010. A general approach to
causal mediation analysis. Psychol. Methods (2010).
[54] Kosuke Imai, Luke Keele, and Teppei Yamamoto. 2010. Identification, inference
and sensitivity analysis for causal mediation effects. (2010).
[55] Guido W Imbens and Donald B Rubin. 2015. Causal inference in statistics, social,
and biomedical sciences.
[56] Fattaneh Jabbari, Joseph Ramsey, Peter Spirtes, and Gregory Cooper. 2017.
Discovery of causal models that contain latent variables through Bayesian
scoring of independence constraints. In ECML PKDD.
[57] Utkarshani Jaimini and Amit Sheth. 2022. CausalKG: Causal Knowledge Graph
Explainability using interventional and counterfactual reasoning. IEEE Internet
Computing 26, 1 (2022), 43â€“50.
[58] Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, LYU Zhiheng,
Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan,
et al.2023. Cladder: Assessing causal reasoning in language models. In NeurIPS.
[59] Nathan Kallus, Xiaojie Mao, and Madeleine Udell. 2018. Causal inference with
noisy and missing covariates via matrix factorization. In NeurIPS.
[60] Nathan Kallus, Aahlad Manas Puli, and Uri Shalit. 2018. Removing hidden
confounding by experimental grounding. In NeurIPS.
[61] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. 2020.
Variational autoencoders and nonlinear ICA: A unifying framework. In AISTATS.
[62] Emre KÄ±cÄ±man, Robert Ness, Amit Sharma, and Chenhao Tan. 2023. Causal
reasoning and large language models: Opening a new frontier for causality.
arXiv (2023).
[63] Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt,
Dominik Janzing, and Bernhard SchÃ¶lkopf. 2017. Avoiding discrimination
through causal reasoning. In NeurIPS.
[64] Diederik P Kingma and Max Welling. 2014. Auto-encoding variational Bayes.
InICLR.
[65] Manabu Kuroki. 2007. Graphical identifiability criteria for causal effects in
studies with an unobserved treatment/response variable. Biometrika 94, 1 (2007),
37â€“47.
[66] Manabu Kuroki and Judea Pearl. 2014. Measurement bias and effect restoration
in causal inference. Biometrika (2014).
[67] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counter-
factual fairness. In NeurIPS.
[68] Thuc Duy Le, Tao Hoang, Jiuyong Li, Lin Liu, Huawen Liu, and Shu Hu. 2016.
A fast PC algorithm for high dimensional causal discovery with multi-core PCs.
IEEE/ACM TCBB 16, 5 (2016), 1483â€“1495.
[69] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022. Out-of-distribution
generalization on graphs: A survey. arXiv (2022).
[70] Mingzhou Liu, Xinwei Sun, Lingjing Hu, and Yizhou Wang. 2024. Causal
discovery from subsampled time series with proxy variables. In NeurIPS.
[71] Mingzhou Liu, Xinwei Sun, Yu Qiao, and Yizhou Wang. 2023. Causal discovery
with unobserved variables: A proxy variable approach. arXiv (2023).[72] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and
Max Welling. 2017. Causal effect inference with deep latent-variable models. In
NeurIPS.
[73] Jing Ma, Chen Chen, Anil Vullikanti, Ritwick Mishra, Gregory Madden, Daniel
Borrajo, and Jundong Li. 2023. A Look into Causal Effects under Entangled
Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection.
InKDD. 4584â€“4594.
[74] Jing Ma, Ruocheng Guo, Chen Chen, Aidong Zhang, and Jundong Li. 2021.
Deconfounding with networked observational data in a dynamic environment.
InWWW.
[75] Jing Ma, Ruocheng Guo, and Jundong Li. 2023. Causal Inference on Graphs. In
Machine Learning for Causal Inference.
[76] Jing Ma, Ruocheng Guo, Saumitra Mishra, Aidong Zhang, and Jundong Li. 2022.
Clear: Generative counterfactual explanations on graphs. In NeurIPS.
[77] Jing Ma, Ruocheng Guo, Mengting Wan, Longqi Yang, Aidong Zhang, and
Jundong Li. 2022. Learning fair node representations with graph counterfactual
fairness. In WSDM.
[78] Jing Ma, Ruocheng Guo, Aidong Zhang, and Jundong Li. 2021. Multi-cause
effect estimation with disentangled confounder representation. In IJCAI.
[79] Jing Ma and Jundong Li. 2022. Learning causality with graphs. AI Magazine 43,
4 (2022), 365â€“375.
[80] Jing Ma, Mengting Wan, Longqi Yang, Jundong Li, Brent Hecht, and Jaime
Teevan. 2022. Learning causal effects on hypergraphs. In KDD. 1202â€“1212.
[81] Yunpu Ma and Volker Tresp. 2021. Causal inference under networked interfer-
ence and intervention policy enhancement. In AISTATS. 3700â€“3708.
[82] David P MacKinnon, Amanda J Fairchild, and Matthew S Fritz. 2007. Mediation
analysis. Annu. Rev. Psychol. (2007).
[83] Wang Miao, Zhi Geng, and Eric J Tchetgen Tchetgen. 2018. Identifying causal
effects with proxy variables of an unmeasured confounder. Biometrika (2018).
[84] Jacob M Montgomery, Brendan Nyhan, and Michelle Torres. 2018. How condi-
tioning on posttreatment variables can ruin your experiment and what to do
about it. AJPS (2018).
[85] Krikamol Muandet, Arash Mehrjou, Si Kai Lee, and Anant Raj. 2020. Dual
instrumental variable regression. In NeurIPS.
[86] Bengt MuthÃ©n. 1984. A general structural equation model with dichotomous,
ordered categorical, and continuous latent variable indicators. Psychometrika
49, 1 (1984), 115â€“132.
[87] Bengt MuthÃ©n and Tihomir Asparouhov. 2015. Causal effects in mediation
modeling: An introduction with applications to latent variables. Struct. Equ.
Modeling (2015).
[88] Jessica A Myers, Jeremy A Rassen, Joshua J Gagne, Krista F Huybrechts, Sebas-
tian Schneeweiss, Kenneth J Rothman, Marshall M Joffe, and Robert J Glynn.
2011. Effects of adjusting for instrumental variables on bias and precision of
effect estimates. American Journal of Epidemiology 174, 11 (2011), 1213â€“1222.
[89] Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In AAAI.
[90] Ignavier Ng, Shengyu Zhu, Zhitang Chen, and Zhuangyan Fang. 2019. A graph
autoencoder approach to causal structure learning. arXiv (2019).
[91] Austin Nichols. 2007. Causal inference with observational data. The Stata
Journal (2007).
[92] Xinkun Nie and Stefan Wager. 2021. Quasi-oracle estimation of heterogeneous
treatment effects. Biometrika (2021).
[93] Ana Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi, and
JoÃ£o Gama. 2022. Methods and tools for causal discovery and causal inference.
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 12, 2
(2022), e1449.
[94] Juan Miguel Ogarrio, Peter Spirtes, and Joe Ramsey. 2016. A hybrid causal
search algorithm for latent variable models. In ICPGM. 368â€“379.
[95] Judea Pearl. 1995. Causal diagrams for empirical research. Biometrika 82, 4
(1995), 669â€“688.
[96] Judea Pearl. 2009. Causality.
[97] Judea Pearl. 2012. The mediation formula: A guide to the assessment of causal
pathways in nonlinear models. Causality: Statistical Perspectives and Applications
(2012), 151â€“179.
[98] Judea Pearl. 2012. On measurement bias in causal inference. arXiv (2012).
[99] Judea Pearl. 2022. Direct and indirect effects. In Probabilistic and Causal Inference:
The Works of Judea Pearl.
[100] Judea Pearl and Dana Mackenzie. 2018. The book of why: The new science of
cause and effect.
[101] Alexander Peysakhovich and Akos Lada. 2016. Combining observational and
experimental data to find heterogeneous treatment effects. arXiv (2016).
[102] Mario Alfonso Prado-Romero, Bardh Prenkaj, Giovanni Stilo, and Fosca Gian-
notti. 2023. A survey on graph counterfactual explanations: definitions, methods,
evaluation, and research challenges. Comput. Surveys (2023).
[103] Kristopher J Preacher. 2015. Advances in mediation analysis: A survey and
synthesis of new developments. Annu. Rev. Psychol. (2015).
[104] Zhaozhi Qian, Alicia Curth, and Mihaela van der Schaar. 2021. Estimating multi-
cause treatment effects via single-cause perturbation. In NeurIPS. 23754â€“23767.
6686Causal Inference with Latent Variables: Recent Advances
and Future Prospectives KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[105] Vineet K Raghu, Joseph D Ramsey, Alison Morris, Dimitrios V Manatakis, Peter
Sprites, Panos K Chrysanthis, Clark Glymour, and Panayiotis V Benos. 2018.
Comparison of strategies for scalable causal discovery of latent variable models
from mixed data. International Journal of Data Science and Analytics 6 (2018),
33â€“45.
[106] Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, and Clark Glymour.
2017. A million variables and more: The fast greedy equivalence search algorithm
for learning high-dimensional graphical causal models, with an application to
functional magnetic resonance images. International Journal of Data Science
and Analytics 3 (2017), 121â€“129.
[107] Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, and Chao Huang. 2024. A
Survey of Large Language Models for Graphs. arXiv (2024).
[108] Paul R Rosenbaum and Donald B Rubin. 1983. The central role of the propensity
score in observational studies for causal effects. Biometrika (1983).
[109] Donald B Rubin. 2005. Causal inference using potential outcomes: Design,
modeling, decisions. J. Am. Stat. Assoc (2005).
[110] Shiv Kumar Saini, Sunny Dhamnani, Akil Arif Ibrahim, and Prithviraj Chavan.
2019. Multiple treatment effect estimation using deep generative model with
task embedding. In WWW. 1601â€“1611.
[111] AJ Sasco, MB Secretan, and K Straif. 2004. Tobacco smoking and cancer: A brief
review of recent epidemiological evidence. Lung Cancer (2004).
[112] Bernhard SchÃ¶lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal
Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. 2021. Toward causal repre-
sentation learning. Proc. IEEE 109, 5 (2021), 612â€“634.
[113] Glenn Shafer and Vladimir Vovk. 2008. A tutorial on conformal prediction.
JMLR 9, 3 (2008).
[114] Uri Shalit, Fredrik D Johansson, and David Sontag. 2017. Estimating individual
treatment effect: Generalization bounds and algorithms. In ICML.
[115] Claudia Shi, Victor Veitch, and David M Blei. 2021. Invariant representation
learning for treatment effect estimation. In UAI.
[116] Rahul Singh, Maneesh Sahani, and Arthur Gretton. 2019. Kernel instrumental
variable regression. In NeurIPS.
[117] Peter Spirtes. 2001. An anytime algorithm for causal inference. In AISTATS.
278â€“285.
[118] Peter Spirtes, Clark N Glymour, and Richard Scheines. 2000. Causation, predic-
tion, and search.
[119] Peter Spirtes and Kun Zhang. 2016. Causal discovery and inference: Concepts
and recent methodological advances. In Appl. Inform.
[120] Peter L Spirtes, Christopher Meek, and Thomas S Richardson. 2013. Causal
inference in the presence of latent variables and selection bias. arXiv (2013).
[121] Eric V Strobl, Shyam Visweswaran, and Peter L Spirtes. 2018. Fast causal
inference with non-random missingness by test-wise deletion. JDSA (2018).
[122] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng
Chua. 2022. Causal attention for interpretable and generalizable graph classifi-
cation. In KDD.
[123] Rongqian Sun, Xiaoxiao Zhou, and Xinyuan Song. 2021. Bayesian causal media-
tion analysis with latent mediators and survival outcome. Structural Equation
Modeling: A Multidisciplinary Journal 28, 5 (2021), 778â€“790.
[124] Matt Taddy, Matt Gardner, Liyun Chen, and David Draper. 2016. A nonparamet-
ric bayesian analysis of heterogenous treatment effects in digital experimenta-
tion. JBES (2016).
[125] Juntao Tan, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li,
and Yongfeng Zhang. 2022. Learning and evaluating graph neural network
explanations based on counterfactual and factual reasoning. In WWW. 1018â€“
1027.
[126] Eric J Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi, and Wang Miao.
2020. An introduction to proximal causal learning. arXiv (2020).
[127] Jin Tian and Judea Pearl. 2002. A general identification condition for causal
effects. In IAAI.
[128] Fani Tsapeli and Mirco Musolesi. 2015. Investigating causality in human be-
havior from smartphone sensor data: a quasi-experimental approach. EPJ Data
Science 4, 1 (2015), 24.
[129] Mark van der Laan, Sky Qiu, and Lars van der Laan. 2024. Adaptive-TMLE for
the Average Treatment Effect based on Randomized Controlled Trial Augmented
with Real-World Data. arXiv (2024).
[130] Mark J Van der Laan, Sherri Rose, et al .2011. Targeted learning: Causal inference
for observational and experimental data. Vol. 4.
[131] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. In NeurIPS.
[132] Victor Veitch, Dhanya Sridhar, and David Blei. 2020. Adapting text embeddings
for causal inference. In UAI.
[133] Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneous
treatment effects using random forests. JASA (2018).
[134] Guangya Wan, Yuqi Wu, Mengxuan Hu, Zhixuan Chu, and Sheng Li. 2024.
Bridging causal discovery and large language models: A comprehensive survey
of integrative approaches and future directions. arXiv (2024).[135] Dongjie Wang, Zhengzhang Chen, Jingchao Ni, Liang Tong, Zheng Wang, Yanjie
Fu, and Haifeng Chen. 2023. Hierarchical graph neural networks for causal
discovery and root cause localization. arXiv (2023).
[136] Haotian Wang, Kun Kuang, Haoang Chi, Longqi Yang, Mingyang Geng, Wan-
rong Huang, and Wenjing Yang. 2023. Treatment effect estimation with adjust-
ment feature selection. In SIGKDD.
[137] Lun Wang, Qi Pang, and Dawn Song. 2020. Towards practical differentially
private causal graph discovery. In NeurIPS. 5516â€“5526.
[138] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al .2023.
Knowledge editing for large language models: A survey. arXiv (2023).
[139] Yixin Wang and David M Blei. 2019. The blessings of multiple causes. JASA
(2019).
[140] Yixin Wang, Dawen Liang, Laurent Charlin, and David M Blei. 2020. Causal
inference for recommender systems. In RecSys.
[141] Yuhao Wang, Liam Solus, Karren Yang, and Caroline Uhler. 2017. Permutation-
based causal inference algorithms with interventions. In NeurIPS.
[142] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
Quoc V Le, Denny Zhou, et al .2022. Chain-of-thought prompting elicits rea-
soning in large language models. In NeurIPS.
[143] Lili Wu and Shu Yang. 2022. Integrative ğ‘…-learner of heterogeneous treatment
effects combining experimental and observational studies. In CLeaR.
[144] Pengzhou Wu and Kenji Fukumizu. 2022. ğ›½-Intact-VAE: Identifying and esti-
mating causal effects under limited overlap. In ICLR.
[145] Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, Yucheng Shi, Fan Yang, Tianming
Liu, Xiaoming Zhai, Wenlin Yao, Jundong Li, Mengnan Du, et al .2024. Usable
XAI: 10 strategies towards exploiting explainability in the LLM era. arXiv
(2024).
[146] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. 2019. PC-fairness: A
unified framework for measuring causality-based fairness. In NeurIPS.
[147] Ziqi Xu, Debo Cheng, Jiuyong Li, Jixue Liu, Lin Liu, and Kui Yu. 2024. Causal
inference with conditional front-door adjustment and identifiable variational
autoencoder. In ICLR.
[148] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang.
2021. CausalVAE: Disentangled representation learning via neural structural
causal models. In CVPR.
[149] Shu Yang, Donglin Zeng, and Xiaofei Wang. 2020. Improved inference for hetero-
geneous treatment effects using real-world data subject to hidden confounding.
arXiv (2020).
[150] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. 2019. DAG-GNN: DAG structure learning
with graph neural networks. In ICML. 7154â€“7163.
[151] Junkun Yuan, Anpeng Wu, Kun Kuang, Bo Li, Runze Wu, Fei Wu, and Lanfen Lin.
2022. Auto IV: Counterfactual prediction via automatic instrumental variable
decomposition. TKDD (2022).
[152] Alessio Zanga, Elif Ozkirimli, and Fabio Stella. 2022. A survey on causal discov-
ery: Theory and practice. International Journal of Approximate Reasoning 151
(2022), 101â€“129.
[153] Matej ZeÄeviÄ‡, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting.
2023. Causal parrots: Large language models may talk causality but are not
causal. arXiv (2023).
[154] Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Xin Wen, and Bingchen Zhao.
2023. What if the TV was off? Examining counterfactual reasoning abilities of
multi-modal language models. In CVPR. 4629â€“4633.
[155] Weijia Zhang, Lin Liu, and Jiuyong Li. 2021. Treatment effect estimation with
disentangled latent factors. In AAAI.
[156] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey
of large language models. arXiv (2023).
[157] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. 2018. DAGs
with no tears: Continuous optimization for structure learning. In NeurIPS.
[158] Yaochen Zhu and Zhenzhong Chen. 2022. Mutually-regularized dual collabo-
rative variational auto-encoder for recommendation systems. In WWW. 2379â€“
2387.
[159] Yaochen Zhu, Jing Ma, and Jundong Li. 2023. Causal Inference and Recommen-
dations. In Machine Learning for Causal Inference. Springer, 207â€“245.
[160] Yaochen Zhu, Jing Ma, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2023.
Path-Specific Counterfactual Fairness for Recommender Systems. In SIGKDD.
3638â€“3649.
[161] Yaochen Zhu, Jing Ma, Liang Wu, Guo Qi, Liangjie Hong, and Jundong Li. 2024.
Treatment effect estimation with mixed latent post-treatment variables. (2024).
[162] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2024. Collabo-
rative large language model for recommender systems. In WWW.
[163] Yaochen Zhu, Jing Yi, Jiayi Xie, and Zhenzhong Chen. 2022. Deep causal
reasoning for recommendations. ACM TIST (2022).
[164] Hao Zou, Peng Cui, Bo Li, Zheyan Shen, Jianxin Ma, Hongxia Yang, and Yue
He. 2020. Counterfactual prediction for bundle treatment. In NeurIPS.
[165] Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, and Mingming Gong.
2022. Counterfactual fairness with partially known causal graph. In NeurIPS.
1238â€“1252.
6687