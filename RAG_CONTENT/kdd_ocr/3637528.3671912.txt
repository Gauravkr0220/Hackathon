Pre-Training Identification of Graph Winning Tickets in Adaptive
Spatial-Temporal Graph Neural Networks
Wenying Duan
wenyingduan@ncu.edu.cn
Jiangxi Provincial Key Laboratory of Intelligent Systems
and Human-Machine Interaction, Nanchang University
Nanchang, ChinaTianxiang Fang
6109121076@email.ncu.edu.cn
Nanchang University
Nanchang, China
Hong Rao
raohong@ncu.edu.cn
School of Software
Nanchang University
Nanchang, ChinaXiaoxi Heâˆ—
hexiaoxi@um.edu.mo
Faculty of Science and Technology
University of Macau
Macau, China
ABSTRACT
In this paper, we present a novel method to significantly enhance
the computational efficiency of Adaptive Spatial-Temporal Graph
Neural Networks (ASTGNNs) by introducing the concept of the
Graph Winning Ticket (GWT), derived from the Lottery Ticket
Hypothesis (LTH). By adopting a pre-determined star topology
as a GWT prior to training, we balance edge reduction with effi-
cient information propagation, reducing computational demands
while maintaining high model performance. Both the time and
memory computational complexity of generating adaptive spatial-
temporal graphs is significantly reduced from O(ğ‘2)toO(ğ‘). Our
approach streamlines the ASTGNN deployment by eliminating the
need for exhaustive training, pruning, and retraining cycles, and
demonstrates empirically across various datasets that it is possible
to achieve comparable performance to full models with substan-
tially lower computational costs. Specifically, our approach enables
training ASTGNNs on the largest scale spatial-temporal dataset
using a single A6000 equipped with 48 GB of memory, overcom-
ing the out-of-memory issue encountered during original training
and even achieving state-of-the-art performance. Furthermore, we
delve into the effectiveness of the GWT from the perspective of
spectral graph theory, providing substantial theoretical support.
This advancement not only proves the existence of efficient sub-
networks within ASTGNNs but also broadens the applicability of
the LTH in resource-constrained settings, marking a significant
step forward in the field of graph neural networks. Code is available
at https://anonymous.4open.science/r/paper-1430.
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671912CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks.
KEYWORDS
spatial-temporal graph neural network, lottery ticket hypothesis,
spatial-temporal data mining
ACM Reference Format:
Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He. 2024. Pre-
Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal
Graph Neural Networks. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671912
1 INTRODUCTION
Spatial-Temporal Graph Neural Networks (STGNNs) have estab-
lished themselves as a formidable tool for mining the hidden pat-
terns present in spatial-temporal data, displaying remarkable pro-
ficiency in modeling spatial dependencies via graph structures
[28]. The construction of these spatial graphs is a pivotal aspect
of STGNNs, in which the complex and implicit nature of spatial-
temporal relationships has paved the way for the recently emerging
self-learned methods that dynamically generate graphs to capture
these intricate dependencies in a data-driven manner. Adaptive
Spatial-Temporal Graph Neural Networks (ASTGNNs), a state-of-
the-art approach to spatial-temporal data processing, are partic-
ularly adept at creating adaptive graphs through learnable node
embeddings, as exemplified by models such as Graph WaveNet [ 29]
and AGCRN [3].
Despite their advanced performance, ASTGNNs are encumbered
by substantial computational overheads during both the training
and inference phases, primarily due to the exhaustive calculations
required for learning the adaptive adjacency matrices of complete
graphs, and the computationally intensive nature of the aggrega-
tion phase. This presents a significant challenge when dealing with
large-scale spatial-temporal data, where computational efficiency is
paramount. Pioneering work [ 10] has explored this aspect, improv-
ing the efficiency of ASTGNNs during inference via sparsification
of the spatial graph. However, the sparsification of the spatial graph
relies heavily on the training framework and can only be conducted
 
701
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He
after the training phase, leaving the efficiency of the training phase
itself untouched.
In order to improve the efficiency of both the training and in-
ference phases of ASTGNNs, our research introduces and explores
the concept of the Graph Winning Ticket (GWT) for the learnable
spatial graphs in ASTGNNs, an extension of the Lottery Ticket
Hypothesis (LTH) in the context of ASTGNN. The original LTH
posits the existence of smaller, efficient sub-networksâ€”â€™winning
ticketsâ€™â€”that can match the performance of the full network with
a fraction of the computational cost [ 12]. This concept has been
extended to the realm of ASTGNNs, where the identification of such
sub-networks within the learnable spatial graphs, i.e., GWTs, holds
the potential to markedly accelerate the training and inference
processes. However, a simple adoption of the LTH in the context
of ASTGNN is not sufficient for practically improving their effi-
ciency during both training and inference phases, as the traditional
method of finding winning tickets involves a compute-intensive
cycle of training, pruning, and retraining.
In contrast, our work aims to streamline this process by preemp-
tively identifying a GWT for the spatial graph in ASTGNNs. We
posit that a star topology, as a spanning tree of the complete graph,
serves as an effective pre-determined GWT, striking a balance be-
tween edge reduction and efficient information propagation. We
argue that the effectiveness of traditional ASTGNNs is enabled by
the adoption of a complete spatial graph, which has a diameter of
1 and thus allows for optimally efficient information propagation.
However, by relaxing the diameter of the graph from 1 to 2, our
star topology significantly trims the number of edges while still
preserving the integrity of spatial-temporal communication. We
empirically validate the performance of this star topology across
various datasets and benchmarks, solidifying its role as a winning
ticket for the spatial graphs in ASTGNNs.
We summarize our main contributions as follows:
â€¢To the best of our knowledge, we are the first to improve the
efficiency of ASTGNNs during both training and inference
phases, with an emphasis on the training phase. By leverag-
ing the concept of the Lottery Ticket Hypothesis (LTH), we
posit that an efficient subgraph of ASTGNNâ€™s spatial graph
can achieve comparable performance to the complete graph
with significantly reduced computational overhead. We in-
troduce a star topology as this winning ticket, which is not
only sparser but also retains the essential connectivity to
ensure effective information propagation across the network.
This pre-determined topology obviates the need for the tradi-
tional, exhaustive search process involving training, pruning,
and retraining, thereby streamlining the deployment of AST-
GNNs and substantially improving their efficiency during
both the training and inference phases.
â€¢Our research also expands the theoretical foundation of the
LTH by providing empirical evidence and substantial theoret-
ical support for the existence of winning tickets in the spatial
graphs of ASTGNNs. The discovery of a pre-determined win-
ning ticket is a significant stride in the application of the LTH,
as it demonstrates that such efficient sub-networks can be
identified without resorting to the computationally intensivemethods traditionally employed. This advance not only reaf-
firms the LTH within the domain of graph neural networks,
but also paves the way for its practical implementation in
scenarios where computational resources are limited. By
circumventing the need for iterative training and pruning,
our approach enhances the feasibility of adopting the LTH
in real-world settings, where efficiency and scalability are
critical.
â€¢We trained two representative ASTGNNs (AGCRN & Graph
Wavenet) with our pre-identified GWTs on five of the largest
known spatial-temporal datasets. The performance of the
ASTGNNs with the GWTs can match or even surpass that
of training with the full spatial graph, and its training and
inference costs are drastically smaller. This provides empir-
ical evidence for the existence of winning graph tickets in
ASTGNNs, demonstrating that the GWTs identified are sta-
ble winning tickets of the spatial graphs within ASTGNNs,
highlighting their scalability and superiority.
2 RELATED WORK
2.1 Spatial-Temporal Graph Neural Networks
The analysis of spatial-temporal data necessitates an understand-
ing of dynamic interactions within time-varying signals across
spatial domains[ 26,27]. Spatial-Temporal Graph Neural Networks
(STGNNs) are proficient in uncovering latent patterns in these
graph-structured data [ 28]. A key characteristic of STGNNs is
their capability to model spatial dependencies among nodes, effec-
tively learning adjacency matrices. Depending on their approach
to constructing these matrices, STGNNs can be categorized into
pre-defined and self-learned methods.
Pre-defined STGNNs typically employ prior knowledge to con-
struct graphs. For example, ASTGNN [ 13] and STGCN [ 33] uti-
lize road network structures for graph creation. However, these
pre-defined graphs encounter limitations due to their reliance on
extensive domain knowledge and the inherent quality of the graph
data. Given the implicit and complex nature of spatial-temporal re-
lationships, self-learned methods for graph generation have gained
prominence. These methods introduce innovative techniques to
capture complex spatial-temporal dependencies, thereby offering
significant advantages over traditional pre-defined models.
Self-learned STGNNs can be further divided into two primary
categories: feature-based and randomly initialized methods. Feature-
based approaches, such as PDFormer [ 15] and DG [ 21], construct
dynamic graphs from time-variant inputs, enhancing the accuracy
of the model. On the other hand, randomly initialized STGNNs,
also known as Adaptive Spatial-Temporal Graph Neural Networks
(ASTGNNs), facilitate adaptive graph generation through randomly
initialized, learnable node embeddings. Graph WaveNet [ 29] intro-
duced an Adaptive Graph Convolutional Network (AGCN) layer to
learn a normalized adaptive adjacency matrix. AGCRN [ 3] further
developed this concept with a Node Adaptive Parameter Learning
enhanced AGCN (NAPL-AGCN) to discern node-specific patterns.
Owing to its remarkable performance, the NAPL-AGCN model has
been incorporated into various recent models [8, 9, 16].
Despite the enhanced performance of ASTGNNs, they are bur-
dened with considerable computational overhead. This is primarily
 
702Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
due to two factors: i)the process of learning an adaptive adjacency
matrix necessitates calculating the edge weight between each pair of
nodes, and ii)the aggregation phase of these networks is inherently
computationally intensive. Our research is centered on identify-
ing the graph winning ticket â€”a concept derived from the Lottery
Ticket Hypothesisâ€”in order to accelerate training and inference
in ASTGNNs. This approach is particularly relevant for handling
large-scale spatial-temporal data, where efficiency is crucial.
2.2 Lottery Ticket Hypothesis.
The Lottery Ticket Hypothesis (LTH) suggests that within large
neural networks, there exist smaller sub-networks (termed "win-
ning tickets") that, when trained in isolation from the start, can
reach a similar performance level as the original network in a com-
parable number of iterations. [ 12] This finding has attracted lots
of research attention as it implies the potential of training a much
smaller network to reach the accuracy of a dense, much larger net-
work without going through the time and cost-consuming pipeline
of fully training the dense network, pruning and then retraining
it to restore the accuracy. The "Early Bird Lottery Ticket" concept
builds on the original LTH. It suggests that winning tickets can be
identified very early in the training process, much earlier than what
was originally proposed in LTH. This finding could further optimize
the training of neural networks by allowing significant pruning
and resource reduction very early in the training phase.[ 7,31]. Fur-
ther, [ 6] generalised LTH to GNNs by iteratively applying UGS
to identify graph lottery tickets. GEBT discovers the existence of
graph early-bird tickets [ 32]. DGLT generalizes Dual Lottery Ticket
Hypothesis (DLTH) to the graph to address information loss and
aggregation failure issues caused by sampling-based GNN pruning
algorithms [ 25]. However, the pruned GNNs are still hard to gener-
alize to unseen graphs [ 23]. RGLT is proposed to find more robust
and generalisable GLT to tackle this issue [24].
For extremely large models and graphs, identifying graph win-
ning tickets typically necessitates a resource-intensive process in-
volving training the network, followed by pruning and retraining.
However, our methodology significantly streamlines the deploy-
ment of ASTGNNs. It achieves this by obviating the requirement
for exhaustive cycles of training, pruning, and retraining.
3 PRELIMINARIES
3.1 Notations and Problem Definition
Frequently used notations are summarized in Table 6. Following the
conventions in spatial-temporal graph neural network researches [ 2,
14,30,33], we denote the spatial-temporal data as a sequence of
frames:{X1,X2,..., Xğ‘¡,...}, where a single frame Xğ‘¡âˆˆRğ‘Ã—ğ·is
theğ·-dimensional data collated from ğ‘different locations at time
ğ‘¡. For a chosen task time ğœ, we aim to learn a function mapping the
ğ‘‡ğ‘–ğ‘›historical observations into the future observations in the next
ğ‘‡ğ‘œğ‘¢ğ‘¡timesteps:
X(ğœ+1):(ğœ+ğ‘‡ğ‘œğ‘¢ğ‘¡)â† âˆ’F( X(ğœâˆ’ğ‘‡ğ‘–ğ‘›+1):ğœ) (1)
3.2 GAT vs. AGCN
Graph Attention Network. Given an undirected graph G=
{V,E},Vis the set of nodes, EandX={ğ‘¥ğ‘¢}ğ‘
ğ‘¢=1âˆˆRğ‘Ã—ğ·isTable 1: Summary of Notations
Symbol Description
G Undirected graph
ğ‘ Number of nodes in the graph
Xğ‘¡Feature matrix at time step ğ‘¡
ğ¸ Learnable node embedding matrix
ğ‘‘ Node embedding dimension
V Set of nodes in a graph
E Set of edges in a graph
Kğ‘ Complete graph with ğ‘nodes
T Spanning tree
Tâ˜…Star Topology Spanning Tree
ğ‘’ğ‘ Node embedding vector of the central node
Î˜ Model parameters
A Adjacency matrix
GAT Graph Attention Network function
Zğ‘¡Output of the model at time step ğ‘¡
the corresponding set of edges and node features, respectively,
whereğ‘=|V|is the number of nodes, ğ·is the feature dimension.
The adjacent matrix can be denoted as A=[Ağ‘¢ğ‘£], where Ağ‘¢ğ‘£=1
if there is an edge (ğ‘¢,ğ‘£)âˆˆE andAğ‘¢ğ‘£=0otherwise. To account
for the importance of neighbor nodes in learning graph structure,
GAT integrates the attention mechanism into the node aggregation
operation as:
ğ‘§ğ‘¢=âˆ‘ï¸
ğ‘£âˆˆNğ‘¢Ağ‘¢ğ‘£ğ‘¥ğ‘£Î˜,
Ağ‘¢ğ‘£=exp(LeakyReLU(ğ‘ ğ‘¢ğ‘£))Ã
ğ‘˜âˆˆNğ‘–exp(LeakyReLU(ğ‘ ğ‘¢ğ‘˜)),ğ‘ ğ‘¢ğ‘£=ğ‘(ğ‘¥ğ‘¢,ğ‘¥ğ‘£).(2)
Here,Î˜âˆˆRğ·Ã—ğ·â€²is the weight matrix, ğ‘(Â·,Â·)is the function of
computing attention scores. To simplify, we abbreviate GAT as:
Z=AXÎ˜,
A=GAT(G,X),(3)
where ZâˆˆRğ‘Ã—ğ·â€²,GAT(Â·)is the graph attention function.
Adaptive Graph Convolution Network. Adaptive Graph Con-
volutional Network (AGCN) facilitates adaptive learning of graph
structures through randomly initialized learnable matrices. This
approach lays the groundwork for the evolution of Adaptive Spatial-
Temporal Graph Neural Networks (ASTGNNs). Among the notable
ASTGNN models are Graph WaveNet and AGCRN. Within the
Graph WaveNet framework, the AGCN is characterized as follows.
Zğ‘¡=AXğ‘¡Î˜,
A=Softmax(ReLU(ğ¸1ğ¸âŠ¤
2),(4)
whereğ¸1âˆˆRğ‘Ã—ğ‘‘andğ¸2âˆˆRğ‘Ã—ğ‘‘are the source node embeddings
and target node embeddings, respectively. While in AGCRN, AGCN
is defined as:
Zğ‘¡=AXğ‘¡Î˜,
A=Softmax(ReLU(ğ¸ğ¸âŠ¤)),(5)
whereğ¸âˆˆRğ‘Ã—ğ‘‘is the node embeddings. Eq.(4) and Eq.(5) are ex-
tremely similar in form, with Eq.(5) being more concise. Therefore,
 
703KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He
the general form of AGCN referred to Eq.(5) in this paper. Upon
close observation of Eq.(5), it is not difficult to find that AGCN can
be reformulated as the following mathematical expression likes
GAT:
ğ‘§ğ‘¡
ğ‘¢=âˆ‘ï¸
ğ‘£âˆˆVAğ‘¢ğ‘£ğ‘¥ğ‘¡
ğ‘£Î˜,
Ağ‘¢ğ‘£=exp(ReLU(ğ‘ ğ‘¢ğ‘£))Ã
ğ‘˜âˆˆVexp(ReLU(ğ‘ ğ‘¢ğ‘˜)),ğ‘ ğ‘¢ğ‘£=ğ‘’ğ‘¢ğ‘’âŠ¤
ğ‘£,(6)
which is similar to Eq.(2). Thus, AGCN can be considered a special
kind of graph attention network on a complete graph with self-
loops. We further abbreviate AGCN as the following equations:
ğ‘ğ‘¡=AXğ‘¡Î˜,
A=GAT(ËœKğ‘,ğ¸),(7)
where ËœKğ‘is theğ‘-order complete graph Kğ‘with self-loops. As
the diameter ofKğ‘is 1, AGCN facilitates the aggregation of in-
formation from all nodes to each individual node within ËœKğ‘. This
characteristic significantly enhances the networkâ€™s capability to
model global spatial dependencies, culminating in its state-of-the-
art performance in relevant tasks, as documented in [3, 8].
The model utilizing multi-layers of AGCN for modeling spatial
dependencies is designated as ASTGNN (Adaptive Spatio-Temporal
Graph Neural Network). The spatial-temporal forecasting problem
when addressed using ASTGNN is mathematically expressed as:
X(ğœ+1):(ğœ+ğ‘‡ğ‘œğ‘¢ğ‘¡)â† âˆ’F( X(ğœâˆ’ğ‘‡ğ‘–ğ‘›+1):ğœ;ğœƒ,ËœKğ‘), (8)
In this formulation, Frepresents the forecasting function of AST-
GNN parameterised by ğœƒ, which predicts future values X(ğœ+1):(ğœ+ğ‘‡ğ‘œğ‘¢ğ‘¡)
based on the input sequence X(ğœâˆ’ğ‘‡ğ‘–ğ‘›+1):ğœand the structural infor-
mation encoded in the graph ËœKğ‘.
However, a notable limitation arises during the training phase.
The computational complexity associated with calculating adja-
cency matrices and executing graph convolution operations on
complete graphs is of O(ğ‘2). This significant computational de-
mand imposes a constraint on the modelâ€™s scalability, particularly in
scenarios involving large spatial-temporal datasets, where reducing
computational complexity is crucial for practical applicability.
3.3 Graph Tickets Hypothesis
The Graph Tickets Hypothesis represents an extension of the orig-
inal Lottery Tickets Hypothesis, initially introduced by UGS [ 6].
UGS demonstrated that GWT (i.e., compact sub-graphs) are present
within randomly initialized GNNs, which can be retrained to achieve
performance comparable to, or even surpassing, that of GNNs
trained on the original full graph. This finding underscores the
potential for efficiency improvements in GNN training method-
ologies. However, designing a graph pruning method to identify
GWTs in ASTGNNs proves to be a nontrivial task. The state-of-the
art, AGS demonstrates that spatial graphs in ASTGNNs can un-
dergo sparsification up to 99.5% with no detrimental impact on test
accuracy [ 10]. Nonetheless, this robustness to sparsification does
not hold uniformly; when ASTGNNs, sparsified beyond 99%, are
reinitialized and retrained on the same dataset, there is a notable
and consistent decline in accuracy. This dichotomy underscores the
nuanced complexity inherent in finding winning graph tickets in
ASTGNNs and calls for further investigation.4 METHOD
4.1 Pre-Identifying the Graph Winning Ticket
Our objective is to identify a sparse subgraph of the spatial graph
pre-training and to train ASTGNNs efficiently on this subgraph
without compromising performance. An ASTGNN with a spatial
graph Ë†G, equipped with ğ¾-layer AGCN can be formulated as:
Zğ‘¡=Ë†AÂ·Â·Â·(Ë†A(Ë†AXğ‘¡Î˜1)Î˜2)Â·Â·Â·Î˜ğ¾|                               {z                               },
Ë†A=GAT(Ë†G,ğ¸),(9)
where Ë†Gis a sparse subgraph of Kğ‘. However, employing Ë†Galone
does not ensure the capability to model global spatial dependencies.
To maintain the global spatial modeling ability of AGCN and to
train ASTGNNs efficiently, we argue that it is essential to use a
spanning treeTofËœKğ‘, instead of ËœKğ‘, with a sufficient ğ¾:
Zğ‘¡=ËœAÂ·Â·Â·(ËœA(ËœA
|       {z       }
ğ¾Xğ‘¡Î˜1)Î˜2)Â·Â·Â·Î˜ğ¾,
ËœA=GAT(T,ğ¸),(10)
To mitigate the risk of excessive parameters and overfitting due
to a high number of network layers, it is crucial to minimize ğ‘Ÿ
as much as possible. In light of this, we found that star topology
spanning trees (with diameter ğ‘Ÿ=2) can function as GWTs. We
make two notes on the star topology spanning tree:
(1)Motivation: GAT is a message-passing network, and AGCN
can be viewed as modeling a fully connected GAT, allows any
node to communicate globally. A spanning tree Tğ‘, as the
minimum connected graph of complete graph, can achieve
message passing to all other nodes in the graph by stacking
ğ‘˜GAT layers, where k is the diameter of Tğ‘. Our goal is to
minimize the computational complexity of ASTGNNs, so itâ€™s
necessary to minimize ğ‘˜. Clearly,Tğ‘with a diameter of 1
doesnâ€™t exist. So we start with ğ‘˜=2to examine the existence
of spanning trees and we found that there exist Tğ‘with a
diameter of 2, uniquely forming a star topology. Weâ€™ll detail
this motivation in final version.
(2)Theoretical Analysis: Based on spectral graph theory, if
one graph is a ğœ-ğ‘ğ‘ğ‘ğ‘Ÿğ‘œğ‘¥ğ‘–ğ‘šğ‘ğ‘¡ğ‘–ğ‘œğ‘› of another, they have similar
eigensystems and properties. We can prove that Tğ‘is an N-
approximation ofKğ‘. SoKğ‘andTğ‘have similar properties,
allowingTğ‘with fewer edges to effectively replace Kğ‘for
learning good representations. The complete proof can be
found in Appendix A.2.
Hypothesis 1. Given an N-order complete spatial graph Kğ‘of an
ASTGNN, we investigate an associated star spanning tree: Tâ˜…=
V,Eâ˜…	, whereEâ˜…={(ğ‘¢ğ‘,ğ‘£) |ğ‘£âˆˆV\{ğ‘¢ğ‘}}, withğ‘¢ğ‘desig-
nated as the central node, ğ‘£designated as the leaf node. All such
Tâ˜…are Graph Winning Ticket (GWT) for the spatial graph of the
corresponding ASTGNN.
To ensure the existence of the associated star spanning tree, we
have the following proposition:
Proposition 1. In an N-order complete graph Kğ‘, there exists
a graphTsuch thatTis a spanning tree of Kğ‘and the diameter
 
704Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) A complete graph Kğ‘.
(b) A spanning tree of Kğ‘with
a diameter of 2, identified as a
Graph Winning Ticket
Figure 1: A complete graph and a star spanning tree with a
pre-specified node number.
ofTis 2, and the topology of Tunequivocally satisfies definition of
star spanning tree in Hypothesis 1.
The proof of Proposition 1 is given in Appendix A.1. To verify
Hypothesis 1, we provide empirical evidence demonstrating that
suchTâ˜…are GWTs for their corresponding ASTGNNS in Sec. 5.
Sparsity ofTâ˜….The sparsity of the Tâ˜…is quantified as 1âˆ’2
ğ‘.
This represents a significant level of sparsity, particularly as the
number of nodes ğ‘increases. In such cases, the sparsity becomes
increasingly pronounced, highlighting the efficiency of these GWTs
in large-scale spatial-temporal datasets.
4.2 Further Enhancements
In this section, we discuss two additional enhancements made
to training ASTGNNs within Tâ˜…. In the context of an ASTGNN
F(Â·;ğœƒ,ËœKğ‘), which comprises multiple AGCN layers, a straightfor-
ward approach might involve substituting ËœKğ‘withTâ˜…to facilitate
rapid training. However, this seemingly intuitive method encoun-
ters two primary issues:
â€¢Efficiency: The method lacks optimal efficiency in training.
â€¢Central Node Selection: The random selection of the central
nodeğ‘£ğ‘could lead to sub-optimal performance.
Efficiency. The computational complexity of Graph Neural Net-
work (GNN) training and inference encompasses two primary com-
ponents: Deep Neural Network (DNN) computation and Graph
convolution operation. Considering the relaxation of the graphâ€™s
diameter from 1 to 2, an ASTGNN necessitates a minimum of two
layers of AGCN to maintain comprehensive spatial-temporal com-
munication:
Zğ‘¡=Aâ˜…(Aâ˜…XtÎ˜1)Î˜2,
Aâ˜…=GAT(Tâ˜…,ğ¸),(11)
To ameliorate the computational complexity of Equation (11)in
terms of DNN computation, we introduce a streamlined formulation
by excluding the parameter Î˜2. This modification facilitates 2-hop
message passing within a singular AGCN layer, thereby providing
the ability to model the global spatial dependencies:
Zğ‘¡=Aâ˜…(Aâ˜…Xğ‘¡Î˜),
Aâ˜…=GAT(Tâ˜…,ğ¸),(12)
(a) Message passing path of
central nodeâ€™s feature.
(b) Message passing path of
leaf nodeâ€™s feature.
Figure 2: 2-hop message passing path of Tâ˜…with pre-
specified node numbers. The red node is the central node ğ‘¢ğ‘£
and the gray nodes are leaf nodes ğ‘£âˆˆ{V\{ğ‘¢ğ‘}}.
From the perspective of graph convolution operations, (11) ex-
hibits informational redundancy in its message-passing process.
The message-passing trajectory delineated in 2 reveals that the
pathsğ‘¢ğ‘â†’ğ‘£andğ‘£â†’ğ‘¢ğ‘are executed twice, engendering su-
perfluous aggregation. Such redundancy could potentially impede
the modelâ€™s efficiency. We therefore perform message passing as
illustrated in Figure 2b using two directed graphs, denoted asâ†âˆ’âˆ’
Tâ˜…
andâˆ’âˆ’â†’
Tâ˜…. This process can be expressed by the following equations:
Zğ‘¡=Uâ˜…(Lâ˜…Xğ‘¡Î˜),
Lâ˜…=GAT(â†âˆ’âˆ’
Tâ˜…,ğ¸),
Uâ˜…=GAT(âˆ’âˆ’â†’
Tâ˜…,ğ¸),(13)
here,â†âˆ’âˆ’
Tâ˜…={V,â† âˆ’E}, whereâ† âˆ’E=<ğ‘£,ğ‘¢ğ‘>|ğ‘£âˆˆV\{ğ‘¢ğ‘}.âˆ’âˆ’â†’
Tâ˜…=
{V,âˆ’ â†’E}, whereâˆ’ â†’E=<ğ‘¢ğ‘,ğ‘£>|ğ‘£âˆˆV\{ğ‘¢ğ‘}. The computational
complexity of graph convolution operations experiences a notable
reduction in Eq.(13). To elaborate, the complexity in Eq.(11) is
O(2ğ‘), whereas it is diminished to O(ğ‘)in Eq.(13).
Despite this enhancement, Eq.(11) still faces limitations in terms
of hardware compatibility. At the hardware level, graph convolu-
tion operations are intrinsically linked to the sparse and irregular
nature of graph structures. This characteristic might not be com-
patible with certain hardware architectures, leading to an increased
frequency of random memory accesses and limited opportunities
for data reuse. Consequently, this can result in significantly higher
inference latency for graph convolutions when compared to other
neural network architectures. Then, we introduce a self-loop to the
central node ğ‘¢ğ‘in bothâ†âˆ’âˆ’
Tâ˜…andâˆ’âˆ’â†’
Tâ˜…. Consequently, we reformulate
Eq.(13) to a network namely GWT-AGCN as follows:
Zğ‘¡=Softmax ReLU ğ¸ğ‘’âŠ¤
ğ‘Softmax ReLU ğ‘’ğ‘ğ¸âŠ¤Xğ‘¡Î˜ (14)
Here,ğ‘’ğ‘âˆˆR1Ã—ğ‘‘represents the node embedding vector of node ğ‘¢ğ‘.
This GWT-AGCN layer can serve as an alternative to the AGCN
layer in constructing ASTGNNs.
The advantages of Eq.(14) are manifold: The equation solely
comprises matrix multiplication and standard activation functions,
thereby enhancing its compatibility with hardware. In contrast to
 
705KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He
Table 2: Spatial-temporal datasets statistics.
Dataset #Nodes Time Range
PEMS07 883 05/01/2017-08/06/2017
CA 8,600 01/01/2017-12/31/2021
GLA 3,834 01/01/2019-12/31/2019
GBA 2,352 01/01/2019-12/31/2019
SD 716 01/01/2019-12/31/2019
Eq.(13), the complexity increased by only O(2), a change that can
be considered inconsequential.
Central Node Selection. Owing to the non-uniqueness of Tâ˜…
in the complete graph Kğ‘, directly employing Tâ˜…for training
ASTGNNs presents the challenge of central node selection. Viewed
through the lens of AGCN, the random selection of a node ğ‘¢ğ‘from
the vertex setVis analogous to initializing the node embedding
ğ‘’ğ‘randomly. This approach, however, might introduce bias in the
construction of the adaptive graph. To ensure that the selected
central node embedding vector ğ‘’ğ‘is positioned at the physical
center of the node embedding space ğ¸, we opt for a setting where
ğ‘’ğ‘=Mean(ğ¸), a technique we refer to as averaged initialization.
We empirically show that such operation provides better on the
prediction accuracy (see Sec. 5.3).
5 EVALUATION
5.1 Experimental Settings
In this section, we conduct extensive experiments to validate our
Hypothesis 1.
Neural Network Architecture. We evaluate the existence of
GWT on two quintessential ASTGNN architectures: AGCRN and
Graph WaveNet (GWNET). AGCRN integrates an RNN frame-
work, specifically combining AGCN layers with Gated Recurrent
Unit (GRU) layers. The AGCN layers are adept at capturing spatial
dependencies, whereas the GRU layers are employed to model the
temporal dependencies effectively. Conversely, GWNET represents
a CNN-based ASTGNN architecture. It amalgamates AGCN, GCN
layers, and dilated 1D convolution networks. Here, both GCN and
AGCN layers are instrumental in capturing spatial dependencies,
whilst the dilated 1D convolution networks are utilized to model
the temporal dependencies. AGCRNâ˜…and GWENTâ˜…respectively
represent AGCRN and GWNET trained within Tâ˜…, while AGCRNâˆ—
and GWNETâˆ—represent AGCRN and GWNET with in GWT-AGCN
described in Sec. 4.2, respectively.
Datasets. We conduct experiments on five of the largest known
spatial-temporal datasets. These include PEMS07, a dataset exten-
sively studied [ 5], along with SD, GBA, GLA, and CA, which were
recently introduced in the LargeST dataset [ 20]. Table 2 summarizes
the specifications of the datasets used in our experiments. These
datasets were partitioned in a 6:2:2 ratio for training, validation,
and testing, respectively. The traffic flow data in PEMS07 is aggre-
gated into 5-minute intervals, whereas for SD, GBA, GLA, and CA,
the aggregation occurs in 15-minute intervals. We implemented
a 12-sequence-to-12-sequence forecast, adhering to the standard
protocol in this research domain.Implementation Details. For all evaluated models, we set the
number of training iterations to 100. Other training-related config-
urations adhere to the recommended settings provided in the re-
spective code repositories. To ensure reproducibility and reliability,
experiments were conducted ten times on all datasets, except for CA
and GLA. Due to their substantially larger data scales, experiments
on CA and GLA were limited to three repetitions. These experi-
ments were performed on an NVIDIA RTX A6000 GPU, equipped
with 48 GB of memory.
Metrics. Our comprehensive evaluation encompasses the following
dimensions: i) Performance: We assess the forecasting accuracy
using three established metrics: Mean Absolute Error (MAE), Root
Mean Square Error (RMSE), and Mean Absolute Percentage Error
(MAPE), and ii) Efficiency: Model efficiency is evaluated in terms of
both training and inference wall-clock time. Additionally, the batch
size during training is reported, reflecting the modelsâ€™ capability
to manage large-scale datasets. We set a maximum batch size limit
of 64. If a model is unable to operate with this configuration, we
progressively reduce the batch size to the highest possible value
that fully utilizes the memory capacity of the A6000 GPU.
5.2 Main Results
The experimental results are organised as follows: Test accuracies
and efficiency comparisons are reported in Table 3 and Table 4,
respectively. We also compare the
We make following observations from Table 3 and Table 4:
â€¢Graph lottery tickets are existent in ASTGNNs. Specifically,
AGCRNâ‹†and GWENTâ‹†demonstrate performance that
is comparable or even superior across all datasets. These
findings indicate that Tâ˜…is a stable â€™graph winning lottery
ticketâ€™ within ASTGNNs when evaluated on datasets such
as PEMS07, SD, GBA, GLA, and CA.
â€¢Our proposed approach is demonstrably scalable. The CA
dataset presents substantial challenges to existing ASTGNNs,
evidenced by AGCRNâ€™s inability to operate on it. However,
the proposed approach facilitates the training of AGCRN
on the CA dataset. This capability not only underscores the
scalability of the proposed approach but also its superiority.
Conventional pruning-based methods necessitate starting
the training process with a complete graph. This approach
often leads to their inadequacy in identifying graph lottery
tickets in large-scale datasets like CA, a limitation that the
proposed approach effectively overcomes.
â€¢GWT-AGCN has the potential to be an ideal substitute for
AGCN. In comparison, ASTGNN within GWT-AGCN demon-
strates enhanced overall performance, particularly in terms
of speed, surpassing its predecessor.
â€¢GWT-AGCN significantly accelerates the training and infer-
ence of the ASTGNNs. The acceleration is more prominent
on AGCRNs against GWNETs because a larger portion of
the total computation required by GWNETs is used on their
GCN layers.
 
706Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Performance comparisons. We bold the best results. Standard deviations are suppressed for the sake of room.
Data MethodHorizon3 Horizon6 Horizon12 Average
MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
PEMS07AGCRN 19.31 31.68 8.18% 20.70 34.52 8.66% 22.74 37.94 9.71% 20.64 34.39 8.74%
AGCRNâ˜…19.36 31.56 8.11% 20.67 33.95 8.63% 22.76 37.32 9.59% 20.67 33.95 8.67%
AGCRNâˆ—19.27 31.83 8.09% 20.57 34.41 8.57% 22.63 37.97 9.39% 20.57 34.42 8.59%
GWNET 18.69 30.69 8.02% 20.26 33.37 8.56% 22.79 37.11 9.73% 20.25 33.32 8.63%
GWNETâ˜…18.64 30.61 8.01% 20.36 33.57 8.68% 22.39 36.61 9.51% 20.09 33.13 8.59%
GWNETâˆ—18.48 30.42 8.20% 19.91 32.98 8.71% 21.81 36.21 9.72% 19.80 32.84 8.62%
SDAGCRN 15.71 27.85 11.48% 18.06 31.51 13.06% 21.86 39.44 16.52% 18.09 32.01 13.28%
AGCRNâ˜…16.06 28.56 11.59% 18.32 31.65 12.41% 22.67 39.06 16.13% 18.56 31.89 13.19%
AGCRNâˆ—15.49 25.89 10.55% 18.12 30.96 12.13% 22.24 38.79 15.36% 18.13 30.92 12.41%
GWNET 15.24 25.13 9.86% 17.74 29.51 11.70% 21.56 36.82 15.13% 17.74 29.62 11.88%
GWNETâ˜…15.19 24.97 10.09% 17.39 29.01 11.82% 21.61 36.55 15.35% 17.90 29.56 13.01%
GWNETâˆ—15.24 25.07 10.71% 17.87 29.18 12.21% 21.94 36.69 15.17% 17.89 30.06 12.57%
GBAAGCRN 18.31 30.24 14.27% 21.27 34.72 16.89% 24.85 40.18 20.80% 21.01 34.25 16.90%
AGCRNâ˜…18.24 30.18 14.09% 21.27 34.37 16.77% 24.82 39.68 20.80% 20.71 33.75 15.93%
AGCRNâˆ—17.62 29.49 12.99% 20.73 34.25 15.65% 24.72 40.37 18.72% 20.51 33.57 15.33%
GWNET 17.85 29.12 13.92% 21.11 33.69 17.79% 25.58 40.19 23.48% 20.91 33.41 17.66%
GWNETâ˜…17.72 28.98 13.86% 20.95 33.47 17.81% 25.55 41.02 23.68% 20.96 34.16 17.79%
GWNETâˆ—17.76 28.97 13.53% 21.09 33.46 17.58% 26.01 40.63 24.11% 21.01 33.38 17.66%
GLAAGCRN 17.27 29.70 10.78% 20.38 34.82 12.70% 24.59 42.59 16.03% 20.25 34.84 12.87%
AGCRNâ˜…17.12 28.10 10.81% 20.42 33.02 11.77% 24.48 42.33 15.83% 20.15 32.57 12.17%
AGCRNâˆ—16.85 27.38 10.83% 20.13 32.90 11.84% 24.71 40.36 14.85% 20.04 32.72 12.03%
GWNET 17.28 27.68 10.18% 21.31 33.70 13.02% 26.99 42.51 17.64% 21.20 33.58 13.18%
GWNETâ˜…17.18 27.32 10.65% 21.0 33.00 13.29% 26.39 42.01 17.05% 21.10 32.97 13.34%
GWNETâˆ—17.16 27.15 10.87% 21.23 33.17 13.12% 26.41 41.95 17.10% 21.08 33.03 13.13%
CAAGCRN â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“
AGCRNâ˜…16.56 26.88 11.93% 20.13 31.87 15.25% 24.59 39.65 19.86% 19.77 31.79 15.21%
AGCRNâˆ—16.44 26.97 12.05% 19.90 32.20 15.15% 24.78 39.55 19.90% 19.78 31.98 15.19%
GWNET 17.14 27.81 12.62% 21.68 34.16 17.14% 28.58 44.13 24.24% 21.72 34.20 17.40%
GWNETâ˜…17.19 28.16 10.09% 22.03 24.66 13.62% 27.05 40.83 21.93% 21.38 33.16 16.58%
GWNETâˆ—17.44 28.08 10.49% 21.23 24.06 13.12% 27.08 41.39 22.33% 21.08 32.90 16.12%
Table 4: Efficiency comparisons. BS: batch size set during training. Train: training time (in seconds) per epoch. Infer: inference
time (in seconds) on the validation set. Total: total training time (in hours).
Metho
dPEMS07 SD GBA GLA CA
BS T
rain Infer T
otal BS T
rain Infer T
otal BS T
rain Infer T
otal BS T
rain Infer T
otal BS T
rain Infer T
otal
A
GCRN 64
131 22 4 64
92 15 3 64
536 83 17 45
1413 245 46 â€“
â€“ â€“ â€“
AGCRNâ˜…64
86 14 3 64
78 13 3 64
222 33 7 45
430 69 14 11
1513 196 47
AGCRNâˆ—64
56 10 2 64
55 10 2 64
153 24 4 64
286 46 9 16
1021 132 38
GWNet 64
161 21 5 64
82 12 3 64
483 66 15 64
1028 139 32 44
4105 548 113
GWNetâ˜…64
138 15 4 64
76 12 3 64
380 55 12 64
818 112 26 50
2583 319 81
GWNetâˆ—64
116 13 4 64
71 11 2 64
327 47 1 64
700 96 19 50
2382 295 74
5.3 Analysis
Convergence. Figure 3 illustrates the training loss and test Mean
Absolute Error (MAE) curves of the original AGCRN and AGCRNâˆ—
under identical hyper-parameter settings on the PEMS07 dataset.
Similarly, Figure 4 presents these curves for the same models on
the SD dataset. Pre-determined GWT ensures convergence that is
as consistent, rapid, and stable as a complete graph model. This
feature is particularly advantageous for training on large-scale
spatial-temporal data, as it significantly reduces computational
overhead without compromising the quality of convergence.Additionally, the convergence behavior of AGCRNâˆ—demonstrates
its robustness in capturing complex spatial-temporal dependencies.
This attribute is crucial for reliable forecasting in dynamic systems,
such as traffic networks, where understanding intricate patterns is
key to accuracy.
AGCRNâˆ—&GWNETâˆ—vs. SOTAs. AGCRN and GWNET, as rep-
resentative ASTGNNs introduced between 2019 and 2020, are of
significant interest in our study. Our objective is to evaluate the per-
formance of AGCRN and GWNET, particularly when trained using
GWT, in comparison with the current state-of-the-art STGNNs. To
 
707KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He
(a) Loss Curves
 (b) MAE Curves
Figure 3: Training loss (a) and testing MAE (b) curve of origi-
nal AGCRN and AGCRNâˆ—trained on PEMS07, respectively.
(a) Loss Curves
 (b) MAE Curves
Figure 4: Training loss (a) and testing MAE (b) curve of origi-
nal AGCRN and AGCRNâˆ—trained on SD, respectively.
this end, we selected five advanced STGNNs as baselines: DGCRN
[19], MegaCRN [ 17], STGODE [ 11], D2STGNN [ 22], and DSTAGNN
[18]. These models reflect the most recent trends in the field. DGCRN
and MegaCRN, seen as variations of AGCRN, epitomize the latest
developments in ASTGNN. STGODE employs neural ordinary dif-
ferential equations innovatively to effectively model the continuous
dynamics of traffic signals. In contrast, DSTAGNN and D2STGNN
focus on capturing the dynamic correlations among sensors in traf-
fic networks. From the results presented in Table 6, we make the
following observations: i)ASTGNNs such as DGCRN and MegaCRN
consistently exhibit strong performance across most benchmarks.
However, their intricate model designs limit scalability, particularly
in larger datasets like GLA and CA. ii)Methods introduced four
years ago, such as AGCRN when trained within GWT-AGCN (i.e.,
AGCRNâˆ—), continue to demonstrate robust performance across var-
ious evaluated datasets. Remarkably, they achieve state-of-the-art
performance on specific datasets including GBA, GLA, and CA.
These findings suggest that GWT-AGCN could play a crucial role
in the development of scalable ASTGNNs for future research.
Impact of averaged initialization of node embedding ğ‘’ğ‘.In
this study, we employ AGCRNâˆ—as a benchmark to evaluate the
impact of averaged initialization of node embedding ğ‘’ğ‘. Table 7
presents a comparative analysis between AGCRNâˆ—andAGCRNâˆ—,
i.e., random initialization of ğ‘’ğ‘. The results indicate that AGCRNâˆ—
consistently outperforms AGCRNâˆ—in terms of forecasting accuracy.
This finding underscores the significance of deliberate initialization
Figure 5: Testing accuracies, measured in MAE, for AGCRN
on the PEMS07, SD, GBA, GLA, and CA datasets, with a per-
turbation ratio of ğ‘ranging from 0% to 50%.
strategies for ğ‘’ğ‘in enhancing the predictive performance of the
model.
Comparison with AGS. We compared our method with AGS, the
state-of-the-art approach, to validate its superiority. The perfor-
mance of AGS with a sparsity of 99.7% is reported on PEMS07, SD,
GBA and GLA, while the sparsity of our method is 99.8%/99.7%/
99.99%/99.99% for PEMS07/SD/GBA/GLA. Since AGS does not pro-
vide an implementation on GWNet, we only report the results for
AGCRN. The lack of CA results is due to AGS encountering out-of-
memory (OOM) issues. From Table 5, we can see that our method
significantly outperforms AGS.
PerturbedTâ˜….We attribute the effectiveness of Tâ˜…to its robust
connectivity, which is crucial for ASTGNNâ€™s ability to model global
spatial dependencies. To further validate this perspective, we intro-
duce a perturbation process illustrated in Figure 6 to Tâ˜…, resulting
inËœTâ˜…, according to the following steps:
(1)For a givenTâ˜…withğ‘nodes, we randomly remove ğ‘€edges
between the center node and the leaf node. The perturbation
ratioğ‘of the removed edges is defined asğ‘€
ğ‘âˆ’1.
(2)Subsequently, we randomly add ğ‘€new edges, connecting
previously isolated nodes.
These steps intentionally disrupt the original connectivity in Tâ˜…,
while ensuring that the overall sparsity of the network remains con-
stant. Figure 5 show the MAE curves of AGCRN trained via ËœTâ˜…of
a ratioğ‘from 0 to 50%. We can see that as ğ‘increases, the accuracy
of the model decreases. This indicates the importance of preserving
the graphâ€™s connectivity to model global spatial dependencies
6 CONCLUSION
This paper introduces a novel approach in the realm of ASTGNNs
by leveraging the GWT concept, inspired by the Lottery Ticket
Hypothesis. This method markedly reduces the computational com-
plexity of ASTGNNs, transitioning from a quadratic to a linear scale,
thereby streamlining their deployment. Our innovative strategy of
adopting a star topology for GWT, without necessitating exhaustive
 
708Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 5: Comparative experimental results between AGS and our method within AGCRN.
PEMS07 SD GBA GLAMethodsMAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
AGCRN(AGS)@99.7% 24.37 39.33 9.57% 21.83 37.10 15.03% 23.61 38.32 17.19% 22.23 35.55 13.39%
AGCRNâ˜…20.67 33.95 8.67% 18.56 31.89 13.19% 20.71 33.75 15.95% 20.15 32.57 12.17%
AGCRNâˆ—20.57 34.42 8.59% 18.13 30.92 12.41% 20.51 33.57 15.33% 20.04 32.72 12.03%
Table 6: AGCRNâˆ—and GWNETâˆ—compared with current state-
of-the-art STGNNs. We report the results averaged in 12 hori-
zons. Results without standard deviations are sourced from
published papers.
Dataset Method MAE RMSE MAPE(%)
PEMS07AGCRNâˆ—20.57Â±0.11 34.42+0.05 8.59Â±0.13
GWNETâˆ—19.80Â±0.19 32.84Â±0.25 8.62Â±0.11
DGCRN 20.5Â±0.23 33.32Â±0.23 8.45Â±0.20
MegaCRN 19.86Â±0.36 32.69Â±0.36 8.62Â±0.21
STGODE 22.99 37.54 10.14
D2STGNN 20.50 33.08 8.42
DSTAGNN 20.50 34.51 9.01
SDAGCRNâˆ—18.13Â±0.30 30.92+0.32 12.41Â±0.10
GWNETâˆ—17.89Â±0.18 30.06+0.19 12.57Â±0.21
DGCRN 18.02 30.09 12.07
MegaCRN 17.76Â±0.21 29.62+0.17 12.69Â±0.13
STGODE 21.79 35.37 13.22
D2STGNN 17.85 29.51 11.54
DSTAGNN 21.82 34.68 14.40
GBAAGCRNâˆ—20.51Â±0.13 33.57+0.27 15.3Â±0.17
GWNETâˆ—21.01Â±0.07 33.38Â±0.30 17.66Â±0.15
DGCRN 20.91 33.83 16.88
MegaCRN 20.69Â±0.17 33.61Â±0.14 15.52Â±0.09
STGODE 21.79 35.37 18.26
D2STGNN 20.71 33.65 15.04
DSTAGNN 23.82 37.29 20.16
GLA AGCRNâˆ—20.04Â±0.07 32.72Â±0.19 12.03Â±0.18
GWNETâˆ—21.08Â±0.05 33.03Â±0.11 13.13Â±0.20
STGODE 21.49 36.14 13.72%
DSTAGNN 24.13 38.15 15.07%
CA AGCRNâˆ—19.78Â±0.08 31.98+0.29 15.59Â±0.43
GWNETâˆ—21.08Â±0.05 33.03Â±0.11 13.13Â±0.20
STGODE 20.77 36.60 16.80
Table 7: Ablation study of averaged initialization of ğ‘’ğ‘. We
report the results averaged in 12 horizons.
Dataset Method MAE RMSE MAPE(%)
PEMS07AGCRNâˆ—20.57Â±0.11 34.42Â±0.05 8.59Â±0.13
AGCRNâˆ—20.93Â±0.17 35.17Â±0.12 8.87Â±0.26
SDAGCRNâˆ—18.13Â±0.30 30.92+0.32 12.41Â±0.10
AGCRNâˆ—18.63Â±0.28 32.19Â±0.29 12.67Â±0.24
GBAAGCRNâˆ—20.51Â±0.13 33.57+0.27 15.3Â±0.17
AGCRNâˆ—21.16Â±0.37 34.28+0.30 16.12Â±0.35
GLAAGCRNâˆ—20.04Â±0.07 32.72+0.19 12.03Â±0.18
AGCRNâˆ—20.23Â±0.09 33.16Â±0.20 12.32Â±0.17
CAAGCRNâˆ—19.78Â±0.08 31.98+0.29 15.59Â±0.43
AGCRNâˆ—20.01+0.06 32.64Â±0.19 16.73Â±0.56
Figure 6: Perturbation process with a perturbation ratio of ğ‘,
with a pre-specified node number.
training cycles, maintains high model performance with signifi-
cantly lower computational demands. Empirical validations across
various datasets underscore our methodâ€™s capability to achieve
performance on par with full models, but at a fraction of the compu-
tational cost. This breakthrough not only underscores the existence
of efficient sub-networks of the spatial graphs within ASTGNNs,
but also extends the applicability of the Lottery Ticket Hypothe-
sis to scenarios where resources are limited. Consequently, this
work represents a significant leap forward in the optimization and
practical application of graph neural networks, particularly in envi-
ronments where computational resources are constrained. In the
future, we will develop new STGNNs based on pre-determined
GWT, aimed at long-term spatial-temporal forecasting.
 
709KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He
REFERENCES
[1]Daniel A.Spielman. [n. d.]. Spectral and Algebraic Graph Theory. Online. http:
//cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf
[2]Lei Bai, Lina Yao, Salil S. Kanhere, Xianzhi Wang, and Quan Z. Sheng. 2019.
STG2Seq: Spatial-Temporal Graph to Sequence Model for Multi-step Passenger
Demand Forecasting. In Proceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019,
Sarit Kraus (Ed.). ijcai.org, 1981â€“1987.
[3]Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive graph
convolutional recurrent network for traffic forecasting. In Advances in Neural
Information Processing Systems. 17804â€“17815.
[4]Joshua Batson et al .2013. Spectral sparsification of graphs: theory and algorithms.
Commun. ACM 56, 8 (2013), 87â€“94.
[5]Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng Jia.
2001. Freeway Performance Measurement System: Mining Loop Detector Data.
Transportation Research Record 1748, 1 (2001), 96â€“102. https://doi.org/10.3141/
1748-12
[6]Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang.
2021. A Unified Lottery Ticket Hypothesis for Graph Neural Networks. In
Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139),
Marina Meila and Tong Zhang (Eds.). PMLR, 1695â€“1706.
[7]Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and
Jingjing Liu. 2021. EarlyBERT: Efficient BERT Training via Early-bird Lottery
Tickets. In Proceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers). 2195â€“2207.
[8]Yuzhou Chen, Ignacio Segovia-Dominguez, Baris Coskunuzer, and Yulia R. Gel.
2022. TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge
Representation with Spatio-Supra Graph Convolutional Networks for Time-Series
Forecasting. In The Tenth International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022.
[9]Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, and Noseong Park. 2022.
Graph Neural Controlled Differential Equations for Traffic Forecasting. In Thirty-
Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Confer-
ence on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth
Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual
Event, February 22 - March 1, 2022. AAAI Press, 6367â€“6374.
[10] Wenying Duan, Xiaoxi He, Zimu Zhou, Lothar Thiele, and Hong Rao. 2023.
Localised Adaptive Spatial-Temporal Graph Neural Network. In Proceedings of
the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
KDD 2023, Long Beach, CA, USA, August 6-10, 2023. ACM, 448â€“458. https:
//doi.org/10.1145/3580305.3599418
[11] Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie. 2021. Spatial-
temporal graph ode networks for traffic flow forecasting. In Proceedings of the
27th ACM SIGKDD conference on knowledge discovery & data mining. 364â€“373.
[12] Jonathan Frankle and Michael Carbin. 2019. The Lottery Ticket Hypothesis: Find-
ing Sparse, Trainable Neural Networks. In International Conference on Learning
Representations.
[13] Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, and Gao Cong. 2021.
Learning dynamics and heterogeneity of spatial-temporal graph data for traffic
forecasting. IEEE Transactions on Knowledge and Data Engineering 34, 11 (2021),
5415â€“5428.
[14] Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, and Weiyang Kong.
2020. LSGCN: Long Short-Term Traffic Prediction with Graph Convolutional
Networks. In Proceedings of the Twenty-Ninth International Joint Conference on
Artificial Intelligence, IJCAI 2020, Christian Bessiere (Ed.). ijcai.org, 2355â€“2361.
[15] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.
PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for
Traffic Flow Prediction. In Thirty-Seventh AAAI Conference on Artificial Intel-
ligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Arti-
ficial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances
in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023,
Brian Williams, Yiling Chen, and Jennifer Neville (Eds.). AAAI Press, 4365â€“4373.
https://ojs.aaai.org/index.php/AAAI/article/view/25556
[16] Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Yasumasa
Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura. 2023.
Spatio-Temporal Meta-Graph Learning for Traffic Forecasting. In Thirty-Seventh
AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on
Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium
on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC,
USA, February 7-14, 2023, Brian Williams, Yiling Chen, and Jennifer Neville (Eds.).
AAAI Press, 8078â€“8086. https://doi.org/10.1609/aaai.v37i7.25976
[17] Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Ya-
sumasa Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura.
2023. Spatio-temporal meta-graph learning for traffic forecasting. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 37. 8078â€“8086.[18] Shiyong Lan, Yitong Ma, Weikang Huang, Wenwu Wang, Hongyu Yang, and
Pyang Li. 2022. Dstagnn: Dynamic spatial-temporal aware graph neural network
for traffic flow forecasting. In International conference on machine learning. PMLR,
11906â€“11917.
[19] Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin,
and Yong Li. 2023. Dynamic graph convolutional recurrent network for traffic
prediction: Benchmark and solution. ACM Transactions on Knowledge Discovery
from Data 17, 1 (2023), 1â€“21.
[20] Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, Chao
Huang, Zhenguang Liu, Bryan Hooi, and Roger Zimmermann. 2023. LargeST: A
Benchmark Dataset for Large-Scale Traffic Forecasting. In Advances in Neural
Information Processing Systems.
[21] Hao Peng, Bowen Du, Mingsheng Liu, Mingzhe Liu, Shumei Ji, Senzhang Wang,
Xu Zhang, and Lifang He. 2021. Dynamic graph convolutional network for
long-term traffic flow prediction with reinforcement learning. Inf. Sci. 578 (2021),
401â€“416. https://doi.org/10.1016/J.INS.2021.07.007
[22] Zezhi Shao, Zhao Zhang, Wei Wei, Fei Wang, Yongjun Xu, Xin Cao, and Chris-
tian S Jensen. 2022. Decoupled dynamic spatial-temporal graph neural network
for traffic forecasting. Proceedings of the VLDB Endowment 15, 11 (2022), 2733â€“
2746.
[23] Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xiao-
jiang Peng, Yuxuan Liang, and Yang Wang. 2023. The snowflake hypothesis: Train-
ing deep GNN with one node one receptive field. arXiv preprint arXiv:2308.10051
(2023).
[24] Kun Wang, Yuxuan Liang, Xinglin Li, Guohao Li, Bernard Ghanem, Roger Zim-
mermann, Huahui Yi, Yudong Zhang, Yang Wang, et al .2023. Brave the wind
and the waves: Discovering robust and generalizable graph lottery tickets. IEEE
Transactions on Pattern Analysis and Machine Intelligence (2023).
[25] Kun Wang, Yuxuan Liang, Pengkun Wang, Xu Wang, Pengfei Gu, Junfeng Fang,
and Yang Wang. 2023. Searching Lottery Tickets in Graph Neural Networks: A
Dual Perspective. In The Eleventh International Conference on Learning Represen-
tations. https://openreview.net/forum?id=Dvs-a3aymPe
[26] Kun Wang, Hao Wu, Guibin Zhang, Junfeng Fang, Yuxuan Liang, Yuankai Wu,
Roger Zimmermann, and Yang Wang. 2024. Modeling spatio-temporal dynamical
systems with neural discrete learning and levels-of-experts. IEEE Transactions on
Knowledge and Data Engineering (2024).
[27] Hao Wu, Yuxuan Liang, Wei Xiong, Zhengyang Zhou, Wei Huang, Shilong Wang,
and Kun Wang. 2024. Earthfarsser: Versatile Spatio-Temporal Dynamical Systems
Modeling in One Model. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 38. 15906â€“15914.
[28] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
Transactions on Neural Networks and Learning Systems 32, 1 (2020), 4â€“24.
[29] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.
Graph Wavenet for Deep Spatial-Temporal Graph Modeling. In Proceedings of
the 28th International Joint Conference on Artificial Intelligence (Macao, China)
(IJCAIâ€™19). AAAI Press, 1907â€“1913.
[30] Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial Temporal Graph Con-
volutional Networks for Skeleton-Based Action Recognition. In Proceedings of
the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th
innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,
Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger
(Eds.). AAAI Press, 7444â€“7452.
[31] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,
Yingyan Lin, Zhangyang Wang, and Richard G. Baraniuk. 2020. Drawing Early-
Bird Tickets: Toward More Efficient Training of Deep Networks. In International
Conference on Learning Representations.
[32] Haoran You, Zhihan Lu, Zijian Zhou, Yonggan Fu, and Yingyan Lin. 2022. Early-
bird gcns: Graph-network co-optimization towards more efficient gcn training
and inference via drawing early-bird lottery tickets. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 36. 8910â€“8918.
[33] Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-Temporal Graph Con-
volutional Networks: A Deep Learning Framework for Traffic Forecasting. In
Proceedings of the Twenty-Seventh International Joint Conference on Artificial In-
telligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, JÃ©rÃ´me Lang (Ed.).
ijcai.org, 3634â€“3640.
A APPENDIX
A.1 Proof of Proposition 1
Initially, in the case of ğ‘=3as illustrated in Figure 7, all span-
ning trees of this complete graph meet the diameter ğ‘Ÿ=2, and
satisfy definition of star spanning tree in Hypothesis 1. Their count
corresponds to the number of nodes ğ‘in the complete graph.
 
710Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 7: The 3-order spanning trees of the complete graph
are all star spanning trees.
Figure 8: Adds the new node to the native tree.
Subsequently, assuming ğ‘=ğ‘˜âˆ’1, the complete graph Kğ‘˜âˆ’1
aligns with this conclusion, and the star spanning tree is Tâ˜…
ğ‘˜âˆ’1.
In the scenario where ğ‘=ğ‘˜, the original graph is equivalent
to inserting a new node into Tâ˜…
ğ‘˜âˆ’1. Figure 8 shows two possible
scenarios.
Only in the first scenario, does the spanning tree Tâ˜…
ğ‘˜meet the
diameterğ‘Ÿ=2. The second scenario will increase some paths that
are longer than 2. For the spanning tree Tâ˜…
ğ‘˜formed in the first
scenario, it still conforms to the definition of star spanning tree in
Hypothesis 1.
A.2 Justify the effectiveness of star topology
theoretically
Given two graphsGand Ë†G, ifğ‘³Gâª¯ğ‘³Ë†G, we denote this as: Gâª¯ Ë†G.
Here, ğ‘³Gandğ‘³Ë†Grepresent the Laplacians of Gand Ë†G, respec-
tively. The symbol âª¯denotes the Loewner partial order, applicable
to certain pairs of symmetric matrices.
The Courant-Fisher Theorem provides that:
ğœ†ğ‘–(ğ´)=max
ğ‘†:dim(ğ‘†)=ğ‘–min
ğ‘¥âˆˆğ‘†ğ‘¥ğ‘‡ğ´ğ‘¥
ğ‘¥ğ‘‡ğ‘¥. (15)
Thus, assuming ğœ†1,...,ğœ†ğ‘are the eigenvalues of ğ‘³GandËœğœ†1,..., Ëœğœ†ğ‘›
are the eigenvalues of ğ‘³Ë†G. The relation ğ¿Gâª¯ğ¿Ë†Gmeans for all i,
ğœ†ğ‘–â‰¤Ë†ğœ†ğ‘–.
Graph Spectral Similarity. [1][4] Ifğ¿Ë†G/ğœâª¯ğ¿Gâª¯ğœğ¿Ë†G, we say
graphsGand Ë†Gareğœ-spectral similar. Thus, Ë†Gis ağœ-approximation
ofG.
Based on spectral graph theory [ 1][4], if a graph is a ğœâˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘œğ‘¥ğ‘–ğ‘šğ‘ğ‘¡ğ‘–ğ‘œğ‘›
of another one. We mean they have similar eigensystems, there-
fore with similar properties. Thus, if ğ‘³Tğ‘/ğœâª¯ğ¿Kğ‘âª¯ğœğ¿Tğ‘,Kğ‘andTğ‘have similar properties. Such a Tğ‘can effectively replace
Kğ‘to learn a good representation, where the edges of Tğ‘are
much fewer than those of Kğ‘. Below, we will prove that Tğ‘is a
ğœâˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘œğ‘¥ğ‘–ğ‘šğ‘ğ‘¡ğ‘–ğ‘œğ‘› ofKğ‘.
Lemma 1. The laplacian ofKğ‘has eigenvalue 0 with multiplicity
1 andğ‘with multiplicity ğ‘âˆ’1.
Proof of Lemma 1. To compute the non-zero eigenvalues, let
ğbe any non-zero vector orthogonal to the all-1s vector, so
âˆ‘ï¸
ğ‘ğ(ğ‘)=0. (16)
The Laplacian Matrix of a weighted graph G=(ğ‘‰,ğ¸,ğ‘¤),ğ‘¤:
ğ¸â†’R+, is designed to capture the Laplacian quadratic form:
(ğ‘³Gğ’™)(ğ‘)=âˆ‘ï¸
(ğ‘,ğ‘)âˆˆğ¸ğ‘¤ğ‘,ğ‘(ğ’™(ğ‘)âˆ’ğ’™(ğ‘))
=ğ‘‘(ğ‘)ğ’™(ğ‘)âˆ’âˆ‘ï¸
(ğ‘,ğ‘)âˆˆğ¸ğ‘¤ğ‘,ğ‘ğ’™(ğ‘).(17)
We now compute the first coordinate of ğ‘³Kğ‘›ğ. Using the expres-
sion for the action of the Laplacian as an operator, we find
(ğ‘³Kğ‘›ğ)(1)=âˆ‘ï¸
ğ‘£â‰¥2(ğ(1)âˆ’ğ(ğ‘))
=(ğ‘›âˆ’1)ğ(1)âˆ’ğ‘›âˆ‘ï¸
ğ‘£=2ğ(ğ‘)=ğ‘›ğ(1).(18)
As the choice of coordinate was arbitrary, we have ğ‘³ğ=ğ‘ğ.
So, every vector orthogonal to the all-1s vector is an eigenvector of
eigenvalueğ‘. â–¡
Lemma 2. LetG=(V,E)be a graph, and let ğ‘andğ‘be vertices
of degree one that are both connected to another vertex ğ‘. Then, the
vector ğ=ğœ¹ğ’‚âˆ’ğœ¹ğ’ƒis an eigenvector of ğ‘³ğºof eigenvalue 1
Proof of Lemma 2. Just multiply ğ‘³ğºbyğ, and check (using
(17)) vertex-by-vertex that it equals ğ.
As eigenvectors of different eigenvalues are orthogonal, this im-
plies that ğğ‘=ğğ‘for every eigenvector with eigenvalue different
from 1. â–¡
Lemma 3. The laplacian ofTğ‘has eigenvalue 0 with multiplic-
ity 1, eigenvalue 1 with multiplicity ğ‘âˆ’2, and eigenvalue ğ‘with
multiplicity 1.
Proof of Lemma 3. Applying Lemma 2.1 to vertices ğ‘–andğ‘–+1
for2â‰¤ğ‘–<ğ‘, we findğ‘âˆ’2linearly independent eigenvectors of
the form ğœ¹ğ’Šâˆ’ğœ¹ğ’Š+1, all with eigenvalue 1. As 0 is also an eigenvalue,
only one eigenvalue remains to be determined. Recall that the trace
of a matrix equals both the sum of its diagonal entries and the sum
of its eigenvalues. We know that the trace of ğ‘³Tğ‘›is2ğ‘âˆ’2, and
we have identified ğ‘âˆ’1eigenvalues that sum to ğ‘âˆ’2. So, the
remaining eigenvalue must be ğ‘. â–¡
From Lemma 1-3, we deduce:
Lemma 4.Tğ‘is an N-approximation of Kğ‘.
 
711KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He
Proof of Lemma 5. Assumeğœ†1,...,ğœ†ğ‘are the eigenvalues of
The Laplacian ofKğ‘, andğ‘¢1,...,ğ‘¢ğ‘are the eigenvalues of the
Laplacian ofTğ‘. For i =1,ğœ†ğ‘–=ğ‘,ğ‘¢ğ‘–=ğ‘, satisfyingğ‘¢ğ‘–/ğ‘â‰¤ğœ†_ğ‘–â‰¤
ğ‘ğ‘¢ğ‘–. For 2â‰¤ğ‘–â‰¤ğ‘âˆ’1,ğœ†ğ‘–=ğ‘,ğ‘¢ğ‘–=1, satisfying ğ‘¢ğ‘–/ğ‘â‰¤ğœ†ğ‘–â‰¤ğ‘ğ‘¢ğ‘–.For i =N,ğœ†ğ‘–=0,ğ‘¢ğ‘–=0, satisfying ğ‘¢ğ‘–/ğ‘â‰¤ğœ†ğ‘–â‰¤ğ‘ğ‘¢ğ‘–. Thus, for all i,
ğ‘¢ğ‘–/ğ‘â‰¤ğœ†ğ‘–â‰¤ğ‘ğ‘¢ğ‘–, i.e.,ğ¿Tğ‘/ğ‘âª¯ğ¿Kğ‘âª¯ğ‘ğ¿Tğ‘. â–¡
In conclusion, we have theoretically proven that star topology
Tğ‘is a good approximation of Kğ‘, and therefore, can learn spa-
tiotemporal dependencies effectively.
 
712