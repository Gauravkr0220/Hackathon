Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute
Shinji Tajima
Nagoya Institute of Technology
Nagoya, Aichi, JapanRen Sugihara
Nagoya Institute of Technology
Nagoya, Aichi, Japan
Ryota Kitahara
Nagoya Institute of Technology
Nagoya, Aichi, JapanMasayuki Karasuyama
karasuyama@nitech.ac.jp
Nagoya Institute of Technology
Nagoya, Aichi, Japan
ABSTRACT
The graph classification problem has been widely studied; how-
ever, achieving an interpretable model with high predictive perfor-
mance remains a challenging issue. This paper proposes an inter-
pretable classification algorithm for attributed graph data, called LA-
GRA (Learning Attributed GRAphlets). LAGRA learns importance
weights for small attributed subgraphs, called attributed graphlets
(AGs), while simultaneously optimizing their attribute vectors. This
enables us to obtain a combination of subgraph structures and
their attribute vectors that strongly contribute to discriminating
different classes. A significant characteristics of LAGRA is that all
the subgraph structures in the training dataset can be considered
as a candidate structures of AGs. This approach can explore all
the potentially important subgraphs exhaustively, but obviously, a
naÃ¯ve implementation can require a large amount of computations.
To mitigate this issue, we propose an efficient pruning strategy
by combining the proximal gradient descent and a graph mining
tree search. Our pruning strategy can ensure that the quality of
the solution is maintained compared to the result without pruning.
We empirically demonstrate that LAGRA has superior or compa-
rable prediction performance to the standard existing algorithms
including graph neural networks, while using only a small number
of AGs in an interpretable manner.
CCS CONCEPTS
â€¢Computing methodologies â†’Supervised learning by clas-
sification; Feature selection.
KEYWORDS
Graph classification, graph mining, proximal gradient descent
ACM Reference Format:
Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama.
2024. Learning Attributed Graphlets: Predictive Graph Mining by Graphlets
with Trainable Attribute. In Proceedings of the 30th ACM SIGKDD Conference
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671970on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671970
1 INTRODUCTION
Prediction problems with a graph input, such as graph classification
problems, have been widely studied in the data science commu-
nity. A graph representation is useful to capture structural data,
and graph-based machine learning algorithms have been applied
to variety of application problems such as chemical composition
analysis [ 2,16] and crystal structure analysis [ 11,21]. In real-word
datasets, graphs often have node attributes as a continuous value
vector (note that we only focus on node attributes throughout the
paper, but the discussion is same for edge attributes). For example,
a graph created by a chemical composition can have a three dimen-
sional position of each atom as an attribute vector in addition to a
categorical label such as atomic species. In this paper, we consider
building an interepretable prediction model for a graph classification
problem in which an input graph has continuous attribute vectors.
As we will see in Section 3, this setting has not been widely studied
despite its practical importance.
Our framework can identify important small subgraphs, called
graphlets, in which each node has an attribute vector. Note that
we use the term graphlet simply to denote a small connected
subgraph [ 17], though in some papers, it only indicates induced
subgraphs [ 15]. Figure 1 shows an illustration of our prediction
model. In the figure, the output of the prediction model ğ‘“(ğº)=
ğ›½0+ğ›½ğ»1ğœ“(ğº;ğ»1)+ğ›½ğ»2ğœ“(ğº;ğ»2))+Â·Â·Â· for an input attributed graph
ğºis defined through a linear combination of attributed graphlets
(AGs), represented as ğ»1,ğ»2,..., each one of which is weighted by
parameters ğ›½ğ»1,ğ›½ğ»2,.... The function ğœ“(ğº;ğ»)evaluates a match-
ing score between ğºand an AGğ»in a sense that how precisely ğº
contains the AG ğ». We apply a sparse regularization to the parame-
terğ›½ğ»by which a small number of important AGs for classification
can be obtained, i.e., an AG with non-zero ğ›½ğ»(in particular, if it
has large|ğ›½ğ»|) can be regarded as a discriminative AG.
Important subgraphs and attribute vectors are usually unknown
beforehand. The basic strategy of our proposed method, called
LAGRA (Learning Attributed GRAphlets), is as follows:
â€¢To explore potentially important substructures of graphs,
i.e., subgraphs, LAGRA uses graph mining by which all the
subgraphs in the given dataset can be considered up to the
given maximum graph size.
 
2830
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
Ïˆ Ïˆ Ïˆ ( ) ( ) ( ) = + + + ; ; ;
AG AG AG H1 H2 H3Î²0+Î²H1 Î²H2 Î²H3 f(G) G G G
Figure 1: Illustration of our attributed graphlet (AG) based
prediction model. The colors of each graph node represents
a graph node label, and a bar plot associated with each graph
node represents a trainable attribute vector.
2
 1
 0 1
f(G_i)2
0246LDA (in {S})
(a) Training data
2
 1
 0 1
f(G_i)2
024LDA (in {S})
 (b) Test data
Figure 2: Scatter plots of ğ‘“(ğºğ‘–)and the direction obtained by
linear discriminant analysis on the orthogonal complement
ofğœ·ğ‘†, where ğœ·ğ‘†is a subvector of ğœ·selected by LAGRA. The
dataset is BZR.
â€¢For continuous attributes in AGs, we optimize them as train-
able parameters.
Figure 2 is a visualization of the identified subspace by LAGRA.
Both the axes are defined by a linear combination of the selected
small number of AGs (in this case, 63AGs selected that is quite
small compared with 148903 candidate subgraphs in this dataset).
The plot indicates that the subspace created by selected AGs is
highly discriminative, while both the axes are interpretable because
of the linearity with respect to AGs. See the last paragraph of Sec-
tion 4.2 for detail. Since the number of the possible subgraphs is
quite large and an attribute vector exists for each node in each one
of subgraphs, a naÃ¯ve implementation becomes computationally
intractable. For the efficient optimization, we employ a block coor-
dinate update [ 23] based approach in which ğœ·(a vector containing
ğ›½ğ»), the bias term ğ›½0, and attribute vectors are alternately updated.
In the alternate update of ğœ·, we apply the proximal gradient de-
scent [ 1,19], which is known as an effective algorithm to optimize
sparse parameters. For this step, we propose an efficient pruning
strategy, enabling us to identify dimensions that are not required
to update at that iteration. This pruning strategy has the three
advantages. First, by combining the sparsity during the proximal
update and the graph mining tree search, we can eliminate unnec-
essary dimensions without enumerating all the possible subgraphs.
Second, for removed variables ğ›½ğ»at that iteration, attribute vectors
inğ»are also not required to be updated, which also accelerates
the optimization. Third, our pruning strategy is designed so that
it can maintain the update result compared with when we do not
perform the pruning (In other words, our pruning strategy does
not deteriorate the resulting model accuracy).
Our contributions are summarized as follows:â€¢We propose an interpretable graph classification model, in
which the prediction is defined through a linear combina-
tion of graphlets that have trainable attribute vectors. By
imposing a sparse penalty on the coefficient of each AG, a
small number of important AGs can be identified.
â€¢To avoid directly handling an intractably large size of opti-
mization variables, we propose an efficient pruning strategy
based on the proximal gradient descent, which can safely
ignore AGs that do not contribute to the update.
â€¢We verify effectiveness of LAGRA by empirical evaluations.
Although our prediction model is simple and interpretable,
we show that prediction performance of LAGRA was supe-
rior to or comparable with well-known standard graph classi-
fication methods, and in those results, LAGRA actually only
used a small number of AGs. Further, we also show examples
of selected AGs to demonstrate the high interpretability.
2 PROPOSED METHOD: LAGRA
In this section, we describe our proposed method, called Learning
Attributed GRAphlets (LAGRA). First, in Section 2.1, we show the
formulation of our model and the definition of the optimization
problem. Second, in Section 2.2, we show an efficient optimization
algorithm for LAGRA.
2.1 Formulation
2.1.1 Problem Setting. We consider a classification problem in
which a graph ğº=(ğ‘‰ğº,ğ¸ğº,ğ¿ğ‘£,ğ’›ğºğ‘£)is an input. A set of nodes and
edges ofğºare written as ğ‘‰ğºandğ¸ğº, respectively. Each one of
nodesğ‘£âˆˆğ‘‰ğºhas a categorical label ğ¿ğ‘£and a continuous attribute
vector ğ’›ğºğ‘£âˆˆRğ‘‘, whereğ‘‘is an attribute dimension. In this paper, an
attribute indicates a continuous attribute vector. We assume that a
label and an attribute vector are for a node, but the discussion in
this paper is completely same as for an edge label and attribute. A
training dataset is{(ğºğ‘–,ğ‘¦ğ‘–)}ğ‘–âˆˆ[ğ‘›], in whichğ‘¦ğ‘–âˆˆ{âˆ’ 1,+1}is a binary
label andğ‘›is the dataset size, where [ğ‘›]={1,...,ğ‘›}. Although
we only focus on the classification problem, our framework is also
applicable to the regression problem just by replacing the loss
function.
2.1.2 Attributed Graphlet Inclusion Score. We consider extract-
ing important small attributed graphs, which we call attributed
graphlets (AGs), that contributes to the classification boundary.
Note that throughout the paper, we only consider a connected
graph as an AG for a better interpretability (do not consider an
AG by a disconnected graph). Let ğœ“(ğºğ‘–;ğ»)âˆˆ[ 0,1]be a feature
representing a degree that an input graph includes an AG ğ». We
refer toğœ“(ğºğ‘–;ğ»)as the AG inclusion score (AGIS). Our proposed
LAGRA identifies important AGs by applying a feature selection to
a model with this AGIS feature.
Suppose that ğ¿(ğº)=(ğ‘‰ğº,ğ¸ğº,ğ¿ğ‘£)is a labeled graph in which
an attribute ğ’›ğºğ‘£for each node is removed from the original ğº. We
define AGIS so that it has a non-zero value only when ğ¿(ğ»)is
included in ğ¿(ğºğ‘–):
ğœ“(ğºğ‘–;ğ»)=(
ğœ™ğ»(ğºğ‘–)ifğ¿(ğ»)âŠ‘ğ¿(ğºğ‘–),
0 otherwise,(1)
 
2831Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
G i
zGi
1zGi
2zGi
3
zGi
4zGi
5Graph
HzH
1zH
2
Graphlet
H/primeGraphletzH/prime
2 zH/prime
1L(G i)
L(H)/subsetsqequalL(G i)
L(H/prime)/negationslash/subsetsqequalL(G i)m
m/primeG i
HzH
1zH
2zGi
1zGi
2zGi
3
zGi
4zGi
5Graph
Graphlet
(a) (b)
Figure 3: Examples of matchings between a graph and AGs
(colors of graph nodes are node labels). (a) For two AGs ğ»
andğ»â€²,ğ¿(ğºğ‘–)only contains ğ¿(ğ»), andğ¿(ğ»â€²)is not contained.
Then,ğœ“(ğºğ‘–;ğ»)>0andğœ“(ğºğ‘–;ğ»â€²)=0. (b) An example of the set
of injections ğ‘€={ğ‘š,ğ‘šâ€²}, whereğ‘š(1)=2,ğ‘š(2)=3,ğ‘šâ€²(1)=1,
andğ‘šâ€²(2)=4. The figure shows that ğ‘šandğ‘šâ€²are label and
edge preserving.
whereğ¿(ğ»)âŠ‘ğ¿(ğºğ‘–)means thatğ¿(ğ»)is a subgraph of ğ¿(ğºğ‘–), and
ğœ™ğ»(ğºğ‘–)âˆˆ( 0,1]is a function that provides a continuous inclusion
score ofğ»inğºğ‘–. The condition ğ¿(ğ»)âŠ‘ğ¿(ğºğ‘–)makes AGIS highly
interpretable. For example, in the case of chemical composition
data, ifğ¿(ğ»)represents O-C (oxygen and carbon are connected)
andğœ“(ğºğ‘–;ğ»)>0, then we can guarantee that ğºğ‘–must contain O-C.
Figure 3(a) shows examples of ğ¿(ğºğ‘–)andğ¿(ğ»).
The function ğœ™ğ»(ğºğ‘–)needs to be defined so that it can represent
how strongly the attribute vectors in ğ»can be matched to those
ofğºğ‘–. Whenğ¿(ğ»)âŠ‘ğ¿(ğºğ‘–), there exists at least one injection ğ‘š:
ğ‘‰ğ»â†’ğ‘‰ğºğ‘–in whichğ‘š(ğ‘£)forğ‘£âˆˆğ‘‰ğ»preserves node labels and
edges among ğ‘£âˆˆğ‘‰ğ». Figure 3(b) shows an example of when there
exist two injections. Let ğ‘€be a set of possible injections ğ‘š. We
define a similarity between ğ»and a subgraph of ğºğ‘–matched by
ğ‘šâˆˆğ‘€as follows
Sim(ğ»,ğºğ‘–;ğ‘š)=expÂ©Â­
Â«âˆ’ğœŒâˆ‘ï¸
ğ‘£âˆˆğ‘‰ğ»ğ’›ğ»
ğ‘£âˆ’ğ’›ğºğ‘–
ğ‘š(ğ‘£)2ÂªÂ®
Â¬,
whereğœŒ>0is a fixed parameter that adjusts the length scale.
Inexp, the sum of squared distances of attribute vectors between
matched nodes are taken. To use this similarity in AGIS (1), we take
the maximum among all the matchings ğ‘€:
ğœ™ğ»(ğºğ‘–)=MaxPooling({Sim(ğ»,ğºğ‘–;ğ‘š):ğ‘šâˆˆğ‘€}) (2)
An intuition behind (2) is that it evaluates inclusion of ğ»inğºğ‘–
based on the best macthing in a sense of Sim(ğ»,ğºğ‘–;ğ‘š)forğ‘šâˆˆğ‘€.
Ifğ¿(ğ») âŠ‘ğ¿(ğºğ‘–)and there exists ğ‘šsuch that ğ’›ğ»ğ‘£=ğ’›ğºğ‘–
ğ‘š(ğ‘£)for
âˆ€ğ‘£âˆˆğ‘‰ğ», then,ğœ™ğ»(ğºğ‘–)takes the maximum value (i.e., 1).
2.1.3 Model definition. Our prediction model linearly combines
the featureğœ“(ğºğ‘–;ğ»)as follows:
ğ‘“(ğºğ‘–)=âˆ‘ï¸
ğ»âˆˆHğœ“(ğºğ‘–;ğ»)ğ›½ğ»+ğ›½0=ğâŠ¤
ğ‘–ğœ·+ğ›½0, (3)
whereğ›½ğ»andğ›½0are parameters,His a set of candidate AGs,
andğœ·andğğ‘–are vectors containing ğ›½ğ»andğœ“(ğºğ‘–;ğ»)forğ»âˆˆH,
Training data L H
zG1
1
zG1
2zG1
3
zG2
1 zG2
2
zG2
3
zG2
4G1
G2
G3zG3
1
zG3
2zG3
3zG3
4
zG3
5zH1
1
zH2
1
zH3
1
zH4
1zH3
2
zH4
2(maxpat = 2)
H1
H2
H3
H4Figure 4: An example of training data, LandH. SinceL
only includes subgraphs in the training data, â€œ
 â€ is not
included inL.His created fromLby adding trainable at-
tribute vectors ğ’›ğ»ğ‘–ğ‘£(ğ‘£âˆˆğ‘‰ğ»ğ‘–).
respectively. LetL={ğ¿|ğ¿âŠ†ğ¿(ğºğ‘–),ğ‘–âˆˆ[ğ‘›],|ğ¿|â‰¤maxpat}be
a set of all the labeled subgraphs contained in the training input
graphs{ğºğ‘–}ğ‘–âˆˆ[ğ‘›], where|ğ¿|is the number of nodes in the labeled
graphğ¿andmaxpat is the user-specified maximum size of AGs.
The number of the candidate AGs |H|is set as the same size as |L|.
We setHas a set of attributed graphs created by giving trainable
attribute vectors ğ’›ğ»ğ‘£(ğ‘£âˆˆğ‘‰ğ»)to each one of elements in L. Figure 4
shows a toy example. Our optimization problem for ğ›½ğ»,ğ›½0andğ’›ğ»ğ‘£
is defined as the following regularized loss minimization in which
the sparseğ¿1penalty is imposed on ğ›½ğ»:
min
ğœ·,ğ›½0,ZH1
2ğ‘›âˆ‘ï¸
ğ‘–=1â„“(ğ‘¦ğ‘–,ğ‘“(ğºğ‘–))+ğœ†âˆ‘ï¸
ğ»âˆˆH|ğ›½ğ»|, (4)
whereZH={ğ’›ğ»ğ‘£|ğ‘£âˆˆğ‘‰ğ»,ğ»âˆˆH} , andâ„“is a convex differentiable
loss function. Here, we employ the squared hinge loss function [ 7]:
â„“(ğ‘¦ğ‘–,ğ‘“(ğºğ‘–))=max(1âˆ’ğ‘¦ğ‘–ğ‘“(ğºğ‘–),0)2.
Since the objective function (4) induces a sparse solution for ğ›½ğ», we
can identify a small number of important AGs as ğ»having the non-
zeroğ›½ğ». However, this optimization problem has an intractably
large number of optimization variables ( Hcontains all the possible
subgraphs in the training dataset and each one of ğ»âˆˆH has the
attribute vector ğ’›ğ»ğ‘£âˆˆRğ‘‘for each one of nodes). We propose an
efficient optimization algorithm that mitigates this problem.
2.2 Optimization
Our optimization algorithm is based on the block coordinate update
[23] algorithm, in which the (proximal) gradient descent alternately
updates a block of variables. We update one of ğœ·,ğ›½0andZHalter-
nately, while the other two parameters are fixed. First, the proximal
gradient update is applied to ğœ·because it has the ğ¿1penalty. Second,
forğ›½0, we calculate the optimal solution under fixing the other vari-
able because it is easy to obtain. Third, for ZH, we apply the usual
gradient descent update because it does not have sparse penalty.
The difficulty of the optimization problem (4) originates from
the size ofH. We select a small size of a subset W âŠ†H , and
only ğœ·WâˆˆR|W|, defined by ğ›½ğ»forğ»âˆˆW , and corresponding
attribute vectorsZW={ğ’›ğ»ğ‘£|ğ‘£âˆˆğ‘‰ğ»,ğ»âˆˆ W} âŠ† ZHare
updated. We propose an efficient pruning strategy by combining
the proximal gradient with the graph mining, which enables us
 
2832KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
to selectWwithout enumerating all the possible subgraphs. A
notable characteristics of this approach is that it can obtain the
completely same result compared with when we do not restrict the
size of variables.
2.2.1 Update ğœ·,ğ›½0andZH.Before introducing the subset W,
we first describe update rules of each variable. First, we apply the
proximal gradient update to ğœ·. Let
ğ‘”ğ»(ğœ·)=ğ‘›âˆ‘ï¸
ğ‘–=1ğœ•â„“(ğ‘¦ğ‘–,ğ‘“(ğºğ‘–))
ğœ•ğ‘“(ğºğ‘–)ğœ“(ğºğ‘–;ğ») (5)
be the derivative of the loss term in (4) with respect to ğ›½ğ». Then,
the update of ğœ·is defined by
ğ›½(new)
ğ»â†prox(ğ›½ğ»âˆ’ğœ‚ğ‘”ğ»(ğœ·)), (6)
whereğœ‚>0is a step length, and
prox(ğ‘)=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³ğ‘âˆ’ğœ‚ğœ† ifğ‘â‰¥ğœ‚ğœ†,
0 ifğ‘âˆˆ(âˆ’ğœ‚ğœ†,ğœ‚ğœ†),
ğ‘+ğœ‚ğœ† ifğ‘â‰¤âˆ’ğœ‚ğœ†
is a proximal operator (Note that the proximal gradient for the ğ¿1
penalty is often called ISTA [ 1], for which an accelerated variant
called FISTA is also known. We here employ ISTA for simplicity).
We select the step length ğœ‚by the standard backtrack search.
The bias term ğ›½0is update by
min
ğ›½0ğ‘›âˆ‘ï¸
ğ‘–=0max(1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0),0)2,
which is the optimal solution of the original problem (4) for given
other variables ğœ·andZH. Since the objective function of ğ›½0is a
differential convex function, the update rule of ğ›½0can be derived
from the first order condition as
ğ›½(new)
0â†Ã
ğ‘–âˆˆI(new)(ğ‘¦ğ‘–âˆ’ğâŠ¤
ğ‘–ğœ·)
|I(new)|, (7)
whereI(new)={ğ‘–|1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½(new)
0)>0}. This update rule
containsğ›½(new)
0inI(new). However, it is easy to calculate the up-
date (7) without knowing ğ›½(new)
0beforehand. Here, we omit detail
because it is a simple one dimensional problem (see supplementary
appendix A).
Forğ’›ğ»ğ‘£âˆˆZH, we employ the standard gradient descent:
ğ’›ğ»(new)
ğ‘£â†ğ’›ğ»
ğ‘£âˆ’ğ›¼ğ‘›âˆ‘ï¸
ğ‘–=1ğœ•â„“(ğ‘¦ğ‘–,ğ‘“(ğºğ‘–))
ğœ•ğ‘“(ğºğ‘–)Ã—
ğ›½ğ»ğœ•ğœ“(ğºğ‘–;ğ»)
ğœ•ğ’›ğ»ğ‘£
,(8)
whereğ›¼>0is a step length to which we apply the standard
backtrack search.
2.2.2 Gradient Pruning with Graph Mining. In every update of ğœ·,
we incrementally add required ğ»intoWâŠ†H . For the complement
setW=H\W , which contains AGs that have never been updated,
we initialize ğ›½ğ»=0forğ»âˆˆW. For the initialization of a node
attribute vector ğ’›ğ»ğ‘£âˆˆZH, we set the same initial vector if the
node (categorical) labels are same, i.e., ğ’›ğ»ğ‘£=ğ’›ğ»â€²
ğ‘£â€²ifğ¿ğ‘£=ğ¿ğ‘£â€²for
âˆ€ğ»,ğ»â€²âˆˆH (in practice, we use the average of the attribute vectors
within each node label). This constraint is required for our pruningcriterion, but it is only for initial values. After the update (8), all ğ’›ğ»ğ‘£
can have different values.
Sinceğ›½ğ»=0forğ»âˆˆW, it is easy to derive the following
relation from the proximal update (6):
|ğ‘”ğ»(ğœ·)|â‰¤ğœ†andğ»âˆˆW â‡’ 0=prox(ğ›½ğ»âˆ’ğœ‚ğ‘”ğ»(ğœ·)).(9)
This indicates that if the conditions in the left side hold, we do not
need to update ğ›½ğ»because it remains 0. Therefore, we set
Wâ†Wâˆªn
ğ»|ğ‘”ğ»(ğœ·)|>ğœ†,âˆ€ğ»âˆˆWo
, (10)
and apply the update (6) only to ğ»âˆˆW . However, evaluating
|ğ‘”ğ»(ğœ·)|>ğœ†for allğ»âˆˆWcan be computationally intractable
because it needs to enumerate all the possible subgraphs. The fol-
lowing theorem can be used to avoid this difficulty:
Theorem 2.1. Letğ¿(ğ»â€²) âŠ’ğ¿(ğ»)andğ»,ğ»â€²âˆˆW. Then, the
absolute value of the derivative (5) is bounded as
|ğ‘”ğ»â€²(ğœ·)|â‰¤ğ‘”ğ»(ğœ·)
where
ğ‘”ğ»(ğœ·)=max(âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–>0}ğ‘¦ğ‘–ğœ“(ğºğ‘–;ğ»)(1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0)),
âˆ’âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–<0}ğ‘¦ğ‘–ğœ“(ğºğ‘–;ğ»)(1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0)))
,
whereI={ğ‘–|1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0)>0}.
See supplementary appendix B for the proof. Note that here I
is defined by the current ğ›½0unlike (7). This theorem indicates
that the gradient|ğ‘”ğ»â€²(ğœ·)|for anyğ»â€²whoseğ¿(ğ»â€²)containsğ¿(ğ»)
as a subgraph can be bounded by ğ‘”ğ»(ğœ·). It should be noted that
ğ‘”ğ»(ğœ·)can be calculated without generating ğ»â€², and it mainly needs
only the model prediction with the current parameter ğâŠ¤
ğ‘–ğœ·+ğ›½0,
which can be immediately obtained at each iteration, and AGIS
ğœ“(ğºğ‘–;ğ»). The rule (9) reveals that, to identify ğ›½(new)
ğ»â€²=0, we only
require to know whether |ğ‘”ğ»â€²(ğœ·)|â‰¤ğœ†holds, and thus, an important
consequence of theorem 2.1 is the following rule:
ğ‘”ğ»(ğœ·)â‰¤ğœ†andğ»âˆˆW
â‡’ |ğ‘”ğ»â€²(ğœ·)|â‰¤ğœ†forâˆ€ğ»â€²âˆˆ{ğ»â€²|ğ¿(ğ»â€²)âŠ’ğ¿(ğ»),ğ»â€²âˆˆW}.
(11)
Therefore, if the conditions in the first line in (11) hold, any ğ»â€²
whoseğ¿(ğ»â€²)containsğ¿(ğ»)as a subgraph can be discarded during
that iteration. Further, from (8), we can immediately see that at-
tribute vectors ğ’›ğ»ğ‘£forâˆ€ğ‘£âˆˆğ‘‰ğ»are also not necessary to be updated
ifğ›½ğ»=0. This is an important fact because updates of a large
number of variables can be omitted.
Figure 5 shows an illustration of the forward and backward
(gradient) computations of LAGRA. For the gradient pruning, an
efficient algorithm can be constructed by combining the rule (11)
and a graph mining algorithm. A well-known efficient graph min-
ing algorithm is gSpan [ 24], which creates the tree by recursively
expanding each graph in the tree node as far as the expanded graph
is included in a given set of graphs as a subgraph. An important
characteristics of the mining tree is that all graphs must contain
any graph of its ancestors as subgraphs. Therefore, during the tree
 
2833Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
traverse (depth-first search) by gSpan, we can prune the entire sub-
tree (all descendant nodes) if ğ‘”ğ»(ğœ·)â‰¤ğœ†holds for the AG ğ»in a
tree node (Figure 5(b)). This means that we can update Wby (10)
without exhaustively investigating all the elements in W.
gSpan has another advantage for LAGRA. To calculate the feature
ğœ™ğ»(ğºğ‘–), defined in (2), LAGRA requires a set of injections ğ‘€(an
example is shown in Figure 3(b)). gSpan keeps the information of ğ‘€
during the tree traverse because it is required to expand a subgraph
in eachğºğ‘–(see the authors implementation https://sites.cs.ucsb.
edu/~xyan/software/gSpan.htm). Therefore, we can directly use ğ‘€
created by gSpan to calculate (2).
2.2.3 Algorithm. We here describe entire procedure of the opti-
mization of LAGRA. We employ the so-called regularization path
following algorithm (e.g., [ 5]), in which the algorithm starts from
a large value of the regularization parameter ğœ†and gradually de-
creases it while solving the problem for each ğœ†. This strategy can
start from highly sparse ğœ·, in which usuallyWalso becomes small.
Further, at each ğœ†, the solution obtained in the previous ğœ†, can
be used as the initial value by which faster convergence can be
expected (so-called warm start).
Algorithm 1 shows the procedure of the regularization path fol-
lowing. We initialize ğœ·=0, which is obviously optimal when ğœ†=âˆ.
In line 2 of Algorithm 1, we calculate ğœ†maxat which ğœ·starts having
non-zero values: ğœ†max=maxğ»âˆˆHÃğ‘›
ğ‘–âˆˆ[ğ‘›]ğ‘¦ğ‘–ğœ“(ğºğ‘–;ğ»)(1âˆ’ğ‘¦ğ‘–ğ›½0),
whereğ›½0=Ã
ğ‘–âˆˆ[ğ‘›]ğ‘¦ğ‘–/ğ‘›. See supplementary appendix D for deriva-
tion.ğœ†maxcan also be written as ğœ†max=maxğ»âˆˆH|ğ‘”ğ»(0)|. To find
maxğ»âˆˆH, we can use almost the same gSpan based pruning strategy
by using an upper bound of ğ‘”ğ»(ğœ·)as shown in Section 2.2.2 (the
only difference is to search the max value only, instead of searching
allğ»satisfying|ğ‘”ğ»(ğœ·)|>ğœ†), though in Algorithm 1, this process
is omitted for brevity. After setting ğœ†0â†ğœ†max, the regularization
parameterğœ†is decreased by using a pre-defined decreasing fac-
torğ‘…as shown in line 6 of Algorithm 1. For each ğœ†1>Â·Â·Â·>ğœ†ğ¾,
the parameters ğœ·,ğ›½0andZHare alternately updated as described
in Section 2.2.1 and 2.2.2. We stop the alternate update by moni-
toring performance on the validation dataset in line 14 (stop by
thresholding the decrease of the objective function is also possible).
The algorithm of the pruning strategy described in Section 2.2.2
is shown in Algorithm 2. This function recursively traverses the
graph mining tree. At each tree node, first, ğ‘”ğ»(ğœ·)is evaluated to
prune the subtree if possible. Then, if |ğ‘”ğ»(ğœ·)|>ğœ†ğ‘˜,ğ»is included
inW. The expansion from ğ»(creating children of the graph tree)
is performed by gSpan, by which only the subgraphs contained in
the training set can be generated (see the original paper [ 24] for
detail of gSpan). The initialization of the trainable attribute ğ’›ğ»â€²
ğ‘£is
performed when ğ»â€²is expanded (line 15).
3 RELATED WORK
For graph-based prediction problems, recently, graph neural net-
works (GNNs) [ 29] have attracted wide attention. However, inter-
preting GNNs is not easy in general. According to a recent review of
explainable GNNs [ 27], almost all of explainability studies for GNNs
are instance-level explanations, which provides input-dependent
explanations (Here, we do not mention each one of input-dependent
approaches because the purpose is clearly different from LAGRA).Algorithm 1: Optimization of LAGRA
1function Reguralization-Path( ğ¾,ğ‘…, MaxEpoch)
2ğ»0â†a graph at the root node of the mining tree
3Wâ†âˆ…
4 ğœ·â†0, ğ›½0=Ã
ğ‘–âˆˆ[ğ‘›]ğ‘¦ğ‘–/ğ‘›
5ğœ†0â†ğœ†max Computeğœ†max
6 forğ‘˜=1,2,...,ğ¾ do
7ğœ†ğ‘˜â†ğ‘…ğœ†ğ‘˜âˆ’1
8 forepoch =1,2,..., MaxEpoch do
9Wâ†Wâˆª GradientPruning(ğ»0, ğœ†ğ‘˜)
10 Update ğœ·by (6) forğ»âˆˆW
11 Updateğ›½0by (7)
12 Update ğ’›ğ»ğ‘£by (8) forğ»âˆˆW
13 val_lossâ†Compute validation loss
14 ifval_loss has not been improved in the past ğ‘
iterations then
15 break Inner loop stopping condition
16 else
17M(ğ‘˜)â†(W,ğœ·,ğ›½0)
18 return{M(ğ‘˜)}ğ¾
ğ‘˜=0
Algorithm 2: Gradient Pruning
1function GradientPruning( ğ», ğœ†ğ‘˜)
2Wâ†âˆ…
3 ifğ‘”ğ»(ğœ·)â‰¤ğœ†ğ‘˜then
4 returnâˆ… Prune the subtree
5 if|ğ‘”ğ»(ğœ·)|>ğœ†ğ‘˜then
6Wâ†Wâˆª{ ğ»}
7Câ† CreateChildren(ğ»)
8 forğ»â€²âˆˆCdo
9Wâ†Wâˆª GradientPruning(ğ»â€², ğœ†ğ‘˜)
10 returnW
11function CreateChildren( ğ»)
12 ifchildren ofğ»have never been created by gSpan then
13Câ† graphs expanded from ğ»by gSpan
14 forğ»â€²âˆˆCdo
15 ğ’›ğ»â€²
ğ‘£â†mean{ğ’›ğºğ‘–
ğ‘£â€²|ğ‘£â€²âˆˆğ‘‰ğºğ‘–,ğ¿ğ‘£=ğ¿ğ‘£â€²,ğ‘–âˆˆ[ğ‘›]}
16 Compute{ğœ“(ğºğ‘–;ğ»â€²)}ğ‘›
ğ‘–=1usingğ‘€created by
gSpan
An exception is XGNN [ 26], in which important discriminative
graphs are generated for a given already trained GNN by maximiz-
ing the GNN output for a target label. However, unlike our method,
the prediction model itself remains black-box, and thus, it is difficult
to know underlying dependency between the identified graphs and
the prediction.
A classical approach to graph-based prediction problems is the
graph kernel [ 10]. Although graph kernel itself does not identify
 
2834KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
m
m/primezH
1zH
2
zGi
1zGi
2zGi
3
zGi
4zGi
5zGi
1 zGi
2zGi
3
zGi
4 zGi
5
G i GraphH Attributed graphlet
mm/prime
zGi
1zGi
2zGi
3
zGi
4zGi
5Attributed graphlet......
...(b) Graph mining tree for 
gradient pruningf(G i)Training input
m
zGi
1zGi
2zGi
3
zGi
4zGi
5Attributed graphletObjective function
......
H/prime
zH/prime
1zH/prime
2Sim
SimMax 
Pooling
Max 
PoolingSim
Sim
Max 
PoolingSimForward pass
Backward pass
Î²0AGIS
AGIS
AGISH
H/prime
(a) AGIS computation
Figure 5: An illustration of LAGRA. a) In the forward pass, only passes with |ğ›½ğ»|>0contribute to the output. AGIS is defined
by the best matching between an input graph and an AG. b) For the backward pass, the gradient can be pruned when the rule
(11)is satisfied. In this illustration, ğ»â€²â€²is pruned by which graphs expanded from ğ»â€²â€²are not required to compute the gradient.
Table 1: Classification accuracy and the number of selected non-zero ğ›½ğ»by LAGRA (the bottom row). The average of five runs
and its standard deviation are shown. The underlines indicate the best average accuracy for each dataset and the bold-face
indicates that the result is comparable with the best method in a sense of one-sided ğ‘¡-test (significance level 5%). # best indicates
frequency that the method is the best or comparable with the best method.
AIDS BZR COX2 DHFR ENZYMES PRO
TEINS SYN
THETIC #
best
GH 0.9985Â±0.0020 0.8458Â±0.0327 0.7872Â±0.0252 0.7250Â±0.0113 0.6050Â±0.0857 0.7277Â±0.0332 0.6767Â±0.0655 2
ML 0.9630Â±0.0062 0.8289Â±0.0141 0.7787Â±0.0080 0.7105Â±0.0300 0.6000Â±0.0652 0.6205Â±0.0335 0.4867Â±0.0356 0
P
A 0.9805Â±0.0086 0.8313Â±0.0076 0.7809Â±0.0144 0.7316Â±0.0435 0.7500Â±0.0758 0.6884Â±0.0077 0.5400Â±0.0859 1
DGCNN 0.9830Â±0.0046 0.8169Â±0.0177 0.8021Â±0.0401 0.7289Â±0.0192 0.7289Â±0.0192 0.7509Â±0.0114 0.9867Â±0.0125 3
GCN 0.9840Â±0.0030 0.8290Â±0.0460 0.8340Â±0.0257 0.7490Â±0.0312 0.7000Â±0.0837 0.6880Â±0.0202 0.9630Â±0.0194 2
GA
T 0.9880Â±0.0041 0.8220Â±0.0336 0.7830Â±0.0274 0.7110Â±0.0156 0.7100Â±0.0768 0.7160Â±0.0108 0.9800Â±0.0267 2
GIN 0.9990Â±0.0014 0.8265Â±0.0290 0.7851Â±0.0254 0.7526Â±0.0350 0.7150Â±0.0285 0.7372Â±0.0266 0.9133Â±0.0274 3
LGARA
(Proposed) 0.9900Â±0.0050 0.8892Â±0.0207 0.8043Â±0.0229 0.8171Â±0.0113 0.6450Â±0.0797 0.7491Â±0.0142 1.0000Â±0.0000 4
#
non-zeroğ›½ğ» 50.4Â±17.1 52.4Â±19.0 45.4Â±14.9 40.0Â±11.6 7.2Â±8.4 25.8Â±9.4 35.8Â±35.0 -
important substructures, recently, [ 3] has proposed an interpretable
kernel-based GNN, called KerGNN. KerGNN uses a graph kernel
function as a trainable filter, inspired by the well-known convolu-
tional networks, and the filter updates the node attributes of the
input graph so that it embeds similarity to learned important sub-
graphs. Then, [ 3] claims that resulting graph filter can be seen as
a key structure. However, a learned subgraph in a graph kernel
filter is difficult to interpret. The kernel-based matching does not
guarantee the existence of a subgraph unlike our AGIS (1), and
further, only 1-hop neighbors of each node in the input graph are
matched to a graph filter.
Another graph mining based approach is [ 13]. This approach
also uses a pruning based acceleration for the optimization, but it isbased on the optimality of the convex problem while our proximal
gradient pruning is applicable to the non-convex problem of LA-
GRA. Further, more importantly, [ 13] cannot deal with continuous
attributes. The prediction model of LAGRA is inspired by a method
for learning time-series shaplets (LTS) [ 6]. LTS is also based on
a linear combination of trainable shaplets, which is a short frag-
ment of a time-series sequence. Unlike time-series data, possible
substructures in graph data have a combinatorial nature because
of which our problem setting has a computational difficulty that
does not exist in the case of LTS, for which LAGRA provide a graph
mining based efficient strategy.
 
2835Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4 EXPERIMENTS
Here, we empirically verify effectiveness of LAGRA. We used stan-
dard graph benchmark datasets, called AIDS, BZR, COX2, DHFR,
ENZYMES, PROTEINS and SYNTHETIC, retrieved from https://
ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets (for
ENZYMES, we only used two classes among original six classes
to make a binary problem). To simplify comparison, we only used
node labels and attributes, and did not use edge labels and attributes.
Statistics of datasets are summarized in supplementary appen-
dix E. The datasets are randomly divided into train :validation :
test=0.6 : 0.2 : 0.2. For the regularization path algorithm (Al-
gorithm 1), we created candidate values of ğœ†by uniformly divid-
ing[log(ğœ†max),log(0.01ğœ†max)]into 100grid points. We selected
ğœ†,maxpatâˆˆ{5,10}andğœŒâˆˆ{1,0.5,0.1,0.05,0.01}based on the
validation performance.
4.1 Prediction Performance
For the prediction accuracy comparison, we used graph kernels and
graph neural networks (GNN). We used three well-known graph
kernels that can handle continuous attributes, i.e., graph hopper ker-
nel (GH) [ 4], multiscale Laplacian kernel (ML) [ 9] and propagation
kernel (PA) [ 14], for all of which the library called GraKeL [ 18] was
used. For the classifier, we employed the ğ‘˜-nearest neighbor ( ğ‘˜-NN)
classification for which each kernel function ğ‘˜(ğºğ‘–,ğºğ‘—)defines the
distance function as âˆ¥ğºğ‘–âˆ’ğºğ‘—âˆ¥=âˆšï¸
ğ‘˜(ğºğ‘–,ğºğ‘–)âˆ’2ğ¾(ğºğ‘–,ğºğ‘—)+ğ‘˜(ğºğ‘—,ğºğ‘—).
The number of neighbors ğ‘˜is optimized by the validation set. For
GNN, we used deep graph convolutional neural network (DGCNN)
[28], graph convolutional network (GCN) [ 8], graph attention net-
work (GAT) [ 20], and graph isomorphism network (GIN) [ 22]. For
DGCNN, the number of hidden units {64,128,256}and epochs are
optimized by the validation set. The other settings were in the de-
fault settings of the authors implementation https://github.com/
muhanzhang/pytorch_DGCNN. For GCN and GAT, we also selected
the number of hidden units and epochs as above. For other settings,
we followed [ 25]. For GIN, we modified the authors implemen-
tation so that node attributes can be incorporated, and the same
hyper-parameter tuning as [ 22] was employed. The hidden dimen-
sion{16,32,64}, dropout ratio{0,0.5}, and batch size{32,64,128}
were selected by the validation performance, and other settings
were the default setting.
The results are shown in Table 1. LAGRA was the best or compa-
rable with the best method (in a sense of one-sided ğ‘¡-test) for BZR,
DHFR, PROTEINS and SYNTHETIC (4 out of 7 datasets). For AIDS
and COX2, LAGRA has similar accuracy values to the best methods
though they were not regarded as the best accuracy in ğ‘¡-test. The
three GNNs also show stable performance overall. Although our
main focus is to build an interepretable model, we see that LAGRA
achieved comparable accuracy with the current standard methods.
Further, LAGRA only used a small number of AGs shown in the
bottom row of Table 1, which suggests high interpretability of the
learned models.
4.2 Selected AGs
We here show examples of identified important AGs. Figure 6 and 7
show AGs having the two largest positive and negative ğ›½ğ»for DHFR
and BZR datasets, respectively. In each figure, a labeled graphletğ¿(ğ»)is shown in the left side (the numbers inside the graph nodes
are the graph node labels) and optimized attribute vectors for each
one of nodes are shown as bar plots in the right side. We can clearly
see important substractures not only by as structural information
of a graph but also attribute values associated with each node.
In a few datasets, two classes can be separated even in two
dimensional space of AGIS. Figure 8 show scatter plots of the test
dataset (not the training dataset) with the axes of identified features
by the LAGRA training. Let ğ»+andğ»âˆ’be AGs having the largest
positive and negative ğ›½ğ», respectively. The horizontal and vertical
axes of plots are ğœ“(ğºğ‘–,ğ»+)andğœ“(ğºğ‘–,ğ»âˆ’). In particular, in the AIDS
dataset, for which classification accuracy was very high in Table 1,
two classes are clearly separated. For DHFR, we can also see points
in two classes tend to be located on the upper left side and the lower
right side. The dashed lines are boundaries created by (class-balance
weighted) logistic regression fitted to the test points in these two
dimensional spaces. The estimated class conditional probability
hasAUC =0.94and0.62for AIDS and BZR, respectively, which
indicate that differences of two classes are captured even only by
two AGs in these datasets.
Another visualization of the space of the selected AGs are shown
in Fig. 2. The ğ‘¥-axis isğ‘“(ğºğ‘–), and theğ‘¦-axis is defined by linear
discriminant analysis (LDA). LDA is applied to the space defined
byğœ™(ğºğ‘–;ğ»)with selected ğ»under the condition that the projected
space is orthogonal to ğœ·ğ‘†, by which the axes in Fig. 2 are an orthog-
onal basis (Note that applying LDA to the original feature space
defined by allâˆ€ğ»is impossible because of its high dimension. This
visualization becomes possible due to the AG selection of LAGRA).
Both the axes are defined by a linear combination of ğœ™(ğºğ‘–;ğ»)by
which they are interpretable as a weighted sum of the contribu-
tions from the selected AGs. Figure 2 indicates that the space of the
selected AGs clearly discriminates two classes. The same scatter
plots on other datasets are shown in Fig. 12.
4.3 Discussion on Computational Time
Finally, we verify computational time of LAGRA. As we describe in
Section 2.1.3, the size of candidate AGs |H|is equal to|L|, which
is the number of all the possible subgraphs in the training datasets.
Therefore, it can be quite large. In each dataset, we counted |H|
and obtained AIDS =134281, BZR =148903, COX2 =101185,
DHFR =137872, ENZYMES >15464000, PROTEINS >13987000,
andSYNTHETIC >699000. For the ENZYMES, PROTEINS, and
SYNTHETIC datasets, the counts are the lower bounds because
they require too long time to count all candidates.
The optimization variables in the objective function (4) are ğœ·,ğ›½0
andZH. The dimension of ğœ·is|H|and the node attribute vector
ğ’›ğ»ğ‘£âˆˆRğ‘‘exists for each one of nodes in ğ»âˆˆH. Thus, the number
of optimization variables in (4) is 1+|H|+Ã
ğ»âˆˆH|ğ»|Ã—ğ‘‘, which
can be prohibitively large.
Figure 9 shows the computational time during the regularization
path. The horizontal axis is ğ‘˜ofğœ†ğ‘˜in Algorithm 1. The datasets
are AIDS and ENZYMES. In the regularization path algorithm, the
number of non-zero ğ›½ğ»typically increases during the process of
decreasingğœ†, because the ğ¿1penalty becomes weaker gradually. As
a results, in both the plots, the total time increases with the ğœ†index.
Although LAGRA performs the traverse of the graph mining tree
 
2836KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
(a) (b) (c) (d)
Figure 6: Selected important AGs for DHFR dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for
the two largest negative coefficients.
(a) (b) (c) (d)
Figure 7: Selected important AGs for BZR dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for
the two largest negative coefficients.
0.0 0.2 0.4 0.6 0.8 1.0
(Gi,H+)
0.00.20.40.60.81.0(Gi,H)
 1
-1
(a) AIDS ( ğ‘¥-axis: Fig. 11(a) and ğ‘¦-
axis:Fig. 11(c)) in appendix
0.0 0.2 0.4 0.6 0.8
(Gi,H+)
0.00.10.20.30.40.50.6(Gi,H)
 1
-1
(b) DHFR ( ğ‘¥-axis: Fig. 6(a) and ğ‘¦-
axis:Fig. 6(c))
Figure 8: Scatter plots of AGIS for graphlets ğ»+andğ»âˆ’, hav-
ing the largest positive and negative coefficients, respectively.
in every iteration of the gradient update (line 9 in Algorithm 1),
Figure 9 shows that the traverse time was not necessarily dominant(Note that the vertical axis is in log scale). In particular, when only
a small number of tree nodes are newly expanded at that ğœ†, the cal-
culation for the tree search becomes faster because AGIS ğœ“(ğºğ‘–,ğ»)
is already computed at the most of tree nodes. The computational
times were at most about 103sec for these datasets. We do not claim
that LGARA is computationally faster compared with other stan-
dard algorithms (such as graph kernels), but as the computational
time of the optimization problem with 1+|H|+Ã
ğ»âˆˆH|ğ»|Ã—ğ‘‘
variables, the results obviously indicate effectiveness of our pruning
based optimization approach.
Figure 10 shows the average number of traversed graph mining
tree nodes and the size of selected |W| for AIDS and ENZYMES.
In this figure, both the values increased with the decrease of ğœ†
because the effect of the sparse penalty becomes weaker. The total
number of the tree nodes were 134281 and more than 15464000 for
AIDS and ENZYMES, respectively. Figure 10 shows the number of
traversed nodes were at most about 6Ã—103/134281(â‰ˆ0.05)and9Ã—
102/15464000(â‰ˆ6Ã—10âˆ’5), respectively. This clearly indicates that
our pruning strategy can drastically reduce the tree nodes, at which
the evaluation of ğ‘”ğ»(ğœ·)is required as shown in Algorithm 2. In the
figure, we can see that |W| was further small. This indicates the
updated ğœ·was highly sparse, by which the update of ğ’›ğ»ğ‘£becomes
easier because it requires only for non-zero ğ›½ğ». The same plots on
the BZR, COX2, and DHFR datasets are in Fig. 13 in appendix.
Unfortunately, LAGRA is still computationally demanding com-
pared with other standard graph classification methods (Compu-
tational time of LAGRA and the GNNs are shown in Table 3 in
supplementary appendix F). However, to our knowledge, LAGRA
is the first method that can identify important attributed graphlets
 
2837Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 20 40 60 80 100
 index
100101102103TimeTraverse (Identify W)
Other processes
(a) AIDS
0 20 40 60 80 100
 index
101
100101102TimeTraverse (Identify W)
Other processes (b) ENZYMES
Figure 9: Transition of computational time (sec) on regular-
ization path. For each ğœ†, the total time and the time required
to traverse the graph mining tree (in other words, the time
required to identify W) is shown separately.
0 20 40 60 80 100
 index
100101102103104105
Mean |W|
#visit nodes
# candidate H
(a) AIDS
0 20 40 60 80 100
 index
100101102103104105106107Mean |W|
#visit nodes
# candidate H (b) ENZYMES
Figure 10: The average number of visited tree nodes and
|W|. Note that # candidate Hin ENZYMES is a lower bound
because we cannot count the exact number due to the large
computational cost.
through the exact match of labeled subgraphs, by which higher
interpretability is obtained than other approaches.
5 CONCLUSION
This paper proposed LAGRA (Learning Attributed GRAphlets),
which learns a prediction model that linearly combines attributed
graphlets (AGs). In LAGRA, graph structures of AGs are generated
through a graph mining algorithm, and attribute vectors are op-
timized as a continuous trainable parameters. To identify a small
number of AGs, the ğ¿1sparse penalty is imposed on coefficients of
AGs. We employed a block coordinate update based optimization
algorithm, in which an efficient pruning strategy was proposed by
combining the proximal gradient update and the graph mining tree
search. Our empirical evaluation showed that LAGRA has superior
or comparable performance with standard graph classification algo-
rithms. We further demonstrated that LAGRA actually can identify
a small number of discriminative AGs that have high interpretabil-
ity. On the other hand, there exist several important remaining
issues.
â€¢An obvious limitation of LAGRA is its scalability as discussed
in the end of Section 4.3.
â€¢Further, we only focus on the prediction task of a label as-
sociated with the entire graph. Another well-known settingis to predict label of each node in a single large graph, to
which current LAGRA is not applicable.
ACKNOWLEDGMENTS
This work was partially supported by MEXT KAKENHI (21H03498,
23K21696, 22H00300, 23K17817), and International Joint Usage/Research
Project with ICR, Kyoto University (2023-34, 2024-30).
REFERENCES
[1]Amir Beck and Marc Teboulle. 2009. A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences 2, 1
(2009), 183â€“202.
[2]Felix A. Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S. Schoenholz,
George E. Dahl, Oriol Vinyals, Steven Kearnes, Patrick F. Riley, and O. Anatole
von Lilienfeld. 2017. Prediction Errors of Molecular Machine Learning Models
Lower than Hybrid DFT Error. Journal of Chemical Theory and Computation 13,
11 (2017), 5255â€“5264.
[3]Aosong Feng, Chenyu You, Shiqiang Wang, and Leandros Tassiulas. 2022.
KerGNNs: Interpretable Graph Neural Networks with Graph Kernels. In Thirty-
Sixth AAAI Conference on Artificial Intelligence, AAAI. AAAI Press, 6614â€“6622.
[4]Aasa Feragen, Niklas Kasenburg, Jens Petersen, Marleen de Bruijne, and Karsten
Borgwardt. 2013. Scalable kernels for graphs with continuous attributes. In
Advances in Neural Information Processing Systems. 216â€“224.
[5]Jerome Friedman, Trevor Hastie, Holger HÃ¶fling, and Robert Tibshirani. 2007.
Pathwise coordinate optimization. The Annals of Applied Statistics 1, 2 (12 2007),
302â€“332.
[6]Josif Grabocka, Nicolas Schilling, Martin Wistuba, and Lars Schmidt-Thieme.
2014. Learning Time-Series Shapelets. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. Association
for Computing Machinery, New York, NY, USA, 392â€“401.
[7]Katarzyna Janocha and Wojciech Marian Czarnecki. 2016. On Loss Functions for
Deep Neural Networks in Classification. Schedae Informaticae 25 (2016), 49.
[8]Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net.
[9]Risi Kondor and Horace Pan. 2016. The multiscale Laplacian graph kernel. In
Advances in Neural Information Processing Systems. 2990â€“2998.
[10] Nils M. Kriege, Fredrik D. Johansson, and Christopher Morris. 2020. A survey on
graph kernels. Applied Network Science 5 (2020), 6.
[11] Steph-Yves Louis, Yong Zhao, Alireza Nasiri, Xiran Wang, Yuqi Song, Fei Liu, and
Jianjun Hu. 2020. Graph convolutional neural networks with global attention
for improved materials property prediction. Phys. Chem. Chem. Phys. 22 (2020),
18141â€“18148. Issue 32.
[12] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,
and Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for
learning with graphs. In ICML 2020 Workshop on Graph Representation Learning
and Beyond (GRL+ 2020). www.graphlearning.io
[13] Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, and
Ichiro Takeuchi. 2016. Safe pattern pruning: An efficient approach for predictive
pattern mining. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, 1785â€“1794.
[14] Marion Neumann, Roman Garnett, Christian Bauckhage, and Kristian Kersting.
2016. Propagation kernels: efficient graph kernels from propagated information.
Machine Learning 102, 2 (2016), 209â€“245.
[15] N. PrÅ¾ulj. 2007. Biological network comparison using graphlet degree distribution.
Bioinformatics 23, 2 (2007), e177â€“e183.
[16] Liva Ralaivola, Sanjay J. Swamidass, Hiroto Saigo, and Pierre Baldi. 2005. Graph
kernels for chemical informatics. Neural Networks 18, 8 (2005), 1093â€“1110.
[17] Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten
Borgwardt. 2009. Efficient graphlet kernels for large graph comparison. In
Artificial Intelligence and Statistics. 488â€“495.
[18] Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Kon-
stantinos Skianis, and Michalis Vazirgiannis. 2020. GraKeL: A Graph Kernel
Library in Python. Journal of Machine Learning Research 21, 54 (2020), 1â€“5.
[19] Marc Teboulle. 2017. A simplified view of first order methods for optimization.
Mathematical Programming 170 (2017), 67â€“96.
[20] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
[21] Tian Xie and Jeffrey C. Grossman. 2018. Crystal Graph Convolutional Neural
Networks for an Accurate and Interpretable Prediction of Material Properties.
Phys. Rev. Lett. 120 (2018), 145301. Issue 14.
 
2838KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
[22] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful
are Graph Neural Networks?. In International Conference on Learning Representa-
tions.
[23] Yangyang Xu and Wotao Yin. 2017. A Globally Convergent Algorithm for Non-
convex Optimization Based on Block Coordinate Update. Journal of Scientific
Computing 72 (2017), 700â€“734.
[24] Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-based substructure pattern
mining. In Proceedings. 2002 IEEE International Conference on Data Mining. IEEE,
721â€“724.
[25] J. You, J. M. Gomes-Selman, R. Ying, and J Leskovec. 2021. Identity-aware Graph
Neural Networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 35. 10737â€“10745.
[26] Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. 2020. XGNN: Towards Model-
Level Explanations of Graph Neural Networks. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. Asso-
ciation for Computing Machinery, New York, NY, USA, 430â€“438.
[27] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. 2022. Explainability in
Graph Neural Networks: A Taxonomic Survey. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2022).
[28] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An end-
to-end deep learning architecture for graph classification. In Proceedings of AAAI
Conference on Artificial Inteligence.
[29] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI Open 1 (2020), 57â€“81.
A UPDATE ğ›½0
The objective of ğ›½0can be re-written as
min
ğ›½0ğ‘›âˆ‘ï¸
ğ‘–=0max(1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0),0)2=min
ğ›½0âˆ‘ï¸
ğ‘–âˆˆI(1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0))2.
(12)
For simplicity, we assume that all ğâŠ¤
ğ‘–ğœ·forğ‘–âˆˆ[ğ‘›]have different
values (even when this does not hold, the optimal solution can be
obtained by the same approach). Depending on ğ›½(new)
0, elements in
I(new)={ğ‘–|1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½(new)
0)>0}changes in a piecewise
constant manner. The point that I(new)changes are characterized
by the solution of the equation ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0)=1(ğ‘–âˆˆ[ğ‘›])with
respect toğ›½0, i.e., there exist ğ‘›+1segments on the space of ğ›½0âˆˆR.
Letğµ(ğ‘˜)=[ğ›½ğ‘ ğ‘˜
0,ğ›½ğ‘’ğ‘˜
0]be theğ‘˜-th segment ( ğ‘˜âˆˆ{0,...,ğ‘›}) andI(ğ‘˜)
isI(new)whenğ›½(new)
0âˆˆğµ(ğ‘˜). Note that ğ›½ğ‘’ğ‘˜
0=ğ›½ğ‘ ğ‘˜+1
0(ğ‘˜âˆˆ [ğ‘›]),
which is the solution of ğ‘¦ğ‘˜(ğâŠ¤
ğ‘˜ğœ·+ğ›½0)=1, andğ›½ğ‘ 0
0=âˆ’âˆand
ğ›½ğ‘’ğ‘›
0=âˆ. Under the assumption of ğ›½0âˆˆğµ(ğ‘˜), the optimal ğ›½0
isË†ğ›½(ğ‘˜)
0=Ã
ğ‘–âˆˆI(ğ‘˜)(ğ‘¦ğ‘–âˆ’ğâŠ¤
ğ‘–ğœ·)
|I(ğ‘˜)|. Since (12) is convex with respect to
ğ›½0, the obtained Ë†ğ›½(ğ‘˜)
0must be the optimal solution if it satisfies
Ë†ğ›½(ğ‘˜)
0âˆˆğµ(ğ‘˜). Thus, the optimal ğ›½0can be found by calculating Ë†ğ›½(ğ‘˜)
0
for allğ‘˜âˆˆ{0,...,ğ‘›}.
B PROOF OF THEOREM 2.1
From the definition of AGIS, the following monotonicity property
is guaranteed:
ğ¿(ğ»â€²)âŠ’ğ¿(ğ»)andğ»,ğ»â€²âˆˆW â‡’ğœ“(ğºğ‘–;ğ»)â‰¥ğœ“(ğºğ‘–;ğ»â€²)
Letğ‘€(ğºğ‘–;ğ»)be the set of injections ğ‘€betweenğºğ‘–andğ». The
above monotonicity property can be easily verified from the fact
min
ğ‘šâˆˆğ‘€(ğºğ‘–;ğ»)ğ·(ğ‘š)
ğ»,ğºğ‘–â‰¤ min
ğ‘šâˆˆğ‘€(ğºğ‘–;ğ»â€²)ğ·(ğ‘š)
ğ»â€²,ğºğ‘–.Define(
ğ‘ğ‘–=ğ‘¦ğ‘–(1âˆ’ğ‘¦ğ‘–(ğâŠ¤
ğ‘–ğœ·+ğ›½0))ifğ‘–âˆˆI,
ğ‘ğ‘–=0 otherwise .
Note that the sign of ğ‘ğ‘–is same asğ‘¦ğ‘–. Usingğ‘ğ‘–, we re-write ğ‘”ğ»â€²(ğœ·)
as
ğ‘”ğ»â€²(ğœ·)=âˆ‘ï¸
ğ‘–âˆˆIğ‘ğ‘–ğœ“(ğºğ‘–;ğ»â€²)
=âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–>0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»â€²)+âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–<0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»â€²).
From the monotonicity inequality ğœ“(ğºğ‘–;ğ»)â‰¥ğœ“(ğºğ‘–;ğ»â€²), we seeâˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–<0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»)â‰¤âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–<0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»â€²)â‰¤ğ‘”ğ»â€²(ğœ·)
â‰¤âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–>0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»â€²)â‰¤âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–>0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»).
From these inequalities, we obtain
|ğ‘”ğ»â€²(ğœ·)|â‰¤ maxï£±ï£´ï£´ ï£²
ï£´ï£´ï£³âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–>0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»),âˆ’âˆ‘ï¸
ğ‘–âˆˆIâˆ©{ğ‘–|ğ‘¦ğ‘–<0}ğ‘ğ‘–ğœ“(ğºğ‘–;ğ»)ï£¼ï£´ï£´ ï£½
ï£´ï£´ï£¾.
C BOUNDS FOR LOGISTIC LOSS
The logistic loss function for the multi-class label ğ‘¦ğ‘–âˆˆ{1,...,ğ¾}is
â„“(ğ‘¦ğ‘–,ğ‘“(ğºğ‘–))=âˆ’ğ¾âˆ‘ï¸
ğ‘˜=1I(ğ‘¦ğ‘–=ğ‘˜)logexp
ğâŠ¤
ğ‘–ğœ·(ğ‘˜)+ğ›½(ğ‘˜)
0
Ãğ¾
ğ‘˜â€²=1exp
ğâŠ¤
ğ‘–ğœ·(ğ‘˜â€²)+ğ›½(ğ‘˜â€²)
0
=âˆ’
ğâŠ¤
ğ‘–ğœ·(ğ‘¦ğ‘–)+ğ›½(ğ‘¦ğ‘–)
0
+ğ¾âˆ‘ï¸
ğ‘˜=1exp
ğâŠ¤
ğ‘–ğœ·(ğ‘˜)+ğ›½(ğ‘˜)
0
,
where ğœ·(ğ‘˜)andğ›½(ğ‘˜)
0are parameters for the ğ‘˜-th class. The deriva-
tive ofâ„“(ğ‘¦ğ‘–,ğ‘“(ğºğ‘–))with respect to ğ›½(ğ‘˜)
ğ»is
ğ‘”(ğ‘˜)
ğ»({ğœ·(ğ‘˜)}ğ¾
ğ‘˜=1)
=âˆ’ğ‘›âˆ‘ï¸
ğ‘–=1I(ğ‘¦ğ‘–=ğ‘˜)ğœ“(ğºğ‘–;ğ»)+ğ‘›âˆ‘ï¸
ğ‘–=1exp
ğâŠ¤
ğ‘–ğœ·(ğ‘¦ğ‘–)+ğ›½(ğ‘¦ğ‘–)
0
ğœ“(ğºğ‘–;ğ»).
=ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘âˆ’
ğ‘–ğœ“(ğºğ‘–;ğ»)+ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘+
ğ‘–ğœ“(ğºğ‘–;ğ»),
whereğ‘âˆ’
ğ‘–=âˆ’I(ğ‘¦ğ‘–=ğ‘˜)â‰¤0andğ‘+
ğ‘–=exp
ğâŠ¤
ğ‘–ğœ·(ğ‘¦ğ‘–)+ğ›½(ğ‘¦ğ‘–)
0
>0
are constants for any ğ». Then, the upper bound of |ğ‘”(ğ‘˜)
ğ»â€²({ğœ·(ğ‘˜)}ğ¾
ğ‘˜=1)|
for anyğ¿(ğ»â€²)âŠ’ğ¿(ğ»)can be obtained by the same derivation as
appendix B
D DERIVATION OF ğœ†max
When ğœ·=0, the objective function of ğ›½0is written as the following
piecewise quadratic function:
min
ğ›½01
2âˆ‘ï¸
ğ‘–âˆˆI(ğ›½0)(1âˆ’ğ‘¦ğ‘–ğ›½0)2s.t.I(ğ›½0)=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³{ğ‘–|ğ‘¦ğ‘–>0}ğ›½0â‰¤âˆ’1,
[ğ‘›]ğ›½0âˆˆ[âˆ’1,1],
{ğ‘–|ğ‘¦ğ‘–<0}ğ›½0â‰¥1.
In the region ğ›½0â‰¤âˆ’1, the minimum value is achieved by ğ›½0=âˆ’1,
and forğ›½0â‰¥1, the minimum value is achieved by ğ›½0=1. This
 
2839Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Statistics of datasets.
AIDS BZR COX2 DHFR ENZYMES PRO
TEINS SYN
THETIC
#
instances 2000 405 467 756 200 1113 300
Dim.
of attribute vector ğ‘‘ 4 3 3 3 18 1 1
A
vg. # nodes 15.69 35.75 41.22 42.43 32.58 39.06 100.00
A
vg. # edges 16.20 38.36 43.45 44.54 60.78 72.82 196.00
0
0
0
00
0
H:0.211026
NodeID:0
NodeID:1
NodeID:2
NodeID:3NodeID:4
NodeID:51
01NodeID:0
1
01NodeID:1
1
01NodeID:2
1
01NodeID:3
01231
01NodeID:401231
01NodeID:5
(a)
000101
H:0.186228
NodeID:0NodeID:1NodeID:2NodeID:3NodeID:4NodeID:5
0.5
0.00.5NodeID:0
0.5
0.00.5NodeID:1
0.5
0.00.5NodeID:2
0.5
0.00.5NodeID:3
01230.5
0.00.5NodeID:401230.5
0.00.5NodeID:5 (b)
0
0
0
0
0
H:-2.063934
NodeID:0
NodeID:1
NodeID:2
NodeID:3
NodeID:42.5
0.02.5NodeID:0
2.5
0.02.5NodeID:1
2.5
0.02.5NodeID:2
2.5
0.02.5NodeID:3
01232.5
0.02.5NodeID:4 (c)
000
0
00
H:-1.234134
NodeID:0NodeID:1
NodeID:2
NodeID:3
NodeID:4NodeID:52.5
0.02.5NodeID:0
2.5
0.02.5NodeID:1
2.5
0.02.5NodeID:2
2.5
0.02.5NodeID:3
01232.5
0.02.5NodeID:401232.5
0.02.5NodeID:5 (d)
Figure 11: Selected important AGs for AIDS dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for
the two largest negative coefficients.
5
 4
 3
 2
 1
 0 1
f(G_i)8
6
4
2
024LDA (in {S})
(a) AIDS Training data
4
 3
 2
 1
 0 1
f(G_i)6
4
2
02LDA (in {S})
 (b) AIDS Test data
2
 1
 0 1
f(G_i)2
024LDA (in {S})
 (c) COX2 Training data
2
 1
 0 1
f(G_i)2
0246LDA (in {S})
 (d) COX2 Test data
1
 0 1
f(G_i)4
3
2
1
012LDA (in {S})
(e) DHFR Training data
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
f(G_i)3
2
1
0123LDA (in {S})
 (f) DHFR Test data
1
 0 1 2
f(G_i)4
3
2
1
012LDA (in {S})
 (g) ENZYMES Training data
1
 0 1 2
f(G_i)1.5
1.0
0.5
0.00.51.01.52.0LDA (in {S})
 (h) ENZYMES Test data
0.5
 0.0 0.5 1.0
f(G_i)2
1
012LDA (in {S})
(i) PROTEINS Training data
0.5
 0.0 0.5 1.0
f(G_i)2
1
0123LDA (in {S})
 (j) PROTEINS Test data
4
 2
 0 2 4
f(G_i)5.0
2.5
0.02.55.07.5LDA (in {S})
 (k) SYNTHETIC Training data
4
 2
 0 2 4
f(G_i)7.5
5.0
2.5
0.02.55.07.5LDA (in {S})
 (l) SYNTHETIC Test data
Figure 12: Scatter plots by ğ‘“(ğºğ‘–)and LDA.
 
2840KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
Table 3: Training time (sec) of GNNs and LAGRA. The hyper-parameter setting of each method is the best setting selected by
the their validation performance. Note that precise comparisons of running times have difficulty due to the following two
reasons. 1) For LAGRA, the computational time for Algorithm 1 is shown, which contains the sum of the computational times
for all values of ğœ†(i.e., all the times shown in Fig 9 in the main text). 2) The implementation of GNNs are by Python, while
LAGRA is implemented by C++.
AIDS BZR COX2 DHFR ENZYMES PRO
TEINS SYN
THETIC
DGCNN 7.63e+02 3.02e+02 3.52e+02 5.13e+02 9.23e+01 4.51e+02 2.12e+02
GA
T 2.48e+02 7.95e+01 1.11e+02 1.76e+02 5.02e+01 2.60e+02 1.83e+02
GCN 1.70e+02 6.62e+01 6.98e+01 1.23e+02 3.31e+01 1.94e+02 1.41e+02
GIN 1.96e+02 2.44e+02 4.40e+02 3.08e+02 2.60e+02 5.53e+02 5.14e+02
LA
GRA 8.58e+04 1.59e+04 5.51e+03 4.45e+04 1.72e+04 4.67e+05 1.08e+04
0 20 40 60 80 100
 index
101102103104105
Mean |W|
#visit nodes
# candidate H
(
a) BZR
0 20 40 60 80 100
 index
100101102103104105
Mean |W|
#visit nodes
# candidate H (
b) COX2
0 20 40 60 80 100
 index
100101102103104105
Mean |W|
#visit nodes
# candidate H (
c) DHFR
Figure 13: The average number of visited tree nodes and |W|.
indicates that the optimal solution should exist in ğ›½0âˆˆ [âˆ’ 1,1]
because the objective function is a smooth convex function. There-
fore, the minimum value in the region ğ›½0âˆˆ[âˆ’ 1,1]achieved by
ğ›½0=Ã
ğ‘–âˆˆ[ğ‘›]ğ‘¦ğ‘–/ğ‘›, defined as Â¯ğ‘¦, becomes the optimal solution.
When ğœ·=0andğ›½0=Â¯ğ‘¦, we obtain
ğ‘”ğ»(0)=âˆ‘ï¸
ğ‘–âˆˆ[ğ‘›]ğ‘¦ğ‘–ğœ“(ğºğ‘–;ğ»)(1âˆ’ğ‘¦ğ‘–Â¯ğ‘¦).
From (9), we see that |ğ‘”ğ»(0)|=ğœ†is the threshold that ğ›½ğ»have a
non-zero value. This means that ğ»having the maximum |ğ‘”ğ»(0)|is
the firstğ»that start having a non-zero value by decreasing ğœ†from
âˆ. Therefore, we obtain
ğœ†max=max
ğ»âˆˆHâˆ‘ï¸
ğ‘–âˆˆ[ğ‘›]ğ‘¦ğ‘–ğœ“(ğºğ‘–;ğ»)(1âˆ’ğ‘¦ğ‘–Â¯ğ‘¦).
E STATISTICS OF DATASETS
Statistics of datasets is show in Table 2. AIDS, BZR, COX2, and
DHFR are molecule graph datasets. The attributes of the AIDS
dataset are chem ,charge ,x, and y(details are not shown in the data
repository [ 12]). The BZR, COX2, and DHFR datasets contain 3D
coordinate attributes (Although we used 3D coordinate attributes
directly, using rotation invariant representations might be more
appropriate for these coordinate information. It is possible to apply
LAGRA to rotation invariant 3D descriptors such as the atomic
distance distribution around each atom node). In the ENZYMES
and PROTEINS datasets, the nodes are secondary structure ele-
ments, and attributes contain physical and chemical measurements.
In the SYNTHETIC dataset, attributes are created from normal
distributions. See e.g., [14] for more detail.F ADDITIONAL EVALUATION OF
COMPUTATIONAL TIME
Table 3 shows computational time of GNNs and LAGRA. As de-
scribed in the second last paragraph of Sec 4.3, we do not claim
computational efficiency compared with other methods that do not
enumerate all subgraphs. Comparing with the naive enumeration
without our gradient pruning, LAGRA is highly efficient as shown
in the last paragraph of Sec 4.3. Figure 13 is additional evaluations
of the number of visited nodes shown in Fig. 10
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
 
2841