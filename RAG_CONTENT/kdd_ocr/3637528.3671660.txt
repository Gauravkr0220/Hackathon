HiFGL: A Hierarchical Framework for Cross-silo Cross-device
Federated Graph Learning
Zhuoning Guo
The Hong Kong University of Science and Technology
(Guangzhou) & The Hong Kong University of Science and
Technology
Guangzhou & Hong Kong, China
zhuoning.guo@connect.ust.hkDuanyi Yao
The Hong Kong University of Science and Technology
Hong Kong, China
dyao@connect.ust.hk
Qiang Yang
The Hong Kong University of Science and Technology
Hong Kong, China
qyang@cse.ust.hkHao Liuâˆ—
The Hong Kong University of Science and Technology
(Guangzhou) & The Hong Kong University of Science and
Technology
Guangzhou & Hong Kong, China
liuh@ust.hk
ABSTRACT
Federated Graph Learning (FGL) has emerged as a promising way
to learn high-quality representations from distributed graph data
with privacy preservation. Despite considerable efforts have been
made for FGL under either cross-device or cross-silo paradigm,
how to effectively capture graph knowledge in a more complicated
cross-silo cross-device environment remains an under-explored
problem. However, this task is challenging because of the inherent
hierarchy and heterogeneity of decentralized clients, diversified
privacy constraints in different clients, and the cross-client graph
integrity requirement. To this end, in this paper, we propose a
Hierarchical Federated Graph Learning (HiFGL) framework for
cross-silo cross-device FGL. Specifically, we devise a unified hi-
erarchical architecture to safeguard federated GNN training on
heterogeneous clients while ensuring graph integrity. Moreover,
we propose a Secret Message Passing (SecMP) scheme to shield
unauthorized access to subgraph-level and node-level sensitive in-
formation simultaneously. Theoretical analysis proves that HiFGL
achieves multi-level privacy preservation with complexity guar-
antees. Extensive experiments on real-world datasets validate the
superiority of the proposed framework against several baselines.
Furthermore, HiFGLâ€™s versatile nature allows for its application in
either solely cross-silo or cross-device settings, further broadening
its utility in real-world FGL applications.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671660CCS CONCEPTS
â€¢Theory of computation â†’Graph algorithms analysis; â€¢
Security and privacy â†’Database and storage security.
KEYWORDS
federated graph learning, graph neural network, multi-level privacy
preservation
ACM Reference Format:
Zhuoning Guo, Duanyi Yao, Qiang Yang, and Hao Liuâˆ—. 2024. HiFGL: A Hier-
archical Framework for Cross-silo Cross-device Federated Graph Learning.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671660
1 INTRODUCTION
Federated Learning (FL) has emerged as a transformative approach
by enabling multiple parties to contribute to a shared machine
learning model without the need for direct data exchange [ 34,36].
Along this line, Federated Graph Learning (FGL) has been proposed
to collaboratively train the Graph Neural Network (GNN) to ex-
tract distributed knowledge from interconnected subgraphs held
privately by each party. Recently, FGL has been adopted to a wide
range of application domains, such as finance [ 29], recommender
system [20], and transportation [38].
Existing FGL approaches predominantly revolve around two
paradigms [ 9].1) Cross-silo FGL [2,31,40], as illustrated in Fig-
ure 1 (a), formulates data silos as clients, each of which possessing
a subgraph consists of nodes and connected edges. This paradigm
is particularly relevant for institutions wishing to maintain pri-
vate subgraphs while contributing to a global graph structure, such
as cross-platform recommendation for E-commerce [ 16].2) Cross-
device FGL [23,25,33], as shown in Figure 1 (b), regards each device
as a client, which holding a node and its associated edges. It is more
suitable for scenarios where numerous devices maintain private
connections within a global graph, e.g., user-centric social net-
works [ 27]. Table 1 summarizes the privacy, utility, and efficiency
tradeoff of existing FGL approaches.
 
968
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
(a) Cross-silo FGL
Cross-client EdgeIntra-client EdgeNode
Server
Feature(b) Cross-device FGL (c) Cross-silo Cross-device FGL
1
56
3
7
2 4
1
 2
 3
 4
1
5
6
7
3
4
2
Client for silo
Client for device
Figure 1: Illustration of three FGL paradigms: cross-silo, cross-device, and cross-silo cross-device FGL.
Despite the success of the above two paradigms in unleashing
the power of isolated graph data, none of them can be directly
adopted to fully address the complexities of a mixed cross-silo
cross-device environment, as demonstrated in Figure 1 (c). In real-
world scenarios, institutions and users may have common privacy
concerns but with varying privacy and utility requirements [ 42,43].
We illustrate the use case of FGL under the cross-silo cross-device
setting via the following real-world application.
Example 1. Anomaly detection in financial transactions..
In this scenario, banks oversee customer transactions, forming a fed-
erated graph where each bank controls a subgraph of accounts (i.e.,
nodes) and transactions (i.e., edges). The collaborative analysis of
the cross-bank federated graph can enhance the anomaly detection
task. However, the transaction records are confidential, and regula-
tions forbid the banks to disclose the subgraph structure. Meanwhile,
customers are also unwilling to expose their sensitive information to
institutions to avoid personal privacy leakage.
To bridge the gap, we investigate the cross-silo cross-device fed-
erated graph learning problem, where institutions and customer
devices collaboratively optimize a GNN model under their diverged
privacy constraints. However, three major technical challenges
arise. 1) Inherent hierarchy and heterogeneity of decentral-
ized clients. The participants in cross-silo cross-device FGL natu-
rally form a hierarchy, where an institution client may have a more
comprehensive local structure of devices, and devices may preserve
their local sensitive features. Besides, the cross-silo cross-device
FGL involves clients with varying computational capabilities. Such
hierarchy and heterogeneity pose significant challenges in design-
ing a unified FGL framework that can effectively operate across a
varied landscape. 2) Diversified privacy constraints in different
clients. As depicted in Table 1, privacy concerns in FGL vary across
different types of clients. In a word, cross-silo FGL focuses more on
subgraph-level privacy (i.e., the privacy of structures), while cross-
device FGL weighs more on node-level privacy (i.e., the privacy of
features). The varied privacy requirements necessitate a flexible
approach that can adapt to the specific needs of different types
of clients while ensuring the overall utility of the federated graph
learning process. 3) Cross-client graph integrity. In FGL, each
client contributes to a portion of the overall graph. Maintaining
the integrity of graph data across multiple clients is critical to the
utility of the joint model. However, it is a non-trivial task to protect
the cross-client graph information without sacrificing the model
performance. For example, FedSage+ [ 40] protects cross-silo struc-
tures by generatively approximating edges across local subgraphs,
while Glint [ 19] chooses to expose node embeddings to guaranteemodel utility. It is challenging to preserve graph privacy without
sacrificing graph integrity.
In this paper, we propose a unified cross-silo cross-device frame-
work, Hierarchical Federated Graph Learning (HiFGL), for multi-
level privacy preservation (i.e., both subgraph-level and node-level)
without sacrificing graph information integrity. Specifically, we
first construct a hierarchical architecture comprising three key
components: device-client, silo-client, and server. The hierarchical
architecture enables federated graph learning with the flexibility
of applying diversified privacy preservation strategies on different
types of clients. Moreover, we propose a Secret Message Passing
(SecMP ) scheme for multi-level privacy protection. In particular,
a Neighbor-Agnostic Aggregation protocol and a Hierarchical La-
grangian Embedding protocol are proposed to reduce subgraph-
level and node-level privacy leakage, respectively. Furthermore,
a tailored resource-efficient optimization algorithm is introduced
for the unified framework. Notably, HiFGL can also be applied in
solely cross-silo or cross-device scenarios, and is compatible with
diverse FL algorithms (e.g., FedAvg [ 22], FedProx [ 15]) and GNN
variants (e.g., GCN [ 13], GraphSage [ 7]), further broaden its utility
in real-world FGL applications.
The main contributions of our work are listed as follows: 1) To
our knowledge, HiFGL is the first framework tailored for the cross-
silo cross-device federated graph learning problem, which has rarely
been studied before. 2) We construct a hierarchical architecture
to enable flexible privacy-preservation of heterogeneous clients
without sacrificing graph integrity. 3) We propose a secret message
passing scheme to simultaneously safeguard subgraph-level and
node-level privacy against semi-honest adversaries during collabo-
rative GNN training. 4) We theoretically analyze the privacy and
complexity of HiFGL. The results show that our methods not only
preserve subgraph-level and node-level privacy but also achieve
information integrity with guaranteed complexity. 5) We conduct
extensive experiments on real-world graph datasets, and the results
demonstrate that HiFGL outperforms state-of-the-art baselines in
learning more effective GNN models.
2 RELATED WORK
In this section, we review state-of-the-art federated graph learning
approaches, including two paradigms: cross-silo and cross-device.
2.1 Cross-silo Federated Graph Learning
A common scenario of cross-silo FGL is that institutions collabora-
tively learn models while keeping local data private. Besides various
 
969HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 1: Comparison of typical FGL works and HiFGL in the dimension of Multi-level Privacy Preservation (including Subgraph-
level andNode-level ),Information Integrity, and Efficiency. We evaluate properties by Low, Medium, or High by considering how
much a framework satisfies requirements relatively among baselines.
Model FGL Paradigm Multi-level Privacy Preservation Information Integrity Efficiency
Subgraph-level Node-level
GCN [13]+FedAvg [22] Cross-silo Medium Low Medium High
FedSage+ [40] Cross-silo Medium Low Medium Low
FedPUB [2] Cross-silo Medium Low Medium Medium
GraphFL [31] Cross-silo Medium Low Medium Medium
FedGraph [4] Cross-silo High Low High Low
Glint [19] Cross-silo Low High High Medium
PPSGCN [39] Cross-silo Medium Low High Low
FedCog [14] Cross-silo Medium Low High Low
CNFGNN [23] Cross-device - Medium High Medium
FedPerGNN [33] Cross-device - High High Low
FedWalk [25] Cross-device - High High Medium
Lumos [26] Cross-device - High High Low
SemiDFEGL [27] Cross-device - High High Medium
HiFGL (Ours) Cross-silo Cross-device (Both) High High High Medium
GNN modules for improving effectiveness, existing cross-silo FGL
works have different strategies for processing graph structure data.
The first strategy is to drop the cross-client edges to prevent data
leakage across clients. An intuitive way is that each client holds a
GNN (e.g., GCN [ 13]) and a subgraph without cross-client edges and
trains a global model through FedAvg [ 22], a custom FL framework.
In addition, FedSage+ [ 40] generates local nodesâ€™ neighbor features
to offset the ignorance of cross-client edges due to subgraph-level
privacy preservation, which improves predicting ability with sig-
nificant extra training costs. FedPUB [ 2] focuses on the prediction
improvement via personalized masked graph convolutional net-
work, where their pairwise similarity is measured by subgraph
representation. GraphFL [ 31] solves the semi-supervised graph
learning problem on federated unconnected subgraphs, where node
label domains vary and are not identically distributed. Another
strategy is to maintain cross-client edges either stored by the server
or clients. In this way, we have to face either less practicability
or more privacy leakage. For example, FedGraph [ 4] uses a cen-
tral server to keep cross-client edges and federally trains GNNs
with graph sampling through huge communication for expanding
neighbors between clients and the server. Unfortunately, the frame-
work is not practical in the real world [ 18]. Glint [ 19] decentralized
trains graph convolutional networks, where nodes are fully aware
of cross-client neighbors. PPSGCN [ 39] leverages the graph sam-
pling method to enhance efficiency and scalability, which hides
node information but exposes nodes across clients. FedCog [ 14]
decouples subgraphs according to intra- or cross-client edges to
construct a border graph for each client, which is a bipartite graph
between internal nodes and external nodes, with two separated
graph learning operations.
We propose quantifying subgraph-level privacy leakage in Ap-
pendix A.1. The results demonstrate the unsolved issue of balancing
subgraph-level privacy protection and cross-client graph integrity.
Our approach aims to achieve two objectives simultaneously.2.2 Cross-device Federated Graph Learning
Cross-device FGL assumes users hold private data and learn mod-
els without accessing private data. To name a few, CNFGNN [ 23]
combines the spatiotemporal GNN model with FL, where the data
storage scheme hides the original information of nodes but ex-
poses hidden states and neighboring nodes. FedPerGNN [ 33] ap-
plies GNNs for recommender systems where a user keeps a local
user-item subgraph. A privacy-preserving graph expansion method
anonymously acquires neighbor information with high commu-
nication costs. FedWalk [ 25] adopts the random walk algorithm
for federated graph node embedding learning with node-level vis-
ibility for covering raw graph information. Lumos [ 26] utilizes
local differential privacy and zero-knowledge protocol to protect
node featuresâ€™ and degreesâ€™ privacy among decentralized devices.
SemiDFEGL [ 27] collaborates ego graph-corresponded devices via
a peer-to-peer manner for scalability improvement and communi-
cation reduction on recommendation tasks. In summary, existing
works either utilize inefficient privacy-preserving schemes to align
pairwise relationships or rely on a server with extreme representa-
tion exposure to achieve cross-client edge integrity. Our framework
constructs a three-level architecture with multiple privacy preser-
vation protocols to achieve effective and secure FGL.
3 PRELIMINARIES
3.1 Federated Graph Definition
In FGL, graphs are required to be distributively stored with privacy
preservation. We focus on the scenario where a complete graph
consists of subgraphs without overlapped nodes and nodes have
their features and edges [ 40]. Letğ‘£denote a node associated with a
multi-dimensional feature vector â„ğ‘£, an edgeğ‘’=(ğ‘£ğ‘–,ğ‘£ğ‘—)is defined
as a directed linkage from ğ‘£ğ‘–toğ‘£ğ‘—. A subgraph is defined as Gğ‘ =
(Vğ‘ ,Eğ‘ ), where Vğ‘ is the node set and Eğ‘ is the edge set. Note
âˆƒğ‘£ğ‘—âˆ‰Vğ‘ ,ğ‘’=(ğ‘£ğ‘–,ğ‘£ğ‘—)âˆˆEğ‘ ,i.e., a node in a subgraph can connect
 
970KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
with the nodes from other subgraphs. Then, we define the federated
graph as a union of subgraphs under a FL setting.
Definition 1. Federated Graph. Federated graph is denoted
asG=(V,E)={Gğ‘ |ğ‘ =1,2,Â·Â·Â·,|G|}, where Vis the node set,
Eis the edge set, Gğ‘ =(Vğ‘ ,Eğ‘ )is theğ‘ -th subgraph, and |G|is
the number of subgraphs. Gsatisfies 1) V1âˆªV2âˆªÂ·Â·Â·âˆª V|G|=V,
2)E1âˆªE2âˆªÂ·Â·Â·âˆª E|G|âŠ†E, 3)Vğ‘–âˆ©Vğ‘—=âˆ…,âˆ€1â‰¤ğ‘–â‰¤|G|,1â‰¤ğ‘—â‰¤
|G|,ğ‘–â‰ ğ‘—, 4)Eğ‘–âˆ©Eğ‘—=âˆ…,âˆ€1â‰¤ğ‘–â‰¤|G|,1â‰¤ğ‘—â‰¤|G|,ğ‘–â‰ ğ‘—.
In this work, we associate each node with a device and each
subgraph with a data silo. Under the cross-silo and cross-device
setting, nodes and graph structures may be placed in multiple het-
erogeneous clients for federated graph learning.
3.2 Problem Formulation
In this work, we aim to collaboratively train GNNs over distributed
graph data in a federated way, which is formally defined below.
Problem 1. Cross-silo Cross-device Federated Graph Learn-
ing. Given a federared graph G=(V,E)consisting of the subgraph
set{Gğ‘ }|G|
1. We aim to learn the parameter ğœƒğ‘ ofğ‘ -th silo-client for
the global GNN model Gto minimize the global loss for downstream
predictive or regressive tasks,
{ğœƒ1,Â·Â·Â·,ğœƒ|G|}=arg minâˆ‘ï¸
L(G(Gğ‘ ;ğœƒğ‘ ),ğ‘Œğ‘ ), (1)
whereğ‘–=1,2,Â·Â·Â·,|G|andLdenotes the loss function between
estimationG(Gğ‘ ;ğœƒğ‘ )and ground truth ğ‘Œğ‘ .
3.3 Threat Model
Here we define the threat model for cross-silo cross-device FGL,
which concurrently considers nodesâ€™ and subgraphsâ€™ privacy. Specif-
ically, we assume all parties are semi-honest, i.e., honest-but-curious,
which means the adversary will try its best to obtain private infor-
mation but not break protocols or cause malicious damage to the
modelâ€™s ability. We first define nodesâ€™ and subgraphsâ€™ privacy as
node-level privacy and subgraph-level privacy, respectively.
Definition 2. Node-level Privacy. A node usually represents a
device of a user in FGL applications, which should not leak the privacy
of the raw features and adjacent neighbors to other participants, i.e.,
devices and data silos.
Definition 3. Subgraph-level Privacy. A subgraph corresponds
to a data silo of an institution in FGL applications, which should not
expose its subgraph to other data silos and devices.
As illustrated in Example 1, where banks correspond to sub-
graphs and accounts correspond to nodes, the corresponding partic-
ipants in cross-silo cross-device FGL are inherently heterogeneous.
Then, we introduce the potential attack types between heteroge-
neous clients, including node-node attack, subgraph-node attack,
and subgraph-subgraph attack.
Definition 4. Node-Node Attack. A node acts as an attacker,
and another node acts as a defender. Nodes only see their features thus
they hope to obtain neighbor embeddings to infer more information.
Definition 5. Subgraph-Node Attack. A subgraph acts as an
attacker, and a node acts as a defender. Subgraphs have no accessto nodesâ€™ features and neighbors, and thus subgraphs are desired to
utilize this information by cooperating with nodes. This cooperation
may raise data leakage from nodes to subgraphs.
Definition 6. Subgraph-Subgraph Attack. A subgraph acts
as an attacker, and another subgraph acts as a defender. The adver-
sary hopes to acquire other subgraphs as well as cross-client edges to
enhance their subgraph.
Note we donâ€™t assume nodes will attack subgraphs. Due to the
hierarchy of heterogenous clients, the device can access the model
from their corresponding data silo.
Overall, in this work, we aim to preserve node-level privacy
against node-node attacks and subgraph-node attacks, and preserve
subgraph-level privacy against subgraph-subgraph attacks.
4 FRAMEWORK
In this section, we present the proposed Hierarchical Federated
Graph Learning (HiFGL) framework in detail. In particular, we
first present the hierarchical architecture for heterogeneous clients.
Then, we introduce the Secret Message Passing (SecMP) scheme
to achieve multi-level privacy preservation. Finally, we detail the
optimization scheme tailored for the framework.
4.1 The Hierarchical Architecture
We construct the hierarchical architecture of HiFGL consisting of
three modules, i.e., device-client, silo-client, and server, in bottom-
up order, as shown in Figure 2. Generally, under this hierarchy,
modules are set to administrate the ones at a subordinate level
and keep information private against the ones at a superordinate
level1. Specifically, the device-client holds local data and cooperates
with its silo-client for learning without data exposure. A silo-client
preserves local models that are federally optimized with the server,
administrates device-clients containing graph data, and determines
the privatization method. The server executes the federated op-
timization method for FL. In the following parts, we detail three
modules and their communication and visibility.
4.1.1 Device-client. The device-client has two roles, including stor-
ing the ego graph of each node and computing node gradients with
the silo-client, as shown in Figure 2 (c).
1) Storage. A device-client stores an ego graph consisting of the
target nodeâ€™s features and its neighbors.
Definition 7. Ego Graph. An ego graph stores ğ‘—-th nodeâ€™s feature
â„(0)
ğ‘—, labelğ‘¦ğ‘—, and 1-hop directed neighbors. We denote the ego graph
asËœğ‘”ğ‘—=(â„(0)
ğ‘—,Nğ‘—).
We store data separately in each device for two reasons. First,
storing data at the device level can prevent node features from
being seen directly by adversaries at the silo or server level. Second,
cross-client edges are risky for exposing the siloâ€™s node identities.
Suppose a directed cross-client edge connects a source and a target
node from two silos. However, storing the edge in any one of the
two silos will violate subgraph-level privacy requirements because
1In this paper, we use â€œthe device-clientâ€™s silo-clientâ€ to denote â€œthe silo-client that
administrates the device-clientâ€ and â€œthe silo-clientâ€™s device-clientâ€ to denote â€œthe
device-clients administrated by the silo-clientâ€.
 
971HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
(b) Silo-client(a) Server
(c) Device-client
Privatized 
EmbeddingModel Gradient Model Parameter
Model Parameter
 Model Parameter
Invisiable
GNN
Encoding-Decoding 
Parameters
Optimization 
Scheme
Feature
Neighbors
(d) Communication & Visibility
Invisiable Invisiable
Figure 2: The architecture of HiFGL.
there will be at least one node to be seen by a silo-client that is not
its own. To address this challenge, we propose allowing two devices
corresponding to the two nodes to have access to this cross-client
edge in order to reduce the visibility of silos.
To construct ego graphs, we allow any two devices to connect
in a peer-to-peer manner to ensure that the connected edges are
only known by these two device-clients. For instance, two users
can build a friend relationship without the awareness of the social
network administrator. In the semi-honest setting, we consider the
connected edges to be true. In this way, we can construct the graph
in a privacy-preserving and decentralized way.
2) Gradients computation Considering the limited computing
power and local data size, instead of optimizing the full parameters
of the local model, the device-clients are only required to calcu-
late gradients associated with their local data. Specifically, for a
modelGparameterized by ğœƒ, theğ‘—-th device-client computes the
corresponding gradients by âˆ‡ğ‘—ğœƒ=âˆ‡ğ‘“ğ‘™(Gğœƒ(â„(0)
ğ‘—),ğ‘¦ğ‘—), whereğ‘“ğ‘™(Â·)
is the loss function. These gradients are not considered as private
information and can be shared with the corresponding silo-client.
4.1.2 Silo-client. As shown in Figure 2 (b), the silo-client is a mod-
ule between the server and the device-client, which is in charge
of local model optimization based on data in its device-clients. We
introduce two roles of each silo-client ğ¶ğ‘–, including storage and
model update.
1) Storage. The silo-client stores three elements, the subgraph,
the local model, and encoding-decoding parameters. First, ğ¶ğ‘–indi-
rectly keeps a federated subgraph Ëœğºğ‘–consisting of federated ego
graphs distributed in its device-clients Dğ‘–={ğ·ğ‘—|ğ‘—=1,2,Â·Â·Â·}. We
define the federated subgraph as below.
Definition 8. Federated Subgraph. Federated subgraph is de-
noted as Ëœğº={ğ·ğ‘—|ğ‘—=1,2,Â·Â·Â·}, where each ğ·ğ‘—stores a Ëœğ‘”ğ‘—, theğ‘—-th
ego graph whose node ğ‘£ğ‘—is in subgraph ğº.Ëœğºonly keeps the identities
of the subordinate device-clients, but does not store any features and
neighbor information.
The federated subgraph avoids exposure of node features and
graph structures and therefore protects device-level privacy in the
federated learning process.Moreover, since the silo-client usually has more powerful com-
puting resources, we let ğ¶ğ‘–keep the local GNN Gğ‘–for optimiza-
tion. Additionally, ğ¶ğ‘–also preserves a set of individual encoding-
decoding parameters ğœ‡ğ‘–generated by a polynomial function Ffor
privatizing node embeddings. Specifically, we define the parameters
ğœ‡={ğ›¼1,Â·Â·Â·,ğ›¼ğ‘‡+1,ğ›½1,Â·Â·Â·,ğ›½ğ‘‡+1,ğ‘§2,Â·Â·Â·,ğ‘§ğ‘‡+1}, whereğ›¼1,Â·Â·Â·,ğ›¼ğ‘‡+1,
ğ›½1,Â·Â·Â·,ğ›½ğ‘‡+1are2ğ‘‡+2distinct elements from the finite field ğ‘­2,
satisfying{ğ›¼1,Â·Â·Â·,ğ›¼ğ‘‡+1}âˆ©{ğ›½1,Â·Â·Â·,ğ›½ğ‘‡+1}=âˆ…, andğ‘§2,Â·Â·Â·,ğ‘§ğ‘‡+1
areğ‘‡uniformly distributed vector.
2) Model update. The silo-client ğ¶ğ‘–is responsible to optimize Gğ‘–
based on gradients computed by device-clients in D. Specifically, ğ¶ğ‘–
collects the gradients set {âˆ‡ğ‘—ğœƒğ‘–|ğ·ğ‘—âˆˆDğ‘–}and compute the average
gradients asâˆ‡ğœƒğ‘–=1
|Dğ‘–|Ã
ğ·ğ‘—âˆˆDğ‘–âˆ‡ğ‘—ğœƒğ‘–, whereğœƒğ‘–is parameters ofGğ‘–.
The averaged gradients can be further utilized for optimization on
Gğ‘–. For example, we can leverage gradient descent as ğœƒğ‘–=ğœƒğ‘–âˆ’ğœšâˆ‡ğœƒğ‘–
whereğœšis the learning rate.
4.1.3 Server. The server coordinates the entire framework as shown
in Figure 2 (a), whose role is to enable federated optimization
schemes (e.g., FedAvg [ 22]) among silo-clients. In practice, the
server can be an administrator of silo-clients to supervise their
sensitive activities in optimization. For instance, in a finance sce-
nario, the server can be a banking authority (e.g., European Banking
Authority), silo-clients are banks, and device-clients are different
users. The federation is under the authorityâ€™s control to prevent
potential attacks among banks and users.
4.1.4 Communication and Data Visibility. The cross-level com-
munication and data visibility among three different modules are
shown in Figure 2 (d). First, the server and silo-clients can trans-
fer model parameters during federated optimization. Second, silo-
clients are forbidden to exchange information, including device
identities, models, and encoding-decoding parameters. Third, dur-
ing the joint learning process, the device-client can access the model
of the silo-client and send local gradients to the silo-client for model
update, while the silo-client can not directly access the device-
clientâ€™s data. Last, device-clients communicate with each other
through privatized embeddings without exposure of node features
and neighborhood information.
4.2 Secret Message Passing
We further propose a novel graph learning method, SecMP, to pre-
serve multi-level privacy without information loss. In particular,
we develop the neighbor-agnostic aggregation andhierarchical La-
grangian embedding against potential subgraph-node and subgraph-
subgraph attacks. Specifically, the neighbor-agnostic aggregation
decomposes the feature extraction function in conventional GNN
into two individual steps to mutually cover neighbor information
between any connected node pair, preventing subgraph-level sensi-
tive information exchange. Meanwhile, the hierarchical Lagrangian
embedding utilizes Lagrange polynomial functions to mask sharing
node embeddings during message passing recoverably.
4.2.1 Neighbor-agnostic aggregation. The aggregation step in GNNs
may leak subgraph-level privacy, since such a process requires ac-
cessing structural information from cross-client neighbors [ 10,13,
2Following [37], we assume that ğ‘­is a finite field with 11elements.
 
972KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
30]. Take GCN [ 13] as an example, the aggregation process can
be denoted by â„ğ‘¢=Ã
ğ‘£âˆˆNğ‘¢(1/(âˆšï¸
|Nğ‘¢|âˆšï¸
|Nğ‘£|))â„ğ‘£W+ğ‘,whereâ„is
node embedding, ğ‘andWare learnable parameters, and |N|are the
number of neighbors. Calculating |Nğ‘¢|and|Nğ‘£|requires knowing
precise numbers of ğ‘¢andğ‘£, which is risky if the computation only
inside either device-client ğ‘¢orğ‘£.
To address this problem, we devise the neighbor-agnostic aggre-
gation to split GNN aggregating operations into source-side and
target-side steps, which makes the aggregation process agnostic
with neighborsâ€™ information. Specifically, we operate a message
passing from node ğ‘£to nodeğ‘¢in the source device-client and tar-
get device-client that correspond to two nodes, respectively. First,
device-client ğ‘£will calculate a source-side function on â„ğ‘£to get Ë†â„ğ‘£
and pass it to device-client ğ‘¢. Then, device-client ğ‘¢will calculate
a target-side function to update its embedding. The source-side
function can be defined as
Ë†â„ğ‘£=(1/âˆšï¸
|Nğ‘£|)Â·â„ğ‘£, (2)
and the target-side function can be defined as
â„ğ‘¢=ğ‘+(1/âˆšï¸
|Nğ‘¢|)âˆ‘ï¸
ğ‘£âˆˆNğ‘¢Ë†â„ğ‘£Â·W. (3)
In this way, we can preserve the privacy of both device-client ğ‘¢and
ğ‘£â€™s neighbor lists by delegating operation-related private data to be
computed within device-clients.
4.2.2 Hierarchical Lagrangian embedding. Sharing embeddings
with nodesâ€™ neighbors may induce severe node-level privacy leak-
age. Therefore, we privatize device-clientsâ€™ information without in-
formation loss by introducing Lagrange Coded Computing (LCC) [ 37],
a secret sharing technique in GNN optimization. Specifically, our
method follows an encoding-decoding workflow based on a group
of parameters ğœ‡introduced in Section 4.1.2. In the encoding step,
we use Lagrange interpolation polynomial to encode data to coded
versions. In the decoding step, the aggregated coded vectors can be
decoded without any information loss.
Encoding. Any ego device-clients ğ·ğ‘ edged with a target device-
clientğ·ğ‘¡of silo-client ğ¶will accessğœ‡and use them for constructing
a polynomial function based on ğ·ğ‘ â€™s embedding3â„ğ‘£as
ğ‘”ğ‘£(ğ‘¥)=â„ğ‘£Â·Ã–
ğ‘˜âˆˆ[ğ‘‡+1]\{1}ğ‘¥âˆ’ğ›½ğ‘˜
ğ›½1âˆ’ğ›½ğ‘˜+ğ‘‡+1âˆ‘ï¸
ğ‘—=2ğ‘§ğ‘—Â·Ã–
ğ‘˜âˆˆ[ğ‘‡+1]\{ğ‘—}ğ‘¥âˆ’ğ›½ğ‘˜
ğ›½ğ‘—âˆ’ğ›½ğ‘˜,(4)
ğ·ğ‘ generatesğ‘‡+1coded embeddings Ëœâ„ğ‘£=ğ‘”ğ‘£(ğ›¼),ğ›¼âˆˆğ›¼1,Â·Â·Â·,ğ›¼ğ‘‡+1.
Decoding.ğ·ğ‘¡will receiveğ‘‡+1coded embeddings4â„1,Â·Â·Â·,â„ğ‘‡+1
from each source neighbor device-client ğ·ğ‘ âˆˆNğ·ğ‘¡.ğ·ğ‘¡will delegate
the aggregated coded embedding to ğ¶for decoding by calculating
values of the Lagrange interpolated polynomial function at ğ‘¥=ğ›½1.
4.2.3 Overall workflow. We summarize the overall workflow of
SecMP that incorporates neighbor-agnostic aggregation and hier-
archical Lagrangian embedding for subgraph-level and node-level
privacy protection. As illustrated in Figure 3, SecMP follows gen-
eral GNN pipeline [ 6] in three major steps, i.e.,privatized message,
secure aggregation, and neighbor-agnostic update.
3The embedding can be computed by a source-side function as Equation 2.
4The embedding can be computed by a target-side function as Equation 3.
1
3
5
4
2
Silo-client A
Silo-client B
Device-client 3
Device-client 43
4
NodeServer
Edge
 GNN
Encoding-Decoding 
ParametersEncode
Send
Aggregate
DecodeAggregateRepresentingFeature
Feature EmbeddingEmbedding Coded Embedding
Representing
Update
Aggregated 
Coded 
EmbeddingAggregated 
EmbeddingFigure 3: Overall workflow of Secret Message Passing.
1) Privatized message. We first emit encoded embedding to the
target device-client. In this way, device-clients are only aware of
neighborsâ€™ coded embeddings instead of any concrete features. This
step includes three sub-steps. a) Device-clients project raw features
into node embeddings on the source-side. b) Device-clients request
encoding-decoding parameters from neighborsâ€™ silo-clients. For
each neighbor, the device-client will have one corresponding group
of parameters for encoding. c) Device-clients encode embeddings
and send encoded embeddings to both intra-client and cross-client
target neighbor device-clients.
2) Secure aggregation. Then, the encoded embeddings from
multiple neighbors are aggregated to derive a unified embedding.
The silo-client can decode the aggregated embeddings without
accessing the embeddings of each individual node, while device-
clients cannot decode encrypted embeddings, therefore protecting
individual embeddings. Two sub-steps of secure aggregation are
elaborated below. a) Device-clients aggregate received encoded em-
beddings into a unified embedding. b) Device-clients send the aggre-
gated embeddings to their silo-clients, which can use its encoding-
decoding parameters to decode the aggregated embedding and send
it back to device-clients.
3) Neighbor-agnostic update. After secure aggregation, the
target device-clients obtain the aggregated embeddings computed
from the source side, and update their embeddings without requir-
ing access to neighborsâ€™ structure information.
Note that an exceptional case is that when a node has only one
neighbor, the corresponding device-client can get the node embed-
ding after aggregation because the decoded aggregated embedding
is exactly the embedding of the neighbor. A possible solution is
to perturb embeddings by Differential Privacy (DP) techniques [ 5]
to cover sensitive information. The DP-based masking operation
also prevents attacks that infer original information by gradients or
embeddings [ 44]. Besides, we build HiFGL on the Trusted Execution
Environment (TEE) [ 24] and ensure the transition is anonymous
for any receivers to eliminate exposure.
4.3 Privacy-preserving Optimization
Then, we present the privacy-preserving optimization algorithm for
HiFGL. Without loss generality, we assume the local model is GCN
for the node classification task, and the federated optimization algo-
rithm is FedAvg. The full pipeline is reported in Appendix A.2. We
 
973HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
specify two critical steps in training, including local optimization
and federated optimization.
Local optimization. We optimize local models based on updated
embeddings computed by SecMP. Specifically, we first calculate
estimated classification probability as Ë†ğ‘¦=SoftMax(â„(ğ‘˜))whereğ‘˜
is the index of the last GNN layer. Then each device-client applies
cross entropy loss on predicted labels Ë†ğ‘¦and true labels ğ‘¦to calculate
the loss value ğ‘™ğ‘—=Ã
ğ‘ğ‘¦ğ‘logË†ğ‘¦ğ‘and generate gradients âˆ‡ğ‘™ğ‘—. After
that, gradients from device-clients will be transmitted to their silo-
clients. Lastly, silo-clients average them as gradients of this round
asâˆ‡Lğ‘–=(1/|Dğ‘–|)Ã
ğ·ğ‘—âˆˆDğ‘–âˆ‡ğ‘™ğ‘—=(1/|Dğ‘–|)Ã
ğ·ğ‘—âˆˆDğ‘–âˆ‡Ã
ğ‘ğ‘¦ğ‘logË†ğ‘¦ğ‘,
for local optimization on GNN parameters ğœƒ(ğ‘›+1)
ğ‘–.
Federated optimization. When silo-clients finish their local op-
timization, the server will leverage FedAvg to average local models
as the new global model as ğœƒ(ğ‘›+1)=(1/|C|)Ã
ğ¶ğ‘–âˆˆCğœƒ(ğ‘›+1)
ğ‘–. Updated
parameters are the initial ones of the next round of optimization.
Federated optimization is recurrently executed until the perfor-
mance of each silo-client tends to converge.
5 ANALYSIS
5.1 Privacy Analysis
Subgraph-level privacy. In our solution, we preserve privacy by
developing a hierarchical structure and a secure learning method.
Specifically, for storage structure, edges are preserved between two
device-clients corresponding to the two nodes instead of within
the silo-clients, restraining featuresâ€™ exposure to other silo-clients.
Besides, to protect against privacy leakage during message passing,
the neighbor-agnostic strategy ensures that the target device-client
only gets the encoded embedding and sends the aggregated one to
its silo-client for decoding, where the silo-client cannot know the
raw embedding of neighbors. In this way, 0%subgraph-level privacy
is leaked when executing SecMP according to the quantifying metric
defined in Appendix A.1.
Node-level privacy. We analyze the privacy leakage of node
embeddings â„of device-clients. Based on Equation 4, we select
ğ›¼ğ‘–âˆˆğ›¼1,Â·Â·Â·,ğ›¼ğ‘‡+1to encodeâ„asËœâ„=ğ‘”(ğ›¼ğ‘–)=(â„,ğ‘§2,Â·Â·Â·,ğ‘§ğ‘‡+1)Â·ğ‘ˆğ‘–,
whereğ‘”(ğ›½ğ‘–)=ğ‘§ğ‘–andğ‘ˆâˆˆğ‘­(ğ‘‡+1)Ã—(ğ‘‡+1)is the encoding matrix
defined as
ğ‘ˆğ‘–,ğ‘—=Ã–
ğ‘˜âˆˆ[ğ‘‡+1]\{ğ‘–}ğ›¼ğ‘—âˆ’ğ›½ğ‘˜
ğ›½ğ‘–âˆ’ğ›½ğ‘˜. (5)
Any neighbor device-client will receive an encoded embedding
Ëœâ„=â„ğ‘ˆtop
ğ‘¡+ğ‘ğ‘ˆbottom
ğ‘¡,whereğ‘¡âˆˆ [ğ‘‡],ğ‘=(ğ‘§2,Â·Â·Â·,ğ‘§ğ‘‡+1), and
ğ‘ˆtopâˆˆğ‘­ğ‘‡,ğ‘ˆbottomâˆˆğ‘­ğ‘‡Ã—ğ‘‡are theğ‘¡-th columns top and bottom
submatrices in ğ‘ˆ.
Lemma 5.1. Theğ‘‡Ã—ğ‘‡matrixğ‘ˆbottomis invertible.
Please refer Appendix A.3 for Lemma 5.1. Since ğ‘ˆbottomis invert-
ible, we can mask encoded data â„ğ‘ˆtop
ğ‘¡asğ‘is uniformly randomized,
andâ„cannot be directly decoded for any ğ‘‡â‰¥1,i.e.,ğ‘ˆbottom
ğ‘¡exists.
5.2 Complexity Analysis
Here we analyze the communication, encoding and decoding, and
space complexity of HiFGL. We denote ğœ‰as the size of model pa-
rameters,ğ‘‘as the dimension of coded or non-coded embeddings â„
orËœâ„,ğ›¾as the dimension of output prediction.Table 2: Overall information of datasets
Datasets Cora CiteSeer PubMed
#of Silo-clients 5 5 5
#of Node 542 665 3943
#of Intra-client Edges 431 183 1772
#of Cross-client Edges 4199 3637 35461
#of Classes 7 6 3
Partition 6/2/2 6/2/2 6/2/2
Communication complexity. We analyze three types of communi-
cation complexity guarantees of HiFGL. The communication com-
plexity isO(ğ‘‘ğ‘‡)between a pair of connected device-clients. For
a silo-client with its device-clients, the communication complex-
ity for message passing and decoding is O(2|D|ğœ‰)andO(ğ‘‘|ğ‘‰ğ‘–|+Ã
ğ‘£ğ‘—âˆˆğ‘‰ğ‘–ğ‘‘ğ‘‡|ğ‘ğ‘—|), respectively, where |D|is the number of the silo-
clientâ€™s device-clients. Last, the communication complexity between
silo-clients and the server is O(2ğœ‰)for each silo-client and O(2|C|ğœ‰)
for the server. More details are provided in Appendix A.4.1. Encod-
ing and decoding complexity. The encoding and decoding complex-
ity and be approximately guaranteed both as O(ğ‘‘ğ‘‡)according to
analysis in Appendix A.4.2. Space complexity. As analyzed in Ap-
pendix A.4.3, the space complexity of HiFGL is O(|G|Â·(ğœ‰+3ğ‘‡+2)+
|E|+|H|)and we guarantee the increased complexity ğ›¿Ssatisfying
O(|G|ğœ‰)â‰¤O( Î”S)â‰¤O(| G|ğœ‰+|E\(E1âˆªE2âˆªÂ·Â·Â·)|) .
6 EXPERIMENTS
6.1 Experimental Setup
Datasets. We leverage popular graph datasets for node classifica-
tion tasks, including Cora [ 28], CiteSeer [ 28], and PubMed [ 28].
Following previous FGL benchmarks [ 32,35], we split the above
graph datasets randomly to 5subgraphs with comparable node
numbers. Details of datasets are presented in Table 2.
Baselines. For framework-level experiments, we compare HiFGL
with five baseline frameworks (Local, FedAvg [ 22], FedProx [ 15],
FedPer [ 1], and Global) incorporated with two popular GNNs vari-
ants (GCN [ 13] and GraphSage [ 7]) as backbones. Besides, we also
test performance of state-of-the-art FGL methods, including Fed-
PerGNN [ 33], FedSage+ [ 40], and FED-PUB [ 2]. More information
on baseline models is listed in Appendix A.5.
Metrics. We evaluate node classification performance by accu-
racy (ACC), i.e., the global percentage of accurately predicted sam-
ples, rather than the average of silo-client accuracy. In particu-
lar, we design a new metric, i.e.,Graph Information Gain, to show
how much graph information has been modeled as Gain(â˜…-G)=
ACC(â˜…-G)âˆ’ ACC(Local-MLP)
ACC(Global-G)âˆ’ ACC(Local-MLP),where â˜…-Gdenotes a GNN back-
bone model under a framework.
Implementation details. We set the hidden dimension for GNNs
and multi-layer perceptron as 64, and the input and output dimen-
sions depend on the raw feature dimensions and the number of
classes for each dataset. We implement all GNNs with 2layers and
train them for maximum 50epochs or two hours with Adam [ 12]
optimizer where the learning rate is set as 0.01and multiplies
gamma 0.9for every 4epoch. The ğ‘‡in embedding privatization
 
974KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
Table 3: The prediction ACC and graph information gain of different FGL frameworks.
ACC Cora CiteSeer PubMed
Mean Std Gain Mean Std Gain Mean Std Gain
Local-MLP 0.5698 Â±0.0071 +0% 0.6450 Â±0.0061 +0% 0.8051 +0.0006 +0%
Local-GCN 0.8095 Â±0.0149 +80.14% 0.7429 Â±0.0135 +75.77% 0.8525 Â±0.0073 +89.10%
FedAvg-GCN 0.8358 Â±0.0135 +88.93% 0.7601 Â±0.0152 +89.09% 0.8603 Â±0.0095 +103.76%
HiFGL-GCN 0.8555 Â±0.0162 +95.52% 0.7724 Â±0.0108 +98.61% 0.8626 Â±0.0064 +108.08%
Global-GCN 0.8689 Â±0.0182 +100% 0.7742 Â±0.0115 +100% 0.8583 Â±0.0033 +100%
Local-GraphSage 0.6207 Â±0.0103 +17.03% 0.6125 Â±0.0077 -25.04% 0.8221 Â±0.0101 +29.72%
FedAvg-GraphSage 0.8095 Â±0.0123 +80.22% 0.7656 Â±0.0139 +92.91% 0.8444 Â±0.0098 +68.71%
HiFGL-GraphSage 0.8642 Â±0.0288 +98.53% 0.7791 Â±0.0112 +103.31% 0.8504 Â±0.0169 +79.20%
Global-GraphSage 0.8686 Â±0.0215 +100% 0.7748 Â±0.0127 +100% 0.8623 Â±0.0058 +100%
is set as 1. The HiFGL framework is implemented based on Py-
Torch, and PyTorch-Lightning runs on the machine with Intel Xeon
Gold 6148 @ 2.40GHz, V100 GPU and 64G memory. Our codes are
open-sourced at https://github.com/usail-hkust/HiFGL.
6.2 Overall Prediction Performance
In this subsection, we show the results of the node classification
task including HiFGL and baseline frameworks and algorithms.
Specifically, we first compare HiFGL with three different frame-
works to demonstrate the increased predictive ability brought by
information integrity. Second, we conduct experiments to illustrate
that for FGL, information integrity is more important than any
other algorithm-level improvement.
Framework-level results. We investigate predictive knowledge
retrieved under different frameworks measured by Graph Infor-
mation Gain. First, we define the predictability lower bound and
upper bound as 1) Lower bound (0%) : the performance of Local-MLP,
which only separately learns the knowledge from raw features
through a 2-layer multi-layer perceptron; 2) Upper bound ( 100%) :
the performance of trained 2-layer backbone GNN (GCN and Graph-
Sage) on the Global setting, which extracts both knowledge from
raw features and graph information. Then, we consider the gap be-
tween the lower and upper bound as graph information knowledge.
Therefore, we test GNNs under different frameworks and collect
the mean and standard deviation of ACC for five times experiments
and graph information gain in Table 3. Results show that under
HiFGL, GCN and GraphSage have more graph information gain over
than Local and FedAvg and comparable with under Global. Specif-
ically, on three datasets, HiFGL-GCN outperforms 6.59%, 9.52%,
and4.32%than FedAvg-GCN, and HiFGL-GraphSage outperforms
18.31%, 10.40%, and 10.49% than FedAvg-GraphSage. The model
improvement demonstrates that besides ensuring privacy and com-
plexity, HiFGL enhances information integrity to achieve better
accuracy approximately equal ones without FL settings because
the graph information gain of HiFGL is obtained from preserved
cross-client edges, which offers rich relational knowledge to learn
more effective embeddings.
Algorithm-level results. The compared results among different
FGL methods are depicted in Table 4. We compare HiFGL with three
optimization schemes, i.e., FedAvg, FedProx, and FedPer, with two
GNNs, i.e., GCN and GraphSage. Besides, we involve FedPerGNN,Table 4: The prediction ACC of different FGL algorithms.
Cora CiteSeer PubMed
FedAvg-GCN 0.8358 0.7601 0.8603
FedProx-GCN 0.8238 0.7612 0.8305
FedPer-GCN 0.8202 0.7403 0.8471
FedAvg-GraphSage 0.8295 0.7540 0.8524
FedProx-GraphSage 0.8256 0.7593 0.8391
FedPer-GraphSage 0.8174 0.7625 0.8439
FedPerGNN 0.8351 0.7488 0.8346
FedSage+ 0.7767 0.7567 0.8394
FED-PUB 0.8370 0.7579 0.8457
HiFGL-GCN 0.8555 0.7724 0.8626
HiFGL-GraphSage 0.8642 0.7791 0.8504
FedSage+, and FED-PUB, which design tailored advanced graph
learning modules for FGL. We observe that under HiFGL, GCN,
and GraphSage defeat all tested methods. Specifically, with the in-
tegrated information preserved by the HiFGL, GCN achieves an
accuracy of 0.8555, over 2.3%,3.8%, and 4.3%improvement than Fe-
dAvg, FedProx, and FedPer on Cora dataset, respectively. Similarly,
GraphSage gets 4.1%,4.6%, and 5.7%improvement. Significant per-
formance promotion is attained on CiteSeer and PubMed datasets.
The reason is that for GCN and GraphSage that highly depend on
structure information, graph integrity can offer great benefits. In ad-
dition, against methods with more advanced GNNs, HiFGL wins on
prediction accuracy only by incorporating simple backbones. For ex-
ample, under HiFGL, GraphSage surpasses FedPerGNN, FedSage+,
and FED-PUB by 4.0%,3.0%, and 2.8%, respectively, on CiteSeer.
These experimental results demonstrate that graph integrity can
bring much effectiveness improvement than GNN modification
including high-order relationships incorporated in FedPerGNN,
missing neighbor generation in FedSage+, and community-aware
personalization in FED-PUB.
6.3 Efficiency Results
We study the efficiency by evaluating time and memory cost through
varying two GNN hyper-parameters, i.e., layer numbers and hidden
dimensions, and take HiFGL-GCN as the testing model.
 
975HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 5: Training results of different GNN layers.
#of GNN Layers 1 2 4 8 16
ACC (%) 73.04 77.91 70.87 39.84 26.02
Epoch Time (s) 6.5 4.3 5.6 8.4 16.4
Memory (GB) 3.41 3.45 3.49 3.59 3.80
Table 6: Training results of different hidden dimensions.
Hidden Dimensions 4 8 16 32 64 128
ACC (%) 83.45 85.15 85.33 85.27 85.88 86.39
Epoch Time ( ğ‘ )17.2 18.05 20.81 19.84 23.49 44.30
Memory (GB) 3.753Â±0.005
GNN layers. First, we set GNN ranging from 1to16layers, with
a hidden dimension of 64, on the CiteSeer dataset. We display results
in Table 5 that the highest ACC occurs in 2-layer GCN, and the
performance decreases for more and fewer layers. The training time
per epoch and the memory both increase along with the growth of
the layer number. The results show that 2-layer GCN in the HiFGL
framework is the most efficient model for ACC and training time,
and it does not need expensive memory for training.
GNN dimension. We also set different hidden dimensions of a
2-layer GCN, ranging from 4to128, on the PubMed dataset. Table 6
demonstrates that performance increases with larger hidden di-
mensions since a more complicated model can catch more intricate
patterns while obstructing high efficiency. Besides, the memory
for different hidden dimensions is only slightly different because
parameters are not dominant for memory costs compared with the
other parts of the HiFGL framework under our implementation.
In conclusion, higher performance is not always brought by ex-
pensive executive costs that multiple layers of GNNs need a heavy
training burden but are predicted poorly. Increasing hidden dimen-
sions of GNNs helps the ACC, but the time consumption grows
exponentially. Therefore, we select 2-layer GNNs with the hidden
dimension of 64for our main experiments. Our framework has
sacrificed efficiency to achieve more advanced privacy preserva-
tion, especially during training stages equipping with SecMP. The
imperfection of inefficient processes remains a future development
such as faster secret sharing techniques.
6.4 Local Prediction Performance
We also investigate the local performance of each silo-client. Specif-
ically, we collect the ACC of local models in all training epochs
and show them in Figure 4, respectively. We discover that local
models do not simultaneously upgrade or degrade during train-
ing in Figure 4(a). Instead, the five curves fluctuate in inconsistent
upward patterns, which means a tradeoff is made among them
during co-optimization directed by the server for global ACC im-
provement. Moreover, the converged performance of silo-clients is
slightly diverse for their data distribution difference in Figure 4(b).
The non-IID issue naturally exists in FL. In our example, the dis-
tributions of node features and graph structures in silo-clients are
diverse naturally, which causes local models to be optimized in
different directions during global loss degradation.
0 10 20 30 40
Epochs0.10.20.30.40.50.60.70.80.9AccuracyClient 1
Client 2
Client 3
Client 4
Client 5(a) Training ACC of silo-client models.
Client 1 Client 2 Client 3 Client 4 Client 5
Client0.00.10.20.30.40.50.60.70.8Accuracy (b) Average ACC of silo-client models.
Figure 4: Local model performance on Cora dataset.
1 5 10 20 30 50
Client Number0.7000.7250.7500.7750.8000.8250.8500.8750.900AccuracyHiFGL
FedAvg
(a) ACC
1 5 10 20 30 50
Client Number0.00.51.01.52.02.53.03.5AccuracyHiFGL
FedAvg (b) Epoch time
Figure 5: Sensitivity for different silo-client numbers under
the HiFGL and FedAvg frameworks.
6.5 Sensitivity of Silo-client Number
We evaluate the sensitivity of the silo-client number under different
frameworks. Specifically, we vary it from 1to50and take GraphSage
as the backbone model to find out the variation of ACC in Figure 5(a).
Moreover, we also present executive time costs for different silo-
client numbers in Figure 5(b). The number hardly influences model
performance under HiFGL, while the ACC decreases as the number
increases, and models of HiFGL are trained slightly longer than
FedAvg. Because for FedAvg, a large number of silo-clients means
that fewer time-consuming cross-client message passing operations
are taken, and fragmentary information remaining on each silo-
client is inadequate for precise training. These results further prove
the importance of graph integrity for an FGL framework.
7 CONCLUSION
This paper first studies cross-silo cross-device FGL, where we pro-
pose a HiFGL framework based on a novel hierarchical architecture
on heterogeneous clients. Moreover, we devise a tailored graph
learning algorithm, SecMP, for multi-level privacy-preserving opti-
mization with graph integrity. Theoretical analyses are provided for
privacy and efficiency guarantees. Extensive experiments demon-
strate the prediction improvement gained from graph integrity
on three datasets. HiFGL offers versatility in a wider range of real-
world applications, even in solely cross-silo or cross-device settings.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Foun-
dation of China (Grant No.62102110, No.92370204), National Key
R&D Program of China (Grant No.2023YFF0725004), Guangzhou-
HKUST(GZ) Joint Funding Program (Grant No. 2022A03J00056,
No.2023A03J0008), Education Bureau of Guangzhou Municipality.
 
976KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
REFERENCES
[1]Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav
Choudhary. 2019. Federated learning with personalization layers. arXiv preprint
arXiv:1912.00818 (2019).
[2]Jinheon Baek, Wonyong Jeong, Jiongdao Jin, Jaehong Yoon, and Sung Ju Hwang.
2023. Personalized subgraph federated learning. In International Conference on
Machine Learning. PMLR, 1396â€“1415.
[3]E Berlekamp. 2006. Nonbinary BCH decoding (Abstr.). IEEE Transactions on
Information Theory 14, 2 (2006), 242â€“242.
[4]Fahao Chen, Peng Li, Toshiaki Miyazaki, and Celimuge Wu. 2021. Fedgraph:
Federated graph learning with intelligent sampling. IEEE Transactions on Parallel
and Distributed Systems 33, 8 (2021), 1775â€“1786.
[5]Cynthia Dwork, Aaron Roth, et al .2014. The algorithmic foundations of differ-
ential privacy. Foundations and TrendsÂ® in Theoretical Computer Science 9, 3â€“4
(2014), 211â€“407.
[6]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
conference on machine learning. PMLR, 1263â€“1272.
[7]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[8]Bing He, Dian Zhang, Siyuan Liu, Hao Liu, Dawei Han, and Lionel M Ni. 2018.
Profiling driver behavior for personalized insurance pricing and maximal profit.
In2018 IEEE International Conference on Big Data (Big Data). IEEE, 1387â€“1396.
[9]Chao Huang, Jianwei Huang, and Xin Liu. 2022. Cross-silo federated learning:
Challenges and opportunities. arXiv preprint arXiv:2206.12949 (2022).
[10] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang.
2020. Graph structure learning for robust graph neural networks. In Proceedings
of the 26th ACM SIGKDD international conference on knowledge discovery & data
mining. 66â€“74.
[11] Kiran S Kedlaya and Christopher Umans. 2011. Fast polynomial factorization
and modular composition. SIAM J. Comput. 40, 6 (2011), 1767â€“1802.
[12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[13] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations.
[14] Runze Lei, Pinghui Wang, Junzhou Zhao, Lin Lan, Jing Tao, Chao Deng, Junlan
Feng, Xidian Wang, and Xiaohong Guan. 2023. Federated Learning Over Coupled
Graphs. IEEE Transactions on Parallel and Distributed Systems 34, 4 (2023), 1159â€“
1172.
[15] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of Machine learning and systems 2 (2020), 429â€“450.
[16] Tzu-Heng Lin, Chen Gao, and Yong Li. 2019. Cross: Cross-platform recommen-
dation for social e-commerce. In Proceedings of the 42nd International ACM SIGIR
Conference on Research and Development in Information Retrieval. 515â€“524.
[17] Fan Liu, Hao Liu, and Wenzhao Jiang. 2022. Practical adversarial attacks on spa-
tiotemporal traffic forecasting models. Advances in Neural Information Processing
Systems 35 (2022), 19035â€“19047.
[18] Rui Liu, Pengwei Xing, Zichao Deng, Anran Li, Cuntai Guan, and Han Yu. 2024.
Federated Graph Neural Networks: Overview, Techniques, and Challenges. IEEE
Transactions on Neural Networks and Learning Systems (2024).
[19] Tao Liu, Peng Li, and Yu Gu. 2021. Glint: Decentralized federated graph learning
with traffic throttling and flow scheduling. In 2021 IEEE/ACM 29th International
Symposium on Quality of Service (IWQOS). IEEE, 1â€“10.
[20] Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S Yu. 2022. Feder-
ated social recommendation with graph neural network. ACM Transactions on
Intelligent Systems and Technology (TIST) 13, 4 (2022), 1â€“24.
[21] Hui Luo, Jingbo Zhou, Zhifeng Bao, Shuangli Li, J Shane Culpepper, Haochao
Ying, Hao Liu, and Hui Xiong. 2020. Spatial object recommendation with hints:
When spatial granularity matters. In Proceedings of the 43rd International ACM
SIGIR Conference on research and development in information retrieval. 781â€“790.
[22] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics . PMLR,
1273â€“1282.
[23] Chuizheng Meng, Sirisha Rambhatla, and Yan Liu. 2021. Cross-Node Federated
Graph Neural Network for Spatio-Temporal Data Modeling. knowledge discovery
and data mining (2021).
[24] Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino, and
Nicolas Kourtellis. 2021. PPFL: privacy-preserving federated learning with trusted
execution environments. In Proceedings of the 19th annual international conference
on mobile systems, applications, and services. 94â€“108.
[25] Qiying Pan and Yifei Zhu. 2022. FedWalk: Communication Efficient Federated
Unsupervised Node Embedding with Differential Privacy. knowledge discovery
and data mining (2022).[26] Qiying Pan, Yifei Zhu, and Lingyang Chu. 2023. Lumos: Heterogeneity-aware Fed-
erated Graph Learning over Decentralized Devices. In 2023 IEEE 39th International
Conference on Data Engineering (ICDE). IEEE Computer Society, 1914â€“1926.
[27] Liang Qu, Ningzhi Tang, Ruiqi Zheng, Quoc Viet Hung Nguyen, Zi Huang, Yuhui
Shi, and Hongzhi Yin. 2023. Semi-decentralized federated ego graph learning for
recommendation. In Proceedings of the ACM Web Conference 2023. 339â€“348.
[28] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93â€“93.
[29] Toyotaro Suzumura, Yi Zhou, Natahalie Baracaldo, Guangnan Ye, Keith Houck,
Ryo Kawahara, Ali Anwar, Lucia Larise Stavarache, Yuji Watanabe, Pablo Loyola,
Daniel Klyashtorny, Heiko Ludwig, and Kumar Bhaskaran. 2019. Towards Fed-
erated Graph Learning for Collaborative Financial Crimes Detection. Research
Papers in Economics (2019).
[30] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations.
[31] Binghui Wang, Ang Li, Meng Pang, Hai Li, and Yiran Chen. 2022. Graphfl: A
federated learning framework for semi-supervised node classification on graphs.
In2022 IEEE International Conference on Data Mining (ICDM). IEEE, 498â€“507.
[32] Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and
Jingren Zhou. 2022. Federatedscope-gnn: Towards a unified, comprehensive and
efficient package for federated graph learning. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 4110â€“4120.
[33] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Tao Qi, Yongfeng Huang, and Xing
Xie. 2022. A federated graph neural network framework for privacy-preserving
personalization. Nature Communications 13, 1 (2022), 1â€“10.
[34] Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope: A Flexible Fed-
erated Learning Platform for Heterogeneity. Proceedings of the VLDB Endowment
16, 5 (2023), 1059â€“1072.
[35] Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope: A Flexible Fed-
erated Learning Platform for Heterogeneity. Proceedings of the VLDB Endowment
16, 5 (2023), 1059â€“1072.
[36] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Machine
Learning: Concept and Applications. ACM Transactions on Intelligent Systems
and Technology (2019).
[37] Qian Yu, Songze Li, Netanel Raviv, Seyed Mohammadreza Mousavi Kalan, Mahdi
Soltanolkotabi, and Salman A Avestimehr. 2019. Lagrange coded computing:
Optimal design for resiliency, security, and privacy. In The 22nd International
Conference on Artificial Intelligence and Statistics. PMLR, 1215â€“1225.
[38] Xiaoming Yuan, Jiahui Chen, Jiayu Yang, Ning Zhang, Tingting Yang, Tao Han,
and Amir Taherkordi. 2022. Fedstn: Graph representation driven federated learn-
ing for edge computing enabled urban traffic flow prediction. IEEE Transactions
on Intelligent Transportation Systems (2022).
[39] Binchi Zhang, Minnan Luo, Shangbin Feng, Ziqi Liu, Jun Zhou, and Qinghua
Zheng. 2021. Ppsgcn: A privacy-preserving subgraph sampling based distributed
gcn training method. arXiv preprint arXiv:2110.12906 (2021).
[40] Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, and Siu Ming Yiu. 2021. Sub-
graph federated learning with missing neighbor generation. Advances in Neural
Information Processing Systems 34 (2021), 6671â€“6682.
[41] Weijia Zhang, Hao Liu, Lijun Zha, Hengshu Zhu, Ji Liu, Dejing Dou, and Hui
Xiong. 2021. MugRep: A multi-task hierarchical graph representation learning
framework for real estate appraisal. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 3937â€“3947.
[42] Xiaojin Zhang, Hanlin Gu, Lixin Fan, Kai Chen, and Qiang Yang. 2022. No free
lunch theorem for security and utility in federated learning. ACM Transactions
on Intelligent Systems and Technology 14, 1 (2022), 1â€“35.
[43] Xiaojin Zhang, Yan Kang, Kai Chen, Lixin Fan, and Qiang Yang. 2023. Trading
Off Privacy, Utility, and Efficiency in Federated Learning. ACM Transactions on
Intelligent Systems and Technology 14, 6 (2023), 1â€“32.
[44] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage from gradients.
Advances in neural information processing systems 32 (2019).
A APPENDIX
A.1 Quantifying Subgraph-level Privacy
Leakage
We analyze the subgraph-level privacy leakage of previous cross-
client FGL frameworks. Subgraph-level privacy leakage derives
from the information sharing across cross-client edges, which lets
silo-clients see nodes from others. There are works such as Fed-
Sage [ 40] and FedPUB [ 2] that drop cross-client edges to sacrifice
 
977HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Algorithm 1: Training algorithm for GCN based on SecMP.
Input: The graph G, the serverğ‘†,|G|silo-clients C(eachğ¶ğ‘–
with a local model Gğ‘–),ğ‘›ğ‘‘device-clients D(eachğ·ğ‘—
with feature â„(0)and neighborsNğ·ğ‘—}).
Output: The converged global model parameterized with
ğœƒ(ğ‘›)trained after ğ‘›epochsGğœƒ(ğ‘›).
1Initialize each silo-clientâ€™s model parameters with ğœƒ(0)and
encoding-decoding parameters
ğœ‡={ğ›¼1,Â·Â·Â·,ğ›¼ğ‘‡+1,ğ›½1,Â·Â·Â·,ğ›½ğ‘‡+1,ğ‘§2,Â·Â·Â·,ğ‘§ğ‘‡+1};
2whileGğœƒ(ğ‘›)is not converged do
3 foreachğ¶ğ‘–âˆˆCin parallel do
4 foreachğ·ğ‘—âˆˆDğ‘–in parallel do
5 Ë†â„(0)â†1âˆš
|Nğ‘—|â„(0);
6 foreachğ·ğ‘âˆˆNğ‘—do
7 Getğœ‡ğ‘fromğ·ğ‘â€™s silo-client;
8 Ëœâ„(0)â†F( Ë†â„(0);ğœ‡ğ‘);
9 Send coded embedding Ëœâ„(0)toğ·ğ‘;
10 end
/* Afterğ·ğ‘—receives all coded
embeddings ËœHNğ‘—fromNğ‘— */
11 Ëœâ„(ğ‘˜)
Nğ‘—â†Ã
Ëœâ„(0)
ğ‘âˆˆËœHNğ‘—1âˆš
|Nğ‘—|Gğ‘–(Ëœâ„(0)
ğ‘;ğœƒğ‘–);
12 Send Ëœâ„(ğ‘˜)
Nğ‘—toğ¶ğ‘–, andğ¶ğ‘–decodes it according to
Fandğœ‡ğ‘–asâ„(ğ‘˜)
Nğ‘—and send it back to ğ·ğ‘—;
13 â„(ğ‘˜)â†â„(ğ‘˜)
Nğ‘—;
14 Calculate estimated probability as
Ë†ğ‘¦â†SoftMax(â„(ğ‘˜));
15 Compute loss ğ‘™ğ‘—of prediction Ë†ğ‘¦and labelsğ‘¦;
16 Send gradientâˆ‡ğ‘™ğ‘—toğ¶ğ‘–;
17 end
18 Optimizeğœƒ(ğ‘›+1)
ğ‘–withâˆ‡Lğ‘–â†1
|Dğ‘–|Ã
ğ·ğ‘—âˆˆDğ‘–âˆ‡ğ‘™ğ‘—;
19 end
20 Update global parameters as ğœƒ(ğ‘›+1)â†1
|C|Ã
ğ¶ğ‘–âˆˆCğœƒ(ğ‘›+1)
ğ‘–;
21end
the information integrity to perfectly preserve subgraph-level pri-
vacy. However, other works such as Glint [ 19] choose to permit
silo-clients to access the features of the nodes from other silo-clients
connected with their own nodes. Here the subgraph-level privacy
leakage ofğ‘–-th silo-client ğœ–ğ‘–ğ‘can be defined as
ğœ–ğ‘–
ğ‘=|{Dğ‘—|Dğ‘—âˆˆCğ‘–ğ‘ğ‘›ğ‘‘Dâ€²âˆ‰Cğ‘–,âˆƒDâ€²âˆˆNğ‘—}|, (6)
where we utilize the number of nodes that possess cross-client
edges (i.e., neighbors out of their own silo-clients). Subsequently,
we compute the average of ğœ–ğ‘–ğ‘among silo-clients as the global
privacy leakage by ğœ–ğ‘=1
|C|Ãğ‘–=1
|C|ğœ–ğ‘–ğ‘.Under this formulation, on experimental datasets introduced in
Section 6.1, we observe that privacy leakage is remarkably signifi-
cant, which is 95.16%on Cora, 88.70%on CiteSeer, and 90.31%on
PubMed for 5silo-clients. Otherwise, 90.69%, 95.21%, and 95.24%
edges are lost on Cora, CiteSeer, and PubMed for 5silo-clients, re-
spectively, if we protect privacy by dropping all cross-client edges.
Therefore, preventing nodes from being seen by other silo-clients
is necessary for FGL frameworks.
A.2 Training Pipeline
As an example, here we appoint GCN as the backbone model and Fe-
dAvg as the federated optimization method to explain the workflow
in Algorithm 1. Specifically, after initialization of model parameters
withğœƒ(0)and encoding-decoding parameters ğœ‡, the optimization
round will be executed recurrently. Training will be stopped after
global convergence.
A.3 Proof of Lemma 5.1
Proof. Before we give proof, we revisit the selected 2ğ‘‡distinct
elements which satisfying {ğ›¼1,Â·Â·Â·,ğ›¼ğ‘‡+1}âˆ©{ğ›½1,Â·Â·Â·,ğ›½ğ‘‡+1}=âˆ….
Thusğ›¼ğ‘–âˆ’ğ›¼ğ‘—,ğ›½ğ‘–âˆ’ğ›½ğ‘—, andğ›¼ğ‘–âˆ’ğ›½ğ‘—are non-zero for any different
ğ‘–andğ‘—. Here we start to validate ğ‘ˆbottomâ€™s invertibility. First, we
multiply every ğ‘–-th row byÃ
ğ‘˜âˆˆ[ğ‘‡+1]\{ğ‘–}(ğ›½ğ‘–âˆ’ğ›½ğ‘˜)â‰ 0to obtain
ğ‘ˆbottom
ğ‘–,ğ‘—=Ã–
ğ‘˜âˆˆ[ğ‘‡+1]\{ğ‘–}(ğ›¼ğ‘—âˆ’ğ›½ğ‘˜). (7)
Second, we multiply every ğ‘—-th column byÃ
ğ‘˜âˆˆ[ğ‘‡+1]1
ğ›¼ğ‘—âˆ’ğ›½ğ‘˜â‰ 0as
ğ‘ˆbottom
ğ‘–,ğ‘—=1
ğ›¼ğ‘—âˆ’ğ›½ğ‘–, (8)
Third, we subtract the ğ‘›-th column from the first ğ‘›âˆ’1columns and
extract the common factor as
ğ‘ˆbottom
ğ‘›Ã—ğ‘›=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘1âˆ’ğ‘ğ‘›
(ğ‘1âˆ’ğ‘1)(ğ‘1âˆ’ğ‘ğ‘›)ğ‘2âˆ’ğ‘ğ‘›
(ğ‘1âˆ’ğ‘2)(ğ‘1âˆ’ğ‘ğ‘›)Â·Â·Â·1
ğ‘1âˆ’ğ‘ğ‘›
ğ‘1âˆ’ğ‘ğ‘›
(ğ‘2âˆ’ğ‘1)(ğ‘2âˆ’ğ‘ğ‘›)ğ‘2âˆ’ğ‘ğ‘›
(ğ‘2âˆ’ğ‘2)(ğ‘2âˆ’ğ‘ğ‘›)Â·Â·Â·1
ğ‘2âˆ’ğ‘ğ‘›
ğ‘1âˆ’ğ‘ğ‘›
(ğ‘ğ‘›âˆ’ğ‘1)(ğ‘ğ‘›âˆ’ğ‘ğ‘›)ğ‘2âˆ’ğ‘ğ‘›
(ğ‘ğ‘›âˆ’ğ‘2)(ğ‘ğ‘›âˆ’ğ‘ğ‘›)Â·Â·Â·1
ğ‘ğ‘›âˆ’ğ‘ğ‘›ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
=ğ‘›âˆ’1Ã
ğ‘–=1(ğ‘ğ‘–âˆ’ğ‘ğ‘›)
ğ‘›Ã
ğ‘—=1(ğ‘ğ‘—âˆ’ğ‘ğ‘›)ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1
ğ‘1âˆ’ğ‘11
ğ‘1âˆ’ğ‘2Â·Â·Â·1
1
ğ‘2âˆ’ğ‘11
ğ‘2âˆ’ğ‘2Â·Â·Â·1
......Â·Â·Â·...
1
ğ‘ğ‘›âˆ’ğ‘11
ğ‘ğ‘›âˆ’ğ‘2Â·Â·Â·1ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»(9)
Obviously, the values of the ğ‘›-th column are the same, thus we
can subtract the last row from the first ğ‘›âˆ’1rows and extract the
common factor as
ğ‘ˆbottom
ğ‘›Ã—ğ‘›=ğ‘›âˆ’1Ã
ğ‘–=1(ğ‘ğ‘–âˆ’ğ‘ğ‘›)
ğ‘›Ã
ğ‘—=1(ğ‘ğ‘—âˆ’ğ‘ğ‘›)ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ‘ğ‘›âˆ’ğ‘1
(ğ‘1âˆ’ğ‘1)(ğ‘ğ‘›âˆ’ğ‘1)ğ‘ğ‘›âˆ’ğ‘1
(ğ‘1âˆ’ğ‘2)(ğ‘ğ‘›âˆ’ğ‘2)Â·Â·Â·0
ğ‘ğ‘›âˆ’ğ‘2
(ğ‘2âˆ’ğ‘1)(ğ‘ğ‘›âˆ’ğ‘1)ğ‘ğ‘›âˆ’ğ‘2
(ğ‘2âˆ’ğ‘2)(ğ‘ğ‘›âˆ’ğ‘2)Â·Â·Â·0
1
ğ‘ğ‘›âˆ’ğ‘11
ğ‘ğ‘›âˆ’ğ‘2Â·Â·Â·1ï£¹ï£ºï£ºï£ºï£ºï£»
=ğ‘›âˆ’1Ã
ğ‘–=1(ğ‘ğ‘›âˆ’ğ‘ğ‘–)(ğ‘ğ‘–âˆ’ğ‘ğ‘›)
ğ‘›Ã
ğ‘—=1(ğ‘ğ‘—âˆ’ğ‘ğ‘›)ğ‘›âˆ’1Ã
ğ‘˜=1(ğ‘ğ‘›âˆ’ğ‘ğ‘˜)ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1
ğ‘1âˆ’ğ‘11
ğ‘1âˆ’ğ‘2Â·Â·Â·1
ğ‘1âˆ’ğ‘ğ‘›âˆ’1
1
ğ‘2âˆ’ğ‘11
ğ‘2âˆ’ğ‘2Â·Â·Â·1
ğ‘2âˆ’ğ‘ğ‘›âˆ’1
......Â·Â·Â·...
1
ğ‘ğ‘›âˆ’1âˆ’ğ‘11
ğ‘ğ‘›âˆ’1âˆ’ğ‘2Â·Â·Â·1
ğ‘ğ‘›âˆ’1âˆ’ğ‘ğ‘›âˆ’1ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
(10)
where we can obtain a recursion formula that
ğ‘ˆbottom
ğ‘›Ã—ğ‘›=Ãğ‘›âˆ’1
ğ‘–=1(ğ‘ğ‘›âˆ’ğ‘ğ‘–)(ğ‘ğ‘–âˆ’ğ‘ğ‘›)
Ãğ‘›
ğ‘—=1(ğ‘ğ‘—âˆ’ğ‘ğ‘›)Ãğ‘›âˆ’1
ğ‘˜=1(ğ‘ğ‘›âˆ’ğ‘ğ‘˜)ğ‘ˆbottom
(ğ‘›âˆ’1)Ã—(ğ‘›âˆ’1).(11)
 
978KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
Last, by continuing the recursive process, we can derive the deter-
minant as
detğ‘ˆbottom
ğ‘›Ã—ğ‘›=Ã
1â‰¤ğ‘–<ğ‘—â‰¤ğ‘›(ğ‘ğ‘—âˆ’ğ‘ğ‘–)(ğ‘ğ‘–âˆ’ğ‘ğ‘—)
Ãğ‘›
ğ‘–,ğ‘—=1(ğ‘ğ‘–âˆ’ğ‘ğ‘—)â‰ 0. (12)
Hence, the matrix ğ‘ˆbottomis invertible. â–¡
A.4 Complexity Analysis Details
Here we analyze communication, encoding and decoding, and space
complexity, which are based on the following conditions. The size
of encoding-decoding parameters ğœ‡is3ğ‘‡+2. Proofs are satisfied
with 1)ğ‘‡â‰¤10, 2)ğ›¾â‰¤10, 3)ğ‘‘>>ğ‘‡, andğ‘‘>>ğ›¾based on realistic
assumption and experimental datasets.
A.4.1 Communication Complexity. The communication is com-
posed of three parts, including 1) communication between device-
clients : coded embedding exchange between device-clients and
device-clients, 2) communication between device-clients and silo-
clients : aggregated embedding communication between device-
clients and silo-clients, and 3) communication between silo-clients
and the server : model parameters sharing between silo-clients and
the server.
1)Communication between device-clients. The communication
between any two connected device-clients (from a source device-
client to a target device-client) consists of two parts. First, the
target device-client passes the 3ğ‘‡+2encoding parameters of its
silo-client to the source device-client. Second, after coding by the
source device-client, the coded embedding will be sent to the target
device-client as ğ¾parts byğ‘‡+1times at least. Therefore, for any
directed edge, the minimum communication complexity between
two device-clients can be computed as O(3ğ‘‡+2+ğ‘‘(ğ‘‡+1))â‰ˆO(ğ‘‘ğ‘‡).
2)Communication between device-clients and silo-clients. Each
device-client will download the latest model parameters and up-
load its gradients at each training round. Thus, the communication
complexity between a single device-client and the corresponding
silo-client can be measured by ğœ‰, the size of the model parameters.
In a word, the communication complexity for a device-client with
its silo-client isO(2ğœ‰), and for a silo-client with its device-clients is
O(2|D|ğœ‰)that|D|is the number of the silo-clientâ€™s device-clients. In
the decoding stage, when device-client ğ·ğ‘—transmits coded embed-
dings to the silo-client ğ¶ğ‘–for decoding, the complexity is O(ğ‘‘ğ‘‡|ğ‘ğ‘—|)
fromğ·ğ‘—toğ¶ğ‘–andO(ğ‘‘)fromğ¶ğ‘–toğ·ğ‘—. Totally, the complexity of
communication for ğ¶ğ‘–isO(ğ‘‘|ğ‘‰ğ‘–|+Ã
ğ‘£ğ‘—âˆˆğ‘‰ğ‘–ğ‘‘ğ‘‡|ğ‘ğ‘—|)at each round.
3)Communication between silo-clients and the server. The commu-
nication between silo-clients and the server obeys the traditional
FL setting, which can be computed as O(2ğœ‰)for each silo-client
andO(2|C|ğœ‰)for the server.
A.4.2 Encoding and Decoding Complexity. We then present the
analysis of Lagrange interpolation-based encoding and decoding
complexity. Specifically, according to existing mathematical proof [ 11],
the complexity of interpolating a ğ‘˜-degree polynomial can be eval-
uated asO(ğ‘˜log2ğ‘˜log logğ‘˜). Therefore, we compute the encodingcomplexity asO(ğ‘‘(ğ‘‡+1)log2(ğ‘‡+1)log log(ğ‘‡+1)) â‰ˆ O(ğ‘‘ğ‘‡).
Besides, using the proposed technique [ 3], we can decode the em-
bedding with a complexity as O(ğ‘‘(ğ‘‡+1)log2ğ‘‡log logğ‘‡)â‰ˆO(ğ‘‘ğ‘‡).
A.4.3 Space Complexity. The space complexity Sof HiFGL on
a graph G=(V,E,H), is the summation of silo-clientâ€™s model
complexityO(|G|Â·(ğœ‰+3ğ‘‡+2))and device-clientâ€™s data (raw features
and neighbor device-client pointers) complexity O(|E|+|H|), which
can be formulated as
S=O(|G|Â·(ğœ‰+3ğ‘‡+2)+|E|+|H|), (13)
where|G|is the number of silo-clients (or subgraphs), |E|is the
number of edges, and |H|is the raw feature complexity.
Conventionally, in FGL, researchers proposed two schemes for
graph storage. 1) Trusted server for cross-client edges and silo-
clients for intra-client edges. Its space complexity is O(|G|Â·ğœ‰+
|E|+|H|). 2) Only silo-clients store intra-client edges. Its space
complexity isO(|G|Â·ğœ‰+Ã
Gğ‘–âˆˆ{G}|Eğ‘–|+|H|). In this work, the HiFGL
framework only needs tiny extra storage memory Î”Scompared
with two traditional schemes, which can be stated as
O(|G|ğœ‰)â‰¤O( Î”S)â‰¤O(| G|ğœ‰+|E\(E1âˆªE2âˆªÂ·Â·Â·)|), (14)
where|E\(E1âˆªE2âˆªÂ·Â·Â·)|â‰¤| E|.
A.5 Baseline Information
We involve five baseline frameworks in our experiments, including
â€¢Local : models are individually trained on silo-clients.
â€¢FedAvg [22]: a collaborative learning framework that aver-
ages local model parameters for global optimization.
â€¢FedProx [ 15]: a more robust federated optimization method
based on FedAvg with regularization of parameters.
â€¢FedPer [1]: a personalized FL algorithm allowing some layers
to be free from FedAvg training for better local fitness.
â€¢Global : a usual graph training way without distributed or
private constraints.
Furthermore, we compare two popular GNNs as backbone models
within federated frameworks, including
â€¢GCN [ 13]: a spectral-based graph convolutional network
form capturing first-order structure feature with node infor-
mation for node representation learning.
â€¢GraphSage [ 7]: a spatial-based graph model aggregating sam-
pled neighbor information with ego features for inductive
graph learning.
We also leverage 2-layer multi-layer perceptron (MLP) under Local
frameworks for comparison with GNNs. Besides, we test classifica-
tion performance on state-of-the-art FGL methods, including
â€¢FedPerGNN [ 33]: a cross-device FL method on personalized
federated item recommendation.
â€¢FedSage+ [ 40]: a federated inductive graph learning model
with neighbor generation for missing edge reconstruction.
â€¢FED-PUB [ 2]: a personalized federated subgraph learning
method incorporating community structures and masked
graph convolutional networks.
 
979