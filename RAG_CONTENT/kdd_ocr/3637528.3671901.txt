CoRAL: Collaborative Retrieval-Augmented Large Language
Models Improve Long-tail Recommendation
Junda Wu
juw069@ucsd.edu
University of California San Diego
La Jolla, California, USACheng-Chun Chang
cc4900@columbia.edu
Columbia University
New York, New York, USATong Yu
tyu@adobe.com
Adobe Research
San Jose, California, USA
Zhankui He
zhh004@eng.ucsd.edu
University of California San Diego
La Jolla, California, USAJianing Wang
lygwjn@gmail.com
University of California San Diego
La Jolla, California, USAYupeng Hou
yphou@ucsd.edu
University of California San Diego
La Jolla, California, USA
Julian McAuley
jmcauley@ucsd.edu
University of California San Diego
La Jolla, California, USA
Abstract
The long-tail recommendation is a challenging task for traditional
recommender systems, due to data sparsity and data imbalance
issues. The recent development of large language models (LLMs)
has shown their abilities in complex reasoning, which can help
to deduce usersâ€™ preferences based on very few previous inter-
actions. However, since most LLM-based systems rely on itemsâ€™
semantic meaning as the sole evidence for reasoning, the collab-
orative information of user-item interactions is neglected, which
can cause the LLMâ€™s reasoning to be misaligned with task-specific
collaborative information of the dataset. To further align LLMsâ€™
reasoning to task-specific user-item interaction knowledge, we in-
troduce collaborative retrieval-augmented LLMs, CoRAL, which
directly incorporate collaborative evidence into the prompts. Based
on the retrieved user-item interactions, the LLM can analyze shared
and distinct preferences among users, and summarize the patterns
indicating which types of users would be attracted by certain items.
The retrieved collaborative evidence prompts the LLM to align its
reasoning with the user-item interaction patterns in the dataset.
However, since the capacity of the input prompt is limited, finding
the minimally-sufficient collaborative information for recommen-
dation tasks can be challenging. We propose to find the optimal
interaction set through a sequential decision-making process and
develop a retrieval policy learned through a reinforcement learn-
ing (RL) framework, CoRAL. Our experimental results show that
CoRAL can significantly improve LLMsâ€™ reasoning abilities on spe-
cific recommendation tasks. Our analysis also reveals that CoRAL
can more efficiently explore collaborative information through re-
inforcement learning.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671901CCS Concepts
â€¢Information systems â†’Collaborative filtering; Recom-
mender systems; Language models.
Keywords
Large language models, Collaborative Filtering, Long-tail Recom-
mendation
ACM Reference Format:
Junda Wu, Cheng-Chun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng
Hou, and Julian McAuley. 2024. CoRAL: Collaborative Retrieval-Augmented
Large Language Models Improve Long-tail Recommendation. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671901
1 Introduction
Recommendation systems are valuable tools for users to explore
content that matches their preferences. Traditional data-driven
recommendation algorithms (e.g. , collaborative filtering) can fail in
long-tail recommendation, due to the uneven distribution in user-
item interactions [ 26,30,40,73]. In this paper, we aim to address the
challenges associated with long-tail items in collaborative filtering-
based recommender systems [ 71,73]. In such scenario, long-tail
items may have very few associated interactions, such that data-
driven algorithms cannot accurately capture user-item interaction
patterns [ 12,28,48]. In addition, models trained on such uneven
datasets can suffer from selection bias [ 37,53], exposure bias [ 15,18]
and popularity bias [ 1,57,68]. These biases can cause the models
to overfit on popular items.
To tackle popularity bias and improve long-tail recommendation
performance, data augmentation, and re-balancing methods can
be directly applied. Data re-balancing methods [ 8,11,32,64] try
to reduce the distribution discrepancy between popular items and
long-tail items in the training stage. However, these methods often
obtain sub-optimal solutions due to learning inefficiency problems
3391
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junda Wu et al.
on long-tail data [ 71,73]. This inefficiency leads to knowledge for-
getting in the majority of the data, namely the popular items [ 73].
Since the goal of these methods is to achieve a compromise be-
tween the modelâ€™s attention on popular items and more diversified
recommendations on long-tail items, achieving accurate recom-
mendations for long-tail items can be challenging. Causal debiasing
learning [ 6,44,76] is another line of work that focuses on how to
learn the underlying user preferences, instead of simply learning
user-item correlation from the data. Since long-tail items only have
limited numbers of previous interactions, the modelâ€™s fine-grained
reasoning abilities become essential for learning user preferences.
Large language models (LLMs) have recently demonstrated great
reasoning abilities on very complex reasoning tasks [ 47,50,66], in
which fine-grained reasoning paths can be generated to help with
obtaining the correct answers [ 56,74]. Previous works have also
tried to adapt LLMsâ€™ reasoning abilities to recommender systems
[54,55]. One line of previous works tries to use the language de-
scription of item content as the reasoning context [ 16,43], which
can be augmented by the LLMâ€™s internal knowledge [ 4,58,63,70].
By representing items as natural language, item representation
distribution is aligned with the LLMâ€™s knowledge. This alignment
allows for a universal semantic representation of items, potentially
mitigating the issue of long-tail items. In addition, by aligning rec-
ommendation tasks to the reasoning paradigms of language models,
LLMs can be leveraged for more fine-grained reasoning based on
the semantic contexts of users and items [ 54,55]. However, due to
several misalignment issues of LLMs in recommendation [ 31], di-
rectly prompting LLMs can be problematic. Specifically, the LLMâ€™s
understanding of user preferences over items can be misaligned
with real user-item interaction patterns. For example, in Figure 1a,
conventional LLM-based methods may simply recommend similar
items (e.g., â€œCaillou Magic Playhouseâ€ is recommended because the
user likes â€œCaillou Four Seasons of Funâ€).
In this work, we propose to formulate long-tail recommendation
tasks as natural language reasoning problems and use LLMs to
enable fine-grained reasoning about user preferences on long-tail
items. To further align the reasoning of LLMs with task-specific
knowledge of user-item interactions, we introduce collaborative
retrieval-augmented LLMs, CoRAL, which directly incorporate
collaborative evidence into the prompts, via collaborative prompt-
ing. For example, in Figure 1b, additional user-item interaction
information can be retrieved by an additional lightweight model
and included in the prompt. Based on the retrieved collaborative
information, the LLM can find that although the items share the
same theme (e.g., â€œCaillouâ€), the item (e.g., â€œCaillou Four Seasons of
Funâ€) is disliked by users who share such interests (e.g., preference
on â€œCaillou Four Seasons of Funâ€). However, due to the limited size
of the prompts, a large amount of collaborative information cannot
be included, and duplicate information may also distract the LLMâ€™s
reasoning process. Thus, we develop a retrieval policy to find the
minimal-sufficient collaborative information which serves as the
supporting evidence of the LLMâ€™s reasoning on the given user-item
pair. Since the number of users and items in a recommendation task
is significantly larger than the capacity of a prompt input in LLMs,
the retrieval policy should learn to explore diversified users and
items for potential information gain, as well as exploit the collected
collaborative information to maximize prediction accuracy. Based
Review History
Caillou Four 
Seasons of 
Fun
Mia's 
Science 
Adventurelike
doesnâ€™t 
likeQuestion : 
Would user #0 like â€œCail-
lou Magic Playhouseâ€?Answer : 
1. The item shares the same theme with â€œCaillou Four Seasons of Funâ€.2. The item less focuses on traditional academic learning like â€œMia's Science Adventureâ€.Based on this analysis, user #0 would like the item.
 User #0(a) Conventional item-based [16, 43] LLM reasoning process.
User #0 #1 #3
User #2
User #0 #3Review History
User #0 #1Caillou Four 
Seasons of 
Fun
Caillou 
Magic 
PlayhouseQuestion: 
Would user #0 like â€œCail-lou Magic Playhouseâ€?Answer : 
1. The item has shown a divisivepreference among users with similar
interests.
2. Although the item shares the
same theme with â€œCaillou Four Seasons of Funâ€
, the item is disliked 
by users who share such interests.Based on this analysis, user #0 
would not like  the item.
Jay Jay the 
Jet Planelikedoesnâ€™t 
likelike
like
doesnâ€™t 
like
(b) Collaborative Retrieval Augmented LLM reasoning process.
Figure 1: By text comprehension and extracting information-
rich semantic features [ 42], LLMs in (a) can handle long-
tail items [ 43], but still cannot directly leverage collabo-
rative information. To handle long-tail items in collabora-
tive filtering-based recommender systems, by collaborative
prompting, LLMs in (b) can reason the fact that even if the cur-
rent item shares the same theme with previously liked items,
users with similar interests still dislike this item, which pro-
vides the rationale to not recommending it.
on the necessity to balance between exploration and exploitation in
learning the retrieval policy, we propose to formulate the retrieval
process as a sequential decision-making problem and employ rein-
forcement learning methods to maximize the long-term reward.
To improve the data efficiency and reduce the early exploration
time, we also propose to use the data from popular items to provide
a warm start for the learning of the retrieval policy. Before the
reinforcement learning stage, the user and item representations are
learned from the data from popular items by conventional collab-
orative filtering methods. Then, the retrieval policy network will
be initialized by the learned user and item representations, which
improves the exploration efficiency and helps to solve the reward-
sparsity problem. We summarize our contributions as follows:
â€¢We identify the misalignment problem between the LLMâ€™s
reasoning process and long-tail recommendation tasks, which
is caused by the lack of collaborative information.
â€¢We propose to retrieve additional user-item interactions as
collaborative information for collaborative prompting, which
helps to align the LLMâ€™s reasoning process to general rec-
ommendation tasks.
â€¢We formulate the retrieval process as a sequential decision-
making task and propose an RL framework, in which the
3392CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
retrieval policy learns to find the minimal-sufficient collabo-
rative information specific to a recommendation task.
â€¢To further improve data efficiency, we propose to learn the
prior knowledge from more abundant data of popular items
and to provide a warm start for the retrieval policy.
2 Related Works
2.1 Long-tail Recommendation
Long-tail recommendation plays a crucial role in mitigating the
issue of highly skewed distributions of long-tail items in recommen-
dation tasks. Previous works have proposed knowledge transfer
learning methods based on novel model architecture designs. Zhang
et al. [71] introduces a dual transfer learning framework by leverag-
ing a model-level knowledge transfer and an item-level transfer to
link head and tail items through shared features. Zhang et al . [73]
propose a Cross Decoupling Network (CDN). This network aims
to improve tail item recommendations while simultaneously main-
taining overall system efficiency. Wu et al . [59] propose a domain
transfer learning method (DACIR) for the sequential recommenda-
tion. However, the popularity bias in such long-tail or cold-start
recommendation datasets is less discussed in their model designs.
To address the issue of bias within the dataset in recommender
systems, extensive efforts [ 6,27,60,61,76] have been dedicated to
designing different training frameworks via causal debiasing. In
this work, we propose to handle long-tail items by leveraging LLMsâ€™
abilities in fine-grained reasoning on collaborative information and
extracting rich semantic features.
2.2 LLMs in Recommender Systems
A few studies underscore the expanding role of LLMs in recom-
mender systems. Harte et al . [16] focus on enhancing sequential
recommendation models with LLM embeddings, while Sanner et al .
[43]explore the use of LLMs in processing language-based user pref-
erences in dialog interfaces. A surge of approaches [ 4,58,63,70]
propose content augmentation method to reduce the cost of re-
training or fine-tuning LLMs. To improve LLMsâ€™ reasoning capabil-
ity for recommender systems, [54] proposes RecMind, specifically
designed to deliver personalized recommendations. Wang et al .
[55], on the other hand, use a retriever-reranking framework to en-
hance collaborative in-context understanding. However, the LLMâ€™s
understanding of user preferences over items can be misaligned
with real user-item interaction patterns, due to the lack of enough
collaborative information.
To further align the LLMâ€™s reasoning process to specific rec-
ommendation tasks, several works have proposed to inject collab-
orative knowledge into LLMs by soft-prompt instruction tuning
[41,72,75,77]. [23] also introduces a method to transform discrete
task-specific prompts into continuous prompt vectors, effectively
linking IDs and words while decreasing inference time. However,
such methods require an even larger amount of data to achieve
good alignment, which would not help in our long-tail recommen-
dation setting. Instead, we propose a lightweight retrieval policy to
augment collaborative information in LLMsâ€™ reasoning process.3 Problem Formulation
In this paper, we focus on collaborative filtering-based recom-
mender systems with long-tail items [ 71,73]. We formulate long-tail
recommendation as a complex reasoning task in LLMs [ 20,22,49].
When the LLM-empowered recommender system interacts with a
userğ‘¢âˆˆU, the task is to predict the userâ€™s preference for a long-
tail itemğ‘–âˆˆI. Since LLMs have no prior internal knowledge about
a certain userâ€™s preference as well as the collaborative informa-
tion of a certain recommendation task, the reasoning process can
only be enabled by incorporating supporting evidence. To provide
the information for the user ğ‘¢, itemsIsupp
ğ‘¢=(ğ‘–ğ‘¢
1,ğ‘–ğ‘¢
2,...,ğ‘–ğ‘¢ğ‘š)âŠ†I
from the userâ€™s previous interactions are included, which helps the
LLM to understand the preference of the user [ 5,17,54]. To help
the LLM further understand how the user ğ‘¢would rate a certain
itemğ‘–, collaborative information will be retrieved as evidence of
user-item interaction patterns. Due to the limitation of the LLMâ€™s
reasoning context capacity, instead of including all the user-item
interaction information, for a certain user-item pair ğ‘§=(ğ‘¢,ğ‘–),
the retrieval policy ğœ‹ğœƒwill find a sequence of supporting users
Ucollğ‘§=(ğ‘¢ğ‘§
1,ğ‘¢ğ‘§
2,...,ğ‘¢ğ‘§
ğ‘¡)âŠ†U and a sequence of supporting items
Icollğ‘§=(ğ‘–ğ‘§
1,ğ‘–ğ‘§
2,...,ğ‘–ğ‘§
ğ‘¡)âŠ†I . At each time step ğ‘¡, the retrieval policy
ğœ‹ğœƒneeds to retrieve the next user-item pair (ğ‘¢ğ‘§
ğ‘¡+1,ğ‘–ğ‘§
ğ‘¡+1)to augment
current supporting evidence. In this work, we focus on how to
obtain a minimal-sufficient information support for the LLM to
deduce the accurate rating of ğ‘§.
3.1 MDP Formulation for Retrieval Policy
We formulate the sequential retrieval process as a Markov Decision
Process (MDP)M=(S,A,ğ‘ƒ,ğ‘Ÿ,ğœŒ,ğ›¾), where
â€¢S is a continuous state space that encodes the collaborative
information from the retrieved users and items as well as
their collaborative preference patterns. The details of the
design of state ğ’”âˆˆS encoding networks are explained in
Section 4.2.
â€¢A is a continuous action space that represents the retrieval
queries of the next user and next item. At time step ğ‘¡+1, the
retrieval queries will try to retrieve the most relevant user
and item in their feature spaces, as well as try to explore po-
tentially useful users and items in the under-explored regions.
The details of the action ğ’‚âˆˆA prediction are explained in
Section 4.2.
â€¢ğ‘ƒ:SÃ—AÃ—S â†’ R, is the state transition probability
distribution, which captures the dynamics of the retrieval
process.
â€¢ğ‘Ÿ:SÃ—Aâ†’ R, is the reward function ğ‘Ÿ(ğ’”,ğ’‚)given current
state ğ’”and action ğ’‚. The details of the reward function design
are explained in Section 3.2.
At each time step ğ‘¡, the policy will generate the retrieval query
ğ’‚ğ‘¡âˆˆA and retrieve the next user-item pair (ğ‘¢ğ‘§
ğ‘¡,ğ‘–ğ‘§
ğ‘¡). The learn-
ing objective is to find the optimal retrieval policy ğœ‹âˆ—
ğœƒ:Sâ†’A
to achieve the long-term goal of obtaining a minimal-sufficient
information support for the LLM, by maximizing the cumulative
reward:
ğœ‹âˆ—
ğœƒ=arg max
ğœ‹âˆˆÎ E"ğ‘‡âˆ‘ï¸
ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿ(ğ’”ğ‘¡,ğ’‚ğ‘¡)#
,
3393KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junda Wu et al.
in whichğ›¾is the discount rate of future rewards and Î is the policy
search space. When the retrieval policy observes the next user-item
pair(ğ‘¢ğ‘§
ğ‘¡,ğ‘–ğ‘§
ğ‘¡), the state is updated by encoding the user-item pair
into the state information ğ’”ğ‘¡+1=ğ‘ƒ(Â·|ğ’”ğ‘¡,[ğ‘¢ğ‘§
ğ‘¡,ğ‘–ğ‘§
ğ‘¡]).
3.2 Reward Function
For each time step ğ‘¡, the user-item rating prediction ğ‘¦ğ‘¡will be
prompted from the large language model ğ‘ƒğœ™by the context ğ¶ğ‘¡
constructed from user information Isupp
ğ‘¢, and previously collected
collaborative information Ucollğ‘§andIcollğ‘§,
ğ‘ğ‘¡=ğ‘ƒğœ™(ğ‘¦ğ‘¡|ğ¶ğ‘¡), (1)
ğ¶ğ‘¡=ğ¶
Isupp
ğ‘¢,Ucoll
ğ‘§,Icoll
ğ‘§
, (2)
in whichğ‘ğ‘¡is the prediction likelihood of whether the user ğ‘¢likes
the itemğ‘–, andğ¶is the prompt template (detailed description in
Section 4.1) which composes the collected information into a natural
language query.
Since the motivation of the retrieval policy is to maximize cu-
mulative information gain, we use the marginal information gain
at each time step ğ‘¡as the reward signal ğ‘Ÿğ‘¡, which is calculated by
the prediction discrepancy,
ğ‘Ÿğ‘¡ ğ‘ ğ‘¡,(ğ‘¢ğ‘§
ğ‘¡,ğ‘–ğ‘§
ğ‘¡)=ğ‘ğ‘¡âˆ’1âˆ’ğ‘¦ğ‘”ğ‘¡
|        {z        }
discrepancy at ğ‘¡âˆ’1âˆ’ğ‘ğ‘¡âˆ’ğ‘¦ğ‘”ğ‘¡
|     {z     }
discrepancy at ğ‘¡, (3)
in whichğ‘¦ğ‘”ğ‘¡=M(ğ‘¢,ğ‘–)is the ground truth label of the userâ€™s pref-
erence on the item from the training rating matrix ğ‘€. Following
[35,78], we reward those retrieved user-item pairs which will lead
the constructed prompt to find a more accurate prediction based
on the LLM ğ‘ƒğœ™.
4 Proposed Framework: CoRAL
In this section, we first explain the prompting method designed to
incorporate collaborative information and collect the LLMâ€™s pre-
diction as the recommendation prediction. Then, we introduce the
collaborative retrieval policy network as well as the reinforcement
learning process, which is illustrated in Algorithm 1. In reinforce-
ment learning, we treat the LLM as part of the environment and
thus the LLM is frozen while a lightweight retrieval policy is learn-
able with significantly fewer learning parameters. To further im-
prove the policyâ€™s learning efficiency and accommodate long-tail
recommendation, we propose to use collaborative filtering models
learned on the short-head data as the model initialization (detailed
experimental settings and comparison results are in Section 5.3).
4.1 Collaborative Prompting
In this section, we will explain how to construct the prompt ğ¶ğ‘¡
with the retrieved collaborative information in Eq. (2), and how to
obtain the prediction probability ğ‘ğ‘¡in Eq. (1), given the retrieval
results from the policy ğœ‹ğœƒ. Details about the policy network design
will be explained in Section 4.2.
Collaborative Information. At time step ğ‘¡, given the user-item
pairğ‘§=(ğ‘¢,ğ‘–), the retrieval policy ğœ‹ğœƒobtains the supporting users
Ucollğ‘§and itemsIcollğ‘§. To describe the users and items in natural
language and incorporate them into the prompt, we represent each
userğ‘¢ğ‘§
ğ‘¡âˆˆUcollğ‘§by their user index idxU(ğ‘¢ğ‘§
ğ‘¡), and each item ğ‘–ğ‘§
ğ‘¡âˆˆIcollğ‘§by its item index idxI(ğ‘–ğ‘§
ğ‘¡). We further extract a short text
description descI(ğ‘–ğ‘§
ğ‘¡)for each item from the metadata (detailed
descriptions in Section 5.1.1), to assist the LLMâ€™s understanding of
the item. Based on the rating matrix ğ‘€in the training dataset, we
can summarize usersâ€™ shared preference for each item ğ‘–âˆˆIcollğ‘§in
the following format:
POS(ğ‘–,Ucoll
ğ‘§)=n
M(ğ‘–,ğ‘¢)â‰¥ğ‘¦ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„,ğ‘¢âˆˆUcoll
ğ‘§o
,
NEG(ğ‘–,Ucoll
ğ‘§)=n
M(ğ‘–,ğ‘¢)<ğ‘¦ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„,ğ‘¢âˆˆUcoll
ğ‘§o
, (4)
in which the rating threshold ğ‘¦ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„is to determine if the rating
is positive or negative. By aggregating the preference of a group of
users for each item, the length of the prompt can be significantly
reduced, and such descriptions prompt the LLM to focus more on
the comparative preference among the users. To construct the first
part of the prompt which contains collaborative information, we
design the prompt as follows:
â€¢Role-play: As a recommender system please solve the fol-
lowing problem.
â€¢Collaborative Information: Repeat ğ‘–âˆˆIcollğ‘§ The item descI(ğ‘–)is liked by the users POS(ğ‘–,Ucollğ‘§).
The item descI(ğ‘–)is disliked by the users NEG(ğ‘–,Ucollğ‘§).
â€¢Summarization: Try to understand the pattern that the
itemdescI(ğ‘–)is typically liked by what kinds of users based
on the above information.
Based on our empirical observations, the last Summarization
instruction is essential to align the LLMâ€™s reasoning with the goal
of this task.
User Preference Representation. To include more information
on the userâ€™s preference, we follow previous works [ 4,58,63,70]
to include the userâ€™s previously interacted items Isupp
ğ‘§ and their
text descriptions. However, different from previous works, we also
divide the previous items Isupp
ğ‘§ of userğ‘¢into positive and negative
sets, and then query the LLM to deduce the rating for the user-item
pairğ‘§=(ğ‘¢,ğ‘–),
POS(Isupp
ğ‘§,ğ‘¢)=n
M(ğ‘–,ğ‘¢)â‰¥ğ‘¦ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„,ğ‘–âˆˆIcoll
ğ‘§o
,
NEG(Isupp
ğ‘§,ğ‘¢)=n
M(ğ‘–,ğ‘¢)<ğ‘¦ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„,ğ‘–âˆˆIcoll
ğ‘§o
, (5)
Then, we construct the second part of the prompt by including the
userâ€™s previously interacted items Isupp
ğ‘§:
â€¢Userâ€™s Positive Preference: Items the user idxU(ğ‘¢)likes
are as follows: POS(Isupp
ğ‘§,ğ‘¢).
â€¢Userâ€™s Negative Preference: Items the user idxU(ğ‘¢)does
not likes are as follows: NEG(Isupp
ğ‘§,ğ‘¢).
â€¢Query: For the item described as idxI(ğ‘–), would you rec-
ommend it to the user idxU(ğ‘¢)?
With the prompt design (denoted as ğ¶) described above, we can
aggregate the information retrieved at time step ğ‘¡and transform the
information into a natural language prompt ğ¶ğ‘¡=ğ¶
Isupp
ğ‘¢,Ucollğ‘§,Icollğ‘§
.
To obtain the LLMâ€™s prediction as well as its confidence score, we
extract the prediction probability ğ‘ğ‘¡=ğ‘ƒğœ™(ğ‘¦ğ‘¡|ğ¶ğ‘¡)of the next token
generated from the LLM. Specifically, we strictly ask the LLM to
answer either â€œYesâ€ or â€œNoâ€ without additional text provided, and
3394CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
we take the probability of the LLM generating the token â€œYesâ€ as
our final score ğ‘ğ‘¡.
4.2 Retrieval Policy Network
In this section, we design the retrieval policy ğœ‹ğœƒto sequentially
include additional users and items, which may provide an infor-
mation gain for the LLMâ€™s reasoning. Since the prompt has only a
limited capacity of users and items included, the goal of the retrieval
policy is to construct a minimal-sufficient prompt that contains
complete information about the current recommendation task of
the user-item pair ğ‘§=(ğ‘¢,ğ‘–), Specifically, the retrieval policy needs
to maximize its long-term information gain by maximizing the
cumulative reward function.
Instead of learning the action distribution over all the users and
items like value-based reinforcement learning methods [ 33,45], we
choose to directly learn the continuous vector representations of
the next user and item based on the DDPG algorithm [ 25], which
helps to learn a low-rank decision space and also makes the solution
more scalable even with new users and items included during the
inference stage.
4.2.1 State Representation. For each user-item pair ğ‘§=(ğ‘¢,ğ‘–), the
retrieval process starts with the user-item embedding ğ’”0=[ğ’–,ğ’Š]âˆˆ
R2ğ‘‘, in which ğ’–andğ’Šare the user and item embeddings in ğ‘‘di-
mensions. Notably, the user and item embeddings are randomly
initialized by the multivariate normal distribution ğ’–âˆ¼N( ğ,ğšº)and
ğ’Šâˆ¼N( ğ,ğšº), in which ğis ağ‘‘-dimensional zero-vector and ğšºis a
ğ‘‘Ã—ğ‘‘unit matrix.
During the early stage of the reinforcement learning process,
when the retrieval policy behaves randomly, similar users and items
are likely to be retrieved due to the large-scale user and item spaces,
which makes the reward of the policyâ€™s exploration very sparse. To
overcome the exploration difficulty, we initialize the policy with
the embeddings pre-trained on the portion of the dataset with
popular items, which can provide a warm start for the learning of
the retrieval policy (detailed comparison results are explained in
Section 5.2 and Section 5.3).
4.2.2 User-item Retrieval. At each time step ğ‘¡, based on the current
state ğ’”ğ‘¡, the retrieval policy ğœ‹ğœƒwill find the next user-item pair.
Due to the large user and item spaces, direct exploration in the
discrete spaces of the users and items can be extremely inefficient.
Thus, we employ a continuous action space AâŠ†R2ğ‘‘which also
covers the user-item embedding space. The retrieval policy will
first generate a user-item query based on the current state ğ’‚ğ‘¡+1=
[ğ’‚ğ‘¢
ğ‘¡+1,ğ’‚ğ‘–
ğ‘¡+1]=ğœ‹ğœƒ(Â·|ğ’”ğ‘¡), and try to find the nearest user and item in
terms of a distance measurement ğ‘‘(Â·,Â·)defined on the embedding
spaces,
ğ‘¢ğ‘§
ğ‘¡+1=min
ğ‘¢âˆˆUğ‘‘(ğ’–,ğ’‚ğ‘¢
ğ‘¡+1), ğ‘–ğ‘§
ğ‘¡+1=min
ğ‘–âˆˆUğ‘‘(ğ’Š,ğ’‚ğ‘–
ğ‘¡+1), (6)
in which ğ’–andğ’Šdenote the embeddings of the user ğ‘¢and itemğ‘–
respectively, and we use Euclidean distance for ğ‘‘. The retrieved
user and item will be added to the collaborative information Ucollğ‘§
andIcollğ‘§.
4.2.3 State Transition. The state ğ’”ğ‘¡encodes the current collabora-
tive information, which will be updated for each time step after theuser and item is retrieved. To track the retrieval process and aggre-
gate the collected information, we use the multi-layer perception
model (MLP) for state transition modeling,
ğ’”ğ‘¡+1=MLP(ğ’”ğ‘¡,[ğ’–ğ‘§
ğ‘¡+1,ğ’Šğ‘§
ğ‘¡+1]), (7)
in which ğ’–ğ‘§
ğ‘¡+1andğ’Šğ‘§
ğ‘¡+1are the embeddings of the retrieved user
and item at time step ğ‘¡.
4.3 Minimal-sufficient Collaborative
Information via Reinforcement Learning
We follow the standard DDPG [ 25] reinforcement learning frame-
work to train our retrieval policy with the continuous action space.
In the Actor-Critic framework [ 25], the critic is learning a Q-value
function with episodic mini-batch sampled from the replay buffer
[34],
ğ¿(ğœƒğ‘„)=Eğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²
ğ‘Ÿ+ğ›¾ğ‘„ğœƒğ‘„â€² ğ‘ â€²,ğœ‹ğœƒğœ‡â€² Â·|ğ‘ â€²âˆ’ğ‘„ğœƒğ‘„(ğ‘ ,ğ‘)2,(8)
in whichğœƒğ‘„â€²is the target network [ 25] of the critic, which is fixed
during the act network update. Based on the learning objective
in Eq. (8), we can derive the gradient âˆ‡ğœƒğ‘„ğ¿(ğœƒğ‘„)to update the act
network of the critic. Since the critic provides an approximation of
the Q-value function, the optimization step of the actor network
can be achieved by policy gradient,
âˆ‡ğœƒğœ‡ğ¿(ğœƒğœ‡)=Eğ‘ 
âˆ‡ğ‘ğ‘„ğœƒğ‘„(ğ‘ ,ğ‘)âˆ‡ğœƒğœ‡ğœ‹ğœƒğœ‡(Â·|ğ‘ )
, (9)
similar to the critic network, the policy gradient only updates the
act network of the actor, while the target actor network ğœ‹ğœƒğœ‡â€²will
be synchronized after each update step.
To further enable continuous space exploration, we follow [ 25] to
add explorationNnoise to the target policy ğœ‹ğœƒğœ‡â€²to find unexplored
but informative users and items,
ğœ‹ğœƒğœ‡â€²(Â·|ğ‘ )=ğœ‹ğœƒğœ‡(Â·|ğ‘ )+N, (10)
in which we choose the Ornsteinâ€“Uhlenbeck [ 38] random process
as the exploration process N.
5 Experiments
In this section, we conduct extensive experiments on multiple
datasets to investigate the following research questions (RQs):
â€¢RQ1: How does collaborative information help to align the
LLMâ€™s reasoning process to general recommendation tasks?
â€¢RQ2: Can CoRAL find sufficient collaborative evidence to
enhance LLMsâ€™ reasoning?
â€¢RQ3: Can CoRAL find minimally-sufficient collaborative
evidence to fit the size of prompts?
5.1 Experimental Settings
5.1.1 Datasets. We evaluate CoRAL and baselines on four Amazon
Product [ 36] tasks, which are used in the evaluations of many
collaborative filtering methods [69]:
â€¢Appliances refers to a category of home and kitchen devices
sold on Amazon. This subset contains 602,777 reviews with
515,650 users and 30,252 products. We use the â€œtitleâ€ of the
items in the metadata as item descriptions.
3395KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junda Wu et al.
Algorithm 1 Training Procedure of CoRAL
Input episode length ğ¿, Maximum steps in an episode ğ‘‡.
Initialize actor network ğœƒğœ‡and critic network ğœƒğ‘„
Initialize target networks ğœƒğœ‡â€²â†ğœƒğœ‡andğœƒğ‘„â€²â†ğœƒğ‘„
Initialize the replay bufferD=âˆ…
whileğ‘™â‰¤ğ¿do
Receive a user-item pair ğ‘§=(ğ‘¢,ğ‘–)
InitializeIsupp
ğ‘¢=âˆ…,Ucollğ‘§=âˆ…,Icollğ‘§=âˆ…
Construct the prompt of user preference Isupp
ğ‘¢ as in Eq. (5)
Get the initial prediction ğ‘0according to Eq. (2)
whileğ‘¡â‰¤ğ‘‡do
User-item Retrieval
Generate the current action ğ’‚ğ‘¡from the policy ğœ‹ğœƒğœ‡â€²
Locate the next user-item pair (ğ‘¢ğ‘§
ğ‘¡,ğ‘–ğ‘§
ğ‘¡)as in Eq. (6)
Add to the support sets, Ucollğ‘§â†ğ‘¢ğ‘§
ğ‘¡andIcollğ‘§â†ğ‘–ğ‘§
ğ‘¡
Collaborative Prompting
Construct the prompt of collaborative information Ucollğ‘§
andIcollğ‘§according to Eq. (4)
Get the current prediction ğ‘ğ‘¡as in Eq. (1)
Calculate the current reward ğ‘Ÿğ‘¡according to Eq. (3)
Observe the next state ğ’”ğ‘¡+1according to Eq. (7)
Store the transition quadruple (ğ’”ğ‘¡,ğ’‚ğ‘¡,ğ‘Ÿğ‘¡,ğ’”ğ‘¡+1)inD
Networks Update
Sample a minibatch of the quadruple (ğ’”,ğ’‚,ğ‘Ÿ,ğ’”â€²)fromD
Calculate the minibatch loss ğ¿(ğœƒğ‘„)for the critic network
according to Eq. (8)
Update the critic network by the gradient âˆ‡ğœƒğ‘„ğ¿(ğœƒğ‘„)
Update the actor network with the sampled policy gradi-
ent according to Eq. (9)
Update the target networks:
ğœƒğ‘„â€²â†ğœğœƒğ‘„+(1âˆ’ğœ)ğœƒğ‘„â€²
ğœƒğœ‡â€²â†ğœğœƒğœ‡+(1âˆ’ğœ)ğœƒğœ‡â€²
end while
end while
â€¢Gift Cards on Amazon are prepaid store value cards that
can be used as an alternative to cash. This subset contains
147,194 reviews with 128,877 users and 1,548 products. We
use the â€œdescriptionâ€ of the items in the metadata.
â€¢Prime Pantry on Amazon refers to a service offering a wide
range of everyday household items and groceries. The sub-
set contains 471,614 reviews with 247,659 users and 10,814
products. We use the original â€œdescriptionâ€ in the metadata
as the item descriptions.
â€¢Software goods on Amazon refer to digital products. The
subset contains 459,436 reviews with 375,147 users and 21,663
products. The software product titles in the metadata are
used as the item descriptions.
Due to the missing descriptions of items in the metadata of some
datasets, we use the item titles as the replacement. To determine
the boundary between popular items and long-tail items, we follow
the typical 80/20 rule [ 29,46,65,67], which defines the least 80%
items as long-tail items. Due to the sparsity of the datasets, manyusers and items only have very few entries in the datasets, in which
case collaborative information is almost inaccessible. To maintain a
certain number of interaction data samples, We follow [ 9,19,24,72]
to filter out users and items with fewer than 5 interactions. For
learning-based baselines and CoRAL, we use 70%of the long-tail
data and the remaining data in the dataset as the training data.
The remaining 30% of the long-tail data is split equally into the
validation and test data. We follow the standard preprocessing
method [ 52,72] to convert the original 5-score into binary labels
by the threshold of 3.
5.1.2 Metrics. We follow the metrics AUC andF1in long-tail rec-
ommendation [ 13,68] and collaborative filtering [ 3,72]. The Area
Under the Curve (AUC) metric is a performance measurement for
classification models that evaluates the tradeoff between true posi-
tive rate and false positive rate across different thresholds, where a
higher AUC indicates better model performance. The F1 metric is a
statistical measure used in classification tests, combining precision
and recall to provide a score that balances both false positives and
false negatives, calculated as the mean of precision and recall.
5.1.3 Baselines. We introduce baselines in our experiments from
three lines of work, collaborative filtering, popularity debiasing,
and LLM-based recommendation methods:
Collaborative Filtering:
â€¢AFM [62]: A model learns the significance of each feature
interaction from data through a neural attention network.
â€¢DCN [51]: A deep neural network featuring a cross-structure
is designed for enhanced efficiency in learning specific bounded-
degree feature interactions.
â€¢DFM [14]: A unified neural network architecture for rec-
ommender systems is proposed, integrating factorization
machines and deep learning.
â€¢WDL [10]: A method that integrates wide linear models
with deep neural networks is proposed for enhancing rec-
ommender systems. This approach synergistically leverages
the strengths of both memorization and generalization.
Popularity debiasing baselines directly enhance the collaborative
filtering methods via causal debiasing:
â€¢IPS[44]: An approach utilizes causal inference techniques
to address selection biases in data, ensuring unbiased perfor-
mance estimation with biased data.
â€¢CausE [6]: A domain adaptation technique is developed to
train on historical data that captures results from a recom-
mendation system biased by a specific policy and makes
predictions for recommendation outcomes under random
exposure conditions.
To understand the benefit of collaborative information in LLMs,
we consider a LLM-based baseline:
â€¢LLM-Language [43]: A LLM prompting method that de-
scribes the userâ€™s interacted items and the userâ€™s preferences
before asking for the userâ€™s preference on new items.
To understand the behavior of our approach, we consider vari-
ants of CoRAL:
â€¢CoRAL-Method : The collaborative information augmented
LLMs, in which the retrieval policy is initialized by the
3396CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Softwar
e Prime
Pantry Gift
Cards Appliances A
verage
AUC
F1 AUC
F1 AUC
F1 AUC
F1 AUC
F1
AFM [62] 75.12
58.39 69.47
52.51 46.93
61.56 76.86
65.52 67.10
59.49
DCN [51] 76.75
66.20 73.30
49.99 55.59
67.07 80.70
71.15 71.59
63.60
DFM [14] 76.04
66.63 72.92
57.86 66.76
60.01 81.83
77.37 74.39
65.47
WDL [10] 78.20
69.25 73.77
56.43 60.81
57.66 73.82
74.56 71.65
64.48
IPS[44] 78.24
71.32 72.24
61.65 64.79
63.95 82.28
75.65 74.39
66.23
CausE [6] 77.78
70.84 73.69
59.80 70.51
65.39 76.86
72.04 74.71
67.02
LLM-Language [43] 73.10
66.32 51.48
41.47 83.52
74.85 74.36
70.52 70.61
63.29
CoRAL-random 77.56
58.60 64.07
50.15 91.30
59.66 77.51
61.35 77.61
57.44
CoRAL-DFM 95.25
88.68 93.32
86.73 96.52
67.51 90.87
86.76 93.99 82.42
CoRAL-
WDL 93.97 91.18 87.08
80.52 92.22
70.74 92.55 89.22 91.45
82.92
CoRAL-AFM 93.99 88.41 89.10
86.17 98.99
76.17 92.66 84.55 93.69 83.83
CoRAL-DCN 91.74
87.20 85.75
77.59 97.16
70.63 91.73
86.28 91.59
80.43
Table 1: Experimental results (AUC and F1) on four Amazon Product datasets.
Method. The Method in our experiments includes DFM,
WDL, AFM, and DCN.
â€¢CoRAL-random: The LLM is also augmented by collabora-
tive information. However, the retrieval policy is a rule-based
method, which retrieves a random item from the candidates
and the closest user based on Jaccard similarity.
5.1.4 Implementation Details. We implement our retrieval policy
network using PyTorch 2.1. For reinforcement learning, the DDPG
[25] policy network is implemented using Stable-Baselines3 [39].
We set the memory buffer size to 1000 and the training batch size
to16for both the actor and the critic. For the continuous action
for the next user and item, we set the dimensions for both as 128,
which aligns with the size of the user and item embedding. The
Ornsteinâ€“Uhlenbeck noise [ 38] added in each dimension of the
continuous action space for exploration is set to zero-mean and the
standard deviation as ğœ=0.1. The reinforcement learning process
starts at the 10-th iteration, which enables a warm start. We use
Adam optimizer [ 21] for all the model learning with the learning
rate as 0.001, and we set the maximal learning iterations to 2,000.
We implement the reinforcement learning environment using
Gym[ 7], and we use a GPT-4 [ 2] model as the backbone large
language model to provide reward. To extract probability for each
generated token, we directly query the GPT-4 API1to obtain each
tokenâ€™s generation probability. During the training stage, we allow
up to 10interactions within a single episode and enable early stop if
the absolute value of the discrepancy between the predicted rating
and the ground truth rating is less than 0.1. During the evaluation
stage, for each data sample, we consistently let the policy retrieve
5rounds of users and items as collaborative information.
5.2 Recommendation Performance (RQ1)
We show the comparison results of CoRAL and various baselines
to demonstrate the effectiveness of augmenting LLMs with collabo-
rative information as reasoning evidence.
5.2.1 Effect of the Retrieval Policy. In Table 1, we can observe that
by adding collaborative information into the LLMâ€™s prompt, even
1https://platform.openai.com/docs/api-reference/chat/create#chat-create-logprobif the retrieved users and items are randomly chosen, CoRAL-
random consistently outperforms LLM-Language in terms of
the AUC scores. Such observations may imply that collaborative
information is still crucial to specific recommendation tasks, even
if the LLM can understand the general semantic meanings of the
items and the userâ€™s preference. On the other hand, we also observe
thatCoRAL-random generally performs worse (except for Prime
Pastry) than LLM-Language in terms of the F1 scores. One reason-
able explanation is that since CoRAL-random is not specifically
curating its selection of users and items, irrelevant information may
bring additional bias into the recommendation process and cause
the model to have a poor precision-recall trade-off.
5.2.2 Effect of Online Reinforcement Learning. In Table 1, we ob-
serve an inconsistency in performance comparison between tra-
ditional recommendation baselines and LLM-based baselines, as
the LLM-based methods can sometimes (e.g., Prime Pastry and Ap-
pliances) perform substantially worse than traditional baselines.
This inconsistency in the performance of LLM-based methods sug-
gests the misalignment between the LLMâ€™s reasoning and specific
recommendation tasks. The proposed method CoRAL specifically
aligns the LLMâ€™s reasoning with the recommendation tasks through
reinforcement learning. With the LLMâ€™s reasoning process aligned
to the user-item interaction patterns, we can observe significant
improvements up to 21.1%and25.1%for AUC and F1 scores respec-
tively on average.
5.3 Sufficient Collaborative Information from
Popular Items (RQ2)
We conduct analytical experiments in this section to show how
learning from popular items can benefit online reinforcement learn-
ing. We choose DFM and WDL as the backbone models in the
analytical experiments to show their different learning behaviors.
5.3.1 Comparison to Randomly Initialized Policy. In Table 2, we
compare our method, which initializes the retrieval policy by learn-
ing from the popular items, with the variant that randomly initial-
izes the policy. We show that the policies initialized with models
learned from popular items are generally performing better than
3397KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junda Wu et al.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000017
/uni00000013/uni00000011/uni00000016
/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000014
/uni00000013/uni00000011/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000024/uni00000046/uni00000057/uni00000052/uni00000055/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000026/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000046/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
(a) CoRAL-DFM on Gift Cards
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000014
/uni00000013/uni00000011/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000024/uni00000046/uni00000057/uni00000052/uni00000055/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000018/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000026/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000046/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011 (b) CoRAL-DFM on Prime Pantry
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000016
/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000014
/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000024/uni00000046/uni00000057/uni00000052/uni00000055/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000018/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000026/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000046/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011 (c) CoRAL-WDL on Gift Cards
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000017
/uni00000013/uni00000011/uni00000016
/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000014
/uni00000013/uni00000011/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000024/uni00000046/uni00000057/uni00000052/uni00000055/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000026/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000046/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000011 (d) CoRAL-WDL on Prime Pantry
Figure 2: CoRALâ€™s (DFM and WDL) learning curves on Gift Cards and Prime Pantry datasets.
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000024/uni00000038/uni00000026
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f
(a) Appliances (AUC)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000038/uni00000026
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f (b) Gift Cards (AUC)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000024/uni00000038/uni00000026
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f (c) Prime Pantry (AUC)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000024/uni00000038/uni00000026
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f (d) Software (F1)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000029/uni00000014
 /uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f
(e) Appliances (F1)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000029/uni00000014
 /uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f (f) Gift Cards (F1)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000029/uni00000014
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f (g) Prime Pantry (F1)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000029/uni00000014
 /uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000024/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000026/uni00000031
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni00000027/uni00000029/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000024/uni0000002f/uni00000010/uni0000003a/uni00000027/uni0000002f (h) Software (F1)
Figure 3: CoRALâ€™s performance (AUC and F1) w.r.t number of iterations of user-item retrieval on four Amazon Product datasets.
randomly initialized policies, which suggests the data-efficiency
advantage of our method. Since at the early steps of reinforcement
learning, the exploration stage may take a long time to navigate and
find high-value actions through trial and error, without efficient
exploration strategies or some good embedding spaces, the actor
networks could easily overfit and fail to explore better actions.
5.3.2 Actor-critic Learning Curves. In Figure 2, we show the learn-
ing curves of the actor and critic networks, for policies with and
without short-head data initialization. We choose the more chal-
lenging datasets, Gift Cards, and Prime Pantry, in terms of methods
general F1 performance in Table 1, which suggests that these tasks
require better balancing between exploration and exploitation. We
can observe a consistent pattern that the actor networks of the poli-
cies with random initialization converge faster than policies withshort-head data initialization. However, the critic networks of the
policies with random initialization have higher learning loss than
policies with short-head data initialization. Such an observation sug-
gests that the randomly initialized policies could easily overfit and
thus the actor-critic learning process can be done asynchronously.
With the pre-trained user and item embedding spaces on the short-
head training dataset, the exploration in the continuous embedding
space can be more efficient.
5.4 Minimally-sufficient Collaborative
Information from Iterative Retrieval (RQ3)
In Figure 3, we show the modelsâ€™ performance w.r.t the number of
rounds of retrieval. We observe that for all the policies of CoRAL,
in each iteration, they manage to retrieve informative users and
3398CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
CoRAL-DFM CoRAL-WDL
w/ init. w/o init. w/ init. w/o init.
SoftwareAUC 95.25 93.58 93.97 92.35
F1 88.68 88.36 91.18 88.87
Prime PantryAUC 93.32 89.33 87.08 89.49
F1 86.73 80.76 80.52 81.86
Gift CardsAUC 96.52 96.52 92.22 96.98
F1 67.51 64.25 70.74 68.81
AppliancesAUC 90.87 94.48 92.55 91.84
F1 86.76 88.82 89.22 83.00
AverageAUC 93.99 93.48 91.45 92.66
F1 82.42 80.55 82.92 80.64
Table 2: Ablation study of CoRALâ€™s performance with or
without short-head data initialization for DFM and WDL as
the collaborative filtering backbones.
items, while consistently achieving information gain with the in-
formation gain margin decreasing. Comparing the backbones DFM
and DCN of CoRAL, we find a common exploration-exploitation
behavior of these two policies, as the DCN acts more greedy and
reaches its upper-bound performance sooner, while the DFM tends
to be more explorative in the early stage and achieves better final
performance. Such an observation suggests the importance of the
exploration-exploitation trade-off, which can be more efficiently
achieved through the proposed reinforcement learning framework.
6 Conclusion
In this paper, we focus on collaborative filtering-based recom-
mender systems with long-tail items [ 71,73]. We introduce CoRAL,
an approach for enhancing long-tail recommendations in tradi-
tional collaborative filtering-based recommender systems, over-
coming the limitations of data sparsity and imbalance that hamper
collaborative filtering methods. CoRAL integrates collaborative
retrieval-augmented LLMs to align the modelâ€™s reasoning with ac-
tual user-item interaction patterns. This alignment is pivotal in
addressing the common oversight in LLM-based systems that rely
heavily on semantic interpretations, neglecting the collaborative
dimensions of user-item interactions. Additionally, CoRAL employs
a reinforcement learning framework to develop a retrieval policy,
identifying an optimal set of user-item interactions as the sup-
porting evidence for the LLMâ€™s reasoning. This strategy ensures
minimal yet sufficient collaborative information is used, enhanc-
ing the LLMâ€™s ability to accurately deduce user preferences and
interaction dynamics, hence offering a significant improvement on
LLM-based recommendation.
References
[1]Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher, and
Edward Malthouse. 2021. User-centered evaluation of popularity bias in recom-
mender systems. In Proceedings of the 29th ACM Conference on User Modeling,
Adaptation and Personalization. 119â€“129.
[2]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).[3]Vito Walter Anelli, Alejandro BellogÃ­n, Tommaso Di Noia, and Claudio Pomo.
2021. Reenvisioning the comparison between neural collaborative filtering and
matrix factorization. In Proceedings of the 15th ACM Conference on Recommender
Systems. 521â€“529.
[4]Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Sujay Kumar Jauhar,
et al.2023. Knowledge-Augmented Large Language Models for Personalized
Contextual Query Suggestion. arXiv preprint arXiv:2311.06318 (2023).
[5]Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.
2023. Tallrec: An effective and efficient tuning framework to align large language
model with recommendation. arXiv preprint arXiv:2305.00447 (2023).
[6]Stephen Bonner and Flavian Vasile. 2018. Causal embeddings for recommendation.
InProceedings of the 12th ACM conference on recommender systems. 104â€“112.
[7]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schul-
man, Jie Tang, and Wojciech Zaremba. 2016. Openai gym. arXiv preprint
arXiv:1606.01540 (2016).
[8]Jonathon Byrd and Zachary Lipton. 2019. What is the effect of importance
weighting in deep learning?. In International conference on machine learning .
PMLR, 872â€“881.
[9]Chong Chen, Min Zhang, Yongfeng Zhang, Weizhi Ma, Yiqun Liu, and Shaoping
Ma. 2020. Efficient heterogeneous collaborative filtering without negative sam-
pling for recommendation. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 34. 19â€“26.
[10] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7â€“10.
[11] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. Class-
balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. 9268â€“9277.
[12] Zhen Gong, Xin Wu, Lei Chen, Zhenzhe Zheng, Shengjie Wang, Anran Xu, Chong
Wang, and Fan Wu. 2023. Full Index Deep Retrieval: End-to-End User and Item
Structures for Cold-start and Long-tail Item Recommendation. In Proceedings of
the 17th ACM Conference on Recommender Systems. 47â€“57.
[13] Yulong Gu, Zhuoye Ding, Shuaiqiang Wang, Lixin Zou, Yiding Liu, and Dawei Yin.
2020. Deep multifaceted transformers for multi-objective ranking in large-scale
e-commerce recommender systems. In Proceedings of the 29th ACM International
Conference on Information & Knowledge Management. 2493â€“2500.
[14] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[15] Shantanu Gupta, Hao Wang, Zachary Lipton, and Yuyang Wang. 2021. Correcting
exposure bias for link recommendation. In International Conference on Machine
Learning. PMLR, 3953â€“3963.
[16] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Diet-
mar Jannach, and Marios Fragkoulis. 2023. Leveraging large language models
for sequential recommendation. In Proceedings of the 17th ACM Conference on
Recommender Systems. 1096â€“1102.
[17] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy,
Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs Understand
User Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint
arXiv:2305.06474 (2023).
[18] Sami Khenissi and Olfa Nasraoui. 2020. Modeling and counteracting exposure
bias in recommender systems. arXiv preprint arXiv:2001.04832 (2020).
[19] Heung-Nam Kim, Ae-Ttie Ji, Inay Ha, and Geun-Sik Jo. 2010. Collaborative filter-
ing based on collaborative tagging for enhancing the quality of recommendation.
Electronic Commerce Research and Applications 9, 1 (2010), 73â€“83.
[20] Jeonghwan Kim, Giwon Hong, Sung-Hyon Myaeng, and Joyce Whang. 2023.
FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional
Reasoning in GPT-4. In Findings of the Association for Computational Linguistics:
EMNLP 2023. 3763â€“3775.
[21] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[22] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes,
Michael Lewis, and Katia Sycara. 2023. Theory of mind for multi-agent collabo-
ration via large language models. arXiv preprint arXiv:2310.10701 (2023).
[23] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-
based recommendation. In Proceedings of the 32nd ACM International Conference
on Information and Knowledge Management. 1348â€“1357.
[24] Roger Zhe Li, JuliÃ¡n Urbano, and Alan Hanjalic. 2021. Leave no user behind:
Towards improving the utility of recommender systems for non-mainstream
users. In Proceedings of the 14th ACM International Conference on Web Search and
Data Mining. 103â€“111.
[25] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with
deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).
[26] Siyi Liu and Yujia Zheng. 2020. Long-tail session-based recommendation. In
Proceedings of the 14th ACM Conference on Recommender Systems. 509â€“514.
3399KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junda Wu et al.
[27] Xu Liu, Tong Yu, Kaige Xie, Junda Wu, and Shuai Li. 2024. Interact with the
Explanations: Causal Debiased Explainable Recommendation System. In Proceed-
ings of the 17th ACM International Conference on Web Search and Data Mining.
472â€“481.
[28] Yaokun Liu, Xiaowang Zhang, Minghui Zou, and Zhiyong Feng. 2023. Co-
occurrence Embedding Enhancement for Long-tail Problem in Multi-Interest
Recommendation. In Proceedings of the 17th ACM Conference on Recommender
Systems. 820â€“825.
[29] Andrew Luke, Joseph Johnson, and Yiu-Kai Ng. 2018. Recommending long-tail
items using extended tripartite graphs. In 2018 IEEE International Conference on
Big Knowledge (ICBK). IEEE, 123â€“130.
[30] Sichun Luo, Chen Ma, Yuanzhang Xiao, and Linqi Song. 2023. Improving Long-
Tail Item Recommendation with Graph Augmentation. In Proceedings of the
32nd ACM International Conference on Information and Knowledge Management .
1707â€“1716.
[31] Tianhui Ma, Yuan Cheng, Hengshu Zhu, and Hui Xiong. 2023. Large Language
Models are Not Stable Recommender Systems. arXiv preprint arXiv:2312.15746
(2023).
[32] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain,
Andreas Veit, and Sanjiv Kumar. 2020. Long-tail learning via logit adjustment.
arXiv preprint arXiv:2007.07314 (2020).
[33] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
[34] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al .2015. Human-level control through deep reinforcement learning.
nature 518, 7540 (2015), 529â€“533.
[35] Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh, and
Abhishek Das. 2019. Improving generative visual dialog by answering diverse
questions. arXiv preprint arXiv:1909.10470 (2019).
[36] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations
using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
2019 conference on empirical methods in natural language processing and the 9th
international joint conference on natural language processing (EMNLP-IJCNLP).
188â€“197.
[37] Zohreh Ovaisi, Ragib Ahsan, Yifan Zhang, Kathryn Vasilaky, and Elena Zheleva.
2020. Correcting for selection bias in learning-to-rank systems. In Proceedings of
The Web Conference 2020. 1863â€“1873.
[38] Grigorios A Pavliotis. 2016. Stochastic processes and applications. Springer.
[39] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus,
and Noah Dormann. 2021. Stable-baselines3: Reliable reinforcement learning
implementations. The Journal of Machine Learning Research 22, 1 (2021), 12348â€“
12355.
[40] Hossein A Rahmani, Mohammadmehdi Naghiaei, Mahdi Dehghan, and Moham-
mad Aliannejadi. 2022. Experiments on generalizability of user-oriented fairness
in recommender systems. In Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 2755â€“2764.
[41] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei
Yin, and Chao Huang. 2024. Representation learning with large language models
for recommendation. In Proceedings of the ACM on Web Conference 2024. 3464â€“
3475.
[42] Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai,
et al.2023. Lkpnr: Llm and kg for personalized news recommendation framework.
arXiv preprint arXiv:2308.12028 (2023).
[43] Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, and Lucas Dixon.
2023. Large language models are competitive near cold-start recommenders for
language-and item-based preferences. In Proceedings of the 17th ACM conference
on recommender systems. 890â€“896.
[44] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and
Thorsten Joachims. 2016. Recommendations as treatments: Debiasing learning
and evaluation. In international conference on machine learning. PMLR, 1670â€“
1679.
[45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[46] Rama Syamala Sreepada and Bidyut Kr Patra. 2020. Mitigating long tail effect
in recommendations using few shot learning technique. Expert Systems with
Applications 140 (2020), 112887.
[47] Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi.
2023. Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of
the Question Answering Performance of the GPT LLM Family. In International
Semantic Web Conference. Springer, 348â€“367.
[48] Shuai Tang and Xiaofeng Zhang. 2021. CADPP: An Effective Approach to Rec-
ommend Attentive and Diverse Long-tail Items. In IEEE/WIC/ACM International
Conference on Web Intelligence and Intelligent Agent Technology. 218â€“225.
[49] Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, and Ming Gao. 2023. Boosting
Language Models Reasoning with Chain-of-Knowledge Prompting. arXiv preprintarXiv:2306.06427 (2023).
[50] Jianing Wang, Junda Wu, Yupeng Hou, Yao Liu, Ming Gao, and Julian McAuley.
2024. InstructGraph: Boosting Large Language Models via Graph-centric In-
struction Tuning and Preference Alignment. arXiv preprint arXiv:2402.08785
(2024).
[51] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDDâ€™17. 1â€“7.
[52] Wenjie Wang, Yiyan Xu, Fuli Feng, Xinyu Lin, Xiangnan He, and Tat-Seng Chua.
2023. Diffusion Recommender Model. arXiv preprint arXiv:2304.04971 (2023).
[53] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016.
Learning to rank with selection bias in personal search. In Proceedings of the 39th
International ACM SIGIR conference on Research and Development in Information
Retrieval. 115â€“124.
[54] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah
Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang. 2023. Rec-
mind: Large language model powered agent for recommendation. arXiv preprint
arXiv:2308.14296 (2023).
[55] Yu Wang, Zhiwei Liu, Jianguo Zhang, Weiran Yao, Shelby Heinecke, and Philip S
Yu. 2023. DRDT: Dynamic Reflection with Divergent Thinking for LLM-based
Sequential Recommendation. arXiv preprint arXiv:2312.11336 (2023).
[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
Quoc V Le, Denny Zhou, et al .2022. Chain-of-thought prompting elicits reasoning
in large language models. Advances in Neural Information Processing Systems 35
(2022), 24824â€“24837.
[57] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He.
2021. Model-agnostic counterfactual reasoning for eliminating popularity bias
in recommender system. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining. 1791â€“1800.
[58] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng
Wang, Dawei Yin, and Chao Huang. 2023. Llmrec: Large language models with
graph augmentation for recommendation. arXiv preprint arXiv:2311.00423 (2023).
[59] Junda Wu, Zhihui Xie, Tong Yu, Handong Zhao, Ruiyi Zhang, and Shuai Li.
2022. Dynamics-aware adaptation for reinforcement learning based cross-domain
interactive recommendation. In Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 290â€“300.
[60] Junda Wu, Tong Yu, and Shuai Li. 2021. Deconfounded and explainable interactive
vision-language retrieval of complex scenes. In Proceedings of the 29th ACM
International Conference on Multimedia. 2103â€“2111.
[61] Yu Xia, Junda Wu, Tong Yu, Sungchul Kim, Ryan A Rossi, and Shuai Li. 2023.
User-regulation deconfounded conversational recommender system with bandit
feedback. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2694â€“2704.
[62] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.
2017. Attentional factorization machines: Learning the weight of feature interac-
tions via attention networks. arXiv preprint arXiv:1708.04617 (2017).
[63] Jing Yao, Wei Xu, Jianxun Lian, Xiting Wang, Xiaoyuan Yi, and Xing Xie. 2023.
Knowledge Plugins: Enhancing Large Language Models for Domain-Specific
Recommendations. arXiv preprint arXiv:2311.10779 (2023).
[64] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee
Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural
modeling for large corpus item recommendations. In Proceedings of the 13th ACM
Conference on Recommender Systems. 269â€“277.
[65] Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, and Chen Chen. 2012. Challenging the
long tail recommendation. arXiv preprint arXiv:1205.6700 (2012).
[66] Junchi Yu, Ran He, and Rex Ying. 2023. Thought propagation: An analogical
approach to complex reasoning with large language models. arXiv preprint
arXiv:2310.03965 (2023).
[67] Arlisa Yuliawati, Hamim Tohari, Rahmad Mahendra, and Indra Budi. 2022. On
the Long Tail Products Recommendation using Tripartite Graph. International
Journal of Advanced Computer Science and Applications 13, 1 (2022).
[68] Fan Zhang and Qijie Shen. 2023. A Model-Agnostic Popularity Debias Train-
ing Framework for Click-Through Rate Prediction in Recommender System.
InProceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval. 1760â€“1764.
[69] Kaike Zhang, Qi Cao, Fei Sun, Yunfan Wu, Shuchang Tao, Huawei Shen, and Xueqi
Cheng. 2023. Robust Recommender System: A Survey and Future Directions.
arXiv preprint arXiv:2309.02057 (2023).
[70] Wenxuan Zhang, Hongzhi Liu, Yingpeng Du, Chen Zhu, Yang Song, Hengshu
Zhu, and Zhonghai Wu. 2023. Bridging the Information Gap Between Domain-
Specific Model and General LLM for Personalized Recommendation. arXiv
preprint arXiv:2311.03778 (2023).
[71] Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong,
and Ed H Chi. 2021. A model of two tales: Dual transfer learning framework for
improved long-tail item recommendation. In Proceedings of the web conference
2021. 2220â€“2231.
[72] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He.
2023. Collm: Integrating collaborative embeddings into large language models
for recommendation. arXiv preprint arXiv:2310.19488 (2023).
3400CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[73] Yin Zhang, Ruoxi Wang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi,
Lichan Hong, James Caverlee, and Ed H Chi. 2023. Empowering Long-tail Item
Recommendation through Cross Decoupling Network (CDN). In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
5608â€“5617.
[74] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain
of thought prompting in large language models. arXiv preprint arXiv:2210.03493
(2022).
[75] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, and Ji-
Rong Wen. 2023. Adapting large language models by integrating collaborativesemantics for recommendation. arXiv preprint arXiv:2311.09049 (2023).
[76] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021.
Disentangling user interest and conformity for recommendation with causal
embedding. In Proceedings of the Web Conference 2021. 2980â€“2991.
[77] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2024. Collab-
orative large language model for recommender systems. In Proceedings of the
ACM on Web Conference 2024. 3162â€“3172.
[78] Yong Zhuang, Tong Yu, Junda Wu, Shiqu Wu, and Shuai Li. 2022. Spatial-Temporal
Aligned Multi-Agent Learning for Visual Dialog Systems. In Proceedings of the
30th ACM International Conference on Multimedia. 482â€“490.
3401