Privileged Knowledge State Distillation for Reinforcement
Learning-based Educational Path Recommendation
Qingyao Li
ly890306@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, ChinaWei Xia
xiawei24@huawei.com
Huawei Noahâ€™s Ark Lab
Shenzhen, ChinaLiâ€™ang Yin
yinla@apex.sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
Jiarui Jin
jinjiarui97@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, ChinaYong Yuâˆ—
yyu@apex.sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
ABSTRACT
Educational recommendation seeks to suggest knowledge concepts
that match a learnerâ€™s ability, thus facilitating a personalized learn-
ing experience. In recent years, reinforcement learning (RL) meth-
ods have achieved considerable results by taking the encoding of the
learnerâ€™s exercise log as the state and employing an RL-based agent
to make suitable recommendations. However, these approaches
suffer from handling the diverse and dynamic learnerâ€™s knowledge
states. In this paper, we introduce the privileged feature distillation
technique and propose the Privileged Knowledge State Distillation
(PKSD) framework, allowing the RL agent to leverage the â€œactualâ€
knowledge state as privileged information in the state encoding to
help tailor recommendations to meet individual needs. Concretely,
our PKSD takes the privileged knowledge states together with the
representations of the exercise log for the state representations
during training. And through distillation, we transfer the ability
to adapt to learners to a knowledge state adapter. During inference,
theknowledge state adapter would serve as the estimated privileged
knowledge states instead of the real one since it is not accessible.
Considering that there are strong connections among the knowl-
edge concepts in education, we further propose to collaborate the
graph structure learning for concepts into our PKSD framework.
This new approach is termed GEPKSD (Graph-Enhanced PKSD).
As our method is model-agnostic, we evaluate PKSD and GEPKSD
by integrating them with five different RL bases on four public
simulators, respectively. Our results verify that PKSD can consis-
tently improve the recommendation performance with various RL
methods, and our GEPKSD could further enhance the effectiveness
of PKSD in all the simulations.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671872CCS CONCEPTS
â€¢Applied computing â†’E-learning; â€¢Information systems
â†’Recommender systems .
KEYWORDS
Educational Path Recommendation; Reinforcement Learning; On-
line Education; Privileged Feature Distillation
ACM Reference Format:
Qingyao Li, Wei Xia, Liâ€™ang Yin, Jiarui Jin, and Yong Yu. 2024. Privileged
Knowledge State Distillation for Reinforcement Learning-based Educational
Path Recommendation. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3637528.3671872
1 INTRODUCTION
Educational path recommendation acts as an essential aspect of
online education [ 5,41], which recommends personalized learning
resources to learners. The recommendation process can be modeled
as a Markov Decision Process (MDP). Based on this formulation,
reinforcement learning (RL) techniques [ 18] have been widely uti-
lized in this domain, allowing for sequential recommendations and
the maximization of long-term rewards [ 13,17,22,27]. The RL
models are usually trained based on interactions with the learners.
Due to the high cost of interacting with real learners, these meth-
ods typically build a simulator to simulate the learnerâ€™s knowledge
state, and the RL agent recommends knowledge concepts based on
observations to maximize the reward (the degree of improvement
in the learnerâ€™s knowledge level).
However, RL models are limited in handling the use cases with
multiple learners, primarily due to the diverse dynamics among
different learners. Specifically, the diverse dynamics in an educa-
tional scenario mean that the learnersâ€™ learning outcomes may vary
even if they follow the same educational path because they are at
different knowledge levels. This would cause the RL agent to face
an environment with unstable dynamics, which could undermine
the training effectiveness [ 2]. Previous approaches addressed the
challenge by employing knowledge tracing (KT) models [ 22,27].
These methods typically involve a sequence model, like GRU [ 7]
or LSTM [ 34], to predict the learnerâ€™s knowledge state based on
his/her learning sequence of concepts, often referred to as exercise
log. The RL agent then recommends an educational path based on
 
1621
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qingyao Li, Wei Xia, Liâ€™ang Yin, Jiarui Jin, &Yong Yu
Exercise Log
Privileged
Knowledge StateKnowledge State
Encoder
Knowledge DistillationTraining
Inference
LearnerPath
Reward
RL Agent
Exercise Log Log Encoder
Knowledge State
Adapter
LearnerPath
RL AgentLog Encoder
Figure 1: The training-inference paradigm of privileged
knowledge state distillation.
the predicted knowledge state. Yet, we contend that KT is unneces-
sary in this context for two main reasons: 1) While a personalized
learning path should reflect the learnerâ€™s knowledge state, the gran-
ular detail KT provides on concept mastery is more than whatâ€™s
needed for making recommendations. 2) Since the predicted knowl-
edge state typically serves as input for another neural network
and is converted into a hidden state, which contains partial use-
ful information for recommendation. It might be more efficient
to first extract the latent useful information and transfer it to the
recommendation model.
This paper presents a unique perspective on the problem. The
idea is that we do not need to predict the accurate knowledge state
of a learner when doing the recommendation task. By treating the
knowledge state as privileged information, we bypass the complex-
ity of its prediction. Our focus shifts to deriving useful insights from
the knowledge state and conveying this information to an adapter
for times when the privileged knowledge state is not accessible.
Essentially, our approach involves three key steps: 1) Creating an
oracle model capable of supplying the Privileged Knowledge State
(PKS, usually built in the simulator). 2) Learning to harness valuable
insights from the PKS for making recommendations when it is avail-
able. 3) Distilling the insights gained from the PKS into an adapter,
enabling it to gather similar information for recommendations in
the absence of the PKS.
In light of this, we propose a privileged feature distillation frame-
work for educational path recommendation called Privileged Knowl-
edge State Distillation (PKSD), where the learnerâ€™s underlying â€œac-
tualâ€ knowledge state obtained from the simulator is used as a
privileged feature to train RL-based models. Our framework aims
at transferring the privileged information extraction ability to the
knowledge state adapter, enabling the RL agent to adapt to multiple
learners effectively. The training-inference paradigm is shown in
Figure 1. During training, the privileged knowledge state would
be encoded by the knowledge state encoder and fed into the RL
agent for recommendation. However, it cannot be applied to the
real world where the knowledge state cannot be acquired, so we
train a knowledge state adapter, which aims to extract useful in-
formation thatâ€™s initially derived from the privileged knowledge
state from the regular exercise log of the learner. During inference,
when the privileged knowledge state is unavailable, the knowledgestate adapterâ€™s output would replace the encoding of the privileged
knowledge state to help recommendation. Our approach has ad-
vantages in using privileged knowledge state as a more powerful
information source, and our model primarily focuses on estimating
the output of the knowledge state encoder instead of the knowledge
state itself, which is easier to learn since the output is the partial
information extracted from the privileged knowledge state.
Intuitively, a personalized educational path should be constructed
based on two parts of information: 1) The current knowledge state
of the learner. 2) The relationships between learning concepts, such
as the necessity to understand â€œadditionâ€ before tackling â€œmulti-
plicationâ€. Acknowledging the need to incorporate these intrinsic
relationships among knowledge concepts, we further introduce a
graph-enhanced version of PKSD, named GEPKSD. This approach
combines the knowledge state (considered as a privileged feature)
with the knowledge graph. By using a graph neural network to pro-
cess this combined information, we could gain structure and knowl-
edge state-aware representation of the current state, which helps
the RL agent to make recommendations suitable for the learner and
adhere to the knowledge structure.
Our main contributions are summarized as follows:
â€¢We propose Privileged Knowledge State Distillation (PKSD),
which leverages knowledge states of learners as privileged fea-
tures and employs a knowledge state adapter trained through
distillation to help RL agents generate personalized educational
paths. To the best of our knowledge, this is the first attempt to
incorporate privileged information modeling into educational
scenarios.
â€¢To achieve knowledge structure-aware educational path recom-
mendation, we further introduce the Graph-Enhanced PKSD
(GEPKSD) framework, which generates a comprehensive rep-
resentation that encompasses both the learner and knowledge
concepts and poses a better way of utilizing the privileged knowl-
edge state.
â€¢We conduct extensive experiments in four simulators. The results
demonstrate that our proposed framework can not only prove the
performance of representative base RL models but also surpass
previous methods, which demonstrates the effectiveness of the
framework.
2 RELATED WORK
2.1 Educational Path Recommendation
The goal of educational path recommendation is to customize a
sequence of knowledge concepts suggested to individual learners
in order to maximize their knowledge advancement [ 30]. Previ-
ous solutions for educational path recommendation can be mainly
divided into two categories: non-RL [ 3,10,11,36] and RL-based
methods [ 13,17,22,24,27]. Non-RL methods include sequence
recommendation [ 42], graph recommendation [ 14,40], and other
recommendation methods. These methods usually use the learnerâ€™s
path in the dataset as the label, which does not align with the true
objective of promoting the learnerâ€™s mastery level since the original
educational path may not necessarily be the optimal one.
RL-based educational path recommendation models often model
the sequence recommendation problem as a Markov Decision Pro-
cess (MDP) and interact with and train with the simulatorâ€™s learner
 
1622Privileged Knowledge State Distillation for Reinforcement Learning-based Educational Path Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
as the environment [ 25]. The learnerâ€™s exercise logs are modeled as
observed states, and the RL agent takes actions (recommends knowl-
edge concepts) based on states to maximize the reward (the degree
of improvement in the learnerâ€™s knowledge level). One of the most
representative works is the CSEAL model proposed in [ 27], which
uses the Deep Knowledge Tracing (DKT) [ 32] model to predict the
knowledge state based on the learnerâ€™s problem-solving records,
and then inputs it as the state into Actor-Critic for recommendation.
The other work followed is GEHRL proposed in [ 24], which is based
on hierarchical reinforcement learning that separate the recommen-
dation process into goal planning and goal reaching, improving
the efficiency of achieving goals for learners. Moreover, in [ 22], a
model-based RL method is used for recommendation to alleviate
the data sparsity problem. Although these RL-based methods have
achieved considerable results, using only the knowledge tracing
model to extract learner-state information may limit the recom-
mendation effect facing multiple learners. In this paper, we propose
using privileged feature distillation to enable the model to recognize
learner knowledge states and use them for recommendation.
2.2 Privileged Feature Distillation
Training with privileged features and testing with regular features
has been a popular paradigm in recent years [ 28,35,37,39]. A priv-
ileged feature refers to a feature that greatly aids in a task but can
only be obtained during the training phase. The idea of privileged
feature distillation is to train the model using distillation techniques,
enabling it to generate outputs that closely resemble those obtained
with privileged features, even when they are unavailable. LUPI
proposed in [28] try to minimize the following loss:
min
ğ‘Šğ‘ (1âˆ’ğœ†)âˆ—ğ¿ğ‘(ğ‘¦,ğ‘“(ğ‘‹;ğ‘Šğ‘ ))+ğœ†âˆ—ğ¿ğ‘‘(ğ‘“(ğ‘‹âˆ—;ğ‘Šğ‘¡),ğ‘“(ğ‘‹;ğ‘Šğ‘ ))(1)
whereğ‘“is the function that map input to the output; ğ¿ğ‘is the
classification loss; ğ¿ğ‘‘is the distillation loss; ğ‘‹is the regular feature
andğ‘‹âˆ—is the privileged feature. The loss function ensures that
the student model, which receives the regular feature as input,
produces outputs that closely match the teacher model, which
takes the privileged feature as input. After that, [ 38] proposed a
PFD framework for Taobao recommendation, which changes the
above loss into the following:
min
ğ‘Šğ‘ (1âˆ’ğœ†)âˆ—ğ¿ğ‘(ğ‘¦,ğ‘“(ğ‘‹;ğ‘Šğ‘ ))+ğœ†âˆ—ğ¿ğ‘‘(ğ‘“(ğ‘‹âˆ—,ğ‘‹;ğ‘Šğ‘¡),ğ‘“(ğ‘‹;ğ‘Šğ‘ ))(2)
The major difference between them is the input of the teacher
model. LUPT relies solely on privileged information as input, while
PFD combines both standard and privileged information for its in-
put. The paradigm has since been applied across various fields. Wang
et al. [35] proposed using a privileged graph to address the cold start
issue, and Liu et al . [26] aimed to mitigate recommender system
biases with uniform data. Further, integrating reinforcement learn-
ing (RL), Kumar et al . [23] introduced the RMA method, enabling
legged robots to navigate complex terrains by leveraging environ-
ment configurations as privileged information during RL training.
Despite achieving impressive results, the exploration and appli-
cation of privileged information in educational contexts remain
largely unexplored. Actually, in the educational path recommenda-
tion problem, the knowledge states of learners play a crucial role,
particularly for models based on reinforcement learning (RL), whichperform optimally in stable environments. Taking the knowledge
states of learners can mitigate the challenges posed by dynamic
environments, offering a more consistent setting for the RL agent
to navigate and improve upon.
3 PROBLEM FORMULATION
This paper addresses the problem of session-based educational path
recommendation, where our objective is to recommend a sequential
order of knowledge concepts to learners interactively. Normally, at
each step of the learning session, the input is the learnerâ€™s exercise
logHğ‘¡={(ğ‘1,ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ 1),(ğ‘2,ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ 2),...(ğ‘ğ‘¡,ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘¡)}, whereğ‘ ğ‘ğ‘œğ‘Ÿğ‘’âˆˆ
{0,1}is the learnerâ€™s feedback of not mastered or mastered. In our
work, we additionally take the learnerâ€™s knowledge state ğ‘˜ğ‘¡as the
privileged information. Here is the definition of it:
Knowledge state. A learnerâ€™s knowledge state at time step ğ‘¡
is a vectorğ‘˜ğ‘¡with the length equaling the number of knowledge
concepts. Each element ğ‘˜ğ‘¡[ğ‘–]represents the learnerâ€™s mastery level
of knowledge concept ğ‘–. It is provided by an oracle model in the
simulator. Since all the learnerâ€™s behavior is simulated based on it,
ğ‘˜ğ‘¡is taken as the learnerâ€™s underlying â€œactualâ€ knowledge state.
The model recommends a concept ğ‘ğ‘¡+1âˆˆK to the learner ac-
cording toHğ‘¡andğ‘˜ğ‘¡. The learner would provide the feedback score
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘¡+1. Then we updateHğ‘¡+1byHğ‘¡+1=Hğ‘¡âˆª(ğ‘ğ‘¡+1,ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘¡+1).
The actual interaction log of a learner in the dataset serves as
an indicator of their preferences, making it an appropriate training
label for modeling user preferences. However, in educational path
recommendation, the path chosen and logged by a learner may
not always be the best one. This discrepancy arises because our
goal is to find the most effective educational path to improve a
learnerâ€™s proficiency, not just to replicate their preferred choices.
To address this issue, we adopt the strategy from Liu et al . [27] ,
using simulators designed to simulate the learnerâ€™s progress over
their education. Below is the explanation of what a simulator is:
Simulator. A simulator functions as a virtual learner that in-
teracts with the recommendation models by performing activities
such as completing exercises, taking exams, and advancing their
knowledge states. To simulate these actions, a widely used approach
involves maintaining a vector that signifies the learnerâ€™s level of
knowledge, as highlighted in [27].
At the beginning of each learning session, the learner undergoes
a test and obtains an initial score, denoted as ğ¸ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ . After complet-
ing the entire path, the learner takes a final test and obtains a final
score, denoted as ğ¸ğ‘’ğ‘›ğ‘‘. We aim to maximize ğ¸ğ‘’ğ‘›ğ‘‘âˆ’ğ¸ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ .
4 METHODOLOGY
4.1 Framework Overview
Figure 1 shows the training and inference paradigm. Now we dis-
cuss the detailed training process. To ensure that the knowledge
state encoder accurately estimates valuable information from the
privileged knowledge state, we divide the training process into two
stages. The first stage is mainly to train the knowledge state encoder
to learn the latent encoding of the privileged knowledge states
while the second stage is mainly for training the knowledge state
adapter to estimate the latent encoding of the privileged knowledge
state when the it is not available (the same situation as the inference
time). The overall framework of PKSD is shown in Figure 2.
 
1623KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qingyao Li, Wei Xia, Liâ€™ang Yin, Jiarui Jin, &Yong Yu
Log Encoder    
Knowledge State Encoder        Stage 1
...
...
Knowledge Graph Encoder     
Log Encoder    
Knowledge State Adapter     Stage 2
Knowledge Graph Encoder    
    Learner
    EncoderDistillation
......Learner
ï¼šShared
ï¼šPKSD
ï¼šGEPKSDAgent
Knowledge GraphKnowledge GraphorExercise Log
Exercise Log
Agent
orPrivilegedTeahcer Model
Student Model0.40.80.60.30.5
Figure 2: The overall framework of PKSD and GEPKSD. The translucent sections illustrate the GEPKSD framework. In PKSDâ€™s
stage 1, the privileged knowledge state is encoded using an MLP encoder, while in the graph-enhanced version, it is integrated
with the knowledge graph and encoded using a GCN. In stage 2, the difference between PKSD and GEPKSD is also whether the
estimation Ë†ğ‘ğ‘¡is based on the combination of the GRU-encoded exercise log and the knowledge graph or the GRU encoding
alone.
In the first stage, the regular and privileged features are combined
as input. The regular feature is an RNN-encoded exercise log, which
is the same as most RL-based methods. The privileged feature is the
encoding of the learnerâ€™s â€œactualâ€ knowledge state ğ‘˜ğ‘¡taken from the
simulator. It is encoded into latent vector ğ‘ğ‘¡by an encoding network
ğ‘“ğ‘’. The agent decides what knowledge concept to recommend based
on the regular feature and privileged feature. In this stage, the
agent, trained via RL in simulation, learns to leverage the learnerâ€™s
knowledge state information to generate the educational path and
adapt to diverse learners. The knowledge state encoder learns to
extract useful information from privileged knowledge state to help
make recommendations.
In the second stage, we donâ€™t take the privileged knowledge
state as the direct input. What we need to do is to estimate the
latent encoding ğ‘ğ‘¡byË†ğ‘ğ‘¡. This estimation task is facilitated by the
knowledge state adapter, denoted as ğ‘“ğ‘. During the training phase,
we utilize supervised learning to train the knowledge state adapter
ğ‘“ğ‘as the privileged knowledge state is available.
4.2 Privileged Knowledge State Distillation
We aim to help the RL agent adapt to the diverse and dynamic
knowledge states of learners through privileged knowledge state
distillation. To achieve this, we developed a knowledge state encoder,
which is to extract useful information for recommendation from the
privileged knowledge state ğ‘˜ğ‘¡to a latent vector ğ‘ğ‘¡to help recom-
mendation, and a knowledge state adapter, which tries to estimate
ğ‘ğ‘¡from the learnerâ€™s exercise log.4.2.1 Log encoder ğ‘“ğ‘œ.The log encoder is to generate the regular
feature. From the learnerâ€™s exercise log Hğ‘¡, we try to extract the
learnerâ€™s recent learning interest to help with the recommendation.
We use one of the most popular sequence data processing models [ 8]
- Gated recurrent unit (GRU), to model the sequence data.
ğ‘œğ‘¡=ğ‘“ğ‘œ(Hğ‘¡)=ğºğ‘…ğ‘ˆ(Hğ‘¡) (3)
ğ‘œğ‘¡âˆˆRğ‘‘ğ‘œis the learned encoding of exercise log.
4.2.2 Knowledge state encoder ğ‘“ğ‘’.In stage 1, we take the learnerâ€™s
â€œactualâ€ knowledge state from the simulator as a privileged feature.
The knowledge state encoder is to extract useful information for
the recommendation from the learnerâ€™s knowledge state vector ğ‘˜ğ‘¡.
Sinceğ‘˜ğ‘¡is a dense vector, we utilize Multilayer Perceptron(MLP)
to implement the knowledge state encoder.
ğ‘ğ‘¡=ğ‘“ğ‘’(ğ‘˜ğ‘¡)=ğ‘€ğ¿ğ‘ƒ(ğ‘˜ğ‘¡) (4)
ğ‘ğ‘¡âˆˆRğ‘‘ğ‘is the latent encoding of he privileged knowledge state.
The purpose of the log encoder and knowledge state encoder is
to extract information useful for the recommendation, so they are
input into the agent and trained through RL.
4.2.3 Knowledge State Adapter ğ‘“ğ‘.In the second stage, when the
privileged knowledge state is unavailable, we employ a knowledge
state adapter, specifically a GRU-based sequence processing model,
to estimate the relevant information ğ‘ğ‘¡extracted from the learnerâ€™s
exercise log.
Ë†ğ‘ğ‘¡=ğ‘“ğ‘(Hğ‘¡)=ğºğ‘…ğ‘ˆ(Hğ‘¡) (5)
 
1624Privileged Knowledge State Distillation for Reinforcement Learning-based Educational Path Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Ë†ğ‘ğ‘¡âˆˆRğ‘‘ğ‘is the estimation of ğ‘ğ‘¡.
4.2.4 Distillation. The objective of the knowledge state adapter is to
output Ë†ğ‘ğ‘¡as a substitution for ğ‘ğ‘¡.ğ‘ğ‘¡is the latent encoding extracted
byknowledge state encoder from the privileged knowledge state
and Ë†ğ‘ğ‘¡is extracted from the regular feature. We take knowledge
state encoder as the teacher model and the knowledge state adapter
as the student model and train it through distillation. Specifically,
whenever the adapter generates an estimate Ë†ğ‘ğ‘¡, we utilize the actual
knowledge state ğ‘˜ğ‘¡and process it through the knowledge state
encoder to obtainğ‘ğ‘¡, which we use as a label for training. Therefore,
the distillation loss for the knowledge state adapter is the L2 distance
betweenğ‘ğ‘¡and Ë†ğ‘ğ‘¡:
ğ¿ğ‘‘=âˆ¥ğ‘ğ‘¡âˆ’Ë†ğ‘ğ‘¡âˆ¥2 (6)
Itâ€™s important to clarify that we do not attempt to predict the
learnerâ€™s entire knowledge state; instead, we focus only on ex-
tracting the aspects that are useful for making recommendations.
Predicting the full scope of a learnerâ€™s knowledge state is a more
intricate task. Our experiments aim to highlight the distinctions
between focusing on useful recommendation information versus
predicting the complete knowledge state.
4.3 RL-based Recommendation
An RL agent is developed to decide what action to take under the cor-
responding state. To conduct an RL-based recommendation model,
we need to model the process as a Markov Decision Process(MDP).
The key notations are as follows:
â€¢Stateğ‘ ğ‘¡.The state in training stage 1 is defined as the concate-
nation of the exercise log encoding and privileged knowledge
state encoding, formally ğ‘ ğ‘¡=ğ‘œğ‘¡âŠ•ğ‘ğ‘¡. While in training stage
2, the privileged knowledge state encoding is replaced by its
estimation Ë†ğ‘ğ‘¡.
â€¢Actionğ‘ğ‘¡.The action represents the recommended knowledge
concept,ğ‘ğ‘¡âˆˆK.
â€¢Rewardğ‘Ÿğ‘¡.The reward is given at the end of the whole path,
which represents the learnerâ€™s knowledge mastery promotion
after following the path,
ğ‘Ÿğ‘¡=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³ğ¸ğ‘’ğ‘›ğ‘‘âˆ’ğ¸ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡
ğ¸ğ‘ ğ‘¢ğ‘âˆ’ğ¸ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡,ifğ‘¡is the last time step
0, otherwise(7)
whereğ¸ğ‘ ğ‘¢ğ‘is the maximum score of the test; ğ¸ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ andğ¸ğ‘’ğ‘›ğ‘‘are
the scores the learner get at the start and the end of the session,
respectively.
In general, we take the state ğ‘ ğ‘¡at each time step as the input to
the RL agent, which outputs the action to be taken at that moment,
corresponding to a specific knowledge concept.
ğ‘ğ‘¡+1âˆ¼ğœ‹(ğ‘ ğ‘¡;ğœƒ) (8)
whereğœ‹(ğ‘ ğ‘¡;ğœƒ)means the policy which we aim to learn of RL agent,
andâˆ¼means theğ‘ğ‘¡+1is sampled from the action distribution of
the policy. In real-world scenarios, this corresponds to recommend-
ing learning resources related to that knowledge concept to the
learner. However, for the sake of abstraction, we limit our focus to
recommending the knowledge concepts themselves.4.4 Graph-enhanced Privileged Knowledge
State Distillation
Knowing the learnerâ€™s knowledge state can make the recommended
educational paths personalized, while understanding the depen-
dencies between knowledge concepts can improve the alignment
of paths with educational principles. Therefore, incorporating the
knowledge graph is an intuitive idea for leveraging the privileged
knowledge state. Thus, we further propose Graph-Enhanced Priv-
ileged Knowledge State Distillation (GEPKSD), which contains a
knowledge graph encoder ğ‘“ğ‘”to encode both the privileged knowl-
edge state and knowledge graph and a learner encoder ğ‘“ğ‘™to replace
privileged information during inference.
4.4.1 Knowledge Graph. Knowledge graph in educational scenario
represents knowledge conceptsâ€™ prerequisite relationship [ 4]. A
node in the knowledge graph corresponds to a knowledge concept,
and an edge represents the prerequisite relationship between the
two connected knowledge concepts.
4.4.2 Knowledge Graph Encoder ğ‘“ğ‘”.We propose to incorporate the
knowledge graph by adding the privileged knowledge state as the
initial feature of the graph nodes. By leveraging Graph Convolu-
tional Network (GCN), we propagate information throughout the
graph to generate a comprehensive representation encompassing
both the learnerâ€™s knowledge state and the relationships between
concepts. Any graph embedding models could be used here. We
choose GCN for its simplicity and effectiveness in the graph encod-
ing area.
â„0
ğ‘–=ğ‘¥ğ‘–âŠ•ğ‘˜ğ‘¡[ğ‘–] (9)
â„ğ‘™
ğ‘–=ğœ(1
|Nğ‘–|âˆ‘ï¸
ğ‘—âˆˆNğ‘–âˆª{ğ‘–}ğ‘¤ğ‘™â„ğ‘™âˆ’1
ğ‘—+ğ‘ğ‘™) (10)
ğ‘ğ‘¡=1
|N|âˆ‘ï¸
ğ‘–âˆˆNâ„ğ‘™
ğ‘–(11)
whereğ‘˜ğ‘¡[ğ‘–]is the i-th element of the knowledge state; â„0
ğ‘–is the
initial feature of node ğ‘–;Nğ‘–is the neighbor set of node ğ‘–;Nis the
node set;ğ‘¤ğ‘™andğ‘ğ‘™are the weight and bias of the l-th layer of
GCN. The GCN is trained by optimizing the final recommendation
objective in an end-to-end manner.
4.4.3 Learner Encoder ğ‘“ğ‘™.Since the knowledge state is not available
in stage 2, we need to replace ğ‘˜ğ‘¡with the encoding of the exercise
log. We employ a GRU-based learner encoder to complete the task.
The knowledge graph is not privileged information which is then
used in both stage 1 and stage 2.
4.5 Optimization
The whole model is updated depending on the RL algorithm used.
Generally, there would be a policy net ğœ‹(ğ‘ ğ‘¡;ğœƒ)predicting the action
and a value net ğ‘‰(ğ‘ ğ‘¡;ğœ™)estimating the future reward of the state,
whereğœƒandğœ™are the parameters of the corresponding neural
network.
 
1625KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qingyao Li, Wei Xia, Liâ€™ang Yin, Jiarui Jin, &Yong Yu
The value net is trained based on the classical mean squared loss
(MSE):
ğ¿ğ‘£=E(||ğ‘‡âˆ’ğ‘¡âˆ‘ï¸
ğ‘–=0ğ›¾ğ‘–ğ‘Ÿğ‘¡+ğ‘–âˆ’ğ‘‰(ğ‘ ğ‘¡;ğœ™)||)2(12)
And the policy net is trained based on the value netâ€™s estimation:
ğ¿ğ‘=E[ğ‘‡âˆ‘ï¸
ğ‘¡=0(ğ‘Ÿğ‘¡+ğ›¾ğ‘‰(ğ‘ ğ‘¡+1;ğœ™)âˆ’ğ‘‰(ğ‘ ğ‘¡;ğœ™))Â·ğ‘™ğ‘œğ‘”(ğœ‹(ğ‘ğ‘¡+1|ğ‘ ğ‘¡;ğœƒ))](13)
After all, in stage 1, all the modules are updated based on ğ¿ğ‘£and
ğ¿ğ‘In stage 2, the agent is updated based on ğ¿ğ‘£andğ¿ğ‘while the
log encoder and knowledge state adapter are updated based on ğ¿ğ‘‘,
ğ¿ğ‘£andğ¿ğ‘.
5 EXPERIMENT
In this section, we conduct extensive experiments on 4 simulators
to evaluate the effectiveness of our model. Our experiments aim at
answering the following questions:
â€¢RQ1. Could PKSD/GEPKSD framework improve the educational
path recommendation performance across different base RL
agents?
â€¢RQ2. How does PKSD/GEPKSD perform compared with previ-
ous educational path methods?
â€¢RQ3. Is distillation on the latent encoding of the privileged
knowledge state a better way for recommendation than tracing
the exact knowledge state?
â€¢RQ4. In GEPKSD, is the privileged knowledge state important
or is it working only for using the knowledge graph?
â€¢RQ5. Could our approach handle multiple learners with diverse
knowledge states better?
5.1 Experiment Setup
5.1.1 Datasets. Our experiments are based on three public edu-
cational datasets: Junyi1ASSISTments20092andASSITments20153.
Junyi dataset provides a prerequisite graph of the knowledge con-
cepts while others donâ€™t, so we build a transition graph [ 31] as an
estimation of the prerequisite graph. The statistics of the datasets
are presented in Table 1.
Table 1: Dataset Statistics
Dataset Junyi ASSIST15 ASSIST09
#Concepts 835 100 167
#Learners 525,061 69,675 4,217
#Records 21,460,249 2,420,200 346,860
Positive label rate 54.38% 73.17% 63.81%
5.1.2 Simulators and Oracle Models. We construct two types of sim-
ulators proposed in previous work [27]: the Knowledge Structure-
based Simulator (KSS) and the Knowledge Evolution-based Simula-
tor (KES). The KSS is based on Item Response Theory and a rule-
based formula to determine the learnerâ€™s knowledge state changes.
1https://www.kaggle.com/datasets/junyiacademy/learning-activity-public-dataset-
by-junyi-academy
2https://www.kaggle.com/datasets/junyiacademy/learning-activity-public-dataset-
by-junyi-academy
3https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-dataKES is a data-driven simulator that employs a trained Deep Knowl-
edge Tracing (DKT) model on a specific dataset to simulate the
learnerâ€™s knowledge growth and learning feedback. Specifically,
each simulated learner is initialized based on the first 60% of the
exercise log in the dataset(not available for the recommendation
model). Subsequently, as the recommendations are made, new ex-
ercise logs are incorporated, and the DKT is used to update the
learnerâ€™s knowledge state. We constructed 4 simulators based on
3 datasets: KSS (rule-based), KES-junyi, KES-ASSIST15, and KES-
ASSIST09. The KES simulators are trained using three different
datasets to serve as the foundations for their respective DKT-based
simulators.
In the simulator, the algorithm that determines a learnerâ€™s knowl-
edge state serves as the oracle providing privileged knowledge
states. For KSS, this is represented by the formulas deciding a
learnerâ€™s knowledge, and in KES, itâ€™s the environmentâ€™s DKT model.
The DKT model is trained on the entire dataset, making it a reli-
able oracle. However, DKT models outside the simulator can only
use the data interacting with the simulator to train, making the
environmentâ€™s DKT-determined knowledge states the decisive and
privileged ones.
5.1.3 Methods for comparison. Since our framework is model-
agnostic, we first combine our method and several representative
RL methods to show the effectiveness in prompting RL methodsâ€™
performance:
â€¢DQN [ 29]: One of the basic RL algorithms, which mainly esti-
mates the value of each state and action pair by a neural network.
â€¢AC [ 21]: Vanilla Actor-Critic algorithm that contains a policy
net to output the action distribution and a value net to determine
the value of the state.
â€¢PPO [ 33]: An advanced RL algorithm that constrains the update
magnitude of the policy network so that the training is more
stable.
â€¢TD3 [ 12]: Twin Delayed DDPG is a model-free off-policy RL
algorithm that uses twin critics to estimate the Q-values and
reduces overestimation bias in Q-learning.
â€¢SAC [ 15]: SAC (Soft Actor-Critic) is an off-policy RL algorithm
that incorporates an entropy regularization term to balance ex-
ploration and exploitation, allowing for more stable and efficient
learning.
Then, we further compare with two classes of previous educational
path recommendation methods: Non-RL methods and RL-based
methods:
(1) Non-RL methods:
â€¢GRU4Rec [ 16]: An RNN-based model for the session-based
recommendation, where the session data is first one-hot en-
coded and then embedded. This processed data is fed into a
GRU network, which outputs the probability of each item being
recommended.
â€¢SASRec [ 19]: A self-attention-based sequential recommenda-
tion model that effectively captures long-term dependencies
but focuses on a few important items in sequential data for
accurate item recommendation.
 
1626Privileged Knowledge State Distillation for Reinforcement Learning-based Educational Path Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Comparison between PKSD/GEPKSD and backbone RL methods. â€œ*â€ denotes that the improvement are significant at
level ofğ‘<0.05with paired t-test (PKSD compares with base and GEPKSD compares with PKSD).
Base ModelsKSS Junyi ASSISTments2015 ASSISTments2009
base PKSD GEPKSD base PKSD GEPKSD base PKSD GEPKSD base PKSD GEPKSD
SAC 0.3738 0.4199âˆ—0.5558âˆ—-0.2224 0.0505âˆ—0.2216âˆ—0.2280 0.3918âˆ—0.6947âˆ—0.6545 0.6614âˆ—0.6638âˆ—
TD3 0.3732 0.4527âˆ—0.4842âˆ—-0.1405 -0.1046âˆ—-0.0598âˆ—0.2972 0.5160âˆ—0.5721âˆ—0.2468 0.4270âˆ—0.6671âˆ—
DQN 0.3855 0.4191âˆ—0.4476âˆ—0.2386 0.2982âˆ—0.3215âˆ—0.3675 0.6004âˆ—0.7020âˆ—0.0503 0.4511âˆ—0.5857âˆ—
AC 0.5051 0.5454âˆ—0.5539âˆ—0.3093 0.7600âˆ—0.7746âˆ—0.7282 0.8181âˆ—0.8457âˆ—0.6619 0.6773âˆ—0.6782âˆ—
PPO 0.7337 0.7546âˆ—0.7723âˆ—0.2399 0.3260âˆ—0.3309âˆ—0.8856 0.9176âˆ—0.9213âˆ—0.6602 0.6673âˆ—0.6676
Table 3: Comparison between PKSD/GEPKSD and previous methods. â€œ*â€ denotes that the improvement are significant at level
ofğ‘<0.05with paired t-test comparing with previous methodsâ€™ best performance.
SimulatorsNon-RL
Methods RL-based Methods Ours
GRU4Re
c SASRec DKTRec CB RLtutor CSEAL GEHRL PKSD(PPO) GEPKSD(PPO) PKSD(GEHRL) GEPKSD(GEHRL)
KSS 0.1905
0.7183 0.3069 0.4779 0.6227 0.6833 0.8331 0.7546 0.7723 0.8480 0.8536âˆ—
Junyi 0.0009
-0.1518 -0.1471 0.1730 -0.1306 0.3942 0.5830 0.3260 0.3309 0.6398 0.6568âˆ—
ASSIST
ments2015 0.1025 0.1975 0.5280 0.6413 0.8713 0.8015 0.8121 0.9176 0.9213âˆ—0.8265
0.8825
ASSISTments2009 -0.2347 -0.5222 -0.0814 0.3586 0.6396 0.6347 0.5758 0.6673 0.6676âˆ—0.6004
0.6009
â€¢DKTRec: Use a DKT to predict the learnerâ€™s knowledge state
on the knowledge concepts and recommend the one closest to
0.5
(2) RL-based methods:
â€¢CB [17]: An RL-based method that takes the educational path
recommendation problem as a contextual bandits problem and
applies Q-learning to solve it.
â€¢CSEAL [ 27]: Actor-Critic based educational path recommenda-
tion model, which contains a cognitive navigation module so
that the path is aligned with educational principles.
â€¢RLTutor [ 22]: Utilizing model-based RL, which builds an in-
ner model DAS3H [ 6] to simulate the learner and apply PPO
algorithm to build the RL agent.
â€¢GEHRL [ 24]: Utilizing hierarchical reinforcement learning as
the base with graph-based candidate selector to coordinate the
learning path with learning structure.
We evaluate these methods with the average promotion of the
mastery of the learning goals after path recommendation, which is
given by the converge reward from the simulators.
5.1.4 Implementation Details. We use Adam [ 20] as the optimizer,
and the learning rate is set to be 5Ã—10âˆ’5. For each model, we run
for three random seeds: 1, 5, 10, and calculate the average mastery
promotion on 2000 learner samples across three seeds. The path
length is set to 20. For the models that employ DKT, we perform pre-
training by utilizing the data interacting with the simulator. In the
case of KSS, we pre-train the DKT model and other non-RL methods
using the experimental data obtained from the interaction of the
trained PPO with the simulator. The source code of our proposed
approach is available4.
4Source code for model implementations: https://github.com/mindspore-lab/models/
tree/master/research/huawei-noah/PKSD5.2 Overall Performance
5.2.1 Improvement over Backbone RL Models (RQ1). We pick five
representative RL methods as the backbone to assess the effects of
PKSD/GEPKSD, with the results displayed in Table 2. Key observa-
tions include: 1) The significant performance enhancement with
PKSD framework. For example, DQNâ€™s performance on ASSIST09
improved markedly from 0.0503 to 0.4511 with PKSD, highlight-
ing the effectiveness of introducing privileged knowledge states. 2)
GEPKSD, incorporating a knowledge graph, further boosts PKSDâ€™s
impact across all methods, demonstrating the advantage of com-
bining learner-specific information (privileged knowledge state)
and knowledge domain insights (knowledge graph) for more effec-
tive recommendation paths based on learner states and knowledge
structures.
5.2.2 Improvement over Other Recommendation Methods (RQ2).
Next, we compare PKSD/GEPKSD with other recommendation
baselines. We also integrate our framework with one of the base-
lines GEHRL, proposed by Li et al . [24] , to show the effectiveness
of our framework with the advancing RL-based educational path
recommendation method. The outcomes are presented in Table 3,
revealing that: 1) The best results were achieved by GEPKSD, with
PKSD also surpassing previous baselines, including non-RL and RL-
based methods, demonstrating the advancement of our approach
comparing with previous methods. 2) Generally, RL-based meth-
ods outperformed non-RL methods, indicating the limitations of
merely mimicking existing learnersâ€™ paths. In contrast, RL methods,
through exploration and interaction, can discover more effective
strategies. 3) The combination of PKSD and GEPKSD with strong
backbone models like PPO and GEHRL led to superior performance,
which shows that our framework possesses strong model-agnostic
characteristics; it not only improves the effectiveness of base RL
methods but also synergizes with advanced RL-based methods to
achieve superior results.
 
1627KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qingyao Li, Wei Xia, Liâ€™ang Yin, Jiarui Jin, &Yong Yu
Table 4: The DBI of different representation learned in three
datasets.
Embeddings KSS Junyi ASSIST15 ASSIST09
Knowledge States 1.2220 1.1813 1.4018 1.2803
Extracted Latent Vectors 0.6155 0.5985 0.4910 0.4472
5.3 Compare with knowledge tracing-based
methods (RQ3)
We argued that for the task of recommending learning paths, it is
unnecessary to predict the learnerâ€™s exact knowledge state. To sup-
port this claim, we perform experiments to investigate the feature
of the knowledge states and the extracted latent encodings.
First of all, we explore the inherent properties of the privileged
knowledge states and the latent encodings. Specifically, we calcu-
late the Davies-Bouldin Index (DBI) [ 9] of 1000 learnersâ€™ knowledge
states and extracted latent vectors ğ‘ğ‘¡. DBI assesses the data separa-
tion state between clusters. A lower DBI indicates better-separated
classes. We use K-Means to cluster the embeddings and calculate
DBIs, as shown in Table 4. We can see that the extracted latent
vectors have a much lower DBI than the original knowledge states,
which indicates that a better clustering pattern that is easier to
catch.
Actor-Critic CSEAL PPO0.300.440.580.720.86Average Promotion0.550.75 0.75
0.530.72
0.68
0.550.78 0.77
0.510.76
0.71PKSD
PKSD(DKT)
GEPKSD
GEPKSD(DKT)
(a) KSS
Actor-Critic CSEAL PPO0.00.20.40.60.8Average Promotion0.76
0.52
0.330.75
0.47
0.150.77
0.56
0.330.510.49
0.23PKSD
PKSD(DKT)
GEPKSD
GEPKSD(DKT) (b) KES-junyi
Actor-Critic CSEAL PPO0.600.720.840.961.08Average Promotion0.81
0.780.92
0.780.770.89
0.85
0.820.92
0.83
0.760.91PKSD
PKSD(DKT)
GEPKSD
GEPKSD(DKT)
(c) KES-ASSIST15
Actor-Critic CSEAL PPO0.600.640.680.720.76Average Promotion0.68
0.640.67 0.67
0.640.650.68
0.650.67 0.67
0.630.66PKSD
PKSD(DKT)
GEPKSD
GEPKSD(DKT) (d) KES-ASSIST09
Figure 3: Compare with DKT-based PKSD
Furthermore, we conduct experiments that trace the knowledge
state directly instead of ğ‘ğ‘¡as a knowledge tracing way to leverage
the privileged knowledge state. For the graph-enhanced version
GEPKSD, we use DKT to estimate the learnerâ€™s knowledge state in
stage 2 to combine with the knowledge graph.
The results are shown in Figure 3. It can be observed that using
the DKT model to estimate the learnerâ€™s exact knowledge state
results in a decline in performance. This decline is particularly
evident on the KES-junyi dataset, which consists of 835 knowledge
concepts. Due to the larger number of knowledge concepts, theprediction difficulty for the DKT model is greater, leading to a more
significant decrease in performance.
5.4 PKSD for GEPKSD (RQ4)
GEPKSD enhances the recommendation capabilities of RL agents by
incorporating a graph that considers not only the learnerâ€™s knowl-
edge state but also the dependency relationships between concepts
on the knowledge graph. To ascertain whether the improvements
are due to the proposed PKSD or the introduction of the graph neu-
ral network, experiments comparing GEPKSD with and without
PKSD are conducted. The results are depicted in Figure 4.
KSS KES-junyi KES-ASSIST15 KES-ASSIST090.300.420.540.660.78Average Promotion0.55
0.480.79
0.58
0.550.770.85
0.68GEPKSD(AC) w/o PKSD
GEPKSD(AC)
(a) Actor-Critic
KSS KES-junyi KES-ASSIST15 KES-ASSIST090.100.320.540.760.98Average Promotion0.59
0.150.88
0.540.77
0.330.92
0.67GEPKSD(PPO) w/o PKSD
GEPKSD(PPO) (b) PPO
Figure 4: Compare the performance of with/without PKSD
on GEPKSD.
Removing PKSD significantly diminishes GEPKSDâ€™s performance
across four simulators. This suggests that merely incorporating a
graph neural network without a meaningful training task is inef-
fective. The distillation process provides a clear goal for the knowl-
edge state adapter, which is to estimate information related to the
learnerâ€™s knowledge level that is crucial for recommendations. This
is vital since the recommended learning path should be primarily
based on the learnerâ€™s knowledge state and the structure of the
knowledge itself.
5.5 Performance across various number of
learners (RQ5)
To illustrate that our proposed PKSD framework could adapt to mul-
tiple learners better, we conduct experiments on different numbers
of learners. The result is shown in 5. The experimental results reveal
that the advantages of our method become increasingly evident as
the number of learners grows. This is not only due to the privileged
knowledge state providing stronger signals for training compared
to the learnerâ€™s exercise log but also because the knowledge state
adapter effectively helps the RL agent identify differences between
learners to enhance generalization performance.
As the number of learners increases, the effectiveness of PKSD
initially decreases and then rises. This pattern occurs because when
the learner population starts to grow, the dynamic changes in learn-
ersâ€™ knowledge states make it challenging for the RL agent to adapt,
leading to a dip in performance. However, as the number of learn-
ers reaches a certain level, the volume of data becomes sufficient
for the knowledge state adapter to be fully trained, resulting in a
recovery in effectiveness. While vanilla RL methods without PKSD
exhibit a similar trend, there is a noticeable difference in perfor-
mance when dealing with a larger learner count. These methods
struggle to accommodate learners with dynamic knowledge states
without the knowledge state adapter and the distillation process.
 
1628Privileged Knowledge State Distillation for Reinforcement Learning-based Educational Path Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The experiments generally demonstrate that our model could adapt
to multiple learners better.
50 500 1000 all
Number of learners0.300.360.420.480.54Average Promotion Performance
Actor-Critic
PKSD(Actor-Critic)
(a) KSS
50 500 1000 all
Number of learners0.00.20.40.60.8Average Promotion Performance
Actor-Critic
PKSD(Actor-Critic) (b) KES-junyi
50 500 1000 all
Number of learners0.400.540.680.820.96Average Promotion Performance
Actor-Critic
PKSD(Actor-Critic)
(c) KES-ASSIST15
50 500 1000 all
Number of learners0.400.470.540.610.68Average Promotion Performance
Actor-Critic
PKSD(Actor-Critic) (d) KES-ASSIST09
Figure 5: Performances with various number of learners.
6 CONCLUSION
We propose a novel framework called Privileged Knowledge State
Distillation (PKSD) for educational path recommendation. We trans-
fer the privileged information knowledge through distillation to the
knowledge state adapter and combine it with the knowledge graph
to generate a comprehensive representation, enabling the RL agent
to adapt more effectively to learners with diverse knowledge states.
This marks the first attempt to incorporate privileged knowledge
distillation into educational path recommendations. Extensive ex-
periments demonstrate the frameworkâ€™s effectiveness in efficiently
recommending educational paths to enhance learnersâ€™ knowledge
levels.
ACKNOWLEDGMENTS
The SJTU team is partially supported by Shanghai Municipal Sci-
ence and Technology Major Project (2021SHZDZX0102) and Na-
tional Natural Science Foundation of China (62177033). The work is
also sponsored by Huawei Innovation Research Program. We grate-
fully acknowledge the support of MindSpore [ 1], CANN (Compute
Architecture for Neural Networks), and Ascend AI Processor used
for this research.
REFERENCES
[1] 2020. MindSpore. https://www.mindspore.cn/
[2]Mohammadamin Barekatain, Ryo Yonetani, and Masashi Hamaya. 2019. Multipo-
lar: Multi-source policy aggregation for transfer reinforcement learning between
diverse environmental dynamics. arXiv preprint arXiv:1909.13111 (2019).
[3]Cun-Ling Bian, De-Liang Wang, Shi-Yu Liu, Wei-Gang Lu, and Jun-Yu Dong. 2019.
Adaptive learning path recommendation based on graph theory and an improved
immune algorithm. KSII Transactions on Internet and Information Systems (TIIS)
13, 5 (2019), 2277â€“2298.
[4]Haw-Shiuan Chang, Hwai-Jung Hsu, and Kuan-Ta Chen. 2015. Modeling Exercise
Relationships in E-Learning: A Unified Approach.. In EDM. 532â€“535.
[5]Chih-Ming Chen. 2009. Ontology-based concept map for planning a personalised
learning path. British Journal of Educational Technology 40, 6 (2009), 1028â€“1058.[6]Benoit Choffin, Fabrice Popineau, Yolaine Bourda, and Jill-Jenn Vie. 2019. DAS3H:
modeling student learning and forgetting for optimally scheduling distributed
practice of skills. arXiv preprint arXiv:1905.06873 (2019).
[7]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[8]Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
InNIPS 2014 Workshop on Deep Learning, December 2014.
[9]David L Davies and Donald W Bouldin. 1979. A cluster separation measure. IEEE
transactions on pattern analysis and machine intelligence 2 (1979), 224â€“227.
[10] Pragya Dwivedi, Vibhor Kant, and Kamal K Bharadwaj. 2018. Learning path
recommendation based on modified variable length genetic algorithm. Education
and information technologies 23, 2 (2018), 819â€“836.
[11] Lumbardh Elshani and Krenare Pireva NuÃ§i. 2021. Constructing a personalized
learning path using genetic algorithms approach. arXiv preprint arXiv:2104.11276
(2021).
[12] Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function ap-
proximation error in actor-critic methods. In International conference on machine
learning. PMLR, 1587â€“1596.
[13] Jibing Gong, Yao Wan, Ye Liu, Xuewen Li, Yi Zhao, Cheng Wang, Yuting Lin, Xi-
aohan Fang, Wenzheng Feng, Jingyi Zhang, et al .2022. Reinforced moocs concept
recommendation in heterogeneous information networks. ACM Transactions on
the Web (2022).
[14] Jibing Gong, Shen Wang, Jinlong Wang, Wenzheng Feng, Hao Peng, Jie Tang,
and Philip S Yu. 2020. Attentional graph convolutional networks for knowledge
concept recommendation in moocs in a heterogeneous view. In Proceedings
of the 43rd international ACM SIGIR conference on research and development in
information retrieval. 79â€“88.
[15] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. In International conference on machine learning . PMLR, 1861â€“
1870.
[16] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based recommendations with recurrent neural networks. arXiv
preprint arXiv:1511.06939 (2015).
[17] Wacharawan Intayoad, Chayapol Kamyod, and Punnarumol Temdee. 2020. Rein-
forcement learning based on contextual bandits for personalized online learning
recommendation systems. Wireless Personal Communications 115, 4 (2020), 2917â€“
2932.
[18] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. 1996. Rein-
forcement learning: A survey. Journal of artificial intelligence research 4 (1996),
237â€“285.
[19] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197â€“206.
[20] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[21] Vijay Konda and John Tsitsiklis. 1999. Actor-critic algorithms. Advances in neural
information processing systems 12 (1999).
[22] Yoshiki Kubotani, Yoshihiro Fukuhara, and Shigeo Morishima. 2021. RLTutor:
Reinforcement Learning Based Adaptive Tutoring System by Modeling Virtual
Student with Fewer Interactions. arXiv preprint arXiv:2108.00268 (2021).
[23] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. 2021. Rma: Rapid
motor adaptation for legged robots. arXiv preprint arXiv:2107.04034 (2021).
[24] Qingyao Li, Wei Xia, Liâ€™ang Yin, Jian Shen, Renting Rui, Weinan Zhang, Xianyu
Chen, Ruiming Tang, and Yong Yu. 2023. Graph Enhanced Hierarchical Reinforce-
ment Learning for Goal-oriented Learning Path Recommendation. In Proceedings
of the 32nd ACM International Conference on Information and Knowledge Manage-
ment. 1318â€“1327.
[25] Yuanguo Lin, Yong Liu, Fan Lin, Lixin Zou, Pengcheng Wu, Wenhua Zeng, Huan-
huan Chen, and Chunyan Miao. 2021. A survey on reinforcement learning for
recommender systems. arXiv preprint arXiv:2109.10665 (2021).
[26] Dugang Liu, Pengxiang Cheng, Zinan Lin, Jinwei Luo, Zhenhua Dong, Xiuqiang
He, Weike Pan, and Zhong Ming. 2022. KDCRec: Knowledge Distillation for Coun-
terfactual Recommendation Via Uniform Data. IEEE Transactions on Knowledge
and Data Engineering (2022).
[27] Qi Liu, Shiwei Tong, Chuanren Liu, Hongke Zhao, Enhong Chen, Haiping Ma,
and Shijin Wang. 2019. Exploiting cognitive structure for adaptive learning.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 627â€“635.
[28] David Lopez-Paz, LÃ©on Bottou, Bernhard SchÃ¶lkopf, and Vladimir Vapnik. 2015.
Unifying distillation and privileged information. arXiv preprint arXiv:1511.03643
(2015).
[29] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
 
1629KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qingyao Li, Wei Xia, Liâ€™ang Yin, Jiarui Jin, &Yong Yu
[30] Amir Hossein Nabizadeh, JosÃ© Paulo Leal, Hamed N Rafsanjani, and Rajiv Ratn
Shah. 2020. Learning path personalization and recommendation methods: A
survey of the state-of-the-art. Expert Systems with Applications 159 (2020), 113596.
[31] Hiromi Nakagawa, Yusuke Iwasawa, and Yutaka Matsuo. 2019. Graph-based
knowledge tracing: modeling student proficiency using graph neural network.
InIEEE/WIC/ACM International Conference on Web Intelligence. 156â€“163.
[32] Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami,
Leonidas J Guibas, and Jascha Sohl-Dickstein. 2015. Deep knowledge tracing.
Advances in neural information processing systems 28 (2015).
[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[34] Ralf C Staudemeyer and Eric Rothstein Morris. 2019. Understanding LSTMâ€“a
tutorial into long short-term memory recurrent neural networks. arXiv preprint
arXiv:1909.09586 (2019).
[35] Shuai Wang, Kun Zhang, Le Wu, Haiping Ma, Richang Hong, and Meng Wang.
2021. Privileged graph distillation for cold start recommendation. In Proceedings
of the 44th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 1187â€“1196.
[36] Zhengyang Wu, Ming Li, Yong Tang, and Qingyu Liang. 2020. Exercise recom-
mendation based on knowledge concept prediction. Knowledge-Based Systems
210 (2020), 106481.[37] Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin. 2022. A peep
into the future: Adversarial future encoding in recommendation. In Proceedings
of the Fifteenth ACM International Conference on Web Search and Data Mining.
1177â€“1185.
[38] Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei
Sun, Jian Wu, Hanxiao Sun, and Wenwu Ou. 2020. Privileged features distillation
at taobao recommendations. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2590â€“2598.
[39] Guan Yang, Minghuan Liu, Weijun Hong, Weinan Zhang, Fei Fang, Guangjun
Zeng, and Yue Lin. 2022. Perfectdou: Dominating doudizhu with perfect infor-
mation distillation. Advances in Neural Information Processing Systems 35 (2022),
34954â€“34965.
[40] Hang Yin, Zhiyu Sun, Yanchun Sun, and Gang Huang. 2021. Automatic Learning
Path Recommendation for Open Source Projects Using Deep Learning on Knowl-
edge Graphs. In 2021 IEEE 45th Annual Computers, Software, and Applications
Conference (COMPSAC). IEEE, 824â€“833.
[41] Qian Zhang, Jie Lu, and Guangquan Zhang. 2021. Recommender Systems in
E-learning. Journal of Smart Environments and Green Computing 1, 2 (2021),
76â€“89.
[42] Yuwen Zhou, Changqin Huang, Qintai Hu, Jia Zhu, and Yong Tang. 2018. Person-
alized learning full-path recommendation model based on LSTM neural networks.
Information sciences 444 (2018), 135â€“152.
 
1630