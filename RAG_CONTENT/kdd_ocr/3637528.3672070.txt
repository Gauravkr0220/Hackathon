Topology-Driven Multi-View Clustering via Tensorial Refined
Sigmoid Rank Minimization
Zhibin Gu
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
20112013@bjtu.edu.cnZhendong Li
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
23120371@bjtu.edu.cnSonghe Fengâˆ—
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
shfeng@bjtu.edu.cn
Abstract
Benefiting from the effective exploitation of the high-order corre-
lations across multiple views, tensor-based multi-view clustering
(TMVC) has garnered considerable attention in recent years. Never-
theless, prior TMVC techniques commonly involve assembling mul-
tiple view-specific spatial similarity graphs into a three-dimensional
tensor, overlooking the intrinsic topological structure essential for
precise clustering of data within a manifold. Additionally, main-
stream techniques are constrained by equally shrinking all singular
values to recover a low-rank tensor, limiting their capacity to distin-
guish significant variations among different singular values. In this
investigation, we present an innovative TMVC framework termed
toPology-driven multi-view clustering viA refined teNsorial sig-
moiD rAnk minimization (PANDA). Specifically, PANDA extracts
view-specific topological structures from Euclidean graphs and
intricately integrates them into a low-rank three-dimensional ten-
sor, facilitating the concurrent utilization of intra-view topological
connectivity and inter-view high-order correlations. Moreover, we
develop a refined sigmoid function as the tighter surrogate to tensor
rank, enabling the exploration of significant information of hetero-
geneous singular values. Meanwhile, the topological structures are
merged into a unified structure with varying weights, associated
with a connectivity constraint, empowering the significant diver-
gence among views and the explicit cluster structure of the target
graph are simultaneously leveraged. Extensive experiments demon-
strate the superiority of PANDA, outperforming SOTA methods.
âˆ—Corresponding author of this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672070CCS Concepts
â€¢Computing methodologies â†’Cluster analysis; â€¢Informa-
tion systemsâ†’Information systems applications ;â€¢Theory of
computationâ†’Design and analysis of algorithms.
Keywords
Multi-view clustering, Topological manifold learning, Low-rank
tensor representation, Tensorial refined sigmoid rank
ACM Reference Format:
Zhibin Gu, Zhendong Li, and Songhe Feng. 2024. Topology-Driven Multi-
View Clustering via Tensorial Refined Sigmoid Rank Minimization. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3672070
1 Introduction
In recent years, the widespread emergence of multi-view data across
various real-world applications has sparked sustained interest in
the field of multi-view data analysis [ 11,19,23,33,41,42,46].
Multi-view clustering (MVC), which aims to leverage the rich infor-
mation encapsulated within multiple views to efficiently partition
data points into multiple disjoint subclusters, has emerged as a
highly regarded research direction [ 7,33,40,44,59,67]. From a
methodological standpoint, existing MVC methods can be broadly
categorized into four categories: matrix factorization-oriented ap-
proaches [ 12,56,66], kernel-oriented approaches [ 29â€“31], affinity-
oriented approaches [ 3,13,22] and deep learning-oriented ap-
proaches [ 2,47,48]. Among them, the affinity-oriented methods
have dominated the field of multi-view clustering due to their im-
pressive performance and solid mathematical foundations. Par-
ticularly, the affinity-oriented multi-view clustering (AMVC) ap-
proaches endeavor to derive a consensus affinity matrix that effec-
tively integrates rich information across multiple features, subse-
quently partitioned to assign clustering labels to the data points.
The fundamental challenge of AMVC methods lies in construct-
ing an informative affinity matrix that effectively captures the
inter-point similarity of data points. Based on the construction
mechanisms of the affinity matrix, existing AMVC approaches can
be further classified into two subcategories: subspace-based ap-
proaches and graph-based approaches [ 51]. Based on the construc-
 
920
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhibin Gu, Zhendong Li, & Songhe Feng.
Figure 1: Illustration of the PANDA model. PANDA first constructs multiple view-specific spatial similarity graphs based on
the Euclidean distances between data points. Subsequently, multiple topological manifolds are extracted from the Euclidean
structures and reorganized into a low-rank 3-D tensor. Additionally, we employ a refined tensorial sigmoid function as a tighter
approximation to the tensor rank, thereby facilitating the exploration of significance information of different singular values.
Meanwhile, multiple topological structures are merged with varying weights into a consensus one, enabling the significant
divergence among views and the explicit cluster structure of the target graph are effectively leveraged.
tion mechanisms of the affinity matrix, existing AMVC approaches
can be further classified into two subcategories: subspace-based
approaches and graph-based approaches [ 51]. The former type
of approaches aims to exploit the linear relationship among data
points to recover a subspace representation that effectively mod-
els the pair-wise similarity of data points. For example, Li et al.
[25] employed the Hilbert-Schmidt independence criterion as a
consistency regularization term to learn a comprehensive represen-
tation, thereby facilitating the recovery of a significant consensus
subspace representation. Yang et al. [ 60] proposed to decompose
multiple subspace representations into a consensus component and
multiple view-specific components, enabling simultaneous utiliza-
tion of multi-view consistency and complementarity. Pan and Kang
[39] incorporated contrastive loss into the learned subspace ma-
trix, effectively attracting similar data points toward each other
and separating dissimilar data points. The graph-based methods
endeavor to derive an informative similarity graph compatible with
multiple features by leveraging the Euclidean distances between
data points. For instance, Nie et al. [ 35] developed a local structured
graph learning mechanism to directly obtain a consensus graph
across multiple views, which possesses a clearly defined number
of connected components. Tang et al. [ 45] leveraged the graph
diffusion mechanism to acquire enhanced graphs from multiple
pre-defined similarity graphs for exploiting valuable information.
Huang et al. [ 17] integrated graph learning and cross-view incon-
sistency detection within a unified framework, effectively driving
a comprehensive graph that can capture the underlying similarity
among data points. Furthermore, the research efforts of [ 24], [57],and [ 64] employed bipartite graphs to estimate the similarity be-
tween data points and anchor points, resulting in notable strides
towards enhanced computational efficiency.
To effectively exploit the high-order correlations underlying mul-
tiple features, a scheme of stacking multiple view-specific affinity
matrices into a low-rank three-dimensional (3-D) tensor has gar-
nered sustained attention in recent years. For example, Zhang et
al. [63] introduced a pioneering tensor-based multi-view subspace
clustering approach, which minimizes the weighted sum of nuclear
norms of multiple affinity matrices to leverage the high-order in-
formation across views. Extending the work of [ 63], Xie et al. [ 54]
applied the tensor Singular Value Decomposition (t-SVD) based
tensor nuclear norm to the rotated tensor, allowing for effective
exploration of cross-view consistency. Subsequently, numerous
variants inspired by the seminal work of [ 54] have emerged. For
instance, both Wu et al. [ 52] and Chen et al. [ 4] incorporated the
local graph learning and low-rank tensor construction into a uni-
fied model, enabling the joint optimization of the learning process
for multiple view-specific similarity graphs and global low-rank
tensor. Jia et al. [ 20] imposed low-rank and sparse constraints on
the frontal and horizontal slices of the constructed tensor, yielding a
tensor representation tailored to clustering tasks. In addition, Xia et
al. [53] employed the Schatten- ğ‘norm as an explicit considerations
for approximating the minimization of low-rank tensor, with the
aim of incorporating the salient information of different singular
values.
Despite the promising clustering performance demonstrated by
the aforementioned tensor-based multi-view clustering (TMVC)
 
921Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
methods, there are still areas that offer the potential for further im-
provement and refinement in the following three key perspectives.
Firstly, previous TMVC algorithms typically stack multiple view-
specific spatial similarity graphs into a low-rank three-dimensional
(3-D) tensor, failing to consider the topological structure within the
data that are essential for clustering data on the manifold. Secondly,
most mainstream TMVC techniques are constrained by their re-
liance on equally shrinking all singular values to recover a low-rank
tensor. However, large and small singular values in tensor data are
commonly regarded as carriers of valuable dominant information
and residual noise information, respectively. Consequently, such
an equal scaling strategy hinders the detection of significant differ-
ences between heterogeneous singular values, ultimately leading
to inferior tensor representations. Furthermore, the generation of
cluster labels and the learning of the consensus affinity matrix are
typically conducted as separate steps, giving rise to a potential
mismatch between the learned consensus matrix and its efficacy
in accurately generating cluster labels. As a consequence, this dis-
crepancy leads to suboptimal clustering outcomes.
Building upon the aforementioned insights and motivations,
this paper presents the PANDA model, a novel topology-driven
tensor-based multi-view clustering framework, which seamlessly
integrates the discovery of topological relevance of data points, the
detection of the salient differences among distinct singular values
of tensor data, and the exploration of explicit clustering structure of
the consensus affinity matrix within a unified optimization frame-
work. By this way, the three learning procedures can be seamlessly
interconnected, resulting in the achievement of a superior solu-
tion and ultimately leading to improved clustering performance.
Specifically, as depicted in Fig. 1, the PANDA model initiates by
constructing multiple view-specific spatial similarity graphs based
on the Euclidean distances between data pairs. Subsequently, from
these Euclidean structures, multiple topological manifolds are ex-
tracted and reorganized into a low-rank 3-D tensor, which facilitates
the simultaneous utilization of intra-view topological connectivity
among data points and inter-view high-order correlations across
multiple features. Moreover, it is of particular significance that we
develop a refined tensorial sigmoid function as a notably tighter
approximation to the tensor rank minimization, thereby enabling
the exploration of significance information of different singular
values in the tensor data. Furthermore, multiple enhanced topolog-
ical structures are merged with varying weights into a consensus
one, which is associated with an explicit connectivity constraint,
enabling the significant divergence among views and the explicit
cluster structure of the target affinity matrix to be simultaneously
leveraged. Compared to the existing TMVC algorithms, the con-
tributions and innovations of this paper can be summarized as
follows:
â€¢An innovative TMVC framework named PANDA is proposed,
which integrates the extraction of topological manifolds, con-
struction of low-rank tensor, and learning of the consensus
structured graph into a unified optimization framework. In
this way, the three learning procedures can be seamlessly
interconnected, resulting in the achievement of a superior
solution and ultimately leading to improved clustering per-
formance.â€¢We develop a refined sigmoid function as a tighter approx-
imation to the tensor rank minimization, allowing for the
selective application of different degrees of shrinkage to
small and large singular values. This novel approach facili-
tates the effective exploration of the significance associated
with diverse singular values in tensor data.
â€¢Instead of reorganizing multiple view-specific spatial sim-
ilarity graphs into a low-rank 3-D tensor conventionally,
our approach stacks multiple topological manifolds into a
unified tensor representation, enabling effective exploration
of the topological connectivity among data points.
â€¢An efficient iterative algorithm based on the alternating di-
rection method of multipliers (ADMM) is designed to op-
timize the objective function, and extensive experimental
results demonstrate the superiority of our approach over
SOTA methods.
2 Notations and Preliminary
In this section, we provide an overview of the mathematical no-
tations and preliminary definitions used throughout this paper.
Specifically, scalars, vectors, matrices, and tensors are denoted by
lowercase letters, bold lowercase letters, and bold uppercase let-
ters, and bold calligraphy letters (i.e., ğ‘,a,A, andA), respectively.
For matrix A, its Frobenius norm and nuclear norm are denoted
asâˆ¥Aâˆ¥ğ¹=âˆšï¸ƒÃ
ğ‘–ğ‘—|A(ğ‘–,ğ‘—)|2andâˆ¥Aâˆ¥âˆ—=Ã
ğ‘£ğ›¿ğ‘£(A), where A(ğ‘–,ğ‘—)
andğ›¿ğ‘£(A)represent the ğ‘–ğ‘—-th element and the ğ‘£-th largest singular
value of A, respectively. For the tensor AâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3, its t-SVD
definition is as A=Oâˆ—Pâˆ—Qğ‘‡, where OâˆˆRğ‘›1Ã—ğ‘›1Ã—ğ‘›3and
QâˆˆRğ‘›2Ã—ğ‘›2Ã—ğ‘›3are orthogonal tensors, and PâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3is a
ğ‘“-diagonal tensor composed of diagonal matrices as frontal slices.
Definition 1 (TNN [ 32],[68]).For the tensor AâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3,
then the tensor nuclear norm can be defined as follows:
âˆ¥Aâˆ¥âŠ›=1
ğ‘›3ğ‘›3âˆ‘ï¸
ğ‘˜=1â„âˆ‘ï¸
ğ‘–=1Pğ‘˜
ğ‘“(ğ‘–,ğ‘–) (1)
wherePâˆˆRğ‘›1Ã—ğ‘›1Ã—ğ‘›3is derived from t-SVD (i.e., A=Oâˆ—Pâˆ—Qğ‘‡
) in the Fourier domain.
3 The Proposed Method
3.1 The PANDA Model
Given a multi-view dataset {X(ğ‘£)}ğ‘š
ğ‘£=1withğ‘šviews andğ‘›individ-
uals, where X(ğ‘£)âˆˆRğ‘›Ã—ğ‘‘ğ‘£denotes the ğ‘£-th view feature matrix and
ğ‘‘ğ‘£is the corresponding feature dimension. The affinity-oriented
multi-view clustering (AMVC) aims to leverage the rich information
enclosed within multiple features to learn an informative consensus
affinity matrix, thereby facilitating the partitioning of data points
into distinct subclusters. The generalized mathematical representa-
tion of the AMVC process is outlined as follows:
min
{Z(ğ‘£)},U,{ğœ‰(ğ‘£)}ğ‘šâˆ‘ï¸
ğ‘£=1
L(X(ğ‘£),Z(ğ‘£))+ğœ‰(ğ‘£)âˆ¥Z(ğ‘£)âˆ’Uâˆ¥2
ğ¹	
+ğ›½âˆ¥Uâˆ¥Î˜
s.t. z(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘§(ğ‘£)
ğ‘–ğ‘—â‰¥0, u(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘¢(ğ‘£)
ğ‘–ğ‘—â‰¥0,(2)
 
922KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhibin Gu, Zhendong Li, & Songhe Feng.
where Z(ğ‘£)âˆˆRğ‘›Ã—ğ‘›andUâˆˆRğ‘›Ã—ğ‘›denote theğ‘£-th view-specific
affinity matrix and the consensus affinity matrix, respectively. ğ›½is
a trade-off parpmeter. L(X(ğ‘£),Z(ğ‘£))is responsible for the compu-
tation of the view-specific affinity matrix. The second term amalga-
mates the affinity matrices from multiple views {Z(ğ‘£)}ğ‘š
ğ‘£=1into a
unified structure U, proficiently leveraging the abundant informa-
tion inherent in the multi-view data. In this context, ğœ‰(ğ‘£)signifies
the weight attributed to the ğ‘£-th view in the amalgamation pro-
cess. The ultimate regularization term encompasses a structured
penalty imposed upon the consensus affinity matrix, with varying
selections of Î˜(includingâ„“0,â„“1,â„“2, or the nuclear norm) resulting
in diverse properties of the affinity matrix, such as sparsity and
connectivity [61],[28],[58].
To alleviate the burden of parameter tuning during the weight
allocation process, we adopt the inverse matrix strategy to dynam-
ically assign weights to different views [ 36], [49], which can be
mathematically expressed as follows:
ğœ‰(ğ‘£)=1
2âˆ¥Z(ğ‘£)âˆ’Uâˆ¥2
ğ¹. (3)
By optimizing Eq. (2) with the weight allocation scheme outlined
in Eq. (3), we can obtain a consensus affinity matrix Uthat efficiently
integrates valuable information from multiple views. Nonetheless,
Model (2) only considers pair-wise sample correlation and pair-
wise view correlation, neglecting higher-order correlations among
multiple views, thereby limiting the exploration of the abundant
information embedded within multiple views. In view of this, an ef-
fective strategy emerges: the aggregation of multiple view-specific
affinity matrices is restructured into a 3-D tensor endowed with a
low-rank structure. This strategic amalgamation provides a path-
way to delve into the inherent higher-order correlations within
the multiple features, thereby contributing to an enhancement in
clustering performance. Consequently, an improved tensor-based
affinity-oriented multi-view clustering model takes the following
form:
min
{Z(ğ‘£)},U,{ğœ‰(ğ‘£)}ğ‘šâˆ‘ï¸
ğ‘£=1
L(X(ğ‘£),Z(ğ‘£))+ğœ‰(ğ‘£)âˆ¥Z(ğ‘£)âˆ’Uâˆ¥2
ğ¹	
+ğ›½âˆ¥Zâˆ¥âŠ›
s.t. z(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘§(ğ‘£)
ğ‘–ğ‘—â‰¥0, u(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘¢(ğ‘£)
ğ‘–ğ‘—â‰¥0,
ğœ‰(ğ‘£)â‰¥0,Z=ğœ“(Z(1),Z(2),...,Z(ğ‘š)),
(4)
whereZâˆˆRğ‘›Ã—ğ‘šÃ—ğ‘›represents a three-dimensional tensor formed
by stacking multiple view-specific affinity matrices, and ğœ“(Â·)is the
tensorization operator. âˆ¥Â·âˆ¥âŠ›denotes the tensor nuclear norm (TNN),
which is employed to preserve the low-rank property inherent in
the tensor data.
Despite Model (4) exhibits efficacy in capturing higher-order
correlations beneath multiple views, it falls short in considering
the intrinsic topological connectivity between data pointsâ€”a piv-
otal aspect for proficient clustering tasks. More specifically, due to
the inherent nature of datasets in the real world, which often dis-
plays non-linear low-dimensional manifold structures embedded in
high-dimensional spaces, there exists a compelling hypothesis that
the topological connections among individuals can be propagated
from proximate to distant entities [ 65],[34],[50]. In other words, if
a given pair of data points shows a substantial Euclidean distancewhile maintaining shared neighbor connections, they should still
display a strong relevance and have a high likelihood of belonging
to the same cluster. As shown in Fig. 2, although Nodes 1 and 5
exhibit a significant spatial separation, their pronounced topolog-
ical correlation emerges from the shared adjacency with Node 3.
This shared neighbor engenders a substantial degree of topological
interdependence between Nodes 1 and 5, fostering heightened con-
nectivity. Conversely, Nodes 5 and 6 exhibit the opposite scenario,
where their proximity in spatial positioning contrasts with their
low topological affinity due to the absence of shared neighboring
connections. Nonetheless, such pivotal topological connectivity
relationships are overlooked within the Euclidean spatial struc-
ture, leading to a degradation in the representational capacity of
the constructed three-dimensional tensor. Consequently, this limi-
tation significantly impacts the clustering performance, yielding
suboptimal outcomes.
Figure 2: Comparing
Euclidean Spatial Simi-
larity with Topological
Structural Similarity.To address the aforementioned
limitation of inadequate explo-
ration of topological connectivity
among data points, we depart from
the conventional Euclidean frame-
work employed in capturing inter-
data point similarity. Instead, we
turn to the utilization of topologi-
cal manifold, extracted from the Eu-
clidean spatial structure, to approx-
imate the inherent similarity between data points. Subsequently,
the multiple extracted topological manifolds are combined and or-
ganized into a 3-D low-rank tensor, allowing for the simultaneous
utilization of topological connectivity among data points and high-
order correlations across multiple features. As a result, an enhanced
topology-driven tensor-based AMVC mathematical model can be
formulated as follows:
min
{A(ğ‘£)},U,{ğœ‰(ğ‘£)}1
2ğ‘šâˆ‘ï¸
ğ‘£=1(ğ‘›âˆ‘ï¸
ğ‘–,ğ‘—,ğ‘˜=1Z(ğ‘£)
ğ‘—ğ‘˜(A(ğ‘£)
ğ‘–ğ‘—âˆ’A(ğ‘£)
ğ‘–ğ‘˜)2
+ğ›¼âˆ¥A(ğ‘£)âˆ’Iâˆ¥2
ğ¹+ğœ‰(ğ‘£)âˆ¥A(ğ‘£)âˆ’Uâˆ¥2
ğ¹)
+ğ›½âˆ¥Aâˆ¥âŠ›
s.t. a(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0, u(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘¢(ğ‘£)
ğ‘–ğ‘—â‰¥0,
ğœ‰(ğ‘£)â‰¥0,A=ğœ“(A(1),A(2),...,A(ğ‘š)),(5)
where Z(ğ‘£)âˆˆRğ‘›Ã—ğ‘›denotes the ğ‘£-th view-specific affinity matrix,
which is generated by the off-the-shelf local structured graph learn-
ing paradigm [ 38].A(ğ‘£)âˆˆRğ‘›Ã—ğ‘›denotes the ğ‘£-th view topological
manifold, with element ğ‘(ğ‘£)
ğ‘–ğ‘—encoding the topological relevance
between the ğ‘–-th andğ‘—-th data points. AâˆˆRğ‘›Ã—ğ‘šÃ—ğ‘›constitutes a
three-dimensional tensor organized by multiple extracted topologi-
cal manifolds{A(ğ‘£)}ğ‘š
ğ‘£=1.ğ›¼andğ›½are two trade-off parameters. The
first term in Eq. (5) functions as a smoothness constraint that aligns
with the topological connectivity, ensuring that ğ‘—-th andğ‘˜-th data
points share a comparable topological relationship with ğ‘–-th data
point when they exhibit similarity. The inclusion of the second term
serves to avoid obtaining trivial solutions in the manifold A(ğ‘£).
Furthermore, to mitigate the potential influence of a data pointâ€™s
connections with numerous similar neighbors on the objective
 
923Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
value, a normalized version of Eq. (5) is utilized, which is formulated
passively as follows:
min
{A(ğ‘£)},U,{ğœ‰(ğ‘£)}1
2ğ‘šâˆ‘ï¸
ğ‘£=1(ğ‘›âˆ‘ï¸
ğ‘–,ğ‘—,ğ‘˜=1Z(ğ‘£)
ğ‘—ğ‘˜ A(ğ‘£)
ğ‘–ğ‘—âˆšï¸ƒ
D(ğ‘£)
ğ‘—ğ‘—âˆ’A(ğ‘£)
ğ‘–ğ‘˜âˆšï¸ƒ
D(ğ‘£)
ğ‘˜ğ‘˜!2
+ğ›¼âˆ¥A(ğ‘£)âˆ’Iâˆ¥2
ğ¹+ğœ‰(ğ‘£)âˆ¥A(ğ‘£)âˆ’Uâˆ¥2
ğ¹)
+ğ›½âˆ¥Aâˆ¥âŠ›
s.t. a(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0, u(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘¢(ğ‘£)
ğ‘–ğ‘—â‰¥0,
ğœ‰(ğ‘£)â‰¥0,A=ğœ“(A(1),A(2),...,A(ğ‘š)).(6)
where D(ğ‘£)is the degree matrix of Z(ğ‘£).
0 0.5 1 1.5 2 2.5 3 3.5 4
x00.511.522.533.54yTrue Rank
TNN
TSLpN, p=0.5
TRS,  = 0.1
TRS,  = 0.01
Figure 3: Comparing
Tensor Rank Estima-
tion Methods.Thus far we have constructed an
objective function with topological
manifold learning scheme and ten-
sor low-rank regularization mecha-
nism, which ensures that the topo-
logical connectivity among data
points and the high-order correla-
tions beneath multiple views can
be effectively explored simultane-
ously. However, the current model is still constrained by its reliance
on equally shrinking all singular values to recover a low-rank ten-
sor, preventing the effective discovery of the salience differences
between heterogeneous singular values. The principal reason lies
in the inherent characteristics of singular values within tensor data,
where larger and smaller ones typically carry dominant valuable in-
formation and sparse noise information, respectively. Consequently,
applying an equal shrinking strategy to all singular values leads to
excessive penalization of larger values and insufficient penalization
of smaller ones, thereby yielding suboptimal tensor representations.
In light of this, we develop a Tensorizal Refined Sigmoid rank (TRS)
as a tighter approximation to the tensor rank minimization (i.e.,
ğ‘“(ğ‘¥)=ğ‘¥
ğ‘¥+ğ‘’ğ‘¥ğ‘(âˆ’ğ‘¥
ğ›¿)). Unlike the linear penalty to different singulars
employed in traditional tensor nuclear norm (TNN) methods (i.e.,
ğ‘“(ğ‘¥)=ğ‘¥), the TRS allows for the adaptive imposition of varying
degrees of punishment on large and small singular values, which is
defined as follows:
Definition 2. For the given tensor AâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3, its Tensorial
Refined Sigmoid rank (TRS) definition is as follows:
âˆ¥Aâˆ¥TRS:=1
ğ‘›3ğ‘›3âˆ‘ï¸
ğ‘˜=1âˆ¥Ağ‘˜
ğ‘“âˆ¥TRS
=1
ğ‘›3ğ‘›3âˆ‘ï¸
ğ‘˜=1â„âˆ‘ï¸
ğ‘–=1 Pğ‘˜
ğ‘“(ğ‘–,ğ‘–)
Pğ‘˜
ğ‘“(ğ‘–,ğ‘–)+ğ‘’ğ‘¥ğ‘(âˆ’Pğ‘˜
ğ‘“(ğ‘–,ğ‘–)/ğ›¿)
,(7)
where 0<ğ›¿â‰¤1,â„=ğ‘šğ‘–ğ‘›(ğ‘›1,ğ‘›2).Ağ‘˜
ğ‘“denotes theğ‘˜-th frontal slice
ofAandPğ‘“is the representation of the Fourier domain obtained by
decomposition of the tensor-SVD (i.e., A=Oğ‘“Pğ‘“QâŠ¤
ğ‘“).
By substituting the tensor nuclear norm (TNN) in Model (6)
with the tensorial refined sigmoid rank (TRS) defined in Eq. (7), we
can effectively consider the significance information of different
singular values of the tensor data. This approach facilitates thegeneration of the desired view-specific affinity matrix A(ğ‘£), thereby
promoting the creation of a valuable consensus affinity matrix U.
As a result, this consensus affinity matrix serves as a fundamental
basis for subsequent clustering tasks.
In an ideal scenario, if the connectivity components within the
affinity matrix Uare equal to the true number of clusters, we can
directly obtain the clustering labels from the structure of the graph
matrix, thereby circumventing the need for additional graph-cut
procedures. Coincidentally, such an ideal structure can be achieved
by imposing a rank constraint on the Laplacian matrix of the con-
sensus graph (i.e., ğ‘Ÿğ‘ğ‘›ğ‘˜(LU)=ğ‘›âˆ’ğ‘)[38], where LUandğ‘are the
Laplacian matrix and number of clusters, respectively. Moreover,
due to the positive semi-definite nature of LU, the rank constraint
problem can be reformulated as a singular value optimization prob-
lem (i.e.,Ãğ‘
ğ‘–=1ğœ”ğ‘–(LU)=0), whereğœ”ğ‘–(U)represents the ğ‘–-th smallest
singular value of LU. According to Ky Fanâ€™s theorem [ 8], the non-
convex singular value optimization problem can be further relaxed
into the following convex problem:
ğ‘âˆ‘ï¸
ğ‘–=1ğœ”ğ‘–(LU)=min
PâŠ¤P=ITr(PâŠ¤LUP), (8)
where PâˆˆRğ‘›Ã—ğ‘denotes the embedding matrix.
By consolidating Eqs. (6)-(8) within a cohesive framework, we
arrive at the comprehensive formulation of the PANDA model in
the following form:
min
{A(ğ‘£)},U,
{ğœ‰(ğ‘£)},P1
2ğ‘šâˆ‘ï¸
ğ‘£=1(ğ‘›âˆ‘ï¸
ğ‘–,ğ‘—,ğ‘˜=1Z(ğ‘£)
ğ‘—ğ‘˜ A(ğ‘£)
ğ‘–ğ‘—âˆšï¸ƒ
D(ğ‘£)
ğ‘—ğ‘—âˆ’A(ğ‘£)
ğ‘–ğ‘˜âˆšï¸ƒ
D(ğ‘£)
ğ‘˜ğ‘˜!2
+ğ›¼âˆ¥A(ğ‘£)âˆ’Iâˆ¥2
ğ¹
+ğœ‰(ğ‘£)âˆ¥Uâˆ’A(ğ‘£)âˆ¥2
ğ¹)
+ğ›½âˆ¥Aâˆ¥TRS+2ğœ†Tr(PâŠ¤LUP)
s.t. a(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0, u(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘¢(ğ‘£)
ğ‘–ğ‘—â‰¥0,
ğœ‰(ğ‘£)â‰¥0,A=ğœ“(A(1),A(2),...,A(ğ‘š)),PâŠ¤P=I,(9)
whereğœ†is a self-tuned parameter whose value determines whether
the target affinity matrix Uexhibitsğ‘connected components [ 35].
Technically, if the number of connected components is smaller than
the desired number ğ‘, we increase ğœ†, and vice versa.
Remark 1. [The benefits of Tensorial Refined Sigmoid rank
(TRS)] We develop a refined sigmoid function as the non-convex
surrogate for the tensor rank function. In essence, the developed
TRS satisfies ğ‘“ğ‘‡ğ‘…ğ‘†(ğ‘¥)=0, aligning with the true rank function.
For clarity, Fig. 3 presents a visual comparison between the ten-
sor true rank, two commonly used rank approximation methods
(i.e., TNN [ 54] and LTSğ‘N [10]), and our TRS. From Fig. 3, it can
be observed that our proposed TRS outperforms TNN and LTS ğ‘N
in approximating the tensor rank, particularly for values close to
zero and large singular values. Specifically, as ğ‘¥approaches zero,
ğ‘“ğ‘‡ğ‘…ğ‘†(ğ‘¥)â‰«ğ‘¥andğ‘“ğ‘‡ğ‘…ğ‘†(ğ‘¥)â‰«ğ‘“ğ¿ğ‘‡ğ‘†ğ‘ğ‘(ğ‘¥), indicating a strong penal-
ization on small singular values. This observation highlights the
robustness against noise, as small singular values often arise from
noise and outliers. Conversely, as ğ‘¥tends towards a larger value,
ğ‘“ğ‘‡ğ‘…ğ‘†(ğ‘¥)â‰ªğ‘¥andğ‘“ğ‘‡ğ‘…ğ‘†(ğ‘¥)<ğ‘“ğ¿ğ‘‡ğ‘†ğ‘ğ‘(ğ‘¥), indicating the preserva-
tion of significant large singular values and minimizing the penalty
on valuable information.
 
924KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhibin Gu, Zhendong Li, & Songhe Feng.
Remark 2. [The advantages of topological manifolds] In-
spired by the propagation-based manifold learning [ 50], the PANDA
model deploys topological manifolds extracted from Euclidean simi-
larity graphs, rather than traditional Euclidean similarity graphs (i.e.
,{Z(ğ‘£)}ğ‘š
ğ‘£=1) to approximate data point similarities. Subsequently,
these extracted topological manifold representations are aggregated
to form a three-dimensional tensor. As illustrated in Fig. 2, the uti-
lization of topological manifolds breaks free from the constraints of
traditional Euclidean structures dependent on pairwise Euclidean
distances, ingeniously harnessing the inherent topological correla-
tions among data points. This affinity learning strategy effectively
captures their underlying affinity relationships with enhanced pre-
cision, ultimately leading to improved clustering performance.
3.2 Optimization
An iterative scheme based on the Alternating Direction Method of
Multipliers (ADMM) is proposed to optimize the objective function.
To ensure the separability of each variable in problem (9), an auxil-
iary variable Jis firstly introduced. This allows us to transform
the optimization of Eq. (9) into subproblems involving five sets
of variables P,{A(ğ‘£)},U,{ğœ‰(ğ‘£)}, andJ. Due to space constraints,
detailed optimization steps and pseudocode are presented in the
Appendix A.
3.3 Convergence Analysis
The convergence of the optimization algorithm is ensured by the
validation outlined in Theorem 1.
Theorem 1. The sequenceSğ‘¡={A(ğ‘£)
ğ‘¡,Uğ‘¡,Pğ‘¡,Ağ‘¡,Jğ‘¡}âˆ
ğ‘¡=1re-
sulting from the optimization process adheres to the following two
principles:
â€¢{Sğ‘¡}âˆ
ğ‘¡=1is bouned;
â€¢Any accumulation point of {Sğ‘¡}âˆ
ğ‘¡=1converges to a stationary
Karush-Kuhn-Tucker (KKT) point.
3.4 Complexity Analysis
The computational complexity of optimization process revolves
around five sets of variables: {A(ğ‘£)},U,J,P, and{ğœ‰ğ‘£}. For{A(ğ‘£)},
we utilize the augmented Lagrangian multiplier method, result-
ing in a computational complexity of O(ğ‘›ğ‘š2ğ‘+ğ‘›ğ‘šğ‘). Moving on
to the consensus affinity matrix U, its optimization incurs a com-
putational complexity of O(ğ‘¡ğ‘¢ğ‘šğ‘›2), whereğ‘¡ğ‘¢signifies the corre-
sponding number of iterations. In optimizing J, a combination
of Fast Fourier Transform (FFT) and its inverse operation on a
three-dimensional tensor, along with calculating the singular value
decomposition for ğ‘›matrices in the Fourier domain, collectively
contribute to a computational complexity of O(2ğ‘›2ğ‘šlog(ğ‘›)). Addi-
tionally, updating variable Pinvolves the eigendecomposition of
a Laplacian matrix, which can be executed with a computational
complexity ofO(ğ‘ğ‘›2). Determining the weights {ğœ‰ğ‘£}in the final
step requires a computational effort of O(ğ‘šğ‘›2). Summarily, the
overall computational complexity of our method can be approxi-
mated asğœ‘O(ğ‘›ğ‘š2ğ‘+ğ‘›ğ‘šğ‘+(ğ‘¡ğ‘¢ğ‘š+2ğ‘›2ğ‘šlog(ğ‘›)+ğ‘+ğ‘š)ğ‘›2), where
ğœ‘represents the number of iterations.Table 1: Statistical Characteristics of the Six Real-World
Datasets.
Datasets
Objects/Clusters Views Dimensions
3sour
ces 169/6 3 3560, 3631, 3086
NGs 500/5 3 2000,2000,2000
BBCsport 544/2 2 3183,3203
Wiki-feature 2866/6 3 200,200,200
Hdigit 10000/10 2 784,256
ALOI-100 10800/100 4 77,13,64,125
3.5 Comparative Analysis with Prior Research
In recent years, several relevant methodologies have been proposed,
such as t-SVD-MVC [ 54], JLMVC [ 5] UGLTL [ 52], TMVC [ 21], and
ULTLSE [ 9], which stack multiple view-specific affinity matrices
into a 3-D low-rank tensor to explore the higher-order correlations
beneath multi-view data. However, these methods exhibit funda-
mental differences from our proposed PANDA model. Specifically,
these approaches typically utilize the convex tensor nuclear norm
(TNN) as an approximation for the low-rank minimization problem,
thereby disregarding the significant information among distinct
singular values and resulting in suboptimal tensor representations.
In light of this, TBGL [53] and LTS ğ‘N [10] employ the Schatten- ğ‘
norm and logarithmic Schatten- ğ‘norm, respectively, as explicit
considerations for approximating the minimization of low-rank
tensor, with the aim of incorporating the salient information of dif-
ferent singular values. However, their scaling remains insufficient
for small singular values, thereby leading to the persistence of noise
and suboptimal clustering performance. Moreover, all of these ap-
proaches inherently restructure the multiple Euclidean structures
into a 3-D low-rank tensor, consequently merely capturing the
spatial similarity of data point and inadequately exploring the criti-
cal topological relevance that are essential for clustering task on
the manifold. Once again, despite the findings of the MSCTM [ 18],
MVCTM [ 16], and SMGC [ 43] highlighting the significance of the
data pointâ€™s underlying topological manifolds for clustering tasks,
these approaches are primarily grounded in 2-D matrix spaces.
As a result, they are limited in their ability to effectively explore
the higher-order correlations concealed within multi-view data,
let alone account for the distinct saliency disparities among the
singular values encapsulated within tensor data.
In contrast to these approaches, the developed PANDA model
constructs a low-rank 3-D tensor by stacking multiple view-specific
topological manifolds. Furthermore, we introduce a novel refined
sigmoid function as a compact approximation for the low-rank ten-
sor function, enabling simultaneous exploration of the topological
connectivity among data points and the significance of different
singular values. These groundbreaking contributions of the PANDA
model signify significant progress in the field, offering novel in-
sights into the exploration of high-order correlations in multi-view
data.
4 Experiment
In this section, we conduct extensive experiments to validate the
effectiveness of the PANDA model.
 
925Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Performance Comparison (Mean Â±Standard Deviation) of SOTA Models on the Hdigit and ALOI-100 Datasets.
Datasets 3sour
ces NGs
Metho
ds A
CC NMI PUR F-score ARI A
CC NMI PUR F-score ARI
A
WP 0.467Â±0.000
0.403Â±0.000 0.592Â±0.000 0.395Â±0.000 0.416Â±0.000 0.200Â±0.000
0.086Â±0.000 0.308Â±0.000 0.316Â±0.000 0.023Â±0.000
MCGC 0.337Â±0.000
0.135Â±0.000 0.408Â±0.000 0.311Â±0.000 -0.039Â±0.000 0.288Â±0.000
0.092Â±0.000 0.306Â±0.000 0.321Â±0.000 0.013Â±0.000
DGF 0.482Â±0.005
0.437Â±0.017 0.632Â±0.02 0.434Â±0.012 0.266Â±0.03 0.305Â±0.002
0.070Â±0.001 0.324Â±0.000 0.302Â±0.000 0.038Â±0.000
SFMC 0.373Â±0.000
0.064Â±0.000 0.379Â±0.000 0.383Â±0.000 0.021Â±0.000 0.218Â±0.000
0.021Â±0.000 0.220Â±0.000 0.329Â±0.000 0.001Â±0.000
MSC2D 0.662Â±0.015
0.522Â±0.075 0.723Â±0.028 0.575Â±0.048 0.402Â±0.081 0.979Â±0.003
0.933Â±0.008 0.979Â±0.003 0.959Â±0.005 0.949Â±0.007
MSCTM 0.763Â±0.000
0.626Â±0.000 0.799Â±0.000 0.670Â±0.000 0.544Â±0.000 0.440Â±0.000
0.351Â±0.000 0.500Â±0.000 0.432Â±0.000 0.205Â±0.000
MFLVC 0.627Â±0.000
0.543Â±0.000 0.716Â±0.000 0.632Â±0.000 0.524Â±0.000 0.710Â±0.000
0.567Â±0.000 0.736Â±0.000 0.632Â±0.000 0.534Â±0.000
MVCTM 0.793Â±0.000
0.633Â±0.000 0.799Â±0.000 0.726Â±0.000 0.621Â±0.000 0.264Â±0.000
0.071Â±0.000 0.266Â±0.000 0.334Â±0.000 0.015Â±0.000
TBGL 0.379Â±0.000
0.067Â±0.000 0.379Â±0.000 0.377Â±0.000 0.018Â±0.000 0.236Â±0.000
0.035Â±0.000 0.24Â±0.000 0.327Â±0.000 0.002Â±0.000
PANDA 0.805Â±0.000
0.689Â±0.000 0.840Â±0.000 0.735Â±0.000 0.642Â±0.000 0.982Â±0.000
0.941Â±0.000 0.982Â±0.000 0.964Â±0.000 0.955Â±0.000
Datasets BBCsp
ort Wiki-featur
e
A
WP 0.542Â±0.000
0.254Â±0.000 0.572Â±0.000 0.467Â±0.000 0.237Â±0.000 0.467Â±0.000
0.331Â±0.000 0.509Â±0.000 0.349Â±0.000 0.269Â±0.000
MCGC 0.412Â±0.000
0.095Â±0.000 0.430Â±0.000 0.398Â±0.000 0.046Â±0.000 0.430Â±0.003
0.336Â±0.005 0.494Â±0.003 0.306Â±0.001 0.217Â±0.001
DGF 0.747Â±0.002
0.652Â±0.001 0.751Â±0.002 0.708Â±0.002 0.593Â±0.002 0.291Â±0.001
0.180Â±0.000 0.306Â±0.000 0.225Â±0.000 0.052Â±0.000
SFMC 0.366Â±0.000
0.021Â±0.000 0.369Â±0.000 0.385Â±0.000 0.004Â±0.000 0.377Â±0.000
0.347Â±0.000 0.440Â±0.000 0.239Â±0.000 0.088Â±0.000
MSC2D 0.599Â±0.014
0.449Â±0.039 0.629Â±0.02 0.567Â±0.018 0.349Â±0.032 0.378Â±0.047
0.272Â±0.057 0.384Â±0.048 0.275Â±0.038 0.124Â±0.055
MSCTM 0.752Â±0.000
0.601Â±0.000 0.789Â±0.000 0.689Â±0.000 0.577Â±0.000 0.159Â±0.000
0.030Â±0.000 0.188Â±0.000 0.158Â±0.000 0.001Â±0.000
MFLVC 0.643Â±0.000
0.453Â±0.000 0.684Â±0.000 0.510Â±0.000 0.375Â±0.000 0.494Â±0.000
0.414Â±0.000 0.494Â±0.000 0.394Â±0.000 0.304Â±0.000
MVCTM 0.421Â±0.000
0.201Â±0.000 0.461Â±0.000 0.445Â±0.000 0.125Â±0.000 0.151Â±0.000
0.021Â±0.000 0.176Â±0.000 0.177Â±0.000 -0.004Â±0.000
TBGL 0.548Â±0.000
0.277Â±0.000 0.55Â±0.000 0.473Â±0.000 0.188Â±0.000 0.495Â±0.000
0.519Â±0.000 0.579Â±0.000 0.407Â±0.000 0.320Â±0.000
PANDA 0.813Â±0.000
0.744Â±0.000 0.849Â±0.000 0.808Â±0.000 0.741Â±0.000 0.529Â±0.000
0.490Â±0.000 0.559Â±0.000 0.439Â±0.000 0.348Â±0.000
Datasets Hdigit ALOI-100
A
WP 0.718Â±0.000
0.767Â±0.000 0.765Â±0.000 0.691Â±0.000 0.655Â±0.000 0.619Â±0.000
0.716Â±0.000 0.631Â±0.000 0.499Â±0.000 0.493Â±0.000
MCGC 0.712Â±0.032
0.688Â±0.022 0.731Â±0.028 0.632Â±0.031 0.589Â±0.034 0.637Â±0.000
0.697Â±0.000 0.645Â±0.000 0.167Â±0.000 0.548Â±0.000
DGF 0.523Â±0.000
0.582Â±0.000 0.566Â±0.001 0.441Â±0.000 0.346Â±0.000 0.267Â±0.007
0.359Â±0.015 0.288Â±0.006 0.040Â±0.002 0.021Â±0.002
SFMC 0.998Â±0.000
0.993Â±0.000 0.998Â±0.000 0.995Â±0.000 0.995Â±0.000 0.680Â±0.000
0.702Â±0.000 0.691Â±0.000 0.129Â±0.000 0.114Â±0.000
MSC2D 0.997Â±0.001
0.991Â±0.001 0.997Â±0.001 0.994Â±0.001 0.994Â±0.001 0.681Â±0.036
0.727Â±0.024 0.701Â±0.029 0.159Â±0.031 0.144Â±0.032
MSCTM 0.670Â±0.000
0.712Â±0.000 0.705Â±0.000 0.622Â±0.000 0.573Â±0.000 0.441Â±0.000
0.652Â±0.000 0.514Â±0.000 0.187Â±0.000 0.173Â±0.000
MFLVC 0.998Â±0.000
0.992Â±0.000 0.998Â±0.000 0.995Â±0.000 0.995Â±0.000 0.274Â±0.000
0.687Â±0.000 0.274Â±0.000 0.228Â±0.000 0.215Â±0.000
MVCTM 0.659Â±0.000
0.718Â±0.000 0.705Â±0.000 0.640Â±0.000 0.590Â±0.000 0.443Â±0.000
0.619Â±0.000 0.509Â±0.000 0.111Â±0.000 0.095Â±0.000
TBGL 0.996Â±0.000
0.986Â±0.000 0.996Â±0.000 0.991Â±0.000 0.990Â±0.000 0.694Â±0.000
0.728Â±0.000 0.709Â±0.000 0.162Â±0.000 0.147Â±0.000
PANDA 0.998Â±0.000
0.993Â±0.000 0.998Â±0.000 0.996Â±0.000 0.995Â±0.000 0.737Â±0.000
0.755Â±0.000 0.745Â±0.000 0.203Â±0.000 0.640Â±0.000
00.2
0.00010.4
0.001ACC0.6
0.010.81
0.1
1
1 100.11000.01
0.001 1000
0.0001100001e-05
(
a)ğ›¼=1
00.2
0.00010.4
0.001ACC0.6
0.010.81
0.1
1
1 100.11000.01
0.001 1000
0.0001100001e-05 (
b)ğ›½=1
00.2
0.00010.4
0.001ACC0.6
0.010.8
100001
0.110001 100
10 1011000.1
0.01 10000.001100000.0001 (
c)ğ›¿=1
Figure 4: Parameter Analysis of ğ›¼,ğ›½, andğ›¿in terms of ACC
on NGs Datasets.
4.1 Experimental Setup
Datasets: The following six benchmark multi-view datasets of
varying scales are utilized to demonstrate the superiority of the pro-
posed PANDA model: 3sources, NGs, BBCsport, Wiki-feature
andALOI-100. A comprehensive summary of the statistical infor-
mation for these six datasets is available in Table 1.
Baselines: To validate the effectiveness of the proposed PANDA
model, we employed a set of representative SOTA affinity-induced
methods as benchmarks, including, AWP (2018, KDD) [37],MCGC
(2019, TIP) [62],DGF (2019, ICDM) [27],SFMC (2022, TPAMI)
Topological Manifold Euclidean structure0102030405060708090100ACC(%)
Hdigit
NGs
BBCsport
3sources
ALOI-100
Wiki-feature(
a) ACC
ACC NMI PUR F-score ARI
Metrics0.80.820.840.860.880.90.920.940.960.981Performance0.982
0.9410.982
0.964
0.9550.968
0.9020.968
0.937
0.921
Topological Manifold
Euclidean Structure (
b) NGs
ACC NMI PUR F-score ARI
Metrics00.10.20.30.40.50.60.70.80.91Performance0.813
0.7440.849
0.808
0.741
0.515
0.4230.619
0.525
0.287
Topological Manifold
Euclidean Structure (
c) BBCsport
Figure 5: Ablation Experiments: (a) Topological vs. Euclidean
Structure; (b), (c) Comparison w/ and w/o TRS.
[26],MSC2D (2022, TKDE) [15],MSCTM (2022, NeurIPS) [18],
MFLVC (2022, CVPR) [55],MVCTM (2023, AAAI) [16],TBGL
(2023, TPAMI) [53].
Implementation Details: For the comparison methods, we con-
duct clustering experiments by adapting the source code provided
by the respective authors and fine-tune the parameters with the
guidance presented in the corresponding literature. For PANDA,
the hyperparameters ğ›¼,ğ›½, andğ›¾are set to 1 across all datasets. Ex-
periments with the shallow model are executed in MATLAB 2018a
on a system featuring a 3.70GHz i9-10900k CPU and 64GB RAM.
For the deep model, PyTorch 1.12 is employed on an RTX 4060
 
926KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhibin Gu, Zhendong Li, & Songhe Feng.
-25 -20 -15 -10 -5 0 5 10 15-15-10-505101520
1
2
3
4
5
(a) The 1st view Z(1)
-20 -15 -10 -5 0 5 10 15-15-10-5051015
1
2
3
4
5(b) The 2nd view Z(2)
-15 -10 -5 0 5 10 15 20-20-15-10-5051015
1
2
3
4
5(c) The 3rd view Z(3)
-20 -15 -10 -5 0 5 10-20-15-10-50510
1
2
3
4
5 (d) The consensus Z
-25 -20 -15 -10 -5 0 5 10 15-15-10-505101520
1
2
3
4
5
(e) The 1st view A(1)
-15 -10 -5 0 5 10 15-15-10-50510
1
2
3
4
5(f) The 2nd view A(2)
-15 -10 -5 0 5 10 15-20-15-10-505101520
1
2
3
4
5(g) The 3rd view A(3)
-35 -30 -25 -20 -15 -10 -5 0 5 10 15-20-15-10-505101520
1
2
3
4
5 (h) The consensus A
-30 -20 -10 0 10 20 30 40-30-20-10010203040
1
2
3
4
5 (i) The structured graph U
Figure 6: t-SNE Visualization of Clustering Structures among Affinity Matrices from Different Stages of the PANDA Model.
GPU. Each experiment is repeated 10 times, and the final results
are reported as the average and standard deviation of these runs.
Evaluation metrics: We employed five evaluation metrics, in-
cluding ACC, NMI, PUR, F-score, and ARI, to assess the clustering
performance. The ARI ranges from [-1.1], while the other metrics
fall within the [0, 1] range. Higher values of these metrics indicate
better cluster performance.
4.2 Performance Comparison
Table 2 illustrates the clustering performance of the PANDA model
and 9 SOTA models, with the best results highlighted in redand
the second-best results in blue. From these findings, two notable
observations can be derived:
(1) The proposed PANDA model exhibits competitive clustering
performance across all datasets when compared to other SOTA
methods. Taking the BBCsport dataset as an example, the PANDA
model demonstrates a remarkable performance advantage over the
second-best model (i.e., MVCTM) with improvements of 6.1%, 14.4%,
6.0%, 11.9%, and 16.4% in terms of ACC, NMI, PUR, F-score, and ARI,
respectively. These findings highlight the efficacy of the proposed
PANDA model, which adeptly integrates topological manifold ex-
traction, robust low-rank 3-D tensor construction, and structured
graph learning into a cohesive and unified framework. This seam-
less integration enables the PANDA model to effectively unearth
valuable information from multi-view data, leading to a significant
enhancement in clustering performance.
(2) Compared to the representative deep learning-based model
MFLVC, our proposed PANDA model consistently outperforms
MFLVC across all evaluation metrics on all datasets. As an illus-
tration, within the context of the 3sources dataset, our PANDA
model demonstrates superior performance compared to the MFLVC
models 11.8%, 14.6%, 12.4%, 10.3%, and 16.4% across the evaluationmetrics ACC, NMI, PUR, F-score, and ARI, respectively. This ob-
servation demonstrates that our approach effectively leverages
the valuable information inherent in multi-view data, resulting in
improved clustering performance.
4.3 Parameters Sensitivity Analysis
In this subsection, we conduct parameter analysis experiments to
investigate the sensitivity of the PANDA model to three parameters
(i.e.,ğ›¼,ğ›½, andğ›¿). Specifically, we selected a pair of parameters from
the three variables to serve as independent variables and employed
a cross-validation strategy to systematically adjust these param-
eters within predefined ranges. The adjustment ranges for ğ›¼,ğ›½,
andğ›¿are [0.0001, 1000], [0.0001, 1000], and [0.00001, 1], respec-
tively. Fig. 4 shows the evolution of the clustering performance, as
measured by the accuracy (ACC), of the proposed PANDA model
on NGs dataset, with respect to variations in parameter values.
As a result, the experiment results for PANDA exhibit remarkable
stability even when subjected to extensive parameter variations.
This outcome highlights the algorithmâ€™s insensitivity to signifi-
cant parameter perturbations, emphasizing its capacity to maintain
consistent performance across diverse parameter ranges.
4.4 Ablation Study
We conduct ablation experiments to investigate the impact of Topo-
logical Manifold Extraction (TME) and Tensorial Refined Sigmoid
Rank (TRS) on the overall model performance. We individually
remove modules TME and TRS from the PANDA model and record
the degraded clustering performance in Figs. 5a- 5c. It can be ob-
served that regardless of removing TME or TRS, the performance of
the PANDA model degrades to a certain extent. This indicates that
the PANDA model effectively leverages the topological correlations
among samples and the differential information of tensor singular
values, thereby enhancing clustering performance.
 
927Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 5 10 15 20
Number of  Iterations 5.30555.3065.30655.3075.30755.3085.30855.3095.3095Obj104
(a) 3sources
0 5 10 15 20
Number of  Iterations 1.60651.6071.60751.6081.60851.609Obj105 (b) NGs
0 5 10 15 20
Number of  Iterations 1.15581.1561.15621.15641.15661.15681.157Obj105 (c) BBCSport
Figure 7: Convergence Curves of the PANDA Model.
4.5 Visual Analysis of the PANDA Model
To facilitate a comprehensive and visually informative analysis of
the iterative enhancements within each stage of the clustering pro-
cess, we present a visualization of the affinity matrices at different
levels on the NGs dataset. This visualization includes the view-
specific Euclidean structures, the extracted topological manifolds,
and the ultimate structured affinity matrix, as depicted in Fig. 6.
Specifically, (a), (b), (c), and (d) depict the original view-specific Eu-
clidean structures and the consensus Euclidean structure. Similarly,
(e), (f), (g), and (h) show the corresponding extracted topological
manifolds. Notably, (f) highlights the structured affinity matrix
learned by the PANDA model. Such an approach provides a pro-
found insight into the progressive refinements employed by the
PANDA model during the clustering process. Based on the analysis
of Fig.4, the following two significant conclusions can be derived:
(1) Compared to the multiple Euclidean structures, the topolog-
ical manifold structures extracted by our PANDA model exhibit
significantly clearer clustering patterns at both the individual view
level and the consensus level. This finding indicates that leveraging
topological relevance, as opposed to Euclidean structures, can pro-
vide a more accurate description of the affinity relationships among
data points, thereby facilitating improved clustering performance.
(2) At the level of consensus affinity matrices, the consensus Eu-
clidean structure Zis found to be inferior to the consensus manifold
structure A, while the latterâ€™s structure is not as distinct as that
of the structured affinity matrix U. This progressive phenomenon
aligns precisely with our initial expectations. It is the exploration
of topological connectivity that enables the consensus topological
structure to demonstrate superior connectivity compared to the Eu-
clidean structure. Moreover, it is the interplay among the extraction
of manifold structures, the exploration of tensor prior information,
and the collaborative imposition of structural constraints that leads
to the successful acquisition of high-quality affinity matrices.
4.6 Convergence Behavior
In the end, we illustrate the convergence property of optimization
algorithm by plotting the convergence curves in Fig. 7. The hori-
zontal axis represents the number of iterations, and the vertical axis
represents the objective value. It is evident from the plots that, as the
number of iterations increases, the value of the objective function
decreases sharply during the initial iterations, and the optimiza-
tion algorithm achieves stability within no more than 10 iterations
across all datasets. These results demonstrate the effectiveness of
the algorithm in minimizing the objective function.5 Conclusion
This paper proposes a novel TMVC model, named PANDA, which
enhances clustering performance by exploring topological rela-
tionships among data points and significant differences in tensor
singular values. Specifically, the PANDA model extracts topological
structures from the Euclidean graph to efficiently capture the simi-
larity of data points. Additionally, we develop an refined sigmoid
function, serving as a notably tighter approximation for tensor rank
minimization. This allows for the exploration of significant infor-
mation in different singular values of tensor data. Experimental
results on six benchmark datasets demonstrate the effectiveness of
our method in multi-view clustering compared to state-of-the-art
methods.
Acknowledgments
This work was supported by the Beijing Natural Science Foundation
(No. 4242046) and the Fundamental Research Funds for the Central
Universities (No. 2022JBZY019).
References
[1]Dimitri Bertsekas. 1997. Nonlinear Programming. Journal of the Operational
Research Society 48, 3 (1997), 334â€“334.
[2]Jie Chen, Hua Mao, Wai Lok Woo, and Xi Peng. 2023. Deep Multiview Clustering
by Contrasting Cluster Assignments. In 2023 IEEE/CVF International Conference
on Computer Vision (ICCV). 16706â€“16715.
[3]Man-Sheng Chen, Chang-Dong Wang, Dong Huang, Jian-Huang Lai, and Philip S.
Yu. 2022. Efficient Orthogonal Multi-View Subspace Clustering. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
127â€“135.
[4]Man-Sheng Chen, Chang-Dong Wang, and Jian-Huang Lai. 2023. Low-Rank
Tensor Based Proximity Learning for Multi-View Clustering. IEEE Transactions
on Knowledge and Data Engineering 35, 5 (2023), 5076â€“5090.
[5]Yongyong Chen, Xiaolin Xiao, and Yicong Zhou. 2020. Jointly Learning Kernel
Representation Tensor and Affinity Matrix for Multi-View Clustering. IEEE
Transactions on Multimedia 22, 8 (2020), 1985â€“1997.
[6]Tao Pham Dinh and Hoai An Le Thi. 1997. Convex analysis approach to d.c.
programming: Theory, Algorithm and Applications. (1997).
[7]Zhibin Dong, Siwei Wang, Jiaqi Jin, Xinwang Liu, and En Zhu. 2023. Cross-view
Topology Based Consistent and Complementary Information for Deep Multi-
view Clustering. In 2023 IEEE/CVF International Conference on Computer Vision
(ICCV). 19383â€“19394.
[8]K Fan. 1950. On a Theorem of Weyl Concerning Eigenvalues of Linear Transfor-
mations: II. Proceedings of the National Academy of Sciences of the United States
of America 36, 1 (1950), 31â€“35.
[9]Lele Fu, Zhaoliang Chen, Yongyong Chen, and Shiping Wang. 2022. Unified
Low-Rank Tensor Learning and Spectral Embedding for Multi-View Subspace
Clustering. IEEE Transactions on Multimedia (2022), 1â€“14.
[10] Jipeng Guo, Yanfeng Sun, Junbin Gao, Yongli Hu, and Baocai Yin. 2023. Log-
arithmic Schatten- ğ‘p Norm Minimization for Tensorial Multi-View Subspace
Clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 3
(2023), 3396â€“3410.
[11] Junlin Hu, Jiwen Lu, and Yap-Peng Tan. 2018. Sharable and Individual Multi-View
Metric Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence
40, 9 (2018), 2281â€“2288. https://doi.org/10.1109/TPAMI.2017.2749576
[12] Aiping Huang, Tiesong Zhao, and Chia-Wen Lin. 2020. Multi-View Data Fusion
Oriented Clustering via Nuclear Norm Minimization. IEEE Transactions on Image
Processing 29 (2020), 9600â€“9613.
[13] Dong Huang, Chang-Dong Wang, and Jian-Huang Lai. 2023. Fast Multi-view
Clustering via Ensembles: Towards Scalability, Superiority, and Simplicity. IEEE
Transactions on Knowledge and Data Engineering (2023), 1â€“16.
[14] Jin Huang, Feiping Nie, and Heng Huang. 2015. A New Simplex Sparse Learning
Model to Measure Data Similarity for Clustering. In Proceedings of the 24th
International Conference on Artificial Intelligence. 3569â€“3575.
[15] Shudong Huang, Yixi Liu, Ivor W. Tsang, Zenglin Xu, and Jiancheng Lv. 2022.
Multi-View Subspace Clustering by Joint Measuring of Consistency and Diversity.
IEEE Transactions on Knowledge and Data Engineering (2022), 1â€“12. https://doi.
org/10.1109/TKDE.2022.3199587
[16] Shudong Huang, Ivor Tsang, Zenglin Xu, Jiancheng Lv, and Quan-Hui Liu. 2022.
Multi-View Clustering on Topological Manifold. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 36. 6944â€“6951.
 
928KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhibin Gu, Zhendong Li, & Songhe Feng.
[17] Shudong Huang, Ivor W. Tsang, Zenglin Xu, and Jiancheng Lv. 2022. CGDD: Mul-
tiview Graph Clustering via Cross-Graph Diversity Detection. IEEE Transactions
on Neural Networks and Learning Systems (2022), 1â€“14.
[18] Shudong Huang, Hongjie Wu, Yazhou Ren, Ivor Tsang, Zenglin Xu, Wentao Feng,
and Jiancheng Lv. 2022. Multi-view Subspace Clustering on Topological Manifold.
InProceedings of the Neural Information Processing Systems. 1â€“12.
[19] Xiaodong Jia, Xiao-Yuan Jing, Xiaoke Zhu, Songcan Chen, Bo Du, Ziyun Cai,
Zhenyu He, and Dong Yue. 2021. Semi-Supervised Multi-View Deep Discriminant
Representation Learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence 43, 7 (2021), 2496â€“2509.
[20] Yuheng Jia, Hui Liu, Junhui Hou, Sam Kwong, and Qingfu Zhang. 2021. Multi-
View Spectral Clustering Tailored Tensor Low-Rank Representation. IEEE Trans-
actions on Circuits and Systems for Video Technology 31, 12 (2021), 4784â€“4797.
[21] Guangqi Jiang, Jinjia Peng, Huibing Wang, Zetian Mi, and Xianping Fu. 2022.
Tensorial Multi-View Clustering via Low-Rank Constrained High-Order Graph
Learning. IEEE Transactions on Circuits and Systems for Video Technology 32, 8
(2022), 5307â€“5318.
[22] Zhao Kang, Wangtao Zhou, Zhitong Zhao, Junming Shao, Meng Han, and Zenglin
Xu. 2020. Large-Scale Multi-View Subspace Clustering in Linear Time. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence. 4412â€“4419.
[23] Majid Komeili, Narges Armanfard, and Dimitrios Hatzinakos. 2021. Multiview
Feature Selection for Single-View Classification. IEEE Transactions on Pattern
Analysis and Machine Intelligence 43, 10 (2021), 3573â€“3586.
[24] Lusi Li and Haibo He. 2022. Bipartite Graph Based Multi-View Clustering. IEEE
Transactions on Knowledge and Data Engineering 34, 7 (2022), 3111â€“3125.
[25] Ruihuang Li, Changqing Zhang, Qinghua Hu, Pengfei Zhu, and Zheng Wang.
2019. Flexible Multi-View Representation Learning for Subspace Clustering. In
Proceedings of the 28th International Joint Conference on Artificial Intelligence.
2916â€“2922.
[26] Xuelong Li, Han Zhang, Rong Wang, and Feiping Nie. 2022. Multiview Clustering:
A Scalable and Parameter-Free Bipartite Graph Fusion Method. IEEE Transactions
on Pattern Analysis and Machine Intelligence 44, 1 (2022), 330â€“344.
[27] Youwei Liang, Dong Huang, and Chang-Dong Wang. 2019. Consistency Meets
Inconsistency: A Unified Graph Learning Framework for Multi-view Clustering.
In2019 IEEE International Conference on Data Mining (ICDM). 1204â€“1209.
[28] Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. 2013.
Robust Recovery of Subspace Structures by Low-Rank Representation. IEEE
Transactions on Pattern Analysis and Machine Intelligence 35, 1 (2013), 171â€“184.
[29] Jiyuan Liu, Xinwang Liu, Yuexiang Yang, Qing Liao, and Yuanqing Xia. 2023.
Contrastive Multi-View Kernel Learning. IEEE Transactions on Pattern Analysis
and Machine Intelligence 45, 8 (2023), 9552â€“9566.
[30] Xinwang Liu. 2023. SimpleMKKM: Simple Multiple Kernel K-Means. IEEE
Transactions on Pattern Analysis and Machine Intelligence 45, 4 (2023), 5174â€“5186.
[31] Xinwang Liu, Miaomiao Li, Chang Tang, Jingyuan Xia, Jian Xiong, Li Liu, Marius
Kloft, and En Zhu. 2021. Efficient and Effective Regularized Incomplete Multi-
View Clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence
43, 8 (2021), 2634â€“2646.
[32] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng
Yan. 2016. Tensor Robust Principal Component Analysis: Exact Recovery of
Corrupted Low-Rank Tensors via Convex Optimization. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). 5249â€“5257.
[33] Zhoumin Lu, Feiping Nie, Rong Wang, and Xuelong Li. 2023. A Differentiable
Perspective for Multi-View Spectral Clustering With Flexible Extension. IEEE
Transactions on Pattern Analysis and Machine Intelligence 45, 6 (2023), 7087â€“7098.
[34] HÃ  Quang Minh, Loris Bazzani, and Vittorio Murino. 2014. A Unifying Framework
in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization
and Co-Regularized Multi-view Learning. Journal of Machine Learning Research
17 (2014), 1â€“72.
[35] Feiping Nie, Guohao Cai, and Xuelong Li. 2017. Multi-View Clustering and
Semi-Supervised Classification with Adaptive Neighbours. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelligence. 2408â€“2414.
[36] Feiping Nie, Jing Li, and Xuelong Li. 2017. Self-Weighted Multiview Clustering
with Multiple Graphs. In Proceedings of the 26th International Joint Conference on
Artificial Intelligence. 2564â€“2570.
[37] Feiping Nie, Lai Tian, and Xuelong Li. 2018. Multiview Clustering via Adap-
tively Weighted Procrustes. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. 2022â€“2030.
[38] Feiping Nie, Xiaoqian Wang, and Heng Huang. 2014. Clustering and Projected
Clustering with Adaptive Neighbors. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 977â€“986.
[39] ErLin Pan and Zhao Kang. 2021. Multi-view Contrastive Graph Clustering. In
Advances in Neural Information Processing Systems, Vol. 34. 2148â€“2159.
[40] Xi Peng, Zhenyu Huang, Jiancheng Lv, Hongyuan Zhu, and Joey Tianyi Zhou.
2019. COMIC: Multi-view Clustering Without Parameter Selection. In Proceedings
of the 36th International Conference on Machine Learning. 5092â€“5101.
[41] Wei Shen, Yang Yang, and Yinan Liu. 2022. Multi-View Clustering for Open
Knowledge Base Canonicalization. In Proceedings of the 28th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining. 1578â€“1588.[42] Shiliang Sun, Wenbo Dong, and Qiuyang Liu. 2021. Multi-View Representation
Learning With Deep Gaussian Processes. IEEE Transactions on Pattern Analysis
and Machine Intelligence 43, 12 (2021), 4453â€“4468.
[43] Yuze Tan, Yixi Liu, Shudong Huang, Wentao Feng, and Jiancheng Lv. 2023. Sample-
Level Multi-View Graph Clustering. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 23966â€“23975.
[44] Chang Tang, Zhenglai Li, Jun Wang, Xinwang Liu, Wei Zhang, and En Zhu.
2023. Unified One-Step Multi-View Spectral Clustering. IEEE Transactions on
Knowledge and Data Engineering 35, 6 (2023), 6449â€“6460.
[45] Chang Tang, Xinwang Liu, Xinzhong Zhu, En Zhu, Zhigang Luo, Lizhe Wang,
and Wen Gao. 2020. CGD: Multi-View Clustering via Cross-View Graph Diffusion.
InAAAI Conference on Artificial Intelligence. 5924â€“5931.
[46] Xudong Tian, Zhizhong Zhang, Cong Wang, Wensheng Zhang, Yanyun Qu,
Lizhuang Ma, Zongze Wu, Yuan Xie, and Dacheng Tao. 2023. Variational Distilla-
tion for Multi-View Learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence (2023), 1â€“18.
[47] Daniel J. Trosten, Sigurd LÃ¸kse, Robert Jenssen, and Michael Kampffmeyer. 2021.
Reconsidering Representation Alignment for Multi-view Clustering. In 2021
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1255â€“
1265.
[48] Daniel J. Trosten, Sigurd LÃ¸kse, Robert Jenssen, and Michael C. Kampffmeyer.
2023. On the Effects of Self-supervision and Contrastive Alignment in Deep
Multi-view Clustering. In 2023 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR). 23976â€“23985.
[49] Hao Wang, Yan Yang, and Bing Liu. 2020. GMC: Graph-Based Multi-View Cluster-
ing.IEEE Transactions on Knowledge and Data Engineering 32, 6 (2020), 1116â€“1129.
[50] Qi Wang, Mulin Chen, and Xuelong Li. 2017. Quantifying and Detecting Collec-
tive Motion by Manifold Learning. In AAAI Conference on Artificial Intelligence.
1â€“13.
[51] Jianlong Wu, Zhouchen Lin, and Hongbin Zha. 2019. Essential Tensor Learning
for Multi-View Spectral Clustering. IEEE Transactions on Image Processing 28, 12
(2019), 5910â€“5922.
[52] Jianlong Wu, Xingyu Xie, Liqiang Nie, Zhouchen Lin, and Hongbin Zha. 2020.
Unified Graph and Low-Rank Tensor Learning for Multi-View Clustering. In
Proceedings of the AAAI Conference on Artificial Intelligence. 6388â€“6395.
[53] Wei Xia, Quanxue Gao, Qianqian Wang, Xinbo Gao, Chris Ding, and Dacheng
Tao. 2023. Tensorized Bipartite Graph Learning for Multi-View Clustering. IEEE
Transactions on Pattern Analysis and Machine Intelligence 45, 4 (2023), 5187â€“5202.
[54] Yuan Xie, Dacheng Tao, Wensheng Zhang, Yan Liu, Lei Zhang, and Yanyun Qu.
2018. On Unifying Multi-View Self-Representations for Clustering by Tensor
Multi-Rank Minimization. International Journal of Computer Vision 126 (11 2018),
1157â€“1179.
[55] Jie Xu, Huayi Tang, Yazhou Ren, Liang Peng, Xiao lan Zhu, and Lifang He.
2021. Multi-level Feature Learning for Contrastive Multi-view Clustering. In
2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
16030â€“16039.
[56] Ben Yang, Xuetao Zhang, Feiping Nie, Fei Wang, Weizhong Yu, and Rong Wang.
2021. Fast Multi-View Clustering via Nonnegative and Orthogonal Factorization.
IEEE Transactions on Image Processing 30 (2021), 2575â€“2586.
[57] Haizhou Yang, Quanxue Gao, Wei Xia, Ming Yang, and Xinbo Gao. 2022. Mul-
tiview Spectral Clustering With Bipartite Graph. IEEE Transactions on Image
Processing 31 (2022), 3591â€“3605.
[58] Jufeng Yang, Jie Liang, Kai Wang, Paul L. Rosin, and Ming-Hsuan Yang. 2020.
Subspace Clustering via Good Neighbors. IEEE Transactions on Pattern Analysis
and Machine Intelligence 42, 6 (2020), 1537â€“1544.
[59] Mouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. 2023.
Robust Multi-View Clustering With Incomplete Information. IEEE Transactions
on Pattern Analysis and Machine Intelligence 45, 1 (2023), 1055â€“1069. https:
//doi.org/10.1109/TPAMI.2022.3155499
[60] Zhiyong Yang, Qianqian Xu, Weigang Zhang, Xiaochun Cao, and Qingming
Huang. 2019. Split Multiplicative Multi-View Subspace Clustering. IEEE Transac-
tions on Image Processing 28, 10 (2019), 5147â€“5160.
[61] Ming Yin, Junbin Gao, and Zhouchen Lin. 2016. Laplacian Regularized Low-Rank
Representation and Its Applications. IEEE Transactions on Pattern Analysis and
Machine Intelligence 38, 3 (2016), 504â€“517.
[62] Kun Zhan, Feiping Nie, Jing Wang, and Yi Yang. 2019. Multiview Consensus
Graph Clustering. IEEE Transactions on Image Processing 28, 3 (2019), 1261â€“1270.
[63] Changqing Zhang, Huazhu Fu, Si Liu, Guangcan Liu, and Xiaochun Cao. 2015.
Low-Rank Tensor Constrained Multiview Subspace Clustering. In Proceedings of
the 2015 IEEE International Conference on Computer Vision (ICCV). 1582â€“1590.
[64] Han Zhang, Feiping Nie, and Xuelong Li. 2023. Large-Scale Clustering With
Structured Optimal Bipartite Graph. IEEE Transactions on Pattern Analysis and
Machine Intelligence 45, 8 (2023), 9950â€“9963.
[65] Zhenyue Zhang, Jing Wang, and Hongyuan Zha. 2012. Adaptive Manifold Learn-
ing. IEEE Transactions on Pattern Analysis and Machine Intelligence 34, 2 (2012),
253â€“265.
[66] Handong Zhao, Zhengming Ding, and Yun Fu. 2017. Multi-View Clustering via
Deep Matrix Factorization. In Proceedings of the Thirty-First AAAI Conference on
 
929Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Artificial Intelligence. 2921â€“2927.
[67] Guo Zhong and Chi-Man Pun. 2022. Improved Normalized Cut for Multi-View
Clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 12
(2022), 10244â€“10251.
[68] Pan Zhou, Canyi Lu, Jiashi Feng, Zhouchen Lin, and Shuicheng Yan. 2021. Tensor
Low-Rank Representation for Data Recovery and Clustering. IEEE Transactions
on Pattern Analysis and Machine Intelligence 43, 5 (2021), 1718â€“1732.
A Model Optimization
In this part, an iterative scheme based on the Alternating Direction
Method of Multipliers (ADMM) is proposed to optimize the objec-
tive function. To ensure the separability of each variable in problem
(9), an auxiliary variable Jis firstly introduced. This allows us to
derive the augmented Lagrangian function for the problem (9) as
follows:
min
{A(ğ‘£)},U,
{ğœ‰(ğ‘£)},P,J1
2ğ‘šâˆ‘ï¸
ğ‘£=1(ğ‘›âˆ‘ï¸
ğ‘–,ğ‘—,ğ‘˜=1Z(ğ‘£)
ğ‘—ğ‘˜ A(ğ‘£)
ğ‘–ğ‘—âˆšï¸ƒ
D(ğ‘£)
ğ‘—ğ‘—âˆ’A(ğ‘£)
ğ‘–ğ‘˜âˆšï¸ƒ
D(ğ‘£)
ğ‘˜ğ‘˜!2
+ğ›¼âˆ¥A(ğ‘£)âˆ’Iâˆ¥2
ğ¹
+ğœ‰(ğ‘£)âˆ¥Uâˆ’A(ğ‘£)âˆ¥2
ğ¹)
+ğ›½âˆ¥Jâˆ¥TRS+2ğœ†Tr(PâŠ¤LUP)+ğœŒ
2Aâˆ’J+Y
ğœŒ2
ğ¹
s.t. a(ğ‘£)
ğ‘–âŠ¤1=1, ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0, ğœ‰(ğ‘£)â‰¥0,
 u(ğ‘£)
ğ‘–âŠ¤1=1, ğ‘¢(ğ‘£)
ğ‘–ğ‘—â‰¥0,PâŠ¤P=I,A=J.
(10)
whereYâˆˆRğ‘›Ã—ğ‘šÃ—ğ‘›denotes the Lagrange multipliers, and ğœŒrep-
resents the corresponding penalty factor. Consequently, the opti-
mization of problem (10) can be further simplified into following
five sub-problems:
â€¢Solving Pwith fixed{A(ğ‘£)},U,{ğœ‰(ğ‘£)}, andJ. Since the vari-
ablePexclusively appears the rank constraint term in the PANDA
model, we can achieve the optimal value for Pby solving the fol-
lowing problem:
min
PTr(PâŠ¤eLUP),s.t. PâŠ¤P=I. (11)
The optimal solution Pis comprised of the eigenvectors corre-
sponding to the ğ‘smallest eigenvalues of eLU.
â€¢Solving {A(ğ‘£)}with fixed P,U,{ğœ‰(ğ‘£)}, andJ. In such a
scenario, we can optimize the following problem to obtain the
optimal solution for A(ğ‘£):
min
{A(ğ‘£)}1
2ğ‘šâˆ‘ï¸
ğ‘£=1(ğ‘›âˆ‘ï¸
ğ‘–,ğ‘—,ğ‘˜=1Z(ğ‘£)
ğ‘—ğ‘˜ A(ğ‘£)
ğ‘–ğ‘—âˆšï¸ƒ
D(ğ‘£)
ğ‘—ğ‘—âˆ’A(ğ‘£)
ğ‘–ğ‘˜âˆšï¸ƒ
D(ğ‘£)
ğ‘˜ğ‘˜!2
+ğ›¼A(ğ‘£)âˆ’I2
ğ¹
+ğœ‰(ğ‘£)Uâˆ’A(ğ‘£)2
ğ¹+ğœŒ
2A(ğ‘£)âˆ’J(ğ‘£)+Y(ğ‘£)
ğœŒ2
ğ¹)
s.t. a(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0.(12)Due to Eq. (12) is view-independent, it can be reformulated as
follows:
min
{a(ğ‘£)
ğ‘–} a(ğ‘£)
ğ‘–âŠ¤ Iâˆ’Dâˆ’1
2Z(ğ‘£)Dâˆ’1
2a(ğ‘£)
ğ‘–+ğ›¼a(ğ‘£)
ğ‘–âˆ’eğ‘–2
2
+ğœ‰(ğ‘£)uğ‘–âˆ’a(ğ‘£)
ğ‘–2
2+ğœŒ
2a(ğ‘£)
ğ‘–âˆ’j(ğ‘£)
ğ‘–+y(ğ‘£)
ğ‘–
ğœŒ2
ğ¹
s.t. a(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0.(13)
LetC= 1+ğ›¼+ğœ‰(ğ‘£)+ğœŒ
2Iâˆ’Dâˆ’1
2Z(ğ‘£)Dâˆ’1
2andd=2ğ›¼eğ‘–+
2ğœ‰(ğ‘£)uğ‘–+ğœŒj(ğ‘£)
ğ‘–âˆ’y(ğ‘£)
ğ‘–. By utilizing these definitions, Eq.(13) can be
expressed in a simplified form as follows:
min 
ğ’‚(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0 ğ’‚(ğ‘£)
ğ‘–âŠ¤Cğ’‚(ğ‘£)
ğ‘–âˆ’ ğ’‚(ğ‘£)
ğ‘–âŠ¤d.(14)
It is apparent that Eq. (14) constitutes a quadratic convex opti-
mization problem, which can be effectively solved using the classical
augmented Lagrangian multiplier (ALM) method [ 1]. In this regard,
we further introduce an auxiliary variable w, and the augmented
Lagrangian function of Eq. (14) can be expressed as follows:
min 
a(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0,w ğ’‚(ğ‘£)
ğ‘–âŠ¤Cwâˆ’ ğ’‚(ğ‘£)
ğ‘–âŠ¤d
+ğœƒ
2 a(ğ‘£)
ğ‘–âˆ’w+1
ğœƒew2
2,(15)
where ewandğœƒrepresent the penalty coefficient and parameter,
respectively, while a(ğ‘£)
ğ‘–andwcan be updated by an alternating
iterative strategy.
Specifically, for variable a(ğ‘£)
ğ‘–, the Lagrangian function corre-
sponding to Eq. (15) is given as follows:
 ğ’‚(ğ‘£)
ğ‘–âŠ¤Cw+ğœƒ
2 a(ğ‘£)
ğ‘–âˆ’w+1
ğœƒew2
2, (16)
By taking the derivative of Eq. (16) with respect to wand setting
the derivative to zero, we can derive the solution for Eq. (16) in
terms of w:
w=a(ğ‘£)
ğ‘–âˆ’1
ğœƒ CâŠ¤a(ğ‘£)
ğ‘–+ew(17)
For variable a(ğ‘£)
ğ‘–, we can readily derive its Lagrangian function
with respect to Eq. (17):
min 
a(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘(ğ‘£)
ğ‘–ğ‘—â‰¥0a(ğ‘£)
ğ‘–âˆ’w+1
ğœƒew+C(ğ‘£)wâˆ’d
ğœƒ2
2, (18)
The closed-form solution for Eq. (18) can be easily obtained using
the optimization algorithm proposed in [14].
â€¢Solving Uwith fixed{A(ğ‘£)},{ğœ‰(ğ‘£)},P, andJ. In such a sce-
nario, the optimization of Ucan be considered as the optimization
of the following problem:
min
U1
2ğ‘šâˆ‘ï¸
ğ‘£=1ğœ‰(ğ‘£)âˆ¥Uâˆ’A(ğ‘£)âˆ¥2
ğ¹+2ğœ†Tr(PâŠ¤LUP)
s.t.
ğ’–(ğ‘£)
ğ‘–âŠ¤
1=1,ğ‘¢ğ‘–ğ‘—â‰¥0,(19)
 
930KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhibin Gu, Zhendong Li, & Songhe Feng.
After algebraic transformation, the Eq. (19) can be rewritten as:
minuğ‘–ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘›âˆ‘ï¸
ğ‘—=1ğœ‰(ğ‘£)(ğ‘¢ğ‘–ğ‘—âˆ’ğ‘(ğ‘£)
ğ‘–ğ‘—)2+ğœ†ğ‘›âˆ‘ï¸
ğ‘—=1âˆ¥ğ’‘ğ‘–âˆ’ğ’‘ğ‘—âˆ¥2
2ğ‘¢ğ‘–ğ‘—
,
s.t.
ğ’–(ğ‘£)
ğ‘–âŠ¤
1=1,ğ‘¢ğ‘–ğ‘—â‰¥0,(20)
Denoteğ‘’ğ‘–ğ‘—=âˆ¥ğ’‘ğ‘–âˆ’ğ’‘ğ‘—âˆ¥2
2, whereğ‘’ğ‘–ğ‘—represents the ğ‘—-th element
inğ’†ğ‘–. The same notation is used for ğ’‚ğ‘–and ğ’–ğ‘–. Thus, the above
equation can be concisely expressed in a vectorized form:
min 
u(ğ‘£)
ğ‘–âŠ¤1=1,ğ‘¢(ğ‘£)
ğ‘–ğ‘—â‰¥0ğ’–ğ‘–âˆ’Ãğ‘š
ğ‘£=1ğœ‰(ğ‘£)ğ’‚(ğ‘£)
ğ‘–âˆ’ğœ†
2ğ’†ğ‘–Ãğ‘š
ğ‘£=1ğœ‰(ğ‘£)2
2(21)
â€¢Solving{ğœ‰(ğ‘£)}with fixed{A(ğ‘£)},U,P, and J. In this sce-
nario, solving the following problem leads to the optimal ğœ‰(ğ‘£):
ğœ‰(ğ‘£)=1
2âˆ¥A(ğ‘£)âˆ’Uâˆ¥2
ğ¹. (22)
â€¢Solving Jwith fixed{A(ğ‘£)},{ğœ‰(ğ‘£)},UandP. In this scenario,
solving the problem described below yields the optimal J:
min
Jğ›½
ğœŒâˆ¥Jâˆ¥TRS+1
2Jâˆ’ A+Y
ğœŒ2
ğ¹(23)
The problem (23) can be regarded as a tensorial refined sigmoid
rank minimization (TRSRM) problem, and it can be effectively re-
solved by leveraging the following theorem:
Theorem 2. Given tensor GâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3and its t-SVD operator
G=Oâˆ—Pâˆ—QâŠ¤. The problem of tensorial refined sigmoid rank
minimization can be formulated as follows:min
Jğœâˆ¥Jâˆ¥TRS+1
2âˆ¥Jâˆ’Gâˆ¥2
ğ¹, (24)
whose optimal solution for Jis as follows:
Jâˆ—=Oâˆ—ğ‘–ğ‘“ğ‘“ğ‘¡(Î˜ğ‘“,ğœ(Pğ‘“),[],3)âˆ—QâŠ¤, (25)
whereğ‘–ğ‘“ğ‘“ğ‘¡(Î˜ğ‘“,ğœ(P(ğ‘˜)
ğ‘“),[],3)is ağ‘“-diagonal tensor, and satisfies
the following equation:
Î˜ğ‘“,ğœ(P(ğ‘˜)
ğ‘“(ğ‘–ğ‘–))=min
ğ‘¥â‰¥01
2(ğ‘¥âˆ’Pğ‘˜
ğ‘“(ğ‘–ğ‘–)2)+ğœğ‘“(ğ‘¥), (26)
whereğ‘“(ğ‘¥)=ğ‘¥
ğ‘¥+ğ‘’ğ‘¥ğ‘(âˆ’ğ‘¥
ğ›¿).
This specific structure suggests the application of the difference
of convex (DC) programming [ 6]. Following the DC algorithm,
a closed-form solution can be obtained through iterative conver-
gence:
ğœ™ğ‘–ğ‘¡ğ‘’ğ‘Ÿ+1=
P(ğ‘˜)
ğ‘“(ğ‘–ğ‘–)âˆ’ğ›½ğœ•ğ‘“(ğœ™ğ‘–ğ‘¡ğ‘’ğ‘Ÿ)
ğœŒ
+(27)
whereğœ™=Î˜ğ‘“,ğ›½
ğœŒ(P(ğ‘˜)
ğ‘“(ğ‘–ğ‘–)),ğ‘“(ğ‘¥)=ğ‘¥
ğ‘¥+ğ‘’ğ‘¥ğ‘(âˆ’ğ‘¥
ğ›¿)andğ‘–ğ‘¡ğ‘’ğ‘Ÿ is the
number of iterations.
To conclude, the Lagrange multiplier and related penalty param-
eters can be updated in the following manner:
Y=Y+ğœŒ(Aâˆ’J),
ğœŒ=min(ğœ‡âˆ—ğœŒ,ğœŒğ‘šğ‘–ğ‘›).(28)
Up to this point, we have derived closed-form solutions for each
variable in the problem (9).
 
931