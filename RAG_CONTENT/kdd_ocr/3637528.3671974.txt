FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal
Healthcare Prediction
Muhao Xu
Institute of Information Science,
Beijing Jiaotong University
Beijing Key Laboratory of Advanced
Information Science and Network
Technology
Beijing, China
mhxu1998@bjtu.edu.cnZhenfeng Zhuâˆ—
Institute of Information Science,
Beijing Jiaotong University
Beijing Key Laboratory of Advanced
Information Science and Network
Technology
Beijing, China
zhfzhu@bjtu.edu.cnYouru Li
Institute of Information Science,
Beijing Jiaotong University
Beijing Key Laboratory of Advanced
Information Science and Network
Technology
Beijing, China
liyouru@bjtu.edu.cn
Shuai Zheng
Institute of Information Science,
Beijing Jiaotong University
Beijing Key Laboratory of Advanced
Information Science and Network
Technology
Beijing, China
zs1997@bjtu.edu.cnYawei Zhao
Kunlun He
Medical Big Data Research Center,
Chinese PLA General Hospital
Beijing, China
csyawei.zhao@gmail.com
kunlunhe@plagh.orgYao Zhao
Institute of Information Science,
Beijing Jiaotong University
Beijing Key Laboratory of Advanced
Information Science and Network
Technology
Beijing, China
yzhao@bjtu.edu.cn
ABSTRACT
Multimodal electronic health record (EHR) data can offer a holistic
assessment of a patientâ€™s health status, supporting various predic-
tive healthcare tasks. Recently, several studies have embraced the
multitask learning approach in the healthcare domain, exploiting
the inherent correlations among clinical tasks to predict multi-
ple outcomes simultaneously. However, existing methods necessi-
tate samples to possess complete labels for all tasks, which places
heavy demands on the data and restricts the flexibility of the model.
Meanwhile, within a multitask framework with multimodal in-
puts, how to comprehensively consider the information disparity
among modalities and among tasks still remains a challenging prob-
lem. To tackle these issues, a unified healthcare prediction model,
also named by FlexCare, is proposed to flexibly accommodate in-
complete multimodal inputs, promoting the adaption to multiple
healthcare tasks. The proposed model breaks the conventional par-
adigm of parallel multitask prediction by decomposing it into a
series of asynchronous single-task prediction. Specifically, a task-
agnostic multimodal information extraction module is presented
to capture decorrelated representations of diverse intra- and inter-
modality patterns. Taking full account of the information disparities
between different modalities and different tasks, we present a task-
guided hierarchical multimodal fusion module that integrates the
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671974refined modality-level representations into an individual patient-
level representation. Experimental results on multiple tasks from
MIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets demonstrate the
effectiveness of the proposed method. Additionally, further anal-
ysis underscores the feasibility and potential of employing such
a multitask strategy in the healthcare domain. The source code is
available at https://github.com/mhxu1998/FlexCare.
CCS CONCEPTS
â€¢Applied computing â†’Health informatics; â€¢Information
systemsâ†’Data mining.
KEYWORDS
electronic health record, healthcare prediction, multimodal data,
multitask learning
ACM Reference Format:
Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Yawei Zhao, Kunlun He,
and Yao Zhao. 2024. FlexCare: Leveraging Cross-Task Synergy for Flexible
Multimodal Healthcare Prediction. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671974
1 INTRODUCTION
To comprehensively assess a patientâ€™s health status, clinical prac-
tice often employs a variety of methods to capture diverse patient
information, resulting in multimodal EHR data that comprise both
structured and unstructured data. These diverse multimodal EHR
data can underpin multiple predictive tasks in the clinical, facilitat-
ing the identification of high-risk patients for early intervention.
For instance, time-series data such as vital signs and laboratory
test results are frequently utilized for clinical risk and outcome
prediction [ 21,35]. Medical images (e.g., X-rays and computerized
3610
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Muhao Xu et al.
(a) Patient Admission
Time In-hospital -mortality
Phenotyping/ReadmissionLength -of-stay/DecompensationDiagnosis
MultitaskTime -seriesMedical Image
Clinical Note
 Multimodal
(b) (c)
Figure 1: (a) Illustration of the multimodal data and the multi-
task predictions during a patientâ€™s admission; (b) The number
of samples from multiple tasks that depend on time-series
data in the MIMIC-IV dataset; (c) The number of samples
with different modality data in the MIMIC-IV dataset.
tomography scans), are instrumental in detecting, localizing, and
classifying diseases relevant to patients [ 9]. Compared to struc-
tured information, textual clinical notes furnish comprehensive in-
sights into a patientâ€™s medical history, symptoms, and the reasoning
for diagnoses, offering a more macroscopic and holistic perspec-
tive [ 16,24]. Considering the complementary nature of information
within multimodal EHR data, some previous methodologies have
been developed to integrate various modalities for enhancing the
accuracy of clinical event prediction [ 6,15,23,28,37]. Further-
more, some studies [ 1,5,40] leverage the inherent correlations
among clinical tasks, employing multitask learning approaches to
simultaneously predict multiple tasks.
However, current multitask models for healthcare prediction typ-
ically necessitate complete labels for all tasks [ 5,40]. Such demand
for data is exceedingly stringent, especially within the medical do-
main. Indeed, the requirements for EHR data manifest considerable
variation across different clinical tasks for the same patient, encom-
passing distinct time spans and modalities, as illustrated in Figure
1(a). For example, in-hospital-mortality prediction commonly relies
on time-series data from the first 48 hours after a patient is admitted
to the hospital, while disease diagnosis mandates the presence of
image modality, with no restrictions on the other modalities. Hence,
requiring identical input data and complete labeling for multitask
constitutes a significant waste of already scarce healthcare data.
The statistical information for the MIMIC-IV dataset [ 10â€“12] pre-
sented in Figure 1(b) and (c) illustrates that samples with all task
labels and samples with all modalities represent only a small frac-
tion of the total dataset. In summary, to overcome the limitations of
previous models that require completeness in data and labels, mul-
timodal multitask healthcare prediction models face the following
challenges:
Single -task
Mode lTask
Modality 1
Modality 2
Modality 3Conventional
Multi- task ModelÂ·Â·Â·
Flexible
Multi- task ModelÂ·Â·Â·
(a)Dataset Dataset Â·Â·Â·
(b) (c)Dataset 1 Dataset TTask 1 Task T Task 1 Task TFigure 2: (a) Single-task model; (b) Conventional multi-task
model; (c) Our proposed flexible multi-task model.
Challenge 1: How to develop a flexible model capable of
supporting multimodal inputs and adapting to various het-
erogeneous tasks, without requiring comprehensive labels
for each sample across all tasks? Flexibility here is manifested in
the model not requiring each sample to possess inputs for all modal-
ities and labels for all tasks. An intuitive approach is to deconstruct
parallel multitask simultaneous predictions into asynchronous mul-
tiple single-task predictions, where each sample corresponds to a
single task label, thus satisfying the data differences in terms of
time spans and modalities used between multiple tasks for the same
patient. As shown in Figure 2, the hallmark of a flexible multitask
model lies in employing a unified model to process multiple hetero-
geneous datasets, encompassing both heterogeneous multimodal
inputs and heterogeneous tasks. In the realm of universal natural
language and visual understanding, MT-DNN [ 20] and Unit [ 7]
have made attempts with a multitask approach using heteroge-
neous datasets. However, current studies [ 1,5,40] still lack such
consideration in the field of healthcare prediction.
Challenge 2: How to deal with the information disparities
among modalities and tasks comprehensively within a multi-
task framework? Three forms of information disparity need to be
considered in the unified framework: modality-modality disparity,
task-task disparity, and task-data disparity. Concretely, in the het-
erogeneous EHR data, different modalities may encapsulate distinct
aspects of a patientâ€™s health status. It is imperative that this infor-
mational disparity present among multimodal data be taken into
consideration. Furthermore, different clinical tasks focus on EHR
data from various time spans and modalities of the patient, posing
challenges to the construction of the multitask model. On the other
hand, due to the extensive sharing of modules in multitask mod-
els, negative interference may arise when inter-task correlations
are weak. In multimodal multitask models, the shared modules are
not only cross-task but also cross-modal, further complicating the
above issue.
By jointly considering the above issues, we propose a model
leveraging cross-task synergy for flexible multimodal healthcare
prediction (FlexCare). Our main contributions are summarized as
follows:
â€¢To the best of our knowledge, this work is the first attempt to
study a unified healthcare prediction model that flexibly supports
3611FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
multimodal inputs and adapts to healthcare multitask. It is em-
powered with multitasking capabilities in the form of multiple
single-task asynchronous predictions.
â€¢To comprehensively capture the information from various modal-
ity combination patterns, a task-agnostic multimodal information
extraction module is presented, with the covariance regulariza-
tion to decorrelate the different modality combination represen-
tations.
â€¢A task-guided hierarchical multimodal fusion module is designed
to learn adaptive representations for different tasks, by incorpo-
rating implicit task indication in the aggregation process from
modality-level to patient-level representation.
â€¢Extensive experiments conducted on MIMIC-IV dataset show that
our model achieves competitive results on multiple tasks. Besides,
further analysis demonstrates the feasibility and potential of
adopting such multitask strategy to construct a unified model in
the healthcare domain.
2 RELATED WORK
Multimodal learning for healthcare. In the quest for a thorough
comprehension of patient patterns to enhance the precision of clini-
cal event prediction, researchers have delved into the realm of multi-
modal learning utilizing healthcare data [ 4,19,23,31,34,38,39,41].
HAIM [ 28] leverages different pre-trained feature extraction models
to process multimodal inputs and obtains the overall representation
of the patient. Zhou et al. [ 42] proposes a transformer-based model
that processes multimodal EHR data in a unified manner. However,
the aforementioned methods lack consideration for incomplete
modalities, which limits the application scenarios of the models.
To handle the pervasive issue of missing modalities in clinical
practice, many researchers have developed models capable of either
imputing missing modalities or adapting to the absence of certain
modalities. MedFuse [ 6] is an LSTM-based fusion model that can
process uni-modal as well as multimodal input. Lee et al. [ 15] learns
the EHR data with missing modal by the modality-aware attention
with skip bottleneck. To overcome the limitations of traditional
parallel fusion approaches, MultiModN [ 30] sequentially inputs any
number or combination of modalities into a sequence of modality-
specific encoders and can skip over missing modalities. Compared
with the above method of ignoring missing modalities, M3Care [ 37]
imputes the information of the missing modalities in the latent space
from the similar neighbors of each patient. While these methods
are effective, when extending to a multitasking setting, the process
of handling multimodal information must take into account the
different focal points of various tasks.
Multitask learning for healthcare. Multitask learning, an effec-
tive method that enhances performance through the joint learning
of multiple related tasks, has been explored for use in healthcare
prediction. In the healthcare domain, the term "tasks" has different
definitions. Several studies [ 18,29] treat the mortality prediction
of different patient cohorts as multiple tasks. Some studies intro-
duce auxiliary tasks, such as time series reconstruction [ 36] and
prediction based on unimodal data [ 33], to enhance the modelâ€™s
representational capacity and downstream task performance. Other
studies predict multiple clinical tasks simultaneously, such as risk
prediction [ 1,5,27,32] and disease diagnosis [ 13,25]. GenHPF [ 8]converts EHRs into hierarchical textual representations and offers
a solid framework for multi-task and multi-source learning, but it
can only handle EHRs in text form. Recently, UniMed [ 40] sequen-
tially predict four medical tasks based on multimodal EHR data,
utilizing the time-progressive correlation between tasks. However,
these methods require samples to have labels for all tasks, imposing
stricter demands on the already scarce medical data.
3 PROBLEM FORMULATION
In this section, we present the problem formulation and symbol
notation used throughout the paper.
Definition 1 (Patient multitask data). The datasets for a set of
tasks are denoted as {Dğœ}ğ‘‡
ğœ=1, whereğ‘‡is the number of tasks. Thus,
the corresponding training set of the ğœ-th task is represented as:
Dğœ=n
(X(ğ‘›)
ğœ,ğ‘¦(ğ‘›)
ğœ)oğ‘ğœ
ğ‘›=1, whereğ‘ğœis the number of samples, X(ğ‘›)
ğœ
andğ‘¦(ğ‘›)
ğœâˆˆYğœare the multimodal input and ground truth of the
ğ‘›-th sample, respectively. Yğœis the set of labels for the ğœ-th task.
Definition 2 (Patient multimodal data). GivenM={ğ‘¡,ğ‘–,ğ‘›}a
set of modalities (i.e., time-series data, image, note), the input of
ğ‘›-th sample can be defined as: X(ğ‘›)
ğœ={X(ğ‘›),ğ‘š
ğœ}ğ‘šâˆˆM. Considering
the absence of some modalities, the incomplete input is X(ğ‘›)
ğœ=
{X(ğ‘›),ğ‘š
ğœ}ğ‘šâˆˆM(ğ‘›), whereM(ğ‘›)are the modalities actually present
in theğ‘›-th sample, andM(ğ‘›)âŠ†M . Note that|M(ğ‘›)|â‰¥1, because
at least one modality is present for each sample.
Definition 3 (Modality combination). The modality combination
set represents all patterns of unimodal or multimodal combination,
defined as:C=2M\âˆ…(i.e., all nonempty subsets of M). When
|M|=3, the number of modality combination set |C|=7.
Multitask prediction problem. Given the multimodal datasets
for different tasks, the objective is to learn a unified task-adaptive
Table 1: Notations used in this paper
Notation Definition
ğœ;ğ‘‡ Theğœ-th task; total number of tasks
Dğœ;ğ‘ğœ The dataset of the ğœ-th task; the number of samples
ğ‘š;M Modality; set of multimodal
ğ‘;C Modality combination; set of modality combination
X(ğ‘›)
ğœ Theğ‘›-th patientâ€™s data of the ğœ-th task
ğ‘¦(ğ‘›)
ğœ Ground truth of the target
Ë†ğ‘¦(ğ‘›)
ğœ Prediction result
Hğ‘šğœ Learned representations of modality ğ‘š
htaskğœ;Hcombğœ Learnable representation of task token and modality
combination tokens
ztaskğœ;Zcombğœ Learned representation of task token and modality
combination tokens through intra/inter-modality
encoder
Mğœ The mask that aids modality combination tokens in
capturing specific information
ğ¶ğœ The covariance regularization term of Zcombğœ
sğ‘ğœ The refined representation of modality combination
ğ‘
sğ‘
ğœ The patient-level representation of the patient
3612KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Muhao Xu et al.
â€¦
BioBERT 
EmbeddingIntra/Inter -modality Encoder
Linear 
EmbeddingPatch 
EmbeddingTask
Router
Modality
Router
Expert 1
Expert 3
Expert n -1
Expert nExpert 2Attention -based FusionTask 1
Task 2
Task 3
Task T -1
Task T
â€¦Task -agnostic 
Multimodal Information Extraction
CMasked 
Multi -head 
AttentionAdd & NormFeed 
ForwardAdd & Norm
Task token
Modality 
Combination Tokens
Time -series
InputImage 
InputNote
Input
Concatenation
Lï‚´
ï´MPoint -wise 
MultiplicationPoint -wise 
Addition
t
ï´x
i
ï´x
n
ï´x
cov
Q
K
V
task
ï´h
comb
ï´H
t
ï´H
i
ï´H
n
ï´H
task
ï´h
comb
ï´H
t
ï´H
i
ï´H
n
ï´H
task
ï´zTokens Decorrelation
 Top k
Task/Modality -aware MoEPatient -level 
Representation LearningTask -specific 
Prediction Heads
â€¦
C
comb
ï´ZTask -guided Hierarchical 
Multimodal Information Fusion
Figure 3: The framework of the FlexCare model. It consists of three modules: (a) Task-agnostic multimodal information
extraction; (b) Task-guided hierarchical multimodal fusion; (c) Task-specific prediction heads.
predictive function: Ë†ğ‘¦(ğ‘›)
ğœ=ğ‘“ğœƒ(X(ğ‘›)
ğœ,ğœ), whereğ‘“ğœƒ(Â·,Â·)is parameter-
ized byğœƒandË†ğ‘¦(ğ‘›)
ğœdenotes the corresponding prediction result of
theğœ-th task.
Besides, the necessary notations used in the paper are listed in
Table 1 for ease of understanding. Please note that in the Section
4 Methodology, we take a single patient as an example. Therefore,
we simplify X(ğ‘›)
ğœtoXğœto enhance the reading experience.
4 METHODOLOGY
4.1 Overview
The overall framework of the proposed model is shown in Figure 3,
which mainly consists of three components:
â€¢TheTask-agnostic multimodal information extraction mod-
ule leverages unimodal feature extractors and a unified multi-
modal encoder to learn a spectrum of modality combination
representations.
â€¢TheTask-guided hierarchical multimodal fusion module
achieves hierarchical fusion from modality-level to patient-level
through the task/modality-aware Mixture of Experts (MoE) and
an attention-based fusion mechanism.
â€¢TheTask-specific prediction heads are configured with indi-
vidual predictors for each task, making predictions for the current
task based on patient-level representation.
Due to the model being trained and tested in an asynchronous
multiple single-task paradigm, the following introduction will take
one task as an example.
4.2 Task-agnostic Multimodal Information
Extraction
To enhance the generalization capability of the model, the task-
agnostic multimodal information extraction module is deeply shared
across multiple tasks. The module maps raw data with differentdimensions and modalities into latent representations in a unified
space, and captures task information and modality-level informa-
tion through learnable task token and modality combination tokens
respectively.
4.2.1 Unimodal Information Extraction. Given the sample Xğœ
in theğœ-th task dataset and the unimodal representation extraction
modelğ‘“ğ‘š(Â·), the raw input of modality ğ‘šis converted into a series
of 1D tokens Hğ‘šğœâˆˆRğ‘ğ‘šÃ—ğ‘‘with the same dimension via:
Hğ‘š
ğœ=ğ‘“ğ‘š(Xğ‘š
ğœ)+pğ‘š, (1)
where pğ‘šâˆˆRğ‘ğ‘šÃ—ğ‘‘is a learned positional embedding added to
the tokens to retain positional information, ğ‘‘is the dimension
of the latent representation and ğ‘ğ‘šdenotes the number of to-
kens for modality ğ‘š. For the three modalities {ğ‘¡,ğ‘–,ğ‘›}, the unimodal
modelsğ‘“ğ‘š(Â·)are set as follows: a linear projection, patch projec-
tion [ 3] followed by a linear projection, and the pre-trained & frozen
BioBERT [14], respectively.
4.2.2 Intra/Inter-modality Encoder. To inject task-specific in-
formation into the model, thereby facilitating subsequent implicit
guidance for multimodal representation learning under specific
tasks, a learnable task token is allocated for each task category. For
the givenğœ-th task, its corresponding task token is represented as
htaskğœâˆˆRğ‘‘. Furthermore, in order to thoroughly capture the infor-
mation contained in individual intra-modality as well as various
inter-modality patterns, a set of learnable modality combination
tokens Hcombğœ={hğ‘ğœ}ğ‘âˆˆCis presented.
To realize the information extraction from intra- and inter-modality,
the task token, modality combination tokens, and tokens extracted
from each modality, are stacked along the token dimension to ob-
tain a multimodal sequence: H0ğœ=[htaskğœ,Hcombğœ,Hğ‘¡ğœ,Hğ‘–ğœ,Hğ‘›ğœ], where
H0ğœâˆˆRğ‘â„Ã—ğ‘‘andğ‘â„=1+|C|+ğ‘ğ‘¡+ğ‘ğ‘–+ğ‘ğ‘›. Subsequently, the
multimodal sequence is fed into the encoder, facilitating the in-
teraction of multimodal information and obtaining the output of
3613FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
multimodal fusion:
eHğ‘™
ğœ=LN(Hğ‘™âˆ’1
ğœ+M-MHSA(Hğ‘™âˆ’1
ğœ,Hğ‘™âˆ’1
ğœ,Hğ‘™âˆ’1
ğœ,Mğœ)),
Hğ‘™
ğœ=LN(eHğ‘™
ğœ+FFN(eHğ‘™
ğœ)),(2)
whereğ‘™=1,...,ğ¿ is the number of encoder layers. FFN(Â·)andLN(Â·)
refer to the feed-forward network and layer normalization, respec-
tively. M-MHSA(Â·,Â·,Â·,Â·)represents a specially designed masked
multi-head self-attention mechanism, defined as follows:
M-MHSA(Q,K,V,M)=Softmax(QWğ‘„(KWğ¾)T
âˆš
ğ‘‘+M)VWğ‘‰,(3)
where Q,K,Vrepresent the query, key, and value embedding, respec-
tively. Wğ‘„,Wğ¾,Wğ‘‰denote the weight matrices, ğ‘‘is the dimension
of the latent representation, and Mis the additional mask.
The additional mask MğœâˆˆRğ‘â„Ã—ğ‘â„enables modality combi-
nation tokens to precisely target information relevant to diverse
modality combination patterns. Specifically, for modality combina-
tion tokens and modality tokens, (i.e., 0<ğ‘–,ğ‘—<ğ‘â„), the mask is
denoted as:
Mğœ(ğ‘–,ğ‘—)=0, ğœ™(ğ‘—)âˆˆğœ™(ğ‘–)orğœ™(ğ‘—)=ğœ™(ğ‘–)
âˆ’âˆ,otherwise, (4)
whereğœ™:indexâ†¦â†’(M|C) defines a function that maps the token
to the modality ğ‘šor modality combination ğ‘it belongs to. ğœ™(ğ‘—)âˆˆ
ğœ™(ğ‘–)indicates that the token Hğœ,ğ‘—originates from modality ğ‘š, the
token Hğœ,ğ‘–=hğ‘ğœfrom modality combination ğ‘, andğ‘šâˆˆğ‘.ğœ™(ğ‘—)=
ğœ™(ğ‘–)indicates that the token Hğœ,ğ‘—and token Hğœ,ğ‘–is from the same
modalityğ‘šor modality combination ğ‘.
Given our intention for the multimodal information extraction
process to be task-agnostic, the task token exclusively aggregates in-
formation from other tokens unidirectionally, without transmitting
its own information to them. Consequently, for the mask associated
with the task token (i.e., ğ‘–=0and0â‰¤ğ‘—<ğ‘â„), we establish that
Mğœ(ğ‘–,ğ‘—)=0.
Throughğ¿layers of the encoder equipped with a specialized
attention mask, we have obtained representation enriched with
task-specific information ztaskğœ=htask,ğ¿
ğœ and various task-agnostic
modality combination representations Zcombğœ=Hcomb,ğ¿
ğœ .
4.2.3 Representation Decorrelation of Modality Combina-
tion. The modality combination tokens, due to the sharing of
partial modality information within the encoder, may result in
a phenomenon of feature redundancy. To decorrelate the differ-
ent embeddings of the modality combination tokens and prevent
them from encoding similar information, a token-level covariance
regularization method is proposed.
Unlike the previous method [ 2] that constrains the correlations
between different dimensions of the embeddings, our approach
computes the covariance matrix along the token dimension rather
than the feature dimension. The function to calculate the covariance
matrix Cov(Â·)and the token-level covariance regularization term
ğ¶ğœare defined as follows:
Cov(Z)=1
ğ‘‘âˆ’1ğ‘‘âˆ‘ï¸
ğ‘—=1(z:,ğ‘—âˆ’z)(z:,ğ‘—âˆ’z)T,where z=ğ‘‘âˆ‘ï¸
ğ‘—=1z:,ğ‘—,(5)ğ¶ğœ=1
(|C|âˆ’ 1)2âˆ‘ï¸
ğ‘–â‰ ğ‘—[Cov(Zcomb
ğœ)]2
ğ‘–,ğ‘—. (6)
The loss function encourages the off-diagonal elements of the
covariance matrix to approach 0, compelling the modality combina-
tion tokens to capture multimodal information that is uncorrelated
with one another:
Lğ‘ğ‘œğ‘£=1
ğ‘ğœğ‘ğœâˆ‘ï¸
ğ‘›=1ğ¶(ğ‘›)
ğœ (7)
4.3 Task-guided Hierarchical Multimodal
Fusion
To facilitate the adaptive representation learning for the specific
task, it is necessary to infuse implicit task information into task-
agnostic modality-level representations for their refinement. Guided
by the specific task, these representations are subsequently aggre-
gated into patient-level representations.
4.3.1 Task/Modality-aware Mixture of Experts .Within a mul-
titask multimodal framework, it is imperative to consider the infor-
mation disparities among modalities and tasks. Employing identical
network layers for different modalities across varied tasks may lead
to negative interference, hindering the individualized representa-
tion learning for specific downstream tasks.
Specifically, after acquiring the task token and various modality
combination tokens from the original embeddings, a task/modality-
aware Mixture-of-Experts module is employed for the refinement
of multimodal representations in the context of a specific task.
The module obtains the output sğ‘ğœâˆˆRğ‘‘for the input token zğ‘ğœby
weighted average of the selected ğ‘˜experts from a total of ğ‘ğ‘’, which
can be formulated as:
sğ‘
ğœ=ğ‘ğ‘’âˆ‘ï¸
ğ‘–=1ğ‘…(zğ‘
ğœ,ztask
ğœ)ğ‘–ğ¸ğ‘–(zc
ğœ), (8)
whereğ¸ğ‘–(Â·)stands for the feature representations produced from
theğ‘–-th expert network (i.e., FFN). The router ğ‘…(Â·,Â·)concurrently
receives implicit task directives and modality information, selecting
ğ‘˜experts for the current token:
ğ‘…(zğ‘
ğœ,ztask
ğœ)=Softmax(TopK(zğ‘
ğœWğ‘…
1+ztask
ğœWğ‘…
2,ğ‘˜)), (9)
TopK(v,ğ‘˜)ğ‘–=vğ‘–,ifvğ‘–is within the top-k elements of v
âˆ’âˆ,otherwise,
(10)
where Wğ‘…
1âˆˆRğ‘‘Ã—ğ‘ğ‘’andWğ‘…
2âˆˆRğ‘‘Ã—ğ‘ğ‘’are learnable projectors
for the modality combination token and task token, respectively.
To mitigate the adverse impacts of imbalanced loading, we have
incorporated regularization terms aimed at balancing the distri-
bution of expert assignments, following the design and default
hyperparameters in previous work [26].
4.3.2 Patient-level Representation Learning. Currently, the
representations for various modality combinations under a specific
task have been obtained. The next step involves aggregating them
into a final patient-level representation. We design an attention-
based mechanism that leverages implicit task information for guid-
ance, achieving task-specific differentiated attention to various
3614KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Muhao Xu et al.
Algorithm 1: Algorithm of FlexCare
Input:
Multimodal multitask EHR dataset {Dğœ}ğ‘‡
ğœ=1
Training:
1Initialize weights;
2forepoch in 1,2,...,ğ‘’ğ‘ğ‘œğ‘â„ğ‘šğ‘ğ‘¥ do
3 fortaskğœin1,2,...,ğ‘‡ do
4 forB in mini-batches do
5 Extract Hğ‘šğœof each modality ğ‘švia Eq.1;
6 Obtain the multimodal sequence:
H0ğœ=[htaskğœ,Hcombğœ,Hğ‘¡ğœ,Hğ‘–ğœ,Hğ‘›ğœ];
7 Obtain ztaskğœandZcombğœ via Eq.2-3;
8 Obtain the refined representation sğ‘ğœfor the modality
combination ğ‘for the specific task via Eq.8;
9 Obtain the patient-level representation sğ‘
ğœvia Eq.11;
10 Make prediction for the task ğœ;
11 Update the parameters by optimizing Eq.13.
12 end
13 end
14end
modal information. The patient-level representation sğ‘
ğœis defined
as:
sğ‘
ğœ=[ztask
ğœ||LN(âˆ‘ï¸
ğ‘âˆˆCğ›¼ğ‘sc
ğœ)], ğ›¼ğ‘=exp(eğ›¼ğ‘/ğœ€)Ã
ğ‘âˆˆCexp(eğ›¼ğ‘/ğœ€), (11)
eğ›¼ğ‘=Tanh([ztask
ğœ||sğ‘
ğœ]Wğ´
1)Â·Wğ´
2, (12)
whereğ›¼ğ‘is the attention score, Wğ´
1âˆˆR2ğ‘‘Ã—ğ‘‘andWğ´
2âˆˆRğ‘‘Ã—1are
the weight matrix. ğœ€is a temperature coefficient and [Â·||Â·] represents
the concatenation operation.
4.4 Task-specific Prediction Heads
For different tasks, we employ task-specific prediction heads to
obtain the prediction results: Ë†ğ‘¦ğœ=ğ‘ƒğœ(sğ‘
ğœ), whereğ‘ƒğœ(Â·)is the pre-
diction head for the ğœ-th task. For the binary classification task and
multi-label classification task, ğ‘ƒğœ(Â·)contains a linear transformation
with a Sigmoid activation. For the multi-class classification task,
ğ‘ƒğœ(Â·)contains a liner transformation with a Softmax activation.
The overall objective function for the ğœ-th task is as follows:
Lğœ=Lğ‘ğ‘Ÿğ‘’ğ‘‘
ğœ+ğ›½Lğ‘ğ‘œğ‘£,Lğ‘ğ‘Ÿğ‘’ğ‘‘
ğœ=1
ğ‘ğœğ‘ğœâˆ‘ï¸
ğ‘›=1â„“ğœ(ğ‘¦(ğ‘›)
ğœ,Ë†ğ‘¦(ğ‘›)
ğœ) (13)
whereLğ‘ğ‘Ÿğ‘’ğ‘‘
ğœ is the prediction loss for the ğœ-th task,â„“ğœ(Â·,Â·)de-
notes a task-specific loss function (binary cross-entropy loss or
cross-entropy loss), and ğ›½is the hyperparameter to strike a balance
between the different loss functions.
For ease of understanding, Algorithm 1 delineates the training
procedure of our model.
5 EXPERIMENT
In this section, we evaluate our proposed FlexCare model focusing
on the following research questions:
â€¢RQ1: How does FlexCare perform on multiple real-world tasks?Table 2: Statistics of the datasets for multiple tasks.
Task # NumberMissing rate per modality
Time-series Image Note
IHM 26,318 0% 76.40% 7.49%
LOS 59,495 0% 85.16% 8.27%
DEC 59,269 0% 85.15% 8.24%
PHE 59,798 0% 81.94% 8.30%
REA 55,712 0% 82.38% 8.06%
DIA 132,576 76.34% 0% 32.56%
â€¢RQ2: How does each component influence the performance of
FlexCare?
â€¢RQ3: Whether FlexCare leverages the cross-task synergy?
â€¢RQ4: How is the extensibility of FlexCare?
5.1 Experimental Settings
5.1.1 Datesets. In this study, we use three EHR datasets: MIMIC-
IV1, MIMIC-CXR JPG2and MIMIC-IV-NOTE3[10â€“12]. Since these
datasets share the same patient cohort, we gathered time-series
data from MIMIC-IV, X-ray images from MIMIC-CXR, and clinical
notes from MIMIC-NOTE, thereby constructing a comprehensive
multimodal patient dataset. We divide the patients into a training
set, a validation set, and a test set in a 7:1:2 ratio. On the basis
of the multimodal dataset, we extract sub-datasets for 6 tasks: in-
hospital-mortality (IHM), length-of-stay (LOS), decompensation
(DEC), phenotyping (PHE), readmission (REA) and diagnosis (DIA).
It is imperative to note that each of these tasks utilizes heteroge-
neous datasets, meaning the input data and output labels vary across
tasks. Table 2 provides detailed statistics of datasets for multiple
tasks, revealing variations in sample sizes and modality messiness
among different tasks.
5.1.2 Baselines and Implementation Details. To assess the
effectiveness of our proposed model, we compare it against the
following multimodal baselines: MedFuse [ 6], MT [ 22], M3Care [ 37],
MMF [15] and MultiModN [30].
For binary classification tasks (IHM, DEC, and REA), we use
AUROC and AUPRC as evaluation metrics. For multi-class tasks
(PHE and DIA), we use macro-AUROC and micro-AUROC; and for
multi-label classification task (LOS), we use macro-F1 and micro-F1.
Due to differences in the problem setting, previous multitask
models are not applicable to our constructed dataset. Because these
datasets are independent, each sample has a label for only one task.
Moreover, it should be noted that as some of the above models
do not encompass all modalities present in our dataset, we have
extended them accordingly while ensuring the consistency of the
foundational embedding layer across all models (i.e., a linear pro-
jection, patch projection and the pre-trained & frozen BioBERT).
Given the varying training difficulties across different tasks, we
employ distinct weights for each task in FlexCare and mitigate the
issue of inconsistent convergence rates among multitask learning
through weight decay.
1https://physionet.org/content/mimiciv/2.0/
2https://physionet.org/content/mimic-cxr-jpg/2.0.0/
3https://physionet.org/content/mimic-iv-note/2.2/
3615FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Performance comparison between baselines and the proposed method on multiple tasks. The best and second-best
results are highlighted in bold and underline, respectively.
Task Metric MedFuse [6] MT [22] M3Care [37] MMF [15] MultiModN [ 30]FlexCare-st FlexCare
IHMAUROC 0.8772 (0.003) 0.8726 (0.002) 0.8732 (0.006) 0.8804 (0.001) 0.8751 (0.002) 0.8749 (0.004) 0.8823 (0.002)0.8823 (0.002)0.8823 (0.002)
AUPRC 0.5158 (0.006) 0.5133 (0.008) 0.5148 (0.017) 0.5136 (0.010) 0.5055 (0.006) 0.5116 (0.007) 0.5372 (0.006)0.5372 (0.006)0.5372 (0.006)
LOSma-F1 0.1487 (0.006) 0.1531 (0.006) 0.1549 (0.007) 0.1554 (0.006)0.1554 (0.006)0.1554 (0.006) 0.1503 (0.010) 0.1492 (0.005) 0.1479 (0.005)
mi-F1 0.6289 (0.005) 0.6298 (0.003) 0.6267 (0.004) 0.6282 (0.004) 0.6307 (0.006) 0.6317 (0.001) 0.6358 (0.003)0.6358 (0.003)0.6358 (0.003)
DECAUROC 0.9396 (0.002) 0.9409 (0.001) 0.9406 (0.004) 0.9435 (0.001) 0.9470 (0.001) 0.9420 (0.002) 0.9538 (0.001)0.9538 (0.001)0.9538 (0.001)
AUPRC 0.4782 (0.006) 0.4792 (0.010) 0.4911 (0.011) 0.4981 (0.008) 0.4922 (0.005) 0.4926 (0.010) 0.5123 (0.006)0.5123 (0.006)0.5123 (0.006)
PHEma-AUROC 0.8340 (0.001) 0.8362 (0.001) 0.8429 (0.001) 0.8446 (0.001)0.8446 (0.001)0.8446 (0.001) 0.8424 (0.000) 0.8417 (0.000) 0.8393 (0.005)
mi-AUROC 0.8785 (0.001) 0.8769 (0.001) 0.8830 (0.001) 0.8845 (0.000)0.8845 (0.000)0.8845 (0.000) 0.8826 (0.000) 0.8820 (0.000) 0.8803 (0.004)
REAAUROC 0.7598 (0.002) 0.7585 (0.002) 0.7618 (0.001) 0.7627 (0.002) 0.7622 (0.001) 0.7604 (0.002) 0.7680 (0.002)0.7680 (0.002)0.7680 (0.002)
AUPRC 0.3618 (0.003) 0.3481 (0.008) 0.3562 (0.003) 0.3482 (0.006) 0.3526 (0.004) 0.3517 (0.003) 0.3702 (0.004)0.3702 (0.004)0.3702 (0.004)
DIAma-AUROC 0.6651 (0.007) 0.6715 (0.005) 0.6756 (0.006) 0.6692 (0.002) 0.6717 (0.005) 0.6750 (0.005) 0.6845 (0.006)0.6845 (0.006)0.6845 (0.006)
mi-AUROC 0.8920 (0.002) 0.8960 (0.002) 0.8955 (0.001) 0.8960 (0.001) 0.8944 (0.001) 0.8948 (0.001) 0.8984 (0.001)0.8984 (0.001)0.8984 (0.001)
5.2 Model Performance (RQ1)
In the experiments, baseline models are independently trained for
each task, whereas FlexCare is trained on all tasks within a single
model. As shown in Table 3, two key conclusions can be drawn: (1)
Compared to single-task baseline models, FlexCare achieves
competitive results across various evaluation metrics while
also being adaptive to a range of tasks. Overall, the perfor-
mance differences among the baseline models are not significant.
This could be attributed to the fact that after replacing with the
unified unimodal encoders, various multimodal fusion strategies
under a single-task are capable of effectively addressing the current
issue, yet they have reached a performance plateau. Unlike other
models that are optimized for a single task, our multi-task model
aims at the overall performance improvement and may not achieve
optimal results in some individual metrics across all 6 tasks. By
means of the synergistic effect between tasks, our model achieves
the best results on the IHM, DEC, REA, and DIA tasks, with per-
formance on the remaining two tasks closely matching that of the
baselines. (2) Through leveraging cross-task synergy, tasks
that are closely related can benefit from each other. FlexCare
achieves significant performance improvements on the IHM and
DEC tasks, which are related to patient mortality risk. This demon-
strates that FlexCare can leverage cross-task synergy to acquire
additional knowledge, aiding in achieving superior performance for
related tasks. Moreover, comparing to FlexCare-st that is trained
independently for each task, we can observe that the strategy of
heterogeneous multitask joint training is both rational and effective,
with certain tasks benefiting from other related tasks.
5.3 Analysis of Model Design (RQ2)
5.3.1 Ablation Study. To evaluate the contributions of the dif-
ferent modules of our model to the prediction performance, we
conduct an ablation study by comparing three variants of the model:Table 4: Ablation study setup.
Model Modality Com-
bination tokensRepresentation
DecorrelationTask/Modality-
aware MoE
FlexCare a-Ã— Ã— Ã—
FlexCare b-âœ“Ã— Ã—
FlexCare c-âœ“ âœ“ Ã—
FlexCare d-âœ“Ã— âœ“
FlexCare âœ“ âœ“ âœ“
FlexCare a-, FlexCare b-, FlexCare c-and FlexCare d-. The specific set-
ting is outlined in Table 4, where different models comprise various
combinations of three modules.
As shown in Figure 4, the performance disparity between Flex-
Care and both FlexCare a-and FlexCare b-highlights the effective-
ness of modality combination tokens and covariance regulariza-
tion. Furthermore, FlexCare c-performs well on several tasks, but
worse on others (e.g., DEC, REA, DIA), even falling below that
of FlexCare a-and FlexCare b-. Additionally, FlexCare c-exhibits a
higher overall standard deviation, indicating less stability. These re-
sults corroborate the phenomenon of negative interface in multitask
learning mentioned above.
5.3.2 Analysis of Learnable Task Token. To facilitate multi-
modal representation learning for specific tasks, we incorporate
task information as input and learn a corresponding task token
for each task. To demonstrate the impact of the task token on the
final representations, we randomly select 30 mini-batch samples
(960 samples in total) for each task, comparing scenarios with and
without the use of the task token. Figure 5 shows the visualized
results of final embeddings learned w/o and w/ the task token. It is
observed that without explicitly providing task information as input
to the model, the representations of samples from different tasks are
3616KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Muhao Xu et al.
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000024/uni00000038/uni00000035/uni00000032/uni00000026/uni0000002c/uni0000002b/uni00000030
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni00000018/uni00000015/uni00000013/uni00000013/uni00000011/uni00000018/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000016/uni00000013/uni00000013/uni00000011/uni00000018/uni00000016/uni00000018/uni00000013/uni00000011/uni00000018/uni00000017/uni00000013/uni00000013/uni00000011/uni00000018/uni00000017/uni00000018/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000024/uni00000038/uni00000033/uni00000035/uni00000026/uni0000002c/uni0000002b/uni00000030
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni00000014/uni00000015/uni00000013/uni00000011/uni00000014/uni00000016/uni00000013/uni00000011/uni00000014/uni00000017/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000014/uni00000019/uni00000050/uni00000044/uni00000010/uni00000029/uni00000014/uni0000002f/uni00000032/uni00000036
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni00000019/uni00000016/uni00000013/uni00000013/uni00000011/uni00000019/uni00000016/uni00000015/uni00000013/uni00000011/uni00000019/uni00000016/uni00000017/uni00000013/uni00000011/uni00000019/uni00000016/uni00000019/uni00000013/uni00000011/uni00000019/uni00000016/uni0000001b/uni00000013/uni00000011/uni00000019/uni00000017/uni00000013/uni00000050/uni0000004c/uni00000010/uni00000029/uni00000014/uni0000002f/uni00000032/uni00000036
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000019/uni00000013/uni00000024/uni00000038/uni00000035/uni00000032/uni00000026/uni00000027/uni00000028/uni00000026
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni00000017/uni00000019/uni00000013/uni00000011/uni00000017/uni0000001b/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000018/uni00000017/uni00000024/uni00000038/uni00000033/uni00000035/uni00000026/uni00000027/uni00000028/uni00000026
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni0000001b/uni00000016/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000016/uni00000015/uni00000013/uni00000011/uni0000001b/uni00000016/uni00000017/uni00000013/uni00000011/uni0000001b/uni00000016/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000016/uni0000001b/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000015/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000017/uni00000050/uni00000044/uni00000010/uni00000024/uni00000038/uni00000035/uni00000032/uni00000026/uni00000033/uni0000002b/uni00000028
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001a/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001b/uni0000001a/uni00000018/uni00000050/uni00000044/uni00000010/uni00000024/uni00000038/uni00000033/uni00000035/uni00000026/uni00000033/uni0000002b/uni00000028
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni0000001a/uni00000019/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000019/uni00000015/uni00000013/uni00000011/uni0000001a/uni00000019/uni00000017/uni00000013/uni00000011/uni0000001a/uni00000019/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000019/uni0000001b/uni00000013/uni00000011/uni0000001a/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni0000001a/uni00000015/uni00000013/uni00000011/uni0000001a/uni0000001a/uni00000017/uni00000024/uni00000038/uni00000035/uni00000032/uni00000026/uni00000035/uni00000028/uni00000024
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni00000016/uni00000019/uni00000013/uni00000013/uni00000013/uni00000011/uni00000016/uni00000019/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000019/uni00000018/uni00000013/uni00000013/uni00000011/uni00000016/uni00000019/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000016/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000011/uni00000016/uni0000001a/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni0000001a/uni00000018/uni00000013/uni00000013/uni00000011/uni00000016/uni0000001a/uni0000001a/uni00000018/uni00000024/uni00000038/uni00000033/uni00000035/uni00000026/uni00000035/uni00000028/uni00000024
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni00000019/uni00000019/uni00000013/uni00000011/uni00000019/uni0000001a/uni00000013/uni00000011/uni00000019/uni0000001b/uni00000013/uni00000011/uni00000019/uni0000001c/uni00000050/uni00000044/uni00000010/uni00000024/uni00000038/uni00000035/uni00000032/uni00000026/uni00000027/uni0000002c/uni00000024
/uni00000029/uni0000004f/uni00000048/uni0000005b/uni00000026/uni00000044/uni00000055/uni00000048 /uni00000044/uni00000010 /uni00000045/uni00000010 /uni00000046/uni00000010 /uni00000047/uni00000010/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001c/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000050/uni00000044/uni00000010/uni00000024/uni00000038/uni00000033/uni00000035/uni00000026/uni00000027/uni0000002c/uni00000024
Figure 4: Results of ablation study.
/uni0000000b/uni00000044/uni0000000c/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000057/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
 /uni0000000b/uni00000045/uni0000000c/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000057/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni00000037/uni00000044/uni00000056/uni0000004e
/uni0000002c/uni0000002b/uni00000030
/uni0000002f/uni00000032/uni00000036
/uni00000027/uni00000028/uni00000026
/uni00000033/uni0000002b/uni00000028
/uni00000035/uni00000028/uni00000024
/uni00000027/uni0000002c/uni00000024
Figure 5: Visualization of patient-level representation
learned w/o and w/ the task token.
intermingled and indistinguishable. However, upon inputting task
information, the representations of various tasks are effectively seg-
regated. Moreover, from Figure 5(b), it should be noted that samples
from the five tasks primarily based on time-series modalities are
more clustered together, whereas samples from the DIA task, which
relies on the imaging modality, are situated farther from the others.
This indicates the ability of FlexCare to learn the modality-specific
differences inherent to each task.
5.4 Analysis of Cross-Task Synergy (RQ3)
5.4.1 Expert Selection. As shown in Figure 6, we visualize the
frequency of experts being selected for each task and each modality
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni00000003/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni0000002c/uni0000002b/uni00000030
/uni0000002f/uni00000032/uni00000036
/uni00000027/uni00000028/uni00000026
/uni00000033/uni0000002b/uni00000028
/uni00000035/uni00000028/uni00000024
/uni00000027/uni0000002c/uni00000024/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000035/uni00000052/uni00000058/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000053/uni00000048/uni00000046/uni0000004c/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000057/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni0000002f/uni00000048/uni00000059/uni00000048/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni00000003/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000057/uni0000000e/uni0000004c/uni0000000e/uni00000051
/uni00000057/uni0000000e/uni0000004c
/uni00000057/uni0000000e/uni00000051
/uni0000004c/uni0000000e/uni00000051
/uni00000057
/uni0000004c
/uni00000051/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000035/uni00000052/uni00000058/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000053/uni00000048/uni00000046/uni0000004c/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000057/uni00000003/uni00000030/uni00000052/uni00000047/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000002f/uni00000048/uni00000059/uni00000048/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017Figure 6: Routing specialization at task and modality levels.
combination. From Figure 6(a), we observe that the experts em-
ployed for task DIA exhibit significant differences compared to
those used for other tasks. This discrepancy arises because DIA
primarily relies on the imaging modality, with other modalities serv-
ing as auxiliary information, whereas other tasks predominantly
depend on time-series modality. Figure 6(b) presents the results
of modality-level routing specialization, reflecting the distinct ca-
pabilities of each expert, which aids in the refined processing of
multimodal information.
5.4.2 Interaction Between Tasks. To demonstrate whether Flex-
Care effectively utilizes cross-task synergy, we analyze the impact
of the current training task on other tasks. Specifically, we sequen-
tially train the six tasks in each epoch: IHM, LOS, DEC, PHE, REA,
and DIA. Upon the completion of training for the current task, we
report the test performance across all tasks. As illustrated in Figure
7, it can be observed that after the training of a specific task, the
performance of other tasks does not necessarily decline; rather, it
3617FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015/uni00000014/uni00000016/uni00000014/uni00000017/uni00000014/uni00000018/uni00000014/uni00000019/uni00000014/uni0000001a/uni00000014/uni0000001b/uni00000014/uni0000001c/uni00000015/uni00000013/uni00000015/uni00000014/uni00000015/uni00000015
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni0000002c/uni0000002b/uni00000030
/uni0000002f/uni00000032/uni00000036
/uni00000027/uni00000028/uni00000026
/uni00000033/uni0000002b/uni00000028
/uni00000035/uni00000028/uni00000024
/uni00000027/uni0000002c/uni00000024/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048
/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000053/uni00000055/uni00000052/uni00000046/uni00000048/uni00000056/uni00000056
Figure 7: Performance analysis of each task in the training
process.
Table 5: Results of DRG prediction.
Method1% Labeled 100% Labeled
ma-F1 mi-F1 ma-F1 mi-F1
MedFuse 0.0054 0.1125 0.2149 0.4367
MT 0.0128 0.1567 0.2283 0.4322
M3Care 0.0320 0.2491 0.2424 0.4402
MMF 0.0253 0.2316 0.2474 0.4423
MultiModN 0.0228 0.2162 0.2424 0.4412
FlexCare pretrain 0.03960.03960.0396 0.2695 0.26950.2695 0.2404 0.4387
FlexCare 0.0294 0.2390 0.2487 0.24870.2487 0.4440 0.44400.4440
may even exhibit improvement. For instance, the performance of
tasks IHM and DEC maintains an upward trend even when other
tasks are being trained. This demonstrates the ability of FlexCare to
effectively leverage cross-task synergy, enhancing the performance
of a task by other related tasks.
5.5 Analysis of Extensibility (RQ4)
As a flexible multimodal multitask model, FlexCare can be adapted
to other new tasks. We construct a new task, Diagnosis-related
Group (DRG) prediction [ 17], which primarily relies on clinical
notes. This task features a sizable dataset, encompassing 236,770
samples. We conduct experiments under two settings, training for
40 epochs with 1% of the training data and 100% of the training
data, respectively. Table 5 presents the experimental results, where
FlexCare pretrain employs the model trained on the six tasks men-
tioned previously, while FlexCare denotes the model trained from
scratch on the current task. It can be observed that with limited
training data, both the baseline models and FlexCare that trained
from scratch, do not perform as well as pre-trained FlexCare. How-
ever, when utilizing 100% of the training data, the advantage of pre-
trained FlexCare is diminished due to the significantly larger sample
size of this task compared to the others. This experiment demon-
strates FlexCare can be flexibly extended to new tasks, achieving
commendable performance even with smaller data sizes.6 CONCLUSION
In this paper, we introduce FlexCare, a unified healthcare predic-
tion model that flexibly accommodates incomplete multimodal in-
puts and adapts to multiple tasks. It is endowed with multitasking
capabilities realized through asynchronous multiple single-task
predictions. Specifically, the task-agnostic multimodal information
extraction module is designed to thoroughly capture information
across a spectrum of modality combination patterns. Meanwhile, a
token-level covariance regularization method is developed to pre-
vent different modality combination tokens from encoding similar
information. Furthermore, we propose the task-guided hierarchical
multimodal fusion module to learn adaptive representation tailored
to the specific task. In addition, we experimented on multiple tasks
from MIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets to show the
effectiveness of the proposed model.
In future work, we will investigate effective solutions to address
the issues of gradient conflicts and inconsistencies in convergence
rates during multitask training, contributing to the advancement
of a general prediction model in the healthcare domain.
ACKNOWLEDGMENTS
This work was supported in part by the National Key Research and
Development Program of China under Grant No.2021ZD0140407,
the Beijing Natural Science Foundation under Grant No.7222313,
and the National High Level Hospital Clinical Research Funding
under Grant No.2022-PUMCH-C-041.
REFERENCES
[1]Raquel Aoki, Frederick Tung, and Gabriel L Oliveira. 2022. Heterogeneous multi-
task learning with expert diversity. IEEE/ACM Transactions on Computational
Biology and Bioinformatics 19, 6 (2022), 3093â€“3102.
[2]Adrien Bardes, Jean Ponce, and Yann LeCun. 2022. VICReg: Variance-Invariance-
Covariance Regularization for Self-Supervised Learning. In ICLRâ€™22.
[3]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is
Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLRâ€™21.
[4]Michal Golovanevsky, Carsten Eickhoff, and Ritambhara Singh. 2022. Multimodal
attention-based deep learning for Alzheimerâ€™s disease diagnosis. Journal of the
American Medical Informatics Association 29, 12 (2022), 2014â€“2022.
[5]Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram
Galstyan. 2019. Multitask learning and benchmarking with clinical time series
data. Scientific Data 6, 1 (2019), 96.
[6]Nasir Hayat, Krzysztof J Geras, and Farah E Shamout. 2022. MedFuse: Multi-
modal fusion with clinical time-series data and chest X-ray images. In MLHCâ€™22.
479â€“503.
[7]Ronghang Hu and Amanpreet Singh. 2021. Unit: Multimodal multitask learning
with a unified transformer. In ICCVâ€™21. 1439â€“1449.
[8]Kyunghoon Hur, Jungwoo Oh, Junu Kim, Jiyoun Kim, Min Jae Lee, Eunbyeol
Cho, Seong-Eun Moon, Young-Hak Kim, Louis Atallah, and Edward Choi. 2024.
GenHPF: General Healthcare Predictive Framework for Multi-Task Multi-Source
Learning. IEEE Journal of Biomedical and Health Informatics 28, 1 (2024), 502â€“513.
[9]Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris
Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al .
2019. Chexpert: A large chest radiograph dataset with uncertainty labels and
expert comparison. In AAAIâ€™19. 590â€“597.
[10] Alistair Johnson, Lucas Bulgarelli, Tom Pollard, Steven Horng, Leo Anthony Celi,
and Roger Mark. 2020. Mimic-iv. PhysioNet. Available online at: https://physionet.
org/content/mimiciv/1.0/(accessed August 23, 2021) (2020).
[11] Alistair Johnson, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark.
2023. MIMIC-IV-Note: Deidentified free-text clinical notes.
[12] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren,
Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and
Steven Horng. 2019. MIMIC-CXR-JPG, a large publicly available database of
labeled chest radiographs. arXiv preprint arXiv:1901.07042 (2019).
3618KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Muhao Xu et al.
[13] Trent Kyono, Fiona J Gilbert, and Mihaela Schaar. 2019. Multi-view multi-task
learning for improving autonomous mammogram diagnosis. In MLHCâ€™19. 571â€“
591.
[14] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,
Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language
representation model for biomedical text mining. Bioinformatics 36, 4 (2020),
1234â€“1240.
[15] Kwanhyung Lee, Soojeong Lee, Sangchul Hahn, Heejung Hyun, Edward Choi,
Byungeun Ahn, and Joohyung Lee. 2023. Learning Missing Modal Electronic
Health Records with Unified Multi-modal Data Embedding and Modality-Aware
Attention. In MLHCâ€™23. 423â€“442.
[16] Fei Li and Hong Yu. 2020. ICD coding from clinical text using multi-filter residual
convolutional neural network. In AAAIâ€™20, Vol. 34. 8180â€“8187.
[17] Jinghui Liu, Daniel Capurro, Anthony Nguyen, and Karin Verspoor. 2021. Early
prediction of diagnostic-related groups and estimation of hospital cost by pro-
cessing clinical notes. NPJ Digital Medicine 4, 1 (2021), 103.
[18] Luchen Liu, Zequn Liu, Haoxian Wu, Zichang Wang, Jianhao Shen, Yipiing
Song, and Ming Zhang. 2020. Multi-task learning via adaptation to similar tasks
for mortality prediction of diverse rare diseases. In AMIA Annual Symposium
Proceedings, Vol. 2020. 763.
[19] Sicen Liu, Xiaolong Wang, Yongshuai Hou, Ge Li, Hui Wang, Hui Xu, Yang Xiang,
and Buzhou Tang. 2022. Multimodal data matters: language model pre-training
over structured and unstructured electronic health records. IEEE Journal of
Biomedical and Health Informatics 27, 1 (2022), 504â€“514.
[20] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-
Task Deep Neural Networks for Natural Language Understanding. In ACLâ€™19.
4487â€“4496.
[21] Liantao Ma, Chaohe Zhang, Yasha Wang, Wenjie Ruan, Jiangtao Wang, Wen
Tang, Xinyu Ma, Xin Gao, and Junyi Gao. 2020. Concare: Personalized clinical
feature embedding via capturing the healthcare context. In AAAIâ€™20. 833â€“840.
[22] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng. 2022. Are
multimodal transformers robust to missing modality?. In CVPRâ€™22. 18177â€“18186.
[23] Chantal Pellegrini, Nassir Navab, and Anees Kazi. 2023. Unsupervised pre-
training of graph transformers on patient population graphs. Medical Image
Analysis 89 (2023), 102895.
[24] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. 2021. Med-BERT:
pretrained contextualized embeddings on large-scale structured electronic health
records for disease prediction. NPJ Digital Medicine 4, 1 (2021), 86.
[25] Wei Shao, Tongxin Wang, Liang Sun, Tianhan Dong, Zhi Han, Zhi Huang, Jie
Zhang, Daoqiang Zhang, and Kun Huang. 2020. Multi-task multi-modal learning
for joint diagnosis and prognosis of human cancers. Medical Image Analysis 65
(2020), 101795.
[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The
sparsely-gated mixture-of-experts layer. In ICLRâ€™17.
[27] Yuqi Si and Kirk Roberts. 2019. Deep patient representation of clinical notes via
multi-task learning for mortality prediction. AMIA Summits on Translational
Science Proceedings 2019 (2019), 779.
[28] Luis R Soenksen, Yu Ma, Cynthia Zeng, Leonard Boussioux, Kimberly Villalo-
bos Carballo, Liangyuan Na, Holly M Wiberg, Michael L Li, Ignacio Fuentes, and
Dimitris Bertsimas. 2022. Integrated multimodal artificial intelligence framework
for healthcare applications. NPJ Digital Medicine 5, 1 (2022), 149.
[29] Harini Suresh, Jen J Gong, and John V Guttag. 2018. Learning tasks for multitask
learning: Heterogenous patient populations in the icu. In KDDâ€™18. 802â€“810.
[30] Vinitra Swamy, Malika Satayeva, Jibril Frej, Thierry Bossy, Thijs Vogels, Martin
Jaggi, Tanja KÃ¤ser, and Mary-Anne Hartley. 2023. MultiModN-Multimodal, Multi-
Task, Interpretable Modular Networks. In NeurIPSâ€™23.
[31] Siyi Tang, Amara Tariq, Jared A Dunnmon, Umesh Sharma, Praneetha Elugunti,
Daniel L Rubin, Bhavik N Patel, and Imon Banerjee. 2023. Predicting 30-day
all-cause hospital readmission using multimodal spatiotemporal graph neural
networks. IEEE Journal of Biomedical and Health Informatics 27, 4 (2023), 2071â€“
2082.
[32] Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Linfeng Li, Haiyan Wu, and
Yao Zhao. 2023. Cooperative dual medical ontology representation learning for
clinical assisted decision-making. Computers in Biology and Medicine (2023),
107138.
[33] Muhao Xu, Zhenfeng Zhu, Yawei Zhao, Kunlun He, Qinghua Huang, and Yao
Zhao. 2024. RedCDR: Dual Relation Distillation for Cancer Drug Response
Prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics
(2024), 1â€“12.
[34] Yongxin Xu, Kai Yang, Chaohe Zhang, Peinie Zou, Zhiyuan Wang, Hongxin Ding,
Junfeng Zhao, Yasha Wang, and Bing Xie. 2023. VecoCare: visit sequences-clinical
notes joint learning for diagnosis prediction in healthcare data. In IJCAIâ€™23. 4921â€“
4929.
[35] Yuyang Xu, Haochao Ying, Siyi Qian, Fuzhen Zhuang, Xiao Zhang, Deqing Wang,
Jian Wu, and Hui Xiong. 2023. Time-Aware Context-Gated Graph Attention
Network for Clinical Risk Prediction. IEEE Transactions on Knowledge and Data
Engineering 35, 7 (2023), 7557â€“7568.[36] Ruoxi Yu, Yali Zheng, Ruikai Zhang, Yuqi Jiang, and Carmen C. Y. Poon. 2020.
Using a Multi-Task Recurrent Neural Network With Attention Mechanisms to
Predict Hospital Mortality of Patients. IEEE Journal of Biomedical and Health
Informatics 24, 2 (2020), 486â€“492.
[37] Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang,
and Junfeng Zhao. 2022. M3Care: Learning with Missing Modalities in Multimodal
Healthcare Data. In KDDâ€™22. 2418â€“2428.
[38] Xinlu Zhang, Shiyang Li, Zhiyu Chen, Xifeng Yan, and Linda Ruth Petzold. 2023.
Improving medical predictions by irregular multimodal electronic health records
modeling. In ICMLâ€™23. 41300â€“41313.
[39] Xianli Zhang, Buyue Qian, Yang Li, Yang Liu, Xi Chen, Chong Guan, and Chen
Li. 2021. Learning robust patient representations from multi-modal electronic
health records: a supervised deep learning approach. In SDMâ€™21. 585â€“593.
[40] Xiongjun Zhao, Xiang Wang, Fenglei Yu, Jiandong Shang, and Shaoliang Peng.
2022. UniMed: Multimodal Multitask Learning for Medical Predictions. In
BIBMâ€™22. 1399â€“1404.
[41] Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Zhenyu Guo, Yang Liu, Yuchen Yang,
and Yao Zhao. 2022. Multi-modal graph learning for disease prediction. IEEE
Transactions on Medical Imaging 41, 9 (2022), 2207â€“2216.
[42] Hong-Yu Zhou, Yizhou Yu, Chengdi Wang, Shu Zhang, Yuanxu Gao, Jia Pan, Jun
Shao, Guangming Lu, Kang Zhang, and Weimin Li. 2023. A transformer-based
representation-learning model with unified processing of multimodal input for
clinical diagnostics. Nature Biomedical Engineering 7, 6 (2023), 743â€“755.
A DETAILS OF EXPERIMENTAL SETTINGS
A.1 Tasks
Following previous works for dataset creation, we extract sub-
datasets for 7 tasks from MIMIC-IV dataset, 6 of which are utilized
for multitask performance comparison, and the remaining one for
extensibility analysis:
â€¢In-hospital mortality (IHM): predicts in-hospital mortality
based on the first 48 hours of an ICU stay. This is a binary classi-
fication task.
â€¢Length-of-stay (LOS): predicts remaining time spent in ICU.
We define it as a classification problem with 10 classes (one for
ICU stays shorter than a day, seven day-long buckets for each
day of the first week, one for stays of over one week but less than
two, and one for stays of over two weeks).
â€¢Decompensation (DEC): predicts whether a patient will decease
in the next 24 hours. This is a binary classification task.
â€¢Phenotyping (PHE): classifies which of 25 acute care conditions
are present in a given patient ICU stay record. This is a multilabel
classification problem.
â€¢Readmission (REA): predicts whether the patient will be read-
mitted within the next 30 days. This is a binary classification
task.
â€¢Diagnosis (DIA): predicts diagnosis based on chest radiograph.
This is a multilabel classification problem.
â€¢Diagnosis-related Group (DRG): predicts the DRG code as-
signed to the patient. This is a multi-class classification task.
A.2 Data Preprocessing
â€¢Time series data: 17 variables are sampled every one hour, and
the missing value is imputed using the previous one. Then, they
are discretized and standardized to obtain the 76-dimension input
at each time-step.
â€¢Image data: Images classified as AP (Anteroposterior) are re-
tained. Following the previous work [ 6], during the training
phase, the images are resized to 256 Ã—256 pixels, and random
horizontal flip and random affine transformation are applied, fol-
lowed by cropping to 224 Ã—224 pixels. For validation and testing
3619FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
phases, only resizing and center cropping to 224 Ã—224 pixels are
performed.
â€¢Note data: We only extract the Past Medical History of the
clinical notes for tasks other than DRG to avoid information
leakage. For DRG task, we extract Brief Hospital Course part as
the overall admission summary.
A.3 Baselines
The detailed description about baselines is as follows:
â€¢MedFuse [6]: develops an LSTM-based fusion module that can
accommodate uni-modal as well as multi-modal input.
â€¢MT[22]: employs the Transformer architecture with attention
mask to fuse multimodal data with missing modality.
â€¢M3Care [37]: imputes the information of the missing modalities
in the latent space from the similar neighbors of each patient.
â€¢MMF [15]: learns the EHR data with missing modal by the
modality-aware attention with skip bottleneck.
â€¢MultiModN [30]: sequentially inputs any number or combina-
tion of modalities into a sequence of modality-specific encoders
and can skip over missing modalities.
A.4 Model Implementation
Our model is implemented using PyTorch 1.10.0 and Python 3.7.9.
The experiment environment is a machine equipped with Ubuntu
20.04 and NVIDIA GeForce RTX 3090 GPU. For models with trans-
former encoders, the number of transformer layers is set to 4 and
the number of attention heads is set to 2. For all models, the dimen-
sion of the hidden layer is 128. The number of the selected experts
ğ‘˜and the total experts ğ‘ğ‘’are 2 and 10. Due to large search space
for 6 task weights, we set them with {0.2,0.5,0.2,1,0.2,0.2} based on
loss magnitude and training difficulty of single-task models. We
use Adam as the optimizer with batch size 32, learning rate 1e-3 or5e-4. We train baseline models for 40 epochs and modify random
seeds for five repetitions.
B FURTHER ANALYSIS ON TASK TOKEN
In Section 5.3.2, we visualize the patient-level representation learned
w/o and w/ the task token. Here, we present quantitative exper-
imental results under two scenarios. As observed in Table 6, the
performance across various tasks declines when task information
is not provided to the model, which is precisely because the rep-
resentations of different tasks are mixed together and cannot be
clearly distinguished.
Table 6: Performance comparison using FlexCare w/o and w/
the task token.
Task Metric w/o task token FlexCare
IHMAUROC 0.8831 (0.003)0.8831 (0.003)0.8831 (0.003) 0.8823 (0.002)
AUPRC 0.5292 (0.007) 0.5372 (0.006) 0.5372 (0.006)0.5372 (0.006)
LOSma-F1 0.1321 (0.004) 0.1479 (0.005) 0.1479 (0.005)0.1479 (0.005)
mi-F1 0.6351 (0.004) 0.6358 (0.003) 0.6358 (0.003)0.6358 (0.003)
DECAUROC 0.9463 (0.001) 0.9538 (0.001) 0.9538 (0.001)0.9538 (0.001)
AUPRC 0.4891 (0.010) 0.5123 (0.006) 0.5123 (0.006)0.5123 (0.006)
PHEma-AUROC 0.8338 (0.005) 0.8393 (0.005) 0.8393 (0.005)0.8393 (0.005)
mi-AUROC 0.8747 (0.005) 0.8803 (0.004) 0.8803 (0.004)0.8803 (0.004)
REAAUROC 0.7698 (0.002)0.7698 (0.002)0.7698 (0.002) 0.7680 (0.002)
AUPRC 0.3625 (0.007) 0.3702 (0.004) 0.3702 (0.004)0.3702 (0.004)
DIAma-AUROC 0.6778 (0.001) 0.6845 (0.006) 0.6845 (0.006)0.6845 (0.006)
mi-AUROC 0.8974 (0.001) 0.8984 (0.001) 0.8984 (0.001)0.8984 (0.001)
3620