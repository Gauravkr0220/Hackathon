Leveraging Pedagogical Theories to Understand Student Learning
Process with Graph-based Reasonable Knowledge Tracing
Jiajun Cui
cuijj96@gmail.com
East China Normal University
Shanghai, ChinaHong Qian
hqian@cs.ecnu.edu.cn
East China Normal University
Shanghai, China
Bo Jiang
bjiang@deit.ecnu.edu.cn
East China Normal University
Shanghai, ChinaWei Zhangâˆ—
zhangwei.thu2011@gmail.com
East China Normal University
Shanghai, China
ABSTRACT
Knowledge tracing (KT) is a crucial task in intelligent education,
focusing on predicting studentsâ€™ performance on given questions to
trace their evolving knowledge. The advancement of deep learning
in this field has led to deep-learning knowledge tracing (DLKT)
models that prioritize high predictive accuracy. However, many
existing DLKT methods overlook the fundamental goal of track-
ing studentsâ€™ dynamical knowledge mastery. These models do not
explicitly model knowledge mastery tracing processes or yield un-
reasonable results that educators find difficulty to comprehend and
apply in real teaching scenarios. In response, our research conducts
a preliminary analysis of mainstream KT approaches to highlight
and explain such unreasonableness. We introduce GRKT, a graph-
based reasonable knowledge tracing method to address these issues.
By leveraging graph neural networks, our approach delves into the
mutual influences of knowledge concepts, offering a more accurate
representation of how the knowledge mastery evolves throughout
the learning process. Additionally, we propose a fine-grained and
psychological three-stage modeling process as knowledge retrieval,
memory strengthening, and knowledge learning/forgetting, to con-
duct a more reasonable knowledge tracing process. Comprehensive
experiments demonstrate that GRKT outperforms eleven baselines
across three datasets, not only enhancing predictive accuracy but
also generating more reasonable knowledge tracing results. This
makes our model a promising advancement for practical imple-
mentation in educational settings. The source code is available at
https://github.com/JJCui96/GRKT.
âˆ—Corresponding author. This work was supported in part by National Key R&D Program
of China (No. 2023YFC3341200), National Natural Science Foundation of China (No.
92270119 and No. 62072182), and Key Laboratory of Advanced Theory and Application
in Statistics and Data Science, Ministry of Education.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671853CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks; â€¢Applied
computingâ†’Education; â€¢Information systems â†’Data min-
ing.
KEYWORDS
knowledge tracing, student behavior modeling, data mining, peda-
gogical theory, reasonable knowledge tracing
ACM Reference Format:
Jiajun Cui, Hong Qian, Bo Jiang, and Wei Zhang. 2024. Leveraging Ped-
agogical Theories to Understand Student Learning Process with Graph-
based Reasonable Knowledge Tracing. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671853
1 INTRODUCTION
In personalized learning, Knowledge Tracing (KT) is crucial for
tracking studentsâ€™ evolving knowledge mastery based on their his-
torical question responses [ 3,33]. Early researchers addressed this
challenge by leveraging the monotonicity assumption [ 7], linking
better mastery of one knowledge concept (KC) to a higher probabil-
ity of correctly answering related questions. They trained models to
predict student responses on given questions, proposing typical ma-
chine learning-based KT methods [ 3,21]. Consequently, predicting
student performance became the primary task, with prediction accu-
racy as the mainstream metric for evaluating KT models, promoting
the emergence of deep learning knowledge tracing (DLKT) methods.
However, many DLKT approaches prioritize prediction ability over
the fundamental objective of knowledge tracing, sometimes forgo-
ing tracing altogether [ 2,11]. Others use internal network weights
to represent knowledge mastery [ 27,38], facing challenges in con-
structing meaningful tracing results due to the low interpretability
and reasonability of deep neural network structures. Hidden neu-
rons in these networks adaptively learn from data without explicit
meaning [ 12]. It is worth noting that the cognitive diagnosis task
also assesses knowledge mastery but usually focuses on static test-
ing instead of dynamic learning process [ 15,17]. Therefore, we do
not delve into it within this paper.
Figure 1 illustrates the traced dynamic knowledge mastery of an
example student by two DLKT models: DKT [ 23] and LPKT [ 27].
DKT is a pioneering approach that directly applies recurrent neural
 
502
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
LPKT
Ideal Model
Area TriangleTimeline
Calculations with similar Figures Conversion of Fraction Decimals Percents Ordering Integers Multiplication and Division Positive DecimalsDKT
Learning Curve
Mastery Change of Unrelated KCs
 Inconsistent Mastery Change Direction
No Mastery Change of Related KCsLPKT
Testing Effects
 Transfer of LearningForgetting Curve
Figure 1: Illustration of a studentâ€™s evolving knowledge mastery while answering ten questions, traced by two DLKT models,
along with an assumed ideal tracing result. The student is sampled from the ASSIST12 dataset, introduced in Section 5.1.1.
networks (RNNs) to the KT task. In this case, when the student
responds to the initial four questions related to the blue KC Calcula-
tions with Similar Figures, their knowledge mastery of the unrelated
green KC Ordering Integers increases, presenting an unreasonable
outcome. Furthermore, a correct response to the sixth question
results in a contrary decrease in its corresponding KCâ€™s mastery,
demonstrating an inconsistent change in direction. LPKT, as a time-
aware method, models learning and forgetting processes for more
reasonable knowledge tracing. However, it struggles to capture
the relation between the yellow KC Area Triangle and the blue
KCCalculations with Similar Figures, as evidenced by the decreas-
ing mastery of the blue curve following a correct response to an
question of yellow. Both of these two KCs examine studentsâ€™ calcu-
lations about the base and height of triangles, which suggests their
underlying relation. Beneath the figure is a tracing result from an
assumed ideal model, which we design based on comprehensive
pedagogical effects. As shown, the student mastery will increase
and drop according to their right/wrong responses based on the
testing effect [ 24]. The mastery of the yellow KC would relatedly
increase due to the correct response to the sixth orange KC, accord-
ing to the transfer of learning [ 22]. Besides, the mastery between
responses should also vary due to studentsâ€™ learning and forgetting
behaviors modeled by the learning and forgetting curves [6, 36].
From this example, we summarize three deficiencies of current
DLKT methods in dynamic knowledge tracing reasonability: (i)
Mastery change of unrelated KCs - learning one KC affects un-
related KC mastery; (ii) No mastery change of related KCs -
learning one KC does not impact related KCs; (iii) Inconsistent
mastery change direction - correct answers may decrease KCmastery, and vice versa. These stem from opaque deep neural net-
works, whose parameters serve the overarching objective of perfor-
mance prediction. Moreover, many researches use RNNs to model
knowledge application and update by the recurrent unitsâ€™ output
and state transition [ 16,23,26,27]. This mixes the effects of students
answering questions and their spontaneous behaviors, leading to
confusing tracing results. For example, incorrect responses may
strengthen wrong knowledge retrieval and get a mastery drop of
the related KC. But when they get feedback and learn from their
errors, they can make a final progress. This fine-grained knowledge
mastery changing is not captured. To address these above issues, we
introduce GRKT, a Graph-based Reasonable Knowledge Tracing
to enhance knowledge tracing reasonability while retaining neural
networksâ€™ representational power.
To be specific, we integrate pedagogical theories [ 6,22,24,36]
into the KT modeling, dividing the learning process into three
distinct stages. (i) The knowledge retrieval stage analyzes how
students respond to questions. This stage draws from cognitive psy-
chology [ 18], viewing learning as encoding, storing, and retrieving
memories. When students answer questions, retrieval from mem-
ory becomes crucial. We start this stage by retrieving the encoded
memory related to the questionâ€™s KC and project it into a mastery
value. We then compare this value with the questionâ€™s difficulty
score to predict if the student could correctly answer the question.
(ii) The memory strengthening stage focuses on how answering
questions impacts studentsâ€™ knowledge mastery. Here, students
strengthen their memory retrieval routes, aligning with the Testing
Effect theory [ 14,24]. Correct retrievals enhance learning, while
incorrect ones reinforce errors. We encode this positive/negative
 
503Graph-based Reasonable Knowledge Tracing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
memory strengthening in the knowledge memory of the relevant
KC based on whether the question is correctly solved. (iii) The
knowledge learning/forgetting stage explores what students
do after question answering. This stage aims to model the active
learning and natural forgetting behaviors based on the Learning
curve [36] and the Forgetting curve [6]. Both curves suggest a de-
creasing rate of learning and forgetting over time. Concretely, we
first introduce a learning decider to determine whether students
will continue learning the KCs just practiced or the KCs for future
study. Then, we employ KC-specific time-aware kernels to model
the learning/forgetting curves of all involved KCs based on these
decisions. By applying this three-stage modeling process iteratively
across studentsâ€™ response sequences, we establish a coherent and
reasonable knowledge tracing framework. This approach effectively
captures mastery changes resulting from question answering and
subsequent behaviors, addressing the issue of inconsistent mas-
tery change direction.
To handle the two other issues of mastery change of unrelated
KCs andno mastery change of related KCs, we utilize the mes-
sage passing mechanism of graph neural networks (GNNs) applied
to KC relation graphs. This mechanism establishes clear boundaries
between related and unrelated KCs. Specifically, changes in knowl-
edge mastery of one KC are propagated through the graph edges
to its related KCs within a specific number of hops. From the peda-
gogical perspective, this message passing aligns with the Transfer
of Learning theory [22], which explains humansâ€™ ability to transfer
knowledge between similar fields to solve problems and acquire
skills. We integrate this understanding into our three-stage learning
process modeling using KC relation-based GNNs. For instance, in
the first stage of GRKT, instead of solely retrieving knowledge from
the target questionâ€™s KC, we utilize graph aggregation to synthesize
the memory of the KCâ€™s neighbors for solving the question. Simi-
larly, during the second stage, the memory strengthening process
involves propagating the gain and loss of knowledge mastery to
the KCâ€™s neighbors, and this process is also applied in the third
stageâ€™s knowledge learning. Additionally, we exploit the homophily
of GNNs to generate similar time-aware kernels for related KCs, ef-
fectively modeling their similar learning/forgetting processes. This
defines the boundaries between related and unrelated KCs based
on the number of hops in GNN operations, effectively addressing
challenges associated with mastery changes between different KCs.
Itâ€™s worth noting that KCs have various types of relations, including
prerequisite, similarity, collaboration, remedial, and hierarchy [ 10].
In GRKT, we primarily focus on leveraging the two most commonly
used relations: prerequisite and similarity.
To the best of our knowledge, this work represents the first
comprehensive analysis of the reasonability issues in current DLKT
methods, and integrates multiple pedagogical theories to address
these concerns. The main contributions of this paper are as follows:
â€¢Motivation. We identify the reasonability issues arising from the
widespread adoption of deep learning techniques in the KT task.
Many DLKT methods tend to excessively prioritize student per-
formance prediction, often overlooking unreasonable knowledge
tracing results due to the inherent interpretability challenges
posed by neural networks.â€¢Methods. We outline three primary reasonability issues preva-
lent in current DLKT methods. To address these issues, we intro-
duce GRKT, a graph-based reasonable knowledge tracing, which
establishes a three-stage learning process modeling. Additionally,
we utilize the KC relation graph to mitigate mutual effects among
KCs. The incorporation of multiple pedagogical theories provide
sufficient support for our proposed method.
â€¢Experiments. Comprehensive experimental results showcase
that our GRKT exhibits superior prediction performance and
yields reasonable knowledge tracing results when compared to
eleven baselines across three widely-used datasets.
2 RELATED WORK
2.1 Reasonable Knowledge Tracing
Early KT methods in machine learning, such as Bayesian Knowledge
Tracing (BKT) [ 3], initially showcased reasonable results due to
their transparent and interpretable internal structure. BKT utilizes
Hidden Markov Models (HMMs) to probabilistically represent the
student learning process. It transitions knowledge mastery and
emits probabilities of correct responses, while also considering
guessing and slipping behaviors. Subsequent KT methods expanded
upon BKT by incorporating additional pedagogical factors such as
question difficulty [21] or prior student information [39].
However, traditional methods show inferior prediction perfor-
mance when compared to subsequent emerging DLKT methods [ 2,
5,11,16,20,23,26], which reach high prediction performance due
to the power of neural networks. Even so, these DLKT methods
fail to produce reasonable knowledge tracing results due to their
inherently opaque structures. Efforts have been made to tackle this
challenge. Shen et al. [ 27] proposed Learning Process-consistent
Knowledge Tracing (LPKT), which utilizes student response dura-
tion and interval time to capture learning and forgetting behaviors.
However, it only focuses on knowledge learning and forgetting
and does not model the interplay of knowledge mastery changes
between KCs, limiting its reasonability. Similarly, Yin et al. [ 38]
introduced the Diagnostic Transformer (DTransformer), which di-
agnoses student knowledge mastery from each tackled question
and employs a contrastive learning framework to produce more sta-
ble knowledge tracing. While this stability enhances reasonability
to some extent, its transformer-based structures do not adequately
reflect the transition of knowledge mastery between continuous stu-
dent responses. Therefore, while these approaches improve model
reasonability from specific angles, they do not offer a comprehen-
sive method to generate reasonable knowledge tracing results cov-
ering both KC relations and continuous learning processes.
We address this gap with our proposed GRKT, which utilizes
GNNs to model KC relations and introduces a three-stage learn-
ing process to capture evolving knowledge mastery. By integrat-
ing these techniques, GRKT achieves high prediction performance
while also generating more reasonable knowledge tracing results.
2.2 Graph-based Knowledge Tracing
Graph Neural Networks (GNNs) [ 25] serve as an efficient tool to
capture intricate relations between instances in real-world sce-
narios. Their message aggregation and propagation operations on
 
504KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
graphs yield deep representations for node features, enhancing per-
formance in various downstream tasks across different domains. In
the context of KT, researchers explore various structures to harness
the power of GNNs. Nakagawa et al. [ 19] pioneered the incorpora-
tion of GNNs into KT by reformulating it as a time-series node-level
classification problem based on KC relation graphs. Gan et al. [ 9]
leveraged this structure to enhance graph representation learning,
generating more informative question and concept embeddings.
Except for KC relations, question-question and question-KC rela-
tions are also widely considered. For instance, Bi-CLKT [ 28] applied
contrastive learning to question-KC and KC-KC graphs to generate
question embeddings enriched with question and KC structural
information. Another work [ 35] leveraged question-KC relations
to address question sparsity and multi-skill problems. In our paper,
we specifically focus on utilizing GNNs to model the mutual ef-
fects of KCs during studentsâ€™ knowledge leveraging and changing,
constructing a more reasonable approach to knowledge tracing.
It is worth noting that some other GNN-based or memory-based
methods (e.g., GKT [ 19] and DKVMN [ 40]) also update mastery
between KCs. However, their knowledge state updating is still
potentially performed by the erase-followed-by-add mechanism,
which uses GRU/LSTM cells unable to solve the reasonability is-
sues such as not guaranteeing the direction of consistency change
between KCs.
3 PRELIMINARY
3.1 Task Formulation
Knowledge tracing aims to trace the dynamic evolution of studentsâ€™
knowledge mastery throughout their learning processes character-
ized by their responses to questions. Suppose there are a student
setU, a question setQ, and a KC setC. Each student ğ‘¢âˆˆU has
a historical response sequence Hğ‘¢={ğ‘Ÿğ‘¢
1,ğ‘Ÿğ‘¢
2,Â·Â·Â·,ğ‘Ÿğ‘¢
|Hğ‘¢|}, where
each response ğ‘Ÿğ‘¢
ğ‘¡= ğ‘ğ‘¢
ğ‘¡,ğ‘ğ‘¢
ğ‘¡,ğ‘ğ‘¢
ğ‘¡,ğ‘‡ğ‘¢
ğ‘¡comprises the involved ques-
tionğ‘ğ‘¢
ğ‘¡âˆˆQ, the correctness ğ‘ğ‘¢
ğ‘¡âˆˆ{0,1}(whereğ‘ğ‘¢
ğ‘¡=1means a
correct response), the KC ğ‘ğ‘¢
ğ‘¡âˆˆCexamined by the question, and
the timestamp ğ‘‡ğ‘¢
ğ‘¡of the response. It is worth noting that there
could be multiple KCs associated with one question. To be con-
cise, we use the notations with just one KC to describe the task
setting and the proposed method, but our method is easily ex-
tended to the setting of multiple KCs (e.g., averaging the KC repre-
sentations as mentioned in Section 4.3). The objective is to track
and monitor the evolving knowledge mastery of ğ‘¢after each re-
sponse,Mğ‘¢={mğ‘¢
1,mğ‘¢
2,Â·Â·Â·,mğ‘¢
|Hğ‘¢|}where mğ‘¢
ğ‘¡is stacked with
{ğ‘šğ‘¢
ğ‘ğ‘–,ğ‘¡|ğ‘ğ‘–âˆˆC} andğ‘šğ‘¢
ğ‘ğ‘–,ğ‘¡signifies the studentâ€™s knowledge mastery
of the KCğ‘ğ‘–at time stepğ‘¡. A higher value denotes a superior level of
mastery. However, the absence of annotated mastery levels neces-
sitates researchers to resort to the student performance prediction
task as a surrogate measure [ 17]. In this paradigm, given Hğ‘¢, the
objective is to predict whether student ğ‘¢can correctly answer a
new question ğ‘ğ‘¢
|Hğ‘¢|+1, with its associated KC ğ‘ğ‘¢
|Hğ‘¢|+1at timestamp
ğ‘‡ğ‘¢
|Hğ‘¢|+1. This hinges on the monotonicity assumption [ 7], which
posits that higher knowledge mastery leads to a higher probability
of answering questions correctly. For brevity, we omit the super-
scriptğ‘¢in the later method description.4 METHODOLOGY
As shown in Figure 2, GRKT conducts a recurrent modeling within a
three-stage learning process: knowledge retrieval, memory strength-
ening, and knowledge learning/forgetting. The proposed KC relation-
based graph neural networks capture knowledge mastery variation
between KCs throughout these stages. This section introduces the
KC relation-based GNNs first, then explains the three-stage learn-
ing process modeling with these GNNs. For ease of understanding
GRKT, we list and explain all relevant notations in Appendix A.
4.1 KC Relation-based Graph Neural Networks
Based on the transfer of learning theory [ 22], we introduce KC
relation-based GNNs to transfer the knowledge leveraging and
changing throughout the three-stage learning process, as shown in
Figure 2. To avoid repetition, we first elaborate on a prototype of
KC relation-based GNNs in this section and highlight differences
when applied to different stages in the subsequent sections.
Due to the lack of KC relation annotations, we follow previous
works [ 19,28] that construct KC relations based on the data statis-
tics. Details could be referred to in Appendix B. Besides, we focus
on the two most common relations, prerequisite and similarity and
extend three relation graphs P,S,R, whose edges denote one KC
being prerequisite/subsequent/relevant (similar) to another one.
This is because the forward and backward message passed along
the unidirectional prerequisite relation should be differentiated.
Based on this, we design the KC relation-based GNNs with multiple
layers. They receive KC node features such as knowledge memory,
knowledge gain/loss, or knowledge learnt in the three stages, which
would be introduced later. To capture the graph information, each
layer first aggregates the features of each nodeâ€™s neighbors for each
graphGâˆˆ{P,S,R}from the last layer as
Â¯fG,(ğ‘™)
ğ‘ğ‘–=1
|G(ğ‘ğ‘–)|âˆ‘ï¸
ğ‘ğ‘—âˆˆG(ğ‘ğ‘–)
ğ›½G
ğ‘ğ‘–,ğ‘ğ‘—Â·Ëœf(ğ‘™âˆ’1)
ğ‘ğ‘—WG,(ğ‘™)
ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ
(1)
ËœfG,(ğ‘™)
ğ‘ğ‘–=ReLU
Â¯fG,(ğ‘™)
ğ‘ğ‘–
OG,(ğ‘™)
ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ. (2)
where WG,(ğ‘™)
ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œâˆˆRğ‘‘ğ‘™âˆ’1Ã—ğ‘‘ğ‘™âˆ’1andOG,(ğ‘™)
ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œâˆˆRğ‘‘ğ‘™âˆ’1Ã—ğ‘‘ğ‘™are the learn-
able weight matrices in this layer. G(Â·) is the neighbor function of
G.ReLU(Â·)is an activation function to introduce non-linearity to
enhance model representability. ğ›½G
ğ‘ğ‘–,ğ‘ğ‘—is the correlation score of KC
ğ‘ğ‘–andğ‘ğ‘—on graphG, obtained by
ğ›½G
ğ‘ğ‘–,ğ‘ğ‘—=ğœ
kT
ğ‘ğ‘–WG
ğ‘ğ‘œğ‘Ÿkğ‘ğ‘—
. (3)
kğ‘ğ‘–,kğ‘ğ‘—âˆˆR1Ã—ğ‘‘ğ‘’are the two KCsâ€™ embeddings where ğ‘‘ğ‘’is the
number of embedding dimensions. WG
ğ‘ğ‘œğ‘ŸâˆˆRğ‘‘ğ‘’Ã—ğ‘‘ğ‘’is the trainable
matrix forG, andğœ(Â·)denotes the sigmoid function, which regular-
izes the score in(0,1). We then fuse the aggregated features from
the three graphs by
Ëœf(ğ‘™)
ğ‘ğ‘–=(Ã
Gâˆˆ{P,S,R}ËœfG,(ğ‘™)
ğ‘ğ‘–+Ëœf(ğ‘™âˆ’1)
ğ‘ğ‘–,ifğ‘‘ğ‘™âˆ’1=ğ‘‘ğ‘™,
Ã
Gâˆˆ{P,S,R}ËœfG,(ğ‘™)
ğ‘ğ‘–, ifğ‘‘ğ‘™âˆ’1â‰ ğ‘‘ğ‘™,(4)
where we apply a residual connection [ 29] whenğ‘‘ğ‘™âˆ’1=ğ‘‘ğ‘™to
stabilize the training process. In this prototype, we denote the
input features of all KCs as ËœF(0)âˆˆR|ğ¶|Ã—ğ‘‘0and one of them as
 
505Graph-based Reasonable Knowledge Tracing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Aggregated Memory 
à·œğ‘=ğœ( âˆ’ )
Stage1:  Knowledge RetrievalTarget questionâ€™s repr.
Target KCâ€™s memory
Knowledge LossKnowledge Gain
Stage2: Memory StrengtheningMastery
ProjectorTarget questionâ€™s repr
Target KCâ€™s memoryNext questionâ€™s repr
Next KCâ€™s memory
Knowledge LearntKnowledge Forgot
Stage3: Knowledge Learning/ForgettingLearn?
DeciderHidden Knowledge 
Memory
Mastery DifficultyKC Embeddings
Forgetting
Curve
Learning
Curve
Target questionâ€™s concept
 Target questionâ€™s binary correctness KC relation -based 
GNNs à·œğ‘ ğœ Estimated score Sigmoid functionRetrieval Updateğ’•âˆ’ğ’•
Update
ğ’•âˆ’,ğ’•,ğ’•+ğŸâˆ’Time stepHidden Knowledge 
MemoryHidden Knowledge 
Memoryğ’•+ğŸâˆ’
Mastery
Projector
Mastery
Knowledge TracingTrace
Single KC  Memory ğ’•âˆ’ğŸâˆ’ ğ’•+ğŸâˆ’Timeline Timeline
Figure 2: The entire framework of GRKT encompasses three recurrent stages: knowledge retrieval, memory strengthening, and
knowledge learning/forgetting.
Ëœf(0)
ğ‘ğ‘–âˆˆR1Ã—ğ‘‘0for KCğ‘ğ‘–, and the output features as ËœF(ğ¿)âˆˆR|ğ¶|Ã—ğ‘‘ğ¿
andËœf(ğ¿)
ğ‘ğ‘–âˆˆR1Ã—ğ‘‘ğ¿, whereğ‘‘0,ğ‘‘ğ¿are the numbers of input and output
feature dimensions. Then this prototype GNN is formulated as:
ËœF(ğ¿)=GNNğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ(ËœF(0)|ğ‘‘0,ğ‘‘1,Â·Â·Â·,ğ‘‘ğ¿) (5)
Ëœf(ğ¿)
ğ‘ğ‘–=GNNğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ(Ëœf(0)
ğ‘ğ‘–|ğ‘‘0,ğ‘‘1,Â·Â·Â·,ğ‘‘ğ¿). (6)
This prototype is then extended for different student learning stages
to construct reasonable knowledge tracing based on the transfer of
learning theory. Besides, the number of layers ğ¿controls the number
of hops the feature propagates on the graphs, which clarifies the
boundary between related and non-related KCs.
4.2 Knowledge Memory & Knowledge Tracing
GRKT aims to model the process of student retrieving and learning
knowledge with their memory. Therefore, we employ a dynamic
knowledge memory bank denoted as HâˆˆR|ğ¶|Ã—ğ‘‘ğ‘˜, where each
rowhğ‘ğ‘–encodes the current knowledge memory of KC ğ‘ğ‘–for the
student. Here, ğ‘‘ğ‘˜signifies the number of memory dimensions. This
memory bank evolves alongside the studentâ€™s learning process,
represented as Hğ‘¡, with a learnable initial state H0representing
their prior knowledge before engaging in any learning behavior.
To track the knowledge mastery of a specific KC, we apply a non-
negative projection vector wâ„âˆˆRğ‘‘ğ‘˜Ã—1
â‰¥0tohğ‘ğ‘–,ğ‘¡using the equation:
Ë†ğ‘šğ‘ğ‘–,ğ‘¡=hğ‘ğ‘–,ğ‘¡Â·wâ„, (7)
which yields the mastery of KC ğ‘ğ‘–at time step ğ‘¡. The non-negative
constraint on the network weights guarantees the monotonic re-
lationship between mastery and each memory dimension. This
technique has been widely adopted in numerous studies [ 31,32] to
satisfy the monotonicity assumption. Moreover, we leverage this
constraint to establish a foundation for reasonable knowledge trac-
ing, which would be gradually refined in subsequent descriptions.4.3 Stage I: Knowledge Retrieval
In this stage, students retrieve stored knowledge from memory
to solve given questions, a mechanism explained by memory the-
ory [ 18]. Additionally, the transfer of learning theory [ 22] suggests
that learners transfer knowledge from similar fields to tackle prob-
lems. Leveraging this insight, we employ a KC relation-based GNN
to model knowledge transfer from related KCs. Specifically, we
aggregate the knowledge memory of the given KC ğ‘ğ‘¡to solve its
corresponding question ğ‘ğ‘¡before time step ğ‘¡(represented as ğ‘¡âˆ’):
Ëœh(ğ¿)
ğ‘ğ‘¡,ğ‘¡âˆ’=GNNğ‘Ÿğ‘¡ğ‘£(Ëœh(0)
ğ‘ğ‘¡,ğ‘¡âˆ’|{ğ‘‘ğ‘˜}ğ¿+1), (8)
with initializing Ëœh(0)
ğ‘ğ‘¡,ğ‘¡âˆ’=hğ‘ğ‘¡,ğ‘¡âˆ’. Recognizing that different ques-
tions have different mastery requirements of KCs, we incorporate
question-KC correlation scores into the aggregation process in this
GNN, which are calculated by:
ğ›¼ğ‘ğ‘–,ğ‘ğ‘—=ğœ
eT
ğ‘ğ‘–Wğ‘Ÿğ‘’ğ‘kğ‘ğ‘—
, (9)
where eğ‘ğ‘–âˆˆRğ‘‘ğ‘’Ã—1andkğ‘ğ‘—are the embeddings of ğ‘ğ‘–andğ‘ğ‘—, and
Wğ‘Ÿğ‘’ğ‘âˆˆRğ‘‘ğ‘’Ã—ğ‘‘ğ‘’is a learnable matrix. Then, the graph message
aggregation process of Equation 8 is actually
ËœhG,(ğ‘™)
ğ‘ğ‘¡=1
|G(ğ‘ğ‘¡)|âˆ‘ï¸
ğ‘ğ‘–âˆˆG(ğ‘ğ‘¡)
ğ›¼ğ‘ğ‘¡,ğ‘ğ‘–Â·ğ›½G
ğ‘ğ‘¡,ğ‘ğ‘–Â·Ëœh(ğ‘™âˆ’1)
ğ‘ğ‘–WG,(ğ‘™)
ğ‘Ÿğ‘¡ğ‘£
.(10)
We also remove the non-linear feed-forward process and restrict
WG,(ğ‘™)
ğ‘Ÿğ‘¡ğ‘£âˆˆRğ‘‘ğ‘˜Ã—ğ‘‘ğ‘˜
â‰¥0to ensure higher values of the related KCsâ€™ mem-
ory bring higher knowledge mastery. After getting the aggregating
knowledge memory from this GNN, we get the knowledge mastery
as Equation 7 and compare it with the question difficulty ğ‘‘ğ‘ğ‘¡to
generate the predictive probability of solving the question:
Ë†ğ‘ğ‘¡=ğœ
Ëœh(ğ¿)
ğ‘ğ‘¡,ğ‘¡Â·wâ„âˆ’ğ‘‘ğ‘ğ‘¡
. (11)
 
506KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
For multi-KC questions, we average the KCsâ€™ memory. The difficulty
ğ‘‘ğ‘ğ‘¡of questionğ‘ğ‘¡is generated by a Multi-Layer Perception (MLP):
ğ‘‘ğ‘ğ‘¡=ReLU
Â¯eğ‘ğ‘¡W(1)
ğ‘‘ğ‘–ğ‘“ğ‘“+b(1)
ğ‘‘ğ‘–ğ‘“ğ‘“
W(2)
ğ‘‘ğ‘–ğ‘“ğ‘“+b(2)
ğ‘‘ğ‘–ğ‘“ğ‘“. (12)
Here, Â¯eğ‘ğ‘¡=[kğ‘ğ‘¡âŠ•eğ‘ğ‘¡]is the concatenated representation of ğ‘ğ‘¡and
its examined KC ğ‘ğ‘¡â€™s embeddings. For multi-KC questions, we use
the KCsâ€™ average embedding. W(1)
ğ‘‘ğ‘–ğ‘“ğ‘“âˆˆR2ğ‘‘ğ‘’Ã—ğ‘‘â„,W(2)
ğ‘‘ğ‘–ğ‘“ğ‘“âˆˆRğ‘‘â„Ã—1,
b(1)
ğ‘‘ğ‘–ğ‘“ğ‘“âˆˆR1Ã—ğ‘‘â„, and b(2)
ğ‘‘ğ‘–ğ‘“ğ‘“âˆˆR1Ã—1are learnable matrices and
vectors.ğ‘‘â„is the number of hidden dimensions. We denote the pro-
cess of this two-layer MLP as ğ‘‘ğ‘ğ‘¡=MLPğ‘‘ğ‘–ğ‘“ğ‘“(Â¯eğ‘ğ‘¡|2ğ‘‘ğ‘’,ğ‘‘â„,1), and a
similar notation is applied for brevity in subsequent descriptions.
Hereinafter, we accurately model the process whereby students
retrieve knowledge from memory to answer new questions.
4.4 Stage II: Memory Strengthening
The testing effect theory [ 24] reveals that a correct retrieval strength-
ens the storage of knowledge in memory, while an unsuccessful
retrieval can lead to incorrect strengthening. Without correction
or active learning after the error, this may reduce knowledge mas-
tery [ 14]. In this stage, we determine the memory strengthening
process based on whether the examined KC is correctly retrieved
to solve the question, resulting in either knowledge gain or loss.
Additionally, these knowledge changes are propagated to related
KCs based on the transfer of learning theory. To enhance memory
from a correct response to question ğ‘ğ‘¡, we first combine and input
the current memory hğ‘ğ‘¡,ğ‘¡âˆ’of KCğ‘ğ‘¡and the question information
Â¯eğ‘ğ‘¡into an MLP to obtain an initial memory feature:
gğ‘ğ‘¡,ğ‘¡=MLPğ‘”ğ‘ğ‘–ğ‘› [hğ‘ğ‘¡,ğ‘¡âˆ’âŠ•Â¯eğ‘ğ‘¡]|ğ‘‘ğ‘˜+2ğ‘‘ğ‘’,ğ‘‘â„,ğ‘‘ğ‘˜. (13)
For multi-KC question, we calculate all the associated KCsâ€™ features.
This feature serves as a spark to propagate knowledge changes
via another KC relation-based GNN. Specifically, by initializing an
input feature matrix ËœG(0)
ğ‘¡, where Ëœg(0)
ğ‘ğ‘–,ğ‘¡=gğ‘ğ‘¡,ğ‘¡ifğ‘ğ‘–=ğ‘ğ‘¡andËœg(0)
ğ‘ğ‘–,ğ‘¡=0
ifğ‘ğ‘–â‰ ğ‘ğ‘¡, the knowledge gain for all KCs is obtained as follows:
ËœG(ğ¿)
ğ‘¡=ReLU(GNNğ‘”ğ‘ğ‘–ğ‘›(ËœG(0)
ğ‘¡|{ğ‘‘ğ‘˜}ğ¿+1)). (14)
The ReLU(Â·)activation function ensures that the knowledge
gain to be positive. Moreover, due to the zero feature initialization
except for the examined KC, the knowledge gain is only propagated
to KCs within ğ¿hops, delineating a boundary between related and
unrelated KCs. Similarly, we could derive the negative knowledge
loss ËœL(ğ¿)
ğ‘¡when students provide incorrect responses and wrongly
strengthen their memory, by using a similar network GNN ğ‘™ğ‘œğ‘ ğ‘ (Â·).
Subsequently, we update the knowledge memory bank with
respect to the response ğ‘ğ‘¡as follows:
Hğ‘¡=Hğ‘¡âˆ’+ğ‘ğ‘¡ËœG(ğ¿)
ğ‘¡+(1âˆ’ğ‘ğ‘¡)ËœL(ğ¿)
ğ‘¡. (15)
It is worth noting that different questions also have different effects
on strengthening studentsâ€™ memory of KCs. Therefore, similar to
Equation 10, these two GNNs also add the question-KC correla-
tion scores during message passing. Henceforth, the second stage,
memory strengthening, is reasonably modeled based on the testing
effect and the transfer of learning.4.5 Stage III: Knowledge Learning/Forgetting
After students answer questions, their subsequent actions vary de-
pending on the feedback received. They may review their correct
answers or correct their mistakes. Besides, they might prepare for
the next questionâ€™s KC they would encounter. These active learn-
ing behaviors contribute to improving their knowledge mastery,
which we model as the knowledge learning process in this stage.
Concretely, the KC of the last question and the next question both
influence the studentâ€™s learning target. Therefore, we use an MLP
to determine if the student actively learns them based on his/her
current knowledge memory and the involved questionsâ€™ informa-
tion. For KC ğ‘ğ‘–âˆˆ{ğ‘ğ‘¡,ğ‘ğ‘¡+1}(or more involved KCs for multi-KC
questions), the two-dimension policy distribution is calculated by:
ğœ‹ğ‘ğ‘–,ğ‘¡=softmax MLPğ‘‘ğ‘ğ‘ ([hğ‘ğ‘–,ğ‘¡âŠ•Â¯eğ‘ğ‘¡âŠ•Â¯eğ‘ğ‘¡+1]|ğ‘‘ğ‘˜+4ğ‘‘ğ‘’,ğ‘‘â„,2).
(16)
Here, argmaxğœ‹ğ‘ğ‘–,ğ‘¡=0indicates that the first dimension is bigger.
We suppose there is no active learning. Contrarily, argmaxğœ‹ğ‘ğ‘–,ğ‘¡=1
indicates the student would learn ğ‘ğ‘–. Under this circumstance, we
calculate the progress of learning ğ‘ğ‘–in a similar way:
pğ‘ğ‘–,ğ‘¡=MLPğ‘ğ‘Ÿğ‘”([hğ‘ğ‘–,ğ‘¡âŠ•Â¯eğ‘ğ‘¡âŠ•Â¯eğ‘ğ‘¡+1]|ğ‘‘ğ‘˜+4ğ‘‘ğ‘’,ğ‘‘â„,ğ‘‘ğ‘˜)).(17)
Based on the transfer of learning theory, this progress is also prop-
agated to related KCs using another KC relation-based GNN. Af-
ter initializing ËœP(0)
ğ‘¡where Ëœp(0)
ğ‘ğ‘–,ğ‘¡=pğ‘ğ‘–,ğ‘¡forğ‘ğ‘–âˆˆ {ğ‘ğ‘¡,ğ‘ğ‘¡+1}with
argmaxğœ‹ğ‘ğ‘–,ğ‘¡=1, and Ëœp(0)
ğ‘ğ‘–,ğ‘¡=0otherwise, we compute
ËœP(ğ¿)
ğ‘¡=ReLU(GNNğ‘ğ‘Ÿğ‘”(ËœP(0)
ğ‘¡|{ğ‘‘ğ‘˜}ğ¿+1)). (18)
This active learning process continues until the student answers the
next question, allowing us to model each KCâ€™s progress Ëœp(ğ¿)
ğ‘ğ‘–,ğ‘¡,ğ‘ğ‘–âˆˆC
with a KC-specific time-aware kernel function to update:
hğ‘ğ‘–,(ğ‘¡+1)âˆ’=hğ‘ğ‘–,ğ‘¡+ğ“ğ‘ğ‘–(Ëœp(ğ¿)
ğ‘ğ‘–,ğ‘¡,Î”ğ‘‡ğ‘¡+1) (19)
where Î”ğ‘‡ğ‘¡+1=ğ‘‡ğ‘¡+1âˆ’ğ‘‡ğ‘¡is the time duration until the next question.
According to the learning curve [ 36], the efficiency of students
in learning a specific KC tends to be high initially and gradually
decreases over both the learning time and frequency. Therefore, we
design the kernel function in an exponential form:
ğ“ğ‘ğ‘–(Ëœp(ğ¿)
ğ‘ğ‘–,ğ‘¡,Î”ğ‘‡ğ‘¡+1)=Ëœp(ğ¿)
ğ‘ğ‘–,ğ‘¡âŠ™(1âˆ’exp(âˆ’(ğ‘›ğ‘ğ‘–,ğ‘¡+1)Î”ğ‘‡ğ‘¡+1Â·Ëœğœ¸(ğ¿)
ğ‘ğ‘–)),(20)
whereâŠ™is the Hadamard product. ğ‘›ğ‘ğ‘–,ğ‘¡is the number of times that
ğ‘ğ‘–has been learned by the student. Ëœğœ¸(ğ¿)
ğ‘ğ‘–represents the KC-specific
kernel parameters of ğ‘ğ‘–generated by another KC relation-based
GNN. It leverages the property of graph homophily that makes
related KCs have similar learning ratios:
Ëœğœ¸(ğ¿)
ğ‘ğ‘–=softplus(GNNğ‘™ğ‘Ÿğ‘›(Ëœğœ¸(0)
ğ‘ğ‘–|ğ‘‘ğ‘’,{ğ‘‘ğ‘˜}ğ¿)), (21)
with initializing Ëœğœ¸(0)
ğ‘ğ‘–=kğ‘ğ‘–which isğ‘ğ‘–â€™s embedding. Here, softplus(Â·)
is an activation function to restrict the parameter to be positive.
On the other hand, for KCs that students have acquired before but
they do not choose to learn, we introduce the knowledge forgetting
process. Therefore, for the KCs students do not make progress on
(i.e., Ëœp(ğ¿)
ğ‘ğ‘–,ğ‘¡=0), their previously acquired knowledge fades over
time:
hğ‘ğ‘–,(ğ‘¡+1)âˆ’=hğ‘ğ‘–,ğ‘¡âˆ’ğœ¿ğ‘ğ‘–(Î”hğ‘ğ‘–,ğ‘¡,Î”ğ‘‡ğ‘¡+1) (22)
 
507Graph-based Reasonable Knowledge Tracing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Statistics of the three preprocessed datasets.
Dataset ASSIST09 ASSIST12 Junyi
#response 0.4m 2.6m 25.4m
#sequence 7.4k 38.1k 325.4k
#question 13.5k 51.0k 2.8k
#concept 140 198 722
#concept/question 1.22 1.0 1.0
where Î”hğ‘ğ‘–,ğ‘¡=hğ‘ğ‘–,ğ‘¡âˆ’hğ‘ğ‘–,0represents the total knowledge ac-
quisition the student has accumulated. According to the forget-
ting curve [ 6], the speed that students forget knowledge follows a
pattern of initially rapid decay and then a gradual decrease over
time and the review frequency. Therefore, we similarly design KC-
specific forgetting kernel functions in an exponential form:
ğœ¿ğ‘ğ‘–(Î”hğ‘ğ‘–,ğ‘¡,Î”ğ‘‡ğ‘¡+1)=Î”hğ‘ğ‘–,ğ‘¡âŠ™(1âˆ’exp(âˆ’(ğ‘›ğ‘ğ‘–,ğ‘¡+1)Î”ğ‘‡ğ‘¡+1Â·Ëœğœ½(ğ¿)
ğ‘ğ‘–)),
(23)
where the kernel parameters Ëœğœ½(ğ¿)
ğ‘ğ‘–are similarly generated by an-
other KC relation-based GNN:
Ëœğœ½(ğ¿)
ğ‘ğ‘–=softplus(GNNğ‘“ğ‘”ğ‘¡(Ëœğœ½(0)
ğ‘ğ‘–|ğ‘‘ğ‘’,{ğ‘‘ğ‘˜}ğ¿)) (24)
with initializing Ëœğœ½(0)
ğ‘ğ‘–=kğ‘ğ‘–. Consequently, based on the learning
and forgetting curves, we have derived the updated knowledge
memory H(ğ‘¡+1)âˆ’in this stage, which is recursively used for an-
swering the next question.
4.6 Model Training
The three-stage modeling is recurrent along the student response
sequence. After learning/forgetting knowledge in the third stage,
the updated knowledge memory is prepared for the first stage to
answer the next question. This makes GRKT an end-to-end style
so we directly train the model by the binary cross-entropy loss,
aligning the predictive probability Ë†ğ‘ğ‘¢
ğ‘¡from Equation 11 with the
ground-truth response correctness label ğ‘ğ‘¢
ğ‘¡:
L=âˆ’âˆ‘ï¸
ğ‘¢âˆˆUâˆ‘ï¸
ğ‘Ÿğ‘¢
ğ‘¡âˆˆHğ‘¢ğ‘ğ‘¢
ğ‘¡logË†ğ‘ğ‘¢
ğ‘¡+(1âˆ’ğ‘ğ‘¢
ğ‘¡)log(1âˆ’Ë†ğ‘ğ‘¢
ğ‘¡). (25)
Here, we omit the averaging notation for brevity. Besides, we also
apply theğ‘™2normalization to the model parameters during the
training process to avoid the over-fitting issue.
5 EXPERIMENTS
In this section, we design comprehensive experiments to address
the following research questions:
Q1: Does GRKT achieve competitive results in terms of both pre-
diction performance and knowledge tracing reasonability
compared to current state-of-the-art DLKT methods?
Q2: What are the roles and impacts of different components of
GRKT on the overall performance and reasonability?
Q3: How reasonable is the knowledge mastery traced by GRKT
from an intuitive perspective?
Additionally, we conduct other experiments such as hyper-parameter
analysis. Due to space constraints, we include them in Appendix C.4.5.1 Experimental Setup
5.1.1 Datasets. We evaluate the performance of GRKT on three
widely-used public KT datasets:
â€¢ASSIST09 [8]1: ASSISTments is an online tutoring system for
mathematics, which collected this dataset from 2009 to 2010. We
use the combined version. For the missing timestamp information,
we approximate it using the field order_id.
â€¢ASSIST12 [8]2: Another dataset from ASSISTments, collected
during the period of 2012 to 2013.
â€¢Junyi [1]3: This dataset is collected from the Junyi Academy
online platform in 2015. It contains a part of annotated KC rela-
tionships which are suitable for the requirements of GRKT. We
use the junyi_ProblemLog_original.csv version.
For preprocessing each dataset, we partition the response se-
quences of every student into subsequences, each containing 100
responses. Subsequences containing fewer than 10 responses are
eliminated, while those with less than 100 responses are padded
with zeros to meet the required length. Statistics of the processed
datasets can be found in Table 1.
5.1.2 Evaluation. As a binary classification task of predicting stu-
dent responses, we utilize the area under the curve (AUC) and ac-
curacy (ACC) as the evaluation metrics for prediction performance.
For evaluating model reasonability, we introduce three metrics:
â€¢Consistency: We propose this metric to measure the ratio of
consistent variation between the mastery of KCs. When a stu-
dentâ€™s mastery of the corresponding KC declines after answering
a certain question, the mastery of other KCs should either decline
(for related KCs) or remain unchanged (for unrelated KCs). We
calculate this percentage.
â€¢GAUCM: This metric calculates the average AUC scores with
respect to the mastery of each questionâ€™s examined KC. Its reflects
the monotonicity assumption: a question could be more likely to
be correctly answered if students have higher mastery of its KC.
This metric is proposed by Zhang et al. [41].
â€¢Repetition: This metric is proposed by Yeung et al. [ 37], stating
that a reasonable KT method should satisfy: after a student has
finished a question and is given this same question again, the
response result (correct or incorrect) should remain the same.
We calculate the accuracy under this circumstance.
The formulas of these metrics are presented in Appendix C.1. More-
over, we employ a five-fold cross-validation to assess the modelâ€™s
performance. 10% of the sequences of each fold serve as the val-
idation set for parameter tuning. We stop the training when the
validation performance fails to improve for 10 consecutive epochs.
5.1.3 Baselines. To compare with mainstream DLKT methods cov-
ering different aspects, we select eleven baselines from 2015 to 2023,
including DKT [ 23], DKVMN [ 40], DKT+ [ 37], SAKT [ 20], GKT [ 19],
AKT [ 11], SKT [ 30], LPKT [ 27], DIMKT [ 26], Dtransformer [ 38] and
LBKT [ 34]. Among them, GKT and SKT leverages the KC graph,
and LPKT leverage the timestamp information. DKT+, LPKT and
Dtransformer consider some aspects of model reasonability: the
1https://sites.google.com/site/assistmentsdata/home/2009-2010-assistment-data
2https://sites.google.com/site/assistmentsdata/home/2012-13-school-data-with-
affect
3https://pslcdatashop.web.cmu.edu/Files?datasetId=1198
 
508KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
Table 2: Results of the main experiments. The best results among GRKT and the baselines are in bold. The second ones are in
italic. * indicates statistical significance over the best baseline, measured by T-test with p-value â‰¤0.05. â€œCONSâ€, â€œGAUCâ€ and
â€œRPTâ€ are short for the three metrics for reasonability, consistency, GAUCM and Repetition.
Dataset ASSIST09 ASSIST12 Junyi
Metric AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT
DKT 0.7695 0.7246 0.6463 0.7172 0.8131 0.7303 0.7358 0.6772 0.6929 0.7955 0.8003 0.8541 0.7432 0.6415 0.8790
DKVMN 0.7680 0.7239 0.8708 0.7116 0.8061 0.7279 0.7349 0.9273 0.6729 0.7971 0.8004 0.8541 0.9455 0.6379 0.8780
DKT+ 0.7707 0.7245 0.6364 0.7089 0.8395 0.7300 0.7353 0.6809 0.6766 0.8172 0.7993 0.8539 0.7624 0.6436 0.8869
SAKT 0.7634 0.7206 0.8539 0.7101 0.7749 0.7227 0.7329 0.8202 0.6866 0.7797 0.7995 0.8535 0.8600 0.6387 0.8747
GKT 0.7702 0.7252 0.6697 0.7183 0.8124 0.7339 0.7372 0.7450 0.6971 0.7986 0.8023 0.8547 0.7403 0.6398 0.8788
AKT 0.7820 0.7320 0.5870 0.7113 0.8184 0.7665 0.7514 0.5909 0.6892 0.8172 0.8161 0.8593 0.5810 0.6398 0.8734
SKT 0.7732 0.7273 0.7023 0.7098 0.8092 0.7354 0.7398 0.7813 0.6952 0.7934 0.8045 0.8552 0.7792 0.6420 0.8805
LPKT 0.7869 0.7369 0.7909 0.7124 0.8205 0.7740 0.7556 0.8174 0.6839 0.8255 0.8153 0.8585 0.7238 0.6453 0.8845
DIMKT 0.7814 0.7351 0.7899 0.7153 0.8221 0.7711 0.7550 0.8099 0.6995 0.8198 0.8163 0.8594 0.8945 0.6424 0.8850
DTrans 0.7858 0.7345 0.8928 0.7126 0.8253 0.7720 0.7542 0.9217 0.6863 0.8249 0.8149 0.8577 0.9274 0.6420 0.8893
LBKT 0.7865 0.7372 0.8054 0.7134 0.8225 0.7763 0.7562 0.8123 0.6814 0.8230 0.8140 0.8568 0.8123 0.6409 0.8871
GRKT 0.7914* 0.7398* 1.0000* 0.7209* 0.8486* 0.7794* 0.7576 1.0000* 0.7064* 0.8319* 0.8207* 0.8624* 1.0000* 0.6473* 0.8957*
improv. 0.57% 0.35% 12.01% 0.36% 1.09% 0.40% 0.19% 8.50% 0.98% 0.78% 0.54% 0.35% 7.83% 0.31% 0.72%
Table 3: Results of the ablation experiments.
Dataset ASSIST09 ASSIST12 Junyi
Metric AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT
GRKT 0.7914 0.7398 1.0000 0.7209 0.8486 0.7794 0.7576 1.0000 0.7064 0.8319 0.8207 0.8624 1.0000 0.6473 0.8957
-LF 0.7871 0.7367 1.0000 0.7066 0.8243 0.7767 0.7558 1.0000 0.6809 0.8276 0.8170 0.8598 1.0000 0.6401 0.8815
-SIM-PRE 0.7578 0.7161 1.0000 0.6197 0.8246 0.7502 0.7424 1.0000 0.6223 0.8291 0.7921 0.8481 1.0000 0.6084 0.8781
-SIM 0.7896 0.7375 1.0000 0.7135 0.8402 0.7777 0.7564 1.0000 0.6888 0.8259 0.8186 0.8611 1.0000 0.6447 0.8862
-PRE 0.7897 0.7384 1.0000 0.7149 0.8437 0.7779 0.7563 1.0000 0.6915 0.8264 0.8191 0.8615 1.0000 0.6452 0.8897
knowledge tracing stability or learning/forgetting behaviors, but
not comprehensively address the DLKT unreasonableness issue. For
the methods not providing the proxy of tracing knowledge mastery,
AKT and DIMKT, we follow previous works [ 4,16] that replace
input question features with zeros to estimate the mastery. We note
that cognitive diagnosis baselines are not considered because they
usually focus on static testing environments [ 15] but we study in
the dynamic learning situation.
5.1.4 Implementation Details. We employ the Adam optimizer [ 13]
for all methods to achieve their best performance. We choose their
learning rates from {1e-2, 5e-3, 1e-3, 5e-4, 1e-4}, and fixed the em-
bedding and hidden dimension numbers at 128 for fairness. We
strictly follow the original papers of all methods to set their hyper-
parameters. For GRKT, detailed hyper-parameter setting is referred
in the Appendix C.2. Furthermore, for the non-negative constraint
on the specified network weights in Equations 7 and 10, we use
the softmax operation along the knowledge memory dimension,
which performs best in practice. Besides, the Junyi dataset includes
some labeled relations, which we experiment with and present the
results in Appendix C.3.
5.2 Overall Performance (Q1)
Table 2 illustrates the comprehensive performance comparison be-
tween GRKT and eleven other baselines. Notably, GRKT showcases
the highest efficacy, surpassing the leading baselines by marginsranging from 0.19% to 12.01% across both prediction performance
and reasonability metrics. For metrics such as AUC and ACC, which
primarily gauge predictive accuracy, the state-of-the-art DLKT tech-
niques, LPKT, and DIMKT exhibit exemplary performance owing
to their sophisticated neural architectures. Besides, methods that
emphasize aspects of reasonability, such as enhancing knowledge
tracing stability and explicitly modeling learning and forgetting be-
haviors, DKT+, LPKT, and DTransformer, demonstrate competitive
performance across reasonableness metrics. These methods secure
seven out of nine second-place positions in reasonability metrics.
Remarkably, GRKT achieves a perfect score of 1.0 on the consistency
metric, signifying its ability to effectively address the challenge of
maintaining consistency in knowledge mastery changes across KCs
by the network constraints.
5.3 Ablation Study (Q2)
The ablation study aims to evaluate the impact of each component
in GRKT by removing specific techniques and comparing the results
with the full model. Four components are removed:
â€¢-LF: Removal of the third stage, knowledge learning/forgetting.
â€¢-SIM: Removal of the similarity relation.
â€¢-PRE: Removal of the prerequisite relation.
â€¢-SIM-PRE: Removal of the leverage of KC relation graphs.
As shown in Table 3, GRKT-SIM-PRE experiences the most sig-
nificant deterioration, emphasizing the crucial role of KC relations
in the KT task. Moreover, when only one of these two relations is
 
509Graph-based Reasonable Knowledge Tracing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Area TriangleTimeline
Calculations with similar Figures Conversion of Fraction Decimals Percents Ordering Integers Multiplication and Division Positive DecimalsGRKT
Forgetting Curve
Learning CurveTesting Effect
No Mastery Change of Unrelated KCConsistent Mastery Change Direction
Mastery Change of Related KC :
Transfer  of Learnin g
Figure 3: Case study of the same studentâ€™s evolving knowledge mastery exemplified in Section 1.
GRKT
LPKT
DKTTimeline testing effect forgetting curve
no mastery change of related KCs mastery change of unrelated KCs
Addition and Subtraction Integers Multiplication and Division Integers Square Rootshigh mastery
low mastery
Correct Responses
Incorrect Responses
Figure 4: Knowledge tracing heatmap of GRKT, LPKT and DKT tracing one another studentâ€™s mastery on KC Addition and
Subtraction Integer s. Different colors represent different KCs.
utilized, there is a notable improvement in performance, indicating
that each provides meaningful information for GRKT. Moreover,
the performance is further enhanced when both relations are used
together. Additionally, the degradation of GRKT-LF underscores the
importance of modeling the knowledge learning/forgetting stage.
5.4 Reasonable Knowledge Tracing (Q3)
To intuitively validate the resonability of GRKT, we present one
studentâ€™s dynamic knowledge mastery traced by GRKT in Figure 3.
As depicted, the result aligns well with our hypothesis of a com-
prehensive and reasonable knowledge tracing model integrating
various effects based on pedagogical theories. Furthermore, it ad-
dresses three key issues in the reasonableness of existing DLKT
methods: mastery changes of unrelated KCs, not mastery changes
of related KCs, and inconsistent mastery change direction. We also
present GRKT, LPKT and DKT tracing one another studentâ€™s mas-
tery on KC Addition and Subtraction Integers in Figure 4. As shown,
GRKT yields reasonable knowledge tracing results such as the fine-
grained knowledge changing from testing effects and the faded
knowledge with forgetting curves. LPKT and DKT still have rea-
sonable issues such as the mastery change of unrelated KCs and no
mastery change of related KCs.
5.5 Complexity Analysis
Although the detailed methodology description of GRKT, its inter-
nal composition of only GNNs and MLPs does not make the infer-
ence complicated. Suppose ğ‘¡is the length of response sequence,
ğ¶is the KC set, ğ¸is the KC relation edge set, ğ‘‘is the hidden di-
mension number we set as a small value of 16, and ğ‘˜is the GRKTâ€™smemory dimension number. The time complexity of GRKT is then
ğ‘‚(ğ‘¡|ğ¸|ğ‘˜+|ğ¸|ğ‘‘+ğ‘¡|ğ¶|ğ‘˜2+|ğ¶|ğ‘‘2+ğ‘¡ğ‘‘2), consisting of feature ag-
gregationğ‘‚(ğ‘¡|ğ¸|ğ‘˜+|ğ¸|ğ‘‘)and feature non-linear transformation
ğ‘‚(ğ‘¡|ğ¶|ğ‘˜2+|ğ¶|ğ‘‘2)of the GNNs, and ğ‘‚(ğ‘¡ğ‘‘2)of the MLPs. In contrast,
other comparable attention or RNN-based methods usually have
time complexity ğ‘‚(ğ‘¡ğ‘‘2+ğ‘¡2ğ‘‘). In real scenarios, ğ‘¡,|ğ¶|,ğ‘‘usually lie
in 100-200 and the KC relation graphs are sparse. Therefore, we
can approximately assume ğ‘¡=ğ‘‘=|ğ¶|=ğ‘˜2=ğ‘›and|ğ¸|=ğ‘˜Â·|ğ¶|to
facilitate the complexity comparison, which indicates the GRKTâ€™s
time complexity is actually in the same order of magnitude ğ‘‚(ğ‘›3)
as other methods. We also test the inference speed of GRKT. It av-
eragely costs 60ms for one student, which is acceptable in practice.
6 CONCLUSION
In this paper, we point out the issue that many existing DLKT
approaches prioritize predictive accuracy over tracking studentsâ€™
dynamic knowledge mastery. This often results in models that
yield unreasonable outcomes, complicating their application in
real teaching scenarios. To this end, our study introduces GRKT, a
graph-based reasonable knowledge tracing. It employs graph neural
networks and consists of a finer-grained three-stage modeling pro-
cess based on pedagogical theories, conducting a more reasonable
knowledge tracing. Extensive experiments across multiple datasets
demonstrate that GRKT not only enhances predictive accuracy but
also generates more reasonable knowledge tracing results. In the
future, we plan to address certain limitations of GRKT, such as en-
hancing the modelâ€™s ability to provide more fine-grained responses,
including multiple-choice or essay answers. Furthermore, we would
evaluate GRKT in real teaching scenarios.
 
510KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
REFERENCES
[1]Haw-Shiuan Chang, Hwai-Jung Hsu, and Kuan-Ta Chen. 2015. Modeling Exercise
Relationships in E-Learning: A Unified Approach.. In EDM. 532â€“535.
[2]Youngduck Choi, Youngnam Lee, Junghyun Cho, Jineon Baek, Byungsoo Kim,
Yeongmin Cha, Dongmin Shin, Chan Bae, and Jaewe Heo. 2020. Towards an ap-
propriate query, key, and value computation for knowledge tracing. In Proceedings
of the seventh ACM conference on learning@ scale. 341â€“344.
[3]Albert T Corbett and John R Anderson. 1994. Knowledge tracing: Modeling the
acquisition of procedural knowledge. User modeling and user-adapted interaction
4 (1994), 253â€“278.
[4]Jiajun Cui, Zeyuan Chen, Aimin Zhou, Jianyong Wang, and Wei Zhang. 2023. Fine-
Grained Interaction Modeling with Multi-Relational Transformer for Knowledge
Tracing. ACM Transactions on Information Systems 41, 4 (2023), 1â€“26.
[5]Jiajun Cui, Minghe Yu, Bo Jiang, Aimin Zhou, Jianyong Wang, and Wei Zhang.
2024. Interpretable Knowledge Tracing via Response Influence-based Counter-
factual Reasoning. In Proceedings of the 40th IEEE International Conference on
Data Engineering.
[6]Hermann Ebbinghaus. 1885. Ãœber das gedÃ¤chtnis: untersuchungen zur experi-
mentellen psychologie. Duncker & Humblot.
[7]Susan E Embretson and Steven P Reise. 2013. Item response theory. Psychology
Press.
[8]Mingyu Feng, Neil Heffernan, and Kenneth Koedinger. 2009. Addressing the
assessment challenge with an online system that tutors as it assesses. User
modeling and user-adapted interaction 19 (2009), 243â€“266.
[9]Wenbin Gan, Yuan Sun, and Yi Sun. 2022. Knowledge structure enhanced graph
representation learning model for attentive knowledge tracing. International
Journal of Intelligent Systems 37, 3 (2022), 2012â€“2045.
[10] Weibo Gao, Hao Wang, Qi Liu, Fei Wang, Xin Lin, Linan Yue, Zheng Zhang,
Rui Lv, and Shijin Wang. 2023. Leveraging transferable knowledge concept
graph embedding for cold-start cognitive diagnosis. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 983â€“992.
[11] Aritra Ghosh, Neil Heffernan, and Andrew S Lan. 2020. Context-aware atten-
tive knowledge tracing. In Proceedings of the 26th ACM SIGKDD international
conference on knowledge discovery & data mining. 2330â€“2339.
[12] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca
Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black
box models. ACM computing surveys (CSUR) 51, 5 (2018), 1â€“42.
[13] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[14] Nate Kornell, Matthew Jensen Hays, and Robert A Bjork. 2009. Unsuccessful
retrieval attempts enhance subsequent learning. Journal of Experimental Psychol-
ogy: Learning, Memory, and Cognition 35, 4 (2009), 989.
[15] Jacqueline Leighton and Mark Gierl. 2007. Cognitive diagnostic assessment for
education: Theory and applications. Cambridge University Press.
[16] Qi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui Xiong, Yu Su, and Guoping Hu.
2019. Ekt: Exercise-aware knowledge tracing for student performance prediction.
IEEE Transactions on Knowledge and Data Engineering 33, 1 (2019), 100â€“115.
[17] Qi Liu, Shuanghong Shen, Zhenya Huang, Enhong Chen, and Yonghe Zheng.
2021. A survey of knowledge tracing. arXiv preprint arXiv:2105.15106 (2021).
[18] Arthur W Melton. 1963. Implications of short-term memory for a general theory
of memory. Journal of verbal Learning and verbal Behavior 2, 1 (1963), 1â€“21.
[19] Hiromi Nakagawa, Yusuke Iwasawa, and Yutaka Matsuo. 2019. Graph-based
knowledge tracing: modeling student proficiency using graph neural network.
InIEEE/WIC/ACM International Conference on Web Intelligence. 156â€“163.
[20] Shalini Pandey and George Karypis. 2019. A Self-Attentive Model for Knowledge
Tracing. International Educational Data Mining Society (2019).
[21] Zachary A Pardos and Neil T Heffernan. 2011. KT-IDEM: Introducing item
difficulty to the knowledge tracing model. In User Modeling, Adaption and Per-
sonalization: 19th International Conference, UMAP 2011, Girona, Spain, July 11-15,
2011. Proceedings 19. Springer, 243â€“254.
[22] David N Perkins, Gavriel Salomon, et al .1992. Transfer of learning. International
encyclopedia of education 2 (1992), 6452â€“6457.
[23] Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami,
Leonidas J Guibas, and Jascha Sohl-Dickstein. 2015. Deep knowledge tracing.
Advances in neural information processing systems 28 (2015).
[24] Henry L Roediger III and Jeffrey D Karpicke. 2006. Test-enhanced learning:
Taking memory tests improves long-term retention. Psychological science 17, 3
(2006), 249â€“255.
[25] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2008. The graph neural network model. IEEE transactions on neural
networks 20, 1 (2008), 61â€“80.
[26] Shuanghong Shen, Zhenya Huang, Qi Liu, Yu Su, Shijin Wang, and Enhong Chen.
2022. Assessing Studentâ€™s Dynamic Knowledge State by Exploring the Question
Difficulty Effect. In Proceedings of the 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 427â€“437.[27] Shuanghong Shen, Qi Liu, Enhong Chen, Zhenya Huang, Wei Huang, Yu Yin, Yu
Su, and Shijin Wang. 2021. Learning process-consistent knowledge tracing. In
Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data
mining. 1452â€“1460.
[28] Xiangyu Song, Jianxin Li, Qi Lei, Wei Zhao, Yunliang Chen, and Ajmal Mian. 2022.
Bi-CLKT: Bi-graph contrastive learning based knowledge tracing. Knowledge-
Based Systems 241 (2022), 108274.
[29] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. 2017.
Inception-v4, inception-resnet and the impact of residual connections on learning.
InProceedings of the AAAI conference on artificial intelligence, Vol. 31.
[30] Shiwei Tong, Qi Liu, Wei Huang, Zhenya Hunag, Enhong Chen, Chuanren Liu,
Haiping Ma, and Shijin Wang. 2020. Structure-based knowledge tracing: An
influence propagation view. In 2020 IEEE international conference on data mining
(ICDM). IEEE, 541â€“550.
[31] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yu Yin, Shijin Wang, and Yu Su.
2022. NeuralCD: a general framework for cognitive diagnosis. IEEE Transactions
on Knowledge and Data Engineering (2022).
[32] Xinping Wang, Caidie Huang, Jinfang Cai, and Liangyu Chen. 2021. Using knowl-
edge concept aggregation towards accurate cognitive diagnosis. In Proceedings of
the 30th ACM International Conference on Information & Knowledge Management.
2010â€“2019.
[33] Siyu Wu, Yang Cao, Jiajun Cui, Runze Li, Hong Qian, Bo Jiang, and Wei Zhang.
2024. A Comprehensive Exploration of Personalized Learning in Smart Education:
From Student Modeling to Personalized Recommendations. arXiv:2402.01666
[34] Bihan Xu, Zhenya Huang, Jiayu Liu, Shuanghong Shen, Qi Liu, Enhong Chen,
Jinze Wu, and Shijin Wang. 2023. Learning behavior-oriented knowledge tracing.
InProceedings of the 29th ACM SIGKDD conference on knowledge discovery and
data mining. 2789â€“2800.
[35] Yang Yang, Jian Shen, Yanru Qu, Yunfei Liu, Kerong Wang, Yaoming Zhu, Weinan
Zhang, and Yong Yu. 2021. GIKT: a graph-based interaction model for knowledge
tracing. In Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2020, Ghent, Belgium, September 14â€“18, 2020, Proceedings,
Part I. Springer, 299â€“315.
[36] Louis E Yelle. 1979. The learning curve: Historical review and comprehensive
survey. Decision sciences 10, 2 (1979), 302â€“328.
[37] Chun-Kit Yeung and Dit-Yan Yeung. 2018. Addressing two problems in deep
knowledge tracing via prediction-consistent regularization. In Proceedings of the
fifth annual ACM conference on learning at scale. 1â€“10.
[38] Yu Yin, Le Dai, Zhenya Huang, Shuanghong Shen, Fei Wang, Qi Liu, Enhong
Chen, and Xin Li. 2023. Tracing Knowledge Instead of Patterns: Stable Knowledge
Tracing with Diagnostic Transformer. In Proceedings of the ACM Web Conference
2023. 855â€“864.
[39] Michael V Yudelson, Kenneth R Koedinger, and Geoffrey J Gordon. 2013. Individ-
ualized bayesian knowledge tracing models. In Artificial Intelligence in Education:
16th International Conference, AIED 2013, Memphis, TN, USA, July 9-13, 2013.
Proceedings 16. Springer, 171â€“180.
[40] Jiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan Yeung. 2017. Dynamic key-
value memory networks for knowledge tracing. In Proceedings of the 26th inter-
national conference on World Wide Web. 765â€“774.
[41] Moyu Zhang, Xinning Zhu, Chunhong Zhang, Wenchen Qian, Feng Pan, and
Hui Zhao. 2023. Counterfactual Monotonic Knowledge Tracing for Assessing
Studentsâ€™ Dynamic Mastery of Knowledge Concepts. In Proceedings of the 32nd
ACM International Conference on Information and Knowledge Management. 3236â€“
3246.
A NOTATION TABLE
We list and explain the notations in our methodology introduction
in Table 4 and 5.
B METHOD DETAILS
B.1 KC Relation Graph Construction
In the absence of KC relation annotations in the datasets, we con-
struct the KC relation graph based on data statistics. For the sim-
ilarity between KCs ğ‘ğ‘–andğ‘ğ‘—, we estimate their similarity score
using:
ğ‘ ğ‘–ğ‘šğ‘ğ‘–,ğ‘ğ‘—=Ã
ğ‘¢âˆˆUÃ
ğ‘Ÿğ‘¢
ğ‘¡,ğ‘Ÿğ‘¢
ğ‘¡â€²âˆˆHğ‘¢ğ¼(ğ‘ğ‘¢
ğ‘¡=ğ‘ğ‘¢
ğ‘¡â€²,ğ‘ğ‘¢
ğ‘¡=ğ‘ğ‘–,ğ‘ğ‘¢
ğ‘¡â€²=ğ‘ğ‘—)
Ã
ğ‘¢âˆˆUÃ
ğ‘Ÿğ‘¢
ğ‘¡,ğ‘Ÿğ‘¢
ğ‘¡â€²âˆˆHğ‘¢ğ¼(ğ‘ğ‘¢
ğ‘¡=ğ‘ğ‘–,ğ‘ğ‘¢
ğ‘¡â€²=ğ‘ğ‘—),
whereğ¼(Â·)is the indicator function that takes value 1 if the condi-
tion is satisfied. This approximates the probability that a student
 
511Graph-based Reasonable Knowledge Tracing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 4: The notation table of GRKT. We omit the superscript
of the target student ğ‘¢whose knowledge is to be traced.
Task formulation
U,Q,C sets of students, questions, KCs
ğ‘ğ‘–,ğ‘ğ‘—,ğ‘ğ‘–,ğ‘ğ‘— certain KCs, questions
ğ‘¢,ğ‘¡ the target student, time step
H response history of ğ‘¢
ğ‘Ÿğ‘¡ response of ğ‘¢atğ‘¡
ğ‘ğ‘¡,ğ‘ğ‘¡ question and examined KC of ğ‘Ÿğ‘¡
ğ‘ğ‘¡,ğ‘‡ğ‘¡ binary correctness and timestamp of ğ‘Ÿğ‘¡
M evolving knowledge mastery of ğ‘¢
mğ‘¡ knowledge mastery of ğ‘¢atğ‘¡
ğ‘šğ‘ğ‘–,ğ‘¡ knowledge mastery of ğ‘ğ‘–ofğ‘¢atğ‘¡
KC relation-based GNN
P,S,R prerequisite, subsequence, similarity graphs
P(Â·),S(Â·),R(Â·) neighbor functions of P,S,R
G certain graph inP,S,R
G(Â·) neighbor function of G
ğ¿ number of GNN layers
GNNğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ prototype of KC relation-based GNN
ğ‘‘0,ğ‘‘1,...,ğ‘‘ğ¿ # of dimensions of prototype GNNâ€™s layers
Ëœf(0)
ğ‘ğ‘–,ËœF(0)prototype input of ğ‘ğ‘–and all to GNN ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ
Ëœf(ğ¿)
ğ‘ğ‘–,ËœF(ğ¿)prototype output of ğ‘ğ‘–and all from GNN ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ
Ëœf(ğ‘™)
ğ‘ğ‘–,ËœF(ğ‘™) prototype intermedium of ğ‘ğ‘–and all of
ğ‘™ğ‘¡â„layer of GNN ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ
WG,(ğ‘™)
ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ,OG,(ğ‘™)
ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œweight matrices of ğ‘™ğ‘¡â„of GNNğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ forG
ËœfG,(ğ‘™)
ğ‘ğ‘–prototype intermedium of ğ‘ğ‘–ofğ‘™ğ‘¡â„layer
of GNNğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œ forG
GRKT basic factors
eğ‘ğ‘–,eğ‘ğ‘¡,kğ‘ğ‘–,kğ‘ğ‘¡ embeddings of ğ‘ğ‘–,ğ‘ğ‘¡,ğ‘ğ‘–,ğ‘ğ‘¡
Â¯eğ‘ğ‘–,Â¯eğ‘ğ‘¡ concatenation of ğ‘ğ‘–and its KCâ€™s embeddings
ğ›¼ğ‘ğ‘¡,ğ‘ğ‘— requirement score of ğ‘ğ‘¡requiringğ‘ğ‘—
Wğ‘Ÿğ‘’ğ‘ matrix to calculate requiring scores
ğ›½G
ğ‘ğ‘–,ğ‘ğ‘—correlation score of ğ‘ğ‘–andğ‘ğ‘—forG
WG
ğ‘ğ‘œğ‘Ÿ matrix to calculate correlation scores for G
H0 initial knowledge memory of ğ‘¢
Hğ‘¡âˆ’ knowledge memory of ğ‘¢at a moment before ğ‘¡
Hğ‘¡ knowledge memory of ğ‘¢atğ‘¡
hğ‘ğ‘–,ğ‘¡ knowledge memory of ğ‘ğ‘–ofğ‘¢atğ‘¡
ğ‘‘ğ‘’,ğ‘‘ğ‘˜,ğ‘‘â„ embedding, memory, and hidden dimensions
wâ„ vector to project knowledge memory to mastery
Ë†ğ‘šğ‘ğ‘–,ğ‘¡ modeled knowledge mastery of ğ‘ğ‘–atğ‘¡
ğ‘‘ğ‘ğ‘¡ question difficulty of ğ‘ğ‘¡
MLPğ‘‘ğ‘–ğ‘“ğ‘“ MLP to generate question difficulty
W(1)
ğ‘‘ğ‘–ğ‘“ğ‘“,W(2)
ğ‘‘ğ‘–ğ‘“ğ‘“weight matrices in MLP ğ‘‘ğ‘–ğ‘“ğ‘“
b(1)
ğ‘‘ğ‘–ğ‘“ğ‘“,b(2)
ğ‘‘ğ‘–ğ‘“ğ‘“weight vectors in MLP ğ‘‘ğ‘–ğ‘“ğ‘“
could answer questions of ğ‘ğ‘–correctly while he/her could also an-
swer questions of ğ‘ğ‘—correctly (or both incorrectly), indicating an
underlying similarity between them.
For the prerequisite relationship between ğ‘ğ‘–andğ‘ğ‘—, we assume
that ifğ‘ğ‘–is prerequisite to ğ‘ğ‘—, then answering questions of ğ‘ğ‘–cor-
rectly butğ‘ğ‘—incorrectly is more likely than answering questions ofTable 5: The continuing notation table for the three stages.
Stage I: knowledge retrieval
GNNğ‘Ÿğ‘¡ğ‘£ KC relation-based GNN for knowledge retrieval
Ëœh(0)
ğ‘ğ‘–,ğ‘¡âˆ’,ËœH(0)
ğ‘¡âˆ’ memory input of ğ‘ğ‘–and all to GNN ğ‘Ÿğ‘¡ğ‘£beforeğ‘¡
Ëœh(ğ¿)
ğ‘ğ‘–,ğ‘¡âˆ’,ËœH(ğ¿)
ğ‘¡âˆ’memory output of ğ‘ğ‘–and all from GNN ğ‘Ÿğ‘¡ğ‘£beforeğ‘¡
Ë†ğ‘ğ‘¡ predictive probability of ğ‘ğ‘¡
Stage II: memory strengthening
MLPğ‘”ğ‘ğ‘–ğ‘› MLP to get memory feature for knowledge gain
gğ‘ğ‘¡,ğ‘¡ memory feature of ğ‘ğ‘¡atğ‘¡for knowledge gain
GNNğ‘”ğ‘ğ‘–ğ‘› KC relation-based GNN for knowledge gain
Ëœg(0)
ğ‘ğ‘¡,ğ‘¡,ËœG(0)
ğ‘¡ memory feature input of ğ‘ğ‘¡and all to GNN ğ‘”ğ‘ğ‘–ğ‘› atğ‘¡
Ëœg(ğ¿)
ğ‘ğ‘¡,ğ‘¡,ËœG(ğ¿)
ğ‘¡ knowledge gain of ğ‘ğ‘¡and all from GNN ğ‘”ğ‘ğ‘–ğ‘› atğ‘¡
MLPğ‘™ğ‘œğ‘ ğ‘  MLP to get memory feature for knowledge loss
lğ‘ğ‘¡,ğ‘¡ memory feature of ğ‘ğ‘¡atğ‘¡for knowledge loss
GNNğ‘™ğ‘œğ‘ ğ‘  KC relation-based GNN for knowledge loss
Ëœl(0)
ğ‘ğ‘¡,ğ‘¡,ËœL(0)
ğ‘¡ memory feature input of ğ‘ğ‘¡and all to GNN ğ‘™ğ‘œğ‘ ğ‘ atğ‘¡
Ëœl(ğ¿)
ğ‘ğ‘–,ğ‘¡,ËœL(ğ¿)
ğ‘¡ knowledge loss of ğ‘ğ‘¡and all from GNN ğ‘™ğ‘œğ‘ ğ‘ atğ‘¡
Stage III: knowledge learning/forgetting
MLPğ‘‘ğ‘ ğ‘ MLP to get policy distribution for active learning
ğœ‹ğ‘ğ‘–,ğ‘¡ policy distribution if ğ‘¢decide to learn ğ‘ğ‘–atğ‘¡
MLPğ‘ğ‘Ÿğ‘” MLP to get initial knowledge progress
pğ‘ğ‘–,ğ‘¡ initial knowledge progress of ğ‘ğ‘–atğ‘¡
GNNğ‘ğ‘Ÿğ‘” KC relation-based GNN for knowledge progress
Ëœp(0)
ğ‘ğ‘–,ğ‘¡,ËœP(0)
ğ‘¡ initial progress input of ğ‘ğ‘–and all to GNN ğ‘ğ‘Ÿğ‘”atğ‘¡
Ëœp(ğ¿)
ğ‘ğ‘–,ğ‘¡,ËœP(ğ¿)
ğ‘¡ knowledge progress of ğ‘ğ‘–and all from GNN ğ‘ğ‘Ÿğ‘”atğ‘¡
Î”ğ‘‡ğ‘¡+1 time interval between ğ‘‡ğ‘¡andğ‘‡ğ‘¡+1
ğ“ğ‘ğ‘–KC-specific time-aware kernel for learning ğ‘ğ‘–
ğ‘›ğ‘ğ‘–,ğ‘¡ # of timesğ‘¢has learntğ‘ğ‘–
GNNğ‘™ğ‘Ÿğ‘› KC relation-based GNN to get parameters of ğœ¸ğ‘ğ‘–
Ëœğœ¸(0)
ğ‘ğ‘–input feature of ğ‘ğ‘–initialized as kğ‘ğ‘–to GNNğ‘™ğ‘Ÿğ‘›
Ëœğœ¸(ğ¿)
ğ‘ğ‘–output parameters of ğœ¸ğ‘ğ‘–forğ‘ğ‘–from GNN ğ‘™ğ‘Ÿğ‘›
ğœ¿ğ‘ğ‘– KC-specific time-aware kernel for forgetting ğ‘ğ‘–
GNNğ‘“ğ‘”ğ‘¡ KC relation-based GNN to get parameters of ğœ½ğ‘ğ‘–
Ëœğœ½(0)
ğ‘ğ‘–input feature of ğ‘ğ‘–initialized as kğ‘ğ‘–to GNNğ‘“ğ‘”ğ‘¡
Ëœğœ½(ğ¿)
ğ‘ğ‘–output parameters of ğœ¿ğ‘ğ‘–forğ‘ğ‘–from GNN ğ‘“ğ‘”ğ‘¡
ğ‘ğ‘–incorrectly but ğ‘ğ‘—correctly. Therefore, we use:
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–,ğ‘ğ‘—=Ã
ğ‘¢âˆˆUÃ
ğ‘Ÿğ‘¢
ğ‘¡,ğ‘Ÿğ‘¢
ğ‘¡â€²âˆˆHğ‘¢ğ¼(ğ‘ğ‘¢
ğ‘¡=1,ğ‘ğ‘¢
ğ‘¡â€²=0,ğ‘ğ‘¢
ğ‘¡=ğ‘ğ‘–,ğ‘ğ‘¢
ğ‘¡â€²=ğ‘ğ‘—)
Ã
ğ‘¢âˆˆUÃ
ğ‘Ÿğ‘¢
ğ‘¡,ğ‘Ÿğ‘¢
ğ‘¡â€²âˆˆHğ‘¢ğ¼(ğ‘ğ‘¢
ğ‘¡â‰ ğ‘ğ‘¢
ğ‘¡â€²,ğ‘ğ‘¢
ğ‘¡=ğ‘ğ‘–,ğ‘ğ‘¢
ğ‘¡â€²=ğ‘ğ‘—),
to approximate the probability that ğ‘ğ‘–is a prerequisite to ğ‘ğ‘—.
Finally, we set a threshold ğœ‚to determine whether ğ‘ğ‘–is simi-
lar/prerequisite to ğ‘ğ‘—(byğ‘ ğ‘–ğ‘šğ‘ğ‘–,ğ‘ğ‘—â‰¥ğœ‚andğ‘ğ‘Ÿğ‘’ğ‘ğ‘–,ğ‘ğ‘—â‰¥ğœ‚, respectively).
Additionally, KC pairs with a co-occurrence frequency under 10
times in the dataset are not considered.
C SUPPLEMENTS FOR EXPERIMENTS
C.1 Metrics for Reasonability
We formulate the three metrics for model reasonability in this
section:
 
512KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
Table 6: Hyperparameter setting of GRKT applying for the
three datasets.
Parameter ASSIST09 ASSIST12 Junyi
ğ‘™ğ‘Ÿ 5e-3 5e-3 5e-3
ğ¿ 2 2 2
ğ‘‘ğ‘˜ 16 16 16
ğœ‚ 0.6 0.7 0.8
ğ‘™2 1e-6 1e-5 1e-5
C.1.1 Consistency. This metric measures the ratio of consistent
variation between the mastery of KCs:
ğ¶ğ‘œğ‘›ğ‘ ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦ =âˆ‘ï¸
ğ‘¢âˆˆUâˆ‘ï¸
ğ‘Ÿğ‘¢
ğ‘¡âˆˆHğ‘¢Ã
ğ‘ğ‘–âˆˆCğ¼(ğ‘šğ‘¢
ğ‘ğ‘–,ğ‘¡â‰¥ğ‘šğ‘¢
ğ‘ğ‘–,ğ‘¡+1)
Ã
ğ‘ğ‘–âˆˆCğ¼(ğ‘šğ‘¢
ğ‘ğ‘¢
ğ‘¡,ğ‘¡â‰¥ğ‘šğ‘¢
ğ‘ğ‘¢
ğ‘¡,ğ‘¡+1). (26)
Here, we omit the averaging operation over the students and re-
sponses for conciseness. We only consider the situation where a
studentâ€™s mastery of the learnt KC of the current question declines
while other KCs do not increase, instead of the current one increas-
ing and the others declining. This is because the latter case might
be due to natural forgetting behaviors.
C.1.2 GAUCM. This metric calculates the average AUC scores
with respect to the mastery of each questionâ€™s examined KC:
ğºğ´ğ‘ˆğ¶ğ‘€ =Ã
ğ‘ğ‘–âˆˆQğ‘(ğ‘ğ‘–)Â·ğ´ğ‘ˆğ¶h
{Ë†ğ‘šğ‘¢
ğ‘ğ‘¢
ğ‘¡,ğ‘¡},{ğ‘ğ‘¢
ğ‘¡}iğ‘ğ‘¢
ğ‘¡=ğ‘ğ‘–
ğ‘¢âˆˆU,ğ‘Ÿğ‘¢
ğ‘¡âˆˆHğ‘¢
Ã
ğ‘ğ‘–âˆˆQğ‘(ğ‘ğ‘–).
(27)
ğ´ğ‘ˆğ¶[Ë†Y,Y]ğµ
ğ´indicates the AUC score of the prediction set Ë†Yand
the ground-truth set Y, given the range ğ´and the condition ğµ.
ğ‘(ğ‘ğ‘–)is the number of ğ‘ğ‘–being answered. For evaluating GRKT,
we use the aggregated mastery instead of the single KCâ€™s mastery to
calculate AUC because we consider the transfer of learning theory
that students may leverage related KCs to solve questions.
C.1.3 Repetition. This metric supposes that a reasonable KT method
should adhere to the following rule: after a student has finished a
question and is given the same question again, the response result
(correct or incorrect) should remain the same:
ğ‘…ğ‘’ğ‘ğ‘’ğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› =ğ´ğ¶ğ¶
{KT(ğ‘ğ‘¢
ğ‘¡|{ğ‘Ÿğ‘¢
ğ‘¡â€²|1â‰¤ğ‘¡â€²â‰¤ğ‘¡})},{ğ‘ğ‘¢
ğ‘¡}
ğ‘¢âˆˆU,ğ‘Ÿğ‘¢
ğ‘¡âˆˆHğ‘¢.
(28)
ğ´ğ¶ğ¶(Â·)denotes the accuracy score whose notation is similar to
theğ´ğ‘ˆğ¶(Â·)in Equation 27. KT(ğ‘ğ‘¢
ğ‘¡|{ğ‘Ÿğ‘¢
ğ‘¡â€²|1â‰¤ğ‘¡â€²â‰¤ğ‘¡})denotes the
prediction score if ğ‘¢could correctly answer ğ‘ğ‘¢
ğ‘¡given his/her past ğ‘¡
responses{ğ‘Ÿğ‘¢
ğ‘¡â€²|1â‰¤ğ‘¡â€²â‰¤ğ‘¡}including the response to ğ‘ğ‘¢
ğ‘¡itself.
C.2 Hyper-parameter Setting
We provide the hyper-parameter settings in Table 6. The notations
on the left side indicate the learning rate, the number of GNN
layers, the number of knowledge memory dimensions, the graph
construction threshold, and the value of ğ‘™2normalization.
C.3 Experimental Results Using labeled Graph
Relations
The Junyi dataset includes KC similarity and prerequisite relations
annotated by experts with confidence scores ranging from 1 to
9. We select relations with average scores higher than 5 as graphTable 7: Comparison of GRKT applied to the Junyi dataset
with labeled KC relations (GRKT-L), statistics-based relations
(GRKT-S), and no relations (GRKT-0). The two values in the
â€œsparsityâ€ column respectively denote the constructed KC
similarity and prerequisite graphsâ€™ sparsity.
Model Sparsity AUC ACC CONS GAUC RPT
GRKT-S 0.171, 0.169 0.8207 0.8624 1.0000 0.6473 0.8957
GRKT-L 0.006, 0.003 0.8108 0.8562 1.0000 0.6423 0.8861
GRKT-0 0.000, 0.000 0.7921 0.8481 1.0000 0.6084 0.8781
1.000
1.0000.525
0.3050.245
0.2210.055
0.140 0.008
0.072
0.001
0.025
0.000
0.000
1.000
1.0000.574
0.3290.296
0.2370.076
0.1520.017
0.085
0.003
0.038
0.000
0.000
Figure 5: Experimental results analyzing the effects of hyper-
parameters in GRKT are presented. The green and red deci-
mals on the right side respectively indicate the sparsity of
the constructed KC similarity and prerequisite graphs based
on the specified threshold.
edges. Table 7 presents the experimental results of GRKT leveraging
expert-labeled relations compared with statistics-based relations
and no relations. As shown, the graphs established on expert anno-
tations are too sparse, with only an average of 1-2 related KCs for
one KC, which may not reflect real scenarios. Despite the experi-
mental results based on expert-labeled relations being inferior to the
statistics-based version, they still exhibit noticeable improvement
compared to the version without any relations.
C.4 Hyper-parameter Analysis
We conduct experiments to analyze the effects of various hyperpa-
rameters on GRKTâ€™s performance. The experiments are performed
on the two ASSIST datasets, as shown in Figure 5. The results show
that setting the number of layers in the KC relation-based graphs
to 2 achieves the best performance for GRKT, suggesting that re-
trieving information from further distances over the graph can
enhance the model. However, employing more layers may lead to
overfitting issues. For the KC graph construction threshold, the
performance peaks at around 0.6 to 0.8. In this interval, the sparsity
of the two graphs ranges from 0.01 to 0.3, indicating that too many
relations lead to structural redundancy, while too few result in
limited information sharing between KCs.
 
513