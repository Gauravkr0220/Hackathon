Effective Clustering on Large Attributed Bipartite Graphs
Renchi Yang
Hong Kong Baptist University
Hong Kong SAR, China
renchi@hkbu.edu.hkYidu Wuâˆ—
Chinese University of Hong Kong
Hong Kong SAR, China
yiduwu@cuhk.edu.hkXiaoyang Lin
Hong Kong Baptist University
Hong Kong SAR, China
csxylin@hkbu.edu.hk
Qichen Wang
Hong Kong Baptist University
Hong Kong SAR, China
qcwang@hkbu.edu.hkTsz Nam Chan
Shenzhen University
Shenzhen, China
edisonchan@szu.edu.cnJieming Shi
Hong Kong Polytechnic University
Hong Kong SAR, China
jieming.shi@polyu.edu.hk
Abstract
Attributed bipartite graphs (ABGs) are an expressive data model
for describing the interactions between two sets of heterogeneous
nodes that are associated with rich attributes, such as customer-
product purchase networks and author-paper authorship graphs.
Partitioning the target node set in such graphs into ğ‘˜disjoint clus-
ters (referred to as ğ‘˜-ABGC) finds widespread use in various do-
mains, including social network analysis, recommendation systems,
information retrieval, and bioinformatics. However, the majority
of existing solutions towards ğ‘˜-ABGC either overlook attribute
information or fail to capture bipartite graph structures accurately,
engendering severely compromised result quality. The severity of
these issues are accentuated in real ABGs, which often encompass
millions of nodes and a sheer volume of attribute data, rendering
effectiveğ‘˜-ABGC over such graphs highly challenging.
In this paper, we propose TPO, an effective and efficient approach
toğ‘˜-ABGC that achieves superb clustering performance on multiple
real datasets. TPOobtains high clustering quality through two major
contributions: (i) a novel formulation and transformation of the ğ‘˜-
ABGC problem based on multi-scale attribute affinity specialized for
capturing attribute affinities between nodes with the consideration
of their multi-hop connections in ABGs, and (ii) a highly efficient
solver that includes a suite of carefully-crafted optimizations for
sidestepping explicit affinity matrix construction and facilitating
faster convergence. Extensive experiments, comparing TPOagainst
19 baselines over 5 real ABGs, showcase the superior clustering
quality of TPOmeasured against ground-truth labels. Moreover,
compared to the state of the arts, TPOis often more than 40Ã—faster
over both small and large ABGs.
CCS Concepts
â€¢Mathematics of computing â†’Computations on matrices ;â€¢
Computing methodologies â†’Cluster analysis; Spectral meth-
ods; â€¢Information systems â†’Clustering.
âˆ—Work done while at Hong Kong Baptist University.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671764Keywords
clustering, bipartite graphs, attributes, eigenvector
ACM Reference Format:
Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, and Jiem-
ing Shi. 2024. Effective Clustering on Large Attributed Bipartite Graphs. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671764
1 Introduction
Bipartite graphs are an indispensable data structure used to model
the interplay between two sets of entities from heterogeneous
sources, e.g., author-publication associations, customer-merchant
transactions, query-webpage pairing, and various user-item interac-
tions on social media, e-commerce platforms, search engines, etc. In
the real world, such graphs are often associated with rich attributes,
e.g., the user profile in social networks, web page content in web
graphs, hallmarks of pathways in cancer signaling networks, and
paper keywords in academic graphs, which are termed Attributed
Bipartite Graphs (hereinafter ABGs).
Given an ABGGwith two disjoint node sets UandV,ğ‘˜-
Attributed Bipartite Graph Clustering (ğ‘˜-ABGC), a fundamental task
of analyzing ABGs, seeks to partition the nodes in the node set of in-
terest, e.g.,UorV, intoğ‘˜non-overlapping clusters C1,C2,Â·Â·Â·,Cğ‘˜,
such that nodes within the same cluster Cğ‘–are close to each other
in terms of both their attribute similarity and topological prox-
imity inG. Due to the omnipresence of ABGs, ğ‘˜-ABGC has seen
a wide range of practical applications in social network analy-
sis, recommender systems, information retrieval, and bioinformat-
ics, such as user/content tagging [ 45,81], market basket analysis
[83,84], document categorization [ 8,59], identification of protein
complexes, disease genes, and drug targets [ 46,64], and many oth-
ers [26, 34, 50, 67, 69].
As reviewed in Section 5, existing solutions towards ğ‘˜-ABGC
primarily rely on bipartite graph co-clustering (BGCC), attributed
graph clustering (AGC), and attributed network embedding (ANE)
techniques. Amid them, BGCC has been extensively investigated
in the literature [ 2,8,9,28,29,63] for clustering non-attributed
bipartite graphs, whose basic idea is to simultaneously group nodes
inUandVmerely based on their interactions in G, instead of
clustering them severally. As pinpointed in prior works [ 4], the
attributes present rich information to characterize the properties
of nodes and hence, can complement scant topological information
for better node clustering. Consequently, BGCC methods exhibit
3782
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Renchi Yang et al.
subpar performance on ABGs as they overlook such information.
To leverage the complementary nature of graph topology and
attributes for enhanced clustering effectiveness, considerable efforts
[4,6,12,30,77,85] have been invested in recent years towards
devising effective AGC models and algorithms. Although these
approaches enjoy improved performance over unipartite attributed
graphs by fusing graph connectivity and attribute information of
nodes via deep learning or sophisticated statistical models, they are
sub-optimal for ABGs.
Over the past decade, network embedding has emerged as a
popular and powerful tool for analyzing graph-structured data, es-
pecially those with nodal attributes. Notwithstanding a plethora of
network embedding techniques invented [7, 14, 75], most of them
are designed for unipartite graphs. To capture the unique character-
istics of bipartite graphs, Huang et al . [23] extend node2vec [18] to
ABGs, at the expense of tremendous training overhead. Adopting
this category of approaches for ğ‘˜-ABGC requires a rear-mounted
phase (e.g., ğ‘˜-Means) to cluster the node embeddings, which is
not cost-effective given the high embedding dimensions (typically
128). To summarize, existing approaches to ğ‘˜-ABGC either dilute
clustering quality due to inadequate exploitation of attributes and
bipartite graph topology, or incur vast computation costs, especially
on sizable ABGs encompassing thousands of attributes, millions of
nodes, and billions of edges.
In response to these challenges, we propose TPO, a novel Three-
Phase Optimization framework for ğ‘˜-ABGC that significantly ad-
vances the state of the art in ğ‘˜-ABGC, in terms of both result ef-
fectiveness and computation efficiency. First and foremost, TPO
formulates the ğ‘˜-ABGC task as an optimization problem based on
multi-scale attribute affinity (MSA), a new node affinity measure
dedicated to ABGs. More concretely, the MSA of two homoge-
neous nodes ğ‘¢ğ‘–,ğ‘¢ğ‘—inUof ABGGevaluates the similarity of their
attributes aggregated from multi-hop neighborhoods, which effec-
tively captures the affinity of nodes with consideration of both their
attributes and topological connections in bipartite graphs. How-
ever, calculating the MSA of all node pairs in Gfor clustering is
prohibitively expensive for large graphs, as it entails colossal con-
struction time and space consumption ( ğ‘‚(|U|2)). On top of that,
the exact optimization of our ğ‘˜-ABGC objective is also infeasible
as an aftermath of its NP-hardness.
To tackle these issues, TPOadopts a three-phase optimization
scheme for an approximate solution with time and space costs lin-
ear to the size ofG. Under the hood, similar in spirit to kernel tricks
[36],TPOfirst leverages a mathematical apparatus, random features
[49,82], to bypass the materialization of the all-pairwise MSA. The
clustering task is later framed as a non-negative matrix factoriza-
tion, followed by a matrix approximation problem, based on our
theoretically-grounded problem transformation. Particularly, the
former attends to yielding an intermediate, while the latter itera-
tively refines the intermediary result to derive the eventual clusters.
In addition to the linear-time iterative solvers, TPOfurther includes
a greedy initialization trick for speeding up the convergence, and
an attribute dimension reduction approach to conspicuously boost
the practical efficiency of TPOover graphs with large attribute sets,
without degrading result quality. Our empirical studies, which in-
volved 5 real ABGs and compared against 19 existing algorithms,
demonstrate that TPOconsistently attains superior or comparableTable 1: Frequently used notations.
Notation Description
U,V,E The node setsU,V, and the edge setEof ABGG.
XU,XV Attribute vectors of nodes in UandV.
ğ‘‘U,ğ‘‘V Attribute dimensions of nodes in UandV.
ğ‘¤(ğ‘¢ğ‘–,ğ‘£ğ‘—)Weight of edge(ğ‘¢ğ‘–,ğ‘£ğ‘—)inE.
ğ‘˜ The number of clusters.
ğ›¼ Balance coefficient used in Eq. (3).
ğ›¾ Maximum number of iterations used in Eq. (31).
ğ‘‘ Dimension of Xâ€²
Uin Eq. (15) (ğ‘‘â‰¤ğ‘‘U).
ğ‘‡ğ‘“,ğ‘‡ğ‘” Maximum number of iterations used in Algorithms 2
and 3, respectively.
LU Normalized adjacency matrix defined in Eq. (7).
ZU,bZU Feature vectors of nodes in Uand their normalized
version defined in Eq. (6) and Eq. (2), respectively.
ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—) The MSA between nodes ğ‘¢ğ‘–andğ‘¢ğ‘—defined in Eq. (1).
R Matrix satisfies R[ğ‘–]Â·R[ğ‘—]â‰ˆğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—).
Y The NCI matrix defined in Eq. (10).
ğš¼ The continuous version of Ysatisfying Eq. (11).
clustering quality at a fraction of the cost compared to the state-of-
the-art methods. For instance, on the largest Amazon dataset with
over 10 million nodes and 22 million edges, TPOobtains the best
clustering accuracy within 3 minutes, whereas the state-of-the-art
demands more than 4 hours to terminate.
2 Problem Formulation
2.1 Notation and Terminology
We denote matrices using bold uppercase letters, e.g., MâˆˆRğ‘›Ã—ğ‘‘,
and theğ‘–-th row (resp. the ğ‘—-th column) of Mis represented as M[ğ‘–]
(resp. M[:,ğ‘—]). Accordingly, M[ğ‘–,ğ‘—]signifies the entry at the ğ‘–-th
row andğ‘—-th column of M. For each vector M[ğ‘–], we useâˆ¥M[ğ‘–]âˆ¥to
represent its ğ¿2norm andâˆ¥Mâˆ¥ğ¹to represent the Frobenius norm
ofM.
LetG=(UâˆªV,E,XU,XV)symbolize an attributed bipartite
graph (ABG), whereEis composed of edges connecting nodes in
two disjoint node sets UandVand each edge(ğ‘¢ğ‘–,ğ‘£ğ‘—)is associated
with an edge weight ğ‘¤(ğ‘¢ğ‘–,ğ‘£ğ‘—). Each node ğ‘¢ğ‘–âˆˆU (resp.ğ‘£ğ‘–âˆˆV) of
Gis characterized by a length- ğ‘‘U(resp. length- ğ‘‘V) attribute vec-
torXU[ğ‘–](resp. XV[ğ‘–]). Further, we denote by BUâˆˆR|U|Ã—|V|
the adjacency matrix of Gfrom the perspective of U, in which
BU[ğ‘–,ğ‘—]=ğ‘¤(ğ‘¢ğ‘–,ğ‘£ğ‘—)if(ğ‘¢ğ‘–,ğ‘£ğ‘—)âˆˆE and 0 otherwise. Let DU(resp.
DV) be a|U|Ã—|U| (resp.|V|Ã—|V| ) diagonal matrix wherein the
diagonal entry DU[ğ‘–,ğ‘–](resp. DV[ğ‘–,ğ‘–]) stands for the sum of the
weights of edges incident to ğ‘¢ğ‘–(resp.ğ‘£ğ‘–), i.e.,Ã
(ğ‘¢ğ‘–,ğ‘£â„“)âˆˆEğ‘¤(ğ‘¢ğ‘–,ğ‘£â„“)
(resp.Ã
(ğ‘¢â„“,ğ‘£ğ‘–)âˆˆEğ‘¤(ğ‘¢â„“,ğ‘£ğ‘–)). Table 1 lists the frequently used nota-
tions throughout the paper.
The overarching goal of ğ‘˜-ABGC is formalized in Definition 2.1
and exemplified in Figure 1. Note that by default, we regard Uas
the target node set to cluster. The number ğ‘˜can be specified by
users or configured by a preliminary procedure [41].
Definition 2.1 ( ğ‘˜-Attributed Bipartite Graph Clustering ( ğ‘˜-ABGC)).
Given an ABGG, the target node set U, and the number ğ‘˜of clus-
ters,ğ‘˜-ABGC aims to partition node set Uintoğ‘˜disjoint clusters
{C1,C2,Â·Â·Â·,Cğ‘˜}such that nodes within the same cluster are close
to each other in terms of both topological proximity and attribute
3783Effective Clustering on Large Attributed Bipartite Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ğ’1 ğ’2 ğ’3
ğ‘¢1ğ‘¢2 ğ‘¢3ğ‘¢4ğ‘¢5 ğ‘¢6ğ‘¢7
ğ‘£1ğ‘£2 ğ‘£3ğ‘£4ğ‘£5 ğ‘£6ğ‘£7ğ‘£8ğ’°
ğ’±â„°ğ—ğ’°
Figure 1: An Illustrative Example of ğ‘˜-ABGC
similarity while nodes across diverse clusters are distant.
2.2 Multi-Scale Attribute Affinity (MSA)
Notice that Definition 2.1 cannot directly guide the generation of
clusters, as it lacks a concrete optimization objective that quantifies
node affinities. To this end, we first delineate our novel affinity mea-
sure MSA for nodes in terms of both graph structure and attributes,
before formally introducing our objective in Section 2.3.
MSA formulation. We first assume that each node ğ‘¢ğ‘–âˆˆU can be
represented by a feature vector ZU[ğ‘–], which characterizes both
the attributes as well as the rich semantics hidden in the bipartite
graph topology. Following the popular Skip-gram model [ 40] and
its extension to graphs [ 18,47], we can model pair-wise affinity of
nodes as a softmax unit [ 16] parametrized by a dot product of their
feature vectors. Rather than using the vanilla softmax function,
we adopt a symmetric softmax function and formulate the MSA
ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)between any two nodes ğ‘¢ğ‘–,ğ‘¢ğ‘—inUas follows:
ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)=ğ‘’bZU[ğ‘–]Â·bZU[ğ‘—]
âˆšï¸ƒÃ
ğ‘¢â„“âˆˆUğ‘’bZU[ğ‘–]Â·bZU[â„“]Â·âˆšï¸ƒÃ
ğ‘¢â„“âˆˆUğ‘’bZU[ğ‘—]Â·bZU[â„“], (1)
wherebZUis a normalized ZUwhose each ğ‘–-th row satisfies
bZU[ğ‘–]=ZU[ğ‘–]/âˆ¥ZU[ğ‘–]âˆ¥. (2)
MSAğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)is symmetric, i.e., ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)=ğ‘ (ğ‘¢ğ‘—,ğ‘¢ğ‘–)âˆ€ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆU.
Additionally, by imposing a normalization, âˆ’1â‰¤bZU[ğ‘–]Â·bZU[ğ‘—]â‰¤1
âˆ€ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆU, and hence, the MSA values w.r.t. any node ğ‘¢ğ‘–âˆˆU are
scaled to a similar range.
Optimization Objective for ZU.Next, we focus on the obtain-
ment of the feature vector bZU[ğ‘–]for each node ğ‘¢ğ‘–âˆˆU. A favor-
able choice might be graph neural networks (GNNs) [ 27], which,
however, cannot be readily applied to ABGs as existing GNNs are
primarily designed for general graphs, and it is rather costly to
train classic GNNs. As demystified by recent studies [ 38,70,89],
many popular GNNs models can be unified into an optimization
framework from the perspective of numeric optimization, which
essentially produces node feature vectors being smooth on nearby
nodes in terms of the underlying graph. Inspired by this finding,
we extend this optimization framework to ABGs. More specifically,
its objective is as follows:
min
ZU(1âˆ’ğ›¼)Â·Oğ‘+ğ›¼Â·Oğ‘”, (3)
which includes a non-negative coefficient ğ›¼âˆˆ[0,1]and two terms:
(i) a fitting termOğ‘in Eq. (4)aiming at ensuring ZUis close to the
input attribute vectors XU,
Oğ‘=âˆ¥ZUâˆ’XUâˆ¥2
ğ¹(4)
and (ii) a regularization term Oğ‘”in Eq. (5)constraining the featurevectors of two nodes with high connectivity to be similar.
Oğ‘”=1
2âˆ‘ï¸
ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆUbğ‘¤(ğ‘¢ğ‘–,ğ‘¢ğ‘—)Â·ZU[ğ‘–]âˆšï¸
DU[ğ‘–,ğ‘–]âˆ’ZU[ğ‘—]âˆšï¸
DU[ğ‘—,ğ‘—]2
(5)
The regularization term requires scaling ZU[ğ‘–]of each node ğ‘¢ğ‘–
in Eq. (5)with a factor 1/âˆšï¸
DU[ğ‘–,ğ‘–]to avoid distorting the values
inZU[ğ‘–]whenğ‘¢ğ‘–connects to massive or scant links. The weight
bğ‘¤(ğ‘¢ğ‘–,ğ‘¢ğ‘—)in Eq. (5) is defined by
bğ‘¤(ğ‘¢ğ‘–,ğ‘¢ğ‘—)=âˆ‘ï¸
ğ‘£â„“âˆˆN(ğ‘¢ğ‘–)âˆ©N(ğ‘¢ğ‘—)ğ‘¤(ğ‘¢ğ‘–,ğ‘£â„“)Â·ğ‘¤(ğ‘£â„“,ğ‘¢ğ‘—)
DV[â„“,â„“],
which reflects the strength of connections between two homoge-
neous nodes ğ‘¢ğ‘–andğ‘¢ğ‘—inU(e.g., researchers) via their common
neighbors in the counterparty V(e.g., co-authored papers). As an
example for illustration, consider researchers ğ‘¢3,ğ‘¢4in Figure 1,
bğ‘¤(ğ‘¢3,ğ‘¢4)=1
3+1
2+1
4, where the denominators 3,2, and 4corre-
spond to the numbers of authors in papers ğ‘£3,ğ‘£4andğ‘£5. Accordingly,
bğ‘¤(ğ‘¢3,ğ‘¢4)evaluates the overall contributions of ğ‘¢3,ğ‘¢4to their col-
laborated research works in V. Thus, theOğ‘”term in Eq. (3)is to
minimize the distance of feature vectors of researchers who have
extensively collaborated with each other with high contributions.
The hyper-parameter ğ›¼balances the attribute and topology in-
formation encoded into ZU. In particular, when ğ›¼=0, feature
vectors ZU=XU, and at the other extreme, i.e., ğ›¼=1,ZUis
entirely dependent on the topology of G.
Closed-form Solution of ZU.Given anğ›¼, Lemma 2.21indicates
that the optimal feature vectors ZUto Eq. (3)can be computed via
iterative sparse matrix multiplications in Eq. (6)without undergoing
expensive training.
Lemma 2.2. Whenğ›¾â†’âˆ ,ZUin Eq. (6)is the closed-form solu-
tion to the optimization problem in Eq. (3).
ZU=(1âˆ’ğ›¼)ğ›¾âˆ‘ï¸
ğ‘Ÿ=0ğ›¼ğ‘ŸÂ·
LULâŠ¤
Uğ‘Ÿ
XU, (6)
where LUis a normalized version of adjacency matrix BU, i.e.,
LU=Dâˆ’1
2
UBUDâˆ’1
2
V. (7)
In practice, we set ğ›¾in Eq. (6)to a finite number (typically 5) for
efficiency. Intuitively, the computation of ZUessentially aggregates
attributes from other homogeneous nodes as per their multi-scale
proximities (e.g., the strength of connections via multiple hops (at
mostğ›¾hops)) inG. As such, the feature vectors of nodes with
numerous direct or indirect linkages will be more likely to be close,
yielding a high MSA in Eq. (1).
2.3 Objective Function
Based on the foregoing definitions of ğ‘˜-ABGC and MSA, we formu-
late the problem of ğ‘˜-ABGC as the following objective function:
min
C1,C2,Â·Â·Â·,Cğ‘˜ğ‘˜âˆ‘ï¸
â„“=11
|Câ„“|âˆ‘ï¸
ğ‘¢ğ‘–âˆˆCâ„“,ğ‘¢ğ‘—âˆˆU\Câ„“ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—), (8)
More precisely, Eq. (8)is to identify ğ‘˜disjoint clustersC1,C2,Â·Â·Â·,Cğ‘˜
inUsuch that the average MSA of two nodes in different clusters
is low. Meanwhile, with this optimization objective, the average
1All proofs appear in Appendix A.
3784KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Renchi Yang et al.
MSA of any two nodes in the same cluster will be maximized; in
other words, nodes within the same cluster are tight-knit.
According to [ 52], Eq. (8)is an NP-complete combinatorial op-
timization problem. Hence, the exact solution to Eq. (8)is compu-
tationally infeasible when Ucontains a large number of nodes.
Moreover, the direct optimization of Eq. (8)demands materializing
ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)of every node pairs in UÃ—U . As such, deriving an approx-
imate solution by optimizing Eq. (8)with even a handful of epochs
entails anğ‘‚(|E|Â·|U|Â·ğ‘‘U)computational cost and a quadratic
space overhead ğ‘‚(|U|2), rendering it incompetent for large ABGs.
3 The TPOAlgorithm
To address the above-said challenges, this section presents our
Three- Phase Optimization framework ( TPO) toğ‘˜-ABGC computation
based on Eq. (8) without explicitly constructing the MSA matrix.
3.1 Synoptic Overview
At a high level, TPOdraws inspiration from the equivalence between
the optimization objectives in Eq. (8) and Eq. (9), as Lemma 3.1.
Lemma 3.1. Eq.(8)is equivalent to the following objective:
min
Yâ‰¥0,Hâ‰¥0âˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹s.t.Yis an NCI matrix. (9)
Specifically, if we can identify a matrix Rsuch that R[ğ‘–]Â·R[ğ‘—]=
ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)âˆ€ğ‘¢ğ‘–.ğ‘¢ğ‘—âˆˆU, the computation of ğ‘˜non-overlapping clus-
tersC1,C2,Â·Â·Â·,Cğ‘˜towards optimizing Eq. (8)is equivalent to
decomposing Rinto two non-negative matrices YandH, where Y
represents a normalized cluster indicator (NCI) matrix YâˆˆR|U|Ã—ğ‘˜,
as defined in Eq. (10).
Y[ğ‘–,â„“]=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³1âˆš
|Câ„“|ifğ‘¢ğ‘–belongs to in cluster Câ„“,
0otherwise.(10)
According to Eq. (10), for each node ğ‘¢ğ‘–âˆˆU , its corresponding
vector Y[ğ‘–]in the NCI matrix comprises solely one non-zero entry
Y[ğ‘–,â„“]indicating the clustering membership of ğ‘¢ğ‘–, and the value
should be 1/âˆšï¸
|Câ„“|. This characteristic ensures that Yis column-
orthogonal, i.e., YâŠ¤Y=I. However, this constraint on Yrenders the
factorization of Rhard to converge. Instead of directly computing
the exact Y, we employ a two-step approximation strategy. More
specifically, TPOfirst builds a|U|Ã—ğ‘˜matrix ğš¼(a continuous version
ofY) which minimizes the factorization loss in Eq. (11):
min
ğš¼â‰¥0,Hâ‰¥0âˆ¥Râˆ’ğš¼HâŠ¤âˆ¥2
ğ¹s.t.ğš¼âŠ¤ğš¼=I, (11)
in which the constraint on Yin Eq. (9)is relaxed to be ğš¼â‰¥0and
ğš¼âŠ¤ğš¼=I. Afterward, the task is to transform ğš¼into an NCI matrix
Yby minimizing their difference about Eq. (9).
As outlined in Figure 2, given an ABG G, the number of ğ‘˜of
clusters, and the node set Uto be partitioned as input, TPOoutputs
an approximate solution to the ğ‘˜-ABGC problem in Eq. (8)through
three phases: (i) constructing a low-dimensional matrix Rsuch that
R[ğ‘–]Â·R[ğ‘—]â‰ˆğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)âˆ€ğ‘¢ğ‘–.ğ‘¢ğ‘—âˆˆU without explicitly materializing
the MSA of all node pairs (Algorithm 1, Section 3.2); (ii) factorizing
Ras per Eq. (11)to create aUÃ—ğ‘˜non-negative column-orthogonal
matrix ğš¼(Algorithm 2, Section 3.3); and (iii) effectively converting
ğš¼into an NCI Y(Algorithm 3, Section 3.4). In what follows, we
elaborate on the algorithmic details of these three subroutines. Due
to space limit, we defer the complexity analysis of them and TPOto
(i) MSA Approximation (Algorithm 1 )(ii) Greedy Orthogonal NMF
(iii) NCI Generation (Algorithm 3)â‰ˆâˆ™(Algorithm 2 )âˆ™
ğ‘ ğ‘T
ğ‘¢1
ğ‘¢2
ğ‘¢3
ğ‘¢4
ğ‘¢5
ğ‘¢6
ğ‘¢7ğ‘¢1
ğ‘¢2
ğ‘¢3
ğ‘¢4
ğ‘¢5
ğ‘¢6
ğ‘¢7ğ‘ ğš¼ ğ‡
ğš¼
NCIğ˜ğ‘¢1ğ‘¢2ğ‘¢3ğ‘¢4ğ‘¢5ğ‘¢6ğ‘¢7ğš¼â‰¥ğŸ
ğ‡â‰¥ğŸ
ğ‘¢1
ğ‘¢2
ğ‘¢3
ğ‘¢4
ğ‘¢5
ğ‘¢6
ğ‘¢7
ğ‘¢1ğ‘¢2ğ‘¢3ğ‘¢4ğ‘¢5ğ‘¢6ğ‘¢7
ğ‘£1ğ‘£2ğ‘£3ğ‘£4ğ‘£5ğ‘£6ğ‘£7ğ‘£8
ğ’”(ğ’–ğ’Š,ğ’–ğ’‹)ğ‘˜=3
ğ‘¢1
ğ‘¢2
ğ‘¢3
ğ‘¢4
ğ‘¢5
ğ‘¢6
ğ‘¢7ğ‘˜=3, target node set ğ’°
â„°
2ğ‘‘=4ğ’°
ğ’±ğ—ğ’°Figure 2: Overview of TPO
our technical report [78].
3.2 MSA Approximation via Random Features
Algorithm 1 illustrates the pseudo-code of linearizing the approxi-
mate computation of MSA in Eq. (1)as the matrix product RÂ·RâŠ¤
with matrix R. The fundamental idea is to leverage and tweak the
random features [49,82] technique designed for approximating the
Gaussian kernel ğ‘’âˆ’âˆ¥xâˆ’yâˆ¥2/2of any vectors xandy.
After taking as input the ABG Gand parameters ğ›¼,ğ›¾, Algorithm 1
begins by calculating LUaccording to Eq. (7)and initializing ZU
asğ›¼XU(Lines 1-2). At Lines 3-4, we update ZUviağ›¾iterations of
the following matrix multiplication:
ZUâ†ğ›¼Â·(XU+LUÂ·(LâŠ¤
UZU)). (12)
Particularly, we structure the matrix multiplication LULâŠ¤
UZUas
LUÂ·(LâŠ¤
UZU)in Eq. (12)to boost the computation efficiency. Sub-
sequently, Algorithm 1 transforms ZUintobZUby applying an ğ¿2
normalization to each row in ZU(Line 5) and then proceeds to
constructing R(Lines 6-9).
To be specific, we first generate a ğ‘‘UÃ—ğ‘‘UGaussian random ma-
trixGwith every entry sampled independently from the standard
normal distribution (Line 6) and then apply a QR decomposition
over it to get a ğ‘‘UÃ—ğ‘‘Uorthogonal matrix Q(Line 7). The matrix
Qis distributed uniformly on the Stiefel manifold, i.e., the space of
all orthogonal matrices [43]. Next, Algorithm 1 calculates Râ€²by
Râ€²=âˆšï¸‚ğ‘’
ğ‘‘UÂ·
ğ‘ ğ‘–ğ‘›(bZâ—¦
U)âˆ¥ğ‘ğ‘œğ‘ (bZâ—¦
U)
âˆˆR|U|Ã— 2ğ‘‘U, (13)
where bZâ—¦
U=âˆšï¸
ğ‘‘UÂ·bZUÂ·QâŠ¤andâˆ¥represents the horizontal
concatenation operator for matrices (Line 8). Finally, in Line 9, the
matrix Ris obtained by normalizing each row of Râ€²as
R[ğ‘–]=Râ€²[ğ‘–]âˆšï¸
Râ€²[ğ‘–]Â·râˆˆR2ğ‘‘Uwhere r=âˆ‘ï¸
ğ‘¢â„“âˆˆURâ€²[â„“]. (14)
Theorem 3.2. For any two nodes ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆU, ifRis the output of
Algorithm 1, then the following inequality holds:
1âˆ’17
16ğ‘‘2
Uâˆ’1
4ğ‘‘U
1+17
16ğ‘‘2
U+1
4ğ‘‘UÂ·ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)â‰¤E[R[ğ‘–]Â·R[ğ‘—]]â‰¤1+17
16ğ‘‘2
U+1
4ğ‘‘U
1âˆ’17
16ğ‘‘2
Uâˆ’1
4ğ‘‘UÂ·ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)
Theorem 3.2 indicates that E[R[ğ‘–]Â·R[ğ‘—]]serves as an accurate
estimator of ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—), exhibiting extremely low bias, particularly
becauseğ‘‘Uoften exceeds hundreds in practical scenarios.
3785Effective Clustering on Large Attributed Bipartite Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1: MSA Approximation
Input: An ABGG=(UâˆªV,E,XU), target node setU,
the balance coefficient ğ›¼, the number ğ›¾of iterations
Output: Matrix R
1Calculate LUaccording to Eq. (7);
2ZUâ†ğ›¼XU;
3forğ‘Ÿâ†1toğ›¾do
4 Update ZUaccording to Eq. (12);
5Normalize ZUasbZUby Eq. (2);
6Sample a Gaussian random matrix GâˆˆRğ‘‘UÃ—ğ‘‘U;
7Compute Qby a QR decomposition over G;
8Compute Râ€²according to Eq. (13);
9forğ‘¢ğ‘–âˆˆU doCompute R[ğ‘–]according to Eq. (14);
10return R;
3.2.1 SVD-based Attribute Dimension Reduction. Although
Algorithm 1 circumvents the need to construct the MSA for all
node pairs, it remains tenaciously challenging when dealing with
ABGs with vast attribute sets, i.e., ğ‘‘Ubeing large. Recall that the
major computation expenditure in Algorithm 1 lies at Lines 3-4 and
Lines 7-8, which need ğ‘‚(ğ›¾Â·|E|Â·ğ‘‘U)andğ‘‚(ğ‘‘3
U+|U|Â·ğ‘‘2
U)time,
respectively. As a result, when ğ‘‘Uis high, e.g., ğ‘‘U=ğ‘‚(|U|) , the
computational complexity of Algorithm 1 increases dramatically to
be cubic, rendering it impractical for large-scale ABGs.
To address this, we refine the input attribute vectors XUby
reducing their dimension from ğ‘‘Uto a much smaller constant ğ‘‘
(ğ‘‘â‰ªğ‘‘U). This approach aims to ensure that the ğ‘‘-dimensional
approximation Xâ€²
UofXUstill accurately preserves the MSA as per
Eq.(1). This adjustment reduces the computational cost to a linear
time complexity of ğ‘‚(ğ›¾Â·|E|+|U|) sinceğ‘‘is a constant. To realize
this idea, we first apply the top- ğ‘‘singular value decomposition
(SVD) over XUto produce the decomposition result XUâ‰ˆğšªğšºğš¿âŠ¤.
Utilizing the column-orthogonal (semi-unitary) property of ğš¿, i.e.,
ğš¿âŠ¤ğš¿=I, we have XUXâŠ¤
Uâ‰ˆğšªğšºğš¿âŠ¤ğš¿ğšºğšªâŠ¤=ğšªğšº2ğšªâŠ¤, implying
Xâ€²
U=ğšªğšºâˆˆR|U|Ã—ğ‘‘, (15)
which can be employed as a low-dimensional substitute of XUinput
to Algorithm 1. Along this line, we can derive a low-dimensional
version Zâ€²
Uof feature vectors ZUusing the iterative process at
Lines 2-4 in Algorithm 1 by simply replacing XUasXâ€²
U, i.e.,
Zâ€²
U=(1âˆ’ğ›¼)âˆâˆ‘ï¸
ğ‘Ÿ=0ğ›¼ğ‘ŸÂ·
LULâŠ¤
Uğ‘Ÿ
Xâ€²
U.
Lemma 3.3. Letğšªğšºğš¿âŠ¤be the exact top- ğ‘‘SVD of XU.
Zâ€²
U[ğ‘–]Â·Zâ€²
U[ğ‘—]âˆ’ZU[ğ‘–]Â·ZU[ğ‘—]â‰¤ğœ2
ğ‘‘+1âˆšï¸
DU[ğ‘–,ğ‘–]Â·DU[ğ‘—,ğ‘—]
1âˆ’ğ›¼
holds for every two nodes ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆU , whereğœğ‘‘+1is the(ğ‘‘+1)-th
largest singular value of XU.
Lemma 3.3 establishes the approximation guarantee of Zâ€²
U, which
theoretically assures the accurate approximation of the MSA de-
fined in Eq. (1). Aside from the capabilities of preserving MSA and
reducing computation load, this SVD-based trick can surprisingly
denoise attribute data for enhanced clustering by its close connec-
tion to principal component analysis (PCA), as validated by ourAlgorithm 2: Greedy Orthogonal NMF
Input: Matrix R, the number ğ‘˜of clusters, the number ğ‘‡ğ‘“of
iterations
Output: Matrix ğš¼
1ğšª,ğšº,ğš¿â†RandomizedSVD(R,ğ‘˜);
2Initialize ğš¼andHaccording to Eq. (18);
3forğ‘¡â†1toğ‘‡ğ‘“do
4 Update H[ğ‘—,â„“]âˆ€1â‰¤ğ‘—â‰¤2ğ‘‘,1â‰¤â„“â‰¤ğ‘˜by Eq. (16);
5 Update ğš¼[ğ‘–,â„“]âˆ€ğ‘¢ğ‘–âˆˆU,1â‰¤â„“â‰¤ğ‘˜by Eq. (17);
6return ğš¼;
experiments in Section 4.2.
3.3 Greedy Orthogonal NMF
Upon constructing RâˆˆR|U|Ã— 2ğ‘‘(withğ‘‘=ğ‘‘Uif the dimension
reduction from Section 3.2.1 is not applied) in Algorithm 1, TPO
passes it to the second phase, i.e., conducting an orthogonal non-
negative matrix factorization (NMF) of Ras in Eq. (11)to create
ğš¼. The pseudo-code of our solver to this problem is presented in
Algorithm 2, iteratively updating ğš¼andHusing an alternative
framework towards optimizing the objective function in Eq. (11).
(Lines 3-5). Specifically, given the number ğ‘‡ğ‘“of iterations and initial
guess of Handğš¼, in each iteration, we first update each (ğ‘—,â„“)-entry
(1â‰¤ğ‘—â‰¤2ğ‘‘and1â‰¤â„“â‰¤ğ‘˜) inHfollowing Eq. (16)while fixing
ğš¼, and then update ğš¼[ğ‘–,â„“]forğ‘¢ğ‘–âˆˆU and1â‰¤â„“â‰¤ğ‘˜as in Eq. (17)
with Hfixed.
H[ğ‘—,â„“]=H[ğ‘—,â„“]Â·(RâŠ¤ğš¼)[ğ‘—,â„“]
(HÂ·(ğš¼âŠ¤ğš¼))[ğ‘—,â„“](16)
ğš¼[ğ‘–,â„“]=ğš¼[ğ‘–,â„“]Â·âˆšï¸„
(RH)[ğ‘–,â„“]
(ğš¼Â·(ğš¼âŠ¤Â·(RH)))[ğ‘–,â„“](17)
The above update rules for solving Eq. (11)can be derived by
utilizing the auxiliary function approach [32] with Lagrangian mul-
tipliers in convex optimization, whose convergence is guaranteed
by the monotonicity theorem [ 10]. Note that we reorder the matrix
multiplications Hğš¼âŠ¤ğš¼andğš¼ğš¼âŠ¤RHin Eq. (16)and(17)toHÂ·(ğš¼âŠ¤ğš¼)
andğš¼Â·(ğš¼âŠ¤Â·(RH)), respectively, so as to avert materializing 2ğ‘‘Ã—|U|
dense matrix Hğš¼âŠ¤and|U|Ã—|U| dense matrix ğš¼ğš¼âŠ¤. As such, the
computational complexities of updating Handğš¼in Eq. (16)and
(17) are reduced to ğ‘‚(|U|ğ‘‘ğ‘˜+|U|ğ‘˜2)per iteration.
The aforementioned computation is still rather costly due to the
numerous iterations needed for the convergence of ğš¼andH, espe-
cially when ğš¼andHare initialized randomly. We resort to a greedy
seeding strategy to expedite convergence, as in many optimization
problems. That is, we carefully select a good initialization of ğš¼and
Hin a fast but theoretically grounded manner. As described in Lines
1-2 in Algorithm 1, we set ğš¼andHas follows:
ğš¼=ğšª,H=ğš¿ğšº, (18)
where ğšªandğš¿are the top- ğ‘˜left and right singular vectors of R,
respectively, and ğšºis a diagonal matrix whose diagonal entries
are top-ğ‘˜singular values of R, which are obtained by invoking the
truncated randomized SVD algorithm [ 19] with Randğ‘˜. Note that
this routine consumes ğ‘‚(|U|ğ‘‘ğ‘˜+(U+ğ‘‘)ğ‘˜2)time [ 19] and can be
done efficiently in practice in virtue of its randomized algorithmic
design as well as the highly-optimized libraries (LAPACK and BLAS)
for matrix operations under the hood.
3786KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Renchi Yang et al.
Algorithm 3: Effective NCI Generation
Input: Matrix ğš¼and the number ğ‘‡ğ‘”of iterations
Output: The NCI matrix Y
1ğš½=I;
2forğ‘¡â†1toğ‘‡ğ‘”do
3 forğ‘¢ğ‘–âˆˆU do
4 Determineâ„“âˆ—by Eq. (21);
5 Update Yby Eq. (22);
6 Normalize Ysuch that each column has a unit ğ¿2norm;
7ğš½â†ğš¼âŠ¤Y;
8return Y;
Given the fact that singular vectors ğš¼=ğšªare column-orthogonal,
i.e.,ğš¼âŠ¤ğš¼=I, the Eckart-Young Theorem [ 15] (Theorem A.1 in
Appendix A) pinpoints that Eq. (18)offers the optimal solution to
Eq.(11)when the non-negative constraints over ğš¼andHare relaxed.
In simpler terms, Eq. (18)immediately gains a rough solution to
our optimization objective in Eq. (11), thereby drastically curtailing
the number of iterations needed for Lines 3-5.
3.4 Effective NCI Generation
In its final stage, TPOgenerates an NCI matrix Yby minimizing
the â€œdifferenceâ€ between ğš¼returned by Algorithm 2 and the tar-
get NCI matrix Y. Recall from Eq. (9), our original objective is
to find a|U|Ã—ğ‘˜NCI matrix Yand a 2ğ‘‘Ã—ğ‘˜non-negative H
such that the total squared reconstruction error âˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹=Ã
ğ‘¢ğ‘–âˆˆUÃğ‘‘
ğ‘—=1(R[ğ‘–,ğ‘—]âˆ’Y[ğ‘–]Â·H[ğ‘—])2is minimized. Considering ğš¼
is a continuous version of Y(relaxing the constraint in Eq. (10)),
âˆ¥Râˆ’ğš¼HâŠ¤âˆ¥2
ğ¹is capable of attaining a strictly lower reconstruction
error compared toâˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹. Therefore, an ideal solution Yto Eq.
(9)ensures thatâˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹closely approximates âˆ¥Râˆ’ğš¼HâŠ¤âˆ¥2
ğ¹in
Eq.(11). Mathematically, the conversion from matrix ğš¼into the NCI
matrix Ycan be formulated as the minimization of the difference
of their reconstruction errors, i.e.,âˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹âˆ’âˆ¥Râˆ’ğš¼HâŠ¤âˆ¥2
ğ¹=trace((YYâŠ¤âˆ’ğš¼ğš¼âŠ¤)Â·RRâŠ¤)by Lemma 3.4.
Lemma 3.4. The following equation holds:
âˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹âˆ’âˆ¥Râˆ’ğš¼HâŠ¤âˆ¥2
ğ¹=trace((YYâŠ¤âˆ’ğš¼ğš¼âŠ¤)Â·RRâŠ¤).(19)
Further, we reformulate the problem as follows:
min
ğš½,Yâˆ¥ğš¼ğš½âˆ’Yâˆ¥2s.t.ğš½ğš½âŠ¤=IandYis an NCI matrix , (20)
which implies that, if the NCI matrix Yand theğ‘˜Ã—ğ‘˜row-orthogonal
matrix ğš½minimizeâˆ¥ğš¼ğš½âˆ’Yâˆ¥2,YYâŠ¤âˆ’ğš¼ğš¼âŠ¤â‰ˆğš¼ğš½ğš½âŠ¤ğš¼âŠ¤âˆ’ğš¼ğš¼âŠ¤â‰ˆ0
holds and the objective loss in Eq. (19) is therefore minimized.
To solve Eq. (20), we develop Algorithm 3 in TPO, which obtains
the NCI matrix Ythrough an iterative framework wherein ğš½and
Yare refined in an alternative fashion till convergence. Initially,
Algorithm 3 starts by taking as input the matrix ğš¼and the number
ğ‘‡ğ‘”of iterations and initializing ğš½as ağ‘˜Ã—ğ‘˜identity matrix (Line 1).
It then launches an iterative process at Lines 2-7 to jointly refine Y
andğš½. Specifically, in each of the ğ‘‡ğ‘”iterations, TPOfirst determines
the cluster id of each node ğ‘¢ğ‘–âˆˆU via (Line 4)
â„“âˆ—=arg max
1â‰¤â„“â‰¤ğ‘˜ğš¼[ğ‘–]Â·ğš½[:,â„“] (21)
and then updates the cluster indicator Y[ğ‘–]of nodeğ‘¢ğ‘–as followsTable 2: Attributed Bipartite Graphs
Name CiteSe
er Cora Mo
vieLens Go
ogle A
mazon
|
U| 1,237 1,312 6,040 64,527 2,330,066
|
V| 742 789 3,883 868,937 8,026,324
|
E| 1,665 2,314 1,000,209 1,487,747 22,507,155
ğ‘‘U 3,703 1,433 30 1,024 800
ğ‘‘V 3,703 1,433 21 - -
ğ‘˜ 6 7 21 5 3
(Line 5):
Y[ğ‘–,â„“]=(
1ifâ„“=â„“âˆ—,
0otherwise,âˆ€1â‰¤â„“â‰¤ğ‘˜. (22)
Each column in Yis laterğ¿2-normalized, i.e.,
âˆ€1â‰¤â„“â‰¤ğ‘˜âˆšï¸ƒÃ
ğ‘¢ğ‘–âˆˆUY[ğ‘–,â„“]2=1, (23)
in accordance with the NCI constraint in Eq. (10)(Line 6). In a
nutshell, Lines 3-6 optimizes Eq. (20)by updating Ywithğš½fixed. To
explain, recall the constraint of the NCI matrix Ystated in Eq. (10),
each row of Yhas merely one non-zero entry. Hence, by locating the
column idâ„“âˆ—whose corresponding entry (ğš¼ğš½)[ğ‘–,â„“âˆ—]is maximum
in theğ‘–-th row of ğš¼ğš½(i.e., Eq. (21)) and meanwhile updating Y[ğ‘–]
as Eqs. (22)and(23)as Lines 5-6, the distance between ğš¼ğš½andY
in Eq. (20) is naturally minimized.
With the refined Yat hand, the subsequent work turns into
updating the ğ‘˜Ã—ğ‘˜matrix ğš½towards optimizing
min
ğš½âˆ¥ğš¼ğš½âˆ’Yâˆ¥2s.t.ğš½ğš½âŠ¤=I.
Given Y, the minimizer to this problem is ğš½=ğš¼âŠ¤Yby utilizing
Lemma 4.14 in [61]. Therefore, ğš½is updated to ğš¼âŠ¤Yat Line 7.
After repeating the above procedure for ğ‘‡ğ‘”iterations, TPOreturns
Yas the final clustering result. Practically, a dozen iterations are
sufficient to yield high-caliber Y, as validated in Section 4.3.
4 Experiments
In this section, we experimentally evaluate our proposed ğ‘˜-ABGC
method TPOagainst 19 competitors over five real ABGs in terms of
clustering quality and efficiency. All the experiments are conducted
on a Linux machine powered by 2 Xeon Gold 6330 @2.0GHz CPUs
and 1TB RAM. For reproducibility, the source code and datasets are
available at https://github.com/HKBU-LAGAS/TPC.
4.1 Experimental Setup
Datasets. Table 2 lists the statistics of the five datasets used in the
experimental study. |U|,|V|, and|E|denote the cardinality of two
disjoint node setsU,V, and edge setEofG, respectively, while ğ‘‘U
(resp.ğ‘‘V) stands for the dimensions of attribute vectors of nodes in
U(resp.V). The number of ground-truth clusters of nodes UinG
isğ‘˜.Citeseer andCora are synthesized from real citation graphs in
[27] by dividing nodes in each cluster into two equal-sized partitions
(i.e.,UandV) and removing intra-partition edges and isolated
nodes as in [ 62]. In particular, nodes represent publications, edges
denote their citation relationships, and labels correspond to the
fields of study. The well-known MovieLens dataset [ 20] comprises
user-movie ratings, where clustering labels are usersâ€™ occupations
inU.Google andAmazon are extracted from the Google Maps [ 68]
3787Effective Clustering on Large Attributed Bipartite Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Clustering Quality (Larger ACC, NMI, and ARI indicate higher clustering quality).
MethodCiteSeer Cora MovieLens Google AmazonRankACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI
KMeans [21] 0.526 0.277 0.225 0.431 0.229 0.137 0.298 0.363 0.170 0.370 0.053 0.012 0.502 0.038 0.079 5.4
SpecClust [56] 0.222 0.017 -0.001 0.311 0.026 0.003 0.318 0.392 0.197 - - - - - - 11.27
NMF [65] 0.508 0.222 0.228 0.380 0.165 0.110 0.568 0.611 0.482 0.384 0.069 0.062 0.390 0.006 0.015 5.6
SCC [8] 0.243 0.015 0.012 0.280 0.040 0.017 0.128 0.030 0.004 0.425 0.038 0.038 0.470 0.018 -0.016 10.27
SBC [28] 0.239 0.015 0.012 0.262 0.059 0.023 0.116 0.035 0.006 0.394 0.006 -0.003 0.485 0.003 0.005 10.87
CCMOD [2] 0.200 0.010 0.003 0.264 0.066 0.040 0.141 0.029 0.010 OOM OOM OOM OOM OOM OOM 12.27
SpecMOD [29] 0.238 0.012 0.003 0.290 0.023 -0.007 0.125 0.033 0.009 OOM OOM OOM OOM OOM OOM 12.6
InfoCC [9] 0.235 0.013 0.007 0.223 0.035 0.018 0.095 0.036 0.007 0.277 0.008 0.008 0.378 0.007 0.003 11.54
DeepCC [63] 0.205 0.013 0.004 0.213 0.014 0.006 0.093 0.027 0.004 0.324 0.105 0.031 - - - 12.67
HOPE [72] 0.473 0.169 0.288 0.268 0.025 0.043 0.115 0.009 0.037 0.269 0 0 0.452 0 0.002
ACMin [77] 0.450 0.143 0.141 0.650 0.470 0.410 0.122 0.032 0.009 0.312 0.023 0.020 0.428 0.012 0.003 7.67
GRACE [12] 0.469 0.209 0.199 0.351 0.136 0.103 0.298 0.249 0.195 0.420 0 0 0.427 0.008 0 7.47
AGCC [85] 0.448 0.144 0.153 0.650 0.517 0.406 0.538 0.589 0.480 OOM OOM OOM OOM OOM OOM 7.34
Dink-Net [37] 0.308 0 0 0.231 0.004 0.007 0.123 0.001 0 0.429 0 0 OOM OOM OOM
node2vec [18] 0.209 0.007 0.001 0.220 0.008 0 0.074 0.011 0 0.280 0 0 - - - 14.8
BiNE [13] 0.196 0.005 0 0.174 0.005 -0.002 0.071 0.012 0 - - - - - - 15.54
GEBE [73] 0.231 0.013 0.002 0.293 0.014 0.006 0.095 0.014 -0.002 0.428 0 0 0.489 0 0 12.14
PANE [75, 76] 0.443 0.154 0.136 0.537 0.526 0.339 0.855 0.923 0.838 0.359 0.127 0.070 0.497 0.083 0.102 4.2
BiANE [23] 0.259 0.057 0.016 0.341 0.239 0.080 0.091 0.053 0.013 - - - - - - 10.21
TPO(ğ‘‘=ğ‘‘U)0.541 0.256 0.265 0.662 0.477 0.408 0.931 0.961 0.957 0.367 0.112 0.091 0.502 0.045 0.091 2.54
TPO 0.625 0.322 0.348 0.671 0.475 0.416 0.931 0.961 0.957 0.444 0.135 0.138 0.504 0.055 0.104 1.27
and Amazon review dataset [ 22], where edges represent the reviews
on restaurants and books posted by users.
Competitors and Parameters. We compare TPOagainst 19 exist-
ing methods, which can be categorized into four groups as follows:
â€¢Data Clustering: KMeans [21], NMF [65], and SpecClust [56];
â€¢Network Embedding: node2vec [18],BiNE [13],BiANE [23],PANE
[75, 76], and GEBE [73];
â€¢Attributed Graph Clustering: AGCC [85],ACMin [77],GRACE [12],
Dink-Net [37];
â€¢Bipartite Graph Clustering: SCC [8],SBC [28],InfoCC [9],Spec-
MOD [29], CCMOD [2],DeepCC [63], and HOPE [72].
Unless otherwise specified, on all datasets, we set the numbers ğ‘‡ğ‘“
andğ‘‡ğ‘”of iterations required by Algorithms 2 and 3 in our proposed
TPOto5and20, respectively. Regarding parameters ğ›¼andğ›¾, by
default, we set ğ›¼=0.6,ğ›¾=6onCiteSeer andMovieLens,ğ›¼=0.9,ğ›¾=
10onCora andGoogle, and ğ›¼=0.5,ğ›¾=1onAmazon, respectively.
To deal with the high attribute dimensions ğ‘‘Uof the CiteSeer, Cora,
Google, and Amazon datasets, we set their new attribute dimensions
ğ‘‘in Section 3.2.1 to 32,128,32, and 64, respectively. We refer to the
version of TPOwithout the attribute dimension reduction module
in Section 3.2.1 as TPO(ğ‘‘=ğ‘‘U). More implementation details of
our method and baselines are in our technical report [78].
Evaluation Metrics. Following convention, we adopt three widely
used measures [ 6,25,31,57,77,85] to assess the clustering quality,
namely (i) Clustering Accuracy (ACC), (ii) Normalized Mutual In-
formation (NMI), and (3) Adjusted Rand Index (ARI), for measuring
the quality of clusters produced by each evaluated method in the
presence of the ground-truth clusters of the tested dataset. Particu-
larly, ACC and NMI scores range from 0to1.0, whilst ARI ranges
fromâˆ’0.5to1.0. For each of these metrics, higher values indicate
better clustering quality. Regarding efficiency evaluation, we re-
port the running time in seconds (measured in wall-clock time) of
each method on each dataset, excluding the time for input (loading
datasets) and output (saving clustering results). The formulas for
evaluation metrics are in our technical report [78].4.2 Clustering Performance
This set of experiments reports the clustering quality achieved
byTPOand all competitors over the five datasets, as well as their
respective running times. We omit a method if it cannot report the
results within three days or incur out-of-memory (OOM) errors.
Since TPOis randomized, we repeat it five times and report the
average performance.
Clustering Quality. Table 3 shows the ACC, NMI, and ARI scores
of all methods on five ABGs, and their overall average performance
rankings. We highlight the top-3 best clustering results on each
dataset in gray with darker shades indicating higher quality. TPO
consistently outperforms the 17 competitors on the CiteSeer, Movie-
Lens, and Google datasets in terms of ACC, NMI, and ARI, by sub-
stantial margins of up to 9.9%for ACC, 4.5%for NMI, and 12%for
ARI, respectively. The only exceptions are on Cora and Amazon,
where TPOachieves the highest ACC and ARI results but inferior
NMI scores compared to PANE orAGCC. In addition, TPO(ğ‘‘=ğ‘‘U)
exhibits competitive clustering effectiveness, which either is second
only to TPOor obtains the third best clustering results in most cases.
Specifically, TPO(ğ‘‘=ğ‘‘U) is comparable to TPOonCora, Movie-
Lens, and Amazon with a performance degradation at most 0.9%
in ACC, 1.0%in NMI, and 1.3%in ARI. Over all datasets, TPOand
TPO(ğ‘‘=ğ‘‘U) attain the best and second best average performance
rank (smaller rank is better), respectively. The evident superiority
ofTPOandTPO(ğ‘‘=ğ‘‘U) manifests the accuracy of our proposed
MSA model in Section 2.2 in preserving the attribute similarity and
topological connections between nodes, as well as the effectiveness
of theoretically-grounded three-phase optimization framework in-
troduced in Section 3.
At this point, a keen reader may wonder why TPOwith attribute
dimension reduction outperforms TPO(ğ‘‘=ğ‘‘U) on most datasets,
especially CiteSeer and Google, as it seems that the former is an
approximate version of the latter. Notice that TPOandTPO(ğ‘‘=ğ‘‘U)
output identical results, as dimension reduction is not needed on
MovieLens andTPOturns to be TPO(ğ‘‘=ğ‘‘U). Recall that the only
difference between TPOandTPO(ğ‘‘=ğ‘‘U) is that TPOemploys a
truncated SVD over the input attribute vectors XUof a node inU
3788KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Renchi Yang et al.
10âˆ’210âˆ’1100101running time (sec)
TPO
TPO(ğ‘‘=ğ‘‘U)
A
GCC
ACMin
PANE
KMeans
NMF
(a)Cora10âˆ’1100101102running
time (sec)
TPO
TPO(ğ‘‘=ğ‘‘U)
P
ANE
NMF
A
GCC
Sp
ecClust
GRA
CE
(
b)MovieLens100101102103104105running
time (sec)
TPO
TPO(ğ‘‘=ğ‘‘U)
P
ANE
NMF
SCC
De
epCC
KMeans
(
c)Google100101102103104running
time (sec)
TPO
TPO(ğ‘‘=ğ‘‘U)
P
ANE
KMeans
A
CMin
SBC
NMF
(
d)Amazon
Figure 3: Running time in seconds.
CiteSe
er Cora Mo
vieLens Go
ogle
0.1 0.3 0.5 0.7 0.90.40.50.60.70.80.9A
CC
(a) Varyingğ›¼0 2 4 6 8 100.40.50.60.70.80.9A
CC
(b) Varyingğ›¾0 5 10 15 200.40.50.60.70.80.9A
CC
(c) Varyingğ‘‡ğ‘“0 5 10 15 200.40.50.60.70.80.9A
CC
(d) Varyingğ‘‡ğ‘”16 32 64 128 2560.40.50.60.70.80.9A
CC
(e) Varyingğ‘‘
Figure 4: Clustering accuracy when varying parameters.
for dimension reduction as stated in Section 3.2.1. Aside from the
crucial theoretical assurance offered by this SVD-based approach
in the MSA approximation, it implicitly conducts a PCA on the
attribute vectors, extracting key features from the input attributes
while eradicating noisy ones. In brief, the SVD-based trick in Section
3.2.1 grants TPOthe additional ability to denoise the attribute data,
thus elevating the resultsâ€™ quality.
Efficiency. For clarity, we compare the empirical efficiency of TPO
and TPO(ğ‘‘=ğ‘‘U) only against competitors ranked in the top
7 for clustering quality, as shown in Table 3. Figure 3 plots the
computation times required by each of these methods on Cora,
MovieLens, Google, and Amazon. The ğ‘¦-axis is the running time
(seconds) in the log scale. On each of the diagrams in Figures 3(a),
3(b), 3(c), and 3(d), all the bars are displayed from left to right in an
ascending order w.r.t. their average performance rank in Table 3.
Accordingly, except the first two bars from the left for TPOandTPO
(ğ‘‘=ğ‘‘U), the third bars (from the left) in these figures illustrate
the running times of the best competitors, i.e., AGCC onCora, and
PANE onMovieLens, Google, and Amazon, respectively. As we can
see,TPOis consistently faster than the state-of-the-art approaches,
AGCC orPANE, on four datasets, often by orders of magnitude. For
instance, on Cora, Google, and Amazon, TPOtakes 0.47,28.7, and 178
seconds, respectively, whereas the best baselines AGCC orPANE
cost around 19seconds, 23minutes, and 4.1hours, respectively,
attaining 40Ã—,48Ã—, and 83Ã—speedup. In addition, TPOalso enjoys
a considerable efficiency gain of up to 19.9Ã—over TPO(ğ‘‘=ğ‘‘U),
attributed to the SVD-based dimension reduction (Section 3.2.1). On
theMovieLens dataset, the input attribute dimension ğ‘‘U=30is low,
and the attribute dimension reduction is therefore disabled, making
TPOandTPO(ğ‘‘=ğ‘‘U) yield the same running time, which is 3.46Ã—
over the best competitor PANE. Although NMF, KMeans, and SCC
run much faster than TPOon some datasets by either neglecting the
graph topology or discarding the attribute data, their result quality
is no match for our solution TPO.In summary, TPOconsistently delivers superior results for ğ‘˜-
ABGC tasks over ABGs with various volumes while offering high
practical efficiency, which corroborates the efficacy of our novel
objective function based on MSA in Section 2 and the optimization
solver with careful algorithmic designs developed in Section 3.
4.3 Parameter Analysis
In these experiments, we empirically investigate the impact of five
key parameters in TPO:ğ›¼,ğ›¾,ğ‘‡ğ‘“,ğ‘‡ğ‘”, andğ‘‘. For each of them, we run
TPOover CiteSeer, Cora, MovieLens, and Google, respectively, by
varying the parameter with others fixed as in Section 4.1.
Varyingğ›¼andğ›¾.Figures 4(a) shows that on Cora and Google,
TPOâ€™s clustering performance markedly improves as ğ›¼increases
from 0.1to0.9, indicating the importance of graph structure in
these datasets. On CiteSeer and MovieLens, setting ğ›¼=0.6will
be a favorable choice, which results in an optimal combination of
attributes and graph topology and hence the highest ACC scores.
Figures 4(b) depicts the ACC scores when ğ›¾increases from 0to
10. Whenğ›¾=0, the graph structure is disregarded in TPO, namely
ZU=XU. It can be observed on all datasets that the clustering
quality rises with ğ›¾increasing except CiteSeer andMovieLens, where
the ACC results reach a plateau after ğ›¾â‰¥6. This is consistent with
the fact that a larger ğ›¾produces a more accurate solution ZUto
the objective in Eq. (3), and thus, higher clustering quality.
Varyingğ‘‡ğ‘“andğ‘‡ğ‘”.Figures 4(c) presents the ACC scores when
theğ‘‡ğ‘“of iterations in Algorithm 2 is varied from 0to20. We can
conclude that our greedy seeding strategy described in Section
3.3 is highly effective in enabling swift convergence, as additional
optimization iterations merely bring minor gains in clustering per-
formance. On Cora andCiteSeer, the ACC scores see an uptick when
varyingğ›¾from 0to10, followed by a pronounced downturn. Such
a performance decline is caused by overfitting in solving Eq. (11).
From Figures 4(d) reporting clustering performance changes when
3789Effective Clustering on Large Attributed Bipartite Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
varyingğ‘‡ğ‘”from 0to20, we can make analogous observations on
the four datasets. The evaluation scores first experience a sharp
increase as ğ‘‡ğ‘”increases from 0to5. After that, the ACC remain
invariant with ğ›¾increasing. The results manifest the effectiveness
of our solver developed in Section 3.4 in fast NCI generation.
Varyingğ‘‘.Intuitively, a large ğ‘‘may lead to accurate preservation
of MSA as per Lemma 3.3 and further improve clustering quality.
However, in practice, the original attribute vectors XUembody
noises, especially when ğ‘‘Uis high. As pinpointed and validated in
Sections 3.2.1 and 4.2, our SVD-based dimension reduction inher-
ently applies a PCA over XUfor noise elimination, considerably
upgrading the empirical result quality. That is to say, the choice of ğ‘‘
strikes a balance between capturing MSA and removing noisy data,
consistent with our empirical results in Figure 4(e). In particular,
onCiteSeer, Cora, and Google, picking 32,128, and 32for dimension
ğ‘‘, respectively, can strike a good balance between MSA preserva-
tion and noisy reduction for superior clustering performance. On
MovieLens, the attribute dimension reduction is not enabled when
ğ‘‘â‰¥32since its original dimension ğ‘‘U=30. We refer interested
readers to our technical report [ 78] for NMI and efficiency results.
5 Related Work
Bipartite Graph Clustering. A classic methodology [ 87] for bi-
partite graph clustering first projects a bipartite graph Ginto a
unipartite graph by connecting every two nodes from the same
partitionUif they share common neighbors in G. Then, a standard
graph clustering algorithm for node clustering can be adopted on
the constructed unipartite graph. However, the projection often
leads to unipartite graphs ğ‘‚(|U|2)edges, which is intolerable for
even medium-sized graphs [ 71]. In our previous work [ 72], we
addressed this problem by transforming it into a two-stage approx-
imation framework.
Unlike the projection-based methods, another line of research
focuses on simultaneously clustering two disjoint sets of nodes
(i.e.,UandV) in a bipartite graph. These co-clustering techniques
have been extensively investigated in the literature [ 17] and span a
variety of applications in bioinformatics and text mining. Several
attempts [ 8,28,29] are made to extend spectral clustering to bi-
partite graphs. Analogously, Ailem et al . [2] and Dhillon et al . [9]
propose generating co-clusters by extending and optimizing classic
metrics of modularity and mutual information on bipartite graphs,
respectively. DeepCC [63] creates low-dimension instances and
features using a deep autoencoder, then assigns clusters using a
variant of the Gaussian mixture model. To handle the resolution
limit in prior works as well as incorporate attribute information,
Kim et al . [25] designed ABC, which incurs a severe efficiency issue
due to its quadratic running time ğ‘‚(|U|2+|V|2).
Attributed Graph Clustering. As surveyed in [ 4,5,33,77], there
is a large body of work on attributed graph clustering (AGC). Ac-
cording to [ 77], existing AGC techniques can be categorized into
four groups: edge-weigh-based methods [ 51,79], distance-based
methods [ 11,88], statistics-based models [ 66,77,80], and graph
learning-based methods [ 12,42,57,58,85]. Among them, graph
learning-based approaches [ 12,37,42,85] have achieved state-of-
the-art performance, as reported in [ 30,77]. These methods obtainhigh clustering quality on attributed graphs at the cost of costly
neural network training, thus incurring poor scalability on large
graphs. To our knowledge, the statistical-model-based solution,
ACMin [77], is the only AGC method that scales to massive graphs
with millions of nodes and billions of edges, while attaining high
result quality. However, none of them are custom-made for ABGs,
producing compromised result quality for ğ‘˜-ABGC.
Network Embedding. In recent years, network embedding, which
converts each node in a graph into an embedding vector capturing
the surrounding structures, has been employed in a wide range
of graph analytics tasks, and has seen remarkable success [ 7,14].
In particular, by simply feeding them into data clustering meth-
ods, e.g., KMeans, such embedding vectors can be utilized to cope
withğ‘˜-ABGC. However, the majority of network embedding works
[13,18,47,48,53,54,73,74] are designed for graphs in the ab-
sence of node attributes. To bridge this gap, a series of efforts
[6,24,35,44,55,60,75,76] have been made towards incorporat-
ing node attributes into embedding vectors for enhanced result
utility. These approaches still suffer from sub-optimal clustering
performance as they fall short of preserving the hidden seman-
tics underlying bipartite graphs. To learn effective node embed-
dings over ABGs, [ 1,23] extend SkipGram models [ 39] to ABGs by
picking node-pair samples with consideration of both their intra-
partition/inter-partition proximities and attribute similarities. Athar
et al. [3] project the ABG into two homogeneous graphs based on
topological connections and attribute similarities, and then invoke
unsupervised GNNs on the constructed graphs for embedding gen-
eration. Moreover, Zhang et al . [86] propose IGE[86] for learning
node embeddings on dynamic ABGs with a focus on temporal
dependence of edges rather than the bipartite graph structures.
These works either fall short of preserving multi-hop relationships
between nodes or struggle to cope with large ABGs due to the
significant expense of training.
6 Conclusion
This paper presents TPO, an effective and efficient solution for ğ‘˜-
ABGC tasks. TPOachieves remarkable performance, attributed to
a novel problem formulation based on the proposed multi-scale
attribute affinity measure for nodes in ABGs, and a well-thought-
out three-phase optimization framework for solving the problem.
Through a series of theoretically-grounded efficiency techniques
developed in this paper, TPOis able to scale to large ABGs with
millions of nodes and hundreds of millions of edges while offering
state-of-the-art result quality. The superiority of TPOover 19 base-
lines is experimentally validated over 5 real ABGs in terms of both
clustering quality and empirical efficiency.
Acknowledgments
Renchi Yang is supported by the NSFC Young Scientists Fund (No.
62302414) and the Hong Kong RGC ECS grant (No. 22202623).
Qichen Wang is supported by Hong Kong RGC Grants (Project
No. C2004-21GF and C2003-23Y). Tsz Nam Chan is supported by
the NSFC grant 62202401. Jieming Shi is supported by Hong Kong
RGC ECS (No. 25201221) and NSFC 62202404.
3790KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Renchi Yang et al.
A Theorems and Proofs
Theorem A.1 (Eckartâ€“Young Theorem [ 15]).Suppose that
Mğ‘˜âˆˆRğ‘›Ã—ğ‘˜is the rank-ğ‘˜approximation to MâˆˆRğ‘›Ã—ğ‘›obtained by
exact SVD, then minğ‘Ÿğ‘ğ‘›ğ‘˜(bM)â‰¤ğ‘˜âˆ¥Mâˆ’bMâˆ¥2=âˆ¥Mâˆ’Mğ‘˜âˆ¥2=ğœğ‘˜+1,
whereğœğ‘–represents the ğ‘–-th largest singular value of M.
Proof of Lemma 2.2. We first rewrite that Eq. (5) as
trace(ZâŠ¤
UÂ·(Iâˆ’LULâŠ¤
U)Â·ZU). (24)
The equivalence can be deduced by the definition of Eq. (5),
Oğ‘”=1
2âˆ‘ï¸
ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆUbğ‘¤(ğ‘¢ğ‘–,ğ‘¢ğ‘—)Â·ZU[ğ‘–]âˆšï¸
DU[ğ‘–,ğ‘–]âˆ’ZU[ğ‘—]âˆšï¸
DU[ğ‘—,ğ‘—]2
=âˆ‘ï¸
ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆUbğ‘¤(ğ‘¢ğ‘–,ğ‘¢ğ‘—)
2 
âˆ¥ZU[ğ‘–]âˆ¥2
DU[ğ‘–,ğ‘–]+âˆ¥ZU[ğ‘—]âˆ¥2
DU[ğ‘—,ğ‘—]âˆ’2ZU[ğ‘–]Â·ZU[ğ‘—]âˆšï¸
DU[ğ‘–,ğ‘–]âˆšï¸
DU[ğ‘—,ğ‘—]!
=âˆ‘ï¸
ğ‘£â„“âˆˆVâˆ‘ï¸
ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆUğ‘¤(ğ‘¢ğ‘–,ğ‘£â„“)ğ‘¤(ğ‘¢ğ‘—,ğ‘£â„“)
DV[â„“,â„“] 
âˆ¥ZU[ğ‘–]âˆ¥2
DU[ğ‘–,ğ‘–]âˆ’ZU[ğ‘–]Â·ZU[ğ‘—]âˆšï¸
DU[ğ‘–,ğ‘–]âˆšï¸
DU[ğ‘—,ğ‘—]!
=âˆ‘ï¸
ğ‘¢ğ‘–âˆˆUâˆ¥ZU[ğ‘–]âˆ¥2âˆ’âˆ‘ï¸
ğ‘£â„“âˆˆVâˆ‘ï¸
ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆULU[ğ‘–,â„“]Â·L[ğ‘—,â„“]Â·ZU[ğ‘–]Â·ZU[ğ‘—]
=âˆ‘ï¸
ğ‘¢ğ‘–ZU[ğ‘–]Â·ZU[ğ‘–]âˆ’âˆ‘ï¸
ğ‘£â„“âˆˆV(LâŠ¤
UZU)[â„“]Â·(LâŠ¤
UZU)[â„“]
=trace(ZâŠ¤
UÂ·(Iâˆ’LULâŠ¤
U)Â·ZU).
Next, we set the derivative of Eq. (24)w.r.t. ZUto zero and get the
optimal ZUas:
ğœ•{(1âˆ’ğ›¼)Â·âˆ¥ZUâˆ’XUâˆ¥2
ğ¹+ğ›¼Â·trace(ZâŠ¤
U(Iâˆ’LULâŠ¤
U)ZU)}
ğœ•ZU=0
=â‡’(1âˆ’ğ›¼)Â·(ZUâˆ’XU)+ğ›¼(Iâˆ’LULâŠ¤
U)âŠ¤ZU=0
=â‡’ZU=((1âˆ’ğ›¼)Â·I+ğ›¼Â·(Iâˆ’LULâŠ¤
U))âˆ’1Â·(1âˆ’ğ›¼)XU.
By Neumann series, i.e.,(Iâˆ’M)âˆ’1=Ãâˆ
ğ‘˜=0Mğ‘˜, we have

(1âˆ’ğ›¼)Â·I+ğ›¼(Iâˆ’LULâŠ¤
U)âˆ’1
=
(1âˆ’ğ›¼)Â·I+ğ›¼(Iâˆ’LULâŠ¤
U)âˆ’1
=
Iâˆ’ğ›¼Â·LULâŠ¤
Uâˆ’1
=âˆâˆ‘ï¸
ğ‘Ÿ=0ğ›¼ğ‘ŸÂ·(LULâŠ¤
U)ğ‘Ÿ,
which seals the proof. â–¡
Proof of Lemma 3.1. We first need the following lemma:
Lemma A.2. The optimization objective in Eq. (8)is equivalent to
optimizing: max Ytrace YâŠ¤SY, where Yis defined in Eq. (10)andS
is a|U|Ã—|U| matrix where S[ğ‘–,ğ‘—]=ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—).
Then, by the connection of the Frobenius norm and trace of
matrices, i.e.,âˆ¥Mâˆ¥2
ğ¹=trace(MâŠ¤M)=trace(MMâŠ¤), we have
J=âˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹=trace(RRâŠ¤âˆ’RHYâŠ¤âˆ’YHâŠ¤RâŠ¤+YHâŠ¤HYâŠ¤)
=trace(RRâŠ¤)âˆ’trace(RHYâŠ¤)âˆ’trace(YHâŠ¤RâŠ¤)
+trace(YHâŠ¤HYâŠ¤)
=trace(RRâŠ¤âˆ’2YHâŠ¤RâŠ¤)âˆ’trace(YâŠ¤YHâŠ¤H)
=trace(RRâŠ¤âˆ’2YHâŠ¤RâŠ¤+HâŠ¤H).
The zero gradient conditionğœ•J
ğœ•H=âˆ’2RâŠ¤Y+2Y=0leads to H=
RâŠ¤Y. Hence,J=trace(RRâŠ¤âˆ’2YHâŠ¤RâŠ¤+HâŠ¤H)
=trace(RRâŠ¤)âˆ’2 trace(YYâŠ¤RRâŠ¤)+trace(YâŠ¤RRâŠ¤Y)
=trace(RRâŠ¤)âˆ’2 trace(YâŠ¤RRâŠ¤Y)+ trace(YâŠ¤RRâŠ¤Y)
=trace(RRâŠ¤)+trace(YâŠ¤RRâŠ¤Y). (25)
Since trace(RRâŠ¤)is a constant, maximizing J=âˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹is
equivalent to maximizing trace(YâŠ¤RRâŠ¤Y)=trace(YâŠ¤SY). â–¡
Proof of Lemma A.2. LetSbe a|U|Ã—|U| matrix where S[ğ‘–,ğ‘—]=
ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)andSğ‘‘be a|U|Ã—|U| diagonal matrix in which Sğ‘‘[ğ‘–,ğ‘–]=Ã
ğ‘¢ğ‘—âˆˆUğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—). Then,
1
|Câ„“|âˆ‘ï¸
ğ‘¢ğ‘–âˆˆCâ„“,ğ‘¢ğ‘—âˆˆU\Câ„“ğ‘ (ğ‘¢ğ‘–,ğ‘¢ğ‘—)=1
2âˆ‘ï¸
ğ‘¢ğ‘–,ğ‘¢ğ‘—âˆˆUS[ğ‘–,ğ‘—]Â·(Y[ğ‘–,â„“]âˆ’Y[ğ‘—,â„“])2
=Y[:,â„“]âŠ¤Â·(Sğ‘‘âˆ’S)Â·Y[:,â„“].
Thus, we can rewrite Eq. (8)asminYtrace(YâŠ¤(Sğ‘‘âˆ’S)Y). Note that
it can be further simplified as max Ytrace YâŠ¤SY, which completes
the proof. â–¡
Proof of Theorem 3.2. Letz=bZU[ğ‘–]âˆ’bZU[ğ‘—]andbQ=âˆšï¸
ğ‘‘UÂ·
Q. Based on the definition of Râ€²in Eq. (13), we can get
E[Râ€²[ğ‘–]Â·Râ€²[ğ‘—]]=E"
ğ‘’
ğ‘‘Uğ‘‘Uâˆ‘ï¸
â„“=1ğ‘ ğ‘–ğ‘›(bQ[â„“]Â·bZU[ğ‘–])Â·ğ‘ ğ‘–ğ‘›(bQ[â„“]Â·bZU[ğ‘—])
âˆ’ğ‘ğ‘œğ‘ (bQ[â„“]Â·bZU[ğ‘–])Â·ğ‘ğ‘œğ‘ (bQ[â„“]Â·bZU[ğ‘—])#
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ‘’
ğ‘‘Uğ‘‘Uâˆ‘ï¸
â„“=1ğ‘ğ‘œğ‘ (bQ[â„“]Â·(bZU[ğ‘–]âˆ’bZU[ğ‘—]))ï£¹ï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ‘’
ğ‘‘Uğ‘‘Uâˆ‘ï¸
â„“=1ğ‘ğ‘œğ‘ (bQ[â„“]Â·z)ï£¹ï£ºï£ºï£ºï£ºï£»(26)
By Lemma 5 in [82], for any vector bQ[â„“],E[ğ‘ğ‘œğ‘ (bQ[â„“]Â·z)]
ğ‘’âˆ’âˆ¥zâˆ¥2/2âˆ’
1âˆ’âˆ¥zâˆ¥4
4ğ‘‘Uâ‰¤âˆ¥zâˆ¥4Â·(âˆ¥zâˆ¥4+8âˆ¥zâˆ¥2+8)
16ğ‘‘2
U.
Note that for each ğ‘¢ğ‘–âˆˆU, the row vector bZU[ğ‘–]isğ¿2normalized
(Line 5), i.e.,âˆ¥zâˆ¥2âˆˆ[0,1]. Based thereon, the above inequality can
be transformed into
1âˆ’17
16ğ‘‘2
Uâˆ’1
4ğ‘‘Uâ‰¤E[ğ‘ğ‘œğ‘ (bQ[â„“]Â·z)]
ğ‘’âˆ’âˆ¥zâˆ¥2/2â‰¤1+17
16ğ‘‘2
U+1
4ğ‘‘U.(27)
According to Line 5 of Algorithm 1, âˆ¥bZU[ğ‘–]âˆ¥2=1âˆ€ğ‘¢ğ‘–âˆˆU. Thus,
âˆ¥bZU[ğ‘–]âˆ’bZU[ğ‘—]âˆ¥2=âˆ¥bZU[ğ‘–]âˆ¥2+âˆ¥bZU[ğ‘—]âˆ¥2âˆ’2bZU[ğ‘–]Â·bZU[ğ‘—]
=2(1âˆ’bZU[ğ‘–]Â·bZU[ğ‘—]).
Thus,ğ‘’Â·ğ‘’âˆ’âˆ¥zâˆ¥2
2=ğ‘’Â·ğ‘’âˆ’âˆ¥bZU[ğ‘–]âˆ’bZU[ğ‘—]âˆ¥2
2 =ğ‘’bZU[ğ‘–]Â·bZU[ğ‘—]. Combining
Eq. (27) and Eq. (26) leads to
1âˆ’17
16ğ‘‘2
Uâˆ’1
4ğ‘‘Uâ‰¤E[Râ€²[ğ‘–]Â·Râ€²[ğ‘—]]
ğ‘’bZU[ğ‘–]Â·bZU[ğ‘—]â‰¤1+17
16ğ‘‘2
U+1
4ğ‘‘U.(28)
Further, Eq. (28) implies
1âˆ’17
16ğ‘‘2
Uâˆ’1
4ğ‘‘Uâ‰¤E[Ã
ğ‘¢â„“âˆˆURâ€²[ğ‘–]Â·Râ€²[â„“]]
Ã
ğ‘¢â„“âˆˆUğ‘’bZU[ğ‘–]Â·bZU[â„“]â‰¤1+17
16ğ‘‘2
U+1
4ğ‘‘U.
(29)
3791Effective Clustering on Large Attributed Bipartite Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
According to Eq. (14), we can obtain
E[R[ğ‘–]Â·R[ğ‘—]]=E"
Râ€²[ğ‘–]Â·Râ€²[ğ‘—]âˆšï¸
Râ€²[ğ‘–]Â·rÂ·âˆšï¸
Râ€²[ğ‘—]Â·r#
.
Note that, by râ€™s definition in Eq. (14), we have E[Râ€²[ğ‘–]Â·r]=
E[Ã
ğ‘¢â„“âˆˆURâ€²[ğ‘–]Â·Râ€²[â„“]]. Therefore, plugging Eq. (28)and Eq. (29)
into the above equation proves the theorem. â–¡
Proof of Lemma 3.4. According to Eq. (25), optimizing Eq. (9)
is equivalent to maximizing trace(YâŠ¤RRâŠ¤Y). Using the cyclic prop-
erty of matrix trace, we have trace(YâŠ¤RRâŠ¤Y)=trace(YYâŠ¤RRâŠ¤)
andtrace(ğš¼âŠ¤RRâŠ¤ğš¼)=trace(ğš¼ğš¼âŠ¤RRâŠ¤). Consequently,âˆ¥Râˆ’YHâŠ¤âˆ¥2
ğ¹âˆ’âˆ¥Râˆ’ğš¼HâŠ¤âˆ¥2
ğ¹ (30)
=trace(YâŠ¤RRâŠ¤Y)âˆ’ trace(ğš¼âŠ¤RRâŠ¤ğš¼)
=trace((YYâŠ¤âˆ’ğš¼ğš¼âŠ¤)RRâŠ¤).
The lemma is therefore proved. â–¡
Proof of Lemma 3.3. First, define PUas follows:
PU=(1âˆ’ğ›¼)âˆâˆ‘ï¸
ğ‘Ÿ=0ğ›¼ğ‘ŸÂ·
LULâŠ¤
Uğ‘Ÿ
. (31)
Suppose that ğšªğšºğš¿âŠ¤be the exact top- ğ‘‘SVD of XU, by Eckartâ€“Young
Theorem [ 15] (Theorem A.1), we have âˆ¥ğšªğšºğš¿âŠ¤âˆ’XUâˆ¥2â‰¤ğœğ‘‘+1,
whereğœğ‘‘+1is the(ğ‘‘+1)-th largest singular value of XU. Addition-
ally, we can obtain âˆ¥ğšªğšº2ğšªâŠ¤âˆ’XUXâŠ¤
Uâˆ¥2â‰¤ğœ2
ğ‘‘+1. By the definition
ofPUin Eq. (31), we have
PU=D1/2
UÂ·ğ›¾âˆ‘ï¸
ğ‘Ÿ=0ğ›¼ğ‘¡Â·
Dâˆ’1
UBDâˆ’1
VBâŠ¤ğ‘Ÿ
Â·Dâˆ’1/2
U,
where Dâˆ’1
UBandDâˆ’1
VBâŠ¤are two row-stochastic matrices, i.e., the
entries at each row sum up to 1. Let M=Dâˆ’1
UBDâˆ’1
VBâŠ¤. Since
the multiplication of two row-stochastic matrices yields a row-
stochastic matrix, M=Dâˆ’1
UBDâˆ’1
VBâŠ¤is a row-stochastic matrix,
which further connotes that Mğ‘Ÿ=
Dâˆ’1
UBDâˆ’1
VBâŠ¤ğ‘Ÿ
is row-stochastic,
i.e.,âˆ¥M[ğ‘–]âˆ¥1=Ã
ğ‘¢â„“âˆˆUM[ğ‘–,â„“]=1âˆ€ğ‘¢ğ‘–âˆˆU. Hence, given a matrix
ğš·=Ãâˆ
ğ‘Ÿ=0ğ›¼ğ‘ŸMğ‘Ÿ,
âˆ¥ğš·[ğ‘–]âˆ¥1=âˆâˆ‘ï¸
ğ‘Ÿ=0ğ›¼ğ‘Ÿ=1
1âˆ’ğ›¼âˆ€ğ‘¢ğ‘–âˆˆU.
Therefore, we can derive that
|F[ğ‘–]Â·F[ğ‘—]âˆ’ZU[ğ‘–]Â·ZU[ğ‘—]|
=|(PUğšªğšº)[ğ‘–]Â·(PUğšªğšº)[ğ‘—]âˆ’(PUXU)[ğ‘–]Â·(PUXU)[ğ‘—]|
=PU[ğ‘–]Â·(ğšªğšº2ğšªâŠ¤âˆ’XUXâŠ¤
U)Â·PU[ğ‘—]âŠ¤
=âˆ‘ï¸
ğ‘¢â„“âˆˆUPU[ğ‘–,â„“]Â·âˆ‘ï¸
ğ‘¢â„âˆˆUPU[ğ‘—,â„]Â·
ğšªğšº2ğšªâŠ¤âˆ’XUXâŠ¤
U
[â„“,â„]
â‰¤âˆ‘ï¸
ğ‘¢â„“âˆˆUPU[ğ‘–,â„“]Â·âˆ‘ï¸
ğ‘¢â„âˆˆUPU[ğ‘—,â„]Â·ğœ2
ğ‘‘+1
=âˆ‘ï¸
ğ‘¢â„“âˆˆUâˆšï¸„
DU[ğ‘–,ğ‘–]
DU[â„“,â„“]Â·ğš·[ğ‘–,â„“]Â·âˆ‘ï¸
ğ‘¢â„âˆˆUâˆšï¸„
DU[ğ‘—,ğ‘—]
DU[â„,â„]Â·ğš·[ğ‘—,â„]Â·ğœ2
ğ‘‘+1
â‰¤âˆšï¸
DU[ğ‘–,ğ‘–]Â·DU[ğ‘—,ğ‘—]Â·ğœ2
ğ‘‘+1
1âˆ’ğ›¼,
which finishes the proof. â–¡References
[1]Hasnat Ahmed, Yangyang Zhang, Muhammad Shoaib Zafar, Nasrullah Sheikh,
and Zhenying Tai. 2020. Node embedding over attributed bipartite graphs. In
KSEM. Springer, 202â€“210.
[2]Melissa Ailem, FranÃ§ois Role, and Mohamed Nadif. 2015. Co-clustering document-
term matrices by direct maximization of graph modularity. In CIKM. 1807â€“1810.
[3]Sajjad Athar, Rabeeh Ayaz Abbasi, Zafar Saeed, Anwar Said, Imran Razzak,
and Flora D Salim. 2023. ASBiNE: Dynamic Bipartite Network Embedding for
incorporating structural and attribute information. WWW (2023), 1â€“19.
[4]CÃ©cile Bothorel, Juan David Cruz, Matteo Magnani, and Barbora Micenkova. 2015.
Clustering attributed graphs: models, measures and methods. Network Science 3,
3 (2015), 408â€“444.
[5]Petr Chunaev. 2019. Community detection in node-attributed social networks: a
survey. arXiv preprint arXiv:1912.09816 (2019).
[6]Ganqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. 2020. Adaptive graph encoder
for attributed graph embedding. In SIGKDD. 976â€“985.
[7]Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. 2018. A survey on network
embedding. TKDE 31, 5 (2018), 833â€“852.
[8]Inderjit S Dhillon. 2001. Co-clustering documents and words using bipartite
spectral graph partitioning. In SIGKDD. 269â€“274.
[9]Inderjit S Dhillon, Subramanyam Mallela, and Dharmendra S Modha. 2003.
Information-theoretic co-clustering. In SIGKDD. 89â€“98.
[10] Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonnegative
matrix t-factorizations for clustering. In SIGKDD. 126â€“135.
[11] Issam Falih, Nistor Grozavu, Rushed Kanawati, and YounÃ¨s Bennani. 2017. Anca:
Attributed network clustering algorithm. In Complex Networks.
[12] Barakeel Fanseu Kamhoua, Lin Zhang, Kaili Ma, James Cheng, Bo Li, and Bo
Han. 2023. Grace: A general graph convolution framework for attributed graph
clustering. TKDD 17, 3 (2023), 1â€“31.
[13] Ming Gao, Leihui Chen, Xiangnan He, and Aoying Zhou. 2018. Bine: Bipartite
network embedding. In SIGIR. 715â€“724.
[14] Edward Giamphy, Jean-Loup Guillaume, Antoine Doucet, and Kevin Sanchis.
2023. A survey on bipartite graphs embedding. SNAM 13, 1 (2023), 54.
[15] Gene H Gloub and Charles F Van Loan. 1996. Matrix computations. Johns Hopkins
Universtiy Press, 3rd edtion (1996).
[16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. 6.2. 2.3 softmax units
for multinoulli output distributions. Deep learning 180 (2016).
[17] GÃ©rard Govaert and Mohamed Nadif. 2013. Co-clustering: models, algorithms and
applications. John Wiley & Sons.
[18] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In SIGKDD. 855â€“864.
[19] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure
with randomness: Probabilistic algorithms for constructing approximate matrix
decompositions. SIAM review 53, 2 (2011), 217â€“288.
[20] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History
and context. TIIS5, 4 (2015), 1â€“19.
[21] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means
clustering algorithm. J R Stat Soc Ser C Appl Stat 28, 1 (1979), 100â€“108.
[22] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual
evolution of fashion trends with one-class collaborative filtering. In WWW. 507â€“
517.
[23] Wentao Huang, Yuchen Li, Yuan Fang, Ju Fan, and Hongxia Yang. 2020. Biane:
Bipartite attributed network embedding. In SIGIR. 149â€“158.
[24] Xiao Huang, Jundong Li, and Xia Hu. 2017. Accelerated attributed network
embedding. In ICDM. SIAM, 633â€“641.
[25] Junghoon Kim, Kaiyu Feng, Gao Cong, Diwen Zhu, Wenyuan Yu, and Chunyan
Miao. 2022. ABC: attributed bipartite co-clustering. PVLDB 15, 10 (2022), 2134â€“
2147.
[26] Jungeun Kim, Jae-Gil Lee, Byung Suk Lee, and Jiajun Liu. 2020. Geosocial co-
clustering: A novel framework for geosocial community detection. TIST 11, 4
(2020), 1â€“26.
[27] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[28] Yuval Kluger, Ronen Basri, Joseph T Chang, and Mark Gerstein. 2003. Spectral
biclustering of microarray data: coclustering genes and conditions. Genome
research 13, 4 (2003), 703â€“716.
[29] Lazhar Labiod and Mohamed Nadif. 2011. Co-clustering for binary and categorical
data with maximum modularity. In ICDM. IEEE, 1140â€“1145.
[30] Xinying Lai, Dingming Wu, Christian S Jensen, and Kezhong Lu. 2023. A Re-
evaluation of Deep Learning Methods for Attributed Graph Clustering. In CIKM.
1168â€“1177.
[31] Andrea Lancichinetti, Santo Fortunato, and JÃ¡nos KertÃ©sz. 2009. Detecting the
overlapping and hierarchical community structure in complex networks. New
journal of physics 11, 3 (2009), 033015.
[32] Daniel Lee and H Sebastian Seung. 2000. Algorithms for non-negative matrix
factorization. NeurIPS 13 (2000).
[33] Yiran Li, Renchi Yang, and Jieming Shi. 2023. Efficient and Effective Attributed
3792KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Renchi Yang et al.
Hypergraph Clustering via K-Nearest Neighbor Augmentation. SIGMOD 1, 2
(2023), 1â€“23.
[34] Zhao Li, Xin Shen, Yuhang Jiao, Xuming Pan, Pengcheng Zou, Xianling Meng,
Chengwei Yao, and Jiajun Bu. 2020. Hierarchical bipartite graph neural networks:
Towards large-scale e-commerce applications. In ICDE. 1677â€“1688.
[35] Jie Liu, Zhicheng He, Lai Wei, and Yalou Huang. 2018. Content to node: Self-
translation network embedding. In SIGKDD. 1794â€“1802.
[36] Weifeng Liu, Jose C Principe, and Simon Haykin. 2011. Kernel adaptive filtering:
a comprehensive introduction. Vol. 57. John Wiley & Sons.
[37] Yue Liu, Ke Liang, Jun Xia, Sihang Zhou, Xihong Yang, Xinwang Liu, and Stan Z.
Li. 2023. Dink-Net: Neural Clustering on Large Graphs. (2023).
[38] Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. 2021.
A unified view on graph neural networks as graph signal denoising. In CIKM.
1202â€“1211.
[39] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
estimation of word representations in vector space. arXiv preprint arXiv:1301.3781
(2013).
[40] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality.
Advances in neural information processing systems 26 (2013).
[41] Glenn W Milligan and Martha C Cooper. 1985. An examination of procedures
for determining the number of clusters in a data set. Psychometrika 50 (1985),
159â€“179.
[42] Nairouz Mrabah, Mohamed Bouguessa, Mohamed Fawzi Touati, and Riadh Ksan-
tini. 2022. Rethinking graph auto-encoder models for attributed graph clustering.
TKDE (2022).
[43] Robb J Muirhead. 2009. Aspects of multivariate statistical theory. John Wiley &
Sons.
[44] Guosheng Pan, Yuan Yao, Hanghang Tong, Feng Xu, and Jian Lu. 2021. Unsuper-
vised attributed network embedding via cross fusion. In WSDM. 797â€“805.
[45] Weisen Pan, Shizhan Chen, and Zhiyong Feng. 2013. Automatic clustering
of social tag using community detection. Applied Mathematics & Information
Sciences 7, 2 (2013), 675â€“681.
[46] Georgios A Pavlopoulos, Panagiota I Kontou, Athanasia Pavlopoulou, Costas
Bouyioukos, Evripides Markou, and Pantelis G Bagos. 2018. Bipartite graphs in
systems biology and medicine: a survey of methods and applications. GigaScience
7, 4 (2018), giy014.
[47] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning
of social representations. In SIGKDD. 701â€“710.
[48] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Chi Wang, Kuansan Wang, and Jie
Tang. 2019. Netsmf: Large-scale network embedding as sparse matrix factoriza-
tion. In WWW. 1509â€“1520.
[49] Ali Rahimi and Benjamin Recht. 2007. Random features for large-scale kernel
machines. NeurIPS 20 (2007).
[50] Yuxiang Ren, Hao Zhu, Jiawei Zhang, Peng Dai, and Liefeng Bo. 2021. Ensemfdet:
An ensemble approach to fraud detection based on bipartite graph. In ICDE.
2039â€“2044.
[51] Yiye Ruan, David Fuhry, and Srinivasan Parthasarathy. 2013. Efficient community
detection in large networks using content and links. In WWW.
[52] Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation.
TPAMI 22, 8 (2000), 888â€“905.
[53] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. Line: Large-scale information network embedding. In WWW. 1067â€“1077.
[54] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel MÃ¼ller. 2018.
Verse: Versatile graph embeddings from similarity measures. In WWW. 539â€“548.
[55] Petar VeliÄkoviÄ‡, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio,
and R Devon Hjelm. 2018. Deep Graph Infomax. In ICLR.
[56] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
computing 17, 4 (2007), 395â€“416.
[57] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang.
2019. Attributed graph clustering: a deep attentional embedding approach. In
IJCAI.
[58] Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. 2017.
Mgae: Marginalized graph autoencoder for graph clustering. In CIKM.
[59] Chenguang Wang, Yangqiu Song, Ahmed El-Kishky, Dan Roth, Ming Zhang, and
Jiawei Han. 2015. Incorporating world knowledge to document clustering via
heterogeneous information networks. In SIGKDD. 1215â€“1224.
[60] Hao Wang, Enhong Chen, Qi Liu, Tong Xu, Dongfang Du, Wen Su, and Xiaopeng
Zhang. 2018. A united approach to learning sparse attributed network embedding.
InICDM. IEEE, 557â€“566.
[61] David P Woodruff et al .2014. Sketching as a tool for numerical linear algebra.Foundations and TrendsÂ® in Theoretical Computer Science 10, 1â€“2 (2014), 1â€“157.
[62] Tian Xie, Chaoyang He, Xiang Ren, Cyrus Shahabi, and C-C Jay Kuo. 2022.
L-BGNN: Layerwise Trained Bipartite Graph Neural Networks. TNNLS (2022).
[63] Dongkuan Xu, Wei Cheng, Bo Zong, Jingchao Ni, Dongjin Song, Wenchao Yu,
Yuncong Chen, Haifeng Chen, and Xiang Zhang. 2019. Deep co-clustering. In
SDM. SIAM, 414â€“422.
[64] Panpan Xu, Nan Cao, Huamin Qu, and John Stasko. 2016. Interactive visual
co-cluster analysis of bipartite graphs. In PacificVis. 32â€“39.
[65] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on non-
negative matrix factorization. In SIGIR. 267â€“273.
[66] Zhiqiang Xu, Yiping Ke, Yi Wang, Hong Cheng, and James Cheng. 2012. A
model-based approach to attributed graph clustering. In SIGMOD.
[67] Zongyu Xu, Yihao Zhang, Long Yuan, Yuwen Qian, Zi Chen, Mingliang Zhou,
Qin Mao, and Weibin Pan. 2023. Effective Community Search on Large Attributed
Bipartite Graphs. IJPRAI 37, 02 (2023), 2359002.
[68] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. 2023.
Personalized Showcases: Generating multi-modal explanations for recommenda-
tions. In SIGIR. 2251â€“2255.
[69] Hongqiang Yan, Yan Jiang, and Guannan Liu. 2018. Telecomm fraud detection
via attributed bipartite network. In ICSSSM. 1â€“6.
[70] Liang Yang, Chuan Wang, Junhua Gu, Xiaochun Cao, and Bingxin Niu. 2021.
Why do attributes propagate in graph convolutional neural networks?. In AAAI,
Vol. 35. 4590â€“4598.
[71] Renchi Yang. 2022. Efficient and Effective Similarity Search over Bipartite Graphs.
InTheWebConf. 308â€“318.
[72] Renchi Yang and Jieming Shi. 2024. Efficient High-Quality Clustering for Large
Bipartite Graphs. Proceedings of the ACM on Management of Data 2, 1 (2024),
23:1â€“23:27.
[73] Renchi Yang, Jieming Shi, Keke Huang, and Xiaokui Xiao. 2022. Scalable and
Effective Bipartite Network Embedding. In SIGMOD. 1977â€“1991.
[74] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S Bhowmick.
2020. Homogeneous network embedding for massive graphs via reweighted
personalized PageRank. PVLDB 13, 5 (2020), 670â€“683.
[75] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Sourav S Bhowmick, and
Juncheng Liu. 2023. PANE: scalable and effective attributed network embedding.
VLDBJ (2023), 1â€“26.
[76] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Juncheng Liu, and Sourav S
Bhowmick. 2020. Scaling attributed network embedding to massive graphs.
PVLDB 14, 1 (2020), 37â€“49.
[77] Renchi Yang, Jieming Shi, Yin Yang, Keke Huang, Shiqi Zhang, and Xiaokui Xiao.
2021. Effective and scalable clustering on massive attributed graphs. In WWW.
3675â€“3687.
[78] Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, and Jieming
Shi. 2024. Effective Clustering on Large Attributed Bipartite Graphs. arXiv
preprint arXiv:2405.11922 (2024).
[79] Tianbao Yang, Rong Jin, Yun Chi, and Shenghuo Zhu. 2003. Clustering relational
data using attribute and link information. In Proceedings of the text mining and
link analysis workshop. 9â€“15.
[80] Tianbao Yang, Rong Jin, Yun Chi, and Shenghuo Zhu. 2009. Combining Link
and Content for Community Detection: A Discriminative Approach. In SIGKDD.
927â€“936.
[81] Ting Yao, Tao Mei, Chong-Wah Ngo, and Shipeng Li. 2013. Annotation for free:
Video tagging by mining user search behavior. In Multimedia. 977â€“986.
[82] Felix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N
Holtmann-Rice, and Sanjiv Kumar. 2016. Orthogonal random features. NeurIPS
29 (2016).
[83] Hongyuan Zha, Xiaofeng He, Chris Ding, Horst Simon, and Ming Gu. 2001.
Bipartite graph partitioning and data clustering. In CIKM. 25â€“32.
[84] Lili Zhang, Jennifer Priestley, Joseph DeMaio, Sherry Ni, and Xiaoguang Tian.
2021. Measuring customer similarity and identifying cross-selling products by
community detection. Big data 9, 2 (2021), 132â€“143.
[85] Xiaotong Zhang, Han Liu, Qimai Li, and Xiao-Ming Wu. 2019. Attributed graph
clustering via adaptive graph convolution. In IJCAI. 4327â€“4333.
[86] Yao Zhang, Yun Xiong, Xiangnan Kong, and Yangyong Zhu. 2017. Learning node
embeddings in interaction graphs. In CIKM. 397â€“406.
[87] Tao Zhou, Jie Ren, MatÃºÅ¡ Medo, and Yi-Cheng Zhang. 2007. Bipartite network
projection and personal recommendation. Physical review E 76, 4 (2007), 046115.
[88] Yang Zhou, Hong Cheng, and Jeffrey Xu Yu. 2009. Graph Clustering Based on
Structural/Attribute Similarities. PVLDB 2 (2009), 12.
[89] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. 2021. Interpreting
and unifying graph neural networks with an optimization framework. In WWW.
1215â€“1226.
3793