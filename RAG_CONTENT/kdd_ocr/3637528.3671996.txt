F
AST: An Optimization Framework for Fast Additive
Segmentation in Transparent ML
Brian Liu
Massachusetts Institute of Technology
Cambridge, Massachusetts, USA
briliu@mit.eduRahul Mazumder
Massachusetts Institute of Technology
Cambridge, Massachusetts, USA
rahulmaz@mit.edu
ABSTRACT
We present FAST, an optimization framework for fast additive seg-
mentation. FAST segments piecewise constant shape functions for
each feature in a dataset to produce transparent additive models.
Theframeworkleveragesanoveloptimizationproceduretofitthese
models âˆ¼2ordersofmagnitudefasterthanexistingstate-of-the-art
methods, such as explainable boosting machines [20]. We also de-
velop new feature selection algorithms in the FAST framework to
fit parsimonious models that perform well. Through experiments
and case studies, we show that FAST improves the computational
efficiency and interpretability of additive models.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning .
KEYWORDS
Interpretable Machine Learning; Additive Models; Sparsity
ACM Reference Format:
Brian Liu and Rahul Mazumder. 2024. FAST: An Optimization Framework
for Fast Additive Segmentation in Transparent ML. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671996
1 INTRODUCTION
Additive models are popular in machine learning for balancing a
high degree of explainability with good predictive performance
[2,3].Thesemodels,whenfitonadatasetwith ğ‘features,takethe
formâˆ‘ğ‘
ğ‘—=1ğ‘ ğ‘—(ğ‘¥ğ‘—). Each additive component ğ‘ ğ‘—is the shape func-
tionoffeature ğ‘¥ğ‘—,andsincethecontributionofeachfeaturecanbe
readily observed from its shape function, additive models are said
tobeinherentlytransparent.Onesuchadditivemodel,explainable
boosting machines (EBMs), combines this inherent transparency
with the powerful predictive performance of tree ensembles [20].
EBMs use single-feature decision trees, fit via a cyclic boosting
heuristic, to construct shape functions. As such, the shape func-
tions built are piecewise constant, a departure from classical and
popular smooth components such as those based on polynomials
orsplines[7].Usingpiecewiseconstantshapefunctions,EBMscan
capture discontinuities in the underlying data, patterns that are
This
work is licensed under a Creative Commons Attribu-
tion International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671996unobserved by smooth additive models and which often have real-
world significance [2, 14]. EBMs have also been shown to match
the predictive performance of black box methods in various ap-
plications while preserving model transparency [3]. Due to these
advantages, EBMs are rapidly becoming ubiquitous in high-stakes
applications of ML, such as criminal justice [3] and healthcare [2],
where model explainability is critical.
Inspired by the success of EBMs, and stemming from a reinter-
pretation of the method, we propose an alternative, FAST. FAST
is a formal optimization-based procedure to fit piecewise constant
additive models (PCAMs). Both methods construct piecewise con-
stant shape functions, but FAST does so by minimizing a regu-
larized optimization objective while EBMs use a cyclic boosting
heuristic.
Moreover, the main goal of FAST is to address the limitations
of EBMs that result from this cyclic boosting heuristic. Starting
from the null model, EBMs are fit by cycling round-robin over the
features and building single-feature decision trees on the boosted
residuals, which are dampened by a learning rate. To ensure that
theorderingofthefeaturesisirrelevant,thislearningratemustbe
kept small. As a result, many cyclic boosting iterations and trees
are required to fit an EBM that performs well. This increases the
complexity and computational cost of the algorithm and conse-
quently, EBMs struggle to scale for larger datasets. As a motivat-
ing example, consider the UK Black Smoke dataset (9 million rows
and 14 columns) used by [30] to test the computational feasibility
of splines. It takes the InterpretML package [20] nearly 4 hours
to fit an EBM using the default hyperparameters, which are opti-
mized for computation time. FAST, on the other hand, leverages a
specialized greedy optimization algorithm to fit a PCAM that per-
formsthesameintermsofaccuracyinunder 1minute .Thecyclic
heuristic used to fit EBMs also produces feature-dense models by
design. This may harm interpretability since an EBM fit on a high
dimensional dataset ( ğ‘>50features) will contain too many shape
functions for a practitioner to explain. FAST introduces two novel
feature selection algorithms to remedy this, and these new meth-
odsoutperformexistingfeature-sparsePCAMalgorithmsbyupto
a30%reduction in test error. We summarize the contributions of
our paper below.
Main Contributions
â€¢We introduce FAST, an efficient optimization framework to fit
PCAMs that supports feature sparsity.
 
1863
KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
â€¢FASTusesanovelproceduretoimprovecomputationalefficiency.
To solve optimization problems in FAST, we apply a computa-
tionally cheap greedy block selection rule to an implicit refor-
mulation of our original problem in order to guide a block coor-
dinatedescentalgorithm.ThisprocedurecanfitPCAMs2orders
of magnitude faster than existing SOTA methods.
â€¢We introduce 2 new feature selection algorithms to build sparse
PCAMs,aniterativealgorithmthatreliesonourgreedyblockse-
lection rule and a group â„“0-regularized optimization algorithm.
â€¢We investigate how correlated features impact feature selection
andshapefunctionsinPCAMsanddiscussimplicationsformodel
trustworthiness.
WefirstdiscusstheadvantagesofPCAMsoversmoothadditive
models and overview existing algorithms to build PCAMs. Follow-
ingthesepreliminaries,weintroducetheFASToptimizationframe-
work (Â§2) and present its novelties: the greedy optimization proce-
dure used to accelerate computation (Â§3) and the feature selection
algorithms used to support feature sparsity (Â§4).
An open-source implementation of FAST along with an online
supplementcontainingproofsandexperimentaldetailscanbefound
in thisproject repository1.
1.1 Why PCAMs?
Comparedtosmoothadditivemodelssuchassplines,PCAMshave
the advantage that they are able to capture discontinuities in the
shape functions. These discontinuities can reveal interesting in-
sights about the underlying data. Consider the example shown in
Figure 1. The scatterplot shows the daily number of car accidents
in New York City over a 12-year period and there is a large jump
discontinuity in early 2020 due to the COVID-19 pandemic [23].
This discontinuity is captured by the shape function from a PCAM
(in blue) but is interpolated and obscured by the smoothing spline
(in orange).
2012 2014 2016 2018 2020 2022 2024
Date200400600800Daily Number of Car AccidentsNew York City Car Accidents
Smooth
PCAM
Figur
e 1: PCAM shape
functionscanbeusedto
uncover discontinuities
in the underlying data.
ThediscontinuitiesobservedinPCAMshapefunctionshavebeen
used to uncover hidden patterns in mortality risk models for ICU
patients[14]andpatientswithpneumonia[2].Thesepatternswould
have been difficult to detect with smooth additive models or black-
box methods. PCAMs also have the advantage that fitted piece-
wiseconstantshapefunctionscanberepresentedbyasetofbreak-
points.Asaresult,fittedPCAMsarestraightforwardtoproduction-
izeandcanbehard-codedintoanylanguagewithconditionalstate-
ments (e.g. SQL). Finally, PCAM predictions only require lookups
and addition so PCAMs are extremely fast at inference [20].
1https://github
.com/brianliu12437/FASTopt1.2 Existing PCAM Algorithms:
Asmentionedearlier,EBMsusesingle-featuredecisiontrees,fitvia
a cyclic boosting heuristic, to build PCAMs [18]. EBMs are inter-
pretable and perform well, but are slow to train and feature-dense
by design [20]. Besides EBMs, various methods have been used
to construct PCAMs. Additive isotonic models use isotonic regres-
sion with backfitting to build PCAMs with monotonic shape func-
tions [1]. Spline-based frameworks can also fit PCAMs using zero-
degreesplines[27].Morerecently,thefusedLASSOhasbeenused
tofitPCAMsviaADMM[4]orcyclicblockcoordinatedescent[25].
The latter approach is better known as the fused LASSO additive
model (FLAM) and is considered a SOTA algorithm for building
PCAMs. As such, we primarily compare FAST against FLAMs and
EBMs for fitting feature-dense PCAMs.
2 FAST OPTIMIZATION FRAMEWORK
WeintroduceFASTandoutlinetheoptimizationalgorithmusedto
solve problems in our framework. More importantly, we motivate
why our greedy optimization procedure (Â§3) improves efficiency.
Given data matrix ğ‘‹âˆˆRğ‘›Ã—ğ‘and target vector ğ‘¦âˆˆRğ‘›, our goal
is to fit additive modelâˆ‘ğ‘
ğ‘—=1ğ‘ ğ‘—(ğ‘¥ğ‘—), where each shape function ğ‘ ğ‘—
is piecewise constant. To accomplish this, we introduce a decision
variable for each entry in ğ‘‹. These decision variables are grouped
into decision vectors ğ›½ğ‘—âˆˆRğ‘›forğ‘—âˆˆ [ğ‘], where each decision
vectorğ›½ğ‘—representstheblockofdecisionvariablesthatcorrespond
to featureğ‘¥ğ‘—. The decision variables in ğ›½ğ‘—are ordered with respect
to the sorted values of ğ‘¥ğ‘—and the sum of decision vectors gives
the prediction of our model. Wefit this prediction to ğ‘¦and recover
shape functions ğ‘ ğ‘—from the fitted decision vectors ğ›½âˆ—
ğ‘—.
2.1 Optimization Problem
Letğ›½denote the set of decision vectors {ğ›½1...ğ›½ ğ‘}. FAST mini-
mizes the objective ğ¿(ğ‘¦,ğ›½) +ğ‘†(ğ›½)to fit PCAMs, where ğ¿is a loss
functionthatcapturesdatafidelityand ğ‘†isasegmentationpenalty
that encourages piecewise constant segmentation in the fitted de-
cision vectors. The optimization problem can be written as:
min
ğ›½1,...,ğ›½ ğ‘1
2âˆ¥ğ‘¦âˆ’ğ‘âˆ‘
ğ‘—=1ğ‘„âŠº
ğ‘—ğ›½ğ‘—âˆ¥2
2+ğœ†ğ‘“ğ‘âˆ‘
ğ‘—=1âˆ¥ğ·
ğ›½ğ‘—âˆ¥1.(1)
Thefirsttermintheobjectiveisquadraticloss,where ğ‘„ğ‘—âˆˆ {0,1}ğ‘›Ã—ğ‘›
isthesquaresortingmatrixforfeature ğ‘¥ğ‘—.Inotherwords, ğ‘„ğ‘—ğ‘¥ğ‘—re-
turnstheelementsof ğ‘¥ğ‘—sortedinascendingorderand ğ‘„âŠº
ğ‘—(ğ‘„ğ‘—ğ‘¥ğ‘—)=
ğ‘¥ğ‘—. Since each decision vector ğ›½ğ‘—is ordered with respect to the
sorted values of ğ‘¥ğ‘—,âˆ‘ğ‘
ğ‘—=1ğ‘„âŠº
ğ‘—ğ›½ğ‘—gives the prediction of our model.
The second term in the objective is the fused LASSO segmentation
penalty, where ğœ†ğ‘“is the parameter that controls the number of
piecewiseconstantsegmentsintheshapefunctions.Highervalues
ofğœ†ğ‘“result in less flexible shape functions with fewer segments.
Matrixğ·âˆˆ {âˆ’ 1,0,1}(ğ‘›âˆ’1)Ã—ğ‘›isthedifferencingmatrix,where ğ·ğ›½ğ‘—
returns a vector of the successive differences of ğ›½ğ‘—.
Problem1fitsfeature-densePCAMs.Anoptionalgroupsparsity
constraint can applied over the blocks ğ›½ğ‘—to select features and we
discuss this further in Â§4.
 
1864FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
2.2 Optimization Algorithm
Problem 1 is convex and separable over blocks ğ›½ğ‘—; we develop a
block coordinate descent (BCD) algorithm to solve this problem to
optimality. Our algorithm has two components: block selections
and block updates, and starting with all blocks ğ›½ğ‘—=0we alternate
between the two until convergence.
Block Updates: It is critical to note that block updates in FAST
areexpensive.Foraselectedblock ğ‘˜,letğ›¿={1...ğ‘}\ğ‘˜anddefine
residual vector ğ‘Ÿ=ğ‘¦âˆ’âˆ‘
ğ‘—âˆˆğ›¿ğ‘„âŠº
ğ‘—ğ›½ğ‘—. Each block update solves:
min
ğ›½ğ‘˜1
2âˆ¥ğ‘Ÿâˆ’ğ‘„âŠº
ğ‘˜ğ›½ğ‘˜âˆ¥2
2+ğœ†ğ‘“âˆ¥ğ·
ğ›½ğ‘˜âˆ¥1, (2)
whichisequivalenttoafusedLASSOsignalapproximation(FLSA)
problem on ğ‘„ğ‘˜ğ‘Ÿ. These FLSA problems are solved using dynamic
programming [11] which is computationally expensive.
Block Selections: Since block updates are expensive, improv-
ingtheefficiencyofourBCDalgorithmreliesonreducingthenum-
berofblockupdatesthatweconduct.Todoso,wetrytoselectthe
block that makes the most progress towards the optimal solution
in each BCD iteration. Other selection rules, such as cyclic or ran-
domizedselection[22],bottleneckBCDwithunnecessaryupdates.
We also must select blocks cheaply since block selection would
be ineffective if the cost of selecting the best block to update is
similar to the cost of updating all blocks. One novelty in FAST is
that we develop a greedy optimization procedure to select blocks
extremely efficiently. We present this procedure below.
3 GREEDY OPTIMIZATION PROCEDURE
Our greedy optimization procedure hinges on the fact that we can
transform Problem 1 into an equivalent LASSO problem with ğ‘›
rows and (ğ‘›âˆ’1)ğ‘variables. While many LASSO algorithms exist
[6], it is infeasible to solve this problem directly since there are
too many variables when ğ‘›is large and the variables are heavily
correlated by design [26]. Rather, we use this LASSO reformulated
problem to guide block selection when we apply BCD to Problem
1.
Importantly, we exploit the structure of the design matrix in
our LASSO reformulation to derive an extremely efficient block
selection rule. In fact, our block selection rule only requires an im-
plicit LASSO reformulation of the original problem (Problem 1),
where the design matrix is not explicitly constructed. This is cru-
cialsinceconstructingthedesignmatrixrequiresaspacecomplex-
ity ofğ‘‚(ğ‘›2ğ‘), which is infeasible for large data. For example, the
designmatrixfortheUKBlackSmokeproblem(9millionrowsand
14columns)mentionedintheintroductionwouldtakeover 109TB
of memory if explicitly constructed.
3.1 Implicit LASSO Reformulation
We define a new set of decision vectors ğœƒğ‘—âˆˆRğ‘›âˆ’1forğ‘—âˆˆ [ğ‘],
where each vector ğœƒğ‘—contains the successive differences of vector
ğ›½ğ‘—.Letğ´âˆˆ {0,1}ğ‘›Ã—(ğ‘›âˆ’1)beapaddedlowertriangularmatrixwith
zeros in the first row. We first reformulate Problem 1 as:
min
ğœƒ1,...,ğœƒ ğ‘1
2âˆ¥ğ‘¦âˆ’ğ‘âˆ‘
ğ‘—=1ğ‘„âŠº
ğ‘—ğ´
ğœƒğ‘—âˆ¥2
2+ğœ†ğ‘“ğ‘âˆ‘
ğ‘—=1âˆ¥ğœƒğ‘—âˆ¥1.(3)LetğœƒâˆˆR(ğ‘›âˆ’1)ğ‘represent the decision vectors {ğœƒ1...ğœƒ ğ‘}ver-
tically stacked. Let ğ´â€²âˆˆ {0,1}ğ‘›ğ‘Ã—(ğ‘›âˆ’1)ğ‘be the matrix formed
by stacking ğ´submatrices ğ‘times along the main diagonal. Let
ğ‘„âŠºâˆˆ {0,1}ğ‘›ğ‘Ã—ğ‘›ğ‘be the matrix formed by stacking {ğ‘„âŠº
1...ğ‘„âŠº
ğ‘}
along the main diagonal. Finally, let ğ‘€âˆˆ {0,1}ğ‘›Ã—ğ‘›ğ‘be the matrix
formedbystacking ğ‘identitymatricesofdimension ğ‘›Ã—ğ‘›horizon-
tally. We show a visualization of these matrices in the appendix
(suppl. A). Problem 3 and Problem 1 are equivalent to:
min
ğœƒ1
2âˆ¥ğ‘¦âˆ’ğ‘€
ğ‘„âŠºğ´â€²ğœƒâˆ¥2
2+ğœ†ğ‘“âˆ¥ğœƒâˆ¥1, (4)
whichisablock-separableLASSOproblemwithdesignmatrix ğ‘€ğ‘„âŠºğ´â€²âˆˆ
Rğ‘›Ã—(ğ‘›âˆ’1)ğ‘. We show in the next section that we do not need to
construct this matrix for our greedy selection rule.
3.2 Block Selection Rule (BGS rule)
Since Problem 4 is an equivalent LASSO reformulation of Problem
1,weusethisreformulationtoselectwhichblockstoupdatewhen
performing BCD on Problem 1. For each BCD iteration, we apply
a block Gauss Southwell (BGS) greedy selection rule to Problem 4
to select the next block to update. BGS selection has been shown
in theory and in practice to make more progress per iteration than
cyclic or random selection [5, 21], however, on many problems,
BGS selection is prohibitively expensive [22]. One critical aspect
of our procedure is that we exploit problem structure to develop a
BGS steepest direction (BGS-s) rule that is cheap to compute.
Letğ‘“(ğœƒ)=1
2âˆ¥ğ‘¦âˆ’ğ‘€
ğ‘„âŠºğ´â€²ğœƒâˆ¥2
2. For BGS-s selection, we first com-
pute vector ğ‘‘âˆˆRğ‘›ğ‘which stores the magnitude of the most neg-
ative directional derivative for each coordinate. This vector is de-
fined coordinate-wise by
ğ‘‘ğ‘–={
|ğ‘†ğœ†ğ‘“(âˆ‡ğ‘–ğ‘“(ğœƒ)| ifğœƒğ‘–=0
|âˆ‡ğ‘–ğ‘“(ğœƒ) +sign(ğœƒğ‘–)ğœ†ğ‘“|ifğœƒğ‘–â‰ 0,(5)
whereğ‘†ğœ†ğ‘“is the soft-thresholding operator. Let ğ‘‘ğ‘˜âˆˆRğ‘›represent
theelementsinvector ğ‘‘associatedwithblock ğ‘˜.Weselectthebest
blockğ‘˜âˆ—to update via:
ğ‘˜âˆ—=arg max
ğ‘˜âˆˆ [ğ‘]âˆ¥ğ‘‘ğ‘˜âˆ¥2
2.(6)
Equations 5 and 6 form our BGS selection rule, which is com-
putationally bottlenecked by the cost of computing the full gradi-
entâˆ‡ğ‘“(ğœƒ). The LASSO design matrix ğ‘€ğ‘„âŠºğ´â€²is also only used to
computethisgradient.Below,weshowhowtoefficientlycompute
gradient âˆ‡ğ‘“(ğœƒ)without forming the LASSO design matrix.
FastGradientProcedure: Wehavethat âˆ‡ğ‘“(ğœƒ)=âˆ’ğ´â€²âŠºğ‘„ğ‘€âŠºğ‘Ÿâ€²,
whereğ‘Ÿâ€²=ğ‘¦âˆ’ğ‘€ğ‘„âŠºğ´â€²ğœƒ.Sinceouralgorithmiszero-initialized,we
can storeğ‘Ÿâ€²and update the residual vector at each BCD iteration
to avoid multiplying the design matrix with ğœƒ. Matrixğ‘€âŠºconsists
ofğ‘identity matrices stacked vertically which makes the gradient
expressionblock-separable.Forafixedblock ğ‘˜âˆˆ [ğ‘],wehavethat
âˆ‡ğ‘˜ğ‘“(ğœƒ)=âˆ’ğ´âŠºğ‘„ğ‘˜ğ‘Ÿâ€², whereğ‘„ğ‘˜is the sorting matrix for feature ğ‘¥ğ‘—.
The matrixğ´âŠºis a padded upper triangular matrix, so computing
thegradientforblock ğ‘˜simplyinvolvesordering ğ‘Ÿâ€²withrespectto
the sorted values of ğ‘¥ğ‘—and taking a rolling sum down the ordered
vector, which is extremely efficient. Computing the full gradient
can be embarrassingly parallelized across blocks.
 
1865KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
With this procedure, our BGS-s selection rule is efficient, par-
allelizable, and can be computed without constructing the LASSO
design matrix. Below, we formalize our greedy block coordinate
descent (GBCD) algorithm and analyze its convergence properties.
3.3 BGS-GBCD Algorithm
To solve Problem 1, we use the following GBCD algorithm. Start
withğ›½ğ‘—=0for all blocks ğ‘—âˆˆ [ğ‘]and repeat until convergence: ap-
ply our BGS selection rule to Problem 4 (LASSO reformulation) to
selectablocktoupdateandsolveProblem2(originalblockupdate
problem) with dynamic programming to update the block. This re-
turns a sequence of solutions ğ›½ğ‘¡that correspond to a sequence of
decreasing objective values.
3.3.1 Convergence Analysis. Thesequenceofsolutions ğ›½ğ‘¡returned
by BGS-GBCD converges to the minimizer for Problems 1 and 4.
More generally, we show that BGS-GBCD converges to optimality
when applied to block-separable LASSO problems. We prove the
next proposition in the appendix (suppl. B.1).
PRoposition 1. Given composite problem
min
ğœƒğ¹(ğœƒ)=ğ‘“(ğœƒ) +ğœ†âˆ¥ğœƒâˆ¥1,
whereğ‘“is convex and coordinate-wise L-smooth and ğœƒis both block
and coordinate separable, every limit point of BGS-GBCD coincides
with a minimizer for ğ¹(ğœƒ). Any sequence of solutions ğœƒğ‘¡generated
by BGS-GBCD converges to a limit/minimum point.
Weproveforthefirsttimethatgreedyblockcoordinatedescent
using block Gauss-Southwell-s selection converges to the mini-
mum point when applied to â„“1-composite problems. We also show
that under certain conditions, BGS-GBCD updates make provably
good progress towards the minimum. Proposition 2 states a prop-
erty that we exploit in Â§4 when developing feature selection algo-
rithms.
PRoposition 2. If blockğœƒğ‘¡
ğ‘˜=0is selected via the BGS rule, the
progress after one GBCD update is bound by:
ğ¹(ğœƒğ‘¡+1) âˆ’ğ¹(ğœƒğ‘¡) â‰¤ min
ğ›¾âˆˆRğ‘›âˆ‡ğ‘“(ğœƒğ‘¡)âŠºğ›¾+
L
2snorm (ğ›¾)
+ğœ†âˆ¥ğœƒğ‘¡+ğ›¾âˆ¥1âˆ’ğœ†âˆ¥ğœƒğ‘¡âˆ¥1,
where snorm( ğ›¾) sums theâ„“2-norm of each block in ğ›¾.
Each block in our optimization problem corresponds to a fea-
ture;ğ›½ğ‘—gives the contribution of feature ğ‘¥ğ‘—to the additive model.
Proposition 2 states that when the BGS rule is used to select a fea-
ture (block) to enter the support, the corresponding block update
makes substantial progress towards the minimum. The proof for
this proposition is also in the appendix (suppl. B.2).
3.4 Discussion
In most LASSO problems, greedy selection offers little advantage
over cyclic selection since the computational cost of selecting the
blockwiththesteepestdirectionalderivativesissimilartothecost
of updating all of the blocks [31]. Greedy BCD is effective in FAST,
however,sinceblockselections,whichinvolveembarrassinglypar-
allel summations are much cheaper than block updates, which re-
quire expensive dynamic programming calls.We observe that BGS-GBCD greatly reduces the number of dy-
namic programming block updates (Problem 2) required to solve
Problem 1, compared to cyclic block selection. For example, in Fig-
0 1 2 3 4
# Dynamic Programming Block Updates (log10)0:10:20:30:40:5Training ErrorCyclic vs. Greedy Block Selection
Cyclic Block Selection
Greedy Block Selection
Figur
e2:Greedyselec-
tion reduces the num-
ber of dynamic pro-
gramming block up-
dates by 2 orders of
magnitude.
ure 2, we use greedy and cyclic BCD to fit FAST on the Elevators
dataset [29]. The horizontal axis shows the number of dynamic
programming block updates (log10) and the vertical axis shows
training loss. We observe that BGS-GBCD requires nearly 100Ã—
fewerupdatestoconverge.Thiscorrespondstosubstantialcompu-
tational speedups, which we show in our experiments in Â§5.1.
3.5 Binning
FAST can also incorporate binning, a popular heuristic used by
EBMs [20] and LightGBMs [13], to reduce computation time for
a nominal cost in model expressiveness. FAST performs binning
using a novel equivalent optimization formulation while existing
methods,suchasEBMs,pre-processthedata.Givenasetofbinsfor
each feature ğ‘¥ğ‘—, we add the constraints for all entries (ğ‘–1,ğ‘–2) âˆˆ [ğ‘›]
that if entries (ğ‘¥ğ‘—)ğ‘–1and(ğ‘¥ğ‘—)ğ‘–2fall in the same bin, then (ğ›½ğ‘—)ğ‘–1=
(ğ›½ğ‘—)ğ‘–2.Weshowintheappendix(suppl.C)thatwecanreformulate
these constraints into a weighted smooth loss function in the ob-
jective and efficiently solve this unconstrained problem with BGS-
GBCD. Binning directly reduces the number of decision variables
in FAST by a factor of # bins over # rows and combining BGS-
GBCD with binning further reduces computation time.
4 FEATURE-SPARSE FAST
OurFASTframeworkisquiteflexible;herewediscussanextension
of the framework to explicitly account for variable selection. We
add this group sparsity constraint to Problem 1:âˆ‘ğ‘
ğ‘—=1/x31(ğ›½ğ‘—â‰ 0) â‰¤
ğ¾, whereğ¾is the maximum number of features to select. Problem
1withthisconstraintisNP-hardanddifficulttosolvetooptimality
due to the large number of variables; we have a variable for each
entry ofğ‘‹. As such, we develop two approximate algorithms to
find good solutions. These algorithms have different strengths in
terms of solution quality and runtime, but both algorithms rely
on the BGS rule presented in Â§3.2 and the fact that BGS selection
makesprovablygoodprogresswhenselectingfeaturestoenterthe
support (Prop. 2).
4.1 Approximate Greedy Iterative Selection
For Approximate Greedy Iterative Selection (AGIS), we partition
theblocksintothesupport ğ‘†={ğ‘—âˆˆ [ğ‘] |ğ›½ğ‘—â‰ 0}andcomplement
ğ‘†ğ‘andstartwithallblocksequalto 0.WeusetheBGSruletoselect
 
1866FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
the best block ğ‘˜âˆˆğ‘†ğ‘to update and we perform a block update by
solving Problem 2 to add ğ‘˜intoğ‘†. If|ğ‘†|>1, we iterative through
the blocks in ğ‘†and conduct block updates until convergences. We
repeat this procedure, interlacing BGS selection with sweeps on
the support ğ‘†until the condition |ğ‘†|=ğ¾is reached. AGIS returns
asequenceofPCAMswitheveryfeaturesparsitylevelfrom 1...ğ¾.
To improve solution quality across all sparsity levels we apply this
local search heuristic.
4.1.1 BGS Local Search: After each sweep of ğ‘†converges, use the
BGS rule to select the best block to update in ğ‘†ğ‘and denote that
blockğ›½âˆ—
ğ‘—. This is the block that we will swap into the support. To
find the best block to swap out of the support, iterate over ğ›½ğ‘—âˆˆğ‘†.
For each block, set ğ›½ğ‘—=0and conduct a block update on ğ›½âˆ—
ğ‘—, and
selecttheblockin ğ‘†thatwhenswappedimprovestheobjectivethe
most. After this swap, conduct another sweep over ğ‘†until conver-
gence to obtain the final solution. We present our full AGIS algo-
rithm, with local search, in Algorithm 1.
Algorithm
1:AGIS
Input:ğ¾,ğœ†ğ‘“,ğ·,ğ‘„ğ‘—âˆ€ğ‘—âˆˆ
[ğ‘]
1Initializeğ›½ğ‘—=0âˆ€ğ‘—âˆˆ [ğ‘],ğ‘†=âˆ…,ğ‘†ğ‘ğ‘™ğ‘™=âˆ…
2repeat
3Use
BGS rule to select ğ‘˜âˆˆğ‘†ğ‘.
4Update block ğ‘˜(Problem 2).
5ğ‘†=ğ‘†âˆªğ›½ğ‘˜, ğ‘†ğ‘=ğ‘†ğ‘\ğ›½ğ‘˜
6repeat
7 Sw
eep through ğ‘†and update blocks (Problem 2).
8until converged
9BGS local search.
10ğ‘†ğ‘ğ‘™ğ‘™=ğ‘†ğ‘ğ‘™ğ‘™âˆªğ‘†
11until |ğ‘†|=ğ¾
Output: Sequence of models ğ‘†ğ‘ğ‘™ğ‘™
4.2
Groupâ„“0-FAST
InadditiontoAGIS,wecanuseagroup â„“0-sparsitypenaltytoselect
features in FAST. This approach often obtains better solutions at
the cost of increased computation time, which we discuss in Â§5.2.
We use this Lagrangian formulation:
min
ğ›½1
2âˆ¥ğ‘¦âˆ’ğ‘âˆ‘
ğ‘—=1ğ‘„âŠº
ğ‘—ğ›½ğ‘—âˆ¥2
2+
ğœ†ğ‘“ğ‘âˆ‘
ğ‘—=1âˆ¥ğ·
ğ›½ğ‘—âˆ¥1+ğœ†ğ‘ ğ‘âˆ‘
ğ‘—=1/x31(ğ›½ğ‘—â‰ 0),(7)
whereğœ†ğ‘ isthesparsityhyperparameter.Thegroupsparsitypenalty
is block-separable over ğ›½ğ‘—so we can apply BCD methods to find
good solutions to this problem. Given fixed block ğ‘˜and residual
vectorğ‘Ÿ, we can write each block update problem as:
min
ğ›½ğ‘˜1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ’ğ›½ğ‘˜âˆ¥2
2+ğœ†ğ‘“âˆ¥ğ·
ğ›½ğ‘˜âˆ¥1+ğœ†ğ‘  /x31(ğ›½ğ‘˜â‰ 0).This problem can be solved by first setting ğœ†ğ‘ =0and solving the
FLSA forğ›½âˆ—
ğ‘˜. We then check the thresholding condition:
1
2âˆ¥ğ‘Ÿâˆ¥2
2âˆ’1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ’ğ›½âˆ—
ğ‘˜âˆ¥2
2âˆ’ğœ†ğ‘“âˆ¥ğ·
ğ›½âˆ—
ğ‘˜âˆ¥1â‰¤ğœ†ğ‘ 
and setğ›½âˆ—
ğ‘˜=0if the condition is satisfied. We show the derivation
for this in the appendix (suppl. D).
Sincethegroupsparsitypenaltyisnotcontinuous,itisnotclear
if BGS-GBCD can be extended here. To find high-quality solutions
to Problem 7, we use cyclic block coordinate descent and apply
our BGS local search heuristic (Â§4.1.1) when CBCD converges. We
interlace CBCD sweeps with local search steps until the objective
no longer improves.
4.3 Discussion
We show an example of the impact of local search on solution
quality and discuss the strengths and weaknesses of both group
â„“0-FAST and AGIS.
2 4 6 8 100:20:30:40:50:6Test MSEAGIS no LS
AGIS w/ LS
2 4 6 8 10Group`0no LS
Group`0w/ LS
Number of FeaturesEect of Local Search
Figur
e 3: BGS local search improves the solution quality for
both of our feature-sparse PCAM algorithms.
4.3.1 Local Search Performance. We observe empirically that our
BGSlocalsearchheuristicimprovestheout-of-sampleperformance
of both feature selection algorithms. For example, in Figure 3, we
usegroupâ„“0-FASTandAGIStobuildfeature-sparsePCAMsonthe
Elevatorsdataset(16500rowsand16columns)[29].Wevary ğ¾,the
sparsitybudgetinthemodelfrom1to10,andcomparethetestper-
formance of the model measured via MSE. For both methods, the
local search heuristic improves performance.
4.3.2 Group â„“0-FAST vs. AGIS. In our experiments in Â§5.2, we ob-
serve that group â„“0-FAST generally outperforms AGIS at building
sparse PCAMs. AGIS, however, is computationally faster since the
algorithm can leverage greedy block selection. Fitting group â„“0-
FASTrequiresCBCDupdatesduetothenon-convexityofthegroup
sparsity penalty. In addition, AGIS is easier to use since the algo-
rithm by design outputs a sequence of PCAMs with with every
support size from 1toğ¾. The sparsity hyperparameter ğœ†ğ‘ in group
â„“0-FASTmustbetunedandthealgorithmmayskipcertainsupport
sizes due to non-convexity [9].
5 EXPERIMENTS
We evaluate the computation time of FAST against existing algo-
rithms and assess how well the framework performs at building
feature-sparse PCAMs.
 
1867KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
5.1 Computation Time Experiment
We compare the computation time of FAST against existing SOTA
algorithms for building feature-dense PCAMs: EBMs and FLAMs.
5.1.1 Experimental Procedure. On 10 large regression benchmark
datasets from OpenML [29], we use FAST, EBM, and FLAM to fit
PCAMs.Forthecompetingmethods,weusetheInterpretMLpack-
age[20]tofitEBMsinPythonandtheFLAMpackageinR[24].We
use the default hyperparameters for InterpretML EBMs, which are
optimized for fast runtime. For FLAM, we match the fusion hy-
perparameter with the value used in FAST. The test errors of the
models fit using the 3 methods, under these configurations, are
comparable (as intended). We conduct this experiment on a M2
MacbookProwith10coresandmatchthenumberofcoresusedin
the methods that support multiprocessing (FAST and EBMs). Ad-
ditional details can be found in the appendix (suppl. E).
Dataset
/ Method F
AST EBM FLAM
Black
Smoke +
(9214951, 41)329.6s
(1.2)
0.6315h
49m 31s
0.63__
Black
Smoke
(9214951, 14)43s
(2.8)
0.623h
57m 9s
0.61__
P
hysiochemical
(5023496, 9)33.4s
(0.3)
0.5043m
31s (31.9)
0.50__
A
uto Horsepower
(900000, 17)1.63s
(0.01)
0.6385s
(3.9)
0.64__
Ailer
ons BNG
(669994, 38)2.57s
(0.08)
0.5785s
(2.0)
0.56__
Slice
Localization
(35845, 351)6.7s
(0.05)
0.8258.7s
(2.2)
0.8114m
50s (30.4)
0.81
Sup
erconduct
(21263, 79)0.45s
(0.01)
0.887.6s
(0.05)
0.8913.0s
(0.03)
0.87
Scm1d
(8828,
280)0.7s
(0.01)
0.927.9s
(0.47)
0.91190s
(5.1)
0.90
Rf2
(8212,
448)1.34s
(.01)
0.9878.5s
(4.3)
0.98180s
(3.0)
0.97
Isolet
(7017,
613)2.50s
(0.02)
0.728.67s
(0.3)
0.70360s
(8.0)
0.68
T
able 1: Timing experiment results. FAST achieves 2 orders
of magnitude speedups for large problems.
5.1.2 Results. Table 1 shows the results of our experiment. The
leftmost column shows dataset names and dimensions: (ğ‘›,ğ‘). In
each cell in the other columns, the top entry shows the computa-
tion time of the method averaged over runs along with the stan-
dard deviation. The bottom entry shows the test ğ‘…2of the model.
Thetop5rowsofthistableshowtimingresultsonlarge ğ‘›datasets
with more than 500000rows. On these datasets, we are unable to
apply FLAM due to problem scale so we compare FAST against
EBMs. We observe that FAST fits PCAMs up to 2 orders of magni-
tude faster than EBMs. For example on an augmented version of
the UK Black Smoke dataset, with 9 million rows and 41 columns,
it takes over 15 hours to fit an EBM. FAST on the other hand can
fit a PCAM that performs the same in around 5 minutes.
The bottom 5 rows of Table 1 show results on large ğ‘datasets
with more than 50 columns, but less than 50,000 rows. We observeherethatFASTfitsPCAMs2ordersofmagnitudefasterthanFLAM
and around 1 order of magnitude faster than EBMs. For example
on the Slice Localization dataset, with over 300 columns, it takes
nearly 15 minutes to fit a FLAM. FAST can fit a PCAM that per-
forms the same in under 10 seconds.
In all, we find that FAST substantially outperforms EBMs and
FLAMs in terms of computation time across various large prob-
lems.
0 10 20 30 400:40:5Test MSE1
2
3
4
5Black Smoke n107
0:0 0:5 1:0 1:50:50:61
2
3
45Auto Horsepower n106
Training Time (seconds)
Figur
e 4: FAST can fit low optimization tolerance models
that perform well extremely quickly.
5.1.3 Low Optimization Tolerance Models. As an aside, we note
that we can leverage our greedy BCD algorithm to fit FAST with
low optimization tolerances, in order to quickly produce a PCAM
thatstillperformswellout-of-sample.InFigure4,weshowthetest
error of FAST (vertical axes) plotted against the training time in
seconds(horizontalaxes)fortheUKBlackSmokeandAutoHorse-
power [29] datasets. We vary the training time of FAST by early-
stopping the optimization algorithm after a fixed number of itera-
tions, the first 5 GBCD iterations are plotted in red. In both exam-
ples,thefirst5iterationsgreatlyreducethe testerrorofthemodel.
For the UK Black Smoke dataset, FAST can fit a low optimization
tolerance model that performs well in less than 10 seconds.
5.2 Feature Selection Experiment
Here we evaluate how well FAST performs at building feature-
sparse PCAMs.
5.2.1 Experimental Setup. We repeat this procedure on 20 regres-
sion datasets from OpenML and use a 10-fold CV on each dataset.
The full list of datasets can be found in the appendix (suppl. E). On
the training folds, we use group â„“0-FAST and AGIS to fit feature-
sparse PCAMs by varying the sparsity budget ğ¾âˆˆ {2,4,6,8,10}.
We evaluate the MSE of each sparse model on the test fold.
We compare the performance of these models against the fol-
lowing SOTA algorithms to construct feature-sparse PCAMs.
â€¢FLAM-GL (2016): In FLAM group LASSO [25], we fit a FLAM
withagroupLASSOpenaltyoverthefeatures.Wetunethespar-
sity hyperparameter such that at most ğ¾features are selected.
â€¢EBM-RS(2019): InEBMrankandselect[20],wefirstfitanEBM
on the training data and rank the features by importance scores;
the contribution of each feature averaged over the training ob-
servations. We select the top ğ¾features and refit an EBM. This
method is computationally expensive since it fits two PCAMs.
 
1868FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Comp
eting Alg. /
Sparsity2 4 6 810
FLAM-GL84.1%
81.5%102.0%
81.6%77.9%
62.3%50.8%
48.6%45.1%
41.4%
EBM-RS48.9%
45.1%24.3%
18.3%29.7%
22.7%23.7%
22.0%22.8%
19.8%
Contr
olBurn28.4%
27.7%57.5%
47.5%71.3%
60.1%68.1%
65.4%70.4%
65.6%
FastSparseGAM131.5%
126.5%85.6%
73.4%72.4%
60.7%50.9%
48.6%48.2%
44.2%
SAM103.5%
102.4%64.0%
51.7%52.8%
40.4%30.0%
28.4%23.5%
20.5%
LASSO89.6%
89.5%71.9%
65.8%56.1%
49.1%46.0%
44.1%41.2%
38.2%
T
able 2: Average % decrease in test error between feature-
sparse FAST and our competing algorithms across sparsity
budgets(distributionsshowninFigure5).Positivevaluesin-
dicate that feature-sparse FAST outperforms the competing
algorithm. In each cell, the top value shows group â„“0-FAST
and the bottom value shows AGIS.
â€¢ControlBurn (2021): ControlBurn [15] is a flexible framework
for building feature-sparse nonlinear models. The feature selec-
tion algorithm in the framework first constructs a specialized
tree ensemble that is diverse, where each tree in the ensemble
uses a different subset of features. Then, the weighted LASSO is
used to select feature-sparse subsets of trees that perform well.
We refit the final model, in this case, an EBM, on the ğ¾selected
features. ControlBurn with an EBM has been used to construct
high-performing,feature-sparsePCAMsforheartfailurepredic-
tion in clinical machine learning [28].
â€¢FastSparseGAM (2022): FastSparseGAM [16] is a package for
sparseregressionbuiltontopoftheL0Learnframework[9].The
packagecanbeadaptedtoconstruct extremely sparsePCAMsby
one-hotencodingthefeaturesandselectingasmallsubsetofthe
resulting components [17].
We also compare feature-sparse FASTagainst two traditional algo-
rithmsthatproducenon-piecewiseconstantadditivemodels,Sparse
Additive Models (SAM), which uses the group LASSO to sparsify
splines, and the linear LASSO. Additional details on our experi-
mental procedure can be found in the appendix (suppl. E).
5.2.2 Results. For each run of our experiment, we compute the
percent decrease in test MSE between feature-sparse FAST and
each competing algorithm, given by:
% decrease MSE =MSE Competing Alg. âˆ’MSE FAST
MSE
FAST,
for each sparsity budget. A positive percent decrease in test error
indicates that feature-sparse FAST performs better than the com-
peting algorithm for that sparsity budget.
In Table 2 we report the average percent decrease in test error
betweengroup â„“0-FASTandthecompetingalgorithms(top values)
andAGISandthecompetingalgorithms( bottom values)acrossall
sparsity budgets. These averages are taken across all datasets and
folds in our experiment. In Figure 5, we show the full distributions
of our results. In each plot, the horizontal axis shows the sparsity
budget and the vertical axis shows the percent decrease in test er-
rorbetweenfeature-sparseFASTandthecompetingalgorithm;theleft plot shows group â„“0-FAST and the right plot shows AGIS. The
grouped violin plots show the distribution of the results for each
sparsity budget and the averages of each distribution are marked
by horizontal lines, which correspond to the averages in Table 2.
From Figure 5, we see that group â„“0-FAST and AGIS largely out-
perform all of our competing algorithms. The distributions of the
percent decrease in test error between these two methods and our
competing algorithms are nearly entirely positive across all spar-
sity budgets. We also observe that group â„“0-FAST consistently per-
formsslightlybetter thanAGIS.In Table2,theaveragepercentde-
crease in test error for group â„“0- FAST (top value) is always higher
than that for AGIS (bottom value). However, as discussed in Â§4.3.2,
AGISalsohasseveraladvantagesintermsofspeedandeaseofuse.
Forsparsitybudget ğ¾=2,groupâ„“0-FASTattainsa 28%decrease
in test error compared to the best competing algorithm, Control-
Burn.Interestingly,theperformanceofControlBurndegradesas ğ¾
increases. This is because the framework selects features indepen-
dently of the final EBM that is refitted [15]. While ControlBurn is
usefulforselectingafewimportantfeatures,ourresultsshowthat
the framework fails at building sparse PCAMs for larger values of
ğ¾.Forğ¾âˆˆ {4,6,8,10},groupâ„“0-FASTattains upto a 30%decrease
in test error compared to the best competing algorithm, EBM-RS.
Inall,weobservethatgroup â„“0-FASTconsistentlyoutperformsthe
best competing algorithm across all sparsities.
Inaddition,feature-sparseFASTsubstantiallyoutperformsFLAM-
GL and FastSparseGAM, by over a 100%decrease in test error for
some sparsities. FLAM-GL is affected by over-shrinkage from the
groupLASSO,whichisespeciallypronouncedsincetheFLAMfor-
mulation uses a large number of variables. The â„“0-based penalties
and constraints in feature-sparse FAST are shrinkage-free and ro-
bust to this effect. FastSparseGAM indirectly selects features by
sparsifyingpiecewisesegmentsintheadditivemodel[17].Feature-
sparse FAST, on the other hand, directly accounts for feature spar-
sity in the optimization framework and outperforms this compet-
ing method. We also note that feature-sparse FAST substantially
outperformsourcompetingalgorithmsthatdonotfitPCAMs:SAM,
which also uses the group LASSO, and the linear LASSO.
Finally, we observe that many distributions in Figure 5 have
heavy positive tails, notably for the EBM-RS, FLAM-GL, and Con-
trolBurncompetingalgorithms.Theseheavytailstypicallycontain
theresultsfromdatasetswithcorrelatedfeatures;weshowcorrela-
tion matrices and the distribution of errors in the appendix (suppl.
F). In Â§6.1, we show through a semi-synthetic experiment that cor-
related features degrade the performance of EBM-RS, FLAM-GL,
andControlBurn.Group â„“0-FASTandAGIS,ontheotherhand,can
effectively build sparse PCAMs regardless of feature correlations.
6 PCAMS AND FEATURE CORRELATIONS
We conclude by investigating how correlated features impact the
interpretability of PCAMs.
6.1 Correlated Feature Selection
We present here a semi-synthetic example to investigate how cor-
related features affect our feature selection experiment (Â§5.2). We
startwiththeHousesdataset[29]andbuildfeature-sparsePCAMs
 
1869KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
2 4 6 8 100%100%200%300%400%500%600%Group`0-FAST % Decrease in Test MSE
2 4 6 8 10AGIS % Decrease in Test MSE
FLAM-GL
EBM-RS
ControlBurn
FastSparseGAM
SAM
LASSO
Sparsity
Figur
e 5: Distribution of results from our feature selection experiment (averages shown in Table 2). The distributions are
mostly entirely positive, which indicates that feature-sparse FAST outperforms our competing algorithms.
usingFAST,EBM-RS,FLAM-GL,andControlBurnbyvaryingspar-
sity budget ğ¾. The top left plot in Figure 6 shows the test perfor-
mance of these sparse models.
0:40:60:8Original Data
AGIS
Group`0
EBM-RS
FLAM-GL
ControlBurn
3 Correlated Features Added
2 4 6 80:40:60:86 Correlated Features Added
2 4 6 89 Correlated Features Added
SparsityTest MSE
Figur
e 6: Group â„“0-FAST and AGIS perform well even after
adding correlated features.
We then add 3, 6, and 9 correlated features to the data. As the
other plots in Figure 6 show, the performances of EBM-RS, Con-
trolBurn, and FLAM-GL degrade significantly with added corre-
lations but the performances of group â„“0-FAST and AGIS remain
unaffected.
EBMfeatureimportancescorescapturethecontributionofeach
feature to the prediction of the model, averaged over all training
observations. Given a pair of highly correlated features, the cyclic
round-robin algorithm used to fit EBMs will split the contribution
of the features evenly between the pair. As such, the average fea-
ture importance score/ranking of a group of correlated featureswill be suppressed, which degrades the performance of EBM-RS.
This effect is analogous to the so-called correlation bias observed
in random forest feature rankings by the ControlBurn paper [15].
ControlBurn attempts to address correlation bias by using the
weighted LASSO to select features and we indeed observe in Fig-
ure 6 that the algorithm is more robust than EBM-RS to added
correlations. However, the LASSO penalty used in ControlBurn
still imparts shrinkage which biases sparse selection in the pres-
enceofmulticollinearity[8,10,19].Additionally,thegroupLASSO
penalty in FLAM-GL is known to suffer from over-shrinkage and
performsevenworseatselectingsparsesubsetsofcorrelatedgroups
[10]. The penalties and constraints used to select features in FAST
are shrinkage-free and, as a result, our algorithms are unaffected
by the added correlated features.
6.2 Identifying Discontinuities
One attractive property of PCAMs is their ability to capture dis-
continuouspatternsintheunderlyingdata.Here,wepresentacase
studytodemonstratehowcorrelatedfeaturescandegradetheabil-
ity of EBMs to identify discontinuities.
WeusetheHouses[29]datasettobuildPCAMstopredicthouse
pricesusingdemographicfeatures.Unsurprisingly,thereisanearly
linearrelationshipbetweenthemedianincomeofadistrictandthe
price of homes in that district. We add the following artificial dis-
continuitytothedata:foralldistrictswithamedianincomeabove
$40,000 a year, we drop the price of homes by $20,000.
We then fit feature-dense PCAMs using FAST and EBMs. Both
methods capture the discontinuity in the shape function for me-
dian income, as shown in the left two plots in Figure 7. Next, we
add 10 synthetic features that are correlated with median income
and refit.
The right two plots in Figure 7 show the new shape functions.
Theredlinesshowmedianincomeandthegreylinesshowthesyn-
theticcorrelatedfeatures.WeobservethatforFAST(topright),the
shape function of median income is preserved and that the discon-
tinuous pattern can still be easily identified. The shape functions
 
1870FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
-1-0.50FAST original
Median Income
Correlated FeaturesFAST w/ correlations
0 4 8-1-0.50EBM original
0 4 8EBM w/ correlations
Feature ValueContribution
Figur
e 7: Correlated features can mask discontinuities in
PCAM shape functions.
of the synthetic correlated features are reduced in magnitude and
severalareimplicitlyregularizedtozero.TheEBMshapefunctions
(bottom right) tell a different story. Due to the cyclic algorithm
used to fit EBMs, the contribution of median income is evenly dis-
tributed among the noisy correlated features. All the shape func-
tionsinthisgrouparecompressedandthediscontinuityisdifficult
to detect.
Ifadatasetcontainsagroupofcorrelatedfeatures,andonlyone
feature in that group contains an interesting discontinuity, FAST
maybemorelikelytocapturethispatterncomparedtoEBMs.Also,
consider the case where a dataset contains a sensitive attribute
with a discontinuous pattern. A potential adversarial attack would
be to add features correlated with this sensitive attribute to mask
thisdiscontinuouspatternfromEBMs.Duetogreedymodelfitting,
FAST again may be more robust to this attack.
We emphasize that both FAST and EBMs produce transparent
PCAMs,however,theinterpretationsofthemodelschangedepend-
ing on whether the PCAMs were fit greedily or cyclically. Model
transparency does not guarantee trustworthiness and practition-
ers should still interpret transparent models cautiously.
7 CONCLUSION
FAST is an optimization-based framework that leverages a novel
greedyoptimizationproceduretofitPCAMsupto2ordersofmag-
nitude faster than SOTA methods. The framework also introduces
two feature selection algorithms that significantly outperform ex-
isting methods at building sparse PCAMs. Using FAST, we investi-
gatehowcorrelatedfeaturesimpacttheinterpretabilityofPCAMs
in terms of selecting important features and interpreting shape
functions. These phenomena should be considered when evaluat-
ing the trustworthiness of additive models.
ACKNOWLEDGMENTS This research is funded in part by a
grant from the Office of Naval Research (ONR-N00014-21-1-2841).
REFERENCES
[1] Peter Bacchetti. Additive isotonic models. Journal of the American Statistical
Association, 84(405):289â€“294, 1989.[2] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie
Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hos-
pital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international
conference on knowledge discovery and data mining, pages 1721â€“1730, 2015.
[3] Chun-Hao Chang, Sarah Tan, Ben Lengerich, Anna Goldenberg, and Rich Caru-
ana. How interpretable and trustworthy are gams? In Proceedings of the 27th
ACM SIGKDD Conference on Knowledge Discovery & Data Mining , pages 95â€“105,
2021.
[4] Eric Chu, Arezou Keshavarz, and Stephen Boyd. A distributed algorithm for
fitting generalized additive models. Optimization and Engineering , 14(2):213â€“
224, 2013.
[5] InderjitDhillon,PradeepRavikumar,andAmbujTewari.Nearestneighborbased
greedy coordinate descent. Advances in Neural Information Processing Systems ,
24, 2011.
[6] Jerome Friedman, Trevor Hastie, Holger HÃ¶fling, and Robert Tibshirani. Path-
wise coordinate optimization. 2007.
[7] Trevor Hastie and Robert Tibshirani. Generalized additive models for medical
research. Statistical methods in medical research , 4(3):187â€“196, 1995.
[8] Trevor Hastie, Robert Tibshirani, and R Tibshirani. Best subset, forward step-
wise or lasso. Analysis and recommendations based on extensive comparisons:
Statistical Science, 2020.
[9] Hussein Hazimeh and Rahul Mazumder. Fast best subset selection: Coordinate
descent and local combinatorial optimization algorithms. Operations Research ,
68(5):1517â€“1537, 2020.
[10] Jian Huang, Patrick Breheny, and Shuangge Ma. A selective review of group
selection in high-dimensional models. Statistical science: a review journal of the
Institute of Mathematical Statistics, 27(4), 2012.
[11] Nicholas A Johnson. A dynamic programming algorithm for the fused lasso
and l 0-segmentation. Journal of Computational and Graphical Statistics , 22(2):
246â€“260, 2013.
[12] Sai Praneeth Karimireddy, Anastasia Koloskova, Sebastian U Stich, and Martin
Jaggi. Efficient greedy coordinate descent for composite problems. In The 22nd
International Conference on Artificial Intelligence and Statistics, pages 2887â€“2896.
PMLR, 2019.
[13] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qi-
weiYe,andTie-YanLiu. Lightgbm:Ahighlyefficientgradientboostingdecision
tree. Advances in neural information processing systems, 30, 2017.
[14] Benjamin J Lengerich, Rich Caruana, Mark E Nunnally, and Manolis Kellis.
Death by round numbers: Glass-box machine learning uncovers biases in medi-
cal practice. medRxiv, pages 2022â€“04, 2022.
[15] Brian Liu, Miaolan Xie, and Madeleine Udell. Controlburn: Feature selection by
sparse forests. In Proceedings of the 27th ACM SIGKDD conference on knowledge
discovery & data mining, pages 1045â€“1054, 2021.
[16] Jiachang Liu. fastsparsegams, 2023. URL https://pypi.org/project/
fastsparsegams/.
[17] Jiachang Liu, Chudi Zhong, Margo Seltzer, and Cynthia Rudin. Fast sparse clas-
sification for generalized linear and additive models. Proceedings of machine
learning research , 151:9304, 2022.
[18] Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classifi-
cation and regression. In Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 150â€“158, 2012.
[19] RahulMazumder. Discussionofâ€œbestsubset,forwardstepwiseorlasso?analysis
and recommendations based on extensive comparisonsâ€. Statistical Science , 35
(4), 2020.
[20] Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. Interpretml:
A unified framework for machine learning interpretability. arXiv preprint
arXiv:1909.09223, 2019.
[21] Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, and Hoyt
Koepke. Coordinatedescentconvergesfasterwiththegauss-southwellrulethan
random selection. In International Conference on Machine Learning , pages 1632â€“
1641. PMLR, 2015.
[22] Julie Nutini, Issam Laradji, and Mark Schmidt. Letâ€™s make block coordinate de-
scent converge faster: faster greedy rules, message-passing, active-set complex-
ity, and superlinear convergence. Journal of Machine Learning Research, 23(131):
1â€“74, 2022.
[23] NYPD. Motor vehicle collisions - crashes: Nyc open data, Sep 2023.
[24] Ashley Petersen. flam: Fits Piecewise Constant Models with Data-Adaptive Knots ,
2018. URL https://CRAN.R-project.org/package=flam. R package version 3.2.
[25] Ashley Petersen, Daniela Witten, and Noah Simon. Fused lasso additive model.
Journal of Computational and Graphical Statistics, 25(4):1005â€“1025, 2016.
[26] Junyang Qian and Jinzhu Jia. On stepwise pattern recovery of the fused lasso.
Computational Statistics & Data Analysis , 94:221â€“237, 2016.
[27] Charles J Stone and Cha-Yong Koo. Additive splines in statistics. Proceedings of
the American Statistical Association Original pagination is p, 45:48, 1985.
[28] Mike Van Ness, Tomas Bosschieter, Natasha Din, Andrew Ambrosy, Alexander
Sandhu, and Madeleine Udell. Interpretable survival analysis for heart failure
risk prediction. In Machine Learning for Health (ML4H), pages 574â€“593. PMLR,
2023.
 
1871KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
[29] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml:
networked science in machine learning. ACM SIGKDD Explorations Newsletter,
15(2):49â€“60, 2014.
[30] Simon N Wood, Zheyuan Li, Gavin Shaddick, and Nicole H Augustin. General-
ized additive models for gigadata: modeling the uk black smoke network daily
data. Journal of the American Statistical Association, 112(519):1199â€“1210, 2017.
[31] Tong Tong Wu and Kenneth Lange. Coordinate descent algorithms for lasso
penalized regression. 2008.
APPENDIX
We show proofs and derivations below, a full online supplement
containing experiment details and visualizations can be found at:
https://github.com/brianliu12437/FASTopt.
PROOF OF PROPOSITION 1 (B.1)
We start with the following preliminaries. We have,
min
ğœƒğ¹(ğœƒ)=ğ‘“(ğœƒ) +ğœ†âˆ¥|ğœƒâˆ¥1,(1)
whereğœƒis both coordinate separable and block separable, with
specified blocks and ğ‘“(ğœƒ)is coordinate-wise L-smooth. Recall we
define direction vector ğ‘‘(same dimensions of ğœƒ) elementwise by
ğ‘‘ğ‘—={
|ğ‘†ğœ†(âˆ‡ğ‘—ğ‘“(ğœƒ)| ifğœƒğ‘—=0
|âˆ‡ğ‘—ğ‘“(ğœƒ) +sign(ğœƒğ‘—)ğœ†|ifğœƒğ‘—â‰ 0,(2)
and we select the block to update whose corresponding elements
ofğ‘‘have the largest â„“2-norm.
We also have this formula for the generalized directional deriv-
ative forğ¹(ğœƒ)with respect to arbitrary direction vector ğ‘£[31].
ğœ•ğ‘£ğ¹(ğœƒ)=âˆ‘
ğ‘—âˆ‡ğ‘—ğ‘“(ğœƒ)ğ‘£ğ‘—+âˆ‘
ğ‘—{
ğœ†sign(ğœƒğ‘—)ğ‘£ğ‘—ifğœƒğ‘—â‰ 0
ğœ†|ğ‘£ğ‘—| ifğœƒğ‘—=0(3)
Part1a: Wefirstwanttoshowthatanylimitpointofasequence
of solutions ğœƒğ‘¡generated via BGS-GBCD coincides with a mini-
mum point of ğ¹(ğœƒ). We have that ğœƒğ‘¡reaches a limit point when
ğ‘‘=0.
First we show that limit points of ğœƒğ‘¡correspond to minimums
ofğ¹(ğœƒ). Letğœƒâˆ—be a limit point of sequence ğœƒğ‘¡. Consider the ğ‘—ğ‘¡â„
element ofğœƒâˆ—.
Case 1:ğœƒâˆ—
ğ‘—=0Sinceğœƒâˆ—is stationary we have that ğ‘‘ğ‘—=0, and
that|ğ‘†ğœ†ğ‘“(âˆ‡ğ‘—ğ‘“(ğœƒ)|=0.
This implies |âˆ‡ğ‘—ğ‘“(ğœƒ)| â‰¤ğœ†. Consider the ğ‘—ğ‘¡â„component contri-
bution to the generalized directional derivative at ğœƒâˆ—for arbitrary
directionğ‘£. We have that
(ğœ•ğ‘£ğ¹(ğœƒâˆ—))ğ‘—=ğ‘“(ğœƒ)ğ‘£ğ‘—+ğœ†|ğ‘£ğ‘—| â‰¥0 (4)
Case 2:ğœƒâˆ—
ğ‘—â‰ 0We have that âˆ‡ğ‘—ğ‘“(ğœƒ)=âˆ’sign (ğœƒğ‘—)ğœ†, and that
(ğœ•ğ‘£ğ¹(ğœƒâˆ—))ğ‘—=âˆ‡ğ‘—ğ‘“(ğœƒ)ğ‘£ğ‘—+ğœ†sign(ğœƒğ‘—)ğ‘£ğ‘—=0 (5)
We compute ğœ•ğ‘£ğ¹(ğœƒâˆ—)by summing up the ğ‘—ğ‘¡â„components for
both cases. Since ğ‘£is arbitrary we have that
ğœ•ğ‘£ğ¹(ğœƒâˆ—) â‰¥0âˆ€ğ‘£,
and therefore ğœƒâˆ—is the minimum for ğ¹(ğœƒ).
Part1b: Wenextshowthatminimumpointsof ğ¹(ğœƒ)correspond
to stationary points. This is easy to verify from the LASSO subgra-
dient optimality conditions. Let ğœƒâˆ—now correspond to the mini-
mum ofğ¹(ğœƒ). Consider the ğ‘—ğ‘¡â„element ofğœƒâˆ—.Case 1:ğœƒâˆ—
ğ‘—=0
WehavefromthesubgradientoptimalityconditionsoftheLASSO
that|âˆ‡ğ‘—ğ‘“(ğœƒâˆ—)| â‰¤ğœ†. Thereforeğ‘‘ğ‘—=|ğ‘†ğœ†(âˆ‡ğ‘—ğ‘“(ğœƒâˆ—)|=0.
Case 2:ğœƒâˆ—
ğ‘—â‰ =0
WehavefromthesubgradientoptimalityconditionsoftheLASSO
thatâˆ‡ğ‘—ğ‘“(ğœƒ) +ğœ†sign(ğœƒğ‘—)=0. Therefore ğ‘‘ğ‘—=0.
Therefore combining parts 1a and 1b, we see that stationary
points of our BGS-GBCD algorithm and minimum points of ğ¹(ğœƒ)
coincide.
Part 2:Next, we want to show that any sequence of solutions
generatedbyBGS-GBCDconvergestoastationary/minimumpoint.
We first discuss some preliminaries. We rely on this key prop-
erty, sinceğ¹(ğœƒ)is both block andcoordinate separable, a block
update is equivalent to updating all of the coordinates in a block.
For this convergence analysis, we adopt the notation of good
and bad coordinate updates introduced by [12]. In a good coordi-
nate update, the updated coordinate does not cross the origin. For-
mally, given coordinate ğ‘ğ‘—>0and updated coordinate ğ‘+
ğ‘—we have
thatğ‘ğ‘—ğ‘+
ğ‘—>0; all updates on a zero coordinate ğ‘ğ‘—=0are good up-
dates. Bad coordinate updates, on the other hand, cross the origin,
i.e. givenğ‘ğ‘—>0we have that ğ‘ğ‘—ğ‘+
ğ‘—â‰¤0.
After updating a block update, we can partition the updated co-
ordinatesintheblockintogoodandbadupdate.Wealsoadoptthe
post-processing step used in [12]. For bad coordinate updates, we
setğ‘+
ğ‘—=0. This ensures that each coordinate will never have two
consecutive bad updates, since updates on a zero coordinates are
always good.
[12] shows these properties for coordinate updates on ğ¹(ğœƒ).
The progress made by a good update on coordinate ğ‘—is bound
by:
ğ¹(ğœƒ+
ğ‘—) âˆ’ğ¹(ğœƒğ‘—) â‰¤ âˆ’1
2L(ğ‘‘ğ‘—)2, (6)
and
the progress made by a bad update on coordinate ğ‘—after post-
processing is bound by
ğ¹(ğœƒ+
ğ‘—) âˆ’ğ¹(ğœƒğ‘—) â‰¤0. (7)
Again since ğ¹(ğœƒ)is both block and coordinate separable, the
progress made by a block update is equivalent to the sum of the
progressmadebyeachcoordinateintheblock,andthesameholds
for the bounds.
Wenowstartourproof.Considersomearbitrarysequenceofso-
lutionsğœƒğ‘¡generatedbyBGS-GBCD,andassumethatthissequence
does not converge to to a stationary/minimum point.
Sinceğœƒğ‘¡does not converge to a stationary point, we have that
direction vector ğ‘‘is nonzero for each update. Therefore, in each
block update, we have at least one index ğ‘—in the block s.t. ğ‘‘ğ‘—â‰ 
0.The decrease in objective value for a block update is at least as
large as the decrease in objective value for a coordinate update of
a coordinate in the block.
If the update on coordinate ğ‘—whereğ‘‘ğ‘—â‰ 0is a good coordinate
update, we have this bound on the decrease in objective value for
the corresponding blockupdate,
ğ¹(ğœƒğ‘¡+1) âˆ’ğ¹(ğœƒğ‘¡) â‰¤ âˆ’1
2L(ğ‘‘ğ‘—)2wher
eğ‘‘ğ‘—>0. (8)
If the update on coordinate ğ‘—is a bad update, then we have that:
 
1872FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
ğ¹(ğœƒğ‘¡+1) âˆ’ğ¹(ğœƒğ‘¡) â‰¤0. (9)
Due to post-processing, we can not have consecutive bad up-
dates on the same coordinate. Since we have a finite number of
coordinates, there can not exist a infinite contiguous subsequence
of BGS-GBCD block updates where the only contribution to the
decrease in objective values come from bad coordinate updates in
the block (9).
Therefore, consider our sequence of solutions ğœƒğ‘¡generated by
BGS-GBCD updates. We have that all contiguous subsequences of
BGS-GBCD updates must contain updates where the decrease in
objective value is bound by (8). As such, one of two things can oc-
cur. Either the objective value continues to decrease by âˆ’1
2L(ğ‘‘ğ‘—)2
wher
eğ‘‘ğ‘—>0, and since ğ¹(ğœƒ)is continuous and bounded from be-
lowby 0,thesequenceconvergestotheminimum(whichcoincides
to stationary point for ğœƒğ‘¡). Or, we can have that ğ¹(ğœƒğ‘¡+1) âˆ’ğ¹(ğœƒğ‘¡)
converge to zero for a contiguous subsequence, if ğ‘‘ğ‘—converges to
zero for all coordinates, but this would mean that ğœƒğ‘¡converges
to a stationary point. Either way, we reach a contradiction on the
assumption that ğœƒğ‘¡does not converge to a stationary/minimum
point, which completes our proof.
PROOF OF PROPOSITION 2 (B.2)
Wederiveheretheboundontheprogressmade(decreaseinobjec-
tive value) for a BGS-GBCD update on ğœƒğ‘¡
ğ‘˜=0, where each entry
in the block is zero.
We discuss the following preliminaries. Consider vector ğœƒ; the
entries ofğœƒare separable into ğ‘prespecified blocks and ğœƒğ‘˜cor-
responds to the sub-vector of entries of ğœƒin blockğ‘˜. We define
ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘š (ğ‘¥)as the sum of the â„“2-norms of each block in ğ‘¥, i.e.
ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘š (ğ‘¥)=ğ‘âˆ‘
ğ‘˜=1âˆ¥ğ‘¥âˆ¥2. (10)
We have that the dual of this norm is equal to:
ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘š (ğ‘§)âˆ—=ğ‘šğ‘ğ‘¥(âˆ¥ğ‘§1âˆ¥,âˆ¥ğ‘§2âˆ¥,..., âˆ¥ğ‘§ğ‘âˆ¥). (11)
We want to show:
ğ¹(ğœƒğ‘¡+1) âˆ’ğ¹(ğœƒğ‘¡) â‰¤ min
ğ›¾âˆˆRğ‘›âˆ‡ğ‘“(ğœƒğ‘¡)âŠºğ›¾
+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾) +ğœ†âˆ¥ğœƒğ‘¡+ğ›¾âˆ¥1âˆ’ğœ†âˆ¥ğœƒğ‘¡âˆ¥1,(12)
We have that:
(ğœƒğ‘¡+1âˆ’ğœƒğ‘¡)âŠºâˆ‡ğ‘“(ğœƒğ‘¡) +L
2âˆ¥ğœƒğ‘¡+1âˆ’ğœƒğ‘¡âˆ¥2
2+ğœ†âˆ¥ğœƒğ‘¡+1âˆ¥1âˆ’ğœ†âˆ¥ğœƒğ‘¡âˆ¥1
Letğµğ‘˜b
e the indicies in ğœƒthat correspond to block ğœƒğ‘˜. If block
ğœƒğ‘¡
ğ‘˜=0was selected via the BGS rule and updated to obtain ğœƒğ‘¡+1,
we have that:
ğ¹(ğœƒğ‘¡+1) âˆ’ğ¹(ğœƒğ‘¡)
â‰¤âˆ‘
ğ‘—âˆˆğµğ‘˜[
(ğœƒğ‘¡+1
ğ‘—âˆ’ğœƒğ‘¡
ğ‘—)âˆ‡ğ‘“(ğœƒğ‘¡
ğ‘—) +L
2(ğœƒğ‘¡+1
ğ‘—âˆ’ğœƒğ‘¡
ğ‘—)2+ğœ†(
|ğœƒğ‘¡+1
ğ‘—| âˆ’ |ğœƒğ‘¡
ğ‘—|)]
And sinceğœƒğ‘¡
ğ‘˜=0, we follow the steps from [12] (Lemma 9) to
obtain:
ğ¹(ğœƒğ‘¡+1) âˆ’ğ¹(ğœƒğ‘¡) â‰¤ âˆ’1
2Lâˆ‘
ğ‘—âˆˆğµğ‘˜ğ‘‘2
ğ‘—=âˆ’1
2Lâˆ¥ğ‘‘ğ‘˜âˆ¥2
2(13)No
w, consider this expression:
âˆ‡ğ‘“(ğœƒğ‘¡)âŠºğ›¾+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾) +ğœ†âˆ¥ğœƒğ‘¡+ğ›¾âˆ¥1âˆ’ğœ†âˆ¥ğœƒğ‘¡âˆ¥1,
whereğ›¾âˆˆRğ‘›. We follow the algebra in [12] (Lemma 8)2to obtain:
âˆ‡ğ‘“(ğœƒğ‘¡)âŠºğ›¾+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾)+ğœ†âˆ¥ğœƒğ‘¡+ğ›¾âˆ¥1âˆ’ğœ†âˆ¥ğœƒğ‘¡âˆ¥1â‰¥ğ‘‘âŠºğ›¾+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾)
We minimize both sides of the inequality with respect to ğ›¾to
obtain.
min ğ›¾[
âˆ‡ğ‘“(ğœƒğ‘¡)âŠºğ›¾+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾) +ğœ†âˆ¥ğœƒğ‘¡+ğ›¾âˆ¥1âˆ’ğœ†âˆ¥ğœƒğ‘¡âˆ¥1]
â‰¥min ğ›¾[
ğ‘‘âŠºğ›¾+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾)]
Using convex conjugates we have that
min ğ›¾[
ğ‘‘âŠºğ›¾+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾)]
=âˆ’1
2L(ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ‘‘)âˆ—)2
=âˆ’1
2L(ğ‘š
ğ‘ğ‘¥(âˆ¥ğ‘‘1âˆ¥2...âˆ¥ğ‘‘ğ‘âˆ¥2))2
Sinceğœƒğ‘¡
ğ‘˜is selected using BGS we have that:
ğ‘šğ‘ğ‘¥(âˆ¥ğ‘‘1âˆ¥2...âˆ¥ğ‘‘ğ‘âˆ¥2)=âˆ¥ğ‘‘ğ‘˜âˆ¥2
and that:
min ğ›¾[
âˆ‡ğ‘“(ğœƒğ‘¡)âŠºğ›¾+L
2ğ‘ 
ğ‘›ğ‘œğ‘Ÿğ‘š (ğ›¾)+ğœ†âˆ¥ğœƒğ‘¡+ğ›¾âˆ¥1âˆ’ğœ†âˆ¥ğœƒğ‘¡âˆ¥1]
â‰¥ âˆ’1
2Lâˆ¥ğ‘‘ğ‘˜âˆ¥2
2.
(14)
Combining
(13) and (14) yields (12) completing our proof.
BINNING FORMULATION (C)
Given a prespecified set of bins for each feature ğ‘¥ğ‘—, we can formal-
izebinningintotheFASTframeworkbyaddingtheconstraintthat
all value ofğ›½ğ‘—within each bin are fit the same value.
ConstRaint 1. Assume each feature ğ‘¥ğ‘—, ğ‘—âˆˆ [ğ‘]hasğ‘šprespec-
ified sorted bins, ğ‘1...ğ‘ ğ‘š. For eachğ‘âˆˆğ‘1...ğ‘ ğ‘šand all pairs of
indicies {ğ‘–1, ğ‘–2} âˆˆğ‘, we add the constraint that (ğ›½ğ‘—)ğ‘–1=(ğ›½ğ‘—)ğ‘–2.
UnconstrainedReformulation: Wereformulatetheseconstraints
into the following unconstrained problem by defining a new set of
decision vectors ğ›½â€²
ğ‘—âˆˆRğ‘šforğ‘—âˆˆ [ğ‘]to represent the value fit for
each bin. We define mapping matrices ğ‘ƒğ‘—âˆˆ {0,1}ğ‘›Ã—ğ‘šsuch that
(ğ‘ƒğ‘—)(ğ‘–,ğ‘)=1if(ğ‘¥ğ‘—)ğ‘–is in binğ‘. The unconstrained problem can be
expressed by:
min
ğ›½â€²1
2âˆ¥ğ‘¦âˆ’ğ‘âˆ‘
ğ‘—=1ğ‘„âŠº
ğ‘—ğ‘ƒğ‘—ğ›½â€²
ğ‘—âˆ¥2
2+ğœ†ğ‘“ğ‘âˆ‘
ğ‘—=1âˆ¥ğ·â€²ğ›½â€²
ğ‘—âˆ¥1,(15)
wher
eğ·â€²is the (ğ‘šâˆ’1) Ã—ğ‘šdifferencing matrix. This problem
is equivalent to Problem (1) in the main text with Constraint (1)
added. Problem 15 is block separable and we can apply our BGS-
GBCD algorithm to solve the problem to optimality. We first dis-
cuss block updates below.
2The
only difference between the above expression and the one presented in Lemma
8 of [12] is the choice of the norm on ğ›¾. The algebraic steps do not involve the norm
term.
 
1873KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
Block Update: Given a fixed block ğ‘˜and residual vector ğ‘Ÿ=
ğ‘¦âˆ’âˆ‘
ğ‘—âˆˆğ›¿ğ‘„âŠº
ğ‘—ğ‘ƒğ‘—ğ›½â€²
ğ‘—, we derive that each block update problem can
be expressed by:
min
ğ›½â€²
ğ‘˜1
2âˆ¥ğ‘Šğ‘—Â¯ğ‘Ÿâˆ’ğ‘Šğ‘—ğ›½â€²
ğ‘˜âˆ¥2
2+ğœ†ğ‘“âˆ¥ğ·â€²ğ›½â€²
ğ‘—âˆ¥1,(16)
wher
eÂ¯ğ‘ŸâˆˆRğ‘šisavectorofthebinmeansof ğ‘Ÿ,sortedwithrespect
toğ‘¥ğ‘—and binned w.r.t to the prespecified bins for ğ‘¥ğ‘—andğ‘Šğ‘—âˆˆ
Rğ‘šÃ—ğ‘šisadiagonalmatrixwiththesquarerootofthecardinalities
of each bin along the main diagonal,âˆš
|ğ‘1|.
..âˆš
|ğ‘ğ‘š|.
Each
block update minimizes:
1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ’ğ‘ƒğ‘˜ğ›½â€²
ğ‘˜âˆ¥2
2+ğœ†ğ‘“âˆ¥ğ·â€²ğ›½â€²
ğ‘˜âˆ¥1,
with
respect toğ›½â€²
ğ‘˜.
Considerthesmoothlossfunction;wecanrewritethelossfunc-
tions by summing over the loss for each bin in ğ‘¥ğ‘—.
1
2ğ‘šâˆ‘
â„=1âˆ‘
ğ‘–âˆˆğ‘â„(ğ‘Ÿğ‘–âˆ’
(ğ›½â€²
ğ‘˜)â„)2
Expanding the polynomial, we get that
1
2ğ‘šâˆ‘
â„=1âˆ‘
ğ‘–âˆˆğ‘â„(ğ‘Ÿ2
ğ‘–âˆ’2(ğ›½â€²
ğ‘˜)â„ğ‘Ÿğ‘–+
(ğ›½â€²
ğ‘˜)2
â„)
=1
2ğ‘šâˆ‘
â„=1âˆ‘
ğ‘–âˆˆğ‘â„(ğ‘Ÿ2
ğ‘–âˆ’2(ğ›½â€²
ğ‘˜)â„ğ‘Ÿğ‘–+
(ğ›½â€²
ğ‘˜)2
â„)
=1
2ğ‘šâˆ‘
â„=1(âˆ‘
ğ‘–âˆˆğ‘â„ğ‘Ÿ2
ğ‘–âˆ’2(ğ›½â€²
ğ‘˜)â„âˆ‘
ğ‘–âˆˆğ‘â„ğ‘Ÿğ‘–+
|ğ‘â„|(ğ›½â€²
ğ‘˜)2
â„)
We complete the square to get:
=1
2ğ‘šâˆ‘
â„=1(
|ğ‘â„|(
(ğ›½â€²
ğ‘˜)â„âˆ’1
|ğ‘â„|âˆ‘
ğ‘–âˆˆğ‘â„ğ‘Ÿğ‘–)2
+
K)
where Kdoes not depend of (ğ›½â€²
ğ‘˜)â„.
We have1
|ğ‘â„|âˆ‘
ğ‘–âˆˆğ‘â„ğ‘Ÿğ‘–=Â¯ğ‘Ÿâ„,
i.e
., the mean of ğ‘Ÿfor binâ„.
Combining this with the expression above, we have:
=1
2ğ‘šâˆ‘
â„=1(âˆš
|ğ‘â„|
(ğ›½â€²
ğ‘˜)â„âˆ’âˆš
|ğ‘â„|Â¯ğ‘Ÿâ„)2
+
K.
This is equivalent to Problem 16, since Kdoes not depend on
ğ›½â€²
ğ‘˜. We can restrict our binning procedure so that all bins have the
same cardinality:
|ğ‘1|=|ğ‘2|=...|ğ‘ğ‘š|.
In this case, the block update is equivalent to:
min
ğ›½â€²
ğ‘˜1
2âˆ¥Â¯ğ‘Ÿâˆ’ğ›½â€²
ğ‘˜âˆ¥2
2+ğœ†ğ‘“
|ğ‘1|âˆ¥ğ·â€²ğ›½â€²
ğ‘—âˆ¥1, (17)
This
1D-FLSA can be solved using dynamic programming to com-
plete a block update.Block Selection: Next, we derive our BGS block selection rule.
Assumethateachfeatureisbinnedintoequally-sizedbins ğ‘1...ğ‘ ğ‘š.
We start with our unconstrained formulation in Problem 15.
min
ğ›½â€²1
2âˆ¥ğ‘¦âˆ’ğ‘âˆ‘
ğ‘—=1ğ‘„âŠº
ğ‘—ğ‘ƒğ‘—ğ›½â€²
ğ‘—âˆ¥2
2+ğœ†ğ‘“ğ‘âˆ‘
ğ‘—=1âˆ¥ğ·â€²ğ›½â€²
ğ‘—âˆ¥1,
A
gain, letğœƒâ€²
ğ‘—âˆˆRğ‘šâˆ’1be contain the successive differences of
ğ›½â€²
ğ‘—. We again use our implicit reformulation matrices to rexpress
Problem 15 into this form:
min
ğœƒâ€²1
2âˆ¥ğ‘¦âˆ’ğ‘€
ğ‘„âŠºğ‘ƒğ´ğœƒâ€²âˆ¥2
2+ğœ†ğ‘“âˆ¥ğœƒâ€²âˆ¥1,
where:
ğ‘ƒ=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘ƒ1 0 0 0 0
0ğ‘ƒ2 0 0 0
...............
0 0 0 ğ‘ƒğ‘âˆ’1 0
0 0 0 0 ğ‘ƒğ‘ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£».
We can apply our BGS rule on this reformulated problem. We
have that the gradient of the smooth loss function here is equiva-
lent too:
âˆ’ğ´âŠºğ‘ƒâŠºğ‘„ğ‘€âŠºğ‘Ÿâ€²,
where râ€™ is the residual. This can be computed quickly for each
block j by sorting ğ‘Ÿâ€²with respect to ğ‘¥ğ‘—computing bin sums and
taking a rolling sum down the bin sums.
GROUPâ„“0-FAST THRESHOLDING (D)
Here, we derive the thresholding procedure for the block updates
in groupâ„“0-FAST. Recall the optimization objective is given by:
min
ğ›½1
2âˆ¥ğ‘¦âˆ’ğ‘âˆ‘
ğ‘—=1ğ‘„âŠº
ğ‘—ğ›½ğ‘—âˆ¥2
2+ğœ†ğ‘“ğ‘âˆ‘
ğ‘—=1âˆ¥ğ·
ğ›½ğ‘—âˆ¥1+ğœ†ğ‘ ğ‘âˆ‘
ğ‘—=1/x31(ğ›½ğ‘—â‰ 0).
(18)
Given fixed block ğ‘˜and residual vector ğ‘Ÿ=ğ‘¦âˆ’âˆ‘
ğ‘—âˆˆğ›¿ğ‘„âŠº
ğ‘—ğ›½ğ‘—, we
write each block update problem as:
min
ğ›½ğ‘˜1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ’ğ›½ğ‘˜âˆ¥2
2+ğœ†ğ‘“âˆ¥ğ·
ğ›½ğ‘˜âˆ¥1+ğœ†ğ‘  /x31(ğ›½ğ‘˜â‰ 0).(19)
Letğ›½âˆ—
ğ‘˜be the optimal solution to:
min
ğ›½ğ‘˜1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ’ğ›½ğ‘˜âˆ¥2
2+ğœ†ğ‘“âˆ¥ğ·
ğ›½ğ‘˜âˆ¥1.
Consider the following two cases.
Case 1:ğ›½ğ‘˜=0The minimum objective value of Problem (19) is
equal to
1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ¥2
2=1
2âˆ¥ğ‘Ÿâˆ¥2
2, (20)
sinceğ‘„ğ‘˜is
the sorting matrix.
Case 2:ğ›½ğ‘˜â‰ 0The minimum objective value of Problem (19) is
equal to:
1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ’ğ›½âˆ—
ğ‘˜âˆ¥2
2+ğœ†ğ‘“âˆ¥ğ·
ğ›½âˆ—
ğ‘˜âˆ¥1+ğœ†ğ‘  (21)
Combining (20) and (21) we get
1
2âˆ¥ğ‘Ÿâˆ¥2
2âˆ’1
2âˆ¥ğ‘„ğ‘˜ğ‘Ÿâˆ’ğ›½âˆ—
ğ‘˜âˆ¥2
2âˆ’ğœ†ğ‘“âˆ¥ğ·
ğ›½âˆ—
ğ‘˜âˆ¥1â‰¤ğœ†ğ‘ ,
which is the desired thresholding inequality.
 
1874