Online Preference Weight Estimation Algorithm with Vanishing
Regret for Car-Hailing in Road Network
Yucen Gao
guo_ke@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, ChinaZhehao Zhu
zhehaozhu111@gmail.com
Northwestern University
Evanston, Illinois, the USAMingqian Ma
mingqianma@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
Fei Gao
danzergaofei@didiglobal.com
Didi Global Inc.
Beijing, ChinaHui Gao
deangaohui@didiglobal.com
Didi Global Inc.
Beijing, ChinaYangguang Shi
shiyangguang@sdu.edu.cn
Shandong University
Qingdao, China
Xiaofeng Gaoâˆ—
gao-xf@cs.sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
ABSTRACT
Car-hailing services play an important role in the modern trans-
portation system, and the utilities of the service providers highly
depend on the efficiency of route planning algorithms. A widely
adopted route planning framework is to assign weights to roads
and compute the routes with the shortest path algorithms. Existing
techniques of weight-assigning often focus on the traveling time
and length of the roads, but cannot incorporate with the preferences
of the passengers (users).
In this paper, a set of preference weight estimation models is
employed to capture the usersâ€™ preferences over paths in car-hailing
with their historical choices. Since the user preferences may vary
dynamically over time, it is a challenging task to make real-time
decisions over the models. The main technical contribution of this
paper is to propose an online learning-based preference weight
chasing (PWC) algorithm to solve this problem. The worst-case
performance of PWC is analyzed with the metric regret, and it is
proved that PWC has a vanishing regret, which means that the time-
averaged loss concerning the fixed in-hindsight best model tends
to zero. Experiments based on real-world datasets are conducted to
verify the effectiveness and efficiency of our algorithm. The code is
available at https://github.com/GaoYucen/PWC.
CCS CONCEPTS
â€¢Information systems â†’Data mining; â€¢Applied computing
â†’Forecasting.
âˆ—Xiaofeng Gao is the corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671664KEYWORDS
Preference weight, stateful online learning, vanishing regret, dy-
namic deterministic markov decision process
ACM Reference Format:
Yucen Gao, Zhehao Zhu, Mingqian Ma, Fei Gao, Hui Gao, Yangguang Shi,
and Xiaofeng Gao. 2024. Online Preference Weight Estimation Algorithm
with Vanishing Regret for Car-Hailing in Road Network. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, Long Beach, CA, USA,
9 pages. https://doi.org/10.1145/3637528.3671664
1 INTRODUCTION
The advent of car-hailing services has revolutionized urban trans-
portation by providing users with convenient, on-demand mobility
solutions. These car-hailing platforms, characterized by their seam-
less integration of mobile applications, GPS tracking, and efficient
matching algorithms, have fundamentally transformed the way
people commute within cities [ 29]. With the increasing demand for
traveling, the significance of car-hailing services is becoming in-
creasingly important. Central to the success of car-hailing services
is their ability to provide users with proper routes.
Previous work usually aimed to find the shortest route that mini-
mize distance or travel time. Traditional route planning algorithms,
such as Dijkstraâ€™s algorithm and A* search, have been widely em-
ployed in these services [ 18]. However, the shortest route does not
mean the most suitable route for users [ 5]. Users may consider a
variety of factors, including the number of traffic lights in the route,
whether the route is a major road or highway, route smoothness.
As shown in Figure 1, when a user uses a car-hailing platform, such
as Didi Chuxing, the platform would recommend multiple routes,
including routes with shorter distance, faster travel time and routes
taken by more people. The most frequently chosen route is not the
one with the minimum travel time or distance.
As a result, in recent years, many efforts have begun to explore
planning more appropriate routes based on learning about historical
trajectories. Existing data-driven work can be roughly categorized
 
863
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucen Gao et al.
Figure 1: Interface of car-hailing paltform
into two classes. One is based on machine learning and the other
is based on database indexing. However, both types of work have
limitations. Machine learning based approaches require real-time
computation of the proper route based on the userâ€™s current location,
leading to the problem of route changes during the trip, inviting
complaints from both drivers and users. Database indexing based
methods are unable to give a proper route when a new request for
an unseen origin-destination pair arises. Thus, we need to develop
a method that can satisfy the following requirements.
â€¢Fast response: The method needs to respond to online re-
quests as quickly as possible.
â€¢Initial solution Optimality: The method needs to maintain
consistency of the optimal solution at the initial position and
during the trip.
â€¢Global Computability: The method needs to have the abil-
ity to find the optimal routes for globally arbitrary origin-
destination pairs.
Based on the observation of the shortest path algorithm, we
expect to use a Preference Weight Estimation (PWE) model to
construct the preference weights for road networks. On a road
network with preference weights, we can invoke the shortest path
algorithm to quickly compute the most preferred routes of users
and satisfy the above requirements. However, a challenge is that
user preferences change over time [ 17]. This means that preference
weights need to be updated dynamically. However, it is difficult
to accurately predict how user preferences change during each
update cycle. Existing incremental update technology cannot give
theoretical performance guarantees for an arbitrary sequence of
order sets. Therefore, in this paper, we design an online learning
based algorithm called Preference Weight Chasing (PWC) to provide
good theoretical performance guarantees.
Given a set of PWE models for reference, PWC consists of a
model selector ğ”–to select a target model from the set and a chas-
ing oracle ğ”’to determine the preference weights. Specifically, the
model selector maps multiple PWE models to the arms of On-
line Learning with Switching Cost (OLSC) problem and selects a
model strategy referring to the Following the Perturbed Leader
(FPL) algorithm [ 24]. The chasing oracle is an iterative tracking
sub-procedure, which determines the preference weights based on
the target model for each time slot. PWC ensures the performance
loss of the tracking process is always sublinear.We then make the theoretical analysis. Regret is often consid-
ered as the theoretical performance metric in online learning al-
gorithms [ 6]. It refers to the difference in performance between
an algorithm and the best model fixed in-hindsight. Our goal is to
achieve vanishing regret, which means that the regret of each time
slot converges to 0over time. By introducing the Online Learn-
ing with Switching Cost (OLSC) problem [ 24] and analyzing the
regret of the chasing procedure, we prove that PWC can achieve
vanishing regret. Moreover, we conduct experiments on real data
from Didi, a car-hailing service provider. The results demonstrate
PWC performs well for various situations and can achieve the best
performance in terms of the average performance.
2 RELATED WORK
2.1 Route Planning
In the navigation and transportation domain, efficient route plan-
ning is essential. Research in this field has explored various tech-
niques to optimize route planning, aiming to improve the quality
of planned routes and enhance user satisfaction [27, 35].
Route planning heavily relies on the edge weights of road net-
works, where edge weights represent the cost of traversing each
road segment [ 30,31]. To plan the most preferred route for users,
most existing approaches use deterministic weight modeling, often
combining multiple factors such as travel time, distance, and road
hierarchy using expert-designed rules [ 3,4]. However, when consid-
ering providing personalized route recommendations for different
users, their preferences in different cities and regions may differ,
making it challenging to design a general weighting scheme. Many
edge weighting algorithms aim to design different weight sets for
specific route planning problems [19, 28].
Still, designing weighting schemes for different time periods
is challenging. In real scenarios, the order sequence is revealed
sequentially as an online problem. Thus, a single weighting algo-
rithm usually cannot capture the change in order distribution, usersâ€™
preferences and other factors over time [10].
2.2 Online Learning
Online learning is a method of machine learning in which data
becomes available in sequential order and is used to update the best
treatment for future data at each step [ 21]. online learning tech-
niques can be applied to solve a variety of tasks in a wide range of
real-world application domains, such as supervised learning tasks,
bandit learning tasks and unsupervised learning tasks. Besides, on-
line learning algorithms are founded on solid theory with rigorous
regret bounds. Online learning is widely used in advertising auc-
tion [ 25], recommendation system [ 7,32], scheduling [ 2,34] and
other fields. In the paper, we apply the solid theory of online learn-
ing to the O-PWE task, and design the preference weight chasing
algorithm with a theoretical guarantee.
3 PROBLEM AND PRELIMINARIES
3.1 Problem Statement
Definition 1 (Order Set). An order set consists of numerous or-
ders during time slot ğ‘¡, which could be denoted by ğ‘‚ğ‘¡={ğ‘œğ‘¡
1,Â·Â·Â·,ğ‘œğ‘¡
|ğ‘‚ğ‘¡|}.
 
864Online Preference Weight Estimation Algorithm with Vanishing Regret for Car-Hailing in Road Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
An orderğ‘œğ‘¡
ğ‘–owns three attributes âŸ¨ğ‘ ğ‘¡
ğ‘–,ğ‘‘ğ‘¡
ğ‘–,ğ‘Ÿğ‘¡
ğ‘–âŸ©, whereğ‘ ğ‘¡
ğ‘–andğ‘‘ğ‘¡
ğ‘–repre-
sent the origin and destination, respectively. ğ‘Ÿğ‘–
ğ‘¡represents the actual
route ofğ‘œğ‘–
ğ‘¡. Time slots usually represent time intervals of the same
length. Note that ğ‘‚ğ‘¡can not be known until time slot ğ‘¡+1.
In commercial car-hailing systems, such as Didi Chuxing, the
time slot is usually 15minutes, one hour, or one day. Each order ğ‘œğ‘¡
ğ‘–
arrives at the platform at a specific moment in the time slot ğ‘¡. How-
ever, since the platform only adjusts its route planning algorithm
at the beginning or end of a time slot, we consider all orders in ğ‘‚ğ‘¡
to be processed at the beginning of the time slot ğ‘¡+1.
Definition 2 (Seqence of Order Sets). A sequence of order
sets is denoted byO={ğ‘‚1,ğ‘‚2,Â·Â·Â·,ğ‘‚ğ‘‡}, whereğ‘‡represents the
length of time slots.
The time horizon ğ‘‡in commercial platforms usually spans over
a week or a month. After ğ‘‡time slots, the car-hailing platform will
evaluate the quality of the route planning during this period. Refer-
ring to common setting [16, 33], we define the choice assumption.
Assumption 1 (Choice assumption). For each order ğ‘œğ‘¡
ğ‘–, it is
assumed that the actual route chosen by the user in the car-hailing
platform is the most preferred route between the origin-destination
pairğ‘ ğ‘¡
ğ‘–andğ‘‘ğ‘¡
ğ‘–.
Assumption 1 implies that at the end of each time slot, after the
users that correspond to the order set decide their actual routes,
the most preferred routes are also revealed as input. So now we can
explicitly define the Route Planning Problem, an online problem of
maximizing the overlapping ratio between the planned routes and
the most preferred routes.
Definition 3 (Route Planning Problem). Given a directed
graphğº=(ğ‘‰,ğ¸)representing the road network and a sequence of
order setO, the Route Planning Problem aims to find a planned route
ğ‘Ÿğ‘¡
ğ‘–ğ‘for each order ğ‘œğ‘¡
ğ‘–to maximize the cumulative accuracy.
ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡(1)
where
ğ‘ğ‘ğ‘ğ‘¡=Ã|ğ‘‚ğ‘¡|
ğ‘–=11(ğ‘Ÿğ‘¡
ğ‘–ğ‘,ğ‘Ÿğ‘¡
ğ‘–,ğœƒ)
|ğ‘‚ğ‘¡|(2)
andğ‘Ÿğ‘¡
ğ‘–ğ‘represents the route planned by the platform for ğ‘œğ‘¡
ğ‘–.1(ğ‘Ÿğ‘¡
ğ‘–ğ‘,ğ‘Ÿğ‘¡
ğ‘–,ğœƒ)
is a0âˆ’1function, returning 1when the length of the overlapping
route ofğ‘Ÿğ‘¡
ğ‘–ğ‘andğ‘Ÿğ‘¡
ğ‘–is greater than ğœƒtimes the length of ğ‘Ÿğ‘¡
ğ‘–. Otherwise,
it returns 0.ğœƒâˆˆ(0,1), and is usually set to 0.95in practice.
The Route Planning problem described in Definition 3 requires
the algorithm to explicitly specify the planned routes. As discussed
in Section 1, in this paper we are more interested in a variation
of Route Planning where the algorithm is only responsible for
assigning weights to the edges (roads), and the planned routes are
specified with the shortest paths with respect to the edge weights.
Before give the full definition of this variation problem, we first
clarify the relation between the edge weights and the shortest path
(planned path) with the following proposition.Definition 4 (Shortest Path Algorithm). Given a directed
graphğº=(ğ‘‰,ğ¸)with preference weights ğ‘Šand an order ğ‘œwith
the originğ‘ and destination ğ‘‘, we can find the most preferred route
ğ‘Ÿ=ğ‘†ğ‘ƒ(ğ‘ ,ğ‘‘,ğº,ğ‘Š)with a Shortest Path algorithmğ‘†ğ‘ƒ(Â·), such as Di-
jkstraâ€™s algorithm [ 13]. For convenience, We use ğ‘†ğ‘ƒ(ğ‘œ,ğ‘Š)to represent
ğ‘†ğ‘ƒ(ğ‘ ,ğ‘‘,ğº,ğ‘Š).
Based on the shortest path algorithm, we can give the definition
of the variation problem.
Definition 5 (Route Planning Problem with Edge Weights
Estimation). Given a directed graph ğº=(ğ‘‰,ğ¸)and a sequence of
order setO, the Route Planning Problem with Edge Weights Estimation
aims to output weights for all edges in ğ¸at each time slot ğ‘¡and
useğ‘†ğ‘ƒ(Â·)to get the planned route ğ‘Ÿğ‘¡
ğ‘–ğ‘for each order ğ‘œğ‘¡
ğ‘–such as to
maximize the cumulative accuracy by Equation (1).
As discussed in Section 1, existing work often decides the edge
weights in route planning with the traveling time or the length
of roads, while in this paper, we employ the Preference Weight
Estimation (PWE) models to learn mass preferences as edge weights
from historical data. We emphasize that our technical framework
does not make much assumption on the internal implementation
of the PWE models, but only requires them to satisfy the following
anonymous assumption.
Assumption 2 (Anonymous assumption). In consideration of
protecting usersâ€™ privacy, the preference weight estimation model is
based on the observation of the historical actual routes of all users,
instead of constructing unique edge weights for each user.
Definition 6 (Preference Weight Estimation Model). Given
a directed graph ğº=(ğ‘‰,ğ¸)and the order sets{ğ‘‚1,ğ‘‚2,Â·Â·Â·,ğ‘‚ğ‘¡âˆ’1}, a
preference weight estimation model ğ›¾outputs the preference weights
ğ‘Šğ‘¡for all edges in ğ¸at time slotğ‘¡.
Note that, for the sake of stability, the car-hailing platform would
not directly take the preference weights ğ‘Šğ‘¡output by a PWE model
as the final weights, but would use the Time-Decay function to
output the final weights at time slot ğ‘¡[8].
Definition 7 (Time-Decay Function ğ‘‡ğ·(Â·)).Given a directed
graphğº=(ğ‘‰,ğ¸), the preference weights set for the previous ğ¿time
slotsWğ‘¡={ğ‘Šğ‘—}ğ‘¡âˆ’1
ğ‘—=max{ğ‘¡âˆ’ğ¿,1}and the current preference weights ğ‘Šğ‘¡
output by a PWE model, the time-decay function is used to calculate
the final preference weight Ë†ğ‘Šğ‘¡at time slotğ‘¡.
Ë†ğ‘Šğ‘¡=ğ‘¡âˆ‘ï¸
ğ‘—=max{ğ‘¡âˆ’ğ¿,1}ğ‘ğ‘—ğ‘Šğ‘—, ğ‘ğ‘—=1
ğ‘’ğ‘¡âˆ’ğ‘—(3)
Here,ğ¿can be set according to the actual needs of different sce-
narios, usually 3. In the field of route planning, the length of edges
are non-negative real numbers, and smaller weights are associated
with the routes that are shorter [ 12,22]. Similarly, we assume that
the route with smaller preference weight is more preferred by users.
Therefore, we can use the shortest path algorithm to find the most
preferred routes for orders within ğºwith preference weights ğ‘Š.
We then give the formal definition of the online preference weight
estimation problem.
Definition 8 (Online Preference Weight Estimation Prob-
lem (O-PWE)). Given a directed graph ğº=(ğ‘‰,ğ¸)representing the
 
865KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucen Gao et al.
road network, the time horizon ğ‘‡and a set of PWE models for ref-
erence, at time slot ğ‘¡, we are revealed ğ‘‚ğ‘¡âˆ’1. The goal is to maximize
the cumulative accuracy by designing a preference weight estimation
algorithmğ›¾, which determines the sequence of preference weights
W={ğ‘Š1,ğ‘Š2,Â·Â·Â·,ğ‘Šğ‘‡}.
Figure 2 gives an example of the O-PWE problem. At time slot
ğ‘¡, we first need to give preference weights ğ‘Šğ‘¡for all edges by ğ›¾
and calculateË†ğ‘Šğ‘¡through the time-decay function. When an order
arrives, the appropriate route is obtained using the shortest path
algorithm and compared with the actual route of this order. When
the last order of this time slot arrives, we can compute the accuracy
of time slot ğ‘¡ofğ›¾, e.g.,ğ‘ğ‘ğ‘(ğ‘‚ğ‘¡,ğ›¾)orğ‘ğ‘ğ‘ğ‘¡ğ›¾. The goal of the O-PWE
problem is to maximize the cumulative accuracy of all ğ‘‡time slots.
O-PWE Problem
time slot ğ’•time slot ğ’•+ğŸâ€¦â€¦Order Set	ğ‘¶ğ’•ğ’…ğŸğ’•ğ’”ğŸğ’•ğ’ğŸğ’•ğ’…ğ’•ğŸğ’”ğŸğ’•ğ’ğŸğ’•Procedure for time slot t:Outputthepreferenceweightsğ‘Š!byÎ³andcalculateğ‘Š!#bythetime-decayfunctionFind the routeğ‘Ÿ"#!for each order ğ‘œ"!bytheshortestpathalgorithmCalculateaccuracyattimeslotğ‘¡by comparing planned routes and actual routesğ‘ğ‘ğ‘ğ‘‚!,ğ›¾=âˆ‘1(ğ‘Ÿ"#!,ğ‘Ÿ"!,ğœƒ)|%!|"&'|ğ‘‚!|1234567Order ğ‘œ!":  ğ‘ !"=3,ğ‘‘!"=7Planned route: 3 -> 2 -> 4-> 7Actual route: 3 ->2 -> 1-> 7Order Sequence
Figure 2: An example of the O-PWE problem
3.2 Preliminaries
We first introduce some preliminaries in order to better describe
the chasing algorithm in detail and analyze its performance.
3.2.1 Model Set. The model set Î“consists of various preference
weight estimation models.
3.2.2 Regret. Regret is the accumulative reward difference of an
algorithm to the best model fixed in-hindsight over a series of
actions [ 11]. Given a finite model set Î“and a sequence of order sets
O, the regret in the O-PWE problem can be defined as:
Regret =max
ğ›¾âˆˆÎ“ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ’ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘(ğ‘‚ğ‘¡,Ë†ğ‘Šğ‘¡) (4)
whereğ›¾represents the best model for the offline scenario.
3.2.3 Vanishing Regret. The vanishing regret is also called no re-
gret or sublinear regret [ 23,26]. It means that the regret per time
slot tends to zero, and is widely recognized as a great theoreti-
cal performance metric [ 1]. The vanishing regret in the O-PWE
problem can be defined as:
Avg. Regret =ğ‘…ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘¡
ğ‘‡â†’0,asğ‘‡â†’âˆ (5)
3.2.4 The Statefulness of Model Selection for O-PWE. The stateful-
ness of online learning means that it is not possible to make the
current state identical to the state of the target model by the action
of the current step alone. Since Ë†ğ‘Šğ‘¡andğ‘ğ‘ğ‘ğ‘¡ğ›¾do not just depend on
the preference weights at time slot ğ‘¡, but also on the ones at thepreviousğ¿time slots, we cannot get the same Ë†ğ‘Šğ‘¡andğ‘ğ‘ğ‘ğ‘¡ğ›¾as the
target model simply by determining ğ‘Šğ‘¡. This means O-PWE is a
stateful online learning problem.
3.2.5 Reduction to Dynamic deterministic Markov Decision Process.
For a stateful online learning problem, we usually reduce it to a
Dynamic deterministic Markov Decision Process (Dd-MDP) in order
to propose a solution to the problem. Here, we introduce concepts
of Dd-MDP for the O-PWE.
State: The state should be able to reflect the information about
historical order sets as well as the execution performance of the
algorithm. Hence, we use the tuple sequence of the past ğ¿time slots
{ğ‘Šğ‘—,ğ‘‚ğ‘—}ğ‘¡âˆ’1
ğ‘—=ğ‘¡âˆ’ğ¿as the state at time slot ğ‘¡.
Action: We identify the action with the help of the model set Î“.
Specifically, at each time slot ğ‘¡, we select a target model ğ›¾âˆ—from Î“,
and update the preference weights ğ‘Šğ‘¡based onğ›¾âˆ—.
Reward: According to ğ‘Šğ‘¡, we can calculate Ë†ğ‘Šğ‘¡throughğ‘‡ğ·(Â·)
and then compute the accuracy ğ‘ğ‘ğ‘ğ‘¡at time slot ğ‘¡as the reward.
Transition function: The state transition function is used to
calculate the state at time slot ğ‘¡+1. It is determined by the preference
weights of the past ğ¿time slots and selected target model at ğ‘¡, i.e.,
ğ‘”({ğ‘Šğ‘—}ğ‘¡âˆ’1
ğ‘—=ğ‘¡âˆ’ğ¿,ğ›¾âˆ—)={ğ‘Šğ‘—}ğ‘¡
ğ‘—=ğ‘¡âˆ’ğ¿+1. Hence, The state at time slot
ğ‘¡+1is{ğ‘Šğ‘—,ğ‘‚ğ‘—}ğ‘¡
ğ‘—=ğ‘¡âˆ’ğ¿+1.
4 ALGORITHM OF PREFERENCE WEIGHT
CHASING
In this section, we design an online learning-based algorithm called
Preference Weight Chasing (PWC) to solve the O-PWE problem.
Figure 3 illustrates the framework of the PWC algorithm.
First, we propose our Model Selector ğ”–based on the Following
the Perturbed Leader (FPL) algorithm [ 24]. Algorithm 1 shows the
pseudo-code. Given a model set Î“, at time slot ğ‘¡,ğ”–first samples
a possible accuracy ğ‘ğ‘¡ğ›¾from an exponential distribution for each
modelğ›¾(Lines 1âˆ’2). Here, the purpose of our construction of the ex-
ponential distribution asâˆšï¸
logğ‘›/ğ¿ğ‘‡ğ‘’âˆ’âˆš
logğ‘›/ğ¿ğ‘‡ğ‘¥is to subsequently
reach vanishing regret, which we will explain more in Section 5. ğ”–
then selects the model ğ›¾âˆ—with the maximum cumulative accuracy
as the target model (Line 3).
Algorithm 1: Model Selector ğ”–
Input: Model set Î“, time slotğ‘¡.
Output: Target model ğ›¾âˆ—.
1forğ›¾âˆˆÎ“do
2 Sampleğ‘ğ‘¡ğ›¾â‰¥0from distribution
ğ‘‘ğœ‡(ğ‘¥)=âˆšï¸
log|Î“|/ğ¿ğ‘‡ğ‘’âˆ’âˆš
log|Î“|/ğ¿ğ‘‡ğ‘¥;
3Output the target model ğ›¾âˆ—with maximum cumulative
accuracyÃğ‘¡âˆ’1
ğ‘—=1ğ‘ğ‘ğ‘ğ‘—
ğ›¾+ğ‘ğ‘¡ğ›¾;
The Chasing Oracle ğ”’outputs preference weights based on the
target model. Algorithm 2 shows the corresponding pseudo-code.
Given the target model ğ›¾âˆ—, initial time slot to start chasing ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡,
the corresponding state ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡, and final time slot ğ‘¡ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ that is the
last time slot for which ğ›¾âˆ—is the target model, for time slot ğ‘¡(Line
1),ğ›¾âˆ—first outputs the preference weights ğ‘Šğ‘¡
ğ›¾âˆ—(Line 2), which we
 
866Online Preference Weight Estimation Algorithm with Vanishing Regret for Car-Hailing in Road Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Model  Set Î“
ğ›¾!
ğ›¾"
ğ›¾#
ğ›¾|%|Â·Â·Â·
Model Selector ğ”–Sample FromDistributionğ‘!"=log |Î“|/LTğ‘’âˆ’log |#|/LT$
Cumulative acc
Currentacccalculationğ‘ğ‘ğ‘ğ‘‚!,ğ›¾=âˆ‘1(ğ‘Ÿ"#!,ğ‘Ÿ"!,ğœƒ)|%!|"&'|ğ‘‚!|ğ›¾âˆ—'(!	â†’ğ›¾âˆ—'Chasing Process
ğ›¾âˆ—"=ğ›¾âˆ—"#$	
NoChasing
YesContinueCumulative acc
InChasing?NoYes
Continue Chasing
ğ‘Ÿ&'âˆ—"â†ğ‘†ğ‘ƒğ‘œ&",ğ‘Š*'âˆ—"
TargetModelğœ¸âˆ—ğ’•Time Sequence
Time Decay+Shortest Path
ShortestPath Algorithm1234567ğ‘Š!"=$ğ‘"ğ‘Š"!"#!$%
Time Decay +Shortest Path
Figure 3: The framework of PWC algorithm
assign toğ‘Šğ‘¡(Line 3). We then calculate Ë†ğ‘Šğ‘¡using the time-decay
functionğ‘‡ğ·(Â·)by Definition 7 (Line 4). For all orders within ğ‘‚ğ‘¡,
we obtain the corresponding routes using ğ‘†ğ‘ƒ(Â·)(Lines 5âˆ’6) and
calculate the accuracy ğ‘ğ‘ğ‘ğ‘¡(Line 7). Similarly, we calculate Ë†ğ‘Šğ‘¡
ğ›¾âˆ—and
the accuracy of the target model ğ›¾âˆ—at time slotğ‘¡(Lines 8âˆ’11). The
above calculation is repeated until the target model changes.
Algorithm 2: Chasing Oracle ğ”’
Input: Target model ğ›¾âˆ—, initial time slot ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡, final time slot
ğ‘¡ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ , initial state ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡.
Output: Preference weights and accuracy sequence.
1forğ‘¡â†ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡toğ‘¡ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ do
2 Target model ğ›¾âˆ—outputsğ‘Šğ‘¡
ğ›¾âˆ—;
3 Update the preference weight ğ‘Šğ‘¡=ğ‘Šğ‘¡
ğ›¾âˆ—;
4 Calculate Ë†ğ‘Šğ‘¡usingğ‘‡ğ·(Wğ‘¡,ğ‘Šğ‘¡);
5 forğ‘œğ‘¡
ğ‘–âˆˆğ‘‚ğ‘¡do
6 Obtain the route ğ‘Ÿğ‘¡
ğ‘–for orderğ‘œğ‘¡
ğ‘–usingğ‘†ğ‘ƒ(ğ‘œğ‘¡
ğ‘–,Ë†ğ‘Šğ‘¡);
7 Calculate accuracy ğ‘ğ‘ğ‘ğ‘¡;
8 Calculate Ë†ğ‘Šğ‘¡
ğ›¾âˆ—usingğ‘‡ğ·(Wğ‘¡
ğ›¾âˆ—,ğ‘Šğ‘¡
ğ›¾âˆ—);
9 forğ‘œğ‘¡
ğ‘–âˆˆğ‘‚ğ‘¡do
10 Obtain the route ğ‘Ÿğ‘¡
ğ‘–ğ›¾âˆ—for orderğ‘œğ‘¡
ğ‘–usingğ‘†ğ‘ƒ(ğ‘œğ‘¡
ğ‘–,Ë†ğ‘Šğ‘¡
ğ›¾âˆ—);
11 Calculate accuracy ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ—;
Algorithm 3 shows pseudo-code of the Preference Weights Chas-
ing algorithm. We refer to the main model of our PWC algorithm as
the ensemble model. At time slot ğ‘¡(Line 1), we first call the model
selector ğ”–to choose a target model ğ›¾âˆ—ğ‘¡(Line 2). If the target model
atğ‘¡is different from that at ğ‘¡âˆ’1, the chasing oracle ğ”’is invokedwithğ›¾âˆ—ğ‘¡, thus obtaining the preference weights ğ‘Šğ‘¡and accuracy
ğ‘ğ‘ğ‘ğ‘¡of the ensemble model (Lines 3âˆ’5). Otherwise, we continue to
execute the previous ğ”’to getğ‘Šğ‘¡andğ‘ğ‘ğ‘ğ‘¡(Lines 6âˆ’7). Simultane-
ously, PWC algorithm needs to obtain the preference weights and
accuracy of each base model (Lines 8âˆ’11). Finally, PWC algorithm
updates the states of the ensemble and base models (Line 12).
Algorithm 3: Preference Weights Chasing Algorithm
Input: Order sequenceO, model set Î“, model selector ğ”–,
chasing oracle ğ”’, initial state ğ‘ 1.
Output: Preference weights and accuracy sequence.
1forğ‘¡â†1toğ‘‡do
2 Invoke ğ”–to select a target model ğ›¾âˆ—ğ‘¡;
3 ifğ‘¡â‰¥2&ğ›¾âˆ—ğ‘¡â‰ ğ›¾âˆ—ğ‘¡âˆ’1then
4 Invoke ğ”’with target model ğ›¾âˆ—ğ‘¡;
5 Get preference weights ğ‘Šğ‘¡and accuracy ğ‘ğ‘ğ‘ğ‘¡;
6 else
7 Continue the existing run of ğ”’, thus obtaining
preference weights ğ‘Šğ‘¡and accuracy ğ‘ğ‘ğ‘ğ‘¡;
8 foreachğ›¾âˆˆÎ“do
9 forğ‘œğ‘¡
ğ‘–âˆˆğ‘‚ğ‘¡do
10 Obtain route ğ‘Ÿğ‘¡
ğ‘–ğ›¾for orderğ‘œğ‘¡
ğ‘–usingğ‘†ğ‘ƒ(ğ‘œğ‘¡
ğ‘–,Ë†ğ‘Šğ‘¡ğ›¾);
11 Calculate accuracy ğ‘ğ‘ğ‘ğ‘¡ğ›¾;
12 Update states of the ensemble and base models;
5 THEORETICAL GUARANTEES
In this section, we give the proof that the proposed algorithm PWC
can achieve vanishing regret.
 
867KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucen Gao et al.
5.1 Online Learning with Switching Cost (OLSC)
In an online learning problem, one decision maker should make a
sequence of decisions without knowledge of the future. One version
of the problem is the case with ğ‘›experts. At each time slot, the
decision maker selects one expert and then observes the cost âˆˆ[0,1]
for each expert. Note that, for the Online Learning with Switching
Cost scenario, an extra cost termed switching cost is incurred every
time the decision maker switches from one expert to another [ 24].
Here, we first introduce Lemma 1 about OLSC.
Lemma 1 ([ 14]).Online Learning with switching cost with Î”as the
upper bound on the switching cost admits an online learning algorithm
with regretğ‘‚(âˆšï¸
Î”Â·ğ‘‡logÎ“), e.g., the Following The Perturbed Leader
(FPL) algorithm ğ”„[24].
Here, we introduce the FPL algorithm as shown in Algorithm 4
and the corresponding Lemma 2.
Algorithm 4: FPL Algorithm ğ”„
Input: Expert set Î“.
Output: Cumulative reward.
1forğ‘¡â†1toğ‘‡do
2 forğ›¾âˆˆÎ“do
3 Sampleğ‘ğ‘¡ğ›¾â‰¥0from distribution ğ‘‘ğœ‡(ğ‘¥)=ğœ–ğ‘’âˆ’ğœ–ğ‘¥;
4Choose expert with maximalÃğ‘¡âˆ’1
ğ‘—=1ğ‘Ÿğ‘—
ğ›¾+ğ‘ğ‘¡ğ›¾;
5Obtain the reward ğ‘Ÿğ‘¡of the chosen expert;
Lemma 2. FPL gives the following small regret relative to the best
expert fixed in-hindsight.
Regretâ‰¤ğœ–(max reward in hindsight)+ğ‘‚(log|Î“|)
ğœ–(6)
Since the reward each time slot is âˆˆ[0,1]andÎ”is the upper
bound on the switching cost, max reward in hindsight â‰¤(1+Î”)ğ‘‡.
Hence, when ğœ–=âˆšï¸ƒ
log|Î“|
(1+Î”)ğ‘‡,
ğ‘…ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘¡â‰¤âˆšï¸„
log|Î“|
(1+Î”)ğ‘‡(1+Î”)ğ‘‡+ğ‘‚(âˆšï¸
(1+Î”)ğ‘‡log|Î“|)
=âˆšï¸
(1+Î”)ğ‘‡log|Î“|+ğ‘‚(âˆšï¸
(1+Î”)ğ‘‡log|Î“|)
Since Î”is usually greater than 1, FPL algorithm can achieve the
regretğ‘‚(âˆšï¸
Î”Â·ğ‘‡logÎ“)when we set ğœ–=âˆšï¸ƒ
log|Î“|
(1+Î”)ğ‘‡.
5.2 Chasing Regret Analysis
We call the process that occurs in a chasing oracle ğ”’, i.e., when the
target model is unchanged, the chasing procedure, and thus define
the chasing regret [14].
Definition 9 (Chasing Regret). The chasing regret ğœis defined
to describe the regret generated in the chasing procedure, as shown in
Equation (7).
ğœâ‰œğ‘¡ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™âˆ‘ï¸
ğ‘¡=ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ’ğ‘¡ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™âˆ‘ï¸
ğ‘¡=ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘ğ‘ğ‘ğ‘¡(7)
We then claim Theorem 1 and give the corresponding proof.Theorem 1. The chasing regret generated by the chasing Oracle
ğ”’corresponding to the O-PWE problem is ğ‘‚(ğ¿).
Proof. According to the definition of state ğ‘ for the Dd-MDP,
we can find that after chasing ğ¿time-steps as Algorithm 2, the state
(preference weight sequence and order set sequence) of ensemble
model will be completely consistent with that of target model ğ›¾âˆ—.
Therefore, the chasing regret of ğ”’is
ğœ=ğ‘¡ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™âˆ‘ï¸
ğ‘¡=ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ—âˆ’ğ‘¡ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™âˆ‘ï¸
ğ‘¡=ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘ğ‘ğ‘ğ‘¡=ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡+ğ¿âˆ‘ï¸
ğ‘¡=ğ‘¡ğ‘–ğ‘›ğ‘–ğ‘¡(ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ—âˆ’ğ‘ğ‘ğ‘ğ‘¡). (8)
Asğ‘ğ‘ğ‘ğ‘¡âˆˆ[0,1],ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ—âˆ’ğ‘ğ‘ğ‘ğ‘¡âˆˆ[0,1]. Hence, the chasing regret
ofğ”’isğ‘‚(ğ¿). â–¡
5.3 Reduction to Analysis of OLSC Algorithm
When the chasing regret is verified to be sublinear, we can modify
FPL algorithm to our proposed model selector ğ”–so as to solve the
stateful online learning problem with the help of the solution of the
OLSC problem. The key design idea is to map the chasing regret to
the switching cost by our porposed chasing oracle ğ”’.
We first recall the regret of the OLSC problem.
Regret =max
ğ›¾âˆˆÎ“ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ’(ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
ğ›¾ğ‘¡âˆ’Î”Â·ğ‘‡âˆ‘ï¸
ğ‘¡=21ğ›¾ğ‘¡â‰ ğ›¾ğ‘¡âˆ’1) (9)
When our algorithm satisfies the following conditions, we can
obtain the Thorem 2.
â€¢the expert set of ğ”„is identified with the model set Î“in the
O-PWE problem;
â€¢the number of steps of ğ”„equals to that in the O-PWE prob-
lem, denoted by ğ‘‡;
â€¢the switching cost of ğ”„is set to Î”=ğœ.
Theorem 2. The regret of PWC with a chasing oracle of chasing
regretğœfor the O-PWE problem is ğ‘‚(âˆšï¸
ğœğ‘‡logÎ“).
Proof. We split the ğ‘‡time slots into episodes {1,2,Â·Â·Â·}. Each
episodeğœƒrepresents a maximal contiguous time slot sequence
during which ğ”„keeps the model ğ›¾ğœƒunchanged. Assuming that ğ‘¡ğœƒ
andğ‘¡â€²
ğœƒdenote the first and last time slot of episode ğœƒrespectively,
PWC follows the sequential actions generated by ğ”’during[ğ‘¡ğœƒ,ğ‘¡â€²
ğœƒ]
and the chasing regret of ğ”’is upper bounded by ğœ=Î”. The chasing
regret follows that
ğ‘¡â€²
ğœƒâˆ‘ï¸
ğ‘¡=ğ‘¡ğœƒğ‘ğ‘ğ‘ğ‘¡
ğ›¾ğœƒâˆ’ğ‘¡â€²
ğœƒâˆ‘ï¸
ğ‘¡=ğ‘¡ğœƒğ‘ğ‘ğ‘ğ‘¡â‰¤Î”.
Therefore, for each model ğ›¾âˆˆÎ“, we have
ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ’ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
â‰¤ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ’âˆ‘ï¸
ğœƒ(ğ‘¡â€²
ğœƒâˆ‘ï¸
ğ‘¡=ğ‘¡ğœƒğ‘ğ‘ğ‘ğ‘¡
ğ›¾ğœƒâˆ’Î”)
=ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
ğ›¾âˆ’(ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘ğ‘ğ‘ğ‘¡
ğ›¾ğ‘¡âˆ’Î”Â·ğ‘‡âˆ‘ï¸
ğ‘¡=21ğ›¾ğ‘¡â‰ ğ›¾ğ‘¡âˆ’1)
 
868Online Preference Weight Estimation Algorithm with Vanishing Regret for Car-Hailing in Road Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
PWC
POWER-D
POWER-S
ConSTGAT
LengthPWC
POWER-D
POWER-S
ConSTGAT
Length
70
60
50
40
30
20
10
0
(a) Beijing
PWC
POWER-D
POWER-S
ConSTGAT
LengthPWC
POWER-D
POWER-S
ConSTGAT
Length
50
40
30
20
10
0 (b) Shanghai
PWC
POWER-D
POWER-S
ConSTGAT
LengthPWC
POWER-D
POWER-S
ConSTGAT
Length
60
50
40
30
20
10
0 (c) Qingdao
Figure 4: Accuracy comparison: The color block represents the cumulative accuracy of the
method at the vertical axis minus that of the method at to the horizontal axis.
3691215182124
T1234567Day
âˆ’4âˆ’3âˆ’2âˆ’10123Figure 5: The accuracy loss of
PWC (Beijing) (%)
By Lemma 1, the regret is at most
ğ‘‚(âˆšï¸
Î”Â·ğ‘‡log|Î“|)=ğ‘‚(âˆšï¸
ğœÂ·ğ‘‡log|Î“|).â–¡
5.4 Regret of PWC for O-PWE Problem
Since the chasing regret of our chasing oracle ğ”’isğ‘‚(ğ¿), we set
ğœ–=âˆšï¸
log|Î“|(1+ğ¿)ğ‘‡according to the deduction in Section 5.1, thus
obtaining the exp. distribution ğ‘‘ğœ‡(ğ‘¥)=âˆšï¸
log|Î“|/ğ¿ğ‘‡ğ‘’âˆ’âˆš
log|Î“|/ğ¿ğ‘‡ğ‘¥
and our model selector ğ”–. We recall the O-PWE problem and the
PWC algorithm. Given the model set Î“, the model selector ğ”–, the
chasing oracle ğ”’, PWC outputs preference weights and obtains the
accuracy at each time slot. Combining Theorem 1 and Theorem 2,
we can obtain Theorem 3.
Theorem 3. The regret of PWC with model selector ğ”–and chasing
oracleğ”’for O-PWE problem is ğ‘‚(âˆšï¸
ğ¿ğ‘‡log|Î“|).
Becauseğ¿is taken based on specific scenarios, but must be much
smaller thanâˆš
ğ‘‡, the regret is sublinear in terms of ğ‘‡. That is to
say, PWC can achieve vanishing regret for the O-PWE problem.
6 EXPERIMENTS
6.1 Datasets
We use the Beijing, Shanghai and Qingdao road network from
Didi, a carpooling service provider, because they have different
road network structures. We set each time slot to one hour and
randomly select samples from each time slot of the historical orders.
The attributes of each sample include the timestamp, staring point,
ending point, and actual route.
Table 1: Statistics of the Datasets
Dataset #Edges #Nodes Time Range
Beijing 2,575,216 1,119,143 04/16/2023-04/22/2023
Shanghai 2,840,477 1,225,903 04/16/2023-04/22/2023
Qingdao 2,135,189 821,661 04/16/2023-04/22/2023
Table 1 shows the statistics of different datasets. Figure 6 shows
the difference in the road network structure of different cities (labels
on maps are marked in Chinese). Beijing has a relatively more
regular road network structure as shown in Figure 6(a), while road
network structures of Shanghai and Qingdao are freer as shown in
Figure 6(b) and (c).
Beijing(a) Square Grid Road Network(c)FreestyleRoad Network(b) Circular Radial Road NetworkQingdaoShanghaiFigure 6: Road network structure of different cities
6.2 Baseline Methods
Because preference weight estimation is not yet well-studied issues,
we adopt four baseline methods used in the industry to our scenario.
We present their core ideas here.
Length Weight: Because the length of the roads rarely changes,
using length weight as preference weight can be considered essen-
tially static over ğ‘‡time slots.
ConSTGAT: We use the road time weight prediction model
ConSTGAT to generate the time weights for different time slots [ 15].
Unlike length weights, time weights are changeable across time
slots, which are influenced by a variety of factors such as travel
demand and weather.
POWER-S: A deep learning method called POWER is designed
by Didi based on DeepFM [ 20], which is easy to deploy with effec-
tiveness and efficiency. We use data from the last 3weeks to get
the static preference weight.
POWER-D: Unlike the static preference weight, the dynamic
preference weight has a corresponding preference weight for each
hour of a week.
6.3 Performance Analysis
We denote the number of orders sampled in each time slot as ğ‘.
Figure 4 shows the difference between PWC and its base models
in terms of cumulative accuracy for the online order sequences
sampled in the three cities with ğ‘=20,ğ‘‡=168. It can be seen that
the PWC algorithm achieves the best cumulative accuracy in all
three cities. Meanwhile, The lead of the PWC algorithm compared
to the base models varies relatively widely across cities, which
show that the performance of the preference weight estimation
methods is affected by different attributes such as the structure of
 
869KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucen Gao et al.
1224364860728496108120132144156168
T020406080AccLength
ConSTGAT
POWER-S
POWER-D
PWC
(a) Front view
1224364860728496108120132144156168
TBeijingShanghaiQingdao
City020406080
AccLength
ConSTGAT
POWER-S
POWER-D
PWC (b) 3-D view
Figure 7: Experiment Results for PWC on different online
order sequences: 3-dimensional view for the number of time
slotsğ‘‡, city versus cumulative accuracy: (a) Front view: A
straight line formed by points of the same color reflects the
change in cumulative accuracy of each method in a city with
different values of ğ‘‡. Asğ‘‡grows, the relative performance
of the PWC algorithm gradually improves. (b) 3-D view: The
PWC algorithm performs better when ğ‘‡is relatively large,
while the POWER-S and POWER-D methods perform better
whenğ‘‡is relatively small.
the road network in different cities. Besides, the performance of
using length and time weights directly as preference weights is
significantly weaker than that of the remaining methods, which
shows that targeting user preferences is not exactly equivalent
to travel time or distance. However, based on the comparison of
ConSTGAT and length weights, it can be seen that users attach
more importance to travel time.
We also conducted experiments in Beijing for different values of
ğ‘‡. In order to compare the performance of the PWC algorithm with
that of the best method fixed in-hindsight, we define the accuracy
loss as shown in Equation (10):
Accuracy loss =Acc of PWCâˆ’max Acc ofğ›¾âˆˆÎ“
max Acc ofğ›¾âˆˆÎ“(10)
Figure 5 shows the accuracy loss for different values of ğ‘‡, where
positive values represent that the PWC algorithm outperforms
the best model among the baseline methods, and negative values
represent that the performance of the PWC algorithm is weaker
than the best model among the baseline methods. Since each time
slot corresponds to one hour, the total time corresponding to each
square in the heat map is (Dayâˆ’1)âˆ—24+ğ‘‡. The average accuracy loss
for these 56instances is 1.28%, which represents the PWC algorithm
has the best average performance. Note that when time is 105, the
accuracy loss takes the maximum value of 2.71%. And when time
is15, the accuracy loss achieves the minimum value of âˆ’3.29%.
Combined with the trend of the accuracy loss in the heat map,
PWC achieves the best performance when time is large enough,
which is consistent with our theoretical analysis in Section 5. Note
that tiny enhancements in industrial applications have tremendous
value [ 9,36]. Based on the annual report from carpooling service
providers, the average amount of daily orders can reach up to 2.5
million. This means that small improvements in accuracy metric can
lead to significant corporate word-of-mouth and financial benefits.Figure 7 shows the cumulative accuracy for different values of ğ‘‡
and cities. The results show that the PWC algorithm achieves the
best cumulative accuracy in most cases and there is a significant
difference in the absolute values of the cumulative accuracy for
different cities. Figure 7a shows how the model performance varies
with the growth of ğ‘‡. Figure 7b gives the full 3d view. From this, it
can be seen that different base models have their own advantages
in different scenarios, and PWC can synthesize the advantages of
each base model to better cope with future scenarios and achieve
good performance in terms of cumulative accuracy.
Sensitivity to ğ‘.To verify the stability of PWC, we change
the number of orders sampled from each time slot for sensitivity
analysis. Table 2 shows the experimental results. It can be seen that
the POWER-Dâ€™s performance is best when the number of sampled
orders is small, and when the number of sampled orders increases,
the performance advantage of PWC gradually expands from 0.1
for15orders to 0.25for20orders. This demonstrates that PWC is
suitable for scenarios with large-scale data, which is suitable for the
practical application needs of the car-hailing service providers. In
addition, the results show that PWC can strike a balance between
performance and efficiency using sampling methods.
Table 2: Accuracy w.r.t. Order Number/Time Slot (Beijing)
Modelğ‘10 15 20
Length 14.00 14.35 13.60
ConSTGAT 69.85 70.45 70.65
POWER-S 81.20 80.75 81.15
POWER-D 82.10 82.17 82.50
PWC 81.40 82.27 83.75
7 CONCLUSION
In this paper, we formulate the Online Preference Weight Estima-
tion (O-PWE) problem with the goal of maximizing the cumulative
accuracy of the route planning task for the car-hailing service.
Solving the O-PWE problem helps to recommend routes to users
that match their preferences, which leads to more users and in-
creased platform revenue. However, since the usersâ€™ preference
changes over time and future data is uncertain, it is difficult to
achieve good performance with existing models. Moreover, exist-
ing models cannot give a theoretical guarantee. Hence, inspired by
the online learning algorithm, we propose a online learning-based
algorithm called Preference Weight Chasing (PWC) to provide the
great theoretical guarantee, vanishing regret. PWC consists of a
model selector ğ”–and a chasing oracle ğ”’. We use four baseline
methods as PWE model set to execute experiments on real-world
datasets. The results show the superiority of PWC.
8 ACKNOWLEDGE
This work was supported by the National Natural Science Founda-
tion of China [U23A20309, 62272302, 62172276, 62372296, 62302273];
Shanghai Municipal Science and Technology Major Project [2021-
SHZDZX0102]; Science Fund Program of Shandong Province for
Distinguished Oversea Young Scholars [2023HWYQ-006]; and CCF-
DiDi GAIA Collaborative Research Funds [202307].
 
870Online Preference Weight Estimation Algorithm with Vanishing Regret for Car-Hailing in Road Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
REFERENCES
[1]Hassene Aissi, Cristina Bazgan, and Daniel Vanderpooten. 2009. Min-max and
min-max regret versions of combinatorial optimization problems: A survey. Eu-
ropean Journal of Operational Research 197, 2 (2009), 427â€“438.
[2]Omer Amar, Ilana Sarfati, and Kobi Cohen. 2023. An Online Learning Approach
to Shortest Path and Backpressure Routing in Wireless Networks. IEEE Access 11
(2023), 57253â€“57267.
[3]Krystian Birr, Kazimierz Jamroz, and Wojciech Kustra. 2014. Travel Time of
Public Transport Vehicles Estimation. Transportation Research Procedia 3 (2014),
359â€“365.
[4]Marta Borowska-StefaÅ„ska, MichaÅ‚ Kowalski, Filip TuroboÅ›, and Szymon
WiÅ›niewski. 2022. On determining the weight of edges in map-representing
graphs-applications of heuristic methods in planning escape routes. Journal of
Traffic and Transportation Engineering 9, 6 (2022), 1027â€“1043.
[5]Guochen Cai, Kyungmi Lee, and Ickjai Lee. 2018. Itinerary recommender system
with semantic trajectory pattern mining from geo-tagged photos. Expert Systems
with Applications 94 (2018), 32â€“40.
[6]Kechao Cai, Xutong Liu, Yu-Zhen Janice Chen, and John C. S. Lui. 2018. An
Online Learning Approach to Network Application Optimization with Guarantee.
InIEEE Conference on Computer Communications (INFOCOM). 2006â€“2014.
[7]Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang, and Hai-Hong
Tang. 2018. Stabilizing Reinforcement Learning in Dynamic Environment with
Application to Online Recommendation. In ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (SIGKDD). 1187â€“1196.
[8]Yi-Cheng Chen, Lin Hui, and Tipajin Thaipisutikul. 2021. A collaborative filtering
recommendation system with dynamic time decay. The Journal of Supercomputing
77, 1 (2021), 244â€“262.
[9]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan
Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.
2016. Wide & Deep Learning for Recommender Systems. In Workshop on Deep
Learning for Recommender Systems (DLRS@RecSys). 7â€“10.
[10] Razvan-Gabriel Cirstea, Bin Yang, Chenjuan Guo, Tung Kieu, and Shirui Pan.
2022. Towards Spatio- Temporal Aware Traffic Time Series Forecasting. In IEEE
International Conference on Data Engineering (ICDE). 2900â€“2913.
[11] Ofer Dekel and Elad Hazan. 2013. Better Rates for Any Adversarial Deterministic
MDP. In International Conference on Machine Learning (ICML). 675â€“683.
[12] Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester,
Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Brett Wiltshire, Pe-
ter W. Battaglia, Vishal Gupta, Ang Li, Zhongwen Xu, Alvaro Sanchez-Gonzalez,
Yujia Li, and Petar Velickovic. 2021. ETA Prediction with Graph Neural Net-
works in Google Maps. In International Conference on Information and Knowledge
Management (CIKM). 3767â€“3776.
[13] Edsger W. Dijkstra. 1959. A note on two problems in connexion with graphs.
Numer. Math. 1 (1959), 269â€“271.
[14] Yuval Emek, Ron Lavi, Rad Niazadeh, and Yangguang Shi. 2020. Stateful Posted
Pricing with Vanishing Regret via Dynamic Deterministic Markov Decision
Processes. In Advances in Neural Information Processing Systems (NeurIPS).
[15] Xiaomin Fang, Jizhou Huang, Fan Wang, Lingke Zeng, Haijin Liang, and Haifeng
Wang. 2020. ConSTGAT: Contextual Spatial-Temporal Graph Attention Net-
work for Travel Time Estimation at Baidu Maps. In ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (SIGKDD). 2697â€“2705.
[16] Ziquan Fang, Yuntao Du, Xinjun Zhu, Danlei Hu, Lu Chen, Yunjun Gao, and
Christian S. Jensen. 2022. Spatio-Temporal Trajectory Similarity Learning in
Road Networks. In ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (SIGKDD). 347â€“356.
[17] JoÃ£o Gama, Indre Zliobaite, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid
Bouchachia. 2014. A survey on concept drift adaptation. ACM Computer Survey
46, 4 (2014), 44:1â€“44:37.[18] Nandani Garg and Sayan Ranu. 2018. Route Recommendations for Idle Taxi
Drivers: Find Me the Shortest Route to a Customer!. In ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (SIGKDD). 1425â€“1434.
[19] Dian Gong, Xuemei Zhao, and GÃ©rard G. Medioni. 2012. Robust Multiple Manifold
Structure Learning. In International Conference on Machine Learning (ICML).
[20] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In
International Joint Conference on Artificial Intelligence (IJCAI). 1725â€“1731.
[21] Steven C. H. Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. 2021. Online learning:
A comprehensive survey. Neurocomputing 459 (2021), 249â€“289.
[22] Jizhou Huang, Zhengjie Huang, Xiaomin Fang, Shikun Feng, Xuyi Chen, Jiaxiang
Liu, Haitao Yuan, and Haifeng Wang. 2022. DuETA: Traffic Congestion Propaga-
tion Pattern Modeling via Efficient Graph Learning for ETA Prediction at Baidu
Maps. In International Conference on Information and Knowledge Management
(CIKM). 3172â€“3181.
[23] Shinji Ito, Daisuke Hatano, Hanna Sumita, Akihiro Yabe, Takuro Fukunaga,
Naonori Kakimura, and Ken-ichi Kawarabayashi. 2017. Efficient Sublinear-Regret
Algorithms for Online Sparse Linear Regression with Limited Observation. In
Annual Conference on Neural Information Processing Systems (NeurIPS). 4099â€“
4108.
[24] Adam Tauman Kalai and Santosh S. Vempala. 2005. Efficient algorithms for online
decision problems. J. Comput. System Sci. 71, 3 (2005), 291â€“307.
[25] Ning Ma, Mustafa Ispir, Yuan Li, Yongpeng Yang, Zhe Chen, Derek Zhiyuan
Cheng, Lan Nie, and Kishor Barman. 2022. An Online Multi-task Learning
Framework for Google Feed Ads Auction Models. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (SIGKDD). 3477â€“3485.
[26] Mojmir Mutny and Andreas Krause. 2021. No-regret Algorithms for Captur-
ing Events in Poisson Point Processes. In International Conference on Machine
Learning (ICML). 7894â€“7904.
[27] Paul Newson and John Krumm. 2009. Hidden Markov map matching through
noise and sparseness. In ACM SIGSPATIAL International Symposium on Advances
in Geographic Information Systems. 336â€“343.
[28] Longquan Tao, Jinli Cao, and Fei Liu. 2018. Dynamic feature weighting based on
user preference sensitivity for recommender systems. Knowledge Based Systems
149 (2018), 61â€“75.
[29] Mikhail B. Vialtsev and Mikhail M. Komarov. 2023. A study of the impact of
implementation of smart contracts in the sharing economy. In IEEE Conference
on Business Informatics (CBI). 1â€“8.
[30] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
Computing 17, 4 (2007), 395â€“416.
[31] Fei Wang and Changshui Zhang. 2008. Label Propagation through Linear Neigh-
borhoods. IEEE Transactions on Knowledge and Data Engineering (TKDE)) 20, 1
(2008), 55â€“67.
[32] Jia Wang, Shuhao Jiang, and Jincheng Ding. 2023. Online learning resource
recommendation method based on multi-similarity metric optimization under
the COVID-19 epidemic. Computer Communications 206 (2023), 152â€“159.
[33] Yuan Xu, Jiajie Xu, Jing Zhao, Kai Zheng, An Liu, Lei Zhao, and Xiaofang Zhou.
2022. MetaPTP: An Adaptive Meta-optimized Model for Personalized Spatial
Trajectory Prediction. In ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (SIGKDD). 2151â€“2159.
[34] Zichuan Xu, Wenhao Ren, Weifa Liang, Wenzheng Xu, Qiufen Xia, Pan Zhou,
and Mingchu Li. 2022. Schedule or Wait: Age-Minimization for IoT Big Data
Processing in MEC via Online Learning. In IEEE International Conference on
Computer Communications (INFOCOM). 1809â€“1818.
[35] Tim Zeitz. 2022. Engineering Algorithms for Dynamic and Time-Dependent Route
Planning. Ph. D. Dissertation. Karlsruhe Institute of Technology, Germany.
[36] Zuowu Zheng, Changwang Zhang, Xiaofeng Gao, and Guihai Chen. 2022. HIEN:
Hierarchical Intention Embedding Network for Click-Through Rate Prediction. In
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR). 322â€“331.
 
871