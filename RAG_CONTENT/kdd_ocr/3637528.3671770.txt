Co-Neighbor Encoding Schema: A Light-cost Structure Encoding
Method for Dynamic Link Prediction
Ke Cheng
ckpassenger@buaa.edu.cn
CCSE Lab, Beihang University
Beijing, ChinaLinzhi Peng
lzpeng626@buaa.edu.cn
CCSE Lab, Beihang University
Beijing, ChinaJunchen Yeâˆ—
junchenye@buaa.edu.cn
School of Transportation Science and
Engineering, Beihang University
Beijing, China
Leilei Sun
leileisun@buaa.edu.cn
CCSE Lab, Beihang University
Beijing, ChinaBowen Du
dubowen@buaa.edu.cn
Zhongguancun Laboratory
School of Transportation Science and
Engineering, Beihang University
Beijing, China
ABSTRACT
Structure encoding has proven to be the key feature to distinguish-
ing links in a graph. However, Structure encoding in the temporal
graph keeps changing as the graph evolves, repeatedly computing
such features can be time-consuming due to the high-order sub-
graph construction. We develop the Co-Neighbor Encoding Schema
(CNES) to address this issue. Instead of recomputing the feature by
the link, CNES stores information in the memory to avoid redundant
calculations. Besides, unlike the existing memory-based dynamic
graph learning method that stores node hidden states, we introduce
a hashtable-based memory to compress the adjacency matrix for
efficient structure feature construction and updating with vector
computation in parallel. Furthermore, CNES introduces a Temporal-
Diverse Memory to generate long-term and short-term structure
encoding for neighbors with different structural information. A dy-
namic graph learning framework, Co-Neighbor Encoding Network
(CNE-N), is proposed using the aforementioned techniques. Exten-
sive experiments on thirteen public datasets verify the effectiveness
and efficiency of the proposed method.
CCS CONCEPTS
â€¢Information systems â†’Data mining.
KEYWORDS
Graph Neural Networks, Dynamic Graph, Network Embedding
ACM Reference Format:
Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, and Bowen Du. 2024. Co-
Neighbor Encoding Schema: A Light-cost Structure Encoding Method for
âˆ—Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $
https://doi.org/10.1145/3637528.3671770Dynamic Link Prediction. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671770
1 INTRODUCTION
Temporal graphs denote entities as nodes and represent their inter-
actions as edges with timestamps. This is a powerful way to model
the dynamics and evolution of complex systems over time. Re-
searchers have leveraged this approach and built many practical sys-
tems in a variety of real-world scenarios such as recommendations
in social networks [ 3,17,27], financial networks [ 6,23,34],user-
item interaction systems [ 13,18,38,39,41]. Dynamic graphs can
be represented in two ways: Discrete-time and Continuous-time.
Discrete-time Dynamic Graphs (DTDG) are sequences of static
graph snapshots captured at regular intervals. In contrast, Continuous-
time Dynamic Graphs (CTDG) are represented as timed lists of
events that include edge or node addition or deletion. CTDGs offer
better flexibility and performance than DTDGs and speed up com-
putation by estimating the full graph encoding with a neighborhood
subgraph.
According to whether to use heuristic features, existing CTDG
works can be divided into two main methods to encode the dynamic
graph: the vanilla method and the structure encoding-based method.
We discuss the difference between the two methods in Figure 1. The
vanilla method individually encodes the node neighborhood and
concentrates more on the encoding of temporal information. For
example, Jodie[ 17] models how a userâ€™s preferences evolve with an
RNN-based encoder that updates the node representation as the
user-item interaction occurs. GraphMixer[ 9], and TCL[ 35] discuss
the benefits of capturing the temporal correlation between nodes
in the historical interaction sequence with an MLPMixer encoder
or a Transformer encoder. However, these methods with only one
anchor node treat neighborhood structure as a way to aggregate
temporal information, ignoring the structural information itself,
which results in the loss of rich information in high-order structures
such as relative position information between nodes.
In contrast, the structure encoding-based method encodes the
link by generating query-specific encoding of the two end nodes[ 30,
 
421
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
Table 1: Summary of different CTDG models. We also list the time complexity (per sample, the first element before + stands
for sample complexity) and space complexity (per graph, âˆ’stands for no memory to store) for each algorithm, with hidden
dimension ğ‘‘, neighbor sample length ğ¿, neighbor sample hops ğ‘˜, nodes number ğ‘, node memory/feature dim ğ‘€, we assume all
the model shares a single layer full connected neural network as feature encoding model, detailed discussion can be found in
Section 2.2.
Mo
del Jo
die[17] TGN[25] CA
WN[37] DyGFormer[40] NAT[20] PINT[28] CNE-N
Strucute
Encoding âœ—
âœ— âœ”
âœ” âœ” âœ” âœ”
Lightcost
Construction âœ”
âœ— âœ—
âœ” âœ” âœ— âœ”
Parallel
Computation âœ”
âœ” âœ—
âœ— âœ” âœ” âœ”
T
emporal Diversity âœ—
âœ— âœ—
âœ— âœ— âœ— âœ”
Time ğ‘€
ğ‘‘ ğ¿ğ‘˜+ğ¿ğ‘˜ğ‘€ğ‘‘ğ¿ğ‘˜+ğ¿ğ‘˜ğ‘€
ğ‘‘ ğ¿+ğ¿(ğ¿+ğ‘€ğ‘‘)ğ¿ğ‘€ğ‘‘ ğ¿ğ‘˜+ğ¿ğ‘˜ğ‘ğ‘‘ğ¿+ğ¿
ğ‘€ğ‘‘
Space ğ‘
ğ‘€ ğ‘ğ‘€ -
- ğ‘ğ‘€ ğ‘2ğ‘
ğ‘€
Figure 1: An example to show the difference between meth-
ods w/o structure encoding, vanilla temporal graph learning
methods can not distinguish between link (u,v) and (v,w)
while structure encoding-based methods can show the dif-
ference by counting neighborhood overlap co-neighbors.
43], which is to encode one nodeâ€™s neighborhood condition on an-
other node, the method can increase model express ability by better
distinguish node neighborhood encoding with the heuristic struc-
ture feature like the number of common neighbors. For example,
CAWN[ 37] extracts several random walk sequences starting from
each end node and defines the relative position label as the node
position in the sequences. DyGFormer[ 40] extracts the first hop
neighbor sequence of the end nodes and computes the neighbor
co-occurrence encoding as the first-order relative position label.
The effectiveness of the structure encoding is largely influenced by
the size of the neighborhood to be encoded, a larger neighborhood
subgraph will lead to a higher encoding accuracy as well as time
complexity[28].
Previous methods for generating structure encoding have three
drawbacks. Firstly, constructing a neighborhood subgraph is
inefficient. Existing structure encoding is generated by construct-
ing a high-order subgraph assigning relative position labels to each
neighbor node, the irregular neighbor size of each node makes it
hard to construct the subgraph in parallel. Besides, due to the huge
memory cost to store the adjacency matrix, for each end node pair,
subgraphs are constructed with CPU-based neighbor samples. As a
result, subgraph construction and encoding are not computationally
efficient. Secondly, encoding with limited-sized neighborhood
will lose information. Due to the low efficiency of neighborsampling, most existing works only encode with a limited-sized
subset of the edge high-order neighborhoods or only the first-order
neighbors. As a result, this approach fails to capture the necessary
information needed for accurate modeling of the graph structure,
which leads to relatively poor performance. Thirdly, extracting
subgraph with single strategy ignores the diverse temporal
information of different time intervals. Existing works only
encode with a single anchor-induced subgraph due to high compu-
tation cost. However, a temporal triangle of a recently interacted
neighbor provides different information than a temporal triangle
of a historically interacted neighbor. Therefore, single subgraph
extraction ignores the diverse temporal information of different
time intervals.
To address the issues above, we propose a novel structure encod-
ing method named Co-Neighbor Encoding Schema (CNES). It uses
regular, low-order subgraphs with index querying and vector-based
parallel co-neighbor computation on GPU to generate structure
encoding. Each node in the temporal graph is assigned a hashtable-
based memory, acting as a compressed adjacent matrix that can be
stored on the GPU. In this way, we can replace CPU-based neigh-
bor sampling with GPU-based index querying to access high-order
structures. With hashtable-based memory, CNES determines the
common neighbors between the other end node and the neighbor
nodes through regular-sized vector computation in parallel for all
node pairs in a batch, instead of assigning relative position labels,
for the neighbor nodesâ€™ irregular high-order query-induced sub-
graphs. Unlike existing sample-based methods, CNES can encode
structures with a larger neighborhood size while maintaining high
efficiency, and reducing information loss in structure encoding
caused by the limited subgraph size. In addition, CNES introduces
aTemporal-Diverse Memory to generate structure encoding for
subgraphs at different time intervals.
We also propose a dynamic graph learning framework named
Co-Neighbor Encoding Network (CNE-N), a light-cost model for
dynamic link prediction. The proposed method is proven to be
effective and efficient both in theory (as shown in Table 1) and
experimentally.
Our contributions can be summarized as follows:
â€¢An efficient structure encoding method CNES is proposed.
The method accelerates structure encoding by compressing
the adjacency matrix to hashtable-based memory and com-
puting co-neighbors with vector-based parallel computation.
â€¢This paper generates a long-term and a short-term struc-
ture encoding for neighbors with temporal-diverse memory,
enabling the capture of temporal structural patterns.
 
422Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢Extensive experiments on 13 datasets show the effectiveness
and the efficiency of the proposed method. Previous models
equipped with the proposed structure encoding also result
in a better performance.
2 PRELIMINARIES
Definition 2.1. Dynamic Graph. We define the dynamic graph
as a sequence of edges Gğ‘‡whereGğ‘‡={ğ‘’1,Â·Â·Â·,ğ‘’ğ‘–,Â·Â·Â·,ğ‘’ğ‘›},0<
ğ‘¡1â‰¤Â·Â·Â·â‰¤ğ‘¡ğ‘–â‰¤Â·Â·Â·â‰¤ğ‘¡ğ‘›â‰¤ğ‘‡andğ‘’ğ‘–={ğ‘¢,ğ‘£,ğ‘¡},ğ‘¢andğ‘£âˆˆğ‘
represent the source node and the destination node of an edge, and
ğ‘¡means the edge occurs at timestamp ğ‘¡.ğ‘is the node set of the
dynamic graph. Each node ğ‘¢âˆˆğ‘has a feature vector ğ‘¥ğ‘(ğ‘¢)âˆˆğ‘…ğ‘‘ğ‘,
and each edge(ğ‘¢,ğ‘£,ğ‘¡)has an edge feature vector ğ‘¥ğ‘¡
ğ¸(ğ‘¢,ğ‘£)âˆˆğ‘…ğ‘‘ğ¸,
whereğ‘‘ğ‘andğ‘‘ğ¸are the dimensions of the node feature and link
feature.
Definition 2.2. k-order edge neighborhood subgraph. We
define the k-order edge neighborhood subgraph ğ‘†ğ‘¡
ğ‘˜(ğ‘¢,ğ‘£)as the
neighbors within ğ‘˜hopsğ‘ğ‘˜ğ‘¢andğ‘ğ‘˜ğ‘£of the two end nodes ğ‘¢andğ‘£
at the timestamp ğ‘¡. The subgraph contains the historical interactions
between the nodes.
Definition 2.3. Problem Formalization. Given a dynamic graph,
Gğ‘¡, Dynamic Link Prediction (DLP) task aims to learn the dynamic
representation â„ğ‘¡ğ‘¢andâ„ğ‘¡ğ‘£for the source node ğ‘¢and destination node
ğ‘£with the k-order edge neighborhood subgraph ğ‘†ğ‘¡
ğ‘˜(ğ‘¢,ğ‘£)and predict
the interact probability ğ‘ğ‘¡ğ‘¢,ğ‘£of the two end nodes at timestamp ğ‘¡.
2.1 Structure encoding
Table 2: Comparing of different designs of structure encoding
functions.
Metho
d Compr
ess Function Updation Involve Nodes
CA
WN time-w
eighted sample recompute u,v
DyGformer r
ecent sample recompute u,v
PINT -
u,v u,v, ğ‘ğ‘¢,ğ‘ğ‘£
NAT single
hash function u,v u,v
CNEN (ours) multiple
hash function u,v, ğ‘ğ‘¢,ğ‘ğ‘£ u,v,ğ‘ğ‘¢,ğ‘ğ‘£
Figure 2: Structure Encoding can be formalized as two func-
tions, a compress function to get the neighbor set from the
adjacency matrix, and a relation function to generate encod-
ing from the union of the two neighbor sets.
The structure encoding of the dynamic graph is to generate a
structure feature ğ‘¥ğ‘¡
ğ‘†(ğ‘¢,ğ‘£)âˆˆğ‘…ğ‘‘ğ‘†for each edge(ğ‘¢,ğ‘£,ğ‘¡)âˆˆGğ‘‡, where
ğ‘‘ğ‘†is the dimension of the structure feature. The structure encoding
ğ‘‰ğ‘¡
ğ‘˜(ğ‘¢,ğ‘£)represents the structural role of the source node ğ‘¢, thedestination node ğ‘£, and their neighbor nodes {ğ‘–âˆˆğ‘ğ‘¡ğ‘¢âˆªğ‘ğ‘¡ğ‘£}in the
ğ‘˜-order edge neighborhood subgraph.
As shown in Figure 2, we formalize the structure encoding as a
compress function ğ‘”ğ‘›(Â·)to get the neighbor set from the adjacency
matrix, and a relation function ğ‘”ğ‘ (Â·)to generate encoding from the
union of the two neighbor sets. We then give a simpler mathematical
form of structure encoding as follows:
ğ‘‰ğ‘¡
ğ‘˜(ğ‘¢,ğ‘£)=1
ğ‘€vutğ‘€âˆ‘ï¸
ğ‘–=0(Ë†ğ´ğ‘¡
ğ‘˜[ğ‘¢,ğ‘–]Â·Ë†ğ´ğ‘¡
ğ‘˜[ğ‘£,ğ‘–])2, (1)
whereË†ğ´ğ‘¡
ğ‘˜=ğ‘”ğ‘ (ğ´ğ‘¡
ğ‘˜)=ğ´ğ‘¡
ğ‘˜Ã—Mğ‘¡
ğ‘˜is the compressed adjacency
matrix,Mğ‘¡
ğ‘˜âˆˆğ‘…ğ‘Ã—ğ‘€is the compress matrix. We further summarize
the difference in the structure encoding methods in Table 2.
The existing methods face a trade-off between accuracy and com-
plexity. For example, CAWN and DyGformer achieve accuracy by
recomputing the neighborhood with every update, but this process
takes a lot of time. PINT stores the encoding without compression,
requiring large storage space. NAT compresses the neighborhood
using a single hash function and only updates the corresponding
two center nodes when a link occurs, which means it cannot gen-
erate accurate up-to-date neighborhood encodings.
Our method extends existing methods by balancing this trade-
off. We follow NATâ€™s approach to compress the matrix with a hash
function and propose temporal diversity encoding to avoid infor-
mation loss in hash conflicts. Additionally, we generate encoding
and update memory with the k-order edge neighborhood subgraph
instead of only the two nodes. As a result, CNEN achieves a balance
between accuracy and complexity.
2.2 Discussion of the Model Complexity
According to Table 1, we analyze the time and space complexity
of each model under the following assumptions: 1) each model
shares the same feature encoder, with hidden dimension ğ‘‘=50; 2)
subgraph-based model samples the same length ğ¿=20of sequence
for every order; 3) Memory-based model shares a same sized mem-
ory with dimension ğ‘€=100, no matter what the memory stores,
we assume the node feature size is also ğ‘€(because memory can be
regarded as an aggregation of neighbor features, this assumption
can help compare between models with/without memory); 4) The
dynamic graph contains ğ‘=1000 nodes.
JODIE[ 17] is a memory-based model. Therefore, the space com-
plexity to store the node memories is O(ğ‘ğ‘€); the model does not
depend on the neighbor sample and computes link probability only
with end nodes. Therefore, the time complexity to predict one link
isO(ğ‘€)+O(ğ‘€ğ‘‘)+O(ğ‘€)=O(ğ‘€ğ‘‘), where the twoO(ğ‘€)corre-
sponding to memory readout and update, O(ğ‘€ğ‘‘)is the complexity
to encode the link features. The model is the most efficient baseline
in theory and practice but may suffer from inductive issues (mem-
ory can not be generated for unseen nodes), and the model is not
equipped with structure encoding.
TGN[ 25] is a memory-based equipped with a graph neural net-
work to encode high-order temporal information. It shares the
same memory design as JODIE does. Therefore, the space com-
plexity to store the node memories is also O(ğ‘ğ‘€), the model ex-
tracts ağ‘˜order subgraph to gather information from neighbors;
 
423KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
in practice, ğ‘˜=1, so the time complexity to predict one link is
ğ¿ğ‘˜=1+ğ¿ğ‘˜=1Ã—(O(ğ‘€)+O(ğ‘€ğ‘‘)+O(ğ‘€))=O(ğ¿+ğ¿ğ‘€ğ‘‘). By neigh-
bor sampling, the model overcomes inductive issues, and the model
is not equipped with structure encoding.
CAWN[ 37] is a structure encoding model with the random-walk
neighbor sampler and does not depend on memory. The model
extracts several numbers of walk sequences (we assume it to be 1)
with sequence length ğ‘˜(for most datasets, ğ‘˜â‰¥2). For each node in
the sequence, the model assigns a relative position feature between
it and the walk head node; the sequence is later encoded with
an RNN-based model. Therefore the to predict one link is ğ¿ğ‘˜=2+
ğ¿ğ‘˜=2Ã—(O(ğ‘€ğ‘‘))=O(ğ¿2+ğ¿2ğ‘€ğ‘‘). The model is computationally
inefficient due to the complex and irregular subgraph construction.
NAT[ 20] is a memory-based model but replaces the single vector-
based memory in JODIE with dictionary-based memory. The repre-
sentation of each neighbor is stored in the corresponding position
without aggregation. Each memory contains ğ‘€1position for first-
order neighbors and ğ‘€2position for second-order neighbors. For
each neighbor, the model stores a representation of size ğ¹, the
method keeps(ğ‘€1+ğ‘€2)Ã—ğ¹â‰ˆğ‘€, and we assume ğ‘€1+ğ‘€2=ğ¿
(for example, the default setting of the model is ğ‘€1=32,ğ‘€2=
16,ğ¹=4). Therefore, the model does not depend on a neighbor
sample to construct a subgraph and compute the structure encod-
ing. As a result, the space complexity to store the node memories
isO((ğ‘€1+ğ‘€2)Ã—ğ¹)=O(ğ‘ğ‘€); the time complexity to predict
one link isO((ğ‘€1+ğ‘€2)ğ‘€ğ‘‘)+O(ğ‘€1ğ‘‘+ğ‘€2)=O(ğ¿ğ‘€ğ‘‘), where
O((ğ‘€1+ğ‘€2)ğ‘€ğ‘‘)is the feature encoding for the ğ‘€1+ğ‘€2neigh-
bors with feature dimension ğ‘€. The model extends JODIE with
structure encoding but still suffers from inductive issues.
DyGFormer[ 40] is a sequence-based model with first-order
neighbor extraction. The model computes neighbor co-occurrence
encoding by validating how often a neighbor node occurs in the
neighbor sequences of the two end nodes. Therefore, the time com-
plexity for structure encoding is O(ğ¿Ã—ğ¿)since it will compare
over the whole sequence for each node. The encoding is expensive
whenğ¿is large. DyGFormer extracts a longer historical interac-
tion sequence than other baseline models and reduces the time
complexity with the patch technique (assume the patch size is 16).
When encoding sequence with a Transformer encoder, the time
complexity isO(ğ¿2ğ‘€ğ‘‘)before using the patch technique and is
O((ğ¿/16)2(16ğ‘€)ğ‘‘)=O(ğ¿2ğ‘€ğ‘‘/16).
PINT[ 28] extends TGN with a link-wise relative position matrix
âˆˆğ‘…ğ‘Ã—ğ‘to store structure encoding between every pair of nodes,
for each node, the model readout all corresponding relative posi-
tions in the matrix. As a result, the space complexity to store the
node memories isO(ğ‘2); the model extracts neighbor nodes in 2
2-order subgraphs. Therefore, the time complexity to predict one
link isğ¿ğ‘˜=2+ğ¿ğ‘˜=2Ã—(O(ğ‘)+O(ğ‘ğ‘‘)+O(ğ‘))=O(ğ¿2+ğ¿2ğ‘ğ‘‘),
where the twoO(ğ‘)corresponding to memory readout and update,
O(ğ‘ğ‘‘)is the complexity to encode the link features.
CNE-N replace the representation-based node memory in TGN
to the neighbor set memory estimated by the hashtable, and the
computation of co-neighbor encoding is regular with time complex-
ityO(ğ‘€). Therefore, the model shares the same complexity with
TGN withğ‘˜=1(ğ‘€ğ¿+ğ‘€ğ‘†=ğ‘€). Where the space complexity tostore the node memories is O(ğ‘ğ‘€); the time complexity to predict
one link isğ¿+ğ¿Ã—(O(ğ‘€)+O(ğ‘€ğ‘‘)+O(ğ‘€))=O(ğ¿+ğ¿ğ‘€ğ‘‘).
3 METHODOLOGY
This section presents the details of the Co-Neighbor Encoding
Network(CNE-N). The framework of CNE-N is shown in Figure 3.
Figure 3: We develop a dynamic graph learning frame-
work Co-Neighbor Encoding Network (CNE-N), a light-cost
model for dynamic link prediction. The model generates
co-neighbor encoding with a hashtable-based memory and
low-order neighborhood subgraph extracting, and the mem-
ory updates by the graph evolves.
3.1 Memory for structure Encoding
We first introduce the hashtable-based memory in CNE-N to gen-
erate and update co-neighbor encoding, and long-short memory
to generate structure encoding in different time intervals. All the
computation can be done on GPU in parallel for multiple nodes in
a batch.
Co-neighbor Encoding. Letğ‘‰ğ‘¡
ğ‘˜(ğ‘¢,ğ‘–)be the common neigh-
bors for the two nodes ğ‘¢andğ‘–in theğ‘˜-order edge neighborhood
subgraph at timestamp ğ‘¡. We compute the co-neighbor encoding
[ğ‘‰ğ‘¡
ğ‘˜(ğ‘¢,ğ‘–),ğ‘‰ğ‘¡
ğ‘˜(ğ‘£,ğ‘–)]for each node ğ‘–in the subgraph ğ‘†ğ‘¡
ğ‘˜(ğ‘¢,ğ‘£,ğ‘¡)of the
link(ğ‘¢,ğ‘£). The count can be computed efficiently by first getting
the intersection of ğ‘†ğ‘¡
ğ‘˜(ğ‘¢)andğ‘†ğ‘¡
ğ‘˜(ğ‘–)(whereğ‘†ğ‘¡
ğ‘˜(ğ‘–)corresponding to
the node set of node ğ‘–â€™s k-order subgraph, a nodeâ€™s 0-order subgraph
is itself), and computing the cardinal number of it:
ğ‘‰ğ‘¡
ğ‘˜(ğ‘¢,ğ‘–)=|ğ‘†ğ‘¡
ğ‘˜(ğ‘¢)âˆ©ğ‘†ğ‘¡
ğ‘˜(ğ‘–)|, (2)
ğ‘‰ğ‘¡
ğ‘˜(ğ‘£,ğ‘–)=|ğ‘†ğ‘¡
ğ‘˜(ğ‘£)âˆ©ğ‘†ğ‘¡
ğ‘˜(ğ‘–)|. (3)
Hashtable-based Memory. In practice, we implement subgraph
node sets with hash tables. Such data structure can estimate a set
with a fixed-size sequence ğ»âˆˆğ‘…|ğ‘|Ã—ğ‘€, node-ids are mapped to the
row index with a hash function â„ğ‘ğ‘ â„(ğ‘–)=(ğ‘âˆ—ğ‘–)(modğ‘€)where
ğ‘€is the size of the hash table ğ»ğ‘¡. We store node ğ‘inğ‘–â€™s hash table
ğ»ğ‘¡
ğ‘–by settingğ»ğ‘¡
ğ‘–[â„ğ‘ğ‘ â„(ğ‘)]=ğ‘.
Encoding Computation. With hashtable ğ»ğ‘¡ğ‘¢,ğ»ğ‘¡ğ‘£andğ»ğ‘¡
ğ‘–, we can
efficiently compute the co-neighbor encoding [ğ‘‰ğ‘¡
ğ‘˜(ğ‘¢,ğ‘–),ğ‘‰ğ‘¡
ğ‘˜(ğ‘£,ğ‘–)]by
 
424Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
counting the same element at the corresponding location:
ğ‘‰ğ‘¡
ğ‘˜(ğ‘¢,ğ‘–)=ğ‘€âˆ’1âˆ‘ï¸
ğ‘š=0(ğ»ğ‘¡
ğ‘¢[ğ‘š]=ğ»ğ‘¡
ğ‘–[ğ‘š]), (4)
ğ‘‰ğ‘¡
ğ‘˜(ğ‘£,ğ‘–)=ğ‘€âˆ’1âˆ‘ï¸
ğ‘š=0(ğ»ğ‘¡
ğ‘£[ğ‘š]=ğ»ğ‘¡
ğ‘–[ğ‘š]). (5)
Memory Update. A dynamic graph keeps evolving; the neigh-
bors of a node are also time-varied, so whenever a new link occurs,
the neighbor set memory of the nodes in the link-induced subgraph
should be updated. The updating can be estimated by replacing
the neighbor set memory with the union of the memory of the
node and the updated neighbor ğ‘†ğ‘¡+1
ğ‘˜(ğ‘¢)=ğ‘†ğ‘¡
ğ‘˜(ğ‘¢,ğ‘¡)âˆªğ‘†ğ‘¡
ğ‘˜âˆ’1(ğ‘£,ğ‘¡),
whereğ‘†ğ‘¡
ğ‘˜âˆ’1(ğ‘£)stands for the ğ‘˜âˆ’1-hop subgraph node-set, the
nodes have become node ğ‘¢â€™s new updated neighbor in the ğ‘˜-hop
subgraph. In practice, the update via set union can be implemented
by hash_insert ğ»ğ‘¡ğ‘¢[â„ğ‘ğ‘ â„(ğ‘£)]â†ğ‘£.
Temporal-Diverse Memory. Hashtable-based memory stores
all the neighbors of a node without temporal information from
different intervals. When encoding a dynamic graph with a few
nodes like Social Evo., the neighbor storage of each node will even-
tually be almost the same. Co-neighbor encoding can no longer
provide useful structural information for link prediction. Besides, a
single subgraph to encode the structure encoding can not use the
evolving pattern in the dynamic structure, namely the temporal
issue. Therefore, we propose long-short memory, which contains
two parts of the hashtable: a short memory ğ»ğ‘ âˆˆğ‘…|ğ‘|Ã—ğ‘€ğ‘ with a
smaller hashtable size to store recent neighbor nodes and a long
memoryğ»ğ‘™âˆˆğ‘…|ğ‘|Ã—ğ‘€ğ‘™with a larger hashtable size to store more
historical neighbor nodes. The two hashtables individually gener-
ate structure information and update the memory by the sequence.
With the limited storage space, the short memory will replace neigh-
bors in the hashtable more frequently. Therefore, it can generate
co-neighbor encoding of the link short-term subgraph. We verify
the effectiveness of temporal-diverse memory and report the result
in Section 4.3.
3.2 Neural Encoding for Subgraph
In this section, we introduce how to predict future links with the
proposed model CNE-N.
Extracting edge neighborhood subgraph. We first extract the
neighbors of the two end nodes to construct the ğ‘˜-order edge neigh-
borhood subgraph with CPU-based neighbor sampling. In practice,
to balance the trade-off between effectiveness and efficiency, we
follow most existing works to extract the first-hop subgraph as
historically interacted neighbors sequence with neighbor sampling,
namelyğ‘†ğ‘¡
1(ğ‘¢,ğ‘£), given an interaction (ğ‘¢,ğ‘£,ğ‘¡), for the source node ğ‘¢
and the destination node ğ‘£, we obtain the subgraphs that involves
historical interactions of ğ‘¢andğ‘£before timestamp ğ‘¡(including them-
selves), which are denoted by ğ‘†ğ‘¡
1(ğ‘¢)={(ğ‘¢,ğ‘–,Ë†ğ‘¡)|Ë†ğ‘¡<ğ‘¡,ğ‘–âˆˆğ‘1ğ‘¢}and
ğ‘†ğ‘¡
1(ğ‘£)={(ğ‘£,ğ‘–,Ë†ğ‘¡)|Ë†ğ‘¡<ğ‘¡,ğ‘–âˆˆğ‘1ğ‘£}. In practice, to avoid the influence
of the different lengths of historical interaction sequences, we pad
each sequence with a fixed size ğ‘™ğ‘ . As a result,|ğ‘†ğ‘¡
1(ğ‘¢)|=|ğ‘†ğ‘¡
1(ğ‘£)|=ğ‘™ğ‘ .
Constructing Structure Encoding with Co-neighbor Encod-
ing Schema. With the proposed hashtable-based memory, we can
efficiently compute the co-neighbor encoding between neighbornodes in the source node ğ‘¢neighborhood and the destination node
ğ‘£neighborhood. Mathematically, given sequence ğ‘†ğ‘¡
1(ğ‘¢)andğ‘†ğ‘¡
1(ğ‘¢),
we can reach the stored neighbor memory of each corresponding
node in the sequence as ğ‘†ğ‘¡
1,ğ»âˆ—(ğ‘¢)âˆˆğ‘…ğ‘™ğ‘ Ã—ğ‘€andğ‘†ğ‘¡
1,ğ»âˆ—(ğ‘£)âˆˆğ‘…ğ‘™ğ‘ Ã—ğ‘€,
and the neighbor hash table for source node ğ‘¢and destination node
ğ‘£asğ»âˆ—,ğ‘¡
ğ‘¢andğ»âˆ—,ğ‘¡
ğ‘£, whereâˆ—corresponding to ğ‘™for long memory and
ğ‘ for short memory. Then we compute the co-neighbor encoding of
the neighbor list ğ‘‹ğ‘¡
ğ‘¢,ğ¶,âˆ—âˆˆğ‘…ğ‘™ğ‘ Ã—2=[ğ‘‰âˆ—,ğ‘¡
ğ‘˜(ğ‘¢,ğ‘–),ğ‘‰âˆ—,ğ‘¡
ğ‘˜(ğ‘£,ğ‘–)]{ğ‘–âˆˆğ‘†ğ‘¡
ğ‘˜(ğ‘¢)}
andğ‘‹ğ‘¡
ğ‘£,ğ¶,âˆ—âˆˆğ‘…ğ‘™ğ‘ Ã—2=[ğ‘‰âˆ—,ğ‘¡
ğ‘˜(ğ‘£,ğ‘—),ğ‘‰âˆ—,ğ‘¡
ğ‘˜(ğ‘¢,ğ‘—)]{ğ‘—âˆˆğ‘†ğ‘¡
ğ‘˜(ğ‘£)}with Equa-
tion 4.
For example, if the historical node for ğ‘¢andğ‘£are[ğ‘¢,ğ‘,ğ‘]and
[ğ‘£,ğ‘,ğ‘¢], givenğ‘‰1(ğ‘¢,ğ‘)=4,ğ‘‰1(ğ‘¢,ğ‘£)=5andğ‘‰1(ğ‘£,ğ‘)=1, the struc-
ture encoding for ğ‘¢andğ‘£are[(ğ‘€,5),(4,1),(4,1)]and[(ğ‘€,5),(1,4),
(5,ğ‘€)], whereğ‘€is the size of the hashtable.
The co-neighbor encoding scheme is light cost and can be easily
integrated into dynamic graph learning methods for better results.
For each node, the method generates structure encoding by count-
ing triangles between up to 1+ğ‘™ğ‘ +ğ‘™ğ‘ ğ‘€nodes in parallel, which
is larger than most of the existing models. For example, the DyG-
Former only considers first-hop neighbors with up to 1+ğ‘™ğ‘ nodes,
and CAWN considers 1+ğ‘™ğ‘ +ğ‘™2ğ‘ nodes but with high sample complex-
ity. However, due to the high frequency of multiple-time interaction
in the temporal graph, the sample-based model often repeatedly
computes structure encoding between the same node pairs, which
makes them consider fewer triangles than CNES with a hashtable-
based memory to store neighbors. We demonstrate its efficiency in
Section 5.
Encoding Side information and Time Intervals. As discussed
in Section 2, there are some dynamic graphs with side information
like node feature or edge feature, we encode those features in node
ğ‘¢â€™s historical interaction sequence as ğ‘‹ğ‘¡
ğ‘¢,ğ‘âˆˆğ‘…ğ‘™ğ‘ Ã—ğ‘‘ğ‘andğ‘‹ğ‘¡
ğ‘¢,ğ¸âˆˆ
ğ‘…ğ‘™ğ‘ Ã—ğ‘‘ğ¸. As for the time interval between the link and historical
intervals, we follow TGAT[ 37] and encode the time intervals Î”Ë†ğ‘¡=
ğ‘¡âˆ’Ë†ğ‘¡with Fourier Time encoding as ğ‘‹ğ‘¡
ğ‘¢,ğ‘‡âˆˆğ‘…ğ‘™ğ‘ Ã—ğ‘‘ğ‘‡:
ğ‘‹ğ‘¡
ğ‘¢
,ğ‘‡=âˆšï¸ƒ
1
ğ‘‘ğ‘‡[ğ‘
ğ‘œğ‘ (Î”Ë†ğ‘¡ğ‘¤1),ğ‘ ğ‘–ğ‘›(Î”Ë†ğ‘¡ğ‘¤2),...,ğ‘ğ‘œğ‘ (Î”Ë†ğ‘¡ğ‘¤ğ‘‘ğ‘‡âˆ’1),ğ‘ ğ‘–ğ‘›(Î”Ë†ğ‘¡ğ‘¤ğ‘‘ğ‘‡)],(6)
whereğ‘¤1,Â·,ğ‘¤ğ‘‘ğ‘‡âˆˆğ‘…1Ã—1are trainable parameters to encode time
interval.
Fusing Features. We first assign each feature to the same di-
mensionğ‘‘with several one-layer full connected neural networks
ğ‘“(Â·), and concatenated them together:
ğ‘ğ‘¡
ğ‘¢=ğ‘“ğ‘(ğ‘‹ğ‘¡
ğ‘¢,ğ‘)âˆ¥ğ‘“ğ¸(ğ‘‹ğ‘¡
ğ‘¢,ğ¸)âˆ¥ğ‘“ğ‘‡(ğ‘‹ğ‘¡
ğ‘¢,ğ‘‡)âˆ¥ğ‘“ğ¶,ğ‘™(ğ‘‹ğ‘¡
ğ‘¢,ğ¶,ğ‘™)âˆ¥ğ‘“ğ¶,ğ‘ (ğ‘‹ğ‘¡
ğ‘¢,ğ¶,ğ‘ ),
(7)
ğ‘ğ‘¡
ğ‘£=ğ‘“ğ‘(ğ‘‹ğ‘¡
ğ‘£,ğ‘)âˆ¥ğ‘“ğ¸(ğ‘‹ğ‘¡
ğ‘£,ğ¸)âˆ¥ğ‘“ğ‘‡(ğ‘‹ğ‘¡
ğ‘£,ğ‘‡)âˆ¥ğ‘“ğ¶,ğ‘™(ğ‘‹ğ‘¡
ğ‘£,ğ¶,ğ‘™)âˆ¥ğ‘“ğ¶,ğ‘ (ğ‘‹ğ‘¡
ğ‘¢,ğ¶,ğ‘ ),
(8)
Then, we employ a feature fusion encoder to encode the historical
interaction sequence for subgraph-based embedding. The Layer
Normalization (LN) is employed after every block. The input is
processed via the feature fusion encoder to fuse the information
from different features:
ğ‘ğ‘¡,ğ‘™
ğ‘¢=ğ¿ğ‘(ğ‘ğ‘¡,ğ‘™âˆ’1
ğ‘¢ğ‘Šğ‘™
ğ‘“ğ‘¢+ğ‘ğ‘™
ğ‘“ğ‘¢), (9)
ğ‘ğ‘¡,ğ‘™
ğ‘£=ğ¿ğ‘(ğ‘ğ‘¡,ğ‘™âˆ’1
ğ‘£ğ‘Šğ‘™
ğ‘“ğ‘¢+ğ‘ğ‘™
ğ‘“ğ‘¢), (10)
 
425KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
whereğ‘Šğ‘™
ğ‘“ğ‘¢âˆˆğ‘…4ğ‘‘Ã—4ğ‘‘,ğ‘ğ‘™
ğ‘“ğ‘¢âˆˆğ‘…4ğ‘‘are trainable parameters at the
ğ‘™-th layer of the encoder ğ‘“ğ‘“ğ‘¢={âˆ—}ğ‘Šğ‘“ğ‘¢+ğ‘ğ‘“ğ‘¢. The output of the
L-th layer is ğ‘ğ‘¡,ğ¿
ğ‘¢andğ‘ğ‘¡,ğ¿
ğ‘£âˆˆğ‘…ğ‘™ğ‘ Ã—5ğ‘‘.
Generating Node Representation. The time-aware represen-
tations of node ğ‘¢andğ‘£at timestamp ğ‘¡,â„ğ‘¡ğ‘¢âˆˆğ‘…ğ‘‘ğ‘œandâ„ğ‘¡ğ‘£âˆˆğ‘…ğ‘‘ğ‘œğ‘¢ğ‘¡are
derived by taking the mean pooling of their related representations
inğ‘ğ‘¡,ğ¿with an output layer ğ‘“ğ‘œ={âˆ—}ğ‘Šğ‘œ+ğ‘ğ‘œ:
â„ğ‘¡
ğ‘¢=(1
ğ‘™ğ‘ âˆ‘ï¸
(ğ‘ğ‘¡,ğ¿
ğ‘¢))ğ‘Šğ‘œ+ğ‘ğ‘œ, (11)
â„ğ‘¡
ğ‘£=(1
ğ‘™ğ‘ âˆ‘ï¸
(ğ‘ğ‘¡,ğ¿
ğ‘£))ğ‘Šğ‘œ+ğ‘ğ‘œ, (12)
whereğ‘Šğ‘œâˆˆğ‘…5ğ‘‘Ã—ğ‘‘ğ‘œ,ğ‘ğ‘œâˆˆğ‘…ğ‘‘ğ‘œare trainable parameters at the
output layer.
Predicting Link Probability. The predicted probability of the
linkâ„ğ‘¡ğ‘¢,ğ‘£âˆˆğ‘…ğ‘‘ğ‘œare merged with a merge layer ğ‘“ğ‘šas:
ğ‘ğ‘¡
ğ‘¢,ğ‘£=ğœ((â„ğ‘¡
ğ‘¢,â„ğ‘¡
ğ‘£)ğ‘Šğ‘š+ğ‘ğ‘š), (13)
whereğœis the sigmoid function, ğ‘Šğ‘šâˆˆğ‘…2ğ‘‘ğ‘œÃ—1,ğ‘ğ‘šâˆˆğ‘…1are train-
able parameters at the merge layer. We train the model with the
Binary Cross-entropy loss between the positive link and the nega-
tive sampled node pair [ğ‘¢,ğ‘›ğ‘’ğ‘”]with the random sample strategy:
ğ‘™ğ‘œğ‘ ğ‘ =âˆ’(ğ‘™ğ‘œğ‘”(ğ‘ğ‘¡
ğ‘¢,ğ‘£)+ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ğ‘¡
ğ‘¢,ğ‘›ğ‘’ğ‘”)). (14)
Updating Node Memories in the subgraph. Finally, we up-
date the node co-neighbor memory with the new link-induced
subgraph. Firstly, we update ğ‘¢andğ‘£â€™s memory with the 1-order
subgraphğ»âˆ—ğ‘¢[â„ğ‘ğ‘ â„(ğ‘£)] â†ğ‘£,ğ»âˆ—ğ‘£[â„ğ‘ğ‘ â„(ğ‘¢)] â†ğ‘¢, whereâˆ—corre-
sponding to ğ‘™for long memory and ğ‘ for short memory, and then
update the memory with neighbor nodes in the 2-order subgraph
ğ»âˆ—ğ‘¢[â„ğ‘ğ‘ â„(ğ‘—)]â†ğ‘—,{ğ‘—âˆˆğ‘†ğ‘¡
1(ğ‘£)},ğ»âˆ—ğ‘£[â„ğ‘ğ‘ â„(ğ‘–)]â†ğ‘–,{ğ‘–âˆˆğ‘†ğ‘¡
1(ğ‘¢)}, and
we update the memory of the neighbor nodes ğ»âˆ—
ğ‘–[â„ğ‘ğ‘ â„(ğ‘£)] â†
ğ‘£,{ğ‘–âˆˆğ‘†ğ‘¡
1(ğ‘¢)},ğ»âˆ—
ğ‘—[â„ğ‘ğ‘ â„(ğ‘¢)]â†ğ‘¢,{ğ‘—âˆˆğ‘†ğ‘¡
1(ğ‘£)}. We verify the effec-
tiveness of updating the neighbor node memory and updating with
the 2-order subgraph update in Section 4.3.
4 EXPERIMENTS
4.1 Experimental Settings
In this section, we introduce the experimental settings to examine
the effectiveness and efficiency of the proposed model.
Datasets. We test CNE-N and baseline models on thirteen com-
monly used datasets: Wikipedia, Reddit, MOOC, LastFM, Enron,
Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote,
and Contact, which are collected by Edgebank[ 22]. Details of the
datasets are shown in Appendix A.1.
Evaluation Metrics. We follow TGN[ 25] to evaluate the model
performance on the dynamic link prediction task, which is to pre-
dict whether two nodes interact with each other at a certain time.
The task contains two settings: transductive setting predicts future
links between previously observed nodes in the training process
and inductive setting tests link prediction between unseen nodes.
Average Precision (AP) and Area Under the Receiver Operating
Characteristic Curve (AUC-ROC) are adopted as the evaluation
metrics. We use the random negative sample strategy to generatenegative links. We considered a 70%-15%-15% (train-val-test) split
for each dataset.
Baselines. We compare CNE-N with ten popular continuous-
time dynamic graph learning baselines, including two memory-
based models, JODIE[17] and DyRep[16]. Two graph convolution-
based models, TGAT[ 37] and TGN[ 25]; four sequence-based models,
EdgeBank[ 22], TCL[ 35], GraphMixer[ 9], and DyGFormer[ 40]; and
two structure encoding-based models, NAT and CAWN[ 36]. Details
of the baseline models are listed in Appendix A.2. We test our
model and baselines under DyGLib[ 40]. The code is available in
https://github.com/ckpassenger/DyGLib_CNEN/tree/CNEN.
Model Configurations . For baselines, we follow the result re-
ported by DyGFormer[ 40]. As for CNE-N, We set the size of the
long memory hashtable ğ‘€ğ‘™to 64 and set the short memory size ğ‘€ğ‘ 
to 16. We search the historical interaction sequence with a maxi-
mum length ğ‘™ğ‘ in[4,10,20,32,64,100]. For all layers in the model,
we set their hidden dimension ğ‘‘ğ‘‡,ğ‘‘,ğ‘‘ğ‘œ, andğ‘‘ğ‘što 50.
Implementation Details. We employ the Adam optimizer with
a learning rate of 0.0001 in all our experiments. The batch size of
the input data is fixed at 200 for training, validation, and testing.
The dropout rate was set at 0.1 for all experiments and all datasets.
We conduct the experiments five times and took the average of the
results. The experiments are carried out on a Windows machine
with an AMD RYZEN 7 5800 CPU @ 3.30GHz having 6 physical
cores. The GPU device used is an NVIDIA GTX3060 with a memory
capacity of 12 GB.
4.2 Performance Comparison
In this section, we compare the performance between our method
and baselines.
Result. We report the performance of different methods on
the AP/AUC-ROC metric for the transductive setting of dynamic
link prediction in Table 3, and in the inductive setting in ??. Note
that EdgeBank[ 22] can be only evaluated for transductive dynamic
link prediction, so its results under the inductive setting are not
presented. For some of the results, we follow the results reported
by their original papers. We also test model scalability in large-
scale TGB dataset, please refer to Appendix A.3 for more discussion.
Furthermore, the performance and time per epoch for different
dynamic learning methods are presented in Figure 4. From Table 3,
and Figure 4, we have two main observations.
(1) In all datasets, except for Can. Parl. and Social Evo., CNE-N
outperforms other baselines. The co-neighbor encoding performs
best in social or interaction datasets with sufficiently large neigh-
bors. This result confirms the effectiveness of our proposed co-
neighbor encoding schema. The structure encoding helps the model
differentiate between new links among highly related and unrelated
nodes. However, Can. Parl is a policy dataset and highly relies on
first-order neighbor modeling[ 40]. Therefore, DyGFormer performs
well with long neighbor sequence. CNE-N fails to generate a precise
encoding because the model depends on the number of common
neighbors instead of individual neighbor information. Additionally,
Social Evo. has very few nodes, leading to all hashtable-based mem-
ory being the same, rendering co-neighbor encoding useless. This
inspired us to develop temporal-diverse memory to encode recent
high-frequency interacting neighbors.
 
426Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: AP/AUC-ROC for transductive and inductive dynamic link prediction.
Metrics Datasets JODIE
DyRep TGAT TGN CAWN EdgeBank TCL GraphMixer NAT DyGFormer CNE-N
T
rans-APWikip
edia 96.50 Â±0.14
94.86 Â±0.06 96.94 Â±0.06 98.45 Â±0.06 98.76 Â±0.03 90.37 Â±0.00 96.47 Â±0.16 97.25 Â±0.03 97.50 Â±0.04 99.03 Â±0.02 99.09 Â±0.04
Re
ddit 98.31 Â±0.14
98.22 Â±0.04 98.52 Â±0.02 98.63 Â±0.06 99.11 Â±0.01 94.86 Â±0.00 97.53 Â±0.02 97.31 Â±0.01 99.10 Â±0.21 99.22 Â±0.01 99.22 Â±0.01
MOOC 80.23 Â±2.44
81.97 Â±0.49 85.84 Â±0.15 89.15 Â±1.60
80.15 Â±0.25 57.97 Â±0.00 82.38 Â±0.24 82.78 Â±0.15 87.21 Â±0.63 87.52 Â±0.49 94.18 Â±0.07
LastFM 70.85 Â±2.13
71.92 Â±2.21 73.42 Â±0.21 77.07 Â±3.97 86.99 Â±0.06 79.29 Â±0.00 67.27 Â±2.16 75.61 Â±0.24 88.57 Â±1.76 93.00 Â±0.12 93.55 Â±0.12
Enr
on 84.77 Â±0.30
82.38 Â±3.36 71.12 Â±0.97 86.53 Â±1.11 89.56 Â±0.09 83.53 Â±0.00 79.70 Â±0.71 82.25 Â±0.16 90.81 Â±0.31 92.47 Â±0.12 92.48 Â±0.10
So
cial Evo. 89.89 Â±0.55
88.87 Â±0.30 93.16 Â±0.17 93.57 Â±0.17 84.96 Â±0.09 74.95 Â±0.00 93.13 Â±0.16 93.37 Â±0.07 91.23 Â±0.37 94.73 Â±0.01 94.60 Â±0.03
UCI 89.43 Â±1.09
65.14 Â±2.30 79.63 Â±0.70 92.34 Â±1.04 95.18 Â±0.06 76.20 Â±0.00 89.57 Â±1.63 93.55 Â±0.57 94.26 Â±0.37 95.79 Â±0.17 96.85 Â±0.08
F
lights 95.60 Â±1.73
95.29 Â±0.72 94.03 Â±0.18 97.95 Â±0.14 98.51 Â±0.01 89.35 Â±0.00 91.23 Â±0.02 90.99 Â±0.05 97.66 Â±0.80 98.91 Â±0.01 98.92 Â±0.01
Can.
Parl. 69.26 Â±0.31
66.54 Â±2.76 70.73 Â±0.72 70.88 Â±2.34 69.82 Â±2.34 64.55 Â±0.00 68.67 Â±2.67 77.04 Â±0.46 83.83 Â±1.20 97.36 Â±0.45 86.09 Â±0.14
US
Legis. 75.05 Â±1.52
75.34 Â±0.39 68.52 Â±3.16 75.99 Â±0.58 70.58 Â±0.48 58.39 Â±0.00 69.59 Â±0.48 67.74 Â±1.02 77.56 Â±0.21 71.11 Â±0.59 78.69 Â±0.12
UN
Trade 64.94 Â±0.31
63.21 Â±0.93 61.47 Â±0.18 65.03 Â±1.37 65.39 Â±0.12 60.41 Â±0.00 62.21 Â±0.03 62.61 Â±0.27 72.32 Â±0.69 66.46 Â±1.29 76.92 Â±0.06
UN
Vote 63.91 Â±0.81
62.81 Â±0.80 52.21 Â±0.98 65.72 Â±2.17 52.84 Â±0.10 58.49 Â±0.00 51.90 Â±0.30 52.11 Â±0.16 69.70 Â±0.49 55.55 Â±0.42 66.40 Â±0.12
Contact 95.31 Â±1.33
95.98 Â±0.15 96.28 Â±0.09 96.89 Â±0.56 90.26 Â±0.28 92.58 Â±0.00 92.44 Â±0.12 91.92 Â±0.03 97.25 Â±0.33 98.29 Â±0.01 98.40 Â±0.02
A
vg. Rank 7.08
7.85 7.62 4.38 5.92 9.61 8.76 7.77 3.31 2.53 1.38
T
rans-AUCWikip
edia 96.33 Â±0.07
94.37 Â±0.09 96.67 Â±0.07 98.37 Â±0.07 98.54 Â±0.04 90.78 Â±0.00 95.84 Â±0.18 96.92 Â±0.03 96.72 Â±0.21 98.91 Â±0.02 99.01 Â±0.05
Re
ddit 98.31 Â±0.05
98.17 Â±0.05 98.47 Â±0.02 98.60 Â±0.06 99.01 Â±0.01 95.37 Â±0.00 97.42 Â±0.02 97.17 Â±0.02 99.02 Â±0.10 99.15 Â±0.01 99.15 Â±0.01
MOOC 83.81 Â±2.09
85.03 Â±0.58 87.11 Â±0.19 91.21 Â±1.15 80.38 Â±0.26 60.86 Â±0.00 83.12 Â±0.18 84.01 Â±0.17 88.38 Â±0.71
87.91 Â±0.58 95.69 Â±0.07
LastFM 70.49 Â±1.66
71.16 Â±1.89 71.59 Â±0.18 78.47 Â±2.94 85.92 Â±0.10 83.77 Â±0.00 64.06 Â±1.16 73.53 Â±0.12 86.94 Â±2.29 93.05 Â±0.10 93.13 Â±0.11
Enr
on 87.96 Â±0.52
84.89 Â±3.00 68.89 Â±1.10 88.32 Â±0.99 90.45 Â±0.14 87.05 Â±0.00 75.74 Â±0.72 84.38 Â±0.21 92.02 Â±0.32 93.33 Â±0.13 93.12 Â±0.09
So
cial Evo. 92.05 Â±0.46
90.76 Â±0.21 94.76 Â±0.16 95.39 Â±0.17 87.34 Â±0.08 81.60 Â±0.00 94.84 Â±0.17 95.23 Â±0.07 93.22 Â±0.13 96.30 Â±0.01 96.24 Â±0.14
UCI 90.44 Â±0.49
68.77 Â±2.34 78.53 Â±0.74 92.03 Â±1.13 93.87 Â±0.08 77.30 Â±0.00 87.82 Â±1.36 92.52 Â±0.67 93.02 Â±0.48 94.49 Â±0.26 96.03 Â±0.08
F
lights 96.21 Â±1.42
95.95 Â±0.62 94.13 Â±0.17 98.22 Â±0.13 98.45 Â±0.01 90.23 Â±0.00 91.21 Â±0.02 91.13 Â±0.01 97.32 Â±0.34 98.93 Â±0.01 98.96 Â±0.02
Can.
Parl. 78.21 Â±0.23
73.35 Â±3.67 75.69 Â±0.78 76.99 Â±1.80 75.70 Â±3.27 64.14 Â±0.00 72.46 Â±3.23 83.17 Â±0.53 87.70 Â±1.37 97.76 Â±0.41 89.71 Â±0.07
US
Legis. 82.85 Â±1.07
82.28 Â±0.32 75.84 Â±1.99 83.34 Â±0.43 77.16 Â±0.39 62.57 Â±0.00 76.27 Â±0.63 76.96 Â±0.79 84.68 Â±0.35 77.90 Â±0.58 84.14 Â±0.63
UN
Trade 69.62 Â±0.44
67.44 Â±0.83 64.01 Â±0.12 69.10 Â±1.67 68.54 Â±0.18 66.75 Â±0.00 64.72 Â±0.05 65.52 Â±0.51 76.76 Â±0.81
70.20 Â±1.44 78.57 Â±0.07
UN
Vote 68.53 Â±0.95
67.18 Â±1.04 52.83 Â±1.12 69.71 Â±2.65 53.09 Â±0.22 62.97 Â±0.00 51.88 Â±0.36 52.46 Â±0.27 74.44 Â±2.01 57.12 Â±0.62 69.52 Â±0.34
Contact 96.66 Â±0.89
96.48 Â±0.14 96.95 Â±0.08 97.54 Â±0.35 89.99 Â±0.34 94.34 Â±0.00 94.15 Â±0.09 93.94 Â±0.02 97.64 Â±0.58 98.53 Â±0.01 98.77 Â±0.01
A
vg. Rank 6.31
7.69 7.92 4.23 6.15 9.31 9.23 7.69 3.15 2.85 1.31
Ind-
APWikip
edia 94.82 Â±0.20
92.43 Â±0.37 96.22 Â±0.07 97.83 Â±0.04 98.24 Â±0.03 - 96.22 Â±0.17 96.65 Â±0.02 95.40 Â±0.04 98.59 Â±0.03 98.37 Â±0.03
Re
ddit 96.50 Â±0.13
96.09 Â±0.11 97.09 Â±0.04 97.50 Â±0.07 98.62 Â±0.01 - 94.09 Â±0.07 95.26 Â±0.02 98.56 Â±0.21 98.84 Â±0.02 98.78 Â±0.01
MOOC 79.63 Â±1.92
81.07 Â±0.44 85.50 Â±0.19 89.04 Â±1.17
81.42 Â±0.24 - 80.60 Â±0.22 81.41 Â±0.21 83.59 Â±1.58 86.96 Â±0.43 91.89 Â±0.31
LastFM 81.61 Â±3.82
83.02 Â±1.48 78.63 Â±0.31 81.45 Â±4.29 89.42 Â±0.07 - 73.53 Â±1.66 82.11 Â±0.42 86.87 Â±1.95 94.23 Â±0.09 94.64 Â±0.12
Enr
on 80.72 Â±1.39
74.55 Â±3.95 67.05 Â±1.51 77.94 Â±1.02 86.35 Â±0.51 - 76.14 Â±0.79 75.88 Â±0.48 89.03 Â±0.83 89.76 Â±0.34 89.66 Â±0.22
So
cial Evo. 91.96 Â±0.48
90.04 Â±0.47 91.41 Â±0.16 90.77 Â±0.86 79.94 Â±0.18 - 91.55 Â±0.09 91.86 Â±0.06 91.22 Â±0.32 93.14 Â±0.04 93.29 Â±0.37
UCI 79.86 Â±1.48
57.48 Â±1.87 79.54 Â±0.48 88.12 Â±2.05 92.73 Â±0.06 - 87.36 Â±2.03 91.19 Â±0.42 87.30 Â±0.15 94.54 Â±0.12 95.03 Â±0.16
F
lights 94.74 Â±0.37
92.88 Â±0.73 88.73 Â±0.33 95.03 Â±0.60 97.06 Â±0.02 - 83.41 Â±0.07 83.03 Â±0.05 96.59 Â±1.67 97.79 Â±0.02 97.72 Â±0.04
Can.
Parl. 53.92 Â±0.94
54.02 Â±0.76 55.18 Â±0.79 54.10 Â±0.93 55.80 Â±0.69 - 54.30 Â±0.66 55.91 Â±0.82 60.62 Â±2.06 87.74 Â±0.71 68.31 Â±0.59
US
Legis. 54.93 Â±2.29
57.28 Â±0.71 51.00 Â±3.11 58.63 Â±0.37
53.17 Â±1.20 - 52.59 Â±0.97 50.71 Â±0.76 57.54 Â±0.80 54.28 Â±2.87 59.44 Â±0.44
UN
Trade 59.65 Â±0.77
57.02 Â±0.69 61.03 Â±0.18 58.31 Â±3.15 65.24 Â±0.21 - 62.21 Â±0.12 62.17 Â±0.31 69.29 Â±1.59 64.55 Â±0.62 66.58 Â±0.27
UN
Vote 56.64 Â±0.96
54.62 Â±2.22 52.24 Â±1.46 58.85 Â±2.51 49.94 Â±0.45 - 51.60 Â±0.97 50.68 Â±0.44 66.35 Â±4.06
55.93 Â±0.39 69.71 Â±0.48
Contact 94.34 Â±1.45
92.18 Â±0.41 95.87 Â±0.11 93.82 Â±0.99 89.55 Â±0.30 - 91.11 Â±0.12 90.59 Â±0.05 96.79 Â±0.37 98.03 Â±0.02 98.02 Â±0.05
A
vg. Rank 7.23
8.46 7.62 6.00 6.08 - 8.38 7.77 3.62 2.69 1.46
Ind-
AUCWikip
edia 94.33 Â±0.27
91.49 Â±0.45 95.90 Â±0.09 97.72 Â±0.03 98.03 Â±0.04 - 95.57 Â±0.20 96.30 Â±0.04 94.74 Â±0.44 98.48 Â±0.03 98.23 Â±0.01
Re
ddit 96.52 Â±0.13
96.05 Â±0.12 96.98 Â±0.04 97.39 Â±0.07 98.42 Â±0.02 - 93.80 Â±0.07 94.97 Â±0.05 97.99 Â±0.52 98.71 Â±0.01 98.62 Â±0.01
MOOC 83.16 Â±1.30
84.03 Â±0.49 86.84 Â±0.17 91.24 Â±0.99
81.86 Â±0.25 - 81.43 Â±0.19 82.77 Â±0.24 6.13 Â±3.55 87.62 Â±0.51 92.76 Â±0.29
LastFM 81.13 Â±3.39
82.24 Â±1.51 76.99 Â±0.29 82.61 Â±3.15 87.82 Â±0.12 - 70.84 Â±0.85 80.37 Â±0.18 83.07 Â±2.32 94.08 Â±0.08 94.38 Â±0.08
Enr
on 81.96 Â±1.34
76.34 Â±4.20 64.63 Â±1.74 78.83 Â±1.11 87.02 Â±0.50 - 72.33 Â±0.99 76.51 Â±0.71 89.92 Â±0.72 90.69 Â±0.26 90.18 Â±0.15
So
cial Evo. 93.70 Â±0.29
91.18 Â±0.49 93.41 Â±0.19 93.43 Â±0.59 84.73 Â±0.27 - 93.71 Â±0.18 94.78 Â±1.00 92.11 Â±0.07 95.29 Â±0.03 95.16 Â±0.14
UCI 78.80 Â±0.94
58.08 Â±1.81 77.64 Â±0.38 86.68 Â±2.29 90.40 Â±0.11 - 84.49 Â±1.82 89.30 Â±0.57 83.81 Â±1.28 92.63 Â±0.13 93.34 Â±0.17
F
lights 95.21 Â±0.32
93.56 Â±0.70 88.64 Â±0.35 95.92 Â±0.43 96.86 Â±0.02 - 82.48 Â±0.01 82.27 Â±0.06 96.36 Â±1.51 97.80 Â±0.02 97.82 Â±0.04
Can.
Parl. 53.81 Â±1.14
55.27 Â±0.49 56.51 Â±0.75 55.86 Â±0.75 58.83 Â±1.13 - 55.83 Â±1.07 58.32 Â±1.08 61.62 Â±2.50 89.33 Â±0.48 70.22 Â±0.77
US
Legis. 58.12 Â±2.35
61.07 Â±0.56 48.27 Â±3.50 62.38 Â±0.48
51.49 Â±1.13 - 50.43 Â±1.48 47.20 Â±0.89 62.85 Â±0.84 53.21 Â±3.04 61.52 Â±0.52
UN
Trade 62.28 Â±0.50
58.82 Â±0.98 62.72 Â±0.12 59.99 Â±3.50 67.05 Â±0.21 - 63.76 Â±0.07 63.48 Â±0.37 72.56 Â±1.47 67.25 Â±1.05 67.26 Â±0.10
UN
Vote 58.13 Â±1.43
55.13 Â±3.46 51.83 Â±1.35 61.23 Â±2.71 48.34 Â±0.76 - 50.51 Â±1.05 50.04 Â±0.86 66.26 Â±5.48 56.73 Â±0.69 69.17 Â±0.53
Contact 95.37 Â±0.92
91.89 Â±0.38 96.53 Â±0.10 94.84 Â±0.75 89.07 Â±0.34 - 93.05 Â±0.09 92.83 Â±0.05 96.67 Â±0.45 98.30 Â±0.02 98.38 Â±0.02
A
vg. Rank 6.62
7.77 7.23 5.31 5.62 - 7.92 7.15 3.31 2.62 1.46
(2) CNE-N achieves the best performance with low computa-
tion cost. This is due to three reasons. Firstly, CNE-N constructs
an edge neighborhood subgraph with a short first-order sequence
length and considers high-order neighbors by accessing memory,
which is more efficient than other sample-based subgraph construc-
tion methods like TGAT and TGN. Secondly, CNE-N computes
co-neighbor encoding through vector-based parallel computing,
which is more efficient than other structure encoding methods like
CAWN or DyGFormer. Lastly, unlike other memory-based methods
such as NAT and Jodie, CNE-N does not store the nodeâ€™s hidden
state in memory and instead accesses/updates the memory using
neural networks.4.3 Ablation Study
We conducted an ablation study to validate the effectiveness of
certain designs in CNE-N. This included examining the use of Co-
neighbor Encoding (CnE), the use of temporal-diverse memory
(TD), updating neighbor memory (Nup), and updating memory
with neighbors in the two-order subgraph (Tup). We removed each
module separately and referred to the remaining parts as w/o CnE,
w/o TD, w/o Nup, and w/o Tup. We evaluated the performance of
different variants on MOOC, LastFM, UCI, and UN Trade datasets
from four domains and presented the results in Figure 5. Our find-
ings indicate that CNE-N mostly performs best when using all the
components. Co-neighbor encoding has the most significant impact
 
427KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
Figure 4: CNE-N vs SOTA CTDG methods on MOOC, UNtrade,
UCI, and LastFM. The horizontal axis shows the relative train-
ing time for each method as a multiple of CNE-Nâ€™s running
time. The vertical axis shows average precision.
on performance as it effectively captures the structural encoding
between query node pairs in the subgraph. The temporal-diverse
memory benefits model performance by generating structure en-
coding for subgraph different time intervals. Updating neighbor
memory can increase the influence of newly occurred links, provid-
ing more information to predict future links between higher-order
neighbors. Updating memory with two-order neighbors achieved
the best performance in some settings. For instance, in a bipar-
tite dataset like LastFM, two-order neighbors may replace other
neighbors in the hashtable as they cannot provide much helpful
information, to be more specific, an user does not directly interact
with another user.
4.4 Parameter Sensitivity
In this part, we test how the number of triangles being considered in
the subgraph influences model performance. As discussed in Section
3.2, the number of nodes being considered is up to 1+ğ‘™ğ‘ +ğ‘™ğ‘ ğ‘€, to
support our motivation that a larger subgraph results in a better
model performance, we test model performance by changing over
the two parameters.
Sizes of Hashtables . Hashtable size ğ‘€also influences the co-
neighbor encoding accuracy. Reducing the size of the hashtable
will increase the likelihood of hash conflict, which can affect the
accuracy of co-neighbor encoding. We evaluated the sensitivity of
the hashtable size parameter on MOOC, LastFM, UCI, and UN Trade,
as shown in Figure 6. The results indicate that in the transductive
setting, model performance improves as we increase the hashtable
size. However, in the inductive setting, performance is negatively
impacted when the size of the hashtable is too large.
Length of the Sequence. The length of the historical interac-
tion sequence ğ‘™ğ‘ is an important factor as it contains both valuable
information from neighbors and noise and also affects the number
of neighbor-induced subgraphs. In Figure 7, we tested the sensitiv-
ity of the Sequence length parameter on MOOC, LastFM, UCI, and
UN Trade datasets. For social datasets like UCI and MOOC, recent
neighbors can provide valuable information, whereas former neigh-
bors bring more noise. On the other hand, for user-item interactiondatasets like UNtrade and LastFM, longer historical information
can better model user preference.
5 RELATED WORK
5.1 Link Prediction in Graph
Previous work on temporal network representation learning used
GNNs to process static graph snapshot sequences taken at time
intervals, because of limitations in expressive power such as the
inability to count triangles and the inability to distinguish nodes
with the same structural roles, graph neural networks (GNNs) usu-
ally cannot extract the network evolution patterns, such as the
triadic closure rule that is common in social networks, and perform
poorly on link prediction (LP) tasks[ 5,36]. To enhance the expres-
sivity of GNNs on LP tasks, many methods have been proposed,
Assigning unique node IDs can distinguish different structural node
representations, but at the expense of generalization [ 1] and train-
ing convergence [ 26]. However, these methods are still limited by
the ability of GNNs to extract structural features from the shared
neighborhood of multiple nodes. To simplify computation and im-
prove generalization ability, the state-of-the-art LP methods only
perform computation on subgraphs containing links, given a set
of queried node sets, CAW-N [ 36], SEAL[ 30,43], GraIL[ 29] and
SubGNN[ 2] and other SGRL models first extract subgraphs (named
query-induced subgraphs) around the queried node sets, and then
encode the extracted subgraphs for prediction [ 5]. A large number
of studies have shown that SGRL models are more robust[14], and
more expressive[4, 14], compared with more complex techniques.
5.2 Dynamic Graph Learning
Representation learning on dynamic graphs is divided into DTDG
/CTDG methods. Discrete-time dynamic graph learning (DTDG)
methods, based on a low-cost discrete mechanism, tend to split
event sequences into several individual snapshots[ 19]. Each snap-
shot contains a part of the event sequence by time interval from
one month to one year. The model treats each snapshot as a static
graph with time features in edge and encodes them with tradi-
tional graph neural networks for spatial information propagation[ 7].
Some works factorized the connectivity matrix of each snapshot
for node representation[ 12,21,42]. Another strategy is to take ran-
dom walks on the snapshot and model the walk behaviors[ 10,11].
Node embedding was later aggregated by weighted sum [ 44], tem-
poral smoothness constraint, or sequential models like LSTM[ 15]
in E-LSTM-D[ 8] and Know-Evolve[ 31]. Those sequential models
were added along the snapshots to capture the temporal evolving
information among nodes in each snapshot. DTDG method can
apply to tasks that have seasonal patterns since encoding along
snapshots tends to learn patterns in a regular time interval. But at
the same time, these methods require manual determination of time
intervals, ignoring the temporal order of nodes in each snapshot,
and have low accuracy.
Continuous-Time Dynamic Graph Learning (CTDG) methods
are based on timestamped graphs[ 33] (where evolution is repre-
sented as a continuous-time function), and typically have better
flexibility and achieve higher accuracy. These methods split the
event sequence into fixed-length edge batches (200 to 600 events per
batch), leading to a large number of batches, therefore, to control
 
428Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) transductive setting
 (b) inductive setting
Figure 5: AP in both settings for ablation study of the CNE-N.
Figure 6: AP in the two settings with varying hashtable size.
Figure 7: AP in the two settings with varying historical inter-
action sequence length.the computation cost, CTDG methods only pass the message on
a sampled local subgraph, mailbox (also known as memory) were
designed to store information to be propagated between differ-
ent subgraphs. Representative methods include temporal random
walk[ 20,36] and neural extensions of temporal point processes
[32,33,45]. DynRep[ 32] and JODIE[ 17] developed a memory layer
to store history edge messages and evolve node representation
with a recurrent-based model when an edge occurs. TGAT[ 37] sam-
ple on a temporal graph for a local subgraph, and generate node
representation by graph embedding layer with random Fourier
features to encode timestamps. TGN generalizes the graph net-
work model of static graphs and most graph message passing type
architectures[ 24], which combines Jodie and TGAT. Edgebank[ 22]
discusses the influence of different negative sample strategies. Be-
sides, Causal Anonymous Walk-Networks (CAW-N)[ 36] computes
the node embedding with a set of anonymized random walks, and
aggregates them with recurrent model and attention operation.
6 CONCLUSION
In this paper, we discussed the trade-off between accuracy and
complexity in structure encoding in CTDG methods. We proposed
a Co-Neighbor Encoding Schema (CNES) method to achieve the
balance. CNES generates structure encoding by attaching a regular-
sized hashtable-based neighbor memory to each node and calcu-
lating the number of common neighbors between the end node
and each neighbor node with vector-based parallel computation. A
Temporal-Diverse Memory is used to generate structure encoding
for subgraphs at different time intervals. With the aforementioned
techniques, we proposed a dynamic graph learning model called
CNE-N. It achieves the highest performance in 10 of 13 datasets
while being computationally efficient.
7 ACKNOWLEDGMENTS
This work was supported by the National Natural Science Founda-
tion of China (62272023, 51991395, 51991391, U1811463) and, the
S&T Program of Hebei(225A0802D).
 
429KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
REFERENCES
[1]Ralph Abboud, Ä°smail Ä°lkan Ceylan, Martin Grohe, and Thomas Lukasiewicz.
2020. The Surprising Power of Graph Neural Networks with Random Node
Initialization. arXiv preprint arXiv:2010.01179 (2020). ^5^
[2]Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. 2020. Sub-
graph Neural Networks. In Advances in Neural Information Processing Systems
(NeurIPS), Vol. 33. 8017â€“8029.
[3]Unai Alvarez-Rodriguez, Federico Battiston, Guilherme Ferraz de Arruda, Yamir
Moreno, MatjaÅ¾ Perc, and Vito Latora. 2021. Evolutionary dynamics of higher-
order interactions in social networks. Nature Human Behaviour 5, 5 (2021),
586â€“595.
[4]Giorgos Bouritsas, Fabrizio Frasca, Stefanos P Zafeiriou, and Michael Bronstein.
2022. Improving graph neural network expressivity via subgraph isomorphism
counting. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
[5]Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca,
Thomas Markovich, Nils Hammerla, Michael M Bronstein, and Max Hansmire.
2022. Graph neural networks for link prediction with subgraph sketching. arXiv
preprint arXiv:2209.15486 (2022).
[6]Yen-Yu Chang, Pan Li, Rok Sosic, MH Afifi, Marco Schweighauser, and Jure
Leskovec. 2021. F-fade: Frequency factorization for anomaly detection in edge
streams. In Proceedings of the 14th ACM International Conference on Web Search
and Data Mining. 589â€“597.
[7]Jinyin Chen, Xueke Wang, and Xuanheng Xu. 2022. GC-LSTM: Graph convolution
embedded LSTM for dynamic network link prediction. Applied Intelligence (2022),
1â€“16.
[8]Jinyin Chen, Jian Zhang, Xuanheng Xu, Chenbo Fu, Dan Zhang, Qingpeng Zhang,
and Qi Xuan. 2019. E-LSTM-D: A deep learning framework for dynamic network
link prediction. IEEE Transactions on Systems, Man, and Cybernetics: Systems 51,
6 (2019), 3699â€“3712.
[9]Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang
Tong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated Model
Architectures For Temporal Networks? arXiv preprint arXiv:2302.11636 (2023).
[10] Sam De Winter, Tim Decuypere, Sandra MitroviÄ‡, Bart Baesens, and Jochen
De Weerdt. 2018. Combining temporal aspects of dynamic networks with
Node2Vec for a more efficient dynamic link prediction. In 2018 IEEE/ACM interna-
tional conference on advances in social networks analysis and mining (ASONAM) .
IEEE, 1234â€“1241.
[11] Lun Du, Yun Wang, Guojie Song, Zhicong Lu, and Junshan Wang. 2018. Dy-
namic network embedding: An extended approach for skip-gram based network
embedding.. In IJCAI, Vol. 2018. 2086â€“2092.
[12] Daniel M Dunlavy, Tamara G Kolda, and Evrim Acar. 2011. Temporal link
prediction using matrix and tensor factorizations. ACM Transactions on Knowledge
Discovery from Data (TKDD) 5, 2 (2011), 1â€“27.
[13] Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng, and Philip S Yu.
2021. Continuous-time sequential recommendation with temporal graph collab-
orative transformer. In Proceedings of the 30th ACM international conference on
information & knowledge management. 433â€“442.
[14] Fabrizio Frasca, Beatrice Bevilacqua, Michael M Bronstein, and Haggai Maron.
2022. Understanding and Extending Subgraph GNNs by Rethinking Their Sym-
metries. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 35.
[15] Alex Graves and Alex Graves. 2012. Long short-term memory. Supervised sequence
labelling with recurrent neural networks (2012), 37â€“45.
[16] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2018. DyRep: Learning Repre-
sentations Over Dynamic Graphs. arXiv preprint arXiv:1803.04051 (2018). ^18^
[17] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-
bedding trajectory in temporal interaction networks. In Proceedings of the 25th
ACM SIGKDD international conference on knowledge discovery & data mining.
1269â€“1278.
[18] Xiaohan Li, Mengqi Zhang, Shu Wu, Zheng Liu, Liang Wang, and Philip S Yu.
2021. Dynamic Graph Collaborative Filtering. arXiv preprint arXiv:2101.02844
(2021).
[19] David Liben-Nowell and Jon Kleinberg. 2007. The link-prediction problem for
social networks. Journal of the American Society for Information Science and
Technology 58, 7 (2007), 1019â€“1031.
[20] Yuhong Luo and Pan Li. 2022. Neighborhood-aware scalable temporal network
representation learning. In Learning on Graphs Conference. PMLR, 1â€“1.
[21] Yunpu Ma, Volker Tresp, and Erik A Daxberger. 2019. Embedding models for
episodic knowledge graphs. Journal of Web Semantics 59 (2019), 100490.
[22] Farimah Poursafaei, Andy Huang, Kellin Pelrine, and Reihaneh Rabbany. 2022. To-
wards Better Evaluation for Dynamic Link Prediction. In Thirty-sixth Conference
on Neural Information Processing Systems Datasets and Benchmarks Track.
[23] Stephen Ranshous, Shitian Shen, Danai Koutra, Steve Harenberg, Christos Falout-
sos, and Nagiza F Samatova. 2015. Anomaly detection in dynamic networks:
a survey. Wiley Interdisciplinary Reviews: Computational Statistics 7, 3 (2015),
223â€“247.
[24] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal graph networks for deep learningon dynamic graphs. arXiv preprint arXiv:2006.10637 (2020).
[25] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael M. Bronstein. 2020. Temporal Graph Networks for Deep
Learning on Dynamic Graphs. CoRR abs/2006.10637 (2020).
[26] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. 2020. Random Features
Strengthen Graph Neural Networks. arXiv preprint arXiv:2002.03155 (2020). ^10^
[27] Weiping Song, Zhiping Xiao, Yifan Wang, Laurent Charlin, Ming Zhang, and Jian
Tang. 2019. Session-based social recommendation via dynamic graph attention
networks. In Proceedings of the Twelfth ACM international conference on web
search and data mining. 555â€“563.
[28] A. H. Souza, D. Mesquita, S. Kaski, and V. Garg. 2022. Provably expressive
temporal graph networks. In Advances in Neural Information Processing Systems
(NeurIPS).
[29] Komal K. Teru, Etienne Denis, and Will Hamilton. 2020. Inductive Relation
Prediction by Subgraph Reasoning. In Proceedings of the International Conference
on Machine Learning (ICML). 9448â€“9457. ^41^
[30] Komal K. Teru, Etienne Denis, and William L. Hamilton. 2022. Labeling Trick: A
Theory of Using Graph Neural Networks for Multi-Node Representation Learning.
arXiv preprint arXiv:2201.07858v1 [cs.LG] (2022). ^42^
[31] Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. 2017. Know-evolve: Deep
temporal reasoning for dynamic knowledge graphs. In international conference
on machine learning. PMLR, 3462â€“3471.
[32] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019.
Dyrep: Learning representations over dynamic graphs. In International conference
on learning representations.
[33] Wang Y. Song L.. Trivedi R., Dai H. 2017. Know-evolve: Deep temporal reasoning
for dynamic knowledge graphs. In International Conference on Machine Learning
(ICML). 3462â€“3471.
[34] Andrew Z Wang, Rex Ying, Pan Li, Nikhil Rao, Karthik Subbian, and Jure Leskovec.
2021. Bipartite dynamic representations for abuse detection. In Proceedings of the
27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3638â€“3648.
[35] Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He,
Le Song, Jingren Zhou, and Hongxia Yang. 2021. Tcl: Transformer-based dynamic
graph modelling via contrastive learning. arXiv preprint arXiv:2105.07944 (2021).
[36] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. 2021.
Inductive Representation Learning in Temporal Networks via Causal Anonymous
Walks. In International Conference on Learning Representations (ICLR).
[37] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
2020. Inductive Representation Learning on Temporal Graphs. arXiv preprint
arXiv:2002.07962 (2020).
[38] Le Yu, Bowen Du, Xiao Hu, Leilei Sun, Liangzhe Han, and Weifeng Lv. 2021. Deep
spatio-temporal graph convolutional network for traffic accident prediction.
Neurocomputing 423 (2021), 135â€“147.
[39] Le Yu, Zihang Liu, Tongyu Zhu, Leilei Sun, Bowen Du, and Weifeng Lv. 2022. Mod-
elling Evolutionary and Stationary User Preferences for Temporal Sets Prediction.
arXiv preprint arXiv:2204.05490 (2022).
[40] Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. 2023. Towards Better Dy-
namic Graph Learning: New Architecture and Unified Library. arXiv preprint
arXiv:2303.13047 (2023).
[41] Le Yu, Guanghui Wu, Leilei Sun, Bowen Du, and Weifeng Lv. 2022. Element-
guided Temporal Graph Representation Learning for Temporal Sets Prediction.
InProceedings of the ACM Web Conference 2022. 1902â€“1913.
[42] Wenchao Yu, Wei Cheng, Charu C Aggarwal, Haifeng Chen, and Wei Wang. 2017.
Link prediction with spatial and temporal consistency in dynamic networks.. In
IJCAI. 3343â€“3349.
[43] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural
Networks. Advances in Neural Information Processing Systems (NeurIPS) (2018).
^23^
[44] Jia Zhu, Qing Xie, and Eun Jung Chin. 2012. A hybrid time-series link prediction
framework for large social network. In International Conference on Database and
Expert Systems Applications (DEXA). Springer, 345â€“359.
[45] Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, and Junjie Wu. 2018.
Embedding Temporal Network via Neighborhood Formation. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. ACM, 2857â€“2866.
 
430Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A APPENDIX
In the appendix, details of the experiments are introduced.
A.1 Descriptions of Datasets
These thirteen datasets (Wikipedia, Reddit, MOOC, LastFM, Enron,
Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote,
and Contact) are collected by Edgebank[ 22] and cover diverse do-
mains, detailed statistics are shown in Table 4.
Wikipedia: consists of the bipartite interaction graph between
editors and Wiki pages over a month [ 22]. Nodes represent editors
and pages, and links denote the editing behaviors with timestamps.
The features of the links are 172-dimensional Linguistic Inquiry
and Word Count (LIWC) vectors.
Reddit: records thebipartite posts of users under subreddits
during one month[ 17]. Users and subreddits are the nodes, and
links are the timestamped posting requests. Each link has a 172-
dimensional LIWC feature.
MOOC: is a a student bipartite interaction network of online
sources units and models the studentâ€™s access to course videos or
questions as links, and represents them by four-dimensional feature
vectors[17].
LastFM: is a bipartite network that records the songs listened
to by users over one month. Users and songs are nodes, and links
represent the listening behaviors of users.
Enron: records the email communications between employees
of the ENRON energy corporation over three years.
UCI: is an online communication network that models university
students as nodes and messages posted by the students as links.
Flights: is a dynamic flight network that illustrates the evo-
lution of air traffic during the COVID-19 pandemic. Airports are
represented by nodes, and links denote tracked flights. Each link is
associated with a weight indicating the number of flights between
two airports in a day.
Can. Parl.: is a dynamic political network that captures interac-
tions between Canadian Members of Parliament (MPs) from 2006 to
2019. Each node in the network represents an MP from an electoral
district, and a link is established between two MPs when they both
vote â€œyesâ€ on a bill. The weight of each link corresponds to the
number of times one MP voted â€œyesâ€ for another MP in a year.
US Legis.: is a network that tracks the social interactions be-
tween legislators in the US Senate. It records the number of times
two congresspersons have co-sponsored a bill in a given congress,
and the weight of each link represents this number.
UN Trade: tracks the food and agriculture trade between 181
nations for over 30 years. The weight of each link represents the
total sum of normalized agriculture import or export values between
two specific countries.
UNvote: records roll-call votes in the United Nations General
Assembly. If two nations both voted "yes" to an item, the weight of
the link between them is increased by one.
Contact: describes how the physical proximity evolves among
about 700 university students over a month. Each student has a
unique identifier and links denote that they are within 16 proximity
to each other. Each link is associated with a weight, revealing the
physical proximity between students.A.2 Descriptions of Baselines
We select the following ten baselines:
JODIE[ 17] focuses on bipartite networks of instantaneous user-
item interactions. It employs two coupled RNNs to update the
representation of the users and items recursively. A projection
operation is introduced to learn the future representation trajectory
of each user/item.
DyRep[ 16] has a custom RNN that updates node representations
upon observation of a new edge. For obtaining the neighbor weights
at each time, DyRep uses a temporal attention mechanism, which
is parameterized by the recurrent architecture.
TGAT[ 37] computes the node representation by aggregating
features from each nodeâ€™s temporal-topological neighbors based
on the self-attention mechanism. It is also equipped with a time
encoding function for capturing temporal patterns.
TGN[ 25] maintains an evolving memory for each node and
updates this memory when the node is observed in an interaction,
which is achieved by the message function, message aggregator, and
memory updater. An embedding module is leveraged to generate
the temporal representations of nodes.
CAWN[ 36] first extracts multiple causal anonymous walks for
each node, which can explore the causality of network dynamics
and generate relative node identities. Then, it utilizes recurrent
neural networks to encode each walk and aggregates these walks
to obtain the final node representation.
EdgeBank[22] is a pure memory-based approach for transduc-
tive dynamic link prediction. It stores the observed interactions in
the memory unit and updates the memory through various strate-
gies. An interaction will be predicted as positive if it is retained in
the memory and negative otherwise.
TCL[ 35] generates each nodeâ€™s interaction sequence by perform-
ing a breadth-first search algorithm on the temporal dependency
interaction sub-graph. Then, it presents a graph transformer that
considers both graph topology and temporal information to learn
node representations. It also incorporates a cross-attention opera-
tion for modeling the interdependencies of two interaction nodes.
GraphMixer[ 9] shows that a fixed-time encoding function per-
forms better than the trainable version. It incorporates the fixed
function into a link encoder based on MLP-Mixer to learn from
temporal links. A node encoder with neighbor mean-pooling is
employed to summarize node features.
NAT[ 20] adopts a novel dictionary-type neighborhood repre-
sentation to gather the temporal neighbors of each node, and it
then uses a recurrent process to learn the node representation from
the historical neighbors of the current node and an RFF-based time
embedding. The method constructs the query-induced subgraph
without using neighbor samples to reduce computation costs.
DyGFormer[ 40] is a Transformer-based architecture for dy-
namic graph learning that only picks up information from pre-
vious first-hop interactions between nodes. It creates a neighbor
co-occurrence encoding scheme based on a patching method and
feeds them to the Transformer. This method enables the model to
benefit from longer histories by exploring the correlations of the
source node and destination node based on their sequences.
 
431KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
Table 4: Statistics of the datasets
Datasets Domains
#Nodes #Links #Node & Link Features Bipartite Duration
Wikip
edia So
cial 9,227 157,474 - & 172 True 1 month
Reddit So
cial 10,984 672,447 - & 172 True 1 month
MOOC Interaction
7,144 411,749 â€“ & 4 True 17 months
LastFM Interaction
1,980 1,293,103 â€“ & â€“ True 1 month
Enron So
cial 184 125,235 â€“ & â€“ False 3 years
Social Evo. Pr
oximity 74 2,099,519 â€“ & 2 False 8 months
UCI So
cial 1,899 59,835 â€“ & â€“ False 196 days
Flights T
ransport 13,169 1,927,145 â€“ & 1 False 4 months
Can. Parl. Politics
734 74,478 â€“ & 1 False 14 years
US Legis. Politics
225 60,396 â€“ & 1 False 12 congresses
UN Trade Economics
255 507,497 â€“ & 1 False 32 years
UN Vote Politics
201 1,035,742 â€“ & 1 False 72 years
Contact Pr
oximity 692 2,426,279 â€“ & 1 False 1 month
Table 5: Statistics of the TGB datasets
Datasets Domains
#Nodes #Links #Steps #Surprise
tgbl-wiki Interaction
9,227 157,474 152,757 0.108
tgbl-review Rating
352,637 4,873,540 6,865 0.987
tgbl-coin T
ransaction 638,486 22,809,486 1,295,720 0.120
tgbl-comment So
cial 994,790 44,314,507 30,998,030 0.823
Table 6: MRR for dynamic link property prediction, where
Val is the abbreviation of Validation.
Sets Metho
ds tgbl-wiki
tgbl-review tgbl-coin tgbl-comment
V
alJODIE 71.42Â±0.76 34.76Â±0.06 -
-
D
yRep 59.38Â±1.82
33.85Â±0.18 51.20Â±1.40
29.10Â±2.80
T
GAT 65.14Â±1.22
17.24Â±0.89 60.47Â±0.22 50.73Â±2.47
T
GN 73.80Â±0.39
33.17Â±0.13 60.70Â±1.40 35.60Â±1.90
CA
WN 75.36Â±0.34
20.00Â±0.10 - -
EdgeBankâˆ 56.13Â±0.00
2.29Â±0.00 31.54Â±0.00 10.87Â±0.00
EdgeBank tw-ts 66.51Â±0.00
2.90Â±0.00 49.67Â±0.00 12.44Â±0.00
T
CL 80.82Â±0.14
17.99Â±1.72 66.85 Â±0.27 65.10Â±0.67
GraphMixer 63.87Â±0.53
28.28Â±2.07 70.38Â±0.40 70.19Â±0.23
NA
T 77.30Â±1.10
30.20Â±1.10 - -
D
yGFormer 81.62Â±0.46
21.92Â±1.74 72.97Â±0.23
61.33Â±0.27
CNE-N 82.29Â±0.12 19.70Â±0.18 74.39Â±0.24
73.21 Â±0.21
T
estJODIE 63.05Â±1.69 41.43Â±0.15 -
-
D
yRep 51.91Â±1.95
40.06Â±0.59 45.20Â±4.60
28.90Â±3.30
T
GAT 59.94Â±1.63
19.64Â±0.23 60.92Â±0.57 56.20Â±2.11
T
GN 68.93Â±0.53
37.48Â±0.23 58.60Â±3.70 37.90Â±2.10
CA
WN 73.04Â±0.60
19.30Â±0.10 - -
EdgeBankâˆ 52.50Â±0.00
2.29Â±0.00 35.90Â±0.00 12.85Â±0.00
EdgeBank tw-ts 63.25Â±0.00
2.94Â±0.00 57.36Â±0.00 14.94Â±0.00
T
CL 78.11Â±0.20
16.51Â±1.85 68.66Â±0.30 70.11Â±0.83
GraphMixer 59.75Â±0.39
36.89Â±1.50 75.57Â±0.27 76.17Â±0.17
NA
T 74.90Â±1.00
34.10Â±2.00 - -
D
yGFormer 79.83Â±0.42 22.39Â±1.52
75.17Â±0.38 67.03Â±0.14
CNE-N 80.24Â±0.20 26.12Â±0.25 77.24Â±0.21
78.97 Â±0.14A.3 Performance on TGB
Recently, researchers have noticed the size of benchmark datasets
for the dynamic link prediction task is relatively small and proposed
a large-scale temporal graph benchmark (TGB1), we tested our
method with the code provided by DyGLib-TGB2. Because the
validation of the datasets takes days for each model, we directly
use the baseline performance reported by DyGLib-TGB. Besides,
due to the downloading issue in tgbl-flight, we do not compare our
method on that dataset. The statistics of the datasets are listed in
Table 5
We have two key observations from Table 6. Firstly, CNE-N
achieves the best performance on all four datasets against baseline
models, even on datasets with low neighbor node re-interact prob-
ability (bad performance with EdgeBank, the model ), indicating
co-neighbor encoding can effectively describe the evolution of the
dynamic graph rather than the re-occurrence of the neighbor node.
Secondly, we compare the efficiency between our method and two
efficient baselines Jodie and GraphMixer in the largest two datasets
tgbl-coin and tgbl-comment, Jodie takes 124 hours and 63 hours
for one epoch of the training set, GraphMixer takes 2 hours, and 4
hours, and CNEN takes 0.67 hours and 1.42 hours, indicating the
efficiency of our method on large scale dataset.
1https://tgb.complexdatalab.com
2https://github.com/yule-BUAA/DyGLib_TGB
 
432