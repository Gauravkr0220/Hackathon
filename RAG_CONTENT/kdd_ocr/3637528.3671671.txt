Label Learning Method Based on Tensor Projection
Jing Li
School of Telecommunications
Engineering, Xidian University
Xiâ€™an, Shaanxi, China
jinglxd@stu.xidian.edu.cnQuanxue Gaoâˆ—
School of Telecommunications
Engineering, Xidian University
Xiâ€™an, Shaanxi, China
qxgao@xidian.edu.cnQianqian Wang
School of Telecommunications
Engineering, Xidian University
Xiâ€™an, Shaanxi, China
qqwang@xidian.edu.cn
Cheng Deng
School of Electronic Engineering,
Xidian University
Xiâ€™an, Shaanxi, China
chdeng@mail.xidian.edu.cnDeyan Xie
School of Science and Information
Science, Qingdao Agricultural
University
Qingdao, Shandong, China
xdy123@qau.edu.cn
ABSTRACT
Multi-view clustering method based on anchor graph has been
widely concerned due to its high efficiency and effectiveness. In
order to avoid post-processing, most of the existing anchor graph-
based methods learn bipartite graphs with connected components.
However, such methods have high requirements on parameters,
and in some cases it may not be possible to obtain bipartite graphs
with clear connected components. To end this, we propose a label
learning method based on tensor projection (LLMTP). Specifically,
we project anchor graph into the label space through an orthogonal
projection matrix to obtain cluster labels directly. Considering that
the spatial structure information of multi-view data may be ignored
to a certain extent when projected in different views separately, we
extend the matrix projection transformation to tensor projection,
so that the spatial structure information between views can be
fully utilized. In addition, we introduce the tensor Schatten ğ‘-norm
regularization to make the clustering label matrices of different
views as consistent as possible. Extensive experiments have proved
the effectiveness of the proposed method.
CCS CONCEPTS
â€¢Computing methodologies â†’Cluster analysis; â€¢Informa-
tion systemsâ†’Clustering and classification ;Clustering .
KEYWORDS
Multi-View clustering, Anchor graph, Tensor Schatten ğ‘-norm,
Tensor projection
ACM Reference Format:
Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, and Deyan Xie. 2024.
Label Learning Method Based on Tensor Projection. In Proceedings of the
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $15.00
https://doi.org/10.1145/3637528.367167130th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671671
1 INTRODUCTION
As one of the common techniques of data mining, clustering can
be used to discover the internal structure and organization of data
and divide them into different meaningful clusters. With the wide
application of various sensors and other technologies, the descrip-
tion of the same object is more and more prone to diversification
and isomerization. For example, an event can be described by a
text, a picture, a voice and a video; A picture can be described with
different features, such as GIST [ 14], CMT [ 32], HOG [ 1], etc. A
section of path information in automatic driving can be represented
by liDAR point cloud data, depth camera data, and infrared data, etc.
Data like this can be called multi-view data. Multi-view clustering
(MVC) is the operation of clustering multi-view data.
As an effective data mining method, multi-view clustering has
been widely concerned [ 4,11,23,29,33,36,38]. One of the most
representative methods is the graph-based multi-view clustering
method [ 5,13,16,18,26,37]. These method involve similarity
graphs construction and eigen-decomposition of Laplacian matrices,
and the computational complexity is O(ğ‘›2)andO(ğ‘›3), respectively,
whereğ‘›represents the number of samples. With the advent of the
era of big data, data acquisition is becoming easier, resulting in
the continuous expansion of the scale of datasets. So graph-based
multi-view clustering methods have been somewhat difficult to deal
with large-scale multi-view data. On this basis, anchor graph-based
multi-view clustering methods have been proposed and widely
used.
The core idea of the anchor graph-based method is to select ğ‘š
representative points (called anchors) from the ğ‘›samples, and then
learn the relationship between the samples and anchors (called
anchor graph). The anchor graph is ğ‘›Ã—ğ‘š. Generally speaking,
ğ‘šâ‰ªğ‘›, so the anchor graph-based multi-view clustering methods
can handle large-scale multi-view data well. In addition to being able
to handle large-scale data, anchor graph-based methods also inherit
the advantages of graph-based methods represented by spectral
clustering, i.e., they are not affected by the geometric distribution of
data. This method usually first explores the geometrical structure
of multi-view data by constructing anchor graph, and then clusters
 
1599
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, & Deyan Xie
on the anchor graph using existing clustering techniques. However,
the second step is still time-consuming.
To end this, another method using anchor graph clustering is
proposed. These methods use anchor graphs to obtain bipartite
graphs, and then learns bipartite graphs with ğ¾connected com-
ponents (where ğ¾is the number of classes), so that the clustering
results can be obtained directly without post-processing. However,
these methods of learning bipartite graphs have high parameter
requirements so may not find ğ¾connected components in some
cases.
In order to avoid the above problems and avoid post-processing,
we consider the use of projection matrix to project the anchor graph
directly into the label space, i.e., theğ‘›Ã—ğ‘šanchor graph is regarded
as feature matrices with ğ‘›samples and ğ‘šfeature dimensions, and
theğ‘›Ã—ğ‘cluster label matrix can be directly obtained after the
projection transformation. Generally speaking, the above process
requires the projection transformation of the anchor graph on each
view, and then the clustering label matrix of each view is obtained,
finally these matrices are fused to obtain the clustering results of
multi-view data. However, projective transformation in each view
separately may cause the spatial structure information between
different views not fully utilized.
To end this, we propose a multi-view data label learning method
based on tensor projection (LLMTP). In order to get the cluster
labels directly from the anchor graph, we consider projecting the
anchor graph into the label space, so that the cluster results can
be obtained directly. Considering that the projection of each view
separately will cause the model to be unable to make good use of
the complementary information and spatial structure information
between different views, we extend the matrix projection transfor-
mation of the anchor graph to the tensor projection transformation
of the anchor graph tensor, i.e., project the third-order tensor di-
rectly. So that the spatial structure information embedded between
different views can be preserved to a large extent. Thus, better clus-
tering performance can be obtained. Extensive experiments have
proved the superiority of our proposed model.
In summary, the main contributions of this paper are as follows:
â€¢It is proposed to project the anchor graph into the label
space to obtain the clustering label directly. Meanwhile, the
matrix projection is extended to tensor projection so that
complementary information and spatial structure informa-
tion between views can be fully mined.
â€¢From clustering perspective, our method can integrates multi-
view information at both the data level and decision level.
â€¢We propose an algorithm to optimize the tensor projection
and verify the convergence of this algorithm by experiments.
â€¢We introduce the tensor Schatten p-norm to exploit comple-
mentary information across different views, facilitating the
derivation of a common consensus label matrix.
2 RELATED WORKS
2.1 Anchor Graph-Based Multi-View Clustering
Methods
Since the computational complexity of ğ‘›Ã—ğ‘›similarity graph is
O(ğ‘‰ğ‘›2)during graph construction, and O(ğ‘›3)during eigen de-
composition of Laplacian matrix (where ğ‘‰andğ‘›are the numberof views and samples, respectively), it is difficult for graph-based
multi-view clustering methods to deal with large-scale multi-view
data. By using ğ‘šanchors to cover ğ‘›sample point clouds, and con-
structing the relationship between ğ‘›samples and ğ‘šanchors, the
anchor graph is obtained. Because ğ‘šâ‰ªğ‘›, the multi-view clustering
method based on anchor graph can deal with large-scale multi-view
data effectively.
In general, anchor graph-based methods explore the geometry of
multi-view data by constructing anchor graphs, and then it requires
additional clustering techniques such as K-means to cluster the ob-
tained anchor graph. However, the additional clustering techniques
is time-consuming.
To avoid post-processing, it is common to use anchor graphs to
learn bipartite graphs with K connected components (where K is
the number of clusters). Therefore, post-processing can be avoided
and clustering results can be obtained directly from bipartite graphs
with K connected components. The representative methods are as
follows. MVGL [ 35] gains insights from various single view graphs
to construct a global graph. Instead of relying on post-processing
techniques, it utilizes K connected components to extract clustering
metrics. However, its scalability for handling large-scale data might
be limited. SFMC [ 10] introduces a parameter-free approach to fuse
multi-view cluster graphs, resulting in cohesive composite graphs
through the utilization of self-supervised weighting. Moreover, it
employs K connected components to symbolize clusters. LCBG [ 39]
takes the intra-view and inter-view spatial low-rank structures of
the learned bipartite graphs into account by minimizing tensor
Schattenğ‘-norm and nuclear norm. MSC-BG [ 31] leverages the
Schattenğ‘-norm to investigate the synergistic information across
diverse views, and deriving clusters through K connecting com-
ponents. TBGL [ 25] employs the Schatten ğ‘-norm to delve into
the resemblances among different views, while simultaneously in-
tegratingâ„“1,2-norm minimization regularization and connectivity
constraints to investigate the similarities within each view.
But these methods of learning bipartite graphs have high param-
eter requirements and may not find ğ¾connected components in
some cases.
2.2 Multi-View Clustering Method Based on
Projection
In the process of multi-view clustering, in most cases, we directly
process the original data or the constructed similarity graph, but
there may be redundant information, noise or outlier information
in the original data or similarity graph. They will adversely affect
the final clustering performance. Multi-view clustering methods
based on projection are mostly studied to solve such problems.
The general practice of this kind of method is to construct an or-
thogonal projection matrix to projective the original data, and then
get a relatively clean representation matrix in the embedded space.
Gao et al.[ 2] propose a new multi-view clustering framework that
combines dimensionality reduction, manifold structure learning
and feature selection, which maps high-dimensional data to low-
dimensional spaces to reduce data complexity and reduce the effects
of data noise and redundancy. Wang et al.[ 17] proposed a robust
self-weighted multi-view projection clustering (RSwMPC) based
onâ„“2,1-norm. It can simultaneously reduce dimension, suppress
 
1600Label Learning Method Based on Tensor Projection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
noise and learn local structure graph. The resulting optimal graph
can be directly used for clustering without any other processing.
Sang et al.[ 15] proposed a consensus graph-based auto-weighted
multi-view projection clustering (CGAMPC). It can simultaneously
reduce dimension, save manifold structure and learn consensus
structure graph. The information similarity graph is constructed on
the projected data to ensure the removal of redundant and noisy in-
formation in the original similarity graph, and the â„“2,1-norm is used
to select adaptive discriminant features. Wang et al.[ 20] proposed
consistency and diversity preserving with projection decomposi-
tion for multi-view clustering (CDP2D). It automatically learns the
shared projection matrix and analyzes multi-view data through
projection matrix decomposition. Li et al.[9] propose a projection-
based coupled tensor learning method (PCTL). It constructs an
orthogonal projection matrix to obtain the main feature informa-
tion of the raw data of each view, and learns the representation
matrix in a clean embedded space. Moreover, tensor learning is used
to coupling projection matrix and representation matrix, mining
higher-order information between views, and constructing more
suitable and better representation of embedded space.
Inspired by the above method, we consider whether the ğ‘›Ã—ğ‘š
anchor graph can be directly regarded as an feature matrix with ğ‘›
samples and ğ‘šdimensional features, according to [ 21]. Then the
feature matrix is directly projected into the label space through
projection changes, and the final clustering label is obtained directly.
3 NOTATIONS
We will cover t-product and the definition of the tensor Schatten
ğ‘-norm in this section.
Definition 1 (t-product [ 7]).Suppose AâˆˆRğ‘›1Ã—ğ‘šÃ—ğ‘›3and
BâˆˆRğ‘šÃ—ğ‘›2Ã—ğ‘›3, the t-product Aâˆ—BâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3is given by
Aâˆ—B=ifft(bdiag(AB),[],3),
where A=bdiag(A)and it denotes the block diagonal matrix. The
blocks of Aare frontal slices of A.
Definition 2. [3] Given HâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3,â„=min(ğ‘›1,ğ‘›2), the
tensor Schatten ğ‘-norm of His defined as
âˆ¥Hâˆ¥Spâ—‹= 
ğ‘›3Ã
ğ‘–=1H(ğ‘–)ğ‘
Spâ—‹!1
ğ‘
= 
ğ‘›3Ã
ğ‘–=1â„Ã
ğ‘—=1ğœğ‘—
H(ğ‘–)ğ‘!1
ğ‘
, (1)
where, 0<ğ‘â©½1,ğœğ‘—(H(ğ‘–))denotes the j-th singular value of H(ğ‘–).
It should be pointed out that for 0<ğ‘â©½1whenğ‘is appropri-
ately chosen, the Schatten ğ‘-norm provides quite effective improve-
ments for a tighter approximation of the rank function [27, 34].
Also we introduce the notations used throughout this paper. We
use bold calligraphy letters for 3rd-order tensors, HâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3,
bold upper case letters for matrices, H, bold lower case letters for
vectors, h, and lower case letters such as â„ğ‘–ğ‘—ğ‘˜for the entries of H.
Moreover, the ğ‘–-th frontal slice of HisH(ğ‘–).His the discrete
Fourier transform (DFT) of Halong the third dimension, H=
fft(H,[],3). Thus, H=ifft(H,[],3). The trace and transpose
of matrix Hare expressed as tr(H)andHT. The F-norm of His
denoted byâˆ¥Hâˆ¥ğ¹.4 METHODOLOGY
4.1 Motivation and Objective Function
Multi-view clustering method based on anchor graph can deal
with large-scale multi-view data efficiently. By learning a bipartite
graph with K connected components using anchor graphs, the
cluster labels can be obtained directly without any post-processing.
However, this method has high requirements on parameters, and
in some cases it may not be able to obtain a clear bipartite graph
with K connected components.
Inspired by the excellent performance of the orthogonal projec-
tion matrix in processing redundant information in the raw data
matrix, and referring to [ 21], we consider the ğ‘›Ã—ğ‘šanchor graph as
an feature matrix composed of ğ‘›ğ‘š-dimensional feature vectors and
theğ‘›Ã—ğ‘cluster label matrix can be obtained directly by projecting
the feature matrix (i.e.anchor graph) into the label space (as shown
in Figure 1), where ğ‘›,ğ‘šandğ‘are the number of samples, anchors
and clusters, respectively. In this way, the clustering results can
be obtained directly without post-processing. When dealing with
multi-view data, we have the following formula:
minğ‘‰âˆ‘ï¸
ğ‘£=1S(ğ‘£)G(ğ‘£)âˆ’H(ğ‘£)2
ğ¹+ğœ†ğ‘‰âˆ‘ï¸
ğ‘£=1R(H(ğ‘£))
s.t. H(ğ‘£)â©¾0,H(ğ‘£)TH(ğ‘£)=I,G(ğ‘£)TG(ğ‘£)=I,(2)
where, S(ğ‘£)is the anchor graph from the ğ‘£-th view, the anchors
selection method and the anchor graph construction method are
the same as that of [ 25].G(ğ‘£)is the orthogonal projection matrix.
H(ğ‘£)is the cluster label matrix. It can be more interpretable by
constraining it to be non-negative and orthogonal, i.e., each row
ofH(ğ‘£)has only one non-zero value, and the position of the value
indicates the cluster to which the corresponding element of the
row belongs. The purpose of the regular term with coefficient of ğœ†
is to make the clustering label matrices of different views tend to
be consistent.
ğ‡(ğŸ)
ğ‡(ğŸ)
ğ‡(ğ’—)K
NNM
Anchor Graph Space Label SpaceProjection
Figure 1: Anchor graph space to label space. ğ‘,ğ‘€andğ¾are
the number of samples, anchors and clusters, respectively.
Considering that the projection transformation in Eq. (2) is car-
ried out separately in each view, the complementary information
and spatial structure information embedded in different views may
 
1601KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, & Deyan Xie
Multi-view Data
Anchor Graph Construction
Anchor Tensor Construction
Anchor Tensor
ProjectionIndicator Tensor
Clustering
Figure 2: The flowchart of LLMTP
not be fully utilized. To end this, we consider extending the two-
dimensional matrix projection into a third-order tensor projection.
Eq. (2) is extended as follows:
minâˆ¥Sâˆ—Gâˆ’Hâˆ¥2
ğ¹+ğœ†ğ‘‰âˆ‘ï¸
ğ‘£=1R(H(ğ‘£))
s.t.Hâ©¾0,HTâˆ—H=I,GTâˆ—G=I(3)
In multi-view clustering, we should try our best to make the
H(ğ‘£)of different views in (2) tend to be the same. Inspired by the
excellent performance of the tensor Schatten ğ‘-norm [ 3,8,25,31],
we fully explore the complementary information in label matrices
of different views by introducing the regular term of the tensor
Schattenğ‘-norm.
The final objective function is as follows and the flowchart of
LLMTP is shown in Figure 2:
minâˆ¥Sâˆ—Gâˆ’Hâˆ¥2
ğ¹+ğœ†âˆ¥Hâˆ¥ğ‘
Spâ—‹
s.t.Hâ©¾0,HTâˆ—H=I,GTâˆ—G=I(4)
where 0<ğ‘â©½1,ğœ†is the hyper-parameter of the Schatten ğ‘-norm
term. The construction process of the tensor His shown in Figure
3. Remark 1 briefly describes the role of the tensor Schatten ğ‘-norm.
Remark 1 (Explanation of the tensor Schatten ğ‘-norm).
For the tensor H, as depicted in Fig 3, its ğ‘˜-th lateral slice Î˜ğ‘˜repre-
sents the relationship of ğ‘›samples with the ğ‘˜-th cluster across different
views. Multi-view clustering aims to harmonize the sample-cluster
relationships in different views, making H(1)
:,ğ‘˜,Â·Â·Â·,H(ğ‘£)
:,ğ‘˜as congruent
as possible. However, the clustering structures often vary significantly
across views. Applying the tensor Schatten ğ‘-norm to Hensures
thatÎ¦ğ‘˜maintains a spatially low-rank structure, leveraging comple-
mentary information across views and fostering consistency in the
clustering labels.
4.2 Optimization
Inspired by Augmented Lagrange Multiplier (ALM), we introduce
two auxiliary variables QandJand let H=Q,H=J, respec-
tively, where Qâ©¾0. Then, we rewrite the model as the following
KV
NTensorğ“—
ğ‡(ğŸ)ğ‡(ğŸ)ğ‡(ğ’—)ğ‡(ğŸ)
ğ‡(ğŸ)
ğ‡(ğ’—)K
Nğ“ğğ§ğ¬ğ¨ğ«ğ‚ğ¨ğ§ğ¬ğ­ğ«ğ®ğœğ­ğ¢ğ¨ğ§
ğš¯(ğ’Œ)Figure 3: Tensor construction
unconstrained problem:
minâˆ¥Sâˆ—Gâˆ’Hâˆ¥2
ğ¹+ğœ†âˆ¥Jâˆ¥ğ‘
Spâ—‹
+ğœ‡
2Hâˆ’Q+Y1
ğœ‡2
ğ¹+ğœŒ
2Hâˆ’J+Y2
ğœŒ2
ğ¹
s.t.Qâ©¾0,HTâˆ—H=I,GTâˆ—G=I(5)
whereY1,Y2represent Lagrange multipliers and ğœ‡,ğœŒare the
penalty parameters. The optimization process can therefore be
separated into four steps:
â€¢Solve Gwith fixed Q,H,J.(5) becomes:
min
GTâˆ—G=Iâˆ¥Sâˆ—Gâˆ’Hâˆ¥2
ğ¹,(6)
(6) is equivalent to the following in the frequency domain:
min
(G(ğ‘£))TG(ğ‘£)=Iğ‘‰âˆ‘ï¸
ğ‘£=1S(ğ‘£)G(ğ‘£)âˆ’H(ğ‘£)2
ğ¹, (7)
whereG=fft(G,[],3), and the others in the same way.
(7) can be reduced to:
min
(G(ğ‘£))TG(ğ‘£)=Itr
(G(ğ‘£))T(S(ğ‘£))TS(ğ‘£)G(ğ‘£)
âˆ’2tr
(G(ğ‘£))T(S(ğ‘£))TH(ğ‘£)
,(8)
 
1602Label Learning Method Based on Tensor Projection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(8) is equivalent to:
max
(G(ğ‘£))TG(ğ‘£)=Itr
(G(ğ‘£))TW(ğ‘£)
1G(ğ‘£)
+2tr
(G(ğ‘£))TW(ğ‘£)
2
,
(9)
whereW(ğ‘£)
1=ğ›½Iâˆ’(S(ğ‘£))TS(ğ‘£)andW(ğ‘£)
2=(S(ğ‘£))TH(ğ‘£),
whereğ›½is an arbitrary constant to ensure that W(ğ‘£)
1is a positive
definite matrix.
To solve (9), we introduce the following Theorem:
Theorem 1. [28] For the model:
max
GTG=Itr(GTBG)+ 2tr(GTK) (10)
Gis solved iteratively and Gâˆ—=UVT, where U,Vis from the SVD
decomposition: UXVT=BG+K.
According to Theorem 1, the G(ğ‘£)can be solved iteratively and
the solution is:
Gâˆ—(ğ‘£)=U(ğ‘£)(V(ğ‘£))T, (11)
whereU(ğ‘£)X(ğ‘£)(V(ğ‘£))T=W(ğ‘£)
1G(ğ‘£)+W(ğ‘£)
2.
â€¢Solve Hwith fixed Q,G,J.(5) becomes:
min
HTâˆ—H=Iâˆ¥Sâˆ—Gâˆ’Hâˆ¥2
ğ¹+ğœ‡
2Hâˆ’Q+Y1
ğœ‡2
ğ¹
+ğœŒ
2Hâˆ’J+Y2
ğœŒ2
ğ¹,(12)
(12) is equivalent to the following in the frequency domain:
min
(H(ğ‘£))TH(ğ‘£)=Iğ‘‰âˆ‘ï¸
ğ‘£=1S(ğ‘£)G(ğ‘£)âˆ’H(ğ‘£)2
ğ¹
+ğ‘‰âˆ‘ï¸
ğ‘£=1ğœ‡
2H(ğ‘£)âˆ’Q(ğ‘£)+Y(ğ‘£)
1
ğœ‡2
ğ¹
+ğ‘‰âˆ‘ï¸
ğ‘£=1ğœŒ
2H(ğ‘£)âˆ’J(ğ‘£)+Y(ğ‘£)
2
ğœŒ2
ğ¹,(13)
whereH=fft(H,[],3), and the others in the same way.
(13) can be reduced to:
min
(H(ğ‘£))TH(ğ‘£)=Iâˆ’2tr
(H(ğ‘£))TS(ğ‘£)G(ğ‘£)
âˆ’ğœ‡tr
(H(ğ‘£))TW(ğ‘£)
3
âˆ’ğœŒtr
(H(ğ‘£))TW(ğ‘£)
4
(14)
whereW(ğ‘£)
3=Q(ğ‘£)âˆ’Y(ğ‘£)
1
ğœ‡andW(ğ‘£)
4=J(ğ‘£)âˆ’Y(ğ‘£)
2
ğœŒ.
(14) can be reduced to:
max
(H(ğ‘£))TH(ğ‘£)=Itr
(H(ğ‘£))TA(ğ‘£)
(15)
whereA(ğ‘£)=2S(ğ‘£)G(ğ‘£)+ğœ‡W(ğ‘£)
3+ğœŒW(ğ‘£)
4.
To solve (15), we introduce the following Theorem:Theorem 2. Given GandP, where G(G)T=IandPhas the
singular value decomposition P=Î›S(V)T, then the optimal solution
of
max
G(G)T=Itr(GP) (16)
isGâˆ—=V[I,0](Î›)T.
Proof. From the SVD P=Î›S(V)Tand together with (10), it is
evident that
tr(GP)=tr(GÎ›S(V)T)
=tr(S(V)TGÎ›)
=tr(SH)
=âˆ‘ï¸
ğ‘–ğ‘ ğ‘–ğ‘–â„ğ‘–ğ‘–,(17)
where H=(V)TGÎ›,ğ‘ ğ‘–ğ‘–andâ„ğ‘–ğ‘–are the(ğ‘–,ğ‘–)elements of SandH,
respectively. It can be easily verified that H(H)T=I, where Iis an
identity matrix. Therefore âˆ’1â©½â„ğ‘–ğ‘–â©½1andğ‘ ğ‘–ğ‘–â©¾0, Thus we have:
tr(GP)=âˆ‘ï¸
ğ‘–ğ‘ ğ‘–ğ‘–â„ğ‘–ğ‘–â©½âˆ‘ï¸
ğ‘–ğ‘ ğ‘–ğ‘–. (18)
The equality holds when His an identity matrix. tr(GP)reaches
the maximum when H=[I,0]. â–¡
According to Theorem 2 the solution of (15) is:
H(ğ‘£)=ğš²(ğ‘£)(ğ‘½(ğ‘£))T(19)
where ğš²(ğ‘£)andğ‘½(ğ‘£)can be obtained by SVD A(ğ‘£)=ğš²(ğ‘£)X(ğ‘½(ğ‘£))T
â€¢Solve Qwith fixed H,G,J.(5) becomes:
min
Qâ©¾0=ğœ‡
2Hâˆ’Q+Y1
ğœ‡2
ğ¹, (20)
(20) is obviously equivalent to:
min
Qâ©¾0ğœ‡
2(H+Y1
ğœ‡)âˆ’Q2
ğ¹(21)
According to [30], the solution of (21) is:
Q=
H+Y1
ğœ‡
+(22)
â€¢Solve Jwith fixed H,G,Q.(5) becomes:
min=ğœ†âˆ¥Jâˆ¥ğ‘
Spâ—‹+ğœŒ
2Hâˆ’J+Y2
ğœŒ2
ğ¹, (23)
We can deduce
Jâˆ—=arg min1
2H+Y2
ğœŒâˆ’J2
ğ¹+ğœ†
ğœŒâˆ¥Jâˆ¥ğ‘
Spâ—‹, (24)
which has a closed-form solution as Lemma 1 [3]:
Lemma 1. LetZâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3have a t-SVDZ=Uâˆ—Sâˆ—VT,
then the optimal solution for
min
X1
2âˆ¥Xâˆ’Zâˆ¥2
ğ¹+ğœâˆ¥Xâˆ¥ğ‘
Spâ—‹. (25)
isXâˆ—=Î“ğœ(Z)=Uâˆ—ifft(ğ‘ƒğœ(Z))âˆ—VT, whereğ‘ƒğœ(Z)is an f-diagonal
3rd-order tensor, whose diagonal elements can be found by using the
GST algorithm introduced in [3].
 
1603KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, & Deyan Xie
Algorithm 1 Label Learning Method Based on Tensor Projection
(LLMTP)
Input: Data matrices{X(ğ‘£)}ğ‘‰
ğ‘£=1âˆˆRğ‘Ã—ğ‘‘ğ‘£; anchors numbers ğ‘š;
cluster number ğ¾.
Output: Cluster labels Yof each data points.
1:Initialize:ğœ‡=10âˆ’5,ğœŒ=10âˆ’5,ğœ‚=1.5,Y1=0,Y2=0,Q(ğ‘£)
is identity matrix;
2:Compute graph matrix S(ğ‘£)of each views;
3:while not condition do
4: Update G(ğ‘£)by solving (11);
5: Update H(ğ‘£)by solving (19);
6: Update Q(ğ‘£)by solving (22);
7: Update Jby using (24);
8: Update Y1,Y2,ğœ‡andğœŒ:Y1=Y1+ğœ‡(Hâˆ’Q),Y2=
Y2+ğœ‡(Hâˆ’J),ğœ‡=min(ğœ‚ğœ‡,1013),ğœŒ=min(ğœ‚ğœŒ,1013);
9:end while
10:Calculate the ğ¾clusters by using
H=Ãğ‘‰
ğ‘£=1H(ğ‘£)/ğ‘‰;
11:return Clustering result (The position of the largest element
in each row of the indicator matrix is the label of the corre-
sponding sample).
The solution of (24) is:
Jâˆ—=Î“ğœ†
ğœŒ(H+Y2
ğœŒ). (26)
Finally, the optimization procedure for Label Learning Method
Based on Tensor Projection (LLMTP) is outlined in Algorithm 1.
4.3 Complexity of LLMTP
For the computational complexity, the process of constructing S
has a computational complexity of O(ğ‘‰ğ‘›ğ‘šğ‘‘+ğ‘‰ğ‘›ğ‘šğ‘™ğ‘œğ‘”(ğ‘š)). When
updating the four variables, G,H,QandJ, their respective com-
putational complexities are O(ğ‘‰ğ‘šğ‘˜2+ğ‘‰ğ‘›ğ‘˜ğ‘™ğ‘œğ‘”(ğ‘‰ğ‘›)),O(ğ‘‰ğ‘›ğ‘˜2+
ğ‘‰ğ‘šğ‘˜ğ‘™ğ‘œğ‘”(ğ‘‰ğ‘š)),O(ğ‘‰ğ‘›ğ‘˜)andO(2ğ‘‰ğ‘›ğ‘˜ğ‘™ğ‘œğ‘”(ğ‘‰ğ‘˜)+ğ‘‰2ğ‘˜ğ‘›), whereğ‘‰,ğ‘š,
ğ‘˜,ğ‘›,ğ‘‘are numbers of views, anchors, clusters, samples and sum of
all features.
5 EXPERIMENTS
In this section, we demonstrate the performance of our proposed
method through extensive experiments. We evaluate the clustering
performance by 3 metrics used widely, i.e., 1) ACC; 2) NMI; 3)
Purity. The higher the value the better the clustering results for
all metrics mentioned above. To ensure reliability, we conducted
10 independent trials for each method, recording the mean and
variance of the results. Experiments using the MSRC, HandWritten4,
Mnist4, and Scene15 datasets were conducted on a laptop equipped
with an Intel Core i5-8300H CPU and 16 GB RAM, utilizing Matlab
R2018b. In contrast, the Reuters and NoisyMnist were processed on
a standard Windows 10 server, featuring dual Intel(R) Xeon(R) Gold
6230 CPUs at 2.10 GHz and 128 GB RAM, with MATLAB R2020a.Table 1: Multi-view datasets used in our experiments
#Dataset
#Samples #View #Class #Feature
MSRC
210 5 7 24, 576, 512, 256, 254
HandWritten4 2000 4 10 76, 216, 47, 6
Mnist4 4000 3 4 30, 9, 30
Scene15 4485 3 15 1800, 1180, 1240
Reuters 18758 5 6 21531, 24892, 34251, 15506, 11547
The datasets employed in our experiments are detailed in Table
1. We compare our approach against the following state-of-the-
art methods: CSMSC [ 12], GMC [ 19], ETLMSC [ 24], LMVSC [ 6],
FMCNOF [30], SFMC [10], and FPMVS-CAG [22].
5.1 Clustering Performance
Table 2 and Table 3 show the clustering performance of the proposed
model on different datasets. The optimal performance is indicated
bybold, and the sub-optimal performance is indicated by underline .
It can be seen from the three tables that the method proposed in this
paper is superior to other comparison methods. Among compari-
son methods, ETLMSCâ€™s performance is relatively suboptimal. It is
worth mentioning that this method is a tensor-based spectral clus-
tering method, while others are non-tensor methods. It may means
that tensor-based methods have a certain degree of performance
improvement compared with non-tensor methods.
5.2 Parameter Analysis
The hyper-parameters in the model are analyzed experimentally,
including the anchor rate (affecting the constructed anchor graph),
the value of ğ‘in the tensor Schatten ğ‘-norm, and the value of the
coefficientğœ†of the regular term of the tensor Schatten ğ‘-norm.
The effect of anchor rate on clustering performance is shown in
Figure 4. We conducted experiments on a small dataset, MSRC, and
a medium-sized dataset, Mnist4. They obtained the best indicators
at anchor rates of 0.7 and 0.4, respectively. The experimental re-
sults show that larger anchor rate is not always better. In addition,
according to common sense, the larger the anchor rate, the more
time and space it takes to construct the anchor graph. Figure 5
confirms this statement. It can be found that with the increase of
the anchor rate, the running time of the algorithm also increases
in an approximate linear relationship. It is more obvious on the
Mnist4. It also shows the importance of the choice of anchor rate
on the other hand. In general, for large data sets, we tend to use
smaller anchor rates.
The influence of the value ğ‘on the clustering performance is
shown in Figure 6. We start from ğ‘=0.1and conduct experiments
at intervals of 0.1 until ğ‘=1. It can be seen from the figure that
the best clustering performance is achieved on MSRC and Mnist4
whenğ‘is 0.9 and 0.2 respectively. It can also be shown from the
experimental point of view that, compared with the nuclear norm
(whereğ‘=1), Schatten ğ‘-norm minimization can ensure that the
rank of the tensor is closer to the target rank. Thus, the comple-
mentary information between different views can be better mined,
and better clustering performance can be obtained.
Figure 7 shows the effect of different values of ğœ†on clustering
performance. Among them, when ğœ†achieves 51 and 50, the best
 
1604Label Learning Method Based on Tensor Projection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Clustering performance on MSRC, HandWritten4, Mnist4 and Scene15
Datasets MSRC HandWritten4
Metrices ACC NMI Purity ACC NMI Purity
CSMSC 0.758Â±0.007 0.735Â±0.010 0.793Â±0.008 0.806Â±0.001 0.793Â±0.001 0.867Â±0.001
GMC 0.895Â±0.000 0.809Â±0.000 0.895Â±0.000 0.861Â±0.000 0.859Â±0.000 0.861Â±0.000
ETLMSC 0.962Â±0.000 0.937Â±0.000 0.962Â±0.000 0.938Â±0.001 0.893Â±0.001 0.938Â±0.001
LMVSC 0.814Â±0.000 0.717Â±0.000 0.814Â±0.000 0.904Â±0.000 0.831Â±0.000 0.904Â±0.000
FMCNOF 0.440Â±0.039 0.345Â±0.046 0.449Â±0.042 0.385Â±0.092 0.370Â±0.092 0.386Â±0.090
SFMC 0.810Â±0.000 0.721Â±0.000 0.810Â±0.000 0.853Â±0.000 0.871Â±0.000 0.873Â±0.000
FPMVS-CAG 0.786Â±0.000 0.686Â±0.000 0.786Â±0.000 0.744Â±0.000 0.753Â±0.000 0.744Â±0.000
Ours 0.986Â±0.000 0.971Â±0.000 0.986Â±0.000 0.963Â±0.000 0.937Â±0.000 0.963Â±0.000
Datasets Mnist4 Scene15
Metrices ACC NMI Purity ACC NMI Purity
CSMSC 0.641Â±0.000 0.601Â±0.010 0.728Â±0.008 0.334Â±0.008 0.313Â±0.005 0.378Â±0.003
GMC 0.920Â±0.000 0.807Â±0.000 0.920Â±0.000 0.140Â±0.000 0.058Â±0.000 0.146Â±0.000
ETLMSC 0.934Â±0.000 0.847Â±0.000 0.934Â±0.000 0.709Â±0.000 0.774Â±0.000 0.887Â±0.000
LMVSC 0.892Â±0.000 0.726Â±0.000 0.892Â±0.000 0.355Â±0.000 0.331Â±0.000 0.399Â±0.000
FMCNOF 0.697Â±0.119 0.490Â±0.102 0.711Â±0.096 0.218Â±0.033 0.166Â±0.022 0.221Â±0.029
SFMC 0.916Â±0.000 0.797Â±0.000 0.916Â±0.000 0.344Â±0.000 0.522Â±0.000 0.347Â±0.000
FPMVS-CAG 0.885Â±0.000 0.715Â±0.000 0.885Â±0.000 0.619Â±0.000 0.567Â±0.000 0.651Â±0.000
Ours 0.982Â±0.000 0.933Â±0.000 0.982Â±0.000 0.800Â±0.000 0.835Â±0.000 0.808Â±0.000
Table 3: Clustering performance and running time (sec.) on
Reuters ("OM" means out of memory, and "-" means the algo-
rithm ran for more than three hours.)
Datasets Reuters
Metrices A
CC NMI Purity Time
CSMSC OM
OM OM OM
GMC -
- - -
ETLMSC OM
OM OM OM
LMVSC 0.589Â±0.000
0.335Â±0.000 0.615Â±0.000 150.51
FMCNOF 0.343Â±0.000
0.125Â±0.000 0.358Â±0.000 186.45
SFMC 0.602Â±0.000 0.354Â±0.000 0.604Â±0.000
494.68
FPMVS-CAG 0.526Â±0.000
0.323Â±0.000 0.603Â±0.000 2252.20
Ours 0.770Â±0.000
0.690Â±0.000 0.783Â±0.000 630.05
clustering results are obtained on MSRC and Mnist4, respectively.
Moreover, it can be found from the figure that MSRC is more sensi-
tive to the value of ğœ†than Mnist4. When conducting experiments on
other small and medium-sized datasets, the value of ğœ†is generally
adjusted to a small range around 50.
5.3 Experiments of Convergence
We tested the convergence of the model, as shown in Figure 8. Since
we introduced two auxiliary variables QandJwhen solving the
model, we judged whether the model could gradually converge after
enough iterations by calculating the difference between variables
HandQand the difference between HandJin Eq. (5). As shown
in Figure 8, both differences approach 0 approximately within 50
iterations, so it can be judged that the model proposed in this paperis convergent. At the same time, we also draw the change curve of
clustering index ACC with the number of iterations. It can be found
from the figure that ACC also converges with the convergence of
the model.
5.4 Visualization of Experimental Results
We conducted a visualization experiment on the label matrix H
after final fusion according to Algorithm 1, as shown in Figure
9. In the cluster label matrix Hafter fusion, the position where
the maximum value of each row is located can be regarded as the
cluster to which the sample of that row belongs. It can be found
that MSRC, Mnist4 and HandWritten4 are clearly divided into 7
clusters, 4 clusters and 10 clusters, respectively.
5.5 Ablation Experiments
In this section, we verify the effect of third-order tensor projection
and the Schatten ğ‘-norm on the indicator tensor on clustering
performance.
Table 4: The contribution of each component
Comp
onents Reuters
S.P
. T.P. A
CC NMI Purity
âœ˜
âœ” 0.395
0.145 0.473
âœ” âœ˜ 0.766
0.676 0.777
âœ” âœ” 0.770
0.690 0.783
 
1605KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, & Deyan Xie
(a) MSRC
(b) Mnist4
Figure 4: Clustering performance with different anchor rate
on MSRC and Mnist4
(a) MSRC (b) Mnist4
Figure 5: Running time (sec.) with different anchor rate on
MSRC and Mnist4
In Table 4, S.P. means using tensor Schatten ğ‘-norm on the
indicator tensor. T.P. means using third-order tensor projection.
According to Table 4, it can be found that the tensor Schatten
ğ‘-norm on indicator tensor is very important for the clustering
performance. In addition, extending the matrix projection to third-
order tensor projection also improves the clustering performance.
And the S.P. is more important than T.P. according to the table.
æŒ‡æ ‡ ACC NMI Purity æŒ‡æ ‡ ACC NMI
0.1 0.98225 0.933267 0.98225 0.1 0.8 0.797155
0.2 0.98225 0.939078 0.98225 0.2 0.780952 0.803499
0.3 0.97025 0.907059 0.97025 0.3 0.847619 0.799247
0.4 0.9675 0.896753 0.9675 0.4 0.819048 0.781354
0.5 0.96525 0.901904 0.96525 0.5 0.904762 0.844011
0.6 0.96675 0.904645 0.96675 0.6 0.87619 0.829117
0.7 0.97 0.912215 0.97 0.7 0.961905 0.92943
0.8 0.9705 0.914459 0.9705 0.8 0.857143 0.762216
0.9 0.972 0.912557 0.972 0.9 0.985714 0.970882
1 0.956 0.882517 0.956 10.709524 0.655446
00.10.20.30.40.50.60.70.80.91
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Clustering Performance
value of p
 ACC
 NMI
 Purity
(a) MSRC
(b) Mnist4
00.10.20.30.40.50.60.70.80.91
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Clustering Performance
value of p
 ACC
 NMI
 PurityFigure 6: Clustering performance with different ğ‘on MSRC
and Mnist4
(a) MSRC
(b) Mnist4
Figure 7: Clustering performance with different ğœ†on MSRC
and Mnist4
6 CONCLUSIONS
In this paper, we propose a label learning method based on tensor
projection (LLMTP). LLMTP projects the anchor graph space into
the label space and thus we can get the clustering results from
the label tensor directly. Meanwhile, in order to make full use of
the complementary information and spatial structure information
 
1606Label Learning Method Based on Tensor Projection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) MSRC
(b) Mnist4
(c) HandWritten4
Figure 8: Convergence experiment on MSRC, Mnist4 and
HandWritten4
between different views, we extend the view-by-view matrix pro-
jection process to tensor projection processing multi-view data
directly, and use the tensor Schatten ğ‘-norm to make the clustering
label matrix of each view tend to be consistent. Extensive experi-
ments have proved the effectiveness of the proposed method.
ACKNOWLEDGMENTS
This work is supported by the National Natural Science Foundation
of China under Grants 62176203, the Natural Science Foundation of
Shandong Province under Grant ZR202102180986, the Fundamental
Research Funds for the Central Universities and the Innovation
Fund of Xidian University (YJSJ24017), the Guangxi Key Laboratory
(a) MSRC
(b) Mnist4
(c) HandWritten4Figure 9: Label visualization on MSRC, Mnist4 and Hand-
Written4
of Digital Infrastructure under Grant GXDIOP2023010, the Natural
Science Foundation of Guangdong Province, 2023A1515011845.
 
1607KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, & Deyan Xie
REFERENCES
[1]Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human
detection. In 2005 IEEE computer society conference on computer vision and pattern
recognition (CVPRâ€™05), Vol. 1. Ieee, 886â€“893.
[2]Quanxue Gao, Zhizhen Wan, Ying Liang, Qianqian Wang, Yang Liu, and Ling
Shao. 2020. Multi-view projected clustering with graph learning. Neural Networks
126 (2020), 335â€“346. https://doi.org/10.1016/J.NEUNET.2020.03.020
[3]Quanxue Gao, Pu Zhang, Wei Xia, Deyan Xie, Xinbo Gao, and Dacheng Tao.
2021. Enhanced Tensor RPCA and its Application. IEEE Transactions on Pattern
Analysis and Machine Intelligence 43, 6 (2021), 2133â€“2140.
[4]Zhezheng Hao, Zhoumin Lu, Guoxu Li, Feiping Nie, Rong Wang, and Xuelong Li.
2024. Ensemble Clustering With Attentional Representation. IEEE Trans. Knowl.
Data Eng. 36, 2 (2024), 581â€“593. https://doi.org/10.1109/TKDE.2023.3292573
[5]Yanfang He and Umi Kalsom Yusof. 2023. Self-Weighted Graph-Based Framework
for Multi-View Clustering. IEEE Access 11 (2023), 30197â€“30207. https://doi.org/
10.1109/ACCESS.2023.3260971
[6]Zhao Kang, Wangtao Zhou, Zhitong Zhao, Junming Shao, Meng Han, and Zenglin
Xu. 2020. Large-scale multi-view subspace clustering in linear time. In Proceedings
of the AAAI conference on Artificial Intelligence. 4412â€“4419.
[7]Misha E Kilmer and Carla D Martin. 2011. Factorization strategies for third-order
tensors. Linear Algebra Appl. 435, 3 (2011), 641â€“658.
[8]Jing Li, Quanxue Gao, Qianqian Wang, Ming Yang, and Wei Xia. 2023. Orthogonal
Non-negative Tensor Factorization based Multi-view Clustering. In Thirty-seventh
Conference on Neural Information Processing Systems.
[9]Jinghao Li, Xiaoqian Zhang, Jing Wang, Xiao Wang, Zhen Tan, and Huaijiang Sun.
2023. Projection-based coupled tensor learning for robust multi-view clustering.
Inf. Sci. 632 (2023), 664â€“677. https://doi.org/10.1016/J.INS.2023.03.072
[10] Xuelong Li, Han Zhang, Rong Wang, and Feiping Nie. 2022. Multiview Clustering:
A Scalable and Parameter-Free Bipartite Graph Fusion Method. IEEE Transactions
on Pattern Analysis and Machine Intelligence 44, 1 (2022), 330â€“344.
[11] Han Lu, Huafu Xu, Qianqian Wang, Quanxue Gao, Ming Yang, and Xinbo Gao.
2024. Efficient Multi-View -Means for Image Clustering. IEEE Trans. Image
Process. 33 (2024), 273â€“284. https://doi.org/10.1109/TIP.2023.3340609
[12] Shirui Luo, Changqing Zhang, Wei Zhang, and Xiaochun Cao. 2018. Consistent
and specific multi-view subspace clustering. In Thirty-second AAAI conference on
artificial intelligence.
[13] Shikun Mei, Wenhui Zhao, Quanxue Gao, Ming Yang, and Xinbo Gao. 2023. Joint
feature selection and optimal bipartite graph learning for subspace clustering.
Neural Networks 164 (2023), 408â€“418. https://doi.org/10.1016/J.NEUNET.2023.04.
044
[14] Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: A
holistic representation of the spatial envelope. International journal of computer
vision 42 (2001), 145â€“175.
[15] Xiaoshuang Sang, Jianfeng Lu, and Hong Lu. 2022. Consensus graph learning
for auto-weighted multi-view projection clustering. Inf. Sci. 609 (2022), 816â€“837.
https://doi.org/10.1016/J.INS.2022.07.119
[16] Chuan Tang, Kun Sun, Chang Tang, Xiao Zheng, Xinwang Liu, Jun-Jie Huang,
and Wei Zhang. 2023. Multi-view subspace clustering via adaptive graph learning
and late fusion alignment. Neural Networks 165 (2023), 333â€“343. https://doi.org/
10.1016/J.NEUNET.2023.05.019
[17] Beilei Wang, Yun Xiao, Zhihui Li, Xuanhong Wang, Xiaojiang Chen, and Dingyi
Fang. 2020. Robust Self-Weighted Multi-View Projection Clustering. In The Thirty-
Fourth AAAI Conference on Artificial Intelligence, 2020. AAAI Press, 6110â€“6117.
https://doi.org/10.1609/AAAI.V34I04.6075
[18] Haiyue Wang, Quan Wang, Qiguang Miao, and Xiaoke Ma. 2024. Joint learning
of data recovering and graph contrastive denoising for incomplete multi-view
clustering. Inf. Fusion 104 (2024), 102155. https://doi.org/10.1016/J.INFFUS.2023.
102155
[19] Hao Wang, Yan Yang, and Bing Liu. 2019. GMC: Graph-based multi-view cluster-
ing.IEEE Transactions on Knowledge and Data Engineering 32, 6 (2019), 1116â€“1129.
[20] Haiyue Wang, Wensheng Zhang, and Xiaoke Ma. 2022. Clustering of noised and
heterogeneous multi-view data with graph learning and projection decomposi-
tion. Knowl. Based Syst. 255 (2022), 109736. https://doi.org/10.1016/J.KNOSYS.
2022.109736
[21] Siwei Wang, Xinwang Liu, Suyuan Liu, Jiaqi Jin, Wenxuan Tu, Xinzhong Zhu, and
En Zhu. 2022. Align then Fusion: Generalized Large-scale Multi-view Clusteringwith Anchor Matching Correspondences. In Advances in Neural Information
Processing Systems 35, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,
K. Cho, and A. Oh (Eds.).
[22] Siwei Wang, Xinwang Liu, Xinzhong Zhu, Pei Zhang, Yi Zhang, Feng Gao, and En
Zhu. 2021. Fast parameter-free multi-view subspace clustering with consensus
anchor guidance. IEEE Transactions on Image Processing 31 (2021), 556â€“568.
[23] Ziyu Wang, Lusi Li, Xin Ning, Wenkai Tan, Yongxin Liu, and Houbing Song. 2024.
Incomplete multi-view clustering via structure exploration and missing-view
inference. Inf. Fusion 103 (2024), 102123. https://doi.org/10.1016/J.INFFUS.2023.
102123
[24] Jianlong Wu, Zhouchen Lin, and Hongbin Zha. 2019. Essential Tensor Learning
for Multi-View Spectral Clustering. IEEE Transactions on Image Processing 28, 12
(2019), 5910â€“5922.
[25] Wei Xia, Quanxue Gao, Qianqian Wang, Xinbo Gao, Chris Ding, and Dacheng
Tao. 2023. Tensorized Bipartite Graph Learning for Multi-View Clustering. IEEE
Trans. Pattern Anal. Mach. Intell. 45, 4 (2023), 5187â€“5202. https://doi.org/10.1109/
TPAMI.2022.3187976
[26] Qingjiang Xiao, Shiqiang Du, Kaiwu Zhang, Jinmei Song, and Yixuan Huang.
2023. Adaptive sparse graph learning for multi-view spectral clustering. Appl.
Intell. 53, 12 (2023), 14855â€“14875. https://doi.org/10.1007/S10489-022-04267-9
[27] Yuan Xie, Shuhang Gu, Yan Liu, Wangmeng Zuo, Wensheng Zhang, and Lei
Zhang. 2016. Weighted Schatten ğ‘-norm minimization for image denoising and
background subtraction. IEEE Trans. Image Process. 25, 10 (2016), 4842â€“4857.
[28] Huiling Xu, Xiangdong Zhang, Wei Xia, Quanxue Gao, and Xinbo Gao. 2020. Low-
rank tensor constrained co-regularized multi-view spectral clustering. Neural
Networks 132 (2020), 245â€“252.
[29] Ben Yang, Jinghan Wu, Xuetao Zhang, Xinhu Zheng, Feiping Nie, and Badong
Chen. 2024. Discrete correntropy-based multi-view anchor-graph clustering. Inf.
Fusion 103 (2024), 102097. https://doi.org/10.1016/J.INFFUS.2023.102097
[30] Ben Yang, Xuetao Zhang, Feiping Nie, Fei Wang, Weizhong Yu, and Rong Wang.
2021. Fast Multi-View Clustering via Nonnegative and Orthogonal Factorization.
IEEE Transactions on Image Processing 30 (2021), 2575â€“2586.
[31] Haizhou Yang, Quanxue Gao, Wei Xia, Ming Yang, and Xinbo Gao. 2022. Mul-
tiview Spectral Clustering With Bipartite Graph. IEEE Trans. Image Process. 31
(2022), 3591â€“3605. https://doi.org/10.1109/TIP.2022.3171411
[32] Hui Yu, Mingjing Li, Hong-Jiang Zhang, and Jufu Feng. 2002. Color texture mo-
ments for content-based image retrieval. In Proceedings. International Conference
on Image Processing, Vol. 3. IEEE, 929â€“932.
[33] Yu Yun, Jing Li, Quanxue Gao, Ming Yang, and Xinbo Gao. 2023. Low-rank
discrete multi-view spectral clustering. Neural Networks 166 (2023), 137â€“147.
https://doi.org/10.1016/J.NEUNET.2023.06.038
[34] Zhiyuan Zha, Xin Yuan, Bihan Wen, Jiantao Zhou, Jiachao Zhang, and Ce Zhu.
2020. A benchmark for sparse coding: When group sparsity meets rank mini-
mization. IEEE Transactions on Image Processing 29 (2020), 5094â€“5109.
[35] Kun Zhan, Changqing Zhang, Junpeng Guan, and Junsheng Wang. 2018. Graph
Learning for Multiview Clustering. IEEE Trans. Cybern. 48, 10 (2018), 2887â€“2895.
https://doi.org/10.1109/TCYB.2017.2751646
[36] Zihao Zhang, Qianqian Wang, Zhiqiang Tao, Quanxue Gao, and Wei Feng. 2023.
Dropping Pathways Towards Deep Multi-View Graph Subspace Clustering Net-
works. In Proceedings of the 31st ACM International Conference on Multimedia.
ACM, 3259â€“3267. https://doi.org/10.1145/3581783.3612332
[37] Mingyu Zhao, Weidong Yang, and Feiping Nie. 2023. Auto-weighted orthogonal
and nonnegative graph reconstruction for multi-view clustering. Inf. Sci. 632
(2023), 324â€“339. https://doi.org/10.1016/J.INS.2023.03.016
[38] Wenhui Zhao, Quanxue Gao, Shikun Mei, and Ming Yang. 2023. Contrastive
self-representation learning for data clustering. Neural Networks 167 (2023),
648â€“655. https://doi.org/10.1016/J.NEUNET.2023.08.050
[39] Qian Zhou, Haizhou Yang, and Quanxue Gao. 2022. Low-rank constraint bipartite
graph learning. Neurocomputing 511 (2022), 426â€“436. https://doi.org/10.1016/J.
NEUCOM.2022.09.002
A ADDITIONS TO THE EXPERIMENTS
We provide more detailed clustering metrics on four datasets on
Table 5, including MSRC, HandWritten4, Mnist4 and Scene15.
 
1608Label Learning Method Based on Tensor Projection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 5: Clustering performance on MSRC, HandWritten4, Mnist4 and Scene15. (The best result is in bold, and the second-best
result is underlined.)
Dataset MSRC
Metrics
ACC NMI Purity PER REC F-score ARI
CSMSC
0.758Â±0.007 0.735Â±0.010 0.793Â±0.008 0.736Â±0.014 0.673Â±0.008 0.703Â±0.010 0.653Â±0.012
GMC 0.895Â±0.000 0.809Â±0.000 0.895Â±0.000 0.788Â±0.000 0.814Â±0.000 0.801Â±0.000 0.768Â±0.000
ETLMSC 0.962Â±0.000 0.937Â±0.000 0.962Â±0.000 0.926Â±0.000 0.931Â±0.000 0.928Â±0.000 0.917Â±0.000
LMVSC
0.814Â±0.000 0.717Â±0.000 0.814Â±0.000 0.676Â±0.000 0.692Â±0.000 0.684Â±0.000 0.632Â±0.000
FMCNOF 0.440Â±0.039 0.345Â±0.046 0.449Â±0.042 0.290Â±0.036 0.606Â±0.074 0.395Â±0.036 0.249Â±0.051
SFMC 0.810Â±0.000 0.721Â±0.000 0.810Â±0.000 0.657Â±0.000 0.782Â±0.000 0.714Â±0.000 0.663Â±0.000
FPMVS-CAG 0.786Â±0.000 0.686Â±0.000 0.786Â±0.000 0.684Â±0.000 0.642Â±0.000 0.731Â±0.000 0.629Â±0.000
ours 0.986Â±0.000 0.971Â±0.000 0.986Â±0.000 0.970Â±0.000 0.972Â±0.000 0.971Â±0.000 0.967Â±0.000
Dataset HandW
ritten4
Metrics
ACC NMI Purity PER REC F-score ARI
CSMSC
0.806Â±0.001 0.793Â±0.001 0.867Â±0.001 0.778Â±0.001 0.743Â±0.001 0.760Â±0.001 0.733Â±0.001
GMC 0.861Â±0.000 0.859Â±0.000 0.861Â±0.000 0.799Â±0.000 0.855Â±0.000 0.826Â±0.000 0.806Â±0.000
ETLMSC 0.938Â±0.001 0.893Â±0.001 0.938Â±0.001 0.886Â±0.001 0.890Â±0.001
0.888Â±0.001 0.876Â±0.001
LMVSC
0.904Â±0.000 0.831Â±0.000 0.904Â±0.000 0.819Â±0.000 0.825Â±0.000 0.822Â±0.000 0.802Â±0.000
FMCNOF 0.385Â±0.092 0.370Â±0.092 0.386Â±0.093 0.254Â±0.077 0.688Â±0.101 0.360Â±0.070 0.250Â±0.097
SFMC 0.853Â±0.000 0.871Â±0.000 0.873Â±0.000 0.775Â±0.000 0.910Â±0.000 0.837Â±0.000
0.817Â±0.000
FPMVS-CAG 0.744Â±0.000 0.753Â±0.000 0.744Â±0.000 0.681Â±0.000 0.636Â±0.000 0.762Â±0.000 0.642Â±0.000
ours 0.963Â±0.000 0.937Â±0.000 0.963Â±0.000 0.924Â±0.000 0.930Â±0.000 0.927Â±0.000 0.919Â±0.000
Dataset Mnist4
Metrics
ACC NMI Purity PER REC F-score ARI
CSMSC
0.641Â±0.000 0.601Â±0.010 0.728Â±0.008 0.607Â±0.014 0.767Â±0.008 0.677Â±0.010 0.553Â±0.012
GMC 0.920Â±0.000 0.807Â±0.000 0.920Â±0.000 0.853Â±0.000 0.861Â±0.000 0.857Â±0.000 0.809Â±0.000
ETLMSC 0.934Â±0.000 0.847Â±0.000 0.934Â±0.000 0.878Â±0.000 0.885Â±0.000 0.881Â±0.000 0.842Â±0.000
LMVSC
0.892Â±0.000 0.726Â±0.000 0.892Â±0.000 0.808Â±0.000 0.812Â±0.000 0.810Â±0.000 0.747Â±0.000
FMCNOF 0.697Â±0.119 0.490Â±0.102 0.711Â±0.096 0.558Â±0.118 0.683Â±0.073 0.611Â±0.095 0.460Â±0.145
SFMC 0.916Â±0.000 0.797Â±0.000 0.916Â±0.000 0.846Â±0.000 0.855Â±0.000 0.850Â±0.000 0.800Â±0.000
FPMVS-CAG 0.885Â±0.000 0.715Â±0.000 0.885Â±0.000 0.800Â±0.000 0.795Â±0.000 0.815Â±0.000 0.733Â±0.000
ours 0.982Â±0.000 0.933Â±0.000 0.982Â±0.000 0.964Â±0.000 0.965Â±0.000 0.965Â±0.000 0.953Â±0.000
Dataset Scene15
Metrics
ACC NMI Purity PER REC F-score ARI
CSMSC
0.334Â±0.008 0.313Â±0.005 0.378Â±0.003 0.227Â±0.003 0.239Â±0.002 0.233Â±0.003 0.174Â±0.003
GMC 0.140Â±0.000 0.058Â±0.000 0.146Â±0.000 0.071Â±0.000 0.893Â±0.000 0.131Â±0.000 0.004Â±0.000
ETLMSC 0.709Â±0.000 0.774Â±0.000 0.887Â±0.000 0.665Â±0.000 0.688Â±0.000
0.676Â±0.000 0.652Â±0.000
LMVSC
0.355Â±0.000 0.331Â±0.000 0.399Â±0.000 0.238Â±0.000 0.244Â±0.000 0.241Â±0.000 0.184Â±0.000
FMCNOF 0.218Â±0.033 0.166Â±0.022 0.221Â±0.029 0.117Â±0.016 0.471Â±0.062 0.186Â±0.019 0.085Â±0.026
SFMC 0.344Â±0.000 0.522Â±0.000 0.347Â±0.000 0.284Â±0.000 0.171Â±0.000 0.855Â±0.000 0.191Â±0.000
FPMVS-CAG 0.619Â±0.000 0.567Â±0.000 0.651Â±0.000 0.479Â±0.000 0.481Â±0.000 0.477Â±0.000 0.441Â±0.000
ours 0.800Â±0.000 0.835Â±0.000 0.808Â±0.000 0.735Â±0.000 0.739Â±0.000 0.737Â±0.000 0.717Â±0.000
 
1609