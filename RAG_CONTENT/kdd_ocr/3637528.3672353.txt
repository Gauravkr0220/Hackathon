Decision Focused Causal Learning for Direct Counterfactual
Marketing Optimization
Hao Zhouâˆ—
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
Meituan
Beijing, China
zhouhao29@meituan.comRongxiao Huangâˆ—
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
rxhuang@smail.nju.edu.cnShaoming Li
Meituan
Beijing, China
shaoming.li@outlook.com
Guibin Jiang
Meituan
Beijing, China
jiangguibin@meituan.comJiaqi Zhengâ€ 
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
jzheng@nju.edu.cnBing Cheng
Meituan
Beijing, China
bing.cheng@meituan.com
Wei Lin
Meituan
Beijing, China
lwsaviola@163.com
Abstract
Marketing optimization plays an important role to enhance user
engagement in online Internet platforms. Existing studies usually
formulate this problem as a budget allocation problem and solve it
by utilizing two fully decoupled stages, i.e., machine learning (ML)
and operation research (OR). However, the learning objective in
ML does not take account of the downstream optimization task in
OR, which causes that the prediction accuracy in ML may be not
positively related to the decision quality.
Decision Focused Learning (DFL) integrates ML and OR into
an end-to-end framework, which takes the objective of the down-
stream task as the decision loss function and guarantees the con-
sistency of the optimization direction between ML and OR. How-
ever, deploying DFL in marketing is non-trivial due to multiple
technological challenges. Firstly, the budget allocation problem in
marketing is a 0-1 integer stochastic programming problem and
the budget is uncertain and fluctuates a lot in real-world settings,
which is beyond the general problem background in DFL. Secondly,
âˆ—Both authors contributed equally to this research.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672353the counterfactual in marketing causes that the decision loss can-
not be directly computed and the optimal solution can never be
obtained, both of which disable the common gradient-estimation
approaches in DFL. Thirdly, the OR solver is called frequently to
compute the decision loss during model training in DFL, which
produces huge computational cost and cannot support large-scale
training data. In this paper, we propose a decision focused causal
learning framework (DFCL) for direct counterfactual marketing
optimization, which overcomes the above technological challenges.
Both offline experiments and online A/B testing demonstrate the
effectiveness of DFCL over the state-of-the-art methods. Currently,
DFCL has been deployed in several marketing scenarios in Meituan,
one of the largest online food delivery platform in the world.
CCS Concepts
â€¢Computing methodologies â†’Machine learning approaches;
â€¢Applied computing â†’Electronic commerce.
Keywords
Causal Inference, Decision Focused Learning, Marketing Optimiza-
tion
ACM Reference Format:
Hao Zhou, Rongxiao Huang, Shaoming Li, Guibin Jiang, Jiaqi Zheng, Bing
Cheng, and Wei Lin. 2024. Decision Focused Causal Learning for Direct
Counterfactual Marketing Optimization. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3672353
6368
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Zhou et al.
1 Introduction
Conducting marketing campaigns is a popular and effective way
used by online Internet platforms to boost user engagement and
revenue. For example, coupons in Taobao[ 37] can stimulate user
activity, dynamic pricing in Airbnb[36] and discounts in Uber[11]
encourage users to use the products.
Despite the incremental revenues, marketing campaigns could
incur significant costs. In order to be sustainable, a marketing cam-
paign is usually conducted under a limited budget. In other words,
only a portion of individuals (e.g., shops or goods) may receive
marketing treatments due to a limited budget. Hence, assigning the
appropriate marketing treatments to different individuals is essen-
tial for the effectiveness of a marketing campaign since users would
respond differently to various promotional offers. Such decision
problems can be formalized as resource allocation problems and
have been investigated for decades.
The mainstream solution for these problems is a two-stage method
[2,3,11,32,38]. In the first stage, the individual-level (incremental)
response under different treatments is predicted using ML models.
The second stage is OR, and the predictions are fed into the combi-
nation optimization algorithms to achieve optimal overall revenue.
However, the objectives of the two stages are not aligned: the for-
mer focuses on the predictive precision of the ML models, while
the latter focuses on the quality of decisions. The method has some
defects due to the isolation of ML and OR. First, the prediction preci-
sion of ML models has no strict positive correlation with the quality
of the final decision. This is because standard loss functions (e.g.,
mean square error, cross-entropy error) do not take the interplay
between the predictions into account, which can affect decision
quality. Second, ML models often fall short of perfect precision, and
the complex operations performed on the predictions in OR lead to
the amplification or accumulation of prediction errors. Thus, the
two-stage method usually obtains suboptimal decisions and is even
inferior to heuristic strategies in some scenarios.
Recently, Decision-Focused Learning (DFL) [ 4,12,20,26] has
received increasing attention as an appropriate alternative to the
two-stage method. The paradigm integrates prediction and opti-
mization into an end-to-end system, which effectively aligns the
objectives of both stages and achieves better performance on many
challenging tasks. The key idea is to train ML models using a loss
function that directly measures the quality of the decisions obtained
from the predictions. Specifically, the ML models are trained un-
der the predict-then-optimize framework [ 12], which (1) makes
predictions based on historical data, (2) solves the optimization
problem based on the predictions, and (3) computes the decision
loss to update the ML model parameters using stochastic gradient
descent (SGD).
Nevertheless, deploying DFL in marketing is non-trivial due to
the following challenges.
Uncertainty of constraints. Most prior works of DFL have
investigated the optimization problem where the unknown param-
eters appear in the objective function. The reason behind this is
that the unknown parameters in the constraints lead to uncertainty
in the solution space, and the optimal solution derived from the
predictions may not be feasible under the real parameters. Within
the constraints of our optimization problem, there are two distinctforms of uncertainty: intrinsic and extrinsic. The inherent uncer-
tainty in the constraints refers to the costs consumed by the indi-
viduals under different treatments, which can be predicted based
on historical data. Extrinsic uncertainty is the frequently changing
marketing budget, determined by the external environment. An
ML model is required to guarantee superior performance under
different marketing budgets. Thus, our optimization objective is the
effectiveness of the decision under any budget, and the optimization
problem is a 0-1 integer stochastic programming.
Counterfactuals in marketing. Computing decision loss in
marketing is challenging due to the presence of counterfactuals.
Specifically, observing the values and costs of an individual under
different treatments is impossible because the individual can only
receive one treatment, which is also called the fundamental problem
of causal inference [ 27]. In addition, the optimal solution of the
optimization problem cannot be obtained based on offline data
due to the counterfactuals, which disables the common gradient-
estimation methods (e.g., SPO [12], LODL [28], LTR [18]) in DFL.
Computational cost of large-scale dataset. Computational
cost is one of the major roadblocks for DFL involving large-scale
optimization. As mentioned above, DFL integrates prediction and
optimization into an end-to-end system, where the solver will be
called frequently during training to solve the optimization problem.
Therefore, the computational cost of DFL is high, leading prior
works to investigate toy-level problems with few decision variables.
In real-world applications, we need to train models for tens of
millions of data, which is unsupportable by traditional DFL.
In this paper, we propose Decision-Focused Causal-Learning
(DFCL) to address the above challenges. The main contributions of
this work can be summarized as follows.
Generalization. In order to address both endogenous uncer-
tainty (cost of individual consumption) and exogenous uncertainty
(marketing budget) in the constraints, the uncertainty constraints
are transformed into the objective function of the dual problem
using Lagrangian duality theory. The optimization objective of
the dual problem is then used as the decision loss. Moreover, we
prove that the budget of the primal problem corresponds to the
Lagrange multipliers of the dual problem, and thus optimizing the
dual solution under different Lagrange multipliers is equivalent to
optimizing the quality of decisions under different budgets.
Counterfactual Decision Loss. Optimal solution, decision loss,
and gradient cannot be computed directly due to the existence of
counterfactuals in marketing, thus we propose two solutions: (1)
surrogate loss function and (2) black-box optimization based on
the Expected Outcome Metric (EOM) [ 2,39,40]. Inspired by Policy
Gradient in Reinforcement Learning, we transform the decision
problem of discrete actions into the problem which maximizes ex-
pected revenue under the probability distribution of the actions,
and combine the Maximum Entropy Regularizer as well as the
Lagrangian duality theory to give two kinds of surrogate loss func-
tions: Policy Learning Loss and Maximun Entropy Regularized Loss.
We theoretically guarantee continuity, convexity and equivalence
of the surrogate loss functions. For black-box optimization, we em-
ploy the EOM to give an unbiased estimation of the decision loss
and improve the finite difference strategy to develop an efficient
estimator of the gradient, which enables us to update the model
parameters using gradient descent.
6369Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Scalability. In real-world applications, we need to train models
for tens-of-millions of data. The surrogate functions proposed in
this paper are smooth convex loss functions with almost the same
computational efficiency as the two-stage method. For black-box
optimization, frequently solving the optimization problem after
perturbation incurs huge computational overhead. We accelerate
the problem solving and modify the gradient estimator using the
Lagrangian duality theory, which significantly improves the train-
ing efficiency and reduces the training time from hour-level to
second-level per epoch compared to the black-box method based
on the primal problem.
We conduct extensive experiments to evaluate the performance
of DFCL. Both offline experiments and online A/B testing show the
superior performance of our method over state-of-the-art baselines.
DFCL is deployed to several scenarios in Meituan, an online food
delivery platform, and achieves significant revenue.
2 Related Works
Two-stage Method. The mainstream solution to the resource al-
location problem in marketing usually follows the two-stage para-
digm [ 2,3,32,38], which handles the two stagesâ€”machine learn-
ing (ML) and operation research (OR)â€”independently. In the first
stage, the uplift models are deployed to predict the treatment ef-
fects of individuals. Some prior works have focused on the design
of uplift models, including Meta-Learners [ 16,23], Causal Forests
[2,5,31,39], representation learning [ 13,29,35] and rank model
[8,17]. However, standard loss functions (such as mean square error
and cross-entropy error) for training uplift models do not take the
downstream OR into account. In the second stage, the resource
allocation problem is represented as a multi-choice knapsack prob-
lem (MCKP), which is NP-Hard and efficiently solved based on
Lagrangian duality theory [2, 3, 32, 40].
Decision-Focused Learning(DFL). DFL is considered an appro-
priate alternative to the two-stage method, which integrates predic-
tion and optimization into an end-to-end system. Since computing
decision loss requires solving optimization problems, which usu-
ally involve non-differentiable operations, automatic differentiation
in machine learning frameworks (such as Pytorch [ 25] and Ten-
sorflow [ 1]) cannot give the correct gradient. Three categories of
approaches to gradient computation are proposed by prior DFL
works: analytical smoothing of optimization mappings, smooth-
ing by random perturbations, and differentiation of surrogate loss
function. The first method derives the analytic gradient of deci-
sion loss by using the KKT condition or the homogenous self-dual
formulation, including Optnet [ 4], DQP [ 10], QPTL [ 33], and In-
tOpt [ 19]. However, when the optimization problem is discrete, the
method requires a continuous relaxation of the primal problem,
which results in suboptimality. A potential resolution is to consider
every optimization problem as a black-box optimization and utilize
random perturbations, such as DBB [ 26], DPO [ 7], and I-MLE [ 24],
to generate approximate gradient. Furthermore, the decision loss
is typically discontinuous and nonconvex, so some of these works
suggest convex surrogate functions, including SPO [ 12], LTR [ 18],
NCE [22], and LODL [28], for the decision loss.
The most related works to ours are DRP [ 11] and DPM [ 40]. DRP
proposes to directly learn ROI (ratio between incremental valuesand incremental costs) to rank and choose individuals in the binary
treatment setting. It has been shown by [ 40] that the loss function in
DRP is unable to converging to a stable extreme point. DPM extend
the idea to the multiple treatments setting by directly learning the
unbiased estimation of the decision factor in OR. However, the
construction of the decision factor in multi-treatment setting relies
on the law of diminishing marginal utility, which does not hold
strictly in some scenarios of marketing.
3 Problem Formulation
In this section, we formalize the resource allocation problem and
introduce the overall optimization objective in marketing.
We start with a common marketing scenario that has ğ‘€types
of treatments. Let ğ‘Ÿğ‘–ğ‘—andğ‘ğ‘–ğ‘—be the revenue and cost of individual
ğ‘–under treatment ğ‘—, respectively. The objective is to find an opti-
mal allocation strategy for a group of individuals to maximize the
revenue of the platform, given a limited budget ğµ. Therefore, the
budget allocation problem with multiple treatments (MTBAP) can
be formulated as an integer programming problem (1):
maxğ‘§ğ¹(ğ‘§,ğµ)=âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğ‘§ğ‘–ğ‘—ğ‘Ÿğ‘–ğ‘—,
s.t.âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğ‘§ğ‘–ğ‘—ğ‘ğ‘–ğ‘—â‰¤ğµ,
âˆ‘ï¸
ğ‘—ğ‘§ğ‘–ğ‘—=1,âˆ€ğ‘–,
ğ‘§ğ‘–ğ‘—âˆˆ{0,1},âˆ€ğ‘–,ğ‘—,(1)
whereğ‘§ğ‘–ğ‘—âˆˆ{0,1}is the decision variable to denote whether to as-
sign treatment ğ‘—to individual ğ‘–. The first constraint is the limitation
of the budget and the second one requires that only one treatment
is assigned to each individual. Since the budget ğµfluctuates a lot in
real-world settings, the objective is regared as a function of the bud-
get and the overall marketing goal is to maximize revenue ğ¹(ğ‘§,ğµ)
within arbitrary given budget.
Combinatorial Optimization Algorithm. When the value of
ğ‘Ÿğ‘–ğ‘—andğ‘ğ‘–ğ‘—are known in advance, MTBAP is a classical multiple
choice knapsack problem (MCKP) [ 30], which remains NP-Hard.
Existing studies usually solve this problem by using greedy algo-
rithms or Lagrangian duality theory, both of which can provide a
approximation ratio of
ğœŒ=1âˆ’maxğ‘–ğ‘—ğ‘Ÿğ‘–ğ‘—
OPT,
where OPT is the optimal solution. In the above equation, maxğ‘–ğ‘—ğ‘Ÿğ‘–ğ‘—
refers to the revenue of one individual (e.g., one user or one shop),
which is negligible compared with OPT that is the sum of the
revenue of all the individuals in marketing. Therefore, it indicates
that both greedy algorithms and Lagrangian duality theory can
achieve near optimal performance, which are also the most common
algorithms to solve MTBAP in marketing. The details can be found
in existing works, which will not be discussed in this paper.
Model Prediction. However, the value of ğ‘Ÿğ‘–ğ‘—andğ‘ğ‘–ğ‘—are unknown
during decision making in real-world applications, which are usu-
ally replaced with the prediction value. Therefore, how to make the
prediction of ğ‘Ÿğ‘–ğ‘—andğ‘ğ‘–ğ‘—plays important roles in marketing effec-
tiveness, which will be addressed in this paper. In the traditional
6370KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Zhou et al.
two-stage approaches, the machine learning (ML) model is trained
with the direction of optimizing prediction accuracy, which may
be not consistent with the direction of optimizing decision quality.
In the following sections, we mainly focus on the design of the loss
function, to make a tradeoff between the prediction accuracy and
the decision quality.
4 Learning Framework of DFCL
In the learning framework, the loss function includes two parts, the
prediction loss and the decision loss, i.e.,
Lğ·ğ¹ğ¶ğ¿ =ğ›¼Lğ‘ƒğ¿+Lğ·ğ¿.
The formerLğ‘ƒğ¿aims to decrease the prediction error, which con-
tributes to improving the generalization ability of a ML model. The
latterLğ·ğ¿measures the decision quality of the downstream task,
which is exactly the objective of marketing optimization.
4.1 Prediction Loss
In the traditional two-stage method, the ML model is trained by min-
imizing the difference between the predictions Ë†ğ‘Ÿ,Ë†ğ‘and the ground-
truth values ğ‘Ÿ,ğ‘. For instance, in a regression problem, the mean
squared error (MSE) is usually used to train the ML model:
Lğ‘€ğ‘†ğ¸(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=1
ğ‘ğ‘€âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’Ë†ğ‘Ÿğ‘–ğ‘—)2+(ğ‘ğ‘–ğ‘—âˆ’Ë†ğ‘ğ‘–ğ‘—)2.(2)
Due to the counterfactuals in marketing, observing the revenue
or cost of an individual under different treatments is impossible
because each individual can only receive one treatment, which is
also called the fundamental problem of causal inference.
Definition 1 (The fundamental problem of causal infer-
ence). For all individuals, only one of all the potential outcomes
under different treatments can be observed in real-world data.
Therefore,Lğ‘€ğ‘†ğ¸ cannot be directly computed according to Eq. 2
sinceğ‘Ÿğ‘–ğ‘—1andğ‘Ÿğ‘–ğ‘—2(or equivalently, ğ‘ğ‘–ğ‘—1andğ‘ğ‘–ğ‘—2) cannot be simul-
taneously observed for any ğ‘—1â‰ ğ‘—2. To solve this problem, we
first formulate the training data set and then develop a equivalent
prediction loss in marketing.
Data Set. Suppose that there is a data set of size ğ‘collected
from random control trials (RCT). The ğ‘–-th sample is denoted by
(ğ‘¥ğ‘–,ğ‘¡ğ‘–,ğ‘Ÿğ‘–ğ‘¡ğ‘–,ğ‘ğ‘–ğ‘¡ğ‘–), whereğ‘¥ğ‘–is the features of individual ğ‘–,ğ‘¡ğ‘–is the
assigned treatment, and ğ‘Ÿğ‘–ğ‘¡ğ‘–,ğ‘ğ‘–ğ‘¡ğ‘–are the revenue and the cost of
individualğ‘–under treatment ğ‘¡ğ‘–. Denote the count of the samples
(individuals) receiving treatment ğ‘—byğ‘ğ‘—.
Prediction Loss. Given the above data set, we present the predic-
tion loss in marketing in Eq. (3). Theorem 1 presents the equivalency
and the detailed proof can be found in Appendix A.
Lğ‘ƒğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=1
ğ‘€âˆ‘ï¸
ğ‘–1
ğ‘ğ‘¡ğ‘–[(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–)2+(ğ‘ğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘ğ‘–ğ‘¡ğ‘–)2].(3)
Theorem 1. The prediction lossLğ‘ƒğ¿is equivalent toLğ‘€ğ‘†ğ¸, i.e.,
Lğ‘ƒğ¿=Lğ‘€ğ‘†ğ¸.4.2 Decision Loss
As is stated in Sec. 3, the ground-truth value of ğ‘Ÿandğ‘are usually
unknown in advance, which are replaced with the prediction Ë†ğ‘Ÿand
Ë†ğ‘during decision making. Therefore, denote the original optimiza-
tion problem ğ¹(ğ‘§,ğµ)byğ¹(ğ‘§,ğµ, Ë†ğ‘Ÿ,Ë†ğ‘), and the solution ğ‘§âˆ—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)is
obtained by solve MTBAP ğ¹(ğ‘§,ğµ, Ë†ğ‘Ÿ,Ë†ğ‘), i.e.,
ğ‘§âˆ—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)=arg maxğ‘§ğ¹(ğ‘§,ğµ, Ë†ğ‘Ÿ,Ë†ğ‘).
The objective value achieved by the current solution ğ‘§âˆ—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)can
be expressed with the ground-truth value of ğ‘Ÿasâˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğ‘Ÿğ‘–ğ‘—ğ‘§âˆ—
ğ‘–ğ‘—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘).
The decision loss under budget ğµis defined as the negative of
the objective value with ground-truth ğ‘Ÿand predicted decision
ğ‘§âˆ—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘), i.e.,
Lğ·ğ¿(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğ‘Ÿğ‘–ğ‘—ğ‘§âˆ—
ğ‘–ğ‘—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘).
As is descripted in Sec. 3, the budget ğµfluctuates a lot in real-world
settings and the overall marketing objective is to maximize the
revenue under arbitrary budget. Therefore, the decision loss in
marketing is defined as
Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ«âˆ
0Lğ·ğ¿(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)ğ‘‘ğµ
=âˆ«âˆ
0âˆ’âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğ‘Ÿğ‘–ğ‘—ğ‘§âˆ—
ğ‘–ğ‘—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)ğ‘‘ğµ.
For ease of calculation, we can also discretize the budget and com-
pute the decision loss by
Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ‘ï¸
ğµLğ·ğ¿(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘).
4.3 Learning Framework
Algorithm 1 presents the framework of decision focused causal
learning (DFCL). The most crucial step in this framework is the
gradient estimation of Lğ·ğ¹ğ¶ğ¿ in line 10 of Algorithm 1. However,
it is non-trival in marketing due to the following technological
challenges, i.e., uncertainty of constraints, counterfactual and com-
putation cost. In this paper, we will show how to address these
challenges and how to deploy DFCL in marketing optimization.
Algorithm 1 Decision Focused Causal Learning (DFCL)
Input: training data Dâ‰¡{(ğ‘¥ğ‘–,ğ‘¡ğ‘–,ğ‘Ÿğ‘–ğ‘¡ğ‘–,ğ‘ğ‘–ğ‘¡ğ‘–)}ğ‘
ğ‘–=1
1:Initializeğœ”
2:foreach epoch do
3: Ë†ğ‘Ÿ,Ë†ğ‘=ğ‘šğœ”(ğ‘¥).
4:Lğ‘ƒğ¿=Ã
ğ‘–1
ğ‘ğ‘¡ğ‘–[(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–)2+(ğ‘ğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘ğ‘–ğ‘¡ğ‘–)2].
5:foreach budget ğµdo
6:ğ‘§âˆ—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)=arg maxğ‘§ğ¹(ğ‘§,ğµ, Ë†ğ‘Ÿ,Ë†ğ‘).
7:Lğ·ğ¿(ğµ)=âˆ’Ã
ğ‘–Ã
ğ‘—ğ‘Ÿğ‘–ğ‘—ğ‘§âˆ—
ğ‘–ğ‘—(ğµ,Ë†ğ‘Ÿ,Ë†ğ‘).
8:Lğ·ğ¿=Ã
ğµLğ·ğ¿(ğµ)
9:Lğ·ğ¹ğ¶ğ¿ =ğ›¼Lğ‘ƒğ¿+Lğ·ğ¿.
10:ğœ”=ğœ”âˆ’ğœ‚ğœ•Lğ·ğ¹ğ¶ğ¿
ğœ•ğœ”
6371Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
5 Gradient Estimation of DFCL
The loss of DFCL consists of the prediction loss and the decision loss.
The former is a continuously differentiable function whose gradient
can be directly computed. Hence, the gradient estimation of the
decision loss is the key focus of this section. Firstly, we introduce the
equivalent dual decision loss to remove the uncertain constraints
and reduce the computation cost of combinatorial optimization
algorithms. Secondly, we develop two surrogate loss functions and
improve the black-box optimization algorithm to provide a gradient
estimation of the dual decision loss.
5.1 Dual Decision Loss
Based on the Lagrangian duality theory, the upper bound of the orig-
inal problem ğ¹(ğ‘§,ğµ,ğ‘Ÿ,ğ‘)can be obtained by solving the following
dual problem (4).
min
ğœ†â‰¥0Â©Â­Â­
Â«maxğ‘§ğœ†ğµ+Ã
ğ‘–Ã
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)ğ‘§ğ‘–ğ‘—
ğ‘ .ğ‘¡.Ã
ğ‘—ğ‘§ğ‘–ğ‘—=1,âˆ€ğ‘—
ğ‘§ğ‘–ğ‘—âˆˆ{0,1},âˆ€ğ‘–,ğ‘—ÂªÂ®Â®
Â¬
=min
ğœ†â‰¥0maxğ‘§ğ»(ğ‘§,ğœ†,ğµ,ğ‘Ÿ,ğ‘)
=min
ğœ†â‰¥0ğº(ğœ†,ğµ,ğ‘Ÿ,ğ‘). (4)
The optimal Lagrange multiplier ğœ†âˆ—for the dual problem (4)can be
obtained by using a gradient descent algorithm or a binary search
method with the terminal condition of ğµâˆ’Ã
ğ‘–Ã
ğ‘—ğ‘ğ‘–ğ‘—ğ‘§ğ‘–ğ‘—â‰¤ğœ–orğœ†â‰¤ğœ–.
In addition, an approximately optimal solution for the original
problem can be derived by maximizing ğ»(ğ‘§,ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘). Theorem 2
presents the relationship between the original problem ğ¹(ğ‘§,ğµ,ğ‘Ÿ,ğ‘)
and the dual problem ğº(ğœ†,ğµ,ğ‘Ÿ,ğ‘).
Theorem 2. Denote byğ¹ğ‘(ğ‘§,ğµ,ğ‘Ÿ,ğ‘)the relaxation form of ğ¹(ğ‘§,ğµ,ğ‘Ÿ,ğ‘)
where the decision variables ğ‘§are relaxed to continuous variables (i.e.,
ğ‘§ğ‘–ğ‘—âˆˆ[0,1]forâˆ€ğ‘–,ğ‘—). Denote the optimal solution by
ğ‘§âˆ—
ğ‘(ğµ,ğ‘Ÿ,ğ‘)=arg maxğ‘§ğ¹ğ‘(ğ‘§,ğµ,ğ‘Ÿ,ğ‘),
ğ‘§âˆ—(ğµ,ğ‘Ÿ,ğ‘)=arg maxğ‘§ğ¹(ğ‘§,ğµ,ğ‘Ÿ,ğ‘),
ğœ†âˆ—(ğµ,ğ‘Ÿ,ğ‘)=arg min
ğœ†â‰¥0ğº(ğœ†,ğµ,ğ‘Ÿ,ğ‘).
Given the optimal Lagrange multiplier ğœ†âˆ—, an approximation solution
for the original problem can be derived by
ğ‘§ğ‘‘(ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘)=arg maxğ‘§ğ»(ğ‘§,ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘).
Based on these definitions, we claim that ğœ†âˆ—is monotonic decreasing
with the increment of the budget ğµ, and we have
ğ¹(ğ‘§ğ‘‘,ğµ,ğ‘Ÿ,ğ‘)â‰¤ğ¹(ğ‘§âˆ—,ğµ,ğ‘Ÿ,ğ‘)
â‰¤ğ¹ğ‘(ğ‘§âˆ—
ğ‘,ğµ,ğ‘Ÿ,ğ‘)
=ğº(ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘)
â‰¤ğ¹(ğ‘§ğ‘‘,ğµ,ğ‘Ÿ,ğ‘)+max
ğ‘–ğ‘—ğ‘Ÿğ‘–ğ‘—
The detailed proof can be found in [ 14]. Given the optimal ğœ†âˆ—,
Theorem 2 indicates that the solution ğ‘§ğ‘‘(ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘)obtained bymaximizing ğ»(ğ‘§,ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘)is approximately optimal with an ap-
priximation ratio of
ğœŒ=ğ¹(ğ‘§ğ‘‘,ğµ,ğ‘Ÿ,ğ‘)
ğ¹(ğ‘§âˆ—,ğµ,ğ‘Ÿ,ğ‘)â‰¥ğ¹(ğ‘§âˆ—,ğµ,ğ‘Ÿ,ğ‘)âˆ’maxğ‘–ğ‘—ğ‘Ÿğ‘–ğ‘—
ğ¹(ğ‘§âˆ—,ğµ,ğ‘Ÿ,ğ‘)
=1âˆ’maxğ‘–ğ‘—ğ‘Ÿğ‘–ğ‘—
ğ¹(ğ‘§âˆ—,ğµ,ğ‘Ÿ,ğ‘)
â‰ˆ1
The last equality holds because ğ¹(ğ‘§âˆ—,ğµ,ğ‘Ÿ,ğ‘)is the sum of the rev-
enue of millions of individuals in marketing, which means that
ğ¹(ğ‘§âˆ—,ğµ,ğ‘Ÿ,ğ‘)â‰« maxğ‘–ğ‘—ğ‘Ÿğ‘–ğ‘—.
Therefore, instead of the original problem, the optimization of
the dual problem ğ»(ğ‘§,ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘)is taken as the learning objective,
which we call the dual decision loss. Given the optimal ğœ†âˆ—and
the prediction value Ë†ğ‘Ÿ,Ë†ğ‘, the solution ğ‘§ğ‘‘(ğœ†âˆ—,ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)is obtained by
maximizing ğ»(ğ‘§,ğœ†âˆ—,ğµ,Ë†ğ‘Ÿ,Ë†ğ‘), i.e.,
ğ‘§ğ‘‘(ğœ†âˆ—,ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)=arg maxğ‘§ğ»(ğ‘§,ğœ†âˆ—,ğµ,Ë†ğ‘Ÿ,Ë†ğ‘).
Notice that ğœ†âˆ—ğµcan be taken as a constant, and removing it from
ğ»(ğ‘§,ğœ†âˆ—,ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)does not influence ğ‘§ğ‘‘. Therefore, the solution ğ‘§ğ‘‘can
be rewritten as
ğ‘§ğ‘‘(ğœ†âˆ—,Ë†ğ‘Ÿ,Ë†ğ‘)=arg maxğ‘§ğ»(ğ‘§,ğœ†âˆ—,Ë†ğ‘Ÿ,Ë†ğ‘),
whereğ»(ğ‘§,ğœ†âˆ—,Ë†ğ‘Ÿ,Ë†ğ‘)is the form of ğ»(ğ‘§,ğœ†âˆ—,ğµ,Ë†ğ‘Ÿ,Ë†ğ‘)after removing
ğœ†âˆ—ğµ. The dual decision loss achieved by the current solution ğ‘§ğ‘‘is
Lğ·ğ·ğ¿(ğœ†âˆ—,ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’(ğœ†âˆ—ğµ+âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†âˆ—ğ‘ğ‘–ğ‘—)ğ‘§ğ‘‘
ğ‘–ğ‘—(ğœ†âˆ—,Ë†ğ‘Ÿ,Ë†ğ‘)).
Similarly, since ğœ†âˆ—andğµis irrelevant to the prediction value Ë†ğ‘Ÿ,Ë†ğ‘,
ğœ†âˆ—ğµcan be regarded as a constant and removed from the dual
decision loss. According to Theorem 2, ğœ†âˆ—is monotonic decreasing
with the increment of the budget ğµand there is an unique ğœ†âˆ—for
the dual problem when given the budget ğµ. Therefore, the decision
lossLğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)in the original problem under arbitrary budget ğµ
can be transformed to the dual decision loss Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)under
arbitrary Lagrange multiplier ğœ†âˆ—, i.e.,
Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ«âˆ
0Lğ·ğ·ğ¿(ğœ†âˆ—,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)ğ‘‘ğœ†âˆ—
=âˆ«âˆ
0Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)ğ‘‘ğœ†
=âˆ’âˆ«âˆ
0âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)ğ‘§ğ‘‘
ğ‘–ğ‘—(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)ğ‘‘ğœ†.
By discretizing the Lagrange multiplier ğœ†, the dual decision loss
can be computed by
Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ‘ï¸
ğœ†Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘).
5.2 Policy Learning Loss
Notice that the dual problem ğ»(ğ‘§,ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)can be solved by
maxğ‘§ğ»(ğ‘§,ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=Â©Â­Â­
Â«maxğ‘§Ã
ğ‘–Ã
ğ‘—(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)ğ‘§ğ‘–ğ‘—
ğ‘ .ğ‘¡.Ã
ğ‘—ğ‘§ğ‘–ğ‘—=1,âˆ€ğ‘—
ğ‘§ğ‘–ğ‘—âˆˆ{0,1},âˆ€ğ‘–,ğ‘—ÂªÂ®Â®
Â¬
=âˆ‘ï¸
ğ‘–max
ğ‘—(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)
6372KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Zhou et al.
Therefore, the solution ğ‘§ğ‘‘(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=arg maxğ‘§ğ»(ğ‘§,ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)can be
expressed by
ğ‘§ğ‘‘
ğ‘–ğ‘—(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=Iğ‘—=arg maxğ‘—Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—.
Hence, the dual decision loss is rewritten as
Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)Iğ‘—=arg maxğ‘—Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—
However,Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)is not differentiable with respect to Ë†ğ‘Ÿand
Ë†ğ‘due to the indicator function. Instead, we utilize a softmax function
to smooth the dual decision loss, i.e.,
Lâ€²
ğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)exp(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)Ã
ğ‘˜exp(Ë†ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘˜)(5)
Letğ‘ğ‘–ğ‘—(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=exp(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)/Ã
ğ‘˜exp(Ë†ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘˜)be the proba-
bility of assigning treatment ğ‘—to individual ğ‘–. Takeğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—as the
reward of assigning treatment ğ‘—to individual ğ‘–. Hence, minimizing
Lâ€²
ğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)is equivalent to maximizing the expected reward of
policyğœ‹=ğ‘ğ‘–ğ‘—(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)under different Lagrange multipliers. There-
fore,Lâ€²
ğ·ğ·ğ¿is also called the policy learning loss.
Due to the counterfactual in marketing, Lâ€²
ğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)cannot
be directly computed by Eq. (5)in training data sets. Instead, we
propose a surrogate loss, i.e.,
Lğ‘ƒğ¿ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–ğ‘
ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)exp(Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘¡ğ‘–)Ã
ğ‘—exp(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—).
Theorem 3 presents the equivalence between the original dual
decision lossLğ·ğ·ğ¿ and the surrogate policy learning loss ğ¿ğ‘ƒğ¿ğ¿.
The detailed proof can be found in Appendix B.
Theorem 3.Lğ·ğ·ğ¿,Lâ€²
ğ·ğ·ğ¿andLğ‘ƒğ‘ƒğ¿are equivalent, i.e.,
Lğ‘ƒğ¿ğ¿(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=Lâ€²
ğ·ğ·ğ¿(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)
and
min
Ë†ğ‘Ÿ,Ë†ğ‘Lğ‘ƒğ¿ğ¿(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=min
Ë†ğ‘Ÿ,Ë†ğ‘Lğ·ğ·ğ¿(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘).
5.3 Maximum Entropy Regularized Loss
In order to obtain a differentiable closed form of ğ‘§ğ‘‘(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)with
respect to Ë†ğ‘Ÿand Ë†ğ‘, we relax the discrete constraint ğ‘§âˆˆ{0,1}to a
continuous one ğ‘¥âˆˆ[0,1]and add a maximum entropy regular-
izer to the objective function in ğ»(ğ‘§,ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘). Hence,ğ»(ğ‘§,ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)is
transformed to a nonlinear convex function, i.e.,
maxğ‘§âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)ğ‘§ğ‘–ğ‘—âˆ’ğœâˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğ‘§ğ‘–ğ‘—lnğ‘§ğ‘–ğ‘—,
ğ‘ .ğ‘¡.âˆ‘ï¸
ğ‘—ğ‘§ğ‘–ğ‘—=1,âˆ€ğ‘–,
ğ‘§ğ‘–ğ‘—âˆˆ[0,1],
whereğœdenotes the penalty weight. The Lagrange relaxation func-
tion can be further rewritten as
ğ¿(ğ‘§,ğ›½)=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘€âˆ‘ï¸
ğ‘—=1(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)ğ‘§ğ‘–ğ‘—âˆ’ğœğ‘âˆ‘ï¸
ğ‘–=1ğ‘€âˆ‘ï¸
ğ‘—=1ğ‘§ğ‘–ğ‘—lnğ‘§ğ‘–ğ‘—âˆ’âˆ‘ï¸
ğ‘–ğ›½ğ‘–(1âˆ’âˆ‘ï¸
ğ‘—ğ‘§ğ‘–ğ‘—),whereğ›½is the dual variables on the equality constraint. When
ğœ•ğ¿(ğ‘§,ğ›½)
ğœ•ğ‘§=0andğœ•ğ¿(ğ‘§,ğ›½)
ğœ•ğ›½=0, the optimal solution is obtained by
ğ‘§ğ‘‘
ğ‘–ğ‘—=exp[(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)/ğœ]Ã
ğ‘˜exp[(Ë†ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘˜)/ğœ],
which is continuously differentiable with respect to Ë†ğ‘ŸandË†ğ‘. Hence,
the dual decision loss can be rewritten as
Lâ€²â€²
ğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)exp[(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)/ğœ]Ã
ğ‘˜exp[(Ë†ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘˜)/ğœ]
Similarly,Lâ€²â€²
ğ·ğ·ğ¿cannot be directly computed due to the counter-
factual in marketing. We propose a surrogate loss Lğ‘€ğ¸ğ‘…ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
as follows, which we call the maximum entropy regularized loss,
Lğ‘€ğ¸ğ‘…ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–ğ‘
ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)exp[(Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘¡ğ‘–)/ğœ]Ã
ğ‘—exp[(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)/ğœ].
Notice thatLğ‘ƒğ¿ğ¿is a special case of Lğ‘€ğ¸ğ‘…ğ¿ , where the solution
ğ‘§ğ‘‘can be regarded as a temperature softmax function in Lğ‘€ğ¸ğ‘…ğ¿ .
5.4 Improved Finite-Difference Strategy
In addition to constructing surrogate loss functions, we can also use
the Expected Outcome Metric (EOM) [ 2,39,40] to give an unbiased
estimate of the decision loss and leverage black-box optimization
for decision-focused learning.
EOM is a commonly used method for offline strategy evaluation
based on randomized dataset. Given a batch of ğ‘random sam-
ples and model predictions Ë†ğ‘ŸandË†ğ‘, an arbitrary allocation strategy
ğ‘§(Ë†ğ‘Ÿ,Ë†ğ‘)can be evaluated: (1) find the set of individuals whose re-
ceived treatment is equal to the treatment in the allocation strategy
ğ‘§(Ë†ğ‘Ÿ,Ë†ğ‘), (2) then empirically estimate their per capita revenue and
per capita cost:
Â¯ğ‘Ÿ(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=1
ğ‘âˆ‘ï¸
ğ‘–1
ğ‘ğ‘¡ğ‘–ğ‘Ÿğ‘¡ğ‘–Iğ‘¡ğ‘–=arg maxğ‘—ğ‘§ğ‘–ğ‘—,
Â¯ğ‘(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=1
ğ‘âˆ‘ï¸
ğ‘–1
ğ‘ğ‘¡ğ‘–ğ‘ğ‘¡ğ‘–Iğ‘¡ğ‘–=arg maxğ‘—ğ‘§ğ‘–ğ‘—,
whereğ‘ğ‘¡ğ‘–denotes the probability that a treatment is equal to ğ‘¡ğ‘–in
the randomized dataset. For the primal MCKP with budget ğµ, we
can use binary search to empirically estimate the per capita revenue
under a per capita budgetğµ
ğ‘as is shown in Appendix C.
Therefore, we can redefine the decision loss as follows:
Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğµÂ¯ğ‘Ÿ(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘).
Since the computation of Â¯ğ‘Ÿ(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)involves many nondifferen-
tiable operations, we consider them as black-box functions and
estimate the gradient by perturbation. Using the finite difference
strategy, the gradient of the decision quality with respect to Ë†ğ‘Ÿğ‘–ğ‘—is
estimated as:
ğœ•Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿğ‘–ğ‘—=Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ+ğ‘’ğ‘–ğ‘—â„,Ë†ğ‘)âˆ’Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
â„,
whereâ„is a small constant, and ğ‘’ğ‘–ğ‘—âˆˆ {0,1}ğ‘Ã—ğ‘€is a matrix
where only the element in the i-th row and j-th column is 1, and
all other elements are 0. The gradient termğœ•Lğ·ğ¿(ğ‘Ÿ,ğ‘,Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘ğ‘–ğ‘—can be
computed similarly. We estimate the gradient by perturbing the
predictions one by one and obtain the gradient matrixğœ•Lğ·ğ¿(ğ‘Ÿ,ğ‘,Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿ
6373Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
andğœ•Lğ·ğ¿(ğ‘Ÿ,ğ‘,Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘âˆˆğ‘…ğ‘Ã—ğ‘€. Finally, we derive the following loss
function, which is able to train the ML model via gradient descent:
Lğ¹ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğœ•Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿğ‘–ğ‘—Ë†ğ‘Ÿğ‘–ğ‘—+ğœ•Lğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘ğ‘–ğ‘—Ë†ğ‘ğ‘–ğ‘—.
Since the perturbations are performed one by one, Â¯ğ‘Ÿ(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
requires frequent evaluation, leading to a considerable time com-
plexity of black-box optimization based on the primal MCKP [ 34].
In practice, we find that the number of samples ğ‘tends to be in
the millions or even tens of millions, so the time consumption for a
training epoch reaches the level of hours. A possible approach is to
only perturb some of the samples by sampling, but this may incur
the loss of much of the gradient information.
Instead, we accelerate the problem solving and modify the gradi-
ent estimator by using Lagrangian duality theory. Since the budget
ğµin the primal MCKP corresponds one-to-one to the ğœ†in the duality
problem, the dual decision loss can be redefined as:
Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ’âˆ‘ï¸
ğœ†Â¯ğ‘Ÿ(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)âˆ’ğœ†Â¯ğ‘(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘).
Although we avoid solving the primal MCKP, it is still necessary
to frequently evaluate the per capita revenue and per capita cost
after perturbation under multiple Lagrangian multipliers. We ob-
serve that the decision making is independent for each individual
thanks to the decomposition of the Lagrangian duality theory. Thus,
for each sample, the smallest perturbation that causes a change
in the dual decison loss is first calculated, and the loss after the
perturbation is obtained by correcting only the original result. Ap-
pendix D provides details of the modified gradient estimator, which
greatly reduces the computational overhead. Finally, the black-box
optimization loss function can be rewritten as
Lğ¼ğ¹ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğœ•Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿğ‘–ğ‘—Ë†ğ‘Ÿğ‘–ğ‘—+ğœ•Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘ğ‘–ğ‘—Ë†ğ‘ğ‘–ğ‘—.
It is sufficient to support model training on tens of million of data
since the computational cost of incremental updating ğ¿ğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
after perturbation is much less than that of re-evaluating it. To im-
prove numerical stability in training, we truncate the perturbation
matrixğ‘ƒâˆˆRğ‘Ã—ğ‘€. Further, the loss function can be smoothed
using Softmax to reduce the difficulty of training.
Lğ¼ğ¹ğ·ğ¿âˆ’ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—ğœ•Lğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•ğ‘ğ‘–ğ‘—ğ‘ğ‘–ğ‘—,
whereğ‘ğ‘–ğ‘—=ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—), and onlyğ‘is perturbed for gra-
dient estimation and no longer for Ë†ğ‘Ÿ,Ë†ğ‘.
6 Evaluation
In this section, we will conduct large-scale offline and online exper-
iments to compare our methods with other benchmarks to validate
their performance.
6.1 Offline Experiment
6.1.1 Dataset. Two types of datasets are provided in this paper.
â€¢CRITEO-UPLIFT v2. This public dataset is provided by the
AdTech company Criteo in the AdKDDâ€™18 workshop[ 9]. The
dataset contains 13.9 million samples collected from a randomcontrol trial (RCT) that prevents a random part of users from be-
ing targeted by advertising. Each sample has 12 features, 1 binary
treatment indicator and 2 response labels(visit/conversion). In
order to study resource allocation problem under limited budget
using the dataset, we follow[ 40] and take the visit/conversion
label as the cost/value respectively. We randomly sample 70%
samples for training and the remaining samples for test.
â€¢Marketing data. Discounting is a common marketing campaign
in Meituan, an online food delivery platform. We conduct a two-
week RCT to collect data in this platform. The online shops on
the platform offer daily discounts to users. Note that to avoid
price discrimination, the discount of a shop is the same for all
individuals, but it changes randomly each day and varies from
shop to shop. The data in the first week is used for training and
the other for test. The discount ğ‘‡âˆˆ{0,5,10,15,20}is taken as
the treatment, where ğ‘‡=ğ‘¡meansğ‘¡%off for each order whose
price meets a given threshold. The dataset contains 2.8 million
samples, and each sample has 107 features, 1 treatment label and
2 response labels (daily cost/orders).
6.1.2 Evaluation Metrics. Multiple evaluation metrics are provided
for offline evaluation in this experiment. In addition to adopting
the evaluation metrics commonly used in two-stage models, such
as Logloss and MSE, we also use the following metrics for policy
evaluation with counterfactuals, which are more significant.
â€¢AUCC (Area under Cost Curve). A common metric used in
existing works [ 2,11,40], which is designed for evaluating the
performance to rank ROI of individuals in the binary treatment
setting. We use the metric to compare the performance of differ-
ent methods in CRITEO-UPLIFT v2.
â€¢EOM (Expected Outcome Metric). EOM is also commonly
used in [ 2,39,40]. Based on RCT data, an unbiased estimation
of the expected outcome (per-capita revenue/per-capita cost) for
arbitrary budget allocation policy can be obtained. The details
of EOM are shown in Sec. 5.4. We use the metric to compare the
performance of different methods in Marketing data.
6.1.3 Benchmarks. For each dataset in this paper, multiple models
and algorithms are implemented and taken as benchmarks.
â€¢TSM-SL. The two-stage method is mentioned in many exsting
works[ 2,3,32,38]. In the first stage, a well-trained S-Learner
model is used to predict the response (revenue/cost) of individu-
als under different treatments. In the second stage, we find the
optimal budget allocation solution for an MCKP formulation
based on the predictions.
â€¢TSM-CF. Also a two-stage method, the difference with TSM-SL
is that instead of S-learner, we use a Causal Forests [ 5] to predict
the incremental response in the first stage. It is implemented
here base on EconML packages [ 6], which can support binary
treatment and multiple treatments.
â€¢DPM. This method[ 40] designs the decision factor for the MCKP,
and proposes a surrogate loss to directly learn the decision factor.
â€¢CN. This method[ 32] imposes a monotonic constraint between
outcome predictions and treatments, which is particularly useful
for ITE estimation under multiple treatments. The method is
trained with MSE loss and evaluated only on marketing data,
which is a multi-treatment experiment.
6374KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Zhou et al.
â€¢CN+DFCL-PL. The constraint network is trained with Decision-
Focused Causal Learning (DFCL) loss, which comprises MSE loss
(Lğ‘ƒğ¿) and policy learning loss ( Lğ‘ƒğ¿ğ¿).
â€¢DFCL-PL. The DFCL method based on policy learning loss pro-
posed in this paper.
â€¢DFCL-MER. The DFCL method proposed in this paper utilizes
the surrogate loss derived by Maximun Entropy Regularizer
â€¢DFCL-IFD. The DFCL method proposed in this paper for gradient
estimation using the improved finite difference strategy.
6.1.4 Implementation Details.
â€¢CRITEO-UPLIFT v2. For the baseline methods (TSM-SL, TSM-
CF and DPM), we cite the results directly from[ 40]. The DFCL
model uses the same DNN architecture with a shared layer that
is a single-layer MLP of dimension 128 and four head networks
that are two-layer MLPs of dimension [64, 1]. Except for the
final output layer, the remaining layers use ReLU activations.
For DFCL-MER, we set the temperature ğœ=3. Our models are
trained for 40 epochs with the Adam optimizer [15]. In order to
accelerate the training, the first twenty epochs are warmstarting
[21] using the cross-entropy loss, and then the models are trained
using the DFCL loss.
â€¢Marketing data. In the multi-treatment experiment, the models
need to predict the revenue and cost under five treatments. TSM-
SL, CN, CN+DFCL-PL, DFCL-PL, DFCL-MER and DFCL-IFD use
the same DNN architecture: a 4 layers MLP (64-32-32-10). The
first five outputs of the models are the predicted revenue, and the
remaining outputs are the predicted cost. For DFCL-MER, we set
the temperature ğœ=0.01. For DPM, a S-Learner model is trained
using the customized loss proposed in [ 40] to directly predict
marginal utility under different treatments. The DPM model has
4 layers of MLP (64-32-32-1), with the last layer using a sigmoid
activation and each of the remaining layers with ReLU activations.
All neural network-based models are trained for 500 epochs using
the Adam optimizer. For TSM-CF, we set ğ‘›_ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  =256,
ğ‘šğ‘–ğ‘›_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ _ğ‘™ğ‘’ğ‘ğ‘“=300andğ‘‘ğ‘’ğ‘ğ‘¡â„ =24.
All experiments are run on AMD EPYC 7502P Rome 32x@ 2.50GHz
processor with 64GB memory.
Table 1: Comparison of common metrics, noting that DPM
and TSM-CF predict the decision factor and the incremental
intervention effect, respectively, and thus do not apply to
these two metrics.
ModelCRITEO-UPLIFT v2 Marketing data
Logloss MSE
TSM-SL 0.2165Â±0.0001 0.2625Â±0.0009
CN / 0.2639Â±0.0012
CN+DFCL-PL / 0.2703Â±0.0015
DFCL-PL 0.2186Â±0.0008 0.2678Â±0.0010
DFCL-MER 0.2178Â±0.0012 0.2650Â±0.0005
DFCL-IFD 0.2170Â±0.0003 0.2642Â±0.0009
6.2 Experimental Results
6.2.1 Overall performance. In Table 1, we present the prediction
loss of different models on the two datasets. Clearly, the two-stageTable 2: AUCC(CRITEO-UPLIFT v2)
Model AUCC Improvement
TSM-SL 0.7561Â±0.0113 /
TSM-CF 0.7558Â±0.0012 -0.03%
DPM 0.7739Â±0.0002 +2.35%
DFCL-PL 0.7713Â±0.0025 +2.01%
DFCL-MER 0.7727Â±0.0015 +2.20%
DFCL-IFD 0.7859Â±0.0021 +3.94%
0.0 0.2 0.4 0.6 0.8 1.0
Incremental cost0.00.20.40.60.81.0Incremental rewardTSM-SL
TSM-CF
DPM
DFCL-IFD
DFCL-PL
DFCL-MER
Random
(a) AUCC(CRITEO-UPLIFT v2)
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000015/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000013/uni00000014/uni00000011/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000013/uni00000014/uni00000011/uni00000014/uni00000015/uni00000018/uni00000014/uni00000011/uni00000014/uni00000018/uni00000013/uni00000014/uni00000011/uni00000014/uni0000001a/uni00000018/uni0000002c/uni00000051/uni00000046/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047
/uni00000037/uni00000036/uni00000030/uni00000010/uni00000036/uni0000002f
/uni00000037/uni00000036/uni00000030/uni00000010/uni00000026/uni00000029
/uni00000027/uni00000033/uni00000030
/uni00000026/uni00000031
/uni00000026/uni00000031/uni0000000e/uni00000027/uni00000029/uni00000026/uni0000002f/uni00000010/uni00000033/uni0000002f
/uni00000027/uni00000029/uni00000026/uni0000002f/uni00000010/uni0000002c/uni00000029/uni00000027
/uni00000027/uni00000029/uni0000002f/uni00000010/uni00000033/uni0000002f
/uni00000027/uni00000029/uni0000002f/uni00000010/uni00000030/uni00000028/uni00000035 (b) EOM (Marketing data)
Figure 1: Offline experiment results
method performs best on common metrics, which minimizes MSE
or Logloss on the training set. However, what we really focus on is
the decision quality of predictions. Fig. 1a and Table 2 present the
comparison between our proposed methods and other benchmarks
in CRITEO-UPLIFT v2 on AUCC [ 11], which represents the deci-
sion quality under binary treatments. We can see that DFCL-IFD
achieves the best performance, DFCL-PL, and DFCL-MER perform
similarly to DPM, and the two-stage methods perform the worst.
In marketing data, we use EOM method to calculate per-capita
orders and per-capita budgets based on predictions. The results are
shown in Table 3 and Fig. 1b. Our models significantly outperform
the baseline models in terms of per-capita orders at all per-capita
budgets. DPM is on par with the two stage methods in the low
per-capita budgets and outperforms them in the high per-capita
budgets. CN has a marginal improvement of 0.16% compared to
the two-stage methods. Further evaluation is carried out on the
model trained with DFCL loss, which comprises MSE loss ( Lğ‘ƒğ¿)
and policy learning loss ( Lğ‘ƒğ¿ğ¿). The integration of policy learning
loss yielded a notable enhancement in performance, with the con-
strained network showing a significant increase of 1.26%. These
findings suggest that our proposed DFCL approach is versatile
and can be integrated into existing methodologies. Interestingly,
the constraint network combined with policy learning loss (CN +
DFCL-PL) did not outperform DFCL-PL alone. We hypothesize that
this may be due to the predefined constraints within the network,
which potentially restrict the expansiveness of the decision space.
6.2.2 Prediction Loss vs Decision Loss tradeoff. As mentioned above,
we integrate the prediction loss as a regularizer into the training
objective. In this experiment, we will consider how the weight
of the prediction loss affects the performance of DFCL. We set
ğ›¼âˆˆ{0.1,0.5,1,2,3,4,5,10}and measure the per-capita orders under
a fixed per-capita budget. As shown in Fig. 2a, increasing ğ›¼in a
certain range does not lead to a decrease in model performance.
6375Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 3: EOM (Marketing data)
Mo
delBudgetImprovement1
2 3 4 5 6
TSM-SL 1.0000Â±0.0023
1.0300Â±0.0022 1.0611Â±0.0023 1.0873Â±0.0022 1.1140Â±0.0020 1.1437Â±0.0021 /
TSM-CF 1.0006Â±0.0007 1.0306Â±0.0006 1.0592Â±0.0004 1.0866Â±0.0006 1.1109Â±0.0003 1.1353Â±0.0008 -0.19%
DPM 0.9983Â±0.0011 1.0366Â±0.0010 1.0720Â±0.0006 1.1050Â±0.0003 1.1305Â±0.0010 1.1594Â±0.0007 1.00%
CN 1.0047Â±0.0013 1.0339Â±0.0010 1.0622Â±0.0007 1.0910Â±0.0009 1.1151Â±0.0011 1.1384Â±0.0015 0.16%
CN+DFCL-PL 0.9995Â±0.0003 1.0366Â±0.0008 1.0739Â±0.0009 1.1071Â±0.0005 1.1367Â±0.0006 1.1650Â±0.0009 1.26%
DFCL-PL 1.0104Â±0.0005 1.0465Â±0.0006 1.0812Â±0.0004 1.1118Â±0.0007 1.1407Â±0.0011 1.1638Â±0.0018 1.98%
DFCL-MER 1.0178Â±0.0008 1.0501Â±0.0005 1.0810Â±0.0002 1.1121Â±0.0010 1.1410Â±0.0013 1.1674Â±0.0009 2.06%
DFCL-IFD 1.0197Â±0.0012 1.0574Â±0.0022 1.0902Â±0.0024 1.1221Â±0.0026 1.1516Â±0.0028 1.1796Â±0.0030 2.85%
However, ifğ›¼is too large, the prediction loss dominates the training
objective and the model will be reduced to the two-stage method.
The experiment suggests it is possible to choose a value of ğ›¼so that
we can achieve better performance and more accurate predictions.
6.2.3 Impact of Lagrange multiplier. Next, we would like to discuss
the impact of the Lagrange multiplier ğœ†on the performance of
DFCL model. Since a given Lagrange multiplier ğœ†corresponds to
the MCKP for a certain budget constraint, DFCL model can learn
allocation policies for different budgets simultaneously by changing
or addingğœ†to the DFCL loss. We set up different combinations
of Lagrange multipliers ( ğœ†âˆˆ{{0.1},{0.1,0.5},{0.1,0.5,1.0}}) and
use to train DFCL models. Fig. 2b shows the results using DFCL-
IFD models trained by combinations of Lagrange multipliers. We
can observe that ğœ†is a hyperparameter that can have a significant
impact on model performance. A small ğœ†enables the model to learn
the allocation policy efficiently under high budget and vice versa.
Moreover, models trained with multiple Lagrange multipliers can
balance performance with different budgets.
0 2 4 6 8 10
Alpha1.11001.11251.11501.11751.12001.12251.12501.1275Incremental Reward (Under a given budget)
(a) Impact of the prediction loss
weightğ›¼
1 2 3 4 5 6
Budget1.021.041.061.081.101.121.141.161.18Incremental Reward
DFCL_{0.1}
DFCL_{0.1,0.5}
DFCL_{0.1,0.5,1.0}(b) Impact of Lagrange multiplier
Figure 2: Offline experiment results
6.3 Online A/B testing
6.3.1 Setups. We deploy DFCL, DPM and TSM-SL to support a dis-
count campaign in Meituan (a food delivery platform), and conduct
an online A/B testing for four weeks. The experiment contains 310K
online shops and they are randomly divided every day into three
groups called G-DFCL, G-DPM and G-TSL respectively. Each shop
will be assigned a discount ğ‘¡âˆˆ{0,5,10,15,20}as the treatmemt,
which means ğ‘¡%off for each order whose price meets a given thresh-
old. The marketing goal is to maximize the orders by assigning an
appropriate discount to each store every day for a limited budget.
The online deployment of DFCL is shown in Fig. 3a: (1) Beforethe campaign starts each day, we use the DFCL model to make
predictions and allocate the appropriate discounts to each store
based on budget and other constraints in an offline environment.
(2) The users visit the online shop and get discounts which will
stimulate them to make purchases. (3) During model training, we
use historical random data and resource allocation optimizer to
update the model parameters.
(a) Online deployment of DFCL
1 2 3 4
Week0.951.001.051.101.151.201.25Incremental order
G-TSL
G-DPM
G-DFCL (b) Orders
Figure 3: Online A/B testing
6.3.2 Results. Fig. 3b illustrates the improvement in weekly orders
for G-DFCL and G-DPM relative to G-TSM. In order to preserve
data privacy, all data points in Fig. 3b have been normalized that are
divided by the orders of TSM-SL in the first week. We can see that
DFCL achieves a significant average improvement of 2.17% relative
to TSM-SL and also outperforms DPM with a relative improvement
of 0.85%. The detailed results can be found in Appendix E.
7 Conclusion
In this paper, we propose a decision focused causal learning frame-
work (DFCL) for direct counterfactual marketing optimization,
which overcomes the technological challenges of DFL deployment
in marketing. By designing surrogate losses and constructing black-
box optimisation, we efficiently align the objectives of ML and OR.
Both offline experiments and online A/B testing demonstrate the
effectiveness of DFCL over the state-of-the-art methods.
Acknowledgments
This work was supported in part by National Key R&D Program
of China (2023YFB4502400), the NSF of China (62172206), and the
Xiaomi Foundation.
6376KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Zhou et al.
References
[1]MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al .
2016. Tensorflow: Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467 (2016).
[2]Meng Ai, Biao Li, Heyang Gong, Qingwei Yu, Shengjie Xue, Yuan Zhang, Yunzhou
Zhang, and Peng Jiang. 2022. LBCF: A Large-Scale Budget-Constrained Causal
Forest Algorithm. In Proceedings of the ACM Web Conference 2022. 2310â€“2319.
[3]Javier Albert and Dmitri Goldenberg. 2022. E-commerce promotions personaliza-
tion via online multiple-choice knapsack with uplift modeling. In Proceedings of
the 31st ACM International Conference on Information & Knowledge Management.
2863â€“2872.
[4]Brandon Amos and J Zico Kolter. 2017. Optnet: Differentiable optimization as a
layer in neural networks. In International Conference on Machine Learning. PMLR,
136â€“145.
[5]Susan Athey, Julie Tibshirani, and Stefan Wager. 2019. Generalized random
forests. (2019).
[6]Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna
Oprescu, and Vasilis Syrgkanis. 2019. EconML: A Python package for ML-Based
heterogeneous treatment effects estimation. Version 0. x (2019).
[7]Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe
Vert, and Francis Bach. 2020. Learning with differentiable pertubed optimizers.
Advances in neural information processing systems 33 (2020), 9508â€“9519.
[8]Artem Betlei, Eustache Diemert, and Massih-Reza Amini. 2021. Uplift model-
ing with generalization guarantees. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 55â€“65.
[9]Eustache Diemert, Artem Betlei, Christophe Renaudin, and Massih-Reza Amini.
2018. A large scale benchmark for uplift modeling. In KDD.
[10] Priya Donti, Brandon Amos, and J Zico Kolter. 2017. Task-based end-to-end model
learning in stochastic optimization. Advances in neural information processing
systems 30 (2017).
[11] Shuyang Du, James Lee, and Farzin Ghaffarizadeh. 2019. Improve User Retention
with Causal Learning. In The 2019 ACM SIGKDD Workshop on Causal Discovery.
PMLR, 34â€“49.
[12] Adam N Elmachtoub and Paul Grigas. 2022. Smart â€œpredict, then optimizeâ€.
Management Science 68, 1 (2022), 9â€“26.
[13] Fredrik Johansson, Uri Shalit, and David Sontag. 2016. Learning representations
for counterfactual inference. In International conference on machine learning.
PMLR, 3020â€“3029.
[14] Hans Kellerer, Ulrich Pferschy, David Pisinger, Hans Kellerer, Ulrich Pferschy,
and David Pisinger. 2004. The multiple-choice knapsack problem. Knapsack
Problems (2004), 317â€“347.
[15] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[16] SÃ¶ren R KÃ¼nzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. Metalearners for
estimating heterogeneous treatment effects using machine learning. Proceedings
of the national academy of sciences 116, 10 (2019), 4156â€“4165.
[17] Finn Kuusisto, Vitor Santos Costa, Houssam Nassif, Elizabeth Burnside, David
Page, and Jude Shavlik. 2014. Support vector machines for differential prediction.
InMachine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II 14.
Springer, 50â€“65.
[18] Jayanta Mandi, VÄ±ctor Bucarey, Maxime Mulamba Ke Tchomba, and Tias Guns.
2022. Decision-focused learning: through the lens of learning to rank. In Interna-
tional Conference on Machine Learning. PMLR, 14935â€“14947.
[19] Jayanta Mandi and Tias Guns. 2020. Interior point solving for lp-based prediction+
optimisation. Advances in Neural Information Processing Systems 33 (2020), 7272â€“
7282.
[20] Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey,
Tias Guns, and Ferdinando Fioretto. 2023. Decision-focused learning: Foun-
dations, state of the art, benchmark and future opportunities. arXiv preprint
arXiv:2307.13565 (2023).
[21] Jayanta Mandi, Peter J Stuckey, Tias Guns, et al .2020. Smart predict-and-optimize
for hard combinatorial optimization problems. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, Vol. 34. 1603â€“1610.
[22] Maxime Mulamba, Jayanta Mandi, Michelangelo Diligenti, Michele Lombardi,
Victor Bucarey, and Tias Guns. 2020. Contrastive losses and solution caching for
predict-and-optimize. arXiv preprint arXiv:2011.05354 (2020).
[23] Xinkun Nie and Stefan Wager. 2021. Quasi-oracle estimation of heterogeneous
treatment effects. Biometrika 108, 2 (2021), 299â€“319.
[24] Mathias Niepert, Pasquale Minervini, and Luca Franceschi. 2021. Implicit MLE:
backpropagating through discrete exponential family distributions. Advances in
Neural Information Processing Systems 34 (2021), 14567â€“14579.
[25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems 32 (2019).[26] Marin Vlastelica PoganÄiÄ‡, Anselm Paulus, Vit Musil, Georg Martius, and Michal
Rolinek. 2019. Differentiation of blackbox combinatorial solvers. In International
Conference on Learning Representations.
[27] Jasjeet S Sekhon. 2008. The Neyman-Rubin Model of Causal Inference and
Estimation via Matching Methods. The Oxford Handbook of Political Methodology
2 (2008), 1â€“32.
[28] Sanket Shah, Kai Wang, Bryan Wilder, Andrew Perrault, and Milind Tambe. 2022.
Decision-focused learning without decision-making: Learning locally optimized
decision losses. Advances in Neural Information Processing Systems 35 (2022),
1320â€“1332.
[29] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting Neural Networks for
the Estimation of Treatment Effects. Advances in Neural Information Processing
Systems (NIPS) 32 (2019).
[30] Prabhakant Sinha and Andris A Zoltners. 1979. The multiple-choice knapsack
problem. Operations Research 27, 3 (1979), 503â€“515.
[31] Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneous
treatment effects using random forests. J. Amer. Statist. Assoc. 113, 523 (2018),
1228â€“1242.
[32] Chao Wang, Xiaowei Shi, Shuai Xu, Zhe Wang, Zhiqiang Fan, Yan Feng, An You,
and Yu Chen. 2023. A Multi-stage Framework for Online Bonus Allocation Based
on Constrained User Intent Detection. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 5028â€“5038.
[33] Bryan Wilder, Bistra Dilkina, and Milind Tambe. 2019. Melding the data-decisions
pipeline: Decision-focused learning for combinatorial optimization. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 33. 1658â€“1665.
[34] Ziang Yan, Shusen Wang, Guorui Zhou, Jingjian Lin, and Peng Jiang. 2023. An
End-to-End Framework for Marketing Effectiveness Optimization under Budget
Constraint. arXiv preprint arXiv:2302.04477 (2023).
[35] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. 2018.
Representation learning for treatment effect estimation from observational data.
Advances in neural information processing systems 31 (2018).
[36] Peng Ye, Julian Qian, Jieying Chen, Chen-hung Wu, Yitong Zhou, Spencer
De Mars, Frank Yang, and Li Zhang. 2018. Customized regression model for
airbnb dynamic pricing. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 932â€“940.
[37] Yang Zhang, Bo Tang, Qingyu Yang, Dou An, Hongyin Tang, Chenyang Xi,
Xueying Li, and Feiyu Xiong. 2021. BCORLE ( ğœ†): An Offline Reinforcement
Learning and Evaluation Framework for Coupons Allocation in E-commerce
Market. Advances in Neural Information Processing Systems 34 (2021), 20410â€“
20422.
[38] Kui Zhao, Junhao Hua, Ling Yan, Qi Zhang, Huan Xu, and Cheng Yang. 2019. A
unified framework for marketing budget allocation. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
1820â€“1830.
[39] Yan Zhao, Xiao Fang, and David Simchi-Levi. 2017. Uplift modeling with mul-
tiple treatments and general response types. In Proceedings of the 2017 SIAM
International Conference on Data Mining. SIAM, 588â€“596.
[40] Hao Zhou, Shaoming Li, Guibin Jiang, Jiaqi Zheng, and Dong Wang. 2023. Direct
heterogeneous causal learning for resource allocation problems in marketing. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 5446â€“5454.
A The Proof of Theorem 1
Proof. First of all, we introduce some notations. Following the
potential outcome framework [ 27], letğ‘‹âˆˆRğ‘‘denote the feature
vector andğ‘‡âˆˆ{1,2,...,ğ‘€}be the treatment. Let ğ‘Œğ‘Ÿ(ğ‘‡)andğ‘Œğ‘(ğ‘‡)
be the potential outcome of the revenue and the cost respectively
when the individual receives treatment ğ‘‡. letbğ‘Œğ‘Ÿ(ğ‘‡)andbğ‘Œğ‘(ğ‘‡)be
the predicted outcome of the revenue and the cost respectively
when the individual receives treatment ğ‘‡. ForLğ‘€ğ‘†ğ¸, we have
Lğ‘€ğ‘†ğ¸(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=1
ğ‘ğ‘€âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’Ë†ğ‘Ÿğ‘–ğ‘—)2+(ğ‘ğ‘–ğ‘—âˆ’Ë†ğ‘ğ‘–ğ‘—)2
=Eğ‘‹,ğ‘‡[(ğ‘Œğ‘Ÿ(ğ‘‡)âˆ’bğ‘Œğ‘Ÿ(ğ‘‡))2+(ğ‘Œğ‘(ğ‘‡)âˆ’bğ‘Œğ‘(ğ‘‡))2].
6377Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
ForLğ‘ƒğ¿, we have
Lğ‘ƒğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
=1
ğ‘€âˆ‘ï¸
ğ‘–1
ğ‘ğ‘¡ğ‘–[(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–)2+(ğ‘ğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘ğ‘–ğ‘¡ğ‘–)2]
=1
ğ‘€âˆ‘ï¸
ğ‘—âˆ‘ï¸
ğ‘–:ğ‘¡ğ‘–=ğ‘—1
ğ‘ğ‘¡ğ‘–[(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–)2+(ğ‘ğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘ğ‘–ğ‘¡ğ‘–)2]
=1
ğ‘€âˆ‘ï¸
ğ‘—1
ğ‘ğ‘¡ğ‘–âˆ‘ï¸
ğ‘–:ğ‘¡ğ‘–=ğ‘—[(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–)2+(ğ‘ğ‘–ğ‘¡ğ‘–âˆ’Ë†ğ‘ğ‘–ğ‘¡ğ‘–)2]
=1
ğ‘€âˆ‘ï¸
ğ‘—Eğ‘‹[(ğ‘Œğ‘Ÿ(ğ‘—)âˆ’bğ‘Œğ‘Ÿ(ğ‘—))2+(ğ‘Œğ‘(ğ‘—)âˆ’bğ‘Œğ‘(ğ‘—))2|ğ‘‡ğ‘–=ğ‘—]
=1
ğ‘€âˆ‘ï¸
ğ‘—Eğ‘‹[(ğ‘Œğ‘Ÿ(ğ‘—)âˆ’bğ‘Œğ‘Ÿ(ğ‘—))2+(ğ‘Œğ‘(ğ‘—)âˆ’bğ‘Œğ‘(ğ‘—))2] (ğ‘‡âŠ¥ğ‘‹)
=Eğ‘‹,ğ‘‡[(ğ‘Œğ‘Ÿ(ğ‘‡)âˆ’bğ‘Œğ‘Ÿ(ğ‘‡))2+(ğ‘Œğ‘(ğ‘‡)âˆ’bğ‘Œğ‘(ğ‘‡))2],
whereğ‘‡âŠ¥ğ‘‹holds because the data set is from random control
trials (RCT). (RCT). Therefore, we finish the proof. â–¡
B The Proof of Theorem 3
Proof. Follow the notations in Appendix A and let
softmax(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)=exp(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)Ã
ğ‘˜exp(Ë†ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘˜)
be the softmax function. Hence, Lâ€²
ğ·ğ·ğ¿can be rewritten as
Lâ€²
ğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)softmax(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)
=âˆ’ğ‘ğ‘€âˆ‘ï¸
ğœ†1
ğ‘ğ‘€âˆ‘ï¸
ğ‘–âˆ‘ï¸
ğ‘—(ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†ğ‘ğ‘–ğ‘—)softmax(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)
=âˆ’ğ‘ğ‘€âˆ‘ï¸
ğœ†Eğ‘‹,ğ‘‡[(ğ‘Œğ‘Ÿ(ğ‘‡)âˆ’ğœ†ğ‘Œğ‘(ğ‘‡))softmax(bğ‘Œğ‘Ÿ(ğ‘‡)âˆ’ğœ†bğ‘Œğ‘(ğ‘‡))]
In addition, we have
Lğ‘ƒğ¿ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–ğ‘
ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)softmax(Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘¡ğ‘–)
=âˆ’âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘–ğ‘
ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)softmax(Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘¡ğ‘–)
=âˆ’ğ‘âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘—âˆ‘ï¸
ğ‘–:ğ‘¡ğ‘–=ğ‘—1
ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)softmax(Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘¡ğ‘–)
=âˆ’ğ‘âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘—Eğ‘‹[(ğ‘Œğ‘Ÿ(ğ‘—)âˆ’ğœ†ğ‘Œğ‘(ğ‘—))softmax(bğ‘Œğ‘Ÿ(ğ‘—)âˆ’ğœ†bğ‘Œğ‘(ğ‘—))|ğ‘‡ğ‘–=ğ‘—]
=âˆ’ğ‘âˆ‘ï¸
ğœ†âˆ‘ï¸
ğ‘—Eğ‘‹[(ğ‘Œğ‘Ÿ(ğ‘—)âˆ’ğœ†ğ‘Œğ‘(ğ‘—))softmax(bğ‘Œğ‘Ÿ(ğ‘—)âˆ’ğœ†bğ‘Œğ‘(ğ‘—))]
=âˆ’ğ‘ğ‘€âˆ‘ï¸
ğœ†1
ğ‘€âˆ‘ï¸
ğ‘—Eğ‘‹[(ğ‘Œğ‘Ÿ(ğ‘—)âˆ’ğœ†ğ‘Œğ‘(ğ‘—))softmax(bğ‘Œğ‘Ÿ(ğ‘—)âˆ’ğœ†bğ‘Œğ‘(ğ‘—))]
=âˆ’ğ‘ğ‘€âˆ‘ï¸
ğœ†Eğ‘‹,ğ‘‡[(ğ‘Œğ‘Ÿ(ğ‘‡)âˆ’ğœ†ğ‘Œğ‘(ğ‘‡))softmax(bğ‘Œğ‘Ÿ(ğ‘‡)âˆ’ğœ†bğ‘Œğ‘(ğ‘‡))].
Therefore,Lâ€²
ğ·ğ·ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=Lğ‘ƒğ¿ğ¿(ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)holds.Algorithm 2 Lagrangian duality gradient estimator
Input: training data Dâ‰¡{(ğ‘¥ğ‘–,ğ‘¡ğ‘–,ğ‘Ÿğ‘–ğ‘¡ğ‘–,ğ‘ğ‘–ğ‘¡ğ‘–)}ğ‘
ğ‘–=1, Lagrange multiplier
ğœ†, the predicted revenue/cost Ë†ğ‘Ÿ/Ë†ğ‘
Output:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿ,ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘
1:Initializeğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿ=0,ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘=0,ğ‘§ğ‘–ğ‘—=0âˆ€ğ‘–,ğ‘—
2:ğ‘=Ë†ğ‘Ÿâˆ’ğœ†Ë†ğ‘
3:âˆ€ğ‘–,ğ‘—, ğ‘§ğ‘–ğ‘—=Iğ‘—=arg maxğ‘—(ğ‘ğ‘–ğ‘—)
4:Â¯ğ‘Ÿ(Ë†ğ‘Ÿ,Ë†ğ‘,ğœ†)=1
ğ‘Ã
ğ‘–1
ğ‘ğ‘¡ğ‘–ğ‘Ÿğ‘¡ğ‘–Iğ‘¡ğ‘–=arg maxğ‘—ğ‘§ğ‘–ğ‘—
5:Â¯ğ‘(Ë†ğ‘Ÿ,Ë†ğ‘,ğœ†)=1
ğ‘Ã
ğ‘–1
ğ‘ğ‘¡ğ‘–ğ‘ğ‘¡ğ‘–Iğ‘¡ğ‘–=arg maxğ‘—ğ‘§ğ‘–ğ‘—
6:Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=Â¯ğ‘Ÿ(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)âˆ’ğœ†Â¯ğ‘(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
7:matching_indices= {ğ‘–|ğ‘¡ğ‘–=arg maxğ‘—ğ‘§ğ‘–ğ‘—,âˆ€ğ‘–}
8:mismatching_indices= {ğ‘–|ğ‘¡ğ‘–â‰ arg maxğ‘—ğ‘§ğ‘–ğ‘—,âˆ€ğ‘–}
9:for allğ‘–âˆˆmatching_indices do
10:â„ğ‘Ÿ
ğ‘–ğ‘¡ğ‘–=ğ‘šğ‘ğ‘¥ğ‘—â‰ ğ‘¡ğ‘–ğ‘ğ‘–ğ‘—âˆ’ğ‘ğ‘–ğ‘¡ğ‘–,â„ğ‘
ğ‘–ğ‘¡ğ‘–=(ğ‘ğ‘–ğ‘¡ğ‘–âˆ’maxğ‘—â‰ ğ‘¡ğ‘–ğ‘ğ‘–ğ‘—)
ğœ†
11:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–=âˆ’1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘Ÿ
ğ‘–ğ‘¡ğ‘–
12:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘ğ‘–ğ‘¡ğ‘–=âˆ’1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘
ğ‘–ğ‘¡ğ‘–
13: for allğ‘—âˆˆ{1,2,...,ğ‘€}ğ‘ğ‘›ğ‘‘ ğ‘— â‰ ğ‘¡ğ‘–do
14:â„ğ‘Ÿ
ğ‘–ğ‘—=ğ‘ğ‘–ğ‘¡ğ‘–âˆ’ğ‘ğ‘–ğ‘—,â„ğ‘
ğ‘–ğ‘—=(ğ‘ğ‘–ğ‘—âˆ’ğ‘ğ‘–ğ‘¡ğ‘–)
ğœ†
15:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿğ‘–ğ‘—=âˆ’1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘Ÿ
ğ‘–ğ‘—
16:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘ğ‘–ğ‘—=âˆ’1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘
ğ‘–ğ‘—
17:for allğ‘–âˆˆmismatching_indices do
18:ğ‘—=arg maxğ‘—ğ‘ğ‘–ğ‘—
19:â„ğ‘Ÿ
ğ‘–ğ‘¡ğ‘–=ğ‘ğ‘–ğ‘—âˆ’ğ‘ğ‘–ğ‘¡ğ‘–,â„ğ‘Ÿ
ğ‘–ğ‘—=âˆ’â„ğ‘Ÿ
ğ‘–ğ‘¡ğ‘–
20:â„ğ‘
ğ‘–ğ‘¡ğ‘–=(ğ‘šğ‘ğ‘¥ğ‘—ğ‘ğ‘–ğ‘—âˆ’ğ‘ğ‘–ğ‘¡ğ‘–)
ğœ†,â„ğ‘
ğ‘–ğ‘—=âˆ’â„ğ‘
ğ‘–ğ‘¡ğ‘–
21:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿğ‘–ğ‘¡ğ‘–=1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘Ÿ
ğ‘–ğ‘¡ğ‘–
22:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘ğ‘–ğ‘¡ğ‘–=1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘
ğ‘–ğ‘¡ğ‘–
23:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿğ‘–ğ‘—=1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘Ÿ
ğ‘–ğ‘—
24:ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘ğ‘–ğ‘—=1
ğ‘ğ‘ğ‘¡ğ‘–(ğ‘Ÿğ‘–ğ‘¡ğ‘–âˆ’ğœ†ğ‘ğ‘–ğ‘¡ğ‘–)
â„ğ‘
ğ‘–ğ‘—
25:returnğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘Ÿ,ğœ•Lğ·ğ·ğ¿(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
ğœ•Ë†ğ‘
Forâˆ€ğ‘–,ğ‘—=arg maxğ‘˜ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†ğ‘ğ‘–ğ‘˜, let Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—â†’+âˆ ; forâˆ€ğ‘–,ğ‘—â‰ 
arg maxğ‘˜ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†ğ‘ğ‘–ğ‘˜, letË†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—â†’âˆ’âˆ. Hence, we have
softmax(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)â†’Iğ‘—=arg maxğ‘˜Ë†ğ‘Ÿğ‘–ğ‘˜âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘˜.
Therefore, we further get
min
Ë†ğ‘Ÿ,Ë†ğ‘Lğ‘ƒğ¿ğ¿(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=min
Ë†ğ‘Ÿ,Ë†ğ‘Lâ€²
ğ·ğ·ğ¿(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘)=min
Ë†ğ‘Ÿ,Ë†ğ‘Lğ·ğ·ğ¿(ğœ†,Ë†ğ‘Ÿ,Ë†ğ‘).
â–¡
C Policy Evaluation Based on EOM
Given a batch of ğ‘random samples and model predictions Ë†ğ‘Ÿand
Ë†ğ‘, we can use binary search to empirically estimate the per capita
6378KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Zhou et al.
Table 4: Online A/B testing results with the confidence interval
GroupWeekImprovement1st 2nd 3rd 4th
G-TSL 1.0000Â±0.00285 1.1706Â±0.00298 1.1565Â±0.00293 1.0851Â±0.00289 /
G-DPM 1.0113Â±0.00284 1.1891Â±0.00298 1.1704Â±0.00293 1.1000Â±0.00288 1.32%
G-DFCL 1.0235Â±0.00285 1.2062Â±0.00297 1.1786Â±0.00293 1.1000Â±0.00288 2.17%
Algorithm 3 An implementation of per capita revenue estimation
for primal MCKP with budget ğµ
Input: training data Dâ‰¡{(ğ‘¥ğ‘–,ğ‘¡ğ‘–,ğ‘Ÿğ‘–ğ‘¡ğ‘–,ğ‘ğ‘–ğ‘¡ğ‘–)}ğ‘
ğ‘–=1, the budget ğµ, the
predicted revenue/cost Ë†ğ‘Ÿ/Ë†ğ‘, a small constant ğœ–
Output: the expected per capita revenue Â¯ğ‘Ÿ(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
1:Initializeğœ†min=0,ğœ†max=maxğ‘–,ğ‘—(Ë†ğ‘Ÿğ‘–ğ‘—
Ë†ğ‘ğ‘–ğ‘—),ğ‘§ğ‘–ğ‘—=0âˆ€ğ‘–,ğ‘—
2:whileğµ
ğ‘âˆ’Â¯ğ‘(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)<ğœ–do
3:ğœ†=ğœ†max+ğœ†min
2
4:âˆ€ğ‘–,ğ‘—, ğ‘§ğ‘–ğ‘—=Iğ‘—=arg maxğ‘—(Ë†ğ‘Ÿğ‘–ğ‘—âˆ’ğœ†Ë†ğ‘ğ‘–ğ‘—)
5: Â¯ğ‘Ÿ(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=1
ğ‘Ã
ğ‘–1
ğ‘ğ‘¡ğ‘–ğ‘Ÿğ‘¡ğ‘–Iğ‘¡ğ‘–=arg maxğ‘—ğ‘§ğ‘–ğ‘—
6: Â¯ğ‘(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=1
ğ‘Ã
ğ‘–1
ğ‘ğ‘¡ğ‘–ğ‘ğ‘¡ğ‘–Iğ‘¡ğ‘–=arg maxğ‘—ğ‘§ğ‘–ğ‘—
7:ifğµ
ğ‘âˆ’Â¯ğ‘(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)>0then
8:ğœ†max=ğœ†
9:else
10:ğœ†min=ğœ†
11:Â¯ğ‘Ÿ(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)=Â¯ğ‘Ÿ(ğœ†,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)
12:return the expected per capita revenue Â¯ğ‘Ÿ(ğµ,ğ‘Ÿ,ğ‘, Ë†ğ‘Ÿ,Ë†ğ‘)revenue under a per capita budgetğµ
ğ‘, Algorithm 3 summarizes this
approach.
D Lagrangian Duality Gradient Estimator
The decision making is independent for each individual thanks
to the decomposition of the Lagrangian duality theory. Thus, for
each sample, the smallest perturbation that causes a change in the
dual decison loss is first calculated, and the loss after the perturba-
tion is obtained by correcting only the original result. Algorithm 2
provides details of the modified gradient estimator, which greatly
reduces the computational overhead. Note that for comprehensibil-
ity, Algorithm 2 is described with for loops, while in practice we
work with matrix operations.
E Supplementary Experimental Results
Tabel 4 presents the detailed online A/B testing results. In order
to preserve data privacy, all data points have been normalized by
dividing by the orders of TSM-SL in the first week. The confidence
interval (ğ›¼=0.05) is computed by a t-test.
6379