Asynchronous Vertical Federated Learning for Kernelized AUC
Maximization
Ke Zhang
College of Informatics
Huazhong Agricultural University
Wuhan, Hubei, China
keezhang@webmail.hzau.edu.cnGanyu Wang
Department of Computer Science
Western University
London, Ontario, Canada
gwang382@uwo.caHan Liâˆ—
College of Informatics
Huazhong Agricultural University
Wuhan, Hubei, China
Engineering Research Center of
Intelligent Technology for Agriculture
Wuhan, Hubei, China
lihan125@mail.hzau.edu.cn
Yulong Wang
College of Informatics
Huazhong Agricultural University
Wuhan, Hubei, China
Engineering Research Center of
Intelligent Technology for Agriculture
Wuhan, Hubei, China
wangyulong6251@gmail.comHong Chen
College of Informatics
Huazhong Agricultural University
Wuhan, Hubei, China
Engineering Research Center of
Intelligent Technology for Agriculture
Wuhan, Hubei, China
chenh@mail.hzau.edu.cnBin Guâˆ—
School of Artificial Intelligence
Jilin University
Changchun, Jilin, China
Mohamed bin Zayed University of
Artificial Intelligence
Masdar, United Arab Emirates
jsgubin@gmail.com
ABSTRACT
Vertical Federated Learning (VFL) has garnered significant atten-
tion due to its applicability in multi-party collaborative learning
and the increasing demand for privacy-preserving measures. Most
existing VFL algorithms primarily focus on accuracy as the training
model metric. However, the data we access is often imbalanced in
the real world, making it difficult for models based on accuracy
to correctly classify minority samples. The Area Under the Curve
(AUC) serves as an effective metric to evaluate the performance
of a model on imbalanced data. Therefore, optimizing AUC can
enhance the modelâ€™s ability to handle imbalanced data. Besides,
computational resources within VFL systems are also imbalanced,
which makes synchronous VFL algorithms are difficult to apply in
the real world. To address the double imbalance issue, we propose
Asynchronous Vertical Federated Kernelized AUC Maximization
(AVFKAM). Specifically, AVFKAM asynchronously updates a kernel
model based on triply stochastic gradients with respect to (ğ‘¤.ğ‘Ÿ.ğ‘¡.)
the pairwise loss and random feature approximation. To facilitate
theoretical analysis, we transfer the asynchrony of model coeffi-
cients to the functional gradient through a dual relationship be-
tween coefficients and objective function. Furthermore, we demon-
strate that AVFKAM converges to the optimal solution at a rate of
O(1/ğ‘¡), whereğ‘¡represents the global iteration number, and discuss
âˆ—Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671930the security of the model. Finally, experimental results on various
benchmark datasets demonstrate that AVFKAM maintains high
AUC performance and efficiency.
CCS CONCEPTS
â€¢Computing methodologies â†’Distributed algorithms; Ma-
chine learning approaches.
KEYWORDS
AUC maximization, asynchronous parallel optimization, vertical
federated learning, kernel methods
ACM Reference Format:
Ke Zhang, Ganyu Wang, Han Li, Yulong Wang, Hong Chen, and Bin Gu.
2024. Asynchronous Vertical Federated Learning for Kernelized AUC Maxi-
mization. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671930
1 INTRODUCTION
Federated learning [ 29] paradigm has received much attention in
recent years. In a federated learning system, there are many partic-
ipants, who train collaboratively a model with preserving privacy.
According to the distribution of data among different parties, feder-
ated learning is divided into horizontal federated learning (HFL),
vertical federated learning (VFL) and federated transfer learning
(FTL). HFL is a sample-based federated learning where datasets
share the same feature space but are different in samples. VFL is a
feature-based federated learning holding the sample ID space but
differs in feature space. FTL differs both in samples and feature
space [ 42]. In this paper, we mainly discuss some challenges in the
VFL scenarios ( ğ‘–.ğ‘’., financial and medical fields).
4244
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ke Zhang et al.
For classification tasks, most VFL algorithms [ 5,10,12,50] con-
sider accuracy as metric to evaluate the model performance. How-
ever, data from the real world is often imbalanced where the number
of data points from one class is much larger than that of another
class. Continuing to consider accuracy as the metric, the model
may ignore the minority instances and struggle to correctly classify
them. An informative measure is Receiver Operating Characteristic
(ROC) curve and Area Under the Curve (AUC). AUC concerns the
overall performance of a functional family of classifiers and quan-
tifies their ability of correctly ranking any positive instance with
regards to a randomly chosen negative instance.
In recent years, AUC maximization algorithms have continuously
been proposed. Some stochastic AUC maximization algorithms
were proposed with several approaches. Some of them [ 11,23,24]
are offline methods utilizing stochastic gradients of the pairwise
loss function. Meanwhile, online learning is also an approach to op-
timize AUC problem. Some algorithms [ 16,45] consider SGD to opti-
mize AUC with an infinite buffer in the online setting. Besides, some
stochastic primal-dual algorithms for AUC maximization were pro-
posed by solving a minimax objective, such as SOLAM [ 44], SPAM
[31], SPDAM [ 32]. With the development of deep neural networks,
deep AUC maximization (DAM) [ 15,26,33] also receives increasing
attention. In the federated learning system, there are some AUC
maximization algorithms, including CoDA [ 14], CODASCA [ 46]
and FeDXL [ 13]. Wu et al. [ 38] proposed a serverless federated
learning algorithm to optimize the Area Under the Precision-Recall
Curve (AUPRC). However, these federated AUC maximization al-
gorithms are restricted to the HFL scenario. The vertical federated
AUC maximization algorithm also urgently requires resolution.
In real-world vertical federated learning applications, apart from
data imbalances, computational resources among participating en-
tities are also imbalanced. In such environments, the efficiency of
synchronous algorithms is often constrained by significant wait-
ing times. To overcome the limitations of synchronous algorithms,
some asynchronous algorithms [ 4,12,19,49,50] have been pro-
posed with the expectation of achieving faster training efficiency.
However, these algorithms all ignore the issue of data imbalance
taking accuracy as the metric. Therefore, proposing an asynchro-
nous vertical federated learning algorithm for AUC maximization
becomes particularly significant.
In this paper, we propose a novel kernel-based vertical federated
AUC maximization method named Asynchronous Vertical Feder-
ated Kernelized AUC Maximization (AVFKAM) to solve the above
double imbalance issue. For vertical federated kernel learning al-
gorithms, FDSKL [ 10] is a state-of-the-art vertical federated kernel
learning algorithm. However, FDSKL disregards the problem of
imbalanced data while it is a synchronous algorithm, which causes
lower performance than expected. To break the double imbalanced
limitation, AVFKAM considers optimizing AUC in the asynchro-
nous setting. Specifically, we asynchronously train a kernel model
with triply stochastic functional gradients. We update the model
coefficients in practice but update rules rely on functional gradi-
ent, which complicates our theoretical analysis. To address this
challenge, we transfer the asynchrony of model coefficients to the
functional gradient through the dual relationship between coeffi-
cients and objective function. In addition, we discuss the security
under the semi-honest assumption. Finally, experiment results onvarious benchmark datasets demonstrate that AVFKAM is signifi-
cantly faster than its synchronous counterpart while maintaining
high AUC performance. The main contributions are summarized
as follows.
â€¢To the best of our knowledge, we propose the first asynchro-
nous vertical federated kernel learning for AUC maximiza-
tion AVFKAM, and AVFKAM outperforms existing vertical
federated kernel learning algorithms in terms of efficiency
and AUC score on the imbalanced data.
â€¢In real-world VFL applications, both data and computational
resources are often imbalanced. AVFKAM can address these
imbalances by asynchronously updating a kernel model us-
ing a pairwise loss function.
â€¢We provide theoretical analysis of AVFKAM by leveraging
the dual relationship between model coefficients and func-
tion. AVFKAM converges to the optimal solution at a rate of
O(1/ğ‘¡), whereğ‘¡represents the global iteration number.
2 RELATED WORKS
In this section, we briefly review related techniques, including AUC
maximization, vertical federated learning, and asynchronous paral-
lel optimization. Besides, we summarize the differences between
some related works and our work in Table 1.
2.1 AUC Maximization
AUC maximization has been concerned with different learning
paradigms. In recent years, online methods, stochastic methods,
and deep learning methods have been the main approaches to solv-
ing the AUC optimization problem. Stochastic primal-dual methods
can directly apply stochastic methods for addressing the min-max
formulations. Ying et al.[ 44] first propose the idea of solving a
minimax objective. Many methods apply this idea to solve AUC
maximization, such as SOLAM [ 44], SPAM [ 31], SPDAM [ 32]. In
addition, a straightforward approach for designing stochastic AUC
maximization algorithm is using the pairwise loss function for the
pairs samples. Gu et al. [ 11] propose the AdaDSG algorithm by
solving regularized pairwise learning problems. TSAM [ 8] uses
a pairwise loss function to solve a AUC problem by proposing
triply stochastic gradients. As a significant machine learning para-
digm, federated AUC maximization has received much attention.
Guo et al. [ 14,46] proposed CoDA and CODASCA by solving the
min-max formulations. Subsequently, FeDXL [ 13] is proposed by
pairwise loss and active-passive decomposition, and obtains supe-
rior performance. Besides, we noticed that Wu et al. [ 38] propose
a serverless AUPRC maximization approach that holds better per-
formance (AUC in this paper refers to Area Under the Receiver
Operating Characteristic, AUROC). However, these federated learn-
ing algorithms are proposed to solve AUC maximization problem
on horizontally partitioned data, there remains a requirement to re-
solve the AUC maximization problem in vertical federated learning
scenarios.
2.2 Vertical Federated Learning
Vertical federated learning is a machine learning paradigm where
multiple participants with different features but the same set of
users (vertically partitioned data) collaboratively train machine
4245Asynchronous Vertical Federated Learning for Kernelized AUC Maximization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 1: Comparison with state-of-the-art federated AUC maximization and asynchronous VFL algorithms. (LR=Logistic
Regression, NN=Neural Networks)
Algorithm Data Distribution Asynchronous Model Metric Convergence Rate
CoDA [14] Horizontal Ã— NN AUROC Score O(1/ğ‘‡)
CODASCA [51] Horizontal Ã— NN AUROC Score O(1/ğ‘‡)
FeDXL [13] Horizontal Ã— NN AUROC Score O(1/4âˆš
ğ‘‡)
STATE & STATE-M [38] Horizontal Ã— NN AUPRC Score O(1/4âˆš
ğ‘‡)&O(1/3âˆš
ğ‘‡)
FDML [19] Vertical âœ“ LR/NN Accuracy O(1/âˆš
ğ‘‡)
VAFL [4] Vertical âœ“ LR/NN Accuracy O(1/âˆš
ğ‘‡)
AF-VP [12] Vertical âœ“ LR Accuracy O(ğ‘’âˆ’ğ‘‡)
VFB2[50] Vertical âœ“ LR Accuracy O(ğ‘’âˆ’ğ‘‡)
AsySQN [49] Vertical âœ“ LR Accuracy O(ğ‘’âˆ’ğ‘‡)
AVFKAM (Our) Vertical âœ“ Kernel AUROC Score O(1/ğ‘‡)
learning models. This scenario arises in e-commerce, financial,
and healthcare applications [ 17]. According to combine features
in data ownersâ€™ joint user base, VFL algorithms can train a more
reliable global model than the local model. Compared to the HFL
setting, the VFL setting faces more complex challenges [ 4,19,20,27].
For example, the global model update at a server is an additive
aggregation of the local models in HFL, where clients use their own
data. In contrast, the global model in VFL concats local models,
which are coupled by the loss function.
In the real scenario of VFL, network heterogeneity, geographical
conditions, and the large size of encrypted data result in the bottle-
neck of algorithms. Therefore, many VFL methods aim to improve
efficiency involving reducing the cost of coordination and compress-
ing data transmitted between parties. For example, multiple local
updates during each iteration is a straightforward way to reduce the
communication cost. Some VFL methods [ 1,28,48] are proposed
based on multiple local updates. Besides, asynchronous machine
learning also can improve training efficiency by breaking synchro-
nization. There are some VFL algorithms [ 4,12,19] asynchronously
train a VFL model and obtain considerable performance.
2.3 Asynchronous Parallel Optimization
Asynchronous and parallel optimization algorithms are often used
to solve the case that synchronous training has long waiting time
and low computational resource utilization in the imbalanced com-
putation resource. Recht et al. [ 34] proposed a lock-free stochas-
tic gradient descent (SGD) algorithm in the shared memory envi-
ronment. In addition, there are asynchronous primal-dual algo-
rithm [ 18] and asynchronous SAGA [ 22]. Asynchronous cross-
device machine learning increasingly considers network topology
and communication. Lian et al. [ 25] propose an asynchronous de-
centralized parallel SGD to remove the central server bottleneck
in the asynchronous distributed learning system. Federated learn-
ing faces the same problem as distributed learning during training,
thus asynchronous optimization is also a common and significant
approach to improving training efficiency. Xu et al. [ 40] summarize
most existing asynchronous federated learning algorithms. Xie et
al. [39] propose FedAsync and obtain faster convergence than Fe-
dAvg [ 29]. Recently, Chen et al. [ 2] provided the theoretical analysis
of FedZO [ 9] in the asynchronous setting. The above algorithmsare all based on HFL. In contrast, the asynchronous VFL algorithms
are equally significant but rarely studied. FDML [ 19] and VAFL [ 4]
are proposed to asynchronously train a global model in the VFL
setting. They consider using neural networks to address linear (lin-
ear regression and logistical regression) and nonlinear problems.
Meanwhile, there are asynchronous VFL algorithms based on linear
assumption [12, 49, 50].
3 PRELIMINARIES
In this section, we first introduce how to calculate the AUC score,
and then briefly review triply stochastic optimization.
3.1 Problem Setting
LetXâŠ‚Rğ‘‘be the instance space where {ğ‘¥ğ‘–}ğ‘
ğ‘–=1âˆˆX, andY=
{âˆ’1,+1}be the label space. We denote the training instance set
asD, and divideDintoD+andDâˆ’, which are composed of ğ‘+
positive instances and ğ‘âˆ’negative instances, respectively. Let
decision function ğ‘“:Xâ†’R, the AUC metric on Dcan be defined
as follow:
ğ´ğ‘ˆğ¶(ğ‘“)=1âˆ’Ãğ‘+
ğ‘–=1Ãğ‘âˆ’
ğ‘—=1I(ğ‘“(ğ‘¥+
ğ‘–)â‰¤ğ‘“(ğ‘¥âˆ’
ğ‘—))
ğ‘+ğ‘âˆ’
=E(ğ‘¥+,ğ‘¦+)âˆ¼D+,(ğ‘¥âˆ’,ğ‘¦âˆ’)âˆ¼Dâˆ’
I(ğ‘“(ğ‘¥+)â‰¥ğ‘“(ğ‘¥âˆ’))(1)
where I(ğ‘)is the indicator function that equals 1 when ğ‘is true
and 0 otherwise. For the convenience of optimization, the indicator
function is usually replaced by surrogate convex loss functions [ 43].
Based on the surrogate pairwise loss functions, we minimize a reg-
ularized pairwise learning problem to achieve AUC maximization
in Reproducing Kernel Hilbert Spaces (RKHS):
arg min
ğ‘“âˆˆHğ‘…(ğ‘“)=E(ğ‘¥+,ğ‘¦+)âˆ¼ğ·+
(ğ‘¥âˆ’,ğ‘¦âˆ’)âˆ¼ğ·âˆ’ğ‘™(ğ‘“(ğ‘¥+),ğ‘“(ğ‘¥âˆ’))+ğœ†
2âˆ¥ğ‘“âˆ¥2
H,(2)
whereğœ†is the regularized coefficient, and âˆ¥Â·âˆ¥His the norm in
RKHSH. In the vertical federated learning scenario, we assume
the number of total workers is ğ‘, andğ‘¥is vertically distributed
amongğ‘workers. Specifically, for âˆ€â„“âˆˆ{1,2,Â·Â·Â·,ğ‘},â„“-th workers
provide part of feature ğ‘‘â„“and(ğ‘¥)Gâ„“. Therefore, the total features
ğ‘‘=Ãğ‘
â„“=1ğ‘‘â„“and instance ğ‘¥=[(ğ‘¥)G1,(ğ‘¥)G2,Â·Â·Â·,(ğ‘¥)Gğ‘].
4246KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ke Zhang et al.
3.2 Triply Stochastic Optimization
Based onğ‘“âˆˆH and the reproducing property [ 47] in the RKHS,
Dang et al. [ 8] propose triply stochastic optimization to solve the
equation (2). The gradient of the equation (2) can be denoted as:
âˆ‡ğ‘…(ğ‘“)=ğ‘™â€²
1(ğ‘“(ğ‘¥+
ğ‘–),ğ‘“(ğ‘¥âˆ’
ğ‘—))ğ‘˜(ğ‘¥+
ğ‘–,Â·)+ğ‘™â€²
2(ğ‘“(ğ‘¥+
ğ‘–),ğ‘“(ğ‘¥âˆ’
ğ‘—))ğ‘˜(ğ‘¥âˆ’
ğ‘—,Â·)+ğœ†ğ‘“,
(3)
whereğ‘™â€²
1(ğ‘“(ğ‘¥+
ğ‘–),ğ‘“(ğ‘¥âˆ’
ğ‘—))ğ‘˜(ğ‘¥+
ğ‘–,Â·)is the derivative of pairwise loss
functionğ‘™(ğ‘“(ğ‘¥+
ğ‘–),ğ‘“(ğ‘¥âˆ’
ğ‘—))with respect to ğ‘“(ğ‘¥+
ğ‘–), andğ‘™â€²
2(ğ‘“(ğ‘¥+
ğ‘–),ğ‘“(ğ‘¥âˆ’
ğ‘—))
ğ‘˜(ğ‘¥âˆ’
ğ‘—,Â·)is with respect to ğ‘“(ğ‘¥âˆ’
ğ‘—). To simplify the representation, we
denoteğ‘™â€²
1(ğ‘“(ğ‘¥+
ğ‘–),ğ‘“(ğ‘¥âˆ’
ğ‘—))andğ‘™â€²
2(ğ‘“(ğ‘¥+
ğ‘–),ğ‘“(ğ‘¥âˆ’
ğ‘—))asğ‘™â€²
1andğ‘™â€²
2, respec-
tively. Therefore, the stochastic functional gradient of loss function
withğ‘˜(Â·,Â·)can be denoted as equation (4)
ğœ‰(Â·)=ğ‘™â€²
1ğ‘˜(ğ‘¥+,Â·)+ğ‘™â€²
2ğ‘˜(ğ‘¥âˆ’,Â·). (4)
From equation (4), we have to compute the kernel matrix ğ¾by
kernel mapping function ğ‘˜(Â·,Â·). For a dataset with ğ‘instances,
computing the kernel matrix involves O(ğ‘2)operations, and stor-
ing it requiresO(ğ‘2)space. When ğ‘is large, equation (4) is hard
to apply. To remedy this issue, there are some kernel approxima-
tion techniques to improve training efficiency. Random Fourier
feature approximation is a common approach to approximate ker-
nel mapping function. According to Mercerâ€™s theorem [ 30], for any
stationary kernel function ğ‘˜(ğ‘¥,ğ‘¦)=ğ‘˜(ğ‘¥âˆ’ğ‘¦), denoteğœ™ğœ”(ğ‘¥)as ran-
dom feature basis function, we can approximate kernel mapping
function as
ğ‘˜(ğ‘¥ğ‘–,ğ‘¥ğ‘—)â‰ˆ1
ğ‘ğ‘âˆ‘ï¸
ğ‘§=1ğœ™ğœ”ğ‘§(ğ‘¥ğ‘–)ğœ™ğœ”ğ‘§(ğ‘¥ğ‘—).
If the kernel function is Gaussian kernel, ğœ™ğœ”(ğ‘¥):=âˆš
2 cos(ğœ”ğ‘‡ğ‘¥+ğ‘).
Random direction ğœ”is sampled from a Gaussian distribution P(ğœ”)
with density proportional to exp(âˆ’ğœ2âˆ¥ğœ”âˆ¥2/2)by random seed ğ‘–.
Andğ‘âˆ¼P(ğ‘), where P(ğ‘)is a uniform distribution on [0,2ğœ‹].
For addressing large-scale nonlinear datasets, triply stochastic
optimization further approximates the functional gradient (4) re-
placeğ‘˜(ğ‘¥,Â·)withğœ™ğœ”(ğ‘¥)ğœ™ğœ”(Â·)as follow
ğœ(Â·)=ğ‘™â€²
1ğœ™ğœ”(ğ‘¥+)ğœ™ğœ”(Â·)+ğ‘™â€²
2ğœ™ğœ”(ğ‘¥âˆ’)ğœ™ğœ”(Â·). (5)
Based on the stochastic functional gradient (5) with random fea-
tures, we update the solution of equation (2) by a constant stepsize
ğ›¾. Letğ‘“1(Â·)=0,ğ‘™â€²
(Â·,ğ‘–)represents the functional gradient of the loss
function at the ğ‘–-th iteration, the solution of ( ğ‘¡+1)-th iteration is:
ğ‘“ğ‘¡+1(Â·)=ğ‘“ğ‘¡(Â·)âˆ’ğ›¾(ğœğ‘¡(Â·)+ğœ†ğ‘“ğ‘¡(Â·))
=ğ‘¡âˆ‘ï¸
ğ‘–=1âˆ’ğ›¾Î ğ‘¡
ğ‘—=ğ‘–+1(1âˆ’ğ›¾ğœ†)ğœğ‘–(Â·)
=ğ‘¡âˆ‘ï¸
ğ‘–=1âˆ’ğ›¾(1âˆ’ğ›¾ğœ†)ğ‘¡âˆ’ğ‘–(ğ‘™â€²
1,ğ‘–ğœ™ğœ”(ğ‘¥+)+ğ‘™â€²
2,ğ‘–ğœ™ğœ”(ğ‘¥âˆ’))
|                                               {z                                               }
ğ›¼ğ‘¡
ğ‘–ğœ™ğœ”(Â·).(6)
4 ASYNCHRONOUS VERTICAL FEDERATED
KERNELIZED AUC MAXIMIZATION
According to triply stochastic optimization, we expend it to VFL
scenario, and propose AVFKAM. We first introduce the framework
ğ‘«ğŸ+,ğ‘«ğŸâˆ’ğ‘«ğ’+ğŸ+,ğ‘«ğ’+ğŸâˆ’ğ‘«ğ’+,ğ‘«ğ’âˆ’ğ‘«ğ’’+,ğ‘«ğ’’âˆ’
Compute ğ“ğ’˜ğ’™+,ğ“ğ’˜ğ’™âˆ’Compute {ğ’‡ğ’™+,ğ’‡ğ’™âˆ’}
Active Worker
Passive Worker
ğœ¶ğœ¦ğŸ ğœ¶ğœ¦ğ’ ğœ¶ğœ¦ğ’+ğŸ ğœ¶ğœ¦ğ’’
â€¦ â€¦CoordinatorFigure 1: Framework of AVFKAM
of AVFKAM, and then describe the procedure of AVFKAM. Finally,
we discuss the challenge of convergence analysis due to asynchrony.
4.1 Asynchronous Framework
We show the asynchronous framework of AVFKAM in Figure 1.
All parties collaboratively train a joint kernel model by providing
their own local data with privacy-preserving. There are two types
of parties in our framework: active workers and passive workers.
For a general representation, we assume that ğ‘workers contain ğ‘š
active workers and (ğ‘âˆ’ğ‘š)passive workers. Active workers hold
label information and provide their own features, passive workers
only provide their own features. Therefore, the dominator is one
of the active workers actively launching to update the model, and
the rest of workers (other active workers and passive workers) are
collaborators in each iteration. In the asynchronous framework,
each worker launches the next update request without waiting for
others to finish. Therefore, faster workers have much higher update
frequency in the asynchronous fashion than in the synchronous
fashion, which results that convergence speed of our algorithm is
faster with regard to wall clock time as shown in Figure 2.
Algorithm 1 AVFKAM for the â„“-th active worker
Input:{ğœ™ğœ”ğ‘–(ğ‘¥+
ğ‘–),ğ‘¦+
ğ‘–}ğ‘+
ğ‘–=1,{ğœ™ğœ”ğ‘–(ğ‘¥âˆ’
ğ‘–),ğ‘¦âˆ’
ğ‘–}ğ‘âˆ’
ğ‘–=1stored on the â„“-th
worker, constant stepsize ğ›¾, regularization coefficient ğœ†.
Output: the local coefficient ğ›¼Î›â„“.
1:Initializeğ‘“(Â·)=0.
Each active worker keeps doing in parallel
2: Pick up a random seed ğ‘–and obtainğœ™ğœ”ğ‘–(ğ‘¥+
ğ‘–),ğœ™ğœ”ğ‘–(ğ‘¥âˆ’
ğ‘–).
3: Aggregate{bğ‘“(ğ‘¥+
ğ‘–),bğ‘“(ğ‘¥âˆ’
ğ‘–)}=Ãğ‘
â„“â€²=1{bğ‘“â„“â€²(ğ‘¥+
ğ‘–),bğ‘“â„“â€²(ğ‘¥âˆ’
ğ‘–)}by
calling Algorithm 2.
4: Compute bğ›¼ğ‘–=âˆ’ğ›¾[ğ‘™â€²
1(bğ‘“(ğ‘¥+
ğ‘–),bğ‘“(ğ‘¥âˆ’
ğ‘–))ğœ™ğœ”ğ‘–(ğ‘¥+
ğ‘–)+ğ‘™â€²
2(bğ‘“(ğ‘¥+
ğ‘–),bğ‘“(
ğ‘¥âˆ’
ğ‘–))ğœ™ğœ”ğ‘–(ğ‘¥âˆ’
ğ‘–)].
5: Update bğ›¼ğ‘—=(1âˆ’ğ›¾ğœ†)bğ›¼ğ‘—for all previous ğ‘—in theâ„“-th worker
but only informs other workers to update their own coef-
ficients.
End parallel
4.2 AVFKAM
We summarize our main produce in Algorithm 1, and Algorithm 2
describe compute the local prediction for â„“-th worker. The Algo-
rithm 1 update the solution of equation (2) as follow:
4247Asynchronous Vertical Federated Learning for Kernelized AUC Maximization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Algorithm 2 AVFKAM for each worker â„“to calculate ğ‘“â„“(ğ‘¥)
Input:ğœ™ğœ”(ğ‘¥),ğ›¼Î›â„“,Î›â„“.
Output:ğ‘“â„“(ğ‘¥)
1:Initializeğ‘“â„“(Â·)=0.
2:whileğ‘–âˆˆÎ›â„“do
3: Obtainğœ™ğœ”ğ‘–(ğ‘¥)andbğ›¼ğ‘–from local memory.
4:bğ‘“â„“(ğ‘¥)=bğ‘“â„“(ğ‘¥)+bğ›¼ğ‘–ğœ™ğœ”ğ‘–(ğ‘¥).
5:end while
â€¢Sample random seed ğ‘–: we randomly sample an index as ran-
dom seed from{1,2,...,ğ‘}. According to the seed ğ‘–, we can
restore the correspond random direction ğœ”ğ‘–and random fea-
tureğœ™ğœ”ğ‘–(ğ‘¥+
ğ‘–),ğœ™ğœ”ğ‘–(ğ‘¥âˆ’
ğ‘–). Because we generate random num-
bers by a pseudo-random number generator, we can directly
readğœ™ğœ”ğ‘–(ğ‘¥+
ğ‘–),ğœ™ğœ”ğ‘–(ğ‘¥âˆ’
ğ‘–)obtained from preprocessing in the
memory.
â€¢Compute{bğ‘“(ğ‘¥+
ğ‘–),bğ‘“(ğ‘¥âˆ’
ğ‘–)}: To compute the prediction of global
modelğ‘“(ğ‘¥), we aggregate all local prediction. For â„“-th (1â‰¤
â„“â‰¤ğ‘) worker, we only read local coefficient ğ›¼Î›â„“, and com-
pute a local prediction ğ‘“â„“(ğ‘¥). This produce is summarized
in Algorithm 2. It is notice that bğ›¼Î›â„“may not be same as ğ›¼Î›â„“
due to asynchronous update, and results in bğ‘“(ğ‘¥ğ‘–)also may
be different from ğ‘“(ğ‘¥ğ‘–).
â€¢Update model: After computing {bğ‘“(ğ‘¥+
ğ‘–),bğ‘“(ğ‘¥âˆ’
ğ‘–)}, the domina-
tor computes a new coefficient ğ›¼ğ‘–and locally save it. Based
on equation (6), the previous coefficients ğ›¼ğ‘—also are updated
by(1âˆ’ğ›¾ğœ†). To improve the training efficiency, the dominator
sends an update task to other workers without waiting for
their completion. Therefore, our algorithm is considered in
the asynchronous setting.
Active Worker #1
Active Worker #2
Active Worker #3
Active Worker #4 ğ›¼0ğ›¼1 â€¦ Globalğ›¼2ğ›¼ğ‘–ğ‘–-thglobal model coefficients Write Read Compute
(a) Synchronous
Active Worker #1
Active Worker #2
Active Worker #3
Active Worker #4 ğ›¼0ğ›¼1ğ›¼2ğ›¼3ğ›¼4ğ›¼5ğ›¼6ğ›¼7ğ›¼8â€¦ Global
(b) Asynchronous
Figure 2: Synchronous v.s. Asynchronous4.3 Preprocessing
We notice that there are multiple aggregations including compute
ğœ™ğœ”(ğ‘¥)andğ‘“(ğ‘¥). Multiple aggregations may be limited to training
asynchronously. Excessive communication may cause the algorithm
to block the aggregation operations, which makes asynchronous
training of the algorithm difficult in training efficiently. To break
this limitation, we expect to reduce communication during training
by pseudo-random number generator [ 7]. The sampling procedure
can be restored by random seeds. Therefore, we can preprocess
training data with randomness and security and locally save ğœ™ğœ”(ğ‘¥),
which eliminates the influence due to frequently calculating ğœ™ğœ”(ğ‘¥).
More details are described in Appendix.
4.4 Challenge of Convergence Analysis
Although the significant improvement brought by the asynchro-
nous algorithm is obvious, there is a challenge to ensuring model
convergence. Our asynchrony directly impacts the model coef-
ficients. However, our convergence analysis relies on functional
gradients. If we expect to analyze convergence results based on the
functional gradient, finding the relationship between coefficients
ğ›¼and objective function ğ‘“in asynchronous scenarios is crucial.
To overcome this challenge, we leverage the duality between the
coefficients and the corresponding objective function, and transfer
the asynchrony of the coefficients to the objective function. Specifi-
cally, if the primal problem is (2), the dual problem can be denoted
as
ğ·(ğ›¼)=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘™âˆ—(âˆ’ğ›¼ğ‘–(ğ‘¥+
ğ‘–,ğ‘¥âˆ’
ğ‘–))+ğœ†
2âˆ¥ğ‘âˆ‘ï¸
ğ‘–=1ğ›¼ğ‘–(ğ‘¥+
ğ‘–,ğ‘¥âˆ’
ğ‘–)ğœ™ğœ”ğ‘–(ğ‘¥ğ‘–)âˆ¥2
H,(7)
whereğ‘™âˆ—is the conjugate of the loss function ğ‘™. Note that from
equation (2), we know that ğ›¼is influenced by pairwise data rather
than pointwise data. To simplify the representation, we still use ğ›¼
in the following content. Because ğ‘“(Â·)have
ğ‘“(ğ‘¥ğ‘–)=ğ‘âˆ‘ï¸
ğ‘–=1ğ›¼ğ‘–ğœ™ğœ”ğ‘–(ğ‘¥ğ‘–). (8)
We can obtain the primal solution by solving the dual problem
solution. Similar to the dual coordinate gradient algorithm [ 18,36,
37], we express the variation of ğ‘“(ğ‘¥)as a function of ğ›¼ğ‘¡
ğ‘–, as shown
in equation (9).
Î”bğ‘“ğ‘¡(ğ‘¥)=ğ‘¡âˆ‘ï¸
ğ‘ =1(Î”bğ›¼ğ‘¡
ğ‘ )ğœ™ğœ”(ğ‘¥). (9)
According to the equation (9), we can represent the variation of
functionğ‘“. More details are introduced in the next section.
5 THEORETICAL ANALYSIS
In this section, we provide convergence result and security guaran-
tee of AVFKAM.
5.1 Convergence Analysis
Assumption 5.1. Suppose the following conditions hold.
1. There exists an optimal solution, denoted as ğ‘“âˆ—to the optimiza-
tion problem.
2. The first order derivate of loss function ğ‘™(ğ‘§,ğ‘§â€²)isğ¿+-Lipschitz
4248KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ke Zhang et al.
continous in terms of ğ‘“(ğ‘¥+), andğ¿âˆ’-Lipschitz continous in terms
ofğ‘“(ğ‘¥âˆ’).
3. There exists 0â‰¤ğ‘€+â‰¤ âˆ and 0â‰¤ğ‘€âˆ’â‰¤ âˆ , such that
|ğ‘™â€²
1|â‰¤ğ‘€+,|ğ‘™â€²
2|â‰¤ğ‘€âˆ’.
4. The kernel functions and random feature norms are bounded,
ğ‘–.ğ‘’.,ğ‘˜(ğ‘¥,ğ‘¥â€²)â‰¤ğœ…,|ğœ™ğœ”(ğ‘¥)ğœ™ğœ”(ğ‘¥â€²)|â‰¤ğœ™.
For asynchronous algorithms, we consider the asynchrony effect
due to reading inconsistently. Leblond et al. [ 22] proposed an "after
read" approach to help analysis for asynchronous perturb itera-
tion framework. Many asynchronous algorithms [ 12,49,50] follow
this approach to their convergence analysis. In our algorithm, we
asynchronously update model coefficients ğ›¼ğ‘¡
ğ‘–in practice, which
means that the asynchrony effect of the objective functional is not
accessed directly. To address this difficulty, we first analyze the
asynchronous effect on the model coefficients. After ğ‘¡-th iteration,
there exists a set consisting of the iteration indices of coefficients
saved inâ„“-th worker, we define this set as Î›(â„“,ğ‘¡). Define Î”ğ›¼ğ‘ as the
variation of the(ğ‘¡+1)-th iteration in{ğ›¼ğ‘¡+1ğ‘ }ğ‘¡+1
ğ‘ =1, we can represent
the variation of ğ›¼ğ‘¡
ğ‘–
Î”bğ›¼ğ‘ =bğ›¼ğ‘¡+1
ğ‘ âˆ’bğ›¼ğ‘¡
ğ‘ 
=(
ğœ‚ğ‘ , ğ‘  =ğ‘¡+1
ğ›¾ğœ†(1âˆ’ğ›¾ğœ†)|Î›(â„“,ğ‘¡)|âˆ’|Î›(â„“,ğ‘ )|ğœ‚ğ‘ , ğ‘ âˆˆÎ›(â„“,ğ‘¡)(10)
whereğœ‚ğ‘ =âˆ’ğ›¾(ğ‘™â€²
1(ğ‘“(ğ‘¥+ğ‘ ),ğ‘“(ğ‘¥âˆ’ğ‘ ))ğœ™ğœ”ğ‘ (ğ‘¥+ğ‘ )+ğ‘™â€²
2(ğ‘“(ğ‘¥+ğ‘ ),ğ‘“(ğ‘¥âˆ’ğ‘ )ğœ™ğœ”ğ‘ (ğ‘¥âˆ’ğ‘ )).
Additionally, we denote the variation of function with the dual re-
lationship. Since ğ‘“ğ‘¡(Â·)=Ãğ‘¡
ğ‘–=1bğ›¼ğ‘¡
ğ‘–ğœ™ğœ”(Â·),
ğ‘“ğ‘¡+1(Â·)âˆ’ğ‘“ğ‘¡(Â·)=âˆ‘ï¸
ğ‘ âˆˆÎ›(â„“,ğ‘¡)(Î”bğ›¼ğ‘¡
ğ‘ )ğœ™ğœ”(Â·)=âˆ’ğ›¾
ğœ‚(ğ‘¡,ğœ™)+ğœ†bğ‘“â„“
ğ‘¡
,(11)
whereğœ‚(ğ‘¡,ğœ™)=ğ‘™â€²
1(ğ‘“(ğ‘¥+ğ‘ ),ğ‘“(ğ‘¥âˆ’ğ‘ ))ğœ™ğœ”ğ‘¡(ğ‘¥+
ğ‘¡)ğœ™ğœ”(Â·)+ğ‘™â€²
2(ğ‘“(ğ‘¥+ğ‘ ),ğ‘“(ğ‘¥âˆ’ğ‘ ))
ğœ™ğœ”ğ‘¡(ğ‘¥âˆ’
ğ‘¡)ğœ™ğœ”(Â·). Note that bğ‘“â„“
ğ‘¡in equation (11) represents the decision
value of the â„“-th worker and is different from bğ‘“ğ‘¡, which is partial
composition of bğ‘“ğ‘¡,ğ‘–.ğ‘’.,âˆ¥bğ‘“ğ‘¡âˆ¥Hâ‰¥âˆ¥bğ‘“â„“
ğ‘¡âˆ¥H. Defineğ‘”ğ‘¡,â„“:=ğœ‚(ğ‘¡,ğœ™)+ğœ†bğ‘“â„“
ğ‘¡,
we obtain the following explicit effect of asynchrony
bğ‘“ğ‘¡âˆ’ğ‘“ğ‘¡=ğ›¾âˆ‘ï¸
ğ‘¢âˆˆğ·(ğ‘¡)ğ‘”ğ‘¡,ğœ“(ğ‘¢),(12)
whereâˆ€ğ‘¢âˆˆğ·(ğ‘¡), and we have ğ‘¢â‰¤ğ‘¡. We defineğ·(ğ‘¡)in Assumption
5.2.
Assumption 5.2. We defineğ·(ğ‘¡)as a set of iterations, and assume
that there exists an upper bound ğœsuch thatğœâ‰¥ğ‘¡âˆ’min{ğ‘¢|ğ‘¢âˆˆğ·(ğ‘¡)}
for all iterations ğ‘¡.
Given the epoch number ğ‘£(ğ‘¡)as a global iteration counter, we
provide the convergence result for AVFKAM, and full details are
provided in Appendix. Because ğ‘“ğ‘¡may not be in the RKHS H, we
construct an intermediate function â„ğ‘¡inH. Following the analysis
framework of [ 7,8], we first decompose the error into the error
from random features |ğ‘“ğ‘¡+1(ğ‘¥)âˆ’â„ğ‘¡+1(ğ‘¥)|2and the error from data
randomnessâˆ¥â„ğ‘¡+1âˆ’ğ‘“âˆ—âˆ¥2
Hin equation (13).
|ğ‘“ğ‘¡+1(ğ‘¥)âˆ’ğ‘“âˆ—(ğ‘¥)|2â‰¤2|ğ‘“ğ‘¡+1(ğ‘¥)âˆ’â„ğ‘¡+1(ğ‘¥)|2+2ğœ…âˆ¥â„ğ‘¡+1âˆ’ğ‘“âˆ—âˆ¥2
H.
(13)
Before we show the Theorem 5.7, let ğ¿=ğ¿++ğ¿âˆ’,ğ‘€=ğ‘€++ğ‘€âˆ’,
we provide two significant lemmas.Lemma 5.3. Forâˆ€ğ‘¥âˆˆX and0â‰¤ğ›¾â‰¤1
ğœ†, we denote ğ‘Šğ‘–(ğ‘¥):=
ğ‘ğ‘¡
ğ‘–(ğœğ‘–(ğ‘¥)âˆ’ğœ‰ğ‘–(ğ‘¥)). We obtain the upper bound of the error due to
random features.
E|ğ‘“ğ‘¡+1(ğ‘¥)âˆ’â„ğ‘¡+1(ğ‘¥)|2â‰¤ğ‘€2(ğœ…+ğœ™)2ğ›¾
ğœ†. (14)
Remark 5.4. Lemma 5.3 shows that the error due to random
features is independent of time counter ğ‘¡, meaning this error is
stable in the perturbed iterate framework.
Lemma 5.5. Set1
ğœ†â‰¥ğ›¾â‰¥0, let1
ğœ†â‰¥ğ›¾â‰¥0, letğ¶1(ğ›¾):=
ğ›¾âˆšğœ…ğ¿ğ‘€ 2ğ›¾ğœâˆšï¸
ğœ™+2ğ›¾ğœâˆšğœ…+ğœ™+ğœ…
ğœ†andğ¶2(ğ›¾):=4ğ›¾2ğœğœ…ğ¿ğ‘€2 2ğ›¾ğœâˆšï¸
ğœ™+
2ğ›¾ğœâˆšğœ…+ğœ™+ğœ…
ğœ†+4ğ›¾2ğœ…ğ‘€2(1+2ğœâˆ’4ğ›¾ğœğœ†), we will reach Eâˆ¥â„ğ‘¡âˆ’ğ‘“âˆ—âˆ¥2
Hâ‰¤ğœ–,
the epoch number ğ‘£(ğ‘¡)should satisfy the following condition
ğ‘£(ğ‘¡)â‰¥1
3ğ›¾ğœ†log 2Eâˆ¥â„1âˆ’ğ‘“âˆ—âˆ¥2
H
ğœ–!
, (15)
whereğ›¾should satisfy
ğ¶1(ğ›¾)2+ğ›¾ğœ†ğ¶ 2(ğ›¾)
3ğ›¾2ğœ†2â‰¤ğœ–
2. (16)
Remark 5.6. Lemma 5.5 shows that the convergence result of the
error due to random data is O(1âˆšğœ–log(1
ğœ–)).
According to the above lemmas, we obtain the total convergence
results in expectation as shown in Theorem 5.7.
Theorem 5.7 (Convergence result). Setğœ–>0, under as-
sumption 1 and 2, to achieve the accuracy ğœ–of asynchronous algo-
rithm,ğ‘–.ğ‘’.,E|ğ‘“ğ‘¡(ğ‘¥)âˆ’ğ‘“âˆ—(ğ‘¥)|2â‰¤ğœ–, letğ¶1(ğ›¾):=ğ›¾âˆšğœ…ğ¿ğ‘€ 2ğ›¾ğœâˆšï¸
ğœ™+
2ğ›¾ğœâˆšğœ…+ğœ™+ğœ…
ğœ†andğ¶2(ğ›¾):=4ğ›¾2ğœğœ…ğ¿ğ‘€2 2ğ›¾ğœâˆšï¸
ğœ™+2ğ›¾ğœâˆšğœ…+ğœ™+ğœ…
ğœ†+
4ğ›¾2ğœ…ğ‘€2(1+2ğœâˆ’4ğ›¾ğœğœ†), the epoch number ğ‘£(ğ‘¡)should satisfy the
following condition
ğ‘£(ğ‘¡)â‰¥1
3ğ›¾ğœ†log 4ğœ…Eâˆ¥â„1âˆ’ğ‘“âˆ—âˆ¥2
H
ğœ–!
, (17)
andğ›¾should satisfy
1
ğœ†â‰¥ğ›¾â‰¥0,
ğ›¾â‰¥ğœ–ğœ†
4ğ‘€2(âˆšğœ…+âˆšï¸
ğœ™)2,
ğ¶1(ğ›¾)2+ğ›¾ğœ†ğ¶ 2(ğ›¾)
3ğ›¾2ğœ†2â‰¤ğœ–
4ğœ….(18)
Remark 5.8. Theorem 5.7 shows that, for any given data ğ‘¥, the
evaluated value of ğ‘“ğ‘¡atğ‘¥will converge to that of a solution close
toğ‘“âˆ—in terms of the Euclidean distance. Noting that the second
condition of ğ›¾dominates the order of convergence rate, thus the
convergence rate of AVFKAM is O(1/ğ‘¡), if eliminating the log(1/ğœ–)
factor.
4249Asynchronous Vertical Federated Learning for Kernelized AUC Maximization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
020040060080010001200140016000.8600.8650.8700.8750.8800.8850.890AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—)
(a) a9a
050010001500200025003000350040000.98850.98900.98950.99000.99050.9910AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—) (b) shuttle
0200400600800100012001400160018000.580.600.620.640.660.680.70AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—) (c) susy
02000400060008000100001200014000160000.760.780.800.820.840.860.880.900.920.940.96AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—) (d) real-sim
01 0002 0003 0004 0000.820.840.860.880.900.920.94AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—)
(e) rcv1
01 00020003000400050000.800.820.840.860.880.90AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—) (f) news20
01000020000300004000050000600000.890.900.910.920.930.940.95AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—) (g) ijcnn1
0500100015002000250030000.780.800.820.840.860.880.900.92AUC ScoreT
ime(s) VFKAM    (1Ã—) 
VFKAM    (2Ã—) 
VFKAM    (4Ã—) 
AVFKAM (1Ã—) 
AVFKAM (2Ã—) 
AVFKAM (4Ã—) (h) sector
Figure 3: AUC Score v.s. training time on all datasets with different delay fractions.
5.2 Security Analysis
We discuss the data security of AVFKAM under the semi-honest
assumption, a commonly adopted assumption in previous works
[6,10,41,49]. According to this assumption, our AVFKAM can
prevent the inference attack. More details are provided in Appendix.
6 EXPERIMENTS
In this section, we provide the experimental results on imbalanced
datasets to evaluate the performance of AVFKAM.
6.1 Compared Methods
We compare our algorithm with some state-of-the-art methods with
regard to AUC maximization. All methods are implemented in C++
using MPI and Armadillo [35].
â€¢FDSKL [ 10]: a vertical federated kernel learning algorithm
with random feature approximation. FDSKL is a pointwise
algorithm and is limited in synchronous computation.
â€¢TSAM [ 8]: a kernel-based AUC maximization learning algo-
rithm with triply stochastic optimization.
â€¢RankSVM with kernel approximation1[3]: a modified Kernel
RankSVM algorithm with random Fourier feature approxi-
mation and NystrÃ¶m.
â€¢Sparse Kernel AUC2[21]: a sparse kernel-based classifier
and greedily adding the required number of basis functions
into the model.
1https://github.com/KaenChan/rank-kernel-appr.
2https://www.dropbox.com/s/6e4fsj2hlq1b71n/Code.rar?dl=0.Table 2: Dataset Descriptions.
Datasets Size Features ğ‘âˆ’/ğ‘+
shuttle 44950 9 3.6483
SUSY 1000000 18 1.1713
ijcnn1 141691 22 9.3008
a9a 48842 123 3.1527
real-sim 72309 20958 2.2399
rcv1 534135 47236 26.8426
sector 20242 55197 0.9234
news20 19928 62061 18.9937
6.2 Experimental Setup
Our experiment results are performed on an Intel dual-socket ma-
chine with 187GB RAM, each socket is associated with 14 com-
putation cores. The information of the imbalanced datasets3is
summarized in Table 2. We use the training dataset or randomly
sample 80% data from the dataset, and the testing dataset or the
rest as the testing data. The multi-class classification datasets are
transformed into class-imbalanced binary datasets ( ğ‘–.ğ‘’., we denote
the1-st class as the positive class, and the rest of the classes as
the negative class). We set a synthetic straggler worker which is
slower than the fastest worker for simulating the real application
scenario. In our experiments, we choose a suitable learning rate ğ›¾
from 1ğ‘’1to1ğ‘’âˆ’3, and regularization coefficient ğœ†from 1ğ‘’0to1ğ‘’âˆ’6.
For the kernel-based model, we use the Gaussian kernel and tune
the kernel bandwidth ğœ.
3These datasets are available at https://www.csie.ntu.edu.tw/ âˆ¼cjlin/libsvmtools/datasets/
4250KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ke Zhang et al.
Table 3: Average AUC performance (mean Â±std) with all baselines. (â€˜ - â€™ means runtime is too long)
TASM Sparse Kernel AUC RankRandomFourier RankNystrÃ¶m FDSKL AVFKAM
shuttle 0.9981Â±0.0008 0.9127Â±0.0083 0 .9025Â±0.0013 0.9216Â±0.0032 0.4974Â±0.0031 0.9978Â±0.0002 0.9978Â±0.0002 0.9978Â±0.0002
rcv1 0.9742Â±0.0018 0.9714Â±0.0052 0 .9611Â±0.0024 0.9583Â±0.0017 0.4813Â±0.0043 0.9739Â±0.0040 0.9739Â±0.0040 0.9739Â±0.0040
SUSY 0.8104Â±0.0212 0.7746Â±0.0034 âˆ’ âˆ’ 0.5799Â±0.0183 0.8066Â±0.0128 0.8066Â±0.0128 0.8066Â±0.0128
ijcnn1 0.9642Â±0.0120 0.6627Â±0.0731 âˆ’ âˆ’ 0.5169Â±0.0023 0.9636Â±0.0067 0.9636Â±0.0067 0.9636Â±0.0067
a9a 0.8999Â±0.0021 0.8765Â±0.0015 0 .9011Â±0.0024 0.8972Â±0.0034 0.4996Â±0.0016 0.9000Â±0.0012 0.9000Â±0.0012 0.9000Â±0.0012
real-sim 0.9661Â±0.0012 0.8214Â±0.0098 0 .9213Â±0.0048 0.9418Â±0.0016 0.5757Â±0.0061 0.9631Â±0.0008 0.9631Â±0.0008 0.9631Â±0.0008
sector 0.9706Â±0.0042 0.8168Â±0.0812 0 .9581Â±0.0523 0.9627Â±0.0032 0.4990Â±0.0128 0.9702Â±0.0033 0.9702Â±0.0033 0.9702Â±0.0033
news20 0.8567Â±0.0010 0.8711Â±0.0047 0 .8623Â±0.0058 0.8543Â±0.0084 0.4748Â±0.0084 0.8563Â±0.0007 0.8563Â±0.0007 0.8563Â±0.0007
1234567812345678SpeedupT
ime(s) Ideal 
Sync 
Asyn(ours)
(a) a9a
1234567812345678Speedup#
worker Ideal 
Sync 
Asyn(ours) (b) shuttle
1234567812345678Speedup#
worker Ideal 
Sync 
Asyn(ours) (c) susy
1234567812345678Speedup#
worker Ideal 
Sync 
Asyn(ours) (d) real-sim
1234567812345678Speedup#
worker Ideal 
Sync 
Asyn(ours)
(e) rcv1
1234567812345678Speedup#
worker Ideal 
Sync 
Asyn(ours) (f) news20
1234567812345678Speedup#
worker Ideal 
Sync 
Asyn(ours) (g) ijcnn1
1234567812345678Speedup#
worker Ideal 
Sync 
Asyn(ours) (h) sector
Figure 4: Multiple-workers speedup scalability.
6.3 Asynchronous Efficiency
In these experiments, we demonstrate the efficiency of AVFKAM by
comparing it with the synchronous counterpart (VFKAM). Specifi-
cally, we set ğ‘=8,ğ‘š=4and select appropriate hyper-parameters.
To simulate the real application scenario, we set a delay fraction to
evaluate the superiority of AVFKAM. The delay fraction is selected
from{1,2,4}, which indicates that the straggler worker operates
several times slower than faster workers. For example, if the delay
fraction is "2Ã—", the fastest worker costs 1s in an iteration, then the
straggler costs 3s (1+2 times). Therefore, we demonstrate the curves
of AUC score versus training time on the imbalance datasets in
Figure 3. The convergence speed of AVFKAM is faster than VFKAM.
The results show that the asynchronous algorithm is more efficient
than the synchronous algorithm. As the delay fraction increases,
the runtime of the synchronous counterpart becomes longer while
AVFKAM remains stable within a small time range. Therefore, our
AVFKAM is more adaptable to complex heterogeneous networks
compared to the synchronous counterpart.6.4 AUC Performance
We present the AUC performance in Table 3. Specifically, we set
ğ‘=8,ğ‘š=4for parallel algorithms and select appropriate hyper-
parameters to report the AUC score. For each dataset, we average
the results over 10 trials and calculate the standard deviation. From
the results in Table 3, AVFKAM achieves comparable AUC perfor-
mance with the state-of-the-art kernel-based AUC maximization
algorithms. Compared to the existing vertical federated kernel learn-
ing algorithm FDSKL, AVFKAM demonstrates powerful superiority
in handling imbalanced data.
6.5 Linear Speedup
Speedup :=Training time for the serial computation
Training time for the multi-parties.
In these experiments, we intend to evaluate the speedup efficiency
of AVFKAM. Specifically, TSAM is used as the serial method, we
calculate the speedup performance with different numbers of work-
ers synchronously and asynchronously, respectively. We draw up
4251Asynchronous Vertical Federated Learning for Kernelized AUC Maximization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
the results in Figure 4. The results show that AVFKAM exhibits sig-
nificantly better speedup scalability compared to its synchronous
counterpart and achieves close to linear speedup.
6.6 Sensitivity Analysis of AVFKAM
The consideration of hyper-parameter settings is fundamental for
our algorithmâ€™s performance. We analyze the bandwidth param-
eterğœand the regularization coefficient ğœ†across eight datasets.
Changing one of the parameters and maintaining other parame-
ters, we report the AUC score with respect to the training time.
Experimental results and discussions are provided in Appendix.
Table 4: Average AUC performance with VFL methods.
V
AFL AFSGD-VP AsySQN AVFKAM
r
eal-sim 0.9735Â±0.0024 0.9487Â±0.0026 0.9489Â±0.0014 0.9636Â±0.0067
rcv1 0.9843Â±0.0015 0.9601Â±0.0012 0.9699Â±0.0024 0.9739Â±0.0002
sector 0.9836Â±0.0003 0.9612Â±0.0014 0.9569Â±0.0008 0.9702Â±0.0033
news20 0.9500Â±0.0002 0.8304Â±0.0092 0.8283Â±0.0062 0.8563Â±0.0007
6.7 Compare with Existing VFL Methods
In this subsection, we compared the performance of our algorithm
with existing VFL algorithms (Please see Table 1). Specifically, we
setğ‘=8,ğ‘š=4and compare with linear models (AFSGD-VP and
AsySQN) and neural network model (VAFL). We measure the AUC
performance of these methods and report average results over 10
trials. The results are shown in Table 4.
From Table 4, we find the performance of AVFKAM is superior
to linear model algorithms why the kernel-based model often can
achieve better prediction performance than the linear model. Be-
sides, we noticed that the AUC score of AVFKAM is slightly lower
than VAFL. However, it does not mean that AVFKAM is mean-
ingless. Neural network algorithms are often difficult to apply in
scenarios with high interpretability requirements. Instead, kernel
methods are highly interpretable by mapping nonlinear data into
linearly separable feature spaces, making inferences in a human-
understandable manner. It is worth noting that many VFL applica-
tions are in sensitive domains where models are required to have
reliable interpretability (e.g., healthcare, and finance).
7 CONCLUSION
In this paper, we propose a novel asynchronous vertical federated
kernelized AUC maximization algorithm to address two imbalanced
issues in the real world: imbalanced data and imbalanced compu-
tational resources. To the best of our knowledge, AVFKAM is the
first asynchronous vertical federated kernel learning for AUC max-
imization. More significantly, on the basis of the dual relationship
between model coefficients and objective function, we provide the
convergence result of AVFKAM to be O(1/ğ‘¡)whereğ‘¡represents the
global iteration. Meanwhile, AVFKAM can guarantee data security
under the semi-honest assumption. Extensive experimental results
demonstrate that AVFKAM can converge faster than its synchro-
nous counterpart and achieve approximately linear speedup. From
the perspective of AUC performance, AVFKAM has the more pow-
erful ability to handle imbalanced data compared with the existing
state-of-the-art vertical federated kernel method and maintains asimilar AUC performance to existing kernel-based AUC maximiza-
tion algorithms.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science
Foundation of China (Nos. 12071166, 62076041, 62076138, 62276111,
62376104), and the Fundamental Research Funds for the Central
Universities of China (Nos. 2662023XXPY002, 2662023LXPY005,
2662024XXPY001).
REFERENCES
[1]Timothy Castiglia, Shiqiang Wang, and Stacy Patterson. 2023. Flexible vertical
federated learning with heterogeneous parties. IEEE Transactions on Neural
Networks and Learning Systems (2023).
[2]Jun Chen, Hong Chen, Bin Gu, and Hao Deng. 2023. Fine-Grained Theoretical
Analysis of Federated Zeroth-Order Optimization. In Thirty-seventh Conference
on Neural Information Processing Systems.
[3]Kai Chen, Rongchun Li, Yong Dou, Zhengfa Liang, Qi Lv, et al .2017. Ranking
support vector machine with kernel approximation. Computational intelligence
and neuroscience 2017 (2017).
[4]Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. 2020. Vafl: a method of
vertical asynchronous federated learning. arXiv preprint arXiv:2007.06081 (2020).
[5]Weijing Chen, Guoqiang Ma, Tao Fan, Yan Kang, Qian Xu, and Qiang Yang. 2021.
Secureboost+: A high performance gradient boosting tree framework for large
scale vertical federated learning. arXiv preprint arXiv:2110.10927 (2021).
[6]Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, Dimitrios Papadopou-
los, and Qiang Yang. 2021. Secureboost: A lossless federated learning framework.
IEEE Intelligent Systems 36, 6 (2021), 87â€“98.
[7]Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le
Song. 2014. Scalable kernel methods via doubly stochastic gradients. Advances
in Neural Information Processing Systems 27 (2014).
[8]Zhiyuan Dang, Xiang Li, Bin Gu, Cheng Deng, and Heng Huang. 2020. Large-scale
nonlinear auc maximization via triply stochastic gradients. IEEE Transactions on
Pattern Analysis and Machine Intelligence 44, 3 (2020), 1385â€“1398.
[9]Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N Jones, and Yong
Zhou. 2022. Communication-efficient stochastic zeroth-order optimization for
federated learning. IEEE Transactions on Signal Processing 70 (2022), 5058â€“5073.
[10] Bin Gu, Zhiyuan Dang, Xiang Li, and Heng Huang. 2020. Federated doubly
stochastic kernel learning for vertically partitioned data. In Proceedings of the
26th ACM SIGKDD international conference on knowledge discovery & data mining .
2483â€“2493.
[11] Bin Gu, Zhouyuan Huo, and Heng Huang. 2019. Scalable and efficient pairwise
learning to achieve statistical accuracy. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 33. 3697â€“3704.
[12] Bin Gu, An Xu, Zhouyuan Huo, Cheng Deng, and Heng Huang. 2021. Privacy-
preserving asynchronous vertical federated learning algorithms for multiparty
collaborative learning. IEEE transactions on neural networks and learning systems
33, 11 (2021), 6103â€“6115.
[13] Zhishuai Guo, Rong Jin, Jiebo Luo, and Tianbao Yang. 2023. FeDXL: provable
federated learning for deep X-risk optimization. In International Conference on
Machine Learning. PMLR, 11934â€“11966.
[14] Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li Shen, Wei Liu, and Tianbao Yang.
2020. Communication-efficient distributed stochastic auc maximization with
deep neural networks. In International conference on machine learning. PMLR,
3864â€“3874.
[15] Zhishuai Guo, Yan Yan, Zhuoning Yuan, and Tianbao Yang. 2023. Fast Objective &
Duality Gap Convergence for Non-Convex Strongly-Concave Min-Max Problems
with PL Condition. Journal of Machine Learning Research 24 (2023), 1â€“63.
[16] Zheng-Chu Guo, Yiming Ying, and Ding-Xuan Zhou. 2017. Online regularized
learning with pairwise loss functions. Advances in Computational Mathematics
43 (2017), 127â€“150.
[17] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne. 2017. Private federated learning on vertically
partitioned data via entity resolution and additively homomorphic encryption.
arXiv preprint arXiv:1711.10677 (2017).
[18] Cho-Jui Hsieh, Hsiang-Fu Yu, and Inderjit Dhillon. 2015. Passcode: Parallel
asynchronous stochastic dual co-ordinate descent. In International Conference on
Machine Learning. PMLR, 2370â€“2379.
[19] Yaochen Hu, Di Niu, Jianming Yang, and Shengping Zhou. 2019. FDML: A
collaborative machine learning framework for distributed features. In Proceedings
of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 2232â€“2240.
4252KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ke Zhang et al.
[20] Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al .2021. Advances and open problems in federated learning.
Foundations and TrendsÂ® in Machine Learning 14, 1â€“2 (2021), 1â€“210.
[21] Vishal Kakkar, Shirish Shevade, S Sundararajan, and Dinesh Garg. 2017. A sparse
nonlinear classifier design using AUC optimization. In Proceedings of the 2017
SIAM International Conference on Data Mining. SIAM, 291â€“299.
[22] RÃ©mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. 2017. Asaga: Asyn-
chronous parallel saga. In Artificial Intelligence and Statistics. PMLR, 46â€“54.
[23] Yunwen Lei, Antoine Ledent, and Marius Kloft. 2020. Sharper generalization
bounds for pairwise learning. Advances in Neural Information Processing Systems
33 (2020), 21236â€“21246.
[24] Yunwen Lei, Mingrui Liu, and Yiming Ying. 2021. Generalization guarantee of
SGD for pairwise learning. Advances in Neural Information Processing Systems 34
(2021), 21216â€“21228.
[25] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. 2018. Asynchronous decentral-
ized parallel stochastic gradient descent. In International Conference on Machine
Learning. PMLR, 3043â€“3052.
[26] Tianyi Lin, Chi Jin, and Michael Jordan. 2020. On gradient descent ascent for
nonconvex-concave minimax problems. In International Conference on Machine
Learning. PMLR, 6083â€“6093.
[27] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye
Ouyang, Ya-Qin Zhang, and Qiang Yang. 2022. Vertical federated learning. arXiv
preprint arXiv:2211.12814 (2022).
[28] Yang Liu, Xinwei Zhang, Yan Kang, Liping Li, Tianjian Chen, Mingyi Hong, and
Qiang Yang. 2022. FedBCD: A communication-efficient collaborative learning
framework for distributed features. IEEE Transactions on Signal Processing 70
(2022), 4277â€“4290.
[29] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics . PMLR,
1273â€“1282.
[30] James Mercer. 1909. Xvi. functions of positive and negative type, and their
connection the theory of integral equations. Philosophical transactions of the
royal society of London. Series A, containing papers of a mathematical or physical
character 209, 441-458 (1909), 415â€“446.
[31] Michael Natole, Yiming Ying, and Siwei Lyu. 2018. Stochastic proximal algorithms
for AUC maximization. In International Conference on Machine Learning. PMLR,
3710â€“3719.
[32] Michael Natole Jr, Yiming Ying, and Siwei Lyu. 2019. Stochastic AUC optimiza-
tion algorithms with linear convergence. Frontiers in Applied Mathematics and
Statistics 5 (2019), 30.
[33] H Rafique, M Liu, Q Lin, and T Yang. 1810. Non-convex minâ€“max optimization:
provable algorithms and applications in machine learning (2018). arXiv preprint
arXiv:1810.02060 (1810).
[34] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild!:
A lock-free approach to parallelizing stochastic gradient descent. Advances in
neural information processing systems 24 (2011).[35] Conrad Sanderson and Ryan Curtin. 2016. Armadillo: a template-based C++
library for linear algebra. Journal of Open Source Software 1, 2 (2016), 26.
[36] Shai Shalev-Shwartz and Tong Zhang. 2013. Stochastic dual coordinate ascent
methods for regularized loss minimization. Journal of Machine Learning Research
14, 1 (2013).
[37] Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht.
2016. Large scale kernel learning using block coordinate descent. arXiv preprint
arXiv:1602.05310 (2016).
[38] Xidong Wu, Zhengmian Hu, Jian Pei, and Heng Huang. 2023. Serverless federated
auprc optimization for multi-party collaborative imbalanced data mining. In
Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2648â€“2659.
[39] Cong Xie, Sanmi Koyejo, and Indranil Gupta. 2019. Asynchronous federated
optimization. arXiv preprint arXiv:1903.03934 (2019).
[40] Chenhao Xu, Youyang Qu, Yong Xiang, and Longxiang Gao. 2023. Asynchronous
federated learning on heterogeneous devices: A survey. Computer Science Review
50 (2023), 100595.
[41] Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, and Heiko Ludwig. 2019.
Hybridalpha: An efficient approach for privacy-preserving federated learning.
InProceedings of the 12th ACM workshop on artificial intelligence and security.
13â€“23.
[42] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine
learning: Concept and applications. ACM Transactions on Intelligent Systems and
Technology (TIST) 10, 2 (2019), 1â€“19.
[43] Tianbao Yang and Yiming Ying. 2022. AUC maximization in the era of big data
and AI: A survey. Comput. Surveys 55, 8 (2022), 1â€“37.
[44] Yiming Ying, Longyin Wen, and Siwei Lyu. 2016. Stochastic online AUC maxi-
mization. Advances in neural information processing systems 29 (2016).
[45] Yiming Ying and Ding-Xuan Zhou. 2016. Online pairwise learning algorithms.
Neural computation 28, 4 (2016), 743â€“777.
[46] Zhuoning Yuan, Zhishuai Guo, Yi Xu, Yiming Ying, and Tianbao Yang. 2021.
Federated deep AUC maximization for hetergeneous data with a constant com-
munication complexity. In International Conference on Machine Learning. PMLR,
12219â€“12229.
[47] Haizhang Zhang, Yuesheng Xu, and Jun Zhang. 2009. Reproducing Kernel Banach
Spaces for Machine Learning. Journal of Machine Learning Research 10, 12 (2009).
[48] Jie Zhang, Song Guo, Zhihao Qu, Deze Zeng, Haozhao Wang, Qifeng Liu, and
Albert Y Zomaya. 2022. Adaptive vertical federated learning on unbalanced
features. IEEE Transactions on Parallel and Distributed Systems 33, 12 (2022),
4006â€“4018.
[49] Qingsong Zhang, Bin Gu, Cheng Deng, Songxiang Gu, Liefeng Bo, Jian Pei, and
Heng Huang. 2021. Asysqn: Faster vertical federated learning algorithms with
better computation resource utilization. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 3917â€“3927.
[50] Qingsong Zhang, Bin Gu, Cheng Deng, and Heng Huang. 2021. Secure bilevel
asynchronous vertical federated learning with backward updating. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 35. 10896â€“10904.
[51] Xinwen Zhang, Yihan Zhang, Tianbao Yang, Richard Souvenir, and Hongchang
Gao. 2023. Federated Compositional Deep AUC Maximization. arXiv preprint
arXiv:2304.10101 (2023).
4253Asynchronous Vertical Federated Learning for Kernelized AUC Maximization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
A SENSITIVITY ANALYSIS OF AVFKAM
We obtain results to analyze their affects in Figure 5, 6. From Figure 5, ğœis a key parameter for training kernel model, inappropriate ğœmay
result in untrustworthy model. Therefore, choosing a appropriate ğœis important for each datasets. Figure 6 provides the results on different
regularization parameters. It shows that excessive penalty parameter may result in performance degradation on the most of datasets. The
phenomenon is consistent with our theoretical result.
01 020304050600.8880.8900.8920.8940.8960.8980.9000.902AUC ScoreT
ime(s) Ïƒ = 1e-5 
Ïƒ = 5e-4 
Ïƒ = 5e-3
(a) a9a
01 0002 0003 0004 0000.880.900.920.940.960.981.00AUC ScoreT
ime(s) Ïƒ = 5e-3  
Ïƒ = 1e0 
Ïƒ = 5e0 (b) shuttle
0500100015002000250030000.500.550.600.650.700.750.800.85AUC ScoreT
ime(s) Ïƒ = 1e-5 
Ïƒ = 1e-3 
Ïƒ = 5e-3 (c) susy
01 0002 0003 0004 0000.50.60.70.80.91.0AUC ScoreT
ime(s) Ïƒ = 1e-3 
Ïƒ = 1e0 
Ïƒ = 5e3 (d) real-sim
Figure 5: AUC Score v.s. training time on all datasets with different kernel bandwidth ğœ.
01020304050600.8600.8650.8700.8750.8800.8850.8900.8950.900AUC ScoreT
ime(s) Î» = 1e-5 
Î» = 1e-4 
Î» = 1e-1
(a) a9a
050010001500200025003000350040000.9550.9600.9650.9700.9750.9800.9850.9900.995AUC ScoreT
ime(s) Î» = 1e-6 
Î» = 5e-5 
Î» = 5e-4 (b) shuttle
01002003004005006007008000.800.850.900.951.00AUC ScoreT
ime(s) Î» = 5e-3 
Î» = 5e-2 
Î» = 1e-1 (c) susy
01 0002 0003 0004 0000.780.800.820.840.860.880.900.920.940.96AUC ScoreT
ime(s) Î» = 1e-2 
Î» = 1e-1 
Î» = 5e-1 (d) real-sim
Figure 6: AUC Score v.s. training time on all datasets with different regularization parameters ğœ†.
B TOW TOTALLY DIFFERENT TREE STRUCTURES
Take 4 workers as an example, we depict that the result from workers {2, 3, 4} can be added to worker 1 in Figure 7. Figure 7 demonstrates
two tree communication structures: ğ‘‡1(left) andğ‘‡2(right). Although the result from worker 1 is consistent, the intermediate results are
different. The reason is they utilize different pairing manners.
Definition B.1 (Two totally different tree structures). For two tree structures ğ‘‡1andğ‘‡2, they are totally different if there does not exist a
subtree bğ‘‡1ofğ‘‡1and a subtree bğ‘‡2ofğ‘‡2whose size is larger than 1 and smaller than ğ‘‡1andğ‘‡2, respectively, such that leaf ( bğ‘‡1) = leaf ( bğ‘‡2).
Forğ‘‡1, worker 1 aggregates the results from worker 1 and worker 2; worker 3 aggregates the results from worker 3 and worker 4. Finally,
worker 1 aggregates the results from worker 1 and worker 3. For ğ‘‡2, worker 1 aggregates the results from worker 1 and worker 3; worker 2
aggregates the results from worker 2 and worker 4. Finally, worker 1 aggregates the results from worker 1 and worker 2.
The different aggregation paths correspond to different subtrees, which means that there is not a subtree bğ‘‡1ofğ‘‡1and a subtree bğ‘‡2ofğ‘‡2
such that leaf ( bğ‘‡1) = leaf ( bğ‘‡2). Therefore, we can easily find that totally different tree structures can prevent data leakage using ğ‘‡1andğ‘‡2.
C PREPROCESSING
We present our preprocessing in this section. Before asynchronously training our kernel model, all workers aggregate features and obtain a
sequence{ğœ™ğ‘¤ğ‘–(ğ‘¥ğ‘–)}ğ‘‡
ğ‘–=1. We apply the RBF kernel, ğœ™ğ‘¤ğ‘–(ğ‘¥ğ‘–)is denoted as ğœ™ğ‘¤ğ‘–(ğ‘¥ğ‘–)=âˆš
2 cos(ğ‘¤ğ‘–ğ‘¥ğ‘–+ğ‘), where(ğ‘¤ğ‘–ğ‘¥ğ‘–+ğ‘)=(Ãğ‘
â„“=1(ğ‘¤ğ‘–)Gâ„“(ğ‘¥ğ‘–)Gâ„“+ğ‘).
We summarize the details of preprocessing in Algorithm 3.
If theâ„“-th worker aims to compute ğ‘¤ğ‘‡
ğ‘–ğ‘¥ğ‘–+ğ‘, it aggregates other workersâ€™ results using the above tree communication structure. â„“â€²-th
worker (â„“â€²=1,...,ğ‘ ) samples(ğœ”ğ‘–)Gâ„“â€²from a Gaussian distribution P(ğœ”)andğ‘â„“â€²from a uniform distribution [0,2ğœ‹]by seedğ‘–. The local
(ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²+ğ‘â„“â€²is calculate by step 3. Aggregating the local (ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²+ğ‘â„“â€²from all workers through ğ‘‡1,â„“-th worker obtain
ğ›½=Ãğ‘
â„“â€²=1 (ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²+ğ‘â„“â€². However,â„“-th worker holds multiple values of ğ‘withğ‘times, we remove(ğ‘âˆ’1)values ofğ‘. To recover
4254KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ke Zhang et al.
5 8 21 10
15 29
445 8 21 10
26 18
44Workers
Workers
Coordinator
Same worker Differ ent worker1 2 3 4
Workers
Workers
Coordinator1 2 3 4
Same worker Differ ent worker
Figure 7: Two totally different trees for security aggregation.
Algorithm 3 Securely aggregate local information to obtain ğœ™ğœ”ğ‘–(ğ‘¥ğ‘–)on theâ„“-th worker
Input:(ğ‘¥ğ‘–)Gâ„“,(ğœ”ğ‘–)Gâ„“and indexğ‘–.
Output:ğœ™ğœ”ğ‘–(ğ‘¥ğ‘–)
Doing this loop in parallel
1:whileâ„“â€²=1,...,ğ‘ do
2: Randomly generate a number ğ‘â„“â€²from a uniform distribution [0,2ğœ‹]with random seed ğ‘–.
3: Calculate(ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²+ğ‘â„“â€².
4:end while
5:Calculateğ›½=Ãğ‘
â„“â€²=1 (ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²+ğ‘â„“â€²through tree structure ğ‘‡1.
6:Pick upâ„“âˆ—âˆˆ{1,...,â„“âˆ’1,â„“+1,...,ğ‘}at random.
7:Calculate Â¯ğ‘â„“âˆ—=Ã
â„“â€²â‰ â„“âˆ—ğ‘â„“â€²through tree structure ğ‘‡2.
8:Calculateğ›½âˆ’Â¯ğ‘â„“âˆ—.
9:Calculateğœ™ğœ”ğ‘–(ğ‘¥ğ‘–)byğ›½âˆ’Â¯ğ‘â„“âˆ—.
the value of ğœ”ğ‘‡
ğ‘–ğ‘¥ğ‘–+ğ‘, we pick up a ğ‘â„“âˆ—from{1,...,ğ‘}âˆ’{â„“}as the value of ğ‘. Obtaining the value of ğ‘has the risk of leaking information
which leads to attackers successfully implementing inference attacks (inference attack is introduced in the next section). Therefore, we use
ğ‘‡2to calculate Â¯ğ‘â„“âˆ—=Ã
â„“â€²â‰ â„“âˆ—ğ‘â„“â€². Finally,â„“-th worker obtainsÃğ‘
â„“â€²=1 (ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²+ğ‘â„“âˆ—by removing Â¯ğ‘â„“âˆ—. Consequently, â„“-th worker get
ğœ™ğœ”ğ‘–(ğ‘¥ğ‘–)=âˆš
2 cosÃğ‘
â„“â€²=1 (ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²+ğ‘â„“âˆ—
.
Each worker locally saves the output of Algorithm 3 from {1,...,ğ‘}, whereğ‘is denoted as the size of data. In practice, our algorithm
asynchronously iterates ğ‘¡times. Therefore, if we use the same rand seed in preprocessing and training, our complexity is O(ğ‘¡)in preprocessing.
D SECURITY ANALYSIS
In this section, we discuss the security guarantee of AVFKAM under the semi-honest assumption. Because data security is not related to
{ğ‘¥+}and{ğ‘¥âˆ’}, we uniformly use {ğ‘¥}to represent data. Consequently, we denote ğ‘œğ‘–=(ğœ”ğ‘–)ğ‘‡
Gâ„“(ğ‘¥ğ‘–)Gâ„“.
Assumption D.1 (Semi-honest security). All workers will follow the protocol or algorithm to perform the correct computation. However,
they may retain records of the intermediate computation results which they may use later to infer the data of other workers.
Definition D.2 (Inference attack). An inference attack on the â„“-th worker is to infer a certain feature group Gof sampleğ‘¥ğ‘–which belongs
to other workers without directly accessing it.
According to the Semi-honest assumption and inference attack, we find the key to preventing the inference attack is to mask the value of
ğ‘œğ‘–. As described in Algorithm 3, we obtain ğœ™ğœ”(ğ‘¥)by aggregating local product (ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€²from all workers. To improve security, we add
an extra random variable ğ‘â„“â€²instead of transferring directly (ğœ”ğ‘–)ğ‘‡
Gâ„“â€²(ğ‘¥ğ‘–)Gâ„“â€². Therefore, the receiver is hard to infer the value of ğ‘œğ‘–. Theâ„“-th
worker will obtain the global sum in step 5 through ğ‘‡1. Because it holds multiple values of ğ‘, we remove(ğ‘âˆ’1)values ofğ‘. Therefore, we
only compute Â¯ğ‘â„“âˆ—, and useğ›½to minus it. Consequently, we need to prove that computing Â¯ğ‘â„“âˆ—can not disclose the value of ğ‘â„“â€². Two totally
different trees can satisfy this requirement. From Appendix B, we provide an example to illustrate that two trees can aggregate to produce
the same final result, but the intermediate values are not the same, which makes it difficult for attackers to infer the values of leaf nodes.
Therefore, AVFKAM can prevent the inference attack under the semi-honest assumption.
4255