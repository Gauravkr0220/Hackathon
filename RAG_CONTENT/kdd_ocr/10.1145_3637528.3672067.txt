Towards Robust Information Extraction via Binomial Distribution
Guided Counterpart Sequence
Yinhao Bai
College of Software, Nankai
University & Tianjin Key Laboratory
of Software Experience and Human
Computer Interaction
Tianjin, China
yinhao@mail.nankai.edu.cnYuhua Zhao
College of Software, Nankai
University
Tianjin, China
zyh22@mail.nankai.edu.cnZhixin Han
College of Software, Nankai
University
Tianjin, China
zhixinhan@mail.nankai.edu.cn
Hang Gao
College of Artificial Intelligence,
Tianjin University of Science and
Technology
Tianjin, China
mailmeki@163.comChao Xue
JD Explore Academy
Beijing, China
xuechao19@jd.comMengting Huâˆ—
College of Software, Nankai
University & Tianjin Key Laboratory
of Software Experience and Human
Computer Interaction
Tianjin, China
mthu@nankai.edu.cn
ABSTRACT
Information extraction (IE) aims to extract meaningful structured
tuples from unstructured text. Existing studies usually utilize a
pre-trained generative language model that rephrases the original
sentence into a target sequence, which can be easily decoded as
tuples. However, traditional evaluation metrics treat a slight error
within the tuple as an entire prediction failure, which is unable to
perceive the correctness extent of a tuple. For this reason, we first
propose a novel IE evaluation metric called ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ to eval-
uate the correctness of the predicted tuples in more detail. Moreover,
previous works have ignored the effects of semantic uncertainty
when focusing on the generation of the target sequence. We ar-
gue that leveraging the built-in semantic uncertainty of language
models is beneficial for improving its robustness. In this work, we
propose B inomial distribution guided c ounterpart s equence (BCS)
method, which is a model-agnostic approach. Specifically, we pro-
pose to quantify the built-in semantic uncertainty of the language
model by bridging all local uncertainties with the whole sequence.
Subsequently, with the semantic uncertainty and ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ ,
we formulate a unique binomial distribution for each local decoding
step. By sampling from this distribution, a counterpart sequence
is obtained, which can be regarded as a semantic complement
to the target sequence. Finally, we employ the Kullback-Leibler
divergence to align the semantics of the target sequence and its
counterpart. Extensive experiments on 14 public datasets over 5
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/https://doi.org/10.1145/3637528.3672067information extraction tasks demonstrate the effectiveness of our
approach on various methods. Our code and dataset are available
at https://github.com/byinhao/BCS.
CCS CONCEPTS
â€¢Computing methodologies â†’Information extraction.
KEYWORDS
Information extraction, Language model, Sentiment analysis
ACM Reference Format:
Yinhao Bai, Yuhua Zhao, Zhixin Han, Hang Gao, Chao Xue, and Mengting
Hu. 2024. Towards Robust Information Extraction via Binomial Distribution
Guided Counterpart Sequence. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/https://doi.org/10.1145/3637528.3672067
1 INTRODUCTION
Information extraction (IE) is a vital task in natural language pro-
cessing, aiming to extract meaningful and structured information
from unstructured text data [ 1,14], such as extracting aspect senti-
ment tuples [ 41], entity relation tuples [ 23]. For example, given the
sentence â€œVirginia is in the United States . â€, IE tasks need to extract
various structures such as entity-relation triplet (Virginia, country,
United States ). As a crucial technology, IE has brought benefits to
many web applications (e.g., information retrieval, web mining,
user review analysis, and hot topic analysis) and has attracted the
attention of many researchers.
Previously, IE has been addressed with sequence tagging [ 6,
38,43], span classification [ 20,33,35], and machine reading com-
prehension [ 12]. Recently, the emergence of the generation-based
paradigm as a new trend [ 13,23,41] involves language models
(LMs) generating target sequences following specific templates,
from which tuples are extracted. Zhang et al . [41] first reformu-
late several extraction tasks as a text generation problem using
 
83
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yinhao Bai et al.
Input Sentence Virginia is in the United States .
Gold Tuple ( Virginia, country, United States )
Predicted Tuple 1 ( Virginia, capital, United States )
Predicted Tuple 2 ( Virginia, in the, States )
Figure 1: Two predicted error tuples are shown. The Rela-
tional tuple is shown in the order of ( ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦ 1,ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ,ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦ 2),
and the highlighted parts are the predicted error elements.
annotation-style and extraction-style paradigms. Lu et al . [23] em-
ploy text-to-structure generation architecture to encode diverse IE
structures into a consistent representation. Hu et al . [16] design
dataset-level and instance-level data augmentation for generative
quad prediction task via template-order permutation. Gou et al .
[13] introduce element order prompts to guide the LMs in gener-
ating sentiment tuples with various element orders. These works
have made significant progress in handling IE tasks by utilizing the
generative paradigm.
While the generation-based paradigm holds promise, there are
still two potential limitations:
â€¢A tuple is evaluated in an exact matching manner, without
perceiving its degree of correctness.
â€¢They ignore effects of semantic uncertainty [ 18], which are
common in natural language caused by â€œsemantic equiva-
lenceâ€ such as semantic-similar tokens.
For the first limitation, previous works [ 9,41] stipulate that a
predicted tuple is correct when each of its components (i.e., el-
ements) is correct. This manner neglects the various degrees of
correctness in different tuples. For example, as illustrated in Figure
1, the Predicted Tuple 1 andPredicted Tuple 2 have a wrong element
and two wrong elements, respectively. They are both regarded as
wrong tuples in existing evaluation patterns. However, the former
obviously contains fewer errors than the latter. There is a need for
a comprehensive assessment method to more accurately differen-
tiate the correctness levels of tuples. To solve this limitation, we
first propose ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ , which is designed for the internal
matching of tuples, to enable a more comprehensive evaluation of
the accuracy of tuples generated by the LMs.
For the second limitation, according to Kuhn et al . [18] , LMs hold
a large amount of semantically similar representation. For example,
when predicting â€œcountryâ€, the LMs also tend to use â€œnationâ€, â€œstateâ€,
andâ€œhomelandâ€ to represent it. Even though these elements have
various forms, they exhibit semantic equivalence, indicating that
different elements can convey the same meaning. This may result
in LMs being uncertain about whether to generate â€œcountryâ€ or
â€œnationâ€, called semantic uncertainty. However, this uncertainty
can contribute to improving the performance of LMs. Intuitively,
LMs learning diverse representations of the same meaning enable
them to comprehend the tuples from a broader perspective, similar
to viewing scenery from different angles, which provides a more
comprehensive understanding.
In this paper, we propose a model-agnostic method, called Binomial
distribution guided counterpart sequence (BCS), to guide the target
sequence and its counterpart sequence simultaneously. Concretely,in the training phase, the target sequence is constructed with pre-
defined templates and gold tuples. Then it is learned in light of
the sequence-to-sequence learning objective. As for learning its
counterpart sequence, we first propose quantifying inherent seman-
tic uncertainties within the LMs by normalizing the uncertainty
at each local uncertainty in the entire target sequence. Then, we
combine the semantic uncertainty and ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ to create a
unique binomial distribution for each local decoding step. As such,
LMs can better understand the amount of semantics that needs to
be supplemented for a target sequence. Subsequently, by sampling
from this binomial distribution, we obtain the counterpart sequence,
which can be regarded as a semantic complement to the target se-
quence. Finally, the Kullback-Leibler (KL) divergence is employed
to align the semantics of the target sequence and its counterpart.
In summary, the contributions of this paper are as follows:
â€¢To address the limitation of conventional evaluation metrics
in detecting subtle errors within a tuple, we first introduce
a novel evaluation approach ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ . It is designed
to partially match tuples, facilitating a more comprehensive
assessment of the tuples generated by the model.
â€¢We study generative IE tasks from the view of learning se-
mantic uncertainty. To the best of our knowledge, this is
the first work to study semantic uncertainty in this task. We
propose a model-agnostic approach, called Binomial distri-
bution guided counterpart sequence (BCS), enhancing the
robustness of LMs towards complicated semantics with a
specially-designed counterpart sequence.
â€¢Experimental results on 14 public datasets over five infor-
mation extraction tasks demonstrate that our method has
universal effectiveness.
2 METHODOLOGY
2.1 Formulation and Overview
IE tasks can be formulated as text-to-structure problems, where dif-
ferent IE tasks correspond to different structures. Given a sentence,
for different tasks, it may contain one or more of the following ele-
ments: entity ( ğ‘’), entity type ( ğ‘’ğ‘¡), relation (ğ‘Ÿ), aspect term ( ğ‘ğ‘¡), aspect
category (ğ‘ğ‘), opinion term ( ğ‘œğ‘¡), and sentiment polarity ( ğ‘ ğ‘), and so
on. In IE tasks, the elements to be predicted vary from different sub-
tasks: Named Entity Recognition (NER) aims to extract entity
and their corresponding entity type as pairs {(ğ‘’,ğ‘’ğ‘¡)};Joint En-
tity and Relation Extraction (JERE) aims to extract entities and
relationships as triplets {(ğ‘’1,ğ‘Ÿ,ğ‘’2)};Aspect Sentiment Triplet
Extraction (ASTE) aims to discover more complicated aspect-
level sentiment triplets {(ğ‘ğ‘¡,ğ‘œğ‘¡,ğ‘ ğ‘)};Target Aspect Sentiment
Detection (TASD) is the task to detect all {(ğ‘ğ‘¡,ğ‘ğ‘,ğ‘ ğ‘)}triplets for
a given sentence; Aspect Sentiment Quad Prediction (ASQP)
is to predict all aspect-based sentiment elements {(ğ‘ğ‘¡,ğ‘œğ‘¡,ğ‘ğ‘,ğ‘ ğ‘)}
quadruplets. In TASD and ASQP tasks, if aspect term ğ‘ğ‘¡(or opinion
termğ‘œğ‘¡) is not mentioned explicitly in the sentence, ğ‘ğ‘¡(orğ‘œğ‘¡) should
be considered implicit information and represented by null.
As shown in Figure 2, the process starts with feeding the input
sentence into the encoder-decoder framework. The learning of tar-
get sequences is inspired by the sequence-to-sequence learning
objective. During training, our method first quantifies the seman-
tic uncertainty for each local decoding step. Then, by combining
 
84Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
â‘  Semantic Uncertainty QuantificationHidden State H
Binomial Distribution
00.050.10.150.20.250.30.350.40.45
0 1 2 3 4 5 6 7 8Match 
ScoreEncoder DecoderSampling a 
counterpart 
sequence
SoftplusUncertainty
argmaxVirginia is in the United States . <s> United States is country of Virginia
Probabilizing...... ...... ......u1
u2
u8......p1
p2
p8
ypred : "The USA is 
nation of Virginia"KLKLLgL
Decoder
<s> The USA is nation of VirginiacL
p: (Virginia, 
nation, The USA)Recover ) 1(MS nNï€­ï‚´ï€½] ,..., ,[8 2 1 p pp Pï€½Semantic Uncertainty 
00.10.20.30.40.50.60.70.80.91
1 2 3 4 5 6 7
â‘¡ Counterpart Sequence AcquisitionM
Target Sequence ySamplingyâ€™
Figure 2: An overview of proposed method BCS. We provide a detailed explanation of the process for obtaining the binomial
distribution-guided counterpart sequence, where ğ‘€=[ğ‘š1, ...,ğ‘šğ‘›]. Besides, in this process, ğ’šğ‘ğ‘Ÿğ‘’ğ‘‘ predicted sequence, ğ’‘is a
predicted tuple, red fonts indicate incorrectly predicted elements.
semantic uncertainty and ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ , we formulate a unique
binomial distribution for each local decoding step. Sampling tokens
from these binomial distributions form the counterpart sequence.
Finally, to ensure that the model learns semantic complementary
information, we utilize KL divergence to align the semantics of the
target sequence and its counterpart. Next, letâ€™s delve into the details
of the individual components involved.
2.2 Sequence-to-Sequence Learning
The overall framework is in light of the sequence-to-sequence
learning. It is worth noting that during training, tuples are for-
mulated into a target sequence ğ’šfollowing pre-defined templates,
as shown in Figure 6. For example, in the JERE task for the entity-
relationship triplet (Virginia, country, United States ), its target se-
quence is â€œUnited States is country of Virginiaâ€. Given input sentence
ğ’™and the target sequence ğ’š, we first compute the hidden represen-
tation ğ‘¯of each token:
ğ‘¯=Encoder(ğ’™) (1)
where Encoder(Â·)is a transformer encoder. Then the target se-
quence ğ’šis fed into the decoder as teacher forcing [ 37] during
training. At step ğ‘¡of decoding, the model generates the ğ‘¡-th repre-
sentation of the vocabulary distribution ğ’ğ‘¡as follows:
ğ’ğ‘¡=Decoder(ğ‘¯,ğ’š<ğ‘¡) (2)
where ğ’ğ’•is calculated based on the input sequence ğ’™and previous
outputs ğ’š<ğ‘¡. At theğ‘¡-th time step, the decoder output ğ’šğ‘¡is calcu-
lated with the input ğ’™and the previous outputs ğ’š<ğ‘¡, formulating
as below:
ğ‘ğœƒ(ğ’šğ‘¡|ğ’™,ğ’š<ğ‘¡)=Softmax(ğ’ğ‘¡) (3)
During training, we follow the previous work [ 10] to exploit the
likelihood training to optimize the target sequence probabilities byminimizing the cross-entropy loss:
Lğ‘”=âˆ’ğ‘›âˆ‘ï¸
ğ‘¡=1logğ‘ğœƒ(ğ’šğ‘¡|ğ’™,ğ’š<ğ‘¡) (4)
whereğ‘›is the length of target sequence ğ’š.
2.3 Semantic Uncertainty Quantification
As depicted in Figure 1, LMs produce alike representations for
words that frequently appear in similar contexts, such as â€œcountryâ€
andâ€œnationâ€. The in-depth reason relies on that LMs are pre-trained
based on distributional semantics theory [ 2]. Then, when predict-
ing the target sequence, the LMs may consider tokens with similar
semantics to be the same. Such semantic equivalences lead to un-
certainty. Nonetheless, it is considered beneficial because it can
improve the modelâ€™s understanding of tuples from more diverse
perspectives. Therefore, we argue that the model should quantify
and learn semantic uncertainty. To accurately quantify such un-
certainty, we re-design the decoder side, considering both each
local decoding step uncertainty (i.e. local uncertainty) and whole
sequence semantics. Concretely, as depicted in Figure 2, to allow
the model to learn posterior distribution information during train-
ing, we first attain local uncertainty. We follow a commonly used
method of distribution-based uncertainty modeling [ 17], specifi-
cally Dirichlet distributionâ€™s uncertainty, to quantify uncertainty,
which is formulated as follows:
ğ‘¢ğ‘¡=|ğ‘‰|
Ã|ğ‘‰|
ğ‘–=1ğœ(ğ’ğ‘¡[ğ‘–])+1(5)
whereğœ(Â·)is the Softplus activation function and |ğ‘‰|is the size
of the vocabulary. The â€œ+1â€ serves as a smoothing factor, where
ğ‘œğ‘¡, as referred to in formula 2, can be viewed as the evidence mag-
nitude at each location. These magnitudes are transformed into
 
85KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yinhao Bai et al.
non-negative values using Softplus, inspired by evidential neural
networks [ 42]. In this way, we can easily obtain the uncertainty of
the model without having to perform multiple forwards, which can
save computational time and space to some extent.
Then, with these all local uncertainties, we further calculate
semantic uncertainties by considering linguistic consistency within
the overall target sequence. Specifically, we use ğ‘¢ğ‘šğ‘ğ‘¥ andğ‘¢ğ‘šğ‘–ğ‘›to
respectively represent the maximum and minimum values in all
local uncertainties of the whole sequence, i.e. [ğ‘¢1,...,ğ‘¢ğ‘›]. Then they
are further exploited to normalize local uncertainty, which finally
yields the semantic uncertainty ğ‘ğ‘¡for each local decoding step:
ğ‘ğ‘¡=ğ‘¢ğ‘¡âˆ’ğ‘¢ğ‘šğ‘–ğ‘›
ğ‘¢ğ‘šğ‘ğ‘¥âˆ’ğ‘¢ğ‘šğ‘–ğ‘›(6)
whereğ‘ğ‘¡âˆˆ[0, 1]. A lower value indicates that the language model
is more certain about the predicted semantics.
2.4 Counterpart Sequence Acquisition
In this subsection, we first introduce the proposed ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ ,
followed by how to acquire a counterpart sequence based on the
combination of semantic uncertainty and ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ .
Matching Score As shown in Figure 1, previous works [ 9,41] es-
tablish that a predicted tuple is considered accurate only when all its
elements are correct but ignore the varying degrees of correctness
within different tuples. Therefore, in order to solve this limitation,
we propose a novel evaluation method ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ . Firstly, the
predicted sequence ğ’šğ‘ğ‘Ÿğ‘’ğ‘‘ is obtained by taking the maximum value
in distribution ğ’ğ’•in a greedy manner:
ğ’šğ‘ğ‘Ÿğ‘’ğ‘‘=argmax(ğ’1,...,ğ’ğ‘›) (7)
where argmax(Â·)refers to the token with the highest predicted
value at each decoding step. These tokens are concatenated to form
the predicted sequence ğ’šğ‘ğ‘Ÿğ‘’ğ‘‘.
Subsequently, due to ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘” ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ focusing on tuple-level
evaluation, ğ’šğ‘ğ‘Ÿğ‘’ğ‘‘ needs to be rebuilt into predicted tuples ğ’‘. For
instance, as shown in Figure 2, the predicted sequence â€œUnited States
is country of Virginiaâ€ are converted to predicted tuples [(Virginia,
nation, The USA)] by corresponding rules. Then, the quality of
predicted tuples ğ’‘can be assessed by calculating the ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”
ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ (MSfor short):
MS=|set(ğ’‘)âˆ©set(ğ’ˆ)|
|set(ğ’‘)âˆªset(ğ’ˆ)|Ã—1
|ğ’‘||ğ’‘|âˆ‘ï¸
ğ‘–=1Match(ğ’‘[ğ‘–],ğ’ˆ) (8)
where|ğ’‘|is the number of predicted tuples and set(Â·)indicates the
set consisting of the elements inside the tuples. The intersection of
two sets indicates how many positive elements are captured. The
union of two sets includes all elements correctly and incorrectly.
Using the example in Figure 2, ğ’”ğ’†ğ’•(ğ’‘)={Virginia, nation, The USA}
andğ’”ğ’†ğ’•(ğ’ˆ)={Virginia, country, United States }. The intersection and
union of these sets yield {Virginia } and {Virginia, nation, country,
The USA, United States }. Therefore, by comparing intersection and
union, MScan effectively discern elements that do not belong to the
reference tuple.
Furthermore, the MSalso considers the extent to which each
predicted tuple ğ’‘[ğ‘–]matches the gold tuples ğ’ˆ. Since each predicted
tuple ğ’‘[ğ‘–]may match multiple different gold tuples ğ’ˆ[ğ‘˜], whereğ‘˜âˆˆ[1,|ğ’ˆ|], we use the score that matches in most relevant as the
final score:
Match(ğ’‘[ğ‘–],ğ’ˆ)=max
1â‰¤ğ‘˜â‰¤|ğ’ˆ|1
ğ¸ğ¸âˆ‘ï¸
ğ‘—=11ğ’‘[ğ‘–][ğ‘—]=ğ’ˆ[ğ‘˜][ğ‘—] (9)
whereğ¸represents the number of internal elements in a tuple. Such
as, in the above case, due to the one tuple being a triplet, ğ¸is 3.
The match function Match(Â·)calculates the degree of agreement
between predicted and gold tuples. This match function Match(Â·)
calculates the degree of agreement between predicted and gold
tuples. In this way, the MSoffers a more detailed and accurate
evaluation of predicted tuples, and its value range is [0, 1]. To
enhance the understanding of the MSand highlight its benefits,
several case studies and experimental analyses are presented in
Appendix Â§A.
Binomial Distribution Guided Counterpart Sequence By
combining the semantic uncertainty and ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ , we create
a unique binomial distribution for each local decoding step:
ğµğ‘¡(ğ‘,ğ‘ƒ)=ğµğ‘–ğ‘›ğ‘œğ‘šğ‘–ğ‘ğ‘™(ğ‘=ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘((1âˆ’MS)âˆ—ğ‘›),ğ‘ƒ=ğ‘ğ‘¡) (10)
whereğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ convert floating point numbers into integers and
ğµğ‘¡(ğ‘,ğ‘ƒ)indicate the binomial distribution of the ğ‘¡-th token.ğ‘ƒ
is the probability of performing a successful Bernoulli experiment
andğ‘is the number of Bernoulli experiments. In our work, seman-
tic uncertainty and MScorrespond to ğ‘ƒandğ‘, respectively. The
former denotes semantic uncertainty, while the latter controls the
quantity for additional semantic complement. Therefore, to pro-
duce a good counterpart sequence, we think about using a binomial
distribution to bridge these two parts.
Subsequently, we obtain a value ğ‘‹ğ‘¡by sampling from the bino-
mial distribution ğµğ‘¡(ğ‘,ğ‘ƒ), whereğ‘‹ğ‘¡captures the count of success-
ful outcomes within the ğ‘trials. Then guided by ğ‘‹ğ‘¡, the counterpart
sequence ğ’šğ’„is obtained by choosing either from ğ’šorğ’šğ‘ğ‘Ÿğ‘’ğ‘‘ at each
time step, formulated as follows:
ğ’šğ’„[ğ‘¡]=ğ’šğ‘ğ‘Ÿğ‘’ğ‘‘[ğ‘¡], ğ‘‹ğ‘¡>0
ğ’š[ğ‘¡], ğ‘‹ğ‘¡=0
where ğ’šğ’„[ğ‘¡]indicatesğ‘¡-th token of the counterpart sequence. For
the above process, the counterpart sequence ğ’šğ’„is obtained by re-
placing some of the tokens in the target sequence using the seman-
tics obtained from semantic-uncertainty-guided sampling. There-
fore, the counterpart sequence can be regarded as the semantic
complement to the target sequence.
2.5 Semantic Alignment
Recently, some studies [ 4,32] have found that maximizing agree-
ment between multiple learning objectives enables the model to
learn them stably. Inspired by this, to learn better the target and
counterpart sequence, we design a consistency loss Lğ¾ğ¿to align
their semantics as well as ensure stable training:
Lğ¾ğ¿=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1(Dğ¾ğ¿(ğ‘ğœƒ(ğ’šğ‘¡|ğ’™,ğ’š<ğ‘¡)||ğ‘ğœƒ(ğ’šğ‘¡|ğ’™,ğ’šğ’„
<ğ‘¡)) (11)
Dğ¾ğ¿(ğ’‘||ğ’’)=âˆ‘ï¸
|ğ‘‰|ğ’‘log(ğ’‘
ğ’’)
 
86Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
whereğ‘›is the length of target sequence ğ’š. Besides, to better learn
the counterpart sequence, we train it using the same approach as
the target sequence:
Lğ‘=âˆ’ğ‘›âˆ‘ï¸
ğ‘¡=1logğ‘ğœƒ(ğ’šğ‘¡|ğ’™,ğ’šğ’„
<ğ‘¡) (12)
2.6 Joint Training Objective
The final training objective is jointly to combine the above three
losses:
L=(1âˆ’ğ›¼)Lğ‘”+ğ›¼Lğ‘+ğ›½Lğ¾ğ¿ (13)
whereğ›¼andğ›½are the hyperparameter. These two hyperparameters
are to balance the learning of the target sequence and counterpart
sequence. Therefore they are necessary for the calculation of our
final loss.
3 EXPERIMENTS
3.1 Experiment Settings
3.1.1 Datasets. To prove the effectiveness of our proposed method,
we conduct experiments on 14 datasets across 5 IE tasks, involving
ASQP, ASTE, TASD, NER, and JERE. The used datasets includes
Laptop [3],Restaurant [3],Laptop14 [9,24],Rest14 [9,24,27],
Rest15 [9,24,26,40],Rest16 [9,24,25,40],WNUT16 [34],WNUT17
[7],Webnlg [11],CoNLL04 [30]. The Rest15 and Rest16 among
them exist on multiple tasks simultaneously. Then, for a fair com-
parison, we use the same data split as previous works. The statistics
of the datasets are shown in Table 5.
3.1.2 Baseline Methods. We evaluate the proposed approach by
applying it to various SOTA baseline methods, and we keep all
the training parameters the same as in the baseline methods, the
hyperparameter details are presented in Appendix Â§B.
â€¢GAS [41] GAS formulates several IE tasks into a unified gen-
erative framework, which mainly incorporates two paradigms,
i.e. annotation-style and extraction-style.
â€¢DLO [16] DLO explores ASQP task from the standpoint
of template orders, hypothesizing that different orders offer
diverse views of the quadruplet. It pinpoints the most suitable
orders and then merges multiple fitting templates for data
augmentation.
â€¢UIE [23] UIE proposes a unified text-to-structure genera-
tion framework, which has the capacity to universally model
varying IE tasks and generate targeted structures.
â€¢MvP [13] MvP designs a method based on the order of
elements. MvP amalgamates tuples generated in different
orders, taking advantage of the innate understanding of
problem-solving processes from different views.
3.1.3 Model Variants. To evaluate the individual components of
our method, several model variants are further designed.
â€¢w/o U The semantic uncertainty is removed, which is re-
placed by the confidence, i.e. largest value in ğ‘ğœƒ.
â€¢w/o MSWe remove the ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ in Binomial distri-
bution and set the parameter as ğ‘=ğ‘›.
â€¢w/o ST The parameter to control sampling times in the
Bernoulli distribution is set to ğ‘=1.
â€¢w/o KL The semantic alignment loss Lğ¾ğ¿is removed.â€¢w/oLğ‘The learning of binomial distribution guided coun-
terpart sequence is removed.
â€¢w/o B We remove the binomial guide and use only ğ’šğ‘ğ‘Ÿğ‘’ğ‘‘
as the counterpart sequence.
â€¢w/o Jaccard We remove the Jaccard coefficient in Equation
8 (on the left side of the equation).
â€¢w/o all We remove all the components of our method,
which degenerates to GAS.
3.2 Evaluation on Various Tasks
As shown in Table 1, the results on 14 public datasets across 5 tasks
are reported. Firstly, it can be observed that +BCS improves the
performance of the baseline on nearly all datasets and tasks. Espe-
cially, compared with the strong baseline GAS, GAS+BCS achieves
average F1improvements by +2.45%, +1.47%, +0.34%, +0.34%, and
+1.45% in ASQP, ASTE, TASD, NER, and JERE tasks, respectively.
It is worth noting that our method exhibits the most significant
improvement on the more challenging ASQP task while showing
comparative on the simpler NER task. This phenomenon suggests
that our approach excels in addressing complex IE tasks, enabling
the model to harness the inherent semantic uncertainty for han-
dling intricate structures and relationships, thus enhancing the
modelâ€™s robustness. Conversely, when dealing with simpler tasks
that are inherently straightforward, the model demonstrates more
stable performance, leaving limited room for improvement.
Similarly, for the proposed metric MS, +BCS can still achieve
significantly better performance on these tasks. In addition, MS
provides a more comprehensive reflection of model performance.
For example, when comparing MSbetween the Rest15 andLaptop
datasets in the ASQP task, we observe an obvious difference in
F1but minimal variation in MS. This discrepancy highlights that
GAS makes reasonably accurate predictions on the Laptop dataset,
incurring minor errors in predicting a range of tuples. It can be
inferred that there is a pressing need to improve these minor errors
within tuples. The significant improvements of F1demonstrate that
BCS bolsters this aspect of our modelâ€™s capabilities. Meanwhile, MS
can provide a more comprehensive evaluation for tuples extracted
by various IE tasks.
3.3 Evaluation on Various Models
To further validate the effectiveness of BCS on generation-based IE
frameworks, we apply BCS to various strong methods. The evalua-
tion results on the Rest16 dataset are presented in Table 3.
It can be observed that when adding BCS to four frameworks, the
performances are consistently improved on almost all evaluation
metrics. Especially, compared with strong baselines GAS, GAS+BCS
achieves the absolute F1andMSimprovement by +2.26% (+3.14%
relatively) and +1.81% (+2.41% relatively), respectively. Meanwhile,
UIE+BCS outperforms UIE by +1.32% (+1.83% relatively) and +1.53%
(+2.03% relatively) on the F1andMS.
It is worth noting that our approach does not improve signifi-
cantly on methods such as DLO or MvP. A possible reason is that
both methods use multiples of data to train the model. This may
allow the model to have potentially learned part of the semantics.
However, this learning method is insufficient, and our approach
 
87KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yinhao Bai et al.
Table 1: Evaluation results on various tasks in terms of precision ( Pre, %), recall ( Rec, %), F1 Score ( F1, %) and ğ‘´ğ’‚ğ’•ğ’„ğ’‰ğ’Šğ’ğ’ˆ ğ‘ºğ’„ğ’ğ’“ğ’† (MS,
%). +BCS indicates that applying our approach to GAS.
Tasks DatasetsGAS +BCS
Pre Rec F1 MS Pre Rec F1 MS
ASQPLaptop 43.45 43.29 43.37 62.15 44.38 43.79 44.08 62.59
Restaurant 57.09 57.51 57.30 70.26 61.23 59.53 60.37 72.04
Rest15 45.31 46.70 45.98 61.02 48.56 49.52 49.03 62.12
Rest16 54.54 57.62 56.04 69.43 59.20 61.04 60.11 70.46
ASTELaptop14 63.52 58.87 61.10 68.08 64.31 60.04 62.10 69.59
Rest14 70.14 71.83 70.97 74.64 73.80 73.57 73.68 77.64
Rest15 61.74 64.19 62.93 70.23 64.46 67.42 65.91 72.49
Rest16 69.92 74.32 72.05 75.12 72.64 76.07 74.31 76.93
TASDRest15 64.29 60.23 62.19 69.38 66.61 61.66 64.03 70.53
Rest16 70.01 68.49 69.24 74.57 72.31 70.86 71.58 76.13
NERWNUT16 57.15 54.42 55.75 70.01 57.36 55.66 56.49 70.33
WNUT17 67.60 42.10 51.88 67.45 66.46 43.44 52.54 67.55
JEREWebnlg 87.53 87.37 87.45 87.43 88.28 87.97 88.13 87.90
CoNLL04 72.40 66.74 69.46 81.56 74.46 68.72 71.46 83.31
Table 2: Evaluation results of ablation study. The token â€œw/oâ€ denotes removing components.
ModelASQP ASTE TASD
Pre Rec F1 MS Pre Rec F1 MS Pre Rec F1 MS
Our 48.56 49.52 49.03 62.12 64.46 67.42 65.91 72.49 66.61 61.66 64.03 70.53
w/o U 46.44 48.18 47.29 60.87 63.58 66.60 65.06 71.92 65.25 61.10 63.11 69.63
w/oMS 47.09 47.88 47.48 61.53 64.19 65.63 64.90 72.20 65.21 60.67 62.85 70.05
w/o ST 47.52 48.97 48.23 61.38 63.12 66.94 64.97 71.23 64.09 59.76 61.85 69.18
w/o KL 47.44 48.55 47.99 61.84 63.01 65.57 64.26 71.37 64.92 61.46 63.14 69.80
w/oLğ‘47.10 47.97 47.53 60.91 63.54 66.46 64.97 71.72 65.41 62.13 63.73 70.13
w/o B 46.62 47.80 47.20 61.12 63.42 65.77 64.57 71.85 64.58 61.22 62.85 70.35
w/o Jaccard 46.93 48.74 48.12 61.73 62.53 64.37 63.41 71.69 65.39 61.29 63.37 69.97
w/o all 45.31 46.70 45.98 61.02 61.74 64.19 62.93 70.23 64.29 60.23 62.19 69.38
Table 3: Evaluation results of applying BCS to various models
in the ASTE task.
MethodsRest16
Pre Rec F1 MS
GAS 69.92 74.32 72.05 75.12
+BCS 72.64 76.07 74.31 76.93
DLO 69.82 76.53 73.03 76.98
+BCS 70.59 76.14 73.76 77.12
UIE 70.54 74.85 72.86 75.55
+BCS 72.06 75.13 73.59 77.08
MvP 72.08 74.19 73.48 77.32
+BCS 72.21 76.13 74.59 78.25
still enables its effectiveness to be improved. Therefore, these re-
sults verify that BCS can be easily adopted in any generation-based
framework and has universal effectiveness.3.4 Ablation Study
To validate the effectiveness of individual components, we perform
a systematic ablation study. The experimental results are presented
in Table 2. It can be observed that by removing various components,
the performances on three tasks are consistently decreasing. When
removing all the components, the performances decrease most
significantly. This validates the effectiveness of the constituent part
of our method.
Concretely, we see that w/o U causes significant performance
declines on all tasks. This presents that the proposed quantification
method for semantic uncertainty is effective and enables LMs to
better handle ambiguous or polysemous text. This enhances the
generalization of complicated semantics and successfully makes tu-
ple extraction more accurate. Moreover, w/o MSand w/o Jaccard also
lead to consistent performance drops. This further demonstrates
that the proposed ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ helps the model in perceiving the
degree of correctness within the tuple, allowing for more detailed
improvements of the predicted tuples. Then, after ablating ST, the
extraction ability of BCS is significantly decreased. This indicates
 
88Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
that the language model is able to learn more valuable semantic
uncertainties by controlling sampling times.
In addition, we also observe that w/o KL brings consistent degra-
dation. This shows that the KL divergence can maintain the seman-
tic consistency of the target sequence and its counterpart sequence.
It can also be seen that w/o Lğ‘makes the results deteriorate on
all three tasks. This presents that the learning process for bino-
mial distribution guided counterpart sequence is also important,
which can play a role in data augmentation and successfully provide
complementary semantics for the target sequence.
Notably, w/o B brings a significant reduction in performance.
This certainly proves that the binomial distribution guided method
to build the counterpart sequence can make the model learn better
semantic uncertainty. Finally, w/o all brings a precipitous decline
in performance. This further implies that leveraging the built-in
semantic uncertainty of language models is beneficial for the pre-
diction of the language models.
3.5 Hyperparameter Study
In section Â§2.6, we introduce two hyperparameters, ğ›¼, controlling
the loss between target sequence and counterpart sequence, and
ğ›½, balancing the effects of semantic alignment. In order to further
explore the impact of the two hyperparameters, we choose GAS
as the baseline and conduct experiments on two datasets (Rest15,
Rest16 ) and two metrics ( F1,MS) in the ASTE task. The curves are
depicted in Figure 3.
Hyperparameter Study for ğœ¶As shown in the chart on the up-
per of Figure 3, with ğ›½fixed at 0.05, we vary ğ›¼from 0.1 to 0.9. Firstly,
itâ€™s evident that our method significantly improves performance
across all datasets, regardless of parameter variations. This high-
lights the robustness of our approach in enhancing performance
across various scenarios. Secondly, when ğ›¼is set to 0.5, both F1
andMSreach their peaks. This implies that an extreme preference
for the target sequence can lead to overfitting while favoring the
counterpart sequence too much can degrade performance. When
both types of losses contribute equally, it facilitates a better under-
standing of the relationship between the target sequence and the
counterpart sequence during training. Alternatively, as ğ›¼varies, F1
andMSexhibit consistent increases and decreases, indicating that
whenğ›½is fixed at 0.05, KL divergence Lğ¾ğ¿effectively maintains
semantic consistency.
Hyperparameter Study for ğœ·As shown below in Figure 3, with
ğ›¼fixed at 0.5, we vary ğ›½from 0.01 to 0.09. From the trends of the
curves in the two subplots, it can be observed that our method still
achieves significantly better performance than GAS. Furthermore,
we can observe that, with ğ›¼fixed at 0.5, as ğ›½varies, there is signifi-
cant fluctuation in both F1andMS. However, as ğ›½passes through
0.05, the magnitude of the fluctuations reduces, which indicates that
Lğ¾ğ¿does maintain the semantic consistency of the target sequence
and its counterpart during training.
3.6 Comparison of MSand Human Evaluation
To show our metricâ€™s validity, we evaluated the alignment of metrics
with human judgment by comparing the Mean Square Error (MSE)
values between our metric MSand theğ¹1ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ when correlated
with human scores. To this end, we use all the data from the test
0.2 0.4 0.6 0.8
Î±(Î²= 0.05)606366697275F1 (%)
0.2 0.4 0.6 0.8
Î±(Î²= 0.05)656871747780MS (%)
0.02 0.04 0.06 0.08
Î²(Î±= 0.5)606366697275F1 (%)
0.02 0.04 0.06 0.08
Î²(Î±= 0.5)656871747780MS (%)
(1) FixÎ²
(2) FixÎ±Rest15 (GAS) Rest15 (GAS+BCS) Rest16 (GAS) Rest16 (GAS+BCS)Figure 3: Hyperparameter study for ğ›¼andğ›½. In the above
plots, the charts focus on ğ›¼as the variable of interest, with ğ›½
fixed at 0.05. The below charts focus on ğ›½as the variable of
interest, with ğ›¼fixed at 0.5.
sets in Rest15 andRest16 . The results, presented below in Table
4, indicate that the MSmetric exhibits a closer correspondence to
human scoring than the F1 score, as evidenced by a significantly
lower MSE value for MS. This finding underscores the effectiveness
ofMSin capturing the nuances of tuple correctness as perceived by
humans.
Table 4: Comparison of MSE between F1 (%) and MS(%) metrics
with human scores.
Datasets F1 MS HumanMSE (& Human)
F1 MS
Rest15 49.03 62.12 65.46 184.69 15.32
Rest16 60.11 70.46 72.45 248.46 10.37
3.7 Number of Counterpart Sequences Study
In section Â§2.4, we acquire the counterpart sequence through sam-
pling from a binomial distribution. Furthermore, by conducting
multiple samplings, we can derive additional counterpart sequences.
To further investigate the influence of the number of counterpart
sequences on the model, we select GAS [ 41] as the baseline and
perform experiments on the Rest15 dataset in the ASTE task. For
multiple counterpart sequences, we transform Lğ¾ğ¿andLğ‘into
an average as the calculation component of the final loss. Figure 4
shows the results of the performance in terms of F1andMS.
Here, itâ€™s worth noting that with just one direct counterpart
sequence, both F1andMSsignificantly outperform the baseline.
Observing the left half of Figure 4, we can see that as the number of
 
89KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yinhao Bai et al.
1 2 6 8 9 10
Num of counterpart sequence6364656667F1 (%)
1 2 6 8 9 10
Num of counterpart sequence7071727374MS (%)
GAS GAS+BCS
Figure 4: The relationships between the number of counter-
part sequences and two evaluation metrics, i.e. F1and MS.
counterpart sequences increases, F1initially rises and then declines,
peaking at 6 iterations. In the right half of Figure 4, MSshows slight
fluctuations with an overall trend of initially increasing and then
decreasing, peaking at 8 iterations. From the above observations,
it is evident that incorporating a moderate number of counterpart
sequences can further enhance the modelâ€™s performance.
However, multiple samplings mean multiple forward propaga-
tions in the neural network model, which significantly increases
computational demands. At the same time, the improvement in final
results is not particularly substantial. By increasing the number of
counterpart sequences beyond a certain threshold, the performance
is decreased. This may be due to potential information distortion
resulting from excessive samplings for the counterpart sequence.
This distortion could introduce noise during the modelâ€™s training
process, likely contributing to the observed decline in final results.
Therefore, to strike a careful balance between computational effi-
ciency and the final results, we have decided not to increase the
number of counterpart sequences in the main experiments and only
use one counterpart sequence.
3.8 Case Study
To show the advantages of BCS, we select two cases from the Rest15
dataset in the ASTE task for case study and present results by GAS
[41] and BCS in Table 5. In both cases, the tuple structure is ( ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡
ğ‘¡ğ‘’ğ‘Ÿğ‘š ,ğ‘œğ‘ğ‘–ğ‘›ğ‘–ğ‘œğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘š ,ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ğ‘›ğ‘¡ ğ‘ğ‘œğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ). GAS consistently errors
in predicted tuple structure and elements, while BCS accurately
captures the correct target sequence structure and gold tuples. Next,
we provide a detailed analysis of these cases.
In the first case, GAS error predicts the ğ‘œğ‘ğ‘–ğ‘›ğ‘–ğ‘œğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘š asâ€œhalf/neutralâ€
and misses the ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ğ‘›ğ‘¡ ğ‘ğ‘œğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ , resulting in a wrong target se-
quence structure. This highlights that structural errors in the pre-
dicted sequence can lead to tuple prediction failure. In the second
case, GAS successfully predicts all elements of gold tuples but fails
to predict structural information, resulting in a failed tuple. These
examples highlight the crucial need to address structural errors in
the predicted sequence to improve the modelâ€™s robustness.
Fortunately, our proposed method BCS, which combines seman-
tic uncertainty with MSto obtain the counterpart sequence, accu-
rately predicts these cases and greatly improves model robustness
compared to GAS.
Input SentenceThe service was spectacular as the waiter knew everything about 
the menu and his recommendations were amazing !
Gold Tuple [( service, spectacular, positive ), ( waiter, amazing, positive )]
Predicted Sequence (GAS)( service, spectacular, positive ); ( waiter, knew everything about 
the menu, amazing, positive )
Predicted Tuple (GAS) [( service, spectacular, positive ), ( null, null, null)]
Predicted Sequence (BCS)( service, spectacular, positive ); ( waiter, amazing, positive )
Predicted Tuple (BCS) [( service, spectacular, positive ), ( waiter, amazing, positive )]Input Sentence We had half/half pizza, ...... and it was sooo huge!
Gold Tuple [( half/half pizza, huge, positive )]
Predicted Sequence (GAS)( half/half pizza, half/neutral ); ( eggplant, huge, positive )
Predicted Tuple (GAS) [( null, null, null), ( eggplant, huge, positive )]
Predicted Sequence (BCS)( half/half pizza, huge, positive )
Predicted Tuple (BCS) [( half/half pizza, huge, positive )]
Figure 5: Two cases are shown. The relational tuple is shown
in the order of ( ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğ‘š ,ğ‘œğ‘ğ‘–ğ‘›ğ‘–ğ‘œğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘š ,ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ğ‘›ğ‘¡ ğ‘ğ‘œğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ).
4 RELATED WORK
IE has significantly impacted internet society and is a fundamental
research direction in natural language processing and data min-
ing. It extracts specific information from unstructured or semi-
structured texts, involving multiple sub-tasks [ 14]. Over the past
decade, methods decomposed extraction into sub-tasks like named
entity recognition [ 5,19], event extraction [ 36], and more. Ap-
proaches for modeling extraction include sequence-tagging [ 19,44],
span classification [ 20,33,35], and others. However, task-specific
models limit knowledge transfer between tasks and frameworks,
affecting latent semantic sharing and causing inductive bias in
transfer learning [21].
Large-scale LMs, with their proficiency in knowledge sharing
and semantic generalization, bring the opportunity to handle mul-
tiple IE tasks using the generation-based paradigm [ 28,31]. By
developing sophisticated schema-based prompts and structural gen-
eration specifications, IE tasks can be transformed into text-to-text
and text-to-structure formats using large-scale generative language
models [ 8], such as T5 [ 28]. Recent studies highlight the effec-
tiveness of generation-based templates in the format of a natural
language response to natural language input. Peng et al . [24] fo-
cus on extracting aspect sentiment triplets (WHAT, HOW, WHY)
from texts, while Cai et al . [3] work on extracting aspect-category-
opinion-sentiment quadruples for aspect-based sentiment analysis
with implicit aspects and opinions. UIE [ 23] introduces a structure
generation network that encodes heterogeneous IE structures uni-
formly and uses a structural schema instructor mechanism. USM
[22] employs three unified token linking operations to decouple
various IE tasks, allowing for the sharing of extraction capabilities
across different target structures and semantic schemas, embodying
the concept of â€œone model for solving all tasksâ€.
The generation-based IE frameworks, while excelling in acquir-
ing general knowledge from multi-source prompts, encounter in-
herent limitations in cross-task modeling. Firstly, their generative
nature, concentrating on target sequence generation during train-
ing, often overlooks the impact of minor tuple errors. Meanwhile,
traditional metrics (e.g., F1, ROUGE, BLEU, Bert-Score, etc.) com-
monly used in previous works [ 9,41] often define a predicted tuple
as correct only when each element in the tuple is correct. This leads
to these metrics failing to accurately measure tuple correctness.
 
90Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Secondly, these generation-based IE frameworks primarily focus
on sequence generation, often overlooking semantic uncertainty, a
common issue in language models. The semantic uncertainty has
been leveraged for image semantic segmentation [ 15], question
answer [ 18] and text classification [ 39], but has not been fully ex-
plored for the IE domain. We argue that LMs learning the semantic
uncertainty enables them to comprehend tuples from a broader
perspective, providing a more comprehensive understanding.
5 CONCLUSION
In this paper, we introduce a model-agnostic training framework
called the Binomial distribution guided counterpart sequence (BCS)
method, which can enhance the modelâ€™s robustness by leveraging
the built-in semantic uncertainty of language models. Specifically,
we propose to quantify the built-in semantic uncertainty of the
language model by bridging all local uncertainties with the whole
sequence. Then, to perceive the correctness within the tuple, we
first propose ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ to evaluate the predicted tuples in
more detail. Subsequently, with semantic uncertainty and ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”
ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ , we formulate a unique binomial distribution for each step. By
sampling from this distribution, we obtain a counterpart sequence,
which can be regarded as a semantic complement to the target se-
quence. Finally, we employ the Kullback-Leibler (KL) divergence to
align the semantics of the target sequence and its counterpart. Our
method is model-agnostic and easily applicable to multiple methods.
Experimental results demonstrate its competitive performance on
14 public datasets across 5 IE tasks, confirming its versatility and
universal effectiveness.
ACKNOWLEDGEMENTS
We sincerely thank all the anonymous reviewers for providing valu-
able feedback. This work is supported by the youth program of Na-
tional Science Fund of Tianjin, China (Grant No. 22JCQNJC01340).
REFERENCES
[1]Peggy M. Andersen, Philip J. Hayes, Alison K. Huettner, Linda M. Schmandt,
Irene B. Nirenburg, and Steven P. Weinstein. 1992. Automatic Extraction of
Facts from Press Releases to Generate News Stories. In Proceedings of the Third
Conference on Applied Natural Language Processing (Trento, Italy) (ANLC â€™92).
Association for Computational Linguistics, USA, 170â€“177. https://doi.org/10.
3115/974499.974531
[2]Gemma Boleda. 2020. Distributional semantics and linguistic theory. An-
nual Review of Linguistics 6 (2020), 213â€“234. https://ui.adsabs.harvard.edu/
abs/2019arXiv190501896B/abstract
[3]Hongjie Cai, Rui Xia, and Jianfei Yu. 2021. Aspect-Category-Opinion-Sentiment
Quadruple Extraction with Implicit Aspects and Opinions. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing (ACL-IJCNLP) .
340â€“350. https://aclanthology.org/2021.acl-long.29.pdf
[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning. PMLR, 1597â€“1607.
[5]Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. 2021. Template-based
named entity recognition using BART. arXiv preprint arXiv:2106.01760 (2021).
[6]Dai Dai, Xinyan Xiao, Yajuan Lyu, Shan Dou, Qiaoqiao She, and Haifeng Wang.
2019. Joint extraction of entities and overlapping relations using position-
attentive sequence labeling. In Proceedings of the AAAI conference on artificial
intelligence. 6300â€“6308. https://ojs.aaai.org/index.php/AAAI/article/view/4591
[7]Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. 2017.
Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition.
InProceedings of the 3rd Workshop on Noisy User-generated Text. Association for
Computational Linguistics, Copenhagen, Denmark, 140â€“147. https://doi.org/10.
18653/v1/W17-4418[8]Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng
Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-
training for natural language understanding and generation. Advances in neural
information processing systems 32 (2019).
[9]Zhifang Fan, Zhen Wu, Xinyu Dai, Shujian Huang, and Jiajun Chen. 2019. Target-
oriented opinion words extraction with target-fused neural sequence labeling. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). 2509â€“2518.
[10] Zhifang Fan, Zhen Wu, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. 2019. Target-
oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling.
InProceedings of the 2019 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.).
Association for Computational Linguistics, Minneapolis, Minnesota, 2509â€“2518.
https://doi.org/10.18653/v1/N19-1259
[11] Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-
Beltrachini. 2017. Creating training corpora for nlg micro-planning. In 55th
annual meeting of the Association for Computational Linguistics (ACL). https:
//inria.hal.science/hal-01623744/document
[12] Ankur Goswami, Akshata Bhat, Hadar Ohana, and Theodoros Rekatsinas. 2020.
Unsupervised Relation Extraction from Language Models using Constrained
Cloze Completion. In Findings of the Association for Computational Linguistics:
EMNLP 2020. Association for Computational Linguistics, Online, 1263â€“1276.
https://doi.org/10.18653/v1/2020.findings-emnlp.113
[13] Zhibin Gou, Qingyan Guo, and Yujiu Yang. 2023. MvP: Multi-view Prompting
Improves Aspect Sentiment Tuple Prediction. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics. https://arxiv.org/abs/
2305.12627
[14] Ralph Grishman. 2019. Twenty-five years of information extraction. Natural
Language Engineering 25, 6 (2019), 677â€“692.
[15] Ahmed Hammam, Frank Bonarens, Seyed Eghbal Ghobadi, and Christoph Stiller.
2022. Predictive Uncertainty Quantification of Deep Neural Networks using
Dirichlet Distributions. In Proceedings of the 6th ACM Computer Science in Cars
Symposium. 1â€“10.
[16] Mengting Hu, Yike Wu, Hang Gao, Yinhao Bai, and Shiwan Zhao. 2022. Improving
Aspect Sentiment Quad Prediction via Template-Order Data Augmentation. Pro-
ceedings of the 2022 Conference on Empirical Methods in Natural Language Process-
ing (EMNLP) (2022), 7889â€“7900. https://aclanthology.org/2022.emnlp-main.538
[17] Mengting Hu, Zhen Zhang, Shiwan Zhao, Minlie Huang, and Bingzhe Wu. 2023.
Uncertainty in Natural Language Processing: Sources, Quantification, and Appli-
cations. arXiv:2306.04459 [cs.CL]
[18] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic Uncertainty: Lin-
guistic Invariances for Uncertainty Estimation in Natural Language Generation.
arXiv:2302.09664 [cs.CL]
[19] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,
and Chris Dyer. 2016. Neural architectures for named entity recognition. arXiv
preprint arXiv:1603.01360 (2016).
[20] Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. 2018. Nugget Proposal Networks
for Chinese Event Detection. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics, Melbourne, Australia, 1565â€“1574. https://doi.org/10.
18653/v1/P18-1145
[21] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-
task deep neural networks for natural language understanding. arXiv preprint
arXiv:1901.11504 (2019).
[22] Jie Lou, Yaojie Lu, Dai Dai, Wei Jia, Hongyu Lin, Xianpei Han, Le Sun, and Hua
Wu. 2023. Universal Information Extraction as Unified Semantic Matching. arXiv
preprint arXiv:2301.03282 (2023).
[23] Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and
Hua Wu. 2022. Unified Structure Generation for Universal Information Extraction.
InProceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,
Dublin, Ireland, 5755â€“5772. https://doi.org/10.18653/v1/2022.acl-long.395
[24] Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2020. Knowing
what, how and why: A near complete solution for aspect-based sentiment analysis.
InProceedings of the AAAI Conference on Artificial Intelligence . 8600â€“8607. https:
//ojs.aaai.org/index.php/AAAI/article/view/6383
[25] Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou, Ion Androutsopoulos,
Suresh Manandhar, Mohammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao,
Bing Qin, OrphÃ©e De Clercq, et al .2016. Semeval-2016 task 5: Aspect based
sentiment analysis. In International workshop on semantic evaluation (SemEval
2016). 19â€“30. https://aclanthology.org/S16-1002.pdf
[26] Maria Pontiki, Dimitrios Galanis, Harris Papageorgiou, Suresh Manandhar, and
Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment
analysis. In Proceedings of the 9th international workshop on semantic evaluation
(SemEval 2015). 486â€“495. https://aclanthology.org/S15-2082.pdf
[27] Maria Pontiki, Dimitris Galanis, Ioannis Pavlopoulos, Harris Papageorgiou, Ion
Androutsopoulos, and Suresh Manandhar. 2014. SemEval 2014 Task 4: Aspect
 
91KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yinhao Bai et al.
Based Sentiment Analysis. (2014).
[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of
transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research 21, 1 (2020), 5485â€“5551.
[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits
of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine
Learning Research (JMLR) 21, 140 (2020), 1â€“67. http://jmlr.org/papers/v21/20-
074.html
[30] Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global
inference in natural language tasks. In Proceedings of the eighth conference on
computational natural language learning (CoNLL-2004) at HLT-NAACL 2004. 1â€“8.
https://aclanthology.org/W04-2401.pdf
[31] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika,
Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al .
2021. Multitask prompted training enables zero-shot task generalization. arXiv
preprint arXiv:2110.08207 (2021).
[32] Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. 2020.
A simple but tough-to-beat data augmentation approach for natural language
understanding and generation. arXiv preprint arXiv:2009.13818 (2020).
[33] Mohammad Golam Sohrab and Makoto Miwa. 2018. Deep Exhaustive Model
for Nested Named Entity Recognition. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing. Association for Computational
Linguistics, Brussels, Belgium, 2843â€“2849. https://doi.org/10.18653/v1/D18-1309
[34] Benjamin Strauss, Bethany Toma, Alan Ritter, Marie-Catherine de Marneffe,
and Wei Xu. 2016. Results of the WNUT16 Named Entity Recognition Shared
Task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT).
The COLING 2016 Organizing Committee, Osaka, Japan, 138â€“144. https://
aclanthology.org/W16-3919
[35] David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity,
Relation, and Event Extraction with Contextualized Span Representations. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China,
5784â€“5789. https://doi.org/10.18653/v1/D19-1585[36] Ziqi Wang, Xiaozhi Wang, Xu Han, Yankai Lin, Lei Hou, Zhiyuan Liu, Peng
Li, Juanzi Li, and Jie Zhou. 2021. CLEVE: contrastive pre-training for event
extraction. arXiv preprint arXiv:2105.14485 (2021).
[37] Ronald J Williams and David Zipser. 1989. A learning algorithm for continu-
ally running fully recurrent neural networks. Neural computation 1, 2 (1989),
270â€“280. https://direct.mit.edu/neco/article-abstract/1/2/270/5490/A-Learning-
Algorithm-for-Continually-Running-Fully
[38] Bowen Yu, Zhenyu Zhang, Xiaobo Shu, Yubin Wang, Tingwen Liu, Bin Wang,
and Sujian Li. 2019. Joint extraction of entities and relations based on a novel
decomposition strategy. arXiv preprint arXiv:1909.04273 (2019). https://ebooks.
iospress.nl/doi/10.3233/FAIA200356
[39] Dell Zhang, Murat Sensoy, Masoud Makrehchi, Bilyana Taneva-Popova, Lin
Gui, and Yulan He. 2023. Uncertainty quantification for text classification. In
Proceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval. 3426â€“3429.
[40] Wenxuan Zhang, Yang Deng, Xin Li, Yifei Yuan, Lidong Bing, and Wai Lam. 2021.
Aspect Sentiment Quad Prediction as Paraphrase Generation. In Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing (EMNLP).
9209â€“9219. https://aclanthology.org/2021.emnlp-main.726
[41] Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and Wai Lam. 2021. Towards
Generative Aspect-Based Sentiment Analysis. In Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (ACL-IJCNLP). 504â€“510. https:
//aclanthology.org/2021.acl-short.64
[42] Zhen Zhang, Mengting Hu, Shiwan Zhao, Minlie Huang, Haotian Wang, Lemao
Liu, Zhirui Zhang, Zhe Liu, and Bingzhe Wu. 2023. E-NER: Evidential Deep Learn-
ing for Trustworthy Named Entity Recognition. arXiv preprint arXiv:2305.17854
(2023).
[43] Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu.
2017. Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.
InProceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,
Vancouver, Canada, 1227â€“1236. https://doi.org/10.18653/v1/P17-1113
[44] Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu.
2017. Joint extraction of entities and relations based on a novel tagging scheme.
arXiv preprint arXiv:1706.05075 (2017).
 
92Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 6: Hyperparameters of the baseline methods.
Models Learning Rate Beam Size Epoch
GAS 3e-4 1 20
MvP 1e-4 1 20
UIE 2e-4 1 50
DLO 1e-4 5 20
Table 7: Hyperparameters of the proposed BCS.
Tasks Datasets ğ›¼ ğ›½
ASQPLaptop 0.5 9e-3
Restaurant 0.5 2e-1
Rest15 0.5 4e-2
Rest16 0.5 7e-3
ASTElaptop14 0.5 3e-2
Rest14 0.5 1e-3
Rest15 0.5 6e-1
Rest16 0.5 6e-2
TASDRest15 0.5 2e-1
Rest16 0.5 6e-2
NERWNUT16 0.5 7e-3
WNUT17 0.5 9e-2
JEREWebnlg 0.5 2e-1
CoNLL04 0.5 7e-3
Table 8: Average running time of each model.
Models Rest15 Restaurant
GAS 316s 579s
+BCS 537s 826s
DLO 1462s 2496s
+BCS 1927s 3420s
UIE 418s 709s
+BCS 612s 948s
MvP 5691s 8912s
+BCS 7249s 10384s
Inputs Sentence The food is good.
Quadruplet (at, ot, ac, sp)( food, good, food quality, positive )
Semantic Mapping 
(xat, xot, xac, xsp)( food, good, food quality, great )
GAS (at, ot, ac, sp)
Target sequence ( food, good, food quality, positive )
Paraphrase xac is xsp because xat is xot
Target sequence food quality is great because food is good
Special Symbols [AT] xat [OT] xot [AC] xac [SP] xsp
Target sequence [AT] food [OT] good [AC] food quality [SP] great
Figure 6: Template details of various methods.Table 5: Data statistics. #S and #Q denote the number of sen-
tences and quadruplets respectively.
Tasks DatasetsTrain Dev Test
#S #Q #S #Q #S #Q
ASQPLaptop 1530 2484 171 261 583 916
Restaurant 2934 4172 326 440 816 1161
Rest15 834 1354 209 347 537 795
Rest16 1264 1989 316 507 544 799
ASTELaptop14 906 1460 219 346 328 543
Rest14 1266 2338 310 577 492 994
Rest15 605 1013 148 249 322 485
Rest16 857 1394 210 339 326 514
TASDRest15 1120 1654 10 13 582 845
Rest16 1708 2507 29 44 587 859
NERWNUT16 2394 1483 1000 646 3849 3374
WNUT17 3394 1949 1009 826 1287 1072
JEREWebnlg 5019 11313 500 1224 703 1607
CoNLL04 922 1283 231 343 288 422
Inputs Great food and amazing service .
Label [( Great, food, POS ), ( amazing, service, POS )]
Prediction Set(Prediction) MS
Case-1[( Great, food, POS ), ( amazing, 
service, POS )]{Great, food, amazing, 
service, POS}100.0
Case-2[( Great, food, POS ), ( amazing, 
service, POS ), ( Great, service, 
POS )]{Great, food, amazing, 
service, POS}88.9
Case-3 [( Great, food, POS )] {Great, food, POS} 60.0
Case-4[( Great, food, POS ), ( Great, 
service, POS )]{Great, food, service, 
POS}66.7
Case-5[( Great, food, POS ), ( Good, 
service, POS )]{Great, food, amazing, 
service, POS, Good}55.6
Case-6 [( good, foods, NEG )] {good, foods, NEG} 0.0
Figure 7:ğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ for several different error types.
A EVALUATION CASE STUDY
In the preceding analysis, we introduce our proposed evaluation
methodğ‘€ğ‘ğ‘¡ğ‘â„ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ . To enhance readersâ€™ comprehension, we
introduce several illustrative cases to provide a more intuitive ex-
planation of the MScalculation under different error scenarios.
As shown in Figure 7, for the input sentence, its gold tuples is
[(Great, food, POS), (amazing, service, POS)] and set( ğ’ˆ) is {Great,
food, amazing, service, POS}. First, Case-1 describes a completely
correct prediction, its MSis 100%. If and only if the prediction result
is completely correct, MSwill be calculated with a maximum score of
100%. Second, Case-2 shows that when predicting multiple groups,
this prediction should be imperfect, so the MSis 88.8%. Then, we
observe that in Case-6, when all elements within the tuple are
predicted incorrectly, its MSscore is 0.
Besides, it is worth noting that both Case-4 and Case-5 only have
one wrong element, but since â€œGoodâ€ is a prediction beyond the set
of gold tuples, the MSof Case-5 is lower than that of Cas-6. MScan
distinguish different types of error benefits from the way we use
 
93KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yinhao Bai et al.
Table 11: Comparison with other strategies.
MethodsASQP [Rest15] ASTE [Rest15] TASD [Rest15]
F1 MS F1 MS F1 MS
Binomial 49.03 62.12 65.91 72.49 64.03 70.53
Gaussian 48.94 62.04 64.57 72.06 64.72 70.94
Table 9: Comparison with MvP.
TasksMvP MvP+BCS
F1 MS F1 MS
ASQP [Rest16] 60.39 70.74 61.21 72.10
ASTE [Rest16] 73.48 77.32 74.59 78.25
TASD [Rest15] 64.53 72.25 65.01 72.83
NER [Wnut17] 61.30 64.21 61.77 64.98
JERE [Conll04] 67.02 77.14 68.55 78.54
Table 10: Comparison with UIE.
TasksUIE UIE+BCS
F1 MS F1 MS
ASQP [Rest16] 57.31 69.54 59.39 70.42
ASTE [Rest16] 72.86 75.55 73.59 77.08
TASD [Rest15] 61.47 69.24 63.06 70.18
NER [Wnut17] 52.67 68.59 53.91 69.41
JERE [Conll04] 75.00 80.03 76.02 81.19
sets as part of the MScalculation in Equation 8. In summary, our
evaluation method MScan more accurately identify different types
of errors in prediction tuples.
B EXPERIMENTAL SETUP
In the experiments, all the reported results are the average of 3
runs. For all baselines and our methods, we use T5-base [ 29] as our
pre-trained language model. When applying our method, as shown
in Table 6, we keep all the parameters the same as in the baseline
methods. In detail, beam size is the number of paths searched by the
beam search at the inference stage. And beam size is 1, indicating
that the inference stage uses the greedy search for decoding.Otherwise, we set ğ›¼andğ›½for the final training objective in our
proposed BCS, the hyperparameter details are presented in Table 7.
C SUPPLEMENTARY EXPERIMENTS
C.1 More Evaluation on Baselines
To enhance the credibility of our results, we evaluated other base-
line methods, which are the current state-of-the-art models on the
assessed datasets (e.g., MvP as shown in Table 9, and UIE as shown
in Table 10). During the evaluation process, we ensured that our
model and the baseline models were rerun under the same number
of runs and seed settings, and compared them on a dataset across
five different IE tasks (including ASQP, ASTE, TASD, NER, and
JERE). For the baseline results, we directly referred to the data re-
ported in the original papers. For the UIE baseline, we used the
UIE-base model.
The experimental results show that BCS method is generally
applicable to other models and achieve superior performance across
various tasks. This further validates the effectiveness and robustness
of our methods.
C.2 Comparison with Other Strategies
In bridging semantic uncertainty and MS, we explored other dis-
tribution strategies with two parameters, specifically the Gaussian
distribution, to observe the impact of the distribution on the coun-
terpart sequence. As shown in Table 11, we found that in the ASQP
and ASTE tasks, the binomial distribution slightly outperformed the
Gaussian distribution. Conversely, in the TASD task, the Gaussian
distribution performed better. This indicates that different distri-
butions can influence the construction of counterpart sequences.
C.3 Running Time Analysis
The comparison of average running times across various models is
detailed in Table 8, highlighting that the BCS method incurs longer
training durations across four generation-based methodologies. It
is worth noting that the MvP method requires 15 times the amount
of data for training compared to GAS, resulting in a proportional
increase in training duration. Consequently, for the sake of simplic-
ity and efficiency in our main experiments, GAS is designated as
the baseline method. While acknowledging this increased time as
a drawback, itâ€™s important to note that BCS does not necessitate
extra human effort, offering significant advantages for baselines.
 
94