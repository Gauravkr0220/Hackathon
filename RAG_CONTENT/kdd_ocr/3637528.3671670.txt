Fast and Accurate Domain Adaptation for
Irregular Tensor Decomposition
Junghun Kim
Seoul National University
Seoul, South Korea
bandalg97@snu.ac.krKa Hyun Park
Seoul National University
Seoul, South Korea
kahyun.park@snu.ac.krJun-Gi Jang
University of Illinois
Urbana-Champaign
Urbana, Illinois, USA
jungi@illinois.eduU Kang
Seoul National University
Seoul, South Korea
ukang@snu.ac.kr
Abstract
Given an irregular tensor from a newly emerging domain, how can
we quickly and accurately capture its patterns utilizing existing
irregular tensors in multiple domains? The problem is of great im-
portance for various tasks such as nding patterns of a new disease
using pre-existing diseases data. This is challenging as new target
tensors have limited information due to their recent emergence.
Thus, carefully utilizing the existing source tensors for analyzing
the target tensor is helpful. PARAFAC2 decomposition is a strong
tool for nding the patterns of irregular tensors, and the patterns
are used in many applications such as missing value prediction and
anomaly detection. However, previous PARAFAC2-based works
cannot adaptably handle newly emerging target tensors utilizing
the source tensors.
In this work, we propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain
adaptation method for irregular tensor decomposition. M/e.sc/t.sc/a.sc/hyphen.scP2
generates a meta factor matrix from the multiple source domains,
by domain adaptation and meta-update steps. M/e.sc/t.sc/a.sc/hyphen.scP2 quickly
and accurately nds the patterns of the new irregular tensor utiliz-
ing the meta factor matrix. Extensive experiments on real-world
datasets show that M/e.sc/t.sc/a.sc/hyphen.scP2 achieves the best performance in var-
ious downstream tasks including missing value prediction and
anomaly detection tasks.
CCS Concepts
â€¢Computing methodologies !Factorization methods; Trans-
fer learning.
Keywords
multi-domain, irregular tensor, tensor decomposition
ACM Reference Format:
Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang. 2024. Fast and
Accurate Domain Adaptation for Irregular Tensor Decomposition . In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671670
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specic permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671670
New themeMultiplesectorsTensors ofsource domainsNew tensor oftarget domainResult
Decomposition resultsfor the target tensorPatterns of AIGivenGoal
Utilize source domainsHealth careMaterialsFinancialsEnergyTimesStocksTimesDatabaseAI
StocksFigure 1: An example of domain adaptation for irregular ten-
sors. We are given a stock database, which is represented as ir-
regular tensors of multiple domains. Each domain represents
a specic sector, and each slice Xğ‘‘;ğ‘˜denotes time-varying
stock information (such as opening and closing prices) of ğ‘˜-th
stock in sector ğ‘‘. Then, we encounter a new unseen themed-
stocks (target tensor). The goal is to nd patterns within this
new tensor through tensor decomposition exploiting data
from multiple source domains.
1 Introduction
How can we quickly and accurately capture the patterns of a new
irregular tensor from a target domain utilizing irregular tensors from
various source domains? Many real-world data often take the form
of irregular tensors across various domains. For example, stocks
from diverse industries are represented as irregular tensors in mul-
tiple domains. Despite their distinct listing periods, these tensors
share a common set of features, such as the opening or closing
price. In this scenario, the objective is to eciently and accurately
capture patterns within the stocks of a recently emerged industry
by leveraging pre-existing stock data (see Figure 1). The challenge
involves addressing how to quickly identify the patterns of a new
irregular tensor within the target domain, utilizing insights gained
from irregular tensors sourced from various domains. The problem
includes many other scenarios such as nding new disease patterns
using pre-existing diseases data as well as nding action patterns
of a new person using the records of multiple people.
To nd patterns of high-dimensional data, decomposition-based
methods have been actively studied in data mining area [ 24,36?
]. Among the various methods, PARAFAC2 decomposition meth-
ods are widely used for nding the patterns of irregular tensors
by representing them with low-rank latent factor matrices. The
 
1383
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
low-rank latent factor matrices are used for many applications in-
cluding missing value prediction [ 3], anomaly detection [ 5], failure
prediction [43], and image recommendation [30].
However, existing PARAFAC2-based decomposition methods are
not easy to be directly used in our problem. There are two chal-
lenges for them: 1) the limited information of target tensor due to
their recent emergence, and 2) disparities between the domains (e.g.,
stock movements dier across various industries). A naive approach
would be to conduct PARAFAC2 decomposition only on the new
target tensor. This neglects valuable information from the source
domains, and leads to an inaccurate decomposition of the target
tensor. Another approach would perform PARAFAC2 decomposi-
tion on source and target tensors simultaneously by combining all
tensors. However, this approach fails to nd accurate latent factor
matrices of the target tensor if the source and target domains have
dierent properties. Furthermore, the approach should decompose
the tensors from entire domains whenever a new target tensor is
given, thus is computationally inecient. Therefore, it is important
to carefully extract useful information from source domains that
help analyze the target tensor in advance.
In this work, we propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain
adaptation method for irregular tensor decomposition. M/e.sc/t.sc/a.sc/hyphen.scP2
carefully learns a general latent factor matrix from multiple source
domains based on the meta-learning approach (see Section 2.3
for details). Then the latent factor matrix is used for analyzing a
new target tensor. This process enables us to eectively transfer
the general knowledge from source domains to the target domain,
leading to accurate decomposition of the target tensor.
M/e.sc/t.sc/a.sc/hyphen.scP2 successfully tackles the challenges emerged in previous
approaches. First, M/e.sc/t.sc/a.sc/hyphen.scP2 eciently supplements the limited in-
formation of the target tensor by proactively extracting useful infor-
mation from the source tensors. Unlike existing methods, M/e.sc/t.sc/a.sc/hyphen.scP2
does not require tensors from entire domains to be decomposed
together when a new target tensor emerges. Second, M/e.sc/t.sc/a.sc/hyphen.scP2 accu-
rately decomposes the target tensor in scenarios where the source
and target domains exhibit distinct characteristics. This is achieved
by extracting general and easily adaptable information from the
source domains, instead of domain-specic one.
Our contributions are summarized as follows:
Method. We propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain
adaption method for decomposing irregular tensors. M/e.sc/t.sc/a.sc/hyphen.sc
P2eectively nds the patterns of a new irregular tensor via
meta-learning.
Theory. We give theoretical analysis on the properties of
M/e.sc/t.sc/a.sc/hyphen.scP2, including the time complexities, and the relation
between the meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 and the update
step of the previous meta-learning approach.
Experiments. Extensive experiments on real-world datasets
show that M/e.sc/t.sc/a.sc/hyphen.scP2 achieves the best performance in missing
value prediction and anomaly detection problems.
The rest of this paper is organized as follows. Section 2 gives
preliminaries and a formal denition of our problem. Section 3 pro-
poses M/e.sc/t.sc/a.sc/hyphen.scP2 and analyzes its time complexity. Section 4 shows
experimental results, and Section 5 introduces related works. We
conclude the paper in Section 6. The code and datasets are available
athttps://github.com/snudatalab/Meta-P2.Table 1: Symbols.
Symb
ol Description
fXğ‘˜gğ¾
ğ‘˜=1irr
egular tensorÂ¹2Rğ¼ğ‘˜ğ½ğ¾forğ‘˜=1Â•Â”Â”Â”ğ¾Âº
Xğ‘˜ğ‘˜-th
slice offXğ‘˜gğ¾
ğ‘˜=1Â¹2Rğ¼ğ‘˜ğ½Âº
Uğ‘˜Â•Sğ‘˜Â•Qğ‘˜ factor
matrices of the ğ‘˜-th slice Xğ‘˜
HÂ•V factor
matrices offXğ‘˜gğ¾
ğ‘˜=1
fXğ‘‘;ğ‘˜gğ¾
ğ‘˜=1irr
egular tensor of domain ğ‘‘
Xğ‘‘;ğ‘˜ğ‘˜-th
slice offXğ‘‘;ğ‘˜gğ¾
ğ‘˜=1Â¹2Rğ¼ğ‘˜ğ½Âº
Uğ‘‘;ğ‘˜Â•Sğ‘‘;ğ‘˜Â•Qğ‘‘;ğ‘˜factor
matrices of the ğ‘˜-th slice Xğ‘‘;ğ‘˜
Hğ‘‘Â•Vğ‘‘ factor
matrices offXğ‘‘;ğ‘˜gğ¾
ğ‘˜=1
XÂ¹ğ‘›Âº mo
de-ğ‘›matricization of X2Rğ¼1ğ¼2ğ¼ğ‘
 Khatri-Rao
product
 Hadamar
d product
ğ‘… target
rank of decomposition
2 Preliminaries and Problem
We describe tensor notations and operations, PARAFAC2, and meta-
learning. We then formally dene the problem.
2.1 Tensor Notations and Operations
We provide the symbols used in this paper in Table 1.
Irregular Tensor. We denote an irregular tensor as a set fXğ‘˜gğ¾
ğ‘˜=1
where Xğ‘˜2Rğ¼ğ‘˜ğ½is theğ‘˜-th slice of the tensor, and ğ¾is the num-
ber of slices. Note that the row sizes ğ¼ğ‘˜of the slices are dierent
for allğ‘˜. An irregular tensor in domain ğ‘‘is denoted asfXğ‘‘;ğ‘˜gğ¾
ğ‘˜=1.
We denotefXğ‘‘;ğ‘˜gğ¾
ğ‘˜=12Rğ¼ğ‘˜ğ½ğ¾asfXğ‘‘gin this paper for brevity.
Mode-ğ‘›Matricization. Given a tensor X2Rğ¼1ğ¼ğ‘, we de-
note mode-ğ‘›matricization of XasXÂ¹ğ‘›Âº. The resulting unfolded
matrix is of sizeÂ¹ğ¼ğ‘›Â•Ãğ‘
ğ‘š=1Â•ğ‘šâ‰ ğ‘›ğ¼ğ‘šÂº. The operation arranges the
mode-ğ‘›bers to be the columns of the resulting matrix.
Khatri-Rao Product. Khatri-Rao product of two matrices A2
Rğ‘ğ‘ŸandB2Rğ‘ğ‘Ÿ, which is denoted by AB2Rğ‘ğ‘ğ‘Ÿ, is com-
puted as AB=Â»AÂ¹:Â•1Âº
BÂ¹:Â•1Âºjj AÂ¹:Â•ğ‘ŸÂº
BÂ¹:Â•ğ‘ŸÂºÂ¼where
is Kronecker product and jis the horizontal concatenation. The
Kronecker product AÂ¹:Â•ğ‘˜Âº
BÂ¹:Â•ğ‘˜Âºofğ‘˜-th column is computed as
Â»AÂ¹1Â•ğ‘˜ÂºBÂ¹:Â•ğ‘˜Âº>jjAÂ¹ğ‘Â•ğ‘˜ÂºBÂ¹:Â•ğ‘˜Âº>Â¼>.
Hadamard Product. Given two matrices AandBof the same
size, the Hadamard product ABis a matrix of the same size as
the operands, with elements given by Â¹ABÂºÂ¹ğ‘–Â•ğ‘—Âº=AÂ¹ğ‘–Â•ğ‘—ÂºBÂ¹ğ‘–Â•ğ‘—Âº.
2.2 PARAFAC2 Decomposition
PARAFAC2 decomposition analyzes an irregular tensor by approxi-
mating it with a multiplication of latent factor matrices. The objec-
tive of PARAFAC2 is to minimize the following objective function:
min
fUğ‘˜gÂ•fSğ‘˜gÂ•Vâˆ‘ï¸ğ¾
ğ‘˜=1kXğ‘˜ Uğ‘˜Sğ‘˜V>k2
ğ¹(1)
where Xğ‘˜2Rğ¼ğ‘˜ğ½is ağ‘˜-th slice of the original irregular tensor
fXğ‘˜gğ¾
ğ‘˜=12Rğ¼ğ‘˜ğ½ğ¾, and Uğ‘˜2Rğ¼ğ‘˜ğ‘…,Sğ‘˜2Rğ‘…ğ‘…, and V2Rğ½ğ‘…
are the decomposed factor matrices of Xğ‘˜with rankğ‘…. Note that V
is shared along the slices.
To ensure the uniqueness of the results, Harshman [ 14] replaced
Uğ‘˜withQğ‘˜Hwhere Qğ‘˜2Rğ¼ğ‘˜ğ‘…is a column orthogonal matrix,
 
1384Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
andH2Rğ‘…ğ‘…is a common matrix for all the slices. Then the
objective function of PARAFAC2 is reformulated as
min
fQğ‘˜gÂ•fSğ‘˜gÂ•HÂ•Vâˆ‘ï¸ğ¾
ğ‘˜=1kXğ‘˜ Qğ‘˜HSğ‘˜V>k2
ğ¹Â” (2)
ALS (Alternating Least Square) is commonly used for optimizing
Equation (2)due to its fast speed and accuracy [ 46]. CP-ALS, which
uses ALS for PARAFAC2 decomposition, iteratively updates each
factor matrix while xing the other ones. A single iteration of
CP-ALS updates factor matrices by running Equations (3)-(7)once.
CP-ALS rst nds Qğ‘˜while xing HÂ•Sğ‘˜, andVfor allğ‘˜:
Ağ‘˜Î£B>
ğ‘˜ Xğ‘˜VSğ‘˜H>(3)
Qğ‘˜ Ağ‘˜B>
ğ‘˜(4)
where Equation (3)is the truncated SVD with rank ğ‘…. Then, CP-ALS
updates HÂ•fSğ‘˜g, andVas follows:
W YÂ¹3ÂºÂ¹VHÂºÂ¹V>VH>HÂºy(5)
H YÂ¹1ÂºÂ¹WVÂºÂ¹W>WV>VÂºy(6)
V YÂ¹2ÂºÂ¹WHÂºÂ¹W>WH>HÂºy(7)
where theğ‘˜-th row of Wğ‘ 2Rğ¾ğ‘…is the diagonal entries of Sğ‘˜,Y2
Rğ‘…ğ½ğ¾is a constructed tensor from slices Yğ‘˜for allğ‘˜, andYğ‘˜2
Rğ‘…ğ½isQ>
ğ‘˜Xğ‘˜. For eachğ‘˜,Sğ‘˜andUğ‘˜are updated as ğ‘‘ğ‘–ğ‘ğ‘”Â¹WÂ¹ğ‘˜Â•:ÂºÂº
andQğ‘˜H, respectively. CP-ALS repeats Equations (3)-(7)until the
results converge.
2.3 Model-Agnostic Meta-Learning (MAML)
Meta-learning is a powerful framework to address the problem
of limited data in many machine learning areas. Meta-learning
trains a general model with multiple given tasks so that it adapts
to a new problem quickly, instead of training task-specic models.
Previous meta-learning methods train the general model using
gradient information from the given tasks [ 16,39]. Among them,
MAML [ 10] has been widely used due to its robustness and accuracy.
MAML trains a model on multiple given tasks Tğ‘‘forğ‘‘=1Â•Â”Â”Â”Â•ğ· ,
with the goal of nding a set of initial parameters ğœƒmeta that can
be ne-tuned to perform well on a new target task quickly and
accurately. Note that the objective of ğœƒmeta is not to perform well
on the given tasks, but to quickly adapt to a new unseen target task.
For every given task Tğ‘‘, MAML adapts the meta model parame-
tersğœƒmeta to eachTğ‘‘by performing a single gradient descent update
with the objective function LTğ‘‘:
ğœƒ0
ğ‘‘ ğœƒmeta ğ›¼rğœƒmetaLTğ‘‘Â¹ğ‘“ğœƒmetaÂº (8)
whereğœƒ0
ğ‘‘denotes the adapted ğœƒmeta to eachTğ‘‘. This simulates
the adaptation process of ğœƒmeta to a new target task with a few
gradient-descent updates.
MAML then updates ğœƒmeta following the mean of gradients
rğœƒLTğ‘‘Â¹ğ‘“ğœƒ0
ğ‘‘Âºforğ‘‘=1Â•Â”Â”Â”Â•ğ· as follows:
ğœƒmeta ğœƒmeta ğ›½rğœƒmetaâˆ‘ï¸ğ·
ğ‘‘=1LTğ‘‘Â¹ğ‘“ğœƒ0
ğ‘‘ÂºÂ” (9)
Note that the meta-gradient is a gradient in terms of ğœƒmeta whereas
the loss function is parameterized by ğœƒ0
ğ‘‘. This enables ğœƒmeta to con-
tain easily transplantable information from all tasks Tğ‘‘. We denote
rğœƒmetaÃğ·
ğ‘‘=1LTğ‘‘Â¹ğ‘“ğœƒ0
ğ‘‘Âº, which is a specially designed gradient formeta-learning, as meta-gradient in the rest of this paper. We repeat
the process until the maximum iteration is reached.
2.4 Problem Denition
We formally dene the problem in Problem 1.
P/r.sc/o.sc/b.sc/l.sc/e.sc/m.sc 1 (D/o.sc/m.sc/a.sc/i.sc/n.sc /a.sc/d.sc/a.sc/p.sc/t.sc/a.sc/t.sc/i.sc/o.sc/n.sc /f.sc/o.sc/r.sc /i.sc/r.sc/r.sc/e.sc/g.sc/u.sc/l.sc/a.sc/r.sc /t.sc/e.sc/n.sc/s.sc/o.sc/r.sc /d.sc/e.sc/hyphen.sc
/c.sc/o.sc/m.sc/p.sc/o.sc/s.sc/i.sc/t.sc/i.sc/o.sc/n.sc). We are given a set Sof source domains. For each
domainğ‘ 2S, there is an irregular tensor fXğ‘ g. Then, we have a
new irregular tensor fXğ‘‡gfrom an unseen target domain ğ‘‡. The
problem aims to quickly nd the latent factor matrices of fXğ‘‡gthat
reconstructfXğ‘‡gaccurately exploiting the tensors of source domains.
The objective function Lof the problem is as follows:
min
ğœƒğ‘‡LÂ¹fXğ‘‡g;ğœƒğ‘‡Âº=âˆ‘ï¸ğ¾
ğ‘˜=1kXğ‘‡;ğ‘˜ Uğ‘‡;ğ‘˜Sğ‘‡;ğ‘˜V>
ğ‘‡k2
ğ¹(10)
where Xğ‘‡;ğ‘˜2Rğ¼ğ‘˜ğ½is ağ‘˜-th slice offXğ‘‡g2Rğ¼ğ‘˜ğ½ğ¾, andğœƒğ‘‡=
ffUğ‘‡gÂ•fSğ‘‡gÂ•Vğ‘‡g. Note that the knowledge of source domains is
utilized for optimizing the objective function in Equation (10).
3 Proposed Method
We propose M/e.sc/t.sc/a.sc/hyphen.scP2, our fast and accurate domain adaptation
method for irregular tensors. The main challenges and our ap-
proaches are as follows:
(1)How can we utilize given source tensors for decom-
posing a new tensor in a target domain quickly and
accurately? We learn a general meta factor matrix from
given source domains, which is easily transferred to the new
target domain (Section 3.1).
(2)How can we nd the meta factor matrix from source do-
mains which is easily transferred to the target domain?
We perform meta-learning-based PARAFAC2 decomposition
on source domains. This is achievable by carefully modeling
the gradients of reconstruction error in terms of the meta
factor matrix (Section 3.2).
(3)How can we utilize the trained meta factor when de-
composing a new tensor of a target domain? We adapt
the meta factor to the target domain while preventing the
decomposed results of the target domain from completely
forgetting the information of source domains (Section 3.3).
3.1 Meta Factor Matrix
The objective of M/e.sc/t.sc/a.sc/hyphen.scP2 is to quickly nd accurate latent factor
matrices of a new irregular tensor fXğ‘‡gof the target domain ğ‘‡,
using the given irregular tensors fXğ‘ gin multiple source domains
ğ‘ 2S(see Section 2.4 for a formal problem denition and its ob-
jective function). Specically, we aim to accurately decompose the
target tensorfXğ‘‡gwith limited information in a few ALS iteration
updates. To do this, it is important to dene the information from
source domains that helps decompose the target tensor. If the in-
formation is slice-specic or domain-specic, it cannot directly be
used for analyzing the new unseen target tensor.
For each tensorfXğ‘ gin a source domain ğ‘ , the decomposed
results are represented by the multiplication of three components:
fUğ‘ ;ğ‘˜g,fSğ‘ ;ğ‘˜g, andVğ‘ . Eachğ‘˜-th slice Xğ‘ ;ğ‘˜offXğ‘ gis decomposed
intoUğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜Vğ‘ . Note that Vğ‘ contains representative information
 
1385KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
Sourceadaptation
Meta updateAt first, initialize         as           Residual connectionSource domain 1Source domain 2Source domain S
Multiple source domainsNew target domain T
Figure 2: Overall process of M/e.sc/t.sc/a.sc/hyphen.scP2. We train the easily
adaptable meta factor Vmetafrom multiple source tensors
fXğ‘ gforğ‘ 2f1Â•Â”Â”Â”Â•ğ‘†g. This is done by iteratively performing
the source adaptation and meta-update steps. We utilize the
trained Vmetafor decomposing a new target tensor fXğ‘‡g.
of domainğ‘ since Vğ‘ is used for constructing every slice of fXğ‘ g,
whilefUğ‘ ;ğ‘˜gandfSğ‘ ;ğ‘˜gare specic to each slice.
We propose to share Vğ‘ along the source domains, which we
denote as Vmeta, and utilize it for analyzing the target domain. The
intuition is to learn general information from the source domains by
forcing Vmeta to reconstruct every source tensor well. A challenge
is that naively sharing Vmeta only aims to reconstruct the source
tensors, and fails to achieve accurate results on the target domain
when the target has unseen properties in source domains.
To address this, M/e.sc/t.sc/a.sc/hyphen.scP2 enforces Vmeta to possess an additional
property: easily adaptable. We learn Vmeta to quickly and accurately
adapt to any new domain, not only to the given source domains.
This is done by training Vmeta based on the meta-learning scheme.
As a result, M/e.sc/t.sc/a.sc/hyphen.scP2 learns general andeasily adaptable meta factor
matrix Vmeta from the source tensors.
The overall process of M/e.sc/t.sc/a.sc/hyphen.scP2 is illustrated in Figure 2. We
rst compute the meta factor matrix Vmeta from multiple source
domains. Then we leverage the trained Vmeta for decomposing
a new target tensor fXğ‘‡g. This enables us to eectively utilize
useful information of source domains for analyzing the target tensor
without decomposing all the domains together.
3.2 Decomposition on Source Domains
M/e.sc/t.sc/a.sc/hyphen.scP2 learns the general andeasily adaptable meta factor Vmeta
from the multiple source tensors based on the meta-learning scheme.
The meta-learning aims to train a general model that adapts to a
new task quickly and accurately utilizing multiple source tasks.
The meta-learning based decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 in the source
domains consists of two steps: a) source adaptation and b) meta-
update steps. The source adaptation step adapts Vmeta to every
source domain. Using the adapted results, the meta-update step
nds Vmeta that is prepared for adaptation to any new domain,
rather than accurate reconstruction of the source tensors. We repeat
those two steps until the maximum iteration is reached. Algorithm 1
shows the overall process of decomposition in source domains.
Using the objective function LofM/e.sc/t.sc/a.sc/hyphen.scP2 in Equation (10), the
source adaptation of Vmeta for each source domain ğ‘ formulated as:
Vğ‘ =Vmeta ğœ‚rVmetaâˆ‘ï¸ğ¾
ğ‘˜=1kXğ‘ ;ğ‘˜ Uğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜V>
metak2
ğ¹(11)Algorithm 1: Computing the meta factor matrix from mul-
tiple source domains.
input : Irregular tensorsfXğ‘ ;ğ‘˜gğ¾
ğ‘˜=12Rğ¼ğ‘˜ğ½ğ¾from
all source domains ğ‘ =1Â•Â”Â”Â”Â•ğ‘†
output : Meta factor matrix Vmeta2Rğ½ğ‘…
parameters: Target rank ğ‘…
1Randomly initialize Wğ‘ 2Rğ‘…ğ‘…,Hğ‘ 2Rğ‘…ğ‘…for all
ğ‘ =1Â•Â”Â”Â”Â•ğ‘† , andVmeta;
2repeat
/*Source adaptation step */
3 fors=1,Â”Â”Â”, Sdo
4 Adapt Vmeta to domainğ‘ by a single ALS iteration,
and save the result as Vğ‘ ; // Equations (15), (16)
5 end
/*Meta-update step */
6 Update Vmeta along the meta-gradient, which is
computed by aggregating all V0ğ‘ ; // Equation (21)
7until the maximum iteration is reached ;
whereğœ‚is a learning rate. To remove the constant term Uğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜in
front of Vmeta, we rewriteLÂ¹fXğ‘ g;VmetaÂºas follows:
Lğ‘‰Â¹fXğ‘ g;VmetaÂº=âˆ‘ï¸ğ¾
ğ‘˜=1kEğ‘ ;ğ‘˜Xğ‘ ;ğ‘˜ V>
metak2
ğ¹Â” (12)
where Eğ‘ ;ğ‘˜=ğ¾Â¹Ãğ¾
ğ‘—=1S>
ğ‘ ;ğ‘—U>
ğ‘ ;ğ‘—Uğ‘ ;ğ‘—Sğ‘ ;ğ‘—Âº 1S>
ğ‘ ;ğ‘˜U>
ğ‘ ;ğ‘˜. Note that mini-
mizingLğ‘‰in Equation (12)is equivalent to minimizing Lin Equa-
tion(10)for training Vmeta (see Appendix A.1 for proof). Then the
source adaptation step of M/e.sc/t.sc/a.sc/hyphen.scP2 is expressed as follows:
Vğ‘ =Vmeta ğœ‚rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºÂ” (13)
The meta-update step of Vmeta is as follows:
Vmeta Vmeta ğœ‚rVmetaâˆ‘ï¸
ğ‘ 2SLğ‘‰Â¹fXğ‘ g;Vğ‘ ÂºÂ” (14)
However, CP-ALS algorithm is widely used for learning the latent
factors of irregular tensors [ 1,19,31] compared to the gradient-
descent one. This is mainly because the ALS-based decomposition
gives accurate factors in a few iterations and guarantees the unique-
ness in the decomposed results [ 14]. In the following Sections 3.2.1
and 3.2.2, we describe how we design the gradient-based meta-
learning by the ALS-based meta-learning for M/e.sc/t.sc/a.sc/hyphen.scP2.
3.2.1 Source adaptation step. In the source adaptation step, we
adapt Vmeta to each source domain by performing a single ALS iter-
ation instead of the single gradient-based update in Equation (13).
The idea stems from the observation that both gradient-based and
ALS-based training nds the optimal parameter by gradually mini-
mizing the objective function. This idea also aligns with the typical
meta-learning scenario which constrains the numbers of gradient-
descent updates to adapt the meta model parameters to each given
task. To perform the ALS-based adaptation of Vmeta, we rst update
Wğ‘ andHğ‘ using Equations (5) and (6), respectively:
Wğ‘ =Â¹Yğ‘ ÂºÂ¹3ÂºÂ¹VmetaHğ‘ ÂºÂ¹V>
metaVmetaH>
ğ‘ Hğ‘ Âºy
Hğ‘ =Â¹Yğ‘ ÂºÂ¹1ÂºÂ¹Wğ‘ VmetaÂºÂ¹W>
ğ‘ Wğ‘ V>
metaVmetaÂºy(15)
 
1386Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
where Yğ‘ 2Rğ‘…ğ½ğ¾is constructed from slices Yğ‘ ;ğ‘˜=Q>
ğ‘ ;ğ‘˜Xğ‘ ;ğ‘˜for
allğ‘˜=1Â•Â”Â”Â”Â•ğ¾ , and Qğ‘ ;ğ‘˜2Rğ¼ğ‘˜ğ‘…is computed using Equation (4).
Theğ‘˜-th row of Wğ‘ 2Rğ¾ğ‘…corresponds to the diagonal entries of
Sğ‘˜. Then we adapt Vmeta to each domain ğ‘ using Equation (7):
Vğ‘ =Â¹Yğ‘ ÂºÂ¹2ÂºÂ¹Wğ‘ Hğ‘ ÂºÂ¹Wğ‘ >Wğ‘ Hğ‘ >Hğ‘ ÂºyÂ” (16)
Note that we express the gradient-based source adaptation step in
Equation (13) by an ALS-based adaptation in Equation (16).
3.2.2 Meta-update step. In the meta-update step, we update the
meta factor matrix Vmeta to perform well after adapting to any new
target domain, rather than generally performing well across all
source domains. Our approach for designing the gradient-based
meta-update in Equation (14)by an ALS-based meta-update for
M/e.sc/t.sc/a.sc/hyphen.scP2 is to express the meta-gradientrVmetaÃ
ğ‘ 2SLğ‘‰Â¹fXğ‘ g;Vğ‘ Âº
using the factors computed by ALS iterations. However, directly
dierentiatingLğ‘‰Â¹fXğ‘ g;Vğ‘ gin terms of Vmeta is challenging since
Vğ‘ is not parameterized by Vmeta.
To address this, we break down each rVmetaLğ‘‰Â¹fXğ‘ g;Vğ‘ Âºof
the meta-gradient into two gradients rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºand
rVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âº. In the following, we rst describe how we ex-
press the two gradients using only the factors computed by ALS
iterations, and then we show the relationship between the meta-
gradient and the two gradients.
Two Gradients. LetV0ğ‘ denote Vğ‘ adapted to domain ğ‘ again:
V0
ğ‘ =Vğ‘  ğœ‚rVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âº (17)
=Â¹Yğ‘ ÂºÂ¹2ÂºÂ¹W0
ğ‘ H0
ğ‘ ÂºÂ¹W0
ğ‘ >W0
ğ‘ H0
ğ‘ >H0
ğ‘ Âºy(18)
where W0ğ‘ andH0ğ‘ denote Wğ‘ andHğ‘ after performing an additional
ALS updates in Equation (15)while replacing Vmeta withVğ‘ . The
second equality uses our approximation in the source adaptation
step that a single gradient-descent update is equal to a single ALS
iteration update (see Equations (13)and(16)). Then the two gra-
dientsrVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºandrVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âºare simply
expressed byÂ¹Vmeta Vğ‘ ÂºÂğœ‚andÂ¹Vğ‘  V0ğ‘ ÂºÂğœ‚using Equations (13)
and(17), respectively. Note that Vğ‘ andV0ğ‘ are computed by the
ALS iterations in Equations (16) and (18).
Relationship between Meta-gradient and the Two Gra-
dients. Lemma 3.1 shows the relationship between each term
rVmetaLğ‘‰Â¹fXğ‘ g;Vğ‘ Âºof the meta-gradient and the two gradients
rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºandrVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âº.
L/e.sc/m.sc/m.sc/a.sc 3.1. Each termrVmetaLğ‘‰fXğ‘ g;Vğ‘ Âºof the meta-gradient is
proportional to the sum of the two gradients rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂº
andrVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âº:
rVmetaLğ‘‰Â¹fXğ‘ g;Vğ‘ Âº=ğ¶Â¹rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºÂ¸
rVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ ÂºÂº (19)
whereğ¶=Â¹1 2ğœ‚ğ¾Âº2ÂÂ¹2Â¹1 ğœ‚ğ¾ÂºÂºis a constant. ğœ‚is the learning rate
andğ¾is the number of slices in each source domain (see Appendix A.2
for proof).
Using Lemma 3.1, we derive Theorem 3.2 which expresses the
meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in terms of V0ğ‘ that is computed by
the ALS iteration in Equation (18).Algorithm 2: Tensor decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on a new
target tensor exploiting the meta factor matrix.
input : Irregular tensor Xğ‘‡2Rğ¼ğ‘˜ğ½ğ¾, and a trained
meta factor Vmeta2Rğ½ğ‘…
output : Decomposed matrices Uğ‘‡;ğ‘˜2Rğ¼ğ‘˜ğ‘…and
Sğ‘‡;ğ‘˜2Rğ‘…ğ‘…for allğ‘˜, andVğ‘‡
parameters: Target rank ğ‘…
1Randomly initialize Hğ‘‡2Rğ‘…ğ‘…, andSğ‘‡;ğ‘˜for allğ‘˜;
2Initialize Vğ‘‡withVmeta;
3repeat
4 UpdatefQğ‘‡gÂ•Hğ‘‡Â•Wğ‘‡Â•Vğ‘‡using Equations (3)-(7) ;
5 Vğ‘‡ Â¹Vğ‘‡Â¸VmetaÂºÂ2; // residual connection
6until the maximum iteration is reached ;
7fork=1,Â”Â”Â”, Kdo
8 Sğ‘‡;ğ‘˜ diag(Wğ‘‡Â¹ğ‘˜Â•:Âº);
9 Uğ‘‡;ğ‘˜ Qğ‘‡;ğ‘˜Hğ‘‡;
10end
T/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc 3.2. The meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in Equation (14)
is expressed in terms of V0ğ‘ that is computed using the ALS iterations:
Vmeta VmetaÂ¸ğ¶âˆ‘ï¸
ğ‘ 2SÂ¹V0
ğ‘  VmetaÂºÂ” (20)
ğ¶=Â¹1 2ğœ‚ğ¾Âº2ÂÂ¹2Â¹1 ğœ‚ğ¾ÂºÂºis a constant with learning rate ğœ‚and
the numberğ¾of slices in each domain (see Appendix A.3 for proof).
We chooseğœ‚that satises ğ¶=1ÂjSj. As a result, the meta-update
step of Vmeta is simply expressed by the mean of V0ğ‘ for all source
domainsğ‘ 2Sas follows:
Vmeta VmetaÂ¸1
jSjâˆ‘ï¸
ğ‘ 2SÂ¹V0
ğ‘  VmetaÂº=1
jSjâˆ‘ï¸
ğ‘ 2SV0
ğ‘ Â” (21)
Note that we express the meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in Equa-
tion (14)using only V0ğ‘ , which is computed by performing ALS
iterations. We learn Vmeta from the source tensors by repeatedly
performing the source adaptation step in Equation (16)and the
meta-update step in Equation (21).
3.3 Inference on Target Domain
M/e.sc/t.sc/a.sc/hyphen.scP2 analyzes a new tensor Xğ‘‡of the target domain ğ‘‡using the
patterns of source domains. The patterns are represented as a meta
factor Vmeta. The main idea is to initialize the latent factor matrix
Vğ‘‡of target domain ğ‘‡withVmeta, and nd the adapted factor
matrices to the target domain by running a few ALS iterations. The
decomposed results are utilized for performing many downstream
tasks on the target domain including missing value prediction,
anomaly detection, feature extraction, etc.
For performing a downstream task, overtting the latent factors
to the target domain degrades the performance when there are only
a few entries in the target tensor. To prevent forgetting the trained
patterns Vmeta of source domains while decomposing the target
tensor, we exploit a residual connection between Vmeta andVğ‘‡.
When decomposing a new target tensor, abnormal values such
as anomalies and missing entries deteriorate the quality of the de-
composed results. Since a single ALS step of M/e.sc/t.sc/a.sc/hyphen.scP2 reconstructs
 
1387KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
the target tensor accurately using the general information Vmeta of
source domains, M/e.sc/t.sc/a.sc/hyphen.scP2 imputes the abnormal values with the
corresponding reconstructed values after a single ALS step. For
example, when there are anomalies in the target tensor, M/e.sc/t.sc/a.sc/hyphen.scP2
replaces the detected anomalous entries with the reconstructed
values after a single ALS step on the target tensor.
3.4 Theoretical Analysis
We theoretically analyze the time complexity of M/e.sc/t.sc/a.sc/hyphen.scP2. Specif-
ically, we show that the meta-learning-based decomposition of
M/e.sc/t.sc/a.sc/hyphen.scP2 has the same complexity to that of a naive PARAFAC2.
L/e.sc/m.sc/m.sc/a.sc 3.3. The decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on source domains (Al-
gorithm 1) has the same time complexity as that of a naive PARAFAC2
decomposition on the source domains (see Appendix A.4 for proof).
L/e.sc/m.sc/m.sc/a.sc 3.4. Given the meta factor matrix Vmeta, the time complex-
ity of M/e.sc/t.sc/a.sc/hyphen.scP2 for nding latent factors of a target tensor (Algorithm 2)
is the same as that of performing PARAFAC2 decomposition on the
target tensor (see Appendix A.5 for proof).
4 Experiments
We perform experiments to answer the following questions.
Q1.Missing Value Prediction. How accurately does M/e.sc/t.sc/a.sc/hyphen.scP2
predict missing values of a new target tensor using existing
tensors in multiple source domains (Section 4.2)?
Q2.Anomaly Detection. Does M/e.sc/t.sc/a.sc/hyphen.scP2 show superior perfor-
mance than the baselines for detecting anomalies of a new
target tensor exploiting the source tensors (Section 4.3)?
Q3.Domain Adaptation Speed. How fast does M/e.sc/t.sc/a.sc/hyphen.scP2 adapt
to the new target domain (Section 4.4)?
Q4.Ablation Study. Does each step of M/e.sc/t.sc/a.sc/hyphen.scP2 contribute to
the performance of the downstream task (Section 4.5)?
Q5.Case Study. What are the discoveries of analyzing real world
data using M/e.sc/t.sc/a.sc/hyphen.scP2 (Section 4.6)?
4.1 Experimental Settings
We present our experimental setup including datasets and baselines.
All experiments are done with a single GTX 3080.
Datasets. We use ve real-world datasets from two distinct
elds: stock and human action. We summarize the statistics of
datasets in Table 2.
Nasdaq, S&P500, and Korea stock are stock datasets which con-
tain selected features of daily stock prices in Nasdaq, S&P 500, and
Korea stock market, respectively. We use 1) the opening, closing,
highest, and lowest prices, and 2) trading volume of the day as
features. Stocks are grouped into sectors, and we treat their sectors
as domains. This is because the statistics of stocks, such as price
or trading volume, are similar within each sector, but dier among
sectors. In each sector, the data are in the form of (day, feature,
stock) ; a stock corresponds to a slice matrix of size (day, feature).
NATOPS and Cricket are human activity datasets measured with
various sensors. NATOPS is a collection of 20 people repeatedly
performing 24 aircraft handling actions. We treat the people as
domains since each person has dierent physical conditions and
unique habits. For each person, the actions are in the form of (time-
frame, sensor, action) ; an action corresponds to a slice matrix whoseTable 2: Summary of datasets.
Name #
Domain Max ğ¼ğ‘˜Â¹ğ½Â•ğ¾Âº# Non-zero Type
Nasdaq111
12,709 (6, 11) 2,742K stock
S&P500111
13,321 (6, 13) 7,318K stock
Korea stock211
3,089 (6, 10) 2,038K stock
NATOPS320
2,009 (77, 24) 42,720K HAR
Cricket412
1,197 (6, 10) 1,292K HAR
1https://kaggle.com/datasets/paultimothymooney/stock-market-data
2https://github.com/jungijang/KoreaStockData
3https://github.com/yalesong/natops
4https://timeseriesclassication.com
form is (timeframe, sensor). Cricket is a collection of four umpires
performing twelve signals in the cricket game, each with ten rep-
etitions. We consider each signal type, such as No Ball or Dead
Ball, as a domain for the Cricket dataset. This is because the sensor
values are dierent depending on the type of signal due to the
dierences in the type of movements involved. For each type of
signal, the signals are in the form of (timeframe, sensor, person) ; a
person corresponds to a slice matrix, and its form is (timeframe,
sensor).
If the number of slices in a specic domain is extremely large, the
decomposed results that do not consider the diversity of domains
are biased towards the domain. This is unfair to the competitors
including PARAFAC2 and PARAFAC2-I. Thus, we randomly sample
ğ¾slices for each domain where ğ¾is the number of slices of the
domain with the least slices. In stock datasets, scales vary largely
across the columns; e.g., scales of the price and trading volume are
dierent. Thus, we perform L2 normalization for each ğ‘—-th column
XÂ¹:Â•ğ‘—Âºof a slice matrix X:XÂ¹ğ‘–Â•ğ‘—Âº=XÂ¹ğ‘–Â•ğ‘—ÂºÂkXÂ¹:Â•ğ‘—Âºk2.
Baselines. M/e.sc/t.sc/a.sc/hyphen.scP2 aims to decompose the target tensor quickly
and accurately utilizing the information of given source tensors.
Thus, we compare M/e.sc/t.sc/a.sc/hyphen.scP2 with previous approaches for irregular
tensor decomposition. We do not compare M/e.sc/t.sc/a.sc/hyphen.scP2 with COPA [ 1]
and SPARTan [ 31] that aim to improve the scalability of the original
PARAFAC2 since our objective is to nd accurate latent factors of
the target tensor with limited number of slices.
PARAFAC2 [ 14] performs PARAFAC2 decomposition on the
whole domains including source and target. The method optimizes
the latent factor matrices based on ALS approach. PARAFAC2-T
performs PARAFAC2 decomposition on the target domain only.
PARAFAC2-I initializes the factor matrix Vof the target domain
with the result of a naive PARAFAC2 decomposition on source
domains. AO-ADMM [ 33] is a recent state-of-the-art decomposition
method for irregular tensors. AO-ADMM employs an alternating
optimization (AO) approach in conjunction with the alternating
direction method of multipliers (ADMM) for tting PARAFAC2.
Evaluation and Seings. To evaluate the quality of decom-
posed results of M/e.sc/t.sc/a.sc/hyphen.scP2, we perform two downstream tasks: miss-
ing value prediction and anomaly detection. For each dataset, we set
the last domain (in alphabetical order) as the unseen target domain,
and the rest as source domains. For example, in stock datasets, we
align sectors alphabetically and use the last sector, Utilities in our
case, as the target domain. We randomly split the data of the target
domain into training and test missing (anomalous) entries with the
missing (anomaly) ratio. In the missing value prediction task, we
 
1388Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 3: The missing value prediction error of M/e.sc/t.sc/a.sc/hyphen.scP2 and baselines. The lowest errors are in bold. M/e.sc/t.sc/a.sc/hyphen.scP2 shows the best
prediction performance in all cases.
Missing
Value Prediction
Mo
delsMissing
ratioğ‘Ÿğ‘š= 0.1 Missing
ratioğ‘Ÿğ‘š= 0.3
Nasdaq
S&P500 Korea stock NATOPS Cricket Nasdaq
S&P500 Korea stock NATOPS Cricket
P
AFARAC2 0.31490.0412
0.35510.0360 0.40850.0095 0.38810.0157 1.34690.1304 0.49980.0593
0.49480.0608 0.48940.0207 0.61720.0081 1.13920.0602
PARAFAC2-I 0.99050.0033
1.00750.0035 0.99330.0024 0.81190.0041 1.02030.0056 1.00540.0014
1.00350.0006 1.00870.0015 0.84980.0019 1.02120.0050
PARAFAC2-T 0.99020.0025
1.00940.0012 0.99740.0015 0.82430.0026 1.02350.0041 1.00970.0020
1.00410.0004 1.00860.0022 0.85580.0013 1.02180.0032
AO-ADMM 0.52630.0038
0.37610.0019 0.46450.0134 0.90810.0268 0.76300.0649 0.59440.0033
0.42460.0052 0.61290.0097 1.32700.0235 1.07670.0071
M/e.sc/t.sc
/a.sc/hyphen.scP2 (proposed) 0.28840.0287
0.32370.0278 0.38490.0260 0.29070.0042 0.90240.0627 0.38960.0279
0.38520.0295 0.46660.0228 0.38900.0044 0.90780.0335
Table 4: Anomaly detection performance of M/e.sc/t.sc/a.sc/hyphen.scP2 and baselines in terms of F1 score. Bold numbers denote the best
performance. M/e.sc/t.sc/a.sc/hyphen.scP2 shows the best detection performance among the methods in most of the cases.
Anomaly
Detection
Mo
delsAnomaly
ratioğ‘Ÿğ‘= 0.05 Anomaly
ratioğ‘Ÿğ‘= 0.1
Nasdaq
S&P500 Korea stock NATOPS Cricket Nasdaq
S&P500 Korea stock NATOPS Cricket
P
AFARAC2 0.00690.0002
0.00410.0002 0.00120.0005 0.02970.0001 0.40940.0203 0.01450.0004
0.00660.0004 0.00130.0006 0.06480.0006 0.47420.0137
PARAFAC2-I 0.24000.0199
0.24820.0719 0.45940.0530 0.23650.0036 0.08670.0150 0.21670.0222
0.19850.0439 0.38300.0532 0.25100.0019 0.12990.0112
PARAFAC2-T 0.23010.0397
0.19200.0348 0.40170.0613 0.22560.0033 0.07580.0096 0.20960.0233
0.16910.0177 0.32550.0462 0.23540.0031 0.12800.0104
AO-ADMM 0.12930.0061
0.35910.0162 0.51000.0118 0.10890.0052 0.01230.0021 0.19720.0066
0.40490.0060 0.54000.0122 0.11610.0032 0.03240.0024
M/e.sc/t.sc
/a.sc/hyphen.scP2 (proposed) 0.35080.0097
0.46560.0121 0.59590.0165 0.29600.0017 0.07520.0094 0.39120.0092
0.49930.0107 0.62820.0133 0.32980.0009 0.12920.0085
use the normalized error as the evaluation metric; given an original
tensor Xand the reconstructed one Ë†X, the normalized error ğ‘Ÿerris:
ğ‘Ÿerr=Ãğ¾
ğ‘˜=1Ã
Â¹ğ‘–Â•ğ‘—Âº2Î©ğ‘˜kXğ‘˜Â¹ğ‘–Â•ğ‘—Âº Ë†Xğ‘˜Â¹ğ‘–Â•ğ‘—Âºk2
ğ¹Ãğ¾
ğ‘˜=1Ã
Â¹ğ‘–Â•ğ‘—Âº2Î©ğ‘˜kXğ‘˜Â¹ğ‘–Â•ğ‘—Âºk2
ğ¹(22)
where Xğ‘˜is theğ‘˜-th frontal slice, Ë†Xğ‘˜is the reconstructed one
ofXğ‘˜with target rank ğ‘…, and Î©ğ‘˜is the set of test entries of the
ğ‘˜-th slice matrix. In the anomaly detection task, we use the F1
score as the evaluation metric. For each dataset, we set the target
rankğ‘…as the minimum mode length of the target tensor, and the
maximum number of iterations as 10. We run all experiments ten
times with random seeds, and report the average and standard
deviation removing the highest and the lowest results.
4.2 Missing Value Prediction (Q1)
We present the missing value prediction errors of M/e.sc/t.sc/a.sc/hyphen.scP2 and
baselines in Table 3. M/e.sc/t.sc/a.sc/hyphen.scP2 consistently outperforms competi-
tors in various scenarios, achieving the lowest reconstruction error
across dierent missing ratios ğ‘Ÿğ‘š2f0Â”1Â•0Â”3gÂ”This indicates that
it is important to carefully use the information of source domains
when decomposing the target domain. Furthermore, PARAFAC2-T
which focuses solely on the new target tensor presents lower accu-
racy than other methods including PARAFAC2 and PARAFAC2-I.
This is because solely reconstructing the target tensor with lim-
ited information incurs overtting problem, showing inaccurate
prediction for the missing entries.
4.3 Anomaly Detection (Q2)
To evaluate the anomaly detection performance of M/e.sc/t.sc/a.sc/hyphen.scP2, we
randomly select anomalous entries in the target tensor Xğ‘‡with
the anomaly ratio ğ‘Ÿğ‘2f0Â”05Â•0Â”1g. Then we change the values of
the entries by dividing them with a constant ğœ“=2, to make them
anomalies. We detect ğ¾anomalies by nding top- ğ¾entries with
the highest error between the anomaly-injected tensor and our
reconstructed one.Table 4 presents that M/e.sc/t.sc/a.sc/hyphen.scP2 gives the highest F1 score with
large margin in most of the cases. This indicates that M/e.sc/t.sc/a.sc/hyphen.scP2 suc-
cessfully discovers the factor matrices that contribute to accurately
detecting anomalies in a new target tensor.
In Cricket, PARAFAC2 shows better performance than M/e.sc/t.sc/a.sc/hyphen.sc
P2. The main reason is that Cricket has extremely small number
of entries in the target domain, with excessively small number of
anomalies. Thus, PARAFAC2 easily neglects the anomalous entries
while reconstructing the tensor of entire domains, showing higher
reconstruction performance on those anomaly values. However,
PARAFAC2 completely fails to detect anomalies in other datasets,
and does not perform well for missing value prediction task with
markedly slower speed (see Figure 4).
4.4 Domain Adaptation Speed (Q3)
M/e.sc/t.sc/a.sc/hyphen.scP2 initializes the latent factor matrix Vğ‘‡of the target do-
mainğ‘‡with the meta factor matrix Vmeta. To show that Vmeta
adapts to the target domain quickly and eectively, we compare
the missing value prediction performance of M/e.sc/t.sc/a.sc/hyphen.scP2 with dier-
ently initialized Vğ‘‡. PARAFAC2-initialized method initializes Vğ‘‡
with the decomposed result of naive PARAFAC2 decomposition
on the source domains. Random-initialized method initializes Vğ‘‡
randomly. We adopt the imputation process of missing entries and
the residual connection of M/e.sc/t.sc/a.sc/hyphen.scP2 to those baselines. We report
the performance of the methods over the ALS iterations in Figure 3.
We also show the runtime of them per iteration in Figure 4.
Figure 3 shows that M/e.sc/t.sc/a.sc/hyphen.scP2 eectively reduces the prediction
error with only a few ALS iterations where the runtime per ALS
iteration is the same for all initializing methods (see Figure 4).
The prediction errors of other methods are generally monotonous
and poor compared to M/e.sc/t.sc/a.sc/hyphen.scP2. This shows that initializing Vğ‘‡
withVmeta assists in discovering accurate latent factor matrices of
the target tensor quickly. Moreover, PARAFAC2-initialized consis-
tently shows poor performance. This implies that simply tuning
the decomposed results to a new target domain cannot consider
the dierence between source and target domains.
 
1389KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
Rapid errordropRapid errordroplarge error dropSmall dropError sustainsSlow dropError increases
Figure 3: The missing value prediction performance over the ALS iterations. We compare M/e.sc/t.sc/a.sc/hyphen.scP2 with two other initialization
approaches of Vğ‘‡for a target domain ğ‘‡. Note that the error of M/e.sc/t.sc/a.sc/hyphen.scP2 drops rapidly, achieving the best prediction performance
among the initialization methods. The errors of other baselines drop slowly, or even increase in Cricket.
2.34Ã—2.67Ã—3.49Ã—1.64Ã—3.87Ã—PARAFAC2 is markedly slowSimilar runtime betweenthe initializing methods
Figure 4: Running time for each ALS iteration on a target
domain. Note that M/e.sc/t.sc/a.sc/hyphen.scP2 achieves up to 3Â”87faster speed
than PARAFAC2 which decomposes the entire tensors in
multiple domains whenever a new target tensor emerges.
Meta-P2 shows similar runtime to other initializing methods.
4.5 Ablation Study (Q4)
We provide an ablation study on the missing value prediction with
respect to the eectiveness of each module in M/e.sc/t.sc/a.sc/hyphen.scP2. M/e.sc/t.sc/a.sc/hyphen.scP2-
meta is M/e.sc/t.sc/a.sc/hyphen.scP2 without the meta-learning-based decomposition
on the source domains; it performs a naive PARAFAC2 decomposi-
tion instead of meta-learning-based decomposition on the source
domains, and use the naively decomposed result when adapting to
a new target domain. M/e.sc/t.sc/a.sc/hyphen.scP2-imp and M/e.sc/t.sc/a.sc/hyphen.scP2-res are M/e.sc/t.sc/a.sc/hyphen.scP2
without considering the missing values by imputation, and M/e.sc/t.sc/a.sc/hyphen.scP2
without the residual connection of the meta factor, respectively.
Table 5 shows that each module of M/e.sc/t.sc/a.sc/hyphen.scP2 eectively reduces
the prediction error. Specically, M/e.sc/t.sc/a.sc/hyphen.scP2-meta shows poor per-
formance compared to the proposed M/e.sc/t.sc/a.sc/hyphen.scP2 in every case. This
indicates that the meta-learning-based decomposition on source
domains eectively enhances the quality of decomposition result
of a new target domain.
4.6 Case Study (Q5)
We showed that M/e.sc/t.sc/a.sc/hyphen.scP2 successfully detects or predicts the abnor-
mal values through Sections 4.2 and 4.3. Our further question is, how
is an anomalous point related to a real-world event? To answer this,
we present discoveries of analyzing historical prices of S&P500. WeTable 5: Ablation study on missing value prediction problem.
The lowest errors are in bold. Note that each module of M/e.sc/t.sc/a.sc/hyphen.sc
P2improves the performance of missing value prediction.
Missing
Value Prediction (missing ratio ğ‘Ÿğ‘š= 0.5)
Datasets MTD-imp
MTD-res MTD-meta M/e.sc/t.sc
/a.sc/hyphen.scP2 (proposed)
Nasdaq 0.92400.0093
0.70790.0433 0.65260.0396 0.51020.0261
SP500 0.87210.0152
0.66710.0302 0.60530.0209 0.47120.0251
K
orea stock 0.91000.0274
0.72420.0401 0.65300.0417 0.55950.0172
NA
TOPS 0.78870.0012
0.61050.0024 0.57280.0055 0.50970.0042
Cricket 1.01190.0045
0.97020.0262 0.97630.0161 0.92970.0205
compose a new target domain by selecting ten largest AI-themed
stocks within S&P500 dataset (see Figure 1), while keeping other
settings consistent with those described in Section 4.1. We detect
ve abnormal days by nding the days with the top-5 largest errors
between the true stock features and the reconstructed ones.
Figure 5 compares the abnormal days detected by M/e.sc/t.sc/a.sc/hyphen.scP2 and
PARAFAC2-T in IBM and Netix. We observe that the detected days
ofM/e.sc/t.sc/a.sc/hyphen.scP2 are closely related to real-world events, showing its
ecacy in interpreting anomalies. We also note that the detected
points of PARAFAC2-T are densely distributed compared to M/e.sc/t.sc/a.sc/hyphen.sc
P2, failing to detect several real-world events that M/e.sc/t.sc/a.sc/hyphen.scP2 has
successfully identied.
5 Related Works
We review previous PARAFAC2 decomposition approaches for ir-
regular tensors and meta learning methods.
Tensor decomposition. Tensor decomposition methods are ap-
plied to many real-world tasks including model compression [ 32,
38,41,45], anomaly detection [ 9,27,37,47], forecasting [ 8,26,35],
answering time range queries [ 2,17,18], and missing value pre-
diction [ 25]. However, those decomposition methods for regular
tensors assume that every dimension has the same length across
the slices, which is not true for irregular tensors. Recently, neural
tensor factorization methods have been actively studied in recom-
mender systems [ 4,42]. The main drawback of the neural tensor
models compared to ALS-based tensor decomposition methods is its
computational complexity. Neural tensor models involve training a
neural network, which can be computationally intensive, especially
for large datasets and complex architectures.
 
1390Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
S&P 500 hits record highsQ2â€™ 20 reportCOVID19New subscriber count report
PARAFAC2-T cannot detect COVID19Cannot detectlarge dropCOVID192019 annualreportQ2â€™ 19 report2020 annualreportPARAFAC2-T does notdetect the events(a) Meta-P2 (IBM)(b) Meta-P2 (Netflix)
(c) PARAFAC2 (IBM)(d) PARAFAC2 (Netflix)
Figure 5: Top-5 anomaly detection results of M/e.sc/t.sc/a.sc/hyphen.scP2 on (a) IBM and (b) Netix, and PARAFAC2-T on (c) IBM and (d) Netix.
Note that the detected anomalies (red circles) of M/e.sc/t.sc/a.sc/hyphen.scP2 are closely related to real-world events during the corresponding
periods. PARAFAC2-T detect densely distributed abnormal points, leading to the oversight of various real-world events.
PARAFAC2 decomposition. Many works have been studied to
nd patterns of irregular tensors based on PARAFAC2 [ 6,29,30,40,
43]. DPar2 [ 19,20] is a preprocessing algorithm for decomposing
irregular dense tensors. The method eectively compresses each
slice matrix of a given irregular tensor by careful reordering of com-
putations. SPARTan [ 31] is a scalable PARAFAC2 decomposition
algorithm for irregular sparse tensors. COPA [ 1] is an extension of
SPARTan, which improves the performance of it with additional
constraints. To nd the latent factors in large streaming tensors,
SPADE [ 12] denes a non-temporal factor and processes the newly
incoming part using the accelerated MTTKRP [ 31]. Dash [ 21] ef-
ciently nds the latent factors by avoiding naive computations
involved with old data when new data are given. However, those
methods have limitations in decomposing a new target tensor using
given tensors in multiple domains since they assume all tensors are
in the same domain.
Various task-specic irregular tensor decomposition methods
have been studied to improve the performance of their target tasks.
To predict missing entries in irregular tensors, Atom [ 22] reformu-
lates the objective function to completely exclude missing values,
ensuring more accurate predictions. LogPar [ 44] addresses the miss-
ing values by modeling the irregular tensor with a Bernoulli dis-
tribution, parameterized by an underlying real-valued tensor, and
approximates this tensor using a positive-unlabeled learning loss
function. However, those task-specic methods (e.g., missing value
prediction) are applicable only to specic tensors (e.g., tensors with
missing values). Meta-P2 is a task-agnostic decomposition method
(general framework) that can be applied to many downstream tasks.
Meta-learning. Many meta-learning methods have been pro-
posed to deal with the problem of limited data in many machine
learning areas [ 11,15,34]. FOMAML (rst-order MAML) [ 28] is
a variation of MAML that uses rst-order gradient information
to update the modelâ€™s parameters. However, the previous works
on meta-learning are not directly applicable for PARAFAC2 since
they update model parameters based on gradient descent update
whereas the decomposition methods update them through CP-ALS.Applications of meta-learning. Previous works have used
the meta-learning algorithm to solve few show learning problems.
Chien et al. [ 7] employed meta-learning algorithm for hyperpa-
rameter optimization in dialogue system. Gupta et al. [ 13] used
unsupervised meta-learning algorithm for reinforcement learning.
Lian et al. [ 23] applied meta-learning algorithm to neural archi-
tecture search to learn a meta-architecture that is able to adapt to
a new task quickly through a few gradient steps. To the best of
our knowledge, M/e.sc/t.sc/a.sc/hyphen.scP2 is the rst domain adaptation method for
decomposing irregular tensors based on meta-learning approach.
6 Conclusion
We propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain adaptation method
for decomposing irregular tensors. M/e.sc/t.sc/a.sc/hyphen.scP2 obtains a meta factor
matrix from existing tensors in multiple source domains, and the
meta factor helps nd accurate patterns of a new target tensor
quickly. This is achieved by training the meta factor to possess
two key properties: 1) generality by sharing the meta factor along
the source domains, and 2) easy adaptability by ALS-based meta-
learning. Extensive experiments on real-world datasets show that
M/e.sc/t.sc/a.sc/hyphen.scP2 achieves the best performance in missing value prediction
and anomaly detection problems. Future works include extending
M/e.sc/t.sc/a.sc/hyphen.scP2 for other decomposition methods beyond PARAFAC2.
Acknowledgments
This work was supported by the National Research Foundation
of Korea (NRF) funded by MSIT (2022R1A2C3007921). This work
was also supported by Institute of Information & communications
Technology Planning & Evaluation (IITP) grant funded by the Ko-
rea government (MSIT) [No.2022-0-00641, XVoice: Multi-Modal
Voice Meta Learning], [No.RS-2021-II211343, Articial Intelligence
Graduate School Program (Seoul National University)], and [No.RS-
2021-II212068, Articial Intelligence Innovation Hub (Articial In-
telligence Institute, Seoul National University)]. U Kang is the cor-
responding author.
 
1391KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
References
[1]Ardavan Afshar, Ioakeim Perros, Evangelos E. Papalexakis, Elizabeth Searles,
Joyce C. Ho, and Jimeng Sun. 2018. COPA: Constrained PARAFAC2 for Sparse &
Large Datasets. In CIKM.
[2]Dawon Ahn, Jun-Gi Jang, and U Kang. 2022. Time-aware tensor decomposition
for sparse tensors. Mach. Learn. (2022).
[3]Dawon Ahn, Seyun Kim, and U Kang. 2021. Accurate Online Tensor Factorization
for Temporal Tensor Streams with Missing Values. In CIKM.
[4]Huiyuan Chen and Jing Li. 2020. Neural tensor model for learning multi-aspect
factors in recommender systems. In IJCAI.
[5]Peter A. Chew, Brett W. Bader, Stephen Helmreich, Ahmed Abdelali, and Stephen J.
Verzi. 2011. An information-theoretic, vector-space-model approach to cross-
language information retrieval. Nat. Lang. Eng. 17, 1 (2011), 37â€“70.
[6]Peter A. Chew, Brett W. Bader, Tamara G. Kolda, and Ahmed Abdelali. 2007.
Cross-language information retrieval using PARAFAC2. In SIGKDD. ACM.
[7]Jen-Tzung Chien and Wei Xiang Lieow. 2019. Meta Learning for Hyperparameter
Optimization in Dialogue System. In Interspeech.
[8]Beyza Ermis, Evrim Acar, and Ali Taylan Cemgil. 2015. Link prediction in
heterogeneous data via generalized coupled tensor factorization. Data Min.
Knowl. Discov. (2015).
[9]Hadi Fanaee-T and JoÃ£o Gama. 2016. Tensor-based anomaly detection: An
interdisciplinary survey. Knowl. Based Syst. (2016).
[10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-
Learning for Fast Adaptation of Deep Networks. In ICML.
[11] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. 2019. Online
meta-learning. In ICML.
[12] Ekta Gujral, Georgios Theocharous, and Evangelos E. Papalexakis. 2020. SPADE:
Streaming PARAFAC2 DEcomposition for Large Datasets. In SDM.
[13] Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. 2018.
Unsupervised Meta-Learning for Reinforcement Learning. CoRR (2018).
[14] R. A. Harshman. 1972. PARAFAC2: Mathematical and Technical Notes. In UCLA
Working Papers in Phonetics.
[15] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey.
2022. Meta-Learning in Neural Networks: A Survey. IEEE Trans. Pattern Anal.
Mach. Intell. (2022).
[16] Muhammad Abdullah Jamal and Guo-Jun Qi. 2019. Task Agnostic Meta-Learning
for Few-Shot Learning. In CVPR.
[17] Jun-Gi Jang, Dongjin Choi, Jinhong Jung, and U Kang. 2018. Zoom-SVD: Fast
and Memory Ecient Method for Extracting Key Patterns in an Arbitrary Time
Range. In CIKM.
[18] Jun-Gi Jang and U Kang. 2021. Fast and Memory-Ecient Tucker Decomposition
for Answering Diverse Time Range Queries. In KDD.
[19] Jun-Gi Jang and U Kang. 2022. DPar2: Fast and Scalable PARAFAC2 Decomposi-
tion for Irregular Dense Tensors. In ICDE.
[20] Jun-Gi Jang and U Kang. 2023. Static and Streaming Tucker Decomposition for
Dense Tensors. ACM Trans. Knowl. Discov. Data 17, 5 (2023), 66:1â€“66:34.
[21] Jun-Gi Jang, Jeongyoung Lee, Yong-chan Park, and U Kang. 2023. Fast and
Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors - Algorithm and
Application. In KDD.
[22] Jun-Gi Jang, Jeongyoung Lee, Jiwon Park, and U Kang. 2022. Accurate PARAFAC2
decomposition for temporal irregular tensors with missing values. In Big Data.
[23] Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Jun-
zhou Huang, and Shenghua Gao. 2020. Towards Fast Adaptation of Neural
Architectures with Meta Learning. In ICLR.
[24] Yu-Ru Lin, Jimeng Sun, Paul C. Castro, Ravi B. Konuru, Hari Sundaram, and
Aisling Kelliher. 2009. MetaFac: community discovery via relational hypergraph
factorization. In SIGKDD.
[25] Hanpeng Liu, Yaguang Li, Michael Tsang, and Yan Liu. 2019. Costco: A neural
tensor completion model for sparse tensors. In SIGKDD.
[26] Xiaoyang Ma, Lan Zhang, Lan Xu, Zhicheng Liu, Ge Chen, Zhili Xiao, Yang Wang,
and Zhengtao Wu. 2019. Large-scale User Visits Understanding and Forecasting
with Deep Spatial-Temporal Tensor Factorization Framework. In SIGKDD.
[27] Maxwell McNeil, Carolina Mattsson, Frank W Takes, and Petko Bogdanov. 2023.
CADENCE: Community-Aware Detection of Dynamic Network States. In SDM.
[28] Alex Nichol, Joshua Achiam, and John Schulman. 2018. On First-Order Meta-
Learning Algorithms. CoRR (2018).
[29] Yannis Panagakis and Constantine Kotropoulos. 2011. Automatic music tagging
via PARAFAC2. In ICASSP. IEEE, 481â€“484.
[30] Evangelia Pantraki and Constantine Kotropoulos. 2015. Automatic image tagging
and recommendation via PARAFAC2. In MLSP.
[31] Ioakeim Perros, Evangelos E. Papalexakis, Fei Wang, Richard W. Vuduc, Elizabeth
Searles, Michael Thompson, and Jimeng Sun. 2017. SPARTan: Scalable PARAFAC2
for Large & Sparse Data. In SIGKDD.
[32] Anh Huy Phan, Konstantin Sobolev, Konstantin Sozykin, Dmitry Ermilov, Julia
Gusak, Petr TichavskÃ½, Valeriy Glukhov, Ivan V. Oseledets, and Andrzej Cichocki.
2020. Stable Low-Rank Tensor Decomposition for Compression of Convolutional
Neural Network. In ECCV.[33] Marie Roald, Carla Schenker, Vince D Calhoun, Tulay Adali, Rasmus Bro,
Jeremy E Cohen, and Evrim Acar. 2022. An AO-ADMM approach to constraining
PARAFAC2 on all modes. SIAM Journal on Mathematics of Data Science (2022).
[34] Nicolas Schweighofer and Kenji Doya. 2003. Meta-learning in Reinforcement
Learning. Neural Networks (2003).
[35] Alessandro Spelta. 2017. Financial market predictability with tensor decomposi-
tion and links forecast. Appl. Netw. Sci. 2 (2017), 7.
[36] Stephan Spiegel, Jan Hendrik Clausen, Sahin Albayrak, and JÃ©rÃ´me Kunegis. 2011.
Link Prediction on Evolving Data Using Tensor Factorization. In PAKDD.
[37] Leo Tisljaric, Soa Fernandes, Tonci Caric, and JoÃ£o Gama. 2020. Spatiotemporal
Trac Anomaly Detection on Urban Road Network Using Tensor Decomposition
Method. In Discovery Science - 23rd International Conference.
[38] Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. 2018. Tensor Decomposi-
tion for Compressing Recurrent Neural Network. In IJCNN.
[39] Ricardo Vilalta and Youssef Drissi. 2002. A Perspective View and Survey of
Meta-Learning. Artif. Intell. Rev. (2002).
[40] Barry M Wise, Neal B Gallagher, and Elaine B Martin. 2001. Application of
PARAFAC2 to fault detection and diagnosis in semiconductor etch. Journal of
Chemometrics: A Journal of the Chemometrics Society (2001).
[41] Bijiao Wu, Dingheng Wang, Guangshe Zhao, Lei Deng, and Guoqi Li. 2020.
Hybrid tensor decomposition in neural network compression. Neural Networks
132 (2020), 309â€“320.
[42] Xian Wu, Baoxu Shi, Yuxiao Dong, Chao Huang, and Nitesh V Chawla. 2019.
Neural tensor factorization for temporal interaction learning. In WSDM.
[43] Kejing Yin, Ardavan Afshar, Joyce C. Ho, William K. Cheung, Chao Zhang, and
Jimeng Sun. 2020. LogPar: Logistic PARAFAC2 Factorization for Temporal Binary
Data with Missing Values. In SIGKDD.
[44] Kejing Yin, Ardavan Afshar, Joyce C Ho, William K Cheung, Chao Zhang, and
Jimeng Sun. 2020. LogPar: Logistic PARAFAC2 factorization for temporal binary
data with missing values. In SIGKDD.
[45] Miao Yin, Siyu Liao, Xiao-Yang Liu, Xiaodong Wang, and Bo Yuan. 2020. Com-
pressing Recurrent Neural Networks Using Hierarchical Tucker Tensor Decom-
position. CoRR abs/2005.04366 (2020).
[46] Dave Zachariah, Martin Sundin, Magnus Jansson, and Saikat Chatterjee. 2012.
Alternating Least-Squares for Low-Rank Matrix Reconstruction. IEEE Signal
Process. Lett. 19, 4 (2012), 231â€“234.
[47] Xing Zhang, Gongjian Wen, and Wei Dai. 2016. A Tensor Decomposition-Based
Anomaly Detection Algorithm for Hyperspectral Image. IEEE Trans. Geosci.
Remote. Sens. (2016).
A Appendix
A.1 Relation of Equation (12)and Equation (10)
L/e.sc/m.sc/m.sc/a.sc A.1. Suppose we are given a loss function LÂ¹fXğ‘ g;VmetaÂº=Ãğ¾
ğ‘˜=1kXğ‘ ;ğ‘˜ Uğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜V>
metak2
ğ¹. Then nding optimal Vmeta by mini-
mizingLÂ¹fXğ‘ g;VmetaÂºis equivalent to nding it by minimizing the
following loss function:
Lğ‘‰Â¹fXğ‘ g;VmetaÂº=ğ¾âˆ‘ï¸
ğ‘˜=1kEğ‘ ;ğ‘˜Xğ‘ ;ğ‘˜ V>
metak2
ğ¹Â” (23)
where Eğ‘ ;ğ‘˜=ğ¾Â¹Ãğ¾
ğ‘—=1S>
ğ‘ ;ğ‘—U>
ğ‘ ;ğ‘—Uğ‘ ;ğ‘—Sğ‘ ;ğ‘—Âº 1S>
ğ‘ ;ğ‘˜U>
ğ‘ ;ğ‘˜.
P/r.sc/o.sc/o.sc/f.sc. To nd optimal Vmeta that minimizes a given loss, we
dierentiate the loss in terms of Vmeta and nd Vmeta where the
derivative equals zero.
We rst nd Vmetathat minimizes the original loss LÂ¹fXğ‘ g;VmetaÂº:
rVmetaLÂ¹fXğ‘ g;VmetaÂº (24)
= 2ğ¾âˆ‘ï¸
ğ‘˜=1Â¹X>
ğ‘ ;ğ‘˜ VmetaS>
ğ‘ ;ğ‘˜U>
ğ‘ ;ğ‘˜ÂºUğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜=0 (25)
,Vmetağ¾âˆ‘ï¸
ğ‘˜=1S>
ğ‘ ;ğ‘˜U>
ğ‘ ;ğ‘˜Uğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜=ğ¾âˆ‘ï¸
ğ‘˜=1X>
ğ‘ ;ğ‘˜Uğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜ (26)
,Vmeta=ğ¾âˆ‘ï¸
ğ‘˜=1X>
ğ‘ ;ğ‘˜Uğ‘ ;ğ‘˜Sğ‘ ;ğ‘˜Â¹ğ¾âˆ‘ï¸
ğ‘—=1S>
ğ‘ ;ğ‘—U>
ğ‘ ;ğ‘—Uğ‘ ;ğ‘—Sğ‘ ;ğ‘—Âº 1(27)
 
1392Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
We then discover Vmeta that minimizes the reformulated loss
Lğ‘‰Â¹fXğ‘ g;VmetaÂº:
rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂº (28)
= 2ğ¾âˆ‘ï¸
ğ‘˜=1Â¹X>
ğ‘ ;ğ‘˜E>
ğ‘ ;ğ‘˜ VmetaÂº=0 (29)
,Vmeta=1
ğ¾ğ¾âˆ‘ï¸
ğ‘˜=1X>
ğ‘ ;ğ‘˜E>
ğ‘ ;ğ‘˜Â” (30)
Since Eğ‘ ;ğ‘˜=ğ¾Â¹Ãğ¾
ğ‘—=1S>
ğ‘ ;ğ‘—U>
ğ‘ ;ğ‘—Uğ‘ ;ğ‘—Sğ‘ ;ğ‘—Âº 1S>
ğ‘ ;ğ‘˜U>
ğ‘ ;ğ‘˜, Equation (30)is
equal to Equation (27) which ends the proof.

A.2 Proof of Lemma 3.1
P/r.sc/o.sc/o.sc/f.sc. We denote Eğ‘ ;ğ‘˜Xğ‘ ;ğ‘˜in Equation (12)asZğ‘ ;ğ‘˜for simplic-
ity. Then the loss function Lğ‘‰Â¹fXğ‘ g;VmetaÂºis written as:
Lğ‘‰Â¹fXğ‘ g;VmetaÂº=ğ¾âˆ‘ï¸
ğ‘˜=1kZğ‘ ;ğ‘˜ V>
metak2
ğ¹Â” (31)
The gradientrVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºis computed as:
rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂº=rVmetağ¾âˆ‘ï¸
ğ‘˜=1kZğ‘ ;ğ‘˜ V>
metak2
ğ¹
=2Â¹ğ¾Vmeta ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜ÂºÂ”(32)
The adapted factor matrix Vğ‘ to each source domain ğ‘ is ex-
pressed as follows:
Vğ‘ =Vmeta ğœ‚rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂº
=Â¹1 2ğœ‚ğ¾ÂºVmetaÂ¸2ğœ‚ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜Â”(33)
where the rst equation uses Equation (13)and the second one
is derived from Equation (32). The gradientrVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âºis
expressed in terms of Vmeta:
rVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âº=2Â¹ğ¾Vğ‘  ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜Âº
=2Â¹ğ¾Â¹1 2ğœ‚ğ¾ÂºVmetaÂ¸2ğœ‚ğ¾ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜ ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜Âº
=2Â¹1 2ğœ‚ğ¾ÂºÂ¹ğ¾Vmeta ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜ÂºÂ”(34)
Thus, the sum of two gradients in Equations (32)and(34)is ex-
pressed with Vmeta as the following equation:
rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºÂ¸r Vğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âº=
4Â¹1 ğœ‚ğ¾ÂºÂ¹ğ¾Vmeta ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜ÂºÂ”(35)The gradientrVmetaLğ‘‰Â¹fXğ‘ g;Vğ‘ Âºis computed as follows:
rVmetaLğ‘‰Â¹fXğ‘ g;Vğ‘ Âº=rVmetağ¾âˆ‘ï¸
ğ‘˜=1kZğ‘ ;ğ‘˜ V>
ğ‘ k
=rVmetağ¾âˆ‘ï¸
ğ‘˜=1kZğ‘ ;ğ‘˜ Â¹1 2ğœ‚ğ¾ÂºV>
meta 2ğœ‚ğ¾âˆ‘ï¸
ğ‘™=1Zğ‘ ;ğ‘™k2
ğ¹
=2Â¹1 2ğœ‚ğ¾Âº 
Â¹1 2ğœ‚ğ¾Âºğ¾Vmeta ğ¾âˆ‘ï¸
ğ‘˜=1Â¹Z>
ğ‘ ;ğ‘˜ 2ğœ‚ğ¾âˆ‘ï¸
ğ‘™=1Z>
ğ‘ ;ğ‘™Âº!
=2Â¹1 2ğœ‚ğ¾Âº 
Â¹1 2ğœ‚ğ¾Âºğ¾Vmeta Â¹1 2ğœ‚ğ¾Âºğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜!
=2Â¹1 2ğœ‚ğ¾Âº2Â¹ğ¾Vmeta ğ¾âˆ‘ï¸
ğ‘˜=1Z>
ğ‘ ;ğ‘˜Âº(36)
where the fourth equality usesÃğ¾
ğ‘˜=1Ãğ¾
ğ‘™=1Z>
ğ‘ ;ğ‘™=ğ¾Ãğ¾
ğ‘˜=1Z>
ğ‘ ;ğ‘˜. The
proof is nished by comparing Equations (35) and (36). 
A.3 Proof of Theorem 3.2
P/r.sc/o.sc/o.sc/f.sc. Using Lemma 3.1, the meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in
Equation (14) is expressed as follows:
Vmeta Vmeta ğœ‚rVmetaâˆ‘ï¸
ğ‘ 2SLğ‘‰Â¹fXğ‘ g;Vğ‘ Âº
=Vmeta ğœ‚ğ¶âˆ‘ï¸
ğ‘ 2S rVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºÂ¸r Vğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ ÂºÂ”
(37)
Note thatrVmetaLğ‘‰Â¹fXğ‘ g;VmetaÂºandrVğ‘ Lğ‘‰Â¹fXğ‘ g;Vğ‘ Âºare ex-
pressed byÂ¹Vmeta Vğ‘ ÂºÂğœ‚andÂ¹Vğ‘  V0ğ‘ ÂºÂğœ‚using Equations (13)
and(17), respectively. Then the meta-update step in Equation (37)
is written as follows:
Vmeta Vmeta ğœ‚ğ¶âˆ‘ï¸
ğ‘ 2SÂ¹Â¹Vmeta Vğ‘ ÂºÂğœ‚Â¸Â¹Vğ‘  V0
ğ‘ ÂºÂğœ‚Âº
=VmetaÂ¸ğ¶âˆ‘ï¸
ğ‘ 2SÂ¹V0
ğ‘  VmetaÂº(38)
which ends the proof.

A.4 Proof of Lemma 3.3
P/r.sc/o.sc/o.sc/f.sc. A naive PARAFAC2 decomposition on source domains
takesOÃğ¾
ğ‘˜=1ğ¼ğ‘˜ğ½ğ‘…ğ‘†
time where ğ‘†is the number of source do-
mains, andğ‘…is the target rank.
The meta-learning-based decomposition of MTD on source do-
mains consists of two steps: 1) performing a typical PARAFAC2
decomposition for each source domain ğ‘ 2 S, and 2) updating
the meta factor matrix Vmeta. To update the meta factor matrix
Vmeta,M/e.sc/t.sc/a.sc/hyphen.scP2 rst computes V0ğ‘ for each domain ğ‘ 2 S (see
line 13 of Algorithm 1), and merges them (see line 15 of Algo-
rithm 1). Computing V0ğ‘ of domainğ‘ takesO ğ½ğ¾ğ‘…2time. There-
fore, updating Vmeta by merging V0ğ‘ forğ‘ =1Â•Â”Â”Â”Â•ğ‘† takesO ğ½ğ¾ğ‘…2ğ‘†
time, and the decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on source domains takes
O
Â¹Ãğ¾
ğ‘˜=1ğ¼ğ‘˜ğ½ğ‘…ğ‘†ÂºÂ¸ğ½ğ¾ğ‘…2ğ‘†
time. SinceOÂ¹Ãğ¾
ğ‘˜=1ğ¼ğ‘˜ÂºÂ¡OÂ¹ğ¾ğ‘…Âº, the
meta-learning-based decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on source domains
takesOÂ¹Ãğ¾
ğ‘˜=1ğ¼ğ‘˜ğ½ğ‘…ğ‘†Âºtime, which is the same time complexity
as that of a naive PARAFAC2 decomposition on the source do-
mains. 
 
1393KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
A.5 Proof of Lemma 3.4
P/r.sc/o.sc/o.sc/f.sc. Decomposing a new irregular tensor Xğ‘‡in a target do-
mainğ‘‡consists of two steps: 1) initializing Vğ‘‡withVmeta, and 2)performing PARAFAC2-ALS algorithm. Therefore, the time com-
plexity of M/e.sc/t.sc/a.sc/hyphen.scP2 for decomposing a new target tensor is the same
as that of PARAFAC2-ALS decomposition on the target tensor. 
 
1394