A Novel Feature Space Augmentation Method to Improve
Classification Performance and Evaluation Reliability
Sakhawat Hossain Saimon
sakhawathossain.saimon@utsa.edu
Department of Computer Science
The University of Texas at San
Antonio
San Antonio, Texas, USATanzira Najnin
tanzira.najnin@utsa.edu
Department of Computer Science
The University of Texas at San
Antonio
San Antonio, Texas, USAJianhua Ruan
jianhua.ruan@utsa.edu
Department of Computer Science
The University of Texas at San
Antonio
San Antonio, Texas, USA
ABSTRACT
Classification tasks in many real-world domains are exacerbated
by class imbalance, relatively small sample sizes compared to high
dimensionality, and measurement uncertainty. The problem of class
imbalance has been extensively studied, and data augmentation
methods based on interpolation of minority class instances have
been proposed as a viable solution to mitigate imbalance. It remains
to be seen whether augmentation can be applied to improve the
overall performance while maintaining stability, especially with
a limited number of samples. In this paper, we present a novel
feature-space augmentation technique that can be applied to high-
dimensional data for classification tasks and address these issues.
Our method utilizes uniform random sampling and introduces syn-
thetic instances by taking advantage of the local distributions of
individual features in the observed instances. The core augmenta-
tion algorithm is class-invariant, which opens up an unexplored
avenue of simultaneously improving and stabilizing performance by
augmenting unlabeled instances. The proposed method is evaluated
using a comprehensive performance analysis involving multiple
classifiers and metrics. Comparative analysis with existing feature
space augmentation methods strongly suggests that the proposed
algorithm can result in improved classification performance while
also increasing the overall reliability of the performance evaluation.
CCS CONCEPTS
â€¢Computing methodologies â†’Model development and anal-
ysis; Supervised learning by classification; Learning settings ;
Machine learning approaches.
KEYWORDS
data augmentation, classification, class imbalance, small sample
size, measurement uncertainty
ACM Reference Format:
Sakhawat Hossain Saimon, Tanzira Najnin, and Jianhua Ruan. 2024. A
Novel Feature Space Augmentation Method to Improve Classification Per-
formance and Evaluation Reliability. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.367173625â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671736
1 INTRODUCTION
Classification algorithms that work well in theoretical settings
can falter in the face of peculiarities often observed in real-world
datasets. Many such issues stem from small sample sizes, whether
it is the total number of samples available relative to the high
dimensionality or complexity of the decision function needed, or
the skewed distribution of sample sizes of different classes. In the
presence of these issues, accurate estimation of class boundaries
can be affected unduly by the majority-class instances, uncertainty
in feature values, outlier instances, or simply the lack of instances
to learn complex classification models.
A commonly encountered problem is class imbalance, a phe-
nomenon where one or more classes have a relatively low number
of instances compared to the others. Imbalance affects the classi-
fication accuracy of the minority class and presents a significant
hurdle in classification tasks [ 15,16,18]. The problem of class im-
balance is well-studied, with many different solutions proposed
and evaluated. Resampling, whether random or targeted, has been
investigated as a potential remedy for imbalance [ 19,21,22,30].
Class rebalancing using random under/oversampling is, however,
akin to assigning weights to individual instances and has limited
benefits. A more sophisticated approach is augmentation, which
can be used to artificially inflate the number of instances in a dataset.
One or more realinstances are used to generate additional synthetic
instances that typically inherit the class label of their respective
parents. Synthetic Minority Oversampling Technique (SMOTE) is a
popular augmentation method that generates synthetic instances
along the lines joining the ğ‘˜-nearest neighbors of real instances
[4]. The success of SMOTE has inspired a plethora of synthetic
data generation techniques operating in feature space [ 2,3,5,6,8â€“
11,13,14,24,26,28,29], all of which employ some form of instance
interpolation. SMOTE and its variations are primarily designed
to address imbalance, and in most practical scenarios, synthetic
instances are generated for the minority classes only.
Deep neural networks designed for image and speech classifi-
cation problems have taken an altogether different approach to
augmentation. As model complexity grows, so does the need for
additional training data, even when curated datasets contain mil-
lions of training examples with balanced classes. Augmentation
methods intuitively defined as composite transformations are there-
fore leveraged to grow the number of instances per class. As an
example, color distortions, rotations, and mirroring can be applied
 
2512
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sakhawat Hossain Saimon, Tanzira Najnin, and Jianhua Ruan
to an image to produce a synthetic image that preserves the char-
acteristics and class label of the parent while introducing varia-
tion. Modality-specific transformations can therefore be tailored
to inflate the entire training data, not just the minority class sam-
ples, which can in turn significantly improve performance across
the board [ 1,7,17]. Unfortunately, transformations that are in-
tuitively defined for images and text do not generalize to many
high-dimensional, human-imperceptible data modalities. Sampling
and interpolation-based techniques such as SMOTE can be applied
to these domains, albeit they have not been demonstrated to be
successful beyond the scope of imbalanced datasets.
Learning from small sample sizes can be challenging in gen-
eral, both in the context of training sufficiently complex yet stable
models to properly fit the data, and providing robust and reliable
performance evaluation to select the best model for deployment and
estimate true performance in the field [ 27]. Furthermore, real-world
data often contains noisy features due to measurement uncertainty
[23]. Popular strategies to improve classification performance on
high-dimensional datasets with small sample sizes, such as margin
maximization, feature selection, and sparsity-based regularization,
can be sensitive to outliers and measurement uncertainty, as indi-
vidual instances contribute heavily towards the outcome. Currently
available feature-space augmentation methods do not handle this
problem properly, since synthetic instances are sampled from class-
specific distributions, while in general the measurement uncertainty
is expected to be class-invariant (e.g., fluctuations of body weight
measured using a scale should be independent of a subjectâ€™s gender).
Given the small number of samples, the performance evaluation can
suffer from a lack of confidence as high variance may be observed
regardless of the choice of the performance metric. We hypothe-
size that feature-space augmentation can be harnessed to improve
classification performance on small sample sizes and potentially
noisy features, for which the challenge exists both in learning ro-
bust classification models and in conducting reliable performance
evaluations.
In this paper, we present a novel algorithm for numeric data
augmentation in feature space by disregarding the class labels and
focusing on the distribution of individual features. We demonstrate
that this class-invariant algorithm can be effective in resolving
several commonly encountered problems in classification tasks,
including class imbalance and small sample sizes. We show that
our method outperforms existing augmentation tools for multiple
classifiers trained on a wide range of datasets with both balanced
and imbalanced classes.
2 METHODOLOGY
2.1 Rationale of the Proposed Method
We propose a novel augmentation method in feature space, Class-
invariant Feature Range Uniform Sampling (CiFRUS), to address
common challenges in real-world classification problems such as
class imbalance, small sample size, high dimensionality, and fea-
ture uncertainty. Our method introduces synthetic data points by
generating feature values that are randomly drawn from a locally
approximated distribution of feature values in the real data. The
result is that each real instance is expanded into a collection of
Class label
0 (majority)
1 (minority)
Sample type
Organic
SyntheticFigure 1: A schematic representation of how CiFRUS gener-
ates points in feature space. Each real point represents an
observation in high-dimensional space. Synthetic points ac-
count for feature uncertainty by filling up the empty regions
in feature space, which results in a more refined and stable
classification boundary.
instances forming a point cloud in feature space, where the expan-
sion in each dimension is determined by the probability density
near the real parent instance (Figure 1). A key difference between
our method and the existing augmentation methods is that our
synthesis process relies on the distribution of individual features
from the entire dataset, disregarding the class labels. This approach
stands in stark contrast to the instance-level, intra-class interpola-
tion adopted by SMOTE and its many successors.
There are several important consequences of this seemingly
small difference. First of all, our method significantly constricts the
inter-class regions, which can be useful for improving classifica-
tion performance on both class-balanced and imbalanced datasets.
Synthetic instances introduced by currently available methods are
confined in the convex hull of the observed instances for each class
and therefore contribute little to refining the decision boundaries
already defined by the observed instances. With our method, syn-
thetic instances are more likely to expand into the gap between
different classes, which helps to learn more complex classifiers.
This is especially important for high-dimensional datasets with rel-
atively few samples. In such cases, simpler models such as logistic
regression or small decision trees / tree-ensembles are generally
preferred to avoid overfitting, but these simple models may fail to
capture the real relationship in the data, limiting the prediction
performance. Our method constricts the region in feature space
where a decision boundary can be established, which in turn can
force the approximated non-linear decision boundaries to curve
around the point clouds and result in an overall more complex but
stable classifier without increasing the risk of overfitting.
Secondly, as our augmentation method takes into account fea-
ture values from all classes, it can be intuitively interpreted as
emulating measurement uncertainty. SMOTE and its variants em-
ploy intra-class interpolation, which in practice, only applies to the
minority class. Even if interpolation is applied to non-minority class
 
2513A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
instances, the generation process is determined entirely by class-
specific densities. Measurement uncertainty, on the other hand,
is not unique to individual classes. In our method, the synthetic
data instances and their associated real parents represent an en-
semble of points for each real one. The cloud of synthetic points
associated with each real point signifies variations of that point
that can arise from measurement uncertainty. As a result, both
the model-building and evaluation processes are forced to take
into consideration sample variations, thus reducing the impact of
measurement uncertainty.
Lastly, while the synthetic data points generated in our method
can inherit the class label of their respective parent, the labels of the
parent data points are allowed to be unknown. This makes our aug-
mentation method versatile for different purposes. Synthetic data
points can be added to the training set for building more complex
models, and to the testing set (with labels hidden) or prediction set
(with unknown labels) for more robust predictions as well as more
reliable performance evaluation results. The latter is especially im-
portant for small or imbalanced datasets, where robust methods for
performance evaluation are lacking.
2.2 Uniform Sampling from Feature Range
We now outline the procedure to produce a synthetic instance
from a given query instance by leveraging an observed training
dataset of real instances. The training data is fitted by independently
sorting each feature vector and storing them, with the class labels
disregarded. To augment a query instance, the insertion locations of
its feature values into the corresponding sorted feature vectors are
determined. A high and low feature value proximal to the insertion
location is then chosen for setting the upper and lower bounds
for uniform sampling of a synthetic feature value. Let, ğ‘‹âˆˆRğ‘›Ã—ğ‘š
denote the training dataset with ğ‘›instances and ğ‘šfeatures. Let, ğ¹=
{ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›}âŠ¤denote the column vector for a particular feature in
ğ‘‹. Sortingğ¹yields Ë†ğ¹={Ë†ğ‘¥1,Ë†ğ‘¥2,...,Ë†ğ‘¥ğ‘›}âŠ¤such that Ë†ğ‘¥1â‰¤Ë†ğ‘¥2â‰¤...â‰¤
Ë†ğ‘¥ğ‘›. For a query instance with corresponding feature value ğ‘, we
find its insertion location ğ‘–into Ë†ğ¹such thatâˆ€ğ‘—(ğ‘—<ğ‘–â‡’Ë†ğ‘¥ğ‘—<ğ‘)
andâˆ€ğ‘—(ğ‘—â‰¥ğ‘–â‡’Ë†ğ‘¥ğ‘—â‰¥ğ‘). Usingğ‘–as the midpoint, we acquire two
proximal indices:
ğ‘™=max(1,ğ‘–âˆ’ğ‘˜) (1)
ğ‘¢=min(ğ‘›,ğ‘–+ğ‘˜âˆ’1) (2)
Here,ğ‘™andğ‘¢are the indices of Ë†ğ¹containing the bounds for
random sampling. These bounds are denoted as ğ‘ğ‘™=Ë†ğ‘¥ğ‘™andğ‘ğ‘¢=
Ë†ğ‘¥ğ‘¢. The feature value for a new synthetic instance is drawn from
a uniform random distribution with lower bound ğ‘ğ‘™and upper
boundğ‘ğ‘¢. The parameter ğ‘˜controls the width of the local sampling
range in feature space. Using this approach, the synthetic feature
value is bounded by [Ë†ğ‘¥1,Ë†ğ‘¥ğ‘›). However, in the event of an outlier
query feature value that falls outside of these bounds, the generated
values may not accurately represent the query sample. In this case,
the bounds of the uniform distribution are expanded as follows:
ğ‘ğ‘™=min(Ë†ğ‘¥ğ‘™,ğ‘)andğ‘ğ‘¢=max(Ë†ğ‘¥ğ‘¢,ğ‘). This process is independent
for each feature and can be generalized to any number of features.
Algorithm 1 implements this synthesis procedure. Using feature
space sampling, the algorithm generates ğ‘Ÿsynthetic instances for
every instance in a set of query instances. The parameter ğ‘Ÿis theAlgorithm 1: CiFRUS-Resample(ğ‘‹,ğ‘„,ğ‘Ÿ,ğ‘˜)
Input: Training dataset ğ‘‹âˆˆRğ‘›Ã—ğ‘š, set of query instances ğ‘„.
Parameters: Sampling range, ğ‘˜; scalar synthesis rate, ğ‘Ÿ.
Output: Set of synthetic samples, ğ‘†, where|ğ‘†|=ğ‘Ÿ|ğ‘„|.
1ğ‘†=âˆ…
2Obtain Ë†ğ‘‹by sorting each feature column in ğ‘‹.
3forğ‘={ğ‘1,...ğ‘ ğ‘š}âˆˆğ‘„do
4 forğœŒâˆˆ1,2,...,ğ‘Ÿ do
5 Initializeğ‘ ={ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘š}={0,0,...,0}
6 forğ‘—âˆˆ1,2,...ğ‘š do
7 Let, Ë†ğ¹=Ë†ğ‘¥1,Ë†ğ‘¥2,...,Ë†ğ‘¥ğ‘›is theğ‘—-th column vector of
Ë†ğ‘‹.
8 ğ‘–=Binary-Search(Ë†ğ¹,ğ‘ ğ‘—)
9 ğ‘™=max(1,ğ‘–âˆ’ğ‘˜)
10 ğ‘¢=min(ğ‘›,ğ‘–+ğ‘˜âˆ’1)
11 ğ‘ğ‘™=min(Ë†ğ‘¥ğ‘™,ğ‘ğ‘—)
12 ğ‘ğ‘¢=max(Ë†ğ‘¥ğ‘¢,ğ‘ğ‘—)
13 ğ‘ ğ‘—=Uniform-Random(ğ‘ğ‘™,ğ‘ğ‘¢)
14 end
15ğ‘†=ğ‘†âˆª{ğ‘ }
16 end
17end
18returnğ‘†
synthesis rate. Unlike other feature-space augmentation methods
such as SMOTE and ADASYN, the class label is not factored into
the augmentation algorithm. Therefore, our algorithm can augment
instances from the training dataset, as well as unlabeled instances
using the distribution of the training set. When training instances
with known class labels are augmented, every synthetic instance
is assigned the same class labels as the real parent instance. When
instances with hidden or unknown labels are augmented, the class
label assignment step is skipped. Figure 2 demonstrates a run of the
algorithm with ğ‘˜=2,ğ‘Ÿ=3for a single query instance and a training
dataset with 5 instances and 4 features. The time complexity of this
algorithm is discussed in Appendix A.
Algorithm 2: CiFRUS-Resample-classes (ğ‘‹,ğ‘¦,ğ‘…,ğ‘˜)
Input: Training dataset ğ‘‹âˆˆRğ‘›Ã—ğ‘š; class labels ğ‘¦âˆˆğ¶ğ‘›Ã—1.
Parameters: Sampling range ğ‘˜; synthesis rate
ğ‘…={ğ‘Ÿğ‘:ğ‘âˆˆğ¶}.
Output: Set of labeled synthetic instances, ğ‘†
1ğ‘†=âˆ…
2forğ‘âˆˆğ¶do
3ğ‘„ğ‘={ğ‘‹ğ‘–:ğ‘¦ğ‘–=ğ‘,ğ‘–=1,2,...,ğ‘›}
4ğ‘†ğ‘=CiFRUS(ğ‘‹,ğ‘„ ğ‘,ğ‘Ÿğ‘,ğ‘˜)
5 Assign class label ğ‘to each instance in ğ‘†ğ‘
6ğ‘†=ğ‘†âˆªğ‘†ğ‘
7end
8returnğ‘†
 
2514KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sakhawat Hossain Saimon, Tanzira Najnin, and Jianhua Ruan
Figure 2: Example generation of 3 synthetic samples ( ğ‘†1,ğ‘†2,ğ‘†3) from a single query sample ğ‘and a training set ğ‘‹consisting of 5
observed samples. The query sample and training dataset have 4 features each. If the class label of the query sample is known,
then it is inherited by the synthetic samples.
2.3 Class-specific Synthesis Rates
The generation of synthetic instances using our method does not
rely on the class label of their parent instances. However, in the case
of augmenting instances from the training set, the class labels are
known. It is therefore possible to apply a different synthesis rate to
each class of instances. The set of unique class labels is denoted as ğ¶,
andğ‘¦={ğ‘¦1,...,ğ‘¦ ğ‘›}âˆˆğ¶ğ‘›Ã—1indicates the class label of the instances
inğ‘‹. We appended a subscript ğ‘to the synthesis rate parameter ğ‘Ÿ
such thatğ‘Ÿğ‘represents the synthesis rate for instances that belong
to classğ‘. Algorithm 2 implements class-specific synthesis rates and
acts as a wrapper for Algorithm 1. While this algorithm requires
the user to specify the synthesis rate for each class, it can be utilized
to perform class balancing, which is discussed in the next section.
2.4 Balanced Augmentation of Labeled Data
The class-imbalance problem can be solved using our algorithm
by applying a different synthesis rate to each class such that the
imbalance is minimized or eliminated after augmentation. If the
synthesis rate for either the majority or the minority class is fixed,
then the required rate to balance the other classes can be determined
with ease. In our implementation, we parameterize the synthesis
rate for the majority class and dynamically determine the synthesis
rates for other classes such that the total number of instances in
non-majority classes after augmentation is at least equal to that of
the majority. As an example, let us consider a two-class dataset with
23 and 7 instances for the classes. We revert to a scalar synthesis
rate parameter ğ‘Ÿ, which will only apply to the majority class. In
practice, augmented data includes both synthesized and their parent
real instances. If ğ‘Ÿ=5, then the total number of instances for the
majority class after augmentation is 23Ã—(ğ‘Ÿ+1)=138. To exceed
138 total instances, the synthesis rate for the minority class should
beâŒˆ(138âˆ’7)/7âŒ‰=19. After augmentation, the minority class will
therefore have a total of 7Ã—(1+19)=140instances. Generally,
letğ‘›majdenote the number of instances in the majority class. For
any classğ‘withğ‘›ğ‘instances before augmentation, the synthesis
rate can be determined as âŒˆğ‘›maj(ğ‘Ÿ+1)/ğ‘›ğ‘âˆ’1âŒ‰. This scheme offers
control over the total number of samples post-augmentation while
achieving coarse-grained class balancing for any number of classes.
The determination of synthesis rates for non-majority classes canbe prefixed to Algorithm 2 to acquire a balancing variant of our
augmentation method.
2.5 Ensemble Classification of Augmented
Unlabeled Data
Feature-space augmentation methods typically require knowledge
of the class labels to augment the data. Since our proposed algo-
rithm does not require class labels to be known for fixed synthesis
rate augmentation, it is possible to augment instances whose labels
are unknown or hidden. The advantage of doing so is that an indi-
vidual real instance from the test set can be augmented into a point
cloud of instances, ğ‘†, followed by ensemble classification. Each of
the instances in ğ‘†can be classified in isolation, and the predicted
class label of the parent instance can be determined based on the
aggregation of predictions for all instances in ğ‘†. As we will observe
later, augmentation of unlabeled instances before classification can
greatly benefit classification performance.
3 EXPERIMENTS
We evaluate our proposed method and compare the classification
performance with existing methods using a multi-faceted experi-
mental setup.
Datasets: 62 benchmarking datasets for binary classification avail-
able in the UCI Machine Learning repository, Kaggle, and [ 12] were
used. The datasets contain between 72 and 38,501 observations
and between 3 and 1,522 features (Figure 3). The class imbalance
also varies widely. The mammography dataset has an imbalance
ratio of 42.01, with only 2.32% of the instances belonging to the
minority class. In contrast, several datasets such as hill-valley and
seed contain an approximately even split of the two classes.
Classifiers: State-of-the-art classifiers were trained on the baseline
as well as augmented train splits of each dataset. Random Forest
(RF), Decision Tree (DT), ğ‘˜-nearest-neighbor (KNN), ADABOOST
(ADB), Multilayer Perceptron (MLP), Logistic Regression (LR) and
Gaussian Naive-Bayes (GNB) classifiers were trained using default
parameter settings from scikit-learn [25].
Augmentation: We evaluate three different variants of the CiFRUS
augmentation technique (Figure 4). Each dataset is split into train
andtestsets. The testsetâ€™s class labels or feature distributions are
 
2515A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
102103104
nthe-forest-cover-dataset
adult-dataset
the-mammography-dataset
phoneme-dataset
iranian-churn-dataset
banknote-authentication
hill-valley
oocytes-merluccius-nucleus-4d
statlog-german-credit-1
statlog-german-credit-2
mammographic
tic-tac-toe
the-oil-dataset
oocytes-trisopterus-nucleus-2f
pima-indian-not-normalized
pima
cervical-cancer
breast-cancer-wisc
statlog-australian-credit-1
credit-approval-1
statlog-australian-credit-2
credit-approval-2
climate-model-simulation-crashes
ilpd-indian-liver
breast-cancer-wisc-diag
cylinder-bands-2
cylinder-bands-1
las-vegas-trip
congressional-voting
student-performance
ionosphere
mesothelioma
vertebral-column-2clases
haberman-survival
horse-colic-2
horse-colic-1
breast-cancer-nki
heart-hungarian-1
heart-hungarian-2
colposcopies
breast-cancer-2
breast-cancer-1
breast-cancer-wang
statlog-heart-1
statlog-heart-2
spect
spectf
glass-id
conn-bench-sonar-mines-rocks
breast-cancer-wisc-prog
parkinsons
planning
wine
hepatitis
seed-1
seed-2
seed-3
echocardiogram
acute-nephritis
acute-inflammation
breast-cancer-coimbra
cervical-cancer-behaviour-risk
101102103
m0 5 10+
IR
Figure 3: List of benchmarking datasets with number of in-
stancesğ‘›, number of features ğ‘š, and imbalance ratio (IR).
never used to fit the augmentation algorithms or the classifiers.
All three CiFRUS variants see and augment the train set. The clas-
sifiers are trained on the augmented train sets. The first variant,
CiFRUS (train), implements the basic augmentation process de-
scribed in section 2.2, with a fixed synthesis rate of ğ‘Ÿ=10. CiFRUS
(train/test) extends the first variant by augmenting testinstances
without seeing their class labels, using the feature distributions
oftrain (section 2.5). Each individual testinstance is augmented
into a set of instances, whose class probability scores are predicted
by the classifier. The label for the parent testinstance is then pre-
dicted using the average probability score. The testinstances are
augmented using the same synthesis rate of 10. The third variant,
CiFRUS (balanced-train/test) extends CiFRUS (train/test) by per-
forming class balancing while augmenting the train splits. With
this variant, the synthesis rate for the majority class is set to 5,
and the rate for the other class is determined dynamically (section
2.4). When augmenting testinstances, the synthesis rate can not be
dynamically determined as the class label is not known. Therefore,
thetestinstances are augmented with a fixed synthesis rate of 10.
In all three variations, the value of the sampling range parameter ğ‘˜
is calculated as a heuristic function of the number of instances in
thetrain set, such that ğ‘˜=min(âˆšğ‘›,20).
We also augmented the train sets using Random Oversampling
(ROS), Undersampling (RUS), SMOTE, Borderline-SMOTE [ 13],
SVM-SMOTE [ 24], ADASYN [ 14], and G-SMOTE [ 9]. The python
C0
0
0
1
?
?Fit(X)
Resample(X, [y])
Resample_balanced(X, y)Fit(X,y)
Predict(X)
x1 0
x2 0
0
1
?
?Fit(X)
Resample(X, [y])
Resample_balanced(X, y)Fit(X,y)
Predict(X)x3
x4
x5
x6x1 0
x1, 1 0
0
0x2
x2, 1
x3 0
x3, 1 0
1
1x4
x4, 1
0
0
0
1
?
?Fit(X)
Resample(X, [y])
Resample_balanced(X, y)Fit(X,y)
Predict(X)0
0
0
0
0
0
1
1CiFRUS
CiFRUS
CiFRUS ClassifierClassifierClassifier
0
0
0
1
?
?Fit(X)
Resample(X, [y])
Resample_balanced(X, y)Fit(X,y)
Predict(X)CiFRUS Classifierx1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x61
1x1
x1, 1
x2, 1
x3
x3, 1
x4
x4, 1x2
0
0
0
1
?
?Fit(X)
Resample(X, [y])
Resample_balanced(X, y)Fit(X,y)
Predict(X)x5
x5, 1
x5, 2CiFRUS Classifier
x6
x6, 1
x6, 2x1
x2
x3
x4
x5
x61
0
0
1
1
0Vote0
1Train Test Train Test Train Test Train Test Train Testa
b
c
d
e... ...
1 x4, 5Figure 4: Schematic representation of the (a) fitting, (b)-(c)
classifier training, and (d)-(e) performance evaluation work-
flows adopted by the three CiFRUS variants. The features and
the class labels are denoted as ğ‘‹andğ‘¦, respectively, and the
dataset is split into train and test sets. (a) All three variants
fit the train set. (b) Variants train and train/test augment
the train set, followed by classifier training. (c) Variant bal.-
train/test augments the train set while balancing the classes,
followed by classifier training. (d) Variant train uses typi-
cal evaluation using the test set. (e) Variant train/test and
bal.-train/test augment the test set, followed by ensemble
classification.
package implementation imbalanced-learn [20] was adopted for
all augmentation methods except G-SMOTE, for which we used
the author-provided implementation. All existing augmentation
methods were initialized with default parameter settings.
Performance Evaluation: Performance was evaluated strictly
using real testset instances and their associated labels. The metrics
 
2516KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sakhawat Hossain Saimon, Tanzira Najnin, and Jianhua Ruan
Table 1: Augmentation methods ranked on the AUC metric.
Overall rank is determined using the geometric mean of in-
dividual ranks for 62 datasets. Smaller ranks correspond to
high AUC. The top-ranked method in each column is marked
with an asterisk. CiFRUS variants that outrank other meth-
ods as well as the baseline (no augmentation) are highlighted
in blue.
GNB
KNN
LR
MLP
D
T
ADB
RF
Baseline
3.9 3.7 *3.5 4.0 6.2 5.1 4.1
ROS 3.9 5.5 4.0 4.0 6.3 5.4 4.6
RUS 5.1 4.3 5.0 5.1 5.1 7.0 6.5
SMOTE *3.6 4.2 4.2 4.8 5.9 6.0 4.9
Bord.-SMOTE 5.0 5.5 5.5 4.7 6.6 6.7 5.2
SVM-SMOTE 4.6 4.5 4.5 4.3 5.3 4.9 4.7
ADASYN 4.9 4.8 4.7 4.6 6.0 6.2 5.3
G-SMOTE 4.5 3.9 5.0 4.5 4.6 4.9 3.9
CiFRUS
train 4.0 7.3 4.4 4.3 5.0 3.3 4.6
train/test 4.5 3.5 4.1 2.8 1.7 *2.2 3.1
bal.-train/test 4.9 *2.8 4.2 *2.8 *1.7 2.3 *2.9
Area Under ROC Curve (AUC), F1-score, Kappa statistic, balanced
accuracy, and Matthews Correlation Coefficient (MCC) were used
to evaluate performance. Each performance metric was averaged
over 10 repeats of 10-fold stratified cross-validation. True-positives
(TP), true-negatives (TN), false-positives (FP), and false-negatives
(FN) were used to calculate the following metrics:
Precision =TP
TP+FP;Re
call=TP
TP+FN;Sensitivity =TP
TP+FN;
Sp
ecificity =TN
TN+FP;Balance
d-accuracy =Sensitivity+Specificity
2;
F1-scor
e=2Â·PrecisionÂ·Recall
Pr
ecision+Recall;Kappa =2(TPÂ·TNâˆ’FNÂ·FP)
(TP+FP)
(FP+TN)+(TP+FN)(FN+TN);
MCC =(TPÂ·TNâˆ’FPÂ·FN)âˆš
(TP+FP)
(TP+FN)(TN+FP)(TN+FN). For every classifier-dataset
pair, augmentation methods were ranked based on the performance
metric averaged over 10 repeats of cross-validation. The overall
rank of each augmentation method for a specific classifier was
determined by taking the geometric mean of the ranks across all
62 datasets. All randomized algorithms were seeded with a global
constant to ensure the reproducibility of the results.
4 RESULTS
4.1 CiFRUS Outperforms Other Augmentation
Methods
The second and third CiFRUS variants, with their ability to augment
both labeled and unlabeled instances, have the top average AUC
rank for classifiers KNN, MLP, DT, ADB, and RF, outranking other
augmentation methods as well as the baseline dataset variants (Ta-
ble 1). This trend is consistent for other performance metrics such as
the Kappa statistic and F1-score (Appendix B, Table 3). In addition
to the overall rank, we counted the number of datasets for which
each augmentation method exhibited the highest performance met-
ric value. Using the Decision Tree classifier, CiFRUS (train/test)
augmentation produced the highest AUC in 22 out of 62 datasets,
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)augmentation8 10 10 12 2 4 12
8 5 8 14 2 6 6
7 14 8 10 5 4 4
14 5 7 9 3 2 4
10 5 9 8 2 3 8
6 7 8 9 3 4 8
7 6 8 8 3 4 7
11 9 5 10 5 4 10
16 4 11 9 2 10 7
10 13 12 17 22 22 16
4 21 12 20 33 21 17
CiFRUS (any) outperforms
 or matches baseline38 39 39 48 62 53 47
GNB KNN LR MLP DT ADB RFCiFRUS (any) outperforms
or matches other methods26 35 32 36 52 48 36
0.0 0.2 0.4 0.6 0.8 1.0
classifier0.00.20.40.60.81.0Figure 5: Number of datasets for which each augmentation
tool yielded the best average cross-validation AUC. The last
two rows indicate the number of datasets for which the best-
performing CiFRUS variant matched or exceeded the perfor-
mance on the baseline and other methods, respectively.
and CiFRUS (balanced-train/test) produced the highest AUC for 33
(Figure 5). The three CiFRUS variants also produced the highest
AUC for 7, 16, and 17 datasets, respectively, using the Random For-
est classifier. Overall, the highest AUC for any CiFRUS variant was
no less than the highest AUC for any other augmentation tool in
over half of the datasets across all compared classifiers (62, 52, and
47 datasets for DT, ADB, and RF, respectively). Compared to when
trained on the baseline datasets, the classifiers KNN, MLP, DT, ADB,
and RF trained on CiFRUS-augmented datasets match or exceed the
performance for 35, 36, 52, 47, and 36 datasets, respectively. CiFRUS
variants also outperform other augmentation methods in terms of
the number of datasets for metrics such as the F1-score and Kappa
statistic (Appendix B, Figure 9). While CiFRUS showed promising
results when paired with a majority of the classifiers, tree-based
classifiers exhibited the greatest improvement. These classifiers
have non-linear decision boundaries and benefit the most from our
data augmentation method which generates synthetic data points
surrounding each real point, effectively forcing the classification
algorithm to produce more refined decision boundaries far away
from the real data points. LR and GNB are less likely to benefit
from it, as the augmented data points have less impact on the lin-
ear decision boundary and the probability distribution of feature
values.
To investigate whether CiFRUS can improve the performance
beyond what is attainable by classifiers sans augmentation, we
focus on Random Forest, one of the best-performing classifiers
across our benchmark suite. RF trained on baseline datasets has the
lowest average AUC rank of 6.6 among all classifier-augmentation
pairs excluding ours (data not shown). This suggests that improv-
ing the performance of vanilla RF is a more challenging task than
for other classifiers. The performance difference of Random For-
est between CiFRUS (balanced-train/test) augmentation and the
baseline is positive for most datasets, as evidenced by the distri-
butions of the metric differences (Figure 6). Paired t-tests between
 
2517A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0.05
 0.00 0.05025p = 0.06 AUC
0.25
 0.00 0.25p = 0.01 F1-score
0.1
 0.0 0.1p = 0.00 Kappa
0.1
 0.0 0.1p = 0.00 balanced-accuracy
0.2
 0.0 0.2
RFp = 0.03 MCC
0.1
 0.0 0.1025p = 0.00
0.25
 0.00 0.25p = 0.12
0.1
 0.0 0.1p = 0.07
0.1
 0.0 0.1p = 0.06
0.1
 0.0 0.1
DTp = 0.06
0.05
 0.00 0.05025p = 0.00
0.25
 0.00 0.25p = 0.08
0.2
 0.0 0.2p = 0.36
0.1
 0.0 0.1p = 0.00
0.2
 0.0 0.2
ADBp = 0.19
0.2
 0.0 0.2025p = 0.10
0.2
 0.0 0.2p = 0.02
0.2
 0.0 0.2p = 0.19
0.2
 0.0 0.2p = 0.00
0.25
 0.00 0.25
KNNp = 0.27
0.5
 0.0 0.5025p = 0.05
0.5
 0.0 0.5p = 0.13
0.5
 0.0 0.5p = 0.47
0.25
 0.00 0.25p = 0.08
0.5
 0.0 0.5
MLPp = 0.35
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0number of datasets
Figure 6: Distribution of the performance metric difference of classifiers trained on CiFRUS balanced-train/test and baseline
datasets. The p-values are computed using paired t-tests on the metric scores.
the metric scores (e.g. AUC) for CiFRUS-augmented and baseline
datasets suggest that the observed performance improvement is
statistically significant (p-value â‰¤0.05) for all metrics except AUC,
for which the p-value is 0.058. The performance differences are also
mostly positive for DT, ADB, and KNN, two of which are tree-based
classifiers. This strongly suggests that CiFRUS can be employed to
improve classification performance across the board.
4.2 Improved Classification Performance on
Imbalanced or Sample-Scarce Datasets
Augmentation with CiFRUS can counter two of the most com-
mon problems encountered in the classification of many real-world
datasets: class imbalance, and a small number of observed instances
compared to a large number of feature variables. We refer to datasets
that exhibit these problems as imbalanced andsample-scarce, re-
spectively. Specifically, we consider datasets in our benchmark suite
with no more than 25% of all instances belonging to the minority
class (imbalance ratio â‰¥3) as imbalanced, whereas datasets con-
taining fewer minority class instances than twice the number of
features (ğ‘›minority <2ğ‘š) are considered sample-scarce. Using these
specifications, 12 of the 62 datasets are imbalanced, 12 are sample-
scarce, and 5 fall into both groups. The difference between with-
and without-augmentation performance of the classifiers is used to
measure the effectiveness of each augmentation tool in mitigating
class imbalance and sample scarcity. To minimize the effect of sto-
chastic factors, we only consider datasets for which the absolute
metric difference exceeds a threshold value of 0.005. Figure 7 shows
the number of imbalanced and sample-scarce datasets for which
performance improves or worsens by at least the threshold amount.
The second and third variants of the CiFRUS algorithm improvethe AUC by at least 0.005 for more datasets than any other aug-
mentation method. This observation holds for all classifiers used
in the benchmarking. Interestingly, this includes the GNB and LR
classifiers, where our method results in improved AUC when only
imbalanced and sample-scarce datasets are considered. As for the
Kappa statistic, a majority of the augmentation methods, including
ours, perform poorly with the GNB and LR classifiers. Nonetheless,
our method is among the most successful in terms of raising the
Kappa statistic as well as the F1-score and MCC for KNN, DT, ADB,
and RF. These results suggest that CiFRUS is particularly useful
when dealing with class imbalance and sample scarcity, and can be
a reliable tool to improve the performance of datasets that exhibit
these challenges.
4.3 CiFRUS Increases Reliability of
Performance Evaluation
One of the most critical aspects of classification tasks is the evalua-
tion of classifiers and associated configurations that best fit a partic-
ular domain. Performance evaluation is typically conducted using
cross-validation followed by the computation of a performance
metric such as AUC, which is used to select the best classifier. In
the case of datasets with very few samples, the evaluation itself can
be unreliable. Reliability can be measured by computing variability
or confidence intervals of AUC for different folds or repeats. We
claim that compared to other augmentation algorithms, CiFRUS
augmentation can increase reliability while retaining or improving
the average value of the performance metric. To inspect the impact
of augmentation on both classifier performance and evaluation reli-
ability, we calculated the variance and average of each performance
metric over 10 repeats of stratified cross-validation for every dataset.
Each augmentation method was assigned two separate ranks, one
 
2518KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sakhawat Hossain Saimon, Tanzira Najnin, and Jianhua Ruan
010GNB KNN LR MLP DT ADB RF
Post-augmentation
AUC change
0.005
0.005
010Post-augmentation
Kappa change
0.005
0.005
010Post-augmentation
F1-score change
0.005
0.005
010Post-augmentation
balanced-accuracy change
0.005
0.005
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)010
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)Post-augmentation
MCC change
0.005
0.005
0.0 0.2 0.4 0.6 0.8 1.0
Augmentation0.00.20.40.60.81.0Number of datastes
Figure 7: Number of imbalanced and sample-scarce datasets for which a performance difference of at least 0.005 is observed
due to augmentation. A total of 19 datasets are identified as imbalanced or sample-scarce.
based on the average and the other based on the variance of the per-
formance metrics. Smaller rank values indicate larger averages and
smaller variances. Figure 8 shows the overall ranks of each augmen-
tation method using the RF, LR, and ADB classifiers, calculated over
all datasets as well as imbalanced and sample-scarce datasets only.
In the case of RF, the second and third CiFRUS variants both rank
favorably, with the third variant outperforming most of the other
augmentation methods in addition to the baseline on improving the
metric average and minimizing variance. The same two variants
rank exceptionally well with the ADB classifier across all metrics.
As for LR, while the average and variance ranks for all datasets
are not significantly better, CiFRUS outperforms others in terms of
the variance rank for imbalanced and sample-scarce datasets. For
datasets with these characteristics, other augmentation methods,
paired with most of the classifiers, rank poorly in terms of the vari-
ance in F1-score, Kappa, and balanced accuracy, with G-SMOTE
being the only exception. In contrast, CiFRUS variants consistently
achieve greater performance and generally small variances when
imbalanced and sample-scarce datasets are considered, suggesting
that they excel in simultaneously improving performance and in-
creasing confidence in the reliability of classifier evaluation. Wealso ranked the augmentation methods after pairing them with
GNB, KNN, MLP, and DT (Appendix B, Figure 10). While some of
these classifiers favor the baseline, the performance of CiFRUS is
generally on par with the other augmentation methods when all
datasets are considered. In the case of imbalanced and sample-scarce
datasets, CiFRUS is ranked higher than other methods in terms of
performance average and does moderately well in variance.
It is interesting to note that only the last two variants of CiFRUS
achieve high performance and reliability when paired with most of
the classifiers. The most likely explanation points to their ability to
augment unlabeled test data. The decision boundary drawn by a
classifier may not be necessarily accurate for individual instances
that fall close to the boundary, resulting in misclassification. How-
ever, when a test point instance is expanded into a cloud of points
through augmentation, the predicted class label is determined by
the classification of all points in that cloud. If the classifier correctly
predicts the class label for the majority of the synthetic points, then
the misclassification of the parent instance is corrected through
wisdom of crowds, which best explains the superior performance of
the train/test and balanced-train/test CiFRUS variants.
 
2519A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)4.67 4.53 5.44 5.54
5.21 5.25 5.98 4.42
7.26 8.06 8.11 7.85
5.63 4.29 5.31 3.70
6.12 5.53 5.31 4.79
5.25 5.13 5.25 4.22
5.96 5.66 6.25 5.55
4.38 4.59 4.60 5.12
5.05 5.23 4.37 6.29
3.40 3.75 2.47 4.17
3.19 3.93 3.29 3.72AllImbalanced
 or sample-scarce
5.21 4.62 5.33 4.14
4.78 4.61 4.23 4.73
4.82 5.27 7.68 6.11
5.68 4.85 5.44 5.12
6.13 6.01 6.18 6.28
5.35 5.62 5.01 5.11
5.47 5.27 5.85 5.62
5.02 4.29 5.61 3.99
4.28 4.85 3.51 4.87
4.36 4.71 3.33 4.39
3.96 4.82 3.56 4.24AllImbalanced
 or sample-scarce
5.44 4.68 6.97 5.08
4.78 4.88 4.94 5.08
6.60 5.28 7.29 5.10
5.46 4.76 5.28 4.79
5.81 5.85 5.71 6.10
4.69 5.48 3.71 4.56
5.48 5.30 5.64 5.63
4.93 4.57 4.21 3.87
4.57 4.89 4.44 5.68
4.10 4.38 3.98 4.13
3.51 4.78 3.39 4.46AllImbalanced
 or sample-scarce
7.15 4.36 9.74 4.61
5.06 4.84 4.92 4.84
3.70 5.44 2.84 5.28
5.37 4.96 5.06 5.11
5.14 5.98 5.10 5.85
4.37 5.56 3.42 4.82
4.69 5.38 4.34 5.74
4.77 4.30 3.61 3.65
5.72 4.65 6.66 5.52
5.63 4.38 7.27 4.30
3.91 5.10 4.39 4.73AllImbalanced
 or sample-scarce
5.04 4.88 6.49 5.36
4.96 4.82 5.12 5.04
6.18 5.24 7.05 5.01
5.73 4.52 5.78 4.63
6.16 5.94 6.18 5.75
5.07 5.28 4.30 4.46
5.80 5.20 6.17 5.75
4.85 4.51 4.22 3.70
4.22 5.06 3.70 6.22
3.85 4.69 3.37 4.51
3.58 4.68 3.42 4.17AllImbalanced
 or sample-scarce
RF
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)5.74 4.79 5.75 4.79
5.93 5.05 5.45 4.86
7.39 7.17 6.50 8.86
6.50 6.22 6.75 6.54
7.38 5.94 7.68 5.38
5.23 5.99 5.31 6.11
6.82 5.62 7.11 4.55
5.32 5.36 5.40 5.17
3.51 3.58 2.77 3.49
2.29 3.26 2.65 3.29
2.40 3.32 2.53 3.335.46 5.13 4.07 7.56
4.47 5.02 4.37 4.43
6.79 5.83 8.11 5.31
5.49 5.25 6.62 5.12
5.83 5.74 5.26 5.31
5.13 5.18 5.36 4.37
5.64 4.48 5.83 4.25
5.67 5.35 5.32 5.85
4.29 4.98 3.39 4.84
3.87 4.46 3.11 4.95
3.17 3.66 4.53 3.175.13 5.32 4.25 7.87
4.80 4.53 5.51 4.24
7.63 5.97 8.16 5.39
5.32 5.17 6.13 5.09
6.17 5.64 5.08 5.56
5.06 5.41 4.99 4.50
5.60 4.64 5.69 4.19
5.45 5.27 5.08 5.59
3.68 5.19 3.18 5.05
3.69 4.24 3.43 4.47
3.54 3.73 4.26 3.296.95 4.82 7.80 6.96
4.42 4.81 4.28 4.43
5.45 6.33 3.89 6.00
5.20 5.19 5.52 4.71
5.89 5.75 5.36 5.66
4.45 5.49 4.48 5.00
4.99 4.69 5.10 4.14
6.30 5.23 6.15 5.38
5.49 4.88 5.98 4.87
5.20 4.07 6.03 4.39
2.23 3.89 2.05 3.414.97 5.36 4.20 8.05
4.97 4.59 5.65 4.40
7.35 6.02 7.71 5.44
5.76 4.95 6.35 5.02
6.15 5.49 5.06 5.25
5.26 5.31 5.14 4.32
5.67 4.53 5.71 4.11
5.42 5.28 5.07 5.66
3.80 5.39 3.27 5.39
3.52 4.34 3.23 4.46
3.29 3.80 4.32 3.25
ADB
Avg Var Avg Var
AUC rankBaseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)4.10 2.98 4.81 3.72
4.43 4.52 4.21 4.42
5.62 6.67 5.74 8.30
4.80 4.96 4.27 5.32
6.13 5.69 4.44 5.58
5.08 5.32 5.69 5.86
5.51 5.80 4.51 6.67
5.69 5.75 6.84 4.78
4.95 4.03 5.87 3.55
4.34 5.22 4.87 3.99
4.46 5.00 3.61 3.73
Avg Var Avg Var
F1-score rank4.66 4.48 4.19 4.09
4.60 5.03 4.52 6.07
5.33 5.93 6.30 7.20
4.02 4.99 4.49 4.31
6.38 5.60 5.31 5.69
4.68 5.05 4.32 6.00
5.12 4.36 5.19 4.88
5.00 5.23 5.66 3.57
4.92 4.95 5.11 5.74
5.27 4.82 4.42 4.47
4.91 4.45 4.89 3.40
Avg Var Avg Var
Kappa rank4.31 4.21 3.96 4.44
4.55 5.11 4.54 5.49
5.67 6.16 6.58 7.06
4.12 5.14 4.48 4.58
6.55 5.68 6.00 5.27
4.29 5.18 4.37 5.79
5.60 4.07 5.73 4.09
4.87 5.12 6.34 3.63
5.29 5.54 4.66 7.16
4.76 4.53 3.58 5.19
5.09 4.43 4.75 3.00
Avg Var Avg Var
balanced-accuracy rank5.91 3.66 7.65 3.74
4.20 5.35 3.49 6.00
4.62 6.14 3.93 7.07
3.80 5.51 3.72 4.89
5.68 5.90 5.08 5.53
3.95 5.40 4.33 6.34
4.69 4.26 5.09 4.68
4.35 5.06 4.28 3.45
7.46 4.81 8.36 5.78
7.12 4.19 7.53 4.06
4.23 5.06 3.39 3.86
Avg Var Avg Var
MCC rank4.24 4.28 3.85 4.47
4.60 4.87 4.50 5.21
5.56 6.12 6.04 6.97
4.33 5.06 4.90 4.31
6.42 5.78 6.04 5.35
4.43 5.25 4.57 6.15
5.40 3.80 5.82 4.14
5.05 5.11 6.27 3.61
5.28 5.38 4.79 6.99
4.72 5.08 3.45 5.33
4.94 4.39 4.69 3.12
LR
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0augmentation
Figure 8: Augmentation methods ranked by average and variance of each performance metric using Random Forest (top row),
Logistic Regression (middle row), and AdaBoost (bottom row). Each augmentation method was ranked on metric average as
well as variance over 10 repeats of stratified cross-validation. The overall rank is determined using the geometric mean of
ranks from individual datasets. Smaller ranks correspond to high average and low variance.
5 CONCLUSION
In this paper, we presented a numeric data augmentation algorithm
to address common issues in constructing and evaluating classi-
fication models on data with class imbalance and small sample
sizes. The novelty of our method lies in taking advantage of the
distributions of the individual features while ignoring the class
labels. Compared to previous work in this field which is mostly
directed toward resolving class imbalance, we demonstrate that our
method can improve performance across the board and works just
as well for datasets with imbalance and small sample sizes. Our
method can also be used to increase the reliability of performance
evaluation. Although this approach can be readily applied to multi-
classification problems, further experiments can be conducted to
determine its effectiveness in tackling more complex classification
tasks.
ACKNOWLEDGMENTS
This work received computational support from UTSAâ€™s HPC clus-
ter, operated by University Tech Solutions.REFERENCES
[1]Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George
Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020. Do Not Have
Enough Data? Deep Learning to the Rescue! Proceedings of the AAAI Conference
on Artificial Intelligence 34, 05 (Apr. 2020), 7383â€“7390.
[2]Sukarna Barua, Md. Monirul Islam, Xin Yao, and Kazuyuki Murase. 2014.
MWMOTEâ€“Majority Weighted Minority Oversampling Technique for Imbal-
anced Data Set Learning. IEEE Transactions on Knowledge and Data Engineering
26, 2 (2014), 405â€“425.
[3]Hong Cao, Xiao-Li Li, Yew-Kwong Woon, and See-Kiong Ng. 2011. SPO: Structure
Preserving Oversampling for Imbalanced Time Series Classification. In 2011 IEEE
11th International Conference on Data Mining. 1008â€“1013.
[4]Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.
2002. SMOTE: synthetic minority over-sampling technique. Journal of artificial
intelligence research 16 (2002), 321â€“357.
[5]Baiyun Chen, Shuyin Xia, Zizhong Chen, Binggui Wang, and Guoyin Wang. 2021.
RSMOTE: A self-adaptive robust SMOTE for imbalanced problems with label
noise. Information Sciences 553 (2021), 397â€“428.
[6]Sheng Chen, Haibo He, and Edwardo A. Garcia. 2010. RAMOBoost: Ranked
Minority Oversampling in Boosting. IEEE Transactions on Neural Networks 21, 10
(2010), 1624â€“1642.
 
2520KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sakhawat Hossain Saimon, Tanzira Najnin, and Jianhua Ruan
[7]Dan Ciregan, Ueli Meier, and JÃ¼rgen Schmidhuber. 2012. Multi-column deep
neural networks for image classification. In 2012 IEEE Conference on Computer
Vision and Pattern Recognition. 3642â€“3649.
[8]Barnan Das, Narayanan C. Krishnan, and Diane J. Cook. 2015. RACOG and
wRACOG: Two Probabilistic Oversampling Techniques. IEEE Transactions on
Knowledge and Data Engineering 27, 1 (2015), 222â€“234.
[9]Georgios Douzas and Fernando Bacao. 2019. Geometric SMOTE a geometrically
enhanced drop-in replacement for SMOTE. Information sciences 501 (2019),
118â€“135.
[10] Alberto FernÃ¡ndez, Salvador Garcia, Francisco Herrera, and Nitesh V Chawla.
2018. SMOTE for learning from imbalanced data: progress and challenges, mark-
ing the 15-year anniversary. Journal of artificial intelligence research 61 (2018),
863â€“905.
[11] Francisco FernÃ¡ndez-Navarro, CÃ©sar HervÃ¡s-MartÃ­nez, and Pedro Antonio GutiÃ©r-
rez. 2011. A dynamic over-sampling procedure based on sensitivity for multi-class
problems. Pattern Recognition 44, 8 (2011), 1821â€“1833.
[12] Zhen Gao, Maryam Zand, and Jianhua Ruan. 2019. A novel multiple classifier
generation and combination framework based on fuzzy clustering and individ-
ualized ensemble construction. In 2019 IEEE International Conference on Data
Science and Advanced Analytics (DSAA). IEEE, 231â€“240.
[13] Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. 2005. Borderline-SMOTE: A
New Over-Sampling Method in Imbalanced Data Sets Learning. In Advances
in Intelligent Computing, De-Shuang Huang, Xiao-Ping Zhang, and Guang-Bin
Huang (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 878â€“887.
[14] Haibo He, Yang Bai, Edwardo A. Garcia, and Shutao Li. 2008. ADASYN: Adaptive
synthetic sampling approach for imbalanced learning. In 2008 IEEE International
Joint Conference on Neural Networks (IEEE World Congress on Computational
Intelligence). 1322â€“1328.
[15] Haibo He and Edwardo A. Garcia. 2009. Learning from Imbalanced Data. IEEE
Transactions on Knowledge and Data Engineering 21, 9 (2009), 1263â€“1284.
[16] Thomas Ryan Hoens and Nitesh V. Chawla. 2012. Learning in non-stationary
environments with class imbalance. In Proceedings of the 18th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (Beijing, China)
(KDD â€™12). Association for Computing Machinery, New York, NY, USA, 168â€“176.
[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-
sification with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-
berger (Eds.), Vol. 25. Curran Associates, Inc.
[18] Miroslav Kubat, Robert C Holte, and Stan Matwin. 1998. Machine learning for
the detection of oil spills in satellite radar images. Machine learning 30 (1998),
195â€“215.
[19] Miroslav Kubat, Stan Matwin, et al .1997. Addressing the curse of imbalanced
training sets: one-sided selection. In ICML, Vol. 97. Citeseer, 179.
[20] Guillaume LemaÃ®tre, Fernando Nogueira, and Christos K. Aridas. 2017.
Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets
in Machine Learning. Journal of Machine Learning Research 18, 17 (2017), 1â€“5.
[21] David D. Lewis and Jason Catlett. 1994. Heterogeneous Uncertainty Sampling for
Supervised Learning. In Machine Learning Proceedings 1994, William W. Cohen
and Haym Hirsh (Eds.). Morgan Kaufmann, San Francisco (CA), 148â€“156.
[22] Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. 2008. Exploratory undersampling
for class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics,
Part B (Cybernetics) 39, 2 (2008), 539â€“550.
[23] David F Nettleton, Albert Orriols-Puig, and Albert Fornells. 2010. A study of
the effect of different types of noise on the precision of supervised learning
techniques. Artificial intelligence review 33 (2010), 275â€“306.
[24] Hien M Nguyen, Eric W Cooper, and Katsuari Kamei. 2011. Borderline over-
sampling for imbalanced data classification. International Journal of Knowledge
Engineering and Soft Data Paradigms 3, 1 (2011), 4â€“21.
[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825â€“2830.
[26] Luca Piras and Giorgio Giacinto. 2012. Synthetic pattern generation for im-
balanced learning in image retrieval. Pattern Recognition Letters 33, 16 (2012),
2198â€“2205.
[27] Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla. 2010. Consequences of Vari-
ability in Classifier Performance Estimates. In 2010 IEEE International Conference
on Data Mining. 421â€“430.
[28] Enislay Ramentol, YailÃ© Caballero, Rafael Bello, and Francisco Herrera. 2012.
Smote-rs b*: a hybrid preprocessing approach based on oversampling and un-
dersampling for high imbalanced data-sets using smote and rough sets theory.Knowledge and information systems 33 (2012), 245â€“265.
[29] Putthiporn Thanathamathee and Chidchanok Lursinsap. 2013. Handling im-
balanced data sets with synthetic boundary data generation using bootstrap
re-sampling and AdaBoost techniques. Pattern Recognition Letters 34, 12 (2013),
1339â€“1347.
[30] Shuo Wang, Leandro L. Minku, and Xin Yao. 2015. Resampling-Based Ensemble
Methods for Online Class Imbalance Learning. IEEE Transactions on Knowledge
and Data Engineering 27, 5 (2015), 1356â€“1368.
A TIME COMPLEXITY
The running time of CiFRUS is compared with Random Sampling
and SMOTE in Table 2. It should be noted that ğ‘Ÿâ‰¤ğ‘›in most practi-
cal scenarios. Random Sampling is faster than ours by a ğ‘‚(logğ‘›)
factor, and CiFRUS has a more favorable runtime than SMOTE for
a typical synthesis rate ğ‘Ÿâ‰¤ğ‘›
logğ‘›.
Table 2: Time complexity comparison
Fit Augment Combined
Sampling ğ‘‚(1)ğ‘‚(ğ‘šğ‘›ğ‘Ÿ)ğ‘‚(ğ‘šğ‘›ğ‘Ÿ)
SMOTE ğ‘‚(ğ‘šğ‘›2)ğ‘‚(ğ‘šğ‘›ğ‘Ÿ) (ğ‘šğ‘›2)
CiFRUS ğ‘‚(ğ‘šğ‘›logğ‘›)ğ‘‚(ğ‘šğ‘›ğ‘Ÿ logğ‘›)ğ‘‚(ğ‘šğ‘›ğ‘Ÿ logğ‘›)
It is important to consider that SMOTE and Random Sampling al-
gorithms have an altogether narrow scope of application compared
to our method. In these algorithms, the fitandaugment operations
are implemented jointly and expected to execute exactly once per
dataset. The effective runtime of fit-augment in SMOTE algorithms
isğ‘‚(ğ‘šğ‘›2)whenğ‘Ÿis constant and ğ‘›denotes the number of minority
instances. SMOTE and Random Sampling are also only applicable
to labeled data, whereas the augment operation of CiFRUS can be
executed as many times as needed for augmenting new batches of
unlabeled data. Furthermore, the initial execution of fit-augment
can be optimized to run in ğ‘‚(ğ‘šğ‘›logğ‘›)time for constant ğ‘Ÿby ex-
ploiting the fact that the feature value insertion locations in the
train set are known at the time of sorting.
B SUPPLEMENTARY RESULTS
Table 3 shows the overall ranks of the augmentation methods calcu-
lated for the F1-score, Kappa, MCC, and balanced-accuracy. Smaller
ranks correspond to high values of the performance metrics. For
each performance metric, the top-ranked method in each column
is marked with an asterisk. CiFRUS variants that outrank other
methods and the baseline (no augmentation) are highlighted in
blue. CiFRUS variants consistently outrank under/over-sampling,
SMOTE, and its successors, as well as the baseline for most classi-
fiers in the benchmark except GNB and LR.
Figure 9 shows the number of datasets for which each augmenta-
tion tools produce the highest cross-validation metric score. Figure
10 presents the average and variance rank of each augmentation
method for the classifiers GNB, KNN, MLP, and DT.
 
2521A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Geometric mean rank of augmentation methods
MetricA
ugmentation ClassifierGNB
KNN LR MLP
D
T
ADB
RFF1-scor
eBaseline
3.9 4.4 4.0 3.9 4.7 4.9 4.6
ROS 3.9 4.7 4.1 *3.5 5.0 4.1 4.2
RUS 4.4 4.9 4.7 4.7 4.5 6.5 4.4
SMOTE *3.5 4.3 *3.6 4.0 5.4 5.0 5.1
Bord.-SMOTE 4.2 5.3 5.7 4.5 6.1 5.3 5.5
SVM-SMOTE 4.4 4.9 4.2 4.6 4.9 4.6 4.7
ADASYN 4.1 5.3 4.6 4.4 5.6 5.2 4.8
G-SMOTE 4.7 3.9 4.5 4.3 4.4 5.2 4.5
CiFRUS
train 4.9 3.9 4.8 4.1 *3.1 4.0 4.0
train/test 6.0 4.3 5.1 3.9 3.3 3.6 4.1
bal.-train/test 6.2 *3.5 4.7 3.7 3.7 *2.9 *3.7K
appaBaseline
*3.7 4.0 3.7 3.8 4.5 4.6 4.8
ROS 4.4 4.7 4.0 3.6 4.8 4.5 4.3
RUS 4.6 5.2 5.1 5.4 6.6 7.2 6.1
SMOTE 3.8 4.7 *3.7 4.1 5.3 4.9 4.9
Bord.-SMOTE 4.8 6.1 5.9 4.4 6.2 5.6 5.2
SVM-SMOTE 4.1 4.9 3.8 4.2 4.7 4.6 4.1
ADASYN 4.5 5.8 5.0 4.2 5.3 5.2 4.9
G-SMOTE 4.8 4.0 4.4 4.3 4.0 4.9 4.4
CiFRUS
train 4.1 4.1 5.1 3.9 3.2 3.4 4.2
train/test 5.3 3.3 4.6 *3.5 *3.1 3.5 3.8
bal.-train/test 6.0 *3.2 4.9 4.0 3.4 *3.3 *3.2MCCBaseline
4.0 3.8 *3.6 3.9 4.6 4.4 4.5
ROS 4.4 5.0 4.1 3.7 4.8 4.6 4.4
RUS 4.5 5.4 5.0 5.2 6.0 7.0 5.7
SMOTE *3.9 4.5 3.8 4.2 5.4 5.3 5.1
Bord.-SMOTE 4.8 5.6 5.8 4.4 6.3 5.7 5.5
SVM-SMOTE 4.2 4.9 4.0 4.2 4.7 4.7 4.4
ADASYN 4.5 5.2 4.7 4.3 5.4 5.3 5.2
G-SMOTE 4.7 4.1 4.6 4.2 4.0 5.0 4.3
CiFRUS
train 4.0 4.5 5.1 4.0 3.3 3.5 3.9
train/test 5.5 3.4 4.5 *3.5 *3.1 3.3 3.6
bal.-train/test 5.6 *3.3 4.9 3.8 3.5 *3.0 *3.3balance
d accuracyBaseline
*4.0 5.5 5.1 4.2 5.1 6.2 6.3
ROS 4.1 4.7 3.7 3.8 5.4 4.0 4.5
RUS 4.4 4.5 4.1 3.9 4.0 5.1 *3.4
SMOTE 4.0 4.0 *3.4 4.2 5.1 4.8 4.8
Bord.-SMOTE 4.4 4.9 5.1 4.6 6.2 5.5 4.6
SVM-SMOTE 4.1 4.2 3.4 4.2 4.4 4.0 3.8
ADASYN 4.1 4.2 4.2 4.2 5.2 4.6 4.2
G-SMOTE 5.1 4.4 3.9 3.6 *3.6 5.8 4.3
CiFRUS
train 4.5 5.5 7.2 4.6 3.7 5.1 5.3
train/test 5.8 4.4 6.8 4.5 3.9 4.8 5.2
bal.-train/test 5.4 *3.2 4.1 *3.4 3.8 *2.0 3.6
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)10 13 12 12 5 7 10
11 6 6 14 5 10 5
9 8 8 13 16 5 17
13 8 10 10 4 6 5
9 5 5 10 4 6 5
6 5 9 8 6 7 5
8 6 7 11 4 7 8
11 9 7 8 7 5 10
9 13 11 12 13 10 11
4 12 11 14 13 15 13
3 12 7 14 10 20 10
F1-score
CiFRUS (any) outperforms
 or matches baseline34 42 44 49 52 50 46
CiFRUS (any) outperforms
or matches other methods20 35 30 33 35 41 29
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)10 11 15 13 7 8 7
6 7 7 14 6 9 6
7 9 5 11 3 6 8
10 5 8 10 4 8 6
8 5 5 11 4 7 7
9 7 10 12 6 6 8
9 5 8 12 6 6 6
12 8 7 11 12 8 13
11 11 8 15 10 14 11
7 14 12 14 13 13 11
5 14 8 10 16 13 15
Kappa
CiFRUS (any) outperforms
 or matches baseline34 41 40 47 51 47 48
CiFRUS (any) outperforms
or matches other methods25 36 31 33 38 36 31
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)11 9 11 13 6 6 7
7 7 9 13 5 8 8
8 10 8 13 14 10 20
9 8 10 9 5 6 4
8 5 6 11 4 5 9
9 7 13 10 7 8 6
11 8 9 12 6 7 5
10 7 8 13 12 5 11
13 8 4 13 9 6 8
4 10 4 10 9 8 8
4 18 10 16 10 29 12
balanced-accuracy
CiFRUS (any) outperforms
 or matches baseline39 42 42 47 49 53 50
CiFRUS (any) outperforms
or matches other methods26 31 19 32 26 37 24
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)7 13 15 13 7 8 8
7 6 7 13 6 8 6
8 8 6 11 4 7 8
10 6 8 10 4 7 6
6 5 5 11 4 7 6
7 7 9 12 6 6 7
6 6 9 12 6 6 6
13 8 7 11 12 8 13
12 10 7 14 10 12 11
5 13 11 14 13 14 12
4 14 9 12 15 15 15
MCC
CiFRUS (any) outperforms
 or matches baseline38 40 40 48 51 47 47
GNB KNN LR MLP DT ADB RFCiFRUS (any) outperforms
or matches other methods23 33 30 34 37 38 32
0.0 0.2 0.4 0.6 0.8 1.0
classifier0.00.20.40.60.81.0augmentationFigure 9: Number of datasets for which each augmentation
tool yielded the highest average cross-validation F1-score,
Kappa, balanced-accuracy, and MCC. The bottom two rows
below each plot show the number of datasets for which the
best-performing CiFRUS variant matched or exceeded the
performance on baseline datasets, and other augmentation
tools, respectively.
 
2522KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Sakhawat Hossain Saimon, Tanzira Najnin, and Jianhua Ruan
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)4.56 3.13 5.02 2.46
4.49 4.88 5.04 6.05
5.70 7.27 5.28 7.12
4.23 5.25 7.01 5.79
5.85 6.80 7.95 7.40
5.25 6.04 6.85 5.05
5.66 4.62 7.68 4.89
4.95 4.46 4.31 4.39
4.40 3.73 2.51 3.64
4.76 5.05 2.96 5.12
5.20 4.92 3.19 4.36AllImbalanced
 or sample-scarce
4.45 4.65 3.42 4.58
4.55 4.73 6.00 3.96
4.93 7.25 5.52 9.36
3.97 5.50 3.89 5.90
4.82 6.60 5.41 6.20
5.00 5.54 5.22 5.60
4.73 3.95 6.46 3.60
5.20 4.63 3.69 4.22
5.14 2.97 4.04 2.95
6.03 5.13 5.46 6.05
6.16 5.07 6.09 4.23AllImbalanced
 or sample-scarce
4.24 4.69 3.55 5.23
5.04 5.01 6.54 4.94
5.16 7.14 5.41 8.36
4.29 5.62 4.95 6.22
5.48 6.27 6.66 5.71
4.58 5.53 5.32 5.68
5.12 3.78 6.48 3.50
5.30 4.14 4.41 3.63
4.37 3.07 3.21 2.74
5.30 5.60 4.26 6.09
6.04 5.13 4.65 4.32AllImbalanced
 or sample-scarce
4.60 4.38 3.48 5.09
4.77 5.21 6.02 5.33
4.91 6.98 4.78 8.62
4.59 5.71 5.69 5.97
4.99 6.56 5.74 5.84
4.61 5.27 5.41 4.78
4.71 4.19 6.30 4.07
5.64 4.17 4.96 3.24
4.73 2.90 3.98 2.92
5.79 5.27 4.97 5.74
5.44 5.39 3.65 4.63AllImbalanced
 or sample-scarce
4.52 4.46 3.60 5.14
4.96 5.05 6.60 5.17
5.04 6.85 5.24 7.88
4.34 5.55 5.19 5.88
5.42 6.33 6.56 5.54
4.63 5.36 5.50 5.10
5.13 4.02 6.44 3.63
5.22 4.20 3.86 3.60
4.24 3.27 2.87 2.84
5.59 5.47 4.67 6.41
5.68 5.17 5.12 4.82AllImbalanced
 or sample-scarce
GNB
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)4.31 3.08 5.73 3.59
6.24 5.26 7.10 6.38
4.94 4.92 6.33 4.48
4.79 4.74 4.47 4.83
6.24 5.63 5.50 5.42
5.11 5.66 5.62 6.20
5.66 5.06 5.30 5.23
4.43 4.46 4.54 4.11
7.92 7.74 7.27 6.64
3.82 4.83 2.88 4.37
3.00 4.62 2.24 3.845.03 3.71 4.82 3.24
5.38 5.41 6.29 5.47
5.59 5.10 8.16 5.06
4.69 5.39 6.70 6.25
5.86 5.56 6.65 5.51
5.55 5.97 6.17 5.60
6.20 4.03 7.78 3.87
4.42 4.43 3.68 3.45
4.17 5.53 2.70 5.77
4.51 5.19 2.80 6.14
3.82 4.88 2.74 4.914.60 3.71 5.03 3.56
5.30 5.32 5.51 5.44
5.73 5.23 7.85 5.44
5.16 4.84 7.05 4.73
6.75 5.32 6.57 4.14
5.28 5.58 5.81 4.73
6.87 4.04 8.16 3.49
4.41 5.03 4.46 4.54
4.52 5.75 3.21 7.18
3.72 5.44 2.32 6.69
3.46 4.85 2.52 5.386.29 3.23 7.56 2.92
5.30 5.47 5.37 5.87
5.00 5.22 6.28 5.44
4.31 4.93 5.14 4.73
5.46 5.67 5.25 4.69
4.70 6.09 4.95 6.34
4.93 4.32 4.63 3.78
4.94 4.87 5.21 4.06
5.86 5.56 5.42 6.73
4.94 4.96 3.86 5.90
3.50 5.02 2.32 4.984.38 3.70 4.67 3.78
5.67 5.26 6.51 5.18
5.93 5.09 8.22 5.33
4.85 4.97 6.12 5.00
6.23 5.68 6.30 4.91
5.28 5.77 5.68 5.06
6.11 4.24 6.68 3.75
4.60 4.83 4.55 4.12
4.96 5.52 3.67 6.34
3.87 5.38 2.36 6.50
3.58 4.65 2.78 4.85
KNN
Baseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)4.99 5.01 3.98 4.96
4.90 4.95 3.65 4.55
6.28 7.22 6.02 7.44
6.02 5.21 6.28 5.25
5.99 5.35 5.65 5.51
5.37 4.91 5.71 5.07
5.91 5.34 4.72 5.48
5.50 5.17 4.90 5.50
5.05 5.04 6.31 4.07
3.28 3.32 4.69 2.91
3.19 4.59 4.16 5.804.99 4.68 3.87 5.56
4.47 4.59 3.72 4.10
5.67 5.65 7.92 5.96
5.12 5.20 5.33 5.78
5.68 4.06 5.21 3.79
5.71 4.53 5.55 4.47
5.49 4.84 4.84 4.87
5.23 5.79 5.72 5.63
4.68 5.41 4.29 5.14
4.45 5.28 5.23 4.96
4.05 5.45 4.62 5.494.91 4.61 4.52 6.23
4.70 4.66 4.47 4.24
6.65 5.74 6.28 5.83
5.21 5.36 5.54 5.47
5.54 4.31 5.12 3.96
5.15 4.52 5.01 4.34
5.35 4.91 4.80 5.25
5.38 5.60 5.41 5.54
4.50 5.16 5.09 5.03
3.96 5.03 4.53 4.66
4.39 5.51 4.67 5.195.47 4.44 5.41 6.12
4.92 4.62 4.75 4.47
4.82 6.05 3.84 6.23
5.40 5.39 5.53 5.34
5.79 4.24 6.10 3.79
5.22 4.53 5.48 4.35
5.43 5.09 4.85 5.78
4.47 5.76 3.63 5.60
5.28 5.00 6.57 4.72
5.14 4.74 6.30 4.43
3.70 5.67 3.79 5.045.00 4.63 4.64 6.03
4.74 4.57 4.56 4.30
6.36 5.75 6.17 5.92
5.38 5.35 5.56 5.54
5.59 4.31 5.07 3.98
5.22 4.44 5.10 4.33
5.43 4.85 4.91 5.31
5.24 5.53 5.16 5.04
4.60 5.19 5.12 5.10
3.95 5.24 4.56 4.82
4.20 5.55 4.53 5.32
MLP
Avg Var Avg Var
AUC rankBaseline
ROS
RUS
SMOTE
Borderline-SMOTE
SVM-SMOTE
ADASYN
G-SMOTE
CiFRUS (train)
CiFRUS (train/test)
CiFRUS (bal.-train/test)6.86 4.78 7.43 3.42
6.92 4.56 7.97 4.78
5.63 5.83 4.29 5.65
6.49 5.17 5.58 6.27
7.44 5.07 7.74 3.95
5.70 4.76 4.58 4.84
6.79 5.74 6.53 6.33
4.93 4.98 3.77 5.85
5.26 5.31 6.48 5.24
1.85 4.41 2.17 4.94
1.83 4.20 2.29 3.77
Avg Var Avg Var
F1-score rank5.25 5.17 4.45 4.45
5.57 4.59 5.32 4.27
5.02 5.71 7.21 6.39
6.02 5.36 6.04 6.98
6.84 5.41 7.38 4.32
5.50 4.57 4.53 5.09
6.47 4.91 6.61 5.53
4.77 4.48 4.44 4.36
3.33 5.30 2.88 4.60
3.40 4.97 3.35 3.90
3.85 4.29 4.11 4.95
Avg Var Avg Var
Kappa rank5.11 4.84 4.79 3.73
5.35 4.72 5.28 4.76
7.43 5.34 8.95 4.40
5.94 5.36 5.22 6.58
6.87 5.26 7.27 4.53
5.25 4.54 3.81 5.16
6.17 5.38 6.18 5.72
4.34 4.58 3.36 5.33
3.40 5.32 3.62 4.71
3.12 5.50 3.61 5.06
3.61 3.97 4.49 4.57
Avg Var Avg Var
balanced-accuracy rank5.78 4.44 6.07 3.22
6.07 4.75 6.81 4.75
4.50 5.80 3.51 5.38
5.72 5.28 4.71 6.16
6.86 5.14 7.02 4.02
4.95 4.56 3.42 4.79
6.01 5.45 5.83 6.16
3.87 4.97 2.88 6.18
3.94 5.01 5.06 5.01
3.96 5.34 5.74 5.00
4.02 4.08 4.97 4.29
Avg Var Avg Var
MCC rank5.17 4.75 4.94 3.51
5.39 4.66 5.47 4.69
6.70 5.49 7.73 4.71
6.06 5.43 5.37 6.59
7.00 5.24 7.38 4.41
5.30 4.45 3.83 4.93
6.21 5.29 6.26 5.82
4.30 4.62 3.28 5.41
3.44 5.34 3.64 4.93
3.16 5.56 3.70 5.06
3.67 4.00 4.55 4.59
DT
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0augmentation
Figure 10: Augmentation methods ranked by average and variance of each performance metric using GNB, KNN, MLP, and DT
classifiers. Each augmentation method was ranked on metric average and variance over 10 repeats of stratified cross-validation.
The overall rank is determined using the geometric mean of ranks from individual datasets.
 
2523