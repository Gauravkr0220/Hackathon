A Survey of Large Language Models for Graphs
Xubin Ren
University of Hong Kong
Hong Kong, China
xubinrencs@gmail.comJiabin Tang
University of Hong Kong
Hong Kong, China
jiabintang77@gmail.comDawei Yin
Baidu Inc.
Beijing, China
yindawei@acm.org
Nitesh Chawla
University of Notre Dame
Indiana, USA
nchawla@nd.eduChao Huang‚àó
University of Hong Kong
Hong Kong, China
chaohuang75@gmail.com
ABSTRACT
Graphs are an essential data structure utilized to represent rela-
tionships in real-world scenarios. Prior research has established
that Graph Neural Networks (GNNs) deliver impressive outcomes
in graph-centric tasks, such as link prediction and node classifica-
tion. Despite these advancements, challenges like data sparsity and
limited generalization capabilities continue to persist. Recently,
Large Language Models (LLMs) have gained attention in natu-
ral language processing. They excel in language comprehension
and summarization. Integrating LLMs with graph learning tech-
niques has attracted interest as a way to enhance performance in
graph learning tasks. In this survey, we conduct an in-depth re-
view of the latest state-of-the-art LLMs applied in graph learning
and introduce a novel taxonomy to categorize existing methods
based on their framework design. We detail four unique designs:
i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integra-
tion, and iv) LLMs-Only, highlighting key methodologies within
each category. We explore the strengths and limitations of each
framework, and emphasize potential avenues for future research, in-
cluding overcoming current integration challenges between LLMs
and graph learning techniques, and venturing into new applica-
tion areas. This survey aims to serve as a valuable resource for re-
searchers and practitioners eager to leverage large language models
in graph learning, and to inspire continued progress in this dynamic
field. We consistently maintain the related open-source materials
athttps://github.com/HKUDS/Awesome-LLM4Graph-Papers.
CCS CONCEPTS
‚Ä¢General and reference ‚ÜíSurveys and overviews; ‚Ä¢Informa-
tion systems ‚ÜíData mining; Language models; ‚Ä¢Mathemat-
ics of computing ‚ÜíGraph algorithms.
‚àóCorresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671460KEYWORDS
Large Language Models, Graph Learning
ACM Reference Format:
Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, and Chao Huang. 2024.
A Survey of Large Language Models for Graphs. In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, Barcelona, Spain, 11 pages.
https://doi.org/10.1145/3637528.3671460
1 INTRODUCTION
Graphs, comprising nodes and edges that signify relationships, are
essential for illustrating real-world connections across various do-
mains. These include social networks [ 39,52], molecular graphs [ 4],
recommender systems [ 23,59], and academic networks [ 27]. This
structured data form is integral in mapping complex interconnec-
tions relevant to a wide range of applications.
In recent years, Graph Neural Networks (GNNs) [ 79] have emerged
as a powerful tool for a variety of tasks, including node classifica-
tion [ 82] and link prediction [ 89]. By passing and aggregating infor-
mation across nodes and iteratively refining node features through
supervised learning, GNNs have achieved remarkable results in
capturing structural nuances and enhancing model accuracy. To
accomplish this, GNNs leverage graph labels to guide the learning
process. Several notable models have been proposed in the litera-
ture, each with its own strengths and contributions. For instance,
Graph Convolutional Networks (GCNs) [ 34] have been shown to
be effective in propagating embeddings across nodes, while Graph
Attention Networks (GATs) [ 67] leverage attention mechanisms to
perform precise aggregation of node features. Additionally, Graph
Transformers [ 35,86] employ self-attention and positional encod-
ing to capture global signals among the graph, further improving
the expressiveness of GNNs. To address scalability challenges in
large graphs, methods such as Nodeformer [ 77] and DIFFormer [ 76]
have been proposed. These approaches employ efficient attention
mechanisms and differentiable pooling techniques to reduce com-
putational complexity while maintaining high levels of accuracy.
Despite these advancements, current GNN methodologies still face
several challenges. For example, data sparsity remains a signifi-
cant issue, particularly in scenarios where the graph structure is
incomplete or noisy [ 85]. Moreover, the generalization ability of
GNNs to new graphs or unseen nodes remains an open research
question, with recent works highlighting the need for more robust
and adaptive models [17, 80, 93].
6616
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang
Large Language Models (LLMs) [ 96], which show great gener-
alization abilities for unseen tasks [ 12,56,72], have emerged as
powerful tools in various research fields, including natural lan-
guage processing [ 1], computer vision [ 43,44], and information
retrieval [ 26,41,99]. The advent of LLMs has sparked significant
interest within the graph learning community [ 29,33,37], prompt-
ing investigations into the potential of LLMs to enhance perfor-
mance on graph-related tasks. Researchers have explored various
approaches to leverage the strengths of LLMs for graph learning,
resulting in a new wave of methods that combine the power of
LLMs with graph neural networks. One promising direction is to
develop prompts that enable LLMs to understand graph structures
and respond to queries effectively. For instance, approaches such
as InstructGLM [ 84] and NLGraph [ 68] have designed specialized
prompts that allow LLMs to reason over graph data and generate
accurate responses. Alternatively, other methods have integrated
GNNs to feed tokens into the LLMs, allowing them to understand
graph structures more directly. For example, GraphGPT [ 63] and
GraphLLM [ 5] use GNNs to encode graph data into tokens, which
are then fed into the LLMs for further processing. This synergy
between LLMs and GNNs has not only improved task performance
but also demonstrated impressive zero-shot generalization capabili-
ties, where the models can accurately answer queries about unseen
graphs or nodes.
In this survey, we offer a systematic review of the advancements
in Large Language Models (LLMs) for graph applications, and we
explore potential avenues for future research. Unlike prior surveys
that categorize studies based on the role of LLMs [ 33,37] or fo-
cus primarily on integrating LLMs with knowledge graphs [ 53],
our work highlights the model framework design, particularly the
inference and training processes, to distinguish between existing
taxonomies. This perspective allows readers to gain a deeper under-
standing of how LLMs effectively address graph-related challenges.
We identify and discuss four distinct architectural approaches: i)
GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and
iv)LLMs-Only, each illustrated with representative examples. In
summary, the contributions of our work can be summarized as:
‚Ä¢Comprehensive Review of LLMs for Graph Learning. We
offer a comprehensive review of the current state-of-the-art Large
Language Models (LLMs) for graph learning, elucidating their
strengths and pinpointing their limitations.
‚Ä¢Novel Taxonomy for Categorizing Research. We introduce
a novel taxonomy for categorizing existing research based on
their framework design, which provides a deeper insight into
how LLMs can be seamlessly integrated with graph learning.
‚Ä¢Future Research Avenues. We also explore potential avenues
for future research, including addressing the prevalent challenges
in merging LLMs with graph learning methods and venturing
into novel application areas.
2 PRELIMINARIES AND TAXONOMY
In this section, we first provide essential background knowledge
on large language models and graph learning. Then, we present
our taxonomy of large language models for graphs.2.1 Definitions
Graph-Structured Data. In computer science, a graph G=(V,E)
is a non-linear data structure that consists of a set of nodes V, and a
set of edgesEconnecting these nodes. Each edge ùëí‚ààEis associated
with a pair of nodes (ùë¢, ùë£), where ùë¢andùë£are the endpoints of the
edge. The edge may be directed, meaning it has a orientation from
ùë¢toùë£, or undirected, meaning it has no orientation. Furthermore,
A Text-Attributed Graph (TAG) is a graph that assigns a sequential
text feature (i.e., sentence) to each node, denoted as tùë£, which is
widely used in the era of large language models. The text-attributed
graph can be formally represented as GùëÜ=(V,E,T), whereTis
the set of text features.
Graph Neural Networks (GNNs) are deep learning architectures
for graph-structured data that aggregate information from neigh-
boring nodes to update node embeddings. Formally, the update of a
node embedding hùë£‚ààRùëëin each GNN layer can be represented as:
h(ùëô+1)
ùë£=ùúì(ùúô({h(ùëô)
ùë£‚Ä≤:ùë£‚Ä≤‚ààN( ùë£)}),h(ùëô)
ùë£), (1)
where ùë£‚Ä≤‚ààN( ùë£)denotes a neighbor node of ùë£, and ùúô(¬∑)andùúì(¬∑)
are aggregation and update functions, respectively. By stacking ùêø
GNN layers, the final node embeddings can be used for downstream
graph-related tasks such as node classification and link prediction.
Large Language Models (LLMs). Language Models (LMs) is a
statistical model that estimate the probability distribution of words
for a given sentence. Recent research has shown that LMs with bil-
lions of parameters exhibit superior performance in solving a wide
range of natural language tasks (e.g., translation, summarization
and instruction following), making them Large Language Models
(LLMs). In general, most recent LLMs are built with transformer
blocks that use a query-key-value (QKV)-based attention mecha-
nism to aggregate information in the sequence of tokens. Based on
the direction of attention, LLMs can be categorized into two types
(given a sequence of tokens x=[ùë•0, ùë•1, ..., ùë• ùëõ]):
‚Ä¢Masked Language Modeling (MLM) . Masked Language Mod-
eling is a popular pre-training objective for LLMs that involves
masking out certain tokens in a sequence and training the model
to predict the masked tokens based on the surrounding context.
Specifically, the model takes into account both the left and right
context of the masked token to make accurate predictions:
ùëù(ùë•ùëñ|ùë•0, ùë•1, ..., ùë• ùëõ). (2)
Representative models include BERT [12] and RoBERTa [47].
‚Ä¢Causal Language Modeling (CLM). Causal Language Modeling
is another popular training objective for LLMs that involves
predicting the next token in a sequence based on the previous
tokens. Specifically, the model only considers the left context of
the current token to make accurate predictions:
ùëù(ùë•ùëñ|ùë•0, ùë•1, ..., ùë• ùëñ‚àí1) (3)
Notable examples include the GPT (e.g., ChatGPT) and Llama [ 66].
2.2 Taxonomy
In this survey, we present our taxonomy focusing on the model in-
ference pipeline that processes both graph data and text with LLMs.
Specifically, we summarize four main types of model architecture
design for large language models for graphs, as follows:
6617A Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, SpainLarge
Language Models for GraphsGNNs as PrefixNode-level TokenizationGprahGPT
[63], HiGPT [64], GraphTranslator [88], UniGraph [25],
GIMLET [92], XRec [51]
Graph-level TokenizationGraphLLM
[5], GIT-Mol [45], MolCA [48], InstructMol [4],
G-Retriever [24], GNP [65]
LLMs as PrefixEmbs. from LLMs for GNNs G-Pr
ompt [30], SimTeG [14], GALM [81], OFA [42], TAPE [22], LLMRec [73]
Labels from LLMs for GNNs Op
enGraph [80], LLM-GNN [9], GraphEdit [21], RLMRec [58]
LLMs-Graphs IntergrationAlignment between GNNs and LLMsMoMu
[60], ConGraT [3], G2P2 [74], GRENADE [36], MoleculeSTM [46],
THLM [100], GLEM [94]
Fusion Training of GNNs and LLMs Gr
easeLM [90], DGTL [54], ENGINE [98], GraphAdapter [31]
LLMs Agent for Graphs Pangu
[19], Graph Agent [71], FUXI [18], Readi [10], RoG [49]
LLMs-OnlyTuning-freeNLGraph
[68], GPT4Graph [20], Beyond Text [28], Graph-LLM [8], GraphText [95],
Talk like a Graph [15], LLM4DyG [91], GraphTMI [11], Ai et al. [2]
Tuning-requiredInstructGLM
[84], WalkLM [62], LLaGA [7], InstructGraph [69], ZeroG [38],
GraphWiz [6], GraphInstruct [50], MuseGraph [61]
Figure 1: The proposed taxonomy ofLarge Language Models (LLMs) forgraphs, featuring representativ eworks.
‚Ä¢GNNs asPrefix. GNNs serveasthefirst comp onent toprocess
graph data andprovide structur e-awar etokens (e.g.,node-le vel,
edge-le vel,orgraph-le veltokens) forLLMs forinfer ence.
‚Ä¢LLMs asPrefix. LLMs firstprocessgraph data with textual infor-
mation andthen provide nodeembeddings orgenerate dlabels
forimpr ovedtraining ofgraph neural netw orks.
‚Ä¢LLMs-Graphs Intergration. Inthisline,LLMs achie veahigher
levelofintegration with graph data, such asfusion training or
alignment with GNNs, andalsobuild LLM-base dagents tointer-
actwith graph information.
‚Ä¢LLMs-Only. This linedesigns practical prompting metho dsto
ground graph-structur eddata intosequences ofwordsforLLMs
toinfer ,while some alsoincorp orate multi-mo daltokens.
3LARGE LANGU AGEMODELS FOR GRAPHS
3.1 GNNs asPrefix
Inthissection, wediscuss theapplication ofgraph neural netw orks
(GNNs) asstructural enco ders toenhance theunderstanding of
graph structur esbyLLMs, therebybenefiting various downstr eam
tasks, i.e.,GNNs asPrefix. Inthese metho ds,GNNs generally play
theroleofatokenizer ,enco ding graph data into agraph token
sequence richinstructural information, which isthen input into
LLMs toalign with natural language .These metho dscangener-
allybedivide dintotwocategories: i)Node-le velTokenization :each
nodeofthegraph structur eisinput into theLLM, aiming tomake
theLLM understand fine-graine dnode-le velstructural informa-
tion anddistinguish relationships. ii)Graph-le velTokenization :the
graph iscompr essedintoafixed-length token sequence using aspe-
cific pooling metho d,aiming tocaptur ehigh-le velglobal semantic
information ofthegraph structur e.
3.1.1 Node-le velTokenization. Forsome downstr eam tasks in
graph learning, such asnodeclassification and link prediction,
themodelneedstomodelthefine-graine dstructural information
atnodelevel,anddistinguish thesemantic differ ences between
differ entnodes.Traditional GNNs usually enco deaunique repre-
sentation foreach nodebasedontheinformation ofneighb oring
nodes,anddirectlyperform downstr eam nodeclassification orlinkprediction. Inthisline,thenode-le veltokenization metho disuti-
lized,which canretain theunique structural representation ofeach
nodeasmuch aspossible ,therebybenefiting downstr eam tasks.
Within this line,GraphGPT [63]proposes toinitially align
thegraph enco derwith natural language semantics through text-
graph grounding, andthen combine thetraine dgraph enco der
with theLLM using aprojector.Through thetwo-stage instruc-
tion tuning paradigm, themodelcandirectly complete various
graph learning downstr eam tasks with natural language ,thus per-
form strong zero-shot transferability andmulti-task compatibil-
ity.TheproposedChain-of- Thought distillation metho dempowers
GraphGPT tomigrate tocomple xtasks with small parameter sizes.
Then, HiGPT [64]proposes tocombine thelanguage-enhance d
in-conte xtheter ogene ousgraph tokenizer with LLMs, solving the
challenge ofrelation typeheter ogeneity shift betweendiffer ent
heter ogene ousgraphs. Meanwhile ,thetwo-stage heter ogene ous
graph instruction-tuning injectsbothhomogeneity andheter ogene-
ityawar eness into theLLM. And theMixtur e-of- Thought (MoT)
metho dcombine dwith various prompt engine ering further solves
thecommon data scarcityproblem inheter ogene ousgraph learn-
ing.GIMLET [92],asaunifie dgraph-te xtmodel,leverages natural
language instructions toaddressthelabelinsufficiency challenge
inmole cule-r elate dtasks, effectivelyalleviating thereliance on
expensiv elabexperiments fordata annotation. Itemplo ysagen-
eralize dposition embedding andattention mechanism toenco de
bothgraph structur esandtextual instructions asaunifie dtoken
combination thatisfedintoatransformer decoder.GraphT ransla-
tor[88]proposes theuseofatranslator with sharedself-attention
toalign boththetarget nodeandinstruction, andemplo yscross
attention tomap thenoderepresentation enco dedbythegraph
modeltofixed-length semantic tokens. Theproposeddaul-phase
training paradigm empowerstheLLM tomake predictions based
onlanguage instructions, providing aunifie dsolution forboth
pre-define dandopen-ende dgraph-base dtasks. Instead ofusing
pre-compute dnodefeatur esofvarying dimensions, UniGraph [25]
leverages Text-Attribute dGraphs forunifying noderepresentations,
featuring acascade darchite ctureoflanguage models andgraph
6618KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang
GNNsGraphs
What is this node ? nodeLLMsThis node ‚Ä¶Node -level
What is this graph ? graphLLMsThis graph ‚Ä¶Graph- level
Figure 2: GNNs as Prefix.
neural networks as backbone networks. In recent research on rec-
ommendation systems, XRec [51] has been proposed as a method
that utilizes the encoded user/item embeddings from graph neural
networks as collaborative signals. These signals are then integrated
into each layer of large language models, enabling the generation
of explanations for recommendations, even in zero-shot scenarios.
3.1.2 Graph-level Tokenization. On the other hand, to adapt
to other graph-level tasks, models need to be able to extract global
information from node representations, to obtain high-level graph
semantic tokens. In the method of GNN as Prefix, Graph-level tok-
enization abstracts node representations into unified graph repre-
sentations through various "pooling" operations, further enhancing
various downstream tasks.
Within this domain, GraphLLM [5] utilizes a graph transformer
that incorporates the learnable query and positional encoding to en-
code the graph structure and obtain graph representations through
pooling. These representations are directly used as graph-enhanced
prefix for prefix tuning in the LLM, demonstrating remarkable effec-
tiveness in fundamental graph reasoning tasks. MolCA [48] with
Cross-Modal Projector and Uni-Modal Adapter is a method that
enables a language model to understand both text- and graph-based
molecular contents through the proposed dual-stage pre-training
and fine-tuning stage. It employs a cross-modal projector imple-
mented as a Q-Former to connect a graph encoder‚Äôs representation
space and a language model‚Äôs text space, and a uni-modal adapter
for efficient adaptation to downstream tasks. InstructMol [4] in-
troduces a projector that aligns the molecular graph encoded by
the graph encoder with the molecule‚Äôs Sequential information and
natural language instructions, with the first stage of Alignment
Pretraining and the second stage of Task-specific Instruction Tun-
ing enabling the model to achieve excellent performance in vari-
ous drug discovery-related molecular tasks. GIT-Mol [45] further
unifies the graph, text, and image modalities through interaction
cross-attention between different modality encoders, and aligns
these three modalities, enabling the model to simultaneously per-
form four downstream tasks: captioning, generation, recognition,
and prediction. GNP [65] employs cross-modality pooling to in-
tegrate the node representations encoded by the graph encoder
with the natural language tokens, resulting in a unified graph rep-
resentation. This representation is aligned with the instruction
through the LLM to demonstrate superiority in commonsense and
biomedical reasoning tasks. Recently, G-Retriever [24] utilizes
retrieval-augmented techniques to obtain subgraph structures. It
completes various downstream tasks in GraphQA (Graph Question
Answering) through the collaboration of graph encoder and LLMs.3.1.3 Discussion. The GNN as Prefix approach aligns the mod-
eling capability of GNNs with the semantic modeling capability
of LLMs, demonstrating unprecedented generalization, i.e., zero-
shot capability, in various graph learning downstream tasks and
real-world applications. However, despite the effectiveness of the
aforementioned approach, the challenge lies in whether the GNN
as Prefix method remains effective for non-text-attributed graphs.
Additionally, the optimal coordination between the architecture
and training of GNNs and LLMs remains an unresolved question.
3.2 LLMs as Prefix
The methods presented in this section leverage the information
produced by large language models to improve the training of
graph neural networks. This information includes textual content,
labels, or embeddings derived from the large language models.
These techniques can be categorized into two distinct groups: i)
Embeddings from LLMs for GNNs, which involves using embeddings
generated by large language models for graph neural networks, and
ii)Labels from LLMs for GNNs, which involves integrating labels
generated by large language models for graph neural networks.
3.2.1 Embeddings from LLMs for GNNs. The inference pro-
cess of graph neural networks involves passing node embeddings
through the edges and then aggregating them to obtain the next-
layer node embeddings. In this process, the initial node embeddings
are diverse across different domains. For instance, ID-based em-
beddings in recommendation systems or bag-of-words embeddings
in citation networks can be unclear and non-diverse. Sometimes,
the poor quality of initial node embeddings can result in subop-
timal performance of GNNs. Furthermore, the lack of a universal
design for node embedders makes it challenging to address the
generalization capability of GNNs in unseen tasks with different
node sets. Fortunately, the works in this line leverage the power-
ful language summarization and modeling capabilities of LLMs to
generate meaningful and effective embeddings for GNNs‚Äô training.
In this domain, G-Prompt [30] adds a GNN layer at the end of
a pre-trained language models (PLMs) to achieve graph-aware fill-
masking self-supervised learning. By doing so, G-Prompt can gen-
erate task-specific, explainable node embeddings for downstream
tasks using prompt tuning. SimTeG [14] first leverages parameter-
efficient fine-tuning on the text embeddings obtained by LLMs for
downstream tasks (e.g., node classification). Then, the node embed-
dings are fed into GNNs for inference. Similarly, GALM [81] utilizes
BERT as a pre-language model to encode text embeddings for each
node. Then, the model is pre-trained through unsupervised learning
tasks, such as link prediction, to minimize empirical loss and find
optimal model parameters, which enables GALM to be applied for
various downstream tasks. Recently, OFA [42] leverages LLMs to
unify graph data from different domains into a common embedding
space for cross-domain learning. It also uses LLMs to encode task-
relevant text descriptions for constructing prompt graphs, allowing
the model to perform specific tasks based on context. TAPE [22]
uses customized prompts to query LLMs, generating both prediction
and text explanation for each node. Then, DeBERTa is fine-tuned
to convert the text explanations into node embeddings for GNNs.
Finally, GNNs can use a combination of the original text features,
explanation features, and prediction features to predict node labels.
6619A Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Textual
Data‚äï
GraphsLLMs Embedder
input
Embs . from LLMs
for GNNs
LLMs Labeler
Labels from LLMs
for GNNsLabelstrain
Figure 3: LLMs as Prefix.
In the field of recommendation, LLMRec [73] achieves graph aug-
mentation on user-item interaction data using GPT-3.5, which not
only filters out noise interactions and adds meaningful training
data, but also enriches the initial node embeddings for users and
items with generated rich textual profiles, ultimately improving the
performance of recommenders.
3.2.2 Labels from LLMs for GNNs. Another approach leverages
the generated labels from large language models as supervision
to improve the training of graph neural networks. Notably, the
supervised labels in this context are not limited to categorized
labels in classification tasks, but can take various forms such as
embeddings, graphs, and more. The generated information from
the LLMs is not used as input to the GNNs, but rather forms the
supervision signals for better optimization, which enables GNNs
to achieve higher performance on various graph-related tasks.
Follow this line, OpenGraph [80] employs LLMs to generate
nodes and edges, mitigating the issue of sparse training data. The
generation process for nodes and edges is refined using the Gibbs
sampling algorithm and a tree-of-prompt strategy, which is then
utilized to train the graph foundation model. LLM-GNN [9] lever-
ages LLMs as annotators to generate node category predictions
with confidence scores, which serve as labels. Post-filtering is then
employed to filter out low-quality annotations while maintaining
label diversity. Finally, the generated labels are used to train GNNs.
GraphEdit [21] leverages the LLMs to build an edge predictor,
which is used to evaluate and refine candidate edges against the
original graph‚Äôs edges. In recommender systems, RLMRec [58]
leverages LLMs to generate text descriptions of user/item prefer-
ences. These descriptions are then encoded as semantic embeddings
to guide the representation learning of ID-based recommenders
using contrastive and generative learning techniques [57].
3.2.3 Discussion. Despite the progress made by the aforemen-
tioned methods in enhancing graph learning performance, a lim-
itation persists in their decoupled nature, where LLMs are not
co-trained with GNNs, resulting in a two-stage learning process.
This decoupling is often due to computational resource limitations
arising from the large size of the graph or the extensive parameters
of LLMs. Consequently, the performance of the GNNs is heavily
dependent on the pre-generated embeddings/labels of LLMs or even
the design of task-specific prompts.3.3 LLMs-Graphs Intergration
The methods introduced in this section aim to further integrate
large language models with graph data, encompassing various
methodologies that enhance not only the ability of LLMs to tackle
graph tasks but also the parameter learning of GNNs. These works
can be categorized into three types: i) Fusion Training of GNNs
and LLMs, which aims to achieve fusion-co-training of the parame-
ters of both models; ii) Alignment between GNNs and LLMs, which
focuses on achieving representation or task alignment between
the two models; and iii) LLMs Agent for Graphs, which builds an
autonomous agent based on LLMs to plan and solve graph tasks.
3.3.1 Alignment between GNNs and LLMs. In general, GNNs
and LLMs are designed to handle different modalities of data, with
GNNs focusing on structural data and LLMs focusing on textual
data. This results in different feature spaces for the two models. To
address this issue and make both modalities of data more beneficial
for the learning of both GNNs and LLMs, several methods use tech-
niques such as contrastive learning or Expectation-Maximization
(EM) iterative training to align the feature spaces of the two models.
This enables better modeling of both graph and text information,
resulting in improved performance on various tasks.
Within this topic, MoMu [60] is a multimodal molecular foun-
dation model that includes two separate encoders, one for handling
molecular graphs (GIN) and another for handling text data (BERT).
It uses contrastive learning to pre-train the model on a dataset
of molecular graph-text pairs. This approach enables MoMu to
directly imagine new molecules from textual descriptions. Also
in the bioinfo domain, MoleculeSTM [46] combines the chemi-
cal structure information of molecules (i.e., molecular graph) with
their textual descriptions (i.e., SMILES strings), and uses a con-
trastive learning to jointly learn the molecular structure and textual
descriptions. It show great performance on multiple benchmark
tests, including structure-text retrieval, text-based editing tasks,
and molecular property prediction. Similarly, in ConGraT [3], a
contrastive graph-text pretraining technique is proposed to align
the node embeddings encoded by LMs and GNNs simultaneously.
The experiments are conducted on social networks, citation net-
works, and link networks, and show great performance on node
and text classification as well as link prediction tasks. Furthermore,
G2P2 [74,75] enhances graph-grounded contrastive pre-training
by proposing three different types of alignment: text-node, text-
summary, and node-summary alignment. This enables G2P2 to
leverage the rich semantic relationships in the graph structure to
improve text classification performance in low-resource environ-
ments. GRENADE [36] is a graph-centric language model that
proposes graph-centric contrastive learning and knowledge align-
ment to achieve both node-level and neighborhood-level alignment
based on the node embeddings encoded from GNNs and LMs. This
enables the model to capture text semantics and graph structure
information through self-supervised learning, even in the absence
of human-annotated labels. In addition to contrastive learning,
THLM [100] leverages BERT and HGNNs to encode node embed-
dings and uses a positive-negative classification task with negative
sampling to improve the alignment of embeddings from two differ-
ent modalities. Recently, GLEM [94] adopts an efficient and effec-
tive solution that integrates graph structure and language learning
6620KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang
answerFusion
ModelGraphsStructure Info.
Textual Info.GNNs
LLMsalignAlignment  between GNNs and LLMs
GNNs
LLMs‚äï
Graphstrain onFusion Training of GNNs and LLMs 
LLM
AgentMemory
ToolsGraph
Questions?
planningLLMs Agent  for Graphs
Figure 4: LLMs-Graphs Intergration.
through a variational expectation-maximization (EM) framework.
By iteratively using LMs and GNNs to provide labels for each other
in node classification, GLEM aligns their capabilities in graph tasks.
3.3.2 Fusion Training of GNNs and LLMs. Although align-
ment between the representations of GNNs and LLMs achieves
co-optimization and embedding-level alignment of the two mod-
els, they remain separate during inference. To achieve a higher
level of integration between LLMs and GNNs, several works have
focused on designing a deeper fusion of the architecture of the
modules, such as transformer layers in LLMs and graph neural lay-
ers in GNNs. Co-training GNNs and LLMs can result in a win-win
bi-directional benefit for both modules in graph tasks.
Along this line, GreaseLM [90] integrates transformer layers and
GNN layers by designing a specific forward propagation layer that
enables bidirectional information passing between LM and GNN
through special interaction markers and interaction nodes. This
approach allows language context representations to be grounded
in structured world knowledge, while subtle linguistic differences
(such as negation or modifiers) can affect the representation of
the knowledge graph, which enables GreaseLM to achieve high
performance on Question-Answering tasks. DGTL [54] proposes
disentangled graph learning to leverage GNNs to encode disentan-
gled representations, which are then injected into each transformer
layer of the LLMs. This approach enables the LLMs to be aware of
the graph structure and leverage the gradient from the LLMs to fine-
tune the GNNs. By doing so, DGTL achieves high performance on
both citation network and e-commerce graph tasks. ENGINE [98]
adds a lightweight and tunable G-Ladder module to each layer of
the LLM, which uses a message-passing mechanism to integrate
structural information. This enables the output of each LLM layer
(i.e., token-level representations) to be passed to the correspond-
ing G-Ladder, where the node representations are enhanced and
then used for downstream tasks such as node classification. More
directly, GraphAdapter [31] uses a fusion module (typically a
multi-layer perceptrons) to combine the structural representations
obtained from GNNs with the contextual hidden states of LLMs
(e.g., the encoded node text). This enables the structural information
from the GNN adapter to complement the textual information from
the LLMs, resulting in a fused representation that can be used for
supervision training and prompting for downstream tasks.
3.3.3 LLMs Agent for Graphs. With the powerful capabilities of
LLMs in understanding instructions and self-planning to solve tasks,an emerging research direction is to build autonomous agents based
on LLMs to tackle human-given or research-related tasks. Typically,
an agent consists of a memory module, a perception module, and an
action module to enable a loop of observation, memory recall, and
action for solving given tasks. In the graph domain, LLMs-based
agents can interact directly with graph data to perform tasks such
as node classification and link prediction.
In this field, Pangu [19] pioneered the use of LMs to navigate
KGs. In this approach, the agent is designed as a symbolic graph
search algorithm, providing a set of potential search paths for the
language models to evaluate in response to a given query. The re-
maining path is then utilized to retrieve the answer. Graph Agent
(GA) [71] converts graph data into textual descriptions and gen-
erates embedding vectors, which are stored in long-term memory.
During inference, GA retrieves similar samples from long-term
memory and integrates them into a structured prompt, which is
used by LLMs to explain the potential reasons for node classification
or edge connection. FUXI [18] framework integrates customized
tools and the ReAct [ 83] algorithm to enable LLMs to act as agents
that can proactively interact with KGs. By leveraging tool-based
navigation and exploration of data, these agents perform chained
reasoning to progressively build answers and ultimately solve com-
plex queries efficiently and accurately. Readi [10] is another ap-
proach that first uses in-context learning and chain-of-thought
prompts to generate reasoning paths with multiple constraints,
which are then instantiated based on the graph data. The instanti-
ated reasoning paths are merged and used as input to LLMs to gen-
erate an answer. This method has achieved significant performance
improvements on KGQA (knowledge graph question answering)
and TableQA (table question answering) tasks. Recently, RoG [49]
is proposed to answer graph-retaled question in three steps: plan-
ning, retrieval, and reasoning. In the planning step, it generates
a set of associated paths based on the structured information of
the knowledge graph according to the problem. In the retrieval
step, it uses the associated paths generated in the planning stage to
retrieve the corresponding reasoning paths from the KG. Finally,
it uses the retrieved reasoning paths to generate the answer and
explanation for the problem using LLMs.
3.3.4 Discussion. The integration of LLMs and graphs has shown
promising progress in minimizing the modality gap between struc-
tured data and textual data for solving graph-related tasks. By
combining the strengths of LLMs in language understanding and
the ability of graphs to capture complex relationships between
entities, we can enable more accurate and flexible reasoning over
graph data. However, despite the promising progress, there is still
room for improvement in this area. One of the main challenges
in integrating LLMs and graphs is scalability. In alignment and
fusion training, current methods often use small language models
or fix the parameters of LLMs, which limits their ability to scale to
larger graph datasets. Therefore, it is crucial to explore methods
for scaling model training with larger models on web-scale graph
data, which can enable more accurate and efficient reasoning over
large-scale graphs. Another challenge in this area is the limited
interaction between graph agents and graph data. Current methods
for graph agents often plan and execute only once, which may not
be optimal for complex tasks requiring multiple runs. Therefore,
6621A Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
GraphsDiverse
Features‚äï
Input PromptsTuning -free LLMs
Tuning -required  LLMsor Answer
Figure 5: LLMs-Only.
it is necessary to investigate methods for agents to interact with
graph data multiple times, refining their plans and improving their
performance based on feedback from the graph. This can enable
more sophisticated reasoning over graph data and improve the ac-
curacy of downstream tasks. Overall, the integration of LLMs and
graphs is a promising research direction with significant potential
for advancing the state-of-the-art in graph learning. By address-
ing the aforementioned challenges and developing more advanced
methods for integrating LLMs and graphs, we can enable more
accurate and flexible reasoning over graph data and unlock new
applications in areas such as knowledge graph reasoning, molecular
modeling, and social network analysis.
3.4 LLMs-Only
In this section, we will elaborate in detail on the direct application of
LLMs for various graph-oriented tasks, namely the LLMs-Only cat-
egory. These methods aim to allow LLMs to directly accept graph
structure information, understand it, and perform inference for
various downstream tasks in combination with this information.
These methods can mainly be divided into two broad categories: i)
Tuning-free methods aim to design prompts that LLMs can un-
derstand to express graphs, directly prompting pre-trained LLMs
to perform graph-oriented tasks; ii) Tuning-required approaches
focus on converting graphs into sequences in a specific way and
aligning graph token sequences and natural language token se-
quences using fine-tuning methods.
3.4.1 Tuning-free. Given the unique structured characteristics
of graph data, two critical challenges arise: effectively construct-
ing a graph in natural language format and determining whether
Large Language Models (LLMs) can accurately comprehend graph
structures as represented linguistically. To address these issues,
tuning-free approaches are being developed to model and infer
graphs solely within the text space, thereby exploring the potential
of pre-trained LLMs for enhanced structural understanding.
NLGraph [68],GPT4Graph [20] and Beyond Text [28] col-
lectively examine the capabilities of LLMs in understanding and
reasoning with graph data. NLGraph proposes a benchmark for
graph-based problem solving and introduces instruction-based ap-
proaches, while GPT4Graph and Beyond Text investigate the pro-
ficiency of LLMs in comprehending graph structures and empha-
sizes the need for advancements in their graph processing capa-
bilities. And Graph-LLM [8] explores the potential of LLMs in
graph machine learning, focusing on the node classification task.
Two pipelines, LLMs-as-Enhancers andLLMs-as-Predictors, are in-
vestigated to leverage LLMs‚Äô extensive common knowledge and
semantic comprehension abilities. Through comprehensive stud-
ies, it provides original observations and insights that open new
possibilities for utilizing LLMs in learning on graphs. Meanwhile,GraphText [95] translates graphs into natural language by deriv-
ing a graph-syntax tree and processing it with an LLM. It offers
training-free graph reasoning and enables interactive graph rea-
soning, showcasing the unexplored potential of LLMs. Talk like a
Graph [15] conducts an in-depth examination of text-based graph
encoder functions for LLMs, evaluating their efficacy in transform-
ing graph data into textual format to enhance LLMs‚Äô capabilities in
executing graph reasoning tasks, and proposes the GraphQA bench-
mark to systematically measure the influence of encoding strate-
gies on model performance. And LLM4DyG [91] benchmarks the
spatial-temporal comprehension of LLMs on dynamic graphs, intro-
ducing tasks that evaluate both temporal and spatial understanding,
and suggests the Disentangled Spatial-Temporal Thoughts (DST2)
prompting technique for improved performance. To facilitate the
integration of multimodality, GraphTMI [11] presents an innova-
tive approach to integrating graph data with LLMs, introducing
diverse modalities such as text, image, and motif encoding to en-
hance LLMs‚Äô efficiency in processing complex graph structures, and
proposes the GraphTMI benchmark for evaluating LLMs in graph
structure analysis, revealing that the image modality outperforms
text and prior GNNs in balancing token limits and preserving essen-
tial information. Ai et al. [2] introduces a multimodal framework
for graph understanding and reasoning, utilizing image encoding
and GPT-4V‚Äôs advanced capabilities to interpret and process diverse
graph data, while identifying challenges in Chinese OCR and com-
plex graph types, suggesting directions for future enhancements in
AI‚Äôs multimodal interaction and graph data processing.
3.4.2 Tuning-required. Due to the limitations of expressing graph
structural information using pure text, the recent mainstream ap-
proach is to align graphs as node token sequences with natural lan-
guage token sequences when inputting them to LLMs. In contrast
to the aforementioned GNN as Prefix approach, the Tuning-required
LLM-only approach discards the graph encoder and adopts a spe-
cific arrangement of graph token sequences, along with carefully
designed embeddings of graph tokens in prompts, achieving promis-
ing performances in various downstream graph-related tasks.
InstructGLM [84] introduces an innovative framework for graph
representation learning that combines natural language instruc-
tions with graph embeddings to fine-tune LLMs. This approach
allows LLMs to effectively process graph structures without re-
lying on specialized GNN architectures. WalkLM [62] integrates
language models with random walks to create unsupervised attrib-
uted graph embeddings, focusing on the technical innovation of
transforming graph entities into textual sequences and utilizing
graph-aware fine-tuning. This technique captures both attribute
semantics and graph structures. Recently, LLaGA [7] has utilized
node-level templates to restructure graph data into organized se-
quences, which are then mapped into the token embedding space.
This allows Large Language Models to process graph-structured
data with enhanced versatility, generalizability, and interpretability.
InstructGraph [69] proposes a methodological approach to im-
prove LLMs for graph reasoning and generation through structured
format verbalization, graph instruction tuning, and preference align-
ment. This aims to bridge the semantic gap between graph data and
textual language models, and to mitigate the issue of hallucination
in LLM outputs.
6622KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang
ZeroG [38] then leverages a language model to encode node
attributes and class semantics, employing prompt-based subgraph
sampling and lightweight fine-tuning strategies to address cross-
dataset zero-shot transferability challenges in graph learning. Fur-
thermore, GraphWiz [6] utilizes GraphInstruct, an instruction-
tuning dataset, to augment language models for addressing var-
ious graph problems, employing Direct Preference Optimization
(DPO) [ 55] to enhance the clarity and accuracy of reasoning pro-
cesses. GraphInstruct [50] presents a comprehensive benchmark
of 21 graph reasoning tasks, incorporating diverse graph genera-
tion methods and detailed reasoning steps to enhance LLMs with
improved graph understanding and reasoning capabilities. And,
MuseGraph [61] fuses the capabilities of LLMs with graph mining
tasks through a compact graph description mechanism, diverse in-
struction generation, and graph-aware instruction tuning, enabling
a generic approach for analyzing and processing attributed graphs.
3.4.3 Discussion. The LLMs-Only approach is an emerging re-
search direction that explores the potential of pre-training Large
Language Models specifically for interpreting graph data and merg-
ing graphs with natural language instructions. The main idea be-
hind this approach is to leverage the powerful language understand-
ing capabilities of LLMs to reason over graph data and generate
accurate responses to queries. However, effectively transforming
large-scale graphs into text prompts and reordering graph token
sequences to preserve structural integrity without a graph encoder
present significant ongoing challenges. These challenges arise due
to the complex nature of graph data, which often contains intri-
cate relationships between nodes and edges, as well as the limited
ability of LLMs to capture such relationships without explicit guid-
ance. As such, further research is needed to develop more advanced
methods for integrating LLMs with graph data and overcoming the
aforementioned challenges.
4 FUTURE DIRECTIONS
In this section, we explore several open problems and potential
future directions in the field of large language models for graphs.
4.1 LLMs for Multi-modal Graphs
Recent studies have demonstrated the remarkable ability of large
language models to process and understand multi-modal data [ 78],
such as images [ 44] and videos [ 87]. This capability has opened up
new avenues for integrating LLMs with multi-modal graph data,
where nodes may contain features from multiple modalities [ 40].
By developing multi-modal LLMs that can process such graph data,
we can enable more accurate and comprehensive reasoning over
graph structures, taking into account not only textual information
but also visual, auditory, and other types of data.
4.2 Efficiency and Less Computational Cost
In the current landscape, the substantial computational expenses
associated with both the training and inference phases of LLMs
pose a significant limitation [ 13,16], impeding their capacity to
process large-scale graphs that encompass millions of nodes. This
challenge is further compounded when attempting to integrate
LLMs with GNNs, as the fusion of these two powerful models
becomes increasingly arduous due to the aforementioned compu-
tational constraints [ 94]. Consequently, the necessity to discoverand implement efficient strategies for training LLMs and GNNs
with reduced computational costs becomes paramount. This is not
only to alleviate the current limitations but also to pave the way for
the enhanced application of LLMs in graph-related tasks, thereby
broadening their utility and impact in the field of data science.
4.3 Tackling Different Graph Tasks
The prevailing methodologies LLMs have primarily centered their
attention on conventional graph-related tasks, such as link predic-
tion and node classification. However, considering the remarkable
capabilities of LLMs, it is both logical and promising to delve into
their potential in tackling more complex and generative tasks, in-
cluding but not limited to graph generation [ 97], graph understand-
ing, and graph-based question answering [ 32]. By expanding the
horizons of LLM-based approaches to encompass these intricate
tasks, we can unlock a myriad of new opportunities for their appli-
cation across diverse domains. For instance, in the realm of drug
discovery, LLMs could facilitate the generation of novel molecular
structures; in social network analysis, they could provide deeper in-
sights into intricate relationship patterns; and in knowledge graph
construction, they could contribute to the creation of more com-
prehensive and contextually accurate knowledge bases.
4.4 User-Centric Agents on Graphs
The majority of contemporary LLM-based agents, specifically de-
signed to address graph-related tasks, are predominantly tailored for
single graph tasks. These agents typically adhere to a one-time-run
procedure, aiming to resolve the provided question in a single at-
tempt. Consequently, these agents are neither equipped to function
as multi-run interactive agents, capable of adjusting their generated
plans based on feedback or additional information, nor are they
designed to be user-friendly agents that can effectively manage a
wide array of user-given questions. An LLM-based agent [ 70] that
embodies the ideal qualities should not only be user-friendly but
also possess the capability to dynamically search for answers within
graph data in response to a diverse range of open-ended questions
posed by users. This would necessitate the development of an agent
that is both adaptable and robust, able to engage in iterative interac-
tions with users and adept at navigating the complexities of graph
data to provide accurate and relevant answers.
5 CONCLUSION
In this comprehensive survey, we delve into the current state of
large language models specifically tailored for graph data, propos-
ing an innovative taxonomy grounded in the distinctive designs
of their inference frameworks. We meticulously categorize these
models into four unique framework designs, each characterized by
its own set of advantages and limitations. Additionally, we provide
a detailed discussion on these characteristics, enriching our analysis
with insights into potential challenges and opportunities within
this field. Our survey not only serves as a critical resource for re-
searchers keen on exploring and leveraging large language models
for graph-related tasks but also aims to inspire and guide future
research endeavors in this evolving domain. Through this work,
we hope to foster a deeper understanding and stimulate further
innovation in the integration of LLMs with graphs.
6623A Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
REFERENCES
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[2]Qihang Ai, Jianwu Zhou, Haiyun Jiang, Lemao Liu, and Shuming Shi. 2023. When
Graph Data Meets Multimodal: A New Paradigm for Graph Understanding and
Reasoning. arXiv preprint arXiv:2312.10372 (2023).
[3]William Brannon et al .2023. Congrat: Self-supervised contrastive pretraining
for joint graph and text embeddings. arXiv preprint arXiv:2305.14321 (2023).
[4]He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. 2023. Instructmol: Multi-
modal integration for building a versatile and reliable molecular assistant in
drug discovery. arXiv preprint arXiv:2311.16208 (2023).
[5]Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen
Huang, and Yang Yang. 2023. Graphllm: Boosting graph reasoning ability of
large language model. arXiv preprint arXiv:2310.05845 (2023).
[6]Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. 2024. GraphWiz: An Instruction-
Following Language Model for Graph Problems. arXiv preprint arXiv:2402.16029
(2024).
[7]Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024.
LLaGA: Large Language and Graph Assistant. arXiv preprint arXiv:2402.08170
(2024).
[8]Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,
Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al .2024. Exploring the
potential of large language models (llms) in learning on graphs. ACM SIGKDD
Explorations Newsletter 25, 2 (2024), 42‚Äì61.
[9]Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang,
Hui Liu, and Jiliang Tang. 2023. Label-free node classification on graphs with
large language models (llms). arXiv preprint arXiv:2310.04668 (2023).
[10] Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting
Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, et al .2024. Call Me
When Necessary: LLMs can Efficiently and Faithfully Reason over Structured
Environments. arXiv preprint arXiv:2403.08593 (2024).
[11] Debarati Das, Ishaan Gupta, Jaideep Srivastava, and Dongyeop Kang. 2023.
Which Modality should I use‚ÄìText, Motif, or Image?: Understanding Graphs
with Large Language Models. arXiv preprint arXiv:2311.09862 (2023).
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805 (2018).
[13] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,
Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al .2023. Parameter-
efficient fine-tuning of large-scale pre-trained language models. Nature Machine
Intelligence 5, 3 (2023), 220‚Äì235.
[14] Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang Ooi, Qizhe
Xie, and Junxian He. 2023. Simteg: A frustratingly simple approach improves
textual graph learning. arXiv preprint arXiv:2308.02565 (2023).
[15] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023. Talk like a graph:
Encoding graphs for large language models. arXiv preprint arXiv:2310.04560
(2023).
[16] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. 2022.
Parameter-efficient mixture-of-experts architecture for pre-trained language
models. arXiv preprint arXiv:2203.01104 (2022).
[17] Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. 2020. Generalization and
representational limits of graph neural networks. In ICML. PMLR, 3419‚Äì3430.
[18] Yu Gu et al .2024. Middleware for LLMs: Tools Are Instrumental for Language
Agents in Complex Environments. arXiv preprint arXiv:2402.14672 (2024).
[19] Yu Gu, Xiang Deng, and Yu Su. 2022. Don‚Äôt Generate, Discriminate: A Proposal
for Grounding Language Models to Real-World Environments. arXiv preprint
arXiv:2212.09736 (2022).
[20] Jiayan Guo, Lun Du, and Hengyu Liu. 2023. Gpt4graph: Can large language
models understand graph structured data? an empirical evaluation and bench-
marking. arXiv preprint arXiv:2305.15066 (2023).
[21] Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang
Pang, Tat-Seng Chua, and Chao Huang. 2024. GraphEdit: Large Language
Models for Graph Structure Learning. arXiv preprint arXiv:2402.15183 (2024).
[22] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and
Bryan Hooi. 2023. Harnessing explanations: Llm-to-lm interpreter for enhanced
text-attributed graph representation learning. In The Twelfth International Con-
ference on Learning Representations.
[23] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval. 639‚Äì648.
[24] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann Le-
Cun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-Augmented
Generation for Textual Graph Understanding and Question Answering. arXiv
preprint arXiv:2402.07630 (2024).[25] Yufei He and Bryan Hooi. 2024. UniGraph: Learning a Cross-Domain Graph
Foundation Model From Natural Language. arXiv preprint arXiv:2402.13630
(2024).
[26] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,
and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers
for recommender systems. In European Conference on Information Retrieval.
Springer, 364‚Äì381.
[27] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets
for machine learning on graphs. Advances in neural information processing
systems 33 (2020), 22118‚Äì22133.
[28] Yuntong Hu, Zheng Zhang, and Liang Zhao. 2023. Beyond Text: A Deep Dive
into Large Language Models‚Äô Ability on Understanding Graph Data. arXiv
preprint arXiv:2310.04944 (2023).
[29] Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, and Nitesh Chawla. 2024.
Large Language Models for Graphs: Progresses and Directions. In Companion
Proceedings of the ACM on Web Conference 2024. 1284‚Äì1287.
[30] Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang,
Yang Yang, and Qi Zhu. 2023. Prompt-based node feature extractor for few-shot
learning on text-attributed graphs. arXiv preprint arXiv:2309.02848 (2023).
[31] Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei
Chai, and Qi Zhu. 2024. Can GNN be Good Adapter for LLMs? arXiv preprint
arXiv:2402.12984 (2024).
[32] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019. Knowledge
graph embedding based question answering. In Proceedings of the twelfth ACM
international conference on web search and data mining. 105‚Äì113.
[33] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023.
Large language models on graphs: A comprehensive survey. arXiv preprint
arXiv:2312.02783 (2023).
[34] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with
graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[35] Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, and Chao Huang.
2023. Graph transformer for recommendation. In SIGIR. 1680‚Äì1689.
[36] Yichuan Li, Kaize Ding, and Kyumin Lee. 2023. GRENADE: Graph-Centric Lan-
guage Model for Self-Supervised Representation Learning on Text-Attributed
Graphs. arXiv preprint arXiv:2310.15109 (2023).
[37] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and
Jeffrey Xu Yu. 2023. A survey of graph meets large language model: Progress
and future directions. arXiv preprint arXiv:2311.12399 (2023).
[38] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG:
Investigating Cross-dataset Zero-shot Transferability in Graphs. arXiv preprint
arXiv:2402.11235 (2024).
[39] Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei
Yin, and Chao Huang. 2024. Urbangpt: Spatio-temporal large language models.
arXiv preprint arXiv:2403.00813 (2024).
[40] Wanying Liang, Pasquale De Meo, Yong Tang, and Jia Zhu. 2024. A Survey of
Multi-modal Knowledge Graphs: Technologies and Trends. Comput. Surveys
(2024).
[41] Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-
Seng Chua. 2024. Data-efficient Fine-tuning for LLM-based Recommendation.
arXiv preprint arXiv:2401.17197 (2024).
[42] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,
and Muhan Zhang. 2023. One for all: Towards training one graph model for all
classification tasks. arXiv preprint arXiv:2310.00149.
[43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved base-
lines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 26296‚Äì26306.
[44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual
instruction tuning. Advances in neural information processing systems 36 (2024).
[45] Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren. 2024. Git-mol: A multi-
modal large language model for molecular science with graph, image, and text.
Computers in Biology and Medicine 171 (2024), 108073.
[46] Shengchao Liu et al .2023. Multi-modal molecule structure‚Äìtext model for
text-based retrieval and editing. Nature Machine Intelligence (2023).
[47] Yinhan Liu et al .2019. Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692 (2019).
[48] Zhiyuan Liu et al .2023. Molca: Molecular graph-language modeling with
cross-modal projector and uni-modal adapter. arXiv preprint arXiv:2310.12798
(2023).
[49] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning
on graphs: Faithful and interpretable large language model reasoning. arXiv
preprint arXiv:2310.01061 (2023).
[50] Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi
Jiang, Xing Xie, and Hai Jin. 2024. GraphInstruct: Empowering Large Language
Models with Graph Understanding and Reasoning Capability. arXiv preprint
arXiv:2403.04483 (2024).
[51] Qiyao Ma, Xubin Ren, and Chao Huang. 2024. XRec: Large Language Models
for Explainable Recommendation. arXiv preprint arXiv:2406.02377 (2024).
6624KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang
[52] Seth A Myers, Aneesh Sharma, Pankaj Gupta, and Jimmy Lin. 2014. Informa-
tion network or social network? The structure of the Twitter follow graph. In
Proceedings of the 23rd international conference on world wide web. 493‚Äì498.
[53] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.
2024. Unifying large language models and knowledge graphs: A roadmap. IEEE
Transactions on Knowledge and Data Engineering (2024).
[54] Yijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2023. Disentangled
representation learning with large language models for text-attributed graphs.
arXiv preprint arXiv:2310.18152 (2023).
[55] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano
Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neural Information Processing
Systems 36 (2024).
[56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text transformer. Journal of machine
learning research 21, 140 (2020), 1‚Äì67.
[57] Xubin Ren, Wei Wei, Lianghao Xia, and Chao Huang. 2024. A Comprehen-
sive Survey on Self-Supervised Learning for Recommendation. arXiv preprint
arXiv:2404.03354 (2024).
[58] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei
Yin, and Chao Huang. 2024. Representation learning with large language models
for recommendation. In Proceedings of the ACM on Web Conference 2024. 3464‚Äì
3475.
[59] Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Disen-
tangled contrastive collaborative filtering. In Proceedings of the 46th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
1137‚Äì1146.
[60] Bing Su, Dazhao Du, Zhao Yang, et al .2022. A molecular multimodal founda-
tion model associating molecule graphs with natural language. arXiv preprint
arXiv:2209.05481 (2022).
[61] Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, and Carl
Yang. 2024. MuseGraph: Graph-oriented Instruction Tuning of Large Language
Models for Generic Graph Mining. arXiv preprint arXiv:2403.04780 (2024).
[62] Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, and Carl Yang. 2024. Walklm:
A uniform language model fine-tuning framework for attributed graph embed-
ding. Advances in Neural Information Processing Systems 36 (2024).
[63] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin,
and Chao Huang. 2023. Graphgpt: Graph instruction tuning for large language
models. arXiv preprint arXiv:2310.13023 (2023).
[64] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, and Chao
Huang. 2024. HiGPT: Heterogeneous Graph Language Model. arXiv preprint
arXiv:2402.16024 (2024).
[65] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang,
Nitesh V Chawla, and Panpan Xu. 2024. Graph neural prompting with large
language models. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 38. 19080‚Äì19088.
[66] Hugo Touvron et al .2023. Llama: Open and efficient foundation language
models. arXiv preprint arXiv:2302.13971 (2023).
[67] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[68] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and
Yulia Tsvetkov. 2024. Can language models solve graph problems in natural
language? Advances in Neural Information Processing Systems 36 (2024).
[69] Jianing Wang, Junda Wu, Yupeng Hou, et al .2024. InstructGraph: Boosting
Large Language Models via Graph-centric Instruction Tuning and Preference
Alignment. arXiv preprint arXiv:2402.08785 (2024).
[70] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
Zhiyuan Chen, Jiakai Tang, et al .2024. A survey on large language model based
autonomous agents. Frontiers of Computer Science 18, 6 (2024), 186345.
[71] Qinyong Wang, Zhenxiang Gao, and Rong Xu. 2023. Graph Agent: Explicit
Reasoning Agent for Graphs. arXiv preprint arXiv:2310.16421 (2023).
[72] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, et al .2023. How far can camels
go? exploring the state of instruction tuning on open resources. Advances in
Neural Information Processing Systems 36 (2023), 74764‚Äì74786.
[73] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng
Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models
with graph augmentation for recommendation. In Proceedings of the 17th ACM
International Conference on Web Search and Data Mining. 806‚Äì815.
[74] Zhihao Wen and Yuan Fang. 2023. Augmenting low-resource text classification
with graph-grounded pre-training and prompting. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 506‚Äì516.
[75] Zhihao Wen and Yuan Fang. 2023. Prompt tuning on graph-augmented low-
resource text classification. arXiv preprint arXiv:2307.10230 (2023).[76] Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi
Yan. 2023. Difformer: Scalable (graph) transformers induced by energy con-
strained diffusion. arXiv preprint arXiv:2301.09474 (2023).
[77] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-
former: A scalable graph structure learning transformer for node classification.
Advances in Neural Information Processing Systems 35 (2022), 27387‚Äì27401.
[78] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023. Next-gpt:
Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519 (2023).
[79] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4‚Äì24.
[80] Lianghao Xia, Ben Kao, and Chao Huang. 2024. OpenGraph: Towards Open
Graph Foundation Models. arXiv preprint arXiv:2403.01121 (2024).
[81] Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N Ioannidis, Xiang Song,
Qing Ping, Sheng Wang, Carl Yang, Yi Xu, et al .2023. Graph-aware language
model pre-training on a large graph corpus can help multiple graph applications.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 5270‚Äì5281.
[82] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[83] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao. 2022. React: Synergizing reasoning and acting in language
models. arXiv preprint arXiv:2210.03629 (2022).
[84] Ruosong Ye, Caiqi Zhang, et al .2023. Natural language is all a graph needs.
arXiv preprint arXiv:2308.07134 (2023).
[85] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph
contrastive learning automated. In International Conference on Machine Learning.
PMLR, 12121‚Äì12132.
[86] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim.
2019. Graph transformer networks. Advances in neural information processing
systems 32 (2019).
[87] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instruction-
tuned audio-visual language model for video understanding. arXiv preprint
arXiv:2306.02858 (2023).
[88] Mengmei Zhang et al. 2024. GraphTranslator: Aligning Graph Model to Large
Language Model for Open-ended Tasks. arXiv preprint arXiv:2402.07197 (2024).
[89] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural
networks. Advances in neural information processing systems 31 (2018).
[90] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang,
et al.2022. Greaselm: Graph reasoning enhanced language models for question
answering. arXiv preprint arXiv:2201.08860 (2022).
[91] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, and
Wenwu Zhu. 2023. LLM4DyG: Can Large Language Models Solve Problems on
Dynamic Graphs? arXiv preprint arXiv:2310.17110 (2023).
[92] Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng,
Lingpeng Kong, and Qi Liu. 2024. Gimlet: A unified graph-text model for
instruction-based molecule zero-shot learning. Advances in Neural Information
Processing Systems 36 (2024).
[93] Jianan Zhao, Hesham Mostafa, Michael Galkin, Michael Bronstein, Zhaocheng
Zhu, and Jian Tang. 2024. GraphAny: A Foundation Model for Node Classifica-
tion on Any Graph. arXiv preprint arXiv:2405.20445 (2024).
[94] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and
Jian Tang. 2022. Learning on large-scale text-attributed graphs via variational
inference. arXiv preprint arXiv:2210.14709 (2022).
[95] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein,
Zhaocheng Zhu, and Jian Tang. 2023. Graphtext: Graph reasoning in text space.
arXiv preprint arXiv:2310.01089 (2023).
[96] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey
of large language models. arXiv preprint arXiv:2303.18223 (2023).
[97] Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, and
Shu Wu. 2022. A survey on deep graph generation: Methods and applications.
InLearning on Graphs Conference. PMLR, 47‚Äì1.
[98] Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024. Efficient Tuning
and Inference for Large Language Models on Textual Graphs. arXiv preprint
arXiv:2401.15569 (2024).
[99] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2023. Col-
laborative large language model for recommender systems. arXiv preprint
arXiv:2311.01343 (2023).
[100] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bowen Du. 2023. Pretraining
language models with text-attributed heterogeneous graphs. arXiv preprint
arXiv:2310.12580 (2023).
6 APPENDIX
In Table 1, we provide an overview of notable graph learning tech-
niques that utilize large language models.
6625A Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Summary of representative graph learning methods with large language models.
Categor
y Method Pipeline Domain Venue Year
GNNs
as PrefixGraphGPT [63] Node-level Tokenization General Graph SIGIR 2024
HiGPT [64] Node-level Tokenization Heterogeneous Graph KDD 2024
GraphTranslator [88] Node-level Tokenization General Graph WWW 2024
UniGraph [25] Node-level Tokenization General Graph arXiv 2024
GIMLET [92] Node-level Tokenization Bioinformatics NeurIPS 2024
XRec [51] Node-level Tokenization Recommendation arXiv 2024
GraphLLM [5] Graph-level Tokenization Graph Reasoning arXiv 2023
GIT-Mol [45] Graph-level Tokenization Bioinformatics Comput Biol Med 2024
MolCA [48] Graph-level Tokenization Bioinformatics EMNLP 2023
InstructMol [4] Graph-level Tokenization Bioinformatics arXiv 2023
G-Retriever [24] Graph-level Tokenization Graph-based QA arXiv 2024
GNP [65] Graph-level Tokenization Graph-based QA AAAI 2024
LLMs
as PrefixG-Prompt [30] Embs. from LLMs for GNNs General Graph arXiv 2023
SimTeG [14] Embs. from LLMs for GNNs General Graph arXiv 2023
GALM [81] Embs. from LLMs for GNNs General Graph KDD 2023
OFA [42] Embs. from LLMs for GNNs General Graph ICLR 2024
TAPE [22] Embs. from LLMs for GNNs General Graph ICLR 2024
LLMRec [73] Embs. from LLMs for GNNs Recommendation WSDM 2024
OpenGraph [80] Labels from LLMs for GNNs General Graph arXiv 2024
LLM-GNN [9] Labels from LLMs for GNNs General Graph ICLR 2024
GraphEdit [21] Labels from LLMs for GNNs General Graph arXiv 2023
RLMRec [58] Labels from LLMs for GNNs Recommendation WWW 2024
LLMs-Graphs
InteractionMoMu [63] Alignment between GNNs and LLMs Bioinformatics arXiv 2022
ConGraT [64] Alignment between GNNs and LLMs General Graph arXiv 2023
G2P2 [88] Alignment between GNNs and LLMs General Graph SIGIR 2023
GRENADE [25] Alignment between GNNs and LLMs General Graph EMNLP 2023
MoleculeSTM [92] Alignment between GNNs and LLMs Bioinformatics Nature MI 2023
THLM [51] Alignment between GNNs and LLMs Heterogeneous Graph EMNLP 2023
GLEM [5] Alignment between GNNs and LLMs General Graph ICLR 2023
GreaseLM [90] Fusion Training of GNNs and LLMs Graph-based QA ICLR 2022
DGTL [54] Fusion Training of GNNs and LLMs General Graph arXiv 2023
ENGINE [98] Fusion Training of GNNs and LLMs General Graph arXiv 2024
GraphAdapter [31] Fusion Training of GNNs and LLMs General Graph WWW 2024
Pangu [19] LLMs Agent for Graphs Graph-based QA ACL 2023
Graph Agent [71] LLMs Agent for Graphs General Graph arXiv 2023
FUXI [18] LLMs Agent for Graphs Graph-based QA arXiv 2024
Readi [10] LLMs Agent for Graphs Graph-based QA arXiv 2024
RoG [49] LLMs Agent for Graphs Graph-based QA ICLR 2024
LLMs-OnlyNLGraph
[68] Tuning-free Graph Reasoning NeurIPS 2024
GPT4Graph [20] Tuning-free Graph Reasoning & QA arXiv 2023
Beyond Text [28] Tuning-free General Graph arXiv 2023
Graph-LLM [8] Tuning-free General Graph KDD Exp. News. 2023
GraphText [95] Tuning-free General Graph arXiv 2023
Talk like a Graph [15] Tuning-free Graph Reasoning arXiv 2023
LLM4DyG [91] Tuning-free Dynamic Graph arXiv 2023
GraphTMI [11] Tuning-free General Graph arXiv 2023
Ai et al. [2] Tuning-free Multi-modal Graph arXiv 2023
InstructGLM [84] Tuning-required General Graph EACL 2024
WalkLM [62] Tuning-required General Graph NeurIPS 2024
LLaGA [7] Tuning-required General Graph ICML 2024
InstructGraph [69] Tuning-required General Graph & QA & Reasoning arXiv 2024
ZeroG [38] Tuning-required General Graph arXiv 2024
GraphWiz [6] Tuning-required Graph Reasoning arXiv 2024
GraphInstruct [50] Tuning-required Graph Reasoning & Generation arXiv 2024
MuseGraph [61] Tuning-required General Graph arXiv 2024
6626