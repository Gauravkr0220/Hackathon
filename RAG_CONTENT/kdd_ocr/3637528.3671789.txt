How Powerful is Graph Filtering for Recommendation
Shaowen Peng
peng.shaowen@naist.ac.jp
NARA Institute of Science and Technology
Nara, JapanXin Liu
xin.liu@aist.go.jp
National Institute of Advanced Industrial Science and
Technology
Tokyo, Japan
Kazunari Sugiyama
sugiyama-k@g.osaka-seikei.ac.jp
Osaka Seikei University
Osaka, JapanTsunenori Mine
mine@m.ait.kyushu-u.ac.jp
Kyushu University
Fukuoka, Japan
Abstract
It has been shown that the effectiveness of graph convolutional
network (GCN) for recommendation is attributed to the spectral
graph filtering. Most GCN-based methods consist of a graph filter
or followed by a low-rank mapping optimized based on supervised
training. However, we show two limitations suppressing the power
of graph filtering: (1) Lack of generality. Due to the varied noise
distribution, graph filters fail to denoise sparse data where noise is
scattered across all frequencies, while supervised training results
in worse performance on dense data where noise is concentrated
in middle frequencies that can be removed by graph filters without
training. (2) Lack of expressive power. We theoretically show that
linear GCN (LGCN) that is effective on collaborative filtering (CF)
cannot generate arbitrary embeddings, implying the possibility that
optimal data representation might be unreachable.
To tackle the first limitation, we show close relation between
noise distribution and the sharpness of spectrum where a sharper
spectral distribution is more desirable causing data noise to be
separable from important features without training. Based on this
observation, we propose a generalized graph normalization ( G2N)
with hyperparameters adjusting the sharpness of spectral distri-
bution in order to redistribute data noise to assure that it can be
removed by graph filtering without training. As for the second
limitation, we propose an individualized graph filter (IGF) adapt-
ing to the different confidence levels of the user preference that
interactions can reflect, which is proved to be able to generate ar-
bitrary embeddings. By simplifying LGCN, we further propose a
simplified graph filtering for CF (SGFCF)1which only requires the
top-ğ¾singular values for recommendation. Finally, experimental
results on four datasets with different density settings demonstrate
the effectiveness and efficiency of our proposed methods.
1https://github.com/tanatosuu/sgfcf
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671789CCS Concepts
â€¢Information systems â†’Collaborative filtering ;Recommender
systems.
Keywords
Recommender System, Collaborative Filtering, Graph Convolu-
tional Network
ACM Reference Format:
Shaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine. 2024.
How Powerful is Graph Filtering for Recommendation. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671789
1 Introduction
Personalized recommendations have been widely applied to e-
commerce, social media platforms, online video sites, etc., and has
been indispensable to enrich peopleâ€™s daily life by offering the items
user might be interested in based on the data such as user-item
interactions, reviews, social relations, temporal information, etc.
Among various recommendation scenarios, we focus on collabora-
tive filtering (CF), a fundamental task for recommender systems.
Conventional CF methods such as matrix factorization (MF) [ 22]
characterizes users and items as low dimensional vectors and pre-
dict the rating via the inner product between the corresponding
embedding vectors. Subsequent works replace the linear design of
MF with other advanced algorithms such as neural networks [ 9,15],
attention mechanisms [ 5,20], transformer [ 23,40], diffusion mod-
els [43], etc. to model complex user-item relations.
While the aforementioned advanced recommendation algorithms
show superior non-linear ability to model user-item relations, their
performance are unstable due to the data sparsity issue in rec-
ommendation datasets. Graph Convolutional Networks (GCNs)
recently have shown great potential in recommender systems due
to the ability of capturing higher-order neighbor signals that can
augment the training data to alleviate the sparsity issue. Early GCN-
based methods adapt classic GCNs such as vanilla GCN [ 21] and
GraphSage [ 13] to recommendation [ 44,49,51]. Subsequent works
empower GCN by incorporating other advanced algorithms such as
contrastive learning [ 47], learning in hyperbolic space [ 7,41], dis-
entangled representation learning [ 45], etc., slimming GCN model
 
2388
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine
architectures to improve efficiency and scalability [ 6,14,30], and
study the effectiveness of GCN [29, 37].
Recent studies have shown that the effectiveness of GCN for
recommendation is mainly attributed to the spectral graph filter-
ing which emphasizes important features (i.e., low frequencies)
and filters out useless information. Most existing graph filtering
designs can be classified to two categories: (1) Repeatedly propagat-
ing the node embeddings across the graph where the embeddings
are optimized based on supervisory signals. This type of methods
is actually equivalent to a low pass filter followed by a low-rank
mapping [ 12,14,29] (see Equation (3)). (2) A simple graph filter
without model training [ 25,37]. This type of methods only relies on
the graph filters to denoise. We can see these two kinds of methods
are actually contradictory: the type (2) methods implies that data
noise is only distributed in certain frequencies that can be simply
removed by graph filters without training, which is contrary to
the type (1), that we need model training to further remove data
noise. However, we show that neither of them are perfect by point-
ing out two limitation suppressing the power of graph filtering.
Firstly, both of the two designs lack generality. We find that noise
distribution varies on datasets with different densities. Particularly,
graph filters show poor performance on sparse datasets where data
noise is scattered across all frequencies as they denoise by masking
certain frequencies while fail to remove intrinsic noise in the fea-
tures. On the other hand, despite the superior ability of supervised
training learning from data polluted by noise, it results in worse
performance on dense datasets on which the noise is concentrated
in middle frequencies that can be simply removed by graph fil-
ters. Moreover, most effective GCN-based methods are basically
linear GCNs (LGCNs) without non-linearity. We theoretically show
that they are incapable of generating arbitrary embeddings with
multi-dimensions, implying the possibility that they cannot gener-
ate desirable user/item representations and demonstrating the lack
of expressive power.
To tackle the first limitation, we further show the close relation
between noise distribution and the sharpness of spectral distribu-
tion, that a sharper distribution is more desirable on which noise
and important features are separable by graph filtering without
supervised training. Based on this observation, we propose a gen-
eralized graph normalization ( G2N) to adjust the sharpness of spec-
trum via hyperparameters. As a result, data noise is redistributed
through G2N, making the graph filtering generalizable on datasets
with different densities. To tackle the second limitation, we propose
an individualized graph filter (IGF) which is proved to generate
arbitrary embeddings. Specifically, considering interactions do not
equally reflect user preference (i.e., more (less) similar of the inter-
acted items implies a higher (lower) consistency between the userâ€™s
future and past behaviour), the proposed IGF emphasizes different
frequencies based on the distinct confidence levels of user prefer-
ence that interactions can reflect. Finally, by simplifying LGCN,
we propose a simplified graph filtering for CF (SGFCF) only re-
quiring the top- ğ¾singular values. Our main contributions can be
summarized as follows: :
â€¢We point out two limitations suppressing the power of graph
filtering for recommendation: (1) The lack of generality due to the
the performance inconsistency of graph filters and supervisedtraining on data with different densities and (2) The lack of
expressive power of LGCN that is effective for CF.
â€¢We propose a generalized graph normalization to tackle the
first limitation, which redistributes data noise by adjusting the
sharpness of spectrum, enabling the graph filtering to denoise
without training on datasets with different densities.
â€¢We propose an individualized graph filtering to adapt to distinct
confidence levels of user preference that interactions can reflect.
It is proved to generate arbitrary data representations thus solves
the second limitation.
â€¢Extensive experimental results on four datasets with different
density settings demonstrate the efficiency and effectiveness of
our proposed method.
2 Preliminaries
We first introduce a commonly used GCN-learning paradigm for
CF. Given an interaction matrix with implicit feedbacks containing
|U|users and|I|items: Râˆˆ{0,1}|U|Ã—|I|, we can define a bipar-
tite graphG=(V,E), where the node set contains all users and
items:V=U+I , the edge set contains the user-item pairs with
interactions:E=R+where R+={ğ‘Ÿğ‘¢ğ‘–=1|ğ‘¢âˆˆU,ğ‘–âˆˆI} . For sim-
plicity, we let|U|+|I| =ğ‘›. Then, we can define the corresponding
adjacency matrix AofGand formulate the updating rules as:
H(ğ‘™+1)=ğœ
Ë†AH(ğ‘™)W(ğ‘™+1)
, (1)
whereğœ(Â·)is an activation function, Ë†Ais a symmetric normalized
adjacency matrix defined as follows:
Ë†A=0 Ë†R
Ë†Rğ‘‡0
, (2)
where Ë†R=D-1
2
ğ‘ˆRD-1
2
ğ¼is a normalized interaction matrix, Dğ‘ˆand
Dğ¼are matrices with diagonal elements representing user and item
degrees, respectively. The initial state is the stacked user/item em-
beddings H(0)=EâˆˆRğ‘›Ã—ğ‘‘, where each user ğ‘¢and itemğ‘–are
represented as learnable low-dimensional vectors. W(ğ‘™+1)âˆˆRğ‘‘Ã—ğ‘‘
is a linear transformation. It has been shown that the following
linear GCN (LGCN) removing the activation function and linear
transformation is effective for CF [ 6,14], with the final representa-
tions Ogenerated as:
O=ğ‘”
Ë†A
E, (3)
whereğ‘”(Ë†A)is usually defined as a polynomial graph filter:
ğ‘”
Ë†A
=ğ¿âˆ‘ï¸
ğ‘™=0ğœƒğ‘™Ë†Ağ‘™=Vğ‘‘ğ‘–ğ‘ğ‘” ğ¿âˆ‘ï¸
ğ‘™=0ğœƒğ‘™ğœ†ğ‘™
ğ‘˜!
Vğ‘‡, (4)
where{ğœƒ0,Â·Â·Â·,ğœƒğ¿}is a set of parameters; ğœ†ğ‘˜andVare an eigen-
value and the stacked eigenvectors, respectively; ğ‘‘ğ‘–ğ‘ğ‘”(Â·)is a diago-
nalization operator. The rating is predicted as:
Ë†ğ‘Ÿğ‘¢ğ‘–=oğ‘‡
ğ‘¢oğ‘– (5)
Definition 1. (Graph Frequency). Given the eigenvalue and -
vector pairs(ğœ†ğ‘˜,vğ‘˜)ofË†Awhereğœ†ğ‘˜âˆˆ[âˆ’ 1,1], the graph frequency is
defined based on the variation of a signal on the graphvğ‘˜âˆ’Ë†Avğ‘˜=
1âˆ’ğœ†ğ‘˜âˆˆ[0,2]. We call components with small variations low fre-
quencies, components with high variations high frequencies.
 
2389How Powerful is Graph Filtering for Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: The accuracy (nDCG@10) of SGF and LGCN with
different density settings.
Datasets
Methods 80% 60% 40% 20%
CiteULikeSGF 0.2267
0.2222 0.1661 0.0859
LGCN,ğ‘‘=64 0.1610 0.2023 0.2120 0.1267
LGCN,ğ‘‘=128 0.1699 0.2067 0.2157 0.1347
LGCN,ğ‘‘=256 0.1701 0.2110 0.2204 0.1390
Pinter
estSGF 0.0816 0.1008 0.1106 0.0629
LGCN,ğ‘‘=64 0.0707 0.0912 0.1204 0.1317
LGCN,ğ‘‘=128 0.0706 0.0924 0.1218 0.1338
LGCN,ğ‘‘=256 0.0699 0.0928 0.1240 0.1341
According to Definition 1, low (high) frequencies emphasize
the similarity (dissimilarity) between nodes and their neighbor-
hood. We can adjust the weight of different frequencies via ğ‘”(ğœ†ğ‘˜)
asğ‘”(Ë†A)=Ã
ğ‘˜ğ‘”(ğœ†ğ‘˜)vğ‘˜vğ‘‡
ğ‘˜. It has been shown that low frequen-
cies are significantly contributive to recommendation [ 29,37], and
most GCN-based methods can be classified to two categories: (1)
a low pass filter followed by a linear mapping and optimization
(i.e.,LGCN) and (2) a simple graph filter without training [ 37]. The
learning process can be formulated as follows:
GSpectrumâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ VâˆˆRğ‘›Ã—ğ‘›ğ‘”(ğœ†ğ‘˜)âˆ’âˆ’âˆ’âˆ’âˆ’â†’ V(ğ¾)âˆˆRğ‘›Ã—ğ¾Eâˆ’ â†’OâˆˆRğ‘›Ã—ğ‘‘,(6)
where V(ğ¾)is the top-ğ¾low frequency components. Here, we raise
two questions on existing works: (1) The underlying assumption of
the second type of methods is that data noise is only concentrated
in certain frequencies that does not pollute important features
(i.e., V(ğ¾)), which is contrary to the first type of methods that
further training is required to denoise V(ğ¾). Since the two types
of methods seem to be contradictory, are they general on different
datasets? (2) Despite the effectiveness and simplicity of LGCN, it
also has a weakened expressive power compared with vanilla GCN,
is it capable of generating desirable data representations? We will
answer the two questions in Section 3.
3 Analysis on Graph Filtering
In this section, we evaluate existing graph filtering designs in terms
of generality and expressive power. In the light of the effectiveness
of LGCN for CF, our analysis is mainly based on LGCN and a simple
graph filter (SGF) ğ‘”(Ë†A). The difference between the two models lies
in the necessity of model training.
3.1 Generality
3.1.1 Performance Inconsistency under Different Densities. We first
evaluate graph filtering on datasets with different densities. We
change the data density by adjusting the training ratio ğ‘¥%(the
remaining is used as the test set), where ğ‘¥={80,60,40,20}%. We
choose an extensively used setting for CF with ğœƒ0=,Â·Â·Â·,=ğœƒğ¿[14]
whereğ‘”(Ë†A)is a low pass filter. We compare LGCN and SGF on
two datasets and report the results in Table 1. We observe that
the performance of SGF and LGCN consistently decrease and in-
crease as the training data is sparser, respectively. Particularly, SGF
tends to be effective on the dense data (e.g., ğ‘¥=80% and 60%)
and significantly outperforms LGCN, indicating the uselessness of
00.040.080.120.160.20.24 Recall@20
Top K%SGF
LGCN(a)ğ‘¥=80on CiteULike.
0.020.040.060.080.1Recall@20
Top K%SGF
LGCN (b)ğ‘¥=20on CiteULike.
00.030.060.090.12 Recall@20
Top K%SGF
LGCN
(c)ğ‘¥=80on Pinterest.
00.020.040.060.080.10.12 Recall@20
Top K%SGF
LGCN (d)ğ‘¥=20on Pinterest.
Figure 1: The accuracy (Recall@20) of SGF and LGCN when
only considering top- ğ¾% low frequencies.
model training. On the other hand, the positive effect of training
can be identified on the sparse data (e.g., ğ‘¥=20%and 40%) as LGCN
shows better performance. In addition, despite the improvement
brought by increasing the embedding size, the performance tends
to converge and still underperforms SGF on the dense data.
3.1.2 Noise Distribution Varies on Densities. The above observa-
tions show the inconsistency and lack of generality of LGCN and
SGF. Since SGF is a low pass filter, the poor performance on sparse
data leads to a reasonable assumption that low frequencies might be
polluted by noise. To verify this assumption, we conduct frequency
analysis with the following filter:
ğ‘”
Ë†A
=V(ğ¾)V(ğ¾)ğ‘‡, (7)
We investigate the noisiness of certain frequencies by increasing
ğ¾from 0 and observing how accuracy changes after introducing
certain frequencies. As the original polynomial filterÃğ¿
ğ‘™ğœ†ğ‘™
ğ‘˜empha-
sizes more on the lower frequencies leading to biased results, we
choose a uniform filter here. We focus on ğ‘¥=80%andğ‘¥=20%as
SGF (LGCN) is most (least) and least (most) effective, respectively.
We observe the followings from the results shown in Figure 1:
â€¢Onğ‘¥=80% ((a) and (c)), the accuracy of SGF increases then
significantly drops and rises again as ğ¾increases, and the best
performance outperforms LGCN, showing that the noise is con-
centrated in middle frequencies that can be removed by SGF.
â€¢Onğ‘¥=20% ((b) and (d)), SGF underperforms LGCN as ğ¾in-
creases, indicating that the noise is distributed across all frequen-
cies and pollutes the important features as well.
 
2390KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine
0.20.30.40.50.60.70.80.91Eigenvalue
Descending Order20%
40%
60%
80%
100%
(a)
0.40.50.60.70.80.91Î»'k/Î»k
Low                         â†’                         Middle40%
60%
80%
100% (b)
Figure 2: (a) Spectral distribution of CiteULike with differ-
ent density settings. (b) the eigenvalue ratio ( ğœ†â€²
ğ‘˜/ğœ†ğ‘˜) ofğ‘¥=
{40,60,80,100}(ğœ†â€²
ğ‘˜) toğ‘¥=20(ğœ†ğ‘˜).
â€¢The stable performance of LGCN when incorporating noisy fre-
quencies demonstrates the ability of model training of learning
from noise. While it tends to be redundant and results in worse
performance when the features are not polluted.
â€¢The superior performance when only incorporating low frequen-
cies shows that important graph features are distributed in low
frequencies on both settings.
The above observations verify our assumption that the poor perfor-
mance of SGF on sparse data is attributed to the noise distribution
that is across all frequencies. More importantly, both graph filters
and supervised training lack generality due to their inconsistent
performance on data with different densities, thus a more generic
design is required. In addition, we notice that the performance of
SGF is highly symmetric with respect to the middle frequency (i.e.,
ğœ†ğ‘›/2=0) which is due to the following theorem and corollary:
Theorem 1. Given PandQas the left and right singular vectors
ofË†R, we have the following relation:
V=P P
Qâˆ’Q
/âˆš
2. (8)
Particularly, given a eigenvalue ğœ†ğ‘˜>0with eigenvector vğ‘˜=
[pğ‘˜,qğ‘˜]ğ‘‡, there always exists a âˆ’ğœ†ğ‘˜with corresponding eigenvector
[pğ‘˜,âˆ’qğ‘˜]ğ‘‡.
Corollary 1. Letğ‘Ÿ(ğ¾)
ğ‘¢ğ‘–be the rating estimated based on the rep-
resentation of Equation (7), we have the following relation:
ğ‘Ÿ(ğ¾)
ğ‘¢ğ‘–=ğ‘Ÿ(ğ‘›âˆ’ğ¾)
ğ‘¢ğ‘–. (9)
Theorem and Corollary 1 show that low and high frequency are
actually highly symmetric due to the bipartivity of G. Therefore,
we can only focus on the low and middle frequencies with ğœ†ğ‘˜â‰¥0.
3.2 Expressive Power
Despite many existing works that can be summarized as LGCN
variants[ 12,14,29] choosing different filters to model user-item
relations, its expressive power is still unknown.
Theorem 2. Assuming Ë†Ahas no repeated eigenvalues, then LGCN
can produce arbitrary embeddings when ğ‘‘=1. Forğ‘‘>1, each
0.30.40.50.60.70.80.91
Low                        â†’                          MiddleÎ±=5
Î±=10
Î±=15
ğ›Œğ¤ /ğ›Œğ¤ (a)
0.40.50.60.70.80.91
Low                            â†’                          Middle   Îµ=-0.4
Îµ=-0.3
Îµ=-0.2
ğ›Œğ¤ /ğ›Œğ¤ (b)
Figure 3: Eigenvalue ratio of ËœAtoË†Awith varying ğ›¼andğœ–on
CiteULike.
dimension requires an individual filter:
O=V(BÎ˜)âŠ™Vğ‘‡E, (10)
where BâˆˆRğ‘›Ã—ğ‘›is a full-rank matrix with Bğ‘˜ğ‘™=ğœ†ğ‘™
ğ‘˜,Î˜âˆˆRğ‘›Ã—ğ‘‘is a
parameter matrix. Equation (10) degenerates to LGCN when ğ‘‘=1.
Theorem 2 shows that a shared global filter [ğœƒ1,Â·Â·Â·,ğœƒğ‘›]cannot
generate arbitrary multidimensional embeddings, and it is apparent
one-dimensional representation is not enough to characterize users
and items, which also demonstrates the incapability of LGCN to
generate the optimal representations and shows the poor expressive
power despite its effectiveness. Theorem 2 can be directly applied to
SGF by setting ğ‘‘=ğ‘›. Since the output has ğ‘›dimensions, it requires
ğ‘›different filters to fit arbitrary embeddings.
4 Methodology
4.1 Sharpness of Spectrum Matters
Ideally, if the spectrum is a low pass filter defined as follows:
ğœ†ğ‘˜=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³â‰«0ğ‘˜â‰¤ğ¾
â‰ˆ0ğ‘˜>ğ¾,(11)
whereğœ†1>Â·Â·Â·>ğœ†ğ‘›/2. Most of the energy is concentrated in the
top-ğ¾low frequencies while the information distributed in the
middle frequencies is trivial and noisy to data representations. In
this case, the important features and noise can be separated by
graph filtering without training. Inspired by this observation, we
report the eigenvalue distribution with different density settings in
Figure 2 (a). We observe that the denser data on which the noise
and important features tend to be separable has a sharper spectrum.
Particularly, in Figure 2 (b), we can see that the eigenvalue closer to
the middle frequency tends to drop faster than the low frequency.
This means that the energy of middle frequencies is close to 0 while
low frequencies stay important to data representation, causing the
important features and noise to be distributed in different frequen-
cies instead of mixed up together. Therefore, a sharper spectral
distribution is more desirable as it is closer to an ideal low pass
filter. Furthermore, consider a rank- ğ¾approximation consisting of
the top-ğ¾low frequencies: Ë†Ağ¾=Ãğ¾
ğ‘˜=1ğœ†ğ‘˜vğ‘˜vğ‘‡
ğ‘˜, then we have:
 
2391How Powerful is Graph Filtering for Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Theorem 3. LetËœAbe a normalized adjacency matrix of Gwith a
sharper spectral distribution than Ë†A:1=Ëœğœ†1
ğœ†1â‰¥Â·Â·Â·â‰¥Ëœğœ†ğ¾
ğœ†ğ¾>Ëœğœ†ğ¾+1
ğœ†ğ¾+1â‰¥
Â·Â·Â·â‰¥Ëœğœ†ğ‘›/2
ğœ†ğ‘›/2. Then ËœAğ¾is a better approximation than Ë†Ağ¾with the
following equation measuring the quality of an approximation:
appro
ËœAğ¾
=ËœAğ¾2
ğ¹ËœA2
ğ¹, (12)
where|Â·|ğ¹stands for the Frobenius norm.
Equation (12) measures to what extent ËœAğ¾can recover ËœAwhere
appro
ËœAğ¾
âˆˆ[0,1]. As the spectral distribution becomes sharper,
ËœAğ¾is a better approximation indicating that more important in-
formation is concentrated in the top- ğ¾low frequencies while the
remaining information is more trivial that is distributed in the
middle frequencies, causing them to be more separable.
4.2 Generalized Graph Normalization (G2N)
The analysis in Section 4.1 provides a solution to tackle the dilemma
mentioned in Section 3.1, that graph filters and supervised train-
ing perform inconsistently on data with different density settings
which is attributed to the varied noise distribution. If we are able
to generate a desirable spectrum in a way that clearly differentiates
important features from noise, it becomes possible to depend exclu-
sively on graph filters for recommendation. Since the spectrum is
closely related to how we normalize the graph, we study what graph
normalization leads to a desirable spectrum in this subsection.
According to Definition 1, we know that putting more emphasis
on low frequencies leads to a higher similarity between nodes and
neighborhood. Thus, it is reasonable to assume that more energy is
concentrated in the low frequencies if we increase the similarity by
modifying the graph normalization, leading to a sharper spectrum.
Sinceğ‘”(ğœ†ğ‘˜)shares the same eigenspace with ğœ†ğ‘˜, we can simply
define the similarity on ğ‘”(Ë†A)=Ë†A:
Ë†Ağ‘¢âˆ—âˆ‘ï¸
ğ‘£âˆˆN2ğ‘¢Ë†Ağ‘‡
ğ‘£âˆ—=(Vğ‘¢âˆ—âŠ™ğ€)Vğ‘‡  âˆ‘ï¸
ğ‘£Vğ‘£âˆ—âŠ™ğ€!
Vğ‘‡!ğ‘‡
=(Vğ‘¢âˆ—âŠ™ğ€) âˆ‘ï¸
ğ‘£Vğ‘£âˆ—âŠ™ğ€!ğ‘‡
,(13)
where ğ€is a vector containing all eigenvalues, Vğ‘¢âˆ—refers to the ğ‘¢-th
row of V,âŠ™is the operator for element-wise multiplication, N2ğ‘¢
stands for the second-order neighborhood as only the second-order
neighbors have similarity with ğ‘¢(there is no similarity between a
user and an item).
Definition 2. (Variation on the second-order Graph). The varia-
tion of the eigenvectors on the second-order graph is defined as:
vğ‘˜âˆ’Ë†A2vğ‘˜=1âˆ’ğœ†2
ğ‘˜âˆˆ[0,1]. (14)
Interpretation of Equation (13). Definition 2 measures the
difference between the signal samples of eigenvectors at each node
(Vğ‘¢ğ‘˜) and at its second-order neighbors (Ã
ğ‘£Vğ‘£ğ‘˜). Intuitively, vğ‘˜
with|ğœ†ğ‘˜|â†’ 1implies that the nodes are similar to their second-
order neighborhood: |Vğ‘¢ğ‘˜âˆ’Ã
ğ‘£Vğ‘£ğ‘˜|â†’ 0, while the middle fre-
quency with|ğœ†ğ‘˜|â†’ 0emphasizes the difference between them.Consider ğ€as a band-pass filter, if the node similarity increases, the
components with|ğœ†ğ‘˜|â†’ 1and|ğœ†ğ‘˜|â†’ 0should be correspondingly
emphasized and suppressed to make the equation hold, respectively,
leading to a sharper spectrum. In other words, the sharpness of the
spectral distribution is closely related to the average node similarity
defined on the normalized adjacency matrix.
Then, our question is transformed to: how do we increase the
node similarity through a new graph normalization? The original
setting is defined as Ë†Ağ‘¢ğ‘–=1âˆšğ‘‘ğ‘¢âˆšğ‘‘ğ‘–. Here, we define a renormal-
ized adjacency matrix with ËœAğ‘¢ğ‘–=ğ‘¤(ğ‘‘ğ‘¢)ğ‘¤(ğ‘‘ğ‘–), and the average
similarity between users and items can be defined as follows:
SIMğ‘ˆ=Ã
ğ‘¢,ğ‘£âˆˆUË†Ağ‘¢âˆ—Ë†Ağ‘‡ğ‘£âˆ—
|U|2=âˆ‘ï¸
ğ‘–âˆˆI2ğ‘¤(ğ‘‘ğ‘–)2Ã
ğ‘¢,ğ‘£âˆˆNğ‘–ğ‘¤(ğ‘‘ğ‘¢)ğ‘¤(ğ‘‘ğ‘£)
|U|2,
SIMğ¼=Ã
ğ‘–,ğ‘—âˆˆIË†Ağ‘‡
âˆ—ğ‘–Ë†Aâˆ—ğ‘—
|I|2=âˆ‘ï¸
ğ‘¢âˆˆU2ğ‘¤(ğ‘‘ğ‘¢)2Ã
ğ‘–,ğ‘—âˆˆNğ‘¢ğ‘¤(ğ‘‘ğ‘–)ğ‘¤(ğ‘‘ğ‘—)
|I|2,
(15)
whereNğ‘¢/Nğ‘–is the set of first-hop neighborhood of ğ‘¢/ğ‘–. We can
see that the user/item with a higher degree has a larger impact
on the average similarity (proportional to ğ‘‘2ğ‘¢/ğ‘‘2
ğ‘–), leading to the
conclusion that the higher weights over the high-degree nodes
results in higher node similarity. Based on the original design, we
can propose two new designs with higher weights over high-degree
nodes: (1)ğ‘¤(ğ‘‘ğ‘¢)=1âˆšğ‘‘ğ‘¢+ğ›¼, and (2)ğ‘¤(ğ‘‘ğ‘¢)=ğ‘‘ğœ–ğ‘¢, and propose a
generalized graph normalization as follows:
ËœA=(D+ğ›¼I)ğœ–A(D+ğ›¼I)ğœ–, (16)
whereğ›¼â‰¥0,ğœ–âˆˆ[âˆ’0.5,0].ËœA=Ë†Awhenğ›¼=0andğœ–=âˆ’0.5.
Theorem 4. Given Ëœğœ†ğ‘˜as the eigenvalue of ËœA, we have:
ğ‘‘ğ‘šğ‘ğ‘¥(ğ‘‘ğ‘šğ‘ğ‘¥+ğ›¼)2ğœ–ğœ†ğ‘˜â‰¥Ëœğœ†ğ‘˜â‰¥ğ‘‘ğ‘šğ‘–ğ‘›(ğ‘‘ğ‘šğ‘–ğ‘›+ğ›¼)2ğœ–ğœ†ğ‘˜, (17)
whereğ‘‘ğ‘šğ‘–ğ‘›andğ‘‘ğ‘šğ‘ğ‘¥ are the min and max node degree, respectively.
Particularly, increasing ğ›¼andğœ–shrinks and scales the eigenvalue,
respectively, making the spectrum not normalized any more. As we
focus on the sharpness of the spectrum, we normalize Ëœğœ†ğ‘˜and visu-
alize Ëœğœ†ğ‘˜/ğœ†ğ‘˜to observe how our proposed G2Nadjusts the spectrum
shown in Figure 3. We observe that the eigenvalue closer to the
middle frequency drops more quickly, while the one closer to the
low frequency tends to remain unchanged, and such a trend is more
obvious asğ›¼orğœ–increases. The results in Figure 3 indicate that
G2Ncan generate a desirable spectrum which is more equivalent
to an ideal low pass filter assuring that data noise and important
features are linearly separable without further training.
4.3 Individualized Graph Filtering (IGF)
Owing to G2N, we can only rely on graph filters for recommenda-
tion. By applying the results in Theorem 2, we can empower SGF
via the following enhanced model which is capable of generating
arbitrary embeddings:
O=V(BÎ˜)âŠ™Vğ‘‡=Â©Â­Â­
Â«VâŠ™ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘”1(ğœ†ğ‘˜)
...
ğ‘”ğ‘›(ğœ†ğ‘˜)ï£¹ï£ºï£ºï£ºï£ºï£ºï£»ÂªÂ®Â®
Â¬Â©Â­Â­
Â«VâŠ™ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘”1(ğœ†ğ‘˜)
...
ğ‘”ğ‘›(ğœ†ğ‘˜)ï£¹ï£ºï£ºï£ºï£ºï£ºï£»ÂªÂ®Â®
Â¬ğ‘‡
,
BÎ˜=BÎ˜1,Â·Â·Â·,Î˜ğ‘›
=ğ‘”2
1(ğœ†ğ‘˜),Â·Â·Â·,ğ‘”2ğ‘›(ğœ†ğ‘˜)
.(18)
 
2392KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine
Table 2: Statistics of datasets.
Datasets #User #Item #Interactions Density%
CiteULike 5,551 16,981 210,537 0.223
Pinterest 37,501 9,836 1,025,709 0.278
Yelp 25,677 25,815 731,672 0.109
Gowalla 29,858 40,981 1,027,370 0.084
Without loss of generality, we assume the filter BÎ˜is non-negative.
Here, each row of Vcan be considered as a user/item feature vector,
and[ğ‘”1(ğœ†ğ‘˜),Â·Â·Â·,ğ‘”ğ‘›(ğœ†ğ‘˜)]ğ‘‡is the corresponding individualized fil-
ters related to users/items. There are different ways to implement
the individualized filter, we present our solution as follows.
Definition 3. a (an) userâ€™s/itemâ€™s homophilic ratio measuring
the similarity of the the past interactions is defined as follows:
homo(ğ‘¢)=Ã
ğ‘–,ğ‘—âˆˆNğ‘¢1DG/ğ‘¢(ğ‘–,ğ‘—)<ğ›¿
|Nğ‘¢|2,
homo(ğ‘–)=Ã
ğ‘¢,ğ‘£âˆˆNğ‘–1DG/ğ‘–(ğ‘¢,ğ‘£)<ğ›¿
|Nğ‘–|2,(19)
whereDG(Â·,Â·)is the graph distance2,DG/ğ‘¢(ğ‘–,ğ‘—)measures the dis-
tance between ğ‘–andğ‘—which does not pass through ğ‘¢, and 1is an
indicator function producing 1 if two nodes are close enough (i.e., <ğ›¿).
Intuitively, if a userâ€™s interactions are similar (i.e., high homophilic
ratio), then it is possible his/her future behaviour is consistent with
the past interactions. While it is hard to rely on a userâ€™s past in-
teractions if they are quite different (low homophilic ratio). Given
that different frequencies emphasize the similarity between nodes
and neighborhood with different degrees, it is reasonable to im-
plement the individualized filter based on the homophilic ratio.
After evaluating multiple graph filters, we choose a monomial filter:
ğ‘”(ğœ†ğ‘˜)=ğœ†ğ›½
ğ‘˜, map the homophilic ratio to [ğ›½1,ğ›½2]via a linear func-
tion, where the min and max of the homophilic ratio is mapped to ğ›½1
andğ›½2, respectively. Then, the individualized filter is implemented
asğ‘”ğ‘¢(ğœ†ğ‘˜)=ğœ†ğ›½ğ‘¢
ğ‘˜, whereğ›½ğ‘¢âˆˆ[ğ›½1,ğ›½2]is determined by homo(ğ‘¢).
4.4 Simplified Graph Filtering for CF (SGFCF)
Theorem 5. The prediction matrix of SGF can be formulated as
follows:
Oğ‘ˆOğ‘‡
ğ¼=(Pğ‘‘ğ‘–ğ‘ğ‘”(ğœ“(ğœğ‘˜)))(Qğ‘‘ğ‘–ğ‘ğ‘”(ğœ”(ğœğ‘˜)))ğ‘‡, (20)
whereğœğ‘˜is the singular value of Ë†R,ğœ“(ğœğ‘˜)=Ã
ğ‘™={0,2,Â·Â·Â·}ğœğ‘™
ğ‘˜,ğœ”(ğœğ‘˜)=Ã
ğ‘™={1,3,Â·Â·Â·}ğœğ‘™
ğ‘˜.
Here,ğœ“(ğœğ‘˜)andğœ”(ğœğ‘˜)are equivalent to low pass filters making
no difference. By applying G2Nand IGF, we can design our SGFCF
by modifying Equation (20) as follows:
Oğ‘ˆOğ‘‡
ğ¼=
ËœP(ğ¾)âŠ™ğº(Ëœğœğ‘˜) 
ËœQ(ğ¾)âŠ™ğº(Ëœğœğ‘˜)
+ğ›¾ËœRËœRğ‘‡ËœR, (21)
whereğº(Ëœğœğ‘˜)=[ğ‘”1(Ëœğœğ‘˜),Â·Â·Â·,ğ‘”ğ‘›(Ëœğœğ‘˜)]ğ‘‡;ËœP(ğ¾)and ËœQ(ğ¾)are the top-
ğ¾left and right singular vectors corresponding to low frequencies,
2The number of edges in the shortest path between two nodes.Ëœğœğ‘˜is the singular value of ËœR=(Dğ‘ˆ+ğ›¼I)ğœ–R(Dğ¼+ğ›¼I)ğœ–, respectively.
In practice, we notice that non-low frequencies are not completely
noisy, thus we add a term Â¯RÂ¯Rğ‘‡Â¯Rcontaining all frequencies.
4.5 Discussion
Compared with existing GCN-based methods, our proposed SGFCF
mainly differs from them in three aspects: (1) We provide a closed-
form solution with complexity only as: O(ğ¾R++ğ¾2|U|+ğ¾2|I|).
(2) Our method is generalizable on datasets with different densi-
ties. (3) Compared with the methods that can be summarized as
LGCN, our method is proven to have stronger expressive power
which is capable of generating arbitrary embeddings. Particularly,
compared with non-parametric methods such as GFCF [ 37], our
superiority comes from two aspects: (1) GFCF is equivalent to a low
pass filter which does not consider the varied noise distribution on
data with different densities, thus is not generalizable. (2) It can be
summarized as a LGCN showing poor expressive power. We will
empirically demonstrate our superiority in Section 5.
5 Experiment
5.1 Experimental Setup
5.1.1 Datasets and Evaluation Metrics. We evaluate our method
on four datasets in this work, the statistics are summarized in
Table 2. CiteULike3is collected from a social bookmarking service
CiteULike which allows users to bookmark and share research
articles; Pinterest [ 15] is constructed for evaluating content-based
image recommendation; Yelp [ 16] is from the Yelp Challenge data;
Gowalla [ 44] is a check-in dataset which records the locations users
have visited. We focus on ğ‘¥=80%andğ‘¥=20%, randomly select 5%
as validation set, and leave the remaining for test. We adopt Recall
and nDCG [ 18], two widely used evaluation metrics for personalized
recommendation. The recommendation list is generated by ranking
unobserved items and truncating at position ğ‘˜=10.
5.1.2 Implementation. We use stochastic gradient descent (SGD)
as the optimizer for training-based models. The embedding size ğ‘‘
is set to 64, the regularization rate ğ›¾is set to 0.01 on all datasets,
the learning rate is tuned with step size 0.1, the model parameters
are initialized with Xavier initialization [ 11] and the batch size is
set to 256.ğ›¼â‰¥0andğœ–âˆˆ[âˆ’ 0.5,0]are tuned with step size 1 and
0.02, respectively. Other hyperparameters in this work are all tuned
with step size 0.1. We set ğ›¿=2which is the smallest graph distance
between homogeneous nodes; we use a monomial filter: ğ‘”(ğœ†ğ‘˜)=ğœ†ğ›½
ğ‘˜,
ğ›½1â‰¤ğ›½andğ›½2â‰¥ğ›½are tuned after determining the best ğ›½.
5.1.3 Baselines. We compare our proposed methods with com-
petitive baselines which can be categorized to training-based and
non-parametric (i.e., training-free) methods. For training-based
methods, we choose BPR [ 33] and DirectAU [ 42] implemented on
MF, and seven GCN-based methods: LightGCN [ 14], SGL-ED [ 47],
LightGCL [ 3], XSimGCL [ 50], DCCF [ 32], GDE [ 29], and JGCF [ 12].
Additionally, we adopt four non-parametric methods: EASE [ 39],
GF-CF [ 37], PGSP [ 25], and BPSM [ 8]. Note that the hyperparam-
eters are properly set after conducting tuning for them on the
datasets used in this work.
3https://github.com/js05212/citeulike-a
 
2393How Powerful is Graph Filtering for Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Performance comparison on CiteULike and Yelp. Improv.% denotes the improvement over the best baseline.
CiteULike
Yelp
x=80%
x=20% x=80% x=20%
Method nDCG
Recall nDCG Recall nDCG Recall nDCG Recall
T
raining-BasedBPR 0.1620
0.1778 0.0674 0.0621 0.0487 0.0607 0.0635 0.0600
DirectAU 0.2102
0.2260 0.1348 0.1280 0.0721 0.0872 0.0857
0.0839
LightGCN 0.1610
0.1777 0.1273 0.1206 0.0572 0.0721 0.0751 0.0725
SGL-ED 0.1890
0.2117 0.1217 0.1167 0.0676 0.0837 0.0817 0.0784
XSimGCL 0.2024
0.2229 0.1360 0.1289 0.0691 0.0847 0.0837 0.0809
LightGCL 0.2096
0.2214 0.1270 0.1224 0.0673 0.0836 0.0682 0.0661
DCCF 0.1641
0.1819 0.0791 0.0740 0.0626 0.0746 0.0668 0.0627
GDE 0.1890
0.2055 0.1518 0.1429 0.0653
0.0805 0.0866 0.0839
JGCF 0.1557
0.1752 0.1438 0.1386 0.0626 0.0789 0.0895 0.0868
T
raining-FreeEASE 0.2368
0.2468 0.1313 0.1211 0.0719 0.0859 0.0360 0.0346
GFCF 0.2405 0.2562 0.0902
0.0896 0.0644 0.0806 0.0722 0.0725
BPSM 0.2333
0.2466 0.0886 0.0872 0.0622 0.0748 0.0113 0.0110
PGSP 0.2357
0.2501 0.1121 0.1105 0.0643 0.0789 0.0732 0.0717
OursSGFCF 0.2663
0.2797 0.1542 0.1478 0.0824 0.0998 0.0963 0.0930
Improv.% +8.69
+9.17 +1.58 +3.43 +14.29 +14.45 +7.60 +3.56
Table 4: Performance comparison on Pinterest and Gowalla. Improv.% denotes the improvement over the best baseline.
Pinter
est Gowalla
x=80%
x=20% x=80% x=20%
Method nDCG
Recall nDCG Recall nDCG Recall nDCG Recall
T
raining-BasedBPR 0.0668
0.0780 0.0976 0.0946 0.1164 0.1186 0.1086 0.0917
DirectAU 0.0840
0.0964 0.1338 0.1289 0.1286 0.1349 0.1864 0.1648
LightGCN 0.0699
0.0828 0.1297 0.1263 0.0987 0.1074 0.1477 0.1368
SGL-ED 0.0755
0.0888 0.1355 0.1300 0.1343 0.1417 0.1789 0.1563
XSimGCL 0.0842
0.0973 0.1396 0.1346 0.1288 0.1392 0.1861 0.1643
LightGCL 0.0717
0.0826 0.1342 0.1287 0.1368 0.1436 0.1524 0.1329
DCCF 0.0767
0.1002 0.1241 0.1151 0.1179 0.1181 0.1452 0.1244
GDE 0.0748
0.0862 0.1403 0.1351 0.1261
0.1313 0.1847 0.1645
JGCF 0.0765
0.0885 0.1363 0.1305 0.1173 0.1260 0.1845 0.1631
T
raining-FreeEASE 0.0853
0.0964 0.1064 0.1020 0.1350 0.1440 0.1405 0.1247
GFCF 0.0859
0.0981 0.0851 0.0857 0.1387 0.1483 0.1662
0.1474
BPSM 0.0858
0.0950 0.0567 0.0558 0.1432 0.1482
0.0737 0.0694
PGSP 0.0876 0.1000 0.1264
0.1219 0.1395 0.1462 0.1640 0.1450
OursSGFCF 0.0923
0.1052 0.1437 0.1384 0.1432 0.1527 0.1973 0.1762
Improv.% +5.36
+4.80 +2.42 +2.44 +0.00 +2.97 +5.85 +6.92
5.2 Comparison
5.2.1 Accuracy. We report the accuracy of baselines and our method
in Table 3 and 4. We have the following observations:
â€¢Overall, GCN-based methods show better performance espe-
cially on sparse data, indicating the superior ability to tackle
data sparsity by incorporating higher-order neighborhood. For
instance, LightGCN underperforms BPR on two datasets with
ğ‘¥=80%while significantly outperforms BPR on four datasets
with sparse setting ğ‘¥=20%.
â€¢Comparing non-parametric methods (i.e., Ease, GFCF, PGSP
and BPSM) with training-based methods, we can see that non-
parametric methods tend to be effective on dense data (e.g.,
ğ‘¥=80%) and show relatively poor performance on sparse data
(e.g.,ğ‘¥=20%). For instance, GFCF achieves the best baseline
onğ‘¥=80% while shows the second worst performance on
ğ‘¥=20%on CiteULike. The relatively poor performance of GFCFon sparse data also verifies our previous analysis in Section 3.1
that a simple low pass filter cannot work well on datasets with
different densities due to the varied noise distribution.
â€¢Contrary to non-parametric methods, GCN-based methods re-
quiring training such as LightGCN, GDE, and JGCF show supe-
rior performance on sparse data while are less effective on dense
data, which further verifies the analysis in Section 3.1 showing
the performance inconsistency of graph filters and supervised
training on data with different densities.
â€¢Our proposed SGFCF, significantly outperforms competitive
baselines almost across all datasets, demonstrating the effec-
tiveness of our proposed method. Particularly, SGFCF outper-
forms GFCF which shares similarities with our designs by 14.0%
onğ‘¥=80% and by 41.0% on ğ‘¥=20% on average, in terms
of nDCG@10. The larger improvement on sparser data further
proves the superiority of our proposed designs over GFCF.
 
2394KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine
0.5 0.44 0.38 0.32
0.050.0550.060.0650.070.0750.080.085|Îµ|nDCG@10
Î±Î±
Îµ
(a)ğ‘¥=80% on Yelp.
0.5 0.44 0.41 0.38 0.35 0.32
0.0650.070.0750.080.0850.090.095|Îµ|nDCG@10
Î±Î±
Îµ (b)ğ‘¥=20% on Yelp.
0.5 0.45 0.4 0.35 0.03 0.025
0.2150.2250.2350.2450.2550.265|Îµ|nDCG@10
Î±Î±
Îµ
(c)ğ‘¥=80% on CiteULike.
0.5 0.45 0.4 0.35 0.03 0.025
0.070.090.110.130.15|Îµ|nDCG@10
Î±Î±
Îµ (d)ğ‘¥=20% on CiteULike.
Figure 4: How performance changes with varying ğ›¼andğœ–.
Table 5: Comparison on training time.
Dataset Y
elp Go
walla
Method x=80%
x=20% x=80%
x=20%
Dir
ectAU 3.33Ã—103s7.92Ã—102s4.53Ã—103s3.30Ã—104s
LightGCN 1.13Ã—104s2.73Ã—103s5.09Ã—103s1.22Ã—103s
GFCF 71.0s 1.47Ã—102s 48.1s 2.40Ã—102s
SGFCF 6.3s
2.4s 12.3s
5.5s
5.2.2 Efficiency. We report the training time of SGFCF and several
baselines that have been shown efficient in Table 5, where the re-
sults are obtained on a server equipped with AMD Ryzen 9 5950X
and GeForce RTX 3090. Despite the light architecture compared
with other GCN-based methods, LightGCN still requires much more
training time than DirectAU whose complexity is comparable to MF.
However, it requires hundreds of training epochs before conver-
gence for training-based methods due to the non-convex loss func-
tions, causing them to be inefficient compared with GFCF which
does not require model training. Our proposed SGFCF achieves
over 10x and 1000x speedup over GFCF and LightGCN, respectively.
Particularly, the higher efficiency of SGFCF over GFCF is attrib-
uted to G2N. By generating a desirable spectrum via G2N, graph
information is concentrated in fewer low frequency components,
reducing the number of required spectral features ğ¾. For instance,
ğ¾=100, 50, and 90 on CiteULike, Yelp, and Gowalla with ğ‘¥=20%
when the accuracy is maximized, as opposed to ğ¾=512on GFCF.
5.3 Study of SGFCF
5.3.1 Effect of ğ›¼andğœ–.We study how accuracy changes with ğ›¼and
ğœ–and report the results in Figure 4. Since we showed that either ğ›¼or
0.2550.2570.2590.2610.2630.265nDCG@10
Î³(a)ğ‘¥=80% on CiteULike.
0.1430.1450.1470.1490.1510.153nDCG@10
Î³ (b)ğ‘¥=20% on CiteULike.
Figure 5: How performance changes with varying ğ›¾.
0.1360.1380.140.142 nDCG@10
Î²1
(a)ğ‘¥=20% on Gowalla.
0.1890.1910.1930.1950.197 nDCG@10
Î²2 (b)ğ‘¥=80% on Gowalla.
Figure 6: How performance changes with ğ›½1andğ›½2.
ğœ–can adjust the sharpness of spectrum in Section 4.2, we separately
tune them and choose the better one, where we fix ğ›¼=0andğœ–=
âˆ’0.5when studying the other one. We can observe that the accuracy
is more sensitive to ğœ–thanğ›¼. Note thatğ›¼â†’+âˆ andğœ–=0when
nodes are equally weighted, thus1âˆšğ‘‘ğ‘¢+ğ›¼is a smoother function than
ğ‘‘ğœ–ğ‘¢sinceğ›¼âˆˆ[0,+âˆ] whileğœ–âˆˆ[âˆ’ 0.5,0], explaining why ğ›¼overall
performs better than ğœ–. Moreover, the hyperparameter value is
larger on the dense setting ğ‘¥=80%than the sparse setting ğ‘¥=20%.
A reasonable explanation is that nodes have smaller degrees on
the sparse setting on average, on which the model performance is
more sensitive to changes in hyperparameters. We can also see the
difference between the values on ğ‘¥=20%andğ‘¥=80%when the
best performance is achieved is roughly consistent with their node
degree difference (i.e., around 4 times).
5.3.2 Effect of ğ›¾.We study the effect of ğ›¾and report the results in
Table 6 and Figure 5. We can see that introducing ğ›¾leads to a better
performance, demonstrating that middle frequency components
still contain some information contributing to the data representa-
tions. Particularly, the improvement on the dense setting ğ‘¥=80%
tends to be more significant than that on the sparse setting ğ‘¥=20%.
Intuitively, the sparser data are composed of fewer spectral features.
For instance, only ğ¾=20spectral features are required on Yelp
withğ‘¥=20%when the best performance is achieved, as opposed
toğ¾=300withğ‘¥=80%, and we observe a similar trend on other
datasets. This observation implies that the middle frequencies are
more noisy and useless on sparse datasets. The results in Figure 5
 
2395How Powerful is Graph Filtering for Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 6: The improvement% of ğ›¾in terms of nDCG@10.
Yelp CiteULike Pinterest Gowalla
x=80% 2.35 4.00 0.55 3.66
x=20% 0.21 3.66 0.42 0.42
further verifies our observation, that the accuracy is more sensitive
to the change in ğ›¾on sparse setting.
5.3.3 Effect of IGF. We show how accuracy changes with ğ›½1and
ğ›½2in Figure 6, where we fix ğ›½2=2.0in (a) andğ›½1=2.5in (b).
We can observe that an individualized filter adapting to different
users/items shows better performance than a shared graph filter.
5.3.4 Impact of ğ›¿.Intuitively, the homophilic ratio of distinctive
users/items tends to shift towards 1 as increasing ğ›¿, making their
difference more insignificant. Meanwhile, a larger ğ›¿also results in
higher computational complexity. As shown in Table 7, the best
performance is achieved at ğ›¿=2on most settings except the slight
improvement on CiteULike with ğ‘¥=20%. Thus, we set ğ›¿=2
considering the trade-off between effectiveness and efficiency.
6 Related Work
6.1 Collaborative Filtering
Collaborative filtering is a fundamental task for recommender sys-
tems as it provides recommendations by learning from the user-
item historical interactions without rely on specific knowledge
or user and item profiles. The underling assumption of CF is that
similar users tend to have similar preference [ 2,35]. Matrix fac-
torization [ 22], one of the simplest yet effective methods for CF,
characterizes users and items as learnable low-dimensional vectors,
where the rating between a user and an item is estimated as the
inner product between user and item vectors. Most CF methods can
be considered as enhanced MF variants addressing drawbacks of
MF which can be mainly classified into three categories: (1) Due to
the limited available data, some works incorporate side information
to help infer user preference. Rendle et al. [ 34] introduces temporal
information and combine MF with Markov chain (MC) to predict
usersâ€™ next behaviour. Ma et al. [ 26] integrates social relations and
user-item interactions with MF. [ 24] is an enhanced MF to incor-
porate geological information. (2) To address the drawback that
MF uses a simple linear function to model complex user-item rela-
tions, much effort has been devoted to exploit advanced algorithms
to learn form user-item interactions, such as multilayer percep-
tron [ 15,48], autoencoder [ 36], attention mechanism [ 5], trans-
former [ 40], etc. (3) Due to the data sparsity, negative sampling is
critical to generate desirable data representations. Therefore, a lot
of effective sampling strategies have been proposed [27, 28, 42].
6.2 GCN-based CF methods
Graph convolution networks (GCNs) have shown great potential
in recommender systems and collaborative filtering (CF). Early
attempts simply apply classic GCN architectures for CF which
do not necessarily show desirable performance in recommenda-
tion. For instance, SpectralCF [ 51] shares similarities with Cheb-
Net [ 10], the designs of NGCF [ 44] are based on GCN [ 21], andTable 7: The effect of ğ›¿evaluated by nDCG@10.
Settings
Datasets ğ›¿=2ğ›¿=4ğ›¿=6
x=80%CiteULike 0.2658
0.2631 0.2629
Yelp 0.0825 0.0823 0.0824
Pinterest 0.0919 0.0919 0.0919
Gowalla 0.1432 0.1386 0.1386
x=20%CiteULike 0.1542
0.1547 0.1548
Yelp 0.0963 0.0960 0.0953
Pinterest 0.1436 0.1438 0.1439
Gowalla 0.1971 0.1951 0.1932
PinSage [ 49] is closely related to GraphSAGE [ 13]. Subsequent
works show the redundancy of GCN-based methods such as non-
linearity and linear transformation [ 6,14], demystify how GCNs
contribute to recommendation [ 30,31] and analyze the expressive
power of GCN for recommendation [ 4,37]. Furthermore, research
effort has also be devoted to empower GCN with other advanced
algorithms, such as transformer [ 23], sampling strategy [ 17], con-
trastive learning [19, 47], etc. and achieve further improvement.
Spectral-bsed GCNs, focusing the spectral domain of graphs,
have also received much attention [ 1,46]. By analyzing GCN from
a perspective of graph signal processing, recent works show that
GCN is essentially a low pass filter, and low/high frequencies are
significantly contributive to recommendation accuracy [ 29,37].
Based on this finding, several spectral GCN-based methods have
been proposed and show superiority, that can be classified into two
categories: (1) non-parametric graph filters [ 25,37] and (2) graph
filters combined with supervised training [ 12,29]. However, we
empirically demonstrated that they fail to perform well on datasets
with different densities due to the varied noise distribution.
7 Conclusion
In this work, we addressed two limitations of existing GCN-based
methods: the lack of generality and expressive power. We proposed
a generalized graph normalization ( G2N) to adjust the sharpness of
spectrum, making graph filtering generalizable on datasets with dif-
ferent densities, and an individualized graph filtering (IGF), where
we emphasize different frequencies based on the homophilic ratio
measuring the distinct confidence levels of user preference that
interactions can reflect, which is proved to generate arbitrary em-
beddings. Finally, we proposed a simplified graph filtering for CF
(SGFCF) requiring only the top- ğ¾singular values. Extensive ex-
perimental results on four datasets demonstrated the effectiveness
and efficiency of our proposed designs. In future work, we plan to
analyze the potential of GCNs from other perspectives and apply
our proposed method to other recommendation tasks.
Acknowledgement
This paper is based on results obtained from the project, â€œResearch
and Development Project of the Enhanced infrastructures for Post-
5G Information and Communication Systemsâ€ (JPNP20017), com-
missioned by the New Energy and Industrial Technology Develop-
ment Organization (NEDO).
 
2396KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine
References
[1]Muhammet Balcilar, Renton Guillaume, et al .2021. Analyzing the Expressive
Power of Graph Neural Networks in a Spectral Perspective. In ICLRâ€™21.
[2]John S Breese, David Heckerman, and Carl Kadie. 1998. Empirical Analysis of
Predictive Algorithms for Collaborative Filtering. In UAIâ€™98. 43â€“52.
[3]Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple
Yet Effective Graph Contrastive Learning for Recommendation. In ICLRâ€™23.
[4]Xuheng Cai, Lianghao Xia, Xubin Ren, and Chao Huang. 2023. How Expressive
are Graph Neural Networks in Recommendation?. In CIKMâ€™23. 173â€“182.
[5]Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-
Seng Chua. 2017. Attentive Collaborative Filtering: Multimedia Recommendation
with Item- and Component-Level Attention. In SIGIRâ€™17. 335â€“344.
[6]Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting
graph based collaborative filtering: A linear residual graph convolutional network
approach. In AAAI-20. 27â€“34.
[7]Mengru Chen, Chao Huang, Lianghao Xia, Wei Wei, Yong Xu, and Ronghua
Luo. 2023. Heterogeneous Graph Contrastive Learning for Recommendation. In
WSDMâ€™23. 544â€“552.
[8]Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. 2023.
Blurring-Sharpening Process Models for Collaborative Filtering. In SIGIRâ€™23.
1096â€“1106.
[9]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural Networks for
Youtube Recommendations. In RecSysâ€™16. 191â€“198.
[10] MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convo-
lutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In
Neuripsâ€™16. 3844â€“3852.
[11] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In AISTATSâ€™10. 249â€“256.
[12] Jiayan Guo, Lun Du, Xu Chen, Xiaojun Ma, Qiang Fu, Shi Han, Dongmei Zhang,
and Yan Zhang. 2023. On Manipulating Signals of User-Item Graph: A Jacobi
Polynomial-Based Graph Collaborative Filtering. In KDDâ€™23. 602â€“613.
[13] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation
Learning on Large Graphs. In Neuripsâ€™17. 1024â€“1034.
[14] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network
for Recommendation. In SIGIRâ€™20. 639â€“648.
[15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural Collaborative Filtering. In WWWâ€™17. 173â€“182.
[16] Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast
Matrix Factorization for Online Recommendation with Implicit Feedback. In
SIGIRâ€™16. 549â€“558.
[17] Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu
Wang, and Jie Tang. 2021. MixGCF: An Improved Training Method for Graph
Neural Network-based Recommender Systems. In KDDâ€™21. 665â€“674.
[18] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated Gain-Based Evaluation
of IR Techniques. TOIS 20, 4 (2002), 422â€“446.
[19] Yangqin Jiang, Chao Huang, and Lianghao Huang. 2023. Adaptive Graph Con-
trastive Learning for Recommendation. In KDDâ€™23. 4252â€“4261.
[20] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive Sequential Recom-
mendation. In ICDMâ€™18. 197â€“206.
[21] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLRâ€™17.
[22] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Tech-
niques for Recommender Systems. Computer 8 (2009), 30â€“37.
[23] Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, and Chao Huang.
2023. Graph Transformer for Recommendation. In SIGIRâ€™23. 1680â€“1689.
[24] Defu Lian, Cong Zhao, Xing Xie, Guangzhong Sun, Enhong Chen, and Yong
Rui. 2014. GeoMF: Joint Geographical Modeling and Matrix Factorization for
Point-of-Interest Recommendation. In KDDâ€™14. 831â€“840.
[25] Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, and Ning
Gu. 2023. Personalized Graph Signal Processing for Collaborative Filtering. In
WWWâ€™23. 1264â€“1272.
[26] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: Social
Recommendation Using Probabilistic Matrix Factorization. In ICDMâ€™08. 931â€“940.[27] Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao,
and Xiuqiang He. 2021. SimpleX: A Simple and Strong Baseline for Collaborative
Filtering. In CIKMâ€™21. 1243â€“1252.
[28] Seongmin Park, Mincheol Yoon, Jae-woong Lee, Hogun Park, and Jongwuk Lee.
2023. Toward a Better Understanding of Loss Functions for Collaborative Filtering.
InCIKMâ€™23. 2034â€“2043.
[29] Shaowen Peng, Kazunari Sugiyama, and Tsunenori Mine. 2022. Less is More:
Reweighting Important Spectral Graph Features for Recommendation. In SIGIRâ€™22.
1273â€“1282.
[30] Shaowen Peng, Kazunari Sugiyama, and Tsunenori Mine. 2022. SVD-GCN: A
Simplified Graph Convolution Paradigm for Recommendation. In CIKMâ€™22. 1625â€“
1634.
[31] Shaowen Peng, Kazunari Sugiyama, and Tsunenori Mine. 2024. Less is More:
Removing Redundancy of Graph Convolutional Networks for Recommendation.
TOIS 42, 3 (2024), 1â€“26.
[32] Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Dis-
entangled Contrastive Collaborative Filtering. In SIGIRâ€™23. 1137â€“1146.
[33] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. In UAIâ€™09. 452â€“
461.
[34] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factoriz-
ing Personalized Markov Chains for Next-Basket Recommendation. In WWWâ€™10.
811â€“820.
[35] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based
Collaborative Filtering Recommendation Algorithms. In WWWâ€™01. 285â€“295.
[36] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.
Autorec: Autoencoders Meet Collaborative Filtering. In WWWâ€™15. 111â€“112.
[37] Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, B Khaled Letaief, and
Dongsheng Li. 2021. How Powerful is Graph Convolution for Recommendation?.
InCIKMâ€™21. 1619â€“1629.
[38] Daniel Spielman. 2012. Spectral graph theory. Combinatorial scientific computing
18 (2012).
[39] Harald Steck. 2019. Embarrassingly Shallow Autoencoders for Sparse Data. In
WWWâ€™19. 3251â€“3257.
[40] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre-
sentations from Transformer. In CIKMâ€™19. 1441â€“1450.
[41] Jianing Sun, Zhaoyue Cheng, Saba Zuberi, Felipe PÃ©rez, and Maksims Volkovs.
2021. HGCF: Hyperbolic Graph Convolution Networks for Collaborative Filtering.
InWWWâ€™21. 593â€“601.
[42] Chenyang Wang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu,
and Shaoping Ma. 2022. Towards Representation Alignment and Uniformity in
Collaborative Filtering. In KDDâ€™22. 1816â€“1825.
[43] Wenjie Wang, Yiyan Xu, Fuli Feng, Xinyu Lin, Xiangnan He, and Tat-Seng Chua.
2023. Diffusion Recommender Model. In SIGIRâ€™23. 832â€“841.
[44] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural Graph Collaborative Filtering. In SIGIRâ€™19. 165â€“174.
[45] Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua.
2020. Disentangled Graph Collaborative Filtering. In SIGIRâ€™20. 1001â€“1010.
[46] Xiyuan Wang and Muhan Zhang. 2022. How Powerful are Spectral Graph Neural
Networks. In ICMLâ€™22. 23341â€“23362.
[47] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian,
and Xing Xie. 2021. Self-Supervised Graph Learning for Recommendation. In
SIGIRâ€™21. 726â€“735.
[48] Hong-Jian Xue, Xinyu Dai, et al .2017. Deep Matrix Factorization Models for
Recommender Systems. In IJCAI-17. 3203â€“3209.
[49] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale
Recommender Systems. In KDDâ€™18. 974â€“983.
[50] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and
Hongzhi Yin. 2024. XSimGCL: Towards Extremely Simple Graph Contrastive
Learning for Recommendation. TKDE 36, 2 (2024), 913â€“926.
[51] Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, and Philip S Yu. 2018. Spectral
Collaborative Filtering. In RecSysâ€™18. 311â€“319.
 
2397How Powerful is Graph Filtering for Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 8: Ablation study on SGFCF evaluated by nDCG@10.
Dataset Y
elp CiteULike Go
walla
Setting x=80%
x=20% x=80%
x=20% x=80%
x=20%
SGFCF 0.0824
0.0963 0.2667
0.1542 0.1432
0.1973
w/o IGF 0.0820
0.0949 0.2651
0.1532 0.1388
0.1922
w/o IGF
&G2N0.0537
0.0729 0.2287
0.0924 0.1255
0.1313
Table 9: Performance comparison on x=40% and 60%.
Setting Metho
ds
Dataset Metric LightGCN
GFCF JGCF SGFCF Improv.%
CiteULikex=40%nDCG 0.2120 0.2021
0.2095 0.2440 +15.09
Recall 0.1989 0.1951
0.1994 0.2298 +15.54
x=60%nDCG 0.2023
0.2502 0.2039 0.2739 +9.47
Re
call 0.1969
0.2425 0.1986 0.2622 +8.12
Pinter
estx=40%nDCG 0.1204
0.1275 0.1294 0.1371 +5.95
Re
call 0.1170
0.1221 0.1241 0.1313 +5.80
x=60%nDCG 0.0912
0.1064 0.1025 0.1117 +4.98
Re
call 0.0891
0.1022 0.0990 0.1067 +4.40
Table 10: Performance (nDCG@10) on different graph fil-
ters.
Dataset Y
elp Pinter
est Go
walla
Setting x=80%
x=20% x=80%
x=20% x=80%
x=20%
Monomial 0.0820
0.0949 0.0922
0.1435 0.1388
0.1922
Exp 0.0816
0.0940 0.0919
0.1432 0.1385 0.1922
Marko
v 0.0802
0.0945 0.0904
0.1406 0.1335
0.1714
Jacobi 0.0770
0.0784 0.0856
0.1425 0.1381
0.1909
A Supplementary Experiments
A.1 Ablation Study
To demonstrate the effectiveness of our proposed designs, we com-
pare three variants: (1) SGFCF, (2) SGFCF without IGF, and (3)
SGFCF without both G2Nand IGF, and report the results in Ta-
ble 8. We observe that removing either of G2Nand IGF results
in performance degradation, showing that both G2Nand IGF are
contributive to model performance. Particularly, IGF tends to be
more effective on the sparse setting, indicating that it might require
stronger expressive power to generate the optimal representations
on the sparse data. Moreover, G2Ncontributes more to the accuracy
than IGF, as the poor performance of graph filters is mainly due to
the varied noise distribution polluting the low frequencies being
important to the data representations.
A.2 Experiments on Other Density Settings
To further verify the effectiveness of our methods, we compare our
SGFCF with several competitive baselines on x=40% and 60%, and
report the results in Table 9. We can observe that GFCF without
training shows better performance on the denser setting x=60%,
while training-based methods JGCF and LightGCN achieve better
results on the sparser setting x=40% as supervised training shows
superior ability learning from noisy data. Our proposed SGFCF
outperforms all baselines across the board, demonstrating the gen-
erality of our proposed designs.A.3 Graph Filter Designs
We compare four commonly used graph filters for recommendation:
monomial filter, exponential diffusion kernel, Markov diffusion ker-
nel, and Jacobi polynomials (with detailed introduction as follows)
and report the results in Table 10. Overall, the monomial filter: ğœ†ğ›½
ğ‘˜with the simplest design shows better performance than other fil-
ters. Due to the important features that are concentrated in only a
few low frequencies via G2N, a simple increasing function is already
able to appropriately emphasize different frequencies according to
their importance instead of complicated band-pass filters such as
the Markov diffusion kernel and Jacobi polynomials.
A.3.1 Monomial Filter. The eigenvalue of Ë†Ağ‘™isğœ†ğ‘™
ğ‘˜, here we extend
it toğœ†ğ›½
ğ‘˜whereğ›½â‰¥0. It is a simple increasing function.
A.3.2 Exponential Diffusion Kernel. The exponential diffusion ker-
nel is defined as:
exp(ğ›½Ë†A)=âˆâˆ‘ï¸
ğ‘™=0ğ›½ğ‘™Ë†Ağ‘™
ğ‘™!, (22)
where the corresponding eigenvalue is ğ‘’ğ›½ğœ†ğ‘˜.
A.3.3 Markov Diffusion Kernel. The Markov diffusion kernel is de-
fined as:1
ğ¿Ãğ¿
ğ‘™=0Ë†Ağ‘™where the corresponding eigenvalue isÃğ¿
ğ‘™=0ğœ†ğ‘™
ğ‘˜
ğ¿.
A.3.4 Jacobi Polynomials. The Jacobi basis for ğ‘˜â‰¥2is defined as:
ğ‘ƒğ‘,ğ‘
ğ‘˜(Ë†A)E=ğœƒğ‘˜Ë†Ağ‘ƒğ‘,ğ‘
ğ‘˜âˆ’1(Ë†A)E+ğœƒâ€²
ğ‘˜ğ‘ƒğ‘,ğ‘
ğ‘˜âˆ’1(Ë†A)Eâˆ’ğœƒâ€²â€²
ğ‘˜ğ‘ƒğ‘,ğ‘
ğ‘˜âˆ’2(Ë†A)E,(23)
where
ğœƒğ‘˜=(2ğ‘˜+ğ‘+ğ‘)(2ğ‘˜+ğ‘+ğ‘âˆ’1)
2ğ‘˜(ğ‘˜+ğ‘+ğ‘),
ğœƒâ€²
ğ‘˜=(2ğ‘˜+ğ‘+ğ‘âˆ’1)(ğ‘2âˆ’ğ‘2)
2ğ‘˜(ğ‘˜+ğ‘+ğ‘)(2ğ‘˜+ğ‘+ğ‘âˆ’2),
ğœƒâ€²â€²
ğ‘˜=(ğ‘˜+ğ‘âˆ’1)(ğ‘˜+ğ‘âˆ’1)(2ğ‘˜+ğ‘+ğ‘)
ğ‘˜(ğ‘˜+ğ‘+ğ‘)(2ğ‘˜+ğ‘+ğ‘âˆ’2).(24)
Forğ‘˜<2,
ğ‘ƒğ‘,ğ‘
0(Ë†A)E=E,
ğ‘ƒğ‘,ğ‘
0(Ë†A)E=E=ğ‘âˆ’ğ‘
2E+ğ‘+ğ‘+2
2Ë†AE.(25)
The final representations are generated by summing up the embed-
dings from each iteration:Ãğ¿
ğ‘˜=0ğ‘ƒğ‘,ğ‘
ğ‘˜(Ë†A).
B Proofs
B.1 Proof of Theorem 1 and Corollary 1
Proof. Letpğ‘˜,qğ‘˜, andğœğ‘˜be the left, right singular vector, and
the singular value of Ë†R, we can show the following relations:
0 Ë†R
Ë†Rğ‘‡0 pğ‘˜
qğ‘˜
=Ë†Rqğ‘˜
Ë†Rğ‘‡pğ‘˜
=ğœğ‘˜pğ‘˜
qğ‘˜
,
0 Ë†R
Ë†Rğ‘‡0 pğ‘˜
âˆ’qğ‘˜
=âˆ’Ë†Rqğ‘˜
Ë†Rğ‘‡pğ‘˜
=âˆ’ğœğ‘˜pğ‘˜
qğ‘˜
,(26)
whereğœğ‘˜andâˆ’ğœğ‘˜are two eigenvalues of Ë†A, with[pğ‘˜,qğ‘˜]and
[pğ‘˜,âˆ’qğ‘˜]as the corresponding eigenvectors, respectively. Accord-
ing to Theorem 1, it is easy to show ğ‘Ÿ(ğ¾)
ğ‘¢ğ‘–=ğ‘Ÿ(ğ‘›âˆ’ğ¾)
ğ‘¢ğ‘–=P(ğ¾)Q(ğ¾)ğ‘‡.
 
2398KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine
Corollary 1 implies that the rating estimated by the high frequencies
are exactly opposite to the symmetric (to ğœ†ğ‘›/2=0) low frequencies
based on Equation (7). â–¡
B.2 Proof of Theorem 2
Proof. We first consider ğ‘‘=1. If LGCN can generate arbitrary
embeddings, then the following Equation holds:
Vğ‘‡O=ğ‘‘ğ‘–ğ‘ğ‘”(ğ‘”(ğœ†ğ‘˜))Vğ‘‡E, (27)
which is equivalent to the following Equation:
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°(Vğ‘‡O)1
(Vğ‘‡E)1...
(Vğ‘‡O)ğ‘›
(Vğ‘‡E)ğ‘›ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Ã
ğ‘™ğœƒğ‘™ğœ†ğ‘™
1...Ã
ğ‘™ğœƒğ‘™ğœ†ğ‘™ğ‘›ï£¹ï£ºï£ºï£ºï£ºï£ºï£»=Bï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğœƒ1
...
ğœƒğ‘›ï£¹ï£ºï£ºï£ºï£ºï£ºï£», (28)
where BâˆˆRğ‘›Ã—ğ¿,Bğ‘˜ğ‘™=ğœ†ğ‘™
ğ‘˜. By extending the order of ğ‘”(ğœ†ğ‘˜)to
ğ‘›(i.e.,ğ¿=ğ‘›),Bbecomes a full rank matrix as long as Ë†Ahas no
repeated eigenvalue, thus we can always find a solution to satisfy
Equation (28). To generalize the above result to the situation of
ğ‘‘>1, multiple filters are required:
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°(Vğ‘‡O)11
(Vğ‘‡E)11,Â·Â·Â·,(Vğ‘‡O)1ğ‘‘
(Vğ‘‡E)1ğ‘‘......
(Vğ‘‡O)ğ‘›1
(Vğ‘‡E)ğ‘›1,Â·Â·Â·,(Vğ‘‡O)ğ‘›ğ‘‘
(Vğ‘‡E)ğ‘›ğ‘‘ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=Bï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğœƒ11,Â·Â·Â·,ğœƒ1ğ‘‘
...
ğœƒğ‘›1,Â·Â·Â·,ğœƒğ‘›ğ‘‘ï£¹ï£ºï£ºï£ºï£ºï£ºï£», (29)
By applying Equation (29) to LGCN, we get Equation (10). â–¡
B.3 Proof of Theorem 3
Proof. We first show that the Frobenius norm can be defined
as follows:
Ë†A2
ğ¹=trace
Ë†Ağ‘‡Ë†A
=ğ‘›âˆ‘ï¸
ğ‘˜=1ğœ†2
ğ‘˜(30)
According to Theorem 1,Ãğ‘›/2
ğ‘˜=1ğœ†2
ğ‘˜=Ãğ‘›
ğ‘˜=ğ‘›/2ğœ†2
ğ‘˜whereğœ†ğ‘›/2=0.
LetËœğœ†2
ğ‘˜
ğœ†2
ğ‘˜=ğ›¼ğ‘˜, thenğ›¼1â‰¥Â·Â·Â·â‰¥ğ›¼ğ¾>ğ›¼ğ¾+1â‰¥Â·Â·Â·â‰¥ğ›¼ğ‘›/2, and the
following relation holds:
2ËœAğ¾2
ğ¹ËœA2
ğ¹=Ãğ¾
ğ‘˜=1Ëœğœ†2
ğ‘˜Ãğ‘›/2
ğ‘˜=1Ëœğœ†2
ğ‘˜=Ãğ¾
ğ‘˜=1ğ›¼ğ‘˜ğœ†2
ğ‘˜Ãğ¾
ğ‘˜=1ğ›¼ğ‘˜ğœ†2
ğ‘˜+Ãğ‘›/2
ğ‘—=ğ¾+1ğ›¼ğ‘—ğœ†2
ğ‘—
â‰¥ğ›¼ğ¾Ãğ¾
ğ‘˜=1ğœ†2
ğ‘˜
ğ›¼ğ¾Ãğ¾
ğ‘˜=1ğœ†ğ‘˜+Ãğ¾
ğ‘˜=1ğ›¼ğ‘˜ğœ†2
ğ‘˜+Ãğ‘›/2
ğ‘—=ğ¾+1ğ›¼ğ‘—ğœ†2
ğ‘—
â‰¥ğ›¼ğ¾Ãğ¾
ğ‘˜=1ğœ†2
ğ‘˜
ğ›¼ğ¾Ãğ¾
ğ‘˜=1ğœ†ğ‘˜+ğ›¼ğ¾+1Ãğ‘›/2
ğ‘—=ğ¾+1ğœ†2
ğ‘—
>Ãğ¾
ğ‘˜=1ğœ†2
ğ‘˜Ãğ‘›/2
ğ‘˜=1ğœ†2
ğ‘˜=2Ë†Ağ¾2
ğ¹Ë†A2
ğ¹(31)The inequalities above are based on the observation thatğ‘¥
ğ‘¥+ğ‘¦=
1âˆ’ğ‘¦
ğ‘¥+ğ‘¦decreases as ğ‘¥decreases and ğ‘¦increases for ğ‘¥>0and
ğ‘¦>0. â–¡
B.4 Proof of Theorem 4
Proof. According to Courant-Fischer Theorem [38], we have:
ğœ†ğ‘˜=ğ‘šğ‘ğ‘¥
ğ‘‘ğ‘–ğ‘š(ğ‘†)=ğ‘˜ğ‘šğ‘–ğ‘›
ğ‘¥âˆˆğ‘†xğ‘‡Ë†Axğ‘ .ğ‘¡.|x|=1 (32)
where the eigenvalue is arranged in descending order. Then,
Ëœğœ†ğ‘˜=ğ‘šğ‘ğ‘¥
ğ‘‘ğ‘–ğ‘š(ğ‘†)=ğ‘˜ğ‘šğ‘–ğ‘›
ğ‘¥âˆˆğ‘†xğ‘‡ËœAx=ğ‘šğ‘ğ‘¥
ğ‘‘ğ‘–ğ‘š(ğ‘†)=ğ‘˜ğ‘šğ‘–ğ‘›
ğ‘¥âˆˆğ‘†âˆ‘ï¸
(ğ‘¢,ğ‘–)âˆˆE2xğ‘¢xğ‘–
(ğ‘‘ğ‘¢+ğ›¼)-ğœ–(ğ‘‘ğ‘–+ğ›¼)-ğœ–
=ğ‘šğ‘ğ‘¥ğ‘šğ‘–ğ‘›âˆ‘ï¸
(ğ‘¢,ğ‘–)âˆˆE2xğ‘¢xğ‘–
(ğ‘‘ğ‘¢)0.5(ğ‘‘ğ‘–)0.5âˆšï¸„
ğ‘‘ğ‘¢ğ‘‘ğ‘–
(ğ‘‘ğ‘¢+ğ›¼)(ğ‘‘ğ‘–+ğ›¼)((ğ‘‘ğ‘¢+ğ›¼)(ğ‘‘ğ‘–+ğ›¼))0.5+ğœ–
â‰¤ğ‘‘ğ‘šğ‘ğ‘¥(ğ‘‘ğ‘šğ‘ğ‘¥+ğ›¼)2ğœ–
ğ‘šğ‘ğ‘¥ğ‘šğ‘–ğ‘› xğ‘‡Ë†Ax
=ğ‘‘ğ‘šğ‘ğ‘¥(ğ‘‘ğ‘šğ‘ğ‘¥+ğ›¼)2ğœ–ğœ†ğ‘˜.
(33)
In the second last step,ğ‘‘ğ‘¢
ğ‘‘ğ‘¢+ğ›¼increases over ğ‘‘ğ‘¢, thusâˆšï¸ƒ
ğ‘‘ğ‘¢ğ‘‘ğ‘–
(ğ‘‘ğ‘¢+ğ›¼)(ğ‘‘ğ‘–+ğ›¼)â‰¤
ğ‘‘ğ‘šğ‘ğ‘¥
ğ‘‘ğ‘šğ‘ğ‘¥+ğ›¼. It is evident that((ğ‘‘ğ‘¢+ğ›¼)(ğ‘‘ğ‘–+ğ›¼))0.5+ğœ–â‰¤(ğ‘‘ğ‘šğ‘ğ‘¥+ğ›¼)1+2ğœ–.
Similarly, we can prove Ëœğœ†ğ‘˜â‰¥ğ‘‘ğ‘šğ‘–ğ‘›(ğ‘‘ğ‘šğ‘–ğ‘›+ğ›¼)2ğœ–ğœ†ğ‘˜.
â–¡
B.5 Proof of Theorem 5
Proof. The power of the adjacency matrix can be rewritten as
follows:
Ë†Ağ‘™=ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
Ë†RË†Rğ‘‡ğ‘™
20
0
Ë†Rğ‘‡Ë†Rğ‘™
2ï£¹ï£ºï£ºï£ºï£ºï£ºï£»ğ‘™={0,2,4,Â·Â·Â·}
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0 Ë†R
Ë†Rğ‘‡Ë†Rğ‘™-1
2
Ë†Rğ‘‡
Ë†RË†Rğ‘‡ğ‘™-1
20ï£¹ï£ºï£ºï£ºï£ºï£ºï£»ğ‘™={1,3,5,Â·Â·Â·}.(34)
According to singular value decomposition (SVD): Ë†R=Pğ‘‘ğ‘–ğ‘ğ‘”(ğœ†ğ‘˜)Qğ‘‡,
it is easy to show the following relations:

Ë†RË†Rğ‘‡ğ‘™
2=Pğ‘‘ğ‘–ğ‘ğ‘”
ğœğ‘™
ğ‘˜
Pğ‘‡,
Ë†Rğ‘‡Ë†Rğ‘™
2=Qğ‘‘ğ‘–ğ‘ğ‘”
ğœğ‘™
ğ‘˜
Qğ‘‡,
Ë†R
Ë†Rğ‘‡Ë†Rğ‘™-1
2=Pğ‘‘ğ‘–ğ‘ğ‘”
ğœğ‘™
ğ‘˜
Qğ‘‡,Ë†Rğ‘‡
Ë†RË†Rğ‘‡ğ‘™-1
2=Qğ‘‘ğ‘–ğ‘ğ‘”
ğœğ‘™
ğ‘˜
Pğ‘‡.
(35)
Then, the final embeddings of SGF can be formulated as follows:
O=ğ¿âˆ‘ï¸
ğ‘™=0Ë†Ağ‘™=Pğ‘‘ğ‘–ğ‘ğ‘”(ğœ“(ğœğ‘˜))Pğ‘‡Pğ‘‘ğ‘–ğ‘ğ‘”(ğœ”(ğœğ‘˜))Qğ‘‡
Qğ‘‘ğ‘–ğ‘ğ‘”(ğœ”(ğœğ‘˜))Pğ‘‡Qğ‘‘ğ‘–ğ‘ğ‘”(ğœ“(ğœğ‘˜))Qğ‘‡
, (36)
whereğœ“(ğœğ‘˜)=Ã
ğ‘™={0,2,Â·Â·Â·}ğœğ‘™
ğ‘˜,ğœ”(ğœğ‘˜)=Ã
ğ‘™={1,3,Â·Â·Â·}ğœğ‘™
ğ‘˜. Then, the
ratings are estimated as:
Oğ‘ˆOğ‘‡
ğ¼=Pğ‘‘ğ‘–ğ‘ğ‘”(ğœ™(ğœ†ğ‘˜)ğœ”(ğœ†ğ‘˜))Qğ‘‡(37)
â–¡
 
2399