Mutual Distillation Extracting Spatial-temporal Knowledge for
Lightweight Multi-channel Sleep Stage Classification
Ziyu Jia
Institute of Automation, Chinese Academy of Sciences
Beijing, China
jia.ziyu@outlook.comHaichao Wangâˆ—
Tsinghua-Berkeley Shenzhen Institute, Tsinghua
University
Shenzhen, China
hychaowang@outlook.com
Yucheng Liu
University of Southern California
Los Angeles, U.S.A
yuchengliu1214@outlook.comTianzi Jiang
Institute of Automation, Chinese Academy of Sciences
Beijing, China
tianzi.jiang.iacas@gmail.com
ABSTRACT
Sleep stage classification has important clinical significance for
the diagnosis of sleep-related diseases. To pursue more accurate
sleep stage classification, multi-channel sleep signals are widely
used due to the rich spatial-temporal information contained. How-
ever, it leads to a great increment in the size and computational
costs, which constrain the application of multi-channel sleep mod-
els on hardware devices. Knowledge distillation is an effective way
to compress models, yet existing knowledge distillation methods
cannot fully extract and transfer the spatial-temporal knowledge
in the multi-channel sleep signals. To solve the problem, we pro-
pose a general knowledge distillation framework for multi-channel
sleep stage classification called spatial-temporal mutual distillation.
Based on the spatial relationship of human body and the temporal
transition rules of sleep signals, the spatial and temporal mod-
ules are designed to extract the spatial-temporal knowledge, thus
help the lightweight student model learn the rich spatial-temporal
knowledge from large-scale teacher model. The mutual distilla-
tion framework transfers the spatial-temporal knowledge mutually.
Teacher model and student model can learn from each other, fur-
ther improving the student model. The results on the ISRUC-III and
MASS-SS3 datasets show that our proposed framework compresses
the sleep models effectively with minimal performance loss and
achieves the state-of-the-art performance compared to the baseline
methods.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning; Artificial
intelligence; â€¢Information systems â†’Data mining.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671981KEYWORDS
Physiological Signal Processing; Electroencephalogram; Sleep Stage
Classification; Knowledge Distillation;
ACM Reference Format:
Ziyu Jia, Haichao Wang, Yucheng Liu, and Tianzi Jiang. 2024. Mutual Dis-
tillation Extracting Spatial-temporal Knowledge for Lightweight Multi-
channel Sleep Stage Classification. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671981
1 INTRODUCTION
Sleep stage classification plays a crucial role in diagnosing sleep
disorders. To determine sleep stages, sleep experts analyze the
electrical activity recorded by sensors attached to various regions
of the human body. These electrical signals acquired are called
polysomnography (PSG), including electroencephalography (EEG),
electrooculography (EOG), electromyography (EMG). PSG signals
are sliced into 30-second segments and then assigned with a sleep
stage by human experts following American Academy of Sleep
Medicine (AASM) rules [ 2], which identifies sleep stages into Wake
(W), Rapid Eye Movements (REM), Non REM1 (N1), Non REM2
(N2), and Non REM3 (N3). Despite AASM rules offer significant
insights, the classification of sleep stages by human experts is still
a tedious and time-consuming task. Furthermore, the results are
influenced by the variability and subjectivity of sleep experts.
Manual sleep stage classification is time-consuming and requires
a large amount of labor. To address this problem, neural networks [ 9,
33,36] are introduced to achieve automatic sleep stage classification
by analyzing temporal knowledge. Temporal knowledge is a strong
reference in sleep stage classification [ 11]. During sleep, the human
brain undergoes a series of changes among different sleep stages. As
shown in Figure 1, temporal knowledge represents the transition
rules between sleep epochs within a sleep signal sequence. For
example, the N1 stage often serves as a transition stage between
the W stage and other stages. To capture temporal knowledge,
architectures like CNN-RNN [ 26,33] are employed for automatic
sleep stage classification.
These models above utilize single-channel sleep signal, which
has strong limits on accurate sleep stage classification. To address
 
1279
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziyu Jia, Haichao Wang, Yucheng Liu, & Tianzi Jiang
this limitation, multi-channel sleep signals are introduced into au-
tomatic sleep stage classification because they contain rich spatial
knowledge, which is a strong reference to sleep stage classification.
As shown in Figure 1, spatial knowledge refers to the relationship of
the human body. For instance, multi-channel EEG signals reflect the
structural and functional relationship within the human brain. The
spatial relationship between WAKE, REM, and N1 is strong, while
the spatial correlation between N2 and N3 is weak [ 17]. Chambon
et al. [3] utilize Convolutional Neural Networks (CNN) to apply
spatial filtering and capture spatial knowledge, while Jia et al . [10]
employ Graph Convolutional Networks (GCN) for this purpose.
However, in the pursuit of automatic sleep stage classification,
the size and computational complexity of deep neural networks
rapidly increase. This impedes their application in some resource-
constrained environments. For example, clinical care settings are
hard to meet the performance required for large-scale deep neural
networks. Hence, lightweight models are important for sleep stage
classification. Knowledge distillation is a useful and general frame-
work to compress neural networks by transferring the knowledge
from a complex model (teacher model) to a simpler model (student
model). However, current knowledge distillation approaches cannot
effectively be applied to sleep models because of two challenges.
On the one hand, existing knowledge distillation approaches
cannot fully extract the spatial-temporal knowledge within the
multi-channel sleep signals. For example, Liang et al . [15] propose a
multi-level knowledge distillation with a teacher assistant module.
However, it is designed specifically for single-channel EEG. Zhang
et al. [39] extract the epoch-wise and sequence-wise knowledge for
the distillation. However, its focus on the temporal knowledge ne-
glects the spatial knowledge. Moreover, common knowledge distil-
lation methods like Fitnets [ 28] and Hintonâ€™s knowledge distillation
[6] cannot closely bound up with the characteristic of sleep signals,
which hinders the extraction of sleep spatial-temporal knowledge.
TemporalKnowledgeSpatialKnowledgeÂ·Â·Â·
Figure 1: Visualization of spatial and temporal knowledge.
On the other hand, conventional knowledge distillation frame-
works cannot fully transfer the knowledge to the student model. As
shown in Figure 2, conventional knowledge transfer frameworks
in [6,25,28,41] train the teacher model in advance and then distill
knowledge to the student model by a fully-trained teacher model
which is static in the student training process. They overlook the
knowledge exchange between teacher and student during training.
These frameworks limit the transfer of spatial-temporal knowledge
and affects the performance of the student model adversely. Ad-
ditionally, conventional knowledge transfer frameworks adopt a
two-stage training process that first involves training the teacher
model followed by the student model, resulting in considerable
time and computational costs.
Teacher NetworkStudent	NetworkKnowledge
Teacher NetworkStudent	NetworkKnowledgeConventional DistillationMutual Distillation
Teacher NetworkFigure 2: In the conventional distillation, the teacher model
needs to be trained in advance and subsequently transfers
knowledge to the student network. If we distill knowledge
mutually, teacher and student model can learn from each
other during training, enhancing the distillation. Moreover,
it can be done in a one-stage training.
To solve the challenges above, we propose a general knowledge
distillation framework for multi-channel sleep stage classification
models to compress the sleep models effectively with minimal per-
formance loss. It is applicable to a wide range of multi-channel
sleep models. Our main contributions are as follows:
(1)To our best knowledge, it is the first attempt to introduce
spatial-temporal knowledge distillation into multi-channel
sleep stage classification.
(2)We propose spatial-temporal knowledge modules to fully ex-
tract spatial-temporal knowledge from multi-channel sleep
signals.
(3)We design a mutual distillation framework capable of en-
hancing the transfer of spatial-temporal knowledge while
also enabling single-stage training.
(4)Experiments indicate that our distillation effectively reduces
the scale of sleep models while preserving its classifica-
tion performance on both ISRUC-III and MASS-SS3 datasets.
Moreover, it achieves state-of-the-art performance compared
to other knowledge distillation frameworks.
2 RELATED WORKS
2.1 Spatial-temporal Knowledge in Sleep Stage
Classification
Sleep stage classification can help diagnose sleep disorders. In ear-
lier studies, researchers employ machine learning methods to clas-
sify sleep stages [ 1,32,37,43]. However, these methods require a
large amount of a priori knowledge, which means that a significant
manual cost is required to extract features [ 23]. Therefore, deep
learning methods [ 8,19,33,36] are employed in automatic sleep
stage classification, where spatial and temporal knowledge are key
features to extract.
Temporal knowledge. Researchers classify sleep stages by cap-
turing contextual dependencies between sleep stages. Based on this,
a series of sleep stage classification models [ 42] are designed to
extract temporal knowledge of sleep signals. For example, Supratak
et al. [33] use Bi-LSTM to extract sequential features of sleep signals.
A CNN-based model proposed by Sun et al . [31] devise a hierarchi-
cal neural network to learn temporal features for the sleep stage
classification. Mousavi et al . [22] employ a bidirectional recurrent
neural network to capture long-term and short-term contextual
 
1280Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
dependencies. Both MLP and LSTM are applied by Dong et al . [4]
for the extraction and mining of temporal features.
Spatial knowledge. Researchers classify sleep stages with multi-
channel sleep signals from sensors in different body parts [ 5,16].
For example, Chambon et al . [3] use convolutional layers across
channels to extract spatial knowledge. Shi et al . [29] use the joint
collaborative representation to fuse EEG representations and ex-
tract spatial knowledge. 2D CNN is applied by Sokolovsky et al .
[30] to capture the spatial knowledge of EEG and EOG. Jia et al .
[13] improve the classification performance of sleep stage classifi-
cation models by exploring the correlation of individual channels.
In addition, there are also methods that extract both temporal rela-
tionship and spatial knowledge [ 9]. For example, Jia et al . [10] use
deep graph neural networks to model spatial knowledge for more
accurate sleep stage classification of multi-channel sleep signals.
The extraction of spatial-temporal knowledge leads to high com-
putational and storage costs for the models in practical applications,
making it difficult to achieve deployment in hardware devices. How-
ever, they prove the effectiveness of spatial-temporal knowledge
in sleep stage classification models, which makes it meaningful to
extract spatial-temporal knowledge in knowledge distillation for
sleep stage classification, transferring the valuable spatial-temporal
knowledge from teacher model to student model. As far as we know,
we are the first attempt to introduce spatial-temporal knowledge
to knowledge distillation for sleep stage classification.
2.2 Knowledge Distillation
Knowledge distillation is an important approach in model compres-
sion. It is a general framework for a series of models. In knowledge
distillation, there are two main aspects: knowledge extraction and
knowledge transfer.
Knowledge extraction . Knowledge extraction is about what
knowledge should be taught to student model. Hinton et al . [6]
use the output of the teacher model as a kind of soft label to par-
ticipate in the training of the student model. Fitnets [ 28] uses the
intermediate layer features of teacher models as hints to guide stu-
dent models for training. Park et al . [25] focus on the multivariate
relationship between each sample and transfer the relationship
matrix as a kind of knowledge to the student model. Tian et al .
[35] encourage the positive samples to be closer and penalize the
negative samples to make them farther away by the relationship
between positive and negative samples. Minami et al . [20] construct
relationships as graphs for relationship-based graph knowledge
transfer. In conclusion, what knowledge to extract is an important
aspect in knowledge distillation.
Knowledge transfer. Knowledge transfer is about how to teach
the knowledge from teacher to student. To better transfer knowl-
edge, a series of distillation frameworks are proposed. For example,
Mirzadeh et al . [21] design a teaching assistant model to help reduce
the gap between teachers and students. Recently, a new type of
distillation utilizing mutual learning to help knowledge transfer. In
this circumstance, the knowledge is mutually transferred between
multiple models. For example, Zhang et al . [40] abandon the tradi-
tional teacher-student architecture and allowed each pair of models
in the model set to learn from each other. Ren et al . [27] propose
a Master model to update teacher and student models alternately.Liu et al . [18] propose an adaptive feedback mechanism that allows
the teacher model to dynamically adjust based on the performance
of the student model during the process of knowledge distillation.
In sleep stage classification task, it is vital to propose a knowl-
edge distillation approach tightly combined with the characteristics
of sleep signals. The knowledge distillation approach we proposed
extracts the spatial-temporal knowledge of sleep signals and trans-
fers sleep spatial-temporal knowledge in a single-stage mutual
distillation framework.
3 PRELIMINARY
Definition 1. In the task of sleep stage classification with multi-
channel sleep signals, we define the input signals ğ‘‹as follows:
ğ‘‹=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘¥11Â·Â·Â·ğ‘¥1ğ¿
.........
ğ‘¥ğ¶1Â·Â·Â·ğ‘¥ğ¶ğ¿ï£¹ï£ºï£ºï£ºï£ºï£ºï£», ğ‘¥ğ‘–ğ‘—âˆˆRğ‘›(1)
whereğ¿denotes the length of a sequence. ğ¶denotes the number of
channels.ğ‘›represents the length of a sleep epoch.
Definition 2. Consider a multi-channel sleep stage classification
modelğ‘“, which can be represented as the composition ğ‘“=ğ‘“1âŠ™ğ‘“2,
whereğ‘“1represents the encoder part of the model and ğ‘“2represents
the classifier.
Definition 3. The multi-channel features can be obtained by ap-
plyingğ‘‹toğ‘“1:
ğ»=ğ‘“1(ğ‘‹)=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°â„11Â·Â·Â·â„1ğ¿
.........
â„ğ¶1Â·Â·Â·â„ğ¶ğ¿ï£¹ï£ºï£ºï£ºï£ºï£ºï£», â„ğ‘–ğ‘—âˆˆRğ‘š(2)
whereğ‘šrepresents the feature length of an sleep epoch.
Definition 4. The classification results can be obtained by in-
putting the feature matrix ğ»into the classifier ğ‘“2:
Ë†ğ‘Œ=ğ‘“2(ğ»)={Ë†ğ‘¦1,Â·Â·Â·,Ë†ğ‘¦ğ¿} (3)
Ë†ğ‘¦ğ‘–={ğ‘1,Â·Â·Â·,ğ‘ğ‘˜},ğ‘ğ‘–âˆˆ(0,1) (4)
where Ë†ğ‘¦ğ‘–represents the probability distribution of each class with a
length ofğ‘˜andğ‘ğ‘–is the probability of the ğ‘–-th class.ğ‘˜corresponds
to the number of classes, which is 5 under the AASM rules.
4 MUTUAL DISTILLATION EXTRACTING
SPATIAL-TEMPORAL KNOWLEDGE
As shown in Figure 3, our framework consists of the spatial knowl-
edge module, the temporal knowledge module and a mutual distilla-
tion framework to compress multi-channel sleep stage classification
models. Specifically, encoders extracts multi-channel features from
raw sleep signals at the beginning. To model the spatial knowledge,
a sleep graph is constructed from the multi-channel features to
represent the spatial relationship of sleep signals. As for temporal
knowledge, it is modeled by measuring the temporal relationship
vector within the sleep signal sequence, thus guiding the student
model to learn the temporal knowledge contained in the teacher
model. In addition, the spatial-temporal knowledge is transferred
mutually under the mutual distillation framework.
 
1281KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziyu Jia, Haichao Wang, Yucheng Liu, & Tianzi Jiang
Encoderğ‘“!"
Encoderğ‘“!#
Teacher Features 	ğ»"
Student Features ğ»$SpatialModuleTemporalModuleSpatialModuleTemporalModuleSpatialKnowledgeTemporalKnowledgeğ¿#%&"'&(ğ¿")*%+,&(Multi-channel SleepSignals
ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ 
Decoder ğ‘“-"ğ¿."ğ¿.#Decoder ğ‘“-$
Figure 3: The overall process of our distillation. Initially, the multi-channel sleep signals are encoded by both the teacher and
student encoder, extracting corresponding multi-channel features. Subsequently, the temporal knowledge module and the
spatial knowledge module extract spatial-temporal knowledge and then mutually transferred under the mutual distillation
framework.
4.1 Spatial Knowledge Module
Spatial knowledge is a key reference for multi-channel sleep stage
classification. It remains a question that how to extract the rich
spatial knowledge from sleep models. For the extraction of spatial
knowledge, we design the spatial knowledge module. It starts with
sleep graph construction to represent spatial knowledge as a graph.
Then, we measure the difference between graphs from the teacher
and the student to convey the spatial knowledge.
It is a key question that how to represent the spatial knowledge
within the multi-channel sleep signals. Spatial knowledge repre-
sents the relationship of the human body. We can describe the
relationship of the human body by taking signals from the parts of
the human body as nodes. The relationship between these parts of
the human body as edges, thus create a sleep graph to represent
and extract spatial knowledge. Sleep graph is constructed from
multi-channel features encoded from the multi-channel sleep sig-
nals. Each channel can be denoted as a node ğ‘£ğ‘–, while the edge
betweenğ‘£ğ‘–andğ‘£ğ‘—are denoted as ğ‘’ğ‘–ğ‘—. The edge is measured by a
regularized form as follows:
ğ‘’ğ‘–ğ‘—=ğ‘…ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™(ğ‘£ğ‘–,ğ‘£ğ‘—)
Ãğ¶
ğ‘—=1ğ‘…ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™(ğ‘£ğ‘–,ğ‘£ğ‘—)(5)
ğ‘…ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™(ğ‘¥,ğ‘¦)=ğ‘’ğ¶ğ‘œğ‘ (ğ‘¥,ğ‘¦)(6)
whereğ‘…ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ is a similarity function which measures the relation-
ship of each pair of nodes. ğ¶ğ‘œğ‘ is the cosine similarity distance.
In the process of knowledge distillation, knowledge transfer is
conducted by utilizing the distance of the teacher and student model.For the sleep graph we propose, we measure the spatial distance of
the sleep graphs by bringing the KL Divergence to each node. As
for nodeğ‘–, the spatial distance ğ·ğ‘–is calculated as follows:
ğ·ğ‘–=ğ¾ğ¿ğ·(ğ‘’ğ‘†ğ‘¡ğ‘¢
ğ‘–âˆ¥ğ‘’ğ‘‡ğ‘’ğ‘
ğ‘–)=ğ¶âˆ‘ï¸
ğ‘—=0ğ‘’ğ‘†ğ‘¡ğ‘¢
ğ‘–ğ‘—logğ‘’ğ‘†ğ‘¡ğ‘¢
ğ‘–ğ‘—
ğ‘’ğ‘‡ğ‘’ğ‘
ğ‘–ğ‘—(7)
whereğ‘’ğ‘†ğ‘¡ğ‘¢
ğ‘–={ğ‘’ğ‘†ğ‘¡ğ‘¢
ğ‘–1,Â·Â·Â·,ğ‘’ğ‘†ğ‘¡ğ‘¢
ğ‘–ğ¶}is the spatial relationship vector
corresponding to node ğ‘£ğ‘†ğ‘¡ğ‘¢
ğ‘–in the studentâ€™s sleep graph. ğ‘’ğ‘‡ğ‘’ğ‘
ğ‘–=
{ğ‘’ğ‘‡ğ‘’ğ‘
ğ‘–1,...,ğ‘’ğ‘‡ğ‘’ğ‘
ğ‘–ğ¶}is the spatial relationship vector corresponding to
nodeğ‘£ğ‘‡ğ‘’ğ‘
ğ‘–of the teacherâ€™s sleep graph. In this calculation, if ğ·ğ‘–is
smaller, it means that the teacherâ€™s node ğ‘£ğ‘‡ğ‘’ğ‘
ğ‘–and the studentâ€™s
nodeğ‘£ğ‘†ğ‘¡ğ‘¢
ğ‘–have more similar spatial knowledge.
Therefore, the loss function for spatial knowledge can be derived
as follows:
ğ¿ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ =1
ğ¶ğ¶âˆ‘ï¸
ğ‘–=1ğ·ğ‘– (8)
whereğ¿ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ is the spatial loss to distill the spatial knowledge by
bringing all nodes into the calculation.
4.2 Temporal Knowledge Module
Sleep signal sequences naturally contain temporal knowledge. It
represents contextual dependencies between sleep epochs. The
classification of a certain sleep epoch can be inferred from the
relationship with the back-and-forth sleep epochs. In the existing
distillation for sleep models, they directly align the features of a
 
1282Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
sequence instead of modeling the relationship of sleep epochs. It is
not accurate and has problems like dimension alignment. To extract
the relationship of a sleep signal sequence, we design a temporal
knowledge module. We choose to model the relationship between
the sleep epochs as contextual constraints over a sequence, which
is more in line with the characteristics of sleep signals.
Specifically, the temporal knowledge module is computed as
follows: Given sleep signal sequence features ğ»with a length of ğ¿
sleep epochs. We take ğ‘–-th sleep epoch which contains ğ¶channels
asğ‘¢ğ‘–. To model the relationship between the features ğ‘¢ğ‘–andğ‘¢ğ‘—of
two sleep epochs, it can be expressed as follows:
ğ‘…ğ‘–ğ‘—=ğ‘…ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™(ğ‘¢ğ‘–,ğ‘¢ğ‘—) (9)
whereğ‘…ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ denotes a relationship function computed by Eu-
clidean distance.
By applying the relationship to all the sleep epochs in the se-
quence in pairs, we can get a temporal relationship vector ğ‘£ğ‘’ğ‘=
{ğ‘…ğ‘–ğ‘—|ğ‘–,ğ‘—âˆˆ[1,ğ¿]}. After computing the temporal relationship vec-
tor for both the teacher and the student model, we can get the
corresponding temporal relationship vector denoted as ğ‘£ğ‘’ğ‘ğ‘‡ğ‘’ğ‘and
ğ‘£ğ‘’ğ‘ğ‘†ğ‘¡ğ‘¢.
In the end, the temporal loss can be expressed as follows:
ğ¿temporal =ğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ¿ 1(ğ‘£ğ‘’ğ‘ğ‘‡ğ‘’ğ‘,ğ‘£ğ‘’ğ‘ğ‘†ğ‘¡ğ‘¢)
=(
0.5|ğ‘£ğ‘’ğ‘ğ‘‡ğ‘’ğ‘âˆ’ğ‘£ğ‘’ğ‘ğ‘†ğ‘¡ğ‘¢|2, if|ğ‘£ğ‘’ğ‘ğ‘‡ğ‘’ğ‘âˆ’ğ‘£ğ‘’ğ‘ğ‘†ğ‘¡ğ‘¢|<1
|ğ‘£ğ‘’ğ‘ğ‘‡ğ‘’ğ‘âˆ’ğ‘£ğ‘’ğ‘ğ‘†ğ‘¡ğ‘¢|âˆ’0.5,otherwise
(10)
whereğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ¿ 1loss function is used for calculating the difference
of temporal relationship vectors of the teacher and student. It com-
bines the advantages of ğ¿1andğ¿2loss functions, making it more
robust when dealing with outliers.
4.3 Mutual Distillation Framework
Traditional knowledge distillation employs a static teacher model in
the distillation. In the sleep stage classification task, it constrains the
knowledge transfer and limits the studentâ€™s performance. Moreover,
they utilize a two-stage training methodology, commencing with
the teacher model and followed by the student model, leading to
substantial time and computational costs. For better knowledge
transfer and single-stage training, we design a mutual distillation
framework to transfer spatial-temporal knowledge.
On the training epoch ğ‘–, the update of both teacher and student
model can be expressed as follows:
ğ¿ğ‘‡ğ‘’ğ‘
ğ‘ğ‘™ğ‘ =ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘“ğ‘‡ğ‘’ğ‘(ğ‘¥),ğ‘¦) (11)
ğ¿ğ‘†ğ‘¡ğ‘¢
ğ‘ğ‘™ğ‘ =ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘“ğ‘†ğ‘¡ğ‘¢(ğ‘¥),ğ‘¦) (12)
ğ¿ğ‘œğ‘ ğ‘ ğ‘‡ğ‘’ğ‘=ğ›¼ğ¿ğ‘‡ğ‘’ğ‘
ğ‘ğ‘™ğ‘ +ğ›½ğ¿ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™+ğ›¾ğ¿ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ (13)
ğ¿ğ‘œğ‘ ğ‘ ğ‘†ğ‘¡ğ‘¢=ğ›¼ğ¿ğ‘†ğ‘¡ğ‘¢
ğ‘ğ‘™ğ‘ +ğ›½ğ¿ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™+ğ›¾ğ¿ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ (14)
whereğ›¼,ğ›½, andğ›¾are three hyperparameters which stands for the
weights to balance the losses. ğ¿ğ‘‡ğ‘’ğ‘
ğ‘ğ‘™ğ‘ denotes the classification loss
of the teacher model while ğ¿ğ‘†ğ‘¡ğ‘¢
ğ‘ğ‘™ğ‘ denotes the classification loss of
the student model.5 EXPERIMENTS
5.1 Datasets
We conduct experiments on two publicly available sleep datasets.
These two datasets contain adequate multi-channel signals and are
scored by experts according to the AASM manual that can be used
for evaluating sleep model performance.
ISRUC-III is obtained from a sample of 8,549 PSG sleeps over
8 hours from 10 healthy adult subjects, including one male and
nine females. We use 8 subjects as the training set, 1 subject as the
validation set, and 1 subject as the test set.
MASS-SS3 contains 59,056 PSG sleep samples from the sleep
data of 62 healthy subjects, including 28 males and 34 females. We
also use 50 subjects as the training set, 6 subjects as the validation
set, and 6 subjects as the test set.
5.2 Baselines
To evaluate our method, we compare it with multiple baseline
knowledge distillation methods on both sleep datasets:
â€¢Hinton-KD [ 6]:Use the teacherâ€™s output probability distri-
bution to guide studentâ€™s training process.
â€¢Fitnets [ 28]:Extend the idea of the traditional knowledge
distillation by using both the output of the teacher model
and the intermediate representation as a hint to the student.
â€¢NST [ 7]:Match the distributions of the neuron selectiv-
ity patterns with maximum mean discrepancy between the
teacher and student networks.
â€¢DML [ 40]:The teacher and student collaboratively learn
and teach each other throughout the entire training process.
â€¢RKD [ 25]:Use distance and angle distillation loss to penalize
the difference in relation structure.
â€¢GCN-KD [ 38]:Transfer topological semantics of a pre-
trained GCN by a local structure preservation module.
â€¢DKD [ 41]:Re-express the loss as two parts, target class, and
non-target class, which focus on the classification correct-
ness and probability distribution separately.
5.3 Experiment Settings
To conduct a fair comparison, we bring the same data and model
settings to all knowledge distillation baselines and our framework.
The sampling rate is 100 Hz on both ISRUC-III and MASS-SS3. The
experiments are based on 6-channel EEG/EOG.
The spatial-temporal knowledge naturally exists in most of the
sleep models, whose most popular architectures are CNN-RNN
and CNN-GCN. Based on the inspiration of classical sleep models
such as CNN-RNN-based TinySleepNet[ 34] and CNN-GCN-based
GraphSleepNet[ 12], we design two pairs of multi-channel teacher-
student models for the comparison of knowledge distillation frame-
works. In the CNN-RNN architecture, we delete the units of the
dense layer and the LSTM as well as the number of filters in the
CNNs. In the CNN-GCN architecture, we delete units of the Graph
Convolution layer and the number of filters in the CNNs. The de-
tailed design of the teacher and student model is shown in Figure 4.
Details about the implementation of models are shown in Appen-
dix B.
 
1283KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziyu Jia, Haichao Wang, Yucheng Liu, & Tianzi Jiang
Conv 128
Conv 128
Dropout
Maxpool
Conv 128
Conv 128
(a)TeacherEncoder
Conv 32
Conv 32
Dropout
Maxpool
Conv 32
Conv 32
(b)StudentEncoder
Teacher Encoder
Teacher Encoder
Teacher Encoderâ€¦
Channel 1
Channel 2
Channel N
LSTM 128
Student Encoder
Student Encoder
Student Encoderâ€¦
Channel 1
Channel 2
Channel N
LSTM 32
CNN-RNN Teacher
CNN-RNN Student
Dense 128
Dense 1024
Teacher Encoder
Teacher Encoder
Teacher Encoderâ€¦
Channel 1
Channel 2
Channel N
GraphConv1024
Student Encoder
Student Encoder
Student Encoderâ€¦
Channel 1
Channel 2
Channel N
GraphConv32
CNN-GCN Teacher
CNN-GCN Student
Dense 128
Dense 1024
(c) CNN-RNN model
(d)CNN-GCN model
Figure 4: The key design and hyperparameters of the teacher
and student model. Subfigure (a) and (b) are the teacher and
student encoder. Subfigure (c) is the design of the teacher and
student model on CNN-RNN architectures. Subfigure (d) is
the design of the teacher and student model on CNN-GCN
architectures.
In the implementation of the models, we use one RTX 3090 GPU,
and TensorFlow 2.9.0 as the deep learning framework. In this paper,
we use Adam as the optimizer for each model with a learning rate of
0.0001 and a batch size of 8 during training. We use a weight setting
ofğ›¼:ğ›½:ğ›¾= 1:3:1 on ISRUC-III and ğ›¼:ğ›½:ğ›¾= 1:5:1 on MASS-SS3, and the
loss weights of other baseline methods are shown in Appendix C.
5.4 Overall Results
As the experiment results shown in Table 1 and 2, the student model
demonstrates a remarkable compression on the number of param-
eters, size, and FLOPS. It shows that the student model distilled
by our method reduces both the scale and computational costs.
However, the accuracy and F1-score still maintain a performance
near the teacher model.
Table 1: The performance, scale, and computational costs of
the teacher and student model on CNN-RNN models. #Param
denotes the number of parameters. Size denotes storage
the model occupied. Floating point Operations Per Second
(FLOPS) denotes the computational costs of the model.
Model Accuracy F1-score #Param Size FLOPS
Teacher 83.47% 80.50% 8.72M 34.9MB 11.34B
Student 82.42% 80.06% 0.29M 1.2MB 1.15BTable 2: The performance, scale and computational costs of
the teacher and student model on CNN-GCN models.
Model Accuracy F1-score #Param Size FLOPS
Teacher 85.93% 83.95% 5.49M 22MB 1.61B
Student 84.69% 81.96% 2.13M 8.6MB 0.034B
We compare our knowledge distillation framework with other
baseline knowledge distillation methods. Table 3 and Table 4 in-
dicate that our knowledge distillation framework achieves state-
of-the-art performance on almost every comparison, exceeding
baseline methods by more than 1%. The only exception is the F1-
score of the CNN-GCN model on the MASS-SS3 dataset, where it
remains a competitive result.
The reasons why our method is superior to the baseline methods
are due to the following: 1) The extraction of spatial-temporal
knowledge. Unlike Knowledge Distillation and Decoupled Knowl-
edge Distillation, which only utilize output knowledge, our method
effectively captures spatial-temporal knowledge. Baseline methods
like Fitnets, NST, and DML focus on intermediate features but ne-
glect the spatial relationships in multi-channel sleep signals. While
RKD considers the relationship of contextual epochs and GCN-KD
takes into account the spatial relationship, none of these methods
can extract both spatial and temporal knowledge comprehensively.
2) The mutual distillation framework. Our method employs a
mutual distillation framework, which surpasses traditional knowl-
edge transfer approaches used by most baseline methods. Even
methods like RKD and GCN-KD, which consider spatial or temporal
knowledge, are limited in performance compared to our approach
because of the insufficient knowledge transfer.
Table 3: Comparison with baseline methods on CNN-RNN
architecture.
MethodISRUC-III MASS-SS3
Accuracy F1-score Accuracy F1-score
Hinton-KD 77.47% 73.82% 81.27% 69.27%
DKD 79.26% 75.68% 82.51% 70.61%
Fitnets 78.21% 73.92% 81.09% 67.80%
NST 78.42% 74.18% 81.79% 70.04%
RKD 79.26% 76.75% 82.55% 71.30%
GCN-KD 77.16% 73.75% 82.95% 72.29%
DML 80.63% 77.31% 82.20% 70.59%
Ours 82.42% 80.06% 84.22% 73.94%
5.5 Visualization
To better show the extraction and transfer of spatial knowledge
under our proposed knowledge distillation framework, it is neces-
sary to compare the spatial knowledge contained in the teacher and
student model. As mentioned before, spatial knowledge denotes
the functional connectivity of the human body. Visualization of the
spatial connectivity of different channels can be a representation of
 
1284Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 4: Comparison with baseline methods on CNN-GCN
architecture.
MethodISRUC-III MASS-SS3
Accuracy F1-score Accuracy F1-score
Hinton-KD 75.07% 72.35% 84.75% 75.60%
DKD 82.44% 80.26% 84.79% 80.32%
Fitnets 81.88% 80.76% 84.96% 75.82%
NST 83.31% 80.94% 85.51% 76.81%
RKD 76.68% 73.19% 80.50% 64.19%
GCN-KD 82.65% 79.69% 83.67% 82.48%
DML 81.27% 77.84% 83.89% 72.64%
Ours 84.69% 81.96% 85.71% 77.98%
spatial knowledge. We visualize the spatial connectivity of different
channels under each sleep stage on ISRUC-III.
Additionally, we also visualize the relationship between sleep
epochs under each stage. As mentioned in experiment settings, the
sequence length of sleep signals are set as 5. Each sleep epoch is
denoted as{ğ‘¡âˆ’2,ğ‘¡âˆ’1,ğ‘¡,ğ‘¡+1,ğ‘¡+2}. We calculate the distance
between each pair of sleep epochs to show the temporal knowledge
of teacher and student model.
The visualization results are shown in Figure 5 and Figure 6.
It can be summarized that, the spatial connectivity and temporal
relationship of teacher and student model are similar under each
sleep stage, which means the effective extraction and transfer of
spatial-temporal knowledge.
5.6 Sensitivity of Loss Weights
The weights of each loss term play a crucial role in our knowledge
distillation framework. To evaluate the sensitivity of loss weights,
we conduct a series of experiments under static experiment settings
based on a CNN-GCN-based architecture on the ISRUC-III dataset.
The results, as presented in Table 5, suggest that the performance
of our framework exhibits minimal sensitivity to variations in the
weights assigned to each loss term.
Table 5: Results under different weights settings.
Weights(ğ›¼,ğ›½,ğ›¾ ) Accuracy F1-score
(1,1,1) 84.25% 81.93%
(1,3,1) 84.69% 81.96%
(1,5,1) 84.26% 81.22%
(1,7,1) 84.07% 81.60%
5.7 Different Distance Measurements
In the previous experiments, we employ KL Divergence to measure
the distance between two sleep graphs. We further test other dis-
tance measurement function on CNN-GCN-based model under the
same experiment settings. Table 6 shows that our proposed frame-
work is robust under all measurement functions, where Wasserstein
distance reaches the best performance.Table 6: The results of different measurement functions.
Measurements Accuracy F1-score
KL Divergence 84.69% 81.96%
MMD 84.50% 82.10%
Wasserstein distance 85.20% 82.82%
5.8 Ablation Study
Our method consists of three parts: temporal knowledge module,
spatial knowledge module, and mutual distillation framework. This
combination forms the optimal performance of spatial-temporal
mutual distillation. In order to further study the effectiveness of
the method, we conduct ablation experiments to evaluate each
specific module and prove the effectiveness of each component of
the method.
The experiment settings of the ablation study are as follows:
â€¢Variant I: Training without neither spatial-temporal knowl-
edge nor mutual distillation framework;
â€¢Variant II: Training with only temporal knowledge module,
without mutual distillation framework and spatial knowl-
edge module;
â€¢Variant III: Training with temporal knowledge module and
spatial knowledge module, without mutual distillation frame-
work;
â€¢Variant IV: Training with mutual spatial-temporal knowl-
edge distillation.
Through the results shown in Table 7, it can be observed that the
temporal knowledge module has a positive impact on the knowl-
edge distillation performance because of transferring the temporal
knowledge. Then, the spatial knowledge module also contributes
to the performance by extracting and conveying spatial knowledge
of multi-channel sleep signals. In addition, the gain of the mutual
distillation framework indicates mutual knowledge transfer helps
further improve the distillation.
Table 7: The results of each variant on CNN-RNN model.
MethodISRUC-III MASS-SS3
Accuracy F1-score Accuracy F1-score
Variant I 77.47% 73.82% 81.27% 69.27%
Variant II 78.10% 75.07% 83.03% 72.29%
Variant III 80.52% 77.40% 83.58% 73.29%
Variant IV(Ours) 82.42% 80.06% 84.22% 73.94%
6 CONCLUSION
We propose a novel knowledge distillation approach for the sleep
stage classification task with multi-channel sleep signals. It con-
sists of three parts: spatial knowledge module, temporal knowledge
module, and mutual distillation framework. The spatial knowledge
module constructs the sleep graph and conveys the spatial knowl-
edge extracted from multi-channel sleep signals. Meanwhile, the
temporal knowledge module transfers the relationship between
 
1285KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziyu Jia, Haichao Wang, Yucheng Liu, & Tianzi Jiang
Figure 5: Visualization analysis of spatial knowledge transfer. The five figures on the top are the spatial relationship of
teacher model, while the five figures on the bottom are the spatial relationship of student model. It can be concluded that the
relationship is similar under each sleep stage, proving the effectiveness of spatial knowledge transfer.
Figure 6: Visualization analysis of temporal knowledge transfer. The five figures on the top are the temporal relationship of
teacher model, while the five figures on the bottom are the temporal relationship of student model. It can be concluded that the
relationship is similar under each sleep stage, proving the effectiveness of temporal knowledge transfer.
sleep epochs inside a sequence. To further improve the distilla-
tion, the mutual distillation framework is designed to mutually
transfer the spatial-temporal knowledge between the teacher and
student. Our experiments indicate that our method significantly
compresses the model while maintaining its performance. It attains
state-of-the-art performance on two public sleep datasets, ISRUC-
III and MASS-SS3. Furthermore, each component of our method
is confirmed effective through the ablation study. Our framework
is a general distillation framework for multi-channel time series
classification. In the future, this method can be extended to other
large-scale multivariate time series models.
7 ACKNOWLEDGEMENTS
This work was partially supported by the National Natural Science
Foundation of China (Grant No. 62306317) and Sichuan Science and
Technology Plan Project (Grant No. 2022YFSY0014) and STI2030-
Major Projects (Grant No. 2021ZD0200200) and Postdoctoral Fel-
lowship Program of CPSF (Grant No. GZC20232992) and China
Postdoctoral Science Foundation (Grant No. 2023M733738).REFERENCES
[1]A Jameer Basha, B Saravana Balaji, S Poornima, M Prathilothamai, and K Venkat-
achalam. 2021. Support vector machine and simple recurrent network based
automatic sleep stage classification of fuzzy kernel. Journal of ambient intelligence
and humanized computing 12 (2021), 6189â€“6197.
[2]Richard B Berry, Rita Brooks, Charlene E Gamaldo, Susan M Harding, C Marcus,
Bradley V Vaughn, et al .2012. The AASM manual for the scoring of sleep and
associated events. Rules, Terminology and Technical Specifications, Darien, Illinois,
American Academy of Sleep Medicine 176 (2012), 2012.
[3]Stanislas Chambon, Mathieu N Galtier, Pierrick J Arnal, Gilles Wainrib, and
Alexandre Gramfort. 2018. A deep learning architecture for temporal sleep stage
classification using multivariate and multimodal time series. IEEE Transactions
on Neural Systems and Rehabilitation Engineering 26, 4 (2018), 758â€“769.
[4]Hao Dong, Akara Supratak, Wei Pan, Chao Wu, Paul M Matthews, and Yike Guo.
2017. Mixed neural network approach for temporal sleep stage classification.
IEEE Transactions on Neural Systems and Rehabilitation Engineering 26, 2 (2017),
324â€“333.
[5]Hongyang Gao and Shuiwang Ji. 2019. Graph u-nets. In international conference
on machine learning. PMLR, 2083â€“2092.
[6]Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
 
1286Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[7]Zehao Huang and Naiyan Wang. 2017. Like what you like: Knowledge distill via
neuron selectivity transfer. arXiv preprint arXiv:1707.01219 (2017).
[8]Ziyu Jia, Xiyang Cai, and Zehui Jiao. 2022. Multi-modal physiological signals
based squeeze-and-excitation network with domain adversarial learning for sleep
staging. IEEE Sensors Journal 22, 4 (2022), 3464â€“3471.
[9]Ziyu Jia, Junyu Ji, Xinliang Zhou, and Yuhan Zhou. 2022. Hybrid spiking neural
network for sleep electroencephalogram signals. Science China Information
Sciences 65, 4 (2022), 140403.
[10] Ziyu Jia, Youfang Lin, Jing Wang, Xiaojun Ning, Yuanlai He, Ronghao Zhou,
Yuhan Zhou, and H Lehman Li-wei. 2021. Multi-view spatial-temporal graph
convolutional networks with domain generalization for sleep stage classification.
IEEE Transactions on Neural Systems and Rehabilitation Engineering 29 (2021),
1977â€“1986.
[11] Ziyu Jia, Youfang Lin, Jing Wang, Xuehui Wang, Peiyi Xie, and Yingbin Zhang.
2021. SalientSleepNet: Multimodal salient wave detection network for sleep
staging. arXiv preprint arXiv:2105.13864 (2021).
[12] Ziyu Jia, Youfang Lin, Jing Wang, Ronghao Zhou, Xiaojun Ning, Yuanlai He,
and Yaoshuai Zhao. 2020. GraphSleepNet: Adaptive spatial-temporal graph
convolutional networks for sleep stage classification.. In IJCAI, Vol. 2021. 1324â€“
1330.
[13] Ziyu Jia, Youfang Lin, Yuhan Zhou, Xiyang Cai, Peng Zheng, Qiang Li, and
Jing Wang. 2023. Exploiting Interactivity and Heterogeneity for Sleep Stage
Classification Via Heterogeneous Graph Neural Network. In ICASSP 2023-2023
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .
IEEE, 1â€“5.
[14] Sirvan Khalighi, Teresa Sousa, JosÃ© Moutinho Santos, and Urbano Nunes. 2016.
ISRUC-Sleep: A comprehensive public dataset for sleep researchers. Computer
methods and programs in biomedicine 124 (2016), 180â€“192.
[15] Heng Liang, Yucheng Liu, Haichao Wang, and Ziyu Jia. 2023. Teacher Assistant-
Based Knowledge Distillation Extracting Multi-level Features on Single Channel
Sleep EEG. In Proceedings of the Thirty-Second International Joint Conference
on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China.
ijcai.org, 3948â€“3956. https://doi.org/10.24963/ijcai.2023/439
[16] Chenyu Liu, Xinliang Zhou, Zhengri Zhu, Liming Zhai, Ziyu Jia, and Yang Liu.
[n. d.]. VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks
for Cross-subject Emotion Recognition. In The Twelfth International Conference
on Learning Representations.
[17] Yuchen Liu and Ziyu Jia. 2022. Bstt: A bayesian spatial-temporal transformer for
sleep staging. In The Eleventh International Conference on Learning Representa-
tions.
[18] Yucheng Liu, Ziyu Jia, and Haichao Wang. 2023. EmotionKD: A Cross-Modal
Knowledge Distillation Framework for Emotion Recognition Based on Physiologi-
cal Signals. In Proceedings of the 31st ACM International Conference on Multimedia,
MM 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023, Abdulmo-
taleb El-Saddik, Tao Mei, Rita Cucchiara, Marco Bertini, Diana Patricia Tobon
Vallejo, Pradeep K. Atrey, and M. Shamim Hossain (Eds.). ACM, 6122â€“6131.
https://doi.org/10.1145/3581783.3612277
[19] Shuo Ma, Yingwei Zhang, Yiqiang Chen, Tao Xie, Shuchao Song, and Ziyu Jia.
2024. Exploring Structure Incentive Domain Adversarial Learning for General-
izable Sleep Stage Classification. ACM Transactions on Intelligent Systems and
Technology 15, 1 (2024), 1â€“30.
[20] Soma Minami, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi.
2020. Knowledge transfer graph for deep collaborative learning. In Proceedings
of the Asian Conference on Computer Vision.
[21] Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Mat-
sukawa, and Hassan Ghasemzadeh. 2020. Improved Knowledge Distillation via
Teacher Assistant. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in
Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI
Press, 5191â€“5198. https://doi.org/10.1609/aaai.v34i04.5963
[22] Sajad Mousavi, Fatemeh Afghah, and U Rajendra Acharya. 2019. SleepEEGNet:
Automated sleep stage scoring with sequence to sequence deep learning approach.
PloS one 14, 5 (2019), e0216456.
[23] Xiaojun Ning, Jing Wang, Youfang Lin, Xiyang Cai, Haobin Chen, Haijun Gou,
Xiaoli Li, and Ziyu Jia. 2023. MetaEmotionNet: Spatial-Spectral-Temporal based
Attention 3D Dense Network with Meta-learning for EEG Emotion Recognition.
IEEE Transactions on Instrumentation and Measurement (2023).[24] Christian Oâ€™reilly, Nadia Gosselin, Julie Carrier, and Tore Nielsen. 2014. Montreal
Archive of Sleep Studies: an open-access resource for instrument benchmarking
and exploratory research. Journal of sleep research 23, 6 (2014), 628â€“635.
[25] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. 2019. Relational knowledge
distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 3967â€“3976.
[26] Huy Phan, Fernando Andreotti, Navin Cooray, Oliver Y ChÃ©n, and Maarten
De Vos. 2019. SeqSleepNet: end-to-end hierarchical recurrent neural network
for sequence-to-sequence automatic sleep staging. IEEE Transactions on Neural
Systems and Rehabilitation Engineering 27, 3 (2019), 400â€“410.
[27] Sucheng Ren, Yong Du, Jianming Lv, Guoqiang Han, and Shengfeng He. 2021.
Learning from the master: Distilling cross-modal advanced knowledge for lip
reading. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 13325â€“13333.
[28] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang,
Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin deep nets. arXiv
preprint arXiv:1412.6550 (2014).
[29] Jun Shi, Xiao Liu, Yan Li, Qi Zhang, Yingjie Li, and Shihui Ying. 2015. Multi-
channel EEG-based sleep stage classification with joint collaborative represen-
tation and multiple kernel learning. Journal of neuroscience methods 254 (2015),
94â€“101.
[30] Michael Sokolovsky, Francisco Guerrero, Sarun Paisarnsrisomsuk, Carolina Ruiz,
and Sergio A Alvarez. 2019. Deep learning for automated feature discovery and
classification of sleep stages. IEEE/ACM transactions on computational biology
and bioinformatics 17, 6 (2019), 1835â€“1845.
[31] Chenglu Sun, Chen Chen, Wei Li, Jiahao Fan, and Wei Chen. 2019. A hierarchical
neural network for sleep stage classification based on comprehensive feature
learning and multi-flow sequence learning. IEEE journal of biomedical and health
informatics 24, 5 (2019), 1351â€“1366.
[32] Kalaivani Sundararajan, Sonja Georgievska, Bart HW Te Lindert, Philip R
Gehrman, Jennifer Ramautar, Diego R Mazzotti, SÃ©verine Sabia, Michael N Wee-
don, Eus JW van Someren, Lars Ridder, et al .2021. Sleep classification from
wrist-worn accelerometer data using random forests. Scientific reports 11, 1
(2021), 24.
[33] Akara Supratak, Hao Dong, Chao Wu, and Yike Guo. 2017. DeepSleepNet: A
model for automatic sleep stage scoring based on raw single-channel EEG. IEEE
Transactions on Neural Systems and Rehabilitation Engineering 25, 11 (2017), 1998â€“
2008.
[34] Akara Supratak and Yike Guo. 2020. TinySleepNet: An efficient deep learning
model for sleep stage scoring based on raw single-channel EEG. In 2020 42nd
Annual International Conference of the IEEE Engineering in Medicine & Biology
Society (EMBC). IEEE, 641â€“644.
[35] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2019. Contrastive representation
distillation. arXiv preprint arXiv:1910.10699 (2019).
[36] Orestis Tsinalis, Paul M Matthews, Yike Guo, and Stefanos Zafeiriou. 2016. Auto-
matic sleep stage scoring with single-channel EEG using convolutional neural
networks. arXiv preprint arXiv:1610.01683 (2016).
[37] Katerina D Tzimourta, Athanasios Tsilimbaris, Katerina Tzioukalia, Alexandros T
Tzallas, Markos G Tsipouras, Loukas G Astrakas, and Nikolaos Giannakeas. 2018.
EEG-based automatic sleep stage classification. Biomed J 1, 6 (2018).
[38] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. 2020.
Distilling knowledge from graph convolutional networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7074â€“7083.
[39] Chao Zhang, Yiqiao Liao, Siqi Han, Milin Zhang, Zhihua Wang, and Xiang Xie.
2022. Multichannel Multidomain-Based Knowledge Distillation Algorithm for
Sleep Staging With Single-Channel EEG. IEEE Transactions on Circuits and
Systems II: Express Briefs 69, 11 (2022), 4608â€“4612.
[40] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. 2018. Deep
mutual learning. In Proceedings of the IEEE conference on computer vision and
pattern recognition. 4320â€“4328.
[41] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. 2022. Decoupled
knowledge distillation. In Proceedings of the IEEE/CVF Conference on computer
vision and pattern recognition. 11953â€“11962.
[42] Xinliang Zhou, Chenyu Liu, Jiaping Xiao, and Yang Liu. 2023. EEG-based Sleep
Staging with Hybrid Attention. In 2023 IEEE Conference on Artificial Intelligence
(CAI). IEEE, 112â€“115.
[43] Xinliang Zhou, Chenyu Liu, Liming Zhai, Ziyu Jia, Cuntai Guan, and Yang Liu.
2023. Interpretable and robust ai in eeg systems: A survey. arXiv preprint
arXiv:2304.10755 (2023).
 
1287KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziyu Jia, Haichao Wang, Yucheng Liu, & Tianzi Jiang
A DESCRIPTION OF DATASETS
We evaluate our paper on MASS-SS3 [ 24] and ISRUC-III [ 14] datasets.
The details of these datasets can be seen in Table 8.
Table 8: The description of datasets on MASS and ISRUC-III.
Dataset Signal Type Label Frequency Rate
MASSEEGC3 256Hz
C4 256Hz
Cz 256Hz
F3 256Hz
F4 256Hz
F7 256Hz
F8 256Hz
O1 256Hz
O2 256Hz
P3 256Hz
P4 256Hz
Pz 256Hz
T3 256Hz
T4 256Hz
T5 256Hz
T6 256Hz
Fp1 256Hz
Fp2 256Hz
Fpz 256Hz
EOG / 256Hz
EMG / 256Hz
ECG / 512Hz
ISRUC-IIIEOGLOC-A2 100Hz
ROC-A1 100Hz
F3-A2 200Hz
C3-A2 200Hz
EEGO1-A2 200Hz
F4-A1 200Hz
C4-A1 200Hz
O2-A1 200Hz
Chin-EMG X1 200Hz
ECG X2 200Hz
Leg1-EMG X3 200Hz
Leg2-EMG X4 200Hz
B DETAILS OF THE TEACHER-STUDENT
NETWORK
CNN-RNN-based network. We design our network with the hy-
brid architecture of CNN and RNN. This kind of model is usually
made up of two parts. One of them is the feature encoder. This
part of the network extracts the features from the sleep epochs
of each channel by individual encoders. After the encoding, the
multi-channel features are concatenated as the input of the rest
of the network. The rest of the network consists of a BiLSTM and
a dense layer. BiLSTM is employed to capture the contextual fea-
tures of several continuous sleep epochs during the transition to
improve classification accuracy. We use a dense layer as a classifier
to generate the output.As for this model, we use the strategy that quarters the number of
kernels in convolution layers in each CNN stream and the number
of units in the BiLSTM layer simultaneously. The details of the
implementation of the teacher and student are shown in Table 9,
10, 11, and 12.
Table 9: Details of the teacher encoder.
Layer Type #Filters Size Stride Activation Mode
Input / / / / /
Convolution 1D 128 fs/2 fs/4 relu same
Dropout / 0.5 / / /
Maxpooling 1D / 8 8 / /
Convolution 1D 128 8 1 relu same
Convolution 1D 128 8 1 relu same
Convolution 1D 128 8 1 relu same
Maxpooling 1D / 4 4 / /
Dropout / 0.5 / / /
Table 10: Details of the rest of the teacher network.
Layer Type Size Activation
Encoder / /
Concatenate / /
BiLSTM 128 /
Dropout 0.5 /
Dense 5 softmax
Table 11: Details of the student encoder.
Layer Type #Filters Size Stride Activation Mode
Input / / / / /
Convolution 1D 32 fs/2 fs/4 relu same
Dropout / 0.5 / / /
Maxpooling 1D / 8 8 / /
Convolution 1D 32 8 1 relu same
Convolution 1D 32 8 1 relu same
Convolution 1D 32 8 1 relu same
Maxpooling 1D / 4 4 / /
Dropout / 0.5 / /
Table 12: Details of the rest of the student network.
Layer Type Size Activation
Encoder / /
Concatenate / /
BiLSTM 32 /
Dropout 0.5 /
Dense 5 softmax
 
1288Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
CNN-GCN-based Network. We design a teacher-student net-
work based on CNN-GCN architecture. It is made up of two parts:
feature extractor and GCN. The feature extractor extracts the fea-
tures from the sleep epochs of each channel by individual encoders
as the input of GCN. GCN learns the spatial knowledge from the
multi-channel sleep signals.
We use the strategy of deleting the number of CNN filters and
Graph Convolution units. The feature extractor structure is the
same as the encoder of the CNN-RNN-based network. The details
of GCN are shown in Table 13 and Table 14.
Table 13: Details of the rest of the teacher network.
Layer Type Size Activation
Feature Extractor / /
Concatenate / /
Graph Convolution 1024 /
Dropout 0.5 /
Dense 5 softmaxTable 14: Details of the rest of the student network.
Layer Type Size Activation
Feature Extractor / /
Concatenate / /
Graph Convolution 32 /
Dropout 0.5 /
Dense 5 softmax
C THE HYPERPARAMETERS FOR TRAINING
Here are the hyperparameters we choose for the experiments:
Table 15: Training parameters for baseline methods.
MethodEpoch loss weights
ISRUC-III MASS-SS3 ISRUC-III MASS-SS3
Hinton-KD 30 0.1:0.9
Fitnets 25 0.5:0.5
NST 25 0.5:0.5
RKD 25 0.5:0.5
GCN-KD 25 1:5
DKD 25 1:1
DML 25 0.1:0.9
 
1289