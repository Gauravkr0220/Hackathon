Influence Maximization via Graph Neural Bandits
Yuting Feng
Department of Electrical and Computer
Engineering (ECE)
National University of Singapore
Singapore
yt.f@nus.edu.sgVincent Y. F. Tan
Department of Mathematics,
Department of ECE,
National University of Singapore
Singapore
vtan@nus.edu.sgBogdan Cautis
University of Paris-Saclay,
CNRS LISN
Gif-sur-Yvette, France
bogdan.cautis@universite-paris-saclay.fr
ABSTRACT
We consider a ubiquitous scenario in the study of Influence Max-
imization (IM), in which there is limited knowledge about the
topology of the diffusion network. We set the IM problem in a
multi-round diffusion campaign, aiming to maximize the number of
distinct users that are influenced. Leveraging the capability of ban-
dit algorithms to effectively balance the objectives of exploration
and exploitation, as well as the expressivity of neural networks,
our study explores the application of neural bandit algorithms to
the IM problem. We propose the framework IM-GNB (Influence
Maximization with Graph Neural Bandits), where we provide an es-
timate of the usersâ€™ probabilities of being influenced by influencers
(also known as diffusion seeds). This initial estimate forms the basis
for constructing both an exploitation graph and an exploration
one. Subsequently, IM-GNB handles the exploration-exploitation
tradeoff, by selecting seed nodes in real-time using Graph Con-
volutional Networks (GCN), in which the pre-estimated graphs
are employed to refine the influencersâ€™ estimated rewards in each
contextual setting. Through extensive experiments on two large
real-world datasets, we demonstrate the effectiveness of IM-GNB
compared with other baseline methods, significantly improving the
spread outcome of such diffusion campaigns, when the underlying
network is unknown.
CCS CONCEPTS
â€¢Information systems â†’Social recommendation; Social ad-
vertising; â€¢Human-centered computing â†’Social media; So-
cial recommendation ;â€¢Networksâ†’Social media networks .
KEYWORDS
Information diffusion, influence, influencer marketing, contextual
bandits, graph neural networks.
ACM Reference Format:
Yuting Feng, Vincent Y. F. Tan, and Bogdan Cautis. 2024. Influence Maxi-
mization via Graph Neural Bandits. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671983
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36719831 INTRODUCTION
Motivated by the rise of â€œinfluencer marketingâ€ in social media
advertising, a class of algorithmic problems termed Influence Max-
imization (IM) has emerged, starting with the pioneering work
of [10,18]. These algorithms aim to identify the most influential
nodes within a diffusion network for initiating the spread of spe-
cific information, thereby maximizing its reach. In many ways, this
research directly mirrors the increasingly prevalent and successful
marketing strategy of targeting key individuals (influencers).
The objective of IM is typically formulated by maximizing the
expected spread under a stochastic diffusion model, which charac-
terizes the information dissemination process. The work of [ 18] laid
the foundations for the IM literature, by introducing two prominent
models: Linear Threshold (LT) and Independent Cascade (IC). These
models, widely adopted in subsequent research, represent diffusion
networks as probabilistic graphs, where the edges are weighted by
probabilities of information transmission.
Selecting the seed nodes maximizing the expected spread is NP-
hard under common diffusion models [ 18]. Despite the development
of approximate algorithms, exploiting the monotonicity and sub-
modularity of the spread, scaling IM algorithms to large networks
remains challenging. Acquiring meaningful influence probabilities
is equally challenging, as learning them from past information cas-
cades (e.g., as in [ 11,13]) can be data-intensive and thus impractical.
Moreover, the applicability of such models is limited in scenarios
where historical cascades are not available.
In the face of these challenges, since even the most efficient IM
algorithms such as [ 16,30] rely on assumptions and parameters
that often fail to capture the complex reality of how information
spreads online, a change in research direction has been followed
recently. It consists of approaches that neither rely on pre-defined
diffusion models nor require upfront knowledge of the diffusion
network. Instead, these online methods, such as [ 17,20,33],learn to
spread on the fly. More precisely, they involve a sequential learning
agent that actively gathers information through a multi-round influ-
ence campaign. In each round, the agent selects so-called seed nodes,
observes the resulting information spread, and uses this feedback
to make better choices in subsequent rounds, with the campaignâ€™s
total reward being the objective that is to be optimized. Such a
learning framework leads naturally to a policy that balances explor-
ingunknown aspects (i.e., the diffusion dynamics) with exploiting
known and successful choices (i.e., the high-performing spread seed
nodes), using multi-armed bandits [21].
We consider in this paper such an online IM scenario with limited
network information. Specifically, the diffusion graph is largely
unknown, except for a set of predefined influencers, representing
the potential seeds for information dissemination at each round
 
771
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
of a multi-round diffusion campaign. Additionally, we incorporate
contextual features of both influencers and the information being
diffused. Regarding the latter, the rationale is that within a campaign
aiming to maximize the reach of a specific message, its framing
and presentation can significantly impact its spread. For instance,
a political campaign may use various formats like news articles,
opinion pieces, data visualizations, or multimedia content, each
leading to distinct diffusion patterns.
We leverage such contextual information through the formal
framework of Contextual Multi-Armed Bandits (CMABs) [ 21]. Fur-
thermore, recognizing that significant correlations between the fea-
tures of the basic (to-be-influenced) users may exist, albeit unknown
to the agent, and that by implication their activation probabilities
may be correlated, we enhance the learning framework with mech-
anisms by which each activation can provide useful information
about neighbouring users in the network as well, allowing to refine
the agentâ€™s predictions. We achieve this by adapting to our IM prob-
lem setting the Graph Neural Bandits (GNB) framework of [ 26] (a
bandit algorithm for recommender systems). Correlation graphs
are constructed based on the similarity of users to be influenced
by the same influencer, and GNBs are then employed to handle the
challenges associated with graph-based bandit algorithm. In doing
so, our work is the first to leverage the implicit relationships that
may exist between basic users in the unknown diffusion medium.
In essence, we dynamically model these relationships based on
the observed campaign feedback, and we use them as input for a
graph neural network (GNN)-based learning algorithm guiding the
seed selection process at each round, optimizing choices under the
exploit-explore paradigm.
Overview of our IM scenario. As usual in IM scenarios, we run
campaigns under budget constraints (limited seedings and rounds),
with the goal to maximize the number of distinct users activated,
starting from known influencers. The learning agent chooses seeds
sequentially, i.e., at each round, with potential re-seeding, and feed-
back consists solely of the activated nodes after each round, without
additional details on the triggering causes. The feedback is used
to refine estimates of influencer potential, guiding future seeding
choices. Aligning with the overall objective, each roundâ€™s reward
is the number of newly activated users, and the campaign aims to
maximize the cumulative reward across rounds. In this scenario,
we mimic real-world influencer marketing, where access is limited
to a few influencers, feedback is restricted to user actions (like pur-
chases or subscriptions), and the goal is to reach as many unique
users as possible.
Our contributions. We detail our contributions in the following:
â€¢By introducing the IM-GNB framework, we connect GNBs and
the IM problem. This integration is non-trivial due to the inherent
challenges of learning from graph-structured data and making
sequential decisions under uncertain environments in the context
of diffusion campaigns.
â€¢We tackle the challenge of balancing exploration and exploitation
in dynamic influence propagation by incorporating contextual
bandits into the IM-GNB framework. This enables us to effec-
tively explore the potential rewards while exploiting available
information, resulting in enhanced influence spread.
â€¢We construct user-user correlation graphs for exploitation and
exploration purposes, capturing intricate interactions amongusers and influencers in each round of the diffusion campaign.
This graph-based approach is scalable to various network set-
tings, even without prior knowledge of the networkâ€™s topology
structure.
â€¢We develop a novel algorithm that optimally selects seed nodes
in real-time, with contextual bandits integrated with GNNs to
refine the reward estimates in each contextual setting. Through
extensive experiments, we show that our algorithm outperforms
baseline methods, highlighting the utility of GNBs as a prin-
cipled approach to optimize influence campaigns in uncertain
environments.
2 RELATED WORK
Influence Maximization (IM) addresses the challenge of identify-
ing a set of seeds (influencers) within a social network to maximize
information spread. Researchers first explored this problem in [ 10].
Later, [ 18] provided a clear formulation of the problem, including
how influence spreads through stochastic models like Independent
Cascade (IC) and Linear Threshold (LT). They also described the
important properties of the spread objective, its approximation guar-
antees and hardness results. Since then, such stochastic models have
become widely adopted in the literature, and most works focused
on finding approximate solutions that can be computed efficiently.
A key breakthrough was the concept of reverse influence sampling,
introduced in [ 6] and made practical in [ 23,30,31]. Diffusion model-
based IM approaches rely on diffusion graphs where the edges are
labeled by weights (spread probabilities). In empirical evaluations,
these weights may be data-based [ 14,15] (computed from diffusion
cascades), degree based, or simply assumed random. Some recent
studies [ 12,24] employ representation learning to infer influence
probabilities from ground-truth diffusion cascades, a resource that
may not be readily available in many application scenarios. (See
the recent survey [22] for a review of the IM literature.)
Bandits for Influence Maximization By virtue of their versatil-
ity and sequential nature, bandit algorithms are apt to be used in
IM problems, especially in uncertain diffusion environments with
which a learning agent may interact repeatedly [ 17,29,34,37]. A
multi-round, sequential setting allows to spread information and
gather feedback, striking a balance between influencing / activating
nodes in each round and learning influence parameters for uncer-
tain or unexplored network facets. This strategy closely mirrors
real-world influencer marketing scenarios, in which campaigns
often unfold over time. [ 34] is one of the earliest works that map
an IM problem formulation to a combinatorial multi-armed ban-
dit (CMAB) paradigm, where diffusions are assumed to follow the
IC model. IMLinUCB [ 35] learns the optimal influencers dynam-
ically, while repeatedly interacting with a network under the IC
assumption as well. Vaswani et al. [ 33] introduce a diffusion model-
agnostic framework, based on a pairwise-influence semi-bandit
feedback model and the LinUCB-based algorithm, addressing sce-
narios involving new marketers that exploit existing networks.
Since the aforementioned approaches leverage a given diffusion
graph topology, the inherent difficulty of obtaining such data limits
their practical interest.
Operating in highly uncertain diffusion scenarios that (i) make
no assumption on the diffusion model and (ii) lack knowledge of
the diffusion topology and historical activations (cascades), [ 20]
 
772Influence Maximization via Graph Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
proposes FAT-GT-UCB, where a Goodâ€“Turing estimator is used
to capture the utility (called remaining potential) of an influencer,
throughout the multiple rounds of a diffusion campaign. They also
consider a fatigue effect for influencers, since these may be repeat-
edly chosen in the sequential rounds. GLM-GT-UCB [ 17] considers
the same setting as [ 20], while exploiting contextual information
(e.g., features pertaining to influencers or the information being
conveyed). Our work shares a similar setting, where the network
topology is unknown and no assumptions are made about the diffu-
sion model. In a multi-round diffusion campaign, we select at each
round diffusion seeds, without factoring in influencer fatigue.
Bandits with deep learning Early works [ 1,9,28] in the con-
textual bandit literature focused on linear models, assuming the
expected reward at each round is linear in the feature vector. This
assumption, however, often fails to hold in practice, prompting ex-
ploration into nonlinear or nonparametric contextual bandits [ 7,32].
However, these more complex models impose restrictive assump-
tions on the reward function, such as Lipschitz continuity [ 7], or a re-
ward function from a reproducing kernel Hilbert space (RKHS) [ 32].
To overcome these limitations, several recent studies [ 27,38,41]
leverage the expressivity of deep neural networks (DNNs) to in-
corporate nonlinear models, which require less domain knowledge.
The works of [27, 38] employ DNNs for effective context transfor-
mation with a linear exploration policy, showing notable empirical
success despite the absence of regret guarantees. The work of [ 41]
introduces NeuralUCB, a provably efficient neural contextual bandit
algorithm using DNN-based random feature mappings to construct
the UCB, with a near-optimal regret guarantee. The construct of
the UCB is based on the past gradient of the exploitation function.
The work of [ 40] assigns a normal distribution as the distribution
of the reward of each arm, similar to the deviation computed on the
gradient of the estimation function. Similar to some other studies,
EE-Net [ 2] has an exploitation network to estimate rewards for
each arm. It additionally builds an exploration network to predict
the potential gain for each arm, relative to the current estimate,
where the inputs of the exploration network are the previous gra-
dients of the exploitation function. The work of Qi et al. [26] em-
ploys contextual neural bandits in recommender systems, to build
a graph neural bandit framework where each arm is induced with
an exploitation graph and an exploration one, with the weights of
edges representing usersâ€™ correlations regarding the exploitation
and exploration performed. The effectiveness of [ 26] in the recom-
mendation setting serves as our initial motivation for leveraging its
neural bandits framework in our IM problem. Given the similarities
in predicting user preferences (user-item in recommender systems
or user-influencer susceptibility in IM), we exploit a graph neural
contextual bandit algorithm to maximize the influence spread in
multi-round diffusion campaigns.
3 PROBLEM FORMULATION
We formulate the Influence Maximization (IM) problem with a
discrete-time diffusion model [ 18], adopting a combinatorial multi-
armed bandit paradigm to estimate the influence spread.
IM Problem Within the context of information scenarios charac-
terized by stochastic or epidemic information diffusion phenomena,
particularly on social media, the information spread is initiated byseed users (influencers) and amplified through sharing and retweet-
ing via user interactions. For a campaign of information spread
consisting of ğ‘‡rounds (trials), we select the influencers at each
round to maximize the overall information spread.
We are given a known base set of influencers ğ¾={ğ‘˜ğ‘–}ğ‘›
ğ‘–=1as
seeds, a budget of ğ‘‡rounds (trials). At each round ğ‘¡âˆˆ{1,2...,ğ‘‡},
the environment provides us with the message ğ¶ğ‘¡to diffuse, and
there areğ¿âˆˆ{1,2,...,ğ‘›}seeds to be activated initially. With ğ¼ğ‘¡
(which has cardinality |ğ¼ğ‘¡|=ğ¿) the set of activated seeds, ğ‘†(ğ¼ğ‘¡,ğ¶ğ‘¡)
is the roundâ€™s spread (all activated users) starting from the chosen
seed setğ¼ğ‘¡. Our objective is to maximize the cumulative and distinct
spread of the ğ‘‡rounds, i.e., find
argmax
ğ¼ğ‘¡âŠ†ğ¾,|ğ¼ğ‘¡|=ğ¿,âˆ€1â‰¤ğ‘¡â‰¤ğ‘‡EÃ˜
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘†(ğ¼ğ‘¡,ğ¶ğ‘¡)
. (1)
Adaptation to the bandit setting To adapt the IM problem to a
contextual bandit setting, the set of influencers ğ¾can be considered
the set of arms to be pulled in ğ‘‡rounds. At each round ğ‘¡, with the
provided message ğ¶ğ‘¡as the context, the set of arms ğ¼ğ‘¡={ğ‘˜ğ‘–}ğ¿
ğ‘–=1
is chosen. For each chosen arm ğ‘˜ğ‘–,ğ´ğ‘˜ğ‘–is the set of basic users
activated or influenced by seed (arm) ğ‘˜ğ‘–. For each basic user ğ‘¢,
letğ‘ğ‘¡ğ‘¢denote the total number of times it has been influenced or
activated until round ğ‘¡. With the set of activated users (influence
spread) as the node semi-bandit feedback, the reward is the number
of new activations [17] as
ğ‘…ğ‘¡=âˆ‘ï¸
ğ‘¢âˆˆÃ
ğ‘˜ğ‘–âˆˆğ¼ğ‘¡ğ´ğ‘˜ğ‘–1{ğ‘ğ‘¡
ğ‘¢>0}âˆ’ğ‘…ğ‘¡âˆ’1;ğ‘…0=0, (2)
Note that distinct activations are used for the cumulative reward,
i.e., a given user will be counted only once in the total reward, even
if it has been influenced several times.
Modeling with graph bandits We are mainly motivated by appli-
cation scenarios in social media (e.g., information campaigns for
elections, online advertising, public awareness campaigns, crisis
information diffusion, etc.), where users may exhibit similar prefer-
ences and influence susceptibility for certain diffusion topics (e.g.,
sharing the same political views) initiated by certain influencers
(arms), while they may react differently and be more susceptible to
other influencers for other topics (e.g., entertainment or sports).
Thus, instead of representing the social graph uniformly, in
the bandit setting, we allow each arm ğ‘˜ğ‘–at each round ğ‘¡to in-
duce a distinct graphğºğ‘–,ğ‘¡(U,ğ¸,ğ‘Šğ‘–,ğ‘¡)to represent user connectivity.
With ğ’Œğ‘–theğ‘‘1-dimensional feature vector of arm ğ‘˜ğ‘–andğ‘ªğ‘¡theğ‘‘2-
dimensional context vector, the expected reward1at each round
ğ‘¡âˆˆ[ğ‘‡]brought by arm ğ‘˜ğ‘–is defined as
ğ‘Ÿğ‘–,ğ‘¡=ğ‘“(ğ’Œğ‘–,ğ‘ªğ‘¡,ğºğ‘–,ğ‘¡). (3)
Inğºğ‘–,ğ‘¡, each user ğ‘¢âˆˆU ={1,2,...,ğ‘š}corresponds to a node,
ğ¸is the set of edges connecting users, and ğ‘Šğ‘–,ğ‘¡={ğ‘¤ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²):
ğ‘¢,ğ‘¢â€²âˆˆU} is the set of weights corresponding to each edge ğ‘’âˆˆğ¸.
Modeling real applications, we assume that the weights of the edges
connecting nodes in ğºğ‘–,ğ‘¡represent usersâ€™ similarity w.r.t. the same
influencer (arm ğ‘˜ğ‘–), i.e., the probability to be similarly influenced
1Note that this expected reward ğ‘Ÿğ‘–,ğ‘¡is assessing the distinct activations by arm ğ‘˜ğ‘–at
roundğ‘¡, in alignment with the reward brought by each arm in ğ¼ğ‘¡, as defined in Eq. (2).
 
773KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
by armğ‘˜ğ‘–in roundğ‘¡, which is defined as
ğ‘¤ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)=Î¦(1)
E
ğ‘ğ‘¡
ğ‘–,ğ‘¢|ğ’Œğ‘–,ğ‘ªğ‘¡
,E
ğ‘ğ‘¡
ğ‘–,ğ‘¢â€²|ğ’Œğ‘–,ğ‘ªğ‘¡
, (4)
whereğ‘ğ‘¡
ğ‘–,ğ‘¢=â„ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡)âˆˆ[ 0,1]theexpected diffusion probabil-
itybetween influencer (arm ğ‘˜ğ‘–) and userğ‘¢under the context ğ‘ªğ‘¡,
andÎ¦(1):RÃ—Râ†’Rmaps the expected diffusion probability of
users w.r.t. influencer ğ‘˜ğ‘–to the weights among users in ğºğ‘–,ğ‘¡.
However, the similarity graph ğºğ‘–,ğ‘¡and the function â„ğ‘¢are un-
known in our problem setting. Thus we propose an estimate graph
ğº(1)
ğ‘–,ğ‘¡=(U,ğ¸,ğ‘Š(1)
ğ‘–,ğ‘¡)to approximate ğºğ‘–,ğ‘¡by exploiting the current
observations. This is known as the exploitation graph. We also con-
sider a pre-defined hypothesis function â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡)to approximate
the expected diffusion probability ğ‘ğ‘¡
ğ‘–,ğ‘¢, so as to estimate ğ‘Šğ‘–,ğ‘¡with
ğ‘Š(1)
ğ‘–,ğ‘¡in the graph ğº(1)
ğ‘–,ğ‘¡. With the pre-estimated graph ğº(1)
ğ‘–,ğ‘¡, the
estimate reward of arm ğ‘˜ğ‘–across all users is then expressed as
Ë†ğ‘Ÿğ‘–,ğ‘¡=ğ‘“(1) ğ’Œğ‘–,ğ‘ªğ‘¡,ğº(1)
ğ‘–,ğ‘¡. (5)
To quantify the estimation gap (uncertainty of estimation) between
ğº(1)
ğ‘–,ğ‘¡andğºğ‘–,ğ‘¡(or to measure the potential gain on the estimated
diffusion probability for each user-influencer pair), we also propose
anexploration graph, denoted by ğº(2)
ğ‘–,ğ‘¡=(U,ğ¸,ğ‘Š(2)
ğ‘–,ğ‘¡), where analo-
gously the weights among users ğ‘¤(2)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)âˆˆğ‘Š(2)
ğ‘–,ğ‘¡indicate usersâ€™
correlations w.r.t. potential gains, expressed as
ğ‘¤(2)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)
=Î¦(2) â„ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡)âˆ’â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡),â„ğ‘¢â€²(ğ’Œğ‘–,ğ‘ªğ‘¡)âˆ’â„(1)
ğ‘¢â€²(ğ’Œğ‘–,ğ‘ªğ‘¡)(6)
for some function Î¦(2):RÃ—Râ†’Rwhich is similar to Î¦(1).
With the exploration graph ğº(2)
ğ‘–,ğ‘¡, the potential gain of arm ğ‘˜ğ‘–
across all the users is defined as Ë†ğ‘ğ‘–,ğ‘¡=ğ‘“(2)(ğ’Œğ‘–,ğ‘ªğ‘¡,ğº(2)
ğ‘–,ğ‘¡). At each
roundğ‘¡, the arm set ğ¼ğ‘¡is selected as arg maxğ¼ğ‘¡âŠ‚ğ¾:|ğ¼ğ‘¡|=ğ¿(Ë†ğ‘Ÿğ‘–,ğ‘¡+Ë†ğ‘ğ‘–,ğ‘¡).
This maximizes the overall influence spread in the campaign.
The details of the constructions of the exploitation and explo-
ration graphs are given in Sec. 4 below.
4 PROPOSED FRAMEWORK
Many recent works [ 17,35] on the IM problem that exploit bandits
for the exploration-exploitation trade-off assume that the reward is
a linear or generalized linear function of arm vectors. Considering
the high complexity and dynamicity of social network-related data,
we use the representation power of neural networks to firstly, learn
usersâ€™ connectivity to build exploitation and exploration graphs and
secondly, learn the underlying reward function and the potential
gains on the estimated reward. The overall framework of our model
is illustrated in Fig. 1.
4.1 Estimating the User Graphs
In this section, we first provide a strategy to estimate the usersâ€™
correlations to be influenced by the same arm, forming the basis
for the exploration-exploitation strategy in Sec. 4.2.
4.1.1 User exploitation graph. We bridge the users in the social
graph with diffusion probabilities between influencers and users.
The intuition is that given the same message to be diffused (context
ğ¶ğ‘¡), users who exhibit high correlations in this graph are more likely
to be influenced by the same influencer. As the context changes, aninfluencer may not exert the same influence on users. Thus, at each
roundğ‘¡and for each influencer (arm) ğ‘˜ğ‘–, we induce an exploitation
graphğº(1)
ğ‘–,ğ‘¡to represent the usersâ€™ correlations.
In the exploitation graph ğº(1)
ğ‘–,ğ‘¡, the weights among users are
referred to as usersâ€™ correlations w.r.t. the diffusion probability from
armğ‘˜ğ‘–(hence likelihood to influenced by the same influencer ğ‘˜ğ‘–).
For each user ğ‘¢âˆˆU, we use a neural network as the pre-defined
hypothesis function â„(1)
ğ‘¢=â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡;[P(1)
ğ‘¢]ğ‘¡âˆ’1)to learn these
probabilities ([P(1)
ğ‘¢]ğ‘¡âˆ’1are the updated parameters of the network
from round ğ‘¡âˆ’1). The weights in ğº(1)
ğ‘–,ğ‘¡are
ğ‘¤(1)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)=Î¦(1) â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡),â„(1)
ğ‘¢â€²(ğ’Œğ‘–,ğ‘ªğ‘¡), (7)
where Î¦(1)is the same function in Eq. (4). For each user ğ‘¢,â„(1)
ğ‘¢will
be trained by gradient descent (GD) with the given context and the
chosen arm as input and the reward as label. The loss is defined as
L(1)
ğ‘¢=
â„(1)
ğ‘¢ ğ’Œğ‘–,ğ‘ªğ‘¡;P(1)
ğ‘¢âˆ’ğ‘‘ğ‘¡
ğ‘¢2
, (8)
whereğ‘‘ğ‘¡ğ‘¢=1ifğ‘ğ‘¡ğ‘¢>0andğ‘ğ‘¡âˆ’1ğ‘¢=0, elseğ‘‘ğ‘¡ğ‘¢=0. Recall that we use
ğ‘ğ‘¡ğ‘¢to denote the total number of times user ğ‘¢has been activated
(influenced) up to and including round ğ‘¡, and we only count the
newly activated nodes at each round.
4.1.2 User exploration graph. Recent works on neural bandits [ 2â€“
4,40,41] take advantage of the representation power of neural
networks to learn the uncertainty of estimation (potential gain).
These works use the past gradient to incorporate the feature of arms
and the learned discriminative information of estimation function
(â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡)in our work).
Qi et al. [ 26] applied this paradigm in collaborative filtering
for user-item pair prediction in online recommendation scenarios
and demonstrated its effectiveness. Since the IM problem shares
similarity with predicting user preferences towards items (in our
case susceptibility to influencers), especially when the connections
(correlations) among users are reinforced by social ties, we apply
the past gradient to quantify the â€œexploration bonusâ€ [21].
For a userğ‘¢âˆˆU , we use a neural network â„(2)
ğ‘¢to learn the
uncertainty of the estimated diffusion probability between arm
ğ‘˜ğ‘–and userğ‘¢, i.e.,E[ğ‘ğ‘–,ğ‘¡|ğ‘¢,ğ’Œğ‘–,ğ‘ªğ‘¡]âˆ’â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡), similar to Eq. (6).
As in [ 2], we apply â„(2)
ğ‘¢directly on the previous gradient of â„(1)
ğ‘¢.
Analogously, the exploration graph ğº(2)
ğ‘–,ğ‘¡=(U,ğ¸,ğ‘Š(2)
ğ‘–,ğ‘¡)is con-
structed with ğ‘Š(2)
ğ‘–,ğ‘¡=
ğ‘¤(2)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²):ğ‘¢,ğ‘¢â€²âˆˆU	
, andğ‘¤(2)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)is
the exploration correlation among users, defined as
ğ‘¤(2)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)=Î¦(2)
â„(2)
ğ‘¢ âˆ‡â„(1)
ğ‘¢,â„(2)
ğ‘¢â€² âˆ‡â„(1)
ğ‘¢â€²
. (9)
The previous gradient âˆ‡â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡)=âˆ‡Pâ„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡;[P(1)
ğ‘¢]ğ‘¡âˆ’1)
is the network gradient at round ğ‘¡âˆ’1, with[P(1)
ğ‘¢]ğ‘¡âˆ’1the last up-
dated parameters of â„(1)
ğ‘¢. In addition, Î¦(2)is the function defined
in Eq. (6)andâ„(2)
ğ‘¢will be trained with GD, where the previous
gradient ofâ„(1)
ğ‘¢is computed based on the input samples, and the
residual diffusion probability (potential gain on the estimated diffu-
sion probability) is the label, with the loss given as
L(2)
ğ‘¢=
â„(2)
ğ‘¢ âˆ‡â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡)âˆ’ ğ‘‘ğ‘¡
ğ‘¢âˆ’â„(1)
ğ‘¢(ğ’Œğ‘–,ğ‘ªğ‘¡)2
. (10)
 
774Influence Maximization via Graph Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â„ğ‘¢1áˆº1áˆ» ğ‘¤13áˆº1áˆ» ğ‘Ì‚ğ‘–ğ‘¢1  
ğ‘¤23áˆº1áˆ» ğ‘¤12áˆº1áˆ» 
ğ‘ŸÌ‚ğ‘–,ğ‘¡àµŒà¶¨à·àµ«ğ‘Ì‚ğ‘–,ğ‘¢ğ‘¡àµ¯2
ğ‘–âˆˆğ’° GCN FC Â Network Exploitation Â graph
...Input Output
ğ‘¤13áˆº2áˆ»   
ğ‘¤23áˆº2áˆ» ğ‘¤12áˆº2áˆ» 
GCN FC Â Network Exploration Â graph...Input Output
ğ‘à· ğ‘–,ğ‘¡àµŒà¶¨à·àµ«ğ‘§Ì‚ğ‘–,ğ‘¢ğ‘¡àµ¯2
ğ‘–âˆˆğ’° 
â„ğ‘¢1áˆº2áˆ» â„ğ‘¢2áˆº2áˆ» 
â„ğ‘¢3áˆº2áˆ» 
â„ğ‘¢2áˆº1áˆ» 
â„ğ‘¢3áˆº1áˆ» 
âˆ‡â„ğ‘¢áˆº1áˆ» âˆ‡ğ‘“áˆº1áˆ» â„ğ‘¢áˆº1áˆ»àµ«ğ’Œğ’Š,ğ’•,ğ‘ªğ’•àµ¯
â„ğ‘¢áˆº2áˆ»àµ«âˆ‡â„ğ‘¢áˆº1áˆ»àµ¯ğºğ‘–,ğ‘¡áˆº1áˆ» 
ğºğ‘–,ğ‘¡áˆº2áˆ» ğ‘“áˆº1áˆ» 
ğ‘“áˆº2áˆ» ğ‘Ì‚ğ‘–ğ‘¢2 ğ‘Ì‚ğ‘–ğ‘¢ğ‘š 
ğ‘§Ì‚ğ‘–ğ‘¢1 ğ‘§Ì‚ğ‘–ğ‘¢2 ğ‘§Ì‚ğ‘–ğ‘¢ğ‘š 
Potential Â gainRewardÂ Exploitation
GraphÂ estimation RewardÂ andÂ potential Â gainÂ estimation
ğ‘¤ğ‘¢1,ğ‘¢2áˆº2áˆ»àµŒÎ¦áˆº2áˆ»àµ«hu1áˆº2áˆ»,hu2áˆº2áˆ»àµ¯ FeatureÂ vectorÂ 
&Â contextÂ vector
Explorationğ‘¤ğ‘¢1,ğ‘¢2áˆº1áˆ»àµŒÎ¦áˆº1áˆ»àµ«hu1áˆº1áˆ»,hu2áˆº1áˆ»àµ¯ 
argmax
ğ¼ğ‘¡âŠ‚ğ¾,|ğ¼ğ‘¡|àµŒğ¿áˆºğ‘ŸÌ‚ğ‘–,ğ‘¡àµ…ğ‘à· ğ‘–,ğ‘¡áˆ» ArmÂ selection
Figure 1: The framework of IM-GNB. For each arm, we initially take the arm feature vector and the current context vector
(ğ’Œğ‘–,ğ‘ªğ‘¡)as inputs to estimate the diffusion probability for each user-arm pair with â„(1)
ğ‘¢. Subsequently, we assess the potential
gain on the diffusion probability with the past gradient of â„(1)
ğ‘¢, yielding both exploitation and exploration graphs. With the
pre-estimated graphs, we refine the estimate of the diffusion probability for each user-arm pair with ğ‘“(1)andğ‘“(2). The aggregate
reward of the arm across all users is derived from the sum of all the refined individual diffusion probabilities. The potential
gain is measured similarly. Finally, we select the arm with the highest sum of estimated reward and its potential gain.
Regarding the network structure of â„(1)andâ„(2), since there
are no data characteristics requiring specific models such as RNNs
for sequential dependencies or CNNs for visual content, we simply
employ anğ½-layer fully connected (FC) neural network at this stage
for initial graph estimation.
To summarise, we use â„(1)
ğ‘¢, denoting user ğ‘¢, to obtain the es-
timated diffusion probability from influencer (arm) ğ‘˜ğ‘–toğ‘¢(the
estimation function is built for each user individually, i.e., there are
ğ‘šestimation functions â„(1)
ğ‘¢in total), and the exploitation graph
ğº(1)
ğ‘–,ğ‘¡for armğ‘˜ğ‘–is built such that the basic users are correlated
based on estimated diffusion probabilities. We also apply â„(2)
ğ‘¢to
represent the uncertainty of the estimated diffusion probability,
and the exploration graph ğº(2)
ğ‘–,ğ‘¡corresponding to arm ğ‘˜ğ‘–is built
such that the users are correlated based on the potential gains. The
graph estimation process is given in Lines 13â€“17 of Algorithm 1.
4.2 Exploitation-Exploration with GNNs
With the user correlation graphs ğº(1)
ğ‘–,ğ‘¡andğº(2)
ğ‘–,ğ‘¡forexploitation
andexploration respectively, we now have a refined estimate of the
diffusion probabilities between influencers and users, as well as the
expected total spread (newly activated users), i.e., the reward.
4.2.1 GNN for exploitation. In roundğ‘¡, for each arm ğ‘˜ğ‘–, with the
pre-estimated exploitation graph ğº(1)
ğ‘–,ğ‘¡for armğ‘˜ğ‘–as input, we use
a GNN model ğ‘“(1)(ğ’Œğ‘–,ğ‘ªğ‘¡,ğº(1)
ğ‘–,ğ‘¡;P(1))to estimate the reward de-
scribed in Eq. (3), with P(1)representing the parameters of ğ‘“(1).We define for each arm a symmetric adjacency matrix ğ´(1)
ğ‘–,ğ‘¡âˆˆ
Rğ‘šÃ—ğ‘šfrom the exploitation graph ğº(1)
ğ‘–,ğ‘¡, with each element in the
matrix corresponding to the correlations weights ğ‘¤ğ‘¢,ğ‘¢â€²between
userğ‘¢and userğ‘¢â€²inğº(1)
ğ‘–,ğ‘¡, and the normalized adjacency matrix [ 19]
beingğ‘†(1)
ğ‘–,ğ‘¡=ğ·âˆ’1
2ğ´(1)
ğ‘–,ğ‘¡ğ·âˆ’1
2, withğ·the degree matrix. We concate-
nate the arm feature vector ğ’Œğ‘–with the context vector ğ‘ªğ‘¡to build
the feature matrix ğ‘¿ğ‘–,ğ‘¡=diag([ğ’Œğ‘–,ğ‘ªğ‘¡],[ğ’Œğ‘–,ğ‘ªğ‘¡],...,[ğ’Œğ‘–,ğ‘ªğ‘¡]) âˆˆ
Rğ‘šÃ—ğ‘š(ğ‘‘1+ğ‘‘2).
We adopt a simplified Graph Convolutional Network (GCN)
model [ 36] to learn the aggregated representation of the exploita-
tion graph. With ğ‘†(1)
ğ‘–,ğ‘¡andğ‘¿(1)
ğ‘–,ğ‘¡as inputs, the feature representation
matrix is expressed as
ğ»G=ğœ ğ‘†(1)
ğ‘–,ğ‘¡ğ›¾ğ‘¿(1)
ğ‘–,ğ‘¡;P(1)
G
, (11)
whereğœis the activation function, P(1)
GâˆˆRğ‘š(ğ‘‘1+ğ‘‘2)Ã—ğ‘is the
trainable weight matrix in the GCN model, and ğ›¾is the number of
hops the information propagating over the user graph, indicating
that afterğ‘˜layers a node obtains the feature information from all
nodes found ğ›¾hops away in the graph. In the GCN model, ğ‘¿(1)
ğ‘–,ğ‘¡is
applied to the corresponding weight matrix P(1)
Gso thatP(1)
Gis
partitioned for each user ğ‘¢âˆˆU to get theğ‘-dimensional arm-user
diffusion representation, corresponding to each row of ğ»GâˆˆRğ‘šÃ—ğ‘.
To further refine the ğ‘-dimensional arm-user pair representation
inğ»G, we add an ğ½-layer FC neural network to the GCN model,
and forğ‘™âˆˆ{1,2,...,ğ½âˆ’1}the representation for each layer is
ğ»ğ‘™=ğœ ğ»ğ‘™âˆ’1Â·P(1)
ğ‘™,andğ»0=ğ»G, (12)
 
775KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
withğ»ğ‘™âˆˆRğ‘šÃ—ğ‘, andP(1)
ğ‘™being the trainable parameters in each
layer in the FC network. For the last layer, we have
Ë†ğ‘ƒğ‘–,ğ‘¡=ğ»ğ½âˆ’1Â·P(1)
ğ½, (13)
where theP(1)
ğ½are the parameters in the last layer, and Ë†ğ‘ƒğ‘–,ğ‘¡âˆˆRğ‘šis
theğ‘š-dimensional vector with each element the refined estimated
diffusion probability Ë†ğ‘ğ‘¡
ğ‘–,ğ‘¢âˆˆRbetween arm ğ‘˜ğ‘–and userğ‘¢âˆˆU.
With the refined estimated diffusion probability between arm
ğ‘˜ğ‘–and all the users, the estimated reward for arm ğ‘˜ğ‘–across all the
users is computed as the norm of the output layer:
Ë†ğ‘Ÿğ‘–,ğ‘¡=âˆ¥Ë†ğ‘ƒğ‘–,ğ‘¡âˆ¥=âˆšï¸„âˆ‘ï¸
ğ‘¢âˆˆU Ë†ğ‘ğ‘¡
ğ‘–,ğ‘¢2. (14)
The exploitation network will be trained with GD based on the
influence spread from arm ğ‘˜ğ‘–, where the predicted outputs are the
diffusion probabilities across all users, with the label (reward)
ğ‘Ÿğ‘–,ğ‘¡=âˆ‘ï¸
ğ‘¢âˆˆğ´ğ‘˜ğ‘–ğ‘‘ğ‘¡
ğ‘¢. (15)
Recall thatğ´ğ‘˜ğ‘–is the set of users activated or influenced by arm ğ‘˜ğ‘–
at roundğ‘¡, andğ‘‘ğ‘¡ğ‘¢is defined as the distinct activations in Eq. (8).
For refined learning on each user-arm diffusion pair, we calculate
the quadratic loss w.r.t. each user individually, as
L(1)=âˆ‘ï¸
ğ‘¢âˆˆğ´ğ‘˜ğ‘– Ë†ğ‘ğ‘¡
ğ‘–,ğ‘¢âˆ’ğ‘‘ğ‘¡
ğ‘¢2. (16)
4.2.2 GNN for exploration. Similar to the user graph pre-estimation
described in Sec. 4.1, we follow the exploration-exploitation strat-
egy by applying a gradient-based exploration function w.r.t. the
exploitation function; also see [2, 26, 40, 41] for similar strategies.
In roundğ‘¡, for each arm ğ‘˜ğ‘–, with the induced exploration graph
ğº(2)
ğ‘–,ğ‘¡where the pre-estimated weights in Sec. 4.1.2 represent the ex-
ploration correlations among users, we apply another GNN model
ğ‘“(2)(âˆ‡ğ‘“(1),ğº(2)
ğ‘–,ğ‘¡;P(2))to evaluate the potential gain (the gap be-
tween expected reward and estimated reward) for arm ğ‘˜ğ‘–, where
âˆ‡ğ‘“(1)=âˆ‡Pğ‘“(1)(ğ’Œğ‘–,ğ‘ªğ‘¡;[P(1)]ğ‘¡âˆ’1), andP(1)andP(2)are the
parameters of ğ‘“(1)andğ‘“(2)respectively.
We adopt the same network architecture as in the exploitation
network to learn the representation matrix for the exploration graph
with ağ‘˜-hop simplified GCN, and to predict the potential gain with
anğ½-layer FC neural network. The architecture of ğ‘“(2)can be
also implemented via Eqs. (11)â€“(13), with the input feature vector
ğ‘¿(2)
ğ‘–,ğ‘¡âˆˆRğ‘šÃ—ğ‘šğ‘and trainable matrix P(2)
GâˆˆRğ‘šğ‘Ã—ğ‘in the GCN
model, where ğ‘is the dimension of input gradient. In the GCN of
ğ‘“(2), the input gradient matrix ğ‘¿(2)
ğ‘–,ğ‘¡is similarly applied to partition
the weight matrixP(2)
G, so that each user-arm pair is represented
by ağ‘-dimensional vector for the purpose of exploration.
In the output layer we obtain an ğ‘š-dimensional vector Ë†ğ‘ğ‘–,ğ‘¡,
where each element represents the estimated potential gain Ë†ğ‘§ğ‘¡
ğ‘–,ğ‘¢âˆˆ
R,ğ‘¢âˆˆU (with|U|=ğ‘š) for each user-arm pair. With the esti-
mated potential gains from the output layer, the overall estimated
potential gain for arm ğ‘˜ğ‘–is obtained as the norm of output Ë†ğ‘ğ‘–,ğ‘¡, i.e.,
Ë†ğ‘ğ‘–,ğ‘¡=âˆ¥Ë†ğ‘ğ‘–,ğ‘¡âˆ¥=âˆšï¸„âˆ‘ï¸
ğ‘¢âˆˆU Ë†ğ‘§ğ‘¡
ğ‘–,ğ‘¢2. (17)
When training ğ‘“(2)with GD, the quadratic loss is computed
between the estimated potential gain and the residual gain (the gapbetween the reward in Eq. (15) and the estimated reward), as
L(2)=âˆ‘ï¸
ğ‘¢âˆˆğ´ğ‘˜ğ‘–
Ë†ğ‘§ğ‘¡
ğ‘–,ğ‘¢âˆ’ ğ‘‘ğ‘¡
ğ‘¢âˆ’Ë†ğ‘ğ‘¡
ğ‘–,ğ‘¢2
. (18)
The computations of the reward and potential gain are summa-
rized in Lines 5â€“7 in Algorithm 1.
Algorithm 1: IM-GNB
Input: Influencer set ğ¾, number of selections ğ¿per round
Output: Arm recommendation for each time step ğ‘¡
1Initialization of all the trainable parameters
2forğ‘¡=1,2,3,...,ğ‘‡ do
3 Receive from environment the context ğ‘ªğ‘¡
4 forğ‘˜ğ‘–âˆˆğ¾do
5 construct two user graphs ğº(1)
ğ‘–,ğ‘¡andğº(2)
ğ‘–,ğ‘¡from
Procedure Estimating graphs for arm ğ‘˜ğ‘–
6 Compute estimate of reward
Ë†ğ‘Ÿğ‘–,ğ‘¡=ğ‘“(1)(ğ’Œğ‘–,ğ‘ªğ‘¡,ğº(1)
ğ‘–,ğ‘¡;[P(1)]ğ‘¡âˆ’1)
7 and, potential gain
Ë†ğ‘ğ‘–,ğ‘¡=ğ‘“(2)(âˆ‡[ğ‘“(1)]ğ‘–,ğ‘¡,ğº(2)
ğ‘–,ğ‘¡;[P(2)]ğ‘¡âˆ’1)
8 choose arm set ğ¼ğ‘¡=arg maxğ¼ğ‘¡âŠ‚ğ¾,|ğ¼ğ‘¡|=ğ¿(Ë†ğ‘Ÿğ‘–,ğ‘¡+Ë†ğ‘ğ‘–,ğ‘¡)and
observe the true reward (spread) ğ‘…ğ‘¡in Eq. (2), which
represents the newly activated users.
9 forğ‘¢âˆˆUdo
10 train the user networks â„(1)
ğ‘¢(Â·;P(1)
ğ‘¢),â„(2)
ğ‘¢(Â·;P(2)
ğ‘¢)
11 forğ‘˜âˆˆKdo
12 train the GNN models ğ‘“(1)(Â·;P(1)),ğ‘“(2)(Â·;P(2))
13Procedure, Estimating graphs for arm( ğ‘˜ğ‘–,ğ‘¡)
14 foreach user pair(ğ‘¢,ğ‘¢â€²)âˆˆUÃ—U do
15 Foredge weight ğ‘¤(1)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)âˆˆğ‘Š(1)
ğ‘–,ğ‘¡,update
ğ‘¤(1)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)=Î¦(1)(â„(1)
ğ‘¢(ğ‘˜ğ‘–,ğ‘¡),â„(1)
ğ‘¢â€²(ğ‘˜ğ‘–,ğ‘¡))
16 Foredge weight ğ‘¤(1)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)âˆˆğ‘Š(1)
ğ‘–,ğ‘¡,update
ğ‘¤(2)
ğ‘–,ğ‘¡(ğ‘¢,ğ‘¢â€²)=Î¦(2)(â„(2)
ğ‘¢(âˆ‡â„(1)
ğ‘¢),â„(2)
ğ‘¢â€²(âˆ‡â„(1)
ğ‘¢â€²))
17 returnğº(1)
ğ‘–,ğ‘¡andğº(2)
ğ‘–,ğ‘¡
4.2.3 IM-GNB arm selection. We summarize the IM-GNB frame-
work in Algorithm 1. For an information diffusion campaign with
ğ‘‡rounds, we select ğ¿influencers (arms) from a known influencers
baseğ¾at each round ğ‘¡to diffuse the given message ğ‘ªğ‘¡. At each
roundğ‘¡for each arm ğ‘˜ğ‘–, we firstly construct the two user graphs
i.e., the exploitation graph and the exploration graph via a procedure
(Lines 13â€“17) of pre-estimation on graph weights, which capture
usersâ€™ correlations in terms of exploitation and exploration respec-
tively. With the derived graphs, we compute the overall expected
reward Ë†ğ‘Ÿğ‘–,ğ‘¡and potential gain Ë†ğ‘ğ‘–,ğ‘¡for each arm in Eqs. (14)and(17),
as the norms of the output vectors from ğ‘“(1)andğ‘“(2). Next, we
select the arm set based on the maximum of the sum of reward
estimation and potential gain Ë†ğ‘Ÿğ‘–,ğ‘¡+Ë†ğ‘ğ‘–,ğ‘¡(Line 8). Finally, for each user
ğ‘¢âˆˆU, we train the userâ€™s neural networks from pre-estimation,
and we train for each arm ğ‘˜ğ‘–âˆˆğ¾the GNN models (Lines 9â€“12).
We observe from the above that at each round ğ‘¡,â„(1)
ğ‘¢will take as
input the feature vector of a certain arm ğ‘˜ğ‘–, along with the context
 
776Influence Maximization via Graph Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
vector ğ‘ªğ‘¡to provide an initial estimate on the diffusion probability
for userğ‘¢being influenced by arm ğ‘˜ğ‘–. Subsequently, the gradient
ofâ„(1)
ğ‘¢is employed as input to estimate the potential gain in diffu-
sion probability. Parameters in â„(1)
ğ‘¢andâ„(2)
ğ‘¢undergo continuous
training and updating at each round to refine the approximation
functions for user ğ‘¢, predicting the probability of being influenced
by any arm within various contexts. Similarly, for the reward esti-
mation for each arm with ğ‘“(1)andğ‘“(2),ğ‘“(1)takes as input the arm
feature vector and context vector, as well as the pre-estimated graph
ğº(1)
ğ‘–,ğ‘¡to refine the initial estimation on the diffusion probability, al-
lowing to estimate the reward across all users, and the gradient of
ğ‘“(1)serves as the input of exploration function ğ‘“(2). Bothğ‘“(1)and
ğ‘“(2)undergo continuous training to refine the reward estimation
function (exploitation) and potential gain (exploration). This itera-
tive process ensures that ğ‘“(1)andğ‘“(2)adapt effectively to diverse
contexts and user correlation graphs.
4.2.4 Complexity Analysis. Recall from the previous notation con-
ventions that we have |ğ¾|=ğ‘›arms,ğ‘šusers, and the dimensions
of the feature vectors and context information are ğ‘‘1andğ‘‘2respec-
tively. For simplicity, we use ğ‘‘ğ‘”to denote the dimension of all the
input gradients, and we assume that the same structure is used for
all the FC neural networks in our model. In particular, each neural
network has ğ½layers and each layer has ğ‘›neurons.
For the pre-estimation of user exploitation and exploration graphs,
at each round, the complexity of the pre-defined hypothesis func-
tionsâ„(1)
ğ‘¢andâ„(2)
ğ‘¢(FC neural networks) is ğ‘‚ |ğ¾|ğ‘šğ½(ğ‘‘1+ğ‘‘2)ğ‘›for
exploitation and ğ‘‚(|ğ¾|ğ‘šğ½ğ‘‘ğ‘”ğ‘›)for exploration.
For the refined estimation procedure where we use GCNs, as we
predict correlations among all users, the graphs can be seen as com-
plete. Assuming that the exploration / exploitation GCNs share the
same NN structure, the time and space complexities for exploitation
areğ‘‚ |ğ¾|ğ½(ğ‘š2(ğ‘‘1+ğ‘‘2)+ğ‘š(ğ‘‘1+ğ‘‘2)2)andğ‘‚ |ğ¾|ğ½(ğ‘š2+(ğ‘‘1+ğ‘‘2)2+
ğ‘š(ğ‘‘1+ğ‘‘2))respectively, and for exploration ğ‘‚ |ğ¾|ğ½(ğ‘š2ğ‘‘ğ‘”+ğ‘šğ‘‘2ğ‘”)
andğ‘‚ |ğ¾|ğ½(ğ‘š2+ğ‘‘2ğ‘”+ğ‘šğ‘‘ğ‘”).
From the above discussion, we can observe that the number of
users and the dimension of the input gradient are the most critical
parameters in determining the time complexity. We consider the
following methods to reduce the computational complexity.
User clustering. In applications with billions of users on social
media, it is impractical and excessively costly to predict diffusion
probabilities and correlations at the granularity of individual users.
In response to this challenge, we can leverage the posting activity
(e.g., retweeting history) of users to construct a topic distribution
vector for each user. We can then cluster users into a specific num-
ber of groups, with each user group representing a macro-node
in a smaller social graph. Notably, the theoretical underpinnings
outlined earlier remain applicable to these clustered user groups,
and userğ‘¢becomesğ‘¢ğ‘ğ‘–,ğ‘–=1,2,...,ğ‘šâ€²andâˆªğ‘šâ€²
ğ‘–=1ğ‘¢ğ‘ğ‘–=U, withğ‘šâ€²
denoting the number of clustered groups. Despite this adjustment,
we continue to refer to the user group ğ‘¢ğ‘ğ‘–as userğ‘¢for simplicity.
The introduction of clustering can significantly reduce compu-
tational cost, transforming the space complexity of the adjacency
matrix in the GCN from ğ‘šÃ—ğ‘što a more computationally efficient
scale. Experiments are carried out on the number of clusteringgroups in Sec. 5 to show the impact of the number of clusters on
the performance of the model.
Input gradients. In Sec. 4.1.2 and Sec. 4.2.2, we saw that the in-
put dimensions for the previous gradients can pose computational
challenges due to their potentially large values. This is particu-
larly relevant in Sec. 4.2.2, where the input gradient dimension is
ğ‘š(ğ‘‘1+ğ‘‘2)ğ‘+(ğ½âˆ’1)ğ‘2+ğ‘. To address this issue, and inspired by
approaches commonly employed in CNN-related works, we use the
average pooling technique to effectively reduce the input dimension
and improve efficiency.
5 EXPERIMENTS
In this section, we evaluate our model IM-GNB on datasets from
Twitter and Sina Weibo. We compare it with baselines also designed
for multi-round diffusion campaigns, and we analyze the compar-
ison results in the end. For reproducibility, the IM-GNB code is
available at https://github.com/goldenretriever-5423/IM_GNB.
Datasets Twitter and Weibo are two of the largest social media
platforms. We collected the Twitter dataset through its API. In our
context analysis for Twitter, we apply ğ¾-means clustering on the
public vocabulary glove-twitter-200 [25], available from the Gen-
sim word embedding open-source library2. The resulting clusters
provide centroids that serve as representative themes within the
dataset. Subsequently, we represent them as a distribution across
these centroids (10 in our experiments) to encode tweets. Each word
in a tweet is assigned to its nearest centroid, resulting in the overall
distribution. The feature vector of the influencer is the normalized
aggregation of all its historical tweets. The Weibo dataset [ 39] is
a publicly available one built for information diffusion studies. In
this dataset, each post is encoded with a distribution over the 100
topics [ 39] using latent Dirichlet allocation [ 5]. Similar to the Twit-
ter dataset, the feature vector of the influencer is the normalized
aggregate of the topic distribution of all historical tweets.
To simulate campaigns on social media, we assume that the
marketer has access to only a few of the most important influencers
to diffuse the message in the campaigns. Hence, we fix the size of
the influencer set ğ¾by selecting the users with the highest number
of reposts in our Twitter and Weibo logs, and we keep all the
tweets related to them. The statistics of the datasets before and after
filtering are given in Tables 1 and 2. In each campaign, we randomly
chose the contexts (tweets), referred to as topic distributions, for
each round from the pool of available contexts within the dataset.
Table 1: Description of original datasets.
#users #original tweets # retweets
Twitter 11.6M 242M 341.8M
Weibo 1.8M 300K 23.8M
Table 2: Description of filtered datasets.
#users #original tweets # retweets
Twitter 31.6k 19k 1M
Weibo 54.4k 6K 1M
Baselines We compare IM-GNB to a set of bandit models designed
for the IM problem under the same multi-round campaigns scenario,
2https://pypi.org/project/gensim/
 
777KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
where the underlying network is unknown and no assumptions
about the diffusion models are made. LogNorm-LinUCB [ 17] and
GLM-GT-UCB [ 17] are the recent, state-of-the-art approaches to
maximize information diffusion during such IM campaigns. LogNorm-
LinUCB directly adapts the LinUCB algorithm by using logarith-
mic normalization and contextual information to make sequential
selections of spread seeds, while GLM-GT-UCB employs a gener-
alized linear model and the Goodâ€“Turing estimator to determine
the remaining potential of influencers. We also compare with FAT-
GT-UCB [ 20], a context-free model which has the particularity to
consider the fatigue, i.e., an influencersâ€™ diminishing tendency to
activate basic users as they are re-seeded throughout the campaign.
We also generalize several state-of-the-art neural bandit methodsâ€“
NeuralUCB [ 40],NeuralTS [ 41], and LinUCB [ 8] to our multi-round
IM campaign. Finally, we also implement a reference model that
randomly chooses the influencer(s) at each round, as in [17].
Experimental Setting In our experiments, to reduce the compu-
tation cost, we first cluster all the users into 50groups. For the
pre-estimation of the graph weights, we use a 3-layer FC neural
network as the hypothesis class for both â„(1)
ğ‘¢andâ„(2)
ğ‘¢, to estimate
the diffusion probability and potential gain. The functions Î¦(1)and
Î¦(2)that map the usersâ€™ correlations to the weights in the graphs
are radial basis functions (RBFs), with their bandwidths set to 5. For
the GCN model, we explore the use of 3 hops (i.e., ğ›¾=3) to capture
multi-level relationships within the user graphs, with a 3-layer FC
neural network connected at the end. The pooling step sizes to
reduce dimensions [ 26] for the input gradients in ğ‘“(2)are set to
1,000and10,000respectively for the Twitter and Weibo datasets.
Empirical Results For a diffusion campaign, at each round, the
environment first provides the context, an algorithm then selects
the roundâ€™s influencer(s), and finally, a tweet is sampled for the
specific pair of influencer(s) and context from the dataset. The new
activations are determined by discounting the users previously
encountered from the set of users associated with the sampled
tweet. All our empirical results are averaged over 100 independent
runs (the means and standard deviations are reported), and the
diffusion budget is set to 500 rounds.
Comparison with baselines: We conducted comparisons with various
baselines on the Twitter and Weibo datasets, varying the number
of chosen seeds ( ğ¿) per round within {1,2,..., 5}. The results are
shown in Fig. 2 and Fig. 3 respectively. From the two figures, we can
observe that, across both datasets, our model IM-GNB generally
outperforms the baselines. Notably, on the Twitter dataset, IM-GNB
exhibits a significantly increased advantage over the baselines, as
the number of seeds increases up to ğ¿=3. However, this advantage
diminishes as ğ¿continues to grow, as the probability of selecting
the correct arms increases for all models. In fact, for the extreme
scenario where ğ¿=|ğ¾|=10, this results in the same performances
across all models due to the selection of the entire base set of seeds.
Similarly, for the Weibo dataset, IM-GNB demonstrates its largest
advantage at ğ¿=4. These results validate our motivation to leverage
the expressivity of both neural networks and bandit algorithms in
IM campaigns, enabling us to effectively capture dynamic user-user
and user-influencer interactions using GNBs.
It is worth noting that, in the Weibo dataset, particularly when
ğ¿is small (e.g., ğ¿=1,2), the performance in the initial roundsis surpassed by certain baseline methods, notably GLM-GT-UCB,
which exhibit more rapid learning capabilities. We attribute this
phenomenon to the nature of IM-GNB as a data-driven model, typ-
ical of modern deep learning-based approaches. The efficiency of
IM-GNB improves rapidly with the accumulation of data (i.e., as
more rounds pass by), indicating potential slower convergence ini-
tially, but yielding better results as the number of rounds increases.
Additionally, the Weibo dataset is a publicly available dataset that
consists of artificially extracted data from diffusion cascades, while
the Twitter dataset, albeit sparser than Weibo, offers insights closer
to real-world IM scenarios.
In both datasets, Lognorm-LinUCB generally outperforms the
other baselines. While there are instances where GLM-GT-UCB
briefly outperforms Lognorm-LinUCB in the initial stages, the lat-
ter demonstrates stable performance with smaller error bars. This
underscores the robustness of the log-normal assumption on the
reward distribution. NeuralUCB and NeuralTS, as scalarizations of
the general neural bandit model, exhibit comparable performances
across both datasets. Notably, their effectiveness lags behind mod-
els tailored for multi-round diffusion campaigns when ğ¿is small.
However, when ğ¿increases, the models are empowered with more
data, showing marked performance improvements.
Hyperparameter Analysis
Number of clusters: We conduct experiments on the number of
clusters in the Twitter dataset with ğ¿=2, and the results on the
last round (final accumulated spread) are shown in Fig. 4 of the
appendix. We observe from Fig. 4 that the campaign performance
improves as the number of clusters increases from 2to150at the
beginning. However, beyond 200clusters, performance begins to
decline. This decline can be likely attributed to insufficient data
within each cluster for effective learning with the constraints of
a limited budget on the number of rounds. Additionally, compu-
tational costs escalate exponentially as the cluster size goes up.
Through the analysis, a cluster size of 20to50seems to strike the
right balance between performance and computational efficiency.
This observation not only validates the initial rationale for cluster-
ing users, but also underscores the significance of computational
efficiency in optimizing social campaigns under budget constraints.
Boosted Exploration Scores: Bandit algorithms aim to strike a delicate
balance between exploiting known information to maximize short-
term gains and exploring unknown options to improve long-term
performance. In this spirit, we also consider in the experiments
a variant of exploration, in which we boost the exploration score
of unchosen arms having zero reward outcomes, to increase the
likelihood of exploring alternative arms. We run experiments on
the Twitter dataset with ğ¿=2comparing the use of such artificially
boosted exploration against its absence. The results are presented
in Fig. 5 in the appendix, and confirm the effectiveness of this
approach; this further supports the importance of exploration to
uncover valuable insights and optimize online decision-making.
6 CONCLUSION
In summary, our IM-GNB framework seamlessly leverages the ex-
pressivity of GNBs to tackle the challenges of multi-round IM in
uncertain environments. Our novel approach tackles key issues in
learning from graph-structured data and makes sequential decisions
 
778Influence Maximization via Graph Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 2: Comparison of IM-GNB with baselines on the Twitter dataset.
Figure 3: Comparison of IM-GNB with baselines on the Weibo dataset.
in uncertain environments. By incorporating contextual bandits,
we obtain initial estimates of diffusion probabilities to construct
exploitation and exploration graphs. Subsequently, these estimates
are refined with GCNs, to enhance the influence spread. The frame-
workâ€™s scalability, even without prior knowledge of the network
topology, makes it a valuable and versatile tool for optimizing dif-
fusion campaigns.Acknowledgements. This work is funded by the Singapore Ministry
of Education AcRF Tier 2 (A-8000423-00-00). This research is part
of the programme DesCartes and is supported by the National
Research Foundation, Prime Ministerâ€™s Office, Singapore under
its Campus for Research Excellence and Technological Enterprise
(CREATE) programme. The authors thank Fengzhuo Zhang and
Junwen Yang (both NUS) for valuable discussions.
 
779KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
REFERENCES
[1]Naoki Abe, Alan W Biermann, and Philip M Long. 2003. Reinforcement learning
with immediate rewards and linear hypotheses. Algorithmica 37 (2003), 263â€“293.
[2]Yikun an, Yuchen Yan, Arindam Banerjee, and Jingrui He. 2022. EE-Net:
Exploitation-Exploration Neural Networks in Contextual Bandits. In Proceedings
of International Conference on Learning Representations.
[3]Yikun Ban and Jingrui He. 2021. Local clustering in contextual multi-armed
bandits. In Proceedings of the Web Conference 2021. 2335â€“2346.
[4]Yikun Ban, Jingrui He, and Curtiss B Cook. 2021. Multi-facet contextual bandits:
A neural network perspective. In Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining. 35â€“45.
[5]David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet alloca-
tion. Journal of Machine Learning Research 3, Jan (2003), 993â€“1022.
[6]Christian Borgs, Michael Brautbar, Jennifer Chayes, and Brendan Lucier. 2014.
Maximizing social influence in nearly optimal time. In Proceedings of the Twenty-
Fifth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM, 946â€“957.
[7]SÃ©bastien Bubeck, RÃ©mi Munos, Gilles Stoltz, and Csaba SzepesvÃ¡ri. 2011. X-
Armed Bandits. Journal of Machine Learning Research 12, 5 (2011).
[8]Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. 2011. Contextual bandits
with linear payoff functions. In Proc. of the Fourteenth International Conference
on Artificial Intelligence and Statistics. 208â€“214.
[9]Varsha Dani, Thomas P Hayes, and Sham M Kakade. 2008. Stochastic linear
optimization under bandit feedback. In Proceedings of the Conference on Learning
Theory. 355â€“366.
[10] Pedro M. Domingos and Matthew Richardson. 2001. Mining the network value
of customers. In Proceedings of the seventh ACM SIGKDD international conference
on Knowledge discovery and data mining, San Francisco, CA, USA, August 26-29,
2001. 57â€“66.
[11] Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. 2013. Scalable
influence estimation in continuous-time diffusion networks. In Conference on
Neural Information Processing Systems. 3147â€“3155.
[12] Shanshan Feng, Gao Cong, Arijit Khan, Xiucheng Li, Yong Liu, and Yeow Meng
Chee. 2018. Inf2vec: Latent representation model for social influence embedding.
In2018 IEEE 34th International Conference on Data Engineering (ICDE). IEEE,
941â€“952.
[13] Manuel Gomez-Rodriguez and Bernhard SchÃ¶lkopf. 2012. Influence Maximization
in Continuous Time Diffusion Networks. In Proceedings of the International
Conference on Machine Learning.
[14] Amit Goyal, Francesco Bonchi, and Laks VS Lakshmanan. 2010. Learning influ-
ence probabilities in social networks. In Proceedings of the Third ACM International
Conference on Web search and Data Mining. 241â€“250.
[15] Amit Goyal, Francesco Bonchi, and Laks VS Lakshmanan. 2011. A Data-Based
Approach to Social Influence Maximization. In Proc. VLDB Endow. 73â€“84.
[16] Keke Huang, Sibo Wang, Glenn S. Bevilacqua, Xiaokui Xiao, and Laks V. S.
Lakshmanan. 2017. Revisiting the Stop-and-Stare Algorithms for Influence Maxi-
mization. Proc. VLDB Endow. 10, 9 (2017), 913â€“924.
[17] Alexandra Iacob, Bogdan Cautis, and Silviu Maniu. 2022. Contextual bandits for
advertising campaigns: A diffusion-model independent approach. In Proceedings
of the 2022 SIAM International Conference on Data Mining (SDM). SIAM, 513â€“521.
[18] David Kempe, Jon Kleinberg, and Ã‰va Tardos. 2003. Maximizing the spread of
influence through a social network. In Proceedings of the Ninth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 137â€“146.
[19] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. International Conference on Learning Representations
(2016).
[20] Paul LagrÃ©e, Olivier CappÃ©, Bogdan Cautis, and Silviu Maniu. 2018. Algorithms
for online influencer marketing. ACM Transactions on Knowledge Discovery from
Data (TKDD) 13, 1 (2018), 1â€“30.[21] Tor Lattimore and Csaba SzepesvÃ¡ri. 2020. Bandit Algorithms. Cambridge Uni-
versity Press.
[22] Yandi Li, Haobo Gao, Yunxuan Gao, Jianxiong Guo, and Weili Wu. 2023. A Survey
on Influence Maximization: From an ML-Based Combinatorial Optimization. ACM
Trans. Knowl. Discov. Data 17, 9, Article 133 (Jul 2023), 50 pages.
[23] Hung T Nguyen, My T Thai, and Thang N Dinh. 2016. Stop-and-stare: Optimal
sampling algorithms for viral marketing in billion-scale networks. In Proceedings
of the 2016 international conference on management of data. 695â€“710.
[24] George Panagopoulos, Fragkiskos D Malliaros, and Michalis Vazirgiannis. 2020.
Multi-task learning for influence estimation and maximization. IEEE Transactions
on Knowledge and Data Engineering 34, 9 (2020), 4398â€“4409.
[25] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proc. of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP). 1532â€“1543.
[26] Yunzhe Qi, Yikun Ban, and Jingrui He. 2023. Graph Neural Bandits. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
1920â€“1931.
[27] Carlos Riquelme, George Tucker, and Jasper Snoek. 2018. Deep Bayesian bandits
showdown. In International Conference on Learning Representations, Vol. 9.
[28] Paat Rusmevichientong and John N Tsitsiklis. 2010. Linearly parameterized
bandits. Mathematics of Operations Research 35, 2 (2010), 395â€“411.
[29] Lichao Sun, Weiran Huang, Philip S Yu, and Wei Chen. 2018. Multi-round
influence maximization. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2249â€“2258.
[30] Youze Tang, Yanchen Shi, and Xiaokui Xiao. 2015. Influence Maximization
in Near-Linear Time: A Martingale Approach. In Proceedings of the 2015 ACM
SIGMOD International Conference on Management of Data. 1539â€“1554.
[31] Youze Tang, Xiaokui Xiao, and Yanchen Shi. 2014. Influence maximization: Near-
optimal time complexity meets practical efficiency. In Proceedings of the 2014
ACM SIGMOD International Conference on Management of Data. 75â€“86.
[32] Michal Valko, Nathaniel Korda, RÃ©mi Munos, Ilias Flaounas, and Nelo Cristian-
ini. 2013. Finite-time analysis of kernelised contextual bandits. In Proc. of the
Uncertainty in Artificial Intelligence (UAI).
[33] Sharan Vaswani, Branislav Kveton, Zheng Wen, Mohammad Ghavamzadeh, Laks
V. S. Lakshmanan, and Mark Schmidt. 2017. Model-Independent Online Learn-
ing for Influence Maximization. In Proc. of the 34th International Conference on
Machine Learning. 3530â€“3539.
[34] Sharan Vaswani, Laks Lakshmanan, and Mark Schmidt. 2015. Influence maxi-
mization with bandits. arXiv preprint arXiv:1503.00024 (2015).
[35] Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. 2017. Online
influence maximization under independent cascade model with semi-bandit
feedback. Advances in Neural Information Processing Systems 30 (2017).
[36] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In Proc. of the
International Conference on Machine Learning. PMLR, 6861â€“6871.
[37] Qingyun Wu, Zhige Li, Huazheng Wang, Wei Chen, and Hongning Wang. 2019.
Factorization bandits for online influence maximization. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
636â€“646.
[38] Tom Zahavy and Shie Mannor. 2020. Deep neural linear bandits: Overcoming
catastrophic forgetting through likelihood matching. In International Conference
on Learning Representations.
[39] Jing Zhang, Biao Liu, Jie Tang, Ting Chen, and Juanzi Li. 2013. Social Influence
Locality for Modeling Retweeting Behaviors. In Proc. of the International Joint
Conference on Artificial Intelligence (IJCAI).
[40] Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. 2021. Neural
Thompson sampling. International Conference on Learning Representation (2021).
[41] Dongruo Zhou, Lihong Li, and Quanquan Gu. 2020. Neural contextual bandits
with UCB-based exploration. In Proc. of the International Conference on Machine
Learning. 11492â€“11502.
 
780Influence Maximization via Graph Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A SUPPLEMENTARY COMPLEXITY
EXPERIMENTS
We provide in Table 3 a comparison on running time (in hours) w.r.t.
the number of clustering groups ğ‘šâ€²whenğ¿=2. We can observe
that a finer granularity (more groups) may not result in better per-
formance (as shown in Fig. 4), while cost goes up exponentially. We
chose 50 groups that represents a good tradeoff between accuracy
and complexity in our experiments.
Table 3: Results for running time.
ğ‘šâ€²2
5 10 20 50 100 150 200 250
running
time 5.35 7.02 9.71 12.35 41.43 125.49 225.67 391.54 735.35
B ANALYSIS ON THE NUMBER OF
CLUSTERING GROUPS AND FOR
ARTIFICIAL EXPLORATION
Here, we present the remaining figures (Figs. 4 and 5) that were
mentioned in the main text but omitted due to space considerations.
They pertain to the analysis of the number of clustering groups
and for artificially boosted exploration.
Fig. 4 shows that the campaign performance improves as the
number of clusters increases from 2to150at the beginning. How-
ever, beyond 200clusters, performance begins to decline. This de-
cline can be likely attributed to insufficient data within each cluster
for effective learning under the constraints of a limited rounds
budget.
Fig. 5 confirms the effectiveness of artificially augmenting / boost-
ing the exploration score of the unchosen arms with zero reward
outcomes, in order to increase the likelihood of exploring alterna-
tive arms.
2 5 10 20 50 100 150 200 250
#clusters3000031000320003300034000cumulative spreadT witter - L=2 - round=500Figure 4: Analysis on the number of clustering groups.
0 100 200 300 400 500
rounds050001000015000200002500030000cumulative spreadT witter - L=2
explore
non-artifical
artificial
Figure 5: Analysis on artificially boosted exploration.
 
781