LARP: Language Audio Relational Pre-training for Cold-Start
Playlist Continuation
Rebecca Salganikâˆ—â€ 
University of Rochester
Rochester, NY, United States
rsalgani@ur.rochester.eduXiaohao Liuâˆ—
National University of Singapore
Singapore, Singapore
xiaohao.liu@u.nus.eduYunshan Maâ€¡
National University of Singapore
Singapore, Singapore
yunshan.ma@u.nus.edu
Jian Kang
University of Rochester
Rochester, NY, United States
jian.kang@rochester.eduTat-Seng Chua
National University of Singapore
Singapore, Singapore
dcscts@nus.edu.sg
ABSTRACT
As online music consumption increasingly shifts towards playlist-
based listening, the task of playlist continuation, in which an al-
gorithm suggests songs to extend a playlist in a personalized and
musically cohesive manner, has become vital to the success of music
streaming services. Currently, many existing playlist continuation
approaches rely on collaborative filtering methods to perform their
recommendations. However, such methods will struggle to rec-
ommend songs that lack interaction data, an issue known as the
cold-start problem. Current approaches to this challenge design
complex mechanisms for extracting relational signals from sparse
collaborative signals and integrating them into content represen-
tations. However, these approaches leave content representation
learning out of scope and utilize frozen, pre-trained content models
that may not be aligned with the distribution or format of a specific
musical setting. Furthermore, even the musical state-of-the-art con-
tent modules are either (1) incompatible with the cold-start setting
or (2) unable to effectively integrate cross-modal and relational
signals. In this paper, we introduce LARP, a multi-modal cold-start
playlist continuation model, to effectively overcome these limita-
tions. LARP is a three-stage contrastive learning framework that
integrates both multi-modal and relational signals into its learned
representations. Our framework uses increasing stages of task-
specific abstraction: within-track (language-audio) contrastive loss,
track-track contrastive loss, and track-playlist contrastive loss. Ex-
perimental results on two publicly available datasets demonstrate
the efficacy of LARP over uni-modal and multi-modal models for
playlist continuation in a cold-start setting. Finally, this work pio-
neers the perspective of addressing cold-start recommendation via
âˆ—Equal contribution.
â€ This work was mainly done while Rebecca was visiting the National University of
Singapore.
â€¡Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671772relational representation learning. Code and dataset are released at:
https://github.com/Rsalganik1123/LARP.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems; â€¢Applied
computingâ†’Sound and music computing.
KEYWORDS
music playlist continuation, music representation learning, language-
audio pre-training, cold-start problem
ACM Reference Format:
Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, and Tat-Seng Chua.
2024. LARP: Language Audio Relational Pre-training for Cold-Start Playlist
Continuation. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671772
1 INTRODUCTION
In recent years, the rise of music streaming platforms has created
significant changes in the orientation of music listening practices
[47]. Most notably, the transition from physical to digitized music
has manifested itself in the dominance of playlist-based listening
over, the previously prevalent, album-based listening [ 4,17,28,43].
In the general sense, a playlist can be considered as a collection of
tracks (songs1), intended to be listened to in sequential order and
unified under some underlying theme such as genre, mood, activity,
aesthetic, or other cohesive factor [ 5,14,63]. This radical shift
in consumption habits has highlighted the significance of playlist
continuation, a task which requires an algorithmic curatorial system
to augment a list of seed tracks with musically cohesive suggestions
to complete a playlist [4, 63].
To date, a large portion of approaches for playlist continuation
rely on collaborative filtering (CF), which leverages user-item in-
teractions to learn user and item representations. However, pure
collaborative filtering hardly addresses the cold-start problem, where
a lack of interactive data hinders the ability of a system to gener-
ate high-quality item or user representations and offer meaning-
ful suggestions [ 3,32,46,54,56â€“58,65,66]. Broadly, solutions to
1Track and song are used interchangeably in this paper.
 
2524
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, & Tat-Seng Chua
Within Track ContrastTrack-Track ContrastTrack-Playlist ContrastL1L3
Audio Enc.Text Enc.Uni-modal Encoder
Multi-modal Encoder
RelationAware
L2updateContinuation ModelÎ¨inference(b) LARP (ours)(a) Current ApproachesContinuation Model Î¨Continuation Model Î¨Continuation Model Î¨
Figure 1: Comparison of cold-start approaches. Current cold-
start learning methods (top) delegate content representation
to a pre-trained model and focus on relational extraction.
However, LARP (bottom) simultaneously focuses on rela-
tional and cross-modal learning, addressing both the limita-
tions of previous content models and enhancing the perfor-
mance of downstream cold-start frameworks.
the cold-start problem require methods which infuse a set of con-
tent representations with informative relational signals, enabling
a model to later generalize to unseen users and/or items. Existing
works approach the cold-start problem by designing complex sys-
tems that constrain [ 32,46], align [ 61], or reconstruct [ 3,18,48,56]
relational signals and use them to enhance a set of pre-computed
content representations. Thus, central to the success of these meth-
ods is the quality of the representations generated by their se-
lected content module. Crucially, these approaches leave the gener-
ation of their content representations out of scope, delegating fea-
ture extraction to a frozen, pre-trained content extraction module.
Meanwhile, music and audio representation learning has presented
a wealth of content-based feature extractors which can serve as
content-backbones in the aforementioned cold-start approaches.
These methods encompass uni-modal [ 9,29,30,38,41,49,51,59],
language-audio [ 23,26,37,60], and relational approaches [ 1,21,27].
The state-of-the-art (SOTA) in music representation learning
have several key limitations with regard to the task of playlist con-
tinuation in a cold-start setting: either they (1) rely on collaborative
filtering approaches to extract relational signals and are therefore
incompatible with the cold-start setting due to a reliance on inter-
active data being present during inference or(2) they are unable
to effectively integrate cross-modal and relational signals. Given
our setting, in which inference is performed over a set of unseen
playlists and tracks, there is an urgent need for comprehensive
feature extraction methods that combine both relational and multi-
modal learning but do not rely on collaborative filtering methods
to do so.In this work, we present a fundamental paradigm shift, trans-
ferring the focus of cold-start learning away from the complicated
extraction of realtional signals and towards the training of relational
awareness within a multi-modal content module. We argue that the
quality of the representations learned by a content module poses a
significant bottleneck for cold-start learning. To this end, we intro-
duceLARP, a Language Audio Relational Pre-training model, which
effectively overcomes the limitations of previous SOTA music rep-
resentation learning methods. LARP approaches the task of playlist
continuation through a three-layered contrastive learning frame-
work that integrates both multi-modal and relational signals into
its learned representations with increasing layers of task-specific
abstraction. We extend the canonical contrastive learning frame-
work and introduce the concept of multi-stage contrastive learning
by designing cross-modal contrastive loss functions for within-
track, track-track and track-playlist pairings. Our approach cen-
ters around three key loss functions: (L1) Within-Track Contrastive
Loss, (L2) Track-Track Contrastive Loss, and (L3) Track-Playlist Con-
trastive Loss, visualized in Figure 1. First (L1), individual tracks are
passed through an language-audio encoder, which is trained by
enforcing alignment between their respective textual and audio
embeddings. Then (L2) these representations are further refined by
aligning the representations between pairs of tracks which share
a parent playlist. In the final layer (L3), we generate representa-
tions for playlists enforcing alignment between a subset of tracks
within a playlist. In order to show the superiority of our methodol-
ogy, we augment two publicly available music datasets to include
audio samples and meta-data based textual annotations as multi-
modal sources of information. Extensive experiments show that
our method outperforms both uni-modal and multi-modal models
to achieve impressive results on the cold-start playlist continuation
task.
In summary, the main contributions of this paper are as follows:
â€¢Problem Definition. We argue that the quality of the represen-
tations learned by a content module poses a significant bottleneck
for cold-start learning and address this challenge through rela-
tional pre-training.
â€¢Model Design. We propose LARP, which is the first multi-
modal relational pre-training framework that uses multi-stage
contrastive learning to generate embeddings.
â€¢Experimental Results. We augment two canonical music rec-
ommendation datasets with audio waveforms and their meta-data
based textual annotation counterparts. Then, through extensive
experimentation and analysis, we demonstrate the effectiveness
of our method in addressing the needs of cold-start playlist con-
tinuation
2 PROBLEM FORMULATION
Given a set of tracks, S={ğ‘ 1,ğ‘ 2,Â·Â·Â·,ğ‘ ğ‘}, each track ğ‘ includes
an audio input ğ‘ğ‘ and a text input ğ‘¡ğ‘ , which could be the de-
scription about its genre, artist, or title etc.(For the exact for-
mulations of our own audio and textual inputs, please see Sec-
tion 4.1). We define a playlist as a set2of tracks, denoted as ğ‘=
2Please note: in our setting we do not consider the sequential nature of playlists. Rather,
we leave this characteristic for future work and simplify a playlist to be an unordered
set of tracks.
 
2525LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
{ğ‘ 1,ğ‘ 2,Â·Â·Â·,ğ‘ |ğ‘|}, where|ğ‘|is the size of the playlist. Typically, we
are given a collection of playlists as the training set, denoted as
P={ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘€}and a collection of playlists for testing, de-
noted as Â¯P={Â¯ğ‘ğ‘€+1,Â¯ğ‘ğ‘€+2,Â·Â·Â·,Â¯ğ‘ğ‘€+Â¯ğ‘€}, whereğ‘€is the size of the
training set and Â¯ğ‘€is the size of the testing set. Associated with
each playlist set is a collection of tracks that correspond to the
playlists in the training set, S={ğ‘ 1,ğ‘ 2,Â·Â·Â·,ğ‘ ğ‘}, and testing set,
Â¯S={Â¯ğ‘ ğ‘+1,Â¯ğ‘ ğ‘+2,Â·Â·Â·,Â¯ğ‘ ğ‘+Â¯ğ‘}. The interactions between these tracks
and their parent playlists, are represented by a bipartite graph
Rğ‘€Ã—ğ‘={ğ‘Ÿğ‘ğ‘ |ğ‘âˆˆP,ğ‘ âˆˆS} , whereğ‘Ÿğ‘ğ‘ = 1ifğ‘ âˆˆğ‘andğ‘Ÿğ‘ğ‘ = 0
otherwise. Based on the track-playlist affiliation relations in R, we
further define a track-track co-occurrence relation between every
pair of tracks which share a parent playlist, forming a homogeneous
graph Oğ‘Ã—ğ‘={ğ‘œğ‘ ğ‘–ğ‘ ğ‘—|ğ‘ ğ‘–âˆˆS,ğ‘ ğ‘—âˆˆS} ,ğ‘œğ‘ ğ‘–ğ‘ ğ‘—= 1if the tracks ğ‘ ğ‘–and
ğ‘ ğ‘—belong to the same playlist, and ğ‘œğ‘ ğ‘–ğ‘ ğ‘—= 0otherwise.
â€¢Playlist Continuation: Given a test playlist, Â¯ğ‘ğ‘âŠ‚Â¯ğ‘âˆˆÂ¯P,
consisting of ğ‘seed test tracks, ğ½={Â¯ğ‘ ğ‘—:Â¯ğ‘ ğ‘—âˆˆÂ¯ğ‘ğ‘}such that
|ğ½|<|Â¯ğ‘|, the task of playlist continuation aims to predict the
missing tracks Â¯ğ‘ âˆˆÂ¯ğ‘\ğ½.
â€¢Cold-start Playlist Continuation: In this work, we define
the cold-start setting to mean that there is no overlap between
the tracks and playlists in the training and testing sets. Thus,
Sâˆ© Â¯S=âˆ…andPâˆ© Â¯P=âˆ…3. Here, the set of training tracks, Shas
size,ğ‘and the set of testing tracks, Â¯S={Â¯ğ‘ ğ‘+1,Â¯ğ‘ ğ‘+2,Â·Â·Â·,Â¯ğ‘ ğ‘+Â¯ğ‘},
of size, Â¯ğ‘, is associated with the testing playlists, Â¯P.
3 METHODOLOGY
We introduce LARP, a novel language audio relational pre-training
method, targeted at extracting track representations that are effec-
tive and generalizable to various methods for cold-start playlist
continuation. We first present the model architecture of LARP,
followed by the multi-stage pre-training process, and finally we
describe how to employ LARP for cold-start playlist continuation
in tandem with several canonical cold-start frameworks.
3.1 Model Architecture
In order to generate audio and text embeddings for each track in our
track dataset, we design a multi-modal architecture that is based
on the BLIP framework [ 33]. As shown in Figure 2, our method
consists of two uni-modal encoders: one for audio and the other
for text. For more extensive details on the formatting of out audio
and textual inputs, please see Section 4.1.
Following a similar architecture with [ 60], we select HT-SAT [9],
a spectral transformer, for our audio encoding and Bert [ 12], a lan-
guage transformer, for our textual encoding. In our experimentation,
we tried several other audio backbones such as AST [ 24] and PANN
[30] however each of these had specific drawbacks. First, similarly to
[60], we found that the convolutional architecture of the PANN [ 30]
model did not achieve high performance on the audio encoding task.
Second, due to the computational complexity of the AST [ 24] model,
we found that the number of parameters significantly limited the
batch size that we were able to work with, thus deteriorating perfor-
mance. As such, we use the HT-SAT, a lighter modified version of
AST [ 24], to generated our audio representations and leave further
3Please note: that throughout this paper, we use Â¯ğ‘ âˆˆÂ¯S,Â¯ğ‘âˆˆÂ¯Pto distinguish playlists
and tracks in the test set.backbone analysis for future works. Thus, given a track, ğ‘ ğ‘–, each
uni-modal encoder outputs a representation associated with the
audio and textual caption of the track, ağ‘ ğ‘–âˆˆR1Ã—768,tğ‘ ğ‘–âˆˆR1Ã—768,
respectively. These are then projected into a unified space where
they are represented as ağ‘ ğ‘–âˆˆR1Ã—256,tğ‘ ğ‘–âˆˆR1Ã—256. In addition to
the uni-modal encoders and their cross-modal alignment layers,
our architecture includes a momentum encoder which is used to
store track representations from previous epochs that can be used
during the contrastive loss. This design choice is motivated by the
substantial computational cost of generating and optimizing track
representations, and we follow the previous practice in BLIP [ 33]
for our implementation of the momentum encoder. This encoder
can be seen as a queue which caches representations of tracks from
the forward pass of each batch. Each time, a new representation is
generated, it is integrated into the queue using a first-in first-out
manner. As we explain in the subsequent Section 3.2, this cache is
used for sampling negative examples during the optimization of
the WTC and TTC losses.
3.2 Multi-stage Pre-training
In order to train our model, we design three contrastive loss varia-
tions WTC, TTC, and TPC, each associated with an increasing level
of task abstraction. The three losses are integrated via three consec-
utive stages of training, in which the weights from a previous stage
are transferred to the next. We begin by detailing them individually
and then explain the paradigm used for their combination. For a
visualization, please see Figure 2.
3.2.1 (L1) WTC: Within-Track Contrastive Loss. We borrow
the ITC, Image-Text Contrast, loss formulation from the original
BLIP [ 33] framework and use it for Within-Track Contrastive loss.
The motivation behind this loss is grounded in the improved gener-
alization that contrastive loss can provide [ 1,38,60]. Thus, for our
first layer of representation learning, we formally define this loss
as:
â„“WTC
ğ‘ ğ‘–=Contrast(a ğ‘ ğ‘–,tğ‘ ğ‘–), (1)
whereğ‘ ğ‘–is a track in the training set and ağ‘ ğ‘–,tğ‘ ğ‘–are the audio and
textual embeddings which are extracted by our multi-modal en-
coder Î¦(Â·). Here, the Contrast (Â·,Â·)function follows the formulation
proposed by [ 34]. More concretely, for an input track ğ‘ ğ‘–, we calcu-
late the softmax-normalized audio-to-text (a2t) and text-to-audio
(t2a) similarity as:
Ë†ğ‘¦a2t
ğ‘–=exp(sim(ağ‘ ğ‘–,tğ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘—=1exp(sim(ağ‘ ğ‘–,tğ‘ ğ‘—)/ğœ),
Ë†ğ‘¦t2a
ğ‘–=exp(sim(tğ‘ ğ‘–,ağ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘—=1exp(sim(tğ‘ ğ‘–,ağ‘ ğ‘—)/ğœ),(2)
where sim(Â·) is the cosine similarity function, ğœis a temperature
hyper-parameter, and ğ‘ ğ‘—are the negative samples, sampled from the
momentum queue. Then, given the one-hot ground-truth similarity
vectors,ğ‘¦a2t
ğ‘–,ğ‘¦t2a
ğ‘–âˆˆ{0,1}, the contrastive loss is defined as the
cross entropy CE(Â·)between the predicted similarities Ë†ğ‘¦a2t
ğ‘–,Ë†ğ‘¦t2a
ğ‘–and
the ground truths ğ‘¦a2t
ğ‘–,ğ‘¦t2a
ğ‘–. Such that:
Contrast(a ğ‘ ğ‘–,tğ‘ ğ‘–) =1
2[CE(ğ‘¦a2t
ğ‘–,Ë†ğ‘¦a2t
ğ‘–) +CE(ğ‘¦t2a
ğ‘–,Ë†ğ‘¦t2a
ğ‘–)]. (3)
 
2526KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, & Tat-Seng Chua
ğ‘ !
ğ‘"!ğ‘¡"!ğ’‚"!ğ’•"!Audio Enc.Text Enc.ğ‘ #
ğ‘""ğ‘¡""ğ’‚""ğ’•""L1:	â„“!"#â€¦â€¦â€¦â€¦Track lookup tableğ‘$ğ’‚%#ğ’•%#Self-Att.L3:â„“"$#
suggestedtracksÌ…ğ‘$â€¦â€¦Continuation ModelÎ¨Multi-stage Pre-training: LARPCold-startPlaylist ContinuationL2:â„“""#Audio Enc.Text Enc.
ğ%#
Figure 2: Overview of LARP. During training (left), LARP uses three stages of contrastive loss. In the first stage of training,
LARP performs Within-Track Contrastive Loss (WTC) on the text, audio input from a single track. In the second stage of
training, LARP performs Track-Track Contrastive Loss (TTC) on the text, audio pairs of co-occurring songs. In the final stage,
LARP performs Track-Playlist Contrastive Loss (TPC) on the text, audio pair of a playlist and its child track. During testing
(right), LARP is used to generate track embeddings which are pooled to create playlist representations and used as content-input
to a downstream cold-start framework to perform playlist continuation.
3.2.2 (L2) TTC: Track-Track Contrastive Loss. In the second
stage of training, we refine the representations learned from the
WTC loss by applying contrastive loss to align the modalities be-
tween pairs of tracks. We refer to this loss as TTC, or Track-Track
Contrastive loss. The purpose of this loss is to integrate relational
signals into the learned representations of individual songs. This
layer of contrast allows the model to learn essential information
that can be used to unify songs in both the language and audio
representational spaces. For two tracks ğ‘ ğ‘–andğ‘ ğ‘—, which share a
parent playlist ğ‘ğ‘˜, we can define this loss as the following:
â„“TTC
ğ‘ ğ‘–,ğ‘ ğ‘—=1
2[Contrast(a ğ‘ ğ‘–,tğ‘ ğ‘—) +Contrast(t ğ‘ ğ‘–,ağ‘ ğ‘—)], (4)
where the Contrast (Â·,Â·)loss is similar with Equation 1, but, we
change the contrast pair from single track to two different tracks.
The negative samples are also looked up from the momentum en-
coder. For the full equation, please refer to Appendix A.
3.2.3 (L3) TPC: Track-Playlist Contrastive Loss. Finally, given
that the ultimate goal of automatic playlist continuation requires
matching between playlists and tracks, we build on the previous
loss function to develop a contrastive loss between a track, ğ‘ ğ‘–, and
its parent playlist, ğ‘ğ‘˜. The purpose of this loss is to create align-
ment between clusters of tracks that share a parent playlist, thus
improving the neighbourhood awareness of a learned represen-
tation. In order to achieve this level of alignment, we design the
TPC, short for Track-Playlist Contrastive, loss. Thus, given a cen-
tral track,ğ‘ ğ‘–, its parent playlist, ğ‘ğ‘˜, and a set of neighbour tracks,
ğ½={ğ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ‘ğ‘˜\ğ‘ ğ‘–}, we define the loss as follows:
â„“TPC
ğ‘ğ‘˜,ğ‘ ğ‘–=1
2[Contrast(a ğ‘ğ‘˜,tğ‘ ğ‘–) +Contrast(t ğ‘ğ‘˜,ağ‘ ğ‘–)], (5)
where ağ‘ğ‘˜,tğ‘ğ‘˜âˆˆR1Ã—256are the playlist audio and text representa-
tions, respectively which are obtained by aggregating over a series
ofğ½seed tracks associated with each playlist using the Equation 6
below.
TPC-fusion. We experiment with several aggregation functions in
order to generate a playlist embeddings, (ağ‘ğ‘˜,tğ‘ğ‘˜). While a simpleaverage would be the most computationally efficient, in practice,
this method has some limitations. Primarily, it is unable to account
for outliers in a track collection. For example, in a diverse playlist,
it is difficult to guarantee that all of the selected tracks will be
aligned within the same theme. Thus, using a simple average as
an aggregator can cause the noise in the learned representation of
a playlist, and significantly mislead the optimization of the TPC
objective. To address this problem, we propose a fusion module
to guide the playlist representation during training4. Specifically,
we employ one simple self-attention layer on top of the set of
track representations to automatically weigh the tracks within a
playlist. Thereby, the outlier or noisy tracks would be assigned
lower weights when forming the playlist representation. Formally,
given a playlist ğ‘ğ‘˜, its representation is calculated by:
ağ‘ğ‘˜=Self-Att({aâ€²
ğ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ½}),
tğ‘ğ‘˜=Self-Att({tâ€²
ğ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ½}),(6)
whereğ½={ğ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ‘ğ‘˜\ğ‘ ğ‘–},Self-Att({Â·Â·Â·} )denotes a one-layer
self-attention network, ağ‘ğ‘˜andtğ‘ğ‘˜are the audio and text represen-
tations forğ‘ğ‘˜, and aâ€²ğ‘ ğ‘—andtâ€²ğ‘ ğ‘—are the audio and text representations
of tracks within ğ‘ğ‘˜(excludingğ‘ ğ‘–), which are drawn from the non-
differentiable track representation lookup dictionary AËœğ‘†andTËœğ‘†.
We note that this track representation lookup table differs from the
momentum encoder used in the previous WTC and TTC losses. This
is because the momentum encoder has limited capacity and cannot
store all of the tracks, which could skew the results of the playlist ag-
gregation. To address this issue, we leverage two non-differentiable
lookup tables to cache the latest audio and text representations
for all the tracks during training, denoted as AËœğ‘†âˆˆRğ‘Ã—256and
TËœğ‘†âˆˆRğ‘Ã—256. Both tables are updated during training such that
when a track appears in a batch their representations in AËœğ‘†and
TËœğ‘†will be replaced by the new representations generated by the
4Please note that the fusion module is not used to generate playlist representations
during testing, i.e.,playlist continuation, only training.
 
2527LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
forward pass. For a more detailed breakdown to this loss function
and its calculation procedures, please refer to Appendix A.
Multi-stage Training. We train the model in three stages, using
early stopping to achieve the optimal performance for each loss and
before proceeding to the following stage. The weights from each
intermediate stage are transferred between stages and the model
remains unchanged. Only the loss function is modified to integrate
each increasingly abstract loss. Thus, for a given track ğ‘ ğ‘–and its
affiliated playlist ğ‘ğ‘˜, and neighbour set,{ğ‘ ğ‘—:ğ‘ğ‘˜=ğ‘(ğ‘ ğ‘—),ğ‘ ğ‘—Ì¸=ğ‘ ğ‘–}, we
define the stages as:
Stage 1:â„“1=â„“WTC
ğ‘ ğ‘–; (7)
Stage 2:â„“2=â„“WTC
ğ‘ ğ‘–+â„“TTC
ğ‘ ğ‘–,ğ‘ ğ‘—; (8)
Stage 3:â„“3=â„“WTC
ğ‘ ğ‘–+â„“TTC
ğ‘ ğ‘–,ğ‘ ğ‘—+â„“TPC
ğ‘ğ‘˜,ğ‘ ğ‘–. (9)
Currently, we just use a simple average over the three losses, which
already demonstrates excellent performance. We leave more com-
plex balances between the three loss functions for future work.
3.3 Cold-start Playlist Continuation
After the pre-training stage, we obtain the model Î¦(Â·)for cold-start
playlist continuation task. Specifically, we first generate the text
and audio representations:
ağ‘ ğ‘–,tğ‘ ğ‘–= Î¦([ğ‘ğ‘ ğ‘–,ğ‘¡ğ‘ ğ‘–]), (10)
whereğ‘ğ‘ ğ‘–andğ‘¡ğ‘ ğ‘–stand for the audio and text input of the track ğ‘ ğ‘–,
respectively. Analogous to the pre-training, we employ a simple
average pooling to obtain its unified representation eğ‘ ğ‘–âˆˆR1Ã—256,
formally represented as:
eğ‘ ğ‘–=mean(ağ‘ ğ‘–,tğ‘ ğ‘–). (11)
Thus, we can obtain the embedding table for all tracks in training
setSasESâˆˆRğ‘Ã—256, and testing set Â¯SasEÂ¯SâˆˆRÂ¯ğ‘Ã—256.
Following the typical setting in playlist continuation [ 47,63], we
generate the playlist representations eğ‘andeÂ¯ğ‘ğ‘by simply average
pooling its included tracksâ€™ representations, denoted as:
eğ‘=mean({eğ‘ ğ‘–:ğ‘ ğ‘–âˆˆğ‘}),eÂ¯ğ‘ğ‘=mean({eÂ¯ğ‘ ğ‘–:Â¯ğ‘ ğ‘–âˆˆÂ¯ğ‘ğ‘}),(12)
where eğ‘andeÂ¯ğ‘ğ‘are used for training and testing, respectively.
And the cold-start playlist continuation task aims to predict the
missing tracks Â¯ğ‘ âˆˆÂ¯ğ‘\Â¯ğ‘ğ‘.
In order to showcase the ability of LARP to learn effective and
generalizable representations, we integrate the representations gen-
erated by Equations 10 and 12 with three cold-start playlist continu-
ation methods: ItemKNN [ 45], DropoutNet [ 56], and CLCRec [ 58]. It
should be noted that these methods are originally designed for cold-
start recommendation and we adapt them to the task of cold-start
playlist continuation (please see B for more details).
â€¢ItemKNN [45] is a parameter-free approach which relies on
the cosine similarity between the input partial playlist eÂ¯ğ‘ğ‘and
all the candidate tracks EÂ¯S, to find the ğ¾most similar tracks.
â€¢DropoutNet [56] is designed to treat the cold-start as a robust-
ness task in which sparse interactive signals are reconstructed
using content features. It adopts WMF [ 25] to generate pref-
erence representations for playlists and tracks, and randomly
applies dropout to a training set of preference-content concate-
nations, which are fed into a DNN model for score predictions.â€¢CLCRec [58] is a contrastive learning framework that recon-
structs collaborative signals from the provided content embed-
dings. And it also uses inner product to compute the similarities,
then to find the ğ¾possible tracks in the Â¯ğ‘\Â¯ğ‘ğ‘.
4 EXPERIMENTS
To guide our analysis, we center our experimentation around an-
swering the following research questions:
RQ1. How does our proposed method perform in comparison with
our selected benchmarks on the task of playlist continua-
tion?
RQ2. How does each block of our proposed framework contribute
to the final performance of our method?
RQ3. What are the key properties of our method that make it well
suited to the task of cold-start playlist continuation?
4.1 Experimental Settings
4.1.1 Datasets. We evaluate LARP on The Million Playlist Dataset
(MPD) [ 8] and the Last-FM (LFM) [ 40] datasets (Table 1). Our moti-
vation for selecting these datasets is grounded in two key require-
ments of our setting: (1) direct access to audio samples and metadata,
and (2) inclusion of track-based interactions. While there are several
playlist datasets available [ 22,39,52,64], none of them would give
us access to the audio samples. And, even though the Melon dataset
[22] has pre-computed spectrograms, the sampling frequency and
time duration of these spectrograms are not compatible with the
needs of our backbone audio extraction module.
We define our own procedure for augmenting both MPD and
LFM to include the language-audio pairs necessary for our multi-
modal pre-training methodology. For generating the audio samples,
we scrape all the associated MP3s and truncate them into 10 second
clips sampled randomly from the track (excluding the first and last
20 seconds). Then, to meet the size requirements of our backbone
audio model, HTS-AT [ 9], we truncate each MP3 file into 10 second
sound clips and upsample the audio quality to be 44.8kHz. To gen-
erate the captions, we use the associated Spotify IDs to collect the
track name, artist name, and album name. The captions are then for-
mulated using the following structure: The track <track name>
by<artist name> on album <album name> . We leave the
exploration of further captioning strategies for future work.
Finally, we acknowledge that LFM is not explicitly designed for
playlist continuation, so we make several modifications to align it
with our task. We draw inspiration from the notion of Weekly Dis-
covery. This service, which is available on many popular streaming
platforms [ 4,50], involves the creation of a playlist on the basis of
the aggregated listening habits over a user on a weekly basis. Unlike
static playlist continuation, in which seed tracks are collected from
the user inputs, Weekly Discovery uses implicit feedback from users
to identify the seed tracks used for recommendation. Thus, to create
a playlist continuation dataset from the user-track interaction data
provided by LFM, we perform the following steps:
(1)We randomly select a series of users for each split of training,
validation, and testing.
(2)We treat each user as a â€˜playlistâ€™ and aggregate all the songs
that they have listened to into a single collection.
 
2528KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, & Tat-Seng Chua
Table 1: Statistics of datasets with number of tracks (#S),
number of playlists (#P), and Homogeneity (Homo.). Homo-
geneity is defined as the average pairwise cosine similarity
among the audio embeddings of the tracks in a playlist.
DatasetTraining Evaluation Relevant Stats
#S #P #S Avg #S/P Avg #P/S Homo.
MPD 98,552 100 6,978 98.12 1.41 0.60
LFM 87,558 1,000 17,439 91.04 4.63 0.56
(3)We iteratively remove songs which overlap between the split
pools (i.e., songs in the training set are removed from the sub-
sequent validation and test sets).
(4)To construct our test set, we select a contiguous subset of in-
teractions for each user/playlist to contain more than 30 songs
and less than 100 for standardization between MPD and LFM.
4.1.2 Baseline methods. We compare LARP with four state-of-the-
art music representation learning baselines, including Jukebox [ 13],
MULE [ 38], CO-Playlist [ 1], and CLAP [ 60]. Please note that, due
to the cold-start nature of our dataset design, we do not include
any collaborative filtering baselines, which require interactive data
for representation learning.
â€¢Jukebox [13] is a generative music model with state-of-the-art
performance in various music understanding tasks [7].
â€¢MULE [38] is an audio representation learning model based on
convolutional neural network (CNN) [30].
â€¢CO-Playlist [1] is a contrastive learning model, in which posi-
tive pairs are selected through playlist-track interaction data.
â€¢CLAP-PT/ CLAP-FT [60]5is a multi-modal model. For the pre-
trained (PT) setting, we use the pre-trained model to generate
representations. For the fine-tuned (FT) setting, we fine-tune
before generating representations.
4.1.3 Hyper-parameter settings and reproducibility. We train LARP
in a distributed setting with two Nvidia A40 GPUs, each with 48GB
of memory. We train for 45epochs with early stopping that ter-
minates after 5epochs without improvement. We use a batch size
of 50 and use the Adam optimizer with ğ›½1= 0.9andğ›½2= 0.99.
Our learning rate of 1ğ‘’âˆ’4is controlled by a cosine scheduler after
a3000 step warm up period. Our final results are achieved with
an embedding size of 256. For our temperature parameter we use
ğœ= 0.07. Our momentum encoder has a queue of size 57600 and a
momentum parameter of ğ‘š= 0.995. For the hyperparameters used
in our cold-start frameworks, please see Appendix B.
4.2 Overall Performance Comparison (RQ1)
First, we compare our method with the various encoder benchmarks
across all the downstream recommendation frameworks. As we can
clearly see in Table 2, our method consistently outperforms all the
benchmarks across all metrics and their k values on both LFM and
MPD. This performance shows the consistency of our approach
over a variety of canonical metrics and cold-start music datasets.
Furthermore, we can see that LARP significantly out performs the
other relational representation learning benchmark, CO-Playlist [ 1].
5There is a concurrent work [ 19] that has the same name and similar idea with
CLAP [60] that we implemented in this paper.This indicates the superiority of our relational learning paradigm
and the value of adding textual inputs (which CO-Playlist does not
use). Also, comparing with one of the two strongest benchmarks,
MULE, we can see that even though MULE was trained on 1.8M
examples of high-quality, proprietary music data [ 38], our method
still performs better.
Second, we compare the performance of LARP, our method,
across the various recommendation frameworks. Here, we can see
that the difference between using a simple, non-parametrized rec-
ommendation model such as ItemKNN and a SOTA method such
as DropoutNet or CLCRec is relatively insignificant. This indicates
that LARPâ€™s exceptional performance can be achieved without re-
liance on a complex downstream recommendation model. This
important finding highlights the benefits that the integration of
cross-modal and relational signals can have in the cold-start setting.
Furthermore, the lack of significant gains between various frame-
works indicates that the key differentiating factor between these
methods lies in the quality of the learned content representations.
To us, this highlights the importance of considering the quality
of content module as part of a cold-start recommendation task as
compared to the downstream recommendation framework.
4.3 Ablation Study (RQ2)
To assess the effectiveness of each element in our proposed LARP
architecture, we iteratively remove each component and evaluate
the modelâ€™s performance. In Table 3 we present the results from
each of the unimodal backbone encoders, followed by iterations of
LARP in which we remove first the losses and then the fusion layer.
Figure 3: Within playlist alignment of tracks. Visualization of
tracks associated with individual playlists. Note, gray cloud
indicates the entire embedding set.
 
2529LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Effectiveness results on Recall@K (R@K) and NDCG@K (N@K). Best results in bold and second best with underscore .
Re
c. Model EncoderMPD Dataset LFM Dataset
R@10
N@10 R@20 N@20 R@40 N@40 R@10 N@10 R@20 N@20 R@40 N@40
ItemKNN [45]Jukeb
ox [13] 0.0012 0.0045 0.0025 0.0051 0.0055 0.0059 0.0005 0.0017 0.0008 0.0015 0.0021 0.0019
MULE [38] 0.0156 0.0798 0.0286 0.0738 0.0457 0.0609 0.0036 0.0140 0.0066
0.0131 0.0121 0.0133
CO-Playlist [1] 0.0012 0.0058 0.0022 0.0056 0.0067 0.0076 0.0008 0.0036 0.0016 0.0036 0.0024 0.0029
CLAP-PT [60] 0.0020 0.0099 0.0041 0.0099 0.0065 0.0084 0.0013 0.0061 0.0024 0.0054 0.0038 0.0047
CLAP-FT [60] 0.0217 0.1047 0.0386 0.0967 0.0682 0.0877 0.0033
0.0125 0.0067 0.0131 0.0132 0.0137
LARP (
Ours) 0.0406 0.1964 0.0753 0.1868 0.1306 0.1669 0.0137 0.0536 0.0233 0.0479 0.0389 0.0446
Dr
opoutNet [56]Jukebox [13] 0.0022 0.0106 0.0034 0.0090 0.0063 0.0082 0.0008 0.0040 0.0014 0.0035 0.0023 0.0030
MULE [38] 0.0286 0.1464 0.0534 0.1377 0.0861 0.1166 0.0066
0.0261 0.0131 0.0259 0.0236 0.0257
CO-P
laylist [1] 0.0016 0.0087 0.0034 0.0088 0.0065 0.0083 0.0011 0.0053 0.0017 0.0043 0.0030 0.0038
CLAP-PT [60] 0.0022 0.0114 0.0047 0.0117 0.0081 0.0105 0.0006 0.0031 0.0016 0.0034 0.0031 0.0034
CLAP-FT [60] 0.0283 0.1396 0.0531 0.1325 0.0888 0.1158 0.0070 0.0259
0.0128 0.0247 0.0210 0.0232
LARP (
Ours) 0.0418 0.2171 0.0784 0.2028 0.1375 0.1815 0.0146 0.0558 0.0245 0.0495 0.0417 0.0471
CLCRe
c[58]Jukebox [13] 0.0014 0.0078 0.0038 0.0092 0.0071 0.0088 0.0010 0.0036 0.0013 0.0029 0.0021 0.0025
MULE [38] 0.0294 0.1546 0.0521 0.1391 0.0878 0.1201 0.0087 0.0324 0.0168 0.0321 0.0310 0.0326
CO-P
laylist [1] 0.0026 0.0121 0.0041 0.0103 0.0069 0.0091 0.0009 0.0060 0.0015 0.0048 0.0023 0.0036
CLAP-PT [60] 0.0024 0.0111 0.0042 0.0103 0.0071 0.0091 0.0012 0.0053 0.0017 0.0042 0.0030 0.0038
CLAP-FT [60] 0.0240 0.1137 0.0446 0.1088 0.0814 0.1020 0.0076 0.0289 0.0141 0.0275 0.0241 0.0264
LARP (
Ours) 0.0416 0.2069 0.0779 0.1962 0.1330 0.1737 0.0151 0.0581 0.0259 0.0527 0.0423 0.0487
Table 3: Ablation study with ItemKNN [ 45] as the recommender. The last row is our final method, LARP-TPC-fusion. Each row
bfore is an ablated version of LARP without (w/o) one of the contrastive modules.
Baseline
EncoderMPD Dataset LFM Dataset
R@10
N@10 R@20 N@20 R@40 N@40 R@10 N@10 R@20 N@20 R@40 N@40
T
ext Bert [12] 0.0075 0.0415 0.0122 0.0351 0.0191 0.0285 0.0032 0.0128 0.0052 0.0109 0.0089 0.0104
Audio HTS-AT [9] 0.0012 0.0072 0.0031 0.0079 0.0071 0.0086 0.0006 0.0025 0.0011 0.0024 0.0023 0.0025
w/oâ„“TTCLARP-WTC 0.0256 0.1278 0.0444 0.1168 0.0779 0.1039 0.0057 0.0232 0.0104 0.0218 0.0201 0.0218
w/oâ„“TPCLARP-TTC 0.0373 0.1968 0.0724 0.1833 0.1243 0.1631 0.0141 0.0587 0.0256 0.0539 0.0425 0.0496
w/o fusion LARP-TPC 0.0357 0.1865 0.0641 0.1699 0.1132 0.1517 0.0094 0.0354 0.0171 0.0333 0.0286 0.0320
LARP-fusion LARP-TPC-fusion 0.0406 0.1964 0.0753 0.1868 0.1306 0.1669 0.0137 0.0536 0.0233 0.0479 0.0389 0.0446
First, we notice that each of the individual encoders has signifi-
cantly lower performance than the overall architecture, motivating
the need for the cross-attention layers and the WTC contrastive
loss that is used to unify the uni-modal encoders.
Second, we consider the fundamental architecture of LARP. Con-
trasting the performance of LARP-WTC with respect to its bench-
mark counterpart, ItemKNN/CLAP-FT in Table 2, we can see that
even without the remaining two losses, our method outperforms
this comparable multi-modal benchmark by a significant margin.
Third, we analyze the performance gains achieved by the in-
tegration of each loss. We can see that LARP-TTC, which begins
to integrate relational content via track-track alignment, already
shows noticeable improvement over the LARP-WTC which has
no relational awareness. This is because the â„“TTCloss enables the
model to account for acoustic or linguistic heterogeneity within
the tracks of a playlist. For example, if we consider two tracks from
different genres (e.g. Jazz and Country) that are both titled "Love",
a method which is unaware of their relation in a playlist would be
unaware of the unifying element (title) between them.Fourth, we explore the importance of our fusion layer and its
complementary â„“TPCloss. Simply the integration of a â„“TPCdoes not
improve the performance and actually achieves worse performance
than theâ„“TTCloss. This is because the effectiveness of the â„“TPC
loss hinges on the quality of the playlist representation that is used
to anchor the children tracks. Without the self-attention layer, the
model is unable to properly integrate information from a diverse set
of seed tracks into a meaningful playlist representation. However,
in adding this trained self-attention layer we are able to generate
expressive playlist encodings that contribute to the final success of
our LARP-TPC-fusion method.
Notably, we can see that there is a difference in which method
performs best between the two datasets. For MPD, we can see
that LARP-fusion clearly outperforms LARP-TTC. We attribute this
to the clear alignment between the MPD dataset setting (which
was clearly designed for the playlist continuation task) and the
formulation of the TPC loss that uses the fusion encoder layer.
Meanwhile, on LFM, we can see that the performance of LARP-
TTC supersedes that of LARP-fusion. This is due to the potential
misalignment between the LFM dataset and the goals of the TPC
 
2530KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, & Tat-Seng Chua
Table 4: Generalization results. We apply LARP, pre-trained
on MPD, and test its performance on LFM as compared with
best performing benchmarks (both trained and tested on
LFM). Best results in bold.
Enco
derTraining
DatasetLFM Results
R@10
N@10 R@20 N@20 R@40 N@40
MULE
LFM 0.0036 0.0140 0.0066 0.0131 0.0121 0.0133
CLAP-FT LFM 0.0033 0.0125 0.0067 0.0131 0.0132 0.0137
LARP-TPC-fusion MPD 0.0102 0.1311 0.0165 0.1178 0.0276 0.1079
loss. As we show in Table 1, LFM shows noticeably higher levels
of diversity in its tracks, thus it is possible that there is too much
heterogeneity among the representational spaces of the tracks,
making it difficult for a single attention layer to correctly capture a
meaningful playlist representation. However, the flexibility of our
approach allows for the selection of different variations that can be
tailored to the needs of a dataset.
Finally, we provide a case study in which we visualize the embed-
dings of various tracks in a playlist (see Figure 3) that are generated
by each method presented in the ablation table results above (Table
3). To create these plots, we randomly select an example playlist,
ğ‘, and its associated tracks, ğ‘†ğ‘={ğ‘ ğ‘–:ğ‘ ğ‘–âˆˆğ‘}. We generate em-
beddings for the playlist, ğ‘’ğ‘, and tracks, ğ‘’ğ‘†ğ‘={ğ‘’ğ‘ ğ‘–:ğ‘ ğ‘–âˆˆğ‘†ğ‘}by
Eqs. (11)and(12). We then apply 2-dimensional PCA on the set of
embeddings and plot the outcomes. This visualization is significant
in the context of the industrial setting where online recommen-
dations are served by selecting nearest neighbours from a large
scale embedding space representing an extremely large catalog. In
this setting, efficiency can be gained from having similar songs
tightly clustered in the overall embedding space. We show that
the LARP-TPC-fusion generates song embeddings that are most
closely clustered together in the embedding space which is aligned
with the purpose of the â„“TPCloss. By creating a playlist-track re-
lational awareness in our representation learning module, we are
able to identify clusters in unseen, held out evaluation tracks. Thus,
LARPâ€™s fine-grained representation learning framework facilitates
high performance from even a simple k-means recommendation
framework like ItemKNN.
4.4 Model Study (RQ3)
4.4.1 Generalization Performance. In addition to our evaluation
of unseen playlists and items from within the same dataset, we
also evaluate the generalization capabilities of LARP on completely
different datasets. In Table 4, we showcase the performance of the
best performing methods on the LFM dataset and evaluate against a
LARP model which has been trained on the MPD dataset. Crucially,
we can see that this generalization performance is incredibly strong,
even going so far as to out perform the best benchmark performance
on the LFM dataset. This is extremely important for the cold-start
setting, because it shows that our model is robust to the dataset
distribution - a gold standard in this setting.
4.4.2 Parameter Sensitivity. As we show in the ablation study, one
of the key components of our methodâ€™s impressive performance is
the integration of the TPC, or Track-Playlist Contrast, loss. Critical
to the success of this loss function is a comprehensive methodologyfor generating meaningful playlist representations that can be used
to properly anchor the various tracks within a playlistâ€™s collection.
1 5 10 30
#Seed Tracks0.0360.0380.040R@10
MPD
1 5 10 30
#Seed Tracks0.0130.0140.014
LFM
Figure 4: Sensitivity analysis. We show relationship between
performance and number of seed tracks, ğ½, for generating
playlist representations. Note: the red star signifies the best
results reported in the ablation tables above.
In our experimentation, we found that our model had some
sensitivity to ğ½, or the number of seed tracks which are used to
compute the playlist representations during the training stage. And,
most significantly, this parameter differs between datasets. As we
show in Figure 4, the best performance on the MPD dataset is
achieved by using 10 seed tracks for the generation of the playlist
representation by the fusion playlist encoder (defined in Equation
6). Meanwhile, in the LFM dataset, the best performance is achieved
by using 1 or 30 seed tracks. We believe that this can be attributed
to the difference in overall track diversity between the two datasets.
As shown in Table 1, LFM is generally less homogeneous than
MPD in its overall track collection. This is expected since the LFM
dataset is constructed over aggregate listening patterns while the
MPD dataset is localized to specific playlists. Thus, these results
can be attributed to the complexity of unifying a diverse set of
tracks into a single playlist representation. In LFM, we can either
control this complexity by significantly decreasing the number of
seed tracks or providing a large enough subset that our method can
find commonalities to anchor representations.
4.4.3 Convergence Analysis. The LARP model is trained in three
consecutive stages, where each stage adds an additional loss to
the training framework. In Figure 5, we showcase the trajectory
of performance that is yielded with the addition of each individ-
ual loss. The purpose of this plot is to showcase the significant
improvements that are achieved with the integration of relational
pre-training. Notably, we showcase the performance from the 5
epochs which follow the best performance to highlight that the
optimal performance has, indeed been achieved. As we can see
from these plots, on both datasets, there is a significant advantage
to adding relational awareness. Interestingly, we can see that in the
initial epochs of the â„“ğ‘‡ğ‘ƒğ¶ training, there is a drop in performance.
This can be attributed to the training of the self-attention layers,
which are initialized with random values, and slowly tuned over
the subsequent epochs of LARP-fusion training. We note, that as
mentioned in earlier sections, we can see that for MPD, which is
specifically designed for the playlist continuation task, there are
more advantages to the integration of TPC loss than for LFM. How-
ever, we believe this is expected given the high diversity of the LFM
dataset.
 
2531LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 10 20 30 40
Epoch0.020.04R@10
MPD
0 10 20 30
Epoch0.0000.0050.010
LFM
LARP-WTC LARP-TTC LARP-TPC
Figure 5: Convergence analysis of LARP. Grey, vertical
dashed line signifies the end of one training stage.
5 RELATED WORK
In this section, we review related works to the cold start problem
andmusic understanding.
5.1 Cold-Start Recommendation
The issue of cold-start has been an important focus within the
recommendation domain [ 3,32,46,54,56â€“58,65,66]. Many ap-
proaches in the field borrow from the domain of context based
learning for handling partial or sparse inputs [ 3,65]. Alternatively,
many works randomly mask interactive signals [ 48,56,66]. Fi-
nally, another branch of work uses content-based signals as a form
of regularization or constraints during training [ 18,46]. More re-
cently, recent works have begun to integrate innovations from
other tasks into the cold-start challenge such as contrastive learn-
ing [ 58], meta-learning [ 16,31,61], and zero-shot or multimodal
learning [ 10,15,20,32,35]. Crucially, these methods differ from our
approach in that they are either using pre-trained representation
learning methods, single modality content representations, or limit
their cold-start formulation to include only cold-start users (while
maintaining a shared item pool).
Within the specific field of music recommendation, there have
been many works that consider the topic of cold/warm start recom-
mendation and playlist continuation tasks [ 2,6,44,53,55,62,63].
Notably, these works differ from ours in that they focus on either
cold-start tracks [ 11,53], users [ 6], or playlists [ 2,44,55,62,63] but
not both playlists and tracks (as our setting suggests). Furthermore,
to our knowledge, there have yet to be any approaches that design
methods that use both cross-modal and relational information to
achieve their results.
5.2 Music Understanding
In recent years, the music information retrieval community has used
a variety of approaches to design music representation learning
models. It has become a very standard practice within to apply state
of the art vision models to images of the spectral content of audio
samples using either convolutional neural networks [ 30,37,41,42]
or transformer-based architectures [ 59,60] to learn representations.
In the last two years, innovations in vision-language tasks, have
also found success when applied to audio-language tasks. Most
notably, [ 23,26,36,37,60] have all borrowed architecture from
contrastive learning methods used in vision-language tasks and
applied them to audio by replacing the visual encoder with an audio
backbone model. Several works have also proposed the integration
of collaborative signals into their training paradigms [ 1,27,41]
by using various collaboration-informed heuristics for selectingpositive and negative pairs in their contrastive loss frameworks.
Notably, our work differs from those presented in that none of these
works integrate cross-modal andrelational signals without the use
of collaborative filtering.
6 CONCLUSION & FUTURE WORK
In this work, we proposed LARP, a multi-modal relation-aware
representation learning model for cold-start playlist continuation.
At the core of our approach, we argued that the true bottleneck
of cold-start learning lies in the quality of the content module. To
this end, LARP integrates multi-stage contrastive losses to learn
multimodal and relation-aware item embeddings. First, we com-
pared between the performance of LARP and a series of represen-
tation learning baseline methods, showing its superiority as an
encoder. Second, we evaluated LARP in tandem with several canon-
ical cold-start frameworks, showing that the quality of the embed-
dings produced by LARP significantly narrowed the gap between
complex, parametrized recommendation frameworks (DropOutNet,
CLCRec) and simple, unparameterized ones (ItemKNN). Finally, we
demonstrated our methodâ€™s generalization capabilities by perform-
ing cross-dataset evaluation.
We see many opportunities for future work. From the music
representation perspective, we envision many expansions of the
playlist encoding capacity of our model. Currently, the playlist en-
coder is not used during inference. With some refinement, it could
become an independent playlist encoder during inference. Addi-
tionally, our current track-playlist loss function does not integrate
sequential awareness, which, if included, could potentially improve
performance. Finally, while the application of this work is centered
around the music domain, we believe that the contributions and
outcomes of this work have significance for the broader domains
of multimodal representation learning and cold start recommenda-
tion. Here, we extract relational signal from the playlists, because
they are formed via explicit user feedback and thus represent re-
liable relational signals. However, the intuition behind these loss
functions is applicable to many other domains such as books, pod-
casts, or video and is thus a major contribution. The flexibility of
our relational training paradigm has potential for expansion into
a multi-headed encoder that generates album-level, playlist-level,
or even user-level representations each with its own dimension of
relational awareness. From the recommendation perspective, we
envision further exploration into the warm-start problem, where
we imagine that sparse interaction can be used to guide our re-
lational training paradigm to further refine its representational
qualities. Ultimately, we hope that our work demonstrates both the
importance of content representations in recommendation and the
power of encoding relational awareness into content modules.
ACKNOWLEDGEMENTS
This research/project is supported by the National Research Founda-
tion, Singapore under its Industry Alignment Fund â€“ Pre-positioning
(IAF-PP) Funding Initiative. Any opinions, findings and conclusions
or recommendations expressed in this material are those of the au-
thor(s) and do not reflect the views of National Research Foundation,
Singapore.
 
2532KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, & Tat-Seng Chua
REFERENCES
[1]Pablo Alonso-JimÃ©nez, Xavier Favory, Hadrien Foroughmand, Grigoris Bourdalas,
Xavier Serra, Thomas Lidy, and Dmitry Bogdanov. 2023. Pre-Training Strategies
Using Contrastive Learning and Playlist Information for Music Classification
and Similarity. In ICASSP. IEEE, 1â€“5.
[2]Sebastiano Antenucci, Simone Boglio, Emanuele Chioso, Ervin Dervishaj, Shuwen
Kang, Tommaso Scarlatti, and Maurizio Ferrari Dacrema. 2018. Artist-driven lay-
ering and userâ€™s behaviour impact on recommendations in a playlist continuation
scenario. In RecSys Challenge. ACM, 4:1â€“4:6.
[3]Oren Barkan, Noam Koenigstein, Eylon Yogev, and Ori Katz. 2019. CB2CF: a
neural multiview content-to-collaborative filtering model for completely cold
item recommendations. In RecSys. ACM, 228â€“236.
[4]Geoffray Bonnin and Dietmar Jannach. 2014. Automated Generation of Music
Playlists: Survey and Experiments. ACM Comput. Surv. 47, 2 (2014), 26:1â€“26:35.
[5]ThÃ©o Bontempelli, Benjamin Chapus, FranÃ§ois Rigaud, Mathieu Morlon, Marin
Lorant, and Guillaume Salha-Galvan. 2022. Flow Moods: Recommending Music
by Moods on Deezer. In RecSys. ACM, 452â€“455.
[6]LÃ©a Briand, Guillaume Salha-Galvan, Walid Bendada, Mathieu Morlon, and Viet-
Anh Tran. 2021. A Semi-Personalized System for User Cold Start Recommenda-
tion on Music Streaming Apps. In KDD. ACM, 2601â€“2609.
[7]Rodrigo Castellon, Chris Donahue, and Percy Liang. 2021. Codified audio lan-
guage modeling learns useful representations for music information retrieval. In
ISMIR. 88â€“96.
[8]Ching-Wei Chen, Paul Lamere, Markus Schedl, and Hamed Zamani. 2018. Recsys
challenge 2018: automatic music playlist continuation. In RecSys. ACM, 527â€“528.
[9]Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo
Dubnov. 2022. HTS-AT: A Hierarchical Token-Semantic Audio Transformer for
Sound Classification and Detection. In ICASSP. IEEE, 646â€“650.
[10] Szu-Yu Chou, Yi-Hsuan Yang, Jyh-Shing Roger Jang, and Yu-Ching Lin. 2016.
Addressing Cold Start for Next-song Recommendation. In RecSys. ACM, 115â€“118.
[11] Szu-Yu Chou, Yi-Hsuan Yang, Jyh-Shing Roger Jang, and Yu-Ching Lin. 2016.
Addressing Cold Start for Next-song Recommendation. In RecSys. ACM, 115â€“118.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL-HLT (1). Association for Computational Linguistics, 4171â€“4186.
[13] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Rad-
ford, and Ilya Sutskever. 2020. Jukebox: A Generative Model for Music. CoRR
abs/2005.00341 (2020).
[14] Ricardo Dias, Manuel J. Fonseca, and Ricardo Cunha. 2014. A User-centered
Music Recommendation Approach for Daily Activities. In CBRecSys@RecSys
(CEUR Workshop Proceedings, Vol. 1245). CEUR-WS.org, 26â€“33.
[15] Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, and Hao Wang. 2021. Zero-
Shot Recommender Systems. CoRR abs/2105.08318 (2021).
[16] Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming Zhu. 2020. MAMO:
Memory-Augmented Meta-Optimization for Cold-start Recommendation. In KDD.
ACM, 688â€“697.
[17] Eric Drott. 2018. Why the Next Song Matters: Streaming, Recommendation,
Scarcity. Twentieth-Century Music 15 (10 2018), 325â€“357. https://doi.org/10.1017/
S1478572218000245
[18] Xiaoyu Du, Xiang Wang, Xiangnan He, Zechao Li, Jinhui Tang, and Tat-Seng
Chua. 2020. How to Learn Item Representation for Cold-Start Multimedia Rec-
ommendation?. In ACM Multimedia. ACM, 3469â€“3477.
[19] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.
2023. CLAP Learning Audio Concepts from Natural Language Supervision. In
ICASSP. IEEE, 1â€“5.
[20] Philip J. Feng, Pingjun Pan, Tingting Zhou, Hongxiang Chen, and Chuanjiang Luo.
2021. Zero Shot on the Cold-Start Problem: Model-Agnostic Interest Learning
for Recommender Systems. In CIKM. ACM, 474â€“483.
[21] Andres Ferraro, Jaehun Kim, Sergio Oramas, Andreas F. Ehmann, and Fabien
Gouyon. 2023. Contrastive Learning for Cross-Modal Artist Retrieval. In ISMIR.
375â€“382.
[22] Andres Ferraro, Yuntae Kim, Soohyeon Lee, Biho Kim, Namjun Jo, Semi Lim,
Suyon Lim, Jungtaek Jang, Sehwan Kim, Xavier Serra, and Dmitry Bogdanov. 2021.
Melon Playlist Dataset: A Public Dataset for Audio-Based Playlist Generation
and Music Tagging. In ICASSP. IEEE, 536â€“540.
[23] Josh Gardner, Simon Durand, Daniel Stoller, and Rachel M. Bittner. 2023. LLark:
A Multimodal Foundation Model for Music. CoRR abs/2310.07160 (2023).
[24] Yuan Gong, Yu-An Chung, and James R. Glass. 2021. AST: Audio Spectrogram
Transformer. In Interspeech. ISCA, 571â€“575.
[25] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for
Implicit Feedback Datasets. In ICDM. IEEE Computer Society, 263â€“272.
[26] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and
Daniel P. W. Ellis. 2022. MuLan: A Joint Embedding of Music Audio and Natural
Language. In ISMIR. 559â€“566.
[27] Qingqing Huang, Aren Jansen, Li Zhang, Daniel P. W. Ellis, Rif A. Saurous, and
John R. Anderson. 2020. Large-Scale Weakly-Supervised Content Embeddings
for Music Recommendation and Tagging. In ICASSP. IEEE, 8364â€“8368.[28] Laurie Jakobsen. 2016. Playlists Overtake Albums in Listenership, says Loop
Study. https://musicbiz.org/news/playlists-overtake-albums-listenership-says-
loop-study/
[29] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. 2021.
Slow-Fast Auditory Streams for Audio Recognition. In ICASSP. IEEE, 855â€“859.
[30] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D.
Plumbley. 2020. PANNs: Large-Scale Pretrained Audio Neural Networks for
Audio Pattern Recognition. IEEE ACM Trans. Audio Speech Lang. Process. 28
(2020), 2880â€“2894.
[31] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019.
MeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation.
InKDD. ACM, 1073â€“1082.
[32] Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang, and Zi Huang. 2019.
From Zero-Shot Learning to Cold-Start Recommendation. In AAAI. AAAI Press,
4189â€“4196.
[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022. BLIP: Bootstrap-
ping Language-Image Pre-training for Unified Vision-Language Understanding
and Generation. In ICML (Proceedings of Machine Learning Research, Vol. 162).
PMLR, 12888â€“12900.
[34] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare, Shafiq R. Joty, Caiming
Xiong, and Steven Chu-Hong Hoi. 2021. Align before Fuse: Vision and Language
Representation Learning with Momentum Distillation. In NeurIPS. 9694â€“9705.
[35] Yunshan Ma, Yingzhi He, Wenjun Zhong, Xiang Wang, Roger Zimmermann, and
Tat-Seng Chua. 2024. CIRP: Cross-Item Relational Pre-training for Multimodal
Product Bundling.
[36] Yunshan Ma, Xiaohao Liu, Yinwei Wei, Zhulin Tao, Xiang Wang, and Tat-Seng
Chua. 2024. Leveraging Multimodal Features and Item-level User Feedback for
Bundle Construction. In WSDM. ACM, 510â€“519.
[37] Ilaria Manco, Emmanouil Benetos, Elio Quinton, and GyÃ¶rgy Fazekas. 2022.
Learning Music Audio Representations Via Weak Language Supervision. In
ICASSP. IEEE, 456â€“460.
[38] Matthew C. McCallum, Filip Korzeniowski, Sergio Oramas, Fabien Gouyon, and
Andreas F. Ehmann. 2022. Supervised and Unsupervised Learning of Audio
Representations for Music Understanding. In ISMIR. 256â€“263.
[39] Brian McFee and Gert R. G. Lanckriet. 2012. Hypergraph Models of Playlist
Dialects. In ISMIR. FEUP EdiÃ§Ãµes, 343â€“348.
[40] Alessandro B. Melchiorre, Navid Rekabsaz, Emilia Parada-Cabaleiro, Stefan
Brandl, Oleg Lesota, and Markus Schedl. 2021. Investigating gender fairness
of recommendation algorithms in the music domain. Inf. Process. Manag. 58, 5
(2021), 102666.
[41] Jiyoung Park, Jongpil Lee, Jangyeon Park, Jung-Woo Ha, and Juhan Nam. 2018.
Representation Learning of Music Using Artist Labels. In ISMIR. 717â€“724.
[42] Jordi Pons, Oriol Nieto, Matthew Prockup, Erik M. Schmidt, Andreas F. Ehmann,
and Xavier Serra. 2018. End-to-end Learning for Music Audio Tagging at Scale.
InISMIR. 637â€“644.
[43] Robert Prey. 2020. Locating Power in Platformization: Music Streaming Playlists
and Curatorial Power. Social Media + Society 6 (04 2020), 205630512093329.
https://doi.org/10.1177/2056305120933291
[44] Vasiliy Rubtsov, Mikhail Kamenshchikov, Ilya Valyaev, Vasiliy Leksin, and
Dmitry I. Ignatov. 2018. A hybrid two-stage recommender system for auto-
matic playlist continuation. In RecSys Challenge. ACM, 16:1â€“16:4.
[45] Badrul Munir Sarwar, George Karypis, Joseph A. Konstan, and John Riedl. 2001.
Item-based collaborative filtering recommendation algorithms. In WWW. ACM,
285â€“295.
[46] Martin Saveski and Amin Mantrach. 2014. Item cold-start recommendations:
learning local collective embeddings. In RecSys. ACM, 89â€“96.
[47] Markus Schedl, Hamed Zamani, Ching-Wei Chen, Yashar Deldjoo, and Mehdi
Elahi. 2018. Current challenges and visions in music recommender systems
research. Int. J. Multim. Inf. Retr. 7, 2 (2018), 95â€“116.
[48] Shaoyun Shi, Min Zhang, Xinxing Yu, Yongfeng Zhang, Bin Hao, Yiqun Liu,
and Shaoping Ma. 2019. Adaptive Feature Sampling for Recommendation with
Missing Content Feature Values. In CIKM. ACM, 1451â€“1460.
[49] Janne Spijkervet and John Ashley Burgoyne. 2021. Contrastive Learning of
Musical Representations. In ISMIR. 673â€“681.
[50] Darko Stanisljevic. 2020. The impact of Spotify features on music discovery in
the streaming platform age.
[51] Marco Tagliasacchi, Beat Gfeller, Felix de Chaumont Quitry, and Dominik Roblek.
2020. Pre-Training Audio Representations With Self-Supervision. IEEE Signal
Process. Lett. 27 (2020), 600â€“604.
[52] Roberto Turrin, Massimo Quadrana, Andrea Condorelli, Roberto Pagano, and
Paolo Cremonesi. 2015. 30Music Listening and Playlists Dataset. In RecSys Posters
(CEUR Workshop Proceedings, Vol. 1441). CEUR-WS.org.
[53] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
content-based music recommendation. In Advances in Neural Information Pro-
cessing Systems, C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q.
Weinberger (Eds.), Vol. 26. Curran Associates, Inc. https://proceedings.neurips.
cc/paper_files/paper/2013/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf
 
2533LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[54] Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, and Hugo
Larochelle. 2017. A Meta-Learning Perspective on Cold-Start Recommendations
for Items. In NIPS. 6904â€“6914.
[55] Maksims Volkovs, Himanshu Rai, Zhaoyue Cheng, Ga Wu, Yichao Lu, and Scott
Sanner. 2018. Two-stage Model for Automatic Playlist Continuation at Scale. In
RecSys Challenge. ACM, 9:1â€“9:6.
[56] Maksims Volkovs, Guang Wei Yu, and Tomi Poutanen. 2017. DropoutNet: Ad-
dressing Cold Start in Recommender Systems. In NIPS. 4957â€“4966.
[57] Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua.
2023. Equivariant Learning for Out-of-Distribution Cold-start Recommendation.
InACM Multimedia. ACM, 903â€“914.
[58] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng
Chua. 2021. Contrastive Learning for Cold-Start Recommendation. In ACM
Multimedia. ACM, 5382â€“5390.
[59] Minz Won, Keunwoo Choi, and Xavier Serra. 2021. Semi-supervised Music
Tagging Transformer. In ISMIR. 769â€“776.
[60] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and
Shlomo Dubnov. 2023. Large-Scale Contrastive Language-Audio Pretraining with
Feature Fusion and Keyword-to-Caption Augmentation. In ICASSP. IEEE, 1â€“5.
[61] Zhenchao Wu and Xiao Zhou. 2023. M2EU: Meta Learning for Cold-start Recom-
mendation via Enhancing User Preference Estimation. In SIGIR. ACM, 1158â€“1167.
[62] Hojin Yang, Yoonki Jeong, Minjin Choi, and Jongwuk Lee. 2018. MMCF: Mul-
timodal Collaborative Filtering for Automatic Playlist Continuation. In RecSys
Challenge. ACM, 11:1â€“11:6.
[63] Hamed Zamani, Markus Schedl, Paul Lamere, and Ching-Wei Chen. 2019. An
Analysis of Approaches Taken in the ACM RecSys Challenge 2018 for Automatic
Music Playlist Continuation. ACM Trans. Intell. Syst. Technol. 10, 5 (2019), 57:1â€“
57:21.
[64] Eva Zangerle, Martin Pichl, Wolfgang Gassler, and GÃ¼nther Specht. 2014. #now-
playing Music Dataset: Extracting Listening Behavior from Twitter. In WISMM.
ACM, 21â€“26.
[65] Mi Zhang, Jie Tang, Xuchen Zhang, and Xiangyang Xue. 2014. Addressing cold
start in recommender systems: a semi-supervised co-training algorithm. In SIGIR.
ACM, 73â€“82.
[66] Ziwei Zhu, Shahin Sefati, Parsa Saadatpanah, and James Caverlee. 2020. Rec-
ommendation for New Users and New Items via Randomized Training and
Mixture-of-Experts Transformation. In SIGIR. ACM, 1121â€“1130.
A DETAILED LOSS FORMULATIONS
(L1) WTC: Within-Track Contrastive Loss. As mentioned in
3.2, the L1 loss is formulated for cross-modal alignment within
a track. As defined in Equation 1, this loss performs contrastive
learning on the single-track level. Given a track, ğ‘ ğ‘–, consisting of a
language-audio pair, (ğ‘¡ğ‘ ğ‘–,ğ‘ğ‘ ğ‘–), we formally define this loss as:
â„“WTC
ğ‘ ğ‘–=Contrast(a ğ‘ ğ‘–,tğ‘ ğ‘–). (13)
More specifically, the contrastive loss is formulated as:
Contrast(a ğ‘ ğ‘–,tğ‘ ğ‘–) =1
2[CE(ğ‘¦a2t
ğ‘–,Ë†ğ‘¦a2t
ğ‘–) +CE(ğ‘¦t2a
ğ‘–,Ë†ğ‘¦t2a
ğ‘–)] (14)
where softmax-normalized audio-to-text (a2t) and text-to-audio
(t2a) similarities are defined by
Ë†ğ‘¦a2t
ğ‘–=exp(sim(ağ‘ ğ‘–,tğ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘—=1exp(sim(ağ‘ ğ‘–,tğ‘ ğ‘›)/ğœ),
Ë†ğ‘¦t2a
ğ‘–=exp(sim(tğ‘ ğ‘–,ağ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘—=1exp(sim(tğ‘ ğ‘–,ağ‘ ğ‘›)/ğœ),(15)
such that sim(Â·) is the cosine similarity function, ğœis a tempera-
ture hyper-parameter, ğ‘¦a2t
ğ‘–,ğ‘¦t2a
ğ‘–âˆˆ{0,1}are one-hot ground-truth
similarity vectors indicating the correct language-audio pairing,
andğ‘ ğ‘›is a negative sample, drawn from the momentum queue.
(L2) TTC: Track-Track Contrastive Loss. The second loss ex-
tends the formulation of the WTC (L1) loss to perform track-trackcontrastive learning. For two tracks ğ‘ ğ‘–= (ğ‘¡ğ‘ ğ‘–,ğ‘ğ‘ ğ‘–)andğ‘ ğ‘—= (ğ‘¡ğ‘ ğ‘—,ğ‘ğ‘ ğ‘—),
which share a parent playlist ğ‘ğ‘˜, this loss is formulated as
â„“TTC
ğ‘ ğ‘–,ğ‘ ğ‘—=1
2[Contrast(a ğ‘ ğ‘–,tğ‘ ğ‘—) +Contrast(t ğ‘ ğ‘–,ağ‘ ğ‘—)]. (16)
Here, the each contrastive loss pairing is formulated as:
Contrast(a ğ‘ ğ‘–,tğ‘ ğ‘—) =1
2[CE(ğ‘¦a2t
ğ‘–ğ‘—,Ë†ğ‘¦a2t
ğ‘–ğ‘—) +CE(ğ‘¦t2a
ğ‘—ğ‘–,Ë†ğ‘¦t2a
ğ‘—ğ‘–)],
Contrast(t ğ‘ ğ‘–,ağ‘ ğ‘—) =1
2[CE(ğ‘¦t2a
ğ‘–ğ‘—,Ë†ğ‘¦t2a
ğ‘–ğ‘—) +CE(ğ‘¦a2t
ğ‘—ğ‘–,Ë†ğ‘¦a2t
ğ‘—ğ‘–)],(17)
where softmax-normalized audio-to-text (a2t) and text-to-audio
(t2a) similarities are defined by
Ë†ğ‘¦a2t
ğ‘–ğ‘—=exp(sim(ağ‘ ğ‘–,tğ‘ ğ‘—)/ğœ)
Pğ‘
ğ‘›=1exp(sim(ağ‘ ğ‘–,tğ‘ ğ‘›)/ğœ),
Ë†ğ‘¦a2t
ğ‘—ğ‘–=exp(sim(ağ‘ ğ‘—,tğ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘›=1exp(sim(ağ‘ ğ‘—,tğ‘ ğ‘›)/ğœ),
Ë†ğ‘¦t2a
ğ‘—ğ‘–=exp(sim(tğ‘ ğ‘—,ağ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘›=1exp(sim(tğ‘ ğ‘—,ağ‘ ğ‘›)/ğœ),
Ë†ğ‘¦t2a
ğ‘–ğ‘—=exp(sim(tğ‘ ğ‘–,ağ‘ ğ‘—)/ğœ)
Pğ‘
ğ‘›=1exp(sim(tğ‘ ğ‘–,ağ‘ ğ‘›)/ğœ),(18)
such that sim(Â·) is the cosine similarity function, ğœis a temperature
hyper-parameter, and ğ‘ ğ‘›is a negative sample, drawn from the
momentum queue. And the one-hot ground-truth similarity vectors
indicating the correct language-audio pairing, are calculated as:
ğ‘¦a2t
ğ‘–ğ‘—=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘œğ‘£=ğ‘‚ğ‘–ğ‘£)
,
ğ‘¦a2t
ğ‘—ğ‘–=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘œğ‘£=ğ‘‚ğ‘—ğ‘£)
,
ğ‘¦t2a
ğ‘–ğ‘—=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘œğ‘£=ğ‘‚ğ‘–ğ‘£)
,
ğ‘¦t2a
ğ‘—ğ‘–=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘œğ‘£=ğ‘‚ğ‘—ğ‘£)
,(19)
whereğµis the batch size and ğ‘‚ğ‘Ã—ğ‘is the homogeneous, track-
track co-occurrence graph defined in Section 2.
(L3) TPC: Track-Playlist Contrastive Loss. The third loss ex-
tends the formulation of TTC (L2) loss to perform track-playlist
contrastive learning. For a track, ğ‘ ğ‘–, its parent playlist, ğ‘ğ‘˜, and a set
of neighbour tracks that share the parent playlist, ğ½={ğ‘ ğ‘—âˆˆğ‘ğ‘˜\ğ‘ ğ‘–},
define this loss as:
â„“TPC
ğ‘ğ‘˜,ğ‘ ğ‘–=1
2[Contrast(a ğ‘ğ‘˜,tğ‘ ğ‘–) +Contrast(t ğ‘ğ‘˜,ağ‘ ğ‘–)], (20)
where the contrastive pairing is defined by
Contrast(a ğ‘ğ‘˜,tğ‘ ğ‘–) =1
2[CE(ğ‘¦a2t
ğ‘˜ğ‘–,Ë†ğ‘¦a2t
ğ‘˜ğ‘–) +CE(ğ‘¦t2a
ğ‘–ğ‘˜,Ë†ğ‘¦t2a
ğ‘–ğ‘˜)],
Contrast(t ğ‘ ğ‘˜,ağ‘ ğ‘–) =1
2[CE(ğ‘¦t2a
ğ‘˜ğ‘–,Ë†ğ‘¦t2a
ğ‘˜ğ‘–) +CE(ğ‘¦a2t
ğ‘–ğ‘˜,Ë†ğ‘¦a2t
ğ‘–ğ‘˜)],(21)
 
2534KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, & Tat-Seng Chua
where softmax-normalized audio-to-text (a2t) and text-to-audio
(t2a) similarities are defined by
Ë†ğ‘¦a2t
ğ‘˜ğ‘–=exp(sim(ağ‘ğ‘˜,tğ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘›=1exp(sim(ağ‘ğ‘˜,tğ‘ ğ‘›)/ğœ),
Ë†ğ‘¦a2t
ğ‘–ğ‘˜=exp(sim(ağ‘ ğ‘–,tğ‘ğ‘˜)/ğœ)
Pğ‘
ğ‘›=1exp(sim(ağ‘ ğ‘–,tğ‘ ğ‘›)/ğœ),
Ë†ğ‘¦t2a
ğ‘–ğ‘˜=exp(sim(tğ‘ ğ‘–,ağ‘ğ‘˜)/ğœ)
Pğ‘
ğ‘›=1exp(sim(tğ‘ ğ‘–,ağ‘ ğ‘›)/ğœ),
Ë†ğ‘¦t2a
ğ‘˜ğ‘–=exp(sim(tğ‘ğ‘˜,ağ‘ ğ‘–)/ğœ)
Pğ‘
ğ‘›=1exp(sim(tğ‘ğ‘˜,ağ‘ ğ‘›)/ğœ),(22)
where the playlist embeddings, ğ‘ğ‘ğ‘˜,ğ‘¡ğ‘ğ‘˜, are defined by the output
of the fusion layer defined in Section 3.2 as:
ağ‘ğ‘˜=Self-Att({ağ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ½}),
tğ‘ğ‘˜=Self-Att({tğ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ½}),(23)
and that sim(Â·) is the cosine similarity function, ğœis a temperature
hyper-parameter, and ğ‘ ğ‘›is a negative sample, drawn from the
momentum queue. More formally, if we consider ğ´ğ½={ağ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ½}
andğ‘‡ğ½={tğ‘ ğ‘—:ğ‘ ğ‘—âˆˆğ½}we can define this attention layer as:
Self-Att(A ğ½) =softmaxğ´ğ½ğ‘ŠâŠ¤
ğ‘„ğ´ğ½ğ‘ŠâŠ¤
ğ¾âˆš
ğ‘‘
ğ´ğ½ğ‘ŠâŠ¤
ğ‘‰
Self-Att(Tğ½) =softmaxğ‘‡ğ½ğ‘ŠâŠ¤
ğ‘„ğ‘‡ğ½ğ‘ŠâŠ¤
ğ¾âˆš
ğ‘‘
ğ‘‡ğ½ğ‘ŠâŠ¤
ğ‘‰(24)
whereğ‘Šğ‘„,ğ‘Šğ¾,ğ‘Šğ‘‰âˆˆR|ğ½|Ã—ğ‘‘,ğ‘‘= 256. Meanwhile, the one-hot
ground-truth similarity vectors indicating the correct language-
audio pairing, are calculated as:
ğ‘¦a2t
ğ‘˜ğ‘–=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘Ÿğ‘£=ğ‘…ğ‘–ğ‘£)
,
ğ‘¦a2t
ğ‘–ğ‘˜=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘Ÿğ‘£=ğ‘…ğ‘—ğ‘£)
,
ğ‘¦t2a
ğ‘–ğ‘˜=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘Ÿğ‘£=ğ‘…ğ‘–ğ‘£)
,
ğ‘¦t2a
ğ‘˜ğ‘–=(
ğ‘œâˆˆ{0,1}ğµ:ğµâˆ‘ï¸
ğ‘£ğ‘Ÿğ‘£=ğ‘…ğ‘—ğ‘£)
,(25)
whereğµis the batch size and ğ‘…ğ‘€Ã—ğ‘is the bipartite, playlist-track
interaction graph defined in Section 2.
B COLD-START IMPLEMENTATIONS
B.1 ItemKNN [45]
B.1.1 Formulation. Given the pre-trained content features EÂ¯Sfor
all tracks in testing set and the playlist representation eÂ¯ğ‘ğ‘, we
calculate the scores Ë†ğ‘¦Â¯ğ‘ğ‘for the input partial playlist Â¯ğ‘ğ‘with inner
product to find held-out tracks. Formally:
Ë†S= arg max
Â¯S(Ë†ğ‘¦Â¯ğ‘ğ‘),Ë†ğ‘¦Â¯ğ‘ğ‘=eÂ¯ğ‘ğ‘Â·EÂ¯ğ‘†, (26)where Ë†Sare the predicted tracks. For the final recommendation,
we keep the top-k highest tracks, thus selecting |Ë†S|=ğ¾.
B.2 DropoutNet. [56]
B.2.1 Formulation. This method uses two stages of training.
â€¢WMF model: Given a playlist-track interaction matrix Rğ‘€Ã—ğ‘,
WMF learns the latent collaborative embeddings, ZSandZP.
â€¢DropoutNet model: Given the learned collaborative embed-
dings, ZSandZP, content features ESandEP, generated from
Î¦(Â·)) DropoutNet concatenates them and passes trains DNN mod-
els,ğ‘“ğ‘(Â·)andğ‘“ğ‘ (Â·), with mean squared error (MSE) to recover
these concealed entries and perform score prediction.
During testing, where our cold-start setting precludes the possi-
bility of calculating collaborative features, we follow the original
paperâ€™s formulation to generate collaborative features by averaging
them from the training set:
Ëœzğ‘=mean({zğ‘:ğ‘âˆˆP} ),Ëœzğ‘ =mean({zğ‘ :ğ‘ âˆˆS} ), . (27)
Thus, we input the partial playlist Â¯ğ‘ğ‘in Equation ??and obtain the
predicted scores Ë†ğ‘¦Â¯ğ‘ğ‘forÂ¯ğ‘ğ‘. Final predictions are generated using:
Ë†ğ‘¦Â¯ğ‘ğ‘=ğ‘“ğ‘(eÂ¯ğ‘||Ëœzğ‘)Â·ğ‘“ğ‘ (EÂ¯S||ËœZS), (28)
where ËœZSis a matrix formed by row-wise stacking of Ëœzğ‘ to match
the shape requirement of concatenation with EÂ¯S.
B.2.2 Hyperparameter Settings. We set the learning rate to 0.005,
L2 regularization to 0.1 and utilize the SGD optimizer with momen-
tum of 0.9 for both WMF and DropoutNet training. The embedding
size of WMF is 64 and the dimension of transformed features for
DropoutNet is 256. Additionaly, we use three-layer MLPs to im-
plement each of the transformation DNN models, ğ‘“ğ‘andğ‘“ğ‘ , and a
ratio of 0.2 for the feature dropout.
B.3 CLCRec [58]
B.3.1 Formulation. Unlike DrpoutNet, CLCRec initializes the em-
beddings and learns them during the training procedure. Here, its
training loss is:
â„“CLCRec =Contrast(z ğ‘,zğ‘ ğ‘–) +Contrast(ğ‘“(eğ‘ ğ‘–),ğ‘“(eğ‘ ğ‘—)), (29)
whereğ‘“(Â·)is a DNN model for transforming the pre-trained content
features. CLCRec randomly chooses between the content features
and collaborative embeddings using a predefined probability value
as it learns to maximize the correlation between learned content
features and latent playlist representations. The score prediction
is similar with ItemKNN, but equipped with an additional DNN
model, which can be denoted as:
Ë†ğ‘¦Â¯ğ‘ğ‘=ğ‘“(eÂ¯ğ‘ğ‘)Â·ğ‘“(EÂ¯S). (30)
B.3.2 Hyperparameter Settings. We tune the learning rate to 0.001,
L2 regularization to 0.1 and use the Adam optimizer. The temper-
ature for contrastive learning is 2.0 and we set the probability of
replacement to 0.5.
 
2535