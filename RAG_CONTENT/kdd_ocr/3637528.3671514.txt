Contextual Distillation Model for Diversified Recommendation
Fan Liâˆ—
University of Science and Technology
of China
Hefei, China
lf321@mail.ustc.edu.cnXu Siâˆ—
Tsinghua University
Beijing, China
six21@mails.tsinghua.edu.cnShisong Tangâˆ—
Kuaishou Inc.
Tsinghua University
Beijing, China
tangshisong@kuaishou.com
Dingmin Wang
University of Oxford
Oxford, United Kingdom
dingmin.wang@cs.ox.ac.ukKunyan Han
Kuaishou Inc.
Beijing, China
hankunyan@kuaishou.comBing Han
Kuaishou Inc.
Beijing, China
hanbing@kuaishou.com
Guorui Zhou
Kuaishou Inc.
Beijing, China
zhouguorui@kuaishou.comYang Song
Kuaishou Inc.
Beijing, China
yangsong@kuaishou.comHechang Chenâ€ 
Jilin University
Changchun, China
chenhc@jlu.edu.cn
Abstract
The diversity of recommendation is equally crucial as accuracy in
improving user experience. Existing studies, e.g., Determinantal
Point Process (DPP) and Maximal Marginal Relevance (MMR), em-
ploy a greedy paradigm to iteratively select items that optimize both
accuracy and diversity. However, prior methods typically exhibit
quadratic complexity, limiting their applications to the re-ranking
stage and are not applicable to other recommendation stages with a
larger pool of candidate items, such as the pre-ranking andranking
stages. In this paper, we propose Contextual Distillation Model
(CDM), an efficient recommendation model that addresses diver-
sification, suitable for the deployment in all stages of industrial
recommendation pipelines. Specifically, CDM utilizes the candidate
items in the same user request as context to enhance the diversifi-
cation of the results. We propose a contrastive context encoder that
employs attention mechanisms to model both positive and negative
contexts. For the training of CDM, we compare each target item
with its context embedding and utilize the knowledge distillation
framework to learn the win probability of each target item under the
MMR algorithm, where the teacher is derived from MMR outputs.
During inference, ranking is performed through a linear combina-
tion of the recommendation and student model scores, ensuring
both diversity and efficiency. We perform offline evaluations on
two industrial datasets and conduct online A/Btest of CDM on the
short-video platform KuaiShou. The considerable enhancements
âˆ—Equal contributions.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671514observed in both recommendation quality and diversity, as shown
by metrics, provide strong superiority for the effectiveness of CDM.
CCS Concepts
â€¢Information systems â†’Recommender systems.
Keywords
Recommender System, Knowledge Distillation, Diversified Recom-
mendation
ACM Reference Format:
Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, Guorui
Zhou, Yang Song, and Hechang Chen. 2024. Contextual Distillation Model
for Diversified Recommendation. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671514
1 Introduction
With the prevalence of Web 2.0 and mobile devices, an increasing
number of users are joining online feed stream platforms such as
TikTok1, Douyin2, and KuaiShou3for content sharing and con-
sumption [ 8,9,28â€“30]. To alleviate the issue of content homoge-
nization and enhance user engagement, it is important to consider
the diversity in recommendation systems. The diversity quanti-
fies the dissimilarity among recommended items based on certain
pre-selected attributes (e.g., item category), which is as crucial as ac-
curacy. Enhanced diverity can significantly contribute to the overall
user experience by encouraging exploration of new content and
discovery of new interests [16, 34, 36, 37, 47].
Figure 1 illustrates the stages of an industrial recommenda-
tion pipeline. Existing research on diversified recommendation
[4,5,14,44], such as the Determinantal Point Process (DPP) [ 5]
1https://www.tiktok.com/
2https://www.douyin.com/
3https://www.kuaishou.com/new-reco
5307
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Li et al.
RetrievalRankPre-rankPositivecontextCandidateitemsNegativecontextTargetitemAllitemsMillions TenThousandsHundredsThousandsRe-rank
Figure 1: An industry recommendation pipeline and different
item sets for one user request. Candidate items refer to: the
set of items that are to be scored one user request during the
ranking stage.
and Maximal Marginal Relevance (MMR) [ 4], has predominantly fo-
cused on enhancing diversity in the re-ranking stage. These greedy-
based approaches have successfully augmented the diversity of
re-ranking recommendation model. However, the necessity for di-
versity extends beyond the re-ranking stage, encompassing earlier
stages such as pre-ranking andranking. The absence of diversity in
these preliminary stages will render subsequent efforts to enhance
diversity during re-ranking ineffective against the backdrop of a
highly homogenized candidate item pool, leading to a predicament
metaphorically described as "You canâ€™t make bricks without straw."
Therefore, ensuring the diversity of the entire pipeline stages is
crucial in industrial recommendation pipeline.
However, the existing diversity algorithms designed for the re-
ranking stage are characterized by quadratic computational com-
plexity, making them unsuitable for stages involving extensive
candidate item pools, such as pre-ranking andranking, due to their
prohibitively high computational demands. Consequently, there is
a pressing need to develop an algorithm that is not only efficient
in handling large sets of candidates but also capable of effectively
enhancing diversity during the pre-ranking and ranking stages,
thereby significantly elevating the overall efficacy of recommenda-
tion pipelines.
In this paper, we introduce Contextual Distillation Model (CDM),
an efficient recommendation model designed to enhance diversifi-
cation across all stages of the industrial recommendation pipeline.
CDM innovatively addresses the need for both efficiency and diver-
sity by training a surrogate model of the quadratic time-complexity
MMR algorithm. By employing the knowledge distillation frame-
work [ 13], the surrogate model is trained to estimate the probability
of an item being selected by the MMR algorithm (i.e. the Top-K
items). This strategy enables the deployment of an end-to-end stu-
dent model that enhances diversity in the pre-ranking andranking
stages in an efficient manner.
The next problem is how to architecture the student model. We
know that diversity serves as a metric for evaluating sets. However,
models in pre-ranking andranking stages traditionally score each
item independently (point-wise). Therefore, to introduce set infor-
mation into the scoring process, CDM leverages the candidate items
within the same user request as context to enhance the diversity of
recommendations. Specifically, we propose a Contrastive Context
Encoder (CCE) to model the context for each target item. Given
the large number of candidates in pre-ranking andranking stages,
Candidate items
Historical interested items
Positive context
Negative context
T arget itemFigure 2: The probability density distribution of a target item,
where randomly selected from a candidate set in a single user
request, corresponds to different sets of item embeddings.
The positive context comprises the 100 candidate items most
similar to target item, and the negative context, the least
similar.
itâ€™s impractical to incorporate all context into calculations with
the target item, necessitating a sampling work on the context. We
posit that for any given target item, there exist two distinct types of
contexts within the candidate pool: highly similar items (positive
context) and markedly dissimilar items (negative context). Figure 2
shows the probability density distribution of a target item, where
randomly selected from a candidate set in a single user request,
corresponds to different sets of item embeddings. It is essential to
sample these two contexts, executed by employing a Gumbel-Top- ğ‘˜
strategy [ 42] based on the target attention scores [ 33] between the
target item and the context. After that, we employ the contrastive
attention mechanism [ 7,26] to effectively model the distinguishing
representations between the two opponent contexts, enhancing
the modelâ€™s ability to capture diverse patterns. This mechanism,
initially introduced in the field of computer vision [ 26], has been
primarily utilized for person re-identification by contrastively at-
tending to person and background regions.
Ultimately, by comparing each target item with its context em-
bedding, we can discern whether an item surpasses the aggregate
performance, thereby estimating its probability of being selected
in the MMR algorithm [ 4]. During the inference phase, ranking
is performed through a linear combination of scores from both
the recommendation and student models, ensuring diversity and
efficiency. This approach not only theoretically pioneers a novel
solution but also empirically validates the efficacy of CDM in ele-
vating both the quality and diversity of recommendations through
offline evaluations on two industrial datasets and an online A/B
test on the short-video platform KuaiShou.
5308Contextual Distillation Model for Diversified Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Our contributions can be summarized as follows:
â€¢We propose a Context Distillation Model for diversified rec-
ommendations, which extracts context embeddings from
the candidates of pipeline for each target item, and learns
end-to-end diversity scores in a distillation way.
â€¢We propose a Contrastive Context Encoder to learn con-
text embeddings from the positive and negative contextual
information inherent in each target itemâ€™s candidate items.
â€¢Both offline experiments on two industrial datasets and on-
lineA/Btest on the feed pages of KuaiShou App demonstrate
the effectiveness of our framework in balancing recommen-
dation accuracy and diversity.
2 Preliminaries
2.1 Traditional Recommendation
LetU={ğ‘¢1,...,ğ‘¢ğ‘€}andI={ğ‘–1,...,ğ‘–ğ‘}denote the set of users
andcandidate items within a single user request, respectively,
whereğ‘€is the number of users, ğ‘is the number of candidate
items. The user-item historical interactions are represented by
D={(ğ‘¢,ğ‘–,ğ‘¦)|ğ‘¢âˆˆU,ğ‘–âˆˆI} , whereğ‘¦âˆˆ {0,1}denotes the bi-
nary label (e.g., finish playing or like). The target of the traditional
recommendation training is to learn the scoring function ğ‘“(ğ‘¢,ğ‘–|Î˜)
fromD, which is capable of predicting the preference of user ğ‘¢on
itemğ‘–, where Î˜is the parameters of ğ‘“.
2.2 Diversified Recommendation
Diversified recommendation aims to provide users with a set of
accurate and diverse items. It can be formally defined as a subset se-
lection problem. For each user ğ‘¢âˆˆU, the objective is to determine
a ranked listR={ğ‘–1,ğ‘–2,...,ğ‘–ğ¾}ofğ¾items from the candidate set
Ithat maximizes a linear combination of accuracy and diversifica-
tion, which follows the classical work Maximal Marginal Relevance
(MMR):
R=argmax
Râ€²âŠ‚IAcc(ğ‘¢,Râ€²)+ğœ†âˆ—Div(ğ‘¢,Râ€²). (1)
Here, Acc(Â·)represents accuracy, ensuring that the recommended
items align with the userâ€™s preferences and have a high likelihood
of interaction. On the other hand, Div(Â·)quantifies diversity, ensur-
ing that the recommended items are dissimilar, offering a range of
choices to the user. The parameter ğœ†controls the trade-off between
accuracy and diversity.
2.3 Approximation Calculation
Our current challenge revolves around a discrete optimization prob-
lem. Specifically, it is tasked to select a subset Rof sizeğ¾from
candidate itemsIto optimize MMR. However, existing research has
demonstrated that maximizing a submodular function with cardi-
nality constraints constitutes an NP-hard problem [ 31]. Some works
have designed(1âˆ’1/ğ‘’)-approximation algorithms for this problem
based on greedy techniques [ 4,21]. That is, the construction of R
follows an iterative process according to marginal gain:
ğ‘–â„=argmax
ğ‘–âˆˆI\R 1:â„âˆ’1Acc(ğ‘¢,ğ‘–)+ğœ†âˆ—Div(R1:â„âˆ’1âˆªğ‘–|ğ‘¢), (2)whereâ„represents the current selection step, and our choices are
influenced by the outcomes of the preceding â„âˆ’1steps.
However, the greedy selectionâ€™s quadratic complexity, at ğ‘‚(ğ¾2ğ‘),
limits its utility primarily to the re-ranking stage. Such high compu-
tational demands render it impractical for other recommendation
stages, notably the pre-ranking andranking stages, where a larger
pool of candidate items necessitates more scalable solutions. Con-
sequently, our objective is to achieve the benefits of the greedy
approachâ€™s optimal results while significantly reducing the compu-
tational overhead in these more extensive stages.
Algorithm 1 Maximum Marginal Relevance Algorithm
1:R={argmaxğ‘–âˆˆIAcc(ğ‘¢,ğ‘–)}
2:while|R|<ğ¾do
3: forğ‘–âˆˆI\R do
4:R=Râˆª{ argmax(Acc(ğ‘¢,ğ‘–)+ğœ†âˆ—Div(Râˆª{ğ‘–}|ğ‘¢))}
5: end for
6:end while
3 Method
3.1 Interest-Aware MMR
The fundamental technique for diversity ranking is known as Max-
imum Marginal Relevance (MMR) [ 4], and considerable researches
[20,37] have been dedicated to enhancing and extending algo-
rithms grounded in MMR principles. In this section, we present
an enhancement of MMR that incorporates user interests, thereby
expanding the scope of diversity considerations. For the accuracy
score ACC(ğ‘¢,ğ‘–), we directly employ the score function ğ‘“(ğ‘¢,ğ‘–). To
compute the diversity score, we incorporate the user interest in
the measurement of item similarity. This incorporation links di-
versity measurement to the userâ€™s personal perception of diversity
stemming from distinct interests. Specifically,
Div(R1:â„âˆ’1âˆªğ‘–|ğ‘¢)=1âˆ’max
ğ‘–â€²âˆˆR1:â„âˆ’1ğœ(eğ‘–,ğ‘¢Â·eğ‘–â€²,ğ‘¢), (3)
where eğ‘–,ğ‘¢=eğ‘–âŠ™urefers to the Hadamard product between the
item embedding eğ‘–and the user interest vector u. In this diversity
function, we subtract the maximal similarity between selected items
Rand the target item ğ‘–as the diversity score.
Letâ€™s begin by addressing the selection of the initial item, referred
to asğ‘–1. In this simple case, where no items have been selected yet
(i.e.,R=âˆ…), it simplifies to choosing the item with the highest
single-item accuracy score, as expressed by:
ğ‘–1=argmax
ğ‘–âˆˆIğ‘“(ğ‘¢,ğ‘–). (4)
Subsequently, the selection of the remaining ğ¾âˆ’1items is carried
out iteratively following Eq. 2. The top-K items that can be selected
as positive samples, with the rest as negative samples, guide the
training of the surrogate model using this as the label.
3.2 Contrastive Context Learning
Previously, we discussed the employment of MMRâ€™s outputs as
supervised signals for training a surrogate model, denoted as ğ‘”(Â·),
to learn diversity. Regarding the input to ğ‘”(Â·), we employ the can-
didate items generated by the pre-ranking stage, see Fig 1. In this
5309KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Li et al.
NegativeContextPositiveContextContextualDistillation
GumbelSampling
FC layerContextTargetItemâ„’!"
BackBoneUserTargetItemAccuracyDiversityğ‘–=arg	max!âˆˆ#\â„›â„›SelectedItemsItem1â€¦ItemKâ€¦ItemNItemK+10-1label
Target    attention
Target anti-attentionâ„’!"#$%&'
MMRâ€¦WINLOSEWin prob
CandidatesTeacherStudent
Figure 3: The architecture of our proposed CDM. The teacher
model is the MMR algorithm, and the student model predicts
each itemâ€™s winning probability with context.
section, we focus on how to learn the context embeddings from
the candidates of target item for serving as the inputs to ğ‘”(Â·)in the
ranking stage.
3.2.1 Differentiable Sampling with Gumbel-Top- ğ‘˜Reparam-
eterization. During the ranking stage of industrial recommen-
dation, the size of candidate items ğ‘for a single user request is
considerable, as shown in Fig 1. Incorporating all candidate items
into graph computations brings additional overhead and increases
undesirable noise that affects context learning. Opting for sam-
pling emerges as an efficacious solution. However, simple random
sampling falls short in resolving the aforementioned issues. Upon
delving into the item distribution within each user request, we
discover a significant pattern: each target item invariably has both
relevant and competing counterparts within the candidate items.
Interestingly, the distribution of relevant items tends to be densely
clustered, while that of competing ones exhibit a looser distribu-
tion, as shown in Fig 2. In essence, this observation motivates us to
sample from these two distributions separately, thereby modeling
both positive and negative context information for each target item.
Given a candidate item set Iğ‘¡={ğ‘–1,...,ğ‘–ğ‘š,...,ğ‘–ğ‘}and a target
itemğ‘–ğ‘¡, we compute the attention score ğ‘¤ğ‘š=ğ‘ (ğ‘–ğ‘¡,ğ‘–ğ‘š)between
target itemğ‘–ğ‘¡and each candidate item ğ‘–ğ‘š. However, the sampling of
wis non-differentiable and prevents the model from being trained
well via back-propagation. To solve this issue, we adopt a differen-
tiable ordered subset sampling without replacement by generalizing
the Gumbel-Softmax reparametrization [ 15] to the Gumbel-Top- ğ‘˜
trick [ 42]. We define the probability of ğ‘¤ğ‘šasğ‘(ğ‘–ğ‘š|ğ‘–ğ‘¡)=ğ‘ğ‘š. The
perturbed weights are then obtained by adding Gumbel noise ğ‘”to
the log probabilities ğ‘:
eğ‘¤ğ‘š=logğ‘ğ‘š+ğ‘”ğ‘š, (5)ğ‘”ğ‘š=âˆ’log(âˆ’log(ğ‘¢ğ‘š))âˆ¼ Gumbel(0,1), (6)
whereğ‘¢ğ‘šâˆ¼Uniform(0,1) . Now, we need to consider how to turn
the perturbed weights ew=[eğ‘¤1,...,eğ‘¤ğ‘]into one-hot vectors a=
[a1,...,ağ‘], s.t.Ã
ğ‘šâ€²ağ‘šâ€²=ğ‘˜.
Following the work [ 23], we propose to use a recent top- ğ‘˜re-
laxation based on successive applications of the softmax function.
Specifically, define for all candidate item indices ğ‘š=1,...,ğ‘ and
iteration steps ğ‘—=1,...,ğ‘˜ :
ğ›¼ğ‘—+1
ğ‘š=ğ›¼ğ‘—
ğ‘š+log(1âˆ’ağ‘—
ğ‘š), ğ›¼1
ğ‘š=eğ‘¤ğ‘š, (7)
whereğ›¼ğ‘—
ğ‘šis a sample distribution at iteration step ğ‘—.
When considering ağ‘—
ğ‘š, [23] proposes a replacement using its
expectation in a relaxed manner, i.e.:
P(ağ‘—
ğ‘š=1)=exp(ğ›¼ğ‘—
ğ‘š/ğœ)
Ã
ğ‘šâ€²exp(ğ›¼ğ‘—
ğ‘šâ€²/ğœ), (8)
whereğœ>0is a given temperature hyper-parameter, such that the
new update is:
ğ›¼ğ‘—+1
ğ‘š=ğ›¼ğ‘—
ğ‘š+log(1âˆ’P(ağ‘—
ğ‘š=1)). (9)
In this way, each step ğ‘—produces a relaxed one-hot vector ağ‘—=
[P(ağ‘—
1=1),...,P(ağ‘—
ğ‘=1)], and the output a=Ãğ‘˜
ğ‘—=1ağ‘—is the
relaxedğ‘˜-hot vector, i.e.:
a=ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ‘˜âˆ‘ï¸
ğ‘—=1P(ağ‘—
1=1),...,ğ‘˜âˆ‘ï¸
ğ‘—=1P(ağ‘—
ğ‘=1)ï£¹ï£ºï£ºï£ºï£ºï£». (10)
Previous work [ 23] has demonstrated that as the temperature hyper-
parameterğœâ†’0, the expectation a1=[P(a1
1=1),...,P(a1
ğ‘=1)]of
the initial sampled vector ewapproximates a one-hot encoding. Con-
sequently, the logit update in Eq.(9) will also converge to the hard
update from Eq.(7). By induction, this convergence trend extends to
others, resulting in a one-hot encoding. Significantly, it aligns with
the Gumbel-Softmax reparametrization [ 15] whenğ‘˜=1. In the end,
the output relaxed ğ‘˜-hot vector atransforms into a hard encoding
that satisfies the desired property ofÃ
ğ‘šâ€²ağ‘šâ€²=ğ‘˜asğœâ†’0. To
counterbalance the influence of model initialization and broaden
parameter exploration, we initiate model training with a compara-
tively larger value of ğœfor expectation computation. Subsequently,
we implement a gradual annealing process [ 43] that progressively
reducesğœto a small constant value during training.
Now, we can employ the Gumbel-Top- ğ‘˜trick to sample ğ‘˜relevant
candidate itemsIğ‘ğ‘œğ‘ 
ğ‘¡={ğ‘–ğ‘ğ‘œğ‘ 
1,ğ‘–ğ‘ğ‘œğ‘ 
2,...,ğ‘–ğ‘ğ‘œğ‘ 
ğ‘˜}as positive contexts,
andğ‘˜competing candidate items Iğ‘›ğ‘’ğ‘”
ğ‘¡={ğ‘–ğ‘›ğ‘’ğ‘”
1,ğ‘–ğ‘›ğ‘’ğ‘”
2,...,ğ‘–ğ‘›ğ‘’ğ‘”
ğ‘˜}as
negative contexts for each target item ğ‘–ğ‘¡from its corresponding
candidate item setI.
3.2.2 Contrastive Context Encoder. Prior to the sampling phase,
the attention score wis computed between the target item and its
candidate items as follows [33, 46]:
w=qğ‘‡Kâˆš
ğ‘‘âˆˆRğ‘, (11)
where q=ğ‘–ğ‘¡Â·W1represents the embedding of the target item and
K=[ğ‘–1,...,ğ‘–ğ‘]âˆ—W2corresponds to the embeddings of candidate
items. W1andW2are two learnable parameters.
5310Contextual Distillation Model for Diversified Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Following the sampling procedure, we acquire two distinct con-
texts: a positive context Iğ‘ğ‘œğ‘ 
ğ‘¡and a negative one Iğ‘›ğ‘’ğ‘”
ğ‘¡. The subse-
quent task involves sequences modeling. In this regard, we utilize
thecontrastive attention [7,26] to directly model two sequences and
generate the corresponding context embeddings. The computation
of positive context embedding is used target attention [33, 46]:
Cğ‘ğ‘œğ‘ =softmax(ewğ‘ğ‘œğ‘ )Â·Vğ‘ğ‘œğ‘ , (12)
whereewğ‘ğ‘œğ‘ signifies the perturbed attention weights of the positive
candidate items, generated during the sampling phase, and Vğ‘ğ‘œğ‘ =
[ğ‘–ğ‘ğ‘œğ‘ 
1,...,ğ‘–ğ‘ğ‘œğ‘ 
ğ‘˜]âˆ—W3represents the candidate itemâ€™s embedings. W3
denotes the learnable parameter. Conversely, the negative context
embedding is constructed using an inverse weighting approach
through target anti-attention :
Cğ‘›ğ‘’ğ‘”=softmax(âˆ’ewğ‘›ğ‘’ğ‘”)Â·Vğ‘›ğ‘’ğ‘”, (13)
where Vğ‘›ğ‘’ğ‘”=[ğ‘–ğ‘›ğ‘’ğ‘”
1,...,ğ‘–ğ‘›ğ‘’ğ‘”
ğ‘˜]âˆ—W3utilizes the same parameters as
used in target-attention.
The separation of positive and negative context embeddings
stems from their distinct roles in relation to the target item, aim-
ing to precisely model the set information of the target item. Our
objective is to ensure that the positive context embedding Cğ‘ğ‘œğ‘ 
accurately captures the semantic essence associated with the target
item, whereas the negative one Cğ‘›ğ‘’ğ‘”remains devoid of any con-
nection to the target itemâ€™s attributes. In practice, we leverage the
InfoNCE loss [22] function to reinforce such differentiation.
Lğ¼ğ‘›ğ‘“ğ‘œğ‘ğ¶ğ¸ =âˆ’logexp(qğ‘‡Â·Cğ‘ğ‘œğ‘ /ğ‘¡)Ã
Câ€²âˆˆ{Cğ‘ğ‘œğ‘ ,Cğ‘›ğ‘’ğ‘”}exp(qğ‘‡Â·Câ€²/ğ‘¡),(14)
whereğ‘¡is a temperature parameter of InfoNCE loss.
In the end, we introduce a user interest-aware fusion strategy.
Our intention is to equip the learned context embedding with the
ability to discern its position within the wider user interest distri-
bution. The final context embedding can be computed as follows:
C=FFN(concat(uâŠ™Cğ‘ğ‘œğ‘ ,uâŠ™Cğ‘›ğ‘’ğ‘”)). (15)
3.3 Contextual Distillation Model
In this section, we elaborate on how the Contextual Distillation
Model (Student) distills knowledge from the greedy-based Maxi-
mum Marginal Relevance (Teacher) to achieve low-cost diversified
recommendations. We acknowledge that items selected by the MMR
algorithm (i.e., the winning item set, R), are either from niche tags
or represent the finest items within popular tags. The student model
contrasts the target item with its associated context to discern its
alignment with the majority. Consequently, estimating the win
probability for each item facilitates the guidance of the student
modelâ€™s learning process.
ğ‘¦ğ‘¡ğ‘’ğ‘=I(ğ‘–ğ‘¡âˆˆR) (16)
Regarding the student model ğ‘”(Â·), it introduces the Contrastive Con-
text Encoder, functioning as a surrogate model aimed at predicting
the win probability of each target item. We directly compute the
dot product between the embedding qof each target item ğ‘–ğ‘¡and its
corresponding context embedding Cto obtain the win probability.
ğ‘¦ğ‘ ğ‘¡ğ‘¢=ğ‘”(ğ‘¢,ğ‘–ğ‘¡,I)=ğœ(qğ‘‡Â·C) (17)For training ğ‘”(Â·), we resort to the knowledge distillation [ 13]. The
binary cross-entropy is used to minimize the discrepancy between
the soft output probabilities of the student and teacher model:
Lğ¾ğ·=âˆ’ğ‘¦ğ‘¡ğ‘’ğ‘logğ‘¦ğ‘ ğ‘¡ğ‘¢âˆ’(1âˆ’ğ‘¦ğ‘¡ğ‘’ğ‘)log(1âˆ’ğ‘¦ğ‘ ğ‘¡ğ‘¢) (18)
Therefore, the total training loss for CDM is as follows:
Lğµğ¶ğ¸=âˆ’ğ‘¦ğ‘–logğ‘“(ğ‘¢,ğ‘–)âˆ’(1âˆ’ğ‘¦ğ‘–)log(1âˆ’ğ‘“(ğ‘¢,ğ‘–)) (19)
L=Lğµğ¶ğ¸+ğ›½1âˆ—Lğ¾ğ·+ğ›½2âˆ—Lğ¼ğ‘›ğ‘“ğ‘œğ‘ğ¶ğ¸, (20)
whereğ›½1andğ›½2are the hyper-parameters. During the inference
phase, the final ranking scores are obtained by linearly fusing the
accuracy and diversity scores with a hyper-parameter ğ›¾, i.e.,ğ‘“(ğ‘¢,ğ‘–)+
ğ›¾âˆ—ğ‘¦ğ‘ ğ‘¡ğ‘¢.
3.4 Discussion
3.4.1 Time Complexity. The computational efficiency of online
serving is a critical concern in industrial recommender systems. For
a single user request, where the number of candidate items during
thepre-ranking andranking stage is denoted as ğ‘, andğ¾items
are to be returned. In the case of the MMR algorithm (list-wise),
the time complexity for generating the Top- ğ¾list isğ‘‚(ğ‘ğ¾2). To
elaborate, in each round, every remaining candidate item must be
computed with the selected items R. In contrast, the CDM approach
(point-wise) provides an efficient end-to-end solution, with a time
complexity equivalent to the cost of heap-sort algorithm to get the
Top-ğ¾list, which is ğ‘‚(ğ‘logğ¾).
It is worth noting that previous methods [ 20,44] have predom-
inantly found application in the final re-ranking stage, typically
associated with smaller ğ‘andğ¾values. However, in the pre-ranking
andranking stage, where ğ‘andğ¾are notably more substantial,
concerns pertaining to algorithmic overhead assume heightened
importance. CDM not only reduces the cost of inference but also
enhances the original recommendation modelâ€™s ability to assimilate
candidate information and seamlessly integrate diversity.
3.4.2 Scalability. To improve diversity within the ranking stage,
CDM employs knowledge distillation as an efficient end-to-end
method for learning diversity. This approach can be applied not
only to the ranking stage but also to other stages, serving as a
supplementary module for any embedding-based recommendation
model. Furthermore, we have utilized MMRâ€™s score as the supervisor
signal in this work, itâ€™s important to note that alternative diversity
algorithms, such as the Determinantal Point Process (DPP) [ 5] and
the Gram-Schmidt Process (GSP) [ 14], can be seamlessly substituted
to tailor the diversity learning process to specific requirements.
4 Experiment
In this part, we first present the offline dataset and experimental
configuration, as well as the reproducibility of the experiments in
Section 4.1. Next, Section 4.2 and 4.3 show the evaluation results of
the offline experiments as well as the detailed analysis. Finally, we
show the results of A/B experiments of CDM in real scenarios in
Section 4.4 to further demonstrate the effectiveness of the scheme.
5311KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Li et al.
Table 1: Statistics of two datasets.
Statistics JingDong Kuaishou
#request 1,237,631 8M
#interaction 6,307,945 74M
#user 931,883 0.5M
#query 323,478 -
#item 15,587,269 24M
item_show_ratio 9.95% 3.81%
#avg_candidate_item 213 500
#tag 3,336 4k
4.1 Experimental Setup
4.1.1 Dataset.
â€¢JingDong [49]: Constructed from search logs of a largest
e-commerce platformâ€™s advertising system, this dataset en-
compasses features such as user id, query id, and item, each
item containing a quintuple <item id, category id, vendor id,
price id >.
â€¢KuaiShou4: Owing to a lack of available recommendation
datasets with candidate items from upstream pipelines, a
new dataset was assembled from the KuaiShou app. It aggre-
gates data from 0.5 million active users, randomly sampled,
with their logs on the feed page over 5 days. The dataset
incorporates user id, tab id, and item, each item presenting
a triplet <item id, category id, duration id >. Release of this
dataset is anticipated post-review.
4.1.2 Metrics. For evaluation, we employ a comprehensive set of
metrics to evaluate the performance from both accuracy and diver-
sity perspectives. To measure accuracy, we utilize two widely used
metrics, namely Recall andMean Reciprocal Rank (MRR). Re-
garding recommendation diversity, we incorporate two commonly
used metrics: (1) Intra List Average Distance (ILAD), quantifying
the dissimilarity among items within the Top- ğ¾list. It measures
user-level diversity, where a larger value indicates greater diver-
sity in the Top- ğ¾recommendation list. (2) Category Coverage
(CC), which is the ratio of the number of categories covered by the
Top-ğ¾recommendations to the total number of categories in the
dataset. It focuses on system-level diversity, where a higher value
indicates that a greater variety of categories is being recommended.
The formal definitions of these metrics are as follows:
ILAD@ğ¾=1âˆ’1
ğ¾(ğ¾âˆ’1)âˆ‘ï¸
ğ‘¥âˆˆğ¿âˆ‘ï¸
ğ‘¦âˆˆğ¿,ğ‘¦â‰ ğ‘¥Cosine(ğ‘¥,ğ‘¦),(21)
CC@ğ¾=|ğ¶ğ¾|
|ğ¶|, (22)
whereğ¿denotes the Top- ğ¾list, CC@ğ¾represents the set of unique
categories within the Top- ğ¾recommendation list, and CC signifies
the set of all unique categories within the dataset. Higher values
of ILAD@ğ¾and CC@ğ¾are indicative of greater diversity in the
Top-ğ¾recommendations. We report the results for ğ¾=3andğ¾=5
on JingDong and ğ¾=20andğ¾=50on KuaiShou.
4https://www.kuaishou.com/new-reco4.1.3 Baselines.
â€¢DCN [34]: Deep Crossing Network is a state-of-the-art rec-
ommendation model serving as the backbone model of CDM.
â€¢IPW [24]: During the training phase, Inverse Propensity
Weighting adjusts the weights of items based on propensity
scores, which are calculated from the category distribution
in a userâ€™s historical interactions.
â€¢DPP [5]: Determinantal Point Process is a post-processing
method for diversified recommendations. It produces a di-
verse set of items from the recommended items generated
by DCN with greedy selection.
â€¢SSD [14]: It derives a time series analysis method called
Sliding Spectrum Decomposition that better captures usersâ€™
perception of diversity in long sequences scenario.
â€¢MMR [4]: We implement the diversity scoring function fol-
lowing MMR which use a coarse-grained item-level diversity
through re-ranking.
4.1.4 Hyper-parameters and training details. We implement
the DCN model with three hidden layers MLP, and the activation
function is ReLU. We optimize all models with Adam [ 18] optimizer
with batch sizes of {128,256}on two datasets. We use grid search
to find the optimal hyperparameters. In the CDM framework, ğœ†and
ğ›¾, which balances accuracy and diversity, are searched in the range
of{0.0,0.1,...,0.25}. The learning rate is searched in {1ğ‘’âˆ’3,1ğ‘’âˆ’
4,1ğ‘’âˆ’5}. To avoid overfitting, we set the dropout [ 27] to 0.25 and
the patience of earlystop to 10 epochs.
4.2 Offline Comparison
In this section, we first evaluate all algorithms based on the accu-
racy and diversity as two key metrics. Table 2 presents the results of
our offline experiments on two industrial dataset. Based on these re-
sults, we have the following findings: (1) Compared to the base DCN
model, all algorithms can enhance both user-level and system-level
diversity. (2) IPW re-weights the loss based on category distribution
to improve diversity. However, it dose not consistently outperform
DCN in recommendation diversity or accuracy, due to the inaccu-
rate estimation and high variance of propensity scores [ 45]. (3) DPP
and SSD exhibit relatively similar performance. While they achieve
higher diversity metrics, their accuracy drops more significantly
compared to DCN. Some previous works [ 20,47] suggest that both
of them assume that more orthogonality of recommended items
would result in larger diversity in recommendation results. How-
ever, stronger orthogonality tends to lead to a significant decrease
in accuracy, making it hard for them to effectively balance accuracy
and diversity. (4) Consistent with previous findings [ 20,47], MMR
has the best performance of all baselines. We believe that under
personalized recommendation, it is more appropriate to use em-
bedded dot product to compute diversity scores than orthogonality.
(5) Our proposed CDM effectively improved both recommenda-
tion accuracy and diversity on two datasets compared to the base
DCN model. The effectiveness of CDM can be attributed to three
following factors:
â€¢Generalization: CDM extracts diversity scores for each
user-item pair as supervised signals from MMR, replacing
the iterative strategy of MMR with an end-to-end manner,
which may have better generalization.
5312Contextual Distillation Model for Diversified Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Overall Top- ğ¾performance of different methods on JingDong and KuaiShou. Metric@ ğ¾denotes the corresponding
Top-ğ¾recommendation performance on this metric. RECALL and MRR focus on the accuracy of the recommended results,
ILAD measures the diversity of the user-level recommendation list, and CC assesses the diversity at the system level. For each
dataset, bold scores indicate the best in each column, underlined scores indicate the best baseline, and+/âˆ’represents whether
the value is higher/lower compared to the DCN model. For all metrics, the higher the result, the better.
JingDongModel RECALL@3 MRR@3 ILAD@3 CC@3 RECALL@5 MRR@5 ILAD@5 CC@5
DCN 0.0438 0.1001 0.2372 0.6943 0.0653 0.1124 0.2464 0.7210
IPW 0.0421âˆ’0.0993âˆ’0.2367âˆ’0.6939âˆ’0.0658+0.1138+0.2477+0.7311+
DPP 0.0384âˆ’0.0929âˆ’0.2785+0.7927+0.0471âˆ’0.0830âˆ’0.3025+0.8329+
SSD 0.0407âˆ’0.0955âˆ’0.2571+0.7148+0.0598âˆ’0.0902âˆ’0.2718+0.8016+
MMR 0.0459+0.1038+0.2493+0.7061+0.0637âˆ’0.1104âˆ’0.2609âˆ’0.7642âˆ’
CDM 0.0488+0.1074+0.2611+0.7342+0.0713+0.1192+0.2785+0.7874+
KuaiShouModel RECALL@20 MRR@20 ILAD@20 CC@20 RECALL@50 MRR@50 ILAD@50 CC@50
DCN 0.1057 0.0638 0.3922 0.7418 0.1931 0.0796 0.4502 0.8223
IPW 0.1006âˆ’0.0594âˆ’0.4019+0.7629+0.1828âˆ’0.0733âˆ’0.4719+0.8524+
DPP 0.0842âˆ’0.0513âˆ’0.4917+0.8942+0.1629âˆ’0.0608âˆ’0.5719+0.9640+
SSD 0.0891âˆ’0.0507âˆ’0.4555+0.8007+0.1663âˆ’0.0656âˆ’0.5157+0.8931+
MMR 0.1083+0.0652+0.4268+0.8248+0.2042+0.0805+0.5294+0.8902+
CDM 0.1205+0.0720+0.4426+0.8461+0.2219+0.0894+0.5503+0.9147+
â€¢Capacity: However, just relying on such signals often falls
short of surpassing teacher model. More significantly, CDM
enhances its performance by modeling the distribution of
candidate items produced by pre-rank model. This enables
each item to be aware of relevant/competing items in the can-
didate set, thereby increasing the recommendation modelâ€™s
capacity [49].
â€¢Information Exchange: Additionally, the shared bottom
embeddings benefit from additional update path, facilitating
information exchange between the recommendation model
and branch model. This, to some extent, can enhance the
accuracy of the original recommendation model.
4.3 Further Study
In this section, we conduct more detailed analysis experiments to
demonstrate the effectiveness of CDM.
4.3.1 The impact of context modeling. To simultaneously im-
prove recommendation accuracy and diversity, CDM introduces
an additional branch model on top of the recommendation back-
bone model. Next, we investigate the impact of various components
of CDM on the performance. Specifically, we examine the follow-
ing variables of the framework and compare them through offline
experiments, which showed in Table 3.
In CDM, the Contrastive Context Encoder (CCE) categorizes
candidate items into positive and negative classes and then learns
positive and negative contexts through a contrastive learning ap-
proach. From Table 2, it can be observed that when the InfoNCE
loss, which controls context learning, is removed, CDM experiences
a decrease in both accuracy and diversity. One possible explanation
is that without this constraint, the context may capture redundant
information, leading to reduced distinguishability between positive
and negative contexts.Table 3: Performance comparison of different variants.
Dataset Module Recall MMR ILAD CC
JingDong
Metrics@5CDM 0.0713 0.1192 0.2785 0.7874
-InfoNCE 0.0677 0.1139 0.2693 0.7785
-CCE 0.0624 0.1110 0.2792 0.7862
KuaiShou
Metrics@50CDM 0.2219 0.0894 0.5503 0.9147
-InfoNCE 0.2095 0.0803 0.5257 0.8920
-CCE 0.2036 0.0786 0.5223 0.8884
To further investigate the CCE module, we chose to directly
model the input sequence using the target attention, thus elimi-
nating the need to distinguish context. It was found that CDMâ€™s
performance significantly deteriorated across all metrics. This sug-
gests that CCEâ€™s strategy of contrastive context modeling enables
each item to perceive information about its similar or competing
items when being scored. This approach allows point-wise training
to incorporate global candidate information, making it simpler and
more effective.
4.3.2 The impact of trade-off parameter ğ›¾.In our framework,
each item has two scores: accuracy and diversity. CDM combines
two scores linearly, using ACC(ğ‘¢,ğ‘–)+ğ›¾âˆ—DIV(ğ‘¢,ğ‘–), whereğ›¾is the
hyper-parameter to balance accuracy and diversity. Fig 4 illustrates
performance tuning on two datasets. The y-axis represents the dif-
ference between the metric value at that ğ›¾and its best value, math-
ematically formulated as: Î”Metric(ğ›¾)=Metric(ğ›¾)âˆ’Metric(ğ›¾ğ‘ğ‘’ğ‘ ğ‘¡).
Asğ›¾ascends, RECALL demonstrates a parabolic tendency, first
elevating to a peak and then diminishing, signaling an optimal ğ›¾
value at which RECALL is maximized, which may due to the ability
of CDM to perceive relevant/competing information. In contrast,
ILAD and CC consistently exhibits an upward trajectory with the
5313KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Li et al.
0.00 0.02 0.04 0.06 0.08 0.10
Balance parameter 
0.01
0.000.01 Metrics
Best Recall
 ILAD
 CC
(a) Top@5 on JingDong.
0.00 0.05 0.10 0.15 0.20 0.25
Balance parameter 
0.06
0.04
0.02
0.000.02 Metrics
Best
 Recall
 ILAD
 CC
 (b) Top@50 on KuaiShou.
Figure 4: Performance on accuracy and diversity when vary-
ingğ›¾.
rise ofğ›¾, reflecting a proportional relation that highlights the en-
hanced emphasis on diversity with a larger ğ›¾. The results indicate
that it is crucial to introduce a proper amount of diversity into the
Top-ğ¾list to improve the joint utility of accuracy and diversity for
feed recommendation. Therefore, it is crucial to adjust the parame-
terğ›¾to achieve the optimal balance of accuracy and diversity to
provide the best experience for users in practical applications.
4.4 Online A/BTest
To validate the efficacy of CDM, we deploy it within the feed rec-
ommendation pages of two applications of KuaiShou for online A/B
test. The branch network is integrated into the online two-tower
recommendation model, which additionally receives the output (i.e.
candidate items) from the pre-rank model, ensuring consistency
with the offline experiments.
We evaluate the performance of CDM within a practical appli-
cation context of Kuaishou, referencing primarily the subsequent
three metrics: (1) Watch Time: a crucial measure reflecting user
engagement and satisfaction. (2) #Vertical Categories: it signi-
fies the diversity of presented content, representing the number of
unique video categories showed to all users. (3) Clustering Coef-
ficient: this measures the variability in user interaction with the
recommended content, quantifying the difference of the number
of videos to the number of unique video categories per session on
average. A lower value suggests a concentration of user interactions
within more categories, illustrating increased diversity in content
consumption.
The A/Btest spanned ten consecutive days, with the average
performance on the three aforementioned metrics tabulated in Ta-
ble 4, allowing us to formulate the ensuing conclusions. Initially,
CDM manifests a marked enhancement in Watch Time, corroborat-
ing the modelâ€™s capability to bolster user loyalty and engagement.
Subsequently, the augmentation in the quantity of Vertical Cate-
gories coupled with the diminution in the Clustering Coefficient
of observed videos attests to the promotion of diversity within the
recommended outcomes. In essence, CDM fosters both diversity in
recommendations and user engagement by strategically incorpo-
rating pipeline information, enabling models in the rank stage to
assimilate preferences from upstream models and data regarding
candidates. This substantiates that our framework orchestrates an
optimal balance between precision and diversity.Table 4: Online A/B test on two KuaiShou APP pages. We show
the relative performance of CDM. The grey square brackets
represent the 95% confidence intervals for online metrics.
Pages W
atch Time #Vertical Categories Clustering Coefficient
Main +0.406%
+0.188% -0.957%
[0.33%,
0.48%] [0.14%, 0.23%] [-1.00%, -0.91%]
Lite +0.412%
+0.264% -1.106%
[0.17%,
0.65%] [0.11%, 0.45%] [-1.25%, -0.87%]
5 Related Work
The discussion on diversified recommendation has been extensive
within the research community, highlighting the recognition of fac-
tors beyond recommendation accuracy as pivotal in elevating over-
all user satisfaction [ 3,12,19,37,39â€“41]. This awareness has led to
the emergence of the accuracy-diversity dilemma, a scenario where
the attainment of higher accuracy often compromises diversity, and
vice versa [ 35,50]. A primary cause of this dilemma has been the
historical focus on accuracy [ 34,47]. To improve user satisfaction,
two distinct diversity problems have been delineated: aggregation
diversity [ 10,48] and individual diversity [ 1,2,5,6,25,36,44]. Ag-
gregation diversity strives to diversify recommendations across the
entire user, thereby enhancing the coverage of the recommender
system. On the other hand, individual diversity focuses on diversify-
ing recommendations for a specific user, endeavoring to harmonize
accuracy and diversity by ensuring a range of different items within
a single userâ€™s recommendation list [37].
In this paper, we focus on the issue of individual diversity. A
classical approach is the greedy-based iterative selection algorithms
[37]. For instance, Maximal Marginal Relevance (MMR) [ 4] itera-
tively selects items, taking into account both accuracy and their
similarity to previously selected items. Determinantal point pro-
cess (DPP) [ 11] leverages a symmetric matrix to represent item
qualities and pairwise similarities. Sliding Spectrum Decomposi-
tion (SSD) [ 14] proposes a time series analysis technique to include
out-of-window items into the measurement of diversity to increase
the diversity of a long recommendation sequence and alleviate the
long tail effect as well. Feature Disentanglement Self-Balancing
(FDSB) aims to improve the diversity of re-ranking stage by refin-
ing MMR in relevant recommendation. [ 44] proposes a general re-
ranking framework named Multi-factor Sequential Re-ranking with
Perception-Aware Diversification (MPAD) to jointly optimize accu-
racy and diversity for feed recommendation with DPP. Furthermore,
some approaches take into account the temporal dimension of rec-
ommendations and utilize time series analysis techniques to model
the diversity of recommendation sequences [ 17,36]. These meth-
ods endeavor to capture information pertaining to out-of-window
items, thereby aligning more closely with usersâ€™ perception. Dif-
ferent from the above approaches, our work is the first to consider
both diversity and accuracy in rank stage for recommendation with
a flexible end-to-end algorithm.
6 Conclusion
In conclusion, this paper highlights the critical role of diversity, on
par with accuracy, in enhancing user experience in recommendation
systems. Addressing the limitations of existing methods like DPP
5314Contextual Distillation Model for Diversified Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
and MMR, which are constrained by quadratic complexity to the
re-ranking stage, we introduced the Contextual Distillation Model
(CDM). Designed for efficient diversification across all recommenda-
tion stages, CDM leverages contextual information from candidate
items and employs a contrastive context encoder to model diverse
contexts effectively. Through offline and online evaluations, CDM
demonstrated significant improvements in both recommendation
quality and diversity, also ensuring efficiency.
Acknowledgment
This work is partially supported in part by the National Natural
Science Foundation of China (No. U2341229); the Key R&D Project
of Jilin Province (No. 20240304200SF); and the International Coop-
eration Project of Jilin Province (No. 20220402009GH).
References
[1]Mustafa Abdool, Malay Haldar, Prashant Ramanathan, Tyler Sax, Lanbo Zhang,
Aamir Manaswala, Lynn Yang, Bradley Turnbull, Qing Zhang, and Thomas
Legrand. 2020. Managing diversity in airbnb search. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
2952â€“2960.
[2]Azin Ashkan, Branislav Kveton, Shlomo Berkovsky, and Zheng Wen. 2015. Opti-
mal Greedy Diversity for Recommendation.. In IJCAI, Vol. 15. 1742â€“1748.
[3]Haoyue Bai, Le Wu, Min Hou, Miaomiao Cai, Zhuangzhuang He, Yuyang Zhou,
Richang Hong, and Meng Wang. 2024. Multimodality Invariant Learning for
Multimedia-Based New Item Recommendation. arXiv preprint arXiv:2405.15783
(2024).
[4]Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based
reranking for reordering documents and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference on Research and development
in information retrieval. 335â€“336.
[5]Laming Chen, Guoxin Zhang, and Eric Zhou. 2018. Fast greedy map inference
for determinantal point process to improve recommendation diversity. Advances
in Neural Information Processing Systems 31 (2018).
[6]Peizhe Cheng, Shuaiqiang Wang, Jun Ma, Jiankai Sun, and Hui Xiong. 2017.
Learning to recommend accurate and diverse items. In Proceedings of the 26th
international conference on World Wide Web. 183â€“192.
[7]Xiangyu Duan, Hongfei Yu, Mingming Yin, Min Zhang, Weihua Luo, and Yue
Zhang. 2019. Contrastive Attention Mechanism for Abstractive Sentence Sum-
marization. arXiv preprint arXiv:1910.13114 (2019).
[8]Kairui Fu, Qiaowei Miao, Shengyu Zhang, Kun Kuang, and Fei Wu. 2023. End-to-
End Optimization of Quantization-Based Structure Learning and Interventional
Next-Item Recommendation. In CAAI International Conference on Artificial Intel-
ligence. 415â€“429.
[9]Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, and Jiwei Li. 2024. DIET:
Customized Slimming for Incompatible Networks in Sequential Recommendation.
arXiv:arXiv:2406.08804
[10] Mouzhi Ge, Carla Delgado-Battenfeld, and Dietmar Jannach. 2010. Beyond
accuracy: evaluating recommender systems by coverage and serendipity. In
Proceedings of the fourth ACM conference on Recommender systems. 257â€“260.
[11] Jennifer A Gillenwater, Alex Kulesza, Emily Fox, and Ben Taskar. 2014.
Expectation-maximization for learning determinantal point processes. Advances
in Neural Information Processing Systems 27 (2014).
[12] Zhuangzhuang He, Yifan Wang, Yonghui Yang, Peijie Sun, Le Wu, Haoyue Bai,
Jinqi Gong, Richang Hong, and Min Zhang. 2024. Double Correction Framework
for Denoising Recommendation. arXiv preprint arXiv:2405.11272 (2024).
[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in
a Neural Network. arXiv preprint arXiv:1503.02531 (2015). https://arxiv.org/abs/
1503.02531
[14] Yanhua Huang, Weikun Wang, Lei Zhang, and Ruiwen Xu. 2021. Sliding spectrum
decomposition for diversified recommendation. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining. 3041â€“3049.
[15] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical Reparameterization
with Gumbel-Softmax. arXiv preprint arXiv:1611.01144 (2016).
[16] Komal Kapoor, Vikas Kumar, Loren Terveen, Joseph A Konstan, and Paul Schrater.
2015. " I like to explore sometimes" Adapting to Dynamic User Novelty Pref-
erences. In Proceedings of the 9th ACM Conference on Recommender Systems.
19â€“26.
[17] R Kaptein, M Koolen, and J Kamps. 2009. Result Diversity and Entity Ranking
Experiments: Anchors, Links, Text and Wikipedia, University of Amsterdam.
NIST Special Publication (2009), 500â€“278.[18] Diederik P Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[19] MatevÅ¾ Kunaver and TomaÅ¾ PoÅ¾rl. 2017. Diversity in recommender systemsâ€“A
survey. Knowledge-based systems 123 (2017), 154â€“162.
[20] Zihan Lin, Hui Wang, Jingshu Mao, Wayne Xin Zhao, Cheng Wang, Peng Jiang,
and Ji-Rong Wen. 2022. Feature-aware diversified re-ranking with disentangled
representations for relevant recommendation. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 3327â€“3335.
[21] Michel Minoux. 2005. Accelerated greedy algorithms for maximizing submodular
set functions. In Optimization Techniques: Proceedings of the 8th IFIP Conference
on Optimization Techniques WÃ¼rzburg, September 5â€“9, 1977. Springer, 234â€“243.
[22] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[23] Tobias PlÃ¶tz and Stefan Roth. 2018. Neural nearest neighbors networks. Advances
in Neural information processing systems 31 (2018).
[24] Yuta Saito, Suguru Yaginuma, Yuta Nishino, Hayato Sakata, and Kazuhide Nakata.
2020. Unbiased recommender learning from missing-not-at-random implicit
feedback. In Proceedings of the 13th International Conference on Web Search and
Data Mining. 501â€“509.
[25] Chaofeng Sha, Xiaowei Wu, and Junyu Niu. 2016. A framework for recommending
relevant and diverse items.. In IJCAI, Vol. 16. 3868â€“3874.
[26] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. 2018. Mask-guided
contrastive attention model for person re-identification. In Proceedings of the
IEEE conference on computer vision and pattern recognition. 1179â€“1188.
[27] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from
overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929â€“1958.
[28] Youchen Sun, Zhu Sun, Xiao Sha, Jie Zhang, and Yew Soon Ong. 2023. Disentan-
gling Motives behind Item Consumption and Social Connection for Mutually-
enhanced Joint Prediction. In Proceedings of the 17th ACM Conference on Recom-
mender Systems. 613â€“624.
[29] Shisong Tang, Qing Li, Xiaoteng Ma, Ci Gao, Dingmin Wang, Yong Jiang, Qian
Ma, Aoyang Zhang, and Hechang Chen. 2022. Knowledge-based temporal fusion
network for interpretable online video popularity prediction. In Proceedings of
the ACM Web Conference 2022. 2879â€“2887.
[30] Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, Yong
Jiang, Qian Ma, and Aoyang Zhang. 2023. Counterfactual Video Recommendation
for Duration Debiasing. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 4894â€“4903.
[31] Paolo Toth. 2000. Optimization engineering techniques for the exact solution of
NP-hard combinatorial optimization problems. European journal of operational
research 125, 2 (2000), 222â€“238.
[32] Tim Van Erven and Peter Harremos. 2014. RÃ©nyi divergence and Kullback-Leibler
divergence. IEEE Transactions on Information Theory 60, 7 (2014), 3797â€“3820.
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[34] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDDâ€™17. 1â€“7.
[35] Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, and Tat-Seng Chua. 2021.
Deconfounded recommendation for alleviating bias amplification. In Proceedings
of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.
1717â€“1725.
[36] Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H Chi, and
Jennifer Gillenwater. 2018. Practical diversified recommendations on youtube
with determinantal point processes. In Proceedings of the 27th ACM International
Conference on Information and Knowledge Management. 2165â€“2173.
[37] Qiong Wu, Yong Liu, Chunyan Miao, Yin Zhao, Lu Guan, and Haihong Tang. 2019.
Recent advances in diversified recommendation. arXiv preprint arXiv:1905.06589
(2019).
[38] Qingyun Wu, Huazheng Wang, Yanen Li, and Hongning Wang. 2019. Dynamic
ensemble of contextual bandits to satisfy usersâ€™ changing interests. In The World
Wide Web Conference. 2080â€“2090.
[39] Jingyu Xiao, Zhiyao Xu, Qingsong Zou, Qing Li, Dan Zhao, Dong Fang, Ruoyu
Li, Wenxin Tang, Kang Li, Xudong Zuo, Penghui Hu, Yong Jiang, Zixuan Weng,
and Michael Lyv.R. 2024. Make Your Home Safe: Time-aware Unsupervised
User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask. arXiv
preprint arXiv:2406.10928 (2024). https://arxiv.org/abs/2406.10928
[40] Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Wenxin Tang, Runjie
Zhou, and Yong Jiang. 2023. User Device Interaction Prediction via Relational
Gated Graph Attention Network and Intent-aware Encoder. In Proceedings of
the 2023 International Conference on Autonomous Agents and Multiagent Systems
(AAMAS). 1634â€“1642.
[41] Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Zixuan Weng, Ruoyu Li,
and Yong Jiang. 2023. I Know Your Intent: Graph-enhanced Intent-aware User
Device Interaction Prediction via Contrastive Learning. Proceedings of the ACM
on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMUWT/UbiComp)
7, 3 (2023), 1â€“28.
5315KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Li et al.
[42] Sang Michael Xie and Stefano Ermon. 2019. Reparameterizable subset sampling
via continuous relaxations. arXiv preprint arXiv:1901.10517 (2019).
[43] Mei-Feng Xu, Hong Zhang, Su Zhang, Hugh L Zhu, Hui-Min Su, Jian Liu,
Kam Sing Wong, Liang-Sheng Liao, and Wallace CH Choy. 2015. A low tem-
perature gradual annealing scheme for achieving high performance perovskite
solar cells with no hysteresis. Journal of Materials Chemistry A 3, 27 (2015),
14424â€“14430.
[44] Yue Xu, Hao Chen, Zefan Wang, Jianwen Yin, Qijie Shen, Dimin Wang, Feiran
Huang, Lixiang Lai, Tao Zhuang, Junfeng Ge, et al .2023. Multi-factor Se-
quential Re-ranking with Perception-Aware Diversification. arXiv preprint
arXiv:2305.12420 (2023).
[45] Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. 2021.
A survey on causal inference. ACM Transactions on Knowledge Discovery from
Data (TKDD) 15, 5 (2021), 1â€“46.
[46] Feng Yu. 2020. TAGNN: Target Attentive Graph Neural Networks for Session-
based Recommendation. In International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. Xiâ€™an, China. https://ar5iv.org/abs/2005.
02844
[47] Xiaoying Zhang, Hongning Wang, and Hang Li. 2023. Disentangled Represen-
tation for Diversified Recommendations. In Proceedings of the Sixteenth ACM
International Conference on Web Search and Data Mining. 490â€“498.
[48] Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong,
and Ed H Chi. 2021. A model of two tales: Dual transfer learning framework for
improved long-tail item recommendation. In Proceedings of the web conference
2021. 2220â€“2231.
[49] Kaifu Zheng, Lu Wang, Yu Li, Xusong Chen, Hu Liu, Jing Lu, Xiwei Zhao, Chang-
ping Peng, Zhangang Lin, and Jingping Shao. 2022. Implicit User Awareness
Modeling via Candidate Items for CTR Prediction in Search Ads. In Proceedings
of the ACM Web Conference 2022. 246â€“255.
[50] Yu Zheng, Chen Gao, Liang Chen, Depeng Jin, and Yong Li. 2021. DGCN: Diver-
sified recommendation with graph convolutional networks. In Proceedings of the
Web Conference 2021. 401â€“412.
5316