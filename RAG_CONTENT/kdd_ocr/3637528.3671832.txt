AsyncET: Asynchronous Representation Learning for Knowledge
Graph Entity Typing
Yun-Cheng Wang
yunchenw@usc.edu
University of Southern California
Los Angeles, California, USAXiou Ge
xiouge@usc.edu
University of Southern California
Los Angeles, California, USA
Bin Wang
bwang28c@gmail.com
National University of Singapore
SingaporeC.-C. Jay Kuo
jckuo@usc.edu
University of Southern California
Los Angeles, California, USA
Abstract
Knowledge graph entity typing (KGET) aims to predict the missing
entity types in knowledge graphs (KG). The relationship between
entities and their corresponding types is often expressed using a
single relation, hasType. However, hasType has a limited capability
for modeling diverse entity-type relationships in the embedding
space. In this paper, we first introduce multiple auxiliary relations
to model the complex entity-type relationship. We propose an effi-
cient and robust algorithm to group similar entity types together
and assign a unique auxiliary relation to each group. Then, with
the auxiliary relations, we propose an Asynchronous representa-
tion learning framework for KGET, named AsyncET, where entity
and type embeddings are updated alternatively. Consequently, the
quality of entity embeddings is gradually improved during train-
ing by infusing type information. In addition, entity types with
different granularities and semantics can be properly modeled in
the embedding space. Experimental results show that AsyncET
can substantially improve the performance of embedding-based
methods on the KGET task and has a significant advantage over
state-of-the-art neural network-based methods in terms of model
sizes and inference time.
CCS Concepts
â€¢Computing methodologies â†’Machine learning algorithms ;
Semantic networks.
Keywords
Knowledge Graph Embedding, Knowledge Graph Entity Typing,
Representation Learning, Taxonomy
ACM Reference Format:
Yun-Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo. 2024. AsyncET:
Asynchronous Representation Learning for Knowledge Graph Entity Typing.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671832
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671832
Mark Twainwriterlecturerperson
MissouriSt. Louis Universityuniversitystate in U.S.administrative districtWill Johnsoccer playerBorn inLocated inPlays forLives in
EntityEntity typeRelationKnown typeMissing typeFigure 1: An exemplary KG with missing entity types.
1 Introduction
Knowledge graph (KG) stores data in a directed graph, where nodes
and edges denote entities and relations, respectively. A factual triple
in the form of (head entity, relation, tail entity) is a basic component
in KGs. There are multiple relation types in KGs used to describe the
diverse relationships between two entities. In addition to relation
types, each entity also comes with multiple types1to describe the
high-level abstractions of an entity. Fig. 1 shows an exemplary KG
that contains types of entities. As shown in the figure, each entity
can be labeled with multiple types. For example, the entity â€œMark
Twain â€ has types â€œwriter â€ and â€œlecturer â€ at the same time. KGs
that contain entity types are crucial in many knowledge discovery
and natural language processing (NLP) applications, such as drug
discovery [ 14], entity alignment [ 6,28], and knowledge editing
[4]. In real-world KGs, entity types could be missing, e.g., having
type â€œwriter â€ without type â€œperson â€, due to prediction errors from
information extraction models [ 29,30]. Such missing types can be
inferred from the existing information in KG. For example, in Fig. 1,
we can infer that â€œMark Twain â€ has a missing type â€œperson â€ given
that there is a known type â€œwriter â€ and the relation â€œborn in â€. Such
a task to predict the missing types based on the observed types and
triples in the KGs is called knowledge graph entity typing (KGET).
KGET methods are crucial for refining real-world KGs.
1We refer to â€œentity type" when using the term â€œtype" in the remainder of this paper.
3267
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yun-Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo
Mark TwainwriterlecturerpersonWill Johnsoccer playergeneralcreativitysport playerMissouriSt. Louis Universityuniversitystate in U.S.administrative districtlocationlocationcreativitygeneralinstitute
Figure 2: Illustration of using multiple auxiliary relations to
model the relationship between entities and types.
Knowledge graph embedding (KGE) methods [ 7] learn the repre-
sentations for the entire KG. They achieve great success in predict-
ing missing links [ 11], and are extended to solve the KGET task in
Moon et al . [16,17]by finding the closest types in the embedding
space. Although KGE methods are time- and parameter-efficient,
they cannot achieve good performance on the KGET task since
the relationships between entities and types are modeled using
only a single relation, hasType. Such an overly simplified model-
ing in the embedding space results in limited expressiveness of
the embeddings. In addition, such a method does not consider the
neighborhood information. Thus, another line of work leverages
graph neural networks (GNNs) [ 20] to encode the neighborhood
information into entity embeddings. Afterward, multiple fully con-
nected (FC) layers are applied to predict the entity types based on
the entity embeddings. The KGET task is, therefore, formulated as
a multi-label classification problem. Although GNN-based methods
can improve model performance, they possess a much higher com-
putation complexity. For example, the computation complexity for
the GNN-based models is in the order of ğ‘‚(ğ¿ğ¸ğ‘…ğ‘‘2+ğ‘‡ğ‘‘), whereğ¸,
ğ‘…, andğ‘‡are the number of entities, relations, and types in the KG,
respectively, ğ‘‘is the dimension of the hidden state, and ğ¿is the
number of layers. As a result, they are not applicable to a resource-
constrained environment, such as mobile/edge devices [ 13], under
privacy- or personalization-concerned application scenarios [ 27]
due to their high inference time complexity and large model sizes.
Thus, it is desired to develop a KGET method with low inference
time complexity, small model sizes, and good performance. This is
the objective of this work.
As argued above, a single relation is not sufficient to model the
relationship between entities and types. Here, we introduce more
auxiliary relations to improve the expressiveness of embedding-
based models on the KGET task. This idea is illustrated in Fig.2,
where we show an example of using multiple auxiliary relations
to model the entity-type relationship. It is intuitive that we should
use different auxiliary relations to model relationship for type â€œad-
ministrative district â€ and type â€œperson â€ since they describe two
different concepts of entities. Similarly, type â€œwriter â€ and type â€œsoc-
cer player â€ should adopt different auxiliary relations since they
are semantically different. They should not be close to each other
in the embedding space. On the other hand, for other types, suchas â€œwriter â€ and â€œlecturer â€, they co-occur with each other more fre-
quently. Thus, they can adopt the same auxiliary relation for model
simplicity.
Along this direction, we introduce multiple auxiliary relations
to model different entity-type relationships based on the â€œcontextâ€
of the type. The context of a type is defined as a collection of prop-
erties of its corresponding entities. It can be viewed as a discrete
representation of the type. As such, the local KG structure is im-
plicitly encoded when auxiliary relations are used. In addition, one
of the main shortcomings in KGE methods is that the types are not
properly utilized when training entity embeddings and vice versa.
Thus, we propose Asynchronous embedding learning for KGET,
named AsyncET, to obtain better KG embeddings for the entity
type prediction task. The training process consists of two stages: 1)
link prediction and 2) type prediction. Entity embeddings are first
optimized by training with only factual triples to predict missing
links. Then, the type information is used to train type embeddings
and fine-tune entity embeddings by predicting missing types. Two
training stages alternate as the training progresses. Experiments
conducted on two KGET datasets demonstrate that the multiple
auxiliary relations and AsyncET can substantially improve the
performance of the embedding-based methods for the KGET task.
Furthermore, AsyncET has a significant advantage over existing
methods in model sizes and time complexity.
The main contributions of this paper are summarized below.
â€¢We propose two novel approaches to group similar types
and assign auxiliary relations to model diverse relationships
between entities and types.
â€¢We propose a new asynchronous representation learning
framework, named AsyncET, to obtain better entity and type
embeddings for the KGET task.
â€¢Experiment results show that AsyncET can effectively im-
prove the expressiveness of KGE models and achieve lower
computation complexity than SOTA methods.
2 Related Work
2.1 Embedding-based Methods
Embedding-based methods predict the missing types by finding the
closest entity types in the embedding space. Synchronous represen-
tation learning [ 16] is often adopted in embedding-based methods,
where the factual triples and typing triples are mixed, and entity and
type embeddings are updated simultaneously. Such methods have
advantages in inference and model parameter efficiency. However,
such embeddings have limited expressiveness since they only use
a single relation to model diverse entity-type relationships. Asyn-
chronous representation learning has been previously explored to
improve the expressiveness of the embedding-based methods by
learning disjoint entity and type subspaces separately. Then, a map-
ping between two vector spaces must be learned to predict the miss-
ing connections between entities and types. ETE [ 17] minimized
the L1 distance between the entity and type subspaces. ConnectE
[32] adopted a linear projection matrix to connect entity and type
subspaces. KGE methods can also be extended to complex [ 21,22]
and hypercomplex [ 8] subspaces. For example, CORE [ 9] learns
entity and type embeddings in a complex subspace for better model
expressiveness. Then, a linear regression problem is solved to link
3268AsyncET: Asynchronous Representation Learning for Knowledge Graph Entity Typing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
entities with their corresponding types. Although existing methods
adopting asynchronous representation learning can improve the
expressiveness of the models, they require training two separate
subspaces with additional parameters to model the interactions
between two subspaces. To the best of our knowledge, we are the
first work to explore asynchronous representation learning in a
joint entity-type subspace, which is more intuitive than learning
two disjoint subspaces. As a result, we can largely improve the
performance while retaining the same computation complexity.
2.2 Classification-based Methods
The neighborhood information is important in the KGET task.
Classification-based methods [ 26] leverage such an observation and
use neighboring entities and relations to construct entity features.
Then, a classifier is used to predict the missing types. SDType-
Cond. [ 9] aggregated the conditional probability vectors within
the neighborhood as entity features and used a linear classifier
to predict the missing types. Graph neural network (GNN) based
methods [ 20,23,31] are also effective in capturing neighborhood
information. First, entity features are aggregated from the adjacent
entities and types in GNNs. Then, multiple fully connected (FC)
layers are attached to predict missing types by solving a multi-label
classification problem. Since not all adjacent entities and types con-
tribute to entity type prediction equally, an attention mechanism
has been adopted to achieve better performance in recent work. For
example, ConnectE-MRGAT [ 33] adopted graph attention networks
(GATs) [ 18,25] to solve the KGET task. CET [ 19] used two attention
mechanisms (i.e., N2T and Agg2T) to aggregate the neighborhood
information. AttET [ 34] adopted a type-specific attention mecha-
nism to improve the quality of entity embeddings. TET [ 10] used a
transformer architecture [ 24] as the entity encoder to aggregate the
neighborhood information. Although classification-based methods
can achieve superior performance, their inference time, complexity,
and model sizes are much larger than those of embedding-based
methods due to the neighborhood aggregation functions.
3 Methodology
3.1 Notations
We useGto denote a KG, which is a collection of factual triples,
namely,
G={(ğ‘’head,ğ‘Ÿ,ğ‘’tail)|ğ‘’head,ğ‘’tailâˆˆE,ğ‘ŸâˆˆR}, (1)
whereEandRrepresent sets of entities and relations in the KG,
respectively. The entity-type pairs in the KG are denoted as
I={(ğ‘’,ğ‘¡)|ğ‘’âˆˆE,ğ‘¡âˆˆT}, (2)
whereTis a set of entity types in the KG. In order to group
similar entity types, we define the context of type ğ‘¡, denoted asCğ‘¡,
as a collection of relations ğ‘ŸâˆˆRthat describes all the properties of
typeğ‘¡, namely,
Cğ‘¡={ğ‘Ÿ|(ğ‘’head,ğ‘Ÿ,ğ‘’tail)âˆˆG,(ğ‘’head,ğ‘¡)âˆˆI}
âˆª{ğ‘Ÿâˆ’1|(ğ‘’head,ğ‘Ÿ,ğ‘’tail)âˆˆG,(ğ‘’tail,ğ‘¡)âˆˆI}.(3)For example, the context of type person can be {Born in, Lives in,
Plays for, ... }. It contains all possible properties of the type person.
The context of type ğ‘¡can be seen as a discrete representation for ğ‘¡
that encodes the local structure of the KG.
3.2 Auxiliary Relations
Existing KGET methods convert a typing tuple, (ğ‘’,ğ‘¡), into a typing
triple,(ğ‘’,hasType,ğ‘¡)using a single relation hasType. However, a
single relation is not sufficient to model diverse entity-type patterns.
Here, we aim to find a mapping such that, given entity type ğ‘¡, an
auxiliary relation ğ‘=ğ´ğ‘¢ğ‘¥(ğ‘¡)is assigned to form new typing triples
(ğ‘’,ğ‘,ğ‘¡), whereğ‘¡âˆˆT,ğ‘âˆˆP, andPdenotes a set of newly created
auxiliary relations. The objective is to maximize the capabilities
of auxiliary relations to model diverse entity-type relationships. A
straightforward solution to enhance the expressiveness of auxiliary
relations is to assign a unique auxiliary relation to every single
type. However, when the KG contains a large number of types,
such a solution has several shortcomings. First, the model optimiza-
tion is less stable since the number of model parameters increases
significantly. Second, it is too fine-grained to perform well on the
test dataset. Third, its inference time is much longer. Therefore, it
is essential to group similar types and assign auxiliary relations
to each group of types. Thus, we propose two novel approaches
to effectively group similar types for assigning auxiliary relations
below.
Taxonomy-based grouping. Taxonomy is a hierarchical clas-
sification of types in KGs. For example, type â€œ/film/producer" in
Freebase [ 1] is classified under category â€œfilm" with the subcat-
egory â€œproducer". To group types based on the taxonomy, we
can group them based on the first layer of the taxonomy. For
instance, â€œ/film/producer" will belong to the â€œfilm" group, and
â€œ/sports/sports_team" will belong to the â€œsports" group. However,
such a taxonomy might not be available for some KGs. Further-
more, the first-layer-taxonomy-based grouping may not be enough
to model the similarities of fine-grained types. To address these
issues, we explore a more efficient and effective grouping algorithm
method below.
An efficient grouping algorithm. To strike a balance between
the minimal number of auxiliary relations, |P|, and high expres-
siveness of entity-type modeling, we aim to maximize similarities
among the types in the same group and minimize similarities among
different groups. Mathematically, we adopt the Jaccard similarity
between the type contexts to define similarities between types. It
can be written in the form of
ğ‘†ğ‘–ğ‘š(ğ‘¡,ğ‘¡â€²)=|Cğ‘¡âˆ©Cğ‘¡â€²|
|Cğ‘¡âˆªCğ‘¡â€²|. (4)
We selected Jaccard similarity since it can accurately reflect the
similarity of the neighborhood between entity types. In addition, it
works well for long-tailed entity types, i.e., entity types with fewer
observed triples. Then, based on the defined similarity function
between types, grouping similar types with the minimum number of
groups can be formulated as a min-max optimization problem. Such
an optimization problem is NP-Hard. Here, we develop a greedy
algorithm to find an approximate optimal grouping, as elaborated
in the following. First, we identify several anchor types to be the
centroid of each group. The process of finding anchor types is
3269KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yun-Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo
Algorithm 1 Efficient Algorithm to Find Anchor Types for Entity
Grouping
Initialization:
Uncovered relations: Uâ‰”R
Anchor types:Aâ‰”âˆ…
Context of type ğ‘¡âˆˆT:Cğ‘¡ âŠ²See (3)
Iteration:
1:whileUâ‰ âˆ…do âŠ²Repeat until all relations are covered
2:ğ‘¡=argmaxğ‘¡âˆˆT|Cğ‘¡âˆ©U| âŠ²Select type covering the most
uncovered relations
3:Uâ†U\C ğ‘¡ âŠ²Remove covered relations
4:Aâ†Aâˆª{ ğ‘¡} âŠ²Addğ‘¡to anchor types
5:end while
depicted in Algorithm 1. Initially, the anchor type set Ais empty.
Then, we select the anchor types by picking the type that has the
lowest similarities (minimize) with the other anchor types and the
largest coverage of its context set (maximize) in every iteration. In
each iteration, we greedily pick the type that meets our criteria
as the new anchor type. The iteration ends when the union of all
anchorsâ€™ contexts covers every relation Rin the KG. Then, we assign
a unique auxiliary relation to each group of types. The remaining
types will find their most similar anchor types and share the same
auxiliary relations with the anchor type.
To verify whether the proposed algorithm can generate rea-
sonable results, we show examples of the grouped entity types in
Table 1. We see from the table that the auxiliary relation # 2 is
assigned to types of persons who work in the entertainment in-
dustry, auxiliary relation # 20 is assigned to types that are mostly
sports teams, and auxiliary relation # 27 is assigned to geographical
locations. Similar types are successfully grouped together while
types of distant semantic meanings are separated.
It is worthwhile to mention that the proposed efficient entity
type grouping algorithm is a statistical method that adopts the
Jaccard similarity to measure the similarity of contexts between
different entity types. Thus, the grouping algorithm is robust to
the structure and completeness of the KG as long as the unseen KG
possesses a similar distribution as the observed KG.
3.3 AsyncET: Asynchronous Representation
Learning for Knowledge Graph Entity
Typing
After auxiliary relations are assigned, typing tuples (ğ‘’,ğ‘¡)can be
converted into typing triples (ğ‘’,ğ‘,ğ‘¡). Such typing triples form a
typing graph
H={(ğ‘’,ğ‘,ğ‘¡)|(ğ‘’,ğ‘¡)âˆˆI,ğ‘=ğ´ğ‘¢ğ‘¥(ğ‘¡)}. (5)
Instead of mixing the original triples and the newly constructed
typing triples together during embedding training, we optimize
the entity and type embeddings on the original KG ( G) and the
typing graph (H) alternatively. That is, the representation learning
process is divided into two stages. The two training stages employ
different learning objectives and different loss functions. In stage 1,
the entity embeddings are trained on Gusing the link predictionTable 1: Examples of the entity type grouping results using
the proposed efficient algorithm. Anchor types are labeled
in boldface. The examples are based on the FB15kET dataset.
Auxiliary Relation Entity Types
# 2/film/producer
/TV/tv_director
/film/writer
/film/film_story_contributor
# 20/sports/sports_team
/soccer/football_team
/baseball/baseball_team
/sports/school_sports_team
# 27/location/administrative_division
/film/film_location
/fictional_universe/fictional_setting
/location/citytown
task. The learned entity embeddings serve as an initialization for
embedding learning in stage 2, where we learn type embeddings and
fine-tune entity embeddings using the entity type prediction task.
We only use the typing triples in Hin this stage. The two training
stages are optimized alternatively. Fig. 3 illustrates the training
process of asynchronous representation learning. The alternating
training process offers a clear optimization objective for learning
the embeddings in each stage based on a specific task. Details of
each training stage are elaborated below.
Stage 1: Link prediction. The goal of this stage is to obtain a
good initialization of entity embeddings that can be used to pre-
dict the missing links. We follow the training loss in [ 21] with the
self-adversarial negative sampling since it is more powerful at find-
ing negative samples that can not be predicted correctly. The link
prediction loss is defined as
Lğ‘™ğ‘=âˆ’log(ğœ(ğ‘“(ğ’†head,ğ’“,ğ’†tail)))
âˆ’ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘(ğ‘’â€²
ğ‘–,ğ‘Ÿ,ğ‘’â€²â€²
ğ‘–)log(ğœ(âˆ’ğ‘“(ğ’†â€²
ğ‘–,ğ’“,ğ’†â€²â€²
ğ‘–))),(6)
where(ğ‘’â€²
ğ‘–,ğ‘Ÿ,ğ‘’â€²â€²
ğ‘–)is the negative samples generated by corrupting
the head and tail entities, ğ‘“(ğ’†head,ğ’“,ğ’†tail)is the scoring function
in the KGE model, and ğ’†head,ğ’“,ğ’†tailare embeddings for the head
entity, relation, and tail entity, respectively. The self-adversarial
negative sampling distribution is defined as
ğ‘(ğ‘’â€²
ğ‘—,ğ‘Ÿ,ğ‘’â€²â€²
ğ‘—)=exp(ğ›¼ğ‘“(ğ’†â€²
ğ‘—,ğ’“,ğ’†â€²â€²
ğ‘—))
Ãğ‘›
ğ‘–=1exp(ğ›¼ğ‘“(ğ’†â€²
ğ‘–,ğ’“,ğ’†â€²â€²
ğ‘–)), (7)
whereğ›¼is the temperature to control the smoothness of the
softmax function. As a result, negative samples with lower scores
are assigned smaller weights for optimization as they are well-
trained already, and the model can focus on optimizing the hard
cases.
Stage 2: Entity type prediction. In this stage, we fine-tune
the entity embeddings and train the type embeddings using only
3270AsyncET: Asynchronous Representation Learning for Knowledge Graph Entity Typing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Link predictionEntity type predictionInitialization
Fine-tuning
Figure 3: A two-stage representation learning scheme in AsyncET.
typing triples. We adopt a loss similar to (6) to predict the missing
entity types.
Lğ‘¡ğ‘=âˆ’log(ğœ(ğ‘“(ğ’†,ğ’‘,ğ’•)))
âˆ’ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘(ğ‘’,ğ‘â€²
ğ‘–,ğ‘¡â€²
ğ‘–)log(ğœ(âˆ’ğ‘“(ğ’†,ğ’‘â€²
ğ‘–,ğ’•â€²
ğ‘–))),(8)
where(ğ‘’,ğ‘â€²
ğ‘–,ğ‘¡â€²
ğ‘–)is a negative sample for entity type prediction
generated by replacing the valid types with a random type ğ‘¡â€²
ğ‘–and
the corresponding auxiliary relation ğ‘â€²
ğ‘–. The auxiliary relations
are assigned based on mappings, ğ‘=ğ´ğ‘¢ğ‘¥(ğ‘¡)andğ‘â€²
ğ‘–=ğ´ğ‘¢ğ‘¥(ğ‘¡â€²
ğ‘–).
Since the number of entity types is much fewer than the number
of entities (i.e.|T|<<|E|), false negatives, i.e. (ğ‘’,ğ‘â€²
ğ‘–,ğ‘¡â€²
ğ‘–)âˆˆH , are
more prevalent for entity-type prediction. To address this issue, we
adopt false-negative-aware (FNA) loss introduced in [ 19]. It can be
written as
ğ‘(ğ‘’,ğ‘â€²
ğ‘—,ğ‘¡â€²
ğ‘—)=ğ‘¥âˆ’ğ‘¥2, (9)
where
ğ‘¥=ğœ(âˆ’ğ‘“(ğ’†,ğ’‘â€²
ğ‘–,ğ’•â€²
ğ‘–)). (10)
Then, negative samples with the highest scores are assigned
lower weights as they are possibly false negatives. Similar to the
self-adversarial loss, negative samples with the lowest scores are
already well-trained, so they are assigned smaller negative sampling
weights.
4 Experiments
4.1 Experimental Setup
Datasets. We adopt two commonly used KGET datasets, FB15k-ET
and YAGO43k-ET [ 17], for evaluation. FB15k-ET is derived from a
link prediction dataset, FB15K [ 2], which is extracted from Freebase
[1]. Freebase contains general relations between real-world enti-
ties. YAGO43k-ET is derived from another link prediction dataset,
YAGO43k [ 16], which is extracted from YAGO [ 15]. There are
mostly attributes and relations for persons in YAGO. The dataset
statistics are summarized in Table 2. The two datasets have differ-
ent characteristics, where FB15kET has richer properties for eachTable 2: Dataset statistics.
FB15k-ET YAGO43k-ET
G# entities 14,951 42,334
# relations 1,345 37
# triples 483,142 331,686
H# types 3,584 45,182
# train 136,618 375,853
# valid 15,848 43,111
# test 15,847 43,119
#ğ‘-taxonomy 89 -
#ğ‘-efficient 54 10
entity while YAGO43kET contains more fine-grained entity types.
ğ‘-taxonomy and ğ‘-efficient denote the number of auxiliary rela-
tions obtained from the taxonomy-based grouping and the efficient-
algorithm-based grouping described in Sec. 3.2, respectively. Only
FB15k-ET contains taxonomy labels for the types. It is shown in
Table 2 that the efficient grouping algorithm can generate fewer
and more accurate entity type groups. We make our codes and data
available at https://github.com/yunchengwang/AsyncET.
Scoring functions. We select three representative KGE methods:
1) TransE [ 2], 2) RotatE [ 21], 3) and CompoundE [ 5] as our scoring
functions. TransE models relations as translation vectors in the em-
bedding space. It has several limitations in modeling diverse relation
patterns, e.g. symmetry. RotatE models relations as rotation opera-
tions in the complex embedding space. CompoundE is a recently
proposed KGE method that generalizes the majority of KGE meth-
ods by including compounding geometric transformations such as
translation, rotation, and scaling. Thus, we include these three KGE
methods to test the consistent performance enhancement of our
proposed method across all kinds of baselines.
Evaluation protocols. For the KGET task, the goal is to predict the
missing types given an entity, i.e., (ğ‘’,?). However, in AsyncET, we
reformulate the problem to predict the missing components in the
query triples(ğ‘’,?,?). We evaluate the joint plausibility ğ‘“(ğ‘’,ğ‘â€²,ğ‘¡â€²),
3271KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yun-Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo
Table 3: Results on the KGET task in FB15kET and YAGO43kET. The best performance in each method category is underlined.
The best performance among all methods is further marked in boldface.
Mo
delFB15kET YAGO43kET
MRR
Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10
Emb
edding-based Methods
TransE [2] 0.618 0.504 0.686 0.835 0.427 0.304 0.497 0.663
RotatE [21] 0.632 0.523 0.699 0.840 0.462 0.339 0.537 0.695
CompoundE [5] 0.640 0.525 0.719 0.859 0.480 0.364 0.558 0.703
ETE [17] 0.500 0.385 0.553 0.719 0.230 0.137 0.263 0.422
ConnectE [32] 0.590 0.496 0.643 0.799 0.280 0.160 0.309 0.479
CORE [9] 0.600 0.493 0.653 0.810 0.350 0.242 0.392 0.550
Classification-base
d Methods
HMGCN [12] 0.510 0.390 0.548 0.724 0.250 0.142 0.273 0.437
CompGCN [23] 0.665 0.578 0.712 0.839 0.355 0.274 0.383 0.513
SDType Cond. [9] 0.420 0.276 0.501 0.712 - - - -
CET [19] 0.697 0.613 0.745 0.856 0.503 0.398 0.567 0.696
Conne
ctE-MRGAT [33] 0.630 0.562 0.662 0.804 0.320 0.243 0.343 0.482
AttET [34] 0.620 0.517 0.677 0.821 0.350 0.244 0.413 0.565
RACE2T [35] 0.640 0.561 0.689 0.817 0.340 0.248 0.376 0.523
TET [10] 0.717 0.638 0.762 0.872 0.510 0.408 0.571 0.695
Our
Method (Embedding-based)
AsyncET (TransE) 0.659 0.552 0.729 0.859 0.452 0.341 0.518 0.684
AsyncET (RotatE) 0.668 0.564 0.735 0.864 0.471 0.359 0.556 0.717
AsnycET (CompoundE) 0.688 0.581 0.755 0.885 0.492 0.380 0.574 0.721
Table 4: Link prediction results on FB15K [ 2] dataset. The
best performance is marked in boldface.
Model MRR Hits@1 Hits@3 Hits@10
TransE [2] 0.463 0.297 0.578 0.749
+ AsyncET (Ours) 0.476 0.308 0.602 0.770
RotatE [21] 0.797 0.746 0.830 0.884
+ AsyncET (Ours) 0.805 0.752 0.848 0.903
CompoundE [5] 0.808 0.765 0.845 0.897
+ AsnycET (Ours) 0.817 0.772 0.863 0.914
whereğ‘â€²=ğ´ğ‘¢ğ‘¥(ğ‘¡â€²), for all possible candidates and rank them
accordingly. The valid entity types should be ranked as high as
possible compared to all other candidates. Following the convention
in [2], we adopt the filtered setting, where all entity types in the
KG serve as candidates except for those observed ones. Several
commonly used ranking metrics are adopted, including the Mean
Reciprocal Rank (MRR) and Hits@K (K=1, 3, and 10).
4.2 Experimental Results
4.2.1 KGET Results. Experimental results on two KGET datasets
are shown in Table 3. Methods are clustered into two main cat-
egories: 1) embedding-based methods and 2) classification-based
methods. AsyncET also belongs to an embedding-based method. For
AsyncET, we report the best performance using either ğ‘-taxonomyorğ‘-efficient. Detailed comparison of the effects of different entity
type grouping approaches will be discussed in Sec. 4.4. We have the
following observations from the table. AsyncET is significantly bet-
ter than baseline KGE methods, namely, TransE, RotatE, and Com-
poundE. AsyncET has a consistent performance improvement over
all three baselines in both datasets and all evaluation metrics. This
result demonstrates that AsyncET is applicable to any existing KGE
methods and can achieve substantial performance improvement
in the KGET task. AsyncET outperforms all the embedding-based
methods in both datasets and all evaluation metrics. The current
SOTA methods are mostly classification-based methods. However,
classification-based model and inference time complexity are much
higher than embedding-based methods. More details in complexity
analysis will be discussed in Sec. 4.3. CET and TET perform better
in Hits@1 than AsyncET, while AsyncET can outperform all the
existing methods in Hits@10. This is because AsyncET ranks the
candidates using isotropic distance measurement in the embedding
space, while CET and TET use fully connected layers. Consequently,
AsycnET is useful for filtering unlikely candidates in the KGET
task with a much smaller complexity and faster inference than
classification-based methods. Recent methods, such as TET, CET,
AttET, RACE2T, ConnectE-MRGAT, adopt attention mechanisms
to obtain context-aware entity embeddings to predict the missing
types. AsyncET encodes entitiesâ€™ attributes through auxiliary rela-
tions and asynchronous representation learning. TET incorporated
additional textual information by adopting a pre-trained BERT [ 3]
encoder to encode entities and achieved the best performance in
MRR among all methods.
3272AsyncET: Asynchronous Representation Learning for Knowledge Graph Entity Typing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 5: Time and memory complexity of KGET methods. FLOPs and memory usage are calculated based on the optimal
parameters on the FB15kET dataset.
FB15kET
YAGO43kET
Mo
del Time complexity Memory complexity Infer. FLOPs Model Size Infer. FLOPs Model Size
Conne
ctE [32] ğ‘‚(ğ‘‘2+ğ‘‡ğ‘‘)ğ‘‚((ğ¸+ğ‘…+ğ‘‡)ğ‘‘+ğ‘‘2) 1.51M (1.05x) 16.06MB (1.01x) 18.88M (1.04x) 70.20MB (1.00x)
CompGCN [23] ğ‘‚(ğ¿ğ¸ğ‘…ğ‘‘2+ğ‘‡ğ‘‘)ğ‘‚((ğ¸+ğ‘…+ğ‘‡)ğ‘‘+ğ¿ğ‘‘2) 3.10M (2.16x) 16.06MB (1.01x) 19.40M (1.07x) 70.20MB (1.00x)
CET [19] ğ‘‚(ğ¸ğ‘…ğ‘‡ğ‘‘)ğ‘‚((ğ¸+ğ‘…+2ğ‘‡)ğ‘‘) 59.44M (41.46x) 21.64MB (1.36x) 302.04M (16.7x) 106.19MB (1.52x)
TET [10] - -â‰ˆ142.33M (99.53x) 29.91MB (1.88x) â‰ˆ151.12M (8.36x) 121.89MB (1.74x)
A
syncET (Ours) ğ‘‚(ğ‘‡ğ‘‘)ğ‘‚((ğ¸+ğ‘…+ğ‘‡+ğ‘ƒ)ğ‘‘) 1.43M (1x) 15.95MB (1x) 18.08M (1x) 70.05MB (1x)
4.2.2 Link Prediction Results. One significant advantage of the
embedding-based methods that learn a joint entity and type embed-
ding space is they can be easily applied to other downstream tasks.
We evaluate AsyncET in the link prediction task, where the valid
entities need to be ranked as high as possible given a query (ğ‘’1,ğ‘Ÿ,?),
to see whether the link prediction capability of the entity embed-
dings can be improved through the asynchronous representation
learning framework. Table 4 shows the link prediction results of
the baseline KGE methods and AsyncET using TransE, RotatE, and
CompoundE as the scoring functions, respectively, on FB15K [ 2]. In
the table, we observe that AsyncET can achieve consistent perfor-
mance improvement across all baselines in all evaluation metrics.
While CompoundE [ 5] is already a powerful baseline in the link
prediction task, the asynchronous representation learning frame-
work can further improve its performance. The results demonstrate
that the entity embeddings in AsyncET can not only perform well
on the KGET task, but also have a better expressiveness in link
prediction by infusing the type information asynchronously.
4.3 Complexity Analysis
We conduct complexity analysis to evaluate the inference time and
the number of model parameters for four representative methods in
Table 5, namely ConnectE, CompGCN, CET, and TET and compare
them with our proposed method. We use the number of inference
floating point operations (FLOPs) to reflect the hardware-invariant
inference speed of the methods. ğ‘‘,ğ¸,ğ‘…,ğ‘‡, andğ‘ƒdenote the em-
bedding dimension, numbers of entities, relations, entity types,
and auxiliary relations, respectively. For GNN-based methods, ğ¿
is the number of GNN layers. We calculate the inference FLOPs
and model sizes under ğ‘‘=200and the optimal hyper-parameter
settings of each method. The table shows that AsyncET is the most
efficient method in terms of both inference speed and number of
model parameters on both datasets. ConnectE is also an embedding-
based method, but it tries to learn additional model parameters to
connect the entity and type subspaces. Thus, its time complexity
and number of model parameters are proportional to ğ‘‘2, which are
larger than AsyncET. The complexity of GNN-based methods is
highly correlated with the number of neighbors for each entity. As
a result, the inference time complexity is proportional to ğ¸ğ‘…, which
is highly inefficient. For example, CET requires 41.46 times more
FLOPs on FB15kET and 16.7 times more FLOPs on YAGO43kET
than AsyncET to predict a single query. TET adopted a combination
of multiple transformer architectures so it is challenging to trace the
theoretical time and memory complexity and provide an accurateestimation. In addition, the number of inference FLOPs for TET
varies significantly across different inputs. Thus, we estimate the
amortized numbers of inference FLOPs and model sizes for TET on
the two datasets in Table 5. The results demonstrate that AsyncET
is much more lightweight and efficient compared to TET. The sim-
plicity of embedding-based models, such as AsyncET, in terms of
the model design, enhances the transparency of our method. In ad-
dition, AsyncET can achieve comparable performance to the SOTA
classification-based models while having a much smaller model size
and faster inference. Thus, AsychET is a scalable and explainable
solution for the KGET task.
4.4 Ablation Study
To analyze the effectiveness of asynchronous representation learn-
ing and different entity type grouping approaches, we conduct an
ablation study in Table 6, where the MRR performance of several
methods for two datasets is reported. We compare the following:
â€¢Synchronous vs. asynchronous representation learning;
â€¢Single relation hasType vs. multiple auxiliary relations;
â€¢Different KGE scoring functions.
As shown in the table, asynchronous representation learning
consistently outperforms synchronous representation learning. The
performance improvement of asynchronous training when using
only one auxiliary relation, hasType, is not significant since the sec-
ond stage of the embedding learning is trained on a single-relational
graph. When there are multiple auxiliary relations, mixing typing
triples with original factual triples in embedding training using the
synchronous framework yields poor performance. This could be
attributed to the fact that the original factual relations possess dif-
ferent semantic meanings and patterns from the artificial auxiliary
relations. The performance improves significantly when we decom-
pose the training process into two stages under the asynchronous
framework.
When using multiple auxiliary relations to model the typing re-
lationship, the bijective assignment works well in the dataset with
fewer entity types, e.g. FB15k-ET. However, it performs worse than
the efficient assignment on the dataset with more entity types, e.g.
YAGO43kET. In datasets with many entity types, the bijective as-
signment introduces larger amounts of new parameters, making the
embedding training more difficult to converge. The efficient assign-
ment can achieve the best performance on YAGO43kET, showing
that such an assignment can capture the context in the KG and sim-
ilarities among entity types well. Surprisingly, the taxonomy-based
3273KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yun-Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo
Table 6: Ablation studies on the performance improvement of adopting asynchronous representation learning and auxiliary
relations. The best performance in each column is labeled in boldface. The second best performance in each column is
underlined.
FB15kET YAGO43kET
Training Aux. Rel. TransE RotatE CompoundE TransE RotatE CompoundE
Syn. hasType 0.618 0.632 0.640 0.427 0.462 0.480
Syn.ğ‘-taxonomy 0.545 0.550 0.603 - - -
Syn.ğ‘-efficient 0.565 0.564 0.625 0.418 0.438 0.455
Asyn. hasType 0.621 0.624 0.638 0.443 0.458 0.474
Asyn.ğ‘-taxonomy 0.633 0.641 0.664 - - -
Asyn.ğ‘-efficient 0.654 0.661 0.682 0.452 0.471 0.492
(a)ğ‘ğ‘ = 256 steps (b)ğ‘ğ‘ = 4,096 steps (c)ğ‘ğ‘ = 32,768 steps
Figure 4: Validation loss of the two representation learning stages.
assignment does not perform well on both datasets. One possible
reason is that grouping entity types based on only the first layer
of the taxonomy ignores the granularities of different entity-type
patterns. Such a problem might be solved by considering more
layers in the taxonomy.
4.5 Alternating Frequency in AsyncET
In asynchronous representation learning, we alternate between
the link prediction stage (Stage 1) and the entity type prediction
stage (Stage 2) after a few stochastic gradient descent steps. The
link prediction loss in Eq. (6) and the entity type prediction loss
in Eq. (8) are minimized in Stage 1 and Stage 2, respectively. The
training process begins with Stage 1 and switches to Stage 2 after
ğ‘ğ‘ ,1stochastic gradient descent steps. Similarly, it conducts ğ‘ğ‘ ,2
stochastic gradient descent steps and then switches back to Stage
1. We call one entire cycle of performing Stage 1 and Stage 2 once
an alternating round. In our experiments, we set ğ‘ğ‘ ,1=ğ‘ğ‘ ,2=ğ‘ğ‘ 
to denote the number of training steps before switching from one
training stage to the other.
We plot the validation loss curves under different ğ‘ğ‘ in Fig. 4. The
figures show that the validation loss can be reduced when the two
training stages alternate more frequently, i.e. under a smaller ğ‘ğ‘ . In
addition, both the link prediction and entity type prediction loss are
consistently improved as the training progresses. In other words, the
two training stages are mutually beneficial. When setting a large ğ‘ğ‘ ,Table 7: Top 3 predicted entity types by AsyncET for entities
in YAGO43kET. Groundtruth is marked in boldface.
Entity Top 3 Type Predictions
Mihail MajearuRomanian footballers
Alkmaar players
People from Glasgow
David CrossJewish actors
21st-century American actors
American humorists
ZhejiangCities in Zhejiang
Provincial capitals in China
Administrative divisions of China
where the two training stages only alternate once throughout the
training, as shown in Fig. 4 (c), the loss curves drop much slowlier
than the other two cases. Thus, setting a smaller ğ‘ğ‘ and encouraging
more interactions between two training stages is helpful for finding
a global optimal and model convergence.
4.6 Qualitative Analysis
We show some examples of predicted types in Table 7. In all three
examples, the groundtruth ranks among the top three. In the first
3274AsyncET: Asynchronous Representation Learning for Knowledge Graph Entity Typing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
example, for entity Mihail Majearu, the model can successfully rank
the groundtruth at the top one. In addition, the top three predicted
types are all persons, and the 2nd prediction is also related to
football. In the second example, for entity David Cross, who is
a comedian, the model can rank the groundtruth at the top one
once again. The other two entity types are also relevant to the
entity. They are valid types. In the last example, for entity Zhejiang,
although the groundtruth only ranks as the third, the first two
predicted types are both relevant to the entities. Note that Zhejiang
is a province instead of a city in China. The granularity of the entity
is not predicted correctly in the top two choices. The examples
also support the reason that AsyncET performs well in terms of
Hits@10.
5 Conclusion and Future Work
In this paper, we propose to introduce multiple auxiliary relations
to helping modeling entity-type relationships in the embedding
space. Two methods to group similar types are proposed and com-
pared. The efficient entity type grouping algorithm is recommended
among the two since it is scalable to datasets containing many entity
types. In addition, we propose asynchronous representation learn-
ing to obtain better entity and type embeddings by alternatively
predicting missing links and types. Experimental results demon-
strate that AsyncET outperforms all embedding-based methods.
It also outperforms the SOTA in Hits@10 and achieves compa-
rable performance in other metrics with a much lower inference
complexity and fewer model parameters. As future extensions, we
will investigate how the sparsity of the types affects the perfor-
mance of KGET methods. We aim to not only develop a time- and
parameter-efficient model, but achieve minimal performance degra-
dation when trained with fewer samples.
Acknowledgments
The authors acknowledge the Center for Advanced Research Com-
puting (CARC) at the University of Southern California for pro-
viding computing resources that have contributed to the research
results reported within this publication. URL: https://carc.usc.edu.
References
[1]Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on
Management of data. 1247â€“1250.
[2]Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. Advances in neural information processing systems 26 (2013).
[3]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). Association for Computational Linguistics, 4171â€“4186.
[4]Xiou Ge, Ali Mousavi, Edouard Grave, Armand Joulin, Kun Qian, Benjamin
Han, Mostafa Arefiyan, and Yunyao Li. 2024. Time Sensitive Knowledge Editing
through Efficient Finetuning. arXiv preprint arXiv:2406.04496 (2024).
[5]Xiou Ge, Yun Cheng Wang, Bin Wang, and C.-C. Jay Kuo. 2023. Compounding
Geometric Operations for Knowledge Graph Completion. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Linguistics, 6947â€“6965.
[6]Xiou Ge, Yun Cheng Wang, Bin Wang, C-C Jay Kuo, et al .2023. TypeEA: type-
associated embedding for knowledge graph entity alignment. APSIPA Transac-
tions on Signal and Information Processing 12, 1 (2023).
[7]Xiou Ge, Yun Cheng Wang, Bin Wang, C-C Jay Kuo, et al .2024. Knowledge
Graph Embedding: An Overview. APSIPA Transactions on Signal and InformationProcessing 13, 1 (2024).
[8]Xiou Ge, Yun Cheng Wang, Bin Wang, C-C Jay Kuo, et al .2024. Knowledge
Graph Embedding with 3D Compound Geometric Transformations. APSIPA
Transactions on Signal and Information Processing 13, 1 (2024).
[9]Xiou Ge, Yun-Cheng Wang, Bin Wang, and CC Jay Kuo. 2022. CORE: A knowledge
graph entity type prediction method via complex space regression and embedding.
Pattern Recognition Letters 157 (2022), 97â€“103.
[10] Zhiwei Hu, Victor Gutierrez-Basulto, Zhiliang Xiang, Ru Li, and Jeff Pan. 2022.
Transformer-based Entity Typing in Knowledge Graphs. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 5988â€“6001.
[11] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021.
A survey on knowledge graphs: Representation, acquisition, and applications.
IEEE Transactions on Neural Networks and Learning Systems (2021).
[12] Hailong Jin, Lei Hou, Juanzi Li, and Tiansi Dong. 2019. Fine-grained entity typing
via hierarchical multi graph convolutional networks. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
4969â€“4978.
[13] C-C Jay Kuo and Azad M Madni. 2022. Green learning: Introduction, examples
and outlook. Journal of Visual Communication and Image Representation (2022),
103685.
[14] Yu Lin, Saurabh Mehta, Hande KÃ¼Ã§Ã¼k-McGinty, John Paul Turner, Dusica Vidovic,
Michele Forlin, Amar Koleti, Dac-Trung Nguyen, Lars Juhl Jensen, Rajarshi Guha,
et al.2017. Drug target ontology to classify and integrate drug discovery data.
Journal of biomedical semantics 8, 1 (2017), 1â€“16.
[15] Farzaneh Mahdisoltani, Joanna Biega, and Fabian Suchanek. 2014. Yago3: A
knowledge base from multilingual wikipedias. In 7th biennial conference on inno-
vative data systems research. CIDR Conference.
[16] Changsung Moon, Steve Harenberg, John Slankas, and Nagiza F Samatova. 2017.
Learning contextual embeddings for knowledge graph completion. (2017).
[17] Changsung Moon, Paul Jones, and Nagiza F Samatova. 2017. Learning entity
type embeddings for knowledge graph completion. In Proceedings of the 2017
ACM on conference on information and knowledge management. 2215â€“2218.
[18] Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. 2019. Learn-
ing Attention-based Embeddings for Relation Prediction in Knowledge Graphs.
InProceedings of the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 4710â€“4723.
[19] Weiran Pan, Wei Wei, and Xian-Ling Mao. 2021. Context-aware Entity Typing in
Knowledge Graphs. In Findings of the Association for Computational Linguistics:
EMNLP 2021. Association for Computational Linguistics, 2240â€“2250.
[20] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan
Titov, and Max Welling. 2018. Modeling relational data with graph convolutional
networks. In European semantic web conference. Springer, 593â€“607.
[21] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl-
edge Graph Embedding by Relational Rotation in Complex Space. In International
Conference on Learning Representations.
[22] ThÃ©o Trouillon, Johannes Welbl, Sebastian Riedel, Ã‰ric Gaussier, and Guillaume
Bouchard. 2016. Complex embeddings for simple link prediction. In International
conference on machine learning. PMLR, 2071â€“2080.
[23] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. 2020.
Composition-based Multi-Relational Graph Convolutional Networks. In Interna-
tional Conference on Learning Representations.
[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[25] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. International Confer-
ence on Learning Representations (2018).
[26] Yun Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo. 2023. GreenKGC: A
Lightweight Knowledge Graph Completion Method. In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, 10596â€“10613.
[27] Yun-Cheng Wang, Jintang Xue, Chengwei Wei, and C-C Jay Kuo. 2023. An
overview on generative ai at scale with edge-cloud computing. IEEE Open Journal
of the Communications Society (2023).
[28] Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Zhenxi Lin, and Yefeng
Zheng. 2021. OntoEA: ontology-guided entity alignment via joint knowledge
graph embedding. arXiv preprint arXiv:2105.07688 (2021).
[29] Yadollah Yaghoobzadeh, Heike Adel, and Hinrich SchÃ¼tze. 2017. Noise Mitigation
for Neural Entity Typing and Relation Extraction. In Proceedings of the 15th Con-
ference of the European Chapter of the Association for Computational Linguistics:
Volume 1, Long Papers. Association for Computational Linguistics, 1183â€“1194.
[30] Yadollah Yaghoobzadeh and Hinrich SchÃ¼tze. 2015. Corpus-level Fine-grained
Entity Typing Using Contextual Information. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 715â€“725.
3275KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yun-Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo
[31] Yunxiang Zhao, Jianzhong Qi, Qingwei Liu, and Rui Zhang. 2021. Wgcn: graph
convolutional networks with weighted structural features. In Proceedings of
the 44th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 624â€“633.
[32] Yu Zhao, Anxiang Zhang, Ruobing Xie, Kang Liu, and Xiaojie Wang. 2020.
Connecting embeddings for knowledge graph entity typing. arXiv preprint
arXiv:2007.10873 (2020).
[33] Yu Zhao, Han Zhou, Anxiang Zhang, Ruobing Xie, Qing Li, and Fuzhen Zhuang.
2022. Connecting Embeddings Based on Multiplex Relational Graph Attention
Networks for Knowledge Graph Entity Typing. IEEE Transactions on Knowledge
and Data Engineering (2022).
[34] Jianhuan Zhuo, Qiannan Zhu, Yinliang Yue, Yuhong Zhao, and Weisi Han. 2022.
A Neighborhood-Attention Fine-grained Entity Typing for Knowledge Graph
Completion. In Proceedings of the Fifteenth ACM International Conference on Web
Search and Data Mining. 1525â€“1533.
[35] Changlong Zou, Jingmin An, and Guanyu Li. 2022. Knowledge graph entity type
prediction with relational aggregation graph attention network. In European
Semantic Web Conference. Springer, 39â€“55.
A Appendix
A.1 More details on the scoring functions
The scoring functions of three baseline KGE methods used in the
experiments are provided below.
â€¢TransE [2]:
ğ‘“(ğ’†head,ğ’“,ğ’†tail)=ğ›¾âˆ’âˆ¥ğ’†head+ğ’“âˆ’ğ’†tailâˆ¥,
whereğ›¾is the margin. Itâ€™s a hyperparameter that can be
tuned.
â€¢RotatE [21]:
ğ‘“(ğ’†head,ğ’“,ğ’†tail)=ğ›¾âˆ’âˆ¥ğ’†headâ—¦ğ’“âˆ’ğ’†tailâˆ¥,
whereâ—¦denotes the rotation operation in the complex em-
bedding space.
â€¢CompoundE [5]:
ğ‘“(ğ’†head,ğ’“,ğ’†tail)=ğ›¾âˆ’âˆ¥TrÂ·R(ğœƒr)Â·SrÂ·eheadâˆ’etailâˆ¥,
where Tr,R(ğœƒr), and Srdenote the translation, rotation, and
scaling operations in CompoundE, respectively.
However, it is worthwhile to point out that AsyncET is not
limited to these three scoring functions. AsyncET can be integrated
with all existing KGE methods as long as the scoring functions are
well-defined.A.2 Hyper-parameters settings
For both datasets, we select the best hyper-parameters from a cer-
tain search space under the embedding dimension ğ‘‘=200based
on the performance of the validation set for entity type prediction.
The search space is given below:
â€¢Number of negative samples ğ‘›ğ‘›ğ‘’ğ‘”âˆˆ{128,256,512â˜…â‹„};
â€¢Learning rate ğ‘™ğ‘Ÿâˆˆ{0.01â‹„,0.001â˜…,0.0001};
â€¢Softmax temperature ğ›¼âˆˆ{0.5,1.0â˜…â‹„};
â€¢Marginğ›¾âˆˆ{8.0â˜…,12.0,16.0,20.0â‹„}.
The hyper-parameter settings for FB15k-ET and YAGO43k-ET
are marked with â˜…andâ‹„, respectively. We also search for the optimal
ğ‘ğ‘ to alternate from one training stage to the other in asynchronous
representation learning based on the validation MRR. Fig. 5 is an
example plot of validation MRR on YAGO43kET using TransE as the
scoring function under different ğ‘ğ‘ . We conduct experiments using
ğ‘ğ‘ =1,16,256,4096,65536, respectively, and decide the optimal
setting for each dataset. We see that the validation MRR is better
whenğ‘ğ‘ is smaller, which means the two training stages alternate
more frequently. The result is consistent with our analysis in Sec. 4.5.
We selectğ‘ğ‘ =16for both datasets. All experiments are conducted
using one NVIDIA Tesla P100 GPU.
Figure 5: Validation MRR as a function of the number of
training steps before alternating from one training stage to
the other in asynchronous representation learning.
3276