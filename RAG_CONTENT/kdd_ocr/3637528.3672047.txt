DPHGNN: A Dual Perspective Hypergraph Neural Networks
Siddhant Saxena
IIT Delhi
New Delhi, India
siddhantsaxenaphy@gmail.comShounak Ghatak
IIIT Delhi
New Delhi, India
shounak19109@iiitd.ac.inRaghu Kolla
Meesho
Bangalore, India
raghu.kolla@meesho.com
Debashis Mukherjee
Meesho
Bangalore, India
debashis.mukherjee@meesho.comTanmoy Chakraborty
IIT Delhi
New Delhi, India
tanchak@iitd.ac.in
ABSTRACT
Message passing on hypergraphs has been a standard framework for
learning higher-order correlations between hypernodes. Recently-
proposed hypergraph neural networks (HGNNs) can be categorized
into spatial and spectral methods based on their design choices. In
this work, we analyze the impact of change in hypergraph topology
on the suboptimal performance of HGNNs and propose DPHGNN, a
novel dual-perspective HGNN that introduces equivariant operator
learning to capture lower-order semantics by inducing topology-
aware spatial and spectral inductive biases. DPHGNN employs a
unified framework to dynamically fuse lower-order explicit feature
representations from the underlying graph into the super-imposed
hypergraph structure. We benchmark DPHGNN over eight bench-
mark hypergraph datasets for the semi-supervised hypernode clas-
sification task and obtain superior performance compared to seven
state-of-the-art baselines. We also provide a theoretical framework
and a synthetic hypergraph isomorphism test to express the power
of spatial HGNNs and quantify the expressivity of DPHGNN beyond
the Generalized Weisfeiler Leman (1-GWL) test. Finally, DPHGNN
was deployed by our partner e-commerce company, Meesho for the
Return-to-Origin (RTO) prediction task, which shows 7%higher
macro F1-Score than the best baseline.
CCS CONCEPTS
â€¢Computing methodologies â†’Semi-supervised learning settings ;
Semantic networks.
KEYWORDS
Graph neural network; Hypergraph neural networks; Topological
deep learning; RTO prediction
ACM Reference Format:
Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, and Tan-
moy Chakraborty. 2024. DPHGNN: A Dual Perspective Hypergraph Neural
Networks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672047Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3672047
1 INTRODUCTION
In recent years, the use of spatial and spectral message-passing
neural networks (MPNN) [ 37,43] on graph topology has grown
exponentially to solve various downstream tasks such as node label
classification, link prediction, and graph classification [ 26]. Graph
Neural Networks (GNNs) achieve exceptional performance for rep-
resentation learning on graph-structured data. GNNs improve con-
textual feature representation of nodes in the graph via layer-wise
spatial feature aggregation and message propagation. A wide range
of applications involving graph-based semi-supervised node classi-
fication, such as molecular property prediction [ 44], topic modeling
of research papers from a scientific citation network [ 23], user-
item recommendations in e-commerce networks [ 11], have been
explored. However, the graph structure limits modeling the inter-
actions to pair-wise node connections and, therefore, is restricted
to capturing only the lower-order correlation and relationships
between entities.
Hypergraphs provide a flexible mechanism to model higher-
order data correlation and complex relationships by allowing hy-
peredges of two or more hypernodes. Many real-world applications
of semi-supervised node classification involve higher-order rela-
tion modeling [ 14], such as academic citation networks consisting
of hypernodes as scientific authors and hyperedges as coauthor-
ship relations among authors. [ 15] used hypergraph structure to
describe quantum optical experiments. [ 20] improved collabora-
tive filtering, and [ 2] used session-based recommendations using
higher-order relations. [ 27] considered semi-dynamic hypergraphs
for 3d pose estimation. Recent studies proposed models for Hyper-
graph Neural Networks (HGNNs) [ 12], Hypergraph Convolution
and Hypergraph Attention [ 3]. However, despite the flexibility and
advances of message-passing neural networks on hypergraphs,
there exist some topological and design-related challenges, as men-
tioned below:
â€¢Performance gap between spatial and spectral HGNNs with mod-
eling data into hypergraph topology.
â€¢Effects of over-smoothing and feature collapse in hypergraph
topology with resource-constrained setting (e.g., class imbalance,
sparse incidence structure, a limited set of features, etc.) .
â€¢Exploiting the underlying graph structure to explicitly incorpo-
rate information from the lower-order structure to the higher-
order message-passing framework.
 
2548
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, and Tanmoy Chakraborty
Notation Description Notation Description
ğº=(ğ‘‰,ğ¸) An undirected graph with a set of nodes ğ‘‰and a set of
edgesğ¸ğ»ğº=(ğ‘‰,ğœ‰)An undirected hypergraph with a set of hyper-
nodesğ‘‰and a set of hyperedges ğœ‰
ğ´ Adjacency Matrix ğ» Hypergraph incidence matrix
ğ·ğ‘£ Node degree matrix ğ·ğ‘£ Hypernode degree matrix
ğ¿ğ‘ ğ‘¦ğ‘š Symmetric Laplacian Matrix ğ·ğ‘’ Hyperedge degree matrix
ğºğ‘,ğ´ğ‘ Clique decomposition graph and adjacency Î”ğ»ğºğ‘ğ‘ HGNN Laplacian matrix
ğºâˆ—,ğ´âˆ— Star decomposition graph and adjacency Î”ğ‘ ğ‘¦ğ‘š Symmetric Laplacian matrix
ğºâ„ğ‘¦ğ‘,ğ´â„ğ‘¦ğ‘ HyperGCN decomposition graph and adjacency Î”ğ‘Ÿğ‘¤ Random-walk Laplacian matrix
Table 1: A summary of notations used throughout the paper.
To address these challenges, we propose DPHGNN (Dual Perspective
Hypergraph Neural Network), a mixed spectral and spatial learn-
ing framework that improves feature representation learning on
resource-constrained hypergraph settings. DPHGNN adopts â€“ (i) a
dual-layered feature update mechanism, (ii) a static update layer
to provide spectral inductive biases and lower-order relational fea-
tures to update the static feature matrix of hypernodes, and (iii) a
dynamic update layer to fuse the explicitly aggregated features from
the underlying graph topology in the hypergraph spatial message
propagation. Moreover, with extensive empirical and theoretical
analyses, we show that DPHGNN can tackle the above-mentioned
challenges and produce improved feature representations for the
downstream hypernode classification task.
We summarize our major contributions below1:
â€¢DPHGNN introduces a novel message propagation framework
that explicitly diffuses lower-order graph features to super-imposed
hypergraph structure.
â€¢We introduce equivariant operator learning (EOL) over TAA and
SIB inductive biases. EOL creates an information-rich feature
mixture, rather than cold-start with HG initial features. This also
constitutes maximally expressive, ğ‘˜-order equivariant layers in
DPHGNN.
â€¢We perform extensive empirical analysis on DPHGNNâ€™s gener-
alized performance over eight benchmark datasets, and a new
isomorphic HG classification task. We formalize strong math-
ematical characterization on 3-GWL expressivity of DPHGNN,
automorphism groups, and EOL.
â€¢We introduce CO-RTO, a new real-world application to tackle
the challenge of the RTO problem in e-commerce. DPHGNN
being robust to variations in topological constraints, beats HGNN
baselines with a large margin.
2 PRELIMINARIES, BACKGROUND AND
RELATED WORK
Letğ»ğº=(ğ‘‰,ğœ‰)denote an undirected hypergraph without self-
loops, where ğ‘‰is a set of hypernodes and a hyperedge ğ‘’âˆˆğœ‰is
composed of a set of nodes; therefore, {ğ‘’âŠ‚ğ‘‰}âˆ§{ğ‘’â‰ âˆ…}. The
incident edges of hypernode ğ‘–are denoted by ğ¸ğ‘–={ğ‘’âˆˆğœ‰|ğ‘–âˆˆğ‘’}.
Table 1 summarizes the notations used throughout the paper.
Graph Neural Networks. Graph Neural Networks (GNNs) have
been successful by following the message-passing neural network
(MPNN) paradigm to update representations of nodes in the graph
1The source code of DPHGNN is available at https://github.com/mr-siddy/DPHGNN.
The CO-RTO dataset canâ€™t be released due to privacy concerns.topology by aggregating the feature information from neighboring
nodes. A simple message propagation in GNNs can be formulated
as follows: spectral in
Ë†ğ‘¥(ğ‘™)
ğ‘£=Update(ğ‘™)
ğ‘¥(ğ‘™)
ğ‘£,Aggr(ğ‘™)n
ğ‘¥(ğ‘™âˆ’1)
ğ‘¢âˆ€ğ‘¢âˆˆğ‘(ğ‘£)o
where Aggr(ğ‘™)aggregates node embeddings of the neighborhood
ğ‘(Â·)of nodeğ‘£at layerğ‘™âˆ’1, and Update(ğ‘™)assigns aggregated
embedding to node ğ‘£at layerğ‘™.ğ‘(ğ‘£)is the neighbors of node ğ‘£, and
ğ‘¥(ğ‘™âˆ’1)
ğ‘¢ is the representation of node ğ‘¢at(ğ‘™âˆ’1)layer. The foundation
message-passing GNN models include GCN [ 21], GraphSAGE [ 16],
GIN [38], JK-GCN [39] and GAT [36].
Spatial Hypergraph Networks. To generalize MPNN over hy-
pergraph, several attempts have been made in which spatial HGNNs
tend to aggregate the hypernode embeddings from a hyperedge,
aggregate the embedding from neighboring hyperedges and up-
date the hypernode feature information [ 10,13]. A simple spatial
message propagation in HGNNs can be formulated as follows:
Ë†ğ‘¥(ğ‘™)
ğ‘£=Update(ğ‘™)
ğ‘¥(ğ‘™)
ğ‘£,
Aggr(ğ‘™)nn
Aggr(ğ‘™âˆ’1)
ğ‘¥(ğ‘™âˆ’1)
ğ‘¢
âˆ€ğ‘¢âˆˆğ‘’o
âˆ€ğ‘’âˆˆğ¸ğ‘–o
(1)
The benchmark models following spatial HGNN mechanism include
HGNNP [ 13], HyperSAGE[ 2], HNHN [ 10], HAIN [ 4] and the family
of UniGNNs [18].
Spectral Hypergraph Networks. An HGNN layer uses spectral
convolution to learn node/edge representation via approximating
each cluster to a clique and requires quadratic computation. The
HyperGCN layer [ 41] considers a linear number of edges resulting
in a reduction in training times and uses a Laplacian operator for
hypergraphs to approximate the hyperedges followed by a regular
graph convolution operation on the resulting graph. MPNN-R [ 40]
considers hyperedges as new vertices ğ‘‰={ğ‘‰âˆªğœ‰}and presents
the hypergraph with a |ğ‘‰|Ã—|ğœ‰|matrix. However, it fails to describe
the intersections between hyperedges, resulting in a loss of com-
plex semantic relations. These approaches can be implemented as
smoothing features with trainable hypergraph Laplacian operators.
Some frequently used Laplacian operators include HGNN Laplacian
matrix Î”ğ»ğºğ‘ğ‘ =ğ·âˆ’1/2
ğ‘£ğ»ğ·âˆ’1ğ‘’ğ»ğ‘‡ğ·âˆ’1/2
ğ‘£, Symmetric Laplacian ma-
trixÎ”ğ‘ ğ‘¦ğ‘š=ğ¼âˆ’ğ·âˆ’1/2
ğ‘£ğ»ğ·âˆ’1ğ‘’ğ»ğ‘‡ğ·âˆ’1/2
ğ‘£, Random walk Laplacian
matrix Î”ğ‘Ÿğ‘¤=ğ¼âˆ’ğ·âˆ’1ğ‘£ğ»ğ·âˆ’1ğ‘’ğ»ğ‘‡[30] (see Table 1 for notations).
 
2549DPHGNN: A Dual Perspective Hypergraph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1 DPHGNN: Dual Perspective Hypergraph Neural Net-
work
Input Hypergraph incidence matrix: ğ»âˆˆğ‘…ğ‘šÃ—ğ‘›
Hypernode feature matrix: ğ‘‹ğ»ğº
Target labels:{ğ‘¦}train
Epochs:ğ‘’
Num Layers: ğ‘˜
Output Learned representations bğ‘‹HGfor downstream tasks.
fori=1toğ‘’+1epochs do
Project:ğ‘“(ğ‘¥):ğ‘‹ğ»ğºâ†’ğ‘‹â€²
ğ»ğºhidden dimension
Compute: Î”spectral = Î”ğ‘Ÿğ‘¤âŠ•Î”sym|;|Î”ğ»ğºğ‘ğ‘
Update hypernodes: ğ‘‹ğ»ğº spectral=ğœ ğ¼+ğœ†Î”spectralğ‘‹â€²
ğ»ğºğœƒ
Compute TAA: bğ‘¥ğœ…=Î£ğœ…âˆˆğ‘ğ›½bğ›¼ğ›½ğ‘Œğœ…,bğ‘¥ğ‘§=Î£ğ‘âˆˆğ‘ğ‘¥bğ›¼ğ‘‹ğ‘Œğ‘
Update hypernodes:
ğ‘‹eqv=ğ‘€ğ¿ğ‘ƒ1(Ë†ğ‘¥ğœ…|;|Ë†ğ‘¥ğ‘)âŠ™ğœ(ReLU(ğ‘€ğ¿ğ‘ƒ2(ğ‘‹ğ»ğº spectral)))
Update static features
ğ‘‹static=ğ‘‹eqv|;|ğ‘€ğ¿ğ‘ƒ3(ğ‘‹HG)
forlinğ‘˜âˆ’1layers do
Update hypernodes as:
Feature Fusion:
ğ‘‹Fused=
ğ»ğ‘‡ğ·âˆ’1/2
ğ‘£ğ‘‹static+ğ·âˆ’1ğ‘£ğœ™mask{ğ´ğ‘‹ğºâˆ—}
ğ‘‹DPHGNN =ğœ
ğ‘‹static+ğ»ğ·âˆ’1ğ‘’ğ‘‹FusedÎ˜
Update hypernodes as:
bğ‘‹ğ»
ğº=ğœ
ğ·âˆ’1/2
ğ‘£ğ‘‹DPHGNNÂ¯ğ·âˆ’1/2
ğ‘’Â·ğ·âˆ’1ğ‘’ğ»ğ‘‡ğ·âˆ’1/2
ğ‘£ğ‘‹DPHGNN Î˜
Compute cross-entropy loss between {ğ‘¦}train and predictionsn
bğ‘‹ğ»ğºo
train
Backpropagate loss and finetune parameter set using Adam optimizer
3 DUAL PERSPECTIVE HYPERGRAPH
NEURAL NETWORK (DPHGNN)
Our proposed DPHGNN architecture is presented in Figure 1 and
summarized in Algorithm 1. DPHGNN first induces various struc-
tural and spectral inductive biases to learn the underlying lower-
order relations and spectral features. Following this, a dynamic
feature fusion mechanism fuses explicit aggregated features from
specific graph topology with the hypergraph message passing. We
finetune different sub-layers in the static and dynamic induction
blocks with cross-entropy loss by making the convolution layer
an end-to-end differentiable pipeline. Specific building blocks are
explained in the remaining section.
3.1 Topology-Aware Attention (TAA)
A hypergraph can be decomposed into various substructures rep-
resenting underlying pairwise connections between hypernodes.
We carefully decompose it into three particular graph topologies
â€“ (i) Clique expansion ğºğ‘is used to learn the interconnection
of node-pair entities. (ii) Star expansion ğºâˆ—is used to generate
explicit supernodes utilized in the dynamic feature fusion module
by masking the synthetic supernodes and nodes already present in
the graph. This can be achieved by introducing a mask defined as,
ğœ™mask=1ifğ‘¥ğ‘–âˆˆğ‘‰âˆ—âˆªğ‘†âˆ—
0ifğ‘¥ğ‘–âˆˆ(ğ‘‰âˆ—âˆªğ‘†âˆ—)\ğ‘‰âˆ—
(iii)HyperGCN expansion ğºâ„ğ‘¦ğ‘is used to learn lower-order
graph functions, approximate the hypergraph learning functionsand update the feature representation using a single-layer MPNN
update. The choice of respective graph MPNN is made to maximize
the induction of feature updates in respective graph topologies with
the following rules:
ğ‘‹ğ‘=ğœh
ğ¼+ğ·âˆ’1
ğ‘£ğ´ğ‘
ğ‘‹ğœƒi
;ğ‘‹âˆ—=ğœh
ğ¼+ğ·âˆ’1
ğ‘£ğ´âˆ—
ğ‘‹ğœƒi
ğ‘‹â„ğ‘¦ğ‘=ğœ
bğ·âˆ’1/2
ğ‘£bğ´â„ğ‘¦ğ‘bğ·âˆ’1/2
ğ‘£ğ‘‹ğœƒ
whereğ‘‹ğ‘,ğ‘‹âˆ—, andğ‘‹â„ğ‘¦ğ‘represent single-layer MPNN update of
nodes onğºğ‘,ğºâˆ—, andğºâ„ğ‘¦ğ‘, respectively. We also compute smoothened
features through graph Laplacian smoothing as ğ¿âˆ—=ğ·ğ‘£âˆ’ğ´âˆ—,
ğ¿ğ‘=ğ·ğ‘£âˆ’ğ´ğ‘,ğ¿â„ğ‘¦ğ‘=ğ·ğ‘£âˆ’ğ´â„ğ‘¦ğ‘onğºâˆ—,ğºğ‘, andğºâ„ğ‘¦ğ‘, respec-
tively. This allows aggregating feature representations induced
from different possible semantic relations among lower-order enti-
ties. Moreover, we adopt a cross-attention mechanism for graphs
[42] to generate topology-aware feature representations described
below.
3.1.1 TAA on Spatial Features. Letğ›½={ğ›½ğ‘–=ğ‘¥ğ‘–|âˆ€ğ‘¥ğ‘–âˆˆğœ™ğ‘šğ‘ğ‘ ğ‘˜(ğ‘‹âˆ—)},
ğ›¾={ğ›¾ğ‘—=ğ‘¥ğ‘—|âˆ€ğ‘¥ğ‘—âˆˆğ‘‹ğ‘}, andğœ…={ğœ…ğ‘˜=ğ‘¥ğ‘˜|âˆ€ğ‘¥ğ‘˜âˆˆğ‘‹â„ğ‘¦ğ‘}be the
sets of feature representation of decomposed graph topology ğºâˆ—,
ğºğ‘, andğºâ„ğ‘¦ğ‘. The topology-aware attention weights bğ›¼ğ›½ğ›¾would be
along with trainable attention parameters ğ›¿âˆˆR2ğ‘‘with â€œ;â€ denoting
concatenation operation. The updated feature representation for
ğºâ„ğ‘¦ğ‘is then obtained by,
ğ›¼ğ›½ğ›¾=ğœ
ğ›¿ğ‘‡[ğ‘Šğ›½|;|ğ‘Šğ›¾]
;bğ›¼ğ›½ğ›¾=exp
ğ›¼ğ›½ğ›¾
Ã
ğ‘âˆˆğ‘ğ›½exp
ğ›¼ğ›½ğ›¾;
bğ‘¥ğœ…=Î£ğ‘âˆˆğ‘ğ›½bğ›¼ğ›½ğ›¾ğ‘Šğœ…
3.1.2 TAA on Spectral Features. Letğ‘‹={ğ‘™ğ‘–=ğ‘¥ğ‘–|âˆ€ğ‘¥ğ‘–âˆˆğœ™ğ‘šğ‘ğ‘ ğ‘˜[ğ¿âˆ—âŠ—
ğ‘‹âˆ—]},ğ‘Œ={ğ‘šğ‘—=ğ‘¥ğ‘—|âˆ€ğ‘¥ğ‘—âˆˆ[ğ¿ğ‘âŠ—ğ‘‹ğ‘]}, andğ‘={ğ‘›ğ‘˜=ğ‘¥ğ‘˜|âˆ€ğ‘¥ğ‘˜âˆˆ
[ğ¿â„ğ‘¦ğ‘âŠ—ğ‘‹â„ğ‘¦ğ‘]}be the sets of Laplacian smoothen features of de-
composed topologies ğºâˆ—,ğºğ‘, andğºâ„ğ‘¦ğ‘, respectively. Similarly, the
attention-weighted features for ğ¿â„ğ‘¦ğ‘âŠ—ğ‘‹â„ğ‘¦ğ‘feature matrix would
be obtained by,
ğ›¼ğ‘‹ğ‘Œ=ğœ
ğ›¿ğ‘‡[ğ‘Šğ‘‹|;|ğ‘Šğ‘Œ]
;bğ›¼ğ‘‹ğ‘Œ=exp(ğ›¼ğ‘‹ğ‘Œ)Ã
ğ‘âˆˆğ‘ğ‘‹exp(ğ›¼ğ‘‹ğ‘Œ);
bğ‘¥ğ‘=Î£ğ‘âˆˆğ‘ğ‘‹bğ›¼ğ‘‹ğ‘Œğ‘Šğ‘
Here,ğ‘Šrepresents a linear transformation matrix. After the topo-
logical attention-based re-weighting of the graphâ€™s structural and
spectral features, we elapse them to the Feature Mixer Module.
3.2 Feature Mixture Module
The sub-optimal performance of existing HGNNs, with the change
in hypergraph topology, naturally motivates us to construct a gen-
eralized learning mechanism. To this end, we introduce a feature
mixture module that first aggregates the spectral inductive biases
(SIB) and then produces a mixture of features satisfying the follow-
ing desiderata: TAA concatenation operation (i) provides spatial
and spectral features of lower-order graph topology for more rich
semantic feature representations; (ii) prevents over-smoothing of
relevant information at the time of message passing over sparse
hypergraph topology. SIB concatenation operation provides sym-
metrical random walk-based HGNN Laplacian smoothen features,
 
2550KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, and Tanmoy Chakraborty
Super -nodes Set: {S1, S2, S3 ...  Sn}
g(x)
h(x)
Featur e Mixtur e Module
Feature Matrix w/ Static Inductive BiasesGStar GClique GHypergcnHypergraph: HG1-GNN
Spectral Inductive BiasesÎ» (LHGNN * X) Î» (Lrw * X) Î» (Lsym * X)Topologically Aware
 Attention
(L_GNN)X
(L_GNN)X
(L_GNN)Xf(x)
Dynamic Featur e Fuser
S
S1
S2V -> E
E -> VS2e1
e2
e3S -> EV -> ES -> E
S -> E
E -> VE -> V
V -> EMessage Flow Chart
V->E Vertex to Edge
S->E Super-node to Edge
E->V Edge to V ertexV_mask: False
V_mask: Truesoftmax
softmax MLP1concat
concatMLP2ReLUÏƒ
X_HGMLP3
concat
X_HGX_TAA
X_SIBX'_HG
Figure 1: A schematic diagram of our proposed architecture, DPHGNN. Left: Hypergraph decomposition and topology-aware
Attention (TAA) mechanism. Middle: Feature Mixture that generates static features by incorporating spectral inductive biases
from hypergraph Laplacian smoothing and TAA. Right: Dynamic feature fusion (DFF) that fuses explicitly learned graph
embedding in supernodes with the hypergraph message-passing module.
inherently giving unique identifiers to hypernodes. This helps us
break automorphism groups.
Aggregating Spectral Inductive Biases DPHGNN leverages
smoothened features from symmetrical random walk-based HGNN
Laplacian, denoted by Î”spectral = Î”rwâŠ•Î”sym|;|Î”HGNN. The
SIB block is inherently inspired by different approaches to for-
mulating the hypergraph Laplacian, where the smoothened fea-
turesğ‘‹HG spectral=ğœ
ğ‘‹+ğœ†Î”spectralğ‘‹, and the concatenation op-
eration|;|is crucial to prevent the nullification of spectral fea-
tures. This is because the element-wise sum of Laplacian matrices
Î”HGNNâŠ•Î”symâŠ•Î”rwtrivially results in 3ğ¼âˆ’ğ·âˆ’1ğ‘£ğ»ğ·âˆ’1ğ‘’ğ»ğ‘‡, i.e., it
contains only the random-walk Laplacian information. These fea-
tures provide unique identifiers to hypernodes, implicitly helping
to break automorphism groups in hypergraph topology.
Equivariant Feature Mixing. We employ an equivariant op-
erator learning mechanism to perform static feature updates on
the hypernode features. The aggregated hypernode representations
from TAA and SIB modules are propagated through downscaling
MLPs,ğ‘€ğ¿ğ‘ƒ1:Rğ‘›Ã—2ğ‘‘â†’Rğ‘›Ã—ğ‘‘/2andğ‘€ğ¿ğ‘ƒ2:Rğ‘›Ã—2ğ‘‘â†’Rğ‘›Ã—ğ‘‘/2
in parallel. The original features from hypernodes are propagated
throughğ‘€ğ¿ğ‘ƒ3:Rğ‘›Ã—ğ‘‘â†’Rğ‘›Ã—ğ‘‘/2. We perform a permutation
equivariant operation on aggregated features:
ğ‘‹eqv=ğ‘€ğ¿ğ‘ƒ1(Ë†ğ‘¥ğœ…|;|Ë†ğ‘¥ğ‘)âŠ™ğœ(ReLU(ğ‘€ğ¿ğ‘ƒ2(ğ‘‹ğ»ğº spectral))) (2)
whereâŠ™represents Hadamard product; the static feature update
finally upscales the dense information-rich features through a con-
catenation|;|operation as ğ‘‹static=ğ‘‹eqv|;|ğ‘€ğ¿ğ‘ƒ3(ğ‘‹HG).Mo
del Time Complexity
HGNN O
(ğ‘›ğ‘‘3)
HGNN+ O(ğ‘›ğ‘‘2+ğ‘šğ‘‘2ğ‘’)
HyperGCN O(ğ‘šğ‘›2ğ‘‘2)
HNHN O(ğ‘›ğ‘‘2+ğ‘šğ‘‘2)
UniGCN O(ğ‘šğ‘‘+ğ‘›ğ‘‘2)
DPHGNNO
((ğ‘›+ğ‘ âˆ—)ğ‘‘ğ‘£ğ‘‘+ğ‘›â„ğ‘¦ğ‘ğ‘‘2+ğ‘šâ„ğ‘¦ğ‘ğ‘‘2)
Table 2: Time Complexity of different models.
3.3 Dynamic Feature Fusion (DFF)
Here, we describe the dynamic feature fusion mechanism to ex-
plicitly capture the lower-order semantics from pair-wise node
interactions and diffuse it into the hypergraph message-passing
step to learn generalizable feature representations. The star expan-
sion produces graph topology ğºâˆ—by connecting existing nodes
in the edges to a synthetic node for the hyperedge set [ 45]. This
allows us to extract aggregated information of a neighborhood in
the graph in these synthetic nodes (called supernodes, hereafter).
We utilize this feature representation from supernodes and fuse
it into features aggregated from hypernodes towards hyperedges.
We then update the representations in the neighborhood from the
fused hyperedge towards subsequent hypernodes. Following this,
we reiterate the feature updation without dynamic features on the
hypergraph topology for the final prediction layer and perform class
prediction. The dynamic message-passing mechanism described
here can be formulated using the following update rules:
 
2551DPHGNN: A Dual Perspective Hypergraph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ğ‘š(ğ‘™)
ğ‘=ğœ™maskh
Aggr(ğ‘™)n
ğ‘š(ğ‘™âˆ’1)
ğ‘¢âˆ€ğ‘¢âˆˆğ‘(ğ‘)oi
ğ‘š(ğ‘™âˆ’1)
ğ‘’ =Aggr(ğ‘™âˆ’1)n
ğ‘š(ğ‘™âˆ’1)
ğ‘¢âˆ€ğ‘¢âˆˆğ‘’o
âˆªn
ğ‘š(ğ‘™)
ğ‘âˆ€ğ‘âˆˆğ¸o
ğ‘š(ğ‘™)
ğ‘’=Aggr(ğ‘™)n
ğ‘š(ğ‘™âˆ’1)
ğ‘’âˆ€ğ‘’âˆˆğ¸ğ‘–o
ğ‘¥(ğ‘™)
ğ‘£=Update(ğ‘™)n
ğ‘¥(ğ‘™âˆ’1)
ğ‘£,ğ‘š(ğ‘™)
ğ‘’o
The embedding update mechanism for hypernodes proposed in
DPHGNN can be summarised below:
bğ‘‹DPHGNN =ğœ
ğ‘‹static+ğ»ğ·âˆ’1
ğ‘’
ğ»ğ‘‡ğ·âˆ’1
2ğ‘£ğ‘‹static+ğ·âˆ’1
ğ‘’ğœ™mask
ğ´ğ‘‹ğºâˆ—	
Î˜
(3)
We initialize the hypernode features with ğ‘‹ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ . DPHGNN per-
forms summation over supernodes and hypernodes at the time of
aggregation. The dimensions of the DPHGNN matrix multiplica-
tions are as follows: ğ»âˆˆRğ‘›Ã—ğ‘š,ğ»ğ‘‡âˆˆRğ‘šÃ—ğ‘›,ğ´âˆˆRğ‘›Ã—ğ‘›.ğ‘‹staticâˆˆ
Rğ‘›Ã—ğ‘‘, andğ‘‹ğºâˆ—âˆˆRğ‘›+ğ‘šÃ—ğ‘‘are input matrices. ğ·ğ‘£âˆˆRğ‘›Ã—ğ‘›, and
ğ·ğ‘’âˆˆRğ‘šÃ—ğ‘šare the diagonal matrices. The dimensions involved
in the operations are ğ·âˆ’1ğ‘’ğœ™mask{ğ´ğ‘‹ğºâˆ—},Rğ‘šÃ—ğ‘šÃ—Rğ‘šÃ—ğ‘‘â†’Rğ‘šÃ—ğ‘‘,
ğ»ğ‘‡ğ·âˆ’1/2
ğ‘£ğ‘‹static ,Rğ‘šÃ—ğ‘šÃ—(Rğ‘›Ã—ğ‘›Ã—Rğ‘›Ã—ğ‘‘)â†’Rğ‘šÃ—ğ‘‘. At the DFF,
ğ‘‹âˆˆRğ‘šÃ—ğ‘‘, and finally, Rğ‘›Ã—ğ‘šÃ—(Rğ‘šÃ—ğ‘šÃ—Rğ‘šÃ—ğ‘‘)â†’Rğ‘›Ã—ğ‘‘.
3.4 Time Complexity Analysis
Give an attributed hypergraph HG(ğ‘‰,ğœ‰,ğ‘‘ğ‘£,ğ‘‘ğ‘’,ğ‘‹), whereğ‘‘ğ‘£,ğ‘‘ğ‘’,
andğ‘‹are average degree of ğ‘›hypernodes, average degree of ğ‘š
hyperedges, and multiset of features {ğ‘¥ğ‘£}ğ‘£âˆˆğ‘‰, whereğ‘¥ğ‘£âˆˆRğ‘‘is of
dimensionğ‘‘. The computation time of decomposing hypergraph
into multiple views of graph (clique, hypergcn, star) structure and
applying graph convolution is O(ğ‘›ğ‘ğ‘‘ğ‘£ğ‘‘ğ‘’ğ‘‘+ğ‘›â„ğ‘¦ğ‘ğ‘‘ğ‘£ğ‘‘+(ğ‘›+ğ‘ )âˆ—ğ‘‘ğ‘£ğ‘‘ğ‘’ğ‘‘)
â‰¤O((ğ‘›+ğ‘ )âˆ—ğ‘‘ğ‘£ğ‘‘ğ‘’ğ‘‘+ğ‘›â„ğ‘¦ğ‘ğ‘‘ğ‘£ğ‘‘). For inductive priors, the compute
time is upper bounded by O(ğ‘›â„ğ‘¦ğ‘ğ‘‘ğ‘£ğ‘‘ğ‘’ğ‘‘+(ğ‘›+ğ‘š)â„ğ‘¦ğ‘ğ‘‘2). The time
complexity for DPHGNN message update mechanism is O((ğ‘›+
ğ‘ )âˆ—ğ‘‘ğ‘£ğ‘‘+ğ‘›â„ğ‘¦ğ‘ğ‘‘+ğ‘šâ„ğ‘¦ğ‘ğ‘‘+ğ‘›â„ğ‘¦ğ‘ğ‘‘2+ğ‘šâ„ğ‘¦ğ‘ğ‘‘2)â‰¤O((ğ‘›+ğ‘ )âˆ—ğ‘‘ğ‘£ğ‘‘+
ğ‘›â„ğ‘¦ğ‘ğ‘‘2+ğ‘šâ„ğ‘¦ğ‘ğ‘‘2), subscriptsâˆ—,ğ‘, andâ„ğ‘¦ğ‘denote the respective
vertex setğºâˆ—,ğºğ‘,andğºâ„ğ‘¦ğ‘.Table 2 presents a comparative anal-
ysis between the proposed DPHGNN and baseline HGNNs. We
also provide corresponding run-time analysis in Table 11 in the
appendix.
4 THEORETICAL CHARACTERIZATION
4.1 Equivariant Operator Learning
The DPHGNN architecture generalizes the performance across
the change in hypergraph topology; several operators are used to
bridge the tradeoffs of spatial and spectral HGNNs. In proposition
4.1 we prove that the feature mixer module encodes features from
TAA and SIB blocks in a permutation equivariant manner. We have
provided an in-depth analysis of the correlation between specified
model blocks and hypergraph dataset in the Ablation Study section
in Experiments.
Proposition 4.1. The encoding function ğ‘“:(ğ‘‹HG,ğ‘‹TAA,ğ‘‹SIB)â†’
ğ‘‹static learned by equation 2 is permutation equivariant, i.e, if ğœ‹is a
bijective function; ğ‘“(ğœ‹Â·ğ‘‹)=ğœ‹Â·ğ‘“(ğ‘‹).Proof. Letğ´=ğ‘€ğ¿ğ‘ƒ1(Ë†ğ‘¥ğœ…|;|Ë†ğ‘¥ğ‘), andğµ=(ğ‘€ğ¿ğ‘ƒ2(ğ‘‹ğ»ğº spectral))
are the matrices obtained by equation 2 with elements ğ‘ğ‘–ğ‘—,ğ‘ğ‘–ğ‘—
respectively. Let ğ‘â€²
ğ‘–ğ‘—=ğ‘ğœ‹(ğ‘–)ğœ‹(ğ‘—)andğ‘â€²
ğ‘–ğ‘—=ğ‘ğœ‹(ğ‘–)ğœ‹(ğ‘—)are elements of
matrics A, B after permutation ğœ‹. The Hadamard product matrix (C)
of elements results in ğ‘â€²
ğ‘–ğ‘—=ğ‘â€²
ğ‘–ğ‘—âŠ™ğ‘â€²
ğ‘–ğ‘—. The original Hadamard Product
ğ¶=ğ´âŠ™ğµhas elements ğ‘ğ‘–ğ‘—=ğ‘ğ‘–ğ‘—âŠ™ğ‘ğ‘–ğ‘—, applying permutation ğœ‹to C
will result in the same operation as with ğœ‹to original matrices, i.e.
ğ‘ğœ‹(ğ‘–)ğœ‹(ğ‘—)=ğ‘ğœ‹(ğ‘–)ğœ‹(ğ‘—)âŠ™ğ‘ğœ‹(ğ‘–)ğœ‹(ğ‘—). Hence, function ğ‘“is permutation
equivariant. â–¡
4.2 Expressive Power of DPHGNN
The expressive power of HGNNs is determined by their ability
to learn a function on hypergraph that can distinguish two non-
isomorphic hypergraphs [ 5] and their local substructures. [ 18] for-
mulated a variant of the Generalized Weisfeiler Leman (1-GWL)
test for measuring the expressive power of UniGNNs, following
[6]. However, the increase in symmetry from automorphisms in
graph/hypergraph structure GWL inherits several failure cases
in distinguishing complex substructures. In Proposition 4.2, we
generalize the 1-GWL test for spatial HGNNs which follow mes-
sage passing (c.f. Eq. 1) through the lens of the hypergraph color
refinement algorithm. We then analyze the expressive power of
DPHGNN in theorem 4.3 and prove that providing explicit repre-
sentation information of underlying graph structure helps break
automorphisms [31] via learning equivariant functions.
Proposition 4.2. Given a function ğ‘“ğœƒâˆˆğ¹HGNN(learned by Eq. 1),
and two non-isomorphic hypergraphs, ğ»1andğ»2,ğ‘“ğœƒcan distinguish
ğ‘“ğœƒ(ğ»1)â‰ ğ‘“ğœƒ(ğ»2)if and only if for some ğ‘¡>0, the updated coloring
ğ»ğ¶(ğ‘¡)(ğ‘‰1,ğ»1)â‰ ğ»ğ¶(ğ‘¡)(ğ‘‰2,ğ»2).
Proof Sketch. Here, we provide the skeleton of the proof (see A.1
Supplementary for the detailed proof). We first generalize the color
refinement for hypergraphs by providing tensor representations for
colors. We then establish an injective mapping between the HASH
function of iterative hypergraph color refinement (c.f. Eq. 4) and
general spatial HGNNs update (c.f. Eq. 1)
ğ»ğ¶ğ»
ğ‘¡+1(ğ‘£)=HASH{{{ğ»ğ¶ğ»
ğ‘¡(ğ‘¢)|ğ‘¢âˆˆğ‘“ğ»(ğ‘’)}
|ğ‘’âˆˆğœ‰withğ‘£âˆˆğ‘“ğ»(ğ‘’)}} (4)
Theorem 4.3. Given a function ğ‘”ğœƒâˆˆğ¹DPHGNN(learned by Eq. 3),
and two non-isomorphic hypergraphs ğ»1andğ»2that can be distin-
guished by 3-GWL test, there is at least a function ğ‘”ğœƒthat can also
decideğ‘”ğœƒ(ğ»1)â‰ ğ‘”ğœƒ(ğ»2)ifğ‘”ğœƒis equipped with the following struc-
ture: (i)ğ‘”ğœƒis an order invariant function learned by the composition
ofğ‘˜-equivariant layer(ğ‘˜â‰¥3)HGNN network. (ii) Aggregation and
Readout functions of DPHGNN are injective.
Proof Sketch. Here, we provide the skeleton of the proof (see
A.2 Supplementary for the detailed proof). We first build upon the
work of [ 28,29] to generalize the notion of the ğ¹-WL (Folklore-WL)
test [ 19] for hypergraphs. We then argue that using explicit feature
representations from underlying graph structure as the composition
of equivariant layers is at least as powerful as the ğ¹-GWL test. We
then prove the second part of the theorem.
 
2552KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, and Tanmoy Chakraborty
Dataset |ğ‘‰|
|ğœ‰| |ğœ‡| |ğ‘€|ğ‘‘ ğ¶
CA
-DBLP 2708 1072 0.79 4.2 1433 7
CA-Cora 43413 22535 1.03 4.7 1425 6
CC-Citeseer 2708 1579 1.16 3.0 1433 6
CC-Cora 3312 1079 1.03 3.2 3703 6
YelpRestaurant 50758 679302 26.7 6.6 1862 11
HouseCommittees 1290 341 0.52 34.7 1290 3
Cooking200 7403 2755 0.74 19.9 7403 20
News20 16342 100 0.01 654.5 100 4
CO-RTO 66790 27528 0.82 1.6 10 2
Table 3: Dataset statistics: |ğ‘‰|and|ğœ‰|represent the number of
hypernodes and hyperedges, respectively. |ğœ‡|=2|ğœ‰|
|ğ‘‰|is. the av-
erage hyperedge density, and |ğ‘€|=1
|ğœ‰|Ã
ğ‘’âˆˆğœ‰|ğ‘’|is the average
hypernode density of hyperedges; ğ‘‘andğ¶are the dimension
of features and the number of classes, respectively.
5 EXPERIMENTS
This section presents benchmark datasets, baseline methods, com-
parative analyses and ablation studies of our model.
Benchmark Datasets. We experiment with publicly available
benchmark datasets â€“ Cora, DBLP, and Citeseer, extensively used
in previous studies [ 18,41]. Two standard ways to construct hyper-
graphs from these datasets result in coauthorship (CA) networks
from Cora and DBLP and cocitation networks (CC) from Cora and
Citeseer. Moreover, we also benchmark on the pre-constructed
standard hypergraph datasets â€“ YelpRestaurant, HouseCommittees,
Cooking200, and News20 [ 8]. We utilize the hypergraph convo-
lution layers from the DeepHyperGraph (DHG) package for our
baseline experiments. Table 3 provides detailed statistics of these
datasets.
Baseline Methods. We compare DPHGNN with seven stan-
dard baselines designed for semi-supervised node classification on
hypergraphs. The baselines include a mix of spatial and spectral
approaches for hypergraph representation learning â€“ (i) HGNN: It
parametrizes the smoothing of features from HGNN Laplacian ma-
trix [ 12]; (ii) HGNN+: It uses adaptive hyperedge group fusion strat-
egy to introduce spatial message passing for different modalities
[13]; (iii) HyperGCN: It introduces Laplacian operator to approx-
imate underlying graph structure and perform message passing
over lower-order graph [ 41]; (iv) HNHN: Spatial HGNN with non-
linear activation functions are applied over both hypernodes and
hyperedges [ 10]; UniGNN [ 41] generalises spatial learning GNNs
for hypergraphs and formulates various architectures such as (v)
UniGCN, (vi) UniSAGE, and (vii) UniGAT; (viii) HENNs [ 17]: It
exploits multiple spectrally-similar graph representations of hy-
pergraphs; (ix) HyGNN [ 33]: Spatial HGNN with attention based
hypergraph encoder for optimal hyperedge prediction.
Experimental Setup. For DPHGNN, we deploy a dual-layer
training mechanism (see Figure 4 in Supplementary), in which the
static layer is trained to learn the underlying graph structural and
hypergraph spectral inductive biases, and the dynamic layer is used
to optimize learning on hypergraphs. We use a suitable GNN layer to
the decomposed structure for graph structural learning and update
the feature representation for the 2-hop neighborhood. Moreover,
we use two-layer message passing for hypergraph structural learn-
ing, one for updating the neighbor hypernode features and theother for a class-wise prediction head. Table 10 in Supplementary
includes specific hyperparameter details for our experiments.
5.1 Performance Comparison
We evaluate the performance of the model based on the mean
accuracy, macro F1 score and micro F1 score. We report the average
performance of the model over ten runs, along with the standard
deviation.
5.1.1 Comparative Analysis. Table 4 presents the comparative anal-
ysis. We observe that spatial HGNN methods outperform the spec-
tral ones for co-authorship datasets (by a margin of âˆ¼5%). However,
it is the opposite in the case of co-citation datasets in which spectral
methods stand out (by a significant margin of 7%-9%).
Our proposed DPHGNN method, with TAA, SIB, and DFF+HGNN
modules, is robust to the change in hypergraph topology. We quan-
tify the performance improvement in DPHGNN by comparing it
with the best-performing spatial HGNN on co-authorship datasets.
DPHGNN outperforms UniGCN, UniSAGE, and UniGAT by 1.1%,
1.2%, and 2.3%, respectively, on CA-DBLP, and 3.4%,2.5%, and 2.6%,
respectively, on CA-Cora. For the co-citation datasets, with the
induction of SIB and TAA (spectral), DPHGNN outperforms spatial
UniGNNs (UniGCN, UniSAGE, UniGAT) by 6.3%,4.2%, and 9.6%, re-
spectively. In CC-Citeseer, DPHGNN outperforms the best baseline
by2.2%and gives the best performance on the F1-Score within the
standard deviation range (std). On CC-Cora, HGNN, being a fully
spectral baseline, surpasses DPHGNN by a small margin of 1.8%;
however, DPHGNN outperforms the rest of the spatial baselines,
namely UniGCN, UniSAGE and UniGAT, with a significant margin
of6.4%,2.6%,11.6%, respectively. For HouseCommittees, UniGNNs
surpass DPHGNN by a small margin of 2.8%(within mean and std
range). On YelpRestaurant, Cooking200, and News20 with dense
hypernode distributions, DPHGNN outperforms best spatial base-
lines (HGNN+, UniSAGE, UniGAT) with 2.9%,3.3%,0.8%and best
spectral baselines (HGNN, HGNN, HyperGCN) with 5.5%,2.3%,
1.4%respectively. Moreover, in Table 6, we present a performance
comparison between fully-spatial (HyGNN) and spectral (HEGNNs)
baselines; DPHGNN outperforms with significant absolute differ-
ences of 1.93%and3.06%for HENN and HyGNN, respectively. The
ablation study below empirically validates the effect of each module
in improving the performance of DPHGNN.
Embedding Update Visualization. The performance of a clas-
sification model is inherently dependent on interpolating the un-
derlying distribution and clustering the data points for respective
classes. Figure 2 provides the visualization of the t-SNE [ 35] of
the embedding update of DPHGNN and that of the best baseline
(HGNN). We observe the former segregating classes more effec-
tively than the latter.
5.1.2 Ablation Study. We study the impact of each module of
DPHGNN and report the change in the modelâ€™s performance with
the structure of hypergraph datasets in Table 7.
â€¢On co-authorship datasets (CA-DBLP, CA-Cora), most impact is
due to spatial information flow. Without TAA, the performance
of DPHGNN decreases (1 .3%â†“,1.5%â†“); however, DFF propagates
the lower-order structural information. Without DFF ( 2.5%â†“,
 
2553DPHGNN: A Dual Perspective Hypergraph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Metho
ds CA-DBLP CA-Cora CC-Citeseer CC-Cora
Mean
Acc Macro F1 Micro F1 Mean Acc Macro F1 Micro F1 Mean Acc Macro F1 Micro F1 Mean Acc Macro F1 Micro F1
HGNN
68.24Â±9.8 67.61Â±8.5 68.24Â±9.9 63.92Â±4.2 53.88Â±5.4 64.01Â±5.2 68.56Â±0.3 60.52Â±1.1 68.56 Â±0.2 69.38Â±3.8 69.03Â±3.9 68.92Â±5.4
HGNN+ 84.66 Â±5.1 81.12Â±1.8 84.66Â±5.1 68.76Â±4.8 61.90Â±5.2 69.76Â±3.6 42.02Â±5.3 37.21Â±3.0 42.02 Â±6.2 46.26Â±5.2 49.3Â±3.2 46.26Â±5.2
HyperGCN 84.03Â±5.2 83.29Â±4.2 84.01Â±6.2 63.91Â±7.1 51.22Â±9.2 68.99Â±5.5 63.2Â±1.8 60.24Â±2.2 63.2Â±1.1 67.07Â±2.1 66.52Â±4.9 67.87Â±2.1
HNHN 65.32Â±1.4 61.09Â±2.3 65.32Â±1.5 65.28Â±4.2 54.11Â±7.3 68.91Â±4.8 33.45Â±4.5 31.65Â±3.9 33.45 Â±4.5 48.03Â±4.3 42.55Â±0.5 48.03Â±4.3
UniGCN 88.07Â±0.1 86.03Â±0.4 88.07Â±0.2 71.68Â±4.4 57.31Â±6.2 79.74Â±1.3 52.47Â±8.2 47.54Â±4.8 52.47Â±8.2 61.1Â±7.4 55.24Â±8.2 61.1Â±7.4
UniSAGE 87.97Â±0.2 85.37Â±0.1 87.96Â±0.4 72.54Â±3.4 57.26Â±6.4 78.69Â±3.4 57.59Â±7.5 55.2Â±3.2 57.59Â±7.2 64.87Â±6.0 63.26Â±1.5 64.87Â±6.0
UniGAT 86.69Â±0.1 84.24Â±1.8 87.97Â±1.1 72.48Â±2.8 58.69Â±4.5 75.2Â±1.8 52.68Â±8.4 46.95Â±2.9 51.96Â±8.4 55.83Â±6.5 51.58Â±4.2 55.83Â±6.5
DPHGNN
89.06Â±1.2 86.48Â±2.5 87.16Â±1.8 75.12Â±1.2 68.44Â±1.9 78.94Â±3.3 65.77Â±4.2 56.52Â±3.5 63.77 Â±5.4 67.51Â±6.2 64.55Â±8.2 68.51Â±3.4
Metho
ds Yelp Restaurant House Committees Cooking200 News20
Mean
Acc Macro F1 Micro F1 Mean Acc Macro F1 Micro F1 Mean Acc Macro F1 Micro F1 Mean Acc Macro F1 Micro F1
HGNN
30.69Â±3.2 14.37Â±6.7 28.24Â±2.8 52.33Â±5.1 50.06Â±5.5 50.03Â±3.2 53.45Â±2.6 40.91Â±5.8 50.83 Â±2.9 80.46Â±3.2 78.74Â±0.4 80.46Â±2.4
HGNN+ 32.92 Â±1.8 12.84Â±4.2 30.53Â±1.5 53.1Â±5.9 52.58Â±4.2 51.27Â±3.5 49.64Â±1.2 40.07Â±8.6 48.27 Â±1.5 81.25Â±2.8 78.89Â±1.1 81.25Â±1.2
HyperGCN 30.77Â±1.1 13.3Â±2 28.41Â±3.1 NA NA NA 9.32Â±2.8 4.28 Â±1.6 6.64Â±2.6 81.16Â±0.8 78.95Â±0.5 80.16Â±1.8
HNHN 26.61Â±4.4 7.64Â±1.2 24.75Â±2.5 49.61Â±4.2 47.96Â±1.4 46.68Â±4.2 25.29Â±4.7 20.1Â±4.7 23.33Â±3.3 76.87Â±5.4 78.75Â±0.3 81.13Â±0.6
UniGCN 28.56Â±2.5 10.38Â±0.9 26.6Â±4.2 56.04Â±5.5 54.87Â±2.5 53.32Â±4.8 49.49Â±3.3 38.53Â±1.6 47.43Â±1.4 80.97Â±0.5 78.82Â±0.1 80.97Â±0.5
UniSAGE 27.3Â±0.4 10.41Â±1.5 25.93Â±1.6 61.72Â±8.4 50.87Â±6.7 61.21Â±0.5 52.32Â±3.9 42.24Â±2.5 49.64Â±2.2 81.49Â±1.5 79.47Â±1.8 81.49Â±0.3
UniGAT OOM OOM OOM 57.36Â±3.1 56.31Â±0.2 54.73Â±2.9 34.99Â±1.8 26.17Â±3.6 33.73Â±0.9 78.76Â±4.4 78.15Â±0.3 80.76Â±3.3
DPHGNN
35.82Â±1.4 20.55Â±0.3 36.33Â±1.2 56.87Â±4.3 51.72Â±0.3 54.33Â±3.3 55.74Â±2.1 46.51Â±1.5 51.73 Â±3.4 81.57Â±1.3 79.2Â±0.5 81.43Â±0.3
Table 4: Performance comparison on the benchmark hypergraph datasets over mean accuracy(%), macro F1-score, and micro
F1-score (Â± standard deviation). For every dataset, the best performance is highlighted in red, the best baseline is highlighted in
blue, results within 1-std. dev. are highlighted in brown. NA indicates structural restriction for method; OOM is out of memory.
0123456
0123456
0123456
Figure 2: Visualization of feature embedding update on the CC-Citeseer dataset (6 classes) â€“ initial embedding (left), HGNN
embedding update (middle), DPHGNN embedding update (right).
2.7%â†“), TAA can only provide cross-attention information of
node entities as inductive bias.
â€¢On co-citation datasets, CC-Citeseer, and CC-Cora, the TAA
(spectral) and SIB modules of DPHGNN have the majority of
contributions, without which the performance of DPHGNN de-
teriorates by 6.3%â†“, and 5.9%â†“, respectively. This supports the
empirical analysis of spectral HGNNs performing better than
spatial baselines over co-citation datasets. Without TAA, the
performance of DPHGNN degrades (2 .9%â†“,3.3%â†“) as TAA prop-
agates spectral inductive biases.
â€¢On HouseCommittees and Cooking200, features are initialised
with one-hot encoding of node indices. Therefore, TAA and SIB
have the most impact on empowering features with spatial and
spectral inductive biases; the performance deteriorates without
TAA (4.1%â†“,5.7%â†“) and without SIB (3 .2%â†“,6.2%â†“). Moreover,
due to sparse connectivity ( |ğœ‡|âˆ¼0.5,0.7) with dense hypernode
density (|ğ‘€|âˆ¼34,19) TAA (spectral) complements in the absence
of SIB, and TAA (spatial) for DFF.
â€¢On News20, most of the HGNNs suffer from over-smoothing
of features as the highly dense hypernodes ( |ğ‘€|âˆ¼654) diffuse
features via highly sparse hyperedges ( |ğœ‡|âˆ¼0.01), resulting in
similar performance irrespective of HGNN architecture. WithoutSIB (4.5%â†“), DPHGNN struggles to break automorphisms in this
symmetric structure.
â€¢On YelpRestaurant, with densely connected hypergraph, the per-
formance drops without TAA and DFF (5 .2%â†“,3.3%â†“) as they
provide a major contribution in the spatial message flow. Without
SIB (1.3%â†“), TAA (spectral) provides desired inductive biases.
We also study the modelâ€™s performance with increased message-
passing blocks (i.e., the TAA and DFF blocks). The DFF+HGNN
layers in Table 8 indicate the increase in hypergraph message-
passing layers; DFF stays constant throughout the layer increment.
Moreover, the spectral inductive biases remain unchanged. How-
ever, we observe that with the increase in HGNN layers (4 to 8), the
performance drops significantly (5 .1%â†“,30%â†“); over-smoothing of
features [25] remains the primary reason behind the performance
drop with the increase in layers.
5.1.3 Convergence Analysis. Figure 3 shows the significance of
topological structure for the loss convergence in HGNN. We observe
that the loss function tends to have a weak convergence for the
co-citation networks compared to co-authorship networks.
 
2554KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, and Tanmoy Chakraborty
Metho
d Mean Acc Macro F1 Micro F1
HGNN
80.51Â±1.7 45.01Â±0.6 80.51Â±1.6
HGNN+ 80.47Â±0.9 44.74Â±1.5 80.47Â±0.9
HyperGCN 80.50Â±0.6 44.59Â±0.2 80.49Â±0.6
HNHN 80.49Â±0.8 44.60Â±0.2 80.49Â±0.8
UniGCN 80.50Â±2.6 44.86Â±3.2 80.49Â±2.8
UniSAGE 81.66Â±2.9 46.66Â±3.1 81.66Â±2.9
UniGAT 80.48Â±0.6 44.70Â±0.4 80.48Â±0.6
DPHGNN
86.02Â±1.2 51.39Â±0.8 86.02Â±1.2
Table 5: Performance comparison on the CO-RTO dataset. The
best performance is highlighted in red, and the best baseline
is highlighted in blue.Dataset
\Baseline HENN HyGNN DPHGNN
CA
-DBLP 85.64Â±4.7 88.01Â±0.4 89.06Â±1.2
CA-Cora 72.33Â±3.5 70.42Â±1.5 75.12Â±1.2
CC-Citeseer 68.15Â±2.2 55.86Â±0.5 65.77Â±4.2
CC-Cora 70.56Â±5.6 59.67Â±4.1 67.51Â±6.2
Yelp Restaurant 33.47Â±1.8 33.43Â±2.4 35.82Â±1.4
House Committees 54.83Â±4.9 59.99Â±3.2 56.87Â±4.3
Cooking200 50.09Â±5.5 50.42Â±4.6 55.74Â±2.1
News20 81.32Â±0.6 80.33Â±2.5 81.57Â±1.3
CO-RTO 45.07Â±2.8 48.23Â±0.9 51.39Â±0.8
Table 6: Performance comparison (Mean Accuracy, for CO-
RTO: Macro F1) between HENN, HyGNN, and the proposed
DPHGNN. The best performance is highlighted in red.
EpochsLoss
Figure 3: Convergence analysis of DPHGNN over benchmark
hypergraph and CO-RTO datasets.
Datasets
Overall w/o TAA w/o SIB w/o DFF
CA
-DBLP 89.06 87.72 88.65 86.54
CA-Cora 75.12 73.6 74.52 72.34
CC-Citeseer 65.77 63.86 59.47 64.05
CC-Cora 67.51 64.12 61.15 65.98
YelpRestaurant 35.82 30.56 34.69 32.45
HouseCommitees 56.87 52.86 53.66 55.68
Cooking200 55.74 50.04 49.53 53.09
News20 81.07 80.23 76.55 80.33
CO-RTO 51.39 48.32 46.75 49.12
Table 7: Ablation study to measure the impact of three blocks
of DPHGNN â€“ Topology-Aware Attention (TAA), Spectral
Inductive Biases (SIB), and Dynamic Feature Fusion (DFF).
For benchmark datasets, we report mean accuracy (%); since
the CO-RTO dataset is imbalanced, we report macro F1-score.
Datasets
TAA Layers DFF Layers
2-GCN
4-GCN 8-GCN 2-HGNN 4-HGNN 8-HGNN
CA
-DBLP 89.06 88.34 86.95 89.06 88.92 46.03
CA-Cora 75.12 74.85 73.8 75.12 69.01 32.54
CO-Citeseer 65.77 64.98 63.94 65.77 59.82 25.86
CO-Cora 67.51 66.02 64.22 67.51 58.25 28.05
CoRTO 51.39 49.95 48.38 51.39 28.67 21.03
YelpRestaurant 35.82 34.23 30.23 35.82 30.37 15.35
HouseCommitees 56.87 55.01 51.95 56.87 50.4 20.91
Cooking200 55.74 54.02 50.6 55.74 49.59 18.95
News20 81.07 79.88 76.78 81.07 74.55 39.26Table 8: Ablation study to measure the impact of increment
in message-passing layers over the TAA and DFF blocks.Mo
del Iso HG Non-Iso HG
HGNN
55.23 53.25
HGNN++ 40.89 63.69
HyperGCN 57.31 58.25
HNHN 38.85 77.3
UniGCN 44.83 80.55
UniSAGE 46.35 85.00
UniGAT 48.84 84.21
HENN 53.54 75.26
HyGNN 51.63 80.64
DPHGNN
73.38 87.58
Table 9: Performance comparison on the Isomorphic and
Non-Isomorphic datasets. The best performance is high-
lighted in red, and the best baseline is highlighted in blue.
Performance on Hypergraph Isomorphism Test. To validate our
theoretical framework behind equivariant operator learning (EOL)
within the DPHGNN model, we constructed two synthetic datasets;
"Isomorphic (Iso) HG" and "Non-Isomorphic (Non-Iso) HG". The clas-
sification results on these synthetic tasks are presented in Table
9. The results demonstrate that DPHGNN achieves significant im-
provements, which are attributed to its robust spectral and spatial
inductive biases.
6 THE RETURN-TO-ORIGIN (RTO) TASK
This section describes modeling resource-constrained real-world
data with hypergraph structure and reports the efficacy of DPHGNN
for an industrial application â€“ the Return-to-Origin (RTO) task.
Task Description. RTO refers to the situation where a prod-
uct is returned to the point of origin instead of being delivered to
the intended recipient [ 24]. A product being RTO-ed substantially
impacts global e-commerce and encourages a vicious cycle of finan-
cial losses for merchants, complex fraudulent transactions, etc. We
model RTO as a semi-supervised hypernode classification task. In
collaboration with an e-commerce giant, we deploy our proposed
DPHGNN to address the RTO task.
6.1 The CO-RTO Dataset
Here, we describe the carefully curated CO-RTO (Co-order RTO)
hypergraph structure. An RTO involves various physical entities
from an order transaction cycle (customer, product, supplier, and
order) that can not be expressed using pairwise relations. This
naturally motivates us to model higher-order interactions using
 
2555DPHGNN: A Dual Perspective Hypergraph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
hyperedges, containing entities as hypernodes. The RTO prediction
task boils down to a binary hypernode classification problem â€“
predicting whether order-related nodes are RTOs. We obtained the
CO-RTO dataset from our partner e-commerce company, Meesho
(Table 3).
Hypergraph Construction. The CO-RTO dataset is an or-
derly transaction and entity relation hypergraph dataset. The raw
data accompanies the feature information collected from partner e-
commerce companies. It mainly comprises four major entities that
play key roles in a complete order transaction cycle namely, user,
supplier, courier, and product. In this work, we use an autoencoder
for the conversion of heterogeneous features to homogeneous fea-
tures to enable message flow. We formulate a hypergraph structure
in the form of nodes that represent these entities with their cor-
responding feature vectors and family of hyperedges where each
hyperedge comprises entity nodes from an order transaction flow.
6.2 Experimental Results
Table 5 reports the performance of the competing models for the
RTO task. Due to sparse connectivity, symmetrical structure, and
class imbalance in CO-RTO, baselines suffer from over-smoothing
of features over the message propagation. DPHGNN achieves 51%
macro-F1, outperforming other baselines by a significant margin
(close to 7%). We further report critical observations below.
Symmetric and Sparse Incidence Structure. In the CO-RTO
hypergraph, the edge connectivity is sufficiently sparse (avg. hyper-
node density|ğ‘€|is1.6, hyperedge density |ğœ‡|is0.82; c.f. Table 3),
constraining message passing between subsequent layers in HGNN.
The fixed cardinality in the hyperedges of the CO-RTO hypergraph
and the highly symmetrical structure tend to increase the multi-
plicity of eigenvalues, which is inherently linked with the Cheeger
constant [32] and the over-squashing phenomenon [34].
Automorphism. The hypergraph ğ»ğº=(ğ‘‰,ğœ‰)of the CO-RTO
dataset is a uniform hypergraph containing an automorphism group.
More precisely, if ğœ‹is a permutation function over hypernode set ğ‘‰
ofğ»ğº;ğœ‹:ğ‘‰â†’ğ‘‰â€², with|ğ‘‰|=|ğ‘‰â€²|and for each hyperedge ğ‘’âˆˆğœ‰
and|ğ‘’|=ğ‘˜(hereğ‘˜=4). Hereğœ‹satisfiesğœ‹(ğ‘£)=ğ‘£â€², whereğ‘£âˆˆğ‘‰
andğ‘£â€²âˆˆğ‘‰â€². Alsoâˆ€ğ‘’âˆˆğœ‰âˆƒğ‘’â€²âˆˆğœ‰such thatğ‘’â€²={ğœ‹(ğ‘£)|ğ‘£âˆˆğ‘’}.
Now as every hyperedge ğ‘’ğ‘–âˆˆğœ‰, the image of ğ‘’ğ‘–underğœ‹is also
a hyperedge in ğœ‰. For example: if ğ‘’ğ‘–={ğ‘£ğ‘–,1,ğ‘£ğ‘–,2,ğ‘£ğ‘–,3,ğ‘£ğ‘–,4}then
ğœ‹(ğ‘’ğ‘–)={ğœ‹(ğ‘£ğ‘–,1),ğœ‹(ğ‘£ğ‘–,2),ğœ‹(ğ‘£ğ‘–,3),ğœ‹(ğ‘£ğ‘–,4)}is also inğœ‰. Given the gen-
eral spatial HGNN message-passing framework, automorphism in
hypergraph preserves the incidence relations between vertices ğ‘¢
and edgeğ‘’asğ»ğºğ‘¢,ğ‘’=ğ»ğºğœ‹(ğ‘¢),ğœ‹(ğ‘’). Given two hypernodes, ( ğ‘,ğ‘)
under automorphism ğœ‹areğ‘=ğœ™(ğ‘), their feature update equations
5 and 6 are:
Ë†ğ‘¥(ğ‘™)
ğ‘=Update(ğ‘™)
ğ‘¥(ğ‘™)
ğ‘,Aggr(ğ‘™)nn
Aggr(ğ‘™âˆ’1)
ğ‘¥(ğ‘™âˆ’1)
ğ‘¢
âˆ€ğ‘¢âˆˆğ‘’o
âˆ€ğ‘’âˆˆğ¸ğ‘o
(5)
Ë†ğ‘¥(ğ‘™)
ğ‘=Update(ğ‘™)
ğ‘¥(ğ‘™)
ğ‘,Aggr(ğ‘™)
{{Aggr(ğ‘™âˆ’1)(ğ‘¥(ğ‘™âˆ’1)
ğœ‹(ğ‘¢))
âˆ€ğœ‹(ğ‘¢)âˆˆğœ‹(ğ‘’)}âˆ€ğœ‹(ğ‘’)âˆˆğ¸ğ‘}
(6)
Hence given the automorphism ğœ‹, the feature update leads to Ë†ğ‘¥(ğ‘™)
ğ‘=Ë†ğ‘¥(ğ‘™)
ğ‘.
Therefore, the spatial HGNNs bounded by 1-GWL expressivity perform
sub-optimally due to symmetry-induced automorphism groups. To our
knowledge, our method (DPHGNN) is the first HGNN that breaks the auto-
morphism groups and is 3-GWL expressive.Over-smoothing. We perform experiments with dual-layer HGNNs.
Still, the features get smoothed over message passing due to the sparse
incidence structure. This is the main reason behind the suboptimal perfor-
mance of baselines on CO-RTO. Along with propagating messages with
skip connections [ 7], DPHGNN leverages static inductive priors to prevent
over-smoothing of relevant feature information.
Class Imbalance. In CO-RTO, class imbalance occurs naturally (a ratio
of6568:26827 for RTO:Non-RTO), and HGNN tends to overfit the major-
ity class. These observations lead to a drastic performance drop with the
hypergraph-based MPNN models.
6.3 Deployment Workflow
This section presents a brief description of the deployment of DPHGNN in
Meeshoâ€™s large-scale e-commerce platform with a transductive setting [9].
Data Pipelines. The large-scale e-commerce transaction data is pulled
through the Apache Spark streaming service [ 1] and distributed across mul-
tiple clusters to compute the homogeneous hypernode features and specify
user, product, supplier, and courier entities inside a co-order hyperedge.
These node features and edges are hosted in a feature store infrastruc-
ture built on top of a data lake and partitioned for efficient read and write
operations.
Test-Train Hypergraph creation. Based on this modelâ€™s inference,
some of the orders get actioned. Therefore, the possible outcome of RTO
remains unknown due to the modelâ€™s action, and hence, data is not useful
for future model training. To resolve this, a holdout sample is maintained
where no actions are taken and are used as training data for transductive
training.
Model Building and Inference. In the current transductive setting, the
target variable is only available for the past holdout transactions; the model
is trained only on those hypernodes and hyperedges using training masks.
All the edges and nodes go through the forward pass. Once the model is
trained, the predictions for the latest transactions are collected and stored.
We used NVIDIA T4 GPU clusters with 20 workers for network training
and inference. The orders flagged are published in a Kafka queue [ 22] to be
consumed and blocked by the order management system.
7 CONCLUSION
In this paper, we proposed DPHGNN, a novel hypergraph message-passing
approach for representation learning. It explicitly learns static and dynamic
feature representations from multiple views of lower-order graph topol-
ogy. DPHGNN outperformed seven SOTA baselines over eight benchmark
datasets. We also showed its expressive power theoretically. DPHGNN was
also tested on a new e-commerce dataset (CO-RTO) and deployed by our
industry partner for RTO prediction.
ACKNOWLEDGEMENT
This research was financially supported by Meesho. We thank them for
their generous support in providing the CO-RTO dataset and their overall
contributions to our work.
REFERENCES
[1]Michael Armbrust, Tathagata Das, Joseph Torres, Burak Yavuz, Shixiong Zhu,
Reynold Xin, Ali Ghodsi, Ion Stoica, and Matei Zaharia. 2018. Structured stream-
ing: A declarative api for real-time applications in apache spark. In Proceedings
of the 2018 International Conference on Management of Data. 601â€“613.
[2]Devanshu Arya, Deepak K. Gupta, Stevan Rudinac, and Marcel Worring. 2020.
HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs.
CoRR abs/2010.04558 (2020). http://dblp.uni-trier.de/db/journals/corr/corr2010.
html#abs-2010-04558
[3]Song Bai, Feihu Zhang, and Philip H. S. Torr. 2019. Hypergraph Convolution and
Hypergraph Attention. CoRR abs/1901.08150 (2019). http://dblp.uni-trier.de/db/
journals/corr/corr1901.html#abs-1901-08150
 
2556KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, and Tanmoy Chakraborty
[4]Sambaran Bandyopadhyay, Kishalay Das, and M. Narasimha Murty. 2020. Hy-
pergraph Attention Isomorphism Network by Learning Line Graph Expan-
sion. In 2020 IEEE International Conference on Big Data (Big Data). 669â€“678.
https://doi.org/10.1109/BigData50022.2020.9378335
[5]Alain Bretto. 2013. Hypergraph Theory. Springer, Heidelberg. https://doi.org/10.
1007/978-3-319-00080-0
[6]Jan BÃ¶ker. 2019. Color Refinement, Homomorphisms, and Hypergraphs.. In
WG (Lecture Notes in Computer Science, Vol. 11789), Ignasi Sau and Dimitrios M.
Thilikos (Eds.). Springer, 338â€“350. http://dblp.uni-trier.de/db/conf/wg/wg2019.
html#Boker19
[7]Guanzi Chen, Jiying Zhang, Xi Xiao, and Yang Li. 2022. Preventing Over-
Smoothing for Hypergraph Neural Networks. arXiv:2203.17159 [cs.LG]
[8]Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. 2022. You are AllSet:
A Multiset Function Framework for Hypergraph Neural Networks. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
hpBTIv2uy_E
[9]Giorgio Ciano, Alberto Rossi, Monica Bianchini, and Franco Scarselli. 2021. On
inductiveâ€“transductive learning with graph neural networks. IEEE Transactions
on Pattern Analysis and Machine Intelligence 44, 2 (2021), 758â€“769.
[10] Yihe Dong, Will Sawin, and Yoshua Bengio. 2020. HNHN: Hypergraph Net-
works with Hyperedge Neurons. ArXiv abs/2006.12278 (2020). https://api.
semanticscholar.org/CorpusID:219965680
[11] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference. 417â€“426.
[12] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hy-
pergraph Neural Networks.. In AAAI. AAAI Press, 3558â€“3565. http://dblp.uni-
trier.de/db/conf/aaai/aaai2019.html#FengYZJG19
[13] Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. 2022. HGNN+: General Hyper-
graph Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence 45 (2022), 3181â€“3199. https://api.semanticscholar.org/CorpusID:249644940
[14] Yue Gao, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, and Changqing
Zou. 2022. Hypergraph Learning: Methods and Practices. IEEE Transactions
on Pattern Analysis and Machine Intelligence 44, 5 (2022), 2548â€“2566. https:
//doi.org/10.1109/TPAMI.2020.3039374
[15] Xuemei Gu, Lijun Chen, and Mario Krenn. 2020. Quantum experiments and
hypergraphs: Multiphoton sources for quantum interference, quantum com-
putation, and quantum entanglement. Physical Review A 101, 3 (mar 2020).
https://doi.org/10.1103/physreva.101.033816
[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation
Learning on Large Graphs. In Advances in Neural Information Processing Systems.
11.
[17] Mikhail Hayhoe, Hans Matthew Riess, Michael M. Zavlanos, VICTOR PRE-
CIADO, and Alejandro Ribeiro. 2023. Transferable Hypergraph Neural Net-
works via Spectral Similarity. In The Second Learning on Graphs Conference.
https://openreview.net/forum?id=cHuii4NOB9
[18] Jing Huang and Jie Yang. 2021. UniGNN: a Unified Framework for Graph and
Hypergraph Neural Networks. In Proceedings of the Thirtieth International Joint
Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada,
19-27 August 2021, Zhi-Hua Zhou (Ed.). ijcai.org, 2563â€“2569. https://doi.org/10.
24963/ijcai.2021/353
[19] Ningyuan Teresa Huang and Soledad Villar. 2021. A short tutorial on the
weisfeiler-lehman test and its variants. In ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 8533â€“8537.
[20] Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang, and Yue Gao.
2020. Dual channel hypergraph collaborative filtering. In Proceedings of the 26th
ACM SIGKDD international conference on knowledge discovery & data mining.
2020â€“2029.
[21] Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification
with Graph Convolutional Networks. http://arxiv.org/abs/1609.02907 cite
arxiv:1609.02907Comment: Published as a conference paper at ICLR 2017.
[22] Jay Kreps, Neha Narkhede, Jun Rao, et al .2011. Kafka: A distributed messaging
system for log processing. In Proceedings of the NetDB, Vol. 11. Athens, Greece,
1â€“7.
[23] Guillaume Lachaud, Patricia Conde-Cespedes, and Maria Trocan. 2022. Graph
neural networks-based multilabel classification of citation network. In Asian
Conference on Intelligent Information and Database Systems. Springer, 128â€“140.
[24] Rashmee Lahon. 2022. Return to Origin- Why it Happens, Its Impact, and How
to Solve it? https://razorpay.com/blog/return-to-origin-challenges-and-how-
to-solve-it
[25] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph
convolutional networks for semi-supervised learning. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[26] Fan Liang, Cheng Qian, Wei Yu, David W. Griffith, and Nada Golmie. 2022. Survey
of Graph Neural Networks and Applications. Wireless Communications and Mobile
Computing (2022). https://api.semanticscholar.org/CorpusID:251192566
[27] Shengyuan Liu, Pei Lv, Yuzhen Zhang, Jie Fu, Junjin Cheng, Wanqing Li, Bing
Zhou, and Mingliang Xu. 2020. Semi-Dynamic Hypergraph Neural Networkfor 3D Pose Estimation.. In IJCAI, Christian Bessiere (Ed.). ijcai.org, 782â€“788.
http://dblp.uni-trier.de/db/conf/ijcai/ijcai2020.html#LiuLZFCLZX20 Scheduled
for July 2020, Yokohama, Japan, postponed due to the Corona pandemic..
[28] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. 2019.
Provably powerful graph networks. Advances in neural information processing
systems 32 (2019).
[29] Christopher Morris, Gaurav Rattan, and Petra Mutzel. 2020. Weisfeiler
and Leman go sparse: Towards scalable higher-order graph embeddings. In
Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-
zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Asso-
ciates, Inc., 21824â€“21840. https://proceedings.neurips.cc/paper/2020/file/
f81dee42585b3814de199b2e88757f5c-Paper.pdf
[30] Raffaella Mulas, Christian Kuehn, Tobias BÃ¶hle, and JÃ¼rgen Jost. 2021.
Random walks and Laplacians on hypergraphs: When do they match?
arXiv:2106.11663 [math.SP]
[31] Raffaella Mulas, RubÃ©n J SÃ¡nchez-GarcÄ±a, and Ben D MacArthur. 2020. Hyper-
graph automorphisms. arXiv preprint arXiv:2010.01049 (2020).
[32] Raffaella Mulas and Dong Zhang. 2021. Spectral theory of Laplace operators on
oriented hypergraphs. Discrete Mathematics 344, 6 (jun 2021), 112372. https:
//doi.org/10.1016/j.disc.2021.112372
[33] Khaled Mohammed Saifuddin, Bri Bumgardnerr, Farhan Tanvir, and Esra Akbas.
2022. HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Net-
work. 2023 IEEE 39th International Conference on Data Engineering (ICDE) (2022),
1503â€“1516. https://api.semanticscholar.org/CorpusID:250072419
[34] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen
Dong, and Michael M. Bronstein. 2022. Understanding over-squashing and
bottlenecks on graphs via curvature. arXiv:2111.14522 [stat.ML]
[35] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[36] Petar Velickovic, Guillem Cucurull, A. Casanova, A. Romero, P. LiÃ², and Yoshua
Bengio. 2018. Graph Attention Networks. ArXiv abs/1710.10903 (2018).
[37] Janu Verma, Srishti Gupta, Debdoot Mukherjee, and Tanmoy Chakraborty. 2019.
Heterogeneous edge embedding for friend recommendation. In Advances in
Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne,
Germany, April 14â€“18, 2019, Proceedings, Part II 41. Springer, 172â€“179.
[38] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful
are Graph Neural Networks?. In ICLR. OpenReview.net. http://dblp.uni-trier.de/
db/conf/iclr/iclr2019.html#XuHLJ19
[39] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation Learning on Graphs
with Jumping Knowledge Networks.. In ICML (Proceedings of Machine Learning
Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.). PMLR, 5449â€“5458.
http://dblp.uni-trier.de/db/conf/icml/icml2018.html#XuLTSKJ18
[40] Naganand Yadati. 2020. Neural Message Passing for Multi-Relational Ordered and
Recursive Hypergraphs. In Advances in Neural Information Processing Systems
(NeurIPS) 33. Curran Associates, Inc.
[41] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand
Louis, and Partha Talukdar. 2019. Hypergcn: A new method for training graph
convolutional networks on hypergraphs. Advances in neural information process-
ing systems 32 (2019).
[42] Kai Yang, Hao Sun, Chunbo Zou, and Xiaoqiang Lu. 2021. Cross-attention
spectralâ€“spatial network for hyperspectral image classification. IEEE Transactions
on Geoscience and Remote Sensing 60 (2021), 1â€“14.
[43] Jonathan S. Yedidia. 2011. Message-Passing Algorithms for Inference and Op-
timization. Journal of Statistical Physics 145 (2011), 860â€“890. https://api.
semanticscholar.org/CorpusID:120991239
[44] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong
Sun. 2018. Graph Neural Networks: A Review of Methods and Applications.
ArXiv abs/1812.08434 (2018). https://api.semanticscholar.org/CorpusID:56517517
[45] J.Y. Zien, M.D.F. Schlag, and P.K. Chan. 1999. Multilevel spectral hypergraph
partitioning with arbitrary vertex sizes. IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems 18, 9 (1999), 1389â€“1399. https://doi.org/
10.1109/43.784130
 
2557DPHGNN: A Dual Perspective Hypergraph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A SUPPLEMENTARY MATERIAL
A.1 Proof of Proposition
Proof. We generalize the work of [ 6,28] on hypergraph color refine-
ment from (see Eq. (4)in the main text) and using tensors to encode the color,
for hypergraphs having ğ‘˜vertices inside an edge ğ»ğ¶âˆˆÎ£ğ‘›ğ‘˜, as the color
of a node is induced by the coloring of its edges by a mapğ‘“ğ»(ğ‘’):ğ‘‰ğ‘˜â†’Î£.
Therefore, the tensor ğ»ğ¶âˆˆRğ‘›ğ‘˜Ã—ğ‘encodesğ‘˜vertices inside an edge
ğ‘–âˆˆ[ğ‘›]ğ‘˜withğ»ğ¶ğ‘–âˆˆÎ£and color Î£âˆˆRğ‘. Initialising ğ»ğ¶ğ»
0(ğ‘£)=1for all
ğ‘£âˆˆğ‘‰(ğ»), the color refinement is given by:
ğ»ğ¶ğ»
ğ‘¡+1(ğ‘£)=HASH{{{ğ»ğ¶ğ»
ğ‘¡(ğ‘¢)|ğ‘¢âˆˆğ‘“ğ»(ğ‘’)}|ğ‘’âˆˆğ¸ğ‘–withğ‘£âˆˆğ‘“ğ»(ğ‘’)}},
{{.}}represents multiset notation. ğ»1andğ»2are non-isomorphic if
nn
ğ»ğ¶ğ»1
ğ‘–(ğ‘£)|ğ‘£âˆˆğ‘‰(ğ»1)oo
â‰ nn
ğ»ğ¶ğ»2
ğ‘–(ğ‘£)|ğ‘£âˆˆğ‘‰(ğ»2)oo
We start by proving for any HGNN of the form of Eq1, there exists an
injective mapping Î“between every iteration of color refinement and the
layer of message propagation such that Î“(ğ‘¡,ğ‘™) ğ»ğ¶ğ‘¡ğ‘£â†’
â„ğ‘™ğ‘£
.
Assume that injective mapping Î“(ğ‘¡,ğ‘™)exists. For(ğ‘¡,ğ‘™)=0trivially, both
tensors are initially assigned some value under injectivity ğ»ğ¶0ğ‘£,â„0ğ‘£.
For(ğ‘¡+1,ğ‘™+1), the feature update is given by,
â„ğ‘™+1
ğ‘£=ğœ™3
â„ğ‘™
ğ‘£â€²,ğœ™2nn
ğœ™1nn
â„ğ‘™
ğ‘¢|ğ‘¢âˆˆğ‘’oo
|ğ‘’âˆˆğ¸ğ‘–oo
asğœ™1,ğœ™2,ğœ™3are all injective, they could be composed into an injective func-
tionğœ‘, therefore,
=ğœ‘
â„ğ‘™
ğ‘£,nnnn
â„ğ‘™
ğ‘¢|ğ‘¢âˆˆğ‘’oo
|ğ‘’âˆˆğ¸ğ‘–oo
=ğœ‘
Î“(ğ‘¡,ğ‘™) ğ»ğ¶ğ‘¡
ğ‘£,nnnn
Î“(ğ‘¡,ğ‘™) ğ»ğ¶ğ‘¡
ğ‘¢|ğ‘¢âˆˆğ‘’oo
|ğ‘’âˆˆğ¸ğ‘–oo
=Î“(ğ‘¡,ğ‘™+1) ğ»ğ¶ğ‘¡
ğ‘£â€²,  ğ»ğ¶ğ‘¡
ğ‘¢|ğ‘¢âˆˆğ‘’		|ğ‘’âˆˆğ¸ğ‘–		
=Î“(ğ‘¡+1,ğ‘™+1)
ğ»ğ¶ğ‘¡+1
ğ‘£
Where injective function Î“(ğ‘¡+1,ğ‘™+1)is induced by ğœ‘,Î“(ğ‘¡,ğ‘™). Hence by prin-
ciples of induction, we can say, there exists an injective function Î“(ğ‘¡,ğ‘™)at
timeğ‘¡and layerğ‘™such that Î“(ğ‘¡,ğ‘™) ğ»ğ¶ğ‘¡ğ‘£â†’
â„ğ‘™ğ‘£
.
Therefore, at iteration ğ‘‡, ifğ»ğ¶(ğ‘¡)(ğ‘‰1,ğ»1)â‰ ğ»ğ¶(ğ‘¡)(ğ‘‰2,ğ»2),thennn
â„ğ‘‡
ğ‘£,ğ»1oo
ğ‘£âˆˆğ‘‰1â‰ nn
â„ğ‘‡
ğ‘£,ğ»2oo
ğ‘£âˆˆğ‘‰2. â–¡
A.2 Proof of Theorem 1
Proof. Part 1: We first establish that a function ğ‘”ğœƒâˆˆğ¹DPHGNNbelongs
to a family of ğ‘˜-order invariant networks.
Letğ‘†ğ‘›denote the permutation group on ğ‘›elements. In the dynamic
feature fusion (DFF), dynamic features ğ‘š(ğ‘™)
ğ‘=ğœ™ğ‘šğ‘ğ‘ ğ‘˜[L]whereğ‘âˆˆğ‘‰âˆ—
comes from the update rule:
ğ¿=h
Aggr(ğ‘™)n
ğ‘š(ğ‘™âˆ’1)
ğ‘¢âˆ€ğ‘¢âˆˆğ‘(ğ‘)oi
. The function ğœ™ğ‘šğ‘ğ‘ ğ‘˜ , is bijective;
therefore, the updated representations satisfy L(ğ‘“Â·ğ‘š(ğ‘™)
ğ‘)=ğ‘“Â·ğ¿(ğ‘š(ğ‘™)
ğ‘),
fâˆˆğ‘†ğ‘›. Hence, the update mechanism of DPHGNN follows the form (a
ğ‘˜-order invariant graph network) ğ¹=ğ‘šâ—¦â„â—¦ğ¿ğ‘‘â—¦ğœâ—¦Â·Â·Â·â—¦ğœâ—¦ğ¿1, where
â„is an invariant linear layer, ğœis non-linear activation function and ğ¿ğ‘–are
equivariant layers, ğ‘šis the multilayer perceptron (FC). [ 28], in Theorem 1,
proved the expressive power of ğ‘˜-order invariant graph network equivalent
to theğ‘˜-WL test (for graphs).
Part 2: We quantify the expressive power of DPHGNN as a 1-FWL test.
The main difference is between the update rules of WL and FWL tests.
FWL assigns distinct colorings to the ğ‘˜-tuple of nodes. The feature fusion
mechanism in DPHGNN updates the hypernode aggregation as
Figure 4: Generalized version of experimentation pipeline.
ğ‘š(ğ‘™âˆ’1)
ğ‘’ =Aggr(ğ‘™âˆ’1)
{{ğ‘š(ğ‘™âˆ’1)
ğ‘¢âˆ€ğ‘¢âˆˆğ‘’}}âˆª{{ğ‘š(ğ‘™)
ğ‘âˆ€ğ‘âˆˆğ¸}}
.
Therefore, now the tensor HCâˆˆR(ğ‘›ğ‘˜Ã—ğ‘)+ğ‘encodes the hypernodes, where
coloringğ‘is assigned by mapğ‘“ğº(ğ‘’):ğ‘‰â†’ğ‘‰â€²andğ‘‰â€²âˆˆRğ‘. We formulate
the FWL version of hypergraph color refinement as,
HCğ»
ğ‘¡+1(ğ‘£)=HASH{{{ğ»ğ¶ğ»
ğ‘¡(ğ‘¢)|ğ‘¢âˆˆğœ“ğ»(ğ‘’)}|ğ‘’âˆˆğ¸ğ‘–withğ‘£âˆˆğœ“ğ»(ğ‘’)}},
where mapğœ“ğ»(ğ‘’):=(ğ‘“ğºâ—¦ğ‘“ğ»)(ğ‘’). The update mechanism of the hypern-
odes in DPHGNN is given by:
ğ‘¥(ğ‘™)
ğ‘£=Update(ğ‘™)({ğ‘¥(ğ‘™âˆ’1)
ğ‘£,Aggr(ğ‘™){ğ‘š(ğ‘™âˆ’1)
ğ‘’âˆ€ğ‘’âˆˆğ¸ğ‘–}}). The injectivity of
Aggregation and Readout functions follows similarly from Proposition . The
expressive power of ğ‘˜-FWL test is equivalent to (ğ‘˜+1)-WL forğ‘˜â‰¥2. The
functionğ‘”ğœƒâˆˆğ¹DPHGNNis at least as powerful as the 3-GWL test. â–¡
A.2.1 Convergence Analysis. Here, we describe the convergence analysis
of DPHGNN for the standard hypergraph datasets and the CO-RTO dataset.
Figure 3 reflects the behavior of cross-entropy loss over the change in
hypergraph topology. The convergence for co-citation is slow as the spectral
information is propagated as inductive priors. However, DPHGNN achieves
a fast convergence rate for the Co-authorship datasets as explicit lower-order
information is propagated as dynamic feature fusion. To learn in resource-
constrained settings for the Co-RTO dataset, the loss initially increases and
converges better than in co-citation networks. This signifies the flow of
inductive priors. Due to the sparse and highly symmetrical structure, the
initial performance suffers from providing better representations, the static
and dynamic fusion of lower-order information helps message propagation
to achieve better convergence.
 
2558KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, and Tanmoy Chakraborty
Datasets
Modules lr weight_decay dropout attention_heads hidden # layers
GNN
module 0.1 5.0E-04 0.2 - 64 2
CA-Cora TAA module 0.001 0.001 0.5 2 32 1
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 5.0E-04 0.5 - 64 2
GNN
module 0.1 5.0E-05 0.6 - 64 2
CA-DBLP TAA module 0.1 0.001 0.5 2 32 1
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 5.0E-04 0.6 - 64 2
GNN
module 0.001 0.001 0.5 - 64 2
CC-Cora TAA module 0.01 5.0E-04 0.5 8 8 1
SIB module 0.01 0.0005 0.4 64 1
DFF module 0.01 5.0E-04 0.6 - 64 2
GNN
module 0.1 1.0E-05 0.9 - 64 2
CC-Citeseer TAA module 0.001 0.001 0.5 8 8 1
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 5.0E-04 0.6 - 64 2
GNN
module 0.01 5.0E-05 0.3 - 64 2
CO-RTO TAA module 0.001 0.001 0.5 2 32 1
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 5.0E-04 0.5 - 64 2
GNN
module 0.01 5.0E-05 0.3 - 64 2
YelpRestaurant TAA module 0.001 0.001 0.5 2 64 1
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 5.0E-04 0.5 - 64 2
GNN
module 0.01 5.0E-05 0.3 - 64 2
HouseCommittees TAA module 0.001 0.001 0.5 4 32 4
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 5.0E-04 0.5 - 64 2
GNN
module 0.01 5.0E-05 0.3 - 64 2
Cooking200 TAA module 0.001 0.001 0.5 8 64 1
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 1.0E-03 0.5 - 64 2
GNN
module 0.01 1.0E-2 0.3 - 64 2
News20 TAA module 0.001 0.001 0.5 4 32 1
SIB module 0.01 0.0005 0.4 - 64 1
DFF module 0.01 0.01 0.5 - 64 2
Table 10: Hyperparameters used in our experiments.
Dataset
HGNN HGNN+ HyperGCN HNHN UniGCN UniSAGE UniGAT DPHGNN
CA
-DBLP 605.9 401 563.4 358 432 455 502 528
CA-Cora 228.4 145 183.4 122 161 157 174 182
CC-CiteSeer 290.1 122 171.1 135 140 152 186 168
CC-Cora 301.4 167 205.4 130 174 193 201 220
Yelp Restaurant 570 194 495 185 218 223 247 312.2
House Committees 192 114 153 66 72 96 150 168
Cooking200 502 156 258 144 174 216 216 233
News20 406 138 210 108 102 108 156 174
CO-RTO 384 192 306 108 138 150 174 205
Table 11: Runtime analysis between proposed DPHGNN with baseline architectures (in seconds).
A.3 Implementation Details
Hyperparameter Details. Table 10 summarizes the hyperparameter de-
tails for each submodule of DPHGNN architecture. We used GridSearchCV
(from the sklearn library) for hyperparameter tuning. The sub-modules of
DPHGNN, namely (i) the GNN module comprises the graph convolution
training of multiple views of decomposed graph topology (as described in
the TAA section in the main paper); (ii) the TAA module contains a multi-
head wrapper of cross-attention modules with specific heads for each of
the hypergraph datasets; (iii) the SIB module is trained as a hybrid spectralconvolution layer. For the model parameter optimizations, we use a specific
parameter group for each of the submodules of the DPHGNN architecture.
Experimentation Details. For the semi-supervised hypernode classifica-
tion task, we train the model on each dataset for 400 epochs and infer the
trained model from the last epoch. We perform 50 iterations of DPHGNN
on each dataset, with five train/test splits and ten different random seeds.
Figure 4 presents a concise view of the training mechanism adopted by
DPHGNN.
 
2559