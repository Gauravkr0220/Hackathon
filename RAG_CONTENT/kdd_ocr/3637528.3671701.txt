DFGNN: Dual-frequency Graph Neural Network for Sign-aware
Feedback
Yiqing Wuâˆ—
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
iwu_yiqing@163.comRuobing Xie
Tencent
Beijing, China
ruobingxie@tencent.comZhao Zhangâ€ 
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
zhangzhao2021@ict.ac.cn
Xu Zhang
Tencent
Beijing, China
xuonezhang@tencent.comFuzhen Zhuangâ€¡â€ 
Institute of Artificial Intelligence,
Beihang University
Beijing, China
zhuangfuzhen@buaa.edu.cnLeyu Lin
Zhanhui Kang
Tencent
Beijing, China
{goshawklin,kegokang}@tencent.com
Yongjun Xu
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
xyj@ict.ac.cn
ABSTRACT
The graph-based recommendation has achieved great success in re-
cent years. However, most existing graph-based recommendations
focus on capturing user preference based on positive edges/feedback,
while ignoring negative edges/feedback (e.g., dislike, low rating)
that widely exist in real-world recommender systems. How to utilize
negative feedback in graph-based recommendations still remains
underexplored. In this study, we first conducted a comprehensive
experimental analysis and found that (1) existing graph neural net-
works are not well-suited for modeling negative feedback, which
acts as a high-frequency signal in a user-item graph. (2) The graph-
based recommendation suffers from the representation degenera-
tion problem. Based on the two observations, we propose a novel
model that models positive and negative feedback from a frequency
filter perspective called Dual-frequency Graph Neural Network for
Sign-aware Recommendation (DFGNN). Specifically, in DFGNN,
the designed dual-frequency graph filter (DGF) captures both low-
frequency and high-frequency signals that contain positive and
negative feedback. Furthermore, the proposed signed graph regu-
larization is applied to maintain the user/item embedding uniform
in the embedding space to alleviate the representation degeneration
âˆ—Yiqing Wu is also at the University of Chinese Academy of Sciences, China.
â€ Zhao Zhang and Fuzhen Zhuang are the corresponding authors.
â€¡Fuzhen Zhuang is also at Zhongguancun Laboratory, Beijing, China
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671701problem. Additionally, we conduct extensive experiments on real-
world datasets and demonstrate the effectiveness of the proposed
model. Codes of our model will be released upon acceptance.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems.
KEYWORDS
Graph neural network, Signed graph neural network, Negative
feedback, Sign-aware recommendation
ACM Reference Format:
Yiqing Wu, Ruobing Xie, Zhao Zhang, Xu Zhang, Fuzhen Zhuang, Leyu
Lin, Zhanhui Kang, and Yongjun Xu. 2024. DFGNN: Dual-frequency Graph
Neural Network for Sign-aware Feedback. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671701
1 INTRODUCTION
Personalized recommender systems aim to provide appropriate
items to the users on platforms. In such systems, the interactions
between users and items can naturally be regarded as a user-item
bipartite graph. Inspired by the success of Graph Neural Networks
(GNNs), the graph-based recommendation model has been widely
studied and achieved remarkable progress. However, most existing
graph-based recommendation models are merely built on positive
edges/feedback. Actually, there is always various negative feedback
in real-world recommender systems, such as a low rating, dislike,
and short watching time. The negative feedback generally repre-
sents what people do not like or even hate. Recommendations of
such items to users may significantly harm the user experience.
Besides, only focusing on what users like while ignoring what users
dislike may lead to homogenization in the recommended items [ 42].
3437
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yiqing Wu et al.
Although negative feedback is crucial and is believed to benefit
the recommendation [ 18], how to model the negative feedback in
graph-based recommendation still remains underexplored. A trivial
solution is to introduce negative feedback edges to the graph and
model them in a manner similar to positive feedback. However,
existing GNNs heavily rely on the homophily assumption, where
two nodes connected by an edge tend to be more similar. While,
for negative feedback, two nodes connected by an edge may imply
dissimilarity. A similar field is the signed graph neural network. It is
designed for processing signed graphs, which contain two opposite
edge types (e.g., accept/reject, fraud/trustworthiness). The signal in
the signed graph generally is objective, and most of them are built
upon balance theory, which means the friend of my friend is my
friend, and the enemy of my enemy is my friend. However, the user
feedback in recommendations is subjective and personalized, and
the balance theory does not always hold in recommendations [ 27].
Currently, limited works [ 16] have studied the negative feedback in
graph-based recommendations. While they still adopt GNNs that
rely on the homophily assumption.
Based on the above analysis, we start to ponder "How to model
positive and negative feedback in graph-based recommendation?" To
this end, we deeply contemplate the characteristics of positive and
negative feedback. (1) Positive and negative feedback carry differ-
ent meanings. Generally, if an item fits usersâ€™ interests, the people
will produce positive feedback. In recommendation, this implies a
certain degree of similarity between items and users. In contrast,
negative feedback implies dissimilarity. While this makes the nega-
tive feedback violate the homophily assumption of GNN. (2) The
reasons for negative feedback are heterogeneous. Compared to posi-
tive feedback, the reasons for users produce negative feedback are
diverse and complex. It could stem from a variety of reasons such
as the user hating the categories of the item, the items not meeting
the userâ€™s expectations, or even due to the user hating a specific
word in the title. (3)The negative feedback is generally sparse. The
recommender system aims to recommend what people will like,
the negative feedback actually is an anomaly, making it sparse. For
example, in the Amazon dataset, less than 10% of reviews have low
ratings.
Looking deeper into the structure information of graph-based
recommendation, we can discover that: a user node is connected
to many nodes that are similar to it by positive feedback, result-
ing in a smooth similarity terrain. However, there are also a few
strange peaks that stand out, appearing out of place, that is the
negative feedback. This inspires us to think about positive and neg-
ative feedback essentially from the perspective of signal processing.
In which the smooth and unchanged signals are regarded as low-
frequency signals. While steep, rapid changes are considered as
high-frequency signals. This is highly analogous to the characteris-
tics of positive and negative feedback. To evaluate our assumption,
we utilize graph spectral theory to experimentally analyze the pos-
itive and negative feedback in the frequency-domain (detailed in
Sec. 3.1) and find that the positive feedback indeed implies low-
frequency signals in the graph while the negative feedback implies
high-frequency signals in the graph. Furthermore, the GNNs tend
to over-smooth low-frequency signals. Through experiment, we
also find that the learned embedding suffers from a representation
degeneration problem, which means the embedding shrinks to asmall piece of embedding space thus resulting in a loss of expressive
power. This issue has a more significant impact on signed graphs, as
it leads to an inability to distinguish between negative and positive
feedback.
Considering the characteristics of negative and positive feed-
back, we propose a novel method to model them in graph-based
recommendations, called Frequency-aware signed graph neural
network (DFGNN). Specifically, considering the low-frequency
signal and high-frequency signal contained in positive and neg-
ative feedback, respectively, we propose a dual-frequency graph
filter(DGF). In DGF we utilize the designed low-passing graph fil-
ter to model the positive feedback while adopt the high-passing
filter to model the negative feedback. To alleviate the representa-
tion degeneration problem caused by GNN, we propose a signed
graph regularization (SGR) loss. SGR considers both the similar
and dissimilar information in the signed graph and regularizes the
learned embedding to be uniformly distributed in the space. The
experiments on real-world datasets demonstrate that our model
achieves significant improvements over competitive baselines.
Overall, the contribution of this paper is summarized as follows:
â€¢We first conduct a comprehensive analysis of positive and
negative feedback in the graph-based recommendation from
the frequency perspective. Furthermore, we design a dual-
frequency filter to both model low-frequency signals and
high-frequency signals contained in positive and negative
feedback.
â€¢We investigate the representation degeneration problem in
graph recommendation, and propose a sign graph regular-
ization loss to alleviate it.
â€¢We conduct extensive experiments on real-world datasets
and evaluate the effectiveness of the proposed DFGNN in
two recommendation tasks.
2 PRELIMINARIES
2.1 Signed Graph in Recommendation
In real-world online recommender systems, users generally have
various interactions with items and generate feedback to the sys-
tem, including both positive feedback (e.g. like, buy) and negative
feedback (e.g. dislike, low rating). Based on the positive and neg-
ative feedback, we define the signed graph in recommendation.
Specifically, we denote the signed graph as ğº={ğ‘ˆ,ğ‘‰,{E+,Eâˆ’}},
whereğ‘ˆ={ğ‘¢1,ğ‘¢2,...,ğ‘¢|ğ‘ˆ|}andğ‘‰={ğ‘£1,ğ‘£2,...,ğ‘£|ğ‘‰|are the user set
and item set in the system, and {E+,Eâˆ’}are positive and negative
edges in the graph, respectively. If a user has positive feedback for
an item, there will be a positive edge ğœ‰=(ğ‘¢,ğ‘£,+)âˆˆE+. Conversely,
a negative edge ğœ‰=(ğ‘¢,ğ‘£,+)âˆˆEâˆ’will be created if there is negative
feedback.
2.2 Graph Fourier Transform
To better understand the graph Fourier transform. We first intro-
duce the Fourier transform. Fourier transform is a powerful analyt-
ical tool and is widely applied across various fields. It transforms
signals from the time domain to the frequency domain. The Fourier
transform can be formulated as :
ğ‘“(ğœ”)=F(ğ‘“(ğ‘¥),ğœ”)âˆ«âˆ
âˆ’âˆğ‘“(ğ‘¥)Â·ğ‘’âˆ’2ğœ‹ğ‘–ğœ”ğ‘‘ğ‘¥, (1)
3438DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000005b/uni00000014/uni00000011/uni00000018
/uni00000014/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000056/uni0000004c/uni00000051/uni0000000b/uni0000005b/uni0000000c
(a) sin(x)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000005b/uni00000013/uni00000011/uni00000019
/uni00000013/uni00000011/uni00000017
/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni00000015/uni00000056/uni0000004c/uni00000051/uni0000000b/uni00000014/uni00000013/uni0000005b/uni0000000c (b) 0.2 sin(10x)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni0000005b/uni00000014/uni00000011/uni00000018
/uni00000014/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000056/uni0000004c/uni00000051/uni0000000b/uni0000005b/uni0000000c/uni0000000e/uni00000013/uni00000011/uni00000015/uni00000056/uni0000004c/uni00000051/uni0000000b/uni00000014/uni00000013/uni0000005b/uni0000000c (c) 0.2 sin(10x)+sin(x)
/uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048
(d)F(sin(x))
/uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048
 (e)F(0.2 sin(10x))
/uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048
 (f)F(0.2 sin(10x)+sin(x))
Figure 1: Different frequency signals in the time domain and
frequency domain.
whereFis denoted as Fourier transform operation, ğœ”is the fre-
quency,ğ‘“(ğœ”)are the amplitudes of corresponding ğœ”.Generally,
low frequencies correspond to smooth signals, while high fre-
quencies correspond to non-smooth signals. The amplitude
represents the strength of the signal at the corresponding
frequency. To better understand this, we demonstrate this phenom-
enon in Fig.1. We can see that ğ‘ ğ‘–ğ‘›(ğ‘¥)(i.e., Fig. 1(a)) is smoother, thus
its corresponding low frequency is higher(i.e., Fig. 1(d)). In contrast,
0.2ğ‘ ğ‘–ğ‘›(10ğ‘¥)oscillates rapidly (i.e., Fig. 1(b)), and correspondingly,
the amplitude of high frequencies is higher (i.e. Fig. 1(e)). Fig. 1(c)
looks complex in the time domain, but in the frequency domain
(Fig. 1(f)), it looks simpleâ€”itâ€™s just the superposition of two sine
waves,ğ‘ ğ‘–ğ‘›(ğ‘¥)and0.2ğ‘ ğ‘–ğ‘›(10ğ‘¥).
To extend the Fourier transform to the graph/non-euclidean
domain, Laplacian matrices ğ¿are introduced to the graph processing
field. Classical Laplacian matrice is defined as ğ¿=ğ·âˆ’ğ´, whereğ´
is the adjacent matrix and ğ·is the degree matrix of a graph. ğ¿is a
real symmetric matrix, thus it can be written as ğ¿=ğ¸Î›ğ¸ğ‘‡, where
ğ¸={ğ‘’1,ğ‘’2,...,ğ‘’ğ‘}is the eigenvector and Î›={ğœ†1,ğœ†2,...,ğœ†ğ‘}is
the eigenvalue of ğ¿. Similar to the Fourier transform in the time
domain, the Fourier transform in the graph/non-euclidean domain
is defined as:
ğ‘“(ğœ†ğ‘™)=ğ‘âˆ‘ï¸
0ğ‘¥(ğ‘–)ğ‘’ğœ†ğ‘™(ğ‘–), (2)
whereğ‘’ğœ†ğ‘™is the eigenvector that corresponds to eigenvalue ğœ†ğ‘™, and
ğ‘¥(ğ‘–)is the node feature of ğ‘–-th node. Graph Fourier Transform
shares similar characteristics with the time-domain Fourier Trans-
form. Lowğœ†values represent low-frequency signals, while
highğœ†values represent high-frequency signals. Besides, ğ‘“(ğœ†)
represents the strength of the corresponding frequency.
3 EXPERIMENTAL OBSERVATIONS
In this section, we aim to analyze the characteristics of negative
and positive feedback and the challenges during the processingof signed graphs in recommendation. In the following, we first
analyze the negative feedback from a frequency perspective, then
we analyze the representation degeneration problem in the graph-
based recommendation.
3.1 Analysis of Positive and Negative Feedback
from Frequency-domain
Generally, the user behaviors in a system can be modeled to a signal
graph, where users and items are nodes, edges represent interac-
tions on the graph, and user and node features serve as signals
on the graph. As introduced in Sec.2.3, with the help of the graph
Fourier transform, we can analyze signals from a frequency perspec-
tive. To explore the characteristics of positive/negative feedback
on the graph, we conduct experimental analysis in the frequency
domain. Specifically, we first employ the classical Matrix Factoriza-
tion [ 8] and learn one-dimensional embeddings for users and items
as the signals of the graph. Then we construct two graphs that only
contain positive edges and negative edges respectively, denoted as
ğº+andğºâˆ’. Last, we apply graph Fourier transform to the ğº+and
ğºâˆ’. We conduct this experiment on ml-100k dataset and show the
distribution of frequency in Fig.2. By comparing Fig 2 (a) and Fig 2
(b), we can observe that the signal of ğº+is prominent in low fre-
quencies, while ğºâˆ’is prominent in high-frequency signals. Recall
that, the low frequencies represent the smooth signals, and high
frequencies represent unsmooth signals. In the temporal domain,
smoothness can be defined as the magnitude of the signal change
between timestep ğ‘¡âˆ’1andğ‘¡. Similarly, in the graph, smooth-
ness can be measured by the difference between the nodes
connected by an edge[ 29]. The smaller the difference between
nodes connected by an edge, the smoother the graph. Therefore the
results in Fig 2 align with intuition. In recommendation, we gen-
erally assume that users have higher similarity to items they like,
and correspondingly, the ğº+is dominated by low-frequency sig-
nal. On the contrary, users are dissimilar to items they dislike, and
correspondingly, Gâˆ’exhibits more high-frequency signals. Most
existing GNNs actually act as a low-passing filter[ 12,22,34], which
means the GNN extracts low-frequency signals while discarding
high-frequency signals. Unfortunately, most existing Graph-based
recommendation models adopt the low-passing GNN to encode
graphs. Thus, they may not be suitable for capturing negative feed-
back signals.
3.2 The Representation Degeneration Problem
in Graph-based Recommendation
Recently, graph neural networks have achieved great success in
recommendation systems [ 12,35,40]. However, GNNs are known
as the over-smoothing effect [3, 7], which means the node feature
tends to be similar during the GNN training, especially in deep
graph neural networks. The over-smoothing effect sometimes will
cause the representation degeneration problem. In this issue, the
embeddings shrink to a small piece of space, thus losing the expres-
sive power. To explore whether the graph-based recommendation
will suffer from this problem, we train a graph model (i.e. GCN[ 22])
and a non-graph model (i.e. NCF[ 13]) on Amazon ArtsCrafts review
dataset, respectively. We project the learned embedding matrix into
2D by SVD and visualize it. The results are shown in Fig. 3. From
3439KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yiqing Wu et al.
0 2 4 6 8
Bucket indexe0.00.20.40.60.81.01.2Normalized F()
(a) negative feedback
0 2 4 6 8
Bucket Index0.00.20.40.60.81.0Normalized f(
 (b) positive feedback
Figure 2: The distribution of f( ğœ†) calulated based on nor-
malzied Laplacian matrix. (a) It is the distribution of only
negative edges graph. (b) It is the distribution of only positive
edges graph. We evenly split the frequency ğœ†into ten buckets
and calculate the normalized f( ğœ†).
this figure, we can find that the embedding learned by NCF is more
uniform than that learned by GCN. Most node embeddings learned
by GCN fall into a narrow zone. Furthermore, we analyze the dis-
tribution of singular values of embedding matrices and show the
results in Fig.3. We can observe a rapid decline in the singular val-
ues of GCN, indicating that the learned embeddings are low-rank
and have less expressive power.
(a) NCF
 (b) GCN
 (c) Singular values
Figure 3: The visualized embedding and singular values. (a) is
the embedding learned by NCF. (b) is the embedding learned
by GCN. (c) is the normalized singular values of GCN and
NCF.
4 METHOD
Based on the above experimental observations, in this paper, we
propose a novel graph model called DFGNN to solve the issues
in the sign-aware graph recommendation. As shown in Figure.4,
our DFGNN contains two components: (1) dual frequency graph
filter (DGF) and (2) graph regularization. The dual-frequency graph
filter aims to both capture the high-frequency signal and the low-
frequency in the negative feedback and positive feedback respec-
tively. Furthermore, the graph regularization module is adopted to
alleviate the representation degeneration problem. In the following,
we will make a detailed introduction to our DFGNN.
4.1 Dual Frequency Graph Filter
The negative feedback is crucial and widely present in recommen-
dation systems, which show what people dislike or even hate whatkind of items. However, how to deal with negative feedback is
not a trivial problem. As the analysis in Sec 3.1, different from the
positive feedback, the negative feedback usually acts as the high-
frequency signal in the graph. Hence we propose a dual-frequency
graph filter (DGF), which uses a low-passing graph filter (LGF) to
capture the low-frequency signal in positive feedback and adopts
a high-passing graph filter (HGF) to capture the high-frequency
signal in negative feedback.
4.1.1 Graph Filter. The convolution operation is generally adopted
as a filter to extract useful signals. The graph convolution can be
defined as:
ğ‘¥âˆ—ğ‘”=Fâˆ’1(F(ğ‘¥)âˆ—F(ğ‘”))=Fâˆ’1(ğ‘“(ğœ†)ğ‘”(ğœ†))
=ğ¸((ğ¸ğ‘‡ğ‘”)Â¤(ğ¸ğ‘‡ğ‘¥))=ğ¸ğ‘”(ğœ†)ğ¸ğ‘‡ğ‘¥,(3)
whereğ‘”(ğœ†)is the filter kernel function with respect to the eigenval-
uesğœ†of the Laplacian matrix. Generally, for a low-pass filter, ğ‘”(ğœ†)
is high at low ğœ†but low at high ğœ†. For a high-pass filter, it is the
opposite.(ğ‘”(ğœ†)is high at high ğœ†and low at low ğœ†.
4.1.2 Low-passing Graph Filter. After defining the graph filter, we
can design the low-passing filter. Luckily, most existing GNNs act as
low-passing graph filters. In this work, we adopt the classical GCNs
[22] as our low-passing graph filter. The GCNs can be formulated
as:
ğ»ğ‘™+1=ğœ(Ë†ğ´ğ»ğ‘™ğ‘Šğ‘™),ğ»0=ğ‘‹, (4)
whereğ‘Šğ‘™âˆˆğ‘…ğ‘‘1âˆ—ğ‘‘2is the trainable parameters of the network,
ğ»ğ‘™âˆˆğ‘…ğ‘âˆ—ğ‘‘1is the node representation after ğ‘™layer convolution.
Ë†ğ´=Ëœğ·âˆ’1
2Ëœğ´Ëœğ·1
2is a designed matrix, where Ëœğ´=ğ´+ğ¼are the
augmented adjacent matrix with adding self-loops and Ëœğ·=ğ·+ğ¼
are the degree matrix of Ëœğ´.
4.1.3 High-passing Graph Filter. The Eq. (3) guides us on how to
design a high-passing graph filter. However, it still is not a trivial
task, as the eigenvectors ğ¸are difficult to compute. Hence, we
design a high-passing filter based on the existing low-passing filter
(i.e., GCNs[ 22]). To achieve it, We first hand on why GCNs are low-
passing filter. The normalized Laplacian matrix [ 29] is defined
asğ¿=ğ¼âˆ’ğ·âˆ’1
2ğ´ğ·âˆ’1
2. Thus, the matrix Ë†ğ´can be written as Ë†ğ´=
ğ¼âˆ’(ğ¼âˆ’Ëœğ·âˆ’1
2Ëœğ´Ëœğ·âˆ’1
2)=ğ¼âˆ’Ëœğ¿. AsËœğ¿=Ëœğ¸ËœÎ›Ëœğ¸ğ‘‡, the Ë†ğ´ğ»can be transformed
toË†ğ´ğ»=(ğ¼âˆ’Ëœğ¿)ğ»=Ëœğ¸(ğ¼âˆ’ËœÎ›)Ëœğ¸ğ‘‡ğ». Comparing with Eq.(3), we can
find that the graph kernel of one layer GCNs is ğ‘”(ğœ†)=1âˆ’Ëœğœ†. With
stacking of ğ¾layer GCNs the graph kernel is ğ‘”(ğœ†)=(1âˆ’Ëœğœ†)ğ¾.
Moreover, the eigenvalues of the normalized Laplacian matrix are
[0,2), and the eigenvalues of the Laplacian matrix augmented by
adding a self-loops are [0,1.5). This implies that GCN will amplify
low-frequency signals and attenuate high-frequency signals, as
shown in Fig. 5 (a). Thus GCNs actually act as low-passing filters.
After analyzing why GCNs is a low-passing graph filter, we
design the high-passing as :
ğ»ğ‘™+1=ğœ(ğ¿ğ»ğ‘Š)=ğœ(ğ·âˆ’1
2(ğ·âˆ’ğ´)ğ·âˆ’1
2ğ»ğ‘Š),ğ»0=ğ‘‹, (5)
whereğ´andğ·are the adjacent matrix and degree matrix without
augmentation. The kernel filter function of the high-passing filter is
ğ‘”(ğœ†)=ğœ†. As shown in Fig. 5 it will attenuate low-frequency signals
and amplify high-frequency signals.
3440DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
n layers 
Signed Graph RegularizationGraph Node EmbeddingsHigh Fr equence Filter
Low Fr equence Filter
DGF
Signed Graph with Differ ent Types
of EdgesPositive Edge
Negative Edge
Figure 4: Overall architecture of DFGNN.
0.00 0.25 0.50 0.75 1.00 1.25 1.50
Eigenvalue()
0.4
0.2
0.00.20.40.60.81.0value of g()
GCNs's graph filter kernal  (1-) with layer stack
k=1
k=2
k=3
k=4
k=5
0.00 0.25 0.50 0.75 1.00 1.25 1.50
Eigenvalue()
0.5
0.00.51.01.52.0Our high-passing filter() with layer stack
k=1
k=2
k=3
k=4
k=5
Figure 5: The graph filter kernel function of LGF and HGF
with layer stack. The left is the kernel (1- ğœ†) and the right is ğœ†
4.2 Graph Encoder
In this section, we introduce how to encode the graph information
with our DGF. Generally, the user feedback in recommendation
contains various noises. Looking back to Fig. 1, we can see that
for positive feedback there still exists high-frequency information,
which can be regarded as noise in the graph. The low-passing filter
usually is utilized to denoise. Hence, we first use LGF on the positive
feedback. It has two functions: (1) capturing what users like by the
positive feedback. (2) weaking the noise in the graph. It can be
formulated as:
ğ»ğ‘™+1
ğ‘ğ‘œğ‘ =ğ¿ğºğ¹ğ‘™(ğ»ğ‘™,ğº+) (6)
After applying LGF to positive feedback, we apply HGF to nega-
tive feedback to capture what users dislike. As analyzed before our
HGF will amplify high-frequency signals. It can be formulated as:
ğ»ğ‘™+1
ğ‘›ğ‘’ğ‘”=ğ»ğºğ¹ğ‘™(ğ»ğ‘™,ğºâˆ’) (7)Lastly, we fusion the information aggregated from positive feed-
back and negative feedback with an MLP layer, we formulate it
as:
ğ»ğ‘™+1=ğ‘€ğ¿ğ‘ƒ(ğ»ğ‘™+1
ğ‘ğ‘œğ‘ ||ğ»ğ‘™+1
ğ‘›ğ‘’ğ‘”) (8)
4.3 Signed Graph Regularization
As discussed before, the graph neural network in recommenda-
tion may suffer from representation degeneration problems, which
means the graph embedding shrinks to a piece of embedding space
and loses representative power. Intuitively, if we could make the
embedding uniformly distributed in the space, this issue could be
alleviated. Uniform means that the average distance of each node to
all nodes should be as large as possible, so the uniform constraint
loss is defined as:
ğ‘™ğ‘¢ğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š =|ğ‘ˆ|âˆ‘ï¸
ğ‘¢âˆˆğ‘ˆlog|ğ‘‰|âˆ‘ï¸
ğ‘£âˆˆğ‘‰ğ‘’ğ‘ ğ‘–ğ‘š(ğ‘¢,ğ‘£)/ğœ, (9)
whereğ‘¢andğ‘£are the learned embedding of user and item, the
ğ‘ ğ‘–ğ‘š(Â·)is the distance function, ğœis temperature. In this work, we
adopt cosine similarity to measure the distance between two nodes.
However, complete uniform distribution still will lead to a loss
of representational capacity issue. Because the core of recommen-
dation systems lies in matching items that are similar to the userâ€™s
interests. This implies that representations of some embedding
should be similar. Furthermore, in sign-aware recommendation,
things become even more complex. Negative feedback implies that
some users and items are dissimilar, and their representations in
the space should be as far apart as possible. Therefore, to simulta-
neously consider uniformity and similarity/dis-similarity between
3441KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yiqing Wu et al.
user and items, we designed alignment loss, we formulated it as:
ğ‘™ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘šğ‘’ğ‘›ğ‘¡ =âˆ’âˆ‘ï¸
(ğ‘¢,ğ‘£)âˆˆE+logğ‘’ğ‘ ğ‘–ğ‘š(ğ‘¢,ğ‘£)/ğœ+âˆ‘ï¸
(ğ‘¢,ğ‘£)âˆˆEâˆ’logğ‘’ğ‘ ğ‘–ğ‘š(ğ‘¢,ğ‘£)/ğœ
=âˆ’âˆ‘ï¸
(ğ‘¢,ğ‘£)âˆˆE+ğ‘ ğ‘–ğ‘š(ğ‘¢,ğ‘£)/ğœ+âˆ‘ï¸
(ğ‘¢,ğ‘£)âˆˆEâˆ’ğ‘ ğ‘–ğ‘š(ğ‘¢,ğ‘£)/ğœ,
(10)
where theE+andEâˆ’are positive edge sets and negative edge
set respectively. In fact, ğ‘™ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘šğ‘’ğ‘›ğ‘¡ andğ‘™ğ‘¢ğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š engage in a two-
player game. ğ‘™ğ‘¢ğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š aims to make the learned embeddings more
uniform, while ğ‘™ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘šğ‘’ğ‘›ğ‘¡ aims to make the learned embeddings
more extreme. Finally, the regularization loss is defined as:
ğ‘™ğ‘†ğºğ‘…=ğ‘™ğ‘¢ğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š+ğ‘™ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘šğ‘’ğ‘›ğ‘¡. (11)
In this work, we apply our SGR on the user and item embedding
layer, while do not apply it to the encoded node representation.
Because we empirically found that seting results in worse perfor-
mance. It may be because the encoded representation in the top
layer is highly task-dependent, and forcing the representation to
be uniformly distributed may not be effective.
4.4 Loss Function
After getting the encoded representation of the user and item. We
can predict the value of ğ‘ƒ(ğ‘¢ğ‘–,ğ‘£ğ‘—). Following previous work[ 12,13,
20], here we directly use the inner product of â„ğ‘¢ğ‘–andâ„ğ‘£ğ‘—to calculate
it:
ğ‘(ğ‘¢ğ‘–,ğ‘£ğ‘—)=ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„ğ‘‡
ğ‘¢ğ‘–Â·â„ğ‘£ğ‘—) (12)
In this paper, we adopt classical binary cross-entropy loss[ 14,20]
to optimize the model. The total loss is defined as:
ğ‘™ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™=ğ‘™ğ‘¡ğ‘ğ‘ ğ‘˜+ğ‘¤Â·ğ‘™ğ‘†ğºğ‘…,
ğ‘™ğ‘¡ğ‘ğ‘ ğ‘˜=ğ‘âˆ‘ï¸
ğ‘¦ğ‘(ğ‘¢ğ‘–,ğ‘£ğ‘—)+(1âˆ’ğ‘¦)(1âˆ’ğ‘(ğ‘¢ğ‘–,ğ‘£ğ‘—),(13)
whereğ‘¤is the weight of ğ‘™ğ‘†ğºğ‘….
5 EXPERIMENT
In this section, we aim to answer the following questions: (1)
RQ1:How does DFGNN perform compared with other state-of-the-
art (SOTA) baselines on recommendation task? (Sec. 5.3) (2) RQ2:
How does DFGSN perform compared with other state-of-the-art
(SOTA) baselines on feedback type recognition task? (Sec. 5.4) (3)
RQ3: The proposed two modules in DFGNN indeed work? (Sec. 5.5)
(4) RQ4: How does our signed graph regulation loss help in the
training? (Sec. 5.6)
5.1 Experimental Setting
Datasets In this paper, to evaluate the performance of the pro-
posed DFGNN, we conduct comprehensive experiments on classical
ML1M, different categories of Amazon Review datasets, and
Yelp dataset. They are all real-world rating datasets. each instance
was assigned a rating from 1 to 5. we regard the ratings that are
below 3 as negative feedback and the ratings that are higher than 3
as positive feedback. Due to the space limit, we detailly introduce
and analyze them in the AppendixA.1.
Evaluation Protocols. We evaluate the proposed model on two
tasks called the recommendation ranking task and feedback typerecognition task. For the recommendation ranking task, we adopt
widely accepted ranking metrics[ 20,33]: top-K hit rate (HIT@k),
top-K (NDCG@k), and MRR, with ğ‘˜={10ğ‘ğ‘›ğ‘‘50}. Feedback type
recognition task can be regarded as a binary classification task. Con-
sidering the imbalance between positive and negative samples(as
shown in Table.3), here we use classical AUC and F1-Macro as eval-
uation metrics. It is necessary to note that we do not adopt accuracy,
precision, recall, etc., as metrics, since they are not applicable to
imbalanced datasets. For each task, we report the mean results with
five random runs.
Implement Details. We carefully tune all the baselines and com-
pare them fairly. Due to the space limit, we show the details in
Appendix. A.1.1.
5.2 Baselines
To validate the performance of DFGNN, we compare our method
with classical and state-of-the-art unsigned/signed graph neural
networks: (1) GCN[ 22], (2) GAT [32], (3) SGCN[ 6] (4)SBGNN [14],
(5)SBGCL [44] (6)SIGRec[ 16]. For unsigned GNNs, we only retain
the positive feedback edge. We implement our method by PyG1,
which is a popular graph learning framework. For baseline GCN,
GAT, and SGCN we directly use the implementation of PyG. For
baseline SBGNN and SBGCL, we directly use the authorâ€™s released
codes. For SIGRec as there is no available open-sourced code, we
implement it by ourselves. They are detailly introduced in Appen-
dix B
5.3 Performance on Recommendation
We show the results of our DFGNN in Table 1 on the recommenda-
tion task, which aims to recommend items to users from a set of
candidates. From the table, we can see that:
(1) Compared with the unsigned graph neural network (i.e., GCN
and GAT), the signed graph neural network generally outperforms
them across all datasets. It demonstrates the importance of negative
feedback. The signed graph neural network both considers the
positive feedback and negative feedback information in the graph,
thus achieving a better performance.
(2) We note that the SGCN generally performs worse than SBGCL.
SBGCL is specifically designed for bipartite graphs. It extends the
balance theory tp the butterfly structure in bipartite graphs. It
matches the graph structure characteristics in recommendation,
which are a user-item bipartite graphs. In contrast, SGCN is based
on the balance theory with a triangle structure, which may lead to
its suboptimal performance.
(3) Compared with all baselines, our DFGNN significantly out-
performs them on all datasets, even with improvements of over 50%
observed in some datasets. This can be attributed to our special de-
sign for handling negative feedback. Traditional GNNs are designed
for homogeneous graphs and rely on the homophily assumption,
which assumes that nodes connected by edges are similar. While in
the sign-aware recommendation, users and items nodes connected
by negative feedback intuitively indicate dissimilarity. Our analysis
of negative signals from a frequency view also evaluates it. Based
on this, we design dual-frequency graph filters and signed graph
1https://pytorch-geometric.readthedocs.io/en/latest/index.html
3442DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Results on recommendation task. All improvements are significant over baselines (t-test with p <0.05).
Dataset Model GCN GAT SGCN SBGNN SBGCL SIGRec DFGNN Improve
ML1MMRR 0.5085 0.5096 0.4866 0.5177 0.4707 0.5259 0.5433 3.31%
HIT@10 0.8132 0.8234 0.8041 0.8279 0.7932 0.8381 0.8576 2.33%
NDCG@10 0.2850 0.2882 0.2696 0.2972 0.2645 0.2948 0.3118 4.91%
HIT@50 0.9524 0.9592 0.9558 0.9587 0.9527 0.9608 0.9628 0.21%
NDCG@50 0.3013 0.3060 0.2912 0.3153 0.2877 0.3184 0.3311 3.99%
ArtsMRR 0.0291 0.0296 0.0324 0.0441 0.0301 0.0296 0.0643 45.80%
HIT@10 0.0406 0.0427 0.0482 0.0746 0.0432 0.0409 0.1107 48.39%
NDCG@10 0.0230 0.0234 0.0254 0.0350 0.0236 0.0231 0.0521 48.86%
HIT@50 0.1038 0.1067 0.1093 0.1799 0.1101 0.1076 0.2284 26.96%
NDCG@50 0.0323 0.0326 0.0342 0.0516 0.0334 0.0328 0.0713 38.18%
GFoodMRR 0.0421 0.0400 0.0409 0.0447 0.0372 0.0383 0.0475 6.26%
HIT@10 0.0642 0.0599 0.0615 0.0718 0.0597 0.0585 0.0756 5.29%
NDCG@10 0.0325 0.0314 0.0328 0.0363 0.0289 0.0306 0.0375 3.31%
HIT@50 0.1434 0.1273 0.1248 0.1510 0.1258 0.1189 0.1528 1.19%
NDCG@50 0.0444 0.0417 0.0423 0.0488 0.0390 0.0394 0.0493 1.02%
KindleMRR 0.0381 0.0436 0.0264 0.0476 0.0299 0.0316 0.0703 47.69%
HIT@10 0.0851 0.0930 0.0548 0.0980 0.0621 0.0682 0.1569 60.10%
NDCG@10 0.0221 0.0263 0.0135 0.0315 0.0160 0.0170 0.0502 59.37%
HIT@50 0.2285 0.2299 0.1555 0.2452 0.1740 0.1928 0.3380 37.85%
NDCG@50 0.0416 0.0455 0.0253 0.0531 0.0303 0.0326 0.0796 49.91%
YelpMRR 0.0378 0.0420 0.0448 0.0460 0.0422 0.0313 0.0511 11.09%
HIT@10 0.0819 0.0893 0.0925 0.0975 0.0902 0.0649 0.1112 14.05%
NDCG@10 0.0204 0.0231 0.0250 0.0257 0.0230 0.0164 0.0298 15.95%
HIT@50 0.2189 0.2382 0.2370 0.2590 0.2353 0.1917 0.2875 11.00%
NDCG@50 0.0377 0.0432 0.0442 0.0476 0.0423 0.0317 0.0543 14.08%
regularization, which can break the homophily assumption, and
thus better capture the signal from negative feedback.
5.4 Performance on Feedback Type Recognition
This section evaluates our DFGNN on the feedback type recognition
task. Given a user and item pair that the user has interacted with,
this task aims to predict whether the user will give positive/negative
feedback. This task focuses more on identifying what users dislike,
as recommending items that users find displeasing can significantly
harm the user experience. We report the experiment results in
Table 2. From this table, we have:
(1) We note that compared to the unsigned graph models, the
signed graph neural networks perform almost entirely better than
them. This may be attributed to the positive/negative feedback pre-
diction is a harder task. In fact, users only interact with an item
when they have a certain level of interest in it. Negative feedback
may be provided later after that. Compared to selecting items that
users like from random negative items (task in recommendation),
this task is more difficult. In this scenario, a lack of precise mod-
eling of negative feedback signals may limit the improvement in
prediction accuracy.
(2) Compared to both signed graph neural networks and un-
signed graph neural networks, our DFGNN still outperforms all of
them except F1-Macro in the Yelp dataset. As analyzed before, posi-
tive/negative recognition is not trivial. Though edges constructed
Basic Basic+LGF Basic+DGF DFGNN73.574.074.575.075.576.076.5AUCArts
Basic Basic+LGF Basic+DGF DFGNN57.558.058.559.059.560.060.561.061.5F1-MacroArts
Basic Basic+LGF Basic+DGF FSGNN72.573.073.574.074.575.075.576.076.5AUC GFood
Basic Basic+LGF Basic+DGF FSGNN5859606162F1-Macro GFoodFigure 6: Ablation study of feedback type recognition task
on Arts and GFood dataset
3443KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yiqing Wu et al.
Table 2: Results on feedback type recognition task. All improvements are significant over baselines (t-test with p <0.05).
Dataset Model GCN GAT SGCN SBGNN SBGCL SIGRec DFGNN Impro
ML1MAUC 0.8802 0.8838 0.8679 0.8877 0.8861 0.8665 0.8925 0.54%
F1-Macro 0.7767 0.7791 0.7658 0.7800 0.7845 0.7411 0.7902 0.73%
ArtsAUC 0.7366 0.7303 0.7337 0.7377 0.7324 0.7294 0.7577 2.71%
F1-Macro 0.5904 0.5579 0.5777 0.6001 0.6035 0.5662 0.6095 0.99%
KindleAUC 0.8591 0.8630 0.8389 0.8324 0.8352 0.8606 0.8691 0.71%
F1-Macro 0.6318 0.6133 0.6500 0.6538 0.6632 0.6250 0.6892 3.92%
GFoodAUC 0.7527 0.7057 0.7250 0.7371 0.7402 0.7484 0.7561 0.45%
F1-Macro 0.6043 0.5610 0.5979 0.6047 0.5751 0.5970 0.6144 1.6%
YelpAUC 0.7705 0.7770 0.7715 0.7572 0.7777 0.7725 0.7907 1.67%
F1-Macro 0.6017 0.6633 0.6841 0.6636 0.6605 0.5860 0.6713 -
Basic Basic+LGF Basic+DGF FSGNN4567891011HIT@10Arts
Basic Basic+LGF Basic+DGF FSGNN1.52.02.53.03.54.04.55.05.5NDCG@10Arts
Basic Basic+LGF Basic+DGF DFGNN5.56.06.57.07.58.0HIT@10GFood
Basic Basic+LGF Basic+DGF DFGNN2.252.502.753.003.253.503.754.004.25NDCG@10GFood
Figure 7: Ablation study of recommendation task on Arts
and GFood dataset
based on co-negative relationships (e.g., ğ‘£1,ğ‘£2are both disliked by
ğ‘¢1) in the SBGNN and SBGCL intuitively can utilize low-passing
filters to capture negative feedback information. They still struggle
to capture high-passing signals between nodes with opposing rela-
tionships (e.g., ğ‘¢1dislikesğ‘£1). With the help of our dual-frequency
graph filters, our model can capture the high-frequency signal in
the negative feedback. Furthermore, the signed graph regulariza-
tion further alleviates the representation degeneration problem and
helps the graph filters work better.
5.5 Ablation Study
In this section, we conduct ablation experiments to evaluate the
effectiveness of the proposed module in our DFGNN. We trans-
form our model into three versions (1) Basic is the basic version ofDFGNN, which only contains the positive edge and low-passing
GNN (2) Basic + LGF is the version that applies low-passing graph
filter on negative edges. (3) Basic + DGF, which adopts our DGF
to encode the signed graph (4) DFGNN is the full version of our
model, which both adopts our SGR and DGF to the signed graph.
We conduct the ablation experiment on the recommendation rank-
ing and feedback type recognition task. And show the results in
Fig. 6 and Fig. 7. From the figures, we can find that the proposed
DGF and SGR can indeed improve the performance of two tasks.
The DGF both captures the high-frequency and low-frequency sig-
nals in two types of feedback, based on this the SGR alleviates the
over-smoothing problem. Besides, We note that the Basic model
generally outperforms the Basic+LGF on the feedback type predic-
tion task. However, in the recommendation task, the opposite is
observed. This is because the negative feedback still reflects usersâ€™
interest (similarity), as they interact with rather than ignore them.
So it may benefit the recommendation task.
5.6 Uniform Analysis
In this paper, based on the observation of the representation degen-
erative problem, we propose SGR to alleviate it. To evaluate our
SGR and further understand how our SGR improves performance:
(1) we visualize the learned node embedding, and (2) we give out
the normalized singular values of the embedding encoded by GNNs
and the average distance between nodes. Specifically, we project
the learned embedding to 2D by SVD for visualization. We use the
average L2 distance of normalized embedding to measure uniform.
The larger the value of distance, the more uniform the learned
embedding is. We run the DFGNN and GCN on the Arts dataset.
The results are shown in Fig. 8 and Fig. 9. By observing the Fig. 8
we can find that the DFGNN learned embedding is uniform to that
of GCN. Secondly, the embedding learned by feedback type predic-
tion task is more narrow. This implies the feedback-type prediction
task is a more difficult task. Furthermore, by observing Fig. 9, We
can see singular values of the embeddings learned by our DFGNN
decrease more slowly. The distances of the learned embeddings
are also larger. This indicates the designed SGR indeed can help
learned embedding to be more uniform. Thus, it can improve the
performance of our model.
3444DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) GCN on FTR task
 (b) DFGNN on FTR task
 (c) GCN on RS task
 (d) DFGNN on RS task
Figure 8: The visualized embedding of GCN and FSGNN on two tasks. FTR means Feedback Type Recognition task, and RS
means Recommendation task
.
0 2 4 6 8
singular values index0.00.20.40.60.81.0Feedback Type Recognition
GCN
DFGNN
(a) GCN on FTR task
0 2 4 6 8
singular values index0.00.20.40.60.81.0Recommendation task
GCN
DFGNN (b) DFGNN on FTR task
FTR RS
T ask0.00.20.40.60.81.01.2UniformComparison of Uniform between GCN and FSGNN
Model
GCN
DFGNN (c) Uniform of GCN and
DFGNN
Figure 9: The Singular values and the uniform (average dis-
tance between all nodes) of GCN and DFGNN on two tasks
.
6 RELATED WORK
GNN and Graph-based Recommendation Graph Neural net-
works(GNN) are studied to model graph data and have achieved
great success. GCNs[ 2,5,9,22,28,30,34,36] firstly extend con-
volution operations to graph data. Further, GraphSage, GAT, GIN
et.al[ 10,32,38] abstract GNN to a message-passing schema and
propose various information aggregation strategies. Recommender
system aims to recommend appropriate items to users by utilizing
user-item interaction information. Recently, recommendation meth-
ods based on deep learning[ 4,11,23,24,31,39] have achieved great
success[ 1,13,17,26]. Borrowing the promising performance, GNNs
are introduced to explore high-order interaction information con-
tained in recommendations[ 12,33,37,40,43,46]. Pinsage[ 40] firstly
applies graph neural network to the recommendation. NGCF[ 33]
extends collaborative filtering to graph. LightGCN [ 12] removes
the non-linear layer between GCN sublayer to alleviate the over-
parameter issues. despite the success of graph-based recommen-
dations, they are merely used to model positive feedback. How
to model negative feedback in graph-based recommendation re-
mains underexplored. In this work we focus on modeling negative
feedback in graph-based recommendation.
Signed graph neural network In the signed graph, there are
two opposite edges (e.g. reject/accept, trustworthy/trustworthy).
Signed graph neural networks are proposed to deal with such graph
structures[ 15,19,21,25,41]. SGCN[ 6] first introduces the GCNto the signed graph by utilizing the balance theory. SBGNN[ 14]
extends the balance theory to the bipartite graph. SBGCL[ 44] modi-
fied the graph contrastive learning to the customized signed graph.
However, most of them do not focus on the recommendation sce-
nario and adopt classical graph neural networks as encoders, which
heavily rely on homophily assumption. In this paper, we find this
assumption may not be invalid for recommendation.
7 CONCLUSIONS
In this paper, we first essentially analyze the negative and positive
feedback from the graph signal frequency perspective. Based on
our experimental observations, we propose a novel Dual-frequency
Graph Neural Network (DFGNN), which models positive/negative
feedback with a designed dual-frequency filter and alleviates repre-
sentation degeneration problem with a signed-graph regularization
loss. Extensive experiments validate the power of our FDGNN.
ACKNOWLEDGE
This research work is supported by the National Key Research and
Development Program of China under Grant No. 2021ZD0113602,
the National Natural Science Foundation of China under Grant
Nos.62206266, 62176014, the Fundamental Research Funds for the
Central Universities and the Young Elite Scientists Sponsorship
Program by CAST 2023QNRC001.
REFERENCES
[1]Jinbin Bai, Chunhui Liu, Feiyue Ni, Haofan Wang, Mengying Hu, Xiaofeng Guo,
and Lele Cheng. 2022. Lat: Latent translation with cycle-consistency for video-
text retrieval. arXiv preprint arXiv:2207.04858 (2022).
[2]Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral net-
works and locally connected networks on graphs. arXiv preprint arXiv:1312.6203
(2013).
[3]Chen Cai and Yusu Wang. 2020. A note on over-smoothing for graph neural
networks. arXiv preprint arXiv:2006.13318 (2020).
[4]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 191â€“198.
[5]MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-
tional neural networks on graphs with fast localized spectral filtering. Advances
in neural information processing systems 29 (2016).
[6]Tyler Derr, Yao Ma, and Jiliang Tang. 2018. Signed graph convolutional networks.
In2018 IEEE International Conference on Data Mining (ICDM). IEEE, 929â€“934.
[7]Pantelis Elinas and Edwin V Bonilla. 2022. Addressing Over-Smoothing in Graph
Neural Networks via Deep Supervision. arXiv preprint arXiv:2202.12508 (2022).
3445KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yiqing Wu et al.
[8]Simon Funk. [n. d.]. Funkâ€™s original post. https://sifter.org/~simon/journal/
20061211.html
[9]Hongyang Gao and Shuiwang Ji. 2019. Graph u-nets. In international conference
on machine learning. PMLR, 2083â€“2092.
[10] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[12] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network
for Recommendation. In Proceedings of the 43rd International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval (Virtual Event, China)
(SIGIR â€™20). Association for Computing Machinery, New York, NY, USA, 639â€“648.
https://doi.org/10.1145/3397271.3401063
[13] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international
conference on world wide web. 173â€“182.
[14] Junjie Huang, Huawei Shen, Qi Cao, Shuchang Tao, and Xueqi Cheng. 2021.
Signed bipartite graph neural networks. In Proceedings of the 30th ACM Interna-
tional Conference on Information & Knowledge Management. 740â€“749.
[15] Junjie Huang, Huawei Shen, Liang Hou, and Xueqi Cheng. 2019. Signed graph
attention networks. In Artificial Neural Networks and Machine Learningâ€“ICANN
2019: Workshop and Special Sessions: 28th International Conference on Artificial Neu-
ral Networks, Munich, Germany, September 17â€“19, 2019, Proceedings 28. Springer,
566â€“577.
[16] Junjie Huang, Ruobing Xie, Qi Cao, Huawei Shen, Shaoliang Zhang, Feng Xia,
and Xueqi Cheng. 2023. Negative can be positive: Signed graph neural networks
for recommendation. Information Processing & Management 60, 4 (2023), 103403.
[17] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using
clickthrough data (CIKM â€™13). Association for Computing Machinery, New York,
NY, USA, 2333â€“2338. https://doi.org/10.1145/2505515.2505665
[18] Olivier Jeunen. 2019. Revisiting offline evaluation for implicit-feedback recom-
mender systems. In Proceedings of the 13th ACM Conference on Recommender
Systems. 596â€“600.
[19] Jinhong Jung, Jaemin Yoo, and U Kang. 2020. Signed graph diffusion network.
arXiv preprint arXiv:2012.14191 (2020).
[20] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197â€“206.
[21] Junghwan Kim, Haekyu Park, Ji-Eun Lee, and U Kang. 2018. Side: representation
learning in signed directed networks. In Proceedings of the 2018 world wide web
conference. 509â€“518.
[22] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2017. ImageNet classi-
fication with deep convolutional neural networks. Commun. ACM 60, 6 (2017),
84â€“90.
[24] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature
521, 7553 (2015), 436â€“444.
[25] Yu Li, Yuan Tian, Jiawei Zhang, and Yi Chang. 2020. Learning signed network
embedding via graph attention. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 34. 4772â€“4779.
[26] Huishi Luo, Fuzhen Zhuang, Ruobing Xie, Hengshu Zhu, Deqing Wang, Zhulin
An, and Yongjun Xu. 2024. A survey on causal inference for recommendation.
The Innovation 5, 2 (2024), 100590.
[27] Changwon Seo, Kyeong-Joong Jeong, Sungsu Lim, and Won-Yong Shin. 2022.
SiReN: Sign-aware recommendation using graph neural networks. IEEE Transac-
tions on Neural Networks and Learning Systems (2022).
[28] Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training enhanced
spatial-temporal graph neural network for multivariate time series forecasting.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 1567â€“1577.
[29] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre
Vandergheynst. 2013. The emerging field of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks and other irregular domains.
IEEE signal processing magazine 30, 3 (2013), 83â€“98.
[30] Xiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Hongyang Chen, Guandong Xu,
and Hongzhi Yin. 2023. Self-supervised hypergraph representation learning
for sociological analysis. IEEE Transactions on Knowledge and Data Engineering
(2023).
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[32] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprintarXiv:1710.10903 (2017).
[33] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In Proceedings of the 42nd international ACM
SIGIR conference on Research and development in Information Retrieval. 165â€“174.
[34] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning. PMLR, 6861â€“6871.
[35] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural
networks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 1â€“37.
[36] Yiqing Wu, Ying Sun, Fuzhen Zhuang, Deqing Wang, Xiangliang Zhang, and
Qing He. 2020. Meta-path hierarchical heterogeneous graph convolution network
for high potential scholar recognition. In 2020 IEEE International Conference on
Data Mining (ICDM). IEEE, 1334â€“1339.
[37] Yiqing Wu, Ruobing Xie, Yongchun Zhu, Xiang Ao, Xin Chen, Xu Zhang, Fuzhen
Zhuang, Leyu Lin, and Qing He. 2022. Multi-view multi-behavior contrastive
learning in recommendation. In International conference on database systems for
advanced applications. Springer, 166â€“182.
[38] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[39] Yongjun Xu, Fei Wang, Zhulin An, Qi Wang, and Zhao Zhang. 2023. Artificial
intelligence for scienceâ€”bridging data to wisdom. The Innovation 4, 6 (2023).
[40] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 974â€“983.
[41] Shuhan Yuan, Xintao Wu, and Yang Xiang. 2017. SNE: signed network embedding.
InAdvances in Knowledge Discovery and Data Mining: 21st Pacific-Asia Conference,
PAKDD 2017, Jeju, South Korea, May 23-26, 2017, Proceedings, Part II 21. Springer,
183â€“195.
[42] Quangui Zhang, Longbing Cao, Chengzhang Zhu, Zhiqiang Li, and Jinguang
Sun. 2018. Coupledcf: Learning explicit and implicit user-item couplings in
recommendation for deep collaborative filtering. In IJCAI International Joint
Conference on Artificial Intelligence.
[43] Yuting Zhang, Yiqing Wu, Ran Le, Yongchun Zhu, Fuzhen Zhuang, Ruidong Han,
Xiang Li, Wei Lin, Zhulin An, and Yongjun Xu. 2023. Modeling dual period-
varying preferences for takeaway recommendation. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5628â€“5638.
[44] Zeyu Zhang, Jiamou Liu, Kaiqi Zhao, Song Yang, Xianda Zheng, and Yifei Wang.
2023. Contrastive learning for signed bipartite graphs. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 1629â€“1638.
[45] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,
Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for
sequential recommendation with mutual information maximization. In CIKM.
[46] Yongchun Zhu, Jingwu Chen, Ling Chen, Yitan Li, Feng Zhang, and Zuotao Liu.
2024. Interest Clock: Time Perception in Real-Time Streaming Recommendation
System. arXiv preprint arXiv:2404.19357 (2024).
A EXPERIMENTAL SETTING
A.1 Dataset
In this paper, to evaluate the performance of the proposed DFGNN,
we conduct comprehensive experiments on classical ML1M, dif-
ferent categories of Amazon Review datasets, and Yelp dataset.
Amazon Review datasets are collected from the Amazon shopping
website. Each instance consists of a userâ€™s rating for an item, ranging
from 1 to 5. we select three categories dataset, they are ArtsCrafts
and Sewing ,Grocery and Gourmet Food, and Kindle Store. For all
datasets, we regard the ratings that are below 3 as negative feed-
back, and the ratings that are higher than 3 as positive feedback.
Following previous works[ 20,45], we discard the users and items
with less than five interactions. For each dataset we randomly select
70% instances as training data, 10% instances as validation data, and
20% instances as testing data. The detailed statistical information
of those datasets is shown in Tab. 3
A.1.1 Implement Details. . For all the models the embedding size
of items and users is set to 64 and the batch size is set to 512.
The GNN layer K is set to 2 in our method. For all methods. We
optimize all models with the Adam Optimizer and carefully search
3446DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Statistics of all datasets.
Dataset # user # item # instance # negative rate
Arts 56123 22847 413554 5.85%
Kindle 139729 98821 2020496 5.68%
GFood 127399 41317 990360 8.74%
Yelp 207453 92422 3139963 19.94%
ML1M 6040 3668 739012 22.16%
for the hyper-parameters of all baselines. To avoid overfitting, we
adopt the early stop strategy with a patience of 20 epochs. For
the recommendation ranking task, we adopt the random negative
sampling strategy with a 1:1 sampling rate. We conduct a grid
search for hyper-parameters. We search for modelsâ€™ learning rates
among{1ğ‘’âˆ’2,3ğ‘’âˆ’3,1ğ‘’âˆ’3,3ğ‘’âˆ’4,1ğ‘’âˆ’4,3ğ‘’âˆ’5}.
B BASELINES
To validate our IDP, we conduct a comprehensive comparison.
Specifically, we compare our method with the following baselines:(1)GCN[ 22], (2)GAT [32], (3)SGCN[ 6] (4)SBGNN [14], (5)SBGCL
[44].
â€¢GCN [22],which is a classical graph convolution network. GCN
adopts the first-order Chebyshev polynomial to construct graph
filters.
â€¢GAT [ 32], which is a classical attention-based GNN. It adopts the
attention mechanism to aggregation neighbor nodes.
â€¢SGCN [ 6], which is a classical signed graph convolution net-
work. It carefully designs a message-passing mechanism based
on balanced theory.
â€¢SBGNN [ 14], which is designed for bipartite signed graph. It
extends balance theory to bipartite graphs by butterfly structure.
â€¢SBGCL [ 44],which is a contrastive based signed graph neural
network. It improved the graph contrastive learning to adapt to
signed graphs.
We implement our method by PyG, which is a popular graph
learning framework. For baseline GCN, GAT, and SGCN we di-
rectly use the implementation of PyG. For baseline SBGNN and
SBGCL, we directly use the authorâ€™s released codes.
3447