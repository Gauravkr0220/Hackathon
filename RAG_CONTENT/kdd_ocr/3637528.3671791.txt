The Heterophilic Snowflake Hypothesis:
Training and Empowering GNNs for Heterophilic Graphs
Kun Wangâœ¾âˆ—
wk520529@mail.ustc.edu.cn
University of Science and Technology
of China (USTC)
Hefei, ChinaGuibin Zhangâœ¾âˆ—
bin2003@tongji.edu.cn
Tongji University
Shanghai, ChinaXinnan Zhang
zhan9359@umn.edu
University of Minnesota
Twin Cities, USA
Junfeng Fang
fjf@mail.ustc.edu.cn
University of Science and Technology
of China (USTC)
Hefei, ChinaXun Wu
wuxun21@mails.tsinghua.edu.cn
Tsinghua University
Beijing, ChinaGuohao Li
guohao.li@kaust.edu.sa
Oxford University
Oxford, UK
Shirui Pan
s.pan@griffith.edu.au
Griffith University
Queensland, AustriliaWei Huangâœ‰
weihuang.uts@gmail.com
RIKEN AIP
Tokyo, JapanYuxuan Liangâœ‰
yuxliang@outlook.com
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, China
ABSTRACT
Graph Neural Networks (GNNs) have become pivotal tools for a
range of graph-based learning tasks. Notably, most current GNN
architectures operate under the assumption of homophily, whether
explicitly or implicitly. While this underlying assumption is fre-
quently adopted, it is not universally applicable, which can result
in potential shortcomings in learning effectiveness. In this paper,
for the first time, we transfer the prevailing concept of â€œone node
one receptive field" to the heterophilic graph. By constructing a
proxy label predictor, we enable each node to possess a latent pre-
diction distribution, which assists connected nodes in determining
whether they should aggregate their associated neighbors. Ulti-
mately, every node can have its own unique aggregation hop and
pattern, much like each snowflake is unique and possesses its own
characteristics. Based on observations, we innovatively introduce
the Heterophily Snowflake Hypothesis and provide an effective
solution to guide and facilitate research on heterophilic graphs
and beyond. We conduct comprehensive experiments including (1)
main results on 10 graphs with varying heterophily ratios across
10 backbones; (2) scalability on various deep GNN backbones (SGC,
JKNet, etc.) across various large number of layers (2,4,6,8,16,32 lay-
ers); (3) comparison with conventional snowflake hypothesis; (4)
efficiency comparison with existing graph pruning algorithms. Our
âˆ—Kun Wang and Guibin Zhang are co-first authors. âœ‰denotes corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $
https://doi.org/10.1145/3637528.3671791observations show that our framework acts as a versatile operator
for diverse tasks. It can be integrated into various GNN frame-
works, boosting performance in-depth and offering an explainable
approach to choosing the optimal network depth. The source code
is available at https://github.com/bingreeky/HeteroSnoH.
CCS CONCEPTS
â€¢Computer systems organization â†’Neural networks; â€¢The-
ory of computation â†’Sparsification and spanners.
KEYWORDS
Graph Neural Network, Heterophilic Graph, Graph Pruning
ACM Reference Format:
Kun Wang âœ¾, Guibin Zhang âœ¾, Xinnan Zhang, Junfeng Fang, Xun Wu, Guo-
hao Li, Shirui Pan, Wei Huang âœ‰, and Yuxuan Liang âœ‰. 2024. The Heterophilic
Snowflake Hypothesis: Training and Empowering GNNs for Heterophilic
Graphs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671791
1 INTRODUCTION
Graph Neural Networks (GNNs) [ 21,26,58] have become the de
facto standard for various graph representation learning tasks, such
as node classification [ 2,45], link prediction [ 63,64], and graph
classification [ 19,62]. The superior capabilities of GNNs can be at-
tributed to their message passing paradigm [ 58]. Through iterative
information aggregation and updating, the central node captures
rich information by interacting with its neighboring nodes based
on the connected graph structure [47, 58, 67].
Among the various landscape of GNN architectures and designs,
the homophily assumption [ 31,66,69] serves as a foundational pil-
lar, suggesting that edges predominantly link nodes with identical
labels and analogous node features. Despite its appealing success,
3164
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kun Wang et al.
Early Stop!Layer 1
Layer 2
Layer 3ğ’©â„‹ğŸ(ğŸ):ğŸ.ğŸ—
Aggr
ğ’©â„‹ğŸ(ğŸ):ğŸ.ğŸ–
Aggr
ğ’©â„‹ğŸ(ğŸ‘):ğŸ.ğŸ•ğŸ
Aggr
ğ’©â„‹ğŸ(ğŸ):ğŸ.ğŸ“
Aggr
ğ’©â„‹ğŸ(ğŸ):ğŸ.ğŸ
Early Stop!ğ’©â„‹ğŸ(ğŸ‘):ğŸ.ğŸ
ğ’©â„‹ğŸ(ğŸ):ğŸ.ğŸ•
Aggr
ğ’©â„‹ğŸ(ğŸ):ğŸ.ğŸ“
Early Stop!ğ’©â„‹ğŸ(ğŸ‘):ğŸ.ğŸAggr
Snowflakes:
Figure 1: The algorithm workflow of Heterophilic Snowflake Hy-
pothesis (Hetero-S) and Heterophily-aware Early Stopping (HES).
the performance of current GNNs has dropped sharply as the ho-
mophily of the graph decreases. Specifically, within heterophilic
graphs, a discrepancy is often observed between the labels of neigh-
boring nodes and the central node, a phenomenon referred to as
the local structure discrepancy issue [ 5,36,66,70]. We ascribe the
observed performance degradation to the uniform message pass-
ing framework, in which that the central node initially aggregates
messages from its local neighboring nodes, subsequently updating
the ego node (see Figure 1 left hand).
However, non-euclidean data frequently display heterophily,
which can be observed across diverse domains. For instance, when
users engage with content on Netflix, the people with diverse pref-
erences might be subjected to similar recommendation algorithms,
owing to their interaction with identical video content. The poten-
tial of heterophilic graphs is vast, holding promise in both academic
and industrial spheres, such as social networks [ 14], transporta-
tion systems [ 51,68], and recommendation platforms [ 56,57]. In a
heterophilic context, this mechanism introduces two primary and
challenging limitations:
â€¢In graph topology, local neighbors refer to nearby nodes, often
overlooking distant yet informative nodes. In heterophilic, nodes
sharing structural and semantic characteristics can be more dis-
tantly positioned [32, 33].
â€¢A consistent aggregation and update method often neglects varia-
tions in information from alike/unalike neighbors. In heterophilic
graphs, achieving discerning node representations necessitates
customized message passing to capture distinctive patterns.
Given the above emerging challenges, there has been a shifting
focus towards exploring heterophily in GNNs. This research area
includes a wide range from delving into heterophilic graph sampling
to the evolution of intricate algorithms. The growing interest in
heterophilic graph learning can be attributed to its vast applicability.
From a macro perspective, existing heterophilic GNNs can be
broadly classified into two categories, i.e.,non-local neighbor ex-
tension andGNN architecture refinement [66]. The concept of
non-local neighbor extension in heterophilic GNNs involves broad-
ening the receptive field beyond local neighbors. This is achieved
through strategies like high-order neighbor information mixing
[3,25,53,70] and potential neighbor discovery [ 17,34,38,61], en-
hancing representation by capturing distant but relevant node fea-
tures. With the second class, GNN architecture refinement focuses
on enhancing the expressive capability of GNNs for heterophilicgraphs by optimizing AGGREGATE and UPDATE function [ 58].
Through strategies such as adaptive message aggregation [ 20,46],
ego-neighbor separation [ 44,70], and layer-wise operation [ 7,9,60],
the refinement aims to produce distinguishable and discriminative
node representations.
Recently, a novel paradigm, the Snowflake Hypothesis (SnoH) [ 49],
rooted in the concept of â€œone node, one receptive fieldâ€ has gained
significant attention for its efficacy in addressing the over-smoothing
[40] and over-fitting [ 5,30] issues in GNNs. This hypothesis draws
inspiration from the intricate and distinctive patterns exhibited
by individual snowflakes, assuming that each node can possess its
unique receptive field. It posits that for an ğ¿-layer GNN, every node
in the graph harbors an optimal receptive field width denoted as
ğ‘˜(1â‰¤ğ‘˜â‰¤ğ¿). During the message passing process from 1toğ¿
hops, each node merely aggregates information from the preceding
ğ‘˜hops, after which they cease to aggregate information from the
neighborhood (referred to as â€œnode early stopping â€).
Intuitively, the snowflake hypothesis demonstrates even greater
vitality and significance in the context of heterophilic graphs: (1)
One concept benefits all. The idea behind the snowflake hypoth-
esis can be seamlessly integrated with any heterophilic GNNs de-
signs, showcasing exceptional versatility. Whether it is for non-local
neighbor extension or GNN architecture refinement, the snowflake
can easily be incorporated as a plugin and demonstrates strong com-
patibility. (2) Enhanced pruning requirement. In heterophilic
graphs, given the higher probability of central nodes having differ-
ent labels than surrounding nodes, the need for pruning aggregation
channels becomes even more critical than in homogeneous graphs.
This pruning aids nodes in selectively aggregating information and
updating themselves effectively.
However, the original SnoH and its implementations appear to ex-
hibit limitations when applied to heterophilic graphs. Firstly, SnoH
relies on either gradient information orcosine similarity comparison
for node early stopping. These heterophily-unaware approaches
do not adequately integrate into heterophilic scenarios. Secondly,
SnoH typically demonstrates convincing performance only in deep
GNNs, which contradicts the prevailing focus on shallow designs
for heterophilic GNNs. These necessitates bespoke strategies for
heterophilic graphs. To handle the distinct characteristics between
homophilic and heterophilic graphs, we introduce a Heterophily-
aware Early Stopping (HES) strategy. HES benefits from two key
aspects, thereby overcoming the limitations of SonH:
â–¶HES employs a proxy label predictor, generating pseudo-label
probability distributions for different nodes [ 11,22]. In this way,
we can scrutinize the probability distributions of two connected
nodes and determine the probabilities where two nodes are pre-
dicted to have the same label. This value can further represent
the homophily score between two nodes, subsequently serving
as a replacement for the original adjacent matrix.
â–¶By analyzing the variation in the heterophilic ratio across each
layer, we are able to appropriately determine the depth for early
stopping. This approach contrasts with SonH, which primarily
exhibits efficacy in deeper GNN configurations.
Subsequently, by determining whether the heterophily of nodes
increases before and after aggregation, we implement early stop-
ping at the node receptive field level, thereby ensuring the efficacy
3165The Heterophilic Snowflake Hypothesis:
Training and Empowering GNNs for Heterophilic Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
of information aggregation. For the first time, we validate the
existence of â€œsnowflakesâ€ in heterophilic graphs, underscoring the
significance of the â€œone node, one receptive field" paradigm in such
scenarios. We introduce a universal solution to this issue, which
stands out from previous designs due to its generality and model-
agnostic nature. Remarkably, HES can seamlessly integrate with
virtually arbitrary heterophilic designs, enhancing both its training
and inference speeds. We summarize our contributions as follows:
â€¢We conceptualize â€œone node one receptive fieldâ€ as heterophilic
snowflake hypothesis in the heterophilic graph scenario. To
achieve this target, we develop the heterophily-aware early stop-
ping strategy, and offer theoretical analysis from the graph neural
tangent kernel (GNTK) and stochastic block model (SBM) per-
spective to provide high-level insights of ours paradigm.
â€¢Hetero-S finds broad applicability and HES can aid various back-
bones in discovering optimal receptive fields for each node across
diverse datasets. We verify HES on 10 GNN backbones across
over 16 graph benchmarks. Experiments demonstrate that for all
prevailing backbones, HES can facilitate substantial performance
enhancements, ranging from 0.34% âˆ¼31.86% in homophilic set-
tings and from 0.68% âˆ¼21.73% in heterophilic settings.
â€¢Similar to the conventional snowflake hypothesis, HES can scale
up to deep GNNs effectively without any bells and whistles. Con-
cretely, HES mitigates the performance degradation caused by
excessive aggregation of heterophilic information, resulting in
performance improvements ranging from 0.46% âˆ¼10.37% in 16-
layer configurations and from 1.13% âˆ¼7.92% in 32-layer configu-
rations. These experimental results demonstrate the potential of
HES to be extended to large and densely connected graphs.
â€¢More observations. (I) Hetero-S has been empirically observed
to exhibit comparable or even superior performance to the origi-
nal snowflake hypothesis on both homophilic and heterophilic
settings. In particular, Hetero-S demonstrates performance im-
provements ranging from 0.51% âˆ¼8.44% on MixHop and JKNet in
comparison to SnoH. (II) The snowflakes ( â„)achieves the high-
est multiply-accumulate operations (MACs) saving (25% âˆ¼45%of
the baseline) compared to SOTA graph pruning algorithms [ 8,35],
without any performance compromise.
2 PRELIMINARIES
2.1 Notations
We consider an attributed graph denoted as G={V,E}, where
VandErepresent the sets of nodes and edges, respectively. The
feature matrix ofGis denoted as XâˆˆRğ‘Ã—ğ·, whereğ‘=|V|
is the number of nodes in the graph and ğ·is the dimension of
node features. We use xğ‘–=X[ğ‘–,Â·]to denote the ğ·-dimensional
feature vector corresponding to node ğ‘£ğ‘–âˆˆV. An adjacency matrix
AâˆˆRğ‘Ã—ğ‘, serves to represent the connections between nodes,
where A[ğ‘–,ğ‘—]=1if(ğ‘£ğ‘–,ğ‘£ğ‘—)âˆˆE and0otherwise. To learn the node
representations in a graph G, most GNNs adhere to the following
paradigm of neighborhood aggregation and message passing:
h(ğ‘™)
ğ‘–=COMB
h(ğ‘™âˆ’1)
ğ‘–,AGGR{h(ğ‘˜âˆ’1)
ğ‘—:ğ‘£ğ‘—âˆˆN(ğ‘£ğ‘–)}
,0â‰¤ğ‘™â‰¤ğ¿(1)
whereğ¿is the number of GNN layers, h(0)
ğ‘–=xğ‘–denotes the feature
vector ofğ‘£ğ‘–andh(ğ‘™)
ğ‘–(1â‰¤ğ‘™â‰¤ğ¿)denotes the node embeddingofğ‘£ğ‘–at theğ‘™-th GNN layer. AGGR andCOMB represent functions
used for aggregating neighborhood information and combining
ego- and neighbor-representations, respectively. In the general
node classification setting, after the graph convolution operations,
GNN uses a linear mapping to map the node embedding h(ğ‘™)
ğ‘–to the
corresponding prediction probability value zğ‘–, and eventually get
the model prediction Ë†ğ‘¦ğ‘–:
zğ‘–=softmax(h(ğ‘™)
ğ‘–W),and Ë†ğ‘¦ğ‘–=arg max{zğ‘–}, (2)
where Wis a learnable matrix, zğ‘–âˆˆRğ¶signifies the probabilities
of categorizing ğ‘£ğ‘–into each of the ğ¶categories.
2.2 Heterophily in Graph Neural Networks
Following the previous heterophilic GNNs [ 38], we define the node-
and graph-level homophily ratio as follows:
node-level:H(ğ‘–)
node=|{ğ‘£ğ‘—|ğ‘£ğ‘—âˆˆN(ğ‘£ğ‘–), ğ‘¦ğ‘–=ğ‘¦ğ‘—}|
|N(ğ‘£ğ‘–)|, (3)
graph-level:Hnode=1
|V|âˆ‘ï¸
ğ‘£ğ‘–âˆˆVH(ğ‘–)
node, (4)
whereN(ğ‘£ğ‘–)denotes the 1-hop neighbor of ğ‘£ğ‘–andV(ğ‘£ğ‘–)denotes
the 1-hop neighbors of ğ‘£ğ‘–. Specifically,H(ğ‘–)
noderepresents the average
proportion of neighbors that share the same class with node ğ‘£ğ‘–, and
Hnode represents the global homophily by computing the average
of node homophily. Conversely, the heterophily ratio at the node
and graph level can be expressed as eH(ğ‘–)
node=1âˆ’H(ğ‘–)
nodeandeHnode=
1âˆ’H node. In general, graphs that exhibit strong homophily tend
to haveHnode values approaching 1, whereas those characterized
by pronounced heterophily often have values near 0.
Itâ€™s essential to note that H(ğ‘–)
nodesolely reflects the heterophily
within the 1-hop neighborhood of ğ‘£ğ‘–. Furthermore, we extend this
definition to the k-hop neighborhood:
NHğ‘˜
ğ‘–=|{ğ‘£ğ‘—|ğ‘£ğ‘—âˆˆN(ğ‘˜)(ğ‘£ğ‘–), ğ‘¦ğ‘–=ğ‘¦ğ‘—}|
|N(ğ‘˜)(ğ‘£ğ‘–)|, (5)
whereN(ğ‘˜)(ğ‘£ğ‘–)={ğ‘£ğ‘—|1â‰¤ShortestPath(ğ‘£ğ‘—,ğ‘£ğ‘–)â‰¤ğ‘˜}represents
the k-hop neighborhood of ğ‘£ğ‘–.
3 MOTIVATION
In this section, we prudently introspect the heterophily in differ-
ent graphs and put forward the motivation of HES strategy. We
start from the empirical observations. Concretely, we select both a
heterophilic graph (Squirrel) and a homophilic graph (CS), and com-
puteNHğ‘˜(1â‰¤ğ‘˜â‰¤6)for each node. As shown in Figure 2 (a,b),
we list the following two observations: Obs.1: The homophily ratio
of nodes in Squirrel significantly declines as the hop increases at a
faster rate compared to those in CS; Obs.2: In both types of graphs,
the distribution of node homophily ratios is diverse. Even when the
receptive field size extends to 6 layers (i.e., 6-hop neighborhood),
there exist nodes with homophily ratios approaching 1.
Insights & Reflections. These observations naturally align with
the concept of â€œone node one receptive field." In both types of graph
data, the k-hop homophily distributions of different nodes exhibit
significant variation. This prompts the question: Is it feasible to
3166KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kun Wang et al.
Figure 2: The algorithm workflow of Heterophilic Snowflake Hy-
pothesis (Hetero-S) and Heterophily-aware Early Stopping (HES).
determine the appropriate receptive field for each node based on its k-
hop homophily distribution? Going beyond this insight, we conduct
theempirical experiments on Squirrel and CS with a 6-layer
GNN. We determine their receptive field as follows:
Rğ‘–=max
1â‰¤ğ‘—â‰¤ğ‘˜(ğ‘—Â· 1[NHğ‘—
ğ‘–â‰¤ğœ–]), (6)
whereRğ‘–denotes the determined receptive filed for ğ‘£ğ‘–andğœ–is
a pre-defined homophily threshold. After obtaining the receptive
field, we then reformulate the aggregation operations as follows:
h(ğ‘™)
ğ‘–=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³COMB
h(ğ‘™âˆ’1)
ğ‘–,AGGR{h(ğ‘˜âˆ’1)
ğ‘—:ğ‘£ğ‘—âˆˆN(ğ‘£ğ‘–)}
,0â‰¤ğ‘™<Rğ‘–;
COMB
h(ğ‘™âˆ’1)
ğ‘–,âˆ…
,Rğ‘–â‰¤ğ‘™â‰¤ğ¿(7)
It is noteworthy that, beyond the Rğ‘–-th layer of GNN, node ğ‘£ğ‘–ceases
to aggregate information from its neighbors, thus achieving early
stopping of the receptive field. Further, Figure 2 (c,d) demonstrates
the performance of GCN on Squirrel and CS with different ho-
mophily thresholds, which characterizes a 2.12%â†‘underğœ–=0.3
on Squirrel, and a 1.34%â†‘underğœ–=0.2on CS. This confirms that
early stopping of node receptive fields can indeed assist GNNs in
learning more refined node representations.
4 METHODOLOGY
Considering the aforementioned observation and gained motiva-
tions, we aim to identify an appropriate receptive field for each
node. Analogous to the conventional snowflake hypothesis, we
introduce the â€œHeterophily Snowflake Hypothesis (Hetero-S)" for
the first time in heterophilic graphs settings:Heterophilic Snowflake Hypothesis (Hetero-S): For anğ¿-layer
GNN on a heterophilic graph, each node possesses an optimal receptive
field; training the GNN by aggregating information only from neigh-
bors within this optimal receptive field minimizes the inclusion of
excessive heterophilic information, yielding optimal representations.
More formally, consider an ğ¿-layer GNN, when optimized with sto-
chastic gradient descent (SGD) on heterophilic graphs G={A,X},
the GNN reaches a minimum validation loss and obtains a test
accuracy of ğœ‘. Furthermore, letâ€™s assume the existence of a prun-
ing algorithm guided by the Hnode, which ensures that each node
ğ‘£ğ‘–has a unique optimal receptive field size of ğ‘˜ğ‘–(1â‰¤ğ‘˜ğ‘–â‰¤ğ¿),
allowing it to be early stopped at the ğ‘˜ğ‘–-th GNN layer, i.e., set
N(ğ‘˜+1)(ğ‘£ğ‘–)=Â·Â·Â·=N(ğ¿)(ğ‘£ğ‘–)=âˆ…. This approach helps nodes
avoid over-aggregation of heterophilic information, leading to a
test accuracy of ğœ‘â€². The Hetero-S posits that there exists optimal ğ‘˜ğ‘–
for eachğ‘£ğ‘–to satisfy that ğœ‘â€²>ğœ‘(note as â„).
To achieve this target, we can naturally resort to the implementa-
tion in Sec. 3. However, in practical scenarios (e.g., semi-supervised
settings), a question arises regarding receptive field operation: when
only partial nodes have known labels, how can we estimate the ho-
mophily ratio for each node within a ğ‘˜-hop neighborhood?
4.1 Proxy Label Predictor
As depicted above, in semi-supervised scenarios, we only know a
small fraction of labels in a heterophilic graph. Towards this end,
we formulate a GNN prediction process as a combination of two
processes, i.e.,ğ‘Œ={AGGRâ‹„COMB}â‹„ğ‘“ğ‘Œ. After the aggregation and
combination process, {AGGRâ‹„COMB}, nodes are mapped into a latent
space characterized by distinguishability. This mapping ensures
that nodes with the same label are positioned in similar locations,
facilitating the identification of their relational patterns. Moreover,
by employing an oracle function, we can effectively predict the
outcomeğ‘Œ, leveraging the structured information encapsulated
in this latent space. This process enhances the modelâ€™s ability to
discern and categorize nodes, significantly improving the accuracy
and efficiency of our predictions. Upon reviewing previous work
[37], we first present a lemma:
Lemma 4.1. Assuming thatNHğ‘˜
ğ‘–forğ‘£ğ‘–decreases w.r.t ğ‘˜in pro-
portion toğœ(ğœ>1), meaning that as the receptive field expands, ğ‘£ğ‘–
aggregates more heterophilic information. Under such circumstances,
when employing receptive field stopping, there exists ğ‘˜â‰¥2satisfy-
ing the condition that ğœ‘({ğ´ğºğºğ‘…â‹„ğ¶ğ‘‚ğ‘€ğµ}(ğ‘˜)â‹„ğ‘“ğ‘Œ)>ğœ‘({ğ´ğºğºğ‘…â‹„
ğ¶ğ‘‚ğ‘€ğµ}(ğ‘˜+ğœ‹)â‹„ğ‘“ğ‘Œ), whereğœ‘(Â·)is the performance metric, {ğ´ğºğºğ‘…â‹„
ğ¶ğ‘‚ğ‘€ğµ}(ğ‘˜)signifies aggregating information solely from the ğ‘˜-hop
neighborhood, and ğœ‹âˆˆN+.
With this in mind, we construct a proxy label predictor Pğ‘Œto
determine label-wise graph aggregation [ 11,22]. Concretely, we
resort to a simple predictive model (here we can use MLP and
GNN, and we place ablation results in Sec. 5.5) to obtain pseudo
probability label Ëœzğ‘–with cross-entropy loss:
min
Î˜LCE(Gğ‘¡ğ‘Ÿ,Î˜)=âˆ’1
|Vğ‘¡ğ‘Ÿ|âˆ‘ï¸
ğ‘£ğ‘–âˆˆVğ‘¡ğ‘ŸËœğ’›ğ‘–log(ğ’›ğ‘–), (8)
whereGğ‘¡ğ‘Ÿdenotes the training nodes ( Vğ‘¡ğ‘Ÿ) and graph structure,
andÎ˜denotes network parameters. Different from previous work
3167The Heterophilic Snowflake Hypothesis:
Training and Empowering GNNs for Heterophilic Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Training Graph(s)ğ’—ğ’Š ğ’—ğ’‹1-hop nodes of ğ’—ğ’Šğ’—ğ’Š 1-hop nodes of ğ’—ğ£ 2-hop nodes of ğ’—ğ£ğ’—ğ£
ğ’—ğ’Š
ğ’—ğ’‹
Proxy label predictorâŠ—âŠ—Outer product
âŠ—ğ‘†ğ‘–1=ğ‘¡ğ‘Ÿ(ğ’«ğ‘–1)ğ’—ğŸ
ğ’—ğ‘µ
ğ‘†ğ‘—ğ‘=ğ‘¡ğ‘Ÿ(ğ’«ğ‘—ğ‘)Homophily mask ğ‘ºâˆˆâ„ğ‘µÃ—ğ‘µğ’—ğ’Š ğ’—ğ’‹0.23
0.380.120.07
0.09
0.090.20
ğ’—ğ’‹ğ‘ºğŸğ‘ºğŸğ‘ºğŸğ‘¨âŠ™ğ‘ºğŸ1-hop
2-hop
3-hop
Heterophily Filtering
ğ‘¨âŠ™ğ‘ºğŸ
ğ‘¨âŠ™ğ‘ºğŸ‘
PruningAdmirable 
accuracyScalable
depth Efficient
inference + +One node 
one receptive
Pruned 
Graphğ’¢ğ‘ 2 ğ’¢ğ‘ 1<
Input 
GraphAccuracyImprovement
Sparsity
Identifying Receptive field 
for each node 
Figure 3: The pipeline of our HES framework. For each node, we utilize a proxy model to evaluate the homophily strength of its edges, which is
further used to estimate its multi-hop homophily ratio. Based on the homophily strength at each hop, we perform receptive field-level early
stopping to determine a unique receptive field for each node.
[11,22,48], we derive a pseudo probability distribution solely to
determine the appropriate size of the receptive field.
4.2 Training Homophily Mask
After obtaining the node soft labels, we proceed to define the ho-
mophily mask denoted as S, where Sğ‘–ğ‘—, for edge(ğ‘–,ğ‘—)âˆˆE , represents
the homophily strength of edge (ğ‘–,ğ‘—),i.e., the likelihood that ğ‘£ğ‘–and
ğ‘£ğ‘—share the same label.
To obtain the expression for Sğ‘–ğ‘—, we calculate the label distribu-
tion for nodes ğ‘£ğ‘–andğ‘£ğ‘—by computing the outer product of their
respective predicted probability distributions.
ğ‘†ğ‘–ğ‘—=ğ‘¡ğ‘Ÿâˆ¼ğ’›ğ‘–âŠ—âˆ¼ğ’›ğ‘—
=ğ‘¡ğ‘ŸÂ©Â­Â­Â­
Â«ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âˆ¼ğ’›1,1Â·Â·Â·âˆ¼ğ’›1,ğ¶
.........
âˆ¼ğ’›ğ¶,1Â·Â·Â·âˆ¼ğ’›ğ¶,ğ¶ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»ÂªÂ®Â®Â®
Â¬=âˆ‘ï¸
ğœ‡=1â†’ğ¶âˆ¼ğ’›ğœ‡,ğœ‡ (9)
where Ëœğ‘§ğ›¼,ğ›½denotes the probability that node ğ‘£ğ‘–belong to cate-
goryğ›¼, while node ğ‘£ğ‘—is associated with category ğ›½.âŠ—represents the
out-product. Then we can formulate the ğ‘†via following equation:
S=Â©Â­Â­
Â«ğ‘†1,1Â·Â·Â·ğ‘†1,ğ‘
.........
ğ‘†ğ‘,1Â·Â·Â·ğ‘†ğ‘,ğ‘ÂªÂ®Â®
Â¬(10)
Note that Sis parameterized by the proxy model Pğ‘Œ. The shapes
ofSandAare identical, in which we can co-optimize the weight Î˜
and the Sfrom end to end by utilizing objective Lopt:
Lopt=LCE({SâŠ™A,X},Î˜)=LCE
optğ‘†
min
Î˜ğ‘¡ğ‘ŸGğ‘¡ğ‘Ÿ(Pğ‘Œ)
âŠ™A,X
; Î˜
(11)
whereLCEdenotes cross-entropy loss, we optimize ( opt)Sby func-
tion optğ‘†
min
Î˜trEGğ‘¡ğ‘Ÿ(Rğ‘Œ)
with minimized empirical risk E.4.3 Early Stopping Based on Hop Heterophily
After multiple rounds of training and optimization with Equation
11, we obtain a relatively accurate mask, denoted as Â¤S. In the context
of GNNs, multi-layer aggregation assists the model in capturing
neighbor relationships at varying distances. Here, we employ Â¤Sto
replace Afor capturing more distant neighbor relationships. Con-
cretely, we useÂ¤S(ğ‘˜)=Â¤Sğ‘˜âˆˆRğ‘Ã—ğ‘to further represent the ğ‘˜-hop
neighbor aggregation expression of homophily ratio. Subsequently,
we calculate the row sum values for each hop, excluding self-loops:
ğ‘ ğ‘’ğ‘¡nâˆ‘ï¸
row\diagÂ¤S(ğ‘˜)o
={Â¤S(ğ‘˜)
1,Â¤S(ğ‘˜)
2...Â¤S(ğ‘˜)
ğ‘}, (12)
whereÂ¤S(ğ‘˜)
ğ‘–represents the normalized row sum of the ğ‘–-th row in
Â¤S(ğ‘˜), where the summation excludes the self-loop represented by
row{ğ‘˜,ğ‘˜}in theğ‘˜-th row of the matrix.
ifÂ¤ğ‘†(ğ‘˜)
ğ‘–>ğœŒÂ¤ğ‘†(ğ¸ğ‘†)
ğ‘–
thenDğ‘œ
N(ğ¸ğ‘†)(ğ‘£ğ‘–)=N(ğ¸ğ‘†+1)(ğ‘£ğ‘–)=Â·Â·Â·=âˆ…
,(13)
whereğœŒdenotes the filtering threshold. While it is possible to as-
sign a distinct threshold for each node, for simplicity, we employ
a uniformğœŒacross all nodes to filter the receptive field (the sen-
sitivity analysis on ğœŒis placed in Appendix G). Dğ‘œdenotes an
intervention that forcefully assigns an aggregation status. We em-
ploy heterophily-aware early stopping at ğ¸ğ‘†-th layer, which can
ensure that each node possesses a unique receptive field size.
4.4 Theoretical Analysis
We provide a theoretical guarantee for heterophily snowflake hy-
pothesis through graph neural tangent kernel (GNTK) [ 12,23].
Generally, during training, the NTK is deterministic and static [ 24],
thus GNTK can be used to describe the training behavior of an
3168KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kun Wang et al.
infinitely-wide graph neural network. In this work, we use GNTK
to study the training dynamics of an infinitely-wide GNN with node
classification task. We provide the definition of GNKT as follows:
ğ¾(ğ‘¢,ğ‘¢â€²)=ğ¿âˆ‘ï¸
ğ‘™=1ğœ•ğ‘“(ğ‘¢)
ğœ•ğœƒ(ğ‘™)ğœ•ğ‘“(ğ‘¢â€²)
ğœ•ğœƒ(ğ‘™)â‰œğ¿âˆ‘ï¸
ğ‘™=1ğ¾(ğ‘¢,ğ‘¢â€²)(ğ‘™)(14)
whereğ‘¢andğ‘¢â€²are indexes of nodes in the graph, and ğœƒ(ğ‘™)is the set
of all trainable parameters in ğ‘™-th layer. According to Equation (14),
we need to calculate the GNTK at each layer, which follows the
recursive relation: K(ğ‘™)=G(ğ‘™)K(ğ‘™âˆ’1), where G(ğ‘™)=Ë†A(ğ‘™)Î“(Ë†A(ğ‘™))
isğ‘™-th layer propagation matrix for the GNTK with Ë†A=Dâˆ’1A,
Î“denotes the transpose operation. Our target is to check if the
smallest eigenvalue of GNTK is greater than zero or not. According
to [22], when the smallest eigenvalue of GNTK is greater than
zero, the training loss can be minimized to zero, implying
successful optimization. Inversely, if the smallest eigenvalue of is
zero, then we would say that the GNN cannot be trained successfully.
To study the eigenvalue of GNTK, we introduce a generative model
named Stochastic Block Model (SBM) [ 1,18], which has been used
in the theoretical analysis of GNNs. Then the following Lemma
gives the smallest eigenvalue of the GNTK:
Lemma 4.2. Consider Ais a probability adjacency matrix sam-
pled from a SBM ğœ“(ğ‘,ğ‘,ğ‘), wherein it is postulated that there are
various clusters interconnected with an internal connection prob-
ability ofğ‘, and the inter-cluster connectivity rate is ğ‘. We can
conclude the expected smallest eigenvalue of the GNTK is given by
EAâˆ¼ğœ“(ğ‘,ğ‘,ğ‘)[ğœ†]=Î ğ¿
ğ‘–=11âˆ’ğ‘
(ğ‘âˆ’1)ğ‘+ğ‘ğ‘+1.
In SBM,ğ‘andğ‘can be deemed as the expectation of the adja-
cency matrix. With this in mind, we observe the K(ğ‘™)=G(ğ‘™)K(ğ‘™âˆ’1)=
Â·Â·Â·=G(ğ‘™)Â·Â·Â·G(1)K(0). Subsequently, we can express the general
probabilistic form of K(ğ‘™):
K(ğ‘™)=Ë†A(ğ‘™)Î“(Ë†A(ğ‘™))Ë†A(ğ‘™âˆ’1)Î“(Ë†A(ğ‘™âˆ’1))Â·Â·Â· Ë†A(1)Î“(Ë†A(1))K(0)(15)
whereğ‘™-th adjacency matrix Ë†A(ğ‘™)is further characterized in proba-
bilistic form, and the density of Ë†A(ğ‘™)is correlated with the magni-
tude ofğ‘(ğ‘™)andğ‘(ğ‘™). We observe that with the increasing depth of
GNN, early stopping of the receptive field can assist in the hierar-
chical decrement of ğ‘andğ‘. Consider a simple binary classification
scenario with a balanced distribution, where each category con-
sists ofğ‘nodes. The eigenvalues of matrix EAâˆ¼ğœ“(ğ‘,ğ‘,ğ‘)[G]are as
follows:
ğœ†0=1,ğœ†1=(ğ‘âˆ’1)ğ‘âˆ’ğ‘ğ‘+1
(ğ‘âˆ’1)ğ‘+ğ‘ğ‘+1,ğœ†2=Â·Â·Â·ğœ†2ğ‘âˆ’1=1âˆ’ğ‘
(ğ‘âˆ’1)ğ‘+ğ‘ğ‘+1
(16)
Generally, the probability of connection between nodes with the
same class label is higher than that of different classes, i.e.,ğ‘>ğ‘.
Hence, the inequality (1âˆ’ğ‘)<(ğ‘âˆ’1)ğ‘âˆ’ğ‘ğ‘+1is always
satisfied. Therefore, the smallest eigenvalue of the SBM is given by
1âˆ’ğ‘
(ğ‘âˆ’1)ğ‘+ğ‘ğ‘+1. Considering in subsequent layers, the HES algorithm
is applied such that ğ‘andğ‘remain attenuation while heterophilic
nodes are pruned (the decay rate of ğ‘is slower), the product of the
smallest eigenvalues is given by:
Î ğ¿
ğ‘–=11âˆ’ğ‘(ğ‘–)
(ğ‘âˆ’1)ğ‘(ğ‘–)+ğ‘ğ‘(ğ‘–)+1=Î ğ¿
ğ‘–=1Â©Â­Â­
Â«1âˆ’ğ‘
ğ‘(ğ‘–)+ğ‘(ğ‘–)
 1âˆ’ğ‘(ğ‘–)+ğ‘ ğ‘(ğ‘–)+ğ‘(ğ‘–)ÂªÂ®Â®
Â¬(17)Considerğ‘>ğ‘and p,q are both functions of ğ‘. Here, we conduct a
case study where both ğ‘andğ‘decay quadratically with ğ‘–. Without
loss of generality, we assume ğ‘=1/((ğ‘âˆ’1)âˆ—ğ‘–2)andğ‘=1/(ğ‘âˆ—ğ‘–2).
We observe that, under these conditions, the GNTK eventually
diverges toâˆš
2ğ‘csch(âˆš
2ğœ‹)sin(ğœ‹/âˆš
ğ‘), which concludes the proof.
See details in Appendix J.
5 EXPERIMENTS
In this section, we conduct extensive experiments to answer the
following research questions (RQ):
â€¢RQ1. Can Hetero-S boost the performance of prevailing ho-
mophilic & heterophilic GNNs on heterophilic graphs?
â€¢RQ2. Does Hetero-S facilitate heterophilic GNNs to extend to
more deep network structures?
â€¢RQ3. Can Hetero-S genuinely achieve graph sparsity and ac-
celerate computations compared to mainstream graph pruning
algorithms [4, 8, 15, 16, 29]?
â€¢RQ4. How sensitive is Hetero-S to its key components?
To provide answers to these questions, we orchestrate the experi-
ments including Main experiments, Depth scalability exper-
iments, Comparative analysis with traditional Snowflake
Hypotheses (SnoH) andEfficiency comparison with pruning
algorithms four parts. Detailed descriptions can be found in Ap-
pendix C. Through these experiments, we anticipate drawing clear
conclusions regarding the efficacy of Hetero-S.
5.1 Experiment Setup
Datasets. We verify Hetero-S across 10 graph benchmarks, includ-
ing citation networks: Cora, CiteSeer, and PubMed [ 26]; WebKB
networks: Cornell, Texas, and Wisconsin [ 38]; Wikipedia-derived
networks: Chameleon and Squirrel [ 42]; the actor co-occurrence
network Actor [ 38]; the heterogenous information network DBLP
[52]. Appendix A offers a comprehensive overview of dataset de-
tails. Note that we choose both highly homophily graphs with
Hnode>0.8and heterophilic graphs with Hnode>0.1.
Backbones. We select three categories of GNN designs, includ-
ing the non-local neighbor extension, GNN architecture refinement
as stated in Section 1, along with some general GNN backbones.
Specifically, for non-local neighbor extension, we choose Mixhop
[3] and GPNN [ 61]. For GNN architecture refinement designs, we
opt for backbones like GAT [ 46], H2GCN [ 70], GCNII [ 7], Geom-
GCN [ 38], JKNet [ 60], and MGNN [ 10]. Lastly, we choose some
general-purpose GNNs, such as GCN [ 26] and GIN [ 59], to further
validate the universality of our algorithm.
5.2 Main Results (RQ1)
We initially investigate the presence and identifiability of the het-
erophily snowflake hypothesis (Hetero-S) through the heterophily-
aware early stopping (HES) mechanism. We evaluate HES in con-
junction with selected GNN backbones across 10 graph benchmarks.
Our tests span not only the standard homophilic datasets but also
extend to heterophilic graphs. From the Table 1 and Figure 4, we
list the following Observations:
Obs.1. The snowflake ( â„) broadly exist under 2âˆ¼8layer back-
bones settings. As shown in Table 1, upon implementing the HES
algorithm, the model consistently achieved performance improve-
ments. For instance, under the MGNN+Cora setup, the model at 8
3169The Heterophilic Snowflake Hypothesis:
Training and Empowering GNNs for Heterophilic Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Qantitative performance for different layers, we report the average results of FIVE runs and record the results after adding Hetero-S
(+â„). â€œ8â„â€ in
 column denotes 8-th layer GNN with HES shows the best performance. OOM represents out-of-memory.
Lay
ers 2 4 6 8
 2
4 6 8
 2
4 6 8
Cora (Hno
de=0.57) Citeseer (Hnode=0.74) PubMed (Hnode=0.80)
GCN/
+â„ 81.60 82.10/82.7383.20/84.0083.90/85.10 8â„ 72.80 73.20/73.40 73.80/73 .40 73.20/74.20 8â„ 87.70 87.90/88.5087.40/88.1287.50/87.984â„
GIN/ +â„ 73.60 78.30/79.7081.10/81.3078.80/82.10 8â„ 60.51 68.47/72.9673.08/73.5072.60/74.50 8â„ 87.60 86.37/88.1888.51/89.3788.45/88.706â„
GAT/ +â„ 82.70 83.52/84.2885.31/85.4283.40/85.19 6â„ 73.78 75.15/75.9874.21/74.30 74.15/73.80 4â„ 87.40 85.12/85.5285.10/85.5084.32/85.08 2
Mixhop/ +â„ 85.20 82.80/84.0782.10/82.8082.16/83.90 2 76.98 75.90/77.2575.50/77.8874.30/77.10 6â„ 77.20 77.30/77.4075.70/76.7073.20/77.284â„
Geom-GCN/ +â„ 85.35 21.02/86.7719.42/81.2414.67/78.99 4â„ 78.42 29.98/67.4325.66/68.4712.04/64.26 2 85.95 73.22/87.0343.66/80.3840.58/74.634â„
H2GCN/ +â„ 77.05 75.74/76.5287.14/87.8385.76/87.98 8â„ 78.02 76.42/78.2575.64/78.1875.46/78.24 8â„ 88.54 87.74/88.6686.02/88.7285.28/88.526â„
GCNII/ +â„ 84.40 79.50/84.8576.30/81.5047.90/73.60 4â„ 74.6073.32/72.89 71.70/72.6654.79/57.99 2 86.50 85.57/87.2584.00/84.9282.53/84.604â„
GPNN/ +â„ 81.20 80.73/82.8043.70/78.9946.49/62.18 4â„ 74.20 73.30/74.8173.80/74.3872.50/73.64 4â„ 88.66 89.60/89.7140.70/88.1940.70/85.104â„
JKNet/ +â„ 83.00 82.51/83.3780.50/83.1981.60/83.39 4â„ 74.19 73.28/74.4272.60/73.8873.10/74.05 4â„ 88.78 88.20/89.4188.78/88.0687.70/88.904â„
MGNN/ +â„ 66.80 63.60/74.7273.81/75.2779.30/84.88 8â„ 43.90 53.30/70.4669.50/78.3377.08/79.30 8â„ 89.20 90.07/90.2088.30/89.4788.27/89.164â„
T
exas (Hnode=0.11) Wisconsin (Hnode=0.21) Cornell (Hnode=0.22)
GCN/
+â„ 68.42 73.68/78.9568.42/71.0560.53/63.16 4â„ 56.86 41.48/50.9847.06/60.7852.94/54.90 6â„ 36.84 34.21/44.7439.47/47.3734.21/39.476â„
GIN/ +â„ 63.16 71.02/76.3273.68/76.3277.84/78.85 8â„ 62.75 45.10/54.0854.88/58.8252.17/62.84 8â„ 34.21 28.95/36.2726.77/38.4236.84/37.206â„
GAT/ +â„ 60.53 60.53/65.7957.89/65.7960.53/63.16 6â„ 52.94 51.79/54.9049.62/56.8650.26/54.33 6â„ 36.84 31.58/39.7626.32/34.2126.38/36.844â„
Mixhop/ +â„ 92.11 89.47/94.8686.44/89.68 78.95/76.32 4â„ 80.39 82.35/84.3174.51/82.9368.63/80.28 4â„ 73.68 52.63/68.9950.80/64.3239.47/65.70 2
Geom-GCN/ +â„ 66.53 60.84/66.9843.17/60.3443.08/58.26 4â„ 64.51 61.88/66.9236.87/54.2236.87/55.92 4â„ 60.54 24.78/48.9924.78/52.9624.78/46.71 2
H2GCN/ +â„ 89.54 68.52/92.0273.43/92.2873.78/89.26 6â„ 76.42 72.75/78.2368.73/76.4172.48/78.62 8â„ 55.7665.45/63.5265.24/63.02 44.62/57.464â„
GCNII/ +â„ 72.68 71.65/73.6865.79/68.7363.16/63.894â„ 45.10 49.02/51.3845.77/49.8642.71/45.104â„ 73.68 52.63/68.4950.00/69.7239.47/66.10 2
GPNN/ +â„ 78.95 60.53/84.2160.53/83.1873.66/78.06 4â„ 66.67 70.59/71.28 72.35/70.16 68.39/76.91 8â„ 50.00 52.83/63.24 50.06/49.66 49.72/52.474â„
JKNet/ +â„ 74.89 78.95/84.2184.07/84.3084.07/89.88 8â„ 47.60 60.38/62.9460.38/64.9956.17/63.82 6â„ 34.21 42.17/47.2543.66/50.0744.89/48.656â„
MGNN/ +â„ 84.21 86.84/92.3188.37/89.6484.21/93.09 8â„ 72.55 84.31/86.2782.35/88.7780.17/86.20 6â„ 55.26 68.18/71.22 65.99/71.08 65.03/70.834â„
Squirr
el(Hnode=0.22) Chameleon (Hnode=0.23) Actor (Hnode=0.22)
GCN/
+â„ 55.83 53.51/56.20 51.01/51.87 49.76/53.74 4â„ 67.1163.82/63.39 59.65/63.38 57.89/60.31 2 29.9826.64/25.67 27.50/28.60 26.83/28.22 2
GIN/ +â„ 46.69 43.26/47.9343.23/46.8939.48/44.86 4â„ 63.82 62.28/66.8960.31/64.0458.11/61.62 4â„ 29.34 29.87/30.83 23.95/25.46 24.41/25.68 4â„
GAT/ +â„ 60.42 46.11/47.5519.31/28.4121.33/26.33 2 69.74 70.61/70.83 66.89/66.23 61.84/63.77 4â„ 28.09 27.57/27.8725.26/25.8625.13/26.58 2
Mixhop/ +â„ 54.76 56.20/56.88 54.84/53.67 52.55/52.87 4â„ 66.45 64.69/69,0863.38/66.0462.67/65.89 4â„ 32.63 31.25/34.8035.72/36.6835.20/35.286â„
Geom-GCN/ +â„ 38.44 36.47/40.3331.42/38.6727.00/36.92 4â„ 60.75 61.42/63.9255.73/59.1254.32/58.46 4â„ 31.59 22.64/32.2622.64/33.4022.64/28.216â„
H2GCN/ +â„ 27.3527.94/27.0430.07/28.21 OOM 6â„ 55.8655.92/53.82 53.07/55.2451.24/56.04 8â„ 33.24 32.66/33.74 33.58/33.35 33.02/33.474â„
GCNII/ +â„ 40.61 37.94/43.99 33.24/30.58 28.43/29.66 4â„ 66.89 54.39/54.6944.36/48.46 46.49/29.77 2 31.84 24.61/24.6724.93/26.8524.47/24.80 2
GPNN/ +â„ 43.04 27.95/38.7418.54/29.4718.54/28.61 2 64.47 50.66/65.5548.03/52.7729.39/45.08 4â„ 24.67 25.00/25.6324.67/24.8822.18/25.094â„
JKNet/ +â„ 47.36 59.75/61.0858.69/61.2257.83/61.48 8â„ 64.25 70.18/72.37 70.83/69.44 70.61/70.93 4â„ 28.09 27.17/29.8727.57/30.0428.09/29.656â„
MGNN/ +â„ 41.79 45.44/48.7641.31/43.7939.00/40.28 4â„ 58.33 64.25/64.7961.18/62.80 61.40/59.72 4â„ 30.53 35.99/37.6535.92/37.23 37.17/37.08 4â„
AccuracyCornell + GCN Cornell + JKNet
Squirrel + GCNEpoch Epoch
Squirrel + JKNetAccuracy
Wisconsin + GCNEpoch
Accuracy
Epoch
 EpochWisconsin + JKNetEpoch
Figure 4: The original baselines and + â„results across Cornell, Squir-
rel and Wisconsin three benchmarks on 8-layer settings.
layers remarkably outperformed the 2-layer baseline by 18.08%. This
phenomenon was also observed across MGNN+Citeseer, JKNet+Texas,H2GCN+Wisconsin, and many others, where performance enhance-
ments ranged from 2.02% âˆ¼14.99% over the original 2-layer configu-
rations. These findings substantiate the validity of our â€œsnowflake
hypothesis" in heterophily graphs.
Obs.2. HES algorithm showcases the great flexibility to var-
ious backbones and consistently presents superior perfor-
mance. The introduction of HES consistently results in perfor-
mance enhancements across nearly all tested models and data com-
binations. Specifically, with GCN and GIN on homophily graphs like
Cora, Citeseer, and PubMed, the models exhibit a âˆ¼1% performance
improvement. This trend is even more pronounced in heterophilic
graphs where, for example, GCN+Texas/Wisconsin shows an av-
erage performance increase of 5.94% across 2âˆ¼8layers. These
consistent improvements across various configurations confirm the
effectiveness of our proposed HES.
Obs.3. Graph-specific and GNN-specific analyses. Take a de-
tailed look and analysis of heterophilic graphs combined with tailor-
made GNN architectures, we observed that HES contributes signifi-
cantly to performance gains. For instance, MGNN on 6 datasets with
Hğ‘›ğ‘œğ‘‘ğ‘’ <0.5, such as Texas and Squirrel, can yield performance
improvements close to 10%. H2GCN+Texas achieves an increase
of approximately 15%âˆ¼20%. Further scrutiny of Figure 4 reveals
that +â„feature significantly bolsters the robustness of our train-
ing process. Moreover, this enhancement facilitates the discovery
of superior dependable subgraphs, particularly within the deeper
layers of the network. We showcase more training visualizations
in Appendix D.
3170KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kun Wang et al.
DBLP
 CS
Actor
 Chameleon
Figure 5: The JKNet and + â„results across CS, DBLP, Actor and
Chameleon four benchmarks on 2, 4, 8, 16, 32-layer settings.
5.3 Extend Hetero-S to Deep GNNs (RQ2)
To provide a scalable solution for large and densely connected
heterophilic graphs, we extend the HES algorithm to deep GNN
contexts. Specifically, we select ResGCN [ 30], JKNet [ 60], GCNII
[7] and SGC [ 54] as backbones and conduct tests on 2 homophilic
and 2 heterophilic graphs, assessing performance at depths of up
to 32 layers. For homophilic graphs, we opt for CS and DBLP [ 43],
whoseHnode is 0.81 and 0.82, respectively. For heterophilic graphs,
we choose Actor (Hnode=0.22) and Chameleon ( Hnode=0.23), We
list the following observations:
Obs.4. Hetero-S consistently boost GNNs at all depths. As
illustrated in Figure 5, the blue line represents the enhanced model
performance with the addition of HES, while the red line depicts
the original baseline. We observed that incorporating the HES al-
gorithm leads to performance gains in both homophilic and het-
erophilic graph contexts. Notably, for the Chameleon dataset, the
integration of the HES algorithm resulted in a performance increase
of nearly 2.9% against the JKNet baseline.
Obs.5. Hetero-S can assist the â€œtop-studentâ€ backbones. An
intriguing observation is that JKNet does not exhibit significant
performance degradation with the deepening of both homophilic
and heterophilic graphs. However, even for the specifically deep-
ened network JKNet, HES still demonstrates exceptional auxiliary
performance, further proving the importance of early stopping in
receptive fields. We have placed additional experimental results in
Appendix E, from which we can draw similar conclusions.
Additionally, the conventional SnoH [ 49] is specially designed
for deepening GNNs on homophilic graphs, and we provide further
comparisons between Hetero-S and SnoH in Appendix F.5.4 Compare With Pruning Methods (RQ3)
In this section, we compare HES with current SOTA pruning meth-
ods, UGS [ 8] and DSpar [ 4]. We attempt to understand whether
Hetero-S can (1) achieve satisfactory sparsity without compromis-
ing performance, and (2) genuinely accelerate GNN computations.
As shown in Table 2 and Figure 6, we can list the observations:
85
7984
83
82
81
80Cora
2007M2918M
Baseline
Hetero-S
UGS
DSpar
2
GCN LayersAccuracy(%)
4 6 8 163552M4866M
7962M56
4454
52
50
48
46Squirrel
12210M
16062M
Baseline
Hetero-S
UGS
DSpar
2
GCN LayersAccuracy(%)
4 6 8 1619606M
22740M
36559M1468M1244M1196M
1666M1579M1405M 1733M
1946M1895M
3052M2801M2412M6230M
10187M
9150M
12357M5748M
9744M7694M
10092M
163078M11027M
Figure 6: Summary of performance (y-axis) at different graph and
GNN layers (x-axis) on Cora and Squirrel. The size of markers rep-
resents the inference MACs ( =1
2FLOPS) of each sparse GCN on
the corresponding sparsified graphs. Black circles ( â€¢) indicate the
baseline. Blue circles ( â€¢) are DSpar. Orange circles ( â€¢) represent the
UGS. Red stars ( â˜…) are established by Hetero-S.
0bs.7. Hetero-S consistently achieves the highest sparsity. As
shown in Table 2, with the GNN layers deepening, both UGS and
DSpar experience a drastic decline in extreme sparsity. In contrast,
Hetero-S demonstrates enhanced sparsity capabilities, surpassing
UGS and DSpar by 23.96% and 19.96% on Squirrel+16-layer GCN.
Obs.8. On both datasets, Hetero-S achieved the smallest in-
ference MACs, nearly only 25%-45% of the original baseline.
Specifically, on the Cora+16-layer GCN, we observe that Hetero-S
can achieve comparable or even better performance than UGS and
DSpar, with only 8.04% and 4.89% MACs, respectively. Furthermore,
our algorithm consistently improves performance across various
GCN depths, especially in deeper layers. Hetero-S surpassed UGS
by approximately 2.3% âˆ¼2.8% on Cora and was able to reliably train
16 layers on Squirrel, outperforming the baseline by nearly 6.2%.
Table 2: The extreme graph sparsity that HES, UGS, and DSpar are
capable of achieving, at which GCN suffers no performance degra-
dation compared with the original baseline.
Dataset Method 2 4 6 8 16
CoraUGS 18.55 14.26 14.26 9.75 N/A
DSpar 23.50 21.00 13.00 10.00 7.50
HES 25.18 28.37 30.75 31.98 34.25
SquirrelUGS 14.26 9.75 5.00 5.00 N/A
DSpar 17.00 12.00 7.00 4.00 4.00
HES 19.11 19.77 20.50 22.89 23.96
5.5 Ablation Study (RQ4)
Since our performance is contingent on the predictive accuracy
of the proxy models, we select multiple proxy models to observe
the impact of different proxy predictors on the final prediction
3171The Heterophilic Snowflake Hypothesis:
Training and Empowering GNNs for Heterophilic Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
results. Concretely, we choose 4-layer GCN and 3-layer GAT, SGC
and MLP as proxy predictors to comprehensively verify the model
performance. As shown in Table 3, we can make observations:
Table 3: Ablation study on different proxy models.
Dataset GCN GAT SGC MLP
Cora 82.89Â±0.92 82.11Â±1.24 82.66Â±0.58 82.73Â±0.63
Citeseer 73.34Â±0.82 72.97Â±1.53 73.79Â±1.04 73.40Â±0.93
Texas 77.41Â±3.52 75.81Â±4.96 77.64Â±2.54 78.95Â±2.87
Squirrel 55.58Â±1.72 55.16Â±1.63 56.17Â±1.20 56.20Â±1.54
Chameleon 62.97Â±0.98 63.96Â±1.22 63.08Â±0.95 63.39Â±0.89
Avg. Rank 2.8 3.4 2.2 1.6
Obs.9. HES Shows limited sensitivity to proxy model selec-
tion. Across all five datasets, the performance variance of snowflakes
obtained using different proxy models ranged narrowly between
0.78%and 1.31%. Specifically, the overall performance ranking is
MLP > SGC > GCN > GAT, so we leverage a 3-layer MLP for the
unified experimental setup in all experiments.
6 RELATED WORK
Our research primarily focuses on the domain of GNNs and is highly
pertinent to two specific areas. Due to space constraints, we have
included the comprehensive related work in the appendix I.
Graph Pooling & Clustering devote to reducing the computa-
tional burden of GNNs by applying pruning or compressing meth-
ods [ 6,8,13,13,19,50], which are highly relevant to our research.
We divide existing techniques into two categories. (1) Sampling-
based methods aims at selecting the most expressive nodes or edges
from the original graph to construct a new subgraph [ 19,28,39,65].
Though efficient, the dropping of nodes/edges sometimes results
in severe information loss and isolated subgraphs, which may crip-
ple the performance of GNNs [ 55]. (2) Clustering-based methods
learns how to cluster the whole nodes in the original graph to pro-
duces a informative small graph [ 41,55,62], which can remedy the
aforementioned information loss problem.
Heterophilic GNNs. Existing heterophilic GNNs primarily fall
into two categories: non-local neighbor extension andGNN archi-
tecture refinement [66]. The former emphasizes expanding the
neighborhood scope, achieved via high-order neighbor information
mixing [ 3,25,53,70] and potential neighbor discovery [ 34,38,61].
The latter, delves into enhancing GNNsâ€™ expressive power specifi-
cally for heterophilic graphs. Strategies include adaptive message
aggregation [ 20,46], ego-neighbor separation [ 44,70], and layer-
wise operations [ 7,9,60] to optimize node representation quality.
Itâ€™s worth emphasizing that our work shares similarities with that
of [48], as both approaches utilize proxy models to discern het-
erogeneity. However, our objective is specifically geared towards
pruning the receptive fields that influence aggregation, granting our
approach greater versatility. Additionally, our method can better
aid in model storage and expedite training.
7 CONCLUSION
In this paper, we first propose the â€œone node one receptive fieldâ€
concept in heterophilic graph modeling. We further establish the
heterophily snowflake hypothesis philosophy for GNNs. To achieve
this, we adopt heterophily-aware early stopping to let certain nodeshave their own receptive fields. In general, we consistently ob-
serve â€œsnowflakes" across numerous deep architectures. Further-
more, upon testing virtually every type of heterogenous design, we
have discovered that our algorithm adeptly integrates with various
frameworks, significantly enhancing their performance.
8 ACKNOWLEDGMENT
Yuxuan Liang is funded by Guangzhou-HKUST (GZ) Joint Funding
Program (No. 2024A03J0620).
REFERENCES
[1]Emmanuel Abbe. 2017. Community detection and stochastic block models: recent
developments. The Journal of Machine Learning Research 18, 1 (2017), 6446â€“6531.
[2]Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. 2020. N-
gcn: Multi-scale graph convolution for semi-supervised node classification. In
uncertainty in artificial intelligence. PMLR, 841â€“851.
[3]Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina
Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:
Higher-order graph convolutional architectures via sparsified neighborhood
mixing. In international conference on machine learning. PMLR, 21â€“29.
[4]Anonymous. 2023. Graph Lottery Ticket Automated. In Submitted to The Twelfth
International Conference on Learning Representations. https://openreview.net/
forum?id=nmBjBZoySX under review.
[5]Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring
and relieving the over-smoothing problem for graph neural networks from the
topological view. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 34. 3438â€“3445.
[6]Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph
convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247
(2018).
[7]Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. 2020.
Simple and deep graph convolutional networks. In International conference on
machine learning. PMLR, 1725â€“1735.
[8]Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang.
2021. A unified lottery ticket hypothesis for graph neural networks. In Interna-
tional Conference on Machine Learning. PMLR, 1695â€“1706.
[9]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2020. Adaptive universal
generalized pagerank graph neural network. arXiv preprint arXiv:2006.07988
(2020).
[10] Guanyu Cui and Zhewei Wei. 2023. MGNN: Graph Neural Networks Inspired by
Distance Geometry Problem. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 335â€“347.
[11] Enyan Dai, Shijie Zhou, Zhimeng Guo, and Suhang Wang. 2022. Label-Wise
Graph Convolutional Network for Heterophilic Graphs. In Learning on Graphs
Conference. PMLR, 26â€“1.
[12] Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong
Wang, and Keyulu Xu. 2019. Graph neural tangent kernel: Fusing graph neural
networks with graph kernels. Advances in neural information processing systems
32 (2019).
[13] Talya Eden, Shweta Jain, Ali Pinar, Dana Ron, and C Seshadhri. 2018. Provable
and practical approximations for the degree distribution using sublinear graph
samples. In Proceedings of the 2018 World Wide Web Conference. 449â€“458.
[14] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference. 417â€“426.
[15] Junfeng Fang, Wei Liu, Yuan Gao, Zemin Liu, An Zhang, Xiang Wang, and
Xiangnan He. 2023. Evaluating Post-hoc Explanations for Graph Neural Networks
via Robustness Analysis. In Thirty-seventh Conference on Neural Information
Processing Systems. https://openreview.net/forum?id=eD534mPhAg
[16] Junfeng Fang, Xiang Wang, An Zhang, Zemin Liu, Xiangnan He, and Tat-Seng
Chua. 2023. Cooperative Explanations of Graph Neural Networks. In WSDM.
ACM, 616â€“624.
[17] Guoji Fu, Peilin Zhao, and Yatao Bian. 2022. ğ‘-Laplacian Based Graph Neural
Networks. In International Conference on Machine Learning. PMLR, 6878â€“6917.
[18] Thorben Funke and Till Becker. 2019. Stochastic block models: A comparison of
variants and inference methods. PloS one 14, 4 (2019), e0215296.
[19] Hongyang Gao and Shuiwang Ji. 2019. Graph u-nets. In international conference
on machine learning. PMLR, 2083â€“2092.
[20] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2018.
Predict then propagate: Graph neural networks meet personalized pagerank.
arXiv preprint arXiv:1810.05997 (2018).
[21] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
3172KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kun Wang et al.
[22] Zijian Hu, Zhengyu Yang, Xuefeng Hu, and Ram Nevatia. 2021. Simple: Similar
pseudo label exploitation for semi-supervised classification. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15099â€“15108.
[23] Wei Huang, Yayong Li, Weitao Du, Jie Yin, Richard Yi Da Xu, Ling Chen, and
Miao Zhang. 2021. Towards deepening graph neural networks: A GNTK-based
optimization perspective. arXiv preprint arXiv:2103.03113 (2021).
[24] Arthur Jacot, Franck Gabriel, and ClÃ©ment Hongler. 2018. Neural tangent ker-
nel: Convergence and generalization in neural networks. Advances in neural
information processing systems 31 (2018).
[25] Di Jin, Zhizhi Yu, Cuiying Huo, Rui Wang, Xiao Wang, Dongxiao He, and Ji-
awei Han. 2021. Universal graph convolutional networks. Advances in Neural
Information Processing Systems 34 (2021), 10654â€“10664.
[26] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[27] Boris Knyazev, Graham W Taylor, and Mohamed Amer. 2019. Understanding
attention and generalization in graph neural networks. Advances in neural
information processing systems 32 (2019).
[28] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention graph pooling.
InInternational conference on machine learning. PMLR, 3734â€“3743.
[29] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018. Snip:
Single-shot network pruning based on connection sensitivity. arXiv preprint
arXiv:1810.02340 (2018).
[30] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph
convolutional networks for semi-supervised learning. In Thirty-Second AAAI
Conference on Artificial Intelligence.
[31] Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and
Weining Qian. 2022. Finding global homophily in graph neural networks when
meeting heterophily. In International Conference on Machine Learning. PMLR,
13242â€“13256.
[32] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar
Bhalerao, and Ser Nam Lim. 2021. Large scale learning on non-homophilous
graphs: New benchmarks and strong simple methods. Advances in Neural Infor-
mation Processing Systems 34 (2021), 20887â€“20902.
[33] Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. 2021. New benchmarks for
learning on non-homophilous graphs. arXiv preprint arXiv:2104.01404 (2021).
[34] Meng Liu, Zhengyang Wang, and Shuiwang Ji. 2021. Non-local graph neural
networks. IEEE transactions on pattern analysis and machine intelligence 44, 12
(2021), 10270â€“10276.
[35] Zirui Liu, Kaixiong Zhou, Zhimeng Jiang, Li Li, Rui Chen, Soo-Hyun Choi, and
Xia Hu. 2023. DSpar: An Embarrassingly Simple Strategy for Efficient GNN
Training and Inference via Degree-Based Sparsification. Transactions on Machine
Learning Research (2023).
[36] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan
Zhang, Xiao-Wen Chang, and Doina Precup. 2022. Revisiting heterophily for
graph neural networks. Advances in neural information processing systems 35
(2022), 1362â€“1375.
[37] Xiaojun Ma, Junshan Wang, Hanyue Chen, and Guojie Song. 2021. Improving
graph neural networks with structural adaptive receptive fields. In Proceedings of
the Web Conference 2021. 2438â€“2447.
[38] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang.
2020. Geom-gcn: Geometric graph convolutional networks. arXiv preprint
arXiv:2002.05287 (2020).
[39] Ekagra Ranjan, Soumya Sanyal, and Partha Talukdar. 2020. Asap: Adaptive struc-
ture aware pooling for learning hierarchical graph representations. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 34. 5470â€“5477.
[40] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. Dropedge:
Towards deep graph convolutional networks on node classification. arXiv preprint
arXiv:1907.10903 (2019).
[41] Kashob Kumar Roy, Amit Roy, AKM Mahbubur Rahman, M Ashraful Amin,
and Amin Ahsan Ali. 2021. Structure-Aware Hierarchical Graph Pooling using
Information Bottleneck. In 2021 International Joint Conference on Neural Networks
(IJCNN). IEEE, 1â€“8.
[42] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. 2021. Multi-scale attributed
node embedding. Journal of Complex Networks 9, 2 (2021), cnab014.
[43] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and
Stephan GÃ¼nnemann. 2019. Pitfalls of Graph Neural Network Evaluation.
arXiv:1811.05868 [cs.LG]
[44] Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. 2021.
Breaking the limit of graph neural networks by improving the assortativity
of graphs with local mixing patterns. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 1541â€“1551.
[45] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. stat1050 (2017), 20.
[46] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[47] Clement Vignac, Andreas Loukas, and Pascal Frossard. 2020. Building pow-
erful and equivariant graph neural networks with structural message-passing.Advances in neural information processing systems 33 (2020), 14143â€“14155.
[48] Junfu Wang, Yuanfang Guo, Liang Yang, and Yunhong Wang. 2023. Heterophily-
Aware Graph Attention Network. arXiv preprint arXiv:2302.03228 (2023).
[49] Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xi-
aojiang Peng, Yuxuan Liang, and Yang Wang. 2023. The Snowflake Hypoth-
esis: Training Deep GNN with One Node One Receptive field. arXiv preprint
arXiv:2308.10051 (2023).
[50] Kun Wang, Yuxuan Liang, Xinglin Li, Guohao Li, Bernard Ghanem, Roger Zim-
mermann, Huahui Yi, Yudong Zhang, Yang Wang, et al .2023. Brave the Wind
and the Waves: Discovering Robust and Generalizable Graph Lottery Tickets.
IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).
[51] Kun Wang, Zhengyang Zhou, Xu Wang, Pengkun Wang, Qi Fang, and Yang
Wang. 2022. A2DJP: A two graph-based component fused learning framework
for urban anomaly distribution and duration joint-prediction. IEEE Transactions
on Knowledge and Data Engineering (2022).
[52] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu.
2019. Heterogeneous graph attention network. In The world wide web conference.
2022â€“2032.
[53] Yu Wang and Tyler Derr. 2021. Tree decomposed graph neural network. In
Proceedings of the 30th ACM international conference on information & knowledge
management. 2040â€“2049.
[54] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning. PMLR, 6861â€“6871.
[55] Junran Wu, Xueyuan Chen, Ke Xu, and Shangzhe Li. 2022. Structural Entropy
Guided Graph Hierarchical Pooling. In International Conference on Machine Learn-
ing. PMLR, 24017â€“24030.
[56] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural
networks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 1â€“37.
[57] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.
Session-based recommendation with graph neural networks. In Proceedings of
the AAAI conference on artificial intelligence, Vol. 33. 346â€“353.
[58] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4â€“24.
[59] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[60] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In International conference on machine learn-
ing. PMLR, 5453â€“5462.
[61] Tianmeng Yang, Yujing Wang, Zhihan Yue, Yaming Yang, Yunhai Tong, and Jing
Bai. 2022. Graph pointer neural networks. In Proceedings of the AAAI conference
on artificial intelligence, Vol. 36. 8832â€“8839.
[62] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure
Leskovec. 2018. Hierarchical graph representation learning with differentiable
pooling. Advances in neural information processing systems 31 (2018).
[63] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural
networks. Advances in neural information processing systems 31 (2018).
[64] Muhan Zhang and Yixin Chen. 2019. Inductive matrix completion based on graph
neural networks. arXiv preprint arXiv:1904.12058 (2019).
[65] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Zhao Li, Chengwei Yao,
Dai Huifen, Zhi Yu, and Can Wang. 2021. Hierarchical multi-view graph pooling
with structure learning. IEEE Transactions on Knowledge and Data Engineering
(2021).
[66] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. 2022.
Graph neural networks for graphs with heterophily: A survey. arXiv preprint
arXiv:2202.07082 (2022).
[67] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI open 1 (2020), 57â€“81.
[68] Zhengyang Zhou, Gengyu Lin, Kuo Yang, LEI BAI, Yang Wang, et al .2022. GReTo:
Remedying dynamic graph topology-task discordance via target homophily. In
The Eleventh International Conference on Learning Representations.
[69] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed,
and Danai Koutra. 2021. Graph neural networks with heterophily. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 35. 11168â€“11176.
[70] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
Koutra. 2020. Beyond homophily in graph neural networks: Current limitations
and effective designs. Advances in neural information processing systems 33 (2020),
7793â€“7804.
3173The Heterophilic Snowflake Hypothesis:
Training and Empowering GNNs for Heterophilic Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A DATASETS AND BACKBONES
DESCRIPTIONS.
A detailed description of the datasets and the backbones to enhance
the understanding of our experimental design is placed in the Arxiv
version.
B ALGORITHM TABLE
Algorithm 1: Algorithm workflow of HES
Input :G=(A,X), GNN model ğ‘“Î˜, Proxy modelPğ‘Œ,
Epoch number ğ‘„, GNN layer count ğ¿
1foriteration i in{1,2,Â·Â·Â·,ğ‘„}do
2 Forward proxy model and compute ËœZâ†Pğ‘Œ(A,X)
/* Obtain Homophily mask S */
3 foredge (i,j) inEdo
4ğ‘†ğ‘–ğ‘—â†trace(Ëœğ‘§ğ‘–âŠ—Ëœğ‘§ğ‘—)
5 Computeğ‘˜-hop homophily mask Â¤S(ğ‘˜)=Â¤Sğ‘˜âˆˆRğ‘Ã—ğ‘
6 Compute the row sum of homophily masks
ğ‘ ğ‘’ğ‘¡{Ã
row\diagÂ¤S(ğ‘˜)}={Â¤S(ğ‘˜)
1,Â¤S(ğ‘˜)
2...Â¤S(ğ‘˜)
ğ‘}
7 forlayer l in{1,2,Â·Â·Â·,ğ¿}do
/* Note that for presentation clarity, we compute
embeddings for each node individually here. */
8 fornodeğ‘£ğ‘–inVdo
9 ifÂ¤ğ‘†(ğ‘™)
ğ‘–â‰¤ğœŒÂ¤ğ‘†(1)
ğ‘–then
/* Before receptive early stopping */
10 h(ğ‘™)
ğ‘–â†
COMB
h(ğ‘™âˆ’1)
ğ‘–,AGGR{h(ğ‘™âˆ’1)
ğ‘—:ğ‘£ğ‘—âˆˆN(ğ‘£ğ‘–)}
11 else
/* After receptive early stopping */
12 h(ğ‘™)
ğ‘—â†COMB
h(ğ‘™âˆ’1)
ğ‘–,âˆ…
13 Compute loss function Eq. 8 and Eq. 11
14 Backward to update GNN ğ‘“Î˜and proxy modelPğ‘Œ
C EXPERIMENTAL SETTINGS
In this section, we report our experimental settings according to
the research questions.
â€¢Main experiments (RQ1). In this setup, we integrate Hetero-S
into mainstream heterophilic GNNs, focusing on non-local neigh-
bor extensions (2 backbones), GNN architecture refinements (6
backbones) and general designs (2 backbones).
â€¢Depth scalability experiments (RQ2). We delve into varying
depths of GNN architectures. The aim is to determine whether the
inclusion of Hetero-S enables these GNNs to maintain or enhance
performance as the network goes deeper, avoiding issues like
vanishing gradients or over-smoothing.
â€¢Comparative analysis with traditional Snowflake Hypothe-
ses (RQ3). Here, we juxtapose Hetero-S with its predecessors,
SnoHv1 and SnoHv2, on heterophilic graphs. The experiment
is tailored to elucidate if Hetero-S presents a more harmonious
alignment with the intricacies of heterophily, potentially leading
to better model interpretations and results.â€¢Efficiency comparison with pruning algorithms (RQ4). We
compare Hetero-S with current SOTA graph sparsification meth-
ods (e.g., UGS [ 8], SNIP [ 29], DSpar [ 35]) with a focus on two key
aspects: (1) whether Hetero-S can achieve the desired sparsity
without performance compromise, and (2) whether Hetero-S can
genuinely accelerate model computations.
D ADDITIONAL RESULTS TO ANSWER RQ1.
In this section, we present additional experiments to answer RQ1.
We have included new experimental results for the DBLP dataset
and provided further training details for selected datasets. Due to
the page limit, we place the detailed appendix in this link.
E ADDITIONAL RESULTS TO SNSWER RQ2.
In this section, we showcase the additional experimental results on
DBLP, CS, Chameleon and Actor four graph bechmark. As shown
in Figure 7, Incorporating the HES algorithm consistently enhances
performance across GCN, GCNII, and SGC models. For example,
the GCN+DBLP model reveals a significant performance peak at a
depth of 8 layers, indicating an optimal balance between model com-
plexity and learning capability. The GCN+CS model consistently
performs well, particularly at lower layer counts, suggesting that
the HES algorithm captures the essential representational features
efficiently, even without deeper network architectures. Moreover,
the GCNII variants benefit more markedly from the integration with
the HES algorithm, especially evident in the Actor and Chameleon
benchmarks. The enhanced performance indicates that the HES
algorithmâ€™s approach to leveraging heterogeneity in data aligns
exceptionally well with the sophisticated architectures of GCNII.
Furthermore, the SGC models also show improvements in perfor-
mance, indicating the HES algorithmâ€™s robustness across different
levels of model complexity. This reaffirms its utility as a versatile
enhancer of graph network effectiveness. Notably, the performance
boosts are not solely dependent on depth but also show a trend of
incremental gains with increasing complexity, up to a point where
performance begins to plateau or slightly decline, highlighting the
HES algorithmâ€™s subtle influence on model.
F PERFORMANCE COMPARISON WITH
CONVENTIONAL SNOH
In this section, we focus on a systematic examination of the perfor-
mance merits and limitations of HES, in contrast to conventional
â€œsnowflakeâ€ settings, and the results are available here.
G PARAMETER SENSITIVITY ANALYSIS
In this section, we detail how we determine the filtering threshold
ğœŒin all experiments and how ğœŒinfluences the performance of HES.
In practice, we want to avoid both excessively large ğœŒ, as it could
lead to premature removal of too large a receptive field during
early stopping, resulting in suboptimal model performance, and
excessively small ğœŒ, as they could cause central nodes to absorb too
much heterophilic information from multi-hop neighbors. There-
fore, we search the most suitable ğœŒin a limited range {1e-2, 1e-4,
1e-6, 1e-8, 1e-16} for all experiments. To further demonstrate how
sensitive our method is to ğœŒ, we test the performance of HES on
Squirrel/Chameleon with different filtering threshold settings. As
3174KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kun Wang et al.
GCN+DBLP
 GCN+CS
GCN+Chameleon
 GCN+Actor
GCNII+DBLP
 GCNII+CS
GCNII+Cha
meleon
GCNII+Actor
SGC+DBLP
 SGC+CS
SGC+Chame
leon
SGC+Actor
Figure 7: The GCN, GCNII, SGC and +â„ results across CS, DBLP, Actor and Chameleon four benchmarks on 2, 4, 8, 16, 32-layer settings.
shown in Table 4, we can observe (1) the optimal performance of
HES requires an appropriate choice of ğœŒ. Generally, as ğœŒincreases,
HESâ€™s performance often initially improves (corresponding to in-
creased removal of receptive fields), then declines (corresponding
to excessive removal of receptive fields); (2) Overall, HES is rela-
tively insensitive to the choice of ğœŒ. For instance, on GCN, HES
performance varies by no more than 1.04% and 1.34%.
Table 4: Parameter sensitivity on filtering threshold ğœŒ. We report
the performance with HES under 4-layer settings with different ğœŒ.
Squirrel 1e-2 1e-4 1e-6 1e-8 1e-16
GCN+â„ 55.18 56.20 55.52 55.78 54.74
MixHop+â„ 55.42 56.88 55.46 55.20 54.82
JKNet+â„ 59.77 60.11 60.08 60.02 58.55
MGNN+â„ 45.65 46.81 47.85 47.23 47.56
Chameleon 1e-2 1e-4 1e-6 1e-8 1e-16
GCN+â„ 62.25 63.11 63.38 63.23 62.34
MixHop+â„ 67.74 69.04 68.78 69.08 67.22
JKNet+â„ 70.17 70.38 72.37 71.69 70.55
MGNN+â„ 60.19 62.09 61.76 61.92 60.83
H CASE STUDY
In this section, we endeavor to investigate the efficacy of Hetero-S
from a micro-perspective through the analysis of two case studies.
We choose the superpixel graphs of MNIST [ 27] and Squirrel as
benchmarks to observe the visualized outcomes. The detailed results
are in this link.
I RELATED WORK
More discussions on related work can also be found here.
J PROOF
In this subsection, we present a detailed proof. The notation
EAâˆ¼ğ‘†ğµğ‘€(ğ‘,ğ‘)[Â·]represents the expected pattern of the adjacency
matrix. As depicted in main part, considering in subsequent layers,
the HES algorithm is applied such that ğ‘remains constant whileheterophilic nodes are pruned, thus reducing ğ‘, the product of the
smallest eigenvalues of EAâˆ¼ğœ“(ğ‘,ğ‘,ğ‘)[G]is given by:
Î ğ¿
ğ‘–=11âˆ’ğ‘(ğ‘–)
(ğ‘âˆ’1)ğ‘(ğ‘–)+ğ‘ğ‘(ğ‘–)+1=Î ğ¿
ğ‘–=1Â©Â­Â­
Â«1âˆ’ğ‘
ğ‘(ğ‘–)+ğ‘(ğ‘–)
 1âˆ’ğ‘(ğ‘–)+ğ‘ ğ‘(ğ‘–)+ğ‘(ğ‘–)ÂªÂ®Â®
Â¬
(18)
Consider the case ğ‘>ğ‘are both functions of ğ‘,ğ‘–, and suppose
ğ‘=1/((ğ‘âˆ’1)âˆ—ğ‘–2),ğ‘=1/(ğ‘âˆ—ğ‘–2). In this scenario, as the layer depth
increases, both ğ‘andğ‘undergo decay, yet maintain the condition
ğ‘>ğ‘. Consequently, the product of the smallest eigenvalues can
be expressed as:
Î¨(ğ¿)=Î ğ¿
ğ‘–=1(ğ‘âˆ’1)ğ‘–2âˆ’1
(ğ‘âˆ’1)ğ‘–2+2(ğ‘âˆ’1)(19)
While the network goes infinitely deep, a.k.a,ğ¿â†’âˆ , the infinite
product of the smallest eigenvalues can be calculated as follows:
Î¨(ğ¿)=Î ğ¿
ğ‘–=1(ğ‘âˆ’1)ğ‘–2âˆ’1
(ğ‘âˆ’1)(ğ‘–2+2)
=âˆš
2ğœ‹csch(âˆš
2ğœ‹)(1âˆ’1âˆš
ğ‘âˆ’1)ğ¿(1+1âˆš
ğ‘âˆ’1)ğ¿1
Î“(ğ¿âˆ’ğ‘–âˆš
2+1)Î“(ğ¿+ğ‘–âˆš
2+1)
â‰ˆâˆš
2ğ‘csch(âˆš
2ğœ‹)sin(ğœ‹/âˆš
ğ‘)
(20)
where csch(Â·)denotes the hyperbolic cosecant function, Î“(Â·)de-
notes the Gamma function and (ğ‘)ğ¿=ğ‘(ğ‘+1)...(ğ‘+ğ¿âˆ’1)denotes
the Pochhammer Symbol. When the network goes infinitely deep,
the value of the infinite product asymptotically approaches a non-
zero valueâˆš
2ğ‘csch(âˆš
2ğœ‹)sin(ğœ‹/âˆš
ğ‘), which concludes the proof.
We posit that this phenomenon is not solely confined to bi-
nary classification contexts. Even in multi-class scenarios, a similar
pattern is observed. Utilizing the HES algorithm, we can adeptly fa-
cilitate the divergence of the GNTK, as opposed to its convergence
to zero, thereby enhancing the efficacy of network training.
3175