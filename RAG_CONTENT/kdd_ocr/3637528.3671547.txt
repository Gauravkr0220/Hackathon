Time-Aware Attention-Based Transformer (TAAT)
for Cloud Computing System Failure Prediction
Lingfei Deng‚àó
denglingfei.dlf@alibaba-inc.com
Alibaba Cloud, Alibaba Group
Hangzhou, ChinaYunong Wang‚àó
yunong.wyn@alibaba-inc.com
Alibaba Cloud, Alibaba Group
Hangzhou, ChinaHaoran Wang
hrwang21@hust.edu.cn
Huazhong University of Science and
Technology
Wuhan, China
Xuhua Ma
xuhua.mxh@alibaba-inc.com
Alibaba Cloud, Alibaba Group
Hangzhou, ChinaXiaoming Du
duxiaoming.dxm@alibaba-inc.com
Alibaba Cloud, Alibaba Group
Hangzhou, ChinaXudong Zheng
xudong.zxd@alibaba-inc.com
Alibaba Cloud, Alibaba Group
Hangzhou, China
Dongrui Wu‚Ä†
drwu@hust.edu.cn
Huazhong University of Science and
Technology
Wuhan, China
ABSTRACT
Log-based failure prediction helps identify and mitigate system
failures ahead of time, increasing the reliability of cloud elastic
computing systems. However, most existing log-based failure pre-
diction approaches only focus on semantic information, and do
not make full use of the information contained in the timestamps
of log messages. This paper proposes time-aware attention-based
transformer (TAAT), a failure prediction approach that extracts se-
mantic and temporal information simultaneously from log messages
and their timestamps. TAAT first tokenizes raw log messages into
specific exceptions, and then performs: 1) exception sequence em-
bedding that reorganizes the exceptions of each node as an ordered
sequence and converts them to vectors; 2) time relation estimation
that computes time relation matrices from the timestamps; and, 3)
time-aware attention that computes semantic correlation matrices
from the exception sequences and then combines them with time
relation matrices. Experiments on Alibaba Cloud demonstrated that
TAAT achieves an approximately 10% performance improvement
compared with the state-of-the-art approaches. TAAT is now used
in the daily operation of Alibaba Cloud. Moreover, this paper also
releases the real-world cloud computing failure prediction dataset
used in our study, which consists of about 2.7 billion syslogs from
about 300,000 node controllers during a 4-month period. To our
‚àóBoth authors contributed equally to this research.
‚Ä†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671547knowledge, this is the largest dataset of its kind, and is expected to
be very useful to the community.
CCS CONCEPTS
‚Ä¢Computer systems organization ‚ÜíReliability ;‚Ä¢Hardware
‚ÜíFailure prediction .
KEYWORDS
Log-based failure prediction; Cloud elastic compute service; Trans-
former; Time-aware attention
ACM Reference Format:
Lingfei Deng, Yunong Wang, Haoran Wang, Xuhua Ma, Xiaoming Du,
Xudong Zheng, and Dongrui Wu. 2024. Time-Aware Attention-Based Trans-
former (TAAT) for Cloud Computing System Failure Prediction . In Proceed-
ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671547
1 INTRODUCTION
With the wide-spread applications of cloud computing, its stability
and reliability have received increasing attention [ 1‚Äì4]. A cloud
computing service system consists of many virtual machines (VMs)
running on physical servers. The servers would fail occasionally
and therefore impact the availability of the VMs. The ability of
predicting server failures in advance and migrating VMs actively is
of critical importance to the system reliability. The Alibaba Cloud
Elastic Compute Service (ECS) uses some lightweight data collec-
tion tools to monitor system alerts and logs. The events in these logs
(called exceptions) provide important information for server failure
prediction [ 5]. This paper focuses on log-based failure prediction
for cloud ECS.
Previous studies have shown that server failures can be predicted
from time series of historical exceptions [ 6‚Äì9]. Therefore, we orga-
nize exceptions that occurred on each server in chronological order
4906
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Lingfei Deng, et al.
and predict each server‚Äôs failure risk from them. However, differ
from uniformly sampled time series, timestamps of the exceptions
do not have a uniform distribution. The time interval lengths be-
tween successive exceptions often reflect the urgency and severity
of the anomalies. For instance, a server with 1,000 ‚Äúmachine check
exceptions‚Äù in three days may not fail, but a server with 1,000 such
exceptions in five minutes tends to fail. Therefore, effective failure
prediction must adequately make use of the exception timestamp
information.
Existing log-based failure prediction approaches can be cate-
gorized into two groups, traditional machine learning and deep
learning. The former [ 6,10‚Äì13] extracts features according to ex-
pert knowledge or statistical analysis, and then learns a classifier
to determine whether a server is abnormal. For instance, Baseman
et al. [6] utilized error locations and frequencies to predict DRAM
failures, and Farzaneh et al. [13] used the number of reallocated
sectors and read errors for storage system sector failure prediction.
However, those approaches heavily rely on expert experience. Gath-
ering expert knowledge on failures of various hardware systems
is difficult and costly, making it challenging to scale to large cloud
ECS.
Deep learning approaches, e.g., Long-Short Term Memory (LSTM)
[14] and Bidirectional Encoder Representations from Transformers
(BERT) [ 15], have demonstrated their abilities to extract informa-
tive patterns automatically in handling sequential data. They have
been used in server failure prediction or detection [ 7‚Äì9]; however,
they are not tailored for time-dependent exception sequences, i.e.,
they did not make full use of the log timestamps.
This paper proposes time-aware attention-based transformer
(TAAT), a universal failure prediction model capable of handling
time-dependent sequences for cloud computing systems. Specifi-
cally, TAAT utilizes the encoder of BERT [ 15] as the base feature
extractor, which can dynamically adjust model weights accord-
ing to different inputs to extract various exception patterns [ 16].
TAAT then computes the correlations between timestamps as sup-
plementary weights in the attention mechanism of BERT to inte-
grate temporal and semantic information. Concisely, TAAT has two
advantages: 1) compared with traditional approaches, it is more
generic and expert knowledge-independent, due to its ability to
automatically extract various exception patterns; and 2) compared
with state-of-the-art approach, BERT, it incorporates temporal in-
formation to extract more comprehensive features. Experiments on
Alibaba Cloud demonstrated that TAAT achieved ‚àº10% prediction
performance improvement over BERT.
In addition to algorithms, cloud computing system failure pre-
diction research also lacks high-quality public server log datasets.
Datasets derived from the Self-Monitoring, Analysis and Reporting
Technology (SMART) system [ 17,18] are widely studied, but they
only consider disk failures. Loghub1[19] provides various system
log datasets, but some lack timestamps2, some only include a cer-
tain type of servers [ 20], and some only have a small amount of
data3. To fill this gap, we release a large dataset derived from the
production system of Alibaba Cloud, consisting of ‚àº2.7 billion logs
from‚àº300,000 servers during a 4-month period. This dataset is
1https://github.com/logpai/loghub
2https://doi.org/10.5281/zenodo.1144100
3https://log-sharing.dreamhosters.comalso valuable for general failure prediction or anomaly detection
research, beyond clouding computing systems.
In summary, our main contributions are:
(1)We propose TAAT, a universal approach for cloud computing
server failure prediction, which integrates both semantic and
temporal information from the exception sequences and their
timestamps in the attention mechanism.
(2)We demonstrated the effectiveness of TAAT in the real pro-
duction system of Alibaba Cloud. Compared with BERT,
TAAT had‚àº10% prediction performance improvement.
(3)We release a large cloud computing syslog dataset on real-
world data to benefit the community.
2 BACKGROUND AND MOTIVATION
To clearly explain the problem under investigation, its industry im-
plication and our model design motivation, this section first briefly
introduces Alibaba Cloud operation and maintenance system, and
then presents the challenges and intuitions of our work.
2.1 Alibaba Cloud Operation and Maintenance
System Overview
Alibaba Cloud ECS includes more than one million physical servers,
each hosting multiple VMs. Usually, a cloud ECS uses virtualization
to maintain a scalable and reliable runtime environment. However,
hardware or software issues (e.g., CPU/memory/disk failures, web
errors, software bugs, etc) may cause the failure (unresponsiveness)
of a server, resulting in damage to customers‚Äô business running on
the VMs. Therefore, an operation and maintenance system which
can predict the server status is essential.
Figure 1 shows the workflow of Alibaba Cloud operation and
maintenance system. There are three modules:
(1)Monitor module , which tracks the real-time status/performance
of each server and records certain abnormal events as excep-
tions.
(2)Prediction module , which performs offline updates and online
inferences. It is retrained offline quarterly using prior 3-
month‚Äôs exceptions. The updated model is deployed as a
centralized service for millions of servers to perform real-
time online inferences.
(3)Migration module . Once a server is predicted to fail, the mi-
gration module performs live migration of its VMs promptly
to ensure their availability, i.e., moving its running VMs to
other healthy servers without disconnecting the clients or
applications to minimize the impact on running services.
This paper focuses on the prediction module.
2.2 Motivation
According to our expert knowledge and experience, different ex-
ception patterns may lead to different failure types (CPU, memory,
disk, software, etc.):
(1)Individual exceptions. The occurrence of certain exceptions
indicates high failure risk. For instance, exceptions such
as‚Äúhost machine‚Äôs unreclaimable slab memory exceeds the
allocated quota‚Äù and‚Äúnetwork card cyclic redundancy check
4907Time-Aware Attention-Based Transformer (TAAT)
for Cloud Computing System Failure Prediction
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Figure 1: Workflow of Alibaba Cloud operation and mainte-
nance system.
error‚Äù indicate substantial server failure risks, though there
are no obvious relationships between them.
(2)Sequential exceptions. The occurrence of one exception re-
sults in others, i.e., the abnormalities are sequentially related.
For example, ‚Äúmemory hardware error‚Äù may lead to error
storms, e.g., ‚ÄúCMCI storm detected‚Äù, and chronic accumula-
tion of these errors may eventually cause server failure.
(3)Time intervals. The frequency change of certain exceptions,
or multiple types of exceptions happening within a short
period of time, may also result in failures. For instance, Figure
2 shows the number of ‚Äúmachine check exceptions‚Äù occurring
within 3 days for two servers. Intuitively, the failed server
had a higher anomaly frequency.
In summary, the types, orders and time intervals between excep-
tions are all important patterns. Therefore, we call our problem
time-dependent exception sequence classification.
0 10 20 30 40 50 60 70
Distance to failure time (hours)0100200300400Number of exception occurrenceNormal
Failure
Figure 2: Timestamp distribution of normal and failure
servers.
2.3 New Insights of TAAT
Expert knowledge is not available for all failure types and exception
patterns. So, we collected a dataset consisting of millions of logsrecording exceptions with timestamps, and then developed a data-
driven failure prediction model, TAAT. TAAT improves over BERT
on:
(1)Exception representation. The exception logs are unstructured
text. So, we design a log tokenization module, which clus-
ters logs and assigns a unique index to each category, using
expert knowledge based keyword matching based. Subse-
quently, these categories are vectorized using a word em-
bedding layer [ 15]. Experiment in section 4.5.1 confirms that
log tokenization can reduce computational cost by an or-
der of magnitude while significantly enhancing the model‚Äôs
precision.
(2)Timestamp representation. The timestamp is a continuous
variable, difficult to be represented using embeddings with a
finite number of indices. To reduce the embedding sparsity,
we design a timestamp embedding module which partitions
a timestamp into different levels according to common time
units (e.g., day, hour, minute, and second) and embed them
separately, so each time level can be represented using an
embedding with no more than 60 indices.
(3)Semantic and temporal information integration. BERT adds
positional and sequence embeddings together as inputs. Hi-
erarchical burnout prediction based on activity logs (HiPAL)
[21] and Mercer [ 22] concatenate time and sequence em-
beddings as inputs. However, since timestamps are more
complex than positions, adding or concatenating them di-
rectly to the exception embeddings may confuse the semantic
and temporal information, resulting in difficulties in model
training. To reduce the input complexity, we design a time re-
lation estimation module (TRE), which applies self-attention
to the exception and timestamp embeddings separately, and
then adds the obtained weight matrices. Experiment in sec-
tion 4.5.2 shows that TRE helps training loss of TAAT con-
verge faster and lower with smaller fluctuations.
3 METHODOLOGY
3.1 Log Tokenization and Organization
This subsection introduces how we transform the collected log
sequences into mathematical representations.
An intuitive idea is to sequentially concatenate all logs of a se-
quence into an extended sentence or paragraph, and treat it as an
NLP-like problem. However, raw logs contain redundant words,
which may mask critical information and increase the computa-
tional cost. To tackle this problem, we propose a log tokenization
module to simplify the log sequences. It has two steps, as shown in
Figure 3:
(1)Tokenization , which transforms each raw log into a unique
exception index. Specifically, we extract some key informa-
tion from logs based on expert knowledge, and converts raw
logs with certain keywords or a group of specific keywords
into unique exceptions. We then construct an exception vo-
cabulary with ùê∂integers (ùê∂is the number of exception types),
where each exception is converted into an integer in [1,ùê∂],
according to its index in the vocabulary.
(2)Organization , which arranges all transformed exceptions
of one server into a sequence. For example, as shown in
4908KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Lingfei Deng, et al.
Figure 3: Illustration of the log tokenization and organization
process. In tokenization, we transform the log with keywords
‚Äòmce notify irq‚Äô and ‚Äòcallbacks suppressed‚Äô into ‚ÄòException
1‚Äô, and so on. In organization, we sequentially arrange the
exceptions and their timestamps to obtain an exception se-
quence and a timestamp sequence, respectively.
Figure 3, a string of exceptions from a server is converted into
sequence[1,2,3,3,1,2,3,2,3]. The corresponding timestamp
sequences are also constructed.
3.2 TAAT
We transform logs into time-series data using log tokenization and
organization. BERT is an effective approach for this purpose, but it
does not make use of the timestamps. TAAT fills this gap.
3.2.1 The Overall Framework of TAAT. LetD={(ùíôùëñ,ùíïùëñ,ùë¶ùëñ)}ùëÄ
ùëñ=1be
the training set with ùëÄsamples, where ùíôùëñ‚ààRùêøis theùëñth sample
(ùêøis the sequence length), ùíïùëñ‚ààRùêøis the corresponding timestamp
sequence, and ùë¶ùëñ‚àà{0,1}is the corresponding label (1 for positive
and 0 for negative). Note that the first token in each sequence is
the class token with index 99 in the vocabulary, i.e., ùë•1
ùëñ=99‚àÄùëñ. The
first timestamp in each sequence is the sampling time. Our goal is
to train a good TAAT model on D.
TAAT includes five components, as shown in Figure 4:
(1)Exception embedding , which uses trainable embeddings as
in BERT [15].
(2)Timestamp embedding , which splits an input timestamp se-
quence into different levels and converts them into vectors
respectively.
(3)Time relation estimation (TRE) , which takes the timestamp
embeddings as inputs and extracts the temporal relationship
matrixùëÖ‚àó‚ààRùêø√óùêø. Inspired by [ 23], we apply a multi-head
attention to compute the relation of different timestamps on
the timestamp embedding vectors.
(4)Multi-head time-aware attention (MTAA) block , which takes
the exception sequence embeddings and timestamp embed-
dings as inputs to integrate the semantic and temporal in-
formation.
(5)Classifier , which uses a fully connected layer of neural net-
work to classify the encoder outputs.
All parameters are optimized via back-propagation.
3.2.2 Exception Embedding. Figure 5a illustrates exception em-
bedding. TAAT utilizes the timestamp information instead of the
sequence positions.
Figure 4: Framework of our proposed TAAT, which takes the
exception sequence and its timestamps in Figure 3 as inputs.
Following BERT [ 15], we learn an embedding function ùë£ùê∏(¬∑)that
maps each exception index into a ùëë-dimensional real-valued vector.
Given a sequence ùíôùëñ=[ùë•1
ùëñ,¬∑¬∑¬∑,ùë•ùêø
ùëñ]‚ààRùêø, the output of exception
embedding is:
ùëç(0)
ùëñ=ùëâùëÜ(ùíôùëñ)=Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùë£ùê∏(ùë•1
ùëñ)
...
ùë£ùê∏(ùë•ùêø
ùëñ)Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª‚ààRùêø√óùëë. (1)
Denote the input of the ùëõth network layer as ùëç(ùëõ‚àí1)
ùëñ.Note that the
class token at the beginning of each exception sequence is a special
word in the exception vocabulary.
3.2.3 Timestamp Embedding. Figure 5b illustrates the timestamp
embedding process, which divides a timestamp into different levels
and embeds them separately.
Since the timestamp is a continuous variable, directly embedding
the original timestamps may lead to extremely sparse embedding.
Therefore, we use a pyramid-like timestamp structure, which di-
vides each timestamp into ùúô‚àà[1,4]levels, embeds them separately,
and then adds them together to get the timestamp embedding.
Specifically, given a timestamp sequence ùíïùëñ=[ùë°1
ùëñ,¬∑¬∑¬∑,ùë°ùêø
ùëñ]‚ààRùêø,
we first calculate the time interval between each timestamp and
the sampling time:
Œîùíïùëñ=[Œîùë°1
ùëñ,Œîùë°2
ùëñ,...,Œîùë°ùêø
ùëñ], (2)
Œîùë°ùëó
ùëñ=ùë°1‚àíùë°ùëñ, ùëñ=1,2,...,ùêø, (3)
Then, we convert all Œîùë°to a form that consists of day, hour, minute
and second parts, denoted as Œî1ùë°,Œî2ùë°,Œî3ùë°andŒî4ùë°, respectively.
For instance, Œîùë°=29hours is converted to ‚Äò1 day 5 hours 0 minute
4909Time-Aware Attention-Based Transformer (TAAT)
for Cloud Computing System Failure Prediction
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
(a)
(b)
Figure 5: Embeddings. (a) Exception embedding. [99]is the
class token, which is placed at the beginning of each excep-
tion sequence. (b) Time embedding when ùúô=4, including
day-, hour-, minute- and second-level embeddings. The input
of the time embedding layer is the time interval between the
exception timestamp and the sampling time.
0 second‚Äô. Since we set the pad token to 0, the conversion is Œî1ùë°=2,
Œî2ùë°=6,Œî3ùë°=1, and Œî4ùë°=1.
Finally, the output of time embedding is:
ùëçùëá
ùëñ=ùëâùëá(ùíïùëñ)=Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùë£ùëá(Œîùë°1
ùëñ)
...
ùë£ùëá(Œîùë°ùêø
ùëñ)Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª‚ààRùêø√óùëëùëá, (4)
whereùëëùëáis the dimensionality of the time token‚Äôs hidden repre-
sentation, and
ùë£ùëá(Œîùë°)=ùúô‚àëÔ∏Å
ùëé=1ùë£ùëé(Œîùëéùë°), (5)
in whichùë£ùëé(Œîùëéùë°)(ùëé=1,2,3,4) is the embedding vector corre-
sponding to day, hour, minute and second part of Œîùë°, respectively.
ùúô‚àà[1,4]determines the finest levels used in timestamp embed-
ding.3.2.4 Time Relation Estimation (TRE). TRE uses a multi-head self-
attention layer to extract temporal information from the timestamp
sequences.
Specifically, given an embedded timestamp sequence ùëç‚ààRùêø√óùëëùëá
from (4), its output ùëÖ‚ààRùêø√óùêøis calculated as
ùëÖùëñ=ùëü(ùëçùëá
ùëñ)=ùëçùëá
ùëñùëäùëÑ
ùëá(ùëçùëá
ùëñùëäùêæ
ùëá)‚ä∫
ùúô‚àöÔ∏Å
ùëëùëá, (6)
whereùëäùëÑ
ùëá‚ààRùëëùëá√óùëë‚Ä≤
ùëáandùëäùêæ
ùëá‚ààRùëëùëá√óùëë‚Ä≤
ùëáare projection matrices,
in whichùëë‚Ä≤
ùëá=ùëëùëáis the projection dimensionality for attention
computation.
A class token is placed at the beginning of each sequence. The
final output of TRE, ùëÖ‚àó‚ààRùêø√óùêø, is
ùëÖ‚àó
ùëñùëó=ùëÖùëñùëó, ùëñ‚â†1, ùëó‚â†1
0, otherwise. (7)
3.2.5 Multi-head Time-aware Attention (MTAA). Time-aware At-
tention (TAA), shown in Figure 6, considers the semantic informa-
tion of the exception sequence and the temporal information of the
timestamp sequence simultaneously.
Figure 6: Time-aware attention (TAA).
Given the learned time relation matrix ùëÖ‚àó
ùëñand the output of the
(ùëõ‚àí1)th layerùëç(ùëõ‚àí1)
ùëñ‚ààRùêø√óùëë, TAA is computed as:
TAA(ùëÑ(ùëõ)
ùëñ,ùêæ(ùëõ)
ùëñ,ùëâ(ùëõ)
ùëñ,ùëÖ‚àó
ùëñ)
=softmax 
ùëÑ(ùëõ)
ùëñùêæ(ùëõ)‚ä∫
ùëñ‚àö
ùëë‚Ä≤+ùëÖ‚àó
ùëñ!
ùëâ(ùëõ)
ùëñ, (8)
ùëÑ(ùëõ)
ùëñ=ùëç(ùëõ‚àí1)
ùëñùëä(ùëõ),ùëÑ
ùëç, (9)
ùêæ(ùëõ)
ùëñ=ùëç(ùëõ‚àí1)
ùëñùëä(ùëõ),ùêæ
ùëç, (10)
ùëâ(ùëõ)
ùëñ=ùëç(ùëõ‚àí1)
ùëñùëä(ùëõ),ùëâ
ùëç, (11)
whereùëÑ(ùëõ)
ùëñ‚ààRùêø√óùëë‚Ä≤,ùêæ(ùëõ)
ùëñ‚ààRùêø√óùëë‚Ä≤andùëâ(ùëõ)
ùëñ‚ààRùêø√óùëëare re-
spectively query, key and value matrices of ùëç(ùëõ‚àí1)
ùëñ, andùëä(ùëõ),ùëÑ
ùëç‚àà
Rùëë√óùëë‚Ä≤,ùëä(ùëõ),ùêæ
ùëç‚ààRùëë√óùëë‚Ä≤andùëä(ùëõ),ùëâ
ùëç‚ààRùëë√óùëëare projection ma-
trices corresponding to them. ùëë‚Ä≤is the projection dimensionality
4910KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Lingfei Deng, et al.
for attention computation in TAA.ùëÑ(ùëõ)
ùëñùêæ(ùëõ)‚ä∫
ùëñ‚àö
ùëë‚Ä≤andùëÖ‚àó
ùëñrepresent the
semantic and temporal correlation between the exceptions, respec-
tively.
TAAT consists of ùëÅMTAA blocks, each including an MTAA
sub-layer and a feedforward sub-layer. One MTAA block employs
ùêªparallel TAAs to jointly capture different aspects of semantic
information and temporal information. The output of the ùëõth MTAA
block is:
MTAA(ùëç(ùëõ‚àí1)
ùëñ)
=Concat
head(ùëõ)
1,ùëñ,..., head(ùëõ)
ùêª,ùëñ
ùëä(ùëõ),ùëÇ, (12)
whereùëä(ùëõ),ùëÇ‚ààRùêªùëë‚Ñé√óùëëis the concatenated projection matrix of
theùëõth MTAA block, Concat()is the concatenation function, and
head(ùëõ)
‚Ñé,ùëñ=TAA
ùëÑ(ùëõ)
‚Ñé,ùëñ,ùêæ(ùëõ)
‚Ñé,ùëñ,ùëâ(ùëõ)
‚Ñé,ùëñ,ùëÖ‚àó
‚Ñé,ùëñ
, (13)
ùëÑ(ùëõ)
‚Ñé,ùëñ=ùëç(ùëõ‚àí1)
ùëñùëä(ùëõ),ùëÑ
ùëç,‚Ñé, (14)
ùêæ(ùëõ)
‚Ñé,ùëñ=ùëç(ùëõ‚àí1)
ùëñùëä(ùëõ),ùêæ
ùëç,‚Ñé, (15)
ùëâ(ùëõ)
‚Ñé,ùëñ=ùëç(ùëõ‚àí1)
ùëñùëä(ùëõ),ùëâ
ùëç,‚Ñé, (16)
in whichùëä(ùëõ),ùëÑ
ùëç,‚Ñé‚ààRùëë‚Ñé√óùëë‚Ä≤
‚Ñé,ùëä(ùëõ),ùëÑ
ùëç,‚Ñé‚ààRùëë‚Ñé√óùëë‚Ä≤
‚Ñé,ùëä(ùëõ),ùëÑ
ùëç,‚Ñé‚ààRùëë‚Ñé√óùëëùêª
are projection matrices in the ‚Ñéth head self-attention module of
MTAA.ùëë‚Ñéandùëë‚Ä≤
‚Ñéare the dimensionalities before and after projec-
tion for each head, respectively. ùëÖ‚àó
‚Ñé,ùëñdenotes the ‚Ñéth head attention
of TRE.
After the feedforward sub-layer, the output of the ùëõ-th MTAA
block is
Àúùëç(ùëõ)
ùëñ=LN(MTAA((ùëç(ùëõ‚àí1)))ùëñ+ùëç(ùëõ‚àí1))ùëñ, (17)
ùëç(ùëõ)
ùëñ=LN(FFN(Àúùëç(ùëõ)
ùëñ)+Àúùëç(ùëõ)
ùëñ), (18)
where LNis layer normalization [ 24], and FFN the position-wise
feedforward sub-layer.
4 EXPERIMENTS
4.1 Dataset
We release a large log-based cloud computing system failure predic-
tion dataset https://tianchi.aliyun.com/dataset/159994 , which con-
tains‚àº2.7 billion syslogs from ‚àº300,000 servers in a 4-month period
of the real productional system of Alibaba Cloud, as summarized
in Table 1. We provide the original log lists in this public dataset so
that researchers can experiment different sampling strategies. The
raw logs have been desensitized and log-tokenized, due to Alibaba
Cloud confidentiality policy. More details are given in Data Ap-
pendix. In this paper‚Äôs experiment, we sampled each server every
five minutes, and arranged all exceptions and their corresponding
timestamps within 3 days before the sampling time in reverse order
to form an exception sequence and its corresponding timestamp se-
quence, as a sample for that server. This sampling strategy balances
data timeliness and computational complexity, and has already been
applied online. Table 2 summarizes the training and test sets.Table 1: Statistics of the dataset. ‚ÄòK‚Äô means thousand.
Training set Test set
Number of failure servers 9,485 2,501
Number of normal servers 280,025 12,229
Number of failure server syslogs 337,763 K 30,362 K
Number of normal server syslogs 2,666,147 K 158,078 K
Table 2: Summary of the sample sizes. ‚ÄòK‚Äô means thousand.
DatasetNumber of Samples
Failure Normal Total
Training set 668 K 146,449 K 147,117 K
Test set 155 K 18,526 K 18,682 K
4.2 Baselines
We compared our proposed TAAT with three categories of base-
lines: 1) Traditional machine learning approaches, which manually
extract statistic features, including support vector machine (SVM)
[25], Principal Component Analysis (PCA) [ 20], LogCluster [ 26],
Decision Tree (DT) [ 27] and XGBoost [ 28]; 2) Deep learning ap-
proaches, which automatically extract features, including convolu-
tional neural network (CNN) [ 29], Text-Recurrent Neural Network
(Text-RNN) [ 30], DeepLog [ 7], and TimeJoint [ 31]; and, 3) Self-
attention based approaches, which capture the global contextual
information of sequences, including Transformer [ 16], BERT [ 15],
LogBERT [8], LogEncoder [32] and Mercer [22].
4.3 Implementation Details
For all algorithms, we first performed log tokenization to obtain
the input sequences.
For traditional machine learning baselines, we manually ex-
tracted features from an input sequence by building a counting
series based on the frequency of each exception in it. Then, we used
Loglizer4[33] to evaluate their performance.
For deep learning based approaches, CNN used 4 convolution
layers with kernel size (ùëò,256), whereùëò‚àà{15,11,7,3}. The hidden
size was 256. Text-RNN and TimeJoint used a 4-layer bidirectional
LSTM. Transformer, BERT and Mercer used the structure without
pre-training tasks and had the same settings as our proposed TAAT,
as follows. We set the number of TAA blocks ùëÅ=4, andùêª=8
in each block. The embedding dimensionality of the exceptions
wasùëë=256, andùëëùëá=64. The time embedding level ùúô=3,
i.e., only day-, hour- and minute-level time information was used
(hyper-parameter sensitivity analysis is given in Appendix). When
training the network, following the settings of BERT in [ 15], we
adopted AdamW [ 34] as the optimizer with an initial learning rate
5√ó10‚àí5and a warm-up mechanism in the first 10% training steps.
The number of training epochs and batch size were 3 and 256,
respectively. Other hyperparameters were the same as those in
Transformers5. For all deep learning based approaches, we repeated
each experiment three times.
4https://github.com/logpai/loglizer
5https://github.com/huggingface/transformers
4911Time-Aware Attention-Based Transformer (TAAT)
for Cloud Computing System Failure Prediction
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
During training, 25% of the training set was reserved for vali-
dation to pick the classification thresholds for all supervised ap-
proaches, except PCA, which did not need a threshold since its
predictions were already binary.
4.4 Experimental Results
Table 3 shows the performance of all algorithms. Our proposed
TAAT outperformed all others in terms of ùêπ1score. More specifi-
cally:
Table 3: Performance of different algorithms (%).
Category Approach Precision Recall ùêπ1
TraditionalSVM 85.71 1.00 1.98
PCA 13.10 11.75 12.39
LogCluster 14.64 40.39 21.49
DT 41.88 40.19 41.02
XGBoost 47.60 39.80 43.35
Deep LearningCNN 51.01¬±1.81 39.36¬±0.74 44.43¬±1.05
Text-RNN 38.95¬±2.97 44.13¬±0.95 41.34¬±1.75
TimeJoint 41.27¬±0.96 47.60¬±0.67 44.21¬±0.54
DeepLog 29.26¬±1.03 40.63¬±0.92 34.02¬±0.67
Self-attentionTransformer 47.95¬±8.96 26.26¬±0.49 33.75¬±2.58
BERT 46.98¬±2.15 48.39¬±0.16 47.66¬±1.18
BERT w/o position 47.08¬±0.46 48.37¬±0.16 47.71¬±0.24
LogBert 47.12¬±0.33 48.41¬±0.15 47.75¬±0.21
LogEncoder 47.46¬±0.78 48.39¬±0.61 47.92¬±0.58
Mercer 46.99¬±1.64 49.38¬±0.82 48.16¬±0.97
TAAT 54.05¬±0.84 50.59¬±0.75 52.26¬±0.69
(1)Comparison of different categories of approaches. Generally,
deep learning approaches outperformed traditional approaches
due to the simplicity of manual features that only rely on
the statics of exception frequencies. Self-attention based ap-
proaches generally outperformed deep learning approaches
due to their ability to extract global semantic information of
the full sequence.
(2)Comparison of position and timestamp encodings. Among
the deep learning approaches, TimeJoint outperformed Text-
RNN (both had the same LSTM network backbone, but Time-
Joint additionally took timestamps as inputs). Among the
self-attention based approaches, Mercer outperformed BERT
(they had the same network structure, but Mercer embed-
ded timestamps and BERT embedded positions). These re-
sults demonstrated that timestamps are more important than
positions in handling time-dependent exception sequences.
TAAT had‚àº10%ùêπ1score improvement over BERT.
(3)Comparison of approaches that consider timestamps. Time-
Joint, Mercer and our proposed TAAT all took timestamps
as inputs. Mercer outperformed TimeJoint due to the strong
ability of self-attention. Both TAAT and Mercer utilized self-
attention (Mercer encoded timestamp using trigonometricfunction and concated it with the exception embedding as in-
puts), but TAAT had better performance, demonstrating the
effectiveness of our proposed timestamp encoding function
and information fusion strategy for log-based server failure
prediction. Further analyses of the innovations in TAAT are
introduced in section 4.5.
(4)Necessity of position encoding. Notably, ‚ÄòBERT w/o position‚Äô
outperformed position encoding based approaches, Trans-
former and BERT. One possible reason is that log data are
sometimes not sensitive to the local position information,
i.e., the order in which some exceptions occur varies depend-
ing on different business processes running on the server,
which is irrelevant to the server‚Äôs health status. Adding po-
sitional encoding would introduce redundant information
and interfere with model training.
4.5 Ablation Studies
4.5.1 Effect of Log Tokenization. We compare the failure prediction
performance when inputs are raw and tokenized logs. In Table 4,
‚ÄòTAAT w/o Log Tokenization‚Äô sequentially concatenates a series
of raw logs into a long sentence as a complete sample, and con-
structs a large vocabulary containing all single words in raw logs
for tokenization. All other experimental settings were the same as
TAAT.
Table 4: Ablation study results (%). ‚Äòtime/epoch‚Äô means time
cost (seconds) when training an epoch.
Algorithm Precision Recall F1 time/epoch
TAAT w/o Log Tokenization 30.09 46.63 36.58 5.4√ó104
TAAT w/o TRE 38.31 52.11 44.15 7.8√ó102
TAAT 54.05 50.59 52.26 8.4√ó102
Table 4 shows that removing log tokenization decreased the
performance of TAAT, validating our hypothesis that redundant
words in raw logs may degrade the model‚Äôs ability to extract crit-
ical information. In addition, log tokenization transforms a long
sentence to an integer, significantly reducing the computational
cost (approximately two orders of magnitude).
4.5.2 Effect of Time Relation Estimation. This comparison validates
that applying self-attention to exception and timestamp embed-
dings separately through time relation estimation is better than
adding them directly as inputs (‚ÄòTAAT w/o Time Relation Estima-
tion‚Äô in Table 4), when coping with the challenge of integrating
semantic and temporal information (Section 2.2).
Table 4 shows that TRE significantly improved the precision
while keeping high recall and low computational cost. Figure 7
shows the training loss trajectories of the two models, where the
training loss of ‚ÄòTAAT w/o Time Relation Estimation‚Äô fluctuated
significantly. It demonstrates that adding exception and timestamp
embeddings directly may make inputs overly complex and even
confusing, resulting in difficulty in model training.
4912KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Lingfei Deng, et al.
0 2500 5000 7500 10000 12500 15000 17500
Number of iterations0.10.20.30.40.50.60.7LossT A A T w/o TRE
T A A T
Figure 7: Training loss trajectories.
4.6 Lead Time Analysis
In the real application, the system is often incapable of instanta-
neously addressing all faulty NCs due to factors that include the
lengthy duration necessitated by live migrations and prevailing
inventory deficiencies. Therefore, predictive lead time emerges as
a crucial metric for evaluating the efficacy of predictions. We cate-
gorized the predictive lead time into five intervals and compared
the predictive lead time distributions under different classification
thresholds via a percentage stacked bar chart. As shown in Figure 8,
specifically,
(1)When threshold is 0.5 (In Table 3, TAAT adopted 0.5 as thresh-
old), over 60% positive samples were predicted fail at least 1
hour in advance.
(2)As the classification threshold increases, there is a progres-
sive increase in the quantity of NCs breaking down within
one hour subsequent to their prediction. This phenomenon
suggests that a higher confidence level in the model‚Äôs out-
put is typically indicative of a more imminent crash, which
offers valuable insights for the scheduling in operational
maintenance.
0.5 0.6 0.7 0.8 0.9
Classification threshold0.00.20.40.60.81.0Ratio>12h
3h~12h
1h~3h
30min~1h
5~30min
Figure 8: Predictive lead time distribution when classification
threshold varies from 0.5 to 0.9.
5 DEPLOYMENT CONSIDERATIONS
5.1 Online Validation
TAAT improved the ùêπ1score of BERT by‚àº10% in our offline exper-
iments, as shown in table 3. To validate their online performance,we gray released TAAT and BERT to the production system of Al-
ibaba Cloud ECS (i.e., making predictions without executing live
migrations) for a month. Table 5 shows that, compared with BERT,
TAAT can identify 55 additional failure servers within a month.
Each server hosts on average 5 VMs, so TAAT has the potential to
safeguard an extra 275 VMs, assuming all those VMs successfully ex-
ecute live migration. In addition, the reduction of 290 false positives
is also beneficial, because false positives increase the maintenance
cost, and unnecessary maintenance may hurt the running business
on VMs and even cause a healthy server to fail.
Table 5: Comparison of prediction result between TAAT and
BERT. ‚ÄòTrue Positive‚Äô refers to cases correctly predicted as
failures, and ‚ÄòFalse Positive‚Äô indicates cases incorrectly pre-
dicted as failures when they did not actually fail.
ApproachNumber of Servers
True Positive False Positive
BERT 1,210 1,365
TAAT 1,265 1,075
5.2 Computation Overhead
We used a distributed computing system composed of four servers
to convert collected raw logs into numerical sequences through
log tokenization, and a single server with an NVIDIA GV100GL
GPU and 336 GB RAM to predict and report risks for all sequences.
This system can process over 90,000 sequences per minute, satis-
fying our online application requirement (i.e., 30,000 samples per
minute). Consequently, TAAT had capability to safeguard 1,265
servers within a month (as shown in Table 5) using only 5 servers,
i.e., the operation and maintanence overhead of the system are
negligible compared to its substantial benefits.
6 RELATED WORKS
6.1 Traditional Approaches
Traditional approaches first extract and select features using ex-
pert knowledge or statistical analysis, and then send them to a
classifier, e.g., SVM [ 10,25], decision tree (DT) [ 12,27], or logistic
regression [11], for prediction.
Pitakrat et al. [35] compared 21 classifiers, including probabilistic
models, DTs, rule-based approaches, et al., on hard disk drive failure
prediction. Mahdisoltani et al. [36] extracted 11 and 21 features for
hard disk drives and solid state drives, respectively, and used them
in five models to predict disk sector errors. Aussel et al. [17] selected
task-related SMART attributes reported by drivers and used them
in SVM, Random Forest, and Gradient-Boosted Tree for hard drive
failure prediction. Rosa et al. [37] proposed an online model, which
combines Linear Discriminant Analysis, Quadratic Discriminant
Analysis and Logistic Regression, to predict software and hardware
related job execution failures.
6.2 Deep Learning Approaches
Sun et al. [38] proposed a system-level failure prediction model, in
which a Convolutional Neural Network (CNN) was used to predict
4913Time-Aware Attention-Based Transformer (TAAT)
for Cloud Computing System Failure Prediction
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
the failure probability from device usage data, system logs and
static information. Lin et al. [39] proposed MING, which combines
LSTM and Random Forest for temporal and spatial data processing,
respectively. Lu et al. [18] combined convolutional neural networks
and LSTM in disk failure prediction using SMART signals. Luo et al.
[40] designed a Neighborhood-Temporal Attention Model (NTAM)
to predict disk failures, using simultaneously the disk‚Äôs data and its
neighbors‚Äô data. Levy et al. [41] designed Narya, which extracts em-
poral and spatial features by Set Transformer [ 42] and then makes
prediction using Gradient-Boosted Tree. Narya achieved 5%-10%
performance improvement, compared with DT with handcrafted
features.
Transformer uses self-attention [ 16] to capture long-term de-
pendency, which has achieved great success in sequential data
processing, e.g., NLP [ 15,43]. Li et al. [44] proposed ARB-BERT
to predict failures caused by software aging in Aging-Related Bug
reports. To handle class-imbalance, ARB-BERT reconstructs the
original log data to increase the sample diversity, and employs
random over-sampling and under-sampling to balance different
classes.
6.3 Time-Dependent Event Sequence Processing
Approaches
Liet al. [31] proposed TimeJoint, a time-aware recurrent neural
network, where inter-event time intervals are directly appended
to hidden event representations as inputs. They evaluated the ef-
fectiveness of TimeJoint in financial transactions, APP usage, and
music recommendation.
There are some works considering timestamps in Transformer.
Renet al. [45] used a time-aware Transformer as the base model for
learning healthcare data representations. The absolute time (week
index of each visit) was used for position encoding, and the time
intervals were decoded by a fully-connected layer and added to
the dot-product between keys and queries in self-attention calcula-
tion. Luo et al. [46] proposed a hierarchical time-aware attention
network, in which the time intervals were used as keys and the
outputs of local health record analysis as query. Xu et al. [22] de-
signed a functional feature map that embeds the time span into a
high-dimensional space, and then concatenated the time representa-
tions to the corresponding event embeddings for recommendation
systems.
7 CONCLUSION
This paper uses exception sequences recorded by the monitoring
system of Alibaba Cloud to predict server failures. To accommodate
the challenge of irregular timestamp intervals resulted from irregu-
lar exception occurrences, we proposed TAAT to learn the relation-
ship among different exceptions and timestamps in sequences to
extract semantic and temporal information simultaneously. Experi-
ments on Alibaba Cloud demonstrated the effectiveness of TAAT. It
is now used in the daily practice of Alibaba Cloud. We also release
the real-world cloud computing failure prediction dataset used in
our study, which consists of ‚àº2.7 billion syslogs from ‚àº300,000
servers during a 4-month period. To our knowledge, this is the
largest dataset of its kind, and shall be very useful to the commu-
nity.ACKNOWLEDGMENTS
Thanks to the students from Huazhong University of Science and
Technology for their valuable contributions to this paper, including
Kun Xia, Ruimin Peng, and Yifan Xu. This work was supported by
Alibaba Group through Alibaba Innovative Research Program.
REFERENCES
[1]Phuong Pham, Vivek Jain, Lukas Dauterman, Justin Ormont, and Navendu Jain.
2020. DeepTriage: Automated Transfer Assistance for Incidents in Cloud Services.
InProc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining. Virtual
Event, 3281‚Äì3289.
[2]Xu Zhang, Chao Du, Yifan Li, Yong Xu, Hongyu Zhang, Si Qin, Ze Li, Qingwei
Lin, Yingnong Dang, Andrew Zhou, Saravanakumar Rajmohan, and Dongmei
Zhang. 2021. HALO: Hierarchy-aware fault localization for cloud systems. In
Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining. Virtual
Event, 3948‚Äì3958.
[3]Fangkai Yang, Jue Zhang, Lu Wang, Bo Qiao, Di Weng, Xiaoting Qin, Gregory
Weber, Durgesh Nandini Das, Srinivasan Rakhunathan, Ranganathan Srikanth,
Qingwei Lin, and Dongmei Zhang. 2023. Contextual self-attentive temporal
point process for physical decommissioning prediction of cloud assets. In Proc. of
ACM SIGKDD Conf. on Knowledge Discovery and Data Mining. Long Beach, CA,
5372‚Äì5381.
[4]Da Sun Handason Tam, Yang Liu, Huanle Xu, Siyue Xie, and Wing Cheong Lau.
2023. PERT-GNN: Latency prediction for microservice-based cloud-native appli-
cations via graph neural networks. In Proc. of ACM SIGKDD Conf. on Knowledge
Discovery and Data Mining. Long Beach, CA, 2155‚Äì2165.
[5]Dan Lv, Nurbol Luktarhan, and Yiyong Chen. 2021. ConAnomaly: Content-Based
Anomaly Detection for System Logs. Sensors 21, 18 (2021), 6125.
[6]Elisabeth Baseman, Nathan DeBardeleben, Kurt Ferreira, Scott Levy, Steven
Raasch, Vilas Sridharan, Taniya Siddiqua, and Qiang Guan. 2016. Improving
DRAM fault characterization through machine learning. In Proc. Int‚Äôl Conf. on
Dependable Systems and Networks Workshop. Toulouse, France, 250‚Äì253.
[7]Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. Deeplog: Anomaly
detection and diagnosis from system logs through deep learning. In Proc. of ACM
SIGSAC Conf. on Computer and Communications Security. Dallas, TX, 1285‚Äì1298.
[8]Haixuan Guo, Shuhan Yuan, and Xintao Wu. 2021. LogBERT: Log anomaly
detection via BERT. In Int‚Äôl Joint Conf. on Neural Networks. Shenzhen, China,
1‚Äì8.
[9]Chenyu Zhao, Minghua Ma, Zhenyu Zhong, Shenglin Zhang, Zhiyuan Tan, Xiao
Xiong, LuLu Yu, Jiayi Feng, Yongqian Sun, Yuzhi Zhang, Dan Pei, Qingwei Lin,
and Dongmei Zhang. 2023. Robust multimodal failure detection for microservice
systems. In Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining.
Long Beach, CA, 5639‚Äî-5649.
[10] Errin W. Fulp, Glenn A. Fink, and Jereme N. Haack. 2008. Predicting computer
system failures using support vector machines. In Proc. USENIX Conf. on Analysis
of System Logs. San Diego, CA, 5.
[11] Moises Goldszmidt. 2012. Finding soon-to-fail disks in a haystack. In Proc. 2012
USENIX Workshop on Hot Topics in Storage and File Systems. Boston, MA, 8.
[12] Jing Li, Xinpu Ji, Yuhan Jia, Bingpeng Zhu, Gang Wang, Zhongwei Li, and Xi-
aoguang Liu. 2014. Hard drive failure prediction using classification and regres-
sion trees. In Proc. 44th Annual IEEE/IFIP Int‚Äôl Conf. on Dependable Systems and
Networks. Atlanta, GA, 383‚Äì394.
[13] Farzaneh Mahdisoltani, Ioan Stefanovici, and Bianca Schroeder. 2017. Proactive
error prediction to improve storage system reliability. In USENIX Annual Technical
Conf. Santa Clara, CA, 391‚Äì402.
[14] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long short-term memory. Neural
Computation 9, 8 (1997), 1735‚Äì1780.
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of deep bidirectional transformers for language understanding. In
Proc. 17th Annual Conf. of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. Minneapolis, MN,
4171‚Äì4168.
[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. Advances in Neural Information Processing Systems. Long Beach,
CA, 5998‚Äì6008.
[17] Nicolas Aussel, Samuel Jaulin, Guillaume Gandon, Yohan Petetin, Eriza Fazli,
and Sophie Chabridon. 2017. Predictive models of hard drive failures based on
operational data. In Proc. IEEE Int‚Äôl Conf. on Machine Learning and Applications.
Cancun, Mexico, 619‚Äì625.
[18] Sidi Lu, Bing Luo, Tirthak Patel, Yongtao Yao, Devesh Tiwari, and Weisong Shi.
2020. Making Disk Failure Predictions SMARTer!. In Proc. 18th USENIX Conf. on
File and Storage Technologies. Santa Clara, CA, 151‚Äì167.
[19] Shilin He, Jieming Zhu, Pinjia He, and Michael R Lyu. 2020. Loghub: A large
collection of system log datasets towards automated log analytics. arXiv preprint
4914KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Lingfei Deng, et al.
arXiv:2008.06448 (2020).
[20] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I Jordan. 2010.
Detecting Large-Scale System Problems by Mining Console Logs. In Proc. of Int‚Äôl
Conf. on Machine Learning. Haifa, Israel, 37‚Äì46.
[21] Hanyang Liu, Sunny S. Lou, Benjamin C. Warner, Derek R. Harford, Thomas
Kannampallil, and Chenyang Lu. 2022. HiPAL: A deep framework for physician
burnout prediction using activity logs in electronic health records. In Proc. of
ACM SIGKDD Conf. on Knowledge Discovery and Data Mining. Washington, DC,
3377‚Äì3387.
[22] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
2019. Self-attention with functional time representation learning. Advances in
Neural Information Processing Systems 32 (2019).
[23] Guolin Ke, Di He, and Tie-Yan Liu. 2021. Rethinking positional encoding in
language pre-training. In Int‚Äôl Conf. on Learning Representations. Virtual Event.
[24] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-
tion. arXiv:1607.06450 (2016).
[25] Yinglung Liang, Yanyong Zhang, Hui Xiong, and Ramendra Sahoo. 2007. Failure
prediction in IBM BlueGene/L event logs. In Proc. of IEEE Int‚Äôl Conf. on Data
Mining. Omaha, Nebraska, 583‚Äì588.
[26] Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, and Xuewei Chen. 2016.
Log clustering based problem identification for online service systems. In Proc.
of Int‚Äôl Conf. on Software Engineering Companion. Austin, TX, 102‚Äì111.
[27] Mike Chen, Alice X. Zheng, Jim Lloyd, Michael I. Jordan, and Eric Brewer. 2004.
Failure diagnosis using decision trees. In Proc. of Int‚Äôl Conf. on Autonomic Com-
puting. New York City, NY, 36‚Äì43.
[28] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting system.
InProc. of ACM SIGKDD Int‚Äôl Conf. on Knowledge Discovery and Data Mining. San
Francisco, CA, 785‚Äì794.
[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classi-
fication with deep convolutional neural networks. In Proc. Advances in Neural
Information Processing Systems. Lake Tahoe, NV, 1106‚Äì1114.
[30] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Recurrent neural network
for text classification with multi-task learning. In Proc. Int‚Äôl Joint Conf. on Artificial
Intelligence. New York City, NY, 2873‚Äì2879.
[31] Yang Li, Nan Du, and Samy Bengio. 2017. Time-dependent representation for
neural event sequence prediction. arXiv preprint arXiv:1708.00065 (2017).
[32] Jiaxing Qi, Zhongzhi Luan, Shaohan Huang, Carol Fung, Hailong Yang, Hanlu
Li, Danfeng Zhu, and Depei Qian. 2023. Logencoder: Log-based contrastive
representation learning for anomaly detection. IEEE Trans. on Network and
Service Management 20, 2 (2023), 1378‚Äì1391.
[33] Shilin He, Jieming Zhu, Pinjia He, and Michael R. Lyu. 2016. Experience report:
System log analysis for anomaly detection. In IEEE Int‚Äôl Symposium on Software
Reliability Engineering. Ottawa, Canada, 207‚Äì218.
[34] Johan Bjorck, Kilian Q. Weinberger, and Carla Gomes. 2021. Understanding
decoupled and early weight decay. In Proc. of AAAI Conf. on Artificial Intelligence.
Virtual Event, 6777‚Äì6785.[35] Teerat Pitakrat, Andre Van Hoorn, and Lars Grunske. 2013. A comparison of
machine learning algorithms for proactive hard disk drive failure detection. In
Proc. 4th Int‚Äôl ACM Sigsoft Symposium on Architecting Critical Systems. Vancouver,
Canada, 1‚Äì10.
[36] Farzaneh Mahdisoltani, Ioan Stefanovici, and Bianca Schroeder. 2017. Proactive
error prediction to improve storage system reliability. In Proc. USENIX Annual
Technical Conf. Santa Clara, CA, 391‚Äì402.
[37] Andrea Ros√†, Lydia Y. Chen, and Walter Binder. 2015. Predicting and mitigating
jobs failures in big data clusters. In Proc. IEEE/ACM Int‚Äôl Symposium on Cluster,
Cloud and Grid Computing. Shenzhen, China, 221‚Äì230.
[38] Xiaoyi Sun, Krishnendu Chakrabarty, Ruirui Huang, Yiquan Chen, Bing Zhao,
Hai Cao, Yinhe Han, Xiaoyao Liang, and Li Jiang. 2019. System-level hardware
failure prediction using deep learning. In Proc. 56th ACM/IEEE Design Automation
Conf. San Francisco, CA, 1‚Äì6.
[39] Qingwei Lin, Ken Hsieh, Yingnong Dang, Hongyu Zhang, Kaixin Sui, Yong Xu,
Jian-Guang Lou, Chenggang Li, Youjiang Wu, Randolph Yao, Murali Chintalapati,
and Dongmei Zhang. 2018. Predicting node failure in cloud service systems.
InProc. 26th ACM Joint Meeting on European Software Engineering Conf. and
Symposium on the Foundations of Software Engineering. Lake Buena Vista, FL,
480‚Äì490.
[40] Chuan Luo, Pu Zhao, Bo Qiao, Youjiang Wu, Hongyu Zhang, Wei Wu, Weihai Lu,
Yingnong Dang, Saravanakumar Rajmohan, Qingwei Lin, and Dongmei Zhang.
2021. NTAM: Neighborhood-temporal attention model for disk failure prediction
in cloud platforms. In Proc. Web Conf. 2021. Ljubljana, Slovenia, 1181‚Äì1191.
[41] Sebastien Levy, Randolph Yao, Youjiang Wu, Yingnong Dang, Peng Huang, Zheng
Mu, Pu Zhao, Tarun Ramani, Naga Govindaraju, Xukun Li, Qingwei Lin, Gil Lapid
Shafriri, and Murali Chintalapati. 2020. Predictive and adaptive failure mitigation
to avert production cloud VM interruptions. In Proc. 14th USENIX Symposium on
Operating Systems Design and Implementation. Virtual Event, 1155‚Äì1170.
[42] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and
Yee Whye Teh. 2019. Set Transformer: A framework for attention-based
permutation-invariant neural networks. In Proc. 36th Int‚Äôl Conf. on Machine
Learning. Long Beach, CA, 3744‚Äì3753.
[43] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[44] Mingxi Li and Bei-Bei Yin. 2021. ARB-BERT: An automatic aging-related bug
report classification method based on BERT. In Proc. 8th Int‚Äôl Conf. on Dependable
Systems and Their Applications. Yinchuan, China, 474‚Äì483.
[45] Houxing Ren, Jingyuan Wang, Wayne Xin Zhao, and Ning Wu. 2021. RAPT: Pre-
training of time-aware transformer for learning robust healthcare representation.
InProc. ACM SIGKDD Conf. on Knowledge Discovery and Data Mining. Virtual
Event, 3503‚Äì3511.
[46] Junyu Luo, Muchao Ye, Cao Xiao, and Fenglong Ma. 2020. HiTANet: Hierarchical
time-aware attention networks for risk prediction on electronic health records. In
Proc. ACM SIGKDD Int‚Äôl Conf. on Knowledge Discovery and Data Mining. Virtual
Event, 647‚Äì656.
4915Time-Aware Attention-Based Transformer (TAAT)
for Cloud Computing System Failure Prediction
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
A DATASET
Existing public server log datasets [ 19,20] may not faithfully rep-
resent online cloud ECS systems. Therefore, this paper releases a
large dataset for log-based cloud ECS failure prediction at
https://tianchi.aliyun.com/dataset/159994?lang=en-us .
A.1 Overview
The Alibaba Cloud ECS has monitors to record logs from servers.
The logs are text that record abnormal events (exceptions) in a fixed
format, which provide important information for server failure
prediction. Due to the presence of numerous redundant information
and a wide variety of log types, we clustered different logs by
keyword matching and assigned a unique identifier (exception
index) to each log category. The raw logs have been desensitized
and log-tokenized, due to Alibaba Cloud confidentiality policy.
A.2 Statistics of the Dataset
This dataset contains around 2.7 billion syslogs from about 300,000
servers in a 4-month period of the Alibaba Cloud production sys-
tem (6.66 GB of data), as summarized in Table 1. In this paper‚Äôs
experiment, we sampled each server every 5 minutes, and arranged
all exceptions and their timestamps within 3 days before the sam-
pling time to form an exception sequence and its corresponding
timestamp sequence, as a sample for that server. The training and
test set sizes under our sampling strategy are shown in Table 2. The
public dataset provides the original log lists so that researchers can
experiment different sampling strategies.
This dataset includes 98 exception categories, each correspond-
ing to an exception index ranging from 1 to 98. Table 6 shows the
hardware or software associated with different exception indices
(exceptions corresponding to multiple sources are only categorized
into one class). Table 7 shows the occurrence frequency of 98 excep-
tions in both failure and non-failure (normal) servers in the training
set.
Table 6: Exceptions associated with different hardware and
software.
Related Source Exception Index
CPU2-4, 9, 14, 16-18, 20, 23, 26, 28, 30, 32-34,
40, 43, 49, 51, 53, 55, 59, 74, 82, 96
Disk 11, 24, 42, 61, 76
I / O 22, 25, 38, 47, 65, 67, 84
Memory6-8, 10, 13, 15, 19, 35, 39, 48, 52, 56, 57, 63,
66, 68, 69, 73, 77, 81, 86, 88-94
Network 45, 50, 58, 60, 85, 95
Software 31, 36, 37, 44, 54, 71, 72, 79, 80, 83, 87, 97
Others 1, 5, 12, 21, 27, 29, 41, 46, 62, 64, 70, 75, 78, 98
A.3 Characteristics of the Dataset
Main characteristics of the dataset include:
(1)Irregular timestamp distributions. Figure 2 shows the ex-
ception frequencies of two servers with the same exception
types. In general, the time distributions were uneven, andit is possible for a single server to report a large number of
exceptions within a short period of time.
(2)Multiple log sources. As shown in Table 6, exceptions record
different types of hardware and software anomalies, resulting
in the diversity of exception patterns.
(3)Severe class imbalance. As shown in Table 1, the number of
failure servers was significantly smaller than that of normal
servers.
(4)Severe exception imbalance. As shown in Table 7, there were
significant differences in the occurrence frequencies of dif-
ferent exceptions, e.g., ‚Äòexception13‚Äô occurred frequently,
whereas ‚Äòexception26‚Äô occurred vary rarely.
(5)Significant distribution shift. As shown in Table 7, a few
exceptions, e.g., ‚Äòexception25‚Äô and ‚Äòexception27‚Äô appeared
only in the test set but not in the training set. Since training
and test sets were collected in different time periods, this
demonstrates that the exception distribution shifted over
time.
Table 7: Exception occurrence frequency statistics of the
training set. ‚ÄòIndex‚Äô is the index of an exception. ‚ÄòPosserver‚Äô
and ‚ÄòNegserver‚Äô represent the number of failure and normal
servers on which this exception happened, respectively. ‚ÄòPos-
Freq‚Äô and ‚ÄòNegFreq‚Äô mean average number of occurrences of
this exception on failure and normal servers, respectively.
Index Posserver Negserver PosFreq NegFreq Index Posserver Negserver PosFreq NegFreq
1 4565 61408 5736.7 2528.3 51 702 6839 335.9 386.4
2 3288 26957 3085.0 2001.6 52 800 9618 11336.8 6236.0
3 2444 11978 46.3 71.9 53 112 6268 86.0 79.7
4 1618 11913 69.0 72.2 54 48 2827 85.7 470.7
5 380 1330 24.0 375.6 55 1058 17492 380.6 152.2
6 165 18 2.8 200.0 56 477 2272 318.5 1196.7
7 4049 51034 41841.3 18002.1 57 190 1162 209.0 456.1
8 947 2762 106.2 286.6 58 209 6160 83.5 63.3
9 903 2688 113.9 252.8 59 223 1759 243.4 900.3
10 817 2646 118.2 296.5 60 22 33 4027.9 9799.2
11 195 1231 41.1 133.6 61 76 755 2712.9 1757.5
12 1511 8334 150.3 318.6 62 21 89 19.1 24.5
13 2025 24414 39544.4 22204.3 63 182 5790 57.9 134.2
14 11 116 66.2 271717.3 64 230 6583 59.6 70.3
15 1953 33260 701.5 624.4 65 969 968 15.9 4.8
16 6 38 32.3 387938.4 66 141 992 1063.1 2771.2
17 336 1408 1592.8 8727.6 67 1 29 1.0 1.0
18 113 1079 303.3 193.7 68 41 1057 19576.9 2605.4
19 101 2835 375.9 468.9 69 611 2289 374.6 492.1
20 104 1713 488.3 621.6 70 120 12210 1233.0 1970.0
21 16 187 263.1 420.4 71 647 11845 175.0 90.6
22 1 47 22.0 1364.0 72 47 17 1089.9 200.4
23 30 1029 1.2 1.8 73 917 12292 3638.3 2056.0
24 27 1491 1.0 1.0 74 65 285 7.8 55.6
26 2 25 1.0 1.0 75 372 13111 43.6 127.9
30 579 1790 55.8 810.9 76 106 613 979.2 9259.2
31 2 35 172.0 29.9 77 380 3180 1031.5 738.6
32 79 1095 22.1 13.6 78 279 1550 2712.4 10519.4
33 40 4077 2000.4 685.6 80 22 169 4.6 14185.9
34 1082 20180 3286.5 1700.6 81 32 755 127.8 415.5
35 804 2767 448.6 2356.7 82 176 1252 2637.5 1621.0
36 59 28 15.8 12.5 83 90 2104 145.0 2324.2
37 254 1068 423.8 2317.3 84 213 687 79.6 113.7
38 41 212 549.5 3744.5 85 9 89 10.9 2.8
39 874 7569 822.1 556.0 86 222 472 203.8 666.6
40 90 102 5.7 24367.6 87 112 143 217.3 30.5
41 115 47 8.2 107.0 88 1400 23317 316.7 144.9
42 7 15 24.7 253.3 89 717 3761 1991.7 1152.4
43 1452 24739 5431.2 4394.7 90 160 9988 6543.8 2910.9
44 20 2 46.2 65.0 91 607 6597 118.0 86.5
45 55 601 283.0 27.0 92 417 4812 4.7 2.7
46 340 7826 10.6 6.0 93 3 81 127.7 40.8
47 602 34422 4.5 4.5 94 504 1345 258.0 700.8
48 1150 21887 4471.2 2206.2 95 105 3511 66.0 14.2
49 1077 20189 7604.7 3512.5 96 2561 157035 107.0 192.6
50 1031 3772 2498.7 9589.6 97 3219 212394 14.2 59.7
4916KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Lingfei Deng, et al.
(6)Extremely large scale. As shown in Table 1, many servers
had a large amount of syslogs in their lifecycle, resulting in
high computational cost.
This paper tackles the first and second challenges, by integrating
simultaneously temporal and semantic information in the atten-
tion mechanism. Other challenges will be considered in our future
research.
B SENSITIVITY ANALYSIS
B.1 Effect of Timestamp Granularity
We studied the effectiveness of different timestamp granularities:
1) single granularity, which uses day, hour, minute, or second as
timestamp; and, 2) combined granularity, which uses a pyramid-like
timestamp combination like ‚Äòday+hour+minute+second‚Äô.
‚ÄòTimestamp Format‚Äô panel of Table 8 shows the results. In gen-
eral, the combined granularity outperformed the individual gran-
ularities, which is intuitive. Among the four individual granu-
larities, day- and hour-level timestamps outperformed minute-
and second-level ones. Among the three combined granularities,
‚Äòday+hour+minute‚Äô combination ( ùúô=3) performed the best, indi-
cating that the timestamp should not be too coarse or too fine. For
real-world cloud computing environments, machine types, monitor
performance and delays in log collection pipeline could lead to
second-level fluctuations in time intervals between logs. Therefore,
second-level timestamps may be too fine for robust performance.
B.2 Effect of Down-Sampling Ratio
Down-sampling of the majority class is necessary, due to the ex-
treme imbalance between positive and negative samples. Table 8
compares the model performance under different down-sampling
ratios. Both BERT and TAAT performed well when the ratio be-
tween the negative samples and the positive samples was higher
than 10 : 1 , and 10 : 1 was the best ratio in our experiments.
Since the distribution of log data often changes due to server
version upgrades or software updates, we retrain the model offlinewith the latest data every quarter. In each model upgrade, we iden-
tify the optimal downsampling ratio for the new distribution while
also considering the training cost. Generally, 10 : 1 achieved good
performance.
B.3 Effect of Exception Sequence Length
We studied the effectiveness of input exception sequence length.
TAAT used truncation/padding to align all sequences to a fixed
length before embedding. As shown in Table 8, the performance
of TAAT improved with the increase of input sequence length, but
converged at length 200. To trade-off between computational cost
and performance, we truncated the sequences at length 200.
Table 8: Parameter sensitivity analysis results (%).
Parameter Value Precision Recall F1
Timestamp
FormatSecond 48.43 46.84 47.62
Minute 47.62 48.90 48.25
Hour 52.10 47.64 49.77
Day 51.34 48.48 49.87
Day+Hour 52.91 50.51 51.68
Day+Hour+Minute* 54.05 50.59 52.26
Day+Hour+Minute+Second 52.42 51.14 51.77
Down-sample
Ratio1 5.61 66.18 10.34
3 14.42 61.25 23.34
5 25.79 57.37 35.58
10* 54.05 50.59 52.26
15 56.95 45.75 50.74
20 64.55 40.19 49.53
Sequence
Length50 46.19 40.53 36.11
100 53.26 47.38 48.78
200 54.05 50.59 52.26
300 54.07 50.57 52.27
500 53.63 51.04 52.29
4917