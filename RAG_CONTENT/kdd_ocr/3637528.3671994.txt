Distributional Network of Networks for Modeling
Data Heterogeneity
Jun Wu
University of Illinois at
Urbana-Champaign
Champaign, USA
junwu3@illinois.eduJingrui He
University of Illinois at
Urbana-Champaign
Champaign, USA
jingrui@illinois.eduHanghang Tong
University of Illinois at
Urbana-Champaign
Champaign, USA
htong@illinois.edu
Abstract
Heterogeneous data widely exists in various high-impact applica-
tions. Domain adaptation and out-of-distribution generalization
paradigms have been formulated to handle the data heterogeneity
across domains. However, most existing domain adaptation and
out-of-distribution generalization algorithms do not explicitly ex-
plain how the label information can be adaptively propagated from
the source domains to the target domain. Furthermore, little effort
has been devoted to theoretically understanding the convergence
of existing algorithms based on neural networks.
To address these problems, in this paper, we propose a generic
distributional network of networks ( TENON ) framework, where each
node of the main network represents an individual domain associ-
ated with a domain-specific network. In this case, the edges within
the main network indicate the domain similarity, and the edges
within each network indicate the sample similarity. The crucial idea
ofTENON is to characterize the within-domain label smoothness
and cross-domain parameter smoothness in a unified framework.
The convergence and optimality of TENON are theoretically ana-
lyzed. Furthermore, we show that based on the TENON framework,
domain adaptation and out-of-distribution generalization can be
naturally formulated as transductive and inductive distribution
learning problems, respectively. This motivates us to develop two
instantiated algorithms ( TENON -DAandTENON -OOD) of the proposed
TENON framework for domain adaptation and out-of-distribution
generalization. The effectiveness and efficiency of TENON -DAand
TENON-OOD are verified both theoretically and empirically.
CCS Concepts
â€¢Computing methodologies â†’Transfer learning.
Keywords
network of networks, data heterogeneity, domain adaptation, out-
of-distribution generalization
ACM Reference Format:
Jun Wu, Jingrui He, and Hanghang Tong. 2024. Distributional Network of
Networks for Modeling Data Heterogeneity. In Proceedings of the 30th ACM
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671994
Domain adaptation 
(Transductive distribution learning)
Out-of-distribution generalization 
(Inductive distribution learning)
(b) Network of networksDomain adaptation 
Out-of-distribution generalization 
(a) Heterogeneous domainsSource â„™ğŸğ’”Source â„™ğŸğ’”
Source â„™ğŸ‘ğ’”Source â„™ğŸ’ğ’”Target â„™ğ’•
Target â„™ğŸğ’•
(unseen)
Target â„™ğŸğ’•
(unseen)Source â„™ğŸ‘ğ’”Source â„™ğŸ’ğ’”
Source â„™ğŸğ’”Source â„™ğŸğ’”
Figure 1: Illustration of the network of networks on handing
heterogeneous domains (semantic classification on Amazon
products [ 8] is used where green indicates negative review
and orange indicates positive review). (a) Domain adapta-
tion and out-of-distribution (OOD) generalization involve
different domains. Target domains are unseen during train-
ing for out-of-distribution generalization. (b) In the network
of networks, each node of the main network represents one
domain, and it is formed by a network over domain-specific
samples. For OOD generalization, the dotted lines indicate
that the edges between source and target domains are acces-
sible only during the testing phase.
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671994
1 Introduction
Modern machine learning algorithms have demonstrated remark-
able success across a wide range of high-impact applications, such
as sentiment analysis [ 57], news tagging classification [ 31], etc.
One common assumption behind these algorithms is that the train-
ing and test samples are independently and identically distributed
(IID). However, this IID assumption is often violated in real scenar-
ios where the samples are collected from heterogeneous domains
under distribution shift [ 48], e.g., Amazon review collected from
different products [ 8], news headlines collected from different time
stamps [ 53]. Two learning paradigms have been developed to ad-
dress the challenge of data heterogeneity across domains: domain
adaptation [ 4,57] and out-of-distribution generalization [ 6,34].
3379
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
As shown in Figure 1(a), domain adaptation1aims at learning a
prediction function on a target domain with only unlabeled train-
ing samples, by exploiting knowledge from source domains. In
contrast, out-of-distribution generalization optimizes a domain-
agnostic model from source domains such that this model can be
directly applied to any relevant unseen target domains. Different
from domain adaptation, target domains are unseen during training
for out-of-distribution generalization.
Most existing domain adaptation and out-of-distribution gen-
eralization algorithms [ 2,28,47,57] build a single model to learn
the domain-invariant representation from different domains. The
invariant representation learned by a domain-agnostic model can
be explained as the common knowledge shared by all domains.
Nevertheless, it is a strong assumption that all domains share the
same model parameters. This is because this assumption underes-
timates the domain-specific characteristics encoding class separa-
bility. Though recent works [ 7,40,51] propose to learn both with-
domain specificity and cross-domain commonality, disentangling
domain-invariant and domain-specific representations is a nontriv-
ial task. This is because it is challenging to accurately differentiate
the domain-invariant representation from the domain-specific rep-
resentation within samples. The aforementioned frameworks might
suffer from the following limitations. First, the connection between
the domain relationship and the model (parameters) similarity is
under-explored, e.g., similar domains may share similar model pa-
rameters [ 24,49]. Second, it is not explained how the label informa-
tion can be adaptively propagated from the source domain to the
target domain. Third, little effort has been devoted to understanding
the model convergence of previous algorithms [ 2,45,47,57] based
on deep neural networks.
To this end, in this paper, we propose a generic distributional
network of networks ( TENON ) framework, which allows each do-
main to learn domain-specific model parameters. It is motivated
by recent observation [ 6,50] that both domain adaptation and out-
of-distribution generalization can be explained as follows. Given a
meta-distribution P, the data distributions P1,Â·Â·Â·,Pğ¾of different
domains can be considered as IID realizations of P, and the samples
{ğ‘¥ğ‘˜
ğ‘–,ğ‘¦ğ‘˜
ğ‘–}ğ‘›ğ‘˜
ğ‘–=1within domain ğ‘˜are IID realizations from Pğ‘˜. Having
this in mind, TENON reformulates the heterogeneous domains as a
network of networks [ 36], which encodes both high-level domain
relationships and low-level sample relationships. As shown in Fig-
ure 1(b), the main network characterizes the relationship among
different domains, where each node (blue circles) is a domain, and
the edges (solid or dotted blue lines) imply domain similarity. Each
domain is further represented by a domain-specific network (e.g., a
network within each blue circle), where each node (colored prod-
ucts) is a sample, and the edges (black lines) imply sample similarity.
The intuition behind TENON is that (i) domains share similar model
parameters [ 49,56] if they have similar data distributions, and (ii)
samples tend to have similar class labels if they are similar in the
input space [ 58,60]. To this end, the proposed TENON framework
is composed of both within-domain label smoothness and cross-
domain parameter smoothness regularizations. We theoretically
1It is also termed as "multi-source domain adaptation" to indicate the existence of
multiple source domains in previous works [ 39,57]. In this paper, we use the generic
term "domain adaptation" by assuming that at least one source domain is available.show that the convergence and optimality of TENON can be guar-
anteed when using overparameterized neural networks [ 20,25] to
instantiate the learning functions of TENON.
More specifically, Figure 1(b) shows that domain adaptation [ 57]
and out-of-distribution generalization [ 6,34] can be naturally con-
sidered as transductive and inductive distribution learning prob-
lems, respectively. That is, domain adaptation can build a network of
networks using source and target domains as a pre-processing step.
Here the sample similarity within each domain-specific network
and domain similarity within the main network can be empirically
estimated using input samples. Then using the constructed network
of networks, we propose an instantiated algorithm ( TENON -DA) of
TENON to propagate the knowledge from labeled source domains
to the unlabeled target domain for domain adaptation. In contrast,
out-of-distribution generalization can only access source (training)
domains, and thus we build a network of networks over source
domains during training. During the testing phase, target (testing)
domains will be added to the main network as the new nodes. As a
result, out-of-distribution generalization is formulated as an induc-
tive learning [ 17] problem w.r.t. the network of networks. To solve
this problem, we propose another instantiated algorithm ( TENON -
OOD) ofTENON to generalize the relevant knowledge from source
(training) domains to target (testing) domains. The effectiveness
and efficiency of TENON -DAandTENON -OODare demonstrated in a
variety of data mining tasks. The major contributions of this paper
are summarized as follows.
â€¢Framework: We propose a generic distributional network
of networks ( TENON ) framework for modeling data hetero-
geneity across domains. Notably, TENON provides a unified
viewpoint of domain adaptation and out-of-distribution gen-
eralization. Furthermore, the convergence and optimality of
TENON are theoretically analyzed.
â€¢Algorithms: We provide two instantiated algorithms (i.e.,
TENON-DA andTENON-OOD ) ofTENON for domain adaptation
and out-of-distribution generalization. It is revealed that both
algorithms inherit the convergence properties of the TENON
framework. Besides, in the context of domain adaptation, we
show that TENON-DA minimizes the error upper bound of the
target domain.
â€¢Experiments: Extensive experiments on various data sets
demonstrate the effectiveness and efficiency of the proposed
algorithms for both domain adaptation and out-of-distribution
generalization.
The rest of the paper is organized as follows. Section 2 summa-
rizes the related work and Section 3 provides the problem settings.
In Section 4, we propose a novel distributional network of networks
(TENON ) framework, followed by the instantiated algorithms for do-
main adaptation and out-of-distribution generalization in Section 5.
Section 6 shows the experimental results, and finally, we conclude
the paper in Section 7.
2 Related Work
2.1 Domain Adaptation
Domain adaptation [ 4,35] studies the transfer of knowledge or
information from source domains to a relevant target domain. It is
theoretically shown [ 1,44,47,57] that the generalization error of
3380Distributional Network of Networks for Modeling
Data Heterogeneity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
a learning algorithm within the target domain can be bounded by
the source errors and domain discrepancy. This thus leads to the
domain adaptation algorithms [ 9,14,30,33,39,50,55] by empiri-
cally minimizing the prediction errors within source domains and
distribution discrepancy across domains. The most similar works to
ours include [ 5,52], where Xu et al . [52] build a domain graph to en-
code topological structures among different domains and Berthelot
et al. [5] unify the semi-supervised learning and domain adapta-
tion. However, our TENON framework is fundamentally different
from previous works in the following aspects. First, previous works
leverage a single model to learn domain-invariant representation,
whereas TENON enables the domain-specific models to character-
ize the domain relationship. Second, the global convergence and
optimality of TENON are analyzed theoretically. In contrast, little
theoretical analysis regarding the convergence of domain adapta-
tion algorithms is provided in previous works. Third, our TENON
framework can be applied to both domain adaptation and out-of-
distribution generalization, while previous works consider only the
domain adaptation settings.
2.2 Out-of-Distribution Generalization
Out-of-distribution (OOD) generalization aims at learning a domain-
agnostic model from an arbitrary number of training source do-
mains [ 6,21,34]. In recent years, various OOD generalization al-
gorithms have been proposed from the following aspects: domain-
invariant representation learning [ 2,28], meta regularization [ 3,27],
domain augmentation [ 46,59], gradient operation [ 42,45], etc.
These algorithms directly apply the learned model to the new test-
ing domains. Compared to previous works, the proposed TENON
framework focuses on explicitly propagating model parameters
from training to testing domains based on the distribution simi-
larity among domains. This is in sharp contrast to previous works
which learn a commonly shared model among all domains.
3 Problem Definitions
We letXandYbe the input space and output space, respec-
tively. Suppose there are ğ¾different domains drawn from a meta-
distribution P, i.e.,P1,Â·Â·Â·,Pğ¾âˆ¼Pwhere Pğ‘˜denotes the data
distribution2of theğ‘˜thdomain overXÃ—Y . Each domain is asso-
ciated with a model ğ‘“(Â·;ğœƒğ‘˜):Xâ†’Y parameterized by ğœƒğ‘˜. There
areğ‘›ğ‘˜labeled or unlabeled samples in domain ğ‘˜, whereğ‘¥ğ‘˜
ğ‘–âˆˆXis
the input sample and ğ‘¦ğ‘˜
ğ‘–is the output label if available. In addition,
we let Idenote the identity matrix, ||Â·||ğ‘and||Â·||ğ¹denoteğ¿ğ‘norm
and Frobenius norm, respectively.
Following [ 4], we focus on the problem of learning from different
domains, where data heterogeneity exists among domains. Specif-
ically, in this paper, we focus on two research problems: domain
adaptation [ 4,57] and out-of-distribution generalization [ 6,21].
Both research problems involve modeling the data heterogeneity
across domains. Their goal is to learn a prediction function on
the target domain without label information, by leveraging latent
knowledge from relevant source domains.
2In this paper, we will use Pğ‘˜to denote both the data distribution of domain ğ‘˜and
the domainğ‘˜itself.Problem Definition 1 (Domain Adaptation). Given a set of
source domains{Pğ‘˜}ğ¾âˆ’1
ğ‘˜=1each with labeled samples {ğ‘¥ğ‘˜
ğ‘–,ğ‘¦ğ‘˜
ğ‘–}ğ‘›ğ‘˜
ğ‘–=1, and
a target domain Pğ¾with only unlabeled samples {ğ‘¥ğ¾
ğ‘–}ğ‘›ğ¾
ğ‘–=1, domain
adaptation aims to learn a prediction function on the target domain
using knowledge from source domains.
Problem Definition 2 (Out-of-Distribution Generaliza-
tion). Given a set of source domains {Pğ‘˜}ğ¾
ğ‘˜=1each with samples
{ğ‘¥ğ‘˜
ğ‘–,ğ‘¦ğ‘˜
ğ‘–}ğ‘›ğ‘˜
ğ‘–=1, out-of-distribution generalization aims to learn a predic-
tion function from source domains such that this prediction function
can be directly applied to unseen target domains.
As illustrated in Figure 1, a group of distributions (or domains)
{Pğ‘˜}ğ¾
ğ‘˜=1over a meta-distribution Pcan be formulated as a network
of networks [ 36], where each node of the main network represents
a domain and each network is formed by domain-specific samples.
This motivates us to rethink the modeling of data heterogeneity
by capturing both sample similarity within domains and distribu-
tion similarity across domains. First, in each domain, two samples
tend to have similar output values if they are similar in the input
space [ 19,58,60]. Second, given a learning algorithm ğ‘“(Â·), two
domains would be close in the parameter space if they are distribu-
tionally similar [49, 56].
4 Proposed Framework
In this section, we propose a simple and generic distributional net-
work of networks ( TENON ) framework for modeling heterogeneous
data from multiple domains.
4.1 Distributional Network of Networks
It is shown [ 50] that knowledge transferability can be positively
correlated with the distribution similarity across domains. This
motivates us to model the heterogeneous domains by capturing the
domain relationship in the parameter space (shown in Figure 2). To
this end, we propose a simple yet generic distributional network of
networks ( TENON ) framework with the following objective function.
min
{ğœƒğ‘˜}ğ¾
ğ‘˜=1ğœ†ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–=1ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)âˆ’ğ‘¦ğ‘˜
ğ‘–2
2
|                      {z                      }
Label consistency within domain
+1
2ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–,ğ‘—=1ğ‘ ğ‘˜
ğ‘–ğ‘—ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘–ğ‘–âˆ’ğ‘“(ğ‘¥ğ‘˜
ğ‘—;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘—ğ‘—2
2|                                       {z                                       }
Label smoothness within domain
+1
2ğ¾âˆ‘ï¸
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒğ‘˜âˆšï¸
ğ‘€ğ‘˜ğ‘˜âˆ’ğœƒğ‘˜â€²âˆšï¸
ğ‘€ğ‘˜â€²ğ‘˜â€²2
ğ¹|                                    {z                                    }
Parameter smoothness across domains(1)
whereğ‘ ğ‘˜
ğ‘–ğ‘—indicates the sample similarity between ğ‘¥ğ‘˜
ğ‘–andğ‘¥ğ‘˜
ğ‘—within
theğ‘˜-th domain, and ğ‘‘ğ‘˜ğ‘˜â€²denotes the domain similarity between
theğ‘˜-th domain and the ğ‘˜â€²-th domain. Here ğ·ğ‘˜
ğ‘–ğ‘–=Ãğ‘›ğ‘˜
ğ‘—=1ğ‘ ğ‘˜
ğ‘–ğ‘—and
ğ‘€ğ‘˜ğ‘˜=Ãğ¾
ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€².ğœƒğ‘˜denotes the model parameters within the
ğ‘˜-th domain. ğœ†>0is a hyper-parameter to balance different terms.
3381KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
ğœƒ!
ğœƒ"
ğœƒ#ğœƒ$Parameter space
Domain 2Domain 1Domain 3
Domain 4
Parameter propagation Label propagationParameters True/Predicted labelsLabel 
space
Label 
spaceLabel 
space
Label 
space
Figure 2: Illustration of TENON in information propagation.
Label information is propagated in the labeling space within
each domain, while parameter information is propagated in
the parameter space across domains.
Following [ 19,58], the sample similarity ğ‘ ğ‘˜
ğ‘–ğ‘—can be empirically
estimated as follows.
ğ‘ ğ‘˜
ğ‘–ğ‘—=exp
âˆ’ğœÂ·ğ‘¥ğ‘˜
ğ‘–âˆ’ğ‘¥ğ‘˜
ğ‘—1
whereğœâˆˆRis a hyper-parameter. In addition, a variety of do-
main discrepancy measures have been proposed to model the het-
erogeneous domains, e.g., HÎ”H-divergence [ 4], Maximum Mean
Discrepancy [ 15,30], Wasserstein distance [ 44],ğ‘“-divergence [ 1],
etc. It is flexible in defining ğ‘‘ğ‘˜ğ‘˜â€²in Eq. (1) based on existing do-
main discrepancy measures. In this paper, under the covariate shift
assumption [ 38] (i.e.,P(ğ‘¦|ğ‘¥)is shared for all domains), we use Maxi-
mum Mean Discrepancy (MMD) [ 15] to define the domain similarity
ğ‘‘ğ‘˜ğ‘˜â€²as follows.
MMD(ğ‘˜,ğ‘˜â€²)=1
ğ‘›ğ‘˜ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–=1ğœ™(ğ‘¥ğ‘˜
ğ‘–)âˆ’1
ğ‘›ğ‘˜â€²ğ‘›ğ‘˜â€²âˆ‘ï¸
ğ‘—=1ğœ™(ğ‘¥ğ‘˜â€²
ğ‘—)2
HK
ğ‘‘ğ‘˜ğ‘˜â€²=exp âˆ’ğœÂ·MMD(ğ‘˜,ğ‘˜â€²)
whereğœ™(Â·):Xâ†’HKis a kernel mapping from an input space X
to a reproducing kernel Hilbert space (RKHS) HK.
The intuition behind Eq. (1) is explained as follows. The first term
captures the consistency of models {ğ‘“(Â·;ğœƒğ‘˜)}ğ¾
ğ‘˜=1with the prior la-
bel information. The second term measures the label smoothness
within each domain. It implies that input samples have similar pre-
diction values if they are similar in the input space. Furthermore,
the third term measures the cross-domain model smoothness in the
parameter space. Notably, graph-based parameter smoothness regu-
larization [ 29,56] has been studied in multi-task learning. However,
compared to previous works, our framework of Eq. (1) explicitly re-
veals the connection between the domain distribution discrepancy
and the model (parameters) similarity, i.e., domains have similar
model parameters if they are distributionally similar. Furthermore,
by incorporating the within-domain label smoothness regulariza-
tion (i.e., the second term of Eq. (1)), TENON allows propagating
label information from labeled source samples to unlabeled target
samples, whereas previous works [ 29,56] collaboratively update
the model parameters over the labeled samples from all domains.As shown in Figure 2, the label information encoded by a domain-
specific model is propagated within each domain, while the model
information is propagated across domains in the parameter space.
We show in Subsection 4.3 that in the special case where ğ‘‘ğ‘˜ğ‘˜â€²=0
for all domains ğ‘˜,ğ‘˜â€², the objective of TENON in Eq. (1) exactly re-
covers the label propagation [ 58,60] in every domain. On top of
label propagation, the parameter propagation of TENON enables han-
dling data heterogeneity when samples are collected from multiple
domains [33, 49, 57].
4.2 Convergence Analysis
The convergence and optimality of TENON can be analyzed by con-
sidering different instantiations of learning models {ğ‘“(Â·;ğœƒğ‘˜)}ğ¾
ğ‘˜=1.
In the following, we start with the simple linear regression func-
tions, i.e.,ğ‘“(ğ‘¥;ğœƒğ‘˜)=ğœƒğ‘‡
ğ‘˜ğ‘¥for allğ‘˜âˆˆ{1,2,Â·Â·Â·,ğ¾}. The following
lemma shows the global convergence and optimality of the TENON
framework.
Lemma 3. Given linear models ğ‘“(ğ‘¥;ğœƒğ‘˜)=ğœƒğ‘‡
ğ‘˜ğ‘¥forğ‘˜âˆˆ{1,Â·Â·Â·,ğ¾},
the objective of Eq. (1) can be minimized at
Î˜âˆ—=ğœ†Ë†X
Ë†A+ğœ†I
Ë†Xğ‘‡Ë†X+
Ë†Xğ‘‡Ë†Xâˆ’1Ë†Xğ‘‡Ë†BË†Xâˆ’1
y
where Î˜=[ğœƒğ‘‡
1,Â·Â·Â·,ğœƒğ‘‡
ğ¾]ğ‘‡,Xğ‘˜=[ğ‘¥ğ‘˜
1,ğ‘¥ğ‘˜
2,Â·Â·Â·,ğ‘¥ğ‘˜ğ‘›ğ‘˜],y=[ğ‘¦1
1,Â·Â·Â·,ğ‘¦1ğ‘›1,
Â·Â·Â·,ğ‘¦ğ¾
1,Â·Â·Â·,ğ‘¦ğ¾ğ‘›ğ¾]ğ‘‡and
Ë†X=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°X10Â·Â·Â· 0
0 X 2Â·Â·Â· 0
............
0 0Â·Â·Â· Xğ¾ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£», Ë†A=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Â¯A10Â·Â·Â· 0
0 Â¯A2Â·Â·Â· 0
............
0 0Â·Â·Â· Â¯Ağ¾ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
Ë†B=Iğ¾ğ‘‘ğ‘–ğ‘›Ã—ğ¾ğ‘‘ğ‘–ğ‘›âˆ’BâŠ—Iğ‘‘ğ‘–ğ‘›Ã—ğ‘‘ğ‘–ğ‘›
where Ağ‘˜=(Dğ‘˜)âˆ’1/2Sğ‘˜(Dğ‘˜)âˆ’1/2is the normalized sample similar-
ity matrix of domain ğ‘˜with Â¯Ağ‘˜=Iâˆ’Ağ‘˜, and B=Mâˆ’1/2DMâˆ’1/2is
the normalized domain similarity matrix.3âŠ—denotes the Kronecker
product of two matrices. ğ‘‘ğ‘–ğ‘›is the dimensionality of input samples.
Next, we instantiate the learning models {ğ‘“(Â·;ğœƒğ‘˜)}ğ¾
ğ‘˜=1with over-
parameterized neural networks [ 20,25]. This allows us to reveal
the convergence of TENON in Eq. (1) with commonly used neural
network architectures4. For notation simplicity, we will use ğ‘“(Â·;Î˜)
to denote the overall learning function with ğ‘“(ğ‘¥ğ‘˜;Î˜)=ğ‘“(ğ‘¥ğ‘˜;ğœƒğ‘˜)
for any sample ğ‘¥ğ‘˜from domain ğ‘˜. It is observed [ 25] that neu-
ral network ğ‘“(Â·;Î˜)can be approximated by its linearized version
ğ‘“lin(Â·;Î˜), i.e., supğ‘¡â‰¥0ğ‘“ğ‘¡(ğ‘¥;Î˜)âˆ’ğ‘“lin
ğ‘¡(ğ‘¥;Î˜)=O(â„âˆ’1
2)whereâ„is
the width of neural networks.5ğ‘“ğ‘¡(Â·;Î˜)denotes the model at time
stepğ‘¡, andğ‘“lin(Â·;Î˜)is given by the first order Taylor expansion of
ğ‘“(Â·;Î˜):ğ‘“lin
ğ‘¡(ğ‘¥;Î˜)=ğ‘“0(ğ‘¥;Î˜)+âˆ‡ğ‘“0(ğ‘¥;Î˜)(Î˜ğ‘¡âˆ’Î˜0)Inspired by this
observation, we generalize the results of Lemma 3 by instantiating
{ğ‘“(Â·;ğœƒğ‘˜)}ğ¾
ğ‘˜=1with neural networks. The following theorem shows
3Sğ‘˜is sample similarity matrix of domain ğ‘˜with the entry[Sğ‘˜]ğ‘–ğ‘—=ğ‘ ğ‘˜
ğ‘–ğ‘—, and Dğ‘˜is
a diagnal matrix with the entry [Dğ‘˜]ğ‘–ğ‘–=ğ·ğ‘˜
ğ‘–ğ‘–.Dis domain similarity matrix with
the entry[D]ğ‘˜ğ‘˜â€²=ğ‘‘ğ‘˜ğ‘˜â€², and Mis a diagnal matrix with the entry [M]ğ‘˜ğ‘˜=ğ‘€ğ‘˜ğ‘˜.
4Following [ 25],ğœƒğ‘˜denotes the vectorized parameters of the neural network model
within domain ğ‘˜here.
5In this case, "width" can be the number of neurons in a fully-connected layer or the
number of channels in a convolutional layer.
3382Distributional Network of Networks for Modeling
Data Heterogeneity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
the global convergence of TENON under gradient descent when the
layer width of{ğ‘“(Â·;ğœƒğ‘˜)}ğ¾
ğ‘˜=1goes to infinity.
Theorem 4 (Convergence and Optimality of TENON ).LetX
denote all training samples. In the limit of layer width, the model
parameters Î˜in the objective of Eq. (1) converges to
limğ‘¡â†’âˆÎ˜ğ‘¡=âˆ’âˆ‡Î˜ğ‘“0(X)ğ‘‡Kâˆ’1
NTKÎ“âˆ’1(â„¦âˆ’ğœ†y)+Î˜0
whereğ‘¡is the training time step, Î˜0denotes the initialized parameters,
andğ‘“0(X)=vec(ğ‘“0(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)|ğ‘–âˆˆ{1,2,Â·Â·Â·,ğ‘›ğ‘˜},ğ‘˜âˆˆ{1,2,Â·Â·Â·,ğ¾})is
model output with initialized parameters. Moreover, the prediction
functionğ‘“(Â·;ğœƒğ‘˜)of Eq. (1) for any testing sample ğ‘¥ğ‘˜within domain
ğ‘˜converges to
limğ‘¡â†’âˆğ‘“ğ‘¡(ğ‘¥ğ‘˜;ğœƒğ‘˜)=ğœ†KNTK(ğ‘¥ğ‘˜,X)Kâˆ’1
NTKÎ“âˆ’1y
+ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,X)Kâˆ’1
NTKÎ“âˆ’1â„¦
where
Î“=Ë†A+ğœ†I+Kâˆ’1
NTKâˆ‡Î˜ğ‘“0(X)Ë†Bâˆ‡Î˜ğ‘“0(X)ğ‘‡Kâˆ’1
NTK
â„¦=Kâˆ’1
NTKâˆ‡Î˜ğ‘“0(X)Ë†BÎ˜0+(Ë†A+ğœ†I)ğ‘“0(X)
KNTK(ğ‘¥ğ‘˜,X)=[0,Â·Â·Â·,0, ğœ”ğ‘˜
1,Â·Â·Â·,ğœ”ğ‘˜
ğ‘›ğ‘˜|         {z         }
Within domain ğ‘˜,0,Â·Â·Â·,0]
andKNTK=diag(K11,K22,Â·Â·Â·,Kğ¾ğ¾).Kğ‘˜ğ‘˜is a neural tangent ker-
nel [20] matrix within domain ğ‘˜, i.e., its entry is given by [Kğ‘˜ğ‘˜]ğ‘–ğ‘—=D
âˆ‡ğœƒğ‘˜ğ‘“0(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜),âˆ‡ğœƒğ‘˜ğ‘“0(ğ‘¥ğ‘˜
ğ‘—;ğœƒğ‘˜)E
.ğœ”ğ‘˜
ğ‘–=D
âˆ‡ğœƒğ‘˜ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜),âˆ‡ğœƒğ‘˜ğ‘“0(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)E
.
4.3 Discussion
In this section, we provide a more intuitive explanation regarding
how the proposed TENON framework enables within-domain label
propagation and cross-domain parameter propagation, respectively.
Corollary 5 (Individual Label Propagation). In the special
case whereğ‘‘ğ‘˜ğ‘˜â€²=0(ğ‘˜â‰ ğ‘˜â€²), with the same conditions as Theorem 4,
for anyğ‘˜âˆˆ{1,Â·Â·Â·,ğ¾}, the predicted values of ğ‘“(Â·;ğœƒğ‘˜)in Eq. (1) over
the training samples Xğ‘˜=[ğ‘¥ğ‘˜
1,Â·Â·Â·,ğ‘¥ğ‘˜ğ‘›ğ‘˜]in domainğ‘˜converge to
limğ‘¡â†’âˆğ‘“ğ‘¡(Xğ‘˜;ğœƒğ‘˜)=(1âˆ’ğ›¼)(Iâˆ’ğ›¼Ağ‘˜)âˆ’1yğ‘˜ (2)
whereğ‘“ğ‘¡(Xğ‘˜;ğœƒğ‘˜)=[ğ‘“ğ‘¡(ğ‘¥ğ‘˜
1;ğœƒğ‘˜),ğ‘“ğ‘¡(ğ‘¥ğ‘˜
2;ğœƒğ‘˜),Â·Â·Â·,ğ‘“ğ‘¡(ğ‘¥ğ‘˜ğ‘›ğ‘˜;ğœƒğ‘˜)]ğ‘‡,yğ‘˜=
[ğ‘¦ğ‘˜
1,ğ‘¦ğ‘˜
2,Â·Â·Â·,ğ‘¦ğ‘˜ğ‘›ğ‘˜]ğ‘‡, andğ›¼=1
ğœ†+1. Furthermore, for any testing sample
ğ‘¥ğ‘˜, it holds
limğ‘¡â†’âˆğ‘“ğ‘¡(ğ‘¥ğ‘˜;ğœƒğ‘˜)=(1âˆ’ğ›¼)KNTK(ğ‘¥ğ‘˜,Xğ‘˜)Kâˆ’1
ğ‘˜ğ‘˜(Iâˆ’ğ›¼Ağ‘˜)âˆ’1yğ‘˜
+ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,Xğ‘˜)Kâˆ’1
ğ‘˜ğ‘˜ğ‘“0(Xğ‘˜)(3)
It can be seen from Eq. (2) in Corollary 5 that when all domains
are irrelevant (i.e., ğ‘‘ğ‘˜ğ‘˜â€²=0for anyğ‘˜â‰ ğ‘˜â€²), the objective of TENON
is equivalent to standard label propagation [ 19,58,60] on each
individual domain and no knowledge is shared across domains. Fur-
thermore, previous label propagation approaches [ 58,60] focus on
transductive semi-supervised learning, where labels are inferred for
a set of unlabeled training samples (shown in Eq. (2)), whereas Corol-
lary 5 provides a feasible solution for inductive semi-supervised
learning, where the labels can be inferred for new unseen testing
samples (shown in Eq. (3)).Algorithm 1 TENON-DA
Input:(ğ¾âˆ’1)source domains{Pğ‘˜}ğ¾âˆ’1
ğ‘˜=1, a target domain Pğ¾;
Output: Predicted output values of target samples.
1:â€”â€”â€”â€”â€”â€”â€” Training Stage (Pre-computing) â€”â€”â€”â€”â€”â€”â€”
2:Calculate all sample similarity ğ‘ ğ‘˜
ğ‘–ğ‘—and domain similarity ğ‘‘ğ‘˜ğ‘˜â€²;
3:forğ‘˜=1,Â·Â·Â·,ğ¾do
4: Calculate block neural tangent kernel Kğ‘˜ğ‘˜;
5: Calculate inverse matrix Kâˆ’1
ğ‘˜ğ‘˜;
6:forğ‘˜â€²=ğ‘˜+1,Â·Â·Â·,ğ¾do
7: Calculate block neural tangent kernel Kğ‘˜ğ‘˜â€²;
8:end for
9:end for
10:Calculate Î“KNTK;
11:Calculate yâˆ—=ğœ†Kâˆ’1
NTKÎ“âˆ’1y=ğœ†(Î“KNTK)âˆ’1y;
12:Obtain target propagated labels yâˆ—
ğ¾=[yâˆ—]âˆ’ğ‘›ğ¾:;
13:â€”â€”â€”â€”â€”â€”â€” Inference Stage â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
14:fortesting sample ğ‘¥test
ğ¾from target domain ğ¾do
15: Calculate neural tangent kernel KKK(ğ‘¥test
ğ¾,Xğ¾);
16: Calculateğ‘¦test
ğ¾=KKK(ğ‘¥test
ğ¾,Xğ¾)yâˆ—
ğ¾;
17:end for
Corollary 6 (Global Parameter Propagation). In the special
case whereğ‘ ğ‘˜
ğ‘–ğ‘—=0(ğ‘–â‰ ğ‘—,ğ‘˜=1,Â·Â·Â·,ğ¾), with the same conditions
as Theorem 4, for any ğ‘˜âˆˆ{1,Â·Â·Â·,ğ¾}, the model parameters ğœƒğ‘˜of
ğ‘“(Â·;ğœƒğ‘˜)in Eq. (1) is updated under gradient descent as follows.
ğœƒğ‘˜(ğ‘¡+1)=
(1âˆ’ğœ‚)Iâˆ’ğœ†âˆ‡ğ‘“(Xğ‘˜)ğ‘‡âˆ‡ğ‘“(Xğ‘˜)
ğœƒğ‘˜(ğ‘¡)
+ğœ‚ğ¾âˆ‘ï¸
ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²âˆšï¸
ğ‘€ğ‘˜ğ‘˜ğ‘€ğ‘˜â€²ğ‘˜â€²ğœƒğ‘˜â€²(ğ‘¡)+ğœ‚ğœ†âˆ‡ğ‘“(Xğ‘˜)ğ‘‡yğ‘˜
whereğœ‚is the learning rate and ğœƒğ‘˜(ğ‘¡)denotes the model parameters
ğœƒğ‘˜at time step ğ‘¡.
Corollary 6 reveals that if we do not consider the sample similar-
ity, i.e.,ğ‘ ğ‘˜
ğ‘–ğ‘—=0(ğ‘–â‰ ğ‘—,ğ‘˜=1,Â·Â·Â·,ğ¾), the model parameters ğœƒğ‘˜of the
domainğ‘˜would recursively aggregate knowledge from all other
domains. More specifically, if two domains have similar data distri-
butions, i.e., ğ‘‘ğ‘˜ğ‘˜â€²is large, it is more likely to propagate parameter
knowledge between these two domains. This observation is also
consistent with previous works [24, 49].
5 Proposed Algorithms
In this section, we provide two instantiated algorithms of TENON for
domain adaptation ( TENON -DA) and out-of-distribution generaliza-
tion ( TENON -OOD). The crucial idea is to formulate domain adaptation
and out-of-distribution generalization as transductive distribution
learning and inductive distribution learning w.r.t. network of net-
works [36], respectively.
5.1 Transductive Distribution Learning
We formulate domain adaptation [ 47] as a transductive distribution
learning problem. As shown in Figure 1(b), each domain (source or
target domain) is formulated as a node in the main network, and
samples within each domain form a domain-specific network. Thus,
domain adaptation aims to propagate the label information (1) from
3383KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
source domains to the target domain (domain-level propagation)
and (2) from labeled samples to unlabeled samples (sample-level
propagation). To this end, we instantiate the proposed TENON frame-
work (denoted as TENON-DA) for domain adaptation below.
Givenğ¾âˆ’1source domains{Pğ‘˜}ğ¾âˆ’1
ğ‘˜=1each with labeled samples
{ğ‘¥ğ‘˜
ğ‘–,ğ‘¦ğ‘˜
ğ‘–}ğ‘›ğ‘˜
ğ‘–=1, and a target domain Pğ¾with only unlabeled samples
{ğ‘¥ğ¾
ğ‘–}ğ‘›ğ¾
ğ‘–=1, the objective function of TENON -DAis directly given by Eq.
(1). Here the class label ğ‘¦ğ‘˜
ğ‘–(ğ‘˜=1,Â·Â·Â·,ğ¾âˆ’1) of source training
sampleğ‘¥ğ‘˜
ğ‘–is represented as a one-hot vector, and the class label ğ‘¦ğ¾
ğ‘–
of unlabeled target training sample ğ‘¥ğ¾
ğ‘–is initialized as a zero vector.
Following Theorem 4, we can obtain the closed-form solution of
TENON -DAas follows. Suppose ğ‘“0(Â·;Î˜)=0,Î˜0=0, the predicted
class labels of target training samples are given by
yâˆ—
ğ¾=[yâˆ—]âˆ’ğ‘›ğ¾: where yâˆ—=ğœ†Kâˆ’1
NTKÎ“âˆ’1y
where[yâˆ—]âˆ’ğ‘›ğ¾:denotes the last ğ‘›ğ¾rows of predicted output values
yâˆ—. Moreover, for any new target testing sample ğ‘¥test
ğ¾, the predicted
class label via TENON-DA is
ğ‘¦test
ğ¾=ğœ†KNTK(ğ‘¥test
ğ¾,X)Kâˆ’1
NTKÎ“âˆ’1y=KKK(ğ‘¥test
ğ¾,Xğ¾)yâˆ—
ğ¾(4)
where KKK(ğ‘¥test
ğ¾,Xğ¾)=[KNTK(ğ‘¥test
ğ¾,ğ‘¥1
ğ¾),Â·Â·Â·,KNTK(ğ‘¥test
ğ¾,ğ‘¥ğ¾ğ‘›ğ¾)].
We see that TENON -DAis a non-parametric domain adaptation ap-
proach. As shown in Algorithm 1, we can pre-compute the prop-
agated labels yâˆ—
ğ¾for unlabeled target training samples. Then, the
class label of any testing target sample is inferred using the propa-
gated labels yâˆ—
ğ¾and the neural tangent kernel vector KKK(ğ‘¥test
ğ¾,Xğ¾)
between this testing sample and the target training samples.
In the following, we theoretically analyze the generalization
bound of TENON-DA for domain adaptation.
Theorem 7 (Generalization of TENON -DA).Suppose that the
learning models are instantiated with infinitely wide neural networks,
given the hypothesis space H, for any hypothesis ğ‘“(Â·;ğœƒğ‘˜)âˆˆH and
anyğ›¿âˆˆ(0,1), with probability at least 1âˆ’ğ›¿, the expected error of
the target domain can be upper bounded by
Eğ‘¥âˆ¼Pğ¾hğ‘“(ğ‘¥;ğœƒğ¾)âˆ’ğ‘“ ğ‘¥;ğœƒâˆ—
ğ¾2
2i
â‰¤ğœ"
ğœ†ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–=1ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)âˆ’ğ‘¦ğ‘˜
ğ‘–2
2
+1
2ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–,ğ‘—=1ğ‘ ğ‘˜
ğ‘–ğ‘—ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘–ğ‘–âˆ’ğ‘“(ğ‘¥ğ‘˜
ğ‘—;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘—ğ‘—2
2
+1
2ğ¾âˆ‘ï¸
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒğ‘˜âˆšğ‘€ğ‘˜ğ‘˜âˆ’ğœƒğ‘˜â€²âˆšğ‘€ğ‘˜â€²ğ‘˜â€²2
2#
+1
ğ‘›ğ¾ğ¿Rğ¾âˆ‘ï¸
ğ‘˜=1Î©(Xğ‘˜,ğœƒâˆ—
ğ‘˜)+1
ğ‘›ğ¾ğ¿RÎ”(ğœƒâˆ—
1,Â·Â·Â·,ğœƒâˆ—
ğ¾)+Olog(1/ğ›¿)
ğ‘›ğ¾
where Î©(Xğ‘˜,ğœƒâˆ—
ğ‘˜)=Ãğ‘›ğ‘˜
ğ‘–,ğ‘—=1ğ‘ ğ‘˜
ğ‘–ğ‘—ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒâˆ—
ğ‘˜)âˆšï¸ƒ
ğ·ğ‘˜
ğ‘–ğ‘–âˆ’ğ‘“(ğ‘¥ğ‘˜
ğ‘—;ğœƒâˆ—
ğ‘˜)âˆšï¸ƒ
ğ·ğ‘˜
ğ‘—ğ‘—2
2denotes the la-
bel smoothness over ğœƒâˆ—
ğ‘˜=arg minğœƒâ€²EPğ‘˜[ğ‘“(ğ‘¥ğ‘˜;ğœƒâ€²),ğ‘¦ğ‘˜],Î”(ğœƒâˆ—
1,Â·Â·Â·,ğœƒâˆ—
ğ¾)=
Ãğ¾
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒâˆ—
ğ‘˜âˆšğ‘€ğ‘˜ğ‘˜âˆ’ğœƒâˆ—
ğ‘˜â€²âˆšğ‘€ğ‘˜â€²ğ‘˜â€²2
2andğœ=maxnğ‘ˆR
ğœ†ğ‘›ğ¾ğ¿R,2
ğ‘›ğ¾ğ¿Ro
.ğ¿R
andğ‘ˆRare constants depending on the maximum and minimum
eigenvalues of Lğ¾+Kâˆ’1
ğ¾ğ¾respectively, where Lğ‘˜is the symmetrically
normalized Laplacian matrix of the target domain.Algorithm 2 TENON-OOD
Input:ğ¾source (training) domains {Pğ‘˜}ğ¾
ğ‘˜=1;
Output: Predicted output values of target samples.
1:â€”â€”â€”â€”â€”â€”â€” Training Stage (Pre-computing) â€”â€”â€”â€”â€”â€”â€”
2:Calculate Î“KNTK (same procedures as Lines 2-10 in Alg. 1);
3:Calculate yâˆ—=ğœ†Kâˆ’1
NTKÎ“âˆ’1y=ğœ†(Î“KNTK)âˆ’1y;
4:â€”â€”â€”â€”â€”â€”â€” Inference Stage â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
5:forğ‘¥test
ğ¾+1from target (testing) domain ğ¾+1do
6: Calculate neural tangent kernel Î¦(ğ‘¥test
ğ¾+1,X);
7: Calculateğ‘¦test
ğ¾+1=ğœ†
ğ¾Î¦(ğ‘¥test
ğ¾+1,X)Kâˆ’1
NTKÎ“âˆ’1y;
8:end for
It can be seen from Theorem 7 that the generalization error of
TENON -DAon the target domain is determined by the following cru-
cial factors. One is the empirical prediction error given by TENON -DA
(see Eq. (1)) over source and target training samples. The other one
is the optimal label smoothness within each domain and the opti-
mal parameter smoothness across domains. We would like to point
out that previous works study the generalization performance of
domain adaptation using either domain discrepancy [ 1,55,57] or
label smoothness [ 35] across domains, by assuming that all do-
mains share the same hypothesis. The learned prediction function
in those works might lose domain-specific information, resulting
in sub-optimal performance on the target domain. Though some
recent works [ 40,51] propose to learn both domain-invariant and
domain-specific representations, their theoretical generalization
performance is unclear. Instead, in this paper, we leverage the sim-
ple distributional network of networks framework to model data
heterogeneity in domain adaptation with theoretical guarantees
(e.g., the first three terms of the upper bound in Theorem 7 result
in the optimization framework of Eq. (1) for domain adaptation).
5.2 Inductive Distribution Learning
We can formulate the out-of-distribution generalization [ 16,23] as
an inductive distribution learning problem. As illustrated in Fig-
ure 1(b), all source (training) domains can be used to construct
a network of networks. Since the target (testing) domains are
only available during the testing phase, they will be added to the
main network as new nodes after model training. Therefore, out-
of-distribution generalization can be considered as an inductive
distributional learning problem, given the formulated network of
networks. To solve this problem, we instantiate the proposed TENON
framework (denoted as TENON -OOD) with the following training and
inference stages (see Algorithm 2).
â€¢Training Stage: Given ğ¾source (training) domains {Pğ‘˜}ğ¾
ğ‘˜=1
each with labeled samples {ğ‘¥ğ‘˜
ğ‘–,ğ‘¦ğ‘˜
ğ‘–}ğ‘›ğ‘˜
ğ‘–=1, the objective function
ofTENON -OODduring training can be directly given by Eq.
(1). Thus, based on Theorem 4, we can obtain the closed-
form solution for model parameters {ğœƒğ‘˜}ğ¾
ğ‘˜=1over training
domains.
Î˜âˆ—=ğœ†âˆ‡Î˜ğ‘“0(X)ğ‘‡Kâˆ’1
NTKÎ“âˆ’1y
where Î˜âˆ—=[ğœƒâˆ—ğ‘‡
1,Â·Â·Â·,ğœƒâˆ—ğ‘‡
ğ¾]ğ‘‡. Hereğœƒâˆ—
ğ‘˜denotes the optimized
model parameters within domain ğ‘˜.
3384Distributional Network of Networks for Modeling
Data Heterogeneity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢Inference Stage: In the inference stage, we can learn the
model parameters ğœƒâˆ—
ğ¾+1for a new target (testing) domain
Pğ¾+1as follows. For standard out-of-distribution general-
ization, no prior knowledge regarding the target (testing)
domain is available before model inference. In this case,
we assume that the new target (testing) domain can be
considered as a new (domain) node for the previously de-
rived network of networks. The edge weight6between
this new node and previous nodes within the main net-
work is simply set as 1. Considering the objective func-
tionminğœƒğ¾+1Ãğ¾+1
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒğ‘˜âˆšğ‘€ğ‘˜ğ‘˜âˆ’ğœƒğ‘˜â€²âˆšğ‘€ğ‘˜â€²ğ‘˜â€²2
ğ¹, we obtain the
closed-form solution ğœƒâˆ—
ğ¾+1=1
ğ¾Ãğ¾
ğ‘˜=1ğœƒâˆ—
ğ‘˜, and thus the pre-
dicted class label of any testing sample ğ‘¥test
ğ¾+1is given by
ğ‘¦test
ğ¾+1=ğœ†
ğ¾Î¦(ğ‘¥test
ğ¾+1,X)Kâˆ’1
NTKÎ“âˆ’1y
where Î¦(ğ‘¥test
ğ¾+1,X)=[KNTK(ğ‘¥test
ğ¾+1,ğ‘¥1
1),Â·Â·Â·,KNTK(ğ‘¥test
ğ¾+1,ğ‘¥1ğ‘›1),
Â·Â·Â·,KNTK(ğ‘¥test
ğ¾+1,ğ‘¥1
ğ¾),Â·Â·Â·,KNTK(ğ‘¥test
ğ¾+1,ğ‘¥ğ¾ğ‘›ğ¾)]denotes the neu-
ral tangent kernel between ğ‘¥test
ğ¾+1and samples from training
domains.
5.3 More Discussion Regarding Algorithms 1&2
It can be seen that the term Î“in Algorithms 1&2 involves the
computationally expensive gradient terms âˆ‡Î˜ğ‘“0(ğ‘‹).
Î“=Ë†A+ğœ†I+Kâˆ’1
NTKâˆ‡Î˜ğ‘“0(ğ‘‹)Ë†Bâˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡Kâˆ’1
NTK
However, we have the following observations.
âˆ‡Î˜ğ‘“0(ğ‘‹)Ë†Bâˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡
=âˆ‡Î˜ğ‘“0(ğ‘‹) Iğ¾ğ‘‘ğ‘–ğ‘›Ã—ğ¾ğ‘‘ğ‘–ğ‘›âˆ’BâŠ—Iğ‘‘ğ‘–ğ‘›Ã—ğ‘‘ğ‘–ğ‘›âˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡
=KNTKâˆ’ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘‘11âˆšğ‘€11Â·ğ‘€11K11ğ‘‘12âˆšğ‘€11Â·ğ‘€22K12Â·Â·Â·ğ‘‘1ğ¾âˆšğ‘€11Â·ğ‘€ğ¾ğ¾K1ğ¾
ğ‘‘21âˆšğ‘€22Â·ğ‘€11K21ğ‘‘22âˆšğ‘€22Â·ğ‘€22K22Â·Â·Â·ğ‘‘2ğ¾âˆšğ‘€22Â·ğ‘€ğ¾ğ¾K2ğ¾
............
ğ‘‘ğ¾1âˆšğ‘€ğ¾ğ¾Â·ğ‘€11Kğ¾1ğ‘‘ğ¾2âˆšğ‘€ğ¾ğ¾Â·ğ‘€22Kğ¾2Â·Â·Â·ğ‘‘ğ¾ğ¾âˆšğ‘€ğ¾ğ¾Â·ğ‘€ğ¾ğ¾Kğ¾ğ¾ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
It shows that the term Î“in Algorithms 1&2 can be efficiently calcu-
lated using the domain similarity ğ‘‘ğ‘˜ğ‘˜â€²and neural tangent kernel
Kğ‘˜ğ‘˜â€²between domain ğ‘˜and domain ğ‘˜â€².
5.4 Computational Complexity
Algorithms 1&2 show that the time complexity of TENON -DAand
TENON -OODis determined by the calculation of neural tangent kernel
(NTK) of any pair of training samples and the inversion of the
propagation matrix Î“KNTK. The time complexity of calculating
NTK over all domains is O(Â¯ğ‘›2)[37], where Â¯ğ‘›=Ãğ¾
ğ‘˜=1ğ‘›ğ‘˜denotes
the number of all training samples. The inversion of Î“KNTKrequires
O(Â¯ğ‘›3). Following [ 19], we can use the conjugate gradient method
to solve the linear system (Î“KNTK)yâˆ—=ğœ†y, in order to estimate the
propagated labels yâˆ—. This allows us to reduce the time complexity
fromO(Â¯ğ‘›3)toO(ğ‘Â¯ğ‘›2), whereğ‘is the number of iterations. In this
6Without prior knowledge regarding the unseen target domain, we assume that the
unseen target domain is equally similar to all source domains. In this case, only the
parameter smoothness regularization (i.e., the third term in Eq. (1)) will be available to
optimize the model parameters of this unseen target domain.case, we term the variants of TENON-DA andTENON-OOD algorithms
with conjugate gradient as TENON -DA-Fast andTENON -OOD-Fast ,
respectively (see subsection 6.3.3 for more empirical analysis).
6 Experiments
In the experiment, we evaluate the proposed TENON algorithms on
domain adaptation and out-of-distribution generalization data sets.
6.1 Experimental Setup
6.1.1 Data Sets. We use the following data sets.
â€¢Amazon Review [ 8]: It contains positive and negative prod-
uct reviews from four different domains: Books, DVD, Elec-
tronics, and Kitchen. Following [ 47,57], we use top-5000
frequent unigrams/bigrams to extract the bag-of-words fea-
tures for Amazon reviews. Each review is associated with a
binary label indicating positive or negative sentiment.
â€¢CityCam [ 54]: CityCam is a large-scale web camera data
set. It contains images captured by several cameras in dif-
ferent city locations. Following [ 11], we use images from
four cameras (with IDs: 253, 495, 511, and 572). Each image
has a 2048-dimensional feature vector extracted from the
pre-trained ResNet-50 [ 18]. Specifically, in this paper, we
consider a binary classification task based on the number of
vehicles within the camera images, i.e., whether there are at
least 10 cars in an image.
â€¢Huffpost [ 31]: Huffpost contains article headlines associated
with 11 news categories collected from the Huffington Post
from 2012 to 2018. Following [ 53], we use pre-trained Dis-
tilBERT [ 43] to extract a 768-dimensional feature vector for
each new headline. The task is to identify the news tags of
article headlines as one of the following 11 categories: Black
Voices, Business, Comedy, Crime, Entertainment, Impact,
Queer Voices, Science, Sports, Tech, Travel.
â€¢ArXiv [ 10]: ArXiv provides metadata of arXiv preprints from
2007 to 2023. As illustrated in [ 53], each preprint consists of
a paper title and its corresponding primary categories. The
paper title can further be represented as a 768-dimensional
feature vector using pre-trained DistilBERT [ 43]. The task of
ArXiv is to predict the primary category of arXiv pre-prints
from their paper titles.
â€¢CivilComments [ 22]: CivilComments consists of comments
scraped from the internet. It contains 8 demographic identi-
ties: male, female, LGBTQ, Christian, Muslim, other religions,
Black, or White. Each identity is considered as a single do-
main. CivilComments involves a binary classification task
to determine whether a comment is toxic.
6.1.2 Baselines. In the experiment, we consider the following do-
main adaptation baselines, including (1) semi-supervised learn-
ing: LabelProp [ 12,58], and (2) domain adaptation: DANN [ 14],
MDAN [ 57], M3SAD [ 39], DARN [ 47], and GRDA [ 52]. In addition,
we use the following out-of-distribution generalization baselines:
ERM, DANN [14], IRM [2], SD [41], Fish [45], and EQRM [13].
6.1.3 Configuration. Following [47], we use a 3-layer multi-layer
perceptron (MLP) to instantiate the prediction function for all
baselines. Then we implement our proposed algorithms using the
3385KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
ModelAmazon Review CityCam
Books DVD Electronics Kitchen 253 495 511 572
LabelProp [58] 0.7078Â±0.0040 0.7294Â±0.0093 0.7699Â±0.0079 0.7799Â±0.0082 0.6741Â±0.1360 0.6312Â±0.0470 0.6753Â±0.0888 0.7560Â±0.0564
DANN [14] 0.6958Â±0.0157 0.7229Â±0.0031 0.7818Â±0.0053 0.7879Â±0.0072 0.7804Â±0.0415 0.6716Â±0.0499 0.8498Â±0.0136 0.7563Â±0.0344
MDAN [57] 0.7196Â±0.0095 0.7432Â±0.0205 0.7744Â±0.0121 0.7869Â±0.0156 0.8007Â±0.0579 0.6685Â±0.0339 0.8222Â±0.0227 0.7280Â±0.0261
M3SAD [39] 0.7019Â±0.0232 0.7251Â±0.0210 0.7753Â±0.0117 0.7893Â±0.0134 0.8064Â±0.0581 0.6175Â±0.0286 0.7970Â±0.0357 0.7621Â±0.0531
DARN [47] 0.7175Â±0.0126 0.7412Â±0.0180 0.7703Â±0.0119 0.7888Â±0.0145 0.8243Â±0.0392 0.6795Â±0.0321 0.8271Â±0.0161 0.7547Â±0.0385
GRDA [52] 0.7110Â±0.0099 0.7294Â±0.0110 0.7714Â±0.0091 0.7884Â±0.0056 0.7949Â±0.0751 0.6698Â±0.0340 0.8254Â±0.0196 0.7495Â±0.0389
TENON-DA-Fast 0.7241Â±0.0146 0.7499Â±0.0101 0.7713Â±0.0093 0.7898Â±0.0101 0.8359Â±0.0146 0.7145Â±0.0122 0.7918Â±0.0104 0.7857Â±0.0176
TENON-DA 0.7238Â±0.0135 0.7503Â±0.0094 0.7763Â±0.0065 0.7851Â±0.0037 0.8350Â±0.0156 0.7143Â±0.0134 0.7932Â±0.0097 0.7859Â±0.0182
Table 1: Domain adaptation on Amazon review and CityCam data sets
Model 2013 2014 2015 2016 2017 2018 Avg.
LabelProp [58] 0.5312Â±0.0109 0.2942Â±0.0095 0.2641Â±0.0202 0.3535Â±0.0237 0.3900Â±0.0145 0.5055Â±0.0165 0.3897
DANN [14] 0.4171Â±0.0293 0.3510Â±0.0234 0.3847Â±0.0423 0.4468Â±0.0205 0.4568Â±0.0352 0.5490Â±0.0254 0.4342
MDAN [57] 0.4171Â±0.0293 0.3365Â±0.0481 0.3757Â±0.0546 0.4424Â±0.0207 0.4392Â±0.0267 0.5019Â±0.0322 0.4188
M3SAD [39] 0.4077Â±0.0290 0.3490Â±0.0439 0.4055Â±0.0380 0.4468Â±0.0246 0.4455Â±0.0281 0.4879Â±0.0398 0.4237
DARN [47] 0.4171Â±0.0293 0.3944Â±0.0140 0.3902Â±0.0375 0.4553Â±0.0205 0.4951Â±0.0227 0.5601Â±0.0106 0.4520
GRDA [52] 0.4324Â±0.0330 0.3671Â±0.0234 0.3539Â±0.0255 0.4534Â±0.0150 0.4520Â±0.0153 0.4987Â±0.0137 0.4262
TENON-DA-Fast 0.5850Â±0.0110 0.5028Â±0.0183 0.4573Â±0.0275 0.4995Â±0.0129 0.4556Â±0.0295 0.5201Â±0.0080 0.5033
TENON-DA 0.5851Â±0.0110 0.5028Â±0.0183 0.4575Â±0.0273 0.4987Â±0.0131 0.4651Â±0.0132 0.5198Â±0.0080 0.5048
Table 2: Domain adaptation on the Hoffpost data set ("Avg." indicates the average accuracy over all target domains)
Model 2009 2011 2013 2015 2017 2019 2021 Avg.
LabelProp [58] 0.7006Â±0.0267 0.6938Â±0.0060 0.7186Â±0.0063 0.6780Â±0.0056 0.6717Â±0.0130 0.6857Â±0.0085 0.6741Â±0.0099 0.6889
DANN [14] 0.7483Â±0.0122 0.6943Â±0.0387 0.7187Â±0.0304 0.7283Â±0.0092 0.7183Â±0.0081 0.7293Â±0.0256 0.7213Â±0.0350 0.7226
MDAN [57] 0.7483Â±0.0122 0.7161Â±0.0075 0.6251Â±0.0819 0.6593Â±0.0535 0.6748Â±0.0254 0.6944Â±0.0410 0.7139Â±0.0223 0.6903
M3SAD [39] 0.5533Â±0.0099 0.6026Â±0.1134 0.6845Â±0.0637 0.7151Â±0.0200 0.6980Â±0.0176 0.7384Â±0.0117 0.7473Â±0.0174 0.6770
DARN [47] 0.7483Â±0.0122 0.7228Â±0.0110 0.7123Â±0.0302 0.7177Â±0.0196 0.7126Â±0.0151 0.7456Â±0.0054 0.7471Â±0.0140 0.7295
GRDA [52] 0.7414Â±0.0219 0.7186Â±0.0041 0.6662Â±0.0482 0.6956Â±0.0385 0.6777Â±0.0215 0.7035Â±0.0286 0.7366Â±0.0103 0.7056
TENON-DA-Fast 0.7619Â±0.0098 0.7161Â±0.0064 0.7415Â±0.0071 0.7155Â±0.0070 0.7216Â±0.0034 0.7336Â±0.0076 0.7297Â±0.0124 0.7314
TENON-DA 0.7621Â±0.0100 0.7157Â±0.0063 0.7420Â±0.0067 0.7195Â±0.0053 0.7241Â±0.0014 0.7403Â±0.0078 0.7477Â±0.0096 0.7359
Table 3: Domain adaptation on the ArXiv data set ("Avg." indicates the average accuracy over all target domains)
Model Male Female LGBTQ Christian Muslim Others Black White Avg
ERM 0.6859Â±0.0091 0.6428Â±0.0186 0.6796Â±0.0226 0.7058Â±0.0248 0.7396Â±0.0258 0.7024Â±0.0074 0.7118Â±0.0169 0.6796Â±0.0137 0.6934
DANN [14] 0.6850Â±0.0096 0.6453Â±0.0249 0.6794Â±0.0202 0.7160Â±0.0156 0.7340Â±0.0207 0.6984Â±0.0058 0.6872Â±0.0280 0.6776Â±0.0145 0.6904
IRM [2] 0.6848Â±0.0087 0.6432Â±0.0199 0.6862Â±0.0186 0.7071Â±0.0240 0.7416Â±0.0195 0.7028Â±0.0083 0.7121Â±0.0192 0.6837Â±0.0121 0.6951
SD [41] 0.6777Â±0.0119 0.6449Â±0.0189 0.6847Â±0.0216 0.7068Â±0.0250 0.7464Â±0.0152 0.7031Â±0.0088 0.7092Â±0.0192 0.6784Â±0.0105 0.6939
Fish [45] 0.6882Â±0.0055 0.6650Â±0.0059 0.6793Â±0.0079 0.7363Â±0.0195 0.7512Â±0.0117 0.6944Â±0.0125 0.7219Â±0.0071 0.6912Â±0.0105 0.7034
EQRM [13] 0.6882Â±0.0094 0.6603Â±0.0034 0.6830Â±0.0176 0.7237Â±0.0269 0.7517Â±0.0063 0.6969Â±0.0083 0.7269Â±0.0067 0.6899Â±0.0096 0.7025
TENON-OOD-Fast 0.6922Â±0.0062 0.6678Â±0.0045 0.6968Â±0.0074 0.7236Â±0.0059 0.7502Â±0.0062 0.7006Â±0.0110 0.7037Â±0.0096 0.6813Â±0.0077 0.7020
TENON-OOD 0.6909Â±0.0082 0.6702Â±0.0030 0.7044Â±0.0063 0.7217Â±0.0086 0.7620Â±0.0066 0.7022Â±0.0089 0.7294Â±0.0055 0.6909Â±0.0048 0.7089
Table 4: Out-of-distribution generalization on CivilComments ("Avg." indicates the average accuracy over all testing domains)
NTK [ 26] induced by a 3-layer MLP with infinite width. The classifi-
cation accuracy is used as the evaluation metric in the experiments.
In addition, we set ğœ=2,ğœ†=1in our experiments.6.2 Main Results
In the following, we discuss the evaluation results of TENON algo-
rithms for domain adaptation and out-of-distribution generaliza-
tion.
3386Distributional Network of Networks for Modeling
Data Heterogeneity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
6.2.1 Domain Adaptation. Tables 1-3 provide the evaluation com-
parison between TENON-DA and baselines on various data sets (the
best results are indicated in bold). All the experiments are repeated
five times and then we report the mean and standard deviation
of classification accuracies. For each run, we randomly select 200
samples from each domain as the training samples and others as
the testing samples. Specifically, for Amazon Review and CityCam
data sets, following [ 47], we take one domain (e.g., "Books") as the
target domain, and others domains (e.g., "DVD", "Electronics" and
"Kitchen") as source domains. In contrast, Hoffpost and ArXiv data
sets [ 53] contain evolving domains where the data distribution is
changing over time. In this case, we take one specific time stamp as
the target domain and all historical time stamps as source domains.
We have the following observations from Tables 1-3. (1) Label-
Prop considers propagating the label information within a single
graph. It does not capture the data heterogeneity among different
domains, thus leading to sub-optimal performance in domain adap-
tation. (2) Compared to domain adaptation baselines, our proposed
non-parametric TENON-DA algorithm can achieve superior perfor-
mance in most cases. This observation verifies the effectiveness
ofTENON-DA in handling heterogeneous data across domains. (3)
TENON-DA-Fast achieves comparable performance with TENON-DA .
Furthermore, Figure 3(b) shows that TENON-DA-Fast significantly
reduces the running time compared to TENON-DA.
6.2.2 Out-of-Distribution Generalization. Table 4 shows the results
ofTENON-OOD on the CivilComments data set (the best results are in-
dicated in bold). In this case, we take one domain (e.g., "Male") as the
unseen testing target domain and others (e.g., "Female", "LGBTQ",
"Christian", "Muslim", "Others", "Black", and "White") as source train-
ing domains. It is observed that TENON-OOD outperforms baselines
for out-of-distribution generalization.
6.3 Analysis
6.3.1 Ablation Study. Here we study the impact of within-domain
label smoothness regularization on the proposed TENON -DA/TENON -
OODalgorithms. Table 5 reports the average accuracy of TENON -DA
andTENON -OODon ArXiv and CivilComments respectively. It indi-
cates that the label smoothness regularization improves the model
performance. Besides, Figure 3 compares the TENON -DA/TENON -OOD
algorithms with their approximation introduced in Subsection 5.4.
It can be seen that with only 10 iterations, TENON -DA-Fast /TENON -
OOD-Fast based on conjugate gradient can efficiently achieve simi-
lar performance with their counterparts.
6.3.2 Hyperparameter Sensitivity. We investigate the impact of
hyperparameter ğœ†on the proposed TENON -DAandTENON -OODalgo-
rithms. Figure 4 reports the results of TENON -DAandTENON -OODon
ArXiv and CivilComments respectively. It is observed that both
algorithms are robust to the selection of ğœ†.
6.3.3 Efficiency. Figure 5 shows the efficiency comparison between
TENON-DA algorithm and baselines, where the overall training run-
ning time is reported. It is observed that the proposed TENON-DA
algorithm is more computationally efficient than domain adaptation
baselines involving gradient descent training. Due to the efficient
approximation of matrix inversion, TENON-DA-Fast takes less time
than TENON-DA on Amazon Review and ArXiv data sets.
1234567891011121314151617181920
Iterations of conjugate gradient0.550.600.650.70Accuracy
 TENON-OOD
TENON-DA
TENON-OOD-Fast
TENON-DA-Fast(a) Accuracy
1234567891011121314151617181920
Iterations of conjugate gradient0.51.01.52.02.5Running time (s)
 (b) Running time
Figure 3: Analysis of conjugate gradient
0.20.40.60.81.01.21.41.61.82.0
Î»0.550.600.650.70Accuracy
TENON-OOD
TENON-DA
Figure 4: Impact of ğœ†
Amazon Review ArXiv020406080100120Running time (s)DANN
MDAN
M3SDADARN
TENON-DA
TENON-DA-Fast Figure 5: Efficiency analysis
Data TENON-DA TENON-DA w/o label smoothness
Amazon Review 0.7589 0.7573
CityCam 0.7821 0.7642
Hoffpost 0.5048 0.4987
ArXiv 0.7359 0.7297
Data TENON-OOD TENON-OOD w/o label smoothness
CivilComments 0.7089 0.7054
Table 5: Ablation study
7 Conclusion
In this paper, we propose a generic distributional network of net-
works ( TENON ) framework for modeling data heterogeneity, us-
ing within-domain label smoothness and cross-domain parameter
smoothness. Then we provide two instantiated algorithms of TENON
for domain adaptation and out-of-distribution generalization. The
effectiveness and efficiency of our proposed algorithms are verified
theoretically and empirically.
Acknowledgments
This work is supported by National Science Foundation (2117902
and 2134079), DARPA (HR001121C0165), and Agriculture and Food
Research Initiative (AFRI) grant no. 2020-67021-32799/project ac-
cession no.1024178 from the USDA National Institute of Food and
Agriculture. The views and conclusions are those of the authors
and should not be interpreted as representing the official policies
of the funding agencies or the government.
3387KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
References
[1]David Acuna, Guojun Zhang, Marc T Law, and Sanja Fidler. 2021. ğ‘“-Domain
Adversarial Learning: Theory and Algorithms. In ICML. 66â€“75.
[2]Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[3]Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. 2018. MetaReg:
Towards domain generalization using meta-regularization. NeurIPS 31 (2018).
[4]Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
Jennifer Wortman Vaughan. 2010. A theory of learning from different domains.
Machine Learning 79 (2010), 151â€“175.
[5]David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alexey
Kurakin. 2022. AdaMatch: A unified approach to semi-supervised learning and
domain adaptation. In ICLR.
[6]Gilles Blanchard, Gyemin Lee, and Clayton Scott. 2011. Generalizing from several
related classification tasks to a new unlabeled sample. NeurIPS 24 (2011).
[7]Manh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. 2021. Exploiting domain-
specific features to enhance domain generalization. NeurIPS 34 (2021), 21189â€“
21201.
[8]Minmin Chen, Zhixiang Xu, Kilian Q Weinberger, and Fei Sha. 2012. Marginalized
denoising autoencoders for domain adaptation. In ICML. 1627â€“1634.
[9]Qi Chen and Mario Marchand. 2023. Algorithm-Dependent Bounds for Represen-
tation Learning of Multi-Source Domain Adaptation. In AISTATS. 10368â€“10394.
[10] Colin B Clement, Matthew Bierbaum, Kevin P Oâ€™Keeffe, and Alexander A Alemi.
2019. On the use of arxiv as a dataset. arXiv preprint arXiv:1905.00075 (2019).
[11] Antoine de Mathelin, Guillaume Richard, FranÃ§ois Deheeger, Mathilde Mougeot,
and Nicolas Vayatis. 2021. Adversarial weighting for domain adaptation in
regression. In ICTAI. IEEE, 49â€“56.
[12] Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux. 2005. Efficient non-
parametric function induction in semi-supervised learning. In AISTATS. 96â€“103.
[13] Cian Eastwood, Alexander Robey, Shashank Singh, Julius Von KÃ¼gelgen, Hamed
Hassani, George J Pappas, and Bernhard SchÃ¶lkopf. 2022. Probable domain
generalization via quantile risk minimization. NeurIPS 35 (2022), 17340â€“17358.
[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, FranÃ§ois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. Journal of Machine Learning
Research (2016).
[15] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÃ¶lkopf, and
Alexander Smola. 2012. A kernel two-sample test. Journal of Machine Learning
Research 13, 1 (2012), 723â€“773.
[16] Ishaan Gulrajani and David Lopez-Paz. 2021. In search of lost domain generaliza-
tion. In ICLR.
[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. NeurIPS 30 (2017).
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In CVPR. 770â€“778.
[19] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. 2019. Label
propagation for deep semi-supervised learning. In CVPR. 5070â€“5079.
[20] Arthur Jacot, Franck Gabriel, and ClÃ©ment Hongler. 2018. Neural tangent kernel:
Convergence and generalization in neural networks. NeurIPS 31 (2018).
[21] Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, and Antonio
Torralba. 2012. Undoing the damage of dataset bias. In ECCV. 158â€“171.
[22] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin
Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton
Earnshaw, Imran S. Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma
Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. 2021. Wilds: A benchmark
of in-the-wild distribution shifts. In ICML. 5637â€“5664.
[23] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021. Out-of-
distribution generalization via risk extrapolation (rex). In ICML.
[24] Ananya Kumar, Tengyu Ma, and Percy Liang. 2020. Understanding self-training
for gradual domain adaptation. In ICML. 5468â€“5479.
[25] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak,
Jascha Sohl-Dickstein, and Jeffrey Pennington. 2019. Wide neural networks of
any depth evolve as linear models under gradient descent. NeurIPS 32 (2019).
[26] Ronaldas Paulius Lencevicius. 2022. An Empirical Analysis of the Laplace and
Neural Tangent Kernels. Masterâ€™s thesis. California State Polytechnic University,
Pomona.
[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. 2018. Learning to
generalize: Meta-learning for domain generalization. In AAAI, Vol. 32.
[28] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. 2018. Domain gener-
alization with adversarial feature learning. In CVPR. 5400â€“5409.
[29] Liangyue Li and Hanghang Tong. 2015. The child is father of the man: Foresee
the success at the early stage. In KDD. 655â€“664.[30] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. 2015. Learning
transferable features with deep adaptation networks. In ICML. 97â€“105.
[31] Rishabh Misra. 2022. News Category Dataset. arXiv preprint arXiv:2209.11429
(2022).
[32] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations
of machine learning. MIT press.
[33] Eduardo Fernandes Montesuma and Fred Maurice NgolÃ¨ Mboula. 2021. Wasser-
stein Barycenter for Multi-Source Domain Adaptation. In CVPR. 16785â€“16793.
[34] Krikamol Muandet, David Balduzzi, and Bernhard SchÃ¶lkopf. 2013. Domain
generalization via invariant feature representation. In ICML. 10â€“18.
[35] Debarghya Mukherjee, Felix Petersen, Mikhail Yurochkin, and Yuekai Sun. 2022.
Domain Adaptation meets Individual Fairness. And they get along. NeurIPS 35
(2022), 28902â€“28913.
[36] Jingchao Ni, Hanghang Tong, Wei Fan, and Xiang Zhang. 2014. Inside the atoms:
ranking on a network of networks. In KDD. 1356â€“1365.
[37] Roman Novak, Jascha Sohl-Dickstein, and Samuel S Schoenholz. 2022. Fast finite
width neural tangent kernel. In ICML. 17018â€“17044.
[38] Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE
Transactions on Knowledge and Data Engineering (2010).
[39] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.
2019. Moment matching for multi-source domain adaptation. In ICCV. 1406â€“1415.
[40] Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. 2019. Domain
agnostic learning with disentangled representations. In ICML. 5102â€“5112.
[41] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina
Precup, and Guillaume Lajoie. 2021. Gradient starvation: A learning proclivity in
neural networks. NeurIPS 34 (2021), 1256â€“1272.
[42] Alexandre Rame, Corentin Dancette, and Matthieu Cord. 2022. Fishr: Invariant
gradient variances for out-of-distribution generalization. In ICML. 18347â€“18377.
[43] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 (2019).
[44] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. 2018. Wasserstein distance
guided representation learning for domain adaptation. In AAAI, Vol. 32.
[45] Yuge Shi, Jeffrey Seely, Philip Torr, Siddharth N, Awni Hannun, Nicolas Usunier,
and Gabriel Synnaeve. 2022. Gradient matching for domain generalization. In
ICLR.
[46] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino,
and Silvio Savarese. 2018. Generalizing to unseen domains via adversarial data
augmentation. NeurIPS 31 (2018).
[47] Junfeng Wen, Russell Greiner, and Dale Schuurmans. 2020. Domain aggregation
networks for multi-source domain adaptation. In ICML. 10214â€“10224.
[48] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena,
Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. 2022. A fine-grained
analysis on distribution shift. In ICLR.
[49] Jun Wu, Wenxuan Bao, Elizabeth Ainsworth, and Jingrui He. 2023. Personalized
federated learning with parameter propagation. In KDD. 2594â€“2605.
[50] Jun Wu, Jingrui He, Sheng Wang, Kaiyu Guan, and Elizabeth Ainsworth.
2022. Distribution-informed neural networks for domain adaptation regression.
NeurIPS 35 (2022), 10040â€“10054.
[51] Tongkun Xu, Weihua Chen, Pichao WANG, Fan Wang, Hao Li, and Rong Jin. 2022.
CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation. In
ICLR.
[52] Zihao Xu, Hao He, Guang-He Lee, Bernie Wang, and Hao Wang. 2022. Graph-
relational domain adaptation. In ICLR.
[53] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and
Chelsea Finn. 2022. Wild-time: A benchmark of in-the-wild distribution shift
over time. NeurIPS 35 (2022), 10309â€“10324.
[54] Shanghang Zhang, Guanhang Wu, Joao P Costeira, and Jose MF Moura. 2017.
Understanding traffic density from large-scale web camera data. In CVPR. 5898â€“
5907.
[55] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. 2019. Bridging
theory and algorithm for domain adaptation. In ICML. 7404â€“7413.
[56] Yu Zhang and Dit-Yan Yeung. 2010. A convex formulation for learning task
relationships in multi-task learning. In UAI. 733â€“742.
[57] Han Zhao, Shanghang Zhang, Guanhang Wu, JosÃ© MF Moura, Joao P Costeira,
and Geoffrey J Gordon. 2018. Adversarial multiple source domain adaptation.
NeurIPS 31 (2018), 8568â€“8579.
[58] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard
SchÃ¶lkopf. 2003. Learning with local and global consistency. NeurIPS 16 (2003).
[59] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. 2021. Domain Generaliza-
tion with MixStyle. In ICLR.
[60] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. 2003. Semi-supervised
learning using Gaussian fields and harmonic functions. In ICML. 912â€“919.
3388Distributional Network of Networks for Modeling
Data Heterogeneity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A Appendix
In the appendix, we provide the proof of theoretical results pre-
sented in the paper.
A.1 Proof of Lemma 3
Proof. The objective of Eq. (1) can be rewritten as follows.
J(Î˜)=Î˜ğ‘‡Ë†XË†AË†Xğ‘‡Î˜+Î˜ğ‘‡Ë†BÎ˜+ğœ†Â·Ë†Xğ‘‡Î˜âˆ’y2
2
Then the derivative of J(Î˜)is given by
ğœ•J(Î˜)
ğœ•Î˜=ğœ•
Î˜ğ‘‡Ë†XË†AË†Xğ‘‡Î˜+Î˜ğ‘‡Ë†BÎ˜+ğœ†Â·Ë†Xğ‘‡Î˜âˆ’y2
2
ğœ•Î˜
=2
Ë†XË†AË†Xğ‘‡+Ë†B
Î˜+2ğœ†Ë†XË†Xğ‘‡Î˜âˆ’2ğœ†Ë†Xy
=2
Ë†XË†AË†Xğ‘‡+Ë†B+ğœ†Ë†XË†Xğ‘‡
Î˜âˆ’2ğœ†Ë†Xy
By settingğœ•J(Î˜)
ğœ•Î˜=0, the minimizer of J(Î˜)is obtained at
Î˜âˆ—=ğœ†
Ë†XË†AË†Xğ‘‡+Ë†B+ğœ†Ë†XË†Xğ‘‡âˆ’1Ë†Xy
=ğœ†Ë†X
Ë†AË†Xğ‘‡Ë†X+
Ë†Xğ‘‡Ë†Xâˆ’1
Ë†Xğ‘‡Ë†BË†X
+ğœ†Ë†Xğ‘‡Ë†Xâˆ’1
y
which completes the proof. â–¡
A.2 Proof of Theorem 4
Proof. Following [ 25], we consider the following linearized
neural network
ğ‘“lin
ğ‘¡(ğ‘¥)=ğ‘“0(ğ‘¥)+ âˆ‡ğœƒ0ğ‘“0(ğ‘¥)ğ‘‡(ğ‘¤ğ‘¡)
whereğ‘¤ğ‘¡=ğœƒğ‘¡âˆ’ğœƒ0is the parameter change from the initial values.
LetWğ‘¡=Î˜ğ‘¡âˆ’Î˜0be the change of parameters from the initial val-
ues andğ‘“ğ‘¡(ğ‘‹)=vec(ğ‘“ğ‘¡(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)|ğ‘–âˆˆ{1,2,Â·Â·Â·,ğ‘›ğ‘˜},ğ‘˜âˆˆ{1,2,Â·Â·Â·,ğ¾})
be the vectorized predicted values over all input samples. Based on
continuous time gradient descent [ 25], the evolution of the param-
eters can be expressed as
Â¤Wğ‘¡=âˆ’ğœ‚
2âˆ‡Î˜J(Î˜)=âˆ’ğœ‚
2âˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡
Ë†Afğ‘¡+ğœ†(fğ‘¡âˆ’y)
âˆ’ğœ‚Ë†BÎ˜ğ‘¡
=âˆ’ğœ‚âˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡ 
Ë†Aâˆ‡Î˜ğ‘“0(ğ‘‹)+ğœ†âˆ‡Î˜ğ‘“0(ğ‘‹)
Wğ‘¡
+
âˆ‡Î˜ğ‘“0(ğ‘‹)âˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡âˆ’1
âˆ‡Î˜ğ‘“0(ğ‘‹)Ë†BWğ‘¡
+
âˆ‡Î˜ğ‘“0(ğ‘‹)âˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡âˆ’1
âˆ‡Î˜ğ‘“0(ğ‘‹)Ë†BÎ˜0+Ë†Ağ‘“0(ğ‘‹)+ğœ†ğ‘“0(ğ‘‹)âˆ’ğœ†y!
In this case, the ODE has a closed-form solution below.
Î˜ğ‘¡
=âˆ’âˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡
Ë†AKNTK+ğœ†KNTK+Kâˆ’1
NTK
âˆ‡Î˜ğ‘“0(ğ‘‹)Ë†Bâˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡âˆ’1
Â·
Iâˆ’expn
âˆ’ğœ‚
Ë†AKNTK+ğœ†KNTK+Kâˆ’1
NTK
âˆ‡Î˜ğ‘“0(ğ‘‹)Ë†Bâˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡
ğ‘¡o
Â·
Kâˆ’1
NTKâˆ‡Î˜ğ‘“0(ğ‘‹)Ë†BÎ˜0+Ë†Ağ‘“0(ğ‘‹)+ğœ†ğ‘“0(ğ‘‹)âˆ’ğœ†y
+Î˜0
Thus,
limğ‘¡â†’âˆÎ˜ğ‘¡=âˆ’âˆ‡Î˜ğ‘“0(X)ğ‘‡Kâˆ’1
NTKÎ“âˆ’1(â„¦âˆ’ğœ†y)+Î˜0For an arbitrary point ğ‘¥ğ‘˜, the predicted value is given by
ğ‘“lin
ğ‘¡(ğ‘¥ğ‘˜;ğœƒğ‘˜)
=ğœ†Â·KNTK(ğ‘¥ğ‘˜,X)
Ë†AKNTK+ğœ†KNTK+Kâˆ’1
NTK
âˆ‡Î˜ğ‘“0(X)Ë†Bâˆ‡Î˜ğ‘“0(X)ğ‘‡âˆ’1
Â·
Iâˆ’expn
âˆ’ğœ‚
Ë†AKNTK+ğœ†KNTK+Kâˆ’1
NTK
âˆ‡Î˜ğ‘“0(X)Ë†Bâˆ‡Î˜ğ‘“0(X)ğ‘‡
ğ‘¡o
y
+ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,X)
Ë†AKNTK+ğœ†KNTK+Kâˆ’1
NTK
âˆ‡Î˜ğ‘“0(ğ‘‹)Ë†Bâˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡âˆ’1
Â·
Iâˆ’expn
âˆ’ğœ‚
Ë†AKNTK+ğœ†KNTK+Kâˆ’1
NTK
âˆ‡Î˜ğ‘“0(ğ‘‹)Ë†Bâˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡
ğ‘¡o
Â·
Kâˆ’1
NTKâˆ‡Î˜ğ‘“0(ğ‘‹)Ë†BÎ˜0+Ë†Ağ‘“0(ğ‘‹)+ğœ†ğ‘“0(ğ‘‹)
Thus, it holds that
limğ‘¡â†’âˆğ‘“ğ‘¡(ğ‘¥ğ‘˜;ğœƒğ‘˜)=limğ‘¡â†’âˆğ‘“lin
ğ‘¡(ğ‘¥ğ‘˜;ğœƒğ‘˜)=ğœ†KNTK(ğ‘¥ğ‘˜,X)Kâˆ’1
NTKÎ“âˆ’1y
+ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,X)Kâˆ’1
NTKÎ“âˆ’1â„¦
which completes the proof. â–¡
A.3 Proof of Corollary 5
Proof. In this case, it holds that Ë†B=0ğ¾ğ‘‘ğœƒÃ—ğ¾ğ‘‘ğœƒ. Then for any
sampleğ‘¥ğ‘˜, it holds
ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,X)
Ë†AKNTK+ğœ†KNTK+Kâˆ’1
NTKâˆ‡Î˜ğ‘“0(ğ‘‹)Ë†Bâˆ‡Î˜ğ‘“0(ğ‘‹)ğ‘‡âˆ’1
Â·
Kâˆ’1
NTKâˆ‡Î˜ğ‘“0(X)Ë†BÎ˜0+Ë†Ağ‘“0(X)+ğœ†ğ‘“0(X)
=ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,X)
Ë†AKNTK+ğœ†KNTKâˆ’1
Ë†Ağ‘“0(X)+ğœ†ğ‘“0(X)
=ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,X)Kâˆ’1
NTKğ‘“0(X)
=ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,Xğ‘˜)Kâˆ’1
ğ‘˜ğ‘˜ğ‘“0(Xğ‘˜)
Thus,
limğ‘¡â†’âˆğ‘“ğ‘¡(ğ‘¥ğ‘˜;ğœƒğ‘˜)=ğœ†KNTK(ğ‘¥ğ‘˜,X)
Ë†AKNTK+ğœ†KNTKâˆ’1
y
+ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,Xğ‘˜)Kâˆ’1
ğ‘˜ğ‘˜ğ‘“0(Xğ‘˜)
=ğœ†h
0,Â·Â·Â·,0,K(ğ‘¥ğ‘˜,Xğ‘˜),0,Â·Â·Â·,0i 
Ë†A+ğœ†I
KNTKâˆ’1
y
+ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,Xğ‘˜)Kâˆ’1
ğ‘˜ğ‘˜ğ‘“0(Xğ‘˜)
=ğœ†K
ğ‘¥ğ‘˜,Xğ‘˜
Kâˆ’1
ğ‘˜ğ‘˜ ğœ†Iğ‘›ğ‘˜Ã—ğ‘›ğ‘˜+Iğ‘›ğ‘˜Ã—ğ‘›ğ‘˜âˆ’Ağ‘˜âˆ’1yğ‘˜
+ğ‘“0(ğ‘¥ğ‘˜;ğœƒğ‘˜)âˆ’KNTK(ğ‘¥ğ‘˜,Xğ‘˜)Kâˆ’1
ğ‘˜ğ‘˜ğ‘“0(Xğ‘˜)
For training samples Xğ‘˜=[ğ‘¥ğ‘˜
1,Â·Â·Â·,ğ‘¥ğ‘˜ğ‘›ğ‘˜], the following holds
limğ‘¡â†’âˆğ‘“ğ‘¡(Xğ‘˜;ğœƒğ‘˜)=ğœ†
ğœ†+1
Iğ‘›ğ‘˜Ã—ğ‘›ğ‘˜âˆ’1
ğœ†+1Ağ‘˜âˆ’1
yğ‘˜
which completes the proof. â–¡
A.4 Proof Corollary 6
Proof. With linearized model ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)=âˆ‡ğœƒğ‘“0(ğ‘¥ğ‘˜
ğ‘–)ğ‘‡ğœƒğ‘˜, the
objective of Eq. (1) can be rewritten as follows.
J(Î˜)=1
2ğ¾âˆ‘ï¸
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒğ‘˜âˆšï¸
ğ‘€ğ‘˜ğ‘˜âˆ’ğœƒğ‘˜â€²âˆšï¸
ğ‘€ğ‘˜â€²ğ‘˜â€²2
2+ğœ†ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–=1ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)âˆ’ğ‘¦ğ‘˜
ğ‘–2
2
Using gradient descent, for any ğ‘˜, it holds
ğœƒğ‘˜(ğ‘¡+1)=ğœƒğ‘˜(ğ‘¡)âˆ’ğœ‚ğœ•J(Î˜)
ğœ•ğœƒğ‘˜
3389KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
=ğœƒğ‘˜(ğ‘¡)âˆ’ğœ‚ 
ğœƒğ‘˜(ğ‘¡)âˆ’ğ¾âˆ‘ï¸
ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²âˆšï¸
ğ‘€ğ‘˜ğ‘˜ğ‘€ğ‘˜â€²ğ‘˜â€²ğœƒğ‘˜â€²(ğ‘¡)
+ğœ†
âˆ‡ğ‘“(Xğ‘˜)ğ‘‡âˆ‡ğ‘“(Xğ‘˜)ğœƒğ‘˜(ğ‘¡)âˆ’âˆ‡ğ‘“(Xğ‘˜)ğ‘‡yğ‘˜!
=
(1âˆ’ğœ‚)Iâˆ’ğœ†âˆ‡ğ‘“(Xğ‘˜)ğ‘‡âˆ‡ğ‘“(Xğ‘˜)
ğœƒğ‘˜(ğ‘¡)
+ğœ‚ğ¾âˆ‘ï¸
ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²âˆšï¸
ğ‘€ğ‘˜ğ‘˜ğ‘€ğ‘˜â€²ğ‘˜â€²ğœƒğ‘˜â€²(ğ‘¡)+ğœ‚ğœ†âˆ‡ğ‘“(Xğ‘˜)ğ‘‡yğ‘˜
which completes the proof. â–¡
A.5 Proof of Theorem 7
Proof. Following standard machine learning theory [ 32], given
hypothesis spaceHand the loss function is bounded by ğµ(for any
ğ‘¥,|ğ‘“(ğ‘¥;ğœƒğ‘˜)âˆ’ğ‘“(ğ‘¥;ğœƒâˆ—
ğ‘˜)|â‰¤ğµ), then for any ğ‘“(Â·;ğœƒğ¾)âˆˆH ,
Eğ‘¥âˆ¼Pğ¾
ğ‘“(ğ‘¥;ğœƒğ¾)âˆ’ğ‘“ ğ‘¥;ğœƒâˆ—
ğ¾2
â‰¤1
ğ‘›ğ¾ğ‘›ğ¾âˆ‘ï¸
ğ‘–=1
ğ‘“
ğ‘¥ğ¾
ğ‘–;ğœƒğ¾
âˆ’ğ‘“
ğ‘¥ğ¾
ğ‘–;ğœƒâˆ—
ğ¾ 2
+ğµâˆšï¸„
log|H|+ log(2/ğ›¿)
2ğ‘›ğ¾
where|H|is the size of the hypothesis space and can be further
bounded by the VC dimension of hypothesis space H. It holds
1
ğ‘›ğ¾ğ‘›ğ¾âˆ‘ï¸
ğ‘–=1
ğ‘“
ğ‘¥ğ¾
ğ‘–;ğœƒğ¾
âˆ’ğ‘“
ğ‘¥ğ¾
ğ‘–;ğœƒâˆ—
ğ¾ 2
â‰¤1
ğ‘›ğ¾ğ‘“(Xğ¾;ğœƒğ¾)âˆ’ğ‘” ğ‘“(Xğ‘†;ğœƒğ‘†)2
+1
ğ‘›ğ¾ğ‘” ğ‘“(Xğ‘†;ğœƒğ‘†)âˆ’ğ‘” ğ‘“ Xğ‘†;ğœƒâˆ—
ğ‘† 2
+1
ğ‘›ğ¾ğ‘” ğ‘“ Xğ‘†;ğœƒâˆ—
ğ‘† âˆ’ğ‘“ Xğ¾;ğœƒâˆ—
ğ¾2
â‰¤1
ğ‘›ğ¾ğ‘ˆR
ğ¿Rğ‘“(Xğ‘†;ğœƒğ‘†)âˆ’ğ‘“ Xğ‘†;ğœƒâˆ—
ğ‘†2
+1
ğ‘›ğ¾2
ğ¿RR
ğ‘“(Xğ‘†;ğœƒğ‘†),ğ‘“(Xğ¾;ğœƒğ¾)
+1
ğ‘›ğ¾2
ğ¿RR
ğ‘“ Xğ‘†;ğœƒâˆ—
ğ‘†,ğ‘“(Xğ¾;ğœƒâˆ—
ğ¾)
â‰¤ğœ 
ğœ†ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–=1ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)âˆ’ğ‘¦ğ‘˜
ğ‘–2
2+1
2ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–,ğ‘—=1ğ‘ ğ‘˜
ğ‘–ğ‘—ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘–ğ‘–âˆ’ğ‘“(ğ‘¥ğ‘˜
ğ‘—;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘—ğ‘—2
2
+1
2ğ¾âˆ‘ï¸
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒğ‘˜âˆšğ‘€ğ‘˜ğ‘˜âˆ’ğœƒğ‘˜â€²âˆšğ‘€ğ‘˜â€²ğ‘˜â€²2
2!
+1
ğ‘›ğ¾ğ¿Rğ¾âˆ‘ï¸
ğ‘˜=1Î©(Xğ‘˜,ğœƒâˆ—
ğ‘˜)+1
ğ‘›ğ¾ğ¿RÎ”(ğœƒâˆ—
1,Â·Â·Â·,ğœƒâˆ—
ğ¾)
where Î”(ğœƒâˆ—
1,Â·Â·Â·,ğœƒâˆ—
ğ¾)=Ãğ¾
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒâˆ—
ğ‘˜âˆšğ‘€ğ‘˜ğ‘˜âˆ’ğœƒâˆ—
ğ‘˜â€²âˆšğ‘€ğ‘˜â€²ğ‘˜â€²2
2andğœ=
maxnğ‘ˆR
ğœ†ğ‘›ğ¾ğ¿R,2
ğ‘›ğ¾ğ¿Ro
.
Note that in previous steps, we let
ğ‘”(Ë†yğ‘ )=arg min
Ë†yğ‘¡R
Ë†yğ‘ ,Ë†yğ‘¡
=1
2ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–,ğ‘—=1ğ‘ ğ‘˜
ğ‘–ğ‘—ğ‘“(ğ‘¥ğ‘˜
ğ‘–;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘–ğ‘–âˆ’ğ‘“(ğ‘¥ğ‘˜
ğ‘—;ğœƒğ‘˜)
âˆšï¸ƒ
ğ·ğ‘˜
ğ‘—ğ‘—2
2+1
2ğ¾âˆ‘ï¸
ğ‘˜,ğ‘˜â€²=1ğ‘‘ğ‘˜ğ‘˜â€²ğœƒğ‘˜âˆšï¸
ğ‘€ğ‘˜ğ‘˜âˆ’ğœƒğ‘˜â€²âˆšï¸
ğ‘€ğ‘˜â€²ğ‘˜â€²2
2
where Ë†yğ‘ =ğ‘“(Xğ‘ ;ğœƒğ‘ ). We assume thatRis strongly convex and
smooth. For any ğœƒğ‘ ,ğœƒğ‘¡,ğœƒâ€²
ğ‘¡âˆˆRğ‘‘ğœƒ, the following holds
R
Ë†yğ‘ ,Ë†yğ‘¡
â‰¥R
Ë†yğ‘ ,Ë†yâ€²
ğ‘¡
+D
Ë†yğ‘¡âˆ’Ë†yâ€²
ğ‘¡,ğœ•Ë†yâ€²
ğ‘¡R Ë†yğ‘ ,Ë†yâ€²
ğ‘¡E
+ğ¿R
2Ë†yğ‘¡âˆ’Ë†yâ€²
ğ‘¡2
2
R
Ë†yğ‘ ,Ë†yğ‘¡
â‰¤R
Ë†yğ‘ ,Ë†yâ€²
ğ‘¡
+D
Ë†yğ‘¡âˆ’Ë†yâ€²
ğ‘¡,ğœ•Ë†yâ€²
ğ‘¡R Ë†yğ‘ ,Ë†yâ€²
ğ‘¡E
+ğ‘ˆR
2Ë†yğ‘¡âˆ’Ë†yâ€²
ğ‘¡2
2
Thenğ‘”(Ë†yğ‘ )âˆ’ğ‘” Ë†yâˆ—
ğ‘ 2
2â‰¤ğ‘ˆR
ğ¿RË†yğ‘ âˆ’Ë†yâˆ—
ğ‘ =ğ‘ˆR
ğ¿Rğ‘“(Xğ‘ ;ğœƒğ‘ )âˆ’ğ‘“(Xğ‘ ;ğœƒâˆ—
ğ‘ )
Ë†yğ‘¡âˆ’ğ‘”(Ë†yğ‘ )2
â‰¤2
ğ¿RR(Ë†yğ‘ ,Ë†yğ‘¡)
Next, following [ 35], we show the strong convexity and smooth-
ness ofR.
ğœ•R(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•ğœƒğ‘¡=ğœ•Ë†yğ‘¡
ğœƒğ‘¡Â·ğœ•R(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•Ë†yğ‘¡
ğœ•R(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•ğœƒğ‘¡=âˆ‡ğ‘“(Xğ‘˜)ğ‘‡(Iâˆ’Ağ¾)âˆ‡ğ‘“(Xğ‘˜)ğœƒğ‘¡
+ğ¾âˆ‘ï¸
ğ‘˜â€²=1ğ‘‘ğ¾ğ‘˜â€²âˆšğ‘€ğ¾ğ¾ğœƒğ‘¡âˆšğ‘€ğ¾ğ¾âˆ’ğœƒğ‘˜â€²âˆšğ‘€ğ‘˜â€²ğ‘˜â€²
=âˆ‡ğ‘“(Xğ‘˜)ğ‘‡(Iâˆ’Ağ¾)âˆ‡ğ‘“(Xğ‘˜)ğœƒğ‘¡+ğœƒğ‘¡
âˆ’ğ¾âˆ‘ï¸
ğ‘˜â€²=1ğ‘‘ğ¾ğ‘˜â€²âˆšï¸
ğ‘€ğ‘˜â€²ğ‘˜â€²âˆšğ‘€ğ¾ğ¾ğœƒğ‘˜â€²
ğœ•R2(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•ğœƒ2
ğ‘¡=âˆ‡ğ‘“(Xğ‘˜)ğ‘‡(Iâˆ’Ağ¾)âˆ‡ğ‘“(Xğ‘˜)+I
=âˆ‡ğ‘“(Xğ‘˜)ğ‘‡
Iâˆ’Ağ¾+
âˆ‡ğ‘“(Xğ¾)âˆ‡ğ‘“(Xğ¾)ğ‘‡âˆ’1
âˆ‡ğ‘“(Xğ‘˜)
ğœ•Ë†yğ‘¡
ğœƒğ‘¡=âˆ‡ğ‘“(Xğ‘˜)ğ‘‡âˆˆRğ‘‘ğœƒÃ—ğ‘›ğ¾
ğœ•
ğœ•ğœƒğ‘¡ğœ•Ë†yğ‘¡
ğœƒğ‘¡Â·ğœ•R(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•Ë†yğ‘¡
=âˆ‡ğ‘“(Xğ‘˜)ğ‘‡ğœ•R2(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•Ë†yğ‘¡Â·ğœ•ğœƒğ‘¡
=âˆ‡ğ‘“(Xğ‘˜)ğ‘‡ğœ•R2(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•Ë†yğ‘¡Â·ğœ•Ë†yğ‘¡ğ‘“(Xğ‘˜)
Thus,
ğœ•R2(Ë†yğ‘ ,Ë†yğ‘¡)
ğœ•Ë†yğ‘¡Â·ğœ•Ë†yğ‘¡=Lğ¾+
âˆ‡ğ‘“(Xğ¾)âˆ‡ğ‘“(Xğ¾)ğ‘‡âˆ’1
=Lğ¾+Kâˆ’1
ğ¾ğ¾
where Lğ¾=Iâˆ’Ağ¾is a symmetrically normalized Laplacian matrix,
andKğ¾ğ¾is the neural tangent kernel (NTK) matrix within the
target domain. Thus, ğ¿Randğ‘ˆRare given by the maximum and
minimum eigenvalues of Lğ¾+Kâˆ’1
ğ¾ğ¾. â–¡
3390