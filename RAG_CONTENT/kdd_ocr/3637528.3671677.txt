Noisy Label Removal for Partial Multi-Label Learning
Fuchao Yang
yangfc@seu.edu.cn
College of Software Engineering,
Southeast University
Nanjing, ChinaYuheng Jia∗
yhjia@seu.edu.cn
School of Computer Science and
Engineering, Southeast University
Nanjing, ChinaHui Liu
h2liu@sfu.edu.HK
School of Computing & Information
Sciences, Saint Francis University
Hong Kong, China
Yongqiang Dong
dongyq@seu.edu.cn
School of Computer Science and
Engineering, Southeast University
Nanjing, ChinaJunhui Hou
jh.hou@cityu.edu.hk
Department of Computer Science,
City University of Hong Kong
Hong Kong, China
ABSTRACT
This paper addresses the problem of partial multi-label learning
(PML), a challenging weakly supervised learning framework, where
each sample is associated with a candidate label set comprising both
ground-true labels and noisy labels. We theoretically reveal that an
increased number of noisy labels in the candidate label set leads to
an enlarged generalization error bound, consequently degrading
the classification performance. Accordingly, the key to solving PML
lies in accurately removing the noisy labels within the candidate
label set. To achieve this objective, we leverage prior knowledge
about the noisy labels in PML, which suggests that they only exist
within the candidate label set and possess binary values. Specifically,
we propose a constrained regression model to learn a PML classifier
and select the noisy labels. The constraints of the model strictly
enforce the location and value of the noisy labels. Simultaneously,
the supervision information provided by the candidate label set is
unreliable due to the presence of noisy labels. In contrast, the non-
candidate labels of a sample precisely indicate the classes to which
the sample does not belong. To aid in the selection of noisy labels,
we construct a competitive classifier based on the non-candidate
labels. The PML classifier and the competitive classifier form a com-
petitive relationship, encouraging mutual learning. We formulate
the proposed model as a discrete optimization problem to effec-
tively remove the noisy labels, and we solve it using an alternative
algorithm. Extensive experiments conducted on 6 real-world partial
multi-label data sets and 7 synthetic data sets, employing various
evaluation metrics, demonstrate that our method significantly out-
performs state-of-the-art PML methods. The code implementation
is publicly available at https://github.com/Yangfc-ML/NLR.
∗Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671677CCS CONCEPTS
•Computing methodologies →Learning paradigms.
KEYWORDS
multi-label learning, partial label learning, partial multi-label learn-
ing
ACM Reference Format:
Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, and Junhui Hou. 2024.
Noisy Label Removal for Partial Multi-Label Learning. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671677
Figure 1: An example of partial multi-label learning. Among
the candidate label set, only malamute andphone are the
ground-truth labels of the image, husky andpadare the noisy
labels.
1 INTRODUCTION
Multi-label learning (MLL) [ 4,6,37] is a supervised learning para-
digm where each sample is associated with multiple labels. MLL has
a close connection with real-world scenarios, and thus has attracted
widespread attention and has been applied in various fields such as
image annotation [ 14,21], bioinformatics [ 3,35], and information
retrieval [ 15]. However, obtaining accurate multiple labels is usually
difficult and costly. Moreover, when annotators cannot determine
the exact content of the image, they may select all possible labels
[29] as candidates. For example, as shown in Fig. 1, the annotator
could not determine whether the dog is a husky or a malamute
3724
KDD ’24, August 25–29, 2024, Barcelona, Spain. Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, & Junhui Hou
and could not decide whether the object covered by the dog’s front
paw is a phone or a pad, so the annotator may select these four
labels as the possible labels of the image. This kind of scenario is
abstracted into a new paradigm called partial multi-label learning
(PML)[16, 24, 29, 30].
Formally speaking, let X=R𝑑be the𝑑-dimensional feature
space andY={1,2,...,𝑞}be the label space with 𝑞labels. Given the
partial multi-label training set D={(𝑥𝑖,𝐶𝑖)|1≤𝑖≤𝑚}, where𝑚
is the total number of partial multi-label training samples, 𝑥𝑖∈X
is a𝑑-dimensional feature vector to represent the 𝑖-th sample and
𝐶𝑖⊆Y is the associated candidate label set. For example, husky,
malamute, phone, padform the candidate label set of Fig. 1. Note,
candidate label set includes ground-truth labels malamute, phone
and noisy labels husky, pad, and ground-truth labels can not be
accessed directly. PML aims to induce a multi-label classifier 𝑓:
X→ 2YfromD.
One simple way to deal with PML is to treat all the labels in the
candidate label set as the ground-true labels, and then use off-the-
shelf MLL methods to achieve multi-label classification. However,
due to the presence of noisy labels in the candidate label set, this
naive manner is difficult to achieve superior performance [ 29,32].
Afterwards, some two-stage PML methods [ 27,33] were proposed,
which first performed label disambiguation to estimate the con-
fidence of the candidate labels and then used the disambiguated
labels as the ground-true labels and performed MLL methods to
fulfill multi-label classification. These methods may suffer from
poor classification performance as they are limited by the quality
of the disambiguated labels obtained in the first stage. Recently,
some methods combined label disambiguation and MLL into a sin-
gle model [ 17,25,26,28,30,32] by exploring the label corrections
to recover the ground-true labels. Among them, [ 25,26,30] pro-
posed to eliminate the noisy labels from the candidate label set
based on the sparsity assumption. However, these methods are all
empirically designed without considering the influence of noisy
labels from a theoretical perspective. Moreover, they fail to fully
exploit the prior knowledge of noisy labels. On the one hand, in
PML, the noisy labels can only exist in the candidate label set, while
the noisy labels estimated by the previous methods may appear in
the non-candidate label set. On the other hand, the noise indicator
should be binary values, while those estimated by the previous
methods are all continuous variables. For example, the model will
be very confused by a noise indicator with a value of 0.5, because
it cannot determine whether it is a noisy label or not.
To tackle the above issues, we propose a novel PML method
named NLR (Noisy Label Removal for Partial Multi-Label Learn-
ing). We first establish the generalization error bound for typical
PML models based on the Rademacher complexity, theoretically re-
vealing that more noisy labels will increase the generalization error
bound and consequently degrade the classification performance.
This observation theoretically indicates that correctly noisy label
removal is the key to PML. To remove the noisy labels, we exploit
the prior knowledge of the noisy labels in PML, i.e., they only exist
in the candidate label set and the value of noisy labels should be 0
or 1, and we transform those priors as strict constraints. Moreover,
we leverage the information from both the candidate label set andthe non-candidate label set to construct a PML classifier and a com-
petitive classifier, respectively. The PML classifier indicates which
classes the sample should belong to, while the competitive classifier
indicates which classes the sample should notbelong to. By explor-
ing the competitive relationship between the two classifiers, i.e., a
larger (resp. smaller) value predicted by the PML classifier implies
a smaller (resp. larger) value predicted by the competitive classifier,
the noisy labels are easier to be removed correctly. The proposed
model is finally formulated as a discrete regression problem with
competitive learning and solved by an alternative and iterative al-
gorithm. Extensive experiments on 6 real-world partial multi-label
data sets and 7 synthetic data sets demonstrate the effectiveness of
the proposed PML method.
2 RELATED WORK
PML is an emerging and important weakly supervised learning
framework that has received wide attention in recent years. PML
originates from the widely studied MLL paradigm and partial label
learning (PLL) [ 11–13] paradigm. In this section, we will briefly
introduce the existing MLL methods and PML methods.
MLL deals with the problem where each sample is associated with
multiple labels, and the key to solving MLL is to find the correlations
between labels, also called label correlations. According to different
types of label correlations, the current MLL methods can be roughly
divided into three categories: first-order label correlations, second-
order label correlations, and high-order label correlations. First-
order label correlations methods handled each class independently
[2,36], second-order label correlations methods considered the
pairwise correlations between classes [ 8,9,34], and high-order
label correlations methods considered the mutual influence of all
classes [ 5,10,22,23]. However, since there are noisy labels in the
candidate label set, and the ground-true labels cannot be directly
accessed, the existing MLL methods are often difficult to directly
apply to PML.
An intuitive way to solve PML is label disambiguation, i.e., iden-
tifying the ground-truth labels of samples from their candidate
label set. For example, [ 25,26,30] used the low-rank assumption
of the ground-true labels and the sparse assumption of the noisy
labels, hoping to build a model that can simultaneously identify
the noisy label in the candidate label set and consider the label
correlations of ground-truth labels. [ 28,31] achieved label disam-
biguation by leveraging the similarity relationship of samples in
the feature space and label correlations simultaneously, i.e., if two
samples are similar in the feature space, they are likely to share
the same ground-truth labels and if two classes have strong label
correlations, when one class appears, the other class also has a high
probability of appearing. The above methods usually perform label
disambiguation and model training iteratively and alternatively,
they were called end-to-end PML methods. Some works [ 27,33] di-
vided PML into two stages, the first stage was label disambiguation,
usually achieved by label propagation, and the second stage used
the disambiguated labels as the ground-truth labels of samples to
build a multi-label classifier. For example, [33] achieved the multi-
label classification task by using maximum a posteriori reasoning
or virtual label splitting methods after obtaining the disambiguated
labels. [ 27] achieved the classification task by using a gradient
3725Noisy Label Removal for Partial Multi-Label Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
boosting-based model. The above end-to-end and two-stage PML
methods often lack strict constraints on the noisy labels and ignore
valuable information in the non-candidate label set, which limit
their performance.
3 PROPOSED APPROACH
3.1 Generalization Error Bound
In this section, we first analyze the importance of removing noisy
labels from the perspective of the generalization error bound [20].
Denote by X=[𝑥1,𝑥2,···,𝑥𝑚]T∈R𝑚×𝑑the feature matrix, where
𝑚and𝑑represent the number of samples and the dimension of
features, respectively. Let Y=[𝑦1,𝑦2,···,𝑦𝑚]T∈{0,1}𝑚×𝑞be
the candidate label matrix, where 𝑦𝑖=[𝑦𝑖1,𝑦𝑖2,···,𝑦𝑖𝑞]T∈R𝑞is
the label vector for sample 𝑥𝑖and𝑞is the number of classes. Note
𝑦𝑖𝑗=1, if the𝑗-th label is one of the candidate labels of the sample
𝑥𝑖and𝑦𝑖𝑗=0otherwise.
Lemma 1. If we have a linear PML model with the coefficient
matrix W∈R𝑑×𝑞determined by minimizing the loss function
ℓ:∥YG+E−XW∥2
𝐹, where YG∈R𝑚×𝑞andE∈R𝑚×𝑞are the
ground-truth label matrix and noisy label matrix respectively, ||·
||𝐹represents the Frobenius norm of a matrix. Let H=W×E
represent the family of linear functions. Assuming that the square
loss function ℓis2𝑐-Lipschitz, the sparsity of Eand the complexity
ofWare upper bounded by 𝜏and𝜌respectively, i.e.,∥E∥1≤𝜏and
∥W∥𝐹≤𝜌, where||·|| 1represents𝑙1norm of a matrix. Then the
Rademacher complexity of the loss function ℓis upper bounded by
ˆR𝑆(ℓ◦H)≤2𝑐√
2𝜏
𝑚+2𝑐√2𝑚𝑞𝜌
𝑚. (1)
Theorem 1. Let𝑆={𝑥1,𝑥2,...,𝑥𝑚}be a set of fixed samples, if
the loss function ℓis upper bounded by Θ≥0, then for any 𝛿>0,
with probability at least 1−𝛿, for allℎ∈H we have
LG(ℎ)≤LS(ℎ)+2𝑐√
2𝜏
𝑚+2𝑐√2𝑚𝑞𝜌
𝑚+3Θ√︂
𝑙𝑜𝑔(2/𝛿)
2𝑚, (2)
whereLG(ℎ)andLS(ℎ)are generalization error and empirical
error toℎrespectively. The detailed proofs of Lemma 1 andTheo-
rem 1 are given in the Section A of the supplementary file.
Based on Theorem 1 , the generalization error bound of a linear
PML model depends on the sparsity of the noisy matrix Ethat is
controlled by 𝜏. When there are too many noisy labels, i.e., ∥E∥1is
large, the generalization error bound will be enlarged, consequently
leading to the decline of the classification performance. Suppose
the estimated noisy matrix is N∈R𝑚×𝑞,E=Y−N−YGcan be
regarded as the remaining noisy labels. When the estimated label
matrix Nis more accurate,∥Y−N−YG∥1will be smaller, which
means the upper bound of ∥E∥1is smaller, leading to better general-
ization performance. Therefore, the key to solving PML is accurate
noisy label removal.
3.2 Modeling the Noisy Label in PML
According to Theorem 1, the quality of the estimated noisy label
matrix Nsignificantly affects the performance of the model. In
order to remove noisy labels, we exploit the prior knowledge of
the noisy labels in PML and construct the following constrainedregression model:
min
W,N∥(Y−N)− XW∥2
𝐹+𝛾∥W∥2
𝐹+𝜆∥N∥0 (3)
s.t.N∈{0,1}𝑚×𝑞,∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0,
where the first term aims to map the feature matrix to the label
matrix through the coefficient matrix W∈R𝑑×𝑞, and identify the
noisy labels N∈R𝑚×𝑞in the candidate label set. To prevent over-
fitting, the second term is introduced as the regularization term,
which is the square Frobenius norm on Wand𝛾>0is the trade-off
parameter. The third term is the sparse regularization on the label
noisy matrix N, where||·|| 0denotes the 𝑙0norm, i.e., the number
of non-zero elements in the matrix. We assume the noisy labels
should be sparse, and accordingly only the correct noisy labels with
high confidence will be selected during the training. 𝜆>0is the
parameter of the sparse regularization term. The first constraint
N∈{0,1}𝑚×𝑞strictly limits the elements of the noisy label matrix
to 0 or 1. Note N𝑖𝑗=1represents the 𝑗-th label of sample 𝑥𝑖is a
noisy label, N𝑖𝑗=0represents it is not a noisy label. The second
constraint∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0indicates noisy label N𝑖𝑗only
appears when 𝑦𝑖𝑗is one of the candidate labels, and will not appear
in the non-candidate label set. Note (Y−N)indicates the recovered
ground-truth label matrix. By minimizing Eq. (3), we can obtain
the PML classifier coefficient matrix Wand noisy label matrix N.
Since the𝑙0norm is non-convex and difficult to optimize directly,
we relax it by its convex surrogate 𝑙1norm, and then Eq. (3) becomes
min
W,N∥(Y−N)− XW∥2
𝐹+𝛾∥W∥2
𝐹+𝜆∥N∥1 (4)
s.t.N∈{0,1}𝑚×𝑞,∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0,
where||·||1is the𝑙1norm of a matrix, i.e., the sum of absolute values
of all elements in a matrix. Note that as we require the noisy label
matrix Nonly possesses the binary values, the optimal solutions of
Eq. (3) and Eq. (4) are the same.
3.3 Competitive Classifier Promoted Noisy
Label Removal
Sine the candidate label matrix involves the noisy labels, while the
non-candidate labels of a sample precisely indicate the classes to
which the sample does not belong, we can further improve the
classification performance by the information of non-candidate
label set. Suppose ˆY=[ˆ𝑦1,ˆ𝑦2,···,ˆ𝑦𝑚]T∈{0,1}𝑚×𝑞be the non-
candidate label matrix and ˆ𝑦𝑖=[ˆ𝑦𝑖1,ˆ𝑦𝑖2,···,ˆ𝑦𝑖𝑞]T∈R𝑞. Different
from the candidate label matrix, ˆ𝑦𝑖𝑗=1indicates𝑗-th label is not
the ground-truth label of sample 𝑥𝑖. Therefore, we can construct a
competitive classifiers that reflect which classes the samples should
not be assigned to, i.e.,
min
ˆW,N(ˆY+N)− XˆW2
𝐹+𝛾ˆW2
𝐹+𝜆∥N∥1
s.t.N∈{0,1}𝑚×𝑞,∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0.(5)
Similar to Eq. (4), ˆW∈R𝑑×𝑞is the competitive classifier coefficient
matrix and we also used square Frobenius norm on ˆWto control
its complexity. Note that the initial non-candidate label matrix ˆYis
incomplete, as there are some negative labels in the candidate label
set. Accordingly, we can use the noisy label matrix Nto complete the
non-candidate label matrix, i.e., (ˆY+N). By optimizing Eq. (5), the
noisy label removal will be promoted by the competitive classifier.
3726KDD ’24, August 25–29, 2024, Barcelona, Spain. Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, & Junhui Hou
Moreover, the PML classifier in Eq. (4) predicts which labels
should be assigned to a sample, while the competitive classifier
in Eq. (5) predicts which labels should not be assigned to that
sample. These two classifiers form a competitive relationship, i.e., a
larger (resp. smaller) value predicted by the PML classifier implies
a smaller (resp. larger) value predicted by the competitive classifier,
and vice versa. In the ideal case, the above competitive relationship
can be formulated as 1𝑚×𝑞=XW+XˆW, where 1𝑚×𝑞is an all ones
matrix. By incorporating the competitive relationship into Eq. (5),
we have
min
ˆW,N(ˆY+N)− XˆW2
𝐹+𝛽1𝑚×𝑞−XW−XˆW2
𝐹+𝛾∥ˆW∥2
𝐹+𝜆∥N∥1
s.t.N∈{0,1}𝑚×𝑞,∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0,
(6)
where𝛽is a parameter to introduce the competitive relationship.
3.4 Overall Formulation
By combining the PML classifier in Eq. (4) and the competitive
classifier in Eq. (6), we can obtain the final model, i.e.,
min
W,ˆW,N∥(Y−N)− XW∥2
𝐹+(ˆY+N)− XˆW2
𝐹+𝛽1𝑚×𝑞−XW−XˆW2
𝐹
+𝛾(∥W∥2
𝐹+∥ˆW∥2
𝐹)+𝜆∥N∥1 (7)
s.t.N∈{0,1}𝑚×𝑞,∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0.
The existence of noisy labels in the candidate label set makes
PML a challenging problem. However, since the information in the
non-candidate label matrix accurately indicates whether a sample
does not belong to certain classes, it can be regarded as a kind of
accurate supervision information. Moreover, by exploiting the prior
knowledge of the noisy labels and simultaneously identifying noisy
labels through the PML classifier and the competitive classifier, and
the competitive relationship formed by two classifiers, the noisy
labels can be effectively removed, and the ground-true label matrix
can be recovered. To the best of our knowledge, our method is the
first to consider the relationship of the candidate label matrix and
the non-candidate label matrix to guide noisy label removal in PML.
4 NUMERICAL SOLUTION
Our method has three variables: the coefficient matrix W, the com-
petitive coefficient matrix ˆW, and the noisy label matrix N. We can
optimize Eq. (7) by solving the following subproblems alternatively
and iteratively.
4.1 Wsubproblem
By fixing other variables, the Wsubproblem is expressed as
min
W∥(Y−N)− XW∥2
𝐹+𝛽1𝑚×𝑞−XW−XˆW2
𝐹+𝛾∥W∥2
𝐹.(8)
Eq. (8) reaches the minimum when its first-order derivative with
respect to Wvanishes, leading to
W=
(1+𝛽)XTX+𝛾I𝑑×𝑑−1
XT
Y−N+𝛽1𝑚×𝑞−𝛽XˆW
,(9)
where I𝑑×𝑑∈R𝑑×𝑑is an identity matrix.
4.2 ˆWsubproblem
ˆWsubproblem can be represented as
min
ˆW(ˆY+N)− XˆW2
𝐹+𝛽1𝑚×𝑞−XW−XˆW2
𝐹+𝛾∥ˆW∥2
𝐹. (10)Similar to the Wsubproblem, the solution of Eq. (10) is written as
ˆW=
(1+𝛽)XTX+𝛾I𝑑×𝑑−1
XT
ˆY+N+𝛽1𝑚×𝑞−𝛽XW
.(11)
4.3 Nsubproblem
TheNsubproblem can be formulated as
min
N𝑓(N)+𝑔(N)
s.t.N∈{0,1}𝑚×𝑞,∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0,(12)
where𝑓(N)=∥(Y−N)− XW∥2
𝐹+(ˆY+N)− XˆW2
𝐹and𝑔(N)=
𝜆∥N∥1. Both𝑓(N)and𝑔(N)are convex. Furthermore, 𝑓(N)is
Lipschitz continuous gradient [ 1], i.e.,∥∇𝑓(N1)−∇𝑓(N2)∥𝐹≤
𝐿𝑓∥ΔN∥𝐹, where ΔN=N1−N2and𝐿𝑓is the Lipschitz constant.
Therefore, we can use the iterative shrinkage thresholding algo-
rithm (ISTA) [ 1] to solve it. Specifically, ∇𝑓(N)is the gradient of
𝑓(N)and can be calculate as
∇𝑓(N)=4N−2Y+2ˆY+2XW−2XˆW. (13)
According to Eq. (13), given N1andN2, we have
∥∇𝑓(N1)−∇𝑓(N2)∥𝐹=∥4(N1−N2)∥𝐹≤𝐿𝑓∥ΔN∥𝐹. (14)
Therefore, we set the Lipschitz constant 𝐿𝑓to 4, i.e.,𝐿𝑓=4. Ac-
cording to Eqs. (12), (13) and (14), Nsubproblem can be optimized
by
N=arg min
N𝐿𝑓
2∥N−G(𝑡)∥2
𝐹+𝑔(N)=arg min
N1
2∥N−G(𝑡)∥2
𝐹+𝜆
𝐿𝑓∥N∥1
s.t.N∈{0,1}𝑚×𝑞,∀𝑖,𝑗,N𝑖𝑗=0,if𝑦𝑖𝑗=0, (15)
where G(𝑡)=N(𝑡)−1
𝐿𝑓∇𝑓(N(𝑡))and𝑡indicates the 𝑡-th iteration.
the solution of Eq. (15) can be written as
N(𝑡+1)=T
T𝑠𝑔𝑛
S𝜆/𝐿𝑓
G(𝑡)
, (16)
whereTis the thresholding operator in elementwise, i.e., T(A𝑖𝑗)=
0, if𝑦𝑖𝑗=0.T𝑠𝑔𝑛(𝑎)is the sign function which returns 1for𝑎>0
and0for𝑎≤0.S𝜆/𝐿𝑓is the shrinkage operator, i.e.,
S𝜖(𝑎)= 
𝑎−𝜖,if𝑎>𝜖,
𝑎+𝜖,if𝑎<−𝜖,
0,otherwise.(17)
Algorithm 1 summarizes the pseudo code of our method.
4.4 Complexity Analysis
The computational complexity of Algorithm 1 is mainly determined
by steps 3-5. Specifically, steps 3 and 4 involve the matrices mul-
tiplication with the complexity of 𝑂(𝑚𝑑𝑞). Step 5 solves an ISTA
algorithm, leading to the complexity of 𝑂(𝑚𝑑𝑞). Therefore, the
overall computational complexity of Algorithm 1 in each iteration
is𝑂(3𝑚𝑑𝑞). More analysis about computational complexity can be
found in Section B of the supplementary file.
5 EXPERIMENTS
5.1 Experimental Setting
5.1.1 Data sets. We evaluated the performance of our method on
13 data sets, including 6 real-world partial-multi label data sets1
and 7 synthetic data sets2, where the synthetic partial multi-label
1http://palm.seu.edu.cn/zhangml/
2http://www.uco.es/kdis/mllresources/
3727Noisy Label Removal for Partial Multi-Label Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 1: The Pseudo Code of the Proposed Method
Input:
D: the partial multi-label training set;
𝛽,𝛾,𝜆 : the parameters of model;
Output:
W: the coefficient matrix;
ˆW: the competitive coefficient matrix;
N: the noisy matrix.
1Initialize
W=ˆW=0𝑑×𝑞,N(0)=0𝑛×𝑞,𝑚𝑎𝑥 _𝑖𝑡𝑒𝑟𝑎𝑡𝑖𝑜𝑛 =100.
2for𝑡=0to max_iteration do
3 Update Wby Eq. (9);
4 Update ˆWby Eq. (11);
5 Update N(𝑡+1)by Eq. (16);
6end
7Return result.
Table 1: Characteristics of the synthetic data sets and the
real-world partial multi-label data sets, where AVG. CLs and
AVG. GLs represent the average number of candidate labels
per sample and the average number of ground- truth labels
per sample respectively.
T
ype Data
set Examples Features Classes Avg. CLs Avg. GLs
Real-w
orld
Data SetMusic_emotion
6833 98 11 5.29 2.42
Music_style
6839 98 10 6.04 1.44
Y
eastMF 1408 6139 39 6.54 3.54
Y
eastCC 1771 6139 50 9.30 4.30
Y
eastBP 3794 6139 217 18.84 8.84
Mirflickr
10433 100 7 3.35 1.77
Synthetic
Data
SetScene
2407 294 6 - 1.07
Bir
ds 645 260 19 - 1.01
Me
dical 978 1449 45 - 1.25
Enr
on 1702 1001 53 - 3.38
Chemistr
y 6961 540 175 - 2.11
Chess
1675 585 227 - 2.41
P
hilosophy 3971 842 233 - 2.27
data sets are constructed from the widely used multi-label data
sets. Following [ 17,18], we randomly selected 𝑟labels from the
negative labels as the noisy labels for each sample. Therefore, the
candidate label set of synthetic data sets consist of all ground-truth
labels and𝑟noisy labels. For data sets with less than 10 classes, we
chose𝑟∈{1,2,3}, for data sets with more than 10 classes and less
than 100 classes, we chose 𝑟∈{3,7,11}, and for data sets with more
than 100 classes, we chose 𝑟∈{10,20,30}. Table 1 shows the detailed
information of the data sets, where AVG. CLs represents the average
number of candidate labels per sample and AVG. GLs represents the
average number of ground-truth labels per sample. Note that, in the
experiments, we found that many samples in the real-world partial
multi label data sets YeastMF, YeastCC, and YeastBP do not have
ground-truth labels and only contain a few noisy labels. Therefore,
in the experiments, we removed the samples without ground-truth
labels and randomly added noisy labels to the remaining samples.Specifically, we added 3, 5, and 10 noisy labels to each sample in
data sets YeastMF, YeastCC, and YeastBP, respectively.
5.1.2 Comparing methods. We compared our method with 9 state-
of-the-art PML methods and each method was configured with the
suggested parameters in the literature, i.e., FPML (ICDM 2018) [ 32],
PML-LRS (AAAI 2019) [ 26], PML-NI (TPAMI 2022) [ 30], PARTICLE-
MAP and PARTICLE-VLS (P-MAP and P-VLS for short (TPAMI
2021) [ 33]), PAKS (TCyber 2023) [ 17], GLC (TMM 2022) [ 25] and
PARD (NeurIPS 2023) [ 7]. In addition, we used five widely used
multi-label metrics [ 6,37] to evaluate the performance of each
comparing method, i.e., hamming loss, rank loss, one-error, coverage
andaverage precision. Five-fold cross-validation was performed on
each data set and the mean metric value and the standard deviation
were recorded. Due to the page limit, the experiment results on
hamming loss and coverage are shown in the Section C of the
supplementary file.
5.1.3 Parameter settings. Our method contains three parameters,
parameter𝛾controls the model complexity, parameter 𝛽controls
the competitive relationship, and parameter 𝜆controls the sparse
regularization term. In the experiments, parameters 𝛾and𝛽were
selected from{0.001,0.01,...,1000}, and𝜆was tuned from[1,4]
with step size 0.1 each time. The total number of iterations was set
to 100.
5.2 Experimental Results
5.2.1 Classification performance. Tables 2, 3 and 4 report the exper-
imental results on metrics: rank loss, one-error andaverage precision .
Tables S3 and S4 in the Section C of supplementary file show the
experimental results on hamming loss andcoverage. According to
these tables, our method NLR achieved the best performance in
84.3% (118/140) of all five evaluation metrics. Taking rank loss as
an example, on the real-world data sets YeastMF and YeastCC, NLR
improves the rank loss of the best comparison from 0.229 to 0.200
and from 0.167 to 0.144, respectively. Taking average precision as an
example, on the data sets Scene ( 𝑟= 3) and Medical ( 𝑟= 11), NLR
increases the average precision of the best comparison from 0.735 to
0.822 and from 0.776 to 0.842, respectively. As the number of noisy
labels𝑟increases, it becomes more difficult to find the ground-truth
labels. Accordingly, the average precision of all the PML methods
decreases, but our method NLR still maintains a high average pre-
cision, which demonstrates that our method is effective in more
challenging PML tasks. Furthermore, different methods have differ-
ent characteristics and are suited to different data sets. For example,
PML-NI performs well on data set Medical, but poorly on others;
PAKS is good at data sets Enron and Philosophy, but badly at Scene
and Birds. However, our method performs excellently on almost all
data sets, proving its robustness.
5.2.2 Significance test. Table 5 reports win/tie/loss counts of our
method NLR against each comparing method on the five multi-label
metrics. The results are based on the pairwise t-test at 0.05signif-
icance level. As shown in the table, we can find that our method
significantly outperforms other methods in five evaluation metrics
hamming loss, rank loss, one-error, coverage, and average precision
by 75.0%, 83.8%, 83.8%, 77.3%, and 88.0%, respectively. Moreover,
our method achieves superior performance than deep learning PML
3728KDD ’24, August 25–29, 2024, Barcelona, Spain. Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, & Junhui Hou
Table 2: Experimental results on rank loss (the smaller the better ↓). Bold and underlined indicate the best and second best
results respectively. •/◦indicates whether the rank loss of our method is statistically superior/inferior to the compared method.
Data 𝑟 NLR
FPML[32] PML-LRS[26] PML-NI[30] P-MAP[33] P-VLS[33] PAKS[17] GLC[25] PARD[7]
Music_emotion .238±.007 .292±.006• .242±.006 .245±.007
.259±.013• .268±.007• .243±.007 .242±.007 .246±.007
Music_style .137±.005 .173±.004• .146±.006• .139±.005 .147±.005• .167±.006• .141±.006
.142±.006 .145±.007
YeastMF .200±.009 .320±.022• .321±.019• .267±.021• .370±.015• .414±.013• .229±.006•.310±.020• .299±.020•
Y
eastCC .144±.008 .254±.016• .312±.010• .232±.012• .316±.021• .351±.017• .167±.007•.288±.011• .331±.017•
Y
eastBP .191±.006 .252±.008• .354±.007• .285±.008• .368±.012• .403±.007• .210±.006•.342±.008• .262±.009
Mirflickr
.130±.014 .158±.010• .126±.005 .125±.007 .088±.005◦.192±.016• .131±.005
.127±.004 .133±.004
Scene1.093±.007 .215±.025• .167±.009• .155±.008• .104±.006•.123±.006• .150±.014• .166±.009• .168±.004•
2.098±.011 .220±.016• .221±.013• .206±.013• .127±.014•.150±.009• .210±.019• .220±.012• .221±.013•
3.111±.008 .241±.025• .291±.004• .281±.005• .187±.009•.208±.006• .293±.009• .290±.005• .291±.006•
Bir
ds3.219±.033 .326±.038• .246±.048
.222±.036 .323±.023• .360±.015• .278±.041• .228±.044
.340±.032•
7.291±.023 .350±.033• .404±.026• .346±.044• .381±.018• .397±.036• .300±.027 .392±.032• .369±.040•
11 .332±.038 .395±.025• .430±.012• .392±.022• .473±.031• .480±.035• .353±.017 .420±.016• .402±.016•
Me
dical3.026±.006 .046±.013• .051±.005• .026±.009 .091±.013• .103±.026• .094±.010• .046±.007• .031±.009
7
.045±.009 .095±.012• .139±.013• .043±.010 .102±.006• .110±.011• .113±.005• .086±.011• .047±.012
11 .048±.007 .096±.016• .195±.018• .054±.008 .113±.013• .107±.014• .119±.015• .114±.018• .058±.007
Enr
on3.092±.008 .139±.023• .225±.016• .146±.012• .117±.005• .185±.006• .099±.007 .224±.017• .110±.010•
7.097±.008 .149±.020• .277±.011• .171±.014• .124±.004• .191±.007• .111±.008•.254±.014• .132±.013•
11 .106±.007 .156±.009• .318±.008• .196±.015• .140±.003• .186±.014• .117±.008•.281±.011• .148±.014•
Chemistr
y10 .139±.004 .163±.007• .214±.007• .190±.007• .210±.005• .199±.010• .177±.007• .221±.006• .139±.017
20 .153±.004 .182±.007• .248±.006• .222±.006• .241±.006• .225±.014• .207±.006• .253±.006• .154±.005
30 .164±.005 .203±.009• .272±.006• .245±.007• .280±.008• .239±.007• .229±.008• .276±.006• .176±.008•
Chess10 .136±.010 .150±.010
.243±.011• .200±.011• .197±.011• .243±.017• .149±.008•.256±.010• .136±.009
20 .152±.011 .168±.007• .290±.010• .235±.010• .216±.015• .240±.019• .171±.011• .289±.011• .170±.009•
30 .165±.008 .184±.009• .317±.007• .259±.008• .238±.012• .263±.015• .195±.009• .315±.008• .186±.008•
P
hilosophy10 .143±.010 .159±.008• .240±.007• .210±.009• .199±.005• .199±.009• .160±.011• .247±.007• .134±.008
20 .163±.010 .182±.008• .271±.004• .240±.006• .209±.008• .214±.009• .188±.010• .275±.005• .165±.010
30 .178±.011 .202±.010• .292±.005• .259±.005• .241±.008• .232±.011• .207±.010• .295±.005• .184±.008
Table 3: Experimental results on one-error (the smaller the better ↓). Bold and underlined indicate the best and second best
results respectively. •/◦indicates whether the one-error of Our method is statistically superior/inferior to the compared method.
Data 𝑟 NLR
FPML[32] PML-LRS[26] PML-NI[30] P-MAP[33] P-VLS[33] PAKS[17] GLC[25] PARD[7]
Music_emotion
.448±.015 .561±.009• .459±.014
.477±.016• .524±.029• .362±.051◦.466±.019
.461±.016 .467±.019
Music_style .334±.010 .399±.012• .344±.015
.343±.012 .367±.003• .358±.008• .361±.006• .343±.016 .348±.016
Y
eastMF .528±.019 .759±.033• .744±.028• .634±.027• .952±.018• .859±.047• .593±.009•.705±.032• .736±.028
Y
eastCC .343±.018 .536±.024• .717±.012• .544±.013• .845±.018• .722±.026• .366±.021 .675±.010• .707±.033•
Y
eastBP .507±.008 .634±.007• .817±.010• .672±.013• .822±.028• .749±.026• .531±.018•.783±.019• .643±.016
Mirflickr
.268±.047 .257±.049 .294±.016
.289±.029 .142±.008◦.270±.185
.339±.015• .300±.015 .329±.015•
Scene1.245±.018 .508±.046• .389±.023• .381±.023• .300±.024• .256±.014 .364±.034• .389±.023• .401±.045•
2.258±.029 .503±.029• .495±.027• .477±.029• .329±.021• .271±.026 .469±.033• .491±.025• .503±.028•
3.287±.026 .516±.042• .599±.012• .587±.013• .437±.021• .343±.007•.596±.019• .597±.013• .606±.014•
Bir
ds3.460±.055 .711±.045• .549±.057• .497±.070 .696±.100• .641±.059• .602±.057• .520±.055
.842±.060•
7.547±.033 .789±.072• .836±.044• .765±.034• .863±.028• .699±.050• .601±.059 .856±.049• .891±.008•
11 .663±.028 .800±.071• .867±.024• .827±.068• .913±.058• .800±.037• .752±.054•.839±.050• .919±.039•
Me
dical3.143±.012 .200±.012• .277±.015• .153±.010 .408±.034• .243±.025• .499±.011• .258±.019• .189±.018•
7.158±.021 .317±.025• .592±.031• .204±.018• .437±.036• .305±.045• .503±.021• .416±.019• .224±.033•
11 .181±.013 .322±.023• .729±.043• .276±.028• .446±.069• .303±.030• .503±.023• .540±.053• .274±.026•
Enr
on3 .240±.022 .318±.028• .506±.026• .297±.024• .291±.012• .424±.070• .238±.025 .451±.011• .242±.016
7.240±.024 .308±.025• .687±.013• .358±.014• .311±.062• .505±.102• .241±.025 .579±.020• .257±.014
11
.246±.019 .306±.023• .769±.022• .443±.019• .348±.032• .561±.111• .245±.021 .670±.009• .273±.025
Chemistr
y10 .576±.012 .631±.010• .629±.004• .609±.010• .764±.016• .659±.016• .577±.006 .628±.009• .603±.012•
20 .587±.010 .644±.012• .690±.012• .655±.015• .786±.015• .695±.016• .616±.011• .692±.012• .610±.010•
30 .595±.012 .656±.013• .743±.014• .699±.016• .830±.014• .738±.017• .651±.013• .743±.016• .622±.010•
Chess10 .455±.016 .477±.012• .636±.016• .561±.015• .693±.028• .698±.053• .481±.015• .659±.018• .457±.007
20 .464±.010 .484±.017
.769±.013• .624±.020• .695±.014• .743±.068• .495±.007• .750±.021• .477±.006•
30 .476±.011 .505±.028
.849±.007• .699±.021• .685±.011• .847±.048• .487±.017•.830±.009• .517±.020•
P
hilosophy10 .469±.012 .515±.012• .607±.020• .562±.011• .744±.013• .710±.018• .477±.005 .605±.020• .493±.013•
20 .479±.014 .523±.011• .706±.022• .639±.017• .756±.018• .665±.022• .495±.005•.703±.020• .520±.014•
30 .491±.007 .532±.010• .767±.019• .703±.015• .769±.026• .682±.016• .533±.007• .764±.021• .546±.017•
3729Noisy Label Removal for Partial Multi-Label Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 4: Experimental results on average precision (the larger the better ↑). Bold and underlined indicate the best and second
best results respectively. •/◦indicates whether the average precision of Our method is statistically superior/inferior to the
compared method.
Data 𝑟 NLR
FPML[32] PML-LRS[26] PML-NI[30] P-MAP[33] P-VLS[33] PAKS[17] GLC[25] PARD[7]
Music_emotion .623±.011 .556±.007• .616±.010 .608±.011
.589±.020• .611±.012 .613±.012 .615±.011 .608±.011
Music_style .743±.007 .689±.005• .733±.008• .738±.008 .722±.005• .717±.007• .728±.006• .736±.010
.732±.011
YeastMF .510±.016 .338±.026• .324±.018• .417±.024• .224±.012• .238±.022• .466±.010•.340±.017• .359±.023•
Y
eastCC .644±.012 .481±.021• .324±.010• .455±.009• .276±.011• .313±.020• .620±.015•.354±.008• .333±.025•
Y
eastBP .425±.010 .328±.012• .160±.006• .255±.007• .160±.011• .158±.010• .399±.014•.176±.007• .308±.013•
Mirflickr
.793±.010 .783±.015
.788±.008 .790±.008 .864±.006◦.698±.004• .776±.008• .786±.007
.775±.008•
Scene1.849±.010 .677±.029• .751±.014• .761±.013• .821±.013• .822±.010•.771±.020• .752±.014• .747±.029•
2.840±.017 .674±.020• .679±.016• .694±.018• .798±.015•.796±.012• .696±.023• .681±.015• .677±.015•
3.822±.016 .659±.028• .602±.006• .612±.006• .723±.013• .735±.010•.604±.011• .604±.007• .600±.008•
Bir
ds3.566±.034 .387±.038• .513±.056
.540±.027 .387±.039• .383±.019• .463±.045• .539±.049
.378±.064•
7.484±.015 .351±.037• .300±.023• .351±.035• .287±.015• .341±.029• .449±.038 .297±.032• .332±.033•
11 .411±.037 .314±.034• .278±.017• .302±.037• .232±.048• .272±.028• .352±.040•.286±.025• .286±.027•
Me
dical3.882±.012 .832±.017• .777±.011• .876±.012 .662±.033• .745±.060• .614±.025• .795±.012• .852±.014•
7.858±.015 .722±.030• .510±.025• .828±.015• .643±.019• .695±.051• .601±.024• .657±.021• .811±.022•
11 .842±.008 .715±.025• .381±.032• .776±.020• .623±.043• .703±.012• .595±.035• .553±.043• .773±.017•
Enr
on3 .674±.010 .586±.013• .451±.016• .606±.020• .598±.015• .488±.024• .676±.013 .470±.019• .665±.018
7.669±.014 .586±.022• .339±.010• .546±.020• .584±.016• .433±.044• .664±.017 .390±.013• .629±.022•
11 .660±.015 .579±.018• .274±.012• .488±.022• .548±.021• .412±.063• .656±.018 .333±.011• .597±.022•
Chemistr
y10 .426±.004 .373±.008• .339±.006• .363±.007• .250±.009• .298±.015• .395±.005• .335±.006• .406±.007•
20 .409±.007 .350±.006• .280±.006• .313±.007• .224±.009• .264±.010• .345±.006• .277±.006• .381±.008•
30 .395±.007 .328±.004• .238±.005• .273±.004• .178±.010• .234±.009• .308±.005• .236±.005• .355±.006•
Chess10 .447±.013 .429±.012
.289±.010• .346±.008• .256±.014• .246±.011• .421±.014• .266±.010• .437±.009
20 .428±.012 .406±.009• .192±.007• .283±.007• .250±.006• .221±.021• .400±.012• .201±.007• .393±.006•
30 .411±.014 .385±.019• .144±.006• .234±.009• .239±.011• .165±.027• .381±.014• .155±.007• .359±.012•
P
hilosophy10 .468±.015 .419±.011• .317±.012• .355±.010• .273±.006• .267±.013• .451±.008 .315±.011• .444±.008•
20 .446±.012 .396±.010• .244±.013• .293±.009• .260±.006• .275±.019• .410±.009•.244±.013• .404±.007•
30 .430±.010 .379±.011• .200±.015• .247±.010• .231±.015• .260±.009• .373±.006• .201±.015• .375±.007•
Table 5: Win/tie/loss counts on the five multi-label metrics of our method NLR against each comparing method on real-world
data sets and synthetic data sets according to the pairwise t-test at 0.05significance level.
FPML[32]
PML-LRS[26] PML-NI[30] P-MAP[33] P-VLS[33] PAKS[17] GLC[25] PARD[7] Total
Hamming
Loss 22/5/0 26/1/0 22/5/0 24/2/1 13/11/3 14/13/0 24/3/0 17/3/7 162/43/11
Rank Loss 26/1/0 24/3/0 20/7/0 26/0/1 27/0/0 21/6/0 23/4/0 14/13/0 181/34/1
One Error 24/3/0 24/3/0 23/4/0 26/0/1 23/3/1 19/8/0 23/4/0 19/8/0 181/33/2
Coverage 25/2/0 23/4/0 18/9/0 24/2/1 22/5/0 18/9/0 23/4/0 14/13/0 167/48/1
Average Precision 25/2/0 24/3/0 22/5/0 26/0/1 26/1/0 21/6/0 23/4/0 23/4/0 190/25/1
T
otal 122/13/0 121/14/0 105/30/0 126/4/5 111/20/4 93/42/0 116/19/0 87/41/7 881/183/16
method PARD by 64.4%. By combining all methods with all evalua-
tion metrics, our method significantly outperforms other comparing
methods in 81.6% cases, which demonstrates its effectiveness and
robustness in PML tasks.
5.2.3 Noisy label identification. Fig. 2 compares the noisy label
identification ability of our method with three existing PML meth-
ods: PML-LRS, PML-NI, and GLC. The experiments were conducted
on data sets Scene ( 𝑟= 3) and Birds ( 𝑟= 11). The blue bars denote the
total number of noisy labels of each class in the data set, while the
orange bars indicate the number of noisy labels that are correctly
identified by each method for each class. The green bars represent
the number of labels that are false identified as noisy labels for
each class, i.e., treating the ground-true labels or the non-candidate
labels as the noisy labels. Additionally, each subfigure also displaystheprecision, recall andF1 Score for noisy label identification. Ac-
cording to Fig. 2 (a) - (h), PML-LRS and GLC have poor performance
when identifying noisy labels on both data sets. They only correctly
identify a small fraction of the noisy labels. Consequently, their
F1 Scores are very low, ranging from 14.4% to 38.7%. PML-NI iden-
tify more noisy labels than PML-LRS and GLC, as indicated by the
higher height of the orange bars. However, PML-NI suffers from a
high rate of false identification, as indicated by the high height of
thegreen bars. In contrast, our method outperforms other methods
by a large margin on both data sets. It identifies the majority of
the noisy labels correctly, as indicated by the highest height of the
orange bars and it also ensures a lower false identification rate.
Specifically, our method achieves the highest F1 Score 82.3% on data
sets Scene and 88.6% on data set Birds respectively. These results
demonstrate that our method is very effective and robust in noisy
label identification for PML.
3730KDD ’24, August 25–29, 2024, Barcelona, Spain. Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, & Junhui Hou
Precision: 93.8%  Recall: 24.4%  F1 Score: 38.7%
0 2 4 6 8 10 12 14 16 18 20
GLC050100150200250300350
Number of noisy labels Number of correct identifications Number of false identifications
Precision: 94.9%  Recall: 72.6%  F1 Score: 82.3%
1 2 3 4 5 6
Ours01002003004005006007008009001000
Scene(r = 3)
(
a) Ours
Precision: 74.3%  Recall: 17.1%  F1 Score: 27.8%
1 2 3 4 5 6
PML-LRS01002003004005006007008009001000
Scene(r = 3) (b) PML-LRS
Precision: 50.0%  Recall: 62.5%  F1 Score: 55.6%
1 2 3 4 5 6
PML-NI01002003004005006007008009001000
Scene( r = 3) (c) PML-NI
Precision: 65.2%  Recall: 8.1%  F1 Score: 14.4%
1 2 3 4 5 6
GLC01002003004005006007008009001000
Scene(r = 3) (d) GLC
Precision: 94.8%  Recall: 83.2%  F1 Score: 88.6%
0 5 10 15 20
Ours050100150200250300350
Birds(r = 11)
(e) Ours
Precision: 92.8%  Recall: 20.6%  F1 Score: 33.7%
0 5 10 15 20
PML-LRS050100150200250300350
Birds(r = 11) (f) PML-LRS
Precision: 68.1%  Recall: 73.3%  F1 Score: 70.6%
0 5 10 15 20
PML-NI050100150200250300350
Birds(r = 11) (g) PML-NI
Precision: 93.8%  Recall: 24.4%   F1 Score: 38.7%
0 5 10 15 20
GLC050100150200250300350
Birds(r = 11) (h) GLC
Figure 2: Noisy label performance of four noisy identification PML methods on two data sets. (a) - (d) on data set Scene ( 𝑟=3)
and (e) - (h) on data set Birds ( 𝑟=11). The horizontal axis represents the class index of each data set, the blue bars indicates
the total number of noisy labels for each class in the data set, the orange bars represents the number of noisy labels that are
correctly identified for each class, and the green bars represent the number of labels that are false identified as noisy labels for
each class, i.e., mistaking the ground-true labels or the non-candidate labels as noisy labels. The precision, recall andF1 Score
for noisy identification are displayed at the top of each graph.
Table 6: Ablation study of our method in terms of rank loss andaverage precision on synthetic data sets. NLR- 𝑣1, NLR-𝑣2, and
NLR-𝑣3represent our method without the competitive classifier, without the 𝑙1norm of the noisy label matrix, and without the
constraint on the noisy label matrix, respectively.
Evaluation Metric rank loss (the smaller the better ↓) average precision (the larger the better ↑)
Version NLR NLR- 𝑣1 NLR-𝑣2 NLR-𝑣3 NLR NLR- 𝑣1 NLR-𝑣2 NLR-𝑣3
Scene (𝑟=3) .111±.008 .114±.010
.529±.013 .159±.012 .822±.016 .818±.016
.404±.008 .763±.015
Birds (𝑟=11) .332±.038 .459±.020
.583±.116 .337±.013 .411±.037 .270±.020
.226±.023 .351±.046
Medical (𝑟=11) .048±.007 .054±.003
.096±.007 .066±.011 .842±.008 .806±.005
.672±.009 .707±.031
Enron (𝑟=11) .106±.007 .111±.010
.138±.013 .111±.010 .660±.015 .652±.018
.547±.021 .653±.018
Chemistry ( 𝑟=30) .164±.005 .182±.006
.240±.003 .182±.006 .395±.007 .367±.006
.260±.011 .369±.006
Chess (𝑟=30) .165±.008 .176±.011
.271±.018 .175±.011 .411±.014 .397±.009
.235±.009 .399±.009
Philosophy ( 𝑟=30) .178±.011 .186±.009
.251±.010 .186±.009 .430±.010 .400±.008
.284±.008 .401±.008
Average .158±.012 .183±.010
.301±.026 .174±.010 .567±.015 .530±.023
.375±.013 .520±.038
5.3 Further Analysis
5.3.1 Ablation study. In table 6, we conduct ablation studies to
demonstrate the necessity of each component in our method. NLR-
𝑣1, NLR-𝑣2, and NLR- 𝑣3represent our method without the com-
petitive classifier, without the 𝑙1norm of the noisy label matrix,and without the constraint on the noisy label matrix, respectively.
From table 6, we can find that compared with NLR- 𝑣1, considering
the non-candidate label set information improves the model perfor-
mance on rank loss (the smaller the better ↓) by 2.5% and on average
precision (the larger the better ↑) by 3.7%. Compared with NLR- 𝑣2,
3731Noisy Label Removal for Partial Multi-Label Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
10-310-210-11001011021030.20.40.60.81
Average Precision
Music_style
Birds (r = 3)
Medical ( r = 3)
(a) Varying𝛽
10-310-210-11001011021030.20.40.60.81
Average Precision
Music_style
Birds (r = 3)
Medical ( r = 3) (b) Varying𝛾
1 1.5 2 2.5 3 3.5 40.20.40.60.81
Average Precision
Music_style
Birds (r = 3)
Medical ( r = 3) (c) Varying𝜆
0 20 40 60 80 100
t012345||Wt+1-Wt||FMusic_style
Birds (r = 3)
Medical ( r = 3) (d) Convergence
Figure 3: Parameters sensitivity and convergence curves of our method. (a-c) average precision of our method on data set
Music_style, Birds ( 𝑟=3) and Medical ( 𝑟=3) by varying 𝛽,𝛾and𝜆; (d) convergence curves of our method.
Scene BirdsMedicalEnron
ChemistryChess
Philosophy00.511.522.533.544.55Running time in log10 scale (s)FPML
PML-LRS
PML-NI
P-MAP
P-VLS 
PAKS  
GLC  
PARD 
NLR(Ours)
Figure 4: Running time comparison of different methods.
removing the 𝑙1norm leads to substantial performance degradation,
which proves that the 𝑙1norm effectively captures the label noise
pattern. Compared with NLR- 𝑣3, imposing the constraint on the
noisy label matrix improves the rank loss by 1.6% and increases the
average precision by 4.7% on average. The ablation study validates
the effectiveness of each component in our method and taking all
components into account is the best choice.
5.3.2 Parameters sensitivity. Fig. 3 (a)-(c) reports the parameters
sensitivity of our method. All experiments were conducted on data
sets Music_style, Birds ( 𝑟=3), and Medical ( 𝑟=3), and the ex-
periment results of average precision were recorded. Fig. 3 (a)-(b)
report the performance changes of our method when adjusting
parameters𝛽and𝛾, which control the competitive learning and the
complexity of two coefficient matrices, respectively. We find that
there is an optimal range for both parameters, and too large or too
small values will lead to relatively poor performance. In Fig. 3 (c),
we show the performance changes when adjusting the parameter
𝜆, which controls the sparse regularization term, i.e., the number of
noisy labels identified by the model. We vary 𝜆in the range of [1,4]
with a step size of 0.1. We can observe that the model is relativesensitive to 𝜆, as it has an important impact on the model’s ability
to identify the noisy labels. Therefore, parameter 𝜆needs to be
carefully tuned.
5.3.3 Convergence analysis and comparison of running time. Fig. 3
(d) reports the convergence curves of our model, where the vertical
axis represents the change of the PML classifier between the current
iteration and the previous iteration, i.e.,W𝑡+1−W𝑡𝐹, where𝑡+1
represents the current iteration. It can be seen that our method
empirically converges well on all data sets. Fig. 4 reports the running
time of all PML methods. The running time is shown in a log10
scale, i.e., log10(time+1). From Fig. 4, we find that our method
ranks fourth on average in running time, demonstrating that our
method not only achieves superior classification performance but
is also competitive with existing methods in running time.
6 CONCLUSION
In this paper, we propose a novel noisy label removal method named
NLR for PML. Specifically, we theoretically prove the importance
of removing noisy labels from the perspective of the generalization
error bound. To identify noisy labels, we design a model consists
of a PML classifier that predicts which classes the sample should
belong to and a competitive classifier that predicts which classes
the sample should not belong to. These two classifiers form an
competitive relationship to learn from each other and jointly select
the noisy labels. Moreover, we make full use of prior knowledge
of noisy labels in PML by setting strict constraints on our model.
Extensive experiments and comparisons on 6 real-world partial
multi-label data sets and 7 synthetic data sets have shown that our
method achieves significant improvements over the state-of-the-art
PML methods in terms of noisy label identification and achieves
superior performance on five widely used multi-label metrics.
7 ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science
Foundation of China under Grant 62106044, in part by the Natural
Science Foundation of Jiangsu Province under Grant BK20210221,
and in part by the Hong Kong UGC under grant UGC/FDS11/E02/22.
3732KDD ’24, August 25–29, 2024, Barcelona, Spain. Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, & Junhui Hou
REFERENCES
[1]Amir Beck and Marc Teboulle. 2009. A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM J. Imaging Sci. 2, 1 (2009), 183–202.
[2]Matthew R. Boutell, Jiebo Luo, Xipeng Shen, and Christopher M. Brown. 2004.
Learning multi-label scene classification. Pattern Recognit. 37, 9 (2004), 1757–
1771.
[3]Xiang Cheng, Shu-Guang Zhao, Xuan Xiao, and Kuo-Chen Chou. 2017. iATC-
mISF: a multi-label classifier for predicting the classes of anatomical therapeutic
chemicals. Bioinform. 33, 16 (2017), 2610.
[4]André Elisseeff and Jason Weston. 2001. A kernel method for multi-labelled
classification. In Advances in Neural Information Processing Systems 14. MIT Press,
681–687.
[5]Lei Feng, Bo An, and Shuo He. 2019. Collaboration Based Multi-Label Learning. In
The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019. 3550–3557.
[6]Eva Gibaja and Sebastián Ventura. 2015. A Tutorial on Multilabel Learning. ACM
Comput. Surv. 47, 3 (2015), 52:1–52:38.
[7]Jun-Yi Hang and Min-Ling Zhang. 2023. Partial Multi-Label Learning with
Probabilistic Graphical Disambiguation. In Thirty-seventh Conference on Neural
Information Processing Systems.
[8]Jun Huang, Guorong Li, Qingming Huang, and Xindong Wu. 2015. Learning
Label Specific Features for Multi-label Classification. In 2015 IEEE International
Conference on Data Mining, ICDM 2015. 181–190.
[9]Jun Huang, Guorong Li, Qingming Huang, and Xindong Wu. 2018. Joint Feature
Selection and Classification for Multilabel Learning. IEEE Trans. Cybern. 48, 3
(2018), 876–889.
[10] Shuiwang Ji, Lei Tang, Shipeng Yu, and Jieping Ye. 2008. Extracting shared
subspace for multi-label classification. In The 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 381–389.
[11] Yuheng Jia, Jiahao Jiang, and Yongheng Wang. 2023. Semantic Dissimilarity
Guided Locality Preserving Projections for Partial Label Dimensionality Reduc-
tion. In 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
ACM, 964–973.
[12] Yuheng Jia, Xiaorui Peng, Ran Wang, and Min-Ling Zhang. 2024. Long-Tailed
Partial Label Learning by Head Classifier and Tail Classifier Cooperation. In
Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024. 12857–12865.
[13] Yuheng Jia, Fuchao Yang, and Yongqiang Dong. 2023. Partial Label Learning
with Dissimilarity Propagation guided Candidate Label Shrinkage. In Advances
in Neural Information Processing Systems 36.
[14] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin,
Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, Tom Duerig, and Vittorio Ferrari. 2020. The Open Images Dataset
V4.Int. J. Comput. Vis. 128, 7 (2020), 1956–1981.
[15] Hanjiang Lai, Pan Yan, Xiangbo Shu, Yunchao Wei, and Shuicheng Yan. 2016.
Instance-Aware Hashing for Multi-Label Image Retrieval. IEEE Trans. Image
Process. 25, 6 (2016), 2469–2479.
[16] Bing-Qing Liu, Bin-Bin Jia, and Min-Ling Zhang. 2023. Towards Enabling Binary
Decomposition for Partial Multi-Label Learning. IEEE Trans. Pattern Anal. Mach.
Intell. 45, 11 (2023), 13203–13217.
[17] Gengyu Lyu, Songhe Feng, Yi Jin, Tao Wang, Congyan Lang, and Yidong Li. 2023.
Prior Knowledge Regularized Self-Representation Model for Partial Multilabel
Learning. IEEE Trans. Cybern. 53, 3 (2023), 1618–1628.
[18] Gengyu Lyu, Songhe Feng, and Yidong Li. 2020. Partial Multi-Label Learning via
Probabilistic Graph Matching Mechanism. In The 26th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. ACM, 105–113.[19] Andreas Maurer. 2016. A Vector-Contraction Inequality for Rademacher Com-
plexities. In Algorithmic Learning Theory - 27th International Conference, ALT,
Vol. 9925. 3–17.
[20] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations
of Machine Learning (2nd ed.). The MIT Press.
[21] Guo-Jun Qi, Xian-Sheng Hua, Yong Rui, Jinhui Tang, Tao Mei, and Hong-Jiang
Zhang. 2007. Correlative multi-label video annotation. In Proceedings of the 15th
International Conference on Multimedia 2007. 17–26.
[22] Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. 2011. Classifier
chains for multi-label classification. Mach. Learn. 85, 3 (2011), 333–359.
[23] Chongjie Si, Yuheng Jia, Ran Wang, Min-Ling Zhang, Yanghe Feng, and Qu
Chongxiao. 2023. Multi-Label Classification With High-Rank and High-Order
Label Correlations. IEEE Transactions on Knowledge and Data Engineering (2023),
1–13.
[24] Feng Sun, Ming-Kun Xie, and Sheng-Jun Huang. 2022. A Deep Model for Par-
tial Multi-Label Image Classification with Curriculum Based Disambiguation.
arXiv:2207.02410
[25] Lijuan Sun, Songhe Feng, Jun Liu, Gengyu Lyu, and Congyan Lang. 2022. Global-
Local Label Correlation for Partial Multi-Label Learning. IEEE Trans. Multim. 24
(2022), 581–593.
[26] Lijuan Sun, Songhe Feng, Tao Wang, Congyan Lang, and Yi Jin. 2019. Partial
Multi-Label Learning by Low-Rank and Sparse Decomposition. In The Thirty-
Third AAAI Conference on Artificial Intelligence, AAAI 2019. 5016–5023.
[27] Haobo Wang, Weiwei Liu, Yang Zhao, Chen Zhang, Tianlei Hu, and Gang Chen.
2019. Discriminative and Correlative Partial Multi-Label Learning. In Proceedings
of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI
2019. 3691–3697.
[28] Haobo Wang, Shisong Yang, Gengyu Lyu, Weiwei Liu, Tianlei Hu, Ke Chen,
Songhe Feng, and Gang Chen. 2023. Deep Partial Multi-Label Learning with
Graph Disambiguation. In Proceedings of the Thirty-Second International Joint
Conference on Artificial Intelligence, IJCAI 2023. 4308–4316.
[29] Ming-Kun Xie and Sheng-Jun Huang. 2018. Partial Multi-Label Learning. In
Thirty-Second AAAI Conference on Artificial Intelligence, AAAI 2018. 4302–4309.
[30] Ming-Kun Xie and Sheng-Jun Huang. 2022. Partial Multi-Label Learning With
Noisy Label Identification. IEEE Trans. Pattern Anal. Mach. Intell. 44, 7 (2022),
3676–3687.
[31] Ning Xu, Yun-Peng Liu, and Xin Geng. 2020. Partial Multi-Label Learning with
Label Distribution. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,
AAAI 2020. 6510–6517.
[32] Guoxian Yu, Xia Chen, Carlotta Domeniconi, Jun Wang, Zhao Li, Zili Zhang,
and Xindong Wu. 2018. Feature-Induced Partial Multi-label Learning. In IEEE
International Conference on Data Mining, ICDM 2018. 1398–1403.
[33] Min-Ling Zhang and Jun-Peng Fang. 2021. Partial Multi-Label Learning via
Credible Label Elicitation. IEEE Trans. Pattern Anal. Mach. Intell. 43, 10 (2021),
3587–3599.
[34] Min-Ling Zhang and Lei Wu. 2015. Lift: Multi-Label Learning with Label-Specific
Features. IEEE Trans. Pattern Anal. Mach. Intell. 37, 1 (2015), 107–120.
[35] Min-Ling Zhang and Zhi-Hua Zhou. 2006. Multi-Label Neural Networks with
Applications to Functional Genomics and Text Categorization. IEEE Trans. Knowl.
Data Eng. 18, 10 (2006), 1338–1351.
[36] Min-Ling Zhang and Zhi-Hua Zhou. 2007. ML-KNN: A lazy learning approach
to multi-label learning. Pattern Recognit. 40, 7 (2007), 2038–2048.
[37] Min-Ling Zhang and Zhi-Hua Zhou. 2014. A Review on Multi-Label Learning
Algorithms. IEEE Trans. Knowl. Data Eng. 26, 8 (2014), 1819–1837.
3733Noisy Label Removal for Partial Multi-Label Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
A PROOF OF LEMMA 1 AND THEOREM 1
Definition 1. DenoteHbe a family of functions that map Xto
[0,1]and𝑆={𝑥1,𝑥2,...,𝑥𝑚}is a set of fixed samples. The empirical
Rademacher complexity of Hto set𝑆is defined as
ˆR𝑆(H)=1
𝑚E"
sup
ℎ∈H𝑚∑︁
𝑖=1𝜎𝑖ℎ(𝑥𝑖)#
, (S1)
where(𝜎1,𝜎2,...,𝜎𝑚)are Rademacher variables, and each of them is
an independent uniform random variable taking value in {−1,+1}.
Proof 1. Assume the square loss function ℓ:∥YG+E−XW∥2
𝐹is
2𝑐-Lipschitz. Based on Definition 1, letH=W×Ebe the family
of functions, i.e.,(W,E)∈H . The Rademacher complexity with
respective toHandℓcan be written as
ˆR𝑆(ℓ◦H) =1
𝑚E"
sup
ℎ∈H𝑚∑︁
𝑖=1𝜎𝑖ℓ(ℎ(x𝑖),Y𝑖)#
. (S2)
According to [19], Eq. (S2) is upper bounded by
ˆR𝑆(ℓ◦H)≤2√
2𝑐
𝑚E"
sup
ℎ∈H𝑚∑︁
𝑖=1𝑞∑︁
𝑗=1𝜎𝑖𝑗(𝑥𝑖W.𝑗−E𝑖𝑗)#
, (S3)
where𝜎𝑖𝑗is the Rademacher variable which takes value in {−1,1}.
W.𝑗is𝑗-th column of classifier W. Denote ˆX=[ˆX1,ˆX2,...,ˆX𝑞]T∈
R𝑞×𝑑,ˆX𝑞=Í𝑛
𝑖=1𝜎𝑖𝑞𝑥𝑖. Suppose the sparsity of Eand the complex-
ity of classifier Ware upper bounded by 𝜏and𝜌respectively, i.e.,
∥E∥1≤𝜏and∥W∥𝐹≤𝜌. Eq. (S3) can be further relaxed as:
ˆR𝑆(ℓ◦H)≤2√
2𝑐
𝑚E
sup
ℎ∈H∥E∥1+⟨WT,ˆX⟩
≤2√
2𝑐
𝑚E
sup
ℎ∈H∥E∥1+∥W∥𝐹ˆX
𝐹
≤2√
2𝑐
𝑚E
sup
ℎ∈H𝜏+𝜌ˆX
𝐹
.(S4)
Assume each data sample is normalized, i.e., ∥𝑥𝑖∥2≤1, it is easy
to prove E𝜎ˆX2
𝐹≤𝑚𝑞, then we have Lemma 1
ˆR𝑆(ℓ◦H)≤2𝑐√
2𝜏
𝑚+2𝑐√2𝑚𝑞𝜌
𝑚. (S5)
Proof 2. LetHbe a family of functions. Denote 𝑆={𝑥1,𝑥2,...,𝑥𝑚}
is a set of fixed samples. Loss function ℓis upper bounded by Θ≥0,
then for any 𝛿>0, with probability at least 1−𝛿, for allℎ∈H we
have
LG(ℎ)≤LS(ℎ)+ˆR𝑆(ℓ◦H)+ 3Θ√︂
𝑙𝑜𝑔(2/𝛿)
2𝑚, (S6)
whereLG(ℎ)andLS(ℎ)are generalization error and empirical
error toℎrespectively. By combing Lemma 1 and Eq. (S6), we have
Theorem 1.
B COMPUTATIONAL COMPLEXITY
Table S1 compares the computational complexity of regression-
based PML methods, where 𝑚,𝑑,𝑞 represent the number of samples,
the number of dimensions, and the number of classes, respectively.
We further assume that 𝑚>𝑑>𝑞,𝑇represents the number of
iterations.
According to the time complexity based on linear regression
methods shown in Table S1, our method NLR has a slightly higher
time complexity than PML-NI and PML-LRS, but a lower timeTable S1: Computational complexity of all regression-based
PML methods.
Metho
d Computational
complexity
FPML[32] O
(𝑇(4𝑚𝑑𝑞))
PML-LRS[26] O
(𝑇(𝑚2𝑞+𝑚𝑑𝑞+𝑑2𝑚+𝑑2𝑞+𝑚𝑞)
PML-NI[30] O
(𝑇(𝑚𝑑𝑞+𝑑2𝑞+𝑑𝑞))
PAKS[17] O
(𝑇(3𝑚3+𝑚2𝑑+𝑚2))
GLC[25] O
(𝑇(2𝑚𝑑𝑞+𝑞3+𝑚𝑞))
NLR (Ours) O
(𝑇(3𝑚𝑑𝑞))
complexity than other methods. This is consistent with the running
time presented in Fig. 4 of our main paper.
Table S2: Experimental results of different prediction method
onrank loss on synthetic data sets.
Evaluation Metric rank loss (the smaller the better ↓)
Version 𝑀1𝑀2𝑀3
Scene (𝑟=3) .111±.008 .111±.008 .111±.008
Birds (𝑟=11) .332±.038 .332±.038 .332±.038
Medical (𝑟=11) .048±.007 .048±.008 .048±.007
Enron (𝑟=11) .106±.007 .106±.007 .106±.007
Chess (𝑟=30) .164±.005 .164±.005 .164±.005
Chemistry ( 𝑟=30) .165±.008 .165±.008 .165±.008
Philosophy ( 𝑟=30) .178±.011 .178±.011 .178±.011
C EXPERIMENT DETAILS
Our method has two classifiers, the original classifier Wand the
competitive classifier ˆW, where Wpredicts which classes belong
to the sample, and ˆWpredicts which classes do not belong to the
sample. Therefore, we constructed the following three prediction
methods𝑀1,𝑀2and𝑀3
 
𝑀1:𝑦𝑖=𝑥𝑖W,
𝑀2:𝑦𝑖=1𝑞−𝑥𝑖ˆW,
𝑀3:𝑦𝑖=𝑥𝑖W−𝑥𝑖ˆW,(S7)
where𝑥𝑖denotes a testing sample, 1𝑞∈R1×𝑞is an all ones vector.
𝑀1represents directly using the PML classifier to predict which
classes belong to the sample; 𝑀2represents using the competitive
classifier to predict which classes do not belong to the sample, and
then obtaining which classes belong to the sample by 1𝑞−𝑥𝑖ˆW;
𝑀3represents the two classifiers cooperating to determine which
classes the sample belongs to. We report the rank loss of these three
prediction methods in Table S2. The experimental results show
that, in most cases, the results of the three prediction methods
are the same, proving that the PML classifier and the competitive
classifier have formed a good cooperative relationship and have
strong robustness. Therefore, in this paper, we use 𝑀1to predict
the testing samples.
Table S3 and S4 report experimental results on hamming loss
andcoverage. Note when predicting the hamming loss, a threshold
is needed to separate ground-truth labels and negative labels, so
we set the threshold of all methods on all data sets to 0.9 for fair
comparisons.
3734KDD ’24, August 25–29, 2024, Barcelona, Spain. Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, & Junhui Hou
Table S3: Experimental results on hamming loss (the smaller the better ↓). Bold and underlined indicate the best and second best
results respectively. •/◦indicates whether the hamming loss of Our method is statistically superior/inferior to the compared
method.
Data 𝑟 NLR
FPML[32] PML-LRS[26] PML-NI[30] P-MAP[33] P-VLS[33] PAKS[17] GLC[25] PARD[7]
Music_emotion .212±.002 .233±.002• .256±.005• .216±.002• .227±.004• .212±.005 .215±.004 .215±.002 .220±.002•
Music_style
.116±.003 .124±.003• .161±.032• .114±.002 .117±.002
.119±.002 .116±.002 .115±.003 .144±.002•
Y
eastMF .093±.003 .117±.005• .117±.003• .107±.002• .133±.001• .115±.005• .102±.002• .113±.004• .090±.002
Y
eastCC .074±.003 .090±.003• .108±.002• .094±.003• .113±.004• .095±.004• .077±.003 .102±.003• .083±.002•
Y
eastBP .041±.001 .043±.001• .052±.001• .048±.000• .053±.002• .040±.001 .042±.001
.051±.001• .039±.001◦
Mirflickr
.174±.005 .192±.007• .212±.005• .173±.003 .154±.003◦.173±.004 .179±.004
.175±.004 .254±.001•
Scene1.102±.008 .203±.011• .288±.004• .152±.008• .115±.007•.128±.007• .144±.012• .159±.009• .157±.011•
2.107±.011 .207±.006• .295±.016• .187±.009• .131±.007•.147±.005• .185±.009• .195±.007• .179±.002•
3.118±.009 .198±.009• .293±.013• .231±.004• .173±.008•.204±.009• .231±.007• .236±.005• .179±.002•
Bir
ds3.094±.005 .117±.017• .115±.017• .101±.006 .146±.019• .188±.023• .110±.005• .103±.006• .109±.020
7
.105±.011 .122±.008• .129±.045
.129±.007• .176±.009• .097±.007 .114±.006
.132±.004• .110±.024
11 .115±.006 .134±.022
.154±.033• .135±.004• .172±.008• .133±.009• .125±.007•.135±.005• .136±.016•
Me
dical3.012±.001 .021±.001• .039±.001• .013±.001 .030±.002• .021±.003• .028±.001• .019±.001• .026±.001•
7.013±.001 .021±.001• .038±.001• .017±.001• .034±.003• .024±.001• .029±.002• .033±.003• .026±.001•
11 .014±.000 .021±.001• .038±.001• .022±.003• .045±.007• .023±.003• .028±.002• .043±.004• .025±.001•
Enr
on3.053±.001 .057±.001• .111±.014• .054±.002 .055±.002
.063±.002• .053±.001 .067±.002• .057±.001•
7.053±.001 .057±.001• .120±.013• .060±.002• .059±.003• .064±.002• .053±.001 .077±.002• .057±.002•
11 .053±.001 .058±.002• .108±.016• .067±.002• .059±.002• .064±.002• .053±.002 .086±.002• .057±.002•
Chemistr
y10 .014±.000 .014±.000• .032±.003• .016±.000• .018±.001• .014±.000◦.015±.000• .017±.000• .012±.000◦
20
.014±.000 .015±.000• .028±.002• .018±.000• .022±.001• .014±.000◦.017±.000• .020±.000• .012±.000◦
30
.014±.000 .016±.000• .026±.004• .021±.000• .028±.002• .014±.000 .019±.000• .023±.000• .012±.000◦
Chess10 .011±.000 .011±.001 .091±.017• .014±.000• .014±.001• .011±.000 .011±.000 .017±.000• .012±.000•
20 .011±.000 .011±.001 .069±.013• .017±.000• .015±.000• .011±.000 .012±.000 .020±.000• .020±.001•
30 .011±.000 .012±.001 .047±.013• .018±.000• .016±.000• .011±.000◦.012±.001 .022±.000• .036±.001•
P
hilosophy10 .010±.000 .010±.000 .072±.005• .013±.000• .013±.000•.010±.000 .010±.000 .015±.000• .010±.000◦
20 .010±.000 .011±.000• .063±.005• .016±.000• .013±.001• .010±.000 .012±.000• .018±.000• .010±.000◦
30 .010±.000 .011±.000• .050±.004• .018±.000• .014±.001• .011±.000 .013±.000• .020±.000• .010±.000◦
Table S4: Experimental results on coverage (the smaller the better ↓). Bold and underlined indicate the best and second best
results respectively. •/◦indicates whether the coverage of Our method is statistically superior/inferior to the compared method.
Data 𝑟 NLR
FPML[32] PML-LRS[26] PML-NI[30] P-MAP[33] P-VLS[33] PAKS[17] GLC[25] PARD[7]
Music_emotion .404±.005 .449±.005• .407±.004
.408±.003 .420±.011• .410±.007 .407±.004 .406±.004 .414±.006•
Music_style .197±.007 .233±.005• .207±.008
.198±.008 .206±.007
.207±.008 .199±.008 .203±.008 .206±.008
YeastMF .382±.009 .491±.017• .522±.017• .450±.021• .544±.022• .597±.019• .402±.007• .526±.019• .389±.031
Y
eastCC .319±.009 .440±.021• .545±.010• .452±.016• .534±.031• .616±.022• .349±.008•.530±.013• .478±.020•
Y
eastBP .448±.011 .504±.015• .682±.010• .604±.013• .671±.010• .743±.013• .485±.011•.681±.011• .515±.010•
Mirflickr
.230±.004 .259±.010• .232±.003 .228±.004 .202±.006◦.266±.003
.233±.005 .233±.004 .237±.003•
Scene1.093±.007 .195±.021• .156±.008• .145±.008• .102±.006 .102±.006 .141±.013• .155±.008• .155±.019•
2.097±.010 .199±.013• .201±.011• .188±.012• .120±.012• .119±.007•.191±.017• .200±.010• .201±.013•
3.108±.009 .217±.022• .259±.005• .250±.006• .171±.005• .160±.003•.260±.009• .258±.005• .259±.005•
Bir
ds3.156±.033 .216±.038• .168±.041 .156±.037 .207±.024• .217±.016• .189±.036
.157±.041 .223±.019•
7.194±.027 .223±.032
.253±.031• .222±.040 .237±.027• .239±.037 .202±.029 .246±.036• .232±.038
11 .218±.030 .248±.021
.261±.015• .245±.020 .283±.018• .284±.033• .231±.018 .257±.012• .245±.021
Me
dical3 .041±.008 .066±.016• .066±.006• .040±.012 .117±.018• .113±.024• .115±.014• .060±.008• .046±.012
7.062±.012 .118±.015• .161±.012• .062±.012 .132±.011• .134±.014• .134±.007• .106±.009• .067±.014
11 .066±.009 .120±.020• .221±.017• .073±.007 .142±.015• .135±.020• .142±.018• .136±.017• .078±.008
Enr
on3.259±.022 .312±.017• .463±.022• .367±.028• .306±.011• .387±.017• .267±.018 .473±.023• .298±.027•
7.270±.022 .330±.027• .507±.016• .399±.026• .326±.016• .402±.028• .292±.019 .498±.018• .342±.027•
11 .287±.017 .354±.014• .550±.013• .427±.025• .349±.012• .404±.015• .306±.019 .523±.015• .367±.024•
Chemistr
y10 .244±.008 .272±.012• .347±.011• .318±.011• .334±.009• .320±.014• .301±.011• .356±.012• .243±.026
20 .263±.009 .300±.012• .384±.010• .355±.010• .375±.010• .354±.020• .335±.010• .391±.010• .265±.009
30 .279±.009 .327±.013• .412±.008• .382±.009• .419±.009• .371±.011• .362±.011• .416±.008• .296±.012•
Chess10 .267±.012 .299±.016• .416±.011• .362±.012• .347±.021• .404±.030• .292±.006• .432±.012• .269±.008
20 .292±.014 .320±.016• .465±.012• .403±.009• .383±.015• .412±.027• .321±.013• .467±.012• .316±.010•
30 .310±.013 .339±.019• .494±.008• .431±.008• .413±.013• .440±.018• .350±.013• .494±.007• .338±.013•
P
hilosophy10 .271±.011 .291±.010• .398±.010• .363±.013• .343±.009• .344±.017• .300±.016• .409±.009• .255±.013
20
.303±.009 .323±.008• .434±.006• .398±.009• .360±.015• .364±.016• .335±.013• .440±.006• .302±.016
30 .323±.012 .351±.013• .453±.008• .418±.009• .393±.012• .385±.017• .360±.016• .458±.007• .327±.016
3735