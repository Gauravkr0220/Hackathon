FairMatch: Promoting Partial Label Learning by Unlabeled
Samples
Jiahao Jiang
College of Software Engineering, Southeast University
Nanjing, China
jhjiang@seu.edu.cnYuheng Jiaâˆ—
School of Computer Science and Engineering, Southeast
University
Nanjing, China
yhjia@seu.edu.cn
Hui Liu
School of Computing & InformationSciences, Saint Francis
University
HongKong, China
h2liu@sfu.edu.hkJunhui Hou
Department of Computer Science, City University of Hong
Kong
HongKong, China
jh.hou@cityu.edu.hk
ABSTRACT
This paper studies the semi-supervised partial label learning (SS-
PLL) problem, which aims to improve the partial label learning
(PLL) by leveraging unlabeled samples. Both the existing SSPLL
methods and the semi-supervised learning methods exploit the
information in unlabeled samples by selecting high-confidence
unlabeled samples as the pseudo labels based on the maximum
value of the model output. However, the scarcity of labeled sam-
ples and the ambiguity from partial labels skew this strategy to-
wards an unfair selection of high-confidence samples on each class,
most notably during the initial phases of training, resulting in
slower training and performance degradation. In this paper, we
propose a novel method FairMatch, which adopts a learning state
aware self-adaptive threshold for selecting the same number of
high-confidence samples on each class, and uses augmentation con-
sistency to incorporate the unlabeled samples to promote PLL. In
addition, we adopt the candidate label disambiguation to utilize
the partial labeled samples and mix up the partial labeled samples
and the selected high-confidence unlabeled samples to prevent
the model from overfitting on partial label samples. FairMatch
can achieve maximum accuracy improvements of 9.53%, 4.9%, and
16.45% on CIFAR-10, CIFAR-100, and CIFAR-100H, respectively. The
codes can be found at https://github.com/jhjiangSEU/FairMatch.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning algorithms ;
Learning paradigms .
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671685KEYWORDS
Partial Label Learning, Semi-supervised Learning, Fair Selection
ACM Reference Format:
Jiahao Jiang, Yuheng Jia, Hui Liu, and Junhui Hou. 2024. FairMatch: Pro-
moting Partial Label Learning by Unlabeled Samples. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3637528.3671685
1 INTRODUCTION
The impressive performance of deep learning heavily relies on
supervised training with substantial labeled data [ 5,20]. However,
gathering sufficient high-quality annotated data for training is often
a challenging task. Partial label learning (PLL) is a paradigm to
alleviate this problem, which assumes that each training sample is
equipped with a candidate label (CL) set, among which only one is
the ground-truth label [6â€“10, 15, 21, 23].
Previous PLL researches typically assume that the CL sets are
given for all training examples. However, this assumption could
be unrealistic in many real-world scenarios. For example, in au-
tomatic face annotation [ 1], while it is possible to annotate faces
with ambiguous labels using the names appearing in the scripts and
dialogues, i.e., partial label (PL) samples. In contrast, many images
have no corresponding dialogues, leaving a large number of faces
without any label information, i.e., unlabeled samples. Similar situ-
ations occur in web mining [ 12], multimedia content analysis [ 24]
and ecoinformatics [ 27]. When there are not enough PL samples,
the performance of state-of-the-art (SOTA) PLL methods drops sig-
nificantly [ 17â€“19]. Moreover, these methods fail to make efficient
use of unlabeled data.
To solve this problem, one may consider using semi-supervised
learning (SSL) methods such as FixMatch [ 14] and FreeMatch [ 20],
which are mainly based on augmentation consistency. Generally,
they use a weakly-augmented unlabeled sample with high confi-
dence to generate a pseudo label based on fixed or dynamic thresh-
olds, and enforce consistency against strongly-augmented version
of the same sample. A detailed discussion of them is provided in
Section 2.2. However, the existence of PL samples has compromised
their performance. In Fig. 1, Ceiling (blue line) is the upper limit of
the performance, using FreeMatch in a standard semi-supervised
1269
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahao Jiang, Yuheng Jia, Hui Liu, and Junhui Hou
0 100 200 300 400 500
Epoch0.00.10.20.30.40.50.60.70.80.91.0T est AccuracyCeiling
FixMatch
FixMatch+
FreeMatch
FreeMatch+
ConCont
FairMatch (Ours)
Figure 1: Test accuracy of the first 500 epochs on CIFAR-10
withğœŒ=10%andğ‘=0.7, whereğœŒrepresents the propor-
tion of partial label samples and ğ‘represents the partial rate.
Ceiling (blue line) is the upper limit of performance, using
FreeMatch in a standard semi-supervised setting with accu-
rate label. FixMatch and FreeMatch are state-of-the-art semi-
supervised learning (SSL) methods, and â€œ+â€ represents an SSL
method combined with candidate label disambiguation. Con-
Cont is the only deep learning-based semi-supervised partial
label learning method. The results indicate, FairMatch (our
method) converges quickly and significantly surpasses the
comparison methods, approaching the performance limit.
setting, i.e., the CL set of all PL samples contains only ground-truth
labels without incorrect ones. Compared to it, the performance of
FixMatch (orange solid line) and FreeMatch (purple solid line) have
degraded significantly.
Consequently, semi-supervised partial label learning (SSPLL)
[17â€“19] has been proposed to effectively utilize unlabeled samples
and disambiguate the CL sets of PL samples. To the best of our
knowledge, ConCont [ 18] is the only deep learning-based SSPLL
method. ConCont uses augmentation consistency and confidence
thresholding strategy similar to FixMatch and FreeMatch for both
PL samples and unlabeled samples. Besides, the aforementioned SSL
methods could also be extended to solve the SSPLL by combining
label disambiguation operation (estimate the confidence of the la-
bels in candidate label set), denoted as FixMatch+ and FreeMatch+.
Fig. 1 clearly demonstrates that FixMatch+ (orange dashed line)
and FreeMatch+ (purple dashed line) significantly surpass the pre-
vious performance. ConCont and the improved SSL methods select
high-confidence samples from the unlabeled samples based on the
maximum output of the model, and use the class with the maximum
output as the hard pseudo label for these samples. We abbreviate this
strategy as the maximum selection strategy. However, the scarcity
of labeled samples with ambiguous labels and the varying learning
difficulties of classes lead to an unfair number of high-confidence
samples selected on each class, particularly in the early stages of
training. As shown in Figs. 2 (a1-a3, b1-b3), Ceiling selects uniform
high-confidence samples on each class at different training stages,
while FreeMatch+â€™s selection is severely unfair in the early stagesand even unable to select samples for the fourth category in the
later stages, indicating that the existence of PL samples will exacer-
bate selection unfairness. Furthermore, Figs. 2 (c1-c3) demonstrate
that ConCont has the same problem. The existing methods are all
plagued by unfair selection, which is also the reason for the slow
learning of these methods in Fig. 1.
In this paper, we propose a novel SSPLL method FairMatch,
which achieves fair selection on each class. For PL samples, Fair-
Match disambiguates the CL sets and uses the updated labeling
confidence to guide the learning of these samples. For unlabeled
samples, based on the learning state of the model on these sam-
ples, FairMatch determines a self-adaptive threshold for selecting
an equal number of high-confidence samples on each class, and
uses augmentation consistency to incorporate these samples into
training. As shown in Figs. 2(d1-d3), at different training stages, Fair-
Match ensures fairness in the selection of each class and achieves
excellent selection accuracy. In addition, we use the MixUp [ 26]
strategy based on high-confidence samples and PL samples to gener-
ate new samples for training, to prevent the model from overfitting
on the few PL samples. Under the same experimental setup, Fair-
Match can achieve maximum accuracy improvements of 9.48%,
4.96%, and 16.83% on CIFAR-10, CIFAR-100, and CIFAR-100H, re-
spectively. Our main contributions are summarized as follows:
â€¢We propose a novel SSPLL method named FairMatch, which for
the first time achieves fair selection on each class and adaptively
determines the number of high-confidence samples to be selected
from the unlabeled samples based on the learning state.
â€¢We adopt label disambiguation to explore the information in the
PL samples, train the high-confidence samples based on augmen-
tation consistency and mix them with PL samples to prevent the
model from overfitting on PL samples.
â€¢Extensive experiments on benchmark datasets demonstrate that
FairMatch significantly outperforms SOTA comparison methods,
including PLL methods, the improved SSL methods and SSPLL
methods.
2 RELATED WORK
2.1 Partial Label Learning
With the prevalence of deep learning, deep learning-based PLL
methods have been extensively studied in recent works [ 13,16,21â€“
23]. PRODEN [ 13] proposes a progressive identification algorithm
to disambiguate the candidate label (CL) set. PiCO [ 16] adopts
contrastive representation learning to PLL for the first time and sig-
nificantly improves the performance on several image classification
benchmarks. ABLE [ 22] uses the output of the model on the CL set
as class-level weights to implement weighted contrastive learning.
DPLL [ 21] instantiates the regularization term by matching the
outputs of multiple augmentations of each instance to a conformal
label distribution, similar to the augmentation consistency in semi-
supervised learning methods. PaPi [ 23] guides the optimization
of a prototypical classifier through model output, thus explicitly
encouraging the representation to reflect visual similarity between
classes. However, most of the above methods demand massive par-
tial label (PL) samples for training, which is sometimes not practical.
In addition, they cannot effectively deal with unlabeled samples.
1270FairMatch: Promoting Partial Label Learning by Unlabeled Samples KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0123456789
Classes020406080100120Number of SamplesT otal=359
Correct=325
Acc=0.91100-th Epoch Sample Distribution
Wrong Selection
Correct Selection
(a1) Ceiling (100th)
0123456789
Classes020406080100120Number of SamplesT otal=410
Correct=391
Acc=0.95300-th Epoch Sample Distribution
Wrong Selection
Correct Selection (a2) Ceiling (300th)
0123456789
Classes020406080100120Number of SamplesT otal=427
Correct=412
Acc=0.96500-th Epoch Sample Distribution
Wrong Selection
Correct Selection (a3) Ceiling (500th)
0123456789
Classes020406080100120Number of SamplesT otal=181
Correct=80
Acc=0.44100-th Epoch Sample Distribution (b1) FreeMatch+ (100th)
0123456789
Classes020406080100120Number of SamplesT otal=320
Correct=303
Acc=0.95300-th Epoch Sample Distribution (b2) FreeMatch+ (300th)
0123456789
Classes020406080100120Number of SamplesT otal=372
Correct=339
Acc=0.91500-th Epoch Sample Distribution (b3) FreeMatch+ (500th)
0123456789
Classes020406080100120Number of SamplesT otal=42
Correct=16
Acc=0.38100-th Epoch Sample Distribution
Wrong Selection
Correct Selection
(c1) ConCont (100th)
0123456789
Classes020406080100120Number of SamplesT otal=289
Correct=267
Acc=0.92300-th Epoch Sample Distribution
Wrong Selection
Correct Selection (c2) ConCont (300th)
0123456789
Classes020406080100120Number of SamplesT otal=348
Correct=317
Acc=0.91500-th Epoch Sample Distribution
Wrong Selection
Correct Selection (c3) ConCont (500th)
0123456789
Classes020406080100120Number of SamplesT otal=340
Correct=297
Acc=0.87100-th Epoch Sample Distribution
Wrong
Correct (d1) FairMatch (100th)
0123456789
Classes020406080100120Number of SamplesT otal=390
Correct=356
Acc=0.91300-th Epoch Sample Distribution
Wrong
Correct (d2) FairMatch (100th)
0123456789
Classes020406080100120Number of SamplesT otal=410
Correct=375
Acc=0.91500-th Epoch Sample Distribution
Wrong
Correct (d3) FairMatch (100th)
Figure 2: The distribution of selected high-confidence samples on each class in the first mini-batch of the 100th, 300th, and
500th epochs (CIFAR-10 with ğœŒ=10%andğ‘=0.7). â€œTotalâ€ and â€œCorrectâ€ represent the number of selected and correctly assigned
high-confidence samples, respectively, while â€œAccâ€ represents the proportion of correctly assigned high-confidence samples.
The red dashed line represents the average number of unlabeled samples on each class. At different training stages, FairMatch
(our method) ensures fairness in the selection of each class and achieves excellent selection accuracy on each class.
2.2 Semi-Supervised Learning
In semi-supervised learning (SSL), the primary mechanism for lever-
aging unlabeled data is consistency regularization, which is typ-
ically achieved by perturbing the samples and encouraging the
model to maintain consistent predictions. FixMatch [ 14] proposes
a concise framework, applying a consistency regularization be-
tween the weakly-augmented and strongly-augmented versions for
high-confidence unlabeled samples. In FixMatch, high-confidence
unlabeled samples are selected by a fixed threshold throughout
the training phase, failing to consider different learning status and
learning difficulties of different classes. To solve this problem, Flex-
Match [ 25] propose a curriculum learning approach to flexibly
adjust thresholds for different classes according to the selected
number of each class. Previous study [ 20] points out that both fixed
threshold and ad-hoc threshold adjusting schemes can result in
inferior performance and slow convergence. FreeMatch [ 20] adjust
the local and global confidence thresholds in a self-adaptive manner
according to the modelâ€™s learning status, eliminating the difficulty
of pre-defining thresholds.
Although the above methods have achieved great success in the
SSL field, they still struggle to correctly select unlabeled samples
for training in the presence of PL samples.
2.3 Semi-Supervised Partial Label Learning
Semi-supervised partial label learning (SSPLL) is first proposed in
[17], which iteratively employs label propagation between PL sam-
ples and unlabeled samples to disambiguate the CL sets of partial
label samples as well as assign valid labels to unlabeled samples.
Despite its simplicity, it has to store all the training data during
testing phase, which makes it inefficient. PARM [ 19] introduces
maximum margin formulation to jointly optimize predictive model
and estimate latent labeling confidence, enforcing confidence-rated
margin maximization and confidence manifold preservation overPL samples and unlabeled samples. However, complex optimization
makes it inefficient for application to large-scale datasets.
In contrast, the only deep learning-based method ConCont [ 18]
performs a controller-guided consistency regularization based on
dynamic thresholds and utilizes contrastive learning [ 4,16] to fa-
cilitate learning more discriminative representations. Besides, SSL
methods could also be viewed as solutions of SSPLL by incorpo-
rating label disambiguation operation. However, ConCont and the
improved SSL methods all select the class corresponding to the
maximum output of a high-confidence sample as the pseudo label,
which is unfair for difficult classes. Although some methods have
considered the difficulty of different classes, under the influence
of PL samples, this maximum selection strategy still results in an
unfair number of high-confidence samples selected on each class,
thus affecting model training.
3 THE PROPOSED APPROACH
In this section, we first introduce the notations in Sec. 3.1. Then
we present FairMatch in detail in Secs. 3.2-3.5. Fig. 3 gives a brief
illustration of FairMatch, where the left side shows the overall
framework, and the right side shows an example of fair selection.
3.1 Notations
LetX=Rğ‘‘denote the input space, Y={1,2,...,ğ¶}denote the
label space with ğ¶class labels. The goal of semi-supervised partial
label learning (SSPLL) is to learn a multi-class classifier ğ‘“:Xâ†¦â†’Y
from the partial label (PL) training set Dğ‘={(ğ’™ğ‘–,ğ‘†ğ‘–)|1â‰¤ğ‘–â‰¤ğ‘}
and unlabeled setDğ‘¢={ğ’–ğ‘–|1â‰¤ğ‘–â‰¤ğ‘€}(ğ‘â‰ªğ‘€). Here, ğ’™ğ‘–,ğ’–ğ‘–âˆˆ
X, andğ‘†ğ‘–âŠ† Y is the candidate label associated with PL sam-
pleğ’™ğ‘–. The basic assumption in partial label learning is that the
ground-truth label ğ’šğ‘–forğ’™ğ‘–resides in its candidate label set (i.e.,
ğ’šğ‘–âˆˆğ‘†ğ‘–) which is not accessible to the learning algorithm. The
mini-batches of PL samples and unlabeled samples are denoted
1271KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahao Jiang, Yuheng Jia, Hui Liu, and Junhui Hou
Weakly-augmented
Strongly-augmentedUnlabeled SampleModel
ModelShar e
PredictionPrediction
 Hard Pseudo LabelFair selection
...... ...
Partial Label Sample Weakly-augmentedModel
Prediction... ...DisambiguatePartial Label Data
Classification Loss
MixUp Model
Prediction...
MixUp Confidence...
Shar eShar eMixUpDisambiguated Confidence
MixUp Data
Classification Loss
Unlabeled Data
Consistent LossFair Selection Example
Sample 1
Sample 2
Sample 3
.
.
.
Sample 8
Sample 90.76 0.22 0.02
0.550.36 0.09
0.70 0.14 0.160.99 0.05 0.05
0.950.10 0.05
0.84 0.15 0.01
0.80 0.12 0.08
0.650.10 0.25
0.830.01 0.16Prediction Hard Pseudo Label
   âœ”
âŒ
âŒ
âŒ   âœ”
   âœ”
0.70 0.14 0.161 0 0
0.84 0.15 0.01
0.80 0.12 0.08
0 1 0
0 0 1
Not selectedâŒ Selected âœ” Ground-truth label0.950.10 0.05
0.83 0.01 0.160.76 0.22 0.02âŒ
âŒ
âŒK = 1
SelectClass 1 2 3 Class 1 2 3
ğœ‡ğµ = 9,  C = 3,  m = âŒŠğœ‡ğµ/ğ¶âŒ‹  = 3,  ğœ* = 9
ğœ = (0.99+0.95+0.84)+(0.15+0.22+0.36)+(0.25+0.16+0.16) = 4.08
K = âŒŠmÂ·ğœ/ğœ*âŒ‹ = 1
Top ğ‘š  prediction values on each class
Hard pseudo label
Figure 3: Illustration of FairMatch. For partial label (PL) samples, FairMatch leverages the model outputs for label disambiguation
and uses the disambiguated confidence to guide the model learning. For unlabeled samples, based on the fair selection strategy,
FairMatch adaptively selects an equal number of high-confidence samples on each class and incorporates them into training
using augmentation consistency. To prevent overfitting of the model on PL samples, FairMatch mixes PL samples and selected
high-confidence samples to generate new samples for training. On the right is an example of fair selection, with 9 samples and
3 classes. The orange area represents the top ğ‘šprediction values on each class, while the green area represents the hard pseudo
labels of the selected high-confidence samples. In this example, the number of high-confidence samples selected ğ¾for each
class is 1, i.e., ğ¾=1. Detailed calculations and explanations is provided in Sec. 3.3.
asBğ‘={(ğ’™ğ‘–,ğ‘†ğ‘–)|1â‰¤ğ‘–â‰¤ğµ}andBğ‘¢={ğ’–ğ‘–|1â‰¤ğ‘–â‰¤ğœ‡ğµ}, respec-
tively, where ğœ‡is the ratio of unlabeled data batch size to labeled
data batch size.
For each PL sample ğ’™ğ‘–and unlabeled sample ğ’–ğ‘–, we generate
ğœ”(ğ’™ğ‘–),ğœ”(ğ’–ğ‘–), andÎ©(ğ’–ğ‘–), respectively, where ğœ”(Â·)andÎ©(Â·)repre-
sent weak and strong augmentation functions. Let ğ‘“(Â·)andâ„(Â·)be
the encoder and the classifier with softmax function, respectively.
3.2 Candidate Label Disambiguation for Labeled
Samples
Unlike traditional semi-supervised learning (SSL) where supervised
samples are annotated with accurate labels, SSPLL need to deal
with ambiguous labeling information. Accordingly, it is essential to
disambiguate the candidate label sets of PL samples. In FairMatch,
we employ an alternating and iterative disambiguation strategy,
leveraging the model outputs for disambiguation and using the
disambiguated labeling confidence to guide the model learning.
Specifically, for a weakly-augmented PL sample ğœ”(ğ’™ğ‘–)1, its label-
ing confidence ğ’‘ğ‘–âˆˆRğ¶is initialed as ğ‘ğ‘–ğ‘—=1
|ğ‘†ğ‘–|ifğ‘—âˆˆğ‘†ğ‘–, otherwise
ğ‘ğ‘–ğ‘—=0, where|ğ‘†ğ‘–|denotes the cardinality of ğ‘†ğ‘–. The classifier â„(Â·)
receivesğ‘“(ğœ”(ğ’™ğ‘–))as input and outputs ğ’“ğ‘–=â„(ğ‘“(ğœ”(ğ’™ğ‘–)))âˆˆ Rğ¶.
Then, we obtain the updated labeling confidence by eliminating
1To improve generalization performance, most partial label learning methods [ 16,
22,23] employ the model outputs on weakly-augmented samples for disambiguation,
rather than on the original samples. They also avoid using strongly-augmented samples,
as the model outputs on these are less reliable than on weakly-augmented samples.the model outputs on the non-candidate label set:
ğ‘ğ‘–ğ‘—=(ğ‘Ÿğ‘–ğ‘—Ã
ğ‘™âˆˆğ‘†ğ‘–ğ‘Ÿğ‘–ğ‘™,ifğ‘—âˆˆğ‘†ğ‘–,
0, otherwise.(1)
Leveraging the disambiguated labeling confidence to guide the
model learning, the classification loss of each partial label mini-
batch is defined as:
Lğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ =1
ğµğµâˆ‘ï¸
ğ‘–=1H(ğ’‘ğ‘–,ğ’“ğ‘–)=1
ğµğµâˆ‘ï¸
ğ‘–=1Â©Â­
Â«âˆ’ğ¶âˆ‘ï¸
ğ‘—=1ğ‘ğ‘–ğ‘—Â·logğ‘Ÿğ‘–ğ‘—ÂªÂ®
Â¬,(2)
whereğµis the batch size of Bğ‘,ğ¶is the number of classes, and
H(Â·,Â·)refers to cross-entropy loss.
3.3 Self-Adaptive Fair Selection for Unlabeled
Samples
Under the setting of SSPLL, training only with PL data fails to
effectively purify ambiguous labels, resulting in poor model perfor-
mance [ 17â€“19]. Therefore, we aim to select unlabeled samples with
high confidence to assist in model training, similar to SSL methods.
However, the maximum selection strategy adopted by SSL methods
and ConCont leads to unfair selection under the setting of SSPLL.
To address this problem, a straightforward solution is to select an
equal number of high-confidence samples on each class. Conse-
quently, the primary challenge becomes the determination of an
appropriate quantity of samples to select, denoted as ğ¾.
For each unlabeled mini-batch Bğ‘¢={ğ’–ğ‘–|1â‰¤ğ‘–â‰¤ğœ‡ğµ}, there are
an average of ğ‘š=âŒŠğœ‡ğµ
ğ¶âŒ‹samples in each class, where ğœ‡ğµis the
1272FairMatch: Promoting Partial Label Learning by Unlabeled Samples KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
batch size ofBğ‘¢,ğ¶is the number of classes, and âŒŠÂ·âŒ‹represents
the integer down function. Let ğ’“ğœ”
ğ‘–=â„(ğ‘“(ğœ”(ğ’–ğ‘–)))be the model
prediction on the weakly-augmented version of ğ’–ğ‘–andMğ‘š
ğ‘—be the
set ofğ‘šsamples with the highest prediction on class ğ‘—. We define
thecurrent learning state ğœas the sum of the top ğ‘šprediction
values across all classes:
ğœ=ğœ‡ğµâˆ‘ï¸
ğ‘–=1ğ¶âˆ‘ï¸
ğ‘—=1ğ‘Ÿğœ”
ğ‘–ğ‘—Â·I(ğ’–ğ‘–âˆˆMğ‘š
ğ‘—), (3)
where I(Â·)is the indicator function. Ideally, we desire the modelâ€™s
prediction on these samples to be one-hot, with a value of 1 exclu-
sively on the ground-truth label, indicating the optimal learning
state of the model. Thus, the optimal learning state ğœâˆ—can be ap-
proximately defined as ğœâˆ—=ğœ‡ğµ. Intuitively, the closer the current
learning state ğœis to the optimal learning state ğœâˆ—, the better the
model learns and the more samples should be selected. Therefore,
the number of samples to be selected ( ğ¾) on each class is set as
follows:
ğ¾=âŒŠğ‘šğœ
ğœâˆ—âŒ‹. (4)
Theğ¾determined in this manner is dynamic, moderate, and in-
creases with training performance, meaning that fewer unlabeled
samples with high confidence are selected when the model perfor-
mance is poor, and vice versa, that is, a trade-off between selection
number and selection accuracy. Taking the example on the right
side of Fig. 3, with a total of 9 unlabeled samples across 3 classes,
the average number of samples per class is 3, that is, ğœ‡ğµ=9,ğ¶=3
andğ‘š=3. The current learning state ğœis calculated as 3.99 ac-
cording to Eq. (3), and the optimal learning state ğœâˆ—is 9, then the
numberğ¾of samples selected per class is calculated as 1 by Eq.
(4). In the early stages of model training, model predictions tend to
cluster around simpler classes (class 1), making it challenging for
previous methods to select high-confidence unlabeled samples for
other classes, while our method avoids this problem.
Afterğ¾is determined, we get the unlabeled sample set Mğ¾
ğ‘—(1â‰¤
ğ‘—â‰¤ğ¶)of theğ¾samples with the highest prediction on class ğ‘—. For
each high-confidence sample in Mğ¾
ğ‘—, its hard pseudo label vector
is denoted as Ëœğ’šğ‘—, with itsğ‘—-th element set to 1 and all others to
0, indicating that its pseudo label is assigned to the ğ‘—-th class. By
encouraging the model predictions on strongly-augmented versions
of high-confidence samples to align with the pseudo labels assigned
to their weakly-augmented versions, the consistency regularization
loss of each unlabeled mini-batch can be computed as follows:
Lğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ =1
ğ¾ğ¶ğœ‡ğµâˆ‘ï¸
ğ‘–=1ğ¶âˆ‘ï¸
ğ‘—=1I(ğ’–ğ‘–âˆˆMğ¾
ğ‘—)Â·H( Ëœğ’šğ‘—,ğ’“Î©
ğ‘–). (5)
Here,ğ¾ğ¶represents the total number of high-confidence samples
selected, and ğ’“Î©
ğ‘–is the model prediction on the strongly-augmented
version of ğ’–ğ‘–, i.e., ğ’“Î©
ğ‘–=â„(ğ‘“(Î©(ğ’–ğ‘–))).
3.4 MixUp-Augmented for Both Labeled and
Unlabeled Samples
Considering that the number of PL samples is much smaller than
that of unlabeled samples, to prevent the model from memorizing
the PL data and overfitting, we employ the MixUp [ 26] strategy
as a mitigation measure based on the selected high-confidencesamples. Specifically, we compose Bâ„ğ‘–ğ‘”â„=n
(ğ’–â„
ğ‘–,Ëœğ’šâ„
ğ‘–)|1â‰¤ğ‘–â‰¤ğ¾ğ¶o
consisting of the all high-confidence samples selected from ğ¶class,
where Ëœğ’šâ„
ğ‘–is the pseudo label vector of ğ’–â„
ğ‘–. Then, we construct
new PL training samples Bğ‘›ğ‘’ğ‘¤=
(ğ’™ğ‘›ğ‘’ğ‘¤
ğ‘–,ğ’‘ğ‘›ğ‘’ğ‘¤
ğ‘–)|1â‰¤ğ‘–â‰¤ğµ	by
linearly interpolating a PL sample ğ’™ğ‘–fromBğ‘with a random high-
confidence sample ğ’–â„
ğ‘–fromBâ„ğ‘–ğ‘”â„, as well as their corresponding
labeling confidence (or pseudo label vector):
ğ’™ğ‘›ğ‘’ğ‘¤
ğ‘–=ğœ™ğ’™ğ‘–+(1âˆ’ğœ™)ğ’–â„
ğ‘–, (6)
ğ’‘ğ‘›ğ‘’ğ‘¤
ğ‘–=ğœ™ğ’‘ğ‘–+(1âˆ’ğœ™)Ëœğ’šâ„
ğ‘–, (7)
whereğœ™âˆ¼ğµğ‘’ğ‘¡ğ‘(ğ›¼,ğ›¼)andğ›¼is a hyperparameter. The classification
loss of MixUp samples Bğ‘›ğ‘’ğ‘¤is defined as:
Lğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ =1
ğµğµâˆ‘ï¸
ğ‘–=1H(ğ’‘ğ‘›ğ‘’ğ‘¤
ğ‘–,ğ’“ğ‘›ğ‘’ğ‘¤
ğ‘–), (8)
where ğ’“ğ‘›ğ‘’ğ‘¤
ğ‘–is the model prediction on the weakly-augmented ver-
sion of ğ’™ğ‘›ğ‘’ğ‘¤
ğ‘–, i.e., ğ’“ğ‘›ğ‘’ğ‘¤
ğ‘–=â„(ğ‘“(ğœ”(ğ’™ğ‘›ğ‘’ğ‘¤
ğ‘–))).
3.5 The Overall Formulation
Putting it all together, the overall training objective is:
L=Lğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™+ğœ†Lğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘+ğ›½Lğ‘šğ‘–ğ‘¥ğ‘¢ğ‘, (9)
whereğœ†andğ›½are trade-off parameters. To facilitate model train-
ing under the challenging SSPLL settings, for FairMatch and other
comparison methods, we initially employ only PL samples for a
warm-up period of ğ‘‡ğ‘¤epochs. During the warm-up phase, the dis-
ambiguated confidence is used to guide the model learning, i.e., Eq.
(1), and we also use contrastive learning from [ 16], with the dis-
tinction that only the weakly-augmented and strongly-augmented
versions of the same sample are considered as positive pairs.
4 EXPERIMENTS
4.1 Experiment Setup
4.1.1 Datasets. We evaluate our method on widely used bench-
marks CIFAR-10 and CIFAR-100 [ 11], and further investigate FaiMatch
on fine-grained image classification dataset CIFAR-100H [ 16]. Fol-
lowing [ 13], we generate candidate label set ğ‘ºğ‘–for partial label (PL)
sample ğ’™ğ‘–by flipping negative labels ğ’šğ‘–â‰ ğ’šğ‘–with a probability
ğ‘=ğ‘ƒ(ğ’šğ‘–âˆˆğ‘†ğ‘–|ğ’šğ‘–â‰ ğ’šğ‘–), a.k.a., partial rate. On CIFAR-100H, the
candidate labels for a PL sample only appear within its correspond-
ing superclass, while on other datasets, the candidate labels are
entirely arbitrary. The proportion of PL samples to total samples in
the training set is denoted by ğœŒ, with the remainder samples in the
training set being unlabeled data.
4.1.2 Baselines. The comparing methods include SOTA PLL meth-
ods DPLL [ 21] and PiCO [ 16], which only utilize PL samples for
training, and their variants DPLL* and PiCO*, which treat all op-
tional labels as candidate labels for unlabeled samples to incor-
porate them into training. SOTA SSL methods FixMatch [ 14] and
FreeMatch [ 20], along with their variants FixMatch+ and FreeMatch+,
are included in the comparison, where â€œ+â€ indicates the addition
of candidate label disambiguation. Besides, the only deep learning-
based semi-supervised partial label learning (SSPLL) method Con-
Cont [ 18], is also included. The hyperparameters of the above meth-
ods are set based on their original papers. Besides, the candidate
1273KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahao Jiang, Yuheng Jia, Hui Liu, and Junhui Hou
Table 1: Accuracy (mean Â±std) comparisons on CIFAR-10 with proportion ğœŒâˆˆ{10%,20%}and partial rate ğ‘âˆˆ{0.1,0.3,0.5,0.7}.
Dataset Method ğ‘=0.1ğ‘=0.3ğ‘=0.5ğ‘=0.7
DPLL 88.74 Â±0.06% 87.15 Â±0.17% 86.66 Â±0.16% 63.92 Â±0.45%
DPLL* 84.37 Â±0.34% 83.33 Â±0.20% 80.67 Â±0.51% 66.00 Â±1.22%
PiCO 84.37 Â±0.34% 83.33 Â±0.20% 80.67 Â±0.51% 66.00 Â±1.22%
PiCO* 91.10 Â±0.18% 90.74 Â±0.50% 58.20 Â±3.78% 29.21 Â±2.47%
Fixmatch 67.81 Â±1.82% 56.11 Â±1.01% 46.69 Â±0.71% 37.34 Â±0.98%
Fixmatch+ 93.27 Â±0.10% 91.42 Â±0.42% 83.07 Â±1.48% 40.90 Â±2.84%
Freematch 84.18 Â±0.60% 75.81 Â±5.97% 21.66 Â±0.65% 18.19 Â±0.87%
Freematch+ 93.76 Â±0.17% 93.31 Â±0.19% 92.04 Â±0.38% 86.71 Â±3.17%
ConCont 92.59 Â±0.07% 92.22 Â±0.08% 91.94 Â±0.10% 91.49 Â±0.06%CIFAR-10 (ğœŒ=20%)
FairMatch 95.02 Â±0.08% 94.61 Â±0.12% 94.08 Â±0.23% 93.46 Â±0.06%
DPLL 81.55 Â±0.12% 79.20 Â±0.24% 74.09 Â±0.37% 28.08 Â±0.83%
DPLL* 85.24 Â±0.13% 80.57 Â±0.37% 71.24 Â±0.36% 15.39 Â±0.80%
PiCO 77.87 Â±0.44% 76.15 Â±0.30% 71.01 Â±0.68% 51.61 Â±1.69%
PiCO* 81.96 Â±0.53% 57.19 Â±2.88% 38.28 Â±2.02% 20.02 Â±1.36%
Fixmatch 60.11 Â±0.48% 47.56 Â±1.83% 39.64 Â±0.20% 30.20 Â±0.46%
Fixmatch+ 92.25 Â±0.36% 90.14 Â±0.69% 85.10 Â±1.26% 35.45 Â±0.95%
Freematch 78.01 Â±0.56% 70.90 Â±4.99% 21.62 Â±1.43% 17.70 Â±0.68%
Freematch+ 92.92 Â±0.08% 92.48 Â±0.16% 91.09 Â±0.47% 80.61 Â±0.90%
ConCont 91.23 Â±0.02% 90.89 Â±0.11% 90.46 Â±0.16% 82.20 Â±0.71%CIFAR-10 (ğœŒ=10%)
FairMatch 93.89 Â±0.14% 93.73 Â±0.11% 93.09 Â±0.03% 91.73 Â±0.04%
20 100 200 300 400 500 600 700 800
Epochs0.50.60.70.80.9Accuracy
100200300400500600
Number
(a) CIFAR-100 ( ğœŒ=20%,ğ‘=0.3)
50100 200 300 400 500 600 700 800
Epochs0.20.30.40.50.60.70.80.9Accuracy
100200300400500600
NumberT est Accuracy Transductive Accuracy T otal Correct (b) CIFAR-100 ( ğœŒ=40%,ğ‘=0.1)
50100 200 300 400 500 600 700 800
Epochs0.40.50.60.70.8Accuracy
100200300400500600
Number (c) CIFAR-100H ( ğœŒ=20%,ğ‘=0.5)
Figure 4: The changes in FairMatchâ€™s test accuracy (orange), transductive accuracy of partial label samples (yellow), the number
of selected high-confidence samples (blue), and the number of correctly assigned high-confidence samples (green). Transductive
accuracy reflects the disambiguation effect for partial label samples, the greater the value, the better the recovering the
ground-truth label from the candidate label set. The red dashed line represents the batch size of unlabeled samples mini-batch.
For CIFAR-10, the warm-up ends at the 20th epoch, while for CIFAR-100 and CIFAR-100H, the warm-up ends at the 50th epoch.
label disambiguation methods for PL samples of FixMatch+ and
FreeMatch+ are the same as FairMatch (our method).
4.1.3 Implementation. For all datasets, we use an 18-layer ResNet
following [ 5,16] as the encoder. The classifier â„(Â·)is a single lin-
ear layer, which is also used for inference. Following DPLL [ 21],
weak augmentation function ğœ”(Â·)consists of random crop and
flip, whereas strong augmentation function Î©(Â·)additionally incor-
porates Autoaugment [ 2] and Cutout [ 3]. We set mini-batch size
ğµ=64, training epoch ğ‘‡ğ‘šğ‘ğ‘¥=800,ğœ‡=7,ğ›¼=4,ğœ†=1andğ›½=1
for all datasets, mostly following the settings of the previous meth-
ods [ 14,18,20,23]. Warm-up epoch ğ‘‡ğ‘¤=20for CIFAR-10, while
ğ‘‡ğ‘¤=50for both CIFAR-100 and CIFAR-100H. For all methods, we
present the average classification accuracy with standard deviation
(std) based on three independent runs.4.2 Main Results
Tables 1-3 report the main experimental results on three datasets,
where the methods are mainly divided into three categories: PLL
methods and their variants, SSL methods and their variants, and
SSPLL methods. Fig. 4 shows the trend of changes in test accuracy
(orange), transductive accuracy of PL samples (yellow), the number
of selected high-confidence samples (blue)2, and the number of
correctly assigned high-confidence samples (green). Based on the
above results, it is impressive to observe that:
â€¢Among all the 24 statistical comparisons (3 datasets Ã—2 settings
ofğœŒÃ—4 settings of ğ‘), FairMatch significantly outperforms the
comparison methods in 23 cases. On CIFAR-10 and CIFAR-100,
2On CIFAR-100 and CIFAR-100H with 100 classes, the number of selected high-
confidence samples increases by 100 when ğ¾increases by 1, which explains the
steep change and the distribution only on the hundred values of the blue line.
1274FairMatch: Promoting Partial Label Learning by Unlabeled Samples KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Accuracy (mean Â±std) comparisons on CIFAR-100 with proportion ğœŒâˆˆ{20%,40%}and partial rate ğ‘âˆˆ{0.1,0.3,0.5,0.7}.
Dataset Method ğ‘=0.05ğ‘=0.1ğ‘=0.15ğ‘=0.2
DPLL 68.07 Â±0.13% 66.51 Â±0.10% 62.37 Â±0.08% 55.54 Â±0.37%
DPLL* 68.02 Â±0.26% 64.92 Â±0.27% 54.78 Â±0.56% 45.60 Â±2.25%
PiCO 57.62 Â±0.15% 40.07 Â±0.28% 26.04 Â±0.69% 17.32 Â±0.47%
PiCO* 64.01 Â±0.35% 40.06 Â±1.82% 21.52 Â±1.22% 18.61 Â±0.56%
Fixmatch 24.79 Â±0.61% 18.92 Â±0.29% 15.91 Â±0.35% 13.77 Â±0.49%
Fixmatch+ 60.30 Â±1.42% 16.15 Â±0.48% 11.60 Â±0.42% 9.37 Â±0.71%
Freematch 3.98Â±0.54% 3.08 Â±0.03% 2.04 Â±0.25% 1.87 Â±0.52%
Freematch+ 67.38 Â±0.11% 32.11 Â±4.35% 11.14 Â±0.54% 8.82 Â±0.26%
ConCont 69.17 Â±0.29% 68.46 Â±0.16% 65.70 Â±0.21% 60.59 Â±0.18%CIFAR-100 ( ğœŒ=40%)
FairMatch 70.04 Â±0.10% 68.91 Â±0.12% 67.77 Â±0.23% 64.78 Â±0.98%
DPLL 58.59 Â±0.27% 51.61 Â±0.17% 39.56 Â±0.44% 28.21 Â±0.23%
DPLL* 57.25 Â±0.28% 41.03 Â±0.49% 29.36 Â±0.72% 24.71 Â±0.45%
PiCO 49.05 Â±0.64% 27.67 Â±0.95% 14.85 Â±0.40% 9.59 Â±0.83%
PiCO* 47.17 Â±0.73% 27.10 Â±1.51% 17.70 Â±1.44% 9.65 Â±1.69%
Fixmatch 16.73 Â±0.75% 13.19 Â±0.73% 11.11 Â±0.74% 9.61 Â±0.22%
Fixmatch+ 58.90 Â±0.88% 33.51 Â±1.45% 15.67 Â±0.32% 12.34 Â±0.65%
Freematch 5.71Â±2.68% 2.36 Â±0.50% 2.13 Â±0.13% 1.83 Â±0.27%
Freematch+ 57.00 Â±1.02% 55.01 Â±0.88% 28.73 Â±3.56% 12.02 Â±0.37%
ConCont 63.55 Â±0.10% 62.85 Â±0.26% 58.62 Â±0.02% 49.42 Â±0.31%CIFAR-100 ( ğœŒ=20%)
FairMatch 64.08 Â±0.33% 62.61 Â±0.14% 59.75 Â±0.77% 54.32 Â±0.22%
0 20 50 100 200
Tw0.600.650.700.750.800.850.900.951.00T est Accuracy
CIFAR-10 (=20%,q=0.5)
CIFAR-100 (=40%,q=0.15)
CIFAR-100H (=20%,q=0.7)
(a) Varying ğ‘‡ğ‘¤
0.1 0.5 1 5 10
0.20.30.40.50.60.70.80.91.0T est Accuracy
CIFAR-10 (=20%,q=0.5)
CIFAR-100 (=40%,q=0.15)
CIFAR-100H (=20%,q=0.7)
 (b) Varying ğœ†
0.1 0.5 1 5 10
0.600.650.700.750.800.850.900.951.00T est Accuracy
CIFAR-10 (=20%,q=0.5)
CIFAR-100 (=40%,q=0.15)
CIFAR-100H (=20%,q=0.7)
 (c) Varying ğ›½
0 100 200 300 400 500 600 700 800
Epoch0.50.60.70.80.91.0Transductive Accuracy
FairMatch
w/o mixup
 (d) CIFAR-10
0 100 200 300 400 500 600 700 800
Epoch0.50.60.70.80.91.0Transductive AccuracyFairMatch
w/o mixup
 (e) CIFAR-100
Figure 5: Trend of test accuracy of FairMatch by varying ğ‘‡ğ‘¤âˆˆ{0,20,50,100,200}in (a), varying ğœ†,ğ›½âˆˆ{0.1,0.5,1,5,10}in (b) and (c),
respectively. (d) and (e) show the the transductive accuracy of FairMatch and w/o Lğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ on CIFAR-10(ğœŒ=10%,ğ‘=0.5)and
CIFAR-100(ğœŒ=20%,ğ‘=0.15), respectively, where w/o Lğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ represents FairMatch without Lğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ .
FairMatch attains the largest accuracy improvements of 9.53% and
4.9%, respectively, and on the fine-grained classification dataset
CIFAR-100H, it achieves 16.45%, showing the superiority of our
method.
â€¢As the partial rate ğ‘of PL samples increases, the performance
of FairMatch slightly decreases, while the comparison methods
deteriorate significantly. For example, on CIFAR-10 ( ğœŒ=10%),
FairMatch decreases from 92.97% ( ğ‘=0.5) to 91.68% ( ğ‘=0.7),
while ConCont and FreeMatch+ plunge by over 8%. As the setting
becomes more challenging, with smaller ğœŒand largerğ‘, FairMatch
shows more remarkable improvement, revealing its potential and
effectiveness.
â€¢As shown in Fig. 4, the transductive accuracy steadily increases
with the training process, demonstrating the effectiveness of our
label disambiguation method. Moreover, the number of selected
and correctly assigned high-confidence samples increases along
with the test accuracy, indicating that the ğ¾determined by Eq.
(4) indeed perceives the learning state of the model.â€¢For most cases of PLL methods, DPLL and PiCO outperform
DPLL* and PiCO*, respectively, implying that PLL methods can-
not effectively utilize unlabeled data and instead harm the model
performance. In a few cases where ğœŒis large and ğ‘is small,
PiCO* surpasses PiCO. This is because PiCO* can identify more
reliable positive pairs with sufficient samples that have few am-
biguous labels, which facilitates its contrastive learning and the
disambiguation for unlabeled samples. However, our method still
outperforms PiCO* significantly in these cases.
â€¢For SSL methods, in most cases, the performance of FixMatch+
and FreeMatch+ using the label disambiguation method in Fair-
Match (our method) is significantly better than their original
versions, which also demonstrates the effectiveness of our label
disambiguation method and the importance of label disambigua-
tion for SSPLL. Moreover, on CIFAR-100 ( ğ‘âˆˆ{0.1,0.15,0.2}),
the performance of FixMatch+ and FreeMatch+ decreases as ğœŒ
increases, which is counter-intuitive. After statistical analysis, we
find that as ğœŒincreases, the transductive accuracy of these two
methods does not increase. This means that more PL samples are
1275KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahao Jiang, Yuheng Jia, Hui Liu, and Junhui Hou
Table 3: Accuracy (mean Â±std) comparisons on CIFAR-100H with proportion ğœŒâˆˆ{10%,20%}and partial rate ğ‘âˆˆ{0.3,0.5,0.7,0.9}.
Dataset Method ğ‘=0.3ğ‘=0.5ğ‘=0.7ğ‘=0.9
DPLL 58.76 Â±0.17% 56.67 Â±0.28% 50.21 Â±0.63% 30.28 Â±1.39%
DPLL* 56.88 Â±0.25% 53.81 Â±0.52% 44.07 Â±0.32% 23.01 Â±1.26%
PiCO 49.50 Â±0.13% 47.59 Â±0.43% 43.66 Â±0.47% 28.87 Â±0.53%
PiCO* 54.38 Â±0.30% 46.87 Â±0.33% 33.05 Â±1.05% 20.55 Â±1.16%
Fixmatch 43.93 Â±3.14% 35.06 Â±1.11% 28.88 Â±0.63% 23.61 Â±0.44%
Fixmatch+ 62.94 Â±0.77% 60.92 Â±0.63% 56.93 Â±0.21% 24.77 Â±1.12%
Freematch 53.52 Â±0.43% 49.53 Â±1.77% 20.69 Â±0.24% 17.28 Â±0.19%
Freematch+ 62.44 Â±0.58% 60.80 Â±0.56% 56.95 Â±0.14% 34.72 Â±1.16%
ConCont 64.08 Â±0.27% 62.66 Â±0.23% 59.83 Â±0.16% 34.68 Â±1.03%CIFAR-100H ( ğœŒ=20%)
FairMatch 66.25 Â±0.50% 65.12 Â±0.68% 62.68 Â±0.42% 51.13 Â±2.03%
DPLL 48.66 Â±0.26% 45.28 Â±0.30% 36.44 Â±0.24% 21.85 Â±0.46%
DPLL* 42.67 Â±0.24% 36.23 Â±1.08% 27.45 Â±0.71% 15.09 Â±0.69%
PiCO 38.90 Â±0.28% 37.35 Â±0.68% 31.43 Â±0.57% 16.96 Â±0.98%
PiCO* 32.85 Â±1.05% 24.78 Â±0.66% 20.10 Â±0.92% 15.55 Â±0.79%
Fixmatch 39.34 Â±1.10% 25.88 Â±0.77% 17.96 Â±0.40% 14.50 Â±0.20%
Fixmatch+ 51.56 Â±0.69% 47.73 Â±0.69% 39.97 Â±2.06% 19.84 Â±1.88%
Freematch 47.18 Â±1.21% 39.98 Â±0.32% 24.44 Â±1.12% 16.51 Â±0.23%
Freematch+ 51.14 Â±0.64% 48.45 Â±1.02% 41.51 Â±1.14% 25.87 Â±0.65%
ConCont 56.99 Â±0.23% 54.51 Â±0.33% 48.32 Â±1.67% 24.65 Â±0.48%CIFAR-100H ( ğœŒ=10%)
FairMatch 58.95 Â±0.35% 56.62 Â±0.55% 52.81 Â±0.43% 39.03 Â±0.17%
Table 4: Classification accuracy (mean Â±std) of different vari-
ants of FairMatch.
MehtodCIFAR-10
(ğœŒ=10%,
ğ‘=0.5)CIFAR-100
(ğœŒ=20%,
ğ‘=0.15)CIFAR-100H
(ğœŒ=10%,
ğ‘=0.7)
FairMatch 93.09 Â±0.03% 59.75 Â±0.77% 52.81 Â±0.43%
w/o CLD 77.62 Â±0.73% 14.56 Â±0.65% 30.36 Â±0.53%
w/oLğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ 91.07 Â±1.85% 36.07 Â±1.84% 38.00 Â±0.79%
w/oLğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ 90.91 Â±0.22% 52.85 Â±0.97% 48.04 Â±0.26%
w/oLğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ 81.93 Â±0.90% 48.39 Â±0.58% 40.66 Â±0.31%
incorrectly disambiguated, leading to performance degradation.
Based on the same label disambiguation method, FairMatch (our
method) does not encounter this issue, indicating the effective-
ness of fair selection and MixUp strategies.
4.3 Further Analysis
4.3.1 Sensitivity Analysis. For FairMatch, warm-up epoch ğ‘‡wand
loss weights ğœ†,ğ›½are relatively important. As shown in Figs. 5a-5c,
we report the results of FairMatch with ğ‘‡ğ‘¤âˆˆ{0,20,50,100,200}
andğœ†,ğ›½âˆˆ{0.1,0.5,1,5,10}. We can find that the results is stable
with the change of ğ‘‡ğ‘¤. Asğœ†increases, the results show a decline
on CIFAR-100 and CIFAR-100H, while the results are relatively
stable with different ğ›½. These results indicate that the weight ğœ†of
Lğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ should not be too large, as the high-confidence samples
selected in the early stages of training are not accurate enough.
4.3.2 Ablation and Analysis. As shown in Eq. (9), the total loss of
FairMatch consists three parts, i.e., Lğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ for PL data,Lğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘
for unlabeled data and Lğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ for MixUp data. Furthermore, the
candidate label disambiguation in Eq. (1) is also crucial for SSPLL. Inorder to verify the effectiveness of these parts, we conduct ablation
experiments in this subsection. Specifically, w/o CLD means Fair-
Match without candidate label disambiguation, and the remaining
three items in Table 4 indicate that FairMatch without the corre-
sponding loss terms. The results in Table 4 clearly validate that all
of the modules mentioned above provide significant improvements
to our method. In addition, Figs. 5d and 5e show that the trans-
ductive accuracy of FairMatch is significantly higher than that of
w/oLğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ , and on CIFAR-10, the transductive accuracy of w/o
Lğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ shows a decrease in the later stages of training. These re-
sults further demonstrate that the MixUp strategy can prevent the
model from overfitting on the few PL samples.
5 CONCLUSION
In this paper, we have presented a novel semi-supervised partial
learning method named FairMatch, which for the first time achieves
fair selection on each class and adaptively determines the number of
high-confidence samples to be selected from the unlabeled samples.
We also adopt label disambiguation to explore the information in
the PL samples, and apply the MixUp strategy to generate new
training samples from high-confidence samples and PL samples to
prevent the model from overfitting on PL samples, which proved to
be effective. Extensive experiments on benchmark datasets clearly
validate the superiority and effectiveness of our method, especially
when facing fewer PL samples with higher partial rate.
6 ACKNOWLEDGMENTS
This work was supported by the National Natural Science Founda-
tion of China under Grant 62106044, Natural Science Foundation of
Jiangsu Province under Grant BK20210221, and Hong Kong UGC
under grant UGC/FDS11/E02/22.
1276FairMatch: Promoting Partial Label Learning by Unlabeled Samples KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
REFERENCES
[1]Ching-Hui Chen, Vishal M. Patel, and Rama Chellappa. 2018. Learning from
ambiguously labeled face images. IEEE Transactions on Pattern Analysis and
Machine Intelligence 40, 7 (2018), 1653â€“1667.
[2]Ekin D. Cubuk, Barret Zoph, Dandelion ManÃ©, Vijay Vasudevan, and Quoc V. Le.
2019. AutoAugment: Learning Augmentation Strategies From Data. In Proceedings
of IEEE/CVF Conference on Computer Vision and Pattern Recognition. 113â€“123.
[3]Terrance DeVries and Graham W. Taylor. 2017. Improved Regularization of
Convolutional Neural Networks with Cutout. arXiv:1708.04552
[4]Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020.
Momentum Contrast for Unsupervised Visual Representation Learning. In Pro-
ceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition.
9726â€“9735.
[5]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of IEEE/CVF conference on computer
vision and pattern recognition. 770â€“778.
[6]Eyke HÃ¼llermeier and JÃ¼rgen Beringer. 2006. Learning from ambiguously labeled
examples. Intelligent Data Analysis 10, 5 (2006), 419â€“439.
[7]Yuheng Jia, Jiahao Jiang, and Yongheng Wang. 2023. Semantic Dissimilarity
Guided Locality Preserving Projections for Partial Label Dimensionality Re-
duction. In Proceedings of the 29th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. 964â€“973.
[8]Yuheng Jia, Xiaorui Peng, Ran Wang, and Min-Ling Zhang. 2024. Long-Tailed
Partial Label Learning by Head Classifier and Tail Classifier Cooperation. In
Proceedings of Thirty-Eighth AAAI Conference on Artificial Intelligence. 12857â€“
12865.
[9]Yuheng Jia, Chongjie Si, and Min-Ling Zhang. 2023. Complementary Classi-
fier Induced Partial Label Learning. In Proceedings of the 29th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 974â€“983.
[10] Yuheng Jia, Fuchao Yang, and Yongqiang Dong. 2023. Partial Label Learning
with Dissimilarity Propagation guided Candidate Label Shrinkage. In Advances
in Neural Information Processing Systems.
[11] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[12] Jie Luo and Francesco Orabona. 2010. Learning from candidate labeling sets. In
Advances in Neural Information Processing Systems. 1504â€“1512.
[13] Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. [n. d.].
Progressive Identification of True Labels for Partial-Label Learning. In Proceedings
of the 37th International Conference on Machine Learning, Vol. 119. 6500â€“6510.
[14] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin
Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chunliang Li. 2020. FixMatch:
Simplifying Semi-Supervised Learning with Consistency and Confidence. In
Advances in Neural Information Processing Systems.[15] Deng-Bao Wang, Li Li, and Min-Ling Zhang. 2019. Adaptive Graph Guided
Disambiguation for Partial Label Learning. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 83â€“91.
[16] Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo
Zhao. 2022. PiCO: Contrastive Label Disambiguation for Partial Label Learning.
InProceedings of the Tenth International Conference on Learning Representations.
[17] Qian-Wei Wang, Yu-Feng Li, and Zhi-Hua Zhou. 2019. Partial label learning with
unlabeled data. In Proceedings of the Twenty-Eighth International Joint Conference
on Artificial Intelligence. 3755â€“3761.
[18] Qian-Wei Wang, Bowen Zhao, Mingyan Zhu, Tianxiang Li, Zimo Liu, and Shu-
Tao Xia. 2023. Controller-Guided Partial Label Consistency Regularization with
Unlabeled Data. arXiv:2210.11194
[19] Wei Wang and Min-Ling Zhang. 2020. Semi-supervised partial label learning via
confidence-rated margin maximization. In Proceedings of the 34th International
Conference on Neural Information Processing Systems. 6982â€“6993.
[20] Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong
Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and
Xing Xie. 2023. FreeMatch: Self-adaptive Thresholding for Semi-supervised
Learning. In Proceedings of the Eleventh International Conference on Learning
Representations.
[21] Dong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. 2022. Revisiting Con-
sistency Regularization for Deep Partial Label Learning. In Proceedings of 39th
International Conference on Machine Learning. 24212â€“24225.
[22] Shiyu Xia, Jiaqi Lv, Ning Xu, and Xin Geng. 2022. Ambiguity-Induced Contrastive
Learning for Instance-Dependent Partial Label Learning. In Proceedings of the
Thirty-First International Joint Conference on Artificial Intelligence, Luc De Raedt
(Ed.). 3615â€“3621.
[23] Shiyu Xia, Jiaqi Lv, Ning Xu, Gang Niu, and Xin Geng. 2023. Towards Effective
Visual Representations for Partial-Label Learning. In Proceedings of IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 15589â€“15598.
[24] Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu,
and Yi Ma. 2013. Learning by Associating Ambiguously Labeled Images. In
Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition.
708â€“715.
[25] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu
Okumura, and Takahiro Shinozaki. 2021. FlexMatch: Boosting Semi-Supervised
Learning with Curriculum Pseudo Labeling. In Advances in Neural Information
Processing Systems. 18408â€“18419.
[26] Hongyi Zhang, Moustapha CissÃ©, Yann N. Dauphin, and David Lopez-Paz. 2018.
mixup: Beyond Empirical Risk Minimization. In Proceedings of the sixth Interna-
tional Conference on Learning Representations.
[27] Min-Ling Zhang, Bin-Bin Zhou, and Xu-Ying Liu. 2016. Partial label learning via
feature-aware disambiguation. In Proceedings of the 22nd ACM SIGKDD interna-
tional conference on knowledge discovery and data mining. 1335â€“1344.
1277KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahao Jiang, Yuheng Jia, Hui Liu, and Junhui Hou
A ALGORITHM
We present the complete algorithm for FairMatch in Algorithm 1
and demonstrate the detailed training process.
Algorithm 1: Pseudo Code of FairMatch
Input: Training setsDğ‘andDğ‘¢, mini-batch size ğµ,
training epoch ğ‘‡max, warm-up epoch ğ‘‡w,
hyperparameters ğœ‡,ğ›¼,ğœ†,ğ›½ .
1forğ‘¡=1,2,...,ğ‘‡ğ‘šğ‘ğ‘¥do
2 ifğ‘¡â‰¤ğ‘‡wthen
3 Warm-up;
4 else
5 ShuffleDğ‘¢into|Dğ‘¢|
ğœ‡ğµmini-batches;
6 forğ‘=1,2,...,|Dğ‘¢|
ğœ‡ğµdo
7 Sample partial label mini-batch Bğ‘fromDğ‘;
8 Upadate the labeling confidence by Eq. (1);
9 CalculateLğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ ofBğ‘by Eq. (2);
10 Calculate the number of samples ğ¾to be
selected on each class by Eq. (4);
11 CalculateLğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ ofBğ‘¢by Eq. (5);
12 Generate the MixUp samples Bğ‘›ğ‘’ğ‘¤by Eqs. (6-7);
13 CalculateLğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ ofBğ‘›ğ‘’ğ‘¤by Eq. (8);
14 Minimize the total loss Lin Eq. (9);
15 end
16 end
17end
Output: Model parameters.B RESULTS ON STL-10
STL-10 contains 5,000 labeled images of size 96 Ã—96 from 10 classes
and 100,000 unlabeled images including out-of-distribution unla-
beled images. On STL-10, 5,000 labeled samples will be converted
into partial label samples. As shown in 5, FairMatch outperforms the
comparison methods, indicating that our method is also effective
on large datasets of both size and quantity.
Table 5: Accuracy (mean Â±std) comparisons on STL-10 with
partial rate ğ‘âˆˆ{0.1,0.3,0.5,0.7}.
Metho
dğ‘=0.1ğ‘=0.3ğ‘=0.5ğ‘=0.7
Fixmatch+
90.70 Â±0.34% 89.53 Â±0.43% 87.27 Â±0.52% 81.99 Â±1.28%
Freematch+ 90.77 Â±0.12% 89.65 Â±0.39% 87.44 Â±0.16% 83.15 Â±0.09%
ConCont 89.37 Â±0.16% 87.91 Â±0.24% 85.72 Â±0.29% 79.15 Â±2.17%
Method 91.84 Â±0.16% 90.94 Â±0.16% 88.24 Â±0.50% 85.56 Â±0.57%
1278