Rotative Factorization Machines
Zhen Tianâ€ 
Gaoling School of Artificial
Intelligence, Renmin University of
China
Beijing, China
chenyuwuxinn@gmail.comYuhong Shiâ€¡
Zhejiang University
Hangzhou, China
shi.yh@zju.edu.cnXiangkun Wuâ€¡
Zhejiang University
Hangzhou, China
xkwsta@zju.edu.cn
Wayne Xin Zhaoâˆ—â€ 
Gaoling School of Artificial
Intelligence, Renmin University of
China
Beijing, China
batmanfly@gmail.comJi-Rong Wenâ€ 
Gaoling School of Artificial
Intelligence, Renmin University of
China
Beijing, China
jrwen@ruc.edu.cn
ABSTRACT
Feature interaction learning (FIL) focuses on capturing the complex
relationships among multiple features for building predictive mod-
els, which is widely used in real-world tasks. Despite the research
progress, existing FIL methods suffer from two major limitations.
Firstly, they mainly model the feature interactions within a bounded
order (e.g., small integer order) due to the exponential growth of the
interaction terms. Secondly, the interaction order of each feature is
often independently learned, which lacks the flexibility to capture
the feature dependencies in varying contexts.
To address these issues, we present Rotative Factorization Ma-
chines (RFM), based on the key idea that represents each feature
as a polar angle in the complex plane. As such, the feature inter-
actions are converted into a series of complex rotations, where
the orders are cast into the rotation coefficients, thereby allowing
for the learning of arbitrarily large order. Further, we propose a
novel self-attentive rotation function that models the rotation coef-
ficients through a rotation-based attention mechanism, which can
adaptively learn the interaction orders under different interaction
contexts. Moreover, it incorporates a modulus amplification net-
work to learn the modulus of the complex features, which further
enhances the expressive capacity. Our proposed approach provides
a general FIL framework, and many existing models can be instan-
tiated in this framework, e.g.,factorization machines. In theory, it
possesses more strong capacities to model complex feature rela-
tionships, and can learn arbitrary features from varied contexts.
â€¡Equal contribution.
âˆ—Wayne Xin Zhao (batmanfly@gmail.com) is the corresponding author.
â€ Also with Beijing Key Laboratory of Big Data Management and Analysis Methods.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671740Extensive experiments conducted on five widely used datasets have
demonstrated the effectiveness of our approach.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks.
KEYWORDS
Factorization Machine, Feature Interactions
ACM Reference Format:
Zhen Tianâ€ , Yuhong Shiâ€¡, Xiangkun Wuâ€¡, Wayne Xin Zhaoâˆ—â€ , and Ji-Rong
Wenâ€ . 2024. Rotative Factorization Machines. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671740
1 INTRODUCTION
Feature interaction learning (FIL) is a fundamental technique to sup-
port many real-world predictive tasks, such as click-through rate
(CTR) prediction [ 23,36,41] and product recommendation [ 19,28],
aiming to accurately model the complex relationship among multi-
ple features. Various methods [ 42] have been proposed for learning
effective feature interactions, from early factorization machines
(e.g., FM [ 23]) to recent deep neural networks (e.g., CrossNet [ 37]).
Typically, existing FIL methods are often developed based on
a combination of multiple feature interaction terms, where each
term is modeled as a combination of feature embeddings with their
respective interaction orders, formally denoted by ğ’†ğ›¼1
1âŠ™Â·Â·Â·âŠ™ ğ’†ğ›¼ğ‘šğ‘š
(â€œâŠ™â€ denotes the embedding combination operation). The order ğ›¼ğ‘—
determines the effect of the ğ‘—-th feature embedding ğ’†ğ‘—. To learn the
feature interactions, prior studies [ 15,37,41] often set a maximal
order and conduct feature interactions within the predefined order.
Despite the progress, they suffer from the limitation in the learning
with restricted orders (e.g., integer-only order [15]). Further, due to
the exponential growth of feature combinations, these methods can
only learn the interactions within a small order to maintain the
efficiency, e.g.,second-order feature interactions for FM [23].
Considering the above issues, several studies [ 4,7,31] propose to
automatically learn the interaction orders from data. The core idea
of these approaches is to map features into a special vector space
 
2912
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
(e.g., logarithmic vector space [ 7]). As such, the exponential form of
interaction terms (i.e.,Ãğ’†ğ›¼ğ‘—
ğ‘—) is converted to linear combinations
(i.e., exp(Ãğ›¼ğ‘—logğ’†ğ‘—)), and the orders (i.e., ğ›¼ğ‘—) are cast into learn-
able linear coefficients, allowing for the learning of adaptive-order
interactions. These methods learn the orders either in a field-aware
way or in an instance-aware way. As shown in Figure 1(a) and
Figure 1(b), given two fields along with their feature interaction,
field-aware methods learn a shared order for all features from the
same field (e.g., ğ›¼ğºis shared by Male,Female for field Gender ),
capturing the field-level importance, whereas the instance-aware
methods assign a specific order for each feature (e.g., ğ›¼ğ‘€,ğ›¼ğ¹for
Male,Female) to learn the feature importance.
Although these approaches are more capable of learning the un-
derlying relationships in real-world scenarios, they still suffer from
two main limitations. First, the interaction order of each feature
is independently learned, which lacks the flexibility to capture the
feature dependencies in varying contexts. As increasing evidence
shows [ 35], in real-world applications, the importance of a certain
feature is often influenced by other features. For example, consid-
ering the feature interaction âŸ¨UserGender ,MovieGenre ,ActorâŸ©
in movie recommendation, the Actor features may have different
effects for idolandhorror movie genres. However, it is difficult for
both field-aware and instance-aware models to adaptively capture
varying feature importance in different contexts. As such, we ar-
gue that the importance of a specific feature should be adaptively
learned depending on its relation with the other features involved
in the specific context, which is referred to as relation-aware in this
paper (See Figure 1(c)). Second, since the interaction terms expo-
nentially grow with the order, existing methods can only model the
interactions within a bounded order, which cannot scale to the high-
order cases in industrial scenarios. Considering the two limitations,
we aim to seek a more effective approach to adaptively learn the
interaction order in a relation-aware way, meanwhile surpass the
scale limits of high interaction order in existing work.
ğ’†ğ‘´ğœ¶1âŠ™ğ’†ğ‘¶ğœ·ğŸ
ğ’†ğ‘´ğœ¶2âŠ™ğ’†ğ’€ğœ·ğŸ
ğ’†ğ‘­ğœ¶3âŠ™ğ’†ğ‘¶ğœ·ğŸ‘
(a)Field -aware (b)Instance -aware (c) Relation -aware
(AFN, EulerNet) (ARM -Net) (Ours)ğœ¶ğ‘®ğœ¶1=ğœ¶ğŸ=ğœ¶ğŸ‘=ğœ¶ğ‘®
ğ’†ğ‘´ğœ¶1âŠ™ğ’†ğ‘¶ğœ·ğŸ
ğ’†ğ‘´ğœ¶2âŠ™ğ’†ğ’€ğœ·ğŸ
ğ’†ğ‘­ğœ¶3âŠ™ğ’†ğ‘¶ğœ·ğŸ‘ğœ¶ğ‘´ğœ¶1=ğœ¶ğŸ=ğœ¶ğ‘´ ğœ¶ğŸ‘=ğœ¶ğ‘­
ğœ¶ğ‘­ğ’†ğ‘´ğœ¶1âŠ™ğ’†ğ‘¶ğœ·ğŸ
ğ’†ğ‘´ğœ¶2âŠ™ğ’†ğ’€ğœ·ğŸ
ğ’†ğ‘­ğœ¶3âŠ™ğ’†ğ‘¶ğœ·ğŸ‘ğœ¶1=ğœ¶ğ‘´,ğ‘¶ğœ¶2=ğœ¶ğ‘´,ğ’€ğœ¶ğŸ‘=ğœ¶ğ‘­,ğ‘¶
ğœ¶ğ‘´,ğ‘¶
ğœ¶ğ‘´,ğ’€
ğœ¶ğ‘­,ğ‘¶Fieldï¼š
Instance
Fieldï¼šMale  (M)FeMale  (F)Field
Younger  (Y)Older  (O) Gender (G) Age (O)Instance
Male  (M) FeMale  (F)Feature Field Feature Instance
Younger  (Y) Older  (O)
 Gender (G) Age (O)Feature Field Feature InstanceField Instance
Figure 1: Comparisons of three feature interaction approaches. Field-
aware methods set a fixed interaction order for each feature field ;
instance-aware methods set a unique interaction order for each
feature instance (a.k.a., feature value ); relation-aware methods set a
unique interaction order for each feature combination.
To this end, this paper presents the novel Rotative Factorization
Machines (RFM), for adaptively learning the unbounded-order fea-
ture interactions in a relation-aware way. The key idea of RFM is
to represent each feature as a polar angle (i.e.,ğ‘’ğ‘–ğœ½ğ‘—) in the complex
plane, and conduct the attentive rotations to model complicated
feature interactions. For learning the unbounded-order feature in-
teractions, RFM converts the feature interactions into the com-
plex rotations (i.e.,exp(ğ‘–Ãğ›¼ğ‘—ğœ½ğ‘—)), where the interaction orders arecast into the rotation coefficients (i.e.,ğ›¼ğ‘—), thereby avoiding the
exponential explosion of the interaction terms. For learning the
feature interactions in a relation-aware way, we propose a novel
self-attentive rotation function (i.e., exp(ğ‘–Ãğ›¼ğ‘—,ğ‘™ğœ½ğ‘™)), where the rota-
tion coefficients (i.e., ğ›¼ğ‘—,ğ‘™) are learned by a rotation-based attention
mechanism, capturing the dependencies between feature ğ‘—andğ‘™.
Moreover, we devise a modulus amplification network to learn
the modulus of the complex features (extending the above unit
modulus), which enhances the expressive capacity of the learned
complex representations. Such a network can model all three types
of feature interaction patterns (i.e., field-aware, instance-aware and
relation-aware ), with no need of pre-defined order coefficients.
To our knowledge, it is the first work that is capable of adap-
tively learning the interactions with arbitrarily large order from
the corresponding contexts. Furthermore, it has been proven that
our approach can be instantiated to a variety of traditional inner-
product based interaction models (e.g., FM [ 23]). To evaluate our
model, we conduct extensive experiments on five public datasets,
and the experimental results show that our model consistently out-
performs a number of competitive feature interaction approaches.
2 PRELIMINARY
As the key technique in many prediction tasks [ 40,43], feature
interaction modeling aims to capture the underlying relationships
among multiple features. It takes as input a concatenated vector
of features, denoted as ğ’™=[ğ’™1,...,ğ’™ğ‘š], whereğ‘šrepresents the
number of feature fields (e.g., Gender), and ğ’™ğ‘—is the one-hot vector
of a feature instance (e.g., Male) in the ğ‘—-th field. Due to the high-
dimensional, sparse nature of ğ’™, an embedding look-up operation
E(Â·)is often used to map each feature into a ğ‘‘-dimensional vector
ğ’†ğ‘—=E(ğ’™ğ‘—)âˆˆRğ‘‘. In this context, the feature interaction learning
functionF(Â·) is commonly defined as:
F(A) =âˆ‘ï¸
ğœ¶âˆˆAğ’†ğ›¼1
1âŠ™ğ’†ğ›¼2
2âŠ™Â·Â·Â·âŠ™ ğ’†ğ›¼ğ‘šğ‘š, (1)
where â€œâŠ™â€ denotes the element-wise product, Arepresents the set
of all interaction orders, and each ğœ¶âˆˆA specifies the order for
each feature. Existing methods often manually set feature inter-
action orders. For instance, FM [ 23] assignsA={ğœ¶|Ãğ‘š
ğ‘—=1ğ›¼ğ‘—=
2,âˆ€ğ›¼ğ‘—âˆˆ{0,1}}to capture second-order feature interactions. Fur-
ther, AFN [ 7] and EulerNet [ 31] propose automatically learning
orders (i.e.,A) from data. However, they primarily capture field-
aware interactions, where the order ğœ¶is shared across all features
within a field. Another study ARM-Net [ 4] introduces a gated at-
tention Gate(Â·)forinstance-aware interactions, with ğ›¼ğ‘—=Gate(ğ’†ğ‘—)
evaluating feature importance. In contrast, we focus on learning
relation-aware interactions, where ğ›¼ğ‘—considers the dependencies
between ğ’†ğ‘—and other features.
3 METHODOLOGY
In this section, we present the proposed Rotative Factorization
Machines (RFM) (Figure 2(a)) for effectively modeling feature in-
teractions in various prediction tasks. As the core idea, we repre-
sent each feature as a polar angle in the complex plane and use the
attentive rotation to model complicated feature interactions. For
learning relation-aware interactions, we propose a self-attentive
 
2913Rotative Factorization Machines KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Input FeaturesAngular EmbeddingAdd & Norm
ğ‘µÃ—AmplificationGroup NormAdd
ğ‘³Ã—Prediction
ğ’†ğ’Šğœ½â‡’ğ’„ğ’ğ’”ğœ½ +ğ’Šğ’”ğ’Šğ’ğœ½
Self-Attentive RotationMulti -Head
(a) Model Architecture (b) Self -Attentive Rotation MechanismQuery
KeyWeightCosineDotğ’˜Rotation -based Attention
ğ‘ºğ’Šğ’ˆğ’ğ’ğ’Šğ’…
rp
0ğ‘¸ğ’‹
ğ‘²ğ’ğ‘ºğ’Šğ’ˆğ’ğ’ğ’Š ğ’…ğ’˜ğ‘»ğ’„ğ’ğ’” ğ‘¸ğ’‹âˆ’ğ‘²ğ’ ğœ¶ğ’‹,ğ’â†ğ‘¹ğ’ğ’•ğ‘¨ğ’•ğ’• (ğ‘¸ğ’‹,ğ‘²ğ’)Self-Attentive Rotation
0 rà·¤ğ’†ğŸğœ¶ğ’‹,ğŸâŠ™â€¦âŠ™â€¦âŠ™à·¤ğ’†ğ’ğœ¶ğ’‹,ğ’
âŠ™
â€¦
âŠ™
à·¤ğ’†ğ’ğœ¶ğ’‹,ğ’pAttentive Rotations
à·¤ğ’†ğŸğœ¶ğ’‹,ğŸ â‹¯âŠ™ à·¤ğ’†ğ’ğœ¶ğ’‹,ğ’Â·Â·âŠ™à·¤ğ’†ğ’ğœ¶ğ’‹,ğ’ğ‘¸ğŸğ‘¸ğ’‹ ğ‘¸ğ’ ğ‘¸ğ’
ğ‘²ğŸ ğ‘²ğ’‹ ğ‘²ğ’ ğ‘²ğ’
ğ‘½ğŸ ğ‘½ğ’‹ ğ‘½ğ’ ğ‘½ğ’
ğ‘¹ğ’ğ’•ğ‘¨ğ’•ğ’• (ğ‘¸ğ’‹,ğ‘²ğ’)
â€¦ğœ½ğŸ
ğœ½ğ’ğœ½ğŸInput Angles
exp ğ‘–(Ïƒğœ¶ğ’‹,ğ’ğ‘½ğ’)
Figure 2: Architecture and components of our proposed rotative factorization machines.
rotation layer, which can adaptively learn the orders from different
interaction contexts. Moreover, a modulus amplification network
is incorporated to learn the modulus of the complex features for
enhancing the expressive capacity. In what follows, we introduce
the details of relation-aware interaction modeling (Section 3.1) and
the modulus amplification network (Section 3.2).
3.1 Relation-Aware Feature Interaction
In this part, we propose a novel self-attentive rotation layer for
learning the relation-aware feature interactions, in which it rep-
resents each feature as a polar angle in the complex plane and
employs complex rotation to model complex feature interaction.
3.1.1 Angular Representation of Features. As mentioned in Sec-
tion 2, the feature ğ’™ğ‘—can be mapped into a vector embedding via
the look-up operation E(Â·). In order to support arbitrary interaction,
our solution is to represent the features as a set of polar angles in
the complex plane as follows:
ğœ½ğ‘—=E(ğ’™ğ‘—),ğ’†ğ‘—=ğ‘’ğ‘–ğœ½ğ‘—, (2)
whereğ‘–is the imaginary unit that satisfies ğ‘–2=âˆ’1. In this way,
given the angular feature representations {ğ’†ğ‘—}ğ‘š
ğ‘—=1âˆˆCğ‘šÃ—ğ‘‘, the
interactions are cast into a series of complex rotations :
F(A) =âˆ‘ï¸
ğœ¶âˆˆAğ’†ğ›¼1
1âŠ™Â·Â·Â·âŠ™ ğ’†ğ›¼ğ‘šğ‘š=âˆ‘ï¸
ğœ¶âˆˆAexp
ğ‘–ğ‘šâˆ‘ï¸
ğ‘—=1ğ›¼ğ‘—ğœ½ğ‘—
|        {z        }
Complex Rotation.(3)
In mathematics, a complex rotation (i.e.,exp(ğ‘–Ãğ‘š
ğ‘—=1ğ›¼ğ‘—ğœ½ğ‘—)) performs
a linear transformation on the phase of the complex vectors without
affecting their modulus. In our case, we utilize it to model the feature
interaction, and use the rotation coefficients (i.e.,ğ›¼ğ‘—) to model the
interaction orders. As such, the interactions are learned on a unit
circle (i.e.,modulus are fixed to 1) with a finite norm:
||F(A)|| =âˆ‘ï¸
ğœ¶âˆˆAexp
ğ‘–ğ‘šâˆ‘ï¸
ğ‘—=1ğ›¼ğ‘—ğœ½ğ‘—
â‰¤âˆ‘ï¸
ğœ¶âˆˆAexp
ğ‘–ğ‘šâˆ‘ï¸
ğ‘—=1ğ›¼ğ‘—ğœ½ğ‘—â‰¤|A|ğ‘‘.(4)Since the upper bound is independent of the order ğ›¼ğ‘—, it can effec-
tively learn complicated interactions with arbitrarily large order,
without limitations in prior work (e.g., exponential explosion ).
3.1.2 Self-Attentive Rotation. The self-attentive rotation layer is
the core of our proposed RFM for learning relation-aware feature
interactions. As shown in Figure 2(b), the key idea is to conduct
theattentive rotation with the rotation coefficients modeled by
arotation-based attention mechanism, thereby allowing for the
adaptive learning of feature dependencies in the varying context. It
takes as input a set of angles and outputs a set of rotated angles, and
then we can stack multiple such layers to form a capable network.
Next we describe the attentive rotation within a single layer.
Rotation-based Attention for Attentive Rotations. As shown
in Eq. (3), the interaction with the order ğœ¶is cast into a complex rota-
tion (i.e., exp(ğ‘–Ãğ‘š
ğ‘—=1ğ›¼ğ‘—ğœ½ğ‘—)). To learn the relation-aware interaction,
a major issue is how to effectively model the feature dependencies
in varying contexts. The original self-attention mechanism [ 34]
is designed to model relationships for real vectors, but not suit-
able for modeling relationships among angular representations. As
our solution, we propose a rotation-based attention mechanism to
adaptively model the rotation coefficients (i.e., ğ›¼ğ‘—), which enables
it to effectively learn the dependencies between different angle-
represented features. As shown in Figure 2(b), we adopt a key-value
based self-attention to conduct the attentive rotation. Specifically,
the query-key pairs with similar angles are considered more impor-
tant. Given the input {ğœ½ğ‘—}ğ‘š
ğ‘—=1, the dependency between feature ğ‘—
andğ‘™is learned by the rotation angle from key to query:
ğ›¼ğ‘—,ğ‘™=RotAtt(ğ‘¸ğ‘—,ğ‘²ğ‘™)=Sigmoid(ğ’˜âŠ¤cos(ğœ½ğ‘„
ğ‘—âˆ’ğœ½ğ¾
ğ‘™)), (5)
ğ‘¸âŠ¤
ğ‘—=ğœ½ğ‘„
ğ‘—=ğ‘¾ğ‘„
ğ‘—ğœ½ğ‘—,ğ‘²âŠ¤
ğ‘™=ğœ½ğ¾
ğ‘™=ğ‘¾ğ¾
ğ‘™ğœ½ğ‘™, (6)
where ğ’˜âˆˆRğ‘‘is a weight vector to learn. Given ğ‘šfeature fields,
to enhance the field semantics, we utilize a set of field-specific ma-
trices{ğ‘¾ğ‘„
ğ‘—}ğ‘š
ğ‘—=1,{ğ‘¾ğ¾
ğ‘™}ğ‘š
ğ‘™=1to map the features into a set of queries
{ğœ½ğ‘„
ğ‘—}ğ‘š
ğ‘—=1and keys{ğœ½ğ¾
ğ‘™}ğ‘š
ğ‘™=1, and pack them together into two matri-
cesğ‘¸,ğ‘²âˆˆRğ‘šÃ—ğ‘‘â€²for modeling the interaction orders. Accordingly,
the value vectors{ğœ½ğ‘‰
ğ‘—}ğ‘š
ğ‘—=1are packed into matrix ğ‘½âˆˆRğ‘šÃ—ğ‘‘â€², which
 
2914KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
represents the polar angles of the complex features {Ëœğ’†ğ‘—}ğ‘š
ğ‘—=1(i.e.,
Ëœğ’†ğ‘—=exp(ğ‘–ğœ½ğ‘‰
ğ‘—)). Further, we aggregate all contextual information of
featureğ‘—asËœğœ½ğ‘—=Ãğ‘š
ğ‘™=1ğ›¼ğ‘—,ğ‘™ğœ½ğ‘‰
ğ‘™. As such, the interaction with order
Ağ‘—={ğœ¶ğ‘—}is cast into a self-attentive rotation with coefficients
learned by the proposed rotation-based attention (Eq. (5)):
F(Ağ‘—)=Ëœğ’†ğ›¼ğ‘—,1
1âŠ™Ëœğ’†ğ›¼ğ‘—,2
2âŠ™Â·Â·Â·âŠ™ Ëœğ’†ğ›¼ğ‘—,ğ‘š
ğ‘š
=exp
ğ‘–ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘—,ğ‘™ğœ½ğ‘‰
ğ‘™
|           {z           }
Self-Attentive Rotation=exp(ğ‘–Ëœğœ½ğ‘—). (7)
This formula is the core of RFM for learning relation-aware in-
teraction. The rotation coefficient ğ›¼ğ‘—,ğ‘™, which also represents the
interaction order, can capture the dependencies between feature ğ‘—
andğ‘™. Further, we pack the orders into a matrix ğ‘¨(i.e.,ğ‘¨ğ‘—,ğ‘™=ğ›¼ğ‘—,ğ‘™),
to aggregate the contextual information of all features:
AttentiveRot(ğ‘¸,ğ‘²,ğ‘½)=[Ëœğœ½1,Ëœğœ½2,Â·Â·Â·,Ëœğœ½ğ‘š]âŠ¤=ğ‘¨ğ‘½. (8)
Formally, the tensor-form calculation of the rotation-based atten-
tion score can be also given by:
ğ‘¨=RotAtt(ğ‘¸,ğ‘²)=Sigmoid
Reh
exp(ğ‘–ğ‘¸)diag(ğ’˜)
exp(âˆ’ğ‘–ğ‘²)âŠ¤i
,
(9)
where Re[Â·]is real parts of complex vectors. Proof in Appendix A.1.
Multi-Head Rotation. To learn diversified contextual information
from different subspaces, we further extend the above transforma-
tion to multi-head rotation. Specifically, we introduce â„independent
attention heads performing the rotation function of Eq. (8), and
then concatenate them to obtain final representations:
MultiHeadRo(ğ‘¸,ğ‘²,ğ‘½)=Concat(head 1,head 2,...,headâ„),(10)
headğ‘—=AttentiveRot(ğ‘¸ğ‘¯ğ‘„
ğ‘—,ğ‘²ğ‘¯ğ¾
ğ‘—,ğ‘½ ğ‘¯ğ‘‰
ğ‘—),(11)
where ğ‘¯ğ‘„
ğ‘—,ğ‘¯ğ¾
ğ‘—,ğ‘¯ğ‘‰
ğ‘—âˆˆRğ‘‘â€²Ã—ğ‘‘â„are projection matrices, ğ‘‘â„=ğ‘‘â€²/â„.
In this way, we can use the head number â„to control the number
of interaction terms. Specifically, each head models ğ‘šinteraction
terms, where each term (i.e., Ëœğ’†ğ›¼ğ‘—,1
1âŠ™Â·Â·Â·âŠ™ Ëœğ’†ğ›¼ğ‘—,ğ‘š
ğ‘š) corresponds to the
attention scores ğ‘¨ğ‘—(i.e.,ğ‘—-th row of ğ‘¨), and thus the total interaction
term number is ğ‘šÂ·â„. Further, we can stack multiple layers by taking
the outputs of the previous layer as the input for the next layer, and
set varying â„at different layers to increase the model flexibility.
In addition, to preserve the previously learned representations,
we follow the standard transformer [ 34] that employs a residual
connection with a layer normalization [1] around each layer.
3.2 Modulus Amplification for Enhanced
Feature Interaction Learning
In the above rotation process, the features are limited to a unit circle
with a fixed modulus of one, which may limit the modelâ€™s capacity.
To further enhance the interaction learning, we devise a modulus
amplification network to learn the modulus of the features.
Coordinate Transformation. In the above rotation process, the
modulus of the complex features are fixed at 1, which are difficult
to be identified or learned. Instead of directly learning the modulusof the complex features, our solution is to convert the polar angle-
represented features into the real-imaginary represented complex
vectors, enabling the model to effectively learn the modulus of
different complex features. Specifically, given the output represen-
tationğ‘’ğ‘–Ëœğœ½ğ‘—(See Eq. (8)) of the last self-attentive rotation layer, we
use the Eulerâ€™s formula (i.e., ğ‘’ğ‘–ğœ½=cosğœ½+ğ‘–sinğœ½) to obtain its real
and imaginary parts:
ğ’“ğ‘—=cosËœğœ½ğ‘—,ğ’‘ğ‘—=sinËœğœ½ğ‘—, (12)
whereğ‘—âˆˆ {1,...,ğ‘š}. After the coordinate transformation, each
feature is represented by a rectangular-form complex vector, i.e.,
ğ’“ğ‘—+ğ‘–ğ’‘ğ‘—. Then, we utilize the complex representations {ğ’“ğ‘—+ğ‘–ğ’‘ğ‘—}ğ‘š
ğ‘—=1
for subsequent modulus amplification.
Modulus Amplification. Given the representations in the rectan-
gular form{ğ’“ğ‘—+ğ‘–ğ’‘ğ‘—}ğ‘š
ğ‘—=1, we concatenate their real and imaginary
parts and feed them into a shared multi-layer perceptron (MLP):
ğ’“(0)=Concat(ğ’“1,...,ğ’“ğ‘š),ğ’‘(0)=Concat(ğ’‘1,...,ğ’‘ğ‘š), (13)
ğ’“(ğ‘˜)=GN(ğœ(ğ‘¾ğ‘˜ğ’“(ğ‘˜âˆ’1)+ğ’ƒğ‘˜)),ğ’‘(ğ‘˜)=GN(ğœ(ğ‘¾ğ‘˜ğ’‘(ğ‘˜âˆ’1)+ğ’ƒğ‘˜)),
(14)
whereğ‘˜âˆˆ{1,2,...,ğ¿},ğ¿is the depth, ğœis the activation function,
ğ‘¾ğ‘˜andğ’ƒğ‘˜are the weight and bias of the ğ‘˜-th layer. In the above
transformations, all feature vectors are concatenated into a long
hidden vector as the input of the MLP, which may be difficult to
distinguish semantic information from different features. To address
this problem, we use the group normalization [ 39] (i.e., GN(Â·)) to
preserve the feature-wise information. Formally, given the input
vector ( e.g.,ğ’“âˆˆRğ‘‘ğ‘˜, whereğ‘‘ğ‘˜is the hidden dimension of ğ‘˜-th layer),
we view it as having ğ‘™latent features (i.e., ğ’“=[ğ’“1;ğ’“2;...;ğ’“ğ‘™],ğ‘™|ğ‘‘ğ‘˜),
andGN(Â·)is formulated as follows:
GN(ğ’“ğ‘—)=ğ›¾Â·ğ’“ğ‘—âˆ’ğœ‡ğ‘—âˆšï¸ƒ
ğœ2
ğ‘—+ğœ–+ğ›½, (15)
whereğ‘—âˆˆ {1,2,...,ğ‘™},ğœ‡ğ‘—andğœğ‘—denote the mean and standard
deviation of ğ’“ğ‘—, the scaling parameter ğ›¾and shift parameter ğ›½are
trainable to enhance the representation of the GN (Â·)layer.
Prediction and Training. For prediction, we follow the prior
work [ 31] that incorporates a transformation vector ğ’–to project
the representation of the last layer (i.e., ğ’“(ğ¿)+ğ‘–ğ’‘(ğ¿)):
ğ‘§=ğ’–âŠ¤(ğ’“(ğ¿)+ğ‘–ğ’‘(ğ¿))=ğ‘§ğ‘Ÿ+ğ‘–ğ‘§ğ‘, (16)
Ë†ğ‘¦=ğœ(ğ‘§ğ‘Ÿ+ğ‘§ğ‘). (17)
RFM can be applied to a variety of tasks, such as classification
and regression. Taking the binary classification tasks (e.g., click-
through rate prediction) for example, we use the widely-used binary
cross-entropy loss with a regularization term to train our model:
L(ğš¯)=âˆ’1
ğ‘ğ‘âˆ‘ï¸
ğ‘—=1
ğ‘¦ğ‘—log(Ë†ğ‘¦ğ‘—)+(1âˆ’ğ‘¦ğ‘—)log(1âˆ’Ë†ğ‘¦ğ‘—)
+ğœ†||ğš¯||2
2,(18)
whereğ‘¦ğ‘—and Ë†ğ‘¦ğ‘—are the ground-truth and predicted label of the
ğ‘—-th training sample respectively, ğš¯is the set of model parameters,
andğœ†is theğ¿2-norm penalty.
 
2915Rotative Factorization Machines KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
3.3 Discussion
After introducing the model details, we summarize the advantage
of RFM and discuss the difference with existing work.
Model Merits. Our approach has the following major merits:
â€¢Model capacity : RFM can effectively model all three types of
feature interaction patterns (i.e., field-aware, instance-aware and
relation-aware ) introduced in the Figure 1, and meanwhile can learn
the unbounded-order interactions (Proof in Eq. (4)).
â€¢Model generality : RFM can serve as a general framework for
feature interaction learning, which can be instantiated into other
existing models, including both the field-aware andinstance-aware
feature interaction models (Proof in Appendix A.3). Specially, inner
product-based feature interactions (e.g., FM [23]) are special cases
of our proposed rotation-based interactions (Proof in Appendix A.2,
experiments in Section 4.3.5).
â€¢Theoretical guarantee : It can be proved that in high-dimensional
spaces, RFM can effectively learn the given feature relationships
with infinitesimal loss. In other words, in theory, RFM has the
potential to accurately capture any complex feature interaction
relationships. Further, we theoretically demonstrate that the gradi-
ent growth of RFM is at most linear with the orders, avoiding the
exponential explosion issue in prior work [ 4,7,31] (Experiments
in Section 4.3.4, more detailed proofs are provided in this link).
Difference with Existing Work. In the literature, AFN [ 7], Euler-
Net [ 31] and ARMNet [ 4] have also proposed to model the adaptive-
order interactions, but the order of each feature is independently
learned, which lacks the flexibility to capture the feature depen-
dencies in varying contexts. Although EulerNet has proposed to
enhance the representations in the complex vector space, it still
suffers from the exponential explosion issue when dealing with a
large order (See Section 4.3.4), due to the exponential growth in the
modulus of the complex features. In contrast, RFM is more flexible,
robust in accurately learning the complicated feature interactions
with arbitrarily large order involving massive feature fields. The
comparison of these approaches is presented in Table 1.
Complexity Analysis. We compare the time complexity of the
feature interaction components of different methods in Table 1. For
ease of analysis, we assume that the hidden size of different models
is set to the same number. Specifically, ğ‘šis the field number, â„is
the head number, ğ‘‘andğ‘‘â€²denote the embedding and attention
dimension,ğ¾is the hidden neuron number. For multi-head rotation,
we useğ‘‘â„=ğ‘‘â€²/â„. In each layer, calculating attention scores for
single head takesO(ğ‘šğ‘‘â€²ğ‘‘â„+ğ‘š2ğ‘‘â„)time. As we have â„heads, it
takesO(ğ‘šğ‘‘â€²2+ğ‘š2ğ‘‘â€²)time altogether. Note that ğ¾is much larger
thanğ‘šorğ‘‘[4,7,31], leading to a very high complexity of AFN [ 7]
and ARM-Net [ 4]. In comparison, ğ‘‘â€²is not very large, and it is often
set to 4ğ‘‘in practice. In general, the complexity of the RFM is of the
same order as the Transformer, and is comparable to many efficient
feature interaction models such as EulerNet [ 31] and DCNV2 [ 37]
(See in Table 3 for analysis).
4 EXPERIMENT
We conduct extensive experiments to show the effectiveness of
RFM, and analyze the effect of each learning component in it.Table 1: Comparison of different feature interaction methods. â€œFâ€,
â€œIâ€, â€œRâ€ denotes the capacity to learn the field-aware, instance-aware
and relation-aware feature interaction patterns, respectively.
Metho
ds A
daptive Unbounded Interaction Type Gradient Comple
xity
FM %
% F Exponential O
(ğ‘šğ‘‘)
AFN "
% F Exponential O
(ğ‘šğ‘‘ğ¾)
ARM-Net "
% F, I Exponential O
(ğ‘šğ‘‘â€²ğ¾)
EulerNet "
% F Exponential ğ‘‚(ğ‘š2ğ‘‘2)
RFM "
" F, I, R Linear O
(ğ‘šğ‘‘â€²2+ğ‘š2ğ‘‘â€²)
4.1 EXPERIMENTAL SETTING
We introduce the experimental settings, including the datasets,
baseline approaches, and the details of hyper-parameters.
4.1.1 Datasets. We evaluate RFM with five real-world classifica-
tion datasets on representative tasks, including app recommenda-
tion (Frappe1), movie recommendation (ML-1M2, ML-Tag3), click-
through prediction (Criteo4, Avazu5).
(1)Criteo: it is a prominent benchmark in the domain of CTR
prediction; (2) Avazu: it was utilized in the Avazu CTR prediction
challenge; (3) ML-1M: it is widely recognized as a prominent choice
in the realm of recommendation systems research; (4) ML-Tag: it
encompasses movie tagging data recorded by users across different
time spans; (5) Frappe: it serves as a practical application recom-
mendation dataset, featuring a context-aware log of app usage. To
maintain consistency with prior work, we follow the AFN [ 7] to
process the ML-Tag and Frappe dataset, where the split ratio for
the train/val/test is 7:2:1; and follow the EulerNet [ 31] to process
the Criteo, Avazu and ML-1M dataset, where the split ratio is 8:1:1.
The statistics of the datasets are shown in Table 2.
Table 2: Statistics of all datasets.
Datasets Criteo Avazu ML-1M ML-Tag Frappe
# Field 39 23 7 3 10
# Feature 1.3 M 1.5 M 13.2 K 90.4 K 5.4 K
# Instance 45 M 40 M 739 K 2 M 288 K
4.1.2 Baselines. We compare RFM with the thirteen state-of-the-
art models, including:
First-Order :
â€¢LR [24] utilizes the original field features as input for prediction,
merely combining these features using corresponding weights.
Second-Order :
â€¢FwFM [ 19] takes into account the semantic significance among
distinct feature fields and introduces a scalar weight to eliminate
insignificant feature interactions.
â€¢FmFM [ 28] enhances FwFM by substituting the single scalar
field weight with a matrix, and it computes the kernel product on
the feature embeddings to capture significant feature dependencies.
High-Order :
1https://www.baltrunas.info/research-menu/frappe
2https://grouplens.org/datasets/movielens/
3https://grouplens.org/datasets/movielens/
4https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/
5https://www.kaggle.com/c/avazu-ctr-prediction
 
2916KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
â€¢NFM [ 10] NFM aggregates the result of the element-wise multi-
plication of input feature vectors, which is then processed through
fully connected layers.
â€¢CIN [ 15] generates high-order cross features through the com-
putation of outer products of feature vectors across various orders.
â€¢CrossNet [ 37] models feature interactions explicitly through
the calculation of the kernel product of input feature vectors.
â€¢PNN [ 21] capture feature interactions by combining inner or
outer products of input feature vectors in a pairwise manner.
Ensemble :
â€¢AutoInt [ 25] utilizes Multi-head Self-Attention to autonomously
construct high-order characteristics. It stands as the pioneering en-
deavor to utilize Transformers for acquiring high-order feature
interplays.
â€¢DeepFM [ 9] integrates classical factorization machines with a
multilayer perceptron (MLP) to improve the modeling of high-order
feature interactions.
â€¢xDeepFM [15] integrates the CIN model with an MLP.
â€¢DCNV2 [37] integrates the CrossNet model with an MLP.
Adaptive-Order :
â€¢AFN [ 7] transforms features into a logarithmic space to flexibly
grasp arbitrary-order feature interactions. The AFN+ enhancement
involves the utilization of an MLP to enhance the underlying model.
â€¢ARM-Net [ 4] introduces a gated attention mechanism that
adapts to instances to dynamically learn the orders of feature inter-
actions. On the other hand, ARM-Net+ enhances the underlying
model by incorporating an MLP.
â€¢EulerNet [ 31] employs Eulerâ€™s formula to capture arbitrary-
order feature interactions in the complex vector space, thus over-
coming the non-negativity constraints present in the AFN.
These models compared in our experiments encompass various
forms of feature interaction techniques. Among them, LR [ 24] is the
most straightforward approach, which utilizes feature weights for
direct prediction. Besides, FmFM [ 28] and FwFM [ 19] are relatively
simple models that only capture second-order feature interactions.
NFM [ 10], CIN [ 15], CrossNet [ 37], and PNN [ 22] have the ca-
pacity to model higher-order feature interactions. AutoInt+ [ 25],
DeepFM [ 9], xDeepFM [ 15], and DCNV2 [ 37] are ensemble methods
that incorporate an MLP to enhance high-order feature interactions.
AFN+ [ 7], ARM-Net+ [ 4], and EulerNet [ 31] have the capacity to
learn adaptive-order interactions.
4.1.3 Evaluation Metrics. We use two popular metrics to evaluate
the model performance: AUC and LogLoss.
â€¢AUC [ 16] stands for the Area Under the Receiver Operating
Characteristic Curve, which measures the likelihood that a CTR
predictor will correctly classify a randomly selected positive item
as having a higher score than a randomly selected negative item.
â€¢LogLoss [ 3] is a measure of the deviation between the predicted
score and the ground-truth label for each sample, which is also
known as binary cross entropy loss.
4.1.4 Implementation Details. For each method, the grid search
is applied to find the optimal settings. We reuse baseline models
and implement RFM based on RecBole [ 44], an open-source library.
Extensive grid search is applied to find the optimal settings. We
follow the same experimental settings as EulerNet [ 31], by setting
the size feature embedding to 16, and batch size to 1024. We setthe learning rate from 1e-1 to 1e-4 on a log scale and then narrow
down to 5e-4 on a linear scale. The regularization parameter ğœ†is in
{1e-3, 1e-5, 1e-7}. The optimizer is Adam [ 12]. For RFM, the rotation
layer number is in {1, 2, 3}, the number of attention heads is in {1, 2,
4, 8}, and attention dimension is in {16, 32, 48, 64, 80}. The setting
of amplification network is in {48,128,256Ã—256}. The dimension
ofGroupNorm (i.e.,ğ‘‘ğ‘˜/ğ‘™) is in {2, 4, 8, 16}. More tuning details and
our source code is available at code repository.
4.2 Overall Performance
To show the effectiveness of RFM in learning feature interactions,
we conduct experiments on five public datasets. The overall perfor-
mance is shown in Table 3. We have the following observations:
(1) The low-order models (i.e., LR [24], FwFM [ 19] and FmFM [ 28])
perform worse than the high-order models (i.e., NFM [ 10], CIN [ 15],
CrossNet [37] and PNN [21]), due to limited learning capacity.
(2) The ensemble models (i.e., AutoInt+ [ 25], xDeepFM [ 15],
DCNV2 [ 37] and DeepFM [ 9]) achieve competitive performance
across all datasets, showing the effectiveness of integrating MLPs
for learning enhanced feature interactions.
(3) As for the adaptive-order models, ARM-Net+ [ 4] outperforms
AFN+ [ 7] on the ML-1M, ML-Tag and Frappe datasets, demonstrat-
ing the effectiveness of instance-aware feature interaction learning.
Additionally, EulerNet [ 31] performs very well across all datasets,
indicating that the complex vector space is more suitable for learn-
ing adaptive-order interactions.
(4) RFM consistently outperforms all compared baselines, show-
ing the effectiveness of our proposed self-attentive rotation function
for learning relation-aware interactions.
For efficiency, we observe that the latency of first-order and
second-order models is relatively small due to their simple ar-
chitectures. The high-order and ensemble models are more time-
consuming because they have more complicated architectures. Com-
pared to EulerNet [ 31], AFN+ [ 7] and ARM-Net+ [ 4] have to in-
corporate many more parameters to compensate for the limited
representation capacity. Note that RFM is sufficiently efficient and
is comparable to many efficient approaches (e.g., DCNV2 [37]).
4.3 Further Study
4.3.1 Ablation Study. We analyze how each component influences
the performance of RFM in Table 4. We propose four variants as
follows: (1) w/oAttRo : removing the self-attentive rotation layer,
(2)w/oAmpNet : removing the modulus amplification network, (3)
w/oRes: removing the residual in the self-attentive rotation layer, (4)
w/oCoo Trans : removing the coordinate transformation procedure
(See Section 3.2) that directly feeds the angular representations to an
MLP. We can see that all these variants underperform the complete
RFM, showing that all of our proposed approaches are useful to
improve the performance. Specially, the model performance of
variant (1) shows a significant decrease, indicating that the self-
attentive rotation layer is the core of RFM for learning effective
feature interactions.
Besides, we investigate the effects of our proposed self-attentive
rotation function in Table 4. In variant (5), we remove the weight
vector (i.e., ğ’˜in Eq. (5)) of rotation-based attention algorithm. Vari-
ant (6) replaces the rotation-based attention with the widely used
 
2917Rotative Factorization Machines KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Performance comparisons. Note that a higher AUC or a lower Logloss at the 0.001 level is regarded as significant, as stated in [ 6,9,25,31].
â€œ*â€ denotes that statistical significance for ğ‘<0.01compared to the best baseline. â€œLLâ€ denotes the LogLoss.
T
ype ModelCrite
o A
vazu ML-1M ML-
Tag Frapp
e Effciency
AUC
LL AUC
LL AUC
LL AUC
LL AUC
LL Params
Latency
First-Or
der LR 0.7900
0.4598 0.7663
0.3879 0.8712
0.3506 0.9303
0.3455 0.9379
0.2858 5.39
K 0.76 ms
Se
cond-OrderFwFM 0.8104
0.4414 0.7817
0.3813 0.8934
0.3201 0.9415
0.2761 0.9764
0.1791 91.66
K 1.02 ms
FmFM 0.8112
0.4408 0.7794
0.3819 0.8942
0.3191 0.9595
0.2255 0.9783
0.1675 93.21
K 1.21 ms
High-Or
derNFM 0.8066
0.4456 0.7832
0.3784 0.8931
0.3245 0.9578
0.2353 0.9779
0.1722 216.14
K 2.14 ms
CIN 0.8109
0.4424 0.7852
0.3771 0.8913
0.3255 0.9624
0.2125 0.9816
0.1669 362.27
K 3.79 ms
CrossNet 0.8123
0.4398 0.7874
0.3767 0.8983
0.3156 0.9647
0.2159 0.9817
0.1611 272.31
K 1.78 ms
PNN 0.8120
0.4399 0.7841
0.3773 0.8953
0.3233 0.9635
0.2197 0.9813
0.1567 113.23
K 1.58 ms
EnsembleA
utoInt+ 0.8126
0.4396 0.7841
0.3778 0.8981
0.3195 0.9642
0.2207 0.9810
0.1647 256.13
K 3.27 ms
DeepFM 0.8123
0.4399 0.7856
0.3768 0.8973
0.3166 0.9618
0.2264 0.9812
0.1689 252.17
K 1.61 ms
xDeepFM 0.8124
0.4406 0.7874
0.3761 0.8969
0.3187 0.9625
0.2121 0.9819
0.1580 375.22
K 5.70 ms
DCNV2 0.8129
0.4392 0.7876
0.3757 0.8989
0.3147 0.9649
0.2084 0.9822
0.1531 302.99
K 2.03 ms
A
daptive-OrderAFN+ 0.8125
0.4395 0.7877
0.3756 0.8931
0.3230 0.9607
0.2285 0.9813
0.1697 1976.36
K 3.39 ms
ARM-Net+ 0.8125
0.4396 0.7877
0.3757 0.8969
0.3141 0.9650
0.2096 0.9818
0.1517 1648.16
K 5.62 ms
EulerNet 0.8139 0.4387 0.7879 0.3755 0.9010 0.3098 0.9656 0.2134 0.9832 0.1581 170.76
K 1.88 ms
RFM 0.8147âˆ—0.4374âˆ—0.7890âˆ—0.3749âˆ—0.9026âˆ—0.3090âˆ—0.9667âˆ—0.2049âˆ—0.9843âˆ—0.1506 348.17
K 2.27 ms
Table 4: Ablation study of the proposed RFM.
V
ariantCrite
o ML-
Tag Frapp
e
AUC
LogLoss AUC
LogLoss AUC
LogLoss
(0):
RFM 0.8147
0.4374 0.9667
0.2049 0.9843
0.1506
(1):w/oAttRo 0.8138
0.4381 0.9552
0.2454 0.9763
0.1768
(2):w/oAmpNet 0.8142
0.4381 0.9629
0.2178 0.9804
0.1491
(3):w/oRes 0.8141
0.4383 0.9635
0.2164 0.9806
0.1620
(4):w/oCoo Trans 0.8139
0.4384 0.9637
0.2163 0.9816
0.1611
(5):w/oAttWeight 0.8139
0.4382 0.9656
0.2073 0.9816
0.1533
(6): (1) + wDotAtt 0.8134
0.4381 0.9607
0.2330 0.9813
0.1561
(7):w/oGN 0.8131
0.4393 0.9626
0.2188 0.9789
0.1678
(8): (7) + wLN 0.8141
0.4383 0.9646
0.2137 0.9820
0.1491
(9): (7) + wBN 0.8140
0.4385 0.9653
0.2089 0.9833
0.1475
scaled dot-product attention [ 34]. The performance of both vari-
ants shows a notable decrease. This indicates that our proposed
rotation-based attention mechanism is more effective for the rela-
tion modeling of the angular representations in the complex plane.
In variants (7), (8), and (9), we explore the effects of different nor-
malization methods. The results show that the GroupNorm is more
suitable for learning the feature-wise representations.
4.3.2 Visualizing the Interaction Orders. RFM is capable of adap-
tively learning the interaction orders from different interaction
contexts. Figure 3 visualizes the learned orders of different meth-
ods. We can observe that the orders in field-aware method (i.e.,
EulerNet [ 31]) are the same for all features within each field (i.e.,
the columns in the figure). The instance-aware method (i.e., ARM-
Net [ 4]) can identity the importance of some features (e.g., 2-nd
column), but cannot capture the dependencies between different
fields. In contrast, RFM can learn the varied feature interactions
from different contexts. The diversified orders learned from the
varying context enable it to capture more effective relationships.
4.3.3 Visualizing the Representations. To have an intuitive under-
standing of our approach, we visualize the representations with a
Feature orderInstanceField-Aware (EulerNet)
Feature orderInstanceInstance-Aware (ARM-Net)
Feature orderInstanceRelation-Aware (RFM)
0.5 1.0 âˆ’0.05 0.00 0.25 0.50 0.75Figure 3: Visualizing the interaction orders.
simple case (the embedding dimension ğ‘‘=1) on the MovieLens-1M
dataset. As shown in Figure 4, the left figure visualizes the query
angles (i.e., ğœ½ğ‘„
ğ‘—in Eq. (5)) of the gender features and the key an-
gles (i.e., ğœ½ğ¾
ğ‘—in Eq. (5)) of others, and the right figure illustrates
the conditional mutual information scores on the gender features,
representing the strength of each feature field on the ground-truth
labels given the gender features. We can observe that the fields
(user_id, zip_code anditem_id ) have a strong effect on the results,
and they are closely aligned with the gender features. For the fields
with less importance (age, occupation andrelease_year ), they have
no intersecting features with gender and the corresponding rotation
angles are relatively large. These results indicate that the rotation
angles from keys to queries can reflect the importance of feature
relationships, which enables RFM to capture the effective feature
dependencies for learning varied feature interactions.
4.3.4 Arbitrary-Order Learning Analysis. We investigate the arbitrary-
order learning capacity of different approaches. Figure 5(a) shows
the trajectory of the average feature order (i.e., ğ›¼ğ‘—in Eq. (1)) during
training. We can see that RFM converges to a relatively large order,
while other models tend to approach a zero order. This demonstrates
RFMâ€™s ability to learn more effective interactions.
Then we probe the learning effectiveness with respect to the
number of feature fields (i.e., ğ‘šin Eq. (1)). As shown in Figure 5(b),
 
2918KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
Conditional Mutual Information
Gender
Figure 4: Visualizing the representations.
the average orders learned in EulerNet, AFN and ARM-Net de-
crease when adding the feature fields, since their interaction terms
exponentially grow with the increasing number of feature fields
(i.e.,ğ‘šin Eq. (1)). Figure 5(c) shows the gradients with respect to
the interaction order (i.e.,Ãğ‘š
ğ‘—=1ğ›¼ğ‘—in Eq. (1)). We observe that the
gradient in EulerNet, AFN and ARM-Net exponentially grows with
the increasing order, leading to the gradient explosion issue when
the order reaches a large value. In contrast, the gradient in RFM
remains relatively stable, and it is more robust to a large number of
feature fields or large interaction orders. These results are consis-
tent with our theoretical analysis, demonstrating the superiority of
RFM for learning arbitrary-order feature interactions.
0 5000 10000 15000 20000
training step0.00.20.40.60.81.0 Avg. OrderRFM
EulerNet
ARM-Net
AFN
(a) The learning trajec-
tory of Avg. feature order
2 4 6 8 10
Num_field0.10.20.30.40.50.6 Avg. OrderRFM
EulerNet
ARM-Net
AFN(b) The Avg. feature order
w.r.t feature field number
0 50 100 150 200 250 300
Order02K4K6K8K10K GradientRFM
EulerNet
ARM-Net
AFN(c) The gradient w.r.t the
interaction order
Figure 5: Interaction order learning analysis on the Frappe dataset.
4.3.5 Degradation to High-Order Factorization Machines. As dis-
cussed in Section 3.3, RFM can be degenerated to the traditional
inner-product-based methods ( e.g.,FM [ 23]). To study the effec-
tiveness of RFM in learning high-order feature interactions, we
create synthetic datasets with increasing difficulty as ğ‘“ğ‘š(ğ‘¬)=
ğ’†1âŠ™ğ’†2âŠ™Â·Â·Â·âŠ™ ğ’†ğ‘š. We compare the prediction result learned in
RFM with a complex MLP and self-attention mechanism, and utilize
fitting deviation to evaluate the difference between the prediction
results of the models and the ground-truth high-order feature in-
teractions (i.e., ğ‘“ğ‘š). As shown in Figure 6, we can observe that
the fitting deviation continuously decreases as the dimension ğ‘‘
increases. Whereas the task difficulty increases (the order ğ‘šin-
creases), the fitting deviation increases. These results are consistent
with the theoretical analysis in Appeddix A.2. On the other hand,
the deviation of RFM is very small, showing the approximately loss-
less fitting capability of RFM in learning high-order interactions.
4.3.6 Effect of Modulus Amplification Network. To study the ef-
fectiveness of the proposed modulus amplification network (See
Section 3.2), we visualize the representations before and after mod-
ulus amplification in the complex plane. The results on the Frappe,
0 50 100 150 200 250
Dimension10âˆ’1100DeviationRFM
Complex MLP
Self-attention(a)ğ‘š=5
0 50 100 150 200 250
Dimension100101
DeviationRFM
Complex MLP
Self-attention (b)ğ‘š=10
0 50 100 150 200 250
Dimension100101
DeviationRFM
Complex MLP
Self-attention (c)ğ‘š=15
Figure 6: The fitting deviation curves of different learning models.
ML-Tag, Criteo and Avazu datasets are shown in Figure 7. We can
observe that, before the modulus amplification procedure, the fea-
ture representations are distributed on a unit circle with a fixed
modulus of 1. Specifically, the angular representations learned in
RFM vary from[âˆ’ğœ‹,ğœ‹]on the ML-Tag, Criteo and Avazu datasets.
Whereas on the Frappe datasets, due to its smaller scale, the range
is narrowed to[âˆ’ğœ‹/10,ğœ‹/10]. After amplification, the features are
distributed at various areas in the complex plane, and they have
different modulus. Specially, we can also see that most transformed
representations have the same real part or imaginary part. Such
distributions make the varies of angle have a remarkable influence
on the predicted result, which enables RFM to capture the useful
feature relationships and improves the modelâ€™s capabilities.
Frappe_before
 Frappe_after
 ML_before
 ML_after
Criteo_before
 Criteo_after
 Avazu_before
 Avazu_after
Figure 7: Visualization of the feature representations before and
after the modulus amplification.
4.3.7 Hyper-Parameter Study. We study how the hyper-parameters
impact the performance of RFM, including:
â€¢Attention Dimensions ğ‘‘â€².We investigate the performance with
respect to the attention dimension ğ‘‘â€²in the self-attentive rotation
layer. As shown in Figure 8, on the Criteo and Avazu datasets, we
can see that the performance increases as the attention dimension
increases from 16 to 32. Whereas, on the Frappe dataset, RFM
achieves the best performance as the attention dimension increases
to 48. Continuously increasing the attention dimension does not
yield a sustained improvement in model performance. The reason is
that the model overfits when too many parameters are incorporated.
â€¢Attention Heads â„.As mentioned in Section 3.1.2, the attention
heads number â„controls the number of feature interaction terms.
As shown in Figure 8, we can see that the performance increases
as the attention head number increases from 2 to 4 on the Criteo
and Avazu datasets, showing the effectiveness of incorporating
more feature interactions. The results are different on the Frappe
dataset; the model performance varies significantly across different
attention head numbers. The reason is that this data set is small,
 
2919Rotative Factorization Machines KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
introducing too many interaction terms may introduce irrelevant
noise that hurts the model performance.
â€¢Layer Number ğ‘.RFM is designed by stacking ğ‘self-attentive
rotation layers. To analyze the influence of ğ‘, we varyğ‘in the
range of 1 to 5 to report the results in Figure 8. We can observe
that the performance of RFM increases with the attention layer
number at the beginning. However, model performance degrades
when the attention layer number is set greater than 2 on the Criteo
and Avazu dataset, whereas RFM achieves the best performance
with a single layer. In practice, the layer number of RFM is usually
set to 1 or 2, thereby ensuring the efficiency of our approach.
16 32 48 64 80
Attention Dimension787078757880788578907895 AUC
AUC
LogLoss
374537473750375237553757376037623765
LogLoss
The attention dimension
2 4 6 8 10
Attention Head78807882788478867888789078927894 AUC
AUC
LogLoss
37463748375037523754375637583760
LogLoss
The attention head
1 2 3 4 5
The Number of Layers786578707875788078857890 AUC
AUC
LogLoss
374537503755376037653770
LogLoss
The Number of Layers
16 32 48 64 80
Attention Dimension981098159820982598309835984098459850 AUC
AUC
LogLoss
14601470148014901500151015201530
LogLoss
The attention dimension
2 4 6 8 10
Attention Head981098159820982598309835984098459850 AUC
AUC
LogLoss
146014801500152015401560158016001620
LogLoss
The attention head
1 2 3 4 5
The Number of Layers98009810982098309840 AUC
AUC
LogLoss
1500155016001650170017501800
LogLoss
The Number of Layers
Figure 8: Parameter analysis (Scale: .0001) on the Avazu (the first
row) and Frappe (the second row) datasets.
5 RELATED WORK
5.1 Feature Interaction Learning.
Feature interaction learning (FIL) is a fundamental problem in var-
ious machine learning tasks [ 9,18,37,38,45,46], leading to the
emergence of several interaction models [ 5,11,14,17,23,41]. The
early work mainly conducts enumeration to learn the feature in-
teractions. The basic idea of these approaches is to set a maximal
order and then enumerate all the feature interaction terms within
the designed order. For example, FM [ 23] is the most basic model,
using feature embedding vectors to capture pairwise feature inter-
actions; FwFM [ 19] and FmFM [ 28] propose to use the field-specific
weights to improve the expressive capacity of FMs. Though effec-
tive to some extent, they can only model the second-order feature
interactions that has a limited model capabilities. To effectively
capture the higher-order feature interactions, HOFM [ 2] introduces
a dynamic programming algorithm for higher-order interactions;
xDeepFM [ 15] and DCNV2 [ 37] propose intricate architectures to
iteratively enumerate the high-order interactions. They have signif-
icantly improved performance across various applications. Despite
the progress, their reliance on empirically designed orders may
hinder accurate learning in real-world contexts.
Recently, a number of studies [ 4,7,31] proposes to automatically
learn the orders from data. Among them, AFN [ 7] and ARM-Net [ 4]
uses logarithmic transformation to cast the interaction orders into
learnable linear weights, which provides a way to adaptively learn
the interaction orders. Besides, EulerNet [ 31] proposes to use Eu-
lerâ€™s formula to learn the feature interaction orders in a complex
vector space. However, these methods cannot capture the feature
dependencies in varying contexts, which diminishes the modelâ€™s
capacity. Further, they suffer from the exponential explosion issue,making them unsuitable for scenarios with numerous features or
high orders. Different from them, we use the attentive rotations to
model complicated interactions, which can adaptively capture the
feature dependencies and surpass the scale limits of the interaction
order in existing work.
5.2 Representation Learning in the Complex
Vector Space.
In the literature, a number of approaches [ 13,20,26,27,29â€“31] have
been proposed to learn the data relations in the complex vector
space for enhancing the representations. Typically, the complex
vector space possesses the inherent advantages in modeling the
vector rotations and exponential modeling, and it has drawn great
attention in a wide range of research fields [20, 26, 27, 29â€“31].
In the area of computer vision (CV), WaveMLP [ 30] represents
each image patch as a wave to capture the dynamic vision seman-
tics. In the area of natural language processing (NLP), RotatE [ 29]
defines each relation of a knowledge graph as a rotation from the
source entity to the target entity. RoPE [ 20,26,27] and XPOS [ 27]
use a two-dimensional pairwise rotation method to improve the po-
sitional embedding of Transformers. These approaches have widely
used in the latest large language models (LLMs), such as LLaMa [ 33]
and PaLM [ 8]. Furthermore, AnglE [ 13] proposes a method using
complex polar angles to learn the similarity relationship of differ-
ent vectors. In the area of recommender systems, EulerNet [ 31]
proposes utilizing Eulerâ€™s formula to adaptively learn the arbitrary-
order feature interactions. Besides, EulerFormer [ 32] proposes a
new theoretical framework to formulate the Transformer architec-
ture which provides a new perspective for modeling the positional
embedding and attention mechanism in the complex vector space.
These approaches have a great potentiality to enhance model capa-
bilities in a variety of machine learning tasks.
6 CONCLUSION
In this paper, we proposed the Rotative Factorization Machines
(RFM) for effectively modeling feature interactions in various pre-
diction tasks. RFM represents each feature as a polar angle in the
complex plane and converts the interactions into the complex rota-
tions, avoiding the exponential explosion of the interaction terms. In
RFM, the rotation coefficients are modeled through a rotation-based
attention mechanism, which can adaptively learn the interaction
orders from different interaction contexts. Moreover, we propose a
modulus amplification network to learn the modulus of the com-
plex features for further enhancing the expressive capacity. As the
core component, we proposed a novel self-attentive rotation func-
tion to model complicated feature interactions, providing a way to
adaptively learn the unbounded interaction orders adaptively from
varying contexts. As future work, we consider generalizing RFM to
handle other data types, e.g.,sequences or graphs, and will extend
the current approach to deal with multiple domains or tasks.
ACKNOWLEDGMENTS
This work was partially supported by National Natural Science
Foundation of China under Grant No. 62222215, Beijing Natural
Science Foundation under Grant No. L233008 and 4222027. Xin
Zhao is the corresponding author.
 
2920KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
REFERENCES
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-
tion. arXiv preprint arXiv:1607.06450 (2016).
[2]Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016.
Higher-order factorization machines. Advances in Neural Information Processing
Systems 29 (2016).
[3]Andreas Buja, Werner Stuetzle, and Yi Shen. 2005. Loss functions for binary class
probability estimation and classification: Structure and applications. Working
draft, November 3 (2005), 13.
[4]Shaofeng Cai, Kaiping Zheng, Gang Chen, HV Jagadish, Beng Chin Ooi, and
Meihui Zhang. 2021. Arm-net: Adaptive relation modeling network for structured
data. In Proceedings of the 2021 International Conference on Management of Data.
207â€“220.
[5]Wenqiang Chen, Lizhang Zhan, Yuanlong Ci, Minghua Yang, Chen Lin, and
Dugang Liu. 2019. FLEN: leveraging field for scalable CTR prediction. arXiv
preprint arXiv:1911.04690 (2019).
[6]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7â€“10.
[7]Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization
network: Learning adaptive-order feature interactions. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 34. 3609â€“3616.
[8]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-
bastian Gehrmann, et al .2023. Palm: Scaling language modeling with pathways.
Journal of Machine Learning Research 24, 240 (2023), 1â€“113.
[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[10] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse
predictive analytics. In Proceedings of the 40th International ACM SIGIR conference
on Research and Development in Information Retrieval. 355â€“364.
[11] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-
ture importance and bilinear feature interaction for click-through rate prediction.
InProceedings of the 13th ACM Conference on Recommender Systems. 169â€“177.
[12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[13] Xianming Li and Jing Li. 2023. Angle-optimized text embeddings. arXiv preprint
arXiv:2309.12871 (2023).
[14] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn:
Modeling feature interactions via graph neural networks for ctr prediction. In Pro-
ceedings of the 28th ACM International Conference on Information and Knowledge
Management. 539â€“548.
[15] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and
Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-
teractions for recommender systems. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining. 1754â€“1763.
[16] Jorge M Lobo, Alberto JimÃ©nez-Valverde, and Raimundo Real. 2008. AUC: a
misleading measure of the performance of predictive distribution models. Global
ecology and Biogeography 17, 2 (2008), 145â€“151.
[17] Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan.
2021. A dual input-aware factorization machine for CTR prediction. In Proceedings
of the Twenty-Ninth International Conference on International Joint Conferences
on Artificial Intelligence. 3139â€“3145.
[18] Yuanfei Luo, Hao Zhou, Wei-Wei Tu, Yuqiang Chen, Wenyuan Dai, and Qiang
Yang. 2020. Network on network for tabular data classification in real-world
applications. In Proceedings of the 43rd International ACM SIGIR Conference on
Research and Development in Information Retrieval. 2317â€“2326.
[19] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun,
and Quan Lu. 2018. Field-weighted factorization machines for click-through
rate prediction in display advertising. In Proceedings of the 2018 World Wide Web
Conference. 1349â€“1357.
[20] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn:
Efficient context window extension of large language models. arXiv preprint
arXiv:2309.00071 (2023).
[21] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.
2016. Product-based neural networks for user response prediction. In 2016 IEEE
16th International Conference on Data Mining (ICDM). IEEE, 1149â€“1154.
[22] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo,
Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for user response
prediction over multi-field categorical data. ACM Transactions on Information
Systems (TOIS) 37, 1 (2018), 1â€“35.
[23] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference
on data mining. IEEE, 995â€“1000.
[24] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting
clicks: estimating the click-through rate for new ads. In Proceedings of the 16thinternational conference on World Wide Web. 521â€“530.
[25] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-
attentive neural networks. In Proceedings of the 28th ACM International Conference
on Information and Knowledge Management. 1161â€“1170.
[26] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.
2021. Roformer: Enhanced transformer with rotary position embedding. arXiv
preprint arXiv:2104.09864 (2021).
[27] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable
transformer. arXiv preprint arXiv:2212.10554 (2022).
[28] Yang Sun, Junwei Pan, Alex Zhang, and Aaron Flores. 2021. Fm2: Field-matrixed
factorization machines for recommender systems. In Proceedings of the Web
Conference 2021. 2828â€“2837.
[29] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowl-
edge graph embedding by relational rotation in complex space. arXiv preprint
arXiv:1902.10197 (2019).
[30] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe
Wang. 2022. An image patch is a wave: Phase-aware vision mlp. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10935â€“
10944.
[31] Zhen Tian, Ting Bai, Wayne Xin Zhao, Ji-Rong Wen, and Zhao Cao. 2023. Euler-
Net: Adaptive Feature Interaction Learning via Eulerâ€™s Formula for CTR Predic-
tion. In Proceedings of the 46th International ACM SIGIR Conference on Research
and Development in Information Retrieval. 1376â€“1385.
[32] Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, and Ji-
Rong Wen. 2024. EulerFormer: Sequential User Behavior Modeling with Complex
Vector Attention. In SIGIR.
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[35] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang,
and Ning Gu. 2022. Enhancing CTR prediction with context-aware feature repre-
sentation learning. In Proceedings of the 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 343â€“352.
[36] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and
Ning Gu. 2023. CL4CTR: A Contrastive Learning Framework for CTR Prediction.
InProceedings of the Sixteenth ACM International Conference on Web Search and
Data Mining. 805â€“813.
[37] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. DCN V2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In Proceedings of the Web Conference 2021.
1785â€“1797.
[38] Zhiqiang Wang, Qingyun She, and Junlin Zhang. 2021. MaskNet: Introducing
feature-wise multiplication to CTR ranking models by instance-guided mask.
arXiv preprint arXiv:2102.07619 (2021).
[39] Yuxin Wu and Kaiming He. 2018. Group normalization. In Proceedings of the
European conference on computer vision (ECCV). 3â€“19.
[40] Bo Xiao and Izak Benbasat. 2007. E-commerce product recommendation agents:
Use, characteristics, and impact. MIS quarterly (2007), 137â€“209.
[41] Feng Yu, Zhaocheng Liu, Qiang Liu, Haoli Zhang, Shu Wu, and Liang Wang.
2020. Deep interaction machine: A simple but effective model for high-order
feature interactions. In Proceedings of the 29th ACM International Conference on
Information & Knowledge Management. 2285â€“2288.
[42] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021.
Deep Learning for Click-Through Rate Estimation. In Proceedings of the Thirtieth
International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event /
Montreal, Canada, 19-27 August 2021, Zhi-Hua Zhou (Ed.). ijcai.org, 4695â€“4703.
https://doi.org/10.24963/IJCAI.2021/636
[43] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep
learning for click-through rate estimation. arXiv preprint arXiv:2104.10584 (2021).
[44] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu
Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al .2021. Recbole:
Towards a unified, comprehensive and efficient framework for recommendation
algorithms. In Proceedings of the 30th ACM International Conference on Information
& Knowledge Management. 4653â€“4664.
[45] Zihao Zhao, Zhiwei Fang, Yong Li, Changping Peng, Yongjun Bao, and Weipeng
Yan. 2020. Dimension relation modeling for click-through rate prediction. In
Proceedings of the 29th ACM International Conference on Information & Knowledge
Management. 2333â€“2336.
[46] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.
5941â€“5948.
 
2921Rotative Factorization Machines KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A THEORETICAL ANALYSIS
A.1 Tensor-form attention calculation
Note that the element in ğ‘—-th row and ğ‘™-th column can be given as:
ğ´ğ‘—,ğ‘™=Sigmoid 
Reh
exp(ğ‘–ğ‘¸)diag(ğ’˜)
exp(âˆ’ğ‘–ğ‘²)âŠ¤i!
ğ‘—,ğ‘™
=Sigmoid 
Reh
ğ’˜âŠ¤
exp(ğ‘–ğ‘¸ğ‘—)âŠ™exp(âˆ’ğ‘–ğ‘²ğ‘™)âŠ¤i!
=Sigmoid 
Reh
ğ’˜âŠ¤cos(ğ‘¸âŠ¤
ğ‘—âˆ’ğ‘²âŠ¤
ğ‘™)+ğ‘–ğ’˜âŠ¤sin(ğ‘¸âŠ¤
ğ‘—âˆ’ğ‘²âŠ¤
ğ‘™)i!
=Sigmoid
ğ’˜âŠ¤cos(ğœ½ğ‘„
ğ‘—âˆ’ğœ½ğ¾
ğ‘™)
=ğ›¼ğ‘—,ğ‘™.
A.2 Degenerate to Factorization Machines
Lemma A.1. Following the basic setting of machine learning, we
consider that the input data ğ’™are independently and identically dis-
tributed ( i.i.d) and{ğœ½}ğ‘š
ğ‘—=1are measurable embeddings of them. If
embeddings{ğœ½ğ‘—}ğ‘š
ğ‘—=1âˆˆRğ‘šÃ—ğ‘‘are regularized such that ||ğœ½ğ‘—||2â‰¤1,
then for any given order vector ğœ¶âˆˆ {0,1}ğ‘šand any input fea-
tures{ğœ½ğ‘—}ğ‘š
ğ‘—=1âˆˆRğ‘šÃ—ğ‘‘, the rotation-based interaction pattern ğš«ğº=
H(ğ‘’ğ‘–ğ›¼1ğœ½1âŠ™ğ‘’ğ‘–ğ›¼2ğœ½2âŠ™Â·Â·Â·âŠ™ğ‘’ğ‘–ğ›¼ğ‘šğœ½ğ‘š)can be degenerated to the inner-
product based interaction ğš«ğ¹=ğ’†ğ›¼1
1âŠ™ğ’†ğ›¼2
2âŠ™Â·Â·Â· ğ’†ğ›¼ğ‘šğ‘š. The devia-
tion of E(Â¯ğ‘¹)=E(|Â¯ğš«ğºâˆ’Â¯ğš«ğ¹|)can be bounded by O(ğ‘ /âˆš
ğ‘‘), where
H(ğ’›)=Re(ğ’›)+Im(ğ’›), andğ‘ =Ãğ‘š
ğ‘—=1ğ›¼ğ‘—is the interaction order.
Proof. Note that|H(ğ’›)|â‰¤|| Re(ğ’›)|+| Im(ğ’›)||andcos(ğ›¼ğ‘—ğœ½ğ‘—)=
cosğ›¼ğ‘—(ğœ½ğ‘—)ifğ›¼ğ‘—âˆˆ{0,1}. Let ğ’†ğ‘—:=cos(ğœ½ğ‘—)âˆˆRğ‘‘, and we have:
|ğš«ğºâˆ’ğš«ğ¹|=|H(ğ‘’ğ‘–ğ›¼1ğœ½1âŠ™ğ‘’ğ‘–ğ›¼2ğœ½2âŠ™Â·Â·Â·âŠ™ğ‘’ğ‘–ğ›¼ğ‘šğœ½ğ‘š)âˆ’ğ’†ğ›¼1
1âŠ™Â·Â·Â· ğ’†ğ›¼ğ‘šğ‘š|
=H ğ‘šÃ–
ğ‘—=1
cos(ğ›¼ğ‘—ğœ½ğ‘—)+ğ‘–sin(ğ›¼ğ‘—ğœ½ğ‘—)!
âˆ’ğ‘šÃ–
ğ‘—=1cosğ›¼ğ‘—(ğœ½ğ‘—)
=H ğ‘šÃ–
ğ‘—=1
cos(ğ›¼ğ‘—ğœ½ğ‘—)+ğ‘–sin(ğ›¼ğ‘—ğœ½ğ‘—)!
âˆ’ğ‘šÃ–
ğ‘—=1cos(ğ›¼ğ‘—ğœ½ğ‘—)
=H ğ‘šâˆ‘ï¸
ğ‘™=1âˆ‘ï¸
ğ‘âˆˆğ¶ğ‘™ğ‘šğ‘šÃ–
ğ‘¡=1
ğ‘–ğ‘ğ‘¡cos1âˆ’ğ‘ğ‘¡(ğ›¼ğ‘¡ğœ½ğ‘¡)sinğ‘ğ‘¡(ğ›¼ğ‘¡ğœ½ğ‘¡)!
+ğ‘šÃ–
ğ‘—=1cos(ğ›¼ğ‘—ğœ½ğ‘—)âˆ’ğ‘šÃ–
ğ‘—=1cos(ğ›¼ğ‘—ğœ½ğ‘—)
=H ğ‘šâˆ‘ï¸
ğ‘™=1âˆ‘ï¸
ğ‘âˆˆğ¶ğ‘™ğ‘šğ‘šÃ–
ğ‘¡=1
ğ‘–ğ‘ğ‘¡cos1âˆ’ğ‘ğ‘¡(ğ›¼ğ‘¡ğœ½ğ‘¡)sinğ‘ğ‘¡(ğ›¼ğ‘¡ğœ½ğ‘¡)!
+1âˆ’1
â‰¤ ğ‘šâˆ‘ï¸
ğ‘™=1âˆ‘ï¸
ğ‘âˆˆğ¶ğ‘™ğ‘šğ‘šÃ–
ğ‘¡=1
|cos1âˆ’ğ‘ğ‘¡(ğ›¼ğ‘¡ğœ½ğ‘¡)|âŠ™| sinğ‘ğ‘¡(ğ›¼ğ‘¡ğœ½ğ‘¡)|!
+1âˆ’1
â‰¤ ğ‘šâˆ‘ï¸
ğ‘™=1âˆ‘ï¸
ğ‘âˆˆğ¶ğ‘™ğ‘šğ‘šÃ–
ğ‘¡=1
1âŠ™|sinğ‘ğ‘¡(ğ›¼ğ‘¡ğœ½ğ‘¡)|!
+1âˆ’1
=ğ‘šÃ–
ğ‘—=1
1+|sin(ğ›¼ğ‘—ğœ½ğ‘—)|
âˆ’1Hereğ¶ğ‘™ğ‘šis the set of indices denoting the combinations that select
ğ‘™elements from a set of size ğ‘š,e.g.,ğ¶2
3={[0,1,1],[1,0,1],[1,1,0]}.
We assume each dimension of the embedding vector is indepen-
dent, then the expectation of the embedding deviation is equal
to the one-element deviation. Futher, we have E(Ãğ‘‘
ğ‘˜=1|ğœƒğ‘˜|2)=Ãğ‘‘
ğ‘˜=1E(|ğœƒğ‘˜|2)=ğ‘‘Â·E(|ğœƒğ‘˜|2)â‰¤1. Letğ‘ =Ãğ‘š
ğ‘—=1ğ›¼ğ‘—is the interaction
order, then we have:
E(Â¯ğ‘…)â‰¤E ğ‘šÃ–
ğ‘—=1
1+|sin(ğ›¼ğ‘—ğœƒğ‘—)|
âˆ’1!
â‰¤E Ã–
ğ›¼ğ‘—â‰ 0
1+|sin(ğ›¼ğ‘—ğœƒğ‘—)|
âˆ’1!
â‰¤E Ã–
ğ›¼ğ‘—â‰ 0
1+|ğœƒğ‘—|
âˆ’1!
â‰¤E 
1+Ãğ‘š
ğ‘—=1ğ›¼ğ‘—|ğœƒğ‘—|
Ãğ‘š
ğ‘—=1ğ›¼ğ‘—Ãğ‘š
ğ‘—=1ğ›¼ğ‘—âˆ’1!
â‰¤E ğ‘šâˆ‘ï¸
ğ‘—=1ğ›¼ğ‘—|ğœƒğ‘—|!
=ğ‘šâˆ‘ï¸
ğ‘—=1ğ›¼ğ‘—E(|ğœƒğ‘—|)â‰¤ğ‘šâˆ‘ï¸
ğ‘—=1ğ›¼ğ‘—âˆšï¸ƒ
E(|ğœƒğ‘—|2)â‰¤ğ‘ /âˆš
ğ‘‘
â–¡
A.3 Field-Aware and Instance-Aware Interaction
Learning
The proof is equivalent to proving the following two lemmas:
Lemma A.2. If embeddings{ğœ½ğ‘—}ğ‘š
ğ‘—=1are L2-regularized such that
||ğœƒğ‘—||2â‰¤1,âˆ€ğ‘—âˆˆ{1,...,ğ‘š}, then for any given order ğœ¶âˆˆ{0,1}ğ‘š,
RFM can model the interaction pattern ğš«ğ¹=ğ’†ğ›¼1
1âŠ™ğ’†ğ›¼2
2âŠ™Â·Â·Â· ğ’†ğ›¼ğ‘šğ‘š.
Proof. We add an auxiliary dimension to the input features,
Ëœğœ½ğ‘—=[ğœ½ğ‘—,ğœ–], whereğœ–is a sufficiently small number. We construct
two types of matrices: ğ‘µ=O(ğ‘‘+1)Ã—(ğ‘‘+1)is an all-zero matrix with
a shape of(ğ‘‘+1)Ã—(ğ‘‘+1), and ğ‘´is defined by the following:
ğ‘´="0Â·Â·Â· 0
.........
0Â·Â·Â·ğœ‹
ğœ–#
âˆˆR(ğ‘‘+1)Ã—(ğ‘‘+1).
In this way, we have ğ‘´Ëœğœ½ğ‘—=[0,...,ğœ‹]âŠ¤andğ‘µËœğœ½ğ‘—=[0,...,0]âŠ¤. Here,
we set all query matrices as ğ‘¾ğ‘„
ğ‘—=ğ‘µ,âˆ€ğ‘—={1,2,...,ğ‘š}. Given the
order vector ğ›¼, the key matrices are set by the following rule:
ğ‘¾ğ¾
ğ‘—=ğ‘´, ğ›¼ğ‘—=0
ğ‘µ, ğ›¼ğ‘—=1
In this way, the matrices of the queries are mapped to a zero space,
i.e.,ğœ½ğ‘„
ğ‘—=ğ‘¾ğ‘„
ğ‘—Ëœğœ½ğ‘—=ğ‘µËœğœ½=0. As for the keys, when ğ‘—satisfiesğ›¼ğ‘—=0,
the transformed vector ğœ½ğ¾
ğ‘—=ğ‘¾ğ¾
ğ‘—Ëœğœ½ğ‘—=ğ‘´Ëœğœ½ğ‘—=[0,...,ğœ‹]âŠ¤; whenğ‘—
satisfiesğ›¼ğ‘—=1,ğœ½ğ¾
ğ‘—=ğ‘¾ğ¾
ğ‘—Ëœğœ½ğ‘—=ğ‘µËœğœ½ğ‘—=0. We set the weight vector
ğ’˜=ğ‘†Â·[0,...,1]âŠ¤, andğ‘†>0is a sufficiently large number. Consider
the attention score from ğ‘—-th query, we have:
ğ›¼ğ‘…ğ¹ğ‘€
ğ‘—,ğ‘™=RotAtt(ğ‘¸ğ‘—,ğ‘²ğ‘™)=Sigmoid
ğ’˜âŠ¤cos(ğœ½ğ‘„
ğ‘—âˆ’ğœ½ğ¾
ğ‘™)
=Sigmoid(âˆ’ğ‘†)=0, ğ›¼ğ‘™=0
Sigmoid(ğ‘†)=1, ğ›¼ğ‘™=1
 
2922KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
Therefore, we have ğœ¶ğ‘…ğ¹ğ‘€
ğ‘—=ğœ¶. Furthermore, we define the value
matrix as follows:
ğ‘¾ğ‘‰
ğ‘—="1Â·Â·Â· 0 0
............
0Â·Â·Â· 1 0#
âˆˆRğ‘‘Ã—(ğ‘‘+1).
Therefore ğœ½ğ‘‰
ğ‘—=ğ‘¾ğ‘‰
ğ‘—Ëœğœ½ğ‘—=ğœ½ğ‘—. According to Eq. 7, we have:
Ë†ğœ½ğ‘—=ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘…ğ¹ğ‘€
ğ‘—,ğ‘™ğœ½ğ‘‰
ğ‘™=ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™.
In this scheme, all Ë†ğœ½ğ‘—are the same, and we only consider a single
output of rotated angles. We remove the amplification network,
and set the weight ğ’–(See Eq. 16) as the identity matrix ğ‘°. According
to Eq. 17, when omitting the activation function, the output of RFM
can be given as:
Ë†ğ‘¦=ğ’–âŠ¤(cos(Ë†ğœ½ğ‘™)+sin(Ë†ğœ½ğ‘™))
=cos(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)+sin(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)
=H
cos(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)+ğ‘–sin(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)
=H
exp(ğ‘–ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)
=H(ğ‘’ğ‘–ğ›¼1ğœ½1âŠ™ğ‘’ğ‘–ğ›¼2ğœ½2âŠ™Â·Â·Â·âŠ™ğ‘’ğ‘–ğ›¼ğ‘šğœ½ğ‘š).
Therefore, Lemma A.2 is proved by combining Lemma A.1. â–¡
Lemma A.3. If embeddings{ğœ½ğ‘—}ğ‘š
ğ‘—=1are L2-regularized such that
||ğœ½ğ‘—||2â‰¤1,âˆ€ğ‘—âˆˆ{1,...,ğ‘š}, RFM can model the interaction pattern
ğš«ğ¼=ğ’†ğ›¼1
1âŠ™ğ’†ğ›¼2
2âŠ™Â·Â·Â· ğ’†ğ›¼ğ‘šğ‘š, whereğ›¼ğ‘—=ğ‘“(ğ’†ğ‘—)andğ‘“:Rğ‘‘â†’{0,1}is
an instance importance function.
Proof. Given the set of input feature embeddings {ğ’†ğ‘—=ğœ½ğ‘—}ğ‘š
ğ‘—=1,
we add an auxiliary dimension to the input features, Ëœğœ½ğ‘—=[ğœ½ğ‘—,ğœ–Â·
ğ‘“(ğ’†ğ‘—)], whereğœ–is a sufficiently small number. We construct two
types of matrices: ğ‘µ=O(ğ‘‘+1)Ã—(ğ‘‘+1)is an all-zeros matrix with the
shape of(ğ‘‘+1)Ã—(ğ‘‘+1), and ğ‘´is defined by the following:
ğ‘´="0Â·Â·Â· 0
.........
0Â·Â·Â·ğœ‹
ğœ–#
âˆˆR(ğ‘‘+1)Ã—(ğ‘‘+1).We have ğ‘´Ëœğœ½ğ‘—=[0,...,ğœ‹Â·ğ‘“(ğ’†ğ‘—)]âŠ¤andğ‘µËœğœ½ğ‘—=[0,...,0]âŠ¤. Here
we set all query matrices as ğ‘¾ğ‘„
ğ‘—=ğ‘µ,âˆ€ğ‘—={1,2,...,ğ‘š}and key
matrices as ğ‘¾ğ¾
ğ‘—=ğ‘´,âˆ€ğ‘—={1,2,...,ğ‘š}. We set the weight vector
ğ’˜=[0,...,âˆ’ğ‘†]âŠ¤, andğ‘†>0is a sufficiently large number. In this
way, the matrices for the queries are mapped into a zero space, i.e.,
ğœ½ğ‘„
ğ‘—=ğ‘¾ğ‘„
ğ‘—Ëœğœ½ğ‘—=ğ‘µËœğœ½=0, and the keys are ğœ½ğ¾
ğ‘—=ğ‘¾ğ¾
ğ‘—Ëœğœ½ğ‘—=ğ‘´Ëœğœ½ğ‘—=
[0,...,ğœ‹Â·ğ‘“(ğ’†ğ‘—)]âŠ¤. Sinceğ‘“(ğ’†ğ‘—)âˆˆ{ 0,1}, thus we have Sigmoid
âˆ’
ğ‘†Â·cos
ğœ‹Â·ğ‘“(ğ‘’ğ‘—)
=ğ‘“(ğ’†ğ‘—). Consider the attention score from ğ‘—-th
query, we have:
ğ›¼ğ‘…ğ¹ğ‘€
ğ‘—,ğ‘™=Sigmoid
ğ’˜âŠ¤cos(ğœ½ğ‘„
ğ‘—âˆ’ğœ½ğ¾
ğ‘™)
=Sigmoid
âˆ’ğ‘†Â·cos
ğœ‹Â·ğ‘“(ğ‘’ğ‘™)
=ğ‘“(ğ’†ğ‘™).
Further, we set the value matrices as the following:
ğ‘¾ğ‘‰
ğ‘—="1Â·Â·Â· 0 0
............
0Â·Â·Â· 1 0#
âˆˆRğ‘‘Ã—(ğ‘‘+1).
Therefore ğœ½ğ‘‰
ğ‘—=ğ‘¾ğ‘‰
ğ‘—Ëœğœ½ğ‘—=ğœ½ğ‘—. According to Eq. 7, we have:
Ë†ğœ½ğ‘—=ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘…ğ¹ğ‘€
ğ‘—,ğ‘™ğœ½ğ‘‰
ğ‘—=ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™.
In this scheme, all Ë†ğœ½ğ‘—are the same; we only consider a single output
of rotated angles and set the weight ğ’–(See Eq. 16) as the identity
matrix ğ‘°. According to Eq. 17, when the activation function is
omitted, the output of RFM can be given as:
Ë†ğ‘¦=ğ’–âŠ¤(cos(Ë†ğœ½ğ‘™)+sin(Ë†ğœ½ğ‘™))
=cos(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)+sin(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)
=H
cos(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)+ğ‘–sin(ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)
=H
exp(ğ‘–ğ‘šâˆ‘ï¸
ğ‘™=1ğ›¼ğ‘™ğœ½ğ‘™)
=H(ğ‘’ğ‘–ğ›¼1ğœ½1âŠ™ğ‘’ğ‘–ğ›¼2ğœ½2âŠ™Â·Â·Â·âŠ™ğ‘’ğ‘–ğ›¼ğ‘šğœ½ğ‘š)
=H(ğ‘’ğ‘–ğ‘“(ğ’†1)ğœ½1âŠ™ğ‘’ğ‘–ğ‘“(ğ’†2)ğœ½2âŠ™Â·Â·Â·âŠ™ğ‘’ğ‘–ğ‘“(ğ’†ğ‘š)ğœ½ğ‘š).
Since the construction of the order is independent of the input fea-
tures{ğ’†ğ‘—=ğœ½ğ‘—}ğ‘š
ğ‘—=1, lemma A.3 is proven by combining lemma A.1.
â–¡
 
2923