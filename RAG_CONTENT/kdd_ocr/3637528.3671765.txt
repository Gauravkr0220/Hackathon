Efficient Topology-aware Data Augmentation for High-Degree
Graph Neural Networks
Yurui Lai
Hong Kong Baptist University
Hong Kong, China
csyrlai@hkbu.edu.hkXiaoyang Lin
Hong Kong Baptist University
Hong Kong, China
csxylin@hkbu.edu.hk
Renchi Yang
Hong Kong Baptist University
Hong Kong, China
renchi@hkbu.edu.hkHongtao Wang
Hong Kong Baptist University
Hong Kong, China
cshtwang@hkbu.edu.hk
ABSTRACT
In recent years, graph neural networks (GNNs) have emerged as a
potent tool for learning on graph-structured data and won fruit-
ful successes in varied fields. The majority of GNNs follow the
message-passing paradigm, where representations of each node are
learned by recursively aggregating features of its neighbors. How-
ever, this mechanism brings severe over-smoothing and efficiency
issues over high-degree graphs (HDGs), wherein most nodes have
dozens (or even hundreds) of neighbors, such as social networks,
transaction graphs, power grids, etc. Additionally, such graphs usu-
ally encompass rich and complex structure semantics, which are
hard to capture merely by feature aggregations in GNNs.
Motivated by the above limitations, we propose TADA, an effi-
cient and effective front-mounted data augmentation framework for
GNNs on HDGs. Under the hood, TADA includes two key modules:
(i) feature expansion with structure embeddings, and (ii) topology-
and attribute-aware graph sparsification. The former obtains aug-
mented node features and enhanced model capacity by encoding
the graph structure into high-quality structure embeddings with
our highly-efficient sketching method. Further, by exploiting task-
relevant features extracted from graph structures and attributes, the
second module enables the accurate identification and reduction
of numerous redundant/noisy edges from the input graph, thereby
alleviating over-smoothing and facilitating faster feature aggrega-
tions over HDGs. Empirically, TADA considerably improves the
predictive performance of mainstream GNN models on 8 real ho-
mophilic/heterophilic HDGs in terms of node classification, while
achieving efficient training and inference processes.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks ;Supervised
learning by classification ;â€¢Mathematics of computing â†’Ap-
proximation algorithms .
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671765KEYWORDS
graph neural networks, data augmentation, sketching, sparsification
ACM Reference Format:
Yurui Lai, Xiaoyang Lin, Renchi Yang, and Hongtao Wang. 2024. Efficient
Topology-aware Data Augmentation for High-Degree Graph Neural Net-
works. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671765
1 INTRODUCTION
Graph neural networks (GNNs) are powerful deep learning archi-
tectures for relational data (a.k.a. graphs or networks), which have
exhibited superb performance in extensive domains spanning across
recommender systems [ 97], bioinformatics [ 21,68], transportation
[17,32], finance [ 10,84,102], and many other [ 26,40,51,86]. The
remarkable success of GNN models is primarily attributed to the
recursive message passing (MP) (a.k.a. feature aggregation or fea-
ture propagation) scheme [ 26], where the features of a node are
iteratively updated by aggregating the features from its neighbors.
In real world, graph-structured data often encompasses a wealth
of node-node connections (i.e., edges), where most nodes are adja-
cent to dozens or hundreds of neighbors on average, which are re-
ferred to as high-degree graphs (hereafter HDGs). Practical examples
include social networks/medias (e.g., Facebook, TikTok, LinkedIn),
transaction graphs (e.g., PayPal and AliPay), co-authorship net-
works, airline networks, and power grids. Over such graphs, the
MP mechanism undergoes two limitations: (i) homogeneous node
representations after merely a few rounds of feature aggregations
(i.e., over-smoothing [ 8]), and (ii) considerably higher computation
overhead. Apart from this, the majority of GNNs mainly focus on
designing new feature aggregation rules or model architectures,
where the rich structural features of nodes in HDGs are largely
overlooked and under-exploited.
To prevent overfitting and over-smoothing in GNNs, a series of
studies draw inspiration from Dropout [ 67] and propose to ran-
domly remove or mask edges [ 60], nodes [ 20,99], subgraphs [ 99]
from the input graph Gduring model training. Although such ran-
dom operations can be done efficiently, they yield information loss
and sub-optimal results due to removing graph elements while
overlooking their importance to Gin the context of tasks. Recently,
some researchers [ 48,66] applied graph sparsification techniques
for better graph reduction, which is also task-unaware and fails to
 
1463
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yurui Lai et al.
account for attribute information. Instead of relying on heuristics,
several attempts [ 22,34,107] have been made to search for better
graph structures that augment Gviagraph structure learning. This
methodology requires expensive training, might create additional
edges inG, and thus can hardly cope with HDGs. To tackle the
feature-wise limitation of GNNs, recent efforts [ 63,69,77] resort
to expanding node features with proximity metrics (e.g., hitting
time, commute time) or network embeddings [ 6], both of which
are computationally demanding, especially on large HDGs. In sum,
existing augmentation techniques for GNNs either compromise ef-
fectiveness or entail significant extra computational expense when
applied to HDGs.
In response, this paper proposes TADA, an effective and efficient
data augmentation solution tailored for GNN models on HDGs. In
particular, TADA tackles the aforementioned problems through two
vital contributions: (i) efficient feature expansion with sketch-based
structure embeddings; and (ii) topology- and attribute-aware graph
sparsification. The former aims to extract high-quality structural
features underlying the input HDG Gto expand node features for
bolstered model performance in a highly-efficient fashion, while the
latter seeks to attenuate the adverse impacts (i.e., over-smoothing
and efficiency issues) of feature aggregations on HDGs by expung-
ing redundant/noisy edges in Gwith consideration of both graph
topology and node attributes.
To achieve the first goal, we first empirically and theoretically
substantiate the effectiveness of the intact graph structures (i.e.,
adjacency matrices) in improving GNNs when serving as additional
node attributes. In view of its impracticality on large HDGs, we
further develop a hybrid sketching approach that judiciously inte-
grates our novel topology-aware RWR-Sketch technique into the
data-oblivious Count-Sketch method for fast and accurate embed-
dings of graph structures. Compared to naive Count-Sketch, which
offers favorable theoretical merits but has flaws in handling highly
skewed data (i.e., HDGs) due to its randomness, RWR-Sketch reme-
dies this deficiency by injecting a concise summary of the HDG
using the random walk with restart [73] model. The resulted struc-
ture embeddings, together with node attributes, are subsequently
transformed into task-aware node features via pre-training. On top
of that, we leverage such augmented node features for our second
goal. That is, instead of direct sparsification of the HDG G, we
first construct an edge-reweighted graph Gğ‘¤using the enriched
node features. Building on our rigorous theoretical analysis, a fast
algorithm for estimating the centrality values of edges in Gğ‘¤is
devised for identifying unnecessary/noisy edges.
We extensively evaluate TADA along with 5 classic GNN mod-
els on 4 homophilic graphs and 4 heterophilic graphs in terms of
node classification. Quantitatively, the tested GNNs generally and
consistently achieve conspicuous improvements in accuracy (up
to20.14%) when working in tandem with TADA , while offering
matching or superior efficiency (up to orders of magnitude speedup)
in each training and inference epoch.
To summarize, our paper makes the following contributions:
â€¢Methodologically, we propose a novel data augmentation frame-
work TADA for GNNs on HDGs, comprising carefully-crafted
skecthing-based feature expansion and graph sparsification.â€¢Theoretically, we corroborate the effectiveness of using the adja-
cency matrix as auxiliary attributes and establish related theoret-
ical bounds in our sketching and sparsification modules.
â€¢Empirically, we conduct experiments on 8 benchmark datasets
and demonstrate the effectiveness and efficiency of TADA in
augmenting 5 popular GNN models.
2 RELATED WORKS
Data Augmentation for GNNs. Data augmentation for GNNs
(GDA) aims at increasing the generalization ability of GNN mod-
els through structure modification or feature generation, which
has been extensively studied in the literature [ 2,18,92,106]. Ex-
isting GDA works can be generally categorized into two types: (i)
rule-based methods and (ii) learning-based methods. More specif-
ically, rule-based GDA techniques rely on heuristics (pre-defined
rules) to modify or manipulate the graph data. Similar in spirit to
Dropout [ 67], DropEdge [ 60] and its variants [ 19,20,70,72,82]
randomly remove or mask edges, nodes, features, subgraphs, or mes-
sages so as to alleviate the over-fitting and over-smoothing issues.
However, this methodology causes information loss and, hence,
sub-optimal quality since the removal operations treat all graph
elements equally. In lieu of removing data, Ying et al . [96] propose
to add virtual nodes that connect to all nodes and [ 29,79,83] create
new data samples by either interpolating training samples [ 103] or
hidden states and labels [ 78]. Besides, recent studies [ 46,63,69,77]
explored extracting additional node features from graph structures.
For instance, Song et al . [63] augment node attributes with node
embeddings from DeepWalk [ 57] and Velingker et al . [77] expand
node features with random walk measures (e.g., effective resis-
tance, hitting and commute times). These approaches enjoy better
effectiveness at the expense of high computation costs, which are
prohibitive on large HDGs.
Along another line, learning-based approaches leverage deep
learning for generations of task-specific augmented samples. Moti-
vated by the assumption that graph data is noisy and incomplete,
graph structure learning (GSL) [ 34,35,107] methods learn better
graph structures by treating graph structures as learnable parame-
ters. As an unsupervised learning method, graph contrastive learning
(GCL) [ 99,110] techniques have emerged as a promising avenue
to address the challenges posed by noisy and incomplete graph
data, enhancing the robustness and generalization of graph neu-
ral networks (GNNs) on high-dimensional graphs (HDGs). Unlike
GSL and GCL, [ 33,38,88] extend adversarial training to graph
domains and augments input graphs with adversarial patterns by
perturbing node features or graph structures during model training.
Rationalization methods [ 44,87] seek to learn subgraphs that are
causally related with the graph labels as a form of augmented graph
data, which are effective in solving out-of-distribution and data
bias issues. Recently, researchers [ 50,98,108] utilized reinforce-
ment learning agents to automatically learn optimal augmentation
strategies for different subgraphs or graphs. These learning-based
approaches are all immensely expensive, and none of them tackle
the issues of GNNs on HDGs as remarked in Section 1.
Structure Embedding. The goal of structure embedding (or net-
work embedding) is to convert the graph topology surrounding
each node into a low-dimensional feature vector. As surveyed in [ 6],
 
1464Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
there exists a large body of literature on this topic, most of which can
be summarized into three categories as per their adopted method-
ology: (i) random walk-based methods, (ii) matrix factorization-
based methods, and (iii) deep learning-based models. In particular,
random walk-based methods [ 27,57,74] learn node embeddings
by optimizing the skip-gram model [ 53] or its variants with ran-
dom walk samples from the graph. Matrix factorization-based ap-
proaches [ 54,59,93,94,105] construct node embeddings through
factorizing node-to-node affinity matrices, whereas [ 1,7,80,100]
capitalize on diverse deep neural network models for node rep-
resentation learning on non-attributed graphs. Recent evidence
suggests that using such network embeddings [ 63], or resistive
embeddings [77] and spectral embeddings [69] as complementary
node features can bolster the performance of GNNs, but result in
considerable additional computational costs.
Graph Sparsification. Graph sparsification is a technique aimed
at approximating a given graph Gwith a sparse graph containing
a subset of nodes and/or edges from G[11]. Classic sparsification
algorithms for graphs include cut sparsification [ 23,36] and spectral
sparsification [ 3,64,65]. Cut sparsification reduces edges while
preserving the value of the graph cut, while spectral sparsifiers
ensure the sparse graphs can retain the spectral properties of the
original ones. Recent studies [ 48,66] employ these techniques as
heuristics to sparsify the input graphs before feeding them into
GNN models for acceleration of GNN training [ 47]. In spite of their
improved empirical efficiency, these works fail to incorporate node
attributes as well as task information for sparsification. To remove
task-irrelevant edges accurately, Zheng et al . [109] and Li et al . [41]
cast graph sparsification as optimization problems and apply deep
neural networks and the alternating direction method of multipliers,
respectively, both of which are cumbersome for large HDGs.
3 PRELIMINARIES
3.1 Notations
Throughout this paper, sets are denoted by calligraphic letters, e.g.,
V. Matrices (resp. vectors) are written in bold uppercase (resp.
lowercase) letters, e.g., M(resp. x). We use Mğ‘–andM:,ğ‘–to represent
theğ‘–throw and column of M, respectively.
LetG=(V,E)be a graph (a.k.a. network), where Vis a set ofğ‘›
nodes andEis a set ofğ‘šedges. For each edge ğ‘’ğ‘–,ğ‘—âˆˆEconnecting
nodesğ‘£ğ‘–andğ‘£ğ‘—, we sayğ‘£ğ‘–andğ‘£ğ‘—are neighbors to each other.
We useN(ğ‘£ğ‘–)to denote the set of neighbors of node ğ‘£ğ‘–, where
the degree of ğ‘£ğ‘–(i.e.,|N(ğ‘£ğ‘–)|) is symbolized by ğ‘‘(ğ‘£ğ‘–). Nodesğ‘£ğ‘–
inGare endowed with an attribute matrix XâˆˆRğ‘›Ã—ğ‘‘, whereğ‘‘
stands for the dimension of node attribute vectors. The diagonal
degree matrix ofGis denoted as D=diag(ğ‘‘(ğ‘£1),Â·Â·Â·,ğ‘‘(ğ‘£ğ‘›)). The
adjacency matrix and normalized adjacency matrix are denoted as
AandeA=Dâˆ’1
2ADâˆ’1
2, respectively. The Laplacian and transition
matrices ofGare defined by L=Dâˆ’AandP=Dâˆ’1A, respectively.
3.2 Graph Neural Networks (GNNs)
The majority of existing GNNs [ 4,13,16,25,31,45,76,85,89,90]
follow the message passing (MP) paradigm [ 26], such as GCN [ 37],
APPNP [ 24], and GCNII [ 9]. For simplicity, we refer to all these MP-
based models as GNNs. More concretely, the node representationsTable 1: Classification Accuracy with Xâˆ¥Aas Features.
Dataset WikiCS Squirrel
GCN 84.05%Â±0.76% 54.85%Â±2.02%
GCN (Xâˆ¥A) 84.15%Â±0.48% 56.06%Â±3.12%
GAT 83.74%Â±0.75% 55.70%Â±3.26%
GAT (Xâˆ¥A) 84.15%Â±0.83% 57.77%Â±2.13%
APPNP 85.04%Â±0.60% 54.47%Â±2.06%
APPNP (Xâˆ¥A) 85.24%Â±0.56% 57.64%Â±1.32%
GCNII 85.13%Â±0.56% 53.13%Â±4.29%
GCNII (Xâˆ¥A) 85.28%Â±0.78% 54.35%Â±4.04%
H(ğ‘¡)atğ‘¡-th layer of GNNs can be written as
H(ğ‘¡)=ğœ(ğ‘“trans(ğ‘“aggr(G,H(ğ‘¡âˆ’1)))),
H(0)=ğœ(Xğ›€orig)âˆˆRğ‘›Ã—â„(1)
whereğœ(Â·)stands for a nonlinear activate function, ğ‘“trans(Â·)corre-
sponds to a layer-wise feature transformation operation (usually
an MLP including non-linear activation ReLU and layer-specific
learnable weight matrix), and ğ‘“aggr(G,Â·)represents the operation
of aggregating â„“-th layer features H(â„“)from the neighborhood along
graphG, e.g.,ğ‘“aggr(G,H(ğ‘¡))=eAH(ğ‘¡âˆ’1)in GCN and ğ‘“aggr(G,H(ğ‘¡))=
(1âˆ’ğ›¼)eAH(ğ‘¡âˆ’1)+ğ›¼Â·H(0)in APPNP. Note that H(0)=ğœ(Xğ›€orig)âˆˆ
Rğ‘›Ã—â„is the initial node features resulted from a non-linear trans-
formation from the original node attribute matrix Xusing an MLP
parameterized by learnable weight ğ›€orig. As demystified in a num-
ber of studies [ 25,69,81,85,112], after removing non-linearity, the
node representations H(ğ‘¡)learned at the ğ‘¡-layer in most MP-GNNs
can be rewritten as linear approximation formulas:
H(ğ‘¡)=ğ‘“poly(eA,ğ‘¡)Â·XÂ·ğ›€, (2)
whereğ‘“poly(eA,ğ‘¡)stands for a ğ‘¡-order polynomial, eA(orP) is the
structure matrix of G, andğ›€is the learned weight. For instance,
ğ‘“poly(G,ğ‘¡)=eAğ‘¡Xin GCN and ğ‘“poly(G,ğ‘¡)=Ãğ‘¡
ğ‘–=0ğ›¼ğ‘–eAğ‘–Xin APPNP.
3.3 GNNs over High-Degree Graphs (HDGs)
Although GNNs achieve superb performance by the virtue of the
feature aggregation mechanism, they incur severe inherent draw-
backs, which are exacerbated over HDGs, as analysed below.
Inadequate Structure Features. Intuitively, structure features
play more important roles for HDGs as they usually encompass
rich and complex topology semantics. However, the extant GNNs
primarily capitalize on the graph structure for feature aggregation,
failing to extract the abundant topology semantics underlying G.
To validate this observation, we conduct a preliminary empirical
study with 4 representative GNN models on 2 benchmarking HDGs
[52,56] in terms of node classification. Table 1 manifests that by
concatenating the input attribute matrix X(i.e., Xâˆ¥A) with the
adjacency matrix Aas node features, each GNN model can see
performance gains (up to 3.17%). In our technical report [ 39], we
further theoretically show that expanding features with Acan alle-
viate the feature correlation [69] in standard GNNs and additionally
incorporate high-order proximity information between nodes as in
traditional network embedding techniques [27, 59].
 
1465KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yurui Lai et al.
However, this simple trick demands learning an (ğ‘›+ğ‘‘)Ã—â„trans-
formation weight matrix ğ›€orig, and hence, leads to the significant
expense of training.
Over-Smoothing. Note that HDGs often demonstrate high con-
nectivity, i.e., nodes are connected to dozens or even hundreds of
neighbors on average, which implies large spectral gaps ğœ†[14]. The
matrix powers eAğ‘¡andPğ‘¡will hence quickly converge to stationary
distributions as ğ‘¡increases, as indicated by Theorem 3.1.
Theorem 3.1. Suppose thatGis a connected and non-bipartite
graph. Then, we have
eAğ‘¡
ğ‘–,ğ‘—âˆ’âˆšï¸
ğ‘‘(ğ‘£ğ‘—)Â·ğ‘‘(ğ‘£ğ‘–)
2ğ‘šâ‰¤(1âˆ’ğœ†)ğ‘¡,Pğ‘¡
ğ‘–,ğ‘—âˆ’ğ‘‘(ğ‘£ğ‘—)
2ğ‘šâ‰¤âˆšï¸„
ğ‘‘(ğ‘£ğ‘—)
ğ‘‘(ğ‘£ğ‘–)(1âˆ’ğœ†)ğ‘¡,
whereğœ†<1is the spectral gap of G.
Proof. Letğœ1â‰¥ğœ2â‰¥Â·Â·Â·â‰¥ğœğ‘›be the eigenvalues of eAandğœis
defined byğœ=min{|ğœ2|,|ğœğ‘›|}. By definition, the spectral gap of G
is thenğœ†=1âˆ’ğœ. By Theorem 5.1 in [ 49], it is straightforward to
get
Pğ‘¡
ğ‘–,ğ‘—âˆ’ğ‘‘(ğ‘£ğ‘—)
2ğ‘šâ‰¤âˆšï¸„
ğ‘‘(ğ‘£ğ‘—)
ğ‘‘(ğ‘£ğ‘–)(1âˆ’ğœ†)ğ‘¡. (3)
Recall that eA=Dâˆ’1
2ADâˆ’1
2andP=Dâˆ’1A. Hence,
eAğ‘¡=Dâˆ’1
2A(Dâˆ’1A)ğ‘¡âˆ’1Dâˆ’1
2=D1
2Pğ‘¡Dâˆ’1
2,
meaning that eAğ‘¡
ğ‘–,ğ‘—=âˆšï¸‚
ğ‘‘(ğ‘£ğ‘–)
ğ‘‘(ğ‘£ğ‘—)Â·Pğ‘¡
ğ‘–,ğ‘—. Plugging this into Eq. (3)yields
âˆšï¸„
ğ‘‘(ğ‘£ğ‘–)
ğ‘‘(ğ‘£ğ‘—)Pğ‘¡
ğ‘–,ğ‘—âˆ’âˆšï¸„
ğ‘‘(ğ‘£ğ‘–)
ğ‘‘(ğ‘£ğ‘—)Â·ğ‘‘(ğ‘£ğ‘—)
2ğ‘šâ‰¤âˆšï¸„
ğ‘‘(ğ‘£ğ‘–)
ğ‘‘(ğ‘£ğ‘—)âˆšï¸„
ğ‘‘(ğ‘£ğ‘—)
ğ‘‘(ğ‘£ğ‘–)(1âˆ’ğœ†)ğ‘¡
=eAğ‘¡
ğ‘–,ğ‘—âˆ’âˆšï¸
ğ‘‘(ğ‘£ğ‘—)Â·ğ‘‘(ğ‘£ğ‘–)
2ğ‘šâ‰¤(1âˆ’ğœ†)ğ‘¡.
which finishes the proof. â–¡
As an aftermath, for any node ğ‘£ğ‘–âˆˆğ‘‰, its node representation
H(ğ‘¡)
ğ‘–obtained in Eq. (2) turns into
âˆšï¸
ğ‘‘(ğ‘£ğ‘–)âˆ‘ï¸
ğ‘£ğ‘—âˆˆVâˆšï¸
ğ‘‘(ğ‘£ğ‘—)
2ğ‘šğ‘‘âˆ‘ï¸
â„“=1Xğ‘—ğ›€andâˆ‘ï¸
ğ‘£ğ‘—âˆˆVâˆšï¸
ğ‘‘(ğ‘£ğ‘—)
2ğ‘šXğ‘—ğ›€
whenğ‘“poly(eA,ğ‘¡)=eAğ‘¡andPğ‘¡, respectively, both of which are essen-
tially irrelevant to node ğ‘£ğ‘–. In other words, the eventual represen-
tations of all nodes are overly smoothed with high homogeneity,
rendering nodes in different classes indistinguishable and resulting
in degraded model performance [5, 8, 9].
Costly Feature Aggregation. Aside from the over-smoothing,
the sheer amount of feature aggregation operations of GNNs over
HDGs, especially on sizable ones, engender vast computation cost.
Recall that each round of feature aggregation in GNNs consumes
ğ‘‚(ğ‘šâ„)time. Compared to normal scale-free graphs with average
node degrees ğ‘š/ğ‘›=ğ‘‚(log(ğ‘›))or smaller, the average node de-
grees in HDGs can be up to hundreds, which are approximately
ğ‘‚(log2(ğ‘›)). This implies an ğ‘‚(ğ‘›log2(ğ‘›)Â·â„)asymptotic cost in total
for each round of feature aggregation.
.
.
GNNsğ‡(0)
ğ€Â°ğ‡topoğ‡attr ğ—
ğ€â€²ğ€
0.8
0.8
0.75
0.7
0.6
0.3
0.25
0.21.5
1.5 1.21.41.20.8
0.50.7ğ‡(0).
..
Count -Sketch
RWR -SketchModule I (Feature Expansion)
Module II (Graph Sparsification)ğ’¢Attribute Matrix Adj. Matrix
ğ’¢ğ‘¤1.5 1.2
1.51.4
1.2cos(âˆ™) ERAugmented
Node Features
Sparsified Adj. MatrixStructure
FeaturesAttribute
Features
MLP
MLP
MLPPre-Training
Figure 1: Overview of TADA
A workaround to mitigate the over-smoothing and computation
issues caused by the feature aggregation on HDGs is to sparsify
Gby identifying and eradicating unnecessary or redundant edges.
However, the accurate and efficient identification of such edges
for improving GNN models is non-trivial in the presence of node
attributes and labels and remains under-explored.
In sum, we need to address two technical challenges:
â€¢How to encode Ainto high-quality structure embeddings that
can augment GNNs for better model capacity on HDGs in an
efficient manner?
â€¢How to sparsify the input HDG so as to enable faster feature
aggregation while retaining the predictive power?
4 METHODOLOGY
This section presents our TADA framework for tackling the forego-
ing challenges. Section 4.1 provides an overview of TADA, followed
by detailing its two key modules in Sections 4.2 and 4.3, respectively.
4.1 Synoptic Overview of TADA
As illustrated in Figure 1, TADA acts as a front-mounted stage for
MP-GNNs, which compromises two main ingredients: (i) feature
expansion with structure embeddings (Module I), and (ii) topology-
and attribute-aware graph sparsification (Module II). The goal of
the former component is to generate high-quality structure embed-
dings Htopocapturing the rich topology semantics underlying Gfor
feature expansion, while the latter aims to sparsify the structure of
the input graphGso as to eliminate redundant or noisy topological
connections inGwith consideration of graph topology and node
attributes.
Module I: Feature Expansion. To be more specific, in Module I,
TADA first applies a hybrid sketching technique (Count-Sketch +
RWR-Sketch) to the adjacency matrix AofGand transforms the
sketched matrix Aâ€²âˆˆRğ‘›Ã—ğ‘˜(ğ‘˜â‰ªğ‘›, typicallyğ‘˜=128) into the
structure embeddings HtopoâˆˆRğ‘›Ã—â„of all nodes via an MLP:
Htopo=ğœ(Aâ€²ğ›€topo), (4)
whereğœ(Â·)is a non-linear activation function (e.g., ReLU) and
ğ›€topoâˆˆRğ‘˜Ã—â„stands for learnable transformation weights. In the
meantime, Module I feeds the node attribute matrix Xto an MLP
network parameterized by learnable weight ğ›€attrâˆˆRğ‘‘Ã—â„to obtain
 
1466Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
the transformed node attribute features HattrâˆˆRğ‘›Ã—â„.
Hattr=ğœ(Xğ›€attr) (5)
A linear combination of the structure embeddings Htopoand
transformed node attribute features Hattras in Eq. (6)yields the
initial node representations H(0).
H(0)=(1âˆ’ğ›¾)Â·Hattr+ğ›¾Â·HtopoâˆˆRğ‘›Ã—â„(6)
The hyper-parameter ğ›¾controls the importance of node topology
in the resulting node representations.
Notice that H(0)and related learnable weights are pre-trained
by the task (i.e., node classification) with a single-layer MLP as
classifier (using ğ‘›ğ‘epochs). In doing so, we can extract task-specific
features in H(0)to facilitate the design of Module II, and all these
intermediates can be reused for subsequent GNN training.
Module II: Graph Sparsification. Since H(0)captures the task-
aware structure and attribute features of nodes in G, Module II can
harness it to calculate the centrality values of all edges that assess
their importance to Gin the context of node classification. Given the
sparsification raio ğœŒ, the edges with ğ‘šÂ·ğœŒlowest centrality values are
therefore removed from G, whereas those important ones will be
kept and reweighted by the similarities of their respective endpoints
inH(0). Based thereon, TADA creates a sparsified adjacency matrix
denoted as Aâ—¦as a substitute of A. The behind intuition is that
adjacent nodes with low connectivity and attribute homogeneity
are more likely to fall under disparate classes, and hence, their
direct connection (i.e., edges) can be removed without side effects.
Finally, the augmented initial node representations H(0)and the
sparsified adjacency matrix Aâ—¦are input into the MP-GNN models
ğ‘“GNN(Â·,Â·)for learning final node representations:
H=ğ‘“GNN(H(0),Aâ—¦)
and performing the downstream task, i.e., node classification.
In the succeeding subsections, we elaborate on the designs and
details of Module I and Module II.
4.2 Efficient Feature Expansion with Structure
Embeddings
Recall that in Module I, the linchpin to the feature expansion (i.e.,
building structure embeddings Htopo) isAâ€²âˆˆRğ‘›Ã—ğ‘˜, a sketch of
the adjacency matrix A. Notice that even for HDGs, Ais highly
sparse (ğ‘šâ‰ªğ‘›2) and the distribution of node degrees (i.e., the
numbers of non-zero entries in rows/columns) is heavily skewed,
rendering existing sketching tools for dense matrices unsuitable. In
what follows, we delineate our hybrid sketching approach specially
catered for adjacency matrix A.
Count-Sketch Method. To deal with the sparsity of A, our first-
cut solution is the count-sketch (or called sparse embedding) [ 15]
technique, which achieves ğ‘‚(nnz(A))=ğ‘‚(ğ‘š)time for computing
the sketched adjacency matrix Aâ€²âˆˆRğ‘›Ã—ğ‘˜:
Aâ€²=ARâŠ¤where R=ğš½ğš« (7)
The count-sketch matrix (a.k.a. sparse embedding) RâˆˆRğ‘˜Ã—ğ‘›is
randomly constructed by where
â€¢ğš«âˆˆRğ‘›Ã—ğ‘›is a diagonal matrix with each diagonal entry inde-
pendently chosen to be 1orâˆ’1with probability 0.5, andâ€¢ğš½âˆˆ{0,1}ğ‘˜Ã—ğ‘›is a binary matrix with ğš½â„(ğ‘–),ğ‘–=1and0otherwise
âˆ€1â‰¤ğ‘–â‰¤ğ‘›. The function â„(Â·)mapsğ‘–(1â‰¤ğ‘–â‰¤ğ‘›) toâ„(ğ‘–)=ğ‘—âˆˆ
{1,2,Â·Â·Â·,ğ‘˜}uniformly at random.
In Theorem 4.1, we prove that the count-sketch matrix Ris able
to create an accurate estimator for the product of Aand any matrix
Wwith rigorous accuracy guarantees.
Theorem 4.1. Given any matrix Wwithğ‘›rows and a count-
sketch matrix RâˆˆRğ‘˜Ã—ğ‘›withğ‘˜=2 max ğ‘£ğ‘–âˆˆVğ‘‘(ğ‘£ğ‘–)
ğœ–2ğ›¿Â·maxğ‘—âˆ¥W:,ğ‘—âˆ¥2
2, the
following inequality
P[|(Ağ‘–RâŠ¤)Â·(RW :,ğ‘—)âˆ’Ağ‘–W:,ğ‘—|<ğœ–]>1âˆ’ğ›¿
holds for any node ğ‘£ğ‘–âˆˆV andğ‘—âˆˆ[1,ğ‘˜].
Proof. LetX=(ARâŠ¤)Â·(RW)âˆ’AW. Then Xğ‘–,ğ‘—=(Ağ‘–RâŠ¤)Â·
(RW :,ğ‘—)âˆ’Ağ‘–W:,ğ‘—, where Ağ‘–is theğ‘–-th row of AandW:,ğ‘—is theğ‘—-th
column of W. According to Lemma 4.1 in [ 58], for any two column
vectors x,yâˆˆRğ‘›,E[(Rx)âŠ¤Â·(Ry)]=xâŠ¤Â·y. Then, we have
E(Xğ‘–,ğ‘—)=E (R(Ağ‘–)âŠ¤)âŠ¤Â·(RW :,ğ‘—)âˆ’Ağ‘–W:,ğ‘—=0
Thus, for 1â‰¤ğ‘–,ğ‘—â‰¤ğ‘›,(Ağ‘–RâŠ¤)Â·(RW :,ğ‘—)is an unbiased estimator of
Ağ‘–W:,ğ‘—.
Moreover, by Lemma 4.2 in [ 58] and the Cauchyâ€“Schwarz in-
equality, we have
ğ‘‰ğ‘ğ‘Ÿ(Xğ‘–,ğ‘—)â‰¤1
ğ‘˜Â·
(Ağ‘–W:,ğ‘—)2+âˆ¥Ağ‘–âˆ¥2
2Â·âˆ¥W:,ğ‘—âˆ¥2
2
â‰¤2
ğ‘˜Â·
âˆ¥Ağ‘–âˆ¥2
2Â·âˆ¥W:,ğ‘—âˆ¥2
2
=2ğ‘‘(ğ‘£ğ‘–)
ğ‘˜Â·âˆ¥W:,ğ‘—âˆ¥2
2.
We have E(X2
ğ‘–,ğ‘—)=ğ‘‰ğ‘ğ‘Ÿ(Xğ‘–,ğ‘—)âˆ’E(Xğ‘–,ğ‘—)2â‰¤2ğ‘‘(ğ‘£ğ‘–)
ğ‘˜Â·âˆ¥W:,ğ‘—âˆ¥2
2. Using
Chebyshevâ€™s Inequality, we have
P[X2
ğ‘–,ğ‘—â‰¥ğœ–]â‰¤E(X2
ğ‘–,ğ‘—)
ğœ–2â‰¤2ğ‘‘(ğ‘£ğ‘–)
ğ‘˜Â·âˆ¥W:,ğ‘—âˆ¥2
2
ğœ–2
By settingğ‘˜=2 max ğ‘£ğ‘–âˆˆVğ‘‘(ğ‘£ğ‘–)
ğœ–2ğ›¿Â·maxğ‘—âˆ¥W:,ğ‘—âˆ¥2
2, we can guarantee
P[|(Ağ‘–RâŠ¤)Â·(RW :,ğ‘—)âˆ’Ağ‘–W:,ğ‘—|<ğœ–]>1âˆ’ğ›¿,
which completes the proof. â–¡
Recall that the ideal structure embeddings Hâˆ—
topois obtained
when Aâ€²is replaced by the original adjacency matrix Ain Eq. (4),
i.e.,Hâˆ—
topo=ğœ(Ağ›€topo). Assume that W=ğ›€topois the learned
weights in this case. If we input Aâ€²=ARâŠ¤to Eq. (4)and assume the
newly learned weight matrix is ğ›€topo=RW, the resulted structure
embeddings Htopowill be similar to the ideal one Hâˆ—
topoaccording
to Theorem 4.1, establishing a theoretical assurance for deriving
high-quality structure embeddings Htopofrom Aâ€².
By Theorem 4.1, we can further derive the following properties
ofAâ€²in preserving the structure in G:
â€¢Property 1: For any two nodes ğ‘£ğ‘–,ğ‘£ğ‘—âˆˆV,Aâ€²
ğ‘–Â·Aâ€²
ğ‘—âŠ¤is an approx-
imation of the number of common neighbors |N(ğ‘£ğ‘–)âˆ©N(ğ‘£ğ‘—)|.
Particularly,âˆ¥Aâ€²
ğ‘–âˆ¥2
2approximates the degree ğ‘‘(ğ‘£ğ‘–)of nodeğ‘£ğ‘–.
â€¢Property 2: For any two nodes ğ‘£ğ‘–,ğ‘£ğ‘—âˆˆV,(Aâ€²Aâ€²âŠ¤)ğ‘¡is an ap-
proximation of high-order proximity matrix A2ğ‘¡, where each
(ğ‘–,ğ‘—)-th entry denotes the number of length-2 ğ‘¡paths between
nodesğ‘£ğ‘–andğ‘£ğ‘—.
 
1467KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yurui Lai et al.
â€¢Property 3: LetcAâ€²be the row-based ğ¿2normalization of Aâ€². For
any two nodes ğ‘£ğ‘–,ğ‘£ğ‘—âˆˆV,cAâ€²cAâ€²âŠ¤is an approximation of (PPâŠ¤)ğ‘¡,
where each(ğ‘–,ğ‘—)-th entry denotes the probability of two length- ğ‘¡
random walks originating from ğ‘£ğ‘–andğ‘£ğ‘—meeting at any node.
Due to the space limit, we defer the proofs to our technical re-
port [39].
Limitation of Count-Sketch. Despite the theoretical merits of
approximation guarantees and high efficiency offered by the count-
sketch-based approach, it is data-oblivious (i.e., the sketching matrix
is randomly generated) and is likely to produce poor results, espe-
cially in dealing with highly skewed data (e.g., adjacency matrices).
To explain, we first interpret ğš½as a randomized clustering member-
ship indicator matrix, where ğš½â„(ğ‘–),ğ‘–=1indicates assigning each
nodeğ‘£ğ‘–toâ„(ğ‘–)-th (â„(ğ‘–)âˆˆ{ 1,Â·Â·Â·,ğ‘˜}) cluster uniformly at random.
Each diagonal entry in ğš«is either 1orâˆ’1, which signifies that the
cluster assignment in ğš½is true or false. As such, each entry Rğ‘–,ğ‘—
represents
Rğ‘—,ğ‘–=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³1nodeğ‘£ğ‘–belongs to the ğ‘—-th cluster
âˆ’1 nodeğ‘£ğ‘–does not belong to the ğ‘—-th cluster
0otherwise.(8)
Accordingly, Aâ€²
ğ‘–,ğ‘—quantifies the strength of connections from ğ‘£ğ‘–to
theğ‘—-th cluster via its neighbors. Since ğš½is randomly generated,
distant (resp. close) nodes might fall into the same (resp. different)
clusters, resulting in a distorted distribution in Aâ€².
Optimization via RWR-Sketch. As a remedy, we propose RWR-
Sketch to create a structure-aware sketching matrix SâˆˆRğ‘˜Ã—ğ‘›.
TADA will combine Swith count sketch matrix Rto obtain the final
sketched adjacency matrix Aâ€²:
Aâ€²=AÂ·(RâŠ¤+ğ›½Â·SâŠ¤), (9)
whereğ›½is a hyper-parameter controlling the contribution of the
RWR-Sketch in the result. Unlike R, the construction of Sis framed
as clustering ğ‘›nodes inGintoğ‘˜disjoint clusters as per their topo-
logical connections to each other in G. Here, we adopt the promi-
nent random walk with restart (RWR) model [ 73,95] to summarize
the multi-hop connectivity between nodes. To be specific, we con-
struct Sas follows:
(i)We select a setCof nodes (ğ‘˜â‰¤ |C| â‰ªğ‘›) with highest
in-degrees fromVas the candidate cluster centroids.
(ii)For each node ğ‘£ğ‘–âˆˆV, we compute the RWR score ğœ‹(ğ‘£ğ‘–,ğ‘£ğ‘—)
of every node ğ‘£ğ‘—inCw.r.t.ğ‘£ğ‘–throughğ‘‡power iterations:
ğœ‹(ğ‘£ğ‘–,ğ‘£ğ‘—)=ğ‘‡âˆ‘ï¸
ğ‘¡=0(1âˆ’ğ›¼)ğ›¼ğ‘¡Pğ‘–,ğ‘—, (10)
whereğ›¼âˆˆ(0,1)is a decay factor (0 .5by default).
(iii)Denote byğœ‹(ğ‘£ğ‘—)=Ã
ğ‘£ğ‘–âˆˆVğœ‹(ğ‘£ğ‘–,ğ‘£ğ‘—)
ğ‘›the centrality (i.e., PageR-
ank [ 55]) ofğ‘£ğ‘—âˆˆC. We select a setCğ‘˜ofğ‘˜nodes fromC
with the largest centralities as the final cluster centroids.
(iv)For each node ğ‘£ğ‘–âˆˆV, we pick the node ğ‘£ğ‘—âˆˆCğ‘˜with the
largest RWR score ğœ‹(ğ‘£ğ‘–,ğ‘£ğ‘—)as its cluster centroid and set
Sğ‘—,ğ‘–=1.
(v)After that, we give Sa final touch by applying an ğ¿2normal-
ization for each row.For the interest of space, we refer interested readers to [ 39] for
the complete pseudo-code and detailed asymptotic analysis of our
hybrid sketching approach.
4.3 Topology- and Attribute-Aware Graph
Sparsification
Edge Reweighting. With the augmented initial node features
H(0)(Eq. (6)) at hand, for each edge ğ‘’ğ‘–,ğ‘—âˆˆE, we assign the cosine
similarity of the representations of its endpoints ğ‘£ğ‘–andğ‘£ğ‘—as the
weight ofğ‘’ğ‘–,ğ‘—:
ğ‘¤(ğ‘’ğ‘–,ğ‘—)=cos
H(0)
ğ‘–,H(0)
ğ‘—
. (11)
Accordingly, the â€œdegreeâ€ of node ğ‘£ğ‘–can be calculated via Eq. (12),
which is the sum of weights of edges incident to ğ‘£ğ‘–.
ğ‘‘ğ‘¤(ğ‘£ğ‘–)=âˆ‘ï¸
ğ‘£ğ‘—âˆˆN(ğ‘£ğ‘–)ğ‘¤(ğ‘’ğ‘–,ğ‘—) (12)
Denote byGğ‘¤=(V,Eğ‘¤)this edge-reweighted graph. The subse-
quent task is hence to sparsify Gğ‘¤.
In the literature, a canonical methodology [ 64] to create the
sparsified graphGâ€²is to sample edges with probability proportional
to their effective resistance (ER) [ 49] values and add them with
adjusted weights to Gâ€². Theoretically,Gâ€²is an unbiased estimation
of the original graph Gin terms of the graph Laplacian [ 64] and
requiresğ‘›ğ‘Ÿ=ğ‘‚ğ‘›log(ğ‘›/ğ›¿)
ğœ–2
samples to ensure the Laplacian matrix
Lâ€²ofGâ€²satisfies
âˆ€xâˆˆRğ‘›,ğœ–âˆˆ(0,1] ( 1âˆ’ğœ–)xâŠ¤Lxâ‰¤xâŠ¤Lâ€²xâ‰¤(1+ğœ–)xâŠ¤Lx
with a probability of at least 1âˆ’ğ›¿. First, this approach fails to
account for node attributes. Second, the computation of the ER of
all edges inGis rather costly. Even the approximate algorithms
[64,104] struggle to cope with medium-sized graphs. Besides, the
edge sampling strategy relies on a large ğ‘›ğ‘Ÿas it will repeatedly pick
the same edges.
ER Approximation on Gğ‘¤.To this end, we first conduct a rigorous
theoretical analysis in Lemma 4.2 and disclose that the ER value of
each edgeğ‘’ğ‘–,ğ‘—inGğ‘¤is roughly proportional to1
ğ‘‘ğ‘¤(ğ‘£ğ‘–)+1
ğ‘‘ğ‘¤(ğ‘£ğ‘—).
Lemma 4.2. LetGğ‘¤=(V,Eğ‘¤)be a weighted graph whose node
degrees are defined as in Eq. (12). The ERğ‘Ÿğ‘¤(ğ‘’ğ‘–,ğ‘—)of each edge ğ‘’ğ‘–,ğ‘—âˆˆ
Eğ‘¤is bounded by
1
21
ğ‘‘ğ‘¤(ğ‘£ğ‘–)+1
ğ‘‘ğ‘¤(ğ‘£ğ‘—)
â‰¤ğ‘Ÿğ‘¤(ğ‘’ğ‘–,ğ‘—)â‰¤1
1âˆ’ğœ†21
ğ‘‘ğ‘¤(ğ‘£ğ‘–)+1
ğ‘‘ğ‘¤(ğ‘£ğ‘—)
,
whereğœ†2â‰¤1stands for the second largest eigenvalue of the normal-
ized adjacency matrix of Gğ‘¤.
Proof. We defer the proof to [39]. â–¡
The above finding implies that we can leverage1
ğ‘‘ğ‘¤(ğ‘£ğ‘–)+1
ğ‘‘ğ‘¤(ğ‘£ğ‘—)
as an estimation of the ER of edge ğ‘’ğ‘–,ğ‘—onGğ‘¤, which roughly reflects
the relative importance of edges.
Edge Ranking and Sparsification of Gğ‘¤.On this basis, in lieu of
sampling edges inGğ‘¤for sparsified graph construction, we resort
 
1468Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Statistics of Datasets (K =103and M=106).
Dataset ğ‘›ğ‘šğ‘‘|
Y|ğ‘š/ğ‘›ğ»
ğ‘…
P
hoto [62] 7.7K 238.2K 745 8 31.1 0.83
WikiCS [52] 11.7K 431.7K 300 10 36.9 0.65
Re
ddit2 [101] 233K 23.2M 602 41 99.6 0.78
A
mazon2M [12] 2.45M 61.9M 100 47 25.3 0.81
Squirr
el[56] 5.2K 396.9K 2.1K 5 76.3 0.22
Penn94 [30] 41.6K 1.4M 128 2 32.8 0.47
Ogbn-Pr
oteins [30] 132.5K 39.6M 8 112 298.5 0.38
Poke
c[43] 1.6M 30.6M 65 2 18.8 0.45
to ranking edges in ascending order as per their centrality values
ğ¶ğ‘¤(ğ‘’ğ‘–,ğ‘—)defined by
ğ¶ğ‘¤(ğ‘’ğ‘–,ğ‘—)=ğ‘¤(ğ‘’ğ‘–,ğ‘—)Â·1
ğ‘‘ğ‘¤(ğ‘£ğ‘–)+1
ğ‘‘ğ‘¤(ğ‘£ğ‘—)
, (13)
which intuitively quantifies the total importance of edge ğ‘’ğ‘–,ğ‘—among
all edges incident to ğ‘£ğ‘–andğ‘£ğ‘—. Afterwards, given a sparsification
ratioğœŒ, we delete a subset Ermof edges with ğ‘šğœŒlowest centrality
values fromGğ‘¤and construct the sparsified adjacency matrix Aâ—¦
as follows:
âˆ€ğ‘’ğ‘–,ğ‘—âˆˆEğ‘¤\Erm Aâ—¦
ğ‘–,ğ‘—=ğ‘¤(ğ‘’ğ‘–,ğ‘—). (14)
The pseudo-code and complexity analysis are in [39].
5 EXPERIMENTS
5.1 Experimental Setup
Datasets. Table 2 lists the statistics of 8 benchmark HDGs ( ğ‘š/ğ‘›â‰¥
18) tested in our experiments, which are of diverse types and varied
sizes.|Y|symbolizes the distinct number of class labels of nodes
inG. The homophily ratio (HR) ofGis defined as the fraction of
homophilic edges linking same-class nodes [ 111]. We refer to a
graph with ğ»ğ‘…â‰¥0.5as homophilic and as heterophilic if ğ»ğ‘…<
0.5. Particularly, datasets Photo [62],WikiCS [52],Reddit2 [101],
andAmazon2M [12] are homophilic graphs, whereas Squirrel [56],
Penn94 [30],Ogbn-Proteins [30], and Pokec [43] are heterophilic
graphs. Amazon2M andPokec are two large HDGs with millions of
nodes and tens of millions of edges. More details of the datasets and
train/validation/test splits can be found in our technical report [ 39].
Baselines and Configurations. We adopt five popular MP-GNN
architectures, GCN [ 37], GAT [ 76], SGC [ 85], APPNP [ 24], and
GCNII [ 9] as the baselines and backbones to validate TADA in semi-
supervised node classification tasks (Section 5.2). To demonstrate
the superiority of TADA, we additionally compare TADA against
other GDA techniques in Section 5.3 its variants with other feature
expansion and graph sparsification strategies in Section 5.4. The
implementation details and hyper-parameter settings can be found
in [39].
All experiments are conducted on a Linux machine with an
NVIDIA Ampere A100 GPU (80GB RAM), AMD EPYC 7513 CPU
(2.6 GHz), and 1TB RAM. Source codes can be accessed at https:
//github.com/HKBU-LAGAS/TADA.
5.2 Semi-Supervised Node Classification
Effectiveness. In this set of experiments, we compare TADA-
augmented GCN, GAT, SGC, APPNP, and GCNII models againstGCN SGC APPNP GCNII100101102running time (ms)
(a)Reddit2GCN SGC APPNP GCNII100101102running
time (ms)
(
b)Ogbn-Proteins
Figure 2: Training time per epoch
Baseline Baseline+TADA
GCN SGC APPNP GCNII10âˆ’1100101running time (ms)
(a)Reddit2GCN SGC APPNP GCNII10âˆ’1100101102running time (ms)
(b)Ogbn-Proteins
Figure 3: Inference time per epoch
GCN SGC APPNP GCNII12345Memor
y(GB)
(
a)Reddit2GCN SGC APPNP GCNII13579Memory (GB)
(
b)Ogbn-Proteins
Figure 4: Maximum GPU Memory Usage
their vanilla versions in terms of semi-supervised node classifica-
tion. Table 3 reports their test accuracy results on 8 HDG datasets.
OOM represents that the model fails to report results due to the
out-of-memory issue. It can be observed that TADA consistently
improves the baselines in accuracy on both homophilic and het-
erophilic graphs in almost all cases. Notably, on the Squirrel dataset,
the five backbones are outperformed by their TADA counterparts
with significant margins of 17.29%-20.14%in testing accuracy. The
reason is that Squirrel is endowed with uninformative nodal at-
tributes, and by contrast, its structural features are more conducive
for node classification. By expanding original node features with
high-quality structure embeddings (Module I in TADA), TADA is
able to overcome such problems and advance the robustness and
effectiveness of GNNs. In addition, on Reddit2 andOgbn-Proteins
with average degrees ( ğ‘š/ğ‘›) over hundreds, TADA also yields pro-
nounced improvements in accuracy, i.e., 2.28%and2.94%for GCN,
as well as 1.96% and2.23% for GCNII, respectively. This demon-
strates the effectiveness of our graph sparsification method (Module
II inTADA) in reducing noisy edges and mitigating over-smoothing
issues particularly in graphs (Reddit2 andOgbn-Proteins) consisting
of a huge number of edges (analysed in Section 3.3). On the rest
HDGs, almost all GNN backbones see accuracy gains with TADA .
Two exceptions occur on heterophilic HDG Pokec, where GCN and
SGC get high standard deviations (1 .36%and5.56%) in accuracy
while GCN+TADA and SGC+TADA attenuate average accuracies
but increase their performance stability.
Efficiency. To assess the effectiveness of TADA in the reduction of
GNNsâ€™ feature aggregation overhead on HDGs, Figures 2, 3, and 4
 
1469KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yurui Lai et al.
Table 3: Node classification results (% test accuracy) of different GNN backbones with and without TADA on homophilic and
heterophilic graphs. We conduct 10 trials and report mean accuracy and standard deviation over the trials.
Method Photo WikiCS Reddit2 Amazon2M Squirrel Penn94 Ogbn-Proteins Pokec
GCN 94.63Â±0.15 84.05Â±0.76 92.58Â±0.03 74.12Â±0.19 54.85Â±2.02 75.9Â±0.74 69.75Â±0.6 75.47Â±1.36
GCN + TADA 94.92Â±0.45 84.62Â±0.53 94.86Â±0.22 76.14Â±0.23 73.48Â±1.61 76.06Â±0.43 73.79Â±0.76 75.01Â±0.27
GAT 93.84Â±0.46 83.74Â±0.75 OOM OOM 55.70Â±3.26 71.09Â±1.35 OOM 73.20Â±7.02
GAT + TADA 94.58Â±0.12 84.97Â±0.84 95.97Â±0.04 59.16Â±0.36 72.99Â±2.81 71.19Â±0.78 74.94Â±0.25 74.26Â±0.94
SGC 93.29Â±0.79 83.47Â±0.83 94.78Â±0.02 59.86Â±0.04 52.18Â±1.49 56.77Â±0.14 70.33Â±0.04 67.40Â±5.56
SGC + TADA 94.93Â±0.39 83.97Â±0.71 95.65Â±0.02 73.39Â±0.35 72.32Â±2.72 71.02Â±0.53 74.31Â±0.42 62.06Â±0.52
APPNP 94.95Â±0.33 85.04Â±0.60 90.86Â±0.19 65.51Â±0.36 54.47Â±2.06 69.25Â±0.38 75.19Â±0.58 62.79Â±0.11
APPNP + TADA 95.42Â±0.53 85.19Â±0.56 95.34Â±0.18 69.81Â±0.24 73.24Â±1.38 71.08Â±0.62 75.52Â±0.32 67.03Â±0.27
GCNII 95.12Â±0.12 85.13Â±0.56 94.66Â±0.07 OOM 53.13Â±4.29 74.97Â±0.35 73.11Â±1.93 76.49Â±0.88
GCNII + TADA 95.54Â±0.44 85.42Â±0.60 96.62Â±0.08 77.83Â±0.62 72.89Â±2.45 75.84Â±3.13 75.34Â±1.33 77.64Â±0.32
Table 4: Comparison with GDA Baselines.
Metho
dRe
ddit2 Ogbn-Pr
oteins
A
cc (%) Trng. / Inf. (ms) A
cc (%) Trng. / Inf. (ms)
GCN 92.58Â±0.03 53.4
/ 0.31 69.75Â±0.6 210.59
/ 94.01
GCN+DropEdge 93.59Â±0.05 49.51
/ 0.31 61.46Â±3.33 62.65
/ 93.53
GCN+GraphMix 92.60Â±0.07 128.58
/ 0.38 72.41Â±1.34 441.23/93.95
GCN+T
ADA 94.86Â±0.22 24
/ 0.44 73.79Â±0.76 52.26
/ 0.78
GCNII 94.66Â±0.07 125.6
/ 0.66 73.11Â±1.93 211.31
/ 95.52
GCNII+DropEdge 96.23Â±0.05 72.39
/ 0.66 60.50Â±5.42 67.45
/ 95.39
GCNII+GraphMix 96.19Â±0.05 172.66/0.72 63.75Â±1.72 456.59
/ 95.34
GCNII+TADA 96.62Â±0.08 49.5
/ 0.72 75.34Â±1.33 42.68
/ 1.11
*Best is bolded and runner-up underlined .
plot the training times and inferencetimes per epoch (in millisec-
onds), as well as the maximum memory footprints (in GBs) needed
by four GNN backbones (GCN, SGC, APPNP, and GCNII) and their
TADA counterparts on a heterophilic HDG Ogbn-Proteins and a
homophilic HDG Reddit2. We exclude GAT as it incurs OOM errors
on these two datasets, as shown in Table 3. From Figure 3, we note
that on Ogbn-Proteins, TADA is able to speed up the inferences
of GCN, APPNP, and GCNII to 121.7Ã—,198.2Ã—, and 86Ã—faster, re-
spectively, whereas on Reddit2 TADA achieves comparable runtime
performance to the vanilla GNN models. This reveals that Reddit2
andOgbn-Proteins contains substantial noisy or redundant edges
that can be removed without diluting the results of GNNs if TADA
is included. Apart from the inference, TADA can also slightly ex-
pedite the training in the presence of Module I and Module II (see
Figure 2), indicating the high efficiency of our techniques developed
inTADA . In addition to the superiority in computational time, it
can be observed from Figure 4 that TADA leads to at least a 24%and
16% reduction in memory consumption compared to the vanilla
GNN models.
In a nutshell, TADA successfully addresses the technical chal-
lenges of GNNs on HDGs as remarked in Section 3.3. Besides, we
refer interested readers to [ 39] for the empirical studies of TADA
on low-degree graphs.
5.3 Comparison with GDA Baselines
This set of experiments evaluates the effectiveness TADA in improv-
ing GNNsâ€™ performance against other popular GDA techniques:
DropEdge [ 60] and GraphMix [ 79]. Table 4 presents the test accu-
racy results, training and inference times per epoch (in milliseconds)
achieved by two GNN backbones GCN and GCNII and their aug-
mented versions on Ogbn-Proteins andReddit2. We can make thefollowing observations. First, TADA +GCN and TADA +GCNII dom-
inate all their competitors on the two datasets, respectively, in terms
of classification accuracy as well as training and inference efficiency.
OnOgbn-Proteins, we can see that the classification performance of
GCN+DropEdge, GCNII+DropEdge, and GCNII+GraphMix is even
inferior to the baselines, while taking longer training and inference
times, which is consistent with our analysis of the limitations of
existing GDA methods on HDGs in Sections 1 and 2.
5.4 Ablation Study
Table 5 presents the ablation study of TADA with GCN as the
backbone model on Reddit2 andOgbn-Proteins. More specifically,
we conduct the ablation study in three dimensions. Firstly, we
start with the vanilla GCN and incrementally apply components
Count-Sketch, RWR-Sketch (Module I), and our graph sparsification
technique (Module II) to the GCN. Notice that Module II is built
on the output of Module I, and, thus, can only be applied after it.
From Table 5, we can observe that each component in TADA yields
notable performance gains in node classification on the basis of the
prior one, which exhibits the non-triviality of the modules to the
effectiveness of TADA.
On the other hand, to demonstrate the superiority of our hy-
brid sketching approach introduced in Section 4.2, we substitute
Count-Sketch and RWR-Sketch in Module I with random projec-
tion [ 42],ğ‘˜-SVD [ 28], DeepWalk [ 57], node2vec [ 27], and LINE [ 71],
respectively, while fixing Module II. That is, we employ the random
projections of adjacency matrix A, the top-ğ‘˜singular vectors (as in
[69]), or the node embeddings output by DeepWalk, node2vec, and
LINE as Aâ€²for the generation of structure embeddings. As reported
in Table 5, all these five approaches obtain inferior classification
results compared to TADA with Count-Sketch + RWR-Sketch on
Reddit2 andOgbn-Proteins.
Finally, we empirically study the effectiveness of our topology-
and attribute-aware sparsification method in Section 4.3 (Mod-
ule II) by replacing it with random sparsification(RS), ğ‘˜-Neighbor
Spar [ 61], SCAN [ 91] and the DSpar [ 48]. Random sparsification
removes edges randomly, and ğ‘˜-Neighbor Spar [ 61] samples at
mostğ‘˜edges for each neighbor. SCAN removes the edges with
the lowest modified Jaccard similarity, while Dspar identifies the
subset of dropped edges based on their estimated ER values in the
original unweighted graph. Table 5 shows that all these four vari-
ants are outperformed by TADA by a large margin. On Reddit2 and
 
1470Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 5: Ablation Study
Method Reddit2 Obgn-Proteins
GCN 92.58Â±0.03 69.75Â±0.60
+ Count-Sketch 93.81Â±0.88 72.90Â±2.14
+ RWR-Sketch 94.25Â±0.66 70.33Â±2.54
+ Module II (i.e., GCN+TADA) 94.86Â±0.22 73.79Â±0.76
Random Projection (Module I) 93.99Â±0.74 72.26Â±0.77
ğ‘˜-SVD (Module I) 93.36Â±0.51 69.79Â±1.37
DeepWalk (Module I) 94.48Â±0.34 72.56Â±0.94
node2vec (Module I) 94.47Â±0.41 73.05Â±1.50
LINE (Module I) 94.49Â±0.34 72.49Â±1.66
RS (Module II) 91.04Â±0.04 72.54Â±1.11
ğ‘˜-Neighbor Spar (Module II) 93.97Â±0.7 72.90Â±1.47
SCAN (Module II) 89.93Â±0.78 71.85Â±1.25
DSpar (Module II) 93.58Â±0.08 72.75Â±1.11
*Best is bolded and runner-up underlined .
Re
ddit2 Ogbn-Pr
oteins
416 64128 2560.70.750.80.850.90.95Accuracy
(a) Varyingğ‘˜0.00.10.30.50.70.91.00.650.70.750.80.850.90.95Accuracy
(b) Varyingğ›¾
0.00.10.30.50.70.91.00.70.750.80.850.90.95Accuracy
(c) Varyingğ›½0.10.30.50.70.90.990.70.750.80.850.90.95Accuracy
(d) VaryingğœŒ
Figure 5: Hyper-parameter Analysis.
Ogbn-Proteins, TADA takes a lead of 0.89%in classification accuracy
compared to its best variant with ğ‘˜-Neighbor Spar.
5.5 Hyper-parameter Analysis
This section empirically studies the sensitivity of TADA to hyper-
parameters including the weight for structure embeddings ğ›¾(Eq.(6)),
structure embedding dimension ğ‘˜(Section 4.2), RWR-Sketch weight
ğ›½(Eq.(9)), and sparsification ratio ğœŒ(Section 4.3), on two datasets
Ogbn-Proteins andReddit2.
Figure 5(a) depict the node classification accuracy results of
GCN+TADA when varying ğ‘˜in{4,16,64,128,256}. We can make
analogous observations on Reddit2 andOgbn-Proteins. That is, the
performance of GCN+TADA first improves as ğ‘˜is increased from 4
to128(more structure features are captured) and then undergoes a
decline when ğ‘˜=256, as a consequence of over-fitting.
In Figure 5(b), we plot the node classification accuracy values
attained by GCN+TADA whenğ›¾is varied from 0to1.0. Note that
whenğ›¾=0(resp.ğ›¾=1.0), the initial node features H(0)defined
in Eq. (6)will not embody structure features Htopo (resp. node
(a)GCN
 (b)GCN+TADA
Figure 6: The final node representations of Photo obtained
by GCN and GCN+TADA. Nodes are colored by their labels.
attributes Hattr). It can be observed that GCN+TADA obtains im-
proved classification results on Reddit2 when varying ğ›¾from 0to
0.9, whereas its performance on Ogbn-Proteins constantly down-
grades asğ›¾enlarges. The degradation is caused by its heterophilic
property and using its topological features for graph sprasification
(Section 4.3) will accidentally remove critical connections.
From Figure 5(c), we can see that the best performance is achieved
whenğ›½=0.1andğ›½=0.3onReddit2 andOgbn-Proteins, respectively,
which validates the superiority of our hybrid sketching approach
in Section 4.2 over Count-Sketch or RWR-Sketch solely.
As displayed in Figure 5(d), on Reddit2, we can observe that
GCN+TADA experiences an uptick in classification accuracy when
excluding 10%-70% edges fromGusing Module II in TADA, followed
by a sharp downturn when ğœŒ>70%. On Ogbn-Proteins, the best
result is attained when ğœŒ=0.9, i.e., 90%edges are removed from
G. The results showcase that Module II can accurately identify up
to 70%-90% edges from Gthat are noisy or redundant and obtain
performance enhancements.
5.6 Visualization of TADA
Figure 6 visualizes (using t-SNE [ 75]) the node representations of
thePhoto dataset at the final layers of GCN and GCN+TADA. Nodes
with the same ground-truth labels will be in the same colors. In
Figure 6(b), we can easily identify 8 classes of nodes as nodes with
disparate colors (i.e., labels) are all far apart from each other. In
comparison, in Figure 6(a), three groups of nodes with different
colors are adjacent to each other with partial overlapping and some
nodes even are positioned in other groups and distant from their
true classes. These observations demonstrate that TADA can en-
hance the quality of nodes representations learned by GCN, and
thus, yield the higher classification accuracy, as reported in Table 3.
6 CONCLUSION
In this paper, we present TADA, an efficient and effective data aug-
mentation approach specially catered for GNNs on HDGs. TADA
achieves high result utility through two main technical contri-
butions: feature expansion with structure embeddings via hybrid
sketching, and topology- and attribute-aware graph sparsification.
Considerable experiments on 8 homophilic and heterophilic HDGs
have verified that TADA is able to consistently promote the per-
formance of popular MP-GNNs, e.g., GCN, GAT, SGC, APPNP, and
GCNII, with matching or even upgraded training and inference
efficiency.
 
1471KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yurui Lai et al.
ACKNOWLEDGMENTS
Renchi Yang is supported by the NSFC YSF grant (No. 62302414)
and Hong Kong RGC ECS grant (No. 22202623).
REFERENCES
[1]Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. 2018.
Watch your step: Learning node embeddings via graph attention. NeurIPS 31
(2018).
[2]Michael Adjeisah, Xinzhong Zhu, Huiying Xu, and Tewodros Alemu Ayall.
2023. Towards data augmentation in graph neural network: An overview and
evaluation. Computer Science Review 47 (2023), 100527.
[3]Joshua D Batson, Daniel A Spielman, and Nikhil Srivastava. 2009. Twice-
ramanujan sparsifiers. In STOC. 255â€“262.
[4]Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. 2021.
Graph neural networks with convolutional arma filters. TPAMI 44, 7 (2021),
3496â€“3507.
[5]Chen Cai and Yusu Wang. 2020. A note on over-smoothing for graph neural
networks. arXiv preprint arXiv:2006.13318 (2020).
[6]Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. 2018. A com-
prehensive survey of graph embedding: Problems, techniques, and applications.
TDKE 30, 9 (2018), 1616â€“1637.
[7]Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural networks for
learning graph representations. In AAAI, Vol. 30.
[8]Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring
and relieving the over-smoothing problem for graph neural networks from the
topological view. In AAAI, Vol. 34. 3438â€“3445.
[9]Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. 2020.
Simple and deep graph convolutional networks. In ICML. 1725â€“1735.
[10] Yingmei Chen, Zhongyu Wei, and Xuanjing Huang. 2018. Incorporating cor-
poration relationship via graph convolutional neural networks for stock price
prediction. In CIKM. 1655â€“1658.
[11] Yuhan Chen, Haojie Ye, Sanketh Vedula, Alex Bronstein, Ronald Dreslinski,
Trevor Mudge, and Nishil Talati. 2023. Demystifying Graph Sparsification
Algorithms in Graph Properties Preservation. PVLDB 17, 3 (2023), 427â€“440.
[12] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.
2019. Cluster-gcn: An efficient algorithm for training deep and large graph
convolutional networks. In SIGKDD. 257â€“266.
[13] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2020. Adaptive Universal
Generalized PageRank Graph Neural Network. In ICLR.
[14] Fan RK Chung. 1997. Spectral graph theory. Vol. 92. American Mathematical
Soc.
[15] Kenneth L Clarkson and David P Woodruff. 2013. Low rank approximation and
regression in input sparsity time. In STOC. 81â€“90.
[16] MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-
tional neural networks on graphs with fast localized spectral filtering. NeurIPS
29 (2016).
[17] Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester,
Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Brett Wiltshire, et al.
2021. Eta prediction with graph neural networks in google maps. In CIKM.
3767â€“3776.
[18] Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. 2022. Data augmentation
for deep graph learning: A survey. SIGKDD Explorations Newsletter 24, 2 (2022),
61â€“77.
[19] Taoran Fang, Zhiqing Xiao, Chunping Wang, Jiarong Xu, Xuan Yang, and Yang
Yang. 2023. Dropmessage: Unifying random dropping for graph neural networks.
InAAAI, Vol. 37. 4267â€“4275.
[20] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang
Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph random neural networks
for semi-supervised learning on graphs. NeurIPS 33 (2020), 22092â€“22103.
[21] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. 2017. Protein inter-
face prediction using graph convolutional networks. In Proceedings of the 31st
International Conference on Neural Information Processing Systems. 6533â€“6542.
[22] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. 2019.
Learning discrete structures for graph neural networks. In ICML. 1972â€“1982.
[23] Wai Shing Fung, Ramesh Hariharan, Nicholas JA Harvey, and Debmalya Pani-
grahi. 2011. A general framework for graph sparsification. In STOC. 71â€“80.
[24] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2018.
Predict then Propagate: Graph Neural Networks meet Personalized PageRank.
InICLR.
[25] Johannes Gasteiger, Stefan WeiÃŸenberger, and Stephan GÃ¼nnemann. 2019. Dif-
fusion improves graph learning. NeurIPS 32 (2019).
[26] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In ICML. 1263â€“1272.
[27] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In SIGKDD. 855â€“864.[28] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding struc-
ture with randomness: Probabilistic algorithms for constructing approximate
matrix decompositions. SIAM review 53, 2 (2011), 217â€“288.
[29] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. 2022. G-mixup: Graph
data augmentation for graph classification. In ICML. 8230â€“8248.
[30] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets
for machine learning on graphs. NeurIPS 33 (2020), 22118â€“22133.
[31] Keke Huang, Jing Tang, Juncheng Liu, Renchi Yang, and Xiaokui Xiao. 2023.
Node-wise diffusion for scalable graph learning. In TheWebConf. 1723â€“1733.
[32] Weiwei Jiang and Jiayun Luo. 2022. Graph neural network for traffic forecasting:
A survey. ESA 207 (2022), 117921.
[33] Hongwei Jin and Xinhua Zhang. 2019. Latent adversarial training of graph
convolution networks. In ICML workshop, Vol. 2.
[34] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang.
2020. Graph structure learning for robust graph neural networks. In SIGKDD.
66â€“74.
[35] Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, and Neil Shah.
2022. Empowering Graph Representation Learning with Test-Time Graph
Transformation. In The Eleventh ICLR.
[36] David R Karger. 1994. Random sampling in cut, flow, and network design
problems. In STOC. 648â€“657.
[37] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[38] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem,
Gavin Taylor, and Tom Goldstein. 2020. FLAG: Adversarial Data Augmentation
for Graph Neural Networks. (2020).
[39] Yurui Lai, Xiaoyang Lin, Renchi Yang, and Hongtao Wang. 2024. Efficient
Topology-aware Data Augmentation for High-Degree Graph Neural Networks.
arXiv:2406.05482 [cs.LG]
[40] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger,
Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen,
Weihua Hu, et al .2022. GraphCast: Learning skillful medium-range global
weather forecasting. arXiv preprint arXiv:2212.12794 (2022).
[41] Jiayu Li, Tianyun Zhang, Hao Tian, Shengmin Jin, Makan Fardad, and Reza
Zafarani. 2020. Sgcn: A graph sparsifier based on graph convolutional networks.
InProceedings of the Pacific-Asia Conference in Advances in Knowledge Discovery
and Data Mining. 275â€“287.
[42] Ping Li, Trevor J Hastie, and Kenneth W Church. 2006. Very sparse random
projections. In SIGKDD. 287â€“296.
[43] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar
Bhalerao, and Ser Nam Lim. 2021. Large scale learning on non-homophilous
graphs: New benchmarks and strong simple methods. NeurIPS 34 (2021), 20887â€“
20902.
[44] Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. 2022. Graph
rationalization with environment-based augmentations. In SIGKDD. 1069â€“1078.
[45] Meng Liu, Hongyang Gao, and Shuiwang Ji. 2020. Towards deeper graph neural
networks. In SIGKDD. 338â€“348.
[46] Songtao Liu, Rex Ying, Hanze Dong, Lanqing Li, Tingyang Xu, Yu Rong, Peilin
Zhao, Junzhou Huang, and Dinghao Wu. 2022. Local augmentation for graph
neural networks. In ICML. 14054â€“14072.
[47] Xin Liu, Mingyu Yan, Lei Deng, Guoqi Li, Xiaochun Ye, Dongrui Fan, Shirui
Pan, and Yuan Xie. 2022. Survey on graph neural network acceleration: An
algorithmic perspective. arXiv preprint arXiv:2202.04822 (2022).
[48] Zirui Liu, Kaixiong Zhou, Zhimeng Jiang, Li Li, Rui Chen, Soo-Hyun Choi, and
Xia Hu. 2023. DSpar: An Embarrassingly Simple Strategy for Efficient GNN
training and inference via Degree-based Sparsification. TMLR (2023).
[49] LÃ¡szlÃ³ LovÃ¡sz. 1993. Random walks on graphs. Combinatorics, Paul erdos is
eighty 2, 1-46 (1993), 4.
[50] Youzhi Luo, Michael Curtis McThrow, Wing Yee Au, Tao Komikado, Kanji
Uchino, Koji Maruhashi, and Shuiwang Ji. 2022. Automated Data Augmentations
for Graph Classification. In ICLR.
[51] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol,
Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for mate-
rials discovery. Nature (2023), 1â€“6.
[52] PÃ©ter Mernyei and CÄƒtÄƒlina Cangea. 2020. Wiki-CS: A Wikipedia-Based Bench-
mark for Graph Neural Networks. arXiv preprint arXiv:2007.02901 (2020).
[53] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient esti-
mation of word representations in vector space. arXiv preprint arXiv:1301.3781
(2013).
[54] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asym-
metric transitivity preserving graph embedding. In SIGKDD. 1105â€“1114.
[55] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The
pagerank citation ranking: Bring order to the web. Technical Report. Technical
report, stanford University.
[56] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang.
2019. Geom-GCN: Geometric Graph Convolutional Networks. In ICLR.
[57] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online
learning of social representations. In SIGKDD. 701â€“710.
 
1472Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[58] Ninh Dang Pham. 2014. On the Power of Randomization in Big Data Analytics.
(2014).
[59] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018.
Network embedding as matrix factorization: Unifying deepwalk, line, pte, and
node2vec. In WSDM. 459â€“467.
[60] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. DropEdge:
Towards Deep Graph Convolutional Networks on Node Classification. In ICLR.
[61] Veeru Sadhanala, Yu-Xiang Wang, and Ryan Tibshirani. 2016. Graph sparsifica-
tion approaches for laplacian smoothing. In Artificial Intelligence and Statistics.
PMLR, 1250â€“1259.
[62] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
GÃ¼nnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[63] Rui Song, Fausto Giunchiglia, Ke Zhao, and Hao Xu. 2021. Topological regular-
ization for graph neural networks augmentation. arXiv preprint arXiv:2104.02478
(2021).
[64] Daniel A Spielman and Nikhil Srivastava. 2008. Graph sparsification by effective
resistances. In STOC. 563â€“568.
[65] Daniel A Spielman and Shang-Hua Teng. 2004. Nearly-linear time algorithms
for graph partitioning, graph sparsification, and solving linear systems. In STOC.
81â€“90.
[66] Rakshith S Srinivasa, Cao Xiao, Lucas Glass, Justin Romberg, and Jimeng Sun.
2020. Fast graph attention networks using effective resistance based graph
sparsification. arXiv preprint arXiv:2006.08796 (2020).
[67] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929â€“1958.
[68] Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-
Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae,
Zohar Bloom-Ackermann, et al .2020. A deep learning approach to antibiotic
discovery. Cell180, 4 (2020), 688â€“702.
[69] Jiaqi Sun, Lin Zhang, Guangyi Chen, Peng Xu, Kun Zhang, and Yujiu Yang. 2023.
Feature Expansion for Graph Neural Networks. In ICML. 33156â€“33176.
[70] Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. 2021. MoCL:
Data-driven Molecular Fingerprint via Knowledge-aware Contrastive Learning
from Molecular Graph. SIGKDD (2021).
[71] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. Line: Large-scale information network embedding. In Proceedings of the
24th international conference on world wide web. 1067â€“1077.
[72] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Az-
abou, Eva L Dyer, Remi Munos, Petar VeliÄkoviÄ‡, and Michal Valko. 2021. Large-
Scale Representation Learning on Graphs via Bootstrapping. In ICLR.
[73] Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walk
with restart and its applications. In ICDM. 613â€“622.
[74] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel MÃ¼ller. 2018.
Verse: Versatile graph embeddings from similarity measures. In TheWebConf.
539â€“548.
[75] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using
t-SNE. JMLR 9, 11 (2008).
[76] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.
[77] Ameya Velingker, Ali Kemal Sinop, Ira Ktena, Petar Velickovic, and Sreenivas
Gollapudi. 2023. Affinity-Aware Graph Networks. NeurIPS (2023).
[78] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,
David Lopez-Paz, and Yoshua Bengio. 2019. Manifold mixup: Better representa-
tions by interpolating hidden states. In ICML. 6438â€“6447.
[79] Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho
Kannala, and Jian Tang. 2021. Graphmix: Improved training of gnns for semi-
supervised learning. In AAAI, Vol. 35. 10024â€“10032.
[80] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network
embedding. In SIGKDD. 1225â€“1234.
[81] Hanzhi Wang, Mingguo He, Zhewei Wei, Sibo Wang, Ye Yuan, Xiaoyong Du, and
Ji-Rong Wen. 2021. Approximate graph propagation. In SIGKDD. 1686â€“1696.
[82] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2020.
Graphcrop: Subgraph cropping for graph classification. arXiv preprint
arXiv:2009.10564 (2020).
[83] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2021. Mixup
for node and graph classification. In TheWebConf. 3663â€“3674.
[84] Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and
Jingren Zhou. 2022. Federatedscope-gnn: Towards a unified, comprehensive
and efficient package for federated graph learning. In SIGKDD. 4110â€“4120.
[85] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In ICML. 6861â€“
6871.[86] Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, and Xiaojie Guo. 2022. Graph neural
networks: foundation, frontiers and applications. In SIGKDD. 4840â€“4841.
[87] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. 2021.
Discovering Invariant Rationales for Graph Neural Networks. In ICLR.
[88] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,
and Xue Lin. 2019. Topology attack and defense for graph neural networks: an
optimization perspective. In IJCAI. 3961â€“3967.
[89] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How Powerful
are Graph Neural Networks?. In ICLR.
[90] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In ICML. 5453â€“5462.
[91] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and Thomas AJ Schweiger. 2007.
Scan: a structural clustering algorithm for networks. In Proceedings of the 13th
ACM SIGKDD international conference on Knowledge discovery and data mining .
824â€“833.
[92] Cheng Yang, Deyu Bo, Jixi Liu, Yufei Peng, Boyu Chen, Haoran Dai, Ao Sun,
Yue Yu, Yixin Xiao, Qi Zhang, et al .2023. Data-centric graph learning: A survey.
arXiv preprint arXiv:2310.04987 (2023).
[93] Renchi Yang, Jieming Shi, Keke Huang, and Xiaokui Xiao. 2022. Scalable and
effective bipartite network embedding. In SIGMOD. 1977â€“1991.
[94] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S Bhowmick.
2020. Homogeneous network embedding for massive graphs via reweighted
personalized PageRank. PVLDB 13, 5 (2020), 670â€“683.
[95] Renchi Yang, Jieming Shi, Yin Yang, Keke Huang, Shiqi Zhang, and Xiaokui
Xiao. 2021. Effective and scalable clustering on massive attributed graphs. In
TheWebConf. 3675â€“3687.
[96] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,
Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly for
graph representation? NeurIPS 34 (2021), 28877â€“28888.
[97] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In SIGKDD. 974â€“983.
[98] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph
contrastive learning automated. In ICML. 12121â€“12132.
[99] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. NeurIPS 33
(2020), 5812â€“5823.
[100] Wenchao Yu, Cheng Zheng, Wei Cheng, Charu C Aggarwal, Dongjin Song, Bo
Zong, Haifeng Chen, and Wei Wang. 2018. Learning deep network representa-
tions with adversarially regularized autoencoders. In SIGKDD. 2663â€“2671.
[101] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and
Viktor Prasanna. 2019. Graphsaint: Graph sampling based inductive learning
method. arXiv preprint arXiv:1907.04931 (2019).
[102] Ge Zhang, Zhao Li, Jiaming Huang, Jia Wu, Chuan Zhou, Jian Yang, and Jianliang
Gao. 2022. efraudcom: An e-commerce fraud detection system via competitive
graph neural networks. TOIS 40, 3 (2022), 1â€“29.
[103] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018.
mixup: Beyond Empirical Risk Minimization. In ICLR.
[104] Shiqi Zhang, Renchi Yang, Jing Tang, Xiaokui Xiao, and Bo Tang. 2023. Efficient
Approximation Algorithms for Spanning Centrality. In SIGKDD. 3386â€“3395.
[105] Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, and Wenwu Zhu.
2018. Arbitrary-order proximity preserved network embedding. In SIGKDD.
2778â€“2786.
[106] Tong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan GÃ¼nnemann,
Neil Shah, and Meng Jiang. 2022. Graph data augmentation for graph machine
learning: A survey. arXiv preprint arXiv:2202.08871 (2022).
[107] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil
Shah. 2021. Data augmentation for graph neural networks. In AAAI, Vol. 35.
11015â€“11023.
[108] Tong Zhao, Xianfeng Tang, Danqing Zhang, Haoming Jiang, Nikhil Rao, Yiwei
Song, Pallav Agrawal, Karthik Subbian, Bing Yin, and Meng Jiang. 2022. Autogda:
Automated graph data augmentation for node classification. In LoG. 32â€“1.
[109] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu,
Haifeng Chen, and Wei Wang. 2020. Robust graph representation learning via
neural sparsification. In ICML. 11458â€“11468.
[110] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed,
and Danai Koutra. 2021. Graph neural networks with heterophily. In AAAI,
Vol. 35. 11168â€“11176.
[111] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
Koutra. 2020. Beyond homophily in graph neural networks: Current limitations
and effective designs. NeurIPS 33 (2020), 7793â€“7804.
[112] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. 2021. Interpret-
ing and unifying graph neural networks with an optimization framework. In
TheWebConf. 1215â€“1226.
 
1473