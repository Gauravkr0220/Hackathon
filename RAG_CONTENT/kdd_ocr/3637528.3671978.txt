Expander Hierarchies for Normalized Cuts on Graphs
Kathrin Hanauer
kathrin.hanauer@univie.ac.at
University of Vienna
Faculty of Computer Science
Vienna, AustriaMonika Henzinger
monika.henzinger@ist.ac.at
Institute of Science and Technology
Austria (ISTA)
Klosterneuburg, AustriaRobin MÃ¼nk
robin.muenk@tum.de
Technical University of Munich
Munich, Germany
Harald RÃ¤cke
raecke@in.tum.de
Technical University of Munich
Munich, GermanyMaximilian VÃ¶tsch
maximilian.voetsch@univie.ac.at
University of Vienna
Faculty of Computer Science
UniVie Doctoral School
Computer Science DoCS
Vienna, Austria
ABSTRACT
Expander decompositions of graphs have significantly advanced
the understanding of many classical graph problems and led to
numerous fundamental theoretical results. However, their adoption
in practice has been hindered due to their inherent intricacies and
large hidden factors in their asymptotic running times. Here, we
introduce the first practically efficient algorithm for computing
expander decompositions and their hierarchies and demonstrate its
effectiveness and utility by incorporating it as the core component
in a novel solver for the normalized cut graph clustering objective.
Our extensive experiments on a variety of large graphs show
that our expander-based algorithm outperforms state-of-the-art
solvers for normalized cut with respect to solution quality by a
large margin on a variety of graph classes such as citation, e-mail,
and social networks or web graphs while remaining competitive in
running time.
CCS CONCEPTS
â€¢Information systems â†’Clustering; â€¢Theory of computa-
tionâ†’Sparsification and spanners.
KEYWORDS
normalized cut, expander decomposition, expander hierarchy, graph
partitioning, graph clustering
ACM Reference Format:
Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Max-
imilian VÃ¶tsch. 2024. Expander Hierarchies for Normalized Cuts on Graphs.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671978
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36719781 INTRODUCTION
In recent years, expander decompositions and expander hierarchies
have emerged as fundamental tools within the theory community
for developing fast approximation algorithms and fast dynamic
algorithms for such diverse problems as, e.g., routing [ 12], connec-
tivity [ 11], (approximate) maximum flow [ 6,18], or triangle enu-
meration [ 5]. Informally, an expander is a well-connected graph.
Itsexpansion orconductance ğœ™=minğ‘†âŠ†ğ‘‰border(ğ‘†)
min(vol(ğ‘†),vol(Â¯ğ‘†))is the
minimum ratio between the number of edges leaving any subset of
vertices and the number of edges incident to vertices of the subset.
A large value of ğœ™(close to 1) indicates a graph in which no subset
of vertices can be easily disconnected from the rest of the graph.
Anexpander decomposition of a graph partitions the vertices
such that the subgraph induced by each part is an expander and
the number of edges between different components of the parti-
tion is low.1Many results over the years [ 22,23,32,34,37] have
demonstrated that such a decomposition not only exists for every
graph, but can be computed in near-linear time. At a high level,
the success of expander decomposition-based algorithms is due
to the fact that many problems are â€œeasyâ€ on expanders: One first
identifies regions of a graph where a problem is easy to solve (the
expanders), solves the problem (or a suitable sub-problem) within
each region, and then combines the individual solutions to obtain a
solution for the overall problem.
Expander hierarchies [11] apply expander decompositions in a
hierarchical manner. This is done by computing an expander decom-
position of the graph and then contracting the individual expanders
into single vertices repeatedly. This process is repeated until the
entire graph has been contracted to a single vertex. This recursive
decomposition has a corresponding decomposition tree ğ‘‡(the so-
called (flow) sparsifier ) where the root corresponds to the entire
graph, the leaves correspond to vertices of the original graph, and
inner vertices correspond to the expanders found during the de-
composition procedure. The sparsifier approximately preserves the
cut-flow structure of the original graph in a rigorous sense. This
relationship has been exploited in numerous applications in static
1This can be seen as a special form of a graph clustering.
 
1016
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Maximilian VÃ¶tsch
and dynamic graph problems [ 12,16,19] to obtain fast algorithms
with provable guarantees.
While many algorithms for expander decomposition that offer
strong asymptotic bounds on the running time have been sug-
gested [ 22,23,32,34,37],2practical algorithms based on expander
decomposition have not seen success thus far. This is due to these
algorithms requiring one to solve many maximum flow problems,
which means these algorithms are prohibitively slow in practice for
graphs with many edges. Arvestad [ 1], e.g., reports that decompos-
ing a graph with approximately 105edges can take almost 5min
for various values of ğœ™in their implementation of [32].
In practice, the multilevel graph partitioning framework has been
the de-facto approach for computing high quality solutions for
cut problems on graphs. This framework consists of a coarsening
step, in which a smaller representation of the graph is computed, a
solving step, where a solution is computed on the coarse graph and
arefinement step, during which the solution is improved using local
methods during the graph uncoarsening process. Multilevel graph
partitioning has been successfully applied for computing balanced
cuts [2, 17, 27, 31] and normalized cuts in graphs [10].
We note that algorithms based on the expander hierarchy ap-
proach may also be regarded as a variant of multilevel graph par-
titioning. In this approach, a graph is first coarsened by recursive
contractions to obtain a smaller graph that reflects the same basic
cluster structure as the input graph. Then an initial partition is com-
puted on the sparsifier and, afterwards, in a series of refinement
steps the solution is mapped to the input graph while improving it
locally as we undo the coarsening. Despite the theoretical guaran-
tees brought forward by expander decompositions and expander
hierarchies, the latter have not yet led to similar breakthroughs in
the field of fast, high-quality experimental algorithms. The reason is
that the expander decomposition required at each coarsening step
becomes a significant performance bottleneck in practice. In con-
trast, multilevel graph partitioners like METIS [17] and KaHiP [31]
use a fast matching approach here.
In this work we mitigate the computational bottleneck by intro-
ducing a novel, practically efficient random-walk-based algorithm
for expander decomposition. Based on this, we give the first im-
plementation of the expander hierarchy and, thus, an algorithm
to compute tree flow sparsifiers, allowing us to solve various cut
problems on graphs effectively. Exemplarily, we show that our
approach is eminently suitable to compute normalized ğ‘˜-cuts on
graphs, where the goal is to partition the vertex set into ğ‘˜clusters
such that the sum over the number of edges leaving each cluster,
normalized by the clusterâ€™s volume, is minimized. Normalized cut
is a popular graph clustering objective and particularly able to cap-
ture imbalanced clusterings. Its manifold applications include, e.g.,
community detection [ 20] and mining [ 4], topic reconstruction [ 3],
story segmentation [ 40], bioinformatics [ 10,38], tumor localiza-
tion [ 30], and image segmentation [ 33,36]. It is closely related to
spectral clustering, which uses spectral properties of eigenvalues
and eigenvectors of the graphâ€™s Laplacian. However, the spectral
approach suffers both from very large running times and memory
requirements to compute and store the eigenvectors, and thus does
2Including the near-linear-time algorithm by Saranurak and Wang [ 32], which
finds a decomposition where each component has expansion at least ğœ™in time
O(ğ‘šğœ™âˆ’1log4ğ‘š).not scale well [ 10]. This problem was addressed by Dhillon et al. [ 10]
as well as Zhao et al. [ 41], who presented algorithms for normalized
cut that either use spectral methods only after coarsening [ 10] or
apply them on a spectrally sparsified graph [41].
Contributions. We present the first practically efficient algorithm
for expander decomposition and the first implementation to com-
pute an expander hierarchy. Our approach is based on random
walks and is justified by rigorous theoretic and empirical analysis.
We report on a comprehensive experimental study on normalized
cut solvers, comprising 50medium-sized to very large graphs of
various types, where we compare our expander-based algorithm,
XCut , toGraclus by Dhillon et al. [ 10], the approach by Zhao et
al. [41], as well as the state-of-the-art graph partitioners METIS and
KaHiP, which do not specifically optimize towards the normalized
cut objective, but are fast and in practice often used for this task.
The experiments show that our algorithm produces superior nor-
malized cuts on graph classes such as citation, e-mail, and social
networks, web graphs, and generally scale-free graphs, and is only
slightly worse on others. On average, it is still distinctly the best
across all graphs and values of ğ‘˜.
If only a single value of ğ‘˜is desired, its running time is on av-
erage only 3times slower than the runner-up, Graclus, and never
exceeded 18min. A notable advantage of XCut is that it can quickly
compute solutions for multiple values of ğ‘˜once a sparsifier is com-
puted, which can be faster than running Graclus multiple times.
2 RELATED WORK
Our work is motivated by recent theoretical results [ 5,6,11,12,18]
building on expander decompositions [ 22,23,32,34,37] and the ex-
pander hierarchy [ 11] and inspired by non-spectral approaches [ 10]
to tackle the normalized cut problem.
Mohar [ 21] showed that the computation of the so-called isoperi-
metric number or conductance of a graph (see section 3) is NP-hard,
which implies the hardness of the normalized ğ‘˜-cut problem for
ğ‘˜â‰¥2. Normalized cut remains NP-hard on weighted trees [8].
A number of tools that have been used to solve normalized
cut use spectral methods [ 7,24â€“26,28,39]. This usually requires
to compute ğ‘˜eigenvectors of the Laplacian matrix, which was
shown to scale badly in practice [ 10,41]. Afterwards, an additional
discretization step is necessary to obtain the clustering.
Dhillon et al. [ 10] therefore suggest an algorithm called Graclus,
which is based on the multilevel graph partitioning framework. It
applies the same coarsening steps as METIS, but with a modified
matching procedure. The coarsened graph can then be partitioned
using different approaches, including a spectral one. In the refine-
ment step, Graclus uses a kernel ğ‘˜-means-based local search algo-
rithm for improving the normalized cut objective value. The authors
evaluate their algorithm experimentally against METIS as well as a
spectral clustering algorithm [ 39]. They show that it outperforms
the spectral method w.r.t. normalized cut value, running time, and
memory usage. It also produces better results than METIS and is
comparable w.r.t. running time.
Zhao et al. [ 41] employ a joint spectral sparsification and coars-
ening scheme to produce a smaller representation of the graph
that preserves the eigenvectors of the Laplacian in near-linear time
Ëœğ‘‚(ğ‘š). Afterwards, a normalized cut is computed on the sparsifier
 
1017Expander Hierarchies for Normalized Cuts on Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
using spectral clustering. Their sparsification scheme obtains a sig-
nificant reduction in the number of edges and nodes, which makes
it feasible to apply the spectral method on the reduced graphs, with-
out the quadratic term in the running time growing too large. The
authors again perform an experimental comparison with METIS
and observe that their algorithm overall outperforms METIS w.r.t.
the normalized cut value, while being slightly slower.
METIS [17],KaHiP [31], and many other graph partitioning
tools [ 14,35] do not solve the normalized cut problem and instead
aim to find good solutions to a balanced graph partitioning problem,
where the vertex set is to be partitioned into ğ‘˜sets of (roughly)
equal size, while minimizing the number of edges cut. CHACO [14]
implements a spectral graph partitioning approach, but the number
of clusters is limited to at most ğ‘˜=8.
Nie et al. [ 26] recently presented a spectral normalized cut solver
based on the coordinate descent method along with several speedup
strategies. They evaluate their algorithm against two other spectral
methods [ 7,25] on a number of medium-sized data sets and show
that it consistently computes the best solution and is the fastest. A
notable difference is that ğ‘˜is not an input parameter.
3 PRELIMINARIES
For an undirected graph ğº=(ğ‘‰,ğ¸)we useğ‘‘ğ‘£to denote the degree
of vertexğ‘£âˆˆğ‘‰,Â®ğ‘‘to denote the degree vector, i.e., the vector of
vertex degrees, and ğ·=ğ¼Â®ğ‘‘to denote the corresponding degree
matrix. Î”is used to denote the maximum degree of a vertex in ğº.
For a subset ğ‘†of vertices we define the volume vol(ğ‘†)as the sum
of vertex degrees of vertices in ğ‘†. Itsborder border(ğ‘†)(orcapacity )
is defined as|ğ¸(ğ‘†,Â¯ğ‘†)|, whereğ¸(ğ‘‹,ğ‘Œ)={{ğ‘¥,ğ‘¦}âˆˆğ¸|ğ‘¥âˆˆğ‘‹,ğ‘¦âˆˆğ‘Œ}
is the set of edges between subsets ğ‘‹,ğ‘ŒâŠ†ğ‘‰, and Â¯ğ‘†=ğ‘‰\ğ‘†.
Ağ‘˜-cutis a partitionPof the vertex set into ğ‘˜non-empty parts.
Given such a partition P=(ğ‘†1,...,ğ‘†ğ‘˜), itsnormalized cut value ğœƒ
is defined as
ğœƒ(P)=ğ‘˜âˆ‘ï¸
ğ‘–=1border(ğ‘†ğ‘–)
vol(ğ‘†ğ‘–).
Forğ‘˜=2we will usually specify a 2-cut (or just cut) by its
smaller side, i.e., we will refer to the cut (ğ‘†,Â¯ğ‘†)by justğ‘†, where
vol(ğ‘†)â‰¤vol(Â¯ğ‘†). The conductance of a (2-)cut is defined as Î¦(ğ‘†)=
border(ğ‘†)/vol(ğ‘†)and the conductance of a graph ğº=(ğ‘‰,ğ¸)is
Î¦(ğº)=min cutsğ‘†Î¦(ğ‘†).3The problem of finding a 2-cut with min-
imum conductance is closely related to the problem of finding a
2-cut with minimum normalized cut value, because for any 2-cut,
Î¦(ğ‘†)â‰¤ğœƒ(ğ‘†)â‰¤2Î¦(ğ‘†). The normalized ğ‘˜-cut objective can be seen
as a generalization of conductance to ğ‘˜-cuts.
To properly describe our random walks we need some further
notation. For a cut ğ‘†we call vol(ğ‘†)/vol(ğ‘‰)thebalance of the cutğ‘†
and denote it with ğ‘(ğ‘†). For a subset ğ´âŠ†ğ‘‰we useğº{ğ´}to denote
the subgraph induced by ğ´with self-loops added so that the vertex
degrees do not change (a self-loop counts 1 to the degree of a vertex).
This means the degree of a vertex ğ‘âˆˆğ´is the same in ğºas inğº{ğ´}.
Whenever the graph in question is not clear from the context we
use subscripts to indicate the graph, i.e., we write volğº(ğ‘†),Î¦ğº(ğ‘†),
ğ¸ğº(ğ‘‹,ğ‘Œ), etc. To avoid notational clutter we write volğ´(ğ‘†),Î¦ğ´(ğ‘†),
ğ¸ğ´(ğ‘‹,ğ‘Œ)when we are referring to the graph ğº{ğ´}.
3If there are no cuts in ğº(i.e.,|ğ‘‰|=1) we define its conductance to be 1.We say a graph ğº=(ğ‘‰,ğ¸)is ağœ™-expander ifÎ¦(ğº)â‰¥ğœ™, and
we call a vertex partition (ğ‘‰1,...,ğ‘‰â„“)ağœ™-expander decomposition if
Î¦(ğº{ğ‘‰ğ‘–})â‰¥ğœ™for allğ‘–.
4 EXPANDER DECOMPOSITION USING
RANDOM WALKS
In this section we describe our random-walk-based approach for
obtaining expander decompositions and present its theoretical guar-
antees. The complete proofs for these guarantees can be found in
the full version of the paper.
A natural approach for computing expander decompositions is
to find a low conductance cut to split the graph into two parts and
then to recurse on both sides. If no such cut exists, we have certified
the (sub-)graph to be an expander. In the end, the whole procedure
terminates if each subgraph is an expander, i.e., it terminates with
an expander decomposition. Saranurak and Wang [ 32] used this
general approach to obtain an expander decomposition that runs
in timeO(ğ‘šlog4ğ‘š/ğœ™)and only cutsO(ğœ™ğ‘šlog3ğ‘š)edges. While
their flow-based techniques give very good theoretical guarantees,
the hidden constants do not seem to allow for good practical per-
formance (see e.g. [1]).
In this work we base the cut procedure of the expander decompo-
sition on random walks. As a consequence, we can only guarantee
that our decomposition cuts at most ËœO(âˆšï¸
ğœ™ğ‘š)edges4, since we
are limited by the intrinsic Cheeger barrier of spectral methods.
However, random walks have a very simple structure, which leads
to a simple algorithm with good practical performance. The weaker
dependency on ğœ™(âˆšï¸
ğœ™instead ofğœ™) is not crucial for our graph
partitioning application because when using the expander decom-
position to build an expander hierarchy one chooses ğœ™as large as
possible anyway.
Theorem 1 (Expander Decomposition). Given a graph ğºwith
ğ‘šedges and a parameter ğœ™, there is a random-walk-based algo-
rithm that with high probability finds a ğœ™-expander decomposition
ofğºand cuts at mostO(âˆšï¸
ğœ™ğ‘šlog5/2ğ‘š)edges. The running time is
O ğ‘š+ğ‘›logğ‘›
ğœ™log3ğ‘š.
The main part of our algorithm is the cut procedure described
in Section 4.1. This procedure is then plugged into the framework
of Saranurak and Wang to find an expander decomposition. Our
cut procedure gives the following guarantees.
Theorem 2 (Cut Procedure). Given a graph ğº=(ğ‘‰,ğ¸)withğ‘š
edges and a parameter ğœ™, the cut procedure takes O (ğ‘š+ğ‘›logğ‘›)/ğœ™
steps and terminates with one of these three cases:
(1)We certify that ğºhas conductance Î¦(ğº)â‰¥ğœ™.
(2)We find a cut(ğ´,Â¯ğ´)inğºthat has conductance at most Î¦ğº(ğ´)âˆˆ
O(âˆšï¸
ğœ™log3/2ğ‘š). Then one of the following holds:
(a)either vol(ğ´),vol(Â¯ğ´)are both Î©(ğ‘š/log2ğ‘š), i.e.,(ğ´,Â¯ğ´)is
a relatively balanced low conductance cut;
(b)orvol(Â¯ğ´)âˆˆO(ğ‘š/log2ğ‘š)and A is a near 6ğœ™-expander.
Given the cut procedure, an expander decomposition is computed
as follows. On the current subgraph ğº, execute the cut procedure, to
either find a low conductance cut ğ‘†or certify that none exists. If no
4ËœOhides polylogarithmic factors.
 
1018KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Maximilian VÃ¶tsch
such cut exists, then ğºis a certified ğœ™-expander and we terminate.
Otherwise we check whether ğ‘†is sufficiently balanced, i.e., the
volume of the smaller side is at least Î©(ğ‘š/log2ğ‘š). In that case we
cut the edges across (ğ‘†,Â¯ğ‘†)and recurse on both parts. As both parts
are substantially smaller than ğºwe obtain a low recursion depth.
Otherwise,ğ‘†is very unbalanced but the larger side Â¯ğ‘†is a so-called
near expander â€“ a concept introduced by Saranurak and Wang [ 32]:
Definition 1 (Near ğœ™-expander). Given a graph ğº=(ğ‘‰,ğ¸).
A subsetğ´âŠ†ğ‘‰is a nearğœ™-expander in ğºif for all sets ğ‘‹âŠ†ğ´with
vol(ğ‘‹)â‰¤vol(ğ´)/2 :|ğ¸(ğ‘‹,ğ‘‰\ğ‘‹)|â‰¥ğœ™vol(ğ‘‹).
Note that if the LHS in the above equation were |ğ¸(ğ‘‹,ğ´\ğ‘‹)|
thenğº{ğ´}would be ağœ™-expander.
Ifğº{Â¯ğ‘†}(for Â¯ğ‘†returned by the cut-procedure) is indeed a ğœ™-
expander we can just recurse on the smaller side ğ‘†and would
obtain a low recursion depth. Saranurak and Wang introduced a
trimming procedure that, given a subset ğµthat is a near 6ğœ™-expander
with volume vol(ğµ) â‰¥ 9vol(ğ‘‰)/10and|ğ¸(ğµ,Â¯ğµ)|â‰¤ğœ™vol(ğµ)/10,
computes a subset ğµâ€²âŠ†ğµthat is a proper ğœ™-expander and has
volume vol(ğµâ€²)â‰¥1
2vol(ğµ). By applying this trimming step to Â¯ğ‘†
we can return ğº{Â¯ğ‘†â€²}as a proper ğœ™-expander and recurse on the
remaining graph â€“ still with a small recursion depth. Overall, using
our cut procedure within this framework gives Theorem 1.
4.1 Finding Low Conductance Cuts
We now give a detailed description of the cut procedure that forms
the basis of Theorem 2. The goal is to either certify that ğºis a
ğœ™-expander or to find a low conductance cut that is as balanced as
possible. The idea is to exploit that random walks converge quickly
on expanders and hence when they donâ€™t, we know there must be
a low conductance cut. See Algorithm 1 for an outline.
We employ a concurrent random walk, where each node dis-
tributes its unique commodity in the graph. We are interested in
the probability that after t steps a particle that started say at node i
is at some other node j. The walk has converged if this distribution
is essentially identical for every starting vertex. If we quickly reach
this stationary distribution, there cannot be a low conductance cut
and hence the graph must be an expander. Otherwise we can use
information gathered from the walk to find a low conductance cut.
Such a cut may however be very unbalanced. The procedure
therefore accumulates low conductance cuts until the combined
cut is a balanced low conductance cut or the graph that remains
does not have a low conductance cut anymore. Here we call a cut ğ‘†
balanced if its balance ğ‘(ğ‘†)â‰¥ğ›½:=2/log2ğ‘š.
More precisely, the algorithm maintains a partition of ğ‘‰into
two setsğ´,ğ¿for each iteration ğ‘¡with initial values ğ´:=ğ‘‰,ğ¿ :=âˆ….
We repeat the following for ğ‘‡=1/(12ğœ™)steps: In iteration ğ‘¡, we
generate a new random unit vector ğ‘Ÿand execute ğ‘¡âˆ’1steps of the
random walk, initialized according to ğ‘Ÿ. This walk yields a vector ğ‘¢,
on which we analyze the conductance of all sweep cuts, i.e., cuts of
the formğ‘†ğ‘:={ğ‘£âˆˆğ´:ğ‘¢ğ‘£â‰¤ğ‘}for some value ğ‘. Note that these
conductance values can be calculated in linear time after sorting
the entries of ğ‘¢.
We consider a cut to have low conductance if the value is below
the threshold ğ›¾=O(âˆšï¸
ğœ™log3/2ğ‘š). A lower threshold value ğ›¾would
give better guarantees in case (2) of Theorem 2 but at the same time
it would increase the number of rounds required to converge andAlgorithm 1 Cut Procedure
Input: Graphğº=(ğ‘‰,ğ¸), Target expansion ğœ™
Output: Expander orBalanced(ğ‘†,Â¯ğ‘†)orUnbalanced(ğ‘†,Â¯ğ‘†)
1:ğ‘‡â†1/(12ğœ™)
2:ğ›¾â†343âˆš
ğœ™log(32ğ‘š3)log2(ğ‘›) âŠ²cut threshold
3:ğ›½â†2/log2ğ‘š, âŠ²balance
4:ğ‘Šâ†ğ¼ âŠ²random walk matrix
5:ğ´â†ğ‘‰,ğ¿â†âˆ…
6:forğ‘¡=1, ...,ğ‘‡ do
7:Â®ğ‘Ÿâ†random unit vector in Rğ‘›
8:Â®ğ‘¢â†ğ‘Šğ·âˆ’1ğ‘Ÿ âŠ²apply random walk
9:Â®ğ‘¢â†Â®ğ‘¢âˆ’(Â®ğ‘¢âŠ¤Â®ğ‘‘)/vol(ğ‘‰)Â·1 âŠ²ensureÂ®ğ‘¢âŠ¥Â®ğ‘‘
10:A,ğ·ğ´â†adjacency and degree matrix of ğº{ğ´}
11:ğ‘Šâ† 1
2ğ¼+1
2Ağ·âˆ’1
ğ´ğ‘Š âŠ²extend walk matrix
12:ğ‘†â†SweepCut(Â®ğ‘¢,ğ›¾)
13: ifvol(ğ‘†)â‰¥ğ›½ğ‘šthen return Balanced(ğ‘†,Â¯ğ‘†)
14: else ifğ‘†â‰ âˆ…then
15:ğ¿â†ğ¿âˆªğ‘†,ğ´â†ğ´\ğ‘†
16: ifvol(ğ¿)â‰¥ğ›½ğ‘šthen return Balanced(ğ´,ğ¿)
17:ifğ¿=âˆ…then return Expander
18:else return Unbalanced(ğ´,ğ¿)
hence worsen the guarantee of case (1) in Theorem 2. The value
is chosen to ensure we can guarantee Î¦(ğº)â‰¥ğœ™in case (1) of the
Theorem.
The analysis of the sweep cuts yields one of these three cases:
(1)If all sweep cuts have conductance at least ğ›¾, we continue
with the next iteration.
(2)If there exists a sweep cut with conductance <ğ›¾and balance
â‰¥ğ›½we return this low conductance cut.
(3)Otherwise we consider the two-ended sweep cuts of the form
ğ‘†ğ‘,ğ‘:={ğ‘£âˆˆğ´:ğ‘¢ğ‘£âˆ‰[ğ‘,ğ‘]}for valuesğ‘â‰¤ğ‘and find the
one with largest volume among those with Î¦ğ´(ğ‘†ğ‘,ğ‘)<ğ›¾. If
this cut has balance â‰¥ğ›½we return it, otherwise we move it
fromğ´toğ¿and check whether ğ¿has become balanced.
The random walk can be interpreted as the projection of a much
higher dimensional random walk onto the randomly chosen direc-
tionğ‘Ÿ. This projection step is crucial for the implementation to
become computationally feasible and we show that the projections
approximate the original structure sufficiently well.
In order to argue the correctness of the cut procedure we have
to show that it is highly unlikely that the procedure does not find a
cut on a graph that has expansion less than ğœ™. For this we argue
that afterğ‘‡random walk steps (without finding a cut) the walk will
have â€œconvergedâ€ to its stationary distribution w.h.p. Because of the
choice of parameters such a quick convergence is only possible if
ğºis a near 6ğœ™-expander. Whenever the cut procedure returns a cut,
it is guaranteed to have conductance at most ğ›¾. From this it follows
that the expander decomposition cuts at most O(ğ›¾ğ‘šlogğ‘š)edges.
An outline of the proof can be found in Section A in the appendix,
the entire argument is laid out in the full version of the paper.
 
1019Expander Hierarchies for Normalized Cuts on Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
5XCut â€“ A NEW NORMALIZED CUT
ALGORITHM
In this section, we introduce the algorithm XCut , which is based
on the previous sectionâ€™s novel random walk-based expander de-
composition. We note the apparent similarity between multilevel
graph partitioning and the expander hierarchy and use this as the
basis of XCut . As an outline, we use the novel random walks to
construct the expander hierarchy to obtain a coarse representation
of the graph, the tree flow sparsifier. We then compute an initial
solution on the tree. Finally, we use an iterative refinement step
while descending the hierarchy to improve the solution we found.
Compared to other contraction schemes, which lead to each vertex
in the coarsest graph representing roughly the same number of
nodes in the base graph, the subtrees on each level in the tree flow
sparsifier can represent a vastly different number of vertices.
Expander Decomposition. While the expander decomposition out-
lined in section 4 is much simpler to implement than that of [ 32],
as we do not rely on maximum flow computations at all, we made
several choices in the implementation to speed up computation.
We iterate a single random walk, and after each iteration, we check
whether we can find a sparse cut. If we find a suitable cut, we dis-
connect the edges going across it, but contrary to the algorithm in
section 4, we do not restart the random walk, as we find we can
extract further information about the cut structure of the graph
from the state of the random walk. For example, if the random walk
has mixed very well on one of the new components, it is likely to
be an expander, while if there is another sparse cut in the compo-
nent, then the random walk will likely not have mixed well on the
component. One may think that reducing the number of random
walks might lead to a loss of guarantees and higher variance of
the algorithm, but in experiments conducted while designing the
algorithm, we found that on real graph instances running multiple
concurrent random walks is not necessary. In fact, only a single
graph in our 50 graph benchmark had noticeable variance. See also
the discussion in subsection 6.3.
The main parameter of the expander decomposition is the cut
valueğ›¾, which is the minimal sparsity of the cuts our algorithm
makes. Additionally, we introduce a parameter ğœŒ, to be used as
a threshold for â€œcertifying" that a component is an expander, as
we found that choosing the threshold to be 1/(4vol(ğ‘‰)2)does not
offer any benefits over a much larger value. See also subsection 6.2
for details on choosing the value for this parameter. We make a
final modification to the theoretical algorithm in that we omit the
trimming step on unbalanced cuts, since it does not provide any
further speedup of the expander decomposition routine in practice.
Automatically Choosing ğ›¾.The theoretical analysis in [ 11] sug-
gests a choice for ğ›¾andğœ™that is sufficient to prove the theoretical
results. In preliminary experiments we found this choice to be too
pessimistic and, in fact, by adapting ğœ™andğ›¾to the graph we can
obtain better results. However, there is a trade-off to be made: If ğ›¾
is too small, the expander decomposition will not find many sparse
cuts, and we obtain sparsifiers of low quality. On the other hand, if
ğ›¾is too large, most cuts will be sparser than ğ›¾, which leads to many
cuts being made and increases the running time. In the worst case,
this can even prevent the algorithm from terminating.Thus, our goal is to choose ğ›¾such that it offers a good quality vs.
time trade-off. When choosing ğ›¾, we can only observe whether this
was a good choice in a post hoc fashion. A naive strategy would
be to start with a large ğ›¾and decrease it until the expander hier-
archy terminates within a reasonable amount of time. However,
this approach is wasteful, as we discard previously computed de-
compositions, even if they were good. Instead, we decrease ğ›¾by
multiplying it with constant factor ğœ–<1whenever the expander de-
composition on a specific level cuts too many edges in ğºğ‘–. We then
use the new ğ›¾â€²=ğœ–ğ›¾for the remaining expander decompositions,
decreasing it further as required.
Solving on the Sparsifier. To solve normalized ğ‘˜-cut on the tree
sparsifier obtained from the hierarchy, we want to remove ğ‘˜âˆ’1
edges to decompose it into a forest of ğ‘˜trees. Given a solution, i.e.,
a tuple of edges(ğ‘’1,...,ğ‘’ğ‘˜âˆ’1), we assign vertex ğ‘£âˆˆğ‘‰to the cluster
ğ¶ğ‘—associated with ğ‘’ğ‘—ifğ‘’ğ‘—is the first edge we encounter on the
path fromğ‘£to the root. If no edge in the solution lies on the path
to the root, we assign ğ‘£to clusterğ¶ğ‘˜. As normalized cut is NP-hard
also on trees (see section 2), we introduce two heuristic approaches
that take timeO(ğ‘›ğ‘˜)each:
Greedy: The simple greedy heuristic picks the edge in the sparsi-
fier which minimizes the increase in the normalized cut objective in
every step. By simply computing the cost of cutting each remaining
edge, it takes ğ‘‚(ğ‘›)time to find this edge, assuming the number of
vertices in the sparsifier is O(ğ‘›). To partition a graph into ğ‘˜clus-
ters, we repeat this process ğ‘˜âˆ’1times. Forğ‘˜=2, this algorithm
produces the optimal solution on the tree as we pick the edge that
minimizes the cut objective.
Dynamic Programming: Each row of the dynamic program cor-
responds to a level, and we make one cell for each pair (ğ‘£,ğ‘–)of
the row, where ğ‘£is a vertex in the sparsifier and ğ‘–âˆˆ[0,ğ‘˜]. The
value of each cell is the normalized cut value ğœƒof decomposing
the subtree rooted at ğ‘£intoğ‘–parts. We write ğ·ğ‘ƒ(ğ‘£,ğ‘–)for this value.
Additionally, we write ğ‘ğ‘¢ğ‘¡(ğ‘£,ğ‘–)for the weight of cut edges in the
solution incident to the subtree rooted at ğ‘£, as well asğ‘£ğ‘œğ‘™(ğ‘£,ğ‘–)for
the volume remaining in the subtree.
Without loss of generality, assume the tree is binary, as otherwise
we can binarize the tree by inserting edges of infinite cost (see
Henzinger et al. [ 15] for details). The value of a cell is computed
according to the following rule:
DP(ğ‘£,ğ‘—)=min(cutParent(DP(ğ‘£,ğ‘—âˆ’1)),
min
0â‰¤ğ‘–â‰¤ğ‘—(DP(ğ‘£ğ‘™,ğ‘–)+DP(ğ‘£ğ‘Ÿ,ğ‘—âˆ’ğ‘–))),
where cutParent(DP(ğ‘£,ğ‘—âˆ’1))=DP(ğ‘£,ğ‘—âˆ’1)+ğ‘¤(ğ‘£,ğ‘£â€²)+cut(ğ‘£,ğ‘—âˆ’1)
vol(ğ‘£,ğ‘—âˆ’1)
is the best solution where the edge going to the parent is cut. For
each vertex ğ‘£ofğº0, we initialize the bottom row of the program
with DP(ğ‘£,0)=0,DP(ğ‘£,1)=1andDP(ğ‘£,ğ‘—)=âˆforğ‘—âˆˆ[2,ğ‘˜]. In
the root vertex we use the special rule DP(ğ‘£,ğ‘—)=minğ‘–DP(ğ‘£ğ‘™,ğ‘–)+
DP(ğ‘£ğ‘Ÿ,ğ‘—âˆ’ğ‘–)+cut(ğ‘£ğ‘™,ğ‘–)+cut(ğ‘£ğ‘Ÿ,ğ‘—âˆ’ğ‘–)
vol(ğ‘£ğ‘™,ğ‘–)+vol(ğ‘£ğ‘Ÿ,ğ‘—âˆ’ğ‘–), where the last term ensures we
do not produce a solution with vol(ğ‘£,ğ‘–)=0, leading to cluster ğ‘—
being empty.
The Refinement Step. Finally, we perform an iterative refinement
as we descend the hierarchy. While descending, we introduce new
clusters according to the edges in the solution. On each level we then
perform vertex swaps that improve the normalized cut objective,
 
1020KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Maximilian VÃ¶tsch
100101102103104105
Average Degree / Median Degree0.00.20.40.60.81.01.2 (% change vs Graclus)
CS
NS
CN
EM
FE
OP
BP
RN
IF
US
DM
CF
SN
CL
WB
Figure 1: Relative improvement over Graclus (y-axis) vs.
the ratio of the maximum degree and the median degree (x-
axis). The value on the x-axis is larger if the graphs exhibit
a distribution with large outliers. Note the negative trend,
except for the outlier towards the right corresponding to
instance SN7.
by either reducing the number of cut edges or making the partition
more balanced. For details see the full version of the paper.
Support for Variable Number of Partitions ğ‘˜.A notable strength
of our algorithm is that with both heuristics, it solves the problem
for any value ğ‘˜â€²â‰¤ğ‘˜during their execution. Suppose we are still
determining the number of clusters needed. In that case, we can
compute the solution for the maximum ğ‘˜we are interested in
and obtain solutions for all smaller numbers of partitions during
exploratory data analysis. The only step that needs to be rerun is
the refinement step.
6 EXPERIMENTAL EVALUATION
In the previous sections, we have shown that our approach pro-
vides provable guarantees on the approximation ratio for the value
ğœƒof the normalized cut5(and its relatives, sparse cut, and low-
conductance cut) if ğ‘˜=2. We now turn to evaluate XCut exper-
imentally in different configurations. We compare the objective
value of normalized cuts produced by XCut and its running time
against the normalized cut solver Graclus by Dhillon et al. [ 10] and
the state-of-the-art graph partitioning packages METIS [17] and
KaHiP (kaffpa) [ 31], all of which are available publicly. These algo-
rithms are based on the multilevel graph partitioning framework
and produce disjoint partitions of the vertices into ğ‘˜clusters, where
ğ‘˜is a freely choosable parameter. We note that METIS andKaHiP
solve the balanced ğ‘˜-partitioning problem rather than normalized
cut. Nevertheless, we include these solvers in our comparison, as
they are used for this task in practice and we found that they can out-
perform Graclus on some of the graphs in our benchmark dataset.
By this, we follow the methodology of [10] and [41].
In addition, we compare our results to the values reported for the
normalized cut algorithm by Zhao et al. [ 41]6. We omit comparisons
to solvers employing spectral methods such as the recent works by
Chen et al. [ 7] and Nie et al. [ 26], as this approach does not scale
5See Section 3 for the precise definition.
6Unfortunately, the code is not available publicly and also could not be provided by
the authors upon request before the submission deadline.well to large datasets of millions of nodes [ 10]. In preliminary exper-
iments we found that the solver of Nie et al. [ 26] uses over 330GB of
memory on instance CN3, whereas our solver used less than 400MB
of memory. Furthermore, the algorithm presented in [ 26] does not
necessarily produce ğ‘˜clusters, thus making direct comparisons
difficult. Lastly, the space complexity of spectral methods becomes
prohibitive for larger values of ğ‘˜, as noted by [10].
6.1 Experimental Setup
Instances. Our setup includes 50graphs from various applica-
tions. See Table 1 for an overview and the full version of the paper
for details. To facilitate comparability, our collection contains the
eight instances used by Dhillon et al. [10] (BP1, CF1, CS1â€“3, DM1,
OP1, and OP2) as well as the 21instances used by Zhao et al. [ 41].
In addition, we selected 21real-world networks that cover various
application areas, including some of larger sizes. All instances are
available publicly in the Network Repository [ 29] or the SuiteSparse
Matrix collection [9].
Note that a graph with ğ‘˜or more connected components always
has a (normalized) ğ‘˜-cut of size 0. Surprisingly, we found that XCut
is the only solver tested here that finds the trivial optimal solution
ifğ‘˜is less than the number of connected components. However,
testing connectivity before starting a solver remedies this problem,
which is why we decided to exclude graphs with more than 128
connected components except for one (graph ID 0), which we keep
for consistency reasons as it was used in previous comparisons [ 10].
Methodology. AsXCut ,KaHiP, and METIS are randomized, we
ran each of them â„“=10times per instance with different seeds.
Graclus is deterministic, and we ran it three times to obtain a
stable value for the running time. We use the arithmetic mean
over theâ„“runs for each instance to approximate the expected
value ofğœƒand the running time. When reporting values, we write
XCut mean for the mean value across these 10runs, and XCut min
for the minimum. As KaHiP andMETIS behaved almost identically
over allâ„“runs, we only report mean values for them. For each
algorithm and each graph, we compute a partitioning consisting
ofğ‘˜âˆˆ{2,4,8,16,32,64,128}clusters as well as ğœƒ(smaller is better).
As a second criterion, we compare the algorithmsâ€™ running times.
All experiments were conducted on a server with an Intel Xeon
16 Core Processor and 1.5TBof RAM running Ubuntu 22.04 with
Linux kernel 5.15. XCut is implemented in C++and compiled using
gcc11.4 with full optimization7. For all other solvers, we followed
the build instructions shipped with their code. As Graclus is single-
threaded, we ran the single-threaded version of every algorithm.
METIS was run in its default configuration. We used KaHiP with
thefsocial flag, which is tailored to quickly partitioning social
network-like graphs8andGraclus with options -l 20 -b to enable
the local search step and only consider boundary vertices during
local search, as suggested by the authors for larger graphs [10].
6.2 Configuring XCut
Greedy vs. Dynamic Programming (DP). Section 5 describes two
heuristics for computing normalized cuts on the sparsifier. We
7-O3 -march=native -mtune=native
8We chose this setting as we are especially interested in social network-like graphs.
 
1021Expander Hierarchies for Normalized Cuts on Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Types and number of graphs in our benchmark
dataset. ğš«is the maximum degree, k and M are shorthand
notations for 103and 106, respectively. See the full version of
the paper for a detailed list.
T
ype (Abbreviation) # |ğ‘‰| |ğ¸| Î”
Bipartite
(BP) 1 1.4M 4.3M 1.7k
Computational Fluids (CF) 1 17k 1.4M 269
Clustering (CL) 2 4.8k-100k 6.8k-500k 3-17
Citation Network (CN) 9 226k-1.1M 814k-56M 238-1.1k
Circuit Simulation (CS) 3 5k-30k 9.4k-54k 31-573
Duplicate Materials (DM) 1 14k 477k 80
Email Network (EM) 2 33k-34k 54k-181k 623-1383
Finite Elements (FE) 2 78k-100k 453k-662k 39-125
Infrastructure Network (IF) 2 2.9k-49k 6.5k-16k 19-242
Numerical Simulation (NS) 2 11k-449k 75k-3.3M 28-37
Optimization (OP) 2 37k-62k 131k-2.1M 54-8.4k
Random Graph (RD) 1 14k 919k 293
Road Network (RN) 4 114k-6.7M 120k-7M 6-12
Social Network (SN) 7 404k-4M 713k-28M 626-106k
Triangle Mixture (TM) 7 10k-77k 54k-2M 22-18k
US Census Redistricting (US) 1 330k 789k 58
Web Graph (WB) 3 1.3k-1.9M 2.8k-4.5M 59-2.6k
101
100101
01234Running Time [s]
1e-09
1e-08
1e-07
1e-06
1e-05
0.0001
0.001
0.01
0.1
1.0
10.0
100.0
Graph ID
TM6
CN7
Figure 2: Running time vs. normalized cut for different
choices of ğ†on two different graphs for ğ’Œ=16. Colors denote
different levels of ğ†, while shapes indicate the graph.
compare for both heuristics the running time and ğœƒacross ten
precomputed sparsifiers each for 19 representative graphs. The
quality returned by DP was never better than that of Greedy. While
the running times for both heuristics are linear in the size of the
sparsifier, the DP approach scaled worse in ğ‘˜. For example, for
ğ‘˜=32, DP was three times slower than Greedy, and seven times
slower forğ‘˜=128. See the full version of the paper for a plot with
the results for ğœŒ=10âˆ’4. Greedy was faster and produced no worse
quality than DP for all values of ğœŒ. Thus, we only report results for
Greedy for all further experiments.
Parameter Choice for the Expander Decomposition ( ğœŒ,ğ›¾).In Fig-
ure 2, we examine the effect of the threshold parameter ğœŒon the
quality of solutions and running time. If the parameter is chosen
too large, especially greater than one, the entire graph will likely
be certified as an expander before any cuts are made, leading to
very largeğœƒ. Furthermore, we no longer obtain any significant
0 20 40 60 80 100 120
k05101520
 Algorithm Name
Graclus
KaHiP
METIS
XCutFigure 3: Geometric mean of the cut value ğœ½across all graphs
for each ğ’ŒforXCut mean ,Graclus, METIS, and KaHiP.
improvement in ğœƒfor values of ğœŒbelow 10âˆ’4. At the same time,
the running time increases inversely with ğœŒas extra iterations are
needed to converge, which is shown by the vertical arrangement
of the dots in Figure 2. For graph instance TM6, we also find that
non-Pareto-optimal choices of ğœŒexist, namely 0.1 and 0.01, where
both the running time and the returned value are worse than 10âˆ’3
and10âˆ’4. This suggests that the choice of ğœŒis an important design
decision. On all our instances, ğœŒ=10âˆ’4produced Pareto-optimal
results, so we conclude that we can configure this parameter to be
a constant independently of the graphâ€™s structure.
For the automatic tuning of ğ›¾, we chose a starting threshold
of0.3, as this is around the largest value for which the expander
decomposition finds structurally interesting cuts. Whenever ğ›¾is
too large, i.e., the node reduction |ğºğ‘–+1|/|ğºğ‘–|>0.95, we multiply ğ›¾
by a factor of ğœ–=0.8and restart the expander decomposition.
6.3 Comparison to Graclus, METIS, and KaHiP
Figure 5 depicts the solution quality of every solver relative to that
produced by Graclus on all instances for ğ‘˜=32, showing both the
mean and the minimum of the ten runs for XCut . For other values of
ğ‘˜, the overall picture remains the same, see Figure 3 and Appendix B.
We group the graphs by type, with Figure 5 containing two plots, the
upper showing the disconnected IMDB graph and email, citation,
and social network graphs as well as infrastructure networks, as
these are the graphs on which XCut is particularly strong.
Looking at absolute values of ğœƒ, we find that across all instances,
the geometric mean is at least 70 % lower than our competitors,
see Table 2. Interestingly, when we only consider the seven graphs
from [ 10], the geometric means become 0.84 for XCut minand 0.90
forXCut mean , while they are 1.11, 1.19 and 1.03 for Graclus, METIS,
andKaHiP respectively, which implies that KaHiP slightly outper-
forms Graclus in terms ofğœƒon their benchmark (but not XCut ).
One point of note is that XCut does not always find small nor-
malized cuts on the social network SN7 representing user-user
interactions in the Foursquare social network. This appears to be
due to a very high-degree node that connects to approximately
15 % of all vertices, leading to fast convergence of the random walk.
Due to the averaging effect of such a vertex, many nodes ğ‘£have
almost identical values ğ‘¢ğ‘£that vary only slightly between runs.
Thus, when sorting by ğ‘¢-value, their order can vary greatly depend-
ing on the initial values, leading to very different candidate cuts.
This is the only graph where the minimum and mean of the ten
 
1022KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Maximilian VÃ¶tsch
Table 2: Cut value ( ğœ½) of different algorithms. The geo-
metric mean is taken across all graphs and values of ğ’Œâˆˆ
{2,4,8,16,32,64,128}for each graph type.âˆ—For the overall
geometric mean, instances (graph + ğ’Œ) with ğœ½=0were omit-
ted. Only XCut detected such cases.
T
ype XCut mean XCut min Graclus METIS KaHiP
BP 0.00
0.00 1.18 1.38 1.58
CF 4.61 4.18 3.44 3.51 3.15
CL 0.80 0.72 1.04 1.13 1.02
CN 0.07 0.07 1.42 1.72 1.58
CS 0.44 0.41 0.51 0.59 0.57
DM 2.58 2.42 2.11 2.12 2.23
EM 0.44 0.43 3.44 3.78 3.80
FE 0.56 0.53 0.54 0.54 0.55
IF 0.00 0.00 0.93 1.11 1.12
NS 0.85 0.78 0.77 0.76 0.80
OP 0.71 0.66 1.47 1.48 0.99
RD 13.37 13.37 12.08 12.01 11.94
RN 0.01 0.01 0.01 0.01 0.01
SN 0.25 0.19 3.43 3.98 4.00
TM 2.04 1.93 2.87 3.29 3.04
US 0.04 0.03 0.06 0.06 0.05
WB 0.01 0.01 0.10 0.14 0.14
Allâˆ—0.25 0.22 0.89
1.0 0.94
runs of XCut differ significantly ( âˆ’33 % forXCut minand+39 % for
XCut mean relative to Graclus). This indicates that the theoretical
algorithm sketched in section 4, which uses multiple concurrent
random walks (corresponding to more attempts to find a good cut),
would likely have reduced the variance here. This is also the only
graph where the fact that we only use a single random walk impairs
the quality of the result.
The graph classes on which XCut does not perform as well have
fairly homogeneous degree distributions and often appear grid-like
when drawn. We conjecture that in these grid-like graphs, there are
no good expanders (which XCut is trying to find). Instead, sparse
cuts arise mainly from the fact that the cut is balanced, i.e., the
components we disconnect are all large enough, rather than being
sparsely interconnected, which is exploited by the other solvers.
6.4 Comparison to Zhao et al.
In Table B.3 we compare ğœƒforXCut andGraclus to the values
reported by Zhao et al. [ 41] forğ‘˜=30. We observe a similar but
weaker pattern as in the previous section. On graphs arising from
numerical simulation and finite element problems, XCut performs
worse than Zhao et al., but never by more than 36 %. On the tri-
angle mixture instances, XCut achieves better ğœƒon four instances,
while their solver outperforms XCut on two instances. On citation
networks, clustering instances, and those based on maps (RN1 and
US1), XCut outperforms the algorithm by Zhao et al., with ğœƒbeing
up to 2.5 times lower on US1 and CN7. Altogether, XCut is better
than Zhao et al. on roughly 2/3 of the instances, while Graclus does
best on one instance. The geometric mean across all instances is
1.46 for XCut mean , 1.39 for XCut min, 1.64 for Zhao et al., and 3.06
forGraclus. We note that ours is 11 % lower for XCut mean and15 %
lower for XCut min, while Graclusâ€™s value is almost twice that of
100101
Average Degree / Median Degree0.00.20.40.60.81.01.2 (% change vs Graclus)
CS
NS
CN
EM
FE
OP
BP
RN
IF
US
DM
CF
SN
CL
WBFigure 4: Relative improvement over Graclus (y-axis) vs. the
average degree divided by the median degree (x-axis). Larger
x-values signify that the graph exhibits a skewed, power-law-
like degree distribution. Note the negative trend, except for
the outlier towards the right corresponding to instance SN7.
the other solvers in the comparison due to it producing 5â€“30 times
greaterğœƒon the citation network (CN) instances.
While it is difficult to draw good conclusions for the running
times from the values reported in [ 41] as no source code is available,
we note that on some instances XCut takes less than 10 % of the
reported time while producing higher-quality solutions, which
might be indicative of a running time advantage of our solver.
6.5 Running Time
In our experiments, we found that on many graphs, the running
time is spent mainly on computing the expander hierarchy, where
on some instances, this step accounts for over 80% of the running
time, even for ğ‘˜=128. See Figure B.6 for some examples. How-
ever, even on the largest instances in our benchmark, the absolute
running time never exceeded 18 minutes.
Overall, XCut is on average three times slower than Graclus,
20.8times slower than METIS and6.7times slower than KaHiP for
the mean execution time across all choices of ğ‘˜and all instances.
Interestingly, we find that XCut â€™s running time is lower than Gra-
clusâ€™s on several social network graphs and the triangle mixture
instances while it produces a better solution. See the full version of
the paper for detailed running times on some exemplary instances.
Finally, recall that once our algorithm has computed a sparsifier,
we can obtain a solution on the sparsifier for different values of
ğ‘˜without recomputing the sparsifier, which is a unique feature
among the solvers tested here. If we are interested in all seven
values ofğ‘˜, e.g., and only count the time to compute the sparsifier
once for XCut , our experiments take 0.56,3.82, and 1.24times the
running time to compute partitions across all graphs and values
ofğ‘˜using Graclus, METIS, and KaHiP, respectively. In particular,
XCut then is 44 % faster than Graclus. This demonstrates the utility
ofXCut as a tool for exploratory data analysis. We could achieve
further speedups by only computing a solution for ğ‘˜=128and
then choosing the initial subsets of edges for the other values of ğ‘˜,
only performing the refinement.
 
1023Expander Hierarchies for Normalized Cuts on Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
BP1 CN1 CN2 CN3 CN4 CN5 CN6 CN7 CN8 CN9 EM1 EM2 IF1 IF2 SN1 SN2 SN3 SN4 SN5 SN6 SN7 WB1 WB2 WB3
Instance100
75
50
25
0255075100 (% change vs Graclus)
Algorithm Name
XCut
XCut (min)
KaHiP
METIS
CF1 CL1 CL2 CS1 CS2 CS3 DM1 FE1 FE2 NS1 NS2 OP1 OP2 RD1 RN1 RN2 RN3 RN4 TM1 TM2 TM3 TM4 TM5 TM6 TM7 US1
Instance100
75
50
25
0255075100 (% change vs Graclus)
Algorithm Name
XCut
XCut (min)
KaHiP
METIS
Figure 5: Percentage deviation of the returned normalized cut value relative to Graclus forğ’Œ=32. This means that a value of
-75% indicates that the normalized cut value is 75% lower (i.e., better). The thin black bars indicate the standard error across
our runs. The top graph shows the disconnected IMDB graph (BP1), citation network instances (CN), email networks (EM),
infrastructure graphs (IF), social networks (SN), and web graphs (WB), while the bottom shows the remaining instances. See the
full version of the paper for details.
6.6 Discussion
XCut outperforms other software when computing normalized
cuts on social, citation, email, and infrastructure networks and web
graphs. It performs slightly worse on graphs arising from specific
computational tasks, such as finite elements, circuit, or numerical
simulations. The graphs on which this behavior occurs tend to
have degree distributions concentrated around the average degree,
suggesting that they are not graphs with a scale-free structure.
In Figure 4 we plot the relationship between our improvement
over Graclus and the value of1
ğ‘›Ã
ğ‘£âˆˆğ‘‰ğ‘‘ğ‘£
median ğ‘£âˆˆğ‘‰ğ‘‘ğ‘£for the non-synthetic
graphs of our benchmark. This value measures how much the mean
and median diverge due to outlier nodes with very high degrees. The
graphs with power-law distributions tend to have a higher value
on this measure, and we find that there appears to be a negative
correlation, with one outlier due to instance SN7.
7 CONCLUSION
In this work we introduced XCut , a new algorithm for solving the
normalized cut problem. It is based on a novel expander decom-
position algorithm and to the best of our knowledge, it is the first
practical application of the expander hierarchy. XCut clearly out-
performs other solvers in the experimental study on social, citation,
email, and infrastructure networks and web graphs, and also in
the geometric mean over all instances. It scales to instances withtens of millions of edges and can produce solutions for multiple
numbers of clusters ğ‘˜with little overhead by comparison. We are
confident that with further optimization and the use of parallelism
it will be possible to scale our algorithm to even larger graphs, while
further improving the solution quality, especially since computing
the expander decomposition appears to be highly parallelizable.
We also believe that the expander hierarchy and our expander
decomposition can be applied to other graph cut problems in the
future, as the tree flow sparsifiers approximate allcuts in the graph,
and there are theoretical results that suggest this might be the case.
XCut is open source software and its code is freely available on
GitLab [13].
ACKNOWLEDGMENTS
Monika Henzinger: This project has re-
ceived funding from the European Research
Council (ERC) under the European Unionâ€™s
Horizon 2020 research and innovation programme (Grant agree-
ment No. 101019564) and the Austrian Science Fund (FWF) grant
DOI 10.55776/Z422, grant DOI 10.55776/I5982, and grant DOI
10.55776/P33775 with additional funding from the netidee SCIENCE
Stiftung, 2020â€“2024.
Harald RÃ¤cke, Robin MÃ¼nk: This project has received funding
from the Deutsche Forschungsgemeinschaft (DFG, German Re-
search Foundation) â€“ 498605858 and 470029389.
 
1024KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Maximilian VÃ¶tsch
REFERENCES
[1] Isaac Arvestad. 2022. Near-linear time expander decomposition in practice.
[2]Bas Fagginger Auer and Rob H Bisseling. 2012. Graph coarsening and clustering
on the GPU. Graph Partitioning and Graph Clustering 588, 223 (2012), 2.
[3]Charles-Edmond Bichot. 2010. Co-clustering Documents and Words by Mini-
mizing the Normalized Cut Objective Function. J. Math. Model. Algorithms 9, 2
(2010), 131â€“147. https://doi.org/10.1007/S10852-010-9126-0
[4]Deng Cai, Zheng Shao, Xiaofei He, Xifeng Yan, and Jiawei Han. 2005. Mining
hidden community in heterogeneous social networks. In Proceedings of the 3rd
international workshop on Link discovery, LinkKDD 2005, Chicago, Illinois, USA,
August 21-25, 2005, Jafar Adibi, Marko Grobelnik, Dunja Mladenic, and Patrick
Pantel (Eds.). ACM, 58â€“65. https://doi.org/10.1145/1134271.1134280
[5]Yi-Jun Chang, Seth Pettie, Thatchaphol Saranurak, and Hengjie Zhang. 2021.
Near-optimal Distributed Triangle Enumeration via Expander Decompositions.
J. ACM 68, 3 (2021), 21:1â€“21:36. https://doi.org/10.1145/3446330
[6]Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximilian Probst Gutenberg,
and Sushant Sachdeva. 2022. Maximum Flow and Minimum-Cost Flow in Almost-
Linear Time. In 63rd IEEE Annual Symposium on Foundations of Computer Science,
FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022. IEEE, 612â€“623. https:
//doi.org/10.1109/FOCS54457.2022.00064
[7]Xiaojun Chen, Feiping Nie, Joshua Zhexue Huang, and Min Yang. 2017. Scalable
Normalized Cut with Improved Spectral Rotation. In Proceedings of the Twenty-
Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne,
Australia, August 19-25, 2017, Carles Sierra (Ed.). ijcai.org, 1518â€“1524. https:
//doi.org/10.24963/IJCAI.2017/210
[8]Amir Daneshgar and Ramin Javadi. 2012. On the complexity of isoperimetric
problems on trees. Discret. Appl. Math. 160, 1-2 (2012), 116â€“131. https://doi.org/
10.1016/J.DAM.2011.08.015
[9]Timothy A Davis and Yifan Hu. 2011. The University of Florida sparse matrix
collection. ACM Transactions on Mathematical Software (TOMS) 38, 1 (2011),
1â€“25.
[10] Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. 2007. Weighted graph cuts
without eigenvectors a multilevel approach. IEEE transactions on pattern analysis
and machine intelligence 29, 11 (2007), 1944â€“1957.
[11] Gramoz Goranci, Harald RÃ¤cke, Thatchaphol Saranurak, and Zihan Tan. 2021.
The expander hierarchy and its applications to dynamic graph algorithms. In
Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA).
SIAM, 2212â€“2228.
[12] Bernhard Haeupler, Harald RÃ¤cke, and Mohsen Ghaffari. 2022. Hop-constrained
expander decompositions, oblivious routing, and distributed universal optimal-
ity. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of
Computing. 1325â€“1338.
[13] Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Maximilian
VÃ¶tsch. 2024. XCut/XCut v1.0.0. https://doi.org/10.5281/zenodo.12108189. https:
//doi.org/10.5281/zenodo.12108189
[14] Bruce Hendrickson and Robert W. Leland. 1995. A Multi-Level Algorithm For
Partitioning Graphs. In Proceedings Supercomputing â€™95, San Diego, CA, USA,
December 4-8, 1995, Sidney Karin (Ed.). ACM, 28. https://doi.org/10.1145/224170.
224228
[15] Monika Henzinger, Stefan Neumann, Harald RÃ¤cke, and Stefan Schmid. 2023.
Dynamic Maintenance of Monotone Dynamic Programs and Applications. In
40th International Symposium on Theoretical Aspects of Computer Science, STACS
2023, March 7-9, 2023, Hamburg, Germany (LIPIcs, Vol. 254), Petra Berenbrink,
Patricia Bouyer, Anuj Dawar, and Mamadou Moustapha KantÃ© (Eds.). Schloss
Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 36:1â€“36:16. https://doi.org/10.4230/
LIPICS.STACS.2023.36
[16] Yiding Hua, Rasmus Kyng, Maximilian Probst Gutenberg, and Zihang Wu. 2023.
Maintaining expander decompositions via sparse cuts. In Proceedings of the 2023
Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 48â€“69.
[17] George Karypis and Vipin Kumar. 1998. A fast and high quality multilevel scheme
for partitioning irregular graphs. SIAM Journal on scientific Computing 20, 1
(1998), 359â€“392.
[18] Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. 2014.
An Almost-Linear-Time Algorithm for Approximate Max Flow in Undirected
Graphs, and its Multicommodity Generalizations. In Proceedings of the Twenty-
Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland,
Oregon, USA, January 5-7, 2014, Chandra Chekuri (Ed.). SIAM, 217â€“226. https:
//doi.org/10.1137/1.9781611973402.16
[19] Jason Li. 2021. Deterministic mincut in almost-linear time. In Proceedings of the
53rd Annual ACM SIGACT Symposium on Theory of Computing. 384â€“395.
[20] Jia Li, Honglei Zhang, Zhichao Han, Yu Rong, Hong Cheng, and Junzhou Huang.
2020. Adversarial Attack on Community Detection by Hiding Individuals. In
WWW â€™20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, Yennun
Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen (Eds.). ACM / IW3C2,
917â€“927. https://doi.org/10.1145/3366423.3380171[21] Bojan Mohar. 1989. Isoperimetric numbers of graphs. J. Comb. Theory, Ser. B 47,
3 (1989), 274â€“291. https://doi.org/10.1016/0095-8956(89)90029-4
[22] Danupon Nanongkai and Thatchaphol Saranurak. 2017. Dynamic spanning
forest with worst-case update time: adaptive, las vegas, and o (n1/2- ğœ€)-time. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing.
1122â€“1129.
[23] Danupon Nanongkai, Thatchaphol Saranurak, and Christian Wulff-Nilsen. 2017.
Dynamic minimum spanning forest with subpolynomial worst-case update time.
In2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS).
IEEE, 950â€“961.
[24] Maxim Naumov and Timothy Moon. 2016. Parallel spectral graph partitioning.
NVIDIA, Santa Clara, CA, USA, Tech. Rep., NVR-2016-001 (2016).
[25] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2001. On Spectral
Clustering: Analysis and an algorithm. In Advances in Neural Informa-
tion Processing Systems 14 [Neural Information Processing Systems: Natural
and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia,
Canada], Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani
(Eds.). MIT Press, 849â€“856. https://proceedings.neurips.cc/paper/2001/hash/
801272ee79cfde7fa5960571fee36b9b-Abstract.html
[26] Feiping Nie, Jitao Lu, Danyang Wu, Rong Wang, and Xuelong Li. 2024. A Novel
Normalized-Cut Solver With Nearest Neighbor Hierarchical Initialization. IEEE
Trans. Pattern Anal. Mach. Intell. 46, 1 (2024), 659â€“666. https://doi.org/10.1109/
TPAMI.2023.3279394
[27] Vitaly Osipov and Peter Sanders. 2010. n-Level graph partitioning. In Algorithmsâ€“
ESA 2010: 18th Annual European Symposium, Liverpool, UK, September 6-8, 2010.
Proceedings, Part I 18. Springer, 278â€“289.
[28] Richard Peng, He Sun, and Luca Zanetti. 2017. Partitioning Well-Clustered
Graphs: Spectral Clustering Works! SIAM J. Comput. 46, 2 (2017), 710â€“743.
https://doi.org/10.1137/15M1047209
[29] Ryan A. Rossi and Nesreen K. Ahmed. 2015. The Network Data Reposi-
tory with Interactive Graph Analytics and Visualization. In AAAI. https:
//networkrepository.com
[30] Tapasmini Sahoo and Kunal Kumar Das. 2023. Brain Tumor Localization Using
N-Cut. International Journal of Online & Biomedical Engineering 19, 15 (2023).
[31] Peter Sanders and Christian Schulz. 2012. High quality graph partitioning. Graph
Partitioning and Graph Clustering 588, 1 (2012), 1â€“17.
[32] Thatchaphol Saranurak and Di Wang. 2019. Expander Decomposition and
Pruning: Faster, Stronger, and Simpler. In Proceedings of the Thirtieth Annual
ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, Cal-
ifornia, USA, January 6-9, 2019, Timothy M. Chan (Ed.). SIAM, 2616â€“2635.
https://doi.org/10.1137/1.9781611975482.162
[33] Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts and Image Segmentation.
IEEE Trans. Pattern Anal. Mach. Intell. 22, 8 (2000), 888â€“905. https://doi.org/10.
1109/34.868688
[34] Daniel A Spielman and Shang-Hua Teng. 2004. Nearly-linear time algorithms for
graph partitioning, graph sparsification, and solving linear systems. In Proceedings
of the thirty-sixth annual ACM symposium on Theory of computing. 81â€“90.
[35] Chris Walshaw, Mark Cross, and Martin G. Everett. 1995. A Localized Algorithm
for Optimizing Unstructured Mesh Partitions. Int. J. High Perform. Comput. Appl.
9, 4 (1995), 280â€“295. https://doi.org/10.1177/109434209500900403
[36] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James L.
Crowley, and Dominique Vaufreydaz. 2023. TokenCut: Segmenting Objects in
Images and Videos With Self-Supervised Transformer and Normalized Cut. IEEE
Transactions on Pattern Analysis and Machine Intelligence 45, 12 (2023), 15790â€“
15801. https://doi.org/10.1109/TPAMI.2023.3305122
[37] Christian Wulff-Nilsen. 2017. Fully-dynamic minimum spanning forest with
improved worst-case update time. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing. 1130â€“1143.
[38] Eric P. Xing and Richard M. Karp. 2001. CLIFF: clustering of high-dimensional
microarray data via iterative feature filtering using normalized cuts. In Proceedings
of the Ninth International Conference on Intelligent Systems for Molecular Biology,
July 21-25, 2001, Copenhagen, Denmark. 306â€“315.
[39] Stella X. Yu and Jianbo Shi. 2003. Multiclass Spectral Clustering. In 9th IEEE
International Conference on Computer Vision (ICCV 2003), 14-17 October 2003, Nice,
France. IEEE Computer Society, 313â€“319. https://doi.org/10.1109/ICCV.2003.
1238361
[40] Jin Zhang, Lei Xie, Wei Feng, and Yanning Zhang. 2009. A Subword Normalized
Cut Approach to Automatic Story Segmentation of Chinese Broadcast News.
InInformation Retrieval Technology, 5th Asia Information Retrieval Symposium,
AIRS 2009, Sapporo, Japan, October 21-23, 2009. Proceedings (Lecture Notes in
Computer Science, Vol. 5839), Gary Geunbae Lee, Dawei Song, Chin-Yew Lin,
Akiko N. Aizawa, Kazuko Kuriyama, Masaharu Yoshioka, and Tetsuya Sakai
(Eds.). Springer, 136â€“148. https://doi.org/10.1007/978-3-642-04769-5_12
[41] Zhiqiang Zhao, Yongyu Wang, and Zhuo Feng. 2018. Nearly-linear time spectral
graph reduction for scalable graph partitioning and data visualization. arXiv
preprint arXiv:1812.08942 (2018).
 
1025Expander Hierarchies for Normalized Cuts on Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A THEORETICAL ANALYSIS OF THE
EXPANDER DECOMPOSITION
We now give an outline of the proof of correctness of our cut
procedure which culminates in Theorem 2 and forms the basis
of our expander decomposition algorithm. The full argument and
detailed proofs of each lemma can be found in the full version.
The key ingredient for the analysis is to show that it is very
unlikely that the cut procedure does not return a cut within ğ‘‡
iterations when started on a graph with conductance less than ğœ™.
Consequently, if the algorithm does not return a cut for ğ‘‡iterations
we can declare ğºto be ağœ™-expander (or actually ğº{ğ´}to be a near
6ğœ™-expander), with a small probability of error.
We analyze the random walks in terms of flows. Each node ğ‘£in-
jectsğ‘‘ğ‘£units of flow of a unique commodity. This flow is distributed
according to the random walk. Let ğ¹ğ‘–ğ‘—(ğ‘¡)denote the amount of
flow from node ğ‘—that has reached node ğ‘–afterğ‘¡steps. We define
ğ‘ƒğ‘–ğ‘—(ğ‘¡):=ğ¹ğ‘–ğ‘—(ğ‘¡)
ğ‘‘ğ‘–ğ‘‘ğ‘—for allğ‘¡. Whenever clear from context, we may
omit the explicit indication of the round ğ‘¡.
Letğ´(ğ‘¡)âŠ†ğ‘‰be the set of nodes in the subgraph in round ğ‘¡. We
define a natural average vector ğœ‡(ğ‘¡):=1
vol(ğ´(ğ‘¡))Ã
ğ‘–âˆˆğ´(ğ‘¡)ğ‘‘ğ‘–ğ‘ƒğ‘–(ğ‘¡)
and track the convergence of our random walk to this stationary
distribution with a potential function:
ğœ‘(ğ‘¡):=âˆ‘ï¸
ğ‘–âˆˆğ´(ğ‘¡)ğ‘‘ğ‘–Â·âˆ¥ğ‘ƒğ‘–(ğ‘¡)âˆ’ğœ‡(ğ‘¡)âˆ¥2.
Intuitively, a low potential indicates that the ğ‘ƒ(ğ‘¡)vectors have
mixed well in the set ğ´(ğ‘¡). The following lemma shows that a low
potential implies that ğ´(ğ‘¡)is a near 6ğœ™-expander in the graph ğº.
ğº{ğ´(ğ‘¡)}may not be a proper expander because the random walk
may have used edges outside of ğº{ğ´(ğ‘¡)}.
Lemma 1. Ifğœ‘(ğ‘¡)â‰¤1
4 vol(ğ‘‰)2in any stepğ‘¡â‰¤ğ‘‡, thenğ´(ğ‘¡)is a
near 6ğœ™-expander in ğº.
The intuition for this lemma is that the convergence of the
random walk (i.e., low potential) implies the existence of a low-
congestion all-to-all multicommodity flow which in turn implies
good expansion of the graph.
Next, we show that with constant probability during one it-
eration we either return a balanced low conductance cut or the
potential decreases significantly. For this we first have to analyze
by how much the potential decreases during a random walk step.
Potential Decrease by Random Walk. Fix a round ğ‘¡. Note that the
random walk for round ğ‘¡only takes the ğ‘¡âˆ’1random walk steps for
the graphsğº{ğ´(1)},...,ğº{ğ´(ğ‘¡âˆ’1)}, the step for ğº{ğ´(ğ‘¡)}follows
in the next round. In the following we develop an expression for
how much the potential will decrease due to the random walk step
in the current iteration ğ‘¡. Letğ›¿(ğ‘¡):=(ğœ‘(ğ‘¡)âˆ’ğœ‘(ğ‘¡+1))/ğœ‘(ğ‘¡)be the
relative factor by which the potential of round ğ‘¡decreases after the
random walk step in graph ğº{ğ´(ğ‘¡)}.
Lemma 2. The relative potential decrease due to the random walk
step in iteration ğ‘¡is at least
ğ›¿(ğ‘¡)â‰¥1
2Ã
{ğ‘–,ğ‘—}âˆˆğ¸(ğ´(ğ‘¡))âˆ¥ğ‘ƒğ‘–(ğ‘¡)âˆ’ğ‘ƒğ‘—(ğ‘¡)âˆ¥2
2Ã
ğ‘–âˆˆğ´(ğ‘¡)ğ‘‘ğ‘–âˆ¥ğ‘ƒğ‘–(ğ‘¡)âˆ’ğœ‡(ğ‘¡)âˆ¥2
2.Projected Potential Decrease. The algorithm does not maintain the
ğ‘ƒğ‘–vectors or the potential explicitly, as that would be computation-
ally infeasible. Rather it operates on their projections onto some
random vector ğ‘Ÿ. The entries in the vector ğ‘¢of the algorithm are
actuallyğ‘¢ğ‘–=(ğ‘ƒğ‘–âˆ’ğœ‡)âŠ¤ğ‘Ÿ. This follows since performing the random
walk and then projecting the resulting ğ‘ƒğ‘–vectors onto ğ‘Ÿis equiva-
lent to first projecting onto ğ‘Ÿand then running the random walk
on the vector of projections. Ultimately, subtracting the weighted
average of the ğ‘¢values corresponds to subtracting ğœ‡from theğ‘ƒğ‘–
vectors before the projection. Consider the quotient
ğ‘…(ğ‘¢):=Ã
{ğ‘–,ğ‘—}âˆˆğ¸(ğ´(ğ‘¡))(ğ‘¢ğ‘–âˆ’ğ‘¢ğ‘—)2
Ã
ğ‘–âˆˆğ´(ğ‘¡)ğ‘‘ğ‘–ğ‘¢2
ğ‘–,
and compare it to the bound ğ›¿(ğ‘¡)for the relative potential decrease
in Lemma 2. Up to a factor of 1/2, ğ‘…(ğ‘¢)is obtained by individually
performing a random projection on each vector in the expression
forğ›¿(ğ‘¡). Using properties of random projections, we will show that
ğ‘…(ğ‘¢)is a good indicator for the potential decrease ğ›¿(ğ‘¡).
Lemma 3 (Projection Lemma). Letğ‘£1,...,ğ‘£ğ‘›be a collection of ğ‘‘-
dimensional vectors. Let ğ‘Ÿdenote a random ğ‘‘-dimensional vector with
each coordinate sampled independently from a Gaussian distribution
N(0,1/ğ‘‘). Then
(1)Pr[âˆƒğ‘–,ğ‘—:(ğ‘£ğ‘–âˆ’ğ‘£ğ‘—)âŠ¤ğ‘Ÿâ‰¥11 logğ‘›Â·âˆ¥ğ‘£ğ‘–âˆ’ğ‘£ğ‘—âˆ¥2/ğ‘‘]â‰¤1/ğ‘›
(2)Pr[Ã
ğ‘–(ğ‘£âŠ¤
ğ‘–ğ‘Ÿ)2â‰¥1
20 logğ‘›Ã
ğ‘–âˆ¥ğ‘£ğ‘–âˆ¥2/ğ‘‘]â‰¥1/2forğ‘›â‰¥8.
We call an iteration good if the vector of projections ğ‘¢retains suf-
ficient information of the higher dimensional ğ‘ƒğ‘–vectors. Formally,
we require the following.
Definition 2. An iteration ğ‘¡of the algorithm is good if
â€¢Ã
ğ‘–âˆˆğ´(ğ‘¡)ğ‘‘ğ‘–ğ‘¢2
ğ‘–â‰¥1
20ğ‘›logğ‘›Â·Ã
ğ‘–ğ‘‘ğ‘–âˆ¥ğ‘ƒğ‘–âˆ’ğœ‡âˆ¥2and
â€¢(ğ‘¢ğ‘–âˆ’ğ‘¢ğ‘—)2â‰¤11 logğ‘›
ğ‘›ğ‘ƒğ‘–âˆ’ğ‘ƒğ‘—2âˆ€{ğ‘–,ğ‘—}âˆˆğ¸(ğ´(ğ‘¡)).
From Property 1 and Property 2 of the Projection Lemma we can
directly infer the following.
Claim. An iteration is good with probability at least 1/4forğ‘›â‰¥4.
The next lemma shows that in a good round, a large ğ‘…(ğ‘¢)value
implies a large ğ›¿(ğ‘¡)value, i.e., a large potential decrease.
Lemma 4. In a good round ğ‘¡, we haveğ‘…(ğ‘¢)â‰¤440 log2(ğ‘›)Â·ğ›¿(ğ‘¡).
For the further analysis we always assume that we are in a
good round.
Finding a Cut. With the above lemma we have established that a
largeğ‘…(ğ‘¢)value ensures that the next step of the random walk
reduces the potential a lot. Now we show that a small ğ‘…(ğ‘¢)value
ensures that the algorithm finds a low conductance cut.
Indeed the quotient ğ‘…(ğ‘¢)(the Rayleigh quotient of ğ‘¢using the
graph Laplacian), and its analysis lies at the heart of the proof for
the celebrated Cheeger inequality. It states that a small ğ‘…(ğ‘¢)value
implies the existence of a low conductance sweep cut.
Lemma 5 (Cheeger). For anyğ‘¥âˆˆRğ‘›, withğ‘¥âŠ¥Â®ğ‘‘there is a value
ğ‘, s.t. there is a non-trivial set ğ‘†ğ‘={ğ‘–âˆˆğ‘‰:ğ‘¥ğ‘–â‰¤ğ‘}with conductance
Î¦ğº(ğ‘†ğ‘)â‰¤âˆšï¸
2ğ‘…(ğ‘¥).
 
1026KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kathrin Hanauer, Monika Henzinger, Robin MÃ¼nk, Harald RÃ¤cke, and Maximilian VÃ¶tsch
Note thatğ‘¢âŠ¥Â®ğ‘‘holds by design of the algorithm due to Line 10 of
the algorithm. With this Cheeger Lemma we thus get the guarantee
that there has to be a sweep cut through ğ‘¢whose conductance Î¦
satisfies Î¦â‰¤âˆšï¸
2ğ‘…(ğ‘¢). Together with Lemma 4 this now implies
the following: If we will only make little progress in the potential
with the next random walk step, i.e., ğ›¿(ğ‘¡)â‰¤ğ›¼for some small ğ›¼,
then we find a sweep cut through the vector of projections ğ‘¢with
conductance Î¦â‰¤O(âˆšğ›¼logğ‘›). By contraposition, if all sweep cuts
have conductance at least Î¦, thenğ›¿(ğ‘¡)â‰¥Î©(Î¦2/log2ğ‘›).
This motivates our approach of analyzing the sweep cuts on ğ‘¢
and differentiating three cases: 1) No low conductance sweep cut ex-
ists, 2) A balanced low conductance cut exists, or 3) An unbalanced
low conductance cut exists.
If we find a balanced sweep cut, we return immediately as this
ensures the recursion depth of the surrounding expander decom-
position remains small. If no sweep cut has conductance below
ğ›¾, the above reasoning ensures we make sufficient progress with
the next random walk step. Next we prove that even if we find an
unbalanced cut, the potential still decreases sufficiently.
Handling Unbalanced Cuts. Let(ğ‘†,ğµ)be an unbalanced cut, where
vol(ğ‘†)â‰¤2log2(vol(ğ´))/vol(ğ´)and there is no low conductance
sweep cut through ğµinğ‘¢. We show that if ğ‘…(ğ‘¢)is small and we
thus cannot ensure sufficient progress from the random walk, then
the setğ‘†must contain a large fraction of the potential. Hence we
instead make progress by removing ğ‘†when we set ğ´(ğ‘¡+1)=ğµ.
First we observe that the ğ‘…(ğ‘¢)value cannot increase too much by
removing a small cut. In fact, with the above bound on the volume
of the smaller side ğ‘†, we can show that ğ‘…(ğ‘¢)increases by at most a
factor 2. This leads to the following lemma, where ğœ‘ğµdenotes the
potential generated by summing only over the nodes in ğµ.
Lemma 6. Ifğ‘…(ğ‘¢)<ğ›¾2/4and we find an unbalanced cut (ğ‘†,ğµ)
with vol(ğ‘†)â‰¤2ğ‘š/log2ğ‘š, thenğœ‘ğµ(ğ‘¡)â‰¤ 1âˆ’1/(880 log2ğ‘›)Â·ğœ‘(ğ‘¡).
The above results show that a good round ensures sufficient
progress in decreasing the potential or produces a balanced cut.
Claim. In a good round we either find a balanced cut, or the
potential reduces by a factor of at least 1âˆ’ğ›¾2/(1760 log2ğ‘›).
Proof. LetÎ¦be the minimum value of any sweep cut on ğ‘¢.
IfÎ¦â‰¥ğ›¾, then Lemmas 4 and 5 give ğ›¿â‰¥ğ›¾2/(880 log2ğ‘›)since
ğ›¾2â‰¤Î¦2â‰¤2ğ‘…(ğ‘¢)â‰¤880 log2(ğ‘›)Â·ğ›¿ .
IfÎ¦<ğ›¾, we will find some low conductance cut ğ‘†. Ifğ‘†is balanced,
we can return it, otherwise we move ğ‘†fromğ´toğ¿. Then either
â€¢ğ‘…(ğ‘¢)â‰¥ğ›¾2/4and Lemmas 4 and 5 give ğ›¿â‰¥ğ›¾2/(1760 log2ğ‘›),
or
â€¢ğ‘…(ğ‘¢)<ğ›¾2/4and Lemma 6 gives a factor of 1âˆ’1/(880log2ğ‘›).
In any case, the decrease is at least 1âˆ’ğ›¾2/(1760 log2ğ‘›). â–¡
Iterations. An iteration takesO(ğ‘š)time for the random walk and
O(ğ‘›logğ‘›)for sorting the ğ‘¢values. It only remains to prove the
upper bound on the number of iterations. The threshold for the
potential is chosen to guarantee the existence of a near 6ğœ™-expander
in Lemma 1.Lemma 7. Afterğ‘‡=1/12ğœ™, iterations of the cut procedure, with
high probability ğœ‘(ğ‘‡)â‰¤1
4 vol(ğ‘‰)2.
Altogether, the above arguments imply the correctness of our
Theorem 2. Analogous to the reasoning by Saranurak and Wang [ 32],
we can conclude that our modified guarantees yield Theorem 1
when using our cut step procedure in their expander decomposi-
tion framework.
B ADDITIONAL FIGURES
Table B.3: Cut values and running time of XCut vs the values
reported by Zhao et al [ 41]. All values are for ğ’Œ=30. The Cut-
columns contain the normalized cut value (lower is better).
The minimum value in each row is marked bold. The bottom
row contains the geometric mean of all values.
GID XCut mean XCut min Zhao Graclus
CL1 0.87 0.77 1.05 1.15
CL2 6.00 5.87 7.05 7.61
CN1 0.31 0.29 0.52 4.06
CN3 0.27 0.27 0.49 3.66
CN5 0.13 0.13 0.14 3.24
CN7 0.17 0.16 0.41 2.21
CN8 0.04 0.04 0.06 1.88
FE1 2.06 1.96 1.68 1.74
FE2 1.58 1.55 1.50 1.45
NS1 5.73 5.35 4.71 4.71
NS2 1.45 1.39 1.08 1.17
RD1 28.98 28.98 23.80 26.48
RN4 0.06 0.06 0.07 0.07
TM1 9.53 8.91 6.85 8.48
TM2 12.34 12.06 13.55 16.63
TM3 2.78 2.65 2.72 2.78
TM4 10.09 9.58 10.48 14.45
TM5 7.77 7.48 7.88 13.25
TM6 2.11 1.94 2.09 2.1
TM7 17.51 16.90 12.83 15.6
US1 0.17 0.16 0.41 0.19
All 1.46 1.39 1.64 3.06
2 4 816 32 64128
k0.00.20.40.60.8Time [s]Graph ID = EM2
2 4 816 32 64128
k0.00.51.01.52.0Graph ID = FE1
2 4 816 32 64128
k0102030Graph ID = CN8
2 4 816 32 64128
k020406080100120Graph ID = WB3
2 4 816 32 64128
k050100150200250300Graph ID = SN5
Figure B.6: Plot showcasing the time taken to compute the
expander hierarchy in orange and the total time to compute
a normalized cut in blue over different values of ğ’Œfor five
graph instances of different sizes.
 
1027