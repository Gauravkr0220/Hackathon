Resilient k-Clustering
Sara Ahmadianâˆ—
sahmadian@google.com
Google
Seattle, USAMohammadHossein Bateni
bateni@google.com
Google
New York, USAHossein Esfandiari
esfandiari@google.com
Google
New York, USA
Silvio Lattanzi
silviol@google.com
Google
Barcelona, SpainMorteza Monemizadeh
m.monemizadeh@tue.nl
Department of Mathematics and
Computer Science, TU Eindhoven
NetherlandsAshkan Norouzi-Fard
ashkannorouzi@google.com
Google
Zurich, Switzerland
ABSTRACT
We study the problem of resilient clustering in the metric setting
where one is interested in designing algorithms that return high
quality solutions that preserve the clustering structure under pertur-
bations of the input points. Our first contribution is to introduce a
formal notion of algorithmic resiliency for clustering problems that,
roughly speaking, requires an algorithm to have similar outputs
on close inputs. Then, we notice that classic algorithms have weak
resiliency guarantees and develop new algorithms for fundamen-
tal clustering problems such as ğ‘˜-center,ğ‘˜-median, and ğ‘˜-means.
Finally, we complement our results with an experimental analysis
showing the effectiveness of our techniques on real-world instances.
CCS CONCEPTS
â€¢Theory of computation â†’Approximation algorithms anal-
ysis; Facility location and clustering.
KEYWORDS
Robust clustering, ğ‘˜-clustering, Hierarchical clustering, Minimum
Spanning Tree
ACM Reference Format:
Sara Ahmadian, MohammadHossein Bateni, Hossein Esfandiari, Silvio Lat-
tanzi, Morteza Monemizadeh, and Ashkan Norouzi-Fard. 2024. Resilient k-
Clustering. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3637528.3671888
1 INTRODUCTION
Clustering is a central problem in machine learning with applica-
tions in several fields such as data mining [ 2,25], social network
analysis [ 5,19,40,41], biology [ 22,42] and many more. In cluster-
ing, we are interested in grouping together â€œsimilarâ€ objects and
âˆ—All authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671888separating dissimilar ones. In this paper, we focus on a classic met-
ric clustering problemâ€”the ğ‘˜-center problem [ 23,26,28,30,31]. In
addition, we show how our results can be extended to ğ‘™ğ‘-clustering
for generalğ‘â‰¥1which includes the ğ‘˜-median problem ( ğ‘=1) and
theğ‘˜-means problem ( ğ‘=2).
Inğ‘˜-center, one is given as input a set of points ğ‘ƒin a metric space
ğ‘€(ğ‘ƒ,ğœ‡)with distance function ğœ‡:ğ‘ƒÃ—ğ‘ƒâ†’R+, and an integer ğ‘˜â‰¥1.
The goal is to compute a subset ğ¶âŠ†ğ‘ƒof size|ğ¶|â‰¤ğ‘˜to minimize
cost(ğ¶):=maxğ‘¥âˆˆğ‘ƒğœ‡(ğ‘¥,ğ¶), whereğœ‡(ğ‘¥,ğ‘†):=minğ‘¦âˆˆğ‘†ğœ‡(ğ‘¥,ğ‘¦).
Thanks to its simplicity and practical relevance, ğ‘˜-center has
been extensively studied, and efficient algorithms are known for it
in the classic setting [ 26] as well as in distributed [ 34] and streaming
settings [ 11]. Nevertheless, the solution returned by such algorithms
are often very sensitive even to small input perturbations.
For example, consider the classic farthest point traversal algo-
rithm for the ğ‘˜-center problem [ 26] which is a tight 2-approximate
algorithm under the assumption that ğ‘ƒâ‰ ğ‘ğ‘ƒ. In this algorithm,
the first center in the solution is chosen arbitrarily; then, subse-
quent centers are chosen one by one by selecting the point that
maximizes the distance to the current set of centers. Now imagine
applying the farthest-point traversal algorithm to perturbed points
on a unit sphere. It is easy to come of with simple examples1that
different perturbations (even if tiny) would lead to the generation
of completely different clustering solutions. Interestingly, this is
not just an issue in synthetic instances. In fact, in our experiments,
we show that classic algorithms are often extremely unstable even
on real-world instances.
This phenomenon is particularly worrisome in practice where
data often contains some noise and changes over time. For example,
consider embeddings capturing usersâ€™ behavior in an online plat-
form. It is natural to expect that such embeddings would change
slightly on a daily basis. In this setting, an ideal clustering algo-
rithm should change the output clusters smoothly and preserve
most of the previously outputted solution. Unfortunately this is not
the case for classic algorithms. For these reasons, in this paper we
initiate the formal study of resilient algorithms for classic clustering
problems such as ğ‘˜-center and other ğ‘˜-clustering problems.
Our Results. Our first contribution is a novel notion of algorith-
mic resilience. Intuitively, in our notion, a clustering algorithm is
considered resilient if it returns similar solutions on close input
1For instance the case that all the pairs of points have the same distance and then the
distances are slightly changed at random.
 
29
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Sara Ahmadian et al.
instances. Our notion of closeness between two input instances
captures the fact that in close instances the pairwise distance be-
tween any two points does not change significantly (for a formal
definition, please refer to Section 2). Interestingly, this is enough to
capture many real-world scenarios, as our experiments show. For
instance, it captures the setting [ 3,21,44] where datasets evolve
gradually or smoothly over time, and one is interested in maintain-
ing a stable solution as long as the input instance does not change
too much. This is a practical setting, for example, in geo-location
applications if one considers two close and consecutive snapshots
of the dataset one would expect to have points moved just a little
bit. Another important case is when the position of the points is ob-
tained through measurements with a small error where one would
expect two consecutive measurements of the same set of points to
be close but different. In addition, it captures the setting [ 14,15]
where the underlying input is perturbed randomly or adversarially,
and one is interested in preserving the underlying clusters.
Leveraging this new definition, we design and analyze new
resilient algorithms for the ğ‘˜-center problem. For close input in-
stances, our algorithms return solutions that (1) are similar and
(2) provide good approximation of the optimal solutions for the
corresponding instances. Moreover, our algorithm is efficient: in
fact it runs in time Ëœğ‘‚(ğ‘›ğ‘˜)almost matching the running time of
the classic farthest-point traversal algorithm.2See Theorem 3.1 for
the formal statement of our result for the ğ‘˜-center problems. We
also present extensions of these algorithms for the â„“ğ‘clustering
problems (see Theorem 4.1).
Finally, we provide experiments showing that our algorithms
return clustering solutions of approximately good quality while
at the same time they are significantly more resilient to small per-
turbation in the input than classical clustering algorithms that we
utilize in our benchmarks.
1.1 Related work
Sensitivity and average sensitivity. Our notion of resiliency is
related to the notions of sensitivity and average sensitivity that have
been extensively studied for dynamic programming, clustering,
graph and learning problems [ 32,36,45â€“47]. In fact, both notions
focus on developing algorithms that are resilient over perturbations
of the input space. Nevertheless in previous work the focus is on
the impact of removing or adding a single element to the input
instance, while in our setting we allow allthe points to change
their positions by a fixed amount. More closely, [ 37] introduces a
notion closer to ours (Lipschitz continuity) for graph problems and
provide several algorithms in their setting. However, this notion
is specialized for graph problems and it is not easily extendable to
our clustering setting.
Perturbation resilience. The concept of(ğ›¼,ğœ–)-perturbation re-
silience evolved through a series of papers, including [ 1,6,8,12].
Interestingly, the concept of ğ›¼-perturbation in these works is simi-
lar to(1+ğœ–)-closeness defined in our paper. However, the notion
of resilience in these works differs from the resilience concept we
consider. In particular, the (ğ›¼,ğœ–)-perturbation resilience requires
that all but an ğœ–fraction of points have the same clustering for any
2The notation Ëœğ‘‚(Â·)hides logarithmic factors.two instances that satisfy ğ›¼-perturbation. For such instance, they
could provide clustering algorithms. In contrast, we develop clus-
tering algorithms that are considered resilient if they return similar
solutions on close input instances. This means that the same algo-
rithm (with the same random bits used) returns the same clustering
for all but the ğœ–fraction of points.
Consistent clustering. Another close area of research that re-
ceived a lot of attention in recent years is consistent clustering [ 10,
17,24,29,35,39]. In this setting, the input evolves over time, with
points joining and leaving, and the goal is to maintain a good solu-
tion with as few changes as possible. The results on this interesting
setting, though, do not provide any stability guarantees in the pres-
ence of perturbations of the input.
Differential privacy. Our work is also loosely related to research
in differential privacy [ 20]. In fact, both settings seek to design
algorithms resilient to small variations of the input instances. How-
ever, in differential privacy one wants the output of an algorithm
to change with a small probability when a single point in the in-
put is deleted. Instead in our setting, one wants the output to not
change too much when (possibly) allthe points slightly changes
their positions.
Stability and robustness. Finally, we note that in literature there
are several influential papers studying different notion of stability
or robustness for clustering [ 4,7,9,38]. These papers are quite
interesting although they have a different focus and do not provide
any guarantee on resilience to adversarial perturbation in metric
space.
Outline of the paper. In Section 2, we introduce the commonly
used notations and recall some definitions used in the main body of
the paper, and in particular, we formalize the definition of resiliency
for clustering problems. In section Section 3, we present our resilient
algorithm for ğ‘˜-center. We extend our results to the ğ‘˜-median and
theğ‘˜-means problems in Section 4. Finally, in Section 5 we report
the results of our empirical results.
2 PRELIMINARIES
In this section, we introduce common notations and formally define
the notion of resiliency. We present Chernoff concentration bounds
used in our proofs as well.
Clustering. A metric space ğ‘€(ğ‘ƒ,ğœ‡ğ‘ƒ)is defined on a point set
ğ‘ƒwith a distance function ğœ‡ğ‘ƒ:ğ‘ƒÃ—ğ‘ƒâ†¦â†’Râ‰¥0, which is symmetric,
satisfies triangle inequality, and is zero if and only if the two points
are the same. We use ğœ‡instead ofğœ‡ğ‘ƒ, whenğ‘ƒis clear from the
context. For a set ğ‘ƒâ€²âŠ†ğ‘ƒand a pointğ‘âˆˆğ‘ƒ, we extend the notation
ğœ‡(ğ‘,ğ‘ƒâ€²)=minğ‘âˆˆğ‘ƒâ€²ğœ‡(ğ‘,ğ‘). Ağ‘˜-clusteringC:ğ‘ƒâ†¦â†’ğ¶of a point
setğ‘ƒis defined by a set ğ¶âŠ†ğ‘ƒofğ‘˜centers, which assigns each
pointğ‘âˆˆğ‘ƒto a centerC(ğ‘)âˆˆğ¶. It will be more convenient in the
definitions of resiliency to use an alternative representation of the
clustering as a set of pairs {(ğ‘1,C(ğ‘1)),(ğ‘2,C(ğ‘2)),...}.
Theâ„“ğ‘clustering problems are ğ‘˜-clustering problems where
the goal is to minimize the cost of the returned solution C(forğ‘ƒ)
defined asÃ
ğ‘âˆˆğ‘ƒğœ‡(ğ‘,ğ¶)ğ‘. This family of problems includes well-
known clustering problems such as (i) ğ‘˜-center for sufficiently large
 
30Resilient k-Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
ğ‘, i.e., with cost maxğ‘âˆˆğ‘ƒğœ‡(ğ‘,C(ğ‘)), (ii)ğ‘˜-median for ğ‘=1, and (iii)
ğ‘˜-means forğ‘=2.
LetAbe an algorithm solving a ğ‘˜-clustering problem on the set
ğ‘ƒ. Then we denote with A(ğ‘ƒ,ğ‘˜)its output clustering in the set of
pairs notation{(ğ‘1,C(ğ‘1)),(ğ‘2,C(ğ‘2)),...}.
Resilient Clustering. Let us start by giving an intuitive definition
and then provide an identical formal definition of resilient cluster-
ing. Assume that each point ğ‘âˆˆğ‘ƒhas an identifier which is also
denoted byğ‘(basically the name of the point ğ‘). Then two instances
ğ‘€(ğ‘ƒ,ğœ‡)andğ‘€â€²(ğ‘ƒâ€²,ğœ‡â€²)(with the same set of point identifiers) are
ğœ€-close if for any two points ğ‘,ğ‘, their distance does not differ by
more than a factor (1+ğœ€)inğœ‡andğœ‡â€², i.e.,1
1+ğœ€â‰¤ğœ‡ğ‘ƒâ€²(ğ‘,ğ‘)
ğœ‡ğ‘ƒ(ğ‘,ğ‘)â‰¤1+ğœ€.
Later on we formalize the definition by introducing a mapping
between the points in ğ‘ƒ,ğ‘ƒâ€². We call an algorithm ğ›¾-resilient if the
centers of at most a ğ›¾ğœ€fraction of the points change when the
algorithm runs on ğ‘ƒcompared to when it runs on ğ‘ƒâ€². We again
formalize this using maps.
We are now ready to give a formal definition of resiliency. We
start by defining the notion of ğœ€-close point sets.
Definition 2.1. [ğœ€-close point sets] Given are ğ‘€(ğ‘ƒ,ğœ‡)andğ‘€â€²(ğ‘ƒâ€²,ğœ‡â€²)
with a bijective relation ğœ‹:ğ‘ƒâ†¦â†’ğ‘ƒâ€². Then,ğ‘ƒandğ‘ƒâ€²areğœ€-close if
and only if for any pair of points ğ‘,ğ‘âˆˆğ‘ƒsuch thatğœ‡(ğ‘,ğ‘)â‰ 0we
have
1
1+ğœ€â‰¤ğœ‡â€²(ğœ‹(ğ‘),ğœ‹(ğ‘))
ğœ‡(ğ‘,ğ‘)â‰¤1+ğœ€,
and ifğœ‡(ğ‘,ğ‘)=0, thenğœ‡â€²(ğœ‹(ğ‘),ğœ‹(ğ‘))=0.
Let[ğ‘˜]={1,...,ğ‘˜}for any natural number ğ‘˜. For two sets ğ´,ğµ
we define the symmetric difference ğ´â–³ğµ=(ğ´\ğµ)âˆª(ğµ\ğ´)=
(ğ´âˆªğµ)\(ğ´âˆ©ğµ). With slight abuse of notation, for any mapping
ğ‘“:ğ´â†¦â†’ğµ(in particular, the bijective mapping ğœ‹in the above
definition), we use shorthands ğ‘“((ğ‘1,ğ‘2))=(ğ‘“(ğ‘1),ğ‘“(ğ‘2))and
ğ‘“({ğ‘1,ğ‘2,...})={ğ‘“(ğ‘1),ğ‘“(ğ‘2),...}whereâˆ€ğ‘–:ğ‘ğ‘–âˆˆğ´. Similarly
we defineğ‘“(ğº)for a graphğº(ğ´,ğ¸,ğ‘¤)3to denote a graph obtained
fromğºby mapping all vertices according to ğ‘“. Now we can define
ğ›¾-resiliency.
Definition 2.2 (ğ›¾-resiliency for clustering) .AlgorithmAisğ›¾-
resilient if and only if the outputs of A(ğ‘ƒ,ğ‘˜)andA(ğ‘ƒâ€²,ğ‘˜)differ
by at mostğ›¾ğœ€fraction, i.e.,
E[|ğœ‹(A(ğ‘ƒ,ğ‘˜))â–³A(ğ‘ƒâ€²,ğ‘˜)|]â‰¤ğ›¾ğœ–Â·|ğ‘ƒ|
for anyğœ€-close point sets ğ‘ƒandğ‘ƒâ€²with the corresponding bijective
mappingğœ‹:ğ‘ƒâ†¦â†’ğ‘ƒâ€².
Note that our notion of resilience is characterized by two quan-
tities(ğœ€,ğ›¾). In this paper we think about ğ›¾âˆˆğ‘‚(1). It is worth
mentioning that ğœ€does not have to be small for our results to hold
and be meaningful.4
A nice property of our definition is that it directly relates the
distance between the two inputs with the symmetric difference
between the solutions. So it naturally captures the resilience of the
algorithm to changes in the input. w
Approximation Algorithms. The approximation factor of an
algorithm is defined as the maximum ratio (over all possible inputs)
3ğ´represents the set of the vertices of the graph, ğ¸represents the set of the edges of
the graph and ğ‘¤represents the edge weight function.
4In fact, our results are non-trivial even for constant ğœ€.of the cost of the algorithm to the cost of the optimal solution. An
algorithm with approximation factor ğ›¼is called anğ›¼-approximation
(algorithm). This will be (in the case of ğ‘˜-clustering) a bicriteria
algorithm if the output solutions may use more than the prescribed
ğ‘˜centers. We call this a bicriteria (ğ›¼,ğ›½)-approximation algorithm
if the cost is guaranteed to be within a factor ğ›¼of the optimum and
the number of centers is guaranteed to be at most ğ›½ğ‘˜.
Randomized algorithms. A randomized algorithm is one that
uses a source of randomness as part of its logic. However, we would
like to emphasize that in our results we share random bits between
different executions of the algorithm. For example, a subset of ğ‘˜
random points from a point set ğ‘ƒis guaranteed to be the same
over various runs. This is in fact required to prove any resiliency
guarantee for any randomized algorithm over different inputs.
Graphs. Letğº=(ğ‘‰,ğ¸,ğ‘¤)be a graph on vertex set ğ‘‰of sizeğ‘›,
edge setğ¸with edge weights ğ‘¤:ğ¸â†¦â†’Râ‰¥0. Let Î“ğº(ğ‘£)denote the
set of all edges in ğºincident on vertex ğ‘£. Forğ¸â€²âŠ†ğ¸, letğ‘¤(ğ¸â€²)=Ã
ğ‘’âˆˆğ¸â€²ğ‘¤(ğ‘’). A minimum spanning tree (MST) is a subset of the
edgesğ¸â€²âŠ†ğ¸such thatğº=(ğ‘‰,ğ¸â€²)is acyclic and connected and
such thatğ‘¤(ğ¸â€²)is minimized.
LetAbe an algorithm solving the MST problem on the graph ğº=
(ğ‘‰,ğ¸,ğ‘¤), then we denote with A(ğº)its output MST represented
by the set of edges in the solution.
Chernoff concentration bound [13] Letğ‘‹=Ã
ğ‘–âˆˆ[ğ‘]ğ‘‹ğ‘–, where
ğ‘‹ğ‘–=1with probability ğ‘ğ‘–andğ‘‹ğ‘–=0with probability 1âˆ’ğ‘ğ‘–,
and allğ‘‹ğ‘–â€™s are independent. Let ğœ‡=E[ğ‘‹]=Ã
ğ‘–âˆˆ[ğ‘]ğ‘ğ‘–. Then
Pr[ğ‘‹â‰¥(1+ğœ‰)ğœ‡]â‰¤exp(âˆ’ğœ‰2Â·ğœ‡/3)forğœ‰>0.
3 RESILIENT ğ‘˜-CENTER ALGORITHM
In this section, we present our resilient algorithm for the ğ‘˜-center
problem. In this problem, we are given a point set ğ‘ƒâŠ‚Rğ‘‘and
a parameter ğ‘˜âˆˆNand the goal is to return a set ğ¶âŠ†ğ‘ƒofğ‘˜
centers and an assignment of points in ğ‘ƒto the centers ğ¶so that
the maximum distance between a point and its assigned center is
minimized.
High-level idea of the algorithm The algorithm works in two
phases. In the first phase we select a large enough set of random
points to act as an initial set of centers. Then we carefully assign
points that are â€œcloseâ€ to the initial set of centers such that the
assignment is stable to perturbations in the input space. To achieve
this, we introduce a novel assignment algorithm which can be of
independent interest. In the second phase, we cluster the unassigned
points using the classic farthest-point traversal algorithm. The key
observation is that by selecting enough random points in the first
phase, we are left with very few unassigned points in the second
phase, so we can apply any clustering algorithm on them without
affecting the resilience guarantee too much.
More precisely, we first sample a set ğ¶âŠ†ğ‘ƒof size|ğ¶|=2ğ‘˜log(1/ğœ€)
fromğ‘ƒ. We then construct an auxiliary graph ğº=(ğ‘ƒ,ğ¸), where
(i) edges inside ğ¶have weight zero, and (ii) edges between ğ¶, and
ğ‘ƒ\ğ¶have weight equal to the distance between the two points. We
next invoke Resilient-MST (a novel resilient algorithm for MST
introduced in this work) on the auxiliary graph ğº(ğ‘ƒ,ğ¸), and argue
that the produced tree ğ‘‡ofResilient-MST has a special structure,
where non-selected nodes ğ‘ƒ\ğ¶are directly connected to centers in
ğ¶. In addition, resiliency of the Resilient-MST routine implies that
 
31KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Sara Ahmadian et al.
the majority of nodes have the same assignment over two close in-
stances. However, to guarantee constant factor approximation, we
have to take care of assignments which use edges of large weight,
as they impose a high cost on the solution. To this end, we pick ğ‘˜
additional centers ğ¶â€²(via a non-resilient algorithm) to serve the
node setğ¿âŠ†ğ‘ƒ\ğ¶incident on the ğœ€ğ‘›heaviest edges of ğ‘‡. The final
solution consists of centers ğ¶âˆªğ¶â€², and assigns each point ğ‘âˆˆğ¿via
ğ¶â€²and each point ğ‘âˆˆğ‘ƒ\ğ¿viağ¶. See Algorithm 1 and Algorithm 2
for details.
Algorithm 1 Resilient-ğ‘˜-Center
Input: Metricğ‘€=(ğ‘ƒ,ğœ‡), and parameters ğ‘˜âˆˆN,ğœ€>0.
Output: Assignment set{(ğ‘,ğœ(ğ‘))forğ‘âˆˆğ‘ƒ}.
1:ğ¶â†a set of 2ğ‘˜log(1/ğœ€)random nodes in ğ‘ƒ.
2:ğ¸â†{(ğ‘,ğ‘)|ğ‘,ğ‘âˆˆğ‘ƒand|{ğ‘,ğ‘}âˆ©ğ¶|â‰ 0}.
3:ğ‘¤(ğ‘,ğ‘)â† 0ifğ‘,ğ‘âˆˆğ¶.
4:ğ‘¤(ğ‘,ğ‘)â†ğœ‡(ğ‘,ğ‘)otherwise.
5:ğ»â†weighted graph(ğ‘ƒ,ğ¸,ğ‘¤).
6:ğ‘‡â†Resilient-MST(ğ»).
7:ğœ(ğ‘)â†ğ‘for allğ‘âˆˆğ¶.
8:ğœ(ğ‘)â†ğ‘ifğ‘âˆˆğ‘ƒ\ğ¶and(ğ‘,ğ‘)âˆˆğ‘‡.
9:ğ¿â†vertices inğ‘ƒ\ğ¶incident to the ğœ€ğ‘›heaviest edges of ğ‘‡.
10:ğ¶â€²â†centers selected by [27] on ğ‘ƒwithğ‘˜.
11:ğœ(ğ‘)â†ğ‘â€²forğ‘âˆˆğ¿, whereğ‘â€²is the closest center of ğ¶â€²toğ‘.
12:return{(ğ‘,ğœ(ğ‘))forğ‘âˆˆğ‘ƒ}.
Algorithm 2 Resilient-MST
Input: Graphğº(ğ‘‰,ğ¸,ğ‘¤), bucketizing parameter ğœ†.
Output: A spanning tree ğ‘‡.
1:foreach edgeğ‘’âˆˆğ¸do
2:ğ›¼â†random number in [0,1)
3: ifğ‘¤(ğ‘’)=0then
4:ğ‘¤â€²(ğ‘’)â†ğ‘¤(ğ‘’)
5: else
6:ğ‘–â†âŒˆğ›¼+logğœ†ğ‘¤(ğ‘’)âŒ‰
7:ğ‘¤â€²(ğ‘’)â†ğœ†ğ‘–âˆ’ğ›¼
8:ğ‘‡â†Kruskal(ğ‘‰,ğ¸,ğ‘¤â€²)âŠ²apply consistent tie-breaking rule
9:returnğ‘‡
The main result of this section is the following.
Theorem 3.1. For any 1>ğœ€>3 lnğœ†ln2ğ‘›
ğ‘›, Algorithm 1 is a 7.78-
resilient algorithm for ğ‘˜-center with bicriteria approximation factor
4,1+2 log1
ğœ€
with probability at least 1âˆ’ğ‘‚
1
ğ‘›
. Furthermore the
running time of the algorithm is ğ‘‚(ğ‘›ğ‘˜logğ‘›log(1/ğœ€)).
Analysis. In the rest of this section, we prove the correctness
of the above algorithm. The first step of our analysis is to analyze
resilience properties of the Resilient-MST sub-routine. Before
doing that we introduce the definition of ğœ€-close and resiliency also
for the MST problem.
Definition 3.2. [ğœ€-close graphs] Two weighted graphs ğº(ğ‘‰,ğ¸,ğ‘¤ğº)
andğ»(ğ‘‰,ğ¸,ğ‘¤ğ»)are calledğœ€-close if and only if for each edge ğ‘’âˆˆğ¸,ifğ‘¤ğº(ğ‘’)â‰ 0, then
1
1+ğœ€<ğ‘¤ğ»(ğ‘’)
ğ‘¤ğº(ğ‘’)<1+ğœ€,
and ifğ‘¤ğº(ğ‘’)=0, thenğ‘¤ğ»(ğ‘’)=0.
Definition 3.3. [ğ›¾-resilient for MST] Algorithm Ais anğ›¾-resilient
MST algorithm if and only if the outputs A(ğº)andA(ğ»)differ by
at most ağ›¾ğœ€-fraction, i.e.,
|A(ğº)â–³A(ğ»)|â‰¤ğ›¾ğœ–|A(ğº)|,
for anyğœ€-close graphs ğº(ğ‘‰,ğ¸,ğ‘¤ğº)andğ»(ğ‘‰,ğ¸,ğ‘¤ğ»).
We are now ready to state the main properties of Resilient-MST
whose proof is deferred to Section 3.1.
Theorem 3.4. For any 1>ğœ€>3 lnğœ†ln2ğ‘›
ğ‘›andğœ†>1+ğœ€, with
probability 1âˆ’ğ‘‚(1/ğ‘›),Resilient-MST is a(4(1+ğ‘œ(1))/lnğœ†)-resilient
ğœ†-approximation minimum spanning tree algorithm. In addition,
(1)minğ‘’âˆˆÎ“ğ‘‡(ğ‘£)ğ‘¤(ğ‘’)â‰¤ğœ†minğ‘’âˆˆÎ“ğº(ğ‘£)ğ‘¤(ğ‘’)for anyğ‘£âˆˆğ‘‰; and
(2)any two vertices with a zero-cost path in ğºhave a zero-cost
path inğ‘‡.
Our main result (Theorem 3.1) then follows from combining two
key lemmas (Lemma 3.5, Lemma 3.6) and two simple observations.
We start by arguing that all the points in ğ‘ƒare assigned to a
center inğ¶âˆªğ¶â€². Notice that all ğ‘have an edge in the MST. Also
observe that for any edge at least one of the end points is a point
inğ¶, therefore each point is assigned to a center in ğ¶based on the
solution of the MST and it can be only changed to a center in ğ¶â€²
later on. Therefore the returned assignment is indeed feasible.
Next we bound the running time of our algorithm by ğ‘‚(ğ‘›ğ‘˜log1/ğœ€).
In fact, the set ğ¶can be easily sampled in time ğ‘‚(ğ‘›logğ‘›). The
auxiliary graph ğ»has at most ğ‘‚(ğ‘›ğ‘˜log1/ğœ€)edges, so Resilient-
MST has running time ğ‘‚(ğ‘›ğ‘˜logğ‘›log1/ğœ€). Finally the algorithm
in [27] has running time ğ‘‚(ğ‘›ğ‘˜). So the overall running time is
ğ‘‚(ğ‘›ğ‘˜logğ‘›log 1/ğœ€).
We are now ready to prove approximation and resiliency guar-
antees of our algorithm.
Lemma 3.5. For any 1>ğœ€>3 lnğœ†ln2ğ‘›
ğ‘›, Algorithm 1 is a bi-
criteria
4,1+2 log1
ğœ€
-approximation for ğ‘˜-center with probability
1âˆ’ğ‘‚
1
ğ‘›
.
Proof. First note that the set of open centers is ğ¶âˆªğ¶â€²which has
sizeğ‘˜+2ğ‘˜log(1/ğœ€)=ğ‘˜(1+2log(1/ğœ€)). So it remains to establish the
approximation guarantee. Let ğ‘ŸOPT5be the optimum solution to the
input. Clearly ğœ‡(ğ‘,ğœ(ğ‘))â‰¤ 2ğ‘ŸOPTforğ‘âˆˆğ¿since this assignment
is an output of a 2-approximation algorithm on the input points.
To bound the radius for the points in ğ‘ƒ\ğ¿and conclude the proof,
we argue that the ğ‘›(1âˆ’ğœ€)lightest edges of ğ‘‡are at distance 2ğ‘ŸOPT
fromğ¶w.h.p. The key observation is that large clusters contain
with high probability at least a sampled point in ğ¶and that any two
points belonging to the same cluster are at distance at most 2ğ‘ŸOPT
from each other. Formally, let ğ‘ˆâŠ†ğ‘ƒbe the subset of points ğ‘such
5We useğ‘Ÿğ‘‚ğ‘ƒğ‘‡ as both the optimum solution and its value when it is clear from the
context.
 
32Resilient k-Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
thatğœ‡(ğ‘,ğ¶)>2ğ‘ŸOPTand letğ¶âˆ—
1,ğ¶âˆ—
2,...,ğ¶âˆ—
ğ‘˜be the set of clusters of
an optimum solution.
Letğ‘Œğ‘–forğ‘–âˆˆ[ğ‘›]be a binary random variable that has value 1 if
and only if the the i-th point in ğ‘ƒis at distance larger 2ğ‘ŸOPTfrom
ğ¶and letğ‘‹ğ‘–forğ‘–âˆˆ[ğ‘˜]be a binary random variable that has value
1 if and only if ğ¶âˆ—
ğ‘–âˆ©ğ¶=âˆ…. We bound the expected size of ğ‘ˆas
follows.
E[|ğ‘ˆ|]=E"ğ‘›Ã•
ğ‘–=1ğ‘Œğ‘–#
â‰¤E"ğ‘˜Ã•
ğ‘–=1ğ‘‹ğ‘–|ğ¶âˆ—
ğ‘–|#
â‰¤ğ‘˜Ã•
ğ‘–=1
1âˆ’|ğ¶âˆ—
ğ‘–|
ğ‘›|ğ¶|
|ğ¶âˆ—
ğ‘–|
=ğ‘›ğ‘˜Ã•
ğ‘–=1
1âˆ’|ğ¶âˆ—
ğ‘–|
ğ‘›|ğ¶||ğ¶âˆ—
ğ‘–|
ğ‘›â‰¤ğ‘›exp(âˆ’2ğ‘˜log(1/ğœ€)/ğ‘˜)=ğœ€2ğ‘›,
where the second equality follows from the fact that each point
inğ¶is chosen uniformly at random and ğ‘‹ğ‘–=1with probability 1âˆ’|ğ¶âˆ—
ğ‘–|/ğ‘›|ğ¶|, and the last inequality follows from Claim B.1 (see
Appendix B), the fact that {ğ¶âˆ—
ğ‘–}ğ‘–is a partition of ğ‘ƒand that|ğ¶|>0.
Using Chernoff bounds and the fact that ğœ€>logğ‘›
ğ‘›, we then get that
|ğ‘ˆ|â‰¤ğœ€ğ‘›with probability at least 1âˆ’1/ğ‘›.
So it follows that with probability at least 1âˆ’1/ğ‘›, the points
inğ‘ƒ\ğ¿have a center in ğ¶at distance no more than 2ğ‘ŸOPT. Then
by Theorem 3.4 and by fixing ğœ†=2, we get that Resilient-MST
guarantees that each vertex in ğ‘ƒ\ğ¿has an edge of weight at most
4ğ‘ŸOPT. â–¡
Next we study the resiliency of Algorithm 1.
Lemma 3.6. For any 1>ğœ€>3 lnğœ†ln2ğ‘›
ğ‘›, Algorithm 1 is 7.78-resilient
forğ‘˜-center with probability 1âˆ’ğ‘‚
1
ğ‘›
.
Proof. Let superscripts(1)and(2)specify inputs, internal ar-
tifacts and outputs of the two runs on different data sets that are
ğœ€-close. When these artifacts are the same, we may drop the super-
script. Note that the algorithm uses the same source of randomness,
soğœ‹(ğ¶(1))=ğ¶(2). The edge weights ğ‘¤(1)andğ‘¤(2)are defined
using the distance functions ğœ‡(1)andğœ‡(2)on point sets ğ‘ƒ(1)and
ğ‘ƒ(2)=ğœ‹(ğ‘ƒ(1)), assumingğœ‹is the corresponding bijection between
the two input instances. Nevertheless, the closeness assumption on
the inputs guarantees that 1/(1+ğœ€)â‰¤ğ‘¤(2)(ğœ‹(ğ‘),ğœ‹(ğ‘))/ğ‘¤(1)(ğ‘,ğ‘)â‰¤
(1+ğœ€), hence the two graphs ğœ‹(ğ»(1))andğ»(2)areğœ€-close.
We assign points to centers in two parts in Algorithm 1. One
based on the resilient tree algorithm in lines 7,8 of Algorithm 1.
Theorem 3.4 (fixing ğœ†=2) guarantees that the resulting trees
ğœ‹(ğ‘‡(1))andğ‘‡(2)are5.78ğœ€-close. That is, they may differ in up to
5.78ğœ€ğ‘›edges. Therefore we introduce at most 5.78ğœ–ğ‘›points assigned
to a different center. Moreover the second place that we assign
points to centers is in line 11 of Algorithm 1. Using the fact that
with probability 1âˆ’1
ğ‘›, the size of the sets ğ¿(1)andğ¿(2)are smaller
thanğœ€ğ‘›, we have that even if all assignments on ğœ‹(ğ¿(1))andğ¿(2)
are different, there are at most 7.78ğœ€ğ‘›different assignments with
probability 1âˆ’3
ğ‘›. â–¡
3.1 Resilient Minimum Spanning Tree
In this section, we focus on our results for the MST problem, and
show a resilient algorithm for it that will be used as subroutine to
compute our resilient clustering algorithm.We first give a high-level idea of our approximation algorithm
for MST and the pseudocode of our algorithm then we analyze our
algorithm and show how it achieves resiliency.
High-level idea of the algorithm The main idea behind our
MST algorithm (namely, Resilient-MST ) is a discretization trick. In-
tuitively when the weights of two edges are not far from each other,
we can think of them as having the same weight in order to be re-
silient to small changes of their weights across the ğœ€-close instances.
More formally, our algorithm modifies weight ğ‘¤(ğ‘’)of each edge
into someğ‘¤â€²(ğ‘’)â‰¥ğ‘¤(ğ‘’), and then invokes any MST algorithm (with
consistent tie-breaking) on the new graph. Specifically, we pick a
parameter 0â‰¤ğ›¼<1for each edge of positive weight uniformly
at random, independently of other edges, and set ğ‘¤â€²(ğ‘’)=ğœ†ğ‘–âˆ’ğ›¼for
the smallest integer ğ‘–that guarantees ğ‘¤â€²(ğ‘’)â‰¥ğ‘¤(ğ‘’). Ifğ‘¤(ğ‘’)=0,
we setğ‘¤â€²(ğ‘’)=ğ‘¤(ğ‘’)=0. See Algorithm 2 for the pseudo-code.
Our analysis of this algorithm leads to Theorem 3.4, which is also
restated here.
Theorem 3.4. For any 1>ğœ€>3 lnğœ†ln2ğ‘›
ğ‘›andğœ†>1+ğœ€, with
probability 1âˆ’ğ‘‚(1/ğ‘›),Resilient-MST is a(4(1+ğ‘œ(1))/lnğœ†)-resilient
ğœ†-approximation minimum spanning tree algorithm. In addition,
(1)minğ‘’âˆˆÎ“ğ‘‡(ğ‘£)ğ‘¤(ğ‘’)â‰¤ğœ†minğ‘’âˆˆÎ“ğº(ğ‘£)ğ‘¤(ğ‘’)for anyğ‘£âˆˆğ‘‰; and
(2)any two vertices with a zero-cost path in ğºhave a zero-cost
path inğ‘‡.
Analysis. The analysis consists of several pieces. First, we show
that the output of Resilient-MST has the desired approximation
guarantee. Then we show that Kruskalâ€™s algorithm run on two
instances differing on only one edgeâ€™s weight produces trees which
differ in at most two edges. Finally, we argue that for ğœ€-close in-
stances with weights ğ‘¤(1)andğ‘¤(2), there are at most ğ‘‚(ğœ€ğ‘›)edges
with different discretized weights ğ‘¤â€²(1)â‰ ğ‘¤â€²(2)across the runs
with high probability. Combining these propositions yields the
statement of the Theorem 3.4. We start with the following lemma.
Lemma 3.7. Letğ‘‡be the output of Resilient-MST on graphğº, then
ğ‘‡is anğœ†-approximate MST. Moreover,
(1)minğ‘’âˆˆÎ“ğ‘‡(ğ‘£)ğ‘¤â€²(ğ‘’)â‰¤ğœ†minğ‘’âˆˆÎ“ğº(ğ‘£)ğ‘¤(ğ‘’)for anyğ‘£âˆˆğ‘‰; and
(2)any two vertices with a zero-cost path in ğºhave a zero-cost
path inğ‘‡.
Proof. No edge weight decreases in the discretization process,
and each edgeâ€™s weight increase by at most a factor ğœ†, hence the
optimum solution changes by less than a factor ğœ†. One such solution
is found by Kruskalâ€™s algorithm.
Recall that Kruskalâ€™s algorithm iterates over edges in increasing
order of weight, and picks each edge if and only if it does not form
a cycle with the previously picked edges. Fix a vertex ğ‘£, and letğ‘’ğ‘£
be the first edge incident on ğ‘£encountered by Kruskalâ€™s algorithm
on the graph with modified weight. This edge is picked because it
does not form a cycle. Let ğ‘’âˆ—=arg minğ‘’â€²âˆˆÎ“ğº(ğ‘£)ğ‘¤(ğ‘’â€²)be the lowest-
weight edge incident on ğ‘£inğº. The sort order of Kruskal implies
thatğ‘¤â€²(ğ‘’ğ‘£)â‰¤ğ‘¤â€²(ğ‘’âˆ—)<ğœ†ğ‘¤(ğ‘’âˆ—).
Finally note that edges of weight zero do not change their weight
during discretization, and the other edges have positive weight. So
weight-zero edges are considered first by Kruskalâ€™s algorithm. Thus,
any vertices having a path of zero cost in the input graph will have
a path of cost zero in the output tree as well. â–¡
 
33KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Sara Ahmadian et al.
Next, we see how Resilient-MST handles two graphs that only
differ over the weight of one edge. We basically show that Resilient-
MST produces solutions that may differ by at most two edges.
Lemma 3.8. Letğº(ğ‘‰,ğ¸,ğ‘¤ğº)andğ»(ğ‘‰,ğ¸,ğ‘¤ğ»)be two graphs where
ğ‘¤ğ»andğ‘¤ğºonly disagree on the weight of an edge ğ‘’âˆ—âˆˆğ¸and letğ‘‡ğº
andğ‘‡ğ»be the outputs of Resilient-MST onğºandğ», respectively.
Then|ğ‘‡ğºâ–³ğ‘‡ğ»|â‰¤2.
Proof. Assume without loss of generality that ğ‘¤ğº(ğ‘’âˆ—)<ğ‘¤ğ»(ğ‘’âˆ—).
The statement is trivial when ğ‘’âˆ—âˆ‰ğ‘‡ğºor whenğ‘’âˆ—âˆˆğ‘‡ğ». Now sup-
poseğ‘’âˆ—âˆˆğ‘‡ğºandğ‘’âˆ—âˆ‰ğ‘‡ğ».
With consistent tie-breaking, we can assume that edge weights
are distinct, which implies uniqueness of MST for any subset of
vertices, so we can talk about theMST or thelightest edge in a set.
It is well-known (see, e.g., Theorem 23.1 in [ 18]) that the lightest
edge in any cut6of the graph belongs to the MST. In fact, the generic
algorithmic introduced in [ 18] iteratively finds and contracts7an
edge that belongs to the MST.
Letğ‘’âˆ—=(ğ‘¢,ğ‘£), and letğ‘‰1,ğ‘‰2âŠ†ğ‘‰be the set of vertices reachable
inğ‘‡ğº\{ğ‘’âˆ—}fromğ‘¢andğ‘£, respectively. Clearly ğ‘‰1âˆ©ğ‘‰2=âˆ…and
ğ‘‰1âˆªğ‘‰2=ğ‘‰. Letğºâˆ—be the result of contracting edge ğ‘’âˆ—inğºand
removing self-loops. Let ğ‘‡âˆ—be the unique MST of ğºâˆ—. The lightest
edge in the cut(ğ‘‰1,ğ‘‰2)forğºisğ‘’âˆ—. Letğ‘’+be the lightest edge in
the cut(ğ‘‰1,ğ‘‰2)forğ». Thenğ‘‡âˆ—âˆª{ğ‘’âˆ—}is the unique MST of ğº
andğ‘‡âˆ—âˆª{ğ‘’+}is the unique MST of ğ». They differ in exactly two
edges. â–¡
The final ingredient for proving our main result is to show that
the number of edges with modified discretized weight is in fact
bounded due to discretization. It is simpler to do this for the case
that the input edge weights only increase when going from an input
graphğº(1)to anotherğº(2). In the main proof, we show how to
handle this restriction for general change.
Lemma 3.9. Letğº(1)andğº(2)beğœ€-close graphs with vertices ğ‘‰,
edgesğ¸, and respective edge weight functions ğ‘¤(1)andğ‘¤(2)such
thatğ‘¤(1)(ğ‘’)â‰¤ğ‘¤(2)(ğ‘’)for all edges ğ‘’and1>ğœ€>3 lnğœ†ln2ğ‘›
ğ‘›. Let
ğ‘¤â€²(1),ğ‘¤â€²(2)denote the corresponding discretized edge weights. With
probability 1âˆ’1/ğ‘›, we have
|{ğ‘’âˆˆğ¸|ğ‘¤â€²(1)(ğ‘’)â‰ ğ‘¤â€²(2)(ğ‘’)}|â‰¤ğœ€ğ‘›(1+ğ‘œ(1))/lnğœ†,
whereğœ†is the same value defined in Theorem 3.4.
Proof of Lemma 3.9. Letğ‘‹ğ‘’be a binary random variable indi-
cating whether the discretized weight of ğ‘’âˆˆğ¸differs in the two
runs: i.e.,ğ‘¤â€²(1)(ğ‘’)â‰ ğ‘¤â€²(2)(ğ‘’). Thenğ‘‹=Ã
ğ‘’âˆˆğ¸ğ‘‹ğ‘’is the quantity
we mean to upperbound.
Letğ›¼be the parameter we select for edge ğ‘’in the algorithm. In
case ofğ‘‹ğ‘’=1, we have,
ğ‘¤(1)(ğ‘’)â‰¤ğœ†ğ‘–âˆ’ğ›¼<ğ‘¤(2)(ğ‘’)<(1+ğœ€)ğ‘¤(1)(ğ‘’)
for some integer ğ‘–. Take logarithms and substitute ğ¿ğ‘¤=logğœ†ğ‘¤(1)(ğ‘’)
andğ¿ğœ€=logğœ†(1+ğœ€)to get
ğ¿ğ‘¤â‰¤ğ‘–âˆ’ğ›¼<ğ¿ğ‘¤+ğ¿ğœ€. (1)
6The cut(ğ‘ˆ,ğ‘‰\ğ‘ˆ)inğºconsists of all edges of ğºwith exactly one endpoint in ğ‘ˆ.
7Contracting an edge (ğ‘¢,ğ‘£)inğº(ğ‘‰,ğ¸)and removing the self-loops results in a graph
ğ»(ğ‘‰\{ğ‘¢,ğ‘£}âˆª{ğ‘£âˆ—},ğ¸â€²)whereğ¸â€²={(ğ‘¢â€²,ğ‘£â€²)âˆˆğ¸|ğ‘¢â€²,ğ‘£â€²âˆ‰{ğ‘¢,ğ‘£}}âˆª{(ğ‘£âˆ—,ğ‘£â€²)|
(ğ‘¢â€²,ğ‘£â€²)âˆˆğ¸,ğ‘¢â€²âˆˆ{ğ‘¢,ğ‘£},ğ‘£â€²âˆ‰{ğ‘¢,ğ‘£}}.In other words, ğ‘–âˆ’ğ›¼âˆˆ[ğ¿ğ‘¤,ğ¿ğ‘¤+ğ¿ğœ€)for some integer ğ‘–. Letğ‘—=âŒˆğ¿ğ‘¤âŒ‰.
We consider two cases.
(1)Ifğ¿ğ‘¤+ğ¿ğœ€â‰¤ğ‘—, Inequality (1)impliesğ›¼âˆˆ(ğ‘—âˆ’ğ¿ğ‘¤âˆ’ğ¿ğœ€,ğ‘—âˆ’ğ¿ğ‘¤].
(2)Ifğ‘—<ğ¿ğ‘¤+ğ¿ğœ€â‰¤ğ‘—+1, Inequality (1)impliesğ›¼âˆˆ[0,ğ‘—âˆ’ğ¿ğ‘¤]
orğ›¼âˆˆ(ğ‘—+1âˆ’ğ¿ğ‘¤âˆ’ğ¿ğœ€,1].
In both cases, the allowable range of ğ›¼forğ‘‹ğ‘’=1has measure ğ¿ğœ€.
Sinceğ›¼takes uniform random values from [0,1), we get Pr[ğ‘‹ğ‘’=
1]â‰¤ğ¿ğœ€=logğœ†(1+ğœ€)<ğœ€/lnğœ†, where the last inequality follows
from the Maclaurin series of ln(1+ğ‘¥)and notingğœ€<1.
Summing up yields E[ğ‘‹]<ğœ€ğ‘›/lnğœ†. Apply the Chernoff bound
and settingğœ‰=q
3 lnğ‘›lnğœ†
ğœ€ğ‘›gives
Pr[ğ‘‹>(1+ğ‘œ(1))ğœ€ğ‘›/lnğœ†]â‰¤1âˆ’1/ğ‘›
ifğœ€ğ‘›â‰¥3 lnğœ†ln2ğ‘›.
We remark that if we picked one fixed ğ›¼for all edges ğ‘’, we could
not apply the Chernoff bound in the above argument, though the
expected number of changed edge weights would have the desired
bound. â–¡
We are now ready to prove our main theorem.
Proof of Theorem 3.4. Letğº(0)=(ğ‘‰,ğ¸,ğ‘¤(0))be a graph where
ğ‘¤(0)(ğ‘’)=max{ğ‘¤(1)(ğ‘’),ğ‘¤(2)(ğ‘’)}. Letğ‘¤â€²(0),ğ‘¤â€²(1),ğ‘¤â€²(2)denote
the respective discretized weights of ğ‘¤(0),ğ‘¤(1),ğ‘¤(2). Two applica-
tions of Lemma 3.9 (once to the pair ğº(1),ğº(0)and another time to
the pairğº(2),ğº(0)) proves that, with high probability, ğ‘¤â€²(1),ğ‘¤â€²(2)
disagree in ğ‘§â‰¤2ğœ€ğ‘›(1+ğ‘œ(1))/lnğœ†edges. We can then create a
sequence of graphs ğ»0,ğ»1,...,ğ»ğ‘§on the same vertex and edge set
asğº(1)such thatğ»ğ‘–,ğ»ğ‘–+1disagree in exactly one edge weight, for
0â‰¤ğ‘–<ğ‘§;ğ»0uses the weights ğ‘¤â€²(1); andğ»ğ‘§uses the weights
ğ‘¤â€²(2). Withğ‘§applications of Lemma 3.8, we conclude that the
Resilient-MST output trees ğ‘‡(1),ğ‘‡(2)for inputsğº(1),ğº(2)are
4ğœ€(1+ğ‘œ(1))/lnğœ†-resilient. â–¡
4 EXTENSION TO â„“ğ‘CLUSTERING
Algorithm 1 can be easily extended to work for ğ‘˜-median and
ğ‘˜-means, and in general, for any â„“ğ‘-clustering problem with the
cost functionÃ
ğ‘ğœ‡(ğ‘,C(ğ‘))ğ‘, whereC(ğ‘)denotes the center ğ‘is
assigned to.
The only change in Algorithm 1 is in Step 10, where instead
of running the 2-approximation for ğ‘˜-center, we run the best â„“ğ‘-
clustering algorithm: a 2.406-approximation for ğ‘˜-median and a
5.912-approximation for ğ‘˜-means [ 16]. In general, we denote with
ğ›¼ğ‘the best approximation for the ğ‘norm. Given the similarity,
the pseudo-code for the â„“ğ‘clustering algorithm is deferred to the
Appendix.
The resiliency argument is exactly the same as in Lemma 3.6. The
approximation argument differs and is more complicated because
the summations in â„“ğ‘-clustering objective need a more global view
than the uniform radius bounds in ğ‘˜-center.
Theorem 4.1. For any 1>ğœ€>3 lnğœ†ln2ğ‘›
ğ‘›, Algorithm 3 is a 7.78-
resilient 22ğ‘+ğ›¼ğ‘,1+2 log 1/ğœ€-approximation for â„“ğ‘-clustering with
probability 1âˆ’ğ‘‚(1/ğ‘›), whereğ›¼ğ‘is the approximation ratio of the
algorithm applied in Step 10.
 
34Resilient k-Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Proof. First note that the set of open centers is ğ¶âˆªğ¶â€²which
has sizeğ‘˜+2ğ‘˜log(1/ğœ€)=ğ‘˜(1âˆ’2logğœ€). So it remains to establish
the approximation factor of the algorithm. Consider an optimum
â„“ğ‘-clusteringC, and let OPT(ğ‘£)=ğœ‡(ğ‘£,C(ğ‘£))ğ‘, forğ‘£âˆˆğ‘ƒ, be the
contribution of ğ‘£toOPT, so OPT=Ã
ğ‘£OPT(ğ‘£). Letğ¶â€²(ğ‘£), for
ğ‘£âˆˆğ¿, denote the assignment of ğ‘£withinğ¶â€². Similarly define ğ¶(ğ‘£)
forğ‘£âˆˆğ‘ƒ. ClearlyÃ
ğ‘£âˆˆğ¿ğœ‡(ğ‘£,ğ¶â€²(ğ‘£))ğ‘â‰¤ğ›¼ğ‘OPT, as we use the same
numberğ‘˜of centers and the entire input points, but only assign a
subsetğ¿âŠ†ğ‘ƒusing the selected centers. It remains to bound the
cost associated with ğ‘ƒ\ğ¿.
Letğ‘ˆâŠ†ğ‘ƒbe the set of points whose optimum cluster does not
intersectğ¶. We say these points are not â€œcaptured.â€ We showed in
the proof of Lemma 3.5 that E[|ğ‘ˆ|]â‰¤ğœ€ğ‘›with probability 1âˆ’1/ğ‘›.
We next bound the cost due to the captured points.
Claim 4.2. We haveÃ
ğ‘£âˆ‰ğ‘ˆE[ğœ‡(ğ‘£,ğ¶(ğ‘£))ğ‘]â‰¤22ğ‘OPT.
Fixed a point ğ‘£. Letğ‘„be its optimum cluster, and let ğ‘âˆ—âˆˆğ‘„be
the corresponding optimum center. We have
E
ğ¶
ğœ‡(ğ‘£,ğ¶(ğ‘£))ğ‘|ğ¶âˆ©ğ‘„â‰ âˆ…
â‰¤E
ğ¶
(2ğœ‡(ğ‘£,ğ¶))ğ‘|ğ¶âˆ©ğ‘„â‰ âˆ…
from Lemma 3.7,
â‰¤2ğ‘E
ğ¶
ğœ‡(ğ‘£,ğ¶)ğ‘|ğ¶âˆ©ğ‘„â‰ âˆ…
â‰¤22ğ‘âˆ’1E
ğ¶
ğœ‡(ğ‘£,ğ‘âˆ—)ğ‘+ğœ‡(ğ‘âˆ—,ğ¶)ğ‘|ğ¶âˆ©ğ‘„â‰ âˆ…
by the triangle inequality (i.e., ğœ‡(ğ‘£,ğ¶)â‰¤ğœ‡(ğ‘£,ğ‘âˆ—)+ğœ‡(ğ‘âˆ—,ğ¶)) and
(ğ‘+ğ‘)ğ‘â‰¤2ğ‘âˆ’1(ğ‘ğ‘+ğ‘ğ‘)forğ‘,ğ‘â‰¥0which follows from the
convexity of ğ‘¥ğ‘forğ‘â‰¥1,
â‰¤22ğ‘âˆ’1E
ğ¶
ğœ‡(ğ‘£,ğ‘âˆ—)ğ‘|ğ¶âˆ©ğ‘„â‰ âˆ…
+22ğ‘âˆ’1E
ğ¶
ğœ‡(ğ‘âˆ—,ğ¶)ğ‘|ğ¶âˆ©ğ‘„â‰ âˆ…
â‰¤22ğ‘âˆ’1E
ğ¶
ğœ‡(ğ‘£,ğ‘âˆ—)ğ‘|ğ¶âˆ©ğ‘„â‰ âˆ…
+22ğ‘âˆ’1E
ğ¶
ğœ‡(ğ‘âˆ—,ğ¶)ğ‘|ğ¶âˆ©ğ‘„=1
=22ğ‘âˆ’1OPT(ğ‘£)+22ğ‘âˆ’1
|ğ‘„|Ã•
ğ‘âˆˆğ‘„OPT(ğ‘).
Summing the first term over all ğ‘£âˆ‰ğ‘ˆgivesÃ
ğ‘£âˆ‰ğ‘ˆ22ğ‘âˆ’1OPT(ğ‘£)â‰¤
22ğ‘âˆ’1Ã
ğ‘£OPT(ğ‘£)=OPT. Summing up over all ğ‘£âˆ‰ğ‘ˆfor the second
term gives an upper bound of 2ğ‘âˆ’1OPT, since each expression is
repeated at most|ğ‘„|times, once for each ğ‘£âˆˆğ‘„\ğ‘ˆ. Putting these
together proves the claim.
Recall thatğ¿is the set of the ğœ€ğ‘›â‰¥|ğ‘ˆ|points with the largest
contribution to cost in ğ¶. Thus, the cost of ğ¶associated with ğ‘ƒ\ğ¿
is
E
ğ¶ï£®ï£¯ï£¯ï£¯ï£¯ï£°Ã•
ğ‘£âˆˆğ‘ƒ\ğ¿ğœ‡(ğ‘£,ğ¶(ğ‘£))ğ‘ï£¹ï£ºï£ºï£ºï£ºï£»â‰¤E
ğ¶ï£®ï£¯ï£¯ï£¯ï£¯ï£°Ã•
ğ‘£âˆˆğ‘ƒ\ğ‘ˆğœ‡(ğ‘£,ğ¶(ğ‘£))ğ‘ï£¹ï£ºï£ºï£ºï£ºï£»â‰¤22ğ‘OPT,
where the last step follows from Claim 4.2. Combining this with
the bound on the cost of ğ¶â€²from above, we prove the lemma. â–¡
5 EMPIRICAL EVALUATION
In this section, we empirically evaluate our algorithms on real-world
datasets containing user locations from SNAP8and Kaggle9and
also synthetic 2-d datasets known as Birch-sets [ 48]. Our empirical
analysis focuses on the resilient ğ‘˜-center algorithm (Section 3). We
8https://snap.stanford.edu
9https://www.kaggle.comfirst describe the datasets we use, then we explain the baselines we
benchmark and how we measure the quality of clustering solutions,
and finally we present our empirical results.
5.1 Datasets
We first describe the real-world datasets and then explain the syn-
thetic dataset.
Real-world datasets. These datasets all capture human mobil-
ity. We consider two online location-based social networks, Gowalla
and Brightkite from SNAP, and one location-based dataset derived
from Uber pick-ups from Kaggle. All datasets represent points as
Geo location data (with latitude and longitude) associated with a
timestamp. We transform these Geo locations to ğ‘…3by converting
angular Geo location to Cartesian coordinates.
Gowalla and Brightkite.10Both datasets collect the public check-
in data where Gowalla contains data from Feb. 2009 to Oct. 2010,
and Brightkite contains data from Apr. 2008 to Oct. 2010. The total
number of check-ins is 6.4 million for Gowalla and 4.5 million for
Brightkite. They also include social network showing friendship
links between users which we do not use in our experiments.
We obtainğœ–-close point sets by considering average location of
each user in the first and second week of Jan. 2010 (for each dataset).
We discard users that donâ€™t have this information.
Uber.11The original dataset contains around 18.8 million Uber
pickups in New York City from Apr. to Jun. 2015. We only considers
these pickup locations over the first and the second day of Jun. 2014.
Note that this is raw data and we donâ€™t have ids associated with
these Geo locations. In order to form our ğœ–-close datasets, we find
a minimum weight perfect matching between points of these two
days. We remove unmatched locations and also matched locations
located more than 1 km apart. We assign the same id to matched
pairs and construct one dataset corresponding to each day.
Synthetic datasets. We work with Birch-sets [ 48] that contain
100k points in 2-d and they are basically collection of 100 clusters
located in a regular grid (see visualization on website12). In order
to generateğœ–-close datasets, we move each point of this dataset by
a random noise from a normal (Gaussian) distribution with mean
of 0.5 and standard deviation of 0.5.
5.2 Experimental Setup
5.2.1 Baselines. In the experiments we evaluate the performance
of our resilient ğ‘˜-center algorithm against the following two well-
known baselines.
â€“ Gonzalez Algorithm [ 26] starts with a random center and repeats
the following operations ğ‘˜âˆ’1times: find the point with maximum
distance to the closest center and add it to the set of centers. We
refer to this algorithm as Gonz.
â€“ Carving Algorithm [ 33,43] finds the smallest value ğ‘…such that
the following algorithm opens at most ğ‘˜centers: while there exist
some uncovered points in the instance, pick an uncovered point at
random, add it to the set of centers, and mark all points (including
the center) with distance at most ğ‘…from this new center as covered.
10https://snap.stanford.edu/data/index.html#locnet
11https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city
12https://cs.joensuu.fi/sipu/datasets/
 
35KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Sara Ahmadian et al.
This is intuitively equivalent to carving out balls of radius ğ‘…from
the instance and hence we refer to this algorithm as Carv.
5.2.2 Implementation details and parameters of the algorithm. We
consider 8different implementations of our algorithm (Algorithm 1)
based on the utilized classical ğ‘˜-center algorithm in the second
opening stage and the number of centers that we open in each of two
stages of our algorithm. Recall that our algorithm first opens a set of
random centers and then opens additional centers using a classical
ğ‘˜-center problem. In our configurations, we either open ğ‘˜/2orğ‘˜
centers for each stage and for the choice of classical algorithm, we
consider Gonz andCarv. We utilize the notation â€œConG( ğ›¼,ğ›½)â€ and
â€œConC(ğ›¼,ğ›½)â€ to denote the variant of our algorithm that opens ğ›¼ğ‘˜
centers randomly and constructs a solution with ğ›½ğ‘˜centers using
Gonz algorithm and Carv algorithm, respectively.
5.2.3 Measure of Quality. We evaluate the following measures of
quality for comparing the two clustering solutions produced for
given distance matrices.
â€“Fraction of points that change cluster. This is the fraction of
points that are assigned to different centers between the two solu-
tions.
â€“Solution Cost. The maximum distance between a point and its
assigned center, i.e., the ğ‘˜-center cost.
â€“Number of Clusters (or open centers) . Recall that our algo-
rithm opens centers in two stages and may open upto (ğ›¼+ğ›½)ğ‘˜
centers.
5.3 Experimental Results
Here we compare the quality of the output of our algorithm with
that of the baselines. We present results of four experiments for
ğ‘˜âˆˆ{10,20,50,100}, andğœ†âˆˆ{1.1,1.001}(Fig. 1, Fig. 2, Fig. 3, and
Fig. 4) in the main body and defer more results for different values
ofğ‘˜andğœ†to Appendix C. Recall that ğ‘˜is the number of centers
andğœ†is the parameter of the algorithm in Algorithm 2.
First, we evaluate the fraction of points that change cluster for the
given two distance matrices. Note that in all datasets and settings,
our algorithm achieves significantly higher resilience. In some cases,
e.g., Fig. 1, the baseline may reassign close to 90%of the points where
our algorithm reassigns less than 10%. Ensuring resilience becomes
harder when we allow more centers (e.g., ğ‘˜=50andğ‘˜=100),
but we still observe reassignment of only 20-30% of points by our
algorithm versus 95-99% by the baselines.
In terms of the cost of produced solutions, in order to guarantee
resilience, we sometimes produce solutions with higher costs (up
to twice the cost of the baseline). However our cost is usually
comparable to the baseline and the gap decreases as we allow more
centers.
Similarly in terms of open centers, depending on our configura-
tion we may open more centers, however for some configuration,
despite having the flexibility of opening more centers, our algo-
rithm actually terminates before reaching the bound on the open
centers, e.g., ConC(1.0,1.0) in Fig. 2.
For instance, in Fig. 1 for ğ‘˜=10, the number of clusters in the
solution constructed and the cost of solution by ConG(0.5,1.0) is
slightly more than the baselines (10% to30%). On the other hand,
the center assigned to the points for more than 93%and97%of thepoints alters for the Gonzalez and Carving baselines, respectively,
which is around a factor 30more compared to ConG(0.5,1.0) where
3%of the centers assigned to points alters. Further experiments
with different values of ğ‘˜andğœ†are provided in Appendix C.
CONCLUSION AND FUTURE WORKS
We introduce a new notion of algorithmic resilience and design
new algorithms for classic unsupervised learning problems such as
ğ‘˜-center,ğ‘˜-median,ğ‘˜-means. It is an interesting open problem to
improve our algorithms and in particular to obtain non-bicriteria
algorithms for these problems, and also prove lower bounds (hard-
ness results). It would be also interesting to study other classic data
mining problems under the same notion of resiliency.
REFERENCES
[1]Pankaj K. Agarwal, Hsien-Chih Chang, Kamesh Munagala, Erin Taylor, and Emo
Welzl. 2020. Clustering Under Perturbation Stability in Near-Linear Time. In 40th
IARCS Annual Conference on Foundations of Software Technology and Theoretical
Computer Science, FSTTCS 2020, December 14-18, 2020, BITS Pilani, K K Birla Goa
Campus, Goa, India (Virtual Conference) (LIPIcs), Nitin Saxena and Sunil Simon
(Eds.), Vol. 182. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 8:1â€“8:16.
https://doi.org/10.4230/LIPICS.FSTTCS.2020.8
[2]Charu C. Aggarwal and Chandan K. Reddy. 2013. Data Clustering: Algorithms
and Applications (1st ed.). Chapman Hall/CRC.
[3]Aris Anagnostopoulos, Ravi Kumar, Mohammad Mahdian, Eli Upfal, and Fabio
Vandin. 2012. Algorithms on evolving graphs. In Innovations in Theoretical
Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012, Shafi Goldwasser
(Ed.). ACM, 149â€“160. https://doi.org/10.1145/2090236.2090249
[4]Pranjal Awasthi, Avrim Blum, and Or Sheffet. 2012. Center-based clustering
under perturbation stability. Inform. Process. Lett. 112, 1-2 (2012), 49â€“54.
[5]Lars Backstrom, Cynthia Dwork, and Jon M. Kleinberg. 2011. Wherefore art
thou R3579X?: anonymized social networks, hidden patterns, and structural
steganography. Commun. ACM 54, 12 (2011), 133â€“141. https://doi.org/10.1145/
2043174.2043199
[6]Maria-Florina Balcan, Nika Haghtalab, and Colin White. 2020. k-center Clustering
under Perturbation Resilience. ACM Trans. Algorithms 16, 2 (2020), 22:1â€“22:39.
https://doi.org/10.1145/3381424
[7]Maria-Florina Balcan, Yingyu Liang, and Pramod Gupta. 2014. Robust hierarchical
clustering. The Journal of Machine Learning Research 15, 1 (2014), 3831â€“3871.
[8]Yonatan Bilu and Nathan Linial. 2012. Are Stable Instances Easy? Comb. Probab.
Comput. 21, 5 (2012), 643â€“660. https://doi.org/10.1017/S0963548312000193
[9]Deeparnab Chakrabarty and Maryam Negahbani. 2023. Robust k-center with
two types of radii. Mathematical Programming 197, 2 (2023), 991â€“1007.
[10] TH Hubert Chan, Arnaud Guerqin, and Mauro Sozio. 2018. Fully dynamic k-
center clustering. In Proceedings of the 2018 World Wide Web Conference. 579â€“587.
[11] Moses Charikar, Chandra Chekuri, TomÃ¡s Feder, and Rajeev Motwani. 1997.
Incremental clustering and dynamic information retrieval. In Proceedings of the
twenty-ninth annual ACM symposium on Theory of computing. 626â€“635.
[12] Chandra Chekuri and Shalmoli Gupta. 2018. Perturbation Resilient Cluster-
ing for k-Center and Related Problems via LP Relaxations. In Approximation,
Randomization, and Combinatorial Optimization. Algorithms and Techniques,
APPROX/RANDOM 2018, August 20-22, 2018 - Princeton, NJ, USA (LIPIcs), Eric
Blais, Klaus Jansen, JosÃ© D. P. Rolim, and David Steurer (Eds.), Vol. 116. Schloss
Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 9:1â€“9:16. https://doi.org/10.4230/
LIPICS.APPROX-RANDOM.2018.9
[13] H. Chernoff. 1952. A Measure of Asymptotic Efficiency for Tests of a Hypothesis
Based on the sum of Observations. AMS 23, 4 (1952), 493 â€“ 507.
[14] Flavio Chierichetti, Alessandro Panconesi, Giuseppe Re, and Luca Trevisan.
2022. Spectral Robustness for Correlation Clustering Reconstruction in Semi-
Adversarial Models. In International Conference on Artificial Intelligence and
Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event (Proceedings of Ma-
chine Learning Research), Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel
Valera (Eds.), Vol. 151. PMLR, 10852â€“10880. https://proceedings.mlr.press/v151/
chierichetti22a.html
[15] Antonio Emanuele CinÃ , Alessandro Torcinovich, and Marcello Pelillo. 2022. A
black-box adversarial attack for poisoning clustering. Pattern Recognit. 122 (2022),
108306. https://doi.org/10.1016/j.patcog.2021.108306
[16] Vincent Cohen-Addad, Hossein Esfandiari, Vahab S. Mirrokni, and Shyam
Narayanan. 2022. Improved approximations for Euclidean k-means and k-median,
via nested quasi-independent sets. In STOC â€™22: 54th Annual ACM SIGACT Sympo-
sium on Theory of Computing, Rome, Italy, June 20 - 24, 2022, Stefano Leonardi and
Anupam Gupta (Eds.). ACM, 1621â€“1628. https://doi.org/10.1145/3519935.3520011
 
36Resilient k-Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
ğ‘˜=10 ğ‘˜=20âˆ’0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
ğ‘˜=10 ğ‘˜=20012345Â·105Solution
Cost(
b)
ğ‘˜=10 ğ‘˜=200102030Numb
er of Clusters(
c)
Figur
e 1:Comparison of our algorithm with the baselines for the synthetic dataset for ğœ†=1.1and forğ‘˜=10,ğ‘˜=20.
ğ‘˜=10 ğ‘˜=20âˆ’0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
ğ‘˜=10 ğ‘˜=20051015Solution
Cost(
b)
ğ‘˜=10 ğ‘˜=200102030Numb
er of Clusters(
c)
Figur
e 2:Comparison of our algorithm with the baselines for the Uber dataset for ğœ†=1.1and forğ‘˜=10,ğ‘˜=20.
ğ‘˜=50 ğ‘˜=100âˆ’0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
ğ‘˜=50 ğ‘˜=10005001,0001,5002,0002,500Solution
Cost(
b)
ğ‘˜=50 ğ‘˜=100050100150Numb
er of Clusters(
c)
Figur
e 3:Comparison of our algorithm with the baselines for Brightkite for ğœ†=1.1and forğ‘˜=50,ğ‘˜=100.
ğ‘˜=50 ğ‘˜=100âˆ’0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
ğ‘˜=50 ğ‘˜=10005001,0001,500Solution
Cost(
b)
ğ‘˜=50 ğ‘˜=100050100150Numb
er of Clusters(
c)
Figur
e 4:Comparison of our algorithm with the baselines for Gowalla for ğœ†=1.001andğ‘˜=50,ğ‘˜=100.
[17] Vincent Cohen-Addad, Niklas Oskar D Hjuler, Nikos Parotsidis, David Saulpic,
and Chris Schwiegelshohn. 2019. Fully dynamic consistent facility location.
Advances in Neural Information Processing Systems 32 (2019).
[18] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
2009. Introduction to Algorithms, 3rd Edition. MIT Press. http://mitpress.mit.
edu/books/introduction-algorithms
[19] Easley David and Kleinberg Jon. 2010. Networks, Crowds, and Markets: Reasoning
About a Highly Connected World. Cambridge University Press, USA.
[20] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cali-
brating noise to sensitivity in private data analysis. In Theory of cryptography
conference. Springer, 265â€“284.[21] David Eisenstat, Claire Mathieu, and Nicolas Schabanel. 2014. Facility Location in
Evolving Metrics. In Automata, Languages, and Programming - 41st International
Colloquium, ICALP 2014, Copenhagen, Denmark, July 8-11, 2014, Proceedings, Part
II (Lecture Notes in Computer Science), Javier Esparza, Pierre Fraigniaud, Thore
Husfeldt, and Elias Koutsoupias (Eds.), Vol. 8573. Springer, 459â€“470. https:
//doi.org/10.1007/978-3-662-43951-7_39
[22] Volker A. Erdmann and JÃ¶rn Wolters. 1986. Collection of published
5S, 5.8S and 4.5S ribosomal RNA sequences. Nucleic Acids Research
14, suppl (01 1986), r1â€“r60. https://doi.org/10.1093/nar/14.suppl.r1
arXiv:https://academic.oup.com/nar/article-pdf/14/suppl/r1/6958995/14-
suppl-r1.pdf
 
37KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Sara Ahmadian et al.
[23] TomÃ¡s Feder and Daniel Greene. 1988. Optimal Algorithms for Approximate
Clustering. In Proceedings of the Twentieth Annual ACM Symposium on Theory of
Computing (STOC â€™88). Association for Computing Machinery, New York, NY,
USA, 434â€“444. https://doi.org/10.1145/62212.62255
[24] Hendrik Fichtenberger, Silvio Lattanzi, Ashkan Norouzi-Fard, and Ola Svensson.
2021. Consistent k-clustering for general metrics. In Proceedings of the 2021
ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 2660â€“2678.
[25] Guojun Gan, Chaoqun Ma, and Jianhong Wu. 2007. Data Clus-
tering: Theory, Algorithms, and Applications. Society for Industrial
and Applied Mathematics. https://doi.org/10.1137/1.9780898718348
arXiv:https://epubs.siam.org/doi/pdf/10.1137/1.9780898718348
[26] Teofilo F Gonzalez. 1985. Clustering to minimize the maximum intercluster
distance. Theoretical computer science 38 (1985), 293â€“306.
[27] Teofilo F. Gonzalez. 1985. Clustering to Minimize the Maximum Intercluster
Distance. Theor. Comput. Sci. 38 (1985), 293â€“306. https://doi.org/10.1016/0304-
3975(85)90224-5
[28] J. C. Gower and G. J. S. Ross. 1969. Minimum Spanning Trees and Single Linkage
Cluster Analysis. Journal of the Royal Statistical Society. Series C (Applied Statistics)
18, 1 (1969), 54â€“64. http://www.jstor.org/stable/2346439
[29] Xiangyu Guo, Janardhan Kulkarni, Shi Li, and Jiayi Xian. 2021. Consistent
k-median: Simpler, better and robust. In International Conference on Artificial
Intelligence and Statistics. PMLR, 1135â€“1143.
[30] Sariel Har-Peled. 2004. Clustering Motion. Discret. Comput. Geom. 31, 4 (2004),
545â€“565. https://doi.org/10.1007/s00454-004-2822-7
[31] Sariel Har-Peled and Manor Mendel. 2006. Fast Construction of Nets in Low-
Dimensional Metrics and Their Applications. SIAM J. Comput. 35, 5 (2006),
1148â€“1184. https://doi.org/10.1137/S0097539704446281
[32] Satoshi Hara and Yuichi Yoshida. [n.d.]. Average Sensitivity of Decision Tree
Learning. In The Eleventh International Conference on Learning Representations.
[33] Dorit S Hochbaum and David B Shmoys. 1985. A best possible heuristic for the
k-center problem. Mathematics of operations research 10, 2 (1985), 180â€“184.
[34] Sungjin Im and Benjamin Moseley. 2015. Fast and better distributed mapreduce
algorithms for k-center clustering. In Proceedings of the 27th ACM symposium on
Parallelism in Algorithms and Architectures. 65â€“67.
[35] Mohammad Reza Karimi Jaghargh, Andreas Krause, Silvio Lattanzi, and Sergei
Vassilvtiskii. 2019. Consistent online optimization: Convex and submodular. In
The 22nd International Conference on Artificial Intelligence and Statistics. PMLR,
2241â€“2250.
[36] Soh Kumabe and Yuichi Yoshida. 2022. Average sensitivity of dynamic pro-
gramming. In Proceedings of the 2022 Annual ACM-SIAM Symposium on DiscreteAlgorithms (SODA). SIAM, 1925â€“1961.
[37] Soh Kumabe and Yuichi Yoshida. 2022. Lipschitz Continuous Algorithms for
Graph Problems. arXiv preprint arXiv:2211.04674 (2022).
[38] Silvio Lattanzi, Stefano Leonardi, Vahab Mirrokni, and Ilya Razenshteyn. 2015.
Robust hierarchical k-center clustering. In Proceedings of the 2015 Conference on
Innovations in Theoretical Computer Science. 211â€“218.
[39] Silvio Lattanzi and Sergei Vassilvitskii. 2017. Consistent k-clustering. In Interna-
tional Conference on Machine Learning. PMLR, 1975â€“1984.
[40] Nina Mishra, Robert Schreiber, Isabelle Stanton, and Robert Endre Tarjan. 2007.
Clustering Social Networks. In Algorithms and Models for the Web-Graph, 5th
International Workshop, WAW 2007, San Diego, CA, USA, December 11-12, 2007,
Proceedings (Lecture Notes in Computer Science), Anthony Bonato and Fan R. K.
Chung (Eds.), Vol. 4863. Springer, 56â€“67. https://doi.org/10.1007/978-3-540-
77004-6_5
[41] Nina Mishra, Robert Schreiber, Isabelle Stanton, and Robert Endre Tarjan. 2008.
Finding Strongly Knit Clusters in Social Networks. Internet Math. 5, 1 (2008),
155â€“174. https://doi.org/10.1080/15427951.2008.10129299
[42] Gary J. Olsen. 1988. [53] Phylogenetic analysis using ribosomal RNA. In Ri-
bosomes. Methods in Enzymology, Vol. 164. Academic Press, 793â€“812. https:
//doi.org/10.1016/S0076-6879(88)64084-5
[43] David B Shmoys. 1995. Computing near-optimal solutions to combinatorial
optimization problems. Combinatorial Optimization 20 (1995), 355â€“397.
[44] Adarsh Subbaswamy, Roy Adams, and Suchi Saria. 2021. Evaluating Model
Robustness and Stability to Dataset Shift. In The 24th International Conference
on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual
Event (Proceedings of Machine Learning Research), Arindam Banerjee and Kenji
Fukumizu (Eds.), Vol. 130. PMLR, 2611â€“2619. http://proceedings.mlr.press/v130/
subbaswamy21a.html
[45] Nithin Varma and Yuichi Yoshida. 2021. Average sensitivity of graph algorithms.
InProceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA).
SIAM, 684â€“703.
[46] Yuichi Yoshida and Shinji Ito. 2022. Average Sensitivity of Euclidean k-Clustering.
InAdvances in Neural Information Processing Systems.
[47] Yuichi Yoshida and Samson Zhou. 2020. Sensitivity analysis of the maximum
matching problem. arXiv preprint arXiv:2009.04556 (2020).
[48] T. Zhang, R. Ramakrishnan, and M. Livny. 1997. BIRCH: A new data clustering
algorithm and its applications. Data Mining and Knowledge Discovery 1, 2 (1997),
141â€“182.
 
38