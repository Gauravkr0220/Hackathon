SeBot: Structural Entropy Guided Multi-View Contrastive
Learning for Social Bot Detection
Yingguang Yang
University of Science and Technology
of China
Hefei, China
dao@mail.ustc.edu.cnQi Wu
University of Science and Technology
of China
Hefei, China
qiwu4512@mail.ustc.edu.cnBuyun He
University of Science and Technology
of China
Hefei, China
byhe@mail.ustc.edu.cn
Hao Pengâˆ—
Beihang University
Beijing, China
penghao@buaa.edu.cnRenyu Yang
Beihang University
Beijing, China
renyuyang@buaa.edu.cnZhifeng Hao
Shantou University
Shantou, China
haozhifeng@stu.edu.cn
Yong Liaoâˆ—
University of Science and Technology
of China
Hefei, China
yliao@ustc.edu.cn
ABSTRACT
Recent advancements in social bot detection have been driven by
the adoption of Graph Neural Networks. The social graph, con-
structed from social network interactions, contains benign and bot
accounts that influence each other. However, previous graph-based
detection methods that follow the transductive message-passing
paradigm may not fully utilize hidden graph information and are
vulnerable to adversarial bot behavior. The indiscriminate message
passing between nodes from different categories and communi-
ties results in excessively homogeneous node representations, ul-
timately reducing the effectiveness of social bot detectors. In this
paper, we propose SeBot, a novel multi-view graph-based con-
trastive learning-enabled social bot detector. In particular, we use
structural entropy as an uncertainty metric to optimize the entire
graphâ€™s structure and subgraph-level granularity, revealing the im-
plicitly existing hierarchical community structure. And we design
an encoder to enable message passing beyond the homophily as-
sumption, enhancing robustness to adversarial behaviors of social
bots. Finally, we employ multi-view contrastive learning to maxi-
mize mutual information between different views and enhance the
detection performance through multi-task learning. Experimental
results demonstrate that our approach significantly improves the
performance of social bot detection compared with SOTA methods.
âˆ—Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671871CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning; â€¢Security
and privacyâ†’Social network security and privacy.
KEYWORDS
social bot detection, graph neural networks, contrastive learning,
structural entropy
ACM Reference Format:
Yingguang Yang, Qi Wu, Buyun He, Hao Peng, Renyu Yang, Zhifeng Hao,
and Yong Liao. 2024. SeBot: Structural Entropy Guided Multi-View Con-
trastive Learning for Social Bot Detection. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671871
1 INTRODUCTION
Social bots are automated controlled accounts that widely exist
on social platforms such as Twitter and Weibo, in most cases, for
malicious purposes such as spreading misinformation [ 16,41], ma-
nipulating public opinion [ 18,42], and influencing political elections
[10,29]. They will undoubtedly give rise to societal disharmony in
social network environments.
Conventional methods for social bots detection primarily focus
on extracting discriminative features, ranging from user attributes,
[47], text features [ 21] to structure features[ 5], which can be then
used to train classifiers in a supervised manner. Due to the con-
tinuous evolution of social bots [ 9], including their ability to steal
information from legitimate accounts and mimic normal account
behaviors [ 26], these traditional methods turn out to be ineffective
in identifying the latest generation bots. A recent advancement
in social bot detection is introducing graph neural networks [ 1]
that treat accounts and the interactions in-between as nodes and
edges, respectively. Multi-relational heterogeneous graphs can be
3841
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingguang Yang et at.
Intra -class edge
Inter-class edge
 ?
Root Sub
CommunitiesHierarchical Community Structure
?
?
?
?
Figure 1: Illustration of community structure and inter-class
interactions co-exists in social networks. The abstraction of
the hierarchical community structure is presented in the
form of an encoding tree.
established [ 13] and a Relation Graph Transformer (RGT) is respon-
sible for aggregating information from neighbors. Such approaches
consider the features of each account as interdependent ones and
leverage the semantic information of relationships between ac-
counts to generate semantically-richer representations.
While successful, there are two challenges facing the existing
graph-based methods (illustrated in Figure 1). i) How to fully exploit
the hierarchical information hidden in the graph structure? Unlike
other semi-supervised node classification tasks, some accounts in
social platforms tend to exhibit stronger correlations with others in
the topological structure due to shared interests, events of interest,
etc. [ 9], indicating an intrinsic hierarchical community structure in
the graph constructed by social bot detectors. The existing graph-
based social bot detection methods [ 13,15,20,33,45,49,50,54]
primarily focused on aggregating node-level information on the
original graph, neglecting comprehensive utilization of high-order
graph structural information. As a result, the generated representa-
tions only capture low-order graph information and lack high-order
semantic information. ii) How to handle the adversarial behaviors of
bots intentionally interacting with humans to evade detection? Ex-
isting bot detection methods rely on the assumption that bots and
humans exhibit stronger connections with nodes of their category.
However, for social bots, especially in the case of advanced social
bot control programs, gathering human-specific information from
neighbors would be more advantageous for concealing their true
identity. Therefore, bots intentionally tend to establish associations
with human accounts to escape detection [7, 11].
To tackle these two challenges, we propose SeBot, a novel
Structural Entropy Guided Social BotDetection framework through
graph contrastive learning enhanced classification with both intra-
class and inter-class edges. Motivated by minimum entropy theory
[23], indicating systems at the minimum level of uncertainties, and
structural entropy [ 27] further offers an effective measure of the in-
formation embedded in an arbitrary graph and structural diversity.
We construct encoding trees for both the entire input graph and
the subgraphs of each node by minimizing their structural entropy.
We then use the optimal encoding trees to describe the hierarchical
structural information of the graph and obtain cluster assignment
matrixes for the nodes. Subsequently, node representations are ob-
tained through structural entropy pooling and unpooling for node
and graph classification. On the other hand, we designed an encoder
that can adaptively make neighbors similar or differentiate them to
handle the adversarial behavior of bots. Finally, we utilize the noderepresentations generated by the three modules to calculate both
cross-entropy loss and self-supervised contrastive learning loss.
The contributions of this work can be summarized as follows:
â€¢We are the first to introduce structural entropy to capture se-
mantic information at both subgraph and entire-graph levels in
a self-supervised manner.
â€¢We propose a self-supervised contrastive learning framework
called SeBot, which integrates node-node and subgraph-subgraph
level contrastive learning tasks in a multi-task learning manner
to capture high-order semantic information.
â€¢Experiments on two real-world bot detection benchmark datasets
demonstrate that our method outperforms current state-of-the-
art social bot detection methods.
2 RELATED WORK
Graph-based Social Bot Detection. Graph-based social bot de-
tection has been of ultimate importance in modeling various in-
teractions intrinsically existing in social networks. Previous meth-
ods [ 1,13,15,45] have focused primarily on designing informa-
tion aggregation strategies for better detection performance. [ 1]
takes the first attempt to use graph convolutional neural networks
(GCNs) [ 25] for detecting social bots. Typically, BotRGCN [ 15] uti-
lizes relational graph convolutional networks (RGCNs) to aggregate
neighbor information from edges of different relations. [ 13] pro-
posed RGT, which utilizes a self-attention mechanism to adaptively
aggregate information from neighbors in each relational view of the
graph. Although these methods have shown significant improve-
ments compared to traditional feature engineering and text-based
approaches [ 12], they may not fully exploit the crucial semantic
information concealed in the graph structure and the graph struc-
ture obtained from sampling social networks contain a significant
amount of uncertainty and randomness.
Graph Self-Supervised Learning. Self-supervised learning has
achieved great success in the fields of natural language process-
ing [48] and computer vision [ 24] without the need for prohibitively
costly labeled data. Graph contrastive learning (GCL) is a typical
paradigm of self-supervised learning on graphs, aiming at learn-
ing invariant representations between different graph views. For
instance, DGI [ 40] utilizes a local-global mutual information maxi-
mization approach to obtain node representations. [ 51] proposes
a series of graph augmentation methods including node dropping,
edge perturbation, attribute masking, and so on. [ 59] propose an
adaptive way for graph augmentation, which assigns adaptive prob-
abilities to attribute and topological perturbation. However, these
augmentation methods inevitably suffer from the loss of essential
information or introduce class-redundant noise. Due to the sig-
nificant impact of the quality of generated views on contrastive
learning, [ 43] theoretically proves that the anchor view containing
essential semantic information should have the minimum structural
entropy. Inspired by this, structural entropy is employed by us to
generate the anchor view with minimum uncertainty.
Structural Entropy. After Shannon proposed information entropy
to measure system uncertainty [ 35], the measurement of uncer-
tainty in graph structure has been widely studied, and several meth-
ods [ 30,34,37] have been developed to quantify it. Among them,
3842SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Target	UserBotHumanUnlabeledRelationğ‘š-hop	SubgraphsEncoding	Trees	GenerationEdge	Dropping
GNNGNNSEP-U
GNN	SEPGNNMLP
Contrastive	Pairs
RCM	Layer81%19%
GG	4()(),
,â„’,-.ProjectionHeadProjectionHead
Contrastive	Pairsâ„’/-.ConcatMLP
ğ¡34
ğ¡35ğ¡36Ã—ğ¿G	6Intra-Relation	ConvG	5GNN	SEPSEPSEPGNNSEPGNNSEP-UEncoding	Trees	GenerationResidualConnectionDown	SamplingUp	SamplingGNNSE	MinimizationClusterAssignment0		â€¦		1â€¦â€¦â€¦1		â€¦		0
Figure 2: Overview of our proposed framework SeBot, which mainly consists of three modules: 1) Node-level encoding tree
generation and bottom-up message passing; 2) Subgraph-level encoding trees generation and message-passing; 3) Relational
information aggregation beyond homophily. Contrastive learning loss and classification loss are later calculated on obtained
tree types of representations.
structural entropy [ 6,27,52,53,55] has been widely used in re-
cent years and has shown promising results in graph structure
learning [ 60], graph pooling[ 44], and other tasks. Since structural
entropy can be used as a metric to measure the complexity of graph
hierarchical structure, previous applications have mainly focused
on minimizing the structural entropy of the constructed encod-
ing tree. For instance, SEP [ 44] defines MERGE, REMOVE, and
FILL operations to update the constructed encoding tree based on
the principle of minimizing structural entropy. In this paper, we
employ structural entropy in a self-supervised manner to capture
information at both the node level and subgraph level.
3 PRELIMINARIES
In this section, we first illustrate the graph-based social bot detec-
tion task, followed by an introduction to the definition of graph
contrastive learning.
Definition 3.1. Graph-based Social bot detection. Graph-
based social bot detection can be regarded as a semi-supervised
node binary classification problem on a multi-relational graph. It
involves treating the accounts in social platforms as nodes and
the interactions such as "following" and "follower" as edges of
different relations. Constructed graph in this task can be formulated
asG={V,E,X}, whereV={ğ‘£ğ‘–|ğ‘–=1,2,...,ğ‘›}is the set of
all nodes,E=âˆªğ‘…
ğ‘Ÿ=1Eğ‘Ÿrepresents the set of edges formed by ğ‘…
relations andXis the feature matrix. Row ğ‘–ofXrepresents the
feature vector of the ğ‘–-th node. Total detection process is to use the
graphGand the labels of training nodes Ytrain to predict the labels
of test nodes Ytest:
ğ‘“(G,Ytrain)â†’Â¯Ytest. (1)Definition 3.2. Graph Contrastive Learning. In the general
graph contrastive learning paradigm for node classification, two
augmented graphsGğ›¼, andGğ›½are generated using different graph
augmentation methods (such as edge dropping, feature masking,
etc.) on the input graph G. Subsequently, an encoder consisting of
multi-layer graph neural networks is employed to generate node
representations including topological information existing in graph
structure. During the first training stage, these representations are
further mapped into an embedding space by a shared projection
head for contrastive learning. A typical graph contrastive loss,
InfoNCE [ 31], treats the same node in different views ğ‘£ğ›¼
ğ‘–andğ‘£ğ›½
ğ‘–
as positive pairs and other nodes as negative pairs. The graph
contrastive learning loss Lğ‘–of nodeğ‘£ğ‘–and total lossLcan be
formulated as:
Lğ›¼
ğ‘–=âˆ’ğ‘™ğ‘œğ‘”(ğ‘’ğ‘ ğ‘–ğ‘š(zğ›¼
i,zğ›½
i)/ğœ
Ãğ‘
ğ‘—=11ğ‘—â‰ ğ‘–ğ‘’ğ‘ ğ‘–ğ‘š(zğ›¼
i,zğ›¼
j)/ğœ+ğ‘’ğ‘ ğ‘–ğ‘š(zğ›¼
i,zğ›½
j)/ğœ),
L=ğ¼ğ‘›ğ‘“ğ‘œğ‘ğ¶ğ¸(Zğ›¼,Zğ›½)=ğ‘âˆ‘ï¸
ğ‘–=1Lğ›¼
ğ‘–+Lğ›½
ğ‘–,(2)
whereğ‘is the batch size, ğœis the temperature coefficient, 1ğ‘—â‰ ğ‘–=1
whenğ‘—â‰ ğ‘–andğ‘ ğ‘–ğ‘š(Â·,Â·)stands for cosine similarity function.
4 METHODOLOGY
4.1 Overview of SeBot
The total pipeline of SeBot is illustrated in Figure 2. To begin with,
an attributed multi-relational graph Gis constructed by represent-
ing social network interactions as edges. Subsequently, it is fed into
three different modules to obtain representations at multi-grained
levels under various receptive field scopes hğ‘–=[hğ›¼
ğ‘–âˆ¥hğ›½
ğ‘–âˆ¥hğ›¾
ğ‘–].
3843KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingguang Yang et at.
The two modules above are responsible for constructing encoding
trees by minimizing structural entropy for the entire graph view
Gğ›¼andğ‘š-order subgraph view Gğ›½respectively.Gğ›½is formed by
the target node and surrounding neighbor nodes. The view Gğ›¾is
generated by edge removing. Then, bottom-up information prop-
agation is performed according to these encoding trees to obtain
node embeddings hğ›¼
ğ‘–andhğ›½
ğ‘–. In the third module, to counteract
the intentional evasion behaviors of social bots, we devise a graph
convolutional layer that can make neighbors similar or discrimina-
tive adaptively while maintaining normalization. Simultaneously,
a relational channel-wise mixing (RCM) layer is proposed to inte-
grate information from different relations to obtain hğ›¾
ğ‘–. Finally, the
fusion of the three modules involves computing cross-entropy loss
for classification and utilizing graph self-supervised contrastive
learning loss to capture shared information between three views,
enhancing the classification learning process.
4.2 Community-aware Hierarchical Augment
In social networks, some accounts may exhibit more pronounced
connections with each other due to shared interests, events, and
so on, thus forming communities. However, previous detection
methods have not effectively leveraged the community structure
information within social networks. To reveal the hierarchical struc-
ture within the graph, we utilize structural entropy minimization to
obtain fixed-height encoding trees, where the child nodes of each
node belong to the same community. For the sake of clarity, we first
illustrate the definition of structural entropy and its minimization
algorithm.
Structural Entropy. Structural entropy is initially proposed by
[27] to measure the uncertainty of graph structure information. The
structural entropy of a given graph G={V,E,X}on its encoding
Treeğ‘‡is defined as:
Hğ‘‡(G)=âˆ’âˆ‘ï¸
ğ‘£ğœâˆˆğ‘‡ğ‘”ğ‘£ğœ
ğ‘£ğ‘œğ‘™(V)logğ‘£ğ‘œğ‘™(ğ‘£ğœ)
ğ‘£ğ‘œğ‘™(ğ‘£+ğœ), (3)
whereğ‘£ğœis a node in ğ‘‡except for root node and also stands for a
subsetVğœâˆˆV,ğ‘”ğ‘£ğœis the number of edges connecting nodes in and
outsideVğœ,ğ‘£+ğœis the immediate predecessor of of ğ‘£ğœandğ‘£ğ‘œğ‘™(ğ‘£ğœ),
ğ‘£ğ‘œğ‘™(ğ‘£+ğœ)andğ‘£ğ‘œğ‘™(V) are the sum of degrees of nodes in ğ‘£ğœ,ğ‘£+ğœand
V, respectively. The structural entropy of graph Gis the entropy of
the encoding tree with the minimum structural entropy: H(G) =
minâˆ€ğ‘‡{Hğ‘‡(G)} . According to this definition, structural entropy
can be used to decode the hierarchical structure of a given graph
into an encoding tree as a measurement of community division. In
addition, the generated encoding tree ğ‘‡can be seen as the natural
multi-grained hierarchical community division result.
Minimization Algorithm. In addition to the optimal encoding
tree with the minimum structural entropy, a fixed level of commu-
nity partitioning is preferred for the specific scenario. Considering
this, theğ‘˜-dimension structural entropy of Gis defined as the
optimal encoding tree with a fixed height ğ‘˜:
H(ğ‘˜)(G)= min
âˆ€ğ‘‡:Height(ğ‘‡)=ğ‘˜{Hğ‘‡(G)}. (4)Algorithm 1: Structural entropy minimization algorithm
Input : input undirected graph G=(V,E)and a specific
heightğ‘˜>1
Output: encoding tree ğ‘‡with a fixed height ğ‘˜
1initialize encoding tree ğ‘‡with root node ğ‘£ğ‘Ÿand nodes inV
as its children ;
2// Step 1: full-height binary coding tree construction ;
3while|ğ‘£ğ‘Ÿ.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›|> 2do
4 select child node pair ( ğ‘£1ğ‘,ğ‘£2ğ‘)â†
ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘£1ğ‘,ğ‘£2ğ‘){Hğ‘‡(G)âˆ’Hğ‘‡ğ‘(G)|ğ‘£1ğ‘,ğ‘£2ğ‘âˆˆğ‘£ğ‘Ÿ.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›}
;
5 Merge(ğ‘£1ğ‘,ğ‘£2ğ‘);
6end
7// Step 2: binary coding tree squeeze to height ğ‘˜;
8whileğ»ğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘‡)>ğ‘˜do
9 select node ğ‘£ğœâ†ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›(ğ‘£ğœ){Hğ‘‡(G)âˆ’Hğ‘‡ğ‘š(G)|ğ‘£ğœâˆˆ
ğ‘‡&ğ‘£ğœâ‰ ğ‘£ğ‘Ÿ&ğ‘£ğœâˆ‰V};
10 Drop(ğ‘£ğœ);
11end
12return encoding tree ğ‘‡;
The total process of generation of a encoding tree with a fixed
heightğ‘˜can be divided into two steps: 1) construction of the full-
height binary encoding tree and 2) compression of the binary en-
coding tree to height ğ‘˜. Given root node ğ‘£ğ‘Ÿof the encoding tree ğ‘‡,
all original nodes in graph G=(V,E)are treated as leaf nodes.
We first define two iterative operations on ğ‘‡.
Definition 4.1. Assumingğ‘£1ğ‘andğ‘£2ğ‘as two children of root node
ğ‘£ğ‘Ÿ, the function Merge(ğ‘£1ğ‘,ğ‘£2ğ‘)is defined as adding a new node ğ‘£ğ‘–
as the child of ğ‘£ğ‘Ÿand the parent of ğ‘£1ğ‘andğ‘£2ğ‘:
ğ‘£ğ‘–.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘› ={ğ‘£1
ğ‘,ğ‘£2
ğ‘},
ğ‘£ğ‘Ÿ.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘› ={ğ‘£ğ‘–}âˆªğ‘£ğ‘Ÿ.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›.(5)
Definition 4.2. Given node ğ‘£ğœand its parent node ğ‘£+ğœinğ‘‡, the
function Drop(ğ‘£ğœ)is defined as adding the children of ğ‘£ğœand itself
to the child set of ğ‘£+ğœ:
ğ‘£+
ğœ.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘› =ğ‘£+
ğœ.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›âˆªğ‘£ğœ.ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›. (6)
The generation of the encoding tree with a fixed height ğ‘˜pri-
marily involves iterations through two operations to obtain the
minimum structural entropy, which is shown in Algorithm 1. To
start with, we initialize an encoding tree ğ‘‡by treating all nodes in V
as children of root node ğ‘£ğ‘Ÿ. During step 1, an iterative Merge(ğ‘£1ğ‘,ğ‘£2ğ‘)
is conducted with the aim of minimizing structural entropy to ob-
tain a binary coding tree without height limitation. In this way,
selected leaf nodes are combined to form new community divisions
with minimal structural entropy. Then, to compress height to a
specific hyperparameter ğ‘˜,Drop(ğ‘£ğœ)is leveraged to merge small
divisions into larger ones, and thus the height of the coding tree is
reduced, which is still following the structural entropy minimiza-
tion strategy. Eventually, the encoding tree with fixed height ğ‘˜and
minimal structural entropy is obtained.
3844SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Obtained encoding tree in this manner can also be seen as the
anchor view that includes minimal but sufficient important informa-
tion in an unsupervised manner [ 43]. The quality of views generated
by graph augmentation technologies plays a crucial role in learning
informative representations. According to graph information bottle-
neck theory (GIB) [ 46], retaining important information in a graph
view should involve maximizing mutual information between the
output and labels while reducing mutual information between input
and output. This can be formally expressed as follows:
GIB: maxğ¼(ğ‘“(ğº);ğ‘Œ)âˆ’ğ›½ğ¼(ğº;ğ‘“(ğº)), (7)
whereğ¼(Â·;Â·)represents the mutual information between inputs. A
lot of graph augmentation methods have been proposed, including
edge removal, feature masking, graph diffusion, and more. Yet, these
methods cannot ensure the preservation of essential information
regarding downstream tasks and may introduce class-redundant
noise.
4.3 Message Passing on Encoding Tree
To obtain node representations and subgraph representations, the
message passing on the encoding tree is carried out bottom-up,
where the generated parent nodes aggregate information from their
child nodes. This process begins with the leaf nodes (i.e., the nodes
inV) at the first layer transmitting information to their second-
layer parent nodes. Specifically, given the cluster assignment matrix
Sğ‘¡âˆˆRğ‘›ğ‘¡Ã—ğ‘›ğ‘¡+1whereğ‘›ğ‘¡andğ‘›ğ‘¡+1are the number of nodes and
assigned clusters (i.e. nodes in the next layer) in the ğ‘¡-th layer,
each element in Sğ‘¡equal to 1 indicates that the node belongs to a
corresponding cluster. The adjacency matrix Ağ‘¡+1âˆˆRğ‘›ğ‘¡+1Ã—ğ‘›ğ‘¡+1and
the hidden layer representations Pğ‘¡+1âˆˆRğ‘›ğ‘¡+1Ã—ğ‘‘for the(ğ‘¡+1)-th
layer can be obtained by matrix multiplication:
SEP :Ağ‘¡+1=SâŠ¤
ğ‘¡Ağ‘¡Sğ‘¡;Pğ‘¡+1=SâŠ¤
ğ‘¡Hğ‘¡, (8)
where Ağ‘¡is the adjacency matrix and Hğ‘¡denotes the hidden fea-
tures martix in the ğ‘¡-th layer. As pooling continues, the number of
nodes decreases. To obtain representations of nodes in V, we fur-
ther employ unpooling to ensure that the number of nodes matches
the number of nodes in V:
SEP-U :Ağ‘¡+1=Sğ‘¡Ağ‘¡SâŠ¤
ğ‘¡;Pğ‘¡+1=Sğ‘¡Hğ‘¡, (9)
where Sğ‘¡is the same matrix used in previous pooling layers. The
node-level representation hğ›¼
ğ‘–is obtained through multiple layers
of SEP to obtain high-order community representations and multi-
layer SEP-U reconstructions.
On the other hand, the representation of each subgraph extracted
for every target user can be obtained by concatenating the results
of SEP pooling layers:
SEP-G :hğ›½,ğ‘¡
ğ‘–=ğ‘…ğ‘’ğ‘ğ‘‘ğ‘œğ‘¢ğ‘¡(ğ‘†ğ¸ğ‘ƒğ‘¡(ğºğ¶ğ‘ğ‘¡(Hğ‘¡,Ağ‘¡),Sğ‘¡))
hğ›½
ğ‘–=ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(hğ›½,1
ğ‘–,Â·Â·Â·,hğ›½,ğ‘™
ğ‘–),(10)
whereğ‘†ğ¸ğ‘ƒğ‘¡(Â·)denotes the ğ‘¡-th SEP layer and ğ‘…ğ‘’ğ‘ğ‘‘ğ‘œğ‘¢ğ‘¡(Â·)can be
chosen between average pooling and sum pooling. These high-
order representations obtained in this manner enable social bot
detection to be implemented through subgraph classification.4.4 Relational Information Aggregation
To alleviate graph adversarial attacks by social bots (i.e., actively
establishing relationships with humans), we propose a relational
information aggregation mechanism beyond homophily and a rela-
tion channel-wise mixing layer in this subsection.
4.4.1 Relational graph convolution beyond resemblance limitation.
Previous work [ 15] has adopted RGCN to detect bot accounts
and has shown promising success in modeling different relations.
However, the information aggregation of RGCN is based on the
homophily assumption (i.e., nodes belonging to the same class
tend to be connected), and advanced bots may consciously interact
more with humans. Considering this issue, we incorporate high-
frequency information (i.e., the differences between nodes) into the
information aggregation strategy of RGCN through the generation
of negative attention coefficients. However, positive and negative
weights generated directly through the tanh activation function
can not be normalized. To ensure the consistency of information
aggregation, we further introduce the Gumbel-Max reparametriza-
tion trick [ 22] to make the weights close to 1 or -1. Specifically, the
attention weight ğœ”ğ‘Ÿ,{ğ‘™}
ğ‘–ğ‘—of the edgeğ‘’ğ‘–ğ‘—in relationğ‘Ÿand the layer ğ‘™
is generated by:
ğœ”ğ‘Ÿ,{ğ‘™}
ğ‘–ğ‘—=tanh
(g{ğ‘™},âŠ¤
ğ‘Ÿh
h{ğ‘™âˆ’1}
ğ‘–âˆ¥h{ğ‘™âˆ’1}
ğ‘—i
+ğ‘™ğ‘œğ‘”ğœ–âˆ’ğ‘™ğ‘œğ‘”(1âˆ’ğœ–))/ğœğº
,
(11)
where g{ğ‘™}
ğ‘ŸâˆˆR2ğ·denotes the trainabele parameter, ğœ–âˆ¼ğ‘ˆğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š(0,1)
is the sampled Gumbel random variate and ğœğºis a small tempera-
ture used to amplify ğœ”ğ‘Ÿ,{ğ‘™}
ğ‘–ğ‘—. In this manner, when the weights are
close to 1, it retains similar information with neighbors, whereas
when close to -1, it preserves dissimilar information. In addition,
normalization can be directly performed based on the number of
neighbors:
hğ‘Ÿ,{ğ‘™}
ğ‘–=1
|ğ‘ğ‘Ÿ(ğ‘£ğ‘–)|âˆ‘ï¸
ğ‘£ğ‘—âˆˆğ‘ğ‘Ÿ(ğ‘£ğ‘–)ğœ”ğ‘Ÿ,{ğ‘™}
ğ‘–ğ‘—h{ğ‘™âˆ’1}
ğ‘—ğ‘Š{ğ‘™}
ğ‘Ÿ, (12)
whereğ‘ğ‘Ÿ(ğ‘£ğ‘–)is the neighbors of ğ‘£ğ‘–andğ‘Š{ğ‘™}
ğ‘Ÿis the weight matrix.
Due to the adopted parametrization tick, normalization is approxi-
mately satisfied by |ğ‘ğ‘Ÿ(ğ‘£ğ‘–)|â‰ˆÃ
ğ‘£ğ‘—âˆˆğ‘ğ‘Ÿ(ğ‘£ğ‘–)|ğœ”ğ‘Ÿ,{ğ‘™}
ğ‘–ğ‘—|.
4.4.2 Relational Channel-wise Mixing. After aggregating informa-
tion independently across different relations, it is necessary to
merge this relational information to acquire representations with
richer semantics. Previous adaptive method [ 13] is limited to gen-
erating normalized weight coefficients for representations in each
relation, inevitably introducing some noise information present in
specific relations. Inspired by [ 28], we propose a relational channel-
wise adaptive fusion layer. Specifically, given the embeddings hğ‘Ÿ,{ğ‘™}
ğ‘–
generated in each relation and at layer ğ‘™, we first calculate the
weight vector of the relation:
Ë†uğ‘Ÿ,{ğ‘™}
ğ‘–=[h1,{ğ‘™}
ğ‘–âˆ¥Â·Â·Â·âˆ¥ hğ‘…,{ğ‘™}
ğ‘–]Ë†ğ‘Š{ğ‘™}
ğ‘Ÿ, (13)
where Ë†ğ‘Š{ğ‘™}
ğ‘Ÿis the weight matrix and [Â·||Â·] is the concat operation.
3845KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingguang Yang et at.
Then, we apply a channel-wise softmax function to normalize
weight coefficients at each feature channel:
[u1,{ğ‘™}
ğ‘–,..., uğ‘…,{ğ‘™}
ğ‘–]=ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥([Ë†u1,{ğ‘™}
ğ‘–,..., Ë†uğ‘…,{ğ‘™}
ğ‘–]).(14)
Last, uğ‘Ÿ,{ğ‘™}
ğ‘–can be used to mix representations from different
relations and add transformed original features:
h{ğ‘™}
ğ‘–=h{ğ‘™âˆ’1}
ğ‘–ğ‘Š{ğ‘™}
ğ‘Ÿğ‘œğ‘œğ‘¡+ğ‘…âˆ‘ï¸
ğ‘Ÿ=1hğ‘Ÿ,{ğ‘™}
ğ‘–âŠ™uğ‘Ÿ,{ğ‘™}
ğ‘–, (15)
whereâŠ™denotes the Hadamard product operation and ğ‘Š{ğ‘™}
ğ‘Ÿğ‘œğ‘œğ‘¡is the
weight matrix to transform original features.
In fact, when each edge weight ğœ”ğ‘Ÿ,{ğ‘™}
ğ‘–ğ‘—=1anduğ‘Ÿ,{ğ‘™}
ğ‘–=1, the
proposed encoder is equal to RGCN and thus can be seen as an
extension of it. Besides, ğ¿layers of the two networks described
above are used to generate hğ›¾
ğ‘–onGğ›¾.
4.5 Multi-Task Optimization and Learning
After obtaining node representations from the three modules, it is
necessary to design the loss function as the learning objective. The
primary objective of the model remains classification, to identify
bot accounts. We employ a two-layer MLP as the classification
layer for the concatenation hğ‘–=[hğ›¼
ğ‘–âˆ¥hğ›½
ğ‘–âˆ¥hğ›¾
ğ‘–]and calculate the
classification loss using binary cross-entropy:
Lğ¶ğ¸=âˆ’âˆ‘ï¸
ğ‘£ğ‘–âˆˆVğ‘¡[ğ‘¦ğ‘–ğ‘™ğ‘œğ‘”(ğ‘ğ‘–)+(1âˆ’ğ‘¦ğ‘–)ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ğ‘–)]
ğ‘ğ‘–=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğœ(hğ‘–ğ‘Š1+ğ‘1)ğ‘Š2+ğ‘2).(16)
Additionally, as shown in Figure 1, in real-world scenarios, hi-
erarchical community structure and heterophily coexist in social
networks. Our goal is to learn unified node representations that can
incorporate both types of information simultaneously. However, to
our knowledge, there are no methods that capture both types of
information simultaneously. Therefore, we use three modules to ob-
tain three representations, hğ›¼
ğ‘–,hğ›½
ğ‘–, and hğ›¾
ğ‘–, each containing different
semantic information. Specifically, hğ›¼
ğ‘–includes global hierarchical
structure information, hğ›½
ğ‘–includes local hierarchical structure in-
formation, and hğ›¾
ğ‘–includes specific category information of the
neighborhood.
However, representations obtained through independent mod-
ules are often one-sided. Therefore, we further use self-supervised
contrastive learning to capture the consistency between different
representations. Specifically, by maximizing the mutual informa-
tion between different representations. The proposed two level
contrastive learning losses Lğ‘ğ¶ğ¿ andLğ‘†ğ¶ğ¿are computed based
on the representations of all nodes (i.e., Hğ›¼,Hğ›½andHğ›¾) across
different views:
Lğ‘ğ¶ğ¿=ğ¼ğ‘›ğ‘“ğ‘œğ‘ğ¶ğ¸(ğœ“ğ‘(Hğ›¼),ğœ“ğ‘(Hğ›¾)),
Lğ‘†ğ¶ğ¿=ğ¼ğ‘›ğ‘“ğ‘œğ‘ğ¶ğ¸(ğœ“ğ‘†(Hğ›½),ğœ“ğ‘†(Hğ›¾)),(17)
whereğœ“ğ‘(Â·)andğœ“ğ‘†(Â·)are defined as projection heads for node-
level and subgraph-level contrastive learning, respectively. In this
way, two seemingly contradictory challenges are unified and ad-
dressed in a self-supervised manner, rather than being simply solved
independently.Algorithm 2: The training process of SeBot
Input : input undirected graph G=(V,E,X)and a
specific height ğ‘˜>1, hyperparameter ğ‘š,
temperature ğœand loss weights ğœ†1andğœ†2
Output: node representations H
1generate undirected graph view Gğ›¼and itsğ‘š-hop subgraph
viewGğ›½;
2generate viewGğ›¾through edge dropping ;
3minimizing structural entropy to construct encoding trees
forGğ›¼andGğ›½according to Algorithm 1 ;
4forğ‘’ğ‘ğ‘œğ‘â„â†1,2,Â·Â·Â·do
5 forğ‘£ğ‘–âˆˆVdo
6 obtain node embedding hğ›¼
ğ‘–ofğ‘£ğ‘–fromGğ›¼â†
Equation (8-9) ;
7 obtain node embedding hğ›½
ğ‘–ofğ‘£ğ‘–fromGğ›½â†
Equation (10) ;
8 obtain node embedding hğ›¾
ğ‘–ofğ‘£ğ‘–fromGğ›¾â†
Equation (11-15) ;
9 end
10 classification lossLğ¶ğ¸â†Equation (16) ;
11 contrastive lossLğ‘ğ¶ğ¿ andLğ‘†ğ¶ğ¿â†Equation (17) ;
12 total lossLâ† Equation (18) ;
13 loss backward ;
14end
15return the predicted label set for the test nodes Ë†Ytest.
Ultimately, the overall loss of the proposed method is calculated
by summing the aforementioned three learning losses:
L=Lğ¶ğ¸+ğœ†1Lğ‘ğ¶ğ¿+ğœ†2Lğ‘†ğ¶ğ¿. (18)
whereğœ†1andğœ†2are two hyperparameters ranging from 0 to 1, used
to adjust the magnitudes and weights of different losses.
4.6 Complexity Analysis
Given a mutli-relational graph G={V,E},ğ‘›=|V|andğ‘š=|E|,
the runtime complexity of Algorithm 1 is ğ‘‚(â„ğ‘šğ‘ğ‘¥(ğ‘šğ‘™ğ‘œğ‘”ğ‘›+ğ‘›)), in
whichâ„ğ‘šğ‘ğ‘¥ is the height of coding tree ğ‘‡after the first step. In
general, the coding tree ğ‘‡tends to be balanced in the process of
structural entropy minimization, thus, â„ğ‘šğ‘ğ‘¥ will be around ğ‘™ğ‘œğ‘”ğ‘› .
Besides, a large-scale social network generally has more edges than
nodes, i.e.,ğ‘šâ‰«ğ‘›, thus the runtime of Algorithm 1 almost scales
linearly in the number of edges.
The overall time complexity of the three proposed modules is
ğ‘‚(ğ‘›+ğ‘š+â„ğ‘šğ‘ğ‘¥(ğ‘šlogğ‘›+ğ‘›)). Specifically, in Section 4.3, the time
complexities of SEP, SEP-U, and SEP-G are all ğ‘‚(ğ‘›). In Section 4.4,
the proposed relational information aggregation has a time com-
plexity ofğ‘‚(ğ¿(ğ‘…Â·ğ‘›+ğ‘š)), whereğ¿represents the number of layers
andğ‘…represents the number of relations.
5 EXPERIMENTS
In this paper, we propose the following research questions for a
deep evaluation of the proposed SeBot framework:
â€¢RQ1: How does SeBot perform compared with other baselines?
3846SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢RQ2: How does SeBot benefit from its different modules?
â€¢RQ3: How does SeBot perform concerning different hyperpa-
rameters?
â€¢RQ4: CanSeBot generate more class-discriminative node repre-
sentations than other baselines?
â€¢RQ5: CanSeBot effectively address the two challenges men-
tioned in the introduction?
5.1 Experimental Setup
5.1.1 Datasets. TwiBot-20 [14] and MGTAB [36] are adopted to
evaluate the performance of SeBot. TwiBot-20 contains 229,580
accounts extracted from Twitter and their following and follower
interactions. MGTAB consists of 10,199 accounts and 7 types of
relations including follower, friend, mentioned, reply, quoted, refer-
ence, and hashtag. Details of two datasets are shown in Table 1, in
which homo represents the proportion of edges between nodes of
the same class. In addition, we follow the same splits of datasets
as [14] and [36] for training, validating, and testing.
5.1.2 Baselines. We compare SeBot with previous graph-based
social bot detection methods as well as typical GNNs beyond ho-
mophily and self-supervised graph contrastive learning methods.
All selected baselines are listed below:
â€¢GCN&GAT GCN [ 25] is a typical graph convolution network
that can also be seen as a low-pass filter. GAT [ 39] lies on an
attention-based information aggregation mechanism.
â€¢GraphSage [19] is an inductive GNN capable of predicting node
types that were not seen during the training process.
â€¢FAGCN [4] can adaptively utilize neighbor representations to
be similar or diverse by breaking the limitation of low-frequency
information aggregation.
â€¢H2GCN [57] introduces three simple designs: separating self
and neighbor embeddings, high-order neighbor information, and
concatenating node representations.
â€¢GPRGNN [8] employs a learnable weight for each order of neigh-
bor information.
â€¢Alhosseini et al. [1] is the first to employ graph convolutional
neural networks for detecting social bots.
â€¢FriendBot [2] utilizes network, content, temporal, and user fea-
tures obtained from the communication network, and employs
machine learning classifiers for classification purposes.
â€¢RGT [13] constructs heterogeneous views through various re-
lations and proposes the Relation Graph Transformer to obtain
node representations.
â€¢BotRGCN [15] applies relational graph neural networks to per-
form social bot detection. RGT andBotRGCN are the current
SOTA methods for social bot detection using GNNs.
â€¢DGI [40] captures consistency between nodes and the entire
graph by maximizing local-global mutual information.
â€¢GRACE [58] employs feature masking and edge removal as
graph augmentation techniques to generate views. It also uses a
discriminator to encourage a uniform distribution.
â€¢GBT [3] calculates the empirical cross-correlation matrix using
node representations and computes the loss using the Barlow-
Twins loss function.Table 1: Statistics of TwiBot-20 and MGTAB.
Dataset #nodes #e
dges class #class #relation homo
TwiBot-20 229,580 227,979human
5,2372 0.53bot 6,589
MGTAB 10,199 1,700,108human
7,4517 0.84bot 2,748
Table 2: Hyerparameter setting on TwiBot-20 and MGTAB.
Parameter T-20
MGTAB Parameter T-20
MGTAB
optimizer AdamW
AdamW hidden dimension 32
32
learning rate 0.01 0.01 subgraph order ğ‘š 2
1
dropout 0.5 0.5 L2 regularization
3e-3 3e-3
loss weight ğœ†1 0.09 0.05 loss weight ğœ†2 0.03
0.05
tree depthğ‘˜ 6 6 maximum epo
chs 70 200
tempertureğœ 0.1 0.1 trick temperatur
eğœğº 0.01 0.01
5.1.3 Hyperparameter Setting. Hyperparameter settings of our ex-
periments on TwiBot-20 and MGTAB are listed in Table 2. We used
the AdamW optimizer to update the parameters. The learning rate
is set to 0.01, which is larger compared to baseline models like
RGT, resulting in faster convergence. The dropout mechanism is
employed to prevent overfitting and maintain high generalization
capacity. The weight of two contrastive loss ğœ†1andğœ†2are set ac-
cording to sensitive study results. The tree depth is set to 6, which
is a compromise between training time and accuracy. Gumbel-Max
reparametrization trick temperature ğœğºis set to 0.01 to amplify
edge attention.
5.1.4 Implementation. Pytorch [ 32] and Pytorch Geometric [ 17]are
leveraged to implement SeBot and other baselines. All experiments
are conducted on a cluster with 8 GeForce RTX 3090 GPUs with
24 GB memory, 16 CPU cores, and 264 GB CPU memory. See our
code1for more details.
5.2 RQ1: Performance Analysis
To answer RQ1, we evaluate the performance of SeBot and 11 other
baselines on two social bot detection benchmarks. The experimental
results are presented in Table 3, which illustrates that:
â€¢SeBot demonstrates superior performance compared to all other
baselines on both datasets in terms of Accuracy and F1-score on
both datasets. Furthermore, it achieves relatively high results in
terms of Recall and Precision on TwiBot-20 and MGTAB, indicat-
ing that SeBot is better at uncovering social bots and possesses
stronger robustness. On the other hand, SeBot demonstrates the
better generalization performance across both datasets, a feat
that other methods cannot achieve.
â€¢Compared to traditional GNNs (i.e., GCN, GAT, and GraphSage),
SeBot not only considers the community structure but also is
conscious of the adversarial structure intentionally constructed
by social bots, thus exhibiting stronger detection performance.
This also highlights the need for extra fine-grained designs when
applying graph neural networks to social bot detection.
â€¢Compared to GNNs beyond homophily (i.e., FAGCN, H2GCN,
GPRGNN), SeBot extends further into the social bot detection
1https://github.com/846468230/SEBot
3847KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingguang Yang et at.
Table 3: Performance comparison on TwiBot-20 and MGTAB in terms of accuracy, F1-score, recall and precision. The best and
second-best results are highlighted with bold and underline .â€™-â€™ indicates that the method is not applicable to MGTAB due to
lack of unprocessed raw dataset.
METHODsTwiBot-20 MGTABTYPE
Accuracy F1-scor
e Recall Precision Accuracy F1-scor
e Recall Precision
GCN 77.53Â±1.73 80.86 Â±
0.86 87.62Â±3.31 75.23Â±3.08 80.07Â±0.77 51.71 Â±
4.05 75.07Â±5.93 40.08Â±5.90
CLASSIC GAT 83.27Â±0.56 85.25 Â±
0.38 89.53Â±0.87 81.39Â±1.18 85.07Â±1.19 69.32 Â±
4.02 77.33Â±2.19 63.34Â±7.31
GraphSage 85.44Â±0.43 86.68 Â±
0.61 87.66Â±2.08 85.78Â±0.86 88.58Â±1.11 77.55 Â±
2.00 82.43Â±2.39 73.32Â±3.12
FAGCN 85.43Â±0.40 87.36 Â±
0.32 93.00Â±0.73 82.39Â±0.70 88.11Â±1.43 77.43 Â±
3.20 76.36Â±7.90 79.19Â±3.41
HETEROPHILY H2GCN 85.84Â±0.34 87.57Â±
0.15 92.19Â±1.56 83.44Â±1.32 89.09Â±1.16 79.99Â±
1.53 81.00Â±5.94 79.70Â±5.01
GPRGNN 86.05Â±0.34 87.50 Â±
0.30 90.25Â±0.29 84.92Â±0.41 89.07Â±1.20 80.48 Â±
1.62 83.54Â±1.24 77.67Â±2.37
Alhosseini et al. 59.88Â±0.59 72.07Â±
0.48 95.69Â±1.93 57.81Â±0.43 - - - -
SOTA
sBotRGCN 85.75Â±0.69 87.25 Â±
0.74 90.19Â±1.72 84.52Â±0.54 89.09Â±0.61 79.66Â±0.82 80.22 Â±
3.07 79.39Â±4.00
FriendBot 75.89Â±0.47 79.97 Â±
0.34 88.94Â±0.59 72.64Â±0.52 - - - -
RGT 86.57Â±0.42 88.01Â±0.42 91.06Â±0.80 85.15 Â±
0.28 89.00Â±1.35 79.26 Â±
2.87 78.51Â±6.25 80.48Â±2.93
DGI 84.93Â±0.31 87.09 Â±
0.36 93.94Â±1.13 81.17Â±0.26 87.08Â±0.98 75.67 Â±1.62
74.61Â±2.58 76.81Â±1.56
CONTRASTI VE GBT 84.74Â±0.92 86.87 Â±
0.79 93.28Â±1.14 81.29Â±0.92 84.68Â±0.53 70.12 Â±
1.33 66.87Â±2.30 73.80Â±1.90
GRACE 84.74Â±0.88 86.90 Â±
0.84 93.56Â±1.57 81.13Â±0.55 83.16Â±1.60 66.99 Â±
3.90 63.83Â±6.00 70.77Â±2.02
SeBot 87.24Â±0.10 88.74 Â±
0.13 92.97Â±1.16 84.90Â±0.79 90.46Â±1.44 82.12 Â±
2.42 81.73Â±2.77 82.52Â±2.19 OURs
scenario, specifically on multi-relation directed graphs. In addi-
tion, better performance also implies the significant importance
of considering adversarial heterophily in social bot detection.
â€¢Compared with state-of-the-art graph-based social bot detection
methods (i.e., BotRGCN and RGT), SeBot achieves the best accu-
racy and F1-score as it further takes into account the structural
semantics present in social networks, which proves to be po-
tent for uncovering deeply concealed bots. Meanwhile, previous
graph-based detection methods relied on traditional information
aggregation patterns and could not fully capture the structural
information within the graph.
â€¢Compared with typical self-supervised graph contrastive learn-
ing methods (i.e., DGI, GBT, and GRACE), SeBot retains essential
information within the graph by minimizing structural entropy,
while other graph augmentation methods may unavoidably in-
troduce noise or lead to the loss of crucial information relevant to
downstream tasks. Itâ€™s important to mention that class imbalance
in MGTAB significantly impacts the performance of contrastive
learning techniques like DGI, leading to suboptimal results across
various metrics.
5.3 RQ2: Ablation Study
To address RQ2, we conducted ablation experiments as follows. We
separately removed the encoding tree of the entire graph, encoding
trees of subgraphs and the proposed RCM layer, and evaluated the
performance of the residual modules. We also substitute RGCN for
the encoder, and adopt different policies of graph augmentation.
The results are presented in Table 4. Removing different modules
from SeBot resulted in performance degradation on TwiBot-20
and MGTAB datasets, indicating the pivotal role in overall model
effectiveness. This emphasizes the importance of co-considering
community structure and heterophilic relations in social bot de-
tection. By comparison, graph augmentation, feature masking, or
dropout may disrupt the original feature distribution structure, a
critical step in node classification, meanwhile, adding edges may
introduce some additional structure noise, and thus lead to model
performance degradation.Table 4: Ablation study of SeBot on TwiBot-20 and MGTAB.
SettingsTwiBot-20 MGTAB
Accuracy F1-scor
e Accuracy F1-scor
e
Full model 87.24Â±0.10 88.74Â±
0.13 90.46Â±1.44 82.12Â±
2.42
w/o entire
graph tree 86.39Â±0.30 87.84Â±
0.26 89.71Â±0.98 81.09Â±
1.41
w/o subgraph trees 86.45Â±0.07 88.02Â±
0.09 90.07Â±1.04 81.79Â±
1.73
w/o RCM layer 86.24Â±0.49 87.69Â±
0.55 89.58Â±1.52 80.51Â±
2.63
RGCN as encoder 86.35Â±0.24 87.96Â±
0.26 90.32Â±1.48 81.99Â±2.44
Feature Mask 84.93Â±0.67 87.00Â±
0.70 90.00Â±1.28 81.59Â±
1.85
Feature Dropping 83.39Â±0.61 85.53Â±
0.92 89.36Â±1.48 80.58Â±
1.27
Edge Adding 86.52Â±0.42 87.96Â±0.53 89.29Â±1.39 80.60Â±
2.95
5.4 RQ3: Sensitive Analysis
To answer RQ3, we conducted experiments on TwiBot-20 (MGTAB
see in Appendix A.1) to analyze the impact of fixed encoding tree
depth and contrastive losses. We set ğ‘˜to 6, 7, and 8, and varied
hyperparameters ğœ†1andğœ†2from 0.01 to 0.1 in steps of 0.01. The
results of the experiments are presented in the form of heatmaps in
Figure 3. From this, we can intuitively observe that increasing the
depth of the tree improves the overall performance of the proposed
model. A greater depth enables a finer-grained community parti-
tion, benefiting social bot detection, but it requires longer training
time. Further, as the tree depth increases, the impact of hyperpa-
rametersğœ†1andğœ†2on model performance exhibits a fluctuating
pattern, initially decreasing and then increasing. Notably, when
ğ‘˜=7, the modelâ€™s accuracy shows minimal variation across differ-
ent hyperparameters. However, increasing or decreasing ğ‘˜results
in greater sensitivity of the accuracy to changes in the hyperpa-
rameters. This corresponds to our assertion in Section 4.2 that, in
practical scenarios, a specific tree depth is generally preferred.
5.5 RQ4: Visualization
To address RQ4, we visually represent the 128-dimensional node
embeddings generated by GCN, FAGCN, BotRGCN, RGT, GRACE,
andSeBot on TwiBot-20 by projecting them onto a 2-dimensional
3848SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
87.2486.8186.9086.3186.4886.9885.7186.3986.3986.81
86.5687.2487.3287.0787.0785.8887.0786.7386.4886.64
86.4886.9087.1586.8187.2487.1586.7386.9086.3986.98
87.1586.9886.9087.0787.4987.4087.2486.8187.0787.32
86.5686.4886.8186.9887.2486.7386.8187.1586.5686.73
86.6486.9087.2487.3286.8186.6486.8186.7387.0786.90
86.5686.6486.9086.6486.7386.7386.3987.0786.9086.73
86.9086.7386.6486.4886.4886.6486.3986.6486.7386.81
86.4886.9086.7386.4886.5686.4886.0586.5686.6486.73
86.6486.6486.3186.4886.8186.3986.3986.5686.6486.56
Treeâ€¦ depthâ€¦k=6â€¦ onâ€¦ TwiBot-200.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
86.9886.9086.7387.1586.8186.3187.2486.5686.0586.31
86.7386.7386.0587.3287.2486.8187.0787.3286.1486.81
86.2287.2487.4086.7387.0787.4086.8187.0786.3185.97
86.5686.6486.7386.9087.2486.8187.2486.9887.2486.98
86.9087.0786.9886.8187.0786.3987.0787.0786.5686.98
87.2486.9886.8187.0786.8187.0787.0787.0786.8186.31
86.9087.0786.9886.9886.8186.7386.8186.8186.8186.64
86.9086.9086.8186.9086.9887.3286.9087.0787.3286.98
87.0786.9887.5787.0786.9887.1587.2486.3187.2486.64
87.1587.2486.7387.3287.1587.3286.9086.9886.4886.81
Treeâ€¦ depthâ€¦k=7â€¦ onâ€¦ TwiBot-200.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
86.9087.0786.2286.9086.8185.4686.3986.3986.2286.56
87.1586.7386.9087.3286.9886.6486.0586.5686.3986.22
86.9886.6487.1586.8186.6486.5686.7386.3186.5686.48
87.0786.3986.9086.5686.5686.8186.5686.9086.6486.31
87.3286.9086.7386.3186.8186.7386.7386.3186.5686.56
87.0786.8186.2285.8886.1486.2286.7386.4886.1486.31
86.9886.3985.9786.3986.5686.4886.9086.9086.6486.22
86.3986.2286.8186.3186.0586.3986.4886.3186.8185.97
86.3186.4886.8186.7386.6486.1485.7186.0586.3186.64
86.6486.3986.4886.8186.3986.5685.9785.7186.2286.22
Treeâ€¦ depthâ€¦k=8â€¦ onâ€¦ TwiBot-2085.085.586.086.587.087.5
Figure 3: Sensitive analysis of hyperparameter ğœ†1andğœ†2on TwiBot-20.
(a) GCN
 (b) FAGCN
 (c) BotRGCN
 (d) RGT
 (e) GRACE
 (f)SeBot
Figure 4: Account representations visualization on TwiBot-20. Redrepresents bots, while blue represents humans.
space using T-SNE [ 38], as depicted in Figure 4. The embeddings
from GCN and BotRGCN exhibit more scattering, whereas FAGCN,
RGT, and our SeBot produce denser embeddings. GRACE embed-
dings are quite uniform but suffer from the class collapse issue,
where nodes from different classes are located closely to each
other [ 56] due to the absence of label information. It is worth not-
ing that, compared to RGT and FAGCN, our method exhibits local
clustering rather than smooth curves visible in 2-dimensional space.
This implies that nodes of similar features or belonging to the same
community tend to aggregate together in clusters or beads. This
also indicates the ability of our method to capture the inherent com-
munities present in the graph structure. Overall, the embeddings
generated by SeBot demonstrate relatively better class discrimina-
tion compared to other methods, and the inclusion of reparameteri-
zation techniques ensures that the representations avoid excessive
clustering.
5.6 RQ5: Case Study
To answer RQ5, we visualize a local community consisting of 3
subcommunities and 7 social accounts, one of which is a bot ac-
count. Due to the employment of the reparameterization technique,
edge weights generated adaptively are equal to or near 1 or -1, and
edges of two different colors are to represent them, respectively. As
shown in Figure 5, social networks contain both the edges between
nodes of the same class and the edges between nodes of different
classes. The edge weight adaptive mechanism proposed by us can
simultaneously model both types of edges by enabling positive
and negative edge weights. Furthermore, the fine-grained hierar-
chical community structure is obtained by minimizing structural
entropy with a height constraint. Message passing on the encod-
ing tree provides higher-order feature information favorable for
classification.
User A
Rated (E) for Everyone. Official EA 
SPORTS account for the FIFA franchise. Followers: 7723523 
Friends: 348
FUT Future Star no more; @Inter_enâ€™s 
Lautaro MartÃ­nezâ€™s time is now 
User B
Thaofficial page for Erigga aka 
paperboi ... emirate empire/epic media.Followers: 646773 
Friends: 2448
Congrats you won ğŸ™ŒğŸ»ğŸ™ŒğŸ» send your 
account and phone number
User C
Spanish: @LigadeCampeones Japanese: 
@UCLJapan U19: @UEFAYouthLeagueFollowers:30042227 
Friends: 535
Mission accomplished. @FCBayern are 
champions of Europe.
User D
Steady SteadyFollowers: 0
Friends: 91
domain: Politics
User E
News & Updates: Become a part of the 
@XboxAmbassadorsFollowers: 1587982
Friends: 273256
Players should once again be able to join 
matches in Tom Clancyâ€™s Ghost Recon 
Breakpoint. 
User G
@LaLiga in English. @LaLigaArab | 
@LaLigaJP | https://t.co/Gw30bq46Jd |Followers: 702974 
Friends: 225
Former captain @ivanrakit tic is BACK 
at @SevillaFC_ENG!BotF
A good place to play 11 v 11 competitive 
FIFA Pro Clubs! Followers: 1 
Friends: 11
'@EdCarterRS Hey!, would you be able 
to help me out with some stuff with  fifa
league imdoing? 
Figure 5: A case study of local community structure and gen-
erated edge attention. The same background color represents
belonging to the same sub-community, with the same parent
node on the constructed encoding tree.
6 CONCLUSION
In this paper, we propose SeBot, a novel graph-based social bot
detection framework that takes into consideration both commu-
nity structure and adversarial behaviors of social bots. SeBot ad-
dresses the aforementioned issues using three separate modules
that leverage structural entropy minimization and a heterophily-
aware encoder. SeBot employs self-supervised contrastive learning
to unify and learn the intrinsic characteristics of nodes more ef-
fectively. Comprehensive experiments show that SeBot exhibits
superior generalizability and robustness on two real-world datasets
compared with all other baselines.
ACKNOWLEDGMENTS
This work was supported by the National Key Research and De-
velopment Program of China through the grants 2022YFB3104700,
2022YFB3105405, 2021YFC3300502, NSFC through grants 62322202
and 61932002, Beijing Natural Science Foundation through grant
4222030, Guangdong Basic and Applied Basic Research Founda-
tion through grant 2023B1515120020, Shijiazhuang Science and
Technology Plan Project through grant 231130459A.
3849KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingguang Yang et at.
REFERENCES
[1]Seyed Ali Alhosseini, Raad Bin Tareaf, Pejman Najafi, and Christoph Meinel.
2019. Detect me if you can: Spam bot detection using inductive representation
learning. In WWW. 148â€“153.
[2]David M Beskow and Kathleen M Carley. 2020. You are known by your friends:
Leveraging network metrics for bot detection in twitter. SMA (2020).
[3]Piotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. 2022. Graph Barlow
Twins: A self-supervised representation learning framework for graphs. KBS 256
(2022), 109631.
[4]Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond low-frequency
information in graph convolutional networks. In AAAI. 3950â€“3957.
[5]Nan Cao, Conglei Shi, Sabrina Lin, Jie Lu, Yu-Ru Lin, and Ching-Yung Lin. 2015.
Targetvue: Visual analysis of anomalous user behaviors in online communication
systems. TVCG 22, 1 (2015), 280â€“289.
[6]Yuwei Cao, Hao Peng, Angsheng Li, Chenyu You, Zhifeng Hao, and Philip S Yu.
2024. Multi-Relational Structural Entropy. In UAI.
[7]Nikan Chavoshi, Hossein Hamooni, and Abdullah Mueen. 2017. Temporal pat-
terns in bot activities. In WWW. 1601â€“1606.
[8]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2021. Adaptive Universal
Generalized PageRank Graph Neural Network. In ICLR. OpenReview.net.
[9] Stefano Cresci. 2020. A decade of social bot detection. Commun ACM (2020).
[10] Ashok Deb, Luca Luceri, Adam Badaway, and Emilio Ferrara. 2019. Perils and
challenges of social media and election manipulation analysis: The 2018 us
midterms. In WWW. 237â€“247.
[11] Nicolas Guenon des Mesnards, David Scott Hunter, Zakaria el Hjouji, and Tauhid
Zaman. 2022. Detecting bots and assessing their impact in social networks.
Operations Research 70, 1 (2022), 1â€“22.
[12] Shangbin Feng, Zhaoxuan Tan, and etc. Herun Wan. 2022. TwiBot-22: Towards
Graph-Based Twitter Bot Detection. In NeurIPS.
[13] Shangbin Feng, Zhaoxuan Tan, Rui Li, and Minnan Luo. 2022. Heterogeneity-
aware twitter bot detection with relational graph transformers. In AAAI.
[14] Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, and Minnan Luo. 2021.
Twibot-20: A comprehensive twitter bot detection benchmark. In CIKM.
[15] Shangbin Feng, Herun Wan, Ningnan Wang, and Minnan Luo. 2021. BotRGCN:
Twitter bot detection with relational graph convolutional networks. In SNAM.
[16] Emilio Ferrara. 2017. Disinformation and social bot operations in the run up to
the 2017 French presidential election. First Monday 22, 8 (2017).
[17] Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with
PyTorch Geometric. arXiv (2019).
[18] Sami Abdullah Hamdi. 2022. Mining ideological discourse on Twitter: The case
of extremism in Arabic. Discourse & Communication 16, 1 (2022), 76â€“92.
[19] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. NIPS 30 (2017).
[20] Buyun He, Yingguang Yang, Qi Wu, Hao Liu, Renyu Yang, Hao Peng, Xiang Wang,
Yong Liao, and Pengyuan Zhou. 2024. Dynamicity-aware Social Bot Detection
with Dynamic Graph Transformers. In IJCAI.
[21] Maryam Heidari, James H Jones, and Ozlem Uzuner. 2020. Deep contextualized
word embedding for text-based online user profiling to detect social bots on
twitter. In ICDMW. IEEE, 480â€“487.
[22] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization
with Gumbel-Softmax. In ICLR (Poster). OpenReview.net.
[23] Edwin T Jaynes. 1980. The minimum entropy production principle. ARPC 31, 1
(1980), 579â€“601.
[24] Minguk Kang and Jaesik Park. 2020. Contragan: Contrastive learning for condi-
tional image generation. NIPS 33 (2020), 21357â€“21369.
[25] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR (Poster). OpenReview.net.
[26] Thai Le, Long Tran-Thanh, and Dongwon Lee. 2022. Socialbots on Fire: Modeling
Adversarial Behaviors of Socialbots via Multi-Agent Hierarchical Reinforcement
Learning. In WWW. 545â€“554.
[27] Angsheng Li and Yicheng Pan. 2016. Structural information and dynamical
complexity of networks. TOIT 62, 6 (2016), 3290â€“3339.
[28] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang,
Xiao-Wen Chang, and Doina Precup. 2021. Is heterophily a real nightmare for
graph neural networks to do node classification? arXiv (2021).
[29] Luca Luceri, Ashok Deb, Adam Badawy, and Emilio Ferrara. 2019. Red bots
do it better: Comparative analysis of social bot partisan behavior. In WWW.
1007â€“1012.
[30] Abbe Mowshowitz and Matthias Dehmer. 2012. Entropy and the complexity of
graphs revisited. Entropy 14, 3 (2012), 559â€“570.
[31] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv (2018).
[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019.
Pytorch: An imperative style, high-performance deep learning library. NIPS 32
(2019).[33] Hao Peng, Jingyun Zhang, Xiang Huang, Zhifeng Hao, Angsheng Li, Zhengtao
Yu, and Philip S Yu. 2024. Unsupervised Social Bot Detection via Structural
Information Theory. TOIS (2024).
[34] Nicolas Rashevsky. 1955. Life, information theory, and topology. Bull. Math. Biol.
17 (1955), 229â€“235.
[35] Claude Elwood Shannon. 1948. A mathematical theory of communication. BSTJ
27, 3 (1948), 379â€“423.
[36] Shuhao Shi, Kai Qiao, Jian Chen, Shuai Yang, Jie Yang, Baojie Song, Linyuan
Wang, and Bin Yan. 2023. Mgtab: A multi-relational graph-based twitter account
detection benchmark. arXiv (2023).
[37] Ernesto Trucco. 1956. A note on the information content of graphs. Bull. Math.
Biol. 18 (1956), 129â€“135.
[38] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
JMLR 9, 11 (2008).
[39] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR (Poster).
OpenReview.net.
[40] Petar Velickovic, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio,
and R Devon Hjelm. 2019. Deep graph infomax. ICLR 2, 3 (2019), 4.
[41] Patrick Wang, Rafael Angarita, and Ilaria Renna. 2018. Is this the era of misin-
formation yet: combining social bots and fake news to deceive the masses. In
WWW. 1557â€“1561.
[42] Zixuan Weng and Aijun Lin. 2022. Public opinion manipulation on social media:
Social network analysis of twitter bots during the covid-19 pandemic. JERPH 19,
24 (2022), 16376.
[43] Junran Wu, Xueyuan Chen, Bowen Shi, Shangzhe Li, and Ke Xu. 2023. SEGA:
Structural Entropy Guided Anchor View for Graph Contrastive Learning. In
ICML (Proceedings of Machine Learning Research, Vol. 202). PMLR, 37293â€“37312.
[44] Junran Wu, Xueyuan Chen, Ke Xu, and Shangzhe Li. 2022. Structural entropy
guided graph hierarchical pooling. In ICML. PMLR, 24017â€“24030.
[45] Qi Wu, Yingguan Yang, Buyun He, Hao Liu, Xiang Wang, Yong Liao, Renyu
Yang, and Pengyuan Zhou. 2023. Heterophily-aware Social Bot Detection with
Supervised Contrastive Learning. arXiv (2023).
[46] Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. 2020. Graph information
bottleneck. NIPS 33 (2020), 20437â€“20448.
[47] Yuhao Wu, Yuzhou Fang, Shuaikang Shang, Jing Jin, Lai Wei, and Haizhou Wang.
2021. A novel framework for detecting social bots with deep neural networks
and active learning. KBS 211 (2021), 106525.
[48] Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran
Xu. 2021. ConSERT: A Contrastive Framework for Self-Supervised Sentence
Representation Transfer. In ACL/IJCNLP (1).
[49] Yingguang Yang, Renyu Yang, Yangyang Li, Kai Cui, Zhiqin Yang, Yue Wang, Jie
Xu, and Haiyong Xie. 2023. Rosgas: Adaptive social bot detection with reinforced
self-supervised gnn architecture search. TWEB (2023).
[50] Yingguang Yang, Renyu Yang, Hao Peng, Yangyang Li, Tong Li, Yong Liao, and
Pengyuan Zhou. 2023. FedACK: Federated Adversarial Contrastive Knowledge
Distillation for Cross-Lingual and Cross-Model Social Bot Detection. In WWW.
1314â€“1323.
[51] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. NIPS 33 (2020),
5812â€“5823.
[52] Guangjie Zeng, Hao Peng, Angsheng Li, Zhiwei Liu, Chunyang Liu, S Yu Philip,
and Lifang He. 2023. Unsupervised Skin Lesion Segmentation via Structural
Entropy Minimization on Multi-Scale Superpixel Graphs. In ICDM.
[53] Xianghua Zeng, Hao Peng, and Angsheng Li. 2023. Effective and stable role-based
multi-agent collaboration by structural information principles. In AAAI.
[54] Xianghua Zeng, Hao Peng, and Angsheng Li. 2024. Adversarial socialbots mod-
eling based on structural information principles. In AAAI.
[55] Xianghua Zeng, Hao Peng, Angsheng Li, Chunyang Liu, Lifang He, and Philip S
Yu. 2023. Hierarchical state abstraction based on structural information principles.
InIJCAI.
[56] Mingkai Zheng, Fei Wang, Shan You, Chen Qian, Changshui Zhang, Xiaogang
Wang, and Chang Xu. 2021. Weakly supervised contrastive learning. In ICCV.
10042â€“10051.
[57] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
Koutra. 2020. Beyond homophily in graph neural networks: Current limitations
and effective designs. NIPS 33 (2020), 7793â€“7804.
[58] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.
Deep graph contrastive representation learning. arXiv (2020).
[59] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021.
Graph contrastive learning with adaptive augmentation. In WWW. 2069â€“2080.
[60] Dongcheng Zou, Hao Peng, Xiang Huang, Renyu Yang, Jianxin Li, Jia Wu, Chun-
yang Liu, and Philip S Yu. 2023. SE-GSL: A General and Effective Graph Struc-
ture Learning Framework through Structural Entropy Optimization. In WWW.
499â€“â€“510.
3850SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
86.8186.8186.6486.7386.1486.1486.5686.5686.8185.21
86.7386.8186.7386.3986.7386.5685.8085.4686.8186.05
87.1587.2486.6487.0786.0586.2286.5686.0586.6486.31
86.8186.6486.9886.6486.7386.7386.6486.0586.7386.22
86.8186.9086.6486.7386.7386.7387.0786.9886.0586.39
86.9086.6486.8186.8186.9087.0786.5686.9086.7386.64
86.1486.5687.1586.8186.7386.2286.8186.7386.2286.48
86.3986.3986.8186.8186.7386.6486.9086.4886.7386.56
86.3186.9887.1585.9786.9086.7386.6486.5686.4886.56
86.0585.9786.4885.8886.2287.1586.1486.4886.4886.73
Treeâ€¦ depthâ€¦k=3â€¦ onâ€¦ TwiBot-200.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
86.7386.9086.3186.8187.2485.4686.5686.1486.6486.73
86.7386.5687.2486.4885.6386.1486.3986.3986.9086.98
86.3186.4886.3186.4886.1486.5686.6486.4886.3986.64
86.9886.9086.9086.5686.9886.8186.5686.6486.3985.63
87.0786.8187.1586.9887.5786.8187.2486.6487.0786.56
86.9886.9886.8186.9086.6486.3986.6486.8185.6386.31
85.8087.0787.4087.0786.9886.4886.9086.7387.0785.80
86.4886.9086.7386.9086.3186.8186.6486.6486.3986.14
86.5687.3286.5686.6486.4886.0586.6485.4686.3985.97
86.7387.3286.3986.8186.5686.3186.5685.9786.4886.14
Treeâ€¦ depthâ€¦k=4â€¦ onâ€¦ TwiBot-200.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
86.7386.6486.7386.5685.8085.7185.8086.9086.8186.56
86.9087.4986.8186.9886.9086.7386.3185.7185.6385.97
87.2487.2487.2486.4886.7386.5685.8885.5586.0586.39
87.4086.8186.4885.9786.3186.3185.2185.4685.6385.88
86.9886.9086.9886.0585.0486.6486.7385.9785.3885.71
87.4086.9086.5686.2286.3185.7184.9586.5685.3886.22
87.1586.3186.7386.3186.1485.7185.1285.2986.1486.31
86.7385.9786.8186.5685.6385.8885.5585.9785.8085.97
86.5686.3186.1486.1486.6486.3185.9786.1485.9785.88
86.6486.4886.4885.1286.5685.7186.3185.8085.9786.56
Treeâ€¦ depthâ€¦k=5â€¦ onâ€¦ TwiBot-2085.085.586.086.587.087.5
(a) Sensitive analysis of hyperparameter ğœ†1andğœ†2on TwiBot-20. (Tree depth 3-5)
0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
90.0090.8890.4989.7190.2090.2990.8890.4989.5190.10
90.3989.7190.2090.6990.4990.3990.3990.8890.3990.00
90.4990.0090.3990.2990.2090.5990.1090.3990.8890.10
89.9090.3989.5190.4990.5989.6190.3990.2090.3990.49
90.5990.1090.2989.9090.6990.2090.8890.3990.6990.29
89.4190.0089.8090.4990.6990.5989.9090.2989.7190.59
90.6990.7889.7190.6989.9089.9090.2990.2990.8890.78
90.3990.2089.4189.9089.4189.7190.7889.8090.1090.49
89.7190.6990.1090.3989.7190.2990.4990.4990.2989.41
90.2090.4990.7890.2090.5989.5190.5990.0090.2090.59
Treeâ€¦ depthâ€¦k=3â€¦ onâ€¦ MGTAB0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
90.7890.8890.3990.2990.3990.7890.7890.6990.6990.39
89.8089.9089.7887.7891.2790.1089.7190.2990.7890.20
90.7891.1890.0090.1088.7390.1090.4990.3990.0090.69
90.2990.0090.2990.9890.2089.5191.0889.9090.2090.29
89.8090.4990.5988.4888.7890.4990.5990.2090.0090.49
90.3990.6990.6990.2090.2090.3991.2790.1089.7189.61
90.8890.2090.5991.0890.1090.2989.7190.5990.5991.18
90.7891.0890.3990.8890.8890.4990.3990.4990.7890.00
90.1089.2290.4990.5989.3189.7190.9889.8090.1090.20
90.5990.5990.3990.2990.1090.1089.6190.2090.3990.00
Treeâ€¦ depthâ€¦k=4â€¦ onâ€¦ MGTAB0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
90.5989.9090.2090.3990.6990.2990.6990.4990.6990.49
90.3990.5990.6990.2990.2990.6989.9090.2990.1090.10
89.7190.5989.7190.1089.9089.8090.0090.7890.4989.90
91.3790.2989.9090.1089.9089.3190.2089.7190.0090.00
90.2090.2990.4990.3990.3990.2090.2090.2990.5990.39
89.7189.9090.7890.4989.9090.5990.0089.4190.0089.61
90.4990.3989.8090.0090.6990.5989.9090.1090.1090.59
90.5989.9090.2990.4990.4989.9089.5190.2989.7190.00
90.8890.3989.7189.9089.6189.8090.7890.7890.4990.49
90.9890.3989.8089.8090.0090.1090.5990.2990.2089.90
Treeâ€¦ depthâ€¦k=5â€¦ onâ€¦ MGTAB88.088.589.089.590.090.591.091.5
(b) Sensitive analysis of hyperparameter ğœ†1andğœ†2on MGTAB. (Tree depth 3-5)
0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
90.2990.4990.3990.5990.4990.2090.4990.3990.2090.39
90.1090.6990.5990.2090.0090.7890.3990.8890.5990.49
90.9890.2990.1090.3990.4990.2990.1090.2090.6990.39
89.9091.0890.2090.8890.8890.9890.3990.6990.6989.90
90.3990.3990.6990.3990.3990.9891.0890.3990.5990.39
89.6190.7889.2290.8891.3790.1090.6990.2990.1090.10
90.0090.3989.9090.4991.1890.8891.0890.5990.3990.10
90.8890.4990.5990.0090.6989.6190.1089.9090.2089.71
90.2090.0090.1090.1090.2089.3190.9890.3990.9890.59
90.2090.6990.2090.4989.9090.1090.6990.6990.4990.29
Treeâ€¦ depthâ€¦k=6â€¦ onâ€¦ MGTAB0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
89.7190.3990.2090.8890.9890.5990.3990.5990.1089.90
90.3990.6991.0890.7890.2090.3990.8890.8890.7890.59
90.1089.1291.0889.8089.8089.5190.2990.5990.5990.29
90.3990.5990.8889.7190.8890.8889.1291.2790.6989.31
91.1890.4990.2089.7191.5790.3990.5990.2991.1890.59
89.8090.5990.8889.9090.2990.1091.0889.0290.5989.90
90.6989.9089.9090.1090.3990.7890.4989.9090.5990.98
90.2090.9890.6990.5990.6990.5990.0090.2990.4989.31
90.1090.0090.5989.7190.2989.2291.0890.2989.8089.90
90.4990.4989.5190.2989.8090.2090.7890.4989.9090.10
Treeâ€¦ depthâ€¦k=7â€¦ onâ€¦ MGTAB0.010.020.030.040.050.060.070.080.090.1
2
0.010.020.030.040.050.060.070.080.090.11
89.9090.2990.5990.0090.7890.0090.6990.5990.8890.29
90.2090.5990.7890.4990.1090.5990.2090.1091.1890.49
90.5990.0090.8890.2990.3990.2090.3990.7890.4990.29
90.2990.8890.0089.7190.4989.3189.8090.6991.2790.39
90.3989.6190.5990.4990.2990.2089.8090.7890.0090.10
90.2989.4190.7890.7890.4990.1090.2990.6990.2090.39
89.8090.0090.5990.1090.1090.5990.3990.1089.7189.31
90.1090.7890.2090.2090.2990.2990.3989.7190.4990.88
90.9890.2090.7889.9090.2990.2990.0090.0090.6989.90
89.7190.2990.3989.8090.2990.5990.0090.1090.2090.39
Treeâ€¦ depthâ€¦k=8â€¦ onâ€¦ MGTAB88.088.589.089.590.090.591.091.5
(c) Sensitive analysis of hyperparameter ğœ†1andğœ†2on MGTAB. (Tree depth 6-8)
Figure 6: Sensitive analysis of hyperparameter ğœ†1andğœ†2on TwiBot-20 and MGTAB.
A APPENDIX
A.1 Sensitive Analysis Supplement
In this subsection, we provide additional experimental results on
the impact of hyperparameters ğœ†1,ğœ†2and tree depth ğ‘˜on accuracy,
as shown in Figure 6. From Figure 6a, when ğ‘˜is set to 5, the trained
model is more sensitive to changes in hyperparameters ğœ†1andğœ†2,
and the impact is larger. When ğ‘˜=3, the overall performance is
less affected by hyperparameters, and the differences in accuracy
are smaller. We also conduct experiments on MGTAB in the same
way, which is shown in Figure 6b and 6c. It is visually evident that
the influence of hyperparameters is relatively small on MGTAB
compared to TwiBot-20. Moreover, with the increase in tree depth,
there isnâ€™t a significant overall improvement in performance. Even
whenğ‘˜=3, satisfactory results can be achieved, indicating that theperformance of SeBot on MGTAB is minimally affected by the
granularity of community partitioning.
A.2 Data Efficiency Study
Current approaches to social bot detection predominantly follow
a supervised paradigm, heavily reliant on an adequately rich set
of annotated training data. However, acquiring such a dataset is a
costly endeavor, and issues such as inaccurate annotations, noise,
and insufficient richness are widespread. Consequently, it becomes
imperative to evaluate the performance of SeBot under conditions
of limited training data, relationships, and account features. To
address this, we specifically design experimental conditions by
training solely on a subset of the data, randomly removing edges,
and masking partial features. The results are illustrated in Figure
7. Notably, even under the constraint of utilizing only 50% of the
3851KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingguang Yang et at.
10%20%30%40%50%60%70%80%90%100%0.80.850.90.95Label
87.4487.6687.7187.9888.4488.4988.4988.5188.6288.74F1-score Accuracy
10%20%30%40%50%60%70%80%90%100%0.80.850.90.95Edge
87.7288.0588.3288.4288.5588.5988.5988.6088.6088.74F1-score Accuracy
10%20%30%40%50%60%70%80%90%100%
Dataâ€¦Percentage0.80.850.90.95Feature
87.5688.0488.1488.2088.3888.5588.5988.6888.7288.74F1-score Accuracy
Figure 7: The experimental results of the data efficiency study
regarding training data, edge quantity, and feature quantity.training data, SeBot demonstrates superior performance compared
to RGT [ 13], affirming its capability to mitigate dependence on data.
Furthermore, our observations indicate that increased edges and
features contribute to enhanced performance, suggesting that both
factors play an advantageous role in detection."
3852