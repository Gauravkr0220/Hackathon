Published in Transactions on Machine Learning Research (10/2023)
Complementary Sparsity: Accelerating Sparse CNNs with
High Accuracy on General-Purpose Computing Platforms
Kang Zhaoâˆ—zhaokang29@huawei.com
Huawei Noah Arkâ€™s Lab
Yijun Tanâˆ—tanyj1998@gmail.com
SKL of Processors, Institute of Computing Technology, CAS
Kai Han kai.han@huawei.com
Huawei Noah Arkâ€™s Lab
Ting Hu huting35@huawei.com
Huawei Noah Arkâ€™s Lab
Hanting Chen chenhanting@huawei.com
Huawei Noah Arkâ€™s Lab
Tao Yuan yuantao38@huawei.com
Huawei Noah Arkâ€™s Lab
Yunhe Wangâ€ yunhe.wang@huawei.com
Huawei Noah Arkâ€™s Lab
Jun Yaoâ€ yaojun97@huawei.com
Huawei Noah Arkâ€™s Lab
Reviewed on OpenReview: https: // openreview. net/ forum? id= g1B4qgOw79
Abstract
Model sparsity is a promising approach to reducing parameters and FLOPs of convolutional
neural networks (CNNs). Compared to unstructured or coarse-grained structured sparsity,
ï¬ne-grained structured sparsity, e.g., N:M sparse pattern, can achieve better balance be-
tween accuracy and eï¬ƒciency on general computing platforms like CPUs and GPUs. In
particular, the 2:4 sparsity can accelerate CNN inference by 2 Ã—speed and with negligi-
ble accuracy drop. However, N:M sparsity needs to be supported by GPU within spe-
ciï¬c hardware circuits and hardly achieve signiï¬cant speedups on common GPUs. To ac-
celerate CNNs with general-purposed computing resources and simultaneously retain the
model accuracy as much as possible, this paper proposes complementary sparsity (CS).
CS denotes that only one weight can be retained for weights spaced at the same distance.
On the one hand, CS features high mask ï¬‚exibility, which is naturally favorable to high
model accuracy. Moreover, we propose a CS-speciï¬c sparse training method to improve
CS-based CNNsâ€™ accuracy under high parameter sparsities ( >75%). On the other hand,
CS itself is memory-access balanced and robust to pattern hyperparameters, making it an
ideal candidate for speeding up CS-based convolution computation on CPUs and common
GPUs. We thus propose a CS convolution parallel computing algorithm that adapts to
common GPUs without sparse tensor cores. Experimental results show that compared to
other sparsity patterns, the proposed CS achieves the optimal trade-oï¬€ in terms of accu-
âˆ—Equal contribution.
â€ Corresponding author.
1Published in Transactions on Machine Learning Research (10/2023)
racy and latency for CPUs and common GPUs, respectively. Codes will be available at
https://gitee.com/mindspore/models/tree/master/research/cv/CS.
1 Introduction
Weight sparsiï¬cation is a crucial method to compress CNNs. The rationality behind weight sparsiï¬cation
is that there are redundant weights in regular CNNs which tend to generate overlapped features (Hoeï¬‚er
et al., 2021; Ayinde et al., 2019). Thus, removing a certain amount of weights in CNNs has little or manage-
able impact on CNNâ€™s accuracy, while it can signiï¬cantly lower CNNâ€™s number of ï¬‚oating-point operations
(FLOPs) during inference.
According to the extent of pruning freedom and acceleration aï¬ƒnity, the existent weight sparsiï¬cation
technologies for CNNs can be divided into three categories: unstructured sparsity (US), coarse-grained
structured sparsity (CSS), and ï¬ne-grained structured sparsity (FSS). US, depicted in Fig. 1(a), also called
random sparsity in some studies (Huang et al., 2022), permits pruning weights anywhere inside a weight
tensor (Zhu & Gupta, 2017; Gale et al., 2019; Mostafa & Wang, 2019; Evci et al., 2020; Kusupati et al., 2020;
Liu et al., 2021; Ma et al., 2021; Peste et al., 2021; Tai et al., 2022; Jaiswal et al., 2022; Park & No, 2022; Chen
et al., 2021; Li et al., 2022a). Due to USâ€™s highest degree of pruning freedom, the most important weights
aï¬€ecting network quality can always be retained under any sparsity. Hence, unstructurally sparsed networks
can preserve a decent accuracy even if sparsity is very high ( â‰¥90%). However, the nonuniformity of weight
distribution makes it nearly impossible to accelerate US-based convolution on general-purpose computing
platforms. In contrast, for CSS, e.g., Fig. 1(b)-(d), the pruning granularity is block, channel, or ï¬lter -wise
(Wen et al., 2016; Li et al., 2016; Gray et al., 2017; Ji et al., 2018; Liu et al., 2017; Tang et al., 2020; Ding
et al., 2021; Hou et al., 2022; Liu et al., 2022; Chen et al., 2022; Zhang et al., 2022; He & Xiao, 2023). CSSâ€™s
patterns are generally with high regularity, which can signiï¬cantly speedup the network inference. For some
of CSS patterns such as ï¬lter pruning and channel pruning, i.e., Fig. 1(c) and 1(d), the pruned CNNs can
directly operate on the original platforms without any new acceleration algorithm design. Nonetheless, the
relatively bigger pruning granularity inevitably entails that many important weights are removed together
with unimportant weights. Under the same accuracy, CNNs within CSS own a lower compression ratio
compared to that within US patterns.
In this study, we focus on FSS since it generally results in better tradeoï¬€s between accuracy and eï¬ƒciency.
We classify a sparsity pattern as FSS if its pruning granularity is vector-wise (Yao et al., 2019; Mishra et al.,
2021; Huang et al., 2022; Tan et al., 2022; Meng et al., 2020), e.g., Fig. 1(e)-(h). Compared with US and CSS,
FSS possesses both high prunability and decent acceleration aï¬ƒnity. However, the existing FSS patterns
more or less have some shortcomings, as shown in Table 1. N:M sparsity, which has been the most popular
FSS pattern lately, mainly facilitates inference eï¬ƒciency on speciï¬c hardware, e.g., Amphere architecture
within sparse tensor cores (Choquette & Gandhi, 2020). Some work tries to accelerate N:M-base convolution
on common GPUs without sparse tensor cores (Yao et al., 2019), but the practical speedup beneï¬ts compared
to the dense convolution are probably limited, since the dense convolution has been adequately optimized
and perfectly supported by current common GPUs. Shï¬‚_BW Huang et al. (2022) and OVW Tan et al.
(2022) sparse patterns achieve practical speedups on common GPUs, but CNNs using these two patterns
have not reached a satisï¬catory accuracy so far.
In this paper, we propose a complementarily sparsed pattern to better balance the accuracy and inference
eï¬ƒciency of sparse CNNs. The design principle behind complementary sparsity (CS) is to leverage
the non-conï¬‚ict strengths of the prior sparse patterns as much as possible while addressing
their limitations . Firstly, like N:M sparsity, CS prunes weights inside a vector. Secondly, the positions of
the pruned weights and retained weights are complementary. For example, Fig. 1(e) shows a 50% CS. In
this subï¬gure, a vectorâ€™s shape is 8Ã—1, as marked by the red frame. The â€™complementaryâ€™ means that inside
the vector, if the 1st weight is pruned, then the 5th weight has to be retained. This logic is the same for
the 2nd and 6th weights, and so forth. Lastly, the size of a minimum vector to form CS is variable, which
property is called hyperparameter robustness. The hyperparameter robustness of CS makes the pattern very
adaptive to diï¬€erent computing platforms, such as CPUs and GPUs.
2Published in Transactions on Machine Learning Research (10/2023)
Huawei Proprietary -Restricted Distribution 2
ğ¶ğ‘–ğ‘›Ã—ğ»Ã—ğ‘Š ğ¶ğ‘–ğ‘›
Unstructured Sparsity Block Sparsity
N:M Sparsity
ï¼ˆ2:4ï¼‰OVW Sparsity Complementary 
Sparsity (Ours)
Shfl_BW Sparsityğ¶ğ‘œğ‘¢ğ‘¡ ğ¶ğ‘œğ‘¢ğ‘¡
Ã—ğ»
Ã—ğ‘Š
ğ¶ğ‘–ğ‘›Ã—ğ»Ã—ğ‘Š
ğ¶ğ‘œğ‘¢ğ‘¡ğ¶ğ‘œğ‘¢ğ‘¡Ã—ğ»Ã—ğ‘Š
ğ¶ğ‘–ğ‘› ğ¶ğ‘–ğ‘›
Ã—ğ»
Ã—ğ‘ŠFilter Pruning
 Channel Pruning
ğ¶ğ‘œğ‘¢ğ‘¡
2
SELE.
1ğ¶ğ‘–ğ‘›Ã—ğ»Ã—ğ‘Š
ğ¶ğ‘œğ‘¢ğ‘¡ï¼ˆaï¼‰ ï¼ˆbï¼‰ ï¼ˆcï¼‰ ï¼ˆdï¼‰
ï¼ˆeï¼‰ ï¼ˆfï¼‰ ï¼ˆgï¼‰ ï¼ˆhï¼‰
Figure 1: Visualization of diï¬€erent sparse patterns at the 50% sparsity. (a) Unstructured structured sparsity
that allows to discard weights of arbitratry positions. (b)-(d) Coarse-grained structured sparsity. (e)-(h)
Fine-grained structured sparsity. In particular, (e) is the proposed complementary sparsity and â€™2 SELE. 1â€™
represents retaining one from two weights in all the complementary positions.
The major contributions of this paper are as follows:
â€¢We propose a new sparse patternâ€”CS, which features both high mask ï¬‚exibility and high acceler-
ation aï¬ƒnity. CS allows pruning CNNs with less accuracy reduction and accelerating sparse CNNs
on both CPUs and common GPUs.
â€¢Used in the training phase, a CS-speciï¬c sparse training method is proposed to boost CS-based
CNNsâ€™ accuracy under high sparsities. With the proposed method, CS-based CNNs perform on par
with or better than that with N:M sparsity in terms of accuracy. At the 50% sparsity on ImageNet,
CS-based ResNet50 achieves 76.37% accuracy with negligible accuracy loss. At the 93.75% sparsity,
CS-basedResNet50 achieves 71.07%accuracy witha 5.32% drop, which isbetterthan N:Msparsiï¬ed
ResNet50 which drops 5.8%.
â€¢Used in the inference phase, a parallel acceleration algorithm is proposed to speedup the CS-
based convolutions on common GPUs. With the acceleration algorithm, CNNs within CS achieves
2.59Ã—~3.07Ã—speedups at the 93.75% sparsity over the dense counterparts supported by cuDNN.
Through the algorithm-software co-optimization, the proposed CS reaches better tradeoï¬€s between sparse
CNNsâ€™ model quality and inference eï¬ƒciency compared with other ï¬ne-grained structured sparse patterns.
To be clear, the advantages of our CS over similar works are shown in Table 1.
2 Related Work
Unstructured sparsity (US) Neural networks within US have been researched for a long time. The
winning ticket hypothesis denotes that there always exists a sparse neural network inside a dense network
and the subnetwork can be trained to reach the comparable accuracy as the dense one (Frankle & Carbin,
2018). Since the hypothesis was proposed, a surge of studies have focused on developing good pruning
methods to form US. Gale et al. (2019); Zhu & Gupta (2017) improve the magnitude-based pruning methods
simply by gradually sparsifying and prolonging the training time, respectively. Rather than fully training
a dense network before pruning, Mostafa & Wang (2019); Evci et al. (2020); Ma et al. (2021) adopt the
3Published in Transactions on Machine Learning Research (10/2023)
Pattern Acceleration w/o ASICs Accu.
US Almost impossible High
Block sparsity Yes Low
Filter pruning Yes Low
Channel pruning Yes Low
N:MMishra et al. (2021) Hard High
Shï¬‚_BWHuang et al. (2022) Yes Medium
OVWTan et al. (2022) Yes Medium
CS (Ours) Yes High
Table 1: Comparison among diï¬€erent sparse patterns.
sparse training to directly generate the unstructured sparse neural networks. These sparse training methods
basically contain a common mechanism that periodically prunes and regrows some weights according to
some criterion. Furthermore, Peste et al. (2021) alternatively conducts sparse and dense training. In this
way, both dense and unstructured sparse neural networks are generated after training. Instead of pruning
weights by carefully designed criterion, Kusupati et al. (2020); Tai et al. (2022) learn the sparse masks by
diï¬€erentiation. In particular, the proposed method in Tai et al. (2022) reports the state-of-the-art accuracy
of US-based CNNs on the ImageNet dataset. Generally, CNNs within US can not obtain signiï¬cant speedup
gains on CPUs and common GPUs due to the severe memory-access conï¬‚ict.
Coarse-grained structured Sparsity (CSS) Normally, CSS can be enforced along either the input or
output axes of a weight tensor. Separately shrinking the output and input axis is called ï¬lter and channel
pruning, respectively. Pruning a square block in both input and output axes is called block sparsity (Gray
et al., 2017). To our knowledge, Wen et al. (2016) is the ï¬rst to propose ï¬lter pruning or channel pruning
for CNN compression. In their study, the group LASSO method is directly enforced into weights to induce
sparsities among ï¬lters or channels. Some studies like Liu et al. (2017); Ding et al. (2021) employ extra
indicators to evaluate the ï¬lters, e.g., scaling factors in batch normalization layers, or properly placed 1Ã—1
convolutions that can be absorbed during inference. Since pruning ï¬lters also result in pruning the related
channels in the next layers, the proposed method in Li et al. (2016) jointly considers the impact of ï¬lter and
channel pruning on network accuracy. Rather than developing various hypotheses to measure the importance
of ï¬lters, Tang et al. (2020) assesses ï¬lters by observing the network responses to real data and adversarial
samples. Besides, some principles originally used for US have lately been introduced to realize CSS, e.g.,
Hou et al. (2022); Tai et al. (2022). Despite these eï¬€orts, CSS-based CNNsâ€™ accuracy is still relatively lower
and drops drastically especially when the required sparsity >70%.
Fine-grained structured sparsity (FSS) N:M sparsity is a well-known ï¬ne-grained structured sparse
pattern where at most Nnon-zero weights are retained for every continuous Mweights (Yao et al., 2019;
Mishra et al., 2021; Lu et al., 2023; Zhang et al., 2023). However, N:M sparse pattern needs to be supported
by GPUs embedded with sparse tensor cores. On common GPUs, the pattern hardly outperforms dense
convolutions supported by cuDNN. To tackle the problem, Shï¬‚_BW and OVW sparsity regard a MÃ—1
vector as an entirety which is pruned or retained together (Huang et al., 2022; Tan et al., 2022). By this
design, the retained weights and the related features during convolution computation can be easily indexed.
Thus, Shï¬‚_BWandOVWsparsitycanaccelerateconvolutionsoncommonGPUstoagreatextent. However,
the relatively large pruning unit of MÃ—1still decreases the ï¬‚exibility (Hubara et al., 2021), which results in
reduced model accuracy. In contrast, our CS can help maintain similar or better model accuracy relative to
N:M sparsity, as well as achieve the practical speedups of sparse convolutions on common GPUs and CPUs.
GPU acceleration for convolutions So far, there are basically four sorts of parallelable algorithms to
implement convolutions: direct convolution, Winograd (Chikin & Kryzhanovskiy, 2022), FFT (Wang et al.,
2020), explicit general matrix multiplication (GEMM) (Jiang et al., 2022) and implicit GEMM (Zhou et al.,
2021b). Among these, GEMM-base convolution algorithms are more performant on GPUs since the modern
parallel computing platforms have highly optimized the GEMM operations (Chetlur et al., 2014; Jorda et al.,
2019; Li et al., 2022b). However, explicit GEMM-based convolutions need to ï¬rstly invoke img2col to change
4Published in Transactions on Machine Learning Research (10/2023)
tensors to matrices, which is memory access-intensive and time-consuming. By contrast, implicit GEMM-
based convolutions remove the memory access overheads, which is top-performed in most cases. Moreover,
Tan et al. (2022) employs the implicit GEMM to accelerate sparse convolutions, which implies the potential
of implicit GEMM to speedup other sparse patterns. In this work, the implicit GEMM is also utilized to
develop the parallel acceleration algorithm of CS-based convolutions on common GPUs.
3 Method
3.1 Complementary Sparsity Formulation
Forasparseweighttensor Wwiththeshapeof CoutÃ—CinÃ—FhÃ—Fw, eachï¬lter Wihastheshape CinÃ—FhÃ—Fw
and is ï¬‚attened. We use Wi[j]to denote the jthweight in the ï¬‚attened Wi.Sis the sparsity of the weight
tensor. Two key parameters are introduced to conveniently describe CS: 1) K.Kdenotes the amount of
the complementary positions from which only one single weight should be selected. For instance, at the 50%
sparsity, there should be K= 2complementary positions for selecting a weight. for the 75% sparsity, there
areK= 4complementary positions from which a weight is selected. 2) M.Mrepresents the address oï¬€set
with which weights in the complementary positions can mutually be located. Speciï¬cally,
K=1
1âˆ’S(1)
M=L
K, Lâˆˆ{Cin/c, C inâˆ—Fhâˆ—Fw} (2)
In Equation 2, if L=Cin/c, the typical values of Minclude 2,4,8,16. Here conly means that Cinshould
be divisible by M. Then, a sparse weight tensor is regarded as complementarily sparsed as long as for any
Wi[j], iâˆˆ[1, Cout], jâˆˆ[1, M],
/bardblWi[j], Wi[j+ 1âˆ—M], ...W i[j+ (Kâˆ’1)âˆ—M]/bardbl0< K (3)
Furthermore, a sparse weight tensor is regarded as strictly conforming to the pattern of CS if and only if
/bardblWi[j], Wi[j+ 1âˆ—M], ...W i[j+ (Kâˆ’1)âˆ—M]/bardbl0= 1 (4)
To intuitively understand the deï¬nition of CS, Fig. 2 gives some examples strictly obeying the pattern of
CS across multiple sparsities. Accordingly, the parameter values, i.e., KandMof each example, are listed
in Table 2. Note that Kis only related to the speciï¬ed sparsity Sand is independent of the speciï¬c weight
tensor shapes. Unless otherwise speciï¬ed, the rest of the paper only discusses the strict CS.
Huawei Proprietary -Restricted Distribution 3
(a)
(b)
(c)
(d)
(e)0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.40.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
Figure 2: Examples of CS at diï¬€erent sparsities. The blue shading of (b)-(e) indicates the selected weights.
(a) a dense weight tensor. For convience, the tensor is 2D and with the shape of 1Ã—16, i.e., Cout= 1and
Cinâˆ—Fhâˆ—Fw= 16(b) The 50% CS of the weight tensor. (c) The 75% CS of the weight tensor. (d) The
87.5% CS of the weight tensor. (e) The 93.75% CS of the weight tensor.
5Published in Transactions on Machine Learning Research (10/2023)
Table 2: Parameter values of the examples shown in Fig. 2.
Sparsity
(%)KMEncoding
bit numberEncoding
results
50 2 8 1 0,1,1,0,0,0,1,1
75 4 4 2 1,2,3,0
87.5 8 2 3 7,4
93.75 16 1 4 14
3.2 CS-speciï¬c Gradual Sparse Training Method
The aim of designing a CS-speciï¬c sparse training method is to attain universality. That is, the desired
training method can improve the accuracy of various CS-based CNNs among diï¬€erent sparsities. With the
training method, users do not need to customize training recipes for diï¬€erent CNN structures.
Conventionally, training a sparse neural network follows the process of dense training, single-shot pruning,
and ï¬netuning. We argue the conventional training process is unfavorable to CS-based CNNs under high
sparsities ( >75%) in which case the derived sparse masks from the dense weight tensors tend to be subop-
timal. In contrast, we propose a two-phase training scheme: gradual sparse training followed by retraining.
As demonstrated in Algorithm 1, assuming that the allowed training iteration amount in each phase is I,
the ï¬rst phase starts training with a dense network and the sparsity of the network discretely steps up every
Piterations. The completion of the ï¬rst phase oï¬€ers the sparse masks conforming to the pattern of CS
and the weights as the initialization of the second phase. The second phase simply uses the same training
setups as the ï¬rst phase to retrain the retained weights in networks selected by the sparse masks. Obviously,
the novelty of our training method mainly lies in the ï¬rst phase. The ï¬rst phase comprises two features as
detailed below.
Variable number of steps for gradual sparsiï¬cation . By empirical observation, we ï¬nd the higher
the required sparsity is, the more steps for gradual sparsiï¬cation are needed to obtain the better model
quality. Hence, to obtain a CS-based CNN at the sparsity S, we set Ksteps for gradually sparsifying the
network. For instances, in case that the target sparsity is 75%, the change of sparsities during training
in the ï¬rst phase is 0%â†’25%â†’50%â†’75%, while for the target sparsity of 87.5%, the change is
0%â†’12.5%â†’25%â†’37.5%â†’50%â†’62.5%â†’75%â†’87.5%. To be formulated, for the ith training
iteration, the desired sparsity Siis:
Si=ceiling{i
Iâˆ—K}âˆ’1
K(5)
Equation 5 intuitively means that Siperforms the piecewise linear growth as training iterations. The
position of Equation 5 in the workï¬‚ow of our training method is shown in Algorithm 1. Note that gradual
sparsiï¬cation only means the amounts of weights participating in the forward passes are gradually and
discretely reduced. During backward passes, the gradients of all the weights are computed and every weight
is updated no matter which sparsity step a network is at.
CS-speciï¬c weight reselection . Since all the weights are kept updating in the ï¬rst training phase, it is
beneï¬cial to reselect the weights, i.e., update the sparse masks once in a while. Supposing every Fiterations,
the sparse masks should be updated. The update process for CS is: 1) Reshape. A weight tensor with the
shape CoutÃ—CinÃ—FhÃ—Fwis reshaped into GÃ—KÃ—L, where KandLis deï¬ned in Equation 1 and 2,
respectively. Gcan be inferred given KandL. 2) Reselection. Along Kaxis to reselect kiweights with the
highest absolute value. kis related to the sparsity step that a network is at, i.e.,
ki= (1âˆ’Si)âˆ—K (6)
The positions of the reslected weights in masks are set ones while the other positions are set zeros. 3)
Restoration, which means inversely reshape the weight tensor from GÃ—KÃ—Lback to CoutÃ—CinÃ—FhÃ—Fw.
Fig. 3 shows an example of the CS-speciï¬c weight reselection procedure, which mainly visualizes the step
2). Notably, the gradual sparsiï¬cation is exactly achieved by our weight reselection procedure by properly
setting Fto make Pdivisible by F.
6Published in Transactions on Machine Learning Research (10/2023)
Algorithm 1 Workï¬‚ow of Our Training Method
Initialization: Dense weights W
Input: Required sparsity S, training iterations I, data D
Output: Sparse weights WS
Key params: K, sparse mask M, mask updating freq. F
1:â€”â€”â€”â€”â€”â€”- Gradual Sparse Training Phase â€”â€”â€”â€”â€”â€”-
2:fori= 0;i < I ;i+ +do
3:ifi%Fthen
4: Siâ†Equation 5( i, I, K)
5: Mâ†Weights_Reselect( Si, W)
6:end if
7:Forward( W, M, D)
8:Backward( W, D)
9:Weights_Update( W) #Update all weights
10:end for
11:â€”â€”â€”â€”â€”â€”â€”â€”â€“ Retraining Phase â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
12:fori= 0;i < ite ;i+ +do
13:Forward( W, M, D)
14:Backward( W, M, D)
15:Weights_Update( W, M) #Update selected weights
16:end for
17:â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
18:WSâ†W*M
Huawei Proprietary -Restricted Distribution 4
-1.2 -0.2 1.2 2.0
0.4 -1.2 -0.3 1.9
1.0 -2.3 0.8 -2.3
-0.9 2.0 0.2 -0.7-2.1 -0.1 1.4 2.9
0.3 -0.2 -0.2 1.4
1.1 -1.3 0.9 -2.1
-0.7 1.0 0.1 -0.6
-1.3 -0.1 1.8 1.0
0.3 -0.3 -0.1 1.3
1.1 -2.9 0.4 -2.7
-0.6 1.0 0.1 -0.6-1.2 -0.1 1.2 2.0
0.3 -0.3 -0.1 1.3
1.0 -2.3 0.8 -2.3
-0.6 2.0 0.1 -0.6Selected weight
Pruned weightğ‘†1=0
ğ‘˜1=4ğ‘†2=25%
ğ‘˜2=3
ğ‘†3=50%
ğ‘˜3=2ğ‘†4=75%
ğ‘˜4=1ğ¾ğ¿ ğº
After ğ‘ƒ
iterations
After ğ‘ƒ
iterations
After ğ‘ƒ
iterations
Figure 3: An example of CS-speciï¬c weight reselection.
3.3 Acceleration of CS-based Convolution
The obtained sparse weight tensors after training are stored in the compressed format. That is, only the non-
zero weights and their indices are stored and used for inference. This procedure is formulated by, converting
WStoWsandIdx, where Wsis the non-zero weight tensor with the smaller shapes and Idxis the index
tensor. The index of a non-zero weight denotes the weightâ€™s position number among Kcomplementary
positions, thus the value of an index is constantly less than K. Fig. 4 exempliï¬es the compression of CS at
the 50% sparsity. In this case, the Wshas half of the shape than WS. Although Idxhas the same shape as
Ws, each index in Idxcan be encoded with very low numbers of bits. As shown in Table 2, only at most 4
bits are needed to encode a non-zero weightâ€™s index.
7Published in Transactions on Machine Learning Research (10/2023)
Huawei Proprietary -Restricted Distribution 5
-1.2 -0.2 1.2 2.0
0.4 -1.2 -0.3 1.9
1.0 -2.3 0.8 -2.3
-0.9 2.0 0.4 -0.7-1.2 -2.3 1.2 1.9
-0.9 2.0 0.4 -2.3
0101
1110Non-zero weights
(uniform)
Index-1.2 0 1.2 0
0 0 0 1.9
0-2.3 0-2.3
-0.9 2.0 0.4 0-1.2 -2.3 1.2 1.9
-0.9 2.0 0.4 -2.3
0101
1110Non-zero weights
(uniform)
Index
Figure 4: A diagram of CS compression at the 50% sparsity.
After acquiring WsandIdx, given an input activation X, the output featuremap Ycan be computed in
parallel. Contrary to the conventional method of feature reuse to speedup sparse convolutions Tan et al.
(2022) on common GPUs, we propose a weight reuse-based algorithm. In implicit GEMM-based dense
convolutions, each thread is generally in charge of a subblock in Y, e.g., of size 4âˆ—4. Then,
Yi:i+4,j:j+4=L/summationdisplay
n=0Wi:i+4,nâŠ—Xn,j:j+4 (7)
In Equation 7, Lis only equal to Cinâˆ—Fhâˆ—Fwfor GPUs, andâŠ—represents the outer product operation of
two vectors. When CS-based convolution is conducted, Equation 7 can be modiï¬ed into:
Yi:i+4,j:j+4=Lâˆ—(1âˆ’S)/summationdisplay
n=0Wi:i+4,nâ—¦Xf(n,i),j:j+4, (8)
where
f(n, i) =n+Mâˆ—Idx[i:i+ 4, n] (9)
andâ—¦in Equation 8 denotes the operation:
â—¦=ï£®
ï£¯ï£¯ï£°Wiâˆ—Xn+Mâˆ—Idx[i,n],j:j+4
Wi+1âˆ—Xn+Mâˆ—Idx[i+1,n],j:j+4
Wi+2âˆ—Xn+Mâˆ—Idx[i+2,n],j:j+4
Wi+3âˆ—Xn+Mâˆ—Idx[i+3,n],j:j+4ï£¹
ï£ºï£ºï£»(10)
Equation 8 makes it possible to compute CS-based convolutions by chunk. For a non-zero weight, there are
Krelated sub-blocks in an input activation that may be indexed during convolution. On common GPUs,
by loading all the related Ksub-blocks to GPUsâ€™ shared memory in advance, Equation 8 can be conducted
eï¬ƒciently. Algorithm 2 shows this parallel acceleration process, where 50% CS-based convolution is taken
for example.
On CPUs, the direct method is employed to accelerate CS. Due to the instruction limit, CPUs can hardly
fetch multiple values far apart from each other. Accordingly, our CS allows diï¬€erent values of Mwithout
reducing network accuracy, which is very friendly to CPU operation. During convolutions, CPUs conduct
sparse vector products along the Cinaxis. In this case, L=Cin/c.
4 Experiments
4.1 Datasets, Models and Settings
CIFAR100 Krizhevsky et al. (2009) and ImageNet-1k Deng et al. (2009) are two datasets used to test
the accuracy of CS-based CNNs. Speciï¬cally, on CIFAR100, we evaluate three classical CNNs including
VGG-19 Simonyan & Zisserman (2014), ResNet-18 and ResNet-50 He et al. (2016), and two parameter-
eï¬ƒcient CNNs including MobileNetv2 and SqueezeNet Sandler et al. (2018); Iandola et al. (2016). Since
on CIFAR100, low sparsities ( â‰¤75%) may result in insigniï¬cant accuracy diï¬€erences between CNNs with
8Published in Transactions on Machine Learning Research (10/2023)
Algorithm 2 Parallelization for 50% CS-based convolution
Input: Idx,W,X
Output: Y
Key params: M, L
__Shared__ ï¬‚oat4 local_w, local_idx
__Shared__ ï¬‚oat4 local_x[2], local_y
1:â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Parallelism â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-
2:for all Nâˆ—OCâˆ—OHâˆ—OW/ 16threadsdo
3:Subblock(Y)â†SubConv ( thread[i], Idx,W,X)
4:end for
5:â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Detailsâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
6:function SubConv (tid,Idx,W,X)
7:fori= 0;i < L ;i+ =BMdo
8: local_filterâ†Subblock( filter)
9: local_idxâ†Subblock( Idx)
10: local_x[0]â†Subblock1( X)
11: local_x[1]â†Subblock2( X)
12: Syncthreads();
13: local_y= Eq. 8( local_w,local_x,local_idx)
14:end for
15:return local_y
16:end function
our CS and with other sparse patterns, we test the three classical CNNs under high sparsities: 87.5% and
93.75%, while for MobileNetv2 and SqueezeNet, accuracy results under the 50% and 75% sparsities are
adequately distinguishable for comparison. At each sparsity, each CNN is sparsiï¬ed by US, ï¬lter pruning,
N:M, OVW, and CS, respectively. All the sparse CNNs are ï¬rstly trained with the same training paradigm:
dense training, pruning, and ï¬netuning. This paradigm has been widely used to form various sparse CNNs.
For simplicity, the ï¬netuning phase uses the same setting as the dense training phase, which has been veriï¬ed
as reasonable in Mishra et al. (2021). After that, CS-based CNNs are trained with the proposed CS-speciï¬c
gradual training method for comparison. All the trainings use the common and the same settings. The total
training epoch is 400. For the conventional training paradigm, the dense training phase uses 200 epochs and
the ï¬netuning phase uses the rest. Similarly, for our CS-speciï¬c gradual training method, each training phase
equally uses 200 epochs as well. Each experiment is repeated three times and all the experimental results on
CIFAR100 are listed in the format of "mean Â±standard deviation" to reduce the inï¬‚uence of random factors.
On ImageNet-1k, CS-based ResNet-50 at diï¬€erent sparsities are trained for comparing with other related
works. We use the oï¬ƒcially recommended hypermeter settings for our sparse training method zlm (2022).
Besides, the speedups of CS-based CNNs over the dense counterparts are measured on an Intel(R) Xeon(R)
Gold 6154 CPU and a Tesla V100 GPU without sparse tensor cores, respectively.
4.2 Results on CIFAR100
Table 3 shows the accuracy of diï¬€erent networks within diï¬€erent sparse patterns on CIFAR100. Firstly,
for the three classical CNNs, our CS achieves the best accuracy under high sparsities among all the sparse
patterns. Secondly, for MobileNetv2 and SqueezeNet, our CS also outperforms other ï¬ne-grained structured
sparse pattern at the 75% sparsity. In particular, the accuracy of MobileNetv2 within CS at the 75%
sparsity is even higher than that within the unstructured sparsity, i.e., 62.63% >61.7%. Thirdly, the proposed
training method signiï¬cantly improves the accuracy of a series of CS-based CNNs, with the average accuracy
increasing from 0.48% to 2.83%. Notably, at the 50% sparsity, all types of sparse patterns lead to lossless
accuracy. In this case, we argue the accuracy diï¬€erences among the patterns and training methods are
immaterial.
9Published in Transactions on Machine Learning Research (10/2023)
Spa.
(%)VGG19 Resnet18 Resnet50 Spa.
(%)Mobilenetv2 SqueezeNet
Origin 70.8 74.7 72.2 Origin 65.3 65.1
87.5Unstructured 71.3Â±0.1 73.3Â±0.1 72.3Â±0.3
50Unstructured 66.9Â±0.1 67.7Â±0.2
Filter pruning 47.8Â±0.6 59.0Â±0.2 48.4Â±1.0 Filter pruning 65.5Â±0.5 36.4Â±0.2
N:M(1:8) 70.6Â±0.1 72.9Â±0.1 72.1Â±0.1 N:M(2:4) 66.4Â±0.3 67.2Â±0.1
OVW 63.6Â±0.3 64.1Â±0.7 59.2Â±1.6 OVW 61.8Â±0.3 64.0Â±0.4
CS-C 70.7Â±0.2 73.0Â±0.1 72.5Â±0.3 CS-C 66.4Â±0.1 67.2Â±0.1
CS (Ours) 71.7Â±0.373.5Â±0.173.1Â±0.0 CS (Ours) 66.7Â±0.2 67.0Â±0.2
/triangle +1.4 +0.5 +0.6 /triangle +0.2 -0.2
93.75Unstructured 69.8Â±0.1 71.5Â±0.0 71.3Â±0.0
75Unstructured 61.7Â±0.1 64.5Â±0.2
Filter pruning 33.2Â±0.4 47.9Â±0.4 40.0Â±0.6 Filter pruning 53.4Â±1.1 15.9Â±0.2
N:M(1:16) 68.1Â±0.3 70.4Â±0.2 70.6Â±0.3 N:M(1:8) 58.9Â±0.4 63.4Â±0.3
OVW 59.6Â±0.2 60.6Â±0.3 41.0Â±13.8 OVW Failed 54.6 Â±0.9
CS-C 68.5Â±0.2 70.5Â±0.1 70.5Â±0.2 CS-C 59.8Â±0.1 63.9Â±0.3
CS (Ours) 69.9Â±0.172.2Â±0.373.0Â±0.3 CS (Ours) 62.6Â±0.2 64.4Â±0.1
/triangle +1.4 +1.7 +2.5 /triangle +2.8 +0.5
Table 3: Accuracy of sparse CNNs on CIFAR100. â€™Spa.â€™ means sparsity. â€™CS-Câ€™ represents CS-based
CNNs formed by the Conventional training paradigm, while â€™CS (Ours)â€™ means that formed by the proposed
training method. â€™ /triangleâ€™ means the diï¬€erence between â€™CS-Câ€™ and â€™CS (Ours)â€™. For spatial brevity, all the data
are rounded to one signiï¬cant digit.
Apart from Table 3, the experimental results on CIFAR100 using the same network for all sparsity ratios are
shown in Figure 5. It is observed that the curves of our CS are generally above the curves of other structured
patterns and are overlapped with curves of unstructured sparsity. That observation further demonstrates
the superiority of our CS in terms of accuracy across all the sparsity ratios.
Huawei Proprietary -Restricted Distribution 20
W W Wï¼ˆaï¼‰ ï¼ˆbï¼‰ ï¼ˆcï¼‰
Figure 5: Accuracy vs. sparsity curves of diï¬€erent CNNs whitin diï¬€erent sparse patterns. (a) VGG19. (b)
ResNet18. (c) ResNet50.
Notably, on CIFAR100, our CS performs even better than unstructured sparsity due to the relatively small
dataset size. Generally, compared to modern DNNsâ€™ capacity, CIFAR100 is easy to be overï¬tted. Thus,
within a certain sparsity range, weight sparsiï¬cation can largely regularize the model capacity and lead to
higher accuracy over dense counterparts (Hoeï¬‚er et al., 2021). Compared to unstructured sparsity, ï¬ne-
grained structured sparsity such as CS and N:M constrains a model more strictly. This constraint generally
endows the model favorable regularization on small datasets like CIFAR100 and at low sparsity like 50%
and 75%. For example, as shown in Figure 7, at 50% sparsity for VGG19 on CIFAR100, N:M sparsity leads
to 73.00% accuracy, which is better than 72.82% of unstructured sparsity. At 75% sparsity, our CS-based
VGG19 reaches 72.51% accuracy, while unstructured sparsity-based VGG19 is 72.13%.
10Published in Transactions on Machine Learning Research (10/2023)
Under higher sparsity such as 93.75%, despite the rare possibility of overï¬tting, the accuracy gap between
ï¬ne-grained structured patterns like â€™ CS-Câ€™ and â€™unstructuredâ€™ was still narrow due to the small dataset
size. Hence, with our proposed training scheme, â€™ CS(Ours) â€™ can easily ï¬ll the gap to further become on par
with or to even surpass â€™unstructuredâ€™. By contrast, on ImageNet, our CS and other ï¬ne-grained sparsity
patterns bring constantly lower accuracy than unstructured sparsity regardless of sparsity ratios and network
architectures.
4.3 Results on ImageNet
Table 4 shows the experimental results on ImageNet. Compared with the OVW and Shï¬‚_BW patterns,
our CS with the proposed training scheme leads to better accuracy under high sparsities, e.g., 93.75%. For
other sparsities, our CS achieves comparable accuracy with the state-of-the-art N:M sparsity. However, the
diï¬€erent settings of Min N:M sparsity signiï¬cantly aï¬€ect the network accuracy, e.g., Sun et al. (2021). On
the contrary, our CS is robust to the pattern hyperparameter setting which will be shown in the ablation
study.
Pattern SparsityError(%) Params
(M)Flops
(G) Ori. Pruned Gap
N:M 2:4 77.3 77.0 0.3 13.8 2.15
OVW 50% 76.12 75.76 0.36 13.8 2.15
CS(Ours) 50% 76.39 76.37 0.02 13.8 2.15
N:M 1:4 77.3 75.3 2 7.93 1.17
OVW 70% 76.12 73.35 2.77 9.14 1.37
CS(Ours) 75% 76.39 75.18 1.21 7.93 1.17
Shï¬‚_BW 80% N/A 75.94 N/A 6.78 0.98
CS(Ours) 87.5% 76.39 72.44 3.95 5.02 0.69
N:M 1:16 77.3 71.5 5.8 3.52 0.44
Shï¬‚_BW 90% N/A 73.09 N/A 4.43 0.59
CS(Ours) 93.75% 76.39 71.07 5.32 3.52 0.44
Table 4: ResNet50 accuracy comparison among diï¬€erent ï¬ne-grained structured sparse patterns on Ima-
geNet. The results of N:M, OVW, and Shï¬‚_BW are from Zhou et al. (2021a), Tan et al. (2022) and Huang
et al. (2022), respectively. â€™N/Aâ€™ means the related work does not report the result.
Besides, although this work mainly focuses on CNNs, we also give the preliminary results of CS and N:M
sparsity on Transformer structures as shown in Table 5. The DeiT-small is used and the training settings
of CS-based DeiT-small and N:M-based DeiT-small are identical. That is, our proposed training scheme
is not used this time for an absolutely fair comparison. Experimental results show once more that our CS
also incurs a comparable accuracy as N:M sparsity on Transformer architectures. Note that at 50% sparsity,
both N:M sparsity and CS -based DeiT-small surpass the dense one, so the diï¬€erence between the two sparse
patterns at 50% sparsity is trivial.
With the experimental results of CNN and Transformer architectures, we would like to reaï¬ƒrm that our
CS mainly achieves comparable accuracy to N:M sparsity under identical training settings. The role of our
proposed training scheme is only to improve a CS-based networkâ€™s accuracy under high sparsity ratios. For
instance, at 93.75%, ResNet50 within CS surpasses that within N:M sparsity by a decent margin, i.e., ~0.5%
on ImageNet as shown before in Table 4. In other words, a better acceleration aï¬ƒnity on CPUs and common
GPUs under comparable accuracy, is truly our CSâ€™s advantage over N:M sparsity.
Finally, due to the slow progress in the explainability of DNNs, the model accuracy of CS lacks clear
theoretical support. However, we found a metric called mask diversity that may provide some insights
(Hubara et al., 2021). The metric is deï¬ned as the number of possible masks for a sparse pattern given a
sparsity ratio. For example, for a 8Ã—8weight tensor and a 50% sparsity, our CS has 232= 4,294,967,296
possible masks, while other vector-wise sparsity such as OVW can only have/parenleftbig16
8/parenrightbig
= 12870 masks, not to
11Published in Transactions on Machine Learning Research (10/2023)
mention only/parenleftbig8
4/parenrightbig
= 70masks that channel pruning can provide. Altogether, CSâ€™s high mask ï¬‚exibility
probably promotes the high accuracy of CS-based CNNs.
Sparsity(%) 50 75 87.5 93.75
N:M 77.61 73.69 67.56 60.08
CS(Ours) 77.4 73.66 67.63 60.01
Table 5: Comparison between CS and N:M sparsity using Top1 accuracy of DeiT-small on ImageNet. The
dense DeiT-small is 75.56%.
4.4 Ablation Study
Firstly, we investigate the eï¬€ect of diï¬€erent mask updating frequencies in our training method, i.e., F
mentioned in Algorithm 1, on network accuracy. The results are shown in Table 6. In the table, F= 0
means updating the mask once for every iteration, F= 0.5means that updating frequency is 0.5 epoch, and
soon. Weï¬ndthatthehighertherequiredsparsityis, thehigherthemaskupdatingfrequencyshouldbe. For
example, at the 50% sparsity, F= 8is the best, while at 93.75%, F= 0outperforms others. The Fsettings
in Table 6 is exactly used in training CS-based ResNet50 on ImageNet. Secondly, we investigate a pattern
hypermeter of CS: M. Speciï¬cally, under the same sparsity decided by K, CS-based CNNs with diï¬€erent
settings of Mare trained and we conduct pairwise t-test on these CNNsâ€™ accuracy. As shown in Table 7, all
thepvalues are larger than 0.05, which indicates that our CS is robust to pattern hyperparameters. The
robustness is quite beneï¬cial for acceleration as CPUs and GPUs can employ diï¬€erent values of Mto meet
the respective constraints in memory access and instruction set.
F 50% 75% 87.5% 93.75%
0 74.7Â±0.08 74.14Â±0.63 73.44Â±0.1972.06Â±0.38
0.5 74.78Â±0.0974.55Â±0.06 73.57Â±0.2771.9Â±0.52
1 74.68Â±0.06 74.35Â±0.37 73.47Â±0.6 71.62Â±0.2
2 74.71Â±0.24 74.12Â±0.23 73.16Â±0.37 71.19Â±0.59
4 74.85Â±0.08 74.21Â±0.08 72.85Â±0.23 70.85Â±0.48
874.93Â±0.3774.28Â±0.21 72.54Â±0.61 70.47Â±0.17
10 74.66Â±0.19 74.5Â±0.19 73.06Â±0.17 70.14Â±0.09
Table 6: ResNet18 on CIFAR100: Impact of Facross sparsities.
Sparsity (M, K)VGG19 ResNet18 ResNet50 Sparsity (M, K)MobileNetv2 SqueezeNet
87.5%(2,8) 71.28Â±0.15 73.68Â±0.32 73.46Â±0.85
50%(2,2) 66.81Â±0.14 67.19Â±0.29
(4,8) 71.46Â±0.49 73.2Â±0.39 73.23Â±0.65 (4,2) 66.65Â±0.15 67.07Â±0.04
(8,8) 71.55Â±0.39 73.47Â±0.16 73.38Â±0.39 (8,2) N/A 67.27Â±0.09
t-test 0.77 0.29 0.82 t-test 0.18 0.57
93.75%(2,16) 69.62Â±0.09 71.89Â±0.19 72.23Â±1.4
75%(2,4) 62.72Â±0.14 64.26Â±0.07
(4,16) 69.76Â±0.19 72.22Â±0.39 72.56Â±0.11 (4,4) N/A 64.3Â±0.22
t-test 0.31 0.39 0.71 t-test N/A 0.7
Table 7: Accuracy of CS-based CNNs with diï¬€erent settings of Mon CIFAR100
4.5 Results on Speedups
On CPU, Mis set 2,4,8,16 on demand. On GPUs, we set Mas large as possible, i.e., M=Cinâˆ—Fhâˆ—Fw/K.
We ï¬nd that the larger Mmakes indexing more easier on GPU. Fig. 6 shows the normalized speedups on CS.
Firstly, three typical convolutions in ResNet50 are used for test speedup. As shown in Fig. 6(a), with batch
size equal to 1, CPU achieves 4.27 Ã—, 5.46Ã—and 7.7Ã—speedups for the 3rd, 11th, 41th convolutions at the
12Published in Transactions on Machine Learning Research (10/2023)
Huawei Proprietary -Restricted Distribution 7
W W Wï¼ˆaï¼‰ ï¼ˆbï¼‰ ï¼ˆcï¼‰
Figure 6: Speedups for CS. (a) CPU speedups for three CS-based convolutions in ResNet50. (b) GPU
speedups for three CS-based convolutions in ResNet50. (c) GPU speedups for three CS-based CNNs.
Huawei Proprietary -Restricted Distribution 10
50
75
85
9560
70
807587.5
93.75
7590
Figure 7: ResNet50 on CIFAR100: Normalized accuracy-speedup curves of three ï¬ne-grained structured
sparse patterns. Note N:M sparsity does not have acceleration gains on common GPUs.
93.75% complementary sparsity, respectively. Similarly, as shown in Fig. 6(b), with batch size equal to 64,
GPU respectively achieves 4.02 Ã—, 3.33Ã—and 2.52Ã—speedups at 93.75% over dense counterparts supported by
cuDNN. Secondly, we also estimate the speedups on network-level by averaging all the convolutionsâ€™ runtime.
GPU achieves 2.75 Ã—, 3.07Ã—, and 2.59Ã—speedups for VGG19, ResNet50 and SqueezeNet, respectively. These
speedup performances prove the eï¬ƒciency of the proposed parallel acceleration algorithm. In addition, our
CS generally reaches better accuracy-speedup tradeoï¬€s compared with the OVW and Shï¬‚_BW pattern, as
shown in Fig. 7.
4.6 Results on Practical Inference Performance
Table 8 shows the speedup comparison of CS and N:M sparsity on A100 and V100 GPUs, respectively. Note
that the speedup data of N:M sparsity are directly cited from A100 materials that are publically available.
Although A100 GPUs have a far stronger computing power than previous ones, it is noteworthy that A100
GPUs are quite inï¬‚exible as it can only support the acceleration of 2:4 sparsity. Other sparsity ratios, such
as 75% (1:4), 87.5% (1:8), and 93.75% (1:16) can not incur any speedups on A100. In contrast, our CS
supports a wide range of sparsity ratios on common GPUs.
13Published in Transactions on Machine Learning Research (10/2023)
50% 75% 87.5% 93.75%
N:M
(A100)2 0 0 0
CS
(V100)1.39 1.86 2.48 3.07
Table 8: ResNet50 speedups comparison at diï¬€erent sparsities on V100 and A100
In addition, CS-based ResNet50 inference energy consumptions at diï¬€erent sparsities are shown in Table
9. As the sparsity ratio arises, the reduced FLOPs eï¬ƒciently convert to the shorter inference time. At the
93.75% sparsity and 64 batch size, CS-based ResNet50 only consumes 3.28J energy for one inference, which
is energy-eï¬ƒcient for cloud server scenarios.
Sparsity(%) Gï¬‚ops Times(ms) Energy cost(J)
50 2.15 28.98 7.25
75 1.17 21.59 5.4
87.5 0.69 16.24 4.06
93.75 0.44 13.12 3.28
Table 9: Energy consumption of ResNet50 within CS pattern. batch_size= 64
5 Conclusion and Future Work
We propose a novel CS to accelerate sparse CNNs on CPUs and common GPUs and retain the network
accuracy as much as possible. To our knowledge, we are the ï¬rst to report the practical speedups on
both CPUs and common GPUs for a sparse pattern. Not only does the proposed CS feature high mask
ï¬‚exibility that contributes a lot to sparse CNNsâ€™ accuracy, but also the network accuracy is robust to
pattern hyperparameters. The robustness enhances CSâ€™s adaptability to diï¬€erent computing platforms.
we would like to denote that the limitation of our proposed training scheme for CS is that the scheme is
hardly viable for massive pretraining data. It is because our training scheme needs to be embedded into
the pretraining stage and huge amounts of pretraining data make it actually impossible to replicate the
pretraining. Thus, it is diï¬ƒcult for our scheme to be applied in scenarios such as LLMs. However, we argue
that the limitation does not disqualify our major contributions for CS and we will handle the limitation in
future work.
6 Acknowledgement
We gratefully acknowledge the support of MindSpore (Hua, 2020), CANN (Compute Architecture for Neural
Networks) and Ascend AI Processor used for this research.
References
Huawei, mindspore, 2020. MindSpore.https://www.mindspore.cn/.
Image classiï¬cation reference training scripts, 2022. https://github.com/pytorch/vision/tree/main/
references/classification .
Babajide O Ayinde, Tamer Inanc, and Jacek M Zurada. Regularizing deep neural networks by enhancing
diversity in feature extraction. IEEE transactions on neural networks and learning systems , 30(9):2650â€“
2661, 2019.
Tianlong Chen, Xuxi Chen, Xiaolong Ma, Yanzhi Wang, and Zhangyang Wang. Coarsening the granularity:
Towards structurally sparse lottery tickets. In International Conference on Machine Learning , pp. 3025â€“
3039. PMLR, 2022.
14Published in Transactions on Machine Learning Research (10/2023)
Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng
Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advances
in Neural Information Processing Systems , 34:19637â€“19651, 2021.
Sharan Chetlur, Cliï¬€ Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and
Evan Shelhamer. cudnn: Eï¬ƒcient primitives for deep learning. arXiv preprint arXiv:1410.0759 , 2014.
Vladimir Chikin and Vladimir Kryzhanovskiy. Channel balancing for accurate quantization of winograd
convolutions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 12507â€“12516, 2022.
Jack Choquette and Wish Gandhi. Nvidia a100 gpu: Performance & innovation for gpu computing. In 2020
IEEE Hot Chips 32 Symposium (HCS) , pp. 1â€“43. IEEE Computer Society, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248â€“255. Ieee,
2009.
Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, and Guiguang Ding.
Resrep: Losslesscnnpruningviadecouplingrememberingandforgetting. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 4510â€“4520, 2021.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making
all tickets winners. In International Conference on Machine Learning , pp. 2943â€“2952. PMLR, 2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 , 2018.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint
arXiv:1902.09574 , 2019.
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. arXiv preprint
arXiv:1711.09224 , 3:2, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770â€“778, 2016.
Yang He and Lingao Xiao. Structured pruning for deep convolutional neural networks: A survey. arXiv
preprint arXiv:2303.00566 , 2023.
Torsten Hoeï¬‚er, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning:
Pruning and growth for eï¬ƒcient inference and training in neural networks. The Journal of Machine
Learning Research , 22(1):10882â€“11005, 2021.
Zejiang Hou, Minghai Qin, Fei Sun, Xiaolong Ma, Kun Yuan, Yi Xu, Yen-Kuang Chen, Rong Jin, Yuan
Xie, and Sun-Yuan Kung. Chex: channel exploration for cnn model compression. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12287â€“12298, 2022.
Guyue Huang, Haoran Li, Minghai Qin, Fei Sun, Yufei Ding, and Yuan Xie. Shï¬‚-bw: accelerating deep
neural network inference with tensor-core aware weight pruning. In Proceedings of the 59th ACM/IEEE
Design Automation Conference , pp. 1153â€“1158, 2022.
Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accelerated sparse
neural training: A provable and eï¬ƒcient method to ï¬nd n: m transposable masks. Advances in Neural
Information Processing Systems , 34:21099â€“21111, 2021.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint
arXiv:1602.07360 , 2016.
15Published in Transactions on Machine Learning Research (10/2023)
Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, and Zhangyang Wang. Training your sparse
neural network better with any mask. In International Conference on Machine Learning , pp. 9833â€“9844.
PMLR, 2022.
Yu Ji, Ling Liang, Lei Deng, Youyang Zhang, Youhui Zhang, and Yuan Xie. Tetris: Tile-matching the
tremendous irregular sparsity. Advances in Neural Information Processing Systems , 31, 2018.
Jiazhi Jiang, Dan Huang, Jiangsu Du, Yutong Lu, and Xiangke Liao. Optimizing small channel 3d convolu-
tion on gpu with tensor core. Parallel Computing , 113:102954, 2022.
Marc Jorda, Pedro Valero-Lara, and Antonio J Pena. Performance evaluation of cudnn convolution algo-
rithms on nvidia volta gpus. IEEE Access , 7:70461â€“70473, 2019.
Alex Krizhevsky, Geoï¬€rey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and
Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference
on Machine Learning , pp. 5544â€“5555. PMLR, 2020.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ï¬lters for eï¬ƒcient
convnets. arXiv preprint arXiv:1608.08710 , 2016.
Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Eapruning: Evolutionary pruning for vision transformers and
cnns.arXiv preprint arXiv:2210.00181 , 2022a.
Shigang Li, Kazuki Osawa, and Torsten Hoeï¬‚er. Eï¬ƒcient quantized sparse matrix operations on tensor
cores.arXiv preprint arXiv:2209.06979 , 2022b.
Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pech-
enizkiy, ZhangyangWang, andDecebalConstantinMocanu. Sparsetrainingviaboostingpruningplasticity
with neuroregeneration. Advances in Neural Information Processing Systems , 34:9908â€“9922, 2021.
Yufan Liu, Jiajiong Cao, Bing Li, Weiming Hu, and Stephen Maybank. Learning to explore distillability
and sparsability: a joint framework for model compression. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 2022.
ZhuangLiu, JianguoLi, ZhiqiangShen, GaoHuang, ShoumengYan, andChangshuiZhang. Learningeï¬ƒcient
convolutional networks through network slimming. In Proceedings of the IEEE international conference
on computer vision , pp. 2736â€“2744, 2017.
Yucheng Lu, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Christopher De Sa, and Amir Yazdan-
bakhsh. Step: Learning n: M structured sparsity masks from scratch with precondition. arXiv preprint
arXiv:2302.01172 , 2023.
Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu, Yanzhi Wang, Yen-Kuang Chen, Rong
Jin, and Yuan Xie. Eï¬€ective model sparsiï¬cation by scheduled grow-and-prune methods. arXiv preprint
arXiv:2106.09857 , 2021.
Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, and Xing Sun. Pruning ï¬lter
in ï¬lter. Advances in Neural Information Processing Systems , 33:17629â€“17640, 2020.
Asit Mishra, Jorge Albericio Latorre, Jeï¬€ Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu,
and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378 ,
2021.
Hesham Mostafa and Xin Wang. Parameter eï¬ƒcient training of deep convolutional neural networks by
dynamic sparse reparameterization. In International Conference on Machine Learning , pp. 4646â€“4655.
PMLR, 2019.
16Published in Transactions on Machine Learning Research (10/2023)
Jinhyuk Park and Albert No. Prune your model before distill it. In Computer Visionâ€“ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XI , pp. 120â€“136. Springer,
2022.
Alexandra Peste, Eugenia Ioï¬nova, Adrian Vladu, and Dan Alistarh. Ac/dc: Alternating com-
pressed/decompressed training of deep neural networks. Advances in Neural Information Processing Sys-
tems, 34:8557â€“8570, 2021.
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. C. Chen. Mobilenetv2: Inverted residuals and linear
bottlenecks. IEEE, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Wei Sun, Aojun Zhou, Sander Stuijk, Rob Wijnhoven, Andrew O Nelson, Henk Corporaal, et al. Domi-
nosearch: Find layer-wise ï¬ne-grained n: M sparse schemes from dense neural networks. Advances in
neural information processing systems , 34:20721â€“20732, 2021.
Kai Sheng Tai, Taipeng Tian, and Ser-Nam Lim. Spartan: Diï¬€erentiable sparsity via regularized transporta-
tion.arXiv preprint arXiv:2205.14107 , 2022.
Yijun Tan, Kai Han, Kang Zhao, Xianzhi Yu, Zidong Du, Yunji Chen, Yunhe Wang, and Jun Yao. Acceler-
ating sparse convolution with column vector-wise sparsity. In Advances in Neural Information Processing
Systems, 2022.
Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Scop: Scientiï¬c
controlforreliableneuralnetworkpruning. Advances in Neural Information Processing Systems , 33:10936â€“
10947, 2020.
Qinglin Wang, Dongsheng Li, Xiandong Huang, Siqi Shen, Songzhu Mei, and Jie Liu. Optimizing ï¬€t-
based convolution on armv8 multi-core cpus. In Euro-Par 2020: Parallel Processing: 26th International
Conference on Parallel and Distributed Computing, Warsaw, Poland, August 24â€“28, 2020, Proceedings ,
pp. 248â€“262. Springer, 2020.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep
neural networks. Advances in neural information processing systems , 29, 2016.
Zhuliang Yao, Shijie Cao, Wencong Xiao, Chen Zhang, and Lanshun Nie. Balanced sparsity for eï¬ƒcient
dnn inference on gpu. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence , volume 33, pp.
5676â€“5683, 2019.
Yuxin Zhang, Mingbao Lin, Chia-Wen Lin, Jie Chen, Yongjian Wu, Yonghong Tian, and Rongrong Ji.
Carrying out cnn channel pruning in a white box. IEEE Transactions on Neural Networks and Learning
Systems, 2022.
Yuxin Zhang, Yiting Luo, Mingbao Lin, Yunshan Zhong, Jingjing Xie, Fei Chao, and Rongrong Ji. Bi-
directional masks for eï¬ƒcient n: M sparse training. arXiv preprint arXiv:2302.06058 , 2023.
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hong-
sheng Li. Learning n: M ï¬ne-grained structured sparse neural networks from scratch. arXiv preprint
arXiv:2102.04010 , 2021a.
YangjieZhou,MengtianYang,CongGuo,JingwenLeng,YunLiang,QuanChen,MinyiGuo,andYuhaoZhu.
Characterizing and demystifying the implicit convolution algorithm on commercial matrix-multiplication
accelerators. In 2021 IEEE International Symposium on Workload Characterization (IISWC) ,pp.214â€“225.
IEEE, 2021b.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the eï¬ƒcacy of pruning for model
compression. arXiv preprint arXiv:1710.01878 , 2017.
17