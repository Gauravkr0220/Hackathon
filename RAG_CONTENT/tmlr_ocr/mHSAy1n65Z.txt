Published in Transactions on Machine Learning Research (1/2023)
Optimal Threshold Labeling for Ordinal Regression Methods
Ryoya Yamasaki yamasaki@sys.i.kyoto-u.ac.jp
Department of Systems Science
Graduate School of Informatics, Kyoto University
36-1 Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501 JAPAN
Reviewed on OpenReview: https: // openreview. net/ forum? id= mHSAy1n65Z
Abstract
For an ordinal regression task, a classiï¬cation task for ordinal data, one-dimensional trans-
formation (1DT)-based methods are often employed since they are considered to capture the
ordinal relation of ordinal data well. They learn a 1DT of the observation of the explanatory
variables so that an observation with a larger class label tends to have a larger value of the
1DT, and classify the observation by labeling that learned 1DT. In this paper, we study
the labeling procedure for 1DT-based methods, which have not been suï¬ƒciently discussed
in existing studies. While regression-based methods and classical threshold methods con-
ventionally use threshold labelings, which label a learned 1DT according to the rank of the
interval to which the 1DT belongs among intervals on the real line separated by threshold
parameters, we prove that likelihood-based labeling used in popular statistical 1DT-based
methods is also a threshold labeling in typical usages. Moreover, we show that these thresh-
old labelings can be sub-optimal ones depending on the learning result of the 1DT and the
task under consideration. On the basis of these ï¬ndings, we propose to apply empirical
optimal threshold labeling, which is a threshold labeling that uses threshold parameters
minimizing the empirical task risk for a learned 1DT, to those methods. In experiments
with real-world datasets, changing the labeling procedure of existing 1DT-based methods
to the proposed one improved the classiï¬cation performance in many tried cases.
1 Introduction
Ordinal regression (OR) (or called ordinal classiï¬cation) is the classiï¬cation of ordinal data in which the
underlying target variable is categorical and labeled from a label set (ordinal scale) that is equipped with a
natural ordinal relation for the explanatory variables; see Section 2.1for a detailed formulation. The ordinal
scale is typically formed as a graded (interval) summary of objective indicators like age groups {â€˜0â€“9â€™, â€˜10â€“
19â€™, . . . , â€˜90â€“99â€™, â€˜100â€“â€™} or graded evaluation of subjectivity like human rating {â€˜excellentâ€™, â€˜goodâ€™, â€˜averageâ€™,
â€˜badâ€™, â€˜terribleâ€™}, and ordinal data appear in various practical applications: age estimation ( Niu et al. ,2016;
Cao et al. ,2020), information retrieval ( Liu,2011), movie rating ( Yu et al. ,2006), and questionnaire survey
in social research ( BÃ¼rkner & Vuorre ,2019).
One-dimensional transformation (1DT)-based methods are often applied to the OR problems as
a simple way to capture the ordinal relation of ordinal data: they learn a 1DT of the observation of the
explanatory variables so that an observation with a larger class label tends to have a larger value of the 1DT,
and classify the observation by labeling that learned 1DT, as we will formalize in Section 2.2. For example,
regression-based methods andclassical threshold methods (or called threshold models) conventionally
use a threshold labeling , which labels a learned 1DT according to the rank of the interval to which the 1DT
belongs among intervals on the real line separated by threshold parameters . Regression-based methods
(Kramer et al. ,2001;Agarwal ,2008) learn a 1DT by solving a naÃ¯ve regression task that infers a class label
by the 1DT in a continuous scale, and often apply nearest-neighbor threshold (NNT) labeling that
rounds the learned 1DT to its nearest label (see Section 3.1). Classical threshold methods ( Shashua & Levin ,
2003;Lin & Li ,2006;Chu & Keerthi ,2007;Lin & Li ,2012;Li & Lin ,2007;Pedregosa et al. ,2017) learn
1Published in Transactions on Machine Learning Research (1/2023)
a 1DT using an objective function diï¬€erent from the empirical task risk, and commonly use minimum
threshold (MT) andsummation threshold (ST) labelings that apply parameters obtained incidentally
in that learning procedure as threshold parameters (see Sections 3.2). Also, statistical 1DT-based methods
(statistical methods ) (McCullagh ,1980;Williams ,2006;Cao et al. ,2020;Yamasaki ,2022), in which the
learning procedure of the 1DT can be viewed as statistical modeling of conditional probabilities of the data,
can apply likelihood-based (LB) labeling that is designed to minimize the task risk under the expectation
that the assumed likelihood model is correctly speciï¬ed to the data distribution (see Section 3.3).
We, however, have respective concerns on these existing labeling procedures. First, threshold parameters
of NNT, MT, and ST labelings are generally not designed to minimize the task risk. So their threshold
parameters can be sub-optimal for the minimization of the task risk, as we will demonstrate in Example 1.
On the other hand, the LB labeling is designed to minimize the task risk if the assumed likelihood model is
correctly speciï¬ed to the distribution of the data (see Theorem 2). However, 1DT-based likelihood models
have a strongly restricted representation ability and can be mis-speciï¬ed to the data, and hence the LB
labeling can degrade the classiï¬cation performance depending on the data distribution.
Previous studies have done little theoretical work on the properties of these existing labelings. Therefore,
we ï¬rst study the relationship between these labelings. In particular, we show in Theorem 3that not only
the NNT, MT, and ST labelings but also the LB labeling in typical usages is a threshold labeling. This
ï¬nding motivates us to search for a better labeling function among the class of threshold labelings. Then,
in Section 4, we propose to apply empirical optimal threshold (EOT) labeling , which is a threshold
labeling that uses threshold parameters minimizing the empirical task risk for the learned 1DT, to 1DT-
based methods, under the expectation that the 1DT is learned successfully and the empirical (training) task
risk becomes a good estimate of the (test) task risk. Here, the threshold parameters for the EOT labeling
can be computed with a computational complexity of quasi-linear order OÂ¹ğ‘›logğ‘›Âºregarding the training
sample size ğ‘›using a dynamic programming-based algorithm (Algorithm 1) mentioned in Lin & Li (2006)
(see Section 5for the relation of our proposal to several previous studies including this reference).
In this study, we further performed numerical experiments of the OR task for real-world ordinal data to
conï¬rm the practical eï¬€ectiveness of the EOT labeling (see Section 6and Appendix D). The EOT labeling
gave superior generalization performance (more exactly, smaller test task risk) than the NNT, MT, ST, and
LB labelings, in many tried cases. Also, a modiï¬ed 1DT-based method with the EOT labeling outperformed
an existing 1DT-based method using the ST labeling that has been declared by Cao et al. (2020) to be
state-of-the-art in 2020 for the age estimation from the facial image.
Therefore, in this paper, we propose to change labeling procedures of (existing) 1DT-based OR methods
to the EOT labeling, on the ground of the fact (see Example 1and Theorem 3) that the NNT, MT, and
ST labelings and LB labeling in typical usages are possibly sub-optimal threshold labelings, and empirical
eï¬€ectiveness of the EOT labeling.
2 Preliminaries
2.1 Formulation of OR Problem
OR is the classiï¬cation of ordinal data. The ordinal data have an underlying categorical target variable
ğ‘Œ2Â»ğ¾Â¼â‰”f1,...,ğ¾gthat is equipped with an ordinal relation naturally interpretable in the relationship with
explanatory variables X2Rğ‘‘, whereğ‘‘andğ¾are supposed to be integers larger than or equal to 1and
3, respectively.1We here suppose that the target class labels are encoded to 1,...,ğ¾ in an order-preserving
manner, like from â€˜excellentâ€™ ,..., â€˜terribleâ€™ to 1,..., 5.
The task of the OR ( OR task ) is basically the same as that of the standard (including cost-sensitive)
classiï¬cation, to obtain a good classiï¬er ğ‘“:Rğ‘‘! Â»ğ¾Â¼. For a user-speciï¬ed task loss function â„“:
Â»ğ¾Â¼2! Â» 0,1Âº, it is formulated as minimization of the task risk EÂ»â„“Â¹ğ‘“Â¹XÂº,ğ‘ŒÂºÂ¼, where the expectation
1For better modeling of the ordinal data, it would be important to provide a mathematical characterization and further
discussion of the natural ordinal relation. However, it would be related to learning procedure of the 1DT (deï¬ned in Section 2.2)
more closely, and its necessity is not so great for the analysis and proposal of this study, so we will not mention it in this paper.
Refer to, for example, OR studies ( da Costa et al. ,2008;Yamasaki ,2022) for the discussion on such characterizations.
2Published in Transactions on Machine Learning Research (1/2023)
value EÂ»Â¼is taken for all random variables in its argument (here Xandğ‘Œ). Popular task losses for OR
tasks include not only the zero-one task loss â„“zoÂ¹ğ‘—,ğ‘˜Âºâ‰” /x31Â¹ğ‘—â‰ ğ‘˜Âº, where /x31Â¹ğ‘Âºtakes 1 if a condition ğ‘is
true and 0 otherwise, but also V-shaped losses (for cost-sensitive tasks) reï¬‚ecting oneâ€™s preference of smaller
prediction errors over larger ones such as the absolute deviation task loss â„“adÂ¹ğ‘—,ğ‘˜Âºâ‰”jğ‘— ğ‘˜j,squared
task lossâ„“sqÂ¹ğ‘—,ğ‘˜Âºâ‰”Â¹ğ‘— ğ‘˜Âº2, andâ„“zo,ğ‘Â¹ğ‘—,ğ‘˜Âºâ‰” /x31Â¹jğ‘— ğ‘˜j>ğ‘Âºwithğ‘0.
For the evaluation in the OR task, one may use criteria that cannot decomposed into a sum of losses for
each sample point: for example, quadratic weighted kappa ( Cohen ,1960;1968). Our discussion in this paper
does not cover such criteria.
2.2 Formulation of One-Dimensional Transformation (1DT)-Based Methods and Threshold Methods
In this paper, we discuss only 1DT-based OR methods. We here provide notations and terminologies common
for all the 1DT-based OR methods.
Suppose that one has the sample Dğ‘›â‰”fÂ¹xğ‘–,ğ‘¦ğ‘–Âºgğ‘›
ğ‘–=1, each of which is drawn independently from an identical
distribution ofÂ¹X,ğ‘ŒÂº. First, 1DT-based methods learn a 1DTğ‘:Rğ‘‘!Rof an observation of the
explanatory variables from a user-speciï¬ed class Afğ‘:Rğ‘‘!Rg(1DT class ) (e.g., a neural network
with a ï¬xed network architecture and learnable weight and bias parameters) possibly together with other
objects so that an observation with a larger class label tends to have a larger value of the 1DT; we call
this procedure the learning procedure (of the 1DT). Next, 1DT-based methods build up a classiï¬er as
ğ‘“=â„Â¯ğ‘with a learned 1DT Â¯ğ‘and a labeling function â„:R!Â»ğ¾Â¼; we call this procedure the labeling
procedure . Most existing 1DT-based methods can be seen as adopting one of the NNT, MT, ST, and LB
labelings, depending on the properties of their learning procedure, as we review later in Section 3.
In particular, we call a labeling function â„that can be represented as
â„Â¹ğ‘¢Âº=â„thrÂ¹ğ‘¢;tÂºâ‰”1Â¸ğ¾ 1Ã•
ğ‘˜=1/x31Â¹ğ‘¢ğ‘¡ğ‘˜Âº (1)
with parameters t=Â¹ğ‘¡1,...,ğ‘¡ğ¾ 1Âº2Rğ¾ 1as the threshold labeling . Also, we call the parameters tas the
threshold parameters , and a 1DT-based method using a threshold labeling as the threshold method ,
in this paper. Note that this formulation of the threshold method is a generalization of the one employed
in most previous studies on conventional threshold methods that we will review latter in Section 3.2. The
threshold labeling â„thrÂ¹ğ‘¢;tÂºhas the following properties:
Proposition 1. The threshold labeling â„thrÂ¹ğ‘¢;tÂºis non-decreasing and right-continuous in ğ‘¢2Rand in-
variant regarding the permutation of the threshold parameters ğ‘¡1,...,ğ‘¡ğ¾ 1. Conversely, an arbitrary non-
decreasing right-continuous function â„:R!Â»ğ¾Â¼can be represented by a threshold labeling â„thrÂ¹;tÂºwith
certain threshold parameters t2Rğ¾ 1(i.e., there exist t2Rğ¾ 1such thatâ„Â¹ğ‘¢Âº=â„thrÂ¹ğ‘¢;tÂºfor anyğ‘¢2R)
or their permutation. Also, if ğ‘¡1,...,ğ‘¡ğ¾ 1take onlyğ¿diï¬€erent values, then â„thrÂ¹ğ‘¢;tÂºhasğ¿change points
ğ‘¢=ğ‘¢1,...,ğ‘¢ğ¿such thatâ„thrÂ¹ğ‘¢ğ‘™ ğœ–;tÂºâ‰ â„thrÂ¹ğ‘¢ğ‘™;tÂºwith a suï¬ƒciently small ğœ– >0forğ‘™=1,...,ğ¿ .
The last result implies that the threshold labeling is the simplest as the labeling function in the sense that
the resulting classiï¬er has only Â¹ğ¾ 1Âºdecision boundaries for the learned 1DT at most.
Note that, in the learning procedure, since the empirical task risk1
ğ‘›Ãğ‘›
ğ‘–=1â„“Â¹â„Â¹ğ‘Â¹xğ‘–ÂºÂº,ğ‘¦ğ‘–Âºis discontinuous
with respect to the 1DT ğ‘and hence diï¬ƒcult to optimize numerically, many methods depend on another ob-
jective function. In this paper, we call that objective function the empirical surrogate risk , its population
version the surrogate risk , and its data-dependent minimal component the surrogate loss (function) .
2.3 Formulation of Policy of This Study
The goal of this study is to improve the labeling procedure of 1DT-based methods. Thus, regarding the
learning procedure of the 1DT, this paper adopts those by existing studies, and we do not discuss the
goodness of the learning procedure. Assuming that a learned 1DT Â¯ğ‘and task loss â„“are given, we will
discuss the goodness of the labeling function â„with the task risk EÂ»â„“Â¹â„Â¹Â¯ğ‘Â¹XÂºÂº,ğ‘ŒÂºÂ¼or the empirical task risk
1
ğ‘›Ãğ‘›
ğ‘–=1â„“Â¹â„Â¹Â¯ğ‘Â¹xğ‘–ÂºÂº,ğ‘¦ğ‘–Âºas a criterion, since the aim of the OR task is to minimize the task risk.
3Published in Transactions on Machine Learning Research (1/2023)
3 Review and Analysis of Existing 1DT-Based Methods and Labeling Functions
3.1 Regression-based Method with Nearest-Neighbor Threshold (NNT) Labeling
Regression-based methods ( Kramer et al. ,2001;Agarwal ,2008) learn a 1DT ğ‘by solving a naÃ¯ve regression
task that infers a class label by the 1DT: for a regression loss function ğœ™:RÂ»ğ¾Â¼!Â» 0,1Âº,
min
ğ‘2A1
ğ‘›ğ‘›Ã•
ğ‘–=1ğœ™Â¹ğ‘Â¹xğ‘–Âº,ğ‘¦ğ‘–Âº. (2)
For example, Kramer et al. (2001) used the squared (SQ) loss ğœ™sqÂ¹ğ‘¢,ğ‘¦Âºâ‰”Â¹ğ‘¢ ğ‘¦Âº2, and Agarwal (2008)
used the absolute-deviation (AD) loss ğœ™adÂ¹ğ‘¢,ğ‘¦Âºâ‰”jğ‘¢ ğ‘¦j, as a regression loss function ğœ™.
As a labeling function â„, regression-based methods often use the NNT labeling
â„nntÂ¹ğ‘¢Âºâ‰”â„thrÂ¹ğ‘¢;Â¹1.5,2.5,...,ğ¾ 0.5ÂºÂº (3)
that is a threshold labeling â„thrÂ¹;tÂºwith the threshold parameters ğ‘¡ğ‘˜=ğ‘˜Â¸0.5,ğ‘˜=1,...,ğ¾ 1. The NNT
labeling rounds the learned 1DT to its nearest label.
The regression-based methods using the above two surrogate loss functions and NNT labeling have the
following optimality guarantee for an OR task with a respective speciï¬c task loss:
Theorem 1 (Pedregosa et al. (2017, Theorems 9 and 11)) .LetÂ¹â„“,ğœ™ÂºbeÂ¹â„“ad,ğœ™adÂºorÂ¹â„“sq,ğœ™sqÂº. Then,
for the surrogate risk minimizer Â¯ğ‘2arg minğ‘:Rğ‘‘!REÂ»ğœ™Â¹ğ‘Â¹XÂº,ğ‘ŒÂºÂ¼based on the surrogate loss ğœ™, the
classiï¬erğ‘“nnt=â„nntÂ¯ğ‘minimizes the task risk EÂ»â„“Â¹ğ‘“Â¹XÂº,ğ‘ŒÂºÂ¼for the task loss â„“:EÂ»â„“Â¹ğ‘“nntÂ¹XÂº,ğ‘ŒÂºÂ¼=
minğ‘“:Rğ‘‘!Â»ğ¾Â¼EÂ»â„“Â¹ğ‘“Â¹XÂº,ğ‘ŒÂºÂ¼.
According to this theorem, the regression-based methods ( Kramer et al. ,2001;Agarwal ,2008) would be ex-
pectable to work well if the sample size ğ‘›and 1DT classAare suï¬ƒciently large. On the other hand, if the 1DT
classAis strongly restricted, a classiï¬er based on the NNT labeling may be sub-optimal for the OR task (be-
cause, generally, EÂ»â„“Â¹ğ‘“nntÂ¹XÂº,ğ‘ŒÂºÂ¼â‰ EÂ»â„“Â¹Â¯â„Â¹Â¯ğ‘Â¹XÂºÂº,ğ‘ŒÂºÂ¼forğ‘“nnt=â„nntÂ¯ğ‘with Â¯ğ‘2arg minğ‘2AEÂ»ğœ™Â¹ğ‘Â¹XÂº,ğ‘ŒÂºÂ¼
and Â¯â„2arg minâ„:R!Â»ğ¾Â¼EÂ»â„“Â¹â„Â¹Â¯ğ‘Â¹XÂºÂº,ğ‘ŒÂºÂ¼).
3.2 Classical Threshold Method with Minimum and Summation Threshold (MT and ST) Labelings
Classical threshold methods have been studied actively in the machine learning and statistical litera-
ture ( Shashua & Levin ,2003;Chu & Keerthi ,2007;Li & Lin ,2007;Lin & Li ,2012;Pedregosa et al. ,2017;
Cao et al. ,2020). Many of these methods are formulated with a learning procedure that simultaneously
learnsÂ¹ğ¾ 1Âºparameters in addition to the 1DT ğ‘:
min
ğ‘2A,b2B1
ğ‘›ğ‘›Ã•
ğ‘–=1ğœ™Â¹ğ‘Â¹xğ‘–Âº,b,ğ‘¦ğ‘–Âº, (4)
where we call b=Â¹ğ‘1,...,ğ‘ğ¾ 1Âº2Rğ¾ 1thebias parameters ,BRğ¾ 1is a user-speciï¬ed class for the
bias parameters ( BPs class ), andğœ™:RRğ¾ 1Â»ğ¾Â¼!Â» 0,1Âºis a surrogate loss function. As the labeling
functionâ„, several early works ( Shashua & Levin ,2003;Chu & Keerthi ,2007) use the MT labeling
â„mtÂ¹ğ‘¢;Â¯bÂºâ‰”minfğ‘˜2Â»ğ¾Â¼jğ‘¢< Â¯ğ‘ğ‘˜gwith Â¯ğ‘ğ¾â‰”1 (5)
with learned bias parameters Â¯b=Â¹Â¯ğ‘1,..., Â¯ğ‘ğ¾ 1Âº, and more recent works ( Pedregosa et al. ,2017;Cao et al. ,
2020) use the ST labeling
â„stÂ¹ğ‘¢;Â¯bÂºâ‰”â„thrÂ¹ğ‘¢;Â¯bÂº. (6)
The MT and ST labelings are threshold labelings and have the following relationship:
4Published in Transactions on Machine Learning Research (1/2023)
Proposition 2. Given Â¯b2Rğ¾ 1together with Â¯ğ‘0â‰” 1, letğ‘¡ğ‘˜beÂ¯ğ‘ğ‘–ğ‘˜withğ‘–ğ‘˜â‰”minfğ‘—2f0,...,ğ‘˜gjÂ¯ğ‘ğ‘˜Â¯ğ‘ğ‘—g
for eachğ‘˜=1,...,ğ¾ 1. Then, one has that â„mtÂ¹ğ‘¢;Â¯bÂº=â„thrÂ¹ğ‘¢;tÂºwith t=Â¹ğ‘¡1,...,ğ‘¡ğ¾ 1Âº. Also,â„mtÂ¹ğ‘¢;Â¯bÂº=
â„thrÂ¹ğ‘¢;Â¯bÂºifÂ¯ğ‘1 Â¯ğ‘ğ¾ 1.
We remark that the MT labeling â„mtÂ¹ğ‘¢;Â¯bÂºand ST labeling â„stÂ¹ğ‘¢;Â¯bÂºare diï¬€erent when the learned bias
parameters Â¯bare not ordered.
A surrogate loss function ğœ™Â¹ğ‘¢,b,ğ‘˜Âºfor these threshold methods is often built by replacing step functions
/x31Â¹ğ‘™Âºand /x31Â¹ğ‘™Â¸1Âºin the expression of the task loss â„“Â¹,ğ‘˜Âº=â„“Â¹ğ‘˜,ğ‘˜ÂºÂ¸Ãğ‘˜ 1
ğ‘™=1fâ„“Â¹ğ‘™,ğ‘˜Âº â„“Â¹ğ‘™Â¸1,ğ‘˜Âºg /x31Â¹ğ‘™
ÂºÂ¸Ãğ¾ 1
ğ‘™=ğ‘˜fâ„“Â¹ğ‘™Â¸1,ğ‘˜Âº â„“Â¹ğ‘™,ğ‘˜Âºg /x31Â¹ğ‘™Â¸1Âºby convex surrogates widely-used in binary classiï¬cation ( Lin & Li ,
2012;Pedregosa et al. ,2017) such as the hinge, logistic, and exponential losses, so that ğœ™Â¹,b,ğ‘˜Âºbecomes
a continuous convex upper bound of â„“Â¹â„thrÂ¹;bÂº,ğ‘˜Âº. For example, in the terminology of Pedregosa et al.
(2017), the immediate threshold (IT) loss function
ğœ™Â¹ğ‘Â¹xÂº,b,ğ‘¦Âº=8>>> <
>>>:ğœ‘Â¹ğ‘1 ğ‘Â¹xÂºÂº, ifğ‘¦=1,
ğœ‘Â¹ğ‘Â¹xÂº ğ‘ğ¾ 1Âº, ifğ‘¦=ğ¾,
ğœ‘Â¹ğ‘Â¹xÂº ğ‘ğ‘¦ 1ÂºÂ¸ğœ‘Â¹ğ‘ğ‘¦ ğ‘Â¹xÂºÂº,otherwise(7)
is an upper bound of zero-one task loss â„“zo, and the all threshold (AT) loss function
ğœ™Â¹ğ‘Â¹xÂº,b,ğ‘¦Âº=8>>> <
>>>:Ãğ¾ 1
ğ‘˜=1ğœ‘Â¹ğ‘ğ‘˜ ğ‘Â¹xÂºÂº, ifğ‘¦=1,Ãğ¾ 1
ğ‘˜=1ğœ‘Â¹ğ‘Â¹xÂº ğ‘ğ‘˜Âº, ifğ‘¦=ğ¾,Ãğ‘¦ 1
ğ‘˜=1ğœ‘Â¹ğ‘Â¹xÂº ğ‘ğ‘˜ÂºÂ¸Ãğ¾ 1
ğ‘˜=ğ‘¦ğœ‘Â¹ğ‘ğ‘˜ ğ‘Â¹xÂºÂº,otherwise(8)
is an upper bound of absolute deviation task loss â„“ad. For instance, SVOR-IMC ( Chu & Keerthi ,2007)
uses the IT loss with ğœ‘Â¹ğ‘¢Âº=minf0,1 ğ‘¢g, ï¬xed margin strategy ( Shashua & Levin ,2003) and SVOR-EXC
(Chu & Keerthi ,2007) use the AT loss with ğœ‘Â¹ğ‘¢Âº=minf0,1 ğ‘¢g, ORBoost-LR ( Lin & Li ,2006) uses the
IT loss with ğœ‘Â¹ğ‘¢Âº=ğ‘’ ğ‘¢, ORBoost-ALL ( Lin & Li ,2006) uses the AT loss with ğœ‘Â¹ğ‘¢Âº=ğ‘’ ğ‘¢, and CORAL
(Cao et al. ,2020) uses the AT loss with ğœ‘Â¹ğ‘¢Âº=logÂ¹1Â¸ğ‘’ ğ‘¢Âº.
As the BPs class B, the non-restricted (non-ordered) class Rğ¾ 1and ordered class fb2Rğ¾ 1jğ‘1
ğ‘ğ¾ 1gare often applied. As a simple implementation of the ordered class, Franses & Paap (2001) mentioned
to parametrize the bias parameters bas
ğ‘1=ğ‘0
1,andğ‘ğ‘˜=ğ‘ğ‘˜ 1Â¸ğ‘0
ğ‘˜2forğ‘˜=2,...,ğ¾ 1, (9)
with other parameters ğ‘0
1,...,ğ‘0
ğ¾ 12R. When the ordered BPs class is applied, the MT and ST labelings
bring the same classiï¬cation results. Also, for many AT loss functions, bias parameters Â¯bof the surrogate
risk minimizerÂ¹Â¯ğ‘,Â¯bÂº2arg minğ‘2A,b2Rğ¾ 1EÂ»ğœ™Â¹ğ‘Â¹XÂº,b,ğ‘ŒÂºÂ¼are ensured to be ordered Â¹Â¯ğ‘1 Â¯ğ‘ğ¾ 1Âº(see
Chu & Keerthi (2005, Lemma 1) and Li & Lin (2007, Theorem 2)), and hence the use of the ordered BPs
class will be justiï¬ed.
The use of the surrogate loss ğœ‘and the MT or ST labeling â„, which make ğœ™Â¹,Â¯b,ğ‘¦Âº(with ordered Â¯b) to be an
upper bound of the task loss â„“Â¹â„Â¹Âº,ğ‘¦Âº, is almost like a convention and may facilitate generalization analysis
(Li & Lin ,2007;Lin & Li ,2012), but the goodness of selecting the MT or ST labeling is not supported
by theoretical discussion. We demonstrate in the following example that the MT or ST labeling may be a
negative factor that degrades the classiï¬cation performance of threshold methods:
Example 1. We here consider the IT loss (7)withğœ‘Â¹ğ‘¢Âº=minf0,1 ğ‘¢g, which we denote ğœ™hinge -itÂ¹ğ‘¢,b,ğ‘˜Âº
and call Hinge-IT loss. The Hinge-IT loss is a continuous convex upper bound of the zero-one task loss with
the MT labeling â„“zoÂ¹â„mtÂ¹;bÂº,ğ‘˜Âº(and ST labeling â„“zoÂ¹â„stÂ¹;bÂº,ğ‘˜Âº) when bis ordered (i.e., ğ‘1ğ‘ğ¾ 1):
ğœ™hinge -itÂ¹,b,ğ‘˜Âºis a convex function and ğœ™hinge -itÂ¹,b,ğ‘˜Âºâ„“zoÂ¹â„mtÂ¹;bÂº,ğ‘˜Âº. So one may think that the Hinge-
IT loss and the task with the zero-one task loss ( Task-Z ) have friendly compatibility, from an analogy of
a well-known result, classiï¬cation calibration ( Bartlett et al. ,2006), in binary classiï¬cation. However, the
following demonstration shows that the MT labeling may be sub-optimal in minimizing the task risk as a
labeling function for the combination of the Hinge-IT loss and Task-Z.
5Published in Transactions on Machine Learning Research (1/2023)
We consider a 4-class OR problem (let ğ¾=4), and suppose that the data appear only on 4 diï¬€erent points
xÂ»1Â¼,..., xÂ»4Â¼inRğ‘‘and follow the probability distribution, PrÂ¹xÂ»ğ‘–Â¼Âº=0.25andÂ¹PrÂ¹1jxÂ»ğ‘–Â¼Âº,..., PrÂ¹ğ¾jxÂ»ğ‘–Â¼ÂºÂº=
Â¹0.5,0.4,0,0.1Âº,Â¹0.3,0.5,0,0.2Âº,Â¹0.2,0,0.5,0.3Âº,Â¹0.1,0,0.4,0.5Âºforğ‘–=1,..., 4.2
It can be shown that the surrogate risk minimizer Â¹Â¯ğ‘,Â¯bÂº2arg minğ‘:Rğ‘‘!R,b2Rğ¾ 1EÂ»ğœ™hinge -itÂ¹ğ‘Â¹XÂº,b,ğ‘ŒÂºÂ¼satis-
ï¬esÂ¯ğ‘1=Â¯ğ‘2=Â¯ğ‘3=0,Â¯ğ‘Â¹xÂ»1Â¼Âº=Â¯ğ‘Â¹xÂ»2Â¼Âº= 1, and Â¯ğ‘Â¹xÂ»3Â¼Âº=Â¯ğ‘Â¹xÂ»4Â¼Âº=1(ignore the translation invariance) by
several simple calculations. The MT labeling predicts a label of the data on xÂ»ğ‘–Â¼asâ„mtÂ¹Â¯ğ‘Â¹xÂ»ğ‘–Â¼Âº;Â¯bÂº=1,1,4,4
forğ‘–=1,..., 4(EÂ»â„“zoÂ¹â„mtÂ¹Â¯ğ‘Â¹XÂº;Â¯bÂº,ğ‘ŒÂºÂ¼=0.6), despite that using diï¬€erent threshold parameters (say
t=Â¹ 2,0,2Âº) one can predict it as â„mtÂ¹Â¯ğ‘Â¹xÂ»ğ‘–Â¼Âº;tÂº=2,2,3,3forğ‘–=1,..., 4and yield a smaller value
of the task risk ( EÂ»â„“zoÂ¹â„mtÂ¹Â¯ğ‘Â¹XÂº;tÂº,ğ‘ŒÂºÂ¼=0.55). â–¡
3.3 Statistical Methods with Likelihood-Based (LB) Labeling
In the OR research in statistics, several methods have been developed according to the statistical mod-
eling of the conditional probabilities of the data through a 1DT ( McCullagh ,1980;Williams ,2006;
Chu & Ghahramani ,2005;Yamasaki ,2022). For example, the cumulative link model ( McCullagh ,1980),
which is popular in the OR research, models the conditional probabilities PrÂ¹ğ‘¦jxÂº,Â¹x,ğ‘¦Âº2Rğ‘‘Â»ğ¾Â¼by
ğ‘ƒÂ¹ğ‘¦;ğœ,Ëœğ‘Â¹xÂº,ËœbÂºâ‰”8>>> <
>>>:ğœÂ¹Ëœğ‘ğ‘¦ Ëœğ‘Â¹xÂºÂº, ifğ‘¦=1,
1 ğœÂ¹Ëœğ‘ğ‘¦ 1 Ëœğ‘Â¹xÂºÂº, ifğ‘¦=ğ¾,
ğœÂ¹Ëœğ‘ğ‘¦ Ëœğ‘Â¹xÂºÂº ğœÂ¹Ëœğ‘ğ‘¦ 1 Ëœğ‘Â¹xÂºÂº,otherwise,(10)
whereğœ:R!Â»0,1Â¼is a link function that is non-decreasing and satisï¬es ğœÂ¹ 1Âº=0andğœÂ¹Â¸1Âº=1(i.e.,ğœ
is a cumulative distribution (CPD) function), Ëœğ‘:Rğ‘‘!Ris an assumed 1DT, and Ëœb=Â¹Ëœğ‘1,..., Ëœğ‘ğ¾ 1Âº2Rğ¾ 1
are assumed bias parameters that satisfy Ëœğ‘1 Ëœğ‘ğ¾ 1. As a link function ğœ, ordinal logistic regression
(OLR) ( McCullagh ,1980) applies the CPD function of the logistic distribution (a.k.a. the sigmoid function)
ğœlogisticÂ¹ğ‘¢Âºâ‰”1ÂÂ¹1Â¸ğ‘’ ğ‘¢Âº, and cumulative probit model ( Agresti ,2010, Section 5.2) and Gaussian process OR
(GPOR) proposed by Chu & Ghahramani (2005) use the CPD function of the standard Gaussian distribution
(a.k.a. the inverse function of the probit function) ğœgaussÂ¹ğ‘¢Âºâ‰”Â¯ğ‘¢
 1Â¹2ğœ‹Âº 1Â2ğ‘’ ğ‘£2Â2ğ‘‘ğ‘£.
In the learning procedure of the 1DT, statistical methods apply surrogate loss functions associated with their
statistical modeling. For the cumulative link model, a surrogate loss function ğœ™and the BPs class Bfor the
learning procedure same as ( 4) should satisfy
Â¹Ëœğ‘¢,ËœbÂº=arg min
ğ‘¢2R,b2Bğ¾Ã•
ğ‘¦=1ğ‘ƒÂ¹ğ‘¦;ğœ,Ëœğ‘¢,ËœbÂºğœ™Â¹ğ‘¢,b,ğ‘¦Âº. (11)
A popular instance of the surrogate loss function is the negative log likelihood (NLL) loss function
ğœ™nllÂ¹ğ‘¢,b,ğ‘¦;ğœÂºâ‰” logfğ‘ƒÂ¹ğ‘¦;ğœ,ğ‘¢,bÂºg, for which the learning procedure amounts to the maximum likelihood
estimation for the model ( 10). Also, Cao et al. (2020) used the AT loss ( 8) withğœ‘Â¹ğ‘¢Âº= logfğœÂ¹ğ‘¢Âºg, which we
call the all negative log cumulative likelihoods (ANLCL) loss function and denote ğœ™anlclÂ¹ğ‘¢,b,ğ‘¦;ğœÂº,
forğœ=ğœlogistic . The learning procedure for this loss function can be characterized as the minimization
of the sum of the NLLs of the models of cumulative conditional probabilities PrÂ¹ğ‘Œğ‘˜jX=xÂºfor binary
classiï¬cation problems, â€˜ ğ‘˜or lessâ€™ vs. â€˜more than ğ‘˜â€™,ğ‘˜=1,...,ğ¾ 1.
The above interpretation on using the surrogate losses ğœ™nllandğœ™anlcl under the statistical model ( 10) can
be mathematically understood as follows:
Theorem 2. Assume that the random variable Â¹X,ğ‘ŒÂºunderlying the data has conditional probabilities that
can be represented as (10):PrÂ¹ğ‘¦jxÂº=ğ‘ƒÂ¹ğ‘¦;ğœ,Ëœğ‘Â¹xÂº,ËœbÂºfor everyğ‘¦2Â»ğ¾Â¼and any x2Rğ‘‘in the support of the
distribution of Xwithğœthat is non-decreasing and satisï¬es ğœÂ¹ 1Âº=0andğœÂ¹Â¸1Âº=1(andğœÂ¹ Âº=1 ğœÂ¹Âº
forğœ™=ğœ™anlcl) such asğœlogistic andğœgauss,Ëœğ‘:Rğ‘‘!R, and Ëœb2Rğ¾ 1satisfying Ëœğ‘1 Ëœğ‘ğ¾ 1. LetA
befğ‘”:Rğ‘‘!Rg, andÂ¹ğœ™,BÂºbeÂ¹ğœ™nll,fb2Rğ¾ 1jğ‘1ğ‘ğ¾ 1gÂº,Â¹ğœ™anlcl,Rğ¾ 1Âº, orÂ¹ğœ™anlcl,fb2Rğ¾ 1j
ğ‘1ğ‘ğ¾ 1gÂº. Then, any surrogate risk minimizer Â¹Â¯ğ‘,Â¯bÂº2arg minğ‘2A,b2BEÂ»ğœ™Â¹ğ‘Â¹XÂº,b,ğ‘ŒÂºÂ¼satisï¬es
ğ‘ƒÂ¹ğ‘¦;ğœ,Â¯ğ‘Â¹xÂº,Â¯bÂº=PrÂ¹ğ‘¦jxÂºfor any x2Rğ‘‘in the support of the distribution of X.
2We abbreviate the marginal probability PrÂ¹X=xÂºtoPrÂ¹xÂºand the conditional probability PrÂ¹ğ‘Œ=ğ‘¦jX=xÂºtoPrÂ¹ğ‘¦jxÂº
(this abbreviation applies to an estimate Ë†Pras well).
6Published in Transactions on Machine Learning Research (1/2023)
For such methods, not only the threshold labelings but also the LB labeling that grounds on the assumed
statistical model is a widely-used option for the labeling function. Considering Theorem 2and the equality
EÂ»â„“Â¹ğ‘“Â¹xÂº,ğ‘ŒÂºÂ¼=Ãğ¾
ğ‘¦=1PrÂ¹ğ‘¦jxÂºâ„“Â¹ğ‘“Â¹xÂº,ğ‘¦Âº, and aiming to minimize the task risk, minğ‘“:Rğ‘‘!Â»ğ¾Â¼EÂ»â„“Â¹ğ‘“Â¹ğ‘‹Âº,ğ‘ŒÂºÂ¼,
these methods can predict a label with the LB labeling
â„lbÂ¹ğ‘¢;ğœ,Â¯b,â„“Âºâ‰”min
arg min
ğ‘˜2Â»ğ¾Â¼ğ¾Ã•
ğ‘¦=1ğ‘ƒÂ¹ğ‘¦;ğœ,ğ‘¢, Â¯bÂºâ„“Â¹ğ‘˜,ğ‘¦Âº
(12)
with learned bias parameters Â¯b, under the expectation that the assumed statistical model ( 10) correctly
represents the actual statistical behavior of the data and it is learned successfully. Note that there can
be a tie situation where objective functions with diï¬€erent ğ‘˜of (12) take the same value, and arg minğ‘˜2Â»ğ¾Â¼
operation outputs multiple labels. The overlaid minoperation in ( 12) avoids such a tie situation.
These methods tend to perform better when their assumed statistical model represents the actual statistical
behavior of the data well. One can, however, ï¬nd that the condition in Theorem 2is very restrictive.
Therefore, in many practical situations, their statistical model would deviate from the actual statistical
behavior of the data, and then their 1DT model may not be learned appropriately, and the LB labeling
â„lbÂ¹;ğœ,Â¯b,â„“Âºmay be sub-optimal for the learned 1DT model Â¯ğ‘.
One may still consider that the LB labeling is more ï¬‚exible, in that it is generally not restricted within
the class of non-decreasing threshold labelings, and superior to threshold labelings. However, we found
that the LB labeling takes the form of the threshold labeling, for typical statistical models such as ones
in OLR and GPOR (i.e., the link function ğœsuch asğœlogistic,ğœgauss) and for typical task losses such as
â„“=â„“zo,â„“zo,ğ‘,â„“ad,â„“sq.
Theorem 3. Suppose that ğœis non-decreasing and satisï¬es ğœÂ¹ 1Âº=0andğœÂ¹Â¸1Âº=1and that Â¯ğ‘1
Â¯ğ‘ğ¾ 1. Then, the LB labeling â„lbÂ¹ğ‘¢;ğœ,Â¯b,â„“Âºis
(i)a certain threshold labeling â„thrÂ¹ğ‘¢;tÂºfor some t2Rğ¾ 1, ifâ„“Â¹ğ‘˜,ğ‘™Âºat each ï¬xed ğ‘˜2 Â»ğ¾Â¼is non-
increasing in ğ‘™forğ‘™ğ‘˜and non-decreasing in ğ‘™forğ‘™ğ‘˜, andâ„“ğ‘˜,ğ‘™Â¹ğ‘—Âºâ‰”â„“Â¹ğ‘˜,ğ‘—Âº â„“Â¹ğ‘˜,ğ‘—Â¸1Âº â„“Â¹ğ‘™,ğ‘—ÂºÂ¸
â„“Â¹ğ‘™,ğ‘—Â¸1Âºat each ï¬xed diï¬€erent ğ‘˜,ğ‘™2Â»ğ¾Â¼is non-positive (resp. non-negative) for all ğ‘—2Â»ğ¾ 1Â¼
respectively when ğ‘˜ <ğ‘™ (resp.ğ‘˜ >ğ‘™ ), for example, â„“=â„“ad,â„“sq,
(ii)a certain threshold labeling â„thrÂ¹ğ‘¢;tÂºfor some t2Rğ¾ 1, ifâ„“=â„“zo,â„“zo,ğ‘withğ‘2Â»0,bğ¾Â2cÂº,ğœis
diï¬€erentiable, ğœ0Â¹ğ‘£Âºâ‰”ğ‘‘
ğ‘‘ğ‘£ğœÂ¹ğ‘£Âºis even and non-increasing in ğ‘£ifğ‘£ > 0, andğœ0Â¹ğ‘£1Âº ğœ0Â¹ğ‘£2Âº
ğœÂ¹ğ‘£1Âº ğœÂ¹ğ‘£2Âºis non-
increasing in ğ‘£1with ï¬xedğ‘£2(and inğ‘£2with ï¬xedğ‘£1) ifğ‘£1<ğ‘£2, for example, ğœ=ğœlogistic,ğœgauss,
wherebğ‘£cis the greatest integer less than or equal to ğ‘£,
(iii) the threshold labeling â„thrÂ¹ğ‘¢;Â¯bÂºthat is same as the MT labeling â„mtÂ¹ğ‘¢;Â¯bÂºand ST labeling â„stÂ¹ğ‘¢;Â¯bÂº,
ifâ„“=â„“adandğœÂ¹0Âº=0.5.
Here, Theorem 3(i) assumes that the task loss â„“is V-shaped, and the condition on â„“ğ‘˜,ğ‘™in Theorem 3(i)
holds under the convexity of â„“deï¬ned below:
Theorem 4. â„“ğ‘˜,ğ‘™Â¹ğ‘—Âºat each ï¬xed diï¬€erent ğ‘˜,ğ‘™2Â»ğ¾Â¼is non-positive (resp. non-negative) for all ğ‘—2Â»ğ¾ 1Â¼
respectively when ğ‘˜ <ğ‘™ (resp.ğ‘˜ >ğ‘™ ), if the task risk â„“is convex in the diï¬€erence of the two arguments:
â„“Â¹ğ‘—3,ğ‘˜3ÂºÂ¹ğ‘—3 ğ‘˜3Âº Â¹ğ‘—1 ğ‘˜1Âº
Â¹ğ‘—2 ğ‘˜2Âº Â¹ğ‘—1 ğ‘˜1Âºâ„“Â¹ğ‘—1,ğ‘˜1ÂºÂ¸Â¹ğ‘—2 ğ‘˜2Âº Â¹ğ‘—3 ğ‘˜3Âº
Â¹ğ‘—2 ğ‘˜2Âº Â¹ğ‘—1 ğ‘˜1Âºâ„“Â¹ğ‘—2,ğ‘˜2Âº (13)
for allğ‘—1,...,ğ‘˜ 32Â»ğ¾Â¼such thatğ‘—1 ğ‘˜1â‰ ğ‘—2 ğ‘˜2andğ‘—1 ğ‘˜1ğ‘—3 ğ‘˜3ğ‘—2 ğ‘˜2.
The condition on ğœin Theorem 3(ii) comes from the consideration for non-convex task losses.
A result similar to Theorem 3also holds for 1DT-based likelihood models other than the CL model ( 10); refer
to Theorem 5in Section B. For 1DT-based statistical methods, it may be common that their LB labeling is
a threshold labeling.
7Published in Transactions on Machine Learning Research (1/2023)
4 Proposal of 1DT-based Methods with Empirical Optimal Threshold (EOT) Labeling
In typical usages, not only the NNT, MT, and ST labelings, but also the LB labeling is a threshold labeling,
as we conï¬rmed in Theorem 3. Thus, we consider that it would be meaningful to aim for a better threshold
labeling for improving the classiï¬cation performance of existing 1DT-based methods. Recalling that the ï¬nal
goal is to make the task risk EÂ»â„“Â¹ğ‘“Â¹XÂº,ğ‘ŒÂºÂ¼small, and expecting that the 1DT was learned successfully and
the empirical (training) task risk becomes a good estimate of the (test) task risk, we propose to apply the
EOT labeling
â„Â¹ğ‘¢;teotÂºwith teot2arg min
t2Rğ¾ 11
ğ‘›ğ‘›Ã•
ğ‘–=1â„“Â¹â„thrÂ¹Â¯ğ‘Â¹xğ‘–Âº;tÂº,ğ‘¦ğ‘–Âº (14)
that uses threshold parameters minimizing the empirical task risk for a given learned 1DT model Â¯ğ‘.
The threshold parameters for the EOT labeling can be computed with a dynamic programming-based al-
gorithm (Algorithm 1) mentioned in Lin & Li (2006); see also a researchersâ€™ site ( https:// home. work.
caltech. edu/ ~htlin/ program/ orensemble/ ) ofLin & Li (2006), and Section Cof our paper for its opti-
mality guarantee. This algorithm ï¬rst sorts unique elements of fÂ¯ğ‘Â¹xğ‘–Âºgğ‘›
ğ‘–=1tofÂ¯ğ‘ğ‘—g, and takes advantage of
the recurrence relation ( 46) of the minimizer of the empirical task risk for sample points ğ‘–â€™s s.t. Â¯ğ‘Â¹xğ‘–Âº Â¯ğ‘ğ‘—
along the ascending order of fÂ¯ğ‘ğ‘—gto calculate threshold parameters minimizing the empirical task risk. It
costs a computational complexity of quasi-linear order OÂ¹ğ‘›logğ‘›Âºregarding the training sample size ğ‘›, which
stems from the sorting operation in Line 2while the rest operation in Lines 3â€“15costs a computational
complexity ofOÂ¹ğ‘›ğ¾Âº.
The NNT labeling for regression-based methods (reviewed in Section 3.1) and LB labeling for statistical
methods (reviewed in Section 3.3) have optimality guarantees for the task risk minimization under ideal
situations; refer to Theorems 1and2. Also, many threshold methods employ Â¹ğ¾ 1Âºbias parameters, and
those bias parameters can be directly used in their labeling procedure like MT and ST labelings, as reviewed
in Section 3.2. Presumably, for these reasons, a formulation that allows a threshold labeling with variable
threshold parameters has not been considered for these methods. Our formulation in Section 2.2introduces
the threshold labeling with variable threshold parameters and clearly distinguishes the bias and threshold
parameters. This is also an important contribution of this paper: due to this formulation, it becomes natural
to consider the application of the threshold labeling to 1DT-based methods with a diï¬€erent number of bias
parameters thanÂ¹ğ¾ 1Âºsuch as PO-VS-SL ( Yamasaki ,2022). Furthermore, this led to the EOT labeling
that has a potential to improve the classiï¬cation performance.
We, however, have to provide a remark on the additional learning of the decision boundaries (here threshold
parameters) after the learning of the learner model (here a 1DT): the additional learning generally has a
risk of enlarging the generalization gap. One can adjust the labeling function â„so thatâ„Â¹Â¯ğ‘”Â¹xğ‘–ÂºÂº=ğ‘¦ğ‘–for
every training example ğ‘–=1,...,ğ‘› if allowing arbitrary formats and ğ‘¦ğ‘–1=ğ‘¦ğ‘–2for anyğ‘–1,ğ‘–2s.t.xğ‘–1=xğ‘–2, but
the resulting classiï¬er â„Â¯ğ‘”would have quite low generalization performance. On the other hand, we here
consider the additional learning of the labeling function among the class of threshold labelings. A threshold
labeling has up to Â¹ğ¾ 1Âºdecision boundaries, that is, it is strictly restricted, and we expect that the degree
of the generalization gap will not diï¬€er much with any threshold labelings.
5 Relationship to Other Previous Studies
Most existing studies on 1DT-based methods discuss the learning procedure. In Section 3, we reviewed point-
wise 1DT-based methods in which the learning procedure is formulated with a surrogate loss function
deï¬ned for each point Â¹ğ‘Â¹xğ‘–Âº,ğ‘¦ğ‘–Âº, but some 1DT-based methods employ diï¬€erent formulations. For example,
pair-wise 1DT-based methods including RankSVM ( Herbrich et al. ,1999) and RankBoost ( Lin & Li ,
2006) are formulated with a surrogate loss function that is deï¬ned for each pair of two points Â¹ğ‘Â¹xğ‘–Âº,ğ‘¦ğ‘–Âºand
Â¹ğ‘Â¹xğ‘—Âº,ğ‘¦ğ‘—Âº; see also Lin & Li (2012);Gutierrez et al. (2015) for survey of such methods. The EOT labeling
can be applied to those 1DT-based methods as well.
8Published in Transactions on Machine Learning Research (1/2023)
Algorithm 1: Calculation of the threshold parameters for the EOT labeling
Input: Task lossâ„“, learned 1DT Â¯ğ‘, and training data Dğ‘›=fÂ¹xğ‘–,ğ‘¦ğ‘–Âºgğ‘›
ğ‘–=1.
/*Preparation of fÂ¯ğ‘ğ‘—gğ‘
ğ‘—=1andYğ‘—forğ‘—=1,...,ğ‘ . */
1LetfÂ¯ğ‘0
ğ‘—gğ‘
ğ‘—=1be unique elements of fÂ¯ğ‘Â¹xğ‘–Âºgğ‘›
ğ‘–=1:Â¯ğ‘0
ğ‘—1â‰ Â¯ğ‘0
ğ‘—2for allğ‘—1,ğ‘—22Â»ğ‘Â¼s.t.ğ‘—1â‰ ğ‘—2, and
Â¯ğ‘Â¹xğ‘–Âº2f Â¯ğ‘0
ğ‘—gğ‘
ğ‘—=1for allğ‘–2Â»ğ‘›Â¼.
2SortfÂ¯ğ‘0
ğ‘—gğ‘
ğ‘—=1in the ascending order, and represent the result as fÂ¯ğ‘ğ‘—gğ‘
ğ‘—=1:Â¯ğ‘1 Â¯ğ‘ğ‘.
3Create setsYğ‘—=fğ‘¦ğ‘–jÂ¯ğ‘Â¹xğ‘–Âº=Â¯ğ‘ğ‘—gğ‘›
ğ‘–=1,ğ‘—=1,...,ğ‘ .
/*Calculate matrix ğ¿2Rğ‘ğ¾. */
4 forğ‘˜=1,...,ğ¾ do
5ğ¿1,ğ‘˜=Ã
ğ‘¦ğ‘–2Y1â„“Â¹ğ‘˜,ğ‘¦ğ‘–Âº.
6 forğ‘—=2,...,ğ‘ do
7 forğ‘˜=1,...,ğ¾ do
/*An efficient implementation of ğ¿ğ‘—,ğ‘˜=minğ‘™2Â»ğ‘˜Â¼ğ¿ğ‘— 1,ğ‘™Â¸Ã
ğ‘¦ğ‘–2Yğ‘—â„“Â¹ğ‘˜,ğ‘¦ğ‘–Âº. */
8ğ¿tmp ğ¿ğ‘— 1,1(ifğ‘˜=1),minfğ¿tmp,ğ¿ğ‘— 1,ğ‘˜g(otherwise).
9ğ¿ğ‘—,ğ‘˜=ğ¿tmpÂ¸Ã
ğ‘¦ğ‘–2Yğ‘—â„“Â¹ğ‘˜,ğ‘¦ğ‘–Âº.
/*Calculate threshold parameters Â¯t. */
10ğ¼ minÂ¹arg minğ‘™2Â»ğ¾Â¼ğ¿ğ‘,ğ‘™Âº.
11 ifğ¼â‰ ğ¾then
LetÂ¯ğ‘¡ğ‘˜be a value larger than Â¯ğ‘ğ‘(e.g.,Â¸1) forğ‘˜=ğ¼,...,ğ¾ 1.
12 forğ‘—=ğ‘ 1,..., 1do
13ğ½ minÂ¹arg minğ‘™2Â»ğ¼Â¼ğ¿ğ‘—,ğ‘™Âº.
14 ifğ¼â‰ ğ½then
Â¯ğ‘¡ğ‘˜=Â¹Â¯ğ‘ğ‘—Â¸Â¯ğ‘ğ‘—Â¸1ÂºÂ2forğ‘˜=ğ½,...,ğ¼ 1, andğ¼ ğ½.
15 ifğ¼â‰ 1then
LetÂ¯ğ‘¡ğ‘˜be a value smaller than Â¯ğ‘1(e.g., 1) forğ‘˜=1,...,ğ¼ 1.
Output: Threshold parameters Â¯t=Â¹Â¯ğ‘¡1,..., Â¯ğ‘¡ğ¾ 1Âº.
On the other hand, there are several previous works that have studied the labeling procedures diï¬€erent from
NNT, MT, ST, LB, and EOT labelings. We review the discussion of those previous works, and describe the
relationship between their methods and the EOT labeling, in the following.
Herbrich et al. (1999) considered a pairwise 1DT-based method based on a hinge-type surrogate loss, for the
task with the zero-one task loss. Their method ( Herbrich et al. ,1999, (12)) adopts a threshold labeling with
threshold parameters determined by minimizing a unique criterion that emphasizes the shape of the used
hinge-type loss and is diï¬€erent from the (empirical) surrogate risk, and has no optimality guarantee in the
task risk minimization.
Lin & Li (2006) considered three methods; see RankBoost (4), ORBoost-LR (7), and ORBoost-ALL (8) of
this reference. The third method applies the AT loss with ğœ‘Â¹ğ‘¢Âº=ğ‘’ ğ‘¢(ORBoost-ALL), and its labeling
procedure is the same as the MT and ST labelings in this paper. Also, one can understand that the second
method, ORBoost-LR (7), tries to minimize (an upper bound of) the empirical surrogate risk based on the IT
loss function with ğœ‘Â¹ğ‘¢Âº=ğ‘’ ğ‘¢and ordered BPs class in its learning procedure, and its labeling procedure also
can be seen to follow the idea of the MT and ST labelings. Finally, the ï¬rst method, RankBoost, is a pair-
wise 1DT-based method, and its objective function (4) for the labeling procedure is deï¬ned as the empirical
task risk with the absolute deviation task loss. This labeling procedure is similar to the EOT labeling,
butLin & Li (2006) did not mention the relevance between that objective function and the task under
consideration. Although Lin & Li (2006) presented a description on the threshold parameters determination
based on the zero-one task loss at the part following (4), they tried only the threshold parameters determined
by minimizing the empirical task risk with the absolute deviation task loss even when they considered the
task with the zero-one task loss in their experiments. The main claim of our consideration in Section 4
is that the threshold parameters should be determined via minimizing the (empirical) task risk for the
task under consideration. Therefore, the EOT labeling in Section 4can be interpreted as a variant of
9Published in Transactions on Machine Learning Research (1/2023)
Table 1: Dataset properties, the total sample size ğ‘›tot, the dimension ğ‘‘of the explanatory variables, and
the number ğ¾of classes of the target variable, of the RW datasets used for the experiments in Section 6.
COL PAS SQ1 SQ2 BON TAE AUT NEW TOY ESL
ğ‘›tot 24 36 52 52 57 151 205 215 300 488
ğ‘‘ 6 25 51 52 37 54 71 5 2 4
ğ¾ 3 3 3 3 5 3 6 3 5 9
BAS EUQ LEV ERA SWD WQR CAR MORPH CACD AFAD
ğ‘›tot 625 736 1000 1000 1000 1599 1728 55013 159402 164418
ğ‘‘ 4 91 4 4 10 11 21 12823 12823 12823
ğ¾ 3 5 5 9 4 6 4 55 49 26
those for Lin & Li (2006, (4)) with respect to the relevance between the objective function to determine
the threshold parameters and the task under consideration. Our contributions, namely, are to expand the
scope of applicability of the EOT labeling to regression-based, threshold, and statistical methods reviewed
in Section 3, and to modify the EOT labeling to be performed based on the task loss function corresponding
to the task under consideration, not the development of Algorithm 1as a method for the minimization of
the empirical task risk with a given task loss function.
6 Numerical Experiments with Real-World Datasets
Purpose We performed numerical experiments using real-world (RW) datasets to answer the question,
whether a modiï¬ed 1DT-based method with the EOT labeling can yield better classiï¬cation performance
(i.e., smaller test task risk) for the OR task than existing 1DT-based methods using other labeling functions.
Datasets and Preprocessing In the experiments, we used the 17 various-domain datasets , COL
(contact-lenses), PAS (pasture), SQ1 (squash-stored), SQ2 (squash-unstored), BON (bondrate), TAE (tae),
AUT (automobile), NEW (newthyroid), TOY ( da Costa et al. ,2008), ESL (employee selection), BAS
(balance-scale), EUQ (eucalyptus), LEV (lectures evaluation), ERA (employee rejection/acceptance), SWD
(social workers decision), WQR (winequality-red), CAR (car evaluation) datasets, and the 3 face-age
datasets , MORPH (MORPH Album2), CACD, and AFAD datasets ( Ricanek & Tesafaye ,2006;Chen et al. ,
2014;Niu et al. ,2016). The main reason why we used the various-domain and face-age datasets is respec-
tively to experiment with many real-world datasets in various domains and to conï¬rm whether the proposed
method achieves the classiï¬cation performance competitive to the state-of-the-art method in a modern ap-
plication. For most of the experimental settings, we referred to those of the previous studies ( Cao et al. ,
2020;Gutierrez et al. ,2015) with few modiï¬cations.3
For the various-domain datasets, we used datasets, which Gutierrez et al. (2015) used as OR datasets, follow-
ing the setting of the explanatory and target variables in Gutierrez et al. (2015). One can obtain the various-
domain datasets from a researchersâ€™ site ( http:// www. uco. es/grupos/ ayrna/ orreview ) ofGutierrez et al.
(2015) or our GitHub repository ( https:// github. com/ yamasakiryoya/ OTL). We purchased the MORPH
dataset at https:// ebill. uncw. edu/ C20231_ ustores/ web/ and preprocessed it so that the face spanned
the whole image with the nose tip, which was located by facial landmark detection ( Sagonas et al. ,2016),
at the center by using EyepadAlign function by Raschka (2018). While this dataset contains 55,134 facial
images with ages from 16 to 77, we used 55,013 images with ages from 16 to 70. The CACD dataset can
be downloaded from https:// bcsiriuschen. github. io/CARC/ . We preprocessed this dataset similarly to
the MORPH dataset. Since the CACD dataset collects images from the Internet using computer vision
techniques, it includes some facial images inappropriate for our consideration. Excluding images, in which
no face or more than two faces were detected in the preprocessing, from the original 163,446 images, we
used 159,402 facial images in the age range of 14â€“62 years. For the AFAD dataset obtainable at https://
github. com/ afad-dataset/ tarball , because faces in its images were already centered, we took no further
3For the face-age datasets, we used a part of program codes published in https:// github. com/ Raschka-research-group/
coral-cnn byCao et al. (2020), but results of our reproduction of their method diï¬€er from theirs mainly because we changed
the learning rate from 510 5to10 2.5. See https:// github. com/ yamasakiryoya/ OTLfor program codes that we used.
10Published in Transactions on Machine Learning Research (1/2023)
preprocessing, and used its 164,418 images with ages 15â€“40. For these face-age datasets, we treated the age
rank as the target variable. Table 1summarizes basic dataset properties, the total sample size ğ‘›tot, the
dimensionğ‘‘of the explanatory variables, and the number ğ¾of classes of the target variable, of all the used
datasets.
For the various-domain datasets, we randomly divided each dataset into 72 % training, 8 % validation, and
20 % test sets. For the face-age datasets, we resized all images to 1281283pixels (3 stems from RGB
channels) and randomly divided each dataset into 72 % training, 8 % validation, and 20 % test sets, and the
training phase used images randomly cropped with the size of 1201203pixels as input to improve the
stability of the model against the diï¬€erence of facial positions, and validation and test phases used images
center-cropped to the same size, following procedures by Cao et al. (2020).
Tasks We considered the three popular OR tasks: minimization of the task risk for the zero-one task loss
â„“zoÂ¹ğ‘—,ğ‘˜Âº= /x31Â¹ğ‘—â‰ ğ‘˜Âº(Task-Z ), that for the absolute deviation task loss â„“adÂ¹ğ‘—,ğ‘˜Âº=jğ‘— ğ‘˜j(Task-A ), and
that for the squared task loss â„“sqÂ¹ğ‘—,ğ‘˜Âº=Â¹ğ‘— ğ‘˜Âº2(Task-S ).
Methods For the various-domain datasets, we applied a 1DT class based on a 4-layer fully-connected
neural network, in which every hidden layer has 100 nodes activated with the sigmoid function in addition to
bias nodes. Also, for the face-age datasets, we applied a 1DT class based on ResNet-34 ( He et al. ,2016), a
modern CNN architecture, following ( Cao et al. ,2020)â€™s implementation. It modiï¬es a fully-connected (the
number of classes)-output ï¬nal layer of the conventional ResNet-34 to a fully-connected 1-output layer.
As the surrogate loss function, we tried the AD loss ğœ‘ad; the IT loss ( 7) withğœ‘Â¹ğ‘¢Âº=minf0,1 ğ‘¢g(Hinge-
IT),logÂ¹1Â¸ğ‘’ ğ‘¢Âº(Logistic-IT ); the AT loss ( 8) withğœ‘Â¹ğ‘¢Âº=minf0,1 ğ‘¢g(Hinge-AT ),logÂ¹1Â¸ğ‘’ ğ‘¢Âº
(Logistic-AT ); the NLL loss ğœ™nllfor the statistical model ( 10) withğœ=ğœlogistic (OLR-NLL ).4
As the BPs class, we tried the non-ordered class and the ordered class for the IT losses; the ordered class
for the AT and OLR-NLL losses. Note that the AD loss has no bias parameters.
As the labeling procedure, we tried the NNT and EOT labeling for the AD loss; the MT, ST, and EOT
labelings for the Hinge-IT, Hinge-AT and Logistic-IT losses; the MT, ST, LB, and EOT labelings for the
Logistic-AT and OLR-NLL losses. When using the ordered BPs class, the MT and ST labeling yield the
same result (see Proposition 2). Also, for the Logistic-AT and OLR-NLL losses with the ordered BPs class,
MT, ST, and LB labelings yield the same result under the Task-A (see Theorem 3).
Cao et al. (2020) declare that their method, which is a combination of the Logistic-AT loss and ST labeling,
is the state-of-the-art method for the face-age datasets in 2020. Although they used the non-ordered BPs
class, bias parameters of the surrogate risk minimizer are guaranteed to be ordered, and hence using the
ordered BPs class would have made little diï¬€erence to the result. Thus, we treated results for the method
with the Logistic-AT loss, ordered BPs class, and ST labeling as their results, in the comparison.
Training and Evaluation During the validation and test phases, models are evaluated based on the
mean zero-one error ( MZE ), the mean absolute deviation error ( MAE ), and the root of the mean squared
error ( RMSE ), which are deï¬ned for a classiï¬er ğ‘“andğ‘šused data points as1
ğ‘šÃğ‘š
ğ‘–=1â„“zoÂ¹ğ‘“Â¹xğ‘–Âº,ğ‘¦ğ‘–Âº,
1
ğ‘šÃğ‘š
ğ‘–=1â„“adÂ¹ğ‘“Â¹xğ‘–Âº,ğ‘¦ğ‘–Âº, andf1
ğ‘šÃğ‘š
ğ‘–=1â„“sqÂ¹ğ‘“Â¹xğ‘–Âº,ğ‘¦ğ‘–Âºg1Â2, for the Task-Z, Task-A, and Task-S, respectively. Here,
the root operation of the RMSE is only for adjusting the scale of the error and does not aï¬€ect our discussion
related to the optimality of the EOT labeling.
We ran 50 trials for the various-domain datasets and 10 trials for the face-age datasets, with randomly-set
diï¬€erent divisions of training, validation, and test sets and initial parameters of the network. In each trial,
we trained the network using Adam of the learning rate 10 2.5and mini-batch size 256 (or 16 when the
training sample size is less than 256) as an optimization procedure for 500 epochs when ğ‘›tot2000 (i.e.,
for the various-domain datasets) or 100 epochs otherwise (i.e., for the face-age datasets). Additionally, for
methods using the EOT labeling, we calculated the threshold parameters according to Algorithm 1at the
end of every training epoch. The above errors were evaluated on the validation set at the end of every
4For numerical stability (to avoid logÂ¹0Âº), we used an approximation of the NLL loss in which the logarithmic function logÂ¹Âº
ofğœ™nllis replaced to logÂ¹Â¸10 8Âºin the learning procedure.
11Published in Transactions on Machine Learning Research (1/2023)
(a) AD
 (b) Hinge-IT, non-ordered
 (c) Hinge-IT, ordered
(d) Hinge-AT, ordered
 (e) Logistic-IT, non-ordered
 (f) Logistic-IT, ordered
(g) Logistic-AT, ordered
 (h) OLR-NLL, ordered
Figure 1: The learning curves of the training and test errors over 100 training epochs in a certain trial.
These are for a special case for the AFAD dataset and the RMSE for the Task-S. In Figures ( c), (d), and
(f)â€“(h), the ST and MT labelings yield the same curves.
training epoch, and then we adopted the model at the timing with the smallest error among the obtained
validation error sequences as the test model.
We judge the signiï¬cance on the classiï¬cation performance of the labeling function by the one-sided Wilcoxon
rank sum test with ğ‘-value 0.05 based on errors for all the trials of methods using diï¬€erent labeling functions,
in each combination of the dataset, error, surrogate loss function, and BPs class.
Results Figure 1shows the learning curves of the training and test errors. The EOT labeling results in
smaller training errors as its design to do, which appears to in turn result in smaller test errors. Also, we can
ï¬nd that the EOT labeling may stabilize (or smooth) the learning curve from Figures ( e) and ( f). Tables 2,
3, and 4show the mean and standard deviation of the errors, for the test model, evaluated on the test set.
In many tried cases, the EOT labeling outperformed the NNT, MT, ST, and LB labelings regarding the
test task risk. In particular, for the face-age datasets, modiï¬ed 1DT-based methods using the EOT labeling
provided better performance than the method (Logistic-AT, Ordered, ST) in Cao et al. (2020) that was the
state-of-the-art in 2020. These results suggest the success of the EOT labeling in the subject (aiming for a
better labeling procedure) of our research.
7 Conclusion and Future Prospect
The NNT, MT, and ST labelings and the LB labeling in typical usages are threshold labelings that may be
sub-optimal depending on the learning result of the 1DT, task under consideration, and data distribution.
In this study we propose to change the labeling procedure of the existing 1DT-based methods to the EOT
labeling that applies the threshold parameters minimizing the empirical task risk for a given 1DT, in order to
obtain higher classiï¬cation performance. Experiments in this paper showed the usefulness of this proposal.
We are also interested in the design of the learning procedure, especially the selection of the surrogate loss
function, for the threshold method. One may be able to undertake systematic discussion on the goodness of
the loss function by ï¬xing components of the threshold method other than the loss function to the optimal
ones. In such discussion, the EOT labeling will serve as the optimal other component. This is a future
prospect.
12Published in Transactions on Machine Learning Research (1/2023)
Table 2: Mean (M) and standard deviation (S) of the test MZE for Task-Z and RW datasets in the form
â€˜MSâ€™. The smaller the error, the better that method is for that dataset and that task. In each block speciï¬ed
with the dataset, error, and learning procedure, we highlighted in bold font the best results that were tied
with each other and superior to all other results with respect to the one-sided Wilcoxon rank sum test with
a signiï¬cance level of 0.05, if they exist. Also, we colored the best results in red for each combination of
dataset and error.
Learning Labeling COL PAS SQ1 SQ2 BON TAE AUT NEW TOY ESL
ADNNT.364.207.345.126.360.162.365.141.497.163.427.085.280.082.035.024.102.045.311.046EOT.380.197.333.134.376.154.353.132.503.159.432.089.290.077.037.024 .085 .042.295 .038
Hinge-ITMT.364.177.385.148.395.161.356.133.512.154.461.096.291.095.039.024.080.038.320.048
non-orderedST.364.177.385.148.395.161.356.133.512.154.461.096.290.093.039.024.080.038.320.048EOT.380.180.335.138.367.137.345.127.477.152.431.087.280.074.034.025 .069 .041.285 .045
Hinge-IT MT,ST .356.185.395.157.420.157.389.135.537.166.451.098.293.082.033.028.065.037.303.042ordered EOT .364.182 .333 .143.384.129.376.140.520.159 .414 .078.287.080.035.030.064.032 .289 .040
Hinge-AT MT,ST .388.206.367.159.396.155.373.133.503.146.448.084.288.091.032.025.074.036.303.037ordered EOT .332.177.320.150.396.158.351.132.485.128 .419 .089.293.082.034.027.072.034 .288 .040
Logistic-ITMT.364.186.398.171.442.154.433.143.560.183.415.093.338.079.038.029.071.033.313.053
non-orderedST.364.186.398.171.442.154.433.143.560.183.415.093.309.072.038.029.071.033.312.053EOT.352.194.355.128.391.143 .356 .128.513.142.404.088 .297 .085.034.028.068.034 .294 .040
Logistic-IT MT,ST .364.207.395.142.444.152.418.141.552.169.410.083.307.092.034.025.064.038.297.039ordered EOT .344.200 .345 .140.413.149.373.139.528.128.397.078.299.084.032.022.061.039.292.037
Logistic-ATMT,ST.360.230.375.125.476.137.427.140.633.167.422.080.281.075.033.024.060.035.303.042
orderedLB.348.203.385.152.451.149.415.134.522.158.420.081.291.097.043.040.060.035.303.043EOT.308.197 .355 .133.389.130 .376 .138.520.143.408.086.284.084.034.026.057.031.298.040
OLR-NLLMT,ST.384.243.390.129.467.132.424.140.577.167.406.079.294.084.043.040.063.034.301.045
orderedLB.372.212.393.170.455.144.418.150.548.146.422.081.281.074.043.036.063.034.299.042EOT.340.193.352.132.393.148.371.132 .490 .149.410.089.291.070.031.025.058.028.294.040
Learning Labeling BAS EUQ LEV ERA SWD WQR CAR MORPH CACD AFAD
ADNNT.089.025.385.044.385.026.754.027.411.028.392.026.014.009.875.005.927.002.882.002EOT .067 .029.388.044.384.027 .733 .032.412.025.395.023.013.008.874.003 .925 .001.878 .002
Hinge-ITMT.031.021.402.042.402.028.739.034.413.031.395.027.009.005.882.003.939.002.902.003
non-orderedST.031.021.402.042.402.028.739.034.413.031.395.027.009.005.881.003.939.002.909.002EOT.027.017 .385 .046.387 .025.732.036.416.027.395.026 .008 .005.882.003.937.003.889 .002
Hinge-IT MT,ST .032.022.389.048.382.024.734.035.417.031.409.023.007.005.882.003.958.002.905.006ordered EOT .025.013 .375 .043.385.025.732.030.414.026 .399 .025.007.005.880.003 .949 .002.886 .003
Hinge-AT MT,ST .055.036.395.047.387.029.750.027.413.032.396.027.007.004.873.005.928.002.882.002ordered EOT .031 .016.386.047.387.026 .728 .029.414.027.402.024.007.005.874.004 .926 .001.878 .002
Logistic-ITMT.024.020.420.051.390.030.744.029.414.036.393.023.017.007.884.003.937.001.887.004
non-orderedST.024.020.420.051.390.030.744.029.414.036.394.021.018.008.885.003.936.002.895.002EOT .015 .014.388 .048.387.026 .734 .033.414.031 .384 .021.010 .006.883.004.936.002 .882 .002
Logistic-IT MT,ST .024.023.407.043.385.029.725.031.416.033.390.024.011.006.893.004.947.002.881.002ordered EOT .016 .013.389.050.386.026.729.031.412.030.389.025.010.006.890.002.946.002.882.002
Logistic-ATMT,ST.019.017.414.039.383.028.731.026 .413 .034.397.022.012.006.874.005.929.004.883.002
orderedLB.018.012.399.047.386.029.758.026.418.031.397.022.011.006.874.006.930.005 .880 .002EOT.016.011.383.046.389.025.732.030 .412 .028.393.020.011.006.872.006.929.004 .879 .003
OLR-NLLMT,ST.018.015.414.051.380.025 .725 .035.411.033.394.025.010.005.871.005.926.002.878.002
orderedLB.017.012.395.045.383.029.747.027.419.030.392.024.010.005.870.005.926.003 .876 .002EOT.015.012 .376 .047.384.026 .733 .032.410.031.394.026.010.005.871.004.925.002 .876 .003
Acknowledgments
This work was supported by Grant-in-Aid for JSPS Fellows, Number 20J23367.
References
Shivani Agarwal. Generalization bounds for some ordinal regression algorithms. In Algorithmic Learning
Theory , pp. 7â€“21, 2008.
Alan Agresti. Analysis of Ordinal Categorical Data , volume 656. John Wiley & Sons, 2010.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliï¬€e. Convexity, classiï¬cation, and risk bounds. Journal
of the American Statistical Association , 101(473):138â€“156, 2006.
13Published in Transactions on Machine Learning Research (1/2023)
Table 3: A counterpart of Table 2regarding MAE for Task-A and RW datasets.
Learning Labeling COL PAS SQ1 SQ2 BON TAE AUT NEW TOY ESL
ADNNT.492.272.352.141.373.171.382.154.555.176.587.122.365.129.035.024.102.045.321.048EOT.512.272.338.142.391.166.369.145.535.170.550.117.369.105.037.024 .085 .042.303 .037
Hinge-ITMT.500.266.407.195.418.187.371.149.580.189.601.138.406.168.039.024.080.038.333.052
non-orderedST.500.266.407.195.418.187.371.149.580.189.601.138.403.168.039.024.080.038.333.052EOT.464.257.345.153.378.150.360.142.527.211.572.120.391.151.034.025 .069 .041.294 .038
Hinge-IT MT,ST .492.269.400.164.440.184.409.151.593.203.582.137.380.130.033.028.065.037.320.048ordered EOT .468.261.338.146.398.143.387.152.555.165.545.121.384.156.035.030.064.032 .300 .038
Hinge-AT MT,ST .504.281.372.174.407.170.389.143.558.187.567.119.372.130.032.025.074.036.317.040ordered EOT .448.273.338.168.398.168.364.147.520.163.552.131.379.122.034.027.072.034 .304 .040
Logistic-ITMT.516.274.420.215.484.192.456.156.630.244.544.153.420.109.039.029.071.033.322.061
non-orderedST.516.274.420.215.484.192.456.156.627.236.544.153.384.098.039.029.071.033.320.060EOT.468.290.360.151 .409 .165.369 .140.575.195.508.119.380.121.034.028.068.034.303.040
Logistic-IT MT,ST .508.303.403.172.484.188.440.156.600.192.521.129.390.125.034.025.064.038.312.043ordered EOT .464.290.365.171.425.174 .385 .152.565.163.519.126.389.130.032.022.061.039.304.039
Logistic-AT MT,ST,LB .484.291.410.198.484.184.436.144.573.200.538.124.369.110.043.040.060.035.314.044ordered EOT .420.281.360.155 .407 .149.389.151.563.190.521.114.364.117.034.026.057.031.307.042
OLR-NLL MT,ST,LB .516.300.417.210.489.185.440.161.597.184.519.119.366.110.043.036.063.034.310.043ordered EOT .464.284 .352 .149.413 .168.384 .146.555.185.509.111.380.111 .031 .025.058.028.304.037
Learning Labeling BAS EUQ LEV ERA SWD WQR CAR MORPH CACD AFAD
ADNNT.109.039.424.058.413.031 1.236.069.429.035.430.029.014.009 2.931.025 5.243.029 3.352.024EOT .080 .032.423.054.413.030 1.231.072.428.031.432.028.013.009 2.918.033 5.238.029 3.349.020
Hinge-ITMT.035.026.453.051.435.035 1.243.073.432.034.428.031.009.005 3.183.022 5.937.215 4.029.057
non-orderedST.035.026.453.051.435.035 1.243.073.432.034.428.031.009.005 3.173.017 5.939.217 3.979.063EOT.033.023 .424 .061.412 .029 1.232.068.431.031.429.031 .008 .0053.099 .0215.742 .0653.740 .027
Hinge-IT MT,ST .035.024.431.058.414.029 1.245.074.437.037.446.027.007.005 3.349.040 8.052.279 4.694.116ordered EOT .029.019.417.053.413.030 1.229.075.430.027.437.027.007.0053.204 .0326.581 .0573.761 .107
Hinge-AT MT,ST .066.046.435.059.417.031 1.222.062.433.038.433.031.007.004 2.881.033 5.264.030 3.582.090ordered EOT .043 .023.422.062.413.030 1.226.063.431.034.435.029.007.005 2.863.0225.242 .0343.390 .018
Logistic-ITMT.026.024.463.057.425.036 1.262.073.438.037.430.026.017.007 3.189.049 5.758.101 3.796.080
non-orderedST.026.024.463.057.425.036 1.262.073.438.037.429.026.018.009 3.188.049 5.757.103 3.732.038EOT .017 .018.425 .064.414.0311.228 .070.436.034.421.026 .010 .0063.124 .0335.631 .0933.538 .030
Logistic-IT MT,ST .027.027.456.052.416.030 1.239.076.440.033.426.026.011.006 3.742.084 6.685.131 4.082.043ordered EOT .019.016 .432 .062.413.026 1.226.069.436.031.428.027.010.0063.540 .0386.359 .0423.619 .023
Logistic-AT MT,ST,LB .017.015 .409 .056.412.029 1.225.068.433.032.429.027.010.005 2.874.037 5.381.138 3.375.033ordered EOT .020.014.438.052.416.032 1.233.063.437.034.431.023.011.006 2.856.030 5.342.128 3.367.025
OLR-NLL MT,ST,LB .018.015 .407 .055.416.030 1.230.069.434.035.432.025.011.006 2.873.043 5.243.034 3.345.020ordered EOT .019.015.433.057.417.029 1.234.067.436.035.427.026.010.005 2.845.037 5.228.041 3.343.017
Paul-Christian BÃ¼rkner and Matti Vuorre. Ordinal regression models in psychology: A tutorial. Advances
in Methods and Practices in Psychological Science , 2(1):77â€“101, 2019.
Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka. Rank consistent ordinal regression for neural networks
with application to age estimation. Pattern Recognition Letters , 140:325â€“331, 2020.
Bor-Chun Chen, Chu-Song Chen, and Winston H Hsu. Cross-age reference coding for age-invariant face
recognition and retrieval. In Proceedings of the European Conference on Computer Vision , pp. 768â€“783,
2014.
Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. Journal of Machine Learning
Research , 6(Jul):1019â€“1041, 2005.
Wei Chu and S Sathiya Keerthi. New approaches to support vector ordinal regression. In Proceedings of the
International Conference on Machine Learning , pp. 145â€“152, 2005.
Wei Chu and S Sathiya Keerthi. Support vector ordinal regression. Neural Computation , 19(3):792â€“815,
2007.
Jacob Cohen. A coeï¬ƒcient of agreement for nominal scales. Educational and Psychological Measurement , 20
(1):37â€“46, 1960.
14Published in Transactions on Machine Learning Research (1/2023)
Table 4: A counterpart of Table 2regarding RMSE for Task-S and RW datasets.
Learning Labeling COL PAS SQ1 SQ2 BON TAE AUT NEW TOY ESL
ADNNT.779.343.593.144.613.193.630.145.821.194.883.116.730.164.167.085.310.075.589.056EOT.827.324.576.161.633.167.617.145.797.171 .828 .109.748.149.173.084 .281 .075.574.045
Hinge-ITMT.799.319.652.194.676.190.609.171.866.214.884.124.767.199.179.084.275.068.604.067
non-orderedST.799.319.652.194.676.190.609.171.866.214.884.124.765.200.179.084.275.068.604.067EOT.760.326.584.171.614.169.601.168 .793 .210.857.110.753.161.160.091 .253 .074.570 .046
Hinge-IT MT,ST .789.331.633.156.694.186.649.156.837.204.885.140.745.164.154.095.243.078.591.060ordered EOT .763.325.571.178.642.149.618.166.791.147.848.105.736.151.159.101.242.071.570.051
Hinge-AT MT,ST .786.307.596.194.646.172.628.164.817.199.866.116.741.171.153.092.264.066.598.052ordered EOT .752.343.569.209.638.165.601.166.757.160.834.097.750.156.157.095.260.064 .569 .043
Logistic-ITMT.812.330.656.222.734.193.695.142.875.242.846.132.789.186.175.094.258.064.589.065
non-orderedST.812.330.656.222.734.193.695.142.870.233.846.132.774.192.175.094.258.064.586.064EOT.754.345.597.169 .650 .173.608 .158.806.194.807.100.745.146.161.091.252.068.566.049
Logistic-IT MT,ST .790.352.644.182.723.188.680.144.842.186.837.124.753.159.159.091.239.082.583.055ordered EOT .779.342.611.199 .663 .173.624 .145.793.160.809.110.742.164.160.082.230.091.572.047
Logistic-ATMT,ST.722.368.614.187.685.176.639.144.797.205.812.112.736.150.182.094.234.080.585.046
orderedLB.757.361.655.202.729.181.680.134.815.197.825.117.726.153.182.098.230.082.582.049EOT.732.346.601.178.651.160.623.163.802.199.813.110.720.141.162.086.224.080 .565 .047
OLR-NLLMT,ST.795.347.634.158.706.177.640.153.800.180.821.118.741.157.166.096.239.071.582.051
orderedLB.795.351.651.221.735.182.680.146.830.168.833.111.736.153.184.097.240.073.578.049EOT.754.351.587.161.655.174.620.159.787.181.812.107.751.156.153.089.231.066.569.048
Learning Labeling BAS EUQ LEV ERA SWD WQR CAR MORPH CACD AFAD
ADNNT.391.075.712.062.697.036 1.623.094.684.035.710.030.113.044 3.962.040 7.395.027 4.571.032EOT .341 .069.719.057.698.036 1.589.077.676.030.721.033.108.044 3.942.039 7.410.0354.534 .027
Hinge-ITMT.187.097.748.058.711.039 1.657.092.689.036.708.028.088.033 4.273.028 8.009.277 5.435.086
non-orderedST.187.097.748.058.711.039 1.657.092.689.036.708.028.088.033 4.272.024 8.003.276 5.301.072EOT.190.097 .711 .068.695 .0351.586 .072.681.031.712.030 .081 .0334.137 .0437.721 .0604.927 .024
Hinge-IT MT,ST .188.088.727.068.697.037 1.644.083.692.037.722.025.078.036 4.529.04110.098.2766.374.183ordered EOT .180.092.713.062.693.0361.588 .075.679 .029.716.029.077.0364.288 .0388.475 .0604.958 .116
Hinge-AT MT,ST .277.118.726.072.698.036 1.607.086.685.034.707.027.079.033 3.874.046 7.443.044 4.902.130ordered EOT .260.101.714.071.693.034 1.591.077.676.027.711.025.077.036 3.867.031 7.442.0484.550 .024
Logistic-ITMT.151.103.756.074.706.041 1.655.085.696.035.713.032.133.032 4.257.070 7.870.109 5.151.078
non-orderedST.151.103.756.074.706.041 1.655.085.696.035.710.039.135.036 4.255.068 7.869.110 5.058.065EOT.119.101 .708 .062.694.0341.585 .079.684.030.702.026 .094 .0344.172 .0457.620 .0874.706 .036
Logistic-IT MT,ST .148.112.748.062.698.037 1.644.090.694.033.707.029.102.032 5.016.100 8.902.182 5.720.062ordered EOT .134.096 .708 .065.693.0361.592 .078.684.027.709.031.095.0364.726 .0648.320 .0554.798 .035
Logistic-ATMT,ST.140.065 .706 .060.696.035 1.585.077.686.031.708.027.102.033 3.859.043 7.520.096 4.589.053
orderedLB.137.071.731.066.696.038 1.604.081.689.034.710.025.102.033 3.858.043 7.497.0804.517 .040EOT.130.095 .693 .067.694.036 1.587.080.682.032.713.029.099.037 3.841.034 7.494.0534.517 .022
OLR-NLLMT,ST.134.067.706.064.697.033 1.592.081.683.024.710.028.100.025 3.870.037 7.437.061 4.574.031
orderedLB.129.074.724.061.696.032 1.616.085.690.034.710.029.099.025 3.869.036 7.423.0604.508 .029EOT.128.091.699.066.693.035 1.588.079.682.026.708.028.096.030 3.852.044 7.417.0594.510 .025
Jacob Cohen. Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit.
Psychological Bulletin , 70(4):213, 1968.
Joaquim F Pinto da Costa, Hugo Alonso, and Jaime S Cardoso. The unimodal model for the classiï¬cation
of ordinal data. Neural Networks , 21(1):78â€“91, 2008.
Stephen E Fienberg and William M Mason. Identiï¬cation and estimation of age-period-cohort models in the
analysis of discrete archival data. Sociological Methodology , 10:1â€“67, 1979.
Eibe Frank and Mark Hall. A simple approach to ordinal classiï¬cation. In Proceedings of the European
Conference on Machine Learning , pp. 145â€“156, 2001.
Philip Hans Franses and Richard Paap. Quantitative Models in Marketing Research . Cambridge University
Press, 2001.
Pedro Antonio Gutierrez, Maria Perez-Ortiz, Javier Sanchez-Monedero, Francisco Fernandez-Navarro, and
Cesar Hervas-Martinez. Ordinal regression methods: survey and experimental study. IEEE Transactions
on Knowledge and Data Engineering , 28(1):127â€“146, 2015.
15Published in Transactions on Machine Learning Research (1/2023)
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 770â€“778, 2016.
R Herbrich, T Graepel, and K Obermayer. Support vector learning for ordinal regression. In International
Conference on Artiï¬cial Neural Networks , volume 1, pp. 97â€“102, 1999.
Stefan Kramer, Gerhard Widmer, Bernhard Pfahringer, and Michael De Groeve. Prediction of ordinal classes
using regression trees. Fundamenta Informaticae , 47(1-2):1â€“13, 2001.
Ling Li and Hsuan-Tien Lin. Ordinal regression by extended binary classiï¬cation. In Advances in Neural
Information Processing Systems , pp. 865â€“872, 2007.
Hsuan-Tien Lin and Ling Li. Large-margin thresholded ensembles for ordinal regression: Theory and practice.
InAlgorithmic Learning Theory , pp. 319â€“333. Springer, 2006.
Hsuan-Tien Lin and Ling Li. Reduction from cost-sensitive ordinal ranking to weighted binary classiï¬cation.
Neural Computation , 24(5):1329â€“1367, 2012.
Tie-Yan Liu. Learning to Rank for Information Retrieval . Springer Science & Business Media, 2011.
Peter McCullagh. Regression models for ordinal data. Journal of the Royal Statistical Society: Series B
(Methodological) , 42(2):109â€“127, 1980.
Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple output cnn
for age estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pp. 4920â€“4928, 2016.
Fabian Pedregosa, Francis Bach, and Alexandre Gramfort. On the consistency of ordinal regression methods.
Journal of Machine Learning Research , 18(Jan):1769â€“1803, 2017.
Sebastian Raschka. Mlxtend: Providing machine learning and data science utilities and extensions to
pythonâ€™s scientiï¬c computing stack. Journal of Open Source Software , 3(24):638, 2018.
Karl Ricanek and Tamirat Tesafaye. Morph: A longitudinal image database of normal adult age-progression.
InProceedings of the IEEE International Conference on Automatic Face and Gesture Recognition , pp. 341â€“
345, 2006.
Christos Sagonas, Epameinondas Antonakos, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic.
300 faces in-the-wild challenge: Database and results. Image and Vision Computing , 47:3â€“18, 2016.
Amnon Shashua and Anat Levin. Ranking with large margin principle: Two approaches. In Advances in
Neural Information Processing Systems , pp. 961â€“968, 2003.
WA Thompson Jr. On the treatment of grouped observations in life studies. Biometrics , pp. 463â€“470, 1977.
Richard Williams. Generalized ordered logit/partial proportional odds models for ordinal dependent vari-
ables. The Stata Journal , 6(1):58â€“82, 2006.
Ryoya Yamasaki. Unimodal likelihood models for ordinal data. Transactions on Machine Learning Research ,
2022. URL https:// openreview. net/ forum? id=1l0sClLiPc .
Shipeng Yu, Kai Yu, Volker Tresp, and Hans-Peter Kriegel. Collaborative ordinal regression. In Proceedings
of the International Conference on Machine Learning , pp. 1089â€“1096, 2006.
16Published in Transactions on Machine Learning Research (1/2023)
A Proof of Consistency of Statistical Methods
We here give proof of the theorem on the interpretation of the learning procedure for statistical methods.
Proof of Theorem 2.We can characterize the surrogate risk minimization for the NLL loss as maximum
likelihood estimation for the statistical model ( 10) for multi-class classiï¬cation problem through the equation
min
ğ‘2A,b2BEÂ»ğœ™nllÂ¹ğ‘Â¹XÂº,b,ğ‘Œ;ğœÂºÂ¼=min
ğ‘2A,b2BEğ¾Ã•
ğ‘¦=1PrÂ¹ğ‘¦jXÂºğœ™nllÂ¹ğ‘Â¹XÂº,b,ğ‘¦;ğœÂº
=min
ğ‘2A,b2BE
 ğ¾Ã•
ğ‘¦=1PrÂ¹ğ‘¦jXÂºlogğ‘ƒÂ¹ğ‘¦;ğœ,ğ‘Â¹xÂº,bÂº
.(15)
According to the method of Lagrange multiplier, one solution of a point-wise (at each X=x) minimization
problem
min
fË†PrÂ¹ğ‘˜jxÂºgğ‘˜ ğ¾Ã•
ğ‘¦=1PrÂ¹ğ‘¦jxÂºlogË†PrÂ¹ğ‘¦jxÂº,subject toğ¾Ã•
ğ‘¦=1Ë†PrÂ¹ğ‘¦jxÂº=1 (16)
isË†PrÂ¹ğ‘¦jxÂº=PrÂ¹ğ‘¦jxÂº=ğ‘ƒÂ¹ğ‘¦;ğœ,Ëœğ‘Â¹xÂº,ËœbÂº,ğ‘¦=1,...,ğ¾ , where the existence of such fËœğ‘Â¹xÂº,Ëœbgis assumed in the
statement of the theorem. This solution applies for any x2Rğ‘‘, and one can see that a solution of ( 15) is
fËœğ‘,Ëœbg, which completes the proof of the statement for the NLL loss.
Also, for the ANLCL loss, we can provide the following characterization:
EÂ»ğœ™anlclÂ¹ğ‘Â¹XÂº,b,ğ‘ŒÂºÂ¼
=E
 ğ¾Ã•
ğ‘¦=1PrÂ¹ğ‘¦jXÂºğ‘¦ 1Ã•
ğ‘˜=1logf1 ğ‘„Â¹ğ‘˜;ğœ,ğ‘Â¹XÂº,bÂºgÂ¸ğ¾ 1Ã•
ğ‘˜=ğ‘¦logğ‘„Â¹ğ‘˜;ğœ,ğ‘Â¹XÂº,bÂº
= ğ¾ 1Ã•
ğ‘¦=1E
PrÂ¹ğ‘Œğ‘¦jXÂºlogğ‘„Â¹ğ‘˜;ğœ,ğ‘Â¹XÂº,bÂºÂ¸f 1 PrÂ¹ğ‘Œğ‘¦jXÂºglogf1 ğ‘„Â¹ğ‘¦;ğœ,ğ‘Â¹XÂº,bÂºg
,(17)
whereğ‘„Â¹ğ‘¦;ğœ,ğ‘Â¹xÂº,bÂºâ‰”Ãğ‘¦
ğ‘˜=1ğ‘ƒÂ¹ğ‘˜;ğœ,ğ‘Â¹xÂº,bÂºand the expectation value EÂ»Â¼is taken for X. On the ground
of the binary version, â€˜ ğ‘¦or lessâ€™ vs. â€˜more than ğ‘¦â€™ (ğ‘¦=1,...,ğ¾ 1), of ( 16), one can prove the statement
similarly. â–¡
One may consider the IT loss ( 7) withğœ‘Â¹ğ‘¢Âº= logfğœÂ¹ğ‘¢Âºg, which we call immediate negative log cu-
mulative likelihoods (INLCL) loss function . However, it is diï¬ƒcult to characterize the surrogate risk
minimization with the INLCL loss as a problem with a known solution unlike those for the NLL and ANLCL
losses, and the optimality condition for the INLCL loss is unknown.
B Proof of Relationships between Labeling Functions
This section provides proofs of Theorems 3and4regarding the relationships between the LB and threshold
labelings. Propositions 1and2would be trivial, so we omit proofs of them.
First, we prove Theorem 3.
Proof of Theorem 3.We introduce the functions
ğ‘…ğ‘—Â¹ğ‘¢Âºâ‰”ğ¾Ã•
ğ‘˜=1fğœÂ¹Â¯ğ‘ğ‘˜ ğ‘¢Âº ğœÂ¹Â¯ğ‘ğ‘˜ 1 ğ‘¢Âºgâ„“Â¹ğ‘—,ğ‘˜Âº=â„“Â¹ğ‘—,ğ¾ÂºÂ¸ğ¾ 1Ã•
ğ‘˜=1ğœÂ¹Â¯ğ‘ğ‘˜ ğ‘¢Âºfâ„“Â¹ğ‘—,ğ‘˜Âº â„“Â¹ğ‘—,ğ‘˜Â¸1Âºgforğ‘—=1,...,ğ¾,
(18)
17Published in Transactions on Machine Learning Research (1/2023)
with Â¯ğ‘0= 1 and Â¯ğ‘ğ¾=Â¸1, where the equation holds, since ğœÂ¹ 1Âº =0andğœÂ¹Â¸1Âº =1.
The classiï¬er based on the LB labeling, ğ‘“Â¹xÂº=arg minğ‘—2Â»ğ¾Â¼Ãğ¾
ğ‘˜=1ğ‘ƒÂ¹ğ‘˜;ğœ,Â¯ğ‘Â¹xÂº,Â¯bÂºâ„“Â¹ğ‘—,ğ‘˜Âº, is equal to
arg minğ‘—2Â»ğ¾Â¼ğ‘…ğ‘—Â¹Â¯ğ‘Â¹xÂºÂº. According to Proposition 1, the LB labeling is a certain threshold labeling if and
only if arg minğ‘—2Â»ğ¾Â¼fğ‘…ğ‘—Â¹ğ‘¢1Âºgğ¾
ğ‘—=1arg minğ‘—2Â»ğ¾Â¼fğ‘…ğ‘—Â¹ğ‘¢2Âºgğ¾
ğ‘—=1for anyğ‘¢1,ğ‘¢22Rsuch thatğ‘¢1ğ‘¢2. The latter
condition holds if the situation
ğ‘…ğ‘˜Â¹ğ‘¢Âº> ğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢2Â¹ğ‘ 1,ğ‘ 2Âºandğ‘…ğ‘˜Â¹ğ‘¢Âº< ğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢2Â¹ğ‘ 2,ğ‘ 3Âºwithğ‘˜ <ğ‘™, ğ‘  1<ğ‘ 2<ğ‘ 3 (19)
does not occur. In the following we assume ğ‘˜ <ğ‘™ for the indices ğ‘˜,ğ‘™2Â»ğ¾Â¼.
Proof of (i). Under the assumption described in the statement of the theorem, the diï¬€erence
ğ‘…ğ‘˜Â¹ğ‘¢Âº ğ‘…ğ‘™Â¹ğ‘¢Âº=fâ„“Â¹ğ‘˜,ğ¾Âº â„“Â¹ğ‘™,ğ¾Âºg|                   {z                   }
non-negative
constantÂ¸ğ¾ 1Ã•
ğ‘—=1ğœÂ¹Â¯ğ‘ğ‘— ğ‘¢Âº
|      {z      }
non-negative
non-increasingfâ„“Â¹ğ‘˜,ğ‘—Âº â„“Â¹ğ‘˜,ğ‘—Â¸1Âº â„“Â¹ğ‘™,ğ‘—ÂºÂ¸â„“Â¹ğ‘™,ğ‘—Â¸1Âºg|                                                     {z                                                     }
non-positive
constant(20)
is non-decreasing with respect to ğ‘¢. Thus,ğ‘…ğ‘˜Â¹ğ‘¢Âºğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢ğ‘andğ‘…ğ‘˜Â¹ğ‘¢Âºğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢ğ‘for some
pointğ‘,ğ‘…ğ‘˜Â¹ğ‘¢Âº ğ‘…ğ‘™Â¹ğ‘¢Âºfor anyğ‘¢, orğ‘…ğ‘˜Â¹ğ‘¢Âº ğ‘…ğ‘™Â¹ğ‘¢Âºfor anyğ‘¢, which implies that the above-mentioned
situation ( 19) does not occur. Note that, for the instances â„“=â„“ad,â„“sq, one has that
â„“ğ‘˜,ğ‘™Â¹ğ‘—Âº=â„“Â¹ğ‘˜,ğ‘—Âº â„“Â¹ğ‘˜,ğ‘—Â¸1Âº â„“Â¹ğ‘™,ğ‘—ÂºÂ¸â„“Â¹ğ‘™,ğ‘—Â¸1Âº=(
 2 /x31Â¹ğ‘˜ğ‘—ğ‘™ 1Âºforâ„“=â„“ad,
2Â¹ğ‘˜ ğ‘™Âº, forâ„“=â„“sq.(21)
This completes the proof of the statement (i).
Proof of (ii). Forâ„“=â„“zo,ğ‘withğ‘2Â»0,bğ¾Â2cÂºwhereâ„“zo=â„“zo,0, the function ğ‘…ğ‘—Â¹ğ‘¢Âºreduces to
ğ‘…ğ‘—Â¹ğ‘¢Âº=1 fğœÂ¹ğ‘ğ‘— ğ‘¢Âº ğœÂ¹ğ‘ğ‘— ğ‘¢Âºg, (22)
withğ‘ğ‘—â‰”Â¯ğ‘maxf0,ğ‘— ğ‘gandğ‘ğ‘—â‰”Â¯ğ‘minfğ‘—Â¸ğ‘,ğ¾g, whereğ‘ğ‘—< ğ‘ğ‘—. Lemma 1(described after the proof of
Theorem 3) shows the shape of the function ğ‘…ğ‘—Â¹ğ‘¢Âº: Under the assumption of Theorem 3(ii),ğ‘…ğ‘—Â¹ğ‘¢Âºis
minimized at ğ‘¢=Â¹ğ‘ğ‘—Â¸ğ‘ğ‘—ÂºÂ2â‰”ğ‘ğ‘—, symmetric in ğ‘¢aroundğ‘¢=ğ‘ğ‘—, non-increasing in ğ‘¢forğ‘¢ < ğ‘ğ‘—, and
non-decreasing in ğ‘¢whenğ‘¢ > ğ‘ğ‘—, from Lemma 1(i) and (ii). Also, assuming that ğ‘ğ‘—is ï¬xed, then ğ‘…ğ‘—Â¹ğ‘¢Âºis
non-decreasing in ğ‘ğ‘— ğ‘ğ‘—, from Lemma 1(iii).
Whenğ‘ğ‘˜ ğ‘ğ‘˜=ğ‘ğ‘™ ğ‘ğ‘™, the translated two curves ğ‘…ğ‘˜Â¹ğ‘¢Âºandğ‘…ğ‘™Â¹ğ‘¢Âºhave just one intersection point at
ğ‘¢=Â¹ğ‘ğ‘˜Â¸ğ‘ğ‘™ÂºÂ2, and it holds that ğ‘…ğ‘˜Â¹ğ‘¢Âºğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢Â¹ğ‘ğ‘˜Â¸ğ‘ğ‘™ÂºÂ2andğ‘…ğ‘˜Â¹ğ‘¢Âºğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢Â¹ğ‘ğ‘˜Â¸ğ‘ğ‘™ÂºÂ2.
Therefore, the situation ( 19) does not occur if ğ‘ğ‘˜ ğ‘ğ‘˜=ğ‘ğ‘™ ğ‘ğ‘™.
Then, assume ğ‘ğ‘˜ ğ‘ğ‘˜< ğ‘ğ‘™ ğ‘ğ‘™(the following proof strategy for this setting can be applied to the other
settingğ‘ğ‘˜ ğ‘ğ‘˜> ğ‘ğ‘™ ğ‘ğ‘™). In this setting, ğ‘…ğ‘˜Â¹ğ‘¢Âº> ğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢ğ‘ğ‘™due to the shape of the functions ğ‘…ğ‘˜
andğ‘…ğ‘™. Also, withinÂ»ğ‘ğ‘˜,ğ‘ğ‘™Â¼, they can have one intersection point ğ‘at most such that ğ‘…ğ‘˜Â¹ğ‘¢Âºğ‘…ğ‘™Â¹ğ‘¢Âºfor
ğ‘¢2Â»ğ‘ğ‘˜,ğ‘Â¼andğ‘…ğ‘˜Â¹ğ‘¢Âºğ‘…ğ‘™Â¹ğ‘¢Âºforğ‘¢2Â»ğ‘,ğ‘ğ‘™Â¼, sinceğ‘…ğ‘˜Â¹ğ‘¢Âºandğ‘…ğ‘™Â¹ğ‘¢Âºare respectively non-decreasing and
non-increasing in ğ‘¢. Therefore, the situation ( 19) can be satisï¬ed only in such a situation that there exists
a pointğ‘satisfying
ğ‘…ğ‘˜Â¹ğ‘Âº=ğ‘…ğ‘™Â¹ğ‘Âº, ğ‘…0
ğ‘˜Â¹ğ‘Âº< ğ‘…0
ğ‘™Â¹ğ‘Âº,andğ‘ğ‘ğ‘˜. (23)
The existence of such a point ğ‘implies that
ğœ0Â¹ğ‘ğ‘˜ ğ‘Âº ğœ0Â¹ğ‘ğ‘˜ ğ‘Âº
ğœÂ¹ğ‘ğ‘˜ ğ‘Âº ğœÂ¹ğ‘ğ‘˜ ğ‘Âº<ğœ0Â¹ğ‘ğ‘™ ğ‘Âº ğœ0Â¹ğ‘ğ‘™ ğ‘Âº
ğœÂ¹ğ‘ğ‘™ ğ‘Âº ğœÂ¹ğ‘ğ‘™ ğ‘Âºwithğ‘ğ‘˜ğ‘ğ‘™, ğ‘ğ‘˜ğ‘ğ‘™, ğ‘ğ‘˜ğ‘ğ‘˜, ğ‘ğ‘™ğ‘ğ‘™, ğ‘ğ‘ğ‘˜.(24)
However, the assumption thatğœ0Â¹ğ‘£1Âº ğœ0Â¹ğ‘£2Âº
ğœÂ¹ğ‘£1Âº ğœÂ¹ğ‘£2Âºis non-increasing in ğ‘£1with ï¬xedğ‘£2and inğ‘£2with ï¬xedğ‘£1when
ğ‘£1<ğ‘£2shows that
ğœ0Â¹ğ‘ğ‘˜ ğ‘Âº ğœ0Â¹ğ‘ğ‘˜ ğ‘Âº
ğœÂ¹ğ‘ğ‘˜ ğ‘Âº ğœÂ¹ğ‘ğ‘˜ ğ‘Âºğœ0Â¹ğ‘ğ‘˜ ğ‘Âº ğœ0Â¹ğ‘ğ‘™ ğ‘Âº
ğœÂ¹ğ‘ğ‘˜ ğ‘Âº ğœÂ¹ğ‘ğ‘™ ğ‘Âºğœ0Â¹ğ‘ğ‘™ ğ‘Âº ğœ0Â¹ğ‘ğ‘™ ğ‘Âº
ğœÂ¹ğ‘ğ‘™ ğ‘Âº ğœÂ¹ğ‘ğ‘™ ğ‘Âº, (25)
18Published in Transactions on Machine Learning Research (1/2023)
which contradicts to equation ( 24). Therefore, the situation ( 19) does not occur also when ğ‘ğ‘˜ ğ‘ğ‘˜<ğ‘ğ‘™ ğ‘ğ‘™.
Note that, especially when ğœ=ğœlogistic , one can show that
ğœ0Â¹ğ‘£1Âº ğœ0Â¹ğ‘£2Âº
ğœÂ¹ğ‘£1Âº ğœÂ¹ğ‘£2Âº=ğœlogisticÂ¹ğ‘£1ÂºÂ¹1 ğœlogisticÂ¹ğ‘£1ÂºÂº ğœlogisticÂ¹ğ‘£2ÂºÂ¹1 ğœlogisticÂ¹ğ‘£2ÂºÂº
ğœlogisticÂ¹ğ‘£1Âº ğœlogisticÂ¹ğ‘£2Âº
=1 fğœlogisticÂ¹ğ‘£1ÂºÂ¸ğœlogisticÂ¹ğ‘£2Âºg,(26)
is decreasing in ğ‘£1with ï¬xedğ‘£2and inğ‘£2with ï¬xedğ‘£1. Moreover, when ğœ=ğœgauss, one has that
ğœ0Â¹ğ‘£1Âº ğœ0Â¹ğ‘£2Âº
ğœÂ¹ğ‘£1Âº ğœÂ¹ğ‘£2Âº/ğ‘’ ğ‘£2
1Â2 ğ‘’ ğ‘£2
2Â2
ğœgaussÂ¹ğ‘£1Âº ğœgaussÂ¹ğ‘£2Âºâ‰”ğ‘“1Â¹ğ‘£1,ğ‘£2Âº, (27)
that the derivative of ğ‘“1Â¹ğ‘£1,ğ‘£2Âºwith respect to ğ‘£1,
ğœ•
ğœ•ğ‘£1ğ‘“1Â¹ğ‘£1,ğ‘£2Âº= ğ‘£1ğ‘’ ğ‘£2
1Â2fğœgaussÂ¹ğ‘£1Âº ğœgaussÂ¹ğ‘£2Âºg  ğ‘’ ğ‘£2
1Â2 ğ‘’ ğ‘£2
2Â21p
2ğœ‹ğ‘’ ğ‘£2
1Â2
fğœgaussÂ¹ğ‘£1Âº ğœgaussÂ¹ğ‘£2Âºg2(28)
has the same sign as
ğ‘“2Â¹ğ‘£1,ğ‘£2Âºâ‰” ğ‘£1fğœgaussÂ¹ğ‘£1Âº ğœgaussÂ¹ğ‘£2Âºg 1p
2ğœ‹ğ‘’ ğ‘£2
1Â2 1p
2ğœ‹ğ‘’ ğ‘£2
2Â2
, (29)
and that the derivative of ğ‘“2Â¹ğ‘£1,ğ‘£2Âºwith respect to ğ‘£2is
ğœ•
ğœ•ğ‘£2ğ‘“2Â¹ğ‘£1,ğ‘£2Âº=Â¹ğ‘£1 ğ‘£2Âº1p
2ğœ‹ğ‘’ ğ‘£2
2Â2. (30)
Sinceğœ•
ğœ•ğ‘£2ğ‘“2Â¹ğ‘£1,ğ‘£2Âº<0whenğ‘£1< ğ‘£2andğ‘“2Â¹ğ‘£1,ğ‘£1Âº=0, it holds that ğ‘“2Â¹ğ‘£1,ğ‘£2Âº, which has the same sign
asğœ•
ğœ•ğ‘£1ğœ0Â¹ğ‘£1Âº ğœ0Â¹ğ‘£2Âº
ğœÂ¹ğ‘£1Âº ğœÂ¹ğ‘£2Âº, is negative when ğ‘£1< ğ‘£2, that is,ğœ0Â¹ğ‘£1Âº ğœ0Â¹ğ‘£2Âº
ğœÂ¹ğ‘£1Âº ğœÂ¹ğ‘£2Âºis decreasing in ğ‘£1with ï¬xedğ‘£2when
ğ‘£1<ğ‘£2; monotonicity in ğ‘£2with ï¬xedğ‘£1can be proved by the same discussion.
Proof of (iii). Regarding the MT and ST labelings, let ğ‘¦=â„thrÂ¹ğ‘¢;Â¯bÂºunder the assumption Â¯ğ‘1 Â¯ğ‘ğ¾ 1,
which implies that Â¯ğ‘1 Â¯ğ‘ğ‘¦ 1ğ‘¢Â¯ğ‘ğ‘¦ Â¯ğ‘ğ¾ 1. Regarding the LB labeling for the likelihood
model ( 10), one has that, with the abbreviations ğœğ‘˜â‰”ğœÂ¹Â¯ğ‘ğ‘˜ ğ‘¢Âºforğ‘˜=1,...,ğ¾ ,
ğ‘…ğ‘—Â¹ğ‘¢Âº=ğ¾Ã•
ğ‘˜=1fğœğ‘˜ ğœğ‘˜ 1gjğ‘— ğ‘˜j,
=jğ‘— 1jfğœ1 ğœ0gÂ¸jğ‘— 2jfğœ2 ğœ1gÂ¸Â¸ 2fğœğ‘— 2 ğœğ‘— 3gÂ¸fğœğ‘— 1 ğœğ‘— 2g
Â¸fğœğ‘—Â¸1 ğœğ‘—gÂ¸2fğœğ‘—Â¸2 ğœğ‘—Â¸1gÂ¸Â¸jğ‘— ğ¾Â¸1jfğœğ¾ 1 ğœğ¾ 2gÂ¸jğ‘— ğ¾jfğœğ¾ ğœğ¾ 1g
= jğ‘— 1jğœ0|{z}
0Â¸ğ‘— 1Ã•
ğ‘˜=1ğœğ‘˜
 ğ¾ 1Ã•
ğ‘˜=ğ‘—ğœğ‘˜
Â¸jğ‘— ğ¾jğœğ¾|{z}
1
=ğ‘— 1Ã•
ğ‘˜=1ğœÂ¹Â¯ğ‘ğ‘˜ ğ‘¢ÂºÂ¸ğ¾ 1Ã•
ğ‘˜=ğ‘—f1 ğœÂ¹Â¯ğ‘ğ‘˜ ğ‘¢Âºg,(31)
for everyğ‘—2Â»ğ¾Â¼. Simple calculations show that ğœÂ¹Â¯ğ‘ğ‘˜ ğ‘¢Âº0.5forğ‘˜=1,...,ğ‘¦ 1andf1 ğœÂ¹Â¯ğ‘ğ‘˜ ğ‘¢Âºg 0.5
forğ‘˜=ğ‘¦,...,ğ¾ 1, from Â¯ğ‘1 Â¯ğ‘ğ‘¦ 1ğ‘¢Â¯ğ‘ğ‘¦ Â¯ğ‘ğ¾ 1and the assumption on the shape of ğœ. One
would see that objective function ( 31) is minimized at ğ‘—=ğ‘¦because some summands are replaced by ones
of 0.5 or more if ğ‘—deviates from ğ‘¦, which concludes the proof. â–¡
The following is an auxiliary lemma for the above-described proof of Theorem 3.
Lemma 1. Suppose that ğœis non-decreasing and satisï¬es ğœÂ¹ 1Âº=0andğœÂ¹Â¸1Âº=1. Deï¬neğ‘†Â¹ğ‘¢;ğ‘,ğ‘Âºâ‰”
ğœÂ¹ğ‘ ğ‘¢Âº ğœÂ¹ğ‘ ğ‘¢Âºforğ‘ <ğ‘ . Then, one has that
19Published in Transactions on Machine Learning Research (1/2023)
(i)ğ‘†Â¹ğ‘¢;ğ‘,ğ‘Âºwith ï¬xedğ‘andğ‘is symmetric in ğ‘¢aroundğ‘¢=ğ‘Â¸ğ‘
2, ifğœÂ¹ Âº=1 ğœÂ¹Âº, or ifğœis
diï¬€erentiable and ğœ0is even.
(ii)ğ‘†Â¹ğ‘¢;ğ‘,ğ‘Âºwith ï¬xedğ‘andğ‘is maximized with respect to ğ‘¢atğ‘¢=ğ‘Â¸ğ‘
2, non-decreasing in ğ‘¢for
ğ‘¢ <ğ‘Â¸ğ‘
2, and non-increasing in ğ‘¢forğ‘¢ >ğ‘Â¸ğ‘
2, ifğœis diï¬€erentiable and ğœ0Â¹ğ‘¢Âºis even and non-
increasing in ğ‘¢ifğ‘¢>0.
(iii)ğ‘†Â¹ğ‘¢;ğ‘,ğ‘Âºwith ï¬xedğ‘¢andğ‘Â¸ğ‘
2is increasing with respect to Â¹ğ‘ ğ‘Âº.
Proof of Lemma 1.Proof of (i). The assumptions that ğœÂ¹ 1Âº=0,ğœÂ¹Â¸1Âº=1, andğœ0is even imply that
ğœÂ¹ Âº=1 ğœÂ¹Âº. On the basis of this result, one then has that
ğ‘†
ğ‘¢Â¸ğ‘Â¸ğ‘
2;ğ‘,ğ‘
=ğœ
ğ‘ 
ğ‘¢Â¸ğ‘Â¸ğ‘
2
 ğœ
ğ‘ 
ğ‘¢Â¸ğ‘Â¸ğ‘
2
=ğœğ‘ ğ‘
2 ğ‘¢
 ğœ
 ğ‘ ğ‘
2 ğ‘¢
=ğœğ‘ ğ‘
2 ğ‘¢
 1Â¸ğœğ‘ ğ‘
2Â¸ğ‘¢
,(32)
which implies that
ğ‘†
ğ‘¢Â¸ğ‘Â¸ğ‘
2;ğ‘,ğ‘
=ğ‘†
 ğ‘¢Â¸ğ‘Â¸ğ‘
2;ğ‘,ğ‘
. (33)
Proof of (ii). The above equation ( 32) shows that
ğœ•
ğœ•ğ‘¢ğ‘†
ğ‘¢Â¸ğ‘Â¸ğ‘
2;ğ‘,ğ‘
=ğœ0ğ‘ ğ‘
2Â¸ğ‘¢
 ğœ0ğ‘ ğ‘
2 ğ‘¢
=0,atğ‘¢=0. (34)
Also, one can show that
ğœ•
ğœ•ğ‘¢ğ‘†
ğ‘¢Â¸ğ‘Â¸ğ‘
2;ğ‘,ğ‘
=ğœ0ğ‘ ğ‘
2Â¸ğ‘¢
 ğœ0ğ‘ ğ‘
2 ğ‘¢
=(
ğœ0 jğ‘ ğ‘
2Â¸ğ‘¢j ğœ0 ğ‘ ğ‘
2 ğ‘¢0,forğ‘¢<0,
ğœ0 ğ‘ ğ‘
2Â¸ğ‘¢ ğœ0 jğ‘ ğ‘
2 ğ‘¢j0,forğ‘¢>0.(35)
Here, forğ‘¢<0, we used the fact that ğœ0is even, which implies that ğœ0Â¹ğ‘ ğ‘
2Â¸ğ‘¢Âº=ğœ0Â¹jğ‘ ğ‘
2Â¸ğ‘¢jÂº, andğœ0Â¹ğ‘£Âºis
non-increasing in ğ‘£forğ‘£ >0andğ‘ ğ‘
2 ğ‘¢ >jğ‘ ğ‘
2Â¸ğ‘¢j>0; forğ‘¢ >0, we used the fact that ğœ0is even, which
implies that ğœ0Â¹ğ‘ ğ‘
2 ğ‘¢Âº=ğœ0Â¹jğ‘ ğ‘
2 ğ‘¢jÂº, andğœ0Â¹ğ‘£Âºis non-increasing in ğ‘£forğ‘£ >0andğ‘ ğ‘
2Â¸ğ‘¢>jğ‘ ğ‘
2 ğ‘¢j>0.
Proof of (iii). With change of variables ğ‘¡=ğ‘ ğ‘
2,ğ‘£=ğ‘Â¸ğ‘
2, we introduce a function
ğ‘‡Â¹ğ‘¡;ğ‘¢,ğ‘£Âº=ğ‘†Â¹ğ‘¢;ğ‘£ ğ‘¡,ğ‘£Â¸ğ‘¡Âº=ğœÂ¹ğ‘£ ğ‘¢Â¸ğ‘¡Âº ğœÂ¹ğ‘£ ğ‘¢ ğ‘¡Âº. (36)
For this function, one has that
ğœ•
ğœ•ğ‘¡ğ‘‡Â¹ğ‘¡;ğ‘¢,ğ‘£Âº=ğœ0Â¹ğ‘£ ğ‘¢Â¸ğ‘¡ÂºÂ¸ğœ0Â¹ğ‘£ ğ‘¢ ğ‘¡Âº0, (37)
sinceğœis non-decreasing (i.e., ğœ0Â¹ğ‘¢Âº0for anyğ‘¢). â–¡
Next, we give a proof of Theorem 4.
Proof of Theorem 4.Ifğ‘˜ <ğ‘™ , the convexity shows that
â„“Â¹ğ‘˜,ğ‘—Âºfğ‘˜ ğ‘—g fğ‘˜ Â¹ğ‘—Â¸1Âºg
fğ‘™ ğ‘—g fğ‘˜ Â¹ğ‘—Â¸1Âºgâ„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸fğ‘™ ğ‘—g fğ‘˜ ğ‘—g
fğ‘™ ğ‘—g fğ‘˜ Â¹ğ‘—Â¸1Âºgâ„“Â¹ğ‘™,ğ‘—Âº
=1
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸ğ‘™ ğ‘˜
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘™,ğ‘—Âº,(38)
20Published in Transactions on Machine Learning Research (1/2023)
and that
â„“Â¹ğ‘™,ğ‘—Â¸1Âºfğ‘™ Â¹ğ‘—Â¸1Âºg fğ‘˜ Â¹ğ‘—Â¸1Âºg
fğ‘™ ğ‘—g fğ‘˜ Â¹ğ‘—Â¸1Âºgâ„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸fğ‘™ ğ‘—g fğ‘™ Â¹ğ‘—Â¸1Âºg
fğ‘™ ğ‘—g fğ‘˜ Â¹ğ‘—Â¸1Âºgâ„“Â¹ğ‘™,ğ‘—Âº
=ğ‘™ ğ‘˜
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸1
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘™,ğ‘—Âº.(39)
These inequalities imply that â„“ğ‘˜,ğ‘™is non-positive:
â„“ğ‘˜,ğ‘™Â¹ğ‘—Âº
=fâ„“Â¹ğ‘˜,ğ‘—ÂºÂ¸â„“Â¹ğ‘™,ğ‘—Â¸1Âºg fâ„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸â„“Â¹ğ‘™,ğ‘—Âºg
=fâ„“Â¹ğ‘˜,ğ‘—ÂºÂ¸â„“Â¹ğ‘™,ğ‘—Â¸1Âºg 1
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸ğ‘™ ğ‘˜
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘™,ğ‘—Âº
Â¸ğ‘™ ğ‘˜
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸1
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘™,ğ‘—Âº
=
â„“Â¹ğ‘˜,ğ‘—Âº 1
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸ğ‘™ ğ‘˜
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘™,ğ‘—Âº
Â¸
â„“Â¹ğ‘™,ğ‘—Â¸1Âº ğ‘™ ğ‘˜
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘˜,ğ‘—Â¸1ÂºÂ¸1
ğ‘™ ğ‘˜Â¸1â„“Â¹ğ‘™,ğ‘—Âº
0. (40)
Similarly, one can show that â„“ğ‘˜,ğ‘™is non-negative if ğ‘˜ >ğ‘™ . â–¡
McCullagh (1980, Section 6.1) has proposed the heteroscedastic extension of the cmulative link model ( 10),
ğ‘ƒ2Â¹ğ‘¦;ğœ,ğ‘Â¹xÂº,b,ğ‘ Â¹xÂºÂºâ‰”ğ‘ƒÂ¹ğ‘¦;ğœ,ğ‘Â¹xÂºÂğ‘ Â¹xÂº,bÂğ‘ Â¹xÂºÂº (41)
with the scale model ğ‘ :Rğ‘‘!Â¹0,1Âº, and statistical OR studies, Thompson Jr (1977);Fienberg & Mason
(1979) and Agresti (2010, Section 4.2), have also considered another model
ğ‘ƒ3Â¹ğ‘¦;ğœ,ğ‘Â¹xÂº,bÂºâ‰”ğœÂ¹ğ‘ğ‘¦ ğ‘Â¹xÂºÂºğ‘¦ 1Ã–
ğ‘˜=1f1 ğœÂ¹ğ‘ğ‘˜ 1 ğ‘Â¹xÂºÂºg. (42)
We obtain the following theorem that is similar to Theorem 3and suggests the eï¬ƒciency of the EOT labeling
for statistical methods adopting these other likelihood models:
Theorem 5. Suppose that ğœis non-decreasing and satisï¬es ğœÂ¹ 1Âº=0andğœÂ¹Â¸1Âº=1and that Â¯ğ‘:Rğ‘‘!R,
Â¯b2Rğ¾ 1, and Â¯ğ‘ :Rğ‘‘!Â¹0,1Âº.
(i)arg minğ‘—2Â»ğ¾Â¼Ãğ¾
ğ‘˜=1ğ‘ƒ2Â¹ğ‘˜;ğœ,Â¯ğ‘Â¹xÂº,Â¯b,Â¯ğ‘ Â¹xÂºÂºâ„“Â¹ğ‘—,ğ‘˜Âº=â„thrÂ¹Â¯ğ‘Â¹xÂº;Â¯bÂºifâ„“=â„“ad,ğœÂ¹0Âº=0.5, and Â¯ğ‘1
Â¯ğ‘ğ¾ 1.
(ii) arg minğ‘—2Â»ğ¾Â¼Ãğ¾
ğ‘˜=1ğ‘ƒ3Â¹ğ‘˜;ğœ,Â¯ğ‘Â¹xÂº,Â¯bÂºâ„“Â¹ğ‘—,ğ‘˜Âº=â„thrÂ¹Â¯ğ‘Â¹xÂº;tÂºfor some t2Rğ¾ 1ifâ„“=â„“ad.
Proof of Theorem 5.Proof of (i). The statement (i) of Theorem 5is trivial from the statement (iii) of
Theorem 3.
Proof of (ii). Regarding the LB labeling for the likelihood model ( 42), one has that, with the abbreviations
Â¤ğœğ‘˜â‰”1 ğœÂ¹Â¯ğ‘ğ‘˜ Â¯ğ‘Â¹xÂºÂºforğ‘˜=1,...,ğ¾ ,
ğ‘…ğ‘—Â¹Â¯ğ‘Â¹xÂºÂºâ‰”ğ¾Ã•
ğ‘˜=1ğ‘ƒ3Â¹ğ‘˜;ğœ,Â¯ğ‘Â¹xÂº,Â¯bÂºâ„“adÂ¹ğ‘—,ğ‘˜Âº
=ğ¾Ã•
ğ‘˜=1
Â¹1 Â¤ğœğ‘˜Âºğ‘˜ 1Ã–
ğ‘™=1Â¤ğœğ‘™ 1
jğ‘— ğ‘˜j,
=jğ‘— 1jÂ¹1 Â¤ğœ1ÂºÂ¸jğ‘— 2jÂ¤ğœ1Â¹1 Â¤ğœ2ÂºÂ¸Â¸Â¤ğœ1Â¤ğœğ‘— 2Â¹1 Â¤ğœğ‘— 1Âº
Â¸Â¤ğœ1Â¤ğœğ‘—Â¹1 Â¤ğœğ‘—Â¸1ÂºÂ¸Â¸jğ‘— ğ¾Â¸1jÂ¤ğœ1Â¤ğœğ¾ 2Â¹1 Â¤ğœğ¾ 1ÂºÂ¸jğ‘— ğ¾jÂ¤ğœ1Â¤ğœğ¾ 1
=Â¹ğ‘— 1Âº ğ‘— 1Ã•
ğ‘˜=1ğ‘˜Ã–
ğ‘™=1f1 ğœÂ¹Â¯ğ‘ğ‘™ Â¯ğ‘Â¹xÂºÂºg
Â¸ğ¾ 1Ã•
ğ‘˜=ğ‘—ğ‘˜Ã–
ğ‘™=1f1 ğœÂ¹Â¯ğ‘ğ‘™ Â¯ğ‘Â¹xÂºÂºg
,(43)
21Published in Transactions on Machine Learning Research (1/2023)
for everyğ‘—2Â»ğ¾Â¼. One has that
ğ‘…ğ‘—Â¸1Â¹Â¯ğ‘Â¹xÂºÂº ğ‘…ğ‘—Â¹Â¯ğ‘Â¹xÂºÂº=1 2ğ‘—Ã–
ğ‘™=1f1 ğœÂ¹Â¯ğ‘ğ‘™ Â¯ğ‘Â¹xÂºÂºg, (44)
is non-decreasing in ğ‘—with ï¬xed Â¯ğ‘Â¹xÂº. Therefore, arg minğ‘—2Â»ğ¾Â¼Ãğ¾
ğ‘˜=1ğ‘ƒ3Â¹ğ‘˜;ğœ,Â¯ğ‘Â¹xÂº,Â¯bÂºâ„“adÂ¹ğ‘—,ğ‘˜Âºis the ï¬rst
indexğ‘™such thatğ‘…ğ‘™Â¸1Â¹Â¯ğ‘Â¹xÂºÂº ğ‘…ğ‘™Â¹Â¯ğ‘Â¹xÂºÂº  0, orğ¾ifğ‘…ğ‘™Â¸1Â¹Â¯ğ‘Â¹xÂºÂº ğ‘…ğ‘™Â¹Â¯ğ‘Â¹xÂºÂº>0for allğ‘™=1,...,ğ¾ 1.
Also,ğ‘…ğ‘™Â¸1Â¹Â¯ğ‘Â¹xÂºÂº ğ‘…ğ‘™Â¹Â¯ğ‘Â¹xÂºÂºis non-increasing in Â¯ğ‘Â¹xÂº, for eachğ‘™=1,...,ğ¾ 1. These facts show that
arg minğ‘—2Â»ğ¾Â¼Ãğ¾
ğ‘˜=1ğ‘ƒ3Â¹ğ‘˜;ğœ,Â¯ğ‘Â¹xÂº,Â¯bÂºâ„“adÂ¹ğ‘—,ğ‘˜Âº=â„thrÂ¹Â¯ğ‘Â¹xÂº;tÂºwith the threshold parameters ğ‘¡ğ‘˜,ğ‘˜=1,...,ğ¾ 1
satisfyingğ‘…ğ‘˜Â¸1Â¹ğ‘¡ğ‘˜Âº ğ‘…ğ‘˜Â¹ğ‘¡ğ‘˜Âº=0. â–¡
C Optimality Guarantee of Algorithm for Empirical Optimal Threshold Labeling
Lin & Li (2006) do not describe the optimality guarantee of Algorithm 1in their paper. As a supplement
to their development, we write here the optimality guarantee of Algorithm 1.
Theorem 6. For any task loss â„“:Â»ğ¾Â¼2!Â»0,1Âº, 1DT Â¯ğ‘, and training data Dğ‘›=fÂ¹xğ‘–,ğ‘¦ğ‘–Âºgğ‘›
ğ‘–=1, the threshold
parameters Â¯tobtained by Algorithm 1minimize the empirical task risk for a classiï¬er based on the threshold
labeling: Â¯t2arg mint2Rğ¾ 11
ğ‘›Ãğ‘›
ğ‘–=1â„“Â¹â„thrÂ¹Â¯ğ‘Â¹xğ‘–Âº;tÂº,ğ‘¦ğ‘–Âº.
Proof of Theorem 6.First, we prove â€˜statement( ğ‘—)â€™ that, for each ğ‘˜2Â»ğ¾Â¼,ğ¿ğ‘—,ğ‘˜is the minimum task risk for
a task such that 1DTs fÂ¯ğ‘Â¹xğ‘–ÂºjÂ¯ğ‘Â¹xğ‘–Âº=Â¯ğ‘1g,...,fÂ¯ğ‘Â¹xğ‘–ÂºjÂ¯ğ‘Â¹xğ‘–Âº=Â¯ğ‘ğ‘— 1gare predicted as any of 1,...,ğ‘˜ in a
non-decreasing manner, and 1DTs fÂ¯ğ‘Â¹xğ‘–ÂºjÂ¯ğ‘Â¹xğ‘–Âº=Â¯ğ‘ğ‘—gare predicted as ğ‘˜:
ğ¿ğ‘—,ğ‘˜= min
â„1,...,â„ğ‘—2Â»ğ‘˜Â¼
s.t.â„1â„ğ‘—=ğ‘˜Ã•
ğ‘™2Â»ğ‘—Â¼Ã•
ğ‘¦ğ‘š2Yğ‘™â„“Â¹â„ğ‘™,ğ‘¦ğ‘šÂº (45)
The statement( 1), which is the starting point for mathematical induction, is trivial. Also, according to the
equation,
ğ¿ğ‘—Â¸1,ğ‘˜=min
ğ‘™2Â»ğ‘˜Â¼ğ¿ğ‘—,ğ‘™Â¸Ã•
ğ‘¦ğ‘–2Yğ‘—Â¸1â„“Â¹ğ‘˜,ğ‘¦ğ‘–Âº
=min
ğ‘™2Â»ğ‘˜Â¼
min
â„1,...,â„ğ‘—2Â»ğ‘™Â¼
s.t.â„1â„ğ‘—=ğ‘™Ã•
ğ‘™2Â»ğ‘—Â¼Ã•
ğ‘¦ğ‘š2Yğ‘™â„“Â¹â„ğ‘™,ğ‘¦ğ‘šÂº
Â¸Ã•
ğ‘¦ğ‘–2Yğ‘—Â¸1â„“Â¹ğ‘˜,ğ‘¦ğ‘–Âº
= min
â„1,...,â„ğ‘—Â¸12Â»ğ‘˜Â¼
s.t.â„1â„ğ‘—=ğ‘™â„ğ‘—Â¸1=ğ‘˜Ã•
ğ‘™2Â»ğ‘—Â¸1Â¼Ã•
ğ‘¦ğ‘š2Yğ‘™â„“Â¹â„ğ‘™,ğ‘¦ğ‘šÂºwithğ‘™=arg min
ğ‘™2Â»ğ‘˜Â¼
min
â„1,...,â„ğ‘—2Â»ğ‘™Â¼
s.t.â„1â„ğ‘—=ğ‘™Ã•
ğ‘™2Â»ğ‘—Â¼Ã•
ğ‘¦ğ‘š2Yğ‘™â„“Â¹â„ğ‘™,ğ‘¦ğ‘šÂº
= min
â„1,...,â„ğ‘—Â¸12Â»ğ‘˜Â¼
s.t.â„1â„ğ‘—Â¸1=ğ‘˜Ã•
ğ‘™2Â»ğ‘—Â¸1Â¼Ã•
ğ‘¦ğ‘š2Yğ‘™â„“Â¹â„ğ‘™,ğ‘¦ğ‘šÂº,(46)
one can ï¬nd that the statement( ğ‘—Â¸1) holds with ğ‘—1as well.
The statement( ğ‘) shows that 1DTs fÂ¯ğ‘Â¹xğ‘–ÂºjÂ¯ğ‘Â¹xğ‘–Âº=Â¯ğ‘ğ‘gshould be labeled as minÂ¹arg minğ‘™2Â»ğ¾Â¼ğ¿ğ‘,ğ‘™Âºâ‰”ğ‘€.
Also, for
Â¹Â¯â„1,..., Â¯â„ğ‘Âº2 arg min
â„1,...,â„ğ‘2Â»ğ‘€Â¼
s.t.â„1â„ğ‘=ğ‘€Ã•
ğ‘™2Â»ğ‘Â¼Ã•
ğ‘¦ğ‘š2Yğ‘™â„“Â¹â„ğ‘™,ğ‘¦ğ‘šÂº, (47)
it will also be clear that 1DTs fÂ¯ğ‘Â¹xğ‘–Âº j Â¯ğ‘Â¹xğ‘–Âº=Â¯ğ‘1g,...,fÂ¯ğ‘Â¹xğ‘–Âº j Â¯ğ‘Â¹xğ‘–Âº=Â¯ğ‘ğ‘ 1gshould be labeled as
Â¯â„1,..., Â¯â„ğ‘ 1. The index ğ¼orğ½in Lines 9â€“15tracks Â¯â„ğ‘Â¹=ğ‘€Âº,Â¯â„ğ‘ 1,..., Â¯â„1. Therefore, it can be found that
the obtained threshold parameters are optimal. â–¡
22Published in Transactions on Machine Learning Research (1/2023)
Table 5: Dataset properties, the total sample size ğ‘›tot, and the dimension ğ‘‘of the explanatory variables,
of classes of the target variable, of the benchmark datasets used for the experiments in Appendix D. Note
that AMP originally has 6 missing values and we excluded them.
DIA PYR APR SER TRI WBC CPU AMP BOS STO
ğ‘›tot 43 74 159 167 186 194 209 392 506 950
ğ‘‘ 2 27 15 4 60 32 6 7 13 9
ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ğ‘›tot 4177 7129 8192 8192 8192 8912 8912 8192 8192 9517
ğ‘‘ 8 5 8 8 8 12 21 32 32 6
POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ğ‘›tot 13750 15000 16599 20640 22784 22784 40768 40768 40768
ğ‘‘ 40 48 18 8 8 16 10 10 10
D Additional Experiments with Benchmark Datasets
Purpose Many OR studies use datasets, which are generated by discretizing a real-valued target of bench-
mark datasets commonly used for evaluation in a regression task, in their experiments. For example,
Frank & Hall (2001) applied regression benchmark datasets summarized in https:// www. dcc. fc.up.pt/
~ltorgo/ Regression/ DataSets. html , and experimented with datasets generated via the discretization into
3/5/10 equal-frequency bins. Also, Chu & Ghahramani (2005) tried the discretization into 5/10 equal-length
bins in addition to the 5/10 equal-frequency discretization. Therefore, we performed similar experiments
additionally to enforce our claim, and this section describes results of these experiments.
Settings In the experiments, as regression benchmark datasets, we use 29 datasets, DIA (Diabetes),
PYR (Pyrimidines), APR (Auto Price), SER (Servo), TRI (Triazines), WBC (Wisconsin Breast Can-
cer), CPU (Machine CPU), AMP (Auto MPG), BOS (Boston Housing), STO (Stocks Domain), ABA
(Abalone), AI2 (Delta Ailerons), KRA (Kinematics of Robot Arm), CO1 (Computer Activity (1)), PU1
(Pumadyn Domain (1)), BA1 (Bank Domain (1)), CO2 (Computer Activity (2)), PU2 (Pumadyn Do-
main (2)), BA2 (Bank Domain (2)), EL2 (Delta Elevators), POT (Pole Telecomm), AI1 (Ailerons),
EL1 (Elevators), CAL (California Housing), CE1 (Census (1)), CE2 (Census (2)), 2DP (2D Planes),
FRA (Friedman Artiï¬cial), MVA (MV Artiï¬cial), which are obtainable in https:// www. dcc. fc.up.
pt/~ltorgo/ Regression/ DataSets. html , a researchersâ€™ site of Chu & Ghahramani (2005) (http:// www.
gatsby. ucl. ac.uk/~chuwei/ ordinalregression. html ), our GitHub repository ( https:// github. com/
yamasakiryoya/ OTL); see also Table 5. As the discretization manner, we tried 3/5/10 equal-frequency/length
discretization; we denote these generated datasets, for example, as EF3 and EL10 datasets. We adopted
the same neural network model applied for the RW datasets. All other settings are the same as those in
Section 6.
Results Tables 6â€“23show the mean and standard deviation of the test errors. Table 24summarizes all
the results: the column â€˜SUMâ€™ shows that the EOT labeling was superior to existing other labelings for all
learning methods and tasks, and reinforces our claim regarding the usefulness of the EOT labeling.
23Published in Transactions on Machine Learning Research (1/2023)
Table 6: A counterpart of Table 2regarding MZE for Task-Z and EF3 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.558.163.388.119.150.060.206.076.513.090.596.082.261.062.204.045.250.034.079.017EOT.576.161.372.129.145.065.198.070.518.076.589.080.260.062 .185 .041.248.038.080.023
Hinge-ITMT.534.161.388.107.159.060.198.070.501.089.580.081.264.063.206.048.245.038.079.018
non-orderedST.534.161.388.107.159.060.198.070.501.089.580.081.264.063.206.048.245.038.079.018EOT.568.135.379.110.154.048.208.067.488.088.579.077.254.055 .184 .041.247.038.075.018
Hinge-IT MT,ST .536.153.391.102.152.052.211.070.511.081.583.090.256.065.197.043.244.039.079.017ordered EOT .544.127.365.116.148.052.199.061.510.083.575.082.256.060.190.037.250.036.076.019
Hinge-AT MT,ST .582.155.388.100.152.060.207.068.497.093.595.086.252.063.201.045.243.037.077.017ordered EOT .554.140.363.108.147.053.212.065.505.089.584.099.265.062.193.040.245.037.077.020
Logistic-ITMT.538.167.384.114.158.066.217.060.483.092.587.094.242.063.196.047.241.036.077.018
non-orderedST.538.167.384.114.158.066.217.060.483.092.587.094.242.063.196.047.241.036.077.018EOT.538.171.385.119.146.047.215.047.481.079.568.088.247.061 .178 .039.234.037.081.022
Logistic-IT MT,ST .536.156.400.099.153.068.229.082.475.081.571.092.255.070.195.044.232.038.073.018ordered EOT .550.158.377.095.134.044.219.060.486.082.559.082.252.065.183.033.229.040.074.019
Logistic-ATMT,ST.542.148.397.106.149.063.221.071.487.079.584.085.249.061.191.045.220.032.074.018
orderedLB.580.159.395.104.148.064.218.064.491.081.575.088.245.063.192.038.221.034.075.019EOT.536.167.385.115.136.046.215.063.503.082.580.088.252.065.181.038.227.040.076.016
OLR-NLLMT,ST.552.175.415.104.155.064.214.063.489.089.576.095.250.067.190.040.228.036.073.020
orderedLB.590.136.388.105.146.064.218.067.487.096.591.095.245.071.197.049.223.035.073.020EOT.560.172.392.105.136.049.212.065.486.088.569.103.253.069.177.037.227.039.076.018
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.351.018.350.012.179.008.171.008.321.012.107.008.200.010.167.008.339.011.350.010EOT.351.016.350.011 .176 .009.171.008.323.013.107.008.199.009.166.008.340.010.348.010
Hinge-ITMT.347.015.353.011.157.010.168.008.318.011.102.008.201.011.163.009.340.010.347.011
non-orderedST.347.015.353.011.157.010.168.008.318.011.102.008.201.011.163.009.340.010.347.011EOT.350.017.350.010.156.009.167.009.318.010.102.008.198.010 .160 .009.339.010.347.010
Hinge-IT MT,ST .349.017.354.011.157.010.169.008.321.011.101.008.200.009.159.009.340.010.348.011ordered EOT .349.016 .350 .011.157.009.168.008.318.010.101.008.198.010.158.009.338.010.348.010
Hinge-AT MT,ST .358.017.352.011.156.010.169.008.322.012.101.008.199.010.160.008.341.011.350.010ordered EOT .350 .017.352.011.158.011.168.008.320.010.100.008.198.009.158.009.339.010.347.011
Logistic-ITMT.347.017.348.010.154.009.166.009.319.011.101.007.195.008.174.009.342.011.348.011
non-orderedST.347.017.348.010.154.009.166.009.319.011.101.007.195.008.174.009.342.011.348.011EOT.347.015.347.010.153.008.166.008.319.009.101.008.195.008 .157 .008.340.011.347.010
Logistic-IT MT,ST .349.017.347.010.153.010.167.009.318.010.101.007.196.009.160.009.342.011.349.012ordered EOT .348.016.348.010.152.008.167.008.319.010.100.008.194.009.159.009.339.011.346.011
Logistic-ATMT,ST.348.017.347.010.154.009.167.009.319.010.101.008.197.007.158.009.342.012.349.010
orderedLB.351.017.348.010.154.010.167.008.319.009.101.008.197.009.158.010.342.013.351.011EOT.350.016.349.010.153.010.168.009.319.009.100.008.196.008.156.009.343.015.348.011
OLR-NLLMT,ST.348.017.348.010.152.010.167.009.318.010.101.007.195.008.158.008.341.012.349.011
orderedLB.351.017.347.010.152.009.166.009.318.010.101.007.196.008.158.009.340.011.350.011EOT.349.015.349.010.153.010.167.008.318.011.100.008.194.007.156.009.341.012.347.011
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.315.006.243.007.300.009.258.006.292.007.267.006.134.003.119.003.010.008EOT.315.008.243.008.299.008.257.006.290.007.266.006.134.003.118.003.009.008
Hinge-ITMT.365.015.239.008.298.007.248.006.325.006.301.008.149.008.121.004.007.001
non-orderedST.365.015.239.008.298.007.248.006.325.006.301.008.149.008.121.004.007.001EOT .359 .012.241.008.297.008.247.006 .321 .006.296 .007.137 .003.120 .004.007 .001
Hinge-IT MT,ST .311.008.239.008.298.008.248.007.288.008.266.006.134.003.120.004.007.001ordered EOT .311.007.240.008.297.007.247.007.287.007.265.006.133.003 .119 .004.007 .001
Hinge-AT MT,ST .314.008.239.008.299.008.252.007.290.007.269.007.134.003.120.004.009.008ordered EOT .314.008.239.007.297.007.252.007.288.007 .267 .007.134.003.119.004.008.007
Logistic-ITMT.313.008.235.007.296.008.238.007.282.006.290.006.133.003.121.004.006.001
non-orderedST.313.008.235.007.296.008.238.007.282.006.290.006.133.003.121.004.006.001EOT.311.007.235.008.296.008.237.007.281.007 .286 .006.133.003 .119 .004.005 .001
Logistic-IT MT,ST .313.008.235.008.297.007.239.007.282.006.263.007.133.004.116.004.009.012ordered EOT .311.007.235.008.296.008.238.007.281.007.262.007.133.003.116.004 .008 .012
Logistic-ATMT,ST.312.007.235.008.298.008.241.008.282.006.297.008.133.003.119.004.021.024
orderedLB.312.007.236.008.298.008.241.007.283.006.297.008.133.003.120.004.021.024EOT.310.007.235.007.298.008.240.007.280.006.295.007.133.003.119.004.020.023
OLR-NLLMT,ST.313.007.235.007.298.008.239.007.281.007.290.007.134.003.119.004.017.022
orderedLB.312.007.235.008.297.008.239.007.282.006.290.007.134.003.119.004.017.022EOT.311.007.236.008.298.008.239.007.281.007.288.006.133.003.118.004 .016 .021
24Published in Transactions on Machine Learning Research (1/2023)
Table 7: A counterpart of Table 2regarding MZE for Task-Z and EF5 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.712.146.568.132.397.089.326.079.677.065.757.061.445.072.319.048.347.043.179.025EOT.688.126.547.152.369.068.322.075.685.062.741.062.445.065.318.044.343.042.176.023
Hinge-ITMT.694.155.597.114.418.087.355.110.675.069.751.080.451.087.311.051.349.047.207.039
non-orderedST.696.154.597.114.418.087.355.110.675.069.751.080.451.087.311.051.349.047.207.039EOT.668.127.587.119 .385 .070.338.085.675.064.760.073.448.076.315.051.353.043 .158 .026
Hinge-IT MT,ST .666.153.588.134.403.077.323.078.685.068.751.079.431.075.307.047.333.042.155.026ordered EOT .658.131.571.133.379.083.328.075.679.074.749.061.447.079.315.051.345.044.154.026
Hinge-AT MT,ST .728.159.569.135.397.091.330.088.676.079.780.063.422.078.310.051.340.046.156.025ordered EOT .662.132.557.129.383.081.325.091.685.083 .750 .073.443.078.320.047.345.042.151.025
Logistic-ITMT.690.157.593.127.417.082.366.117.662.078.765.070.447.070.323.047.353.040.164.028
non-orderedST.692.151.593.127.417.082.366.117.662.078.765.070.447.070.323.047.353.040.164.028EOT.668.138.575.112 .385 .075.320 .078.672.080.751.063.454.077.305.049.352.049 .152 .026
Logistic-IT MT,ST .656.142.581.119.405.082.307.082.683.080.761.076.440.069.308.056.345.049.136.024ordered EOT .664.120.577.118.385.086.311.071.668.071.746.076.451.072.310.046.341.048.136.025
Logistic-ATMT,ST .656 .149.547.137.423.085.309.065.677.070 .737 .075.443.083.298.045.330.044.136.022
orderedLB.730.146.565.150.418.085.304.063.671.074.765.065.422.077.305.048.335.051.136.023EOT .660 .131.557.142.399.072.299.070.687.074 .735 .069.455.085.308.041.342.051.139.025
OLR-NLLMT,ST.664.135.569.116.404.089.329.084.663.072 .743 .075.443.081.299.048.331.043.134.022
orderedLB.718.142.585.106.396.088.308.066.652.067.773.061.439.083.303.046.337.042.136.022EOT.684.125.575.129.382.080.308.078.664.075 .740 .068.448.080.309.044.342.044.134.023
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.533.017.489.014.292.011.277.011.483.010.206.010.331.011.271.009.526.012.555.012EOT .517 .017.479 .011.292.013.277.011.482.008.204.011.331.011.268.012 .522 .009.547 .011
Hinge-ITMT.517.016.484.012.279.012.277.011.495.013.205.009.331.011.274.010.520.013.551.011
non-orderedST.517.016.484.012.279.012.277.011.495.013.205.009.331.011.274.010.520.013.551.011EOT.519.018.481.010.277.011.277.013 .485 .010.203.009.329.011 .262 .010.521.011 .545 .011
Hinge-IT MT,ST .521.017.484.012.279.013.275.010.491.010.203.009.329.011.268.011.524.013.554.014ordered EOT .516.016 .480 .010.279.013.276.011 .485 .009.205.009.328.011.265.010.521.012 .546 .011
Hinge-AT MT,ST .531.017.503.013.279.011.276.011.489.010.204.008.330.011.263.010.526.012.554.013ordered EOT .515 .018.478 .010.277.010.276.011 .486 .010.205.010.328.012.263.010 .523 .012.548 .010
Logistic-ITMT.514.017.494.014.281.013.278.013.486.010.204.010.330.010.300.012.523.014.549.013
non-orderedST.514.017.494.014.281.013.278.013.486.010.204.010.330.010.300.012.523.014.549.013EOT.514.019 .482 .011.274 .012.275.011.486.010.203.010.328.012 .268 .010.520.013.546.011
Logistic-IT MT,ST .515.018.493.011.271.010.275.012.484.009.202.009.327.010.268.012.522.016.551.012ordered EOT .514.018 .483 .012.272.011.274.010.485.009.204.010.326.011.268.010.519.011 .546 .011
Logistic-ATMT,ST .521 .017.486.010.274.011.275.011.488.010.203.009.329.011.259.011.524.012.551.011
orderedLB.532.017.493.013.273.011.275.011.489.010.203.009.328.011.258.009.527.011.554.013EOT .521 .016.482 .013.271.011.275.011.491.011.203.009.326.010.258.009.525.012.549.011
OLR-NLLMT,ST .512 .015.485.013.271.012.275.011.486.008.202.010.327.009.260.011.520.014.548.012
orderedLB.526.016.490.012.272.012.275.010.487.009.202.009.328.010.259.011.527.014.553.014EOT .511 .017.479 .012.271.010.274.011.487.009.202.009.325.011.261.011 .521 .012.547.011
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.513.011.397.009.486.010.389.009.502.007.485.008.242.005.220.005.021.004EOT .486 .013.398.008 .479 .008.388.009 .497 .007.479 .007.241 .004.219.005 .017 .005
Hinge-ITMT.474.010.412.009.476.009.448.008.496.007.476.008.245.005.217.005.089.018
non-orderedST.474.010.412.009.476.009.448.008.496.007.476.008.245.005.217.005.089.018EOT.471.013.412.007 .472 .009.448.008 .492 .007.473 .009.241 .005.214 .004.083 .014
Hinge-IT MT,ST .469.008.397.009.474.007.447.008.494.007.474.008.246.004.214.004.015.003ordered EOT .466.011.395.009 .471 .008.447.009.492.008.472.008 .241 .004.213 .004.013 .003
Hinge-AT MT,ST .488.011.412.009.482.010.451.009.505.008.484.008.245.005.216.004.072.022ordered EOT .482 .012.410.008 .476 .009.450.008 .497 .007.477 .007.240 .004.214 .004.065 .020
Logistic-ITMT.469.009.414.008.479.010.449.009.497.008.473.010.244.005.218.006.076.007
non-orderedST.470.010.414.008.479.010.449.009.497.008.473.010.244.005.218.006.076.007EOT .465 .009.412.007.477.010.448.007.496.007 .470 .007.241 .004.211 .005.066 .005
Logistic-IT MT,ST .465.007.395.009.478.009.447.008.497.008.472.009.244.004.212.004.015.003ordered EOT .462 .010.395.008.475.009.447.008.496.007.469.008 .241 .004.211.004 .013 .003
Logistic-ATMT,ST.468.008.412.009 .480 .010.451 .009.501 .007.484 .009.245.004.211.005.086.044
orderedLB.470.011.412.008.483.010.454.008.505.008.488.009.244.005.212.006.087.045EOT .460 .010.412.008 .478 .010.450 .007.499 .007.482 .007.242 .005.210.005 .081 .044
OLR-NLLMT,ST.463.009.411.009.475.010.449.010 .496 .006.472 .008.245.005.212.005.071.036
orderedLB.465.011.410.009.477.008.450.008.501.007.478.008.245.005.211.005.071.036EOT .460 .009.410.008.474.009.448.009 .495 .007.471 .007.241 .004.211.005 .066 .035
25Published in Transactions on Machine Learning Research (1/2023)
Table 8: A counterpart of Table 2regarding MZE for Task-Z and EF10 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.892.096.728.110.647.069.508.119.843.057.875.058.647.072.581.066.537.043.340.035EOT.868.095.713.115 .622 .073.506.081.838.063.859.049.660.068.596.051.543.048.337.036
Hinge-ITMT.920.077.756.103.649.070.568.070.841.059.884.057.673.074.594.067.549.048.372.044
non-orderedST.912.084.756.103.641.076.551.082.841.059.879.050.665.064.594.067.549.048.368.044EOT .866 .091.731.131.639.073.546.084.851.058.876.058.666.062.598.056.561.045 .349 .034
Hinge-IT MT,ST .860.094.727.103.636.074.538.098.837.047.868.053.676.065.597.058.553.040.335.041ordered EOT .868.076.731.129.641.082 .504 .086.839.063.860.045.656.067.597.058.563.047.329.040
Hinge-AT MT,ST .894.114.725.118.632.073.506.104.823.065.877.053.643.073.574.072.557.051.319.033ordered EOT .872.106.727.122.637.072.495.095.831.061.868.055.656.077.586.066.561.044.316.035
Logistic-ITMT.906.099.737.122.653.086.559.104.828.059.882.041.661.083.601.063.565.046.355.048
non-orderedST.908.093.732.119.647.079.555.100.824.057.880.038.658.081.601.063.565.046.353.045EOT .868 .101.752.106.641.082.536.099.827.059.871.045.656.069.594.062.564.044.343.039
Logistic-IT MT,ST .856.098.727.130.635.073.552.103.826.060.869.048.659.071.593.065 .556 .043.315.032ordered EOT .872.083.720.145.636.076.535.095.823.068.863.068.658.069.595.057.573.043.320.038
Logistic-ATMT,ST.866.099.739.119.626.089.505.080.836.063.867.053.669.076.585.055.574.052.301.031
orderedLB.878.090.719.110.655.077.491.088.821.078.888.046.636.074.585.070 .552 .045.301.037EOT.862.106.732.128.639.080.492.073.831.056.868.054.651.063.590.065.575.050.311.038
OLR-NLLMT,ST.878.106.751.105.631.071.495.082.827.060.869.058.680.069.577.059.567.046.309.037
orderedLB.910.078.717.122.637.077.492.084.826.057.884.048.651.071.573.061.553.048.308.035EOT.866.097.736.118.635.078.476.071.829.069.867.052.659.076.585.057.558.053.303.031
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.736.014.701.013.494.013.468.013.721.010.403.014.528.011.465.017.743.011.764.011EOT .722 .017.686 .012.493.015.464.011 .711 .008.405.013.527.012 .458 .016.741.010 .736 .010
Hinge-ITMT.720.016.702.013.492.014.475.012.713.009.404.014.534.011.457.011.742.010.751.012
non-orderedST.719.016.702.013.492.014.475.012.713.009.404.014.534.011.457.011.742.010.751.012EOT.721.015 .690 .013.486 .014.471.012 .711 .009.400.011.533.013.456.010.742.010.736.008
Hinge-IT MT,ST .719.017.695.011.490.015.467.010.719.011.399.013.529.011.458.015.741.009.742.011ordered EOT .724.019 .691 .012.490.013.467.012 .709 .009.401.011.531.013.458.012.741.011 .734 .008
Hinge-AT MT,ST .732.013.716.013.487.015.476.013.725.011.401.015.532.013.449.014.748.011.782.012ordered EOT .722 .015.686 .013.488.013.473.012 .715 .009.400.012.530.011.446.013 .743 .010.739 .010
Logistic-ITMT.722.014.711.011.487.012.478.012.714.009.404.012.536.012.459.016.743.011.752.013
non-orderedST.722.014.711.011.487.012.478.012.714.009.404.012.536.012.459.016.743.011.752.013EOT.721.016 .686 .012.485.015 .472 .011.710 .010.400 .011.535.011.456.013 .740 .011.738 .009
Logistic-IT MT,ST .717 .016.701.011.494.014.471.013.709.009.397.013.532.012.464.013.742.012.744.009ordered EOT .725.016 .692 .012.490.014.467.012.708.009.399.013.535.011.464.012.741.010 .735 .010
Logistic-ATMT,ST .720 .014.696.011.489.014.477.012 .711 .008.397.011 .531 .009.451.014 .743 .014.743.009
orderedLB.734.015.712.013.492.014.480.012.720.008.398.011.538.013.450.014.750.013.767.010EOT .723 .015.687 .012.489.015.473.011.715.010.399.011 .532 .011.447.014 .743 .010.738 .009
OLR-NLLMT,ST .718 .016.699.013.482.015.465.011 .709 .008.398.013.527.012.450.012 .737 .010.739 .009
orderedLB.734.015.708.012.486.014.469.012.717.011.397.013.527.010.449.014.747.012.764.011EOT .721 .017.687 .012.483.013.466.014 .710 .010.398.012.524.010.447.012 .742 .011.738 .009
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.662.010.612.010.676.008.598.009.685.007.665.008.447.006.423.007.189.069EOT .652 .007.609 .009.655 .008.596.009 .678 .008.659 .007.446.005.420.006 .155 .059
Hinge-ITMT.682.010.629.008.661.008.656.009.719.007.705.007.451.005.420.005.300.072
non-orderedST.682.010.629.008.661.008.656.009.719.007.705.007.451.005.420.005.300.072EOT .675 .010.627.007 .647 .009.652.008 .711 .006.698 .006.447 .005.416 .006.220 .068
Hinge-IT MT,ST .643.007.612.010.650.009.655.008.721.007.710.009.452.006.418.006.140.059ordered EOT .643.009.614.009.649.009.654.008 .715 .007.703 .005.447 .005.416 .006.115 .040
Hinge-AT MT,ST .650.009.637.009.675.008.667.008.721.007.711.008.450.006.417.006.179.069ordered EOT .646.009 .629 .009.656 .008.655 .008.717 .007.704 .008.446 .005.414 .005.138 .033
Logistic-ITMT.689.010.631.008.670.009.664.008.720.006.709.010.448.006.424.007.222.070
non-orderedST.688.010.631.008.670.009.664.008.720.006.709.010.448.006.424.007.222.070EOT .673 .010.627 .006.648 .008.653 .008.712 .006.700 .006.447 .005.417 .006.182 .058
Logistic-IT MT,ST .647.008.614.009.656.010.607.008.687.008.661.008.448.006.418.007.128.032ordered EOT .644 .009.614.009.654.010.604.009.686.007.660.007.447.005 .415 .006.114 .030
Logistic-ATMT,ST.655.009.634.008.660.010 .658 .008.720 .007.707 .010.449.006.418.006.187.056
orderedLB.654.009.638.011.676.009.668.008.723.007.714.007.449.006.417.006.194.064EOT .649 .009.629 .008.657 .008.656 .007.719 .006.706 .007.446 .005.415.005 .156 .045
OLR-NLLMT,ST.675.009 .628 .008.647 .009.651 .008.714 .007.695 .007.448.006.416.005.164.068
orderedLB.688.009.634.009.670.008.664.009.719.007.703.007.448.006.416.005.166.069EOT .666 .009.628 .008.648 .007.652 .008.714 .006.695 .006.447.005.415.005 .145 .059
26Published in Transactions on Machine Learning Research (1/2023)
Table 9: A counterpart of Table 2regarding MZE for Task-Z and EL3 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT .330 .140.183.121.139.065.070.038.326.076.527.076.046.033.199.041.222.053.041.017EOT.386.144.175.101.122.060.067.042.319.074.523.085.045.033 .177 .042.200 .039.043.016
Hinge-ITMT.346.147.185.123.147.057.067.048.312.071.510.062.047.034.190.047.197.041.032.012
non-orderedST.346.147.185.123.147.057.067.048.312.071.510.062.047.034.190.047.197.041.032.012EOT.380.144.181.094 .122 .048.068.042.336.082.523.068.042.032 .167 .043.189.034.036.012
Hinge-IT MT,ST .324 .156.180.105.144.051.070.040.311.066.516.063.048.034.195.040.198.041 .031 .012ordered EOT .382.149.176.110 .119 .055.071.044.330.078.527.074.045.031 .165 .040.192.036.036.013
Hinge-AT MT,ST .340.156.172.098.147.059.077.043.305.072.526.085.048.034.191.043.207.051 .030 .014ordered EOT .376.148.179.094 .119 .048.074.040.318.065.541.094.044.034 .163 .042.201.044.034.011
Logistic-ITMT.390.145.209.112.142.065.065.042.319.071.547.065.052.029.168.041.185.046.029.011
non-orderedST.390.145.209.112.142.065.065.042.319.071.547.065.052.029.168.041.185.046.029.011EOT.404.152.187.106 .120 .059.069.043.319.073.544.083.048.031.160.043.182.045.030.011
Logistic-IT MT,ST .392.140.215.128.142.055.063.041.313.055.522.078.053.029.166.038.162.034.029.011ordered EOT .426.137.203.114 .119 .054.073.045.311.071.548.070.052.029.165.042.173.042.031.012
Logistic-ATMT,ST.438.143.211.121.148.052.069.040.308.083.525.081.055.029.162.034.167.039.030.009
orderedLB.408.128.205.115.143.056.069.039.304.084.540.082.055.030.167.043.171.039.030.009EOT.408.141.184.106 .119 .044.068.041.307.075.544.070.052.030.160.040.178.039.030.010
OLR-NLLMT,ST.426.148.204.128.140.059.067.044.320.073.528.076.054.029.166.037.166.033.031.013
orderedLB.396.134.200.119.135.057.068.044.319.071.537.074.054.030.166.038.174.046.031.012EOT.418.134.195.112.115.044.072.045.317.080.535.080.051.030.164.039.182.044.030.010
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.237.013.030.004.198.011.012.004.276.010.065.010.015.003.126.008.088.007.062.004EOT.236.012.030.005 .193 .014.011.003.276.010 .055 .005.015.003 .120 .007.082 .005.061.004
Hinge-ITMT.236.014.030.004.150.010.010.002.270.010.056.007.013.003 .110 .009.085.006.062.004
non-orderedST.236.014.030.004.150.010.010.002.270.010.056.007.013.003 .110 .009.085.006.062.004EOT.235.013.030.004.149.011.011.003.269.011.055.006.013.003.113.010.083.005.061.004
Hinge-IT MT,ST .236.014.030.004.147.012.011.002.269.009.054.006.013.002 .110 .008.085.005.062.004ordered EOT .235.012.030.004.145.011.011.002.269.010.054.006.013.003.114.008.084.006.061.004
Hinge-AT MT,ST .235.014.030.004.144.009.010.003.269.010.055.006.013.002.115.010.084.005.062.004ordered EOT .235.012.030.004.143.010.010.003.268.011.054.005.013.003.117.010 .083 .006.061.004
Logistic-ITMT.232.016.029.004.138.008.011.002.265.009.053.005.014.003.098.008.083.007.061.004
non-orderedST.232.016.029.004.138.008.011.002.265.009.053.005.014.003.098.008.083.007.061.004EOT.230.013.030.004.138.009.011.003.267.010.054.005.013.002.098.008.083.006.061.004
Logistic-IT MT,ST .230.012.029.004.133.008.011.002.267.009.053.004.013.003.092.007.083.006.061.004ordered EOT .229.012.030.005.131.009.010.002.265.009.054.005.013.002.091.007.083.006.061.004
Logistic-ATMT,ST.232.016.029.004.133.009.011.002.265.010.053.005.014.003.092.006.084.007.061.004
orderedLB.230.014.029.004.133.009.011.002.265.010.053.005.014.003.092.006.084.006.061.004EOT.229.014.030.004.131.008.010.002.264.010.054.005.014.003.092.006.083.007.061.004
OLR-NLLMT,ST.230.013.029.004.132.007.010.002.267.009.053.005.013.002.093.007.084.006.061.004
orderedLB.230.013.029.004.132.007.011.003.267.010.054.005.013.002.093.007.084.007.061.004EOT.230.014.030.004.132.008.011.002.265.010.053.005.013.003.093.007.083.007.061.004
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.061.004.068.005.019.002.224.005.033.002.039.003.094.003.125.011.002.001EOT.060.004.069.004 .018 .002.224.006.033.002.039.003.095.003.123.008.001.000
Hinge-ITMT.070.004.068.005.019.002.214.006.031.002.039.002.095.003.094.007.002.001
non-orderedST.070.004.068.005.019.002.214.006.031.002.039.002.095.003.094.007.002.001EOT .061 .004.068.005.018.002.213.006.031.002.039.003.095.003 .092 .006.001 .000
Hinge-IT MT,ST .071.003.069.005.019.002.214.006.032.003.038.003.095.003.091.005.002.001ordered EOT .062 .004.068.005 .018 .002.212.006.031.002 .035 .002.095.003.090.006.002.001
Hinge-AT MT,ST .068.003.068.005.019.002.218.007.032.002.034.002.094.003.091.005.002.001ordered EOT .062.004.069.004 .018 .002.217.007.032.002.035.003.095.003.090.005 .001 .000
Logistic-ITMT.049.004.071.004.019.002.200.006.031.002.038.003.095.003.084.003.002.001
non-orderedST.049.004.071.004.019.002.200.006.031.002.038.003.095.003.084.003.002.001EOT .036 .004.069.005 .018 .002.199.006.032.002.038.003.095.003.083.003 .001 .000
Logistic-IT MT,ST .050.004.070.005.018.002.200.006.032.002.035.003.094.003.081.004.002.000ordered EOT .035 .004.069.005.017.002.199.006.032.002.035.003.095.003.081.003 .001 .000
Logistic-ATMT,ST.041.004.070.005.019.002.200.005.031.002.035.003.095.003.082.003.002.000
orderedLB.038.004.069.005.019.002.201.005.032.002.035.002.095.003.082.003.002.000EOT .034 .003.069.004 .018 .002.199.005.032.002.035.002.095.003.081.004 .001 .000
OLR-NLLMT,ST.038.004.069.005.019.002.201.006.032.002.038.003.095.003.082.003.002.000
orderedLB.037.004.070.005.019.002.200.005.032.002.037.003.095.003.082.003.002.000EOT .034 .004.069.005 .018 .002.199.006.031.002.037.003.095.003.081.003 .001 .000
27Published in Transactions on Machine Learning Research (1/2023)
Table 10: A counterpart of Table 2regarding MZE for Task-Z and EL5 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.582.113.463.137.172.066.114.055.528.071.714.070.087.040.265.050.268.046.169.025EOT.566.138.483.128.191.070.118.060.549.083.694.073.090.044.261.052.264.049.167.031
Hinge-ITMT.518.179.468.171.183.072.119.056.529.073.707.069.092.046.279.051.288.040.209.034
non-orderedST.518.179.468.171.185.074.119.055.530.073.706.069.099.050.279.051.288.040.209.034EOT.554.142.476.142.195.064.124.050.532.069.702.074.097.047 .252 .043.268 .040.164 .024
Hinge-IT MT,ST .542.170.471.130.176.077.130.050.530.079.704.070.098.049.266.044.260.040.147.024ordered EOT .548.143.489.124.179.067.129.052.547.078.699.075.095.053 .253 .049.260.041.146.026
Hinge-AT MT,ST .588.142.452.152.168.075.121.050.536.083.740.067.094.048.266.045.285.052.153.030ordered EOT .538.148.481.121.181.070.128.050.553.076 .705 .081.100.048.262.057.272.042.148.024
Logistic-ITMT.532.142.469.133.201.071.126.052.546.079.715.078.105.044.258.045.283.045.145.023
non-orderedST.532.148.469.133.204.074.125.049.546.077.715.078.107.046.258.045.283.045.145.023EOT.532.163.467.119.196.066.124.044.545.079.713.067.106.043.257.054 .264 .039.140.027
Logistic-IT MT,ST .556.151.463.127.193.067.122.052.532.077.706.081.105.041.245.043.255.039.128.021ordered EOT .538.159.471.131.192.068.128.052.537.076.701.076.110.041.247.056.265.044.127.023
Logistic-ATMT,ST .536 .128.457.147.187.065.121.053.551.086.708.065.108.041.248.044.265.033.131.018
orderedLB.592.135.484.131.195.071.120.058.549.073.722.061.107.044.250.052.261.038.130.019EOT .538 .157.472.107.205.071.124.042.557.083.701.076.106.045.253.057.274.040.130.020
OLR-NLLMT,ST.562.135.485.133.189.064.124.051.544.077.714.077.108.043.249.040.257.039.129.021
orderedLB.560.128.481.120.194.074.122.053.547.080.728.077.108.041.250.044.255.039.129.021EOT.534.152.471.120.191.072.121.054.548.080.708.088.110.042.250.054.265.044.129.019
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.249.026.154.009.223.013.052.006.470.011.113.009.069.006.164.009.176.008.174.008EOT .223 .015.140 .009.220.012 .049 .006.469.012 .110 .007.068.006 .160 .010.175.008 .166 .008
Hinge-ITMT.232.012.152.009.233.016.053.005.496.015.111.008.068.007.202.011.178.009.174.008
non-orderedST.232.012.152.009.233.016.053.005.496.015.111.008.068.007.202.011.178.009.174.008EOT .223 .015.138 .009.229.014 .050 .005.471 .012.108 .008.066.006 .164 .011.177.008 .166 .008
Hinge-IT MT,ST .233.014.145.010.220.014.052.006.483.011.111.008.068.006.169.009.179.009.174.008ordered EOT .221 .014.137 .009.217.014.050.006 .470 .012.108 .009.067.006 .161 .009.177.008 .166 .008
Hinge-AT MT,ST .230.014.146.010.228.013.052.005.476.011.111.008.068.007.164.010.177.008.173.008ordered EOT .223 .013.137 .008.223 .012.050 .006.469 .012.109.008 .066 .006.158 .009.177.009 .166 .008
Logistic-ITMT.217.013.148.008.216.013.054.006.474.011.106.008.068.006.209.010.176.009.168.008
non-orderedST.217.013.137.009.216.013.054.005.474.011.106.008.067.006.209.010.176.009.166.008EOT.218.014.138.009.212.012 .049 .005.470 .010.107.007 .064 .006.160 .011.178.010.166.008
Logistic-IT MT,ST .216.013.137.010.212.014.053.005.473.011.107.007.065.006.161.012.176.009.167.008ordered EOT .218.014.136.008.210.013 .050 .005.472.012.107.007.063.006.157.010.179.009.166.008
Logistic-ATMT,ST.215.014.136.008.217.014.053.005.474.012.108.007.064.007.159.012.176.008.166.009
orderedLB.216.013.137.008.218.014.052.006.474.012.108.007.064.006.160.011.178.008.166.008EOT.216.012.136.009.215.013 .049 .005.474.012.107.008.063.007.157.011.177.007.165.008
OLR-NLLMT,ST.215.014.137.008.212.014.052.005.471.012.106.007.065.006.160.010.178.010.166.008
orderedLB.215.013.137.010.212.014.051.005.471.010.106.007.064.006.160.010.179.010.166.009EOT.217.012.136.008.208.013.050.005.473.012.106.007.063.006.158.011.179.008.165.008
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.043.004.173.006.045.003.349.008.090.004.104.004.172.006.142.006.004.001EOT .041 .003.170 .006.046.004.347.009.090.004.105.004 .167 .005.140.004 .003 .001
Hinge-ITMT.108.006.173.006.061.007.386.009.092.005.104.004.177.003.163.009.032.005
non-orderedST.107.006.173.006.061.007.386.009.093.005.104.004.177.003.163.009.032.005EOT .095 .007.170 .007.051 .004.375 .007.089 .004.102 .004.177.004 .152 .009.006 .001
Hinge-IT MT,ST .035.003.172.006.050.004.379.008.101.007.104.004.172.006.143.007.004.001ordered EOT .034 .003.169 .006.047 .004.375 .007.089 .004.103 .004.165 .005.137 .004.003 .001
Hinge-AT MT,ST .030.003.172.006.054.005.376.008.091.004.104.004.175.005.146.006.004.001ordered EOT .029.003 .169 .006.049 .004.375.008 .090 .004.098 .004.167 .005.138 .004.003 .001
Logistic-ITMT.083.004.175.007.056.005.382.009.089.004.092.004.172.005.144.005.043.008
non-orderedST.083.004.173.006.056.005.382.009.088.004.093.004.172.005.144.005.043.008EOT .064 .006.169 .007.050 .004.378 .006.088.004.092.004 .163 .003.136 .004.007 .002
Logistic-IT MT,ST .029.003.167.006.046.003.380.007.089.004.092.004.163.004.137.004.003.001ordered EOT .028 .003.166.006.046.003 .377 .008.088.004.092.004.163.004 .135 .004.003 .001
Logistic-ATMT,ST.028.003.172.006.052.004.380.008.089.004.094.004.163.004.137.004.004.001
orderedLB.028.003 .169 .006.052.004.379.009.089.004.094.004.163.004.137.004.004.001EOT.028.002 .169 .008.049 .003.378.008.089.004.094.004.162.003 .135 .003.003 .001
OLR-NLLMT,ST.028.003.172.007.050.003.376.007.088.004.092.004.163.004.136.004.010.002
orderedLB.028.003 .170 .006.049.004.376.008.088.004.091.004.163.004.136.004.010.003EOT.028.004 .168 .007.048 .003.376.008.088.004.091.004.162.004.136.004 .007 .001
28Published in Transactions on Machine Learning Research (1/2023)
Table 11: A counterpart of Table 2regarding MZE for Task-Z and EL10 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.756.146.617.101.387.069.272.064.711.067.853.059.189.058.473.073 .442 .048.284.029EOT.768.117.632.113.404.071.274.089.707.071 .833 .059.182.056.470.064.460.054.286.025
Hinge-ITMT.712.144.620.113.393.076.315.069.708.074.843.050.190.059.486.066.463.049.376.027
non-orderedST.712.144.623.110.387.076.345.070.708.070.850.050.194.058.490.067.462.050.376.027EOT.738.131.633.103.397.084 .257 .087.720.070.829.065.183.050 .454 .063.447.049 .289 .029
Hinge-IT MT,ST .698.146.599.116.392.082.244.065.713.086.826.064.188.059.488.064.441.044.281.029ordered EOT .744.130.608.113.398.074.263.082.716.074.826.071.184.055 .447 .061.443.053.279.028
Hinge-AT MT,ST .750.143.616.118.395.077.291.079.725.063.852.058.197.058.471.060.435.049.284.028ordered EOT .730.157.612.112.390.075.284.082.710.063.837.067.186.057.461.062.444.042 .273 .026
Logistic-ITMT.742.137.631.111.405.078.296.073.707.066.848.061.197.054.483.069.441.043.306.031
non-orderedST.742.137.629.103.403.075.358.096.706.072.849.055.206.062.485.067.441.042.306.031EOT.752.145.607.098.384.077 .269 .074.711.070.829.066.194.054 .456 .053.441.044 .279 .027
Logistic-IT MT,ST .740.147.616.122.396.069.232.067.705.077.829.048.189.058.465.062.446.051.273.027ordered EOT .744.120.612.117.395.086.239.081.722.069.838.068.188.062.456.053.443.049.273.027
Logistic-ATMT,ST.718.149.593.115.387.079.248.076.704.075 .822 .060.187.053.477.073.444.047.266.024
orderedLB.748.149.611.130.393.072.259.075.703.082.852.061.193.055.465.071.446.056.266.026EOT.718.138.601.105.396.084.245.069.713.078 .823 .069.184.060.456.055.455.046.263.024
OLR-NLLMT,ST.716.141.604.132.378.080.235.070.700.061.835.058.191.055.468.068.436.044.266.027
orderedLB.770.124.615.111.396.091.229.077.701.067.853.050.188.050.466.066.444.043.262.024EOT.748.146.617.101.392.086.229.082.717.064.837.056.189.059.453.055.454.050.264.025
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.434.017.212.011.399.013.166.009.652.011.248.010.212.011.325.014.313.011.294.009EOT.433.015 .205 .011.400.014.166.009.649.013.250.012.209.009.321.013 .307 .011.294.008
Hinge-ITMT.470.016.212.011.407.015.164.007.662.014.249.013.208.011.347.016.305.012.294.009
non-orderedST.471.016.212.011.407.015.164.008.662.014.249.013.208.011.348.014.305.012.294.009EOT .437 .015.203 .011.396 .016.161 .008.655 .011.245.011 .204 .010.327 .014.306.011.295.009
Hinge-IT MT,ST .432.017.212.010.410.016.160.009.670.012.245.011.205.010.340.012.308.011.293.008ordered EOT .432.016 .202 .011.404 .012.159.009 .656 .010.245.011.204.010.338.014 .304 .010.294.009
Hinge-AT MT,ST .431.016.209.012.406.015.162.008.655.012.246.011.206.012.324.013.311.011.294.008ordered EOT .432.014 .201 .011.403.014.162.009.654.012.246.011.205.011.321.011.308.014.294.009
Logistic-ITMT.444.018.202.012.408.014.163.007.652.011.245.012.204.009.351.017.310.012.291.008
non-orderedST.453.020.206.010.397.015.163.007.652.011.245.012.204.010.351.016.310.012.291.008EOT .430 .017.199 .011.391 .016.158 .010.653.011.243.011 .196 .010.325 .014.308.012.293.009
Logistic-IT MT,ST .426 .013.198.011.398.013.158.009.654.013.243.011.196.010.337.013.306.012.291.008ordered EOT .433.016.199.012.396.013.156.007.652.012.242.010.196.011.333.013.309.013.292.009
Logistic-ATMT,ST.429.013.198.010.398.017.161.006.655.011.244.011.200.010.322.012 .306 .011.293.010
orderedLB.433.013.199.011.396.017.160.007.653.012.244.011.198.010.320.012.315.013.294.010EOT.437.016.199.011.394.015 .157 .008.654.012.245.013.197.010.317.013 .308 .012.293.009
OLR-NLLMT,ST.426.013.198.010.392.015.157.007.652.011.242.011.197.011.321.013.307.012.292.009
orderedLB.430.015.199.011.389.015.157.008.652.011.242.011.197.012.320.012.315.012.293.009EOT.434.016.199.013.389.014.157.008.654.012.243.011.197.010.320.011.308.012.293.010
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.141.008.286.007.267.005.539.013.218.007.211.006.318.005.263.005.050.004EOT .130 .006.283 .007.268.005 .528 .009.216.006.210.005 .315 .005.262.005 .041 .004
Hinge-ITMT.119.009.305.011.310.010.587.008.226.007.226.005.344.005.303.007.074.006
non-orderedST.119.009.305.011.310.010.588.008.226.007.226.005.330.006.288.005.066.006EOT .114 .008.286 .007.289 .008.583 .008.222 .006.227.006 .316 .005.269 .005.039 .003
Hinge-IT MT,ST .151.005.285.007.260.005.521.010.227.006.239.018.323.005.269.004.047.005ordered EOT .129 .005.283.007.260.006 .517 .008.225.006 .229 .006.315 .005.268.005 .038 .004
Hinge-AT MT,ST .120.008.284.007.259.006.535.011.235.005.237.007.324.006.270.005.051.006ordered EOT .106 .008.283.007.260.007 .526 .008.223 .006.230 .006.315 .004.268 .005.043 .004
Logistic-ITMT.104.007.292.008.303.009.586.008.225.006.225.006.342.006.304.009.077.005
non-orderedST.104.007.291.008.303.009.586.008.225.006.225.006.329.006.293.008.071.006EOT .097 .008.283 .008.285 .006.582 .008.222 .006.224.006 .315 .004.268 .005.038 .003
Logistic-IT MT,ST .135.007.278.007.258.007.515.008.207.007.226.006.318.004.260.004.038.004ordered EOT .115 .007.277.006.258.007.514.007.207.006.225.006 .316 .005.260.004 .033 .003
Logistic-ATMT,ST.137.006.287.007.281.007.591.007.229.007.234.006.321.004.269.005.052.004
orderedLB.124.007.285.008.277.006.594.008.231.006.232.006.320.005.268.005.043.004EOT.107.007.283.007.272.006.587.007.225.006.231.006.315.005.266.005.037.003
OLR-NLLMT,ST.126.007.285.007.275.007 .584 .007.223 .007.225.006.319.004.268.006.042.004
orderedLB.110.007.285.008.275.007.589.008.226.007.225.007.319.005.267.005.040.003EOT .100 .008.282 .007.272 .006.582 .008.222 .006.223.006 .315 .005.266 .005.034 .003
29Published in Transactions on Machine Learning Research (1/2023)
Table 12: A counterpart of Table 2regarding MAE for Task-A and EF3 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.658.183.404.125.150.060.214.075.631.115.721.105.271.066.209.048.263.037.080.019EOT.668.189.373.135.145.065.226.091.637.109.748.102.268.065 .183 .039.256.037.079.021
Hinge-ITMT.670.219.396.115.159.060.211.079.644.123.730.104.271.067.209.051.255.041.080.018
non-orderedST.670.219.396.115.159.060.211.079.644.123.730.104.271.067.209.051.255.041.080.018EOT.654.184.392.123.154.048.215.077.616.125.711.112.267.057 .184 .041.254.039.075.018
Hinge-IT MT,ST .666.204.401.114.152.052.218.071.648.109.710.109.262.066.199.045.250.041.079.018ordered EOT .656.171.375.135.148.052.204.061.618.109.706.111.264.063.190.037.256.036.076.019
Hinge-AT MT,ST .696.178.396.104.152.060.218.075.621.124.706.113.256.065.203.046.251.039.077.018ordered EOT .666.194.365.109.147.053.210.057.629.119.717.122.267.066.194.040.248.038.077.020
Logistic-ITMT.622.194.416.134.158.066.228.066.625.099.725.123.249.065.197.047.251.040.077.019
non-orderedST.622.194.416.134.158.066.228.066.625.099.725.123.249.065.197.047.251.040.077.019EOT.666.181.391.115.146.047.226.052.604.094.714.112.250.066 .179 .038.241.038.081.022
Logistic-IT MT,ST .636.199.423.114.153.068.237.083.606.108.721.131.263.070.194.043.239.040.074.018ordered EOT .672.180.385.096.134.044.224.059.611.106.721.119.260.066.182.033.238.046.074.018
Logistic-ATMT,ST.638.208.411.115.148.064.223.067.616.102.715.122.256.066.193.040.230.037.075.019
orderedLB.638.208.411.115.148.064.223.067.616.102.715.122.256.066.193.040.230.037.075.019EOT.690.192.381.111.136.046.221.065.616.110.715.122.261.070.182.038.233.042.076.016
OLR-NLLMT,ST.658.188.409.120.146.064.234.083.611.112.713.137.251.071.196.050.230.037.074.021
orderedLB.658.188.409.120.146.064.234.083.611.112.713.137.251.071.196.050.230.037.074.021EOT.678.186.393.101.136.049.217.058.605.104.733.118.264.073 .177 .039.234.042.076.017
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.375.020.360.012.180.008.173.008.332.012.107.008.202.009.167.009.361.013.365.012EOT.378.019.358.010.177.010.173.008.335.013.107.008.202.010.166.008.360.013.365.012
Hinge-ITMT.375.020.369.012.157.010.170.009.330.012.102.008.203.011.163.009.364.013.364.011
non-orderedST.375.020.369.012.157.010.170.009.330.012.102.008.203.011.163.009.364.013.364.011EOT.374.015 .359 .011.157.010.169.009.332.011.102.008.201.010 .160 .009.359 .014.365.011
Hinge-IT MT,ST .376.020.370.013.158.010.170.008.332.012.101.008.202.009.159.009.364.012.363.011ordered EOT .373.017 .358 .011.157.009.170.008.332.011.101.008.200.009.158.009.359.012.363.010
Hinge-AT MT,ST .376.018.364.015.157.010.170.008.332.012.101.008.201.009.160.008.361.013.365.009ordered EOT .376.021 .359 .011.158.011.170.009.332.011.100.008.200.009.158.009.359.013.365.011
Logistic-ITMT.368.019.355.011.154.009.167.009.329.010.101.007.197.008.174.009.364.013.363.012
non-orderedST.368.019.355.011.154.009.167.009.329.010.101.007.197.008.174.009.364.013.363.012EOT.368.018.352.012.153.008.167.009.329.009.101.008.197.008 .157 .008.359.014.364.011
Logistic-IT MT,ST .369.019.355.011.153.010.168.009.329.011.101.007.198.008.160.009.363.014.363.011ordered EOT .371.017.352.011.152.008.168.009.330.010.100.008.196.009.159.009.361.015.364.011
Logistic-ATMT,ST.368.019.353.010.154.010.168.008.328.011.101.008.199.009.158.009.361.014.364.012
orderedLB.368.019.353.010.154.010.168.008.328.011.101.008.199.009.158.009.361.014.364.012EOT.370.018.353.010.153.010.168.009.329.010.100.008.197.008.156.009.360.014.363.011
OLR-NLLMT,ST.367.017.353.012.152.009.167.008.329.011.101.007.197.008.158.009.361.013.363.012
orderedLB.367.017.353.012.152.009.167.008.329.011.101.007.197.008.158.009.361.013.363.012EOT.369.018.353.010.153.010.169.009.330.009.100.008.197.008.157.009.360.015.363.011
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.316.007.246.007.311.008.268.006.308.008.283.007.134.003.119.003.010.008EOT.316.009.246.008.311.009.268.007.308.008.282.006.134.003.118.003.009.008
Hinge-ITMT.393.026.242.009.311.008.258.007.350.008.324.009.149.008.121.004.007.001
non-orderedST.393.026.242.009.311.008.258.007.350.008.324.009.149.008.121.004.007.001EOT .363 .014.243.008.311.008.257.008 .338 .007.314 .008.137 .003.120 .004.007 .001
Hinge-IT MT,ST .312.007.242.008.311.008.258.008.303.008.282.006.134.003.120.004.007.001ordered EOT .312.008.242.008.310.008.256.007.303.008.280.006.133.003 .119 .004.007 .001
Hinge-AT MT,ST .314.008.242.008.311.008.260.007.302.007.282.007.134.003.120.004.009.008ordered EOT .314.008.241.008.309.008.259.007.301.008.280.007.134.003.119.004.008.007
Logistic-ITMT.313.009.238.007.307.008.245.007.293.007.306.007.133.003.121.004.006.001
non-orderedST.313.009.238.007.307.008.245.007.293.007.306.007.133.003.121.004.006.001EOT.312.009.239.008.307.009.244.008.292.007 .299 .007.133.003 .119 .004.005 .001
Logistic-IT MT,ST .313.008.238.008.308.008.246.007.294.007.275.007.133.004.116.004.009.012ordered EOT .312.007.238.008.306.009.244.007.292.007.274.006.133.003.116.004 .008 .012
Logistic-AT MT,ST,LB .313.008.239.008.310.008.247.008.292.007.310.009.133.003.120.004.021.024ordered EOT .311.007.238.009.309.010.246.007.292.006.308.008.133.003.119.004.020.023
OLR-NLL MT,ST,LB .313.007.238.008.309.009.246.008.293.007.304.007.134.003.119.004.017.022ordered EOT .312.008.238.009.308.009.245.007.291.007 .301 .007.133.003.118.004 .016 .021
30Published in Transactions on Machine Learning Research (1/2023)
Table 13: A counterpart of Table 2regarding MAE for Task-A and EF5 datasets
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT 1.146.279.755.246.414.100.423.107 1.049.147 1.225.189.496.093.328.048.394.055.180.024EOT 1.204.251.740.239.401.081.438.152 1.038.166 1.220.195.468 .073.328.044.390.055.176.023
Hinge-ITMT 1.104.321.781.242.437.098.471.156 1.084.171 1.263.219.507.106.320.052.394.057.207.039
non-orderedST 1.090.316.781.242.437.098.471.156 1.084.171 1.259.216.507.106.320.052.394.057.207.039EOT 1.126.278.795.242 .399 .085.424 .133 1.042.176 1.243.192.459 .074.320.049.390.058 .158 .026
Hinge-IT MT,ST 1.092.292.805.252.430.085.425.126 1.095.158 1.287.221.504.113.317.050.379.054.155.026ordered EOT 1.154.298.792.266 .396 .088.415.122 1.045.151 1.238.196.480.086.325.055.385.053.154.027
Hinge-AT MT,ST 1.170.310.749.231.427.103.411.124 1.075.144 1.217.169.471.098.318.052.377.057.157.025ordered EOT 1.152.251.764.264.398.084.406.131 1.035.131 1.178.186.451.085.331.052.387.054.152.027
Logistic-ITMT 1.114.310.813.253.452.097.459.159 1.087.157 1.230.232.495.109.333.050.405.056.165.030
non-orderedST 1.108.301.813.253.452.097.459.159 1.087.157 1.230.232.495.109.333.050.405.056.165.030EOT 1.110.266.759.239 .408 .082.422 .147 1.052.175 1.240.195.478.082.316.053.392.056 .151 .024
Logistic-IT MT,ST 1.092.293.816.235.432.099.379.119 1.084.153 1.240.200.493.094.310.055.378.061.136.025ordered EOT 1.136.256.763.245.404.097.392.156 1.049.144 1.259.213.485.085.314.050.385.053.137.026
Logistic-AT MT,ST,LB 1.118.270.768.257.453.101.361.088 1.075.150 1.188.189.467.087.308.048.365.059.137.024ordered EOT 1.154.265.757.256.420.088.368.118 1.046.148 1.196.180.459.081.318.048.367.053.139.025
OLR-NLL MT,ST,LB 1.092.275.772.210.435.110.380.115 1.073.137 1.205.193.479.088.310.049.367.046.136.022ordered EOT 1.140.262.739.230.407.095.381.149 1.058.148 1.211.179.467.070.318.048.373.054.136.024
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.668.021.559.017.302.012.297.013.589.013.206.010.361.013.274.010.659.017.666.017EOT.662.024.560.014.300.013.296.011.592.014.204.011.362.013.274.013.658.016.665.016
Hinge-ITMT.688.028.560.016.287.012.296.012.623.019.205.009.361.012.280.011.672.018.709.020
non-orderedST.688.028.560.016.287.012.296.012.623.019.205.009.361.012.280.011.672.018.709.020EOT .661 .024.560.016.284.013.292.013 .589 .012.204.010.356.012 .266 .010.656 .018.660 .015
Hinge-IT MT,ST .688.029.559.015.286.014.294.011.617.018.203.009.357.012.273.011.673.020.722.020ordered EOT .662 .024.560.015.286.013.293.012 .590 .014.205.008.354.012.270.011 .658 .017.662 .015
Hinge-AT MT,ST .659.023.565.017.284.010.292.011.591.013.205.009.355.013.267.011.658.018.668.015ordered EOT .658.023 .559 .015.285.011.291.009.588.014.204.009.355.012.267.010.656.018 .660 .016
Logistic-ITMT.678.024.584.017.290.014.294.012.594.013.204.010.357.013.309.013.665.017.683.016
non-orderedST.678.024.584.017.290.014.294.012.594.013.204.010.357.013.309.013.665.017.683.016EOT .659 .024.559 .013.281 .012.293.011.589.014.203.009.354.012 .273 .011.657 .017.661 .016
Logistic-IT MT,ST .679.027.582.015.278.011.291.013.595.013.202.010.353.012.273.014.667.020.686.015ordered EOT .660 .024.561 .013.278.011.290.011 .589 .012.204.010.351.012.273.011 .657 .019.662 .016
Logistic-AT MT,ST,LB .658.023.565.015.278.012.291.012.590.015.203.009.353.012.261.010.656.019.663.016ordered EOT .658.021 .558 .013.279.012.290.010.589.013.204.008.351.011.261.011.656.016.663.015
OLR-NLL MT,ST,LB .655.021.562.016.277.013.291.010.589.013.202.009.353.012.264.013.655.019.662.015ordered EOT .657.021.558.013.278.012.290.010.588.016.202.009 .349 .011.264.012.656.018.663.016
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.577.014.441.012.581.013.450.011.617.010.592.013.243.005.221.005.022.005EOT .549 .013.439.011.582.013.448.011.616.011.592.009.241.005 .219 .005.019 .005
Hinge-ITMT.606.024.464.011.616.015.539.012.634.015.596.012.246.005.217.005.097.018
non-orderedST.606.024.464.011.616.015.539.012.634.015.596.012.246.005.217.005.097.018EOT .528 .015.454 .010.572 .012.527 .009.612 .011.575 .011.242 .004.214 .004.091 .014
Hinge-IT MT,ST .664.025.441.011.617.013.535.010.632.013.589.013.247.004.214.004.015.004ordered EOT .518 .012.437.011.571.012.527.010.610.011.575.011 .241 .004.213.004 .014 .003
Hinge-AT MT,ST .557.019.456.012.577.012.530.010.615.010.579.012.246.005.216.004.073.023ordered EOT .549 .018.454.011.576.012.529.009.613.010.577.011 .241 .004.215.004 .067 .021
Logistic-ITMT.662.020.464.012.617.015.536.012.630.011.583.014.245.005.219.006.078.008
non-orderedST.662.020.464.012.617.015.536.012.630.011.583.014.245.005.219.006.078.008EOT .519 .012.454 .009.576 .013.527 .009.614 .011.568 .011.242 .004.211 .005.068 .005
Logistic-IT MT,ST .660.030.437.011.616.014.533.010.630.012.578.012.244.004.212.004.015.003ordered EOT .515 .013.434.010 .577 .012.526 .010.614 .010.566 .009.242 .004.211.004 .014 .003
Logistic-AT MT,ST,LB .523.013.455.009.581.011.531.009.614.011.581.013.245.005.212.005.088.046ordered EOT .509.011.453.010 .577 .012.531.009.613.011.579.011 .242 .005.211.005 .082 .045
OLR-NLL MT,ST,LB .520.012.454.010.575.012.527.010.612.011.570.011.245.005.212.005.072.037ordered EOT .506 .011.452.010.573.013.526.010.611.010.567.009 .241 .004.211.005 .068 .036
31Published in Transactions on Machine Learning Research (1/2023)
Table 14: A counterpart of Table 2regarding MAE for Task-A and EF10 datasets
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT 2.466.569 1.397.353.852.134.783.216 2.297.347 2.524.323.995.157.721.094.791.097.362.035EOT 2.554.527 1.396.379.820.130.778.187 2.278.356 2.533.311 1.000.149.746.089.805.093.361.038
Hinge-ITMT 2.436.551 1.413.404.875.132.911.232 2.314.351 2.479.315 1.046.173.742.110.826.091.414.055
non-orderedST 2.444.561 1.424.402.872.132.874.238 2.304.354 2.498.316 1.040.165.742.110.826.091.406.049EOT 2.448.537 1.433.394.817 .123.819.195 2.277.325 2.492.329 1.008.148.733.087.814.096 .374 .043
Hinge-IT MT,ST 2.426.534 1.456.346.874.145.830.186 2.328.307 2.566.358 1.085.165.731.089.812.084.353.043ordered EOT 2.542.489 1.468.375.823.133.798.178 2.259.303 2.505.2721.025 .149.739.075.813.087.347.038
Hinge-AT MT,ST 2.406.499 1.427.384.844.149.742.162 2.240.314 2.541.309.989.152 .702 .094.802.097.330.039ordered EOT 2.476.507 1.504.380.827.142.721.161 2.219.320 2.499.336.992.140.731.082.807.092.329.035
Logistic-ITMT 2.356.496 1.456.411.873.152.825.208 2.303.285 2.531.408 1.042.160.737.092.824.092.384.059
non-orderedST 2.368.476 1.451.407.865.149.817.211 2.295.287 2.536.397 1.033.150.737.092.824.092.383.058EOT 2.522.543 1.447.379.818.116.783.190 2.239.319 2.489.377.995.141.743.082.810.099.365.043
Logistic-IT MT,ST 2.330 .5431.453.365.838.151.841.210 2.302.294 2.606.430 1.069.182.732.084.811.088.331.032ordered EOT 2.530.551 1.487.405.827.124.783.198 2.234.239 2.607.381 1.026.146.735.088.813.087.335.041
Logistic-AT MT,ST,LB 2.324 .4731.465.419.837.130.733.171 2.288.270 2.520.350.995.149.719.093.796.106.312.034ordered EOT 2.526.544 1.471.412.838.131.732.187 2.233.275 2.511.323 1.002.139.727.092.799.096.315.037
OLR-NLL MT,ST,LB 2.362.465 1.465.435.842.118.744.195 2.252.290 2.508.318 1.008.145.695.087.797.088.317.034ordered EOT 2.474.621 1.465.408.812.129.720.185 2.206.283 2.548.381 1.004.131.717.095.805.086.314.030
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT 1.375.040 1.149.026.599.025.606.017 1.249.027.424.014.732.022.542.026 1.379.030 1.335.025EOT 1.373.037 1.148.025.598.021.604.015 1.251.028.425.012.730.022 .532 .023 1.380.029 1.333.024
Hinge-ITMT 1.404.044 1.200.032.595.020.615.017 1.272.026.426.013.746.018.541.019 1.384.031 1.377.027
non-orderedST 1.403.045 1.200.032.595.020.615.017 1.272.026.426.013.746.018.541.019 1.384.031 1.377.027EOT 1.382 .0411.152 .024.581 .018.611.0171.248 .024.419 .011.736 .019.532 .017 1.382.0271.334 .023
Hinge-IT MT,ST 1.463.053 1.218.032.593.023.606.016 1.395.041.420.012.741.021.543.020 1.415.034 1.366.026ordered EOT 1.394 .0391.163 .024.590.023.607.0181.253 .025.423.014 .733 .018.539.0201.391 .0301.339 .026
Hinge-AT MT,ST 1.366.038 1.177.026.586.018.596.015 1.258.025.420.013.720.018.521.019 1.387.032 1.365.022ordered EOT 1.375.0391.149 .022.584.020.599.0131.248 .024.420.011.719.020.517.017 1.382.0311.333 .025
Logistic-ITMT 1.397.044 1.226.030.588.022.611.016 1.266.026.424.013.744.021.541.023 1.386.034 1.380.028
non-orderedST 1.397.044 1.226.030.588.022.611.016 1.266.026.424.013.744.021.541.023 1.386.034 1.382.028EOT 1.383.0421.151 .024.578 .021.609.0141.249 .027.420.012.740.020 .528 .017 1.385.0321.333 .025
Logistic-IT MT,ST 1.437.054 1.244.027.596.022.603.017 1.297.025.419.013.736.025.555.018 1.402.035 1.363.027ordered EOT 1.398 .0401.169 .026.591.022.601.0171.249 .026.419.012.735.020.550.0171.387 .0291.335 .024
Logistic-AT MT,ST,LB 1.363.039 1.151.025.590.018.598.014 1.251.024.418.010.722.021.520.018 1.386.029 1.339.027ordered EOT 1.371.039 1.148.024.587.021.598.016 1.250.027.421.013.720.020.516.019 1.385.028 1.333.024
OLR-NLL MT,ST,LB 1.362 .0411.150.024.577.021.594.014 1.245.026.418.013.721.020.522.018 1.383.029 1.334.027ordered EOT 1.374.042 1.143.022.574.022.594.016 1.248.025.418.012.717.020.523.017 1.381.033 1.331.024
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT 1.155.025.899.014 1.196.019.915.020 1.144.016 1.084.015.496.007.458.009.249.087EOT 1.145.022.898.015 1.191.019.913.018 1.142.017 1.082.015.495.006 .454 .007.217 .082
Hinge-ITMT 1.329.041.947.015 1.234.024 1.112.017 1.323.022 1.237.020.502.006.456.006.373.094
non-orderedST 1.329.041.947.015 1.234.024 1.112.017 1.323.022 1.237.020.502.006.456.006.373.094EOT 1.197 .024.942.0151.177 .0211.094 .0151.294 .0191.210 .018.495 .007.450 .005.295 .088
Hinge-IT MT,ST 1.401.055.912.015 1.266.028 1.138.021 1.421.030 1.341.032.504.007.452.007.171.075ordered EOT 1.121 .020.901 .0131.191 .0221.100 .0141.301 .0191.230 .019.495 .006.448 .007.148 .062
Hinge-AT MT,ST 1.125.021.942.017 1.186.021 1.105.015 1.283.020 1.210.019.500.008.449.007.210.081ordered EOT 1.126.020.940.014 1.185.020 1.102.016 1.277.017 1.205.020.494 .006.447.007 .169 .048
Logistic-ITMT 1.447.035.945.014 1.237.022 1.113.017 1.314.022 1.233.021.499.007.461.010.279.105
non-orderedST 1.446.035.945.014 1.237.022 1.113.017 1.314.022 1.233.021.499.007.461.010.279.105EOT 1.189 .029.940.0161.183 .0211.095 .0161.291 .0181.212 .018.495 .006.451 .007.232 .080
Logistic-IT MT,ST 1.279.032.910.015 1.256.025.943.022 1.219.020 1.124.019.496.007.451.008.153.045ordered EOT 1.119 .021.905.0151.214 .022.933 .0201.172 .0181.093 .014.494 .006.448 .007.138 .043
Logistic-AT MT,ST,LB 1.132.020.942.014 1.191.021 1.107.015 1.287.019 1.221.024.498.007.450.007.227.080ordered EOT 1.128.020.939.016 1.188.020 1.103.0151.280 .0201.214.018.494 .006.446 .006.197 .071
OLR-NLL MT,ST,LB 1.198.023.940.015 1.183.022 1.096.016 1.287.018 1.197.019.498.007.448.006.199.087ordered EOT 1.167 .021.938.0171.175 .0201.092.014 1.284.020 1.194.015.495 .006.447 .006.181 .083
32Published in Transactions on Machine Learning Research (1/2023)
Table 15: A counterpart of Table 2regarding MAE for Task-A and EL3 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT .330 .140.184.122.138.065.073.042.352.092.631.105.048.039.201.040.226.054.041.017EOT.382.141.176.103.126.065.069.047.351.084.621.116.046.038 .181 .043.210 .048.043.016
Hinge-ITMT.346.147.193.130.150.062.070.053.342.086.631.093.048.037.193.048.199.041.032.012
non-orderedST.346.147.193.130.150.062.070.053.342.086.631.093.048.037.193.048.199.041.032.012EOT.382.147.189.095 .124 .052.075.047.355.084.612.103.046.038 .168 .042.193.033.036.012
Hinge-IT MT,ST .324 .156.185.111.146.053.073.045.336.079.643.084.050.037.197.041.201.042.031.012ordered EOT .384.155.183.115 .121 .060.072.049.348.078.620.106.049.037 .170 .043.194.036.036.013
Hinge-AT MT,ST .340.156.179.102.147.062.080.046.329.079.623.107.050.036.194.043.207.048 .030 .014ordered EOT .374.151.187.098 .121 .052.080.047.347.073.626.111.047.039 .168 .044.205.047.034.011
Logistic-ITMT.392.145.223.122.151.075.068.045.365.096.649.107.056.037.168.041.186.046.029.011
non-orderedST.392.145.223.122.151.075.068.045.365.096.649.107.056.037.168.041.186.046.029.011EOT.402.154.196.109 .124 .068.078.049.352.087.625.099.053.040.161.044.184.046.030.011
Logistic-IT MT,ST .394.141.228.140.146.063.066.045.349.074.649.097.056.032.168.039.162.035.029.011ordered EOT .428.140.212.123 .122 .061.079.051.345.086.621.105.055.035.162.042.175.045.031.012
Logistic-AT MT,ST,LB .408.128.219.124.146.061.072.043.341.096.626.106.058.035.166.043.169.039.030.009ordered EOT .406.143.199.114 .121 .050.071.043.347.096.620.101.055.036.161.040.176.038.030.010
OLR-NLL MT,ST,LB .398.135.213.129.139.064.071.047.349.096.625.109.058.036.167.039.175.046.031.012ordered EOT .418.137.207.118.119.051.079.051.349.094.612.093.054.036.165.039.183.045.030.010
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.237.013.030.004.198.011.012.004.287.012.065.010.015.003.126.008.092.009.062.004EOT.238.013.030.005 .194 .014.011.003.285.011 .055 .005.015.003 .120 .007.084 .006.061.004
Hinge-ITMT.236.014.030.004.150.010.010.002.273.011.056.007.013.003 .110 .009.088.006.062.004
non-orderedST.236.014.030.004.150.010.010.002.273.011.056.007.013.003 .110 .009.088.006.062.004EOT.236.014.030.004.149.011.011.003.274.011.055.006.013.003.113.010 .085 .006.061.004
Hinge-IT MT,ST .236.015.030.004.147.012.011.002.274.010.054.006.013.002.110.008.087.006.062.004ordered EOT .236.012.030.004.145.011.011.002.274.010.054.006.013.003.114.008.086.006.061.004
Hinge-AT MT,ST .234.011.030.004.144.009.010.003.272.011.055.006.013.002.115.010.087.005.062.004ordered EOT .235.012.030.004.143.010.010.003.273.012.054.005.013.003.117.010 .085 .005.061.004
Logistic-ITMT.232.013.029.004.138.008.011.003.269.010.053.005.014.003.098.008.086.007.061.004
non-orderedST.232.013.029.004.138.008.011.003.269.010.053.005.014.003.098.008.086.007.061.004EOT.231.013.030.004.138.009.011.003.270.009.054.005.013.002.098.008.086.006.061.004
Logistic-IT MT,ST .231.013.029.004.133.008.011.002.269.010.053.004.013.003.092.007.085.006.061.004ordered EOT .229.012.030.005.131.009.010.002.270.010.054.005.013.002.091.007.086.006.061.004
Logistic-AT MT,ST,LB .231.014.029.004.133.009.011.002.269.010.053.005.014.003.092.006.086.006.061.004ordered EOT .229.014.030.004.131.008.010.002.270.009.054.005.014.003.092.006.086.007.061.004
OLR-NLL MT,ST,LB .231.013.029.004.132.007.011.003.271.010.054.005.013.002.093.007.087.007.061.004ordered EOT .231.013.030.004.132.008.011.002.270.009.053.005.013.003.093.007.086.006.061.004
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.077.005.068.004.019.002.229.006.035.003.044.004.094.003.125.011.002.001EOT.075.006.069.005 .018 .002.228.006.035.002.043.004.095.003.123.008.001.000
Hinge-ITMT.087.005.069.005.019.002.219.006.033.002.044.003.095.003.094.007.002.001
non-orderedST.087.005.069.005.019.002.219.006.033.002.044.003.095.003.094.007.002.001EOT .077 .005.069.005.018.002.218.006.033.003.044.003.095.003 .092 .006.001 .000
Hinge-IT MT,ST .089.005.069.005.019.002.219.006.033.003.044.004.095.003.091.005.002.001ordered EOT .077 .005.068.005 .018 .002.218.007.033.003 .038 .003.095.003.090.006.002.001
Hinge-AT MT,ST .085.005.068.005.019.002.222.006.033.002.037.003.094.003.091.005.002.001ordered EOT .077 .005.069.004 .018 .002.222.007.033.003.038.003.095.003.090.005 .001 .000
Logistic-ITMT.052.004.071.004.019.002.204.005.033.002.043.003.095.003.084.003.002.001
non-orderedST.052.004.071.004.019.002.204.005.033.002.043.003.095.003.084.003.002.001EOT .037 .004.069.005 .018 .002.203.006.033.002 .041 .003.095.003.083.003 .001 .000
Logistic-IT MT,ST .054.004.070.005.018.002.204.006.033.002.038.003.094.003.081.004.002.000ordered EOT .038 .005.069.005.017.002.203.006.033.002.038.003.095.003.081.003 .001 .000
Logistic-AT MT,ST,LB .040.005.070.005.019.002.204.005.033.002.037.003.095.003.082.003.002.000ordered EOT .036 .004.069.004 .018 .002.203.005.033.002.038.003.095.003.081.004 .001 .000
OLR-NLL MT,ST,LB .039.004.070.005.019.002.204.006.033.002.041.003.095.003.082.003.002.000ordered EOT .035 .004.069.005 .018 .002.202.006.033.002.041.003.095.003.081.003 .001 .000
33Published in Transactions on Machine Learning Research (1/2023)
Table 16: A counterpart of Table 2regarding MAE for Task-A and EL5 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.698.182.520.155.203.087.152.093.730.139 1.126.158.102.054.269.053.295.049.168.025EOT.654.218.548.167.213.088.150.087.757.140 1.123.170.103.059.268.055.288.048.167.030
Hinge-ITMT.658.270.521.221.216.092.168.090.743.147 1.109.164.114.068.283.054.321.049.208.033
non-orderedST.658.270.512.200.219.092.168.089.739.142 1.111.164.121.073.283.054.321.049.208.033EOT.656.213.532.173.218.088.186.112.742.136 1.123.179.114.060 .259 .050.297 .042.164 .024
Hinge-IT MT,ST .670.219.531.161.210.097.174.086.744.151 1.121.135.119.069.270.047.292.046.147.025ordered EOT .668.219.529.164.205.086.176.083.759.135 1.094.142.112.066.262.049.290.042.145.025
Hinge-AT MT,ST .722.215.500.153.195.095.168.077.732.132 1.115.161.113.063.267.046.305.058.153.030ordered EOT .644.198.527.156.205.092.168.083.749.128 1.133.178.114.060.264.055.295.044.148.025
Logistic-ITMT.672.203.565.214.235.097.175.076.763.145 1.152.204.135.073.267.047.318.048.145.023
non-orderedST.670.210.540.175.236.097.174.075.757.134 1.151.204.139.081.267.047.318.048.145.023EOT.660.188.525.172.219.088.181.088.734.123 1.136.194.131.067.256.052 .288 .040.141.027
Logistic-IT MT,ST .676.202.531.169.237.098.168.081.741.130 1.141.165.134.068.250.045.279.043.129.021ordered EOT .676.201.517.169.216.088.185.104.737.139 1.121.188.130.067.249.053.287.047.127.023
Logistic-AT MT,ST,LB .680.196.528.163.221.093.161.077.747.127 1.114.154.132.070.252.044.279.042.130.019ordered EOT .666.214.531.151.225.087.168.077.763.126 1.119.184.127.066.259.057.291.051.130.020
OLR-NLL MT,ST,LB .658.181.553.167.225.100.162.085.745.140 1.161.161.135.071.254.044.283.042.129.021ordered EOT .652.217.515.165.215.093.168.101.764.120 1.134.194.133.064.253.050.284.042.129.019
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.284.028.157.009.223.012.052.006.521.014.113.009.069.006.164.009.207.009.178.009EOT .241 .019.140 .009.221.012 .050 .006.519.014 .110 .007.067.006 .160 .010.204.009 .168 .008
Hinge-ITMT.268.018.154.010.234.016.054.006.565.020.111.008.069.007.202.011.227.012.178.008
non-orderedST.268.018.154.010.234.016.054.006.565.020.111.008.069.007.202.011.227.012.178.008EOT .238 .015.138 .009.231.014 .051 .005.522 .012.108 .008.067.006 .164 .011.206 .011.167 .008
Hinge-IT MT,ST .268.018.146.010.221.014.052.005.544.013.111.008.069.006.169.009.226.012.177.008ordered EOT .234 .015.137 .009.218.015.051.005 .521 .013.108 .009.067.006 .161 .009.205 .010.167 .008
Hinge-AT MT,ST .260.019.147.010.228.013.053.005.525.013.111.008.068.007.164.010.213.012.176.009ordered EOT .237 .015.137 .009.224 .014.051 .005.519 .013.109.008.066.007 .158 .009.204 .008.167 .008
Logistic-ITMT.236.017.151.009.217.014.054.005.525.015.106.008.069.006.209.010.208.011.171.009
non-orderedST.236.017.137.009.217.014.054.005.525.015.106.008.068.006.209.010.208.011.168.009EOT.233.016.138.009.213.012 .050 .005.520.012.107.007 .064 .006.160 .011.204 .010.167.008
Logistic-IT MT,ST .232.016.137.010.213.014.053.005.525.014.107.007.066.006.161.012.208.010.168.008ordered EOT .232.016.137.008.210.014 .050 .005.522.013.107.007 .063 .006.157.010 .204 .011.167.008
Logistic-AT MT,ST,LB .230.015.137.008.219.015.052.005.524.014.108.007.064.006.160.011.205.011.167.008ordered EOT .231.014.137.009.216.013 .050 .005.522.013.107.008.063.007.157.011.207.012.167.008
OLR-NLL MT,ST,LB .230.016.137.010.212.014.051.005.522.013.106.007.065.007.159.010.205.010.167.009ordered EOT .231.016.137.008.209.014.050.005.521.014.106.007.063.006.158.011.207.013.167.008
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.049.004.174.007.045.003.387.009.106.006.140.006.172.006.142.006.004.001EOT .046 .004.171 .006.046.004.385.009 .105 .006.132 .006.167 .005.140.004 .003 .001
Hinge-ITMT.171.017.175.006.062.007.445.011.111.008.140.006.177.003.163.009.032.005
non-orderedST.170.017.175.006.062.007.445.011.112.007.140.006.177.003.163.009.032.005EOT .150 .020.170 .007.051 .004.421 .008.103 .005.124 .005.177.004 .152 .009.006 .001
Hinge-IT MT,ST .038.004.174.006.051.004.431.009.133.014.140.006.172.006.143.007.004.001ordered EOT .036 .004.170 .006.048 .004.420 .008.103 .005.125 .005.165 .005.137 .004.003 .001
Hinge-AT MT,ST .031.004.173.006.055.005.421.009.106.005.139.006.175.005.146.006.004.001ordered EOT .030.003 .170 .007.049 .004.420.008 .103 .005.118 .005.167 .005.138 .004.003 .001
Logistic-ITMT.100.008.177.008.057.005.442.009.105.006.112.005.172.005.144.005.043.008
non-orderedST.100.008.174.006.057.005.442.009.103.005.112.005.172.005.144.005.043.008EOT .075 .007.169 .007.050 .004.421 .007.102.004 .109 .005.163 .003.136 .004.007 .002
Logistic-IT MT,ST .030.004.167.006.046.003.433.010.104.005.112.005.163.004.137.004.003.001ordered EOT .029 .004.167.006.046.003 .422 .008.102 .005.110 .005.163.004 .135 .004.003 .001
Logistic-AT MT,ST,LB .029.003.171.006.053.004.422.009.102.005.111.005.163.004.137.004.004.001ordered EOT .028.003.169.008 .049 .004.422.008.102.005.111.005 .162 .003.135 .003.003 .001
OLR-NLL MT,ST,LB .029.003.171.007.050.004.420.008.101.005.109.005.163.004.136.004.010.003ordered EOT .028.004.169.007 .048 .003.420.007.101.005.108.005.162.004.136.004 .007 .001
34Published in Transactions on Machine Learning Research (1/2023)
Table 17: A counterpart of Table 2regarding MAE for Task-A and EL10 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT 1.348.369.852.252.523.154.391.150 1.361.224 2.412.289.261.117.558.091.547.069.286.029EOT 1.412.376.888.261.536.149.431.193 1.351.247 2.366.257.248.107.563.087.557.077.289.027
Hinge-ITMT 1.354.416.881.313.553.166.561.170 1.343.216 2.369.370.272.128.579.077.583.077.382.029
non-orderedST 1.354.416.869.295.564.162.523.155 1.336.216 2.378.333.278.131.585.077.582.078.382.029EOT 1.412.374.872.268.513.157 .414 .165 1.354.229 2.349.322.265.116 .542 .079.555 .077.295 .032
Hinge-IT MT,ST 1.376.422.841.271.522.174.428.171 1.343.227 2.380.359.275.121.571.086.553.081.282.031ordered EOT 1.366.391.851.248.515.166.452.168 1.335.260 2.341.298.259.109 .537 .087.550.094.281.030
Hinge-AT MT,ST 1.322.363.844.224.525.147.467.188 1.334.231 2.350.312.287.129.552.083.542.072.285.028ordered EOT 1.358.401.868.255.493.142.456.192 1.339.239 2.365.320.265.111.549.088.555.076 .276 .028
Logistic-ITMT 1.366.407.924.261.578.155.583.200 1.385.230 2.368.359.294.146.559.077.552.073.307.031
non-orderedST 1.352.411.897.241.572.155.538.153 1.378.255 2.354.358.318.142.561.078.551.074.307.031EOT 1.354.396.840.256 .513 .153.526.227 1.351.259 2.394.360.269.113 .535 .084.557.079 .280 .025
Logistic-IT MT,ST 1.350.365.876.264.537.150.378.187 1.383.203 2.383.347.277.135.552.090.557.078.274.027ordered EOT 1.368.364.833.244.510.150.407.208 1.372.231 2.395.361.275.140.536.086.544.080.274.026
Logistic-AT MT,ST,LB 1.300.359.841.254.530.148.426.178 1.349.223 2.298.271.275.129.542.093.546.083.267.027ordered EOT 1.342.372.843.230.514.141.446.207 1.345.215 2.367.289.268.121.540.078.560.074.265.025
OLR-NLL MT,ST,LB 1.362.364.827.287.518.170.368.163 1.356.209 2.377.304.269.130.544.096.544.073.263.025ordered EOT 1.360.389.857.230.486.135.398.214 1.361.219 2.365.349.267.118.529.085.544.066.266.027
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.534.026.228.012.417.017.167.009.994.020.251.010.214.011.327.014.527.024.331.012EOT.534.021 .215 .012.417.017.167.009.995.020.253.013.212.010.322.014 .516 .022.326 .011
Hinge-ITMT.616.029.229.012.435.015.169.010 1.068.030.253.013.214.012.351.015.543.022.332.012
non-orderedST.607.030.229.012.435.016.168.008 1.068.030.253.013.213.012.353.017.543.022.332.012EOT .549 .023.213 .012.417 .020.164 .009 1.000.020.247 .011.207 .010.329 .015.512 .021.326 .011
Hinge-IT MT,ST .582.029.228.011.436.019.161.008 1.130.031.247.012.208.010.342.013.639.026.332.012ordered EOT .550 .021.210 .012.431.020.161.0091.003 .022.247.012.207.011.342.014.511 .020.325 .012
Hinge-AT MT,ST .557.033.224.013.427.018.163.008 1.014.025.249.011.208.012.326.012.524.019.332.011ordered EOT .533 .022.211 .013.427.019.164.010 .992 .023.248.011.207.011.324.012 .510 .019.323 .012
Logistic-ITMT.588.031.213.012.442.019.167.009 1.016.024.247.012.208.010.352.015.521.023.325.012
non-orderedST.562.029.217.012.421.019.165.007 1.016.024.247.012.208.010.355.016.521.023.323.011EOT .536 .024.208 .011.410 .018.160 .010.989 .021.246.011 .198 .010.327 .014.512 .020.321.011
Logistic-IT MT,ST .548.027.206.012.421.018.159.009 1.028.023.245.011.199.011.341.013.547.024.321.011ordered EOT .540 .027.207.011.420.019.158.007 .989 .021.243.011.200.011.336.014 .520 .021.320.011
Logistic-AT MT,ST,LB .529.020.206.011.417.021.161.007.993.022.246.011.201.010.322.012.511.021.320.012ordered EOT .535.026.208.011.416.020.159.008.991.020.245.012.200.010.320.014.511.022.321.011
OLR-NLL MT,ST,LB .526 .020.206.010.408.020.158.008.989.020.244.011.200.012.322.014.513.021.320.011ordered EOT .534.022.206.011.406.019.158.008.990.021.244.011.202.016.323.011.510.021.321.011
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.321.035.300.008.271.006.746.015.284.010.283.010.319.005.263.005.050.004EOT.312.037.298.008.271.005 .741 .014.285.010.283.009 .316 .004.262.005 .041 .004
Hinge-ITMT.211.023.329.014.327.011.867.013.314.011.348.012.348.005.308.007.080.008
non-orderedST.209.022.329.014.327.011.870.011.314.011.348.012.332.006.289.006.067.007EOT .196 .023.300 .008.301 .008.855 .011.300 .011.324 .011.317 .004.269 .005.039 .003
Hinge-IT MT,ST .392.018.300.009.265.005.772.015.330.013.376.038.325.006.269.004.047.005ordered EOT .257 .017.298 .008.264.006.726.012 .307 .011.339 .010.316 .005.268.005 .038 .004
Hinge-AT MT,ST .195.019.300.007.263.006.737.014.312.011.324.010.326.006.270.005.051.006ordered EOT .163 .016.297 .007.264.006.733.014 .295 .010.315 .010.316 .004.268.005 .043 .004
Logistic-ITMT.153.015.311.010.316.011.862.011.302.010.324.011.345.007.309.010.079.007
non-orderedST.153.015.309.009.316.011.864.011.302.010.321.011.331.006.294.008.071.006EOT .137 .013.298 .008.292 .007.853 .012.298 .011.313 .011.316 .004.268 .005.038 .003
Logistic-IT MT,ST .235.019.292.008.262.007.756.013.283.009.342.012.319.004.260.004.038.004ordered EOT .165 .014.292.007.261.007 .723 .010.272 .009.316 .011.317 .005.260.004 .033 .003
Logistic-AT MT,ST,LB .183.014.300.008.284.007.851.011.297.010.313.010.322.005.268.005.043.004ordered EOT .155 .013.297.007 .275 .006.849.010.295.010.310.010 .316 .005.266.005 .037 .003
OLR-NLL MT,ST,LB .151.013.298.007.281.007.848.012.295.010.307.011.321.004.267.005.040.003ordered EOT .135 .014.297.008 .275 .007.846.011.295.010.304.011.316.004 .266 .005.034 .003
35Published in Transactions on Machine Learning Research (1/2023)
Table 18: A counterpart of Table 2regarding RMSE for Task-S and EF3 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.877.174.642.121.380.075.489.098.897.100.935.111.535.076.459.053.533.047.282.036EOT.884.152.622.135.372.085.486.106.889.108.930.111.521.072 .429 .045.532.049.279.037
Hinge-ITMT.887.200.639.118.391.076.470.098.896.095.937.122.528.074.459.059.520.049.282.035
non-orderedST.887.200.639.118.391.076.470.098.896.095.937.122.528.074.459.059.520.049.282.035EOT.888.126.628.106.387.065.463.102.878.112.919.103.524.070 .431 .050.517.046.273.035
Hinge-IT MT,ST .894.190.649.119.382.080.478.099.911.112.918.110.520.071.447.055.515.050.282.036ordered EOT .900.157.613.109.375.085.450.078.884.114.914.097.522.065.434.044.518.044.275.036
Hinge-AT MT,ST .908.165.639.102.381.078.483.103.900.118.915.112.510.074.450.053.517.045.277.036ordered EOT .874.135 .598 .108.377.070.475.087.893.105.931.119.524.075.436.048.513.048.274.036
Logistic-ITMT.843.183.672.141.388.086.491.088.914.097.938.118.504.073.444.058.516.043.278.035
non-orderedST.843.183.672.141.388.086.491.088.914.097.938.118.504.073.444.058.516.043.278.035EOT.871.146.631.109.377.065.484.075 .872 .096.920.115.508.072 .422 .045.502 .046.279.035
Logistic-IT MT,ST .849.178.671.119.379.097.496.085.896.105.920.127.519.077.441.052.502.048.269.035ordered EOT .876.137.636.097.358.075.473.074.878.100.933.117.515.074.425.042.500.055.271.035
Logistic-ATMT,ST.828.130.645.116.369.095.485.084.900.099.918.107.513.075.434.046.491.049.273.037
orderedLB.855.199.654.115.373.093.485.088.903.093.919.119.516.078.441.048.493.045.272.035EOT.880.146.633.102.360.077.476.082.883.107.925.116.517.083.427.046.496.053.275.030
OLR-NLLMT,ST.840.129.643.116.368.092.491.090.892.094.908.099.506.084.429.037.495.049.270.036
orderedLB.867.187.663.129.370.094.498.100.892.099.906.118.510.080.444.056.493.045.270.040EOT.878.146.633.098.359.081.475.072.882.112.912.109.520.085.422.046.493.052.275.032
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.651.021.613.011.426.010.419.010.597.014.326.012.456.010.409.011.630.014.627.012EOT .640 .018.610.011.422.011.419.010.597.012.327.012.456.012.408.010 .624 .014.623.011
Hinge-ITMT.660.022.631.013.397.012.416.011.593.011.319.013.456.012.404.012.640.017.629.013
non-orderedST.660.022.631.013.397.012.416.011.593.011.319.013.456.012.404.012.640.017.629.013EOT .637 .016.610 .010.396.012.415.011.593.012.318.013.454.011 .399 .010.624 .015.622 .010
Hinge-IT MT,ST .655.023.631.014.397.013.416.010.594.011.318.012.455.011.399.011.640.014.626.013ordered EOT .636 .018.611 .010.396.011.415.010.594.011.317.013.454.011.398.012 .624 .014.623.011
Hinge-AT MT,ST .638.018.620.014.395.013.416.011.591.011.318.013.454.010.401.010.628.013.624.011ordered EOT .638.018 .608 .010.398.013.416.011.592.011.316.012.453.010.398.012.623.014.623.010
Logistic-ITMT.643.023.608.011.393.012.413.011.593.010.318.011.448.009.418.011.636.014.625.011
non-orderedST.643.023.608.011.393.012.413.011.593.010.318.011.448.009.418.011.636.014.625.011EOT .634 .016.604 .010.391.011.411.011.591.010.317.012.448.010 .397 .010.623 .014.622.011
Logistic-IT MT,ST .640.020.607.010.391.012.413.011.593.011.318.011.449.010.401.011.635.014.626.011ordered EOT .633 .017.604.010.391.010.412.011.591.009.316.012.446.010.399.011 .623 .013.621 .011
Logistic-ATMT,ST.632.020.604.011.393.012.413.010.588.010.317.012.450.010.397.011.624.014.622.010
orderedLB.632.018.604.010.392.012.413.010.589.012.317.012.449.011.398.012.627.015.624.011EOT.632.015.602.010.392.013.413.012.589.011.316.012.447.009.395.011.623.014.622.010
OLR-NLLMT,ST.632.019.604.012.390.012.413.011.590.010.318.012.448.010.396.011.623.013.621.011
orderedLB.633.018.605.010.390.012.412.010.590.011.317.011.448.009.397.010.627.014.622.011EOT.633.017.603.009.390.012.412.011.591.011.316.013.447.010.395.012.623.015.622.011
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.564.007.501.008.576.009.537.007.583.009.559.007.366.005.345.005.095.024EOT.564.008.500.009.575.008.535.008.580.008.557.007.367.005.344.005.093.024
Hinge-ITMT.671.038.497.009.578.008.526.010.630.009.609.010.386.011.348.006.084.007
non-orderedST.671.038.497.009.578.008.526.010.630.009.609.010.386.011.348.006.084.007EOT .607 .011.497.009 .575 .008.524.010 .604 .008.588 .009.370 .004.346 .006.080 .007
Hinge-IT MT,ST .560.008.497.009.578.008.526.010.576.009.557.008.366.005.347.006.086.008ordered EOT .559.007.497.009 .575 .009.524.009.573.008.555.008.365.004 .345 .006.082 .007
Hinge-AT MT,ST .562.007.496.008.576.010.525.008.571.009.555.008.366.004.347.006.090.025ordered EOT .561.007.496.009.575.009.523.008.569.008.553.009.365.005.345.005.086.024
Logistic-ITMT.561.008.493.008.575.010.507.008.562.008.582.008.365.005.348.006.076.007
non-orderedST.561.008.493.008.575.010.507.008.562.008.582.008.365.005.348.006.076.007EOT.560.008.494.009 .570 .009.505.008 .558 .007.569 .008.365.004 .345 .006.073 .007
Logistic-IT MT,ST .560.007.493.009.575.009.510.009.561.008.547.008.365.005.341.005.086.035ordered EOT .560.007.493.009 .570 .009.506 .008.558 .007.545.007.365.004.341.005 .083 .036
Logistic-ATMT,ST.560.008.494.009.570.009.508.009.558.007.579.007.365.004.345.006.125.075
orderedLB.560.007.493.008.573.009.509.009.558.008.581.009.365.004.346.006.125.075EOT.559.007.493.009.570.008.507.008.557.008 .576 .008.365.004.344.006.122.074
OLR-NLLMT,ST.560.007.493.009.571.007.508.010.557.007.573.008.366.004.346.006.113.068
orderedLB.560.007.493.009.574.010.509.009.559.007.575.008.366.004.345.006.113.068EOT.559.007.493.009.572.008.507.009.556.007.570.008.365.004.344.005 .108 .067
36Published in Transactions on Machine Learning Research (1/2023)
Table 19: A counterpart of Table 2regarding RMSE for Task-S and EF5 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT 1.435.284 1.051.275.664.098.763.133 1.407.196 1.454.181.769.098.594.055.714.085.425.028EOT 1.399.257 1.021.268.657.095.776.131 1.385.215 1.445.147.728 .088.604.049.712.073.419.031
Hinge-ITMT 1.444.291 1.073.272.702.103.825.150 1.457.245 1.539.243.802.104.583.054.702.068.454.043
non-orderedST 1.431.291 1.073.272.702.103.825.150 1.457.245 1.539.243.802.104.583.054.702.068.454.043EOT 1.339.250 1.068.306.662 .090.793.1281.362 .1941.444 .186.716 .089.592.055.695.065 .397 .033
Hinge-IT MT,ST 1.426.289 1.059.298.685.087.826.179 1.467.221 1.532.220.803.118.580.057.687.071.393.035ordered EOT 1.377.269 1.069.304.651 .089.771 .127 1.393.203 1.464.181.736 .089.579.059.693.064.392.035
Hinge-AT MT,ST 1.430.298 1.027.262.691.100.758.147 1.425.192 1.442.160.750.093.580.057.680.068.395.032ordered EOT 1.415.271 1.060.326.666.084.743.133 1.373.175 1.419.185.724.094.590.060.692.062.390.037
Logistic-ITMT 1.420.281 1.089.294.722.102.829.190 1.465.205 1.480.251.779.115.602.061.715.083.406.039
non-orderedST 1.413.277 1.089.294.722.102.829.190 1.465.205 1.480.251.779.115.602.061.715.083.406.039EOT 1.378.256 1.025.252.659 .087.774.1321.391 .2141.458.172.733 .083.580.059.697.064 .390 .033
Logistic-IT MT,ST 1.439.283 1.100.294.692.110.730.161 1.471.198 1.518.217.774.110.573.062.675.074.369.035ordered EOT 1.371.249 1.031.269.663.103.737.137 1.377.1901.441 .173.742.090.585.056.677.061.369.035
Logistic-ATMT,ST 1.339.267 1.008.310.703.087.706.104 1.386.183 1.415.145.730.088.577.056.670.065.369.032
orderedLB 1.374.273 1.089.299.715.102.697.139 1.426.193 1.447.204.746.098.567.054.665.067.369.033EOT 1.421.253 1.054.292.680.100.715.152 1.370.183 1.450.168.716.088.578.058.681.067.373.036
OLR-NLLMT,ST 1.312.247 1.041.315.700.104.714.1291.387 .1931.440.141.738.070.575.054.676.065.369.033
orderedLB 1.385.263 1.028.247.703.112.722.143 1.452.195 1.452.192.755.094.574.056.666.060.368.031EOT 1.400.281 1.012.283.665 .100.737.1351.378 .2091.436.163.729.079.587.054.675.064.368.034
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.982.026.843.018.570.013.579.011.906.017.455.011.647.014.532.014.975.021.945.017EOT .969 .024.835 .016.566.015 .573 .011.894 .017.453.013.648.013.533.014 .967 .019.923 .014
Hinge-ITMT 1.040.036.849.019.549.013.576.012.952.023.453.010.651.014.538.011 1.011.019 1.023.023
non-orderedST 1.040.036.849.019.549.013.576.012.952.023.453.010.651.014.538.011 1.011.019 1.023.023EOT .966 .024.832 .016.545.012 .572 .013.889 .017.452.010 .640 .012.522 .012.965 .017.921 .012
Hinge-IT MT,ST 1.043.039.847.018.549.013.574.011.946.021.451.011.646.014.530.012 1.013.019 1.044.022ordered EOT .964 .025.831 .016.548.013.570.010 .891 .015.453.009 .639 .013.528.013 .964 .017.922 .013
Hinge-AT MT,ST .968.024.837.017.548.012.570.012.901.017.453.010.639.014.524.012.968.022.935.017ordered EOT .961.021.833.016.546.013.569.010 .889 .016.452.010.637.013.524.011.963.019 .920 .012
Logistic-ITMT 1.026.031.883.022.553.014.570.012.913.019.452.011.641.015.570.014.993.022.988.020
non-orderedST 1.026.031.883.022.553.014.570.012.913.019.452.011.641.015.570.014.993.022.988.020EOT .962 .025.828 .016.540 .014.568.012 .889 .017.451.011 .636 .012.529 .012.963 .019.920 .013
Logistic-IT MT,ST 1.030.032.875.018.541.012.567.013.910.018.451.011.638.015.533.016.995.023.986.016ordered EOT .965 .021.829 .014.538.012.565.012 .889 .016.451.011 .633 .012.532.014 .963 .018.919.012
Logistic-ATMT,ST .959 .021.832 .016.542.012.568.013.890.016.452.010.633.013.515.011 .959 .017.920 .013
orderedLB.970.024.840.018.541.014.566.012.894.017.451.010.635.014.518.011.968.019.936.016EOT .959 .024.827 .016.540.012.566.011.889.015.451.009.633.014.516.012 .960 .018.918 .013
OLR-NLLMT,ST .960 .020.832 .015.537.013.565.011.891.016.451.011.634.015.519.012 .962 .017.922 .012
orderedLB.971.027.840.017.538.015.566.011.896.019.450.010.636.014.520.013.970.020.938.016EOT .961 .023.827 .015.536.013.566.010.888.016.451.010.634.013.520.014 .963 .018.920 .014
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.840.015.728.013.883.012.768.013.937.014.921.014.493.005.471.006.158.017EOT .823 .015.721 .011.876 .012.762 .012.929 .012.907 .011.492.005 .469 .006.148 .019
Hinge-ITMT.910.028.759.014.946.017.868.014.983.019.942.016.498.006.467.005.334.028
non-orderedST.910.028.759.014.946.017.868.014.983.019.942.016.498.006.467.005.334.028EOT .800 .015.732 .012.868 .012.841 .010.927 .012.891 .011.492 .005.464 .005.316 .023
Hinge-IT MT,ST .956.020.730.012.959.017.861.013.981.017.934.015.499.005.463.005.131.016ordered EOT .788 .013.719 .012.867 .013.840 .010.926 .012.890 .012.492 .004.462.004 .126 .014
Hinge-AT MT,ST .837.023.736.013.883.012.844.012.929.013.896.015.498.005.465.005.273.035ordered EOT .821 .019.732 .011.869 .013.840 .010.920 .011.885 .010.492 .005.464 .004.262 .034
Logistic-ITMT.955.020.755.014.936.016.859.014.973.016.922.017.497.006.468.007.285.015
non-orderedST.955.020.755.014.936.016.859.014.973.016.922.017.497.006.468.007.285.015EOT .788 .012.734 .011.871 .012.838 .010.923 .011.881 .010.493 .004.460 .005.267 .011
Logistic-IT MT,ST .967.021.723.012.957.017.854.013.974.016.913.015.496.004.461.005.129.013ordered EOT .784 .012.715 .011.870 .012.837 .011.922 .011.877 .010.493 .005.460.004 .123 .012
Logistic-ATMT,ST.791.012 .731 .012.872 .012.841.011 .917 .011.886 .013.496.005.461.006.295.069
orderedLB.790.013.737.013.888.013.843.012.925.014.896.014.496.005.461.006.295.069EOT .776 .011.730 .011.870 .012.840.010 .915 .011.883 .010.493 .005.460.006 .285 .071
OLR-NLLMT,ST.789.012 .730 .011.868 .012.838.011 .922 .012.879 .011.496.005.461.006.269.060
orderedLB.792.015.737.013.884.014.841.012.929.014.889.013.496.005.461.005.268.059EOT .774 .011.732 .012.867 .012.839.011 .920 .013.876 .011.492 .005.460.006 .261 .058
37Published in Transactions on Machine Learning Research (1/2023)
Table 20: A counterpart of Table 2regarding RMSE for Task-S and EF10 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT 2.899.576 1.882.521 1.152.147 1.339.289 2.900.403 2.983.356 1.379.179 1.030.107 1.280.163.640.037EOT 2.922.529 1.872.499 1.102.148 1.327.247 2.816.352 2.945.320 1.378.189 1.055.102 1.287.150.649.050
Hinge-ITMT 2.861.563 1.919.600 1.182.134 1.433.309 2.956.434 2.961.373 1.480.173 1.044.107 1.306.161.707.073
non-orderedST 2.871.569 1.878.548 1.188.135 1.413.328 2.956.433 2.945.381 1.468.162 1.044.107 1.306.161.695.060EOT 2.811.581 1.860.4551.073 .1451.386.349 2.853.426 2.936.3621.396 .1881.067.113 1.263.156.657 .049
Hinge-IT MT,ST 2.977.551 1.961.543 1.170.136 1.354.304 2.941.417 2.934.356 1.531.186 1.045.107 1.285.137.629.050ordered EOT 2.904.544 1.920.5001.099 .1491.345.261 2.876.374 2.923.3661.398 .1651.054.108 1.277.140.624.044
Hinge-AT MT,ST 2.845.521 1.923.563 1.141.171 1.260.243 2.922.352 2.887.318 1.390.1711.014 .1091.224.123.598.042ordered EOT 2.782.535 1.879.522 1.112.159 1.258.2522.791 .3332.910.357 1.358.182 1.052.107 1.220.130.598.044
Logistic-ITMT 2.767.495 2.014.618 1.163.155 1.366.295 2.944.376 2.925.355 1.458.203 1.035.102 1.259.138.678.084
non-orderedST 2.770.499 2.028.623 1.151.161 1.348.300 2.940.372 2.934.403 1.447.199 1.035.102 1.259.138.676.082EOT 2.843.582 1.886.530 1.104.124 1.319.320 2.874.404 2.925.4041.381 .1571.048.102 1.242.120.648.052
Logistic-IT MT,ST 2.853.513 1.961.503 1.158.146 1.390.244 2.972.444 2.947.365 1.503.191 1.031.094 1.262.142.604.038ordered EOT 2.899.568 1.911.5151.107 .1481.317.247 2.849.378 2.937.4031.412 .1741.034.106 1.238.118.597.035
Logistic-ATMT,ST 2.740.477 1.918.562 1.143.133 1.267.259 2.848.331 2.894.342 1.373.175 1.009.096 1.212.119.577.037
orderedLB 2.802.495 1.981.587 1.147.154 1.286.251 2.906.341 2.890.359 1.400.174 1.017.096 1.216.123.584.043EOT 2.889.567 1.931.517 1.108.145 1.266.246 2.789.360 2.911.340 1.369.167 1.033.093 1.218.128.580.038
OLR-NLLMT,ST 2.792.472 1.888.561 1.159.112 1.241.269 2.876.353 2.881.294 1.400.201 1.006.097 1.227.126.587.033
orderedLB 2.812.494 1.963.627 1.150.146 1.231.249 2.896.365 2.888.332 1.413.210 1.002.103 1.233.137.584.032EOT 2.848.567 1.959.546 1.129.157 1.277.271 2.864.404 2.913.352 1.368.168 1.027.103 1.224.129.582.036
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT 1.857.056 1.558.032.926.031.979.019 1.698.034.688.015 1.124.026.842.026 1.865.038 1.738.031EOT 1.838 .0451.535 .028.917 .026.970 .019 1.690.031.688.013 1.119.027.838.028 1.857.0371.723 .027
Hinge-ITMT 1.915.057 1.626.028.917.026.993.021 1.739.033.687.013 1.148.026.854.020 1.877.035 1.766.031
non-orderedST 1.915.057 1.626.028.917.026.993.021 1.739.033.687.013 1.148.026.854.020 1.877.035 1.766.031EOT 1.849 .0481.533 .029.898 .023.975 .0171.685 .033.682 .0121.121 .025.837 .0201.854 .0381.721 .029
Hinge-IT MT,ST 1.973.063 1.641.030.920.030.987.020 1.912.053.683.012 1.145.030.859.023 1.929.040 1.784.031ordered EOT 1.861 .0451.547 .026.906 .026.971 .0201.695 .034.681.0131.117 .026.850 .0221.856 .0361.725 .026
Hinge-AT MT,ST 1.844.057 1.570.028.905.023.951.018 1.709.035.683.013 1.102.027.821.020 1.860.038 1.747.028ordered EOT 1.826.0461.532 .028.904.022.951.0161.685 .035.680.011 1.099.024.819.018 1.851.0361.720 .029
Logistic-ITMT 1.893.063 1.619.029.905.026.981.018 1.726.035.685.014 1.137.030.856.026 1.869.038 1.761.034
non-orderedST 1.893.063 1.619.029.905.026.981.018 1.726.035.685.014 1.137.030.856.026 1.869.038 1.761.034EOT 1.847 .0471.535 .026.890 .029.967 .0171.686 .036.681.0131.118 .025.833 .0181.853 .0351.720 .028
Logistic-IT MT,ST 1.934.063 1.629.029.920.028.975.018 1.787.037.680.014 1.129.031.876.021 1.892.040 1.769.033ordered EOT 1.863 .0441.552 .030.909 .028.960 .0181.688 .032.679.0131.116 .026.860 .0171.856 .0381.723 .027
Logistic-ATMT,ST 1.824 .0481.533 .026.903.021.951.016 1.683.032.680.013 1.095.023.820.020 1.850.0341.721 .027
orderedLB 1.844.051 1.545.028.905.022.949.017 1.693.033.680.012 1.097.025.823.022 1.862.038 1.732.030EOT 1.826 .0441.531 .028.901.024.947.017 1.683.033.680.012 1.094.022.817.019 1.849.0361.720 .029
OLR-NLLMT,ST 1.831.0501.532 .029.893.030.953.018 1.684.033.681.014 1.097.026.826.0191.851 .0341.720 .027
orderedLB 1.848.054 1.549.031.893.030.957.017 1.693.034.681.013 1.102.024.827.020 1.864.039 1.734.031EOT 1.833.0471.529 .027.891.030.949.014 1.682.032.680.013 1.098.025.827.0201.851 .0371.718 .029
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT 1.569.024 1.291.020 1.669.024 1.386.025 1.620.025 1.574.017.770.007.727.008.632.104EOT 1.557 .0211.287.0211.649 .0191.378 .0241.608 .0201.561 .020.766 .006.724 .007.601 .105
Hinge-ITMT 1.813.044 1.348.024 1.723.029 1.611.024 1.865.034 1.758.028.779.007.727.006.739.110
non-orderedST 1.813.044 1.348.024 1.723.029 1.611.024 1.865.034 1.758.028.779.007.727.006.739.110EOT 1.605 .0261.322 .0221.641 .0221.564 .0191.785 .0241.692 .022.767 .006.720 .005.665 .099
Hinge-IT MT,ST 1.868.045 1.314.017 1.740.031 1.657.029 2.000.040 1.904.040.780.008.723.007.487.106ordered EOT 1.533 .0201.286 .0201.653 .0221.566 .0181.783 .0211.709 .024.767 .007.718 .007.464.096
Hinge-AT MT,ST 1.540.020 1.329.022 1.655.024 1.576.021 1.763.024 1.688.024.776.007.719.007.522.094ordered EOT 1.538.0201.316 .0201.639 .0251.565 .0201.746 .0211.668 .023.767 .006.715 .007.480 .065
Logistic-ITMT 1.893.031 1.341.024 1.711.026 1.600.024 1.847.030 1.748.029.775.007.732.010.635.139
non-orderedST 1.892.031 1.341.024 1.711.026 1.600.024 1.847.030 1.748.029.775.007.732.010.635.139EOT 1.597 .0301.319 .0191.641 .0211.564 .0191.778 .0231.686 .025.767 .006.719 .006.576 .102
Logistic-IT MT,ST 1.824.044 1.308.021 1.718.026 1.416.031 1.743.031 1.653.026.771.007.722.009.456.074ordered EOT 1.529 .0191.288 .0191.670 .0221.391 .0241.635 .0221.572 .020.767 .007.716 .007.437 .069
Logistic-ATMT,ST 1.541.0201.318 .0191.643 .0231.568 .0211.751 .0211.677 .023.773.008.719.007.541.101
orderedLB 1.545.021 1.327.021 1.664.024 1.577.023 1.767.025 1.698.026.772.008.720.008.538.101EOT 1.541.0221.316 .0211.638 .0211.563 .0201.747 .0201.669 .022.767 .006.716 .006.514 .100
OLR-NLLMT,ST 1.596 .0231.319 .0211.639 .0231.562 .0211.767 .0221.668 .023.772.007.717.006.515.114
orderedLB 1.620.026 1.328.023 1.667.028 1.572.024 1.782.027 1.687.024.772.007.718.007.511.114EOT 1.575 .0201.316 .0211.634 .0221.560 .0201.766 .0221.665 .022.766 .007.716.007.492.112
38Published in Transactions on Machine Learning Research (1/2023)
Table 21: A counterpart of Table 2regarding RMSE for Task-S and EL3 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.560.126.407.155.362.083.259.108.629.102.859.113.205.104.452.046.481.058.198.040EOT.607.119.400.131.352.096.258.118.634.093.863.111.197.104 .433 .055.468.056.203.038
Hinge-ITMT.574.129.421.180.384.085.248.120.625.100.922.106.199.105.443.058.449.049.176.032
non-orderedST.574.129.421.180.384.085.248.120.625.100.922.106.199.105.443.058.449.049.176.032EOT.608.130.432.138 .347 .082.264.106.640.095 .853 .106.198.115 .416 .053.442.042.186.033
Hinge-IT MT,ST .552 .137.414.156.380.075.263.098.618.092.936.102.207.106.447.048.448.051 .172 .034ordered EOT .610.140.405.167 .344 .094.268.138.633.088 .870 .113.206.114 .418 .056.444.047.186.032
Hinge-AT MT,ST .567.137.407.164.379.086.276.098.610.088.882.119.208.103.445.048.458.054 .167 .044ordered EOT .597.133.430.142 .343 .083.287.103.625.083.861.112.199.114 .410 .060.454.050.183.030
Logistic-ITMT.617.126.473.159.392.113.245.115.664.110.887.112.232.105.408.053.430.055.168.033
non-orderedST.617.126.473.159.392.113.245.115.664.110.887.112.232.105.408.053.430.055.168.033EOT.621.130 .449 .169.348.099.266.114.631.096 .833 .103.218.121.401.057.427.055.170.033
Logistic-IT MT,ST .618.127.473.177.380.094.244.111.647.097.895.106.231.093.409.050.401.044.168.032ordered EOT .646.119.462.183 .345 .096.269.122.632.094 .835 .106.228.104.401.053.415.055.173.033
Logistic-ATMT,ST.604.140.456.171.381.090.263.098.630.098.801.088.232.099.404.050.417.051.169.029
orderedLB.629.109.467.164.380.088.262.099.630.092.864.120.234.098.406.050.410.048.170.028EOT.626.118.446.155 .346 .082.271.111.637.104.837.105.225.103.400.049.419.047.170.030
OLR-NLLMT,ST.618.119.455.161.366.087.256.108.640.108 .792 .083.227.111.407.052.405.045.171.028
orderedLB.623.120.459.172.372.099.254.108.633.103.859.103.236.100.408.050.416.055.172.033EOT.636.113.461.173.344.086.263.122.631.103 .822 .092.224.104.405.049.425.056.170.029
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.488.014.172.012.445.013.107.016.554.015.255.019.120.014.355.011.317.021.248.008EOT.489.014.173.013.440.016.106.014 .539 .012.233 .011.121.014 .346 .010.297 .012.247.008
Hinge-ITMT.486.012.172.012.387.013.101.011.530.012.235.014.115.012 .331 .013.305.012.248.008
non-orderedST.486.012.172.012.387.013.101.011.530.012.235.014.115.012 .331 .013.305.012.248.008EOT.487.015.172.013.385.014.102.012.530.011.233.013.113.011.336.015.301.014.246.007
Hinge-IT MT,ST .485.012.172.012.383.015.102.010.530.011.232.012.114.011 .331 .013.305.011.248.008ordered EOT .486.013.173.012.381.015.102.011.530.012.233.013.114.011.337.012 .299 .013.247.008
Hinge-AT MT,ST .486.012.172.012.379.012.101.013.529.011.234.012.115.010.338.014.304.011.248.008ordered EOT .486.013.172.012.378.013.101.013.530.011.233.011.114.011.342.014 .299 .012.247.008
Logistic-ITMT.483.015.169.012.372.010.108.014.526.010.231.011.117.013.313.013.301.013.246.008
non-orderedST.483.015.169.012.372.010.108.014.526.010.231.011.117.013.313.013.301.013.246.008EOT.481.013.173.012.372.012 .102 .012.526.010.231.010.114.011.313.013.301.012.246.008
Logistic-IT MT,ST .481.013.170.013.364.011.104.012.526.011.230.010.115.011.303.012.300.013.246.008ordered EOT .480.013.172.014.362.012.102.012.527.011.231.011.114.011.301.012.300.012.247.007
Logistic-ATMT,ST.481.014.171.012.364.012.103.013.526.010.231.010.115.012.304.010.300.014.247.008
orderedLB.481.014.170.012.364.012.103.012.526.010.230.010.116.012.304.010.300.013.247.008EOT.478.014.173.013.362.012.101.012.526.009.231.011.116.012.303.010.301.014.247.008
OLR-NLLMT,ST.480.014.170.012.363.010.101.011.527.010.230.011.115.010.305.011.301.013.246.008
orderedLB.481.014.170.012.363.010.104.012.529.012.231.011.115.010.306.011.301.013.246.008EOT.481.014.173.013.363.011.102.011.527.011.230.011.115.011.306.012.300.013.247.007
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.327.015.262.009.138.007.489.006.197.008.234.013.307.005.353.016.040.007EOT.323.015.262.009 .136 .008.488.007.197.009 .224 .011.308.005.351.012.038.006
Hinge-ITMT.351.012.262.009.138.007.478.007.189.007.235.010.308.005.306.010.041.007
non-orderedST.351.012.262.009.138.007.478.007.189.007.235.010.308.005.306.010.041.007EOT .325 .014.262.008.135.009.477.007.189.008 .225 .010.308.005 .303 .010.038 .006
Hinge-IT MT,ST .353.012.262.009.138.008.477.007.191.008.232.013.307.005.302.009.041.007ordered EOT .327 .014.262.009 .134 .007.476.008.189.007 .210 .009.308.004.300.009.039.007
Hinge-AT MT,ST .345.013.262.009.139.008.481.007.190.008.208.009.307.005.302.009.042.006ordered EOT .326 .014.263.008 .135 .008.481.008.190.007.208.008.308.005.300.008 .038 .005
Logistic-ITMT.242.013.266.008.138.008.459.007.189.007.226.010.308.005.290.006.039.007
non-orderedST.242.013.266.008.138.008.459.007.189.007.226.010.308.005.290.006.039.007EOT .202 .013.263.009 .133 .008.459.007.190.008 .217 .009.308.005.289.006 .035 .006
Logistic-IT MT,ST .246.011.265.009.132.008.460.008.189.007.207.009.307.005.285.007.039.006ordered EOT .201 .014.263.009.131.008.459.008.189.007.208.009.308.005.284.006 .036 .006
Logistic-ATMT,ST.203.013.264.009.137.007.459.007.188.007.206.009.308.005.286.006.039.006
orderedLB.210.014.264.009.137.008.458.006.188.008.207.009.308.005.286.006.039.006EOT .199 .012.263.008 .133 .008.458.008.189.007.208.009.308.005.284.006 .037 .006
OLR-NLLMT,ST .199 .013.264.010.137.008.460.007.189.008.216.008.308.005.287.006.040.006
orderedLB.207.013.264.009.137.008.460.006.188.007.217.009.308.005.287.006.040.006EOT .197 .013.263.010 .133 .008.459.007.188.007.216.009.308.004.285.006 .037 .005
39Published in Transactions on Machine Learning Research (1/2023)
Table 22: A counterpart of Table 2regarding RMSE for Task-S and EL5 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT.975.218.798.203.498.168.481.235 1.089.186 1.375.170.343.130.525.054.603.059.408.031EOT .901 .191.820.205.502.152.498.240 1.084.153 1.362.138.343.136.524.059.605.063.408.039
Hinge-ITMT.991.308.789.263.514.168.520.226 1.115.172 1.389.160.371.159.537.057.631.061.455.035
non-orderedST.991.308.775.237.524.168.520.225 1.108.168 1.390.162.386.164.537.057.631.061.455.035EOT .867 .192.814.215.505.158.524.235 1.107.159 1.367.149.365.131 .514 .053.599 .060.405 .028
Hinge-IT MT,ST .965.231.813.218.502.167.524.217 1.093.175 1.408.159.384.157.529.055.600.056.383.032ordered EOT .898.194.811.206.491.164.508.210 1.086.157 1.375.160.354.138.526.060.604.060.381.033
Hinge-AT MT,ST .998.245.787.215.486.178.514.194 1.077.160 1.364.179.364.152.523.051.601.076.390.039ordered EOT .871 .185.802.204.488.165.513.209 1.069.156 1.356.174.363.124.524.051.605.062.384.032
Logistic-ITMT.973.223.856.268.528.160.525.181 1.123.187 1.400.224.424.186.525.050.624.059.380.030
non-orderedST.969.226.825.215.540.163.522.180 1.115.167 1.399.223.433.198.525.050.624.059.380.030EOT .875 .199.808.226.499.153.529.186 1.074.156 1.370.173.400.171.510.052 .581 .062.374.036
Logistic-IT MT,ST .947.189.815.216.541.157.505.190 1.101.160 1.452.212.418.175.506.050.581.065.358.030ordered EOT .903.196.798.219.501.153.511.183 1.085.151 1.387.187.401.167.512.060.586.065.356.033
Logistic-ATMT,ST.920.227.812.214.492.160.484.182 1.085.146 1.362.168.399.160.517.051.573.057.359.027
orderedLB.929.211.819.232.510.161.481.176 1.090.152 1.365.163.408.171.513.051.579.064.360.027EOT.883.211.812.216.498.142.496.201 1.080.148 1.392.196.384.166.515.053.586.065.359.027
OLR-NLLMT,ST.934.174.821.214.512.159.476.184 1.078.163 1.369.185.421.179.509.045.579.061.356.031
orderedLB.931.233.837.224.515.162.491.199 1.108.163 1.399.201.420.179.514.048.584.060.358.030EOT.883.191.800.225.490.151.502.207 1.084.158 1.393.190.403.156.517.053.588.066.359.028
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.596.032.404.012.474.013.228.013.791.016.336.013.262.012.405.011.522.018.430.011EOT .524 .023.376 .012.471.013 .224 .013.785 .013.331 .011.260.012 .400 .012.505 .016.413 .010
Hinge-ITMT.583.027.398.014.486.017.238.015.843.020.333.012.264.013.450.013.580.020.430.011
non-orderedST.583.027.398.014.486.017.235.013.843.020.333.012.263.012.450.013.580.020.430.011EOT .521 .021.371 .012.483.015 .228 .014.785 .013.329 .012.261.011 .405 .013.507 .016.412 .010
Hinge-IT MT,ST .583.027.385.014.471.016.230.012.820.015.333.012.263.011.411.011.574.023.429.011ordered EOT .512 .022.371 .012.468.016.227.012 .786 .013.329 .013.261.011 .401 .011.501 .017.412 .010
Hinge-AT MT,ST .567.029.387.014.480.014.231.011.793.014.333.012.262.012.405.012.536.022.428.011ordered EOT .511 .021.371 .011.477.014 .226 .011.783 .014.330.012.259.013 .397 .010.503 .017.412 .011
Logistic-ITMT.522.026.394.013.468.016.236.013.794.015.326.012.265.013.458.011.524.019.420.012
non-orderedST.522.026 .371 .012.468.016.236.012.794.015.326.012.262.012.458.011.524.019 .412 .011EOT .509 .018.372 .011.463.014 .226 .012.785 .012.327.011 .255 .012.400 .013.502 .016.412 .010
Logistic-IT MT,ST .513.023.370.014.463.016.233.011.795.014.328.010.259.013.401.015.529.018.413.010ordered EOT .510.019.371.012.461.016 .224 .012.786 .013.326.011 .252 .012.396.012 .501 .016.412.010
Logistic-ATMT,ST.507.020.370.012.470.017.227.011.787.014.328.011.254.012.399.015.505.017.413.010
orderedLB.510.022.370.011.469.016.229.012.791.016.328.011.256.013.400.013.508.016.412.010EOT.508.019.370.012.468.016.224.011.788.013.327.012.252.013.397.013.505.018.412.009
OLR-NLLMT,ST.509.019.370.012.462.016.226.012.787.012.326.011.256.012.399.013.505.014.412.010
orderedLB.508.023.370.014.462.016.229.013.789.014.326.011.256.014.399.013.508.018.412.011EOT.508.018.371.011.459.015.224.011.786.013.325.011.252.011.397.013 .500 .018.412.010
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT.250.015.419.009.213.007.688.010.384.017.492.017.415.007.376.008.065.008EOT .241 .015.415 .007.214.009 .683 .009.373 .014.438 .014.409 .006.374 .006.057 .008
Hinge-ITMT.592.048.421.008.252.015.763.013.397.024.492.017.421.004.404.011.177.014
non-orderedST.589.047.421.008.252.015.763.013.401.022.492.017.421.004.404.011.177.014EOT .534 .057.415 .010.226 .009.718 .009.369 .011.423 .014.421.004 .390 .011.076 .008
Hinge-IT MT,ST .207.014.420.008.229.008.741.010.468.042.492.017.415.007.378.009.065.007ordered EOT .203.014 .414 .007.219 .009.718 .009.369 .012.422 .012.407 .006.370 .005.056 .006
Hinge-AT MT,ST .180.012.419.008.237.010.722.008.373.013.489.018.418.006.381.008.066.009ordered EOT .177.012 .414 .009.223 .010.715 .010.361 .012.410 .013.409 .006.372 .005.058 .007
Logistic-ITMT.370.021.424.009.242.012.762.012.380.015.405.014.414.006.380.006.206.019
non-orderedST.370.021.419.008.242.012.762.012.372.014.402.014.414.006.380.006.206.019EOT .308 .020.414 .010.225 .009.718 .010.364 .012.392 .013.403 .004.369 .005.083 .010
Logistic-IT MT,ST .176.011.412.008.214.006.744.010.381.013.407.014.404.005.370.006.059.007ordered EOT .172.012.411.008.214.008 .718 .008.365 .012.392 .014.403.005 .368 .006.051 .007
Logistic-ATMT,ST.174.011.413.008.235.010 .718 .009.357.012.391.014.404.005.370.005.060.007
orderedLB.174.012.416.008.233.008.721.009.360.014.393.013.404.005.370.005.060.007EOT.171.010.413.009 .222 .008.716 .008.357.011.391.012 .402 .004.367 .005.053 .007
OLR-NLLMT,ST.173.012.414.008.223.009 .717 .009.359.013.390.014.403.005.369.006.098.013
orderedLB.173.012.417.008.224.008.720.009.361.012.393.013.403.005.369.006.098.013EOT.171.013.414.009 .219 .008.715 .008.357.012.389.014.403.004.368.005 .082 .009
40Published in Transactions on Machine Learning Research (1/2023)
Table 23: A counterpart of Table 2regarding RMSE for Task-S and EL10 datasets.
Learning Labeling DIA PYR APR SER TRI WBC CPU AMP BOS STO
ADNNT 1.741.419 1.276.544.961.300.935.411 1.943.315 2.858.296.674.286.867.101.983.135.539.028EOT 1.736.397 1.309.534.898.241 1.034.439 1.927.290 2.817.347.633.277.878.098.997.137.544.028
Hinge-ITMT 1.831.450 1.310.587.984.299 1.192.373 1.941.289 2.856.424.699.306.897.092 1.005.137.627.028
non-orderedST 1.831.450 1.312.569 1.020.301 1.107.361 1.936.304 2.764.330.714.320.904.089 1.007.137.627.028EOT 1.731.400 1.276.497.916.266 1.019.328 1.940.309 2.813.380.691.331.873.116.970.135 .550 .034
Hinge-IT MT,ST 1.801.469 1.282.515.958.306 1.057.400 1.939.314 2.821.450.718.304.884.105.981.148.535.033ordered EOT 1.705.367 1.301.473.896.272 1.055.372 1.915.317 2.761.317.692.285.853.093.963.140.533.029
Hinge-AT MT,ST 1.690.418 1.285.514.950.293 1.032.400 1.895.299 2.769.289.731.351.873.099.966.135.535.028ordered EOT 1.703.345 1.277.487.888.263 1.120.421 1.898.295 2.808.385.671.289.871.111.948.135.531.030
Logistic-ITMT 1.713.467 1.325.539 1.033.290 1.237.389 1.944.293 2.770.348.750.376.868.095.973.144.557.030
non-orderedST 1.708.460 1.310.528 1.042.300 1.114.304 1.920.303 2.785.362.801.374.867.097.972.144.557.030EOT 1.732.427 1.284.468.881 .256 1.099.301 1.907.284 2.825.331.672.278.855.106.971.131 .533 .026
Logistic-IT MT,ST 1.758.411 1.312.527.970.278.942.381 1.959.328 2.806.376.715.380.854.106.963.131.525.029ordered EOT 1.733.352 1.276.486.890 .259.959.349 1.927.318 2.780.374.706.361.856.123.947.141.526.024
Logistic-ATMT,ST 1.679.400 1.242.494.935.256.995.368 1.898.277 2.745.283.707.365.858.098.922.134.519.028
orderedLB 1.658.412 1.252.525.940.257 1.020.390 1.930.284 2.715.269.699.366.863.108.938.143.519.027EOT 1.715.392 1.328.509.910.242 1.089.384 1.911.279 2.793.326.661.300.864.113.956.140.518.029
OLR-NLLMT,ST 1.678.368 1.213.495.914.274.890.321 1.914.277 2.745.298.705.341.842.104.928.128.516.029
orderedLB 1.704.438 1.234.556.919.278.912.349 1.915.298 2.781.348.700.351.855.109.943.142.515.026EOT 1.720.401 1.284.479.863.233.957.379 1.907.278 2.794.332.648.287.842.101.946.133.519.032
Learning Labeling ABA AI2 KRA CO1 PU1 BA1 CO2 PU2 BA2 EL2
ADNNT.886.037.512.019.679.019.412.012 1.392.026.509.011.469.013.575.013 1.064.036.639.017EOT.887.030 .488 .018.676.019.412.011 1.383.025.509.012.468.013.571.0131.004 .033.621 .016
Hinge-ITMT 1.003.048.517.017.703.014.418.017 1.503.039.511.015.477.018.600.015 1.112.039.640.018
non-orderedST.981.047.517.017.701.015.416.017 1.503.039.511.015.472.016.600.016 1.112.039.640.018EOT .913 .036.482 .016.679 .023.413.0161.389 .025.502 .012.463 .013.578 .013.999 .033.622 .015
Hinge-IT MT,ST 1.005.047.515.018.700.020.408.014 1.598.038.501.012.462.014.590.013 1.349.042.641.019ordered EOT .915 .033.478 .015.694.022.407.0131.390 .028.500.012.463.015.590.012 .994 .029.621 .014
Hinge-AT MT,ST .939.053.507.019.692.022.410.012 1.413.030.504.012.463.014.577.012 1.060.036.643.018ordered EOT .883 .033.478 .016.689.023.409.0121.379 .024.503.011.462.013.574.012 .993 .030.616 .015
Logistic-ITMT.982.044.493.020.715.020.413.016 1.422.029.502.013.466.014.601.015 1.043.035.631.021
non-orderedST.913.040.491.017.687.021.411.013 1.422.029.502.013.463.014.601.015 1.043.035.624.016EOT .881 .031.475 .014.670 .022.406 .0151.375 .024.501.011 .454 .012.578 .0141.000 .027.613 .014
Logistic-IT MT,ST .932.036.476.016.689.023.402.011 1.450.027.498.012.454.014.588.013 1.120.044.622.016ordered EOT .884 .033.473.013.685.025.403.0121.377 .022.497.012.455.015 .584 .0121.012 .029.613 .015
Logistic-ATMT,ST.866.027.473.015.680.026.404.010 1.377.024.500.012.456.013.571.012 1.001.027.612.015
orderedLB.875.031.473.014.680.025.404.009 1.384.026.500.011.455.013.572.012 1.019.032.612.015EOT.873.029.474.014.677.024.403.011 1.378.023.499.012.455.013.571.014 .991 .029.614.015
OLR-NLLMT,ST.871.030.474.014.668.026.404.011 1.378.023.499.014.456.013.571.012 1.000.030.614.015
orderedLB.874.030.475.015.670.024.401.011 1.384.026.498.012.456.015.572.013 1.015.032.613.015EOT.877.029.473.014.668.025.403.012 1.375.025.498.012.457.012.572.011 .997 .030.613.014
Learning Labeling POT AI1 EL1 CAL CE1 CE2 2DP FRA MVA
ADNNT 1.084.112.575.010.528.007 1.174.017.710.020.748.030.567.005.513.005.223.009EOT 1.046 .102.574.011.529.007 1.169.016.702 .020.746.030 .564 .004.512.005 .202 .010
Hinge-ITMT.750.075.620.023.607.014 1.324.018.787.024.910.031.597.005.562.008.286.015
non-orderedST.745.073.620.023.607.014 1.324.019.787.024.910.031.580.006.539.006.257.012EOT .698 .063.577 .010.572 .0111.288 .015.739 .025.827 .030.565 .004.519 .005.197 .008
Hinge-IT MT,ST 1.194.047.577.011.523.006 1.259.021.833.029.977.063.572.005.519.004.216.012ordered EOT .838 .049.574.010.521.0071.155 .015.757 .026.845 .029.564 .004.518.004 .196 .011
Hinge-AT MT,ST .679.052.575.011.521.006 1.164.019.765.026.817.027.573.005.520.005.225.013ordered EOT .604 .041.574.010.519.0061.150 .017.715 .026.789 .025.564 .004.519.005 .207 .009
Logistic-ITMT.558.041.593.011.586.012 1.312.019.745.024.836.032.592.006.563.012.283.012
non-orderedST.558.041.590.012.586.012 1.312.019.745.024.825.030.578.006.545.008.267.011EOT .514 .042.574 .011.558 .0101.284 .017.726 .027.794 .028.565 .004.518 .005.194 .008
Logistic-IT MT,ST .744.050.569.010.518.007 1.245.017.752.028.899.036.567.004.510.004.194.011ordered EOT .567.041.568.010.518.0071.151 .016.688 .022.804 .031.565 .004.510.004 .181 .008
Logistic-ATMT,ST .567 .038.574.010.545.0081.276 .017.712 .024.774 .028.569.004.519.005.210.009
orderedLB.616.038.575.011.547.009 1.289.017.724.026.787.029.569.004.519.005.208.011EOT .555 .034.574.010 .532 .0071.275 .018.708 .023.773 .026.564 .004.516 .005.192 .008
OLR-NLLMT,ST.502.042.574.010.545.0091.278 .018.722 .024.780 .029.569.004.518.005.200.008
orderedLB.529.043.574.010.544.008 1.288.018.732.026.791.029.568.004.518.005.199.008EOT .488 .040.573.011 .531 .0071.275 .017.721 .025.780 .028.564 .004.516 .005.183 .008
41Published in Transactions on Machine Learning Research (1/2023)
Table 24: The number ğ‘Z(resp.,ğ‘A,ğ‘S) of times that each method was best for Task-Z (resp., Task-A, S)
in each dataset group in the form â€˜ ğ‘Z,ğ‘A,ğ‘Sâ€™, and their summation in the column â€˜SUMâ€™. The larger the
number, the better than method is for that dataset group and that task.
Learning Labeling RW EF3 EF5 EF10 EL3 EL5 EL10 SUM
ADNNT 2,1,1 0,0,0 0,0,0 1,2,0 1,1,1 4,3,1 0,1,0 8,8,3
EOT 0,2,0 1,0,0 4,1,1 9,5,3 1,2,3 3,3,3 3,1,3 21,14,13
Hinge-ITMT 0,0,0 1,0,0 0,0,0 0,1,0 2,1,1 1,0,0 0,0,0 4,2,1
non-orderedST 0,0,0 1,0,0 0,1,0 0,0,0 2,1,1 1,0,1 0,0,0 4,2,2
EOT 3,3,1 0,0,0 2,0,2 1,0,2 1,3,1 0,0,1 0,0,0 7,6,7
Hinge-IT MT,ST 0,0,0 0,0,0 0,0,0 1,0,0 2,2,1 0,0,0 1,0,0 4,2,1
ordered EOT 2,2,1 1,1,1 2,3,3 1,0,1 1,0,0 0,1,0 2,0,0 9,7,6
Hinge-AT MT,ST 0,1,1 0,1,0 0,0,0 0,1,0 3,3,3 2,2,1 1,2,1 6,10,6
ordered EOT 2,3,4 1,2,1 2,5,0 2,1,2 0,0,1 1,1,3 0,1,1 8,13,12
Logistic-ITMT 0,0,0 2,4,1 0,0,0 0,0,0 1,2,1 0,0,0 0,0,0 3,6,2
non-orderedST 0,0,0 2,4,1 0,0,0 0,0,0 1,2,1 0,0,0 0,0,0 3,6,2
EOT 2,3,4 5,4,5 0,0,0 0,0,1 2,2,2 1,1,1 2,2,1 12,12,14
Logistic-IT MT,ST 1,0,0 1,1,1 1,0,0 2,0,0 3,5,2 1,0,1 3,1,0 12,7,4
ordered EOT 1,0,0 8,7,6 4,4,3 2,2,3 5,5,4 4,6,4 7,8,6 31,32,26
Logistic-ATMT,ST 0,0,1 1,2,3 3,4,5 1,3,4 0,0,1 0,1,3 3,3,3 8,13,20
orderedLB 0,0,0 0,2,1 2,4,3 2,3,0 1,0,1 0,1,0 0,3,3 5,13,8
EOT 2,4,5 3,2,5 4,2,3 1,2,5 2,1,2 4,4,4 2,2,2 18,17,26
OLR-NLLMT,ST 1,0,0 1,2,2 1,3,1 2,4,1 1,0,1 1,1,1 2,4,3 9,14,9
orderedLB 1,0,1 2,2,1 1,3,1 2,4,2 0,0,1 1,1,0 2,4,2 9,14,8
EOT 4,4,0 2,3,2 3,6,7 2,8,5 3,4,3 6,6,5 1,4,4 21,35,26
42