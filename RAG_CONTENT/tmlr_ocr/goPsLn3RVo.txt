Published in Transactions on Machine Learning Research (01/2023)
Defense Against Reward Poisoning Attacks in Reinforce-
ment Learning
Kiarash Banihashemâˆ—kiarash@umd.edu
University of Maryland
Adish Singla adishs@mpi-sws.org
Max Planck Institute for Software Systems
Goran Radanovic gradanovic@mpi-sws.org
Max Planck Institute for Software Systems
Reviewed on OpenReview: https: // openreview. net/ forum? id= goPsLn3RVo
Abstract
We study defense strategies against reward poisoning attacks in reinforcement learning.
As a threat model, we consider cost-eï¬€ective targeted attacksâ€”these attacks minimally
alter rewards to make the attackerâ€™s target policy uniquely optimal under the poisoned
rewards, with the optimality gap speciï¬ed by an attack parameter. Our goal is to design
agents that are robust against such attacks in terms of the worst-case utility w.r.t. the
true, unpoisoned, rewards while computing their policies under the poisoned rewards. We
propose an optimization framework for deriving optimal defense policies, both when the
attack parameter is known and unknown. For this optimization framework, we ï¬rst provide
characterization results for generic attack cost functions. These results show that the
functional form of the attack cost function and the agentâ€™s knowledge about it are critical
for establishing lower bounds on the agentâ€™s performance, as well as for the computational
tractability of the defense problem. We then focus on a cost function based on /lscript2norm,
for which we show that the defense problem can be eï¬ƒciently solved and yields defense
policies whose expected returns under the true rewards are lower bounded by their expected
returns under the poison rewards. Using simulation-based experiments, we demonstrate the
eï¬€ectiveness and robustness of our defense approach.
1 Introduction
One of the key challenges in designing trustworthy AI systems is ensuring that they are technically robust and
resilient to security threats European Commission (2019). Amongst many requirements that an AI system
ought to satisfy in order to be deemed trustworthy is robustness to adversarial attacks Hamon et al. (2020).
Standard approaches to reinforcement learning (RL) Sutton & Barto (2018) have shown to be susceptible to
adversarial attacks which manipulate the feedback that an agent receives from its environment. These attacks
broadly fall under two categories: a) test-time attacks, which manipulate an agentâ€™s input data at test-time
without changing its policy Huang et al. (2017); Lin et al. (2017); Tretschk et al. (2018); Kumar et al. (2021),
and b)training-time attacks that manipulate an agentâ€™s input data at training-time, inï¬‚uencing the agentâ€™s
learned policy Zhang & Parkes (2008); Ma et al. (2019); Huang & Zhu (2019); Rakhsha et al. (2020; 2021);
Zhangetal.(2020b);Sunetal.(2020);Liu&Lai(2021);Rangietal.(2022). Inthispaper, wefocusontraining-
time attacks, and more speciï¬cally, on targeted reward poisoning attacks that modify (i.e., poison) rewards
to force an agent into adopting a targetpolicy Ma et al. (2019); Rakhsha et al. (2021); Rangi et al. (2022).
Prior work on reward poisoning attacks on RL primarily focuses on designing optimal attacks. In this paper,
âˆ—This work was done as a part of an internship project at Max Planck Institute for Software Systems.
1Published in Transactions on Machine Learning Research (01/2023)
we take a diï¬€erent perspective on targetedreward poisoning attacks, and focus on designing defense strategies
against such attacks. This is challenging, given that the attacker is typically unconstrained in poisoning the
rewards to force the target policy, while the agentâ€™s performance is measured under the true reward function,
which is unknown. The key idea that we exploit in our work is that the poisoning attacks have an underlying
structure arising from the attackerâ€™s objective to minimize the cost of the attack needed to force the target
policy. We therefore ask the following question:
Can we design an eï¬€ective defense strategy against targeted reward poisoning attacks by exploiting the cost-
eï¬€ective nature of these attacks?
s0s1s2s3
01 01 1 0 1 0-2.50-2.50
0.500.50
0.500.50
-0.50-0.50
(a)R,Ï€âˆ—s0s1s2s3
01 01 01 01-2.50-2.50
0.500.13
0.070.61
-0.500.19
(b)/hatwideR,Ï€â€ 
s0s1s2s3
01 01 0.940.06 01-2.50-2.50
0.500.13
0.070.61
-0.500.19
(c)/hatwideR,Ï€DMDPPolicyÏ€âˆ—Ï€â€ Ï€D
R 0.34-0.420.03
/hatwideR -0.030.110.03
(d) Scores of policies Ï€âˆ—,Ï€â€ ,Ï€Din MDPs w.r.t. R,/hatwideR
Figure 1: A simple chain environment with 4 states and two possible actions: leftandright.s0is the initial
state. The agent goes in the direction of its action with probability 90%, and otherwise the next state is
selected uniformly at random from the other 3 states. Weights on edges indicate rewards for the action taken.
For example in Fig. 1a, if the agent takes leftin states1, it receives 0.5. We denote the true rewards by R,
the poisoned rewards by /hatwideR, the optimal policy under RbyÏ€âˆ—, the target policy (which is uniquely optimal
under/hatwideR) byÏ€â€ , and the defense policy (which is derived from our framework) by Ï€D.(a)showsRand
Ï€âˆ—. In particular, the numbers above the arrows and the diï¬€erent shades of gray show the probabilities of
taking actions leftandrightunderÏ€âˆ—.(b)shows/hatwideRandÏ€â€ .(c)showsÏ€Dthat our optimization framework
derived from/hatwideR, and by reasoning about the goal of the attack ( Ï€â€ ). In particular, our optimization framework
maximizes the worst-case performance under R: while the optimization procedure does not know R, it can
constrain the set of plausible candidates for Rusing/hatwideR.(d)Table. 1d): Each entry in the table indicates the
score of a (policy, reward function) pair, where the score is a scaled version of the total discounted return
(see Section 3). For example, the score of policy Ï€â€ equalsâˆ’0.42and0.11under/hatwideRandRrespectively. Our
defense policy signiï¬cantly improves upon this and achieves a score of 0.03. For comparison, the score of Ï€âˆ—
equals 0.34. Moreover, unlike for the target policy Ï€â€ , the score of our defense policy Ï€DunderRis always at
least as high as its score under /hatwideR, as predicted by our results (see Theorem 5.1). The results are obtained
with parameters /epsilon1â€ = 0.1,/epsilon1D= 0.2andÎ³= 0.99(see Section 3).
In this paper, we study this question in depth and under diï¬€erent assumption on the agentâ€™s knowledge about
the attack cost function. Perhaps surprisingly, the answer to this question is sometimes aï¬ƒrmative. While an
agent only has access to the poisoned rewards, it may still be able infer some information about the true
reward function, using the fact that the attack is cost-eï¬€ective. By maximizing the worst-case utility over the
set of plausible candidates for the true reward function, the agent can substantially limit the inï¬‚uence of the
attack. The approach we study can be understood from Figure 1 which uses the chain environment from
Rakhsha et al. (2021) to demonstrate the main ideas.
Contributions. We formalize this reasoning, and characterize the utility of our novel framework for designing
defense policies. In summary, the key contributions include:
â€¢We formalize the problem of designing defense policies against targeted and cost-eï¬€ective reward poisoning
attacks, which minimally modify the original reward function to achieve their goal (force a target policy).
2Published in Transactions on Machine Learning Research (01/2023)
â€¢We introduce a novel optimization framework for ï¬nding optimally robust defense policiesâ€”this framework
focuses on optimizing the agentâ€™s worst-case utility among the set of reward functions that are plausible
candidates of the true reward function.
â€¢We provide characterization results that establish feasibility and computational complexity of ï¬nding
optimally robust defense policies for diï¬€erent classes of attack cost functions. These results show that
the functional form of the attack cost function and the agentâ€™s knowledge about it play a critical role
in deriving optimally robust defense policies with provable performance guarantees.
â€¢Focusing on a cost function based on /lscript2norm, we show that optimally robust defense policies can be
eï¬ƒciently computed. We further establish lower bounds on the true performance of defense policies derived
from our framework and computable from the poisoned rewards.
â€¢We empirically demonstrate the eï¬€ectiveness and robustness of our approach using numerical simulations.
To our knowledge, this is the ï¬rst framework for studying this type of defenses against reward poisoning
attacks that try to force a target policy at a minimal cost.
2 Related Work
While this paper is broadly related to the literature on adversarial machine learning (e.g., Huang et al. (2011)),
we recognize four themes in supervised learning (SL) and reinforcement learning (RL) that closely connect to
our work.
Poisoning attacks in SL and RL. This paper is closely related to data poisoning attacks, ï¬rst introduced
and extensively studied supervised learning Biggio et al. (2012); Xiao et al. (2012); Mei & Zhu (2015); Xiao
et al. (2015); Li et al. (2016); Koh & Liang (2017); Biggio & Roli (2018). These attacks are also called
training-time attacks , and unlike test time attacks Szegedy et al. (2014); Pinto et al. (2017); Behzadan &
Munir (2017); Zhang et al. (2020a); Moosavi-Dezfooli et al. (2016); Nguyen et al. (2015); Madry et al. (2018),
which attack an already trained agent, they change data points during the training phase, which in turn
aï¬€ects the parameters of the learned model. Data poisoning attacks have also been studied in the bandits
literature Jun et al. (2018); Ma et al. (2018); Liu & Shroï¬€ (2019) and in RL (see Section 1).
Defenses against poisoning attacks in SL. In supervised learning, defenses against data poisoning
attacks are often based on data sanitization that removes outliers from the training set Cretu et al. (2008);
Paudice et al. (2018), trusted data points that support robust learning Nelson et al. (2008); Zhang et al.
(2018), or robust estimation Charikar et al. (2017); Diakonikolas et al. (2019). Recently, Wu et al. (2022)
have considered aggregation based defenses that can certify an RL agentâ€™s policy against a limited number of
changes in the training dataset. While such defenses can mitigate some attack strategies, they are in general
susceptible to data poisoning attacks Steinhardt et al. (2017); Koh et al. (2018).
Robustness to model uncertainty. There is a rich literature that studies robustness to uncertainties in
reward functions McMahan et al. (2003); Regan & Boutilier (2010), and transition models Nilim & El Ghaoui
(2005); Iyengar (2005); Bagnell et al. (2001) for MDP models. Typically, these works consider settings
in which instead of knowing the exact parameters of the MDP, the agent has access to a set of possible
parameters (uncertainty set). These works design policies that perform well in the worst case. More recent
works have proposed ways to scale up these approaches via function approximation Tamar et al. (2014), as
well as utilize them in online settings Lim et al. (2013). While our work uses the same principles of robust
optimization, we do not assume that the uncertainty set, i.e., the set of all possible rewards, is directly given.
Instead, we show how to derive it from the poisoned reward function.
Robustness to corrupted episodes. Another important line of work is the literature on robust learners
that receive corrupted input during their training phase. Such learners have recently been designed for
bandits and experts settings Lykouris et al. (2018); Gupta et al. (2019); Bogunovic et al. (2020); Amir et al.
(2020), and episodic reinforcement learning Lykouris et al. (2019); Zhang et al. (2021). Typically, these works
consider an attack model in which the adversary can arbitrarily corrupt a limited number of episodes. As we
operate in the non-episodic setting and do not assume a limit in the attackerâ€™s poisoning budget, these works
are orthogonal to the aspects we study in this paper. Instead, we utilize the structure of the attack in order
to design a defense algorithm.
3Published in Transactions on Machine Learning Research (01/2023)
3 Formal Setting
In this section, we describe our formal setting, and identify relevant background details on reward poisoning
attacks, as well as our problem statement. The problem formulation speciï¬es our objectives that we establish
and formally analyze in the next sections.
3.1 Preliminaries
We consider a standard reinforcement learning setting in which the environment is described by a discrete-time
discounted Markov Decision Processes (MDP) Puterman (1994), deï¬ned as M= (S,A,R,P,Î³,Ïƒ ), where:
Sis the state space, Ais the action space, R:SÃ—Aâ†’Ris the reward function, P:SÃ—AÃ—Sâ†’[0,1]
is the transition model with P(s,a,s/prime)deï¬ning the probability of transitioning to state s/primeby taking action
ain states,Î³âˆˆ[0,1)is the discount factor, and Ïƒis the initial state distribution. We consider state and
action spaces, i.e., SandA, that are ï¬nite and discrete, and due to this we can adopt a vector notation for
quantities dependent on states or state-action pairs. W.l.o.g., we assume that |A|â‰¥2.
A generic (stochastic) policy is denoted by Ï€, and it is a mapping Ï€:Sâ†’P(A), whereP(A)is the probability
simplex over action space A. We useÏ€(a|s)to denote the probability of taking action ain states. While
deterministic policies are a special case of stochastic policies, when explicitly stating that a policy Ï€is
deterministic, we assume that it is a mapping from states to actions, i.e., Ï€:Sâ†’A. We denote the set of
all policies by Î and the set of all deterministic policies by Î det. For policy Ï€, we deï¬ne its score,ÏÏ€, as
E/bracketleftbig
(1âˆ’Î³)/summationtextâˆ
t=1Î³tâˆ’1R(st,at)|Ï€,Ïƒ/bracketrightbig
, where state s1is sampled from the initial state distribution Ïƒ, and then
subsequent states stare obtained by executing policy Ï€in the MDP. The score of a policy is therefore its
total expected return scaled by a factor of 1âˆ’Î³.
Finally, we consider occupancy measures. We denote the state-action occupancy measure in the Markov
chain induced by policy Ï€byÏˆÏ€(s,a) =E/bracketleftbig
(1âˆ’Î³)/summationtextâˆ
t=1Î³tâˆ’11[st=s,at=a]|Ï€,Ïƒ/bracketrightbig
. Given the MDP M, the
set of realizable state-action occupancy measures under any (stochastic) policy Ï€âˆˆÎ is denoted by Î¨. Score
ÏÏ€andÏˆÏ€satisfyÏÏ€=/angbracketleftÏˆÏ€,R/angbracketright, where/angbracketleft.,./angbracketrightcomputes the dot product between two vectors of sizes |S|Â·|A|.
We denote by ÂµÏ€(s) =E/bracketleftbig
(1âˆ’Î³)/summationtextâˆ
t=1Î³tâˆ’11[st=s]|Ï€,Ïƒ/bracketrightbig
the state occupancy measure in the Markov chain
induced by policy Ï€âˆˆÎ . State-action occupancy measure ÏˆÏ€(s,a)and state occupancy measure ÂµÏ€(s)satisfy
ÏˆÏ€(s,a) =ÂµÏ€(s)Â·Ï€(a|s). We focus on ergodicMDPs, which in turn implies that ÂµÏ€(s)>0for allÏ€ands
Puterman (1994). This is a standard assumption in this line of work (e.g, see Rakhsha et al. (2021)) and is
used to ensure the feasibility of the attackerâ€™s optimization problem.
3.2 Reward Poisoning Attacks
We consider reward poisoning attacks on an oï¬„ine learning agent that optimally change the original reward
function with the goal of deceiving the agent to adopt a deterministic policy Ï€â€ âˆˆÎ det, calledtarget policy .
This type of attack has been extensively studied in the literature, and here we utilize the attack formulation
based on the works of Ma et al. (2019); Rakhsha et al. (2020; 2021); Zhang et al. (2020b). In the following,
we introduce the necessary notation, the attackerâ€™s model, and the agentâ€™s model (without defense).
Notation. We useMto denote the trueororiginalMDP with true, unpoisoned, reward function R, i.e.,
M= (S,A,R,P,Î³,Ïƒ ). We use/hatwiderMto denote the modiï¬ed orpoisoned MDP with poisoned reward function /hatwideR,
i.e.,/hatwiderM= (S,A,/hatwideR,P,Î³,Ïƒ ). Note that only the reward function Rchanges across these MDPs. Quantities
that depend on reward functions have analogous notation. For example, the score of policy Ï€underRis
denoted by ÏÏ€, whereas its score under /hatwideRis denoted by /hatwideÏÏ€. We denote an optimal policy under RbyÏ€âˆ—, i.e.,
Ï€âˆ—âˆˆarg maxÏ€âˆˆÎ ÏÏ€.
Attack model. The attacker we consider in this paper has full knowledge of M. It can be modeled by a
functionA(c,R/prime,Ï€â€ ,/epsilon1â€ )that returns a set of poisoned rewards functions for a given attack cost function c,
reward function R/prime, target policy Ï€â€ , and a desired attack parameter /epsilon1â€ .1In particular, the attack problem is
deï¬ned by the following optimization problem:
min
Rc(R,R/prime)s.t.ÏÏ€â€ â‰¥ÏÏ€+/epsilon1â€ âˆ€Ï€âˆˆÎ det\{Ï€â€ }. (P1)
1For cost functions cpdeï¬ned by (1) with ï¬nite p>1, this set has a single element.
4Published in Transactions on Machine Learning Research (01/2023)
A common class of cost functions are /lscriptp-norms of manipulations Ma et al. (2019); Rakhsha et al. (2020; 2021),
i.e.,
c(R,R/prime) =cp(R,R/prime) =/bardblRâˆ’R/prime/bardblp, (1)
withpâ‰¥1. As shown by Rakhsha et al. (2021), the attack problem (P1)is feasible for this class of cost
functions and ergodic MDPs. Furthermore, instead of considering all deterministic policies, it is suï¬ƒcient to
consider policies that diï¬€er from Ï€â€ in a single action. Using Ï€â€ {s;a}to denote a policy that follows a/negationslash=Ï€â€ (s)
in statesandÏ€â€ (Ëœs)in states Ëœs/negationslash=s, (P1) can be rewritten as follows:
min
Rc(R,R/prime)s.t.ÏÏ€â€ â‰¥ÏÏ€â€ {s;a}+/epsilon1â€ âˆ€s,a/negationslash=Ï€â€ (s). (P1â€™)
The equivalence between (P1)and(P1â€™)is shown by Rakhsha et al. (2021), but we also provide further
details in Appendix. Intuitively, (P1)and(P1â€™)are equivalent because in order for a policy to be optimal it
is suï¬ƒcient (and necessary) that the policy is better than any of its neighbor policies. This fact allows us to
reduce the number of constraints. Whereas the number of constraints in (P1)is exponential in |S|and|A|,
the number of constraint in (P1â€™) is polynomial in |S|and|A|, which in turn implies that (P1â€™) is tractable
(for a ï¬xed p).
To better understand the optimization problem (P1â€™), we can consider /lscript2attack cost (i.e., c2) deï¬ned as the
Euclidean distance between R/primeandR. By solving this problem, i.e., setting /hatwideRâˆˆA(c2,R,Ï€â€ ,/epsilon1â€ ), the attacker
ï¬nds the closest reward function to Rfor whichÏ€â€ is a uniquely optimal policy (with attack parameter /epsilon1â€ ).
Remark 3.1.Note that the optimization problem (P1)may not be feasible if we lift the assumption that
underlying MDP is ergodic. This can be seen from the constraints of the optimization problem (P1â€™): if
target policy Ï€â€ does not visit a certain state s, then it has the same score as its neighbor policies Ï€â€ {s;a}.
Hence, the primary reason for assuming ergodicity is to make the attack problem feasible.
Agent without defense. The agent receives the poisoned MDP /hatwiderM:= (S,A,/hatwideR,P,Î³,Ïƒ )where the underlying
true reward function R(unknown to the agent) has been poisoned to /hatwideR. In the existing works on reward
poisoning attacks, an agent naively optimizes score /hatwideÏ(score w.r.t. /hatwideR). Because of this, the agent ends up
adopting policy Ï€â€ .
3.3 Problem Statement
Perhaps unsurprisingly, the agent without defense, could perform arbitrarily badly under the true reward
functionR. Our goal is to design a robust agent that derives its policy using the poisoned MDP /hatwiderM:=
(S,A,/hatwideR,P,Î³,Ïƒ ), but has provable worst-case guarantees w.r.t. R. This agent has access to the poisoned
reward vector /hatwideRâˆˆA(c,R,Ï€â€ ,/epsilon1â€ ), butR,Ï€â€ , and/epsilon1â€ are not given to the agent. Figure 2 illustrates a generic
problem setting studied in this paper. In general, the agent does not know c, but is given a class of cost
functionsCthat contains c, i.e.,câˆˆC.Crepresentsâ€™ the agentâ€™s knowledge about the attack cost function;
in a special case when Ccontains only one element, the agent knows the cost function. Notice that Ï€â€ is
obtainable by solving the optimization problem arg maxÏ€/hatwideÏÏ€asÏ€â€ is uniquely optimal in /hatwiderM. On the other
hand,Ris unknown to the agent. In terms of /epsilon1â€ , we will focus on two cases, the case when /epsilon1â€ is known to
the agent, and the case when it is not.
Intheï¬rstcase, wecanformulatethefollowingoptimizationproblemofmaximizingtheworstcaseperformance
of the agent, given that Ris unknown:
max
Ï€min
R,câˆˆCÏÏ€s.t./hatwideRâˆˆA(c,R,Ï€â€ ,/epsilon1â€ ). (P2a)
In other words, we calculate the set of all possible reward functions Rsuch that an attack on Rcouldlead to
the solution/hatwideR, i.e.,/hatwideRâˆˆA(c,R,Ï€â€ ,/epsilon1â€ ). We then ï¬nd a policy Ï€for which the worst-case score, i.e., minR,cÏÏ€
is maximized, where the minimum is over all reward functions Rcalculated previously and all cost functions
câˆˆC.
5Published in Transactions on Machine Learning Research (01/2023)
True reward function ğ‘…"Poisoned reward function ğ‘…#
Attack ğ’œ(cost ğ‘, target ğœ‹')
Defense ğ’Ÿ(score under ğ‘…")Defense policy ğœ‹ğ’Ÿ
Poisoned reward function ğ‘…#
Defense policy ğœ‹ğ’Ÿ
Find the set of reward functions consistent with ğ‘…#and ğ’œ
Find a policy that maximizes the worst-case score over this set
Figure 2: The problem setting studied in this paper. The attack Amodiï¬es the original (true) reward
functionRto forceÏ€â€ while minimizing its cost. The defense Daims to optimize the agentâ€™s score under R,
but it only sees the poisoned reward /hatwideR. Nevertheless, it can (in principle) ï¬nd the set of all reward functions
consistent with /hatwideRandA, and search for a policy that maximizes the worst-case score of the agent over this
consistent set.
Knowledge about attack cost Guarantees on the value Complexity
GeneralC(e.g., s.t.cconstâˆˆC)Nofor any/hatwideR, Proposition 4.2 â€”
C={/bardblRâˆ’R/prime/bardblps.t.pâˆˆ[1,âˆ)} Yes, Theorem 4.4 NP-hard , Theorem 4.5
C={/bardblRâˆ’R/prime/bardblâˆ} Nofor some/hatwideR, Theorem 4.3 NP-hard , Appendix
C={/bardblRâˆ’R/prime/bardbl2} Yes, Section 5 Convex, Section 5
C={/bardblRâˆ’R/prime/bardbl1} Yes, Appendix Convex, Appendix
Table 1: Characterization results for diï¬€erent levels of the agentâ€™s knowledge about the attack cost function,
expressed through C. In general, ifCcan be arbitrary, the optimization problem (P2a)may be unbounded
from below regardless of /hatwideR. For some classes C, e.g., that contain /lscriptp-norm attack costs ( p/negationslash=âˆ),(P2a)has
the optimal solution, but this solution may be computationally hard. When the attack cost function is known,
properties of problem (P2a)depend on the functional form of the attack cost function, as indicated by the
/lscript1-norm,/lscript2-norm, and /lscriptâˆ-norm attack costs.
For the case when the agent does not know /epsilon1â€ , we use the following optimization problem:
max
Ï€min
R,/epsilon1,câˆˆCÏÏ€s.t./hatwideRâˆˆA(c,R,Ï€â€ ,/epsilon1)and0</epsilon1â‰¤/epsilon1D. (P2b)
where the agent uses /epsilon1Das an upper bound on /epsilon1â€ . We denote solutions to the optimization problems (P2a)
and(P2b)byÏ€D, and it will be clear from the context which optimization problem we are referring to with
Ï€D.
Remark 3.2.We note that some structural assumptions on the attack model are needed to guarantee
robustness. For example, prior work typically considers untargeted attacks with budget constraints, e.g.,
that put an upper limit on the cost of the attack or the number of episodes in which the attacker can attack
Lykouris et al. (2019); Zhang et al. (2021). This paper focuses on targeted attacks that minimize the cost of
the attack. Given the strategic nature of the attacker, the structural assumptions on the attack model that
(P2a) and (P2b) rely on are fairly natural.
4 Characterization Results for Generic Attack Cost
In this section, we provide characterization results showing the importance of the agentâ€™s knowledge about
the attack cost function. The overview of the characterization results is shown in Table 1. To corresponding
proofs can be found in Appendix.
Remark 4.1.The results presented in this section are stated for the optimization problem (P2a). However,
the same results also hold for the optimization problem (P2b).
4.1 General Attack Cost
We start by stating what is perhaps an expected result: if the attack cost function can be arbitrary, then no
defense can achieve any provable guarantee. It is relatively easy to see why this claim should hold. If the
6Published in Transactions on Machine Learning Research (01/2023)
agent believes that the attack cost function can be constant, then from the agentâ€™s perspective, Rcan be any
reward function. Since rewards are not bounded, this in turn implies that no matter which policy the agent
selects, no worst-case guarantees are possible. More formally, we obtain the following claim.
Proposition 4.2. Letcconst(R,R/prime)be a constant cost function, and assume that cconstâˆˆC. Then the
optimization problem (P2a)is unbounded from below.
Given this result, it is clear that for provable defenses: a) the attack cost function cannot not be arbitrary
in that/hatwideRhas to be informative about R; b) the agent should have some knowledge about the attack cost
function. In the next subsection, we consider cost functions based on /lscriptpnorms, i.e., cp, commonly adopted
by prior work on reward poisoning attacks Ma et al. (2019); Rakhsha et al. (2020; 2021).
4.2/lscriptpAttack Cost
In contrast to constant cost functions, the strategic nature of the attacker is more apparent when it optimizes
cp, so the agent may infer some information about the true reward function Rfrom the poisoned rewards /hatwideR
which it can access. Our ï¬rst result shows that for p=âˆthe success of such an inference procedure depends
on/hatwideR. This is formally captured by the following theorem.
Theorem 4.3. There exists an instance of the problem setting, i.e., MDP M= (S,A,/hatwideR,P,Î³,Ïƒ )for which
the optimization problem (P2a)is unbounded from below when câˆâˆˆC.
This theorem paints a relatively bleak picture for the possibility of achieving provable guarantees. Note two
important observations. First, the impossibility result is a weaker variant of the result stated in Proposition
4.2 as it holds only for some MDPs. Second, câˆis measuring the maximum modiï¬cation of reward function
R, so critical information about Rmay be lostâ€”this also provides intuition behind the impossibility results.
In contrast, when p/negationslash=âˆ,cpis aï¬€ected by all the modiï¬cations of R. In fact, when pis restricted to take
values in [1,âˆ), a lower bound on the optimal value of (P2a) can always be derived from /hatwideR.
Theorem 4.4. Consider any policy Ï€/negationslashÏ€â€ s.t.Ï€/negationslashÏ€â€ (Ï€â€ (s)|s) = 0. ForC={cps.t.pâˆˆ[1,âˆ)}, the optimal
value of the optimization problem (P2a)is bounded from below by /hatwideÏÏ€/negationslashÏ€â€ .
A direct consequence of Theorem 4.4 is that the optimal solution to (P2a)always exists, and its worst case
performance under Ris at least/hatwideÏÏ€/negationslashÏ€â€ . Note that we can easily ï¬nd a (deterministic) policy Ï€/negationslashÏ€â€ that maximizes
the lower bound by solving maxÏ€s.t.Ï€(s)/negationslash=Ï€â€ (s)/hatwideÏÏ€.2Furthermore, for any policy Ï€/negationslashÏ€â€ s.t.Ï€/negationslashÏ€â€ (Ï€â€ (s)|s) = 0we
have thatÏ/negationslashÏ€â€ â‰¥/hatwideÏ/negationslashÏ€â€ . This means that we can eï¬ƒciently ï¬nd a defense policy with provable performance
guarantees. However, such a defense policy may not be optimally robust in that its performance lower bound
would not match the optimal one. We now turn to computational complexity challenges in deriving optimally
robust defense policies: the next theorem provides a hardness result for C={cps.t.pâˆˆ[1,âˆ)}, which
admits guarantees on the optimal solution to (P2a).
Theorem 4.5. ForC={cps.t.pâˆˆ[1,âˆ)}, it is NP-hard to determine whether the optimal value of the
optimization problem (P2a)is greater than or equal to /hatwideÏÏ€â€ .
Given that even for this natural choice of cost functions, C={cps.t.pâˆˆ[1,âˆ)}, the problem of ï¬nding
optimally robust policies is computationally hard, in the next section we focus on /lscript2attack cost that is known
to the agent, i.e., C={c2}. In Appendix, we provide similar analysis for /lscript1attack cost, i.e.,C={c1}.
5 Characterization Results for /lscript2Attack Cost
In this section, we consider the attack cost function c2, and assume that it is known to the agent ( C={c2}).
In the ï¬rst part, we focus on characterization results for the case when the attack parameter /epsilon1â€ is known to
the agent, i.e., the optimization problem (P2a). In the second part, we focus on the optimization problem
(P2b), and generalize the results from the ï¬rst part to the unknown attack parameter setting. The proofs of
our formal results can be found in Appendix. Figure 3 provides intuition behind the results of this section.
2Stochastic policies are not necessary in this case since we can think of this problem as searching for an optimal policy over a
truncated actions space (because actions Ï€â€ (s)are not admissible), so an optimal deterministic policy always exists.
7Published in Transactions on Machine Learning Research (01/2023)
In particular, it illustrates attack and defense strategies for a single-state MDP with action set {a1,...a 7}and
the/lscript2cost function. We refer the reader to Appendix for more details.
a1a2a3a4a5a6a70246810
ActionsR
Ï€âˆ—1000000
(a)R,Ï€âˆ—a1a2a3a4a5a6a70246810
Actions/hatwideR
Ï€â€ 0001000
(b)/hatwideR,Ï€â€ a1a2a3a4a5a6a70246810
Actions/hatwideR
Ï€D1
3001
3001
3
(c)/hatwideR,Ï€D
Figure 3: A single-state environment environment with 7 actions and state s. In each ï¬gure, the denoted
policy is uniform over actions on or above the dashed line. (a)showsRandÏ€âˆ—. Here, the optimal policy
selects action a1.(b)shows/hatwideRand target policy Ï€â€ with/epsilon1â€ = 1. Here, the target policy selects action a4./hatwideR
is obtained by solving (P1)(or(P1â€™)) with the/lscript2cost function and R/prime=R. In this case, the attack only
modiï¬es the rewards of three actions, a1,a4, anda7.(c)shows/hatwideRandÏ€Dwith/epsilon1D= 2. The defense strategy
only sees poisoned rewards /hatwideR. It ï¬rst calculates the optimal action and the set of all second-best actions
under/hatwideR, in this case{a1,a7}. If the reward of the second-best actions are no worse than /epsilon1D, they form the
setÎ˜/epsilon1={(s,a1),(s,a7)}or simply Î˜/epsilon1
s={a1,a7}. We show in Appendix that the defense strategy, i.e., the
solution to (P2b), selects an action uniformly at random from the set {Ï€â€ (s)}âˆªÎ˜/epsilon1
s={a1,a4,a7}. As we
show in this section, the expected reward of the defense policy under Ris at least as much as its expected
reward under /hatwideR, which can be easily veriï¬ed from the ï¬gures: in this case, both are equal to 22/3.
5.1 Known Parameter Setting
We begin by analyzing the optimization problem (P2a). Denote by Î˜/epsilon1state-action pairs (s,a)for which the
diï¬€erence between /hatwideÏÏ€â€ and/hatwideÏÏ€â€ {s;a}is equal to /epsilon1, i.e., Î˜/epsilon1=/braceleftbig
(s,a) :/hatwideÏÏ€â€ {s;a}âˆ’/hatwideÏÏ€â€ =âˆ’/epsilon1/bracerightbig
.3For the results
of this section, Î˜/epsilon1with/epsilon1=/epsilon1â€ plays a critical roleâ€”as we show in our formal analysis, it characterizes the
feasible set of the optimization problem (P2a). In particular, in our analysis we show that /hatwideRis the solution
to the attack problem for an underlying reward function R, i.e.,/hatwideR=A(R,Ï€â€ ,/epsilon1â€ ), if and only if Rcan be
expressed as
R=/hatwideR+/summationdisplay
(s,a)âˆˆÎ˜/epsilon1â€ Î±s,aÂ·/parenleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /parenrightBig
,
withÎ±s,aâ‰¥0. To see the importance of this result, let us instantiate R=Rand calculate ÏÏ€:
ÏÏ€=/angbracketleftbig
ÏˆÏ€,R/angbracketrightbig
=/angbracketleftBig
ÏˆÏ€,/hatwideR/angbracketrightBig
+/summationdisplay
(s,a)âˆˆÎ˜/epsilon1â€ Î±s,aÂ·/angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,ÏˆÏ€/angbracketrightBig
.
When the occupancy measure of Ï€Dis positively aligned with vectors ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ , the performance of
Ï€Dunder the original reward function Ris at least/hatwideÏÏ€D=/angbracketleftBig
ÏˆÏ€D,/hatwideR/angbracketrightBig
. Hence, constraining Ï€Dto satisfy
/angbracketleftbig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,ÏˆÏ€D/angbracketrightbig
â‰¥0for alls,aâˆˆÎ˜/epsilon1â€ yields a guarantee on the score ÏÏ€D, i.e.,ÏÏ€Dâ‰¥/hatwideÏÏ€D. These
insights are formalized by Theorem 5.1, which also describes a procedure for solving (P2a). In Appendix, we
provide intuition behind our analysis using a special case of our setting.
3In practice, Î˜/epsilon1should be calculated with some tolerance due to numerical imprecision (See Section 6).
8Published in Transactions on Machine Learning Research (01/2023)
Theorem 5.1. Consider the following optimization problem parameterized by /epsilon1:
max
ÏˆâˆˆÎ¨/angbracketleftBig
Ïˆ,/hatwideR/angbracketrightBig
s.t./angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,Ïˆ/angbracketrightBig
â‰¥0âˆ€s,aâˆˆÎ˜/epsilon1. (P3)
For/epsilon1=/epsilon1â€ , this optimization problem is always feasible, and its optimal solution Ïˆmaxspeciï¬es an optimal
solution to the optimization problem (P2a)forC={c2}with
Ï€D(a|s) =Ïˆmax(s,a)/summationtext
a/primeÏˆmax(s,a/prime). (2)
The score of Ï€D(a|s)is lower bounded by ÏÏ€Dâ‰¥/hatwideÏÏ€D.
In addition to providing a characterization of the solution to (P2a), the above theorem provides an eï¬ƒcient
algorithm for ï¬nding this solution using linear programming. As we discuss in Appendix D.3, the set of
vectorsÏˆÏ€â€ andÏˆÏ€â€ {s;a}can be precomputed since the agent is given a poisoned model /hatwiderM(and hence,
knows the transition probabilities); for any policy Ï€, the state-action occupancy measure ÏˆÏ€satisï¬es
ÏˆÏ€(s,a) =ÂµÏ€(s)Â·Ï€(a|s)where the state occupancy measure ÂµÏ€is the unique solution to the following
Bellman identity
ÂµÏ€(s) = (1âˆ’Î³)Ïƒ(s) +Î³/summationdisplay
Ëœs,ËœaÂµÏ€(Ëœs)Ï€(Ëœa|Ëœs)P(Ëœs,Ëœa,s).
In addition, the set of valid occupancy measures Î¨is the set of all vectors ÏˆâˆˆR|S|Â·|A|satisfying the following
Bellman ï¬‚ow constraints
âˆ€s:/summationdisplay
aÏˆ(s,a) = (1âˆ’Î³)Ïƒ(s) +/summationdisplay
Ëœs,ËœaÎ³Â·P(Ëœs,Ëœa,s)Â·Ïˆ(Ëœs,Ëœa),
âˆ€(s,a) :Ïˆ(s,a)â‰¥0.
Therefore, since occupancy measures ÏˆÏ€â€ {s;a}andÏˆÏ€â€ can be precomputed, the optimization problem (P3)
can be eï¬ƒciently solved using linear programming. In other words, computing optimally robust defense
policy is computationally tractable. In order to calculate each ÂµÏ€, we need to solve a |S|Ã—|S|system
of linear equations, which requires |S|3operations in the worst case. Therefore, ï¬nding each ÏˆÏ€for
Ï€âˆˆ{Ï€â€ }âˆª{Ï€â€ {s;a}:a/negationslash=Ï€â€ (s)}requires at most |S|4Â·|A|operations. The linear program (P3)takes
O(|S|4Â·|A|4)time in the worst case as it has |S|Â·|A|constrains and variables. Therefore, the overall complexity
of the approach is O(|S|4Â·|A|4)in the worst case.
Theorem 5.1 also provides a performance guarantee of the defense policy w.r.t. the true reward function, i.e.,
ÏÏ€Dâ‰¥/hatwideÏÏ€D. Such a bound is important in practice since it provides a certiï¬cate of the worst-case performance
under the true reward function R, even though the agent can only optimize over /hatwideR. In contrast to the lower
bound in Theorem 4.4, the lower bound in Theorem 5.1 is optimal.
5.2 Unknown Parameter Setting
In this subsection, we focus on the optimization problem (P2b). First, note the structural diï¬€erence between
(P2a)and(P2b). In the former case, /epsilon1â€ is given, and hence, the defense can infer possible values of Rby
solving an inverse problem to the attack problem (P1). In particular, we know that the original reward
functionRhas to be in the set {R:/hatwideRâˆˆA(R,Ï€â€ ,/epsilon1â€ )}. In the latter case, /epsilon1â€ is not known, and instead we use
parameter/epsilon1Das an upper bound on /epsilon1â€ . We distinguish two cases:
â€¢Overestimating Attack Parameter : If/epsilon1â€ â‰¤/epsilon1D, thenweknowthat Risintheset{R:/hatwideRâˆˆA(R,Ï€â€ ,/epsilon1)s.t.0<
/epsilon1â‰¤/epsilon1D}. Note that this set is a super-set of {R:/hatwideRâˆˆA(R,Ï€â€ ,/epsilon1â€ )}, which means that it is less informative
aboutR.
â€¢Underestimating Attack Parameter : If/epsilon1â€ > /epsilon1D, then the set{R:/hatwideRâˆˆA(R,Ï€â€ ,/epsilon1)s.t.0< /epsilon1â‰¤/epsilon1D}will
have only a single element, i.e., /hatwideR. In other words, this set typically contains no information about R.
We analyze these two cases separately, ï¬rst focusing on the former one.
9Published in Transactions on Machine Learning Research (01/2023)
5.2.1 Overestimating Attack Parameter
When/epsilon1Dâ‰¥/epsilon1â€ , our formal analysis builds on the one presented in Section 5.1, and we highlight the main
diï¬€erences. Given that /epsilon1â€ is not exactly known, we cannot directly operate on the set Î˜/epsilon1â€ . However, since
/epsilon1Dupper bounds /epsilon1â€ , the defense can utilize the procedure from the previous section (Theorem 5.1) with
appropriately chosen /epsilon1to solve (P2b) as we show in the following theorem.
Theorem 5.2. Assume that /epsilon1Dâ‰¥/epsilon1â€ , and deï¬ne/hatwide/epsilon1=mins,a/negationslash=Ï€â€ (s)/bracketleftbig
/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}/bracketrightbig
. Then, the optimization
problem(P3)with/epsilon1=min{/epsilon1D,/hatwide/epsilon1}is feasible and its optimal solution Ïˆmaxidentiï¬es an optimal policy Ï€Dfor
the optimization problem (P2b)withC={c2}via Equation (2). This policy Ï€Dsatisï¬esÏÏ€Dâ‰¥/hatwideÏÏ€D.
To interpret the bounds, let us consider three cases:
â€¢R/negationslash=/hatwideR: If the attack indeed poisoned R, then the smallest /epsilon1/primeâˆˆ(0,/epsilon1D]such that Î˜/epsilon1/prime/negationslash=âˆ…corresponds
to/epsilon1â€ . In this case, it turns out that /epsilon1â€ =/hatwide/epsilon1, and somewhat surprisingly, the defense policies of (P2a)and
(P2b) coincide. (Note that this analysis assumes that /epsilon1Dâ‰¥/epsilon1â€ .)
â€¢R=/hatwideRand/epsilon1D</hatwide/epsilon1: This corresponds to the case when the attack did not poison Rand there is no
/epsilon1/primeâˆˆ(0,/epsilon1D]such that Î˜/epsilon1/prime/negationslash=âˆ…. In this case, it turns out that the optimal solution to the optimization
problem (P2b) is Ï€D=Ï€â€ (indeedÏ€â€ is uniquely optimal under R).
â€¢R=/hatwideRand/epsilon1Dâ‰¥/hatwide/epsilon1: This corresponds to the case when the attack did not poison Rand there is /epsilon1/primeâˆˆ(0,/epsilon1D]
such that Î˜/epsilon1/prime/negationslash=âˆ…. In fact,/hatwide/epsilon1is the smallest such /epsilon1/prime. In this case, it turns out that, in general, the optimal
solution to the optimization problem (P2b) is Ï€D/negationslash=Ï€â€ , even though Ï€â€ is uniquely optimal under R.
These three cases also showcase the importance of choosing /epsilon1Dthat is a good upper bound on /epsilon1â€ . When
R=/hatwideR, the agent should select /epsilon1Dthat is strictly smaller than /hatwide/epsilon1. On the other hand, when R/negationslash=/hatwideR, the agent
should select /epsilon1Dâ‰¥/hatwide/epsilon1, as it will be apparent from the result of the next subsection, in particular Theorem 5.3.
While the agent knows /hatwide/epsilon1, it does not know if R=/hatwideRorR/negationslash=/hatwideR.
5.2.2 Underestimating Attack Parameter
In this subsection, we analyze the case when /epsilon1D< /epsilon1â€ . We ï¬rst state our result, and then discuss its
implications.
Theorem 5.3. If/epsilon1â€ > /epsilon1D, thenÏ€D=Ï€â€ is the unique solution of the optimization problem (P2b)with
C={c2}.
Therefore, together with Theorem 5.2, Theorem 5.3 is showing the importance of having a good prior
knowledge about the attack parameter /epsilon1â€ . In particular:
â€¢When the attack did not poison the reward function (i.e., /hatwideR=R), overestimating /epsilon1â€ implies that Ï€Dmight
not be equal to Ï€â€ for larger values of /epsilon1D, even though Ï€â€ is uniquely optimal under R. This can have a
detrimental eï¬€ect since in this case ÏÏ€D<ÏÏ€â€ =ÏÏ€âˆ—.
â€¢When the attack did poison the reward function R(i.e.,/hatwideR/negationslash=R), underestimating /epsilon1â€ impliesÏ€D=Ï€â€ , but
Ï€â€ might be suboptimal. In this case, the defense policy does not limit the negative inï¬‚uence of the attack
at all, i.e.,ÏÏ€D=ÏÏ€â€ â‰¤ÏÏ€âˆ—.
We further discuss nuances to selecting /epsilon1Din Section 7.
6 Experimental Evaluation
In this section we evaluate our defense strategy in an experimental setting in order to better understand its
eï¬ƒcacy and robustness. We focus on the setting from Section 5.2: c2attack cost functions, which is known
to the agent, i.e., C={c2}, with an unknown attack parameter /epsilon1â€ .
Giventhe resultsin Section5 (Theorems 5.2and 5.3), we use thelinear programmingformulation (P3)together
with the CVXPY solver Diamond & Boyd (2016); Agrawal et al. (2018) for calculating the solution to the
defense optimization problem (P2b). In the experiments, due to limited numerical precision, Î˜/epsilon1is calculated
10Published in Transactions on Machine Learning Research (01/2023)
withatoleranceparameter, setto 10âˆ’4bydefault.4. Inotherwords, Î˜/epsilon1={(s,a) :|/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}âˆ’/epsilon1|â‰¤10âˆ’4}.
s0s1s2s3s4s5s6
s7
s8
SG
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /epsilon1D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /epsilon1â€ )
âˆ’0.150.000.150.30
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /epsilon1D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /epsilon1â€ )
âˆ’1.6âˆ’1.4âˆ’1.2âˆ’1.0âˆ’0.8
(a) (b) (c) (d)
Figure 4: Experimental environments: Figures (a)and(b)show the Navigation and Grid world environment
respectively while ï¬grues (c)and(d)showÏÏ€Din these environments. For comparison, in the navigation
environment, ÏÏ€â€ =âˆ’0.26andÏÏ€âˆ—= 0.45while in the grid world environment, ÏÏ€â€ =âˆ’1.75andÏÏ€âˆ—=âˆ’0.70.
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-0.26-0.080.100.280.46Score w.r.t R
Ï€D
Ï€D+Ï€â€ 
Ï€âˆ—
(a) Navigation, PreAttack
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-0.26-0.080.100.280.46Score w.r.t R
Ï€D
Ï€D+Ï€â€ 
Ï€âˆ— (b) Navigation, PostAttack
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-1.72-1.46-1.20-0.94-0.68Score w.r.t R
Ï€D
Ï€D+Ï€â€ 
Ï€âˆ— (c) Grid world, PreAttack
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-1.72-1.46-1.20-0.94-0.68Score w.r.t R
Ï€D
Ï€D+Ï€â€ 
Ï€âˆ— (d) Grid world, PostAttack
Figure 5: Robustness of the defense policy against random perturbation. Results are based on average of 100
runs for each data point. Error bars around the data points indicate standard error.
Navigation environment . Our ï¬rst environment, shown in Figure 4a is the Navigation environment
taken from Rakhsha et al. (2021). The environment has 9 states and 2 possible actions. The reward
function is action independent and has the following values: R(s0,.) =R(s1,.) =R(s2,.) =R(s3,.) =âˆ’2.5,
R(s4,.) =R(s5,.) = 1andR(s6,.) =R(s7,.) =R(s8,.) = 0. When the agent takes an action, it will
successfully navigate in the direction shown by the arrows with probability 0.9; otherwise, the next state will
be sampled uniformly at random. The bold arrows in the ï¬gure indicate the attackerâ€™s target policy. The
initial state is s0and the discounting factor Î³equals 0.99.
Grid world environment. For our second environment, shown in Figure 4b, we use the grid world
environment from Ma et al. (2019) with slight modiï¬cations in order to ensure ergodicity â€” we add a 10%
failure probability to each action, sampling the next state randomly in case of failure. The environment has
18 states and 4 actions: up,down,rightandleft. The white, gray and blue cells in the ï¬gure represent the
states and the black cells represent walls. In the white and gray states, the agent will attempt to go in the
direction speciï¬ed by its action if there is a neighboring state in that direction. If there is no such state,
the agent will attempt to stay in its own place. In the blue state G, the agent will attempt to stay in its
own place regardless of the action taken. In all states, each attempt will succeed with probability 0.9; with
probability 0.1, the next state will be sampled uniformly at random. In the gray and white states, the agentâ€™s
reward is a function of the state it is attempting to visit. Attempting to visit a gray, white and blue state
will yield a reward of âˆ’10,âˆ’1and2respectively. If the agent is in a blue state, it will always receive a
reward of 0. The bold arrows in the ï¬gure specify the attackerâ€™s target policy. The initial state is Sand the
discounting factor Î³equals 0.9.
Policy score for diï¬€erent values of parameters. We ï¬rst analyze the score of our defense policy in
4The value was chosen because the CVXPY solver uses a precision of 10âˆ’5.
11Published in Transactions on Machine Learning Research (01/2023)
both environments with diï¬€erent values of /epsilon1â€ and/epsilon1D. For comparison, we also report the scores of the target
policy (Ï€â€ ) and the optimal policy ( Ï€âˆ—). The results are shown in Figures 4c and 4d. As seen in the ï¬gures,
as long as/epsilon1Dâ‰¥/epsilon1â€ , our defense policy signiï¬cantly improves the agentâ€™s score compared to Ï€â€ .
Robustness to perturbations. We now analyze our algorithmâ€™s robustness towards uncertainties in the
reward functions used by the attacker and the defender. For our ï¬rst experiment, which we call PreAttack, we
randomly perturb the attackerâ€™s input. In particular, the input to the defenderâ€™s optimization problem /hatwideRis
sampled fromA(c2,R+N(0,Ïƒ2I),Ï€â€ ,/epsilon1â€ )whereIis the identity matrix, Ndenotes the multivariate normal
distribution and Ïƒis the perturbation parameter varied in the experiment. For our second experiment, called
PostAttack, we randomly perturb the reward vector after the attack, sampling the defenderâ€™s input from
A(c2,R,Ï€â€ ,/epsilon1â€ ) +N(0,Ïƒ2I). In both experiments we use /epsilon1â€ = 0.1and/epsilon1D=âˆ. As explained below, when
calculating Î˜/epsilon1, we also experiment with a larger tolerance parameter of 10âˆ’1, denoting the defense policy in
this case with Ï€D+.
The results can be seen in Figure 5. As seen in the ï¬gures, our defense policy Ï€Dconsistently improves on
the baseline obtained with no defense (i.e, Ï€â€ ). It is also clear that the PostAttack perturbations have a
greater negative impact on our defense strategyâ€™s score. Results for Ï€D+indicate that this is due to random
perturbations prohibiting our algorithm from identifying all of the elements in Î˜/epsilon1. While having a higher
tolerance parameter helps with robustness, it can also lead to a lower performance when there is no noise
because Î˜/epsilon1would falsely include additional elements. We leave choosing the tolerance parameter in a more
systematic way for future work.
7 Concluding Discussions
In this paper, we introduced an optimization framework for designing defense strategies against reward
poisoning attacks that change an agentâ€™s reward structure in order to steer the agent to adopt a target policy.
We analyzed the utility of using such defense strategies, providing characterization results and provable
guarantees on their performance. Moving forward we see several interesting future research directions.
Beyond the worst-case utility. In this paper, we deï¬ned the defense objective as the maximization of
the agentâ€™s worst-case utility. While this is a sensible objective, there are other objectives that one could
analyze. For example, instead of focusing on the absolute performance, one can try to optimize performance
relative to the target policy. Notice that this is a somewhat diï¬€erent, and possibly a weaker goal, given
that the target policy can have arbitrarily bad utility under R. Additionally, one could study the agentâ€™s
sub-optimality gap , i.e., the diï¬€erence between its score and the score of the optimal policy Ï€âˆ—, and compare
it to the sub-optimality gap of the target policy. Going back to the example in Figure 3, we can see that the
defense strategy can signiï¬cantly reduce the suboptimality gap relative to not having a defense (in this case,
from 10âˆ’3 = 7to10âˆ’22/3â‰ˆ2.7). However, when the original reward function already forces the target
policy, i.e., when R=/hatwideR, the suboptimality gap of the target policy is equal to 0. In Figure 3, this happens if
the target policy is action a1. Nevertheless, in this case, the suboptimality gap of the defense strategy would
also be low, equal to 1/2(since the defense would randomly select a1ora7), and generally upper bounded by
/epsilon1D= 2if the rewards of sub-optimal actions {a2,...,a 7}were diï¬€erent. Providing a full theoretical treatment
for the general setting is an interesting research direction.
Informed prior. We did not model prior knowledge that an agent might have about the attacker or the
underlying reward function. In practice, the agent may have some information about the underlying true
reward function. Incorporating such considerations calls for a Bayesian approach that could improve the
agentâ€™s defense by, e.g., ruling out implausible candidates for Rin the agentâ€™s inference of Rgiven/hatwideR.
Selecting /epsilon1Dand non-oblivious attacks. The results in Section 5.2 indicate that choosing good /epsilon1Dis
important for having a functional defense. In practice, a selection procedure for /epsilon1Dshould take into account
the cost that the attacker has for diï¬€erent choices of /epsilon1â€ , as well as game-theoretic considerations: attacks
might not be oblivious in that the strategy for selecting /epsilon1â€ might depend on the strategy for selecting /epsilon1D.
Namely, a direct consequence of Theorem 5.2 is that the attack optimization problem (P1)can successfully
achieve its goal if it chooses a large value of /epsilon1â€ to large enough values. However, the cost of the attack also
grows with /epsilon1â€ , so the attack (if strategic) also needs to reason about /epsilon1Dwhen selecting /epsilon1â€ . We leave the full
game-theoretic characterization of the parameter selection problem for the future work.
Unknown-model and scalability . Following prior work, we focused on attacks and defenses that have
12Published in Transactions on Machine Learning Research (01/2023)
access to an accurate transition model and operate in tabular settings. Given Theorems 5.1, 5.2 and 5.3,
the defense problem can be solved using the linear program (P3), which is similar in size to the attack
optimization problem (P1â€™). As such, we expect our method to be computationally scalable as long as the
attack optimization problem can be solved.
For many RL problems however, the transition and reward functions are not known and the MDP is too large
to be modeled in tabular settings. RL solutions for such problems typically rely on function approximation.
For future work, it would be interesting to study the problem, for both attack and defense, in this more
realistic scenario. An immediate question is how the attack problem would need to change in order to
generalise to this setting. In terms of defense, while the new setting would likely pose new challenges,
we expect the general min-max formulation in Section 3, as well as the techniques used for solving the
optimization problems (P2a) and (P2b), to remain useful.
Acknowledgments
The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This
research was, in part, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
â€“ project number 467367360.
References
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for convex
optimization problems. Journal of Control and Decision , 5(1):42â€“60, 2018.
Idan Amir, Idan Attias, Tomer Koren, Roi Livni, and Yishay Mansour. Prediction with corrupted expert
advice.CoRR, abs/2002.10286, 2020.
J Andrew Bagnell, Andrew Y Ng, and Jeï¬€ G Schneider. Solving uncertain markov decision processes.
Technical report, Carnegie Mellon University, 2001.
Kiarash Banihashem, Adish Singla, Jiarui Gan, and Goran Radanovic. Admissible policy teaching through
reward design. CoRR, abs/2201.02185, 2022.
Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it stronger.
CoRR, abs/1712.09344, 2017.
Dimitri P Bertsekas. Control of uncertain systems with a set-membership description of the uncertainty. PhD
thesis, Massachusetts Institute of Technology, 1971.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.
Pattern Recognition , 84:317â€“331, 2018.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In
ICML, 2012.
Ilija Bogunovic, Arpan Losalka, Andreas Krause, and Jonathan Scarlett. Stochastic linear bandits robust to
adversarial attacks. CoRR, abs/2007.03285, 2020.
Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In STOC, pp. 47â€“60,
2017.
Gabriela F Cretu, Angelos Stavrou, Michael E Locasto, Salvatore J Stolfo, and Angelos D Keromytis. Casting
out demons: Sanitizing training data for anomaly sensors. In IEEE Symposium on Security and Privacy ,
pp. 81â€“95. IEEE, 2008.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A
robust meta-algorithm for stochastic optimization. In ICML, pp. 1596â€“1606, 2019.
13Published in Transactions on Machine Learning Research (01/2023)
Steven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for convex optimization.
The Journal of Machine Learning Research , 17(1):2909â€“2913, 2016.
Christos Dimitrakakis, David C Parkes, Goran Radanovic, and Paul Tylkin. Multi-view decision processes:
The helper-ai problem. In NeurIPS , pp. 5443â€“5452, 2017.
European Commission. Ethics Guidelines for Trustworthy Artiï¬cial Intelligence. URL: https://ec.europa.
eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai , 2019. [Online; accessed
15-January-2021].
Ahana Ghosh, Sebastian Tschiatschek, Hamed Mahdavi, and Adish Singla. Towards deployment of robust
cooperative ai agents: An algorithmic framework for learning adaptive policies. In AAMAS, pp. 447â€“455,
2020.
Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarial
corruptions. In COLT, pp. 1562â€“1578, 2019.
Ronan Hamon, Henrik Junklewitz, and Ignacio Sanchez. Robustness and explainability of artiï¬cial intelligence.
Publications Oï¬ƒce of the European Union , 2020.
Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Adversarial
machine learning. In ACM workshop on Security and artiï¬cial intelligence , pp. 43â€“58, 2011.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on
neural network policies. CoRR, abs/1702.02284, 2017.
Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipulations on cost
signals. In GameSec , pp. 217â€“237, 2019.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research , 30(2):257â€“280, 2005.
Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Xiaojin Zhu. Adversarial attacks on stochastic bandits. In
NeurIPS , pp. 3644â€“3653, 2018.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via inï¬‚uence functions. In ICML, pp.
1885â€“1894. PMLR, 2017.
Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization
defenses. CoRR, abs/1811.00741, 2018.
Aounon Kumar, Alexander Levine, and Soheil Feizi. Policy smoothing for provably robust reinforcement
learning. arXiv preprint arXiv:2106.11420 , 2021.
Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based
collaborative ï¬ltering. In NeurIPS , pp. 1885â€“1893, 2016.
Shiau Hong Lim, Huan Xu, and Shie Mannor. Reinforcement learning in robust markov decision processes.
InNeurIPS , pp. 701â€“709, 2013.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of
adversarial attack on deep reinforcement learning agents. In IJCAI, pp. 3756â€“3762, 2017.
Fang Liu and Ness B. Shroï¬€. Data poisoning attacks on stochastic bandits. In ICML, pp. 4042â€“4050, 2019.
Guanlin Liu and Lifeng Lai. Provably eï¬ƒcient black-box action poisoning attacks against reinforcement
learning. Advances in Neural Information Processing Systems , 34, 2021.
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial
corruptions. In STOC, pp. 114â€“122, 2018.
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Corruption robust exploration in
episodic reinforcement learning. CoRR, abs/1911.08689, 2019.
14Published in Transactions on Machine Learning Research (01/2023)
Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. Data poisoning attacks in contextual bandits. In
GameSec , pp. 186â€“204, 2018.
Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning and
control. In NeurIPS , pp. 14543â€“14553, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. In ICLR, 2018.
H Brendan McMahan, Geoï¬€rey J Gordon, and Avrim Blum. Planning in the presence of cost functions
controlled by an adversary. In ICML, pp. 536â€“543, 2003.
Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine
learners. In AAAI, pp. 2871â€“2877, 2015.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In CVPR, pp. 2574â€“2582, 2016.
Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini,
Charles A Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert your spam ï¬lter.
LEET, 8:1â€“9, 2008.
Anh Nguyen, Jason Yosinski, and Jeï¬€ Clune. Deep neural networks are easily fooled: High conï¬dence
predictions for unrecognizable images. In CVPR, pp. 427â€“436, 2015.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition
matrices. Operations Research , 53(5):780â€“798, 2005.
Andrea Paudice, Luis MuÃ±oz-GonzÃ¡lez, Andras Gyorgy, and Emil C Lupu. Detection of adversarial training
examples in poisoning attacks through anomaly detection. CoRR, abs/1802.03041, 2018.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement
learning. In ICML, pp. 2817â€“2826, 2017.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming . John Wiley &
Sons, Inc., 1994.
Goran Radanovic, Rati Devidze, David Parkes, and Adish Singla. Learning to collaborate in markov decision
processes. In ICML, pp. 5261â€“5270, 2019.
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via
environment poisoning: Training-time adversarial attacks against reinforcement learning. In ICML, 2020.
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching in reinforce-
ment learning via environment poisoning attacks. Journal of Machine Learning Research , 22(210):1â€“45,
2021.
Anshuka Rangi, Haifeng Xu, Long Tran-Thanh, and Massimo Franceschetti. Understanding the limits of
poisoning attacks in episodic reinforcement learning. arXiv preprint arXiv:2208.13663 , 2022.
Kevin Regan and Craig Boutilier. Robust policy computation in reward-uncertain mdps using nondominated
policies. In AAAI, volume 24, 2010.
Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certiï¬ed defenses for data poisoning attacks. In NeurIPS,
pp. 3520â€“3532, 2017.
Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl with
unknown dynamics. In International Conference on Learning Representations , 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
15Published in Transactions on Machine Learning Research (01/2023)
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In
ICML, pp. 1032â€“1039, 2008.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In ICLR, 2014.
Csaba SzepesvÃ¡ri. The asymptotic convergence-rate of q-learning. In NeurIPS , volume 10, pp. 1064â€“1070,
1997.
Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In ICML,
pp. 181â€“189, 2014.
Edgar Tretschk, Seong Joon Oh, and Mario Fritz. Sequential attacks on agents for long-term adversarial
goals.CoRR, abs/1805.12487, 2018.
Fan Wu, Linyi Li, Chejian Xu, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, and
Bo Li. Copa: Certifying robust policies for oï¬„ine reinforcement learning against poisoning attacks. arXiv
preprint arXiv:2203.08398 , 2022.
Han Xiao, Huang Xiao, and Claudia Eckert. Adversarial label ï¬‚ips attack on support vector machines. In
ECAI, pp. 870â€“875, 2012.
Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature
selection secure against training data poisoning? In ICML, pp. 1689â€“1698, 2015.
Haoqi Zhang and David C. Parkes. Value-based policy teaching with active indirect elicitation. In AAAI,
2008.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Robust deep
reinforcement learning against adversarial perturbations on observations. CoRR, abs/2003.08938, 2020a.
Xuezhou Zhang, Xiaojin Zhu, and Stephen Wright. Training set debugging using trusted items. In AAAI,
volume 32, 2018.
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against
reinforcement learning. In ICML, 2020b.
Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Robust policy gradient against strong data
corruption. CoRR, abs/2102.05800, 2021.
A List of Appendices
In this section we provide a brief description of the content provided in the appendices of the paper.
â€¢Appendix B provides an intuition of our results for the /lscript2attack cost using special MDPs in which the
agentâ€™s actions do not aï¬€ect the transition dynamics. The proofs of the results presented in this Appendix
can be found in Appendix I.
â€¢Appendix C provides additional details regarding the experiments.
â€¢Appendix D contains some background on reward poisoning attacks, and a brief overview of the MDP
properties that are important for proving our formal results.
â€¢Appendix E contains characterization results for the attack optimization problem (P1).
â€¢Appendix F contains proofs of the formal results in Section 5.
â€“The proof of Theorem 5.1 is in Section F.1.
â€“The proof of Theorem 5.2 is in Section F.2.
â€“The proof of Theorem 5.3 is in Section F.3.
16Published in Transactions on Machine Learning Research (01/2023)
â€¢Appendix G contains the proofs for the results in Section 4 relating to Guarantees on values. The appendix
also includes an optimization framework for solving the defense optimization problem (P2a)for the/lscript1
norm as well as additional characterization results for the defense optimization problem for more general
cost functions which are used for proving the complexity results in Section 4.
â€“The characterization result for the defense optimization problem with for the /lscript1norm is provided in
Section G.1.
â€“Proof of Theorem 4.4 is provided in Section G.2.
â€“Additional characterization results for the defense optimization problem are provided in Section G.3.
â€“Proof of Proposition 4.2 is provided in Section G.4
â€“Proof of Theorem 4.3 is provided in Section G.5.
â€¢Appendix H contains the proofs of the formal results in Section 4 relating to the computational complexity
of the defense optimization problem as well as the computational complexity result for the /lscriptâˆattack cost.
â€“The hardness result for the /lscriptâˆnorm is provided in Section H.1.
â€“Proof of Theorem 4.5 is provided in Section H.2
â€¢Appendix I contains a formal treatment of the results presented in Appendix B
B Intuition of Results using Special MDPs
In this Appendix, we describe characterization results on the /lscript2attack cost for special MDPs, in which the
agentâ€™s actions do not aï¬€ect the transitions, that is, we assume that
P(s,a,s/prime) =P(s,a/prime,s/prime)âˆ€s,a,a/prime,s/prime. (3)
Variants of the above condition have been studied in the literature (e.g., SzepesvÃ¡ri (1997); Dimitrakakis et al.
(2017); Sutton & Barto (2018); Radanovic et al. (2019); Ghosh et al. (2020)). Note that this assumption
implies that any two policies Ï€andÏ€/primehave equal state occupancy measures, so we simplify the notation by
denotingÂµ=ÂµÏ€=ÂµÏ€/prime.
While the results from the previous sections incorporate this special case, we study this setting because: i) the
optimal solutions to the defense problem have a simple form, enabling us to provide intuitive explanations of
our main results from the previous sections, ii) using this setting, we show a tightness result for Theorem 5.2.
A more formal exposition of our results for this setting inlcuding the proofs can be found in Appendix I.
B.1 Optimal Defense Policy
In this subsection, we provide the intuition behind defense policies for the unknown parameter setting
with/epsilon1Dâ‰¥/epsilon1â€ (Section 5.2.1). The key point about the assumption in Equation (3)is that it allows us to
consider each state separately in the defense optimization problems. In particular, it can be shown that the
optimization problem (P3) is equivalent to solving |S|optimization problems of the form
max
Ï€(Â·|s)âˆˆP(A)/angbracketleftBig
Ï€(Â·|s),/hatwideR(s,Â·)/angbracketrightBig
(P3b)
Ï€(a|s)â‰¥Ï€/parenleftbig
Ï€â€ (s)|s/parenrightbig
âˆ€aâˆˆÎ˜/epsilon1
s,
where Î˜/epsilon1
s={a:/hatwideR(s,a)âˆ’/hatwideR(s,Ï€â€ (s)) =âˆ’/epsilon1
Âµ(s)}.5If we instantiate Theorem 5.2 for special MDPs by putting
/epsilon1=min{/epsilon1D,/hatwide/epsilon1}, the set Î˜/epsilon1
shas an intuitive description: it is the set of all â€œsecond-bestâ€ actions (w.r.t /hatwideR) in
statessuch that their poisoned reward is greater than or equal to /hatwideR(Ï€â€ (s))âˆ’/epsilon1
Âµ(s). It turns out that the
defense policy for state sselects an action uniformly at random from the set Î˜/epsilon1
sâˆª{Ï€â€ (s)}. In other words,
the defense policy Ï€Dis given by:
Ï€D(a|s) =/braceleftBigg
1
|Î˜/epsilon1s|+1ifaâˆˆÎ˜/epsilon1
sâˆª{Ï€â€ (s)}
0 otherwise.
5See Lemma 8 in Banihashem et al. (2022).
17Published in Transactions on Machine Learning Research (01/2023)
To see why, note that the objective in (P3b)only improves as we put more probability on selecting Ï€â€ (s)(since
Ï€â€ (s)is optimal under /hatwideR). However, the constraints in (P3b)require that the selection probability of any
action in Î˜/epsilon1
shas to be at least as high as the selection probability of Ï€â€ (s), which in turn give us the uniform
at random selection rule. Figure 3 illustrates attack and defense policies for special MDPs using a single-state
MDP with action set {a1,...a 7}. To obtain defense policy Ï€D, we can solve the optimization problem (P3b),
which implies that Ï€Dshould select an action uniformly at random from the set {Ï€â€ (s)}âˆªÎ˜/epsilon1
s={a1,a4,a7}
C Additional Details Regarding Experiments
In this section we provide additional details regarding the experiments. The source code for our experiments,
as well as instructions for replicating our results can be found in the Supplementary Material.
C.1 Implementation details
Both the attacker optimization problem (P1), and the defense optimization problems (P2a)and(P2b)are
convex since, by Theorems 5.1 and 5.2, the defense optimization problem reduces to the linear program (P3).
We use CVXPY to calculate their solutions. The code for solving these optimization problems can be found
in the ï¬le MDP.py. The speciï¬c functions used for solving these problems are as follows:
â€¢The function attackimplements the attackerâ€™s optimization problem (P1). As explained in the main text,
this is done by solving (P1â€™) since (P1â€™) is equivalent to (P1).
â€¢The function defend_known implements the optimization problem (P2a). As explained in Section 6, the
tolerance parameter is set to 10âˆ’4(default value).
â€¢The function defend_unknown implements the optimization problem (P2b).
C.2 Running time
Following prior work Rakhsha et al. (2021), to test the running times, we use the chain environment from
Rakhsha et al. (2021), but with diï¬€erent number of states (additional states are added between s2ands3,
and the corresponding transitions and rewards are deï¬ned analogously to those for s2). The attack and
defense parameters are set to /epsilon1â€ = 0.1and/epsilon1D= 0.2. Table 2 shows the average running times (across 10
runs) of the attack optimization problem (P1â€™)and the defense optimization problem (P2b)for diï¬€erent sizes
of the chain environment.
It should be noted that the attack and defense optimization problems are similar in size, both solve a problem
with at most|S|Â·(|A|âˆ’1)constraints on R|S|.|A|. However, solving the defense problem takes more time,
partly because Ï€â€ ,/hatwide/epsilon1andÎ˜/epsilon1need to be identiï¬ed before (P3) can be solved.
The machine used for obtaining these results is a Macbook Pro personal computer with 4 Gigabytes of
memory and a 2.4 GHz Intel Core i5 processor.
D Background and Additional MDP Properties
In this section we brieï¬‚y outline the background and MDP properties that we utilize in our proofs.
D.1 Reward Poisoning Attacks
In this section, we provide some background on the cost-eï¬ƒcient reward poisoning attacks, focusing on the
results from Rakhsha et al. (2021).
The setting studied in Rakhsha et al. (2021) incorporates both the average and the discounted reward
optimality criteria in a discrete-time Markov Decision Process (MDP), with ï¬nite state and action spaces.
Our MDP setting is equivalent to their MDP setting under the discounted reward optimality criteria. This
criteria can be speciï¬ed by score Ï. As deï¬ned in the main text, scoreÏÏ€of policyÏ€is the total expected
18Published in Transactions on Machine Learning Research (01/2023)
|S|ProblemAttack Defense
4 0.01sÂ±0.5ms 0.05sÂ±1.6ms
10 0.01sÂ±0.2ms 0.09sÂ±1.5ms
20 0.01sÂ±0.1ms 0.17sÂ±4.8ms
30 0.02sÂ±2.0ms 0.27sÂ±9.7ms
50 0.04sÂ±6.8ms 0.56sÂ±34.6ms
70 0.07sÂ±3.0ms 1.02sÂ±69.7ms
100 0.13sÂ±5.4ms 1.83sÂ±91.2ms
Table 2: Run time of the attack and defense optimization problems for the chain environment with varied
number of states |S|. Reported numbers are average of 10 runs; standard error is shown with Â±.
return scaled by factor 1âˆ’Î³:
ÏÏ€=E/bracketleftBigg
(1âˆ’Î³)âˆ/summationdisplay
t=1Î³tâˆ’1R(st,at)|Ï€,Ïƒ/bracketrightBigg
,
where the state s1is sampled from the initial state distribution Ïƒ, and subsequent states stare obtained by
executing policy Ï€in the MDP. Actions atare sampled from policy Ï€.
As explained in the main text, the following result is important for our analysis, since it allows us to simplify
the optimization problem (P1) into the optimization problem (P1â€™).
Lemma D.1. (Lemma 1 in Rakhsha et al. (2021)) The score of a policy Ï€â€ is at least/epsilon1â€ greater than all
other deterministic policies if and only if its score is at least /epsilon1â€ greater than the score of any policy Ï€â€ {s;a}.
In other words,
/parenleftBig
âˆ€Ï€âˆˆÎ det\{Ï€â€ }:ÏÏ€â€ â‰¥ÏÏ€+/epsilon1â€ /parenrightBig
â‡â‡’/parenleftBig
âˆ€s,a/negationslash=Ï€â€ (s) :ÏÏ€â€ â‰¥ÏÏ€â€ {s;a}+/epsilon1â€ /parenrightBig
.
Remark D.2.As explained in Rakhsha et al. (2021), this lemma implies that the optimization problem (P1)
is equivalent to (P1â€™). Furthermore, the optimization problem is always feasible since any policy can be made
optimal with suï¬ƒcient perturbation of the reward function as formally shown by Rakhsha et al. (2021) and
Ma et al. (2019).
D.2 Overview of Important Quantities
Next, we provide an overview of standard MDP quantities and the quantities introduced in the main text
that are important for our analysis.
In addition to score Ï, we consider state-action value function, or Q-value function, deï¬ned as
QÏ€(s,a) =E/bracketleftBiggâˆ/summationdisplay
t=1Î³tâˆ’1R(st,at)|s1=s,a1=a,Ï€/bracketrightBigg
.
In other words, QÏ€(s,a)is the total expected return when the ï¬rst state is s, the ï¬rst action is a, while
subsequent states stand actions atare obtained by executing policy Ï€in the MDP.
We consider two occupancy measures. By ÏˆÏ€we denote the state-action occupancy measure in the Markov
chain induced by policy Ï€:
ÏˆÏ€(s,a) =E/bracketleftBigg
(1âˆ’Î³)âˆ/summationdisplay
t=1Î³tâˆ’11[st=s,at=a]|Ï€,Ïƒ/bracketrightBigg
.
19Published in Transactions on Machine Learning Research (01/2023)
Given MDP M, the set of realizable occupancy measures under any (stochastic) policy Ï€âˆˆÎ is denoted by
Î¨. Note that the following holds:
ÏÏ€=/angbracketleftÏˆÏ€,R/angbracketright, (4)
where/angbracketleft.,./angbracketrightin the above equation computes a dot product between two vectors of size |S|Â·|A|(i.e., two vectors
inR|S|Â·|A|). We also denote by ÂµÏ€the state occupancy measure in the Markov chain induced policy Ï€âˆˆÎ ,
i.e.:
ÂµÏ€(s) =E/bracketleftBigg
(1âˆ’Î³)âˆ/summationdisplay
t=1Î³tâˆ’11[st=s]|Ï€,Ïƒ/bracketrightBigg
.
Note that
/summationdisplay
s,aÏˆÏ€(s,a) =/summationdisplay
sÂµÏ€(s) = 1.
State-action occupancy measure and state occupancy measure satisfy
ÏˆÏ€(s,a) =ÂµÏ€(s)Â·Ï€(a|s), (5)
which for deterministic Ï€is equivalent to
ÏˆÏ€(s,a) =1[Ï€(s) =a]Â·ÂµÏ€(s). (6)
Apart from the standard MDP quantities mentioned above, we also mention quantities introduced in the
main text. We denote by Î˜/epsilon1state-action pairs (s,a)for which the margin between /hatwideÏÏ€â€ and/hatwideÏÏ€â€ {s;a}is equal
to/epsilon1, i.e.:
Î˜/epsilon1=/braceleftBig
(s,a) :/hatwideÏÏ€â€ {s;a}âˆ’/hatwideÏÏ€â€ =âˆ’/epsilon1/bracerightBig
, (7)
which can be expressed through reward function /hatwideRusing state-action occupancy measures Ïˆ:
Î˜/epsilon1=/braceleftBig
(s,a) :/angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,/hatwideR/angbracketrightBig
=âˆ’/epsilon1/bracerightBig
.
Finally, quantity Î“{s;a}(Ï€)measures how well the occupancy measure of Ï€is aligned with ÏˆÏ€â€ {s;a}relative to
ÏˆÏ€â€ :
Î“{s;a}(Ï€) =/angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,ÏˆÏ€/angbracketrightBig
. (8)
D.3 Occupancy Measures as Linear Constraints
In this subsection, we introduce the Bellman ï¬‚ow linear constraints that characterize ÏˆÏ€andÂµÏ€. In order to
characterize ÏˆÏ€, we require the following constraints:
âˆ€s:/summationdisplay
aÏˆ(s,a) = (1âˆ’Î³)Ïƒ(s) +/summationdisplay
Ëœs,ËœaÎ³Â·P(Ëœs,Ëœa,s)Â·Ïˆ(Ëœs,Ëœa). (9)
âˆ€(s,a) :Ïˆ(s,a)â‰¥0. (10)
The importance of these constraints is reï¬‚ected in the following lemma.
Lemma D.3. (Theorem 2 in Syed et al. (2008)) Let Ïˆbe a vector that satisï¬es the Bellman ï¬‚ow constraints
(9)and(10). Deï¬ne policy Ï€as
Ï€(a|s) =Ïˆ(s,a)/summationtext
ËœaÏˆ(s,Ëœa). (11)
ThenÏˆis the state-action occupancy measure of Ï€, in other words Ïˆ=ÏˆÏ€. Conversely, if Ï€âˆˆÎ is a policy
with state-action occupancy measure Ïˆ(i.e,Ïˆ=ÏˆÏ€) thenÏˆsatisï¬es the Bellman ï¬‚ow constraints (9)and
(10), as well as Equation (11).
20Published in Transactions on Machine Learning Research (01/2023)
As forÂµÏ€, it is well-known (e.g., see Rakhsha et al. (2021)) that a vector Âµis the state occupancy measure
for policyÏ€(i.e.,Âµ=ÂµÏ€), if and only if
Âµ(s) = (1âˆ’Î³)Ïƒ(s) +Î³/summationdisplay
Ëœs,ËœaÂµ(Ëœs)Ï€(Ëœa|Ëœs)P(Ëœs,Ëœa,s). (12)
E Attack Characterization Results
E.1 Characterization results for the /lscript2attack cost
In this section we provide characterization results for the attack optimization problem (P1)for the/lscript2norm,
i.e,C={c2}. We will later use these results for proving the formal results presented in Section 5.1 and Section
5.2. In addition, these results provide intuition for our results about the more general /lscriptpnorms, which we will
discuss in the next sections. In particular, the main result of this appendix is a set of Karushâ€“Kuhnâ€“Tucker
(KKT) conditions that characterize the solution to the optimization problem (P1). As we focus on the cost
functionc=c2in this section, we will drop the dependence on cinA(c,R,Ï€â€ ,/epsilon1â€ ).
To compactly express the KKT characterization results, let us introduce state occupancy diï¬€erence matrix
Î¦âˆˆR|S|Â·(|A|âˆ’1)Ã—|S|Â·|A|as a matrix with rows consisting of the vectors ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ for all neighboring
policiesÏ€â€ {s;a}. Additionally, for all s,a/negationslash=Ï€â€ (s), we use Î¦(s,a)to denote the transpose of the row of
Î¦corresponding to (s,a). Note that Î¦(s,a)is a column vector. In this notation, given Remark D.2 and
Equation (4) , the optimization problem (P1) is equivalent to
min
R1
2/bardblRâˆ’R/prime/bardbl2
2(P1")
s.t. Î¦Â·R4âˆ’/epsilon1â€ Â·1,
where 1is a|S|Â·(|A|âˆ’1)vector whose each element equal to 1, and 4speciï¬es that the left hand side is
element-wise less than or equal to the right hand side. Given this notation, the following lemma states the
KKT conditions for a reward function R(i.e., an|S|Â·|A|vector) to be an optimal solution to the optimization
problem (P1).
Lemma E.1. (KKT characterization) Ris a solution to the optimization problem (P1)if and only if there
exists an|S|Â·|A|vectorÎ»such that
(Râˆ’R/prime) +Î¦TÂ·Î»=0stationarity ,
Î¦Â·R+/epsilon1â€ Â·140primal feasibility ,
Î»<0dual feasibility ,
âˆ€(s,a/negationslash=Ï€â€ (s)) :Î»(s,a)Â·(Î¦(s,a)TÂ·R+/epsilon1â€ ) =0complementary slackness ,
where 0denotes an|S|Â·|A|vector whose each element equal to 0, and likewise, 1denotes an|S|Â·|A|vector
whose each element equal to 0.
Proof.Since(P1)is always feasible (Remark D.2) and all of the constrains are linear, strong duality holds.
Now, the Lagrangian of the optimization problem is equal to
L=1
2/bardblRâˆ’R/prime/bardbl2
2+Î»T(Î¦Â·R+/epsilon1â€ Â·1),
and taking the gradient with respect to Rgives us
âˆ‡RL= (Râˆ’R/prime) +Î¦TÂ·Î».
The statement then follows by applying the standard KKT conditions.
Remark E.2.(Uniqueness) The solution to the optimization problem (P1)is unique since the objective
1
2||Râˆ’R/prime||2
2is strongly convex.
21Published in Transactions on Machine Learning Research (01/2023)
The above lemma, has the following important consequence.
Lemma E.3. Reward function Rsatisï¬es/hatwideR=A(R,Ï€â€ ,/epsilon1â€ )if and only if there exists some Î±s,aâ‰¥0such
that
R=/hatwideR+/summationdisplay
(s,a)âˆˆÎ˜/epsilon1â€ Î±s,aÂ·/parenleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /parenrightBig
.
.
Proof.To prove the statement, we use Lemma E.1. The primal feasibility condition in the lemma always
holds as/hatwideRâˆˆA(R,Ï€â€ ,/epsilon1â€ ). Therefore/hatwideRâˆˆA(R,Ï€â€ ,/epsilon1â€ )if and only if there exists Î»such that the other three
conditions hold. Note that the complementary slackness condition is equivalent to
âˆ€(s,a/negationslash=Ï€â€ (s)) :Î»(s,a) = 0âˆ¨Î¦(s,a)TÂ·R+/epsilon1â€ = 0â‡â‡’ âˆ€ (s,a)/âˆˆÎ˜/epsilon1â€ :Î»(s,a) = 0.
Therefore from dual feasibility, stationarity and complemantary slackness it follows that /hatwideRâˆˆA(R,Ï€â€ ,/epsilon1â€ )if
and only if there exists Î»such that
Î»<0,
R=/hatwideR+/summationdisplay
(s,a)Î»(s,a)Â·/parenleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /parenrightBig
,
âˆ€(s,a)/âˆˆÎ˜/epsilon1â€ :Î»(s,a) = 0.
The Lemma therefore follows by setting Î±s,a=Î»(s,a)since setting Î»(s,a) = 0for all (s,a)/âˆˆÎ˜/epsilon1â€ is equivalent
to not summing over the terms corresponding to (s,a)/âˆˆÎ˜/epsilon1â€ in the stationarity condition.
A direct consequence of this lemma is the following result.
Corollary E.4. Assume that/hatwideR=A(R,Ï€â€ ,/epsilon1â€ )and/hatwideR/negationslash=R. It follows that
/hatwide/epsilon1=/epsilon1â€ ,
where
/hatwide/epsilon1= min
s,a/negationslash=Ï€â€ (s)/bracketleftBig
/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}/bracketrightBig
.
Proof.Assume to the contrary that /hatwide/epsilon1/negationslash=/epsilon1â€ . Given the primal feasibility condition in Lemma E.1, /hatwide/epsilon1â‰¥/epsilon1â€ .
Therefore/hatwide/epsilon1>/epsilon1â€ . It follows that
âˆ€s,a/negationslash=Ï€â€ (s) :/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}>/epsilon1â€ =â‡’Î˜/epsilon1â€ =âˆ….
Given Lemma E.3, this implies that R=/hatwideR, which contradicts the initial assumption R/negationslash=/hatwideR.
F Proofs of Section 5.1
This section of the appendix contains the proofs of the formal results presented in Section 5.
F.1 Proof of Theorem 5.1
Before proving the theorem we prove some results that we need for the proof of this theorem, as well as for
the results in later sections.
Lemma F.1. Consider policy Ï€with state-action occupancy measure ÏˆÏ€. Solution ÏÏ€
minto the following
optimization problem:
min
RÏÏ€s.t./hatwideR=A(R,Ï€â€ ,/epsilon1â€ ), (P4)
22Published in Transactions on Machine Learning Research (01/2023)
satisï¬es:
ÏÏ€
min=/braceleftBigg
/hatwideÏÏ€ifâˆ€s,aâˆˆÎ˜/epsilon1â€ : Î“{s;a}(Ï€)â‰¥0
âˆ’âˆotherwise.
Proof.We separately analyze the two cases: the case when Î“{s;a}(Ï€)â‰¥0for all (s,a)âˆˆÎ˜/epsilon1â€ holds, and the
case when it does not.
Case 1: IfÎ“{s;a}(Ï€)â‰¥0for all (s,a)âˆˆÎ˜/epsilon1â€ , then by using Equation (4) and Lemma E.3 we obtain that
ÏÏ€âˆ’/hatwideÏÏ€=/angbracketleftBig
ÏˆÏ€,Râˆ’/hatwideR/angbracketrightBig
=/summationdisplay
(s,a)âˆˆÎ˜/epsilon1â€ Î±s,aÂ·/angbracketleftBig
ÏˆÏ€,ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /angbracketrightBig
â‰¥0.
Therefore, ÏÏ€â‰¥/hatwideÏÏ€. Furthermore, from Lemma E.3, we know that R=/hatwideRsatisï¬es the constraint in the
optimization problem (P4), so the score of the optimal solution to (P4) is ÏÏ€
min=/hatwideÏÏ€.
Case 2: Now, consider the case when Î“{s;a}(Ï€)<0for a certain state-action pair (s,a)âˆˆÎ˜/epsilon1â€ . LetÎ±s,abe
an arbitrary positive number. From Lemma E.3, we know that
R=/hatwideR+Î±s,aÂ·/angbracketleftBig
ÏˆÏ€,ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /angbracketrightBig
satisï¬es the constraint in the optimization problem (P4), and hence is a solution to (P4). Moreover, by using
this solution together with Equation (4), we obtain
ÏÏ€âˆ’/hatwideÏÏ€=/angbracketleftBig
ÏˆÏ€,Râˆ’/hatwideR/angbracketrightBig
=Î±s,aÂ·/angbracketleftBig
ÏˆÏ€,ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /angbracketrightBig
=Î±s,aÂ·Î“{s;a}(Ï€). (13)
SinceÎ±s,acan be arbitrarily large and Î“{s;a}(Ï€)<0, while/hatwideÏÏ€is ï¬xed,ÏÏ€can be arbitrarily small. Hence,
the score of the optimal solution to (P4) is unbounded from below, i.e., ÏÏ€
min=âˆ’âˆ.
We can now prove Theorem 5.1, that is the following statement.
Statement: Consider the following optimization problem parameterized by /epsilon1:
max
ÏˆâˆˆÎ¨/angbracketleftBig
Ïˆ,/hatwideR/angbracketrightBig
(P3)
s.t./angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,Ïˆ/angbracketrightBig
â‰¥0âˆ€s,aâˆˆÎ˜/epsilon1.
For/epsilon1=/epsilon1â€ , this optimization problem is always feasible, and its optimal solution Ïˆmaxspeciï¬es an optimal
solution to the optimization problem (P2a)with
Ï€D(a|s) =Ïˆmax(s,a)/summationtext
a/primeÏˆmax(s,a/prime). (14)
The score of Ï€D(a|s)is lower bounded by ÏÏ€Dâ‰¥/hatwideÏÏ€D.
Proof.The feasibility of the problem follows from Theorem 4.46. Note that Ïˆmaxalways exists since
(P3)is maximizing a continuous function over a closed and bounded set. Concretely, the constraints/angbracketleftbig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,Ïˆ/angbracketrightbig
â‰¥0and Equations (9)and(10)each deï¬ne closed sets, and since ||Ïˆ||1= 1, the set Î¨is
bounded.
In order to see why Ïˆmaxspeciï¬es an optimal solution to (P2a), note that we can rewrite (P2a) as
max
Ï€ÏÏ€
min,
6The proof of Theorem 4.4 does not rely on this result .
23Published in Transactions on Machine Learning Research (01/2023)
whereÏÏ€
minis the solution to the optimization problem (P4). Due to Lemma F.1, this could be rewritten as
max
Ï€/hatwideÏÏ€
s.t.Î“{s;a}(Ï€)â‰¥0âˆ€(s,a)âˆˆÎ˜/epsilon1â€ .
Namely, maximizing a function f(x)subject to constraint xâˆˆX(whereX/negationslash=âˆ…) is equivalent to maximizing
Ëœf(x), where
Ëœf(x) =/braceleftBigg
f(x)ifxâˆˆX
âˆ’âˆo.w..
Due to (4) and (8), the constrained optimization problem above can be rewritten as
max
Ï€/angbracketleftBig
ÏˆÏ€,/hatwideR/angbracketrightBig
s.t./angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,ÏˆÏ€/angbracketrightBig
âˆ€(s,a)âˆˆÎ˜/epsilon1â€ .
Therefore, given Lemma D.3, Ïˆmaxspeciï¬es a solution to (P2a) via (2).
Finally, note that the constraints of the optimization problem (P3)ensure that a policy Ï€whose occupancy
measure is equal to Ïˆmaxwill have Î“{s;a}(Ï€)â‰¥0â€” in other words, Î“{s;a}(Ï€D)is non-negative for all
(s,a)âˆˆÎ˜/epsilon1â€ . Due to Lemma F.1, we know that such policy Ï€will have the worst case utility equal to /hatwideÏÏ€.
Therefore,ÏÏ€â‰¥/hatwideÏÏ€.
Remark F.2.Given Lemma D.3, the constraint ÏˆâˆˆÎ¨can equivalently be replaced with constraints (9)and
(10), making the optimization problem (P3) a linear program.
F.2 Proof of Theorem 5.2
The proof of the theorem is similar to the proof of Theorem 5.1 and builds on two lemmas which we introduce
in this section.
Lemma F.3. Set/epsilon1= min{/epsilon1D,/hatwide/epsilon1}, where
/hatwide/epsilon1= min
s,a/negationslash=Ï€â€ (s)/bracketleftBig
/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}/bracketrightBig
.
Reward function Rsatisï¬es/hatwideR=A(R,Ï€â€ ,Ëœ/epsilon1)for some Ëœ/epsilon1âˆˆ(0,/epsilon1D]if any only if
R=/hatwideR+/summationdisplay
(s,a)âˆˆÎ˜/epsilon1Î±s,aÂ·/parenleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /parenrightBig
,
for someÎ±s,aâ‰¥0.
Proof.We divide the proof into two parts, respectively proving the suï¬ƒciency and the necessity of the
condition.
Part 1 (Necessity): Assume that /hatwideR=A(R,Ï€â€ ,Ëœ/epsilon1)for some Ëœ/epsilon1âˆˆ(0,/epsilon1D]. From the stationariry and dual
feasibility conditions in Lemma E.1, we deduce
âˆƒÎ»<0 :R=/hatwideR+/summationdisplay
s,a/negationslash=Ï€â€ (s)Î»(s,a)Â·(ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ). (15)
We claim that Î»(s,a) = 0for all (s,a)/âˆˆÎ˜/epsilon1. Note that this would imply the lemmaâ€™s statement by setting
Î±s,a=Î»(s,a), since the terms corresponding to (s,a)/âˆˆÎ˜/epsilon1could be skipped in the summation of (15).
24Published in Transactions on Machine Learning Research (01/2023)
To see why the claim holds, assume that Î»(s,a)/negationslash= 0for some (s,a)wherea/negationslash=Ï€â€ (s). From complementary
slackness, we know that Î¦(s,a)TÂ·R+ Ëœ/epsilon1= 0, which implies that
/hatwide/epsilon1= min
Ëœs,Ëœa/negationslash=Ï€â€ (Ëœs)(âˆ’Î¦(Ëœs,Ëœa)TÂ·R)â‰¤âˆ’Î¦(s,a)TÂ·R= Ëœ/epsilon1. (16)
However, Ëœ/epsilon1â‰¤/hatwide/epsilon1holds by primal feasibility. Therefore, all the inequalities are equalities, which implies Ëœ/epsilon1=/hatwide/epsilon1.
Since Ëœ/epsilon1â‰¤/epsilon1D, we conclude that Ëœ/epsilon1=min{/epsilon1D,/hatwide/epsilon1}=/epsilon1. Since all of the inequalities in (16)are indeed equalities,
we conclude
âˆ’Î¦(s,a)TÂ·R=/epsilon1=â‡’(s,a)âˆˆÎ˜/epsilon1,
which proves the claim.
Part 2 (Suï¬ƒciency): Assume that
R=/hatwideR+/summationdisplay
(s,a)âˆˆÎ˜/epsilon1Î±s,aÂ·/parenleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /parenrightBig
,
for someÎ±s,aâ‰¥0. Set Ëœ/epsilon1=/epsilon1and note that Ëœ/epsilon1â‰¤/epsilon1Dby deï¬nition. Set
Î»(s,a) =/braceleftBigg
Î±s,aif(s,a)âˆˆÎ˜/epsilon1
0o.w..
We now verify all the conditions of Lemma E.1 hold. Stationarity and dual feasibility hold because
R=/hatwideR+/summationtext
s,aÎ»(s,a)Â·Î¦(s,a)andÎ»<0. Primal feasibility holds because Ëœ/epsilon1=min{/epsilon1D,/hatwide/epsilon1}â‰¤/hatwide/epsilon1. Finally,
complementary slackness holds because
Î»(s,a)/negationslash= 0 =â‡’(s,a)âˆˆÎ˜/epsilon1=â‡’Î¦(s,a)TR+/epsilon1= 0.
Lemma F.4. LetÏÏ€
minbe the solution to the following optimization problem
min
RÏÏ€s.t./hatwideR=A(R,Ï€â€ ,Ëœ/epsilon1)âˆ§0<Ëœ/epsilon1â‰¤/epsilon1D. (P5)
Then
ÏÏ€
min=/braceleftBigg
/hatwideÏÏ€ifâˆ€(s,a)âˆˆÎ˜/epsilon1: Î“{s;a}(Ï€)â‰¥0
âˆ’âˆo.w.,
where/epsilon1= min{/epsilon1D,/hatwide/epsilon1}, and
/hatwide/epsilon1= min
s,a/negationslash=Ï€â€ (s)/bracketleftBig
/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}/bracketrightBig
.
Proof.The proof is similar to the proof of Lemma F.1. We separately analyze the two cases: the case when
Î“{s;a}(Ï€)â‰¥0for all (s,a)âˆˆÎ˜/epsilon1holds, and the case when it does not.
Case 1: IfÎ“{s;a}(Ï€)â‰¥0for all (s,a)âˆˆÎ˜/epsilon1, then by using Equation (4) and Lemma F.3 we obtain that
ÏÏ€âˆ’/hatwideÏÏ€=/angbracketleftBig
ÏˆÏ€,Râˆ’/hatwideR/angbracketrightBig
=/summationdisplay
(s,a)âˆˆÎ˜/epsilon1Î±s,aÂ·/angbracketleftBig
ÏˆÏ€,ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /angbracketrightBig
â‰¥0.
Therefore, ÏÏ€â‰¥/hatwideÏÏ€. Furthermore, from Lemma F.3, we know that R=/hatwideRsatisï¬es the constraint in the
optimization problem (P5), so the score of the optimal solution to (P5) is ÏÏ€
min=/hatwideÏÏ€.
25Published in Transactions on Machine Learning Research (01/2023)
Case 2: Now, consider the case when Î“{s;a}(Ï€)<0for a certain state-action pair (s,a)âˆˆÎ˜/epsilon1. LetÎ±s,abe
an arbitrary positive number. From Lemma F.3, we know that
R=/hatwideR+Î±s,aÂ·/angbracketleftBig
ÏˆÏ€,ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /angbracketrightBig
satisï¬es the constraint in the optimization problem (P5), and hence is a solution to (P5). Moreover, by using
this solution together with Equation (4), we obtain
ÏÏ€âˆ’/hatwideÏÏ€=/angbracketleftBig
ÏˆÏ€,Râˆ’/hatwideR/angbracketrightBig
=Î±s,aÂ·/angbracketleftBig
ÏˆÏ€,ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /angbracketrightBig
=Î±s,aÂ·Î“{s;a}.
SinceÎ±s,acan be arbitrarily large and Î“{s;a}<0, while/hatwideÏÏ€is ï¬xed,ÏÏ€can be arbitrarily small. Hence, the
score of the optimal solution to (P5) is unbounded from below, i.e., ÏÏ€
min=âˆ’âˆ.
We are now ready to prove Theorem 5.2.
Statement: Assume that /epsilon1Dâ‰¥/epsilon1â€ , and deï¬ne/hatwide/epsilon1=mins,a/negationslash=Ï€â€ (s)/bracketleftbig
/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}/bracketrightbig
. Then, the optimization
problem(P3)with/epsilon1=min{/epsilon1D,/hatwide/epsilon1}is feasible and its optimal solution Ïˆmaxidentiï¬es an optimal policy Ï€Dfor
the optimization problem (P2b)via Equation (2). This policy Ï€Dsatisï¬esÏÏ€Dâ‰¥/hatwideÏÏ€D.
Proof.The proof is divide into two parts, respectively proving the ï¬rst and the second claim in the theorem
statement.
Part 1 (Solution to (P2b)):We prove that the optimization problem (P3)is feasible, its optimal solution
Ïˆmaxidentiï¬es an optimal solution to (P2b) via Equation (2), and satisï¬es ÏÏ€Dâ‰¥/hatwideÏÏ€D.
The feasibility of the problem follows from Theorem 4.4. Note that Ïˆmaxalways exists since (P3)is maximizing
a continuous function over a closed and bounded set. Concretely, the constraints/angbracketleftbig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,Ïˆ/angbracketrightbig
â‰¥0and
Equations (9) and (10) each deï¬ne closed sets and since ||Ïˆ||1= 1, the set Î¨is bounded.
In order to see why Ïˆmaxspeciï¬es an optimal solution to (P2b), note that we can rewrite (P2b) as
max
Ï€ÏÏ€
min,
whereÏÏ€
minis the solution to the optimization problem (P5). Due to Lemma F.4, this could be rewritten as
max
Ï€/hatwideÏÏ€
s.t.Î“{s;a}(Ï€)â‰¥0âˆ€(s,a)âˆˆÎ˜/epsilon1,
where/epsilon1=min{/epsilon1D,/hatwide/epsilon1}. Namely, maximizing a function f(x)subject to constraint xâˆˆX(whereX/negationslash=âˆ…) is
equivalent to maximizing Ëœf(x), where
Ëœf(x) =/braceleftBigg
f(x)ifxâˆˆX
âˆ’âˆo.w..
Due to (4) and (8), the constrained optimization problem above can be rewritten as
max
Ï€/angbracketleftBig
ÏˆÏ€,/hatwideR/angbracketrightBig
s.t./angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,ÏˆÏ€/angbracketrightBig
âˆ€(s,a)âˆˆÎ˜/epsilon1.
Therefore, given Lemma D.3, Ïˆmaxspeciï¬es a solution to (P2b)via(2). Finally, given Lemma F.4, ÏˆÏ€D
satisï¬es the constraints of (P5) and therefore /hatwideÏÏ€Dis a lower bound on ÏÏ€D.
26Published in Transactions on Machine Learning Research (01/2023)
F.3 Proof of Theorem 5.3
Statement: If/epsilon1â€ >/epsilon1D, thenÏ€â€ is the unique solution of the optimization problem (P2b), henceÏ€D=Ï€â€ .
Proof.As in Theorem 5.2, set /epsilon1= min{/hatwide/epsilon1,/epsilon1D}where
/hatwide/epsilon1= min
s,a/negationslash=Ï€â€ (s)/bracketleftBig
/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}/bracketrightBig
.
From the feasibility of the attack, we have that
âˆ€s,a/negationslash=Ï€â€ (s) :/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}â‰¥/epsilon1â€ >/epsilon1Dâ‰¥/epsilon1=â‡’Î˜/epsilon1=âˆ….
Therefore, given Lemma F.3, the constraint in the optimization problem (P2b)is satisï¬ed only for R=/hatwideR.
This reduces the optimization problem (P2b) to maxÏ€/hatwideÏÏ€, which has a unique optimal solution: Ï€â€ .
G Proofs of Section 4
In this section, we provide the proofs of the results of Section 47as well as additional results that characterize
the defense optimization problem. While our results are stated for the defense optimization problem (P2a),
all results hold for (P2b) as well with /epsilon1= min{/epsilon1D,/hatwide/epsilon1}.
G.1 Solution to the defense optimization problem for the /lscript1attack cost
In this section, we present a convex optimization framework for solving the defense optimization problem
(P2a)forC={c1}. Our main result is the following theorem, the proof of which is presented in Section G.3.
Theorem G.1. LetÎ¦âˆˆR|S|Â·(|A|âˆ’1)Ã—|S|Â·|A|be a matrix with rows consisting of the vectors ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ as
in Section E and let Î¦Î¸be the sub-matrix of Î¦consisting of the rows corresponding to Î˜/epsilon1â€ . Deï¬ne the set
UâŠ†SÃ—Aas the set of state action pairs (Ëœs,Ëœa)for which the following optimization problem is feasible.
âˆ€s,a:/vextendsingle/vextendsingle/parenleftbig
Î¦T
Î¸Î»/parenrightbig
(s,a)/vextendsingle/vextendsingleâ‰¤1and/parenleftbig
Î¦T
Î¸Î»/parenrightbig
(Ëœs,Ëœa) =âˆ’1andÎ»<0.
where <denotes coordinate-wise inequality. Consider the following optimization problem:
max
Ï€/hatwideÏÏ€,R,s.t.Ï€(a|s) = 0âˆ€s,aâˆˆU. (17)
The optimization problem (17)is always feasible and its solution is a solution to the defense optimization
problem (P2a)withC={c1}.
G.2 Proof of Theorem 4.4
Statement: Consider any policy Ï€/negationslashÏ€â€ s.t.Ï€/negationslashÏ€â€ (Ï€â€ (s)|s) = 0. ForC={cps.t.pâˆˆ[1,âˆ)}, the optimal value
of problem (P2a)is bounded from below by /hatwideÏÏ€/negationslashÏ€â€ .
Proof.We claim that /hatwideR(s,a)â‰¤R(s,a)for allRsatisfying/hatwideRâˆˆA(R)and alls,a/negationslash=Ï€â€ (s). To see why, assume
that if this not the case for some Ëœs,Ëœaand deï¬ne/tildewideRas
/tildewideR(s,a) =/braceleftBigg
R(s,a)ifs,a= Ëœs,Ëœa
/hatwideR(s,a)o.w.
It is clear that/vextenddouble/vextenddouble/vextenddouble/tildewideRâˆ’R/vextenddouble/vextenddouble/vextenddouble
p</vextenddouble/vextenddouble/vextenddouble/hatwideRâˆ’R/vextenddouble/vextenddouble/vextenddouble
pfor allpâˆˆ[1,âˆ). We claim that /tildewideRis feasible for the attack optimization
problem (P1)with parameters cp,R,Ï€â€ ,/epsilon1â€ . This would contradict the assumption, /hatwideRâˆˆA(R)as it would
mean/hatwideRis not an optimal solution to (P1), ï¬nishing the proof.
7The results related to computational hardness are discussed separately in Appendix H.
27Published in Transactions on Machine Learning Research (01/2023)
To prove the claim, since (P1) is equivalent to (P1â€™), we need to show that
Ï/tildewideR,Ï€â€ {Ëœs;Ëœa}âˆ’Ï/tildewideR,Ï€â€ â‰¤âˆ’/epsilon1â€ ,
for all Ëœs,Ëœa/negationslash=Ï€â€ (Ëœs). By deï¬nition of /tildewideRhowever,Ï/tildewideR,Ï€â€ {Ëœs;Ëœa}=Ï/hatwideR,Ï€â€ {Ëœs;Ëœa}as
Ï/tildewideR,Ï€â€ {Ëœs;Ëœa}âˆ’Ï/hatwideR,Ï€â€ {Ëœs;Ëœa}=/angbracketleftBig
/tildewideRâˆ’/hatwideR,ÏˆÏ€â€ {Ëœs;Ëœa}/angbracketrightBig
= 0, (18)
where the ï¬rst equality follows from Equation (4), and the second equality follows from the fact that /tildewideRonly
diï¬€ers from/hatwideRin(s,a),ÏˆÏ€â€ {Ëœs;Ëœa}(s,a) = 0. Simliarly Ï/tildewideR,Ï€â€ â‰¥Ï/hatwideR,Ï€â€ as
Ï/tildewideR,Ï€â€ âˆ’Ï/hatwideR,Ï€â€ =/angbracketleftBig
/tildewideRâˆ’/hatwideR,ÏˆÏ€â€ /angbracketrightBig
â‰¥0,
where the second inequality follows from the fact that /tildewideRis never less than /hatwideR. Therefore,
Ï/tildewideR,Ï€â€ {Ëœs;Ëœa}âˆ’Ï/tildewideR,Ï€â€ â‰¤Ï/hatwideR,Ï€â€ {Ëœs;Ëœa}âˆ’Ï/hatwideR,Ï€â€ â‰¤âˆ’/epsilon1â€ ,
where the second inequality follows from the assumption that /hatwideRâˆˆA(R). We haver therefore shown (18),
ï¬nishing the proof.
G.3 Characterization of the inner minimization problem in (P2a)
We begin by providing characterisation results for the inner minimization problem in (P2a)for diï¬€erent
known cost functions. These results can be seen as extensions of Lemma F.1. Formally, for ï¬xed p, consider
the following optimization problem
min
RÏÏ€s.t./hatwideR=A(cp,R,Ï€â€ ,/epsilon1â€ ), (P4)
The following lemmas characterize the value of the above optimization problem for diï¬€erent values of p.
In stating these lemmas, we use Î¦âˆˆR|S|Â·(|A|âˆ’1)Ã—|S|Â·|A|be a matrix with rows consisting of the vectors
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ as in Section E and let Î¦Î¸be the sub-matrix of Î¦consisting of the rows corresponding to Î˜/epsilon1â€ .
Lemma G.2. LetÏ€be a ï¬xed policy and assume that 1<p<âˆis a ï¬xed number. Deï¬ne the function
up:Râ†’Rasup(x) =sgn(x)Â·|x|1
pâˆ’1wheresgn(x) =1[x>0]âˆ’1[x<0]and letup:Rnâ†’Rnbe its
coordinate-wise extension to Rn, i.e,up(x)i=up(xi).
The solution to (P4)equals/hatwideÏÏ€if
/angbracketleftbig
ÏˆÏ€,up(Î¦T
Î¸Î»)/angbracketrightbig
â‰¥0âˆ€Î»<0,
and equalsâˆ’âˆotherwise.
Lemma G.3. LetÏ€be a ï¬xed policy. Deï¬ne the function uâˆ:Râ†’Ras
uâˆ(x) =/braceleftBigg
âˆ’1ifxâ‰¤0
1o.w.,
and letuâˆ:Rnâ†’Rnbe its coordinate-wise extension to Rn, i.e,uâˆ(x)i=uâˆ(x)i. The solution to (P4)
equals/hatwideÏÏ€if
/angbracketleftbig
ÏˆÏ€,uâˆ(Î¦T
Î¸Î»)/angbracketrightbig
â‰¥0âˆ€Î»<0s.t.Î»/negationslash= 0.
and equalsâˆ’âˆotherwise.
Lemma G.4. LetÏ€be a ï¬xed policy and let UâŠ†SÃ—Abe the set of state action pairs (Ëœs,Ëœa)for which the
following optimization problem is feasible.
âˆ€s,a:/vextendsingle/vextendsingle/parenleftbig
Î¦T
Î¸Î»/parenrightbig
(s,a)/vextendsingle/vextendsingleâ‰¤1and/parenleftbig
Î¦T
Î¸Î»/parenrightbig
(Ëœs,Ëœa) =âˆ’1andÎ»<0. (19)
Then the solution to (P4)isâˆ’âˆifÏ€(Ëœa|Ëœs)>0for some Ëœs,ËœaâˆˆUand is/hatwideÏÏ€otherwise.
28Published in Transactions on Machine Learning Research (01/2023)
When it is clear from context, we will drop the dependence on pinup. The proof of Lemmas G.2, G.3 and
G.4 are provided below. Note that Lemma G.4 immediately implies Theorem G.1 since the feasibility of (17)
already follows from Theorem 4.4.
Proof of Lemma G.2. Throughout the proof, we will drop the dependence on cp,/epsilon1â€ andÏ€â€ inA. The proof
follows a similar structure as the results for the /lscript2norm; namely, Lemmas E.1, E.3 and F.1.
We begin by analyzing the constraint A(R) =/hatwideRusing the KKT conditions. Since 1<p<âˆ, we can change
the objective to1
p/vextenddouble/vextenddoubleRâˆ’R/vextenddouble/vextenddoublep
pfor convenience. Of the four KKT conditions, primal feasiblity holds if and only
ifÎ¦TR4âˆ’/epsilon1. For the stationarity condition, forming the lagrangian of (P1), we obtain
L=1
p/vextenddouble/vextenddoubleRâˆ’R/vextenddouble/vextenddoublep
p+Î»T(Î¦Râˆ’/epsilon1â€ ) =/summationdisplay1
p(R(s,a)âˆ’R(s,a))p+Î»TÎ¦Râˆ’/epsilon1â€ Î»T1
Taking the gradient,
âˆ‡RL= 0â‡â‡’sgn(R(s,a)âˆ’R(s,a))Â·/vextendsingle/vextendsingleR(s,a)âˆ’R(s,a)/vextendsingle/vextendsinglepâˆ’1+/parenleftbig
Î¦TÎ»/parenrightbig
(s,a) = 0âˆ€s,a
â‡â‡’/parenleftbig
Î¦TÎ»/parenrightbig
(s,a) =sgn(R(s,a)âˆ’R(s,a))Â·/vextendsingle/vextendsingleR(s,a)âˆ’R(s,a)/vextendsingle/vextendsinglepâˆ’1âˆ€s,a
â‡â‡’/vextendsingle/vextendsingle/parenleftbig
Î¦TÎ»/parenrightbig
(s,a)/vextendsingle/vextendsingle1
pâˆ’1Â·sgn/parenleftbig/parenleftbig
Î¦TÎ»/parenrightbig
(s,a)/parenrightbig
=R(s,a)âˆ’R(s,a)âˆ€s,a
(i)â‡â‡’u(Î¦TÎ») =Râˆ’R,
where for (i)we have used the deï¬nition of u. the complementary slackness condition states that
Î»(s,a) = 0âˆ€(s,a)/âˆˆÎ˜/epsilon1â€ .
Finally, dual feasible states that Î»<0.
Returning to the condition A(R) =/hatwideR, primal feasibility always holds as /hatwideR=A(R). We therefore conclude
that a vector Rsatisï¬esA(R) =/hatwideRif and only if the following problem is feasible
R=/hatwideR+u(Î¦TÎ»)
Î»<0
Î»(s,a) = 0âˆ€(s,a)/âˆˆÎ˜/epsilon1â€ 
By deï¬nition of Î¦Î¸, this means that
A(R) =/hatwideRâ‡â‡’R=/hatwideR+u(Î¦T
Î¸Î»)for someÎ»<0.
Returning back to (P4), the problem can be rewritten as
min
R,Î»/angbracketleftÏˆÏ€,R/angbracketright
s.t.Î»<0
R=u(Î¦T
Î¸Î») +/hatwideR.
or equivalently,
min
Î»<0/angbracketleftbig
ÏˆÏ€,u(Î¦T
Î¸Î»)/angbracketrightbig
+/hatwideÏÏ€. (20)
Now, assume that/angbracketleftbig
ÏˆÏ€,u(Î¦T
Î¸Î»)/angbracketrightbig
<0for someÎ»<0. Observe that u(cÂ·x) =c1
1âˆ’pÂ·xfor positive constants
c>0. Therefore,
/angbracketleftbig
ÏˆÏ€,u/parenleftbig
Î¦T
Î¸(cÂ·Î»)/parenrightbig/angbracketrightbig
=c1
pâˆ’1Â·/angbracketleftbig
ÏˆÏ€,u/parenleftbig
Î¦T
Î¸Î»/parenrightbig/angbracketrightbig
Since 1<p<âˆ, lettingcâ†’âˆ, the value of/angbracketleftbig
ÏˆÏ€,u/parenleftbig
Î¦T
Î¸(cÂ·Î»)/parenrightbig/angbracketrightbig
can be made arbitrarily low. Therefore, the
value of (20)equalsâˆ’âˆ. Conversely, assume that/angbracketleftbig
ÏˆÏ€,u(Î¦T
Î¸Î»)/angbracketrightbig
â‰¥0for allÎ»<0. It follows that the value
of (20) is bigger than equal to /hatwideÏÏ€. SinceÎ»= 0is feasible for (20), the value is exactly /hatwideÏÏ€.
29Published in Transactions on Machine Learning Research (01/2023)
Before we prove Lemma G.3, we will state and prove the following Lemma which characterizes the subgradient
of the/lscriptâˆnorm.
Lemma G.5. The vectors w,ziâˆˆRnsatisfywâˆˆâˆ‚/bardblz/bardblâˆif and only if (a)z= 0and/bardblw/bardbl1â‰¤1or (b)
/bardblw/bardbl1= 1andzâˆˆ/tildewideu(w)where
/tildewideu(w) :=ï£±
ï£´ï£²
ï£´ï£³xâˆˆRn:xiâˆˆï£±
ï£´ï£²
ï£´ï£³{c}ifwi>0
{âˆ’c}ifwi<0
[âˆ’c,c]ifwi= 0for somecâ‰¥0ï£¼
ï£´ï£½
ï£´ï£¾
Proof.By Proposition A.22 in Bertsekas (1971),
âˆ‚/bardblz/bardblâˆ= conv{ws.t/bardblw/bardbl1â‰¤1,zTw=/bardblz/bardblâˆ}={ws.t/bardblw/bardbl1â‰¤1,zTw=/bardblz/bardblâˆ}. (21)
analyzing the above result, note that
zTw=/summationdisplay
ziwi
(a)
â‰¤/summationdisplay
|wi|/bardblz/bardblâˆ
=/bardblw/bardbl1Â·/bardblz/bardblâˆ
(b)
â‰¤/bardblz/bardblâˆ.
SincezTw=/bardblz/bardblâˆ, equality holds in (b), implying/bardblw/bardbl1= 1orz= 0and in (a), implyingziwi=|wi|/bardblz/bardblâˆ
for alli. This means that if |zi|</bardblz/bardblâˆ, thenwi= 0, and if|zi|=/bardblz/bardblâˆ, thenwiâ‰¥0ifziâ‰¥0andwiâ‰¤0if
ziâ‰¤0.
In other words (this time conditioning on w), if/bardblw/bardbl1<1, thenz= 0. Otherwise, if wi>0thenzi=/bardblz/bardblâˆ,
ifwi<0, thenzi=âˆ’/bardblz/bardblâˆand ifwi= 0thenzican be any value in [âˆ’/bardblz/bardblâˆ,/bardblz/bardblâˆ]. Therefore, the proof
follows by setting c=/bardblz/bardblâˆ. (the last condition is obviously true but it is important to make explicit as will
become clear later). This brings us to the following result.
We can now prove Lemma G.3.
Proof of Lemma G.3. The main ideas of the KKT analysis in the proof of Theorem G.2 still hold and the
only condition that changes is the stationarity slackness condition. Formally, the lagrangian equals
/vextenddouble/vextenddoubleRâˆ’R/vextenddouble/vextenddouble
âˆ+Î»T(Î¦Râˆ’/epsilon1â€ ).
Therefore, the stationarity condition can be written as
0âˆˆÎ¦TÎ»+âˆ‚/vextenddouble/vextenddoubleRâˆ’R/vextenddouble/vextenddouble
âˆâ‡â‡’ âˆ’ Î¦TÎ»âˆˆâˆ‚/vextenddouble/vextenddoubleRâˆ’R/vextenddouble/vextenddouble
âˆ(22)
By Lemma G.5, and given the fact that /tildewideu(âˆ’x) =âˆ’/tildewideu(x), the above condition holds if and only if
/parenleftbig
Râˆ’Râˆˆ/tildewideu(Î¦TÎ»)âˆ§/vextenddouble/vextenddoubleÎ¦TÎ»/vextenddouble/vextenddouble
1= 1/parenrightbig
âˆ¨/parenleftbig/vextenddouble/vextenddoubleÎ¦TÎ»/vextenddouble/vextenddouble
1â‰¤1âˆ§Râˆ’R= 0/parenrightbig
Combining with the complementary slackness and dual feasibility, (P4) can be rewritten as
min
v,Î»/hatwideÏÏ€+/angbracketleftÏˆÏ€,v/angbracketright
s.tÎ»<0
/parenleftbig
vâˆˆ/tildewideu(Î¦T
Î¸Î»)âˆ§/vextenddouble/vextenddoubleÎ¦T
Î¸Î»/vextenddouble/vextenddouble
1= 1/parenrightbig
âˆ¨/parenleftbig/vextenddouble/vextenddoubleÎ¦T
Î¸Î»/vextenddouble/vextenddouble
1â‰¤1âˆ§v= 0/parenrightbig
Now, observe that /tildewideu(cÂ·x) =/tildewideu(x)for allc>0. Therefore, the/vextenddouble/vextenddoubleÎ¦T
Î¸Î»/vextenddouble/vextenddouble
1= 1inside the ï¬rst clause of the last
constraint is equivalent to Î¦T
Î¸Î»/negationslash= 0since if Î¦T
Î¸Î»/negationslash= 0and/vextenddouble/vextenddoubleÎ¦T
Î¸Î»/vextenddouble/vextenddouble/negationslash= 1, then/vextenddouble/vextenddoubleÎ¦T
Î¸Î»/vextenddouble/vextenddouble= 1can be satisï¬ed by
30Published in Transactions on Machine Learning Research (01/2023)
replacingÎ»with1
/bardblÎ¦T
Î¸Î»/bardbl1Â·Î». Similarly, the/vextenddouble/vextenddoubleÎ¦T
Î¸Î»/vextenddouble/vextenddouble
1â‰¤1inside the second clause is redundant. Therefore, the
last constraint can be rewritten as
/parenleftbig
vâˆˆ/tildewideu(Î¦T
Î¸Î»)âˆ§Î¦T
Î¸Î»/negationslash= 0/parenrightbig
âˆ¨v= 0
Now, observe that Î¦T
Î¸Î»/negationslash= 0is equivalent to Î»/negationslash= 0as the rows of Î¦Î¸are independent; this is becasue the
only row with nonzero (s,a/negationslash=Ï€â€ (s))is the one corresponding to (s,a/negationslash=Ï€â€ (s)). Therefore, the optimization
problem can be rewritten as
min
v,Î»/hatwideÏÏ€+/angbracketleftÏˆÏ€,v/angbracketright
s.tÎ»<0
/parenleftbig
vâˆˆ/tildewideu(Î¦T
Î¸Î»)âˆ§Î»/negationslash= 0/parenrightbig
âˆ¨v= 0
Assume that there exists Î»/negationslash= 0satisfyingÎ»<0such that/angbracketleftbig
u(Î¦T
Î¸Î»),ÏˆÏ€/angbracketrightbig
<0. Then for any c > 0,
cÂ·u(Î¦T
Î¸Î»)âˆˆ/tildewideu(Î¦T
Î¸Î»)and therefore, the value of the optimization problem is âˆ’âˆ. Otherwise, the value is
lower bounded by /hatwideÏÏ€and the bound is attained with v= 0, concluding the proof.
Proof of Lemma G.4. We begin by analyzing (P1) as before. Forming the Lagrangian,
L=/vextenddouble/vextenddoubleRâˆ’R/vextenddouble/vextenddouble
1+Î»T(Î¦Râˆ’/epsilon1â€ )
Therefore, the stationarity condition states that
Î¦TÎ»âˆˆâˆ’âˆ‚/vextenddouble/vextenddoubleRâˆ’R/vextenddouble/vextenddouble
1
As before, the primal feasibility condition holds, dual feasibility states that Î»<0and complementary
slackness states that Î»(s,a) = 0for all (s,a)/âˆˆÎ˜/epsilon1â€ . Therefore,A(R) =/hatwideRif and only if
Î¦T
Î¸Î»âˆˆâˆ’âˆ‚/vextenddouble/vextenddouble/vextenddouble/hatwideRâˆ’R/vextenddouble/vextenddouble/vextenddouble
1(23)
Note however that vectors z,wsatisfywâˆˆâˆ‚/bardblz/bardbl1if and only if
wiï£±
ï£´ï£²
ï£´ï£³= 1ifzi>0,
=âˆ’1ifzi<0,
âˆˆ[âˆ’1,1]ifzi= 0â‡â‡’ziï£±
ï£´ï£²
ï£´ï£³â‰¥0ifwi= 1
â‰¤0ifwi=âˆ’1
= 0ifwiâˆˆ(âˆ’1,1)
Now, assume that Ï€(Ëœa|Ëœs)>0for some (Ëœs,Ëœa)for which (19)is feasible and Î»be the vector satisfying (19). By
(23), we conclude that the vector R(s,a) =/hatwideR(s,a)âˆ’tÂ·1[(s,a) = (Ëœs,Ëœa)]satisï¬esA(R) =/hatwideRfor anyt>0.
Therefore, if Ï€(Ëœa|Ëœs)>0, the solution to (P4)isâˆ’âˆ. Conversely, if (19)is not feasible for any such Ëœs,Ëœa, then
it follows that R(s,a)â‰¥/hatwideR(s,a)for allR,s,asatisfyingA(R) =/hatwideRandÏ€(a|s)>0. Therefore, the value of
the optimization problem is lower bounded by /hatwideÏÏ€. SinceA(/hatwideR) =/hatwideR, the lower bound is attainble which proves
the lemma.
G.4 Proof of Proposition 4.2
Statement: Letcconst(R,R/prime)be a constant cost function, and assume that cconstâˆˆC. Then problem (P2a)
is unbounded from below.
Proof.We will show that for any Ï€, the value of the inner optimization problem of (P2a)equalsâˆ’âˆ, thereby
proving the result.
LetÏ€be an arbitrary policy and consider an arbitrary vector R. By deï¬nition of cconstand the fact that /hatwideRis
feasible for (P1),/hatwideRâˆˆA(cconst,R,Ï€â€ ,/epsilon1â€ ). Therefore, R,cconstare feasible for (P2a). SinceRwas arbitrary, it
can be chosen such that the ÏÏ€is arbitrarily low, proving the claim.
31Published in Transactions on Machine Learning Research (01/2023)
G.5 Proof of Theorem 4.3
Statement: There exists MDP M= (S,A,/hatwideR,P,Î³,Ïƒ )for which problem (P2a)is unbounded from below
whencâˆâˆˆC.
Proof.LetÏ€be an arbitrary policy. We will build an example where the value of the inner minimization
problem of (P2a) is âˆ’âˆ.
Consider a single-state MDP with the reward function /hatwideR= (1,0,0,0)whereÏ€â€ is the ï¬rst action and /epsilon1â€ = 1.
Since/hatwideRissymmetricwithrespectto a2,a3,a4, weassumewithoutlossofgeneralitythat Ï€(a2)â‰¤Ï€(a3)â‰¤Ï€(a4).
For anyx, consider the reward function R/prime= (1âˆ’x,x,âˆ’x,âˆ’x)with actions a1,a2,a3,a4. We claim that
/hatwideRâˆˆA(câˆ,R/prime,Ï€â€ ,/epsilon1â€ ). To see why, consider the attack optimization problem (P1). The cost of the attack
optimization problem is at least xbecauseR/prime(Ï€â€ )âˆ’R/prime(a2) = 1âˆ’2xand any feasible /tildewideRneeds to satisfy
/tildewideR(Ï€â€ )âˆ’/tildewideR(a2)â‰¥1, which implies
/vextenddouble/vextenddouble/vextenddoubleR/primeâˆ’/tildewideR/vextenddouble/vextenddouble/vextenddouble
âˆâ‰¥max{/tildewideR(Ï€â€ )âˆ’R/prime(Ï€â€ ),R/prime(a2)âˆ’/tildewideR(a2)}
â‰¥1
2/parenleftBig
/tildewideR(Ï€â€ )âˆ’R/prime(Ï€â€ ) +R/prime(a2)âˆ’/tildewideR(a2)/parenrightBig
=1
2(2xâˆ’1 + 1) =x
Since/vextenddouble/vextenddouble/vextenddoubleR/primeâˆ’/hatwideR/vextenddouble/vextenddouble/vextenddouble
âˆ=x, we conclude /hatwideRâˆˆA(câˆ,R/prime,Ï€â€ ,/epsilon1â€ )as claimed.
We now consider the score of the policy Ï€under the reward function R/primeand show that, with the proper
choice ofx, it is unbounded from below. Since /hatwideRâˆˆA(câˆ,R/prime,Ï€â€ ,/epsilon1â€ ), this would show that the value of the
inner minimization in (P2a) equals âˆ’âˆ, proving the theorem.
By deï¬nition, ÏR/prime,Ï€can be written as
ÏR/prime,Ï€=Ï€(a1) +x(Ï€(a2)âˆ’Ï€(a1)âˆ’Ï€(a3)âˆ’Ï€(a4)) =Ï€(a1) +x(Ï€(a2)âˆ’(1âˆ’Ï€(a2))) =Ï€(a1) +x(2Ï€(a2)âˆ’1).
SinceÏ€(a2)â‰¤Ï€(a3)â‰¤Ï€(a4)however, we conclude that Ï€(a2)<0.5. Therefore, tending xto inï¬nity ï¬nishes
the proof.
H Computational hardness results
In this section, we provide computational hardness results for diï¬€erent choices of C, showing that the defense
optimization problem (P2a)is NP-hard in diï¬€erent cases. While our results are stated for the defense
optimization problem (P2a), all results hold for (P2b)as well with /epsilon1=min{/epsilon1D,/hatwide/epsilon1}. Our proofs rely on the
results of Appendix G and we therefore refer to this and rely on notation introduced in this Appendix; namely,
the notation Î¦Î¸andupas introduced in Lemma G.2.
H.1 Hardness result for p=âˆ
4.5 We begin by considering the case of câˆ. By reducing the 3SAT problem to the defense optimization
problem (P2a), we will show that (P2a) is NP-hard. More formally, we prove the following theorem.
Theorem H.1. ForC={câˆ}, it is NP-hard to determine whether the optimal value of problem (P2a)is
greater than or equal to /hatwideÏÏ€â€ .
We assume without loss of generality that the clauses do not contain duplicate variables as the 3SAT instance
remains NP-hard in this case.
Before proving the theorem, we prove a weaker result which essentially proves hardness assuming we can set
the matrix Î¦Î¸and vectorÂµÏ€â€ can be set arbitrarily, without the restriction that they correspond to an MDP.
32Published in Transactions on Machine Learning Research (01/2023)
PropositionH.2. Assume we are given an instance of 3SAT with clauses c1,...,cmand variables (x1,...xm).
Deï¬nek= 2n+ 1and/lscript= 4n+m+ 1. It is possible to build, in polynomial time, a matrix /tildewiderMâˆˆRkÃ—/lscript, a
vector/tildewideÂµâˆˆR/lscriptsuch that/tildewideÂµis strictly positive and
â€¢If the 3SAT instance is satisï¬able, then there exists a Î»/negationslash= 0âˆˆRksatisfyingÎ»<0such that
/angbracketleftBig
uâˆ(/tildewiderMTÎ»),/tildewideÂµ/angbracketrightBig
<âˆ’1
4.
In addition,/tildewiderMTÎ»does not contain any zero entries.
â€¢If the 3SAT instance is not satisï¬able, then for any Î»/negationslash= 0âˆˆRksatisfyingÎ»<0,
/angbracketleftBig
uâˆ(/tildewiderMTÎ»),/tildewideÂµ/angbracketrightBig
>1
4
Proof.We begin by an empty matrix and vector and in each step, add a value to /tildewideÂµand add a column of size
kto/tildewiderM.
We will let (a0,a1,...,an,b1,...,bn)be placeholder names for the coordinates of Î»Since the optimization
problem we are considering involves the matrix product /tildewiderMTÎ», for ease of notation, we will specify each
column of/tildewiderMby the linear map forming the corresponding coordinate of /tildewiderMTÎ». As an example, a0+ 4a3+ 5b3
is a column that has value 1 in a0, 4 ina3, 5 inb3and 0 everywhere else.
Deï¬ningc:= 500k, we ï¬rst add the columns below.
c/parenleftBigg
a0+n/summationdisplay
i=1ai+n/summationdisplay
i=1bi/parenrightBigg
(24)
0.9a0âˆ’(ai+bi)âˆ€iâˆˆ[n] (25)
âˆ’1.1a0+ (ai+bi)âˆ€iâˆˆ[n] (26)
4 (0.9a0âˆ’(aiâˆ’bi))âˆ€iâˆˆ[n] (27)
4 (0.9a0âˆ’(biâˆ’ai))âˆ€iâˆˆ[n] (28)
The value of/tildewideÂµfor(24),(25),(26),(27)and(28)is(6n2+mâˆ’1
2),3n,3n,1and1respectively. Note that
(24)is the all-ones vector. The constant cin(24)and the constant 4in(27)and(28)will not matter for the
proof, but will be useful for later.
Next for each clause with variables (xi,xj,xk), we will add the following column where didenotesaiif the
variablexiappears as positive in the clause and biif the variable appears as negative.
0.95Â·a0âˆ’(di+dj+dk) (29)
The value of/tildewideÂµin the above is 1.
We now prove that /tildewideÂµ,/tildewiderMhave the mentioned properties.
If the 3SAT instance is satisï¬able, this is easy to show. Set a0= 1and set (ai,bi)to(1,0)ifxiis
set to true in the satisï¬able arrangement and to (0,1)ifxiis set to 0. It is clear that the coordinates
ofu(/tildewiderMTÎ»)corresponding to (24),(25)and(26)equal 1,âˆ’1andâˆ’1respectively. Furthermore, since
{aiâˆ’bi,biâˆ’ai}={1,âˆ’1}, exactly half of the coordinates of u(/tildewiderMTÎ»)corresponding to (27)and(28)equal 1,
while the other half equal âˆ’1. Furthermore, all of the coordinates corresponding to (29)equal -1. Therefore,
the inner product/angbracketleftBig
/tildewideÂµ,u(/tildewiderMTÎ»)/angbracketrightBig
equals
(6n2+mâˆ’1
2)âˆ’nÂ·3nâˆ’nÂ·3n+ 0âˆ’m=âˆ’1
2
which proves the claim.
33Published in Transactions on Machine Learning Research (01/2023)
Conversely, assume that the 3SAT instance is not satisï¬able. Let s1,...s/lscriptdenote the coordinates of /tildewideÂµwiths1
corresponding to (24)and letu[s]denote/parenleftBig
u(/tildewiderMTÎ»)/parenrightBig
(s). LetÎ»/negationslash= 0satisfyingÎ»<0be an arbitrary vector.
First note that since Î»/negationslash= 0,/parenleftBig
/tildewiderMTÎ»/parenrightBig
(s1)is strictly positive and therefore u[s1] = 1. Since/tildewideÂµ(s1) = 6n2+mâˆ’1
2,
we need to show that
/angbracketleftBig
u(/tildewiderMTÎ»),/tildewideÂµ/angbracketrightBig
>1
4â‡â‡’/summationdisplay
s/negationslash=s1/tildewideÂµ(s)Â·u[s]>âˆ’6n2âˆ’m+1
2+1
4(30)
â‡â‡’/summationdisplay
s/negationslash=s1/tildewideÂµ(s)Â·(u[s] + 1)
2>1
2Â·/parenleftbigg
âˆ’6n2âˆ’m+3
4+ 2nÂ·3n+ 2nÂ·1 +m/parenrightbigg
(31)
â‡â‡’/summationdisplay
s/negationslash=s1/tildewideÂµ(s)Â·1[u[s] = 1]>n+3
8(32)
Now note that since the value of /tildewideÂµin coordinates corresponding to (25)and(26)is>n+3
8, ifu[s]equals
one in any of these coordinates, then the claim is proved.
Therefore, we assume w.l.o.g that for any i,
0.9a0â‰¤ai+biâ‰¤1.1a0.
This implies that if any of {ai,bi}iâ‰¥1is strictly positive, so is a0. Since at least one of {a0,ai,bi}needs to be
strictly positive by the assumption Î»/negationslash= 0, we conclude that a0is strictly positive.
Now note that u[s]is1in more than half of the coordinates corresponding to (27)and(28), then(32)will
hold. We can therefore assume w.l.o.g that at least half of these coordinates equal âˆ’1. We note however that
ifu[s]equalsâˆ’1forboth(27) and (28) for some 1â‰¤iâ‰¤n, then
0.9a0â‰¤aiâˆ’biâˆ§0.9a0â‰¤biâˆ’ai=â‡’a0â‰¤0 =â‡’a0= 0,
which is not possible. We can therefore conclude that for any ï¬xed i,u(/tildewiderMTÎ»)equals 1 in exactly one of the
two coordinates corresponding to (27)and(28). Therefore, either aiâˆ’biâ‰¥0.9a0orbiâˆ’aiâ‰¥0.9a0. Since
ai+biâ‰¥0.9a0, we get that either aiâ‰¥0.9a0orbiâ‰¥0.9a0. Sinceai+biâ‰¤1.1a0, the bigger one is â‰¥0.9a0
and the smaller one is â‰¤0.2a0.
Finally, note that if u[s]equals 1 for any of the coordinates corresponding to (29), then(32)would hold since
u[s]was already 1 for half of the coordinates corresponding to (27)and(28). We can therefore assume that
u[s]isâˆ’1for all the coordinates corresponding to (29). Note however that if di+dj+dkâ‰¥0.95a0, then at
least one of the dimust have been >0.2. This means that all of the clauses must hold true (in the 3SAT
sense) with xiset to
xi=/braceleftBigg
1,ifaiâ‰¥0.5
âˆ’1,otherwise.
This is not possible however as we assumed that the 3SAT instance was not satisï¬able.
Claim H.3. Deï¬ne/tildewiderMas above and let s1denote the column of the matrix corresponding to (24). For all
rows/tildewidevin/tildewiderM, we have
/tildewidev(s1)â‰¥50/summationdisplay
i>1|/tildewidev(si)|,
which further implies/summationtext
iâ‰¥1|/tildewidev(si)|â‰¤(1 + 1/50)/tildewidev(s1).
Proof.The claim follows trivially from the fact that as /tildewidev(s1) =cand all other entries in /tildewidevare less than 4.
Next, we prove the following lemma which essentially states that for any desired value of Î˜/epsilon1â€ , we can ï¬nd a
plausible reward function /hatwideRsuch that Î˜/epsilon1â€ equals this value.
34Published in Transactions on Machine Learning Research (01/2023)
Lemma H.4. LetMbe an ergodic MDP with unspeciï¬ed reward function and let Ï€â€ be a policy in this MDP.
For any value of /epsilon1â€ >0and any set of state-action pairs Î˜âŠ†SÃ—Asatisfying Î˜âˆ©{(s,Ï€(s)) :sâˆˆS}=âˆ…,
there is a reward function /hatwideRsuch that
1./hatwideRis feasible for the attack problem (P1), i.e, Î¦T/hatwideR4âˆ’/epsilon1â€ .
2.Î˜/epsilon1â€ = Î˜.
Proof.Consider the following reward function,
/hatwideR(s,a) =ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³0 ifa=Ï€â€ (s)
âˆ’/epsilon1â€ 
ÂµÏ€â€ {s;a}(s)ifaâˆˆÎ˜
âˆ’2Â·/epsilon1â€ 
ÂµÏ€â€ {s;a}(s)o.w.
It is clear that ÏÏ€â€ = 0and
ÏÏ€â€ {s;a}=/braceleftBigg
âˆ’/epsilon1â€ if(s,a)âˆˆÎ˜
âˆ’2/epsilon1â€ o.w.,for all (s,a/negationslash=Ï€â€ (s)).
which proves the claim.
We now use the above results to construct an MDP, formally proving Theorem H.1. Given an instance of
3SAT, let/tildewiderM,/tildewideÂµdenote the values speciï¬ed in Proposition H.2. Recall that /tildewiderMâˆˆRkÃ—/lscriptand/tildewideÂµâˆˆR/lscriptwhere
k= 2n+ 1and/lscript= 4n+m+ 1. LetÎ´ >1/2,Î³âˆˆ[1/2,1]be parameters to be speciï¬ed later. (see Claims
H.11, H.7, and H.5.) Intuitively, we need Î´to be close to zero and Î³to be close to one.
Given these values, we will build an MDP with reward vector /hatwideRwith/lscript+ 3states andk+ 1actions for which
|Î˜/epsilon1|=kand the 3SAT instance is satisï¬able if and only if
âˆƒÎ»/negationslash= 0âˆˆRk:Î»â‰¥0âˆ§/angbracketleftbig
uâˆ(Î¦T
Î¸Î»),ÏˆÏ€â€ /angbracketrightbig
<0. (33)
In our construction, the states siâ‰¥1will each correspond to the columns of /tildewiderMwhile the states sâˆ’2,sâˆ’1,s0
will be new. Furthermore, in state s0, each of the actions a/negationslash=Ï€â€ (s0)will correspond to a row of /tildewiderM.
LetMdenote the submatrix of Î¦Î¸with only columns corresponding to (s,Ï€â€ (s)). We ï¬rst observe that (33)
can be simpliï¬ed because ÏˆÏ€â€ (s,a)is 0 for alls,a/negationslash=Ï€â€ (s). Therefore, (33) is equivalent to
âˆƒÎ»/negationslash= 0âˆˆRk:Î»â‰¥0âˆ§/angbracketleftbig
uâˆ(MTÎ»),ÂµÏ€â€ /angbracketrightbig
<0. (34)
In order to specify this MDP, we ï¬rst specify the transition probabilities of Ï€â€ . In states0, followingÏ€â€ leads
tosiforiâˆˆ{âˆ’ 1,âˆ’2}with probability Î´/2and leadssiforiâ‰¥1with the probability (1âˆ’Î´)/hatwideÂµ(si)where
/hatwideÂµ=/tildewideÂµ
/bardbl/tildewideÂµ/bardbl1. (35)
In statess/negationslash=s0, followingÏ€â€ will lead back to swith probability 1. The initial distribution Ïƒof the MDP is
chosen asÏƒ(s0) = 1andÏƒ(si) = 0fori/negationslash= 0.
It is straightforward to see that
ÂµÏ€â€ (s0) = (1âˆ’Î³),âˆ€iâˆˆ{âˆ’ 2,âˆ’1}:ÂµÏ€â€ (si) =Î³Â·Î´
2,âˆ€iâ‰¥1 :ÂµÏ€â€ (si) =Î³Â·(1âˆ’Î´)Â·/tildewideÂµ(si)
/bardbl/tildewideÂµ/bardbl1.
Now, for each of the rows in /tildewiderMlike/tildewidev, we add an action ato states0with the following transition probabilities.
P(s0,a,sj) =ï£±
ï£´ï£²
ï£´ï£³P(s0,Ï€â€ (s0),sj) ifj= 0
P(s0,Ï€â€ (s0),sj)âˆ’Î´
4Î¸Â·(/summationtext
i/tildewidev(si))ifjâˆˆ{âˆ’ 2,âˆ’1}
P(s0,Ï€â€ (s0),sj) +Î´
2Î¸Â·/tildewidev(sj)ifjâ‰¥1. (36)
35Published in Transactions on Machine Learning Research (01/2023)
whereÎ¸is taken to be large enough such that for all /tildewidev,
Î¸Â·min
i/hatwideÂµ(si)â‰¥/bardbl/tildewidev/bardbl1.
Note that the value of Î¸is the same for all the rows of matrix; it is set to the maximum of1
mini/hatwideÂµiÂ·/bardbl/tildewidev/bardbl1
across all rows /tildewidevof/tildewiderM.
Claim H.5. IfÎ´â‰¤1
10, the transition probabilities speciï¬ed in (35)arevalid, i.e., they are in the range [0,1]
and/summationtext
s/primeP(s0,a,s/prime) = 1.
Proof.In order to make sure these transition probabilities are valid, they need to sum to one and they all
need to be non-negative. They sum to one by deï¬nition. As for being non-negative, it holds trivially for
j= 0and holds for j/negationslash= 0by deï¬nition of Î´. Formally, for jâ‰¤âˆ’1,
P(s0,Ï€â€ (s0),s0)âˆ’Î´
4Â·/parenleftBigg/summationdisplay
iv(si)/parenrightBigg
=Î´Â·/parenleftbigg1
2âˆ’1
4Â·/summationtext
i/tildewidev(si)
Î¸/parenrightbigg
â‰¥Î´/parenleftbigg1
2âˆ’1
4Â·(min
i/hatwideÂµi)Â·/summationtext/tildewidev(si)
/bardbl/tildewidev/bardbl1/parenrightbigg
â‰¥Î´(1
2âˆ’1
4)
>0
and forjâ‰¥1,
P(s0,Ï€â€ (s0),sj) +Î´
2Â·/tildewidev(sj)
Î¸â‰¥P(s0,Ï€â€ (s0),sj)âˆ’Î´
2Â·|/tildewidev(sj)|
Î¸
â‰¥(1âˆ’Î´) min
i/hatwideÂµiâˆ’Î´
2Â·/tildewidev(sj)
/bardbl/tildewidev/bardbl1Â·/parenleftBig
min
i/hatwideÂµi/parenrightBig
â‰¥(min
i/hatwideÂµi)Â·(1âˆ’3Î´
2)>0
Now, using Lemma H.4, set the reward for the MDP such that Î˜/epsilon1â€ consists of the added actions in s0.
Note that there are multiple actions in the states si/negationslash=0as well; however, given Lemma H.4, their transition
probabilities are not important and can be set arbitrarily as long as the MDP remains ergodic.
Claim H.6. For alla,/negationslash=Ï€â€ (s0),
(MTÎ»)(a) =/parenleftbigg
âˆ’Î´
4Î¸Â·Î³Â·(/summationdisplay
/tildewidev(sj)),âˆ’Î´
4Î¸Â·Î³Â·(/summationdisplay
/tildewidev(sj)),âˆ’(1âˆ’Î³), Î³Â·Î´
2Î¸Â·/tildewidev/parenrightbigg
,
where/tildewidevis the row corresponding to aand we have abused notation by using Î³Â·Î´
2Â·/tildewidevto denote the last k
entries of the vector.
Proof.As before, it is straightforward to see that ÂµÏ€â€ {s0;a}(s0) = (1âˆ’Î³)and
âˆ€iâˆˆ{âˆ’ 2,âˆ’1}:ÂµÏ€â€ {s0;a}(si) =Î³Â·/parenleftbiggÎ´
2âˆ’Î´
4Î¸Â·(/summationdisplay
/tildewidev(sj))/parenrightbigg
,
and
âˆ€iâ‰¥1 :ÂµÏ€â€ {s0;a}(si) =Î³Â·/parenleftbigg
(1âˆ’Î´)Â·/hatwideÂµ(si) +Î´
2Î¸Â·/tildewidev(si)/parenrightbigg
.
36Published in Transactions on Machine Learning Research (01/2023)
Claim H.7. Ifmax{Î´,1âˆ’Î³}â‰¤1
4/bardbl/tildewideÂµ/bardbl1,then
/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´)ï£«
ï£­0/summationdisplay
j=âˆ’2ÂµÏ€â€ (sj)ï£¶
ï£¸â‰¤1/8
Proof.
/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´)Â·ï£«
ï£­0/summationdisplay
j=âˆ’2ÂµÏ€â€ (sj)ï£¶
ï£¸=Î´Â·Î³+ (1âˆ’Î³)
Î³(1âˆ’Î´)Â·/bardbl/tildewideÂµ/bardbl1 (37)
â‰¤1
4Â·/bardbl/tildewideÂµ/bardbl1Â·(Î´+ (1âˆ’Î³))â‰¤1
8(38)
Claim H.8. The 3SAT instance is satisï¬able if and only if (34)holds.
Proof.Given Claim H.6, the sub-matrix of Mcorresponding to columns sjâ‰¥1equals
Î³Â·Î´
2Â·Î¸Â·/tildewiderM.
Letuâˆ[s]denoteuâˆ/parenleftbig
(MTÎ»)(s)/parenrightbig
. For anyÎ»,
/angbracketleftbig
ÂµÏ€â€ ,u(MTÎ»)/angbracketrightbig
=0/summationdisplay
j=âˆ’2ÂµÏ€â€ (sj)uâˆ[sj] +/angbracketleftbiggÎ³Â·(1âˆ’Î´)
/bardbl/tildewideÂµ/bardbl1Â·/tildewideÂµ,u(Î³Â·Î´
2Â·Î¸Â·/tildewiderMTÎ»)/angbracketrightbigg
=0/summationdisplay
j=âˆ’2ÂµÏ€â€ (sj)uâˆ[sj] +/angbracketleftbiggÎ³Â·(1âˆ’Î´)
/bardbl/tildewideÂµ/bardbl1Â·/tildewideÂµ,u(/tildewiderMTÎ»)/angbracketrightbigg
Multiplying both sides by/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´), it follows that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´)Â·/angbracketleftbig
ÂµÏ€â€ ,u(MTÎ»)/angbracketrightbig
âˆ’/angbracketleftBig
/tildewideÂµ,u(/tildewiderMTÎ»)/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ‰¤/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´)Â·ï£«
ï£­0/summationdisplay
j=âˆ’2|ÂµÏ€â€ (s)uâˆ[s]|ï£¶
ï£¸ (39)
â‰¤/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´)Â·ï£«
ï£­0/summationdisplay
j=âˆ’2ÂµÏ€â€ (s)ï£¶
ï£¸ (40)
â‰¤1
8(41)
Therefore, by Lemma H.4, if the 3SAT is satisï¬able, then there exists a Î»such that
/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´)Â·/angbracketleftbig
ÂµÏ€â€ ,u(MTÎ»)/angbracketrightbig
â‰¤âˆ’1
4+1
8<0.
If the 3SAT is not satisï¬able, then by Lemma H.4, for all feasible Î»,
/bardbl/tildewideÂµ/bardbl1
Î³(1âˆ’Î´)Â·/angbracketleftbig
ÂµÏ€â€ ,u(MTÎ»)/angbracketrightbig
â‰¥1
4âˆ’1
8>0
which proves the claim.
37Published in Transactions on Machine Learning Research (01/2023)
H.2 Proof of Theorem 4.5
Statement: ForC={cps.t.pâˆˆ[1,âˆ)}, it is NP-hard to determine whether the optimal value of problem
(P2b)is greater than or equal to /hatwideÏÏ€â€ We will prove the result using the same MDP as the previous Section.
We need to show the 3SAT instance is satisï¬able if and only if
âˆƒpâˆˆ[1,âˆ),Î»âˆˆRk:Î»â‰¥0âˆ§/angbracketleftbig
up(Î¦T
Î¸Î»),ÏˆÏ€â€ /angbracketrightbig
<0.
As before, let Mdenote the submatrix of Î¦Î¸with only columns corresponding to (s,Ï€â€ (s)). As in the
previous section we note that because ÏˆÏ€â€ (s,a)is 0 for alls,a/negationslash=Ï€â€ (s), the above condition is equivalent to
âˆƒpâˆˆ[1,âˆ),Î»âˆˆRk:Î»â‰¥0âˆ§/angbracketleftbig
up(MTÎ»),ÂµÏ€â€ /angbracketrightbig
<0. (42)
We split the proof into two lemmas.
Lemma H.9. If the 3SAT instance is satisï¬able, then (42)holds.
Proof.In this case, then the same proof as before basically holds. Formally, consider the Î»used before in
the proof and consider the vector MTÎ». We need to show that there exists pfor which/angbracketleftbig
ÂµÏ€â€ ,up(MTÎ»)/angbracketrightbig
is
negative. Note however that
/angbracketleftbig
ÂµÏ€â€ ,up(MTÎ»)/angbracketrightbig
=k/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)Â·up/parenleftbig
(MTÎ»)(si)/parenrightbig
For ï¬xedx/negationslash= 0,up(x)is a continuous function of pandlimâˆup(x) =uâˆ(x). If we show that all of the
coordinates in MTÎ»are non-zero, this would imply that/angbracketleftbig
ÂµÏ€â€ ,up(MTÎ»)/angbracketrightbig
converges to/angbracketleftbig
ÂµÏ€â€ ,uâˆ(MTÎ»)/angbracketrightbig
<0
for large enough pwhich proves the claim. Therefore,/angbracketleftbig
ÂµÏ€â€ ,up(MTÎ»)/angbracketrightbig
is continuous in pand letting pbe
large enough proves the claim.
It remains to verify that all of the coordinates in MTÎ»used in the above proof were non-zero. Since all of
the coordinates of /tildewiderMTÎ»were non-zero by our construction of Î», we conclude that MTÎ»is non-zero on siâ‰¥1.
Foriâˆˆ{âˆ’ 2,âˆ’1}, given claim H.3, we have/summationtext
iâ‰¥1/tildewidev(si)>0, for all rows/tildewidevof/tildewiderM, which means MTÎ»is strictly
negative on these states. Finally, the entry corresponding to s0equalsâˆ’ÂµÏ€â€ (s0)in all rows of M, which
implies/parenleftbig
MTÎ»/parenrightbig
(s0)is strictly negative, proving the claim.
Lemma H.10. If(42)holds, then the 3SAT instance is satisï¬able.
Proof.We assume that Î»/negationslash= 0without loss of generality; if Î»= 0then/angbracketleftbig
ÂµÏ€â€ ,up(MTÎ»)/angbracketrightbig
= 0which is not
negative.
Lettingup[s]denoteup/parenleftbig
(MTÎ»)(si)/parenrightbig
anddp[s]denoteÂµÏ€â€ (s)Â·up[s], then/angbracketleftbig
ÂµÏ€â€ ,up(MTÎ»)/angbracketrightbig
can be rewritten as
/angbracketleftbig
ÂµÏ€â€ ,up(MTÎ»)/angbracketrightbig
=/lscript/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)Â·up[si] (43)
=0/summationdisplay
i=âˆ’2dp[si] +/summationdisplay
(24)dp[s] +/summationdisplay
(25)dp[s] +/summationdisplay
(26)dp[s] +/summationdisplay
(27)dp[s] +/summationdisplay
(28)dp[s] +/summationdisplay
(29)dp[s](44)
where in the above, we have broken the sum in diï¬€erent parts, depending on what sicorresponds to and we
have abused the notation by using the set(24)denote all states that correspond to Equation (24). Note that
the sum corresponding to (24) consists of a single state. We will also use s1to denote this state.
We begin by stating some properties of upanddp.
Claim H.11. LetÎ³be close enough to 1 such that
cÎ´
4Î¸â‰¥(1âˆ’Î³).
38Published in Transactions on Machine Learning Research (01/2023)
Then for all rows /tildewidevfrom/tildewiderM,
âˆ€s/negationslash=s1:|up[s]|â‰¤up[s1] (45)
Proof.We need to show that for all rows vinM,|v(s1)|â‰¥|v(s)|for alls/negationslash=s1. Foriâ‰¥1, this follows
immediately from Claim H.3 and Claim H.6.
As fors0, we need to show that
v(s1)â‰¥|v(s0)| â‡â‡’Î³Â·Î´
2Î¸/tildewidev(s1)â‰¥(1âˆ’Î³)
Note however that
Î³Â·Î´
2Î¸/tildewidev(s1)â‰¥Î´
4Î¸/tildewidev(s1)â‰¥(1âˆ’Î³)
by the assumption on Î³.
Claim H.12. For anyiâˆˆ[n], letting ËœsiandËœs/prime
idenote the states corresponding to (27)and(28)respectively.
dp[Ëœsi] +dp[Ëœs/prime
i]â‰¥0.
Proof.To prove this, observe that
0.9a0âˆ’(aiâˆ’bi) + 0.9a0âˆ’(biâˆ’ai) = 1.8a0â‰¥0
There are therefore two possibilities: (a)Both 0.9a0âˆ’(aiâˆ’bi)and0.9a0âˆ’(biâˆ’ai)are non-negative.
In this case, both dp[Ëœsi]anddp[Ëœs/prime
i]are non-negative and the claim follows. (b)0.9a0âˆ’(aiâˆ’bi)â‰¥0
and0.9a0âˆ’(biâˆ’ai)â‰¤0or vice versa. Assume w.l.o.g that 0.9a0âˆ’(aiâˆ’bi)â‰¥0, i.e,dp[Ëœsi]â‰¥0. In
this case,|0.9a0âˆ’(aiâˆ’bi)|â‰¥|0.9a0âˆ’(biâˆ’ai)|and therefore, since ÂµÏ€â€ (Ëœsi) =ÂµÏ€â€ (Ëœs/prime
i), we conclude that
dp[Ëœsi]â‰¥|dp[Ëœs/prime
i]|and therefore the claim follows.
We split the proof into several cases.
Case 1: There exists sâˆˆ(25)âˆª(26) such that dp[s]â‰¥0.
Letsbe such a state In this case, deï¬ne /tildewidedp[s]as follows
/tildewidedp[s] =ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³0 ifs=s
0 ifsâˆˆ(27)âˆª(28)
dp[s]ifs=s1
âˆ’|dp[s]|o.w.
We ï¬rst claim that/summationtextdp[s]â‰¥/summationtext/tildewidedp[s]. To prove this, note that dp[s]â‰¥/tildewidedp[s]for alls /âˆˆ(27)âˆª(28). We
therefore need to prove that
/summationdisplay
sâˆˆ(27)âˆª(28)dp[s]â‰¥0,
which follows from Claim H.12. by summing over all iâˆˆ[n].
39Published in Transactions on Machine Learning Research (01/2023)
Deï¬ning/tildewideS:=S\({s1,s}âˆª(27)âˆª(28)), we note that
/summationdisplay
s/tildewidedp[s] =dp[s1]âˆ’/summationdisplay
sâˆˆ/tildewideS|dp[si]|
=ÂµÏ€â€ (s1)Â·up[s1]âˆ’/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)Â·|up[si]|
(45)
â‰¥ÂµÏ€â€ (s1)Â·up[s1]âˆ’/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)Â·|up[s1]|
=|up[s1]|ï£«
ï£­ÂµÏ€â€ (s1)âˆ’/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)ï£¶
ï£¸
Deï¬ning/tildewideS+:=/tildewideS\{sâˆ’2,sâˆ’1,s0},
/tildewideÂµ(s1)âˆ’/summationdisplay
sâˆˆ/tildewideS+/tildewideÂµ(s) = 6n2+mâˆ’1
2âˆ’(2nâˆ’1)Â·3nâˆ’m
= 6n2+mâˆ’1
2âˆ’6n2+ 3nâˆ’m
= 3nâˆ’1
2>1
4,
Claim H.7 now gives a contradiction as the inner product in (42) becomes positive.
Case 2:dp[s]<0for allsâˆˆ(25)âˆª(26) and|ajâˆ’bj|â‰¤0.9a0for somejâˆˆ[n].
In this case, we conclude that
ai+biâˆˆ[0.9,1.1]a0âˆ€iâˆˆ[1,n]
Since either aiorbimust be strictly positive for some i(becauseÎ»/negationslash= 0), we conclude that a0>0.
Similar to case 1, we introduce a new vector /tildewidedpsuch that/summationtext
sdp[s]â‰¥/summationtext
s/tildewidedp[s].
Let/tildewidesjand/tildewides/prime
jdenote the states corresponding to (27)and(28)respectively for i=j. Note that by assumption,
up[/tildewidesj],up[/tildewides/prime
j]â‰¥0. Assume without loss of generality that up[/tildewidesj]â‰¥up[/tildewides/prime
j]. Deï¬ne the vector /tildewidedpas
/tildewidedp[s] =ï£±
ï£´ï£²
ï£´ï£³0 ifsâˆˆ(27)âˆª(28)\{/tildewidesj}
dp[s]ifsâˆˆ{s1,/tildewidesj}
âˆ’|dp[s]|o.w.
As before,/summationtextdp[s]â‰¥/summationtext/tildewidedp[s]. More formally, for (27)and(28), we use Claim H.12 and for all the other
states, we have dp[s]â‰¥/tildewidedp[s].
We now claim that
âˆ€sâˆˆ(25)âˆª(26)âˆª(29):|up[s]|â‰¤up[/tildewidesj]. (46)
This is because
(MTÎ»)(/tildewidesj)â‰¥(MTÎ»)(/tildewidesj) + (MTÎ»)(/tildewides/prime
j)
2= 3.6a0,
while/vextendsingle/vextendsingle(MTÎ»)(s)/vextendsingle/vextendsingleâ‰¤2.35a0for allsâˆˆ(25)âˆª(26)âˆª(29). This is because ai+biâˆˆ[0.9Â·a0,1.1Â·a0].
40Published in Transactions on Machine Learning Research (01/2023)
Deï¬ning/tildewideS:=(25)âˆª(26)âˆª(29), we conclude that
/summationdisplay
sdp[s]â‰¥/summationdisplay
s/tildewidedp[s]
=ÂµÏ€â€ (s1)Â·up[s1] +ÂµÏ€â€ (/tildewidesj)Â·up[/tildewidesj] +/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)Â·âˆ’|up[s]|+0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)Â·âˆ’|up[si]|
â‰¥ÂµÏ€â€ (s1)Â·up[s1] +ÂµÏ€â€ (/tildewidesj)Â·up[/tildewidesj] +/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)Â·âˆ’|up[/tildewidesj]|+0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)Â·âˆ’|up[s1]|
=/parenleftBigg
ÂµÏ€â€ (s1)âˆ’0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)/parenrightBigg
Â·up[s1] +ÂµÏ€â€ (/tildewidesj)Â·up[/tildewidesj] +/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)Â·âˆ’|up[/tildewidesj]|
â‰¥/parenleftBigg
ÂµÏ€â€ (s1)âˆ’0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)/parenrightBigg
Â·up[/tildewidesj] +ÂµÏ€â€ (/tildewidesj)Â·up[/tildewidesj] +/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)Â·âˆ’|up[/tildewidesj]|
=up[/tildewidesj]Â·ï£«
ï£­ÂµÏ€â€ (s1) +ÂµÏ€â€ (/tildewidesj)âˆ’0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)âˆ’/summationdisplay
sâˆˆ/tildewideSÂµÏ€â€ (s)ï£¶
ï£¸
Here, for the ï¬nal inequality we have used the fact that ÂµÏ€â€ (s1)âˆ’/summationtext0
i=âˆ’2ÂµÏ€â€ (si)â‰¥0,which follows from
Claim H.7.
As before, note that
/tildewideÂµ(s1) +/tildewideÂµ(/tildewidesj)âˆ’/summationdisplay
sâˆˆ/tildewideS/tildewideÂµ(s) = 6n2+mâˆ’1
2+ 1âˆ’2nÂ·3nâˆ’mâ‰¥1
2
Claim H.7 now gives a contradiction as the inner product in (42) becomes positive.
Case 3:dp[s]<0for allsâˆˆ(25)âˆª(26) and|aiâˆ’bi|â‰¥0.9a0for all 1â‰¤iâ‰¤n.
Similar as before, deï¬ne /tildewidedpas
/tildewidedp[s] =ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³0 ifsâˆˆ(27)âˆª(28)
dp[s] ifs=s1
min{dp[s],0}ifsâˆˆ(29)
âˆ’|dp[s]|o.w.
As before,/summationtext
sdp[s]â‰¥/summationtext
s/tildewidedp[s]. Therefore,
/summationdisplay
sdp[s]â‰¥/summationdisplay
s/tildewidedp[s]
=ÂµÏ€â€ (s1)Â·up[s1]âˆ’/summationdisplay
sâˆˆ(25)âˆª(26)ÂµÏ€â€ (s)Â·|up[s]|+/summationdisplay
sâˆˆ(29)ÂµÏ€â€ (s)Â·min{up[s],0}âˆ’0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)Â·|up[si]|
â‰¥ÂµÏ€â€ (s1)Â·up[s1]âˆ’/summationdisplay
sâˆˆ(25)âˆª(26)ÂµÏ€â€ (s)Â·|up[s1]|âˆ’/summationdisplay
sâˆˆ(29)ÂµÏ€â€ (s)Â·|up[s1]|1[up[s]<0]âˆ’0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)Â·|up[s1]|
=up[s1]Â·ï£«
ï£­ÂµÏ€â€ (s1)âˆ’/summationdisplay
sâˆˆ(25)âˆª(26)ÂµÏ€â€ (s)âˆ’/summationdisplay
sâˆˆ(29)ÂµÏ€â€ (s)1[up[s]<0]âˆ’0/summationdisplay
i=âˆ’2ÂµÏ€â€ (si)ï£¶
ï£¸
Assume for the sake of contradiction that the 3SAT instance is not satisï¬able. There are two possibilities.
The ï¬rst is that for at least one sâˆˆ(29), we have up[s]â‰¥0. In that case,
/tildewideÂµ(s1)âˆ’/summationdisplay
sâˆˆ(25)âˆª(26)/tildewideÂµ(s)âˆ’/summationdisplay
sâˆˆ(29)/tildewideÂµ(s)1[up[s]<0]â‰¥1
2
41Published in Transactions on Machine Learning Research (01/2023)
As before, Claim H.7 gives a contradiction. Otherwise, up[s]<0for allsâˆˆ(29). In that case, we have
/angbracketleftBig
uâˆ(/tildewiderMTÎ»),/tildewideÂµ/angbracketrightBig
= 6n2+mâˆ’1/2âˆ’2nÂ·3nâˆ’m<âˆ’1/2,
which contradicts Proposition H.2 as Î»/negationslash= 0.
I Proofs of Appendix B
In this section, we provide a more formal treatment of the results in Appendix B, formally stating and proving
these results.
Proposition I.1. Assume that condition (3)holds. Set/hatwide/epsilon1as
/hatwide/epsilon1= min
s,a/negationslash=Ï€â€ (s)/bracketleftBig
/hatwideÏÏ€â€ âˆ’/hatwideÏÏ€â€ {s;a}/bracketrightBig
.
Consider the following policy
Ï€D(a|s) =1[aâˆˆÎ˜/epsilon1
sâˆª{Ï€â€ (s)}]
|Î˜/epsilon1s|+ 1. (47)
Equation (47)characterizes the solution to the optimization problems (P2a)and(P2b)with parameters /epsilon1=/epsilon1â€ 
and/epsilon1= min{/hatwide/epsilon1,/epsilon1D}respectively. Furthermore, in both cases ÏÏ€D=/hatwideÏÏ€D
Proof.Given Theorem 5.1, Theorem 5.2, and Lemma D.3, it suï¬ƒces to show that if /epsilon1â‰¤/hatwide/epsilon1, the solution to the
optimization problem
max
ÏˆâˆˆÎ¨/angbracketleftBig
Ïˆ,/hatwideR/angbracketrightBig
(P4)
s.t./angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,Ïˆ/angbracketrightBig
â‰¥0âˆ€s,aâˆˆÎ˜/epsilon1,
corresponds to the occupancy measure of policy Ï€Ddeï¬ned by Equation (47). Namely, the optimization
problems (P2a)and(P2b)correspond to the optimization problem (P3)with parameters /epsilon1=/epsilon1â€ and
/epsilon1=min{/hatwide/epsilon1,/epsilon1D}respectively. Since /hatwide/epsilon1â‰¤/epsilon1â€ , the primal feasibility condition in Lemma E.1 implies that the
solution to the above optimization problem characterizes both cases ((P2a) and (P2b)).
Now, due to Lemma D.3, we have
ÏˆâˆˆÎ¨â‡â‡’Ïˆ<0âˆ§âˆ€s:/summationdisplay
aÏˆ(s,a) = (1âˆ’Î³)Ïƒ(s) +Î³/summationdisplay
Ëœs,ËœaP(Ëœs,Ëœa,s)Ïˆ(Ëœs,Ëœa).
SinceP(Ëœs,Ëœa,s)is independent of Ëœa, the second condition is equivalent to
âˆ€s:/summationdisplay
aÏˆ(s,a) = (1âˆ’Î³)Ïƒ(s) +Î³/summationdisplay
Ëœs/parenleftBig
P(Ëœs,Ï€â€ (Ëœs),s)(/summationdisplay
ËœaÏˆ(Ëœs,Ëœa))/parenrightBig
,
which, due to (12), is equivalent to
/summationdisplay
aÏˆ(s,a) =Âµ(s).
Furthermore, given the independence of the transition distributions from policies, we have the following
/parenleftbig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ /parenrightbig
(Ëœs,Ëœa) =ï£±
ï£´ï£²
ï£´ï£³Âµ(s)if(Ëœs,Ëœa) = (s,a)
âˆ’Âµ(s)if(Ëœs,Ëœa) = (s,Ï€â€ (s))
0o.w. (48)
42Published in Transactions on Machine Learning Research (01/2023)
Therefore, the constraint/angbracketleftbig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,Ïˆ/angbracketrightbig
â‰¥0is equivalent to Ïˆ(s,a)â‰¥Ïˆ(s,Ï€â€ (s)). Furthermore, note
that
(s,a)âˆˆÎ˜/epsilon1â‡â‡’/hatwideÏÏ€â€ {s;a}âˆ’/hatwideÏÏ€â€ =âˆ’/epsilon1â€ 
â‡â‡’/angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,/hatwideR/angbracketrightBig
â‰¤âˆ’/epsilon1â€ 
â‡â‡’/hatwideR(s,a)âˆ’/hatwideR(s,Ï€â€ (s)) =âˆ’/epsilon1â€ 
Âµ(s)
â‡â‡’aâˆˆÎ˜/epsilon1
s.
Putting it all together, the optimization problem (P3) is equivalent to
max
Ïˆ/angbracketleftBig
/hatwideR,Ïˆ/angbracketrightBig
s.t.Ïˆ(s,Ï€â€ (s))â‰¤Ïˆ(s,a)âˆ€s,aâˆˆÎ˜/epsilon1
s/summationdisplay
aÏˆ(s,a) =Âµ(s)âˆ€sâˆˆS
Ïˆ(s,a)â‰¥0âˆ€(s,a).
Note that the maximization is now over all vectors ÏˆâˆˆR|S|.|A|as the constraint ÏˆâˆˆÎ¨has been made
explicit. Furthermore, given Lemma D.3 and Equation (5), any vector Ïˆsatisfying the last two constraints
(the bellman constraints) corresponds to a policy Ï€through
Ï€(a|s) =Ïˆ(s,a)
Âµ(s).
In other words, probability of choosing ain statesis proportional to Ïˆ(s,a).
Now, let us analyze the solution to this optimization problem which we will denote by Ïˆmax. This solution
Ïˆmaxexists, since the optimization problem is maximizing a continuous function on a closed and bounded set.
We ï¬rst claim that if a /âˆˆÎ˜/epsilon1
sâˆª{Ï€â€ (s)}, thenÏˆmax(s,a) = 0. If this is not the case, then Ïˆmaxis not optimal.
Concretely, consider the following vector Ïˆ
Ïˆ(Ëœs,Ëœa) =ï£±
ï£´ï£²
ï£´ï£³Ïˆmax(Ëœs,Ëœa) +1
|Î˜/epsilon1s|+1Ïˆmax(s,a)ifËœs=sâˆ§ËœaâˆˆÎ˜/epsilon1
sâˆª{Ï€â€ (s)}
0ifËœs=sâˆ§Ëœa=a
Ïˆmax(Ëœs,Ëœa)o.w..
In other words, we uniformly spread the probability of choosing action ain statesover the set Î˜/epsilon1
sâˆª{Ï€â€ (s)}.
The vector Ïˆstill satisï¬es the constraints: if ËœaâˆˆÎ˜/epsilon1
s,Ïˆ(s,Ï€â€ (s))âˆ’Ïˆ(s,Ëœa) =Ïˆmax(s,Ï€â€ (s))âˆ’Ïˆmax(s,Ëœa)and
the objective has strictly improved because
/hatwideÏÏ€â€ {s;a}âˆ’/hatwideÏÏ€â€ â‰¤âˆ’/hatwide/epsilon1â‰¤âˆ’/epsilon1=â‡’/hatwideR(s,a)â‰¤/hatwideR(s,Ï€â€ (s))âˆ’/epsilon1
Âµ(s).
Sincea /âˆˆÎ˜/epsilon1
s, the inequality is strict and therefore
âˆ€ËœaâˆˆÎ˜/epsilon1
sâˆª{Ï€â€ (s)}:/hatwideR(s,Ëœa)>/hatwideR(s,a).
This means that Ïˆwas not optimal, contradicting the initial assumption.
Now note that if Ïˆmax(s,a)> Ïˆ max(s,Ï€â€ (s))for someaâˆˆÎ˜/epsilon1
s, then again Ïˆmaxisnâ€™t optimal as we could
replace it with
Ïˆ(Ëœs,Ëœa) =ï£±
ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£³Ïˆmax(Ëœs,Ëœa) +Ïˆmax(s,a)âˆ’Ïˆmax(s,Ï€â€ (s))
|Î˜/epsilon1s|+ 1ifËœs=sâˆ§ËœaâˆˆÎ˜/epsilon1
sâˆª{Ï€â€ (s)}\{a}
Ïˆmax(Ëœs,Ëœa)âˆ’|Î˜/epsilon1
s|(Ïˆmax(s,a)âˆ’Ïˆmax(s,Ï€â€ (s)))
|Î˜/epsilon1s|+ 1ifËœs=sâˆ§Ëœa=a
Ïˆmax(Ëœs,Ëœa)o.w..
43Published in Transactions on Machine Learning Research (01/2023)
Intuitively, since the action awas being chosen with strictly higher probability than action Ï€â€ (s), we have
uniformly spread this excess probability among the set Î˜/epsilon1
sâˆª{Ï€â€ (s)}. This vector would still be feasible as
Ïˆ(s,a) =Ïˆ(s,Ï€â€ (s))and would be strictly better in terms of utility as /hatwideR(s,Ï€â€ (s))>/hatwideR(s,a). This contradicts
our initial assumption and therefore Ïˆmax(s,a) =Ïˆmax(s,Ï€â€ (s))for allaâˆˆÎ˜/epsilon1
s.
Since the occupancy measure Ïˆmaxsatisï¬esÏˆmax(s,a) = 0for alla /âˆˆÎ˜/epsilon1
sâˆª{Ï€â€ (s)}andÏˆmax(s,a) =Ïˆ(s,Ï€â€ (s))
for allaâˆˆÎ˜/epsilon1
s, we conclude that it is the occupancy measure for the policy Ï€Das deï¬ned in Equation (47).
In order to prove ÏÏ€D=/hatwideÏÏ€D, ï¬rst note that for (s,a)âˆˆÎ˜/epsilon1
/angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,ÏˆÏ€D/angbracketrightBig
=Âµ(s)(ÏˆÏ€D(s,a)âˆ’ÏˆÏ€D(s,Ï€â€ (s))
=Âµ(s)2(Ï€D(a|s)âˆ’Ï€D(Ï€â€ (s)|s)) = 0,
where we used Equation (48) and Equation (47). Therefore
ÏÏ€Dâˆ’/hatwideÏÏ€D=/angbracketleftBig
Râˆ’/hatwideR,ÏˆÏ€D/angbracketrightBig
(i)=/summationdisplay
(s,a)âˆˆÎ˜/epsilon1Î±s,a/angbracketleftBig
ÏˆÏ€â€ {s;a}âˆ’ÏˆÏ€â€ ,ÏˆÏ€D/angbracketrightBig
=/summationdisplay
(s,a)âˆˆÎ˜/epsilon1Î±s,aÂ·0
= 0,
where (i)follows from Lemma E.3 in the known parameter case and Lemma F.3 in the unknown parameter
case.
J Additional experiments
In this section, we provide the worst-case score of the policy from the defenderâ€™s perspective , i.e., the value
of the inner minimization in (P2b), for our policy. Given the results in Section 5.2 (Theorems 5.3 and 5.2),
by construction of Ï€D, this minimum is achieved for R=/hatwideRand therefore the worst-case score equals /hatwideÏÏ€D.
As mentioned in Section 5.2, /hatwideÏÏ€Disa certiï¬cate in that we are guaranteed ÏÏ€Dâ‰¥/hatwideÏÏ€Das long as/epsilon1Dâ‰¥/epsilon1â€ .
The results are shown in Figure 6. As shown in the Figure, the value of /hatwideÏÏ€Dis larger when /epsilon1D</epsilon1â€ . This is
in line with results of Section 5.2 as in this case, Ï€D=Ï€â€ andÏ€â€ is the optimal policy in /hatwideR. In addition,
when/epsilon1D</epsilon1â€ , the value of /hatwideÏÏ€Dincreases with /epsilon1â€ . This is because when we increase the attack parameter /epsilon1â€ ,
the policyÏ€â€ needs to become optimal in /hatwideRwith a larger margin, which causes its score to increase.
We note that for both environments, when no defense is employed, i.e, when using the policy Ï€â€ , we will
obtain the same worst-case value when /epsilon1D< /epsilon1â€ asÏ€D=Ï€â€ , and we will obtain the worst-case score âˆ’âˆ
when/epsilon1Dâ‰¥/epsilon1â€ .
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /u1D716D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /u1D716â€ )
123
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /u1D716D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /u1D716â€ )
0510
Figure 6: The value of the worst-case score from the defense perspective, i.e., /hatwideÏÏ€Dunder diï¬€erent values of
/epsilon1â€ ,/epsilon1Dfor the Navigation (left) and Grid world (right) environments.
44