ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in
Chinese with Cloaking Perturbations
Yunze Xiao1,Yujia Hu2, Kenny Tsu Wei Choo2, Roy Ka-wei Lee2
1Carnegie Mellon University2Singapore University of Technology and Design
yunzex@andrew.cmu.edu
{yujia_hu, kenny_choo, roy_lee}@sutd.edu.sg
Abstract
Detecting hate speech and offensive language
is essential for maintaining a safe and respect-
ful digital environment. This study examines
the limitations of state-of-the-art large language
models (LLMs) in identifying offensive content
within systematically perturbed data, with a fo-
cus on Chinese, a language particularly suscep-
tible to such perturbations. We introduce Tox-
iCloakCN , an enhanced dataset derived from
ToxiCN, augmented with homophonic substi-
tutions and emoji transformations, to test the
robustness of LLMs against these cloaking per-
turbations. Our findings reveal that existing
models significantly underperform in detecting
offensive content when these perturbations are
applied. We provide an in-depth analysis of
how different types of offensive content are af-
fected by these perturbations and explore the
alignment between human and model expla-
nations of offensiveness. Our work highlights
the urgent need for more advanced techniques
in offensive language detection to combat the
evolving tactics used to evade detection mecha-
nisms.
Disclaimer :This paper describes violent and
discriminatory content that may be disturbing to
some readers.
1 Introduction
Offensive language, which includes hate speech,
cyberbullying, and adult-oriented content, poses
significant risks to user well-being and social har-
mony (Davidson et al., 2019). With the rapid ex-
pansion and widespread usage of social media plat-
forms, the proliferation of offensive language has
become a critical issue. Consequently, social media
platforms and researchers have explored develop-
ing robust machine learning and linguistic analy-
sis solutions to effectively identify and mitigate
the harmful effects of offensive content (Davidson
et al., 2017; Dhanya and Balakrishnan, 2021).Recent advances in Natural Language Processing
(NLP), particularly with Large Language Mod-
els (LLMs), have significantly improved the abil-
ity to detect offensive language across multiple
languages (Pitsilis et al., 2018; Wei et al., 2021;
Fatemah and Ozlem, 2021; Battistelli et al., 2020;
Beyhan et al., 2022; Dhanya and Balakrishnan,
2021; Deng et al., 2022a; Zhou et al., 2023). How-
ever, these models often struggle with systemati-
cally perturbed data designed to evade detection
mechanisms. Common perturbation techniques
include homophonic substitutions, emoji replace-
ment, insertions, character splits, and synonyms
(Su et al., 2022; Kirk et al., 2022). These tech-
niques, referred to as â€cloakingâ€, exploit linguistic
nuances to mask offensive content, posing a sub-
stantial challenge to both automated systems and
human moderators.
The Chinese language, in particular, is heavily im-
pacted by these techniques due to intensive lexicon-
based censorship, leading to a new linguistic phe-
nomenon (Wiener, 2011) where significant parts
of sentences are replaced by either homophones
or emojis to mask underlying offensive content or
to circumvent censorship rules. Figure 1 shows
two examples of offensive texts cloaked using ho-
mophone and emoji replacement techniques. In
these examples, the words and phrases highlighted
in yellow are replaced with homophones or emo-
jis. In the first example, homophones are used to
replace phrases that identify the target (e.g., â€œ è´ºæ¥ 
ä»â€ as the homophone for â€œ æ²³å—äºº,â€ which means
people from the Henan region in China) and offen-
sive terms such as â€œ å¤ªè´±â€ with â€œè‚½é”®.â€ Similarly,
in the second example, the offensive term â€œ èˆ”ç‹—â€
(i.e., Simps) is replaced with
 . Using such
techniques, users can fool automated offensive lan-
guage detectors into misclassifying these sentences
as non-offensive, even though avid Chinese social
media users will have no problem understanding
1arXiv:2406.12223v1  [cs.CL]  18 Jun 2024Homophone
Replacement
Emoji
Replacementæ²³å—äººå¤ªè´± ï¼Œè¿˜å–œæ¬¢æŠ¥å›¢ï¼Œ åå¿ƒæ€å¤š Text: 
è´ºæ¥ ä»è‚½é”® ï¼Œè¿˜å–œæ¬¢æŠ¥å›¢ï¼Œ æ·®å¿ƒæ€å¤š Cloaked:  
Interpretation: Henan people are too
mean, and they like to band together
and have many bad intentions.
èˆ”ç‹—èµ¶ç´§å»æ­»å§ Text: 
ğŸ‘…ğŸ¶èµ¶ç´§å»â‘£ğŸ± Cloaked:  
Interpretation: Simps should die.
Automated Of fensive
Language DetectorOffensive
Normal
Offensive
NormalPredicted Labels
Predicted LabelsFigure 1: Example of cloaked Chinese offensive lan-
guage using homophone and emoji replacement. By
using such techniques, users will be able to fool the au-
tomated offensive language detector into misclassifying
them as normal sentences.
the offensive context of the text. Addressing this
problem is crucial to improve the effectiveness of
offensive language detection systems. As these
evasion techniques evolve, it becomes increasingly
important for these offensive langauge detection
systems to adapt and accurately identify cloaked
offensive content.
In this work, we introduce ToxiCloakCN , a novel
Chinese offensive content dataset that benchmark
content moderation modelsâ€™ ability to detect of-
fensive texts cloaked using homophone and emoji
replacements. Specifically, we conduct extensive
experiments and evaluate state-of-the-art LLMs on
theToxiCloakCN dataset. The experiments demon-
strated that both perturbation methods significantly
affect the modelsâ€™ capabilities in detecting offen-
sive text. We also analyze the effect of prompts
on the experimental results by testing the models
using six different prompts. Additionally, we ana-
lyze the perturbation effects on different types of
offensive content: sexism, racism, regional bias,
and anti-LGBTQ+. This research underscores the
critical need for developing more robust models
to effectively moderate cloaked online offensive
content.
We summarize the main contributions of this paper
as follows:
â€¢We introduce ToxiCloakCN , a novel dataset
specifically designed to evaluate the robust-
ness of LLMs against homophonic and emoji
perturbations, addressing a significant gap in
current offensive language detection research.
â€¢We conduct a comprehensive evaluation ofstate-of-the-art LLMs. Our experimental re-
sults reveal that leading LLMs struggle to de-
tect cloaked offensive content, highlighting
the limitations of current approaches and the
need for more advanced detection techniques.
â€¢We analyze how different types of offensive
content are impacted by cloaking perturba-
tions, providing critical insights for improv-
ing model robustness and effectiveness in real-
world applications.
2 Related work
2.1 Chinese Offensive Content Dataset
Several datasets have been developed for Chinese
offensive language detection. The Chinese Offen-
sive Language Dataset (COLD) categorizes sen-
tences into groups like individual attacks and anti-
bias (Deng et al., 2022a). TOCP and TOCAB from
Taiwanâ€™s PTT platform address profanity and abuse
(Chung and Lin, 2021). The Sina Weibo Sexism Re-
view (SWSR) focuses on sexism within Chinese so-
cial media (Jiang et al., 2021). The ToxiCN dataset
from platforms like Zhihu and Tieba includes a
multi-level labeling system for offensive language,
hate speech, and other categories (Lu et al., 2023).
In this work, we introduce ToxiCloakCN , a novel
dataset capturing cloaked offensive text using ho-
mophonic and emoji replacements, built on top of
the comprehensive ToxiCN dataset.
2.2 Chinese Offensive Content Detection
Offensive language and hate speech detection
have been explored in various languages, includ-
ing English (Davidson et al., 2017; Pitsilis et al.,
2018; Wei et al., 2021), Arabic (Fatemah and
Ozlem, 2021), French (Battistelli et al., 2020),
Turkish (Beyhan et al., 2022), and Asian languages
(Dhanya and Balakrishnan, 2021). In Chinese,
techniques include lexicon-based models (Zhang
et al., 2010; Deng et al., 2022b), supervised and
adversarial learning models (Jiang et al., 2021; Liu
et al., 2020b), knowledge-based models (Liu et al.,
2020a), and fine-tuned pretrained models (Deng
et al., 2022a) like BERT (Devlin et al., 2019).
Cross-cultural transfer learning models also adapt
to cultural differences (Zhou et al., 2023). Never-
theless, existing models mainly focus on explicit
offensive content. This work addresses the gap by
evaluating modelsâ€™ ability to detect cloaked offen-
sive content.
22.3 Language Perturbation
Various perturbation techniques have been pro-
posed to investigate the vulnerabilities of NLP mod-
els in adversarial scenarios. These include insert-
ing emojis (Kirk et al., 2022), token replacements
and insertions (Garg and Ramakrishnan, 2020),
and probability-based greedy replacements (Ren
et al., 2019). While these methods primarily target
English, adapting them to Chinese is challenging
due to linguistic differences, though some attempts
have been made (Liu et al., 2023).
For Chinese, Su et al. have highlighted adversarial
attacks such as word perturbation, synonyms, and
typos (Su et al., 2022). Subsequent solutions have
focused on BERT-based models to address these at-
tacks (Zhang et al., 2022; Wang et al., 2023; Xiong
et al., 2024). However, previous work mainly eval-
uates BERT-based models and lacks robustness
research on LLMs and social media-based adver-
sarial datasets reflecting current trends. Our work
addresses this gap by providing a new dataset with
realistic perturbations for Chinese offensive lan-
guage detection.
3 Methodology
TheToxiCloakCN dataset builds upon the ToxiCN
dataset (Lu et al., 2023) through a detailed multi-
step process. First, we sampled a balanced dataset
from the base ToxiCN dataset, known as the â€œbaseâ€
dataset. Next, this balanced base dataset was per-
turbed using homophone and emoji replacements
to produce the ToxiCloakCN dataset. After con-
structing the ToxiCloakCN dataset, we explored
pinyin augmentation as a potential solution to ad-
dress the â€œcloakedâ€ offensive content perturbed us-
ing homophone replacements. Finally, we defined
six different instructions for evaluating the perfor-
mance of state-of-the-art large language models on
ToxiCloakCN .
3.1 Dataset Construction
3.1.1 Sampling Base Dataset
The ToxiCN dataset was chosen as the foundational
dataset due to its well-controlled annotation, with
Fleiss Kappas for different granularities exceed-
ing 0.6 (Lu et al., 2023). We first collated the
offensive lexicon (i.e., swearwords) identified in
ToxiCN. Next, we sampled sentences from ToxiCN
labeled as â€œ offensive â€ or â€œ hateful â€ that contained
the offensive lexicon, resulting in 2,293 offensiveTopic All Non-Offensive Offensive
Race 1,769 872 897
Gender 1,229 546 683
LGBTQ+ 913 407 506
Region 671 464 207
Table 1: Base dataset distribution breakdown by content
topics.
sentences. To balance the dataset, we also sam-
pled non-offensive sentences from ToxiCN, giving
preference to sentences containing the offensive
lexicon but labeled as â€œ non-offensive â€. In total,
we sampled 2,289 non-offensive sentences. The
combined 4,582 sampled sentences form the base
dataset that we will use for perturbation.
Table 1 shows the base dataset distribution break-
down by content topics, mirroring the statistical
distribution of the base ToxiCN dataset. It is worth
noting that a substantial amount of offensive con-
tent in the base dataset pertains to racism, followed
by sexism and anti-LGBTQ content. We have rela-
tively lesser regional bias offensive content in the
base dataset.
3.1.2 Homophone Replacement
From the base dataset, we replaced keywords in
each sentences in base dataset with their homo-
phones, based on a specified dictionary1. Key-
words were identified as those present in the speci-
fied dictionary. To ensure the quality of the texts,
we kept the perturbation ratio at 0.3, meaning
that 30% of individual characters in each sentence
were replaced with homophones. Higher perturba-
tion ratios made sentences difficult to understand,
while lower ratios resulted in insufficient disruption.
Therefore, a 0.3 ratio was chosen to ensure both vis-
ible perturbation and readability. The homophone-
replaced sentences will be added into our new Tox-
iCloakCN dataset.
3.1.3 Emoji Replacement
Utilizing the advanced script proposed by Zhang
(2024), we further performed emoji replacement
on the sentences in the base dataset. Initially, text
segmentation techniques were applied to identify
key words within the sentences. These key words
were then replaced with corresponding emojis from
a predefined emoji lexicon dictionary2. The per-
turbation rate was also set at 0.3. When a direct
lexical match was unavailable in the dictionary,
1https://github.com/dongrixinyu/JioNLP/
2https://github.com/THUzhangga/NMSL
3Instruction Template Inputs
English_Text Please decide if the following text is offensive or not. â€˜0â€™ means not offensive, â€˜1â€™ means
offensive. {Text}
Chinese_Text è¯·åˆ¤æ–­ä¸‹åˆ—å¥å­æ˜¯å¦æ˜¯å†’çŠ¯çš„ï¼Ÿâ€˜0â€™æ„å‘³ç€ä¸å†’çŠ¯ï¼Œâ€˜1â€™æ„å‘³ç€å†’çŠ¯.{Text}
English_Pinyin Please determine if the following pinyin-expressed Chinese sentence is offensive. â€˜0â€™
means not offensive, â€˜1â€™ means offensive. {Pinyin}
Chinese_Pinyin è¯·åˆ¤æ–­ä¸‹åˆ—æ‹¼éŸ³æ‰€è¡¨è¾¾çš„ä¸­æ–‡å¥å­æ˜¯å¦æ˜¯å†’çŠ¯çš„ï¼Ÿâ€˜0â€™æ„å‘³ç€ä¸å†’çŠ¯ï¼Œâ€˜1â€™æ„å‘³ç€
å†’çŠ¯.{Pinyin}
English_Text+Pinyin Please determine if the following text is offensive and make your judgment based on
pinyin. â€˜0â€™ means not offensive, â€˜1â€™ means offensive. {Text} {Pinyin}
Chinese_Text+Pinyin è¯·ç»“åˆæ‹¼éŸ³åˆ¤æ–­ä¸‹åˆ—å¥å­æ˜¯å¦æ˜¯å†’çŠ¯çš„ï¼Ÿâ€˜0â€™æ„å‘³ç€ä¸å†’çŠ¯ï¼Œâ€˜1â€™æ„å‘³ç€å†’çŠ¯.
{Text} {Pinyin}
Table 2: Instructions used in prompting LLMs to detect offensive content in ToxiCloakCN .
the algorithm applied a phonetic approximation
method based on the wordâ€™s pinyin to achieve the
replacement. For example, to convert the word
â€˜æ“(Fu*k)â€™ to its emoji representation, the output
would be (
 ). Since there is no direct emoji match
for â€˜æ“(Fu*k)â€™ in the dictionary, and â€˜ æ“(Fu*k)â€™
and â€˜è‰(grass)â€™ are homophones, the emoji (
 ) of
homophonic â€˜ è‰(grass)â€™ is chosen as the replace-
ment. Finally, the emoji-replaced sentences are
added to the ToxiCloakCN dataset.
3.2 Pinyin Augmentation
While we aim to benchmark the state-of-the-art
LLMsâ€™ ability to detect cloaked offensive content
in our newly constructed ToxiCloakCN dataset,
we also explore potential solutions to aid LLMsâ€™ in
the detection task. Specifically, we explore pinyin
augmentation method as a potential solution to de-
tect homophone-replaced offensive sentences in
ToxiCloakCN . Pinyin is the official romanization
system for Standard Mandarin Chinese in main-
land China and Taiwan, using the Latin alphabet
to represent Chinese characters phonetically. The
intuition for this method is that, given the nature
of homophones, the pinyin representation should
look alike, if not the same, thus potentially helping
the model identify the offensiveness. Both Tox-
iCN andToxiCloakCN datasets theoretically share
the same phonetic data, despite their textual differ-
ences. Therefore, we used the pypinyin3package
to derive pinyin of the sentences in ToxiCloakCN .
3.3 Instruction Templates
To observe the effect of prompting on the task, we
propose six distinct instruction templates to verify
the efficacy of our ToxiCloakCN dataset. These
instructions are carefully designed to evaluate the
effects of prompt languages (i.e., English and Chi-
3pypinyinnese) on the offensive content detection task, as
well as the effect of pinyin augmentation. Table 2
shows the six instructions designed and applied in
our experiments.
4 Experiments
4.1 Baselines
Lexicon-based. We employed a lexicon-based
detection method to identify offensive language,
classifying text as offensive if it contained any
words from the ToxiCN offensive lexicon, other-
wise marking it as non-offensive (Xiao et al., 2024;
Lu et al., 2023).
COLDetector. We implemented COLDETEC-
TOR (Deng et al., 2022a), a BERT-based model
for offensive language detection. This approach
involves feeding the text into the BERT model, ex-
tracting the first hidden state from the final layer,
and connecting it to a linear layer for the final
prediction. The model is trained on the COLD
dataset (Deng et al., 2022a), a popular benchmark
for Chinese offensive language detection.
Large Language Models. We evaluate GPT-
4o and three open-source LLMsâ€”LLaMA-
3-8B (AI@Meta, 2024), Qwen1.5-MoE-
A2.7B (Team, 2024), and Mistral-7B (Jiang
et al., 2023)â€”for the Chinese offensive language
detection task. The open-source models were
fine-tuned on the COLD training datasets using
the six proposed instructions. Utilizing the LORA
method (Hu et al., 2021), we introduced 4.1 million
additional parameters, which is only 0.06% of the
total parameters. Fine-tuning was conducted over
three epochs using the LLM-Adapters Toolkit (Hu
et al., 2023). GPt-4o and the fine-tuned models
were then evaluated on the base and ToxiCloakCN
datasets. All fine-tuning and inference phases are
performed on two NVIDIA A6000 GPUs.
4Model Training Set Instruction Type Homophone Emoji Base
Lexicon-based ToxiCN Lexicon - 0.003 (0.297) - 0.300
COLDetector COLD - 0.582 (0.043) 0.615 (0.010) 0.625
English_text 0.637 (0.040) 0.634 (0.043) 0.677
Chinese_text 0.666 (0.023) 0.649 (0.040) 0.689
LLAMA-3-8B COLD English_pinyin 0.637 (0.000) - 0.637
Chinese_pinyin 0.634 (0.000) - 0.634
English_Text+Pinyin 0.637 (0.035) - 0.672
Chinese_text+Pinyin 0.638 (0.034) - 0.672
English_text 0.650 (0.043) 0.618 (0.075) 0.693
Chinese_text 0.669 (0.031) 0.640 (0.060) 0.700
Qwen COLD English_pinyin 0.630 (0.000) - 0.630
Chinese_pinyin 0.613 (0.000) - 0.613
English_Text+Pinyin 0.646 (0.048) - 0.694
Chinese_text+Pinyin 0.649 (0.051) - 0.700
English_text 0.650 (0.038) 0.631 (0.057) 0.688
Chinese_text 0.669 (0.022) 0.649 (0.042) 0.691
Mistral COLD English_pinyin 0.622 (0.000) - 0.622
Chinese_pinyin 0.613 (0.000) - 0.613
English_Text+Pinyin 0.649 (0.037) - 0.686
Chinese_text+Pinyin 0.651 (0.039) - 0.690
English_text 0.709 (0.055) 0.621 (0.143) 0.764
Chinese_text 0.727 (0.069) 0.754 (0.042) 0.796
GPT-4o English_pinyin 0.649 - 0.678
Chinese_pinyin 0.723 (0.018) - 0.741
N/A English_Text+Pinyin 0.719 (0.042) - 0.761
Chinese_Text+Pinyin 0.741 (0.022) - 0.763
Table 3: Macro F1 scores of benchmark models. Note that " Homophone " and " Emoji " denote the homophone-
replaced and emoji-replaced sentences in the ToxiCloakCN dataset, respectively. Best performances are bolded .
Values in () represent the difference between the Macro F1 score on the base dataset and the Homophone/Emoji
datasets (i.e., performance decline).
4.2 Evaluation Metric
To confirm with established research norms (Deng
et al., 2022a; Lu et al., 2023), we utilize Macro
F1 score as the evaluation metrics for the offensive
language detection task. The metric assess the
modelsâ€™ performance in classifying the offensive
languages in the datasets.
4.3 Experimental Results
Table 3 presents the offensive detection outcomes
for all models, showing that GPT-4o achieves the
highest performance with Chinese-only text instruc-
tions. However, all models exhibit a notable per-
formance decline on the homophone and emoji
replaced sentences in ToxiCloakCN dataset com-
pared to the base dataset. This indicates a signif-
icant reduction in their ability to detect offensive
content when the text is perturbed. The drop in
performance is primarily due to the probabilistic
nature of LLMs, which rely on next-word predic-
tion based on statistical probabilities. Perturbations
like homophone and emoji replacements disrupt
this probability chain, compromising the modelsâ€™
ability to generate coherent and contextually accu-
rate responses.4.3.1 Effects of Pinyin Augmentation
When pinyin was added to the text, we observed
a performance reduction across all models on the
homophone-replaced sentences in ToxiCloakCN
dataset compared to text-only inputs. Instead of
enhancing model capabilities, pinyin disrupted
performance, leading to lower classification accu-
racy. Pinyin-only input experiments consistently
showed lower performance compared to text in-
puts, highlighting the modelsâ€™ limitations in pro-
cessing pinyin. This aligns with Li et al.â€™s find-
ing that LLMs recognize pinyin but struggle with
pronunciation, making it difficult to correct phono-
logical errors. This limitation affects the modelsâ€™
ability to detect offensive language significantly
perturbed through homophone replacements. The
performance drop with pinyin augmentation is par-
tially due to pinyin not being directly associated
with the original phrase, causing confusion for the
model.
4.3.2 Effect of Instruction Templates
Across all models, Chinese instructions consis-
tently outperformed English instructions for tasks
involving Chinese text. Conversely, for pinyin-only
tasks, English instructions yielded better results
5Figure 2: The modelsâ€™ error rates comparison between the sentences in the base dataset and the homophone or
emoji-replaced sentences using prompt type Chinese_text , broken down by offensive content type. Note that smaller
error rates represent better performance.
1. Text:æ•´æ€¥è¨€äº†æˆ‘å¯ä»¥å¹²çŒªï¼Œä½†æ˜¯ä¸æ¯æ„Ÿæ²¹éƒ­é»‘äººéš¾æœ‰çš„å¥³äºº
GPT-4o: Yes, thatâ€™s offensive, keywords: â€˜fu*k pig( å¹²çŒª)â€™, â€˜ will fu*k(ä¼šå¹²)â€™
â€˜black(é»‘äºº)â€™, â€˜woman( å¥³äºº)â€™
Human Annotator: The phrase is offensive. The speaker is saying that he
would rather fu*k pig ( å¹²çŒª) than having sex ( æ¯æ„Ÿ) with women that have a
black boyfriend ( æ²¹éƒ­é»‘äººéš¾) - which is both sexist and racist.
2. Text:æˆ‘å·²ç»ä¸¾æŠ¥å®¶çˆ†å•¥è¿™æ¯é’©ä¹
GPT-4o: â€œ å®¶çˆ†â€ - Whilst it is correct to describe a situation of domestic
violence, the direct use of such an acronym may not appear serious and
respectful.â€˜ æ¯é’©ä¹â€™ - - This is an insulting term that is a personal attack on
another person and is clearly derogatory and abusive in nature. thus, the
phrase is offensive.
Human Annotator: The phrase is offensive. The speaker is saying that he has
reported ( ä¸¾æŠ¥) and (å®¶) roasted ( çˆ†å•¥) the bit*h ( æ¯é’©ä¹).
3. Text:å¥‡è¶£è›‹è¿˜æ˜¯æ»šæ¥æ»šå»æ¯”è¾ƒå·
GPT-4o: Your reference to â€˜ å¥‡è¶£è›‹è¿˜æ˜¯æ»šæ¥æ»šå»æ¯”è¾ƒå¥½â€™ sounds like a
humorous description of something or a situation. If this is a reference to a
Chick-fil-A toy (the common chocolate egg with small toys), it could be
describing the fun of such toys, especially when they are rolled around.
Human Annotator: The phrase is offensive. The speaker is saying that
overweight women ( å¥‡è¶£è›‹) is better off ( æ¯”è¾ƒå·) rolling around ( æ»šæ¥æ»š
å»). Note that å¥‡è¶£è›‹is not a homophone but a slur.
Table 4: Examples of offensive sentences with homo-
phone replacements. The offensive text are shown in
black. GPT-4o interpretation of the text are in red, and
the human annotatorâ€™s explanations are in blue.
than Chinese instructions. This is likely due to
the task characteristics: using Chinese instructions
for Chinese text maintains linguistic consistency,
enhancing the modelâ€™s understanding. For pinyin-
only tasks, English instructions, which align better
with the letter-based nature of pinyin, improved
coherence and model performance.
4.4 Case Studies
The decline in Macro F1 scores across all models
onToxiCloakCN datasets highlights the challenges1. Text:å‘Šè¯‰å¥¹
door,
å®¶
æ çš„å…¬
æœ€è¿‘å‘æ˜¥,åŠåˆé•¿åˆå¤§,æ˜¯çœŸçš„å¤§,
æ˜¯æ€•ç‰‡å®£ä¼ æŒ‘
å‡ºæ¥çš„å¤§,é€ç»™å¥¹
å¤œå¤œæ˜¥å®µ,
,æ²¡è‰¾æ»‹ç—…
GPT-4o: Yes, the phrase is harmful because it contains sexual innuendo and
inappropriate metaphors that could be perceived as disrespectful and sexually
harassing. In addition, it incorrectly characterizes health conditions, which
can lead to misunderstanding and discrimination against specific diseases.
Human Annotator: This phrase is offensive. The speaker is saying that his
cowshedâ€™s cow(
 å®¶
æ çš„å…¬
 ) is horny( å‘æ˜¥), and can give them for free
to have sex(é€ç»™å¥¹
å¤œå¤œæ˜¥å®µ,
)
2. Text:å½’æ ¹ç©¶åº•ï¼Œå¹¿å¤§æ™®é€š
æ€§è‹¥æ˜¯é è¿‘æ™®
ï¼Œä¼š
å¹¸ã€‚è¿™æ˜¯æœ€æ­£
ç¡®çš„ç»“è®ºã€‚
GPT-4o: Yes, this statement could be seen as offensive because it imposes
gender roles and expectations, suggesting that women can only be happy if
they are dependent on men, a view that could be considered sexist.
Human Annotator: Yes, this statement is offensive because it is sarcastically
stating that all the women(
 æ€§) will be unfortunate ( ä¼š
å¹¸) if they get
closer to normal men ( æ™®
 ). Note that æ™®ä¿¡ç”·(æ™®
 ) is a abbreviation
that is derogatory towards men.
3. Text:
 house
GPT-4o: The language consists of various emoticons and the English word
â€˜houseâ€™ and lacks clear context. In the absence of obvious harmful content,
the passage itself is not directly harmful.
Human Annotator: The phrase is offensive. The speaker is saying that
simps(
 ) does not deserve a good death(
 house). Note that in this
example, both homophones and emojis has been used.
Table 5: Examples of offensive sentences with emoji
replacements. The offensive text are shown in black.
GPT-4o interpretation of the text are in red, and the
human annotatorâ€™s explanations are in blue.
these models face in offensive detection tasks. In
this section, we aim to empirically explore why
LLMs struggle with offensive content with homo-
phone or emoji replacements. Through case studies,
we analyzed the effects of the perturbations on var-
ious types of offensive content. Additionally, we
analyze specific cases to compare the differences
between LLMs and humanâ€™s understanding of the
6cloaked offensive content.
4.4.1 Effects of Perturbation on Offensive
Content Types
We examined the effects of homophone and emoji
replacements on different offensive content types,
namely racism ,sexism ,anti-LGBTQ+ , and re-
gional bias . Figure 2 depicts the modelsâ€™ error
rates on the base and ToxiCloakCN datasets across
these content types. Note that the performance
is based on the Chinese_Text instruction, and a
smaller error rate indicates better performance in
the offensive language detection task.
Generally, we observe that all models have lower
error rates on the base dataset across all offen-
sive content types, supporting our initial findings
that LLMs struggle to detect cloaked offensive lan-
guage, regardless of content type. Interestingly, for
the open-source LLMs, we notice a smaller differ-
ence between the error rates for regional bias offen-
sive content in the base and ToxiCloakCN datasets.
This could be due to a generalization issue; the
open-source LLMs are fine-tuned on COLD, which
may not contain much content related to regional
bias, resulting in poorer performance in detecting
this type of offensive content, regardless of per-
turbation. However, for the closed-source model,
GPT-4o, we observe performance gaps for regional
bias offensive content when the sentences are per-
turbed using homophone and emoji replacements.
4.4.2 Comparison Between LLMs and
Human Understanding of Cloaked
Offensive Content
To explore the reasons behind the modelsâ€™ poor
performance on the ToxiCloakCN dataset, we con-
ducted a detailed analysis using the top-performing
GPT-4o model and human annotators, focusing
on the comparison between human and modelâ€™s
understanding of cloaked offensive content. Specif-
ically, we randomly selected several offensive sen-
tences from the ToxiCloakCN dataset and exam-
ined GPT-4oâ€™s interpretation of these sentences.
We recruited two human annotatorsâ€”an undergrad-
uate and a postgraduate studentâ€”who are profi-
cient in Chinese and active users of Chinese so-
cial media. They assessed the offensiveness of
the given sentences and provided detailed expla-
nations. Through these case studies, we aim to
compare GPT-4oâ€™s understanding with human un-
derstanding of these cloaked offensive sentences
and empirically identify any gaps in the modelâ€™scomprehension of cloaked offensive content.
Homophones . Table 4 presents three homophone-
replaced offensive sentences from the Toxi-
CloakCN dataset. In the first example, GPT-4o
correctly identifies the offensive content by rec-
ognizing keywords like â€™ å¹²çŒªâ€™ (â€™fu*k pigâ€™). This
suggests that GPT-4o has some understanding of
homophones, enabling it to detect cloaked offen-
sive language. In the second example, while the
model correctly classifies the sentence as offensive,
its explanation does not match the original meaning
of the offensive sentence. For instance, it identi-
fies â€™æ¯é’©ä¹â€™ as offensive but cannot explain why.
The human annotator, however, can reconstruct the
sentence and provide an accurate judgment and ex-
planation. In the third example, GPT-4o misjudges
and misinterprets the phrase due to its inability to
understand the cultural background. This exam-
ple demonstrates the modelâ€™s limitation in recog-
nizing implicit offensive language across different
cultures, whereas human annotators, with their cul-
tural understanding, can make accurate judgments.
Emoji. Table 5 presents three emoji-replaced of-
fensive sentences from the ToxiCloakCN dataset.
In the first example, both GPT-4o and the human
annotator accurately identify the offensive content.
This case is relatively simple because offensive
keywords such as â€˜ åŠâ€™ (a homophone for â€˜di*kâ€™)
and â€˜è‰¾æ»‹ç—…â€™ (AIDS) remain unchanged. In the
second example, although the model classifies the
sentence as offensive, its explanation differs from
that of the human annotator, indicating a misinter-
pretation. This may be due to the modelâ€™s failure to
grasp emoji meanings, such as â€˜
 â€™ (which means
â€˜notâ€™ in this context). The third example involves
complex emoji and homophone replacement, with
â€˜simpâ€™ translated to â€˜ èˆ”ç‹—â€™ in Chinese, represented
by emojis for â€˜ èˆ”â€™ (lick) and â€˜ ç‹—â€™ (dog). â€˜ ä¸å¾—â€™
(not deserve) was replaced by an emoji (
 ) and
the last two characters( å¥½æ­») are phonetically con-
verted to â€˜houseâ€™ in English. GPT-4o misclassifies
and misinterprets this complex content, whereas
the human annotators are able identify it, highlight-
ing the need for developing more robust solutions
capable of handling such cloaked offensive lan-
guages.
5 Conclusion and Future Works
In this study, we explored the robustness of cur-
rent Chinese offensive language detection models
7against cloaking perturbations, specifically homo-
phone and emoji replacements. We developed the
ToxiCloakCN dataset by augmenting the ToxiCN
dataset with these perturbations to simulate real-
world scenarios where users attempt to evade de-
tection systems. Our experimental results demon-
strated that the performance of state-of-the-art mod-
els, including GPT-4o, significantly declines when
faced with these cloaked offensive content. Al-
though our proposed pinyin augmentation method
showed some promise, its effectiveness was limited
across different models, highlighting the complex-
ity of phonetic alignment in offensive language
detection.
Our case studies further revealed notable gaps in
the modelsâ€™ understanding of cloaked offensive
content compared to human annotators. GPT-4o
often missed or misinterpreted offensive keywords
disguised by homophones or emojis, while human
evaluators, with their cultural and contextual un-
derstanding, could accurately identify the offensive
nature of these texts. This emphasizes the need
for models that can better mimic human compre-
hension of nuanced and contextually rich language.
These findings underscore the urgent need for more
advanced techniques to handle such evolving strate-
gies.
Future research should expand cloaking techniques
beyond homophone and emoji perturbations, in-
corporate a wider range of linguistic variations
from real-life internet sources, and develop more
sophisticated phonetic alignment methods to im-
prove model robustness. Additionally, creating
algorithms that integrate deeper semantic under-
standing and context-awareness wifll be essential
for effectively handling cloaked offensive language.
By building on the groundworks of this study and
addressing these areas, future research can signifi-
cantly advance the field of offensive language de-
tection, contributing to safer and more respectful
digital environments.
Limitation
This study has several limitations. Firstly, while
our dataset includes comprehensive homophone
and emoji perturbations, it may not encompass the
entire range of adversarial techniques employed in
real-world scenarios. This limitation could affect
the generalizability of our findings to other pertur-
bation forms not examined in this study. Addition-ally, our reliance on the ToxiCN dataset, despite its
robustness, might not fully capture the diversity of
offensive language across various Chinese dialects
and regional linguistic nuances. This limitation
could impact the broader applicability of our find-
ings. Future research should consider subsampling
perturbed data from real-life internet sources such
as Tieba4and NGA5to gain a more accurate and
timely understanding of these perturbed languages
in real life. Lastly, our work does not provide a
definitive solution for addressing all challenges re-
lated to cloaked offensive language detection. Fu-
ture work should undertake more thorough and
advanced analyses to develop effective solutions
for these challenges
Ethical Statement
This research focuses on the detection of offen-
sive language, particularly in the context of ho-
mophonic and emoji perturbations used to bypass
detection mechanisms. Our primary goal is to high-
light the vulnerabilities of current language models
and enhance their robustness against these cloaking
techniques, thereby contributing to safer and more
respectful online environments.
The study involves using systematically perturbed
data to test the limits of existing models. While
this approach is crucial for understanding and im-
proving detection capabilities, there are inherent
risks associated with the potential misuse of these
findings. Specifically, the techniques developed
to detect cloaked offensive language might also
be studied to refine evasion tactics further. How-
ever, it is important to emphasize that our work is
solely aimed at detecting and mitigating offensive
language, not to facilitate censorship or suppress
free speech.
Our dataset and perturbations are derived from ex-
isting resources; no new data was collected for this
study. The use of ToxiCloakCN aligns with the
ToxiCN datasetâ€™s intention, which states, "All re-
sources are for scientific research only." We have
also carefully adhered to the Apache-2.0 license
used by JioNLP and the MIT license for pypinyin.
Our research is conducted with the explicit aim of
improving the detection of offensive language. Our
efforts are directed towards contributing positively
4https://tieba.baidu.com
5https://nga.cn
8to the broader field of content moderation, ensur-
ing that online platforms can effectively manage
offensive language while respecting the principles
of free and open communication.
References
AI@Meta. 2024. Llama 3 model card.
Delphine Battistelli, Cyril Bruneau, and Valentina Dra-
gos. 2020. Building a formal model for hate detection in
french corpora. Procedia Computer Science , 176:2358â€“
2365.
Fatih Beyhan, Buse Ã‡arÄ±k, Ë™InanÃ§ ArÄ±n, AyÂ¸ secan
Terzio Ë˜glu, Berrin Yanikoglu, and Reyyan Yeniterzi.
2022. A turkish hate speech dataset and detection sys-
tem. In Proceedings of the Thirteenth Language Re-
sources and Evaluation Conference , pages 4177â€“4185.
I Chung and Chuan-Jie Lin. 2021. Tocab: A dataset
for chinese abusive language processing. In 2021 IEEE
22nd International Conference on Information Reuse
and Integration for Data Science (IRI) , pages 445â€“452.
Thomas Davidson, Debasmita Bhattacharya, and In-
gmar Weber. 2019. Racial bias in hate speech and
abusive language detection datasets. arXiv preprint
arXiv:1905.12516 .
Thomas Davidson, Dana Warmsley, Michael Macy, and
Ingmar Weber. 2017. Automated hate speech detection
and the problem of offensive language. In Proceedings
of the international AAAI conference on web and social
media , volume 11, pages 512â€“515.
Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng,
Fei Mi, Helen Meng, and Minlie Huang. 2022a. COLD:
A benchmark for Chinese offensive language detection.
InProceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages 11580â€“
11599, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Yong Deng, Chenxiao Dou, Liangyu Chen, Deqiang
Miao, Xianghui Sun, Baochang Ma, and Xiangang Li.
2022b. BEIKE NLP at SemEval-2022 task 4: Prompt-
based paragraph classification for patronizing and con-
descending language detection. In Proceedings of the
16th International Workshop on Semantic Evaluation
(SemEval-2022) , pages 319â€“323, Seattle, United States.
Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of deep
bidirectional transformers for language understanding.
InProceedings of the 2019 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 4171â€“4186, Minneapo-
lis, Minnesota. Association for Computational Linguis-
tics.
LK Dhanya and Kannan Balakrishnan. 2021. Hate
speech detection in asian languages: a survey. In 2021international conference on communication, control and
information sciences (ICCISc) , volume 1, pages 1â€“5.
IEEE.
Husain Fatemah and Uzuner Ozlem. 2021. A survey
of offensive language detection for the arabic language.
ACM Transactions on Asian and Low-Resource Lan-
guage Information Processing , 20(1).
Siddhant Garg and Goutham Ramakrishnan. 2020.
BAE: BERT-based adversarial examples for text clas-
sification. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 6174â€“6181, Online. Association for
Computational Linguistics.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. Lora: Low-rank adaptation of
large language models. Preprint , arXiv:2106.09685.
Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-
Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Sou-
janya Poria. 2023. Llm-adapters: An adapter family for
parameter-efficient fine-tuning of large language models.
arXiv preprint arXiv:2304.01933 .
Aiqi Jiang, Xiaohan Yang, Yang Liu, and Arkaitz Zu-
biaga. 2021. Swsr: A chinese dataset and lexicon for
online sexism detection. Preprint , arXiv:2108.03070.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,
Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Florian Bressand, Gianna Lengyel, Guillaume
Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv
preprint arXiv:2310.06825 .
Hannah Kirk, Bertie Vidgen, Paul Rottger, Tristan
Thrush, and Scott Hale. 2022. Hatemoji: A test suite
and adversarially-generated dataset for benchmarking
and detecting emoji-based hate. In Proceedings of the
2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Lan-
guage Technologies , pages 1352â€“1368, Seattle, United
States. Association for Computational Linguistics.
Yinghui Li, Haojing Huang, Shirong Ma, Yong Jiang,
Yangning Li, Feng Zhou, Hai-Tao Zheng, and Qingyu
Zhou. 2023. On the (in)effectiveness of large lan-
guage models for chinese text correction. Preprint ,
arXiv:2307.09007.
Hanyu Liu, Chengyuan Cai, and Yanjun Qi. 2023. Ex-
panding scope: Adapting English adversarial attacks
to Chinese. In Proceedings of the 3rd Workshop on
Trustworthy Natural Language Processing (TrustNLP
2023) , pages 276â€“286, Toronto, Canada. Association
for Computational Linguistics.
Nelson F Liu, Browsing Avci, Andres Abeliuk, Rahul
Acharya, Kartikeya Ahuja, Klaus Zhuang, Prajit Dhar,
Madeleine Fatemi, Sayna Guo, Tanmoy Choudhury,
et al. 2020a. Combating negative stereotypes: A compu-
tational approach for exposing implicit bias in chinese.
InProceedings of the 28th International Conference on
Computational Linguistics , pages 6568â€“6581.
9Nelson F Liu, Tony Wu, Duane S Boning, and Tanmoy
Choudhury. 2020b. AI bug detector: Adversarial input
detection for natural language processing models. In
Proceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing: System Demon-
strations , page 187â€“196.
Junyu Lu, Bo Xu, Xiaokun Zhang, Changrong Min,
Liang Yang, and Hongfei Lin. 2023. Facilitating fine-
grained detection of Chinese toxic language: Hierarchi-
cal taxonomy, resources, and benchmarks. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 16235â€“16250, Toronto, Canada. Association for
Computational Linguistics.
Georgios K Pitsilis, Heri Ramampiaro, and Helge
Langseth. 2018. Effective hate-speech detection in twit-
ter data using recurrent neural networks. Applied Intel-
ligence , 48:4730â€“4742.
Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial exam-
ples through probability weighted word saliency. In
Proceedings of the 57th Annual Meeting of the Associa-
tion for Computational Linguistics , pages 1085â€“1097,
Florence, Italy. Association for Computational Linguis-
tics.
Hui Su, Weiwei Shi, Xiaoyu Shen, Zhou Xiao, Tuo Ji,
Jiarui Fang, and Jie Zhou. 2022. RoCBert: Robust Chi-
nese bert with multimodal contrastive pretraining. In
Proceedings of the 60th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers) , pages 921â€“931, Dublin, Ireland. Association
for Computational Linguistics.
Qwen Team. 2024. Qwen1.5-moe: Matching 7b model
performance with 1/3 activated parameters".
Ziming Wang, Xirong Xu, Xinzi Li, Haochen Li, Li Zhu,
and Xiaopeng Wei. 2023. A more robust model to an-
swer noisy questions in kbqa. IEEE Access , 11:22756â€“
22766.
Bencheng Wei, Jason Li, Ajay Gupta, Hafiza Umair,
Atsu V ovor, and Natalie Durzynski. 2021. Offensive
language and hate speech detection with deep learning
and transfer learning. Preprint , arXiv:2108.03305.
Seth Wiener. 2011. Grass-mud horses to victory: The
phonological constraints of subversive puns.
Yunze Xiao, Houda Bouamor, and Wajdi Zaghouani.
2024. Chinese offensive language detection:current sta-
tus and future directions. Preprint , arXiv:2403.18314.
Zi Xiong, Lizhi Qing, Yangyang Kang, Jiawei Liu,
Hongsong Li, Changlong Sun, Xiaozhong Liu, and
Wei Lu. 2024. Enhance robustness of language models
against variation attack through graph integration. In
Proceedings of the 2024 Joint International Conference
on Computational Linguistics, Language Resources and
Evaluation (LREC-COLING 2024) , pages 5866â€“5877,
Torino, Italia. ELRA and ICCL.
Ga Zhang. 2024. Nmsl: A toolkit for deep learn-
ing neural network training and evaluation. https://github.com/THUzhangga/NMSL . Accessed: 2024-
05-27.
Yin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010. Under-
standing bag-of-words model: a statistical framework.
International Journal of Machine Learning and Cyber-
netics , 1:43â€“52.
Zihan Zhang, Jinfeng Li, Ning Shi, Bo Yuan, Xiangyu
Liu, Rong Zhang, Hui Xue, Donghong Sun, and Chao
Zhang. 2022. RoChBert: Towards robust BERT fine-
tuning for Chinese. In Findings of the Association for
Computational Linguistics: EMNLP 2022 , pages 3502â€“
3516, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Li Zhou, Laura Cabello, Yong Cao, and Daniel Her-
shcovich. 2023. Cross-cultural transfer learning for
chinese offensive language detection. arXiv preprint
arXiv:2303.17927 .
10