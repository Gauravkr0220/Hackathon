Fair Column Subset Selection
Antonis Matakos
antonis.matakos@aalto.fi
Aalto University
Espoo, FinlandBruno Ordozgoiti
bruno.ordozgoiti@gmail.com
Unaffiliated
London, UKSuhas Thejaswiâˆ—
thejaswi@mpi-sws.org
Max Planck Institute for Software
Systems
Kaiserslautern, Germany
ABSTRACT
The problem of column subset selection asks for a subset of columns
from an input matrix such that the matrix can be reconstructed as
accurately as possible within the span of the selected columns. A
natural extension is to consider a setting where the matrix rows
are partitioned into two groups, and the goal is to choose a subset
of columns that minimizes the maximum reconstruction error of
both groups, relative to their respective best rank- ğ‘˜approxima-
tion. Extending the known results of column subset selection to
this fair setting is not straightforward: in certain scenarios it is
unavoidable to choose columns separately for each group, resulting
in double the expected column count. We propose a deterministic
leverage-score sampling strategy for the fair setting and show that
sampling a column subset of minimum size becomes NP-hard in
the presence of two groups. Despite these negative results, we give
an approximation algorithm that guarantees a solution within 1.5
times the optimal solution size. We also present practical heuristic
algorithms based on rank-revealing QR factorization. Finally, we
validate our methods through an extensive set of experiments using
real-world data.
CCS CONCEPTS
â€¢Theory of computation â†’Unsupervised learning and clus-
tering; â€¢Computing methodologies â†’Feature selection; â€¢
Mathematics of computing â†’Dimensionality reduction.
KEYWORDS
Column Subset Selection, Dimensionality Reduction, Algorithmic
Fairness, Matrix Factorization, Low Rank Approximation
ACM Reference Format:
Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi. 2024. Fair Col-
umn Subset Selection. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3672005
âˆ—Part of this research was conducted when the author was employed at Aalto University,
Finland.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36720051 INTRODUCTION
Dimensionality reduction techniques such as principal component
analysis (PCA) and non-negative matrix factorization have proven
useful for machine learning and data analysis tasks [ 26,32]. Such
tasks include feature selection, feature extraction, noise removal
and data visualization, among others. These techniques are also
commonly employed as components of larger machine learning
(ML) pipelines to simplify data through lower-dimensional rep-
resentations. However, when the data is divided into subsets (or
groups), an inaccurate low-dimensional representation of any sub-
set can perpetuate inaccuracies in subsequent downstream tasks.
Therefore, there is increasing emphasis on developing techniques
that produce accurate representations for all the different groups.
Notably, Samadi et al . [38] showed that a well-known dimen-
sionality reduction technique, PCA, may incur higher average re-
construction error for a subset of the population than the rest, even
when the groups are of similar size. They proposed fair variants
of PCA, where the objective is to minimize the maximum recon-
struction error for any group. However, a drawback of PCA is that
the results are often hard to interpret, since its output consists of
abstract attributes that might not necessarily be part of the input
data. As an alternative to PCA, one may ask for algorithms that
choose a (small) subset of the original attributes of the dataset to
act as a low dimensional representation.
In the column subset selection problem (CSS), we are given a
data matrix and seek a representative subset of its columns. The
quality of the solution is measured by the residual norm when
the input matrix is projected onto the subspace spanned by the
chosen columns. CSShas been extensively studied, and polynomial-
time approximation algorithms are known for different quality
criteria [1, 4, 6, 13, 14, 31].
In this work, we study the fair column subset selection problem
(FairCSS), where the rows of the data matrix are partitioned into
two groups. The goal is to choose a subset of columns that minimize
the maximum reconstruction error of both groups, through a min-
max objective. This distinguishes our problem from conventional
CSS methods that aim to minimize the reconstruction error of
the entire data matrix and may neglect either group. Here, we
focus on the two-group setting. Generalizing our results to an
arbitrary number of groups is a non-trivial technical leap and thus
left as future work. Nevertheless, binary protected attributes are
encountered often in practice and are commonly the focus of works
on algorithmic fairness [11, 38].
To further motivate the study of FairCSS, consider its application
in drug discovery. Suppose we are given a large dataset of medical
records, where each row represents a patient and the columns
contain genetic information indicating presence (absence) of a gene
expression with binary value 1(0). Due to the size of the genetic
 
2189
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi
data, it is reasonable to ask for a succint representation of the
dataset. Using PCA would yield an arbitrary subspace of the data
that is hard to interpret. On the other hand, classical CSS would
return a representative set of columns (genes), however, if there
is bias in the medical records then the reconstruction error could
skew favouring a (majority) group. Thus, any subsequent insights
derived from the biased (succint) data could also perpetuate these
biases. It is possible that, a drug discovered using this biased data is
more (less) effective for men (women), based on the data majority.
Our proposed method, however, returns a subset of columns that
are representative of both groups, thus mitigating potential biases
and ensuring fair representation in subsequent analysis.
The two-group setting of CSSintroduces significant challenges,
making it necessary to view the problem through a new lens. In
CSS, finding the optimal ğ‘˜-column subset is NP-hard [ 39] and thus,
polynomial-time approximation algorithms are usually sought. A
factor ofğ‘‚(ğ‘˜)with respect to the best rank- ğ‘˜approximation is
possible if we are allowed to choose at most ğ‘˜columns [ 13], and
better results can be obtained if we allow ğ‘â‰¥ğ‘˜columns [ 5]. In
the two-group fair setting, similar approximation bounds can be
reproduced if we allow twice as many columns in the solution;
by optimizing for each group separately. However, addressing the
two groups separately can raise ethical and legal concerns [ 27,38],
potentially conflicting with the principles of demographic parity
and equal treatment [ 3]. Further, this approach may inadvertently
contribute to segregation and stereotyping. Therefore, our focus is
to find a common subset of representative columns for both groups.
Unfortunately, as we show with an example (see Â§ 3), in certain
scenarios it may not be possible to do better than choosing twice
as many columns. Can we hope to obtain guarantees of any kind in
this fair setting? We answer this question affirmatively. To achieve
this, we adapt leverage score sampling to our fair setting. Leverage
scores are obtained using the SVD of the matrix, and they are used
to find provably good column subsets for CSS. In particular, Papail-
iopoulos et al . [31] showed that by choosing columns with highest
leverage scores such that their sum adds to a predefined threshold ğœƒ,
a relative-error approximation is possible. In standard CSS, finding
a column subset of minimum size satisfying threshold ğœƒis trivial,
by sorting the leverage scores. In the fair setting however, finding
a subset of leverage scores of minimum size satisfying threshold
ğœƒfor both groups is NP-hard. While the original result can be ex-
tended to the fair setting by solving for both groups independently
and doubling the column count, we present an efficient algorithm
that achieves this with essentially 1.5as many columns. Whether
this factor can be improved is left as an open question. Finally, we
introduce efficient heuristics for the problem based on QR factoriza-
tions with pivoting, and assess their performance in our empirical
evaluation. Our contributions are summarized as follows:
â€¢We introduce the novel problem of fair column subset selection,
where two groups are present in the data and the maximum
reconstruction error of the two groups must be minimized.
â€¢We extend the approach of deterministic leverage score sam-
pling to the two-group fair setting. We show that the smallest
column subset that achieves the desired guarantees is NP-hard
to find, and give a polynomial-time algorithm wih relative-
error guarantees with a column subset of essentially 1.5times
the minimum possible size.â€¢We present efficient heuristics based on QR factorizations with
column pivoting.
â€¢We empirically evaluate our algorithms on real-world data and
show they are able to select fair columns with high accuracy.
Further, we analyse the price of fairness for our formulation.
The paper is organised as follows: Section 2 covers related work.
Section 3 outlines our problem. We adapt deterministic leverage
score sampling to the fair setting in Section 4. Algorithms based on
QR decomposition are given in Section 5. Experimental results are
presented in Section 6. Finally, we conclude in Section 7.
2 RELATED WORK
Our work builds upon related work in the areas of algorithmic
fairness and column subset selection.
Algorithmic fairness. The influential work of Dwork et al .
[16] established a formal notion of fairness in algorithmic decision-
making, which has served as a foundation for subsequent research
in the field. Pedreschi et al . [33] addressed discrimination in data
mining, while [ 23,24] proposed a framework for mitigating dis-
crimination in classification. Since then, there has been extensive
research on algorithmic fairness from various disciplines, includ-
ing economics, game theory, statistics, ethics, and computer sci-
ence [ 7,35â€“37]. Fair objectives have been introduced into many
classical computer science problems [ 2,11,18,20,38,42â€“44]. Fair
clustering share similarities to fair CSS, with a key distinction: the
former asks for a subset of fair representative data points (rows),
while the later focuses on finding a fair representative column
subset (attributes) [10, 11, 20]. For further reading see [25, 29, 34].
Column Subset Selection. CSScan be viewed as a method for
feature selection. Early work in CSS traces back to the numerical
linear algebra community, and the seminal works of Golub [21]
on pivoted QR factorizations, which was followed by works by
Chan and Hansen [8], Chan [9], Hong and Pan [22] addressing
the problem of finding efficient rank revealing QR factorizations
(RRQR). Recently, CSShas attracted interest in the computer science
community, with approaches that combine sampling with RRQR [ 6,
31], while greedy methods have proved highly effective [1, 17].
Fair PCA. Closest to our work is fair principal component anal-
ysis (FairPCA) by Samadi et al . [38] , where the goal is to find a low-
dimensional projection of the data to optimize a min-max objective
that ensures fairness with respect to two groups. Tantipongpipat
et al. [40] extended FairPCA to a multi-objective setting. Olfat and
Aswani [30] presented a polynomial-time algorithm using convex
programming for the general case with multiple groups. Similar to
FairPCA, we employ a min-max objective, but in contrast, we want
to find a subset of actual columns to approximate the reconstruction
error of both groups. In fact, we show that the problem we define,
fairCSS, is NP-hard for any number of groups (see Â§ 3).
3 PROBLEM STATEMENT
In this section, we describe relevant terminology and formally
define CSSbefore introducing fair CSS. For a positive integer ğ‘›we
denote[ğ‘›]={1,...,ğ‘›}. For any matrix ğ¶,ğ‘ƒğ¶denotes the projection
operator onto the subspace spanned by the columns of ğ¶.
Problem 1 (Column Subset Selection (CSS)). Given a matrix
ğ‘€âˆˆRğ‘šÃ—ğ‘›and a positive integer ğ‘˜. The goal is to choose ğ‘˜columns
 
2190Fair Column Subset Selection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ofğ‘€to form a matrix ğ¶âˆˆRğ‘šÃ—ğ‘˜such that the reconstruction error
ğ‘™ğ‘œğ‘ ğ‘ (ğ‘€,ğ¶)=âˆ¥ğ‘€âˆ’ğ‘ƒğ¶ğ‘€âˆ¥ğ¹
is minimized, where âˆ¥Â·âˆ¥ğ¹denotes the Frobenius norm.
Fairness. A solution to CSSgives the best possible column ap-
proximation for the matrix ğ‘€overall. However, if the matrix rows
are partitioned into two groups, it is possible that the reconstruction
error shows significant disparity when measured on each group
separately, for instance, if one of the groups is a minority. In the fair
setting, our goal is to choose a subset of columns that achieves good
reconstruction error for both groups. To formalize this, assume that
the rows of matrix ğ‘€are partitioned into two subsets ğ´andğµ.
Subscripting a matrix by ğ´orğµdenotes the rows corresponding to
ğ´orğµ. For ease of notation, we override ğ‘€ğ´=ğ´andğ‘€ğµ=ğµ.
Before presenting our fair CSS formulation, we make two con-
siderations. In CSS, the optimal projection is obtained as ğ‘ƒğ¶=ğ¶ğ¶+,
whereğ¶+is the pseudoinverse of ğ¶. In our setting, even though
we choose a common column subset for both groups, the optimal
reconstruction error for each group is attained by a different projec-
tion operator, that is, ğ¶ğ´ğ¶+
ğ´ğ´andğ¶ğµğ¶+
ğµğµ. Second, we are interested
in minimising the relative loss of each group, with respect to the
optimal reconstruction error, which can be obtained using the best
rank-ğ‘˜approximation. This is to avoid excessively penalizing ei-
ther of the two groups, when the other group does not have a good
low-rank representation. Note that measuring the reconstruction
error relatively to the best rank- ğ‘˜approximation is common in CSS
literature. See Boutsidis et al. [5], Papailiopoulos et al. [31].
Definition 1 (Relative group-wise reconstruction error).
Given a matrix ğ‘€âˆˆRğ‘šÃ—ğ‘›, a row subset ğ´âˆˆRğ‘Ã—ğ‘›, withğ´ğ‘˜its best
rank-ğ‘˜approximation, and a matrix ğ¶âˆˆRğ‘šÃ—ğ‘˜formed by choosing
ğ‘˜columns ofğ‘€. The relative reconstruction error of ğ´is:
ğ‘ğ‘™ğ‘œğ‘ ğ‘ ğ´(ğ‘€,ğ¶)=âˆ¥ğ´âˆ’ğ¶ğ´ğ¶+
ğ´ğ´âˆ¥ğ¹
âˆ¥ğ´âˆ’ğ´ğ‘˜âˆ¥ğ¹.
Now we formally define our problem.
Problem 2 (FairCSS-MinMax). Given a matrix ğ‘€âˆˆRğ‘šÃ—ğ‘›with
row partition ğ´,ğµ, and a positive integer ğ‘˜. The goal is to choose ğ‘˜
columns ofğ‘€to form a matrix ğ¶âˆˆRğ‘šÃ—ğ‘˜that optimizes the objective:
min
ğ¶âˆˆRğ‘šÃ—ğ‘˜,
ğ¶âŠ‚ğ‘€maxâˆ¥ğ´âˆ’ğ¶ğ´ğ¶+
ğ´ğ´âˆ¥ğ¹
âˆ¥ğ´âˆ’ğ´ğ‘˜âˆ¥ğ¹,âˆ¥ğµâˆ’ğ¶ğµğ¶+
ğµğµâˆ¥ğ¹
âˆ¥ğµâˆ’ğµğ‘˜âˆ¥ğ¹
.
Thus, we search for a subset of columns that minimizes the
maximum error of either groups. Observe that when groups ğ´and
ğµare identical, FairCSS-MinMax is equivalent to CSS, which is
known to be NP-hard [ 39][Theoreom 2.2]. The hardness results
extend to FairCSS-MinMax. In line with preceding arguments,
FairCSS-MinMax isNP-hard for any number of groups.
Observation 1. FairCSS-MinMax isNP-hard.
Limitations. We show that, in some scenarios it is not possible
to do better than solving for two groups separately. In particular,
any algorithm that attempts to solve CSSfor more than one group,
i.e., to achieve errors smaller than âˆ¥ğ´âˆ¥2
ğ¹andâˆ¥ğµâˆ¥2
ğ¹forğ´andğµ,
respectively, cannot achieve meaningful bounds on the relative-
error with respect to both âˆ¥(ğ´)ğ‘˜âˆ¥2
ğ¹andâˆ¥(ğµ)ğ‘˜âˆ¥2
ğ¹. Consider thefollowing example:ğ‘‹ğ´0ğ´
0ğµğ‘‹ğµ
,
whereğ‘‹ğ´,ğ‘‹ğµare matrices of rank ğ‘›>ğ‘˜. If we pick less than
2ğ‘˜columns, the error is at least minÃğ‘›
ğ‘–=ğ‘˜ğœ2
ğ‘–(ğ´)Ãğ‘›
ğ‘–=ğ‘˜+1ğœ2
ğ‘–(ğ´),Ãğ‘›
ğ‘–=ğ‘˜ğœ2
ğ‘–(ğµ)Ãğ‘›
ğ‘–=ğ‘˜+1ğœ2
ğ‘–(ğµ)
,
where{ğœ1,...,ğœğ‘›}are the singular values with ğœğ‘–â‰¥ğœğ‘–+1. Thus,
the relative error can be unbounded if the rank of either submatrix
is numerically close to ğ‘˜, i.e., ifğœğ‘˜(ğ´) â‰«ğœğ‘˜+1(ğ´)orğœğ‘˜(ğµ) â‰«
ğœğ‘˜+1(ğµ). Clearly, the only way to prevent this is to pick 2ğ‘˜columns.
A similar result is yielded by matrices where the blocks of zeroes
are replaced by small values. Despite this, in the following section
we present an algorithm with a bounded-error relative to the best
rank-ğ‘˜approximation, by relaxing the requirement on the number
of selected columns.
4 PAIRS OF LEVERAGE SCORES
In this section, we discuss leverage scores in the context of the
two-group fair setting. Leverage scores are a pivotal concept that
is extensively studied in CSS literature, as they provide valuable
insights for selecting column subsets with provable approximation
guarantees. We begin by discussing the results of [ 31] on leverage
score sampling. Next, we show that sampling a subset of columns of
minimum size in the two-group fair setting is NP-hard. Finally, we
present an approximation algorithm that samples 1.5times the size
of an optimal solution. An useful concept that is extensively studied
inCSSliterature is leverage scores, which is defined as follows:
Definition 2 (Leverage scores). Letğ‘‰(ğ‘˜)âˆˆRğ‘›Ã—ğ‘˜denote the
top-ğ‘˜right singular vectors of a ğ‘šÃ—ğ‘›matrixğ‘€with rankğœŒ=
ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘€)â‰¥ğ‘˜. Then, the rank- ğ‘˜leverage score of the ğ‘–-th column of
ğ‘€is defined as:
â„“(ğ‘˜)
ğ‘–(ğ‘€)=âˆ¥[ğ‘‰(ğ‘˜)]ğ‘–,:âˆ¥2
2for allğ‘–âˆˆ[ğ‘›].
Here,[ğ‘‰(ğ‘˜)]ğ‘–,:denotes theğ‘–-th row ofğ‘‰(ğ‘˜).
Leverage scores are used to find a solution with approximation
guarantees for CSS. In particular, we focus on the following result
by Papailiopoulos et al. [31].
Theorem 1 ([ 31]).Given a matrix ğ‘€âˆˆRğ‘šÃ—ğ‘›and an integer
ğ‘˜<rank(ğ‘€). Letğœƒ=ğ‘˜âˆ’ğœ–for someğœ–âˆˆ(0,1)andğ‘†be a subset
of column indices such thatÃ
ğ‘–âˆˆğ‘†â„“(ğ‘˜)
ğ‘–â‰¥ğœƒ, andğ¶âˆˆRğ‘šÃ—ğ‘˜be the
matrix ofğ‘€formed by choosing the columns with indices in ğ‘†. Then
we have that
âˆ¥ğ‘€âˆ’ğ¶ğ¶+ğ‘€âˆ¥2
ğ¹â‰¤(1âˆ’ğœ–)âˆ’1âˆ¥ğ‘€âˆ’ğ‘€ğ‘˜âˆ¥2
ğ¹.
In essence, the above result implies that by selecting a column
subset whose leverage scores sum to at least threshold ğœƒ, we obtain
a relative error guarantee with respect to the best rank- ğ‘˜approxi-
mation. They proposed a deterministic algorithm that picks ğ‘â‰¥ğ‘˜
columns with the largest leverage scores that sum to at least thresh-
oldğœƒ. The algorithm runs in time ğ‘‚(min{ğ‘š,ğ‘›}ğ‘šğ‘›).
Depending on the leverage score distribution, sometimes it may
be necessary to pick more than ğ‘˜columns to satisfy the threshold ğœƒ,
andÎ©(ğ‘›)in the worst case. Nevertheless, when the leverage scores
follow power-law decay, a small factor of ğ‘˜suffices [ 31][Theorem 3].
 
2191KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi
Fair deterministic leverage-score sampling. In order to achieve
approximation guarantees for both groups, the leverage scores of
bothğ´andğµmust sum to at least ğœƒ, individually. While seeking
the minimum number of columns to satisfy the threshold is trivial
in the single-group setting, it becomes NP-hard in the presence
of two groups. Let us formally define the problem and analyse its
complexity.
Problem 3 (Min-FairnessScores). Given matrices ğ´âˆˆRğ‘šğ´Ã—ğ‘›
andğµâˆˆRğ‘šğµÃ—ğ‘›,ğ‘˜âˆˆN: 0<ğ‘˜<rank(ğ´),rank(ğµ)and a threshold
ğœƒ=ğ‘˜âˆ’ğœ–for someğœ–âˆˆ(0,1), find the smallest set of indices ğ‘†âŠ†[ğ‘›]
such that:âˆ‘ï¸
ğ‘–âˆˆğ‘†â„“(ğ‘˜)
ğ‘–(ğ´)â‰¥ğœƒandâˆ‘ï¸
ğ‘–âˆˆğ‘†â„“(ğ‘˜)
ğ‘–(ğµ)â‰¥ğœƒ.
If we find a subset of columns that satisfy both inequalities above,
then due to Theorem 1, we have that,
âˆ¥ğ´âˆ’ğ¶ğ´ğ¶+
ğ´ğ´âˆ¥ğ¹â‰¤(1âˆ’ğœ–)âˆ’1/2âˆ¥ğ´âˆ’ğ´ğ‘˜âˆ¥ğ¹=â‡’
ğ‘ğ‘™ğ‘œğ‘ ğ‘ ğ´(ğ‘€,ğ¶)â‰¤( 1âˆ’ğœ–)âˆ’1/2.
and similarly, ğ‘ğ‘™ğ‘œğ‘ ğ‘ ğµ(ğ‘€,ğ¶)â‰¤( 1âˆ’ğœ–)âˆ’1/2.
A solution to Problem 3 gives us an upper bound on the recon-
struction error. Unfortunately, MinFairnessScores isNP-hard.
Theorem 2. MinFairnessScores isNP-hard.
Proof. To establish hardness we reduce the equal cardinality
partition problem (EqCardPartition) to a decision version of
MinFairnessScores, called ğ‘-FairnessScores. The decision ver-
sion asks to find exactlyğ‘indices i.e., ğ‘†âŠ† [ğ‘›],|ğ‘†|=ğ‘such
thatÃ
ğ‘–âˆˆğ‘†â„“(ğ‘˜)
ğ‘–(ğ´) â‰¥ğœƒandÃ
ğ‘–âˆˆğ‘†â„“(ğ‘˜)
ğ‘–(ğµ) â‰¥ğœƒ. Given a set ğ‘=
{ğ‘1,...,ğ‘ğ‘›}ofğ‘›positive integers, EqCardPartition asks to parti-
tionğ‘into two disjoint subsets ğ‘‹,ğ‘Œ such thatğ‘‹âˆªğ‘Œ=ğ‘,|ğ‘‹|=|ğ‘Œ|
andÃ
ğ‘ğ‘–âˆˆğ‘‹ğ‘ğ‘–=Ã
ğ‘ğ‘—âˆˆğ‘Œğ‘ğ‘—.EqCardPartition is known to be NP-
complete [19, SP12].
Given an instance of EqCardPartition (ğ‘,ğ‘›), we reduce it to a
ğ‘-FairnessScores instance(ğ´,ğµ,ğœƒ,ğ‘)as follows. Let ğ‘ =Ã
ğ‘ğ‘–âˆˆğ‘ğ‘ğ‘–
andğ‘€â‰«ğ‘ be some constant. Let ğ´=[âˆšğ‘1,...,âˆšğ‘ğ‘›]andğµ=
âˆšï¸ƒ
ğ‘€âˆ’ğ‘1
ğ‘ ,...,âˆšï¸ƒ
ğ‘€âˆ’ğ‘ğ‘›
ğ‘ be input matrices such that ğ´âˆˆRğ‘›Ã—1,
ğµâˆˆRğ‘›Ã—1. Finally, we set ğœƒ=1/2andğ‘=ğ‘›/2. We claim that
EqCardPartition is ayesinstance if and only if ğ‘-FairnessScores
is ayesinstance. The reduction is polynomial in the input size.
To make the reduction work, we need a way to map the values
in the instance EqCardPartition to leverage scores, which are
obtained from SVD. The most straightforward way is to compute
the rank-1 leverage scores. For all ğ‘–âˆˆ[ğ‘›]we can get,
 â„“(1)
ğ‘–(ğ´),â„“(1)
ğ‘–(ğµ)=ğ‘ğ‘–
ğ‘ ,ğ‘€âˆ’ğ‘ğ‘–
ğ‘ 
ğ‘›ğ‘€âˆ’1
.
For ease of notation, let ğ›¼ğ‘–=â„“(ğ‘˜)
ğ‘–(ğ´)andğ›½ğ‘–=â„“(ğ‘˜)
ğ‘–(ğµ).
Letğ‘-FairnessScores be ayesinstance. Then we have a subset
ğ‘†âŠ†[ğ‘›]such that|ğ‘†|=ğ‘›/2,
âˆ‘ï¸
ğ‘–âˆˆğ‘†ğ›¼ğ‘–=Ã
ğ‘–âˆˆğ‘†ğ‘ğ‘–
ğ‘ â‰¥1
2which impliesâˆ‘ï¸
ğ‘–âˆˆğ‘†ğ‘ğ‘–â‰¥ğ‘ 
2,
âˆ‘ï¸
ğ‘–âˆˆğ‘†ğ›½ğ‘–=Ã
ğ‘–âˆˆğ‘†(ğ‘€âˆ’ğ‘ğ‘–
ğ‘ )
ğ‘›ğ‘€âˆ’1â‰¥1
2which impliesâˆ‘ï¸
ğ‘–âˆˆğ‘†ğ‘ğ‘–â‰¤ğ‘ 
2.For both of the above inequalities to hold simultaneously, we must
haveÃ
ğ‘–âˆˆğ‘†ğ‘ğ‘–=ğ‘ /2. Thus,ğ‘‹={ğ‘ğ‘–:ğ‘–âˆˆğ‘†},ğ‘Œ={ğ‘ğ‘–:ğ‘–âˆˆ[ğ‘›]\ğ‘†}is
a solution to EqCardPartition.
For the sake of contradiction, assume that ğ‘-FairnessScores is
anoinstance and ğ‘‹,ğ‘Œ are a solution to EqCardPartition. Let ğ‘†
be the set of indices of elements in ğ‘‹. We can choose ğ‘†as solution
forğ‘-FairnessScores since|ğ‘†|=ğ‘›/2,
âˆ‘ï¸
ğ‘–âˆˆğ‘†ğ›¼ğ‘–=âˆ‘ï¸
ğ‘–âˆˆğ‘†ğ‘ğ‘–/ğ‘ =1
2,andâˆ‘ï¸
ğ‘–âˆˆğ‘†ğ›½ğ‘–=Ã
ğ‘–âˆˆğ‘†(ğ‘€âˆ’ğ‘ğ‘–
ğ‘ )
ğ‘›ğ‘€âˆ’1=1
2,
which is a contradiction. Thus, EqCardPartition must be a no
instance.
Given a solution ğ‘†âŠ† [ğ‘›]forğ‘-FairnessScores, we can ver-
ify in polynomial time if |ğ‘†|=ğ‘, compute SVD as well as check
ifÃ
ğ‘–âˆˆğ‘†ğ›¼ğ‘–â‰¥ğœƒandÃ
ğ‘–âˆˆğ‘†ğ›½ğ‘–â‰¥ğœƒ. Thus,ğ‘-FairnessScores isNP-
complete. Naturally, ğ‘-FairnessScores reduces to MinFairnessS-
cores, as the latter finds the smallest ğ‘such that a solution to ğ‘-
FairnessScores exists. Thus, MinFairnessScores isNP-hard. â–¡
Even though MinFairnessScores isNP-hard, a 2-factor approx-
imation is trivial: sort â„“(ğ‘˜)
ğ‘–(ğ´)â€™s in decreasing order and choose
indices with highest leverage scores until they sum to ğœƒ. Repeat the
same forâ„“(ğ‘˜)
ğ‘–(ğµ)â€™s. This results in at most 2ğ‘columns, where ğ‘is
the optimal number of columns to satisfy ğœƒ.
Algorithm 1: FairScoresSampler
Input:ğ‘ƒ={(ğ›¼1,ğ›½1),...,(ğ›¼ğ‘›,ğ›½ğ‘›)},ğœƒ
Output:ğ‘†âŠ†ğ‘ƒ
1ğ‘†â†âˆ… ,ğ‘„â†âˆ…
// add(ğ›¼ğ‘—,ğ›½ğ‘—)toğ‘†until either of the thresholds is
satisfied
2whileÃ
(ğ›¼ğ‘–,ğ›½ğ‘–)âˆˆğ‘†ğ›¼ğ‘–<ğœƒandÃ
(ğ›¼ğ‘–,ğ›½ğ‘–)âˆˆğ‘†ğ›½ğ‘–<ğœƒdo
3(ğ›¼ğ‘—,ğ›½ğ‘—)â† max(ğ›¼ğ‘—,ğ›½ğ‘—)âˆˆğ‘ƒ\ğ‘†(ğ›¼ğ‘—+ğ›½ğ‘—)
4ğ‘†â†ğ‘†âˆª(ğ›¼ğ‘—,ğ›½ğ‘—)
// findğ‘„âˆˆğ‘ƒ\ğ‘†such that the other threshold is
satisfied
5ifÃ
(ğ›¼ğ‘–,ğ›½ğ‘–)âˆˆğ‘†ğ›¼ğ‘–â‰¥ğœƒthen
6ğ‘„â†argmin|ğ‘„|Ã
ğ‘—âˆˆğ‘„ğ›½ğ‘—â‰¥ğœƒ
7else
8ğ‘„â†argmin|ğ‘„|Ã
ğ‘—âˆˆğ‘„ğ›¼ğ‘—â‰¥ğœƒ
9ğ‘†â†ğ‘†âˆªğ‘„
10returnğ‘†
Next, we present an algorithm for MinFairnessScores that
returns at mostâŒˆ3ğ‘/2âŒ‰+1columns (â‰ˆ1.5-approximation).1The
pseudocode is in Algorithm 1. For ease of notation, we denote
ğ›¼ğ‘–=â„“(ğ‘˜)
ğ‘–(ğ´)andğ›½ğ‘–=â„“(ğ‘˜)
ğ‘–(ğµ). The algorithm proceeds in two
stages. In the first stage, at each iteration add to ğ‘†the indexğ‘–
such that the cumulative gain ğ›¼ğ‘–+ğ›½ğ‘–is maximized until a step
ğ‘¡â‰¤ğ‘›, where at least one of the inequalities is satisfied, i.e., eitherÃ
ğ‘–âˆˆğ‘†ğ›¼ğ‘–â‰¥ğœƒorÃ
ğ‘–âˆˆğ‘†ğ›½ğ‘–â‰¥ğœƒ. In the second step, sort the tuples
of leverage scores in [ğ‘›]\ğ‘†based on their contribution to the
unsatisfied inequality, in descending order. Finally, pick the rest of
1An additional column (+1) is required in case ğ‘is odd.
 
2192Fair Column Subset Selection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
the tuples based on this order, until the threshold ğœƒis satisfied. The
following theorem establishes our approximation result. Note that
the approximation is in terms of the number of columns ğ‘in the
optimal solution, and we have already established an (1âˆ’ğœ–)âˆ’1/2
approximation in terms of our objective function.
Theorem 3. Algorithm 1 returns a solution of at most âŒˆ3
2ğ‘âŒ‰+1
columns for MinFairnessScores, where ğ‘is the number of columns
in the optimal solution.
Proof. Givenğ‘ƒ={(ğ›¼1,ğ›½1),...,(ğ›¼ğ‘›,ğ›½ğ‘›)}, the task is to find the
smallest subset ğ‘†âŠ†ğ‘ƒsuch thatÃ
ğ‘–âˆˆğ‘†ğ›¼ğ‘–â‰¥ğœƒandÃ
ğ‘–âˆˆğ‘†ğ›½ğ‘–â‰¥ğœƒ. At
each iteration we select a tuple with the maximum contribution, i.e.,
ğ›¼ğ‘—+ğ›½ğ‘—until some step ğ‘¡, where eitherÃ
ğ‘–âˆˆğ‘†ğ›¼ğ‘–â‰¥ğœƒorÃ
ğ‘–âˆˆğ‘†ğ›½ğ‘–â‰¥ğœƒ
is satisfied. Without loss of generality, at step ğ‘¡we assume thatÃ
(ğ›¼ğ‘–,ğ›½ğ‘–)âˆˆğ‘†ğ›¼ğ‘–â‰¥ğœƒ. Letğ‘†âˆ—be the optimal solution and (ğ›¼âˆ—
ğ‘–,ğ›½âˆ—
ğ‘–)de-
note the contribution of ğ‘–-th tuple inğ‘†âˆ—. We assume ğ‘†âˆ—is sorted in
decreasing order according to ğ›¼âˆ—
ğ‘–+ğ›½âˆ—
ğ‘–. We note that this assumption
makes our analysis easier without losing generality.
First we establish that ğ‘¡â‰¤ğ‘. Assume for contradiction that ğ‘¡>ğ‘
and thatÃğ‘¡
ğ‘–=1ğ›¼ğ‘–<ğœƒand thatÃğ‘¡
ğ‘–=1ğ›½ğ‘–<ğœƒ. Thus the algorithm has
not terminated. From the optimality of ğ›¼ğ‘–+ğ›½ğ‘–at stepğ‘¡it holds that,
ğ‘¡âˆ‘ï¸
ğ‘–=1ğ›¼ğ‘–+ğ‘¡âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–â‰¥ğ‘¡âˆ‘ï¸
ğ‘–=1ğ›¼âˆ—
ğ‘–+ğ‘¡âˆ‘ï¸
ğ‘–=1ğ›½âˆ—
ğ‘–,
ğ‘¡âˆ‘ï¸
ğ‘–=1ğ›¼ğ‘–â‰¥ğ‘âˆ‘ï¸
ğ‘–=1ğ›¼âˆ—
ğ‘–+ğ‘âˆ‘ï¸
ğ‘–=1ğ›½âˆ—
ğ‘–âˆ’ğ‘¡âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–â‰¥2ğœƒâˆ’ğ‘âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–â‰¥ğœƒ,
which is a contradiction, since we assumedÃğ‘¡
ğ‘–=1ğ›¼ğ‘–<ğœƒ. Thus ifÃğ‘¡
ğ‘–=1ğ›¼ğ‘–â‰¥ğœƒthen 1â‰¤ğ‘¡â‰¤ğ‘. We now discern two cases for the value
ofğ‘¡.
Caseğ‘¡â‰¤âŒˆğ‘
2âŒ‰+1:We haveÃğ‘¡
ğ‘–=1ğ›¼ğ‘–â‰¥ğœƒ. To satisfy the second
inequality we choose tuples in ğ‘ƒ\ğ‘†in decreasing order according
toğ›½ğ‘–, which is at most ğ‘columns, since the optimal solution has ğ‘
columns. So, we have a solution with size |ğ‘†|â‰¤ğ‘¡+ğ‘=âŒˆ3
2ğ‘âŒ‰+1.
CaseâŒˆğ‘
2âŒ‰+1<ğ‘¡â‰¤ğ‘:Again, from the optimality of ğ›¼ğ‘–+ğ›½ğ‘–at
each step in 1â‰¤ğ‘¡â‰¤ğ‘we have,
ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=1ğ›¼ğ‘–+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–â‰¥ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=1ğ›¼âˆ—
ğ‘–+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=1ğ›½âˆ—
ğ‘–
â‰¥âŒˆğ‘
2âŒ‰âˆ‘ï¸
ğ‘–=1ğ›¼âˆ—
ğ‘–+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=âŒˆğ‘
2âŒ‰+1ğ›¼âˆ—
ğ‘–+âŒˆğ‘
2âŒ‰âˆ‘ï¸
ğ‘–=1ğ›½âˆ—
ğ‘–+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=âŒˆğ‘
2âŒ‰+1ğ›½âˆ—
ğ‘–
â‰¥ğœƒ+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=âŒˆğ‘
2âŒ‰+1ğ›¼âˆ—
ğ‘–+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=âŒˆğ‘
2âŒ‰+1ğ›½âˆ—
ğ‘–
The third step follows from the assumption that the optimal solution
ğ‘†âˆ—has tuples sorted in decreasing order according to ğ›¼âˆ—
ğ‘–+ğ›½âˆ—
ğ‘–. We
also observe thatÃğ‘¡âˆ’1
ğ‘–=1ğ›¼ğ‘–=ğœƒâˆ’ğ›¼ğ‘¡. Therefore we have
ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–â‰¥ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=ğ‘
2+1ğ›¼âˆ—
ğ‘–+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=ğ‘
2+1ğ›½âˆ—
ğ‘–+ğ›¼ğ‘¡
Supposeğ‘¡=âŒˆğ‘
2âŒ‰+1+ğ‘. This means we can afford to add at most
ğ‘âˆ’ğ‘columns to our solution if we want to satisfy our bound on
the cardinality of ğ‘†.At this stage, the algorithm picks the columns with the largest
values ofğ›½âˆ—
ğ‘–. This means that from those in the optimal solution,
we miss at most ğ‘ğ›½âˆ—
ğ‘–â€™s from among the bottom ones. From above,
we have
ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–â‰¥ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=ğ‘
2+1ğ›¼âˆ—
ğ‘–+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=ğ‘
2+1ğ›½âˆ—
ğ‘–â‰¥ğ‘âˆ‘ï¸
ğ‘–=ğ‘âˆ’ğ‘ğ›¼âˆ—
ğ‘–+ğ‘âˆ‘ï¸
ğ‘–=ğ‘âˆ’ğ‘ğ›½âˆ—
ğ‘–â‰¥ğ‘âˆ‘ï¸
ğ‘–=ğ‘âˆ’ğ‘ğ›½âˆ—
ğ‘–.
This holds since tuples of the optimal solution are sorted in decreas-
ing order of the value of the pair sums, which implies that the value
we miss from not adding the last ğ‘ğ›½âˆ—
ğ‘–â€™s is already covered by what
we had,Ãğ‘¡âˆ’1
ğ‘–=1ğ›½ğ‘–, so|ğ‘†|âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–â‰¥ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=1ğ›½ğ‘–+ğ‘âˆ’ğ‘âˆ‘ï¸
ğ‘–=1ğ›½âˆ—
ğ‘–â‰¥ğœƒ.So the solution
has at most ğ‘¡âˆ’1+ğ‘âˆ’ğ‘=âŒˆğ‘
2âŒ‰+ğ‘+ğ‘âˆ’ğ‘=âŒˆ3
2âŒ‰ğ‘+1columns and
satisfies the threshold. â–¡
Even though Algorithm 1 offers an upper bound on the number of
columns in the optimal solution ğ‘, depending on the task at hand, it
may be undesirable to obtain more than ğ‘˜columns. CSSwith exactly
ğ‘˜columns (the definition of Problem 1) is a well-studied problem
and a wide range of algorithms have been developed for it. These
algorithms are typically based on QR-decomposition. Motivated by
this, in the following section we propose two QR-decomposition-
related algorithms for FairCSS.
Recall the impossibility results from Section 3: unless we pick
2ğ‘˜columns it may be impossible to achieve a relative-error ap-
proximation in terms of the rank- ğ‘˜reconstruction error. Thus, the
following algorithms are heuristics. They can be used either di-
rectly for FairCSS-MinMax or part of a two-stage approach, that
we describe in detail in Section 6.
5 FAIR QRDECOMPOSITIONS
Numerous practical algorithms for CSS originate from numerical
linear algebra, often relying on QRdecomposition with column
pivoting.
Definition 3 (QR decomposition with column pivoting).
Given a matrix ğ‘€âˆˆRğ‘šÃ—ğ‘›withğ‘šâ‰¥ğ‘›and an integer ğ‘˜â‰¤ğ‘›.
Matrixğ‘€can be expressed as the product of an orthonormal matrix
ğ‘„âˆˆRğ‘šÃ—ğ‘šand an upper triangular matrix ğ‘…âˆˆRğ‘›Ã—ğ‘›. More precisely,
ğ‘€Î =ğ‘„ğ‘…=ğ‘„ğ‘…11ğ‘…12
0ğ‘…22
,
whereğ‘…11âˆˆRğ‘˜Ã—ğ‘˜,ğ‘…12âˆˆRğ‘˜Ã—(ğ‘›âˆ’ğ‘˜),ğ‘…22âˆˆR(ğ‘›âˆ’ğ‘˜)Ã—(ğ‘›âˆ’ğ‘˜)and
Î âˆˆRğ‘›Ã—ğ‘›is a permutation matrix.
Column pivoting involves finding a permutation matrix Î for a
given matrix ğ‘€, such thatâˆ¥ğ‘…22âˆ¥ğ¹is minimised. When this comes
with certain guarantees, it leads to a rank revealing QR decompo-
sition (RRQR), which forms the basis for various algorithms with
approximation guarantees in CSS. Specifically, if we denote the first
ğ‘˜columns of the permutation matrix Î asÎ ğ‘˜, then choosing the
column subset ğ¶=ğ‘€Î ğ‘˜guarantees that ğ‘™ğ‘œğ‘ ğ‘ (ğ‘€,ğ¶)is bounded
i.e.,âˆ¥ğ‘€âˆ’ğ‘ƒğ¶ğ‘€âˆ¥ğ¹=âˆ¥ğ‘…22âˆ¥ğ¹[6]. We tailor Low-RRQR (L-RRQR),
originally introduced by [ 8], to accommodate the two-group fair
setting.
We present a brief description of L-RRQR. We start with the QR-
decomposition of matrix ğ‘€to obtain matrices ğ‘„andğ‘…. We initialise
 
2193KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi
ğ‘…11=0,ğ‘…22=ğ‘…and buildğ‘…11incrementally through column
pivoting: at each iteration we permute a column of ğ‘…22to the first
position, through Î . Then, we compute the QR-decomposition
again, drop the first row and column of the resulting ğ‘…, and proceed
recursively on it. The SVD serves in finding the pivot column: if
ğ‘£âˆˆRğ‘›is the right singular vector corresponding to the largest
singular value, then successive permutations such that |(Î ğ‘‡ğ‘£)|1=
âˆ¥ğ‘£âˆ¥âˆlead to a provably small âˆ¥ğ‘…22âˆ¥ğ¹.
Fair pivoting. Note that L-RRQR may introduce unfairness,
since we factorize the matrix ğ‘€, without considering the error
of the two groups, ğ´andğµseparately. So the pivoted columns
may benefit only one group. To address this, we adapt the pivoting
strategy of L-RRQR to benefit the group that suffers the worse
reconstruction error. Thus, we perform simultaneous RRQR on
ğ´andğµand at step- ğ‘–obtain the corresponding ğ‘„ğ´(ğ‘–),ğ‘…ğ´(ğ‘–)and
ğ‘„ğµ(ğ‘–),ğ‘…ğµ(ğ‘–). We inspect the spectra of both ğ‘…ğ´(ğ‘–)andğ‘…ğµ(ğ‘–)and
choose the pivot based on the following strategy: we choose the
right singular vector ğ‘£of eitherğ‘…ğ´(ğ‘–)orğ‘…ğµ(ğ‘–)corresponding to
max{ğœ1(ğ‘…ğ´
22(ğ‘–)),ğœ1(ğ‘…ğµ
22(ğ‘–))}, and select Î such that|(Î ğ‘‡ğ‘£)|1=
âˆ¥ğ‘£âˆ¥âˆ. The algorithm pseudocode can be seen in Algorithm 2.
Algorithm 2: Fair L-RRQR
1Input: QR factorizations ğ´Î ğ´=ğ‘„ğ´ğ‘…ğ´,ğµÎ ğµ=ğ‘„ğµğ‘…ğµ,ğ‘˜
2Output: permutation Î ğ‘˜
1:fori=1,. . . ,k do
2:ğ‘…ğ´
22â†ğ‘…ğ´[ğ‘–:,ğ‘–:],ğ‘…ğµ
22â†ğ‘…ğµ[ğ‘–:,ğ‘–:]
3:ğ‘£â†max{ğœ1(ğ‘…ğ´
22(ğ‘–)),ğœ1(ğ‘…ğµ
22(ğ‘–))}
4: Compute permutation ğ‘ƒsuch that|(ğ‘ƒğ‘‡ğ‘£)1|=âˆ¥ğ‘ƒğ‘‡ğ‘£âˆ¥âˆ
5: Compute QR fact. ğ‘…ğ´
22ğ‘ƒ=ğ‘„ğ´
1Ëœğ‘…ğ´
22andğ‘…ğµ
22ğ‘ƒ=ğ‘„ğµ
1Ëœğ‘…ğµ
22
6:Î â†Î ğ¼0
0ğ‘ƒ
7:ğ‘…ğ´â† 
ğ‘…ğ´
11ğ‘„ğ´
1ğ‘‡ğ‘…ğ´
12
0 Ëœğ‘…ğ´
22!
andğ‘…ğµâ† 
ğ‘…ğµ
11ğ‘„ğµ
1ğ‘‡ğ‘…ğµ
12
0 Ëœğ‘…ğµ
22!
8:end for
returnğ‘†
6 EXPERIMENTS
This section describes the experimental setup, datasets used, and
presents the experimental evaluation results.
Experimental setup. Our implementation is written in python .
We use numpy, scipy andscikit-learn for preprocessing, linear
algebra operations as well as parallelization. Experiments are exe-
cuted on a compute node with 32 cores and 256GB of RAM. Our
implementations are available as open source [28].
Datasets. We use juvenile recidivism data (recidivism) from Cata-
lunya [ 41] and medical expenditure survey data 2015 (meps) [ 12],
as well as various datasets from the UCI-ML repository [ 15]:â€œheart-
cleveland" (heart), â€œadult" (adult), â€œgerman-credit" (german), â€œcredit-
card" (credit), â€œstudent performance" (student), â€œcompas-recidivism"
(compas), â€œcommunities" (communities). Data is processed by remov-
ing protected attributes, converting categorical variables to one-hot
encoding and normalizing each column to unit ğ¿2-norm. Group
membership is based on Sex, except for communities where group
membership is majority white or a non-white community. Dataset
statistics are reported in Table 1.Table 1: Dataset statistics. ğ‘šğ´andğ‘šğµare the number of in-
stances in groups ğ´andğµ, respectively. ğ‘›is the number of
columns.ğ›¾(ğ´),ğ›¾(ğµ) is rank of ğ´,ğµ.
Dataset ğ‘› ğ‘šğ´ğ‘šğµğ›¾(ğ´)ğ›¾(ğµ)
heart 14 201 96 13 13
german 63 690 310 49 47
credit 25 18 112 11 888 24 24
student 58 383 266 42 42
adult 109 21 790 10 771 98 98
compas 189 9 336 2 421 167 73
communities 104 1 685 309 101 101
recidivism 227 1 923 310 175 113
meps 1 247 18 414 17 013 1 217 1 200
Experimental evaluation. Our experiments are in threefold.
First, we assess the efficacy of the proposed algorithms in addressing
the problem of FairCSS-MinMax. This evaluation involves com-
paring the performance of the proposed algorithms, considering
various experimental setups. Second, we evaluate the effectiveness
of the FairCSS-MinMax objective in selecting column subsets that
result in fair reconstruction errors. Specifically, we compare the re-
construction errors of each group in the optimal solutions obtained
using the vanilla CSSobjective versus the FairCSS-MinMax objec-
tive. Last, we investigate the price of fairness. This entails verifying
potential trade-offs or costs associated to attain fairness according
toFairCSS-MinMax.
6.1 Algorithms Evaluation
Algorithms. We refer Algorithm 1 as FairScoresSampler and
fairL-RRQR asLow QR. We also consider the fair version of a
variant of L-RRQR, called H-RRQR [9], (High QR). For details on
this algorithm see Supplementary A. The complexity of our al-
gorithms is dominated by SVD. At each step, Low QR andHigh
QRrequireğ‘‚(ğ‘˜)andğ‘‚(ğ‘›âˆ’ğ‘˜)time, respectively. On the other
hand, FairScoresSampler computes SVD once for each group, and
requiresğ‘‚(ğ‘›logğ‘›)for sorting tuples.
We complement our algorithms with a Greedy algorithm: at
each step it picks the column with the highest direct gain according
toMinMaxLoss. The complexity is dominated by matrix multipli-
cationğ‘‚(ğ‘›Î©). Finally, in Random, we randomly sample ğ‘˜-column
set for 100repetitions and choose a set with best score.
Pickingğ‘â‰¥ğ‘˜columns. Recall that FairScoresSampler chooses
columns based on threshold ğœƒ, and it can choose ğ‘â‰¥ğ‘˜columns.
In the first experiment, we evaluate the algorithms performance
with respect to a specific low-rank subspace of ğ´andğµover dif-
ferent values of ğ‘. Thus, we evaluate the performance according to
MinMaxLoss while keepingâˆ¥ğ´âˆ’ğ´ğ‘˜âˆ¥ğ¹andâˆ¥ğµâˆ’ğµğ‘˜âˆ¥ğ¹fixed. We
perform this for six largest datasets for ğ‘˜=20(for meps,ğ‘˜=50).
Figure 1 shows the results. Note that Low QR andHigh QR can only
sample at most min(ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ´),ğ‘Ÿğ‘ğ‘›ğ‘˜(ğµ))columns. Low QR shows
relatively good performance compared to Greedy, which has the
best performance. In meps, Greedy andHigh QR, due to their higher
complexity did not terminate within 24 hours. Note that in meps,
FairScoresSampler performs worse than Random forğ‘<300,
which implies the rank 50leverage scores do not decay quickly for
 
2194Fair Column Subset Selection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
30 40 50 60 70 80 90 100 110
c0.00.20.40.60.81.0MinMaxLoss
adult
Low QR
High QR
Greedy
FairScoresSampler
Random
24 28 32 36 40 44 48 52 56
c0.000.250.500.751.001.251.501.752.00
student
Low QR
High QR
Greedy
FairScoresSampler
Random
30 40 50 60 70 80 90 100
c0.000.250.500.751.001.251.501.75
communities
Low QR
High QR
Greedy
FairScoresSampler
Random
20 40 60 80 100 120 140 160 180
c0.00.20.40.60.81.01.21.4MinMaxLoss
compas
Low QR
High QR
Greedy
FairScoresSampler
Random
25 50 75 100 125 150 175 200 225
c0.00.20.40.60.81.01.2
recidivism
Low QR
High QR
Greedy
FairScoresSampler
Random
0 150 300 450 600 750 900 1050 1200
c0.00.51.01.52.0
meps
Low QR
FairScoresSampler
Random
Figure 1: MinMaxLoss for different values of ğ‘and fixed target rank ğ‘˜
ğ‘<300, and thus are not very informative. In Supplementary B,
we plot the rank- ğ‘˜leverage scores for datasets for different values
ofğ‘˜.
Two-stage sampling. Recall that, FairCSS-MinMax asks for
exactlyğ‘˜columns. To combine the efficiency of FairScoresSam-
pler and effectiveness of algorithms such as Greedy, we introduce
and evaluate a two-stage sampling approach that returns exactly
ğ‘˜-columns. Similar ideas have been explored in CSSliterature [ 5].
In the first stage, we run FairScoresSampler that takes as input a
thresholdğœƒand the rank- ğ‘˜leverage scores of ğ´andğµ, and returns
ğ‘â‰¥ğ‘˜columns. In the second stage, we run Low QR, High QR
orGreedy on the subset of columns returned in the first stage,
to obtain a column subset of size ğ‘˜. We refer to these methods as
S-Low QR, S-High QR andS-Greedy.
Table 2 reports the results for the seven largest datasets for
various values of ğ‘˜. We setğœƒ=ğ‘˜âˆ’1
2in all cases, except meps,
whereğœƒ=3ğ‘˜
4to reduce the number of sampled columns. Thus
in all datasets (except meps) the resulting number of columns in
the first stage, is aâˆš
2-approximation to the optimal number of
columnsğ‘. Column â€œc" in Table 2 indicates the number of columns
returned in the sampling stage. Note that in some cases the number
of columns is required to satisfy ğœƒis significantly large. For each
experiment, the best performing algorithm is highlighted in bold.
We observe that Greedy performs better in most cases, but does
not terminate always within twenty-four hours. S-Greedy is faster
than Greedy, due to the sampling stage, and the objective values
are close. On the other hand, S-High QR does not perform well
in practice, though in theory we expect it to perform better for ğ‘˜
closer toğ‘›. Finally, we demonstrate that (see meps) small ğ‘˜does
not mean fewer columns are sampled in the first stage, because
the lower-rank leverage scores decay faster; thus more columns
are required to satisfy threshold ğœƒ. A visualization of the decay of
leverage scores is reported in the Supplementary B.6.2 Evaluation of the fair CSSobjective
We examine the imbalance in reconstruction errors of the â€œvanilla"
CSS objective of Problem 1 and FairCSS-MinMax objective for
groupsğ´andğµ. We compute the optimal solution for each objec-
tive for various ğ‘˜through exhaustive enumeration. opt(ğ‘€),opt(ğ´)
andopt(ğµ)denote the reconstruction error of vanilla CSSfor matri-
cesğ‘€,ğ´ andğµ, respectively. minmax(ğ‘€)=minmax(ğ´,ğµ)denote
the reconstruction error of matrix ğ‘€. Further, fair(ğ´)andfair(ğµ)
denote the reconstruction errors of FairCSS-MinMax for groupsğ´
andğµ, respectively. Lastly, fair(ğ‘€)denote the reconstruction error
ofğ‘€corresponding to the optimal solution of FairCSS-MinMax.
We note that brute-force enumeration is expensive, even after par-
allelization and extensive optimization, so we only report results
forheart, student and german datasets.
Fairness of solutions. In Figure 2 we compare opt(ğ´),opt(ğµ),
that is, the optimal solution of vanilla CSS, and the optimal solution
ofFairCSS-MinMax fair(ğ´),fair(ğµ). In most instances, we observe
that the reconstruction errors of groups are disproportionate in
vanilla CSS. However, the degree of imbalance is not uniform across
datasets. One source of imbalance could be vastly different group
sizes. However, as observed in student,|ğ´|>|ğµ|, but opt(ğ´)<
opt(ğµ). This further supports the need for sophisticated approaches
to fairness in CSS, beyond mere normalization.
Price of fairness. Next, we verify how the fairness objective
influences the quality of the solution in terms of reconstruction
error. In Figure 3, we report the optimal solution of vanilla CSSand
FairCSS-MinMax to assess the trade-off between fairness and re-
construction error. This analysis quantifies the extent to which we
sacrifice reconstruction error to achieve our fairness objective. In
most cases, we observe no significant difference in reconstruction
error between vanilla CSSandFairCSS-MinMax, that is, opt(ğ‘€)
andfair(ğ‘€), even though the value of minmax(ğ‘€)is significantly
higher than opt(ğ‘€). Note that, the observations cannot be gener-
alized across all datasets, and we cannot conclusively claim that
 
2195KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi
Table 2: Performance comparison of algorithms.
Dataset c k MinMax Loss
S-Low QR S-High QR S-Greedy Low QR Greedy Random
communities89 10 1.27323 1.50763 1.17121 1.25939 1.16976 1.47485
94 23 1.32462 1.54204 1.2717 1.41701 1.23969 1.65518
98 51 1.40469 1.57761 1.42658 1.40934 1.39823 2.95985
compas118 10 1.04747 1.2738 1.03356 1.05041 1.03057 1.19477
165 19 1.08617 1.34321 1.05688 1.08617 1.0537 1.27599
177 37 1.37174 1.41811 1.14835 1.47138 1.1291 1.67851
adult70 10 1.02345 1.09485 1.02111 1.02345 1.01768 1.05641
96 22 1.03347 1.12764 1.0374 1.03347 - 1.0589
103 49 1.07796 1.19301 1.40252 1.08317 - 1.0994
german53 10 1.08088 1.30176 1.08488 1.07711 1.07349 1.14205
54 15 1.1439 1.34599 1.11798 1.11871 1.11088 1.1966
54 24 1.20605 1.38489 1.192 1.20246 1.18624 1.36138
recidivism134 10 1.02485 1.17864 1.02313 1.02236 1.01483 1.12757
174 24 1.05569 1.29805 1.04054 1.05567 1.03202 1.22332
212 57 1.31688 1.52031 1.16871 1.27495 1.13311 1.59933
student45 10 1.10833 1.42094 1.10559 1.11333 1.10597 1.17856
46 14 1.14467 1.39265 1.14375 1.15592 1.14361 1.26605
47 21 1.18932 1.5756 1.17832 1.209 1.18771 1.56265
meps428 10 1.14642 1.83209 - 1.05601 - 1.17759
382 32 1.20093 1.85843 1.777 1.11665 - 1.26749
338 100 1.33916 1.70488 2.48787 - - 1.47403
3 4 5 6 7 8 9 10
k1.11.21.31.4Reconstruction errorheart
opt(A)
opt(B)
fair(A)
fair(B)
3 4 5 6
k1.01.1Reconstruction errorstudent
opt(A)
opt(B)
fair(A)
fair(B)
3 4 5 6
k1.01.1Reconstruction errorgerman-credit
opt(A)
opt(B)
fair(A)
fair(B)
Figure 2: Comparison of reconstruction error of CSSand FairCSS for groups ğ´andğµ.
there is no trade-off between fairness and reconstruction error. For
instance, in the case of the heart dataset, we observe a significant
difference between values of opt(ğ‘€)andfair(ğ‘€)forğ‘˜=5.
7 CONCLUSION
We introduced a novel CSSvariant for a fair setting when the ma-
trix rows are partitioned into two groups. Our goal is to minimize
the reconstruction error for both groups via a min-max objective.
We utilized leverage scores to present an approximation algorithm
when the column count is relaxed, and presented rank revealing
QR-factorisation-based algorithms when the column count is fixed.Extensive experiments on real-world data validated the effective-
ness of our approach in improving fairness. As future work, a
natural direction is to extend our results to more than two groups.
Also, improving the approximation ratio of MinFairnessScores
requires further investigation.
ACKNOWLEDGEMENTS
Suhas Thejaswi acknowledges support from the European Research
Council (ERC) under the European Unionâ€™s Horizon 2020 research
and innovation program (grant agreement No. 945719) and the
European Unionsâ€™s SoBigData++ Transnational Access Scholarship.
 
2196Fair Column Subset Selection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
3 4 5 6 7 8 9 10
k1.11.21.31.4Reconstruction errorheart
opt(M)
fair(M)
minmax(A,B)
3 4 5 6
k1.01.1Reconstruction errorstudent
opt(M)
fair(M)
minmax(A,B)
3 4 5 6
k1.01.1Reconstruction errorgerman-credit
opt(M)
fair(M)
minmax(A,B)
Figure 3: Price of fairness.
Antonis Matakos acknowledges support from the Academy of
Finland through the grant "Model Management Systems: Machine
learning meets Database Systems"- MLDB (32511).REFERENCES
[1]Jason Altschuler, Aditya Bhaskara, Gang Fu, Vahab Mirrokni, Afshin Ros-
tamizadeh, and Morteza Zadimoghaddam. 2016. Greedy column subset selection:
New bounds and distributed algorithms. In ICML. PMLR, 2539â€“2548.
[2]Aris Anagnostopoulos, Luca Becchetti, Adriano Fazzone, Cristina Menghini, and
Chris Schwiegelshohn. 2020. Spectral relaxations and fair densest subgraphs. In
CIKM. ACM, 35â€“44.
[3]Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2019. Fairness and Machine
Learning: Limitations and Opportunities. fairmlbook.org. http://www.fairmlbook.
org.
[4]Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. 2014. Near-optimal
column-based matrix reconstruction. SIAM J. Comput. 43, 2 (2014), 687â€“717.
[5]Christos Boutsidis, Michael W Mahoney, and Petros Drineas. 2008. Unsupervised
feature selection for principal components analysis. In KDD. 61â€“69.
[6]Christos Boutsidis, Michael W. Mahoney, and Petros Drineas. 2009. An Improved
Approximation Algorithm for the Column Subset Selection Problem. In SODA
(SODA â€™09). SIAM, 968â€“977.
[7]Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy
disparities in commercial gender classification. In FAccT. PMLR, 77â€“91.
[8]Tony F. Chan and Per Christian Hansen. 1994. Low-rank revealing QR factoriza-
tions. Numerical Linear Algebra with Applic. 1, 1 (1994), 33â€“44.
[9]Tony F. Chan. 1987. Rank revealing QR factorizations. Linear Algebra and its
Applic. 88-89 (1987), 67â€“82.
[10] Anshuman Chhabra, Karina Masalkovait Ë™e, and Prasant Mohapatra. 2021. An
overview of fairness in clustering. IEEE Access 9 (2021), 130698â€“130720.
[11] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. 2017.
Fair Clustering Through Fairlets. In NeuRIPS. Curran Associates, Inc., 5029â€“5037.
[12] Joel W Cohen, Steven B Cohen, and Jessica S Banthin. 2009. The medical expen-
diture panel survey: a national information resource to support healthcare cost
research and inform policy and practice. Medical care (2009), S44â€“S50.
[13] Amit Deshpande and Luis Rademacher. 2010. Efficient volume sampling for
row/column subset selection. In FCS. IEEE, 329â€“338.
[14] Amit Deshpande, Luis Rademacher, Santosh S Vempala, and Grant Wang. 2006.
Matrix approximation and projective clustering via volume sampling. Theory of
Computing 2, 1 (2006), 225â€“247.
[15] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http:
//archive.ics.uci.edu/ml
[16] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Innovations in TCS. ACM, 214â€“226.
[17] Ahmed K. Farahat, Ahmed Elgohary, Ali Ghodsi, and Mohamed S. Kamel. 2015.
Greedy Column Subset Selection for Large-Scale Data Sets. Knowl. Inf. Syst. 45, 1
(oct 2015), 1â€“34. https://doi.org/10.1007/s10115-014-0801-8
[18] Vincent Froese, Leon Kellerhals, and Rolf Niedermeier. 2022. Modification-fair
cluster editing. In AAAI, Vol. 36. 6631â€“6638.
[19] M. R. Garey and D. S. Johnson. 1979. Computers and Intractability. W. H. Freeman.
[20] Mehrdad Ghadiri, Samira Samadi, and Santosh Vempala. 2021. Socially fair
k-means clustering. In FAccT. ACM, 438â€“448.
[21] G. Golub. 1965. Numerical Methods for Solving Linear Least Squares Problems.
Numer. Math. 7, 3 (jun 1965), 206â€“216. https://doi.org/10.1007/BF01436075
[22] Y Hong and C. T. Pan. 1992. Rank-Revealing QR Factorizations and the Singular
Value Decomposition. Math. Comp. 58 (1992), 213â€“232.
[23] Faisal Kamiran and Toon Calders. 2010. Classification with no discrimination by
preferential sampling. In Machine Learning Conf., Vol. 1. Citeseer.
[24] Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for
classification without discrimination. KIS33, 1 (2012), 1â€“33.
 
2197KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi
[25] Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Ashesh Rambachan. 2018.
Algorithmic fairness. In AEA papers and proceedings, Vol. 108. 22â€“27.
[26] Daniel D. Lee and H. Sebastian Seung. 1999. Learning the parts of objects by
nonnegative matrix factorization. Nature 401 (1999), 788â€“791.
[27] Zachary Lipton, Julian McAuley, and Alexandra Chouldechova. 2018. Does
mitigating ML's impact disparity require treatment disparity?. In Advances in
Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran As-
sociates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/
8e0384779e58ce2af40eb365b318cc32-Paper.pdf
[28] Antonis Matakos, Suhas Thejaswi, and Bruno Ordozgoiti. 2023. Fair column
subset selection â€“ source code v-1.1. https://github.com/matakos18/FairCSS/.
[29] Shira Mitchell, Eric Potash, Solon Barocas, Alexander Dâ€™Amour, and Kristian
Lum. 2021. Algorithmic fairness: Choices, assumptions, and definitions. Annual
Review of Statistics and Its Application 8 (2021), 141â€“163.
[30] Matt Olfat and Anil Aswani. 2019. Convex Formulations for Fair Principal
Component Analysis. In AAAI. AAAI Press, 663â€“670.
[31] Dimitris S. Papailiopoulos, Anastasios Kyrillidis, and Christos Boutsidis. 2014.
Provable deterministic leverage score sampling. In KDD. ACM, 997â€“1006.
[32] Karl Pearson. 1901. LIII. On lines and planes of closest fit to systems of points in
space. The London, Edinburgh, and Dublin philosophical magazine and journal of
science 2, 11 (1901), 559â€“572.
[33] Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. 2008. Discrimination-
aware data mining. In KDD. ACM, 560â€“568.
[34] Dana Pessach and Erez Shmueli. 2022. A review on fairness in machine learning.
Comput. Surveys 55, 3 (2022), 1â€“44.
[35] Inioluwa Deborah Raji and Joy Buolamwini. 2022. Actionable Auditing Revisited:
Investigating the Impact of Publicly Naming Biased Performance Results of
Commercial AI Products. Commun. ACM 66, 1 (2022), 101â€“108.
[36] Ashesh Rambachan, Jon Kleinberg, Jens Ludwig, and Sendhil Mullainathan. 2020.
An economic perspective on algorithmic fairness. In AEA Papers and Proceedings,
Vol. 110. 91â€“95.
[37] Henrik Skaug SÃ¦tra, Mark Coeckelbergh, and John Danaher. 2022. The AI
Ethicistâ€™s Dirty Hands Problem. Commun. ACM 66, 1 (2022), 39â€“41.
[38] Samira Samadi, Uthaipon Tantipongpipat, Jamie Morgenstern, Mohit Singh, and
Santosh Vempala. 2018. The Price of Fair PCA: One Extra Dimension. In NeuRIPS
(NIPSâ€™18). Curran Associates Inc., 10999â€“11010.
[39] Yaroslav Shitov. 2021. Column subset selection is NP-complete. Linear Algebra
Appl. 610 (2021), 52â€“58.
[40] Uthaipon (Tao) Tantipongpipat, Samira Samadi, Mohit Singh, Jamie Morgenstern,
and Santosh Vempala. 2019. Multi-Criteria Dimensionality Reduction with Appli-
cations to Fairness. In NIPS. Curran Associates Inc., Red Hook, NY, USA, Article
1358, 11 pages.
[41] SongÃ¼l Tolan, Marius Miron, Emilia GÃ³mez, and Carlos Castillo. 2019. Why
machine learning may lead to unfairness: Evidence from risk assessment for
juvenile justice in catalonia. In International Conference on Artificial Intelligence
and Law. 83â€“92.
[42] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P
Gummadi. 2017. Fairness constraints: Mechanisms for fair classification. In
Artificial intelligence and statistics. PMLR, 962â€“970.
[43] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Mega-
hed, and Ricardo Baeza-Yates. 2017. FAIR: A fair top- ğ‘˜ranking algorithm. In
Proceedings of Conference on Information and Knowledge Management. 1569â€“1578.
[44] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013.
Learning fair representations. In ICML. PMLR, 325â€“333.A PSEUDOCODE OF FAIR H-RRQR
High rank revealing QR-factorisation (H-RRQR) is similar to low
rank revealing QR-factorisation (L-RRQR), for details of L-RRQR
see Section 5 of the main paper. In H-RRQR, we begin with ğ‘…11=
ğ‘…,ğ‘…22=0and recursively build ğ‘…22by moving columns to the back.
The fair variant of H-RRQR, at step- ğ‘–, chooses the right singular
vectorğ‘£corresponding to min{ğœğ‘–(ğ‘…ğ´
11(ğ‘–)),ğœğ‘–(ğ‘…ğµ
11(ğ‘–))}, whereğœğ‘–is
the bottom singular value. Then, we construct a permutation Î ğ‘–+1
such that|(Î ğ‘‡
ğ‘–+1ğ‘£)|ğ‘–=âˆ¥ğ‘£âˆ¥âˆ.
Algorithm 3: Fair H-RRQR
1Input: QR factorizations ğ´Î ğ´=ğ‘„ğ´ğ‘…ğ´,ğµÎ ğµ=ğ‘„ğµğ‘…ğµ,ğ‘˜
2Output: permutation Î ğ‘˜
1:fori=n,. . . ,n-k+1 do
2:ğ‘…ğ´
11â†ğ‘…ğ´[:ğ‘–,:ğ‘–],ğ‘…ğµ
11â†ğ‘…ğµ[:ğ‘–,:ğ‘–]
3:ğ‘£â†min{ğœğ‘–(ğ‘…ğ´
11(ğ‘–)),ğœğ‘–(ğ‘…ğµ
11(ğ‘–))}
4: Compute permutation ğ‘ƒsuch that|(ğ‘ƒğ‘‡ğ‘£)ğ‘–|=âˆ¥ğ‘ƒğ‘‡ğ‘£âˆ¥âˆ
5: Compute QR fact. ğ‘…ğ´
11ğ‘ƒ=ğ‘„ğ´
1Ëœğ‘…ğ´
11andğ‘…ğµ
11ğ‘ƒ=ğ‘„ğµ
1Ëœğ‘…ğµ
11
6:Î â†Î ğ‘ƒ0
0ğ¼
7:ğ‘…ğ´â† 
Ëœğ‘…ğ´
11ğ‘„ğ´
1ğ‘‡ğ‘…ğ´
12
0ğ‘…ğ´
22!
andğ‘…ğµâ† 
Ëœğ‘…ğµ
11ğ‘„ğµ
1ğ‘‡ğ‘…ğµ
12
0ğ‘…ğµ
22!
8:end for
returnğ‘†
B DECAY OF LEVERAGE SCORES
We plot the leverage scores of groups ğ´ and ğµ for the experiments 
in Table 2. The leverage scores are sorted separately for the two 
groups and plotted in decreasing order of their value.
 
2198Fair Column Subset Selection KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 20 40 60 80 100
ranking0.00.20.4Lsadult k=10
ls group A
ls group B
0 20 40 60 80 100
ranking0.00.20.4Lsadult k=22
ls group A
ls group B
0 20 40 60 80 100
ranking0.000.250.500.75Lsadult k=49
ls group A
ls group B
0 20 40 60 80 100
ranking0.000.250.500.75Lscommunities k=10
ls group A
ls group B
0 20 40 60 80 100
ranking0.00.51.0Lscommunities k=23
ls group A
ls group B
0 20 40 60 80 100
ranking0.51.0Lscommunities k=51
ls group A
ls group B
0 50 100 150
ranking0.00.20.4Lscompas k=10
ls group A
ls group B
0 50 100 150
ranking0.00.20.4Lscompas k=19
ls group A
ls group B
0 50 100 150
ranking0.000.250.500.75Lscompas k=37
ls group A
ls group B
0 20 40 60
ranking0.00.20.4Lsgerman k=10
ls group A
ls group B
0 20 40 60
ranking0.00.20.4Lsgerman k=15
ls group A
ls group B
0 20 40 60
ranking0.000.250.500.75Lsgerman k=24
ls group A
ls group B
0 250 500 750 1000 1250
ranking0.000.020.040.06Lsmeps k=10
ls group A
ls group B
0 250 500 750 1000 1250
ranking0.00.5Lsmeps k=100
ls group A
ls group B
0 250 500 750 1000 1250
ranking0.00.10.2Lsmeps k=32
ls group A
ls group B
0 50 100 150 200
ranking0.00.10.2Lsrecidivism k=10
ls group A
ls group B
0 50 100 150 200
ranking0.00.20.4Lsrecidivism k=24
ls group A
ls group B
0 50 100 150 200
ranking0.000.250.500.75Lsrecidivism k=57
ls group A
ls group B
0 20 40
ranking0.20.4Lsstudent k=10
ls group A
ls group B
0 20 40
ranking0.20.40.6Lsstudent k=14
ls group A
ls group B
0 20 40
ranking0.000.250.500.75Lsstudent k=21
ls group A
ls group B
Figure 4: Leverage scores of ğ´andğµfor Table 2
 
2199