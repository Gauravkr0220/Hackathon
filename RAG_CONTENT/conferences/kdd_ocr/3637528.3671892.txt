Conversational Dueling Bandits in Generalized Linear Models
Shuhua Yang
University of Science and Technology
of China
Hefei, China
shuashua0608@mail.ustc.edu.comHui Yuan
Princeton University
Princeton, NJ, USA
huiyuan@princeton.eduXiaoying Zhang
ByteDance
Beijing, China
zhangxiaoying.xy@bytedance.com
Mengdi Wang
Princeton University
Princeton, NJ, USA
mengdiw@princeton.eduHong Zhang
University of Science and Technology
of China
Hefei, China
zhangh@ustc.edu.cnHuazheng Wang
Oregon State University
Corvallis, OR, USA
huazheng.wang@oregonstate.edu
Abstract
Conversational recommendation systems elicit user preferences by
interacting with users to obtain their feedback on recommended
commodities. Such systems utilize a multi-armed bandit framework
to learn user preferences in an online manner and have received
great success in recent years. However, existing conversational ban-
dit methods have several limitations. First, they only enable users
to provide explicit binary feedback on the recommended items or
categories, leading to ambiguity in interpretation. In practice, users
are usually faced with more than one choice. Relative feedback,
known for its informativeness, has gained increasing popularity
in recommendation system design. Moreover, current contextual
bandit methods mainly work under linear reward assumptions, ig-
noring practical non-linear reward structures in generalized linear
models. Therefore, in this paper, we introduce relative feedback-
based conversations into conversational recommendation systems
through the integration of dueling bandits in generalized linear
models (GLM) and propose a novel conversational dueling bandit
algorithm called ConDuel. Theoretical analyses of regret upper
bounds and empirical validations on synthetic and real-world data
underscore ConDuelâ€™s efficacy. We also demonstrate the potential to
extend our algorithm to multinomial logit bandits with theoretical
and experimental guarantees, which further proves the applicability
of the proposed framework.
CCS Concepts
â€¢Theory of computation â†’Online algorithms; â€¢Information
systemsâ†’Recommender systems.
Keywords
Conversational Recommendation, Dueling Bandits, Generalized
Linear Model
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671892ACM Reference Format:
Shuhua Yang, Hui Yuan, Xiaoying Zhang, Mengdi Wang, Hong Zhang,
and Huazheng Wang. 2024. Conversational Dueling Bandits in Generalized
Linear Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671892
1 Introduction
Contextual bandit is an essential tool in recommendation systems
to enhance the performance of the system while making a trade-off
between exploitation and exploration [ 1,13]. In recommendation
scenarios, each item is considered as an arm with its contextual
vector summarizing the information of both the arm and the user.
At each round, the recommendation system sequentially suggests
items to the user and collects feedback (e.g., click) on the selected
item. The agentâ€™s goal in the system is to develop an item recom-
mendation (arm selection) strategy that maximizes the cumulative
reward from the user by leveraging information about the user and
items, as well as the userâ€™s previous interaction records.
In many scenarios, it is challenging to effectively utilize user
feedback and recommend optimally for cold-start users due to lim-
ited historical data, with insufficient data to learn usersâ€™ preferences
reliably. To accelerate the learning process of user preferences and
offer optimal recommendations, conversational recommendation
systems (CRSs) have been proposed in [ 7], [28] and [ 24]. In CRSs,
the system not only gathers responses on recommended items
but also sparks conversations by asking users about relevant "key-
terms," such as categories or entities associated with news articles
in news recommendation systems. According to [ 28], these inter-
actions accelerate CRS learning by leveraging key-terms linked to
numerous items, offering valuable insights into user preferences.
Despite previous successes in CRSs, current conversation mech-
anisms, particularly conversational contextual bandit approaches,
often fall short. First and foremost, current conversational contex-
tual bandit approaches concentrate solely on explicit user feedback
for specific items/categories, which can be ambiguous and fail to ef-
fectively capture user preferences. In contrast, relative feedback has
been proven to be informative and is commonly observed in various
settings including recent applications in Reinforcement Learning
with Human Feedback (RLHF) [ 10,17,21,30].To better illustrate
the difference between CRS with different feedback mechanisms,
3806
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shuhua Yang et al.
Figure 1: An illustrative example of a conversational system
with pairwise feedback compared with a system that only
allows explicit feedback.
we give a simple yet illustrative example in Fig 1. For previous
CRSs that only allow explicit feedback, they would inquire about
the userâ€™s preference on a particular commodity category through
queries such as: "Are you interested in digital products?", making
it difficult for the user to respond with a simple "Yes" or "No", es-
pecially when the user is unsure about the specific type of digital
products being asked about. The user may be interested in Ebooks
but dislikes video games, thus providing ambiguous feedback to the
agent. Alternatively, for agents that allow relative feedback and ask
questions such as "Do you prefer digital products or video games?",
the user can provide more decisive feedback, prompting the system
to understand the userâ€™s preference more efficiently.
Building on these observations, our paper proposes to build a
CRS that guarantees relative feedback within the dueling bandit
framework [ 26,27,31]. Inspired by [ 8], which proposed the first
contextualized extension of dueling bandits, we also incorporate
the contextual information into this framework. Furthermore, al-
though most CRSs have adopted the linear bandit framework, the
linear reward assumptions may not align with practical scenarios.
To address this limitation, we relax the linear reward assumption
and design a more practical dueling bandit approach by employing
a generalized linear model (GLM). This approach, as demonstrated,
yields significant improvements over the linear models commonly
utilized in current CRSs [ 12]. Designing a CRS under a dueling
bandits framework in GLM imposes challenges, including (1) deter-
mining key-term pairs to query and (2) selecting arm pairs based on
interactions from both the key-term and arm modules. To address
these, we propose the Conversational Dueling Bandit algorithm
(ConDuel). Our approach involves conversations on "exploratory"
key-term pairs and item pair selection based on the uncertainty
principle. Leveraging feedback from both modules, our ConDuel
algorithm extends CRS to a pairwise dueling bandit model. Throughexperiments on synthetic and real-world datasets, we demonstrate
the competitiveness of our algorithm over baselines. We also demon-
strate the potential to extend the pairwise feedback model into a
multi-choice model, proposing a ConMNL algorithm that can tackle
the multinomial logit bandit problem. In summary, the contribution
of our work is three-fold:
â€¢We propose a new framework for conversation recommender
systems (CRS) that can efficiently utilize relative feedback
upon each query. We specifically design the dueling bandit
algorithm ConDuel to achieve the objective. To the best of
our knowledge, this is the first work that enables informative
pairwise preference-based questions on both key terms and
items in CRS.
â€¢Our ConDuel algorithm applies a generalized linear model
and we provide a sublinear regret upper bound as theoret-
ical support. Besides pairwise comparison, we also extend
ConDuel to multiple comparisons under the choice model
with the proposed ConMNL algorithm.
â€¢Extensive experiments on a synthetic dataset and two real-
world datasets verify the efficiency of the proposed ConDuel
algorithm and ConMNL algorithm.
2 Related work
Our work builds on several research areas, and we review some
recent work in the most related areas.
Conversational Bandits. Contextual bandit algorithms aim to op-
timize the expected cumulative rewards, in the long run [ 1,13].
Traditional linear bandits require extensive exploration to learn
user preferences in recommender systems. [ 7] first proposed multi-
armed bandit models in Conversational recommender systems to
acquire usersâ€™ feedback on each item. Afterward, [ 28] systematically
studied conversational contextual bandit and proposed a ConUCB
algorithm to accelerate online recommendations, which allows the
agent to obtain user feedback on key-terms related to items and
leverage this information to accelerate the system. Building on this
framework, some follow-up works have extended conversational
contextual bandits in various settings, such as using clustering
techniques to create self-generated key-terms [ 23], obtaining rela-
tive feedback from key-terms [ 24], leveraging knowledge graphs
to study the underlying relations between key-terms [ 29], and in-
corporating both information from arm-level and key-term-level
to construct a holistic model [ 22]. Although recent work in [ 24]
has incorporated relative feedback in the key-term module, the
algorithm proposed in their work is a simple empirical extension
of ConUCB in [ 28], in which the system utilizes a pseudo update
over key-terms without theoretical guarantees.
Utility-based Dueling Bandits. In utility-based dueling bandits,
the absolute preference for each arm can be reflected by a real-
valued utility degree [ 4]. When applied to dueling bandits, this
setting is also known as utility-based dueling bandits , where a
latent utility function ğ‘¢:A â†’ Rexists, with ğ‘¢(ğ‘ğ‘–)represent-
ing the utility of an arm ğ‘ğ‘–âˆˆA. The probability of arm ğ‘ğ‘–win-
ning overğ‘ğ‘—can be determined by the difference of their utili-
ties using a link function ğœ‡:Râ†’[0,1], and can be written as
ğ‘ƒ(ğ‘ğ‘–>ğ‘ğ‘—)=ğœ‡(ğ‘¢(ğ‘ğ‘–)âˆ’ğ‘¢(ğ‘ğ‘—)). In contextual bandit, the utility of
3807Conversational Dueling Bandits in Generalized Linear Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
an arm is assumed to be linear based on an unknown preference
vector, which has been studied in [ 5,18,19]. Utility-based dueling
bandits can also be extended to the preference-based reinforcement
learning framework in [25] and [20].
3 Problem Formulation
In this section, we introduce the general framework of conver-
sational dueling bandits with a generalized linear model (GLM).
Suppose there are ğ‘arms denoted byAandğ¾key-terms denoted
byK. At each round ğ‘¡=1,...,ğ‘‡ , the agent is given a subset of
armsAğ‘¡âŠ‚A , where each arm ğ‘âˆˆAğ‘¡is associated with a con-
textual vector ğ‘¥ğ‘,ğ‘¡âˆˆRğ‘‘. Without loss of generality, we assume
that the feature vectors are normalized, i.e., âˆ¥ğ‘¥ğ‘,ğ‘¡âˆ¥2=1. We also
assume that the unknown user preference vector ğœƒâˆ—âˆˆRğ‘‘satisfies
the inequalityâˆ¥ğœƒâˆ—âˆ¥2â‰¤1. The relationship between the arms and
the key-terms can be characterized by a weighted bipartite graph
(A,K,W), whose nodes are divided into two sets AandK, and
weighted edges are represented by the matrix Wâ‰œ[ğ‘¤ğ‘,ğ‘˜]with
ğ‘¤ğ‘,ğ‘˜representing the relationship between arm ğ‘and key-term ğ‘˜.
Without loss of generality, we assumeÃ
ğ‘˜ğ‘¤ğ‘,ğ‘˜=1.
Generalized Linear Dueling Feedback. We assume the latent util-
ity function ğ‘¢:Aâ†’Ris linear and ğ‘¢(ğ‘,ğ‘¡)=ğ‘¥ğ‘‡
ğ‘,ğ‘¡ğœƒâˆ—represents
the utility of the arm ğ‘âˆˆ Ağ‘¡at roundğ‘¡, withğœƒâˆ—âˆˆRğ‘‘being
the unknown preference vector. We also define a link function
ğœ‡:Râ†’[0,1], so that at round ğ‘¡, the probability of arm ğ‘ğ‘–âˆˆAğ‘¡
winning over ğ‘ğ‘—âˆˆAâŠ”satisfies the following equation:
ğ‘ƒ(ğ‘ğ‘–>ğ‘ğ‘—)=ğœ‡(ğ‘¢(ğ‘ğ‘–,ğ‘¡)âˆ’ğ‘¢(ğ‘ğ‘—,ğ‘¡))=ğœ‡(ğ‘¥ğ‘‡
ğ‘–,ğ‘¡ğœƒâˆ—âˆ’ğ‘¥ğ‘‡
ğ‘—,ğ‘¡ğœƒâˆ—).(1)
The link function ğœ‡satisfies the following properties [26]:
â€¢ğœ‡is monotonically increasing, so that an arm with a higher
utility than another arm will have a higher probability to be
chosen than the latter.
â€¢ğœ‡(0)=1/2, indicating that two arms having the same utility
have also the same probability of being chosen.
â€¢ğœ‡(âˆ’âˆ) =0,ğœ‡(âˆ)=1.
It is easy to verify the two most common link functions: logis-
tic function ğœ‡(ğ‘¥)=1/(1+exp(âˆ’ğ‘¥))and linear function ğœ‡(ğ‘¥)=
max{0,min{1,0.5âˆ—(1+ğ‘¥)}}both satisfy the properties. Following
[9], we assume that ğœ‡satisfies the following assumptions.
Assumption 1. ğœ…1=inf{âˆ¥ğ‘¥âˆ¥2â‰¤2,ğœƒâˆˆÎ˜}ğœ‡â€²(ğ‘¥ğ‘‡ğœƒ)>0, where Î˜is a
closed subset of the space Rğ‘‘containingğœƒâˆ—.
Assumption 2. ğœ‡is twice differentiable, and its first and second-
order derivatives are upper-bounded by constant ğ¿ğœ‡andğ‘€ğœ‡respec-
tively. When ğœ‡is the sigmoid function, ğ¿ğœ‡andğ‘€ğœ‡can be 1/4.
Similar to Eq 1, we define the probability of key-term ğ‘˜ğ‘–beating
ğ‘˜ğ‘—at roundğ‘¡as
ğ‘ƒ(ğ‘˜ğ‘–>ğ‘˜ğ‘—)=ğœ‡(Ëœğ‘¢(ğ‘˜ğ‘–,ğ‘¡)âˆ’Ëœğ‘¢(ğ‘˜ğ‘—,ğ‘¡)),
where Ëœğ‘¢(ğ‘˜,ğ‘¡):=Ã
ğ‘âˆˆAğ‘¤ğ‘,ğ‘˜ğ‘¢(ğ‘,ğ‘¡)Ã
ğ‘âˆˆAğ‘¤ğ‘,ğ‘˜, indicating that the utility of ğ‘˜is
determined by averaging over that of its related arms. The relative
feedback of two key-terms in a duel is also determined by their
utilities. Equivalently, we rewrite the inequality as
ğ‘ƒ(ğ‘˜ğ‘–>ğ‘˜ğ‘—)=ğœ‡(Ëœğ‘¥ğ‘‡
ğ‘˜ğ‘–,ğ‘¡ğœƒâˆ—âˆ’Ëœğ‘¥ğ‘‡
ğ‘˜ğ‘—,ğ‘¡ğœƒâˆ—), (2)with Ëœğ‘¥ğ‘˜,ğ‘¡=Ã
ğ‘âˆˆAğ‘¤ğ‘,ğ‘˜ğ‘¥ğ‘,ğ‘¡Ã
ğ‘âˆˆAğ‘¤ğ‘,ğ‘˜representing the feature vector of key-
termğ‘˜at roundğ‘¡.
At roundğ‘¡, given the candidate arm set Ağ‘¡, we present to the
user a pair of arms (ğ‘ğ‘¡,ğ‘â€²
ğ‘¡)âˆˆAğ‘¡Ã—Ağ‘¡and ask for his/her relative
preference. At round ğ‘¡, userâ€™s preference is encoded by a binary
random variable ğ‘œğ‘¡=1(ğ‘ğ‘¡>ğ‘â€²
ğ‘¡). Denote by ğ‘‘ğ‘¡:=ğ‘¥ğ‘ğ‘¡âˆ’ğ‘¥ğ‘â€²
ğ‘¡,ğ‘œğ‘¡
follows the Bernoulli distribution ğµğ‘’ğ‘Ÿ(ğœ‡(ğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—)), and the arm-level
feedback model can be written as
ğ‘œğ‘¡=ğœ‡(ğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—)+ğœ–ğ‘¡, (3)
whereğœ–ğ‘¡is a zero-mean noise defined as
ğœ–ğ‘¡=(
1âˆ’ğœ‡(ğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—), with probability ğœ‡(ğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—),
âˆ’ğœ‡(ğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—),with probability 1âˆ’ğœ‡(ğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—).
It is easy to verify that ğœ–ğ‘¡â€™s areğ‘…-sub-Gaussian with ğ‘…â‰¤1/2.
Conversation on Key-Terms and Frequency. CRS obtains addi-
tional user feedback through additional key-term conversations.
Similarly, the key-term level feedback on comparing (ğ‘˜ğ‘¡,ğ‘˜â€²
ğ‘¡)satis-
fiesËœğ‘œğ‘¡âˆ¼ğµğ‘’ğ‘Ÿ(Ëœğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—), where Ëœğ‘‘ğ‘¡=Ëœğ‘¥ğ‘˜ğ‘¡,ğ‘¡âˆ’Ëœğ‘¥ğ‘˜â€²
ğ‘¡,ğ‘¡. The key-term level
model is presented as
Ëœğ‘œğ‘¡=ğœ‡(Ëœğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—)+Ëœğœ–ğ‘¡. (4)
ğ‘(ğ‘¡)is introduced to model the frequency of conversations, and we
consider the following function:
ğ‘(ğ‘¡)=1, ğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1)>0,
0,otherwise.
The agent conducts âŒŠğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1)âŒ‹conversations with the user
at roundğ‘¡whenğ‘(ğ‘¡)=1and refrains conversations when ğ‘(ğ‘¡)=0.
We also assume the key-term-level conversations are less frequent
than arm-level interactions, ensuring ğ‘(ğ‘¡)â‰¤ğ‘¡for anyğ‘¡to prioritize
usersâ€™ experience.
Cumulative Regret. At roundğ‘¡, denote the best arm as ğ‘âˆ—
ğ‘¡, with
ğ‘âˆ—
ğ‘¡=arg maxğ‘âˆˆAğ‘¡ğ‘¢(ğ‘,ğ‘¡)=arg maxğ‘âˆˆAğ‘¡ğ‘¥ğ‘‡
ğ‘,ğ‘¡ğœƒâˆ—, and the the cho-
sen arms pair as(ğ‘ğ‘¡,ğ‘â€²
ğ‘¡). Following [ 5], the instantaneous dueling
bandit regret is defined as
ğ‘Ÿğ‘¡=ğ‘¢(ğ‘âˆ—
ğ‘¡,ğ‘¡)âˆ’1
2(ğ‘¢(ğ‘ğ‘¡,ğ‘¡)+ğ‘¢(ğ‘â€²
ğ‘¡,ğ‘¡))
=ğ‘¥ğ‘‡
ğ‘âˆ—
ğ‘¡,ğ‘¡ğœƒâˆ—âˆ’1
2(ğ‘¥ğ‘‡
ğ‘ğ‘¡ğœƒâˆ—+ğ‘¥ğ‘‡
ğ‘â€²
ğ‘¡ğœƒâˆ—)
The cumulative dueling bandit regret is defined as
ğ‘…(ğ‘‡)=ğ‘‡âˆ‘ï¸
ğ‘¡=1
ğ‘¢(ğ‘âˆ—
ğ‘¡,ğ‘¡)âˆ’1
2(ğ‘¢(ğ‘ğ‘¡,ğ‘¡)+ğ‘¢(ğ‘â€²
ğ‘¡,ğ‘¡))
=ğ‘‡âˆ‘ï¸
ğ‘¡=1
ğ‘¥ğ‘‡
ğ‘âˆ—
ğ‘¡,ğ‘¡ğœƒâˆ—âˆ’1
2(ğ‘¥ğ‘‡
ğ‘ğ‘¡ğœƒâˆ—+ğ‘¥ğ‘‡
ğ‘â€²
ğ‘¡ğœƒâˆ—)
.(5)
4 Conversational Dueling Bandits with GLM
and Regret Analysis
We propose the Conversational Dueling Bandits algorithm (Con-
Duel) (Alg. 1) to address the three challenges in the conversational
dueling bandit setting: 1) how to exploit the historical feedback
from both arms and key-terms. 2) how to select key-term pairs for
conversation to explore better; 3) how to select arm pairs to mostly
3808KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shuhua Yang et al.
minimize the cumulative regret. In Â§ 4.1, we introduce the full algo-
rithm and discuss in detail the highlights of ConDuel addressing
the three challenges above. Meanwhile, theoretical analysis for
ConDuel is provided and a regret upper bounds is showcased in
section Â§ 4.2.
4.1 ConDuel Algorithm
We now detail the proposed ConDuel algorithm as follows:
â€¢Key-term selection module. If the agent conducts a con-
versation with the user based on the previous interactions,
it will select a pair of â€™explorativeâ€™ key-terms (which will be
discussed in detail later) to query. Then the collected feed-
back from the key-term level will be passed to the full model.
We further assume that ğ‘(ğ‘¡)=ğ‘Â·ğ‘¡in the following analysis
for simplification, where ğ‘âˆˆ(0,1).
â€¢Arm-selection module. Based on the previous interaction
history, the parameter ğœƒğ‘¡is calculated and maintains an
optimistic estimate on the dueling feedback ğ‘œğ‘¡based on the
UCB principle. The agent subsequently constructs a subset
ğ¶ğ‘¡, containing promising arms that are likely to be optimal;
Then the system selects a pair of arms from ğ¶ğ‘¡that are most
uncertain to explore the usersâ€™ preference thoroughly and
updates the parameters based on the userâ€™s choice.
Based on the arm-level feedback and conversational feedback re-
ceived in the previous rounds, the two modules interact with the
users. The agent utilizes feedback from both modules for recom-
mendations. The main difference between our proposed algorithm
and ConUCB lies in the usage of relative feedback from both the
key-term level and arm level, guaranteed by the introduction of a
maximum likelihood estimator (MLE). To the best of our knowledge,
this is the first non-trivial extension of ConUCB in the generalized
linear model with dueling key-terms module. We present the Con-
Duel algorithm in Algorithm 1. The main body of ConDuel con-
tains a key-term selection module (lines 3-11) and an arm selection
module (lines 14-16). Note that at each iteration ğ‘¡, we try to main-
tain a tight estimate ğœƒğ‘¡of the true parameter ğœƒâˆ—utilizing feedback
from both key-term level interactions and arm-level interactions in
our generalized linear model.
4.1.1 Parameter Estimation. Previous CRSs estimate ğœƒâˆ—separately
from arm-level and key-term-level [ 23,24,28], which may cause
waste of observations. In our model, we fully utilize information
from both arm-level feedback and key-term-level feedback to obtain
the MLE ofğœƒâˆ—by solving one optimization problem that maximizes
the log-likelihood function. Based on our model (3)and(4), the
regularized MLE of ğœƒâˆ—in our model is given by
ğœƒğ‘¡âˆˆarg max
ğœƒâˆˆÎ˜{ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1(ğ‘œğ‘ ğ‘‘ğ‘‡
ğ‘ ğœƒâˆ’ğ‘š(ğ‘‘ğ‘‡
ğ‘ ğœƒ))
+ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
Ëœğ‘‘ğ‘ âˆˆËœDğ‘ (Ëœğ‘œğ‘ Ëœğ‘‘ğ‘‡
ğ‘ ğœƒâˆ’ğ‘š(Ëœğ‘‘ğ‘‡
ğ‘ ğœƒ))âˆ’ğœ†
2âˆ¥ğœƒâˆ¥2
2}.(6)
We denoteğ‘‘ğ‘¡=ğ‘¥ğ‘ğ‘¡,ğ‘¡âˆ’ğ‘¥ğ‘â€²
ğ‘¡,ğ‘¡as the difference contextual vector for
the chosen arm pair (ğ‘ğ‘¡,ğ‘â€²
ğ‘¡), and Ëœğ‘‘ğ‘¡=Ëœğ‘¥ğ‘˜ğ‘¡,ğ‘¡âˆ’Ëœğ‘¥ğ‘˜â€²
ğ‘¡,ğ‘¡as the difference
contextual vector for the key-term pair (ğ‘˜ğ‘¡,ğ‘˜â€²
ğ‘¡)being queried at
roundğ‘¡. We also define the Dğ‘¡={ğ‘‘ğ‘¡|ğ‘‘ğ‘¡=ğ‘¥ğ‘ğ‘¡,ğ‘¡âˆ’ğ‘¥ğ‘â€²
ğ‘¡,ğ‘¡,âˆ€ğ‘ğ‘¡,ğ‘â€²
ğ‘¡âˆˆAlgorithm 1: The ConDuel Algorithm
Input:(A,K,W),ğ‘(ğ‘¡),ğœ†,ğœ… 1;
1Initialization: ğ‘€0=ğœ†
ğœ…1ğ¼;
2forğ‘¡= 1,...,T do
3 ifğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1)>0then
4ğ‘ğ‘¡=ğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1);
5 whileğ‘ğ‘¡>0do
6 select a pair of key-terms (ğ‘˜ğ‘¡,ğ‘˜â€²
ğ‘¡)independently
from barycentric spanner B;
7 Receive relative feedback Ëœğ‘œğ‘¡=1(ğ‘˜ğ‘¡>ğ‘˜â€²
ğ‘¡),
Ëœğ‘‘ğ‘¡=Ëœğ‘¥ğ‘˜ğ‘¡âˆ’Ëœğ‘¥ğ‘˜â€²
ğ‘¡;
8 Updateğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1+Ëœğ‘‘ğ‘¡Ëœğ‘‘ğ‘¡ğ‘‡;
9 ğ‘ğ‘¡=ğ‘ğ‘¡âˆ’1;
10 end
11 else
12ğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1
13 end
14ğœƒğ‘¡is estimated based on Eq.(7) or Eq. (8), ğœƒ(1)
ğ‘¡is
computed according to Eq. (10);
15 Construct
ğ¶ğ‘¡={ğ‘âˆˆAğ‘¡|(ğ‘¥ğ‘,ğ‘¡âˆ’ğ‘¥ğ‘â€²,ğ‘¡)ğ‘‡ğœƒ(1)
ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ‘¥ğ‘,ğ‘¡âˆ’ğ‘¥ğ‘â€²,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡>
0,âˆ€ğ‘â€²âˆˆAğ‘¡};
16 Select the arm pair ğ‘ğ‘¡andğ‘â€²
ğ‘¡fromCğ‘¡satisfying:
(ğ‘ğ‘¡,ğ‘â€²
ğ‘¡)=arg maxğ‘,ğ‘â€²âˆˆCğ‘¡{âˆ¥ğ‘¥ğ‘,ğ‘¡âˆ’ğ‘¥ğ‘â€²,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡};
17 Receive feedback ğ‘œğ‘¡=1(ğ‘ğ‘¡>ğ‘â€²
ğ‘¡),ğ‘‘ğ‘¡=ğ‘¥ğ‘ğ‘¡,ğ‘¡âˆ’ğ‘¥ğ‘â€²
ğ‘¡,ğ‘¡;
18 Updateğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1+ğ‘‘ğ‘¡ğ‘‘ğ‘‡
ğ‘¡
19end
Ağ‘¡}and ËœDğ‘¡={Ëœğ‘‘ğ‘¡|Ëœğ‘‘ğ‘¡=Ëœğ‘¥ğ‘˜ğ‘¡,ğ‘¡âˆ’Ëœğ‘¥ğ‘˜â€²
ğ‘¡,ğ‘¡,âˆ€ğ‘˜ğ‘¡,ğ‘˜â€²
ğ‘¡âˆˆKğ‘¡}at roundğ‘¡. Since
the log-likelihood function is strictly concave in ğœƒ, the regularized
MLEğœƒğ‘¡in our model is the unique solution of the following score
equation upon differentiating:
ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1(ğ‘œğ‘ âˆ’ğœ‡(ğ‘‘ğ‘‡
ğ‘ ğœƒ))ğ‘‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
Ëœğ‘‘ğ‘ âˆˆËœDğ‘ (Ëœğ‘œğ‘ âˆ’ğœ‡(Ëœğ‘‘ğ‘‡
ğ‘ ğœƒ))Ëœğ‘‘ğ‘ âˆ’ğœ†ğœƒ=0.(7)
According to Eq. (6)and Eq. (7), we can define the invertible
functionğ‘”ğ‘¡(ğœƒ)and the design matrix ğ‘€ğ‘¡as
ğ‘”ğ‘¡(ğœƒ)=ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğœ‡(ğ‘‘ğ‘‡
ğ‘ ğœƒ)ğ‘‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
Ëœğ‘‘ğ‘ âˆˆËœDğ‘ ğœ‡(Ëœğ‘‘ğ‘‡
ğ‘ ğœƒ)Ëœğ‘‘ğ‘ +ğœ†ğœƒ,
ğ‘€ğ‘¡=ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ‘‘ğ‘ ğ‘‘ğ‘‡
ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
Ëœğ‘‘ğ‘ âˆˆËœDğ‘ Ëœğ‘‘ğ‘ Ëœğ‘‘ğ‘‡
ğ‘ +ğœ†/ğœ…1ğ¼. (8)
In case the MLE ğœƒğ‘¡is outside of the parameter space Î˜, we need
to add a projection step to obtain ğœƒ(1)
ğ‘¡by the techniques in [9]:
ğœƒ(1)
ğ‘¡=arg min
ğœƒâˆˆÎ˜âˆ¥ğ‘”ğ‘¡(ğœƒ)âˆ’ğ‘”ğ‘¡(ğœƒğ‘¡)âˆ¥ğ‘€âˆ’1
ğ‘¡. (9)
Note that when ğœƒğ‘¡âˆˆÎ˜, we setğœƒ(1)
ğ‘¡=ğœƒğ‘¡to save the computation.
3809Conversational Dueling Bandits in Generalized Linear Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.1.2 Arm Selection Module. In this part, we will give a detailed
description of arm pair selection. At round ğ‘¡, for anyğ‘ğ‘¡,ğ‘â€²
ğ‘¡âˆˆAğ‘¡,
our algorithm calculates the UCB estimate on the pairwise feedback:
ğ‘ (ğ‘ğ‘¡,ğ‘â€²
ğ‘¡)=(ğ‘¥ğ‘ğ‘¡,ğ‘¡âˆ’ğ‘¥ğ‘â€²
ğ‘¡,ğ‘¡)ğ‘‡ğœƒ(1)
ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ‘¥ğ‘ğ‘¡,ğ‘¡âˆ’ğ‘¥ğ‘â€²
ğ‘¡,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡,
then the agent constructs a subset ğ¶ğ‘¡which contains all the promis-
ing arms that are superior to the rest of the arms in terms of UCB
estimate. The selected pair of arms (ğ‘ğ‘¡,ğ‘â€²
ğ‘¡)satisfies:(ğ‘ğ‘¡,ğ‘â€²
ğ‘¡)=
arg maxğ‘,ğ‘â€²âˆˆCğ‘¡âˆ¥ğ‘¥ğ‘,ğ‘¡âˆ’ğ‘¥ğ‘â€²,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡. In this way, we can eliminate arms
that are unlikely to be optimal in the first step, and then select the
maximum informative arm pair. Notice that this arm selection strat-
egy strictly follows [ 18], and when|Cğ‘¡|is large, this step can bring
a lot of computation. Therefore in our experiment, the first arm
ğ‘ğ‘¡is randomly sampled from ğ¶ğ‘¡, and the second arm ğ‘â€²
ğ‘¡is defined
asğ‘â€²
ğ‘¡=arg maxâˆ¥ğ‘¥ğ‘â€²
ğ‘¡,ğ‘¡âˆ’ğ‘¥ğ‘ğ‘¡,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡, which can be seen as the most
uncertain arm to compare with the first arm.
Lemma 1. Assumeğœ–ğ‘¡andËœğœ–ğ‘¡defined in Eq. (3)and(4)are conditional
ğ‘…-sub-Gaussian, ğ‘‘ğ‘¡is denoted as the difference contextual vectors for
the selected arm pair (ğ‘¥ğ‘¡,ğ‘¥â€²
ğ‘¡). Then for any ğ‘‘ğ‘¡âˆˆDğ‘¡, with probability
at least(1âˆ’ğ›¿), we have the following inequality:
|ğ‘‘ğ‘‡
ğ‘¡ğœƒ(1)
ğ‘¡âˆ’ğ‘‘ğ‘‡
ğ‘¡ğœƒâˆ—|â‰¤ğ›¼ğ‘¡âˆ¥ğ‘‘ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡, (10)
whereğ›¼ğ‘¡=2
ğœ…1(ğ‘…âˆšï¸ƒ
ğ‘‘log((1+4ğœ…1(ğ‘¡+ğ‘(ğ‘¡))
ğ‘‘ğœ†)/ğœ)+âˆšğœ†ğœ…1âˆ¥ğœƒâˆ—âˆ¥2).
4.1.3 Key-term Selection Module. In this section, we describe how
the algorithm selects key-term pairs. We hope that the key-term
selection module is explorative, that is, to ask questions on key-
terms that accelerate the learning of user preferences. Especially,
We propose a new strategy for selecting key-term pairs from the
barycentric spanner Bfrom the key-term set K, which aims at
exploring key-term information efficiently.
Definition of Barycentric Spanner. According to [ 3], the subset
B={ğ‘˜1,...,ğ‘˜ğ‘‘}is a barycentric spanner for key-term set K, if
everyğ‘˜âˆˆK can be expressed as a linear combination of elements
ofBusing coefficients in [âˆ’1,1], i.e., Ëœğ‘¥ğ‘˜=Ãğ‘‘
ğ‘–=1ğ‘ğ‘–Ëœğ‘¥ğ‘˜ğ‘–(ğ‘ğ‘–âˆˆ[âˆ’ 1,1]).
We assumeKğ‘¡spansRğ‘‘at each round, thus the constructed
barycentric spanner Bğ‘¡forms the basis for Rğ‘‘. Inğ‘¡-th round con-
versation, we sample a pair of key-terms ğ‘˜1,ğ‘˜2âˆ¼Bğ‘¡independently
from the barycentric spanner to obtain relative feedback. This is
efficient in computation because reducing the number of key-terms
can bring a lot of convenience. Based on the definition of barycen-
tric spanner, all information of Kğ‘¡can be seen contained in the
barycentric spanner, therefore exploring Bğ‘¡is sufficient in collect-
ing user feedback. Furthermore, this strategy can also guarantee
some good properties in our algorithm, ensuring a high probability
lower bound of ğœ†min(ğ‘€ğ‘¡)as follows:
Lemma 2. LetÎ£=ğ¸ğ‘¥,ğ‘¦âˆ¼B[(ğ‘¥âˆ’ğ‘¦)(ğ‘¥âˆ’ğ‘¦)ğ‘‡], andğœ†ğµ=ğœ†min(Î£).
As the conversation frequency ğ‘(ğ‘¡)â‰¤ğ‘¡, we assume that ğ‘(ğ‘¡)=ğ‘Â·ğ‘¡
for someğ‘âˆˆ(0,1). Then when ğ‘¡â‰¥4(ğ¶1âˆš
ğ‘‘+ğ¶2âˆš
log(1/ğ›¿))2
ğ‘ğœ†2
ğµâ‰œğ‘¡0, with
probability at least (1âˆ’ğ›¿), we have
ğœ†min(ğ‘€ğ‘¡)â‰¥ğœ†ğµğ‘ğ‘¡
2+ğœ†
ğœ…1, (11)
withğ¶1andğ¶2being constants.Lemma 3. With key-term pair independently sampled from barycen-
tric spannerBğ‘¡at each round, and ğ‘¡0is defined in Lemma 2, then
âˆ€ğ‘¡>ğ‘¡0, we have the following inequality:
ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1âˆ¥ğ‘‘ğ‘ âˆ¥ğ‘€âˆ’1ğ‘ â‰¤8(âˆšï¸‚ğ‘¡
2ğ‘ğœ†ğµâˆ’âˆšï¸‚ğ‘¡0
2ğ‘ğœ†ğµ)â‰¤8âˆšï¸‚ğ‘¡
2ğ‘ğœ†ğµ.
Note that the above lemma uses a different technique to prove the
upper bound forÃ
ğ‘¡âˆ¥ğ‘‘ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡, and as the conversation frequency
ğ‘increases, the regret upper bound decreases accordingly. This
tendency corresponds to our understanding of the conversation
system: the more questions the agent asks, the more feedback it can
leverage, thus the more accurately it can learn the user preferences.
Remark. Notice that the key-term selection module is regret-free,
we donâ€™t need to consider explore-exploit trade-offs and apply the
UCB principle here. Instead, utilizing an explorative strategy such
as choosing key-terms from the barycentric spanner can improve
the performance of the algorithm as well as save computation. In
the future, it would be interesting to investigate other explorative
strategies from best arm identification literature, such as works in
[2] and [11].
4.2 Regret Upper Bound
We give the upper bound of the cumulative dueling regret ğ‘…(ğ‘‡)for
our algorithm as follows, where we assume ğ‘(ğ‘¡)=ğ‘Â·ğ‘¡for some
ğ‘âˆˆ(0,1)from Lemma 2.
Theorem 1. With probability (1- ğ›¿), our algorithm has the following
regret upper bound:
ğ‘…(ğ‘‡)â‰¤ğ‘¡0+32
ğœ…1(ğ‘…âˆšï¸‚
ğ‘‘log((1+4ğœ…1(ğ‘‡+ğ‘ğ‘‡)
ğ‘‘ğœ†)/ğœ)+
+âˆšï¸
ğœ†ğœ…1âˆ¥ğœƒâˆ—âˆ¥2)âˆšï¸„
ğ‘‡
2ğ‘ğœ†ğµ=O(1
ğœ…1âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)) (12)
It can be seen that the upper bound of ğ‘…(ğ‘‡)decreases as ğ‘in-
creases. As far as we know, this is the first work in a conversational
recommender system that directly shows the impact of conversa-
tions and proves an explainable regret upper bound concerning
conversation frequency. While thereâ€™s no direct lower bound of the
conversational dueling bandit problem, [ 18] contained the regret
lower bounds of contextual dueling bandits that can also match
with ours, i.e., Î©(âˆš
ğ‘‘ğ‘‡).
5 Experiments
In this section, we describe experimental results on both synthetic
data and real-world data to validate our proposed algorithm. The
code is available at https://github.com/shuashua0608/Con-Duel.
The arm-level and key-term level pairwise rewards are generated
according to Eq. (3)and Eq. (4), and the barycentric spanner Bis
computed in advance following [ 3]. Specifically, we define the link
functionğœ‡as the sigmoid function, thus leading to a logistic dueling
bandit model. It should be noted that our algorithm can also be
applied to other generalized linear model scenarios.
5.1 Implementation Details
Baselines. We select the following algorithms as baselines to com-
pare with ours:
3810KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shuhua Yang et al.
Figure 2: Cumulative regret on synthetic and real-world datasets
â€¢Random-opt: A variant of MaxInp in [ 18], selecting two
arms randomly from the constructed set ğ¶ğ‘¡without conver-
sation from key-term level. This algorithm compares with
MaxInp and shows the necessity of computing the "maxi-
mum informative pair" principle.
â€¢MaxInp [18]: A recently introduced algorithm designed for
the contextual dueling bandits setting in GLM without a
conversation mechanism.
â€¢ConDuel-Random: A variant of our algorithm that follows
the same arm-pair selection principle but selects key-term
pairs randomly.
â€¢ConDuel-MaxInp: A variant of our algorithm that fol-
lows the same arm selection principle but selects key-term
pairs with maximum information, that is, to choose ğ‘˜,ğ‘˜â€²=
arg maxğ‘˜,ğ‘˜â€²âˆˆKâˆ¥Ëœğ‘¥ğ‘˜âˆ’Ëœğ‘¥â€²
ğ‘˜âˆ¥ğ‘€âˆ’1
ğ‘¡atğ‘¡-th conversation.
Additionally, we compare our algorithms with RelativeConUCB
from [ 24], namely, RelativeConUCB-Pos&Neg and RelativeConUCB-
Difference, which utilize relative feedback from key-term selection
module in CRS. It should be noted that RelativeConUCB is designed
for linear bandits and assumes that the arm-level model is a click
model. For a fair comparison, we adapt their arm-level reward esti-
mated fromğ‘Ÿğ‘âˆ¼ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–(ğ‘¥ğ‘‡ğ‘ğœƒâˆ—)toğ‘Ÿğ‘âˆ¼ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–(ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘¥ğ‘‡ğ‘ğœƒâˆ—))
forğ‘âˆˆA and update the regret as ğ‘…ğ‘‡=Ãğ‘‡
ğ‘ =1((ğ‘¥ğ‘‡
ğ‘âˆ—
ğ‘¡ğœƒâˆ—âˆ’(ğ‘¥ğ‘‡ğ‘ğ‘¡ğœƒâˆ—))
to fit in our problem setting. We rewrite them as Rconucb-PosNeg
and Rconucb-Diff, and give a general description of their key-term
selection strategy:
â€¢Rconucb-PosNeg: Utilize relative feedback from the key-
term level as two observations of absolute feedback: a pos-
itive observation of feedback 1 for the preferred key-term
Ëœğ‘¥ğ‘˜1and a negative observation with feedback 0 for the less
preferred key-term Ëœğ‘¥ğ‘˜2;
â€¢Rconucb-Diff : Incorporate the relative feedback as a single
observation(Ëœğ‘¥ğ‘˜1âˆ’Ëœğ‘¥ğ‘˜2,1).
Metrics. We use the cumulative dueling regret from Eq. (5)to mea-
sure the performance of the algorithms, unless otherwise stated.
Additionally, we plot the standard error for each algorithm to vali-
date the stability of our proposed algorithm. We sequentially run
the experiments ten times per user for each dataset and calculate
the average cumulative regret for each algorithm.5.2 Synthetic Data
Data Generation. We construct the synthetic data following [ 28]
and [ 24]. First, we create a user set Uwith|U|= 200, a key-term
setKwith|K|= 500 and an arm set Awith|A|= 5000, with the di-
mension of feature space to be ğ‘‘=10. We generate each element in
user preference vector ğœƒâˆ—ğ‘¢and arm feature vector ğ‘¥ğ‘independently
from the standard normal distribution ğ‘(0,1). Without loss of gen-
erosity, we normalize âˆ¥ğœƒâˆ—ğ‘¢âˆ¥2=1andâˆ¥ğ‘¥ğ‘âˆ¥2=1. To construct the
weight matrix ğ‘Š=[ğ‘¤ğ‘,ğ‘˜], we follow similar procedures in [ 22]: (1)
We select an integer ğ‘›ğ‘˜uniformly at random from [1, ğ‘€], and select
a subset ofğ‘›ğ‘˜armsAğ‘˜to be related with key-term ğ‘˜.ğ‘€is set to
be 10 in the experiments; (2) We assume each arm ğ‘is related with
ğ‘›ğ‘key-terms subsetKğ‘with equal weight ğ‘¤ğ‘,ğ‘˜=1/ğ‘›ğ‘,âˆ€ğ‘˜âˆˆKğ‘.
In the simulation, we set the time horizon ğ‘‡= 5000, conversation
frequencyğ‘(ğ‘¡)=10âŒŠğ‘¡
50âŒ‹and pool size|ğ´ğ‘¡|= 50, unless otherwise
stated.
Experimental results. The cumulative dueling regret curve with
the standard error plot is shown in Figure 2. Our proposed algo-
rithm ConDuel as well as its variants achieves better performances
than MaxInp and Random-opt, realizing smaller regret and stan-
dard error. ConDuel and ConDuel-MaxInp perform slightly better
than ConDuel-Random, indicating that carrying out explorative
conversations can help reduce cumulative regret. Additionally, Rel-
ativeConUCB performs the worst compared with algorithms with
nonlinear reward assumptions due to the less practical linear model
assumption in the RelativeConUCB algorithm, which does not fit
in with our experimental setting.
Impact of conversation frequency and data dimension. We
next study the impact of different conversation frequencies and
data dimensions. For the impact of conversation frequencies, since
key-term-level conversations are less frequent than arm-level in-
teractions, we consider the linear function: ğ‘(ğ‘¡)=ğ‘›âŒŠğ‘¡/50âŒ‹, which
means asking ğ‘›questions per 50 iterations, as well as the log func-
tion:ğ‘(ğ‘¡)=ğ‘›âŒŠlogğ‘¡âŒ‹. We vary the value of ğ‘›to be 1, 5, 10, and 20 for
both functions. The cumulative regrets of each conversation type
and pool size are shown in Figure 3. According to Figure 3, more
conversations can help reduce cumulative regrets more, for example,
cumulative regret is the largest when ğ‘(ğ‘¡)=âŒŠğ‘¡/50âŒ‹and smallest
whenğ‘(ğ‘¡)=20âŒŠğ‘¡/50âŒ‹. When the conversation frequency increases,
our proposed algorithms utilizing explorative key-term strategy
(namely, ConDuel and ConDuel-MaxInp) demonstrate more advan-
tages than other algorithms. To test the impact of data dimensions
3811Conversational Dueling Bandits in Generalized Linear Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
b(t)=n*âŒŠlog tâŒ‹ b(t)=n*âŒŠt/50âŒ‹ dimension = n
Figure 3: Ablation study on synthetic data
on our algorithms and validate the realizability of the proposed
ConDuel in a higher dimension setting, we generate synthetic data
of different dimensions, the data dimensions are set to be 20, 30,
40, and 50 with ğ‘(ğ‘¡)=10âŒŠğ‘¡/50âŒ‹. As is shown in Figure 3, as data
dimension increases, the cumulative regret naturally increases, yet
our proposed ConDuel algorithm with its variants still maintains
superiority over other algorithms.
5.3 Real-world Datasets
Data Generation. We next display the experimental results on
two real-world datasets, Last.FM and Movielens, which are re-
leased in [ 6]. Last.FM is a dataset for music artist recommenda-
tions that contains 186,479 interaction records between 1,892 users
and 17,632 artists. Movielens is a dataset that extends the original
MovieLens10M dataset to include tagging information in IMDb and
Rotten Tomatoes for movie recommendations and contains 47,957
interaction records between 2,113 users and 10,197 movies.
We prepossess the data following [ 24] and treat artists and
movies as items, we then infer usersâ€™ real feedback on items based
on the interaction records: if the user has assigned attributes to the
item, the feedback is 1, otherwise, the feedback is missing. For both
datasets, we extract |A|= 2,000 with the most assigned attributes
by users and ğ‘ğ‘¢= 100 users who have assigned the most attributes.
For each arm, we keep at most 20 attributes that are related to
most items and treat them as the related key-terms of the item. All
the kept key-terms associated with the arms form the key-term
setKâ€”the number of key-terms for Last.FM is 2,726 and that for
Movielens is 5,585. The weights of all key-terms related to the same
arm are set to be equal, and we set the feature vectors to be ğ‘‘=10
to save the computation complexity.
Results. We compare the performance of algorithms in the two
real datasets. We run the experiments 10 times and calculate all
usersâ€™ average regret over ğ‘‡= 10,000 rounds on the fixed generated
datasets. We set ğ‘(ğ‘¡)=10âŒŠğ‘¡
50âŒ‹and|ğ´ğ‘¡|= 50. The evaluation results
are shown in Figure 2. It can be seen from the figures that on
both datasets, the ConDuel-MaxInp algorithm achieves the best
performance in terms of cumulative regret and standard error, and
the regret of ConDuel is also slightly lower compared with ConDuel-
Random. In both cases, the ConDuel algorithms with explorative
key-term selection strategy show their strengths compared to other
algorithms.6 Extension to MNL Bandit
Besides pairwise comparison for conversational dueling bandits,
we can also extend the conversational mechanism to the multiple
comparison setting under the choice model, also known as the
Multinomial Logit Bandit (MNL) problem [ 15,16]. The arm setA
of sizeğ‘and key-term setKof sizeğ¾are defined in section 3. We
also defineCto be the set of candidate assortments with size less
thanğ‘, i.e.C={ğ¶âŠ‚[ğ‘]:|ğ¶|â‰¤ğ‘}, whereğ‘â‰¥2. In each iteration
ğ‘¡, for the arm-level selection, the agent is offered an assortment
ğ¶ğ‘¡={ğ‘ğ‘–1,...,ğ‘ğ‘–ğ‘¡}âŠ‚C and observes feature vector ğ‘¥ğ‘,ğ‘¡for each
ğ‘âˆˆğ¶ğ‘¡. The user purchase decision ğ‘œğ‘¡âˆˆğ¶ğ‘¡Ã{0}is observed,
and we can denote the user purchase decision for each ğ‘âˆˆğ¶ğ‘¡
asğ‘œğ‘,ğ‘¡=1(ğ‘is chosen)âˆˆ{ 0,1}andğ‘œ0,ğ‘¡indicating not choosing
from the item set. Similarly, at ğ‘¡-th key-term level selection, the
user observes key-term subset ğ¾ğ‘¡with|ğ¾ğ‘¡|â‰¤ğ‘and gives certain
feedback Ëœğ‘œğ‘¡, with Ëœğ‘œğ‘˜,ğ‘¡indicating whether key-term ğ‘˜is chosen and
Ëœğ‘œ0,ğ‘¡representing not choosing.
We define the ConMNL algorithm in Algorithm 2. The user
selection for item ğ‘âˆˆğ¶ğ‘¡and key-term ğ‘˜âˆˆğ¾ğ‘¡at roundğ‘¡is given
by the MNL choice model, defined in the following equations:
ğ‘ğ‘–(ğ¶ğ‘¡,ğœƒâˆ—)=ï£±ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³exp(ğ‘¥ğ‘‡
ğ‘–,ğ‘¡ğœƒâˆ—)
1+Ã
ğ‘—âˆˆğ¶ğ‘¡exp(ğ‘¥ğ‘‡
ğ‘—,ğ‘¡ğœƒâˆ—), ğ‘–âˆˆğ¶ğ‘¡,
1
1+Ã
ğ‘—âˆˆğ¶ğ‘¡exp(ğ‘¥ğ‘‡
ğ‘—,ğ‘¡ğœƒâˆ—), ğ‘–=0,
Ëœğ‘ğ‘¡(ğ¾ğ‘¡,ğœƒâˆ—)=ï£±ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³exp(Ëœğ‘¥ğ‘‡
ğ‘˜,ğ‘¡ğœƒâˆ—)
1+Ã
ğ‘—âˆˆğ¾ğ‘¡exp(Ëœğ‘¥ğ‘‡
ğ‘—,ğ‘¡ğœƒâˆ—), ğ‘˜âˆˆğ¾ğ‘¡,
1
1+Ã
ğ‘—âˆˆğ¾ğ‘¡exp(Ëœğ‘¥ğ‘‡
ğ‘—,ğ‘¡ğœƒâˆ—), ğ‘˜=0.
Hereğœƒâˆ—âˆˆR. Notice that when ğ‘=2, the choice model can be
seen as the dueling bandit model. We can rewrite the arm-level and
key-term level choice model as follows:
ğ‘œğ‘,ğ‘¡=ğ‘ğ‘(ğ¶ğ‘¡,ğœƒâˆ—)+ğœ–ğ‘,ğ‘¡ (13)
Ëœğ‘œğ‘˜,ğ‘¡=Ëœğ‘ğ‘˜(ğ¾ğ‘¡,ğœƒâˆ—)+Ëœğœ–ğ‘˜,ğ‘¡ (14)
It is easy to verify that the noise ğœ–ğ‘,ğ‘¡and Ëœğœ–ğ‘˜,ğ‘¡areğœ2sub-Gaussian
variable with ğœ=0.5. We assume that and the expected revenue
of the assortment ğ¶ğ‘¡to beğ‘…ğ‘¡(ğ¶ğ‘¡,ğœƒâˆ—)=Ã
ğ‘—âˆˆğ¶ğ‘¡ğ‘Ÿğ‘—,ğ‘¡ğ‘ğ‘—(ğ¶ğ‘¡,ğœƒâˆ—), where
ğ‘Ÿğ‘—,ğ‘¡is the revenue from the recommendation if item ğ‘–is chosen by
3812KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shuhua Yang et al.
Algorithm 2: The ConMNL Algorithm
Input:(A,K,W),ğ‘(ğ‘¡),ğ‘, initialization ğ‘‡0;
1Initialization: forğ‘¡âˆˆ[ğ‘‡0]do
2 ifğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1)>0then
3ğ‘ğ‘¡=ğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1);
4 whileğ‘ğ‘¡>0do
5 Randomly select ğ‘key-terms from Barycentric
SpannerB;
6 Updateğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1+Ã
ğ‘˜âˆˆB Ëœğ‘¥ğ‘˜,ğ‘¡Ëœğ‘¥ğ‘˜,ğ‘¡ğ‘‡;
7 ğ‘ğ‘¡=ğ‘ğ‘¡âˆ’1;
8 end
9 else
10ğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1
11 end
12 Randomly choose ğ¶ğ‘¡âˆˆCwith|ğ¶ğ‘¡|=ğ‘;
13 Updateğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1+Ã
ğ‘–âˆˆğ¶ğ‘¡ğ‘¥ğ‘–,ğ‘¡ğ‘¥ğ‘‡
ğ‘–,ğ‘¡
14end
15forğ‘¡=ğ‘‡0+1,...,ğ‘‡ do
16 ifğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1)>0then
17ğ‘ğ‘¡=ğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1);
18 whileğ‘ğ‘¡>0do
19 Offerğ¾ğ‘¡based on key-term selection principle,
and observe key-term level feedback Ëœğ‘œğ‘¡;
20 Updateğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1+Ã
ğ‘˜âˆˆğ¾ğ‘¡Ëœğ‘¥ğ‘˜,ğ‘¡Ëœğ‘¥ğ‘˜,ğ‘¡ğ‘‡;
21 ğ‘ğ‘¡=ğ‘ğ‘¡âˆ’1;
22 end
23 else
24ğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1
25 end
26 MLEğœƒğ‘¡is estimated according to the regularized
log-likelihood function in Eq 17;
27 Offerğ¶ğ‘¡=arg maxğ¶âˆˆCË†ğ‘…ğ‘¡(ğ¶,ğœƒğ‘¡)to the user and
observe user choice ğ‘œğ‘¡;
28 Updateğ‘€ğ‘¡=ğ‘€ğ‘¡âˆ’1+Ã
ğ‘–âˆˆğ¶ğ‘¡ğ‘¥ğ‘–,ğ‘¡ğ‘¥ğ‘‡
ğ‘–,ğ‘¡
29end
user at round ğ‘¡; and the optimal assortment ğ¶âˆ—
ğ‘¡=arg maxğ¶âˆˆCğ‘…ğ‘¡(ğ¶,ğœƒâˆ—).
The cumulative expected regret over time ğ‘‡is defined as:
ğ‘…(ğ‘‡)=ğ‘‡âˆ‘ï¸
ğ‘¡=1Â©Â­
Â«âˆ‘ï¸
ğ‘—âˆˆğ¶âˆ—
ğ‘¡ğ‘Ÿğ‘—,ğ‘¡ğ‘ğ‘—(ğ¶âˆ—
ğ‘¡,ğœƒâˆ—)âˆ’âˆ‘ï¸
ğ‘—âˆˆğ¶ğ‘¡ğ‘Ÿğ‘—,ğ‘¡ğ‘ğ‘—(ğ¶ğ‘¡,ğœƒâˆ—)ÂªÂ®
Â¬
=ğ‘‡âˆ‘ï¸
ğ‘¡=1 ğ‘…ğ‘¡(ğ¶âˆ—
ğ‘¡,ğœƒâˆ—)âˆ’ğ‘…ğ‘¡(ğ¶ğ‘¡,ğœƒâˆ—). (15)
Following the definition of Barycentric Spanner Bof key-term set
in sec Â§ 4.1.3, we assume ğœ†â€²
B=ğœ†min
ğ¸ğ‘˜âˆ¼B[Ëœğ‘¥ğ‘˜Ëœğ‘¥ğ‘‡
ğ‘˜]
>0. Based on
[9, 14] and [15], we also make the following assumptions:
Assumption 3. For everyğ‘¡andğ‘–âˆˆğ¶, there exists a constant ğœ…2>0,
whereğœ…2:=minâˆ¥ğœƒâˆ’ğœƒâˆ—âˆ¥â‰¤1,|ğ¶|â‰¤ğ‘ğ‘ğ‘–(ğ¶,ğœƒ)ğ‘0(ğ¶,ğœƒ).
Assumption 4. Each feature vector ğ‘¥ğ‘–,ğ‘¡,âˆ¥ğ‘¥ğ‘–,ğ‘¡âˆ¥â‰¤1and there exists
a constantğœ0>0, withğ¸[ğ‘¥ğ‘‡
ğ‘–,ğ‘¡ğ‘¥ğ‘–,ğ‘¡]â‰¥ğœ0.We estimate parameter ğœƒâˆ—following similar procedures in sec Â§ 4.1.
The log-likelihood function till ğ‘¡-th round under parameter ğœƒis
given by
ğ¿ğ‘¡(ğœƒ)=ğ‘¡âˆ’1âˆ‘ï¸
ğœ=1âˆ‘ï¸
ğ‘–âˆˆğ¶ğœğ‘œğ‘ ,ğ‘–ğ‘™ğ‘œğ‘”(ğœ‡ğ‘–(ğ¶ğœ,ğœƒ))+ğ‘¡âˆ‘ï¸
ğœ=1âˆ‘ï¸
ğ‘˜âˆˆğ¾ğœËœğ‘œğœ,ğ‘˜ğ‘™ğ‘œğ‘”(Ëœğœ‡ğ‘˜(ğ¾ğœ,ğœƒ).
(16)
Settingâˆ‡ğœƒğ¿ğ‘¡(ğœƒ)=0, the maximum likelihood estimation ğœƒğ‘¡is the
solution of:
ğ‘¡âˆ’1âˆ‘ï¸
ğœ=1âˆ‘ï¸
ğ‘–âˆˆğ¶ğœ(ğ‘œğ‘–,ğ‘ âˆ’ğœ‡ğ‘–(ğ¶ğœ,ğœƒ))ğ‘¥ğ‘–,ğ‘ +ğ‘¡âˆ‘ï¸
ğœ=1âˆ‘ï¸
ğ‘˜âˆˆğ¾ğœ(Ëœğ‘œğ‘˜,ğœâˆ’Ëœğœ‡ğ‘˜(ğ¾ğœ,ğœƒ))Ëœğ‘¥ğ‘˜,ğœ=0.
(17)
Defineğ‘€ğ‘¡=Ãğ‘¡âˆ’1
ğœ=1Ã
ğ‘–âˆˆğ¶ğœğ‘¥ğ‘–,ğœğ‘¥ğ‘‡
ğ‘–,ğœ+Ãğ‘¡
ğœ=1Ã
ğ‘˜âˆˆğ¾ğœËœğ‘¥ğ‘˜,ğœËœğ‘¥ğ‘‡
ğ‘˜,ğœ. We can
calculate the MLE ğœƒğ‘¡to obtain the UCB estimate ğ‘§ğ‘,ğ‘¡=ğ‘¥ğ‘‡
ğ‘,ğ‘¡ğœƒğ‘¡+
ğ›¼ğ‘¡âˆ¥ğ‘¥ğ‘,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡regarding the utility of each ğ‘âˆˆAğ‘¡at timeğ‘¡, withğ›¼ğ‘¡=
1
2ğœ…2âˆšï¸ƒ
2ğ‘‘log(1+ğ‘(ğ‘¡)+ğ‘¡
ğ‘‘)+log(1
ğ›¿). In the item selection module, we
construct the optimal estimate of the expected revenue by choosing
ğ¶as
Ë†ğ‘…ğ‘¡(ğ¶)=Ã
ğ‘–âˆˆğ¶ğ‘¢(ğ‘–,ğ‘¡)exp(ğ‘§ğ‘–,ğ‘¡)
1+Ã
ğ‘–âˆˆğ¶exp(ğ‘§ğ‘–,ğ‘¡), (18)
and offerğ¶ğ‘¡=arg maxğ¶âˆˆCË†ğ‘…ğ‘¡(ğ¶)to the user at time ğ‘¡. For the key-
term query module, we choose ğ‘key-terms with each uniformly
sampled from barycentric spanner Bto formKğ‘¡.
The regret upper bound of ConMNL is given in Theorem 2.
Theorem 2. Assume conversation frequency to be ğ‘(ğ‘¡)=ğ‘Â·ğ‘¡, with
ğ‘âˆˆ (0,1), andğ›¼ğ‘¡=1
2ğœ…2âˆšï¸ƒ
2ğ‘‘log(1+ğ‘(ğ‘¡)+ğ‘¡
ğ‘‘)+2 log(ğ‘¡), andğ‘‡0=
O
ğ‘šğ‘ğ‘¥{1
ğœ…2
2(ğ‘‘log(ğ‘(ğ‘‡)+ğ‘‡
ğ‘‘+4 log(ğ‘‡)),ğ‘/ğœ2,256
ğ‘ğ‘ğœ†â€²
Blog(128ğ‘‘
ğ›¿ğœ†2
Bâ€²)}
. The
expected regret of ConMNL is upper bound by ğ‘…ğ‘‡=O(âˆšï¸
ğ‘‘ğ‘‡ğ‘log(ğ‘‡)).
6.1 Experiments
We compare our ConMNL algorithm with the following baselines
on the previous three datasets:
â€¢UCB-MNL. An algorithm designed for MNL contextual ban-
dit in [15] with no conversation on key-terms.
â€¢ConMNL-ucb: A variant of ConMNL that selects ğ‘key-
terms at each conversation based on UCB estimate Ëœğ‘¥ğ‘‡
ğ‘˜,ğ‘¡ğœƒğ‘¡+
ğ›¼ğ‘¡âˆ¥Ëœğ‘¥ğ‘˜,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡.
â€¢ConMNL-random: A variant of ConMNL that selects ğ‘
key-terms randomly at each conversation.
We set horizon ğ‘‡=2000 andğ‘=4for each dataset, allowing
at mostğ‘items and key-terms for the user to choose from. We
also assume the expected revenue of choosing item ğ‘–is given by
ğ‘Ÿğ‘–,ğ‘¡=ğ‘¥ğ‘‡
ğ‘–,ğ‘¡ğœƒâˆ—, the utility of each item. The conversation frequency
ğ‘(ğ‘¡)is set to be 5âŒŠğ‘¡
50âŒ‹, and pool size|ğ´ğ‘¡|= 50. It should be noted
that the ConMNL-ucb and ConMNL-random follow the same item
assortment selection principle proposed in the ConMNL algorithm,
aiming to compare the impact of different conversation mechanisms
on the performance of recommender systems. We ran experiments
on each user 10 times and calculated the average regret as well as
standard error. The regret curve for each dataset is shown in Fig 4.
3813Conversational Dueling Bandits in Generalized Linear Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 4: Cumulative regret on synthetic and real-world datasets
It can be seen from the figures that our algorithm along with its
variants all perform better than UCB-MNL on each dataset, con-
firming that carrying out conversations on key-terms can enhance
the model performance. Furthermore, ConMNL performs the best
on Last.FM dataset, while ConMNL-ucb achieves relatively bet-
ter results on both synthetic dataset and Movielens dataset. This
may be due to the complexity of constructing a ğ‘-size choice set,
where utilizing information solely from the barycentric spanner
subset from the key-term set may be inadequate to capture the user
preference.
7 Conclusion
In this paper, we study a novel framework of conversational bandits
with an informative feedback mechanism in the generalized linear
models and propose the ConDuel algorithm that can guarantee
relative feedback from both key-term and item modules. We design
new methods to effectively duel key-term pairs and item pairs in our
algorithm, which allow the system to conduct exploratory conver-sations to utilize key-term pairwise feedback in the key-term mod-
ule. Meanwhile, we select the most informative pairs in the item
module to grasp the user preferences more accurately. We prove a re-
gret upper bound of O(âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡))of our algorithm, and extensive
experiments on both synthetic data and real-world datasets have
demonstrated the competitiveness of our algorithm. We also extend
our algorithm to multiple comparisons under the MNL choice model
and propose the ConMNL algorithm with a theoretical guarantee.
For future research, it would be intriguing to consider: 1) Incorpo-
rate additional structure for key-term module, such as knowledge
graph [ 29] or clustering [ 23]; 2) Consider different distributions
on key-terms and higher dimensions of large-scale dataset in real
dataset experiments.
Acknowledgments
Mengdi Wang acknowledges the support by NSF grants DMS-
1953686, IIS-2107304, CMMI-1653435, ONR grant 1006977, and
C3.AI.
3814KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shuhua Yang et al.
References
[1]Yasin Abbasi-Yadkori, DÃ¡vid PÃ¡l, and Csaba SzepesvÃ¡ri. 2011. Improved al-
gorithms for linear stochastic bandits. In Proceedings of the 24th International
Conference on Neural Information Processing Systems (Granada, Spain) (NIPSâ€™11).
Curran Associates Inc., Red Hook, NY, USA, 2312â€“2320.
[2]Jean-Yves Audibert and SÃ©bastien Bubeck. 2010. Best Arm Identification in Multi-
Armed Bandits. In COLT - 23th Conference on Learning Theory - 2010. Haifa, Israel,
13 p. https://enpc.hal.science/hal-00654404
[3]Baruch Awerbuch and Robert D. Kleinberg. 2004. Adaptive routing with end-to-
end feedback: distributed learning and geometric approaches. In Proceedings of
the Thirty-Sixth Annual ACM Symposium on Theory of Computing (Chicago, IL,
USA) (STOC â€™04) . Association for Computing Machinery, New York, NY, USA,
45â€“53. https://doi.org/10.1145/1007352.1007367
[4]Viktor Bengs, RÃ³bert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke HÃ¼llermeier.
2021. Preference-based online learning with dueling bandits: A survey. The
Journal of Machine Learning Research 22, 1 (2021), 278â€“385.
[5]Viktor Bengs, Aadirupa Saha, and Eyke HÃ¼llermeier. 2022. Stochastic Contextual
Dueling Bandits under Linear Stochastic Transitivity Models. In Proceedings of
the 39th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 1764â€“1786. https:
//proceedings.mlr.press/v162/bengs22a.html
[6]Ivan Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. Second workshop on
information heterogeneity and fusion in recommender systems (HetRec2011).
InProceedings of the Fifth ACM Conference on Recommender Systems (Chicago,
Illinois, USA) (RecSys â€™11). Association for Computing Machinery, New York, NY,
USA, 387â€“388. https://doi.org/10.1145/2043932.2044016
[7]Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards
Conversational Recommender Systems. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (San Francisco,
California, USA) (KDD â€™16). Association for Computing Machinery, New York,
NY, USA, 815â€“824. https://doi.org/10.1145/2939672.2939746
[8]Miroslav DudÃ­k, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, and
Masrour Zoghi. 2015. Contextual Dueling Bandits. In Proceedings of The 28th
Conference on Learning Theory (Proceedings of Machine Learning Research, Vol. 40),
Peter GrÃ¼nwald, Elad Hazan, and Satyen Kale (Eds.). PMLR, Paris, France, 563â€“587.
https://proceedings.mlr.press/v40/Dudik15.html
[9]Sarah Filippi, Olivier CappÃ©, AurÃ©lien Garivier, and Csaba SzepesvÃ¡ri. 2010. Para-
metric bandits: the Generalized Linear case. In Proceedings of the 23rd Interna-
tional Conference on Neural Information Processing Systems - Volume 1 (Vancouver,
British Columbia, Canada) (NIPSâ€™10). Curran Associates Inc., Red Hook, NY, USA,
586â€“594.
[10] Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, and Mengdi Wang. 2023.
Provable Benefits of Policy Learning from Human Preferences in Contextual
Bandit Problems. arXiv preprint arXiv:2307.12975 (2023).
[11] Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad
Ghavamzadeh, and Craig Boutilier. 2020. Randomized Exploration in Gener-
alized Linear Bandits. In Proceedings of the Twenty Third International Confer-
ence on Artificial Intelligence and Statistics (Proceedings of Machine Learning
Research, Vol. 108), Silvia Chiappa and Roberto Calandra (Eds.). PMLR, 2066â€“2076.
https://proceedings.mlr.press/v108/kveton20a.html
[12] Lihong Li, Wei Chu, John Langford, Taesup Moon, and Xuanhui Wang. 2012.
An Unbiased Offline Evaluation of Contextual Bandit Algorithms with Gen-
eralized Linear Models. In Proceedings of the Workshop on On-line Trading of
Exploration and Exploitation 2 (Proceedings of Machine Learning Research, Vol. 26),
Dorota Glowacka, Louis Dorard, and John Shawe-Taylor (Eds.). PMLR, Bellevue,
Washington, USA, 19â€“36. https://proceedings.mlr.press/v26/li12a.html
[13] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A contextual-
bandit approach to personalized news article recommendation. In Proceedings of
the 19th International Conference on World Wide Web (Raleigh, North Carolina,
USA) (WWW â€™10). Association for Computing Machinery, New York, NY, USA,
661â€“670. https://doi.org/10.1145/1772690.1772758
[14] Lihong Li, Yu Lu, and Dengyong Zhou. 2017. Provably optimal algorithms for
generalized linear contextual bandits. In Proceedings of the 34th International
Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICMLâ€™17).
JMLR.org, 2071â€“2080.
[15] Min-hwan Oh and Garud Iyengar. 2021. Multinomial logit contextual bandits:
Provable optimality and practicality. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 35. 9205â€“9213.
[16] Mingdong Ou, Nan Li, Shenghuo Zhu, and Rong Jin. 2018. Multinomial logit
bandit with linear utility functions. arXiv preprint arXiv:1805.02971 (2018).
[17] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022.
Training language models to follow instructions with human feedback. Advances
in Neural Information Processing Systems 35 (2022), 27730â€“27744.[18] Aadirupa Saha. 2021. Optimal Algorithms for Stochastic Contextual Preference
Bandits. Advances in Neural Information Processing Systems 34 (2021), 30050â€“
30062.
[19] Aadirupa Saha and Akshay Krishnamurthy. 2022. Efficient and Optimal Algo-
rithms for Contextual Dueling Bandits under Realizability. In Proceedings of The
33rd International Conference on Algorithmic Learning Theory (Proceedings of
Machine Learning Research, Vol. 167), Sanjoy Dasgupta and Nika Haghtalab (Eds.).
PMLR, 968â€“994. https://proceedings.mlr.press/v167/saha22a.html
[20] Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. 2023. Dueling RL: Reinforce-
ment Learning with Trajectory Preferences. In Proceedings of The 26th Interna-
tional Conference on Artificial Intelligence and Statistics (Proceedings of Machine
Learning Research, Vol. 206), Francisco Ruiz, Jennifer Dy, and Jan-Willem van de
Meent (Eds.). PMLR, 6263â€“6289. https://proceedings.mlr.press/v206/saha23a.html
[21] Ayush Sekhari, Karthik Sridharan, Wen Sun, and Runzhe Wu. 2024. Contextual
bandits and imitation learning with preference-based active queries. In Proceed-
ings of the 37th International Conference on Neural Information Processing Systems
(New Orleans, LA, USA) (NIPS â€™23). Curran Associates Inc., Red Hook, NY, USA,
Article 499, 35 pages.
[22] Zhiyong Wang, Xutong Liu, Shuai Li, and John C.S. Lui. 2023. Efficient ex-
plorative key-term selection strategies for conversational contextual bandits.
InProceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence
and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence
and Thirteenth Symposium on Educational Advances in Artificial Intelligence
(AAAIâ€™23/IAAIâ€™23/EAAIâ€™23). AAAI Press, Article 1156, 8 pages. https://doi.org/
10.1609/aaai.v37i8.26225
[23] Junda Wu, Canzhe Zhao, Tong Yu, Jingyang Li, and Shuai Li. 2021. Cluster-
ing of Conversational Bandits for User Preference Learning and Elicitation. In
Proceedings of the 30th ACM International Conference on Information & Knowl-
edge Management. Association for Computing Machinery, New York, NY, USA,
2129â€“2139.
[24] Zhihui Xie, Tong Yu, Canzhe Zhao, and Shuai Li. 2021. Comparison-based Con-
versational Recommender System with Relative Bandit Feedback. In Proceedings
of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval. Association for Computing Machinery, New York, NY,
USA, 1400â€“1409.
[25] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. 2020.
Preference-based reinforcement learning with finite-time guarantees. Advances
in Neural Information Processing Systems 33 (2020), 18784â€“18794.
[26] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. The
k-armed dueling bandits problem. J. Comput. System Sci. 78, 5 (2012), 1538â€“1556.
[27] Yisong Yue and Thorsten Joachims. 2011. Beat the mean bandit. In Proceedings of
the 28th International Conference on International Conference on Machine Learning
(Bellevue, Washington, USA) (ICMLâ€™11). Omnipress, Madison, WI, USA, 241â€“248.
[28] Xiaoying Zhang, Hong Xie, Hang Li, and John CS Lui. 2020. Conversational
contextual bandit: Algorithm and application. In Proceedings of the web conference
2020. Association for Computing Machinery, New York, NY, USA, 662â€“672.
[29] Canzhe Zhao, Tong Yu, Zhihui Xie, and Shuai Li. 2022. Knowledge-aware Conver-
sational Preference Elicitation with Bandit Feedback. In Proceedings of the ACM
Web Conference 2022 (Lyon, France) (WWW â€™22). Association for Computing Ma-
chinery, New York, NY, USA, 483â€“492. https://doi.org/10.1145/3485447.3512152
[30] Banghua Zhu, Michael Jordan, and Jiantao Jiao. 2023. Principled Reinforce-
ment Learning with Human Feedback from Pairwise or K-wise Comparisons. In
Proceedings of the 40th International Conference on Machine Learning (Proceed-
ings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.).
PMLR, 43037â€“43067. https://proceedings.mlr.press/v202/zhu23f.html
[31] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. 2014. Relative
Upper Confidence Bound for the K-Armed Dueling Bandit Problem. In Proceedings
of the 31st International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 32), Eric P. Xing and Tony Jebara (Eds.). PMLR, Bejing,
China, 10â€“18. https://proceedings.mlr.press/v32/zoghi14.html
A Proofs for ConDuel Algorithm
Proof of Lemma 1. According to Eq. (7)and Eq. (8)and the
definition of ğ‘”ğ‘¡(ğœƒ),âˆ€ğœƒ1,ğœƒ2âˆˆÎ˜, we have:
ğ‘”ğ‘¡(ğœƒ1)âˆ’ğ‘”ğ‘¡(ğœƒ2)=(ğºğ‘
ğ‘¡(ğœƒ1,ğœƒ2)+ğºğ‘˜
ğ‘¡(ğœƒ1,ğœƒ2)+ğœ†ğ¼)(ğœƒ1âˆ’ğœƒ2)
â‰œğºğ‘¡(ğœƒ1,ğœƒ2)(ğœƒ1âˆ’ğœƒ2)>ğœ…1ğ‘€ğ‘¡(ğœƒ1âˆ’ğœƒ2), (19)
whereğºğ‘
ğ‘¡(ğœƒ1,ğœƒ2)=Ãğ‘¡âˆ’1
ğ‘ =1ğœ‡â€²(ğ‘‘ğ‘‡ğ‘ Â¯ğœƒ(ğœƒ1,ğœƒ2))ğ‘‘ğ‘ ğ‘‘ğ‘‡ğ‘ , andğºğ‘˜
ğ‘¡(ğœƒ1,ğœƒ2)=Ãğ‘¡
ğ‘ =1Ã
Ëœğ‘‘âˆˆËœDğ‘ ğœ‡â€²(Ëœğ‘‘ğ‘‡ğ‘ Â¯ğœƒ(ğœƒ1,ğœƒ2)Ëœğ‘‘ğ‘ Ëœğ‘‘ğ‘‡ğ‘ .
Based on the mean value theorem, we also have Â¯ğœƒ(ğœƒ1,ğœƒ2)=
ğ‘£ğœƒ1+(1âˆ’ğ‘£)ğœƒ2. According to Eq. (3)and Eq. (4), we have the equality:
3815Conversational Dueling Bandits in Generalized Linear Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ğ‘”ğ‘¡(ğœƒğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒâˆ—)=ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğœ–ğ‘ ğ‘‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
Ëœğ‘‘âˆˆËœDğ‘ Ëœğœ–ğ‘ Ëœğ‘‘ğ‘ âˆ’ğœ†ğœƒâˆ—
â‰œğ‘†ğ‘¡âˆ’ğœ†ğœƒâˆ—>ğœ…1ğ‘€ğ‘¡(ğœƒğ‘¡âˆ’ğœƒâˆ—) (20)
Combined with Eq. (7), (8) and (9), âˆ€ğ‘¥âˆˆRğ‘‘, we thus have:
|ğ‘¥ğ‘‡(ğœƒ(1)
ğ‘¡âˆ’ğœƒâˆ—)|=|ğ‘¥ğ‘‡ğºâˆ’1
ğ‘¡(ğœƒ(1)
ğ‘¡,ğœƒâˆ—)(ğ‘”ğ‘¡(ğœƒ(1)
ğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒâˆ—))|
â‰¤âˆ¥ğ‘¥âˆ¥ğºâˆ’1
ğ‘¡(ğœƒ(1)
ğ‘¡,ğœƒâˆ—)âˆ¥ğ‘”ğ‘¡(ğœƒ(1)
ğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒâˆ—)âˆ¥ğºâˆ’1
ğ‘¡(ğœƒ(1)
ğ‘¡,ğœƒâˆ—)
â‰¤(1)1
ğœ…1âˆ¥ğ‘¥âˆ¥ğ‘€âˆ’1
ğ‘¡âˆ¥ğ‘”ğ‘¡(ğœƒ(1)
ğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒâˆ—)âˆ¥ğ‘€âˆ’1
ğ‘¡
=1
ğœ…1âˆ¥ğ‘¥âˆ¥ğ‘€âˆ’1
ğ‘¡âˆ¥ğ‘”ğ‘¡(ğœƒ(1)
ğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒğ‘¡)+ğ‘”ğ‘¡(ğœƒğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒâˆ—)âˆ¥ğ‘€âˆ’1
ğ‘¡
â‰¤(2)1
ğœ…1âˆ¥ğ‘¥âˆ¥ğ‘€âˆ’1
ğ‘¡(âˆ¥ğ‘”ğ‘¡(ğœƒ(1)
ğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒğ‘¡)âˆ¥ğ‘€âˆ’1
ğ‘¡+âˆ¥ğ‘”ğ‘¡(ğœƒğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒâˆ—)âˆ¥ğ‘€âˆ’1
ğ‘¡)
â‰¤(3)2
ğœ…1âˆ¥ğ‘¥âˆ¥ğ‘€âˆ’1
ğ‘¡âˆ¥ğ‘”ğ‘¡(ğœƒğ‘¡)âˆ’ğ‘”ğ‘¡(ğœƒâˆ—)âˆ¥ğ‘€âˆ’1
ğ‘¡
â‰¤2
ğœ…1(âˆ¥ğ‘†ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡+âˆšï¸
ğœ†ğœ…1âˆ¥ğœƒâˆ—âˆ¥2)âˆ¥ğ‘¥âˆ¥ğ‘€âˆ’1
ğ‘¡(21)
The first inequality (1)comes from ğºğ‘¡(ğœƒ(1)
ğ‘¡,ğœƒâˆ—) â‰¥ğœ…1ğ‘€ğ‘¡>0,
and therefore ğºğ‘¡(ğœƒ(1)
ğ‘¡,ğœƒâˆ—)âˆ’1â‰¤1
ğœ…1ğ‘€ğ‘¡. The second inequality is the
application of triangle inequality, and the second inequality (3)is
based on the definition of ğœƒ(1)
ğ‘¡from Eq. (9).
Notice thatğœ–ğ‘¡andËœğœ–ğ‘¡areğ‘…-subgaussian, andâˆ€ğ‘¡,âˆ¥ğ‘‘ğ‘¡âˆ¥2,âˆ¥Ëœğ‘‘ğ‘¡âˆ¥2â‰¤2.
According to Theorem 1 in [1], we have:
âˆ¥ğ‘†ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡â‰¤ğ‘…âˆšï¸„
2 log(det(ğ‘€ğ‘¡)1/2/(ğœ†
ğœ…1)ğ‘‘/2
ğ›¿)
â‰¤ğ‘…âˆšï¸‚
ğ‘‘log((1+4ğœ…1(ğ‘¡+ğ‘(ğ‘¡))
ğœ†ğ‘‘)/ğ›¿). (22)
Therefore,ğ›¼ğ‘¡â‰œ2
ğœ…1(ğ‘…âˆšï¸ƒ
ğ‘‘log((1+4ğœ…1(ğ‘¡+ğ‘(ğ‘¡))
ğ‘‘ğœ†)/ğœ)+âˆšğœ†ğœ…1âˆ¥ğœƒâˆ—âˆ¥2).
â–¡
Proof of Lemma 2. Lemma 2 follows the existing results of Propo-
sition 1 in [14].
DenoteBas the barycentric spanner for key-term set K. Let
ğ‘¥,ğ‘¦be random vectors sampled independently and uniformly from
B, i.e.ğ‘¥,ğ‘¦ğ‘–ğ‘–ğ‘‘âˆ¼ B . Define Î£â‰œğ¸ğ‘¥,ğ‘¦ğ‘–ğ‘–ğ‘‘âˆ¼B[(ğ‘¥âˆ’ğ‘¦)(ğ‘¥âˆ’ğ‘¦)ğ‘‡]. For ease
of understanding, we assume the pair of key-terms at round ğ‘¡con-
versation with contextual vectors (Ëœğ‘¥ğ‘¡,Ëœğ‘¦ğ‘¡), and the key-term level
design matrix is denoted as Ëœğ‘€ğ‘¡=Ãğ‘¡
ğ‘ =1Ã
Ëœğ‘¥,Ëœğ‘¦ğ‘–ğ‘–ğ‘‘âˆ¼B(Ëœğ‘¥ğ‘ âˆ’Ëœğ‘¦ğ‘ )(Ëœğ‘¥ğ‘ âˆ’Ëœğ‘¦ğ‘ )ğ‘‡.
Defineğ‘§ğ‘¡=Î£âˆ’1/2(ğ‘¥ğ‘¡âˆ’ğ‘¦ğ‘¡), thenğ‘§ğ‘¡is isotropic, namely, ğ¸[ğ‘§ğ‘¡ğ‘§ğ‘‡
ğ‘¡]=
ğ¼. Defineğ‘ˆğ‘¡=Ãğ‘¡
ğ‘ =1Ã
Ëœğ‘¥ğ‘ ,Ëœğ‘¦ğ‘ ğ‘–ğ‘–ğ‘‘âˆ¼Bğ‘§ğ‘ ğ‘§ğ‘‡ğ‘ =Î£âˆ’1/2Ëœğ‘€ğ‘¡Î£âˆ’1/2.
From Lemma 1 in [14], with probability at least (1 âˆ’2exp(âˆ’ğ¶2ğ‘¥2)):
ğœ†min(ğ‘ˆğ‘¡)â‰¥ğ‘(ğ‘¡)âˆ’ğ¶1ğœ2âˆšï¸
ğ‘(ğ‘¡)ğ‘‘âˆ’ğœ2ğ‘¥âˆšï¸
ğ‘(ğ‘¡),
whereğœis the sub-gaussian parameter of ğ‘§and is upper-bounded by
âˆ¥Î£âˆ’1/2âˆ¥=ğœ†min(Î£)âˆ’1/2. Therefore, with probability at least (1âˆ’ğ›¿):
ğœ†min(ğ‘ˆğ‘¡)â‰¥ğ‘(ğ‘¡)âˆ’1
ğœ†min(Î£)(ğ¶1âˆšï¸
ğ‘(ğ‘¡)ğ‘‘+ğ‘¥âˆšï¸
ğ‘(ğ‘¡)).
Furthermore, the minimum eigenvalue of Ëœğ‘€ğ‘¡is bounded as follows:
ğœ†ğ‘šğ‘–ğ‘›(Ëœğ‘€ğ‘¡)=min
ğ‘¥âˆˆBğ‘‘ğ‘¥ğ‘‡Ëœğ‘€ğ‘¡ğ‘¥=min
ğ‘¥âˆˆBğ‘‘ğ‘¥ğ‘‡Î£1/2ğ‘ˆğ‘¡Î£1/2ğ‘¥â‰¥ğœ†min(ğ‘ˆğ‘¡)ğœ†min(Î£)
â‰¥ğœ†min(Î£)(ğ‘(ğ‘¡)âˆ’ğœ†min(Î£)âˆ’1(ğ¶1âˆšï¸
ğ‘(ğ‘¡)ğ‘‘+ğ¶2âˆšï¸
ğ‘(ğ‘¡)log(1/ğ›¿)
=ğœ†min(Î£)ğ‘(ğ‘¡)âˆ’(ğ¶1âˆš
ğ‘‘+ğ¶2âˆšï¸
log(1/ğ›¿))âˆšï¸
ğ‘(ğ‘¡). (23)We denoteğœ†min(Î£)=ğœ†ğµ. Whenğ‘(ğ‘¡)â‰¥4(ğ¶1âˆš
ğ‘‘+ğ¶2âˆš
log(1/ğ›¿))2
ğœ†2
ğµ, we
have:
ğœ†min(Ëœğ‘€ğ‘¡)â‰¥ğœ†ğµğ‘(ğ‘¡)
2. (24)
When the conversation frequency function is linear, i.e., ğ‘(ğ‘¡)=ğ‘Â·ğ‘¡
for someğ‘âˆˆ(0,1), then with probability at least (1âˆ’ğ›¿), we have:
ğœ†min(Ëœğ‘€ğ‘¡)â‰¥ğœ†ğµğ‘ğ‘¡
2,
as long asğ‘¡â‰¥4(ğ¶1âˆš
ğ‘‘+ğ¶2âˆš
log(1/ğ›¿))2
ğ‘ğœ†2
ğµâ‰œğ‘¡0. â–¡
Proof of Lemma 3. According to Lemma 2,
ğœ†min(ğ‘€ğ‘¡)â‰¥ğœ†min(Ëœğ‘€ğ‘¡)+ğœ†
ğœ…1â‰¥ğœ†ğµğ‘(ğ‘¡)
2+ğœ†
ğœ…1
whenğ‘(ğ‘¡)â‰¥4(ğ¶1âˆš
ğ‘‘+ğ¶2âˆš
log(1/ğ›¿))2
ğœ†2
ğµ.
We have assumed that âˆ¥ğ‘¥ğ‘¡âˆ¥2â‰¤1,âˆ€ğ‘¥ğ‘¡âˆˆAğ‘¡, andâˆ¥ğ‘‘ğ‘¡âˆ¥2=âˆ¥ğ‘¥ğ‘¡âˆ’
ğ‘¥â€²
ğ‘¡âˆ¥2â‰¤âˆ¥ğ‘¥ğ‘¡âˆ¥2+âˆ¥ğ‘¥â€²
ğ‘¡âˆ¥2â‰¤2, where(ğ‘¥ğ‘¡,ğ‘¥â€²
ğ‘¡)denote the contextual
vectors for the selected pair of arms at round ğ‘¡. Therefore we can
obtain the following inequality:
âˆ¥ğ‘‘ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
1
ğœ†min(ğ‘€ğ‘¡)âˆ¥ğ‘‘ğ‘¡âˆ¥2â‰¤2âˆšï¸ƒ
1
ğœ†min(ğ‘€ğ‘¡)â‰¤2(ğœ†ğµğ‘(ğ‘¡)
2+ğœ†
ğœ…1)âˆ’1/2,
as well as:
ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1âˆ¥ğ‘‘ğ‘ âˆ¥ğ‘€âˆ’1ğ‘ â‰¤2ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1âˆšï¸‚1
ğœ†min(ğ‘€ğ‘ )â‰¤2ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1(ğœ†ğµğ‘(ğ‘ )
2+ğœ†
ğœ…1)âˆ’1/2
â‰¤2ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1(ğœ†ğµğ‘(ğ‘ )
2)âˆ’1/2â‰¤2âˆ«ğ‘¡
ğ‘ =ğ‘¡0(ğœ†ğµğ‘(ğ‘ )
2)âˆ’1/2ğ‘‘ğ‘ . (25)
Though the upper bound given in (25)is complicated, when the
conversation frequency function is linear, i.e., ğ‘(ğ‘¡)=ğ‘Â·ğ‘¡for some
randomğ‘âˆˆ(0,1), we can easily calculate the following inequality:
ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1âˆ¥ğ‘‘ğ‘ âˆ¥ğ‘€âˆ’1ğ‘ â‰¤8âˆšï¸‚ğ‘ 
2ğ‘ğœ†ğµğ‘¡
ğ‘ =ğ‘¡0=8(âˆšï¸‚ğ‘¡
2ğ‘ğœ†ğµâˆ’âˆšï¸‚ğ‘¡0
2ğ‘ğœ†ğµ)â‰¤8âˆšï¸‚ğ‘¡
2ğ‘ğœ†ğµ
(26)
â–¡
Proof of Theorem 1. This proof lies in expressing the regret
bound in terms of the above concentration results from Lemma 1,
and it is possible owing to the arm selection strategy which follows
themost informative pair strategy from [ 18]. Suppose we have
selected a pair of arms at round ğ‘¡with contextual vector being
(ğ‘¥ğ‘¡,ğ‘¥â€²
ğ‘¡), and assume ğ‘¥âˆ—
ğ‘¡=arg maxğ‘âˆˆAğ‘¡ğ‘¥ğ‘‡
ğ‘,ğ‘¡ğœƒâˆ—, then we have:
2ğ‘Ÿğ‘¡=(ğ‘¥âˆ—
ğ‘¡ğ‘‡ğœƒâˆ—âˆ’ğ‘¥ğ‘‡
ğ‘¡ğœƒâˆ—)+(ğ‘¥âˆ—
ğ‘¡ğ‘‡ğœƒâˆ—âˆ’ğ‘¥â€²
ğ‘¡ğ‘‡ğœƒâˆ—)
â‰¤
(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡)ğ‘‡ğœƒâˆ—+(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡)ğ‘‡ğœƒâˆ—
=
(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡)ğ‘‡(ğœƒâˆ—âˆ’ğœƒ(1)
ğ‘¡)+(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡)ğ‘‡ğœƒ(1)
ğ‘¡
+(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡)ğ‘‡(ğœƒâˆ—âˆ’ğœƒ(1)
ğ‘¡)+(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡)ğ‘‡ğœƒğ‘¡
â‰¤(1)
ğ›¼ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡+âˆ¥ğœƒâˆ—âˆ’ğœƒ(1)
ğ‘¡âˆ¥ğ‘€ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡
+âˆ¥ğœƒâˆ—âˆ’ğœƒ(1)
ğ‘¡âˆ¥ğ‘€ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡
â‰¤(2)
2ğ›¼ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡+2ğ›¼ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡
â‰¤(3)4ğ›¼ğ‘¡âˆ¥ğ‘¥ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡,
3816KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shuhua Yang et al.
where the first inequality (1) holds due to the construction of
Cğ‘¡and the fact that both ğ‘¥ğ‘¡,ğ‘¥â€²
ğ‘¡âˆˆCğ‘¡, so that(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡)ğ‘‡ğœƒ(1)
ğ‘¡â‰¤
ğ›¼ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡and(ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡)ğ‘‡ğœƒ(1)
ğ‘¡â‰¤ğ›¼ğ‘¡âˆ¥ğ‘¥âˆ—
ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡. Inequality
(2) follows from Lemma 1, where we have proved ğ›¼ğ‘¡is the upper
bound forâˆ¥ğœƒâˆ—âˆ’ğœƒ(1)
ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡. The last inequality comes from the arm
selection strategy.
Denoteğ‘‘ğ‘¡=ğ‘¥ğ‘¡âˆ’ğ‘¥â€²
ğ‘¡, combined with the definition of ğ›¼ğ‘¡, we have
ğ‘Ÿğ‘¡â‰¤4
ğœ…1(ğ‘…âˆšï¸‚
ğ‘‘log((1+4ğœ…1(ğ‘¡+ğ‘(ğ‘¡))
ğ‘‘ğœ†)/ğœ)+âˆšï¸
ğœ†ğœ…1âˆ¥ğœƒâˆ—âˆ¥2)âˆ¥ğ‘‘ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡.
Therefore, the cumulative regret over time ğ‘‡is:
ğ‘…(ğ‘‡)=ğ‘¡0âˆ‘ï¸
ğ‘¡=1ğ‘Ÿğ‘¡+ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘¡0+1ğ‘Ÿğ‘¡â‰¤ğ‘¡0+ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘¡0+1ğ‘Ÿğ‘¡
â‰¤ğ‘¡0+2ğ›¼ğ‘‡ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘¡0+1âˆ¥ğ‘‘ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡â‰¤ğ‘¡0+2ğ›¼ğ‘‡ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1(ğœ†ğµğ‘(ğ‘ )
2)âˆ’1/2
â‰¤ğ‘¡0+2ğ›¼ğ‘‡âˆ«ğ‘¡
ğ‘ =ğ‘¡0(ğœ†ğµğ‘(ğ‘ )
2)âˆ’1/2ğ‘‘ğ‘ . (27)
Whenğ‘(ğ‘¡)=ğ‘Â·ğ‘¡, we have:
ğ‘…(ğ‘‡)â‰¤ğ‘¡0+32
ğœ…1
ğ‘…âˆšï¸‚
ğ‘‘log((1+4ğœ…1(ğ‘‡+ğ‘ğ‘‡)
ğ‘‘ğœ†)/ğœ)
+âˆšï¸
ğœ†ğœ…1âˆ¥ğœƒâˆ—âˆ¥2âˆšï¸„
ğ‘‡
2ğ‘ğœ†ğµ=O(1
ğœ…1âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)). (28)
â–¡
B Intuition for ConDuel-MaxInp Algorithm
We design the ConDuel-MaxInp algorithm based on intuition as
follows:
Denoteğ‘›ğ‘¡=âŒŠğ‘(ğ‘¡)âˆ’ğ‘(ğ‘¡âˆ’1)âŒ‹as the number of conversations
between the agent and the user when ğ‘(ğ‘¡)=1, andğ‘›ğ‘¡=0when
ğ‘(ğ‘¡)=0for the key-term selection module. Based on Eq (8), we
rewriteğ‘€ğ‘¡as:
ğ‘€ğ‘¡=ğ‘¡âˆ‘ï¸
ğ‘ =1ğ‘‘ğ‘¡ğ‘‘ğ‘‡
ğ‘¡+ğ‘¡âˆ‘ï¸
ğ‘ =1ğ‘›ğ‘ âˆ‘ï¸
ğ‘—=1Ëœğ‘‘ğ‘—Ëœğ‘‘ğ‘‡
ğ‘—+ğœ†/ğœ…1ğ¼, (29)
and also define ğ‘€ğ‘¡,ğ‘—=ğ‘€ğ‘¡âˆ’1+ğ‘‘ğ‘¡âˆ’1ğ‘‘ğ‘‡
ğ‘¡âˆ’1+Ãğ‘—âˆ’1
ğ‘–=1Ëœğ‘‘ğ‘–Ëœğ‘‘ğ‘‡
ğ‘–forğ‘—âˆˆ
{1,...,ğ‘›ğ‘¡}whenğ‘›ğ‘¡â‰ 0, then it is easy to obtain the following
equation:
ğ‘‘ğ‘’ğ‘¡(ğ‘€ğ‘¡)=ğ‘‘ğ‘’ğ‘¡(ğ‘€ğ‘¡âˆ’1)(1+âˆ¥ğ‘‘ğ‘¡âˆ’1âˆ¥2
ğ‘€âˆ’1
ğ‘¡âˆ’1)ğ‘›ğ‘¡Ã–
ğ‘—=1(1+âˆ¥Ëœğ‘‘ğ‘—âˆ¥2
ğ‘€âˆ’1
ğ‘¡,ğ‘—)
=ğ‘‘ğ‘’ğ‘¡(ğ‘€0)ğ‘¡âˆ’1Ã–
ğ‘ =1(1+âˆ¥ğ‘‘ğ‘ âˆ¥2
ğ‘€âˆ’1ğ‘ )ğ‘¡Ã–
ğ‘ =1ğ‘›ğ‘ Ã–
ğ‘—=1(1+âˆ¥Ëœğ‘‘ğ‘—âˆ¥2
ğ‘€âˆ’1
ğ‘ ,ğ‘—).
Notice that1
2ğ‘¥â‰¤ğ‘™ğ‘œğ‘”(1+ğ‘¥)â‰¤ğ‘¥forğ‘¥âˆˆ[0,1], we have:
ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1âˆ¥ğ‘‘ğ‘ âˆ¥2
ğ‘€âˆ’1ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1ğ‘›ğ‘ âˆ‘ï¸
ğ‘—=1âˆ¥Ëœğ‘‘ğ‘—âˆ¥2
ğ‘€âˆ’1ğ‘ 
â‰¤ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1âˆ¥ğ‘‘ğ‘ âˆ¥2
ğ‘€âˆ’1ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1ğ‘›ğ‘ âˆ‘ï¸
ğ‘—=1âˆ¥Ëœğ‘‘ğ‘—âˆ¥2
ğ‘€âˆ’1
ğ‘ ,ğ‘—â‰¤2 logğ‘‘ğ‘’ğ‘¡(ğ‘€ğ‘¡)
ğ‘‘ğ‘’ğ‘¡(ğ‘€0)
. (30)Therefore, when applying the â€Maxinpâ€ strategy on selecting key-
terms in the ConDuel-Maxinp algorithm, i.e., choosing key-term
satisfying Ëœğ‘˜ğ‘¡=arg max Ëœğ‘‘âˆˆËœğ·ğ‘¡âˆ¥Ëœğ‘‘âˆ¥2
ğ‘€âˆ’1
ğ‘¡atğ‘¡conversation, the system
carries out explorative conversations and reduces uncertainty on
key-terms.
C Proofs for ConMNL Algorithm
We start by giving the following lemmas to prove Theorem 2.
Lemma 4. ([11], Lemma 9) If ğœ†min(ğ‘€ğ‘‡0)â‰¥ğ‘šğ‘ğ‘¥{ğœ2ğœ…âˆ’2
2(ğ‘‘log(ğ‘(ğ‘‡)+ğ‘‡
ğ‘‘)+
2log(1
ğ›¿),ğ‘}, thenâˆ€ğ‘¡â‰¥ğ‘‡0, with probability at least (1âˆ’ğ›¿), we have
âˆ¥ğœƒğ‘¡âˆ’ğœƒâˆ—âˆ¥â‰¤1.
Lemma 5. Supposeâˆ¥ğœƒğ‘¡âˆ’ğœƒâˆ—âˆ¥â‰¤1forğ‘¡â‰¥ğ‘‡0, then with probability
at least(1âˆ’ğ›¿), we have
âˆ¥ğœƒğ‘¡âˆ’ğœƒâˆ—âˆ¥ğ‘€ğ‘¡â‰¤1
2ğœ…2âˆšï¸‚
2ğ‘‘log(1+ğ‘(ğ‘¡)+ğ‘¡
ğ‘‘)+log(1
ğ›¿)â‰œğ›¼ğ‘¡.(31)
Lemma 6. Following the definition of Barycentric Spanner Bof key-
term set, we assume ğœ†â€²
B=ğœ†min
ğ¸ğ‘˜âˆ¼B[Ëœğ‘¥ğ‘˜Ëœğ‘¥ğ‘‡
ğ‘˜]
>0. Defineğ‘(ğ‘¡)=ğ‘ğ‘¡
forğ‘âˆˆ(0,1), thenâˆ€ğ‘¡â‰¥256
ğ‘ğ‘ğœ†â€²
Blog(128ğ‘‘
ğ›¿ğœ†2
Bâ€²)â‰œğ‘¡0, andğ›¿âˆˆ(0,1/8], with
probability at least (1âˆ’ğ›¿), we have
ğ‘¡âˆ‘ï¸
ğ‘ =ğ‘¡0+1âˆ‘ï¸
ğ‘–âˆˆğ¶ğ‘ âˆ¥ğ‘¥ğ‘–,ğ‘¡âˆ¥ğ‘€âˆ’1ğ‘ â‰¤2âˆšï¸„
2ğ‘ğ‘¡
ğ‘ğœ†Bâ€²
Lemma 7. ([15], lemma 4) With ğ¶âˆ—
ğ‘¡defined as the optimal assort-
ment, andğ¶ğ‘¡=arg maxğ¶âˆˆCË†ğ‘…ğ‘¡(ğ¶,ğ‘¡). Ifğ‘§ğ‘–,ğ‘¡â‰¥ğ‘¥ğ‘‡
ğ‘–,ğ‘¡ğœƒâˆ—for everyğ‘–âˆˆğ¶âˆ—
ğ‘¡,
then we have
ğ‘…ğ‘¡(ğ¶âˆ—
ğ‘¡,ğœƒâˆ—)â‰¤Ë†ğ‘…ğ‘¡(ğ¶âˆ—
ğ‘¡)â‰¤Ë†ğ‘…ğ‘¡(ğ¶ğ‘¡).
Lemma 8. ([15], Lemma 3) With ğ›¼ğ‘¡defined in Lemma 5, suppose
ğ‘§ğ‘,ğ‘¡=ğ‘¥ğ‘‡
ğ‘,ğ‘¡ğœƒğ‘¡+ğ›¼ğ‘¡âˆ¥ğ‘¥ğ‘,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡for allğ‘âˆˆAğ‘¡, then we have
0â‰¤ğ‘§ğ‘,ğ‘¡âˆ’ğ‘¥ğ‘‡
ğ‘,ğ‘¡ğœƒâˆ—â‰¤2ğ›¼ğ‘¡âˆ¥ğ‘¥ğ‘,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡.
Proof of Theorem 2. Whenğ‘‡0=O ğ‘šğ‘ğ‘¥{ğ‘¡0,ğ‘¡1,ğ‘/ğœ2}, with
ğ‘¡0=256
ğ‘ğ‘ğœ†â€²
Blog(128ğ‘‘
ğ›¿ğœ†2
Bâ€²), and1
ğœ…2
2(ğ‘‘log(ğ‘(ğ‘‡)+ğ‘‡
ğ‘‘+4log(ğ‘‡)), we have
âˆ¥ğœƒğ‘¡âˆ’ğœƒâˆ—âˆ¥â‰¤1by Lemma 4, and the regret becomes:
ğ‘…ğ‘‡=ğ‘‡0âˆ‘ï¸
ğ‘¡=1 ğ‘…ğ‘¡(ğ¶âˆ—
ğ‘¡,ğœƒâˆ—)âˆ’ğ‘…ğ‘¡(ğ¶ğ‘¡,ğœƒâˆ—)+ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘‡0+1 ğ‘…ğ‘¡(ğ¶âˆ—
ğ‘¡,ğœƒâˆ—)âˆ’ğ‘…ğ‘¡(ğ¶ğ‘¡,ğœƒâˆ—)
â‰¤ğ‘‡0+ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘‡0+1
Ë†ğ‘…ğ‘¡(ğ¶ğ‘¡)âˆ’ğ‘…ğ‘¡(ğ¶ğ‘¡,ğœƒâˆ—)
+ğ‘‡âˆ‘ï¸
ğ‘¡=1O(ğ‘¡âˆ’2)
â‰¤ğ‘‡0+2ğ›¼ğ‘‡ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘‡0+1âˆ‘ï¸
ğ‘–âˆˆğ¶ğ‘¡âˆ¥ğ‘¥ğ‘–,ğ‘¡âˆ¥ğ‘€âˆ’1
ğ‘¡+O( 1)
â‰¤ğ‘‡0+4ğ›¼ğ‘‡âˆšï¸„
2ğ‘ğ‘¡
ğ‘ğœ†Bâ€²+O( 1) (32)
Combined with Lemma 6 and the definition of ğ›¼ğ‘‡, Theorem 2 is
proved. â–¡
3817