CASA: Clustered Federated Learning with Asynchronous Clients
Boyi Liu
SKLCCSE Lab
Beihang University
Beijing, China
boyliu@buaa.edu.cnYiming Ma
SKLCCSE Lab
Beihang University
Beijing, China
yimingma@buaa.edu.cnZimu Zhou
School of Data Science
City University of Hong Kong
Hong Kong, China
zimuzhou@cityu.edu.hk
Yexuan Shi
SKLCCSE Lab
Beihang University
Beijing, China
skyxuan@buaa.edu.cnShuyuan Li
SKLCCSE Lab
Beihang University
Beijing, China
lishuyuan@buaa.edu.cnYongxin Tong
SKLCCSE Lab
Beihang University
Beijing, China
yxtong@buaa.edu.cn
Abstract
Clustered Federated Learning (CFL) is an emerging paradigm to
extract insights from data on IoT devices. Through iterative client
clustering and model aggregation, CFL adeptly manages data het-
erogeneity, ensures privacy, and delivers personalized models to
heterogeneous devices. Traditional CFL approaches, which operate
synchronously, suffer from prolonged latency for waiting slow de-
vices during clustering and aggregation. This paper advocates a shift
to asynchronous CFL, allowing the server to process client updates
as they arrive. This shift enhances training efficiency yet introduces
complexities to the iterative training cycle. To this end, we present
CASA, a novel CFL scheme for Clustering-Aggregation Synergy un-
der Asynchrony. Built upon a holistic theoretical understanding of
asynchronyâ€™s impact on CFL, CASA adopts a bi-level asynchronous
aggregation method and a buffer-aided dynamic clustering strat-
egy to harmonize between clustering and aggregation. Extensive
evaluations on standard benchmarks show that CASA outperforms
representative baselines in model accuracy and achieves 2.28-6.49Ã—
higher convergence speed.
CCS Concepts
â€¢Computing methodologies â†’Learning paradigms.
Keywords
Clustered Federated Learning; Asynchronous Federated Learning;
Sparse Training
ACM Reference Format:
Boyi Liu, Yiming Ma, Zimu Zhou, Yexuan Shi, Shuyuan Li, and Yongxin Tong.
2024. CASA: Clustered Federated Learning with Asynchronous Clients. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671979
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671979
ğ‘¡!,ğ‘¡",â€¦globaldataâ€¦â€¦ğ‘¡ğ‘–ğ‘šğ‘’=0initialğ‘¡ğ‘–ğ‘šğ‘’=10accurate
straggler(a) Synchronous CFL
â€¦ğ‘¡ğ‘–ğ‘šğ‘’=0initialğ‘¡ğ‘–ğ‘šğ‘’=10mis-clusteringğ‘¡"ğ‘¡!,ğ‘¡#ğ‘¡$,ğ‘¡%ğ‘¡&ğ‘¡'partialdatastragglerâ€¦ (b) Asynchronous CFL
Figure 1: Asynchrony can lead to mis-clustering, which may
further impair federated training of cluster-wise models.
1 Introduction
Clustered federated learning (CFL) is a promising solution to har-
ness the decentralized, heterogeneous data of IoT devices for collec-
tive intelligence. In CFL, devices (clients ) with similar data distribu-
tions are grouped to train cluster-wise models under server coor-
dination, while keeping their datasets localized [ 13,23,26,28,36].
This strategy is particularly effective in IoT applications, where
the data is often heterogeneous yet exhibits natural clusterability
[18,26]. For instance, human activities vary across users but may
share notable spatiotemporal similarities [ 29]. By leveraging the
inherent clusterability of heterogeneous data, CFL not only simpli-
fies operations but also creates accurate, personalized models, with
widespread applicability in smart homes [ 43], mobile healthcare
[26], and intelligent transportation [34].
Despite CFLâ€™s efficacy in handling data heterogeneity, it strug-
gles when confronting system heterogeneity inherent in the diverse
computation and communication capabilities of IoT devices [ 44].
This problem stems from the synchronous operations, where the
server awaits simultaneous updates from all clients for clustering
and training [ 13,23,26,28,36]. Consequently, slow devices, often
termed stragglers, force the server into waiting states, leading to
extended training latency . For example, synchronous aggregation
may take 3-12Ã—longer to reach the target accuracy in presence of
stragglers that are 5Ã—slower than others [19].
A promising solution is to integrate asynchrony into CFL, where
the server processes client updates as they arrive, which have been
adopted in federated training of a single model [ 5,6,12,19,41].
 
1851
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boyi Liu et al.
Although such asynchronous mode eliminates long waiting time, it
may severely undermine the effectiveness of CFL. As illustrated in
Fig. 1, integrating asynchrony means clustering clients with partial
and outdated information, which not only induces clustering errors,
but also deteriorate the training of cluster-wise models. Specifically,
asynchronous CFL faces the following challenges.
â€¢How to guarantee effective CFL with asynchronous clients? CFL
iteratively optimizes client clustering and model aggregation
[13,23,26,28,36], which converges in the synchronous
setting [ 23]. Yet it is unknown whether such convergence
still holds in asynchronous environments.
â€¢How to adapt client clustering and model aggregation for asyn-
chronous CFL? Asynchrony necessitates staleness manage-
ment in both clustering and aggregation. Although staleness
control has been studied in federated learning without clus-
tering, e.g., by decaying outdated updates [ 7,19,27,41], these
designs are inapplicable to CFL due to the extra interplay
between clustering and aggregation.
In this paper, we present CASA (Clustering-Aggregation Synergy
under Asynchrony), a new CFL scheme for asynchronous clients.
We analyze the impact of asynchrony on client clustering and model
aggregation separately, as well as on their interplay, via a unified an-
alytical framework. We further derive the conditions for convergent
CFL in asynchronous environments. On this basis, we develop a
buffer-aided dynamic clustering algorithm and a bi-level asynchro-
nous aggregation scheme for effective and efficient asynchronous
CFL. In addition, we also harness sparse training to actively mitigate
the impact of stragglers. Evaluations on standard benchmarks show
that our methods achieve comparable accuracies to synchronous
CFL methods but converge faster by 2.28-6.49Ã—. Compared with
prior asynchronous schemes, we improve the accuracy by up to
30.34%and are up to 39.11Ã—faster to reach the target accuracy.
Our main contributions are as follows:
â€¢To our knowledge, this is one of the first work on clustered
federated learning in a fully asynchronous context, promot-
ing the CFL deployment to heterogeneous IoT devices.
â€¢We present CASA, which features both theoretical analysis
and practical server-side designs for fast and accurate CFL
in asynchronous environments.
â€¢Evaluations show that CASA is both more accurate and faster
than the state-of-arts, which holds promise to simultaneously
address data and system heterogeneity in federated learning.
2 Related Work
Clustered Federated Learning. CFL alleviates data heterogeneity
in FL by grouping clients with similar data distributions, thereby
enhancing homogeneous learning within clusters [ 13,23,26,28,36].
Due to its simplicity and efficacy, this tactic prevails in personaliza-
tion in federated learning [ 4,35], among other strategies [ 11,33,46].
Research on CFL explores client similarity measures [ 22,26,28] and
clustering algorithms [ 2,18,22,36,42] to improve clustering accu-
racy and enhance its interplay with model training [ 13,23,42]. Com-
mon client similarity metrics include cosine similarity [ 28,36,42],
Euclidean distance [ 2,22], KL divergence [ 26], etc. The clustering
algorithms span from naive k-means [ 10,22,26] to hierarchical
clustering [ 2,18,20,28,42]. For instance, CFL [ 28] bi-partitionsclients into clusters until the training stabilizes. IFCA [ 13] itera-
tively refines the clusters via training loss minimization. ICFL [ 42]
adopts both incremental clustering and spectral clustering to dy-
namically discover the clustering structure. These studies operate
insynchronous settings, leaving the coherence between clustering
and training in the asynchronous context unexplored.
Our work adopts the standard cosine distance of model weights
to measure client similarity [ 28,36,42], and focus on the clustering
algorithms that function under asynchrony. A few proposals ex-
plored the semi-asynchronous case [ 49]. However, a comprehensive
analysis on the interactions between clustering and training in fully
asynchronous environments is missing.
Asynchronous Federated Learning. AFL enhances the efficiency
of FL in face of system heterogeneity. While classic FL relies on syn-
chronous model aggregation, introducing considerable latency due
to delays from stragglers, AFL activates model aggregation upon
receiving client updates, effectively minimizing idle wait times
[5,6,12,19,41]. Semi-asynchronous variants have also been pro-
posed to further reduce the client-server communication overhead
[31,40,49]. However, asynchronous aggregation introduces stale
gradients, which can destabilize learning or even cause model di-
vergence [ 5,19,40,41]. Accordingly, AFL algorithms often decay
stale updates during model aggregation [ 7,19,27,41] to mitigate
their impacts on convergence. For example, FedAsync [ 41] first
introduces the decay function to combat stale updates. FedBuff
[25] buffers recent updates and aggregates the averaged gradients
into global model. PORT [ 30] considers both divergence of model
updates and staleness when updating model parameters. TimelyFL
[47] adjusts the partial training rate to boost slow devices and re-
duce staleness. FedASMU [ 19] distributes the fresh global model
to clients during training to control staleness, accompanied with a
time-aware decay function to ensure convergence.
Our work also focuses on managing staleness, but for CFL rather
than the generic FL that merely trains a single global model. Due
to the unique interplay between clustering and training, we design
a new decay function that benefits clustering and training and
ensures their synergy. We also hitchhike the decay function for
sparse training, which actively controls the impact of stragglers.
3 Problem Statement
Clustered federated learning [ 13,23,26,28,36]is a pivotal solution
to handle non-IID data in federated learning. It groups clients based
on the similarity of their data distribution. Within each cluster,
clients collaboratively learn a shared model while the raw data
remains locally on device.
CFL Workflow. In a typical CFL framework, clients learn cluster-
wise models via bi-level optimization, iteratively minimizing the
clustering error andtraining loss till model convergence [ 22,23,42].
This process operates in three steps per round (see Fig. 2): (i) Local
Training (L-phase), in which clients perform local model training
and upload the local updates to the server; (ii) Client Clustering
(C-phase), where the server groups clients into clusters according
to their uploaded model weights; (iii) Model Aggregation (A-phase),
wherein client model weights are aggregated within each cluster,
followed by sending the updated global (cluster-wise) model to
 
1852CASA: Clustered Federated Learning with Asynchronous Clients KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¦L-phaselocal updateC-phaseclient clusteringA-phase model aggregationğ’•ğ’•âˆ’ğ‰ğ’Šğ’•âˆ’ğ‰ğ’‹
ğ’˜ğ’ˆ(ğ’•)ğ’˜ğ’ˆ(ğ’•%ğŸ)
Figure 2: A typical CFL framework, which consists of lo-
cal update (L-phase), client clustering (C-phase), and model
aggregation (A-phase). Asynchrony induces outdated local
updates to both the C- and A-phases.
clients. Importantly, the workflow is synchronous, where the server
waits updates from all clients for clustering and aggregation.
Formally, assume ğ‘›clients{ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›}with local dataset
{ğ·1,ğ·2,Â·Â·Â·,ğ·ğ‘›}are grouped into ğ¾clusters{ğ‘¢1,ğ‘¢2,Â·Â·Â·,ğ‘¢ğ¾}. All
clients in cluster ğ‘¢ğ‘˜share and collaboratively train a global model
ğ‘¤ğ‘”,ğ‘˜to minimize the following training objectiveP:
minğ‘¤ğ‘”,1,...,ğ‘¤ğ‘”,ğ¾P=ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘ğ‘–âˆˆCğ‘˜|ğ·ğ‘–|
|ğ·|E[L(ğ‘¤ğ‘”,ğ‘˜;ğ·ğ‘)] (1)
whereCğ‘˜denotes the clients in cluster ğ‘¢ğ‘˜. Meanwhile, CFL also
minimizes the following clustering errorH:
minğ‘¤ğ‘”,1,...,ğ‘¤ğ‘”,ğ¾,ğ‘¤1,...,ğ‘¤ğ‘›H=ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘ğ‘–âˆˆCğ‘˜|ğ·ğ‘–|
|ğ·|âˆ¥ğ‘¤ğ‘–âˆ’ğ‘¤ğ‘”,ğ‘˜âˆ¥2
2(2)
whereğ‘¤ğ‘”,ğ‘˜is the global model of cluster ğ‘¢ğ‘˜, andğ‘¤ğ‘–is the local
parameters hold by client ğ‘ğ‘–.
CFL under Asynchrony. As mentioned in Sec. 1, we address
asynchronous CFL to enhance training efficiency amidst stragglers.
Adapting to asynchrony calls for significant modifications to both
clustering and aggregation.
â€¢Client Clustering (C-phase) : Prior CFL methods [ 13,23,26,
28,36] base clustering decisions on full client availability.
The asynchronous clients invalidate this assumption, neces-
sitating new strategies for accurate and timely clustering.
â€¢Model Aggregation (A-phase) : Stale updates from slower clients
hinders the training convergence and accuracy [ 5,19,40,41].
Asynchronous aggregation can manage staleness via weight
decay [ 7,19,27,41]. Yet its effectiveness remains unexplored
when an extra clustering stage is involved.
Assumptions and Scope. We focus on server-side solutions to
minimize interventions to clients. We use FedAvg [ 24] as the basic
optimizer given its pervasive adoption, but our strategies should
also apply to other optimizers e.g., FedProx [ 17]. While privacy is
vital in CFL, our primary goal is to improve training efficiency and
accuracy, keeping privacy at levels akin to FedAvg. Explorations
on specific attacks and defenses are beyond our scope.
4 Method
This section presents CASA (Clustering-Aggregation Synergy un-
derAsynchrony), an asynchronous CFL scheme. We analyze the
A-phasedecay coefficient Î±C-phasedynamic cluster size |ğ’!|high mis-clustering rateextra decay coefficient boundFigure 3: Impact of asynchrony on CFL.
impact of asynchrony on the generic CFL framework (Sec. 4.1), and
propose new designs on model aggregation (Sec. 4.2) and client clus-
tering (Sec. 4.3) to retain their synergy. Finally, we further enhance
CASA by incorporating sparse training (Sec. 4.4).
4.1 Understanding Asynchronous CFL
Although the bi-level optimization for CFL (Sec. 3) converges in the
synchronous setting [ 23], it is unknown whether such clustering-
aggregation synergy holds in asynchronous environments. As de-
picted in Fig. 3, asynchrony affects both the A- and C-phases as well
as their interplay. This necessitates the exploration of new require-
ments on clustering and aggregation to preserve their coherence.
Direct Impact. Asynchrony in CFL brings critical modifications
to both client clustering and model aggregation:
â€¢C-phase : Measuring client similarity becomes complex with
partial client information. In synchronous settings, similar-
ity between clients ğ‘ğ‘–andğ‘ğ‘—in roundğ‘¡might be computed
asğ´ğ‘–ğ‘—=cos(ğ‘¤(ğ‘¡)
ğ‘–,ğ‘¤(ğ‘¡)
ğ‘—). However, with asynchronous up-
dates, this measure shifts to ğ´â€²
ğ‘–ğ‘—=cos(ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–,ğ‘¤(ğ‘¡âˆ’ğœğ‘—)
ğ‘—),
based on model parameters received in different rounds, i.e.,
ğ‘¡âˆ’ğœğ‘–â‰ ğ‘¡âˆ’ğœğ‘—. This leads to a deviation from the actual
similarity of local data distributions between ğ‘ğ‘–andğ‘ğ‘—due
to misaligned global model ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘”,ğ‘˜andğ‘¤(ğ‘¡âˆ’ğœğ‘—)
ğ‘”,ğ‘˜.
â€¢A-phase : In asynchronous aggregation, a decay coefficient ğ›¼
is introduced to manage staleness [ 7,19,27,41]. The aggre-
gation process becomes:
ğ‘¤(ğ‘¡+1)
ğ‘”,ğ‘˜=(1âˆ’ğ›¼)ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜+ğ›¼ğ‘¤(ğ‘¡âˆ’ğœ)
ğ‘–(3)
where the decay coefficient ğ›¼is crucial for the convergence
of model training.
Compound Impact. The changes in individual steps also affect the
bi-level frameworkâ€™s ability to optimize the training and clustering
objectives i.e., Eq. (1) and Eq. (2), as our analysis shows.
Theorem 1. (Clustering Error under Asynchrony). When cluster-
ing relies on a similarity matrix ğ´â€²derived with asynchronous model
parameters, the mis-clustering rate ğ‘is bounded by:
ğ‘=O(ğœ†ğ›¼vutğ‘›âˆ‘ï¸
ğ‘–=1(ğ‘›âˆ‘ï¸
ğ‘—=1âˆ¥ğœğ‘–âˆ’ğœğ‘—âˆ¥2)) (4)
whereğœ†=ğœ‚ğ‘„ğœƒğ‘ˆ , andğœ‚is the learning rate, ğ‘„is the local training
steps,ğ‘ˆis the upper bound of gradient, ğœƒis the upper bound of
staleness (details in Appendix A.1.1).
Theorem 1 depicts the impact of asynchronous aggregation on
clustering accuracy. In the synchronous case, âˆ¥ğœğ‘–âˆ’ğœğ‘—âˆ¥is close to
 
1853KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boyi Liu et al.
zero, resulting in negligible clustering error. Conversely, in asyn-
chronous settings,âˆ¥ğœğ‘–âˆ’ğœğ‘—âˆ¥may be large due to client heterogeneity,
thus impacting clustering accuracy. Also, the mis-clustering rate ğ‘
is upper-bounded by the decay coefficient ğ›¼.
Theorem 2. (Convergence of Training Objective). The training
objectivePdecreases monotonically, and thus the CFL framework
converges under asynchrony, if the following condition is met:
ğ›¼â‰¤Î©(ğ‘¡)â„ğ‘–
|Cğ‘˜|(5)
where|Cğ‘˜|is the size of cluster ğ‘¢ğ‘˜,â„ğ‘–is the computational capacity of
ğ‘ğ‘–, andÎ©(ğ‘¡)is a time-decreasing function (details in Appendix A.1.2).
Theorem 2 implies that clustering affects the design of model
aggregation. Specifically, the decay coefficient ğ›¼should adapt to
the cluster size|Cğ‘˜|. This adjustment is reasonable because larger
clusters often exhibit greater staleness [ 9]. Moreover, Theorem 2
suggests that ğ›¼should be dynamic and client-specific, given that
bothÎ©(ğ‘¡)and|Cğ‘˜|varies over time, and â„ğ‘–differs across clients.
Summary. We make the following observations.
â€¢The CFL framework remains effective under asynchrony
with new requirements on clustering and aggregation.
â€¢Clustering must be adaptable to asynchronous data arrival
and manage mis-clustering effectively.
â€¢Aggregation should account for both global information e.g.,
cluster scale and round, and individual client characteristics
when managing stale weights.
Algorithm 1 illustrates our CASA framework, which follows the
standard L-C-A workflow. Next, we introduce practical designs to
fulfill the new requirements on clustering and aggregation. We start
with aggregation since the clustering algorithm is built upon the
decay coefficient. All the proofs are in Appendix A.1.
4.2 Bi-Level Asynchronous Aggregation
Principles. Stale model updates are often decayed by ğ›¼(see Eq. (3))
during asynchronous aggregation for convergent training of generic
FL [5,19,40,41]. Prior research [ 27,41] mainly sets ğ›¼based on
staleness alone, which is oversimplified for CFL. As highlighted in
Sec. 4.1, the decay coefficient should be configured considering
various cluster- andclient-specific factors to enhance clustering ac-
curacy and training convergence. To this end, we suggest a bi-level
decay coefficient design that strategically separates and manages
the complex dependencies on these factors.
Cluster-Level Decay. Following Theorem 2 but ignoring the client-
specific factors, we set the cluster-wise decay for cluster ğ‘¢ğ‘˜as:
ğ›¼(ğ‘¡)
ğ‘,ğ‘˜=ğ›¼0Î©(ğ‘¡)
log(|Cğ‘˜|)(6)
whereğ›¼0is the initial value of ğ›¼. The rationales are three-fold.
â€¢We expect the cluster-level decay to penalize the average
staleness within the cluster, whereas the stragglers would
receive extra penalty by the client-level decay. The cluster-
level decay also facilitates clustering decisions (see Sec. 4.3).
â€¢From Theorem 2, ğ›¼should be bounded by a time-decreasing
function Î©(ğ‘¡). This is because ğ›¼is bounded by the expecta-
tion of local gradients (details in Appendix A.1.2). The localAlgorithm 1: CASA
Input: Clientsâ€™ model parameters ğ‘¤1,...,ğ‘¤ğ‘›, the single
global model parameters ğ‘¤ğ‘”,1
Output: Clientsâ€™ model parameters ğ‘¤1,...,ğ‘¤ğ‘›, global model
parameters after clustering ğ‘¤ğ‘”,1,...,ğ‘¤ğ‘”,ğ¾
1Server Process:
2forasynchronous round ğ‘¡do
3 Receive update ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–from clientğ‘ğ‘–
4 ifğ‘ğ‘–is new client then
5ğ‘˜â†argmax(ğ‘Ÿğ‘’ğ‘ğ‘˜,ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–)as Eq. (15)
6 Assignğ‘ğ‘–to clusterğ¶ğ‘˜
7 //C-phase
8 Update similarity matrix ğ´and cluster based on
Algorithm 3
9 Update buffer space based on Algorithm 2
10 //A-phase
11 Asynchronous aggregation as Eq. (9)
12Client Process:
13while True do
14 Receive global model ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜
15 //L-phase
16 Local training ğ‘¤(ğ‘¡)
ğ‘–â†ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜âˆ’âˆ‡L(ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜,ğ·ğ‘–)
17 Upload weight ğ‘¤(ğ‘¡)
ğ‘–to server asynchronously
gradients are expected to diminish in convergent training,
which is characterized by the round-decaying function Î©(ğ‘¡).
Motivated by learning rate scheduling [ 45], we instantiate
Î©(ğ‘¡)as an exponential decay. Uniquely, we show that the
decay coefficient ğ›¼should depend on not only the relative
staleness, but also the absolute rounds in CFL.
â€¢From Theorem 2, ğ›¼should decrease as the cluster size |Cğ‘˜|
increases. We set ğ›¼as inversely proportional to log(|Cğ‘˜|)
because directly applying |Cğ‘˜|may makeğ›¼susceptible to
dynamic clustering, which affects training stability.
Client-Level Decay. The client-level decay handles the client-
specific staleness on top of the cluster-level decay. From Theorem 2,
the client-specific staleness is captured by â„ğ‘–, which is the propor-
tion of local updates completed by a client per round. However, it
is difficult for the server to monitor the resources of clients and
accurately estimate â„ğ‘–[31]. Alternatively, we only impose extra
decay when the client staleness ğœğ‘–exceeds a threshold ğ‘Ÿ(ğ‘¡)
ğ‘:
ğ›¼(ğ‘¡)
ğ‘–=(
ğ›¼(ğ‘¡)
ğ‘,ğ‘˜, ifğœğ‘–â‰¤ğ‘Ÿ(ğ‘¡)
ğ‘
ğ›¼(ğ‘¡)
ğ‘,ğ‘˜/âˆšğœğ‘–,ifğœğ‘–>ğ‘Ÿ(ğ‘¡)
ğ‘(7)
Such binary classification of staleness is feasible since our proof
on convergent training only assumes bounded staleness (details in
Appendix A.1.2). Unlike previous work [ 41] that pre-defines the
maximal tolerable staleness, we heuristically set ğ‘Ÿ(ğ‘¡)
ğ‘,ğ‘˜as
ğ‘Ÿ(ğ‘¡)
ğ‘,ğ‘˜=|Cğ‘˜|(2âˆ’Î©(ğ‘¡)). (8)
 
1854CASA: Clustered Federated Learning with Asynchronous Clients KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
uploadğ’ƒğ’–ğ’‡ğ’‹ğ’ƒğ’–ğ’‡ğ’Œğ’ƒğ’–ğ’‡ğ‘µinsertâ€¦ğ‘¨=1â‹¯0.7â‹®â‹±â‹®0.7â‹¯1ğ’ƒğ’–ğ’‡ğ’ŠXlarge gapXrecurringğ‘¤!,ğ‘¤"ğ‘¤#ğ‘¤$,ğ‘¤"ğ’˜ğ’Œğ’ƒğ’–ğ’‡ğ’Š,ğ’˜ğ‘µğ’ƒğ’–ğ’‡ğ’Šupdate bufferupdatesimilarityğ’„ğ’Šğ’˜ğ’Š(ğ’•*ğ‰)
ğ’„ğ’‹â€¦
ğ’„ğ’Œğ’„ğ‘µ
Figure 4: Buffer-aided similarity calculation.
This is because the staleness tends to increase with cluster scale
[9], while its impact on training decreases over rounds [ 38]. Finally,
the traditional asynchronous aggregation, i.e., Eq. (3) becomes:
ğ‘¤(ğ‘¡+1)
ğ‘”,ğ‘˜=ğ›¼(ğ‘¡)
ğ‘–ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–+(1âˆ’ğ›¼(ğ‘¡)
ğ‘–)ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜. (9)
4.3 Buffer-Aided Dynamic Clustering
Principles. Clustering in CFL aims to group clients with similar
local data distributions for effective training of cluster-wise models.
We measure client similarity via the cosine distance of their model
weights, as common in CFL literature [ 2,28,36,42], and focus
on the clustering algorithms suited for the asynchronous environ-
ments. As pointed out in Sec. 4.1, asynchrony leads to misaligned
model weights for similarity calculation, necessitates incremental
clustering with partial information, and enlarges clustering error
due to asynchronous aggregation. To address these challenges, we
propose a buffer-aided similarity calculation scheme, and utilize a
multi-partitioning iterative clustering approach, where the partition-
ing criterion is staleness-dependent. These designs enable timely
and accurate clustering (see case studies in Appendix 5.2.3).
Buffer-Aided Similarity Calculation. From Theorem 1, the clus-
tering error is bounded by the overall misalignment between model
weights in time, i.e.,Ãğ‘›
ğ‘–=1Ãğ‘›
ğ‘—=1âˆ¥ğœğ‘–âˆ’ğœğ‘—âˆ¥2. An intuitive solution is
tobuffer the model weights for each client and calculate the client
similarity using the most aligned model versions rather than the
most recent ones. Specifically, on receiving an update ğ‘¤(ğ‘¡)
ğ‘–from
clientğ‘ğ‘–, we select clients ğ‘ğ‘—,...,ğ‘ğ‘˜whose start time is close to that
ofğ‘ğ‘–, and insertğ‘¤(ğ‘¡)
ğ‘–into their buffer space ğ‘ğ‘¢ğ‘“ğ‘—,...,ğ‘ğ‘¢ğ‘“ğ‘˜. Then we
use model parameters inside buffer ğ‘ğ‘¢ğ‘“ğ‘–for similarity calculation:
ğ´ğ‘–,ğ‘—=cos(ğ‘¤(ğ‘¡)
ğ‘–,ğ‘¤ğ‘ğ‘¢ğ‘“ğ‘–
ğ‘—). (10)
Finally, we clear the buffer space of ğ‘ğ‘–, and wait for new updates.
To support the buffer-aided similarity calculation, one needs to
store allhistorical client updates, which is unscalable in practice.
Accordingly, we optimize the buffer storage below (see Fig. 4).
â€¢Redundant Update Pruning. The server directly discards re-
dundant updates without buffering them. Client ğ‘ğ‘–â€™s model
parametersğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–are considered redundant to client ğ‘ğ‘—in
two cases: (i) large gap :ğ‘¡âˆ’ğœğ‘–is within the range ğ‘Ÿğ‘of client
ğ‘ğ‘—â€™s starting round, i.e.,ğœğ‘—âˆ’ğœğ‘–â‰¥ğ‘Ÿğ‘, since large gap leads to
high clustering error (see Theorem 1). (ii) recurring : bufferAlgorithm 2: Prioritized buffer allocation.
Input: Received client update ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–
Output: New buffer for clients ğ‘ğ‘¢ğ‘“1,...,ğ‘ğ‘¢ğ‘“ğ‘›
1forclientğ‘ğ‘—âˆˆğ¶ğ‘˜do
2 ifğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–not redundant then
3 Addğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–toğ‘ğ‘¢ğ‘“ğ‘—
4ifÃğ‘›
ğ‘–=1|ğ‘ğ‘¢ğ‘“ğ‘–|>ğ‘€then
5 Sort all buffers based on
6 uncomputed or not as the primary key,
7 decay function ğ›¼ğ‘–as the secondary key,
8 Delete lastÃğ‘›
ğ‘–=1|ğ‘ğ‘¢ğ‘“ğ‘–|âˆ’ğ‘€items
ğ‘ğ‘¢ğ‘“ğ‘—already contains client ğ‘ğ‘–â€™s model parameters from a
previous instance.
â€¢Prioritized Buffer Allocation. Given a buffer budget ğ‘€, we
optimize buffer allocation as follows:
maxğ‘›âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğ‘—âˆˆğ‘ğ‘¢ğ‘“ğ‘–1{ğ‘ğ‘ğ‘™ğ‘–,ğ‘—=1}
ğ›¼(ğ‘¡)
ğ‘–,s.t.ğ‘›âˆ‘ï¸
ğ‘–=1|ğ‘ğ‘¢ğ‘“ğ‘–|â‰¤ğ‘€ (11)
where 1{ğ‘ğ‘ğ‘™ğ‘–,ğ‘—=1}indicates whether the similarity between
ğ‘ğ‘–andğ‘ğ‘—has already been computed, and ğ›¼(ğ‘¡)
ğ‘–is the client-
level decay defined in Eq. (7). This is because (i)we need all
the pairwise similarities to make clustering decisions, and
(ii)slower clients are more outdated and have less chances
to update the similarity matrix, thus of higher priority for
updating. Eq. (11) is an online knapsack problem and can
be solved in two steps: (i)Sort clientsâ€™ buffers following the
order of two keywords, i.e., # of similarities not computed,
andğ›¼(ğ‘¡)
ğ‘–.(ii)Greedily select the first ğ‘€buffers. Algorithm 2
illustrates our buffer allocation scheme.
Multi-Partitioning Iterative Clustering. Most CFL methods as-
sume a fixed number of clusters [ 13,22], which is unfit for dynamics
in the asynchronous environments. One solution is to incrementally
adjust the number of clusters via iterative bi-partitioning [ 28,42].
Yet the bi-partitioning can be slow to converge. To boost the clus-
tering efficiency, we apply a multi-partitioning-based approach.
Specifically, we consider clients in a cluster as a graph ğº(ğ‘‰,ğ¸),
with clients as vertex set ğ‘‰and their pairwise similarities as edges
setğ¸. Our goal is to decide whether there is an appropriate gap in the
current graph ğºfor partitioning into ğ‘…subgraphs{ğº1,ğº2,...,ğºğ‘…}.
To estimate the number of subgraphs, we calculate the maximum
eigengap of ğºwith the Laplacian matrix of its similarity matrix ğ´:
ğ¿=ğ¼âˆ’ğ·âˆ’1/2ğ´ğ·âˆ’1/2(12)
Let{ğœ†1,ğœ†2,...,ğœ†ğ‘›}be the eigenvalue of ğ¿. We select the maximum
eigengap to retrieve the best ğ‘…partitions [37].
ğ‘…=arg max
ğ‘˜ğœ†ğ‘˜+1âˆ’ğœ†ğ‘˜,s.t.{ğœ†1,ğœ†2,...,ğœ†ğ‘›}â†ğ‘†ğ‘‰ğ·(ğ¿) (13)
From Theorem 1, we should only perform clustering when the
(cluster-level) decay ğ›¼ğ‘,ğ‘˜is not too large. Accordingly, we partition
clients intoğ‘…clusters only when:
ğ›¼ğ‘,ğ‘˜<(ğœ†ğ‘…+1âˆ’ğœ†ğ‘…)ğ›¾(14)
 
1855KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boyi Liu et al.
Algorithm 3: Multi-partitioning iterative clustering.
Input: Similarity matrix ğ´, cluster-level aggregation
parameterğ›¼(ğ‘¡)
ğ‘,ğ‘˜, received client update ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–
Output: New cluster list ğ‘¢1,...,ğ‘¢ğ¾
1forclientğ‘ğ‘—âˆˆğ‘ğ‘¢ğ‘“ğ‘–do
2 Update similarity matrix ğ´ğ‘–,ğ‘—â†cos(ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–,ğ‘¤ğ‘ğ‘¢ğ‘“ğ‘–
ğ‘—)
3Laplacian matrix ğ¿â†ğ¼âˆ’ğ·âˆ’1/2ğ´ğ·âˆ’1/2
4ğœ†1,ğœ†2,...ğœ†ğ‘›â†ğ‘†ğ‘‰ğ·(ğ¿)
5ğ‘…â†arg maxğ‘˜ğœ†ğ‘˜+1âˆ’ğœ†ğ‘˜
6ifğ›¼(ğ‘¡)
ğ‘,ğ‘˜<(ğœ†ğ‘…+1âˆ’ğœ†ğ‘…)ğ›¾then
7ğ‘¢ğ‘˜1,ğ‘¢ğ‘˜2,...,ğ‘¢ğ‘˜ğ‘…â†ğ‘†ğ‘ğ‘’ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘™ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” (ğ‘¢ğ‘˜,ğ‘…)
8 Replaceğ‘¢ğ‘˜withğ‘¢ğ‘˜1,ğ‘¢ğ‘˜2,...,ğ‘¢ğ‘˜ğ‘…
whereğœ†ğ‘…+1âˆ’ğœ†ğ‘…is the maximum eigengap of similarity matrix ğ´,
andğ›¾is a hyperparameter to scale the eigengap comparable to ğ›¼ğ‘,ğ‘˜.
Algorithm 3 shows the overall clustering workflow.
Newcomer Assignment. Correctly clustering new clients in the
context of asynchronous clustered federated learning presents a
challenging problem. The reasons include:
â€¢Weight gap : The inconsistent update frequency among clients
means that, rather than being assigned to a cluster more
similar in terms of data, a new client might be more easily
assigned to a cluster that has been updated less frequently.
â€¢Center shift : In asynchronous cases, the global model does
not effectively represent the information of all clients within
a cluster, as it is dominated by fast clients.
To address these issues, we leverage historical information to assist
the clustering of newcomers. For each cluster ğ‘¢ğ‘˜, we filter out the
historical information of clients in the newcomerâ€™s sampled buffer
ğ‘ğ‘¢ğ‘“ğ‘–. We aggregate historical information in ğ‘ğ‘¢ğ‘“ğ‘–âˆ©ğ‘¢ğ‘˜as Eq. (15) to
obtain a representative information of the cluster, and then select an
appropriate cluster for the newcomer based on this representation.
ğ‘Ÿğ‘’ğ‘ğ‘˜=âˆ‘ï¸
ğ‘ğ‘—âˆˆğ‘ğ‘¢ğ‘“ğ‘–âˆ©ğ‘¢ğ‘˜|ğ·ğ‘—|
|ğ·ğ‘ğ‘¢ğ‘“ğ‘–âˆ©ğ‘¢ğ‘˜|ğ‘¤ğ‘ğ‘¢ğ‘“ğ‘–
ğ‘—(15)
Discussions. We make the following notes on our buffer-aided
dynamic clustering scheme.
â€¢Buffers have been utilized for stable training of a single
model (without clustering) in asynchronous FL [ 25,31,40].
Our work is orthogonal as we apply buffers in clustering.
Our clustering scheme could be integrated with buffer-based
model aggregation for better training.
â€¢Although our clustering algorithm is built upon prior multi-
partitioning methods, the key novelty is to trigger clustering
via Eq. (14). It allows high clustering accuracy guarantee
with Theorem 1, and is noise-resilient since the maximum
eigengap is typically large [37].
4.4 Mitigating Staleness via Sparse Training
Principles. We further improve CASA by actively mitigating stale-
ness, i.e., allocating lower training workload to slower clients. Thebasic idea is to incorporate sparse training [1,48], where person-
alized masks are assigned to clients to reduce their local training
workload, thus enhancing the stability of the model convergence.
Designs. We implement the idea as CASA+, which has two designs:
Firstly, during asynchronous aggregation, we only mix the masked
weights into the global model:
ğ‘¤(ğ‘¡+1)
ğ‘”,ğ‘˜âŠ™ğ‘š(ğ‘¡âˆ’ğœğ‘–)
ğ‘–=((1âˆ’ğ›¼(ğ‘¡)
ğ‘–)ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜+ğ›¼(ğ‘¡)
ğ‘–ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–)âŠ™ğ‘š(ğ‘¡âˆ’ğœğ‘–)
ğ‘–(16)
whereğ‘š(ğ‘¡âˆ’ğœğ‘–)
ğ‘–is the mask for client ğ‘ğ‘–.ğ‘š(ğ‘¡âˆ’ğœğ‘–)
ğ‘–is obtained by
masking the smallest S(ğ‘¡)
ğ‘–-proportion of local parameters ğ‘¤(ğ‘¡)
ğ‘–.
Eq. (16) limits the influence of stale updates on global model with
aggregating partial parameters, which also boosts convergence.
Secondly, we configure the sparsity rate S(ğ‘¡)
ğ‘–as:
S(ğ‘¡)
ğ‘–=S0
2(ğ›¼(ğ‘¡)
ğ‘,ğ‘˜âˆ’ğ›¼(ğ‘¡)
ğ‘–)(1+cos(((ğ‘¡âˆ’1)ğœ‹
ğ‘…ğ‘’ğ‘›ğ‘‘))) (17)
which applies a cosine annealing function [ 21] to the divergence
of cluster- and client-level decay coefficient, where ğ‘…ğ‘’ğ‘›ğ‘‘is prede-
fined total rounds. This ensures that fast clients are unlikely to be
masked, because local ğ›¼(ğ‘¡)
ğ‘–equals to cluster-level decay. The reason
to control the sparsity rate via the decay coefficient is as follows. (i)
Clients with higher staleness will receive a higher degree of sparsity.
(ii)Sparsification impairs similarity calculation, and sparsity rate
before clustering should be less than that after clustering.
5 Experiments
5.1 Experimental Setup
Compared Methods. We compare CASA with representative CFL
and AFL methods. We also extend the CFL baselines to the asyn-
chronous setting. Specifically, we apply decay function in model
aggregation of these CFL baselines, and buffer the recent model
parameters from each client for similarity calculation. The server
clusters or check clustering condition per asynchronous round.
â€¢Standalone: Each client trains its model with its local dataset
only without federated training.
â€¢FedAvg [24]: generic FL that trains a global model via weighted
averaging of local model parameters.
â€¢FedProx [17]: generic FL that applies a proximal to boost
the convergence.
â€¢FedAsync [41]: classic AFL that aggregates model with
staleness-based weight decay.
â€¢FedBuff [25]: AFL that applies buffer to aggregate the most
recentğ¾gradients.
â€¢CFL [28]: synchronous CFL that adopts bi-partitioning clus-
tering based on mean and maximum norm of gradients.
â€¢CFL-Async: asynchronous extension of CFL [28].
â€¢ICFL [42]: synchronous CFL that adopts bi-partitioning and
incremental clustering.
â€¢ICFL-Async: asynchronous extension of ICFL [42].
â€¢IFCA [13]: synchronous CFL that iteratively clusters clients
based on minimizing loss.
â€¢IFCA-Async: asynchronous extension of IFCA [13].
 
1856CASA: Clustered Federated Learning with Asynchronous Clients KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Overall performance. Accis the overall accuracy at convergence, Time is the time to reach the target accuracy. â€œ/â€
means that the method fails to reach the target accuracy. For IFCA, the item in parentheses means its pre-set cluster number ğ‘˜.
Type MethodMNIST CIFAR-10 FEMNIST IMU HARBox
Acc Time Acc Time Acc Time Acc Time Acc Time
N/A Standalone 98.26 4.9 82.6 35.2 93.95 / 90.00 41.9 69.48 /
SyncFedAvg 97.92 80.24 69.49 / 98.34 85.01 85.71 89.08 82.90 251.42
FedProx 97.31 104.68 65.83 / 98.14 114.34 87.57 96.05 81.80 390.96
CFL 99.42 30.63 90.50 209.55 98.75 75.14 86.29 94.93 85.06 170.99
IFCA(k)99.40(3) 11.21 89.10(3) 209.62 97.77(2) 145.34 94.28(2) 80.73 83.73(2) 334.34
99.50(4) 12.8 90.88(4) 77.88 96.59(5) 237.46 92.67(3) 73.28 85.06(4) 226.53
99.48(5) 5.61 91.14(5) 80.39 95.37(8) 279.85 91.81(4) 148.6 87.09(6) 220.98
ICFL 98.68 12.18 84.19 36.49 95.35 121.65 93.23 30.45 82.58 126.43
AsyncFedAsync 97.46 109.53 67.66 / 96.97 183.57 86.86 64.9 78.72 158.97
FedBuff 99.25 53.31 61.11 / 98.21 82.63 80.00 282.9 81.62 55.7
CFL-Async 99.23 9.54 89.97 145.8 98.68 36.67 87.71 69.3 82.43 59.8
IFCA-Async(k)99.28(3) 9.53 83.41(3) 195.30 98.39(2) 81.07 89.61(2) 143.1 77.58(2) 252.60
98.88(4) 8.83 88.99(4) 80.70 97.78(5) 118.27 85.62(3) 143.1 78.13(4) 215.20
99.32(5) 8.57 87.98(5) 83.20 97.62(8) 126.77 89.14(4) 87.77 76.81(6) 236.30
ICFL-Async 98.82 5 83.30 25.3 94.66 / 91.52 81.83 79.65 92.00
OursCASA 99.52 2.80 91.45 23.4 98.97 36.2 95.33 37.47 87.38 54.8
CASA+ 99.34 4.80 90.64 20.3 98.53 35.03 94.57 22.71 87.21 53.6
0.2 0.4 0.6 0.8 1.0
Normalized Time Cost97.598.098.599.099.5AccuracyMNIST
0.2 0.4 0.6 0.8 1.0
Normalized Time Cost60708090AccuracyCIFAR-10
0.6 0.8 1.0 1.2
Normalized Time Cost949596979899AccuracyFEMNIST
0.5 1.0 1.5 2.0 2.5
Normalized Time Cost808590AccuracyIMU
0.2 0.4 0.6 0.8 1.0
Normalized Time Cost70758085AccuracyHARBoxLocal
FedAvg
FedProx
CFL
ICFL
IFCA
FedAsync
FedBuff
Ours
Figure 5: Model accuracy vs. normalized time cost.
Datasets. We test two tasks. (i)image classification (IC): MNIST
[16], CIFAR10 [15], and FEMNIST [3]; (ii)human activity recogni-
tion (HAR): IMU [26], and HARBox [26].
Metrics. We assess the methods with three metrics: (i) Accuracy :
the accuracy on local datasets when converged; (ii) Time to Target
Accuracy : the latency to reach a given accuracy i.e., 95% for MNIST,
80% for CIFAR-10, 95% for FEMNIST, 80% for IMU, 75% for HARBox;
(iii) Time to Convergence : the time required to convergence.
Other details on experimental setups are in Appendix A.2.5.2 Main Results
5.2.1 Time-to-Accuracy. Table. 1 summarizes the time to reach a
target accuracy and the accuracy at convergence. Fig. 5 highlights
the trade-off between model accuracy and normalized latency (w.r.t.
FedAvg) at convergence. We make the following observations.
â€¢Gains over Synchronous FL & CFL. Synchronous CFL outper-
forms the generic counterpart in both accuracy and latency
in most cases, particularly with label-skew, e.g., on MNIST
and CIFAR-10. We achieve higher accuracy than generic FL,
e.g.FedAvg (0.63% to 21.96%), and FedProx (0.83% to 30.34%),
and comparable accuracy to the CFL baselines, even though
we rely on partial information for clustering. However, our
methods are significantly faster, reducing the time to reach
the target accuracy by 2.00-37.39Ã—on MNIST, 1.79-10.33Ã—on
CIFAR-10, 2.15-7.99Ã—on FEMNIST, 1.34-4.23Ã—on IMU, and
2.36-7.29Ã—on HARBox. From Fig. 5, our CASA reduces up
to 3.12Ã—, 3.40Ã—, 2.28Ã—, 2.41Ã—, 6.49Ã—convergence time while
maintaining a comparable and even higher accuracy than
the synchronous CFL baselines.
â€¢Gains over Asynchronous FL & CFL. AFL methods (without
clustering) suffer from low accuracy and long latency on
highly non-IID data. Concretely, the accuracy of our methods
is2.00-23.79%higher than FedAsync, and 0.27-30.34%higher
than FedBuff. They also take more time to reach the target
accuracy, e.g.up to 39.11Ã—on MNIST, 5.25Ã—on FEMNIST,
12,76Ã—on IMU, and 2.97Ã—on HARBox, and they fail to
reach the target accuracy on CIFAR-10. Our methods are also
faster than the asynchronous CFL baselines, and improves
the accuracy by up to 2.68%, 27.53%, 2.56%, 7.72%, 6.83%at
convergence on the five datasets.
5.2.2 Training Curves. This experiment tracks the training dynam-
ics of asynchronous CFL methods. From Fig. 6, CASA converges
faster and achieves a higher test accuracy than other methods.
IFCA-Async managed to capture the correct clustering structures
on label-skewed datasets like MNIST and CIFAR-10, but fails on
 
1857KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boyi Liu et al.
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
(a) MNIST
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (b) CIFAR-10
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (c) FEMNIST
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (d) IMU
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (e) HARBox
Figure 6: Training time and test accuracy of asynchronous CFL methods.
0 10 20 30 40 50 60 70 80 90
Client id0
10
20
30
40
50
60
70
80
90Client id
0.750.800.850.900.951.00
(a) Sims atğ‘¡=100
0 10 20 30 40 50 60 70 80 90
Client id0
10
20
30
40
50
60
70
80
90Client id
0.600.650.700.750.800.850.900.951.00 (b) Sims atğ‘¡=300
0 50 100 150 200 250 300
Time020406080100Accuracy
 (c) Accuracy at two stages
Figure 7: Analysis of effectiveness of clustering.
feature-skewed datasets like FEMNIST, IMU and HARBox. This is
because IFCA-Async determines cluster identity based on training
loss, yet the difference in loss is small in feature-skewed datasets.
Furthermore, the global model in IFCA-Async can be biased due
to asynchrony (see Fig. 10b). CFL-Async performs well in part of
datasets. This is because ğ›¼affects clustering in the asynchronous
scenarios, and such impact differs across datasets (see Fig. 10a).
5.2.3 Effectiveness of Clustering. This experiment assesses the ef-
fectiveness of CASAâ€™s clustering. We test on CIFAR-10 and ob-
serve the similarity matrix of clientsâ€™ model parameters clients at
different times. The data partitioning strategy is the same as in
Appendix A.2.4, with 4clusters, each holding a few unique labels.
As shown in Fig. 7a, at ğ‘¡=100, a fairly clear clustering structure
has emerged, but the inner relationship within a cluster has not
been discovered yet. As shown in Fig. 7b, by ğ‘¡=300, we further
capture the data distributions within each cluster, resulting in finer-
grained and more accurate clusters.
Fig. 7c further illustrates the accuracy during training. At ğ‘¡=100
when there is clear but not fine-grained clustering relationship, the
overall accuracy reaches 86.5%. With training and mining of clientsâ€™
similarity, the large clusters are partitioned into sub-clusters with
higher cluster similarities. The partitioning boosts the accuracy,
with a final accuracy of 91.6%.
5.2.4 Impact of System Heterogeneity. This experiment tests an-
other two system heterogeneity setups on HARBox. Scenario A: It is
a general case where the maximum speed gap is 5, and the clientsâ€™
local training latencies are randomly sampled from this range [ 19].
Scenario B : It is the case with severe system heterogeneity, with
30%clients taking 10Ã—the training time of fast clients.
Table. 2 summarizes the convergence accuracy and training la-
tency. CASA still outperforms the baselines. Compared with the
general case, CASA achieves a comparable accuracy with merely
1.08Ã—higher latency, when facing severe system heterogeneity.Table 2: Impact of system heterogeneity on HARBox.
MethodScenario A (General) Scenario B (Severe)
Acc Time Acc Time
Standalone 70.47 498.9 70.43 531.2
FedAvg 83.18 1115.93 81.15 2015.19
FedProx 81.32 1190.16 80.19 2056.94
CFL 83.27 977.43 84.17 1614.84
IFCA 84.45 707.24 85.85 1687.42
ICFL 82.09 1019.7 82.12 2095.65
FedAsync 80.23 419.1 79.32 489.3
FedBuff 82.14 400.1 80.32 528.8
Ours 87.61 346.6 87.81 374.6
0 50 100 150 200 250
Time2030405060708090Accuracy
CASA
CASA-D
CASA-C
CASA+
0 50 100 150 200 250 300
Time50607080Accuracy
CASA
CASA-D
CASA-C
CASA+
(a) CIFAR-10
0 50 100 150 200 250
Time2030405060708090Accuracy
CASA
CASA-D
CASA-C
CASA+
0 50 100 150 200 250 300
Time50607080Accuracy
CASA
CASA-D
CASA-C
CASA+ (b) HARBox
Figure 8: Contributions of individual components.
Conversely, other baselines (except standalone, which only trains
locally) are 1.17Ã—to2.39Ã—shower in Scenario B. Thus the results
show that CASA is robust to severe system heterogeneity.
5.3 Ablation Study
5.3.1 Contributions of Individual Components. In addition to CASA
and CASA+, we test the following variants to understand the ef-
fective of different designs: (i)CASA-C, which is CASA without
clustering; and (ii)CASA-D, which is CASA without bi-level ğ›¼.
As shown in Fig. 8, both CASA and CASA+ outperform CASA-C
and CASA-D. Due to lack of clustering, CASA-C fails to generate
personalized models adapted to local data distributions, leading to
lower model accuracy. Without the dynamical decay coefficient,
CASA-D not only induces erroneous clustering but also suffers
from slow convergence, hence exhibiting the poorest performance.
Both CASA and CASA+ outperforms CASA-D in accuracy, e.g.up
to34.23%in CIFAR-10, up to 12.37%in HARBox. CASA-C performs
better than CASA-D, but still worse than CASA and CASA+, with
23.62%and7.74%lower accuracy, respectively.
 
1858CASA: Clustered Federated Learning with Asynchronous Clients KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
5 10 20 50 75 100
Buffer Size020406080Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity05101520Time to T arget Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity020406080Accuracy
(a)
5 10 20 50 75 100
Buffer Size020406080Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity05101520Time to T arget Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity020406080Accuracy (b)
5 10 20 50 75 100
Buffer Size020406080Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity05101520Time to T arget Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity020406080Accuracy (c)
Figure 9: Hyperparameter tests: impact of (a) buffer size on
accuracy; impact of sparsity rate on (b) latency to target ac-
curacy and (c) accuracy at convergence.
5.3.2 Impact of Buffer Size. This experiment shows the effective-
ness of prioritized buffer allocation scheme (Sec. 4.3) by recording
accuracy with 100clients on CIFAR-10 given different buffer sizes.
From Fig. 9a, the accuracy rises to 91.21%with a buffer size of
75. Recall from Table. 1, the highest accuracy of asynchronous CFL
baselines is 89.97%. That is, CASA surpasses other asynchronous
CFL methods even when the buffer size is 75. Note that methods
such as CFL-Async and ICFL-Async need a buffer at least equal
to the client scale i.e.,ğ‘›=100to store the most recent model pa-
rameters of each client and calculate the pair-wise client similarity.
Therefore, our prioritized buffer allocation mechanism yields higher
accuracy at lower storage overhead.
5.3.3 Impact of Sparsity Rate. This experiment shows the impact
of sparsity rate on CASA+ (Sec. 4.4), which mitigates staleness with
sparse training. We measure the latency to the target accuracy and
the accuracy at convergence at various sparsity rate. Note that
S0=0means no sparse training, i.e., CASA.
From Fig. 9b, introducing sparse training with a initial sparse
rateS0=0.3results in 1.09Ã—shorter latency to target accuracy
compared with CASA (the first bar). As we increase the sparsity rate
S0, the reduction in latency of CASA+ is 1.19-1.32Ã—. However, when
the sparsity rate increases, the time required to reach the target
accuracy does not always decrease. This is because an over-sparse
model may not effectively contribute to the global model. With
faster convergence, the accuracy of CASA+ only drops 0.29-1.28%
as shown in Fig. 9c, which is acceptable.
5.3.4 Impact of Asynchronous Aggregation on CFL. This experi-
ment tests the impact of asynchronous aggregation on CFL schemes
to highlight the challenges when shifting to asynchronous CFL. We
select CFL [ 28] and IFCA [ 13], two representative synchronous CFL
methods that adopt a hierarchical and a dynamic clustering strategy,
respectively.
â€¢Impact on Hierarchical Clustering. From Theorem 1, a larger
ğ›¼leads to a higher mis-clustering probability, particularly for
naive methods such as hierarchical clustering. We validate
this by testing CFL-Async on CIFAR-10 under various ğ›¼val-
ues. We measure the clustering performance its purity [ 42]
which stands for the accuracy of clustering and the training
performance by the test accuracy. Fig. 10a shows the results.
Asğ›¼increases, the clustering error occurs. The mis-clustered
clients will then introduce unwanted data heterogeneity into
the cluster, which impairs training accuracy. For example,
0.3 0.5 0.7 0.9
Aggregation Decay 
0.00.20.40.60.81.0Purity
020406080
Accuracy(a) Hierarchical clustering
0 50 100 150 200 250 300
Time2030405060708090Accuracy
IFCA-Async(4)
IFCA(4)
IFCA-Async(3)
IFCA(3) (b) Dynamic clustering
Figure 10: Impact of asynchrony on clustering.
whenğ›¼=0.7, CFL-Async suffers high mis-clustering rate,
with an accuracy drop of 10.09%on CIFAR-10, respectively.
Note that CASA avoid these problems because (i)clustering
is only activated when ğ›¼is small, which explicitly controls
clustering errors, (ii)with buffer limiting ğœğ‘–âˆ’ğœğ‘—, according
to Theorem 1, CASA is more robust with respect to ğ›¼.
â€¢Impact on Dynamic Clustering. Asynchrony makes dynamic
clustering e.g., IFCA sensitive to client arrival orders. This is
because the global model is biased towards the most recent
local updates due to weight decay. This would exert chal-
lenges to assign clients to the correct clusters. To validate
this, we compare IFCA and IFCA-Async on CIFAR-10 with
cluster number set to ğ‘˜=3andğ‘˜=4. As shown in Fig. 10b,
IFCA-Async experiences more unstable convergence than
IFCA. IFCA-Async reaches the target accuracy only 1.09Ã—
faster than IFCA when ğ‘˜=4, and IFCA is even faster than
IFCA-Async when ğ‘˜=3. Therefore, the shift to asynchro-
nous CFL does not necessarily improve the convergence
speed, which motivates our designs.
6 Conclusion
This paper presents CASA, a new CFL scheme for asynchronous
clients to boost training efficiency under both data and system het-
erogeneity. We systematically analyze the impact of asynchrony
on CFL. We further propose a bi-level asynchronous aggregation
method and a buffer-aided dynamic clustering strategy to per-
tain the synergy between client clustering and model aggregation
in asynchronous environments. Extensive evaluations show that
CASA outperforms existing CFL and AFL baselines in accuracy and
achieves 2.28-6.49Ã—faster convergence speed. We envision CASA
as a practical solution for swift and personalized federated learning
with heterogeneous IoT devices.
Acknowledgments
We are grateful to anonymous reviewers for their constructive com-
ments. This work is partially supported by National Science Foun-
dation of China (NSFC) under Grant No. U21A20516 and 62336003,
Beijing Natural Science Foundation No. Z230001, Beihang Univer-
sity Basic Research Funding No. YWF-22-L-531, Didi Collaborative
Research Program NO2231122-00047, and CityU APRC grant No.
9610633. Yongxin Tong is the corresponding author.
 
1859KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boyi Liu et al.
References
[1]Sameer Bibikar, Haris Vikalo, Zhangyang Wang, and Xiaohan Chen. 2022. Feder-
ated dynamic sparse training: Computing less, communicating less, yet learning
better. In Proceedings of AAAI, Vol. 36. 6080â€“6088.
[2]Christopher Briggs, Zhong Fan, and Peter Andras. 2020. Federated learning with
hierarchical clustering of local updates to improve training on non-IID data. In
Proceedings of IJCNN. 1â€“9.
[3]Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub KoneÄn `y,
H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018. Leaf: A
benchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018).
[4]Daoyuan Chen, Dawei Gao, Weirui Kuang, Yaliang Li, and Bolin Ding. 2022.
pFL-bench: A comprehensive benchmark for personalized federated learning.
Advances in Neural Information Processing Systems 35 (2022), 9344â€“9360.
[5]Ming Chen, Bingcheng Mao, and Tianyi Ma. 2019. Efficient and robust asynchro-
nous federated learning with stragglers. In Proceedings of ICLR.
[6]Yujing Chen, Yue Ning, Martin Slawski, and Huzefa Rangwala. 2020. Asynchro-
nous online federated learning for edge devices with non-iid data. In Proceedings
of Big Data. 15â€“24.
[7]Yang Chen, Xiaoyan Sun, and Yaochu Jin. 2019. Communication-efficient feder-
ated deep learning with layerwise asynchronous model update and temporally
weighted aggregation. IEEE Transactions on Neural Networks and Learning Systems
31, 10 (2019), 4229â€“4238.
[8]Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. 2021.
Exploiting shared representations for personalized federated learning. In Pro-
ceedings of ICML. 2089â€“2099.
[9]Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marcâ€™aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al .2012. Large
scale distributed deep networks. Advances in Neural Information Processing
Systems 25 (2012).
[10] Yiqun Diao, Qinbin Li, and Bingsheng He. 2023. Exploiting Label Skews in
Federated Learning with Model Concatenation. arXiv preprint arXiv:2312.06290
(2023).
[11] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized fed-
erated learning with theoretical guarantees: A model-agnostic meta-learning
approach. Advances in Neural Information Processing Systems 33 (2020), 3557â€“
3568.
[12] Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi. 2023. A general
theory for federated optimization with asynchronous and heterogeneous clients
updates. Journal of Machine Learning Research 24, 110 (2023), 1â€“43.
[13] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. 2020. An
efficient framework for clustered federated learning. Advances in Neural Infor-
mation Processing Systems 33 (2020), 19586â€“19597.
[14] Ling Huang, Donghui Yan, Nina Taft, and Michael Jordan. 2008. Spectral cluster-
ing with perturbed data. Advances in Neural Information Processing Systems 21
(2008).
[15] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[16] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278â€“
2324.
[17] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of MLSys 2 (2020), 429â€“450.
[18] Youpeng Li, Xuyu Wang, and Lingling An. 2023. Hierarchical Clustering-based
Personalized Federated Learning for Robust and Fair Human Activity Recognition.
Proceedings of IMWUT 7, 1 (2023), 1â€“38.
[19] Ji Liu, Juncheng Jia, Tianshi Che, Chao Huo, Jiaxiang Ren, Yang Zhou, Huaiyu Dai,
and Dejing Dou. 2023. FedASMU: Efficient Asynchronous Federated Learning
with Dynamic Staleness-aware Model Update. arXiv preprint arXiv:2312.05770
(2023).
[20] Jiachen Liu, Fan Lai, Yinwei Dai, Aditya Akella, Harsha V Madhyastha, and
Mosharaf Chowdhury. 2023. Auxo: Efficient Federated Learning via Scalable
Client Clustering. In Proceedings of SoCC. 125â€“141.
[21] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. 2021.
Do we actually need dense over-parameterization? in-time over-parameterization
in sparse training. In Proceedings of ICML. 6989â€“7000.
[22] Guodong Long, Ming Xie, Tao Shen, Tianyi Zhou, Xianzhi Wang, and Jing Jiang.
2023. Multi-center federated learning: clients clustering for better personalization.
World Wide Web 26, 1 (2023), 481â€“500.
[23] Jie Ma, Guodong Long, Tianyi Zhou, Jing Jiang, and Chengqi Zhang. 2022. On
the convergence of clustered federated learning. arXiv preprint arXiv:2202.06187
(2022).
[24] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Proceedings of AISTATS. 1273â€“1282.
[25] John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rab-
bat, Mani Malek, and Dzmitry Huba. 2022. Federated learning with bufferedasynchronous aggregation. In Proceedings of AISTATS. 3581â€“3607.
[26] Xiaomin Ouyang, Zhiyuan Xie, Jiayu Zhou, Jianwei Huang, and Guoliang Xing.
2021. Clusterfl: a similarity-aware federated learning system for human activity
recognition. In Proceedings of MobiSys. 54â€“66.
[27] Jungwuk Park, Dong-Jun Han, Minseok Choi, and Jaekyun Moon. 2021. Sageflow:
Robust federated learning against both stragglers and adversaries. Advances in
Neural Information Processing Systems 34 (2021), 840â€“851.
[28] Felix Sattler, Klaus-Robert MÃ¼ller, and Wojciech Samek. 2020. Clustered feder-
ated learning: Model-agnostic distributed multitask optimization under privacy
constraints. IEEE Transactions on Neural Networks and Learning Systems 32, 8
(2020), 3710â€“3722.
[29] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,
Mikkel Baun KjÃ¦rgaard, Anind Dey, Tobias Sonne, and Mads MÃ¸ller Jensen.
2015. Smart devices are different: Assessing and mitigating mobile sensing
heterogeneities for activity recognition. In Proceedings of SenSys. 127â€“140.
[30] Ningxin Su and Baochun Li. 2022. How Asynchronous can Federated Learning
Be?. In Proceedings of IWQoS. 1â€“11.
[31] Jingwei Sun, Ang Li, Lin Duan, Samiul Alam, Xuliang Deng, Xin Guo, Haim-
ing Wang, Maria Gorlatova, Mi Zhang, Hai Li, et al .2022. FedSEA: A Semi-
Asynchronous Federated Learning Framework for Extremely Heterogeneous
Devices. In Proceedings of SenSys. 106â€“119.
[32] Ahmet Ali SÃ¼zen, Burhan Duman, and BetÃ¼l Åen. 2020. Benchmark analysis of
jetson tx2, jetson nano and raspberry pi using deep-cnn. In Proceedings of HORA.
1â€“5.
[33] Canh T Dinh, Nguyen Tran, and Josh Nguyen. 2020. Personalized federated
learning with moreau envelopes. Advances in Neural Information Processing
Systems 33 (2020), 21394â€“21405.
[34] Afaf Taik, Zoubeir Mlika, and Soumaya Cherkaoui. 2022. Clustered vehicular
federated learning: Process and optimization. IEEE Transactions on Intelligent
Transportation Systems 23, 12 (2022), 25371â€“25383.
[35] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022. Towards person-
alized federated learning. IEEE Transactions on Neural Networks and Learning
Systems (2022).
[36] Saeed Vahidian, Mahdi Morafah, Weijia Wang, Vyacheslav Kungurtsev, Chen
Chen, Mubarak Shah, and Bill Lin. 2023. Efficient distribution similarity identifi-
cation in clustered federated learning via principal angles between client data
subspaces. In Proceedings of AAAI, Vol. 37. 10043â€“10052.
[37] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
computing 17 (2007), 395â€“416.
[38] Haoming Wang and Wei Gao. 2023. Tackling the Unlimited Staleness in Federated
Learning with Intertwined Data and Device Heterogeneities. arXiv preprint
arXiv:2309.13536 (2023).
[39] Yansheng Wang, Yongxin Tong, Zimu Zhou, Ruisheng Zhang, Sinno Jialin Pan,
Lixin Fan, and Qiang Yang. 2023. Distribution-Regularized Federated Learning
on Non-IID Data. In Proceedings of ICDE. 2113â€“2125.
[40] Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten Maple, and Stephen Jarvis.
2020. SAFA: A semi-asynchronous protocol for fast federated learning with low
overhead. IEEE Trans. Comput. 70, 5 (2020), 655â€“668.
[41] Cong Xie, Sanmi Koyejo, and Indranil Gupta. 2019. Asynchronous federated
optimization. arXiv preprint arXiv:1903.03934 (2019).
[42] Yihan Yan, Xiaojun Tong, and Shen Wang. 2023. Clustered Federated Learning in
Heterogeneous Environment. IEEE Transactions on Neural Networks and Learning
Systems (2023).
[43] Zhikai Yang, Yaping Liu, Shuo Zhang, and Keshen Zhou. 2023. Personalized feder-
ated learning with model interpolation among client clusters and its application
in smart home. World Wide Web (2023), 1â€“26.
[44] Mang Ye, Xiuwen Fang, Bo Du, Pong C Yuen, and Dacheng Tao. 2023. Heteroge-
neous Federated Learning: State-of-the-art and Research Challenges. Comput.
Surveys (2023).
[45] Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I Jordan. 2019.
How does learning rate decay help modern neural networks? arXiv preprint
arXiv:1908.01878 (2019).
[46] Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie
Wu. 2021. Parameterized knowledge transfer for personalized federated learning.
Advances in Neural Information Processing Systems 34 (2021), 10092â€“10104.
[47] Tuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, and Salman Avestimehr. 2023. Time-
lyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive
Partial Training. In Proceedings of CVPR. 5063â€“5072.
[48] Wenhao Zhang, Zimu Zhou, Yansheng Wang, and Yongxin Tong. 2023. Dm-pfl:
Hitchhiking generic federated learning for efficient shift-robust personalization.
InProceedings of SIGKDD. 3396â€“3408.
[49] Yu Zhang, Duo Liu, Moming Duan, Li Li, Xianzhang Chen, Ao Ren, Yujuan Tan,
and Chengliang Wang. 2023. FedMDS: An Efficient Model Discrepancy-Aware
Semi-Asynchronous Clustered Federated Learning Framework. IEEE Transactions
on Parallel and Distributed Systems 34 (2023), 1007â€“1019.
 
1860CASA: Clustered Federated Learning with Asynchronous Clients KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A appendix
A.1 Proofs
A.1.1 Proof of Theorem 1. We make the following assumptions as
in [13, 19, 23, 39, 41].
Assumption 1. (Unbiased gradient estimator and Bounded gradi-
ents). The expectation of stochastic gradient âˆ‡ğ‘™(ğ‘¤ğ‘–,ğœ‰ğ‘–)is an unbiased
estimator of the local gradient for each client, and expectation of L2
norm ofâˆ‡ğ‘™(ğ‘¤ğ‘–,ğœ‰ğ‘–)is bounded by a constant U:
Eğœ‰ğ‘–âˆ¼ğ·ğ‘–[âˆ¥âˆ‡ğ‘™(ğ‘¤ğ‘–,ğœ‰)âˆ¥2]â‰¤ğ‘ˆ (18)
Then we prove Theorem 1, which provides an upper-bound for
the mis-clustering rate ğ‘.
Proof. In the synchronous setting, the similarity between two
clients is calculated as ğ´ğ‘–ğ‘—=cos(ğ‘¤(ğ‘¡)
ğ‘–,ğ‘¤(ğ‘¡)
ğ‘—)in roundğ‘¡. In asyn-
chronous setting, for client ğ‘ğ‘–starting at round ğ‘¡âˆ’ğœğ‘–and client
ğ‘ğ‘—starting at round ğ‘¡âˆ’ğœğ‘—, the similarity calculation becomes
ğ´â€²
ğ‘–ğ‘—=cos(ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–,ğ‘¤(ğ‘¡âˆ’ğœğ‘—)
ğ‘—). The divergence between ğ´ğ‘–ğ‘—andğ´â€²
ğ‘–ğ‘—
is:
ğ´â€²
ğ‘–ğ‘—âˆ’ğ´ğ‘–ğ‘—=â‰¤cos(ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘—,ğ‘¤(ğ‘¡âˆ’ğœğ‘—)
ğ‘—)=O(âˆ¥ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘”âˆ’ğ‘¤(ğ‘¡âˆ’ğœğ‘—)
ğ‘”âˆ¥)
(19)
From [41], the gap of global model parameters is bounded as
âˆ¥ğ‘¤(ğ‘¡)
ğ‘”âˆ’ğ‘¤(ğ‘¡âˆ’ğœ)
ğ‘”âˆ¥â‰¤ğ›¼ğ›¾ğ¾ğ»ğ‘šğ‘ğ‘¥O(ğ‘‰2) (20)
whereğ¾is upper bound of staleness, ğ›¾is learning rate, ğ»ğ‘šğ‘ğ‘¥ is
maximum of local steps, ğ‘‰2is upper bound of gradientsâ€™ norm.
Take Eq. (20) into our problem, we have:
ğ´â€²
ğ‘–ğ‘—âˆ’ğ´ğ‘–ğ‘—=O((ğœğ‘–âˆ’ğœğ‘—)ğ›¼ğœ‚ğ‘„ğœƒğ‘ˆ)=O(ğœ†ğ›¼(ğœğ‘–âˆ’ğœğ‘—)) (21)
whereğ‘„refers to the maximum of local steps, ğœ‚is the learning rate,
andğœƒis the upper bound of staleness. For simplicity, we use ğœ†to
representğœ†=ğœ‚ğ‘„ğœƒğ‘ˆ .
The Frobenius-norm of ğ´â€²âˆ’ğ´can be bounded by Eq. (21):
âˆ¥ğ´â€²âˆ’ğ´âˆ¥ğ¹=vutğ‘›âˆ‘ï¸
ğ‘–=1ğ‘›âˆ‘ï¸
ğ‘—=1âˆ¥ğ´â€²
ğ‘–ğ‘—âˆ’ğ´ğ‘–ğ‘—âˆ¥2=O(ğœ†ğ›¼vutğ‘›âˆ‘ï¸
ğ‘–=1ğ‘›âˆ‘ï¸
ğ‘—=1âˆ¥ğœğ‘–âˆ’ğœğ‘—âˆ¥2)
(22)
Given degree matrix ğ·â€²asğ·â€²
ğ‘–ğ‘–=Ãğ‘›
ğ‘—=1cos(ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–,ğ‘¤(ğ‘¡âˆ’ğœğ‘—)
ğ‘—),
the divergence of Forbenius-norm is
âˆ¥ğ·â€²âˆ’ğ·âˆ¥ğ¹=vtğ‘›âˆ‘ï¸
ğ‘–=1(ğ·â€²
ğ‘–ğ‘–âˆ’ğ·ğ‘–ğ‘–)2=O(ğœ†ğ›¼vutğ‘›âˆ‘ï¸
ğ‘–=1(ğ‘›âˆ‘ï¸
ğ‘—=1âˆ¥ğœğ‘–âˆ’ğœğ‘—âˆ¥2))
(23)
The Frobenius-norm of matrix ğ¿â€²âˆ’ğ¿can be rewritten as âˆ¥ğ´â€²âˆ’ğ´âˆ’
(ğ·â€²âˆ’ğ·)âˆ¥ğ¹. For item that meets ğ‘–â‰ ğ‘—, there isğ´â€²
ğ‘–ğ‘—âˆ’ğ´ğ‘–ğ‘—=ğ¿â€²
ğ‘–ğ‘—âˆ’ğ¿ğ‘–ğ‘—.
The only difference lies in the elements on the diagonal. For an
arbitraryğ‘–, asğ´â€²
ğ‘–ğ‘–âˆ’ğ´ğ‘–ğ‘–=1âˆ’1=0, there exists:
âˆ¥ğ¿â€²âˆ’ğ¿âˆ¥ğ¹=âˆšï¸ƒ
âˆ¥ğ´â€²âˆ’ğ´âˆ¥2
ğ¹+âˆ¥ğ·â€²âˆ’ğ·âˆ¥2
ğ¹=O(ğœ†ğ›¼vutğ‘›âˆ‘ï¸
ğ‘–=1(ğ‘›âˆ‘ï¸
ğ‘—=1âˆ¥ğœğ‘–âˆ’ğœğ‘—âˆ¥2))
(24)As proved in [ 14], the mis-clustering rate ğ‘of the spectral parti-
tioning algorithm satisfies
ğ‘â‰¤ğ›¿2=âˆ¥Ëœv2âˆ’v2âˆ¥2â‰¤4âˆ¥Lâ€²âˆ’Lâˆ¥F
vâˆ’âˆš
2âˆ¥Lâ€²âˆ’Lâˆ¥F(25)
where v2is the unit-length second eignvectors of matrix ğ¿.
Theorem 1 is proved by telescoping all equations above. â–¡
A.1.2 Proof of Theorem 2. For simplicity, we use ğœ†ğ‘–to represent
clientğ‘ğ‘–â€™s weight|ğ·ğ‘–|
|ğ·|, andPğ‘¡,ğ¶,Pğ‘¡,ğ´,Pğ‘¡,ğ¿denote the training
objectivePof C-, A-, L-phase in round ğ‘¡, respectively. To prove the
convergence of CASA, we firstly make assumptions below.
Assumption 2. (Convex). Each loss function ğ‘™orLis convex.
ğ‘™(ğ‘¦)â‰¥ğ‘™(ğ‘¥)+âŸ¨âˆ‡ğ‘™(ğ‘¥),ğ‘¦âˆ’ğ‘¥âŸ© (26)
Assumption 3. (Lipschitz Smooth). Each loss function ğ‘™orLis
ğ›½-smooth.
ğ‘™(ğ‘¦)â‰¤ğ‘™(ğ‘¥)+âŸ¨âˆ‡ğ‘™(ğ‘¥),ğ‘¦âˆ’ğ‘¥âŸ©+ğ›½
2âˆ¥ğ‘¦âˆ’ğ‘¥âˆ¥2
2(27)
Assumption 4. (Bounded gradient variance). The variance of
stochastic gradientâˆ‡ğ‘™(ğ‘¤ğ‘–,ğœ‰ğ‘–)is bounded by ğœ2,
Eğœ‰ğ‘–âˆ¼ğ·ğ‘–[âˆ¥âˆ‡ğ‘™(ğ‘¤ğ‘–,ğœ‰ğ‘–)âˆ’âˆ‡ğ‘™(ğ‘¤ğ‘–)âˆ¥2
2]=E[âˆ¥âˆ‡ğ‘™(ğ‘¤ğ‘–,ğœ‰ğ‘–)âˆ¥2
2]âˆ’âˆ¥âˆ‡ğ‘™(ğ‘¤ğ‘–)âˆ¥2
2â‰¤ğœ2
(28)
Assumption 5. (Bounded staleness). At asynchronous training
round t, the server received an update ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–from clientğ‘ğ‘–. The
update staleness is bounded, meeting ğœğ‘–â‰¤ğœƒ.
Lemma 1. From C-phase to A-phase in an arbitrary round ğ‘¡, we
have
Pğ‘¡,ğ´â‰¤Pğ‘¡,ğ¶+ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘ğ‘–âˆˆğ‘¢ğ‘˜ğœ†ğ‘–ğœ‚(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğ‘„ğ‘ˆ(ğ›¼(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğœƒO(ğ‘ˆ)+ğ‘ˆ)(29)
Proof. The distance between cluster ğ‘¢ğ‘˜â€™s global model and re-
ceived parameters from client ğ‘ğ‘–should be:
âˆ¥ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜âˆ’ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–âˆ¥2â‰¤âˆ¥ğ‘¤(ğ‘¡)
ğ‘”,ğ‘˜âˆ’ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘”,ğ‘˜âˆ¥2+âˆ¥ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘”,ğ‘˜âˆ’ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–âˆ¥2
â‰¤ğ›¼(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğœ‚(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğœƒğ‘„O(ğ‘ˆ)+ğœ‚(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğ‘„ğ‘ˆ
(30)
For an arbitrary cluster ğ‘¢ğ‘˜, we have:
âˆ‘ï¸
ğ‘ğ‘–âˆˆğ‘¢ğ‘˜I(ğ‘ğ‘’ğ‘›ğ‘‘
ğ‘–=ğ‘„)ğœ†ğ‘–(L(ğ‘¤(ğ‘¡+1)
ğ‘”,ğ‘˜,ğ·ğ‘–)âˆ’L(ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–,ğ·ğ‘–))
â‰¤âˆ‘ï¸
ğ‘ğ‘–âˆˆğ‘¢ğ‘˜I(ğ‘ğ‘’ğ‘›ğ‘‘
ğ‘–=ğ‘„)ğœ†ğ‘–(âŸ¨âˆ‡L(ğ‘¤(ğ‘¡+1)
ğ‘”,ğ‘˜,ğ·ğ‘–),ğ‘¤(ğ‘¡+1)
ğ‘”,ğ‘˜âˆ’ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–âŸ©)
â‰¤âˆ‘ï¸
ğ‘ğ‘–âˆˆğ‘¢ğ‘˜ğœ†ğ‘–ğœ‚(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğ‘„ğ‘ˆ(ğ›¼(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğœƒO(ğ‘ˆ)+ğ‘ˆ)(31)
Lemma 1 is satisfied with
Pğ‘¡,ğ´âˆ’Pğ‘¡,ğ¶=ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘ğ‘–âˆˆğ‘¢ğ‘˜I(ğ‘ğ‘’ğ‘›ğ‘‘
ğ‘–=ğ‘„)ğœ†ğ‘–(L(ğ‘¤(ğ‘¡+1)
ğ‘”,ğ‘˜,ğ·ğ‘–)âˆ’L(ğ‘¤(ğ‘¡âˆ’ğœğ‘–)
ğ‘–,ğ·ğ‘–))
(32)
â–¡
 
1861KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boyi Liu et al.
Lemma 2. From the A-phase to L-phase in arbitrary communica-
tion round, we have:
E[Pğ‘¡,ğ¿]âˆ’Pğ‘¡,ğ´
â‰¤ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘ğ‘–âˆˆğ‘¢ğ‘˜ğœ†ğ‘–ğ‘„âˆ’1âˆ‘ï¸
ğ‘=0((ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2âˆ’ğœ‚(ğ‘¡)
ğ‘–)E[âˆ¥âˆ‡L(ğ‘¤ğ‘
ğ‘–)âˆ¥2
2]+ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2ğœ2)
(33)
Proof. For clientğ‘ğ‘–in local training, from step ğ‘to stepğ‘+1:
L(ğ‘¤ğ‘+1
ğ‘–)âˆ’L(ğ‘¤ğ‘
ğ‘–)â‰¤âŸ¨âˆ‡L(ğ‘¤ğ‘
ğ‘–),ğ‘¤ğ‘+1
ğ‘–âˆ’ğ‘¤ğ‘
ğ‘–âŸ©+ğ›½
2âˆ¥ğ‘¤ğ‘+1
ğ‘–âˆ’ğ‘¤ğ‘
ğ‘–âˆ¥2
2
=âˆ’ğœ‚(ğ‘¡)
ğ‘–âŸ¨âˆ‡L(ğ‘¤ğ‘
ğ‘–),âˆ‡L(ğ‘¤ğ‘
ğ‘–,ğœ‰ğ‘
ğ‘–)âŸ©+ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2âˆ¥âˆ‡L(ğ‘¤ğ‘
ğ‘–,ğœ‰ğ‘
ğ‘–)âˆ¥2
2
(34)
Take expectation on both sides for a random batch ğœ‰ğ‘
ğ‘–at stepğ‘,
E[L(ğ‘¤ğ‘+1
ğ‘–)]âˆ’L(ğ‘¤ğ‘
ğ‘–)â‰¤(ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2âˆ’ğœ‚(ğ‘¡)
ğ‘–)âˆ¥âˆ‡L(ğ‘¤ğ‘
ğ‘–)âˆ¥2
2+ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2ğœ2
(35)
For an arbitrary client ğ‘ğ‘–starting L-phase at step ğ‘ğ‘–, and ends at
stepğ‘â€²
ğ‘–this round, we telescope them and get:
E[L(ğ‘¤ğ‘â€²
ğ‘–
ğ‘–)]âˆ’L(ğ‘¤ğ‘ğ‘–
ğ‘–)â‰¤ğ‘â€²
ğ‘–âˆ‘ï¸
ğ‘=ğ‘ğ‘–((ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2âˆ’ğœ‚(ğ‘¡)
ğ‘–)âˆ¥âˆ‡L(ğ‘¤ğ‘
ğ‘–)âˆ¥2
2+ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2ğœ2)
(36)
Lemma 2 is satisfied telescoping all clients. â–¡
Now we start to prove Theorem 2.
Proof. From L-phase in asynchronous round ğ‘¡âˆ’1to C-phase
in asynchronous round ğ‘¡âˆ’1, the overall loss does not change, for
the model parameters does not change, so:
Pğ‘¡âˆ’1,ğ¿=Pğ‘¡,ğ¶(37)
According to Lemma 1 and Lemma 2 we can get:
E[Pğ‘¡,ğ¿]âˆ’Pğ‘¡âˆ’1,ğ¿â‰¤ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘ğ‘–âˆˆğ‘¢ğ‘˜ğœ†ğ‘–ğ‘â€²
ğ‘–âˆ‘ï¸
ğ‘=ğ‘ğ‘–(ğœ‚(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğ‘„ğ‘ˆ(ğ›¼(ğ‘¡âˆ’ğœğ‘–)
ğ‘–ğœƒO(ğ‘ˆ)+ğ‘ˆ)
â„ğ‘›
ğ‘–
+(ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2âˆ’ğœ‚ğ‘–)E[âˆ¥âˆ‡L(ğ‘¤ğ‘
ğ‘–)âˆ¥2
2]+ğ›½(ğœ‚(ğ‘¡)
ğ‘–)2
2ğœ2)
(38)
Givenâ„ğ‘–=ğ‘â€²
ğ‘–âˆ’ğ‘ğ‘–
ğ‘„, the right side of Eq. (38) is always negative
when
ğ›¼(ğ‘¡)
ğ‘¡â‰¤(2âˆ’ğ›½ğœ‚(ğ‘¡)
ğ‘–)E[âˆ¥âˆ‡L(ğ‘¤ğ‘
ğ‘–)âˆ¥2
2]âˆ’ğ›½ğœ‚(ğ‘¡)
ğ‘–ğœ2
2ğœƒğ‘„ğ‘ˆO(ğ‘ˆ)Â·â„ğ‘–âˆ’ğ‘ˆ
ğœƒO(ğ‘ˆ)(39)
In the ideal case, the expectation of local gradient E[âˆ¥âˆ‡L(ğ‘¤ğ‘
ğ‘–)âˆ¥2
2]
gradually decreases with time, which can be represented by a time-
decay function. As the upper bound of staleness ğœƒis related to the
cluster size in an asynchronous scenario [ 9], for simplicity, we can
rewrite Eq. (39) as follows:
ğ›¼(ğ‘¡)
ğ‘–â‰¤Î©(ğ‘¡)â„ğ‘–
|Cğ‘˜|(40)
where|Cğ‘˜|refers to the scale cluster ğ‘¢ğ‘˜,â„ğ‘–indicates the system
resource of device ğ‘–, andÎ©(ğ‘¡)is a function that monotonically
decreases over time. â–¡A.2 Additional Experimental Settings
A.2.1 Experimental Environment. We conduct experiments on a
machine with AMD Ryzen 9 5950X 16-Core Processor CPU, and a
NVIDIA GeForce RTX 3090 GPU. The code are implemented with
Python 3.9.13 and PyTorch 1.13.1.
A.2.2 Configuration of Federation. We set client number ğ‘›=100
for MNIST, CIFAR-10, FEMNIST, HARBox, and ğ‘›=7for IMU. For
FEMNIST, we select the top 100clients with the largest size of local
data. For HARBox, we select the first 100clients from the 120clients.
We apply a CNN to MNIST, CIFAR-10 and FEMNIST as [ 8]; and a
two-layer fully connected network for IMU and HARBox as [26].
The wall-clock time is simulated following [ 30]. By default, we
set the local training latency of slow devices as 5Ã—that of normal
devices, with 30% slow devices among all clients. This is because
the speed of mainstream IoT platforms may differ by 5Ã—,e.g., be-
tween Raspberry Pi and NVIDIA Nano [ 32]. Since the concept of
round differs in synchronous and asynchronous settings, we train
for500rounds in synchronous settings, and for 50,000rounds in
asynchronous settings.
A.2.3 Hyperparameters. For all the datasets, we set the batch size
to10, and the learning rate ğœ‚to0.01.
For all asynchronous methods, the basic decay coefficient is set
asğ›¼=0.3. Other method-specific hyperparameters are set as below.
â€¢FedProx [17]: proximal regularization term ğœ‡=0.05.
â€¢FedAsync [41]: hinge version, with ğ‘=1andğ‘=4.
â€¢FedBuff [25]: buffer size ğ¾=10.
â€¢CFL [ 28]: mean gradient bound ğœ€1=0.4; max gradient bound
ğœ€2=0.7.
â€¢ICFL [ 42]: start value ğ›¼âˆ—(0)=0.85, increasing factor ğœ€=4.0.
â€¢CFL-Async: the max gradient bound is set higher than the
synchronous version to avoid frequent triggering of the clus-
tering condition. We set ğœ€2=1.
â€¢ICFL-Async: same as ICFL, except that the time decay rate
becomesÃ—0.01due to the asynchrony nature.
For our CASA, the basic decay coefficient is set ğ›¼0=2, and the
time function is set as Î©(ğ‘¡)=(ğ‘’
2.8)ğ‘˜ğ‘¡, withğ‘˜set to 0.001for most
cases,ğ›¾=0.15. To ensure ğ›¼(ğ‘¡)
ğ‘–is strictly less than 1,|Cğ‘˜|is actually
set to|Cğ‘˜|+3. And to ensure clustering quality, we select the first
ğ‘›=10eigenvalues, and calculate eigengap from them.
A.2.4 Configuration of Datasets. We experiment with two types
of clustering relationships:
â€¢Feature-skew based. We select one IC dataset FEMNIST [ 3]
and two HAR datasets IMU and HARBox [ 26] as the feature-
skew case. These datasets possess realistic feature correla-
tions, such as the writing habits in FEMNIST and the different
activity postures in HAR.
â€¢Label-skew based. We partition two IC datasets MNIST [16]
and CIFAR-10 [ 15] with labels manually. We separate ğ‘›=100
clients into 4 groups, with a proportion of {0.2,0.2,0.3,0.3}
following [ 42]. The labels are split following the same pro-
portion. For instance, label 0 and label 1 are only held by
group 0. To simulate the non-IID setting, we set Dirichlet
distribution with ğ›¼=1inside each cluster.
 
1862