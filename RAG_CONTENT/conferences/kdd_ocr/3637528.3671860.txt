Flexible Graph Neural Diffusion with Latent Class Representation
Learning
Liangtian Wan
Huijin Han
Key Laboratory for Ubiquitous
Network and Service Software of
Liaoning Province, School of Software,
Dalian University of Technology
Dalian, China
wanliangtian@dlut.edu.cn
hanhuijin2022@163.comLu Sun
Department of Communication
Engineering, Institute of Information
Science Technology, Dalian Maritime
University
Dalian, China
sunlu@dlmu.edu.cnZixun Zhang
School of Science and Engineering,
The Chinese University of Hong Kong
Shenzhen, China
zixunzhang@link.cuhk.edu.cn
Zhaolong Ning
School of Communication and
Information Engineering, Chongqing
University of Posts and
Telecommunications
Chongqing, China
z.ning@ieee.orgXiaoran Yanâˆ—
Research Center for Data Hub and
Security, Zhejiang Lab
Hangzhou, China
Xiaoran.a.yan@gmail.comFeng Xia
School of Computing Technologies,
RMIT University
Melbourne, Australia
f.xia@ieee.org
Abstract
In existing graph data, the connection relationships often exhibit
uniform weights, leading to the model aggregating neighboring
nodes with equal weights across various connection types. How-
ever, this uniform aggregation of diverse information diminishes
the discriminability of node representations, contributing signif-
icantly to the over-smoothing issue in models. In this paper, we
propose the Flexible Graph Neural Diffusion (FGND) model, incor-
porating latent class representation to address the misalignment
between graph topology and node features. In particular, we com-
bine latent class representation learning with the inherent graph
topology to reconstruct the diffusion matrix during the graph diffu-
sion process. We introduce the simmetric to quantify the degree of
mismatch between graph topology and node features. By flexibly
adjusting the dependency level on node features through the hyper-
parameter, we accommodate diverse adjacency relationships. The
effective filtering of noise in the topology also allows the model
to capture higher order information, significantly alleviating the
over-smoothing problem. Meanwhile, we model the graphical diffu-
sion process as a set of differential equations and employ advanced
partial differential equation tools to obtain more accurate solutions.
Empirical evaluations on five benchmarks reveal that our FGND
model outperforms existing popular GNN methods in terms of both
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671860overall performance and stability under data perturbations. Mean-
while, our model exhibits superior performance in comparison to
models tailored for heterogeneous graphs and those designed to
address oversmoothing issues.
CCS Concepts
â€¢Computing methodologies â†’Learning latent representa-
tions; â€¢Theory of computation â†’Graph algorithms analysis;
â€¢Information systems â†’Social tagging .
Keywords
Graph learning; Graph convolution neural network; Latent class;
Representation learning; Denoising filters
ACM Reference Format:
Liangtian Wan, Huijin Han, Lu Sun, Zixun Zhang, Zhaolong Ning, Xiaoran
Yan, and Feng Xia. 2024. Flexible Graph Neural Diffusion with Latent Class
Representation Learning. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671860
1 Introduction
Graphs, as a versatile data representation, find widespread applica-
tions in many web applications such as social networks, semantic
networks, web search, citation networks, and recommendation sys-
tems [ 21,27]. These web applications not only possess graph struc-
tures but also exhibit diverse individual attribute features. Graph
Neural Networks (GNNs) have emerged as a powerful framework
for handling data that not only includes node features but also en-
codes topological relationships among nodes, making them suitable
for a variety of real-world problems. Due to the sparsity of labeled
data points in graphs, semi-supervised learning on graphs has been
extensively studied for several decades [ 3,7,14,17,23,26,28]. In
 
2936
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liangtian Wan et al.
this paper, we focus on addressing the semi-supervised learning
task of node classification on graphs.
(a)simandğ›½in Action: Experi-
ment on Pubmed Dataset
(b) Numerical Methods Impact:
Experiment on Cora Dataset
Figure 1: Exploring Model Performance Enhancements: sim
andğ›½& Numerical Methods
Partial Differential Equations (PDEs) are invaluable mathemati-
cal tools widely applied in signal and image processing [ 18], com-
puter graphics [ 5], and machine learning [ 12,24,31]. Diffusion-
based GNNs have close ties with PDEs, as mainstream message-
passing paradigms can be modeled as differential equations. Recent
diffusion-based works [ 3,6,23] have demonstrated that tools from
PDEs can be employed to comprehend existing GNN architectures.
Experimental evidence indicates that modeling the information
propagation process as a diffusion process significantly enhances
the performance of the model compared to traditional spatial-
domain GNN methods. Building upon GRAND [ 3], GRAND++ [ 23]
incorporates a source term into the diffusion process to enhance
the modelâ€™s robustness. On the other hand, GREAD [ 6] further re-
fines the reaction-diffusion equation to improve model performance.
These diffusion-based models further enhance the modelâ€™s ability
to counteract over-smoothing. However, these improvements do
not alter the connectivity relationships in the graph topology at the
data level, still relying on the original node-to-node relationships.
In real-world data, graph edges often contain noise [ 13], and
there exist disparities between node features and graph structure
[4]. Noisy connections can degrade the representation of node
features after aggregating neighbor information. For instance, in
datasets like Amazon â€˜Computersâ€™ and â€˜Photoâ€™, the graph topology
is determined by co-occurrence relationships (products appearing
in the same user reviews or simultaneously appearing in purchase
histories) and user behaviors (user purchase and review activities).
In these topologies, co-occurrence relationships and user behaviors
can generate connection relationships that are irrelevant to product
recommendations themselves. Additionally, the connection weights
between different relationships should be distinct. However, in
actual datasets, these connection relationships are assigned uniform
weights. Excessive noise connections can lead to a decrease in
the discriminability of node hidden layer representations when
aggregating neighbor information. Simultaneously, it is a key factor
contributing to the occurrence of oversmoothing issues in the model.
Therefore, addressing the mismatch between node features and
graph structure characteristics for different real datasets and flexibly
handling the problem is crucial for improving model prediction
accuracy and combating oversmoothing.In this paper, we introduce the Flexible Graph Neural Diffu-
sion (FGND) model. We reconstruct the adjacency matrix used in
the graph diffusion process by considering both the hidden layer
representations of nodes and the original adjacency relationships.
A metric simthat quantifies the severity of the mismatch is in-
troduced and the relationship matrix between each node and its
neighbouring nodes is adaptively adjusted by the parameter ğ›½that
allows the model to adapt to different datasets and task require-
ments by controlling its dependence on the similarity measure
during the information aggregation process. This refinement aims
to strike a better balance between graph topology and node feature
relationships, effectively filtering out noise in the graphâ€™s adja-
cency relationships. The filtering of noise in graph adjacencies also
strengthens the model against transition smoothing when aggre-
gating higher-order information. Figure 1 provides an overview
of our investigation into model performance improvements, cov-
ering the utilization of simandğ›½, along with the exploration of
various numerical methods in the context of graph diffusion. The
contributions of this paper can be summarized as follows:
â€¢Unified Treatment for Graph Topology and Node Fea-
tures: We propose the FGND model to address the mismatch
between graph topology and node features, thereby im-
proving the modelâ€™s predictive accuracy in semi-supervised
node classification problems and its ability to counteract
over-smoothing. By incorporating latent class representa-
tion learning, we achieve a unified treatment of node features
and graph topology, offering a more effective solution to the
noise issues in graph topology structures.
â€¢Enhancing Model Flexibility and Inter-Node Relation-
ship Reconstruction: The introduction of the simmetric
quantifies the severity of mismatch between graph topology
and node features, combined with the parameter ğ›½to pro-
vide the model with greater flexibility. Adjustments are made
flexibly to different inter-dataset relationships in real-world
datasets, facilitating a more effective and rational reconstruc-
tion of inter-node relationships.
â€¢Exploration of Numerical Methods: We investigate the
application of high-precision, high-order numerical solving
methods, as well as the utilization of implicit methods with
enhanced stability for diffusion step sizes, in the process of
solving graph diffusion equations.
â€¢Empirical Flexibility and Superior Performance: Empir-
ically, we demonstrate the flexibility of the proposed model,
allowing adaptation to different datasets by adjusting its
reliance on latent class representational learning. Our ex-
periments show that FGND consistently outperforms the
state of art baselines with a significant margin in semi-
supervised node classification problems. Simultaneously, our
model demonstrates superior performance in addressing is-
sues related to heterogeneous graph data and countering
over-smoothing problems.
The outline of this paper is presented as follows. In Section 2, we
provide a comprehensive review of the existing research on GNNs.
Section 3 systematically introduces our novel algorithm. Section
4 encompasses an analysis of the key parameter ğ›½, which allows
flexible adjustment of our model to achieve the integration of node
 
2937Flexible Graph Neural Diffusion with Latent Class Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
features with graph topology across different datasets. Addition-
ally, it delves into the mitigation of oversmoothing, assessing the
effectiveness of the FGND model. We also compare our algorithm
with popular GNN models. Finally, we conclude with a discussion
and outline potential avenues for future research in Section 5.
2 Related Work
2.1 Spectral Domain GNNs
Spectral GNN [ 2] extends the concept of convolutional neural net-
works (CNNs) to non-Euclidean spaces through Fourier transform.
ChebyNet [ 7] simplifies the convolution process in graph convo-
lutions by incorporating convolution kernels into the convolution
theorem, eliminating the requirement for the Laplacian matrix
decomposition step. This simplification makes convolution only
dependent on polynomial functions of the Laplacian matrix, the
polynomial coefficient and its order, offering the potentiality for
practical spectral methods. GCN [ 14] builds upon ChebyNet by
further limiting the number of parameters and employing only
first-order approximations, thereby reducing the risk of model over-
smothong. GWNN [ 29] leverages the sparsity and localization prop-
erties of graph wavelets in the vertex domain, enhancing model per-
formance by replacing Fourier transform with wavelet transform.
Additionally, researchers such as Wan et al. [ 26], have explored
the use of wavelet transform as denoising filters. By leveraging
the inherent locality and sparsity of wavelets, they filter noise in
the graphâ€™s topological structure, thereby improving model perfor-
mance. In recent years, the effectiveness of spectral domain graph
neural networks has shown some disparity compared to spatial do-
main GNNs, resulting in fewer model contributions in the spectral
domain.
2.2 Spatial Domain GNNs
In the realm of spatial domain methods, DCNN [ 1] takes the inspi-
ration from breadth-first traversal and aggregates the information
fromğ‘˜-hop neighbors separately for each node. This aggregation
involves a parameter block, denoted as ğ‘Š, corresponding to each
hopâ€™s aggregated information. On the other hand, GraphSage [ 11]
focuses on addressing the challenge of aggregating information
from the local neighborhoods of nodes, offering a solution for im-
plementing GNN in large-scale graphs.
GAT [ 25] introduces attention mechanisms into the GNN model.
By stacking layers where nodes can attend to their neighborhoodsâ€™
features, it enables the implicit assignment of varying weights to
different nodes within a neighborhood. This is achieved without
the requirement for computationally expensive matrix operations
or prior knowledge of the graph structure.
Furthermore, MoNet [ 19] formulates a versatile spatial domain
framework by defining pseudo-coordinates denoted as â€˜uâ€™ and
weight functions â€˜w(u)â€™. This framework is designed for learning
in non-Euclidean domains, offering a solution beyond traditional
Euclidean spaces. Regarding Spatial Domain GNN, a significant re-
search direction is the development of GNNs based on diffusion. In
the diffusion based GNN subsection, we provide a detailed overview
of the latest research advancements in diffusion-based GNNs and
discuss the strengths and weaknesses in comparison to the model
proposed in our study.2.3 Diffusion Based GNNs
In the realm of graph diffusion-based techniques, GRAND [ 3] treats
GNN as a discretization of underlying PDE. It extends the heat dif-
fusion equation to graphs, employing PDE tools to solve differential
equations, thereby addressing issues within GNN. This approach
yields more stable and accurate model performance. GRAND++ [ 23]
builds upon the GRAND model by incorporating a source term into
the diffusion process, aimed at mitigating oversmoothing issues. On
the other hand, GREAD [ 6] enhances the reaction-diffusion layer
to improve model performance.
On the other hand, CAD-Net [ 17] introduces a novel aggregation
scheme, adaptively aggregating neighboring nodes while empha-
sizing the incorporation of intra-category node information and
attenuating attention towards inter-category nodes. However, the
classifier in CAD-Net, which is responsible for classifying neigh-
boring nodes, is not trained separately, which has difficulty for
accurately distinguishing intra-category nodes among neighbors.
Furthermore, it can be observed that the optimal ğ›½values for the
CAD-Net model do not vary significantly across different datasets.
This suggests that CAD-Net may overlook certain inter-class node
features in some datasets, indicating that these features might be
valuable for such datasets.
Our novel method amalgamates the strengths of both approaches
mentioned above. we introduce a novel inter-node relationship
based on node hidden layer representations. The introduction of the
parameterğ›½allows for the adjustment of the aggregation approach
of the model based on the sensitivity to the mixing of inter-category
and intra-category nodes, varying according to different datasets,
thereby obtaining a superior representation of node hidden layer
information. Furthermore, We view GNN as discretizations of un-
derlying PDE, employing advanced PDE tools to enhance stability
and accuracy in addressing issues posed by GNN.
2.4 Mitigating Oversmoothing in GNNs
GNN models suffer from significant performance degradation across
various graph learning tasks as the depth of GNNs increases. This
performance decline is widely attributed to the problem of over-
smoothing [ 16][20][4]. Intuitively, GNNs aggregate neighborhood
information to generate new node representations, ultimately using
these representations to differentiate between different nodes. With
increasing depth, node representations become indistinguishable,
leading to oversmoothing. This is because a large amount of noise
exists in the adjacency relationships, which propagates between
layers during the aggregation of neighborhood information. Sev-
eral algorithms have been proposed to alleviate oversmoothing in
GNNs, including skip connections and dilated convolutions [ 15],
JumpKnowledge [ 30], DropEdge [ 22], PairNorm [ 32], GRAND [ 3],
GRAND++ [23], and wave equation motivated GNNs [9].
3 Method
In this section, we present our approach called FGND for semi-
supervised node classification tasks aiming to enhance the modelâ€™s
performance in addressing oversmoothing and the mismatch be-
tween graph topology and node features. On this basis, due to
the effective filtering of noisy connections in the graph topology,
 
2938KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liangtian Wan et al.
we find that the performance of the FGND model for the over-
smoothing problem is also effectively improved.
3.1 Problem Formulation & Preliminaries
In the problem of semi-supervised node classification, let G=
(V,E)be a graph, where Vrepresents the set of nodes and E
represents the set of edges. Each node ğ‘£ğ‘–âˆˆV corresponds to a
data point, and the edges ğ‘’ğ‘–ğ‘—âˆˆEencode the pairwise relationships
between nodes. Xrepresents the node features of the graph G, and
Yrepresents the categories of nodes. The objective of the problem
is to assign a category label Yğ‘–to each node ğ‘£ğ‘–âˆˆV, where Yğ‘–is
drawn from the category set C.
The definition of the diffusion process from the application of
Fourierâ€™s law of heat conduction, where the heat flux, denoted as
â„=âˆ’ğ‘”âˆ‡ğ‘¥[3] , is directly proportional to the temperature gradient
âˆ‡ğ‘¥. In this context, ğ‘”represents the diffusivity characterizing the
thermal conductance properties of the domain Î©. The continuity
condition, expressed asğœ•ğ‘¥
ğœ•ğ‘¡=âˆ’div(â„), implying that temperature
variations solely arise from heat flux as measured by the divergence
operator, gives rise to a PDE commonly known as the diffusion
equation for heat:
ğœ•x(ğ‘¡)
ğœ•ğ‘¡=div[g(x(ğ‘¡),ğ‘¡)âˆ‡x(ğ‘¡)]. (1)
Similarly, in the context of the graph diffusion equation, we
define the equation as follows, where the change in the graph
signal is solely influenced by the divergence of the signal flowing
from each node to its neighbors:
ğœ•x(ğ‘¡)
ğœ•ğ‘¡=div[G(x(ğ‘¡),ğ‘¡)âˆ‡x(ğ‘¡)], (2)
where G(x(ğ‘¡),ğ‘¡)=diag(ğ‘(ğ‘¥ğ‘–(ğ‘¡),ğ‘¥ğ‘—(ğ‘¡),ğ‘¡))represents an ğ‘’Ã—ğ‘’diag-
onal matrix. ğ‘is a function determining the relationships between
nodes, which is used to ascertain the magnitudes of the weights for
each edge. Let ğ’³denote the features associated with the edges. The
divergence(div(ğ’³))ğ‘–is defined on the graph as the summation of
information from all neighboring nodes of node ğ‘–.âˆ‡x(ğ‘¡)represents
the difference between the graph signal of its neighbors ğ‘—and the
diffusion node ğ‘–(i.e.,ğ‘¥ğ‘—âˆ’ğ‘¥ğ‘–). By substituting the expressions for
âˆ‡anddiv, we obtain the following result
ğœ•
ğœ•ğ‘¡x(ğ‘¡)=(A(x(ğ‘¡))âˆ’I)x(ğ‘¡)=A(x(ğ‘¡))x(ğ‘¡), (3)
where Ais the adjacency matrix of the graph. With the initial state
set as X(0)=ğœ™(Xin), we can then define the differential equation
on the graph:
X(ğ‘‡)=X(0)+âˆ«ğ‘‡
0ğœ•X(ğ‘¡)
ğœ•ğ‘¡ğ‘‘ğ‘¡, X(0)=ğœ™(Xin), (4)
whereğœ•X(ğ‘¡)
ğœ•ğ‘¡is given by the graph diffusion equation (2).
3.2 Framework of FGND
The framework of our model is depicted in Figure 2. Firstly, we
reconstructed the relationships between nodes based on the graph
topology and latent class representation. The simis designed as a
metric to measure the degree of mismatch severity. Subsequently,
we combine the parameter ğ›½to reconstruct the graph by diffu-
sion. PDE tools are introduced in the diffusion process, and the
Figure 2: Framework of FGND
advantages and disadvantages of different numerical discretisation
methods are explored in the graph diffusion process. The algorithm
of pseudocode is shown in Algorithm 1.
Algorithm 1 FGND
Input: GraphG=(V,E); Adjacency matrix ğ‘¨; Node features
matrices Xin;Iğ‘–=X(ğ‘‡)ğ‘–andIğ‘—=X(ğ‘‡)ğ‘–forğ‘—âˆˆN(ğ‘–), where
N(ğ‘–)denotes the neighborhood of node ğ‘–;ğœ”ğ‘–ğ‘—is a learnable
parameter initialized to one;
Output: Prediction result of node ğ‘¥(denoted as Y)
1:Get embedding X(0)=ğœ™(Xin)
2:Using PDE tools to Solving differential equation, X(ğ‘‡)=
X(0)+âˆ«ğ‘‡
0ğœ•X(ğ‘¡)
ğœ•ğ‘¡ğ‘‘ğ‘¡,X(0)=ğœ™(Xin),whileğœ•
ğœ•ğ‘¡x(ğ‘¡)=(A(x(ğ‘¡))âˆ’
I)x(ğ‘¡)=A(x(ğ‘¡))x(ğ‘¡)
3:Recalculate the graph diffusion matrix ğ·,ğ·ğ‘–ğ‘— =
softmaxğ‘—âˆˆN(ğ‘–)
ğœ”ğ‘–ğ‘—Iğ‘‡
ğ‘–Iğ‘—
4:Exploit PDE tools to solving differential equation 6 based on ğ·
5:Utilize decoder to obtain the node classification Y
6:Return Y
The central focus of our work is on resolving the mismatch be-
tween graph topology and node features, which is especially critical
in datasets like pubmed, Amazonâ€™s â€˜Photoâ€™ and â€˜Computersâ€™. Our
model outperforms GRAND by introducing effective solutions to
handle this significant issue. The specific solution can be summa-
rized as follows.
â€¢Node Relationship Recomputation: To enhance the rep-
resentation of relationships between nodes, we employ the
learned representations from node hidden layers. The tran-
sition probability from ğ‘£ğ‘–toğ‘£ğ‘—is redefined using a softmax
function based on the inner product of node representations.
â€¢Quantifying Mismatch Severity sim:We introduce the
simmetric to quantify the severity of the mismatch between
graph topology and node features. This metric is positively
correlated with the proportion of intra-class connections in
the neighbourhood of a node.
â€¢Parameter ğ›½for Adaptive Adjustment: The introduction
of the parameter ğ›½enables an adaptive adjustment of the re-
lationship matrix between each node and its neighbors. This
adjustment, guided by the calculated sim, aims to optimize
node representations during the solution of the differential
equation.
 
2939Flexible Graph Neural Diffusion with Latent Class Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The detail of FGND is introduced in the remaining of Section 3.
3.3 Graph Diffusion Matrix Reconstruction
Based on Latent Class Representation
Currently, many models are dedicated to addressing the issue of
incorrect guidance in aggregating information caused by inter-
category connections within adjacency relationships [ 17] [8]. In
order to incorporate node feature information into the diffusion
process and address the issue of partial inconsistency between
node features and adjacency relationships in some real datasets, we
utilize latent class representation along with the graphâ€™s inherent
topological structure. Additionally, we make use of a learnable
parameterğœ”ğ‘–ğ‘—to recompute the graph diffusion matrix.
LetIğ‘–=X(ğ‘‡)ğ‘–andIğ‘—=X(ğ‘‡)ğ‘—forğ‘—âˆˆN(ğ‘–), whereN(ğ‘–)de-
notes the neighborhood of node ğ‘–. Our objective in this study is
to encourage the walker to transition from node ğ‘£ğ‘–toğ‘£ğ‘—when the
probabilities Iğ‘–andIğ‘—are more similar. To achieve this, we define
the transition probability from ğ‘£ğ‘–toğ‘£ğ‘—as:
ğ·ğ‘–ğ‘—=softmaxğ‘—âˆˆN(ğ‘–)
ğœ”ğ‘–ğ‘—Iğ‘‡
ğ‘–Iğ‘—
, (5)
whereğœ”ğ‘–ğ‘—is a learnable parameter initialized to one.
In our model, the final diffusion step utilizes ğ·ğ‘–ğ‘—as the transition
probability from node ğ‘£ğ‘–to nodeğ‘£ğ‘—. This optimization enhances the
diffusion process by promoting the aggregation of similar features
during propagation, thereby mitigating the impact of inter-class
feature aggregation in the adjacency relationships on the final
results, which is given by
X(ğ‘‡+1)=(1âˆ’ğ›¾)X(ğ‘‡)+ğ›¾âˆ«ğ‘‡+1
ğ‘‡ğœ•X(ğ‘¡)
ğœ•ğ‘¡aggregated by ğ·ğ‘‘ğ‘¡. (6)
The parameter ğ›¾is employed to regulate the degree of aggregat-
ing similar features during the diffusion process. Given the varying
influence of node features on the interrelationships between nodes
across different datasets, adjusting the magnitude of parameter ğ›¾
enables us to strike a balance between the features of nodes within
the same class and the adjacent node features within the original
topological structure. The similarity between the features of node ğ‘–
and its neighboring nodes is given by
ğ‘ ğ‘–ğ‘šğ‘–=Ã
ğ‘—ğœ–ğ‘(ğ‘–)ğ¼ğ‘‡
ğ‘–ğ¼ğ‘—
ğ‘ ğ‘ğ‘Ÿğ‘¡[ğ‘‘ğ‘’ğ‘”(ğ‘–)], (7)
where the degree of node ğ‘–, denoted as ğ‘‘ğ‘’ğ‘”(ğ‘–), represents the num-
ber of edges connected to the ğ‘–th node.ğ‘ ğ‘–ğ‘šğ‘–means the similarity
among the features of node ğ‘–and its neighboring nodes. A larger
value ofğ‘ ğ‘–ğ‘šğ‘–indicates that there are more nodes of the same class
among the neighbors of node ğ‘–. The parameter vector ğ›¾is given by
ğ›¾=(1âˆ’ğ›½)Â·ğ‘ ğ‘–ğ‘š+ğ›½, (8)
whereğ›½is a hyperparameter of the model and ğ‘ ğ‘–ğ‘šrepresents a
vector that includes all nodes ğ‘ ğ‘–ğ‘šğ‘–. This allows for a greater aggre-
gation of similar features during the diffusion process in equation
(6).
Here, the range of ğ›½is0â‰¤ğ›½â‰¤1. Since the elements of ğ‘ ğ‘–ğ‘š
are less than 1, this also ensures that the range of the element ğ›¾ğ‘–
inğ›¾is0â‰¤ğ›¾ğ‘–â‰¤1. The vector of ğ›¾is primarily determined by ğ›½
and fine-tuned through vector ğ‘ ğ‘–ğ‘š. If, for a node ğ‘–, the value ofğ‘ ğ‘–ğ‘šğ‘–is relatively high, it indicates that the features of node ğ‘–are
consistent with the topological information between it and its neigh-
boring nodes. Under the same ğ›½value,ğ›¾ğ‘–of nodeğ‘–in the process
of equation (6)will be larger compared to other nodes, leading to a
greater aggregation of information in the graph diffusion matrix
constructed based on node features. By adjusting the parameter ğ›½,
the dependence of model on the node hidden layer representation
in the graph diffusion process can be tailored according to differ-
ent datasets, effectively filtering out noise in the graph topology
structure.
3.4 Exploration of Numerical Methods for
Differential Equations
3.4.1 Explicit Method. The explicit Euler method, commonly known
as the explicit forward Euler method, is a frequently used numerical
technique for solving differential equations. When modeling the
graph diffusion process as a differential equation, the conventional
method for solving corresponding GNN models is the explicit Euler
method. The solving formula for this method is given by
X(ğ‘‡+1)=X(ğ‘‡)+Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡). (9)
By substituting equation (3) into equation (6), we have
X(ğ‘‡+1)=X(ğ‘‡)+Î”ğ‘‡Â·A(x(ğ‘‡))x(ğ‘‡). (10)
With the assistance of PDE tools, we can employ more sophis-
ticated and higher-accuracy numerical methods for solving differ-
ential equations. As a higher-order explicit technique, the Runge-
Kutta method excels in numerical accuracy compared to the explicit
Euler method. The formula for the commonly used fourth-order
Runge-Kutta method is expressed as
K1=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡)=Î”ğ‘‡Â·A(x(ğ‘‡))x(ğ‘‡),
K2=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡+Î”ğ‘‡
2)=Î”ğ‘‡Â·A(x(ğ‘‡+Î”ğ‘‡
2))x(ğ‘‡+Î”ğ‘‡
2),
K3=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡+Î”ğ‘‡
2)=Î”ğ‘‡Â·A(x(ğ‘‡+Î”ğ‘‡
2))x(x(ğ‘‡+Î”ğ‘‡
2),
K4=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡+Î”ğ‘‡)=Î”ğ‘‡Â·A(x(ğ‘‡+Î”ğ‘‡))x(ğ‘‡+Î”ğ‘‡),
X(ğ‘‡+1)=X(ğ‘‡)+1
6Â·(K1+2K2+2K3+K4).
(11)
3.4.2 Implicit Method. By harnessing advanced PDE tools, we can
also utilize the implicit Euler method, which offers greater time
stability in comparison to the explicit Euler method when solving
differential equations. The implicit Euler method, relative to the
explicit Euler method, incorporates an algebraic equation into each
iteration to determine the next time step. The solution of this alge-
braic equation typically employs numerical techniques such as the
Newton-Raphson method or the bisection method. Concurrently,
the implicit Euler method exhibits greater time stability, character-
ized by unconditional stability, which means that it remains stable
regardless of the size of the time step Î”ğ‘‡. This quality is particularly
crucial for problems that demand strict time stability requirements.
Its computational formula is presented as
 
2940KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liangtian Wan et al.
X(ğ‘‡+1)=X(ğ‘‡)+Î”ğ‘‡Â·ğœ•
ğœ•ğ‘‡x(ğ‘‡+1). (12)
By substituting equation (3) into equation (9), we have
X(ğ‘‡+1)=X(ğ‘‡)+Î”ğ‘‡Â·A(x(ğ‘‡+1))x(ğ‘‡+1). (13)
4 Experiments
In this section, we conduct experiments to evaluate our algorithm
and popular graph diffusion-based GNN. We also explore the param-
eter space of the algorithm in our model, particularly the sensitivity
parameterğ›½, which is used to adjust the sensitivity of the model to
the newly constructed adjacency relationships ğ·based on node hid-
den layers, thereby adapting to different application scenarios. Our
code and datasets can be found at https://github.com/hanhuijin/FGND.
4.1 Datasets
We compare our model with traditional graph diffusion models in
the task of semi-supervised node classification. Experiments are
conducted on three citation networks: Cora, Citeseer, and Pubmed,
as well as two product datasets from Amazon: Computers and
Photo. In the citation network datasets, nodes represent research
papers, while edges represent citation relationships between papers.
In the Amazon product datasets, nodes represent product-related
information, and edges are determined based on co-occurrence
relationships among products in the dataset, as well as user pur-
chasing and review behavior. Table 1 provides detailed information
on the five aforementioned datasets. The label rate represents the
percentage of nodes used for training.
Table 1: Graph topology statistics of datasets
Dataset
Nodes Edges Classes Features Label Rate
Cora
2,708 5,429 7 1,433 0.052
Citeseer 3,327 4,732 6 3,703 0.036
PubMed 19,717 44,338 3 500 0.003
Amazon â€˜Computersâ€™ 13,752 505,474 10 767 0.015
Amazon â€˜Photoâ€™ 7,487 245,573 8 745 0.021
Table 2: Mean of same class ratio
Cora
Citeseer PubMed Amazon â€˜Computersâ€™ Amazon â€˜Photoâ€™
86.2%
80.08% 86.46% 81.68% 86.09%
Table 2 shows the proportion of intra-class connections (edges
within the same class) among all connections between different
datasets. It quantifies the ratio of edges that connect nodes of the
same class to the total number of edges in different datasets. Inter-
class connections within the dataset can influence the modelâ€™s repre-
sentation of aggregated neighbor features. Aggregating inter-class
relationships can, in certain scenarios, decrease the distinguisha-
bility of node representations in the hidden layers, making it more
challenging for the model to classify nodes accurately.
Table 3 shows the ratio between the mean weight of inter-class
edges and intra-class edges. A smaller ratio indicates a greater dis-
tinction between inter-class and intra-class connections. According
to Figure 3, when this ratio is smaller, we should appropriately
increase the value of ğ›½to improve the modelâ€™s ability to filter out
noise in the graph topology.Table 3: The ratio of inter-category connections to intra-
category connections among neighboring nodes
Cora
Citeseer PubMed Amazon â€˜Computersâ€™ Amazon â€˜Photoâ€™
85.65%
95.92% 81.13% 80.79% 79.36%
4.2 Experimental Setup
We have opted for three representative types of GCN architectures:
â€¢Spatial-based methods, including GraphSage [ 11], GAT [ 25],
IncepGCN [ 22], IncepGCN-DropEdge [ 22], Deep-gcns [ 15],
SGLR(SGC) [ 10], SGLR(GCN) [ 10], EGNN [ 33], SGC-PN [ 32],
GCN-PN [32] and GAT-PN [32];
â€¢Spectral-based methods, encompassing ChebyNet [ 7] and
GCN [ 14], ChebyNet_WaveThresh [ 26], ChebyNet_WaveShri
nk [26], GCN-_WaveThresh [26], GCN_WaveShrink [26];
â€¢Graph diffusion-based methods, namely GRAND [ 3], GRAND
++ [23], GREAD [6] and CAD-Net [17].
For each dataset, we select 1500 nodes as the training validation set,
we select 20 labeled nodes through uniformly random sampling
from each class to form the training set and the remaining nodes
are used as the validation set for model training. In addition 1000
nodes constitute the test set.
See the Experimental Setup section in the Appendix for specific
parameter settings during the experiment.
4.3 Exploring the Flexibility of Network Models
This chapter explores the flexibility of our model by analyzing the
relationship between the weight ratios of intra-class and inter-class
connections, and the parameter ğ›½across various datasets. The study
validates the adaptability of our model to different relationship
categories in the real world. It autonomously adjusts the weights
of inter-node connections, effectively filtering out noise factors
within node relationships, and consequently, obtaining enhanced
node representations.
From Table 2, it becomes evident that real-world datasets exhibit
adjacency relationships between nodes that encompass both intra-
category and inter-category nodes. Employing a straightforward
aggregation of original adjacency relationships for capturing node
adjacency information within the model might inadvertently blend
features of inter-category and intra-category nodes together. In
our experiments, we observe that such an approach led to deterio-
rated hidden layer representations after aggregation. This primitive
blending approach consequently diminish the classification capac-
ity of the model. To address this concern, our model tackles the
issue by reconstituting the adjacency relationships between nodes
based on the nodesâ€™ inherent hidden layer representations. Under
this novel adjacency scheme, nodes are oriented to aggregate the
hidden layer representations of intra-category nodes more promi-
nently. Simultaneously, the sensitivity of the model is regulated
through the parameter ğ›½.
Figures 3 depict the variation in model accuracy as ğ›½changes,
by using the citation network and Amazon dataset as instances.
To mitigate the impact of experimental randomness, the accuracy
for eachğ›½value has been computed as the average result from
100 independent repetitions of the experiment involving multiple
random dataset partitions.
 
2941Flexible Graph Neural Diffusion with Latent Class Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) Citeseer
 (b) Cora & Amazon â€˜Computersâ€™
(c) Pubmed
 (d) Amazon â€˜Photoâ€™
Figure 3: The accuracy of different ğ›½
It can be observed that the optimal ğ›½values for the model on
different datasets correspond to the results indicated in Table 3. In
the Cora and Citeseer datasets, this ratio of inter-category to intra-
category connections is higher. This suggests that in the adjacency-
based inter-node relationships formed by node hidden layer repre-
sentations, there is a closer allocation of weights between nodes of
different categories and nodes of the same category. This indicates
that node features are more consistent with the information pro-
vided by the topological structure of graph, and there is less noise
in the topological structure. Consequently, the optimal ğ›½values
are lower, indicating that the model places greater emphasis on the
original adjacency relationships.
Conversely, in the Pubmed, Amazon â€˜Photoâ€™, and Amazon â€˜Com-
putersâ€™ datasets, the ratio of inter-category to intra-category connec-
tions is lower. This implies that in the adjacency-based inter-node
relationships formed by node hidden layer representations, there is
a greater disparity in weight allocation between nodes of different
categories and nodes of the same category. This suggests a certain
level of contradiction between node features and the topological
structure of the graph. Therefore, larger values of ğ›½are needed to
enable the model to filter out the noise in the topological structure,
thereby achieving a unified representation of graph node features
and topology.
We have incorporated additional comparative experiments on
our model in the context of heterophilic graphs. Our model adheres
to the data partitioning requirements outlined in [ 34]. Specifically,
we designated 10% of nodes for each category as the training and
validation sets, while the remaining 80% of the data served as the
test set. The results of our experiments are presented in the table
below, indicating that our model outperforms existing approaches
on heterophilic graphs datasets.Table 4: Heterophilous graphs
Model Squirrel Chameleon Mean Acc
FGND 38.8Â±1.30 54.6Â±1.80 46.7
CPGNN-MLP-1 32.70 Â±1.90 51.08Â±2.29 41.89
CPGNN-MLP-2 26.64 Â±1.23 55.46Â±1.42 41.05
CPGNN-Cheby-1 37.03 Â±1.23 53.90Â±2.61 45.465
CPGNN-Cheby-2 27.92 Â±1.53 56.93Â±2.03 42.425
H2GCN 29.50Â±0.77 48.12Â±1.96 38.81
GraphSAGE 34.35 Â±1.09 45.45Â±1.97 39.9
GCN-Cheby 26.52 Â±0.92 36.66Â±1.84 31.59
MixHop 36.42 Â±3.43 46.84Â±3.47 41.63
GCN 33.31 Â±0.89 52.00Â±2.30 42.655
GAT 31.20 Â±2.57 50.54Â±1.97 40.87
MLP 25.50 Â±0.87 37.36Â±2.05 31.43
We have bolded the prediction accuracy of the optimal model in
Table 4, which illustrates that our model exhibits superior perfor-
mance on heterophilic graphs datasets compared to the algorithm
proposed by Zhu et al. [34]
4.4 Mitigating Oversmoothing: Effectiveness of
the FGND Model
(a) 9th-order neighborhood fea-
tures (Citeseer)
(b) 19th-order neighborhood fea-
tures (Cora)
(c) 14th-order neighborhood fea-
tures (Pubmed)
Figure 4: Node hidden layer representations in the FGND
model when stacking high-order neighborhood features
Figure 4 presents the node hidden layer representations of FGND
across different datasets. It is evident that FGND maintains a clear
distinction between nodes of different classes even after aggregat-
ing high-order neighborhood features. This observation indicates
the effectiveness of model in mitigating oversmoothing issues. In
contrast, as shown in Figure 5, the GCN model encounters pro-
nounced oversmoothing when aggregating neighborhood features
 
2942KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liangtian Wan et al.
(a) 9th-order neighborhood fea-
tures (Citeseer)
(b) 19th-order neighborhood fea-
tures (Cora)
(c) 14th-order neighborhood fea-
tures (Pubmed)
Figure 5: Node hidden layer representations in the GCN
model when stacking high-order neighborhood features
of the same order as FGND. In the nodeâ€™s hidden layer represen-
tations, nodes of different classes are mixed together, making it
challenging to differentiate them.
Taking the Cora dataset as an example, we compared our model
with GRAND++, evaluating the effectiveness of both models in
combating oversmoothing when stacking neural networks to the
same depth. The experimental results are presented in the Table 5,
where the reported values represent means accompanied by their
respective standard deviations (mean Â±std), calculated from 10
independent runs.
Table 5: Oversmoothing Comparative Analysis: Model Per-
formance vs GRAND++
Depth
(T) 16 64 128 256
FGND
0.840Â±0.013 0.840Â±0.013 0.840Â±0.011 0.832Â±0.024
GRAND++ 0.829Â±0.011
0.820Â±0.011 0.809Â±0.019 0.731Â±0.108
This direct comparison aims to provide a clearer understanding
of how our model performs against GRAND++ in the context of
oversmoothing.
4.5 Comparison with State-of-the-Art Methods
Table 6 presents the overall results obtained under standard bench-
mark settings. The reported values represent means accompanied
by their respective standard deviations (mean Â±std), calculated
from 20 independent runs. We highlight the best two methods as
follows: First andSecond . Notably, our model exhibits superior
performance across all datasets. Furthermore, our model demon-
strates enhanced stability in its predictive outcomes over the course
of 20 independently partitioned dataset experiments.
Figure 6 and Figure 7 respectively present the results of our
model across 20 random experiments on the citation network and
(a) Citeseer
 (b) Cora
(c) Pubmed
Figure 6: Three citation networks Cora Citeseer Pubmed
compare for 20 splits
(a) Amazon â€˜Photoâ€™
 (b) Amazon â€˜Computersâ€™
Figure 7: Amazon â€˜Photoâ€™ and Amazon â€˜Computersâ€™ compare
for 20 splits
Amazon datasets. It is evident that, in the Citeseer, Cora, and Ama-
zon â€˜Computersâ€™ datasets, the original GRAND model outperforms
CAD-Net. This observation can be rationalized by the fact that, in
these datasets, the influence of inter-category node hidden features
on the performance of the model is relatively minor. Simultane-
ously, the graph diffusion-based GRAND model treats GNN as a
discretization of underlying PDEs, which contributes to a more
stable model performance. Meanwhile, in the Pubmed and Amazon
â€˜Photoâ€™ datasets, the CAD-Net model exhibits superior performance.
This suggests that in these datasets, the sensitivity of model to the
hidden layer representation of features is more pronounced, with
the aggregation of inter-category node hidden features in the ad-
jacency relationships exerting a more substantial impact on the
node hidden layer representation. Our FGND model leverages the
strengths of both methods by incorporating the reconstruction
of inter-node relationships based on hidden layer representations.
Furthermore, it flexibly handles inter-dataset characteristics in the
diffusion process through the adjustment of the simmetric and
the parameter beta. Additionally, we explore various numerical
methods to solve the graph diffusion process. From Figure 6 and
Figure 7, it can be observed that compared to both GRAND and
CAD-Net models, our FGND method consistently maintains supe-
rior predictive accuracy across these five publicly available datasets.
 
2943Flexible Graph Neural Diffusion with Latent Class Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 6: Accuracy (%) under standard benchmark setting
Metho
d Cora Citeseer PubMed Amazon â€˜Photoâ€™ Amazon â€˜Computersâ€™
BaselinesCAD-Net 83.31Â±1.6%
70.80Â±1.4% 78.79Â±2.3% 89.62Â±2.6% 79.31 Â±2.0%
GRAND 83.62Â±1.5% 74.10Â±1.7% 76.49Â±1.9% 90.20Â±1.5% 84.09 Â±1.4%
GRAND++ 82.75Â±1.4% 73.28Â±2.0% 79.88Â±1.1% 92.80Â±0.4%
83.75Â±1.1%
GREAD 82.41Â±1.6% 71.30Â±1.5% 74.25Â±1.8% 89.78Â±1.6% 83.25 Â±1.2%
GAT 78.79Â±1.5% 65.31Â±1.9% 76.27Â±2.4% 89.54Â±1.3% 78.37 Â±2.8%
GraphSage 74.21Â±3.7% 60.25Â±3.7% 74.93Â±1.8% 86.18Â±4.1% 76.75 Â±4.0%
ChebyNet 75.26Â±2.5% 67.82Â±1.6% 67.48Â±2.8% 84.00Â±2.1% 71.14 Â±4.3%
GCN 78.71Â±1.5% 65.74Â±1.6% 77.56Â±1.7% 89.43Â±1.5% 79.08 Â±2.3%
ChebyNet_WaveThresh 75.77Â±1.6% 67.37Â±1.2% 67.96Â±3.4% 84.83Â±2.4% 61.56 Â±7.3%
ChebyNet_WaveShrink 76.22Â±1.2% 67.59Â±0.9% 68.11Â±3.4% 84.19Â±2.1 % 63.26 Â±5.8%
GCN_WaveThresh 78.31Â±1.7 % 64.53Â±1.6% 77.70Â±1.0% 89.94Â±1.3 % 82.75 Â±1.3%
GCN_WaveShrink 78.17Â±2.1% 64.76Â±1.6% 75.49Â±2.3% 90.17Â±1.0% 82.07 Â±1.6%
Deep-gcns 75.26Â±1.6% 58.36Â±2.3% 71.95Â±1.7% 87.94Â±1.3% 78.14 Â±2.2%
SGLR(SGC) 81.24Â±0.8% 68.39Â±0.7% 76.28Â±1.0% 80.66Â±2.6% 66.07 Â±5.9%
SGLR(GCN) 81.95Â±0.3% 69.50Â±0.3% 78.60Â±0.3% 89.28Â±0.6% 74.23 Â±0.8%
SGC-PN 75.85Â±0.6% 63.02Â±0.7% 78.21Â±0.2% 78.99Â±0.1% 73.93 Â±0.2%
GCN-PN 76.09Â±1.4% 54.69Â±2.5% 77.29Â±0.6% 87.22Â±2.7% 80.83 Â±6.1%
GAT-PN 74.95Â±2.2% 56.61Â±1.2% 77.53Â±1.0% 85.13Â±2.2% 70.98 Â±6.5%
EGNN 82.85Â±0.4% 67.68Â±0.3% 79.27Â±0.3% 83.13Â±0.4% 75.44 Â±0.8%
IncepGCN-DropEdge 80.53Â±1.2% 63.82Â±1.0% 52.53Â±7.7% 89.96Â±1.0% 80.74 Â±2.3%
IncepGCN 79.92Â±1.5% 63.88Â±1.4% 53.03Â±7.6% 86.66Â±1.9% 78.25 Â±5.0%
CPGNN-MLP-1 78.76Â±0.5% 64.28Â±0.4% 75.33Â±0.1% 88.21Â±0.3% 77.46 Â±1.2%
CPGNN-MLP-2 76.19Â±0.7% 64.86Â±0.5% 77.59Â±0.7% 85.57Â±1.0% 67.83 Â±3.6%
CPGNN-Cheby-1 79.39Â±1.0% 68.66Â±0.8% 76.95Â±1.0% 86.78Â±0.3% 77.66 Â±1.1%
CPGNN-Cheby-2 76.40Â±1.3% 68.62Â±0.6% 76.37Â±0.9% 84.00Â±0.7% 62.91 Â±2.1%
Our
Proposed AlgorithmGrand+CAD-beta(new)-k(1) 83.78Â±1.2% 74.08Â±1.6%
79.56Â±1.0% 92.35Â±0.5% 85.22 Â±2.6%
Grand+CAD-b
eta(CAD)-k(CAD) 83.61Â±1.4% 73.98Â±1.9% 80.06Â±1.4% 92.83Â±0.8%
84.84Â±2.7%
FGND 84.49Â±1.1% 74.99Â±2.0% 79.77Â±1.5% 92.89Â±0.6% 85.80Â±1.8%
FGND-Explicit
Euler 83.32Â±1.4% 74.52Â±1.2% 79.49Â±1.3%
92.85Â±0.6% 85.00Â±1.8%
Additionally, our FGND method exhibits reduced variability in per-
formance as a response to data partition fluctuations.
We attempt to integrate the GNN solvers GRAND and class-
attentive diffusion network into two versions: Grand+CAD-beta(new)-
k(1) and Grand+CAD-beta(CAD)-k(CAD). The experimental results
are presented in Table 6. Grand+CAD-beta(new)-k(1) fully replaces
the diffusion process with discrete time steps derived from differ-
ential equations, whereas Grand+CAD-beta(CAD)-k(CAD) retains
the original graph diffusion process of the class-attentive diffusion
network. Experimental results demonstrate that both versions of
the model outperform existing models on most datasets, although
they still fall short of the performance achieved by our proposed
FGND model.
Traditional diffusion-based models correspond to the explicit
Euler numerical solution method in their solving process. In Table
6, we present the FGND model using the explicit Euler method to
solve differential equations, denoted as FGND-Explicit Euler. From
the table, it can be observed that by employing advanced PDE tools,
the FGND model can utilize a more efficient approach to solve
differential equations, leading to improved performance.
We also conducted experiments on the large dataset Ogbn-arxiv,
and the results, as presented in Table 9 of the Appendix, demonstrate
that our model continues to achieve superior performance.
5 Conclusion
In this paper, we address the challenge of the inconsistent node fea-
tures and network topology in real-world datasets by introducing
a novel model, FGND. This model leverages advanced PDE tools
and latent class representation to provide flexibility in adapting
to the characteristics of different datasets. It effectively filters outnoise present in the graph edges and maintains stable model per-
formance in the presence of data perturbations. In the future, we
plan to further explore the impact of the contradiction between
node features and network topology on predictive accuracy from
an interpretable perspective. Additionally, we aim to investigate
more complex structures or other more efficient methods for solv-
ing differential equations, such as attention-based mechanisms, to
mitigate edge noise in real-world datasets.
6 Acknowledgement
This work is supported by the National Key Research and Devel-
opment Program of China (No. 2022YFF0712400), National Natural
Science Foundation of China (62101088), Natural Science Foun-
dation of Liaoning Province (2022-MS-157, 2023-MS-108), Funda-
mental Research Funds for the Central Universities ((3132024242),
Zhejiang Lab (2022KG0AN02) and the Science and Technology Re-
search Program for Chongqing Municipal Education Commission
KJZD-M202200601.
References
[1]James Atwood and Don Towsley. 2016. Diffusion-convolutional neural networks.
Advances in Neural Information Processing Systems 29 (2016), 1â€“9.
[2]Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral net-
works and locally connected networks on graphs. arXiv preprint arXiv:1312.6203
(2013), 1â€“13.
[3]Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan
Webb, and Emanuele Rossi. 2021. Grand: Graph neural diffusion. In International
Conference on Machine Learning. 1407â€“1418.
[4]Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring
and relieving the over-smoothing problem for graph neural networks from the
topological view. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 34. 3438â€“3445.
[5]Geng Chen, Hang Dai, Tao Zhou, Jianbing Shen, and Ling Shao. 2022. Automatic
Schelling points detection from meshes. IEEE Transactions on Visualization and
 
2944KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liangtian Wan et al.
Computer Graphics (2022), 2926â€“2939.
[6]Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. 2023.
GREAD: Graph neural reaction-diffusion networks. In International Conference
on Machine Learning. PMLR, 5722â€“5747.
[7]MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-
tional neural networks on graphs with fast localized spectral filtering. Advances
in Neural Information Processing Systems 29 (2016), 1â€“9.
[8]Mehmet F Demirel, Shengchao Liu, Siddhant Garg, Zhenmei Shi, and Yingyu
Liang. 2022. Attentive Walk-Aggregating Graph Neural Networks. Transactions
on Machine Learning Research (2022), 1â€“32.
[9]Moshe Eliasof, Eldad Haber, and Eran Treister. 2021. Pde-gcn: Novel architectures
for graph neural networks motivated by partial differential equations. Advances
in neural information processing systems 34 (2021), 3836â€“3849.
[10] Jhony H Giraldo, Konstantinos Skianis, Thierry Bouwmans, and Fragkiskos D
Malliaros. 2023. On the trade-off between over-smoothing and over-squashing
in deep graph neural networks. In Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management. 566â€“576.
[11] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in Neural Information Processing Systems 30
(2017), 1â€“9.
[12] Jacob Helwig, Xuan Zhang, Cong Fu, Jerry Kurtin, Stephan Wojtowytsch, and
Shuiwang Ji. 2023. Group Equivariant Fourier Neural Operators for Partial
Differential Equations. In International Conference on Machine Learning. 1â€“9.
[13] Arijit Khan, Yuan Ye, Lei Chen, and HV Jagadish. 2018. On Uncertain Graphs.
Springer.
[14] Thomas N. Kipf and Max Welling. 2017. Semi-supervised classification with
graph convolutional networks. arXiv preprint arXiv:1609.02907 (2017), 1â€“9.
[15] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. 2019. Deep-
gcns: Can gcns go as deep as cnns?. In Proceedings of the IEEE/CVF international
conference on computer vision. 9267â€“9276.
[16] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph
convolutional networks for semi-supervised learning. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[17] Jongin Lim, Daeho Um, Hyung Jin Chang, Dae Ung Jo, and Jin Young Choi.
2021. Class-attentive diffusion network for semi-supervised classification. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 8601â€“8609.
[18] Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens SjÃ¶lund, and Thomas B.
SchÃ¶n. 2023. Image Restoration with Mean-Reverting Stochastic Differential
Equations. In International Conference on Machine Learning. 23045â€“23066.
[19] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda,
and Michael M Bronstein. 2017. Geometric deep learning on graphs and manifolds
using mixture model cnns. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 5115â€“5124.
[20] Kenta Oono and Taiji Suzuki. 2019. Graph neural networks exponentially lose
expressive power for node classification. arXiv preprint arXiv:1905.10947 (2019).
[21] Jiaxing Qu, Yuxuan Richard Xie, and Elif Ertekin. 2023. A language-based recom-
mendation system for material discovery. In International Conference on Machine
Learning. 1â€“5.
[22] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. Dropedge:
Towards deep graph convolutional networks on node classification. arXiv preprint
arXiv:1907.10903 (2019).
[23] Matthew Thorpe, Tan Minh Nguyen, Hedi Xia, Thomas Strohmer, Andrea
Bertozzi, Stanley Osher, and Bao Wang. 2022. GRAND++: Graph Neural Diffusion
with A Source Term. In International Conference on Learning Representations. 1â€“9.
[24] Arnaud Vadeboncoeur, Ieva Kazlauskaite, Yanni Papandreou, Fehmi Cirak, Mark
Girolami, and Omer Deniz Akyildiz. 2023. Random grid neural processes for
parametric partial differential equations. In International Conference on Machine
Learning. 34759â€“34778.
[25] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations. 1â€“9.
[26] Liangtian Wan, Xiaona Li, Huijin Han, Xiaoran Yan, Lu Sun, Zhaolong Ning, and
Feng Xia. 2023. Unifying and Improving Graph Convolutional Neural Networks
with Wavelet Denoising Filters. In Proceedings of the ACM Web Conference 2023 .
177â€“187.
[27] Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan, and Huan
Liu. 2021. Graph Learning: A Survey. IEEE Transactions on Artificial Intelligence
2, 2 (2021), 109â€“127. https://doi.org/10.1109/TAI.2021.3076021
[28] Feng Xia, Lei Wang, Tao Tang, Xin Chen, Xiangjie Kong, Giles Oatley, and Irwin
King. 2022. Cengcn: Centralized convolutional networks with vertex imbalance
for scale-free graphs. IEEE Transactions on Knowledge and Data Engineering 35, 5
(2022), 4555â€“4569.
[29] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. 2018. Graph
Wavelet Neural Network. In International Conference on Learning Representations.
1â€“8.
[30] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In International conference on machine learn-
ing. PMLR, 5453â€“5462.[31] Sangeeta Yadav. 2023. Predicting the stabilization quantity with neural networks
for Singularly Perturbed Partial Differential Equations. In International Conference
on Machine Learning. 1â€“8.
[32] Lingxiao Zhao and Leman Akoglu. 2019. Pairnorm: Tackling oversmoothing in
gnns. arXiv preprint arXiv:1909.12223 (2019).
[33] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi,
and Xia Hu. 2021. Dirichlet energy constrained learning for deep graph neural
networks. Advances in Neural Information Processing Systems 34 (2021), 21834â€“
21846.
[34] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed,
and Danai Koutra. 2021. Graph neural networks with heterophily. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 35. 11168â€“11176.
A Appendix
A.1 Experimental Setup
The ogbn-arxiv dataset was added during the rebuttal stage to
validate the performance of our proposed FGND model in larger
datasets. The ogbn-arxiv dataset is a directed graph representing
the citation network of all computer-related papers on the arxiv
website. Each node is an arxiv paper, and each edge represents an
article citing another article. It has 169343 nodes, 1166243 edges.
The detailed performance comparison can be found in Table 9.
Experiments Compute Resource: Except for experiments con-
ducted on the ogbn-arxiv dataset, all other experiments were per-
formed using a single Nvidia RTX 3090 GPU with 24GB of memory.
For experiments on the ogbn-arxiv dataset, we utilized an A-100
GPU for evaluation.
Table 7: Experimental setup in spectral-based methods
Mo
del Dataset Learning Rate Early Stop K (Corresponding to the time step ğ‘‡)
GCNCora
0.01 10 2
Citeseer 0.01 10 2
Pubmed 0.01 10 2
Amazon
â€˜Photoâ€™ 0.001 10 2
Amazon â€˜Computersâ€™ 0.001 10 2
Ogbn-arxiv 0.001 80 2
ChebyNetCora
0.1 30 3
Citeseer 0.1 30 3
Pubmed 0.001 30 3
Amazon
â€˜Photoâ€™ 0.001 30 2
Amazon â€˜Computersâ€™ 0.001 10 2
Ogbn-arxiv 0.001 80 2
In the context of spectral-based methodologies, for the ChebyNet-
_WaveThresh [ 26], ChebyNet_WaveShrink [ 26], GCN_Wa-veThresh
[26], GCN_WaveShrink [ 26] models, we utilize publicly available
code implementations. For the GCN and ChebyNet models, we
construct a two-layer network consisting of 16 hidden units. The
constitution of the test set for each experiment follows a random
process. To enhance the representativeness of outcomes, we conduct
each experiment iteratively for 20 trials and subsequently computed
the mean of these twenty outcomes to establish the ultimate result.
Adam is selected as the optimization algorithm. The configuration of
hyperparameters involves setting â€˜max_epochsâ€™ to 200. In instances
where a reduction in loss is not observed, an early stopping criterion
has been implemented, with termination set at 10 epochs for GCN
and 30 epochs for ChebyNet. For further parameter specifications,
please refer to Table 7.
Regarding the spatial-based techniques, for the IncepGCN [22],
IncepGCN-DropEdge [ 22], Deep-gcns [ 15], SGLR(SGC) [ 10], SGLR(
GCN) [ 10], EGNN [ 33], SGC-PN [ 32], GCN-PN [ 32] and GAT-PN
[32] models, we utilize publicly available code implementations.
For the GAT and GraphSage models, the optimization algorithm
 
2945Flexible Graph Neural Diffusion with Latent Class Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 10: Statistical Values for Edge Weights in Reconstructed
Topology Structure
Dataset Mean Min Max Standard Deviation Variance Range
CoauthorCS 0.8584 0.0434 1 0.1943 0.0377 0.9566
Citeseer 0.8008 0.0526 1 0.2502 0.0626 0.9474
Pubmed 0.8646 0.0212 1 0.2170 0.0471 0.9788
Cora 0.8620 0.1250 1 0.2146 0.0460 0.8750
Photo 0.8609 0.0111 1 0.2054 0.0421 0.9889
Computers 0.8168 0.0119 1 0.2156 0.0465 0.9881
Texas 0.4029 0.0238 1 0.1838 0.0338 0.9761
Squirrel 0.4376 0.0897 1 0.2098 0.0440 0.9103
Ogbn-arxiv 0.7070 0.0017 1 0.2718 0.0073 0.9983
of choice is Adam, with a maximum number of epochs set at 200.
If a decline in loss is not evident, an early stopping mechanism is
implemented, which is terminated after 10 epochs. In the Graph-
Sage framework, a dual-layer network comprising 16 hidden units
has been constructed. The hyperparameter settings encompass a
learning rate of 0.01 for citation network datasets, and a learning
rate of 0.001 for Amazon datasets. As for GAT, the hyperparameters
are configured with a learning rate of 0.001.
For graph diffusion-based methods, the optimization algorithm
of choice is Adam, with a maximum number of epochs set at 200. For
the GRAND, GRAND++, GREAD and CAD-Net models, we utilize
publicly available code implementations. The specific parameters
are outlined in Table 8.
Table 8: Experimental setup in diffusion-based methods
Mo
del Dataset Learning Rate ğ›½ K (Corresponding to the time step ğ‘‡)
CAD-NetCora
0.01 0.8 6
Citeseer 0.03 0.7 3
Pubmed 0.03 0.85 8
Amazon â€˜Photoâ€™ 0.03 0.95 2
Amazon â€˜Computersâ€™ 0.03 0.95 2
GRANDCora
0.01 - 18
Citeseer 0.01 - 8
Pubmed 0.03 - 13
Amazon â€˜Photoâ€™ 0.03 - 4
Amazon â€˜Computersâ€™ 0.03 - 3
Ogbn-arxiv 0.005 - 4
FGNDCora
0.01 0.1 19
Citeseer 0.01 0.15 9
Pubmed 0.03 1 14
Amazon â€˜Photoâ€™ 0.03 0.8 5
Amazon â€˜Computersâ€™ 0.03 0.75 4
Ogbn-arxiv 0.005 0 5
Squirrel 0.017 0.95 8
Chameleon 0.006 0.96 3
GRAND++Cora
0.01 - 18.3
Citeseer 0.01 - 8
Pubmed 0.01 - 13
Amazon â€˜Photoâ€™ 0.01 - 3.6
Amazon â€˜Computersâ€™ 0.01 - 3.2
Ogbn-arxiv 0.005 - 3.6
GREADCora
0.01 - 3.79
Citeseer 0.002 - 2.03
Pubmed 0.01 - 1.73
Amazon â€˜Photoâ€™ 0.01 - 3.6
Amazon â€˜Computersâ€™ 0.01 - 3.2
Ogbn-arxiv 0.01 - 3.8
Table 9: Performance of different graph neural networks on
the Ogbn-arxiv dataset.
GRAND
GCN GraphSage GAT Chebynet GREAD CAD-Net GRAND++ FGND
A
ccuracy 69.40% 60.30% 62.30% 66.10% 63.73% N/A N/A 69.40% 69.70%A.2 Theoretical analysis related to the
parameters
Table 10 below shows the statistical values representing the weights
of edges in the reconstructed topology structure. This indicates
how reconstructing topological relationships based on node latent
representations effectively differentiates the topology of the original
graph. This also aligns with observations from Table 3 in the paper,
where it is noted that reconstruction reduces the weights of inter-
class connections and enhances those of intra-class connections.
The Role of Parameter ğ›¾
According to the equation
X(ğ‘‡+1)=(1âˆ’ğ›¾)X(ğ‘‡)+ğ›¾âˆ«ğ‘‡+1
ğ‘‡ğœ•X(ğ‘¡)
ğœ•ğ‘¡ğ‘‘ğ‘¡, (14)
ğ›¾controls the weighted balance between the current node features
and neighboring node features. When ğ›¾approaches 0, the model
preserves the original node features predominantly. Conversely,
whenğ›¾approaches 1, the model aggregates neighboring node fea-
tures more significantly. Adjusting ğ›¾can optimize the information
propagation effect by controlling the relative contributions of node
features and neighboring node features during the aggregation
process.
The Role of Parameter ğ›½
According to the equation
ğ›¾=(1âˆ’ğ›½)Â·sim+ğ›½, (15)
ğ›½adjusts the dependence of ğ›¾on the similarity measure sim. A
smallerğ›½indicates a stronger reliance on sim, allowing ğ›¾to be
largely determined by the similarity of node features. Conversely,
a largerğ›½makes the model more independent of sim, making ğ›¾
predominantly determined by ğ›½. Thus, adjusting ğ›½allows the model
to adapt to different datasets and task requirements by controlling
its dependence on the similarity measure during the information
aggregation process.
A.3 Theoretical justifications elucidating why
FGND can alleviate the oversmoothing issue
One significant cause of oversmoothing is the presence of numer-
ous erroneous connections in the graph topology, which leads to
a reduction in node embeddings when aggregating information.
These noises propagate between layers, resulting in oversmoothing
issues.
The theoretical foundation for combating oversmoothing using
partial differential equations and graph diffusion methods based on
node embeddings lies in their effectiveness in filtering out noise in
the graph topology. This prevents misaggregation of node informa-
tion due to erroneous inter-class connections and facilitates better
aggregation of neighbor node information. Moreover, the numer-
ical solution methods for graph diffusion processes also include
residual connections similar to those in ResNet.
 
2946KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liangtian Wan et al.
Taking the following formula as an example, in GCN, the aggre-
gation of neighboring nodes is equivalent to:
X(ğ‘‡+1)=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡). (16)
While the explicit Euler method, as the simplest form of PDE-
based approach, adds residual connections on top of GCN:
X(ğ‘‡+1)=X(ğ‘‡)+Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡). (17)
For more complex Runge-Kutta numerical integration methods:
K1=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡)=Î”ğ‘‡Â·A(x(ğ‘‡))x(ğ‘‡),
K2=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡+Î”ğ‘‡
2)=Î”ğ‘‡Â·A(x(ğ‘‡+Î”ğ‘‡
2))x(ğ‘‡+Î”ğ‘‡
2),
K3=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡+Î”ğ‘‡
2)=Î”ğ‘‡Â·A(x(ğ‘‡+Î”ğ‘‡
2))x(ğ‘‡+Î”ğ‘‡
2),
K4=Î”ğ‘‡Â·ğœ•
ğœ•ğ‘¡x(ğ‘‡+Î”ğ‘‡)=Î”ğ‘‡Â·A(x(ğ‘‡+Î”ğ‘‡))x(ğ‘‡+Î”ğ‘‡),
X(ğ‘‡+1)=X(ğ‘‡)+1
6Â·(K1+2K2+2K3+K4).(18)This can be understood as handling temporal models in a manner
similar to long short-term memory networks, using high-precision
numerical solution methods to better aggregate information from
different order neighbors. This allows the model to learn local
information more effectively, capturing more multi-hop neighbor
information while still remembering local details as layers stack.
It is worth noting that in FGND, we distinguish the aggregation
weights for different adjacency relationships. If too much noise
information is incorrectly aggregated in one layer, it will gradu-
ally accumulate with the increase in layers, eventually leading to
oversmoothing. We have also supplemented related analyses in
A.2, demonstrating that our model can effectively assign different
weights to intra-class and inter-class connections, thereby filtering
out this part of noise.
 
2947