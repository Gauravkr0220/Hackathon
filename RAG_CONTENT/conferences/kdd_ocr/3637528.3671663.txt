Preventing Strategic Behaviors in Collaborative Inference for
Vertical Federated Learning
Yidan Xing
Shanghai Jiao Tong University
Shanghai, China
katexing@sjtu.edu.cnZhenzhe Zhengâˆ—
Shanghai Jiao Tong University
Shanghai, China
zhengzhenzhe@sjtu.edu.cnFan Wu
Shanghai Jiao Tong University
Shanghai, China
fwu@cs.sjtu.edu.cn
ABSTRACT
Vertical federated learning (VFL) is an emerging collaborative ma-
chine learning paradigm to facilitate the utilization of private fea-
tures distributed across multiple parties. During the inference pro-
cess of VFL, the involved parties need to upload their local embed-
dings to be aggregated for the final prediction. Despite its remark-
able performances, the inference process of the current VFL system
is vulnerable to the strategic behavior of involved parties, as they
could easily change the uploaded local embeddings to exert direct
influences on the prediction result. In a representative case study
of federated recommendation, we find the allocation of display op-
portunities to be severely disrupted due to the partiesâ€™ preferences
in display content. In order to elicit the true local embeddings for
VFL system, we propose a distribution-based penalty mechanism
to detect and penalize the strategic behaviors in collaborative infer-
ence. As the key motivation of our design, we theoretically prove
the power of constraining the distribution of uploaded embeddings
in preventing the dishonest parties from achieving higher utility.
Our mechanism leverages statistical two-sample tests to distinguish
whether the distribution of uploaded embeddings is reasonable, and
penalize the dishonest party through deactivating her uploaded
embeddings. The resulted mechanism could be shown to admit
truth-telling to converge to a Bayesian Nash equilibrium asymp-
totically under mild conditions. The experimental results further
demonstrate the effectiveness of the proposed mechanism to reduce
the dishonest utility increase of strategic behaviors and promote
the truthful uploading of local embeddings in inferences.
CCS CONCEPTS
â€¢Theory of computation â†’Algorithmic game theory and
mechanism design; â€¢Computing methodologies â†’Machine
learning.
KEYWORDS
Collaborative Inference; Strategic Behaviors; Mechanism Design;
Vertical Federated Learning;
âˆ—Zhenzhe Zheng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671663ACM Reference Format:
Yidan Xing, Zhenzhe Zheng, and Fan Wu. 2024. Preventing Strategic Be-
haviors in Collaborative Inference for Vertical Federated Learning. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671663
1 INTRODUCTION
With the development of machine learning techniques, the con-
sensus that richer features and more available data could enhance
prediction performances has been widely established. In recent
years, federated learning (FL) is proposed as a cutting-edge col-
laborative machine learning paradigm to take advantage of the
distributed data while protecting data privacy. The FL techniques
are generally classified into horizontal FL (HFL) and vertical FL
(VFL) [ 18] according to the distributed patterns of data. Target-
ing at the scenario with each party holding different features for
an aligning set of samples, VFL requires each involved party to
implement a local model which maps her local features to local
embeddings, and requires the server to implement a top model
that maps the aggregated local embeddings uploaded by the parties
to the final prediction result (Figure 1). VFL techniques have been
widely deployed in various scenarios, especially in recommendation
system [16, 38], online advertising [20, 37], and finance [5, 6].
Despite the promising performances of VFL, we notice an un-
explored deficiency of this collaborative paradigm: the inference
process in VFL is vulnerable to the strategic manipulations on the
uploaded local embeddings. Compared to HFL and centralized ma-
chine learning methods, the inference process in VFL requires each
involved party to collaboratively upload the local embeddings for
the current sample. As the learning models have been determined
at the inference stage, the local embedding uploaded by one in-
volved party could exert direct influences on the inference result,
leaving chances for the party to manipulate the inference result in
an predictable way. On the other hand, the involved parties may
indeed have the motivations to strategically change the inference
results towards their desired ones. For example, an organization
may prefer to create better prediction for content belong or rele-
vant to it when providing user behavioral feature embeddings to a
recommendation system, and a bank may prefer to misguide other
banks to provide lower loan limit for factually credible clients, with
the aim to attract those clients and promote its own transactions.
In this work, we aim to formally investigate such kinds of strate-
gic behaviors and the corresponding manipulation-resistant mech-
anism when collaborative inferences meet the strategic intentions
of involved parties in VFL system. To characterize the behavioral
pattern of involved parties, we resort to the celebrated concept of
utility function andNash equilibrium in game theory to describe
3574
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
Features  ğ’™ğ‘¨ Features  ğ’™ğ‘© Features  ğ’™ğ‘ªğœ½ğ‘¨ğœ½ğ‘© ğœ½ğ‘ªğ’‰ğ‘¨(ğœ½ğ‘¨;ğ’™ğ‘¨)ğ’‰ğ‘©(ğœ½ğ‘©;ğ’™ğ‘©) ğ’‰ğ’„(ğœ½ğ’„;ğ’™ğ’„)
Top Model
Utilityğ’‰ğŸ(ğ’•)
ğ’•ğ‘¨ğ’•ğ‘© ğ’•ğ‘ª 
ğ’–ğ‘¨(ğ’‰ğŸ(ğ’•))Utility
ğ’–ğ‘©(ğ’‰ğŸ(ğ’•))Utility
ğ’–ğ‘ª(ğ’‰ğŸ(ğ’•))ğ’•ğ’Š=ğ’‰ğ’Šğœ½ğ’Š;ğ’™ğ’Š?
Figure 1: Overview of VFL System. The local embeddings up-
loaded by a party may differ from the true local embeddings.
the objective of the parties and a stable state of the strategic inter-
actions, respectively.
In order to inspect the potential consequences of such strategic
behaviors, we formulate a representative federated recommenda-
tion game between organizations who collaborate to provide item
recommendation for users. The utilities of these organizations are
defined as the exposure of item owned by them, and they could
manipulate the uploaded user embeddings to change the predicted
score and the allocation of exposure. Prominently, we find the re-
sulted pure Nash equilibrium would indistinguishably allocate the
exposure opportunities in a random way when there are two or-
ganizations in the game with comparable power, regardless of the
properties of items they owned. In other words, when parties are
obsessed with manipulating the uploaded embeddings for their
own utility, the design of recommendation system would losses its
original spirit, necessitating a manipulation-resistant mechanism
against such strategic behaviors in VFL system.
While the strategic behaviors essentially arises from the incon-
sistency between the utilities of parties and the objective of VFL
system, a natural idea is to introduce external monetary transfer to
cover the misalignment between them following the mainstream
of incentive mechanism design [ 40]. However, even if we do not
consider the implementation practicability of a monetary transfer
mechanism within the VFL system, its working principle would
be unaffordable in our context. Since we need to preserve the cor-
rectness of the inference results, the final predictions could not be
modified in any form to satisfy strategic intentions of parties. To
elicit the true local embeddings, the money transfer mechanism
should guarantee the sum of monetary reward and the utility of
current prediction to be larger than any other strategy that may
modify the true embeddings, thus requiring this sum to be at least
the utility of the best possible prediction achievable through manip-
ulation. As a result, the inference result worse for a party should
simultaneously bring her larger monetary reward, leading the re-
sulted expenditure to be incredibly large for the server and highly
fluctuating on the distribution of inference results.Given the infeasibility of adopting external monetary rewards, a
manipulation-resistant mechanism have to be implemented fully
based on the collaborative inference process. Due to the intrinsic
uncertainty of data, it is generally impossible to confirm whether
a specific local embedding has been manipulated, which compels
us to consider utilizing the historical statistics of the embeddings
to identify and constrain strategic behaviors. Nevertheless, it is
unclear what kinds of statistics should be adopted among the mass
of candidates, and whether these metrics could indeed help with
our goal to prevent the considered strategic behaviors.
Inspired by the traditional economics literature [ 17], we con-
sider the distribution information of local embeddings as a strong
candidate to serve as the cornerstone of our manipulation-resistant
mechanism. In particular, when the distribution of uploaded infer-
ence embeddings are enforced to align with its prior distribution,
we demonstrate that the involved parties are unable to realize any
dishonest utility increase in collaborative inference under mild
assumptions describing the partial alignment between the server
prediction function and the utilities of parties, and the training
embeddings could also serve as the reference for prior distribution.
Despite these strong guarantees, due to the high dimensional
nature of local embeddings in VFL applications, it is implausible
for the server to realize precise restriction on the distribution of
uploaded embeddings, thus initiates our final design of distribution-
based penalty mechanism. Our mechanism works in alternative
between two process: detection of potential strategic behaviors
and penalization for the detected strategic behaviors. During the
collaborative inference process, we periodically detect whether the
uploaded embeddings follow the same distribution as the training
embeddings with high probability, which is realized through apply-
ing statistical two-sample tests [ 12,21], and temporarily deactivate
the embeddings uploaded by a party as penalty when she is detected
to be cheating. We theoretically prove the convergence of truth-
telling strategies to a Bayesian Nash equilibrium in the large sample
limit under appropriate mechanism parameters, which underscores
the rationality and efficacy of our design. To further validate its em-
pirical performances, we conduct extensive experiments to observe
the influences of our penalty mechanism on different strategies. It
turns out the utility of various manipulating strategies could be
largely reduced to be similar or less than the utility of truth-telling
strategy, thus effectively alleviating the incentives of parties to
conduct strategic behaviors.
The main contributions of this work are summarized as follows:
â€¢We consider the strategic behaviors to manipulate the local em-
beddings in collaborative inference for VFL system, which are
unexplored in previous work. In a representative federated rec-
ommendation game, we demonstrate the destructive effects of
strategic behaviors on prediction results when involved parties
achieve a Nash equilibrium.
â€¢We propose the distribution-based penalty mechanism as a flex-
ible plug-in module of the vanilla VFL algorithm to prevent the
considered strategic behaviors of manipulating local embed-
dings. With theoretically motivated design, the truth-telling
strategy would converge to Beyesian Nash equilibrium under
large sample limit, thus alleviates the strategic incentives of
involved parties.
3575Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢We evaluate the proposed penalty mechanisms on public datasets
for two typical manipulation strategies and their probabilistic
variants. The empirical results validate the effectiveness of the
proposed mechanisms in reducing the utility obtained by dis-
honest strategies and promoting the parties to upload true local
embeddings during inferences.
2 RELATED WORK
Since the individual participants in FL usually have their own in-
terests, the incentive and strategic problems in FL has been widely
studied to facilitate the deployment of FL applications [ 18,22,40].
Most of the previous work study the methods to evaluate the contri-
bution of participants as a reference for reward allocation or client
selection [ 8,25,27,32], incentivize the participants to keep active
and dedicated in training [ 13,31,33,41], as well as form coalitions
to achieve better training performances for non-i.i.d. data [ 7,9,10].
As all the above work consider the incentive problem in the training
stage of HFL or VFL, to the best of our knowledge, none of the ex-
isting work has considered the strategic behaviors of manipulating
the intermediate local embeddings uploaded to the server during
collaborative inferences in VFL system, which are orthogonal to
the strategic behaviors in training stage.
In fact, the authors of [ 29] have proposed to utilize the same
embedding manipulation approach from the perspective of a mali-
cious attacker. Particularly, they investigate the set of adversarial
dominating inputs (ADI) in inferences of VFL, such that the other
partyâ€™s influence on the inference result would be negligible. The
attacks on FL system typically aim to replicate representative deep
learning attacks for HFL, including [ 2,4,28,34,36,39], while some
other work attacking the VFL system exploit the characteristics of
splitted model to infer the private features or labels of other partic-
ipants [ 11,19,23,24]. In opposed to these work that aim to protect
VFL system against malicious attackers or honest-but-curious par-
ticipants, our mechanism are designed for strategic participants
with their own utility objectives, which provides a complementary
perspective to protect the well-functionality of VFL system.
The main ideology of our distribution-based penalty mechanism
is motivated by the linking mechanism [ 17] that restricts the total
reported preferences of agents in a sequence of public decision
problems to restrain the strategic behaviors. In detail, the agents
are strictly limited in the frequency of reporting each preference
across the problems. The research on linking mechanism is gradu-
ally progressed in terms of its additional properties, variants and
applications [ 14,26,30,35]. Compared with these work, due to the
high-dimensional and complex nature of intermediate embeddings
in VFL system, we could not learn the exact range and distribution
of the high-dimensional embeddings, leading our mechanism and
analysis to be distinct from the existing work.
3 PRELIMINARIES
Consider a typical VFL system with ğ‘€parties1and a server, where
the role of server could be assumed by one of the involved parties.
The features are vertically distributed across the parties, with each
partyğ‘–privately owns ğ‘ğ‘–features of each sample. We use ğ‘¥ğ‘–âˆˆRğ‘ğ‘–to
1We may use parties with participants interchangeably throughout the work, which
also indicate the organizations in the federated recommendation game of Section 4.denote the local features owned by party ğ‘–for sampleğ‘¥. In order to
realize the collaborative prediction for sample ğ‘¥, each party ğ‘–holds
a set of model parameters ğœƒğ‘–and a corresponding local embedding
functionâ„ğ‘–(Â·), which maps the model parameters and the input
local sample features ğ‘¥ğ‘–to local embeddings. The server holds a
set of model parameters ğœƒ0and a server prediction function â„0(Â·),
which maps the server model parameters and all the uploaded
local embeddings to a prediction in R. As this work focuses on the
strategic behaviors during the collaborative inference, we assume
the embedding functions â„ğ‘–and the server model â„0to be some
predetermined randomized functions, and would not go into details
of the training and communication process.
With the above notations, the collaborative inference process
for an unseen sample ğ‘¥could be formally described as follows: 1)
each partyğ‘–compute its local embedding ğ‘¡ğ‘–:=â„ğ‘–(ğ‘¥ğ‘–)using the
local features ğ‘¥ğ‘–; 2) each party ğ‘–uploadsğ‘¡ğ‘–to the server; 3) the
server computes â„0(ğ‘¡1,...,ğ‘¡ğ‘€)usingğ‘¡ğ‘–; and 4) the prediction result
â„0(ğ‘¡1,...,ğ‘¡ğ‘€)is announced and takes effect. We use Tğ‘–to denote
the space of potential local embeddings of party ğ‘–,i.e.,ğ‘¡ğ‘–âˆˆ Tğ‘–,
and useğ‘¡:=Ãğ‘€
ğ‘–=1ğ‘¡ğ‘–to denote the profile of local embeddings
for all the parties. Similarly, we define the potential space of ğ‘¡as
T:=Ãğ‘€
ğ‘–=1Tğ‘–. We denote the distribution of local embeddings as
ğ‘¡âˆ¼ğ‘“, and assume ğ‘¡ğ‘–âˆ¼ğ‘“ğ‘–is independent with ğ‘¡ğ‘—âˆ¼ğ‘“ğ‘—forğ‘—â‰ ğ‘–.
Since the center has no control over the distributed local features,
a partyğ‘–might upload arbitrary local embeddings within Tğ‘–in
collaborative inference. We use ğœğ‘–to denote the strategy of party ğ‘–
when uploading the embeddings, with ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)characterizes the
probability of uploading embedding ğ‘¡â€²
ğ‘–when the true embedding
isğ‘¡ğ‘–under strategy ğœğ‘–, satisfyingâˆ«
ğ‘¡â€²
ğ‘–âˆˆTğ‘–ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)ğ‘‘ğ‘¡â€²
ğ‘–=1,âˆ€ğ‘¡ğ‘–âˆˆTğ‘–.
The strategy profile of all the parties is denoted as ğœ:=(ğœğ‘–)ğ‘€
ğ‘–=1,
and the corresponding feasible space is denoted as Î£:=Ãğ‘€
ğ‘–=1Î£ğ‘–.
For convenience in notations, we would use subscript âˆ’ğ‘–to denote
the embedding profile or its feasible space for all the parties except
partyğ‘–,e.g.,ğ‘¡âˆ’ğ‘–denotes the profile of uploaded local embeddings
except party ğ‘–, andTâˆ’ğ‘–denotes the corresponding feasible space
ofğ‘¡âˆ’ğ‘–. We useğ¼to denote the special truth-telling strategy, with
ğ¼(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)=1whenğ‘¡â€²
ğ‘–=ğ‘¡ğ‘–, and equals 0otherwise.
Considering that the prediction result of the server would affect
the utility of the involved parties, we use ğ‘¢ğ‘–(â„0(ğ‘¡â€²);ğ‘¡ğ‘–)2to denote
the expected utility of party ğ‘–when the uploaded embedding profile
isğ‘¡â€²and the true local embedding of party ğ‘–isğ‘¡ğ‘–. Therefore, the
expected utility of party ğ‘–when the strategy profile is ğœand the
server prediction function is â„0could be calculated as
ğ‘ˆâ„0
ğ‘–(ğœ)=âˆ«
ğ‘¡âˆˆTâˆ«
ğ‘¡â€²âˆˆTğ‘¢ğ‘–(â„0(ğ‘¡â€²);ğ‘¡ğ‘–)ğœ(ğ‘¡,ğ‘¡â€²)ğ‘‘ğ‘¡â€²ğ‘“(ğ‘¡)ğ‘‘ğ‘¡
withğœ(ğ‘¡,ğ‘¡â€²):=Ãğ‘€
ğ‘–=1ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–), and we may abbreviate â„0when the
context is clear.
In this work, we focus on the solution concept of Nash equilib-
rium to describe the stable state of strategic interactions between
parties. In our context that each party only has incomplete infor-
mation of the local embeddings, we consider a strategy profile
2We assume the utility only depends on the prediction result and the partyâ€™s own
true local embeddings, since the party could not access the other partyâ€™s true local
embeddings, and have to rely on her local information to make decisions.
3576KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
ğœ=(ğœ1,...,ğœğ‘€)to be a Bayesian Nash equilibrium (BNE) if
ğ‘ˆğ‘–(ğœ)â‰¥ğ‘ˆğ‘–(ğœâ€²
ğ‘–,ğœâˆ’ğ‘–),âˆ€ğœâ€²
ğ‘–âˆˆÎ£ğ‘–,ğ‘–âˆˆ[ğ‘€].
In other words, when a strategy profile reaches BNE, none of the
parties could achieve larger expected utility through changing her
strategy, and we desire the truth-telling strategy ğœğ¼=(ğ¼)ğ‘€
ğ‘–=1, to be
a BNE, such that the parties would be incentivized to upload the
true local embeddings and the validity of prediction is preserved.
While studying the BNE requires us to make assumptions on
distribution of embeddings, we may focus on one single round of
inference to enable detailed analysis of the strategic interactions
for specific embeddings (Section 4). Under this situation, when
each party chooses a certain local embedding (instead of a distribu-
tion over potential embeddings) for uploading, and the uploaded
embedding profile ğ‘¡â€²satisfies
ğ‘¢ğ‘–(â„0(ğ‘¡â€²
ğ‘–,ğ‘¡â€²
âˆ’ğ‘–);ğ‘¡ğ‘–)â‰¥ğ‘¢ğ‘–(â„0(ğ‘¡â€²â€²
ğ‘–,ğ‘¡â€²
âˆ’ğ‘–);ğ‘¡ğ‘–),âˆ€ğ‘¡â€²â€²
ğ‘–âˆˆTğ‘–,ğ‘–âˆˆ[ğ‘€],
thenğ‘¡â€²is called a pure-strategy Nash equilibrium (PNE). The PNE
in each round of inference is a stronger equilibrium notion than
the BNE over expectation of all the inference rounds, but is also
more difficult and sometimes infeasible to achieve.
4 FEDERATED RECOMMENDATION GAME
Since our discussions until now stay on an abstract level, we would
review a representative application of VFL system to illustrate the
potential strategic behaviors of involved parties more concretely.
As briefly discussed in Section 1, an arising application of VFL is
to aggregate user behavioral features from different organizations
to provide better recommendation results for users [ 16,38], which
we term as federated recommendation. The strategic incentives of
the organizations to manipulate the prediction results naturally
arise here, as each organization prefers to display content beneficial
for them. For example, some candidate items may originate from
one of the organizations or contain content relevant to its business
goal, which are more favorable for the organization to display.
To capture the key idea of this scenario, we assume each orga-
nization owns one unique item and aims to maximize the display
probability for this item [ 3,15] in federated recommendation, which
is a moderate amplification of the competition faced by collabo-
rating parties in practice. Following the literature studying games
between content creators in recommendation system [ 15], we focus
on the popular class of factorization-based recommendation algo-
rithms. That is, each organization uploads the computed local user
embeddingğ‘¡ğ‘–, and the server would use the product of the averaged
user embedding and the item embedding ğ‘ğ‘–of the item owned by
organization ğ‘–as the matching score between the current user and
itemğ‘–. Suppose the server adopts a softmax policy of matching
scores to display items, the expected utility (display probability) of
each organization could then be calculated as
ğ‘¢ğ‘– â„0(ğ‘¡â€²)=exp(ğœâˆ’1ğ‘ ğ‘–)Ã
ğ‘—âˆˆ[ğ‘€]exp(ğœâˆ’1ğ‘ ğ‘—),
whereğ‘ ğ‘–:=âŸ¨ğ‘ğ‘–,Ã
ğ‘–ğ‘¤ğ‘–ğ‘¡â€²
ğ‘–âŸ©is the predicted matching score between
the item of party ğ‘–and the current user, ğ‘¤ğ‘–denotes the aggregation
weight for local embedding of party ğ‘–, andğœ>0is the temperature
parameter to control exploration in recommendation. Since this
utility term does not depend on the true embeddings of parties, wedropğ‘¡ğ‘–from the notation of ğ‘¢ğ‘–. We restrictâˆ¥ğ‘¡ğ‘–âˆ¥2â‰¤1, or otherwise
the party may report âˆ¥ğ‘¡ğ‘–âˆ¥2â†’âˆ to increase its influence on the
aggregated embedding. We use ğ‘ğ‘–ğ‘˜andğ‘¡ğ‘–ğ‘˜to denote the ğ‘˜ğ‘¡â„entry
ofğ‘ğ‘–andğ‘¡ğ‘–, respectively, and denote the number of dimensions of
ğ‘ğ‘–,ğ‘¡ğ‘–asğ‘‘.
In order to evaluate the consequences of strategic interactions
for a specific profile of item and user embeddings, we would analyse
the PNE resulted from the above utility function. If a PNE exists
in the game and truth-telling does not constitute a PNE, then it
indicates the parties would not conform to the truth-telling strategy,
but would instead follow the behavior characterized by the PNE(s).
Due to the limitation of space, the detailed proofs of our results in
Section 4 and 5 are presented in Appendix A.
Theorem 4.1. A PNE always exists in the federated recommenda-
tion game. Moreover, when ğ‘€=2, for anyğ‘1â‰ ğ‘2and any positive
weights, the unique PNE in the corresponding federated recommenda-
tion game is
âˆ€ğ‘˜âˆˆ[ğ‘‘]:ğ‘¡1ğ‘˜=âˆ’ğ‘¡2ğ‘˜=ğ‘1ğ‘˜âˆ’ğ‘2ğ‘˜
Ãğ‘‘
ğ‘˜â€²=1(ğ‘1ğ‘˜â€²âˆ’ğ‘2ğ‘˜â€²)21
2,(1)
Specifically, when ğ‘¤1=ğ‘¤2=1
2, the display probabilities would be
ğ‘¢1=ğ‘¢2=1
2for any item embeddings ğ‘.
In Theorem 4.1, the general PNE existence result is proved
through showing the quasi-concavity of the utility function in the
current setting and applying the Debreu-Glicksberg-Fan existence
theorem. For the uniqueness result when ğ‘€=2, we first show any
interior point with âˆ¥ğ‘¡ğ‘–âˆ¥<1could not be an equilibrium, then de-
rive the detailed expressions of PNE through Karushâ€“Kuhnâ€“Tucker
conditions. As we could observe from Theorem 4.1, despite the
existence of PNE, its uniqueness when ğ‘€=2suggests the failure
of truth-telling strategy to be adopted by the organizations, and the
resulted recommendation outcomes are significantly skewed by the
utility-driven uploading of the involved parties. For the extreme
case ofğ‘€=2andğ‘¤1=ğ‘¤2=1
2, the original properties of the user
and item embeddings are completely disregarded, and the parties
would get equal display chances for any user, leading the federated
recommendation to lose its original design purpose.
While the federated recommendation system display poor per-
formances against the partiesâ€™ strategic behaviors, similar situations
are not minority among the general VFL systems. Since the design
philosophy of the VFL system is to aggregate valuable distributed
features from each party to improve the prediction accuracy, the pre-
diction results need to depend on the precise embeddings uploaded
by the parties, and it is thus generally impossible for a standard VFL
algorithm to prevent strategic manipulation in inferences. As the
system designer, we should not assume that all the participants are
disinterested with the prediction results and refrain from exploiting
the vulnerabilities of the system, but instead need to establish effec-
tive manipulation-resistant mechanisms to mitigate the potential
risks brought by strategic behaviors in collaborative inference.
5 METHODOLOGY
In this section, we would introduce our design of distribution-based
penalty mechanism to prevent the strategic behaviors in collabora-
tive inference, along with the corresponding design considerations.
3577Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
5.1 Theoretical Basis
As indicated by the name of our mechanism, we adopt the distri-
bution of uploaded local embeddings as the criteria to detect and
penalize the strategic behaviors of involved parties. This choice is
motivated by the traditional economics literature [ 17] on linking a
sequence of public decision problems and restricting the number
of reported preferences to overcome incentive issues.
Intuitively, monitoring the distribution of embeddings could ef-
fectively prevent the partiesâ€™ strategic behaviors to always upload
local embeddings from a specific set which are known to have
higher probability of producing better inference results. To formal-
ize the guarantees provided by constraining distribution of em-
beddings as suggested by this intuition, we require two additional
conditions to facilitate rigorous theoretical proofs in our context:
independence in distribution of local embeddings, and the standard
en-ante Pareto efficiency [1] of the server prediction function.
Definition 5.1. A server prediction function â„0is ex-ante Pareto
efficient for the utility functions ğ‘¢=(ğ‘¢ğ‘–)ğ‘€
ğ‘–=1and probability density
functionsğ‘“if there does not exist an alternative server prediction
functionâ„â€²
0such that
âˆ«
ğ‘¡âˆˆTğ‘¢ğ‘–(â„0(ğ‘¡);ğ‘¡ğ‘–)ğ‘“(ğ‘¡)ğ‘‘ğ‘¡â‰¤âˆ«
ğ‘¡âˆˆTğ‘¢ğ‘–(â„â€²
0(ğ‘¡);ğ‘¡ğ‘–)ğ‘“(ğ‘¡)ğ‘‘ğ‘¡,âˆ€ğ‘–,
or equivalently, ğ‘ˆâ„0
ğ‘–(ğ¼ğ‘€)â‰¤ğ‘ˆâ„â€²
0
ğ‘–(ğ¼ğ‘€), and the inequality is strict for
someğ‘–.
In plain words, a server prediction function satisfies ex-ante
Pareto efficiency if there does not exist other server prediction func-
tion, such that every participantâ€™s expected utility under true local
embeddings keeps non-decreasing, and at least one participantâ€™s ex-
pected utility strictly increases. Typical examples for ex-ante Pareto
efficiency are the server prediction function always maximizes
the sum of participantsâ€™ utilities, or the server prediction function
uniquely optimizes the utility function for one of the participants.
As a more concrete example, in the federated recommendation
scenario, as long as the recommendation system always allocate
the full portion of display opportunities to the participants, then
any server prediction function utilized during this allocating pro-
cess would be ex-ante Pareto efficiency. This is because any server
prediction function always trivially maximize the sum of utility
of participants to be equal to one. As the distribution of uploaded
local embeddings is a key measure for us, we define the marginal
embedding distribution of a strategy ğœğ‘–asğ‘“ğœğ‘–
ğ‘–, with
ğ‘“ğœğ‘–
ğ‘–(ğ‘¡â€²
ğ‘–)=âˆ«
ğ‘¡ğ‘–âˆˆTğ‘–ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)ğ‘“ğ‘–(ğ‘¡ğ‘–)ğ‘‘ğ‘¡ğ‘–.
Theorem 5.2. When each participant ğ‘–â€™s strategy is restricted to
{ğœğ‘–:ğ‘“ğœğ‘–
ğ‘–=ğ‘“ğ‘–}and the server prediction function â„0is ex-ante Pareto
efficient onğ‘¢andğ‘“, then the truth-telling strategy {ğ¼ğ‘€}is a BNE.
By Theorem 5.2, under the independence and ex-ante Pareto
efficiency conditions, if each participantâ€™s uploaded embeddings
are strictly constrained to align with their prior distributions, then
no participant could achieve higher utility through manipulating
the uploaded embeddings when all the other participants adopt
the truth-telling strategy. To prove Theorem 5.2, we note the in-
dependence in distributions of ğ‘¡ğ‘–would further indicate a relativeindependence in utility when the marginal distributions of all the
parties are constrained to align with the prior distributions, i.e., for
any strategy profile ğœsatisfyingğ‘“ğ‘–=ğ‘“ğœğ‘–
ğ‘–for each participant,
ğ‘ˆğ‘–(ğœğ‘–,ğœâˆ’ğ‘–)=ğ‘ˆğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1),âˆ€ğ‘–.
As a result, when all the other participants adopt the truth-telling
strategy, their expected utilities are guaranteed to keep stable re-
gardless of the detailed reporting of a specific participant. If some
participantğ‘–could realize a strict utility increment through chang-
ing her strategy, the ex-ante Pareto efficiency of the server predic-
tion function would be broken, thus creates a contradiction.
Although strong guarantees of preventing strategic behaviors
could be provided by the ex-ante Pareto efficiency, this condition
might not always hold in reality. For example, when server predic-
tion function is designed to optimize the accuracy of prediction
and does not prioritize maximizing the utility function of involved
parties, the condition of ex-ante Pareto efficiency would not hold.
Therefore, we would like to investigate the guarantees that con-
straining{ğœğ‘–:ğ‘“ğœğ‘–
ğ‘–=ğ‘“ğ‘–}could provide for more general server
prediction functions. Since we are now under much weaker as-
sumptions on â„0, we focus on the specific form of linear utility
functions
ğ‘¢ğ‘–(â„0(ğ‘¡â€²);ğ‘¡ğ‘–)=ğ‘¥â„0
ğ‘–(ğ‘¡â€²)Â·ğ‘£ğ‘–(ğ‘¡ğ‘–),
whereğ‘¥ğ‘–is a function dependent on â„0. That is, we assume each
prediction result ğ‘¡â€²bringsğ‘¥â„0
ğ‘–(ğ‘¡â€²)unit of valuable item (utility
increase) to participant ğ‘–, and the detailed amount of per-unit item
value depends on participant ğ‘–â€™s true local embedding in the form
ğ‘£ğ‘–(ğ‘¡ğ‘–). The linear utility function is widely-adopted in economics.
Theorem 5.3. Assume that the distributions of local embeddings
are discrete. For a strategic participant ğ‘–with linear utility function,
her utility could not be increased by using any ğœğ‘–â‰ ğ¼when each
participant ğ‘—â€™s strategy is constrained within the range {ğœğ‘—:ğ‘“ğœğ‘—
ğ‘—=
ğ‘“ğ‘—}, ifâˆ€ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–âˆˆTğ‘–withğ‘£ğ‘–(ğ‘¡1
ğ‘–)â‰¥ğ‘£ğ‘–(ğ‘¡2
ğ‘–),
Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡1
ğ‘–,ğ‘¡âˆ’ğ‘–)]â‰¥Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡2
ğ‘–,ğ‘¡âˆ’ğ‘–)]. (2)
To ensure the constraint on marginal distribution is sufficient to
prevent dishonest utility increase, conditions (2) require a mono-
tone property between the expected allocation Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡ğ‘–,ğ‘¡âˆ’ğ‘–)]
and the per-unit value ğ‘£ğ‘–(ğ‘¡ğ‘–)brought by a user with local embedding
ğ‘¡ğ‘–. That is, a user (or other subject of prediction task) who would
bring higher per-unit utility ğ‘£ğ‘–(ğ‘¡ğ‘–)for participant ğ‘–, should simulta-
neously receive more expected allocation of items Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡ğ‘–,ğ‘¡âˆ’ğ‘–)]
after the overall evaluation ğ‘¥â„0
ğ‘–. Compared to the Pareto-efficiency
condition, the monotone conditions (2) characterize another kind
of coincidence between the server prediction function and the util-
ity function of the participant. When conditions (2) hold for each
participantğ‘–, Theorem 5.3 would provide the same BNE guarantee
as in Theorem 5.2. We present Theorem 5.3 in the current form
to emphasize its provided guarantees could be flexibly applied to
each individual participant once the conditions hold, which keeps
relevant independent with other participants in comparison to the
ex-ante Pareto-efficiency condition in Theorem 5.2.
3578KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
Algorithm 1: Distribution-Based Penalty Mechanism
Parameters: the test length ğ‘šğ‘–,ğ‘›ğ‘–, and a non-decreasing
penalty function ğ‘˜ğ‘–:[0,1]â†’ R+;
Oracles: A valid two-sample test ğ‘‡ğ‘–forğ‘šğ‘–uploaded
embeddings and ğ‘›ğ‘–training embeddings, and a data
generatorğºğ‘–approximates the distribution of (training)
embeddings;
1Initialize historical rejection rate of two-sample test ğ‘ğ‘–=0;
2Initialize a cache ğ¶ğ‘–;
3while the collaborative inference is ongoing do
4 Add each uploaded embedding to ğ¶ğ‘–, and use the current
uploaded embedding for collaborative inference;
5 ifğ¶ğ‘–is with length ğ‘šğ‘–then
6 Applyğ‘‡ğ‘–to embeddings in ğ¶ğ‘–andğ‘›ğ‘–random
training embeddings;
7 Clearğ¶ğ‘–and updateğ‘ğ‘–;
8 ifğ‘‡ğ‘–rejects the null hypothesis then
9 forğ‘—=1,...,ğ‘˜ğ‘–(ğ‘ğ‘–)do
10 Deactivate the embedding uploaded by
participantğ‘–and use embeddings generated
byğºğ‘–as substitute for each inference;
5.2 Distribution-Based Penalty Mechanism
Despite the promising guarantees provided by constraining the
distributions of uploaded embeddings to align with its prior dis-
tribution, it is infeasible for the server to exactly implement this
constraint for high-dimensional local embeddings uploaded by the
parties. Although we could regard the distribution of training em-
beddings as an effective approximate to the prior distribution, en-
forcing the involved parties to upload embeddings exactly match
with the training embeddings would lead the uploaded embeddings
to be substantially different from the true local embeddings, and
spoil the generalization capability of the VFL model. To adequately
harness the efficacy of local embedding distributions in preventing
strategic behaviors, we allow arbitrary local embeddings (within its
domain) to be uploaded by the parties, and adopt additional design
to reduce the incentives of conducting strategic behaviors through
penalizing the problematic distributions.
The formal process of the distribution-based penalty mechanism
is presented in Algorithm 1, which works individually for each
partyğ‘–. During the collaborative inference process, Algorithm 1
repeatedly collect embeddings uploaded by party ğ‘–to distinguish
whether a sequence of ğ‘šğ‘–uploaded embeddings comes from the
same distribution of ğ‘›ğ‘–(randomly sampled) training embeddings
with high probability. We leverage corresponding methods in sta-
tistical literature to realize this detection task, technically termed
astwo-sample tests. If the null hypothesis that the two groups of
samples come from the same distribution is rejected in the two-
sample test for party ğ‘–, this indicates party ğ‘–has likely manipulated
the uploaded local embeddings, and we would thus apply a penalty
period to party ğ‘–. During the penalty period, each uploaded em-
bedding of participant ğ‘–is deactivated and substituted with the
random embeddings (output by a generator ğºğ‘–) to eliminate her
Deactivate  the uploaded 
embeddings for  ğ‘˜ğ‘–ğ‘ğ‘–
inferencesğ‘›ğ‘– Training Embeddings
ğ‘šğ‘– Inference Embeddings
(Cached when the inference is 
ongoing)Two -
Sample 
Test
AcceptReject
Feed the uploaded 
embeddings to the 
server model  normallyFigure 2: Illustration of Distribution-Based Penalty Mecha-
nism for Party ğ‘–
influences on the prediction results, and meanwhile leads her ex-
pected utility to decrease. The length of the current penalty period
is calculated by the historical rejection rate ğ‘ğ‘–and the pre-designed
penalty function ğ‘˜ğ‘–non-decreasing in ğ‘ğ‘–. In principle, we desire
the generator ğºğ‘–to approximate the prior distribution of party ğ‘–â€™s
local embeddings, which could be realized by randomly drawing
samples from the training embeddings.
As two-sample test is the key component to detect the consis-
tency of distribution in our penalty mechanism, the analysis of
our mechanism needs to depend on the properties of adopted two-
sample tests. Formally speaking, given two groups of samples ğ‘‹âˆ¼ğ‘
with sizeğ‘šandğ‘Œâˆ¼ğ‘with sizeğ‘›, a two-sample test is a statistical
testğ‘‡(ğ‘‹,ğ‘Œ):ğ‘‹ğ‘šÃ—ğ‘‹ğ‘›â†¦â†’{0,1}to distinguish between the null
hypothesisH0:ğ‘=ğ‘and the alternative hypothesis Hğ´:ğ‘â‰ ğ‘
[12]. Since the test is based on finite samples, it is possible that
errors would be made for some situations. By convention, a type I
error of a two-sample test occurs when the null hypothesis ğ‘=ğ‘is
wrongly rejected based on the observed samples, even though the
data was generated with the same distribution. We define the type I
error rate for a two-sample test ğ‘‡asğ›¼ğ‘‡. Conversely, a type II error
occurs when the null hypothesis ğ‘=ğ‘is accepted on the observed
samples, despite the fact ğ‘â‰ ğ‘. We define the type II error rate of a
two-sample test ğ‘‡against a specific ğ‘â‰ ğ‘asğ›½ğ‘‡(ğ‘).
In our distribution-based penalty mechanism, we require the
adopted two-sample test to satisfy ğ›½ğ‘‡(ğ‘)<ğ›¼ğ‘‡for anyğ‘â‰ ğ‘,i.e.,
the acceptance rate of the null hypothesis is the highest when ğ‘=ğ‘
and be strictly smaller for ğ‘â‰ ğ‘. Since this is a fundamental require-
ment for a well-functioning two-sample test, we call a two-sample
test satisfying the above condition to be valid. Moreover, we also
require each participantâ€™s expected utility to strictly decrease when
her true local embeddings are substituted with random embeddings
(drawn from her prior distribution), which is necessary to ensure
the penalty period could effectively reduce the expected utility of
a participant. We term the problem case (consist of ğ‘¢,ğ‘“andâ„0)
satisfying this utility decrement condition for each party to be feasi-
ble, which could be verified through simulations in practice. When
the above typical conditions hold, the proposed distribution-based
penalty mechanism (Algorithm 1) is able to inherit the guarantees
provided by strictly constraining the distribution (Section 5.1) in
an asymptotic sense.
Theorem 5.4. For any feasible problem case with valid two-sample
testsğ‘‡ğ‘–, supposeâ„0is ex-ante Pareto efficient on ğ‘¢andğ‘“, andğºğ‘–âˆ¼ğ‘“ğ‘–
for each participant ğ‘–, then we could find some penalty functions ğ‘˜ğ‘–(Â·)
3579Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
such that the expected per-round utility of truth-telling strategy {ğ¼ğ‘€}
converges to a BNE with the increase of inference rounds under the
penalty-enabled server prediction function â„âˆ—
0.
Under the stated conditions, Theorem 5.4 guarantees no partic-
ipant could obtain higher expected per-round utility than truth-
telling as the inference proceeds, supposing all the other parties
adopt the truth-telling strategy. The performance guarantee of our
distribution-based penalty mechanism is established on the basis of
results in Section 5.1. In principle, because no party could achieve
larger utility through deviating to a strategy with the same marginal
distribution (Theorem 5.2), the remaining chances to improve utility
fall on the strategies with different marginal distributions. However,
by the validity of two-sample tests, such kinds of strategies would
result in larger historical rejection rate ğ‘ğ‘–and longer penalty period,
thus also brings lower utility in the long term. Whilst Theorem
5.4 is formulated based on Theorem 5.2, an alternative result with
the ex-ante Pareto efficiency condition replaced by conditions (2)
could be formed based on Theorem 5.3. Though our theoretical re-
sults rely on conditions such as Pareto-efficiency and independent
distribution, the principal idea of our design, i.e., monitoring the
distribution of uploaded embeddings, is broadly helpful in restrain-
ing the range of strategic behaviors in collaborative inference, even
if the theoretical conditions are not strictly satisfied. This is also
demonstrated by our experimental results in Section 6.
In both Algorithm 1 and Theorem 5.4, we do not characterize the
detailed form of the penalty function ğ‘˜ğ‘–and the choice of sample
lengthğ‘šğ‘–,ğ‘›ğ‘–in two-sample tests, but instead leave it flexible to
accommodate the need of various scenarios. Choosing a penalty
functionğ‘˜ğ‘–(ğ‘ğ‘–)grows faster with ğ‘ğ‘–could provide stronger guar-
antee against strategic behaviors, but would simultaneously bring
higher risk for honest parties when the number of conducted two-
sample test is small and ğ‘ğ‘–has large variance. A similar tradeoff
exists for the choice of test length ğ‘šğ‘–andğ‘›ğ‘–. While a larger ğ‘šğ‘–
means lower error rate for two-sample tests, a smaller ğ‘šğ‘–allows to
conduct more two-sample tests and get a stable ğ‘ğ‘–, which might be
preferred when the number of total inference rounds is small. To
choose appropriate mechanism parameters in practice, the server
could conduct simulations on training embeddings to estimate the
performances of the considered mechanism.
6 EXPERIMENTS
In the experiments, we aim to validate and investigate the following
questions from an empirical view: (1) whether the considered strate-
gic behaviors in collaborative inference are implementable and
could bring the party substantially higher utility; (2) whether the
proposed distribution-based penalty mechanism could effectively
reduce the involved partiesâ€™ incentives to conduct such strategic
behaviors for practical datasets that not strictly satisfy the theoreti-
cal assumptions; and (3) how to set the parameters in the penalty
mechanism to achieve good performances in practice.
6.1 Experimental Setup
Datasets and VFL Model We conduct experiments on two pub-
lic datasets, Criteo andAvazu, with the task of click-through-rate
(CTR) prediction. We assume there are two parties involved in VFL,
with each party owning half of the features partitioned by theirsequence in dataset. To validate the performances of our design
for VFL models trained with different amount of data, we draw
1,000,000 samples to train and test the VFL model for Avazu, while
the full dataset is available for Criteo. The training and testing
sets are divided with proportion 9:1 for both the datasets. After
the VFL model has been determined, we apply our mechanism on
ğ‘=100,000samples (inference rounds) drawn from the testing
set. We adopt the fully-connected neural network3(FCNN) for both
the parties and the server, with 4 layers for the parties locally and
3 layers for the server, and the sparse features are first processed
with an embedding layer before feeding into the local FCNN. The
intermediate embeddings uploaded by each party are with dimen-
sion 40, which are concatenated to feed into the server network.
Strategic Settings We assume that there exists one strategic party
in the system, which is without loss of generality as our mechanism
works individually for each party. We consider the utility function
of the strategic party to be the form ğ‘¢ğ‘’ğ‘¥ğ‘=Ã
ğ‘—âˆˆ[ğ‘]ğ‘ğ‘¡ğ‘Ÿğ‘—/ğ‘or
ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜=Ã
ğ‘—âˆˆ[ğ‘](ğ‘ğ‘¡ğ‘Ÿğ‘—Â·ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘—)/ğ‘, whereğ‘ğ‘¡ğ‘Ÿğ‘—denotes the predicted
CTR of the ğ‘—ğ‘¡â„test sample, and ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘—denotes its true label. As-
suming that the display opportunity gained by the strategic party
would be equal to the predicted CTR, these two utility functions
represent the typical goal of obtaining more exposure opportunities
and more expected clicks in recommendation.
Manipulation Strategies
â€¢Label-based strategy: Considering that the local features of the
training samples with positive label are likely to increase the pre-
diction of CTR, the label-based strategy samples a local embedding
from the training embeddings with positive labels to upload in each
inference round. This label-based strategy is straightforward to
implement in practice, which only requires the party to know a set
of samples with positive labels.
â€¢Omniscient strategy: In the omniscient strategy, we assume the
strategy of the party is derived by optimizing the total predicted
CTR under â„“2-regularization (applied to the difference between
the original and manipulated embeddings), using the omniscient
knowledge of local embeddings from both parties. To avoid the
less meaningful case that the party extremely increases the scale of
embeddings to dominantly create false-positive cases, we choose a
regularization constant to ensure the resulting strategy achieves a
sufficiently higher utility at an appropriate level. The optimization
is performed using the stochastic gradient descent method.
â€¢Probabilistic Mixtures : To validate our mechanisms against vari-
ous potential strategies, we consider the probabilistic mixtures of
the above two strategies with the true local embeddings, e.g., a
strategy with mixture probability 0.1would report the true embed-
dings with 90%probability, and report according to the label-based
(omniscient) strategy in the remaining 10%probability.
Mechanism Implementation When implementing Algorithm 1,
we adopt the kernel two-sample test based on deep learning [ 21]
with the test length for the training and inference embeddings
set to be equal, i.e.,ğ‘›ğ‘–=ğ‘šğ‘–. To reduce the variance of historical
rejection rate ğ‘ğ‘–in implementation, we postpone all the penalties
to the end of the inference stage, such that once the remaining
3Since the design of our mechanism only concerns the local embeddings uploaded by
the participants, the detailed structure of VFL model would not induce great impacts
on the trend of performances of the proposed mechanism.
3580KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
Table 1: Original Utilities for Different Strategies
Criteo True Omniscient Label-Based Random
ğ‘¢ğ‘’ğ‘¥ğ‘ 0.2424 0.3591 0.3031 0.2345
ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ 0.1024 0.1372 0.1048 0.0836
Avazu True Omniscient Label-Based Random
ğ‘¢ğ‘’ğ‘¥ğ‘ 0.1389 0.2033 0.2761 0.1333
ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ 0.0392 0.0595 0.0478 0.0229
inference rounds are less than the total penalty length calculated
by the latest rejection rate, the party would be penalized in all
the remaining rounds. We choose penalty function in the linear
formğ‘˜ğ‘–(ğ‘ğ‘–)=ğ‘Â·ğ‘›ğ‘–Â·ğ‘ğ‘–,i.e., the penalty length for each rejection
is the partyâ€™s rejection rate times the current test length and a
pre-determined penalty constant ğ‘. The detailed settings of the
penalty constant ğ‘and the test length ğ‘›ğ‘–in the mechanism would
be characterized for each set of experiments.
6.2 Experimental Results
Original Utilities of Different Strategies The original utilities
of different strategies without the penalty mechanism are presented
in Table 1. As we can observe, the omniscient strategy achieves
evident higher utility on both the utility functions for two datasets,
though it only applies small perturbations on true embeddings. The
label-based strategy also achieves evident utility increase except for
ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ in Criteo dataset. The reason might come from the limited
influences of the partyâ€™s local features on the prediction result and
the relatively high proportion of positive samples in Criteo dataset.
Since we would substitute the partyâ€™s original embeddings with
randomly sampled training embeddings as penalty, we also validate
the utility of such random â€œstrategyâ€ on the datasets. We find that
the random strategy would lead to a lower utility for ğ‘¢ğ‘ğ‘¡ğ‘Ÿ, but
achieve a similar (though still lower) utility for ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ compared
with the true embeddings. In other words, using random strategy
would not lead the total exposure of the party to decrease, despite
the resulted mismatch between true label and predicted CTR. As a
result, for parties with utility function ğ‘¢ğ‘’ğ‘¥ğ‘, we could hardly reduce
the utility obtained by strategic manipulations to be smaller than
the utility obtained by true embeddings, and what we could achieve
is to guide the two utilities to be close enough.
Performances of Distribution-Based Penalty Mechanism To
observe the detailed performances of the proposed penalty mecha-
nism, we conduct a set of experiments (Figure 3) that demonstrate
the change in the utility of strategic party when the penalty mecha-
nism is adopted. Due to the different characteristics of two datasets,
we chooseğ‘›ğ‘–=1800 for Criteo dataset, and ğ‘›ğ‘–=600for Avazu
dataset, with both ğ‘=8. As could be observed in Figure 3, the
utilities obtained by different probabilistic mixtures of the omni-
scient (OM) and label-based (LB) strategies are largely reduced to
be similar or less than the utility obtained by truthfully uploading
the embeddings under the penalty mechanism, regardless of the
original utilities obtained by those strategies, which demonstrates
the effectiveness of our mechanism in diminishing the incentives
of parties to adopt strategies other than truthful uploading.
In the trend of penalized utilities for OM and LB strategies on
Avazu dataset, we could observe a slow utility growth after the
	0.24	0.29	0.34	0.39
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty(a)ğ‘¢ğ‘’ğ‘¥ğ‘in Criteo Dataset
	0.08	0.1	0.12	0.14	0.16
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty (b)ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ in Criteo Dataset
	0.12	0.16	0.2	0.24	0.28
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty
(c)ğ‘¢ğ‘’ğ‘¥ğ‘in Avazu Dataset
	0.02	0.03	0.04	0.05	0.06	0.07
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty (d)ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ in Avazu Dataset
Figure 3: Comparison of Utilities obtained by Probabilistic
Mixtures for Label-Based and Omniscient Strategies before
and after the penalty mechanism is enabled, with ğ‘›ğ‘–=1800,
ğ‘=8for Criteo Dataset, and ğ‘›ğ‘–=600,ğ‘=8for Avazu Dataset
mixture probability exceeds 0.4. This is because the mixture strategy
at this point has been penalized in most of the inference rounds, and
the utility increase comes entirely from the beginning inferences
round for conducting the essential two-sample tests. For Criteo
dataset, the utilities of different mixture strategies display slight
fluctuations with the increase of mixture probability, which might
due to the relatively high variance of two-sample tests under a
smaller number of tests for Criteo.
Another important metric we need to observe is the total penalty
length received by true embeddings, as immoderate penalty on
a truthful party can significantly degrade the overall prediction
performances of VFL system when it is not controlled at a relatively
low level. For parameters adopted in Figure 3, the averaged penalty
length received by true embeddings are 5,680for Criteo dataset
and5,560for the Avazu dataset, which is a small and acceptable
penalty length compared to the 100,000samples in total.
Influences of Mechanism Parameters The set of parameters
we adopted in Figure 3 are actually not the deliberately fine-tuned
ones to achieve the best performances. When testing the different
mechanism parameters, we find a broad set of parameters could
achieve satisfying effects around the parameters we present in
Figure 3. Therefore, instead of presenting the similar performances
achieved by successful mechanism parameters, we would like to
demonstrate the importance of tailoring the mechanism parameters
based on the characteristics of datasets and adopted two-sample
tests. As a striking instance, simply applying a small test length to
Criteo dataset and a large test length to Avazu dataset as opposed
to Figure 3, e.g., exchanging the ğ‘›ğ‘–parameters for two datasets,
would largely degrade the mechanism performances. To help with
the evaluation of the mechanismâ€™s overall performances against the
strategic behaviors, we define the metric of utility approximate ratio
ğ›¼to be the largest ratio between the utility obtained by a dishonest
strategy and the utility of true embedding among all the considered
strategies in Figure 3. We regard an utility approximate ratio less
3581Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
	1	1.05	1.1	1.15	1.2	1.25
	4	6	8	10	12	14	16	18	20Î±
Penalty	Constantuexp
uclick
(a)ğ‘›ğ‘–=600with Different
Penalty Constant ğ‘âˆˆ[4,20]in
Criteo Dataset
	0.7	0.8	0.9	1	1.1	1.2
	300 	900	1500	2100	2700Î±
Test	Lengthuexp
uclick(b)ğ‘=8with Different Test
Lengthğ‘›ğ‘–âˆˆ[300,2700]in Avazu
Dataset
Figure 4: Utility Approximate Ratio ğ›¼for Failure Cases in
Criteo and Avazu Dataset
than 1.1to be acceptable for ğ‘¢ğ‘’ğ‘¥ğ‘and 1.05to be acceptable for
ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜, which are both satisfied by the experiments in Figure 3.
In Figure 4a, we report the utility approximate ratios of test
lengthğ‘›ğ‘–=600with different penalty constant ğ‘âˆˆ [4,20]on
Criteo dataset. We can observe that the utility approximate ratio
fails to satisfy the required standard in most cases and gradually
increases with the penalty constant, though with fluctuations. This
is caused by the poor performances of two-sample test with ğ‘›ğ‘–=600
when distinguishing the variants of LB strategies on Criteo dataset,
such that both the true embeddings and the LB variants receives
small penalties. In Figure 4b, we report the utility approximate
ratios ofğ‘=8with different test lengths ğ‘›ğ‘–âˆˆ[300,2700]on Avazu
dataset. Despite the seemingly satisfying results of ğ›¼, the total
penalty received by the true embeddings has generally exceeded
15,000 rounds after ğ‘›ğ‘–â‰¥1500, and the relatively low ğ›¼comes
from applying large penalties for both the truth-telling strategy
and alternative strategies. For its potential causes, the adopted
two-sample test might have relatively high variances on the Avazu
dataset and requires more tests to make ğ‘ğ‘–stable when computing
the penalty length, which could not be provided by ğ‘›ğ‘–â‰¥1500 under
the current number of inference samples.
Simulations of Multi-Party Setup Since our mechanism works
independently for each party irrespective of other partiesâ€™ behav-
iors, we can simulate the performances of our mechanism for the
multi-party setup under the two-party setup. In detail, for any party
in multi-party setup, we could regard all the other parties as a â€œgi-
antâ€ party and run our mechanism only on the local embeddings
uploaded by the considered party. Based on this equivalence prop-
erty, we simulate the 3-party and 4-party setup on Avazu dataset
with two parties, by assuming the strategic party owning (approxi-
mately) 1/3 and 1/4 of the features. We adopt the same mechanism
parameters as in Figure 3c and 3d, and the results are presented
in Table 2. We could observe a significant decrease in the utilities
of the OM and LB strategies under our penalty mechanism (OM-P
and LB-P) compared to their utilities in the vanilla VFL system. In
contrast, the utilities of uploading true local embeddings remained
largely unaffected under the penalty mechanism (True and True-
P), demonstrating the efficacy of our mechanism in various VFL
settings. The discrepancies between utilities obtained by OM and
LB strategies in Table 1 and 2 are likely due to the differences in
the specific features owned by the strategic party and their vary-
ing significance in affecting the final prediction result, and it is
not necessarily the case that owning more features would enableTable 2: Changes in utilities of different strategies when the
penalty mechanism is enabled and the strategic party owns
1/3 and 1/4 of the features for Avazu dataset
1/3 features True OM LB True-P OM-P LB-P
ğ‘¢ğ‘’ğ‘¥ğ‘ 0.1345 0.1449 0.1483 0.1343 0.1278 0.1283
ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ 0.0377 0.0420 0.0371 0.0375 0.0328 0.0323
1/4 features True OM LB True-P OM-P LB-P
ğ‘¢ğ‘’ğ‘¥ğ‘ 0.1390 0.1529 0.1542 0.1389 0.1349 0.1351
ğ‘¢ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ 0.0376 0.0431 0.0369 0.0376 0.0333 0.0326
the party to achieve larger dishonest utility increase when similar
manipulation strategies are adopted (Table 2).
7 CONCLUSION
In this work, we consider the strategic behaviors in collaborative
inference for vertical federated learning, where the parties could
manipulate the uploaded local embeddings to change the inference
results and maximize their own utilities. We model the strategic in-
teractions between parties for a representative federated recommen-
dation application, and our analysis reveals the adverse effects of
the considered strategic behaviors. Specifically, we propose a class
of distribution-based penalty mechanism to prevent such strategic
behaviors. The proposed mechanism works through applying sta-
tistical two-sample tests to distinguish the deviation in embedding
distributions and penalizing the parties based on the test results,
whose performance is theoretically demonstrated. The experimen-
tal results validate the effectiveness of the proposed mechanism in
terms of preventing the considered strategic behaviors.
ACKNOWLEDGEMENT
This work was supported in part by National Key R&D Program
of China (No. 2022ZD0119100), in part by China NSF grant No.
62322206, 62132018, U2268204, 62025204, 62272307, 62372296. The
opinions, findings, conclusions, and recommendations expressed
in this paper are those of the authors and do not necessarily reflect
the views of the funding agencies or the government.
REFERENCES
[1]Kenneth J Arrow. 2012. Social choice and individual values. Vol. 12. Yale University
Press.
[2]Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov. 2020. How to backdoor federated learning. In Proceedings of the 23rd
International Conference on Artificial Intelligence and Statistics. PMLR, 2938â€“2948.
[3]Omer Ben-Porat and Moshe Tennenholtz. 2018. A game-theoretic approach to
recommendation systems with strategic content providers. In Proceedings of the
32nd International Conference on Neural Information Processing Systems. Curran
Associates Inc., Red Hook, NY, USA, 1118â€“1128.
[4]Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
2019. Analyzing federated learning through an adversarial lens. In Proceedings of
the 36th International Conference on Machine Learning. PMLR, 634â€“643.
[5]Chaochao Chen, Jun Zhou, Li Wang, Xibin Wu, Wenjing Fang, Jin Tan, Lei Wang,
Alex X Liu, Hao Wang, and Cheng Hong. 2021. When homomorphic encryption
marries secret sharing: Secure large-scale sparse logistic regression and appli-
cations in risk control. In Proceedings of the 27th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. ACM, 2652â€“2662.
[6]Yong Cheng, Yang Liu, Tianjian Chen, and Qiang Yang. 2020. Federated learning
for privacy-preserving AI. Commun. ACM 63, 12 (2020), 33â€“36.
[7]Sen Cui, Jian Liang, Weishen Pan, Kun Chen, Changshui Zhang, and Fei Wang.
2022. Collaboration Equilibrium in Federated Learning. In Proceedings of the 28th
3582KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
ACM SIGKDD Conference on Knowledge Discovery & Data Mining. ACM, New
York, NY, USA, 241â€“251.
[8]Yongheng Deng, Feng Lyu, Ju Ren, Yi-Chao Chen, Peng Yang, Yuezhi Zhou, and
Yaoxue Zhang. 2021. FAIR: Quality-aware federated learning with precise user
incentive and model aggregation. In Proceedings of the 40th IEEE Conference on
Computer Communications. IEEE, 1â€“10.
[9]Kate Donahue and Jon Kleinberg. 2021. Model-sharing games: Analyzing feder-
ated learning under voluntary participation. In Proceedings of the AAAI Conference
on Artificial Intelligence. AAAI Press, 5303â€“5311.
[10] Kate Donahue and Jon Kleinberg. 2021. Optimality and stability in federated
learning: A game-theoretic approach. In Proceedings of the 35th International
Conference on Neural Information Processing Systems. Curran Associates Inc., Red
Hook, NY, USA, 1287â€“1298.
[11] Chong Fu, Xuhong Zhang, Shouling Ji, Jinyin Chen, Jingzheng Wu, Shanqing
Guo, Jun Zhou, Alex X Liu, and Ting Wang. 2022. Label inference attacks against
vertical federated learning. In 31st USENIX Security Symposium (USENIX Security
22). USENIX Association, 1397â€“1414.
[12] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÃ¶lkopf, and
Alexander Smola. 2012. A kernel two-sample test. The Journal of Machine
Learning Research 13 (2012), 723â€“773.
[13] Jingoo Han, Ahmad Faraz Khan, Syed Zawad, Ali Anwar, Nathalie Baracaldo,
Yi Zhou, Feng Yan, and Ali Raza Butt. 2022. TIFF: Tokenized Incentive for
Federated Learning. In Proceedings of the IEEE 15th International Conference on
Cloud Computing. IEEE, 407â€“416.
[14] Rafael Hortala-Vallve. 2010. Inefficiencies on linking decisions. Social Choice and
Welfare 34, 3 (2010), 471â€“486.
[15] Jiri Hron, Karl Krauth, Michael Jordan, Niki Kilbertus, and Sarah Dean. 2022. Mod-
eling content creator incentives on algorithm-curated platforms. In Proceedings
of the 11th International Conference on Learning Representations.
[16] Yaochen Hu, Di Niu, Jianming Yang, and Shengping Zhou. 2019. FDML: A
collaborative machine learning framework for distributed features. In Proceedings
of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. ACM, 2232â€“2240.
[17] Matthew O Jackson and Hugo F Sonnenschein. 2007. Overcoming incentive
constraints by linking decisions. Econometrica 75, 1 (2007), 241â€“257.
[18] Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al .2021. Advances and open problems in federated learning.
Foundations and Trends in Machine Learning 14, 1â€“2 (2021), 1â€“210.
[19] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Vir-
ginia Smith, and Chong Wang. 2022. Label Leakage and Protection in Two-party
Split Learning. In Proceedings of the 10th International Conference on Learning
Representations.
[20] Wenjie Li, Qiaolin Xia, Hao Cheng, Kouyin Xue, and Shu-Tao Xia. 2022. Vertical
semi-federated learning for efficient online advertising. arXiv preprint (2022).
https://arxiv.org/abs/2209.15635
[21] Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J.
Sutherland. 2020. Learning Deep Kernels for Non-Parametric Two-Sample Tests.
InProceedings of the 37th International Conference on Machine Learning. PMLR,
6316â€“6326.
[22] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye
Ouyang, Ya-Qin Zhang, and Qiang Yang. 2024. Vertical Federated Learning:
Concepts, Advances, and Challenges. IEEE Transactions on Knowledge and Data
Engineering 36, 7 (2024), 3615â€“3634.
[23] Yang Liu, Zhihao Yi, and Tianjian Chen. 2020. Backdoor attacks and defenses in
feature-partitioned collaborative learning. arXiv preprint (2020). https://arxiv.
org/abs/2007.03608
[24] Xinjian Luo, Yuncheng Wu, Xiaokui Xiao, and Beng Chin Ooi. 2021. Feature
inference attack on model predictions in vertical federated learning. In Proceedings
of the 37th IEEE International Conference on Data Engineering. IEEE, 181â€“192.
[25] Hongtao Lv, Zhenzhe Zheng, Tie Luo, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei
Jia, and Chengfei Lv. 2021. Data-free evaluation of user contributions in federated
learning. In 19th International Symposium on Modeling and Optimization in Mobile,
Ad hoc, and Wireless Networks. IFIP, 81â€“88.
[26] Hitoshi Matsushima, Koichi Miyazaki, and Nobuyuki Yagi. 2010. Role of linking
mechanisms in multitask agency with hidden information. Journal of Economic
Theory 145, 6 (2010), 2241â€“2259.
[27] Lokesh Nagalapatti and Ramasuri Narayanam. 2021. Game of gradients: Miti-
gating irrelevant clients in federated learning. In Proceedings of the 35th AAAI
Conference on Artificial Intelligence. AAAI Press, 9046â€“9054.
[28] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy
analysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In 2019 IEEE Symposium on Security and
Privacy. IEEE, 739â€“753.
[29] Qi Pang, Yuanyuan Yuan, Shuai Wang, and Wenting Zheng. 2023. ADI: Adver-
sarial Dominating Inputs in Vertical Federated Learning Systems. In 2023 IEEE
Symposium on Security and Privacy. IEEE Computer Society, Los Alamitos, CA,
USA, 1875â€“1892.[30] AgustÃ­n Santos, Antonio FernÃ¡ndez Anta, JosÃ© A Cuesta, and Luis LÃ³pez FernÃ¡n-
dez. 2016. Fair linking mechanisms for resource allocation with correlated player
types. Computing 98 (2016), 777â€“801.
[31] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan
Kian Hsiang Low. 2020. Collaborative machine learning with incentive-aware
model rewards. In Proceedings of the 37th International Conference on Machine
Learning. PMLR, 8927â€“8936.
[32] Behnaz Soltani, Yipeng Zhou, Venus Haghighi, and John C. S. Lui. 2023. A
Survey of Federated Evaluation in Federated Learning. In Proceedings of the
32th International Joint Conference on Artificial Intelligence. International Joint
Conferences on Artificial Intelligence Organization, 6769â€“6777.
[33] Ming Tang and Vincent WS Wong. 2021. An incentive mechanism for cross-silo
federated learning: A public goods perspective. In Proceedings of the 40th IEEE
Conference on Computer Communications. IEEE, 1â€“10.
[34] Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. 2020. Data poi-
soning attacks against federated learning systems. In 25th European Symposium
on Research in Computer Security. Springer, 480â€“501.
[35] RÃ³bert F Veszteg. 2015. Linking decisions with standardization. Studies in
Microeconomics 3, 1 (2015), 35â€“48.
[36] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. 2020. Attack
of the tails: Yes, you really can backdoor federated learning. In Proceedings of the
34th International Conference on Neural Information Processing Systems. Curran
Associates Inc., Red Hook, NY, USA, 16070â€“16084.
[37] Penghui Wei, Hongjian Dou, Shaoguo Liu, Rongjun Tang, Li Liu, Liang Wang, and
Bo Zheng. 2023. FedAds: A Benchmark for Privacy-Preserving CVR Estimation
with Vertical Federated Learning. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval . ACM,
3037â€“3046.
[38] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. 2022.
FedCTR: Federated Native Ad CTR Prediction with Cross-platform User Behavior
Data. ACM Transactions on Intelligent Systems and Technology 13, 4 (2022), 62:1â€“
62:19.
[39] Chulin Xie, Keli Huang, Pin Yu Chen, and Bo Li. 2020. DBA: Distributed Back-
door Attacks against Federated Learning. In Proceedings of the 8th International
Conference on Learning Representations.
[40] Yufeng Zhan, Jie Zhang, Zicong Hong, Leijie Wu, Peng Li, and Song Guo. 2022. A
survey of incentive mechanism design for federated learning. IEEE Transactions
on Emerging Topics in Computing 10, 2 (2022), 1035â€“1044.
[41] Meng Zhang, Ermin Wei, and Randall Berry. 2021. Faithful edge federated learn-
ing: Scalability and privacy. IEEE Journal on Selected Areas in Communications
39, 12 (2021), 3790â€“3804.
A PROOFS OF RESULTS
For convenience in notations, we may abbreviate the server predic-
tion function â„0in notations when the context is clear.
A.1 Proof for Theorem 4.1
Proof. (Existence of PNE) To show the general existence of PNE,
we apply the Debreu-Glicksberg-Fan PNE existence theorem. That
is, a PNE exists in a game if the following conditions are satisfied:
(1) the strategy space of each player is compact and convex; and (2)
the utility function of each player is continuous and quasi-concave
in her strategy. By our definition of ğ‘¡ğ‘–, it is clear the strategy space
is compact and convex, and ğ‘¢ğ‘–(ğ‘¡ğ‘–,ğ‘¡âˆ’ğ‘–)is continuous in ğ‘¡ğ‘–. It remains
to demonstrate ğ‘¢ğ‘–(ğ‘¡ğ‘–,ğ‘¡âˆ’ğ‘–)is quasi-concave in ğ‘¡ğ‘–.
Sinceğ‘¢ğ‘–(ğ‘¡ğ‘–,ğ‘¡âˆ’ğ‘–)could be fully defined by ğ‘ =(ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘€),
andğ‘ could be obtained by a linear transformation from ğ‘¡ğ‘–=
(ğ‘¡ğ‘–1,ğ‘¡ğ‘–2,...,ğ‘¡ğ‘–ğ‘‘)regardingğ‘¡âˆ’ğ‘–as constants, we only need to show
ğ‘¢ğ‘–(ğ‘¡ğ‘–,ğ‘¡âˆ’ğ‘–)being quasi-concave on ğ‘ . Recall
ğ‘¢ğ‘–(ğ‘¡ğ‘–,ğ‘¡âˆ’ğ‘–)=exp(ğœâˆ’1ğ‘ ğ‘–)Ã
ğ‘—âˆˆ[ğ‘€]exp(ğœâˆ’1ğ‘ ğ‘—),
suppose we have two points ğ‘ andğ‘ â€², such that ğ‘¢ğ‘–(ğ‘ ) â‰¥ğ‘and
ğ‘¢ğ‘–(ğ‘ â€²) â‰¥ğ‘. By definition of ğ‘¢ğ‘–, we would have exp(ğœâˆ’1ğ‘ ğ‘–) â‰¥
ğ‘
1âˆ’ğ‘Â·Ã
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ ğ‘—)
. Taking logarithm on both sides, we have
ğœâˆ’1ğ‘ ğ‘–â‰¥lnğ‘
1âˆ’ğ‘+lnÃ
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ ğ‘—)
.Similarly,ğœâˆ’1ğ‘ â€²
ğ‘–â‰¥lnğ‘
1âˆ’ğ‘+
3583Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
lnÃ
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ â€²
ğ‘—)
.For anyğœ†âˆˆ (0,1), we could thus obtain
ğœâˆ’1(ğœ†ğ‘ ğ‘–+(1âˆ’ğœ†)ğ‘ â€²
ğ‘–) â‰¥ lnğ‘
1âˆ’ğ‘+ğœ†lnÃ
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ ğ‘—)
+(1âˆ’
ğœ†)lnÃ
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ â€²
ğ‘—)
.On the other hand, by Holderâ€™s inequality,
lnÂ©Â­Â­
Â«Â©Â­
Â«âˆ‘ï¸
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ ğ‘—)ÂªÂ®
Â¬ğœ†
Â·Â©Â­
Â«âˆ‘ï¸
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ â€²
ğ‘—)ÂªÂ®
Â¬1âˆ’ğœ†
ÂªÂ®Â®
Â¬
â‰¥lnÂ©Â­
Â«âˆ‘ï¸
ğ‘—â‰ ğ‘–exp
ğœâˆ’1(ğœ†ğ‘ ğ‘—+(1âˆ’ğœ†)ğ‘ â€²
ğ‘—)ÂªÂ®
Â¬,
Therefore, for ğ‘ =ğœ†ğ‘ +(1âˆ’ğœ†)ğ‘ â€²,ğœâˆ’1ğ‘ ğ‘–â‰¥lnğ‘
1âˆ’ğ‘+lnÃ
ğ‘—â‰ ğ‘–exp ğœâˆ’1ğ‘ ğ‘–
,
which indicates ğ‘¢ğ‘–(ğ‘ )â‰¥ğ‘and the quasi-concavity of ğ‘¢ğ‘–(ğ‘ ). â–¡
Proof. (Unique PNE when ğ‘€=2) We would finish the proof
by showing the following statements: When ğ‘›=2, (1) no interior
point ofğ‘¡1orğ‘¡2would be a PNE, i.e.,âˆ¥ğ‘¡1âˆ¥=âˆ¥ğ‘¡2âˆ¥=1; and (2) fix
anyğ‘¡3âˆ’ğ‘–, the best response ğ‘¡ğ‘–must be obtained by scaling vector
ğ‘ğ‘–ğ‘˜âˆ’ğ‘(3âˆ’ğ‘–)ğ‘˜
ğ‘˜âˆˆ[ğ‘‘]with a constant. Note that party 3âˆ’ğ‘–denotes
the other party besides ğ‘–. Fixingğ‘¡âˆ’ğ‘–, the optimization problem faced
by partyğ‘–could be formulated as
maxğ‘¡ğ‘–ğ‘“ğ‘–(ğ‘¡ğ‘–)=exp(ğœâˆ’1ğ‘ ğ‘–)Ã
ğ‘—âˆˆ[ğ‘›]exp(ğœâˆ’1ğ‘ ğ‘—),
s.t. ğ‘ ğ‘—=ğ‘‘âˆ‘ï¸
ğ‘˜=1ğ‘›âˆ‘ï¸
ğ‘–â€²=1ğ‘¡ğ‘–â€²ğ‘˜ğ‘¤ğ‘–â€²ğ‘ğ‘—ğ‘˜,âˆ€ğ‘—âˆˆ[ğ‘›],
ğ‘‘âˆ‘ï¸
ğ‘˜=1ğ‘¡2
ğ‘–ğ‘˜â‰¤1.(3)
Using multi-variable chain rule, we could obtain
ğœ•ğ‘“ğ‘–(ğ‘¡ğ‘–)
ğœ•ğ‘¡ğ‘–ğ‘˜=ğœ•ğ‘“ğ‘–(ğ‘¡ğ‘–)
ğœ•ğ‘ ğ‘–Â·ğœ•ğ‘ ğ‘–
ğœ•ğ‘¡ğ‘–ğ‘˜+âˆ‘ï¸
ğ‘—â‰ ğ‘–ğœ•ğ‘“ğ‘–(ğ‘¡ğ‘–)
ğœ•ğ‘ ğ‘—Â·ğœ•ğ‘ ğ‘—
ğœ•ğ‘¡ğ‘–ğ‘˜
=ğ‘¤ğ‘–exp(ğœâˆ’1ğ‘ ğ‘–)
ğœÃ
ğ‘—â€²âˆˆ[ğ‘›]exp(ğœâˆ’1ğ‘ â€²
ğ‘—)2Â·âˆ‘ï¸
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ ğ‘—)(ğ‘ğ‘–ğ‘˜âˆ’ğ‘ğ‘—ğ‘˜).
By definition of PNE, ğ‘¡ğ‘–must be the solution to problem (3)
supposingğ‘¡âˆ’ğ‘–are fixed. To characterize the conditions of those best-
responseğ‘¡ğ‘–, consider the following Karushâ€“Kuhnâ€“Tucker (KKT)
conditions induced by problem (3).
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³ğœ•ğ‘“ğ‘–(ğ‘¡ğ‘–)
ğœ•ğ‘¡ğ‘–ğ‘˜+2ğœ†ğ‘¡ğ‘–ğ‘˜=0,âˆ€ğ‘˜âˆˆ[ğ‘‘]
ğœ†Ãğ‘‘
ğ‘˜=1ğ‘¡2
ğ‘–ğ‘˜âˆ’1
=0
Ãğ‘‘
ğ‘˜=1ğ‘¡2
ğ‘–ğ‘˜â‰¤1
ğœ†â‰¤0(4)
We would now claim that ğœ†â‰ 0forğ‘›=2andğ‘1â‰ ğ‘2. Ifğœ†=0,
we requireğœ•ğ‘“ğ‘–(ğ‘¡ğ‘–)
ğœ•ğ‘¡ğ‘–ğ‘˜=0,âˆ€ğ‘˜âˆˆ[ğ‘‘].However, when ğ‘›=2, the termÃ
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ ğ‘—)(ğ‘ğ‘–ğ‘˜âˆ’ğ‘ğ‘—ğ‘˜)would reduce to exp(ğœâˆ’1ğ‘ 3âˆ’ğ‘–)(ğ‘ğ‘–ğ‘˜âˆ’
ğ‘(3âˆ’ğ‘–)ğ‘˜),which could not be 0for everyğ‘˜forğ‘1â‰ ğ‘2. As the term
ğ‘¤ğ‘–exp(ğœâˆ’1ğ‘ ğ‘–)
ğœÃ
ğ‘—â€²âˆˆ[ğ‘›]exp(ğœâˆ’1ğ‘ â€²
ğ‘—)2is strictly positive,ğœ•ğ‘“ğ‘–(ğ‘¡ğ‘–)
ğœ•ğ‘¡ğ‘–ğ‘˜=0could not hold
for everyğ‘˜, thusğœ†could not be 0, and we must haveÃğ‘‘
ğ‘˜=1ğ‘¡2
ğ‘–ğ‘˜=1to satisfy conditions (4). As ğœ†â‰ 0, we could represent each entry
ofğ‘¡ğ‘–asğ‘¡ğ‘–ğ‘˜=ğ¶ğ‘–Â·Ã
ğ‘—â‰ ğ‘–exp(ğœâˆ’1ğ‘ ğ‘—)(ğ‘ğ‘–ğ‘˜âˆ’ğ‘ğ‘—ğ‘˜)whereğ¶ğ‘–=âˆ’1
2ğœ†Â·
ğ‘¤ğ‘–exp(ğœâˆ’1ğ‘ ğ‘–)
ğœÃ
ğ‘—â€²âˆˆ[ğ‘›]exp(ğœâˆ’1ğ‘ â€²
ğ‘—)2>0is the same constant for all ğ‘¡ğ‘–ğ‘˜. When
ğ‘›=2, we further denote ğ¶â€²
ğ‘–=ğ¶ğ‘–Â·exp(ğœâˆ’1ğ‘ 3âˆ’ğ‘–), then we must have
ğ‘¡ğ‘–ğ‘˜=ğ¶â€²
ğ‘–Â·(ğ‘ğ‘–ğ‘˜âˆ’ğ‘(3âˆ’ğ‘–)ğ‘˜).Since we have derivedÃğ‘‘
ğ‘˜=1ğ‘¡2
ğ‘–ğ‘˜=1and
ğ¶â€²
ğ‘–>0, we could thus deduce
ğ‘¡ğ‘–ğ‘˜=ğ‘ğ‘–ğ‘˜âˆ’ğ‘(3âˆ’ğ‘–)ğ‘˜
Ãğ‘‘
ğ‘˜â€²=1(ğ‘ğ‘–ğ‘˜â€²âˆ’ğ‘(3âˆ’ğ‘–)ğ‘˜â€²)21
2
is the unique solution (best-response strategy) to problem (3). Com-
bining the unique best-response strategies for both party 1and
party 2finishes the proof for our statement. â–¡
A.2 Proof for Theorem 5.2
Lemma A.1. For any strategy profile ğœsatisfyingğ‘“ğ‘–=ğ‘“ğœğ‘–
ğ‘–,âˆ€ğ‘–,
ğ‘ˆğ‘–(ğœğ‘–,ğœâˆ’ğ‘–)=ğ‘ˆğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1)
Proof. (Lemma A.1) Note that
ğ‘ˆğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1)=âˆ«
ğ‘¡âˆˆTâˆ«
ğ‘¡â€²
ğ‘–âˆˆTğ‘–ğ‘¢ğ‘– (ğ‘¡â€²
ğ‘–,ğ‘¡âˆ’ğ‘–);ğ‘¡ğ‘–ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)ğ‘‘ğ‘¡â€²
ğ‘–ğ‘“(ğ‘¡)ğ‘‘ğ‘¡,
we could have
ğ‘ˆğ‘–(ğœğ‘–,ğœâˆ’ğ‘–)=âˆ«
ğ‘¡âˆˆTâˆ«
ğ‘¡â€²âˆˆTğ‘¢ğ‘–(ğ‘¡â€²;ğ‘¡ğ‘–)ğ‘€Ã–
ğ‘–=1ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)ğ‘‘ğ‘¡â€²ğ‘“(ğ‘¡)ğ‘‘ğ‘¡
=âˆ«
ğ‘¡â€²
1âˆˆT1...âˆ«
ğ‘¡â€²
ğ‘€âˆˆTğ‘€âˆ«
ğ‘¡ğ‘–âˆˆTğ‘–ğ‘¢ğ‘–(ğ‘¡â€²;ğ‘¡ğ‘–)ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)ğ‘“ğ‘–(ğ‘¡ğ‘–)
Ã–
ğ‘—â‰ ğ‘– âˆ«
ğ‘¡ğ‘—âˆˆTğ‘—ğœğ‘—(ğ‘¡ğ‘—,ğ‘¡â€²
ğ‘—)ğ‘“ğ‘—(ğ‘¡ğ‘—)ğ‘‘ğ‘¡ğ‘—!
ğ‘‘ğ‘¡ğ‘–ğ‘‘ğ‘¡â€²
1...ğ‘‘ğ‘¡â€²
ğ‘€
=âˆ«
ğ‘¡â€²
1âˆˆT1...âˆ«
ğ‘¡â€²
ğ‘€âˆˆTğ‘€âˆ«
ğ‘¡ğ‘–âˆˆTğ‘–ğ‘¢ğ‘–(ğ‘¡â€²;ğ‘¡ğ‘–)ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡â€²
ğ‘–)ğ‘“ğ‘–(ğ‘¡ğ‘–)
Ã–
ğ‘—â‰ ğ‘–ğ‘“ğ‘—(ğ‘¡â€²
ğ‘—)ğ‘‘ğ‘¡ğ‘–ğ‘‘ğ‘¡â€²
1...ğ‘‘ğ‘¡â€²
ğ‘€
=ğ‘ˆğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1),
where the last equality comes from regarding ğ‘¡â€²
ğ‘—asğ‘¡ğ‘—forğ‘—â‰ ğ‘–.â–¡
Proof. (Theorem 5.2) For the strategy profile ğœ={ğ¼ğ‘€}, suppose
participantğ‘–deviates to an alternative strategy ğœâ€²
ğ‘–withğ‘“ğœâ€²
ğ‘–
ğ‘–=ğ‘“ğ‘–, and
ğ‘ˆâ„0
ğ‘–(ğœâ€²
ğ‘–,ğ¼ğ‘€âˆ’1)>ğ‘ˆâ„0
ğ‘–(ğ¼ğ‘€). Then consider another (randomized)
server aggregation function â„â€²
0defined asâ„â€²
0(ğ‘¡)=â„0(ğœâ€²
ğ‘–(ğ‘¡ğ‘–),ğ‘¡âˆ’ğ‘–).
Sinceâ„â€²
0just maps the uploaded embedding of participant ğ‘–accord-
ing toğœâ€²
ğ‘–on the basis of â„0,ğ‘ˆâ„â€²
0
ğ‘—(ğ¼ğ‘€)=ğ‘ˆâ„0
ğ‘—(ğ¼,(ğœâ€²
ğ‘–,ğ¼ğ‘€âˆ’2))forğ‘—â‰ ğ‘–,
andğ‘ˆâ„â€²
0
ğ‘–(ğ¼ğ‘€)=ğ‘ˆâ„0
ğ‘–(ğœâ€²
ğ‘–,ğ¼ğ‘€âˆ’1).
From Lemma A.1, as ğ‘“ğ¼
ğ‘—=ğ‘“ğ‘—andğ‘“ğœâ€²
ğ‘–
ğ‘–=ğ‘“ğ‘–, we have
ğ‘ˆâ„0
ğ‘—(ğ¼,(ğœâ€²
ğ‘–,ğ¼ğ‘€âˆ’2))=ğ‘ˆâ„0
ğ‘—(ğ¼ğ‘€)=âˆ«
ğ‘¡âˆˆTğ‘¢ğ‘—(â„0(ğ‘¡);ğ‘¡ğ‘—)ğ‘“(ğ‘¡)ğ‘‘ğ‘¡,âˆ€ğ‘—â‰ ğ‘–.
Therefore, we would have ğ‘ˆâ„â€²
0
ğ‘–(ğ¼ğ‘€)>ğ‘ˆâ„0
ğ‘–(ğ¼ğ‘€)andğ‘ˆâ„â€²
0
ğ‘—(ğ¼ğ‘€)=
ğ‘ˆâ„0
ğ‘—(ğ¼ğ‘€),âˆ€ğ‘—â‰ ğ‘–, contradicting with the Pareto efficiency of â„0.â–¡
3584KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
A.3 Proof for Theorem 5.3
Proof. We conduct the proofs for discrete embeddings ğ‘¡ğ‘–âˆˆğ‘‡ğ‘–.
For any potential strategy ğœğ‘–, construct a directed graph G, whose
nodes correspond to each possible local embedding ğ‘¡ğ‘–âˆˆğ‘‡ğ‘–. We
construct the edges in this graph to denote the strategic report
ğœğ‘–(ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–)>0forğ‘¡1
ğ‘–â‰ ğ‘¡2
ğ‘–, such that each directed edge pointing
fromğ‘¡1
ğ‘–toğ‘¡2
ğ‘–has weight P(ğ‘¡ğ‘–=ğ‘¡1
ğ‘–)Â·ğœğ‘–(ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–),i.e., the probability
that participant ğ‘–has true embedding ğ‘¡1
ğ‘–and misreport embedding
ğ‘¡2
ğ‘–under strategy ğœğ‘–. By{ğœğ‘–:ğ‘“ğœğ‘–
ğ‘–=ğ‘“ğ‘–}, we requireâˆ€ğ‘¡2
ğ‘–âˆˆTğ‘–,
âˆ‘ï¸
ğ‘¡1
ğ‘–âˆˆTğ‘–ğœğ‘–(ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–)Â·P(ğ‘¡ğ‘–=ğ‘¡1
ğ‘–)=P(ğ‘¡ğ‘–=ğ‘¡2
ğ‘–). (5)
Based on (5), we would further haveâˆ‘ï¸
ğ‘¡1
ğ‘–â‰ ğ‘¡2
ğ‘–ğœğ‘–(ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–)Â·P(ğ‘¡ğ‘–=ğ‘¡1
ğ‘–)=(1âˆ’ğœğ‘–(ğ‘¡2
ğ‘–,ğ‘¡2
ğ‘–))Â·P(ğ‘¡ğ‘–=ğ‘¡2
ğ‘–)
=âˆ‘ï¸
ğ‘¡1
ğ‘–â‰ ğ‘¡2
ğ‘–ğœğ‘–(ğ‘¡2
ğ‘–,ğ‘¡1
ğ‘–)Â·P(ğ‘¡ğ‘–=ğ‘¡2
ğ‘–),
where the last inequality is due toÃ
ğ‘¡1
ğ‘–âˆˆTğ‘–ğœğ‘–(ğ‘¡2
ğ‘–,ğ‘¡1
ğ‘–)=1,âˆ€ğ‘¡2
ğ‘–âˆˆTğ‘–.
Therefore, in the constructed graph, each node would have the sum
of weight of in-edges to equal the sum of weight of out-edges.
To prove the statement, we would start from the graph of any
strategyğœğ‘–â‰ ğ¼, and gradually remove all the edges in this graph
to approach the edgeless graph (correspond to the truth-telling
strategyğ¼). We would demonstrate that (1) the adjustment must
finally lead to an edgeless graph, and (2) each step in the adjustment
would result in a feasible strategy with non-decreasing utility, thus
finishes the proof. Our adjustment is as follows: in each step, we
find a cycle in the graph. We remove the edge with the smallest
weight in this cycle, whose weight is denoted as ğ‘¤, and also update
the weight of other edges in this cycle to minus ğ‘¤, then addğ‘¤to
ğœğ‘–(ğ‘¡ğ‘–,ğ‘¡ğ‘–)for each node ğ‘¡ğ‘–in this cycle.
To prove statement (1), since we have formulated a directed
graph, suppose the graph is not edgeless and contains no cycle
during the adjustment, there must exist some sink node and source
node in the graph. However, by our construction, each node must
have the equal sum of weights of in-edges and out-edges, thus leads
to a contradiction. For statement (2), since each step of adjustment
preserves Equation (5), the adjusted strategy is still feasible and
satisfies{ğœğ‘–:ğ‘“ğœğ‘–
ğ‘–=ğ‘“ğ‘–}. We only remains to show each step of
adjustment would lead to non-decreasing utility given conditions
(2). For a strategy ğœğ‘–, its resulted utility could be calculated as
âˆ‘ï¸
ğ‘¡1
ğ‘–âˆˆTğ‘–âˆ‘ï¸
ğ‘¡2
ğ‘–âˆˆTğ‘–P(ğ‘¡ğ‘–=ğ‘¡1
ğ‘–)Â·ğœğ‘–(ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–)Â·ğ‘£ğ‘–(ğ‘¡1
ğ‘–)Â·Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡2
ğ‘–,ğ‘¡âˆ’ğ‘–)]
=âˆ‘ï¸
ğ‘¡1
ğ‘–âˆˆTğ‘–âˆ‘ï¸
ğ‘¡2
ğ‘–âˆˆTğ‘–ğ‘¤(ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–)Â·ğ‘£ğ‘–(ğ‘¡1
ğ‘–)Â·Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡2
ğ‘–,ğ‘¡âˆ’ğ‘–)],
where we use ğ‘¤(ğ‘¡1
ğ‘–,ğ‘¡2
ğ‘–)to denote the weight of edge from ğ‘¡1
ğ‘–toğ‘¡2
ğ‘–for
ğ‘¡1
ğ‘–â‰ ğ‘¡2
ğ‘–, and define ğ‘¤(ğ‘¡1
ğ‘–,ğ‘¡1
ğ‘–)=P(ğ‘¡ğ‘–=ğ‘¡1
ğ‘–)Â·ğœğ‘–(ğ‘¡1
ğ‘–,ğ‘¡1
ğ‘–). W.l.o.g., define
theğ‘›nodes in the current cycle as ğ‘¡1
ğ‘–,...ğ‘¡ğ‘›
ğ‘–(withğ‘¡ğ‘›+1
ğ‘–=ğ‘¡1
ğ‘–for conve-
nience in notations). Then by our construction of the adjustment, ex-
cept for the unchanged parts of the graph, the utility before this step
of adjustment is ğ‘¤Â·hÃğ‘›
ğ‘—=1ğ‘£ğ‘–(ğ‘¡ğ‘—
ğ‘–)Â·Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡ğ‘—+1
ğ‘–,ğ‘¡âˆ’ğ‘–)]i
,and the util-
ity after this step would be ğ‘¤Â·hÃğ‘›
ğ‘—=1ğ‘£ğ‘–(ğ‘¡ğ‘—
ğ‘–)Â·Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡ğ‘—
ğ‘–,ğ‘¡âˆ’ğ‘–)]i
.Byconditions (2), since larger ğ‘£ğ‘–(ğ‘¡ğ‘—
ğ‘–)implies larger Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡ğ‘—
ğ‘–,ğ‘¡âˆ’ğ‘–)],
applying the rearrangement inequality, we would have
ğ‘›âˆ‘ï¸
ğ‘—=1ğ‘£ğ‘–(ğ‘¡ğ‘—
ğ‘–)Â·Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡ğ‘—
ğ‘–,ğ‘¡âˆ’ğ‘–)]â‰¥ğ‘›âˆ‘ï¸
ğ‘—=1ğ‘£ğ‘–(ğ‘¡ğ‘—
ğ‘–)Â·Eğ‘¡âˆ’ğ‘–[ğ‘¥â„0
ğ‘–(ğ‘¡ğ‘—+1
ğ‘–,ğ‘¡âˆ’ğ‘–)],
thus proves the non-decreasing of utility in each step of adjustment.
â–¡
A.4 Proof for Theorem 5.4
Proof. Defineğ‘›to be the number of total inference rounds. It is
sufficient for us to find a penalty function ğ‘˜ğ‘–(Â·)for each participant
ğ‘–, such that when ğ‘›â†’âˆ , there does not exist a strategy ğœğ‘–with
ğ‘ˆâ„âˆ—
0
ğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1)>ğ‘ˆâ„âˆ—
0
ğ‘–(ğ¼ğ‘€). For convenience in notations, we abbre-
viateğ›½ğ‘‡(ğ‘“ğœğ‘–
ğ‘–)to beğ›½(ğœğ‘–), and define ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–))=ğ‘˜ğ‘–(ğ›½(ğœğ‘–))/ğ‘šğ‘–.
To evaluate the change of utility for an arbitrary strategy ğœğ‘–
during the penalty period, we further define the expected util-
ity increment ratio of ğœğ‘–over the penalty reporting strategy ğœğ‘
ğ‘–:
ğœğ‘
ğ‘–(ğ‘¡ğ‘–,Â·)âˆ¼ğºğ‘–,âˆ€ğ‘¡ğ‘–âˆˆTğ‘–when other participants report truthfully
asğ›¿ğ‘–(ğœğ‘–):=ğ‘ˆğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1)
ğ‘ˆğ‘–(ğœğ‘
ğ‘–,ğ¼ğ‘€âˆ’1)âˆ’1.Recall that we are under the feasible
problem case, which indicates ğ›¿ğ‘–(ğ¼)>0. By the strong law of large
number, we would have ğ‘ğ‘–(ğœğ‘–)converges to ğ›½(ğœğ‘–)whenğ‘›â†’âˆ .
Therefore, the proportion of penalty period among the entire infer-
ence period would converge toğ›½(ğœğ‘–)Â·ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–))
1+ğ›½(ğœğ‘–)Â·ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–)),since when each
two-sample test of length ğ‘šğ‘–ends, there is an additional ğ›½(ğœğ‘–)prob-
ability to have a penalty period with length ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–))Â·ğ‘šğ‘–. Thus, we
could calculate the expected per-round utility of strategy ğœğ‘–when
ğ‘›â†’âˆ as
ğ‘ˆâ„âˆ—
0
ğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1)=1
1+ğ›½(ğœğ‘–)Â·ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–))Â·ğ‘ˆâ„0
ğ‘–(ğœğ‘–,ğ¼ğ‘€âˆ’1)
+ğ›½(ğœğ‘–)Â·ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–))
1+ğ›½(ğœğ‘–)Â·ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–))Â·ğ‘ˆâ„0
ğ‘–(ğœğ‘
ğ‘–,ğ¼ğ‘€âˆ’1)
=ğ›¿ğ‘–(ğœğ‘–)
1+ğ›½(ğœğ‘–)Â·ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–))+1
Â·ğ‘ˆâ„0
ğ‘–(ğœğ‘
ğ‘–,ğ¼ğ‘€âˆ’1).
That is, to prove the BNE when ğ‘›â†’âˆ, we need
ğ›¿ğ‘–(ğ¼)
1+ğ›¼Â·ğ‘˜â€²
ğ‘–(ğ›¼)â‰¥ğ›¿ğ‘–(ğœğ‘–)
1+ğ›½(ğœğ‘–)Â·ğ‘˜â€²
ğ‘–(ğ›½(ğœğ‘–)),âˆ€ğœğ‘–âˆˆÎ£ğ‘–. (6)
For anyğœğ‘–withğ‘“ğœğ‘–
ğ‘–=ğ‘“ğ‘–,ğ›½(ğœğ‘–)=ğ›¼, and by Theorem 5.2, ğ›¿ğ‘–(ğ¼)â‰¥
ğ›¿ğ‘–(ğœğ‘–), which holds regardless of the form of functions ğ‘˜ğ‘–. For any
ğœğ‘–withğ‘“ğœğ‘–
ğ‘–â‰ ğ‘“ğ‘–, considerğ‘˜â€²
ğ‘–(ğ›¼)to be in the form of ğ›¼ğ‘Â·ğµwith
constantsğµandğ‘, and substitute it into condition (6), we would
equivalently require
ğ›¿ğ‘–(ğœğ‘–)âˆ’ğ›¿ğ‘–(ğ¼)â‰¤
ğ›¿ğ‘–(ğ¼)Â·ğ›½ğ‘+1(ğœğ‘–)âˆ’ğ›¿ğ‘–(ğœğ‘–)Â·ğ›¼ğ‘+1
Â·ğµ,âˆ€ğœğ‘–.(7)
By feasibility of the problem case and the validity of the two-sample
test, we have ğ›¿ğ‘–(ğ¼)>0andğ›½(ğœğ‘–)>ğ›¼. Therefore, for each ğœğ‘–with
ğ‘“ğœğ‘–
ğ‘–â‰ ğ‘“ğ‘–, we are able to find a large enough positive constant ğ‘ğœğ‘–,
such thatğ›¿ğ‘–(ğ¼)Â·ğ›½ğ‘ğœğ‘–+1(ğœğ‘–)âˆ’ğ›¿ğ‘–(ğœğ‘–)Â·ğ›¼ğ‘ğœğ‘–+1>0. Takingğ‘to be the
supreme over those ğ‘ğœğ‘–, we would have this term to be positive
for all theğœğ‘–withğ‘“ğœğ‘–
ğ‘–â‰ ğ‘“ğ‘–. Then taking ğµto be a large positive
constant such that condition (6) is satisfied for all the ğœğ‘–finishes
the proof. â–¡
3585