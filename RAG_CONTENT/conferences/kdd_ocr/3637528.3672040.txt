Multi-Task Learning for Routing Problem with
Cross-Problem Zero-Shot Generalization
Fei Liu
fliu36-c@my.cityu.edu.hk
City University of Hong Kong
Hong Kong SARXi Lin
xi.lin@my.cityu.edu.hk
City University of Hong Kong
Hong Kong SARZhenkun Wangâˆ—
wangzk3@sustech.edu.cn
Southern University of Science and
Technology
Shenzhen, China
Qingfu Zhangâˆ—
qingfu.zhang@cityu.edu.hk
City University of Hong Kong
Hong Kong SARXialiang Tong
tongxialiang@huawei.com
Huawei Noahâ€™s Ark Lab
Shenzhen, ChinaMingxuan Yuan
yuan.mingxuan@huawei.com
Huawei Noahâ€™s Ark Lab
Hong Kong SAR
ABSTRACT
Vehicle routing problems (VRP) are very important in many real-
world applications and has been studied for several decades. Re-
cently, neural combinatorial optimization (NCO) has attracted grow-
ing research effort. NCO is to train a neural network model to solve
an optimization problem in question. However, existing NCO meth-
ods often build a different model for each routing problem, which
significantly hinders their application in some areas where there
are many different VRP variants to solve. In this work, we make a
first attempt to tackle the crucial challenge of cross-problem gener-
alization in NCO. We formulate VRPs as different combinations of a
set of shared underlying attributes and solve them simultaneously
via a single model through attribute composition. In this way, our
proposed model can successfully solve VRPs with unseen attribute
combinations in a zero-shot generalization manner. In our exper-
iments, the neural model is trained on five VRP variants and its
performance is tested on eleven VRP variants. The experimental
results show that the model demonstrates superior performance on
these eleven VRP variants, reducing the average gap to around 5%
from over 20% and achieving a notable performance boost on both
benchmark datasets and real-world logistics scenarios.
CCS CONCEPTS
â€¢Applied computing â†’Transportation; Supply chain manage-
ment ;â€¢Computing methodologies â†’Machine learning.
KEYWORDS
Cross-problem generalization, Attribute composition, Multi-task
learning, Vehicle routing
âˆ—the corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672040ACM Reference Format:
Fei Liu, Xi Lin, Zhenkun Wang, Qingfu Zhang, Xialiang Tong, and Mingx-
uan Yuan. 2024. Multi-Task Learning for Routing Problem with Cross-
Problem Zero-Shot Generalization. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3672040
1 INTRODUCTION
The vehicle routing problems (VRP) have been widely studied in
academia and are of significant practical importance in real-world
applications such as logistics, transportation, retail distribution,
waste collection, and manufacturing [ 40]. The objective of a VRP is
to optimally manage a fleet of vehicles, minimizing the total cost
while satisfying the demands of customers. Real-world industry
routing problems have diverse attributes (e.g., the capacities of
vehicles, the time window constraints of requests), which lead to
numerous VRP variants [ 5,44]. Developing a different algorithm for
each VRP is very costly and impractical. Therefore, it is desirable to
build a single solver for solving different VRP variants, which can
significantly reduce labor costs and improve operational efficiency.
A few attempts have been made to develop a unified heuristic
framework [12, 37, 43], but they demand much human effort with
domain specific knowledge.
Neural combinatorial optimization (NCO) learns a neural network-
based heuristic to solve combinatorial optimization problems. This
approach has received growing research attention due to its poten-
tial ability to generate high-quality solutions without much human
effort [ 3,26,45]. However, existing NCO methods work in a single-
task manner [ 1,28]. In other words, they need to train a neural
network model for each optimization problem. When the prob-
lem changes, another model must be trained from scratch, which
inevitably leads to high computational costs. Some attempts to over-
come this shortcoming include transfer learning and multiobjective
learning [ 13,29,31,50]. However, the neural network models gen-
erated in these works can only be used to solve problems whose
instances have been used for training. In other words, generaliza-
tion across different problems has not been well addressed.
This paper proposes a multi-task learning approach for cross-
problem generalization on vehicle routing problems to tackle the
challenge. We develop a unified neural network model with at-
tribute composition to handle multiple VRP variants which can
 
1898
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fei Liu et al.
be efficiently trained by reinforcement learning (RL) without any
labeled solutions. We show that the unified model can be used to
solve unseen VRP variants in a zero-shot manner significantly out-
performing other approaches. Our contributions are summarized
as follows:
â€¢We propose a novel learning-based method to tackle cross-
problem generalization in VRPs. It treats VRP variants as dif-
ferent combinations of a set of shared underlying attributes,
and solves various VRPs simultaneously in an end-to-end
multi-task learning manner. To the best of our knowledge,
this is a first attempt to investigate cross-problem neural
solvers for VRPs.
â€¢We develop a unified attention model with an attribute com-
position block. The proposed model has a promising zero-
shot generalization ability to handle unseen VRP variants
with any combination of the basic attributes.
â€¢Our proposed model achieves promising cross-problem gen-
eralization performance on eleven routing problems. The av-
erage gap is reduced to around 5% from over 20% in the exist-
ing single-task approach. Further validation of our zero-shot
generalization approach on real-world benchmark datasets
and an industry logistic application demonstrates a notable
performance boost.
2 RELATED WORK
2.1 Neural Combinatorial Optimization (NCO)
NCO [ 3,26,45] aims to learn neural network models to solve com-
binatorial optimization problems. Compared to other approaches
(e.g., exact methods and heuristics), it requires very little domain-
specific knowledge and usually can generate high-quality solutions
with lower computational overhead. As such, it has gained much
attention in the past decade [3].
Current NCO methods can be classified into two categories: end-
to-end methods [ 2,9,23,27,34,36,45] and improvement-based
methods [ 6,6,20,25]. The former aims to construct a solution
without using other traditional methods, while the latter also needs
to use other algorithms to improve its performance. In this paper,
we focus on the end-to-end approach.
2.2 NCO for Vehicle Routing Problem (VRP)
NCO has been successfully applied to many vehicle routing problem
variants, including traveling salesman problem (TSP) [ 2], capac-
itated VRP (CVRP) [ 34], VRP with time windows (VRPTW) [ 51],
open VRP (OVRP) [ 41], VRP with pickup and delivery [ 30], and
heterogeneous VRP [ 29]. A recent survey of the works on learning-
based methods for different vehicle routing problems can be found
in Li et al. [28].
Despite extensive studies, the existing works have been con-
ducted in a single-task manner, in which an individual neural
model is trained for each problem. The time-consuming training
process for every new problem hinders their practical application.
It should be noted that various VRPs have common underlying
attributes. Nevertheless, these similarities and correlations have
not been adequately studied in the context of NCO. Recently, at-
tempts have been made to explore robust optimization over multiple
distributions[ 4,16,22]. Several works [ 8,11,14,15,33,36] studiedgeneralization to large-scale problems. Zhou et al . [52] and Liu et al .
[32]considered generalization in terms of both problem size and
distribution. However, the cross-problem generalization has not
been studied for routing problems.
2.3 Multi-Task Learning and Zero-Shot
Learning
Multi-task learning (MTL) tackles multiple related learning tasks
in a single learning process. It has been widely studied in various
fields, including computer vision [ 48], bioinformatics [ 17], and
natural language processing [ 10]. However, MTL has also used for
dealing with combinatorial optimization by a few researchers. Reed
et al. [38] and Ibarz et al . [21] proposed a general agent capable of
solving diverse tasks, including several combinatorial optimization
problems. Wang and Yu [46]presented a multi-task learning method
for combinatorial optimization problems with separate encoders
and decoders. Nevertheless, their approaches are unable to deal with
complicated VRPs, and their models need to be re-trained or fine-
tuned for solving new problem variants that were not considered
before.
Zero-shot learning (ZSL) allows the recognition of previously
unseen objects based on their shared semantic properties or at-
tributes [ 35,39,47]. Our idea behind learning on multiple VRPs
with several underlying attributes and generalizing to unseen VRPs
is similar to compositional ZSL [ 39], which composes novel prob-
lems out of known subparts or attributes. In the context of routing
problems, the attributes introduce additional constraints to the so-
lution generation process, distinguishing them from the features
investigated in computer vision tasks [39].
3 PROBLEM STATEMENT AND MOTIVATION
In this section, we introduce the basic CVRP formulation and then
demonstrate that other VRP variants can be considered as exten-
sions of the basic CVRP by incorporating additional attributes.
We denote a CVRP on an undirected graph ğº=(ğ‘‰,ğ¸).ğ‘‰=
{ğ‘£0,...,ğ‘£ğ‘›}, whereğ‘£0is the depot and ğ‘£1,...,ğ‘£ğ‘›areğ‘›customers.
ğ‘‰ğ‘={ğ‘£1,...,ğ‘£ğ‘›}is the customer set. For the ğ‘–-th customer, there is
a demandğ‘‘ğ‘–.ğ¸={ğ‘’ğ‘–ğ‘—},ğ‘–,ğ‘—âˆˆ{0,...,ğ‘›}are the edges between every
two nodes. For each edge ğ‘’ğ‘–ğ‘—, there is an associated cost (distance)
ğ‘ğ‘–ğ‘—. A fleet of homogeneous vehicles with a capacity of ğ¶is sent
out from the depot to visit the customers and return to the depot.
Each customer must be visited once. The objective is to minimize
the total travel distance of all used vehicles.
Figure 1 shows that various VRPs can be regarded as exten-
sions of CVRP by considering one or more attributes. For example,
VRPTW is extended from CVRP by adding time windows, and
OVRPTW involves both time windows and open route attributes.
In addition to the capacity constraints (C), the following at-
tributes are used in this paper:
â€¢Time Windows (TW): we denote the time window for the
ğ‘–-th node as[ğ‘’ğ‘–,ğ‘™ğ‘–],ğ‘–âˆˆ{0,...,ğ‘›}, whereğ‘’ğ‘–andğ‘™ğ‘–are the
early and late time windows. In addition, each node has a
service time ğ‘ ğ‘–. We consider hard time windows, i.e., the
vehicle must visit the node ğ‘–in the time range from ğ‘’ğ‘–toğ‘™ğ‘–.
If the vehicle arrives at node ğ‘–earlier than ğ‘’ğ‘–, the vehicle has
to wait until ğ‘’ğ‘–.
 
1899Multi-Task Learning for Routing Problem with
Cross-Problem Zero-Shot Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
17CVRP
Open route 
(O)
Time windows 
(TW)Backhauls
(B)
Duration limit 
(L)OVRP
VRPB
VRPTW
VRPL
VRPBTW
OVRPL
OVRPLTW
â€¦â€¦Attribute blocksO
B
TW
L
B TW
O L
O L TWCapacity
(C)C
â€¦
Figure 1: VRP variants as combinations of attribute blocks. The basic version is known as the Capacitated Vehicle Routing
Problem (CVRP). VRP variants can be regarded as extensions of CVRP, encompassing additional attributes. For example,
VRPTW extends CVRP by incorporating time windows, while OVRPTW adds an open routes attribute alongside time windows.
â€¢Open routes (O): open routes mean that the vehicle does not
need to return to the depot after it services all the customers
on its route.
â€¢Backhauls (B): the classic CVRP assumes that all vehicles
load demands at the depot and unload at customers. We call
these customers, who require deliveries ğ‘‘ğ‘–>0, linehaul
customers. Correspondingly, backhaul customers are those
customers that need pickup ğ‘‘ğ‘–<0. We consider the VRP
with mixed linehaul and backhaul customers, i.e., the order
of linehaul and backhaul customers can be mixed up in each
route.
â€¢Duration limits (L): duration limits refer to the situation in
which the total length of the routes cannot exceed some pre-
set thresholds. It is commonly used in real-world application
scenarios to maintain a reasonable workload for different
routes. In our setting, we use the same duration limit for all
routes.
Motivation In many real-world applications, there is a crucial
demand to use a single model to solve various VRPs with differ-
ent attributes. However, all current NCO methods need to build
a different model to tackle each routing problem. Unlike previous
approaches, our work treats the VRP variants as extensions of the
basic CVRP with different combinations of the basic attributes (e.g.,
TW, O, B, and L). By leveraging the similarities and correlations
among VRPs with shared underlying attributes, in this work, we
propose to build the first cross-problem neural solver to solve vari-
ous VRPs via a single model in a zero-shot generalization manner.
4 ATTRIBUTE-SHARING MODEL
We consider multiple VRPs as a set of related tasks and propose
training a unified neural model through reinforcement learning to
solve them simultaneously. Figure 2 illustrates the unified model
used in this work. It consists of three parts: encoder, decoder, and
attribute composition. We adopt the typical encoder-decoder frame-
work of the Attention Model (AM) [ 26]. The encoder learns the
node embeddings, and the decoder generates solutions sequentially.Different from the existing works [ 26,27,53], we enable its ability
to handle various VRPs by adding an additional attribute compo-
sition block. The idea is that the diverse VRPs actually consist of
several common underlying attributes. By learning from these at-
tributes, we can solve an exponential number of new VRPs as any
combination of them.
4.1 Model Structure
Encoder The encoder consists of ğ‘stacked multi-head attention
(MHA) blocks [ 26]. The input of the encoder is the node features
ğ‘“ğ‘–,âˆˆ{1,...,ğ‘›}. In this paper, the input features for the ğ‘–-th node
are denoted as ğ‘“ğ‘–={ğ‘¥ğ‘–,ğ‘‘ğ‘–,ğ‘’ğ‘–,ğ‘™ğ‘–}, whereğ‘¥ğ‘–are the coordinates, ğ‘‘ğ‘–
is the demand, and ğ‘’ğ‘–andğ‘™ğ‘–are the early and late time windows,
respectively. The input features are embedded through a linear
projection to generate the initial feature embedding â„(0)
ğ‘–. In each
MHA layer, skip connections [ 18] and instance normalization (IN)
are used:
Ë†â„(ğ‘™)
ğ‘–=ğ¼ğ‘ğ‘™
â„(ğ‘™âˆ’1)
ğ‘–+ğ‘€ğ»ğ´(ğ‘™)
â„(ğ‘™âˆ’1)
1,...,â„(ğ‘™âˆ’1)
ğ‘›
,
â„(ğ‘™)
ğ‘–=ğ¼ğ‘ğ‘™
Ë†â„ğ‘–+ğ¹ğ¹ğ‘™
Ë†â„ğ‘–
,(1)
whereğ‘™andğ‘™âˆ’1represent the current and last MHA layers, respec-
tively. The FF contains a hidden sublayer with ReLU activations.
The above encoding process generates the final node embeddings
â„(ğ‘)
ğ‘–. This encoding is performed only once, and the static node
embeddings are reused for every decoding step.
Decoder The decoder constructs a solution sequentially. The
input of the decoder includes three parts: the node embedding
â„(ğ‘)
1,...,â„(ğ‘)
ğ‘›, the embedding of currently visited node â„(ğ‘)
ğ‘¡, and
the attribute embedding ğ‘ğ‘¡. All the node embeddings are produced
by the decoder. ğ‘ğ‘¡is the embedding of the current state of attributes.
We provide an attribute vector to include various attributes involved
in multiple VRPs. At the ğ‘¡-th step, the attribute vector is ğ‘ğ‘¡=
{ğ‘ğ‘¡,ğ‘¡ğ‘¡,ğ‘™ğ‘¡,ğ‘œğ‘¡}, whereğ‘ğ‘¡is the remaining capacity of the current
vehicle,ğ‘¡ğ‘¡is the current time, ğ‘™ğ‘¡is the current duration of the route,
 
1900KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fei Liu et al.
26 17/5/2023LinearAttribute & MaskDot-Product AttentionConcat
Dot-Product AttentionConcatLinear
LinearAdd & normFFAdd & norm
Ã—ğ‘
InputLinearDot-Product AttentionProbability
OVRP
VRPL
VRPTWTraining Testing
OVRPL
VRPLTW
OVRPLTW
â€¦Encoder Attribute Composition
OVRPTWSoftmaxDecoder
CVRP
ğ‘ğ‘¡,ğ‘šğ‘¡ğ‘1,â€¦,ğ‘ğ‘›
â„1(ğ‘),â€¦,â„ğ‘›(ğ‘)ğ‘“1,â€¦,ğ‘“ğ‘›
Figure 2: Unified model extended from attention model. The model is trained on multiple VRPs with diverse attributes. Then it
can be used to solve numerous unseen VRPs as any combinations of the attributes involved in the training.
andğ‘œğ‘¡indicates whether the route is open or not. Except for ğ‘œğ‘¡, the
others will be updated in each step. Backhauls are not embedded
because they are implicitly considered in the node demands.
The decoder consists of one MHA layer and one single-head
attention (SHA) layer with clipping. The MHA is slightly different
from that used in the encoder. Skip connection, instance normal-
ization, and FF sublayer are not used:
Ë†â„ğ‘=ğ‘€ğ»ğ´ğ‘
â„(ğ‘)
1,...,â„(ğ‘)
ğ‘›,â„(ğ‘)
ğ‘¡,ğ‘ğ‘¡
,
ğ‘¢1...,ğ‘¢ğ‘›=ğ‘†ğ»ğ´ğ‘
â„(ğ‘)
1,...,â„(ğ‘)
ğ‘›,Ë†â„ğ‘
.(2)
The output embedding of MHA is Ë†â„ğ‘, which is used as one of the
inputs of the SHA. The probabilities of choosing the next node
are calculated through a softmax ğ‘ğ‘–=ğ‘’ğ‘¢ğ‘–Ã
ğ‘—ğ‘’ğ‘¢ğ‘—. We omit the step
indicatorğ‘¡for readability. The detailed structure of MHA and SHA
can be found in Appendix A.
In each step, we need to mask some nodes from being selected.
We update a masking vector ğ‘šğ‘¡. The associated positions of un-
wanted nodes in the vector will be set to -inf, which will be used in
the attentions before softmax. Except for masking those nodes that
have already been selected in the previous steps, the infeasibility
caused by various attributes should also be considered. For example,
these nodes that violate time window constraints should not be
selected.
4.2 Attribute Composition
The input of the attribute composition is the input node features
ğ‘“ğ‘–,ğ‘–âˆˆ{1,...,ğ‘›}, the list of visited nodes ğ‘‰ğ‘¡at the current step ğ‘¡,
and the problem attributes ğ´. The output is the attribute vector ğ‘ğ‘¡
and the mask vector ğ‘šğ‘¡.The problem attributes ğ´are given explicitly with the input
problem.ğ´are used to activate the corresponding attribute updating
procedure in the attribute composition block. In this paper, we have
four procedures for the four attributes. Each procedure ğ‘—updates
the corresponding attribute in the attribute vector ğ‘ğ‘—
ğ‘¡and calculates
an infeasible node list that must not be visited in the next step ğ‘šğ‘—
ğ‘¡.
The output attribute vector will include all the updated activated
attributes and pad the inactivated attributes to be a default value.
The output mask vector ğ‘šğ‘¡is the union of all activated attribute
masksğ‘šğ‘¡=ğ‘‰ğ‘¡âˆªÃ
ğ‘—âˆˆğ´ğ‘šğ‘—
ğ‘¡. See Appendix A for the details of the
attribute procedures.
For example, if only the capacity attribute is involved in the
problem (i.e., CVRP), the indicator only activates the capacity up-
dating procedure. In each step, the remaining capacity of the current
vehicle is calculated. The attribute vector ğ‘ğ‘¡={ğ‘ğ‘¡,ğ‘¡ğ‘¡,ğ‘™ğ‘¡,ğ‘œğ‘¡}only
updatesğ‘ğ‘¡, and pads the rest attributes to zero. The infeasible nodes
that exceed vehicle capacity when added to the route are recorded
in the mask ğ‘šğ‘¡=ğ‘‰ğ‘¡âˆªğ‘šğ‘
ğ‘¡.
Most of the investigated VRPs involve subsets of attributes in
our unified model. We are learning the shared underlying attributes
of diverse VRPs. In this way, we can train on a few VRPs and
solve a much larger group of VRPs as arbitrary combinations of
the underlying attributes. This characteristic enables the zero-shot
generalization ability of our model.
4.3 Multi-Task Reinforcement Learning
We use the REINFORCE algorithm with a shared baseline follow-
ing Kwon et al . [27] . We use greedy inference, i.e., a deterministic
trajectory is constructed iteratively based on the policy. In each
iteration, the next node is selected as the node with the maximum
probability predicted by the decoder. ğ‘›trajectories are constructed
fromğ‘›different starting points. Long-term rewards ğ‘…(ğœ1),...,ğ‘…(ğœğ‘›)
 
1901Multi-Task Learning for Routing Problem with
Cross-Problem Zero-Shot Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(negative of the total distances) are calculated after the entire tra-
jectoriesğœ1,...,ğœğ‘›are constructed. For the model with parameters
ğœƒ, the following gradient is used:
âˆ‡ğœƒğ½(ğœƒ)â‰ˆ1
ğ‘›ğµğµâˆ‘ï¸
ğ‘–=1ğ‘›âˆ‘ï¸
ğ‘—=1
ğ‘…
ğ‰ğ‘–
ğ‘—|ğ‘ ğ‘–
ğ‘˜
âˆ’ğ‘ğ‘–
ğ‘ ğ‘–
ğ‘˜
âˆ‡ğœƒlogğ‘ğœƒ
ğ‰ğ‘–
ğ‘—|ğ‘ ğ‘–
ğ‘˜
,
(3)
whereğ‘ ğ‘˜represents the instances for ğ‘˜-th task (VRP). ğ‘ğœƒ(ğ‰ğ‘–
ğ‘—)is
the aggregation of the probability of selection in each step of the
decoder.ğ‘ğ‘–(ğ‘ ğ‘˜)=1
ğ‘›Ãğ‘›
ğ‘—=1
ğ‘…
ğ‰ğ‘–
ğ‘—
is the shared baseline. ğµis the
batch size.
For multi-task learning, many optimizers have been designed to
improve robustness and convergence. Instead of using sophisticated
multi-task optimizers [ 7,24,49], we simply trained the multiple
tasks with equal weight.
5 EXPERIMENTS
We conduct experiments on eleven vehicle routing problems, namely
CVRP, VRPTW, OVRP, VRPB, VRPL, VRPBTW, VRPBL, OVRPL,
OVRPLTW, OVRPBTW, and OVRPBLTW, along with benchmark
datasets and our real-world logistic application cases. We train a
unified model on the former five VRPs simultaneously and use the
model to solve the rest problems in a zero-shot manner. Many of
these routing problems are being addressed by the neural method
for the first time. Note that one can easily extend the model to
consider other attributes. We chose these attributes because they
are among the most frequently used ones [5].
Test Instance Generation We use the same problem setup as
that used in Kool et al . [26] to generate the basic CVRP. For VRPTW,
we use the method introduced in Zhao et al. [51] to generate time
windows and service times. For the rest problems, there is no ex-
isting work that solves exactly the same settings. We make the
following settings: For VRPB, we first generate a CVRP and then
randomly select 20% of customers as backhaul customers, whose
demands are set to be the negative values of the original demands.
For OVRP, we only need to set the open route indicator as active.
For VRPL, the same maximum duration limit of 3is used for each
route.
Model Setting The number of MHA for the encoder is 6, and
the number of heads is 8. The hidden layer size is 512, and the
embedding size is 128.
Training In training, we randomly select one VRP variant in
each batch and generate instances of the selected VRP. We use
10,000 training instances for each epoch with a batch size of 64, and
the number of epochs for training is 10,000. Adam optimizer is used.
The initial learning rate is 1e-4 with a weight decay of 1e-6. We
implement the unified model using PyTorch, and the experiments
are running on a single RTX 2080Ti GPU. Training on the vehicle
routing problems of size 100 costs about ten days.
5.1 Results on Eleven VRP Variants
We provide an overall performance comparison on all eleven VRPs
and then introduce detailed settings and results on both the training
and unseen VRPs.
ST_CVRP ST_VRPTW ST_All Ours10
010203040506070Performance Gap (%)ST_CVRP
ST_VRPTW
ST_All
Ours
CVRPVRPTWOVRPVRPB
VRPL
VRPBL
OVRPL
VRPBTW
OVRPLTWOVRPBTWOVRPBLTW10
0102030405060Figure 3: A comparison of gaps on eleven VRPs (Left: box plot,
Right: radar plot). ST represents the unified model trained
with single-task learning on CVRP, ST_all represents the
unified model with single-task learning on OVRPBLTW, and
MT represents our approach, i.e., the unified model with
multi-task learning on five VRPs. ST_FT and MT_FT are the
fine-tuning models.
Figure 3 compares the performance in terms of the average gap
(%) to the baseline solver hybrid genetic search (HGS) [ 43]. 5,000
instances of size 50 are used for each problem. The box plot on the
left displays the overall performance, while the radar plot on the
right illustrates the average gap on each VRP, with a smaller area
indicating better results. The red one represents the results of our
unified model trained through multi-task learning with attribute
composition. The three compared models in different colors have
the same encoder-decoder structure and training settings as our
unified model. The difference is that they are trained on a single
problem. ST_CVRP and ST_VRPTW were trained on CVRP and
VRPTW, respectively, while ST_All was trained on OVRPBLTW
with all attributes considered.
Our unified model significantly outperforms single-task learning
models with an average gap of less than 4%. It demonstrates strong
generalization capabilities across different VRPs. In comparison,
single-task learning models only perform well on the training prob-
lem. For example, according to the results depicted in the radar
plot, ST_All achieves promising performance on the problem it
was trained on (OVRPBLTW), as well as on two VRP variants with
similar attributes (OVRPBTW and OVRPLTW), even slightly sur-
passing our unified model. However, its performance deteriorates
significantly on the remaining VRPs. It suggests that training with
all attributes fails to generalize effectively to other variants with
only a subset of attributes.
5.1.1 Attribute Correlation. We conduct a comparison of the dis-
tributions of different routing problems on a two-dimensional re-
duction space to visualize and analyze the attribute correlation.
We first extract the sample features from the decoder hidden layer
of our unified model. Each sample feature is the decoder hidden
layer embedding of one inference step. For each VRP, we collect
1,000 feature samples. All the feature samples from different VRPs
are transformed into one two-dimensional reduction space using
t-distributed Stochastic Neighbor Embedding (t-SNE).
Figure 4 shows a comparison of the distributions of five VRPs on
the reduction space. There is a clear distinct distribution of VRPTW
compared with others as the time window attribute poses a strong
 
1902KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fei Liu et al.
60
 40
 20
 0 20 40 60 80
Dimension 1100
75
50
25
0255075Dimension 2t-SNE Results
CVRP
OVRP
VRPB
VRPL
VRPTW
Figure 4: A comparison of distributions of different VRPs
on two-dimensional reduction space of the decoder hidden
layer.
CVRP OVRP VRPB VRPLVRPBL OVRPL VRPTWVRPBTWOVRPLTW OVRPBTW OVRPBLTW051015202530DistanceCVRP
All
Figure 5: Hausdorff distance in reduction space comparing
CVRP and OVRPBLTW (All) with other problems
constraint over the route. In addition to VRPTW, OVRP also shows
a different pattern than others because it does not force routes back
to the depot presenting less constraint. In contrast, CVRP and VRPL
follow a very close distribution and overlap with each other in most
areas. This can be attributed to the fact that our route length limit
is set to 3, which is easily satisfied in our settings.
We employ the Hausdorff distance to quantify the similarity
between two VRPs in the reduced space. Figure 5 illustrates the
Hausdorff distances of CVRP and OVRPBLTW (All) to other VRPs.
The distances observed in the reduced space are consistent with the
zero-shot generalization performance depicted in the radar plot in
Figure 3, with further details provided in the Appendix. For instance,
the model trained on CVRP exhibits a substantial distance from
VRPs with time window constraints, correlating with its subpar
performance on these problems.
5.1.2 Training VRPs. Table 2 lists the experimental results on the
five VRPs used in the training. For CVRP, we compare our results
with the original version of AM [ 26] and SOTA extensions, namely,
POMO [ 27], and GCAM [ 53]. For VRPTW, a deep reinforcement
learning method (DRL) [ 51] and POMO are compared. The results
of LHK3 [ 19] and Gurobi are used as the baseline for CVRP and
VRPTW, respectively. For the rest three problems, there are noTable 1: Training costs between POMO with single-task learn-
ing and our model with multi-task learning on five VRPs
#Mo
dels #Total Para. #Total Epochs Time Cost (day)
POMO
5 6.56M 50,000 49
Ours 1 1.35M 10,000 10.5
existing end-to-end NCO methods for comparison. We implement
POMO on these problems and use the state-of-the-art hybrid genetic
search (HGS) [43] solver as the baseline.
We extended the original POMO on these problems and trained
these models independently on each problem with single-task learn-
ing. We keep the settings of the original paper [ 27]. For other com-
pared single-task learning models, we select the best results from
the corresponding papers. If additional inference techniques are
used, such as beam search (BS) and sampling (Samp), their size is
indicated in the parentheses following the method. We adopt addi-
tional data augmentation (Aug) following POMO [ 27]. In addition,
the advanced inference strategy simulation guided beam search
(SGBS) has been investigated and integrated into our framework
for better zero-shot generation performance.
Tabel 1 presents the required number of models and the total
parameter sizes for single-task learning and our multi-task learning
for five tasks. The required number of models (and the total param-
eters) could be much higher for single-task learning if we train a
different model for each possible attribute combination, which is
not affordable for real-world applications.
For each problem, the evaluations are conducted on 5,000 in-
stances. We compare the performance with respect to three criteria:
the average distance (Dis.), the gap of the average distance to the
baseline results, and the total running time on 5,000 instances. In
general, our unified model is competitive with the existing single-
task NCO methods. On CVRP and VRPTW, the results are better
than the various existing end-to-end methods except for POMO.
The gap between our model and the optimal baselines is less than
5% across all five VRPs tested, and our running time is significantly
less than the baseline methods. The average gap of our unified
model on the five training VRPs can be further reduced to around
1% when integrating with SGBS, with an acceptable increase in
inference time. Despite slightly inferior results compared to POMO
and SGBS, we note that the latter requires training individual neural
networks for each problem and does not generalize well on unseen
problems.
5.1.3 Unseen VRP Variants. We use our unified model to solve
unseen VRPs in a zero-shot manner. The experiments are carried
out on five VRPs (i.e., VRPBTW, VRPBL, OVRPL, OVRPLTW, and
OVRPBTW).
We compare the results of our unified model with single-task
models, two commonly used constructive heuristics, and the SOTA
heuristic HGS. The constructive heuristics are the nearest insertion
method and farthest insertion method. We extended the source
code of HGS so that it can solve these routing problems [ 43]. The
two single-task models are POMO trained on CVRP and VRPTW,
respectively. We added the masking procedure used in our unified
 
1903Multi-Task Learning for Routing Problem with
Cross-Problem Zero-Shot Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Experimental results on five training VRPs. (The
compared neural solvers require training one model for each
VRP)
Pr
oblem MethodN=50 N=100
Dis. Gap Time Dis. Gap Time
CVRP
HGS 10.38 - 7h 15.54 - 14h
LKH3 10.38 0.00% 7h 15.61 0.46% 14h
AM (Samp1280) 10.59 2.02% 7m 16.16 4.00% 30m
GCAM (Samp1280) 10.64 2.50% - 16.29 4.83% -
POMO 10.53 1.41% 3s 15.87 2.13% 10s
POMO (Aug8) 10.44 0.58% 15s 15.75 1.36% 1.1m
SGBS 10.39 0.12% 2.0m 15.63 0.62% 11.8m
Ours 10.56 1.73% 3s 15.90 2.29% 11s
Ours (Aug8) 10.47 0.85% 20s 15.80 1.71% 1.2m
Ours+SGBS 10.40 0.18% 2.3m 15.66 0.81% 12.6m
VRPT
W HGS 16.30 - 7h 26.14 - 14h
LKH3 16.52 1.36% 7h 26.60 1.76% 14h
DRL (BS10) 17.90 9.82% 1m 29.50 12.85% 2m
DRL (BS10) +LNS 16.94 3.93% 11m 27.44 4.97% 65m
POMO 16.78 2.97% 3s 27.13 3.77% 11s
POMO (Aug8) 16.66 2.22% 19s 26.91 2.93% 1.2m
SGBS 16.55 1.52% 2.9m 26.55 1.58% 15.1m
Ours 16.96 4.06% 3s 27.46 5.05% 11s
Ours (Aug8) 16.80 3.09% 20s 27.13 3.81% 1.2m
Ours+SGBS 16.58 1.71% 3.2m 26.63 1.89% 17.9mO
VRP HGS 6.49 - 7h 9.71 - 14h
LKH3 6.52 0.46% 7h 9.75 0.41% 14h
POMO 6.73 3.67% 3s 10.18 4.91% 10s
POMO (Aug8) 6.63 2.14% 16s 10.07 3.76% 1.1m
SGBS 6.56 1.12% 2.1 m 9.89 1.92% 12.1m
Ours 6.81 4.90% 3s 10.34 6.56% 11s
Ours (Aug8) 6.71 3.40% 20s 10.14 4.48% 1.2m
Ours+SGBS 6.59 1.58% 2.5m 9.94 2.38% 13.4m
VRPB
HGS 7.69 - 7h 11.13 - 14h
LKH3 7.70 0.18% 7h 11.29 1.40% 14h
POMO 7.92 3.06% 3s 11.57 3.88% 10s
POMO (Aug8) 7.84 2.05% 15s 11.43 2.68% 1.1m
SGBS 7.78 1.22% 1.9m 11.31 1.59% 11m
Ours 8.17 6.36% 3s 11.72 5.23% 11s
Ours (Aug8) 7.87 2.40% 20s 11.53 3.58% 1.2m
Ours+SGBS 7.78 1.25% 2.1m 11.36 2.06% 11.7m
VRPL
HGS 10.37 - 7h 15.54 - 14h
LKH3 10.37 0.03% 7h 15.61 0.43% 14h
POMO 10.55 1.78% 3s 15.84 1.96% 10s
POMO (Aug8) 10.46 0.91% 16s 15.72 1.14% 1.1m
SGBS 10.40 0.30% 2.3m 15.64 0.66% 13.1m
Ours 10.56 1.88% 3s 15.96 2.72% 11s
Ours (Aug8) 10.47 0.98% 20s 15.80 1.66% 1.2m
Ours+SGBS 10.40 0.33% 2.6m 15.67 0.83% 14.3m
A
verage POMO 10.50 2.58% 3s 16.12 3.33% 10s
POMO (Aug8) 10.41 1.58% 16s 15.97 2.37% 1.1m
SGBS 10.34 0.86% 2.24m 15.81 1.28% 12.6m
Ours 10.61 3.78% 3s 16.27 4.37% 11s
Ours (Aug8) 10.46 2.14% 20s 15.81 3.05% 1.2m
Ours+SGBS 10.35 1.01% 2.5m 15.85 1.59% 14.0m
model to POMO so that they are applicable to different VRPs. Our
unified model and the two single-task models are used in a zero-shot
way without any fine-tuning on the new VRPs.
Table 3 shows the zero-shot performance. The results are evalu-
ated on 5,000 instances for each problem. Our unified model outper-
forms other methods including two heuristic methods except forHGS, which is specifically developed for VRPs. The two single-task
models are inferior to our multi-task unified model. The deficiency
of single-task models is more obvious in problems with very dif-
ferent attributes. For example, the model only trained on CVRP
performs worse on these problems involving time windows at-
tributes, while the model trained on VRPTW has poor performance
on VRPBL. The average gap of our model over these five unseen
VRPs is 4.6% and 7% on VRPs of sizes 50 and 100, respectively.
5.2 Benchmark Datasets
We apply our model on the well-known CVRPLib benchmark datasets
to demonstrate the out-of-distribution performance. Most of them
are derived from real-world instances and have diverse distribu-
tions and sizes. Specifically, we select six test suites with diverse
attributes from CVRPLIB1. There are a total of 181 instances whose
problem size ranges from 30 to 1,000. We normalize the coordinates
of customers so that they are within the unit range of [0,1]. We
also normalize the demands with respect to the vehicle capacity.
To evaluate the performance, we compare our unified model
with POMO, which is trained on CVRP. Both models are trained
on instances of size 100. We use the best-known solutions (BKS)
provided by CVRPLIB as the baseline. The criteria for comparison
include the average distance over all test suite instances and the
average gap to BKS.
Table 4 lists the information of six test suites as well as the
experimental results. The results demonstrate that, although our
model is not specifically trained for CVRP, it outperforms POMO on
all test suites. Overall, the average gap between our unified model
and the BKS is less than 10%, which is about half of POMO.
We divide the 100 instances in X into four groups based on their
problem size. The results of our model and POMO on each group
are summarized in Table 4. Our findings indicate that both models
perform similarly well on small-scale instances, but as the scale of
the problem increases, the performance of POMO deteriorates. In
fact, in the largest group, the average gap of POMO is over 30%.
Conversely, our model is more robust across problem sizes, with
its gap increasing only from 6.2% to 13.8%.
5.3 Industry Logistic Application
Finally, we validate the unified model on a real-world industry
logistic application with three different cases. The test sets are
derived from our historical cross-city transportation logistic data
spanning 30 days. Three types of cases are examined. The first
case involves the typical CVRP, where only the vehicle capacity
constraint is taken into account. Three datasets containing instances
of different vehicle capacities: large(L), middle(M), and small(S) were
collected. These datasets consisted of a total of 64 instances, with
customer numbers varying from 50 to 100 and demands ranging
from 25 to the maximum vehicle capacity. In the second case OVRP,
drivers are not required to return vehicles to the depot, resulting in
an open route scenario. In the final case VRPL, the duration limit
indicated by the route length is imposed on the drivers. In the last
VRPL case, only one dataset is collected with the largest vehicle
capacity, as duration limit constraints are typically not impactful
1http://vrp.atd-lab.inf.puc-rio.br/
 
1904KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fei Liu et al.
Table 3: Zero-shot generalization performance on five new VRPs.
VRP
Methodn50 n100VRP
Methodn50 n100
Dis. Gap Dis. Gap Dis.
Gap Dis. GapVRPBLHGS
7.70 - 11.15 -
O
VRPLTWHGS
10.69 - 17.35 -
NI 11.69 51.86% 17.38 55.92% NI
15.74 47.20% 26.16 50.78%
FI 11.61 50.81% 16.37 46.88% FI
15.22 42.41% 25.81 48.79%
POMO_CVRP 8.21 6.61% 12.41 11.37% POMO_
CVRP 15.23 42.46% 26.75 54.18%
POMO_VRPTW 13.43 74.45% 17.86 60.27% POMO_
VRPTW 11.51 7.70% 19.41 11.88%
Ours 7.97 3.48% 11.65 4.50% Ours 11.50
7.59% 19.34 11.50%O
VRPLHGS
6.49 - 9.71 -
O
VRPBTWHGS
10.67 - 17.31 -
NI 13.85 113.55% 13.61 40.17% NI
15.76 47.77% 26.24 51.60%
FI 14.34 121.11% 13.46 38.62% FI
15.22 42.69% 25.79 48.99%
POMO_CVRP 7.75 19.47% 11.78 21.35% POMO_
CVRP 15.25 42.98% 26.78 54.69%
POMO_VRPTW 10.80 66.54% 18.62 91.78% POMO_
VRPTW 11.52 7.94% 19.48 12.51%
Ours 6.69 3.10% 10.15 4.57% Ours 11.49
7.72% 19.32 11.61%VRPBT
WHGS
16.43 - 26.31 -
A
verageHGS
10.39 - 16.36 -
NI 18.92 15.15% 36.84 40.05% NI
15.19 46.16% 24.05 46.95%
FI 18.33 11.56% 36.59 39.10% FI
14.95 43.78% 23.60 44.25%
POMO_CVRP 22.98 39.85% 38.99 48.20% POMO_
CVRP 13.88 33.56% 23.34 42.64%
POMO_VRPTW 16.63 1.22% 27.18 3.33% POMO_
VRPTW 12.78 22.93% 20.51 25.34%
Ours 16.70 1.66% 27.11 3.05% Ours 10.87
4.59% 17.51 7.03%
Table 4: Results on CVRPLib datasets with diverse distribu-
tions and sizes.
Benchmark
Size BKSPOMO Ours
Dis. Gap Dis. Gap
Set
A 31-79 1041.9 1104.8 6.0% 1066.8 2.4%
Set B 30-77 963.7 1065.7 10.6% 992.2 2.9%
Set F 44-134 707.7 770.6 8.9% 760.6 7.5%
Set M 100-199 1083.6 1145.2 5.7% 1141.9 5.4%
Set P 15-100 587.4 660.4 12.4% 627.1 6.8%
Set
X100-300 33868.5 36299.07 7.2% 35954.7 6.2%
300-500 63774.8 71524.15 12.2% 68841.6 7.9%
500-700 88561.4 107927.9 21.9% 98843.8 11.6%
700-1000 113619.5 149858.6 31.9% 129270.9 13.8%
on vehicles with small capacity. Table 5 lists the statistics of the
industry logistic datasets.
We compared our unified model to models trained using single-
task learning in a zero-shot manner, without any fine-tuning or
adaptation. The three models under comparison were trained on
CVRP, OVRP, and VRPL datasets, respectively. Figure 6 depicts the
average gaps to the baseline HGS in these three cases. For CVRP and
OVRP, the results are averaged across all three sets. ST_Best denotes
the average gaps of the best result among the three single-task
learning models for each instance. Our unified model outperformed
the best results of the compared models, achieving gaps of less than
5% on CVRP and VRPL, and 10% on OVRP.
6 CONCLUSION
This work proposed the first cross-problem neural solver for vehicle
routing problems (VRPs). In our proposed method, different VRPs
are treated as combinations of several shared basic attributes (e.g.,
time windows, open routes, backhauls, and duration limits). In this
way, by learning over a few VRPs with basic attributes, we can
build a single unified model to tackle numerous VRPs in an end-to-
end manner. Experimental results demonstrated that our proposedTable 5: Real-world logistic application instance statistics
Case
Size Capacity Open Limit
CVRPSetA
50-100 L âœ— âœ—
SetB 50-100 M âœ— âœ—
SetC 50-100 S âœ— âœ—
OVRPSetA 50-100 L âœ“ âœ—
SetB 50-100 M âœ“ âœ—
SetC 50-100 S âœ“ âœ—
VRPL 50-100 L âœ— âœ“
CVRP OVRP VRPL0102030405060Performance Gap (%)ST_CVRP
ST_OVRP
ST_VRPL
ST_Best
Ours
Figure 6: Results on real-world logistic application. CVRP,
OVRP, and VRPL represent the three investigated appli-
cation cases. ST_CVRP, ST_OVRP, and ST_VRPL represent
the model trained on CVRP, OVRP, and VRPL, respectively.
ST_Best denotes the best result among the three single-task
learning models for each instance.
model can achieve good zero-shot generalization performance on
unseen VRP variants with new combinations of the basic attributes.
Remarkably, without any fine-tuning, our model can outperform
single-task models substantially, reducing the average performance
gap from over 20% to approximately 5% across eleven VRPs, and
achieving a notable performance boost on benchmark datasets as
well as a real-world logistics application. We provide our code
implementation at https://github.com/FeiLiu36/MTNCO.
 
1905Multi-Task Learning for Routing Problem with
Cross-Problem Zero-Shot Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ACKNOWLEDGMENTS
The work described in this paper was supported by the Research
Grants Council of the Hong Kong Special Administrative Region,
China [GRF Project No. CityU11215622], the National Natural Sci-
ence Foundation of China (Grant No. 62106096), the Natural Science
Foundation of Guangdong Province (Grant No. 2024A1515011759),
the National Natural Science Foundation of Shenzhen (Grant No.
JCYJ20220530113013031).
REFERENCES
[1]Ruibin Bai, Xinan Chen, Zhi-Long Chen, Tianxiang Cui, Shuhui Gong, Wentao He,
Xiaoping Jiang, Huan Jin, Jiahuan Jin, Graham Kendall, et al .2023. Analytics and
machine learning in vehicle routing research. International Journal of Production
Research 61, 1 (2023), 4â€“30.
[2]Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio.
2016. Neural combinatorial optimization with reinforcement learning. arXiv
preprint arXiv:1611.09940 (2016).
[3]Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2021. Machine learning for
combinatorial optimization: a methodological tour dâ€™horizon. European Journal
of Operational Research 290, 2 (2021), 405â€“421.
[4]Jieyi Bi, Yining Ma, Jiahai Wang, Zhiguang Cao, Jinbiao Chen, Yuan Sun, and
Yeow Meng Chee. 2022. Learning Generalizable Models for Vehicle Routing
Problems via Knowledge Distillation. arXiv preprint arXiv:2210.07686 (2022).
[5]Kris Braekers, Katrien Ramaekers, and Inneke Van Nieuwenhuyse. 2016. The
vehicle routing problem: State of the art classification and review. Computers &
industrial engineering 99 (2016), 300â€“313.
[6]Xinyun Chen and Yuandong Tian. 2019. Learning to perform local rewriting for
combinatorial optimization. Advances in Neural Information Processing Systems
32 (2019).
[7]Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018.
Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask
networks. In International conference on machine learning. PMLR, 794â€“803.
[8]Hanni Cheng, Haosi Zheng, Ya Cong, Weihao Jiang, and Shiliang Pu. 2023. Select
and Optimize: Learning to solve large-scale TSP instances. In Proceedings of The
26th International Conference on Artificial Intelligence and Statistics (Proceedings of
Machine Learning Research, Vol. 206), Francisco Ruiz, Jennifer Dy, and Jan-Willem
van de Meent (Eds.). PMLR, 1219â€“1231.
[9]Jinho Choo, Yeong-Dae Kwon, Jihoon Kim, Jeongwoo Jae, AndrÃ© Hottung, Kevin
Tierney, and Youngjune Gwon. 2022. Simulation-guided beam search for neural
combinatorial optimization. Advances in Neural Information Processing Systems
35 (2022), 8760â€“8772.
[10] Ronan Collobert and Jason Weston. 2008. A unified architecture for natural lan-
guage processing: Deep neural networks with multitask learning. In Proceedings
of the 25th international conference on Machine learning. 160â€“167.
[11] Darko Drakulic, Sofia Michel, Florian Mai, Arnaud Sors, and Jean-Marc Andreoli.
2023. BQ-NCO: Bisimulation Quotienting for Generalizable Neural Combinatorial
Optimization. arXiv preprint arXiv:2301.03313 (2023).
[12] Najib Errami, Eduardo Queiroga, Ruslan Sadykov, and Eduardo Uchoa. 2023.
VRPSolverEasy: a Python library for the exact solution of a rich vehicle routing
problem. (2023).
[13] Liang Feng, Yuxiao Huang, Ivor W Tsang, Abhishek Gupta, Ke Tang, Kay Chen
Tan, and Yew-Soon Ong. 2020. Towards faster vehicle routing by transferring
knowledge from customer representation. IEEE Transactions on Intelligent Trans-
portation Systems 23, 2 (2020), 952â€“965.
[14] Zhang-Hua Fu, Kai-Bin Qiu, and Hongyuan Zha. 2021. Generalize a small pre-
trained model to arbitrarily large tsp instances. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, Vol. 35. 7474â€“7482.
[15] Chengrui Gao, Haopu Shang, Ke Xue, Dong Li, and Chao Qian. 2023. Towards
Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with
Transferrable Local Policy. arXiv:2308.14104 [cs.LG]
[16] Simon Geisler, Johanna Sommer, Jan Schuchardt, Aleksandar Bojchevski, and
Stephan GÃ¼nnemann. 2022. Generalization of Neural Combinatorial Solvers
Through the Lens of Adversarial Robustness. arXiv:2110.10942 [cs.LG]
[17] Dan He, David Kuhn, and Laxmi Parida. 2016. Novel applications of multitask
learning and multiple output regression to multiple genetic trait prediction.
Bioinformatics 32, 12 (2016), i37â€“i43.
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[19] Keld Helsgaun. 2017. An extension of the Lin-Kernighan-Helsgaun TSP solver for
constrained traveling salesman and vehicle routing problems. Roskilde: Roskilde
University 12 (2017).
[20] AndrÃ© Hottung and Kevin Tierney. 2019. Neural large neighborhood search for
the capacitated vehicle routing problem. arXiv preprint arXiv:1911.09539 (2019).[21] Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi
Bennani, RÃ³bert CsordÃ¡s, Andrew Joseph Dudzik, Matko BoÅ¡njak, Alex Vitvitskyi,
Yulia Rubanova, et al .2022. A generalist neural algorithmic learner. In Learning
on Graphs Conference. PMLR, 2â€“1.
[22] Yuan Jiang, Yaoxin Wu, Zhiguang Cao, and Jie Zhang. 2022. Learning to solve
routing problems via distributionally robust optimization. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 36. 9786â€“9794.
[23] Chaitanya K Joshi, Quentin Cappart, Louis-Martin Rousseau, and Thomas Lau-
rent. 2022. Learning the travelling salesperson problem requires rethinking
generalization. Constraints 27, 1-2 (2022), 70â€“98.
[24] Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task learning using
uncertainty to weigh losses for scene geometry and semantics. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 7482â€“7491.
[25] Wouter Kool, Herke van Hoof, Joaquim Gromicho, and Max Welling. 2022. Deep
policy dynamic programming for vehicle routing problems. In Integration of
Constraint Programming, Artificial Intelligence, and Operations Research: 19th
International Conference, CPAIOR 2022, Los Angeles, CA, USA, June 20-23, 2022,
Proceedings. Springer, 190â€“213.
[26] Wouter Kool, Herke Van Hoof, and Max Welling. 2018. Attention, learn to solve
routing problems! arXiv preprint arXiv:1803.08475 (2018).
[27] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon,
and Seungjai Min. 2020. Pomo: Policy optimization with multiple optima for
reinforcement learning. Advances in Neural Information Processing Systems 33
(2020), 21188â€“21198.
[28] Bingjie Li, Guohua Wu, Yongming He, Mingfeng Fan, and Witold Pedrycz. 2022.
An overview and experimental study of learning-based optimization algorithms
for the vehicle routing problem. IEEE/CAA Journal of Automatica Sinica 9, 7
(2022), 1115â€“1138.
[29] Jingwen Li, Yining Ma, Ruize Gao, Zhiguang Cao, Andrew Lim, Wen Song, and
Jie Zhang. 2021. Deep reinforcement learning for solving the heterogeneous
capacitated vehicle routing problem. IEEE Transactions on Cybernetics 52, 12
(2021), 13572â€“13585.
[30] Jingwen Li, Liang Xin, Zhiguang Cao, Andrew Lim, Wen Song, and Jie Zhang.
2021. Heterogeneous attentions for solving pickup and delivery problem via deep
reinforcement learning. IEEE Transactions on Intelligent Transportation Systems
23, 3 (2021), 2306â€“2315.
[31] Xi Lin, Zhiyuan Yang, and Qingfu Zhang. 2022. Pareto Set Learning for Neu-
ral Multi-Objective Combinatorial Optimization. In International Conference on
Learning Representations.
[32] Fei Liu, Xi Lin, Weiduo Liao, Zhenkun Wang, Qingfu Zhang, Xialiang Tong,
and Mingxuan Yuan. 2024. Prompt Learning for Generalized Vehicle Routing.
International Joint Conference on Artificial Intelligence (2024).
[33] Sahil Manchanda, Sofia Michel, Darko Drakulic, and Jean-Marc Andreoli. 2023.
On the Generalization of Neural Combinatorial Optimization Heuristics. In Ma-
chine Learning and Knowledge Discovery in Databases: European Conference, ECML
PKDD 2022, Grenoble, France, September 19â€“23, 2022, Proceedings, Part V. Springer,
426â€“442.
[34] Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin TakÃ¡c.
2018. Reinforcement learning for solving the vehicle routing problem. Advances
in neural information processing systems 31 (2018).
[35] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. 2017. Zero-shot
task generalization with multi-task deep reinforcement learning. In International
Conference on Machine Learning. PMLR, 2661â€“2670.
[36] Xuanhao Pan, Yan Jin, Yuandong Ding, Mingxiao Feng, Li Zhao, Lei Song, and
Jiang Bian. 2023. H-TSP: Hierarchically Solving the Large-Scale Traveling Sales-
man Problem. In AAAI 2023.
[37] Bochra Rabbouch, Foued SaÃ¢daoui, and Rafaa Mraihi. 2021. Efficient implementa-
tion of the genetic algorithm to solve rich vehicle routing problems. Operational
Research 21 (2021), 1763â€“1791.
[38] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexan-
der Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost To-
bias Springenberg, et al .2022. A generalist agent. arXiv preprint arXiv:2205.06175
(2022).
[39] Frank Ruis, Gertjan Burghouts, and Doina Bucur. 2021. Independent prototype
propagation for zero-shot compositionality. Advances in Neural Information
Processing Systems 34 (2021), 10641â€“10653.
[40] Paolo Toth and Daniele Vigo. 2014. Vehicle routing: problems, methods, and
applications. SIAM.
[41] Raras Tyasnurita, Ender Ã–zcan, and Robert John. 2017. Learning heuristic selec-
tion using a time delay neural network for open vehicle routing. In 2017 IEEE
Congress on Evolutionary Computation (CEC). Ieee, 1474â€“1481.
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[43] Thibaut Vidal, Teodor Gabriel Crainic, Michel Gendreau, and Christian Prins.
2013. A hybrid genetic algorithm with adaptive diversity management for a large
class of vehicle routing problems with time-windows. Computers & operations
research 40, 1 (2013), 475â€“489.
 
1906KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fei Liu et al.
[44] Thibaut Vidal, Gilbert Laporte, and Piotr Matl. 2020. A concise guide to existing
and emerging vehicle routing problem variants. European Journal of Operational
Research 286, 2 (2020), 401â€“416.
[45] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks.
Advances in neural information processing systems 28 (2015).
[46] Chenguang Wang and Tianshu Yu. 2023. Efficient Training of Multi-task Neural
Solver with Multi-armed Bandits. arXiv:2305.06361 [cs.LG]
[47] Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. 2018.
Zero-shot learningâ€”a comprehensive evaluation of the good, the bad and the
ugly. IEEE transactions on pattern analysis and machine intelligence 41, 9 (2018),
2251â€“2265.
[48] Xiao-Tong Yuan, Xiaobai Liu, and Shuicheng Yan. 2012. Visual classification with
multitask joint sparse representation. IEEE Transactions on Image Processing 21,
10 (2012), 4349â€“4360.
[49] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transac-
tions on Knowledge and Data Engineering 34, 12 (2021), 5586â€“5609.
[50] Zizhen Zhang, Zhiyuan Wu, Hang Zhang, and Jiahai Wang. 2022. Meta-Learning-
Based Deep Reinforcement Learning for Multiobjective Optimization Problems.
IEEE Transactions on Neural Networks and Learning Systems (2022), 1â€“14. https:
//doi.org/10.1109/TNNLS.2022.3148435
[51] Jiuxia Zhao, Minjia Mao, Xi Zhao, and Jianhua Zou. 2020. A hybrid of deep
reinforcement learning and local search for the vehicle routing problems. IEEE
Transactions on Intelligent Transportation Systems 22, 11 (2020), 7208â€“7218.
[52] Jianan Zhou, Yaoxin Wu, Wen Song, Zhiguang Cao, and Jie Zhang. 2023. Towards
Omni-generalizable Neural Methods for Vehicle Routing Problems. In the 40th
International Conference on Machine Learning (ICML 2023).
[53] Tianyu Zhu, Xinli Shi, Xiangping Xu, and Jinde Cao. 2023. An accelerated
end-to-end method for solving routing problems. Neural Networks (2023).
A MODEL DETAILS
A.1 Attention
We use the attention mechanism in Vaswani et al . [42] , which is
a mapping (message passing [ 26]) of queryğ‘„, keyğ¾, and valueğ‘‰
vectors to an output. For each node ğ‘–, the query ğ‘„ğ‘–, keyğ¾ğ‘–, and
valueğ‘‰ğ‘–are projections of the input embedding â„ğ‘–:
ğ‘„ğ‘–=ğ‘Šğ‘„â„ğ‘–,ğ¾ğ‘–=ğ‘Šğ¾â„ğ‘–,ğ‘‰ğ‘–=ğ‘Šğ‘‰â„ğ‘–. (4)
where the parameters ğ‘Šğ‘„andğ‘Šğ¾are of size (ğ‘‘ğ‘˜Ã—ğ‘‘â„) andğ‘Šğ‘‰is
of size (ğ‘‘ğ‘£Ã—ğ‘‘â„). We compute the compatibility ğ‘¢ğ‘–ğ‘—from the queries
and keys as follows:
ğ‘¢ğ‘–ğ‘—=ğ‘„ğ‘‡
ğ‘–ğ¾ğ‘—âˆšï¸
ğ‘‘ğ‘˜. (5)
Then we scale the compatibilities ğ‘¢ğ‘–ğ‘—using softmax to get attention
weightsğ‘ğ‘–ğ‘—âˆˆ[0,1]:
ğ‘ğ‘–ğ‘—=ğ‘’ğ‘¢ğ‘– ğ‘—
Ã
ğ‘—ğ‘’ğ‘¢ğ‘– ğ‘—. (6)
The output vector â„ğ‘œ
ğ‘–for nodeğ‘–is the combination of the weights
ğ‘ğ‘–ğ‘—and valuesğ‘‰ğ‘—:
â„ğ‘œ
ğ‘–=âˆ‘ï¸
ğ‘—ğ‘ğ‘–ğ‘—ğ‘‰ğ‘—. (7)
A.2 Multi-Head Attention (MHA)
Multi-head attention enables the model to learn diverse information
and usually benefits the results. MHA consists of â„heads and each
head is an attention. It concatenates the results from all heads with
a linear projection.
ğ‘€ğ»ğ´(â„1,...,â„ğ‘›)=ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„ğ‘’ğ‘ğ‘‘ 1,...,â„ğ‘’ğ‘ğ‘‘â„)ğ‘Šğ‘‚
â„ğ‘’ğ‘ğ‘‘ğ‘–=ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(â„1,...,â„ğ‘›)(8)
whereğ‘Šğ‘‚has size (â„ğ‘‘ğ‘£Ã—ğ‘‘ğ‘˜). In our experiments, we use 8 heads
with different parameters, and the embedding size is 128. For theattention model in each head, the parameter dimensions are ğ‘‘ğ‘˜=
ğ‘‘ğ‘£=ğ‘‘â„/â„=16.
A.3 Decoder Details
We use an MHA followed by a SHA in the decoder following Kool
et al. [26] . The computing of queries, keys, and values for the MHA
are as follows:
ğ‘„ğ‘=ğ‘Šğ‘„â„ğ‘,ğ¾ğ‘–=ğ‘Šğ¾â„ğ‘–,ğ‘‰ğ‘–=ğ‘Šğ‘‰â„ğ‘–,
â„ğ‘=ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„ğ‘¡,ğ‘ğ‘¡),(9)
whereâ„ğ‘¡is the embedding of the current visited node and ğ‘ğ‘¡is the
attribute vector. â„ğ‘–is the output embedding from the encoder for
nodeğ‘–.
In the SHA, we compute the compatibility using equation (5)
and clip the results within [-10,10] with tanh. We also exclude the
masked nodes by setting their compatibility values to -inf:
ğ‘¢ğ‘ğ‘—=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³10Â·ğ‘¡ğ‘ğ‘›â„ 
ğ‘ğ‘‡ğ‘ğ‘˜ğ‘—âˆšï¸
ğ‘‘ğ‘˜!
ifğ‘—âˆ‰ğ‘šğ‘¡
âˆ’ğ‘–ğ‘›ğ‘“ otherwise(10)
The output probability of selecting next node is computed as the
softmax of the output compatibilities ğ‘ğ‘–=ğ‘’ğ‘¢ğ‘–Ã
ğ‘—ğ‘’ğ‘¢ğ‘—.
A.4 Attribute Procedures
Capacity We track the remaining vehicle capacity ğ‘ğ‘¡at each step
ğ‘¡, which is initially set to be the capacity of the vehicle ğ‘1=1(all
demands have been scaled by the capacity). After selecting a new
nodeğ‘£ğ‘¡, we update the remaining capacity as:
ğ‘ğ‘¡=ğ‘ğ‘¡âˆ’1âˆ’ğ‘‘ğ‘¡ (11)
whereğ‘ğ‘¡âˆ’1represents the remaining capacity from the previous
step, andğ‘‘ğ‘¡represents the demand of the selected node in the
current step ğ‘¡.
We mask these nodes that have already been visited or have
demands that exceed the remaining vehicle capacity.
Time windows We keep track of the current time ğ‘¡ğ‘¡at each
stepğ‘¡, initialized as ğ‘¡1=0. After the selection of a new node, we
update the current time as:
ğ‘¡ğ‘¡=ğ‘šğ‘ğ‘¥(ğ‘¡ğ‘¡âˆ’1+ğ‘(ğ‘¡âˆ’1),ğ‘¡,ğ‘’ğ‘¡)+ğ‘ ğ‘¡ (12)
whereğ‘¡ğ‘¡âˆ’1represents the time from the previous step. ğ‘(ğ‘¡âˆ’1),ğ‘¡
represents the distance between the last node and the current node
(i.e., the traveling time cost between two nodes). ğ‘’ğ‘¡andğ‘ ğ‘¡represent
the early time window and the service time at node ğ‘£ğ‘¡, respectively.
We mask the visited nodes and the nodes whose time windows
cannot be satisfied: 1) when we are unable to visit the node within
the feasible time windows starting from the current node, or 2)
when visiting the node would result in being too late to return back
to the depot.
Duration limit We keep track of the route length ğ‘™ğ‘¡at each step
ğ‘¡, which is initialized to be zero ğ‘™ğ‘¡=0. We update the current route
length by adding the route length from the previous step ğ‘¡âˆ’1and
the distance between the last node and the current node:
ğ‘™ğ‘¡=ğ‘™ğ‘¡âˆ’1+ğ‘(ğ‘¡âˆ’1),ğ‘¡ (13)
We mask the nodes that exceed the duration limit when selected.
 
1907Multi-Task Learning for Routing Problem with
Cross-Problem Zero-Shot Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Open route We only need a fixed binary indicator for the open
route attribute, with ğ‘œğ‘¡=1representing that the route is open and
ğ‘œğ‘¡=0otherwise. It does not contribute to the masking vector.
However, unlike other attributes, the open route attribute results
in a different total distance calculation. The distance between the
last node and the depot is not included.
B DATE GENERATION DETAILS
Coordinates Generation: We createğ‘›+1random points where
each pointâ€™s coordinates, ğ‘¥ğ‘–andğ‘¦ğ‘–, are independently drawn from
a uniform distribution ğ‘ˆ(0,1)for allğ‘–in the set{0,...,ğ‘›}. Here,
the coordinates[ğ‘¥0,ğ‘¦0]denote the depotâ€™s location, while [ğ‘¥ğ‘–,ğ‘¦ğ‘–]
forğ‘–âˆˆ{1,...,ğ‘›}correspond to the locations of the other ğ‘›nodes.
Vehicle Capacity: The carrying capacity ğ¶of the vehicle is set at
40 for problem instances with 50 nodes and 50 for instances with
100 nodes.
Route Type: The route type is determined by an instance-specific
flag. If the flag is set to 1, the route is considered open; if it is set to
0, the route is closed.
Linehaul and Backhaul Demands: Each customerâ€™s demand is
assigned by randomly selecting an integer from {1,..., 9}. In the
Vehicle Routing Problem with Backhauls (VRPB), 20% of the cus-
tomers are designated as backhaul customers. The demands for
these backhaul customers are the negative values of their originally
assigned demands. To normalize, both linehaul and backhaul de-
mands are scaled to the range [0,1]by dividing by the vehicleâ€™s
capacity.
Duration limits: Each route is allocated a specific duration limit,
denoted as ğ¿, which is set at 3. Given that the coordinates for
both the depot and the customers fall within the range [0,1], it
is guaranteed that a feasible route can always be established that
adheres to this duration limit for any customer.
Time Windows: Time windows for each node ğ‘–, whereğ‘–ranges
from 1 toğ‘›, are determined through the following procedure:
(1) Assign a service time ğ‘ ğ‘–randomly chosen from [0.15, 0.2].
(2)Set a time window length ğ‘¡ğ‘–randomly chosen from the in-
terval [0.15, 0.2].
(3) Compute the distance ğ‘‘0ğ‘–from the node to the depot.
(4)Compute the upper limit for the start time â„ğ‘–using the for-
mulaâ„ğ‘–=ğ‘¡ğ‘šğ‘ğ‘¥âˆ’ğ‘ ğ‘–âˆ’ğ‘¡ğ‘–
ğ‘‘0ğ‘–âˆ’1, whereğ‘¡ğ‘šğ‘ğ‘¥ is the maximum al-
lowable time, which is set to be 4.6.
(5)Determine the start time ğ‘’ğ‘–asğ‘’ğ‘–=(1+(â„ğ‘–âˆ’1)Â·ğ‘¢ğ‘–)Â·ğ‘‘0ğ‘–,
whereğ‘¢ğ‘–is generated uniformly from [0, 1].
(6) Calculate the end time ğ‘™ğ‘–asğ‘™ğ‘–=ğ‘’ğ‘–+ğ‘¡ğ‘–.
C FAST FINE-TUNING ON UNSEEN VRPS
We conduct experiments to show the performance of our model
under fast fine-tuning. Two different fine-tuning settings are tested:
1) only updating the decoder while keeping the encoder fixed, and
2) updating the entire model. Each epoch is trained using 10,000
instances with a batch size of 64, and 200 epochs are used. The
learning rate and weight decay are set to 1e-5 and 1e-6, respectively.
The experiments are carried out on the instances of size 100. The
entire fine-tuning process with 200 epochs takes approximately
01020304050
VRPBL OVRPL VRPBTW OVRPLTW OVRPBTWST_CVRP
ST_VRPTW
NI
FI
Ours
Ours FTD
Ours FT
HGSFigure 7: Comparison of the average results (total distances)
of different methods on five new VRPs.
17.018.019.020.021.022.023.0
0 200 400 600 800 1000
Ours + FTD Ours + FT From scratch
Figure 8: Testing distance vs. epoch number. FTD and FT
indicate fine-tuning on the decoder and the entire network
of our pre-train unified model, respectively. The curve with
grey circles represents training a model from scratch.
five hours, while we note that the model typically converges within
the first 50 epochs.
Figure 7 compares different methods on the five VRPs. The abbre-
viations POMO_CVRP and POMO_VRPTW denote the single-task
model (POMO) trained on CVRP and VRPTW instances, respec-
tively. NI and FI represent the nearest and farthest insertion heuris-
tics, and HGS is the SOTA algorithm for VRPs. FTD and FT indicate
fine-tuning on the decoder and the entire network, respectively.
The results indicate that fine-tuning can further improve the
performance of our model on new VRPs. The advantages of fast
fine-tuning become more apparent on OVRPLTW and OVRPBTW,
which have more attributes and are therefore more complicated. In
contrast, fine-tuning only provides minor improvements on VRPBL,
OVRPL, and VRPBTW, where zero-shot generalization has already
produced satisfactory results.
Figure 8 illustrates the convergence of testing distance vs. the
number of fine-tuning epochs on OVRPLTW. We compare the per-
formance of fine-tuning the pre-trained model with training a new
model from scratch. The experimental training settings from scratch
are the same as that used for fine-tuning. The results demonstrate
that fine-tuning based on our pre-trained unified model converges
rapidly. The fine-tuning of the entire pre-trained model takes less
than 50 epochs (about 1.5 hours) to converge, outperforming the
results of training from scratch using 1,000 epochs. The advantage
of fast adaptation highlights the significance of our pre-trained
unified model.
https://github.com/FeiLiu36/MTNCO includes more results on
eleven VRPs.
 
1908