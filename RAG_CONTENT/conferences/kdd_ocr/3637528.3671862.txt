ShapeFormer: Shapelet Transformer for Multivariate Time Series
Classification
Xuan-May Le
xuanmay.le@student.unimelb.edu.au
The University of Melbourne
Melbourne, Victoria, AustraliaLing Luo
ling.luo@unimelb.edu.au
The University of Melbourne
Melbourne, Victoria, Australia
Uwe Aickelin
uwe.aickelin@unimelb.edu.au
The University of Melbourne
Melbourne, Victoria, AustraliaMinh-Tuan Tran
tuan.tran7@monash.edu
Monash University
Melbourne, Victoria, Australia
ABSTRACT
Multivariate time series classification (MTSC) has attracted signifi-
cant research attention due to its diverse real-world applications.
Recently, exploiting transformers for MTSC has achieved state-of-
the-art performance. However, existing methods focus on generic
features, providing a comprehensive understanding of data, but they
ignore class-specific features crucial for learning the representative
characteristics of each class. This leads to poor performance in the
case of imbalanced datasets or datasets with similar overall pat-
terns but differing in minor class-specific details. In this paper, we
propose a novel Shapelet Transformer (ShapeFormer), which com-
prises class-specific and generic transformer modules to capture
both of these features. In the class-specific module, we introduce
the discovery method to extract the discriminative subsequences
of each class (i.e. shapelets) from the training set. We then pro-
pose a Shapelet Filter to learn the difference features between these
shapelets and the input time series. We found that the difference
feature for each shapelet contains important class-specific features,
as it shows a significant distinction between its class and others. In
the generic module, convolution filters are used to extract generic
features that contain information to distinguish among all classes.
For each module, we employ the transformer encoder to capture
the correlation between their features. As a result, the combina-
tion of two transformer modules allows our model to exploit the
power of both types of features, thereby enhancing the classifi-
cation performance. Our experiments on 30 UEA MTSC datasets
demonstrate that ShapeFormer has achieved the highest accuracy
ranking compared to state-of-the-art methods. The code is available
at https://github.com/xuanmay2701/shapeformer.
CCS CONCEPTS
â€¢Information systems â†’Data mining; â€¢Computing method-
ologiesâ†’Machine learning.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671862KEYWORDS
time series; shapelet; transformer; attention; classification
ACM Reference Format:
Xuan-May Le, Ling Luo, Uwe Aickelin, and Minh-Tuan Tran. 2024. Shape-
Former: Shapelet Transformer for Multivariate Time Series Classification.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671862
1 INTRODUCTION
A multivariate time series (MTS) is a collection of data points where
each point is composed of multiple variables that have been ob-
served or measured over time. This data structure is prevalent in
various fields, such as economics [ 29], weather prediction [ 27], ed-
ucation [ 7], and healthcare [ 34]. Time series classification stands
out as a fundamental and crucial aspect within the domain of time
series analysis [ 30]. However, there are still many challenges in the
research on MTS classification (MTSC) [ 30], especially in capturing
the correlations among variables.
Over the past few decades, various approaches have been intro-
duced to enhance the performance of MTSC [ 12,23,33,41,48,50].
Among these, shapelets, which are class-specific time series subse-
quences, have demonstrated their effectiveness in [ 14,21,23,44].
This success comes from the fact that each shapelet contains class-
specific information representative of its class. It is evident that the
distance between the shapelet and the time series of its class is far
smaller than the time series of other classes (see Figure 1). Hence,
there has been an increased focus on harnessing the capabilities of
shapelets in the field of MTSC.
In 2017, Vaswani et al. [ 40] introduced the breakthrough Trans-
former architecture, initially designed for Natural Language Pro-
cessing but later demonstrating success in Computer Vision tasks
[8]. Following these successes, Transformer-based models have
been effectively applied to MTSC. GTN [ 26] employs a two-tower
multi-headed attention approach to extract distinctive information
from input series, SVP-T [ 50] captures short- and long-term depen-
dencies among subseries using clustering and employing them as
inputs for the Transformer, and ConvTran [ 10] integrates absolute
and relative position encoding for improved position embedding in
the Transformer model.
Obviously, Transformers utilised in MTSC have demonstrated
state-of-the-art (SOTA) performances [ 10,47,50]. Existing methods
 
1484
KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain Xuan-May Le, Ling Luo, Uwe Aickelin, & Minh-Tuan Tran
Best
 -
fit Subsequences
 Shapelet
(c)
Other
Class
(b)
Other
Class
(a)
Same
Class
distance=0.1
distance=2.7
distance=1.9
Figure 1: The illustration depicts the shapelet in the Atrial Fibrilla-
tion dataset. The best-fit subsequence is the subsequence with the
sortest distance to the shapelet in the time series. It is clear that the
shapelet can discriminate between classes by utilising their distance
to the best-fit subsequences.
(a) Generic Feature (b) Class -specific Feature 
Figure 2: The separating hyperplane using (a) the generic feature
has a higher overall accuracy, while the hyperplane using (b) the
class-specific feature is better in classifying a single class.
only discover the generic features from timestamps [ 10,26,47] or
common subsequences [ 50] in time series as inputs for the Trans-
former model to capture the correlation among them. These fea-
tures merely contain generic characteristics of time series, offering
a broad understanding of the data. Nevertheless, they overlook the
essential class-specific features necessary to allow the model to
capture the representative characteristics of each class. As a result,
the model exhibits poor performance in two cases: 1) the dataset
has instances that are very similar in overall patterns, differing only
in minor class-specific patterns, effective classification cannot be
achieved using solely generic features; 2) the imbalanced dataset,
where generic features only focus on classifying the majority classes
and ignore those of minority. As can be seen in Figure 2, the hy-
perplane created using the generic feature (Figure 2a) attempts
to classify the majority classes (orange triangles and blue circles)
and ignores the minority (green squares), while the class-specific
feature (Figure 2b) tries to separate each class from the others.
To address the aforementioned problem, we propose a novel
method called Shapelet Transformer (ShapeFormer), which com-
prises class-specific and generic transformer modules to capture
both of these features. In the class-specific module, we initially in-
troduce Offline Shapelet Discovery, inspired by [ 21], to MTS. Based
on this, we extract a small number of high-quality shapelets from
the training set. Subsequently, we propose a Shapelet Filter that
leverages the precomputed shapelets to discover the best-fit subse-
quences in the input time series. Following this, the Shapelet Filterlearns the difference between the embedding of these shapelets and
their most fitting subsequences derived from the input time series.
As shown in Figure 1, the distance of shapelets to the time series in
the same class is far smaller than the time series of other classes.
Similar to the distance, our difference feature also highlights the
substantial distinctions among classes. Additionally, rather than
using the original shapelets extracted from the dataset, we propose
considering these shapelets as the initialisation and then dynami-
cally optimising shapelets during training to effectively represent
the distinguishing information. In the generic module, we utilise
convolution filters for the extraction of features over all classes. For
each module, we employ the transformer encoder to capture the
dependencies between their features. Through the integration of
these two modules, our ShapeFormer excels in capturing not only
class-specific features but also generic characteristics from time
series data. This dual capability contributes to an enhancement in
the overall performance of classification tasks.
Our contributions can be summarised as follows:
â€¢We introduce ShapeFormer, which effectively captures both class-
specific and generic discriminative features in time series.
â€¢We propose the Offline Shapelet Discovery for MTS to effectively
and efficiently extract shapelets from training set.
â€¢We propose the Shapelet Filter, which learns the difference be-
tween shapelets and input time series, which contain important
class-specific features. The shapelets are also dynamically opti-
mised during training to effectively represent the class distin-
guishing information.
â€¢We conduct experiments on all 30 UEA MTS datasets and demon-
strate that ShapeFormer has achieved the highest accuracy rank-
ing compared to SOTA methods.
To the best of our knowledge, our ShapeFormer is a pioneering
transformer-based approach that leverages the power of shapelets
for MTSC.
2 RELATIVE WORKS
2.1 Multivariate Time Series Classification
We categorise the MTSC methods into two main categories: non-
deep learning, and deep learning.
Non-deep learning methods. They primarily utilise distance
measures [ 38,39], such as Euclidean Distance [ 20], Dynamic Time
Warping, and its diverse variants [ 3,19], to calculate the similar-
ity between time series. Otherwise, they leverage special features,
such as bag of patterns [ 24], Symbolic Aggregate approXimation
[22], bag of SFA symbols [ 32], and convolution kernel features
[5,35] for classification. [ 31] gives a comprehensive survey of the
conventional methods mentioned.
Deep learning methods. Various neural network methods were
proposed for MTSC [ 16]. Specifically, the LSTM-FCN [ 17] model
features an LSTM layer and stacked CNN layers which directly
extract features from time series. These features are subsequently
fed into a softmax layer to produce class probabilities. However,
it has a limitation in capturing long dependencies among differ-
ent variables. To address this, Hao et al [ 15]. proposed to use of
two cross-attention modules to enhance their CNN-based model.
TapNet [ 48] constructs an attentional prototype network that in-
corporates LSTM, and CNN to learn multi-dimensional interaction
 
1485ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain
features. RLPAM [ 12] adopts a unique approach by transforming
MTS into a univariate cluster sequence and subsequently employs
reinforcement learning for pattern selection. WHEN [ 41] was pro-
posed to learn heterogeneity by utilising a hybrid attention network,
incorporating both DTW attention and wavelet attention.
2.2 Transformer-Based Time Series Classifiers
In 2017, Vaswani et al. introduced the Transformer architecture,
achieving a breakthrough in Natural Language Processing [ 40] and
demonstrating notable success in Computer Vision tasks [ 8]. Re-
cently, it has proven effective in time series classification tasks.
Specifically, GTN [ 26] utilises a two-tower multi-headed attention
approach for extracting distinctive information from the input se-
ries. The integration of the output from the two towers is achieved
through gating, implemented by a learnable matrix. ConvTran [ 10]
was proposed to enhance the position embedding by leveraging
both absolute and relative position encoding. SVP-T [ 50] uses clus-
tering to identify time series subsequences and employs them as
inputs for the Transformer, enabling the capture of long- and short-
term dependencies among subseries. Recently, the application of
pretrained transformer-based self-supervised learning models like
BERT [ 6] has achieved significant success not only in the field
of NLP but also in other areas [ 36,37,43,45]. Inspired by these
successes, many models attempt to adopt a similar structure for
time series classification [ 46,47]. It is noteworthy that most pre-
vious transformer-based methods effectively exploit the generic
information of time series.
2.3 Shapelet Discovery for Time Series
Shapelets refer to short subsequences within time series that con-
tain class-specific information by exhibiting a small distance to the
time series of the target class and a larger distance to other classes
(see Figure 1). Additionally, each shapelet can encompass crucial
subsequences located at different positions and variables within a
time series. This coverage enables them to effectively represent the
time series. In the last decade, the effectiveness of shapelets for time
series has been proven by many related studies [13, 21, 23, 25, 44].
The original shapelet discovery method [ 44] extracts all possible
subsequences in the training set and considers the subsequences
as shapelets when they have the highest information gain ratio. It
requires excessive computing time and is hard to apply to MTSC.
Other methods use random shapelets that lack position and vari-
able information [ 14], or employ the common subsequences as
shapelets, which unfortunately have limited discriminative features
[23]. Recently, [ 21] proposed the hyperfast Offline Shapelet Dis-
covery (OSD), which utilises important points to extract a small
number of high-quality shapelets from the original time series data.
It has been demonstrated to be a SOTA method for univariate time
series classification.
3 PRELIMINARIES
Multivariate Time Series Classification. We represent MTS as
XâˆˆRğ‘‰Ã—ğ‘‡, whereğ‘‰denotes the number of variables and ğ‘‡rep-
resents the length of the time series. Here, X=ğ‘‹1,...,ğ‘‹ğ‘‰, and
eachğ‘‹ğ‘£corresponds to a time series for variable ğ‘£. Specifically,
Shapelet
Discovery
ğ‘ˆ1ğ‘ˆ2ğ‘ˆ3
Transformer
Encoderğ‘ˆ4
ğ‚ğ¨ğ§ğ¯ğŸğƒ
ğ‘‰1ğ‘‰2ğ‘‰3
Transformer
Encoderğ‘‰4
Classification Headğƒğšğ­ğšğ¬ğğ­ ğˆğ§ğ©ğ®ğ­ ğ“ğ¢ğ¦ğ ğ’ğğ«ğ¢ğğ¬ ğ—
Shapelet  
Filter
ğˆğ§ğ©ğ®ğ­ ğ“ğ¢ğ¦ğ ğ’ğğ«ğ¢ğğ¬ ğ—
ğ‘†1
ğ‹ğ¢ğ§ğğšğ« ğ‹ğšğ²ğğ«ğ¼1
ğ¸ğ¼1ğ¸ğ‘†1
ğ‘ˆ1-ğ‘†2
ğ¼2
ğ¸ğ‘†2ğ¸ğ¼2
ğ‘ˆ2-
(a) Framework of ShapeFormer (b) Shapelet  Filterğ’ğ¡ğšğ©ğğ¥ğğ­ ğğ¨ğ¨ğ¥ğ’ğ¡ğšğ©ğğ¥ğğ­  
ğğ¨ğ¨ğ¥
Best -fit 
Subsequence
Finding
Position Embedding
 Position Embedding+ + + + + + + +Class -specific Transformer Generic Transformer
ğğšğ­ğœğ¡ğğ¨ğ«ğ¦
ğ†ğ„ğ‹ğ”x2Figure 3: The general architecture of ShapeFormer.
ğ‘‹ğ‘£=ğ‘¥ğ‘£
1,...,ğ‘¥ğ‘£
ğ‘‡, whereğ‘¥ğ‘£
1signifies a value for variable ğ‘£at times-
tampğ‘¡within X. Consider a training dataset D={(ğ‘¿ğ‘–,ğ’€ğ‘–)}ğ‘€
ğ‘–=1,
whereğ‘€is the number of time series instances and the pair (ğ‘¿ğ‘–,ğ’€ğ‘–)
represents a training sample and its corresponding label, respec-
tively. The objective of MTSC is to train a classifier ğ‘“(ğ‘‹)to predict
a class label for a multivariate time series with an unknown label.
Time Series Subsequence. Given a time series ğ‘‹of lengthğ‘‡, a
time series subsequence ğ‘‹[ğ‘ğ‘ :ğ‘ğ‘’]=ğ‘¥ğ‘ğ‘ ,...,ğ‘¥ğ‘ğ‘’is a consecutive
subsequence of time series ğ‘‹, whereğ‘ğ‘ is a start index and ğ‘ğ‘’is an
end index.
Perceptual Subsequence Distance (PSD). Given a time series ğ‘‹
of lengthğ‘‡, and a subsequence ğ‘†=ğ‘ 1,...,ğ‘ ğ‘™of lengthğ‘™, withğ‘™â‰¤ğ‘‡,
the PSD [21] of ğ‘‹andğ‘†is determined as:
ğ‘ƒğ‘†ğ·(ğ‘‹,ğ‘†)=ğ‘‡âˆ’ğ‘™+1
min
ğ‘—=1(CID(ğ‘‡[ğ‘—:ğ‘—+ğ‘™âˆ’1],ğ‘†)), (1)
where CID is the complexity-invariant distance, which is commonly
used in time series mining, in general, [ 2] and shapelet discovery
in particular [21].
4 SHAPELET TRANSFORMER MODEL
We propose ShapeFormer, a transformer-based method that lever-
ages the strength of both class-specific and generic features in time
series. In contrast to existing transformer-based MTSC methods
[26,47,50], our approach first extracts shapelets from the training
datasets (Section 4.1). Subsequently, these extracted shapelets are
used to discover discriminative features in time series through the
use of a class-specific transformer module (Section 4.2). Addition-
ally, we introduce the use of convolution layers with a generic trans-
former module to extract generic features in time series (Section
4.3). Finally, the overall architecture of ShapeFormer is summarised
in Section 4.4 and Figure 3.
4.1 Shapelet Discovery
This section introduces the Offline Shapelet Discovery (OSD) method,
inspired by [ 21], to multivariate time series. In contrast with other
methods, our OSD employs Perceptually Important Points (PIPs)
[4], condensing time series data by choosing points that closely
resemble the original, to efficiently select high-quality shapelets.
The selection process is based on the reconstruction distance, with
the highest index continuously chosen. We define the reconstruc-
tion distance as the perpendicular distance between a target point
 
1486KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain Xuan-May Le, Ling Luo, Uwe Aickelin, & Minh-Tuan Tran
Figure 4: The process of Offline Shapelet Discovery.
and a line reconstructed by the two nearest selected important
points[ 4,21]. The process of our OSD is illustrated in Figure 4
and the pseudo-code is presented in Algorithm 1. Given the dataset
D={(ğ‘¿ğ‘–,ğ’€ğ‘–)}ğ‘€
ğ‘–=1, our method contains two main phases, including
shapelet extraction and shapelet selection.
In the first phase, our OSD initially extracts shapelet candidates
by identifying PIPs. Specifically, the first and last indices are added
to the PIPs set. Subsequently, the index with the highest reconstruc-
tion distance is continuously added to the PIPs set. Each time a
new PIP is added, we extract new shapelet candidates with three
consecutive PIPs points. This means that, with each new PIP, a
maximum of three shapelet candidates can be added to the set. In
this paper, we set the number of PIPs as ğ‘›ğ‘ğ‘–ğ‘ =0.2Ã—ğ‘‡, where
ğ‘‡represents the time series length. Our method aims to select a
maximum of 3Ã—ğ‘›ğ‘ğ‘–ğ‘ candidates, therefore, we only extract an av-
erage of 5900 candidates for each dataset. This count is significantly
smaller than the 45 million candidates typically extracted through
classic shapelet discovery methods [ 25,44], thereby significantly
speeding up the process. We then store four types of information
for each shapelet, including the value vector of shapelets, its start
index, end index, and variables.
In the second phase, our method selects an equal number of
shapelets for each class. Given the shapelet candidate ğ‘†ğ‘–of classğ‘Œğ‘–,
we first compute its PSD with all instances in the training datasets
(Eq. 1). After that, their distance will be used to find optimal in-
formation gain. This implies that the optimal information gain is
the highest ratio achievable by the shapelet ğ‘†ğ‘–[21]. Finally, the top
ğ‘”candidates with the highest information gain are chosen as the
shapelets and stored in the shapelet pool ğ‘º.
4.2 Class-Specific Transformer
To utilise the class-specific characteristics of shapelets, we first
propose the Shapelet Filter which is used to effectively discover
input tokens for the transformer model.Algorithm 1: Offline Shapelet Discovery
Input:D={(ğ‘¿ğ‘–,ğ’€ğ‘–)}ğ‘€
ğ‘–=1: dataset; time series length: ğ‘‡; number
of variables: ğ‘‰, number of PIPs: ğ‘˜; number of shapelets: ğ‘”;
set of classes:Y;|ğ‘Œ|is the number of classes.
/* Shapelet Extracting */
1C= [];
2foreachğ‘‹âˆ¼ğ‘«do
3 forğ‘£= 1 toğ‘‰do
4 ğ‘·= [1,ğ‘‡] #Add the first and last index into PIPs set: ğ‘ƒ;
5 forğ‘—= 1 to k -2 do
6 Find indexğ‘from 1 toğ‘‡with maximum
reconstruction distance;
7 ğ‘·.append(ğ‘).sorted() # Add a new index ğ‘intoğ‘·;
8 ğ‘–ğ‘‘ğ‘¥=ğ‘·.index(ğ‘)forğ‘§= 0 to 2 do
9 #Validating newly generated candidates ;
10 ifğ‘–ğ‘‘ğ‘¥+2â‰¤|ğ‘°|andğ‘–ğ‘‘ğ‘¥âˆ’ğ‘§â‰¥1then
11 ğ‘–ğ‘‘ğ‘¥ğ‘ =ğ‘·[ğ‘–ğ‘‘ğ‘¥âˆ’ğ‘§];
12 ğ‘–ğ‘‘ğ‘¥ğ‘’=ğ‘·[ğ‘–ğ‘‘ğ‘¥+2âˆ’ğ‘§];
13 ğ¶=ğ‘‹[ğ‘–ğ‘‘ğ‘¥ğ‘ :ğ‘–ğ‘‘ğ‘¥ğ‘’];
14 Add new candidates ğ¶, its start index ğ‘–ğ‘‘ğ‘¥ğ‘ ,
end indexğ‘–ğ‘‘ğ‘¥ğ‘’, and its variables ğ‘£intoC.
/* Shapelet Selecting */
15foreachğ‘†ğ‘–âˆ¼Cdo
16ğ·= [];
17 foreachğ‘‹âˆ¼ğ‘«do
18ğ‘‘=PSD(ğ‘‹,ğ‘†ğ‘–)(Eq. 1);
19ğ·.append(ğ‘‘);
20 Compute the optimal information gain of ğ‘†ğ‘–usingğ·;
21S= [];
22foreachğ‘Œğ‘–âˆ¼Y do
23 Select the top ğ‘”/|ğ‘Œ|shapelets ranked by information gain in
classğ‘Œğ‘–fromC; Add them to setS;
24returnS
Shapelet Filter. Given a shapelet pool S(as discussed in Section
4.1), an input time series ğ‘‹and its label ğ‘Œ, we first select the best-fit
subsequence for each shapelet in S(refer to Figure 5a). Specifically,
with each shapelet ğ‘†ğ‘–âˆˆS, its lengthğ‘™, start index ğ‘ğ‘–ğ‘ , end index
ğ‘ğ‘–ğ‘’and variables ğ‘£ğ‘–, we calculate the distance CID of them with all
subsequences in time series ğ‘‹[21]. After that, the subsequence with
the shortest distance will be selected as an important subsequence
ğ¼ğ‘–ofğ‘†ğ‘–.
index =argminğ‘‡âˆ’ğ‘™+1
ğ‘—=0ğ¶ğ¼ğ·(ğ‘‹[ğ‘—:ğ‘—+ğ‘™],ğ‘†ğ‘–), (2)
ğ¼ğ‘–=ğ‘‹[index :index+ğ‘™]. (3)
To reduce computing time and effectively utilise the position in-
formation of the shapelet, we propose limiting the search for the
best-fit subsequence to a neighbouring area within the hyperpa-
rameter window size ğ‘¤on both the left and right sides of the actual
position of the shapelet. This means that one shapelet only calcu-
lates the distance with maximum 2ğ‘¤+1subsequences in ğ‘‹.
index =argminğ‘ğ‘ +ğ‘¤+1
ğ‘—=ğ‘ğ‘ âˆ’ğ‘¤ğ¶ğ¼ğ·(ğ‘‹[ğ‘—:ğ‘—+ğ‘™],ğ‘†ğ‘–), (4)
ğ¼ğ‘–=ğ‘‹[index :index+ğ‘™]. (5)
 
1487ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain
Shapelets 
 (
ğ‘†
)
 
of Class A
 Important Subsequences 
 (
ğ¼
)
 Shapelet
 s
 
(
ğ‘†
)
 
of Class B
-
ğ‘†1ğ¼1ğ‘†2ğ¼2ğ‘†3ğ¼3ğ‘†4ğ¼4
Transformer  Encoder
ğ‘ˆ1
High Attention
Score
Low Attention
Score
- - -ğ‘ˆ2 ğ‘ˆ3 ğ‘ˆ4
ğ‘£
=1
ğ‘£
=2
ğ‘£
=3
ğ‘£
=4
(a) Best
 -
fit Subsequence Finding 
(b) Difference Feature Calculation
Difference 
Feature
Low Attention
Score
ğ‘†1
ğ‘†2
ğ‘†3ğ¼1
ğ¼4ğ¼3
ğ¼2
ğ‘†4
Best
 -
fit subsequence
Shapelet Pool 
 (
ğ‘ƒ
)
 Input Time Series 
 (
ğ‘‹
)
 
of Class A 
Figure 5: The illustrations for: (a) best-fit subsequence finding
method; (b) difference feature calculation method.
Subsequently, we compute the difference features ğ‘ˆğ‘–âˆˆR1Ã—ğ‘‘ğ‘ ğ‘ğ‘’
between the embedding of each shapelet and their most fitting
subsequences derived from the input time series (see Figure 5b).
ğ‘ˆğ‘–=Pğ¼(ğ¼ğ‘–)âˆ’Pğ‘†(ğ‘†ğ‘–), (6)
wherePis the linear projector of Rğ‘™Ã—ğ‘‘ğ‘ ğ‘ğ‘’withğ‘™is length of shapelet
andğ‘‘ğ‘ ğ‘ğ‘’is the embedding size of difference features.
Similar to the distance between shapelet and time series, our dif-
ference feature also highlights the substantial distinctions among
classes. Furthermore, by directly incorporating the shapelets in
computing the difference features (Eq. 6), the shapelets are now
considered as the learnable parameters of the Shapelet Filter com-
ponent. Therefore, rather than using fixed shapelets, we can use
them as the initial parameters of the Shapelet Filter, which will be
optimised during training.
Position Embedding. The difference features ğ‘ˆğ‘–are then inte-
grated with position embeddings to capture their order. To better
indicate the position information of shapelets, the embeddings of
three types of positions are considered, including the start index,
end index, and variables. Specifically, we propose to use a one-hot
vector representation for these indices and then employ a linear
projector to learn their embedding.
PE(ğ‘)=Linear(one-hot(ğ‘)), (7)
ğ‘ˆğ‘–=ğ‘ˆğ‘–+PE(ğ‘ğ‘–
ğ‘ )+PE(ğ‘ğ‘–
ğ‘’)+PE(ğ‘£ğ‘–). (8)
We also observed that the performance is enhanced when we only
use the position of shapelets instead of the position of best-fit
subsequences. This improvement can be attributed to the fact that
the fixed position is easier to learn than the unstable position of
best-fit subsequences.Transformer Encoder. The class-specific difference features, along
with their corresponding position embeddings, are then input into
a transformer encoder to learn their correlation. Specifically, we
employ the multi-head attention mechanism (MHA) [ 40] for this
purpose. Given an input series, ğ‘¼=ğ‘ˆ1,...,ğ‘ˆğ‘”and the projections
ğ‘Šğ‘,ğ‘Šğ‘˜,ğ‘Šğ‘£âˆˆRğ‘‘ğ‘ ğ‘ğ‘’Ã—ğ‘‘ğ‘ ğ‘ğ‘’represent query, key, and value matrices,
respectively. These matrices, ğ‘Šğ‘,ğ‘Šğ‘˜,ğ‘Šğ‘£, undergo reshaping into
Râ„Ã—ğ‘‘ğ‘ ğ‘ğ‘’Ã—(ğ‘‘ğ‘ ğ‘ğ‘’/â„)to signify the â„attention heads and are subse-
quently concatenated into standard dimensions after computation.
Each attention head within this set is capable of capturing distinct
relationships of the features. Finally, these matrices are used to
compute an output Zspe=ğ‘spe
1,...,ğ‘spe
ğ‘”whereğ‘spe
ğ‘–âˆˆRğ‘‘ğ‘ ğ‘ğ‘’:
ğ‘spe
ğ‘–=ğ‘”âˆ‘ï¸
ğ‘—=1ğ‘ğ‘–,ğ‘—(ğ‘ˆğ‘—âˆ—ğ‘Šğ‘£), (9)
whereğ‘ğ‘–,ğ‘—is an attention score which is calculated as:
ğ‘ğ‘–,ğ‘—=softmax((ğ‘ˆğ‘–âˆ—ğ‘Šğ‘)(ğ‘ˆğ‘—âˆ—ğ‘Šğ‘˜)
âˆšï¸ğ‘‘ğ‘ ğ‘ğ‘’), (10)
Thanks to the class-represented characteristics of these features,
the attention score for features within the same class is boosted
compared to features in different classes. This enhancement helps
the model better distinguish between different classes. Additionally,
owing to the nature of shapelets, the difference features possess
the ability to identify significant subsequences across different tem-
poral locations and variables within the time series. This capability
enables the module to effectively capture temporal and variable
dependencies in time series data.
Class Token. Existing transformer-based methods apply averaging
pooling to Zspe, to obtain the final token for classification [ 10,26].
However, our class-specific transformer module utilises difference
features that capture the distinctive characteristics of each shapelet.
Applying average pooling may diminish these properties, poten-
tially limiting performance. To address this, we propose using only
the first difference feature of the highest information gain shapelet
ğ‘spe
1as the class token ğ‘spe
âˆ—for final classification. The reason for
this is that when averaging all tokens, there is a loss of informa-
tion regarding distinct features ğ‘ˆğ‘–. Moreover, the first token ğ‘spe
1,
which carries the highest information gain, harbors the most crucial
features for effectively classifying time series.
4.3 Generic Transformer
Besides leveraging the power of class-specific features, in this sec-
tion, we introduce the generic transformer module, utilising convo-
lution filters to extract generic features in the time series. Specifi-
cally, we employ two CNN components [ 10,11], each comprising
Conv1D, BatchNorm, and GELU, to effectively discover generic fea-
tures. The first block is designed to capture the temporal patterns
in time series by using the Conv1D filter âˆˆR1Ã—ğ‘‘ğ‘. On the other
hand, the second block uses the Conv1D filter âˆˆRğ‘‰Ã—1to capture
the correlation between variables in time series. In this context, ğ‘‰
represents the number of variables, and ğ‘‘ğ‘is the kernel size of the
convolution filter, which is fixed at 8 in all experiments. From that,
 
1488KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain Xuan-May Le, Ling Luo, Uwe Aickelin, & Minh-Tuan Tran
the output generic feature ğ‘‰ğ‘–âˆˆR1Ã—ğ‘‘ğ‘”ğ‘’ğ‘›is calculated as follows:
ConvBlock(X) =GELU(BatchNorm(Conv1D(ğ‘‹))), (11)
ğ‘½=ConvBlock(ConvBlock(ğ‘‹)). (12)
Afterward, these features will be fed to the â„multi-attention heads
to learn the correlation. Each attention head has the capacity to
capture distinct patterns within time series data.
ğ’gen=MHA(ğ‘½+ğ‘·), (13)
as the position of each element ğ‘‰ğ‘–âˆˆğ‘½lacks inherent meaning, we
utilise the learnable position embedding ğ‘·for representing them.
Furthermore, since the module takes classic features as input tokens,
we employ averaging pooling to derive the final class token.
ğ’gen
âˆ—=AvgPooling(ğ’gen). (14)
4.4 Overall Architecture of ShapeFormer
To enhance clarity, we present the overall architecture of Shape-
Former in Figure 3. Our method initiates by extracting shapelets
from the training datasets. Subsequently, for a given input time
series ğ‘¿, it is processed through dual transformer modules, com-
prising the class-specific shapelet transformer and the generic con-
volution transformer. The outputs from these two modules are then
concatenated and fed into the final classification head.
ğ’=concat(ğ’spe
âˆ—,ğ’gen
âˆ—), (15)
Ë†ğ‘¦=argmax(softmax(Linear(ğ’))). (16)
The predictions Ë†ğ‘¦are used to optimise the model parameters based
on the following objective function:
L=LCE(Ë†ğ‘¦,ğ‘Œ). (17)
where, CE is the Cross-Entropy Loss, which can be calculated as
LCE(Ë†ğ‘¦,ğ‘Œ)=Ã|ğ‘Œ|
ğ‘–ğ‘¦ğ‘–ğ‘™ğ‘œğ‘”(Ë†ğ‘¦ğ‘–).
5 EXPERIMENTS
5.1 Experimental Setting
Dataset. We assess our approach using the UEA archive, a well-
known benchmark made up of 30 distinct datasets for MTSC [ 1].
It covers various domains, including Human Activity Recognition,
Motion classification, ECG classification, EEG/MEG classification,
Audio Spectra classification, and more. The sample sizes of datasets
in the UEA archive range from 27 to 50,000, the time series lengths
spanning 8 to 17,984, and dimensions varying from 2 to 1,345.
Metrics. We use classification accuracy to evaluate model perfor-
mance and compare methods based on their average ranks and
win/draw/loss counts on all datasets. Finally, we evaluate the sta-
tistical significance of performance differences using the p-value of
Friedman and Wilcoxon signed-rank test [1].
Implementation Details. Our model was trained using the RAdam
optimiser with an initial learning rate set as 0.01, a momentum of
0.9, and a weight decay of 5e-4. The training process involved a
batch size of 16 for a total of 200 epochs. We configured the num-
ber of attention heads to be 16 and followed the protocol outlined
in [47,50]. This protocol involves splitting the training set into
80% for training and 20% for validation, allowing us to fine-tunehyperparameters. Once the hyperparameters were finalised, we con-
ducted model training on the entire training set and subsequently
evaluated its performance on the designated official test set.
Environment. All the experiments are conducted on a machine
with one Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz and one
NVIDIA Tesla V100 SXM2.
5.2 Baselines
We have selected 12 baseline methods for the comparative exper-
iments, comprising two distance-based methods: EDI,ğ·ğ‘‡ğ‘Šğ·[1];
a pattern-based algorithm: WEASEL+MUSE [33]; a feature-based
algorithm: MiniRocket [5]; an ensemble method: LCEM [9]; three
deep learning models: MLSTM-FCNs [18],Tapnet [48],Shapenet
[23]; an attention-based model: WHEN [41]; and three transformer-
based models: TST[47],ConvTran [10],SVP-T [50]. They all attained
the SOTA performance described in the most recent research. The
details of 12 baseline methods are shown in Appendix A.
5.3 Performance Evaluation
Table 1 illustrates the experimental results of our method with 12
other competitors on the UEA multivariate time series classification
archive [ 1]. The accuracy of 12 baseline methods are taken from
[50], except the results of WHEN, and ConvTran are taken from
their original papers [ 10,41]. The best result on each dataset is
indicated in bold, and the summarised information is provided in
the last six lines of the table.
The results show that among all methods, ShapeFormer achieves
the best performance in both the highest average rank (2.5) and
the largest number of top-1 (the best in 15 out of 30 datasets). This
indicates that ShapeFormer can be taken as a SOTA for MTSC. The
rank index signifies that, even on some datasets where our model
does not exhibit the highest performance, its results remain highly
competitive. Specifically, the average rank of our method is slightly
higher compared to that of the runner-up, WHEN, a difference of
0.617. Meanwhile, the gap in average rank between ShaperFormer
and three Transformer-based methods (TST, ConvTran, SVP-T) is
large, with 5.617, 3.15, and 2.783 respectively. The p-value is â‰¤0.05,
which confirms there ranks have statistically significant differences.
Specifically, the p-values for ShapeFormer in comparison to all
methods are below 0.05, which indicates the results are statisti-
cally significant except for WHEN. However, regarding the number
of top-1, our ShapeFormer attained SOTA results in 15 datasets
compared to WHEN, only 4 datasets.
5.4 Ablation Study and Model Design
Effectiveness of Using Shapelets. In Figure 6, we compare the
performance when using random subsequences, common subse-
quences as mentioned in [ 50], and shapelets in our methods. The
results demonstrate that the shapelets outperform the other two
methods in terms of accuracy across all five datasets. This highlights
the benefit of highly discriminative shapelet features in increasing
the performance of the transformer-based model, thereby indicating
the contribution of our work.
Component Evaluation. We begin by evaluating the impact of
two key modules in our ShapeFormer: the Class-specific Trans-
former (Section 4.2) and the Generic Transformer (Section 4.3),
 
1489ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain
Table 1: Accuracies of our proposed method ShapeFormer and 12 compared methods on all datasets of the UEA archive [1].
EDI
DTWDWEASEL
+MUSEMiniRocket LCEMMLSTM
-FCNsTapnet Shapenet WHEN TST ConvTran SVPT Our
Articular
yWordRecognition 0.970
0.987 0.990 0.992 0.993 0.973 0.987 0.987 0.993 0.983 0.983 0.993 0.993
AtrialFibrillation 0.267
0.220 0.333 0.133 0.467 0.267 0.333 0.400 0.467 0.200 0.400 0.400 0.660
BasicMotions 0.676
0.975 1.000 1.000 1.000 0.950 1.000 1.000 1.000 0.975 1.000 1.000 1.000
CharacterTrajectories 0.964
0.989 0.990 0.993 0.979 0.985 0.997 0.980 0.996 0.000 0.992 0.990 0.996
Cricket 0.944 1.000
1.000 0.986 0.986 0.917 0.958 0.986 1.000 0.958 1.000 1.000 1.000
DuckDuckGeese 0.275
0.600 0.575 0.650 0.375 0.675 0.575 0.725 0.700 0.480 0.620 0.700 0.725
ERing 0.133
0.929 0.133 0.981 0.200 0.133 0.133 0.133 0.959 0.933 0.963 0.937 0.966
EigenWorms 0.549
0.618 0.890 0.962 0.527 0.504 0.489 0.878 0.893 N/A 0.593 0.925 0.925
Epilepsy 0.666
0.964 1.000 1.000 0.986 0.761 0.971 0.987 0.993 0.920 0.986 0.986 0.993
EthanolConcentration 0.293
0.323 0.430 0.468 0.372 0.373 0.323 0.312 0.422 0.337 0.361 0.331 0.378
FaceDetection 0.519
0.529 0.545 0.620 0.614 0.545 0.556 0.602 0.658 0.681 0.672 0.512 0.658
FingerMovements 0.550
0.530 0.490 0.550 0.590 0.580 0.530 0.580 0.660 0.776 0.560 0.600 0.700
HandMovementDirection 0.278
0.231 0.365 0.392 0.649 0.365 0.378 0.338 0.554 0.608 0.405 0.392 0.486
Handwriting 0.200
0.286 0.605 0.507 0.287 0.286 0.357 0.452 0.561 0.305 0.375 0.433 0.507
Heartbeat 0.619
0.717 0.727 0.771 0.761 0.663 0.751 0.756 0.780 0.712 0.785 0.790 0.800
InsectWingbeat 0.128
N/A N/A 0.595 0.228 0.167 0.208 0.250 0.657 0.684 0.713 0.184 0.314
JapaneseVowels 0.924
0.949 0.973 0.989 0.978 0.976 0.965 0.984 0.995 0.994 0.989 0.978 0.997
LSST 0.456
0.551 0.590 0.643 0.652 0.373 0.568 0.590 0.663 0.381 0.616 0.666 0.700
Libras 0.833
0.870 0.878 0.922 0.772 0.856 0.850 0.856 0.933 0.844 0.928 0.883 0.961
MotorImagery 0.510
0.500 0.500 0.550 0.600 0.510 0.590 0.610 0.630 N/A 0.560 0.650 0.670
NATOPS 0.850
0.883 0.870 0.928 0.916 0.889 0.939 0.883 0.978 0.900 0.944 0.906 0.989
PEMS-SF 0.973 0.711
N/A 0.522 0.942 0.699 0.751 0.751 0.925 0.919 0.828 0.867 0.925
PenDigits 0.705
0.977 0.948 N/A 0.977 0.978 0.980 0.977 0.987 0.974 0.987 0.983 0.990
PhonemeSpectra 0.104
0.151 0.190 0.292 0.288 0.110 0.175 0.298 0.293 0.088 0.306 0.176 0.293
RacketSports 0.868
0.803 0.934 0.868 0.941 0.803 0.868 0.882 0.934 0.829 0.862 0.842 0.895
SelfRegulationSCP1 0.771
0.775 0.710 0.925 0.839 0.874 0.652 0.782 0.908 0.925 0.918 0.884 0.911
SelfRegulationSCP2 0.483
0.539 0.460 0.522 0.550 0.472 0.550 0.578 0.589 0.589 0.583 0.600 0.633
SpokenArabicDigits 0.967
0.963 0.982 0.620 0.973 0.990 0.983 0.975 0.997 0.993 N/A 0.986 0.997
StandWalkJump 0.200
0.200 0.333 0.333 0.400 0.067 0.400 0.533 0.533 0.267 0.333 0.467 0.600
UWaveGestureLibrary 0.881
0.903 0.916 0.938 0.897 0.891 0.894 0.906 0.919 0.903 0.891 0.941 0.922
A
verage rank 11.200
9.783 7.933 5.900 6.600 9.833 8.233 6.850 3.117 8.117 5.650 5.283 2.500
Number of top-1 1
1 4 6 4 0 2 2 4 3 4 5 15
Wins 29
29 24 21 25 30 28 27 16 25 24 24 -
Draws 0
1 2 2 2 0 1 2 9 0 2 5 -
Loses 1
0 4 7 3 0 1 1 5 5 4 1 -
P-value 0.000
0.000 0.001 0.014 0.003 0.000 0.000 0.002 0.475 0.005 0.024 0.000 -
00.20.40.60.81
Atrial. Duck. Ering Ethan. FaceD.
Common Subsequences Random Subsequences Shapelets
Figure 6: Accuracies of using shapelets and two other types of subse-
quences.
in comparison with the baseline method, SVP-T [ 50] on the first
10 datasets of UEA archive [ 1]. In this process, individual compo-
nents are incrementally incorporated to assess their impact on the
ultimate accuracy. As depicted in Figure 7, applying the generic
transformer alone exhibits a lower accuracy compared to the base-
line. In contrast, utilising only the class-specific module results in
significantly improved performance over the baselines, emphasising
the effectiveness of class-specific features in the transformer-based
time series model. Furthermore, the combination of class-specific
/uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni0000004c/uni00000046
/uni00000045/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048 /uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000010/uni00000056/uni00000053/uni00000048/uni00000046/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000010/uni00000056/uni00000053/uni00000048/uni00000046/uni0000004c/uni00000049/uni0000004c/uni00000046/uni0000000e/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni0000004c/uni00000046Figure 7: Average ranks for 3 variations of ShapeFormer and the
baseline (SVP-T [ 50] - the current SOTA transformer-based method).
and generic components shows a positive impact on the enhance-
ment of classification accuracy. This combination harnesses the
power of both features, significantly boosting overall performance.
Choosing between the Position of Shapelets and Best-fit Sub-
sequences. Our ShapeFormer leverages shapelets to find the best-
fit subsequences and employ the difference features ğ‘ˆğ‘–calculated
by them as the inputs for the Transformer encoder. Then there is
a question "Should we choose the positions of the shapelets or the
best-fit subsequences for position embedding?". Figure 8 illustrates
a comparison between the accuracies achieved by employing the
position of the best-fit subsequences and shapelets, as indicated
in Eq. 8, for position embedding in the Transformer encoder. The
outcomes indicate that our approach exhibits superior performance
when utilising the position of shapelets across all five datasets un-
der consideration. This enhancement can be ascribed to the fact
 
1490KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain Xuan-May Le, Ling Luo, Uwe Aickelin, & Minh-Tuan Tran
00.20.40.60.81
Atrial. Duck. Ering Ethan. FaceD.
Position of Best-fit Subsequences Position of Shapelets
Figure 8: Accuracies of using the position of best-fit subsequences
and shapelets.
/uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni00000045/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000048/uni00000058/uni00000046/uni0000004f/uni0000004c/uni00000047/uni00000048/uni00000044/uni00000051/uni00000042/uni00000047/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048 /uni00000050/uni00000044/uni00000051/uni0000004b/uni00000044/uni00000057/uni00000057/uni00000044/uni00000051/uni00000042/uni00000047/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048/uni00000056/uni00000058/uni00000045/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
Figure 9: Average accuracy ranks of various calculation methods for
difference features.
/uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni00000045/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000044/uni00000059/uni0000004a/uni00000053/uni00000052/uni00000052/uni0000004f /uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000049/uni0000004c/uni00000055/uni00000056/uni00000057
Figure 10: Average accuracy ranks of different class token designs.
that learning from the fixed position of shapelets is easier compared
to the unstable position of the best-fit subsequences.
Comparison with Various Methods for Calculating Differ-
ence Features. The critical difference diagram in Figure 9 displays
the performance of using different methods for calculating differ-
ence features in Eq. 6, including Manhattan Distance, Euclidean
Distance, and the subtraction between Pğ¼(ğ¼ğ‘–)andPğ‘†(ğ‘†ğ‘–). The re-
sults demonstrate that: 1) All calculation methods for difference
features yield better results compared to the SVP-T baseline; 2)
Using subtraction exhibits the highest performance. Although the
subtraction is simple, its superiority lies in effectively capturing
relative changes by considering both the magnitude and direction
of changes between embedding vectors Pğ¼(ğ¼ğ‘–)andPğ‘†(ğ‘†ğ‘–).
Different Class Token Designs. The output of the class-specific
transformer consists of a series of tokens ğ‘spe
1,...,ğ‘spe
ğ‘”. The ques-
tion at hand is, "Are there any effective ways to design class tokens
before feeding them to the classification head?". In Figure 10, we anal-
yse the impact of different class token designs on the performance
of ShapeFormer. The results indicate that: 1) Our ShapeFormer out-
performs the baseline with all types of class token designs, demon-
strating the advantage of our method; 2) Utilising the first token
ğ‘spe
1as the final class token ğ‘spe
âˆ—for ShapeFormer yields the best
performance. This is due to the fact that learning or averaging all
tokens results in a loss of information on difference features ğ‘ˆğ‘–.
Furthermore, the first token, containing the highest information
gain, possesses the most discriminative features for classifying time
series.
5.5 Hyperparameter Sensitivity
Tuning Window Size and Number of Shapelets. In our method,
there are two main parameters related to shapelet discovery that
need tuning, including the window size when calculating the dis-
tances between shapelets and subsequences and the number ofTable 2: The average accuracy for various numbers of PIPs.
Npips
(Ã—ğ‘‡)0.05 0.1 0.15 0.2 0.25 0.3 0.4 0.5
A
ccuracy 0.832 0.848 0.856 0.864 0.864 0.864 0.864 0.864
32 64 128 256
32 0.845 0.858 0.864 0.86
64 0.838 0.855 0.861 0.852
128 0.834 0.842 0.859 0.858
256 0.829 0.836 0.844 0.855
(a) Different scale factors ğ’…ğ’ˆğ’ğ’ğ’ƒğ’‚ğ’  and ğ’…ğ’ğ’ğ’„ğ’‚ğ’0.8400.8450.8500.8550.860
0 0.2 0.4 0.6 0.8Accuracy
Dropout Ratios
(b) Different dropout ratiosğ‘‘ğ‘ ğ‘ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘›
Figure 11: Effectiveness of (a) class-specific and generic scale factors
and (b) different dropout ratios.
shapelets. Regarding the window size, we propose to tune it exclu-
sively during the shapelet discovery phase. For each dataset, we
will select a window size from the set [10, 20, 50, 100, 200], aim-
ing to provide the top 100 shapelets with the highest information
gain. This tuning technique can significantly reduce training time
since it only operates during the shapelet discovery phase. As for
the number of shapelets, considering the diverse characteristics of
different datasets, we choose this number from [1, 3, 10, 30, 100].
The details of our tuned parameters are shown in Appendix B.
Number of PIPs. As shown in the following Table 2, the model
accuracy increases as we increase the number of PIPs (npips) from
0.05T to 0.2T. Afterward, accuracy remains stable even with further
increases in npips. Therefore, we set npips at 0.2 for all of our
experiments.
The Scale Factors ğ‘‘speandğ‘‘gen.In Figure 11a, we compare the
impact of different scale factors of the class-specific and generic
embedding sizes on the classification accuracy of ShapeFormer.
The results show that: 1) The pair of ğ‘‘spe=128andğ‘‘gen=32has
achieved the highest accuracy; 2) In general, a larger class-specific
embedding size has achieved better performance, indicating the
benefit of using shapelets in a transformer-based time series model.
Dropout Ratios. In Figure 11b, we analyse the impact of different
dropout ratios of ShapeFormer. It is evident that our methods work
well and achieve high performance with small dropout ratios, with
the ratio of 0.4 yielding the highest performance.
5.6 Improving Performance in Specific Datasets
by Optimizing Scale Factor
In MTSC, it is crucial to develop models that generalise well across a
majority of datasets rather than models tailored to specific datasets.
For example, in terms of InsectWingbeat dataset, we observed that
settingğ‘‘gen(embedding size of generic feature) to 256 leads to
significantly better performance (0.704) compared to ğ‘‘genat 32
(0.314) (our chosen parameter). However, this improvement comes
at the cost of decreased performance on other datasets (from 0.864
to 0.831). Therefore, we recommend tuning this hyperparameter to
achieve better performance on specific datasets if needed.
Table 3: Accuracy for InsectWingbeat dataset and the first 10 UEA
datasets with various ğ‘‘genfactors.
ğ‘‘gen 32
(our choice) 64 128 256
Inse
ctWingbeat 0.314 0.500 0.634 0.704
First
10 UEA datasets 0.864 0.852 0.844 0.831
 
1491ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain
(a) Generic Transformer (b) Class -specific and Generic Transformer
Figure 12: The t-SNE visualisation in 4 classes of LSST dataset using
(a) our generic transformer and (b) both class-specific and generic
transformers. Each point indicates an instance and the colors of the
points signify the true labels.
5.7 A Case Study of LSST (Imbalanced Datasets)
To illustrate the effectiveness of combining both class-specific and
generic features transformer modules to classify imbalanced data,
we conducted experiments on the LSST dataset. The LSST dataset
comprises 16 classes, and we randomly selected 4 classes to be
represented by the colors blue, orange, green, and red, with 35, 270,
382, and 63 instances, respectively. It is clear that the sizes of blue
and red classes are significantly smaller compared to the sizes of
the green and orange classes. Figure 12a shows that the generic
transformer prioritises majority classes (green and orange), but
neglects minority ones (blue and red). However, in Figure 12b, the
combination of class-specific and generic transformers effectively
distinguishes all four classes.
5.8 A Case Study of BasicMotions
To interpret ShapeFormer results, we use the BasicMotions dataset
from the UEA archive [ 1], focusing on human activity recognition
with 4 classes (playing badminton, standing, walking, and running).
Each class is associated with 6 variables, and 10 shapelets are set for
analysis. Randomly selecting a â€™walkingâ€™ instance from the training
set. Figure 13a showcases the top three shapelets for this class and
three from others, highlighting ShapeFormerâ€™s ability to identify
crucial subsequences across diverse locations and variables in the
time series. Moreover, shapelets within the same â€™walkingâ€™ class
tend to share greater similarity with best-fit subsequences than
those from other classes.
In Figure 13b, the attention heat map for all 40 shapelets across 4
classes reveals that shapelets within the same class generally attain
higher attention scores. For instance, ğ‘†20andğ‘†23belonging to the
â€™walkingâ€™ class show a small difference feature (Eq. 6), resulting in
higher attention scores. This enhanced attention allows the model
to focus more on the correlation between shapelets within the same
classes, thereby improving overall performance.
6 CONCLUSION
In this paper, we propose a novel Shapelet Transformer (Shape-
Former) for multivariate time series classification. It consists of
dual transformer modules aimed at identifying class-specific and
generic features within time series data. In particular, the first
module discovers class-specific features by utilising discriminative
subsequences (shapelets) extracted from the entire dataset. Mean-
while, the second transformer module employs convolution filters
Best
 -
fit Subsequences
 Shapelets
(a) Shapelets  and Their Best -fit Subsequences (b) Attention Heat Map
ID: 20
ID: 23
ID: 25ID: 12 ID: 37ID: 1Figure 13: (a) The green box depicts the top three shapelets, and
the orange box displays three random shapelets from other classes,
extracted in one random input time series of the â€™walkingâ€™ class in the
BasicMotions dataset. (b) The attention heat map for all shapelets.
to extract generic features across all classes. The experimental re-
sults show that by combining both modules, our ShapeFormer has
achieved the highest rank in terms of classification accuracy when
compared to the SOTA methods. In future work, we intend to utilise
the power of shapelets in many different time series analysis tasks
such as forecasting or anomaly detection.
REFERENCES
[1]Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large,
Aaron Bostrom, Paul Southam, and Eamonn Keogh. 2018. The UEA multivariate
time series classification archive, 2018. arXiv preprint arXiv:1811.00075 (2018).
[2]Gustavo EAPA Batista, Xiaoyue Wang, and Eamonn J Keogh. 2011. A complexity-
invariant distance measure for time series. In Proceedings of the 2011 SIAM inter-
national conference on data mining. SIAM, 699â€“710.
[3]Donald J Berndt and James Clifford. 1994. Using dynamic time warping to
find patterns in time series. In Proceedings of the 3rd international conference on
knowledge discovery and data mining. 359â€“370.
[4]Fu Lai Korris Chung, Tak-Chung Fu, Wing Pong Robert Luk, and Vincent To Yee
Ng. 2001. Flexible time series pattern matching based on perceptually important
points. In Workshop on Learning from Temporal and Spatial Data in International
Joint Conference on Artificial Intelligence.
[5]Angus Dempster, Daniel F Schmidt, and Geoffrey I Webb. 2021. Minirocket:
A very fast (almost) deterministic transform for time series classification. In
Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data
mining. 248â€“257.
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[7]Thuc-Doan Do, Tuan Minh Tran, Xuan-May Thi Le, and Thuy-Van T Duong. 2017.
Detecting special lecturers using information theory-based outlier detection
method. In Proceedings of the International Conference on Compute and Data
Analysis. 240â€“244.
[8]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[9]Kevin Fauvel, Ã‰lisa Fromont, VÃ©ronique Masson, Philippe Faverdin, and Alexan-
dre Termier. 2020. Local cascade ensemble for multivariate data classification.
arXiv preprint arXiv:2005.03645 (2020).
[10] Navid Mohammadi Foumani, Chang Wei Tan, Geoffrey I Webb, and Mahsa Salehi.
2023. Improving Position Encoding of Transformers for Multivariate Time Series
Classification. arXiv preprint arXiv:2305.16642 (2023).
[11] Seyed Navid Mohammadi Foumani, Chang Wei Tan, and Mahsa Salehi. 2021.
Disjoint-cnn for multivariate time series classification. In 2021 International
Conference on Data Mining Workshops (ICDMW). IEEE, 760â€“769.
[12] Ge Gao, Qitong Gao, Xi Yang, Miroslav Pajic, and Min Chi. 2022. A reinforcement
learning-informed pattern mining framework for multivariate time series classi-
fication. In In the Proceeding of 31th International Joint Conference on Artificial
Intelligence (IJCAI-22).
[13] Josif Grabocka, Nicolas Schilling, Martin Wistuba, and Lars Schmidt-Thieme.
2014. Learning time-series shapelets. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining. 392â€“401.
[14] Josif Grabocka, Martin Wistuba, and Lars Schmidt-Thieme. 2016. Fast classi-
fication of univariate and multivariate time series through shapelet discovery.
Knowledge and information systems 49 (2016), 429â€“454.
 
1492KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain Xuan-May Le, Ling Luo, Uwe Aickelin, & Minh-Tuan Tran
[15] Yifan Hao and Huiping Cao. 2020. A new attention mechanism to classify
multivariate time series. In Proceedings of the Twenty-Ninth International Joint
Conference on Artificial Intelligence.
[16] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,
and Pierre-Alain Muller. 2019. Deep learning for time series classification: a
review. Data mining and knowledge discovery 33, 4 (2019), 917â€“963.
[17] Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. 2019.
Multivariate LSTM-FCNs for time series classification. Neural networks 116 (2019),
237â€“245.
[18] Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. 2019.
Multivariate LSTM-FCNs for time series classification. Neural networks 116 (2019),
237â€“245.
[19] Eamonn Keogh and Chotirat Ann Ratanamahatana. 2005. Exact indexing of
dynamic time warping. Knowledge and information systems 7 (2005), 358â€“386.
[20] Sang-Wook Kim, Dae-Hyun Park, and Heon-Gil Lee. 2004. Efficient processing
of subsequence matching with the Euclidean metric in time-series databases.
Information processing letters 90, 5 (2004), 253â€“260.
[21] Xuan-May Le, Minh-Tuan Tran, and Van-Nam Huynh. 2022. Learning Percep-
tual Position-Aware Shapelets for Time Series Classification. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases . Springer,
53â€“69.
[22] Xuan-May Thi Le, Tuan Minh Tran, and Hien T Nguyen. 2020. An improvement
of SAX representation for time series by using complexity invariance. Intelligent
Data Analysis 24, 3 (2020), 625â€“641.
[23] Guozhong Li, Byron Choi, Jianliang Xu, Sourav S Bhowmick, Kwok-Pan Chun,
and Grace Lai-Hung Wong. 2021. Shapenet: A shapelet-neural network approach
for multivariate time series classification. In Proceedings of the AAAI conference
on artificial intelligence, Vol. 35. 8375â€“8383.
[24] Jessica Lin, Rohan Khade, and Yuan Li. 2012. Rotation-invariant similarity in time
series using bag-of-patterns representation. Journal of Intelligent Information
Systems 39 (2012), 287â€“315.
[25] Jason Lines, Luke M Davis, Jon Hills, and Anthony Bagnall. 2012. A shapelet
transform for time series classification. In Proceedings of the 18th ACM SIGKDD
international conference on Knowledge discovery and data mining. 289â€“297.
[26] Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui Jiao, Yizhou Chen, Zhiguang Wang,
and Wei Song. 2021. Gated transformer networks for multivariate time series
classification. arXiv preprint arXiv:2103.14438 (2021).
[27] Amy McGovern, Derek H Rosendahl, Rodger A Brown, and Kelvin K Droegemeier.
2011. Identifying predictive multi-dimensional time series motifs: an application
to severe weather prediction. Data Mining and Knowledge Discovery 22 (2011),
232â€“258.
[28] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.
A time series is worth 64 words: Long-term forecasting with transformers. arXiv
preprint arXiv:2211.14730 (2022).
[29] Andrew J Patton. 2012. A review of copula models for economic time series.
Journal of Multivariate Analysis 110 (2012), 4â€“18.
[30] Alejandro Pasos Ruiz, Michael Flynn, James Large, Matthew Middlehurst, and
Anthony Bagnall. 2021. The great multivariate time series classification bake
off: a review and experimental evaluation of recent algorithmic advances. Data
Mining and Knowledge Discovery 35, 2 (2021), 401â€“449.
[31] Alejandro Pasos Ruiz, Michael Flynn, James Large, Matthew Middlehurst, and
Anthony Bagnall. 2021. The great multivariate time series classification bake
off: a review and experimental evaluation of recent algorithmic advances. Data
Mining and Knowledge Discovery 35, 2 (2021), 401â€“449.
[32] Patrick SchÃ¤fer. 2015. The BOSS is concerned with time series classification in the
presence of noise. Data Mining and Knowledge Discovery 29 (2015), 1505â€“1530.
[33] Patrick SchÃ¤fer and Ulf Leser. 2017. Multivariate time series classification with
WEASEL+ MUSE. arXiv preprint arXiv:1711.11343 (2017).
[34] ABA Stevner, Diego Vidaurre, Joana Cabral, K Rapuano, SÃ¸renFÃ¸nsVind Nielsen,
Enzo Tagliazucchi, Helmut Laufs, Peter Vuust, Gustavo Deco, Mark W Woolrich,
et al.2019. Discovery of key whole-brain transitions and dynamics during human
wakefulness and non-REM sleep. Nature communications 10, 1 (2019), 1035.
[35] Chang Wei Tan, Angus Dempster, Christoph Bergmeir, and Geoffrey I Webb.
2022. MultiRocket: multiple pooling operators and transformations for fast and
effective time series classification. Data Mining and Knowledge Discovery 36, 5
(2022), 1623â€“1646.
[36] Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, and Dinh Phung.
2024. Text-Enhanced Data-free Approach for Federated Class-Incremental Learn-
ing. arXiv preprint arXiv:2403.14101 (2024).
[37] Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Quan Hung Tran,
and Dinh Phung. 2023. NAYER: Noisy Layer Data Generation for Efficient
and Effective Data-free Knowledge Distillation. arXiv preprint arXiv:2310.00258
(2023).
[38] Tuan Minh Tran, Xuan-May Thi Le, Hien T Nguyen, and Van-Nam Huynh. 2019.
A novel non-parametric method for time series classification based on k-Nearest
Neighbors and Dynamic Time Warping Barycenter Averaging. Engineering
Applications of Artificial Intelligence 78 (2019), 173â€“185.[39] Tuan Minh Tran, Xuan-May Thi Le, Vo Thanh Vinh, Hien T Nguyen, and Tuan M
Nguyen. 2017. A weighted local mean-based k-nearest neighbors classifier for
time series. In Proceedings of the 9th International Conference on Machine Learning
and Computing. 157â€“161.
[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[41] Jingyuan Wang, Chen Yang, Xiaohan Jiang, and Junjie Wu. 2023. WHEN: A
Wavelet-DTW Hybrid Attention Network for Heterogeneous Time Series Analy-
sis. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining. 2361â€“2373.
[42] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2022. Timesnet: Temporal 2d-variation modeling for general time series
analysis. In The eleventh international conference on learning representations.
[43] Carl Yang and Jiawei Han. 2023. Revisiting citation prediction with cluster-
aware text-enhanced heterogeneous graph neural networks. In 2023 IEEE 39th
International Conference on Data Engineering (ICDE). IEEE, 682â€“695.
[44] Lexiang Ye and Eamonn Keogh. 2009. Time series shapelets: a new primitive for
data mining. In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. 947â€“956.
[45] Wenhui Yu, Xiao Lin, Junfeng Ge, Wenwu Ou, and Zheng Qin. 2020. Semi-
supervised collaborative filtering by text-enhanced domain adaptation. In Pro-
ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
& Data Mining. 2136â€“2144.
[46] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,
Yunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation of
time series. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.
8980â€“8987.
[47] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,
and Carsten Eickhoff. 2021. A transformer-based framework for multivariate time
series representation learning. In Proceedings of the 27th ACM SIGKDD conference
on knowledge discovery & data mining. 2114â€“2124.
[48] Xuchao Zhang, Yifeng Gao, Jessica Lin, and Chang-Tien Lu. 2020. Tapnet: Multi-
variate time series classification with attentional prototypical network. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 6845â€“6852.
[49] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al .2023. One fits all: Power
general time series analysis by pretrained lm. Advances in neural information
processing systems 36 (2023), 43322â€“43355.
[50] Rundong Zuo, Guozhong Li, Byron Choi, Sourav S Bhowmick, Daphne Ngar-
yin Mah, and Grace LH Wong. 2023. SVP-T: a shape-level variable-position
transformer for multivariate time series classification. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 11497â€“11505.
A BASELINES
12 baseline methods are utilised in the comparative experiments,
comprising two distance-based methods, a pattern-based algorithm,
a feature-based algorithm, an ensemble method, three deep learning
models, an attention-based model, and three transformer-based
models. They all attained the SOTA performance described in the
most recent research.
â€¢EDIandğ·ğ‘‡ğ‘Šğ·[1]: The two benchmark classifiers based on Eu-
clidean Distance and dimension-dependent dynamic time warp-
ing.
â€¢WEASEL+MUSE [33]: A classifier based on a bag-of-pattern ap-
proach demonstrated SOTA performance when compared with
similar competitors for MTSC. We choose this algorithm as the
representative baseline among pattern-based methods.
â€¢MiniRocket [5]: A feature-based method utilizes random convo-
lutional kernels to discover features. It performs well in both
univariate and multivariate time series classification.
â€¢LCEM [9]: A hybrid ensemble method that integrates boost-
ing, bagging, divide-and-conquer, and decision tree components.
LCEM demonstrated superior performance when compared to
other random forest methods. We choose it as a representative
illustration of ensemble methods.
â€¢MLSTM-FCNs [18]: A deep learning method for MTSC which
utilizes LSTM layer and stacked CNN layers to discover features.
 
1493ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification KDD 2024, Aug 25 - 29, 2024, Barcelona, Spain
Table 4: The selected window size and number of shapelets for each
UEA MTSC dataset.
Dataset
Window size Number of shapelets (per class)
Articular
yWordRecognition 50 10
AtrialFibrillation 100 3
BasicMotions 100 10
CharacterTrajectories 50 3
Cricket 200 30
DuckDuckGeese 10 100
ERing 50 100
EigenWorms 10 10
Epilepsy 20 30
EthanolConcentration 200 100
FaceDetection 10 10
FingerMovements 20 30
HandMovementDirection 200 100
Handwriting 20 30
Heartbeat 200 100
InsectWingbeat 10 30
JapaneseVowels 10 1
LSST 20 10
Libras 10 30
MotorImagery 100 30
NATOPS 20 1
PEMS-SF 50 10
PenDigits 4 10
PhonemeSpectra 20 30
RacketSports 10 10
SelfRegulationSCP1 100 100
SelfRegulationSCP2 100 100
SpokenArabicDigits 10 100
StandWalkJump 10 100
UWaveGestureLibrary 10 10
â€¢Tapnet [48]: A classifier constructs an attentional prototype net-
work. Tapnet incorporates LSTM, and CNN to learn multi dimen-
sional interaction features. We opt for it as another representative
of the deep learning method.
â€¢Shapenet [23]: Shapenet aims to learn representations of different
shapelet candidates in a unified space and selects final shapelets
by training a dilated causal CNN module followed by standard
classification. This model can capture dependencies among vari-
ables. We choose it as a representative of the shapelet-based
method.â€¢WHEN [41]: An attention-based method that learns heterogene-
ity by utilising a hybrid attention network, incorporating both
DTW attention and wavelet attention. It achieved SOTA perfor-
mance for MTSC on the UEA datasets.
â€¢TST [47]: A transformer-based framework for MTS representa-
tion learning. TST is considered as baseline method that takes the
values at each timestamp as the input for the Transformer model.
It gains great performance for many sequential tasks, such as
regression, and classification.
â€¢ConvTran [10]: A transformer-based method for MTSC that pro-
posed to improve the position embedding in the Transformer
model by leveraging both absolute and relative position encoding.
â€¢SVP-T [50]: A method uses clustering to identify time series sub-
sequences and employs them as inputs for the Transformer, en-
abling the capture of long- and short-term dependencies among
subseries. It achieved SOTA performance for MTSC. We choose
it as another representative of the transformer-based method.
B SELECTED HYPERPARAMETERS
In this section, we follow the setting mentioned in Section 5.5 to
tune the hyperparameter of window size and number of shapelets
per class. In Table 4, we provide the selected window size and
number of shapelets for each class on 30 UEA MTSC datasets.
C COMPARISON WITH TIME SERIES
REPRESENTATION METHODS
We conducted an experiment to compare the average accuracy of
our ShapeFormer against SOTA representation methods, specifi-
cally TST [ 47], PatchTST [ 28], TimesNet [ 42], and GPT2 [ 49] (as
shown in Table 5). By adhering to the GPT2 protocol across 10
datasets, our method outperforms the others on all datasets, achiev-
ing an average accuracy of 0.773.
Table 5: Comparison between our proposed ShapeFormer and SOTA
time series representation methods.
TST GPT2 TimesNet PatchTST Ours
A
veraging Accuracy 0.736 0.740 0.736 0.679 0.773
 
1494