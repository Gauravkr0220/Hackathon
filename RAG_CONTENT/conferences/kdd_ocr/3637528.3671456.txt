Graph Intelligence with Large Language Models and Prompt
Learning
Jia Li
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, China
jialee@ust.hkXiangguo Sun
The Chinese University of Hong Kong
Hong Kong SAR, China
xiangguosun@cuhk.edu.hkYuhan Li
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, China
yuhanli98@gmail.com
Zhixun Li
The Chinese University of Hong Kong
Hong Kong SAR, China
zxli@se.cuhk.edu.hkHong Cheng
Chinese University of Hong Kong
Hong Kong SAR, China
hcheng@se.cuhk.edu.hkJeffrey Xu Yu
Chinese University of Hong Kong
Hong Kong SAR, China
yu@se.cuhk.edu.hk
ABSTRACT
Graph plays a significant role in representing and analyzing com-
plex relationships in real-world applications such as citation net-
works, social networks, and biological data. Graph intelligence is
rapidly becoming a crucial aspect of understanding and exploit-
ing the intricate interconnections within graph data. Recently,
large language models (LLMs) and prompt learning techniques
have pushed graph intelligence forward, outperforming traditional
Graph Neural Network (GNN) pre-training methods and setting
new benchmarks for performance. In this tutorial, we begin by offer-
ing a comprehensive review and analysis of existing methods that
integrate LLMs with graphs. We introduce existing works based
on a novel taxonomy that classifies them into three distinct cate-
gories according to the roles of LLMs in graph tasks: as enhancers,
predictors, or alignment components. Secondly, we introduce a
new learning method that utilizes prompting on graphs, offering
substantial potential to enhance graph transfer capabilities across
diverse tasks and domains. We discuss existing works on graph
prompting within a unified framework and introduce our developed
tool for executing a variety of graph prompting tasks. Addition-
ally, we discuss the applications of combining Graphs, LLMs, and
prompt learning across various tasks, such as urban computing, rec-
ommendation systems, and anomaly detection. This lecture-style
tutorial is an extension of our original work published in
IJCAI 2024 [ 44] and arXiv [ 77] with the invitation of KDD24.
CCS CONCEPTS
â€¢Computing methodologies â†’Artificial intelligence.
KEYWORDS
graph learning; large language model; graph prompting
ACM Reference Format:
Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu.
2024. Graph Intelligence with Large Language Models and Prompt Learning.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671456InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671456
1 INTRODUCTION
Graph theory, or simply graph, is integral to many sectors today, no-
tably in fields like technology, science, and logistics [ 37]. It involves
representing data as nodes and edges to highlight the relationships
and structural linkages among the elements within a graph. Diverse
datasets inherently structured as graphs include citation networks
[66], social networks [ 30], and molecular structures [ 91]. For pro-
cessing and analyzing these graph-based datasets, Graph Neural
Networks (GNNs) [ 41,85] have become increasingly prevalent. The
primary function of GNNs is to develop powerful representations
at the node, edge, or overall graph level, facilitating various ap-
plied tasks by utilizing recursive node-based message passing and
aggregation techniques.
Over recent years, there have been notable developments in
Large Language Models (LLMs) such as Transformers [ 84], BERT
[40], GPT [ 3], and their various iterations. These models have
shown exceptional adaptability and effectiveness in handling nu-
merous natural language processing applications, including senti-
ment analysis, machine translation, text categorization [ 105], and
entity linking [ 45,68]. Initially focused on textual data, there is an
increasing exploration into broadening the applicability of LLMs to
manage different data formats, such as graphs, images, and videos,
enhancing their multi-modal functionality.
Enhancing Graph Tasks with LLMs. The utilization of Large
Language Models (LLMs) has transformed the interaction with
graph data, especially for graphs with text-related node attributes.
As illustrated in Figure 1, the convergence of LLMs and graph
technology has proven effective for a range of applications within
various graph-based domains. The synergy between LLMs and
traditional Graph Neural Networks (GNNs) offers reciprocal bene-
fits and augments the capabilities of graph analytics. GNNs excel
in processing structural data but often utilize node features that
are semantically limited, constraining their ability to fully articu-
late the complexities inherent in the nodes. By integrating LLMs,
GNNs gain access to enriched node features that encapsulate both
the structural and contextual dimensions more effectively. Con-
versely, while LLMs are adept at processing textual information,
6545
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu
Citation Network
Â·Â·Â·Graph Neural 
Network
Large Language
Model
?Multi-domain
Graph Datasets
Graph+LLMs
Link
Prediction
Classification
Reasoning
Recommendation
Â·Â·Â·Downstream Tasks
Social Network
Molecular Graph
Web Link
Figure 1: Across a myriad of graph domains, the integration
of graphs and LLMs demonstrates success in various down-
stream tasks.
they typically lack the capability to analyze the structural data
intrinsic to graphs. By combining LLMs with GNNs, the strengths
of LLMs in textual comprehension can be harmonized with the
structural analytical prowess of GNNs, resulting in enhanced graph
learning capabilities. Following the framework proposed in [ 9], we
organize a detailed first-level taxonomy that classifies the roles
of LLMsâ€”such as enhancers, predictors, and alignersâ€”within the
overall model architecture, introducing greater specificity to the
initial categorizations.
Graph Prompts help graph tasks. Recently, some researchers
have started to introduce prompt learning to graph data [ 7,21,26,
29,55,56,73,74,107]. However, some further studies have found
that the graph prompt is very different from their counterparts
in the NLP area [ 74]. First, the design of graph prompts proves
to be a far more intricate endeavor compared to the formulation
of language prompts. Classic language prompts often comprise
predefined phrases or learnable vectors appended to input text
[3,24]. Here, the primary focus lies in the content of the language
prompt. However, we actually do not know what a graph prompt
looks like. A graph prompt not only contains the prompt "content"
but also includes the undefined task of determining how to struc-
ture these prompt tokens and seamlessly integrate them into the
original graph. Second, the harmonization of downstream graph
problems with the pre-training task is more difficult than language
tasks [ 55,74]. For example, a typical pre-training approach for a
language model is to predict a masked word by the model [ 16].
Then many downstream tasks such as question answering, and sen-
timent classification can be easily reformulated as word-level tasks
[51]. Unlike NLP, where pre-training tasks often share a substantial
task sub-space, graph tasks span node-level [ 27], edge-level [ 100],
and graph-level objectives [ 72,76], making pre-training pretexts
less adaptable. Third, compared with prompts in NLP which are
usually some understandable phrases, graph prompts are usually
less intuitive to non-specialists. The fundamental nature and role
that graph prompts play within the graph model remain somewhat
elusive without comprehensive theoretical analysis. There is also
a lack of clear-cut evaluation criteria for the quality of designed
graph prompts. In addition, there are still many unclear questions
for us to further understand graph prompting. For example, how
effective are these graph prompts? What is their efficiency in terms
Pretrained Graph ModelDownstream TasksFine-tuningPretrained Graph ModelDownstream TasksPromptturningPretraining Graph ModelPretraining Tasks
â€¦++â€¦
TaskDomainPretrainingDomainâ€¦TunedFrozenPromptTaskDomainFigure 2: Graph prompt.
of parameter complexity and training burden? How powerful and
flexible do these prompts manipulate the original graph data? In
light of these intricacies questions, there is a pressing need for
delving deeper into the potential of graph prompts in AGI, thereby
paving the way for a more profound understanding of this evolving
frontier within the broader data science landscape.
2 PRELIMINARY
In this section, we begin by outlining the fundamental concepts of
two critical areas pertinent to our survey: Graph Neural Networks
(GNNs) and Large Language Models (LLMs). Following this, we
provide a concise overview of the newly developed taxonomy.
2.1 Graph Neural Networks
Core Concepts. The majority of current Graph Neural Networks
(GNNs) employ a message-passing framework, which encompasses
both message aggregation and feature updates, as seen in models
like GCN [ 41] and GAT [ 85]. These networks construct represen-
tations of nodes through a process of iteratively gathering and
updating neighbor information using non-linear transformations.
The forward propagation in these networks can be described by
the following equation:
â„(ğ‘™)
ğ‘–=U
â„(ğ‘™âˆ’1)
ğ‘–,M(â„(ğ‘™âˆ’1)
ğ‘–,â„(ğ‘™âˆ’1)
ğ‘—|ğ‘£ğ‘—âˆˆNğ‘–)
Here,â„(ğ‘™)
ğ‘–represents the feature vector of node ğ‘–at layerğ‘™, andNğ‘–
represents the set of neighboring nodes around node ğ‘–. The function
Mis the message-passing function that aggregates information from
these neighbors, while Uis the update function that integrates the
features of the central node with those of its neighbors. By layering
these processes, GNNs are capable of incorporating messages from
extended neighbor networks.
Graph pre-training and prompting. While GNNs have achieved
some success in graph machine learning, they require expensive
annotations and barely generalize to unseen data. To remedy these
deficiencies, graph pre-training aims to extract some general knowl-
edge for the graph models to easily deal with different tasks without
significant annotation cost. The current mainstream graph per-
taining methods can be divided into contrastive and generative
6546Graph Intelligence with Large Language Models and Prompt Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
approaches. For instance, GraphCL [ 97] and GCA [ 109] follow a
contrastive learning framework and maximize the agreement be-
tween two augmented views. [ 75] extend the contrastive idea to
hypergraphs. GraphMAE [ 32], S2GAE [ 78], and WGDN [ 11] mask
the component of the graph and attempt to reconstruct the original
data. The typical learning scheme of â€œpre-training and fine-tuningâ€
is based on the assumption that the pre-training task and down-
stream tasks share some common intrinsic task space. Instead, in
the NLP area, researchers gradually focus on a new paradigm of
â€œpre-training, prompting, and fine-tuningâ€, which aims to reformu-
late input data to fit the pretext. This idea has also been naturally
applied to the graph learning area. GPPT [ 73] first pre-trains graph
model by masked edge prediction, then modify the standalone node
into a token pair and reformulate the downstream classification
as edge prediction task. Additionally, All in One [ 74] proposes a
multi-task prompting framework, which unifies the format of graph
prompts and language prompts.
2.2 Large Language Models
Definitions. Although a precise definition for Large Language
Models (LLMs) remains elusive [ 67], this survey offers a specific de-
lineation for LLMs as mentioned in our discussion. Key distinctions
between LLMs and pre-trained language models (PLMs) are drawn
based on model size and training methodology, as highlighted in
influential surveys [ 94,105]. Specifically, LLMs refer to colossal
models (i.e., those with billions of parameters) that undergo ex-
tensive pre-training on vast datasets. In contrast, PLMs are earlier
models with relatively moderate parameter counts (i.e., millions),
designed for efficient fine-tuning on specific tasks to enhance per-
formance on downstream applications. Considering the typically
smaller parameter size of GNNs and their integration needs, we
align with the broader definition from [ 49], which encompasses
both LLMs and PLMs as previously defined.
Evolution. Large Language Models can be broadly categorized
into non-autoregressive and autoregressive models based on their
approach to language modeling. Non-autoregressive LLMs typi-
cally focus on natural language understanding, utilizing "masked
language modeling" during their pre-training phase. Examples of
such models include BERT [ 40], SciBERT [ 1], and RoBERTa [ 53].
On the other hand, autoregressive LLMs are geared towards natural
language generation, often employing a "next token prediction"
task for foundational training. Recent developments in this category
include models like Flan-T5 [ 14] and ChatGLM [ 99], which utilize
encoder-decoder structures, and models like GPT-3 [ 3], PaLM [ 13],
Galactica [82], and LLaMA [83], which are based on decoder-only
architectures. These advancements in architecture and training
methods have enabled LLMs to exhibit emergent capabilities, such
as performing complex tasks in few-shot or zero-shot settings,
facilitated by techniques like in-context learning [ 17,63] and chain-
of-thought reasoning [88].
2.3 Proposed Taxonomy
We introduce a taxonomy that categorizes methods that merge
graph and text modalities into three primary categories: (1)LLM
as Enhancer, where LLMs are deployed to boost the classification
effectiveness of GNNs; (2)LLM as Predictor, where LLMs leverage
LLMText Attributes
Explanation
LM
GNN
Embeddings
LLM
GNN
EmbeddingsText Attributes
= Tuned
= Frozen= Optional OperationGraphStructure(a) Explanation-based(b) Embedding-basedFigure 3: The illustration of LLM-as-enhancer approaches.
graph structural data to perform predictions; and (3)GNN-LLM
Alignment, where LLMs provide semantic enhancements to GNNs
through various alignment methods. Itâ€™s important to note that
some models scarcely involve LLMs, making them challenging to
classify within these main categories. These exceptions are grouped
under an "Others" category. For instance, in LLM-GNN [ 10], the
LLM serves as an annotator, selecting nodes for ChatGPT to la-
bel, thereby enriching GNN training. GPT4GNAS [ 87] views the
LLM as a knowledgeable controller in the domain of graph neu-
ral architecture search, employing GPT-4 [ 60] to probe the design
landscape and forge new GNN frameworks. Moreover, ENG [ 98]
utilizes the LLM as a sample generator, creating additional labeled
training instances to ensure ample supervisory signals for GNNs.
In the sections that follow, we will delve into a thorough survey of
each primary category in our taxonomy, discussing the integration
of LLMs into graph-centric tasks.
3 INFERENCE FROM PRE-TRAINED MODEL
VIA LLMS
3.1 LLM as Enhancer
GNNs have emerged as effective instruments for analyzing data
with graph structures. Yet, predominant benchmark datasets such
as Cora and Ogbn-Arxiv utilize elementary techniques for encoding
text in text-attributed graphs (TAGs), relying on basic embeddings
like bag-of-words, skip-gram [ 58], or TF-IDF [ 65]. This simplistic
approach can limit the effectiveness of GNNs when applied to TAGs.
The LLM-as-enhancer strategy aims to elevate the quality of node
embeddings by leveraging the capabilities of advanced LLMs. These
enhanced embeddings are then integrated into the graph structure
for use by GNNs or directly fed into downstream classifiers for a
variety of tasks. We classify these methods into two main types:
explanation-based and embedding-based, based on whether the
LLMs are employed to generate additional textual content.
3.1.1 Explanation-based Enhancement. Methods that utilize expla-
nation enhancement to improve textual attributes take advantage
of the powerful zero-shot abilities of LLMs to grasp more intricate
information. Illustrated in Figure 3(a), these techniques generally
6547KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu
involve using LLMs to generate additional, semantically dense data
such as explanations, knowledge entities, and pseudo labels.
For instance, TAPE [ 31] is a pioneer in using explanation-based
enhancement, employing LLMs to generate explanations and pseudo
labels that augment the textual features. These enriched outputs
are then utilized to fine-tune smaller language models, which in
turn create initial node embeddings that incorporate rich semantic
details. Chen et al . [9] explore the applications of LLMs in graph
learning, starting with a comparison of LLMs with visibility to em-
beddings against traditional shallow embedding methods before
presenting KEA. KEA utilizes LLMs to generate a set of knowledge
entities with related text descriptions, which are encoded using
fine-tuned PLMs and sophisticated sentence embedding techniques.
LLM4Mol [ 61] investigates how LLMs can be used to predict molec-
ular properties, specifically by generating detailed explanations for
original SMILES strings, and then fine-tuning a smaller language
model for use in downstream tasks. LLMRec [ 89] aims to tackle
the issues of data sparsity and quality in graph-based recommen-
dation systems by enhancing the edges of user-item interactions
and generating supplementary information for users and items
through LLMs, followed by using a streamlined GNN to process
the enhanced recommendation network.
3.1.2 Embedding-based Enhancement. Embedding-based enhance-
ment methods directly employ LLMs to generate text embeddings
that are used as initial node embeddings in GNN training. This
approach demands the utilization of embedding-visible or open-
source LLMs, as it requires direct access to text embeddings or
the ability to fine-tune LLMs incorporating structural data. How-
ever, many leading-edge LLMs such as GPT4 [ 60] and PaLM [ 13]
are proprietary and only available through online platforms, with
strict controls that limit researchersâ€™ access to their internal pa-
rameters and embeddings. Typically, embedding-based methods
use a sequential approach that leverages structural data during the
pre-training or fine-tuning of language models.
For instance, G ALM [ 92] pre-trains PLMs alongside a GNN aggre-
gator using a vast graph corpus to optimize the capture of useful in-
formation for broad applications, then refines the system on specific
downstream tasks to boost performance. Other studies like GIANT
[12], SimTeG [ 18], and TouchUp-G [ 106] incorporate structural
data in the fine-tuning phase of LLMs to generate node embeddings.
While GIANT utilizes the XR-Transformer to address extreme multi-
label classification rather than link prediction, TouchUp-G applies
negative sampling in link prediction, and SimTeG uses parameter-
efficient techniques to speed up fine-tuning. Additionally, G-Prompt
[36] integrates a graph adapter into PLMs to extract graph-aware
node features, using task-specific prompts post-training to pro-
duce interpretable node representations for various applications.
WalkLM [ 79] is an unsupervised method for learning generic graph
representations that initially generates attributed random walks on
graphs, subsequently creating textual sequences through an auto-
mated textualization program before fine-tuning an LLM to derive
representations. METERN [ 38] incorporates relation-specific prior
tokens to capture relevant signals, employing a single language
encoder to model shared knowledge across relations. LEADING
[93] efficiently fine-tunes LLMs and transfers risk knowledge to
(a) Flatten-based(b) GNN-basedText Attributes
GraphStructure
GNN
InitialFeaturesGraphEmbeddings
LLM
GraphStructureFlattening
LLM
1-hop:
2-hop:
SequenceFigure 4: The illustration of LLM-as-predictor approaches.
downstream GNN models with reduced computational and memory
demands.
A novel approach, OFA [ 48], proposes a versatile graph learn-
ing framework capable of using a unified graph model for adap-
tive downstream predictions. It describes all nodes and edges in
human-readable text and encodes them across different domains
using LLMs, adapting to various tasks by integrating task-specific
prompts into the graph input. ZeroG [ 46] leverages a language
model to encode node attributes and class descriptions, employ-
ing prompt-based subgraph sampling and lightweight fine-tuning
strategies to address cross-dataset zero-shot transferability chal-
lenges in graph learning.
3.2 LLM as Predictor
The core idea behind this category is to utilize LLMs to make predic-
tions for a wide range of graph-related tasks, such as classifications
and reasonings, within a unified generative paradigm. However,
applying LLMs to graph modalities presents unique challenges,
primarily because graph data often lacks straightforward transfor-
mation into sequential text, as different graphs define structures
and features in different ways. We classify the models broadly into
flatten-based and GNN-based predictions, depending on whether
they employ GNNs to extract structural features for LLMs.
3.2.1 Flatten-based Prediction. The majority of the existing at-
tempts that utilize LLMs as predictors employ the strategy of flat-
tening the graph into textual descriptions, which facilitates direct
processing of graph data by LLMs through text sequences. As shown
in Figure 4(a), flatten-based prediction typically involves two steps:
(1)utilizing a flatten function to transform a graph structure into
a sequence of nodes or tokens, and (2)a parsing function is then
applied to retrieve the predicted label from the output generated by
LLMs. As the core of flatten-based prediction, a variety of flatten
functions has been leveraged.
GPT4Graph [ 28] utilizes graph description languages such as
GML and GraphML to represent graphs. These languages provide
standardized syntax and semantics for representing the nodes and
edges within a graph. Inspired by linguistic syntax trees, GraphText
[104] leverages graph-syntax trees to convert a graph structure to
a sequence of nodes, which is then fed to LLMs for training-free
graph reasoning. Furthermore, ReLM [ 69] uses SMILES strings to
6548Graph Intelligence with Large Language Models and Prompt Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
GraphStructureInitialFeatures
GNN
GraphEmbeddings
LLMText Attributes
TextEmbeddingsContrastive/ConcatenateGraphStructureText Attributes
TRM
TRM
...
TRM
TRM
GNN
......Graph Structure
LLMText Attributes
TextEmbeddings
GNN
LLM
Student ModelTeacher Model(a) ContrastiveGraphStructureInitialFeatures
GNN
Pseudo-Labels
LLMText Attributes
E-StepPseudo-LabelsSuperviseM-Step(b) Iterative (c) Graph-Nested (d) Distillation
Figure 5: The illustration of GNN-LLM-Alignment approaches.
provide one-dimensional linearizations of molecular graph struc-
tures. GIMLET [ 102] adopts distance-based position embedding to
extend the capability of LLMs to perceive graph structures. Graph
data can be also represented through methods like adjacency matri-
ces and adjacency lists. Several methods [ 8,22,47,86,101] directly
employ numerically organized node and edge lists to depict the
graph data in plain text. GraphTMI [ 15] further explores different
modalities such as motif and image to integrate graph data with
LLMs.
The use of natural narration to express graph structures is also
making steady progress. [ 9] and [ 33] both integrate the structural in-
formation of citation networks into the prompts, which is achieved
by explicitly representing the edge relationship through the word
â€œciteâ€ and representing the nodes using paper indexes or titles. [ 34],
on the other hand, does not use the word â€œciteâ€ to represent edges
but instead describes the relationships via enumerating randomly
selectedğ‘˜-hop neighbors of the current node. Similarly, Instruct-
GLM [ 96] designs a series of scalable prompts based on the max-
imum hop level. These prompts allow a central paper node to es-
tablish direct associations with its neighbors up to any desired
hop level by utilizing the described connectivity relationships ex-
pressed in natural language. In addition, GPT4Graph [ 28] and [ 9]
imitate the aggregation behavior of GNNs and summarize the cur-
rent neighborâ€™s attributes as additional inputs, aiming to provide
more structural information.
3.2.2 GNN-based Prediction. GNNs have demonstrated impressive
capabilities in understanding graph structures through recursive in-
formation exchange and aggregation among nodes. As illustrated in
Figure 4(b), in contrast to flatten-based prediction, which converts
graph data into textual descriptions as inputs to LLMs, GNN-based
prediction leverages the advantages of GNNs to incorporate inher-
ent structural characteristics and dependencies present in graph
data with LLMs, allowing LLMs to be structure-aware. GNN-based
prediction also relies on a parser to extract the output from LLMs.
Integrating GNN representations into LLMs often requires tuning,
making it easier to standardize the prediction format of LLMs by
providing desirable outputs during training.
Various strategies have been proposed to fuse the structural pat-
terns learned by GNNs and the contextual information captured by
LLMs. For instance, GIT-Mol [ 50] and MolCA [ 54] both implement
BLIP-2â€™s Q-Former [ 42] as the cross-modal projector to map the
graph encoderâ€™s output to the LLMâ€™s input text space. Multiple
objectives with different attention masking strategies are employed
for effective graph-text interactions. GraphLLM [ 5] derives thegraph-enhanced prefix by applying a linear projection to the graph
representation during prefix tuning, allowing the LLM to synergize
with the graph transformer to incorporate structural information
crucial to graph reasoning. Additionally, both GraphGPT [ 81] and
InstructMol [ 4] employ a simple linear layer as the lightweight
alignment projector to map the encoded graph representation to
some graph tokens, while the LLM excels at aligning these tokens
with diverse text information. DGTL [ 62] injects the disentangled
graph embeddings directly into each layer of the LLM, highlighting
different aspects of the graphâ€™s topology and semantics.
3.3 GNN-LLM Alignment
The integration of GNNs with LLMs provides an effective strategy
for merging graph and textual data. This approach maintains the
unique strengths of each model by aligning their embedding spaces
at a specific juncture. Alignment methods can be classified into two
types: symmetric and asymmetric. Symmetric alignment considers
GNNs and LLMs on equal footing, while asymmetric alignment
gives precedence to one model over the other.
3.3.1 Symmetric. Symmetric alignment treats graph and text data
equally during the process of alignment, ensuring that the encoding
systems for each modality perform well in their respective uses.
The architecture of symmetric alignment, depicted in Figure 5(a),
typically features a dual-tower design, with distinct encoders for
graph and text data processing. These modalities only interact
once in the alignment process. Traditional methods such as SAFER
[6] often employ straightforward concatenation to merge these
embeddings.
Recent advancements in dual-tower architectures have embraced
contrastive learning techniques, following models such as CLIP [ 63],
to enhance the alignment of different data types. This process usu-
ally involves first extracting representations from both graphs and
texts, which are then aligned using a modified InfoNCE loss [ 59].
Text2Mol [ 19] implements a cross-modal attention mechanism that
facilitates the early integration of graph and text embeddings. It uti-
lizes a transformer decoder that processes the output from an LLM
as the source sequence and from a GNN as the target sequence,
which are then used in contrastive learning with GNN outputs.
Models like MoMu [ 71], MoleculeSTM [ 52], ConGraT [ 2], and RLM-
Rec [ 64] follow a similar approach, aligning paired graph and text
embeddings using contrastive learning. MoMu and MoleculeSTM,
sourcing molecules from PubChem, pair these with texts from sci-
entific articles and molecule descriptions, respectively. ConGraT
6549KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu
expands this pairing method to include social, knowledge, and ci-
tation networks, while RLMRec integrates semantic spaces from
LLMs with interaction signals in recommendation systems through
contrastive learning. Further refinements in contrastive learning
are seen in G2P2 [90] and GRENADE [43], where G2P2 applies con-
trastive learning at multiple levelsâ€”node-text, text-text summary,
and node-node summaryâ€”during pre-training, and uses prompts
for downstream tasks. G RENADE enhances alignment by incorpo-
rating graph-centric dual-level knowledge alignment, focusing on
both individual nodes and their neighborhoods.
Iterative alignment, illustrated in Figure 5(b), offers a unique
approach by enabling ongoing interactions between modalities.
For instance, GLEM [ 103] uses an Expectation-Maximization (EM)
framework, where encoders iteratively generate pseudo-labels for
each other, continually refining the alignment of the two represen-
tation spaces.
3.3.2 Asymmetric. Symmetric alignment equally considers both
modalities, while asymmetric alignment gives precedence to one,
often using GNNs to enhance LLMs through structural processing.
Prominent techniques in this area include graph-nested transform-
ers and graph-aware distillation.
The graph-nested transformer, highlighted by Graphformer [ 95]
in Figure 5(c), showcases asymmetric alignment by embedding
GNN capabilities into each transformer layer. In this setup, the node
embeddings are derived from the initial token-level embeddings,
specifically from the [ CLS] token. The method involves aggregating
embeddings from pertinent nodes and processing them through a
graph transformer. The resulting output is merged with the initial
embeddings and forwarded to the subsequent transformer layer.
Patton [ 39] builds on this by introducing network-contextualized
masked language modeling and masked node prediction, achiev-
ing notable results in various applications such as classification,
retrieval, reranking, and link prediction.
GRAD [57] utilizes graph-aware distillation to align both modal-
ities, as illustrated in Figure 5(d). This approach uses a GNN as
a teacher model to create soft labels that train an LLM. As the
LLMs share parameters, updates to these parameters also benefit
the GNN by refining its text encodings. Through repeated updates, a
graph-informed LLM is developed, which offers increased inference
scalability due to the elimination of the GNN in the final model.
Similarly, THLM [ 110] uses a heterogeneous GNN to augment LLMs
with advanced topology learning capabilities. This method involves
co-training an LLM and an auxiliary GNN through Context Graph
Prediction and Masked Language Modeling tasks. After the co-
training phase, the auxiliary GNN is removed, allowing the refined
LLM to be fine-tuned for specific downstream tasks.
4 INFERENCE FROM PRE-TRAINED MODEL
VIA GRAPH PROMPT
In this section, we introduce a comprehensive framework for graph
prompt design. As depicted in Figure 6, a graph prompt comprises
three essential components: prompt tokens with an associated vec-
tor; token structures that preserve the intrinsic correlations among
tokens; and insertion patterns for integrating the original graph
with prompts. This framework prompts several pivotal inquiries:
Question 1: How are graph prompts constructed? Question 2: How
PromptGraphOriginal GraphFourKindsofInsertingPatternsPromptedGraph
ByCrossLinksByFeatureAddingPromptedGraph
PromptedGraph
ByConcatenatingByMultiplicationPromptedGraph
TokenfeaturePromptTokenTokenStructure
NodefeatureFigure 6: Visualization of Prompt Tokens, Structures, and
Insertion Patterns.
are downstream tasks redefined to align with pre-training tasks?
Question 3: What methods are effective for prompt learning? With
these questions, we summarize the most representative works pub-
lished recently and present them in Table 2.
4.1 Design Aspects of Graph Prompts
Question 1: How are Graph Prompts Designed?
A. Prompt Representation as Tokens. The fundamental con-
cept of a graph prompt entails augmenting the original graph fea-
tures with additional attributes [ 108]. For instance, consider a graph
feature matrix X={ğ‘¥1,Â·Â·Â·,ğ‘¥ğ‘}âˆˆRğ‘Ã—ğ‘‘, whereğ‘¥ğ‘–âˆˆR1Ã—ğ‘‘rep-
resents the ğ‘–-th nodeâ€™s feature, and ğ‘‘signifies the feature space
dimension. Studies such as Fang et al . [20] and Shirkavand and
Huang [70] conceptualize the basic prompt as a learnable vector
ğ‘âˆˆR1Ã—ğ‘‘, which, when added to each node feature, transforms
the feature matrix to Xâˆ—={ğ‘¥1+ğ‘,Â·Â·Â·,ğ‘¥ğ‘+ğ‘}. This restructured
feature matrix facilitates the utilization of pre-trained graph models
for processing the graph. Subsequent research [ 21] has extended
this concept to multiple tokens, enhancing performance. Various
approaches, including PGCL [ 26] and VNT [ 80], have introduced
distinct prompt vectors for semantic and contextual interpretations,
applying them to graph-level representations through element-wise
multiplication. Differently, GraphPrompt [ 55] employs prompt to-
kens at the modelâ€™s hidden layers, optimizing for graph pooling
operations, thus differing in prompt token dimensionality and ap-
plication.
B. Prompt Representation as Graphs. Innovative approaches
such as All in One [ 74] conceptualize the graph prompt as an ad-
ditional learnable subgraph, incorporating nodes that parallel the
original nodes in representation size and semantic space. This de-
sign facilitates direct manipulation of node features through prompt
tokens. The structural design of prompts encompasses both intra-
token links and cross-links between the prompt and original graphs,
allowing for seamless integration and processing as a unified graph
within pre-trained models.
6550Graph Intelligence with Large Language Models and Prompt Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: A summary of models that leverage LLMs to assist graph-related tasks in literature, ordered by their release time.
Fine-tuning denotes whether it is necessary to fine-tune the parameters of LLMs, and â™¥indicates that models employ parameter-
efficient fine-tuning (PEFT) strategies, such as LoRA and prefix tuning. Prompting indicates the use of text-formatted prompts
in LLMs, done manually or automatically. Acronyms in Task: Node refers to node-level tasks; Link refers to link-level tasks;
Graph refers to graph-level tasks; Reasoning refers to Graph Reasoning; Retrieval refers to Graph-Text Retrieval; Captioning
refers to Graph Captioning.
Mo
del GNN LLM Predictor Fine-tuning Prompting Domain Task CodeLLM
as EnhancerGIAN
T [12] SAGE, RevGAT, etc. BERT GNN âœ— âœ— Citation, Co-purchase Node Link
GALM [92] RGCN, RGAT BERT GNN âœ“ âœ— E-Commerce, Recommendation Node, Link -
TAPE [31] RevGAT ChatGPT GNN âœ— âœ“ Citation Node Link
Chen et al. [9] GCN, GAT ChatGPT GNN âœ— âœ“ Citation, Co-purchase Node -
LLM4Mol [61] - ChatGPT LM âœ— âœ— Molecular Graph Link
SimTeG [18] SAGE, RevGAT, SEAL allMiniLM-L6-v2, etc. GNN âœ“â™¥âœ— Citation, Co-purchase Node, Link Link
G-Prompt [36] SAGE, RevGAT RoBERTa-Large GNN âœ“ âœ“ Citation, Social Node -
TouchUp-G [106] SAGE, MB-GCN, etc. BERT GNN âœ“ âœ— Citation, Co-purchase, Recommendation Node, Link -
OFA [48] R-GCN Sentence-BERT GNN âœ— âœ“ Citation, Web link, Knowledge, Molecular Node, Link, Graph Link
LLMRec [89] LightGCN ChatGPT GNN âœ— âœ“ Recommendation Recommendation Link
WalkLM [79] - DistilRoBERTa MLP âœ“ âœ— Knowledge Node, Link Link
METERN [38] - BERT LM âœ“ âœ— Citation, E-Commerce Node -
LEADING [93] GCN, GAT BERT GNN âœ“ âœ— Citation Node -LLM
as PredictorNLGraph
[86] - Text-davinci-003 LLM âœ— âœ“ - Reasoning Link
GPT4Graph [28] - Text-davinci-003 LLM âœ— âœ“ - Reasoning, Node, Graph Link
GIMLET [102] - T5 LLM âœ“/âœ— âœ“ Molecular Graph Link
Chen et al. [9] - ChatGPT LLM âœ— âœ“ Citation Node Link
GIT-Mol [50] GIN MolT5 LLM âœ“â™¥âœ“ Molecular Graph, Captioning -
InstructGLM [96] - FLAN-T5/LLaMA-v1 LLM âœ“â™¥âœ“ Citation Node Link
Liu et al. [47] - GPT-4, etc. LLM âœ— âœ“ - Reasoning Link
Huang et al. [34] - ChatGPT LLM âœ— âœ“ Citation, Co-purchase Node Link
GraphText [104] - ChatGPT/GPT-4 LLM âœ— âœ“ Citation, Web link Node -
Fatemi et al. [22] - PaLM/PaLM 2 LLM âœ— âœ“ - Reasoning -
GraphLLM [5] Graph Transformer LLaMA-v2 LLM âœ“â™¥âœ“ - Reasoning Link
Hu et al. [33] - ChatGPT/GPT-4 LLM âœ— âœ“ Citation, Knowledge, Social Node, Link, Graph -
MolCA [54] GINE Galactica/MolT5 LLM âœ“â™¥âœ“ Molecular Graph, Retrieval, Captioning Link
GraphGPT [81] Graph Transformer Vicuna LLM âœ“â™¥âœ“ Citation Node Link
ReLM [69] TAG, GCN Vicuna/ChatGPT LLM âœ— âœ“ Molecular Reaction Prediction Link
LLM4DyG [101] - Vicuna/LLaMA-v2/ChatGPT LLM âœ— âœ“ - Reasoning -
DGTL [62] Disentangled GNN LLaMA-v2 LLM âœ“ âœ“ Citation, E-Commerce Node -
GraphTMI [15] - GPT-4/GPT-4V LLM âœ— âœ“ Citation Node -
InstructMol [4] GIN Vicuna LLM âœ“â™¥âœ“ Molecular Graph, Captioning LinkGNN-LLM
AlignmentSAFER[6]
GCN, GAT, etc. RoBERTa Linear âœ“ âœ— News Node Link
GraphFormers [95] Graph Transformer UniLM LLM âœ“ âœ— Citation, E-Commerce, Knowledge Link Link
Text2Mol[19] GCN SciBERT GNN/LLM âœ“ âœ— Molecular Retrieval Link
MoMu [71] GIN BERT GNN/LLM âœ“ âœ— Molecular Graph, Retrieval Link
MoleculeSTM [52] GIN BERT GNN/LLM âœ“ âœ— Molecular Graph, Retrieval Link
GLEM [103] SAGE, RevGAT, etc. DeBERTa GNN/LLM âœ“ âœ— Citation, Co-purchase Node Link
GRAD [57] SAGE SciBERT/DistilBERT LLM âœ“ âœ— Citation, Co-purchase Node Link
G2P2 [90] GCN Transformer GNN/LLM âœ“ âœ“ Citation, Recommendation Node Link
Patton [39] Graph Transformer BERT/SciBERT Linear/LLM âœ“ âœ— Citation, E-Commerce Node, Link, Retrieval, Reranking Link
ConGraT [2] GAT all-mpnet-base-v2/DistilGPT2 GNN/LLM âœ“ âœ— Citation, Knowledge, Social Node, Link Link
THLM [110] R-HGNN BERT LLM âœ“ âœ— Academic, Recommendation, Patent Node, Link Link
GRENADE [43] SAGE, RevGAT-KD, etc. BERT GNN/MLP âœ“ âœ— Citation, Co-purchase Node, Link Link
RLMRec [64] GCCF, LightGCN, etc. ChatGPT, text-embedding-ada-002 GNN/LLM âœ“ âœ— Recommendation Node LinkOthersLLM-GNN
[10] GCN, SAGE ChatGPT GNN âœ— âœ“ Citation, Co-purchase Node Link
GPT4GNAS [87] GCN, GIN, etc. GPT-4 GNN âœ— âœ“ Citation Node -
ENG [98] GCN, GAT ChatGPT GNN âœ— âœ“ Citation Node -
4.2 Task Alignment via Answering Mechanisms
Question 2: Reformulation of Downstream Tasks to Pretexts.
A. Addressing Diverse Task Levels. All in One [ 74] demon-
strates a method to adapt the graph model for graph-level con-
trastive learning, aiming to unify node-level, edge-level, and graph-
level tasks via induced subgraphs. This approach posits that the
addition of a graph prompt inherently simulates various graph ma-
nipulations, thereby enabling the straightforward reformulation of
downstream tasks.
B. Learnable Answering Mechanisms. Adaptable task heads
[74], such as MLPs, facilitate the output of downstream task results
by leveraging graph-level representations from pre-trained models.
This versatility allows for minimal data requirements during tuning,
promoting ease of alignment between tasks [20, 21, 80].
C. Predefined Answering Mechanisms. To minimize tuning
complexity, strategies such as predefined non-trainable answeringfunctions have been proposed. These functions categorize nodes by
aligning them with specific prompts corresponding to node types,
leveraging pre-trained models for direct classification without ad-
ditional trainable components [25, 35, 56, 73].
4.3 Efficient Learning of Graph Prompts
Question 3: Optimizing Graph Prompt Learning.
A. Application of Meta-Learning. To ascertain optimal prompt
parameters, techniques from meta-learning, specifically the Model-
Agnostic Meta-Learning (MAML) framework [ 23], are employed.
This approach facilitates the derivation of prompts that exhibit
generalizability across a diverse array of graph-based tasks, thereby
enhancing their versatility and effectiveness.
B. Task-specific Fine-tuning. While studies such as All in One
[74] aim for general applicability across tasks, others focus on opti-
mizing prompts for specific challenges, such as graph classification.
6551KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu
Table 2: Summary of existing representative works on graph prompt. S: Subgraph. ğ‘‰(S): Node set within subgraph S.ğœ‹:
Pre-trained parameters. ğœ™: Task head parameters. ğœƒ: Prompt parameters. Ëœs: Filled prompt.
pr
e-training task pr
ompt design do
wnstream tasks answ
ering function
Pap
er no
de e
dge graph pr
ompt components inserting
pattern pr
ompt tuning no
de e
dge graph Pr
eset Learnable
GPPT
(KDD
2022 [73])âœ— âœ“ âœ—structur
e token:
sğ‘£âˆˆRğ‘‘
task token:
cğ‘¦âˆˆRğ‘‘sğ‘£ğ‘–â†ğ‘“ğœƒ(ğ‘£ğ‘–)
Ëœsğ‘¦
,ğ‘£ğ‘–â†[cğ‘¦,sğ‘£ğ‘–]Cr
oss Entropy âœ“ âœ— âœ— âœ“ âœ—
GPF
(
arXiv [20])âœ“ âœ“ âœ“ pr
ompt feature pâˆˆRğ‘‘Ëœsğ‘–â†xğ‘–+pmax p,
ğœ™Ã
(ğ‘¦ğ‘–,Ëœsğ‘–)
ğ‘ğœ‹,ğœ™(ğ‘¦ğ‘–|Ëœsğ‘–)âœ— âœ— âœ“ âœ— âœ“
All
in One
(KDD 2023 [74])âœ— âœ— âœ“pr
ompt token:
P={p1,...,p|P|}
token structure:
{(pğ‘–,pğ‘—)|pğ‘–,pğ‘—âˆˆP}ğ‘¤ğ‘–
ğ‘˜â†ğœ(pğ‘˜Â·xğ‘‡
ğ‘–)
ifğœ(pğ‘˜Â·xğ‘‡
ğ‘–)>ğ›¿else0
Ëœsğ‘–â†xğ‘–+Ã|P|
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜pğ‘˜Meta-Learning âœ“ âœ“ âœ“ âœ“ âœ“
GraphPr
ompt
(WWW 2023[55])âœ— âœ“ âœ—pr
ompt token:
pğ‘¡âˆˆRğ‘‘,ğ‘¡âˆˆT
structure token: sâˆˆRğ‘‘
task token: cğ‘¦âˆˆRğ‘‘Ëœsğ‘¡
ğ‘–â†Readout(
{pğ‘¡âŠ™ğ‘“ğœ‹(ğ‘£)|
ğ‘£âˆˆğ‘‰(Sğ‘–)})
cğ‘¦â†Mean({Ëœsğ‘¡
ğ‘—|ğ‘¦ğ‘—=ğ‘¦})minpğ‘¡âˆ’Ã
(ğ‘¦ğ‘–,Sğ‘–)ln
e
xp(sim(Ëœsğ‘¡
ğ‘–,cğ‘¦ğ‘–)/ğœ)Ã
ğ‘¦âˆˆğ‘Œe
xp(sim(Ëœsğ‘¡
ğ‘–,cğ‘¦)/ğœ)âœ“ âœ— âœ“ âœ“ âœ—
PGCL
(
arXiv [26])âœ— âœ— âœ“semantic
token: pğ‘ âˆˆRğ‘‘
contextual token: pğ‘âˆˆRğ‘‘ğ‘§ğ‘
ğ‘ 
ğ‘¥=ğ‘§ğ‘ ğ‘¥âŠ™pğ‘ 
ğ‘§ğ‘ğ‘
ğ‘¥=ğ‘§ğ‘ğ‘¥âŠ™pğ‘minâˆ’Ã
(ğ‘£
,ğ‘,ğ‘)âˆˆTlog
exp
sim
ğ‘§ğ‘
ğ‘£,ğ‘§ğ‘
ğ‘
/ğœ
Ã
ğ‘¢âˆˆ
{ğ‘¥,ğ‘¦}exp
sim
ğ‘§ğ‘
ğ‘£,ğ‘§ğ‘
ğ‘¢
/ğœâœ“ âœ“ âœ“ âœ“ âœ—
PRODIGY
(NeurIPS
2023 [35])âœ— âœ“ âœ—data
graph:
Gğ·âˆ¼âŠ•ğ‘˜
ğ‘–=1N(Vğ‘–,G)âŸ©
task graph:Gğ‘‡Ëœsğ‘–â†ğ‘“ğ‘‡
ğœ‹ğ‘‡(
Gğ‘‡|ğ‘“ğ·
ğœ‹ğ·(Gğ·)) Fixe
d âœ“ âœ“ âœ“ âœ“ âœ—
SGL-PT
(
arXiv [107])âœ“ âœ— âœ“pr
ompt token:
one vector for each graphconne
ct to all nodes in the graphcontrastiv
e loss and
reconstruction lossâœ“ âœ— âœ— âœ“ âœ—
GPF-P
lus
(NeurIPS 2023 [21])âœ“ âœ“ âœ“pr
ompt features
p1,Â·Â·Â·,pğ‘˜âˆˆRğ‘‘Ëœsğ‘–â†xğ‘–+ğœ(p1,
Â·
Â·Â·,pğ‘˜)max p,
ğœ™Ã
(ğ‘¦ğ‘–,Ëœsğ‘–)
ğ‘ğœ‹,ğœ™(ğ‘¦ğ‘–|Ëœsğ‘–)âœ— âœ— âœ“ âœ— âœ“
De
epGPT
(arXiv [70])âœ“ âœ“ âœ“pr
ompt token:
pâˆˆRğ‘‘
prefix token:
PâˆˆR|P|Ã—ğ‘‘Ëœxğ‘–â†xğ‘–+p
Ëœsğ‘–â†ğ‘“P,
ğœ‹(ğº,Ëœxğ‘–)minp,P,
ğœ™Ã
(ğ‘¦ğ‘–,ğ‘£ğ‘–)
L(ğ‘ğœ™(Ëœsğ‘–),ğ‘¦ğ‘–)âœ— âœ— âœ“ âœ— âœ“
ULTRA
-DP
(arXiv [7])âœ— âœ“ âœ—pr
ompt token:
pğ‘–=ptask+ğ‘¤posppos
ğ‘–,
ppos
ğ‘–denotesğ‘£ğ‘–â€™s
positional embeddingcr
eate a virtual node
ğ‘£ğ‘
ğ‘–for target node ğ‘£ğ‘–,
ğºâ€²â†(Vâˆª{ğ‘£ğ‘
ğ‘–},
Eâˆª{(ğ‘£ğ‘
ğ‘–,ğ‘£ğ‘–)},Xâˆª{pğ‘–})Multi-task-Learning âœ“ âœ— âœ— âœ— âœ“
HetGPT
(
arXiv [56])âœ— âœ“ âœ—pr
ompt token:
F={fğ´
ğ‘–}ğ¾
ğ‘–=1
forğ´âˆˆA
task token: cğ‘¦âˆˆRğ‘‘Ëœsğ´
ğ‘–â†xğ´
ğ‘–+Ãğ¾
ğ‘˜=1ğ‘¤ğ‘–
ğ‘˜fğ´
ğ‘˜,
zğ‘–=ğ‘“ğœ‹(ğº,Ëœsğ´
ğ‘–)minC,Fâˆ’Ã
(ğ‘¦ğ‘–,
ğ‘£ğ‘–)log
exp(sim(zğ‘–,cğ‘¦ğ‘–)/ğœ)Ã
ğ‘¦âˆˆYe
xp(sim(zğ‘–,cğ‘¦)/ğœ)âœ“ âœ— âœ— âœ“ âœ—
SAP
(
arXiv [25])âœ“ âœ— âœ—task
token:
P={cğ‘¦}ğ‘¦âˆˆY
structure token:
W={(ğ‘£ğ‘–,ğ‘ğ‘—)}ğ‘£ğ‘–âˆˆV,ğ‘ğ‘—âˆˆPğºâ€²â†
(VâˆªP,EâˆªW)
Z(1)=MLPğœ‹â€²(X)
Z(2)=GNNğœ‹â€²â€²([X,P],
[A,W])minWâˆ’Ã
(ğ‘¦ğ‘–,
ğ‘£ğ‘–)log
exp(sim(z(1)
ğ‘–,z(2)
ğ‘¦ğ‘–)/ğœ)
Ã
ğ‘¦âˆˆYe
xp(sim(z(1)
ğ‘–,z(2)
ğ‘¦)/ğœ)âœ“ âœ— âœ“ âœ“ âœ—
VN
T
(KDD 2023 [80])âœ“ âœ“ âœ—ğ‘·=
ğ’‘1;.
..;ğ’‘ğ‘ƒ
,
ğ’‘ğ‘âˆˆRğ¹
ğ¸1âˆ¥ğ‘1
=ğ¿1 
ğ¸0âˆ¥ğ‘ƒ
âˆˆR(ğ‘‰+ğ‘ƒ)
Ã—ğ¹ Cr
oss Entropy âœ“ âœ— âœ— âœ— âœ“
This targeted approach allows for the precise tuning of prompts
and associated task heads, ensuring maximum performance for
given objectives. For instance, GPF [ 20] optimizes its methodology
specifically for graph classification tasks, aligning the tuning of
prompt tokens and task heads with the goal of enhancing prediction
accuracy.
C. Alignment with Pretext Tasks. Prompt tuning that is coher-
ent with the objectives of pretext tasks offers a natural and effective
strategy for learning. By adopting loss functions and learning ob-
jectives that mirror those used in pre-training, approaches such as
GraphPrompt [ 55] and GPPT [ 73] ensure that the adaptation and
integration of prompts are intrinsically aligned with the underlying
modelsâ€™ capabilities and training paradigms.5 CONCLUSION
The application of LLMs and graph prompts to graph tasks has
emerged as a prominent area of research in recent years. In this
survey, we aim to provide an in-depth overview of existing strate-
gies for adapting LLMs and graph prompts to graphs. Firstly, we
introduce a novel taxonomy that categorizes existing techniques
based on the different roles played by LLMs. Secondly, we systemat-
ically review the representative studies according to the taxonomy.
Thirdly, by exploring the interplay between graph prompts and
models, weâ€™ve revealed fresh insights into the essence of graph
prompts, highlighting their pivotal role in reshaping AI for graph
data. Through this comprehensive review, we aspire to shed light
on the advancements and challenges in the field of graph learning
6552Graph Intelligence with Large Language Models and Prompt Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
with LLMs, thereby encouraging further enhancements in this do-
main. We hope our survey can push forward a new era of insights
and applications in graph intelligence.
REFERENCES
[1]Iz Beltagy, Kyle Lo, et al .2019. SciBERT: A pretrained language model for
scientific text. arXiv preprint arXiv:1903.10676 (2019).
[2]William Brannon, Suyash Fulay, et al .2023. ConGraT: Self-Supervised Con-
trastive Pretraining for Joint Graph and Text Embeddings. arXiv preprint
arXiv:2305.14321 (2023).
[3]Tom Brown, Benjamin Mann, et al .2020. Language models are few-shot learners.
NeurIPS 33 (2020), 1877â€“1901.
[4]He Cao, Zijing Liu, et al .2023. InstructMol: Multi-Modal Integration for Building
a Versatile and Reliable Molecular Assistant in Drug Discovery. arXiv preprint
arXiv:2311.16208 (2023).
[5]Ziwei Chai, Tianjie Zhang, et al .2023. GraphLLM: Boosting Graph Reasoning
Ability of Large Language Model. arXiv preprint arXiv:2310.05845 (2023).
[6]Shantanu Chandra, Pushkar Mishra, et al .2020. Graph-based modeling of online
communities for fake news detection. arXiv preprint arXiv:2008.06274 (2020).
[7]Mouxiang Chen, Zemin Liu, Chenghao Liu, Jundong Li, Qiheng Mao, and Jian-
ling Sun. 2023. ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph
Dual Prompt. arXiv preprint (2023).
[8]Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. 2024. GraphWiz: An Instruction-
Following Language Model for Graph Problems. arXiv preprint arXiv:2402.16029
(2024).
[9]Zhikai Chen, Haitao Mao, et al .2023. Exploring the potential of large language
models (llms) in learning on graphs. arXiv preprint arXiv:2307.03393 (2023).
[10] Zhikai Chen, Haitao Mao, et al .2024. Label-free Node Classification on Graphs
with Large Language Models (LLMS). In ICLR.
[11] Jiashun Cheng, Man Li, et al .2023. Wiener Graph Deconvolutional Network
Improves Graph Self-Supervised Learning. In AAAI. 7131â€“7139.
[12] Eli Chien, Wei-Cheng Chang, et al .2021. Node feature extraction by self-
supervised multi-scale neighborhood prediction. arXiv preprint arXiv:2111.00064
(2021).
[13] Aakanksha Chowdhery, Sharan Narang, et al .2022. Palm: Scaling language
modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).
[14] Hyung Won Chung, Le Hou, et al .2022. Scaling instruction-finetuned language
models. arXiv preprint arXiv:2210.11416 (2022).
[15] Debarati Das, Ishaan Gupta, et al .2023. Which Modality should I useâ€“Text,
Motif, or Image?: Understanding Graphs with Large Language Models. arXiv
preprint arXiv:2311.09862 (2023).
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv preprint (2019).
[17] Qingxiu Dong, Lei Li, et al .2022. A survey for in-context learning. arXiv preprint
arXiv:2301.00234 (2022).
[18] Keyu Duan, Qian Liu, et al .2023. Simteg: A frustratingly simple approach
improves textual graph learning. arXiv preprint arXiv:2308.02565 (2023).
[19] Carl Edwards, ChengXiang Zhai, et al .2021. Text2mol: Cross-modal molecule
retrieval with natural language queries. In EMNLP. 595â€“607.
[20] Taoran Fang, Yunchao Zhang, Yang Yang, and Chunping Wang. 2022. Prompt
tuning for graph neural networks. arXiv preprint (2022).
[21] Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, and Lei Chen. 2023.
Universal Prompt Tuning for Graph Neural Networks. In NeurIPS.
[22] Bahare Fatemi, Jonathan Halcrow, et al .2023. Talk like a Graph: Encoding
Graphs for Large Language Models. arXiv preprint arXiv:2310.04560 (2023).
[23] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-
Learning for Fast Adaptation of Deep Networks. In ICML.
[24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-Trained Language
Models Better Few-Shot Learners. In ACL. 3816â€“3830.
[25] Qingqing Ge, Zeyuan Zhao, Yiding Liu, Anfeng Cheng, Xiang Li, Shuaiqiang
Wang, and Dawei Yin. 2023. Enhancing Graph Neural Networks with Structure-
Based Prompt. arXiv preprint (2023).
[26] Chenghua Gong, Xiang Li, Jianxiang Yu, Cheng Yao, Jiaqi Tan, Chengcheng
Yu, and Dawei Yin. 2023. Prompt Tuning for Multi-View Graph Contrastive
Learning. arXiv preprint (2023).
[27] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In KDD. 855â€“864.
[28] Jiayan Guo, Lun Du, et al. 2023. GPT4Graph: Can Large Language Models Un-
derstand Graph Structured Data? An Empirical Evaluation and Benchmarking.
arXiv preprint arXiv:2305.15066 (2023).
[29] Yuxin Guo, Cheng Yang, Yuluo Chen, Jixi Liu, Chuan Shi, and Junping Du.
2023. A Data-Centric Framework to Endow Graph Neural Networks with
Out-of-Distribution Detection Ability. In KDD. 638â€“648.
[30] Will Hamilton, Zhitao Ying, et al .2017. Inductive representation learning on
large graphs. NeurIPS 30 (2017).[31] Xiaoxin He, Xavier Bresson, et al .2024. Explanations as Features: LLM-Based
Features for Text-Attributed Graphs. In ICLR.
[32] Zhenyu Hou, Xiao Liu, et al .2022. Graphmae: Self-supervised masked graph
autoencoders. In SIGKDD. 594â€“604.
[33] Yuntong Hu, Zheng Zhang, et al .2023. Beyond Text: A Deep Dive into
Large Language Modelsâ€™ Ability on Understanding Graph Data. arXiv preprint
arXiv:2310.04944 (2023).
[34] Jin Huang, Xingjian Zhang, et al .2023. Can LLMs effectively leverage graph
structural information: when and why. arXiv preprint arXiv:2309.16595 (2023).
[35] Qian Huang, Hongyu Ren, Peng Chen, Gregor KrÅ¾manc, Daniel Zeng, Percy
Liang, and Jure Leskovec. 2023. PRODIGY: Enabling In-context Learning Over
Graphs. In NeurIPS.
[36] Xuanwen Huang, Kaiqiao Han, et al .2023. Prompt-based Node Feature Ex-
tractor for Few-shot Learning on Text-Attributed Graphs. arXiv preprint
arXiv:2309.02848 (2023).
[37] Shaoxiong Ji, Shirui Pan, et al .2021. A survey on knowledge graphs: Represen-
tation, acquisition, and applications. TNNLS 33, 2 (2021), 494â€“514.
[38] Bowen Jin, Wentao Zhang, et al .2023. Learning Multiplex Embeddings on
Text-rich Networks with One Text Encoder. arXiv preprint arXiv:2310.06684
(2023).
[39] Bowen Jin, Wentao Zhang, et al .2023. Patton: Language Model Pretraining on
Text-Rich Networks. arXiv preprint arXiv:2305.12268 (2023).
[40] Jacob Devlin Ming-Wei Chang Kenton et al .2019. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding. In Proceedings of
NAACL-HLT. 4171â€“4186.
[41] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[42] Junnan Li, Dongxu Li, et al .2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023).
[43] Yichuan Li, Kaize Ding, et al .2023. GRENADE: Graph-Centric Language Model
for Self-Supervised Representation Learning on Text-Attributed Graphs. arXiv
preprint arXiv:2310.15109 (2023).
[44] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and
Jeffrey Xu Yu. 2023. A survey of graph meets large language model: Progress
and future directions. arXiv preprint (2023).
[45] Yuhan Li, Wei Shen, Jianbo Gao, and Yadong Wang. 2022. Community ques-
tion answering entity linking via leveraging auxiliary data. arXiv preprint
arXiv:2205.11917 (2022).
[46] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG:
Investigating Cross-dataset Zero-shot Transferability in Graphs. arXiv preprint
arXiv:2402.11235 (2024).
[47] Chang Liu and Bo Wu. 2023. Evaluating large language models on graphs:
Performance insights and comparative analysis. arXiv preprint arXiv:2308.11224
(2023).
[48] Hao Liu, Jiarui Feng, et al .2024. One for All: Towards Training One Graph
Model for All Classification Tasks. In ICLR.
[49] Jiawei Liu, Cheng Yang, et al .2023. Towards Graph Foundation Models: A
Survey and Beyond. arXiv preprint arXiv:2310.11829 (2023).
[50] Pengfei Liu, Yiming Ren, et al. 2023. GIT-Mol: A Multi-modal Large Language
Model for Molecular Science with Graph, Image, and Text. arXiv preprint
arXiv:2308.06911 (2023).
[51] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
Graham Neubig. 2023. Pre-train, Prompt, and Predict: A Systematic Survey of
Prompting Methods in Natural Language Processing. Comput. Surveys 55, 9
(2023), 195:1â€“195:35.
[52] Shengchao Liu, Weili Nie, et al .2022. Multi-modal molecule structure-text model
for text-based retrieval and editing. arXiv preprint arXiv:2212.10789 (2022).
[53] Yinhan Liu, Myle Ott, et al .2019. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 (2019).
[54] Zhiyuan Liu, Sihang Li, et al .2023. MolCA: Molecular Graph-Language Mod-
eling with Cross-Modal Projector and Uni-Modal Adapter. arXiv preprint
arXiv:2310.12798 (2023).
[55] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. Graphprompt:
Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. In
The Web Conference. 417â€“428.
[56] Yihong Ma, Ning Yan, Jiayu Li, Masood Mortazavi, and Nitesh V. Chawla. 2023.
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous
Graph Neural Networks. arXiv preprint (2023).
[57] Costas Mavromatis, Vassilis N Ioannidis, et al .2023. Train Your Own
GNN Teacher: Graph-Aware Distillation on Textual Graphs. arXiv preprint
arXiv:2304.10668 (2023).
[58] Tomas Mikolov, Ilya Sutskever, et al .2013. Distributed representations of words
and phrases and their compositionality. NeurIPS 26 (2013).
[59] Aaron van den Oord, Yazhe Li, et al .2018. Representation learning with con-
trastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[60] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
6553KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu
[61] Chen Qian, Huayi Tang, et al .2023. Can large language models empower
molecular property prediction? arXiv preprint arXiv:2307.07443 (2023).
[62] Yijian Qin, Xin Wang, et al .2023. Disentangled Representation Learning
with Large Language Models for Text-Attributed Graphs. arXiv preprint
arXiv:2310.18152 (2023).
[63] Alec Radford, Jong Wook Kim, et al .2021. Learning transferable visual models
from natural language supervision. In ICLR. 8748â€“8763.
[64] Xubin Ren, Wei Wei, et al .2023. Representation Learning with Large Language
Models for Recommendation. arXiv preprint arXiv:2310.15950 (2023).
[65] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in
automatic text retrieval. IPM 24, 5 (1988), 513â€“523.
[66] Prithviraj Sen, Galileo Namata, et al .2008. Collective classification in network
data. AI magazine 29, 3 (2008), 93â€“93.
[67] Erfan Shayegani, Md Abdullah Al Mamun, et al .2023. Survey of Vulnerabilities
in Large Language Models Revealed by Adversarial Attacks. arXiv preprint
arXiv:2310.10844 (2023).
[68] Wei Shen, Yuhan Li, Yinan Liu, Jiawei Han, Jianyong Wang, and Xiaojie Yuan.
2021. Entity linking meets deep learning: Techniques and solutions. IEEE
Transactions on Knowledge and Data Engineering 35, 3 (2021), 2556â€“2578.
[69] Yaorui Shi, An Zhang, et al .2023. ReLM: Leveraging Language Models for
Enhanced Chemical Reaction Prediction. arXiv preprint arXiv:2310.13590 (2023).
[70] Reza Shirkavand and Heng Huang. 2023. Deep Prompt Tuning for Graph
Transformers. arXiv preprint (2023).
[71] Bing Su, Dazhao Du, et al .2022. A molecular multimodal foundation model asso-
ciating molecule graphs with natural language. arXiv preprint arXiv:2209.05481
(2022).
[72] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2020. InfoGraph:
Unsupervised and Semi-supervised Graph-Level Representation Learning via
Mutual Information Maximization. In ICLR.
[73] Mingchen Sun, Kaixiong Zhou, et al .2022. Gppt: Graph pre-training and prompt
tuning to generalize graph neural networks. In SIGKDD. 1717â€“1727.
[74] Xiangguo Sun, Hong Cheng, et al .2023. All in One: Multi-Task Prompting for
Graph Neural Networks. In SIGKDD. 2120â€“2131.
[75] Xiangguo Sun, Hong Cheng, et al. 2023. Self-supervised hypergraph represen-
tation learning for sociological analysis. TKDE (2023).
[76] Xiangguo Sun, Hongzhi Yin, Bo Liu, Hongxu Chen, Jiuxin Cao, Yingxia Shao,
and Nguyen Quoc Viet Hung. 2021. Heterogeneous Hypergraph Embedding for
Graph Classification. In WSDM. 725â€“733.
[77] Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, and Jia Li.
2023. Graph prompt learning: A comprehensive survey and beyond. arXiv
preprint (2023).
[78] Qiaoyu Tan, Ninghao Liu, et al .2023. S2GAE: Self-Supervised Graph Autoen-
coders are Generalizable Learners with Graph Masking. In WSDM. 787â€“795.
[79] Yanchao Tan, Zihao Zhou, et al .2023. WalkLM: A Uniform Language Model
Fine-tuning Framework for Attributed Graph Embedding. In NeurIPS.
[80] Zhen Tan, Ruocheng Guo, Kaize Ding, and Huan Liu. 2023. Virtual Node Tuning
for Few-shot Node Classification. In KDD. 2177â€“2188.
[81] Jiabin Tang, Yuhao Yang, et al .2023. GraphGPT: Graph Instruction Tuning for
Large Language Models. arXiv preprint arXiv:2310.13023 (2023).
[82] Ross Taylor, Marcin Kardas, et al .2022. Galactica: A large language model for
science. arXiv preprint arXiv:2211.09085 (2022).
[83] Hugo Touvron, Thibaut Lavril, et al .2023. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971 (2023).
[84] Ashish Vaswani, Noam Shazeer, et al. 2017. Attention is all you need. NeurIPS
30 (2017).[85] Petar Velickovic, Guillem Cucurull, et al .2018. GRAPH ATTENTION NET-
WORKS. stat1050 (2018), 4.
[86] Heng Wang, Shangbin Feng, et al .2023. Can Language Models Solve Graph
Problems in Natural Language? arXiv preprint arXiv:2305.10037 (2023).
[87] Haishuai Wang, Yang Gao, et al .2023. Graph Neural Architecture Search with
GPT-4. arXiv preprint arXiv:2310.01436 (2023).
[88] Jason Wei, Xuezhi Wang, et al .2022. Chain-of-thought prompting elicits rea-
soning in large language models. NeurIPS 35 (2022), 24824â€“24837.
[89] Wei Wei, Xubin Ren, et al .2023. LLMRec: Large Language Models with Graph
Augmentation for Recommendation. arXiv preprint arXiv:2311.00423 (2023).
[90] Zhihao Wen and Yuan Fang. 2023. Prompt Tuning on Graph-augmented Low-
resource Text Classification. arXiv preprint arXiv:2307.10230 (2023).
[91] Zhenqin Wu, Bharath Ramsundar, et al .2018. MoleculeNet: a benchmark for
molecular machine learning. Chemical science 9, 2 (2018), 513â€“530.
[92] Han Xie, Da Zheng, et al .2023. Graph-Aware Language Model Pre-Training on
a Large Graph Corpus Can Help Multiple Graph Applications. arXiv preprint
arXiv:2306.02592 (2023).
[93] Rui Xue, Xipeng Shen, et al .2023. Efficient Large Language Models Fine-Tuning
On Graphs. arXiv preprint arXiv:2312.04737 (2023).
[94] Jingfeng Yang, Hongye Jin, et al .2023. Harnessing the power of llms in practice:
A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 (2023).
[95] Junhan Yang, Zheng Liu, et al .2021. GraphFormers: GNN-nested transformers
for representation learning on textual graph. NeurIPS 34 (2021), 28798â€“28810.
[96] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang.
2023. Natural language is all a graph needs. arXiv preprint arXiv:2308.07134
(2023).
[97] Yuning You, Tianlong Chen, et al .2020. Graph contrastive learning with aug-
mentations. NeurIPS (2020), 5812â€“5823.
[98] Jianxiang Yu, Yuxiang Ren, et al .2023. Empower Text-Attributed Graphs Learn-
ing with Large Language Models (LLMs). arXiv preprint arXiv:2310.09872 (2023).
[99] Aohan Zeng, Xiao Liu, et al .2022. Glm-130b: An open bilingual pre-trained
model. arXiv preprint arXiv:2210.02414 (2022).
[100] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural
Networks. In NeurIPS.
[101] Zeyang Zhang, Xin Wang, et al .2023. LLM4DyG: Can Large Language Models
Solve Problems on Dynamic Graphs? arXiv preprint arXiv:2310.17110 (2023).
[102] Haiteng Zhao, Shengchao Liu, et al .2023. GIMLET: A Unified Graph-Text
Model for Instruction-Based Molecule Zero-Shot Learning. arXiv preprint
arXiv:2306.13089 (2023).
[103] Jianan Zhao, Meng Qu, et al .2022. Learning on large-scale text-attributed
graphs via variational inference. arXiv preprint arXiv:2210.14709 (2022).
[104] Jianan Zhao, Le Zhuo, et al .2023. Graphtext: Graph reasoning in text space.
arXiv preprint arXiv:2310.01089 (2023).
[105] Wayne Xin Zhao, Kun Zhou, et al .2023. A survey of large language models.
arXiv preprint arXiv:2303.18223 (2023).
[106] Jing Zhu, Xiang Song, et al .2023. TouchUp-G: Improving Feature Representation
through Graph-Centric Finetuning. arXiv preprint arXiv:2309.13885 (2023).
[107] Yun Zhu, Jianhao Guo, and Siliang Tang. 2023. SGL-PT: A Strong Graph Learner
with Graph Prompt Tuning. arXiv preprint (2023).
[108] Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, and Siliang Tang. 2023.
GraphControl: Adding Conditional Control to Universal Graph Pre-trained
Models for Graph Domain Transfer Learning. arXiv preprint (2023).
[109] Yanqiao Zhu, Yichen Xu, et al .2021. Graph contrastive learning with adaptive
augmentation. In WWW. 2069â€“2080.
[110] Tao Zou, Le Yu, et al .2023. Pretraining Language Models with Text-Attributed
Heterogeneous Graphs. arXiv preprint arXiv:2310.12580 (2023).
6554