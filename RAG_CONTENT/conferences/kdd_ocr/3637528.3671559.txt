Deep Bag-of-Words Model: An Efficient and Interpretable
Relevance Architecture for Chinese E-Commerce
Zhe Lin
linzhe.lin@taobao.com
Alibaba Group
HangZhou, ChinaJiwei Tanâˆ—
jiwei.tjw@taobao.com
Alibaba Group
HangZhou, ChinaDan Ou
oudan.od@taobao.com
Alibaba Group
HangZhou, China
Xi Chen
gongda.cx@taobao.com
Alibaba Group
HangZhou, ChinaShaowei Yao
yaoshaowei@taobao.com
Alibaba Group
HangZhou, ChinaBo Zheng
bozheng@alibaba-inc.com
Alibaba Group
HangZhou, China
ABSTRACT
Text relevance or text matching of query and product is an essential
technique for the e-commerce search system to ensure that the
displayed products can match the intent of the query. Many studies
focus on improving the performance of the relevance model in
search system. Recently, pre-trained language models like BERT
have achieved promising performance on the text relevance task.
While these models perform well on the offline test dataset, there
are still obstacles to deploy the pre-trained language model to the
online system as their high latency. The two-tower model is ex-
tensively employed in industrial scenarios, owing to its ability to
harmonize performance with computational efficiency. Regrettably,
such models present an opaque â€œblack boxâ€ nature, which prevents
developers from making special optimizations. In this paper, we
raise deep Bag-of-Words (DeepBoW) model, an efficient and in-
terpretable relevance architecture for Chinese e-commerce. Our
approach proposes to encode the query and the product into the
sparse BoW representation, which is a set of word-weight pairs.
The weight means the important or the relevant score between the
corresponding word and the raw text. The relevance score is mea-
sured by the accumulation of the matched word between the sparse
BoW representation of the query and the product. Compared to
popular dense distributed representation that usually suffers from
the drawback of black-box, the most advantage of the proposed
representation model is highly explainable and interventionable,
which is a superior advantage to the deployment and operation
of online search engines. Moreover, the online efficiency of the
proposed model is even better than the most efficient inner product
form of dense representation. The proposed model is experimented
on three different datasets for learning the sparse BoW representa-
tions, including the human-annotation set, the search-log set and
the click-through set. Then the models are evaluated by experienced
âˆ—Jiwei Tan is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671559human annotators. Both the auto metrics and the online evalua-
tions show our DeepBoW model achieves competitive performance
while the online inference is much more efficient than the other
models. Our DeepBoW model has already deployed to the biggest
Chinese e-commerce search engine Taobao and served the entire
search traffic for over 6 months.
CCS CONCEPTS
â€¢Information systems â†’Similarity measures; Document rep-
resentation ;Query representation .
KEYWORDS
E-Commerce, Text Matching, Relevance
ACM Reference Format:
Zhe Lin, Jiwei Tan, Dan Ou, Xi Chen, Shaowei Yao, and Bo Zheng. 2024. Deep
Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture
for Chinese E-Commerce. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671559
1 INTRODUCTION
The popularization of mobile internet has significantly elevated the
prominence of online commerce in daily life. Hundreds of millions
of customers purchase products they want on large e-commerce
portals, such as Taobao and Amazon. The search engine emerges as
the essential technology in assisting users to discover products that
are in accord with their preferences. Different from general search
engines like Google, commercial e-commerce search engines are
usually designed to improve the userâ€™s engagement and conversion,
possibly at the cost of relevance in some cases [ 2]. The exhibition
of products in search results that are inconsistent with the query
intent has the potential to diminish the customer experience and
hamper customersâ€™ long-term trust and engagement. Consequently,
measuring relevance between the text of search query and products
to filter the irrelevant products is an indispensable process in the
e-commerce search engine.
5398
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhe Lin, et al.
Text relevance has been a long-standing research topic due to
its importance in information retrieval and the search engine. Re-
searchers and engineers have been dedicated to the pursuit of an ef-
ficient and robust model to accurately measure the text relevance be-
tween the query and the product in the e-commerce scenario. Con-
ventional methodologies have traditionally harnessed attributes
such as the word matching ratio, Term Frequency-Inverse Docu-
ment Frequency (TF-IDF, notably BM25), or cosine similarity to
serve as the relevance score, often yielding strong baseline perfor-
mances. Nevertheless, these word-matching approaches may cause
inaccuracies due to inconsistent linguistic expressions of identical
meanings such as synonyms. This issue is particularly pronounced
in the e-commerce scenario since the queries are usually described
by users in daily language while the products are described by
sellers in professional language. Thus severe vocabulary gap may
exist between queries and products [31].
With the development of deep learning technology, neural mod-
els have shown their advantage in addressing the semantic match-
ing problem for the text relevance task [ 6,19,29]. Recently, pre-
trained language models like BERT [ 4] achieve excellent results in
various NLP tasks including text matching. Unfortunately, typical
paradigm of the BERT-based relevance model is the interaction-
based structure, which needs to encode the query-document pair
in real time to measure their relevance. This makes it difficult to
be deployed to online systems with large traffic due to the high
computation and latency. Consequently, it is usually impractical
to deploy the pre-trained model directly to search systems. To ad-
dress this problem, the representation-based model, also known
as the two-tower model, is proposed and mostly applied to indus-
trial search systems. It usually pre-computes the embeddings of
query/document respectively, and measures the relevance online
from the embeddings. SiameseBERT [ 22] leverages BERT as the
encoder and calculates the cosine similarity between the dense
embeddings of query and document as the relevance score. Some
studies like ReprBERT [ 33] explore the more complex MLP classifier
to compute the relevance score between two dense embeddings,
which can achieve improved performance.
However, the representation-based model with dense embed-
dings still faces two major issues. First, the dense embedding may
lose the detailed semantic information of the text, especially for low-
frequency words like product models, entity names, or even brand
identifiers. These words are essential in the e-commerce relevance
task. Second, the dense embedding presents an opaque â€œblack boxâ€
nature, which prevents developers from comprehensively under-
standing the modelâ€™s methodology for calculating relevance scores.
Developers often find it difficult to analyze the reasons for bad cases
in the online system and implement targeted optimizations. In con-
trast, traditional word-matching algorithms like BM25 continue to
be favored in numerous industrial applications[ 27] due to their high
efficiency and robust interpretability. Such word-based algorithm
can capture the match of words that are low-frequency but essential
for the text-relevance task. Unfortunately, these methods are not
without their constraints. They fall short in recognizing different
linguistic expressions that convey identical meanings, such as syn-
onyms, thereby limiting their effectiveness in semantic matching
tasks.Is it possible to combine the advantages of both deep semantic
models and word matching methods? In this paper, we propose
Deep Bag-of-Words (DeepBoW), which can leverage the pre-
trained language model with large language corpora to improve
semantic modeling while preserving the computational efficiency
and interpretability of the word-matching method. We realize this
by designing to learn sparse bag-of-words representation through
deep neural networks. Our model generates the query/product high-
dimensional representation (called the BoW representation) instead
of the low-dimensional distributed representation (i.e. embedding).
The dimensional size is the same as the size of the vocabulary. Each
position in this high-dimensional representation corresponds to
a word in the vocabulary, and with a value represents the weight
of this word in the BoW representation, just like the BoW vector
of TF-IDF. The proposed DeepBoW model encounters two pre-
dominant challenges. Firstly, due to the opaque nature of neural
network models, often colloquially referred to as "black boxes",
it is challenging to correlate positions within high-dimensional
representations to specific words in the vocabulary. Secondly, the
vocabulary size is usually much larger than the dimension of dense
embedding. Expanding the dimensional size to the vocabulary size
may explode the computation and storage resources. For the first
challenge, we elaborately design the architecture of the model and
loss function to align the position of the high-dimensional repre-
sentation and the corresponding word in the vocabulary. For the
second challenge, we add a sparse constraint in the loss function
to reduce valid positions in the high-dimensional representation
since the query/product should not include all words of vocabu-
lary. Finally, we sample the high-dimensional representation to a
small set of non-zero word-weight pairs, which is named as sparse
BoW representation. In addition, although queries and product
descriptions in e-commerce primarily consist of keywords, there
still exists some semantic dependency on the word combination,
which unigrams may not capture adequately. For example, brand
names with multi-words may be incorrectly matched if they are
separated. Consequently, we propose to model n-gram in our Deep-
BoW model, meanwhile introducing an n-gram hashing vocabulary
strategy to avoid the explosion of vocabulary size. Finally, with the
sparse BoW representation, the relevance score is measured in a
most easy way as the weight accumulation of the matching words
in the query/productâ€™s sparse BoW representations, which makes
it highly efficient and interpretable.
The proposed DeepBoW model is evaluated on the three indus-
trial datasets. The results show that our DeepBoW model achieves
more than 2.1% AUC improvement compared to the state-of-the-art
two-tower relevance model on Chinese e-commerce. The sparse
BoW representation generated by our DeepBoW model has posi-
tive interpretability and supports easy problem investigation and
intervention for online systems. The time complexity of the online
relevance score computing program can be optimized to O(ğ‘)by
leveraging the two-pointer algorithm, which is faster than previous
state-of-the-art relevance model. The contribution of this paper is
summarized as follows:
â€¢We reveal that the semantics of query and product in the e-
commerce scenario can be represented by the bag-of-words
5399Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
vectors with importance weight. We show the BoW repre-
sentation can be more suitable for the industrial e-commerce
search system as itâ€™s more interpretable and flexible than the
dense embedding from the BERT-based model.
â€¢We introduce an innovative architecture designed to encode
query/product into two distinct sparse Bag-of-Words (BoW)
representations. We elucidate a methodology by which rele-
vance scoring, based on these sparse BoW representations,
can reduce the latency of the online relevance model while
preserving competitive performance.
â€¢Our proposed DeepBoW model is evaluated both on of-
fline human-annotation datasets in Chinese and online A/B
testing, achieving strong performance and efficiency. The
model has already been deployed on the largest Chinese e-
commerce platform Taobao, and has been serving the entire
search traffic for over six months.
The rest of this paper is organized as follows. In Section 2 we
introduce related work. The proposed method is detailed in Section
3, and experimental results are presented in Section 4. Finally, in
Section 5 we conclude this work and discuss the future work.
2 RELATED WORK
2.1 Text Matching
The text matching task takes textual sequences as input and pre-
dicts a numerical value or a category indicating their relationship.
Text matching is a long-stand problem and a hot research topic
as itâ€™s important in information retrieval and search system. The
e-commerce relevance learning can be regarded as a text-matching
task. Early work mostly performs keyword-based matching that
relies on manually defined features, such as TF-IDF similarity and
BM25 [ 24]. These methods cannot effectively utilize raw text fea-
tures and usually fail to evaluate semantic relevance.
Recently with the development of deep learning, neural-based
text-matching models have been employed to solve the semantic
matching task and have achieved promising performance. The archi-
tecture of the neural-based text-matching model can be roughly di-
vided into the interaction-based model and the representation-based
(two-tower) model. The interaction-based model [6, 19, 19, 20, 29]
usually puts all candidate text together as the input. The model
can employ the full textual feature to calculate the matching fea-
ture as the low layer, and then aggregate the partial evidence of
relevance to make the final decision. So interaction-based model
can leverage sophisticated techniques in the aggregation procedure
and achieve better performance. More recent studies are built upon
the pre-trained language model like BERT [ 4]. With an extremely
large corpus for pre-training, these methods can achieve new state-
of-the-art performance on various benchmarks. The architecture
of these models is the pre-trained bidirectional Transformer [ 28],
which can also be regarded as an interaction-based model. The
typical paradigm of the BERT-based relevance model is to feed text
pair into BERT and then build a non-linear classifier upon BERTâ€™s
[CLS] output token to predict the relevance score [15, 16].
Although having excellent performance in the text-matching
task, interaction-based models are still hard to be deployed to prac-
tical online service as they are mostly time-consuming, and the
features of queries and documents cannot be pre-computed offline.Two-tower models are widely used in many online search systems.
The two-tower model consists of two identical neural networks,
each taking one of the two inputs. DSSM [ 7,25] is a two-tower
model that employs two separate deep full-connected networks to
encode the candidate texts. Meanwhile, more sophisticated archi-
tectures can be adopted to enhance the ability of learning semantic
representations. LSTM-DSSM [ 17] and LSTM-RNN [ 18] use RNNs
to explicitly model word dependencies in the sentences. Typically
dot-product, cosine, or parameterized non-linear layers are used
to measure the similarity between representations of all candidate
texts. Since individually encoding both the queries and documents,
the embeddings of them can be pre-computed offline. Therefore,
representation-based methods are online efficient and are widely
used in industrial search engines. However, the encoding proce-
dure of two inputs is independent with each other, making the final
classifier hard to predict their relationship.
2.2 Search Relevance Matching
Relevance in search engine is a special sub-task of the text-matching
which computes the relevance score between the query and the
product (as the document). Different from the typical text-matching
task which all input texts are semantically similar and homogeneous
(i.e. having comparable lengths), the length of query may be much
shorter than the length of document. Query only needs to match
the partial semantics in the document.
A large number of models are proposed for conducting matching
in search. Neural Tensor Network (NTN) [ 26] is originally pro-
posed to explicitly model multiple interactions of relational data.
It achieves powerful representation ability that can represent mul-
tiple similarity functions, including cosine similarity, dot product,
and bilinear product, etc. Qiao et al . [21] apply the BERT model to
ad-hoc retrieval and passage retrieval. Reimers and Gurevych [23]
propose Sentence-BERT for reducing the computational overhead
for text matching. Bai et al . [1] conduct a pilot study to map the
frequency-based and BoW representation of a document to a sparse
term importance distribution for text retrieval.
E-commerce search is a special scenario of the Web search sys-
tem. Both tasks model the semantic matching between query and
candidate and require high efficiency and low latency in the online
search system. Differently, in Web search the query and document
are usually very different in length, making most methods not fea-
sible for the e-commerce relevance task. Currently, there is not a
commonly-used public benchmark for the Chinese e-commerce
relevance task, so previous works usually evaluate their models
on the online service and the real-world dataset constructed from
the online platforms. Guo et al . [5] introduce a typical framework
for e-commerce relevance learning. A Siamese network is adopted
to learn pair-wise relevance of two products to a query. They in-
vestigate training the model with user clicks and batch negatives,
followed by finetuning with human supervision to calibrate the
score by pair-wise learning. Xiao et al . [31] propose a co-training
framework to address the data sparseness problem by investigating
the instinctive connection between query rewriting and semantic
matching. Yao et al . [32] propose to learn a two-tower relevance
model from click-through data in e-commerce by designing a point-
wise loss function. Zhang et al . [34] also find the weakness of
5400KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhe Lin, et al.
training with click signals, and address this problem by proposing
a multi-task learning framework of query intent classification and
semantic textual similarity to improve semantic matching efficiency.
Nigam et al . [14] introduce a 3-part hinge loss to differentiate mul-
tiple types of training data. They classified training instances into
three categories: random negative examples, impressed but not
purchased examples, and purchased items. Recently Yao et al . [33]
propose ReprBERT, which has the advantages of both excellent per-
formance and low latency, by distilling the interaction-based BERT
model to a representation-based architecture. This framework is
taken as the baseline of our model.
3 METHODOLOGY
3.1 Overview
The proposed DeepBoW model is based on the two-tower archi-
tecture, which encodes the query and document separately and
computes the semantic relevance score with the representations of
query and document. Different from other text relevance models
with dense embeddings, our model encodes the query and docu-
ment into the Bag-of-Words vectors and calculates the relevance
score from sparse BoW representations.
In this section, we introduce the components of our model in
detail. We first describe the multi-granularity encoder to aggregate
the character-grained feature and word-grained feature. Next, we
introduce two different sparse BoW representations including the
term-weighting BoW and the synonym-weighting BoW. Then, we
show how to use N-gram hashing to reduce the semantic loss from
word segmentation and enhance the quality of the sparse BoW
representation. Finally, we describe the training process of our
model in detail and show the deployment of our DeepBoW model to
the online e-commerce search system. Figure 1 shows an overview
of the DeepBoW model.
3.2 Multi-Granularity Encoder
The text encoder aims to obtain the input textâ€™s contextual repre-
sentations. We choose Transformer encoder [ 28] as our sentence
encoder because of its excellent performance in many tasks. The
Transformer encoder is a stack of ğ¿identical layers, and each layer
includes a multi-head self-attention and a fully connected feed-
forward network. For the input senquence ğ‘†, we obtain the out-
put encoding matrix of ğ‘–-th layer as ğ»ğ‘–={â„ğ‘–
1,â„ğ‘–
2,Â·Â·Â·,â„ğ‘–
ğ‘™}, where
â„ğ‘–
ğ‘—âˆˆRğ‘‘is the word embedding vector and ğ‘™is the number of words
inğ‘†. Same to ReprBERT [ 33], we aggregate the output of each layer
as the text encoding representation according to:
Ëœâ„ğ‘–=Â©Â­Â­
Â«1
ğ‘›âˆ‘ï¸
â„ğ‘–
ğ‘—âˆˆğ»ğ‘–â„ğ‘–
ğ‘—ÂªÂ®Â®
Â¬Wm+bm
â„="ğ¿n
ğ‘–=1â„ğ‘–#
Wagg+bagg(1)
where WmâˆˆRğ‘‘Ã—ğ‘‘,WaggâˆˆRğ¿Â·ğ‘‘Ã—ğ‘‘,bm,baggâˆˆRğ‘‘,fis the con-
catenate operation.Usually the character-based model performs better than the
word-based model in Chinese NLP tasks [ 11], and most Chinese-
BERT models are character-based Transformer architecture. How-
ever, since our model intends to catch the relationship between
the semantic and the word of the sentence, we propose to encode
not only the character-segmentation sequence but also the word-
segmentation sequence separately. For convenience, we denote
the character-segmentation sequence and the word-segmentation
sequence of the input text as ğ‘†ğ‘={ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘š}andğ‘†ğ‘¤=
{ğ‘¤1,ğ‘¤2,Â·Â·Â·,ğ‘¤ğ‘›}, respectively, where ğ‘ğ‘–andğ‘¤ğ‘–are the indices of
the token in the vocabulary. The output encoding matrices of the
character-segmentation sequence and the word-segmentation se-
quence are ğ»ğ‘={â„1ğ‘,â„2ğ‘,Â·Â·Â·,â„ğ‘šğ‘}andğ»ğ‘¤={â„1ğ‘¤,â„2ğ‘¤,Â·Â·Â·,â„ğ‘›ğ‘¤},
whereğ‘šandğ‘›are the lengths of the character-segmentation and
the word-segmentation sequence. The text encoding representation
of these two sequences are â„ğ‘andâ„ğ‘¤.
3.3 Sparse BoW Representation
Unlike traditional two-tower architecture models that encode text
into the "embedding" which is a dense distributed representation,
our model encodes the text into the sparse BoW representation.
The sparse BoW representation is a set of word-weight pairs, where
each word corresponds to a weight that indicates the importance or
the relevance of this word to the input text. In this section, we in-
troduce two different sparse BoW representations: term-weighting
BoW representation and synonym-weighting BoW representation,
and describe the module to generate these two sparse BoW repre-
sentations in detail.
3.3.1 Term-Weighting BoW Representation.
In the e-commerce search system, the query inputted by user may
contain some redundant or unrelated words. These words can be
excised with negligible impact on the text semantic. For example,
for the input query from Taobao like "2024 å¹´å¤å­£é€‚åˆå‡†å¦ˆå¦ˆ
å­•å¦‡å¥—è£…"1, "å‡†å¦ˆå¦ˆ " and "å­•å¦‡" both mean a pregnant woman,
but "å­•å¦‡" is more accurate than " å‡†å¦ˆå¦ˆ " at semantic level as
the latter word is polysemous and more colloquial. " é€‚åˆ " which
means suitable, can be regarded as a stop word in the e-commerce
scenario. So " å‡†å¦ˆå¦ˆ " and "é€‚åˆ" can be discarded and the other
words should be retained.
Term-Weighting BoW includes all words of the input text, and
each word is assigned a weight that indicates its significance within
the textâ€™s semantics. Key words like brand and category should
have greater importance weights than the other words. Figure 1
(a) shows the architecture to generate the term-weighting BoW
representation. Then, the term-weighting BoW representation can
be produced as follows:
ğ‘ğ‘–=exp â„âŠ¤ğ‘â„ğ‘–ğ‘¤
Ã
â„ğ‘—
ğ‘¤âˆˆğ»ğ‘¤exp
â„âŠ¤ğ‘â„ğ‘—
ğ‘¤
BoW tw(ğ‘†ğ‘¤):={ğ‘¤ğ‘–:ğ‘ğ‘–,ğ‘¤ğ‘–âˆˆğ‘†ğ‘¤}(2)
We define the BoW tw(Â·)as the term-weighting BoW representation
ofÂ·.ğ‘ğ‘–is the importance weight of ğ‘¤ğ‘–inğ‘†ğ‘¤, andÃ
ğ‘–ğ‘ğ‘–=1.
1This query means â€œ2024 Summer Pregnant Womenâ€™s Clothingâ€.
5401Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¦Tok1Tok2Tok3Tok mâ€¦Tok1Tok2Tok3Tok mCharacter Encoder
â€¦â€¦â€¦Tok1Tok2Tok3Tok mAggregation Layerâ€¦Tok1Tok2Tok3Tok nâ€¦Tok1Tok2Tok3Tok nWord Encoderâ€¦â€¦â€¦Tok1Tok2Tok3Tok n"!w1w2w3wiwi+1wi+2wi+3wi+4â€¦â€¦wi+5VocabularyAggregation Layerw1w2w3wvâ€¦""w1w2w3wvâ€¦â€¦Tok1Tok2Tok3Tok mâ€¦Tok1Tok2Tok3Tok mCharacter Encoder
â€¦â€¦â€¦Tok1Tok2Tok3Tok mAggregation Layerâ€¦Tok1Tok2Tok3Tok nâ€¦Tok1Tok2Tok3Tok nWord Encoderâ€¦â€¦â€¦Tok1Tok2Tok3Tok nâ€¦â„!â„"#â„"$â„"%â„"&â€¦w1w2w3wiwi+1wi+2wi+3wi+4â€¦â€¦wi+5Vocabulary
(a)(b)Term-Weighting BoW RepresentationSynonym-Expansion BoW Representation!!!"!#!$
Figure 1: An overview of the DeepBoW model. Figure (a) shows the architecture that encodes the input text into the Term-
Weighting BoW representation, which gathers the attention weight of each word as its weight in the term-weighting BoW
representation. Figure (b) shows the architecture that encodes the input text into the Synonym-Expansion BoW representation,
which generates sparse BoW representation from character embedding and word embedding respectively, and aggregates these
two representations as the synonym-expansion BoW representation.
3.3.2 synonym-expansion BoW Representation.
Since queries in e-commerce search systems are entered by lay
users, they may differ from the product descriptions and include
colloquialisms and polysemes. Some adjectives or category words
may also have synonyms. Term-weighting BoW representation can
only compute the importance weight of each word in the text, but is
unable to add relevant words and synonyms. Synonym expansion
can greatly improve the performance of the e-commerce search
system. Therefore, we propose the synonym-expansion BoW repre-
sentation to enhance the retrieval performance of the sparse BoW
representation. Figure 1 (b) shows the architecture to generate the
synonym-expansion BoW representation.
We sampleğ‘£words from the training corpora as the vocabulary
Vaccording to the frequency of the word. Our model leverages the
relevance between these words and the input text to represent the
semantics of the query and the products. First, our model aggregates
the word-based text encoding representation and the character-
based text encoding representation as follows:
Ëœâ„ğ‘¤=ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–Â·â„ğ‘–
ğ‘¤
ğ‘‰ğ‘=ğœ(â„ğ‘Wc+bc)
ğ‘‰ğ‘¤=ğœh
â„ğ‘||Ëœâ„ğ‘¤i
Ww+bw(3)
where WcâˆˆRğ‘‘Ã—ğ‘£,WwâˆˆR2ğ‘‘Ã—ğ‘£andbc,bwâˆˆRğ‘£.ğœis the sigmoid
function. Then, the synonym-expansion BoW Representation of
the input text is as follows:
ğ‘ğ‘”=ğœh
â„ğ‘||Ëœâ„ğ‘¤i
Wg+bg
BoW SE(ğ‘†ğ‘¤):={ğ‘¡:ğ‘‰ğ‘(ğ‘¡),ğ‘¡âˆˆVâˆ’ğ‘†ğ‘¤}âˆª
{ğ‘¡:ğ‘ğ‘”ğ‘‰ğ‘(ğ‘¡)+(1âˆ’ğ‘ğ‘”)ğ‘‰ğ‘¤(ğ‘¡),ğ‘¡âˆˆğ‘†ğ‘¤âˆ©V}(4)whereğ‘‰(ğ‘–)denotes the ğ‘–-th value of ğ‘‰. We define the BoW SE(Â·)
as the synonym-expansion BoW representation of Â·.ğ‘¡is the word
(actually is the index of the word) in V. the corresponding weight
is in[0,1], which can be regarded as the relevance score between
this word and the input text.
3.4 N-gram Hashing Vocabulary
In the preceding section, we describe the sparse BoW representation
in detail. Unfortunately, due to the limitation of modelâ€™s parameter
size, we can only leverage the vocabulary within a limited number
of words. Using the â€œ [UNK] â€ to replace all Out-Of-Vocabulary (OOV)
words may lead to significant semantic loss. To mitigate this issue,
we introduce an ensemble of hashing tokens into the vocabulary,
where the OOV word can be replaced with its hashing tokens2.
Semantic loss may occur between the raw text and its BoW rep-
resentation, particularly when syntactically cohesive phrases are
fragmented during the word segmentation process. This issue can
lead to misalignment for the essential semantics such as product
types, entity names, or brand identifiers in query/product. For ex-
ample, the brand name â€œ L'ORÃ‰AL Paris â€ could be inaccurately
divided into separate tokens during word segmentation. To address
this problem, we introduce an N-gram hashing vocabulary strategy.
Concretely, N-gram phrases are incorporated into the textâ€™s BoW
and are subsequently replaced with their respective hashing tokens,
analogous to the treatment of OOV words. The significance of a
particular N-gram phrase is directly proportional to the frequency
of its occurrence within relevant query-product pairs in the cor-
pora. Our model is equipped to ascertain the importance of these
N-gram hashing tokens through the analysis of large-scale corpora.
Consequently, the semantics of these N-gram phrases are retained
within the sparse BoW representation.
2For the wordğ‘¤, we leverage MD5(ğ‘¤)%ğµas its hashing token, where ğµis the hashing
bucket number.
5402KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhe Lin, et al.
3.5 DeepBoW Relevance Model
In this section, we describe the method to compute the relevance
score between the query and the product from the sparse BoW
representations. Note that in the search engine scenario the prod-
uct should match all the semantics of query, while conversely the
query does not need to match all the semantics of the product.
Accordingly, we encode the query as the term-weighting BoW rep-
resentation while encode the product as the synonym-expansion
BoW representation. The relevance score of the query/product can
be calculated as follows:
ğ‘…ğ‘¡(ğ‘„,ğ·)=âˆ‘ï¸
(ğ‘¤:ğ‘)âˆˆBoWTW(ğ‘„)
(ğ‘¡:ğ‘”)âˆˆBoWSE(ğ·)âˆ‘ï¸
ğ‘¤=ğ‘¡ğ‘Ã—ğ‘” (5)
whereğ‘„is the query, ğ·is the product, and ğ‘…ğ‘¡(ğ‘„,ğ·)is the relevance
score between ğ‘„andğ·. We call our DeepBoW model with this
relevance score as DeepBoW(Q-Weight).
We leverage the cross-entropy loss between the output score and
the ground truth to train our model. In addition, we also optimize
the L2 norm of BoW SE(ğ·)to enhance the sparsity of the synonym-
expansion BoW representation, so that only the most relevant words
can get high scores. The loss function is as follows:
ğ‘›ğ‘œğ‘Ÿğ‘š =Â©Â­
Â«âˆ‘ï¸
(ğ‘¡:ğ‘”)âˆˆBoW SE(ğ·)ğ‘”2ÂªÂ®
Â¬1
2
ğ‘™ğ‘œğ‘ ğ‘ ğ‘¡=CE(ğ‘…ğ‘¡,ğ‘™ğ‘ğ‘ğ‘’ğ‘™)+1
ğ‘£ğ‘›ğ‘œğ‘Ÿğ‘š(6)
where CEis the cross-entropy loss, ğ‘™ğ‘ğ‘ğ‘’ğ‘™ is the ground truth of
the training data. We can also encode the query as the synonym-
expansion BoW representation to improve the performance of recall.
The relevance score is as follows:
ğ¶=âˆ‘ï¸
(ğ‘¤:ğ‘)âˆˆBoW SE(ğ‘„)ğ‘
ğ‘…ğ‘ (ğ‘„,ğ·)=1
ğ¶âˆ‘ï¸
(ğ‘¤:ğ‘)âˆˆBoWSE(ğ‘„)
(ğ‘¡:ğ‘”)âˆˆBoWSE(ğ·)âˆ‘ï¸
ğ‘¤=ğ‘¡ğ‘Ã—ğ‘”(7)
We call our DeepBoW model with this relevance score as Deep-
BoW(Q -Synonym). Different from ğ‘™ğ‘œğ‘ ğ‘ ğ‘¡, we also leverage the bag-
of-words of the query to train the productâ€™s synonym-expansion
BoW representation. The loss should be modified as follows:
BoW avg(ğ‘†ğ‘¤):={ğ‘¤ğ‘–: 1/ğ‘›,ğ‘¤ğ‘–âˆˆğ‘†ğ‘¤}
ğ‘…ğ‘ğ‘£ğ‘”(ğ‘„,ğ·)=âˆ‘ï¸
(ğ‘¤:ğ‘)âˆˆBoW avg(ğ‘„)
(ğ‘¡:ğ‘”)âˆˆBoWSE(ğ·)âˆ‘ï¸
ğ‘¤=ğ‘¡ğ‘Ã—ğ‘”
ğ‘™ğ‘œğ‘ ğ‘ ğ‘Ÿ=CE(ğ‘…ğ‘Ÿ,ğ‘™ğ‘ğ‘ğ‘’ğ‘™)+CE ğ‘…ğ‘ğ‘£ğ‘”,ğ‘™ğ‘ğ‘ğ‘’ğ‘™+1
ğ‘£ğ‘›ğ‘œğ‘Ÿğ‘š(8)
For bothğ‘™ğ‘œğ‘ ğ‘ ğ‘¡andğ‘™ğ‘œğ‘ ğ‘ ğ‘ , we optimize the difference between the
queryâ€™s sparse BoW representation and the productâ€™s sparse BoW
representation. This can align the vocabulary between the query
and the product.
3.6 Online Deployment
Since most online search systems have strict latency limitations,
we pre-compute the sparse BoW representation of the queries andthe products offline. We discard the word-weight pairs in the sparse
BoW representation whose weights are lower than the given thresh-
old to optimize memory usage. We sort the word-weight pairs of-
fline by the word index. The relevance score of two sparse BoW
representations can be calculated by using the two-pointer algo-
rithm. Then the time complexity of Eq.5 and Eq.7 can be optimized
toO(ğ‘), which is much faster than the state-of-the-art deep rel-
evance model [ 33]. Although some deep relevance models with
cosine similarity can achieve comparable efficiency, the perfor-
mance of these models is much lower than our model as shown in
the section 4.5.
4 EXPERIMENTS
4.1 Dataset
There is no public dataset and benchmark for the Chinese e-commerce
relevance task, so we conduct experiments on three different types
of industrial datasets to learn the DeepBoW model. The first is a
large-scale Human-Annotation dataset which contains query-
product pairs sampled from the Taobao search logs. Each query-
product pair is labeled as Good (relevant) or Bad (irrelevant) by ex-
perienced human annotators. This is a daily task running in Taobao,
which has accumulated more than 8 million labeled samples. We
split the human-annotated datasets into training, validation and
test sets, as detailed in Table 1.
Dataset sample
query product Good Bad
T
rain 7,439,823
463,387 6,068,089 6,092,745 1,347,078
Valid 372,981
98,254 202,192 305,447 67,534
Test 984,175
134,564 883,691 745,524 238,651
Table 1: Statistic for human-annotation dataset.
The second dataset for training is built by knowledge distilla-
tion, similar to Yao et al . [33] . We leverage the training set of the
human-annotation dataset to finetune the StructBERT [ 30] model,
which results in an interaction-based teacher model with strong
performance. Then the teacher model predicts the relevance scores
of the large unlabeled query-product pairs sampled from the search
logs of Taobao within a year. This training dataset is denoted as
â€œSearch-Logsâ€ in Table 2. Third, we also sample click-through data
from search logs and investigate the performance of our model on
this training set. We denote this dataset as â€œClick-Throughâ€ in
Table 2. Although these are training datasets of different sources,
we all use the human-annotation validation and test dataset to
evaluate the model performance.
4.2 Training Details
We employ Transformer as both the character-based encoder and
the word-based encoder. We reduce the total 12-layer encoder to
improve efficiency. After balancing the effectiveness and efficiency,
our model adopts 2 layers that can still achieve competitive perfor-
mance. We select the top 50000 words as the vocabulary Vaccording
to the wordâ€™s frequency in corpora, and we also add another 10000
hashing tokens into the vocabulary for the OOV words and the
N-gram phrases.
5403Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
We use â€œ PyTorch â€ to implement our model and train the model
with Adam optimizer. The hyper-parameters of Adam optimizer
areğ›½1=0.9,ğ›½2=0.999,ğœ–=10âˆ’8and the learning rate is set to
0.0001. Query-document pairs are batched together by approximate
sequence length. Each training batch contains a set of sentence
pairs with about 50000 tokens. The hyper-parameters and the best
model are chosen from the experimental results on the validation
set. We train our model on 2 Tesla V100 GPU and it usually takes
about 3 days for the model to converge. The convergence is reached
when the ROC-AUC does not improve on the validation set.
4.3 Baseline
We explore the performance of DeepBoW(Q-Weight) and DeepBoW
(Q-Synonym) respectively. The main difference between the two
methods is for DeepBoW(Q-Synonym) we leverage the synonym-
expansion BoW representation to replace the term-weighting BoW
representation for the query. To reduce memory usage and compu-
tation, we truncate the BoW representation to make it sparse. There
are two methods to truncate the sparse BoW representation, one
is to keep the ğ‘˜largest words according to their respective values,
and the other is to discard the terms whose values are smaller than
the giving threshold.
In addition, we adopt several state-of-the-art methods for com-
parison. BERT [ 4], RoBERTa [ 12] and StructBERT [ 30] belong to
the interaction-based architecture which is also known as the cross-
encoder architecture. Siamese BERT [ 9], MASM [ 32], Poly-encoders
[8] and ReprBERT [ 33] belong to the two-tower architecture which
is also known as the bi-encoder architecture. Besides, we investigate
the performance of ReprBERT with cosine similarity score instead
of MLP for online computation of relevance from query/product
embeddings. For fair comparison, we also leverage the pre-trained
model like RoBERTa and StructBERT as the encoder of the two-
tower model. These models are also baselines in our experiment,
which denote as DSSM RoBERT
aand
DSSM StructBERT .
4.4
Evaluation Metrics
We evaluate our model on both offline and online metrics. In offline
evaluation, since the human annotation is binary, the task is evalu-
ated as a classification task. The Receiver Operator Characteristic
Area Under Curve (ROC-AUC) is widely adopted in text relevance
tasks [ 3,10,29]. Note that in the e-commerce relevance scenario,
most instances are positive and we are more concerned about neg-
ative instances. Therefore the PR-AUC used in this paper is the
negative PR-AUC that treats Bad as 1 and Good as 0 following Yao
et al. [32, 33]. This metric is denoted as Neg PR-AUC.
Besides, we also evaluate the different model complexity of pa-
rameters and online computation efficiency. The FLOPs / token
is computed according to Molchanov et al . [13] which shows the
floating-point operations per second (FLOPs) when there is only
1 token being considered. The plus sign separates the online and
offline calculation FLOPs, which means the former part of compu-
tation can be pre-computed offline. Memory indicates the online
memory overhead for storing pre-computed query and product
vectors where we use vector size for comparison. In online evalua-
tion, we use the rate of Good annotated by human annotators andthe number of transactions as the evaluation metrics. The query-
product pairs for human relevance judgment are randomly sampled
from the online search logs according to the amount of Page View
(PV) as the sample weight.
4.5 Results
Table 2 presents an evaluative comparison across various mod-
els. Our DeepBoW model demonstrates robust performance across
three different training sets. For human-annotation dataset, Struct-
BERT has the best performance, and interaction-based models out-
perform two-tower models. Unfortunately, it is infeasible to deploy
the interaction-based model in industrial system because of prohib-
itive computation and resource requirements. The ROC-AUC and
Neg PR-AUC of two-tower models are much lower than the pre-
trained model, because the human-annotation data is insufficient
and the pre-trained model can introduce extra knowledge. Nonethe-
less, our DeepBoW model still outperforms other two-tower models.
The data enhancement sampled from the search logs and labeled
by the teacher model can greatly improve the performance of two-
tower models. Our model achieves the best performance around
the two-tower models in search-logs training sets. Click-through
data is used to train the relevance model in some cases where
lack of human-annotation training data. The models trained on
the click-through data get weak performance. The main reason
is that the click-through data in e-commerce is much more noisy
and misleading, which is not only affected by the query-product
relevance but also by many factors including price, attractive titles,
or images. Even so, our model also performs better than the other
models since it explicitly encodes the semantics to the sparse bag-
of-words while the other models may capture the personalized
information beyond the textual feature.
Our proposed architecture can truncate the sparse BoW represen-
tation to reduce memory usage in the online search system. We can
either truncate the sparse BoW representation to the fixed length
or discard the word-weight pairs whose values are smaller than a
given threshold. Benefiting from the interpretability of our sparse
BoW representation, both truncation methods achieve competitive
performance and only produce a slight loss of performance. We
further explore the distribution of the sparse BoW representation
as case study in Appx. A.
Table 3 shows the parameter, computation and memory con-
sumption of each model. While our model has a considerable num-
ber of parameters since we project the dense vector into the vo-
cabulary (78.8% parameters about 126M come from WcandWwin
Eq.3), DeepBoW is the most efficient at online inference. Our model
only uses CPU to compute the relevance score while the other mod-
els need GPU to speed the inference. Table 4 shows the inference
time of each model. The experiments are performed on a local CPU
platform. We report the average inference time of the model to
score 1000 products per query. The results show that our model is
much faster than the ReprBERT [ 33] which has been deployed in
the Taobao search system.
4.6 Ablation Study
We perform the ablation study on the human-annotation dataset
to investigate the influence of different modules in our model. We
5404KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhe Lin, et al.
Model Human-Annotation Search-Logs Click-Through
ROC-AUC Neg PR-AUC ROC-AUC Neg PR-AUC ROC-AUC Neg PR-AUC
Interaction-Based Models:
BERT[4] 0.850 0.662 - - - -
RoBERTa[12] 0.906 0.692 - - - -
StructBERT[30] 0.923 0.721 - - - -
Two-Tower Models:
Siamese BERT 0.765 0.565 0.821 0.648 - -
MASM[32] 0.795 0.484 0.793 0.582 0.615 0.283
Poly-Encoder[8] 0.808 0.623 0.846 0.605 - -
DSSM RoBERT
a 0.873 0.673 - - - -
DSSM StructBERT 0.858 0.658 - - - -
ReprBERT [33] 0.832 0.543 0.894 0.702 0.798 0.521
ReprBERT +Cosine
Similarity 0.798 0.452 0.847 0.601 0.727 0.399
Ours:
DeepBoW(Q-Weight) 0.874 0.665 0.908 0.705 0.803 0.579
DeepBoW(Q-Weight) +128-
Trunc 0.865 0.645 0.899 0.698 0.787 0.566
DeepBoW(Q-Weight) +0.4-
Trunc 0.869 0.658 0.906 0.701 0.796 0.572
DeepBoW(Q-Synonym) 0.880 0.674 0.914 0.712 0.812 0.585
DeepBoW(Q-Synonym) +128-
Trunc 0.873 0.670 0.906 0.705 0.799 0.571
DeepBoW(Q-Synonym) +0.4-
Trunc 0.877 0.672 0.911 0.710 0.807 0.575
Table 2: Comparison results on test set. Best scores are in bold. +128-
Trunc means keeping 128 largest terms according to the value
of the word-weight pair. +0.4-
Trunc means discarding the terms that the value is smaller than 0.4. We only finetune the pre-trained
based models on the Human-Annotation dataset and do not evaluate these models in the other two training sets, since we
leverage StructBERT as teacher model to label the search-logs dataset. We do not evaluate the performance of MASM and
Poly-Encoder on Click-Through training dataset, because the two models do not converge on the other two training datasets.
Mo
del Params
FLOPs / Token Mem
BERT 101.2M
182M 0
RoBERTa 101.2M
182M 0
StructBERT 101.2M
182M 0
Siamese BER 101.2M
91M + 1.5K 768
MASM 76.8M
674K 640
Poly-Encoder 101.2M
182M + 97.5K 768
ReprBERT 30.6M 30.4M +
296K 256
Ours:
De
epBoW +128-
Trunc 33.4M
+ 126M 159.4M
+ 128 128
DeepBoW +0.4-
Trunc 33.4M
+ 126M 159.4M
+28 28/144
Table 3: The modelsâ€™ efficiency. The Params of our models
include the encoder and the final vocabulary projection ( Wc
andWwin Eq.3). The Mem of DeepBoW +0.4-
Trunc is the mean
value throughout the whole corpora (query/product).
investigate the performance of the 2-layer encoder and the 6-layer
encoder. Besides, we remove the word encoder and the character
encoder separately to show the importance of encoding at both
word-level and character-level. We also employ different capacities
of vocabulary and hashing tokens in the sparse BoW representation.
To further explore the sparsity of BoW representation, we remove
theğ‘™2norm from the loss function. This may lead to a sparsity
deterioration in the sparse BoW representation.Model Inference Time (ms)
StructBERT 321,408
ReprBERT 29,164
DeepBoW 0.732
Table 4: Inference time of different models that score 1000
query-product pairs.
Table 5 shows the results of the ablation study. We can see that
each module in our model does contribute to the overall perfor-
mance. The performance of our model has a significant deteriora-
tion if either the word encoder or the character encoder is removed.
Increasing the capacity of vocabulary or hashing tokens cannot
improve the modelâ€™s performance, because it may lead to insuffi-
cient training for each token. Optimizing our model without the ğ‘™2
norm loss can lead to slight performance decline. There is almost no
difference between the 2-layer and 6-layer encoders in our model.
4.7 Online Evaluation
Online A/B testing is also conducted to evaluate our DeepBoW
model, by replacing the online ReprBERT model with the DeepBoW
model for comparison. Both experiments take about 2.5% proportion
of Taobao search traffic, and the A/B testing lasts for a month. As
a result, DeepBoW improves the number of transactions by about
0.4% on average. The daily human annotation results show that
DeepBoW also improves the rate of relevance by 0.5%. Online A/B
5405Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
DeepBoW ROC-AUC Neg PR-AUC
w/ 2 layers 0.914 0.712
w/ 6 layers 0.911 0.717
w/ 10w vocab 0.907 0.701
w/ 5w hashing tokens 0.899 0.693
w/oğ‘™2norm loss 0.886 0.685
w/o word encoder 0.847 0.611
w/o character encoder 0.821 0.583
Table 5: Ablation study of components of DeepBoW
testing verifies the proposed DeepBoW is superior to previous
state-of-the-art models, and can achieve significant online profit
considering the extremely large traffic of Taobao every day.
Our DeepBoW model has already served the entire Taobao search
traffic. After pre-computing the representations of queries and
products, the online serving latency can be optimized to as low as
4ms on the distributed computing system with CPUs. This is much
faster than the previous online relevance serving model ReprBERT
[33] of 10ms with GPUs and can satisfy the extremely large traffic
of Taobao.
5 CONCLUSION AND FUTURE WORK
In this paper, we study an industrial task of measuring the seman-
tic relevance of queries and products. We propose the DeepBoW
relevance model, which is an efficient and interpretable relevance
architecture for Chinese e-commerce search system. Our model
encodes the query and product as a set of word-weight pairs, which
is called the sparse BoW representation. The model is evaluted
on three different training datasets, and the results show that our
model achieves promising performance and efficiency. The model
has been deployed in the Taobao search system.
In future work, we will explore integrating external knowledge
into the DeepBoW relevance model to improve the performance.
The proposed model can also be evaluated on datasets of other
language.
REFERENCES
[1]Yang Bai, Xiaoguang Li, Gang Wang, Chaoliang Zhang, Lifeng Shang, Jun Xu,
Zhaowei Wang, Fangshan Wang, and Qun Liu. 2020. SparTerm: Learning Term-
based Sparse Representation for Fast Text Retrieval. ArXiv abs/2010.00768 (2020).
https://api.semanticscholar.org/CorpusID:222125038
[2]David Carmel, Elad Haramaty, Arnon Lazerson, Liane Lewin-Eytan, and Yoelle
Maarek. 2020. Why Do People Buy Seemingly Irrelevant Items in Voice Product
Search? On the Relation between Product Relevance and Customer Satisfaction
in eCommerce. In Proceedings of the 13th International Conference on Web Search
and Data Mining (Houston, TX, USA) (WSDM â€™20). Association for Computing
Machinery, New York, NY, USA, 79â€“87. https://doi.org/10.1145/3336191.3371780
[3]Jesse Davis and Mark Goadrich. 2006. The relationship between Precision-
Recall and ROC curves. In Proceedings of the 23rd International Conference on
Machine Learning (Pittsburgh, Pennsylvania, USA) (ICML â€™06). Association for
Computing Machinery, New York, NY, USA, 233â€“240. https://doi.org/10.1145/
1143844.1143874
[4]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association
for Computational Linguistics, Minneapolis, Minnesota, 4171â€“4186. https://doi.
org/10.18653/v1/N19-1423
[5]Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance
Matching Model for Ad-hoc Retrieval. In Proceedings of the 25th ACM Internationalon Conference on Information and Knowledge Management (Indianapolis, Indiana,
USA) (CIKM â€™16). Association for Computing Machinery, New York, NY, USA,
55â€“64. https://doi.org/10.1145/2983323.2983769
[6]Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolu-
tional Neural Network Architectures for Matching Natural Language Sen-
tences. In Advances in Neural Information Processing Systems, Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.), Vol. 27. Cur-
ran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2014/file/
b9d487a30398d42ecff55c228ed5652b-Paper.pdf
[7]Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM International Conference on
Information & Knowledge Management (San Francisco, California, USA) (CIKM
â€™13). Association for Computing Machinery, New York, NY, USA, 2333â€“2338.
https://doi.org/10.1145/2505515.2505665
[8]Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019.
Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate
Multi-sentence Scoring. In International Conference on Learning Representations.
https://api.semanticscholar.org/CorpusID:210063976
[9]Jyun-Yu Jiang, Mingyang Zhang, Cheng Li, Michael Bendersky, Nadav Gol-
bandi, and Marc Najork. 2019. Semantic Text Matching for Long-Form Docu-
ments. In The World Wide Web Conference (San Francisco, CA, USA) (WWW
â€™19). Association for Computing Machinery, New York, NY, USA, 795â€“806.
https://doi.org/10.1145/3308558.3313707
[10] Yunjiang Jiang, Yue Shang, Rui Li, Wen-Yun Yang, Guoyu Tang, Chaoyi Ma, Yun
Xiao, and Eric Zhao. 2019. A unified neural network approach to e-commerce
relevance learning. In Proceedings of the 1st International Workshop on Deep
Learning Practice for High-Dimensional Sparse Data (Anchorage, Alaska) (DLP-
KDD â€™19). Association for Computing Machinery, New York, NY, USA, Article 10,
7 pages. https://doi.org/10.1145/3326937.3341259
[11] Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei
Li. 2019. Is Word Segmentation Necessary for Deep Learning of Chinese Rep-
resentations?. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, Anna Korhonen, David Traum, and LluÃ­s MÃ rquez
(Eds.). Association for Computational Linguistics, Florence, Italy, 3242â€“3252.
https://doi.org/10.18653/v1/P19-1314
[12] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).
https://api.semanticscholar.org/CorpusID:198953378
[13] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2017.
Pruning Convolutional Neural Networks for Resource Efficient Inference. In
International Conference on Learning Representations. https://openreview.net/
forum?id=SJGCiw5gl
[14] Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian (Allen) Ding,
Ankit Shingavi, Choon Hui Teo, Hao Gu, and Bing Yin. 2019. Semantic Product
Search. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD â€™19). Association
for Computing Machinery, New York, NY, USA, 2876â€“2885. https://doi.org/10.
1145/3292500.3330759
[15] Rodrigo Nogueira and Kyunghyun Cho. 2020. Passage Re-ranking with BERT.
arXiv:1901.04085 [cs.IR]
[16] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage
Document Ranking with BERT. arXiv:1910.14424 [cs.IR]
[17] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward. 2015.
Semantic Modelling with Long-Short-Term Memory for Information Retrieval.
arXiv:1412.6629 [cs.IR]
[18] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,
Xinying Song, and Rabab Ward. 2016. Deep sentence embedding using long
short-term memory networks: analysis and application to information retrieval.
IEEE/ACM Trans. Audio, Speech and Lang. Proc. 24, 4 (apr 2016), 694â€“707. https:
//doi.org/10.1109/TASLP.2016.2520371
[19] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng.
2016. Text matching as image recognition. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence (Phoenix, Arizona) (AAAIâ€™16). AAAI Press,
2793â€“2799.
[20] Ankur Parikh, Oscar TÃ¤ckstrÃ¶m, Dipanjan Das, and Jakob Uszkoreit. 2016. A
Decomposable Attention Model for Natural Language Inference. In Proceedings of
the 2016 Conference on Empirical Methods in Natural Language Processing, Jian Su,
Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Linguistics,
Austin, Texas, 2249â€“2255. https://doi.org/10.18653/v1/D16-1244
[21] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding
the Behaviors of BERT in Ranking. arXiv:1904.07531 [cs.IR]
[22] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing. Association for Computational
Linguistics. http://arxiv.org/abs/1908.10084
5406KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhe Lin, et al.
[23] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang,
Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics,
Hong Kong, China, 3982â€“3992. https://doi.org/10.18653/v1/D19-1410
[24] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu,
and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text
REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4,
1994 (NIST Special Publication, Vol. 500-225), Donna K. Harman (Ed.). National
Institute of Standards and Technology (NIST), 109â€“126. http://trec.nist.gov/pubs/
trec3/papers/city.ps.gz
[25] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and GrÃ©goire Mesnil. 2014.
A Latent Semantic Model with Convolutional-Pooling Structure for Informa-
tion Retrieval. In Proceedings of the 23rd ACM International Conference on Con-
ference on Information and Knowledge Management (Shanghai, China) (CIKM
â€™14). Association for Computing Machinery, New York, NY, USA, 101â€“110.
https://doi.org/10.1145/2661829.2661935
[26] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013.
Reasoning with neural tensor networks for knowledge base completion. In Pro-
ceedings of the 26th International Conference on Neural Information Processing
Systems - Volume 1 (Lake Tahoe, Nevada) (NIPSâ€™13). Curran Associates Inc., Red
Hook, NY, USA, 926â€“934.
[27] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna
Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of
Information Retrieval Models. In Thirty-fifth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview.
net/forum?id=wCu6T5xFjeJ
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),
Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[29] Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, and Xueqi Cheng.
2016. Match-SRNN: modeling the recursive matching structure with spatial
RNN. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial
Intelligence (New York, New York, USA) (IJCAIâ€™16). AAAI Press, 2922â€“2928.
[30] Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and
Luo Si. 2020. StructBERT: Incorporating Language Structures into Pre-training
for Deep Language Understanding. In International Conference on Learning Rep-
resentations. https://openreview.net/forum?id=BJgQ4lSFPH
[31] Rong Xiao, Jianhui Ji, Baoliang Cui, Haihong Tang, Wenwu Ou, Yanghua Xiao,
Jiwei Tan, and Xuan Ju. 2019. Weakly Supervised Co-Training of Query Rewritingand Semantic Matching for e-Commerce. In Proceedings of the Twelfth ACM Inter-
national Conference on Web Search and Data Mining (Melbourne VIC, Australia)
(WSDM â€™19). Association for Computing Machinery, New York, NY, USA, 402â€“410.
https://doi.org/10.1145/3289600.3291039
[32] Shaowei Yao, Jiwei Tan, Xi Chen, Keping Yang, Rong Xiao, Hongbo Deng, and
Xiaojun Wan. 2021. Learning a Product Relevance Model from Click-Through
Data in E-Commerce. In Proceedings of the Web Conference 2021 (Ljubljana, Slove-
nia) (WWW â€™21). Association for Computing Machinery, New York, NY, USA,
2890â€“2899. https://doi.org/10.1145/3442381.3450129
[33] Shaowei Yao, Jiwei Tan, Xi Chen, Juhao Zhang, Xiaoyi Zeng, and Keping Yang.
2022. ReprBERT: Distilling BERT to an Efficient Representation-Based Relevance
Model for E-Commerce. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD â€™22).
Association for Computing Machinery, New York, NY, USA, 4363â€“4371. https:
//doi.org/10.1145/3534678.3539090
[34] Hongchun Zhang, Tianyi Wang, Xiaonan Meng, and Yi Hu. 2019. Improving
Semantic Matching via Multi-Task Learning in E-Commerce. In eCOM@SIGIR.
https://api.semanticscholar.org/CorpusID:198120237
A CASE STUDY
Table 6 shows two examples of our DeepBoW model. Both query
and product are encoded to the synonym-expansion BoW repre-
sentation. The sparse BoW representation consists of a collection
of word-weight pairs, which can be regarded as the bag-of-words
with soft weight. The synonym-expansion representation can not
only capture the importance of the words in the original text,
but also incorporates pertinent synonymous terms. The relevance
score can be calculated by aggregating the matching terms of the
queryâ€™s/productâ€™s sparse BoW representation.
These two examples show that our proposed sparse BoW repre-
sentation has positive interpretability, signifying that the developer
can analyze bad cases from the online search system and implement
targeted optimizations. Furthermore, the developer can modify the
terms in the sparse BoW representation directly to achieve the ex-
pected result. In a word, our DeepBoW model surpasses other deep
relevance modeling approaches in terms of interpretability and flex-
ibility, thereby rendering it eminently suitable for the e-commerce
search system.
5407Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Cases
of the sparse BoW representation
Quer
y:å°é¦™é£è¿è¡£è£™
BoW SE(Query) : [(â€˜è¿è¡£è£™â€™, 0.30148), (â€˜é«˜çº§æ„Ÿâ€™, 0.25785), (â€˜å°é¦™é£â€™, 0.2277), (â€˜æ–°æ¬¾â€™, 0.21297)]
Product: é«˜çº§æ„Ÿç§‹å†¬è£…å°é¦™é£é•¿è¢–Vé¢†é’ˆç»‡è¿è¡£è£™é€šå‹¤å¤å¤èµ«æœ¬å°é»‘è£™æ‰“åº•è£™è¿è¡£è£™ 18-24å‘¨å²é’ˆç»‡å¸ƒçº¯è‰²é€šå‹¤Vé¢†é«˜è…°å¥—å¤´é»‘è‰²é…’
çº¢è‰²ç²‰çº¢è‰²å¸¸è§„å•ä»¶å¤å¤Aå­—è£™ç§‹å†¬2023å¹´ç§‹å­£é•¿è¢–ä¸­è£™é“¾æ¡
BoW SE(Product) : [(â€˜å“è´¨â€™, 1.0), (â€˜v é¢†â€™, 1.0), (â€˜ç§‹â€™, 0.99999), (â€˜ é«˜è…°â€™, 0.99999), (â€˜ é«˜çº§æ„Ÿâ€™, 0.99999), (â€˜ é€šå‹¤â€™, 0.99999), (â€˜ ç§‹å­£â€™, 0.99998), (â€˜ ç§‹å†¬â€™,
0.99998), (â€˜ ç§‹å†¬è£… â€™, 0.99998), (â€˜ é»‘è‰² â€™, 0.99995), (â€˜ èµ«æœ¬ â€™, 0.99993), (â€˜ å•ä»¶ â€™, 0.99993), (â€˜18â€™, 0.99992), (â€˜ ä¸­è£™ â€™, 0.99991), (â€˜ é’ˆç»‡å¸ƒ â€™, 0.9999), (â€˜ é•¿è¢– â€™,
0.99985), (â€˜ æ‰“åº•â€™, 0.99981), (â€˜ å¤å¤â€™, 0.9998), (â€˜ é»‘è£™â€™, 0.9998), (â€˜ å°é»‘è£™â€™, 0.99977), (â€˜ é’ˆç»‡â€™, 0.99972), (â€˜ çº¯è‰²â€™, 0.9997), (â€˜ å¸¸è§„â€™, 0.99968), (â€˜a å­—è£™â€™,
0.99961), (â€˜ é“¾æ¡â€™, 0.99957), (â€˜24â€™, 0.99954), (â€˜ å­—â€™, 0.99944), (â€˜ ç²‰çº¢â€™, 0.9994), (â€˜ æ–°æ¬¾â€™, 0.99934), (â€˜ ç²‰çº¢è‰²â€™, 0.9992), (â€˜ æ—¶å°šâ€™, 0.99897), (â€˜ æ°”è´¨â€™, 0.99862),
(â€˜æ‰“åº•è£™â€™, 0.99824), (â€˜ ç§‹æ¬¾â€™, 0.9981), (â€˜ å¥³å£«â€™, 0.99805), (â€˜ å¥³è£™â€™, 0.99726), (â€˜2023â€™, 0.99721), (â€˜ æ—©ç§‹â€™, 0.99687), (â€˜ å°é¦™é£â€™, 0.99657), (â€˜ æ˜¥ç§‹â€™, 0.99627),
(â€˜2023å¹´â€™, 0.99622), (â€˜ å¥³è£…â€™, 0.99621), (â€˜ å¥—å¤´ â€™, 0.99618), (â€˜ é…’çº¢è‰²â€™, 0.99616), (â€˜ é»‘è‰²è¿è¡£è£™ â€™, 0.99583), (â€˜ é’ˆç»‡è¿è¡£è£™ â€™, 0.99577), (â€˜ æ–°æ¬¾è¿è¡£è£™ â€™,
0.99536), (â€˜ è£…â€™, 0.99512), (â€˜ å†¬è£… â€™, 0.99502), (â€˜ ç§‹è£… â€™, 0.9946), (â€˜ è¿èº« â€™, 0.9943), (â€˜ ç§‹å†¬è¿è¡£è£™ â€™, 0.99379), (â€˜ é’ˆç»‡è£™ â€™, 0.99247), (â€˜ æ°”è´¨è¿è¡£è£™ â€™, 0.99237),
(â€˜18-24â€™, 0.99174), (â€˜ é…’çº¢ â€™, 0.99117), (â€˜ å¥³â€™, 0.99094), (â€˜ è¢–å­ â€™, 0.99014), (â€˜ è‡ªåŠ¨å……æ°”å« â€™, 0.98937), (â€˜ è½»ç†Ÿ â€™, 0.98916), (â€˜2019â€™, 0.98725), (â€˜ éŸ©ç‰ˆ â€™, 0.98703),
(â€˜æ˜¥å­£â€™, 0.98666), (â€˜ å¸ƒâ€™, 0.98649), (â€˜ å†…æ­â€™, 0.98606), (â€˜ é«˜è…°è¿è¡£è£™ â€™, 0.98587), (â€˜ é»‘â€™, 0.98563), (â€˜a å­—â€™, 0.98546), (â€˜ä¿®èº«â€™, 0.98471), (â€˜ è¿è¡£è£™ â€™, 0.98461),
(â€˜è£™å­ â€™, 0.98451), (â€˜ è£™â€™, 0.98445), (â€˜åˆç§‹ â€™, 0.98441), (â€˜ä¼‘é—²è¿è¡£è£™ â€™, 0.98398), (â€˜ æµè¡Œ â€™, 0.98365), (â€˜ çº¢è‰²è¿è¡£è£™ â€™, 0.98294), (â€˜ å®½æ¾è¿è¡£è£™ â€™, 0.9822),
(â€˜è¡£æœâ€™, 0.9794), (â€˜ æ˜¾ç˜¦â€™, 0.97937), (â€˜ ç§‹å†¬æ–°æ¬¾â€™, 0.97758), (â€˜ å†¬å­£â€™, 0.97752), (â€˜ å¥³è¿è¡£è£™ â€™, 0.97674), (â€˜ ä¸ªå­â€™, 0.97423), (â€˜ çº¢è‰²è£™å­â€™, 0.9712), (â€˜å
åª›â€™, 0.96755), (â€˜ååª›è¿è¡£è£™ â€™, 0.96739), (â€˜ å¥³ç”Ÿ â€™, 0.96676), (â€˜ æ‰“åº•è¡« â€™, 0.96633), (â€˜ è¿èº«è£™ â€™, 0.9661), (â€˜ æ´‹æ°” â€™, 0.96411), (â€˜ååª›æ°”è´¨è¿è¡£è£™ â€™, 0.96329),
(â€˜ä¿®èº«è¿è¡£è£™ â€™, 0.96265), (â€˜ è¡£â€™, 0.96137), (â€˜ æ‰“åº•è¿è¡£è£™ â€™, 0.95868), (â€˜ å†¬æ¬¾â€™, 0.95856), (â€˜ æ¯›è¡£è£™ â€™, 0.95822), (â€˜ é•¿è¢–è¿è¡£è£™ â€™, 0.95057), (â€˜ è¿è¡£è£™å­â€™,
0.95046), (â€˜æ­â€™, 0.94854), (â€˜ç§‹å†¬è£™å­â€™, 0.94758), (â€˜ä¼‘é—²â€™, 0.94602), (â€˜è®¾è®¡æ„Ÿâ€™, 0.94598)]
Relevance Score: 0.30148Ã—0.98461+0.25785Ã—0.9999+0.2277Ã—0.99657+0.21297Ã—0.99934 =0.9944
Quer
y:ç§‹å†¬åºŠä¸Šå››ä»¶å¥—
BoW SE(Query) : [(â€˜å››ä»¶â€™, â€™0.343â€™), (â€˜åºŠä¸Šâ€™, â€™0.13778â€™), (â€˜å¥—â€™, â€™0.06248â€™), (â€˜ç§‹å†¬â€™, â€™0.09616â€™), (â€˜åºŠä¸Šå››ä»¶å¥—â€™, â€™0.10872â€™), (â€˜å››ä»¶å¥—â€™, â€™0.202â€™)]
Product: ç§‹å†¬å­£åŠ åšç‰›å¥¶ç»’åºŠä¸Šå››ä»¶å¥—çŠç‘šç»’è¢«å¥—åŒé¢æ³•å…°ç»’åŠ ç»’åºŠå•ä¸‰ä»¶å¥—å¡ä¸è¿ªå°”å®¶çººæ——èˆ°åº—åºŠå“å¥—ä»¶/å››ä»¶å¥—/å¤šä»¶å¥—Kiss
Dear/å¡ä¸è¿ªå°”è“„çƒ­ä¿æš–ç‰›å¥¶ç»’å¡é€šåŠ¨æ¼«å¡é€šä¸­å›½å¤§é™†2021å¹´ç§‹å­£åºŠå•å¼åºŠç¬ å¼å¤§ä¼—
BoW SE(Product) : [(â€˜çŠç‘šç»’è¢«å¥—â€™, 1.0), (â€˜çŠç‘šç»’â€™, 1.0), (â€˜æ±Ÿè‹â€™, 1.0), (â€˜2021 å¹´â€™, 1.0), (â€˜å¤§é™†â€™, 0.99999), (â€˜ å¤šä»¶å¥—â€™, 0.99999), (â€˜ è“„çƒ­â€™, 0.99999),
(â€˜åºŠç¬ å¼â€™, 0.99998), (â€˜ å†¬å­£â€™, 0.99997), (â€˜ æ±Ÿè‹çœâ€™, 0.99997), (â€˜ ç§‹å†¬å­£â€™, 0.99993), (â€˜ åºŠå“â€™, 0.99988), (â€˜ ä¸‰ä»¶å¥—â€™, 0.99979), (â€˜ä¿æš–â€™, 0.99979), (â€˜ åºŠä¸Šâ€™,
0.99979), (â€˜åŠ åšâ€™, 0.99975), (â€˜è¢«å¥—â€™, 0.99975), (â€˜å››ä»¶â€™, 0.99965), (â€˜å—é€šâ€™, 0.99955), (â€˜åºŠå“å¥—ä»¶â€™, 0.99948), (â€˜å¥—â€™, 0.99926), (â€˜ä»¶å¥—â€™, 0.99919), (â€˜ç‰›å¥¶â€™,
0.99906), (â€˜ ç§‹â€™, 0.99891), (â€˜ åºŠå•ä¸‰ä»¶å¥— â€™, 0.9984), (â€˜ å¡é€š â€™, 0.99832), (â€˜åŠ¨æ¼« â€™, 0.9981), (â€˜dearâ€™, 0.99762), (â€˜ åºŠä¸Šä¸‰ä»¶å¥— â€™, 0.99754), (â€˜ åŒé¢ â€™, 0.99744),
(â€˜åºŠä¸Šå››ä»¶å¥— â€™, 0.99703), (â€˜åŠ ç»’ â€™, 0.99627), (â€˜ å¥—ä»¶ â€™, 0.99615), (â€˜kissâ€™, 0.99586), (â€˜ åºŠå• â€™, 0.9952), (â€˜ çŠç‘š â€™, 0.99498), (â€˜ è‡ªåŠ¨å……æ°”å« â€™, 0.9935), (â€˜ åºŠå•å››
ä»¶å¥— â€™, 0.98934), (â€˜ è¢«ç½©â€™, 0.98758), (â€˜ åºŠç¬  â€™, 0.98725), (â€˜2021â€™, 0.98496), (â€˜ åºŠç›–â€™, 0.978), (â€˜ åŒé¢ç»’ â€™, 0.96437), (â€˜ æ¯›ç»’ â€™, 0.95704), (â€˜ ç§‹å­£ â€™, 0.94864), (â€˜ å†¬
å¤©â€™, 0.93963), (â€˜ æ¯›æ¯› â€™, 0.9209), (â€˜ ç§‹å†¬â€™, 0.90725), (â€˜ ç”¨å“â€™, 0.89087), (â€˜ å››ä»¶å¥—â€™, 0.88608), (â€˜ å„¿ç«¥å››ä»¶å¥—â€™, 0.87247), (â€˜ çŠç‘šç»’åºŠå•â€™, 0.86876), (â€˜ å†¬â€™,
0.85713), (â€˜ å†¬æ¬¾â€™, 0.85588), (â€˜ åºŠå“å››ä»¶å¥—â€™, 0.82923), (â€˜ å•äººåºŠâ€™, 0.82689), (â€˜ ç»’é¢â€™, 0.82627), (â€˜ å•äººâ€™, 0.81626), (â€˜ æ½®ç‰Œâ€™, 0.78414), (â€˜ ç»’â€™, 0.77402), (â€˜ å®¿
èˆâ€™, 0.76135), (â€˜ å°‘å¥³ â€™, 0.6812), (â€˜ç®€çº¦ â€™, 0.60279), (â€˜ è¢«å­ â€™, 0.59161), (â€˜ å®¶çºº â€™, 0.56128), (â€˜ ç§‹å†¬æ¬¾ â€™, 0.55233), (â€˜â€™, 0.54342), (â€˜ æ——èˆ°åº— â€™, 0.54124), (â€˜ å†¬è£… â€™,
0.52361), (â€˜ åšâ€™, 0.51875), (â€˜ é«˜æ¡£ â€™, 0.51707), (â€˜ å•ä»¶ â€™, 0.51121), (â€˜ è–„ç»’ â€™, 0.50859), (â€˜ å¡é€šå››ä»¶å¥— â€™, 0.48766), (â€˜ åºŠä¸Šç”¨å“ â€™, 0.48612), (â€˜ æ•å¥— â€™, 0.46126),
(â€˜åŒäººâ€™, 0.45578), (â€˜4 ä»¶å¥—â€™, 0.44235), (â€˜ å¥—è£…â€™, 0.43786), (â€˜ä¿æš–è¡£â€™, 0.42136), (â€˜ å…¨å¥—â€™, 0.40858), (â€˜ç¡â€™, 0.39326), (â€˜ çŠç‘šç»’ç¡è¡£â€™, 0.38609), (â€˜ åºŠå¥—â€™,
0.35582), (â€˜ ä¸ç»’ â€™, 0.34883), (â€˜åŠ ç»’è£¤ â€™, 0.32625), (â€˜1.5â€™, 0.30884), (â€˜ å­¦ç”Ÿ â€™, 0.29069), (â€˜ ç¥å™¨ â€™, 0.28852), (â€˜ çº¢è‰²ç³» â€™, 0.28552), (â€˜ ç½‘çº¢ â€™, 0.28548), (â€˜ æ¯›èŒ¸
èŒ¸â€™, 0.28526), (â€˜å¿…å¤‡â€™, 0.28513), (â€˜è£…é¥°å“â€™, 0.28248), (â€˜å†¬å­£ç¡è¡£â€™, 0.27353), (â€˜åŠ åšå¤–å¥—â€™, 0.26865), (â€˜å®¶å±…â€™, 0.26277), (â€˜åºŠâ€™, 0.25459)]
Relevance Score: 0.343Ã—0.99965+0.13778Ã—0.99979+0.09616Ã—0.90725+0.10872Ã—0.99703+0.202Ã—0.88608 =0.8553
Table 6: Two examples of the DeepBoW model. Both the query and the product are encoded to the synonym-expansion BoW representation.
The relevance score can be calculated as shown in the table.
5408