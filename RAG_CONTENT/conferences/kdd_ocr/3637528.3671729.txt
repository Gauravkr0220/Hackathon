Robust Auto-Bidding Strategies for Online Advertising
Qilong Lin
Shanghai Jiao Tong University
Shanghai, China
edersnow@sjtu.edu.cnZhenzhe Zhengâˆ—
Shanghai Jiao Tong University
Shanghai, China
zhengzhenzhe@sjtu.edu.cnFan Wu
Shanghai Jiao Tong University
Shanghai, China
fwu@cs.sjtu.edu.cn
ABSTRACT
In online advertising, existing auto-bidding strategies for bid shad-
ing mainly adopt the approach of first predicting the winning price
distribution and then calculating the optimal bid. However, the
winning price information available to the Demand Side Platforms
(DSPs) is extremely limited, and the associated uncertainties make
it challenging for DSPs to accurately estimate winning price distri-
bution. To address this challenge, we conducted a comprehensive
analysis of the process by which DSPs obtain winning price infor-
mation, and abstracted two types of uncertainties from it: known
uncertainty and unknown uncertainty. Based on these uncertain-
ties, we proposed two levels of robust bidding strategies: Robust
Bidding for Censorship (RBC) and Robust Bidding for Distribution
Shift (RBDS), which offer guarantees for the surplus in the worst-
case scenarios under uncertain conditions. Experimental results
on public datasets demonstrate that our robust bidding strategies
consistently enable DSPs to achieve superior surpluses, both on
test sets and under worst-case conditions.
CCS CONCEPTS
â€¢Applied computing â†’Online auctions; â€¢Information sys-
temsâ†’Display advertising; â€¢Mathematics of computing â†’
Mathematical optimization.
KEYWORDS
Auto-Bidding, Bid Shading, Robust Optimization
ACM Reference Format:
Qilong Lin, Zhenzhe Zheng, and Fan Wu. 2024. Robust Auto-Bidding Strate-
gies for Online Advertising. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671729
1 INTRODUCTION
In recent years, the landscape of auto-bidding in online advertising
has undergone a significant transformation, with the sale of vast
quantities of ad impressions shifting from the traditional second-
price auction to the first-price auction [ 8,26,33]. This shift has
âˆ—Z.Zheng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671729altered the bidding strategies of the auction participants, namely
Demand Side Platforms (DSPs), and has given rise to the issue of
bid shading [32, 34, 35] in the first-price auction context.
From the perspective of auction theory, in contrast to traditional
second-price auctions, first-price auctions do not possess the incen-
tive compatibility property [ 17,18]. For DSPs, this indicates that
truthfully revealing their value of winning the auction may not
necessarily yield the maximal surplus for themselves. Therefore,
DSPs need to develop bidding strategies tailored to the first-price
auction environment in order to maximize their own surplus.
A natural bidding approach in first-price auction is firstly pre-
dicting the distribution of winning price for each auction, where
winning price is the minimal bid that could win the auction. Based
on this distribution, one can solve for the bid that maximizes the
expected surplus. This approach has been adopted by many works
on bid shading, and has led to the development of specialized works
that focus on the prediction of winning prices [19, 30, 31].
However, the uncertainty of winning price is a substantial chal-
lenge. In reality, the real distribution of winning prices is unattain-
able, and we can only estimate it from the sampled winning price
data within each auction. Moreover, a significant issue for DSPs
is that they can only access partial information from this sampled
data. For instance, a DSP may know the exact winning price only
upon winning an auction, whereas in the cases of not winning,
DSP would merely know that winning price is higher than her bid.
Additionally, the proportion of auctions that a DSP wins consti-
tutes a minor segment of all auctions. This dilemma is commonly
referred to as the censorship problem [ 2,27,32]. The impediments
in accessing winning price information result in uncertainties that
cannot be disregarded in the modeling of winning price.
Existing research endeavors to predict the distribution of win-
ning prices in the context of censorship, which inherently necessi-
tates the introduction of certain assumptions as criteria for evalu-
ating the quality of the predicted winning price distributions. For
example, the method of censored regression assumes that the distri-
bution of winning prices should remain consistent across auctions
that a DSP loses and those her wins. Such an assumption may
not correspond to actual conditions [ 32], thereby introducing an
intrinsic bias into the predicted distribution.
In our work, to address the challenges posed by uncertainties,
we propose two levels robust bidding strategies. More specifically,
we categorize the uncertainty within the winning price into two
levels: known uncertainty and unknown uncertainty, corresponding
respectively to the issue of censorship and the sampling process
of winning price. These two types of uncertainties collectively
describe the limited information on the winning price distribution
that the DSP can obtain.
Given that both types of uncertainties pertain to winning price
distribution, We draw inspiration from the idea of Distributionally
 
1804
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qilong Lin, Zhenzhe Zheng, & Fan Wu
Robust Optimization (DRO) [ 5,16,21] and model such uncertain-
ties on distribution by the concept of ambiguity sets. Subsequently,
we solve the distributionally robust optimization problem to select
the optimal bids. Following this approach, we propose two levels
robust bidding strategiesâ€”Robust Bidding for Censorship (RBC)
and Robust Bidding for Distributional Shift (RBDS)â€”for known
and unknown uncertainties, respectively, and design correspond-
ing algorithms to solve the bidding problem. Such robust bidding
strategies aim to optimize the worst-case surplus, thereby providing
lower bound guarantees for the DSPâ€™s surplus in first-price auction
environments, where the winning price is fraught with uncertainty.
Our contributions in this work can be summarized as follows:
â€¢We model the uncertainties of winning price, and propose
the corresponding distributionally robust bidding strategies.
Our robust strategy aims to optimize the worst-case surplus,
thereby achieving advanced performance in the bidding en-
vironments characterized by significant uncertainty.
â€¢From the technical perspective, we designed the construction
of the ambiguity set and the algorithm for the RBC and RBDS
strategies. This approach can provide insights into solving
specialized distributionally robust optimization problems
under discrete distribution scenarios.
â€¢We conduct comprehensive experiments on public datasets.
The experimental results show that our robust bidding strate-
gies outperform the existing strategies, especially exhibiting
better performance in the worst-case situations.
The rest of this work is organized as follows. In Section 2, we pro-
vide an overview of the existing works on bid shading and robust
optimization. In Section 3, we conduct a detailed analysis of the un-
certainty in bid shading, and introduce the robust bidding problem.
In Section 4, in correspondence to the two types of uncertainties
abstracted in the analysis, we specifically propose two levels robust
bidding strategies. In Section 5, we present the experimental results
on public datasets, and demonstrate the effectiveness of our robust
strategies. Finally, in Section 6, we briefly summarize the content
of this work and potential future works.
2 RELATED WORK
In this section, we provide an overview of the existing auto-bidding
strategies in bid shading, and introduce the closely related series of
works on winning price prediction. Additionally, we briefly review
the relevant works on distributionally robust optimization, which
is the primary technique employed in this work.
2.1 Bid Shading
In bid shading, mainstream works adopt the idea of distribution-
based bidding strategies. According to whether the distribution type
of winning price is pre-assumed, we can roughly divide existing
works into two categories. One kind of work assumes that the
winning price follows a certain type of distribution, which is called
the parametric method. Existing work has tried various distribution
types to model winning price distribution [ 11,20,29]. Among them,
[35] compares several basic distribution types, and finds that the
lognormal distribution fits best in actual business.
Although the research on the parametric method is comprehen-
sive, the actual winning price distribution contains lots of detailedinformation, which is difficult to be described by a certain type of
distribution. Based on this observation, another series of works
tries to directly fit the original distribution. This type of method
is called the non-parametric method. [ 34] uses a table-based algo-
rithm to record historical surplus and make bidding decisions based
on these records. At present, the main discussion of non-parametric
methods lies in the series of works about winning price prediction.
2.2 Winning Price Prediction
To deal with censorship problem, a series of works focused on the
winning price prediction model. [ 32] is the first to consider the
censorship problem in distribution prediction. In their work, they
introduce the traditional censored regression method and model
the winning price prediction as a linear fitting problem with normal
noise. [ 7] further models winning price as a mixture distribution
and uses the mixture density network corresponding. [ 14] focuses
on the design of feature engineering, and improves the loss function
of the network. Besides these works related to censored regression,
there is also a series of works that use survival analysis to deal with
censorship problem. [ 30] is the first to introduce survival analysis
into winning price prediction, and they consider combining it with
decision trees to apply to bid shading problem. [ 22,28] further
consider combining survival analysis with recurrent neural network
and Markov network. In general, this series of works gradually
develops towards complex non-parametric methods.
2.3 Distributionally Robust Optimization
Distribution robust optimization is a burgeoning robust optimiza-
tion method that we mainly refer to. Its principal idea is to opti-
mize the worst-case performance within an uncertain environment,
where the optimized variables are distributions. This method can
probably be traced back to a study on the inventory problem [ 24],
and is widely known as distributionally robust optimization after
a more recent study on moment uncertainty [ 4]. Nowadays, this
optimization idea has been applied to various scenarios, such as
machine learning [13] and auction mechanism design [12].
In our work, since the bidding decision function of bid shading is
not a convex function, we need to consider non-convex distribution-
ally robust optimization. Recently, [ 10] and [ 9] have expanded the
inner optimization problem in DRO to rewrite the DRO problem as
a stochastic optimization problem. With this transformation, they
can naturally use existing algorithms to solve the original DRO
problem. However, the number of studies on this topic is small and
further exploration is still needed.
3 PRELIMINARIES
In this section, we conduct a comprehensive analysis of the process
by which DSPs acquire winning price information, and abstract two
types of uncertainties from it: known uncertainty and unknown
uncertainty. Based on this uncertain environment, we introduce
the robust bidding problem, which encapsulates the objectives of
robust strategies in the next section.
3.1 Uncertain Bidding Environment
The environment we consider is the context of first-price auction, in
which a DSP aims to devise a bidding strategy that could maximize
 
1805Robust Auto-Bidding Strategies for Online Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
her surplus. Formally speaking, from the DSPâ€™s perspective, we
define her private value as ğ‘£, and she can submit a bid ğ‘âˆˆB to the
auction platform to compete for the advertisement slot, where B
denote the discrete bidding space with size ğ‘€. Letğ‘¤be the winning
price of the auction. The DSP wins the auction when her bid ğ‘is
greater than or equal to ğ‘¤, at which point she incurs a cost equal to
her bidğ‘, and gets the surplus ğ‘£âˆ’ğ‘. If DSP does not win the auction,
her surplus is 0. Hence, the objective of DSP can be expressed as
optimizing her surplus:
â„(ğ‘,ğ‘¤)=(ğ‘£âˆ’ğ‘)Â·I{ğ‘â‰¥ğ‘¤}, (1)
where I{ğ‘â‰¥ğ‘¤}indicates whether DSP wins or not, and it con-
verts the logical judgment result to 0 or 1 accordingly. In practical
business scenarios, the private value ğ‘£is generally given. Hence,
similar to other bid shading studies, we focus our attention on the
modeling of winning price ğ‘¤.
To model winning price, we analyze its origins and the associated
uncertainty issues from the perspective of DSP. First, we assume
that winning price ğ‘¤follows a distribution ğ’‘. In this work, for
ease of discussion, we utilize this kind of probability vector ğ’‘to
discretely represent the winning price distribution. Specifically, we
defineğ‘0=0, and set the bids in Bto satisfyğ‘0<ğ‘1<...<ğ‘ğ‘€and
form an arithmetic sequence. Consequently, the probability vector
can be denoted as ğ’‘=(ğ‘1,ğ‘2,...,ğ‘ğ‘€), where for any ğ‘—âˆˆ[1,ğ‘€],ğ‘ğ‘—
corresponds to the probability of the winning price being in the
interval[ğ‘ğ‘—âˆ’1,ğ‘ğ‘—).
In reality, the DSP cannot directly access the real winning price
distribution ğ’‘. Instead, it indirectly acquires information about this
distribution through the following process:
â€¢Sampling. Historical auctions contain sampled winning
price data{Ë†ğ‘¤ğ‘–|ğ‘–âˆˆA} , whereğ‘–is used to identify different
auctions in historical auctions set A. These sampled data
constitutes an sampling distribution Ë†ğ’‘=FB({Ë†ğ‘¤ğ‘–|ğ‘–âˆˆA}) .
In our work, we use the function FBto denote the function
that map the dataset to a distribution Ë†ğ’‘onB.
â€¢Censorship. DSP can only obtain the sampled winning price
{Ë†ğ‘¤ğ‘–|ğ‘–âˆˆW} when her wins, where Wis the set of auctions
DSP won. For those losing auctions ğ‘–âˆˆL=A\W , DSP
normally only knows that the sampled winning price exceeds
her own bid. We abstract the situation of these auctions as
cases where the DSP only knows the interval [Ë†ğ‘™ğ‘–,Ë†ğ‘Ÿğ‘–]in which
Ë†ğ‘¤ğ‘–is located.
Here, the censorship problem has already been widely discussed
in related work [ 32], and we further explain the meaning of the
interval[Ë†ğ‘™ğ‘–,Ë†ğ‘Ÿğ‘–]in it. In the most general case, when DSP does not
win the auction, she only knows that the winning price is higher
than her bid, so the left end of the interval Ë†ğ‘™ğ‘–is DSPâ€™s bid in that
auction, and the right end Ë†ğ‘Ÿğ‘–is infinite. However, some studies have
shown that the interval in which the winning price resides can be
empirically narrowed down [ 14]. Therefore, in order to make the
modeling of our problem more universal, we abstract the accessible
information when the DSP does not win as the winning price being
within a known interval Ë†ğ‘¤ğ‘–âˆˆ[Ë†ğ‘™ğ‘–,Ë†ğ‘Ÿğ‘–].
Since the sampling and censorship process introduces consid-
erable uncertainty to the bid shading problem, for further robust
Figure 1: The idea of uncertainty analysis and distribution-
ally robust optimization
strategies design, we divide the uncertainty into two types, corre-
sponding to the sampling and censorship process respectively:
â€¢Known uncertainty. For any losing auction ğ‘–âˆˆL, the
actual winning price Ë†ğ‘¤ğ‘–could be any value within the in-
terval[Ë†ğ‘™ğ‘–,Ë†ğ‘Ÿğ‘–]. Since the sampling distribution is comprised
of winning price data from the dataset, DSP can determine
that the sampling distribution Ë†ğ’‘ğ‘–belongs to an ambiguity set
Pğ‘˜ğ‘›, where the uncertainty can be specified by the intervals.
â€¢Unknown uncertainty. The difference between the sam-
pling distribution Ë†ğ’‘and the real distribution ğ’‘, as well as
the potential changes of the real distribution ğ’‘over time,
constitutes the unknown uncertainty that DSP cannot be
certain about due to the limited information. Corresponding
to this uncertainty, we denote the ambiguity set in which
the real distribution ğ’‘could resides asPğ‘¢ğ‘›.
The term â€œknown uncertaintyâ€ here refers to the situation where
DSP is aware of the set Pğ‘˜ğ‘›but does not know which specific el-
ement from the set is the actual sampling distribution Ë†ğ‘ƒ. In other
words, the DSP knows the range of possible distributions but can-
not pinpoint the exact one within that range. On the other hand,
â€œunknown uncertaintyâ€ implies that DSP cannot even accurately
grasp or define the ambiguity set Pğ‘¢ğ‘›itself.
3.2 Robust Bidding Strategies
In order to achieve the satisfied surplus of DSP in uncertain en-
vironments, we introduce the robust optimization [ 1] into the de-
sign of bidding strategies. Specifically, since uncertainty in bidding
problem arising from the winning price distribution, the bidding
decision can be formulated as the following distributionally robust
optimization problem:
max
ğ‘âˆˆBmin
ğ’‘âˆˆPEğ‘¤âˆ¼ğ’‘[â„(ğ‘,ğ‘¤)], (2)
where the decision function â„is defined in (1). This minimax op-
timization process can be understood as follows: as the real dis-
tribution could be any element within the ambiguity set P, for
each bidğ‘, there exists a range for the expected surplus obtained
by DSP, and we can denote its lower and upper bound as ğ¿andğ‘ˆ
respectively. In this case, distributionally robust hopes to take this
uncertainty into account and chooses the bid ğ‘ğ‘—that maximizes
the minimal surplus ğ¿ğ‘—. Figure 1 provides a visual representation
of this process, where we adopt the known uncertainty and the
corresponding ambiguity set P=Pğ‘˜ğ‘›for illustrating.
To facilitate subsequent discussions, we denote the cumulative
distribution function (CDF) corresponding to the distribution ğ’‘
 
1806KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qilong Lin, Zhenzhe Zheng, & Fan Wu
asğ‘ƒğ’‘, and present a more tractable form for the distributionally
robust optimization problem (2):
max
ğ‘âˆˆBmin
ğ’‘âˆˆP(ğ‘£âˆ’ğ‘)Â·ğ‘ƒğ’‘(ğ‘). (3)
The distributionally robust problem (2) and (3), along with the am-
biguity setPğ‘˜ğ‘›andPğ‘¢ğ‘›considered within it, abstractly formulates
the mathematical problem in bid shading, and provides a general
description of the objective for our robust strategies design. Sub-
sequently, we will further elaborate on the design details of our
robust bidding strategies.
4 DESIGN
In this section, we further elaborate on the design of robust bidding
strategies. In response to the two types of uncertainties summa-
rized in the previous section, we propose two levels robust bidding
strategies: Robust Bidding for Censorship and Robust Bidding for
Distribution Shift, and further provide solutions for the robust
optimization problems within these strategies respectively.
4.1 Robust Bidding for Censorship
We first discuss the design of robust bidding strategy considering
only the known uncertainty. We refer to this as the Robust Bidding
for Censorship (RBC), which addresses a problem of the same form
as (2), but with the ambiguity set defined as follows:
Pğ‘˜ğ‘›={FB({Ë†ğ‘¤ğ‘–|ğ‘–âˆˆW}âˆª{ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆˆ[Ë†ğ‘™ğ‘–,Ë†ğ‘Ÿğ‘–],ğ‘–âˆˆL})},(4)
where we use the sampled winning price set {Ë†ğ‘¤ğ‘–}to represent the
distribution. In this set, the data for subscript ğ‘–âˆˆW is known,
while the data for subscript ğ‘–âˆˆL is uncertain, where the data ğ‘¤ğ‘–
can take any value within the interval [Ë†ğ‘™ğ‘–,Ë†ğ‘Ÿğ‘–].
Problem (2) with the ambiguity set Pğ‘˜ğ‘›appears complex, but it
can be transformed into a simple maximization problem, which is
tractable to some extent. We arrive at the following conclusion:
Remark 4.1. Given the ambiguity set Pğ‘˜ğ‘›, problem (2) is equiva-
lent to the following problem:
max
ğ‘âˆˆBEğ‘¤âˆ¼ğ’‘0[â„(ğ‘,ğ‘¤)], (5)
with the distribution ğ’‘0=Ë†ğ’‘ğ‘, where Ë†ğ’‘ğ‘âˆˆPğ‘˜ğ‘›represents the worst-
case winning price distribution, and it can be expressed as:
Ë†ğ’‘ğ‘=FB({Ë†ğ‘¤ğ‘–|ğ‘–âˆˆW}âˆª{ Ë†ğ‘Ÿğ‘–|ğ‘–âˆˆL}). (6)
This result is quite intuitive. Similar to the equivalence between
problems (2) and (3), problem (5) can be equivalently transformed
into the following problem:
max
ğ‘âˆˆB(ğ‘£âˆ’ğ‘)Â·ğ‘ƒğ’‘0(ğ‘). (7)
By analyzing problem (3), for any given bid ğ‘, the worst-case sce-
nario corresponds to the smallest cumulative probability ğ‘ƒË†ğ’‘ğ‘(ğ‘).
Within the ambiguity set Pğ‘˜ğ‘›, the sampled data in Ë†ğ’‘ğ‘should be as
large as possible, meaning that for each auction ğ‘–âˆˆL, selecting Ë†ğ‘Ÿğ‘–
to form the sampling distribution Ë†ğ’‘ğ‘. Thus problem (3) with the
ambiguity setPğ‘˜ğ‘›is equivalent to problem (7) with the worst-case
distribution Ë†ğ’‘ğ‘. A detailed proof can be found in Appendix A.1.
In fact, problem (5) is a highly versatile optimization problem,
which can be used in conjunction with the winning price distribu-
tion prediction model to formulate a bidding strategy, and we referto this expectation-based optimization as Stochastic Optimization
(SO) in the remaining parts. Broadly speaking, the crux of solving
problem (5) lies in obtaining the distribution ğ’‘0that the winning
priceğ‘¤follows. In existing work, ğ’‘0is the estimated winning price
distribution, which can be predicted using existing methods, such
as the works presented in Section 2.2; in our RBC strategy, ğ’‘0is
the worst-case distribution, but it can also be predicted by emu-
lating existing methods. The detailed approach of predicting the
worst-case distribution is as follows.
Considering that DSP can obtain feature data Ë†ğ’™ğ‘–for each auc-
tion, we design a non-parametric distribution estimation model,
denoted asğ‘“ğ‘, which employs a simple two-layer fully connected
network and utilizes the Softmax function to process the outputs.
This network maps the input feature data Ë†ğ’™ğ‘–to a discrete distri-
bution Ë†ğ’‘ğ‘–=ğ‘“ğ‘(Ë†ğ’™ğ‘–), whereby the output dimension corresponds to
the dimension ğ‘€of the discrete distribution, with the i-th output
corresponding to the value of Ë†ğ‘ğ‘€
ğ‘–. This distribution Ë†ğ’‘ğ‘–estimated
will serve as the worst-case distribution ğ’‘0=Ë†ğ’‘ğ‘within the SO
problem (5) to obtain the robust bids. We denote the probability
density function (PDF) associated with the output distribution Ë†ğ’‘ğ‘–
asğ‘Ë†ğ’‘ğ‘–, and thus the loss function utilizing the concept of maximum
likelihood can be written as:
ğ¿ğ‘ =âˆ’âˆ‘ï¸
ğ‘–âˆˆWlogğ‘Ë†ğ’‘ğ‘–(Ë†ğ‘¤ğ‘–)âˆ’âˆ‘ï¸
ğ‘–âˆˆLlogğ‘Ë†ğ’‘ğ‘–(Ë†ğ‘Ÿğ‘–). (8)
In practice, the right end of the interval can be calculated based on
some empirical circumstances, such as the method in [ 14]. However,
in this work, we wish to consider a more general case, where the
DSP only knows that the winning price is higher than her own bid,
thus assuming that the upper bound of the winning price is infinity
in each auction. Under this assumption, the worst-case distribution
in (6) can be further rewritten as Ë†ğ’‘ğ‘=FB({ğ‘¤ğ‘–|ğ‘–âˆˆW}) , while
keeping the solution of problem (5) unchanged. The loss function
(8) now can be simplified to:
ğ¿â€²
ğ‘ =âˆ’âˆ‘ï¸
ğ‘–âˆˆWlogğ‘Ë†ğ’‘ğ‘–(Ë†ğ‘¤ğ‘–). (9)
This result is natural. Since all known uncertainty resides within
the auctions inL, the most conservative approach is to refrain from
utilizing the uncertain data from these auctions, that is, only using
data in winning auctions Wlike loss function (9).
We would like to further discuss the significance of RBC from
the perspective of existing works. Since censorship issue was raised
by [32], the academic discussion on bid shading has mostly been
limited to the distribution prediction methods under censorship
scenarios, such as the censored regression and survival analysis
mentioned in Section 2.2. However, the results of RBC suggest that
using only the data in winning auctions Whas a certain degree
of robustness, and therefore, it may yield favorable results in an
environment with considerable uncertainty such as bid shading.
Our subsequent experiments have validated this. Moreover, since
RBC considers the worst-case distributions that are only composed
of deterministic data, RBC has the potential to bring in research
on conditional density estimation [ 23] beyond censorship, an area
that is currently lacking in the discussion of bid shading. We will
leave this potential direction for future work.
 
1807Robust Auto-Bidding Strategies for Online Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.2 Robust Bidding for Distribution Shift
In contrast to the RBC strategy, where the ambiguity set Pğ‘˜ğ‘›is
known, the Robust Bidding for Distribution Shift (RBDS) contem-
plates scenarios in which even the information about the ambiguity
setPğ‘¢ğ‘›is unattainable. Analogous to the preceding analysis, the
DSP is restricted to obtaining sampled data of the winning prices,
and there may be a disparity between sampling distribution and
the real distribution of winning prices. Furthermore, the real distri-
bution of winning prices might undergo shifts over time, thereby
introducing an element of inevitable bias into the DSPâ€™s predictions
that are based on the sampled data.
To address this kind of uncertainty, we consider employing the
conventional distributionally robust optimization approach, and
adopting the discrepancy-based ambiguity set. This approach as-
sumes that the discrepancy between the real distribution and the
sampling distribution lies within a certain range, and it ensures a
lower bound on the performance of the real distribution by opti-
mizing for the worst-case scenario among all distributions within
this range. The optimization problem can still be expressed in the
form of problem (2), where the ambiguity set is represented as:
Pğ‘¢ğ‘›(ğ’‘0,ğœ–0)={ğ’‘|ğ‘‘(ğ’‘,ğ’‘0)â‰¤ğœ–0}, (10)
where ğ’‘0is the winning price distribution estimated by DSP, ğœ–0is
the upper bound of the discrepancy between the real distribution
and estimated distribution, and ğ‘‘is a function that measures the
discrepancy between distributions. Therefore, Pğ‘¢ğ‘›mathematically
describes our method of considering all distributions that differ
from the estimated ğ’‘0within a certain range. Conversely, problem
(2) with the ambiguity set Pğ‘¢ğ‘›aims to optimize the worst-case
scenario within this distribution range.
After introducing the complete strategic framework, we will
further provide the definition of the ambiguity set Pğ‘¢ğ‘›and the
algorithm for problem (2) with Pğ‘¢ğ‘›in the following parts.
4.2.1 Ambiguity Set in RBDS. We firstly introduce the estimated
distribution ğ’‘0, the discrepancy function ğ‘‘, and the discrepancy
upper bound ğœ–within the ambiguity set Pğ‘¢ğ‘›. The distribution ğ’‘0is
the distribution estimated based on the sampled data. Similar to the
distribution ğ’‘0in SO problem (5), ğ’‘0inPğ‘¢ğ‘›could be the estimated
winning price distribution, in which case problem (2) can be simply
interpreted as ensuring a lower bound on the bidding surplus by
optimizing the worst-case outcome when there is a discrepancy
between the estimated distribution and the real distribution. Alter-
natively, it could also be the worst-case distribution Ë†ğ’‘ğ‘estimated
in our RBC strategy (6). In this case, problem (2) can similarly be
understood as acknowledging that discrepancies exist between the
sampling worst-case distribution and the real worst-case distribu-
tion, and we likewise ensure a lower bound on bidding surplus by
optimizing for the â€œworst of the worst-caseâ€ distribution. From this
perspective, the RBDS strategy can be regarded as a robust form of
the SO problem (5).
For the discrepancy function ğ‘‘, we adopt the Wasserstein dis-
tance function [ 25] to measure the discrepancy between distribu-
tions. This is a commonly used discrepancy function in distribu-
tionally robust optimization. Furthermore, when employing the
Wasserstein distance, the form of the worst-case winning price
Figure 2: Decision function for the bid shading problem
distribution has an intuitive interpretation, aligning with obser-
vations in some existing works [ 34]. This intuitive interpretation
is provided in the subsequent Section 4.2.2. Here, we first present
the method of calculating this discrepancy function. Specifically,
given any two distributions ğ’‘and ğ’’with discrete representation
(ğ‘1, ğ‘2, ..., ğ‘ğ‘€)and(ğ‘1, ğ‘2, ..., ğ‘ğ‘€), the Wasserstein distance of
ğ’‘andğ’’can be defined as the solution of problem:
minğ’…Ã
ğ‘–Ã
ğ‘—ğ‘‘ğ‘–ğ‘—|ğ‘–âˆ’ğ‘—|
s.t. 0â‰¤Ãğ‘
ğ‘—=1ğ‘‘ğ‘–ğ‘—â‰¤ğ‘ğ‘–
ğ‘ğ‘–+Ãğ‘
ğ‘—=1ğ‘‘ğ‘—ğ‘–âˆ’Ãğ‘
ğ‘–=1ğ‘‘ğ‘–ğ‘—=ğ‘ğ‘–
ğ’…={ğ‘‘ğ‘–ğ‘—},ğ‘–âˆˆ[1,ğ‘€],ğ‘—âˆˆ[1,ğ‘€],(11)
where the matrix ğ’…is the optimized variable, and each ğ‘‘ğ‘–ğ‘—repre-
sents the probability quantity transferred from ğ‘ğ‘–toğ‘ğ‘—, which is
non-negative and cannot be greater than the original probability
quantityğ‘ğ‘–in total. In addition, after all the transfer, the probability
distribution ğ’‘should become ğ’’. Then Wasserstein distance is the
minimum value of the sum of the product of the probability trans-
ferred and the distance under these constraints, and we denote the
result of problem (11) as the Wasserstein distance ğ‘‘(ğ’‘,ğ’’).
Finally,ğœ–0defines the distance between the estimated distribu-
tion ğ’‘0and real distribution. In practice, DSP can never know the
real distribution of winning prices, but can only obtain sampled
data. Therefore, it is impossible for DSP to accurately determine
the value of ğœ–0, which is a specific manifestation of the unknown
uncertainty associated with ğ‘ƒğ‘¢ğ‘›. Given this, we adopt a second-best
approach, considering the integration of the DSPâ€™s objective of max-
imizing surplus to choose ğœ–0. Of course, this method of selection
dilutes its physical meaning, making it more akin to a hyperpa-
rameter. Hence, we will not delve into an extensive discussion on
the selection of ğœ–0, but will only report the relationship between
the DSPâ€™s surplus and ğœ–0in experiments, as well as the maximum
surplus that can be achieved by adjusting ğœ–0.
4.2.2 Algorithm for RBDS. To solve problem (2) with the ambi-
guity setPğ‘¢ğ‘›, we first consider whether we can use traditional
distributionally robust optimization algorithms, which requires the
analysis of the decision function â„. Given winning price value Ë†ğ‘¤,
the image of the function â„(Â·,Ë†ğ‘¤)is shown in Figure 2. It can be
seen that it is a non-continuous and non-convex function, and its
mathematical properties are relatively poor.
In the existing works, the latest researches [ 10] and [ 9] have
studied the non-convex distributionally robust optimization algo-
rithm, but they still require the decision function to satisfy certain
continuous assumptions. Besides, since the decision variable ğ‘is
discrete, the equivalence problem (3) is also similar to the form of
discrete minimax problem [ 36], but in this series of research, the set
of distributions is finite, while our ambiguity set is infinite. If we
 
1808KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qilong Lin, Zhenzhe Zheng, & Fan Wu
Figure 3: An example for the constructive approach
hope to apply these algorithms to our problem, we need to modify
our decision function to meet the requirements of their algorithms.
In our work, due to the particularity of our problem, we consider
designing an algorithm without changing the decision function.
Since the bid spaceBin bid shading problem is finite, we can further
write the problem (3) as:
max
ğ‘âˆˆB{(ğ‘£âˆ’ğ‘)min
ğ’‘âˆˆPğ‘ƒğ’‘(ğ‘)}, (12)
where the key point is to solve the minterm on discrete domain B,
and we denote it as ğ‘“P:
ğ‘“P(ğ‘)=min
ğ’‘âˆˆPğ‘ƒğ’‘(ğ‘). (13)
If we can obtain the values of function ğ‘“PonB, problem (12)
becomes easy to solve, like problem (7) in previous section. Next,
givenP=Pğ‘¢ğ‘›(ğ’‘0,ğœ–0), we will elaborate on the approach to solving
ğ‘“Pand provide a specific algorithm for it.
Letâ€™s start with a simple example. Assuming B={1,2,3}andğ’‘0
can be discretely represented by (ğ‘1
0,ğ‘2
0,ğ‘3
0), whereğ‘1
0=0.6and
ğ‘2
0=ğ‘3
0=0.2. In this case, distribution ğ’‘0can be represented by
the left histogram in Figure 3. Assuming that given ğœ–0=0.6, we are
calculating ğ‘“P(2)=minğ’‘âˆˆPğ‘ƒğ’‘(2), that is, finding a distribution
ğ’’0âˆˆPğ‘¢ğ‘›(ğ’‘0,ğœ–0)with the minimal cumulative probability ğ‘ƒğ’’0(2).
We consider a constructive approach, devising a certain proce-
dure to construct ğ’’0from ğ’‘0. Letâ€™s start with ğ’’0=ğ’‘0. To make
ğ‘ƒğ’’0(2)=ğ‘1
0+ğ‘2
0as small as possible, we need to decrease ğ‘1
0andğ‘2
0,
and increase ğ‘3
0. But which one should we decrease first? Given our
constraintğ‘‘(ğ’‘0,ğ’’0)â‰¤ğœ–0=0.6, it would be sensible to reduce ğ‘2
0
first because reducing ğ‘2
0by the same amount will lead to a smaller
ğ‘‘(ğ’‘0,ğ’’0)compared to reducing ğ‘1
0. We decrease ğ‘2to 0, which
results in an increase of ğ‘3to 0.2. At this point, the distribution can
be represented by the middle histogram in Figure 3, where the red
shaded area indicates the probability reduced relative to ğ’‘0, and the
blue shaded area indicates the probability increased relative to ğ’‘0,
with the computed ğ‘‘(ğ’‘0,ğ’’0)being 0.2. We continue to reduce ğ‘1
0
and increase ğ‘3
0, and we find that after reducing ğ‘1
0by 0.2,ğ‘‘(ğ’‘0,ğ’’0)
becomesğœ–0=0.6; this new distribution can be represented by the
histogram on the right side of Figure 3. At this point, ğ‘ƒğ’’0(2)=0.2
is the function value ğ‘“P(2)that we are looking for.
An issue that this example doesnâ€™t cover is, assuming we allow
for a bid amount of 4, and considering the same initial distribution
ğ’‘0above, should we increase the probability at ğ‘3
0orğ‘4
0? This is
actually similar to the choice of decreasing ğ‘1
0orğ‘2
0first. Since
increasing the same amount of probability, increasing ğ‘3
0results
in a smaller distance ğ‘‘(ğ’‘0,ğ’’0), we similarly choose to increase
ğ‘3
0. In fact, summarizing the previous example, we find that givenAlgorithm 1 Algorithm for ğ‘“Pon Discrete Domain
Require: Distribution ğ’‘0, discrepancy upper bound ğœ–0
1:forğ‘—=1toğ‘€do
2: ğ’’0â†ğ’‘0;
3:ğ‘˜â†ğ‘—;
4: whileğ‘‘(ğ’‘0,ğ’’0)â‰¤ğœ–andğ‘˜>0do
5:ğ‘ğ‘—+1â†ğ‘ğ‘—+1+ğ‘ğ‘˜;
6:ğ‘ğ‘˜â†0;
7:ğ‘˜â†ğ‘˜âˆ’1;
8: end while
9:ğ‘ğ‘˜+1â†(ğ‘‘(ğ’‘0,ğ’’0)âˆ’ğœ–0)/(ğ‘—âˆ’ğ‘˜);
10:ğ‘“P(ğ‘ğ‘—)â†Ãğ‘—
ğ‘˜=1ğ‘ğ‘˜;
11:end for
bidğ‘, the method to construct ğ’’0is to transfer the probabilities
fromğ‘ğ‘
0,ğ‘ğ‘âˆ’1
0,...,ğ‘1
0toğ‘ğ‘+1
0in descending order of index, until the
distance from the original distribution reaches the upper bound
ğœ–0. Based on this approach, we propose a procedural method as
shown in Algorithm 1, which provides the pseudo code for solving
the values of ğ‘“Pon the discretely defined domain. We have the
following conclusion:
Remark 4.2. Algorithm 1 provide the strictly optimal solution for
problem minğ’‘âˆˆPğ‘ƒğ’‘(ğ‘)on discrete domainB, hence provide the exact
values of function ğ‘“PonBdefined in (13).
A detailed proof can be found in Appendix A.2. In addition to
outlining the algorithm, the example in Figure 3 vividly demon-
strates the worst-case scenario considered in distributionally robust
optimization problems. It can be seen that given bid ğ‘=2and the
initial distribution ğ’‘0, in the worst-case scenario, the distribution
ğ’’0forms a spike right after the bid ğ‘=2, specifically at ğ‘3
0. Exist-
ing literature indicates that such spikes are common in real-world
settings [ 34], therefore, employing Wasserstein distance in distribu-
tionally robust optimization can be interpreted as a robust bidding
strategy that accounts for the occurrence of these spikes.
After obtaining the algorithm for solving ğ‘“P(ğ‘ğ‘—), the optimiza-
tion problem (12) of RBDS becomes as easy to solve as the SO
problem (5). In summary and comparison, the RBDS method is
equivalent to performing a robust treatment on the original distri-
bution, and then solving the optimization problem (5) to obtain a
robust bid against unknown uncertainty.
5 EXPERIMENT RESULTS
5.1 Experimental Setup
Firstly, we introduce the experimental datasets we used, the experi-
mental procedures, and the models utilized in the experiments.
5.1.1 Datasets. Our experiments use two public datasets including
the iPinYou dataset [15] and the Criteo dataset [6]:
â€¢The iPinYou dataset is a widely used dataset in bid shading
related research. It contains 10 days of real-world data from
auctions in which the iPinYou DSP participates. We follow
the existing work [ 30] and use the data in the first 7 days as
training data, and the remaining data as test data.
 
1809Robust Auto-Bidding Strategies for Online Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 4: The main process of the experiments
â€¢The Criteo dataset contains 30 days of real-world data from
auctions that the Criteo DSP participates. We use the first
24 daysâ€™ data as the training set, and the remaining 6 daysâ€™
data as the test set.
The datasets above have provided the feature Ë†ğ‘¥ğ‘–and winning
price Ë†ğ‘¤ğ‘–in each auction ğ‘–. However, there is no censored data in the
original datasets, so we need to simulate the censorship problem
on the training set. In this work, we adopt the same method of
simulating censored data as in [ 28], which firstly simulates the
DSP bid Ë†ğ‘ğ‘–, and then divides the training set into the set WandL
according to the relationship between Ë†ğ‘ğ‘–and Ë†ğ‘¤ğ‘–.
In addition, we also need the estimated value data Ë†ğ‘£ğ‘–of the DSP.
However, since the value is confidential for DSP, public datasets do
not include this data, and we can only obtain it through simulation.
In our paper, we assume that the DSP uses a common strategy,
where in each auction, the DSP uses the value multiplied by a shad-
ing rate as her bid [ 8]. We further assume that the shading rate is
randomly selected by DSP, that is, the bid Ë†ğ‘ğ‘–is generated in random
proportion according to the value Ë†ğ‘£ğ‘–. From these assumptions, we
are able to simulate the data Ë†ğ‘£ğ‘–for our experiments.
5.1.2 Experimental Procedure. Our experiment simulates the real
process of the DSP participating in auctions, as shown in Figure
4. In this process, DSP first predicts the distribution Ë†ğ’‘ğ‘–based on
feature Ë†ğ’™ğ‘–using the prediction model. This distribution could be
the winning price distribution or the worst-case distribution in
RBC strategy, corresponding to different prediction models utilized.
After obtaining the distribution, DSP derives the bid Ë†ğ‘ğ‘–according
to the optimization problem where the optimization corresponds
to the aforementioned distribution and could be the SO problem (5)
or the RBDS problem (12). Once the bid is determined, the DSPâ€™s
surplus is calculated based on the actual winning price Ë†ğ‘¤ğ‘–and the
decision function â„.
5.1.3 Prediction Models. In our experiments, we primarily employ
the following bidding strategies:
STM is a winning price distribution prediction method that
combines survival analysis with decision trees [ 30]. We integrate it
with SO problem (5) as a traditional bidding strategy that utilizes
survival analysis.
MCN is a parametric distribution prediction method where the
winning prices are modeled as a mixture of Gaussian distributions
[7]. By integrating this approach with SO problem (5), it is utilized
as a traditional parametric method bidding strategy.
KMMN is a winning price distribution prediction method that
combines survival analysis with Markov network [ 28]. We integrate
it with SO problem (5) to serve as an enhanced survival analysis-
based bidding strategy.
NPM is a non-parametric distribution prediction method that
employs the same network as depicted in RBC strategy. It is similarto the method described in [ 14], but we utilize a simpler network
architecture and loss function. We integrate it with SO problem (5)
to serve as a non-parametric bidding strategy.
RBC is the robust bidding strategy we have proposed for known
uncertainty. Unless otherwise stated, we employ the network de-
picted in previous section and train our model using the loss func-
tion (9) to predict the worst-case distribution. This prediction is
then incorporated into SO problem (5) as the robust bidding strat-
egy. The selection of the distribution prediction model and loss
function will be discussed in subsequent experimental sections.
RBDS is our proposed robust bidding method designed for un-
known uncertainty. It necessitates the integration with the afore-
mentioned distribution prediction model and supersedes the origi-
nal SO problems (5). In our experiments, we denote the combination
of this bidding strategy with any distribution prediction method M
as M+R.
5.2 Performance of RBC Strategy
5.2.1 Overall Performance. We initially conduct experiments on
the surplus performance of each strategy without employing RBDS
strategy, with the results displayed in Table 1. The distribution
prediction model is trained on the training set, and the resulting
bidding strategy is tested on the test set. The first column of Table
1 lists different Campaigns, comprising nine campaigns from the
iPinYou dataset and the overall Criteo dataset. Within the iPinYou
dataset, the winning price distributions vary among different cam-
paigns, hence existing works provide independent results for each
campaign, a practice we also continue. The main body of the table
presents the surplus obtained through different strategies, and it
can be observed that the RBC strategy we propose achieves the
highest surplus in the majority of the campaigns.
Table 1: Overall Surplus of Different Strategies (106)
Camp. STM MCN KMMN NPM RBC
1458 11.29 11.61 11.03 12.06 12.31
2259 4.988 5.442 5.278 5.436 5.590
2261 4.959 5.602 5.802 6.021 6.152
2821 8.191 8.803 8.548 8.784 9.095
2997 2.460 2.939 2.832 2.876 3.005
3358 3.309 3.327 2.961 3.610 3.451
3386 9.253 9.659 9.491 10.34 10.42
3427 7.295 7.767 7.783 8.441 8.486
3476 7.202 7.470 7.389 7.899 8.051
Criteo 101.8 104.0 103.6 109.2 109.3
A natural question arises as to why the robust bidding strategy
RBC, being a conservative approach, can perform better than the
strategy based on direct prediction of winning price distribution.
The primary reason is likely due to the fact that direct prediction of
winning price distribution requires the introduction of assumptions
that may not necessarily hold in practice, leading to an inherent bias
in the predicted winning price distribution. From the experimental
results, it appears that the surplus loss caused by these biases is
greater than the loss due to robustness.
 
1810KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qilong Lin, Zhenzhe Zheng, & Fan Wu
Figure 5: Relation between the upper bound parameter ğœ–0
and surplus on some campaigns
5.2.2 Worst-case Performance. Subsequently, we compare the per-
formance of different strategies under the worst-case scenario.
Here, the worst-case refers to the most adverse situation within the
known uncertainty, corresponding to the case where Ë†ğ‘¤ğ‘–=Ë†ğ‘Ÿğ‘–for
ğ‘–âˆˆL. We set the winning price in the auctions lost by the DSP in
the training set to infinity, and test the surplus of different models
under this worst-case training set. The results are shown in Table
2. It can be observed that our robust approach exhibits the best
worst-case performance across all campaigns.
Table 2: Worst-case Surplus of Different Strategies (107)
Camp. STM MCN KMMN NPM RBC
1458 5.952 5.996 6.107 6.324 6.626
2259 1.145 1.264 1.304 1.277 1.425
2261 0.973 1.076 1.103 1.117 1.215
2821 1.947 2.074 2.125 2.137 2.310
2997 0.479 0.558 0.529 0.540 0.570
3358 2.156 2.273 2.267 2.391 2.584
3386 5.223 5.262 5.475 5.679 5.967
3427 3.704 3.815 3.951 4.100 4.339
3476 2.862 2.907 3.002 3.085 3.271
Criteo 41.05 39.70 40.15 41.68 41.72
5.3 Performance of RBDS Strategy
5.3.1 Relationship between Upper Bound Parameter and Surplus.
We first give a certain relationship between the upper bound ğœ–0
and the surplus on the test set. The results on the test set of some
campaigns with KMMN+R strategy are shown in Figure 5. We
can observe that with the increase of ğœ–0, the surplus on the test
set basically increases first and then decreases. But in different
campaigns, the specific shape of the relation curve between surplus
andğœ–0is different. In practice, we combine this insight with binary
search to find the maximum surplus that can be obtained on the
test set by adjusting ğœ–0.5.3.2 Overall Performance. In this section, we report on the upper
bound of surplus attainable through the adjustment of ğœ–0in the
RBDS strategy. Taking RBC as an example, we compare the surplus
generated by the RBC strategy with the upper bound of surplus that
can be obtained through the RBC+R strategy, as illustrated in Table
3. It can be observed that RBDS has delivered a notable increase in
surplus for DSP in parts of campaigns. At this juncture, a question
similar to that regarding RBC may naturally arise: why does the
employment of the robust strategy RBDS lead to an enhancement
in surplus? This is primarily due to the divergence in the winning
price distribution between the training and testing datasets, which
leads to the model derived from the training set failing to accurately
predict the winning price distribution on the testing set. This may
result in an increased surplus when an appropriate robustness
parameterğœ–0is selected.
Table 3: Surplus of RBC and RBC+R Strategies (106)
Camp. RBC RBC+R
1458 12.31 12.40
2259 5.590 5.596
2261 6.152 6.161
2821 9.095 9.175
2997 3.005 3.008
3358 3.451 3.484
3386 10.42 10.44
3427 8.486 8.490
3476 8.051 8.130
Criteo 109.3 110.5
5.3.3 Worst-case Performance. To verify the robustness of our
RBDS strategy, we simulate the case where the real winning price
distribution is different from the predicted distribution, and then
compare the expected surplus of the SO strategy and our RBDS
strategy. In this context, the SO strategy refers to obtaining optimal
bids by solving problem (5), whereas RBDS strategy derives opti-
mal bids by solving problem (12). Specifically, we assume that the
value of DSP is evenly distributed on [50, 200]. We take the overall
winning price distribution on some campaigns as the predicted
distribution ğ’‘0, and construct the worst-case distributions as the
real distributions subject to different Wasserstein distances. For
the RBDS strategy, we set the upper bound ğœ–0=0.02. Under these
settings, we calculate the expected surplus for SO and the worst-
case expected surplus for RBDS in some of the campaigns, and the
results are shown in Figure 6. We can observe that the performance
of SO strategy is generally better than RBDS strategy when the
distance is small. However, as the distance increases, our RBDS
strategy gradually outperforms the SO strategy. Compared to SO
strategy, it is easy to find that the surplus of our RBDS strategy de-
creases less as the distance increases, which means that the surplus
of RBDS is less affected by the distribution shift in the worst-case
situations. This verifies the robustness of our RBDS strategy.
5.3.4 Further Discussion of RBDS strategy. To better understand
the robustness of RBDS strategy, we specifically construct an ex-
ample to analyze how the SO and RBDS strategies bid for a given
 
1811Robust Auto-Bidding Strategies for Online Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 6: Worst-case performance of RBDS on some cam-
paigns
Figure 7: An example for the bid selection process in SO and
RBDS strategies
valueğ‘£. We use the overall winning price distribution on the 2821
campaign as the predicted distribution of SO problem (5) and RBDS
problem (12) respectively. Assuming that the Wasserstein distance
between the real (worst-case) distribution and the predicted distri-
bution, as well as the upper bound ğœ–0in RBDS are both 0.2, and we
set the DSPâ€™s value ğ‘£=101. At this point, for different bids, the pre-
dicted surplus considered by the SO strategy, the worst-case surplus
considered by the RBDS strategy, and the real surplus are shown
in Figure 7. Among them, the real surplus is corresponding to the
specially constructed worst-case distribution, and it overlaps with
the predicted surplus for most bids, as shown in Figure 7. For better
illustration, we mark the bid prices selected by the SO and RBDS
strategies with dashed lines, which are the values corresponding to
the highest points in the two curves. Based on these selected bids,
we further mark out the surplus of these two strategies under the
real distribution to give an intuitive comparison.As can be observed from Figure 7, there is a sharp increase in the
predicted surplus when the bid price is around 30, which means that
the probability of winning price at this point is very high, forming
a spike in the probability distribution. In this case, compared with
the steep predicted surplus curve of SO strategy, RBDS strategy
considers a smoother worst-case surplus curve in order to prevent
this spike from moving within a small range. Therefore the bid of
RBDS strategy is farther from the spike than SO strategy. In the
worst case, the probability around the spike is shifted, and RBDS can
deal with this situation more robustly and obtain a better surplus.
6 CONCLUSION
In this work, we model the uncertain environment inherent in the
design of auto-bidding strategies within the context of bid shading,
and propose two levels robust bidding strategies to achieve better
surplus in such environments with considerable uncertainty. The
experimental results on public dataset validate the effectiveness
and robustness of our robust bidding strategies.
Since this work is the first to consider the uncertainty issue in
bid shading, there are still many aspects to explore in both the-
ory and experiment. For instance, one could attempt using the
chance-constrained [ 3] surplus instead of worst-case surplus as the
optimization objective, introduce techniques of advanced condi-
tional density estimation [ 23] into RBC, and validate the results of
robust strategies in real bidding environments, etc. We leave these
promising directions to future works.
ACKNOWLEDGMENTS
This work was supported in part by National Key R&D Program
of China (No. 2022ZD0119100), in part by China NSF grant No.
62322206, 62132018, U2268204, 62025204, 62272307, 62372296. The
opinions, findings, conclusions, and recommendations expressed
in this paper are those of the authors and do not necessarily reflect
the views of the funding agencies or the government.
REFERENCES
[1]Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. 2009. Robust Opti-
mization. Princeton University Press.
[2]Jonathan Buckley and Ian James. 1979. Linear regression with censored data.
Biometrika 66, 3 (1979), 429â€“436.
[3]A. Charnes and W. W. Cooper. 1959. Chance-Constrained Programming. Man-
agement science 6, 1 (1959), 73â€“79.
[4]Erick Delage and Yinyu Ye. 2010. Distributionally robust optimization under mo-
ment uncertainty with application to data-driven problems. Operations research
58, 3 (2010), 595â€“612.
[5]Erick Delage and Yinyu Ye. 2010. Wiesemann, Wolfram and Kuhn, Daniel and
Sim, Melvyn. Operations research 62, 6 (2010), 1358â€“1376.
[6]Eustache Diemert, Julien Meynet, Pierre Galland, and Damien Lefortier. 2017.
Attribution Modeling Increases Efficiency of Bidding in Display Advertising. In
Proceedings of ADKDD. 1â€“6.
[7]Aritra Ghosh, Saayan Mitra, Somdeb Sarkhel, Jason Xie, Gang Wu, and
Viswanathan Swaminathan. 2020. Scalable Bid Landscape Forecasting in Real-
Time Bidding. In Machine Learning and Knowledge Discovery in Databases. 451â€“
466.
[8]Djordje Gligorijevic, Tian Zhou, Bharatbhushan Shetty, Brendan Kitts, Shengjun
Pan, Junwei Pan, and Aaron Flores. 2020. Bid Shading in The Brave New World
of First-Price Auctions. In Proceedings of CIKM. 2453â€“2460.
[9]Mert GÃ¼rbÃ¼zbalaban, Andrzej RuszczyÅ„ski, and Landi Zhu. 2022. A Stochastic
Subgradient Method for Distributionally Robust Non-convex and Non-smooth
Learning. Journal of Optimization Theory and Applications 194, 3 (2022), 1014â€“
1041.
[10] Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. 2021. Non-convex
Distributionally Robust Optimization: Non-asymptotic Analysis. Advances in
Neural Information Processing Systems 34 (2021), 2771â€“2782.
 
1812KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qilong Lin, Zhenzhe Zheng, & Fan Wu
[11] Niklas Karlsson and Qian Sang. 2021. Adaptive Bid Shading Optimization of
First-Price Ad Inventory. In 2021 American Control Conference (ACC). 4983â€“4990.
[12] Ã‡aÄŸÄ±l KoÃ§yiÄŸit, Garud Iyengar, Daniel Kuhn, and Wolfram Wiesemann. 2020.
Distributionally Robust Mechanism Design. Management Science 66, 1 (2020),
159â€“189.
[13] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh
Shafieezadeh-Abadeh. 2019. Wasserstein Distributionally Robust Optimization:
Theory and Applications in Machine Learning. arXiv:1908.08729
[14] Xu Li, Michelle Ma Zhang, Zhenya Wang, and Youjun Tong. 2022. Arbitrary
Distribution Modeling with Censorship in Real-Time Bidding Advertising. In
Proceedings of SIGKDD. 3250â€“3258.
[15] Hairen Liao, Lingxiao Peng, Zhenchuan Liu, and Xuehua Shen. 2014. IPinYou
Global RTB Bidding Algorithm Competition Dataset. In Proceedings of the Eighth
International Workshop on Data Mining for Online Advertising. 1â€“6.
[16] Fengming Lin, Xiaolei Fang, and Zheming Gao. 2022. Distributionally Robust
Optimization: A review on theory and applications. Numerical Algebra, Control
and Optimization 12, 1 (2022), 159â€“212.
[17] Roger B Myerson. 1981. Optimal auction design. Mathematics of operations
research 6, 1 (1981), 58â€“73.
[18] Noam Nisan, Tim Roughgarden, Ã‰va Tardos, and Vijay V. Vazirani. 2007. Algo-
rithmic Game Theory. Cambridge University Press.
[19] Weitong Ou, Bo Chen, Yingxuan Yang, Xinyi Dai, Weiwen Liu, Weinan Zhang,
Ruiming Tang, and Yong Yu. 2023. Deep Landscape Forecasting in Multi-Slot
Real-Time Bidding. In Proceedings of SIGKDD. 4685â€“4695.
[20] Shengjun Pan, Brendan Kitts, Tian Zhou, Hao He, Bharatbhushan Shetty, Aaron
Flores, Djordje Gligorijevic, Junwei Pan, Tingyu Mao, San Gultekin, and Jianlong
Zhang. 2020. Bid Shading by Win-Rate Estimation and Surplus Maximization.
arXiv:2009.09259
[21] Hamed Rahimian and Sanjay Mehrotra. 2022. Frameworks and results in distri-
butionally robust optimization. Open Journal of Mathematical Optimization 3
(2022), 1â€“85.
[22] Kan Ren, Jiarui Qin, Lei Zheng, Zhengyu Yang, Weinan Zhang, and Yong Yu. 2019.
Deep Landscape Forecasting for Real-Time Bidding Advertising. In Proceedings
of SIGKDD. 363â€“372.
[23] Jonas Rothfuss, Fabio Ferreira, Simon Walther, and Maxim Ulrich. 2019. Condi-
tional Density Estimation with Neural Networks: Best Practices and Benchmarks.
arXiv:1903.00954
[24] Herbert E Scarf, KJ Arrow, and S Karlin. 1957. A min-max solution of an inventory
problem. Rand Corporation Santa Monica.
[25] SS Vallender. 1974. Calculation of the Wasserstein Distance Between Probability
Distributions on the Line. Theory of Probability & Its Applications 18, 4 (1974),
784â€“786.
[26] Jun Wang, Weinan Zhang, Shuai Yuan, et al .2017. Display advertising with
real-time bidding (RTB) and behavioural targeting. Foundations and TrendsÂ® in
Information Retrieval 11, 4-5 (2017), 297â€“435.
[27] Ping Wang, Yan Li, and Chandan K. Reddy. 2019. Machine Learning for Survival
Analysis: A Survey. ACM Comput. Surv. 51, 6 (2019), 36 pages.
[28] Tengyun Wang, Haizhi Yang, Siyu Jiang, Yueyue Shi, Qianyu Li, Xiaoli Tang,
Han Yu, and Hengjie Song. 2022. Kaplanâ€“Meier Markov network: Learning the
distribution of market price by censored data in online advertising. Know.-Based
Syst. 251 (2022), 11 pages.
[29] Tengyun Wang, Haizhi Yang, Han Yu, Wenjun Zhou, Yang Liu, and Hengjie Song.
2019. A revenue-maximizing bidding strategy for demand-side platforms. IEEE
Access 7 (2019), 68692â€“68706.
[30] Yuchen Wang, Kan Ren, Weinan Zhang, Jun Wang, and Yong Yu. 2016. Functional
Bid Landscape Forecasting for Display Advertising. In Machine Learning and
Knowledge Discovery in Databases. 115â€“131.
[31] Wush Wu, Mi-Yen Yeh, and Ming-Syan Chen. 2018. Deep Censored Learning of
the Winning Price in the Real Time Bidding. In Proceedings of SIGKDD. 2526â€“2535.
[32] Wush Chi-Hsuan Wu, Mi-Yen Yeh, and Ming-Syan Chen. 2015. Predicting Win-
ning Price in Real Time Bidding with Censored Data. In Proceedings of SIGKDD.
1305â€“1314.
[33] Yong Yuan, Feiyue Wang, Juanjuan Li, and Rui Qin. 2014. A survey on real
time bidding advertising. In Proceedings of 2014 IEEE International Conference on
Service Operations and Logistics, and Informatics. 418â€“423.
[34] Wei Zhang, Brendan Kitts, Yanjun Han, Zhengyuan Zhou, Tingyu Mao, Hao
He, Shengjun Pan, Aaron Flores, San Gultekin, and Tsachy Weissman. 2021.
MEOW: A Space-Efficient Nonparametric Bid Shading Algorithm. In Proceedings
of SIGKDD. 3928â€“3936.
[35] Tian Zhou, Hao He, Shengjun Pan, Niklas Karlsson, Bharatbhushan Shetty, Bren-
dan Kitts, Djordje Gligorijevic, San Gultekin, Tingyu Mao, Junwei Pan, Jianlong
Zhang, and Aaron Flores. 2021. An Efficient Deep Distribution Network for Bid
Shading in First-Price Auctions. In Proceedings of SIGKDD. 3996â€“4004.
[36] Shen Zuhe, Huane Zhen Yu, and MA Wolfe. 1997. An Interval Maximum Entropy
Method for a Discrete Minimax Problem. Applied mathematics and computation
87, 1 (1997), 49â€“68.A PROOFS
A.1 Proof of Remark 4.1
We firstly show that for any ğ‘âˆˆB:
min
ğ’‘âˆˆPğ‘˜ğ‘›ğ‘ƒğ’‘(ğ‘)=ğ‘ƒË†ğ’‘ğ‘(ğ‘), (14)
wherePğ‘˜ğ‘›andË†ğ’‘ğ‘are defined in (4) and (6). Since Bis finite:
(a) Ifğ‘=0,ğ‘ƒğ’‘(ğ‘)=0for any ğ’‘, thus (14) holds.
(b) Ifğ‘â‰ 0, we denote ğ‘=ğ‘ğ½âˆˆ B. For rigor, we formally
provide the definition of the probability vector ğ’‘in the paper. For
a probability vector constructed from a winning price dataset ğ’‘=
FB({ğ‘¤ğ‘–|ğ‘–âˆˆA}) , the j-th element ğ‘ğ‘—is defined as:
ğ‘ğ‘—=1
|A|âˆ‘ï¸
ğ‘–âˆˆAI{ğ‘ğ‘—âˆ’1â‰¤ğ‘¤ğ‘–<ğ‘ğ‘—}, (15)
where|A|denotes the size of set A. Then the cumulative distribu-
tion function (CDF) value for any ğ‘ğ½is:
ğ‘ƒğ’‘(ğ‘ğ½)=ğ½âˆ‘ï¸
ğ‘—=1ğ‘ğ‘—=1
|A|âˆ‘ï¸
ğ‘–âˆˆAI{ğ‘¤ğ‘–<ğ‘ğ‘—}. (16)
Defined in (4),Pğ‘˜ğ‘›contains all ğ’‘=FB({ğ‘¤ğ‘–|ğ‘–âˆˆA}) that satisfies
ğ‘¤ğ‘–âˆˆTğ‘–, whereTğ‘–={Ë†ğ‘¤ğ‘–}forğ‘–âˆˆW andTğ‘–={ğ‘¤|Ë†ğ‘™ğ‘–â‰¤ğ‘¤â‰¤Ë†ğ‘Ÿğ‘–}for
ğ‘–âˆˆL. Hence the left-hand side in (14) equals to:
min
ğ’‘âˆˆPğ‘˜ğ‘›ğ‘ƒğ’‘(ğ‘ğ½)= min
{ğ‘¤ğ‘–âˆˆTğ‘–|ğ‘–âˆˆA}1
|A|âˆ‘ï¸
ğ‘–âˆˆAI{ğ‘¤ğ‘–<ğ‘ğ½}
=1
|A|âˆ‘ï¸
ğ‘–âˆˆAmin
ğ‘¤ğ‘–âˆˆTğ‘–I{ğ‘¤ğ‘–<ğ‘ğ½}
=1
|A|âˆ‘ï¸
ğ‘–âˆˆWmin
ğ‘¤ğ‘–âˆˆTğ‘–I{ğ‘¤ğ‘–<ğ‘ğ½}+
1
|A|âˆ‘ï¸
ğ‘–âˆˆLmin
ğ‘¤ğ‘–âˆˆTğ‘–I{ğ‘¤ğ‘–<ğ‘ğ½}(17)
Forğ‘–âˆˆW ,Tğ‘–={Ë†ğ‘¤ğ‘–}andminğ‘¤ğ‘–âˆˆTğ‘–I{ğ‘¤ğ‘–<ğ‘ğ½}=I{Ë†ğ‘¤ğ‘–<ğ‘ğ½}; for
ğ‘–âˆˆL,Tğ‘–={ğ‘¤|Ë†ğ‘™ğ‘–â‰¤ğ‘¤â‰¤Ë†ğ‘Ÿğ‘–}andminğ‘¤ğ‘–âˆˆTğ‘–I{ğ‘¤ğ‘–<ğ‘ğ½})=I{Ë†ğ‘Ÿğ‘–<
ğ‘ğ½}. Hence from (17) we have:
min
ğ’‘âˆˆPğ‘˜ğ‘›ğ‘ƒğ’‘(ğ‘ğ½)=1
|A|(âˆ‘ï¸
ğ‘–âˆˆWI{Ë†ğ‘¤ğ‘–<ğ‘ğ½}+âˆ‘ï¸
ğ‘–âˆˆLI{Ë†ğ‘Ÿğ‘–<ğ‘ğ½}),(18)
which is exactly ğ‘ƒË†ğ’‘ğ‘(ğ‘ğ½)forË†ğ’‘ğ‘defined in (6), thus (14) holds.
We have thus proven that equation (14) holds for any ğ‘âˆˆB. Now,
we further provide the proof of Remark 4.1. Since problems (2) and
(3) are equivalent, we only need to prove that given ğ‘£, problem (3)
with ambiguity set (4) is equivalent to problem (5) with distribution
(6). According to (14), the problem (3) with ambiguity set (4) can be
written as:
max
ğ‘âˆˆBmin
ğ’‘âˆˆPğ‘˜ğ‘›(ğ‘£âˆ’ğ‘)Â·ğ‘ƒğ’‘(ğ‘)=max
ğ‘âˆˆB{(ğ‘£âˆ’ğ‘)min
ğ’‘âˆˆPğ‘˜ğ‘›{ğ‘ƒğ’‘(ğ‘)}}
=max
ğ‘âˆˆB(ğ‘£âˆ’ğ‘)Â·ğ‘ƒË†ğ’‘ğ‘(ğ‘),(19)
which is equivalent to problem (5) with distribution (6). Hence, the
conclusion of Remark 4.1 is established.
 
1813Robust Auto-Bidding Strategies for Online Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A.2 Proof of Remark 4.2
For convenience, we introduce some additional notations. Firstly,
since both ğ’‘0andğœ–0are given values, we will use Pğ‘¢ğ‘›to denote
Pğ‘¢ğ‘›(ğ’‘0,ğœ–0)in the subsequent proof. Besides, we additionally ap-
pended an element of zero to the end of the probability vector,
that is, ğ’‘0=(ğ‘1
0,ğ‘2
0,...,ğ‘ğ‘€
0,ğ‘ğ‘€+1
0=0). This addition does not alter
the probability distribution of ğ’‘0, hence it does not affect the cor-
rectness of our conclusions. Next, we will prove that Algorithm 1
obtains the optimal solution to problem (12) on domain B.
Firstly, sinceBis a finite set, we only need to prove that for any
ğ‘âˆˆB/{ 0}, Algorithm 1 can provide the accurate value of ğ‘“Pğ‘¢ğ‘›(ğ‘)
(clearlyğ‘“Pğ‘¢ğ‘›(0)=0). Therefore, in the subsequent proof, we fix ğ‘
and denote it as ğ‘=ğ‘ğ½âˆˆB, whereğ½âˆˆ[1,ğ‘€].
Next, for the problem of ğ‘“Pğ‘¢ğ‘›(ğ‘ğ½)=minğ’‘âˆˆPğ‘¢ğ‘›ğ‘ƒğ’‘(ğ‘ğ½), we pro-
vide the form of the probability vector that could attains the op-
timal value, that is, the form of ğ’‘âˆ—âˆˆ Pğ‘¢ğ‘›that could satisfies
ğ‘ƒğ’‘âˆ—(ğ‘ğ½)=minğ’‘âˆˆPğ‘¢ğ‘›ğ‘ƒğ’‘(ğ‘ğ½)for any ğ’‘0,ğœ–0andğ½. We subsequently
refer to ğ’‘âˆ—as the "worst-case form".
We show that for any ğ’‘0,ğœ–0andğ½, there exists some ğ‘¡âˆˆ[1,ğ½]
andğ‘ğ‘¡â‰¤ğ‘ğ‘¡
0such that the worst-case probability vector ğ’‘âˆ—can be
constructed as:
â€¢ğ‘ğ‘¡âˆ—=ğ‘ğ‘¡
â€¢ğ‘ğ‘˜âˆ—=0,âˆ€ğ‘˜âˆˆ[ğ‘¡+1,ğ½](ifğ‘¡<ğ½)
â€¢ğ‘ğ½+1
âˆ—=ğ‘ğ½+1
0âˆ’ğ‘ğ‘¡+Ãğ½
ğ‘˜=ğ‘¡ğ‘ğ‘˜
0
â€¢ğ‘ğ‘˜âˆ—=ğ‘ğ‘˜
0,âˆ€ğ‘˜âˆ‰[ğ‘¡,ğ½+1](that is, for the rest element of ğ’‘âˆ—)
This is consistent with the construction results considered in Fig. 3
of our paper. For the proof of correctness regarding the worst-case
form, we conduct a classification discussion:
(a) If forğ‘¡=1andğ‘ğ‘¡=0we haveğ‘‘(ğ’‘0,ğ’‘âˆ—)â‰¤ğœ–0, then since
ğ‘ƒğ’‘âˆ—(ğ‘ğ½)=Ãğ½
ğ‘˜=1ğ‘ğ‘˜âˆ—=0, it attains the optimal value (since the
probability value should not be negative).
(b) If forğ‘¡=1andğ‘ğ‘¡=0we haveğ‘‘(ğ’‘0,ğ’‘âˆ—)>ğœ–0:
(b.1) We first need to demonstrate that there exists ğ‘¡=ğ‘¡0and
ğ‘ğ‘¡=ğ‘ğ‘¡0such that the worst-case form can satisfy ğ‘‘(ğ’‘0,ğ’‘âˆ—)=ğœ–0.
This is because the distribution distance can be represented as:
ğ‘‘(ğ’‘0,ğ’‘âˆ—)=(ğ‘ğ‘¡
0âˆ’ğ‘ğ‘¡)(ğ½+1âˆ’ğ‘¡)+ğ½âˆ‘ï¸
ğ‘˜=ğ‘¡+1ğ‘ğ‘˜
0(ğ½+1âˆ’ğ‘˜), (20)
in which the summation term equals 0 if ğ‘¡=ğ½. Note that when ğ‘¡=ğ½
andğ‘ğ‘¡=ğ‘ğ‘¡
0, we have ğ’‘âˆ—=ğ’‘0such thatğ‘‘(ğ’‘0,ğ’‘âˆ—)=0. Besides, this
expression is continuous with respect to ğ‘¡andğ‘ğ‘¡. Therefore, there
must exist some ğ‘¡0andğ‘ğ‘¡0such thatğ‘‘(ğ’‘0,ğ’‘âˆ—)=ğœ–0.
(b.2) Fixğ‘¡0andğ‘ğ‘¡0, among all ğ’‘within the constraint ğ‘‘(ğ’‘0,ğ’‘)â‰¤
ğœ–0, we show that ğ’‘âˆ—attains the optimal value of ğ‘ƒğ’‘(ğ‘ğ½). Note that:
ğ‘ƒğ’‘(ğ‘ğ½)=ğ½âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜, (21)
we show that if there is some Ë†ğ’‘such thatğ‘ƒË†ğ’‘(ğ‘ğ½)<ğ‘ƒğ’‘âˆ—(ğ‘ğ½), we
haveğ‘‘(ğ’‘0,Ë†ğ’‘)>ğœ–0such that Ë†ğ’‘âˆ‰Pğ‘¢ğ‘›. We first construct Ë†ğ’‘âˆ—from
Ë†ğ’‘that satisfies:
â€¢Ë†ğ‘ğ‘˜âˆ—=Ë†ğ‘ğ‘˜,âˆ€ğ‘˜âˆˆ[1,ğ½]
â€¢Ë†ğ‘ğ½+1
âˆ—=ğ‘ğ½+1
0+Ãğ‘€+1
ğ‘˜=ğ½+1(Ë†ğ‘ğ‘˜âˆ’ğ‘ğ‘˜
0)
â€¢Ë†ğ‘ğ‘˜âˆ—=ğ‘ğ‘˜
0,âˆ€ğ‘˜âˆˆ[ğ½+2,ğ‘€+1](ifğ½<ğ‘€)We haveğ‘ƒË†ğ’‘âˆ—(ğ‘ğ½)=ğ‘ƒË†ğ’‘(ğ‘ğ½)andğ‘‘(ğ’‘0,Ë†ğ’‘)â‰¥ğ‘‘(ğ’‘0,Ë†ğ’‘âˆ—). The proof
of the latter result, although intuitive, is somewhat verbose. Intu-
itively, for the parts with indexes greater than ğ½, the probability of
Ë†ğ’‘being different from ğ’‘0is concentrated at Ë†ğ‘ğ½+1
âˆ—. This reduces the
probability transfer within the indexes greater than ğ½and decreases
the distance for the remaining parts to transfer probabilities to this
portion. Here, for brevity, we skip the intricate proof.
Hence we only needs to prove that ğ‘‘(ğ’‘0,Ë†ğ’‘âˆ—)>ğœ–0. Note that
ğ‘ƒË†ğ’‘âˆ—(ğ‘ğ½)=ğ‘ƒË†ğ’‘(ğ‘ğ½)<ğ‘ƒğ’‘âˆ—(ğ‘ğ½), and Ë†ğ‘ğ‘˜âˆ—=ğ‘ğ‘˜âˆ—=ğ‘ğ‘˜
0for allğ‘˜âˆˆ[ğ½+
2,ğ‘€+1](ifğ½<ğ‘€), we have Ë†ğ‘ğ½+1
âˆ—>ğ‘ğ½+1
âˆ—. Assuming that the
optimal transfer quantities in problem (11) are Ë†ğ’…andğ’…forË†ğ’‘âˆ—and
ğ’‘âˆ—respectively. Then according to the definition of the Wasserstein
distance, by expanding the expression of the Wasserstein distance
between distributions ğ’‘0andË†ğ’‘âˆ—, we can obtain:
ğ‘‘(ğ’‘0,Ë†ğ’‘âˆ—)=ğ‘€+1âˆ‘ï¸
ğ‘–=1ğ‘€+1âˆ‘ï¸
ğ‘—=1Ë†ğ‘‘ğ‘–ğ‘—|ğ‘–âˆ’ğ‘—|
â‰¥ğ½âˆ‘ï¸
ğ‘–=1Ë†ğ‘‘ğ‘–(ğ½+1)(ğ½+1âˆ’ğ‘–)
â‰¥ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1Ë†ğ‘‘ğ‘–(ğ½+1)(ğ½+1âˆ’ğ‘–)+ğ‘¡âˆ‘ï¸
ğ‘–=1Ë†ğ‘‘ğ‘–(ğ½+1)(ğ½+1âˆ’ğ‘¡),(22)
where the firstâ‰¥hold since we only consider a subset of the transfer
quantities, that is, the probabilities transferred from indexes in [1,J]
to index J+1, and the second â‰¥hold forğ½+1âˆ’ğ‘–â‰¥(ğ½+1âˆ’ğ‘¡),âˆ€ğ‘–â‰¤ğ‘¡.
For the first term, according to the definition of Ë†ğ’‘âˆ—, we can expand
it and obtain:
ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1Ë†ğ‘‘ğ‘–(ğ½+1)(ğ½+1âˆ’ğ‘–)
=ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1Ë†ğ‘ğ‘–
0(ğ½+1âˆ’ğ‘–)+ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1(Ë†ğ‘‘ğ‘–(ğ½+1)âˆ’ğ‘ğ‘–
0)(ğ½+1âˆ’ğ‘–)
â‰¥ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1Ë†ğ‘ğ‘–
0(ğ½+1âˆ’ğ‘–)+ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1(Ë†ğ‘‘ğ‘–(ğ½+1)âˆ’ğ‘ğ‘–
0)(ğ½+1âˆ’ğ‘¡),(23)
where theâ‰¥holds for Ë†ğ‘‘ğ‘–(ğ½+1)âˆ’ğ‘ğ‘–
0â‰¤0,âˆ€ğ‘–. Organizing the derivation
of (22) and (23), We now obtain a lower bound for the Wasserstein
distance between distributions ğ’‘0andË†ğ’‘âˆ—:
ğ‘‘(ğ’‘0,Ë†ğ’‘âˆ—)â‰¥ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1ğ‘ğ‘–
0(ğ½+1âˆ’ğ‘–)+(ğ½âˆ‘ï¸
ğ‘–=1Ë†ğ‘‘ğ‘–(ğ½+1)âˆ’ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1ğ‘ğ‘–
0)(ğ½+1âˆ’ğ‘¡),(24)
where the remaining transfer quantities to consider is only those
from indexes in [1,J] to index J+1, and our goal is to compare the
size of the right-hand expression in (24) with that of ğ‘‘(ğ’‘0,ğ’‘âˆ—). Note
that we only need to consider the sum of these transfer quantities,
which is easy to compare:
ğ½âˆ‘ï¸
ğ‘–=1Ë†ğ‘‘ğ‘–(ğ½+1)=Ë†ğ‘ğ½+1
âˆ—âˆ’ğ‘ğ½+1
0>ğ‘ğ½+1
âˆ—âˆ’ğ‘ğ½+1
0=ğ½âˆ‘ï¸
ğ‘–=ğ‘¡ğ‘‘ğ‘–(ğ½+1), (25)
and according to the definition of ğ’‘âˆ—, we haveğ‘‘ğ‘–(ğ½+1)=ğ‘ğ‘–
0for any
ğ‘–âˆˆ[ğ‘¡+1,ğ½]. We now can further derive the right-hand expression
 
1814KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qilong Lin, Zhenzhe Zheng, & Fan Wu
in inequality (24) as follows:
ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1ğ‘ğ‘–
0(ğ½+1âˆ’ğ‘–)+(ğ½âˆ‘ï¸
ğ‘–=1Ë†ğ‘‘ğ‘–(ğ½+1)âˆ’ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1ğ‘ğ‘–
0)(ğ½+1âˆ’ğ‘¡)
>ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1ğ‘ğ‘–
0(ğ½+1âˆ’ğ‘–)+(ğ½âˆ‘ï¸
ğ‘–=ğ‘¡ğ‘‘ğ‘–(ğ½+1)âˆ’ğ½âˆ‘ï¸
ğ‘–=ğ‘¡+1ğ‘ğ‘–
0)(ğ½+1âˆ’ğ‘¡)
=ğ½âˆ‘ï¸
ğ‘–=ğ‘¡ğ‘‘ğ‘–(ğ½+1)(ğ½+1âˆ’ğ‘–)
=ğ‘‘(ğ’‘0,ğ’‘âˆ—)
=ğœ–0,(26)
where the penultimate equation is because only when ğ‘—=ğ½+1
andğ‘–âˆˆ[ğ‘¡,ğ½]we haveğ‘‘ğ‘–ğ‘—â‰ 0. Combining (24) and (26) we have
ğ‘‘(ğ’‘0,Ë†ğ’‘âˆ—)>ğœ–0. Hence the â€œworst-case formâ€ ğ’‘âˆ—attains the optimal
value ofğ‘ƒğ’‘(ğ‘ğ½)in case (b).Combining (a) and (b), we have proven that for any ğ’‘0,ğœ–0andğ½,
the â€œworst-case formâ€ ğ’‘âˆ—satisfiesğ‘ƒğ’‘âˆ—(ğ‘ğ½)=minğ’‘âˆˆPğ‘¢ğ‘›ğ‘ƒğ’‘(ğ‘ğ½).
Finally, we can prove the correctness of Remark 4.2 by demon-
strating that for each ğ‘ğ½, Algorithm 1 provides the correct ğ‘¡0and
ğ‘ğ‘¡0in the previous discussion.
Forğ‘¡0, note that at each end of the step in the â€œwhileâ€ loop, ğ’’0
satisfies the â€œworst-case formâ€ with ğ‘¡=ğ‘˜andğ‘ğ‘¡=0, and the loop
stops for the first time ğ‘‘(ğ’‘0,ğ’’0)>ğœ–, henceğ‘¡0=ğ‘˜upon exiting the
â€œwhileâ€ loop because ğ‘‘(ğ’‘0,ğ’’0)increases as ğ‘¡=ğ‘˜decreases.
Forğ‘ğ‘¡0, since we have got ğ‘¡0, we can calculate its value through
the Wasserstein distance, as shown in line 9 in Algorithm 1.
Therefore, at this point, we have the accurate value of ğ‘“Pğ‘¢ğ‘›(ğ‘ğ½)=Ãğ‘¡0âˆ’1
ğ‘˜=1ğ‘ğ‘˜
0+ğ‘ğ‘¡0, as shown in line 10 in Algorithm 1 (note that ğ‘ğ‘˜
0=0
forğ‘˜âˆˆ[ğ‘¡0+1,ğ½]).
Thus, Algorithm 1 obtains the optimal solution to problem (12)
on domainB, and the conclusion in Remark 4.2 have been proven.
 
1815