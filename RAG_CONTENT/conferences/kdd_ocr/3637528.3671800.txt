Disentangled Multi-interest Representation Learning for
Sequential Recommendation
Yingpeng Du
dyp1993@pku.edu.cn
Nanyang Technological University
SingaporeZiyan Wang
wang1753@e.ntu.edu.sg
Nanyang Technological University
SingaporeZhu Sunâˆ—
sunzhuntu@gmail.com
Singapore University of Technology
and Design
Singapore
Yining Ma
yiningma@u.nus.edu
Nanyang Technological University
SingaporeHongzhi Liu
liuhz@pku.edu.cn
Peking University
Beijing, ChinaJie Zhang
zhangj@ntu.edu.sg
Nanyang Technological University
Singapore
ABSTRACT
Recently, much effort has been devoted to modeling usersâ€™ multi-
interests (aka multi-faceted preferences) based on their behaviors,
aiming to accurately capture usersâ€™ complex preferences. Existing
methods attempt to model each interest of users through a distinct
representation, but these multi-interest representations easily col-
lapse into similar ones due to a lack of effective guidance. In this
paper, we propose a generic multi-interest method for sequential
recommendation, achieving disentangled representation learning
of diverse interests technically and theoretically. To alleviate the
collapse issue of multi-interests, we propose to conduct item parti-
tion guided by their likelihood of being co-purchased in a global
view. It can encourage items in each group to focus on a discrim-
inated interest, thus achieving effective disentangled learning of
multi-interests. Specifically, we first prove the theoretical connec-
tion between item partition and spectral clustering, demonstrating
its effectiveness in alleviating item-level and facet-level collapse
issues that hinder existing disentangled methods. To efficiently
optimize this problem, we then propose a Markov Random Field
(MRF)-based method that samples small-scale sub-graphs from two
separate MRFs, thus it can be approximated with a cross-entropy
loss and optimized through contrastive learning. Finally, we per-
form multi-task learning to seamlessly align item partition learning
with multi-interest modeling for more accurate recommendation.
Experiments on three real-world datasets show that our method
significantly outperforms state-of-the-art methods and can flexibly
integrate with existing multi-interest models as a plugin to enhance
their performances.
âˆ—Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671800CCS CONCEPTS
â€¢Information systems â†’Collaborative filtering; Recom-
mender systems.
KEYWORDS
Recommender Systems; Multi-Interests; Disentangled Learning;
Item Partition.
ACM Reference Format:
Yingpeng Du, Ziyan Wang, Zhu Sun, Yining Ma, Hongzhi Liu, and Jie
Zhang. 2024. Disentangled Multi-interest Representation Learning for Se-
quential Recommendation. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671800
1 INTRODUCTION
Sequential recommendation, a key part of Recommender Systems
(RSs), focuses on modeling usersâ€™ dynamic preferences on items
based on their sequential behaviors [ 39]. In the past few years,
various techniques have been adopted for usersâ€™ preference learning,
such as Markov Chains [ 29], Recurrent Neural Networks (RNN) [ 10],
attention mechanism [ 18], etc. However, these methods only model
the userâ€™s preference with a single representation, which ignores the
discrimination of different interests of users. Figure 1 illustrates the
userâ€™s sequential purchase behaviors on an online shopping website,
which indicates that the user usually shows multi-interests (aka
multi-faceted preferences) on different kinds of items. Therefore,
multi-interest methods [ 2,3,19,46,47] gain their popularity in
RSs, which aims to capture usersâ€™ different interests behind their
behaviors and model each interest by a representation.
According to the information utilization for multi-interest mod-
eling, existing methods can be categorized into two classes. One
class of existing methods exploits auxiliary knowledge such as
item categories to guide multi-interest learning [ 3,22], but the
high-quality and relevant auxiliary information may not always be
accessible. The other class adopts capsule networks or the attention
mechanism to purely derive from usersâ€™ behaviors, implicitly allo-
cating representations of user-engaged items into different interest
facets [ 2,46]. However, these multi-interest representations easily
collapse into similar ones, which hardly capture the userâ€™s distinct
interests in real-world applications.
 
677
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingpeng Du et al.
Group  1
Group  2
Group  3
Usersâ€™sequential behaviors
Multi -interest
modeling 
(C)  Item 
partition
Global item graph(A) item -level collapse :
Misa llocation of some relevant 
items into  the different interes t
(B) Facet -level collapse :
 Multiple facets collapse into one facet Regularization Regularization
Re-adjustRe-adjust
User Items Partition Co-occurrence User Items Partition Co-occurrence
Figure 1: An illustrative example of usersâ€™ multi-interest mod-
eling with (A) item-level collapse; (B) Facet-level collapse;
and (C) Item Partition. The global item graph measures the
co-occurrence patterns of items.
To this end, the majority of existing methods propose to alleviate
the collapse issues by regularization strategies, e.g., minimizing the
correlations of interests [ 42], penalizing the sparsity of the item-
to-interest routing matrix [ 47], and limiting the norm of interest
representations [ 26,36]. Nevertheless, these strategies only suggest
â€œwhat should not be doneâ€ (e.g., avoiding the similarity of different
interests) for multi-interest learning, which does not directly guar-
antee that â€œwhat should be doneâ€ (i.e., allocating relevant items
into the same interest of users) in a proactive manner. As a result,
they may suffer from the item-level collapse of multi-interest
(Figure 1. A) due to the partial misallocation of items. Other meth-
ods [ 27,51] attempt to proactively re-adjust the item-to-interest
alignment to learn distinct interest representations. Specifically,
they first identify representative items exhibiting similarity to a
specific interest, and then pull the interest closer to these items
while keeping away from others. Nevertheless, these methods may
lead to facet-level collapse of multi-interest (Figure 1. B), e.g.,
multiple facets almost collapse into one facet, if two interests share
the overlapped representative items in the training phase. To resolve
these issues, we aim to partition relevant items into similar groups
while ensuring each group is discriminated from the others as in
Figure 1 (C). Therefore, items in each group exert a concentrated
influence on the discriminated interest and show less impact on
others through the attention and routing mechanism, contributing
to better disentangled multi-interest representation learning.
Although straightforwardness of the idea, there remain chal-
lenges in the item partition problem: first, how to formulate it for
alleviating both item-level and facet-level collapse issues without
auxiliary information; second, how to conduct efficient partition
learning to align with multi-interest recommendation tasks.
To address the first challenge, we propose to perform the item
partition based on the global user-item interactions. Specifically,
we first partition items into similar groups if they are likely to
be co-purchased by users [ 16], because these items may comple-
ment each other to meet usersâ€™ specific requirements, e.g., laptop,
mouse, and adapter. Therefore, the item partition helps to model the
discriminated interests of users, contributing to disentangled repre-
sentation learning of multi-interests. Then, we formulate the itempartition problem theoretically equivalent to spectral clustering
on a global item graph that measures the co-occurrence patterns
of items, which consists of two goals to seamlessly alleviate the
item- and facet-level collapse issues. In particular, we prove the
first goal is cutting off the item edges between different groups
with the lowest price. Consequently, any misallocation of items (i.e.,
item-level collapse) leads to a higher price, so different groups will
be discriminated from each other. The second goal is to maximize
an entropy term to balance the item number for all groups, there-
fore any collapsing of two discriminated groups (i.e., facet-level
collapse) will decrease the entropy.
To address the second challenge, we propose a Markov Random
Field (MRF)-based optimization method to approximate the item
partition problem with small-scale sub-graphs for efficiency, as it
is costly to conduct spectral clustering directly [ 37,43]. Specifi-
cally, we first define an item-confidence MRF and an item-partition
MRF based on global co-occurrence patterns of items and item par-
tition, respectively. Then, we sample sub-graphs from these two
MRFs to reformulate the item partition problem as a cross-entropy
loss inspired by [ 34], which can be solved through contrastive
learning between these sub-graphs efficiently. To further align the
item partition learning with multi-interest modeling, we propose
conducting multi-task learning with the shared representations of
itemsâ€™ partitions and embeddings, facilitating the item partition to
help disentangle learning of usersâ€™ multi-interests.
In summary, we propose a generic Disentangled-based Multi-
Interest Representation method (named DisMIR) for sequential rec-
ommendation, alleviating both item-level and facet-level collapse of
multi-interests without auxiliary information. The proposed item
partition encourages items in each group to focus on a discrimi-
nated interest, thus achieving effective disentangled learning of
multi-interests. We also establish its connection with spectral clus-
tering theoretically and provide an efficient way for its optimization
practically. Experiments on three real-world datasets show that the
proposed method DisMIR not only consistently outperforms state-
of-the-art methods, but also can flexibly integrate with existing
multi-interest models as a plugin to enhance their performances.
2 RELATED WORK
2.1 Sequential Recommendation
Early work on sequential recommendation mainly employed the
Markov chains to model the sequential patterns of usersâ€™ behav-
iors [ 13,29]. With the recent advances in deep learning and informa-
tion technology, massive methods attempt to use different models
and information sources for sequential recommendation. One class
of existing methods explores and improves deep model architec-
ture to enhance the model generalization capability, e.g., RNN [ 10],
Memory Networks[ 8,17], attentional mechanism [ 18,31,49], and
graph-based models [ 4,25,48]. The other class of existing methods
explores auxiliary information such as itemsâ€™ attributes to enhance
the representations of usersâ€™ preferences [ 1,17,50]. For example,
Bai et al . [1] employ a hierarchical architecture to integrate at-
tribute information of items. However, these methods rely on a
singular representation to model user preferences, which overlooks
the discrimination of different interests of users and is insufficient
to capture usersâ€™ multi-faceted preferences.
 
678Disentangled Multi-interest Representation Learning for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
2.2 Multi-interest Recommendation
Recently, multi-interest learning-based methods have demonstrated
significant promise in enhancing recommendation performance.
According to the information utilization for usersâ€™ multi-interest
modeling, existing methods can be categorized into two classes:
auxiliary knowledge-based multi-interest methods and interaction-
based multi-interest methods.
The former methods attempt to guide multi-interest learning
with the help of auxiliary knowledge such as usersâ€™ profiles [ 3],
itemsâ€™ categories [ 11,20], multi-type behaviors [ 6,21,28], knowl-
edge graphs [ 22,44,45], usersâ€™ groups [ 23], and multi-modal infor-
mation [ 40,41]. For example, Liu et al . [22] propose to align implicit
disentangled representations that are learned from user-item inter-
actions with the explicit disentangled representations based on a
knowledge graph. Chen et al . [6]propose a curriculum disentangled
recommendation model to learn disentangled representations from
usersâ€™ multi-feedback (e.g., click, unclick, and dislike). However,
these methods rely on high-quality auxiliary information that may
not always be available. The latter methods adopt the attention
mechanism [ 24,26,46] or capsule network [ 2,19,35,47] to purely
derive from usersâ€™ engaged items without auxiliary information.
For example, Li et al . [19] propose to capture usersâ€™ various interests
using dynamic routing through Capsule Network [ 30]. Tian et al .
[35] propose a multi-interest learning method with a combination
of sequential capsule network and graph convolutional aggrega-
tion, aiming to capture multi-interest and multi-level preferences
of users. Although these methods have demonstrated significant
promise in enhancing recommendation performance, multi-interest
facets in these methods may easily collapse into similar ones and
degrade into a single-interest [ 12], making it ineffective in cap-
turing usersâ€™ multi-interests. In addition, these methods provide
limited theoretical analysis about alleviating the collapse of usersâ€™
multi-interest representations.
2.3 Disentangled Representation Learning for
Multi-interest Recommendation
To alleviate the collapse issue, many existing methods aim to achieve
disentangled representation learning for usersâ€™ multi-interests re-
cently. The majority of existing methods propose to alleviate the
collapse issues by additional regularization, e.g., minimizing the
correlations of interests [ 7,42], penalizing the sparsity of the item-
to-interest routing matrix [ 47], and limiting the norm of interest
representations [ 26,36]. For example, Wang et al . [42] propose to
minimize the correlation distance [ 32,33] of multi-faceted repre-
sentation of users and items, which encourages the factor-aware
representations to be independent. Xie et al . [47] introduce the vari-
ance regularizer of the item-to-interest routing matrix, and penalize
it for alleviating routing collapse in capsule networks.
Lreg=||ğ‘‘ğ‘–ğ‘ğ‘”(ğ‘«)||2,ğ‘«=(ğ‘©âˆ’Â¯ğ‘©)âŠ¤(ğ‘©âˆ’Â¯ğ‘©), (1)
where Â¯ğ‘©denotes the mean of the routing weights ğ‘©along the first
axis, andğ‘‘ğ‘–ğ‘ğ‘”(Â·)represents the aggregation of diagonal elements
for a matrix. However, these passive regularization strategies are
not essential for ensuring effective item-to-interest alignment, i.e.,
allocating relevant items into the same interest of users, which
may suffer from the item-level collapse issue. The other methodspropose to re-adjust item-to-interest alignment based on the repre-
sentations of multi-interests [ 27,51]. For instance, Zhang et al . [51]
introduce a backward-flow mechanism to identify representative
items exhibiting similarity to a specific interest, and then pulled
the interest closer to these items while keeping away from others,
Lreg=âˆ’âˆ‘ï¸
ğ’exp(ğ’,ğ’†ğ‘–)
exp(ğ’,ğ’†ğ‘–)+Ã
ğ‘—exp(ğ’,ğ’†ğ‘—), (2)
where ğ’,ğ’†ğ‘–, and ğ’†ğ‘—denote the interest, representative item ğ‘–, and
another item ğ‘—, respectively. However, the userâ€™s multi-interests
may suffer from the facet-level collapse issue in these methods,
because some interests can coincidentally share the overlapped
representative items in the training phase.
3 PRELIMINARY
3.1 Problem Formulation
LetUandIrepresent the sets of ğ‘€users andğ‘items, respectively.
Each userğ‘¢âˆˆ U has engaged a series of items Eğ‘¢=[ğ‘’ğ‘¡|ğ‘¡=
1,Â·Â·Â·,|Eğ‘¢|]in the chronological order, where ğ‘’ğ‘¡âˆˆIdenotes the
ğ‘¡-th item that the user engaged. In this paper, we aim to effectively
provide a recommendation list for the user, which contains items
that he/she is likely to engage with in the near future. The main
notations used throughout the paper are summarized in Table 6.
3.2 Capsule Networks for Multi-Interests
Among multi-interest methods, capsule networks [ 30] have gained
popularity for multi-interest modeling due to their effectiveness [ 2,
19,35,47]. Specifically, the capsule network can generate usersâ€™
ğ¹-interest representations derived from their recent ğœ…sequential be-
haviorsEâ€²ğ‘¢=[ğ‘’|Eğ‘¢|âˆ’ğœ…+1,Â·Â·Â·,ğ‘’|Eğ‘¢|], whereğ¹denotes the pre-defined
number of usersâ€™ multi-interests. Specifically, the ğ‘“-th interest cap-
suleğ’ğ‘“âˆˆRğ‘‘can be calculated as follows:
ğ’ğ‘“=âˆ‘ï¸ğœ…
ğ‘–=1ğ‘ğ‘–ğ‘“Â·ğ‘¾Â·ğ’†ğ‘–, ğ‘“ =1,Â·Â·Â·,ğ¹,
where ğ’†ğ‘–âˆˆRğ‘‘denotes embeddings of the ğ‘–-th items in the userâ€™s
sequential behaviors Eâ€²ğ‘¢, andğ‘‘denotes the dimension of latent
space. ğ‘¾âˆˆRğ‘‘Ã—ğ‘‘is the transformation matrix, and ğ‘ğ‘–ğ‘“denotes the
probability (the routing weight) of item ğ‘’ğ‘–belonging to the ğ‘“-th
interest capsule. The routing weight ğ‘ğ‘–ğ‘“is calculated by the softmax
operation of the routing logits ğ‘ğ‘–ğ‘“that measures the similarity
between the item embedding and squashed vector ğ’—ğ’‡, i.e.,
ğ‘ğ‘–ğ‘“=ğ‘’ğ‘¥ğ‘(ğ‘ğ‘–ğ‘“)
Ãğ¹
ğ‘“=1ğ‘’ğ‘¥ğ‘(ğ‘ğ‘–ğ‘“), ğ‘ğ‘–ğ‘“=ğ’†âŠ¤
ğ‘–Â·ğ‘¾Â·ğ’—ğ’‡,ğ’—ğ’‡=||ğ’ğ‘“||2
||ğ’ğ‘“||2+1ğ’ğ‘“
||ğ’ğ‘“||,
where the values of routing logits ğ‘ğ‘–ğ‘“are initialized to zeros with
the repeated routing process.
4 THE PROPOSED THEORY AND METHOD
The overall architecture of the proposed method is shown in Figure
2. First, we propose to formulate the item partition problem and
prove its relationship to the spectral cluster (Section 4.1). Then,
to efficiently optimize this problem, we propose to approximate it
with a cross-entropy loss based on MRFs, which can be optimized
by contrastive learning (Section 4.2). Finally, we conduct multi-
interest learning to align item partition learning with multi-interest
learning for more accurate recommendation (Section 4.3).
 
679KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingpeng Du et al.
Confidence MRF  STheorem 4
Contrastive Learning
Sub-sample
Users â€™ Multi -interest learningPartition MRF  ZInterest 1 Interest FTheorem 3
Cross-entropy
EP Y|S log(P(Y|Z)) 
PositiveNegatives
AlignmentItem 
embeddingsItem
 partition
Item -partition optimization
Figure 2: The architecture of the disentangled multi-interest
representation learning framework.
4.1 Item Partition Formulation
To guide usersâ€™ disentangled multi-interest learning, we propose to
perform the item partition based on global user-item interactions.
Specifically, we organize the user-item interactions into an item-
confidence graphG=(V,E). The nodes ofGare all items, and
their edges measure the confidence of items inspired by frequency
patterns of usersâ€™ behaviors [ 16], i.e.,ğ‘†ğ‘–ğ‘—=ğ‘ğ‘œ(ğ‘–,ğ‘—)Ãğ‘
ğ‘—=1ğ‘ğ‘œ(ğ‘–,ğ‘—), whereğ‘ğ‘œ(ğ‘–,ğ‘—)
denotes the number of users who interact with the item ğ‘–and the
itemğ‘—simultaneously, and we haveÃğ‘
ğ‘—=1ğ‘†ğ‘–ğ‘—=1. To comprehen-
sively model the item partition in a generic framework, we propose
two kinds of partition scenarios, namely Non-overlapped Partition
and Overlapped Partition.
4.1.1 Non-overlapped partition. Non-overlapped partition handles
the scenario where each item only belongs to one specific group.
To effectively partition items into multiple groups (e.g., ğ¾), we
propose to partition items based on the item-confidence graph,
which contains global information on usersâ€™ behavioral patterns:
OPT=maxâˆ‘ï¸
ğ‘–â‰ ğ‘—âˆ‘ï¸ğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜Â·ğ‘†ğ‘–ğ‘—âˆ’âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—âˆ‘ï¸ğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜,
(3)âˆ‘ï¸ğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜=1, ğ‘¤ğ‘–ğ‘˜âˆˆ{0,1},
where partition weight ğ‘¤ğ‘–ğ‘˜=1means that the item ğ‘–is partitioned
into groupğ‘˜otherwiseğ‘¤ğ‘–ğ‘˜=0.Ãğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜checks whether
the pair of items ğ‘–andğ‘—are partitioned into the same group. The
first term aims to partition items with high confidence ğ‘†ğ‘–ğ‘—into
the same group as much as possible. The second term punishes
trivial solutions, e.g., partitioning all items into the same group. To
explain how the item partition problem helps to alleviate both item-
level and facet-level collapse issues, we illustrate its relationship to
spectral clustering [38] as follows1.
Theorem 1. (Non-overlapped spectral clustering) The item
partition problem in Equation (3) is equivalent to spectral clustering
based on the item-confidence graph,
ğ‘‚ğ‘ƒğ‘‡âˆâˆ’âˆ‘ï¸ğ¾
ğ‘˜=1ğ¶ğ‘¢ğ‘¡(Ağ‘˜,Â¯Ağ‘˜)+ğ‘Â·ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(|Ağ‘˜|/ğ‘),
1Due to space limitation, we provide the proof of all Theorems and Lemmas in the
Appendixwhere{A1,Â·Â·Â·,Ağ¾}denote the item partition groups which satisfy
âˆªğ‘˜Ağ‘˜=IandAğ‘˜1âˆ©Ağ‘˜2=âˆ….Â¯Ağ‘˜=Iâˆ’Ağ‘˜denote the complement
set of Â¯Ağ‘˜.ğ¶ğ‘¢ğ‘¡(ğ´ğ‘˜1,ğ´ğ‘˜2)calculates the price (total weights) of cutting
off all edges between Ağ‘˜1and nodesAğ‘˜2.
For the item-level collapse issue, any misallocation of items
leads to a higher price. Specifically, if the item ğ‘–is more relevant to
items in groupAğ‘˜1than groupAğ‘˜2, i.e.,ğ¶ğ‘¢ğ‘¡(ğ‘–,Ağ‘˜1)>ğ¶ğ‘¢ğ‘¡(ğ‘–,Ağ‘˜2),
we have
ğ¶ğ‘¢ğ‘¡(Ağ‘˜1+ğ‘–,Ağ‘˜2âˆ’ğ‘–)=ğ¶ğ‘¢ğ‘¡(Ağ‘˜1,Ağ‘˜2)âˆ’ğ¶ğ‘¢ğ‘¡(Ağ‘˜1,ğ‘–)+ğ¶ğ‘¢ğ‘¡(Ağ‘˜2,ğ‘–)
<ğ¶ğ‘¢ğ‘¡(Ağ‘˜1,Ağ‘˜2)+ğ¶ğ‘¢ğ‘¡(Ağ‘˜1,ğ‘–)âˆ’ğ¶ğ‘¢ğ‘¡(Ağ‘˜2,ğ‘–)=ğ¶ğ‘¢ğ‘¡(Ağ‘˜1âˆ’ğ‘–,Ağ‘˜2+ğ‘–),
which indicates that misallocating the item ğ‘–in groupAğ‘˜2will in-
troduce a higher price. As the first goal is cutting off the item edges
between different groups with the lowest prices,Ãğ¾
ğ‘˜=1ğ¶ğ‘¢ğ‘¡(Ağ‘˜,
Â¯Ağ‘˜)=2Â·Ãğ¾
ğ‘˜1=1Ãğ¾
ğ‘˜2=ğ‘˜1+1ğ¶ğ‘¢ğ‘¡(Ağ‘˜1,Ağ‘˜2), it can alleviate the item-
level collapse issue by forcing items from different groups to dis-
criminate from each other while allocating relevant items into the
same group. For the facet-level collapse issue, the second goal
is to maximize the entropy term of itemsâ€™ numbers among different
groups, balancing the item number for all groups. Specifically, sup-
pose we have two groups with ğ‘1andğ‘€1items, where ğ‘1â‰ˆğ‘€1.
The first goal is to cut off the item edges between different groups
with the lowest price, however, it may generate an imbalance so-
lution, i.e.,ğ‘2<<ğ‘€2, andğ‘2+ğ‘€2=ğ‘1+ğ‘€1, with fewer edges
asğ‘2âˆ—ğ‘€2<ğ‘1âˆ—ğ‘€1. It may lead to facet-level collapse because
the facet with ğ‘2items nearly disappears. Therefore, balancing
the item number of discriminated groups can prevent imbalance
solutions and alleviate the facet-level collapse issue. Therefore, any
collapsing of two discriminated groups will decrease the entropy.
4.1.2 Overlapped partition. In real-world scenarios, an item usually
belongs to several groups at the same time [ 22], e.g., pumpkins
are not only limited to the kitchen but also used as decorations
during Halloween. To this end, we propose to model the overlapped
partition weight ğ‘¤ğ‘–ğ‘˜by relaxing it into a real value Ras follows.
OPT=maxâˆ‘ï¸
ğ‘–â‰ ğ‘—ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜Â·ğ‘†ğ‘–ğ‘—âˆ’âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—exp(ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜),(4)
whereğ‘¤ğ‘–ğ‘˜âˆˆRand we add the exp(Â·)to ensure the positive value
in regularization term. In this case, the partition goal is equivalent
to overlapped spectral clustering [37, 43] as follows.
Theorem 2. (Overlapped spectral clustering) Maximizing
Equation (4) with ğ‘¤ğ‘–ğ‘˜âˆˆRis equivalent to conducting overlapped
spectral clustering based on the item-confidence graph, i.e.,
ğ‘‚ğ‘ƒğ‘‡ =âˆ’ğ‘¡ğ‘Ÿ(ğ‘¾âŠ¤ğ‘³ğ‘¾)+âˆ‘ï¸
ğ‘–logexp(Ã
ğ‘˜ğ‘¤ğ‘–,ğ‘˜Â·ğ‘¤ğ‘–,ğ‘˜)Ã
ğ‘—exp(Ã
ğ‘˜ğ‘¤ğ‘–,ğ‘˜Â·ğ‘¤ğ‘—,ğ‘˜),
where ğ‘³=ğ‘°âˆ’ğ‘ºdenotes the Laplacian matrix of matrix ğ‘º. It is
equivalent to doing spectral clustering with a repulsion regularization.
In summary, the goal of item partition is to allocate items in a
spectral clustering manner, which helps to partition relevant items
into similar groups while ensuring each group is discriminated from
the others. With the supervision of item partitions, items of each
group can exert a concentrated influence on the specific interest and
show less impact on others with the attention or routing mechanism,
contributing to better disentangled multi-interest learning.
 
680Disentangled Multi-interest Representation Learning for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.2 MRF-based Optimization
In the real-world application of RSs, the number of candidate items
is usually very large, so it is costly to conduct spectral clustering
directly [ 37,43]. To tackle this challenge, we propose an MRF-
based optimization method with sub-graph sampling for efficiency
purposes inspired by [ 34], as comparing the whole graphs is costly.
Specifically, we first define an item-confidence MRF and an item-
partition MRF, and then introduce a probability distribution for
sub-graph sampling of them. Second, we propose to approximate
the item partition problem with cross-entropy loss between sub-
graphs sampled from these two MRFs. Finally, we optimize the
cross-entropy loss in a contrastive learning manner.
4.2.1 Induced probability distributions on MRF. To guide the item
partition with global frequency patterns of usersâ€™ behaviors, we
propose to define an item-confidence MRF and an item-partition
MRF in a symmetric way. Specifically, the item-confidence MRF
models the frequency patterns of usersâ€™ behaviors, whose nodes are
all items and edges are defined to measure the confidence between
nodesğ‘†ğ‘–ğ‘—. The item-partition MRF aims to model the item partition,
whose nodes are all items and edges are defined to measure the
possibility of two items belonging to the same group, i.e., ğ‘ğ‘–ğ‘—=
exp(Ãğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—ğ‘˜). To reduce the complexity of large-scale MRFs,
we introduce an induced probability distribution P(Â·|ğ‘¹)for sub-
graph sampling inspired by [ 34], where nodes in sampled subgraphs
have only one out-going edge.
Definition 1. Given an MRF with ğ‘›variables with their relations
ğ‘¹âˆˆRğ‘›Ã—ğ‘›+, we define a probability distribution P(ğ’€|ğ‘¹)on the directed
unweighted subgraph ğ’€âˆˆ{0,1}ğ‘›Ã—ğ‘›with only one out-going edge
for each node, i.e.,
P(ğ’€|ğ‘¹)=ğ‘‰ğ‘…(ğ’€)Â·Î©(ğ’€)Ã
ğ‘ŒâˆˆYğ‘‰ğ‘…(ğ’€)Â·Î©(ğ’€),
whereY={ğ’€âˆˆ{0,1}ğ‘›Ã—ğ‘›|ğ‘Œğ‘–,ğ‘–=0}denote all possible graphs with
ğ‘›nodes. Î©(ğ’€)â‰œÃ
ğ‘–I(Ã
ğ‘—ğ‘Œğ‘–ğ‘—=1)checks if each node has exactly
one out-going edge. ğ‘‰ğ‘…(ğ’€)â‰œÃ
ğ‘Œğ‘–ğ‘—=1ğ‘…ğ‘–ğ‘—combines the scores of each
edge in the subgraph ğ’€.
4.2.2 Approximation. To estimate the proposed item partition prob-
lem, we first propose two lemmas to reformulate its two terms in
Equation (4) based on MRFs.
Lemma 4.1. The first term in the item partition problem can be
formulated as the expectation of P(Â·|ğ‘º)about ğ’:
âˆ‘ï¸
ğ‘–â‰ ğ‘—âˆ‘ï¸ğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜Â·ğ‘†ğ‘–ğ‘—=EP(ğ’€|ğ‘º)log
Î©(ğ’€)Â·Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—
,
whereğ‘ğ‘–ğ‘—=exp(Ãğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—ğ‘˜).
Lemma 4.2. Maximizing the second terms in the item partition
problem is equivalent to maximizing the aggregation of graphs whose
nodes have exactly one out-going edge:
âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—exp(âˆ‘ï¸ğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜)=âˆ’logâˆ‘ï¸
ğ‘ŒâˆˆYÎ©(ğ’€)Â·Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—
.
Based on these two lemmas, we can approximate the proposed
partition problem as a cross-entropy loss that bridges the item-
confidence MRF and the item-partition MRF, verifying that fre-
quency patterns of usersâ€™ behaviors are important guidance for the
item partition learning.Theorem 3. (Approximation) Maximizing the proposed parti-
tion problem is equivalent to optimizing the cross-entropy loss between
the two MRFs:
max OPTâ‡”maxEP(ğ’€|ğ‘º)log(P(ğ’€|ğ’)),
where ğ‘ºandğ’describe the relations in the item-confidence MRF and
the item-partition MRF, respectively.
4.2.3 Optimization. To further reduce the complexity of cross-
entropy loss between these two MRFs, we divide it into node-level
comparisons as their out-going edges are independent to each
other [ 34]. Specifically, suppose ğ’€â€²
ğ‘–âˆ¼M(ğ‘…ğ‘–1/Ã
ğ‘—ğ‘…ğ‘–ğ‘—,Â·Â·Â·,ğ‘…ğ‘–ğ‘›/Ã
ğ‘—ğ‘…ğ‘–ğ‘—)
(abbreviate as M(ğ’€â€²
ğ‘–|ğ‘¹ğ‘–)) is a one-hot encoding sample from multino-
mial distribution M(Â·), we have P(ğ’€=(Ëœğ’€1,Â·Â·Â·,Ëœğ’€ğ‘›)|ğ‘¹)=Ã
ğ‘–M(ğ’€â€²
ğ‘–=
Ëœğ’€ğ‘–|ğ‘¹ğ‘–)asğ’€â€²
1,Â·Â·Â·,ğ’€â€²ğ‘›are independent of each other, where Ëœğ’€ğ‘–de-
notes theğ‘–-th row of Ëœğ’€. Consequently, item partition optimization
can be achieved by contrastive learning as follows.
Theorem 4. Contrastive learning. The proposed partition goals
in Equation (3) and Equation (4) can be solved in a contrastive learning
manner, i.e.,
Lğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› =EP(ğ’€|ğ‘º)log(P(ğ’€|ğ’))
=âˆ‘ï¸
ğ‘–EM(ğ’€ğ’Š|ğ‘ºğ’Š)logexp(Ã
ğ‘˜ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—ğ‘˜)Ã
ğ‘—â€²exp(Ã
ğ‘˜ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—â€²ğ‘˜),
where ğ’€ğ‘–is a one-hot encoding vector with the ğ‘—-th element ğ’€ğ‘–ğ‘—=1.
In summary, this subsection provides an efficient way to opti-
mize the proposed partition problem. Specifically, for each node ğ‘–
in the item-confidence graph, we can sample one node ğ‘—from its
neighbor according to ğ’€ğ’Šâˆ¼M(Â·|ğ‘ºğ’Š)as the positive node and others
as the negative nodes for the contrastive learning. Intuitively, as the
items connected by an edge with higher item confidence are more
likely to be sampled in the subgraph, they are more likely to be
partitioned into the same group. Therefore, such clustering learning
can make items belonging to different/same groups show distin-
guishable/similar representations, which helps the capsule network
more precisely allocate items into multi-interest facets. As an item
usually belongs to several groups in real-world scenarios [ 22], we
take the real-value partition weights to model the overlapped item
partition. For efficient optimization, we sample ğ‘ğ‘£(e.g., 100) items
to approximate the rest of the items as the negative nodes. There-
fore, the complexity of updating all nodes is around ğ‘‚(ğ‘Â·ğ¾Â·ğ‘ğ‘£),
which is more efficient than spectral clustering with computation
complexityğ‘‚(ğ‘3)[43] andğ‘‚(ğ‘2logğ‘)[37] because of ğ‘>>ğ‘ğ‘£
andğ‘>>ğ¾. In addition, the proposed method provides a generic
way to flexibly integrate with existing multi-interest models based
on multi-task learning, which is challenging for spectral clustering
and the Gaussian mixture model.
4.3 Multi-task Representation Learning
To bridge item partition and multi-interest modeling, we propose
conducting multi-task learning of item partition and multi-interest
recommendation tasks, because the appropriate item partition can
help to disentangle usersâ€™ multi-interest learning to alleviate inter-
est collapse issues. In the following, we first introduce multi-interest
modeling for recommendation, and then align item partition with
the multi-interest recommendation tasks with multi-task learning.
 
681KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingpeng Du et al.
Table 1: Statistics of the experimental datasets
Dataset #Users #Items #Interactions
Gowalla 165,506 174,605 2,061,264
Retail Rocket 33,708 81,635 356,840
Amazon Books 603,668 367,982 8,898,041
To capture usersâ€™ multi-interests, we adopt the capsule networks
to derive from the userâ€™s behaviors Eâ€²ğ‘¢,
ğ’ğ‘¢1,Â·Â·Â·,ğ’ğ‘¢ğ¹=ğ¶ğ‘ğ‘ğ‘ ğ‘¢ğ‘™ğ‘’([ğ’†1,Â·Â·Â·,ğ’†ğœ…])
where[ğ’†1,Â·Â·Â·,ğ’†ğœ…]denotes the item embeddings among the userâ€™s
behaviorsEâ€²ğ‘¢, and ğ’ğ‘¢ğ‘“denotes his/her ğ‘“-th interest. Then, we
adopt the attention mechanism to capture the userâ€™s dynamic pref-
erences on different interests,
ğ‘ğ‘¢,ğ‘“=exp(ğ’âŠ¤
ğ‘¢ğ‘“Â·Â¯ğ’†ğ‘¢)/âˆ‘ï¸ğ¹
ğ‘“â€²=1exp(ğ’âŠ¤
ğ‘¢ğ‘“â€²Â·Â¯ğ’†ğ‘¢),
where Â¯ğ’†ğ’–=ğ´ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’([ğ’†1,Â·Â·Â·,ğ’†ğœ…])measures the userâ€™s general
preference by aggregating his/her historical behaviors. To predict
items that the user may interact with, we fuse the userâ€™s preferences
on items varying with different interests,
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘¢,ğ‘–)=âˆ‘ï¸ğ¹
ğ‘“=1ğ‘ğ‘¢,ğ‘“Â·Ë†ğ‘Ÿğ‘¢,ğ‘“,ğ‘–,Ë†ğ‘Ÿğ‘¢,ğ‘“,ğ‘–=ğ’âŠ¤
ğ‘¢ğ‘“Â·ğ’†{ğ‘–},(5)
where Ë†ğ‘Ÿğ‘¢,ğ‘“,ğ‘– measures the score of item ğ‘–w.r.t. userğ‘¢â€™sğ‘“-th interest,
andğ’†{ğ‘–}denotes the embedding of the item ğ‘–. For the recommenda-
tion task, we adopt the pairwise loss to define the recommendation
objective function as follows,
Lğ‘…ğ‘’ğ‘=âˆ‘ï¸
ğ‘¢,ğ‘–,ğ‘—logğœ(ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘¢,ğ‘–)âˆ’ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘¢,ğ‘—)),
where the train setD={(ğ‘¢,ğ‘–,ğ‘—)|ğ‘¢âˆˆU,ğ‘–,ğ‘—âˆˆI} means that user ğ‘¢
engaged item ğ‘–instead of item ğ‘—.ğœ(Â·)denotes the Sigmoid function.
We select the item ğ‘—with the highest score among negative item
candidates as the hard negative instance inspired by [47].
To align the item partition with the multi-interest recommenda-
tion, we first connect the itemâ€™s partition weights with its embed-
dings based on the shared representations, i.e., ğ’†ğ‘–=[ğ‘¤ğ‘–1,Â·Â·Â·,ğ‘¤ğ‘–ğ¾]
andğ‘‘=ğ¾. Then, we propose to conduct multi-task learning with
consideration of both item partition task and multi-interest recom-
mendation task as follows:
L=Lğ‘…ğ‘’ğ‘+ğœ†Â·Lğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›, (6)
whereğœ†denotes the trade-off coefficient of these two tasks.
5 EXPERIMENT
In this section, we aim to validate the effectiveness of the proposed
method DisMIR. Specifically, we conduct extensive experiments to
study the following research questions:
RQ1: Whether the proposed DisMIR outperforms state-of-the-art
multi-interest recommendation methods?
RQ2: Whether the proposed DisMIR benefits from the item par-
tition for multi-interests disentangled learning?
RQ3: Whether the proposed DisMIR can enhance existing multi-
interest models for reccomendation?
RQ4: How do hyper-parameters influence the performance of
the proposed DisMIR?5.1 Experimental Setup
5.1.1 Datasets. We adopt three public datasets, including Gowalla2,
RetailRocket3, and Amazon Books4. The Gowalla is a checking-in
dataset derived from a social network website named gowalla.com.
RetailRocket is an online shopping dataset collected from the biggest
E-Commerce website (named Taobao) in China. The Amazon Books
dataset is the largest subset in the Amazon series datasets, which is
collected from the Amazon platform with usersâ€™ reviews on various
types of books. All these datasets contain the timestamps or orders
of user behaviors. Following prior studies [ 2,47], we filter out users
and items that have less than 5 records, then we treat check-in, view,
and review behaviors as implicit feedback for these three datasets,
respectively5. The characteristics of these datasets are summarized
in Table 1.
5.1.2 Evaluation Methodology and Metrics. To make the proposed
DisMIR comparable with existing multi-interest recommendation
methods, we set the evaluation methodology and metrics in our
experiments following the prior study6[2,47]. Based on the times-
tamps of interactions, we chronologically split the interaction records
into training, validation, and test sets by the proportion of 8:1:1 for
each user. For evaluation, we test the last 20% items in the userâ€™s
sequence based on the first 80% of usersâ€™ behavior sequences for
their preference inference. To evaluate the performance, we adopt
three widely used evaluation metrics for top- ğ‘›recommendation:
Recall (R), Hit Rate (HR), and Normalized Discounted Cumulative
Gain (ND), where ğ‘›was set as 20/50empirically. Experimental
results are recorded as the average of five runs.
5.1.3 Baselines. We take the following state-of-the-art methods as
the baselines, mainly including two single-interest models and six
multi-interest models for comparison. -DNN [ 9]averages the em-
beddings of different features, and then feeds their concatenation
into a deep neural network for recommendation. -GRU4Rec [ 15]
utilizes the GRU module to model usersâ€™ sequential behaviors for
recommendation. -MIND [ 19]adopts the capsule network [ 30]
module to capture usersâ€™ various interests with a dynamic rout-
ing strategy. -ComiRec [ 2]adopts the capsule network with a
self-attention dynamic routing method to capture usersâ€™ different
interests for diversity-accuracy goals. -Re4 [ 51]utilizes the back-
ward flow as regularization for usersâ€™ multi-interest preference
learning, contributing to alleviating routing collapse of interest
facets. -UMI [ 3]proposes to model usersâ€™ multi-interest prefer-
ences based on usersâ€™ profiles with a hard negative sampling strat-
egy. We follow the same implementation of hard negative sampling
strategy as in [ 47]. -PIMIRec [ 5]is a state-of-the-art multi-interest
framework with consideration of both time information and inter-
activity among items for sequential recommendation. - REMI [ 47]
is a state-of-the-art multi-interest framework with an importance-
negative-sampling strategy and routing variation regularization.
As MarcridVAE [ 26] fails to capture the dynamic multi-interest of
users for sequential recommendation and suffers from GPU mem-
ory costs, we donâ€™t include it in baseline methods.
2https://snap.stanford.edu/data/
34https://www.kaggle.com/retailrocket/ecommerce-dataset
4http://jmcauley.ucsd.edu/data/amazon/links.html
5https://github.com/RUCAIBox/RecSysDatasets
6https://github.com/Tokkiu/REMI
 
682Disentangled Multi-interest Representation Learning for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Performance of different methods. * indicates statistically significant improvement of the proposed method to baseline
models on t-test (p < 0.05). â€˜Improveâ€™ indicates the relative improvements of DisMIR over the strongest baseline.
Dataset Metric DNN GRU4Rec MIND ComiRec Re4 UMI PIMIRec REMI DisMIR improve.
GowallaR@20 0.0864 0.0900 0.0923 0.0623 0.0843 0.0961 0.1193 0.1300 0.1384* 6.48%
HR@20 0.3211 0.3359 0.3122 0.2281 0.3104 0.3314 0.3843 0.4021 0.4312* 7.25%
ND@20 0.1384 0.1433 0.1336 0.0955 0.1287 0.1391 0.1603 0.1715 0.1855* 8.15%
R@50 0.1388 0.1458 0.1521 0.1181 0.1396 0.1642 0.1951 0.2109 0.2169* 2.83%
HR@50 0.4390 0.4577 0.4482 0.3778 0.4224 0.4719 0.5207 0.5498 0.5599* 1.83%
ND@50 0.1434 0.1494 0.1450 0.1199 0.1410 0.1505 0.1660 0.1792 0.1849* 3.16%
Retail RocketR@20 0.1050 0.0827 0.1415 0.1035 0.1397 0.1519 0.1828 0.2129 0.2385* 12.04%
HR@20 0.1711 0.1376 0.2195 0.1602 0.2103 0.2364 0.2764 0.3183 0.3524* 10.72%
ND@20 0.0641 0.0517 0.0804 0.0609 0.0785 0.0875 0.1025 0.1198 0.1330* 11.04%
R@50 0.1608 0.1371 0.2148 0.1666 0.2194 0.2423 0.2811 0.3160 0.3447* 9.08%
HR@50 0.2518 0.2132 0.3183 0.2501 0.3174 0.3574 0.3969 0.4515 0.4815* 6.64%
ND@50 0.0701 0.0593 0.0880 0.0684 0.0884 0.0974 0.1093 0.1281 0.1360* 6.20%
Amazon BooksR@20 0.0467 0.0441 0.0433 0.0539 0.0597 0.0690 0.0682 0.0839 0.0887* 5.81%
HR@20 0.1043 0.1004 0.0907 0.1108 0.1240 0.1423 0.1411 0.1674 0.1805* 7.86%
ND@20 0.0391 0.0378 0.0340 0.0406 0.0476 0.0527 0.0526 0.0629 0.0678* 7.77%
R@50 0.0722 0.0706 0.0677 0.0848 0.0690 0.1053 0.1056 0.1207 0.1366* 13.21%
HR@50 0.1607 0.1553 0.1379 0.1716 0.1975 0.2059 0.2062 0.2326 0.2637* 13.38%
ND@50 0.0457 0.0443 0.0390 0.0481 0.0576 0.0587 0.0583 0.0667 0.0758* 13.63%
5.1.4 Implementation Details. For a fair comparison, all methods
are optimized by the Adam optimizer with the same latent space
dimension (i.e., 64) and learning rate (i.e., 1Ã—10âˆ’3). For hard neg-
ative sampling, we select the hard negative item among 10item
candidates except 50item candidates for Amazon Books dataset due
to its large scale. For the capsule network in the proposed DisMIR,
we set its transformation matrix as the identity matrix inspired
by the LightGCN [ 14] and adopt a one-time routing process for
efficiency. For the hyper-parameters in the proposed DisMIR, we
set trade-off coefficients ğœ†=[0.01,0.1,1.0]and interest numbers
ğ¹=[4,8,6]for Gowalla Retail, Rocket, and Amazon Books, respec-
tively. Detailed analysis of the hyper-parameter can be found in
Section 5.5. For baseline models, we set their parameters as authorsâ€™
implementation if they exist, otherwise we tune them to their best.
5.2 Model Comparison (RQ1)
Table 2 shows the performance of different methods including
single-interest models and multi-interest models. To make the table
notable, we bold the best results and underline the best baseline
results for each dataset with one specific evaluation metric. Firstly,
the proposed method DisMIR significantly outperforms all base-
line methods in every case, which shows the effectiveness of the
proposed method. DisMIR demonstrates notable enhancements,
with average improvements of 4.95%, 9.29%, and 10.28% when com-
pared to the top-performing baseline models on the Gowalla, Retail
Rocket, and Amazon Books, respectively. Second, several multi-
interest methods such as MIND and ComiRec even underperform
single-interest methods such as GRU4Rec and DNN in some cases.
This may be attributed to their shortcoming in capturing usersâ€™
dynamic preferences on different interest facets and the unsuit-
able training scheme. Finally, REMI achieves the best performance
among the baseline methods, which mainly benefits from the neg-
ative sampling strategy and routing variation regularization. Re4
can help to improve the performance of its basic version ComiRecwith the re-adjustment of the item-to-interest alignment. These
also verify the importance of alleviating interest collapse issues.
5.3 Ablation Study (RQ2)
5.3.1 Comparison among the variants of the proposed method. To
examine the efficacy of different components of DisMIR, we com-
pare a number of its variants.
-MIR: It removes the item partition task for disentangled multi-
interest representation learning, i.e., ğœ†=0.
-MIR-RV: It replaces the item partition task with the Routing
Variation regularization (Equation 1) proposed in [ 47], which
penalizes the variance regularizer of the item-to-interest routing
matrix to alleviate routing collapse in capsule networks.
-MIR-BF: It replaces the item partition task with the Backward
Flow re-adjustment strategy (Equation 2) proposed in [ 51], which
aims to learn representative embeddings of different interests.
-w/o DPI: It removes usersâ€™ Dynamic Preference modeling on
different interests over time, assuming equal importance for dif-
ferent interests in Equation (5).
Table 3 shows the performance of the proposed method and its
variant, i.e., MIR, MIR-RV, MIR-BF, DisMIR, and w/o DPI.
-On the one hand, DisMIR with the item partition consistently
outperforms MIR without the item partition. On the other hand,
the variant methods with regularization (i.e., MIR-RV) and re-
adjustment (i.e., MIR-RV) outperform the variant MIR in most
cases. This illustrates the necessity of alleviating the collapse
issue for multi-interest preference learning.
-DisMIR consistently outperforms its variant approaches, namely
MIR-RV and MIR-BF, across all cases. It indicates that the pro-
posed item partition is more effective than existing strategies for
multi-interest disentanglement, which can alleviate both item-
level and facet-level collapse issues for usersâ€™ multi-interests
learning.
 
683KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingpeng Du et al.
Table 3: Performance of variant DisMIR for ablation studies.
Dataset Method R@20 HR@20 ND@20 R@50 HR@50 ND@50
GowallaMIR 0.1332 0.4205 0.1778 0.2132 0.5524 0.1804
MIR-RV 0.1162 0.3790 0.1473 0.1963 0.5247 0.1615
MIR-BF 0.1298 0.4151 0.1712 0.2126 0.5570 0.1820
w/o DPI 0.1303 0.4030 0.1708 0.2125 0.5552 0.1800
DisMIR 0.1384 0.4312 0.1855 0.2169 0.5599 0.1849
Retail
RocketMIR 0.2235 0.3355 0.1253 0.3267 0.4574 0.1277
MIR-RV 0.2144 0.3213 0.1154 0.3303 0.4640 0.1261
MIR-BF 0.2326 0.3450 0.1300 0.3370 0.4732 0.1335
w/o DPI 0.2152 0.3204 0.1201 0.3198 0.4506 0.1266
DisMIR 0.2385 0.3524 0.1330 0.3447 0.4815 0.1360
Amazon
BooksMIR 0.0817 0.1691 0.0631 0.1309 0.2547 0.0728
MIR-RV 0.0844 0.1739 0.0640 0.1318 0.2567 0.0718
MIR-BF 0.0849 0.1733 0.0636 0.1343 0.2602 0.0731
w/o DPI 0.0853 0.1689 0.0638 0.1223 0.2342 0.0669
DisMIR 0.0887 0.1805 0.0678 0.1366 0.2637 0.0758
MIR MIR-BF DisMIR MIR-RV
Figure 3: Itemsâ€™ representation visualization in latent space.
-DisMIR outperforms its variant w/o DPI in all cases, which indi-
cates the necessity of capturing usersâ€™ dynamic preferences on
different interests.
5.3.2 A case study for the distribution of user interests. Moreover,
we further investigate in how the proposed DisMIR contributes to
the itemsâ€™ representation distribution in the latent space. To this
end, we visualize the item representations learned by comparing
DisMIR, MIR, MIR-RV, and MIR-BF. Specifically, we first randomly
sample a user on the Gowalla dataset, and then adopt the t-SNE
transformation to map embeddings of his/her engaged items into a
2-dimension plane as shown in Figure 3, where items (points) with
different colors mean they are partitioned into different interests.
Firstly, the DisMIR exhibits more noticeable clusters that reflect the
userâ€™s different interests among these methods, indicating that the
item partition can help with effective disentangled learning of multi-
interests. Secondly, the variant MIR-RV misallocates the green and
blue points with ineffective item-to-interest alignment, which can
be attributed to the item-level collapse issue of the regularization
strategy. Thirdly, the variant MIR-BF assigns most items into the
same group (i.e., green one) compared to others, which can be
attributed to the facet-level collapse issue of the re-adjustment
strategy. Finally, the variant MIR shows both item-level and facet-
level collapse issues, indicating the necessity of disentangled multi-
interest learning.
5.3.3 A statistical analysis for the distribution of user interests. To
delve deeper into the distribution of user interests, we carry out
a statistical analysis by comparing our proposed method DisMIR
with existing disentangled strategies [ 47,51]. Firstly, to assess theTable 4: statistical analysis for the distribution of user inter-
ests among different methods.
Metric DisMIR MIR MIR-RV MIR-BF
Similarity 72.1 99.1 85.5 57.4
MIN/MAX 12.7/40.1 12.1/ 42.7 12.2/42.4 8.9/50.7
STD 10.8 12.0 11.8 16.8
item misallocation issue in usersâ€™ interests (i.e., item-level collapse),
we measure the average similarity of items that are from different
interest facets for a user, i.e.,
ğ‘†ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ =2
ğ¹Â·(ğ¹âˆ’1)âˆ‘ï¸ğ¹
ğ‘“=1âˆ‘ï¸ğ¹
ğ‘“â€²=ğ‘“+1ğ‘ ğ‘–ğ‘š(ğºğ‘“,ğºğ‘“â€²)
ğ‘ ğ‘–ğ‘š(ğºğ‘“,ğºğ‘“â€²)=âˆ‘ï¸
ğ‘–âˆˆğºğ‘“,ğ‘—âˆˆğºğ‘“â€²<ğ‘’ğ‘–,ğ‘’ğ‘—>
|ğºğ‘“||ğºğ‘“â€²|
whereğºğ‘“denotes items that are partitioned into interest ğ‘“for the
user, andğ‘’ğ‘–denotes the representation of the item ğ‘–. Typically, a high
similarity between different interests usually means a more severe
issue of item misallocation (i.e., item-level collapse) in multi-interest
modeling. Second, to measure item balance in usersâ€™ interests, we
compute the minimal (MIN), maximal (MAX), and standard devi-
ation (STD) of item numbers that are allocated different interest
facets for a user. Typically, a high STD and large gap between MIN
and MAX usually mean an imbalance issue (i.e., facet-level collapse)
in multi-interest modeling.
We randomly sample 100 users and report their average val-
ues, statistical results among different methods (DisMIR, MIR, MIR-
RV [47], MIR-BF [ 51]) are shown in Table 4. Firstly, MIR-BF exhibits
the most pronounced item imbalance issue among the methods, as
evidenced by the highest STD and the largest gap between MIN
and MAX, confirming that the Backward Flow re-adjustment strat-
egy [ 51] is prone to facet-level collapse. Although MIR-BF shows
the lowest similarity between different interests, it is mainly caused
by a facet-level collapse issue where only one interest is domi-
nant (e.g., MIR-BF as shown in Figure 3), which is essentially a
poor multi-interest distribution. Secondly, MIR-RV with Routing
Variation regularization strategy [ 47] shows the high similarity be-
tween different interests, mixing up items from different interests,
which indicates a severe issue of item misallocation (i.e., item-level
collapse) in multi-interest modeling. Thirdly, the MIR without dis-
entangled learning strategies shows both high STD and similarity,
suffering from item-level and facet-level collapse issues. Finally, our
DisMIR method achieves balanced item allocation (evidenced by a
low STD) and distinguishable interest allocation (evidenced by low
similarity between different interests), proving its effectiveness in
addressing both item- and facet-level collapse issues.
5.4 Enhancement with Existing Multi-interest
Recommendation Methods. (RQ3)
As the item partition task is orthogonal to existing multi-interest
methods and frameworks, it is interesting to explore whether this
strategy can be applied to enhance their performance. As such,
we instantiate our DisMIR in three representative and state-of-
the-art multi-interest models (i.e., MIND [ 30], ComiRec [ 2], and
 
684Disentangled Multi-interest Representation Learning for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 0.001 0.01 0.1 1.0
trade-off coef 
0.170.18Performance
Gowalla
ND@20
ND@50
2 4 6 8 10 20
interest number  F0.12750.13000.13250.13500.1375
Gowalla
ND@20
ND@50
10 50 100 200 500
sampled sizes in contrastive loss Nv0.1250.1300.1350.140
Gowalla
ND@20
ND@50
32 64 128 256
number of partition groups K0.100.120.14
Gowalla
ND@20
ND@50
0 0.001 0.01 0.1 1.0
trade-off coef 
0.1200.1250.1300.135Performance
Retail Rocket
ND@20
ND@50
2 4 6 8 10 20
interest number  F0.17500.17750.18000.18250.1850
Retail Rocket
ND@20
ND@50
10 50 100 200 500
sampled sizes in contrastive loss Nv0.170.180.19
Retail Rocket
ND@20
ND@50
32 64 128 256
number of partition groups K0.1700.1750.1800.185
Retail Rocket
ND@20
ND@50
0 0.001 0.01 0.1 1.0
trade-off coef 
0.0650.0700.075Performance
Amazon Books
ND@20
ND@50
2 4 6 8 10 20
interest number  F0.050.060.07
Amazon Books
ND@20
ND@50
10 50 100 200 500
sampled sizes in contrastive loss Nv0.0650.0700.0750.080
Amazon Books
ND@20
ND@50
32 64 128 256
number of partition groups K0.020.040.060.08
Amazon Books
ND@20
ND@50
Figure 4: Performance of the proposed method varying with different trade-off coefficients and interest numbers.
Table 5: Performance of existing multi-interest recommen-
dation methods with the item partition task.
MethodsBackbone MIND ComiRec REMI
Enhance â—‹ â—‹ â—‹ â—‹ â—‹ â—‹
GowallaR@20 0.0923 0.0881 0.0670 0.0623 0.1303 0.1300
HR@20 0.3122 0.3122 0.2403 0.2281 0.4030 0.4021
ND@20 0.1336 0.1353 0.1021 0.0955 0.1708 0.1715
Retail
RocketR@20 0.1415 0.1386 0.1118 0.1035 0.2152 0.2129
HR@20 0.2195 0.2124 0.1703 0.1602 0.3204 0.3183
ND@20 0.0804 0.0764 0.0641 0.0609 0.1201 0.1198
Amazon
BooksR@20 0.0433 0.0423 0.0545 0.0539 0.0853 0.0839
HR@20 0.0907 0.0888 0.1128 0.1108 0.1689 0.1674
ND@20 0.0340 0.0332 0.0420 0.0406 0.0638 0.0629
REMI [ 47]). The results are presented in Table 5. Firstly, models
without regularization (i.e., MIND and ComiRec) benefit from the
item partition task in most cases, which reflects the effectiveness of
the proposed DisMIR in alleviating the interest collapse issue. Sec-
ondly, the REMI with the routing regularization still benefits from
the item partition task, which indicates that the item partition task
is more effective for multi-interest disentanglement. Third, the item
partition task achieves less improvement in REMI than MIND and
ComiRec, which may be attributed to the REMI partially alleviating
the interest collapse issue with the routing regularization.
5.5 Hyper-Parameter Study (RQ4)
There are several key parameters for the proposed method DisMIR,
including the trade-off coefficient ğœ†, interest number ğ¹, sampled
sizesğ‘ğ‘£in contrastive loss, and number of partition groups ğ¾. We
thus investigate how these parameters influence the performance
of DisMIR. As depicted in Figure 4, we assess DisMIRâ€™s performance
by varying the values of these hyper-parameters. For the trade-off
coefficientğœ†and interest number ğ¹, we observe that the highest per-
formance is achieved when trade-off coefficients ğœ†=[0.01,0.1,1.0]and interest numbers ğ¹=[4,8,6]for Gowalla Retail, Rocket, and
Amazon Books datasets, respectively. For real-world applications,
we suggest adopting a grid search strategy as a practical approach
to select the optimal hyper-parameters for the trade-off coefficient
and interest number. For the sampled sizes ğ‘ğ‘£in contrastive loss,
we observe that the highest performance is typically achieved when
ğ‘ğ‘£=100. Therefore, we suggest setting ğ‘ğ‘£=100for implemen-
tation. For the number of partition groups ğ¾, we observe that the
performance of DisMIR improves with larger ğ¾, but the increase
slows down when ğ¾>64. Therefore, considering both effectiveness
and efficiency, we suggest setting ğ¾=64for implementation.
6 CONCLUSION
In this paper, we introduce a generic disentangled multi-interest
representation learning method for sequential recommendation,
which contributes to the field of multi-interest preference learn-
ing both theoretically and practically. To effectively disentangle
the representation of multi-interests, we propose a proactive items
partition based on global usersâ€™ behavior patterns. In addition, we
establish its connection to the spectral cluster to prove its effective-
ness in alleviating item-level and facet-level collapse issues, and
provide an efficient way for its optimization practically. Experi-
ments on three real-world datasets show that the proposed DisMIR
not only consistently outperforms state-of-the-art methods, but
also can flexibly integrate with existing multi-interest models as a
plugin to enhance their performances.
7 ACKNOWLEDGE
This research is supported by the National Research Foundation,
Singapore under its Al Singapore Programme (AISGAward No:
AISG3-RP-2022-031). This research is partially supported by the
Agency for Science, Technology and Research (A*STAR) under
its RIE 2025 - Industry Alignment Fund - Pre Positioning (IAF-
PP) funding scheme (Project No: M23L4a0001). This work is also
partially supported by the MOE AcRF Tier 1 funding (RG13/23).
 
685KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingpeng Du et al.
REFERENCES
[1]Ting Bai, Jian-Yun Nie, Wayne Xin Zhao, Yutao Zhu, Pan Du, and Ji-Rong Wen.
2018. An attribute-aware neural attentive model for next basket recommendation.
InThe 41st International ACM SIGIR Conference on Research & Development in
Information Retrieval. 1201â€“1204.
[2]Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang.
2020. Controllable multi-interest framework for recommendation. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 2942â€“2951.
[3]Zheng Chai, Zhihong Chen, Chenliang Li, Rong Xiao, Houyi Li, Jiawei Wu, Jingxu
Chen, and Haihong Tang. 2022. User-aware multi-interest learning for candidate
matching in recommenders. In Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 1326â€“1335.
[4]Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng
Jin, and Yong Li. 2021. Sequential recommendation with graph neural networks.
InProceedings of the 44th international ACM SIGIR conference on research and
development in information retrieval. 378â€“387.
[5]Gaode Chen, Xinghua Zhang, Yanyan Zhao, Cong Xue, and Ji Xiang. 2021. Ex-
ploring periodicity and interactivity in multi-interest framework for sequential
recommendation. arXiv preprint arXiv:2106.04415 (2021).
[6]Hong Chen, Yudong Chen, Xin Wang, Ruobing Xie, Rui Wang, Feng Xia, and
Wenwu Zhu. 2021. Curriculum disentangled recommendation with noisy multi-
feedback. Advances in Neural Information Processing Systems 34 (2021), 26924â€“
26936.
[7]Wanyu Chen, Pengjie Ren, Fei Cai, Fei Sun, and Maarten De Rijke. 2021. Multi-
interest diversification for end-to-end sequential recommendation. ACM Trans-
actions on Information Systems (TOIS) 40, 1 (2021), 1â€“30.
[8]Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and
Hongyuan Zha. 2018. Sequential recommendation with user memory networks.
InProceedings of the eleventh ACM international conference on web search and
data mining. 108â€“116.
[9]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 191â€“198.
[10] Tim Donkers, Benedikt Loepp, and JÃ¼rgen Ziegler. 2017. Sequential user-based
recurrent neural network recommendations. In Proceedings of the eleventh ACM
conference on recommender systems. 152â€“160.
[11] Yingpeng Du, Hongzhi Liu, and Zhonghai Wu. 2021. Modeling multi-factor and
multi-faceted preferences over sequential networks for next item recommenda-
tion. In Machine Learning and Knowledge Discovery in Databases. Research Track:
European Conference, ECML PKDD 2021, Bilbao, Spain, September 13â€“17, 2021,
Proceedings, Part II 21. Springer, 516â€“531.
[12] Taeyoung Hahn, Myeongjang Pyeon, and Gunhee Kim. 2019. Self-routing capsule
networks. Advances in neural information processing systems 32 (2019).
[13] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov
chains for sparse sequential recommendation. In 2016 IEEE 16th international
conference on data mining (ICDM). IEEE, 191â€“200.
[14] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval. 639â€“648.
[15] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based recommendations with recurrent neural networks. arXiv
preprint arXiv:1511.06939 (2015).
[16] Haoji Hu, Xiangnan He, Jinyang Gao, and Zhi-Li Zhang. 2020. Modeling personal-
ized item frequency information for next-basket recommendation. In Proceedings
of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval. 1071â€“1080.
[17] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang.
2018. Improving sequential recommendation with knowledge-enhanced mem-
ory networks. In The 41st international ACM SIGIR conference on research &
development in information retrieval. 505â€“514.
[18] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197â€“206.
[19] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,
Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest
network with dynamic routing for recommendation at Tmall. In Proceedings of
the 28th ACM international conference on information and knowledge management .
2615â€“2623.
[20] Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, Xin
Jiang, and Qun Liu. 2022. MINER: multi-interest matching network for news
recommendation. In Findings of the Association for Computational Linguistics:
ACL 2022. 343â€“352.
[21] Qingfeng Li, Huifang Ma, Wangyu Jin, Yugang Ji, and Zhixin Li. 2024. Multi-
Interest Network with Simple Diffusion for Multi-Behavior Sequential Recom-
mendation. In Proceedings of the 2024 SIAM International Conference on Data
Mining (SDM). SIAM, 734â€“742.[22] Danyang Liu, Yuji Yang, Mengdi Zhang, Wei Wu, Xing Xie, and Guangzhong
Sun. 2022. Knowledge Enhanced Multi-Interest Network for the Generation
of Recommendation Candidates. In Proceedings of the 31st ACM International
Conference on Information & Knowledge Management. 3322â€“3331.
[23] Xiaolong Liu, Liangwei Yang, Zhiwei Liu, Xiaohan Li, Mingdai Yang, Chen Wang,
and S Yu Philip. 2023. Group-Aware Interest Disentangled Dual-Training for
Personalized Recommendation. In 2023 IEEE International Conference on Big Data
(BigData). IEEE, 393â€“402.
[24] Yaokun Liu, Xiaowang Zhang, Minghui Zou, and Zhiyong Feng. 2024. Attribute
Simulation for Item Embedding Enhancement in Multi-interest Recommendation.
InProceedings of the 17th ACM International Conference on Web Search and Data
Mining. 482â€“491.
[25] Chen Ma, Liheng Ma, Yingxue Zhang, Jianing Sun, Xue Liu, and Mark Coates.
2020. Memory augmented graph neural networks for sequential recommendation.
InProceedings of the AAAI conference on artificial intelligence, Vol. 34. 5045â€“5052.
[26] Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn-
ing disentangled representations for recommendation. Advances in neural infor-
mation processing systems 32 (2019).
[27] Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu.
2020. Disentangled self-supervision in sequential recommenders. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 483â€“491.
[28] Chang Meng, Ziqi Zhao, Wei Guo, Yingxue Zhang, Haolun Wu, Chen Gao,
Dong Li, Xiu Li, and Ruiming Tang. 2023. Coarse-to-fine knowledge-enhanced
multi-interest learning framework for multi-behavior recommendation. ACM
Transactions on Information Systems 42, 1 (2023), 1â€“27.
[29] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-
izing personalized markov chains for next-basket recommendation. In Proceedings
of the 19th international conference on World wide web. 811â€“820.
[30] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic routing
between capsules. Advances in neural information processing systems 30 (2017).
[31] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-
resentations from transformer. In Proceedings of the 28th ACM international
conference on information and knowledge management. 1441â€“1450.
[32] GÃ¡bor J SzÃ©kely and Maria L Rizzo. 2009. Brownian distance covariance. The
annals of applied statistics (2009), 1236â€“1265.
[33] GÃ¡bor J SzÃ©kely, Maria L Rizzo, and Nail K Bakirov. 2007. Measuring and testing
dependence by correlation of distances. (2007).
[34] Zhiquan Tan, Yifan Zhang, Jingqin Yang, and Yang Yuan. 2023. Contrastive Learn-
ing Is Spectral Clustering On Similarity Graph. arXiv preprint arXiv:2303.15103
(2023).
[35] Yu Tian, Jianxin Chang, Yanan Niu, Yang Song, and Chenliang Li. 2022. When
multi-level meets multi-interest: A multi-grained neural model for sequential
recommendation. In Proceedings of the 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 1632â€“1641.
[36] Nhu-Thuat Tran and Hady W Lauw. 2023. Multi-Representation Variational
Autoencoder via Iterative Latent Attention and Implicit Differentiation. In Pro-
ceedings of the 32nd ACM International Conference on Information and Knowledge
Management. 2462â€“2471.
[37] Hadrien Van Lierde, Tommy WS Chow, and Guanrong Chen. 2019. Scalable
spectral clustering for overlapping community detection in large-scale networks.
IEEE Transactions on Knowledge and Data Engineering 32, 4 (2019), 754â€“767.
[38] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
computing 17 (2007), 395â€“416.
[39] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z Sheng, and Mehmet
Orgun. 2019. Sequential recommender systems: challenges, progress and
prospects. In 28th International Joint Conference on Artificial Intelligence, IJCAI
2019. International Joint Conferences on Artificial Intelligence, 6332â€“6338.
[40] Xin Wang, Hong Chen, Yuwei Zhou, Jianxin Ma, and Wenwu Zhu. 2022. Dis-
entangled representation learning for recommendation. IEEE Transactions on
Pattern Analysis and Machine Intelligence 45, 1 (2022), 408â€“424.
[41] Xin Wang, Hong Chen, and Wenwu Zhu. 2021. Multimodal disentangled represen-
tation for recommendation. In 2021 IEEE International Conference on Multimedia
and Expo (ICME). IEEE, 1â€“6.
[42] Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng
Chua. 2020. Disentangled graph collaborative filtering. In Proceedings of the 43rd
international ACM SIGIR conference on research and development in information
retrieval. 1001â€“1010.
[43] Yong Wang, Yuan Jiang, Yi Wu, and Zhi-Hua Zhou. 2011. Spectral clustering on
multiple manifolds. IEEE Transactions on Neural Networks 22, 7 (2011), 1149â€“1161.
[44] Yifan Wang, Suyao Tang, Yuntong Lei, Weiping Song, Sheng Wang, and Ming
Zhang. 2020. Disenhan: Disentangled heterogeneous graph attention network
for recommendation. In Proceedings of the 29th ACM international conference on
information & knowledge management. 1605â€“1614.
[45] Yuling Wang, Xiao Wang, Xiangzhou Huang, Yanhua Yu, Haoyang Li, Mengdi
Zhang, Zirui Guo, and Wei Wu. 2023. Intent-aware recommendation via disentan-
gled graph contrastive learning. In Proceedings of the Thirty-Second International
 
686Disentangled Multi-interest Representation Learning for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Joint Conference on Artificial Intelligence. 2343â€“2351.
[46] Zhibo Xiao, Luwei Yang, Wen Jiang, Yi Wei, Yi Hu, and Hao Wang. 2020. Deep
multi-interest network for click-through rate prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management.
2265â€“2268.
[47] Yueqi Xie, Jingqi Gao, Peilin Zhou, Qichen Ye, Yining Hua, Jae Boum Kim,
Fangzhao Wu, and Sunghun Kim. 2023. Rethinking multi-interest learning
for candidate matching in recommender systems. In Proceedings of the 17th ACM
Conference on Recommender Systems. 283â€“293.
[48] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen
Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph contextualized self-
attention network for session-based recommendation. In Proceedings of the 28th
International Joint Conference on Artificial Intelligence. 3940â€“3946.
[49] Haochao Ying, Fuzhen Zhuang, Fuzheng Zhang, Yanchi Liu, Guandong Xu, Xing
Xie, Hui Xiong, and Jian Wu. 2018. Sequential recommender system based
on hierarchical attention network. In IJCAI International Joint Conference on
Artificial Intelligence.
[50] Xu Yuan, Dongsheng Duan, Lingling Tong, Lei Shi, and Cheng Zhang. 2021. Icai-
sr: Item categorical attribute integrated sequential recommendation. In Proceed-
ings of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval. 1687â€“1691.
[51] Shengyu Zhang, Lingxiao Yang, Dong Yao, Yujie Lu, Fuli Feng, Zhou Zhao, Tat-
seng Chua, and Fei Wu. 2022. Re4: Learning to re-contrast, re-attend, re-construct
for multi-interest recommendation. In Proceedings of the ACM Web Conference
2022. 2216â€“2226.
8 APPENDIX
Table 6: Symbols and notations
Notation Description
ğ‘€,ğ‘ The number of users and items.
U,I The whole set of users and items.
Eğ‘¢ The userğ‘¢â€™s recentğœ…sequential behaviors.
ğ¹ The the number of usersâ€™ multi-interests.
ğ¾ The the number of item partition groups.
ğ’ğ‘“ Theğ‘“-th interest of the userâ€™s preferences.
G=(V,E) The item-confidence graph has nodes Vrepre-
senting item entities and edges Erepresenting
their relations.
ğ‘†ğ‘–ğ‘— The confidence of item-to-item edges measured
by frequency patterns of usersâ€™ behaviors.
ğ‘¤ğ‘–ğ‘˜ The partition weight of the item ğ‘–belongs to the
groupğ‘˜.
Theorem 1. (Non-overlapped spectral clustering) The com-
binatorial optimization problem in Equation (3) is equivalent to
spectral clustering based on the item-confidence graph,
ğ‘‚ğ‘ƒğ‘‡âˆâˆ’ğ¾âˆ‘ï¸
ğ‘˜=1ğ¶ğ‘¢ğ‘¡(Ağ‘˜,Â¯Ağ‘˜)+ğ‘Â·ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(|Ağ‘˜|/ğ‘),
where{A1,Â·Â·Â·,Ağ¾}denote the item partition groups which sat-
isfyâˆªğ‘˜Ağ‘˜=IandAğ‘˜1âˆ©Ağ‘˜2=âˆ….Â¯Ağ‘˜=Iâˆ’ğ´ğ‘˜denote the com-
plement set of Â¯Ağ‘˜.ğ¶ğ‘¢ğ‘¡(ğ´ğ‘–,ğ´ğ‘—)calculates the price (total weights)
of cutting off all edges between Ağ‘˜1and nodesAğ‘˜2.
Proof. For the first term in Equation (3), we have
âˆ‘ï¸
ğ‘–â‰ ğ‘—ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜Â·ğ‘†ğ‘–ğ‘—=ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘–â‰ ğ‘—ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜Â·ğ‘†ğ‘–ğ‘—
(a)=ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘–,ğ‘—âˆˆAğ‘˜ğ‘†ğ‘–ğ‘—=ğ¾âˆ‘ï¸
ğ‘˜=1ğ¶ğ‘¢ğ‘¡(Ağ‘˜,Ağ‘˜)(b)=ğ¾âˆ‘ï¸
ğ‘˜=1[ğ¶ğ‘¢ğ‘¡(I,Ağ‘˜)âˆ’ğ¶ğ‘¢ğ‘¡(Ağ‘˜,Â¯Ağ‘˜)]
=ğ¶ğ‘¢ğ‘¡(I,I)âˆ’ğ‘logğ‘âˆ’ğ¾âˆ‘ï¸
ğ‘˜=1[ğ¶ğ‘¢ğ‘¡(Ağ‘˜,Â¯Ağ‘˜)]
âˆâˆ’ğ¾âˆ‘ï¸
ğ‘˜=1ğ¶ğ‘¢ğ‘¡(Ağ‘˜,Â¯Ağ‘˜),
where Step (a) is established because of ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜=1only and if only
the pair of items ğ‘–andğ‘—are partitioned into the same group ğ‘˜. Step
(b) is established because of the linearity of the cut-off operation,
i.e.,ğ¶ğ‘¢ğ‘¡(ğ´âˆ’ğµ,ğ¶)=ğ¶ğ‘¢ğ‘¡(ğ´,ğ¶)âˆ’ğ¶ğ‘¢ğ‘¡(ğµ,ğ¶)whereğµâŠ†ğ´.
For the section term in Equation (3), we have
âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜=âˆ‘ï¸
ğ‘–logğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘—ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜
(c)=âˆ‘ï¸
ğ‘–logğ¾âˆ‘ï¸
ğ‘˜=1|Ağ‘˜|Â·I(ğ‘–âˆˆAğ‘˜)=ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
ğ‘–âˆˆAğ‘˜log|Ağ‘˜|
=ğ¾âˆ‘ï¸
ğ‘˜=1|Ağ‘˜|log|Ağ‘˜|=ğ‘Â·ğ¾âˆ‘ï¸
ğ‘˜=1|Ağ‘˜|
ğ‘log|Ağ‘˜|
=ğ‘Â·ğ¾âˆ‘ï¸
ğ‘˜=1|Ağ‘˜|
ğ‘log|Ağ‘˜|
ğ‘+ğ‘Â·ğ¾âˆ‘ï¸
ğ‘˜=1|Ağ‘˜|
ğ‘logğ‘
âˆâˆ’ğ‘Â·ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(|Ağ‘˜|
ğ‘),
where Step (c) is established becauseÃ
ğ‘—ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜=|Ağ‘˜|ifğ‘–âˆˆAğ‘˜
elseÃ
ğ‘—ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜=0.
Combining the two terms in Equation (3), we have
ğ‘‚ğ‘ƒğ‘‡âˆâˆ’ğ¾âˆ‘ï¸
ğ‘˜=1ğ¶ğ‘¢ğ‘¡(Ağ‘˜,Â¯Ağ‘˜)+ğ‘Â·ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(|Ağ‘˜|/ğ‘). (7)
â–¡
Theorem 2. (Overlapped spectral clustering) Maximizing
Equation (4) with ğ‘¤ğ‘–ğ‘˜âˆˆRis equivalent to conducting overlapped
spectral clustering based on the item-confidence graph, i.e.,
ğ‘‚ğ‘ƒğ‘‡=âˆ’ğ‘¡ğ‘Ÿ(ğ‘¾âŠ¤ğ‘³ğ‘¾)+âˆ‘ï¸
ğ‘–logexp(Ã
ğ‘˜ğ‘¤ğ‘–,ğ‘˜Â·ğ‘¤ğ‘–,ğ‘˜)Ã
ğ‘—exp(Ã
ğ‘˜ğ‘¤ğ‘–,ğ‘˜Â·ğ‘¤ğ‘—,ğ‘˜),
whereğ¿=ğ¼âˆ’ğ‘†denote the Laplacian matrix of matrix ğ‘º. It is equiv-
alent to doing spectral clustering with a repulsion regularization.
Proof.
ğ¸ğ‘.(4)=ğ‘¡ğ‘Ÿ((ğ´ğ‘šğ‘›)ğ¾Ã—ğ¾)âˆ’âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—exp(ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—ğ‘˜)
=ğ‘¡ğ‘Ÿ ğ‘¾âŠ¤(ğ‘ºâˆ’ğ‘°)ğ‘¾+ğ‘¡ğ‘Ÿ(ğ‘¾âŠ¤ğ‘°ğ‘¾)âˆ’âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—exp(ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜)
=âˆ’ğ‘¡ğ‘Ÿ(ğ‘¾âŠ¤(ğ‘°âˆ’ğ‘º)ğ‘¾)+âˆ‘ï¸
ğ‘–logexp(Ã
ğ‘˜ğ‘¤ğ‘–,ğ‘˜Â·ğ‘¤ğ‘–,ğ‘˜)Ã
ğ‘—exp(Ã
ğ‘˜ğ‘¤ğ‘–,ğ‘˜Â·ğ‘¤ğ‘—,ğ‘˜),
whereğ´ğ‘šğ‘›=Ã
ğ‘–,ğ‘—ğ‘¤ğ‘–ğ‘šğ‘¤ğ‘—ğ‘›ğ‘†ğ‘–ğ‘—denote theğ‘š-the row and the ğ‘›-th
column of the matrix (ğ´ğ‘šğ‘›)ğ¾Ã—ğ¾.ğ‘°andğ‘¡ğ‘Ÿ(Â·)denote the identity
matrix and the trace operation. â–¡
 
687KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yingpeng Du et al.
Lemma 4.1 The first term in the item partition problem can be
formulated as the expectation of P(Â·|ğ‘º)about ğ’:
âˆ‘ï¸
ğ‘–â‰ ğ‘—ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜Â·ğ‘†ğ‘–ğ‘—=EP(ğ’€|ğ‘º)logÂ©Â­
Â«Î©(ğ’€)Â·Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—ÂªÂ®
Â¬,
whereğ‘ğ‘–ğ‘—=exp(Ã
ğ‘˜ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—ğ‘˜).
Proof.
âˆ‘ï¸
ğ‘–â‰ ğ‘—ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜Â·ğ‘†ğ‘–ğ‘—=âˆ‘ï¸
ğ‘–â‰ ğ‘—ğ‘†ğ‘–ğ‘—Â·log
ğ‘’Ãğ¾
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜
(a)=âˆ‘ï¸
ğ‘–â‰ ğ‘—EP(ğ’€|ğ‘º)[I(ğ‘Œğ‘–ğ‘—=1)]Â· logğ‘ğ‘–ğ‘—
(b)=EP(ğ’€|ğ‘º)Â©Â­
Â«âˆ‘ï¸
ğ‘–â‰ ğ‘—I(ğ‘Œğ‘–ğ‘—=1)logğ‘ğ‘–ğ‘—ÂªÂ®
Â¬=EP(ğ’€|ğ‘º)Â©Â­
Â«âˆ‘ï¸
ğ‘Œğ‘–ğ‘—=1logğ‘ğ‘–ğ‘—ÂªÂ®
Â¬
(c)=EP(ğ’€|ğ‘º)logÂ©Â­
Â«Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—ÂªÂ®
Â¬+EP(ğ’€|ğ‘º)log(Î©(ğ’€))
=EP(ğ’€|ğ‘º)logÂ©Â­
Â«Î©(ğ’€)Â·Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—ÂªÂ®
Â¬,
where Step (a) is established according to EP(ğ’€|ğ‘¹)(I(ğ‘Œğ‘–ğ‘—=1))=ğ‘…ğ‘–ğ‘—;
Step (b) is established because ğ‘ğ‘–ğ‘—is independent of ğ‘Œ; Step (c) is
established according to EP(ğ’€|ğ‘¹)log(Î©(ğ’€))=0.â–¡
Lemma 4.2 Maximize the second term in the item partition
problem is equivalent to maximizing the aggregation of graphs
whose nodes have exactly one out-going edge:
âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—exp(ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜)=âˆ’logÂ©Â­
Â«âˆ‘ï¸
ğ‘ŒâˆˆYÎ©(ğ’€)Â·Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—ÂªÂ®
Â¬.
Proof.
âˆ’âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—exp(ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¤ğ‘–ğ‘˜Â·ğ‘¤ğ‘—ğ‘˜)=âˆ’âˆ‘ï¸
ğ‘–logâˆ‘ï¸
ğ‘—ğ‘ğ‘–ğ‘—
=âˆ’Â©Â­
Â«log(âˆ‘ï¸
ğ‘—ğ‘1ğ‘—)+Â·Â·Â·+ log(âˆ‘ï¸
ğ‘—ğ‘ğ‘ğ‘—)ÂªÂ®
Â¬
=âˆ’logÂ©Â­
Â«(âˆ‘ï¸
ğ‘—ğ‘1ğ‘—)Â·Â·Â·(âˆ‘ï¸
ğ‘—ğ‘ğ‘ğ‘—)ÂªÂ®
Â¬=âˆ’logÂ©Â­
Â«âˆ‘ï¸
ğœ‹âˆˆğœ…(1,ğ‘)Ã–
ğ‘–ğ‘ğ‘–ğœ‹ğ‘–ÂªÂ®
Â¬
=âˆ’logÂ©Â­
Â«âˆ‘ï¸
ğ‘ŒâˆˆYÎ©(ğ’€)Â·Ã–
I(ğ‘Œğ‘–ğ‘—=1)ğ‘ğ‘–ğ‘—ÂªÂ®
Â¬,whereğœ…(1,ğ‘)denotes the permutation set that contains all permu-
tations of[1,Â·Â·Â·,ğ‘]. â–¡
Theorem 3. (Approximation) Maximizing the proposed par-
tition problem is equivalent to optimizing the cross-entropy loss
between the two MRFs:
max OPTâ‡”maxEP(ğ’€|ğ‘º)log(P(ğ’€|ğ’)),
where ğ‘ºandğ’describe the relations in the item-confidence MRF
and the item-partition MRF, respectively.
Proof. According to Lemma (4.1) and Lemma (4.2), maximizing
the partition goals is equivalent to
EP(ğ’€|ğ‘º)logÂ©Â­
Â«Î©(ğ’€)Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—ÂªÂ®
Â¬âˆ’logÂ©Â­
Â«âˆ‘ï¸
ğ‘ŒâˆˆYÎ©(ğ’€)Ã–
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—ÂªÂ®
Â¬
=EP(ğ’€|ğ‘º)log Î©(ğ’€)Ã
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—Ã
ğ‘ŒâˆˆYÎ©(ğ’€)Ã
ğ‘Œğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—!
=EP(ğ’€|ğ‘º)log(P(ğ’€|ğ’)).
â–¡
Theorem 4. (Contrastive learning) The proposed partition
goals in Equation (3) and Equation (4) can be solved in a contrastive
learning manner, i.e.,
Lğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› =EP(ğ’€|ğ‘º)log(P(ğ’€|ğ’))
=âˆ‘ï¸
ğ‘–EM(ğ’€ğ’Š|ğ‘ºğ’Š)logexp(Ã
ğ‘˜ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—ğ‘˜)
Ã
ğ‘—â€²exp(Ã
ğ‘˜ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—â€²ğ‘˜),
whereğ‘Œğ‘–is an one-hot encoding vector with the ğ‘—-th element ğ‘Œğ‘–ğ‘—=
1.
Proof.
EP(ğ’€|ğ‘º)log(P(ğ’€|ğ’))(a)=EP(ğ’€|ğ‘º)log Ã–
ğ‘–M(ğ’€ğ’Š|ğ’ğ’Š)!
=âˆ‘ï¸
ğ‘–EP(ğ’€|ğ‘º)log(M(ğ’€ğ’Š|ğ’ğ’Š))=âˆ‘ï¸
ğ‘–EÃ
ğ‘–[M(ğ’€ğ’Š|ğ‘ºğ’Š)]log(M(ğ’€ğ’Š|ğ’ğ’Š))
(b)=âˆ‘ï¸
ğ‘–EM(ğ’€ğ’Š|ğ‘ºğ’Š)log(M(ğ’€ğ’Š|ğ’ğ’Š))=âˆ‘ï¸
ğ‘–EM(ğ’€ğ’Š|ğ‘ºğ’Š)logğ‘ğ‘–ğ‘—Ã
ğ‘—â€²ğ‘ğ‘–ğ‘—â€²
=âˆ‘ï¸
ğ‘–EM(ğ’€ğ’Š|ğ‘ºğ’Š)logexp(Ã
ğ‘˜ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—ğ‘˜)
Ã
ğ‘—â€²exp(Ã
ğ‘˜ğ‘¤ğ‘–ğ‘˜ğ‘¤ğ‘—â€²ğ‘˜).
Step (a) replaces the P(Â·)withM(Â·)according to Property 2 in MRFs;
Step (b) is obvious established because variables (ğ’€1,Â·Â·Â·,ğ’€ğ‘µ)are
independent with each other. â–¡
 
688