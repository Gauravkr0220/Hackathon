AdaRD: An Adaptive Response Denoising Framework for Robust
Learner Modeling
Fangzhou Yao
State Key Laboratory of Cognitive
Intelligence, University of Science
and Technology of China
Hefei, China
fangzhouyao@mail.ustc.edu.cnQi Liuâˆ—
State Key Laboratory of Cognitive
Intelligence, University of Science and
Technology of China & Institute of
Artificial Intelligence, Hefei
Comprehensive National Science
Center
Hefei, China
qiliuql@.ustc.edu.cnLinan Yue
State Key Laboratory of Cognitive
Intelligence, University of Science
and Technology of China
Hefei, China
lnyue@mail.ustc.edu.cn
Weibo Gao
State Key Laboratory of Cognitive
Intelligence, University of Science
and Technology of China
Hefei, China
weibogao@mail.ustc.edu.cnJiatong Li
State Key Laboratory of Cognitive
Intelligence, University of Science
and Technology of China
Hefei, China
satosasara@mail.ustc.edu.cnXin Li
University of Science and Technology
of China & Artificial Intelligence
Research Institute, iFLYTEK Co., Ltd
leexin@ustc.edu.cn
Hefei, China
leexin@ustc.edu.cn
Yuanjing He
The Open University of China
Beijing, China
heyuanjing@ouchn.edu.cn
ABSTRACT
Learner modeling is a crucial task in online learning environments,
where Cognitive Diagnosis Models (CDMs) are employed to assess
learnersâ€™ knowledge mastery levels based on recorded response
logs. However, the prevalence of noise in recorded response data
poses significant challenges, including various behaviors such as
guess and slip, casual answers, and system-induced errors. The
existence of noise degrades the accuracy of diagnosis results and
learner performance predictions. In this work, we propose a gen-
eral framework, Adaptive Response Denoising (AdaRD), designed
to salvage CDMs from the influence of noisy learner-exercise re-
sponses. AdaRD extends existing CDMs, incorporating primary
training for denoised CDMs and auxiliary training for additional
denoising support. The primary training employs binary Gener-
alized Cross Entropy (GCE) loss to slow down the large update
of learner knowledge states caused by noisy responses. Simulta-
neously, we utilize the variance of diagnosed knowledge mastery
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671684levels between primary and auxiliary diagnosis modules as a cri-
terion to downweight high-variance responses that are likely to
be noisy. In this manner, the proposed framework can prune noisy
response learning during training, thereby enhancing the accuracy
and robustness of CDMs. Extensive experiments on both real-world
and synthetic datasets validate AdaRDâ€™s effectiveness in mitigating
the impact of noisy learner-exercise responses.
CCS CONCEPTS
â€¢Applied computing â†’E-learning.
KEYWORDS
Learner modeling, Cognitive diagnosis, Noisy responses, Adaptive
denoising
ACM Reference Format:
Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, and Yuan-
jing He. 2024. AdaRD: An Adaptive Response Denoising Framework for
Robust Learner Modeling. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671684
1 INTRODUCTION
Cognitive diagnosis models (CDMs) are a fundamental computer-
assisted technology widely used in online learning platforms such
as MOOC and Coursera. It aims to assess learnersâ€™ mastery level of
knowledge concepts based on the recorded response logs [ 21,35].
This allows online learning platforms and teachers to monitor their
3886
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, and Yuanjing He
Figure 1: (a) An example of diagnosing learnersâ€™ knowledge
level with noisy response records. The right radar map is the
diagnostic knowledge level. (b) The learnerâ€™s actual knowl-
edge level.
learning states and make timely adjustments to the learning strate-
gies [ 22,31]. In general, traditional CDMs cascade the diagnosis and
the prediction module. Specifically, given the recorded response
logs (whether or not a learner answers exercises correctly, where
each exercise contains several knowledge concepts), the diagnosis
module first yields the inherent traits of the learner (knowledge mas-
tery level) and exercises (i.e. difficulty and discrimination). Then,
based solely on the obtained traits of knowledge mastery level,
difficulty and discrimination, the prediction module predicts the
probability of whether the learner will answer the exercise correctly
or not. After training, we can obtain learner knowledge mastery
level and exercise characteristics from the diagnostic module (see
Section 3.2 for model details). For example, in Figure 1(a), based on
the response logs, through CDMs, we can get the learnerâ€™s mastery
of knowledge concepts such as Alegbra.
However, the presence of noise within the recorded response
data introduces formidable challenges for CDM, stemming from
the inherent volatility of learnersâ€™ subjective behaviors and the
intricate online answering environment. Noteworthy, in this work,
noisy responses contain all the record response logs that are not
the learnerâ€™s true responses. They encompass guess and slip behav-
iors, as theorized by educational scholars [ 2,4], casual answers [ 3],
responses erroneously recorded as incorrect due to system time-
outs [ 27], and various uncontrollable factors inherent in the online
learning process. The incorporation of noisy response records in
the dataset hampers the training of CDMs, rendering the diagnosis
module incapable of accurately reflecting the actual proficiency
levels of learners. This issue is exemplified in Figure 1(a). Let us
assume a learner possesses a high proficiency level in the Algebra
knowledge concept. When faced with ğ‘’3that is related to Algebra,
the learner might answer it incorrectly. This behavior is referred to
as â€œslip behaviorâ€, where a learner generally understands a skill but
provides an incorrect response. Then, the model may erroneously
infer a low mastery level for Algebra, which is inconsistent with
the actual knowledge mastery level of the learner (Figure 1(b)).
In this paper, we argue that noisy learner-exercise response data
degenerates the inaccuracy of diagnosis results and performance
prediction, especially in deep learning-based models. These mod-
els typically assume that the collected responses is clean during
the training phase [ 11,35]. Therefore, they will learn the noisy
information as the actual information, rendering these methods
unreliable [ 42,43]. This claim underscores the imperative need to
address the inherent noise in collected learner-exercise response
records and implement response denoising during CDMâ€™s training.
Despite its significance, learning with noisy responses has sel-
dom been explored on CDMs. Among them, several diagnosticmethods have delved into slip or guess behaviors exhibited by learn-
ers during the response process. IRT [ 24] adopts a simple solution
by setting fixed slip or guess parameters to model the answering
process. Some educational psychology methods seek to enhance
diagnostic accuracy by estimating slip or guess parameters with
additional data, such as answering behaviors [ 2,39]. Nonetheless,
answering behavior data remains inaccessible and costly, and esti-
mating through these behaviors is not always reliable. Moreover,
the presence of other types of noisy responses also necessitates
consideration and systematic handling in the diagnostic process.
Given the complexity and inaccessibility of noisy responses, we
argue that denoising CDMs should meet two key requirements:
(i)it should effectively denoise noisy responses without relying on
explicit supervision signals for noise, and (ii )it should be a versatile
and adaptable framework that can be applied across various CDMs.
Based on these findings and the aforementioned issues, we pro-
pose a general denoising framework, Adaptive Response Denoising
(AdaRD), to salvage cognitive diagnosis from the noisy response
logs. Overall, AdaRD is an extension framework utilizing the back-
bone of existing CDMs, which contains primary training for de-
noised CDMs and auxiliary training for denoising assistance. Specif-
ically, based on our findings (see Section 2.2), noisy responses are
always difficult for models to learn. Meanwhile, based on the anal-
ysis in Section 3.3, we also observe that the difficult responses can
speed the update of the diagnosis module through the standard
binary cross entropy (BCE) loss. These findings lead to the conclu-
sion that noisy responses result in a large update on the diagnosis
module and CDMs will overfit to noisy responses. Therefore, to
denoise noisy responses in CDMs, we first replace the original BCE
loss in CDMs with binary GCE loss in primary training to slow
down the gradient update of the diagnosis module directed by dif-
ficult noisy responses. Then, we utilize the variance of diagnosed
knowledge mastery levels between primary and auxiliary diagnosis
modules as a criterion for downweighting noisy responses with
high variance during the learning process (see Section 3.4). In this
way, the proposed framework can prune noisy response learning
during training, enhancing the accuracy and robustness of CDMs.
Our main contributions are as follows:
â€¢We are among the first to address noisy response records in Cog-
nitive Diagnostic Modeling (CDM) and to formulate the cognitive
diagnosis task with noisy responses.
â€¢We propose a general framework for denoising CDM, Adaptive
Response Denoising (AdaRD), which involves primary training
for denoising and auxiliary training for help denoising. Primary
training slows down the learnerâ€™s state update directed by diffi-
cult noisy responses. Auxiliary training helps calculate the diag-
nosis variance for further pruning noisy responses.
â€¢Extensive experiments on both real-world and synthetic datasets
using two methods of introducing noisy responses validate the ef-
fectiveness of AdaRD in mitigating the impact of noisy responses.
2 PRELIMINARY
In this Section, we stretch by defining the notations and formu-
lating the cognitive diagnosis task with noisy responses. We then
proceed to analyze noisy responses, providing insights that inspire
the design of our proposed framework.
3887AdaRD: An Adaptive Response Denoising Framework for Robust Learner Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
2.1 Problem Formulation
Notations: Letğ‘†={ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘},ğ¸={ğ‘’1,ğ‘’2,...,ğ‘’ğ‘€}andğ¶=
{ğ‘1,ğ‘2,...,ğ‘ğ¾}denote the sets of learners, exercises and knowledge
concepts, respectively, where ğ‘,ğ‘€, andğ¾represent the size of each
set. The correlations between knowledge concepts and exercises
are denoted by a ğ‘„-matrixğ‘„âˆˆ{0,1}ğ‘€Ã—ğ¾whereğ‘„ğ‘–ğ‘—=1indicates
that question ğ‘’ğ‘–involves knowledge concept ğ‘ğ‘—for testing. The
observed response data are provided in the form of the triplet
setğ‘…={(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘–ğ‘—)|ğ‘ ğ‘–âˆˆğ‘†,ğ‘’ğ‘—âˆˆğ¸,ğ‘Ÿğ‘–ğ‘—âˆˆ {0,1}}whereğ‘Ÿğ‘–ğ‘—=1
denotes the learner ğ‘ ğ‘–answers the exercise ğ‘’ğ‘—correctly, and ğ‘Ÿğ‘–ğ‘—=0
otherwise. In this work, we exploit a practical noisy scenario, that
is, there are a few noisy responses eğ‘…in the recorded practice data
ğ‘…, where eğ‘…âŠ‚ğ‘…, and each the response eğ‘Ÿğ‘–ğ‘—âˆˆeğ‘…is the opposite
of learnerâ€™s true response to the exercise ğ‘’ğ‘—. Typically, then noisy
responses are not explicitly provided.
Definition 2.1 (Cognitive diagnosis task with noisy responses).
Given the response set ğ‘…={(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘–ğ‘—)}containing a few noisy
responses eğ‘…âŠ‚ğ‘…that are not true, the goal of cognitive diagnosis
is to provide a reliable diagnosis of actual learnersâ€™ proficiency on
specific knowledge concepts based on the noisy response data.
2.2 Analysis of Noisy Responses
Providing reliable cognitive diagnosis in our context with unknown
noise poses a challenge because we lack any explicit supervision
signals for noisy responses. In such cases, some direct and effective
methods [ 8,34] involving detecting and pruning noisy responses
during training cannot be utilized. Fortunately, research in robust
learning has proved that noisy data is more difficult to fit for many
learning tasks, e.g., image classification [ 41], which can help us
to solve the problem of overfitting to noise. To verify that this
idea is also valid in the cognitive diagnosis scenario, we draw the
training loss (the difference between the predicted response score
and the recorded response [ 23,29]) of the IRT model trained on the
ASSIST2017 dataset with introduced noise. Since explicit labels for
noise are unavailable, we flip 10% of the training response logs to
represent noisy responses, while the remaining logs are considered
normal responses. According to the results in Figure 2, we have
two key findings:
0 5 10 15 20 25 30
Epoch Number0.51.01.3Training lossNoise responses
Normal responses
Mixed responses
Figure 2: The training loss consists of normal, noisy, and
mixed responses. The mixed response loss represents the to-
tal training loss, including both normal and noisy responses.
â€¢(i) Both losses converge during the training process.
â€¢(ii) The training loss of noisy responses is higher than the loss of
normal responses (relatively clean) in the early stage.Finding (i ) reveals that CDM adapts to noisy responses, implying
that if training samples contain noise, the current model tends to
memorize them, leading to a diminished ability to accurately diag-
nose learners. In Finding (ii ), we observe that noisy data possesses
a distinctive trait â€” noisy samples are inherently difficult for the
model to learn.
3 ADAPTIVE RESPONSE DENOISING
FRAMEWORK
3.1 Overview of AdaRD
This section presents our framework Adaptive Response Denoising
(AdaRD) to solve the problem of cognitive diagnosis with noisy re-
sponses. Figure 3 illustrates the overall framework, which includes
primary training for denoised CDM (in Figure (a)) and auxiliary
training for additional denoising support (in Figure (b)). First, we
introduce the CDM backbone (see Section 3.2). Next, we elaborate
on how AdaRD extends the backbone to initialize response denois-
ing. Specifically, we propose replacing the original Binary Cross
Entropy (BCE) loss with Binary Generalized Cross Entropy (GCE)
loss [ 44] in primary training to slow down the gradient update of
learnersâ€™ knowledge states influenced by difficult noisy responses
(see Section 3.3). Subsequently, we leverage the variance of diag-
nosed knowledge mastery levels between the primary and auxiliary
diagnosis modules as a criterion to downweight high-variance re-
sponses likely to be noisy (see Section 3.4). Finally, we present the
optimization objective and learning algorithm (see Section 3.5).
3.2 Cognitive Diagnosis Backbone
AdaRD reserves the structure of CDMs as the backbone so that
AdaRD can be adapted to any CDM. As shown in Figure 3, a cog-
nitive diagnosis model can be abstractly regarded as a cascade
structure that consists of two key modules [ 19,40]: (i) The di-
agnosis module integrates a learner-centric diagnostic function
ğ‘“ğ‘ (Â·;ğœƒğ‘ )and a question-centric diagnostic function ğ‘“ğ‘’(Â·;ğœƒğ‘’), that
transform the input id index of the learner ğ‘ ğ‘–and the exercise ğ‘’ğ‘—
to the diagnosable traits. For the learner, the diagnosable trait is
his/her knowledge state ğ‘§ğ‘–:=ğ‘“ğ‘ (ğ‘ ğ‘–;ğœƒğ‘ )and for the exercise, it typi-
cally includes the difficulty or discrimination of the question, i.e.,
ğ‘£ğ‘—:=ğ‘“ğ‘’(ğ‘’ğ‘—;ğœƒğ‘’).ğœƒğ‘ andğœƒğ‘’are the learnable parameters. (ii ) The
prediction module ğ‘”ğ‘(Â·;ğœ™ğ‘)forecasts the response of learner ğ‘ ğ‘–
to exerciseğ‘’ğ‘—with learnable parameter ğœ™ğ‘, i.e., Ë†ğ‘¦ğ‘–ğ‘—:=ğ‘”ğ‘(ğ‘§ğ‘–,ğ‘£ğ‘—;ğœ™ğ‘)
by modeling the interaction between the learner and the exercises.
With the joint training of the two modules via response predictions,
the diagnosable traits can be optimized as the diagnostic results.
Typically, these two modules take different forms across different
CDMs. For example, IRT [ 24] uses the unidimensional variable ğ‘§ğ‘–
to profile a comprehensive proficiency of each learner ğ‘ ğ‘–on all the
knowledge concepts. It describes the exercise ğ‘’ğ‘—using unidimen-
sional discrimination ğ‘ğ‘—and difficulty ğ‘ğ‘—. IRT takes a logistic-like in-
teractive function for prediction: ğ‘ƒ(ğ‘¦ğ‘–ğ‘—|ğ‘§ğ‘–,ğ‘ğ‘—,ğ‘ğ‘—)=1
1+ğ‘’âˆ’1.7ğ‘ğ‘—(ğ‘§ğ‘–âˆ’ğ‘ğ‘—).
While NeuralCD [ 35] diagnoses multi-dimensional traits of learn-
ersğ‘§ğ‘–âˆˆRğ¾, where each element ğ‘§ğ‘–ğ‘—is learnerğ‘ ğ‘–â€™s proficiency on
conceptğ‘ğ‘—. It exploits neural networks to fit higher-order learner-
exercise interactions, i.e., ğ‘ƒ(ğ‘¦ğ‘–ğ‘—)=ğ¹(ğ‘„ğ‘—â—¦(ğ‘§ğ‘–âˆ’ğ‘ğ‘—)Â·ğ‘ğ‘—)where
ğ‘ğ‘—âˆˆRğ¾andğ‘ğ‘—âˆˆR1andğ‘„ğ‘—is theğ‘—-th row ofğ‘„-matrix.
3888KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, and Yuanjing He
Figure 3: The Adaptive Response Denoising (AdaRD) frame-
work for cognitive diagnosis with noisy responses. It consists
of primary training for denoised CDM (in Figure (a)) and aux-
iliary training for additional denoising support (in Figure
(b)). Both of these models utilize the CD backbone containing
the diagnosis and prediction modules (see Section 3.2).
Notably, to ensure the prediction interpretability, CDMs should
strictly follow the Monotonicity Assumption [ 30]: a learnerâ€™s knowl-
edge mastery level is monotonic with the probability of correctly
responding to a test exercise, i.e., ğœƒğ‘>0. In summary, to optimize
the parameters ğœƒğ‘ ,ğœƒğ‘’, andğœ™ğ‘, a binary cross entropy (BCE) loss
function is typically employed to minimize the difference between
the predicted score and the recorded one as follows:
LC=âˆ’1
|ğ‘…|âˆ‘ï¸
ğ‘Ÿğ‘–âˆˆğ‘…(ğ‘Ÿğ‘–ğ‘—logË†ğ‘¦ğ‘–ğ‘—+(1âˆ’ğ‘Ÿğ‘–ğ‘—)log(1âˆ’Ë†ğ‘¦ğ‘–ğ‘—)). (1)
3.3 Slow Down the Diagnosis Update
The significance of obtaining reliable knowledge states from learn-
ers is crucial for shielding CDMs from the adverse effects of noisy
response instances during training. In Section 2.2, we observed that
noisy responses result in a large loss value during the training pro-
cess, indicating that the noisy samples are more difficult to fit for
CDMs. Thereby, we are motivated to explicitly model the difficulty
feature of each Response Sample to assist the de-noising objective.
Definition 3.1 (Response Sample Difficulty). The difficulty level
of each response is calculated by the difference value within the
prediction scores of the current model Ë†ğ‘¦ğ‘–ğ‘—and the recorded response
resultsğ‘Ÿğ‘–ğ‘—. The larger the difference, the higher the difficulty level.
In the following, we will argue that the existing optimization
tends to focus on learning from difficult noisy instances, leading to
a severe problem where the diagnosis module updates excessively
with those noisy responses. Therefore, it is imperative to enhance
the optimization with a more effective approach.
Gradient update of the diagnosis module. As mentioned in
Section 3.2, existing CDMs are learned through performance pre-
diction with the Binary Cross Entropy (BCE) loss using stochastic
gradient-based techniques [ 9,35], which may inadvertently encour-
age the model to overfit to the noisy response instances that are
likely with high difficult levels. Here, we analyze the gradient up-
date on the diagnosis module. The gradient update of the diagnosismodule, parameterized by ğœƒğ‘ , with the standard BCE loss (Eq. 1), is
as follows:
âˆ‘ï¸
(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘– ğ‘—)âˆˆğ‘…ğœ•LC(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—)
ğœ•ğœƒğ‘ =âˆ‘ï¸
(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘– ğ‘—)âˆˆğ‘…âˆ’(ğ‘Ÿğ‘–ğ‘—
Ë†ğ‘¦ğ‘–ğ‘—âˆ’1âˆ’ğ‘Ÿğ‘–ğ‘—
1âˆ’Ë†ğ‘¦ğ‘–ğ‘—)âˆ‡ğœƒğ‘ Ë†ğ‘¦ğ‘–ğ‘—,
(2)
whereğœƒğ‘ are the parameters of the diagnosis module in CDM. Con-
sidering a recorded correct response (incorrect one similarly), we
notice that noisy instances with high difficulty level â€” indicating
less agreement with the response resultâ€”are given more weight
in the update concerning diagnosis parameters ğœƒğ‘ (specifically, a
higher weight ofğ‘Ÿğ‘–,ğ‘—
Ë†ğ‘¦ğ‘– ğ‘—). This scrutiny reveals a significant trend: BCE
loss makes CDMs tend to fit to the noisy responses that can not
reflect learnersâ€™ actual responses such as guess and slip behavior.
Slow done the update. To mitigate the significant update on
the diagnosis module induced by BCE, our framework employs
the binary Generalized Cross Entropy (GCE) loss [ 44] to alleviate
overfitting to noisy responses. Here, we present the binary GCE loss
function designed to minimize the distance between the predicted
response score Ë†ğ‘¦ğ‘–ğ‘—and the recorded response result ğ‘Ÿğ‘–ğ‘—:
LG(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—)=1
|ğ‘…|âˆ‘ï¸
(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘– ğ‘—)âˆˆğ‘…1âˆ’Ë†ğ‘¦ğ‘
ğ‘–ğ‘—
ğ‘Â·ğ‘Ÿğ‘–ğ‘—+1âˆ’(1âˆ’Ë†ğ‘¦ğ‘–ğ‘—)ğ‘
ğ‘Â·(1âˆ’ğ‘Ÿğ‘–ğ‘—),
(3)
whereğ‘âˆˆ(0,1]is a hyperparameter. The corresponding gradient
on the diagnosis module is given by:
ğœ•LG(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—)
ğœ•ğœƒğ‘ =âˆ’
Ë†ğ‘¦ğ‘âˆ’1
ğ‘–ğ‘—Â·ğ‘Ÿğ‘–ğ‘—âˆ’(1âˆ’Ë†ğ‘¦ğ‘–ğ‘—)ğ‘âˆ’1(1âˆ’ğ‘Ÿğ‘–ğ‘—)
âˆ‡ğœƒğ‘ Ë†ğ‘¦ğ‘–ğ‘—.
(4)
Compared to the gradient of BCE loss (Eq. 2), the GCE loss assigns
a smaller weight Ë†ğ‘¦ğ‘
ğ‘–ğ‘—in the diagnosis moduleâ€™s update. When ğ‘Ÿğ‘–ğ‘—=1,
the relation between binary GCE and standard BCE is:
ğœ•LG(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—)
ğœ•ğœƒğ‘ =Ë†ğ‘¦ğ‘
ğ‘–ğ‘—Â·ğœ•LC(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—)
ğœ•ğœƒğ‘ . (5)
This implies that responses with lower prediction scores Ë†ğ‘¦ğ‘–ğ‘—
receive a smaller weight due to a multiplication term Ë†ğ‘¦ğ‘
ğ‘–ğ‘—compared
to the BCE loss. In this fashion, learning the CDM with the binary
GCE loss can safeguard CDMs from perturbations induced by noisy
responses to a certain extent.
3.4 Uncertainty-based Down-weighting
The GCE loss optimization can alleviate the impact of noisy re-
sponses by considering the discrepancy between predicted and
recorded responses. However, some challenging types of noise per-
sist, particularly those associated with response shifts in uncertain
knowledge states, such as guess or slip response behaviors [ 6].
Drawing from educational psychology, we posit the following as-
sumption:
Assumption 1. When a learnerâ€™s knowledge state is more uncer-
tain, there is a higher likelihood of generating noisy responses.
Here we aim to estimate the response uncertainty as a criterion
for downweighting potential noisy responses. This strategy aims
to eliminate the negative impacts of noisy training instances and
further enhance the CDMâ€™s robustness in the presence of noise. The
learnerâ€™s states ğ‘§ğ‘–in CDM actually is the hidden states obtained
3889AdaRD: An Adaptive Response Denoising Framework for Robust Learner Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
from the diagnosis layer. Drawing inspiration from [ 45], we esti-
mate uncertainty by measuring the distance between two diagnosis
networks. In AdaRD (see Figure 3), alongside the primary CDM opti-
mized with binary GCE, we devise an auxiliary cognitive diagnosis
training model parameterized by ğ‘Šâˆ—=(ğœƒâˆ—ğ‘ ,ğœƒâˆ—ğ‘’,ğœ™âˆ—ğ‘)and optimized
with BCE lossLC(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—). The diagnosis uncertainty on learner ğ‘ ğ‘–
is estimated through the variance of diagnosed knowledge mastery
levels (ğ‘§ğ‘–:=ğ‘“ğ‘ (ğ‘ ğ‘–,ğœƒğ‘ )andğ‘§âˆ—
ğ‘–=ğ‘“ğ‘ (ğ‘ ğ‘–,ğœƒâˆ—ğ‘ )) produced by the diagnosis
modules in primary and auxiliary CDMs. In our framework, we
calculate diagnosis uncertainty for downweighting potential noisy
responses:
Definition 3.2 (Diagnosis Uncertainty). Given knowledge states
ğ‘§ğ‘–andğ‘§âˆ—
ğ‘–produced by the diagnosis modules in the primary and
auxiliary CDMs, diagnosis uncertainty is calculated as the variance
betweenğ‘§ğ‘–andğ‘§âˆ—
ğ‘–using the KL-divergence:
ğ‘‰ğ‘ğ‘Ÿ(ğ‘§ğ‘–,ğ‘§âˆ—
ğ‘–)=ğ·ğ¾ğ¿ ğœ(ğ‘§ğ‘–)||ğœ(ğ‘§âˆ—
ğ‘–). (6)
Theğœis the softmax function and the ğ·ğ¾ğ¿function is the KL-
divergence loss which measures the difference between two distri-
butions. In Eq. 6, if the two diagnosis modules produce different
knowledge mastery levels for learner ğ‘ ğ‘–, the variance ğ‘‰ğ‘ğ‘Ÿ(ğ‘§ğ‘–,ğ‘§âˆ—
ğ‘–)
will be large, reflecting the degree of uncertainty in CDMâ€™s diag-
nosis for this learner. The higher the uncertainty, the noisier the
response made by this learner. Here, we use the diagnosis uncer-
tainty as a criterion for noise by setting a clean confidence weight:
ğ‘¤ğ‘–=exp{âˆ’ğ‘‰ğ‘ğ‘Ÿ(ğ‘§ğ‘–,ğ‘§âˆ—
ğ‘–)
ğ‘‡ğ‘Š}, (7)
whereğ‘‡ğ‘Šâˆˆ(0,1]is the temperature parameter. A larger variance
value resulting in a smaller ğ‘¤ğ‘–indicates that the learner would
more possibly have made noisy responses. We then introduce this
weight into the primary training with Eq. 3 to alleviate the effects
of noisy responses during training:
LGW(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—)=âˆ’1
|ğ‘…|âˆ‘ï¸
(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘– ğ‘—)âˆˆğ‘…ğ‘¤ğ‘–Â·1âˆ’Ë†ğ‘¦ğ‘
ğ‘–ğ‘—
ğ‘Â·ğ‘Ÿğ‘–ğ‘—
+ğ‘¤ğ‘–Â·1âˆ’(1âˆ’Ë†ğ‘¦ğ‘–ğ‘—)ğ‘
ğ‘Â·(1âˆ’ğ‘Ÿğ‘–ğ‘—).(8)
3.5 Optimization
During the training stage, we aim to slow down the gradient update
of noisy response in the primary CDM. Simustimously, we further
downweight possibly noisy responses with the larger variance of
diagnosed learner knowledge states between the primary and aux-
iliary CDM. In that, AdaRD is trained by the uncertainty-based
lossLGW (Eq. 8) of the primary CDM and standard loss LCâˆ—
(Eq. 1) of the auxiliary CDM cooperatively. Moreover, we introduce
a regularization term to prevent CDM from learning the large vari-
anceğ‘‰ğ‘ğ‘Ÿ(ğ‘§ğ‘–,ğ‘§âˆ—
ğ‘–)throughğ‘¤ğ‘–all the time via adding regularization
ğ‘‰ğ‘ğ‘Ÿ(ğ‘§ğ‘–,ğ‘§âˆ—
ğ‘–/ğ‘‡ğ‘‰), whereğ‘‡ğ‘‰is to make the distribution of ğ‘§âˆ—
ğ‘–more
concentrated especially when the dimension of knowledge states
is multiple. This operation emphasizes fit to the distribution of
primary knowledge states ğ‘§ğ‘–. The final training loss is:
LF(Ë†ğ‘¦ğ‘–ğ‘—,ğ‘Ÿğ‘–ğ‘—)=LGW+LCâˆ—+1
|ğ‘…|ğ›¼Â·âˆ‘ï¸
(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘– ğ‘—)âˆˆğ‘…ğ‘‰ğ‘ğ‘Ÿ(ğ‘§ğ‘–,ğ‘§âˆ—
ğ‘–
ğ‘‡ğ‘‰),(9)where theğ›¼âˆˆ(0,1)is a hyperparameter that controls the weights
of the regularization term. The pseudo-code example of the AdaRD
framework for cognitive diagnosis with noisy response is shown
in Algorithm 1. After the training, the trained diagnostic module
ğ‘“ğ‘ (Â·;ğœƒğ‘ )in primary CDM on recorded response data (containing
noisy responses) can be used to obtain the actual traits of the learner
ğ‘§ğ‘–. We show the strong robustness of AdaRD by following extensive
experiments and analysis.
4 EXPERIMENTS
In this section, we first introduce the experimental setup in Section
4.1, followed by the learner performance under noisy responses
(Section 4.2) and the detailed analyses of our proposed framework
(Section 4.3 - 4.4 and Appendix A). Overall, we conduct extensive
experiments to answer the following questions:
â€¢RQ1: Can AdaRD effectively diagnose learnersâ€™ actual states with
noisy responses?
â€¢RQ2: How well does AdaRD handle different response flip ratios?
â€¢RQ3: How does AdaRD perform with each denoising design?
â€¢RQ4: How does uncertainty-based downweighting perform with
different distance functions?
â€¢RQ5: How do hyperparameters affect AdaRDâ€™s performance (see
Appendix A.1)?
4.1 Experimental Setup
4.1.1 Datasets. Experiments are conducted on two real-world datasets,
ASSIST2017 and Junyi to validate the effectiveness of the AdaRD
framework:
â€¢ASSIST20171is a dataset for the 2017 Data Mining Competition
provided by ASSIST2017s, tracking learnersâ€™ learning from 2004-
2007. We use a subset of the original response data starting from
2006 to 2007.
â€¢Junyi2contains online learning logs from a Chinese platform,
widely used for evaluating online education tasks [ 11]. We ran-
domly selected records from 10,000 learners for our experiments.
1https://sites.google.com/view/ASSIST2017sdatamining/home
2https://pslcdatashop.web.cmu.edu/Project?id=244
Algorithm 1 Adaptive Response Denoising Framework
Input: Training Set ğ‘…={(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘–ğ‘—)}containing noisy responses
(ğ‘Ÿğ‘–ğ‘—â‰ eğ‘Ÿğ‘–ğ‘—), Q-matrix
Output: Denoised primary CDM parameters ğ‘Š
Initialize primary CDM parameters ğ‘Šand auxiliary CDM
parametersğ‘Šâˆ—randomly;
1:while not converge do
2: Sample a mini-batch ğ‘…ğµâˆˆğ‘…of size B.
3: Input(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘–ğ‘—)to two CDMs and estimate learner knowl-
edge mastery level using ğ‘“ğ‘ (Â·;ğœƒğ‘ ). Calculate diagnosis vari-
ance using KL-divergence based on Eq.6.
4: Compute variance weights with Eq.7.
5: Calculate variance-based primary loss based on Eq.8.
6: Calculate standard loss for auxiliary model based on Eq.1.
7: Combine the two losses and the regularization term with
respect to Eq. 9, and update parameters ğ‘Šandğ‘Šâˆ—.
8:end while
3890KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, and Yuanjing He
Table 1: The statistics of datasets
Datasets ASSIST2017 Junyi
#learners 788 10,000
#exercises 1,715 717
#knowledge concepts 87 40
#response records 76,740 567,430
#records per learner 97 57
Data statistics of these datasets are depicted in Table 1.
4.1.2 Synthesize Response Noise. Since the precise label of noisy
responses is unknown, neither clean nor noisy data can be obtained.
Therefore, a workable plan is to simulate noisy responses. In our ex-
periments, we synthesize the noisy training data using two methods
to simulate noisy responses in the learning process:
â€¢Response flipping: Simulates subjective guess and slip behavior
as well as erroneous recording by randomly flipping response
resultsğ‘Ÿğ‘–ğ‘—to their opposite one ğ‘Ÿâ€²
ğ‘–ğ‘—.
â€¢Response generating: Generates random responses that do
not reflect studentsâ€™ actual knowledge states by randomly sam-
pling(ğ‘ âˆ—
ğ‘–,ğ‘’âˆ—
ğ‘—,ğ‘Ÿâˆ—
ğ‘–ğ‘—)from existing pools of learners, exercises, and
recorded responses, respectively.
Both methods are applied to the entire dataset with a specified ratio.
For instance, if the flipping ratio is 10%, then 10% of the response
logs will be flipped (i.e., from correct to incorrect).
4.1.3 Baselines. We conduct the AdaRD on both traditional mod-
els (IRT [ 24], MIRT [ 29]) and deep learning based models (Neu-
ralCD [35], KaNCD [36]), as described below:
â€¢IRT [24], a widely adopted statistical model in cognitive diag-
nosis, employs single-dimensional variables to characterize trait
features using the logistic function.
â€¢MIRT [29] serves as the multidimensional extension of IRT, en-
compassing models that capture various knowledge proficiency
level of learners and exercises.
â€¢NeuralCD [35], a prevalent deep learning-based CDM, employs
a multilayer perceptron (MLP) to model intricate and high-order
interactions between learners and exercises.
â€¢KaNCD [36] models the associations among knowledge concepts
based on the architecture of NeuralCD.
4.1.4 Implementation Details. For models employing NeuralCD as
diagnostic functions, we set the dimensions of learner and exer-
cise vectors equal to the number of diagnosed knowledge con-
cepts. Regarding the hyperparameter ğ‘in Eq. (3), we set it to
0.3on ASSIST2017 and 0.2 on Junyi. The temperature ğ‘‡ğ‘‰of the
regularization term is set to 5for multi-dimension models MIRT
and NeuralCD, and the hyperparameter ğ›¼is set to 0.5. Follow-
ing the approach in [ 35], for each dataset, we filter learners with
fewer than 15 historical response records to ensure that every
learner has sufficient exercise records for diagnosis. The mini-
batch size is set to 256 for all datasets and models. The learning
rate is tuned in{0.0001,0.0005,0.001,0.002,0.003,0.005}and fixed
equally for both primary and auxiliary CDMs. Each model is im-
plemented using PyTorch. All network parameters are initialized
with Xavier initialization [ 12] and optimized using the Adam opti-
mizer [ 18]. For each experiment, we conduct 5 independent trialsand report the average test metric results. All experiments are ex-
ecuted on a Linux server equipped with two 3.00GHz Intel Xeon
Gold 5317 CPUs and one Tesla A100 GPU. Code is released at
https://github.com/fannazya/AdaRD.
4.1.5 Evaluation Metrics. As the true knowledge mastery level
remains unknown, assessing the performance of a CDM directly
becomes challenging. Building on prior research [ 9], a reasonable
solution is to measure performance by the prediction scores in
diagnosis models as the diagnostic results can be acquired through
learnersâ€™ performance prediction task. In this context, we assess the
model using classification and regression metrics, including AUC,
ACC, RMSE andF1score.
4.2 Learner Performance Comparison (RQ1)
We initially present the results of learner performance prediction
to evaluate the denoising effectiveness of the AdaRD framework
on Cognitive Diagnosis (CD) backbones introduced in Section 4.1.3.
Table 2 and 3 show the performance of the AdaRD framework
applied to the four standard baselines trained on synthetic noisy
training data using responses flipping and generating operation.
The settings of a noise ratio at 10% and 20% are based on educational
studies focused on guess or slip behaviors [ 1,4]. Then both of them
are tested on normal data without flipping or generating. Table 4
shows the results of real-world ASSIST2017 datasets where the
training data is relatively clean. From these tables, we have the
following findings:
â€¢Finding 1: Examining the results of the ASSIST2017 datasets
presented in Tables 2, 3, and 4, we observe a significant decrease in
performance across the four baselines trained on synthetic noisy
training data using Response Flipping andResponse Generat-
ingtechniques. This decline highlights the adverse effects of noisy
responses on CDM. Moreover, when comparing the performance
of standard baselines on synthetic data generated by response gen-
erating with that on real-world datasets, we find that the decline is
less pronounced than with the flipping operation. In fact, in some
instances such as NeuralCD and KaNCD, even when the generat-
ing ratio is 20%, the performance remains basically the same as
with unoperated training. This phenomenon can be attributed to
the presence of â€œcleanâ€ responses within the randomly generated
responses that align with the actual knowledge states of learners.
â€¢Finding 2: In terms of the denoising ability, Table 2 demonstrates
that AdaRD has superior denoising ability compared to the stan-
dard baseline across all synthetic training datasets, particularly in
flipping datasets. Paired t-tests confirm these improvements are
statistically significant with ğ‘<0.05. AdaRDâ€™s performance at
20% noise rivals the baselines at 10% noise, indicating its ability
to handle 10% more noise. Additionally, the improvement at a 20%
flipping ratio is more pronounced than at 10%, showing that AdaRD
enhances CDMâ€™s robustness to severely noisy datasets and its broad
applicability to most CDMs. Notably, AdaRDâ€™s efficacy extends to
traditional methods like IRT and MIRT, highlighting its potential in
educational measurement. However, the performance on Junyi is
less pronounced than on ASSIST2017, possibly due to the cleaner
collection of Junyi data, suggesting our method can also serve as
an assessment of dataset cleanliness.
â€¢Finding 3: Table 4 illustrates that the AdaRD framework en-
hances the performance of learner performance prediction without
3891AdaRD: An Adaptive Response Denoising Framework for Robust Learner Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Experimental results of learner performance prediction Synthetic Noisy datasets with 10%and 20%flip ratio (FR). The
better learner performance prediction is highlighted in bold, and â†‘(â†“) means the higher (lower) score, the better performance.
These markers are also for the following results.
Metho
dASSIST2017-10%
FR ASSIST2017-20% FR Junyi-10%
FR Junyi-20% FR
AUC%â†‘A
CC%â†‘RMSE%â†“F1score%â†‘AUC%â†‘ACC%â†‘RMSE%â†“F1score%â†‘AUC%â†‘A
CC%â†‘RMSE%â†“F1score%â†‘AUC%â†‘ACC%â†‘RMSE%â†“F1score%â†‘
IRT 74.51
68.62 45.39 66.36 71.40 66.09 46.64 64.22 80.18
81.09 37.31 88.42 78.43 80.47 39.21 88.07
IRT-AdaRD 75.71
69.22 44.88 66.88 72.68 66.99 46.01 65.35 80.77
81.62 36.49 88.62 79.50 80.75 37.84 88.21
MIRT 73.11
66.81 45.96 64.79 69.55 64.71 47.19 63.04 79.80
80.87 37.30 88.15 77.86 80.27 38.10 87.74
MIRT-AdaRD 76.59
69.84 44.55 67.72 74.39 68.30 45.29 66.47 80.83
81.48 36.52 88.54 79.96 81.06 37.58 88.23
NeuralCD 70.82
65.43 47.23 63.49 68.60 63.36 47.71 62.76 79.41
79.56 38.30 87.14 78.75 79.27 40.11 87.08
NeuralCD-AdaRD 73.59
67.72 47.23 65.39 71.72 65.90 47.04 65.15 80.03
80.09 37.34 87.72 79.37 79.72 38.15 87.47
K
aNCD 73.78
67.94 45.87 63.87 70.12 65.03 46.89 63.55 79.40
79.99 38.04 87.65 78.98 79.50 39.88 87.22
KaNCD-AdaRD 74.56
68.33 45.26 65.13 71.14 65.74 46.59 62.86 79.85
80.31 37.47 87.98 79.28 80.06 38.76 87.73
Table 3: Experimental results on Synthetic Noisy datasets
with 10%and 20%generating ratio (GR).
Metho
dASSIST2017-10%
GR ASSIST2017-20% GR
AUC%â†‘A
CC%â†‘RMSE%â†“F1score%â†‘AUC%â†‘ACC%â†‘RMSE%â†“F1score%â†‘
IRT 76.01
69.84 44.83 67.55 75.71 69.60 44.92 67.33
IRT-AdaRD 77.35
70.40 44.39 68.28 76.80 70.24 44.56 68.12
MIRT 75.34
68.89 45.08 66.75 74.32 68.21 45.48 66.17
MIRT-AdaRD 77.59
70.88 44.10 68.81 77.22 70.16 44.20 67.94
NeuralCD 72.40
66.71 46.27 63.25 72.27 65.89 46.60 60.62
NeuralCD-AdaRD 74.98
68.65 46.12 66.09 74.63 68.39 46.04 64.52
K
aNCD 75.92
68.50 44.98 69.08 76.18 68.50 46.31 60.93
KaNCD-AdaRD 76.62
70.12 44.80 67.28 76.70 69.77 45.13 68.96
Table 4: Experimental results of learner performance predic-
tion on real-world datasets.
Metho
dASSIST2017
AUC%â†‘A
CC%â†‘RMSE%â†“F1score%â†‘
IRT 76.91
70.50 44.44 68.08
IRT-AdaRD 77.77
71.06 44.37 68.77
MIRT 75.55
68.81 45.09 66.68
MIRT-AdaRD 77.78
70.73 44.18 68.53
NeuralCD 72.68
66.68 46.49 63.90
NeuralCD-AdaRD 75.10
68.84 46.77 66.33
K
aNCD 76.11
69.48 45.33 66.59
KaNCD-AdaRD 76.76
70.19 45.02 67.23
introducing noise responses. This improvement is likely attributed
to the prevalence of noisy responses in the recorded response data
collected from the online learning platform, with some successfully
denoised during training by our proposed framework.
4.3 Robustness of Denoising (RQ2)
To assess the robustness of the AdaRD framework against noise, we
present its performance by AUC values on the ASSIST2017 dataset
in Table 5. The evaluations span various flip ratios within the set
{5%,10%,20%,30%,40%}. Results show that AdaRD consistently out-
performs the standard CD baselines, particularly in NeuralCD and
MIRT, where the diagnostic knowledge states are multi-dimensional.
This showcases the effectiveness and broad applicability of our
AdaRD combating noise. Furthermore, AdaRDâ€™s efficacy is more
pronounced at larger flip ratios such as 20-30%, demonstrating its
strong robustness in diagnosing responses with higher noise levels.
When the noise level rises to 40%, AdaRDâ€™s efficacy is slight. Ac-
tually, in such high noise, it is almost impossible to infer reliable
features for identifying noisy responses in the training process.Table 5: AUC results of learner performance prediction with
individual flip ratio { 5%, 10%, 20%, 30%,40% }.
Metho
d 5% 10% 20% 30% 40%
MIRT
73.56 73.11 69.55 64.34 58.04
MIRT-AdaRD 77.58 76.59 74.39 70.12 59.01
NeuralCD
71.96 70.81 68.60 65.02 59.02
NeuralCD-AdaRD 74.17 73.59 71.72 67.99 61.00
No FR 10%FR 20%FR0.650.700.75AUC
MIRT
AdaD/w.o. V
AdaD/w.o. G
AdaD
No FR 10%FR 20%FR0.400.450.47RMSE
MIRT
AdaD/w.o. V
AdaD/w.o. G
AdaD
Figure 4: Several ablated variantsâ€™ performance on AS-
SIST2017 with different flip ratios (No FR, 10% FR, 20% FR)
with several ablated variants.
4.4 Ablation Studies (RQ3 & 4)
4.4.1 Effects of denoising components (RQ3). To validate the ef-
fectiveness of each component designed in AdaRD, we conducted
experiments on the synthetic datasets ASSIST2017 (with a 20% flip
ratio) using several ablated variants. Specifically, â€œw.o. Vâ€ represents
without uncertainty-based weights controlling the loss contribu-
tion, whereğ‘¤ğ‘–=1. â€œw.o. Gâ€ means without the GCE loss controlling
the gradient update weights, where we use the CE loss instead. The
results are illustrated in Figure 4.
Firstly, in terms of the AUC and RMSE values of two variants,
AdaRD demonstrates consistently superior efficacy when unifying
both components. Furthermore, AdaRD/w.o. V and AdaRD/w.o. G
show a performance decrease compared to AdaRD, indicating the
vital role each module plays in the overall framework. The decrease
in AdaRD/w.o. Vâ€™s performance suggests that the variance of diag-
nosed states helps in detecting noisy responses, while the decrease
in AdaRD/w.o. Gâ€™s performance highlights the robustness of binary
GCE to noise through slowing down gradient updates. Notably,
compared to the standard baseline MIRT, AdaRD/w.o. V shows
an increase, while its contribution is smaller than AdaRD/w.o. G,
indicating the significant efficacy of uncertainty-based weighting.
3892KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, and Yuanjing He
Table 6: AUC value of employing difference distance function
in MIRT and NeuralCD on Junyi datasets with 20% FR.
Distance function MIRT NeuralCD
Origin 77.86 78.75
MAE 78.82 79.08
MSE 78.60 78.82
KL-divergence (ours) 79.96 79.37
4.4.2 Effects of Distance Function (RQ4). In our framework, we
employ the KL-divergence in Eq. 6 to quantify the variance of
diagnosed knowledge states between the primary and auxiliary di-
agnosis modules. Alternative metrics, such as Mean Squared Error
(MSE) and Mean Absolute Error (MAE), are also viable for calcu-
lating this distance. Additional experiments incorporating these
distance calculation methods are presented in Table 6. Compared to
the original MIRT and NeuralCD, both performances improved no
matter what distance function is used in AdaRD, showing the effec-
tiveness of our proposed framework using variance as a criterion
for pruning noisy responses. Besides, the observed performances
are closely aligned, indicating that the framework exhibits low
sensitivity to the choice of distance metric. Nevertheless, the KL-
divergence outperforms MSE and MAE by a margin due to our use
of KL-divergence to maintain the dominant role of primary CDM.
Consequently, we opt for KL-divergence as the preferred metric in
our framework.
5 RELATED WORK
Cognitive Diagnosis. Cognitive Diagnosis Models (CDMs) are
employed to evaluate learnersâ€™ knowledge proficiency based on
their responses to exercises obtained from online learning plat-
forms. The most representative method, Item Response Theory
(IRT) [ 24], is a standard statistical model for cognitive diagnosis
that utilizes single-dimension variables to represent trait features
and employs the logistic function. Later, Multidimensional Item Re-
sponse Theory (MIRT) [ 29] proposed the use of multidimensional
trait features instead of a single dimension. The advent of deep
learning has sparked significant interest in cognitive diagnosis,
leading to the development of various models [ 9,35]. Wang et al .in-
troduced the NeuralCD framework, employing neural networks to
learn the interaction function between learners and their responses.
Additionally, deep learning-based CDMs have been devised to ad-
dress practical challenges in online education, such as domain-level
zero-shot cognitive diagnosis [ 10] and diagnosis using incremental
response logs [ 32]. It is noteworthy, however, that both traditional
models and newer deep learning-based models share a common
assumption: they presume that the recorded data is clean during
the training of models, neglecting to explicitly address the presence
of noisy responses.
Noisy Responses in Educational Psychology. In our work,
one kind of noisy response in our work is guess and slip behaviors,
an important research topic in educational psychology. A â€œguessâ€
occurs when a learner, despite lacking a comprehensive understand-
ing of a knowledge concept, provides a correct response (slip is the
inverse scenario). Several educational psychology methods [ 2,4,17]
focus on elucidating these behaviors to enhance our understanding
of the response process. The DINA (Deterministic Input, Noisy Andgate) [ 7] diagnoses the mastery state by binary variables and intro-
duces guessing or slip probabilities to account for occasional errors.
Identifying guess and slip responses poses a significant challenge,
demanding meticulous analysis and the consideration of educa-
tional psychology priors [ 28], along with additional behavioral data
from learners [ 5]. This is a stringent criterion for the recorded
dataset. In this study, the identification of guess or slip responses,
coupled with the recognition of other anomalies, is integral in char-
acterizing noisy data within the response records. Addressing these
perturbations is imperative during the CDM training process to
ensure the precise inference of learnersâ€™ knowledge states.
Learning with Noisy Label. Learning with noise is a vital topic
in data-driven methods, such as robust learning [ 20,33,41], im-
age classification [14, 15, 43] and recommender system [16, 37]. A
straightforward solution is to identify noisy instances and remove
them during training [ 25,26,38]. This is challenging because some
difficult samples while clean may be confused with noisy ones. Di-
rectly removing them may injure some difficult yet clean samples.
Instead, some methods reweigh the final loss that imposes different
importance to each example for a weighted training [ 13,37]. Nu-
merous methods have been proposed for learning with noisy labels
with DNNs in recent years, limited emphasis has been placed on
addressing the intricacies associated with noisy responses within
the realm of CDM. This oversight is noteworthy denoising CDM
training is very challenging in that it comprises learner-exercise-
knowledge interaction data [21].
6 CONCLUSION AND FUTURE WORK
In this work, we investigated the significant but rarely explored is-
sue of cognitive diagnosis with noisy responses in the recorded data.
Our proposed framework, Adaptive Response Denoising (AdaRD),
extended existing CDMs by introducing a primary training phase,
employing binary Generalized Cross Entropy (GCE) loss instead
of standard BCE loss to mitigate the impact of noisy responses on
the diagnosis module. Simultaneously, the auxiliary training was
utilized to calculate the variance of diagnosed knowledge mastery
levels between the primary and auxiliary diagnosis modules. Then,
the variance was used to downweight high-variance responses in
primary training, effectively pruning potentially noisy data. Exten-
sive experiments on both real-world and synthetic datasets using
two methods of introducing noise have validated the efficacy of
AdaRD in mitigating the impact of noisy learner-exercise responses.
This work pioneers the integration of noise-aware training strate-
gies within the context of cognitive diagnosis, contributing to a
broader understanding of handling noisy responses in educational
data. Firstly, it is advocated to further refine response denoising
by utilizing response behavior data, such as response time and the
number of hints given when answering. Moreover, this paradigm,
specialized in learner modeling, may provide insights or be applied
to other user modeling systems for interaction denoising, such as
social media and recommender systems.
ACKNOWLEDGMENTS
This research was supported by grants from the National Key Re-
search and Development Program of China (Grant No. 2021YFF09010
00), the National Natural Science Foundation of China (No. 62337001),
and the Fundamental Research Funds for the Central Universities.
3893AdaRD: An Adaptive Response Denoising Framework for Robust Learner Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
REFERENCES
[1]Deepak Agarwal, Nishant Babel, and Ryan S Baker. 2018. Contextual Derivation
of Stable BKT Parameters for Analysing Content Efficacy.. In EDM.
[2]Scott E Allen, Anna McLean Phillips, and Rene F Kizilcec. 2021. Student per-
ceptions of pre-assessments:â€œItâ€™s basically just guessing anyways.â€. In Physics
Education Research Conference, 2021: Making Physics More Inclusive and Eliminat-
ing Exclusionary Practices in Physics. American Association of Physics Teachers
(AAPT), 27â€“32.
[3]Winfred Arthur Jr, Ellen Hagen, and Felix George Jr. 2021. The lazy or dishon-
est respondent: Detection and prevention. Annual Review of Organizational
Psychology and Organizational Behavior 8 (2021), 105â€“137.
[4]Ryan SJ d Baker, Albert T Corbett, and Vincent Aleven. 2008. More accurate
student modeling through contextual estimation of slip and guess probabilities
in bayesian knowledge tracing. In Intelligent Tutoring Systems: 9th International
Conference, ITS 2008, Montreal, Canada, June 23-27, 2008 Proceedings 9. Springer,
406â€“415.
[5]Ryan SJ d Baker, Albert T Corbett, Sujith M Gowda, Angela Z Wagner, Benjamin A
MacLaren, Linda R Kauffman, Aaron P Mitchell, and Stephen Giguere. 2010. Con-
textual slip and prediction of student performance after use of an intelligent tutor.
InUser Modeling, Adaptation, and Personalization: 18th International Conference,
UMAP 2010, Big Island, HI, USA, June 20-24, 2010. Proceedings 18. Springer, 52â€“63.
[6]Albert T Corbett and John R Anderson. 1994. Knowledge tracing: Modeling the
acquisition of procedural knowledge. User modeling and user-adapted interaction
4, 4 (1994), 253â€“278.
[7]Jimmy De La Torre. 2009. DINA model and parameter estimation: A didactic.
Journal of educational and behavioral statistics 34, 1 (2009), 115â€“130.
[8]Rob Fergus, Yair Weiss, and Antonio Torralba. 2009. Semi-supervised learning in
gigantic image collections. Advances in neural information processing systems 22
(2009).
[9]Weibo Gao, Qi Liu, Zhenya Huang, Yu Yin, Haoyang Bi, Mu-Chun Wang, Jianhui
Ma, Shijin Wang, and Yu Su. 2021. Rcd: Relation map driven cognitive diagnosis
for intelligent education systems. In Proceedings of the 44th International ACM
SIGIR Conference on Research and Development in Information Retrieval. 501â€“510.
[10] Weibo Gao, Qi Liu, Hao Wang, Linan Yue, Haoyang Bi, Yin Gu, Fangzhou Yao,
Zheng Zhang, Xin Li, and Yuanjing He. 2024. Zero-1-to-3: Domain-Level Zero-
Shot Cognitive Diagnosis via One Batch of Early-Bird Students towards Three
Diagnostic Objectives. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, Vol. 38. 8417â€“8426.
[11] Weibo Gao, Hao Wang, Qi Liu, Fei Wang, Xin Lin, Linan Yue, Zheng Zhang,
Rui Lv, and Shijin Wang. 2023. Leveraging transferable knowledge concept
graph embedding for cold-start cognitive diagnosis. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 983â€“992.
[12] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics. 249â€“256.
[13] Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong,
Matthew R Scott, and Dinglong Huang. 2018. Curriculumnet: Weakly supervised
learning from large-scale web images. In Proceedings of the European conference
on computer vision (ECCV). 135â€“150.
[14] Yan Han, SOUMAVA ROY, Lars Petersson, and Mehrtash Harandi. 2020. Learning
from noisy labels via discrepant collaborative training. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision. 3169â€“3178.
[15] Ahmet Iscen, Jack Valmadre, Anurag Arnab, and Cordelia Schmid. 2022. Learn-
ing with neighbor consistency for noisy labels. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 4672â€“4681.
[16] Yogesh Jhamb, Travis Ebesu, and Yi Fang. 2018. Attentive contextual denois-
ing autoencoder for recommendation. In Proceedings of the 2018 ACM SIGIR
international conference on theory of information retrieval. 27â€“34.
[17] Ã–mÃ¼r Kaya Kalkan and Ä°smail Ã‡UHADAR. 2020. An evaluation of 4PL IRT and
DINA models for estimating pseudo-guessing and slipping parameters. Journal of
Measurement and Evaluation in Education and Psychology 11, 2 (2020), 131â€“146.
[18] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[19] Jiatong Li, Qi Liu, Fei Wang, Jiayu Liu, Zhenya Huang, Fangzhou Yao, Linbo Zhu,
and Yu Su. 2024. Towards the Identifiability and Explainability for Personalized
Learner Modeling: An Inductive Paradigm. In Proceedings of the ACM on Web
Conference 2024. 3420â€“3431.
[20] Fangfu Liu, Wenchang Ma, An Zhang, Xiang Wang, Yueqi Duan, and Tat-Seng
Chua. 2023. Discovering Dynamic Causal Space for DAG Structure Learning.
arXiv preprint arXiv:2306.02822 (2023).
[21] Qi Liu. 2021. Towards a New Generation of Cognitive Diagnosis. In Proceedings
of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI
2021, Virtual Event / Montreal, Canada, 19-27 August 2021. ijcai.org, 4961â€“4964.
https://doi.org/10.24963/ijcai.2021/703[22] Qi Liu, Shiwei Tong, Chuanren Liu, Hongke Zhao, Enhong Chen, Haiping Ma,
and Shijin Wang. 2019. Exploiting cognitive structure for adaptive learning.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 627â€“635.
[23] Qi Liu, Runze Wu, Enhong Chen, Guandong Xu, Yu Su, Zhigang Chen, and Guop-
ing Hu. 2018. Fuzzy cognitive diagnosis for modelling examinee performance.
ACM Transactions on Intelligent Systems and Technology (TIST) 9, 4 (2018), 1â€“26.
[24] Frederic M Lord. 1980. Applications of item response theory to practical testing
problems. Routledge.
[25] Yueming Lyu and Ivor W Tsang. 2019. Curriculum loss: Robust learning and
generalization against label corruption. arXiv preprint arXiv:1905.10045 (2019).
[26] Eran Malach and Shai Shalev-Shwartz. 2017. Decoupling" when to update" from"
how to update". Advances in neural information processing systems 30 (2017).
[27] D Joe Olmi, Robert C Sevier, and Deborah F Nastasi. 1997. Time-in/time-out
as a response to noncompliance and inappropriate behavior with children with
developmental disabilities: Two case studies. Psychology in the Schools 34, 1
(1997), 31â€“39.
[28] Zacharoula Papamitsiou and Anastasios A Economides. 2016. Process mining of
interactions during computer-based testing for detecting and modelling guessing
behavior. In Learning and Collaboration Technologies: Third International Confer-
ence, LCT 2016, Held as Part of HCI International 2016, Toronto, ON, Canada, July
17-22, 2016, Proceedings 3. Springer, 437â€“449.
[29] Mark D Reckase. 2009. Multidimensional item response theory models. In
Multidimensional item response theory. Springer, 79â€“112.
[30] Paul R Rosenbaum. 1984. Testing the conditional independence and monotonicity
assumptions of item response theory. Psychometrika 49, 3 (1984), 425â€“435.
[31] Bo Shen and Ang Chen. 2006. Examining the interrelations among knowledge,
interests, and learning strategies. Journal of Teaching in Physical Education 25, 2
(2006), 182â€“199.
[32] Shiwei Tong, Jiayu Liu, Yuting Hong, Zhenya Huang, Le Wu, Qi Liu, Wei Huang,
Enhong Chen, and Dan Zhang. 2022. Incremental Cognitive Diagnosis for Intelli-
gent Education. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 1760â€“1770.
[33] Arash Vahdat. 2017. Toward robustness against label noise in training deep
discriminative neural networks. Advances in neural information processing systems
30 (2017).
[34] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge
Belongie. 2017. Learning from noisy large-scale datasets with minimal supervi-
sion. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion. 839â€“847.
[35] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yuying Chen, Yu Yin, Zai Huang,
and Shijin Wang. 2020. Neural cognitive diagnosis for intelligent education
systems. In Proceedings of the AAAI Conference on Artificial Intelligence. 6153â€“
6161.
[36] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yu Yin, Shijin Wang, and Yu Su.
2022. NeuralCD: a general framework for cognitive diagnosis. IEEE Transactions
on Knowledge and Data Engineering (2022).
[37] Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021.
Denoising implicit feedback for recommendation. In Proceedings of the 14th ACM
international conference on web search and data mining. 373â€“381.
[38] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and
Shu-Tao Xia. 2018. Iterative learning with open-set noisy labels. In Proceedings
of the IEEE conference on computer vision and pattern recognition. 8688â€“8696.
[39] Bihan Xu, Zhenya Huang, Jiayu Liu, Shuanghong Shen, Qi Liu, Enhong Chen,
Jinze Wu, and Shijin Wang. 2023. Learning behavior-oriented knowledge tracing.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2789â€“2800.
[40] Fangzhou Yao, Qi Liu, Min Hou, Shiwei Tong, Zhenya Huang, Enhong Chen, Jing
Sha, and Shijin Wang. 2023. Exploiting non-interactive exercises in cognitive
diagnosis. Interaction 100, 200 (2023), 300.
[41] Chun-Kit Yeung and Dit-Yan Yeung. 2018. Addressing two problems in deep
knowledge tracing via prediction-consistent regularization. In Proceedings of the
fifth annual ACM conference on learning at scale. 1â€“10.
[42] Linan Yue, Qi Liu, Yichao Du, Yanqing An, Li Wang, and Enhong Chen. 2022.
DARE: disentanglement-augmented rationale extraction. Advances in Neural
Information Processing Systems 35 (2022), 26603â€“26617.
[43] Linan Yue, Qi Liu, Ye Liu, Weibo Gao, Fangzhou Yao, and Wenfeng Li. 2024. Coop-
erative classification and rationalization for graph generalization. In Proceedings
of the ACM on Web Conference 2024. 344â€“352.
[44] Zhilu Zhang and Mert Sabuncu. 2018. Generalized cross entropy loss for training
deep neural networks with noisy labels. Advances in neural information processing
systems 31 (2018).
[45] Zhedong Zheng and Yi Yang. 2021. Rectifying pseudo label learning via un-
certainty estimation for domain adaptive semantic segmentation. International
Journal of Computer Vision 129, 4 (2021), 1106â€“1120.
3894KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, and Yuanjing He
0.10.20.3 0.5 0.7 0.9
Flip ratio0.600.650.700.75AUC
10% FP 20% FP 30% FP 40% FP
0.10.20.3 0.5 0.7 0.9
Flip ratio0.600.640.680.72ACC
Figure 5: Effects of parameter ğ‘on response denoising at
different flip ratios (10% FR, 20% FR, 30% FR, 40% FR).
Figure 6: (a) A Learnerâ€™s response logs contain a flipped oper-
ation on exercise ğ‘’1associated with knowledge concept (KC)
A. (b) Diagnosis results of NeuralCD and NeuralCD-AdaRD.
A MORE EXPERIMENTAL RESULTS
A.1 Hyper-parameter sensitivity (RQ5)
The parameter ğ‘in the objective function (Eq.3) controls the up-
date rate of the diagnosis module by weighting Ë†ğ‘¦ğ‘
ğ‘–ğ‘—. We conductedexperiments with different values of ğ‘and varied the noise level
by adjusting the flipping ratio (FR) while keeping all other settings
constant. The results are illustrated in Figure 5. We observe that
when the noise level is low, such as when the FR is 10%, a small
ğ‘value at 0.1 or 0.2 is effective enough for denoising. When the
FR increases to 30%-40%, a larger q at 0.3 or 0.4 is better. Notably,
under relatively clean training data, AdaRD with a high ğ‘value,
such as 0.8, results in lower test accuracy. Conversely, when the
noise level is high, a larger ğ‘value leads to higher test accuracy.
Therefore, the ğ‘value should be set according to the cleanliness of
the response dataset to prevent CDMs from overfitting noisy data.
A.2 Case Study
In Figure 6, we present a case study to demonstrate the robustness
of AdaRD when applied in NeuralCD for learner modeling under
noisy responses. Figure 6 (a) displays both the original responses
and the flipped responses of the learners, while the bar charts in
Figure 6 (b) represent the diagnosis reports on A,B,C knowledge
concepts generated by NeuralCD and AdaRD-NeuralCD, respec-
tively. It is observed that when a learnerâ€™s response to exercise ğ‘’1,
associated with knowledge concept A, was flipped from correct to
incorrect, NeuralCD incorrectly inferred a low mastery level for the
related knowledge concept A. In contrast, AdaRDâ€™s diagnosis re-
sults maintained a higher mastery level for the relevant knowledge
concept, indicating AdaRDâ€™s capability to identify noisy responses
and protect NeuralCD from the impact of this flip operation.
Besides, we tested the trained modelâ€™s predictions on the train-
ing set to detect flipped responses. A successful detection occurs
if the predicted label matches the original response without re-
sponse flipping operation. MIRT-AdaRD detects 61.98% of flipped
responses, while MIRT detects 49.31%, suggesting that our method
can effectively detect noisy responses in learner-exercise response
data collected from online platforms.
3895