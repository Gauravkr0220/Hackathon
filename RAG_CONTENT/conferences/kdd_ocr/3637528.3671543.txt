Choosing a Proxy Metric from Past Experiments
Nilesh Tripuraneni
nileshtrip@google.com
Google DeepMind
San Francisco, CA, USALee Richardson
leerich@google.com
Google
San Bruno, CA, USAAlexander Dâ€™Amour
alexdamour@google.com
Google DeepMind
Cambridge, MA, USA
Jacopo Soriano
jacoposoriano@google.com
Google
San Bruno, CA, USASteve Yadlowsky
yadlowsky@google.com
Google DeepMind
San Francisco, CA, USA
Abstract
In many randomized experiments, the treatment effect of the long-
term metric (i.e. the primary outcome of interest) is often difficult
or infeasible to measure. Such long-term metrics are often slow
to react to changes and sufficiently noisy they are challenging to
faithfully estimate in short-horizon experiments. A common alter-
native is to measure several short-term proxy metrics in the hope
they closely track the long-term metric â€“ so they can be used to
effectively guide decision-making in the near-term. We introduce a
new statistical framework to both define and construct an optimal
proxy metric for use in a homogeneous population of randomized
experiments. Our procedure first reduces the construction of an
optimal proxy metric in a given experiment to a portfolio optimiza-
tion problem which depends on the true latent treatment effects
and noise level of experiment under consideration. We then denoise
the observed treatment effects of the long-term metric and a set of
proxies in a historical corpus of randomized experiments to extract
estimates of the latent treatment effects for use in the optimization
problem. One key insight derived from our approach is that the
optimal proxy metric for a given experiment is not apriori fixed;
rather it should depend on the sample size (or effective noise level)
of the randomized experiment for which it is deployed. To instan-
tiate and evaluate our framework, we employ our methodology
in a large corpus of randomized experiments from an industrial
recommendation system and construct proxy metrics that perform
favorably relative to several baselines.
CCS Concepts
â€¢Applied computing â†’Mathematics and statistics ;â€¢Math-
ematics of computing â†’Statistical paradigms.
Keywords
Proxy Metrics; A/B testing; Experimentation; Causal Surrogates;
Meta-Analysis
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671543ACM Reference Format:
Nilesh Tripuraneni, Lee Richardson, Alexander Dâ€™Amour, Jacopo Soriano,
and Steve Yadlowsky. 2024. Choosing a Proxy Metric from Past Experiments.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671543
1 Introduction
Randomized controlled trials (RCTs) are the gold standard approach
for measuring the causal effect of an intervention [ 15]; however,
designing and analyzing high-quality RCTs requires various con-
siderations to ensure scientifically robust results. For example, an
experimenter must clearly define the intervention, control, and
choose a primary outcome for the study. In this work, we will as-
sume that the intervention and control are clearly defined, and
consider the problem of choosing a good primary outcome. A com-
mon approach is to choose the primary outcome to be a key metric
which drives downstream decision-making. Such metrics are critical
components in the decision-making pipelines of many large-scale
technology companies [ 4,23] as well as used to guide policy de-
cisions in economics and medicine [ 1,12]. Unfortunately, direct
measurement of such a metric can be impractical or infeasible. In
many cases, they are long-term outcomes observed with a signifi-
cant temporal delay, making them slow to move (i.e. insensitive)
in the short term, and inherently noisy. Moreover, they may be
prohibitively expensive to query.
On the other hand, proxy metrics (or surrogates) that are eas-
ier to measure or faster to react are often available to use in lieu
of the long-term outcome. For example in clinical settings, CD4
white-blood cell counts in blood serve as a surrogate for mortality
due to AIDS [ 12], while in online experimentation platforms diver-
sity of consumed content serves a proxy for long-term visitation
frequencies [ 30]. A significant literature exists on designing and
analyzing proxy metrics and experiments that use them as a pri-
mary outcome. One important question addressed by this literature
is choosing (or combining) proxy metrics to be a good surrogate for
measuring the effect of the intervention on the long-term outcome
[1,16,19,22,29â€“31]. To do so, one needs a principled reason for
why the measured treatment effect on the proxy outcome is related
to the treatment effect on the long-term outcome. Frequently, this is
done by making causal assumptions about the relationship between
the treatment, proxy outcome, and long-term outcome [see, e.g.,
1,17,28]. However, motivated by the unique way that trials are run
in technology product applications, we take a different approach
5803
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Nilesh Tripuraneni, Lee Richardson, Alexander Dâ€™Amour, Jacopo Soriano, & Steve Yadlowsky
based on statistical regularity assumptions in a population of ex-
periments, similar to meta-analytic approaches such as that taken
by Elliott et al. [12].
RCTs performed in technology products are typically referred to
as A/B tests. They are used for a wide variety of applications in the
technology industry, however one of the most common applications
is for assessing the effect of a candidate launch of a new product
feature or change on the userâ€™s experience. If the results of the A/B
test suggest that the candidate launch has a positive effect on the
user experience, then it will be deployed to all users. Depending
on the scale of the product and engineering team, many candidate
launches requiring many A/B tests may be required on a regular
basis. The results of these A/B tests on long-term outcomes and
proxy metrics are logged, serving as a history of past candidate
launches that we may use to guide the choice of future proxy metrics
to use for decision making.
This perspective has been studied in the technology research
literature previously, for example in Deng et al . [8], Richardson et al .
[24], Wang et al . [30] , to develop useful heuristics for choosing a
proxy metric for use in future A/B tests. In this work, we define a
precise statistical framework for choosing a proxy metric based on
this historical A/B test data, and develop a method for optimizing a
composite proxy, an affine combination of base proxy metrics, that
can be used as a primary outcome for future A/B tests.
The central contributions of our paper are the following:
â€¢We define a new notion of optimality for a proxy metric we
term proxy quality, for use in a homogeneous population of
randomized experiments. Our definition differs from the ex-
isting literature in that it phrases optimality as ensuring the
observed proxy metric closely tracks the unobserved popula-
tiontreatment effect on the long-term outcome (see Figure 1).
Conveniently, our definition also packages two important con-
siderations for a proxy metric â€“ short-term sensitivity of the
proxy metric and directional alignment with the long-term
outcome â€“ into a single objective (see Equation (2)).
â€¢We show this new notion of proxy quality can be used to con-
struct optimal proxy metrics for new A/B tests via an efficient
two-step procedure. The first step reduces the construction of
an optimal weighted combination of base proxies, that maxi-
mize our definition of proxy quality, to a classic portfolio opti-
mization problem. This optimization problem is a function of
the latent variability of the unobservable population treatment
effects and the noise level of the new experiment under con-
sideration (see Section 2.1.2). We then use a hierarchical model
to denoise the observed treatment effects on the proxy and
long-term outcome in a historical corpus of A/B tests to extract
the variation in the unobserved population treatment effects
(see Section 2.2). The variance estimates of the population TEs
are then used as plug-ins to the aforementioned optimization.
â€¢We highlight the adaptivity of our proxy metric procedure to
the inherent noise level of each experiment for which it will be
used. In our framework the optimal proxy metric for a given
experiment is not apriori fixed. Rather it should depend on
the sample size (or effective noise level) of the randomizedexperiment for which it is deployed in order to profitably trade-
off bias from disagreement with the long-term outcome and
intrinsic variance (see Section 2.1.1 and Figure 3).
â€¢Finally, we instantiate and evaluate our framework on a set of
307 real A/B tests from an industrial recommendation engine
showing how the proxy metrics we construct can improve
decision-making in a large-scale system (see Section 3).
1.1 Statistical Framework
Consider a corpus of ğ¾randomized experiments (or A/B tests)
where theğ‘–âˆˆ{1,...,ğ¾}-th experiment is of sample size ğ‘›ğ‘–. In each
experiment, there is a specific intervention that has some popula-
tion treatment effect (TE) that we denote Î”ğ‘–.1In the experiment,
we measure an estimated TE Ë†Î”ğ‘–on the subset of the population
included in our experiment. Note that the entire population may
be included in the experiment, but Ë†Î”ğ‘–remains a random quantity,
given Î”ğ‘–, because of the random assignment of treatments in the
experiment.
To differentiate between the TE on the long-term outcome and
the proxy metrics, we will attach a superscript ğ‘as inÎ”ğ‘
ğ‘–orË†Î”ğ‘
ğ‘–
for the long-term outcome (we use ğ‘since these are sometimes
referred to as north star metrics), and a superscript ğ‘ƒas in âˆ†ğ‘ƒ
ğ‘–and
Ë†âˆ†ğ‘ƒ
ğ‘–for the proxy metrics. Note that there may be multiple base
proxy metrics, so âˆ†ğ‘ƒ
ğ‘–âˆˆRğ‘‘. Throughout, we assume that ğ‘›ğ‘–is large
enough, and that the experimental design is sufficiently regular
such that conditional on (Î”ğ‘
ğ‘–,âˆ†ğ‘ƒ
ğ‘–),(Ë†Î”ğ‘
ğ‘–,Ë†âˆ†ğ‘ƒ
ğ‘–)is well-approximated
by a Normal distribution centered around the population TE2(due
to the central limit theorem); and that the joint (within-experiment)
covariance of(Ë†Î”ğ‘
ğ‘–,Ë†âˆ†ğ‘ƒ
ğ‘–), denoted as ğšµ, has a good estimator, denoted
asË†ğšµ. Our discussion is agnostic to the precise estimator of the TEs
and their covariances. We only require black-box access to their
values. For the historical corpus of ğ¾randomized experiments, we
assume that these triplicates of measurements {(Ë†Î”ğ‘
ğ‘–,Ë†ğš«ğ‘ƒ
ğ‘–,Ë†ğšµğ‘–)}ğ¾
ğ‘–=1
are available.
Our goal in this paper is to revisit the proxy metric problem:
the selection of short-term proxy metrics (or a weighted combina-
tion thereof) that track the long-term outcome in a new ğ¾+1st
experiment where measurements of the long-term outcome are
unavailable, but measurements of short-term proxy metrics are. In
order to develop a statistical framework to construct proxies in a
new experiment, we leverage a meta-analytic approach to model
the relationship between different experiments. To this end, we
assume the population TEs for each experiment are drawn i.i.d.
from a common joint distribution D,
Î”ğ‘
ğ‘–
âˆ†ğ‘ƒ
ğ‘–
âˆ¼D(Â·) i.i.d. (1)
supported over Rğ‘‘+1. We acknowledge this assumption is strong
and not suitable for all applications. However, in our motivating
application of interest â€“ studying a corpus of A/B tests from a large
1This may be an average TE (ATE) or relative ATE, but we will not emphasize the
differences between these two as they can be handle similarly in our work. For our
dataset we use relative ATEs.
2Note that in our framework, the notion of proxy quality in Section 2.1.1 and Sec-
tion 2.1.2 only relies on low-order moments and doesnâ€™t explicitly require Gaussianity
(although does use unbiasedness) but the estimation procedure in Section 2.2 makes
explicit use of this structure.
5804Choosing a Proxy Metric from Past Experiments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
technology company â€“ historical intuition and various tests do
not provide significant evidence this assumption is violated. The
approach of placing a distributional prior on the population ATE in
similar settings of A/B testing at large-scale technology companies
[7,9] as well as other meta-analytic studies of RCTs [ 11,12] has
also been advocated for as a useful assumption in prior work.
2 Methods
With the above setting in place we first define the measure of
quality of a proxy metric which relates the estimated TE on the
short-term proxies to the population TE on the long-term outcome
for a new experiment. Subsequently, we show how the relevant
latent parameters contained in the definition of proxy quality can
be efficiently estimated via a hierarchical model.
2.1 Optimal Proxy Metrics
In order to judge the quality of a proxy metric we first define a
new notion of utility for a proxy metric. Our key insight is that in a
new experiment3where
Î”ğ‘âˆ†ğ‘ƒâŠ¤
âˆ¼D(Â·) , the observed TEs of
the short-term proxies Ë†âˆ†ğ‘ƒshould closely track the latent TE of the
long-term outcome (see Figure 1).4This is because decisions that
are intended to move Î”ğ‘will be made on the basis of Ë†âˆ†ğ‘ƒ. Thus,
we would like these quantities to be well-correlated.
Noisy AB test with  samplesn +Îâ‹…Ïµ
Figure 1: In a new experiment, we view the observed TEs
as being generated from their corresponding (unobserved)
latent values by a noisy channel which adds independent,
mean-zero experimental noise with covariance ğšµ. In this
new experiment the noisy, observed long-term outcome Ë†Î”ğ‘
is inaccessible. We seek to find noisy proxy metrics whose
TEs closely track the population TEs on the long-term out-
come.
2.1.1 Proxy Quality of a Single Short-Term Metric in a New Exper-
iment. For simplicity, we first consider the case when the vector-
valued sequence of proxies reduces to a single scalar proxy. In order
to capture the intuition that the short-term estimated proxy TE, Ë†Î”ğ‘ƒ
should track the population long-term outcome TE, Î”ğ‘we define
theproxy quality as the correlation between the aforementioned
quantities. The correlation is a simple and natural measure which
captures the predictive relationship between the proxy metric and
long-term outcome. Under stronger conditions in Section 2.2, we
3In the following since we assume all the experiments are i.i.d. we suppress index
notation on this arbitary experiment drawn from D(Â·) .
4Note in a new experiment the estimated treatment effect on the long-term outcome
Ë†Î”ğ‘may be unavailable.also argue that optimizing for this measure of proxy quality min-
imizes the probability of a (signed) decision error or surrogate
paradox.
In our setting we consider the case where the estimated TEs are
unbiased estimators of their underlying latent population quantities
â€“ so we can parameterize Ë†Î”ğ‘ƒ=Î”ğ‘ƒ+âˆš
Îğ‘ƒğ‘ƒÂ·ğœ–, whereğœ–is an
independent random zero-mean, unit-variance random variable.
Hence we can define and simplify the proxy quality as:
corr(Î”ğ‘,Ë†Î”ğ‘ƒ)=Cov(Î”ğ‘,Î”ğ‘ƒ)p
Var(Î”ğ‘)(Var(Î”ğ‘ƒ)+Îğ‘ƒğ‘ƒ)
=corr(Î”ğ‘,Î”ğ‘ƒ)q
1+Îğ‘ƒğ‘ƒ
Var(Î”ğ‘ƒ). (2)
In our setting, the definition of proxy quality decomposes the pre-
dictive relationship between the estimated proxy TE and popu-
lation long-term outcome TE into latent predictive correlation
corr(Î”ğ‘ƒ,Î”ğ‘)â€“ a property of the distribution D(Â·) â€“ and an ef-
fective inverse signal-to-noise ratio Îğ‘ƒğ‘ƒ/Var(Î”ğ‘ƒ)â€“ which is also a
function of the noise level of the experiment. We now make several
comments on the aforementioned quantities.
â€¢The latent predictive correlation corr(Î”ğ‘ƒ,Î”ğ‘)tracks the align-
ment between the population proxy metric TE and the pop-
ulation long-term outcome TE. In particular, this correlation
is reflective of the intrinsic predictive quality of a fixed proxy
metric. This quantity is not easily accessible since we do not
directly observe data sampled from D(Â·) . We return to the issue
of estimating such quantities in Section 2.2.
â€¢The quantity Îğ‘ƒğ‘ƒ/Var(Î”ğ‘ƒ)computes the ratio of the within-
experiment noise in the estimated proxy metric TEâ€”due to fluc-
tuations across experimental units and treatment assignmentsâ€”
to the latent variation of the population proxy metric TE across
experiments. For the former quantity we expect Îğ‘ƒğ‘ƒto depend
on the size of the randomized experiment in consideration (i.e.
Îğ‘ƒğ‘ƒâˆ¼1
ğ‘›, whereğ‘›is the sample size of the experiment), since
it is a variance over independent treatment units. Meanwhile
Var(Î”ğ‘ƒ)captures how easily the population proxy metric TE
moves in the experiment population D(Â·) . In a (large enough)
given experiment, Îğ‘ƒğ‘ƒis easily estimated by Ë†Îğ‘ƒğ‘ƒusing the
sample covariance estimator. Meanwhile, Var(Î”ğ‘ƒ)is difficult
to measure directly, just like corr(Î”ğ‘,Î”ğ‘ƒ). We later show how
to use a hierarchical model to estimate these parameters (see
Section 2.2). Finally, it is worth noting this ratio term in the
denominator is also closely related to a formal definition of
metric sensitivity which appears in the A/B testing literature
[7, 24].
Together the numerator and denominator in Equation (2) trade
off two (often competing) desiderata into a single objective: the
numerator favors alignment with the population TE on the long-
term outcome while the denominator downweights this by the
signal-to-noise ratio of the proxy metric.5One unique property
of this proxy quality measure is that, given a set of base proxies,
the â€œoptimal" single proxy is not an intrinsic property of the proxy
metric or distribution of treatment effects captured by D(Â·) . Rather,
5This tradeoff between directional alignment of the proxy metric/long-term outcome
and sensitivity of the proxy metric is further discussed in [24].
5805KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Nilesh Tripuraneni, Lee Richardson, Alexander Dâ€™Amour, Jacopo Soriano, & Steve Yadlowsky
it also depends on the experimental design. Specifically, it is a
function of the experiment sample size ğ‘›, which will control the
size of Îğ‘ƒğ‘ƒ. This behavior represents a form of bias-variance trade-
off. For large sample sizes, as Îğ‘ƒğ‘ƒâ†’0, Equation (2) will favor
less biased metrics whose population-level TEs are aligned with
the long-term outcome (i.e. the numerator is large). Meanwhile, for
small sample sizes where Îğ‘ƒğ‘ƒis large, Equation (2) will favor less
noisy metrics with a high signal-to-noise ratio so the denominator
is small.
2.1.2 Composite Proxy Quality in a New Experiment. The previous
discussion on assessing the quality of a single proxy metric captures
many of the important features behind our approach. However, in
practice, we are often not restricted to picking a single proxy metric
to approximate the long-term outcome. Rather, we are free to con-
struct a composite proxy metric which is a convex combination
of the TEs of a set of base proxy metrics to best predict the effect
on the long-term outcome.
In our framework, the natural extension to the vector-valued
setting takes a convex combination of the proxies corr(Î”ğ‘,wâŠ¤Ë†âˆ†ğ‘ƒ)
for a normalized weight vector w, instead of restricting ourselves
a single proxy. However, beyond just defining the quality of a
weighted proxy metric, we can also optimize for the quality of this
weighted sum of base proxy metrics:
max
wâˆˆRğ‘‘corr(Î”ğ‘,wâŠ¤Ë†âˆ†ğ‘ƒ):1âŠ¤w=1,wâ‰¥0. (3)
Again, the estimated TEs are unbiased estimators of their under-
lying latent population quantities, so we can parameterize Ë†âˆ†ğ‘ƒ=
âˆ†ğ‘ƒ+
ğšµğ‘ƒğ‘ƒ1/2
Â·ğ, where ğis an independent random zero-mean,
identity-covariance random vector. So the objective expands to:
max
wâˆˆRğ‘‘1p
Var(Î”ğ‘)wâŠ¤Cov(Î”ğ‘,âˆ†ğ‘ƒ)q
wâŠ¤(Cov(âˆ†ğ‘ƒ,âˆ†ğ‘ƒ)+ğšµğ‘ƒğ‘ƒ)w:1âŠ¤w=1,wâ‰¥0.
(4)
Essentially all considerations noted in the previous section translate
to the vector-valued setting mutatis mutandis. In particular, the
numerator in Equation (4) captures the alignment between the true
latent weighted proxy and the population long-term outcome, while
the denominator downweights the numerator by the effective noise
in each particular experiment. As before we expect ğšµğ‘ƒğ‘ƒâˆ¼1
ğ‘›with
the sample size, ğ‘›, of the experiment. Hence, the optimal weights
for a given experiment will adapt to the noise level (or equivalently
sample size) of the experiment run.
The formulation in Equation (4) raises the question of how to effi-
ciently compute w. Indeed, at first glance the optimization problem
as phrased in Equation (4) is non-convex. Fortunately, the objective
in Equation (4) (up to a constant pre-factor) maps exactly onto the
Sharpe ratio (or reward-to-volatility ratio) maximization problem
often encountered in portfolio optimization [ 26,27]. As is well-
known in the optimization literature, the program in Equation (4)
can be converted to an equivalent (convex) quadratic programming
problem which can be efficiently solved [ 6, Section 8.2]. We briefly
detail this equivalence explicitly in Appendix B.
The portfolio perspective also lends an additional interpretation
to the objective in Equation (4). If we analogize each specific proxy
metric as an asset to invest in, then Cov(Î”ğ‘,âˆ†ğ‘ƒ)is the returnsvectors of our assets, and Cov(âˆ†ğ‘ƒ,âˆ†ğ‘ƒ)+ğšµğ‘ƒğ‘ƒis their effective
covarianceâ€“so wâŠ¤(Cov(âˆ†ğ‘ƒ,âˆ†ğ‘ƒ)+ğšµğ‘ƒğ‘ƒ)wcaptures the risk of our
portfolio of proxies. Just as in portfolio optimization, where two
highly-correlated assets should not be over-invested in, if two proxy
metrics are both strongly aligned with the long-term outcome, but
are themselves very correlated, the objective in Equation (4) will
not assign high weights to both of them.
2.2 Estimation of Latent Parameters via a
Hierarchical Model
As the last piece of our framework, we finally turn to the question
of obtaining estimates of the unobservable latent quantities arising
in Equation (4). While ğšµğ‘ƒğ‘ƒis easily estimated from the within-
experiment sample covariance Ë†ğšµğ‘ƒğ‘ƒ, the quantities Cov(Î”ğ‘,âˆ†ğ‘ƒ),
Cov(âˆ†ğ‘ƒ,âˆ†ğ‘ƒ)are tied to the latent, unobservable population TEs of
the proxy metrics and long-term outcome.
In order to gain a handle on these quantities, we take a meta-
analytic approach which combines two key pieces. First, as in our
previous discussion, we require the setting described in Equation (1)
â€“ that is we assume the true population TEs are drawn i.i.d. from a
common joint distribution. While only this assumption was needed
for our previous discussion, we now introduce additional para-
metric structure in the form of an explicit generative model to
allow for tractable estimation of the parameters Cov(Î”ğ‘,âˆ†ğ‘ƒ)and
Cov(âˆ†ğ‘ƒ,âˆ†ğ‘ƒ). Second, we assume access to a pool of homogeneous
RCTs for which unbiased estimates of the TE on the short-term
proxy metrics and long-term outcome are available (i.e. (Ë†Î”ğ‘
ğ‘–,Ë†âˆ†ğ‘ƒ
ğ‘–)
forğ‘–âˆˆ{1,...,ğ¾}). With these two pieces we can construct a hier-
archical (or linear mixed) model to estimate the latent parameters:
Î”ğ‘
ğ‘–
âˆ†ğ‘ƒ
ğ‘–
âˆ¼MVNğœ‡ğ‘
ğğ‘ƒ
,ğš²
(5)
Ë†Î”ğ‘
ğ‘–Ë†âˆ†ğ‘ƒ
ğ‘–
|Î”ğ‘
ğ‘–
âˆ†ğ‘ƒ
ğ‘–
âˆ¼MVNÎ”ğ‘
ğ‘–
âˆ†ğ‘ƒ
ğ‘–
,ğšµğ‘–
,âˆ€ğ‘–âˆˆ[ğ¾]. (6)
which due to the joint Gaussianity we can marginalize as:
Ë†Î”ğ‘
ğ‘–Ë†âˆ†ğ‘ƒ
ğ‘–
âˆ¼MVNğœ‡ğ‘
ğğ‘ƒ
,ğšºğ‘–:=ğš²+ğšµğ‘–
,âˆ€ğ‘–âˆˆ[ğ¾]. (7)
We use the notation ğš²to capture the latent covariance of the
joint distributionD(Â·) which we parameterize by a multivari-
ate normal (i.e. MVN). So in our case, Cov(Î”ğ‘,âˆ†ğ‘ƒ)=ğš²ğ‘ğ‘ƒand
Cov(âˆ†ğ‘ƒ,âˆ†ğ‘ƒ)=ğš²ğ‘ƒğ‘ƒfor
Î”ğ‘âˆ†ğ‘ƒâŠ¤
âˆ¼D(Â·) . Moreover, for the
purposes of inference we simply use the plug-in estimate Ë†ğšµâ‰ˆğšµ
which is routinely done in similar hierarchical modeling approaches
[14]. While using multivariate normality in Equation (5) is an as-
sumption (albeit we believe reasonable in our case), it is not essential
to the content of our results. Our proxy quality definition relies
only on inferring low-order moments of D(Â·) for which this para-
metric structure is convenient. The second approximation that the
noisy TEs are multivariate normal around their true latent values
(Equation (6)) is well-justified by the central limit theorem in our
case, since the experiments we consider all have at least ğ‘‚(105)
treatment units. Since inference in this model is not closed-form, we
implement the aforementioned generative model in the open-source
probabilistic programming language NumPyro [ 20] to extract the
5806Choosing a Proxy Metric from Past Experiments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) Latent Population TEs of proxy metric / long-term
outcome
(b) Estimated TEs of proxy metric / long-term outcome
 (c) Estimated TEs with raw / estimated covariance from
hierarchical model
Figure 2: The panel visualizes the denoising effect of fitting a hierarchical model to raw TEs to uncover their latent variation on
synthetic data. We generate 1500 synthetic datapoints sampled from the model in Equation (5) with one proxy metric. Each dat-
apoint represents a synthetic TE measurement from a single A/B test. We use parameters withğœ‡ğ‘
ğœ‡ğ‘ƒ
=0.0
0.0
,ğš²=.01Â·1 0.2
0.2 1
to generate data in Figure 2(a). We add Gaussian noise with covariance ğšµ=.02Â·1 0.7
0.7 1
to them in Figure 2(b). Finally, we fit
the generative model to the observed data in Equation (7) using the within-experiment covariances ğšµin Figure 2(c). Figure 2(c)
illustrates how the hierarchical model denoises the raw observed TEs to disentangle the latent variation in the population
from the experimental noise in each synthetic A/B test.
latent parameters6. Additional details on the inference procedure
are deferred to Appendix C.
Figure 2 provides an example where the inference procedure is
used to extract the latent population variation in a synthetically
generated dataset. Although this example is synthetic (and exag-
gerated), empirically in our corpus we observe many base proxy
metrics with correlations to the long-term outcome of âˆ¼0.6in
their experimental noise matrix Ë†ğšµ. Thus, the example shows a case
where the raw correlation may provide an over-optimistic estimate
of the underlying alignment between a proxy metric and long-term
outcome. The denoising model we fit helps mitigate the impact of
correlated within-experiment noise in our setting. We schemati-
cally detail the end-to-end algorithm which composes the denoising
model fit and portfolio optimization to construct a proxy for a new
A/B test in Algorithm 1.
Lastly, with the generative model in Equations (5) and (6) in place
we can provide an alternative interpretation of our definition of
composite proxy quality. Under the condition thatğœ‡ğ‘
ğğ‘ƒ
=07, the
probability of a signed alignment (or equivalently the complement
of a surrogate paradox Elliott et al. [12]) can be simplified too,
P(Î”ğ‘>0,wâŠ¤Ë†âˆ†ğ‘ƒ>0)=P(Î”ğ‘<0,wâŠ¤Ë†âˆ†ğ‘ƒ<0)=1
4+sinâˆ’1(ğœŒ)
2ğœ‹
(8)
6As an alternative to method presented, we could eschew the parametric Gaussian
structure by using an estimator which uses sample-splitting within each RCT to esti-
mateğš². We prefer to use our current approach in order to implicitly reweight by the
heteroscedasticity in our observations (i.e. Îğ‘–) and avoid sample-splitting/cross-fitting.
Additional details are provided in Appendix C.
7This condition may not be true in all applications but is approximately satisfied in
our dataset with all metrics having the ratio of their global mean to global standard
deviation being bounded by 0.1but often being even less.Algorithm 1 Composite Proxy Algorithm
Input:{(Ë†Î”ğ‘
ğ‘–,Ë†ğš«ğ‘ƒ
ğ‘–,Ë†ğšµğ‘–)}ğ¾
ğ‘–=1(TE and Noise Estimates from Historical
Tests), Ë†ğšµğ‘ƒğ‘ƒ
ğ¾+1(Noise Estimate for New Test).
1:ğ,ğš²â†HM({(Ë†Î”ğ‘
ğ‘–,Ë†ğš«ğ‘ƒ
ğ‘–,Ë†ğšµğ‘–)}ğ¾
ğ‘–=1). Fit model (e.g. using Numpyro)
defined in Equation (7).
2:wâ†Sharpe Ratio( ğš²,Ë†ğšµğ‘ƒğ‘ƒ
ğ¾+1). Solve convex program in Equa-
tion (10) (e.g. using CVXPY).
Output: w,wâŠ¤Ë†ğš«ğ‘ƒ
ğ¾+1(Proxy Weights and Composite Proxy for
New Experiment).
forğœŒ=1âˆš
Î›ğ‘ ğ‘wâŠ¤ğš²ğ‘ğ‘ƒq
wâŠ¤(ğš²ğ‘ƒğ‘ƒ+ğšµğ‘ƒğ‘ƒ)w. This computation relies on the
joint Gaussianity of the model and centering condition to explicitly
compute the alignment probability. Given ğœŒis our definition of
the vector-valued proxy quality and the inverse-sine function is
monotone increasing, optimizing the proxy quality over wcan be
interpreted as minimizing the probability of a signed decision error
in this setting.
3 Results
In this section, we turn to evaluating the performance of our com-
posite proxy procedure against several baselines. As raw proxy
metrics to consider in our evaluations we use a small set of 3 hand-
selected proxy metrics which capture different properties domain
experts believe are relevant to long-term user satisfaction in our
setting. We first highlight a unique feature of our proxy procedure
â€“ its adaptivity to the sample size of the experiment for which it
will be applied. We then perform a comparison of our new proxy
procedure against the raw proxy metrics and a baseline procedure
[24] appearing in the literature.
5807KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Nilesh Tripuraneni, Lee Richardson, Alexander Dâ€™Amour, Jacopo Soriano, & Steve Yadlowsky
3.1 Proxy Quality and Sample Size Dependence
One unique feature of our procedure is its adaptivity to the noise
level (or effectively sample size of the experiment); recall in Equa-
tion (4) the optimal weights will depend on the latent parameters
which are inferred from the pool of homogeneous RCTs on which
they are fit, but also the experiment noise estimate Ë†ğšµwhich depends
on the new A/B test it is to be used for. While in practice one could
recompute a proxy metric depending on the aposteriori results of
each A/B test (so Ë†ğšµis known), it is also desirable to be able to fit a
proxy for each A/B test apriori, without knowledge of its results.
To do so, we found that in our application, ğšµğ‘ƒğ‘ƒ
ğ‘–could be estimated
with reasonable accuracy purely on the basis of historical data of
other A/B tests in our population of experiments by postulating a
scaling of the form ğšµğ‘ƒğ‘ƒ
ğ‘–=ğšµğ‘ƒğ‘ƒ
ref/ğ‘›ğ‘–. Here the reference matrix ğšµğ‘ƒğ‘ƒ
refcan be thought of as the within experiment variance of an A/B
test in the population with one sample. The ansatz ğšµğ‘ƒğ‘ƒ
ğ‘–=ğšµğ‘ƒğ‘ƒ
ref/ğ‘›ğ‘–,
combines two observations. The first is that the variance of a TE
estimate decays asâˆ¼1
ğ‘›ğ‘–in the number of treatment units ğ‘›ğ‘–, which
is immediate from the independence of treatment units. However,
the second is that the constant prefactor in the variance ğšµğ‘ƒğ‘ƒ
refis
approximately the same across different A/B tests in our corpus.
The reference matrix ğšµğ‘ƒğ‘ƒ
refcan then be estimated as a weighted
average of Ë†ğšµğ‘–from the corpus. Additional details and verification
of these hypotheses are provided in Appendix A. The upshot of this
approach is that the computation of the optimal weights in Equa-
tion (4) for a new A/B test can then be done using only the sample
size (ğ‘›ğ‘–) of this new experiment (i.e. before the new experiment is
â€œrun").
To understand the dependence of our new composite proxies
weighting on the experiment sample size, we fit the latent parame-
tersğš²(from the hierarchical model in Equations (5) and (6)) and
ğšµğ‘ƒğ‘ƒ
refon the entire corpus for the results in Figure 3. We then use
the scaling ğšµ=ğšµğ‘ƒğ‘ƒ
ref/ğ‘›to estimate the optimal weights of our new
composite proxy from Equation (4) for different sample sizes ğ‘›for
a hypothetic new A/B test. Figure 3 shows how as the sample size
increases the new composite proxy smoothly increases its weight-
ing on raw metrics which are noisier but more strongly correlated
with the long term outcome. Moreover, while the Auxiliary Metric
3 is an intuitively reasonable metric, its value as determined by the
measure of proxy quality is dominated by a mixture of the other
two components.
3.2 Held-out Evaluation of Proxy Procedures
The primary difficulty of this evaluation is that in TE estimation
there is the lack of ground-truth â€œlabels" of the treatment effect
(i.e. in our framework the population latent TEs such as Î”ğ‘are
never observed). However, in our setting we do have access to a
large corpus of 307 A/B tests as noted earlier. Hence, we use held-
out/cross-validated evaluations of certain criterion which depend
on the noisy metrics aggregated over an evaluation set, to gauge
the performance of proxy metrics fit on a training set.
We consider several relevant criteria for performance which
have been used in the literature. Two important measures which
appear in [ 24] are the proxy score and sensitivity. To define thecriteria, recall that a TE metric is often used to make a down-
stream decision by thresholding its t-statistic, tstat =Ë†Î”âˆš
Var(Ë†Î”)
aststat >2â†’+ (positive) ,âˆ’2<tstat <2â†’0(neutral) , and
tstat <âˆ’2â†’âˆ’ (negative) . Given a corpus of A/B tests we can
then compute the decisions induced by a short-term proxy metric,
the decisions induced by the long-term outcome and check the
number of A/B tests for which they align. After normalization, the
number of detections (both metrics decisions are positive or nega-
tive) minus the number of mistakes (one metric decision is positive
while the other is negative) defines the proxy score. Similarly, for a
short-term proxy metric we can compute its sensitivity â€“ which is
the number of times it triggers a statistically significant decision by
being positive or negative. Loosely speaking, these two criterion
function like the notions of precision and recall in information re-
trieval. Ideally, a short-term metric would maximize both quantities
by being sensitive and triggering often (so as to not miss any A/B
tests where the TE for the long-term outcome is significant) but not
over triggering and leading to many false positives (or negatives).
Additional details on these metrics are provided in the Appendix D8.
As another measure of performance, we also compute and report
our definition of our composite proxy quality for a given composite
proxyâ€™s learned weights.
We use the same set of raw proxy metrics as before in our evalu-
ation. As a procedure to compare our methodology against, we use
the baseline of optimizing the convex combination of these 3 met-
rics to optimize the aforementioned proxy score which is detailed
in [24]. In each case we use stratified 4-fold cross-validation (CV) to
compute the weights for each procedure on a training subset of the
corpus and evaluate the metrics on the held-out set by computing
the aforementioned evaluation scores. The baseline proxy method
and base proxy metrics each learn a fixed set of weights9depend-
ing on the training fold, which is applied to each A/B test in the
evaluation fold. For our procedure we use the ansatz ğšµğ‘ƒğ‘ƒ
ğ‘–=ğšµğ‘ƒğ‘ƒ
ref/ğ‘›ğ‘–
mentioned in the previous setting to calculate ğšµğ‘ƒğ‘ƒ
refas a simple
weighted average from data only in the training fold of our CV split.
The optimal proxy metric for each A/B test in the test fold can then
be refit using only the sample size ğ‘›ğ‘–of that A/B test. This strategy
has the additional benefit of enforcing strict separation of the data
in train/test set folds in our CV split.
Results for our cross-validated evaluation are displayed in Ta-
ble 1 across our corpus of 307 historical A/B tests which come
from a real industrial recommendation engine. Note that both our
new composite proxy and the baseline composite proxy improve
significantly in the proxy score and proxy quality over the raw
metrics without sacrificing unduly on sensitivity relative to Aux-
iliary Metric 2. Moreover, the new composite proxy achieves not
only the highest proxy quality but also proxy score despite not
explicitly optimizing for proxy score on the training set. We believe
this may be a feature of our new composite proxy whose weights
are adaptive to the size of each experiment in the A/B test (which
vary in our corpus from approximately ğ‘‚(106)toğ‘‚(108)in size).
8It is worth noting these measures are still imperfect in the sense that comparisons are
made against the decisions induced by the noisy estimate of the long-term outcome
TE not the latent population long-term outcome TE.
9the base proxy metrics simply place all their weight on themselves.
5808Choosing a Proxy Metric from Past Experiments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 3: The optimal weighting dependence on sample size for our new composite proxy, represents a bias-variance trade-off.
For large sample sizes the weighting favors potentially noisier metrics that are more aligned with the long term outcome.
However, for smaller sample sizes the optimal weighting backs off to metrics which are less noisy but also less aligned to the
long term outcome.
Metric Sensitivity Proxy Score Proxy Quality
New Composite Proxy 0.181 0.666 0.302
Baseline Composite Proxy 0.182 0.611 0.279
Auxiliary Metric 1 0.062 0.611 0.174
Auxiliary Metric 2 0.368 0.222 0.258
Auxiliary Metric 3 0.166 0.104 0.030
Table 1: Comparison of our new composite proxy against a baseline method, and their constituent base proxies alongst several
criterion which are computed on a held-out evaluation. Our new composite proxy performs favorably across all measures â€“
notably achieving the highest proxy score and proxy quality amongst all considered. All evaluation criteria are bounded in
[0,1]and for each higher is better.
4 Conclusion
We have presented a framework for both defining and constructing
optimal composite proxy metrics, which are used to approximate
the decisions induced by a difficult-to-measure long-term metric.
One key insight from our framework is that the optimal proxy for a
given experiment should depend on the noise level (or equivalently
the size) of that experiment. In our work, the first component of
our procedure reduces the composite proxy selection problem to
a portfolio optimization relating the unobserved long-term/north
star TE and observed TE. This does not explicitly use the population
distribution assumption in Equation (1), although it relies on theunobservable treatment effects, and we believe this is a valuable
and natural framing of the proxy selection problem. Equation (1)
is implicitly needed to identify the latent covariance parameters
between a new future RCT and past RCTs, and accordingly for
estimation of the unobserved latents from the corpus of RCTs. Due
to the lack of ground truth (true treatment effects are unobserved
in RCTs) Equation (1) is important for our meta-analysis, although
we acknowledge that it is not suitable for all applications. One of
the limitations of our work is that highly non-stationary settings
(where Equation (1) is not a good approximation) may not be well
addressed by our second-stage denoising procedure. Accordingly,
5809KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Nilesh Tripuraneni, Lee Richardson, Alexander Dâ€™Amour, Jacopo Soriano, & Steve Yadlowsky
this assumption should be probed by model-fit diagnostics and
domain-specific intuition.
An interesting direction for future work is to further relax this as-
sumption through different structural or causal assumptions which
are application-dependent in the second part of the procedure
(while still maintaining the first portfolio optimization component).
Here, more flexible modeling of the joint latent effect distribution
â€“ to accommodate more structured latent effects could be useful.
Extending our framework to handle the construction of nonlinear
composite proxy metrics in situations where higher-order interac-
tions between the north star and short-term proxies are important,
is also an interesting avenue for further research. As an additional
point of exploration, understanding how our approach might gen-
eralize to the problem of heterogeneous treatment effect estimation
â€“ to provide contextual decision-making power â€“ would also be
valuable.
5 Acknowledgements
We thank Jasper Snoek and our anonymous reviewers for their
feedback on this paper.
References
[1]Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. The
Surrogate Index: Combining Short-Term Proxies to Estimate Long-Term Treatment
Effects More Rapidly and Precisely. Working Paper 26463. National Bureau of
Economic Research. https://doi.org/10.3386/w26463
[2]AurÃ©lien Bibaut, Winston Chou, Simon Ejdemyr, and Nathan Kallus. 2024.
Learning the Covariance of Treatment Effects Across Many Weak Experiments.
arXiv:2402.17637 [stat.ME]
[3]A. Charnes and W. W. Cooper. 1962. Programming with lin-
ear fractional functionals. Naval Research Logistics Quarterly
9, 3-4 (1962), 181â€“186. https://doi.org/10.1002/nav.3800090303
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800090303
[4]Albert C. Chen and Xin Fu. 2017. Data + Intuition: A Hybrid Approach to De-
veloping Product North Star Metrics. In Proceedings of the 26th International
Conference on World Wide Web Companion (Perth, Australia) (WWW â€™17 Compan-
ion). International World Wide Web Conferences Steering Committee, Republic
and Canton of Geneva, CHE, 617â€“625. https://doi.org/10.1145/3041021.3054199
[5]Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Chris-
tian Hansen, Whitney Newey, and James Robins. 2018. Double/debiased
machine learning for treatment and structural parameters. The Econo-
metrics Journal 21, 1 (01 2018), C1â€“C68. https://doi.org/10.1111/ectj.12097
arXiv:https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf
[6]Gerard Cornuejols and Reha TÃ¼tÃ¼ncÃ¼. 2006. Optimization methods in finance.
Vol. 5. Cambridge University Press.
[7]Alex Deng. [n. d.]. Metric Sensitivity Decomposition. Causal Inference and Its
Applications in Online Industry. https://alexdeng.github.io/causal/sensitivity.
html#metric-sensitivity-decomposition. [Online; accessed 21-December-2022].
[8]Alex Deng, Michelle Du, Anna Matlin, and Qing Zhang. 2023. Variance Re-
duction Using In-Experiment Data: Efficient and Targeted Online Measurement
for Sparse and Delayed Outcomes. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (<conf-loc>, <city>Long
Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>) (KDD
â€™23). Association for Computing Machinery, New York, NY, USA, 3937â€“3946.
https://doi.org/10.1145/3580305.3599928
[9]Alex Deng and Xiaolin Shi. 2016. Data-driven metric development for online
controlled experiments: Seven lessons learned. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. 77â€“
86.
[10] Pavel Dmitriev and Xian Wu. 2016. Measuring metrics. In Proceedings of the
25th ACM international on conference on information and knowledge management .
429â€“437.
[11] Michael R. Elliott. 2023. Surrogate Endpoints in Clinical Trials. Annual Review of
Statistics and Its Application 10, 1 (2023), 75â€“96. https://doi.org/10.1146/annurev-
statistics-032921-035359 arXiv:https://doi.org/10.1146/annurev-statistics-032921-
035359
[12] Michael R Elliott, Anna SC Conlon, Yun Li, Nico Kaciroti, and Jeremy MG Taylor.
2015. Surrogacy marker paradox measures in meta-analytic settings. Biostatistics
16, 2 (2015), 400â€“412.[13] Andrew Gelman. 2006. Prior distributions for variance parameters in hierarchical
models (comment on article by Browne and Draper). Bayesian Analysis 1, 3 (2006),
515 â€“ 534. https://doi.org/10.1214/06-BA117A
[14] Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. 1995. Bayesian
data analysis. Chapman and Hall/CRC.
[15] Miguel A HernÃ¡n and James M Robins. 2010. Causal inference.
[16] Henning Hohnhold, Deirdre Oâ€™Brien, and Diane Tang. 2015. Focus on the Long-
Term: Itâ€™s better for Users and Business. In Proceedings 21st Conference on Knowl-
edge Discovery and Data Mining. Sydney, Australia. http://dl.acm.org/citation.
cfm?doid=2783258.2788583
[17] Nathan Kallus and Xiaojie Mao. 2022. On the role of surrogates in the efficient esti-
mation of treatment effects with limited outcome data. arXiv:2003.12408 [stat.ML]
[18] Daniel Lewandowski, Dorota Kurowicka, and Harry Joe. 2009. Generating ran-
dom correlation matrices based on vines and extended onion method. Journal of
Multivariate Analysis 100, 9 (2009), 1989â€“2001. https://doi.org/10.1016/j.jmva.
2009.04.008
[19] Layla Parast, Tianxi Cai, and Lu Tian. 2017. Evaluating surrogate marker infor-
mation using censored data. Statistics in medicine 36, 11 (2017), 1767â€“1782.
[20] Du Phan, Neeraj Pradhan, and Martin Jankowiak. 2019. Composable Effects for
Flexible and Accelerated Probabilistic Programming in NumPyro. arXiv preprint
arXiv:1912.11554 (2019).
[21] Nicholas G. Polson and James G. Scott. 2012. On the Half-Cauchy Prior for
a Global Scale Parameter. Bayesian Analysis 7, 4 (2012), 887 â€“ 902. https:
//doi.org/10.1214/12-BA730
[22] Ross L. Prentice. 1989. Surrogate endpoints in clinical trials: definition and
operational criteria. Statistics in medicine 8, 4 (1989), 431â€“440.
[23] Lenny Rachitsky. [n. d.]. Choosing Your North Star Metric. https://future.com/
north-star-metrics/.
[24] Lee Richardson, Alessandro Zito, Dylan Greaves, and Jacopo Soriano. 2023. Pareto
optimal proxy metrics. arXiv preprint arXiv:2307.01000 (2023).
[25] Siegfried Schaible. 1974. Parameter-free convex equivalent and dual programs of
fractional programming problems. Zeitschrift fÃ¼r Operations Research 18 (1974),
187â€“196.
[26] William F. Sharpe. 1966. Mutual Fund Performance. The Journal of Business 39, 1
(1966), 119â€“138. http://www.jstor.org/stable/2351741
[27] William F Sharpe. 1998. The sharpe ratio. Streetwiseâ€“the Best of the Journal of
Portfolio Management 3 (1998), 169â€“85.
[28] Tyler J. VanderWeele. 2013. Surrogate Measures and Consistent Surrogates.
Biometrics 69, 3 (2013), 561â€“569. http://www.jstor.org/stable/24538119
[29] Xuan Wang, Layla Parast, Larry Han, Lu Tian, and Tianxi Cai. 2023. Robust
approach to combining multiple markers to improve surrogacy. Biometrics 79, 2
(2023), 788â€“798.
[30] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson,
Lisa Chung, Ed H Chi, and Minmin Chen. 2022. Surrogate for long-term user
experience in recommender systems. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 4100â€“4109.
[31] Vickie Zhang, Michael Zhao, Anh Le, and Nathan Kallus. 2023. Evaluating the
Surrogate Index as a Decision-Making Tool Using 200 A/B Tests at Netflix. arXiv
preprint arXiv:2311.11922 (2023).
A Within-Experiment Covariance Scaling
As we note before, one interesting feature of our composite proxy
quality procedure is its dependence on the noise level of ğšµof the
randomized experiment it will be applied too. Recall in Equation (4)
the optimal weights will depend not only on the latent parameters
which are instrinsic properties of D(Â·) , but the experiment noise
estimate Ë†ğšµwhich depends on the particular A/B test too which it
is applied.
While in practice, one could recompute a composite proxy metric
after an A/B test is run (so Ë†ğšµis known), in many applications it
is also desirable to be able to fit weights for a composite proxy
metric before each A/B test is run. In order to do this we found
that we could build a simple predictive model for the experimental
noise level ğšµğ‘ƒğ‘ƒ
ğ‘–in a given experiment on the basis of historical
data of other A/B tests in our population of experiments. We did
so by making an ansatz of the form ğšµğ‘ƒğ‘ƒ
ğ‘–=ğšµğ‘ƒğ‘ƒ
ref/ğ‘›ğ‘–, where ğšµğ‘ƒğ‘ƒ
refcan be thought of as the within experiment variance of an A/B
test in the population with one sample. This ansatz follows from
two facts. The first is that the variance of a TE estimate decays as
âˆ¼1
ğ‘›ğ‘–in the number of treatment units ğ‘›ğ‘–, which is immediate from
5810Choosing a Proxy Metric from Past Experiments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
the independence of treatment units. However, the second â€“ that
the constant prefactor in the variance ğšµğ‘ƒğ‘ƒ
refis approximately the
same across different A/B tests â€“ is an empirical observation due
to underlying homogeneity in the population of A/B tests. This
approximate homogeneity is evidenced in Figure 4.
(a) Variance of A/B tests for Auxiliary
Metric 1
(b) Variance of A/B tests for Auxiliary
Metric 2
Figure 4: Both displays show the within-experiment mar-
ginal sample variance (blue dots) for two different metrics
computed across 307 different A/B tests and their corre-
sponding power-law fit (red line). Despite the underlying
A/B tests being different, we found that the variance were
reasonably well modeled by a single inverse-power law with
the same constant prefactor over the entire population.
On the basis of this ansatz for each A/B test,
ğšµğ‘–=ğšµref
ğ‘›ğ‘–,{1,...,ğ¾}
we can construct an estimator for the reference constant matrix
ğšµrefas,
Ë†ğšµref=ğ¾Ã•
ğ‘–=1ğ›¾ğ‘–ğ‘›ğ‘–Ë†ğšµğ‘– (9)
for some convex combination of weights ğ›¾ğ‘–. While we can use
an equal weighting scheme where ğ›¾ğ‘–=1
ğ¾, since the sample vari-
ance estimates Ë†ğšµğ‘–themselves are noisy, we instead use a precision
weighted combination of them to reduce variance by taking ğ›¾ğ‘–âˆğ‘›ğ‘–.
With this estimate Ë†ğšµrefin hand, for a new ğ¾+1st A/B test
with sample size ğ‘›ğ¾+1we can approximates its within-experiment
covariance as,
Ë†ğšµğ¾+1â‰ˆË†ğšµref
ğ‘›ğ¾+1
under the implicit homogeneity assumption we make.
B Sharpe Ratio and Portfolio Optimization
Under the mild condition (which is always satisified for us that at
least one element of ğš²ğ‘ğ‘ƒis positive), we can transform the Sharpe
ratio optimization objective to an equivalent convex quadratic pro-
gram:
min
xâˆˆRğ‘‘xâŠ¤ğšºx:xâ‰¥0,râŠ¤x=1. (10)
where ğšº=ğš²ğ‘ƒğ‘ƒ+Ë†ğšµğ‘ƒğ‘ƒandr=ğš²ğ‘ğ‘ƒ. The solution to the original
problem in Equation (4) can be recovered by normalizing as w=
x
âˆ¥xâˆ¥1. The details of this standard transformation can be found in[6, Section 8.2], although the original reduction is a generalization
of the Charnes-Cooper transformation [ 3] which dates back to at
least [25].
C Inference in Hierarchical Model
In order to extract estimates of the latent parameter we perform full
Bayesian inference over the generative model in Equation (7) using
Numpyro Phan et al . [20] which uses the NUTS sampler to perform
MCMC on the posterior. We found Bayesian inference to be more
stable then estimating the MLE of the model. We augmented the
generative model in Equation (7) with the weak priors:
ğâˆ¼N( 0,1000Â·meanscaleÂ·Iğ‘‘+1)
sâˆ¼1.5Â·Half-Cauchy(devscale)
Câˆ¼LKJ(concentration =1)
ğš²=âˆšsâŠ¤â—¦Câ—¦âˆšs
where we use the operator â—¦to denote coordinatewise broadcasted
multiplication. Here the vector-valued parameters meanscale , and
devscale are set to match the overall scales of the raw mean and
raw covariance of the corpous A/B tests. We found the overall
inferences to be robust to the choice of scales in the Half-Cauchy
prior on the pooled variance parameter and normal prior mean,
which are both weakly-informative. [ 13] and [ 21] both argue for
the use of the Half-Cauchy prior for the top-level scale parameter
in hierarchical linear models as opposed to the more traditional
use of the Inverse-Wishart prior on both empirical and theoretical
grounds. The choice of the LKJ prior with concentration parameter
set to 1 is essentially a uniform prior over the space of correlation
matrices [14, 18].
Inference in this model was performed using the default con-
figuration of the NUTS sampler in Numpyro [ 20]. We also found
it useful to initialize the parameters (ğœ‡ğ‘,ğğ‘ƒ)andğš²to the scales
of the raw mean and raw covariance of the corpus A/B tests. We
diagnosed convergence and mixing of the NUTS sampler using
standard diagnostics such as the r-hat statistic [ 14]. In all our exper-
iments we found the sampler mixed efficiently and we achieved a
perfect r-hat statistic for all parameters of 1.0. For each MCMC run
we generated 10000 burn-in samples and 50000 MCMC samples for
4 parallel chains. We used the posterior means of the samples to
extract estimates of ğš²for use in our proxy quality score.
As noted in the main text an alternative to using the genera-
tive model presented here which eschews the Gaussian parametric
structure (but of course relies on the Equation (1)) is to use a sample-
splitting estimator within each RCT. Our given procedure is agnostic
to the details of TE estimation so long as each estimate is unbiased.
However for the present discussion, assume as before, we have a
corpus of RCTs with true TEs satisfying Equation (1),
âˆ†ğ‘–=Î”ğ‘
ğ‘–
âˆ†ğ‘ƒ
ğ‘–
âˆ¼D(Â·) i.i.d.,ğ‘–âˆˆ{1,...,ğ‘›},
with mean and covariance vectors ğandğš². Further assume in
each RCT we have two unbiased estimates for the TE satisfying
Ë†âˆ†ğ‘–,1=âˆ†ğ‘–+ğšµ1/2Â·ğğ‘–,1and Ë†âˆ†ğ‘–,2=âˆ†ğ‘–+ğšµ1/2Â·ğğ‘–,2with mutually
independent mean-zero observation noise ğğ‘–,1,ğğ‘–,2. Such estimators
can easily be obtained by randomly splitting the units in treat-
ment and control groups in the RCT into two disjoint subsets
5811KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Nilesh Tripuraneni, Lee Richardson, Alexander Dâ€™Amour, Jacopo Soriano, & Steve Yadlowsky
and computing an unbiased TE estimate (such as the difference-
of-means estimator) on each subset. Since they are from same
RCT they will provide unbiased estimates of the same unobserved
TEğš«ğ‘–. So it follows that E[Ë†âˆ†ğ‘–,1Ë†âˆ†âŠ¤
ğ‘–,2]=ğš²+ğğâŠ¤for eachğ‘–âˆˆ
{1,...,ğ‘›}, due to Equation (1) and the independence of the obser-
vation noise. Note the expectation is taken over the observation
noise and latent randomness. Finally, averaging over the corpus
1
ğ‘›âˆ’1Ãğ‘›
ğ‘–=1(Ë†âˆ†ğ‘–,1âˆ’1
ğ‘›Ãğ‘›
ğ‘–=1Ë†âˆ†ğ‘–,1)(Ë†âˆ†âŠ¤
ğ‘–,2âˆ’1
ğ‘›Ãğ‘›
ğ‘–=1Ë†âˆ†âŠ¤
ğ‘–,2)then provides an
unbiased estimate of ğš². The reduction in efficiency due to sample-
splitting for this estimate within each RCT can also be partially
mitigated through cross-fitting techniques [ 5] or a jackknife ap-
proach as explored in Bibaut et al . [2, Section 4.2 ]. In our work we
find the hierarchical modeling approach to be natural as it incorpo-
rates the observed heteroscedasticity in observation noise across
RCTs which vary significantly in size in our corpus.
D Proxy Score and Sensitivity
Here we explain several performance criterion we use for proxy
metrics which are further detailed in the literature [ 24]. In order
to define both quantities, recall that a TE metric is often used to
make a downstream decision by thresholding its t-statistic, tstat=
Ë†Î”âˆš
Var(Ë†Î”)aststat>2â†’+ (positive) ,âˆ’2<tstat<2â†’0(neutral) ,
andtstat <âˆ’2â†’âˆ’ (negative) . The formal definitions of proxy
score and sensitivity are most easily defined in the context of a
contingency table visualized in Figure 5 which takes these decisions
as inputs. The contingency table tabulates the decisions induced
by a particular observed short term proxy metric and the observed
long-term north star metric jointly over 554 A/B tests.
NegativeNeutralPositiveNegative20905Neutral103008Positive28534North Star Metric Long-Term EffectProxy Metric Short-Term Effect
Figure 5: A synthetic contingency table which captures the
alignment of the decisions induced by the t-statistics of the
TEs of the north star metric and a proxy metric.The green cells in Figure 5 represent cases where the proxy and
long-term north star are both statistically significant and move in
the same direction (i.e. Detections ). The red cells in Figure 5 again
represent cases where the proxy and long-term north star are both
statistically significant, but where proxy and long-term north star
are misaligned (i.e. Mistakes ). The remaining cells correspond to
cases where at least one of the metrics is not statistically significant.
The relative importance of these cells is more ambiguous.
In this setting, the sensitivity can be defined as:
Metric Sensitivity =Num. of expts. proxy is significant
Num. of total expts..
Here the numerator can be obtained by summing over the first and
last rows of the table. The proxy score can be defined as,
Proxy Score =Detectionsâˆ’Mistakes
Num. expts. long-term north star is significant.
The denominator here can be obtained by summing over the
first column and last column. The sensitivity metric captures the
ability of a metric to detect a statistically significant effect â€“ which
inherently takes into account its inherent moveability and suscepti-
bility to experimental noise. Given that north star metrics are often
noisy and slow to react in the short-term the goal of a proxy is to
be sensitive.
The proxy score rewards metrics that are both sensitive and
directionally aligned with the north star. Sensitive metrics need
only populate the first and third rows of the contingency table.
However, metrics in the first and third rows can only increase the
proxy score if they are in the same direction as the long-term north
star. A similar score, called Label Agreement, has been used by
[10]. It is worth noting, these measures are still imperfect in the
sense that comparisons are made against the decisions induced
by the noisy estimate of the long-term outcome TE not the latent
population long-term outcome TE. This is further complicated by
the fact that we empirically find that experimental noise in the
A/B tests is correlated between short-term proxies and the long-
term outcome (i.e. the phenomena detailed in Figure 2). In fact,
this phenomena provides partial motivation for our definition of
denoised proxy quality.
5812