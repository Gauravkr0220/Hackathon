Enhancing On-Device LLM Inference with Historical Cloud-Based
LLM Interactions
Yucheng Ding
Shanghai Jiao Tong University
Shanghai, China
yc.ding@sjtu.edu.cnChaoyue Niuâˆ—
Shanghai Jiao Tong University
Shanghai, China
rvince@sjtu.edu.cnFan Wu
Shanghai Jiao Tong University
Shanghai, China
wu-fan@sjtu.edu.cn
Shaojie Tang
University of Texas at Dallas
Richardson, Texas, United States
shaojie.tang@utdallas.eduChengfei Lyu
Alibaba Group
Hangzhou, Zhejiang, China
chengfei.lcf@alibaba-inc.comGuihai Chen
Shanghai Jiao Tong University
Shanghai, China
gchen@cs.sjtu.edu.cn
Abstract
Many billion-scale large language models (LLMs) have been re-
leased for resource-constraint mobile devices to provide local LLM
inference service when cloud-based powerful LLMs are not avail-
able. However, the capabilities of current on-device LLMs still lag
behind those of cloud-based LLMs, and how to effectively and
efficiently enhance on-device LLM inference becomes a practical
requirement. We thus propose to collect the userâ€™s historical interac-
tions with the cloud-based LLM and build an external datastore on
the mobile device for enhancement using nearest neighbors search.
Nevertheless, the full datastore improves the quality of token gen-
eration at the unacceptable expense of much slower generation
speed. To balance performance and efficiency, we propose to select
an optimal subset of the full datastore within the given size limit,
the optimization objective of which is proven to be submodular. We
further design an offline algorithm, which selects the subset after
the construction of the full datastore, as well as an online algorithm,
which performs selection over the stream and can be flexibly sched-
uled. We theoretically analyze the performance guarantee and the
time complexity of the offline and the online designs to demon-
strate effectiveness and scalability. We finally take three ChatGPT
related dialogue datasets and four different on-device LLMs for
evaluation. Evaluation results show that the proposed designs sig-
nificantly enhance LLM performance in terms of perplexity while
maintaining fast token generation speed. Practical overhead test-
ing on the smartphone reveal the efficiency of on-device datastore
subset selection from memory usage and computation overhead.
CCS Concepts
â€¢Information systems â†’Data mining; â€¢Computing method-
ologiesâ†’Natural language processing; â€¢Human-centered
computingâ†’Ubiquitous and mobile computing.
âˆ—Chaoyue Niu is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671679Keywords
On-Device LLM Enhancement; Device-Cloud Hybrid Service; Data-
store Subset Selection
ACM Reference Format:
Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tang, Chengfei Lyu, and Gui-
hai Chen. 2024. Enhancing On-Device LLM Inference with Historical Cloud-
Based LLM Interactions. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671679
1 Introduction
Large language models (LLMs) have revealed remarkable text gen-
eration capability and have revolutionized the conversational user
interfaces for intelligent services. On the side of cloud, the parame-
ter scale of the LLMs normally reaches hundreds of billions, and
massive users can interact with the cloud-based LLMs through
API requests (e.g., ChatGPT, Bard, Claude, Qwen). On the side of
mobile device, tech giants, smartphone manufacturers, and chip-
makers have launched billion-scale LLMs (e.g., Qwen-1.8B, Phi-2.7B,
ChatGLM3-6B, Baichuan2-7B, Gemini-Nano) to continuously pro-
vide local service when the cloud API is not available (e.g., when a
userâ€™s mobile device has poor network connection, the rate limit of
cloud API has been reached, or the cloud platform is overloaded
with excessive requests or experiences an outage). Such a device-
cloud hybrid service framework guarantees the completeness of
LLM service. However, on-device LLMs exhibit markedly lower
performance than cloud-based LLMs, and how to effectively and
efficiently enhance local LLM inference on each resource-constraint
mobile device becomes a new requirement in practice.
For on-device LLM enhancement, a basic way is to acquire high-
quality data. We innovatively propose to exploit a userâ€™s historical
interactions with the cloud API, because the userâ€™s queries to the
cloud-based LLM tend to be similar to his/her queries to the on-
device LLM, and meanwhile, the responses from the cloud-based
powerful LLM can be regarded as reference answers. Another prob-
lem is how to enhance on-device LLM with these historical dia-
logues. Existing LLM enhancement work focused on resource-rich
cloud rather than resource-constraint mobile device. One line of
work resorted to tuning methods, including full model finetuning
and parameter-efficient tuning (e.g., prefix tuning [ 35], adapter [ 29],
LoRA [ 30]). However, tuning a billion-scale LLM for each user can-
not be centralized on the cloud due to the user base of billions,
 
597
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucheng Ding et al.
and also cannot be distributed on each mobile device because of
the limited memory and computation resources. The other line of
non-parametric work introduced an external datastore to assist
LLM inference. Two celebrated methods are ğ‘˜-nearest neighbour
language modeling ( ğ‘˜NN-LM) [ 32] and retrieval-augmented gener-
ation (RAG) [ 34]. In particular, ğ‘˜NN-LM combines the pure LLM
prediction result and the ğ‘˜NN search results over the datastore in
the LLM embedding space, whereas RAG concatenates the retrieval
results over the datastore and the user query as the new input of
LLM. Considering the on-device scenario, ğ‘˜NN-LM is more suitable
in terms of memory efficiency and user input length.
However, the fundamental design of on-device LLM enhance-
ment withğ‘˜NN-LM needs to limit the size of the external datastore,
otherwise the token generation speed cannot meet the real-time
requirement (e.g., 10 tokens per second). In particular, the full data-
store constructed from an active userâ€™s all the dialogues with the
cloud-based LLM within a long work cycle (e.g., 1 month) signif-
icantly exceeds the size limit. In addition, if directly take the dia-
logues in a short cycle (e.g., 1 day or 1 week), the size and the quality
of the datastore are not sufficient for on-device LLM enhancement.
To balance the quality and the speed of on-device token gen-
eration, we propose to select an optimal subset, the size of which
is within the limit, from the full datastore for on-device ğ‘˜NN-LM
enhancement. Nevertheless, such a subset selection problem is NP-
hard. We thus turn to a near-optimal solution that can be found
efficiently on the mobile device. We first prove that the optimiza-
tion objective is a monotone and submodular function. A naive
algorithm, which greedily selects an element from the full datastore
with the highest marginal gain each time, can provide theoretical
performance guarantee, but has unaffordable time complexity. To
improve efficiency, we propose an offline design and an online
design1, which execute selection after building the full datastore
and over the online stream, respectively. In particular, the offline
design overcomes the bottlenecks of the greedy algorithm by (1)
estimating an elementâ€™s marginal gain by its lower bound, which
depends only on itself, rather than calculating the exact marginal
gain related to the full datastore; and (2) selecting a batch of ele-
ments with the highest marginal gains from a randomly sampled
subset, rather than one element from the full datastore each time. In
addition, the offline design is quite salable with performance guar-
antee, and its time complexity is independent of the full datastore
size. We also interpret the offline design from the perspective of
pool-based active learning with training loss as the selection metric.
Regarding the online design, it maintains a small buffer, selects
elements incrementally once the buffer is full, and can be flexibly
scheduled throughout a work cycle. The key difference from the
offline setting is that the full length of the stream is unknown when
doing selection. We thus propose to maintain several candidate
datastore subsets with different selection ratios and return the best
subset at the end of the work cycle.
We summarize the key contributions of this work as follows:
â€¢We focus on the emerging device-cloud hybrid LLM service
framework and consider the new problem of effectively and
efficiently enhancing on-device LLM inference.
1The offline/online design in this paper refers to pool-based/stream-based algorithm,
and does not mean the mobile devices being disconnected/connected to the Internet.
Cloud-Based LLM ServiceOn-Device LLM Local Service 
Cloud API UnavailableFigure 1: Device-cloud hybrid LLM service.
â€¢We, for the first time, propose to take each userâ€™s historical
high-quality interactions with the cloud-based LLM as an
external datastore to enhance on-device LLM using ğ‘˜NN-LM.
â€¢To balance on-device token generation quality and speed,
we consider how to select an optimal subset of the full data-
store. We propose both offline and online designs with near-
optimal guarantees and low time complexities.
â€¢We extensively evaluate the proposed designs using three
ChatGPT related datasets, four different on-device LLMs, and
the testbed of Honor V30 Pro. Key evaluation results include:
(1) compared with pure on-device LLM, enhancement with
an external datastore subset reduces perplexity by 16.07%
on average; (2) compared with on-device LLM with the full
datastore, the subset selection boosts the generation speed
up to 2.3Ã—and achieves 10 tokens per second; and (3) the
memory usage of the on-device datastore subset selection is
within 1.37GB.
2 Background and Problem
To support the deployment of LLMs in various application scenar-
ios with different computational resources, many companies have
launched a series of LLMs with different parameter sizes, such as
the open-source GPT-1, GPT-2, and the API-only GPT-3.5 or Chat-
GPT from OpenAI [ 19]; the API-only Gemini-Nano, Gemini-Pro,
and Gemini-Ultra from Google [ 4]; the open-source 1.8B, 7B, 14B,
and 72B versions of Qwen and the API-only Qwen-Max from Al-
ibaba [ 15]; the open-source 7B, 13B, and 70B versions of Llama 2
from Meta [ 17]; the open-source 7B and 13B versions of Alpaca
from Stanford [ 1]; the open-source 7B, 13B, and 33B versions of
Vicuna from UC Berkeley [ 13]; and the open-source 7B and 13B
versions of Baichuan2 and the API-only Baichuan3 [16].
Among the series of LLMs, the largest model, the size of which
normally reaches hundreds of billions, is deployed on the resource-
rich cloud to serve massive users through online API requests (e.g.,
OpenAIâ€™s ChatGPT, Googleâ€™s Bard, Anthropicâ€™s Claude, Alibabaâ€™s
Qwen, Baiduâ€™s ERNIE Bot), whereas the light-weight model, the size
of which is at the magnitude of billions, supports the deployment
on the resource-constraint mobile devices for local real-time service
(e.g., TinyLlama-1.1B, Qwen-1.8B, ChatGLM3-6B, Baichuan2-7B,
Gemini-Nano). As shown in Figure 1, the cloud-based LLM and
the on-device LLM serve users in a hybrid way to cover all the
time periods, thereby ensuring the completeness of LLM service. In
particular, when the cloud-based LLM service is available to a user
with a mobile device, the mobile device directly sends requests, and
the cloud returns responses; but when the cloud-based LLM service
is not available, such as when the network condition of the mobile
device is poor, the free-tier rate limit of cloud API has been reached
 
598Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Datastore Construction in One Work Cycle
Key-Value PairsDatastore
TokenSequence
User QueryData-StoreOn-Device LLM
Local Response
On-DeviceLLMInferencewith Datastore Enhancement
Conversation
Figure 2: On-device LLM inference enhancement with local
datastore constructed from cloud-based LLM interactions.
(e.g., 200 requests per day for ChatGPT, 1000 messages per day for
Claude), or the cloud service platform experiences congestion or
crashes, the on-device LLM needs to be invoked to serve the user.
However, compared with the cloud-based LLM, the on-device
LLM is smaller in parameter size and limited in model performance.
For example, according to OpenCompass 1.0 LLM Leaderboard [ 20],
the average score and the language-related score of Qwen-1.8B are
16.9 and 12.3 lower than ChatGPT, respectively. To guarantee the
quality of local service when the cloud-based API is not available,
how to enhance the performance of on-device LLM under the device-
cloud hybrid service framework becomes an important problem.
3 Fundamental Design of On-Device LLM
Enhancement
To enhance on-device LLM service, the basic idea of our design
is to build an external datastore from the historical high-quality
interactions with cloud-based LLM to assist local LLM inference.
The key insights include: (1) from query distribution, a certain
userâ€™s historical queries to the cloud-based LLM are close to his/her
queries to the on-device LLM; and (2) from response quality, the
cloud-based LLM exhibits impressive capabilities, far stronger than
the on-device LLM, and its responses can be regarded as reference
answers to dramatically improve local LLM inference. In what
follows, we introduce the details of the fundamental design.
3.1 Local Datastore Construction from
Historical Cloud-Based LLM Interactions
As depicted in Figure 2, a userâ€™s new conversation with the cloud-
based LLM will be stored on the mobile device. In particular, each
conversation is first converted into a standard token sequence,
denoted as(ğ‘¤1,ğ‘¤2,...,ğ‘¤ğ‘™), which can further be split into (ğ‘™âˆ’1)
context-target pairs, denoted as {(ğ‘¤1:ğ‘š,ğ‘¤ğ‘š+1)|ğ‘š=1,2,...,ğ‘™âˆ’1}.
For each context-target pair (ğ‘¤1:ğ‘š,ğ‘¤ğ‘š+1), the context ğ‘¤1:ğ‘šis first
encoded by the on-device LLM to a representation vector ğ‘”(ğ‘¤1:ğ‘š)
for retrieval purpose, where ğ‘”(Â·)denotes the encoding function;
and(ğ‘”(ğ‘¤1:ğ‘š),ğ‘¤ğ‘š+1)constitutes a key-value pair, added to the local
datastoreğ‘ˆon the mobile device.
The construction of a mobile deviceâ€™s local datastore spans a
preset work cycle (e.g., 1 month) and comprises all the key-valuepairs constructed from multiple user interactions with the cloud-
based LLM in this work cycle. This datastore will be used to enhance
the on-device LLM inference until the end of the next work cycle
and then refreshed. We let ğ‘ˆ={(ğ‘”(ğ‘¥ğ‘–),ğ‘¦ğ‘–)|ğ‘–=1,2,...,ğ‘}denote
the latest local datastore with ğ‘key-value pairs in total, where
D={(ğ‘¥ğ‘–,ğ‘¦ğ‘–)|ğ‘–=1,2,...,ğ‘}denotes all the context-target pairs.
3.2 On-Device LLM Inference with Datastore
Given the external datastore ğ‘ˆ, we leverage ğ‘˜NN-LM [ 32] to assist
on-device LLM inference. In essence, LLM inference is to execute the
next token prediction task. We let Ë†ğ‘¤1:ğ‘šdenote an input context and
letË†ğ‘¤ğ‘š+1denote the next token to be predicted. Then, the probability
distribution of Ë†ğ‘¤ğ‘š+1predicted by ğ‘˜NN-LM is the combination of the
on-device LLMâ€™s output and the retrieval results from the datastore
ğ‘ˆ, formulated as
ğ‘(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š,ğ‘ˆ)
=(1âˆ’ğ›¼)ğ‘DLM(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š)+ğ›¼ğ‘ğ‘˜NN(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š,ğ‘ˆ),(1)
where 0<ğ›¼<1is a tuned parameter to adjust the weights of on-
device LLM and datastore in the final prediction; ğ‘DLM(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š)
denotes the output distribution of the on-device LLM over the next
token; andğ‘ğ‘˜NN(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š,ğ‘ˆ)denotes the retrieval result from
ğ‘ˆ. In particular, we let Kdenote theğ‘˜-nearest neighbours in ğ‘ˆ
according to the Euclidean distance between the datastoreâ€™s key
ğ‘˜ğ‘’ğ‘¦and the input contextâ€™s representation vector ğ‘”(Ë†ğ‘¤1:ğ‘š), denoted
asğ‘‘(ğ‘˜ğ‘’ğ‘¦,ğ‘”(Ë†ğ‘¤1:ğ‘š)); and let(ğ‘˜ğ‘’ğ‘¦,ğ‘£)denote any key-value pair in K
for brevity. Then, the retrieval result ğ‘ğ‘˜NN(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š,ğ‘ˆ)can be
computed as
ğ‘ğ‘˜NN(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š,ğ‘ˆ)âˆâˆ‘ï¸
(ğ‘˜ğ‘’ğ‘¦,ğ‘£)âˆˆK1Ë†ğ‘¤ğ‘š+1=ğ‘£ğ‘’âˆ’ğ‘‘(ğ‘˜ğ‘’ğ‘¦,ğ‘”(Ë†ğ‘¤1:ğ‘š)).(2)
3.3 Feasibility Study
To validate the feasibility of the fundamental design, we first mea-
sure the time overhead of local datastore construction and introduce
when it is reasonable to perform this task on a mobile device. For
the token sequence (ğ‘¤1,ğ‘¤2,...,ğ‘¤ğ‘™), the most time-consuming step
of datastore construction is to compute the representation vectors
{ğ‘”(ğ‘¤1:ğ‘š)|ğ‘š=1,2,...,ğ‘™âˆ’1}, the encoding speed of which is similar
to the on-device LLM generation speed. For example, the inference
speed of Qwen-1.8B on the middle-end smartphone using MNN [ 39]
is roughly 18 tokens per second. We also measure the response la-
tency of the cloud-based Qwen API and find that the generation
speed is roughly 10.3 tokens per second, which is slower than the
on-device LLM generation speed. Therefore, after a mobile device
sends a request and is waiting for the response from the cloud, the
mobile device is almost idle and can perform datastore construction,
not requiring any extra time.
We then reveal the performance improvement of on-device LLM
enhancement with a datastore. We take a multi-turn dialogue dataset,
called ShareGPT [ 11], which consists of the real-world conversa-
tions between users and ChatGPT. We filter out the datasets about
5 different topics from ShareGPT. Each topic-related dataset sim-
ulates the conversations of a user. For each user, we take 70% of
conversations as the cloud-based LLM (i.e., ChatGPT) interactions
to construct the local datastore. The number of conversations for
each userâ€™s datastore construction is between 200and400and
 
599KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucheng Ding et al.
Agric. Intel. Marke. Finan. Legal2.53.03.54.04.55.0PerplexityPure
Enhanced
Figure 3: The perplexity of
Qwen-1.8 with and without an
external datastore.
10 tokens/s18 tokens/sThe common size of an active userâ€™s full datastore2.5Ã—Figure 4: The on-device infer-
ence latency of Qwen-1.8 with
different datastore sizes.
aligns with the practical number of ChatGPT conversations for
an active user in one month. The size of key-value pairs in the
datastore is between 2Ã—105and4Ã—105. We take the remaining
30% of conversations to evaluate the perplexity of on-device LLM
(i.e., Qwen-1.8B) with and without datastore enhancement. From
the results shown in Figure 3, we can observe that the datastore
enhancement indeed improves the on-device LLM performance by
sharply reducing perplexity.
We finally identify the bottleneck of on-device LLM generation
speed with different sizes of an external datastore. We take the
smartphone Honor V30 Pro as the testbed and take Qwen-1.8B as
the on-device LLM. In particular, the dimension of the represen-
tation vector in the datastore is 2048, namely, the hidden size of
Qwen-1.8B. In addition, the inference latency per token is estimated
by adding up the original generation latency of Qwen-1.8B and the
ğ‘˜NN-LM search time measured on the mobile device. We vary the
size of datastore from 0 to 4Ã—105, increasing by 105each time, and
show the results in Figure 4. We can see that (1) as the datastore size
grows, the on-device inference latency increases; and (2) to achieve
real-time token generation (i.e., at least 10 tokens per second), the
size of datastore should be kept under 105. In particular, with 4Ã—105
key-value pairs in the datastore, the on-device inference speed is
only 4 tokens per second, 4.5 Ã—slower than the pure on-device LLM
inference, which seriously degrades user interactive experience.
3.4 Newly Introduced Datastore Subset
Selection Problem
To enhance the on-device LLM while guaranteeing the inference
speed, it is necessary to select a subset of full datastore, denoted
asğ‘†âŠ‚ğ‘ˆ, such that the size of the subset is within the preset limit
ğ¿, namely,|ğ‘†|â‰¤ğ¿. In addition, given the full set of the context-
target pairsD={(ğ‘¥ğ‘–,ğ‘¦ğ‘–)|ğ‘–=1,2,...,ğ‘}and the full datastore ğ‘ˆ=
{(ğ‘”(ğ‘¥ğ‘–),ğ‘¦ğ‘–)|ğ‘–=1,2,...,ğ‘}, the optimal subset selection strategy
should maximize the log-likelihood of all the targets given the
corresponding contexts. We define the optimization objective as
ğ‘†âˆ—=arg max
ğ‘†âŠ‚ğ‘ˆ,|ğ‘†|â‰¤ğ¿ğ¹(ğ‘†)=arg max
ğ‘†âŠ‚ğ‘ˆ,|ğ‘†|â‰¤ğ¿âˆ‘ï¸
(ğ‘¥ğ‘–,ğ‘¦ğ‘–)âˆˆDlogğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†),(3)
whereğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)can be computed by equation 1 with ğ‘¥ğ‘–,ğ‘¦ğ‘–, andğ‘†
corresponding to Ë†ğ‘¤1:ğ‘š,Ë†ğ‘¤ğ‘š+1, andğ‘ˆ, respectively.
Unfortunately, directly solving equation 3 is infeasible since it is
NP-hard. We need to consider how to efficiently find a near-optimal
datastore subset on the resource-constraint mobile device.4 Design of On-Device Datastore Subset
Selection
We first derive the submodular property and then introduce efficient
offline and online designs, where the difference is whether the
subset selection is performed after the full datastore construction
or directly over the stream of key-value pairs.
4.1 Submodularity of Subset Selection
Forğ‘˜NN-LM, we assume that ğ‘ğ‘˜NN(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)inğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)is deter-
mined by all the key-value pairs with the value ğ‘£=ğ‘¦ğ‘–, denoted
asğ‘†ğ‘¦ğ‘–âŠ‚ğ‘†. Then, the on-device next token prediction function
ğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)becomes
ğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)=(1âˆ’ğ›¼)ğ‘DLM(ğ‘¦ğ‘–|ğ‘¥ğ‘–)
+ğ›¼âˆ‘ï¸
(ğ‘˜ğ‘’ğ‘¦,ğ‘£)âˆˆğ‘†ğ‘¦ğ‘–ğ‘ğ‘’âˆ’ğ‘‘(ğ‘˜ğ‘’ğ‘¦,ğ‘”(ğ‘¥ğ‘–)), (4)
whereğ‘is a constant. Besides, to ensure the non-negativity of the
objective, we define ğ‘“(ğ‘†)â–³=ğ¹(ğ‘†)âˆ’ğ¹(âˆ…)as the objective function,
whereğ¹(âˆ…)is a constant. We then demonstrate its submodularity.
Theorem 1. Under equation 4, the objective function ğ‘“(ğ‘†)is non-
negative, monotonically non-decreasing, and submodular.
Proof. Obviously,ğ‘“(ğ‘†)â‰¥0andğ‘“(âˆ…)=0. We first proof ğ‘“(ğ‘†)
is monotone non-decreasing. For any subset ğ´1âŠ†ğ´2âŠ†ğ‘ˆand
context-target pair (ğ‘¥ğ‘–,ğ‘¦ğ‘–), we have
ğ‘ğ‘˜NN(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ´1)=âˆ‘ï¸
(ğ‘˜ğ‘’ğ‘¦,ğ‘£)âˆˆğ´ğ‘¦ğ‘–
1ğ‘ğ‘’âˆ’ğ‘‘(ğ‘”(ğ‘¥ğ‘–),ğ‘˜ğ‘’ğ‘¦)
(ğ‘)
â‰¤âˆ‘ï¸
(ğ‘˜ğ‘’ğ‘¦,ğ‘£)âˆˆğ´ğ‘¦ğ‘–
2ğ‘ğ‘’âˆ’ğ‘‘(ğ‘”(ğ‘¥ğ‘–),ğ‘˜ğ‘’ğ‘¦)
=ğ‘ğ‘˜NN(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ´2),(5)
where (a) holds because ğ´ğ‘¦ğ‘–
1âŠ†ğ´ğ‘¦ğ‘–
2. Therefore, we further have
ğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ´1)â‰¤ğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ´2)andğ‘“(ğ´1)â‰¤ğ‘“(ğ´2).
We next prove ğ‘“(ğ‘†)is submodular. For any ğ´1âŠ†ğ´2âŠ†ğ‘‰, key-
value pairğ‘Ÿğ‘–=(ğ‘”(ğ‘¥ğ‘–),ğ‘¦ğ‘–)âˆ‰ğ´2, and(ğ‘¥ğ‘˜,ğ‘¦ğ‘˜)âˆˆD , we have
Î”ğ‘˜(ğ‘Ÿğ‘–|ğ‘†)=logğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†âˆª{ğ‘Ÿğ‘–})
ğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†)
=log
1+ğ›¼(ğ‘ğ‘˜NN(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†âˆª{ğ‘Ÿğ‘˜})âˆ’ğ‘ğ‘˜NN(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†))
ğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†)
=log 
1+ğ›¼ğ‘1ğ‘¦ğ‘˜=ğ‘¦ğ‘–ğ‘’âˆ’ğ‘‘(ğ‘”(ğ‘¥ğ‘–),ğ‘”(ğ‘¥ğ‘˜))
ğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†)!
.(6)
where Î”ğ‘˜(ğ‘Ÿğ‘–|ğ‘†)is defined in equation 8 and ğ‘†âˆˆ{ğ´1,ğ´2}. Besides,
by substituting equation 5 into equation 4, we have ğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ´1)â‰¤
ğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ´2). Therefore, we can derive that
âˆ€(ğ‘¥ğ‘˜,ğ‘¦ğ‘˜)âˆˆD,Î”ğ‘˜(ğ‘Ÿğ‘–|ğ´1)â‰¥Î”ğ‘˜(ğ‘Ÿğ‘–|ğ´2), (7)
showing that Î”(ğ‘Ÿğ‘–|ğ´1)â‰¥Î”(ğ‘Ÿğ‘–|ğ´2). Therefore, ğ‘“(ğ‘†)is submodular.
â–¡
 
600Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Alg. 1Key-Value Pairs Stream
â€¦
â€¦Subset Selection After the Full Datastore Has Been Constructed
Figure 5: Workflow of offline datastore subset selection.
4.2 Offline Datastore Subset Selection
Given the properties of ğ‘“(ğ‘†)in Theorem 1, a near-optimal datastore
subsetğ‘†with the approximation ratio of (1âˆ’1/ğ‘’)can be found by
a naive greedy algorithm. In particular, for each step, the key-value
pairğ‘Ÿğ‘–â–³=(ğ‘”(ğ‘¥ğ‘–),ğ‘¦ğ‘–)with the highest marginal gain, denoted as
Î”(ğ‘Ÿğ‘–|ğ‘†):=ğ‘“(ğ‘†âˆª{ğ‘Ÿğ‘–})âˆ’ğ‘“(ğ‘†), is added to ğ‘†until reaching the limit
of subset size|ğ‘†|=ğ¿. However, this greedy algorithm is inefficient
on the resource-constraint mobile device for the following reasons:
(1) finding the pair with the highest marginal gain among all the
key-value pairs is time-consuming, as the size of the full datastore
is large; (2) computing the exact ğ‘“(ğ‘†)requires the time overhead
proportional to the total number of tokens in the stored sequences;
and (3) once adding a new key-value pair to the datastore subset,
the marginal gains of all the remaining pairs need to be recomputed.
Specific to the above three problems, we propose an efficient
offline datastore subset selection algorithm in Algorithm 1, which
consists of three key optimization techniques: (1) Random Subset
Sampling (line 3): To speed up submodular optimization with
performance guarantee, a common method is to compute over a
random subset rather than the full set [ 43]. Therefore, in each round
ğ‘¡, we randomly sample only ğµkey-value pairs from ğ‘ˆ\ğ‘†, denoted
asğ‘…, for identifying the key-value pair with the highest marginal
gain; (2) Lower Bound on Marginal Gain (line 4): Inspired by the
widely adopted technique of maximizing the lower bound in the
field of optimization, we turn to deriving the lower bound of the
marginal gain that can be quickly computed as the selection metric.
In particular, when adding a key-value pair ğ‘Ÿğ‘–=(ğ‘”(ğ‘¥ğ‘–),ğ‘¦ğ‘–)âˆˆğ‘…to
the datastore subset ğ‘†, the marginal gain can be expressed as
Î”(ğ‘Ÿğ‘–|ğ‘†)=logğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†âˆª{ğ‘Ÿğ‘–})âˆ’logğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)
|                                           {z                                           }
Î”ğ‘–(ğ‘Ÿğ‘–|ğ‘†):Internal gain over its own
+âˆ‘ï¸
ğ‘˜â‰ ğ‘–[logğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†âˆª{ğ‘Ÿğ‘–})âˆ’logğ‘(ğ‘¦ğ‘˜|ğ‘¥ğ‘˜,ğ‘†)]
|                                                 {z                                                 }
Î”ğ‘˜(ğ‘Ÿğ‘–|ğ‘†):External gain over any other context-target pair
â‰¥Î”ğ‘–(ğ‘Ÿğ‘–|ğ‘†)=log
1+ğ›¼ğ‘
ğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)
,(8)
where the inequality comes from the external gain Î”ğ‘˜(ğ‘Ÿğ‘–|ğ‘†)â‰¥0by
equation 4. We also have experimentally found that the internal
gainÎ”ğ‘–(ğ‘Ÿğ‘–|ğ‘†)as the lower bound is a good estimation of the mar-
ginal gain Î”(ğ‘Ÿğ‘–|ğ‘†), since Î”ğ‘–(ğ‘Ÿğ‘–|ğ‘†)is far greater than the external
gainÎ”ğ‘˜(ğ‘Ÿğ‘–|ğ‘†)in most cases; and (3) Margin Gain Update After
Batch Selection (lines 4â€“5): From equation 4, the marginal gains
of the key-value pairs with different values do not interfere with
each other. Further considering the large target token space, the
addition of a small number of key-value pairs has little impact onAlgorithm 1 Offline Datastore Subset Selection
Require: The full datastore ğ‘ˆ; the size limit of the on-device data-
store sutset ğ¿; the number of random sampling times ğ‘‡; the
size of candidate key-value pairs per sampling ğµ; the size of
selected pairs per sampling ğ‘.
Ensure:ğ¿=ğ‘‡Ã—ğ‘.
1:ğ‘†â†âˆ…;
2:forğ‘¡â†1toğ‘‡do
3:ğ‘…â†Randomly sample ğµkey-value pairs from ğ‘ˆ\ğ‘†;
4:ğ‘…ğ‘¡â†Select the top ğ‘key-value pairs from ğ‘…according to
the lower bound on the marginal gain;
5:ğ‘†â†ğ‘†âˆªğ‘…ğ‘¡;
6:returnğ‘†
the marginal gains. Therefore, we propose to choose the top ğ‘key-
value pairs from the random set ğ‘…and then update the datastore
subsetğ‘†for the margin gain computation in the next round.
Algorithm Analysis. We first show that Algorithm 1 can return
a near-optimal datastore subset ğ‘†in expectation
E[ğ‘“(ğ‘†)]â‰¥
1âˆ’1
ğ‘’âˆ’ğ›¾âˆ’ğœ–
ğ‘“(ğ‘†âˆ—), (9)
whereğ›¾,ğœ–are related to ğ‘andğµ/ğ‘, respectively, and ğ›¾,ğœ–â‰ª1with
appropriate settings. The details are deferred to Appendix A.
We also analyze the time complexity. In each round, it takes ğ‘‚(ğµ)
time to compute the lower bound on the marginal gain for ğ‘Ÿâˆˆğ‘…
and further takes ğ‘‚(ğµ)time to choose the top ğ‘key-value pairs.
Therefore, the time complexity of Algorithm 1 is ğ‘‚(ğµğ‘‡), which is
independent of the size of the full datastore ğ‘and is quite scalable.
View from Active Learning. Algorithm 1 can be regarded as a
special case of pool-based active learning [ 52]. We recall that the
lower bound on the ğ‘Ÿğ‘–â€™s marginal gain is monotonically decreasing
with respect to ğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†). Therefore, for a given model (i.e., the
on-device LLM plus the datastore subset), selecting the key-value
pair with the largest lower bound on the marginal gain is actu-
ally choosing the data sample with the largest training loss (i.e.,
âˆ’logğ‘(ğ‘¦ğ‘–|ğ‘¥ğ‘–,ğ‘†)). Analogous to active learning, which repeatedly
collects unlabeled data samples with highest uncertainty for fine-
tuning, Algorithm 1 repeatedly chooses the key-value pairs with
the largest training loss and adds them into the datastore subset to
â€œfinetuneâ€ the on-device LLM in a non-parametric way.
4.3 Online Selection over Key-Value Stream
We propose an online extension of the datastore subset selection,
which supports incremental selection directly over the key-value
stream and can be flexibly scheduled at appropriate time throughout
the entire work cycle. As illustrated in Figure 6, the basic idea is to
introduce a small buffer of ğµkey-value pairs. Once the buffer is full,
the selection will be triggered. The key difference from the offline
algorithm is that the length of stream (i.e., the full datastore size
ğ‘) is unknown when performing selection. Therefore, how many
key-value pairs should be selected from the buffer is also unknown.
To ensure that regardless of ğ‘at least one feasible datastore subset
can be constructed at the end of the work cycle, multiple candidate
datastore subsets are maintained simultaneously to select the key-
value pairs from the buffer with different ratios.
 
601KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucheng Ding et al.
Key-Value Pairs Stream
â€¦
â€¦
*ReturnedHigh Selection RatioLow Selection RatioSelection OncetheSmallBufferis Fullâ€¦
Figure 6: Workflow of online selection with different ratios.
Algorithm 2 Online Key-Value Buffer Selection
Require: The online stream of key-value pairs {ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿğ‘}; the
size limit of datastore subset ğ¿; the candidate datastore subsets
S={ğ‘†1,ğ‘†2,...,ğ‘†â„}; the size of full buffer ğµ; different numbers
of selected key-value pairs over the buffer b={ğ‘1,ğ‘2,...,ğ‘â„}.
1:For eachğ‘†ğ‘—âˆˆS,ğ‘†ğ‘—â†âˆ…;
2:ğµğ‘¢ğ‘“ğ‘“ğ‘’ğ‘Ÿâ†âˆ…;
3:forğ‘–â†1toğ‘do
4: Compute the lower bound Î”ğ‘–(ğ‘Ÿğ‘–|ğ‘†ğ‘—)on the marginal gain
Î”(ğ‘Ÿğ‘–|ğ‘†ğ‘—)for eachğ‘†ğ‘—âˆˆS;
5: if|ğµğ‘¢ğ‘“ğ‘“ğ‘’ğ‘Ÿ|<ğµthen
6:ğµğ‘¢ğ‘“ğ‘“ğ‘’ğ‘Ÿâ†ğµğ‘¢ğ‘“ğ‘“ğ‘’ğ‘Ÿâˆª{ğ‘Ÿğ‘–};
7: if|ğµğ‘¢ğ‘“ğ‘“ğ‘’ğ‘Ÿ|=ğµthen
8: foreachğ‘ğ‘—âˆˆbdo
9:ğ‘…ğ‘—â†Select topğ‘ğ‘—key-value pairs from ğµğ‘¢ğ‘“ğ‘“ğ‘’ğ‘Ÿ ac-
cording to the lower bound on marginal gain over ğ‘†ğ‘—;
10:ğ‘†ğ‘—â†ğ‘†ğ‘—âˆªğ‘…ğ‘—;
11: if|ğ‘†ğ‘—|>ğ¿then
12: Removeğ‘†ğ‘—from Sand remove ğ‘ğ‘—from b;
13:ğµğ‘¢ğ‘“ğ‘“ğ‘’ğ‘Ÿâ†âˆ…;
14:ğ‘šğ‘ğ‘¥â†arg maxğ‘—ğ‘†ğ‘—âˆˆS;
15:returnğ‘†ğ‘šğ‘ğ‘¥
Based on the design rationale, we present the details of the online
design in Algorithm 2. â„candidate datastore subsets are globally
maintained, denoted as S={ğ‘†ğ‘—|ğ‘—=1,2,Â·Â·Â·,â„}. Each datastore
subsetğ‘†ğ‘—will select the top ğ‘ğ‘—key-value pairs from the buffer (line
9). In particular,{ğ‘ğ‘—|ğ‘—=1,2,...,â„âˆ’1}forms an exponentially
growing integer array with ğ‘ğ‘—+1=ğ‘ğ‘—Ã—ğ‘forğ‘â‰¥2andğ‘â„âˆ’1<
ğµ, andğ‘â„=ğµ. Once the size of ğ‘†ğ‘—exceeds the limit ğ¿, it will
become inactive (lines 11-12). Finally, the active datastore subset
ğ‘†ğ‘šğ‘ğ‘¥ with the maximum selection ratio is returned for on-device
LLM enhancement.
Algorithm Analysis. We first demonstrate that Algorithm 2
can return a datastore subset with good performance. The key
insight is that the collection of ğµkey-value pairs in the buffer can
be viewed as randomly sampling ğµpairs in the offline algorithm,
while maintaining multiple candidate datastore subsets ensures
that the returned subset is appropriate for any length of key-value
stream. Formally, we have
ğ‘“(ğ‘†ğ‘šğ‘ğ‘¥)â‰ˆ max
ğ‘†âŠ†ğ‘ˆ,|ğ‘†|â‰¤|ğ‘†ğ‘šğ‘ğ‘¥|ğ‘“(ğ‘†)â‰¥ğ‘“(ğ‘†âˆ—)
ğ‘, (10)
The details are deferred to Appendix A.Table 1: Statistics over the multi-turn dialogue datasets of
ShareGPT and Personalized-Interactive-Conversation.
Datasets
Topic/User Identity #Conversations
Shar
eGPT-Agriculture Agriculture/Environment 324
ShareGPT-Intelligence Artificial Intelligence 435
ShareGPT-Marketing Business, Marketing 583
ShareGPT-Finance Business, Finance 500
ShareGPT-Legal Legal 422
PerIntConv-Biologist
Iâ€™m a marine biologist 423
PerIntConv-Chef Iâ€™m a professional chef 318
PerIntConv-Student Iâ€™m a full-time student 457
PerIntConv-Teacher Iâ€™m a high school teacher 405
PerIntConv-Parent Iâ€™m a stay-at-home parent 406
We then analyze the time complexity of Algorithm 2. When a
new key-value pair comes, it requires to compute the lower bound
on the marginal gain for all the active candidate datastore subsets
(line 4). By equation 8, the time complexity is
âˆ‘ï¸
ğ‘†ğ‘—âˆˆSğ‘‚(|ğ‘†ğ‘—|)â‰¤ğ‘‚(ğ¿+ğ¿
ğ‘+ğ¿
ğ‘2+...)â‰¤ğ‘‚ğ‘ğ¿
ğ‘âˆ’1
, (11)
which is only ğ‘‚(ğ¿), same as one-time ğ‘˜NN-LM inference complex-
ity. In practice, the mobile device can collect a batch of key-value
pairs and then trigger computation in parallel, and the time over-
head is still ğ‘‚(ğ¿). Specifically, according to our evaluation, given
ğ¿=105andâ„=5candidate datastore subsets, computing the
lower bound on the margin value for 1000 key-value pairs on the
smartphone Honor V30 Pro costs roughly 23 seconds. Therefore, the
online design is quite efficient in practice and can flexibly scheduled
at the idle time of a mobile device.
5 Evaluation
5.1 Evaluation Setups
Datasets and User-Level Partition. We take 3 datasets in total
for 2 different application scenarios. (1) Multi-Turn Dialogue:
The first dataset is ShareGPT, which has been introduced in the
feasibility study (Section 3.3). The details on the 5 topics for 5 dif-
ferent users and the size of topic-related conversations are reserved
in Table 1 in Appendix B. The second dataset is Personalized-
Interactive-Conversations [8], which comprises the prompts
about usersâ€™ profiles and the conversations between the users and a
cloud-based assistant. Based on the user information, we filter out
the conversations of 5 different users. The number of one userâ€™s
conversations is between 318 and 423. For the two multi-turn di-
alogue datasets, we take about 70% of the conversations for local
datastore construction and leave the remaining 30% for testing the
performance of on-device LLM inference. The statistics of the sim-
ulated multi-turn dialogue user datasets are recorded in Table 1;
and (2) Single-Turn QA: The third dataset is Chinese-Alpaca [3],
which translates the Alpacaâ€™s training dataset [ 1] to Chinese using
ChatGPT API. We filter out low-quality answers and keep only
the responses with the length longer than 128. We evenly divide
the filtered data to 4 different users. Each user has roughly 2000
question-answer pairs, about 80% for building the local datastore
and the left 20% for testing.
 
602Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Perplexity (lower is better) of the proposed design and baselines with different on-device LLMs over different datasets.
Datasets
On-Device LLMs Pure Random Merging FullSubset Offline vs.
Offline
Online Pure Random Merging
Shar
eGPT-Agriculture Qwen-1.8B 3.5326 2.9415 2.8613 2.6753 2.7428 2.7661 â†“22.36%â†“6.76%â†“4.14%
ShareGPT-Intelligence Qwen-1.8B 5.2997 4.6004 4.4595 4.2308 4.3608 4.3809 â†“17.72%â†“5.21%â†“2.21%
ShareGPT-Marketing Qwen-1.8B 4.6277 4.4120 4.2534 4.0194 4.1786 4.1585 â†“9.70%â†“5.29%â†“1.76%
ShareGPT-Finance Qwen-1.8B 4.3579 4.1084 3.9814 3.7885 3.8857 3.8804 â†“10.84%â†“5.42%â†“2.40%
ShareGPT-Legal Qwen-1.8B 3.9492 2.8039 2.7421 2.5386 2.6336 2.7365 â†“33.31%â†“6.07%â†“3.96%
PerIntConv-BiologistTinyLlama-1.1B
3.9138 3.0852 3.1379 2.9343 3.0795 3.0850 â†“21.32%â†“0.18%â†“1.86%
OPT-1.3B 4.2855 3.6492 3.7104 3.5461 3.6175 3.6549 â†“15.59%â†“0.87%â†“2.50%
Pythia-1.1B 4.6530 3.9049 3.9537 3.7138 3.8290 3.8373 â†“17.71%â†“1.94%â†“3.15%
PerIntConv-ChefTinyLlama-1.1B
3.7845 3.1547 3.2212 3.0216 3.1386 3.1680 â†“17.07%â†“0.51%â†“2.56%
OPT-1.3B 4.1334 3.6720 3.7556 3.5860 3.6068 3.6406 â†“12.74%â†“1.78%â†“3.96%
Pythia-1.1B 4.4137 3.9276 4.0672 3.7910 3.7848 3.8322 â†“14.25%â†“3.64%â†“6.94%
PerIntConv-StudentTinyLlama-1.1B
4.2198 3.5195 3.5352 3.2918 3.4605 3.4638 â†“17.99%â†“1.68%â†“2.11%
OPT-1.3B 4.3181 3.8757 3.9048 3.7174 3.7891 3.7944 â†“12.25%â†“2.23%â†“2.96%
Pythia-1.1B 4.7743 4.2940 4.3464 4.0531 4.1213 4.1295 â†“13.68%â†“4.02%â†“5.18%
PerIntConv-
TeacherTinyLlama-1.1B 4.3038 3.4549 3.5327 3.3037 3.4445 3.4543 â†“19.97%â†“0.30%â†“2.50%
OPT-1.3B 4.3785 3.8069 3.8362 3.6795 3.7621 3.7667 â†“14.08%â†“1.18%â†“1.93%
Pythia-1.1B 4.8305 4.1705 4.2105 3.9761 4.0632 4.0708 â†“15.88%â†“2.57%â†“3.50%
PerIntConv-Par
entTinyLlama-1.1B 4.5374 3.7662 3.7978 3.552 3.7056 3.7091 â†“18.33%â†“1.61%â†“2.43%
OPT-1.3B 4.6678 4.1543 4.2432 4.0396 4.0509 4.1068 â†“13.22%â†“2.49%â†“4.53%
Pythia-1.1B 5.2705 4.6980 4.7869 4.4629 4.4926 4.5239 â†“14.76%â†“4.37%â†“6.15%
CN-
Alpaca (Average) Qwen-1.8B 13.3281 12.8401 12.8463 12.5659 12.7050 12.7425 â†“4.68%â†“1.05%â†“1.10%
Cloud-Based LLM and On-Device LLMs. We take ChatGPT
as the cloud-based LLM, which generates the three datasets by
interacting with different users. For on-device LLMs, we take Qwen-
1.8B [ 45] for ShareGPT, take TinyLlama-1.1B [ 12,53], OPT-1.3B [ 54],
and Pythia-1.1B [ 22] for Personalized-Interactive-Conversations,
and take Qwen-1.8B for Chinese-Alpaca.
Baselines. To compare with the proposed on-device LLM en-
hancement designs with full datastore (â€œFullâ€), offline selected data-
store subset (â€œOfflineâ€), and online selected datastore subset (â€œOn-
lineâ€), we introduce the following baselines: (1) Pure On-Device
LLM (â€œPureâ€), which does not introduce any external datastore; (2)
Random Selection (â€œRandomâ€), which randomly selects ğ¿key-
value pairs from the full datastore for enhancement; and (3) Merg-
ing[28], which heuristically compresses the full datastore by merg-
ing neighboring key-value pairs with the same value. The number
of merging neighbours is set to 100, and only the top ğ¿pairs with the
highest weights after merging are kept for enhancement. We note
that the baselines of random selection and merging are executed in
an offline manner after the construction of the full datastore.
Implementation Settings. The size limit on the datastore sub-
setğ¿is set to 105. For the offline design, the size of candidate
key-value pairs per sampling ğµis set to 50,000, the size of selected
pairs per sampling ğ‘is set to 10,000, and the number of sampling
timesğ‘‡=ğ¿/ğ‘is 10. For the online design, the buffer size is set to
10,000,ğ‘is set to 2, and â„=5different numbers of selected key-
value pairs over the buffer are b={1000,2000,4000,8000,10000}.
For on-device ğ‘˜NN-LM, the number of neighbours ğ‘˜is set to 100,
the weight of the external datastore in the final prediction ğ›¼is setto0.15,0.25,0.25, and 0.3by default for Qwen-1.8B, TinyLlama-
1.1B, OPT-1.3B, and Pythia-1.1B, respectively. The testbed of mobile
device takes Honor V30 Pro with Kirin 990 CPU and 8GB memory,
and the on-device deployment is through Termux.
5.2 Evaluation Results and Analysis
On-Device LLM Inference Performance and Latency. We re-
port in Table 2 the perplexity of the proposed design and the base-
lines. We also show in Figure 7 the inference latency with the full
datastore and the selected datastore subset.
By comparing the enhancement with the full datastore with the
pure on-device LLM, we can see that the perplexity decreases by
18.11% on average. We can draw that the historical interactions
with the cloud-based LLM indeed can significantly improve the
quality of on-device LLM generation.
By comparing the on-device LLM enhancement with the offline
and the online selected datastore subsets and with the full datastore,
we can observe that the offline and the online designs averagely
increase the perplexity by 2.49% and 3.07% (i.e., 0.09 and 0.11 in
absolute value of perplexity), respectively. The slight perplexity
gap comes from dropping up to 74% of key-value pairs. As a result,
the on-device LLM inference speed using the offline and the online
designs is up to 2.3Ã—faster than using the full datastore, where the
latter speed is much lower than the desired 10 tokens per second.
By comparing the on-device LLM enhancement with the pro-
posed offline and online designs with the three baselines, we can
observe that (1) compared with the pure on-device LLM, the per-
plexities of the offline and the online designs decrease by 16.07% and
 
603KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucheng Ding et al.
Offline Design
Online DesignShareGPT-Agriculture (2.5Ã—105 pairs)
ShareGPT-Intelligence (2.5Ã—105 pairs)ShareGPT-Marketing (3.9Ã—105 pairs)
ShareGPT-Finance (3.3Ã—105 pairs)ShareGPT-Legal (2.6Ã—105 pairs)
With Each Userâ€˜sFullDatastore4.3 tokens/s5.0 tokens/s5.7 tokens/s6.0 tokens/s10 tokens/s2.3Ã—
Figure 7: The on-device inference latency
with the full datastore and with the offline
selected datastore subset.
Merging Baseline229s (10K Pairs)23s (1K Pairs)1 Hour(
a) Selection Time
Merging Baseline0.761.371.982.043.072.632.08Honor V30 Memory  = 8GB (
b) Peak Memory
Figure 8: The selection time and peak memory of the proposed offline and online
designs as well as the merging baseline.
ShareGPT-Agriculture
ShareGPT-IntelligenceShareGPT-Marketing
ShareGPT-FinanceShareGPT-Legal
0.0 2.5 5.0 7.5 10.0 12.5 15.0
Size Limit (Ã— 104)2.53.03.54.04.55.05.5Perplexity
(
a) Offline Design
0.0 2.5 5.0 7.5 10.0 12.5 15.0
Size Limit (Ã— 104)2.53.03.54.04.55.05.5Perplexity
 (
b) Online Design
Figure 9: The perplexity of the offline and the online designs
with varying limits on the datastore subset size.
15.59% on average, respectively; (2) compared with the random se-
lection policy, which is the strongest baseline over the Personalized-
Interactive-Conversations and CN-Alpaca datasets, the perplexities
of the offline and the online designs decrease by 2.82% and 2.64%
on average, respectively; and (3) compared with the merging base-
line, which is the strongest baseline over the ShareGPT dataset,
the perplexities of the offline and the online designs decrease by
3.23% and 2.65% on average, respectively. These results reveal that
the datastore subset selection policy based on the metric of the
marginal gain is more effective in finding high-quality key-value
pairs for on-device LLM enhancement.
Additionally, the results over the ShareGPT and Personalized-
Interactive-Conversations datasets demonstrate the consistent en-
hancement of using the datastore (subset) over pure on-device LLM
under different levels of in-dataset topic similarities. Specifically,
each ShareGPT simulated user dataset follows the same topic, a sim-
ulated user dataset from Personalized-Interactive-Conversations
involves highly diverse topics, and the evaluation results over both
datasets in Table 2 show the effectiveness of taking the datastore
(subset) for enhancement.
On-Device Datastore Subset Selection Time. We first com-
pare the on-device subset selection time of the proposed offlinedesign and the merging baseline over the ShareGPT dataset. The
results are shown in Figure 8. Specifically, the primary time con-
suming contributor is the evaluation of marginal gain, which is
estimated by equation 8. As ğ‘DLM(ğ‘¦|ğ‘¥)is already computed when
encoding the interaction data, the extra time consumed for eval-
uating is approximately equal to that for computing ğ‘ğ‘˜NN(ğ‘¦|ğ‘¥,ğ‘†),
which further approximates to the retrieval time. Therefore, we
use the measured retrieval time as the estimation of the execution
time of the algorithms. More specifically, for Algorithm 1, we mea-
sure the total retrieval time; and for Algorithm 2, we measure the
time required for one step of evaluation, which retrieves a batch of
10,000 or 1,000 key-value pairs based on the setting, and plot the
longest time consumption in Figure 8. One key observation is that
the time overhead of the proposed offline design is 47.6 minutes,
reducing up to 6.79 Ã—than the merging baseline. The second key
observation is that as the size of a userâ€™s local full datastore grows,
the time overhead of the offline design does not change, whereas
the time overhead of the merging baseline significantly grows. This
is because the time complexity of the offline design is ğ‘‚(ğµğ‘‡), which
is independent of the full datastore size ğ‘, while the complexity of
the merging baseline is ğ‘‚(ğ‘2).
We then show the efficiency of online selection design over the
key-value stream. When accumulating 1,000 and 10,000 key-value
pairs in a batch, the most time-consuming step of computing the
lower bound on the margin gain in parallel costs roughly 23 seconds
and 229 seconds, respectively.
On-Device Peak Memory. We compare the memory usages of
the proposed offline and online designs as well as the merging base-
line. Specifically, we record the highest total memory consumption
of the active datastore subsets, and plot the results in Figure 8. We
can see that the peak memories of our two designs are consistently
lower than that of the merging baseline, reducing up to 4 Ã—. We
can also observe that the memory usage of the merging baseline is
proportional to the userâ€™s full datastore size, whereas the memory
usages of the offline and online designs depend only on the size
limitğ¿on the datastore subset. We can further see that the peak
 
604Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
memory of the online design is higher than that of the offline design,
because several candidate datastore subsets need to be maintained.
Nevertheless, by equation 11, the peak memory of the online design
does not exceed 2Ã—of the offline designâ€™s memory.
Impact of Datastore Subset Size. We vary the limit on the
datastore subset size ğ¿from 5Ã—104to15Ã—104. From the perplexity
depicted in Figure 9, we can observe that the enhancement with
the offline or the online selected datastore subset significantly out-
performs the pure on-device LLM (i.e., ğ¿=0). We can also see
that asğ¿grows, the perplexity of the offline design consistently
decreases, and the PPL of the online design shows a step-wise de-
scent, since different ğ¿â€™s may return the same datastore subset. Of
course, a larger ğ¿will lead to a slower on-device LLM inference
speed, implying a trade-off between effectiveness and efficiency.
6 Related Work
6.1 On-Device LLM Inference
The LLMs on the mobile devices are good complements to the cloud-
based LLMs with the nice properties of local service, low response
latency, low cost, service customization, and good privacy. Tech
giants (e.g., Alibaba [ 45], Google [ 26], Microsoft [ 36]), smartphone
manufacturers (e.g., Apple [ 2], Xiaomi [ 7], vivo [ 14], Huawei [ 5]),
and smartphone chipmakers (e.g., MediaTek [ 6], Qualcomm [ 9])
are racing to launch light-weight LLMs and integrate them into
mobile operating systems and mobile applications. To deploy LLMs
on heterogeneous mobile devices, MLC-LLM [ 18] provided a high-
performance native deployment solution with machine learning
compilation technique. MNN [ 39] reduced the memory usage by
segmenting LLM and loading each segment from storage one at
a time for on-device inference. Flash-LLM [ 21] enabled on-device
LLM inference by storing the model parameters in large flash mem-
ory and bringing them on demand to small DRAM. At the algorithm
level, some existing work considered how to accelerate token gen-
eration. For example, speculative decoding [ 24,33,50] employed a
light-weight LLM for fast token generation and leveraged a more
powerful LLM for parallel verification. In addition, the sparse model
architecture of mixture of experts was adopted to selectively acti-
vate a few model blocks to reduce inference overhead [41, 51].
Parallel to the above work, we propose to enhance the token
generation quality of on-device LLM with the userâ€™s historical in-
teractions with the cloud-based LLM.
6.2 Device-Cloud Collaborative Learning
To integrate the natural advantages and the resources of both mobile
devices and the cloud, the new paradigm of device-cloud collabo-
rative learning emerges. The celebrated federated learning frame-
work [ 23,31,37,42,44] considered deploying the same model on
the cloud and each mobile device. The model size was limited by
the resource constraints of the mobile device. Later, some work
turned to the collaboration of a powerful model on the cloud and
light-weight models on the mobile devices. Specific to LLMs, model
multiplexing [ 56] invoked the cloud-based LLM only when the
query is not in the cache and the on-device LLM cannot provide
a satisfying answer, the purpose of which is to reduce the over-
head of cloud-based LLM inference. For collaborative training, one
line of work (e.g., CD-CCA [ 48]) proposed to let the cloud trainuser-specific LLMs, while the mobile devices are responsible for
collecting training data and performing inference. Such centralized
design is practically infeasible when the number of mobile devices
is large (e.g., billions in practice). In contrast, this work proposes to
leverage the userâ€™s interactions with the cloud as a data provider
and distribute the LLM enhancement task on each mobile device
without the need of local training. The other line of existing work
(e.g., [ 25,49]) proposed an on-device emulator-based finetuning
technique. These work focused on enhancing cloud-based LLMs
rather than on-device LLMs.
6.3 LLM Enhancement with External Datastore
As introduced in Section 1, RAG [ 27,34] andğ‘˜NN-LM [ 32,55]
are two typical methods for enhancing LLM inference with an ex-
ternal datastore. RAG has been widely used in natural language
tasks [ 38,40], and can effectively eliminate LLM hallucinations [ 46].
Compared with RAG, ğ‘˜NN-LM is memory efficient [ 10] and allows
longer user-side input sequence, which are important for on-device
LLM inference. One line of the following work of ğ‘˜NN-LM con-
sidered how to improve inference efficiency by compressing the
datastore in the area of machine translation. He et al . [28] pro-
posed the greedy merging method to prune the datastore, which
has been introduced as an offline baseline for comparison in our
evaluation, and leveraged principal component analysis (PCA) to
reduce the dimension of the keys in the datastore. Wang et al . [47]
proposed to train a mapping model to compress the dimension of
the representation vector and then perform cluster-based pruning.
Existing work on ğ‘˜NN-LM efficiency focused on compressing the
datastore on the cloud. In contrast, this work requires the datastore
subset selection be executed on the mobile device, imposing the
constraints on the memory usage and the time complexity. As a
result, the techniques of PCA and model training adopted in [ 28,47]
are no longer applicable. In contrast, the proposed designs require
only the estimation of marginal gain for key-value pairs selection.
7 Conclusion
In this work, we have proposed to construct a local high-quality
datastore from the userâ€™s historical interactions with the cloud-
based powerful LLM to effectively and efficiently enhance on-device
LLM inference. To balance the quality and the speed of token gener-
ation, we have proposed both offline and online designs to select a
subset of the full datastore with near-optimal performance guaran-
tees and low time complexities. Extensive evaluation results have
demonstrated the overall superiority of the proposed designs over
the pure on-device LLM, the enhancement with full datastore, the
random selection policy, and the merging method, from perplexity,
generation speed, memory usage, and selection time.
Acknowledgements
This work was supported in part by National Key R&D Program
of China (No. 2022ZD0119100), China NSF grant No. 62025204,
No. 62202296, and No. 62272293, Tencent WeChat Research Fund,
and SJTU-Huawei Explore X Gift Fund. The opinions, findings,
conclusions, and recommendations expressed in this paper are
those of the authors and do not necessarily reflect the views of the
funding agencies or the government.
 
605KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucheng Ding et al.
References
[1]2023. Alpaca Dataset. https://github.com/tatsu-lab/stanford_alpaca/blob/main/
alpaca_data.json.
[2]2023. Apple Language Model Research Report. https://www.theverge.com/2023/
9/6/23861763/apple-ai-language-models-ajax-gpt-training-spending.
[3]2023. Chinese Alpaca Dataset. https://github.com/LC1332/Chinese-alpaca-lora.
[4]2023. Google Gemini. https://cloud.google.com/blog/products/ai-machine-
learning/gemini-support-on-vertex-ai.
[5]2023. Huawei Noah. https://github.com/huawei-noah/Pretrained-Language-
Model.
[6]2023. MediaTech On-Device LLM Report. https://www.mediatek.com/blog/
mediatek-research-launches-the-worlds-first-ai-llm-in-traditional-chinese.
[7] 2023. MiLM. https://github.com/XiaoMi/MiLM-6B.
[8]2023. Personalized Interactive Conversations Dataset. https://huggingface.co/
datasets/erbacher/personalized-interactive-conversations.
[9]2023. Qualcomm On-Device LLM Report. https://www.qualcomm.com/
news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-
applications-usi.
[10] 2023. Retrieval-based Language Models and Applications. https://acl2023-
retrieval-lm.github.io/.
[11] 2023. ShareGPT Dataset. https://huggingface.co/datasets/shareAI/ShareGPT-
Chinese-English-90k.
[12] 2023. TinyLlama. https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.1.
[13] 2023. UC Berkeley Vicuna. https://github.com/eddieali/Vicuna-AI-LLM.
[14] 2023. vivo BlueLM. https://developers.vivo.com/product/ai/bluelm.
[15] 2024. Alibaba Qwen. https://github.com/QwenLM/Qwen.
[16] 2024. Baichuan2. https://github.com/baichuan-inc/Baichuan2.
[17] 2024. Meta Llama. https://github.com/facebookresearch/llama.
[18] 2024. MLC-LLM. https://llm.mlc.ai/.
[19] 2024. OpenAI ChatGPT. https://chat.openai.com/.
[20] 2024. Openompass LLM Leaderboard. https://rank.opencompass.org.cn/
leaderboard-llm.
[21] Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik
Cho, Carlo C. Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. 2023.
LLM in a flash: Efficient Large Language Model Inference with Limited Memory.
CoRR abs/2312.11514 (2023).
[22] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,
Kyle Oâ€™Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and
Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models
Across Training and Scaling. In ICML (Proceedings of Machine Learning Research,
Vol. 202). PMLR, 2397â€“2430.
[23] Dongqi Cai, Yaozong Wu, Shangguang Wang, Felix Xiao, zhu Lin, and Mengwei
Xu. 2023. Efficient Federated Learning for Modern NLP. In MobiCom. ACM,
Madrid, Spain, 14 pages.
[24] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Lau-
rent Sifre, and John Jumper. 2023. Accelerating Large Language Model Decoding
with Speculative Sampling. CoRR abs/2302.01318 (2023).
[25] Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tang, Chengfei Lyu, and Guihai
Chen. 2023. DC-CCL: Device-Cloud Collaborative Controlled Learning for Large
Vision Models. CoRR abs/2303.10361 (2023).
[26] Gemini Team Google. 2023. Gemini: A Family of Highly Capable Multimodal
Models. CoRR abs/2312.11805 (2023).
[27] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
2020. REALM: Retrieval-Augmented Language Model Pre-Training. CoRR
abs/2002.08909 (2020).
[28] Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021. Efficient Near-
est Neighbor Language Models. In EMNLP (1). Association for Computational
Linguistics, 5703â€“5714.
[29] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-Efficient Transfer Learning for NLP. In ICML (Proceedings of Machine
Learning Research, Vol. 97). PMLR, 2790â€“2799.
[30] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large
Language Models. In ICLR. OpenReview.net, Virtual, 13 pages.
[31] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Se-
bastian U. Stich, and Ananda Theertha Suresh. 2020. SCAFFOLD: Stochastic
Controlled Averaging for Federated Learning. In ICML, Vol. 119. PMLR, Virtual,
5132â€“5143.
[32] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike
Lewis. 2020. Generalization through Memorization: Nearest Neighbor Language
Models. In ICLR. OpenReview.net.
[33] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from
Transformers via Speculative Decoding. In ICML (Proceedings of Machine Learning
Research, Vol. 202). PMLR, 19274â€“19286.[34] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-
tÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Genera-
tion for Knowledge-Intensive NLP Tasks. In NeurIPS.
[35] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous
Prompts for Generation. In ACL/IJCNLP (1). Association for Computational Lin-
guistics, 4582â€“4597.
[36] Yuanzhi Li, SÃ©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar,
and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report.
CoRR abs/2309.05463 (2023).
[37] Youpeng Li, Xuyu Wang, and Lingling An. 2023. Hierarchical Clustering-based
Personalized Federated Learning for Robust and Fair Human Activity Recognition.
Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 7, 1 (2023), 20:1â€“20:38.
[38] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2021. Retrieval-
Augmented Generation for Code Summarization via Hybrid GNN. In ICLR. Open-
Review.net.
[39] Chengfei Lv, Chaoyue Niu, Renjie Gu, Xiaotang Jiang, Zhaode Wang, Bin Liu,
Ziqi Wu, Qiulin Yao, Congyu Huang, Panos Huang, Tao Huang, Hui Shu, Jinde
Song, Bin Zou, Peng Lan, Guohuan Xu, Fei Wu, Shaojie Tang, Fan Wu, and Guihai
Chen. 2022. Walle: An End-to-End, General-Purpose, and Large-Scale Production
System for Device-Cloud Collaborative Machine Learning. In OSDI. USENIX,
Carlsbad, CA, USA, 249â€“265.
[40] Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei
Han, and Weizhu Chen. 2021. Generation-Augmented Retrieval for Open-Domain
Question Answering. In ACL/IJCNLP (1). Association for Computational Linguis-
tics, 4089â€“4100.
[41] Saeed Masoudnia and Reza Ebrahimpour. 2014. Mixture of experts: a literature
survey. Artif. Intell. Rev. 42, 2 (2014), 275â€“293.
[42] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise AgÃ¼era y Arcas. 2017. Communication-Efficient Learning of Deep Net-
works from Decentralized Data. In AISTATS. JMLR, Fort Lauderdale, USA, 1273â€“
1282.
[43] Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Von-
drÃ¡k, and Andreas Krause. 2015. Lazier Than Lazy Greedy. In AAAI. AAAI Press,
1812â€“1818.
[44] Jaeyeon Park, Kichang Lee, Sungmin Lee, Mi Zhang, and JeongGil Ko. 2023.
AttFL: A Personalized Federated Learning Framework for Time-series Mobile
and Embedded Sensor Data Processing. Proc. ACM Interact. Mob. Wearable
Ubiquitous Technol. 7, 3 (2023), 116:1â€“116:31.
[45] Alibaba Group Qwen Team. 2023. Qwen Technical Report. CoRR abs/2309.16609
(2023).
[46] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku Rani,
Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A Comprehensive Sur-
vey of Hallucination Mitigation Techniques in Large Language Models. CoRR
abs/2401.01313 (2024).
[47] Dexin Wang, Kai Fan, Boxing Chen, and Deyi Xiong. 2022. Efficient Cluster-
Based $k$-Nearest-Neighbor Machine Translation. In ACL (1). Association for
Computational Linguistics, 2175â€“2187.
[48] Guanqun Wang, Jiaming Liu, Chenxuan Li, Junpeng Ma, Yuan Zhang, Xinyu Wei,
Kevin Zhang, Maurice Chong, Ray Zhang, Yijiang Liu, and Shanghang Zhang.
2023. Cloud-Device Collaborative Learning for Multimodal Large Language
Models. CoRR abs/2312.16279 (2023).
[49] Guangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-Tuning: Transfer Learning
without Full Model. CoRR abs/2302.04870 (2023).
[50] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and
Xuanzhe Liu. 2023. LLMCad: Fast and Scalable On-device Large Language Model
Inference. CoRR abs/2309.04255 (2023).
[51] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei
Xu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language
Models. CoRR abs/2308.14352 (2023).
[52] Xueying Zhan, Huan Liu, Qing Li, and Antoni B. Chan. 2021. A Comparative
Survey: Benchmarking for Pool-based Active Learning. In IJCAI. ijcai.org, 4679â€“
4686.
[53] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama:
An Open-Source Small Language Model. CoRR abs/2401.02385 (2024).
[54] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained
Transformer Language Models. CoRR abs/2205.01068 (2022).
[55] Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training Language Models with
Memory Augmentation. In EMNLP. Association for Computational Linguistics,
5657â€“5673.
[56] Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark W. Barrett, Michael I. Jordan,
and Jiantao Jiao. 2023. On Optimal Caching and Model Multiplexing for Large
Model Inference. CoRR abs/2306.02003 (2023).
 
606Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A Analysis and Proofs
A.1 Analysis of Algorithm 1
First, we show the feasibility of updating Î”(ğ‘Ÿ|ğ‘†)after batch selec-
tion by proving that the probability of key-value pairs with the
same label appearing among ğ‘…ğ‘¡is very low. More specifically, we
assume that the values (target tokens) in ğ‘…ğ‘¡follow a distribution
ğ‘‘ğ‘¡, withğ‘ğ‘‘ğ‘¡(ğ‘£=ğ‘¦)â‰¤ğ‘0for anyğ‘¦, whereğ‘0is very small and at
the magnitude of1
Size of Token Space. Then the probability that the
values of different key-value pairs in ğ‘…ğ‘¡are all different satisfies
Pr(âˆ€(ğ‘”(ğ‘¥ğ‘–),ğ‘¦ğ‘–),(ğ‘”(ğ‘¥ğ‘—),ğ‘¦ğ‘—)âˆˆğ‘…ğ‘¡(ğ‘–â‰ ğ‘—),ğ‘¦ğ‘–â‰ ğ‘¦ğ‘—)
â‰¥ğ‘Ã–
ğ‘–=1(1âˆ’ğ‘0(ğ‘–âˆ’1))â‰¥( 1âˆ’ğ‘0(ğ‘âˆ’1))ğ‘/2â‰¥ğ‘’âˆ’ğ‘0ğ‘2/2
1âˆ’ğ‘0ğ‘(12)
Letğ›¾â–³=ğ‘0ğ‘2/2
1âˆ’ğ‘0ğ‘, andğ›¾could be far less than 1 with appropriate
setting ofğ‘. Specifically, for any 0<Ë†ğ›¾<1, we have
ğ›¾â‰¤Ë†ğ›¾ğ‘¤â„ğ‘’ğ‘› 0<ğ‘â‰¤min(
1
ğ‘0,âˆšï¸„
Ë†ğ›¾2+2Ë†ğ›¾
ğ‘0âˆ’Ë†ğ›¾)
. (13)
With an appropriate ğ‘we can have ğ›¾â‰ª1, and further have
Pr(âˆ€(ğ‘”(ğ‘¥ğ‘–),ğ‘¦ğ‘–),(ğ‘”(ğ‘¥ğ‘—),ğ‘¦ğ‘—)âˆˆğ‘…ğ‘¡(ğ‘–â‰ ğ‘—),ğ‘¦ğ‘–â‰ ğ‘¦ğ‘—)â‰¥ğ‘’âˆ’ğ›¾â‰¥(1âˆ’ğ›¾)
(14)
By equation 14, we show that it is likely for the key-value pairs in
ğ‘…ğ‘¡to have different values, thus the updating the marginal gain after
batch selection can significantly improve efficiency while generally
keeping the performance. In practice, as ğ‘0is extremely small, the
appropriate range of setting ğ‘is actually wide for a small ğ›¾. In
addition, although in our experiments we choose some very large
ğ‘that may exceed the theoretical range for efficiency, Algorithm 1
still maintains good performance.
We next analyze the approximation ratio of Algorithm 1. Let ğ‘†ğ‘¡
denote the selected datastore subset before round ğ‘¡. By equation 4,
when all the key-value pairs in ğ‘…ğ‘¡have distinct values, we have
Î”(ğ‘…ğ‘¡|ğ‘†ğ‘¡)=âˆ‘ï¸
ğ‘Ÿâˆˆğ‘…ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡) (15)
Based on equations 14 and 15, we make the following assumption
for the brevity of the analysis.
Assumption 1. For anyğ‘†ğ‘¡andğ‘…ğ‘¡at stepğ‘¡in Algorithm 1, we
haveÎ”(ğ‘…ğ‘¡|ğ‘†ğ‘¡)â‰¥( 1âˆ’ğ›¾)Ã
ğ‘Ÿâˆˆğ‘…ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡).
We further make an assumption about computing Î”(ğ‘Ÿ|ğ‘†ğ‘¡).
Assumption 2. For anyğ‘†ğ‘¡at stepğ‘¡, we can obtain Î”(ğ‘Ÿ|ğ‘†)(by
exactly computing it or estimating it by its lower bound).
Under the assumptions, we bound the approximation ratio.
Theorem 2. Under Assumptions 1 and 2, we can have a near-
optimal datastore subset in expectation
E[ğ‘“(ğ‘†)]â‰¥( 1âˆ’1
ğ‘’âˆ’ğ›¾âˆ’ğœ–)ğ‘“(ğ‘†âˆ—),
whereğœ–=ğ‘’âˆ’ğµğ¿
ğ‘ğ‘andğ‘†âˆ—denotes the optimal subset with |ğ‘†âˆ—|â‰¤ğ¿.
Proof. We first introduce Lemma 1:Lemma 1. [43] Given the current datastore subset ğ‘†, the expected
gain of selecting the key-value pair with the highest marginal gain
amongğ‘šrandom pairs is at least1âˆ’ğ‘’âˆ’ğ‘šğ¿
ğ‘
ğ¿Ã
ğ‘’âˆˆğ‘†âˆ—\ğ‘†Î”(ğ‘’|ğ‘†).
Proof of Lemma 1. Following [ 43], we examine the probability
thatğ‘…âˆ©(ğ‘†âˆ—\ğ‘†)and have
Pr(ğ‘…âˆ©(ğ‘†âˆ—\ğ‘†)=âˆ…)=
1âˆ’|ğ‘†âˆ—\ğ‘†|
|ğ‘‰\ğ‘†|ğ‘š
â‰¤ğ‘’âˆ’ğ‘š|ğ‘†âˆ—\ğ‘†|
|ğ‘‰\ğ‘†|
â‰¤ğ‘’âˆ’ğ‘š|ğ‘†âˆ—\ğ‘†|
ğ‘.(16)
By the fact that 0â‰¤|ğ‘†âˆ—\ğ‘†|â‰¤ğ¿, we have
Pr(ğ‘…âˆ©(ğ‘†âˆ—\ğ‘†)â‰ âˆ…)â‰¥ 1âˆ’ğ‘’âˆ’ğ‘š|ğ‘†âˆ—\ğ‘†|
ğ‘â‰¥1âˆ’ğ‘’âˆ’ğ‘šğ¿
ğ‘
ğ¿|ğ‘†âˆ—\ğ‘†|.(17)
Whenğ‘…âˆ©(ğ‘†âˆ—\ğ‘†)â‰ âˆ…, letğ‘…denote the sampled pairs and ğ‘Ÿâˆ—
ğ‘…denote
the element with the highest marginal gain among ğ‘…. Overall,ğ‘…is
equally likely to contain each element in ğ‘†âˆ—\ğ‘†, so we have
E[Î”(ğ‘Ÿ|ğ‘†)]â‰¥ Pr(ğ‘…âˆ©(ğ‘†âˆ—\ğ‘†)â‰ âˆ…)Ã
ğ‘Ÿâˆˆğ‘†âˆ—\ğ‘†Î”(ğ‘Ÿ|ğ‘†)
|ğ‘†âˆ—\ğ‘†|
â‰¥1âˆ’ğ‘’âˆ’ğ‘šğ¿
ğ‘
ğ¿âˆ‘ï¸
ğ‘Ÿâˆˆğ‘†âˆ—\ğ‘†Î”(ğ‘Ÿ|ğ‘†).(18)
â–¡
We further view that the process of sampling ğµkey-value pairs
in Algorithm 1 as repeatedly sampling ğµ/ğ‘key-value pairs by ğ‘
times, where the sampling sets are recorded as ğ‘…1,ğ‘…2,Â·Â·Â·,ğ‘…ğ‘(we
have neglected the small differences between the two, which is
practically insignificant when ğµâ‰ªğ‘, while considering such
differences would make the analysis extremely tedious), and by the
topğ‘selection operation, we have
âˆ‘ï¸
ğ‘Ÿâˆˆğ‘…ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡)â‰¥ğ‘âˆ‘ï¸
ğ‘–=1max
ğ‘Ÿâˆˆğ‘…ğ‘–Î”(ğ‘Ÿ|ğ‘†ğ‘¡). (19)
By equation 19 and Lemma 1, we further have
Eï£®ï£¯ï£¯ï£¯ï£¯ï£°âˆ‘ï¸
ğ‘Ÿâˆˆğ‘…ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡)ï£¹ï£ºï£ºï£ºï£ºï£»â‰¥ğ‘(1âˆ’ğ‘’âˆ’ğµğ¿
ğ‘ğ‘)
ğ¿âˆ‘ï¸
ğ‘Ÿâˆˆğ‘†âˆ—\ğ‘†ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡). (20)
By Assumption 1 and equation 20, we have
E[Î”(ğ‘…ğ‘¡|ğ‘†ğ‘¡)]â‰¥ğ‘(1âˆ’ğ›¾)(1âˆ’ğ‘’âˆ’ğµğ¿
ğ‘ğ‘)
ğ¿âˆ‘ï¸
ğ‘Ÿâˆˆğ‘†âˆ—\ğ‘†ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡).(21)
Whenğµ/ğ‘=ğ‘
ğ¿log1
ğœ–, we have
E[Î”(ğ‘…ğ‘¡|ğ‘†ğ‘¡)]â‰¥ğ‘(1âˆ’ğ›¾)(1âˆ’ğœ–)
ğ¿âˆ‘ï¸
ğ‘Ÿâˆˆğ‘†âˆ—\ğ‘†ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡). (22)
By the submodularity of ğ‘“(ğ‘†), we have
âˆ‘ï¸
ğ‘Ÿâˆˆğ‘†âˆ—\ğ‘†ğ‘¡Î”(ğ‘Ÿ|ğ‘†ğ‘¡)â‰¥ğ‘“(ğ‘†âˆ—)âˆ’ğ‘“(ğ‘†ğ‘¡). (23)
 
607KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yucheng Ding et al.
Therefore,
E[ğ‘“(ğ‘†ğ‘¡+1)âˆ’ğ‘“(ğ‘†ğ‘¡)|ğ‘†ğ‘¡]=E[Î”(ğ‘…ğ‘¡|ğ‘†ğ‘¡)]
â‰¥ğ‘(1âˆ’ğ›¾)(1âˆ’ğœ–)
ğ¿ ğ‘“(ğ‘†âˆ—)âˆ’ğ‘“(ğ‘†ğ‘¡).(24)
By taking expectation, we have
E[ğ‘“(ğ‘†ğ‘¡+1)âˆ’ğ‘“(ğ‘†ğ‘¡)]â‰¥ğ‘(1âˆ’ğ›¾)(1âˆ’ğœ–)
ğ¿E[ğ‘“(ğ‘†âˆ—)âˆ’ğ‘“(ğ‘†ğ‘¡)].(25)
By induction and ğ¿=ğ‘ğ‘‡, we have
E[ğ‘“(ğ‘†ğ‘‡)]â‰¥ 
1âˆ’
1âˆ’ğ‘(1âˆ’ğ›¾)(1âˆ’ğœ–)
ğ¿ğ‘‡!
ğ‘“(ğ‘†âˆ—)
â‰¥
1âˆ’ğ‘’âˆ’(1âˆ’ğ›¾)(1âˆ’ğœ–)
ğ‘“(ğ‘†âˆ—)
(ğ‘)
â‰¥
1âˆ’1
ğ‘’âˆ’ğ›¾âˆ’ğœ–
ğ‘“(ğ‘†âˆ—),(26)
where(ğ‘)follows from 1âˆ’ğ‘’ğ‘¥âˆ’1â‰¥1âˆ’1/ğ‘’âˆ’ğ‘¥for any 0â‰¤ğ‘¥â‰¤1.â–¡
A.2 Analysis of Algorithm 2
We first analyze the performance of Algorithm 2. We consider the
collection of key-value pairs as a stochastic process. Therefore,
the collection of ğµkey-value pairs by the buffer can be viewed
as randomly sampling ğµpairs. Thus, the selection of ğ‘†ğ‘—in Algo-
rithm 2 can be seen as repeating randomly sampling ğµpairs and
choosing the top ğ‘ğ‘—with the highest marginal gain for ğ‘/ğµtimes.
This is equivalent to Algorithm 1 to select the near-optimal sub-
set withğ¿ğ‘—:=ğ‘ğ‘—ğ‘/ğµkey-value pairs. In Theorem 1, we have
demonstrated the effectiveness of Algorithm 1. Besides, we let
ğ‘†âˆ—
ğ‘ ğ‘–ğ‘§ğ‘’â–³=max|ğ‘†|â‰¤ğ‘ ğ‘–ğ‘§ğ‘’ğ‘“(ğ‘†)and introduce the following theorem for
further analysis.
Theorem 3. For a non-negative, non-decreasing, and submodular
objective function ğ‘“(ğ‘†)and the full set ğ‘ˆ, when(ğ¿1|ğ¿2)we have
ğ‘“(ğ‘†âˆ—
ğ¿1)â‰¥ğ¿1
ğ¿2ğ‘“(ğ‘†âˆ—
ğ¿2), (27)
Proof. We divideğ‘†âˆ—
ğ¿2intoğ¿2/ğ¿1mutually exclusive subsets,
denoted asğ‘†âˆ—
ğ¿2(1),ğ‘†âˆ—
ğ¿2(2),Â·Â·Â·,ğ‘†âˆ—
ğ¿2(ğ¿2/ğ¿1).
By the definition of ğ‘†âˆ—
ğ¿1, we have
ğ‘“(ğ‘†âˆ—
ğ¿1)â‰¥ğ‘“(ğ‘†âˆ—
ğ¿2(ğ‘–)),âˆ€ğ‘–âˆˆ{1,2,Â·Â·Â·,ğ¿2/ğ¿1}. (28)
Besides, by the submodularity of ğ‘“(ğ‘†), we have
ğ¿2/ğ¿1âˆ‘ï¸
ğ‘–=1ğ‘“(ğ‘†âˆ—
ğ¿2(ğ‘–))â‰¥ğ‘“(ğ‘†âˆ—
ğ¿2). (29)
Therefore, by equations 28 and 29, we have
ğ¿2
ğ¿1ğ‘“(ğ‘†âˆ—
ğ¿1)â‰¥ğ‘“(ğ‘†âˆ—
ğ¿2), (30)
which corresponds to Theorem 3. â–¡
In Algorithm 2, by Theorem 3 and ğ‘“(ğ‘†)â€™s submodularity,
ğ‘“(ğ‘†ğ‘šğ‘ğ‘¥)â‰ˆğ‘“(ğ‘†âˆ—
|ğ‘†ğ‘šğ‘ğ‘¥|)â‰¥ğ‘“(ğ‘†âˆ—
ğ‘|ğ‘†ğ‘šğ‘ğ‘¥|)
ğ‘(âˆ—)
â‰¥ğ‘“(ğ‘†âˆ—
ğ¿)
ğ‘(31)
where(âˆ—)follows from|ğ‘†ğ‘šğ‘ğ‘¥|â‰¥|ğ‘†ğ‘šğ‘ğ‘¥+1|/ğ‘â‰¥ğ¿/ğ‘whenğ‘>ğ¿.
Therefore, Algorithm 2 can return a datastore subset with good
performance compared to ğ‘“(ğ‘†âˆ—
ğ¿)=ğ‘“(ğ‘†âˆ—).Table 3: Perplexity of the proposed design with varying dis-
tribution differences between training and test data.
T
est Set Pure Random Full Offline Design Online Design
Domain
B 5.14 4.72 4.39 4.54 4.53
Domain B&C Mixed 4.51 4.29 4.19 4.22 4.22
B Supplementary Experimental Notes
B.1 Implementation Details
In this section, we introduce some details in our experimental im-
plementation of the selection algorithms and ğ‘˜NN-LM. In the im-
plementation of Algorithm 2, we stipulate that if the size of ğ‘†ğ‘šğ‘ğ‘¥
is greater than 0.6ğ¿, then return ğ‘†ğ‘šğ‘ğ‘¥; otherwise, return ğ‘†ğ‘šğ‘ğ‘¥+1.
For the distance function ğ‘‘(ğ‘˜ğ‘’ğ‘¦,ğ‘”(Ë†ğ‘¤1:ğ‘š))inğ‘˜NN-LM, we adopt
scaled Euclidean distance. Formally,
ğ‘‘(ğ‘˜ğ‘’ğ‘¦,ğ‘”(Ë†ğ‘¤1:ğ‘š))=||ğ‘˜ğ‘’ğ‘¦âˆ’ğ‘”(Ë†ğ‘¤1:ğ‘š)||2
ğœƒ, (32)
whereğœƒis set to 104,500,500, and 104/6for Qwen-1.8B, TinyLlama-
1.1B, OPT-1.3B, and Pythia-1.1B, respectively. Such a small modifi-
cation has no impact on our analysis.
For the datastore subset returned by the selection algorithms, the
kept key-value pairs are typically with small LLM probability, i.e.,
ğ‘DLM(ğ‘¦|ğ‘¥)is small for each(ğ‘”(ğ‘¥),ğ‘¦)in the subset. This is because
Algorithms 1 and 2 generally selects key-value pairs that are not
learned well by the on-device LLM. Therefore, ğ‘˜NN-LM with the
subset may not help with predicting the next token for contexts
where the on-device LLM can already make accurate predictions.
With this observation, we design an adaptive ğ‘ğ‘˜NNweight given an
input context. Specifically, let Ë†ğ‘¤1:ğ‘šdenote the input context, and
the predicted next token distribution is
ğ‘(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š)=(1âˆ’ğ›¼(1âˆ’ğ›½Â·MaxProb))ğ‘DLM(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š)
+ğ›¼(1âˆ’ğ›½Â·MaxProb)ğ‘ğ‘˜NN(Ë†ğ‘¤ğ‘š+1|Ë†ğ‘¤1:ğ‘š),
(33)
where MaxProb: =maxğ‘¦âˆˆToken Spaceğ‘DLM(ğ‘¦|Ë†ğ‘¤1:ğ‘š).ğ›½is set to 1 as
default, set to 0.5 for experiments with TinyLlama-1.1B, and set to 0
for experiments over ShareGPT-Agriculture, ShareGPT-Intelligence,
and ShareGPT-Legal. We do not adopt the design for other baselines
(i.e.,ğ›½=0) as the key-value pairs kept in their subset is not with
smallğ‘DLM(ğ‘¦|ğ‘¥)and such a design even increases their PPL.
B.2 Additional Evaluation Results
In practice, a userâ€™s interest may change over time, causing the
discrepancy between the training set and the test set. Therefore,
we add experiments when there is a difference in topics between
the training set and the test set. More specifically, the training set
is composed of 150 conversations from ShareGPT-Agriculture (do-
main A) and 150 conversations from ShareGPT-Intelligence (domain
B), and the full test set is composed of another 100 conversations
from ShareGPT-Intelligence (domain B) and 100 conversations from
ShareGPT-Marketing (domain C). We evaluate the perplexity of the
proposed methods, where ğ›½(in equation 33) is set to 1 as default,
over the test data with domain B only and domains B & C mixed.
The evaluation results are shown in Table 3. We can observe that
the proposed methods can still enhance on-device LLM for varying
distribution differences between training and test data.
 
608