Optimizing OOD Detection in Molecular Graphs: A Novel
Approach with Diffusion Models
Xu Shenâˆ—
Jilin University
Changchun, China
shenxu23@mails.jlu.edu.cnYili Wangâˆ—
Jilin University
Changchun, China
wangyl21@mails.jlu.edu.cnKaixiong Zhou
Massachusetts Institute of Technology
Cambridge, USA
kz34@mit.edu
Shirui Pan
Griffith University
Goldcoast, Australia
s.pan@griffith.edu.auXin Wangâ€ 
Jilin University
Changchun, China
xinwang@jlu.edu.cn
ABSTRACT
Despite the recent progress of molecular representation learning, its
effectiveness is assumed on the close-world assumptions that train-
ing and testing graphs are from identical distribution. The open-
world test dataset is often mixed with out-of-distribution (OOD)
samples, where the deployed models will struggle to make accurate
predictions. The misleading estimations of moleculesâ€™ properties
in drug screening or design can result in the tremendous waste
of wet-lab resources and delay the discovery of novel therapies.
Traditional detection methods need to trade off OOD detection and
in-distribution (ID) classification performance since they share the
same representation learning model. In this work, we propose to de-
tect OOD molecules by adopting an auxiliary diffusion model-based
framework, which compares similarities between input molecules
and reconstructed graphs. Due to the generative bias towards re-
constructing ID training samples, the similarity scores of OOD
molecules will be much lower to facilitate detection. Although it is
conceptually simple, extending this vanilla framework to practical
detection applications is still limited by two significant challenges.
First, the popular similarity metrics based on Euclidian distance
fail to consider the complex graph structure. Second, the genera-
tive model involving iterative denoising steps is notoriously time-
consuming especially when it runs on the enormous pool of drugs.
To address these challenges, our research pioneers an approach of
Prototypical Graph Reconstruction for Molecular OOd Detection,
dubbed as PGR-MOOD. Specifically, PGR-MOOD hinges on three
innovations: i) An effective metric to comprehensively quantify the
matching degree of input and reconstructed molecules according
to their discrete edges and continuous node features; ii) A creative
graph generator to construct a list of prototypical graphs that are in
âˆ—Both authors contributed equally to this research.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671785line with ID distribution but away from OOD one; iii) An efficient
and scalable OOD detector to compare the similarity between test
samples and pre-constructed prototypical graphs and omit the gen-
erative process on every new molecule. Extensive experiments on
ten benchmark datasets and six baselines are conducted to demon-
strate our superiority: PGR-MOOD achieves more than 8%of aver-
age improvement in terms of detection AUC and AUPR accompa-
nied by the reduced cost of testing time and memory consumption.
The anonymous code is in: https://github.com/se7esx/PGR-MOOD.
CCS CONCEPTS
â€¢Mathematics of computing â†’Graph algorithms; â€¢Comput-
ing methodologies â†’Machine learning; â€¢Applied computing
â†’Chemoinformatics ;â€¢Information systems â†’Data mining .
KEYWORDS
Molecular graphs, out-of-distribution detection, diffusion models
ACM Reference Format:
Xu Shen , Yili Wang , Kaixiong Zhou , Shirui Pan , and Xin Wang .
2024. Optimizing OOD Detection in Molecular Graphs: A Novel Approach
with Diffusion Models. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671785
1 INTRODUCTION
Molecular representation learning, which transforms molecules
into low-dimensional vectors, has emerged as a critical and es-
sential part of many biochemical problems, such as drug property
prediction [ 15,46] and drug design [ 23]. For handling the non-
Euclidean molecules, graph neural networks (GNNs) have been
widely applied to encode both node features and structural infor-
mation based on message-passing strategy [ 7,30,47,57,58]. The
embedding vectors of atoms and/or edges are then summarized to
represent the underlying molecules and adopted to various down-
stream tasks [2, 12, 19, 32, 44, 51, 52].
The recent successes of molecular representation learning are
often built on the assumption that training and testing graphs are
from identical distribution. However, out-of-distribution (OOD)
molecular graphs with different scaffolds or sizes, as shown in
Fig. 1a, is unavoidable when the model is deployed in real-world
 
2640
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, & Xin Wang.
(a) ID and OOD molecules
0 50 100 150 200 250 300
Iters0.00.20.40.60.8Accuracy/LossTrain loss
OOD test auroc
ID test auroc (b) Loss and auroc
Figure 1: (a) Illustration of OOD and ID molecules, which
have different scaffolds or sizes, or both. (b) Vanilla GCNâ€™s
performance declines rapidly when testing on OOD graphs,
even though it performs well on ID graphs.
scenarios [ 17]. Taking antibiotics screening as example, the train-
ing data consists of drugs inhibiting the growth of Gram-negative
pathogens, while the testing data is mixed with antibiotics against
Gram-positive ones [ 26]. Because of the different pharmacological
mechanisms in treating bacteria, a reliable drug screening model
should not only accurately identify more the in-distribution (ID)
samples (e.g., Gram-negative), but also detect â€œunknownâ€ OOD
inputs (e.g., Gram-positive) to avoid misleading predictions during
inference. As illustrated in Fig. 1b, a notable decline in GNNsâ€™ pre-
diction accuracy is observed with OOD samples. This highlights the
significance of OOD detection, which discerns between ID and OOD
inputs, allowing the model to adopt appropriate precautions [14].
Prior arts of graph OOD detection can be roughly grouped into
two categories. One line of the existing work aims to leverage the
original classifier and fine-tune it to improve its detection abil-
ity [24,28]. The another line is to redesign the scoring function
to indicate ID and OOD cases [ 11,50]. Nevertheless, these meth-
ods inevitably require modifications to the original molecular rep-
resentation learning model, leading to a trade-off between OOD
detection and ID prediction [ 6]. Recent advancements in computer
vision have proposed the use of a diffusion model-based recon-
struction approach for the unsupervised OOD detection, which
typically involves an auxiliary generative model that approximates
the ID distribution to reconstruct the input samples during testing
phase [ 6,8,29]. Since the distribution of reconstructed samples is
more biased towards ID than OOD, the disparity between original
inputs and reconstructed outputs can be used as a judge metric for
OOD detection. However, this kind of approach has never been
practiced in the field of molecular graphs.
We first design a naive model called GR-MOOD as shown in Fig.
2, to verify the feasibility of the reconstruction method for molec-
ular OOD detection and draw a positive conclusion through ex-
periments. However, the inherent complexity of molecular graphs,
which are characterized by non-Euclidean structures, poses two
significant challenges. First, this nature of molecular graphs renders
conventional similarity metrics (e.g., Euclidean distance) less effec-
tive to quantify the closeness between original and reconstructed
graphs. Meanwhile, the different molecules often undergo distribu-
tion shifts that include both structural and feature changes, further
complicating the assessment of similarity. This leads to Challenge 1:
Identifying an effective metric to evaluate the similarity between the
Figure 2: Illustration of reconstruction-based OOD detection
with the diffusion model. ID and OOD share different simi-
larities with their respective reconstruction graphs and can
be used as a score for OOD detection.
original input and the reconstruction. More importantly, the diffu-
sion models require hundreds or thousands of sampling steps to de-
noise from a normal standard distribution towards generating new
graphs, which introduces additional complexity. Such extensive re-
quirement becomes impractical, especially when performing recon-
structing for a large volume of test samples. This leads to Challenge
2: Addressing the additional complexity of diffusion model required
for reconstruction. Thus we propose a critical research question:
How can we adopt reconstruction method to effectively and efficiently
handle the unique properties of molecular graphs for OOD detection?
In this paper, we introduce a groundbreaking OOD detection
model, Prototypical Graph Reconstruction for Molecular OOd
Detection (PGR-MOOD for short). For Challenge 1, concerning the
identification of an effective metric for assessing the similarity be-
tween the original input and its reconstruction, PGR-MOOD adopts
Fused Gromov-Wasserstein (FGW) distance [ 40], which utilizes
both the structural and feature information of molecular graphs to
enhance the measurement of their matching degree. To efficiently
address Challenge 2, PGR-MOOD proposes to create a series of
prototypical graphs that are closer to ID samples and away from
OOD ones. We reduce the need of reconstructing every test graph
and just compare its similarities with the prepared prototypical
graphs. With this procedure, we can extend to the large-scale OOD
detection. Our contributions are summarized as follows:
â€¢GR-MOOD Framework: We propose to detect OOD graphs
from a novel perspective, i.e., via comparing the original
molecules with their reconstructed outputs based on the
diffusion model. The technical feasibility and challenges are
analyzed empirically for this new framework.
â€¢PGR-MOOD Framework: To overcome the challenges of
reconstruction measurement and generation efficiency, we
propose a molecular detection method that contains a pro-
totypical graphs generator and a similarity function based
on FGW distance. In the testing phase, one only needs to
measure the similarity between the prototypical graphs and
the current inputs to identify OOD with lower values.
 
2641Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢SOTA Experimental Results: We conduct extensive anal-
ysis on ten benchmark molecule datasets and compare with
six baselines. PGR-MOOD obtains the consistent superiority
over other state-of-the-art models, delivering the average
improvements of AUC and AUPR by 8.54%and8.15%, 13.7%
reduction on FPR95, and substantial savings in time and
memory consumption.
2 RELATED WORK
2.1 Graph Neural Networks
Since graph neural networks can use the topological structure and
node properties of graphs for representation learning, they have
become the most powerful method for processing graph data [ 1,5,
10,53,55,56,59], especially molecular graphs [ 45,48]. GCN [ 20],
the simplest but most efficient method, has been proved to be equiv-
alent to the first-order approximation filter on graphs [ 13] and
thus performs well in node classification [ 12,38,54,60] and link
prediction [ 2]. On graph instance-related tasks, GIN [ 51] proves
that GNN is as powerful as the 1-WL test and leverages an injective
summation operation to increase performance. More and more re-
searchers have proposed more representational methods, but they
all ignore the performance and trustworthiness issues brought by
OOD distribution [43, 49].
2.2 Graph Generative Models
Graph generative models aim to learn the distribution of the graph
data and sample from it to generate novel graphs [ 61], especially for
molecular graphs since it is related to many science issues [ 16,22,
36]. Some graph generation methods are inspired by auto-regressive
models, such as VAE-based [ 33] or normalizing flow-based mod-
els [21]. However, they are limited by the high computational cost
and inability to model permutation invariance of graph [ 18]. In-
spired by the diffusion models in computer vision [ 39], the same
insight on graphs has developed in recent years [ 3,34,41]. Although
diffusion models achieve state-of-the-art performance, they still
suffer from inefficiencies caused by slow denoising processes [ 25].
2.3 OOD Detection on Graphs
Recently, many studies focus on graph OOD detection due to its
importance. GOOD-D is the pioneering work for unsupervised
OOD graph detection, which performs hierarchical contrastive
learning to capture latent ID patterns and detects OOD graphs
based on their semantic inconsistency [ 28]. GraphDE determines
ID and OOD by inferring the environment variables of the graph
generation process [ 24]. AAGOD aims to learn a parameterized
amplifier matrix to emphasize the key patterns which helpful for
graph OOD detection, thereby enlarging the gap between OOD
and ID graphs [ 11]. Anomaly graph detection can also be seen as a
special case of OOD detection, since anomaly graphs with anomaly
structures and features can be caused by distribution shifts and
many methods have been proposed to solve it [ 31,35]. All of the
above methods require redesigning or training well-performing
GNNs on the ID datasets and inevitably lead to a trade-off between
OOD detection and ID prediction.3 PRELIMINARIES
We define an undirected graph ğº=(ğ´,ğ‘‹)withğ‘›nodes, where
ğ´âˆˆRğ‘›Ã—ğ‘›is adjacency matrix to represent the graph topology,
ğ‘‹âˆˆRğ‘›Ã—ğ‘‘is feature matrix of all nodes with the dimensionality of
ğ‘‘.ğºcan also be re-written by Optimal transmission (OT) format [ 42]
to represent as a tuple (ğ´,ğ‘‹,ğœ‡), whereğœ‡âˆˆRğ‘›is a vector of weights
modeling the relative importance of the nodes and we define it as a
uniform weight(1ğ‘›/ğ‘›). In addition, we define ğ·train as the training
dataset that usually consists of ID graphs, and define ğ·testas the
test dataset, which can be divided into in-distribution subset ğ·in
test
and out of distribution subset ğ·out
test.
3.1 Out of Distribution Detection
For OOD detection task, we aim to design a detector ğ‘”to distinguish
whether the input graph ğºis an OOD sample or not:
ğ‘”(ğº;ğœ,ğ½)=0(OOD),ifğ½(ğº)â‰¤ğœ,
1(ID), ifğ½(ğº)>ğœ.(1)
whereğ½denotes a judging function to score the input molecules
andğœdenotes threshold for identifying the OOD samples. A desired
OOD detector should assign judge scores with the maximum gap
between ID and OOD samples. This target can be described as the
following optimization:
max
ğ½Eğºâˆ¼ğ·in
testğ½(ğº)âˆ’Eğºâˆ¼ğ·out
testğ½(ğº). (2)
Supposing the judge score distributions of ID and OOD have signif-
icant divergence, we can distinguish them with a simple intermedi-
ate threshold. For reconstruction-based OOD detection as shown
in Fig. 2, the similarity between the input and the output molecules
of diffusion model FMis often adopted as the judge function:
ğ½(ğº)=sim(FM(ğº),ğº), (3)
where FM(ğº)is the reconstructed output and sim(Â·)is the similar-
ity function. OOD inputs correspond to the lower reconstruction
quality and therefore the lower similarity, while the similarity mea-
surement is higher for the ID inputs.
3.2 Graph Neural Networks
The typical GNNs are based on message passing paradigm. Specifi-
cally, the final representation of graph ğºfor ağ¿-layer GNNs is:
ğ‘š(ğ¿)
ğ‘£=MP
ğ‘š(ğ¿âˆ’1)
ğ‘£,{(ğ‘š(ğ¿âˆ’1)
ğ‘¢),ğ‘¢âˆˆğ‘(ğ‘£)}
, (4)
ğ‘§ğº=Poolingn
ğ‘š(ğ¿)
ğ‘£|ğ‘£âˆˆğºo
, (5)
whereğ‘š(0)
ğ‘£=ğ‘‹ğ‘£is raw node feature, ğ‘(ğ‘£)represents a set of
neighbor nodes with respect to node ğ‘£, and MPis the message
passing process that aggregates neighborhood features (e.g., sum,
mean, or max) and combines them with the local node. GNNs
iteratively perform MPto learn the effective node representations
and utilize function Pooling to map all the node representations
into the graph representations, which is a single vector.
3.3 Graph Generative Model
The generative method based on the diffusion model consists of a
forward diffusion process and a reverse denoising process. At the
forward process, the model progressively adds noise to the original
 
2642KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, & Xin Wang.
AUROC
AUPR
FPR95
Metric020406080ScoreMSP
GOOD-D
GR-GOOD
AUROC
AUPR
FPR95
Metric020406080ScoreMSP
GOOD-D
GR-GOOD
Figure 3: Validation experiments performed in DrugOOD-
IC50-Scaffold (left) and DrugOOD-EC50-Assay (right).
data until a standard normal distribution. At the reverse process, the
model learns the score function (i.e., a neural network) to remove
the perturbed noise with the same amount of steps [4, 27, 39].
Given a graph ğº=(ğ´,ğ‘‹), we can use continuous time ğ‘¡âˆˆ[0,ğ‘‡]
to index the diffusion trajectory {ğºğ‘¡=(ğ´ğ‘¡,ğ‘‹ğ‘¡)}ğ‘‡
ğ‘¡=1, such thatğº0is
the original input graph and ğºğ‘‡approximately follows the normal
distribution. The forward process transforms ğº0toğºğ‘‡through a
stochastic differential equation (SDE):
dğºğ‘¡=fğ‘¡(ğºğ‘¡)dğ‘¡+ğ‘”(ğ‘¡)dw, (6)
where wis standard Wiener process [ 18],fğ‘¡(Â·):Gâ†’G is linear
drift coefficient, ğ‘”(ğ‘¡):Râ†’Ris a scalar function which represents
the diffusion coefficient. fğ‘¡(ğºğ‘¡)andğ‘”(ğ‘¡)relate to the amount of
noise dwadded to the graph at each infinitesimal step dğ‘¡. In order
to generate graphs that follow the distribution of ğº0, we start from
ğºğ‘‡and utilize a reverse-time SDE for denoising from ğ‘‡to0:
dğºğ‘¡=
fğ‘¡(ğºğ‘¡)âˆ’ğ‘”(ğ‘¡)2Sğœƒ(ğºğ‘¡,ğ‘¡)
dğ‘¡+ğ‘”(ğ‘¡)dÂ¯w, (7)
where Sğœƒ(ğºğ‘¡,ğ‘¡)is score function to estimate the scores of perturbed
graphsâˆ‡ğºğ‘¡logğ‘ğ‘¡(ğºğ‘¡)andğ‘ğ‘¡(ğºğ‘¡)is the marginal distribution un-
der the forward process at time ğ‘¡. In practice, two GNNs are utilized
as the score function to denoise both node features and graph struc-
tures. Â¯wis a reverse time standard Wiener process.
4 RECONSTRUCTION OF PROTOTYPICAL
GRAPH FOR OOD DETECTION
In this section, we first propose a naive graph reconstruction method,
termed as GR-MOOD, to analyze its potential and limitations for
molecular graph OOD detection. Then, we propose a novel ap-
proach of PGR-MOOD to reconstruct the prototypical graphs of ID
samples for effective and efficient OOD detection.
4.1 GR-MOOD
Inspired by the generative methods [ 6,29], we design a vanilla
graph reconstruction model (GR-MOOD) for molecular graph OOD
detection. GR-MOOD is pre-trained on a large-scale compound
dataset (e.g., QM9 or ZINC) and fine-tuned on ğ·train. Consider-
ing input graph ğºâˆˆğ·test, we utilize GR-MOOD to perturb and
reconstruct it via:
ğºo=diffuse(ğº,ğœƒ,ğ‘‡), (8)
Ë†ğº=denoise(ğºo,ğœƒ,ğ‘‡), (9)
whereğœƒis the parameters of GR-MOOD, and ğ‘‡is the iteration num-
bers. Function diffuse(Â·)applies Eq. (6)to introduce perturbations
that transform ğºinto a noised state ğºo, while function denoise(Â·)
100 300 500 700 1000
Iteration020406080Score
50100150200250300
Time(s)
DrugOOD-EC50-Scaffold
AUROC
AUPR
FPR95
Time(a) Performance and Time change
with the iteration
0.2
 0.0 0.2 0.4 0.6 0.8 1.0 1.2
OOD judge score0.00.51.01.52.02.5Frequencyin-distribution out-of-distribution(b) Reconstruction score distri-
bution for ID and OOD
Figure 4: Experiments on DrugOOD. (a) Diffusion model re-
quires a large number of iterations to obtain an effective
reconstruction. (b) The reconstruction does not yield the dis-
criminative results as expected.
utilizes Eq. (7)to reverse the process, effectively denoising ğºoto
generate reconstruction graph Ë†ğº.
Upon acquiring the reconstruction graph Ë†ğº, we utilize a GNN
well-trained on the ID dataset to encode both the feature and struc-
ture information of ğºand Ë†ğº, whose representations are denoted as
ğ‘§andË†ğ‘§, respectively. The cosine similarity between them is treated
as OOD judge score and is defined in Eq. (3):
sim(ğº,Ë†ğº)=ğ‘§Â·Ë†ğ‘§
âˆ¥ğ‘§âˆ¥Ã—âˆ¥Ë†ğ‘§âˆ¥. (10)
To validate GR-MOOD effectiveness, we conduct experiments on
two DrugOOD datasets [ 17]. As shown in Fig. 3, the performance of
GR-MOOD is comparable (e.g., AUROC and AUPR) or even outper-
forming (e.g., the smaller score of FPR95 is better) than the SOTA
method of GOOD-D [ 28]. The underlying principle is that since
GR-MOOD is trained to reconstruct graphs that align with the ID
distribution, OOD samples, due to their inherent dissimilarity from
the ID distribution, will typically undergo poorer reconstruction
when being processed. Such discrepancy is quantified as a lower
judge score, which signals the presence of an OOD sample. This
mechanism highlights the critical role of diffusion model based
reconstruction method in identifying graphs that do not conform
to the expected distribution, thereby providing a quantitative basis
for distinguishing between ID and OOD samples.
Limitation of GR-MOOD: Despite the intuitive promise of GR-
MOOD, our evaluation reveals the non-negligible limitations in
terms of its time efficiency and reconstruction quality measure-
ment. First, the primary constraint of GR-MOOD is due to the
inherent structural complexity of molecular graphs. As illustrated
in Fig. 4a, this complexity requires the diffusion model to take an
extensive amount of denoising steps to fulfill the reconstruction,
improving model performance at the expense of efficiency. Even
worse, repeating the generation process for each molecules makes
it challenging to scale in the testing phase, which has to screen on
a large pool of molecule candidates. Second , another issue pertains
to the adequacy of the similarity function employed in our model.
As depicted in Fig. 4b, the reconstruction similarity distributions of
ID and OOD samples calculated based on Eq. (10)are not signifi-
cantly different1. Since graphs embody as non-Euclidean data, the
1There are similar sub-structures among the molecular graphs (e.g., functional groups
like benzene rings), resulting in close representations of the OOD and ID samples.
 
2643Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
standard metrics such as cosine similarity impedes the ability to
accurately capture the nuances of molecular structure and node
features among the molecules. This limitation can result in the
consequential loss of detection accuracy.
4.2 PGR-MOOD
To address the limitations of GR-MOOD, we propose a novel ap-
proach based upon diffusion model, PGR-MOOD (Prototypical
Graph Reconstruction for Molecular OOD Detection). The innova-
tion of PGR-MOOD has three aspects: A strong similarity function,
a prototypical graphs generator, and an efficient and scalable OOD
detector. The architecture of PGR-MOOD is shown in Fig. 5.
A Strong Similarity Function based on FGW. The cosine similar-
ity metric is oriented towards quantifying the angular divergence
between two vectors, while it is not suitable for non-Euclidean data
such as graphs. In fact, measuring the similarity between graphs
is equivalent to calculating the their matching degree, the higher
the matching degree, the more similar they are. Fused Gromov-
Wasserstein ( FGW ) distance has been proved particularly advanta-
geous for the measurement between graphs. It achieves a balance
between the optimal transport (OT) distance with a cost on node
features and the Gromov-Wasserstein (GW) distance among the
toplogical structures.
Specifically, FGW treats the graph associated with topology and
node feature as a probability distribution. It allows for the compu-
tation of costs between two distributions with optimal coupling,
serving as a distance measure between graphs. For two graphs
represented in OT format, ğº1=(ğ´1,ğ‘‹1,ğœ‡1)andğº2=(ğ´2,ğ‘‹2,ğœ‡2),
their FGW distance is defined as:
FGWğ›¼(ğº1,ğº2)=min
Î (ğœ‡1,ğœ‡2)âˆ‘ï¸
ğ‘–ğ‘—ğ‘˜ğ‘™(ğ›¼(ğ´1(ğ‘–,ğ‘—)âˆ’ğ´2(ğ‘˜,ğ‘™))2(11)
+(1âˆ’ğ›¼)âˆ¥ğ‘‹1(ğ‘–)âˆ’ğ‘‹2(ğ‘˜)âˆ¥2
2)ğœ‹ğ‘–ğ‘˜ğœ‹ğ‘—ğ‘™,
whereğ´1(ğ‘–,ğ‘—)represents the element of the ğ‘–-th row and ğ‘—-th col-
umn inğ´1,ğ‘‹1(ğ‘–)represents the ğ‘–th row vector of ğ‘‹,ğ›¼âˆˆ[0,1]is
a parameter to balance the structure term and the feature term,
Î (ğœ‡1,ğœ‡2)={ğœ‹âˆˆğ‘…ğ‘šÃ—ğ‘›+s.t.,Ãğ‘š
ğ‘–=1ğœ‹ğ‘–,ğ‘—=ğœ‡2(ğ‘—),Ãğ‘›
ğ‘—=1ğœ‹ğ‘–,ğ‘—=ğœ‡1(ğ‘–)}
is the set of all admissible couplings between ğœ‡1andğœ‡2.FGW(Â·)
metric exhibits optimal performance in directly discerning both
structural variances and feature disparities between graphs.
A Prototypical Graphs Generator. The naive diffusion model
of GR-MOOD reconstructs graph that favors the distribution of
the input samples, instead of following the distribution learned
during the training phase. It misleads the detectorâ€™s judgment on the
OOD samples. To address this challenge, we propose a prototypical
graphs generator, which generates prototypical graphs satisfying
the following two properties: â€For any input graph ğºinâˆˆğ·in,
whereğ·inrepresents all ID graphs, the prototypical graph ought to
closely resemble the graph ğºin.âFor any input ğºoutâˆˆğ·out, where
ğ·outrepresents all OOD graphs, the prototypical graph should exhibit
significant deviation from the graph ğºout. Consequently, the goal is
to generate a prototypical graph ğºwhich is close to the ID graphs
and far away from the OOD graphs.To satisfy Property â€, Eq. (11)is utilized as the distance metric,
and the loss function LIDis formulated to guide the denosing
process at the generator:
LID=Eğºinâˆ¼ğ·in
train[FGW(ğºin,ğº)]. (12)
Similarly to comply with Property â, we introduce loss function
LOOD to enhance the distance between ğºfrom OOD samples:
LOOD=âˆ’Eğºoutâˆ¼ğ·out
train[FGW(ğºout,ğº)]. (13)
Note that OOD graphs ğºoutare unreachable during the training
phase, precluding the direct formulation of LOOD. Consequently, it
becomes imperative to synthesize graphs as proxies for the absent
OOD samples. Recalling the pre-trained diffusion model FMin
Eq.(7), it adopts socre function Sğœƒto generate graph. The parameter
weights of Sğœƒis given byğœƒM={ğœƒ(ğ‘™)
M}ğ¿
ğ‘™=1, whereğœƒ(ğ‘™)
Mrepresent the
parameters of the ğ‘™-th score function. We propose to directly perturb
parametersğœƒMfor generating OOD graphs ğºout:
ËœğœƒM={ğœƒ(ğ‘™)
M(ğ¼+ğ›¼ğ‘ƒ(ğ‘™))}ğ¿
ğ‘™=1, (14)
whereğ›¼>0is perturbation strength, ğ¼is identity matrix, and
ğ‘ƒ(ğ‘™)is perturbation matrix. By perturbing the parameters ğœƒM, a
new score function SËœğœƒ(Â·)is derived. Experimental observations
(w/oLOOD of Table 2) reveal that SËœğœƒ(Â·)can induce a deviation in
the denoising trajectory away from the original data distribution,
thereby enabling the diffusion model to generate ğºoutduring the
training phase. In light of these researches, a composite loss function
Lguide is formulated by integrating both LOOD andLID:
Lguide=LID+L OOD. (15)
It is leveraged to guides the training of Prototypical Graphs Gener-
ator FPG, which has the same architecture and initial parameters
ğœƒwith FM, to generate prototypical graph ğº. The generation of
ğºbyFPGunfolds in two phases: Firstly, in contrast to generating
directly from Gaussian noise, a graph ğº0fromğ·train is randomly
chosen as the start point of generation. We then add ğ‘‡-step noise
according to Eq. (6)to get the final noise graph ğºğ‘‡(i.e.,ğº0â†’ğºğ‘‡).
Secondly,Lguide guides the denoising step of diffusion model to
generate prototype graph ğº:
dğ‘®ğ‘¡=[fğ‘¡(ğ‘®ğ‘¡)âˆ’ğ‘”(ğ‘¡)2(Sğœƒ(ğ‘®ğ‘¡,ğ‘¡)âˆ’ (16)
âˆ‡ğºğ‘¡Lguide(ğ‘®ğ‘¡))]dğ‘¡+ğ‘”(ğ‘¡)dw,
whereğ‘¡is the indicator of the denoise step and varies from ğ‘‡to
0. The prototype graph ğºgenerated by the above equation can
be viewed as the reconstruction of both ID and OOD graphs, but
has better discrimination than the reconstruction generated in GR-
MOOD. To further reduce the computation, rather than utilizing
the entirety of ğ·train, a fixed batch-size dataset ğ·batch is employed
for the computation of LID. Eachğ·batch can generate one ğº, and
they are combined to formulate a list ğ‘ƒğ¿={ğº(ğ‘–)}ğ¼
ğ‘–=1,ğ¼=âŒˆ|ğ·train|
|ğ·batch|âŒ‰.
An Efficient and Scalable OOD Detector. Diffusion models re-
quire significant time and memory resources during the testing
phases because they need to generate a reconstructed graph for
each input. To alleviate this computational burden, PGR-MOOD
eliminates the necessity of graph reconstruction in the testing phase
 
2644KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, & Xin Wang.
Figure 5: Overview of the proposed PGR-MOOD method. In the training phase, we utilize a pre-trained diffusion model to
generate OODs, then calculate Lguide with OODs and training graphs. Under the guide of Lguide, the prototypical graphs
generator generates prototypical graphs ğºas the reconstruction of testing inputs. In the testing phase, we utilize ğºto calculate
the similarity between testing graphs as the OOD judge score.
via preparing the prototypical graphs in the training phase. PGR-
MOOD leverages the ğºwithin listğ‘ƒğ¿to conduct the similarity
measurement with every new test sample. The maximum similarity
is employed as the definitive judge score for OOD detection:
ğ½(ğº)=max
ğºâˆ¼ğ‘ƒğ¿[sim(ğº,ğº test)], ğºtestâˆˆğ·test. (17)
where sim(Â·) is the similarity function based on the inverse of FGW
distance.
Algorithm 1 PGR-MOOD
Input: A Pre-trained diffusion models FM; The data loader of in-
domain training set ğ·train; An empty prototypical graphs lists
ğ‘ƒğ¿; Denoise step ğ‘‡.
Output: Prototypical graphs lists ğ‘ƒğ¿;
1:Utilize Eq. (14) to perturb the parameters of FMto get Ëœğœƒ;
2:GenerateğºOOD though FMwith parameters Ëœğœƒ;
3:forğºbatch inğ·traindo
4: Random select a graph ğº0fromğºbatch;
5: Utilize Eq. (6) to calculate noise graph ğºğ‘‡withğº0;
6: fort in T to 1 do
7: ComputeLguide withğºbatch andğºOOD.
8: Perform denoise steps in Eq. (16) with Lguide andğºğ‘¡.
9: end for
10: Addğºtoğ‘ƒğ¿;
11:end for
5 EXPERIMENT
In this section, we verify the effectiveness of PGR-MOOD and GR-
MOOD by performing experiments on two graph OOD benchmarks.
5.1 Experiment Setup
5.1.1 Datasets. With the increasing attention on OOD detection
in the molecular graphs, two benchmarks are proposed, GOOD [ 9]
and DrugOOD [ 17], respectively. These two benchmarks provide
the detailed rules to distinguish between ID and OOD. GOOD is
built based on the scaffold and size of the molecular graph, andDrugOOD adds an assay on the basis of these two distribution shifts.
We take six datasets from DrugOOD and four datasets from GOOD
as our experimental datasets. Please see Appendix A.1 for details.
5.1.2 Baselines Methods. To verify the performance of our meth-
ods, namely GR-MOOD and PGR-MOOD, we use the GNNsâ€™ Max
Softmax Score (MSP) [ 14] as a vanilla baseline and then compare
with three SOTA graph OOD detection methods (GOOD-D [ 28],
AAGOD [ 11], and GraphDE [ 24]). Meanwhile, two graph anomaly
detection methods, namely OCGIN [ 35] and GLocalKD [ 31], are
introduced as the baseline. In addition, as the first molecular graph
OOD detection method based on the diffusion model, we also com-
pare the PGR-MOOD with the naive solution GR-MOOD to verify
whether its limitations have been solved. Please see Appendix A.2
for details.
5.1.3 Implementation Details. For our methods, we utilize the dif-
fusion model GDSS [ 18] as the backbone which achieves stat-of-
the-art performance on graph generation. GDSS is pre-trained on
the QM9 dataset, which comprises a large collection of organic
molecules with 113k samples. Following the setting of GraphDE,
we perform 10 random trials and report the average accuracy on
the test set, along with 95%confidence intervals. During training,
we setğ›¼to0.5to balance the topological structure and node fea-
tures when computing the FGW distance. We set ğ·ğ‘ğ‘ğ‘¡ğ‘â„ to 128 and
the number of perturbation steps ğ‘‡âˆˆ[1,10]to reduce memory
allocation and computation complexity. For all baseline methods,
we follow settings reported in their papers. All the experiments
are implemented by PyTorch, and run on an NVIDIA TITAN-RTX
(24G) GPU.
5.2 Performance Analysis
Q: Whether PGR-MOOD achieves the best performance on
the OOD detection in molecular graphs? Yes, we utilize the
new loss functionLguide to guide the diffusion model to generate
prototypical graphs that are more representative of all ID samples,
and more easily detect OOD samples.
 
2645Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: OOD detection performance on the DrugOOD dataset. Scaffold, Size, and Assay are the basis for dividing ID and OOD
graphs. The best and runner-up results are highlighted with bold and underline , respectively.
DrugOOD-IC50
Scafflod Size Assay
OOD Detector AUROCâ†‘ AUPRâ†‘ FPR95â†“ AUROCâ†‘ AUPRâ†‘ FPR95â†“ AUROCâ†‘ AUPRâ†‘ FPR95â†“
MSP 54.57Â±9.18 52.43Â±6.85 90.76Â±4.95 52.57Â±9.07 57.23Â±3.25 88.60Â±4.75 58.19Â±7.23 56.38Â±5.75 89.20Â±3.05
GOOD-D 85.40Â±1.23 87.13Â±2.31 27.40Â±2.37 91.55Â±1.10 87.91Â±3.74 16.95Â±0.47 81.35Â±1.74 79.05Â±0.79 75.02Â±0.57
GraphDE 69.15Â±1.11 67.40Â±0.51 80.30Â±0.33 78.72Â±1.78 79.36Â±1.24 78.97Â±0.75 68.56Â±1.08 66.56Â±0.31 82.20Â±0.93
AAGOD 84.23Â±2.97 83.96Â±1.34 21.56Â±1.08 84.75Â±1.23 83.32Â±1.61 19.80Â±0.93 71.94Â±1.45 72.86Â±1.84 85.62Â±2.71
OCGIN 68.39Â±4.77 66.05Â±5.11 82.80Â±7.50 70.94Â±5.09 68.99Â±3.72 74.80Â±6.46 67.53Â±4.61 66.95Â±5.23 79.80Â±4.60
GLocalKD 63.42Â±0.60 58.03Â±0.64 70.28Â±1.83 69.44Â±0.58 67.29Â±0.77 81.13Â±1.46 62.08Â±0.76 61.93Â±0.61 82.70Â±1.98
GR-MOOD 78.82Â±2.31 77.35Â±1.94 25.43Â±1.72 68.51Â±2.65 69.19Â±3.01 70.78Â±2.33 61.91Â±1.87 62.95Â±1.54 84.87Â±1.39
PGR-MOOD 91.57Â±1.32 90.12Â±0.71 19.42Â±0.22 93.84Â±1.53 94.85Â±2.03 15.57Â±1.03 83.72Â±2.51 80.31Â±1.44 64.65Â±0.57
Improve +7.22% +3.43% -9.89% +2.50% +7.08% -8.41% +2.91% +1.52% -13.80%
DrugOOD-EC50
Scafflod Size Assay
OOD Detector AUROCâ†‘ AUPRâ†‘ FPR95â†“ AUROCâ†‘ AUPRâ†‘ FPR95â†“ AUROCâ†‘ AUPRâ†‘ FPR95â†“
MSP 57.26Â±7.25 57.08Â±5.94 87.26Â±5.12 59.18Â±8.77 58.41Â±4.95 83.76Â±5.60 48.19Â±9.18 46.38Â±6.85 89.26Â±4.95
GOOD-D 82.51Â±1.31 81.98Â±2.71 63.21Â±2.89 92.50Â±1.32 88.37Â±1.26 19.20Â±0.51 65.20Â±1.48 67.22Â±1.61 92.24Â±3.56
GraphDE 68.55Â±1.03 66.56Â±1.90 82.20Â±0.74 79.64Â±1.16 77.75Â±1.48 59.25Â±0.57 66.24Â±1.79 66.28Â±0.98 80.29Â±1.04
AAGOD 77.17Â±5.52 75.32Â±5.56 72.76Â±4.95 78.72Â±6.59 79.23Â±6.30 68.66Â±5.43 74.57Â±9.18 72.43Â±6.85 71.83Â±4.43
OCGIN 69.01Â±3.98 67.83Â±4.87 74.79Â±7.50 78.45Â±5.17 74.30Â±3.96 81.53Â±5.64 71.33Â±2.85 70.94Â±3.69 80.93Â±3.55
GLocalKD 66.59Â±0.71 68.64Â±0.45 71.22Â±1.01 69.59Â±0.98 68.72Â±0.83 68.70Â±1.36 73.32Â±1.65 69.23Â±1.57 75.39Â±2.19
GR-MOOD 71.15Â±2.50 73.02Â±3.21 81.79Â±3.58 73.80Â±2.95 78.49Â±1.63 70.96Â±1.82 60.17Â±1.56 61.69Â±10.27 79.09Â±1.33
PGR-MOOD 87.53Â±1.31 86.16Â±0.72 62.82Â±2.21 97.67Â±1.54 96.32Â±1.47 13.79Â±1.23 86.73Â±3.34 83.56Â±3.28 63.74Â±2.59
Improve +6.02% +5.09% -3.70% +5.58% +8.41% -28.10% +16.30% +15.36% +11.22%
Table 2: Ablation experiment results on four datasets.
AUROCâ†‘ AUPRâ†‘ FPR95â†“
Dataset w/oLID w/oLOOD w/oFGW w/oLID w/oLOOD w/oFGW w/oLID w/oLOOD w/oFGW
DrugOOD-EC50 -4.57 -2.43 -0.76 -7.72 -2.32 -4.75 +5.74 +2.22 +1.63
DrugOOD-IC50 -5.14 -1.75 -1.24 -4.26 -1.98 -3.62 +6.83 +1.77 +2.36
GOOD-HIV -3.26 -2.58 -0.54 -5.83 -2.43 -3.18 +4.72 +2.03 +2.61
GOOD-PCBA -5.89 -1.08 -2.07 -6.44 -3.70 -4.81 +3.62 +1.12 +2.14
â–·Comparison with the naive solution. As shown in Table 1 and
Table 3, compared with GR-MOOD on six datasets of DrugOOD,
PGR-MOOD enhances the average AUC and AUPR by 32.76%and
29.54%, and reduces the average FPR95 by 45.65%. These results
demonstrate that the prototypical graphs of PGR-MOOD generated
with the FGW similarity function are more suitable for distinguish-
ing the original input graphs in the testing phase.
â–·Comparison with the State-of-the-art Methods. To verify the
superiority of our method, we compare it with the previous SOTA
methods. As shown in the last row of Table 1 and Table 3, our
method achieves SOTA results on all datasets. The average im-
provements against the previous SOTA are 8.54%of AUC and 8.15%
of AUPR, and the average reduction on FPR95 is 13.7%. We attribute
these results to the fact that the prototypical graphs generated by
PGR-MOOD can enlarge the judge score gap between ID and OOD
which satisfies the requirement of optimal OOD detector.5.3 Visualization of Score Gap
Q: Whether PGR-MOOD can enlarge the judge score gap
between ID and OOD graphs? Yes, we calculate the similarity
between the prototypical graphs and test graphs, which has a mas-
sive difference for ID and OOD. A more significant gap between
ID and OOD graphs corresponds to a better graph OOD detector.
We present the scoring distributions on two datasets in Fig. 6. The
ID and OOD are perfectly separated into two distinct distributions,
so we can use a simple threshold for OOD detection and achieve
SOTA performance.
5.4 Ablation Experiment
Q: Whether each module in PGR-MOOD contribute to effec-
tively discriminating OOD molecular graphs? Yes, we conduct
experiments on four datasets to verify the role of LID,LOOD, and
FGW modules in PRG-MGOD. The results are shown in Table 2.
â–·Ablation onLIDandLOOD.We removeLIDandLOOD in the
Lguide respectively to explore their impacts on the performance of
OOD detection. We find that merely enlarging the distance between
 
2646KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, & Xin Wang.
Table 3: OOD detection performance on the GOOD dataset. Scaffold and Size are the basis for dividing ID and OOD graphs. Best
and runner-up results are highlighted with bold and underline , respectively.
GOOD-HIV
Dataset Metric MSP GOOD-D GraphDE AAGOD OCGIN GLocalKD GR-MOOD PGR-MOOD Improve
ScaffoldAUROCâ†‘58.55Â±9.18 62.42Â±1.89 65.66Â±1.69 74.81Â±1.56 66.29Â±4.35 64.76Â±0.34 61.22Â±2.68 85.57Â±1.32 +14.38%
AUPRâ†‘ 58.34Â±6.85 69.60Â±2.03 60.94Â±0.48 72.51Â±1.99 65.45Â±5.98 65.92Â±0.64 60.53Â±1.94 85.12Â±0.71 +12.61%
FPR95â†“93.40Â±4.95 87.75Â±0.35 88.40Â±0.43 76.71Â±1.82 85.65Â±6.74 83.98Â±0.89 87.35Â±1.66 66.50Â±2.01 -13.31%
SizeAUROCâ†‘54.96Â±9.07 72.23Â±1.54 66.72Â±1.13 63.44Â±1.92 65.04Â±4.65 68.49Â±1.22 69.67Â±2.71 88.43Â±2.37 +22.47%
AUPRâ†‘ 54.09Â±3.25 76.12Â±1.26 65.55Â±0.30 60.02Â±1.88 64.67Â±4.03 68.23Â±0.97 71.76Â±2.39 87.77Â±2.18 +15.30%
FPR95â†“97.80Â±4.75 68.74Â±3.25 72.20Â±0.89 75.97Â±1.15 73.64Â±5.86 76.13Â±1.55 60.56Â±2.91 65.17Â±2.21 -5.17%
GOOD-PCBA
Dataset Metric MSP GOOD-D GraphDE AAGOD OCGIN GLocalKD GR-MOOD PGR-MOOD Improve
ScaffoldAUROCâ†‘54.57Â±9.07 85.69Â±1.16 68.45Â±1.23 79.06Â±0.48 69.50Â±3.17 70.90Â±1.68 70.07Â±0.60 86.57Â±1.32 +1.02%
AUPRâ†‘52.43Â±6.21 86.97Â±1.76 66.07Â±0.32 72.70Â±0.30 68.34Â±4.11 73.56Â±1.64 71.90Â±0.64 88.12Â±0.71 +1.32%
FPR95â†“90.76Â±4.36 16.04Â±1.90 82.34Â±0.67 60.37Â±0.58 87.94Â±6.98 39.57Â±1.44 55.42Â±1.89 15.01Â±0.32 -6.04%
SizeAUROCâ†‘58.57Â±8.99 78.31Â±1.19 66.24Â±1.90 64.90Â±1.71 70.61Â±3.25 73.58Â±0.50 71.49Â±0.78 83.84Â±1.53 +7.06%
AUPRâ†‘57.23Â±3.25 76.21Â±1.61 64.58Â±0.21 67.24Â±0.87 72.21Â±3.91 67.40Â±0.91 75.31Â±1.09 84.85Â±2.03 +11.33%
FPR95â†“88.60Â±4.75 27.30Â±1.72 88.45Â±0.29 60.03Â±1.06 63.80Â±4.47 60.29Â±0.89 46.37Â±1.29 17.01Â±0.17 -37.61%
0.0 0.2 0.4 0.6 0.8
OOD judge score05101520Frequencyin-distribution out-of-distribution
(a) HIV-Scaffold
0.0 0.2 0.4 0.6 0.8 1.0
OOD judge score0204060Frequencyin-distribution out-of-distribution (b) IC50-Scaffold
0.5
 0.0 0.5 1.0 1.5 2.0 2.5
OOD judge score012345Frequencyin-distribution out-of-distribution (c) IC50-Size
Figure 6: OOD judge score distributions on three datasets.
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000017
/uni00000015
/uni00000013/uni00000015/uni00000017/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048
/uni0000002a/uni00000032/uni00000032/uni00000027/uni00000010/uni0000002b/uni0000002c/uni00000039/uni00000010/uni00000036/uni00000046/uni00000044/uni00000049/uni00000049/uni00000052/uni0000004f/uni00000047
/uni0000002f/uni00000042/uni0000002c/uni00000027
/uni0000002f/uni00000042/uni00000032/uni00000032/uni00000027
/uni0000002f/uni00000042/uni0000004a/uni00000058/uni0000004c/uni00000047/uni00000048
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000017
/uni00000015
/uni00000013/uni00000015/uni00000017/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048
/uni0000002a/uni00000032/uni00000032/uni00000027/uni00000010/uni0000002b/uni0000002c/uni00000039/uni00000010/uni00000036/uni0000004c/uni0000005d/uni00000048
/uni0000002f/uni00000042/uni0000002c/uni00000027
/uni0000002f/uni00000042/uni00000032/uni00000032/uni00000027
/uni0000002f/uni00000042/uni0000004a/uni00000058/uni0000004c/uni00000047/uni00000048
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000017
/uni00000015
/uni00000013/uni00000015/uni00000017/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048
/uni00000027/uni00000055/uni00000058/uni0000004a/uni00000032/uni00000032/uni00000027/uni00000010/uni0000002c/uni00000026/uni00000018/uni00000013/uni00000010/uni00000036/uni00000046/uni00000044/uni00000049/uni00000049/uni00000052/uni0000004f/uni00000047
/uni0000002f/uni00000042/uni0000002c/uni00000027
/uni0000002f/uni00000042/uni00000032/uni00000032/uni00000027
/uni0000002f/uni00000042/uni0000004a/uni00000058/uni0000004c/uni00000047/uni00000048
Figure 7: Loss variation during generation on three datasets.
the prototypical graph from OOD samples (w/o LID) or bringing
it closer to ID samples (w/o LOOD) significantly undermines the
performance of PGR-MOOD. This fully confirms that the Property â€
and Property âare valid and correct. These results demonstrate
that the composition of LIDandLOOD can generate prototypical
graphsğºwith different similarity measurement for ID and OOD
graphs in the testing phase.
â–·Ablation on FGW .We replace the sim(Â·)function based on FGW
in Eq. (17)with Eq. (10)of GR-MOOD to explore its importance on
the performance of OOD detection. We find that the FGW is even
more influential than LOOD on all datasets with different metrics.
These experimental results demonstrate that a proper similarity
measurement is necessary and the FGW can thoroughly evaluate
the similarity between two graphs by considering both their struc-
ture and features.
Q: Whether the prototypical graphs ğºgenerated byLguide-
guided PGR-MOOD follow the Properties â€andâ?Yes, the
prototypical graphs ğºeffectively reduce the distance with the ID
graphs and significantly increase the separation from the OOD
EC50 IC50 HIV PCBA
Dataset02004006008001000Training time (s)PGR-MOOD
GOOD-D
GR-MOOD
EC50 IC50 HIV PCBA
Dataset012345678Testing time (s)PGR-MOOD
GOOD-D
GR-MOOD
EC50 IC50 HIV PCBA
Dataset0200400600800Memory allocation(MB)PGR-MOOD
GOOD-D
GR-MOODFigure 8: Efficiency verification experiments on training time,
testing time, and memory allocation.
graphs. To validate the impact of Lguide, its trend is monitored
throughout the generation phase, as depicted in Fig. 7. Here, LID
andLOOD are computed using Eq. (12)and Eq. (13)and they repre-
sent the distance between ğºand all graphs belong to ID and OOD,
respectively. As the generation progresses, LIDsteadily decreases
towards 0, whereas LOOD escalates sharply. This observation aligns
seamlessly with the foundational principles of PGR-MOOD.
5.5 Computational Complexity Comparison
Q: Whether the PGR-MOOD reduces the complexity of time
and space in the training and testing phases? Yes, to validate
the efficiency and scalability of PGR-MOOD, we conduct compre-
hensive comparisons against the SOTA method GOOD-D and a
baseline GR-MOOD. The comparative results are illustrated in Fig. 8.
Although PGR-MOOD slightly trails GOOD-D in testing time, it
markedly surpasses it in all other aspects.
â–·Efficiency on execution time. During the training phase, PGR-
MOOD exhibits a substantially reduced training duration compared
to both GOOD-D and GR-MOOD. This efficiency stems from GOOD-
Dâ€™s reliance on a time-consuming contrastive learning approach
for model training, whereas GR-MOOD necessitates fine-tuning of
the diffusion model on the training set. In contrast, PGR-MOOD
requires the generation of only a limited set of prototype graphs,
thereby enhancing its training efficiency. During the testing phase,
GOOD-D leverages its trained model to directly classify input
graphs, while PGR-MOODâ€™s method, which entails calculating the
similarity between input graphs and the set of prototypical graphs
individually. Consequently, PGR-MOOD is marginally slower than
 
2647Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
GOOD-D. However, it significantly outpaces GR-MOOD, which
requires the regeneration of reconstructed graphs for each input.
â–·Scalability in memory allocation. To assess the memory effi-
ciency of our method, we evaluate memory allocation during the
testing phase. PGR-MOOD, which eschews the need for any model
for OOD detection, only loads the set of prototypical graphs and
demands the least memory allocation. In contrast, the GOOD-D
method requires loading GNNs, and GR-MOOD necessitates load-
ing a diffusion model for reconstruction graphs, thereby increasing
their memory requirements. The experimental findings underscore
that our approachcan significantly mitigate memory consumption
and enhance model scalability.
6 CONCLUSION
This study explores OOD detection for molecular graphs, starting
with a basic diffusion model-based approach, GR-MOOD, and iden-
tifying key challenges. We introduce PGR-MOOD, an advanced
OOD detection method for molecular graphs that addresses GR-
MOODâ€™s limitations by using a diffusion model to create proto-
typical graphs. These graphs closely resemble ID inputs while dis-
tinctly diverging from OOD inputs. PGR-MOOD utilizes the Fused
Gromov-Wasserstein distance for efficient similarity measurement
and OOD scoring, significantly reducing computational load. Our
approach demonstrates SOTA results across ten datasets, proving
its effectiveness.
7 ACKNOWLEDGE
This work was supported by a grant from the National Natural
Science Foundation of China under grants (No.62372211, 62272191),
and the Foundation of the National Key Research and Development
of China (No.2021ZD0112500), and the International Science and
Technology Cooperation Program of Jilin Province (No.20230402076
GH, No. 20240402067GH), and the Science and Technology Devel-
opment Program of Jilin Province (No. 20220201153GX).
REFERENCES
[1]Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan,
Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. 2021.
Equivariant subgraph aggregation networks. arXiv preprint arXiv:2110.02910
(2021).
[2]Lei Cai, Jundong Li, Jie Wang, and Shuiwang Ji. 2021. Line graph neural networks
for link prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence
44, 9 (2021), 5103â€“5113.
[3]Xiaohui Chen, Jiaxing He, Xu Han, and Liping Liu. 2023. Efficient and Degree-
Guided Graph Generation via Discrete Diffusion Modeling. (2023).
[4]Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah.
2023. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis
and Machine Intelligence (2023).
[5]Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. 2022. How
powerful are k-hop message passing graph neural networks. Advances in Neural
Information Processing Systems 35 (2022), 4776â€“4790.
[6]Ruiyuan Gao, Chenchen Zhao, Lanqing Hong, and Qiang Xu. 2023. DiffGuard:
Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained
Diffusion Models. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 1579â€“1589.
[7]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
conference on machine learning. PMLR, 1263â€“1272.
[8]Mark S Graham, Walter HL Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev,
Sebastien Ourselin, and Jorge Cardoso. 2023. Denoising diffusion models for out-
of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2947â€“2956.
[9]Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. 2022. Good: A graph out-of-
distribution benchmark. Advances in Neural Information Processing Systems 35(2022), 2059â€“2073.
[10] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. 2022. Orthogo-
nal graph neural networks. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 36. 3996â€“4004.
[11] Yuxin Guo, Cheng Yang, Yuluo Chen, Jixi Liu, Chuan Shi, and Junping Du. 2023.
A Data-centric Framework to Endow Graph Neural Networks with Out-Of-
Distribution Detection Ability. (2023).
[12] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[13] Mingguo He, Zhewei Wei, Hongteng Xu, et al .2021. Bernnet: Learning arbitrary
graph spectral filters via bernstein approximation. Advances in Neural Information
Processing Systems 34 (2021), 14239â€“14251.
[14] Dan Hendrycks and Kevin Gimpel. 2016. A baseline for detecting misclassified and
out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136
(2016).
[15] Wolfgang Huber, Vincent J Carey, Li Long, Seth Falcon, and Robert Gentleman.
2007. Graphs in molecular biology. BMC bioinformatics 8, 6 (2007), 1â€“14.
[16] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. 2019. Gen-
erative models for graph-based protein design. Advances in neural information
processing systems 32 (2019).
[17] Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Lanqing Li, Long-Kai Huang,
Tingyang Xu, Yu Rong, Jie Ren, Ding Xue, et al .2023. Drugood: Out-of-
distribution dataset curator and benchmark for ai-aided drug discoveryâ€“a focus
on affinity prediction problems with noise annotations. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 37. 8023â€“8031.
[18] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. 2022. Score-based generative model-
ing of graphs via the system of stochastic differential equations. In International
Conference on Machine Learning. PMLR, 10362â€“10383.
[19] Xin Juan, Fengfeng Zhou, Wentao Wang, Wei Jin, Jiliang Tang, and Xin Wang.
2023. INS-GNN: Improving graph imbalance learning with self-supervision.
Information Sciences 637 (2023), 118935.
[20] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations.
[21] Maksim Kuznetsov and Daniil Polykovskiy. 2021. MolGrow: A graph normalizing
flow for hierarchical molecular generation. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 35. 8226â€“8234.
[22] Junying Li, Deng Cai, and Xiaofei He. 2017. Learning graph-level representation
for drug discovery. arXiv preprint arXiv:1709.03741 (2017).
[23] Yibo Li, Liangren Zhang, and Zhenming Liu. 2018. Multi-objective de novo drug
design with conditional graph generative model. Journal of cheminformatics 10
(2018), 1â€“24.
[24] Zenan Li, Qitian Wu, Fan Nie, and Junchi Yan. 2022. Graphde: A generative
framework for debiased learning and out-of-distribution detection on graphs.
Advances in Neural Information Processing Systems 35 (2022), 30277â€“30290.
[25] Stratis Limnios, Praveen Selvaraj, Mihai Cucuringu, Carsten Maple, Gesine Rein-
ert, and Andrew Elliott. 2023. Sagess: Sampling graph denoising diffusion model
for scalable graph generation. arXiv preprint arXiv:2306.16827 (2023).
[26] Gary Liu, Denise B Catacutan, Khushi Rathod, Kyle Swanson, Wengong Jin, Jody C
Mohammed, Anush Chiappino-Pepe, Saad A Syed, Meghan Fragis, Kenneth
Rachwalski, et al .2023. Deep learning-guided discovery of an antibiotic targeting
Acinetobacter baumannii. Nature Chemical Biology (2023), 1â€“9.
[27] Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. 2023.
Data-Centric Learning from Unlabeled Graphs with Diffusion Model. arXiv
preprint arXiv:2303.10108 (2023).
[28] Yixin Liu, Kaize Ding, Huan Liu, and Shirui Pan. 2023. Good-d: On unsuper-
vised graph out-of-distribution detection. In Proceedings of the Sixteenth ACM
International Conference on Web Search and Data Mining. 339â€“347.
[29] Zhenzhen Liu, Jin Peng Zhou, Yufan Wang, and Kilian Q Weinberger. 2023.
Unsupervised Out-of-Distribution Detection with Diffusion Inpainting. arXiv
preprint arXiv:2302.10326 (2023).
[30] Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, and Xia Hu. 2021. EXACT:
Scalable graph neural networks training via extreme activation compression. In
International Conference on Learning Representations.
[31] Rongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. 2022. Deep
graph-level anomaly detection by glocal knowledge distillation. In Proceedings
of the Fifteenth ACM International Conference on Web Search and Data Mining.
704â€“714.
[32] Rui Miao, Kaixiong Zhou, Yili Wang, Ninghao Liu, Ying Wang, and Xin Wang.
2024. Rethinking Independent Cross-Entropy Loss For Graph-Structured Data.
arXiv preprint arXiv:2405.15564 (2024).
[33] Joshua Mitton, Hans M Senn, Klaas Wynne, and Roderick Murray-Smith. 2021.
A graph vae and graph transformer approach to generating molecular graphs.
arXiv preprint arXiv:2104.04345 (2021).
[34] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and
Stefano Ermon. 2020. Permutation invariant graph generation via score-based
 
2648KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, & Xin Wang.
generative modeling. In International Conference on Artificial Intelligence and
Statistics. PMLR, 4474â€“4484.
[35] Chen Qiu, Marius Kloft, Stephan Mandt, and Maja Rudolph. 2022. Raising the
bar in graph-level anomaly detection. arXiv preprint arXiv:2205.13845 (2022).
[36] Patrick Reiser, Marlen Neubert, AndrÃ© Eberhard, Luca Torresi, Chen Zhou, Chen
Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, et al .
2022. Graph neural networks for materials science and chemistry. Communica-
tions Materials 3, 1 (2022), 93.
[37] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed
Siddiqui, Alexander Binder, Emmanuel MÃ¼ller, and Marius Kloft. 2018. Deep
one-class classification. In International conference on machine learning. PMLR,
4393â€“4402.
[38] Xu Shen, Pietro Lio, Lintao Yang, Ru Yuan, Yuyang Zhang, and Chengbin Peng.
2024. Graph Rewiring and Preprocessing for Graph Neural Networks Based
on Effective Resistance. IEEE Transactions on Knowledge and Data Engineering
(2024).
[39] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. 2021. Maximum like-
lihood training of score-based diffusion models. Advances in Neural Information
Processing Systems 34 (2021), 1415â€“1428.
[40] Vayer Titouan, Nicolas Courty, Romain Tavenard, and RÃ©mi Flamary. 2019. Op-
timal transport for structured data with application on graphs. In International
Conference on Machine Learning. PMLR, 6275â€“6284.
[41] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher,
and Pascal Frossard. 2022. DiGress: Discrete Denoising diffusion for graph
generation. In The Eleventh International Conference on Learning Representations.
[42] CÃ©dric Vincent-Cuaz, RÃ©mi Flamary, Marco Corneli, Titouan Vayer, and Nicolas
Courty. 2022. Template based graph neural network with optimal transport
distances. Advances in Neural Information Processing Systems 35 (2022), 11800â€“
11814.
[43] Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. 2021. Be confident!
towards trustworthy graph neural networks via confidence calibration. Advances
in Neural Information Processing Systems 34 (2021), 23768â€“23779.
[44] Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan
Jia, and Jian Yu. 2020. Traffic flow prediction via spatial temporal graph neural
network. In Proceedings of the web conference 2020. 1082â€“1092.
[45] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. 2022.
Molecular contrastive learning of representations via graph neural networks.
Nature Machine Intelligence 4, 3 (2022), 279â€“287.
[46] Yili Wang, Kaixiong Zhou, Ninghao Liu, Ying Wang, and Xin Wang. 2024. Efficient
Sharpness-Aware Minimization for Molecular Graph Transformer Models. In The
Twelfth International Conference on Learning Representations. https://openreview.
net/forum?id=Od39h4XQ3Y
[47] Yili Wang, Kaixiong Zhou, Rui Miao, Ninghao Liu, and Xin Wang. 2022. AdaGCL:
Adaptive Subgraph Contrastive Learning to Generalize Large-scale Graph Train-
ing. In Proceedings of the 31st ACM International Conference on Information &
Knowledge Management. 2046â€“2055.[48] Oliver Wieder, Stefan Kohlbacher, MÃ©laine Kuenemann, Arthur Garon, Pierre
Ducrot, Thomas Seidel, and Thierry Langer. 2020. A compact review of molec-
ular property prediction with graph neural networks. Drug Discovery Today:
Technologies 37 (2020), 1â€“12.
[49] Bingzhe Wu, Jintang Li, Junchi Yu, Yatao Bian, Hengtong Zhang, CHaochao
Chen, Chengbin Hou, Guoji Fu, Liang Chen, Tingyang Xu, et al .2022. A survey
of trustworthy graph learning: Reliability, explainability, and privacy protection.
arXiv preprint arXiv:2205.10014 (2022).
[50] Qitian Wu, Yiting Chen, Chenxiao Yang, and Junchi Yan. 2023. Energy-
based out-of-distribution detection for graph neural networks. arXiv preprint
arXiv:2302.02914 (2023).
[51] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[52] Yintao Yang, Rui Miao, Yili Wang, and Xin Wang. 2022. Contrastive graph convo-
lutional networks with adaptive augmentation for text classification. Information
Processing & Management 59, 4 (2022), 102946.
[53] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. 2021. From stars to
subgraphs: Uplifting any GNN with local structure awareness. arXiv preprint
arXiv:2110.03753 (2021).
[54] Kaixiong Zhou, Soo-Hyun Choi, Zirui Liu, Ninghao Liu, Fan Yang, Rui Chen, Li
Li, and Xia Hu. 2023. Adaptive label smoothing to regularize large-scale graph
training. In Proceedings of the 2023 SIAM International Conference on Data Mining
(SDM). SIAM, 55â€“63.
[55] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu.
2020. Towards deeper graph neural networks with differentiable group normal-
ization. Advances in neural information processing systems 33 (2020), 4917â€“4928.
[56] Kaixiong Zhou, Xiao Huang, Qingquan Song, Rui Chen, and Xia Hu. 2022. Auto-
gnn: Neural architecture search of graph neural networks. Frontiers in big Data 5
(2022), 1029307.
[57] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi,
and Xia Hu. 2021. Dirichlet energy constrained learning for deep graph neural
networks. Advances in Neural Information Processing Systems 34 (2021), 21834â€“
21846.
[58] Kaixiong Zhou, Zirui Liu, Rui Chen, Li Li, S Choi, and Xia Hu. 2022. Table2graph:
Transforming tabular data to unified weighted graph. In Proceedings of the Thirty-
First International Joint Conference on Artificial Intelligence, IJCAI. 2420â€“2426.
[59] Kaixiong Zhou, Qingquan Song, Xiao Huang, Daochen Zha, Na Zou, and Xia
Hu. 2021. Multi-channel graph neural networks. In Proceedings of the Twenty-
Ninth International Conference on International Joint Conferences on Artificial
Intelligence. 1352â€“1358.
[60] Kaixiong Zhou, Qingquan Song, Xiao Huang, Daochen Zha, Na Zou, and Xia
Hu. 2021. Multi-channel graph neural networks. In Proceedings of the Twenty-
Ninth International Conference on International Joint Conferences on Artificial
Intelligence. 1352â€“1358.
[61] Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, and
Shu Wu. 2022. A survey on deep graph generation: Methods and applications. In
Learning on Graphs Conference. PMLR, 47â€“1.
 
2649Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A APPENDIX
A.1 Descriptions of Datasets and Metric
â€¢DrugOOD [ 17] is a systematic OOD dataset curator and
benchmark for drug discovery, providing large-scale, realis-
tic, and diverse datasets for graph OOD learning problems.
To meet this purpose of covering a wide range of shifts that
naturally occur in molecular graphs, we cautiously consider
three properties as the basis of dividing ID and OOD, includ-
ing assay, molecular size, and molecular scaffold. DrugOOD
provides an automated method for dividing datasets into ID
training sets, ID testing sets, and OOD testing sets. We use
the ID training set to generate prototypical graphs during
the training phase, and process OOD detection on the ID
testing set and OOD testing set since they have different
data distributions.
â€¢GOOD [ 9] is a systematic graph OOD benchmark, which pro-
vide carefully designed data environments for distribution
shifts. Given a domain, it has two kinds of shift strategies:
covariate shift, and concept shift. For a supervised dataset,
each inputs ğ‘‹âˆˆX corresponding to outputs ğ‘ŒâˆˆY and
have the distribution of training set ğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(Â·)and testing set
ğ‘ƒğ‘¡ğ‘’ğ‘ ğ‘¡(Â·). The the joint distribution ğ‘ƒ(ğ‘Œ,ğ‘‹)can be written as
ğ‘ƒ(ğ‘Œ,ğ‘‹)=ğ‘ƒ(ğ‘Œ|ğ‘‹)ğ‘ƒ(ğ‘‹). In covariate shift, the input distribu-
tions have been shifted between training and test data. For-
mallyğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘‹)â‰ ğ‘ƒğ‘¡ğ‘’ğ‘ ğ‘¡(ğ‘‹)andğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘Œ|ğ‘‹)=ğ‘ƒğ‘¡ğ‘’ğ‘ ğ‘¡(ğ‘Œ|ğ‘‹).
For concept shift, the conditional distribution ğ‘ƒ(ğ‘Œ|ğ‘‹)has
been shifted as ğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘‹)=ğ‘ƒğ‘¡ğ‘’ğ‘ ğ‘¡(ğ‘‹)andğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘Œ|ğ‘‹)â‰ 
ğ‘ƒğ‘¡ğ‘’ğ‘ ğ‘¡(ğ‘Œ|ğ‘‹). In order to maintain the consistency of datasets
we adopted covariate shift.
â€¢AUROC (Area Under the Receiver Operating Characteris-
tic curve), AUPR (Area Under the Precision-Recall curve),
and FPR95 (False Positive Rate at 95% True Positive Rate)
are metrics commonly used to evaluate the performance of
classification models, particularly in the context of binary
classification and anomaly or outlier detection tasks such as
OOD (Out-Of-Distribution) detection.
A.2 Descriptions of Baseline Methods
In our experiments, we compare the following six methods as base-
lines:
â€¢MSP [14]: MSP utilizes the backbonesâ€™ max softmax output
as the judge score, where ID has the highest score and OOD
has the lowest score.
â€¢GOOD-D [ 28]: By performing hierarchical contrastive learn-
ing on the augmented graphs, GOOD detects OOD graphs
based on the semantic inconsistency in different granulari-
ties.â€¢GraphDE [ 24]: GraphDE modeling the graph generative pro-
cess to characterize the distribution shifts of graph data
together with an additionally introduced latent environment
variable as an indicator to detect OODs.
â€¢AAGOD [ 11]: AAGOD proposes a learnable amplifier to
increase the focus on the key pattern of the structure to
enlarge the difference between IDs and OODs.
â€¢OCGIN [ 35]: OCGIN is a graph anomaly detection with
a binary classifier where a GIN encoder by the guide of
SVDD [37].
â€¢GLocalKD [ 31]: GLocalKD proposes a deep graph anomaly
detector based on knowledge distillation for both local and
global graphs.
A.3 Analysis of Hyper-Parameters
To analyze the hyper-parameter sensitivity of PGR-MOOD, we
experiment on two datasets with different ğ›¼andğ¼.
A.3.1 Analysis of ğ›¼.To analyze the impact of hyper-parameters ğ›¼
in Eq. (11), which balance the structure term and feature term. We
varyğ›¼in{0.3,0.4,0.5,0.6,0.7}and present the experimental results
in Fig. 9a. PGR-MOOD performs best with ğ›¼equal to 0.5, which
means it is the fairest way for structure and feature. This fits our
needs because we canâ€™t predict which way the OOD shift will be
biased, so it makes sense to weight both terms equally.
A.3.2 Analysis of ğ¼.To analyze the impact of hyper-parameters ğ¼
in Eq. (17), which corresponds to the number of prototypical graph
ğºthat we need to generate. We vary ğ¼in{2,4,8,16}and present the
experimental results in Fig. 9b. The performance of PGR-MOOD is
stable when ğ¼changes. In fact, the size of ğ¼does not have a huge
impact on the final OOD detection result. The calculation of ğºâˆˆğ‘ƒğ¿
can eventually traverse the entire ğ·ğ‘–ğ‘›, only the memory required
for the generation process will be affected.
0.3 0.4 0.5 0.6 0.7
value of Î±1030507090AUROC/AUPRDrugOOD-EC50-Assay
AUROC
AUPR
FPR95
(a)
2 4 8 16 32
Length of TL1030507090AUROC/AUPRDrugOOD-IC50-Scaffold
AUROC
AUPR
FPR95 (b)
Figure 9: Analysis of Hyper-Parameters of our method on
two DrugOOD datasets.
 
2650