On (Normalised) Discounted Cumulative Gain as an Off-Policy
Evaluation Metric for Top- ğ‘›Recommendation
Olivier Jeunen
ShareChat
Edinburgh, United Kingdom
jeunen@sharechat.coIvan Potapov
ShareChat
London, United Kingdom
ipotap@sharechat.coAleksei Ustimenko
ShareChat
London, United Kingdom
aleksei.ustimenko@sharechat.co
Abstract
Approaches to recommendation are typically evaluated in one of
two ways: (1) via a (simulated) online experiment, often seen as the
gold standard, or (2) via some offline evaluation procedure, where
the goal is to approximate the outcome of an online experiment. Sev-
eral offline evaluation metrics have been adopted in the literature,
inspired by ranking metrics prevalent in the field of Information
Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one
such metric that has seen widespread adoption in empirical studies,
and higher (n)DCG values have been used to present new methods
as the state-of-the-art in top-ğ‘›recommendation for many years.
Our work takes a critical look at this approach, and investigates
when we can expect such metrics to approximate the gold standard
outcome of an online experiment. We formally present the assump-
tions that are necessary to consider DCG an unbiased estimator of
online reward and provide a derivation for this metric from first
principles, highlighting where we deviate from its traditional uses
in IR. Importantly, we show that normalising the metric renders
itinconsistent, in that even when DCG is unbiased, ranking com-
peting methods by their normalised DCG can invert their relative
order. Through a correlation analysis between off- and on-line ex-
periments conducted on a large-scale recommendation platform,
we show that our unbiased DCG estimates strongly correlate with
online reward, even when some of the metricâ€™s inherent assump-
tions are violated. This statement no longer holds for its normalised
variant, suggesting that nDCGâ€™s practical utility may be limited.
CCS Concepts
â€¢Information systems â†’Recommender systems; Evaluation
of retrieval results ;â€¢Mathematics of computing â†’Probabilistic
inference problems .
Keywords
Offline Evaluation; Off-Policy Evaluation; Counterfactual Inference
ACM Reference Format:
Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko. 2024. On (Normalised)
Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-
ğ‘›Recommendation. In Proceedings of the 30th ACM SIGKDD Conference
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671687on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12pages. https://doi.org/10.
1145/3637528.3671687
1 Introduction & Motivation
Recommender systems power algorithmic decision-making on plat-
forms across the web. They occur in many different application
domains, but all are centred around a similar question:
â€œWhat content do we recommend to whom ?â€
Research and applications in the field of recommender systems
have undergone a large shift in the last few decades, moving from
rating [7] toitem prediction [ 89] and, more recently, embracing an
interventionist view [ 54]. Throughout this evolution, the evaluation
practices that are commonly adopted in the field have undergone a
parallel shift. Indeed, whereas the continuous and explicit nature
of ratings lends itself to the Root Mean Square Error (RMSE) met-
ric; discrete item predictions are more often viewed in a ranking
setup where Information Retrieval (IR) metrics like Precision, Re-
call, and (normalised) Discounted Cumulative Gain (nDCG) have
been widely adopted [ 100]; and the interventionist view lends it-
self particularly well to counterfactual and off-policy estimation
techniques that can be directly mapped to online metrics [ 83,102].
Despite the fact that evaluation methods are a core topic in
recommender systems research that enjoy a significant amount of
interest, some problems remain that are fundamentally hard to solve.
Online experiments (i.e. randomised controlled trials or A/B-tests)
are seen as the gold standard of evaluation practices, and deservedly
so: by leveraging interactions with users, they allow us to directly
measure an array of online metrics for a given recommendation
model [ 59]. Nevertheless, as they are costly to conduct and the
academic research community seldom has access to platforms with
real users [ 43],offline evaluation practices are a common alternative
used to showcase newly proposed methodsâ€™ performance, both
in the research literature and in industry applications (often as a
precursor to an online experiment).
Even though the need for offline evaluation methods that mimic
the outcome of an online experiment is clear, the reality is that
existing methods seldom do so satisfactorily [ 5,30,52,80], even if
advances in counterfactual estimation techniques have recently led
to several success stories [ 31,32]. The reasons for this â€œoffline-online
mismatchâ€ are manifold â€” and some can be attributed due to offline
evaluation inconsistencies (even before we compare them to online
results). First, there is a fundamental mismatch between many
â€œnext-item predictionâ€ metrics (e.g. recall) and online metrics (e.g.
click-through rate). Although this can be partially alleviated by the
interventionist lens [ 50], a majority of published research remains
focused on IR-inspired metrics. Second, a myriad of evaluation
 
1222
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko
options can lead to contrasting results [ 10], and several offline
metrics differ in robustness and discriminative power [ 100]. Third,
sampled versions of these metrics have been adopted for efficiency
reasons, but are inconsistent with their unsampled counterparts and
should thus be avoided [ 9,60,62]. Fourth, multiple recent works
have reported troubling trends in the reproducibility of widely cited
methods [ 11,28,29,76â€“78]â€”similar issues plagued the adjacent IR
field a decade earlier [4].
This article aims to contribute to this important line of research,
by focusing on the widely adopted (normalised) Discounted Cumu-
lative Gain metric [ 42]. Focusing on the purpose that offline metrics
serve, we ask a simple question:
â€œWhen can we expect (n)DCG to accurately approximate
the gold standard outcome of an online experiment? â€
The main scientific contributions we present in pursuit of an-
swering this question, are the following:
(1)We formally present the assumptions that are necessary
to consider DCG an unbiased estimator of online reward,
providing a derivation for this metric from first principles
whilst linking it to off-policy estimation (Â§ 4).
(2)We formally prove that the widespread practice of normalis-
ingthe DCG metric renders it inconsistent with respect to
DCG, in that the ordering given by nDCG can differ from
that given by DCG, and provide empirical evidence (Â§ 5).
(3)Empirical results from off- and online experiments on a large-
scale recommendation platform show that:
(a)the unbiased DCG metric strongly correlates with online
metrics over time, whereas nDCG does not (Â§ 6.1),
(b)whilst differences in online metrics directionally align
with differences in both nDCG and DCG, the latter can en-
joy improved sensitivity to detect statistically significant
online improvements (Â§ 6.2).
(4)We revisit the assumptions that are necessary to consider
DCG an unbiased estimator: discussing when they can be
reasonable and how we can relax them, giving rise to a
research agenda and ways forward (Â§ 7).
2 Background & Related Work
Offline evaluation methods for recommender systems have been
studied for decades [ 35], and their shortcomings are widely reported.
CaÃ±amares et al .provide an overview of common approaches, high-
lighting how different choices (in pre-processing, metrics, splits,...)
lead to contrasting results [ 10]. More problematic, Ji et al .show
that common train-test-split procedures lead to data leakage issues
that affect conclusions drawn from offline experiments [ 53]. Other
recent work shows that sampled versions of evaluation metrics
that only rank a sample of the item catalogue instead of the full
catalogue, are inconsistent with the full metrics, leading the authors
to explicitly discourage their use [ 9,60,62]. Even when we manage
to steer clear from these pitfalls, biases in logged data can give rise
to undesirable phenomena like Simpsonâ€™s paradox [ 39,49]. General
de-biasing procedures have been proposed to this end [ 104], as well
asoff-policy estimation techniques [ 57,65,92,94] and methods to
evaluate competing estimators [86, 93,98].
Most traditional ranking evaluation metrics stem from IR. Val-
carce et al .find that nDCG offers the best discriminative poweramong them [ 100]. Findings like this reinforce the communityâ€™s
trust in nDCG, and it is commonly used to compare novel top- ğ‘›
recommendation methods to the state-of-the-art, also in repro-
ducibility studies [ 28,29,76,77].Ferrante et al .argue that while
nDCG can be preferable because it is bounded and normalised,
problems can arise because the metric is not easily transformed to
an interval scale [ 27]. They all do not consider the consistency of
(n)DCG.
Other recent work highlights that commonly used online evalu-
ation metrics relying on experimental data (e.g. click-through rate),
differ fundamentally from commonly used offline evaluation met-
rics that rely on organic interactions (e.g. hit-rate) [ 43,50].Deffayet
et al.argue that a similar mismatch is especially pervasive when
considering reinforcement learning methods [ 21], and Diaz empha-
sises critical issues with interpreting organic implicit feedback as
a user preference signal [ 22]. These works together indicate a rift
between online and offline experiments.
Several open-source simulation environments have been pro-
posed as a way to bypass the need for an online ground truth re-
sult [ 36,79,82], and several works have leveraged these simulators
to empirically validate algorithmic advances in bandit learning for
recommendation [ 6,46,47,51,87]. Nevertheless, whether conclu-
sions drawn from simulation results accurately reflect those drawn
from real-world experiments, is still an open research question.
In this work, we focus on the corepurpose that offline evaluation
metrics serve: to give rise to offline evaluation methodologies that
accurately mimic the outcome of an online experiment. To this end,
we focus on the widely used (n)DCG metric, and aim to take a step
towards closing the gap between the off- and online paradigms.
3 Formalising the problem setting
Throughout, we represent the domain for a random variable ğ‘‹
asXand a specific instantiation as ğ‘¥, unless explicitly mentioned
otherwise. We deal with a session-based feed recommendation
setup, describing a userâ€™s journey on the platform as a trajectory ğœ.
Contextual features describing a trajectory are encoded in ğ‘¥âˆˆX,
which includes features describing the user ğ‘¢âˆˆU and possible
historical interactions they have had with items on the platform. In
line with common notation in the decision-making literature, we
will refer to these items as actionsğ‘âˆˆA. As is common in real-
world systems, the size of the item catalogue (i.e. the action space
|A|) can easily grow to be in the order of hundreds of millions,
prohibiting us to score and rank the entire catalogue directly. This
is typically dealt with through a two-stage ranking setup, where a
more lightweight candidate generator stage is followed by a ranking
stage that decides the final personalised order in which we present
items to the user [17, 19,65].
We adopt generalised probabilistic notation for two-stage rankers
in this work, but stress that our insights are model-agnostic and
directly applicable to single-stage rankers as well.
LetAğ‘˜denote all subsets of ğ‘˜actions:Ağ‘˜âŠ†2A:âˆ€ğ‘ğ‘˜âˆˆ
Ağ‘˜,|ğ‘ğ‘˜|=ğ‘˜. A candidate generation policy Gdefines a conditional
probability distribution over such sets of candidate actions, given a
context:G(ğ´ğ‘˜|ğ‘‹)BP(ğ´ğ‘˜|ğ‘‹,G).We will use the shorthand nota-
tionG(ğ‘‹)when context allows it. Note that this general notation
subsumes other common scenarios, such as candidate generation
 
1223On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top- ğ‘›Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
policies that are deterministic, consist of ensembles, or simply yield
the entire item catalogue A(i.e.single-stage ranking systems).
After obtaining a set of candidate items for context ğ‘¥by sampling
ğ‘ğ‘˜âˆ¼G(ğ‘¥), we pass them on to the ranking stage.
In line with our probabilistic framework, we have a ranking
policyRthat defines a conditional probability distribution over
rankings (i.e. permutations of ğ‘ğ‘˜). Define ğˆas a possible permu-
tation over ğ´ğ‘˜. Formally, we have: R(ğˆ|ğ´ğ‘˜,ğ‘‹)BP(ğˆ|ğ´ğ‘˜,ğ‘‹,R).
When context allows it, we will use shorthand notation R(ğ‘‹)to
absorb the candidate generation step. Then, the ranker is given by:
R(ğ‘‹)Bâˆ‘ï¸
ğ‘ğ‘˜âˆˆAğ‘˜P(ğˆ|ğ´ğ‘˜=ğ‘ğ‘˜,ğ‘‹,R)P(ğ´ğ‘˜=ğ‘ğ‘˜|ğ‘‹,G). (1)
Now, for a given context ğ‘¥, we can obtain rankings over actions
by sampling ğœ=(ğ‘1,...,ğ‘ğ‘˜)âˆ¼R(ğ‘¥). These rankings are then
presented to users, who scroll through the ordered list of items and
view them along the way. We will not yetrestrict our setup to a
specific user model that describes how users interact with rankings,
but introduce a binary random variable ğ‘‰to indicate whether a
user has viewed a given item.1As such, we will assume that logged
trajectories only contain items that were viewed by the user (i.e.
ğ‘‰=1). That is, if the user abandons the feed after action ğ‘ğ‘–shown
at rankğ‘…=ğ‘–, we do not log samples for actions ğ‘ğ‘—,âˆ€ğ‘—>ğ‘–.
Users can not only view items, but they can interact with them in
several ways. These interaction signals could be seen as the reward
or(relevance) label. Following traditional notation where rewards
areclicks we will denote them by random variable ğ¶. Nevertheless,
these signals are general and can be binary (e.g. likes), real-valued
(e.g. revenue), or higher-dimensional to support multiple objectives
(e.g. diversity, satisfaction, and fairness [67, 68]).
As users can interact with every item separately, we define
ğ¶ğ‘˜=(ğ‘1,...,ğ‘ğ‘˜)as the logged rewards over all ranks. We do
not place any restrictions on the reward distribution yet, so the re-
ward at any given rank can be dependent on actions at other ranks:
(ğ‘1,...,ğ‘ğ‘˜)âˆ¼P(ğ¶ğ‘˜|ğ‘‹,ğ´ 1,...,ğ´ğ‘˜). Now, a trajectory consists of
contextual information, as well as a sequence of user-item inter-
actions with observed reward labels: ğœ={ğ‘¥ğœ,(ğ‘1,ğ‘1),...(ğ‘ğ‘¡,ğ‘ğ‘¡)},
where|ğœ|=ğ‘¡. The true metric of interest that we care about is the
expectation of reward, over contexts sampled from an unknown
marginal distribution ğ‘¥âˆ¼P(ğ‘‹), candidates sampled from our can-
didate generator ğ‘ğ‘˜âˆ¼G(ğ‘¥), rankings sampled from our ranker
(ğ‘1,...,ğ‘ğ‘˜)âˆ¼R(ğ‘ğ‘˜,ğ‘¥), and rewards sampled from the unknown
reward distribution (ğ‘1,...,ğ‘ğ‘˜)âˆ¼P(ğ¶ğ‘˜|ğ‘‹=ğ‘¥,ğ´ 1=ğ‘1,...,ğ´ğ‘˜=
ğ‘ğ‘˜). We will denote with ğ¶the sum of rewards over all observed
ranks:ğ¶=Ã|ğœ|
ğ‘–=1ğ‘ğ‘–. Note that this notation generalises typical eval-
uation metrics that are used in real-world recommender systems,
such as per-item dwell-time or counters of engagement signals.
In order to obtain an estimate for our true metric of interest E[ğ¶],
we can perform an online experiment. Indeed, in doing so, we effec-
tively sample from the above-mentioned distributions and obtain
an empirical estimate of our metric by averaging observed samples.
For a datasetD0containing logged interactions under policies G0
1Note that this problem setting deviates from traditional work dealing with web search
in IR, where multiple items are shown on screen and item-specific view events cannot
be disentangled trivially. In contrast, our items take up most of the userâ€™s mobile screen
when presented, and we are able to deduce accurate item-level view labels from logged
scrolling behaviour. See e.g. Jeunen [45] for work dealing with this setting.andR0, Eq. 2 shows how to obtain this empirical estimate:
E
(ğ‘1,...,ğ‘ ğ‘˜)âˆ¼R 0(ğ‘¥)[ğ¶]â‰ˆ1
|D0|Â·|ğœ|âˆ‘ï¸
ğœâˆˆD 0|ğœ|âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–. (2)
As we directly measure the quantity of interest that depends on the
deployed policiesG0andR0, this online estimator is often seen as
the gold standard. Nevertheless, it is costly to obtain. Indeed, as has
been widely reported [ 31,43,61], online experiments require us to:
(1) bring hypotheses for new policies up to production standards
for an initial test, (2) wait several days or weeks to deduce statistical
significant improvements, and (3) possibly harm user experience
when the policies are performing subpar. Furthermore, several
pitfalls arise when setting up or interpreting results from online
A/B experiments [44, 58].
For these reasons, we want to be able to perform accurate offline
experiments. That is, given a dataset of interactions D0that were
logged under the production policies G0andR0(often referred to
aslogging policies), we want to estimate what the reward would
have been if we had deployed a new policy Rinstead (assumeR
includes sampling candidates from G). The goal at hand is thus
to devise some function ğ‘“that takes in a dataset of interactions
collected under the logging policy, and is able to approximate the
ground truth metric for a target policy R, as shown in Eq. 3:
E
(ğ‘1,...,ğ‘ ğ‘˜)âˆ¼R(ğ‘¥)[ğ¶]?â‰ˆğ‘“(D0,R). (3)
4 Discounted Cumulative Gain as an unbiased
offline evaluation metric
In reality, this problem can be very complex. Indeed, the reward
that we obtain from presenting a certain ranking to a user can
depend on the entire slate at once (of which there are ğ‘›!versions
in a top-ğ‘›setting), and will be non-i.i.d. over trajectories (i.e. the
reward distribution can depend on a userâ€™s state, influenced by
actions we have taken in the past).
As is typical in machine learning research, we require assump-
tions that make the problem more tractable. These assumptions are
flawed, but they give us a starting point and a strong foundation to
build upon for future iterations. Note that we will lay out the specific
assumptions that are necessary to motivate the use of Discounted
Cumulative Gain as an offline evaluation metricâ€”we discuss ways
of relaxing these assumptions in Section 7.
Assumption 1 (reward independence across trajectories).
The reward for a context-action pair (ğ‘‹,ğ´ğ‘–)in trajectory ğœis inde-
pendent of the rankings presented in other trajectories ğœâ€²âˆˆD\ğœ.
Assumption 2 (position-based model [ 18]).We follow the
position-based model (PBM) to describe user scrolling behaviour, im-
plying that the probability of a user viewing an item is only dependent
on its rank and described by P(ğ‘‰|ğ‘…).
Assumption 3 (reward independence across ranks). The
reward for a context-action pair (ğ‘‹,ğ´ğ‘–)is independent of other actions
in the user trajectory. Formally, ğ¶ğ‘–âŠ¥âŠ¥ğ´ğ‘—|ğ‘‹,ğ´ğ‘–âˆ€ğ‘—â‰ ğ‘–. We describe
the resulting reward distribution as P(ğ¶ğ‘–|ğ‘‹,ğ´ğ‘–,ğ‘…).
Assumption 4 (examination hypothesis [ 18]).The reward for
an actionğ´shown at rank ğ‘…is dependent on its inherent quality
 
1224KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko
(unobserved random variable ğ‘„), and whether it was viewed ğ‘‰. These
quantities relate as:
P(ğ¶|ğ‘‹,ğ´,ğ‘…)=P(ğ‘„|ğ‘‹,ğ´)Â·P(ğ‘‰|ğ‘…),
which implies P(ğ‘„|ğ‘‹,ğ´)=P(ğ¶|ğ‘‹,ğ´,ğ‘…)
P(ğ‘‰|ğ‘…).(4)
Asm. 1allows us to avoid reinforcement learning scenarios, and
Asm. 2prohibits cascading behaviour (that would give rise to other
metrics, such as ERR [ 12]). Asm. 3allows us to avoid modelling
entire slates as individual actions (leading to a combinatorial ex-
plosion of the action space), and through Asm. 4, Eq. 4estimates
the unobserved context-dependent quality of a given item from
observable quantities alone. From Eq. 2, we can now rewrite the
expected reward we obtain under a ranking policy Ras:
E
(ğ‘1,...,ğ‘ ğ‘˜)âˆ¼R(ğ‘¥)[ğ¶]â‰ˆ
1
|D0|Â·|ğœ|âˆ‘ï¸
ğœâˆˆD 0|ğœ|âˆ‘ï¸
ğ‘–=1P(ğ‘„=1|ğ‘‹=ğ‘¥ğœ,ğ´=ğ‘ğ‘–)Â·P(ğ‘‰=1|ğ‘…=ğ‘–).(5)
Through the position-based model and the examination hypothesis,
items shown at lower ranks are discounted. Because we assumed
independence of rewards across ranks, rewards observed at different
ranks are cumulative. A key insight here is that the ranking policy
Ronly affects the rank ğ‘–at which an item is shown, and as such, the
exposure probability that is allocated to the item ğ‘ğ‘–. We formalise
exposure as the expected number of views a target item ğ‘â€²will obtain
under a given context ğ‘¥, for candidate generation and ranking
policiesG,R:
E
G,R[ğ‘‰|ğ‘‹=ğ‘¥,ğ´=ğ‘â€²]=âˆ‘ï¸
ğ´ğ‘˜âˆˆAğ‘˜ 
G
ğ´ğ‘˜|ğ‘‹=ğ‘¥
Â·
âˆ‘ï¸
(ğ‘1,...,ğ‘ ğ‘˜)âˆˆğ‘†(ğ´ğ‘˜)R
(ğ‘1,...,ğ‘ğ‘˜)|ğ´ğ‘˜,ğ‘‹=ğ‘¥ğ‘˜âˆ‘ï¸
ğ‘–=1P(ğ‘‰=1|ğ‘…=ğ‘–)Â·1{ğ‘ğ‘–=ğ‘â€²}!
.
(6)
Note that this general notation accommodates recommendation
scenarios where we retrieve and rank ğ‘˜candidates but only show
the top-ğ‘›to the user, where ğ‘˜>ğ‘›, by simply defining P(ğ‘‰=
1|ğ‘…=ğ‘—)=0âˆ€ğ‘—=ğ‘›+1,...,ğ‘˜ . By encoding cut-offs directly in the
position bias model, we forgo the need to consider metrics like
DCG@ğ‘›[100].
Recent work in unbiased learning-to-rank leverages similar ex-
posure definitions to jointly combat selection and position bias
through Inverse Propensity Score (IPS) weighting [33, 73].
Assumption 5 (full support of the logging policy [ 74]).
Given context ğ‘¥, any item that would be assigned non-zero exposure
under the target policies (G,R)has non-zero exposure under the
logging policies(G0,R0):
âˆ€ğ‘âˆˆA :E
G,R[ğ‘‰|ğ‘‹=ğ‘¥,ğ´=ğ‘]>0â‡’E
G0,R0[ğ‘‰|ğ‘‹=ğ‘¥,ğ´=ğ‘]>0.
(7)
Through Asm. 5and Eq. 6, we can now formulate an impor-
tance sampling estimator for the reward under (G,R)given data
collected under(G0,R0). Letğœ€(ğ‘¥,ğ‘)BEG,R[ğ‘‰|ğ‘‹=ğ‘¥,ğ´=ğ‘]and
ğœ€0(ğ‘¥,ğ‘)BEG0,R0[ğ‘‰|ğ‘‹=ğ‘¥,ğ´=ğ‘].
E
G,R[ğ¶]=E
G0,R0
ğ¶Â·ğœ€
ğœ€0
â‰ˆ1
|D0|Â·|ğœ|âˆ‘ï¸
ğœâˆˆD 0|ğœ|âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–Â·ğœ€(ğ‘¥ğœ,ğ‘ğ‘–)
ğœ€0(ğ‘¥ğœ,ğ‘ğ‘–).(8)Eq.8provides a general unbiased estimator for the reward under
(G,R), computed from data collected under (G0,R0).2For sim-
plicity of notation, but without loss of generality, we now restrict
ourselves to a deterministic ranking policy Rand assumeGis con-
stant (i.e.Gâ‰¡G 0). With a slight abuse of notation, we briefly denote
withR(ğ‘¥,ğ‘)therank at which item ğ‘is placed when policy Ris pre-
sented with context ğ‘¥. In doing so, we observe that the importance
weights in Eq. 8can be simplified to:ğœ€(ğ‘¥,ğ‘)
ğœ€0(ğ‘¥,ğ‘)=P(ğ‘‰=1|ğ‘…=R(ğ‘¥,ğ‘))
P(ğ‘‰=1|ğ‘…=R0(ğ‘¥,ğ‘)).
We stress again that we simply adopt this view for simplified no-
tation, but that the derivation holds for general stochastic two- or
single-stage ranking systems alike.
Recall from Eq. 4 that P(ğ‘„|ğ‘‹,ğ´)=P(ğ¶|ğ‘‹,ğ´,ğ‘…)
P(ğ‘‰|ğ‘…).
Now, we can formally describe the discounted cumulative gain
(DCG) metric as an importance sampling estimator:
E
(ğ‘1,...,ğ‘ ğ‘˜)âˆ¼R(ğ‘¥)[ğ¶]â‰ˆğ‘“DCG(D0,R)
=1
|D0|Â·|ğœ|âˆ‘ï¸
ğœâˆˆD 0|ğœ|âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–Â·P(ğ‘‰=1|ğ‘…=R(ğ‘¥ğ‘–,ğ‘ğ‘–))
P(ğ‘‰=1|ğ‘…=ğ‘–)
â‰ˆ1
|D0|Â·|ğœ|âˆ‘ï¸
ğœâˆˆD 0|ğœ|âˆ‘ï¸
ğ‘–=1P(ğ‘„|ğ‘‹=ğ‘¥ğœ,ğ´=ğ‘ğ‘–)Â·P(ğ‘‰=1|ğ‘…=R(ğ‘¥ğ‘–,ğ‘ğ‘–)).
(9)
Through this derivation, two different views of the DCG metric
arise. That is, we either (1) view it as a pure importance sampling
estimator that reweights the exposure that is allocated to a certain
item in a certain context, or (2) we view it as a way to de-bias
observed interactions (i.e. estimate ğ‘„fromğ¶andğ‘‰), and use the
position-based model (Asm. 2) and the examination hypothesis
(Asm. 4) to obtain a final estimate of the cumulative reward.
If the assumptions laid out above hold, Eq. 9provides an unbi-
ased estimate of the online reward policy Rwill incur, based on
data collected under R0; providing a strong motivation for DCG.
Even though unbiasedness is an attractive theoretical property, this
estimatorâ€™s variance can become problematic in cases where the
logging and target policies (R0,R)diverge. We can adopt meth-
ods that were originally proposed to strike a balance between bias
and variance for general IPS-based estimators, such as clipping the
weights [ 31,38], self-normalising them [ 95], adapting the logging
policy [ 97], or extending the estimator with a reward model to
enable doubly robust estimation [ 24,57,72]. Similarly, when the
logged data that is used for offline evaluation was collected by multi-
ple logging policies, ideas from â€œmultiple importance sampling â€ [25]
are effective at reducing the variance of the final estimator [1, 55].
Saito et al. describe extensions for large action spaces [84, 85].
In the traditional IR use-case of web search, it is often assumed
that we have access to human-annotated relevance labels rel(ğ‘,ğ‘‘)
for query-document pairs (ğ‘,ğ‘‘). Such crowdsourced labels are seen
as a proxy to P(ğ‘„|ğ‘‹,ğ´), which makes them understandably at-
tractive. Nevertheless, for an offline evaluation metric to be useful
in real-world recommendation systems, access to direct relevance
labels is seldom a realistic requirement. The discount function for
DCG that is most often used in practice, makes the assumption that
2Note that, for general two-stage ranking scenarios, this is a novel contribution to the
research literature in and of itself. Existing work on off-policy corrections in two-stage
recommender systems only considers a top-1 scenario, instead of a ranking policy [ 65].
 
1225On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top- ğ‘›Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
P(ğ‘‰=1|ğ‘…=ğ‘–)=1
log2(ğ‘–+1)is a good approximation for empirical
exposure (see, e.g., [ 12]). This gives rise to the more widely recog-
nisable form of DCG, asÃğ‘˜
ğ‘–=1rel(ğ‘,ğ‘‘ ğ‘–)
log2(ğ‘–+1). We note that neither one of
these additional assumptions is likely to hold in real-world applica-
tions, which imply that even if Assumptions 1â€“5hold, this estimator
isbiased. The more general form presented in Eq. 9, however, is
supported by a theoretical framework that allows for counterfactual
evaluation, formally describing settings for its appropriate use.
5 Normalising DCG is Inconsistent
We have introduced the DCG metric from first principles, and have
shown that under several assumptions, it can be seen as an unbiased
estimator for the reward that a new ranking policy Rwill obtain.
Nevertheless, this metric seldom appears in the research literature
in its unadulterated form. Much more prevalent is normalised DCG
(nDCG), which rescales the DCG metric to be at most 1for an
â€œidealâ€ ranking. For notational simplicity, we define ground truth
quality labels as ğœŒ(ğ‘¥)=E[ğ‘„|ğ‘‹=ğ‘¥,ğ´=ğ‘]âˆ€ğ‘âˆˆA
. Then, we can
compute ideal DCG as the DCG obtained under an oracle ranker
that yieldsRâ˜…Bsort(âˆ’ğœŒ(ğ‘¥)). In what follows, we first show that
this is reasonable practice under a single sample (context), in that
it retains a consistent ordering among ranking policies. We then
go on to show that: (1) nDCG yields inconsistent orderings over
competing policies in expectation when compared to DCG, and
(2) defining iDCG is problematic in the realistic setting of partial
information (i.e. ğ‘„and thusğœŒ(ğ‘¥)are unobservable).
Lemma 5.1. The Discounted Cumulative Gain (DCG) and Nor-
malised Discounted Cumulative Gain (nDCG) metrics yield consistent
relative orders over a competing set of policies Î©that are being eval-
uated for a single sample ğ’™. That is,
arg sort
RâˆˆÎ©ğ‘“DCG(ğ‘¥,R)â‰¡ arg sort
RâˆˆÎ©ğ‘“nDCG(ğ‘¥,R),âˆ€ğ‘¥âˆˆX.
Proof. For any given context ğ‘¥âˆˆX, assume a relative order
exists between two methods RandRâ€²for a given non-negative
metricğ‘“: i.e.ğ‘“(ğ‘¥,R)â‰¥ğ‘“(ğ‘¥,Râ€²). Defineğ‘“â˜…(ğ‘¥)as the ideal metric
value, i.e. the metric value that is obtained by the optimal ranking:
ğ‘“â˜…(ğ‘¥)Bğ‘“(ğ‘¥,Râ˜…), whereRâ˜…=arg max
Rğ‘“(ğ‘¥,R)=sort(âˆ’ğœŒ(ğ‘¥)).
Becauseğ‘“is a non-negative metric, ğ‘“â˜…is non-negative, and we
have that:
ğ‘“(ğ‘¥,R)â‰¥ğ‘“(ğ‘¥,Râ€²)â‡’ğ‘“(ğ‘¥,R)
ğ‘“â˜…(ğ‘¥)â‰¥ğ‘“(ğ‘¥,Râ€²)
ğ‘“â˜…(ğ‘¥).
DCG is, in general, not restricted to be non-negative. However,
if we assume that the ideal discounted cumulative gain is non-
negative, we have that the above inequality applies for ğ‘“Bğ‘“DCG
andğ‘“â˜…Bğ‘“iDCG . â–¡
We believe that this (seemingly trivial) insight has led to the
widespread adoption of nDCG as an offline evaluation metric in
the recommender systems and Learning-to-Rank research fields,
as normalised metric values where 1indicates a perfect model
facilitate comparisons of methods over different datasets. Indeed,
when describing its prevalence in the literature, Ferrante et al .argue
that â€œusually nDCG is preferred over DCG because it is bounded and
normalised â€ [27]. Nevertheless, nDCG does notretain consistentorderings with respect to DCG when the metrics are calculated
over multiple samples and aggregated:
Lemma 5.2. The Discounted Cumulative Gain (DCG) and Nor-
malised Discounted Cumulative Gain (nDCG) metrics yield inconsis-
tent relative orders over a competing set of policies Î©that are being
evaluated over a set of samples ğ‘¿. That is,
arg sort
RâˆˆÎ©ğ‘“DCG(ğ‘¿,R).arg sort
RâˆˆÎ©ğ‘“nDCG(ğ‘¿,R)in the general case .
Proof. We provide a proof by counterexample, for which the
details are presented in Table 1. Indeed, even though the ğ‘“DCG and
ğ‘“nDCG metrics align for every sample in isolation (see columns for
ğ‘¥1,ğ‘¥2), they are inconsistent in aggregate (see columns for ğ‘¿).â–¡
Discrepancies between (n)DCG have been touched upon in the
IR literature, focused on search engine evaluation and blaming
â€œa limited number of relevance judgments â€ [3]. Table 1shows that
the issue has deeper roots than this: the normalisation procedure
isinconsistent. This insight is problematic, as virtually all offline
evaluation protocols consist of first aggregating evaluation metrics
over sets of samples, and then inferring preferences over competing
policies based on these metric averages. Our work shows that, when
the assumptions laid out in Section 4are met and DCG provides an
unbiased estimate of reward, this gives rise to a theoretically sound
model selection protocol. The same statement does nothold for
nDCG as it widely appears in the research literature (see e.g. [ 88,
Eq. 8.9] and [ 100, Eq. 5]). We hypothesise that the normalisation
formula has been widely adopted for its ease-of-use, rather than
for its theoretical properties. This suggests that nDCG is of limited
practical use as an offline evaluation metric, and that it should be
avoided by researchers and practitioners who wish to use DCG for
offline evaluation and model selection purposes.
We provide empirical evidence of discrepancies between (n)DCG
on common top- ğ‘›recommendation evaluation tasks on publicly
available data in Appendix A.
In the odd case where metric values that maximise at 1are re-
quired, we propose the use of a post-normalisation procedure, where
ğ‘“pnDCG(D,R)=ğ‘“DCG(D,R)
ğ‘“iDCG(D). Recent work on LTR in IR leverages
this nDCG formulation [ 71]. Indeed, through Lemma 5.1, one can
trivially show that this metric isconsistent with respect to ğ‘“DCG.
Nevertheless, we wish to advise against this practice altogether,
as computing the ideal DCG metric implies that we must construct
theideal ranking policyRâ˜…. To do so, we require full knowledge of
ğœŒ(ğ‘¥), which is hardly realistic in real-world scenarios. In traditional
IR use-cases where human-annotated relevance labels are available
asground truth, these labels can be used to inform ğœŒ(ğ‘¥). In aca-
demic recommendation datasets where we have explicit feedback
andfull observability of the user-item matrix, this can inform ğœŒ(ğ‘¥)
similarly. In practical applications, however, we typically estimate
bğœŒ(ğ‘¥)â‰ˆğœŒ(ğ‘¥)from logged implicit feedback. Aside from the prob-
lems that occur when accurately interpreting this feedback [ 22],
such logged datasets are known to be riddled with biases that com-
plicate estimating bğœŒ(ğ‘¥)[46], resulting in only partial observability
and noisy estimates that should notbe taken at face value to inform
an â€œoptimal â€ ranker.
One final argument in favour of nDCG, is that it partially alle-
viates the impact of outliers. Indeed, the normalisation procedure
 
1226KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko
Top-1 Model DCG( ğ‘¥1)DCG(ğ‘¥2)nDCG(ğ‘¥1)nDCG(ğ‘¥2) DCG(X) nDCG(X)
R(ğ‘¥)=ğ‘1 1.00 1.00 1.00 0.29 1.00 0.64
Râ€²(ğ‘¥)=ğ‘2 0.00 2.50 0.00 0.71 1.25 0.36where E[ğ‘„|ğ‘‹=ğ‘¥1,ğ´=ğ‘1]=1.0,
E[ğ‘„|ğ‘‹=ğ‘¥1,ğ´=ğ‘2]=0.0,
E[ğ‘„|ğ‘‹=ğ‘¥2,ğ´=ğ‘1]=1.0,
E[ğ‘„|ğ‘‹=ğ‘¥2,ğ´=ğ‘2]=2.5,
X={ğ‘¥1,ğ‘¥2}.
Table 1: A proof by example that, while rankings inferred from the DCG and nDCG metrics are consistent for a single sample,
they can be inconsistent when aggregated over multiple samples (i.e. DCG(ğ‘¿,Râ€²)>DCG(ğ‘¿,R)â‡nDCG(ğ‘¿,Râ€²)>nDCG(ğ‘¿,R)).
rescales the contribution of every sample, which can be preferable
in cases where strong outliers are present. Nevertheless, in such
scenarios, we would propose to first devise a more appropriate
online metric than the average cumulative reward, and then derive
an offline estimator for this quantity, rather than trying to repur-
pose the existing DCG estimator. Exactly what such metrics and
estimators would look like, is an interesting area for future work.
6 Experimental Results & Discussion
Until now, we have derived the theoretical conditions that are nec-
essary to consider DCG an unbiased estimator of online reward, and
we have highlighted both theoretically and empirically that nDCG
deviates from this. In what follows, we wish to empirically validate
whether we can leverage the metrics effectively to estimate online
reward for deployed ranking policies, for recommender systems
running on large-scale web platforms. The research questions we
wish to answer with empirical evidence, are the following:
RQ1 Are offline evaluation results obtained using DCG correlated
with online metrics from deployed models?
RQ2 Are offline evaluation results obtained using normalised DCG
correlated with online metrics from deployed models?
RQ3 Does the proposed unbiased DCG formulation with learnt posi-
tion biases and de-biased interaction labels improve correlation
with online metrics over the classical and widely adopted (yet
biased) DCG formulation?
RQ4 Are differences in any of the considered offline evaluation met-
rics predictive of differences in online metrics?
We focus on correlations between off- and online metrics rather
than exact estimation error, because downstream business logic
prevents the model output to exactly match online behaviour [ 41].
To provide empirical evidence for research questions 1â€“4, we re-
quire access to a ground truth online metric. There are two families
of evaluation methods we can consider to obtain this: (1) simula-
tion studies, or (2) online experiments. Both come with their own
(dis)advantages. Indeed, simulation studies are generally repro-
ducible and allow full control over the environment to investigate
which of the assumptions laid out in the theoretical sections of
this work are necessary to retain DCGâ€™s utility as an online metric.
Nevertheless, simulations require us to make additional assump-
tions about user behaviour, that are often non-trivial to validate. As
a result, they would provide no empirical evidence on real-world
value of the metrics, and are limited in the insights they can bring.
Online experiments, on the other hand, are harder to reproduce.
Notwithstanding this, they allow us to directly measure real user
behaviour and give a view of the utility of the DCG metric for
offline evaluation purposes in a real-world deployed system. This
can guide practitioners who need to perform such evaluations. Wefocus on the online family for the remainder of this work, noting
that simulations provide an interesting avenue for future research.
We use data from a large-scale social media platform that utilises
a two-stage ranking system as described earlier to present users
with a personalised feed of short videos they might enjoy. The plat-
form operates a hierarchical feed where users are presented with
a 1st-level feed they can scroll through and engage with content,
and users can enter a 2nd-level â€œmore-like-this â€ feed via any given
1st-level item.3Because of the differences in interaction modalities
and the user interface between the two feeds, they require sepa-
rate models to estimate position bias, and we separate them in our
analysis. The 1st-level feed adopts a recently proposed probabilis-
tic position bias model [ 45], whereas the 2nd-level feed adopts an
exponential form (such as the one underlying rank-biased preci-
sion [ 70]). Because of this difference, the importance weights in the
2nd-level feed exhibit much larger variance, and we adopt a clipping
parameter for IPS which we set at 200to compute the de-biased
DCG metric on this data (and vary it in Section 6.2). Rewards on a
short-video platform can be diverse. We collect both implicit signals
(e.g. watching a video until the end) and explicit signals (e.g. liking
a video), and consider both types of rewards for our on- and offline
metrics, referring to them as CimpandCexprespectively.
6.1 Offlineâ€“Online Metric Correlation (RQ1â€“3)
We collect data over a week of a deployed online experiment with
over 40 million users where we deployed a change to a deterministic
ranking policyR, and kept the candidate generator Gfixed. This
simplifies the exposure calculation that is used in the offline evalu-
ation metrics in Eq. 6to that in Eq. 9, and ensures that the variance
of the offline estimator is lower. For every day ğ‘‘in the experiment,
we (1) aggregate online results per day ğ‘‘as the average number
of logged positive feedback samples per session, and (2) collect a
datasetDğ‘‘
0which we use to compute offline metrics (through Eq. 9
and variants thereof). Then, we compute Pearsonâ€™s correlation co-
efficient between the series of ğ‘‘online metrics from (1), and the ğ‘‘
offline estimates from (2), for competing evaluation metrics. Table 2
presents results from this experiment, with a detailed description
in the caption. Results here align with what theory would suggest:
the unbiased DCG variant that we have formally derived in Sec-
tion 4provides the strongest correlation with online reward. Both
adopting a learnt position bias model as opposed to the classical
logarithmic form, and de-biasing observed interaction labels as
opposed to naÃ¯vely using them, have a significant effect on the per-
formance of the final estimator. On the 1st-level feed, we observe
that our estimator works especially well with an explicit reward
3Examples of well-known social media platforms that operate similar user interfaces
include Instagram, Reddit, and ShareChat.
 
1227On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top- ğ‘›Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
DCG nDCG
log,C log,Ë†Q pbm, C pbm, Ë†Q pbm, Ë†Q
ğ’“ ğ’‘-val ğ’“ ğ’‘-val ğ’“ ğ’‘-val ğ’“ ğ’‘-val ğ’“ ğ’‘-val
1st-levelğ¶expR00.97â€ <0.01 0.96â€ <0.01 0.97â€ <0.01 0.98â€ <0.01âˆ’0.91â€ 0.01
Rğ‘¡0.86â€ 0.03 0.86â€ 0.03 0.87â€ 0.03 0.91â€ 0.01âˆ’0.80 0.06
ğ¶impR00.57 0.24 0.70 0.12 0.57 0.24 0.70 0.12âˆ’0.80 0.06
Rğ‘¡0.05 0.92 0.17 0.73 0.05 0.92 0.17 0.75âˆ’0.32 0.55
2nd-levelğ¶expR00.75 0.14 0.71 0.18 0.77 0.13 0.79 0.11âˆ’0.42 0.48
Rğ‘¡0.28 0.65 0.25 0.68 0.38 0.53 0.49 0.40âˆ’0.34 0.57
ğ¶impR00.10 0.88 0.58 0.30 0.48 0.41 0.85 0.07âˆ’0.93â€ 0.02
Rğ‘¡0.55 0.34 0.88â€ 0.05 0.70 0.19 0.88â€ 0.05âˆ’0.77 0.13
Table 2: Correlation between online reward as measured from an A/B-test and offline evaluation metrics. We consider both
explicit andimplicit reward signals, on two levels of a hierarchical feed structure on a short-video platform. We consider DCG
with a logarithmic discount (log) and a learnt model (pbm); using interaction signals directly ( ğ¶) or de-biasing them to estimate
Ë†ğ‘„=ğ¶/P(ğ‘‰|ğ‘…); for both a logging and target policy R0,Rğ‘¡. We report Pearsonâ€™s correlation coefficient ğ‘Ÿand a two-tailed ğ‘-value
using Studentâ€™s correlation test [91]. Statistically significant correlations ( ğ‘<0.05) are markedâ€ , best performers are bold.
signal. This somewhat reverses for the 2nd-level feed, where the
implicit reward signal leads to a stronger correlation with online
results. We hypothesise that this is directly related to the degree
with which the assumptions laid out in Section 4are violated. Users
leave the 1st-level feed and enter the 2nd-level feed if they click on
a 1st-level video. As such, the implicit reward Cimpof succesfully
watching a video is highly dependent on other items in the feed, vio-
lating Asm. 3(reward independence across ranks), as well as Asm. 2
(absence of cascading behaviour). Note that all assumptions are
expected to be violated to some degree â€” but the strongly positive
correlation results presented in Table 2are promising.
Somewhat surprisingly, not only does (unbiased) normalised
DCG exhibit worse correlation than DCG, it provides a strongly
negative correlation. At first sight, this seems troubling. Neverthe-
less, we provide an intuitive explanation and highlight that this
result alone does notimply that the metric cannot be useful for
offline evaluation purposes. Note that discrepancies stemming from
the normalisation procedure have a disproportionate impact when
the reward is unevenly distributed across sessions or days. Suppose
we observe two sessions: one with a single positive label over two
impressions, and one with 10 positive labels over 1 000 impressions.
Because DCG deals with absolute numbers, the second session will
bear a weight proportional to its positive reward. Normalised DCG,
on the other hand, considers the relative distance to the optimal
ranking. If we reasonably assume that our ranking model is im-
perfect, the distance to the optimal ranking is likely to be higher
for the second session, and nDCG will be lower as a result (even
though we have higher DCG). This same argument can be made
across different days in the experiment, explaining poor correla-
tion results over time. Notwithstanding this, it does not necessarily
imply that nDCG holds no merit as an offline evaluation metric. In
what follows, we consider a more important question, focusing on
differences in online metrics instead.6.2 Offlineâ€“Online Metric Sensitivity (RQ4)
To consider this research question, we restrict ourselves to a setting
where we know that strong statistically significant ( ğ‘â‰ª0.001)
improvements in online metrics are observed for the target policies
{Gğ‘¡,Rğ‘¡}over the logging policies {G0,R0}. We restrict our analy-
sis to the 2nd-level feed, as it generates the majority of user-item
interactions and relatively long sessions. We consider a variety of
explicit and implicit feedback signals as rewards, all of which saw
statistically significant improvements in the online experiment. We
consider three possible notions of alignment between on- and offline
metrics, in increasing levels of expressivity: (1) Without considering
statistical significance, do differences in offline metrics directionally
align with differences in online metrics? (i.e. sign agreement ) (2) For
statistically significant improvements (as validated by the online ex-
periment with ğ‘â‰ª0.001), does the offline metric show statistically
significant improvements with ğ‘<0.01? (i.e. True-Positive-Rate,
recall or sensitivity). (3) For statistically significant improvements,
what confidence does the offline metric have in the improvements?
(i.e. theğ‘-values). We consider 5 days of a deployed online experi-
ment and 4 different reward signals, yielding 20 distinct statistically
significant online metric improvements. The purpose of our offline
estimators, is to reflect these statistically significant online differ-
ences in their offline estimates. As offline estimators, we consider a
clipped variant of the unbiased DCG metric in Eq. 8, where the fac-
tor for the inverse exposure propensity is replaced with min
ğ‘š,1
ğœ€0
,
for varying values of ğ‘š[31,38]. Although this renders the metric
biased in a pessimistic way [ 48], its reduced variance can yield more
favourable performance as an offline evaluation metric. For every
variant of the DCG metric we construct in this way, we compute
the analogous normalised DCG metric. Because these metrics are
aggregated over trajectories, we can use their empirical means and
standard deviations to construct normal confidence intervals for the
metric values and their differences (i.e. the treatment effect). If the
99% confidence interval for the metric difference is strictly positive,
we say the metric indicates a statistically significant improvement
 
1228KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko
100101102103
Max. IPS Weight ( m)0.00.20.40.60.81.0True-Positive-Rate
DCG
nDCG
inf0.00.20.40.60.81.0
(a) True-Positive-Rate for offline metrics ( â†‘).
100101102103
Max. IPS Weight ( m)10âˆ’1010âˆ’710âˆ’410âˆ’1Averagep-value
DCG
nDCG
inf10âˆ’1010âˆ’710âˆ’410âˆ’1 (b) Average ğ‘-values for offline metrics ( â†“).
Figure 1: Sensitivity measures (y-axis ) of (n)DCG for varying values of the capping parameter in IPS (x-axis ).
(withğ‘<0.01). Because all online metric differences we consider
were statistically significant, we know that they are true positives,
and we can compute the sensitivity orTrue-Positive-Rate (TPR) for
the offline metric by counting how often it indicates a statistically
significant offline difference for these true positives. We additionally
record the average ğ‘-value for the null hypothesis (i.e. the hypothe-
sis that the metric difference is â‰¤0), obtained from the confidence
intervals. To measure the weaker notion of sign agreement, we only
consider the mode of the confidence interval ğœ‡, and count â€œagree-
mentâ€ iffğœ‡>0. We vary the clipping hyper-parameter for IPS as
ğ‘šâˆˆ{1,2,4,8,16,32,64,128,256,512,2048,4096,inf}, whereğ‘š=1
corresponds to directly using the interaction labels ğ¶, andğ‘š=inf
yields an unbiased estimator with higher variance.
Reassuringly, alloffline estimators exhibit 100% directional sign
agreement with the true treatment effect we observe for serv-
ing personalised recommendations to users through {Gğ‘¡,Rğ‘¡}over
{G0,R0}. Results for our sensitivity analysis are visualised in Fig-
ure1. In Figure 1a, higher values indicate that the offline evaluation
metric is more likely to detect statistically significant improvements
in the online metric, averaged over the 20 settings described above.
Analogously, lower values in Figure 1bindicate that the metric
yields more statistical confidence. Lower ğ‘-values in Figure 1bad-
ditionally imply that the metric requires less data to achieve the
significance level, potentially reducing costs [ 56]. We observe that
introducing IPS weighting (i.e. ğ‘š>1) to account for position bias
in the logged interactions leads to improved sensitivity. This results
holds for both DCG and nDCG, and both for the TPR metric and the
averageğ‘-values. We additionally observe that for a wide range of
clipping values, the DCG metric has a higher TPR (lower ğ‘-values)
than nDCG. Intuitively, this can be explained by the fact that nDCG
essentially squashes a metric with a high expressive range to the
[0,1]domain, which can only come at a cost of discriminative
power. DCG, on the other hand, directly models the online metrics
we care about (under the assumptions laid out in Section 4).
When we do not clip the IPS weights (i.e. ğ‘š=inf), we observe
from Figure 1that the variance of the DCG metric increases to a
point where its sensitivity is harmed (even if directional alignment
is maintained). Note that the nDCG metric is not affected by this, as
its values are bounded and they exhibit lower variance as a result.
Intuitively, whereas low values of ğœ€0can blow up the unbiased DCG
formulation, they will also do this for ideal DCG, and their ratio (i.e.
nDCG) will be less likely to suffer from this. We observe that withclipped propensities, even at large values, DCG leads to superior
sensitivity over nDCG, striking a favourable bias-variance trade-off.
Our experimental results show promise in using DCG as an
offline estimator of online reward, and the bias-variance trade-off
that is clearly visualised in Figure 1bhelps us to tune this hyper-
parameterğ‘šproperly. Even when allof the metricâ€™s underlying
assumptions violated to some degree, its value is apparent.
7 Perspectives going forward
In what follows, we revisit the assumptions that are necessary to
consider the DCG metric an unbiased estimator of online reward.
1. Reward independence across trajectories is necessary to avoid
having to model any internal user state that is influenced by actions
taken by a ranking policy. Indeed, if we do allow this to happen,
we must resort to Reinforcement Learning (RL) formulations of our
problem, which inhibits the simple form that DCG allows. Neverthe-
less, unbiased evaluation of RL policies is an active research area,
which has found applications in recommendation research [ 14],
also for two-stage policies (without considering rankings) [ 65].Ie
et al.can provide inspiration for learnt RL policies in top- ğ‘›recom-
mendation domains with DCG-like reward structures [37].
2. Position-based model (PBM). The classical PBM allows for gen-
eral formulations of P(ğ‘‰|ğ‘…), including the widely adopted func-
tional form1
log2(ğ‘–+1). The recently proposed Contextual PBM [ 26]
can be plugged into Eq. 6to directly provide an unbiased DCG
formulation with a context-dependent discount function, enjoying
the same theoretical guarantees we have derived for DCG under the
PBM. A variety of other click models [ 15] have been proposed in
the research literature [ 8,13], as well as ways to evaluate them [ 20].
We expect that our work provides a basis for further connections to
be drawn between click models and unbiased evaluation metrics.
3. Reward independence across ranks. When we do not assume
any structure between the actions taken by the ranking policy and
the observed rewards, the problem quickly becomes intractable, as
we suffer from a combinatorial explosion of the action space. This is
a well-known problem, and the independence assumption has been
adopted (either explicitly or implicitly) by a wide array of related
work [ 6,37,47,96]. Note that this assumption does not simply relate
toobserving rewards, but to the underlying distribution of ğ‘„. Indeed,
related work that adopts a cascading user behaviour model also
relies on this assumption, as the cascade relates to the distribution of
ğ‘‰(and thusğ¶) rather than that of ğ‘„[57,66]. Evaluation metrics have
 
1229On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top- ğ‘›Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
been proposed to encode concepts of listwise novelty and diversity
into DCG-like formulations for top- ğ‘›recommendations [ 16,75];
we conjecture they can be extended to our unbiased setup as well.
4. The examination hypothesis implies that exposure bias (through
ğ‘‰) is the main culprit that makes ğ¶a noisy indicator of ğ‘„. Differ-
ences in exposure can then purely come from position bias (as in the
PBM), but they can also be perpetuated by selection bias (as made
evident by Eq. 6) [23,47]. Other sources of bias have been raised in
the literature, such as presentation or trust bias [ 2,101]. We expect
that these types of biases can be incorporated into the theoretical
derivation of Sec. 4to devise DCG-like formulations that remain
unbiased estimators of online reward, even when additional biases
are present. Naturally, such biases are use-case-specific.
5. Full support of the logging policy. The main assumption that
makes IPS work, is that no actions with non-zero probability under
the target policy can have zero probability under the logging policy.
Indeed, if a context-action pair is known not to be present in the
data, we cannot make any inferences about its reward (with guaran-
tees). This is at the heart of policy-based estimation, but especially
problematic in real-world systems where the action space is large
and the cost of such fullrandomisation, even with small probabil-
ities, can be high. Recent work in learning from bandit feedback
deals with such cases empirically [ 46,51] and theoretically [ 63,81],
providing a source of inspiration to (partially) alleviate these issues.
8 Conclusions & Outlook
Offline evaluation of recommender systems is a common task, and
known to be problematic. This work investigates the commonly
used (normalised) discounted cumulative gain metric and its uses in
the research literature. Specifically, we have investigated when we
can expect such metrics to approximate the gold standard outcome
of an online experiment. In a counterfactual estimation framework,
we formally derived the necessary assumptions to consider DCG
an unbiased estimator of online reward. Whilst it is reassuring that
such assumptions exist and we can directly map DCG to online
metrics â€” we also highlighted how this ideal use deviates from the
traditional uses of the metric in IR, and how it often appears in the re-
search literature. We then shifted our focus to normalised DCG, and
demonstrated its inconsistency, both theoretically and empirically
with reproducible experiments. Indeed, even when all neccesary
assumptions hold and DCG provides unbiased estimates of online
reward, nDCG cannot be used to rank competing models, as it does
does notpreserve the rankings we would obtain from DCG.
Through a correlation analysis between results obtained from
off- and on-line experiments on a large-scale recommendation plat-
form, we show that our unbiased DCG estimates strongly correlate
with online metrics in a real-world use-case. Additionally, we show
how the offline metric can be used to detect statistically significant
online improvements with high sensitivity, further highlighting its
promise for offline evaluation in both academia and industry. Nor-
malised DCG, on the other hand, suffers from a weaker correlation
with online results, and lower sensitivity than DCG. These results
suggest that nDCGâ€™s practical utility may be limited.
We believe our work opens up interesting areas for future re-
search, where our theoretical framework can be extended to for-
mally assess the assumptions required by other commonly usedevaluation metrics in the field. Furthermore, theoretical and em-
pirical connections between other types of commonly used online
evaluation metrics (e.g. user retention) would be fruitful.
Acknowledgments
We are grateful to Lien Michiels, co-author of RecPack [ 69], for
early feedback and help setting up the experiment in Appendix A.
References
[1]A. Agarwal, S. Basu, T. Schnabel, and T. Joachims. 2017. Effective Evaluation
Using Logged Bandit Feedback from Multiple Loggers. In Proc. of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD
â€™17). ACM, 687â€“696. https://doi.org/10.1145/3097983.3098155
[2]A. Agarwal, X. Wang, C. Li, M. Bendersky, and M. Najork. 2019. Addressing
Trust Bias for Unbiased Learning-to-Rank. In Proc. of the 2019 World Wide Web
Conference (WWW â€™19). ACM, 4â€“14. https://doi.org/10.1145/3308558.3313697
[3]A. Al-Maskari, M. Sanderson, and P. Clough. 2007. The Relationship between
IR Effectiveness Measures and User Satisfaction. In Proc of. the 30th Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR â€™07). ACM, 773â€“774. https://doi.org/10.1145/1277741.1277902
[4]T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. 2009. Improvements
That Donâ€™t Add up: Ad-Hoc Retrieval Results since 1998. In Proc of. the 18th
ACM Conference on Information and Knowledge Management (CIKM â€™09). ACM,
601â€“610. https://doi.org/10.1145/1645953.1646031
[5]J. Beel, M. Genzmehr, S. Langer, A. NÃ¼rnberger, and B. Gipp. 2013. A Comparative
Analysis of Offline and Online Evaluations and Discussion of Research Paper
Recommender System Evaluation. In Proc. of the International Workshop on
Reproducibility and Replication in Recommender Systems Evaluation (RepSys â€™13).
7â€“14.
[6]W. Bendada, G. Salha, and T. Bontempelli. 2020. Carousel Personalization
in Music Streaming Apps with Contextual Bandits. In Proc. of the 14th ACM
Conference on Recommender Systems (RecSys â€™20). ACM, 420â€“425. https://doi.
org/10.1145/3383313.3412217
[7]J. Bennett, S. Lanning, et al .2007. The Netflix prize. In Proc. of the KDD cup and
workshop, Vol. 2007. 35.
[8]A. Borisov, I. Markov, M. de Rijke, and P. Serdyukov. 2016. A Neural Click Model
for Web Search. In Proc. of the 25th International Conference on World Wide Web
(WWW â€™16). 531â€“541. https://doi.org/10.1145/2872427.2883033
[9]R. CaÃ±amares and P. Castells. 2020. On Target Item Sampling In Offline Recom-
mender System Evaluation. In Proc. of the 14th ACM Conference on Recommender
Systems (RecSys â€™20). ACM, 259â€“268. https://doi.org/10.1145/3383313.3412259
[10] R. CaÃ±amares, P. Castells, and A. Moffat. 2020. Offline evaluation options
for recommender systems. Information Retrieval Journal 23, 4 (01 Aug 2020),
387â€“410. https://doi.org/10.1007/s10791-020-09371-3
[11] E. Cavenaghi, G. Sottocornola, F. Stella, and M. Zanker. 2023. A Systematic Study
on Reproducibility of Reinforcement Learning in Recommendation Systems.
ACM Trans. Recomm. Syst. 1, 3, Article 11 (jul 2023), 23 pages. https://doi.org/
10.1145/3596519
[12] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. 2009. Expected Reciprocal
Rank for Graded Relevance. In Proc of. the 18th ACM Conference on Information
and Knowledge Management (CIKM â€™09). ACM, 621â€“630. https://doi.org/10.
1145/1645953.1646033
[13] J. Chen, J. Mao, Y. Liu, M. Zhang, and S. Ma. 2020. A Context-Aware Click Model
for Web Search. In Proc. of the 13th International Conference on Web Search and
Data Mining (WSDM â€™20) . ACM, 88â€“96. https://doi.org/10.1145/3336191.3371819
[14] M. Chen, A. Beutel, P. Covington, S. Jain, F. Belletti, and E. H. Chi. 2019. Top-K
Off-Policy Correction for a REINFORCE Recommender System. In Proc. of the
12th ACM International Conference on Web Search and Data Mining (WSDM â€™19).
ACM, 456â€“464. https://doi.org/10.1145/3289600.3290999
[15] A. Chuklin, I. Markov, and M. de Rijke. 2015. Click Models for Web Search.
Morgan & Claypool. https://doi.org/10.2200/S00654ED1V01Y201507ICR043
[16] C.L.A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. BÃ¼ttcher,
and I. MacKinnon. 2008. Novelty and Diversity in Information Retrieval Eval-
uation. In Proc. of the 31st Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SIGIR â€™08). ACM, 659â€“666.
https://doi.org/10.1145/1390334.1390446
[17] P. Covington, J. Adams, and E. Sargin. 2016. Deep Neural Networks for YouTube
Recommendations. In Proc. of the 10th ACM Conference on Recommender Systems
(RecSys â€™16). ACM, 191â€“198. https://doi.org/10.1145/2959100.2959190
[18] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. 2008. An Experimental
Comparison of Click Position-Bias Models. In Proc of. the 2008 International
Conference on Web Search and Data Mining (WSDM â€™08). ACM, 87â€“94. https:
//doi.org/10.1145/1341531.1341545
 
1230KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko
[19] V. Dang, M. Bendersky, and W. B. Croft. 2013. Two-Stage Learning to Rank
for Information Retrieval. In Advances in Information Retrieval. Springer Berlin
Heidelberg, 423â€“434.
[20] R. Deffayet, J. Renders, and M. de Rijke. 2022. Evaluating the Robustness of
Click Models to Policy Distributional Shift. ACM Trans. Inf. Syst. (oct 2022).
https://doi.org/10.1145/3569086 Just Accepted.
[21] R. Deffayet, T. Thonet, J. M. Renders, and M. de Rijke. 2023. Offline Evaluation
for Reinforcement Learning-Based Recommendation: A Critical Issue and Some
Alternatives. SIGIR Forum 56, 2, Article 3 (jan 2023), 14 pages. https://doi.org/
10.1145/3582900.3582905
[22] F. Diaz. 2021. On Evaluating Session-Based Recommendation with Implicit
Feedback. In Workshop on Perspectives on Offline Evaluation for Recommender
Systems at RecSys â€™21 (PERSPECTIVES â€™21).
[23] F. Diaz, B. Mitra, M. D. Ekstrand, A. J. Biega, and B. Carterette. 2020. Evaluating
Stochastic Rankings with Expected Exposure. In Proc of. the 29th ACM Interna-
tional Conference on Information & Knowledge Management (CIKM â€™20). ACM,
275â€“284. https://doi.org/10.1145/3340531.3411962
[24] M. DudÃ­k, J. Langford, and L. Li. 2011. Doubly Robust Policy Evaluation and
Learning. In Proc. of the 28th International Conference on International Conference
on Machine Learning (ICMLâ€™11) . 1097â€“1104.
[25] V. Elvira, L. Martino, D. Luengo, and M.F. Bugallo. 2019. Generalized Multiple
Importance Sampling. Statist. Sci. 34, 1 (2019), 129 â€“ 155. https://doi.org/10.
1214/18-STS668
[26] Z. Fang, A. Agarwal, and T. Joachims. 2019. Intervention Harvesting for Context-
Dependent Examination-Bias Estimation. In Proc of. the 42nd International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIRâ€™19).
ACM, 825â€“834. https://doi.org/10.1145/3331184.3331238
[27] M. Ferrante, N. Ferro, and N. Fuhr. 2021. Towards Meaningful Statements in
IR Evaluation: Mapping Evaluation Measures to Interval Scales. IEEE Access 9
(2021), 136182â€“136216. https://doi.org/10.1109/ACCESS.2021.3116857
[28] M. Ferrari Dacrema, S. Boglio, P. Cremonesi, and D. Jannach. 2021. A Troubling
Analysis of Reproducibility and Progress in Recommender Systems Research.
ACM Trans. Inf. Syst. 39, 2, Article 20 (jan 2021), 49 pages. https://doi.org/10.
1145/3434185
[29] M. Ferrari Dacrema, P. Cremonesi, and D. Jannach. 2019. Are We Really Making
Much Progress? A Worrying Analysis of Recent Neural Recommendation Ap-
proaches. In Proc. of the 13th ACM Conference on Recommender Systems (RecSys
â€™19). ACM, 101â€“109. https://doi.org/10.1145/3298689.3347058
[30] F. Garcin, B. Faltings, O. Donatsch, A. Alazzawi, C. Bruttin, and A. Huber. 2014.
Offline and Online Evaluation of News Recommender Systems at Swissinfo.Ch.
InProc. of the 8th ACM Conference on Recommender Systems (RecSys â€™14). 169â€“176.
https://doi.org/10.1145/2645710.2645745
[31] A. Gilotte, C. CalauzÃ¨nes, T. Nedelec, A. Abraham, and S. DollÃ©. 2018. Offline A/B
Testing for Recommender Systems. In Proc. of the Eleventh ACM International
Conference on Web Search and Data Mining (WSDM â€™18). ACM, 198â€“206. https:
//doi.org/10.1145/3159652.3159687
[32] A. Gruson, P. Chandar, C. Charbuillet, J. McInerney, S. Hansen, D. Tardieu,
and B. Carterette. 2019. Offline Evaluation to Make Decisions About Playlist
Recommendation Algorithms. In Proc of. the 12th ACM International Conference
on Web Search and Data Mining (WSDM â€™19). ACM, 420â€“428. https://doi.org/
10.1145/3289600.3291027
[33] S. Gupta, H. Oosterhuis, and M. de Rijke. 2023. Safe Deployment for Coun-
terfactual Learning to Rank with Exposure-Based Risk Minimization. In Proc.
of the 46th International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR â€™23). ACM, 249â€“258. https://doi.org/10.1145/
3539618.3591760
[34] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (Dec. 2015),
19 pages. https://doi.org/10.1145/2827872
[35] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. 2004. Evaluating
Collaborative Filtering Recommender Systems. ACM Transactions on Information
Systems 22, 1 (Jan. 2004), 5â€“53. https://doi.org/10.1145/963770.963772
[36] E. Ie, C. Hsu, M. Mladenov, V. Jain, S. Narvekar, J. Wang, R. Wu, and C. Boutilier.
2019. RecSim: A Configurable Simulation Platform for Recommender Systems.
https://arxiv.org/abs/1909.04847
[37] E. Ie, V. Jain, J. Wang, S. Narvekar, R. Agarwal, R. Wu, H. Cheng, T. Chandra,
and C. Boutilier. 2019. SlateQ: A tractable decomposition for reinforcement
learning with recommendation sets. (2019).
[38] E. L. Ionides. 2008. Truncated Importance Sampling. Journal of Computational
and Graphical Statistics 17, 2 (2008), 295â€“311.
[39] A. H. Jadidinejad, C. Macdonald, and I. Ounis. 2021. The Simpsonâ€™s Paradox in
the Offline Evaluation of Recommendation Systems. ACM Trans. Inf. Syst. 40, 1,
Article 4 (sep 2021), 22 pages. https://doi.org/10.1145/3458509
[40] R. Jagerman, X. Wang, H. Zhuang, Z. Qin, M. Bendersky, and M. Najork. 2022.
Rax: Composable Learning-to-Rank Using JAX. In Proc. of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™22). ACM, 3051â€“3060.
https://doi.org/10.1145/3534678.3539065[41] M. Jakimov, A. Buchholz, Y. Stein, and T. Joachims. 2023. Unbiased Offline
Evaluation for Learning to Rank with Business Rules. In RecSys 2023 Workshop:
CONSEQUENCES â€“ Causality, Counterfactuals and Sequential Decision-Making.
arXiv:2311.01828
[42] K. JÃ¤rvelin and J. KekÃ¤lÃ¤inen. 2002. Cumulated Gain-Based Evaluation of IR
Techniques. ACM Trans. Inf. Syst. 20, 4 (oct 2002), 422â€“446. https://doi.org/10.
1145/582415.582418
[43] O. Jeunen. 2019. Revisiting Offline Evaluation for Implicit-feedback Recom-
mender Systems. In Proc. of the 13th ACM Conference on Recommender Systems
(RecSys â€™19). ACM, 596â€“600. https://doi.org/10.1145/3298689.3347069
[44] O. Jeunen. 2023. A Common Misassumption in Online Experiments with Ma-
chine Learning Models. SIGIR Forum 57, 1 (2023). arXiv:2304.10900 [cs.LG]
[45] O. Jeunen. 2023. A Probabilistic Position Bias Model for Short-Video Recom-
mendation Feeds. In Proc. of the 17th ACM Conference on Recommender Systems
(RecSys â€™23). ACM.
[46] O. Jeunen and B. Goethals. 2021. Pessimistic Reward Models for Off-Policy
Learning in Recommendation. In Proc. of the Fifteenth ACM Conference on Rec-
ommender Systems (RecSys â€™21). ACM, 63â€“74. https://doi.org/10.1145/3460231.
3474247
[47] O. Jeunen and B. Goethals. 2021. Top-K Contextual Bandits with Equity of
Exposure. In Proc of. the 15th ACM Conference on Recommender Systems (RecSys
â€™21). ACM, 310â€“320. https://doi.org/10.1145/3460231.3474248
[48] O. Jeunen and B. Goethals. 2023. Pessimistic Decision-Making for Recommender
Systems. ACM Trans. Recomm. Syst. 1, 1, Article 4 (feb 2023), 27 pages. https:
//doi.org/10.1145/3568029
[49] O. Jeunen and B. London. 2023. Offline Recommender System Evaluation under
Unobserved Confounding. In RecSys 2023 Workshop: CONSEQUENCES â€“ Causal-
ity, Counterfactuals and Sequential Decision-Making. arXiv:2309.04222 [cs.LG]
[50] O. Jeunen, D. Rohde, and F. Vasile. 2019. On the Value of Bandit Feedback for
Offline Recommender System Evaluation. arXiv:1907.12384 [cs.IR]
[51] O. Jeunen, D. Rohde, F. Vasile, and M. Bompaire. 2020. Joint Policy-Value
Learning for Recommendation. In Proc. of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (KDD â€™20). ACM, 1223â€“1233.
https://doi.org/10.1145/3394486.3403175
[52] O. Jeunen, K. Verstrepen, and B. Goethals. 2018. Fair Offline Evaluation Method-
ologies for Implicit-feedback Recommender Systems with MNAR Data. In Proc.
of the REVEAL 18 Workshop on Offline Evaluation for Recommender Systems
(RecSys â€™18).
[53] Y. Ji, A. Sun, J. Zhang, and C. Li. 2023. A Critical Study on Data Leakage in
Recommender System Offline Evaluation. ACM Trans. Inf. Syst. 41, 3, Article 75
(feb 2023), 27 pages. https://doi.org/10.1145/3569930
[54] T. Joachims, B. London, Y. Su, A. Swaminathan, and L. Wang. 2021. Recom-
mendations as Treatments. AI Magazine 42, 3 (Nov. 2021), 19â€“30. https:
//doi.org/10.1609/aimag.v42i3.18141
[55] N. Kallus, Y. Saito, and M. Uehara. 2021. Optimal Off-Policy Evaluation from
Multiple Logging Policies. In Proc. of the 38th International Conference on Machine
Learning (ICML â€™21, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 5247â€“
5256. https://proceedings.mlr.press/v139/kallus21a.html
[56] E. Kharitonov, A. Drutsa, and P. Serdyukov. 2017. Learning Sensitive Com-
binations of A/B Test Metrics. In Proc. of the Tenth ACM International Con-
ference on Web Search and Data Mining (WSDM â€™17). ACM, 651â€“659. https:
//doi.org/10.1145/3018661.3018708
[57] H. Kiyohara, Y. Saito, T. Matsuhiro, Y. Narita, N. Shimizu, and Y. Yamamoto. 2022.
Doubly Robust Off-Policy Evaluation for Ranking Policies under the Cascade
Behavior Model. In Proc of. the Fifteenth ACM International Conference on Web
Search and Data Mining (WSDM â€™22). ACM, 487â€“497. https://doi.org/10.1145/
3488560.3498380
[58] R. Kohavi, A. Deng, and L. Vermeer. 2022. A/B Testing Intuition Busters: Com-
mon Misunderstandings in Online Controlled Experiments. In Proc. of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™22).
ACM, 3168â€“3177. https://doi.org/10.1145/3534678.3539160
[59] R. Kohavi, D. Tang, and Y. Xu. 2020. Trustworthy online controlled experiments:
A practical guide to A/B testing. Cambridge University Press.
[60] W. Krichene and S. Rendle. 2020. On Sampled Metrics for Item Recommendation.
InProc. of the 26th ACM SIGKDD International Conference on Knowledge Discovery
& Data Mining (KDD â€™20). ACM, 1748â€“1757. https://doi.org/10.1145/3394486.
3403226
[61] N. Larsen, J. Stallrich, S. Sengupta, A. Deng, R. Kohavi, and N. T. Stevens. 2023.
Statistical Challenges in Online Controlled Experiments: A Review of A/B
Testing Methodology. The American Statistician 0, 0 (2023), 1â€“15. https://doi.
org/10.1080/00031305.2023.2257237
[62] D. Li, R. Jin, J. Gao, and Z. Liu. 2020. On Sampling Top-K Recommenda-
tion Evaluation. In Proc of. the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining (KDD â€™20). ACM, 2114â€“2124. https:
//doi.org/10.1145/3394486.3403262
[63] R. Lopez, I. S. Dhillon, and M. I. Jordan. 2021. Learning from eXtreme Bandit
Feedback. Proc of. the AAAI Conference on Artificial Intelligence 35, 10 (May
2021), 8732â€“8740. https://doi.org/10.1609/aaai.v35i10.17058
 
1231On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top- ğ‘›Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[64] I. Lyzhin, A. Ustimenko, A. Gulin, and L. Prokhorenkova. 2023. Which Tricks are
Important for Learning to Rank?. In Proc of. the 40th International Conference on
Machine Learning (ICML â€™23â€™, Vol. 202). PMLR, 23264â€“23278. https://proceedings.
mlr.press/v202/lyzhin23a.html
[65] J. Ma, Z. Zhao, X. Yi, J. Yang, M. Chen, J. Tang, L. Hong, and E. H. Chi. 2020.
Off-Policy Learning in Two-Stage Recommender Systems. In Proc. of the 2020
World Wide Web Conference (WWW â€™20). ACM. https://doi.org/10.1145/3366423.
3380130
[66] J. McInerney, B. Brost, P. Chandar, R. Mehrotra, and B. Carterette. 2020.
Counterfactual Evaluation of Slate Recommendations with Sequential Re-
ward Interactions. In Proc of. the 26th ACM SIGKDD International Confer-
ence on Knowledge Discovery & Data Mining (KDD â€™20). ACM, 1779â€“1788.
https://doi.org/10.1145/3394486.3403229
[67] R. Mehrotra, J. McInerney, H. Bouchard, M. Lalmas, and F. Diaz. 2018. Towards a
Fair Marketplace: Counterfactual Evaluation of the Trade-off between Relevance,
Fairness & Satisfaction in Recommendation Systems. In Proc. of the 27th ACM
International Conference on Information and Knowledge Management (CIKM â€™18).
ACM, 2243â€“2251. https://doi.org/10.1145/3269206.3272027
[68] R. Mehrotra, N. Xue, and M. Lalmas. 2020. Bandit Based Optimization of Multiple
Objectives on a Music Streaming Platform. In Proc of. the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining (KDD â€™20). ACM,
3224â€“3233. https://doi.org/10.1145/3394486.3403374
[69] L. Michiels, R. Verachtert, and B. Goethals. 2022. RecPack: An(Other) Experi-
mentation Toolkit for Top-N Recommendation Using Implicit Feedback Data.
InProc. of the 16th ACM Conference on Recommender Systems (RecSys â€™22). ACM,
648â€“651. https://doi.org/10.1145/3523227.3551472
[70] A. Moffat and J. Zobel. 2008. Rank-Biased Precision for Measurement of Retrieval
Effectiveness. ACM Trans. Inf. Syst. 27, 1, Article 2 (dec 2008), 27 pages. https:
//doi.org/10.1145/1416950.1416952
[71] H. Oosterhuis. 2022. Learning-to-Rank at the Speed of Sampling: Plackett-
Luce Gradient Estimation with Minimal Computational Complexity. In Proc
of. the 45th International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR â€™22). ACM, 2266â€“2271. https://doi.org/10.1145/
3477495.3531842
[72] H. Oosterhuis. 2023. Doubly Robust Estimation for Correcting Position Bias
in Click Feedback for Unbiased Learning to Rank. ACM Trans. Inf. Syst. 41, 3,
Article 61 (feb 2023), 33 pages. https://doi.org/10.1145/3569453
[73] H. Oosterhuis and M. de Rijke. 2020. Policy-Aware Unbiased Learning to Rank
for Top-k Rankings. In Proc of. the 43rd International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR â€™20). ACM, 489â€“498.
https://doi.org/10.1145/3397271.3401102
[74] A. B. Owen. 2013. Monte Carlo theory, methods and examples.
[75] J. Parapar and F. Radlinski. 2021. Towards Unified Metrics for Accuracy and
Diversity for Recommender Systems. In Proc. of the 15th ACM Conference on Rec-
ommender Systems (RecSys â€™21). ACM, 75â€“84. https://doi.org/10.1145/3460231.
3474234
[76] S. Rendle, W. Krichene, L. Zhang, and J. Anderson. 2020. Neural Collaborative
Filtering vs. Matrix Factorization Revisited. In Proc of. the 14th ACM Conference
on Recommender Systems (RecSys â€™20). ACM, 240â€“248. https://doi.org/10.1145/
3383313.3412488
[77] S. Rendle, W. Krichene, L. Zhang, and Y. Koren. 2022. Revisiting the Per-
formance of IALS on Item Recommendation Benchmarks. In Proc. of the
16th ACM Conference on Recommender Systems (RecSys â€™22). ACM, 427â€“435.
https://doi.org/10.1145/3523227.3548486
[78] S. Rendle, L. Zhang, and Y. Koren. 2019. On the Difficulty of Evaluating Baselines:
A Study on Recommender Systems. arXiv:1905.01395 [cs.IR]
[79] D. Rohde, S. Bonner, T. Dunlop, F. Vasile, and A. Karatzoglou. 2018. RecoGym:
A Reinforcement Learning Environment for the problem of Product Recommen-
dation in Online Advertising. arXiv preprint arXiv:1808.00720 (2018).
[80] M. Rossetti, F. Stella, and M. Zanker. 2016. Contrasting Offline and Online
Results when Evaluating Recommendation Algorithms. In Proc. of the 10th
ACM Conference on Recommender Systems (RecSys â€™16). ACM, 31â€“34. https:
//doi.org/10.1145/2959100.2959176
[81] N. Sachdeva, Y. Su, and T. Joachims. 2020. Off-Policy Bandits with Deficient
Support. In Proc of. the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining (KDD â€™20). ACM, 965â€“975. https://doi.org/10.1145/
3394486.3403139
[82] Y. Saito, S. Aihara, M. Matsutani, and Y. Narita. 2021. Open Bandit Dataset and
Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation. In Proc of.
the Neural Information Processing Systems Track on Datasets and Benchmarks,
Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/
2021/file/33e75ff09dd601bbe69f351039152189-Paper-round2.pdf
[83] Y. Saito and T. Joachims. 2021. Counterfactual Learning and Evaluation for
Recommender Systems: Foundations, Implementations, and Recent Advances.
InProc. of the 15th ACM Conference on Recommender Systems (RecSys â€™21). ACM,
828â€“830. https://doi.org/10.1145/3460231.3473320
[84] Y. Saito and T. Joachims. 2022. Off-Policy Evaluation for Large Action Spaces via
Embeddings. In Proc. of the 39th International Conference on Machine Learning(ICML â€™22, Vol. 162) . PMLR, 19089â€“19122. https://proceedings.mlr.press/v162/
saito22a.html
[85] Y. Saito, Q. Ren, and T. Joachims. 2023. Off-Policy Evaluation for Large Ac-
tion Spaces via Conjunct Effect Modeling. In Proc. of the 40th International
Conference on Machine Learning (ICML â€™23, Vol. 202). PMLR, 29734â€“29759.
https://proceedings.mlr.press/v202/saito23b.html
[86] Y. Saito, T. Udagawa, H. Kiyohara, K. Mogi, Y. Narita, and K. Tateno. 2021.
Evaluating the Robustness of Off-Policy Evaluation. In Proc. of the 15th ACM
Conference on Recommender Systems (RecSys â€™21). ACM, 114â€“123. https://doi.
org/10.1145/3460231.3474245
[87] O. Sakhi, S. Bonner, D. Rohde, and F. Vasile. 2020. BLOB : A Probabilistic Model
for Recommendation that Combines Organic and Bandit Signals. In Proc. of the
26th ACM Conference on Knowledge Discovery & Data Mining (KDD â€™20). ACM.
https://doi.org/10.1145/3394486.3403121
[88] H. SchÃ¼tze, C. D Manning, and P. Raghavan. 2008. Introduction to information
retrieval. Vol. 39. Cambridge University Press Cambridge.
[89] H. Steck. 2013. Evaluation of Recommendations: Rating-prediction and Ranking.
InProc. of the 7th ACM Conference on Recommender Systems (RecSys â€™13). ACM,
213â€“220. https://doi.org/10.1145/2507157.2507160
[90] H. Steck. 2019. Embarrassingly Shallow Autoencoders for Sparse Data. In The
World Wide Web Conference (WWW â€™19). ACM, 3251â€“3257.
[91] Student. 1908. Probable Error of a Correlation Coefficient. Biometrika 6, 2/3
(1908), 302â€“310. http://www.jstor.org/stable/2331474
[92] Y. Su, M. Dimakopoulou, A. Krishnamurthy, and M. Dudik. 2020. Doubly robust
off-policy evaluation with shrinkage. In Proc. of the 37th International Conference
on Machine Learning (ICML â€™20). PMLR, 9167â€“9176.
[93] Y. Su, P. Srinath, and A. Krishnamurthy. 2020. Adaptive Estimator Selection for
Off-Policy Evaluation. In Proc of. the 37th International Conference on Machine
Learning (ICML â€™20, Vol. 119). PMLR, 9196â€“9205. https://proceedings.mlr.press/
v119/su20d.html
[94] Y. Su, L. Wang, M. Santacatterina, and T. Joachims. 2019. CAB: Continuous Adap-
tive Blending for Policy Evaluation and Learning. In International Conference on
Machine Learning (ICMLâ€™19) . 6005â€“6014.
[95] A. Swaminathan and T. Joachims. 2015. The Self-Normalized Estima-
tor for Counterfactual Learning. In Advances in Neural Information Pro-
cessing Systems. 3231â€“3239. https://proceedings.neurips.cc/paper/2015/file/
39027dfad5138c9ca0c474d71db915c3-Paper.pdf
[96] A. Swaminathan, A. Krishnamurthy, A. Agarwal, M. Dudik, J. Langford,
D. Jose, and I. Zitouni. 2017. Off-policy evaluation for slate recom-
mendation. In Advances in Neural Information Processing Systems, Vol. 30.
Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/
5352696a9ca3397beb79f116f3a33991-Paper.pdf
[97] A. D. Tucker and T. Joachims. 2023. Variance-Minimizing Augmentation Logging
for Counterfactual Evaluation in Contextual Bandits. In Proc of. the Sixteenth
ACM International Conference on Web Search and Data Mining (WSDM â€™23).
ACM, 967â€“975. https://doi.org/10.1145/3539597.3570452
[98] T. Udagawa, H. Kiyohara, Y. Narita, Y. Saito, and K. Tateno. 2023. Policy-Adaptive
Estimator Selection for Off-Policy Evaluation. Proc. of the AAAI Conference on
Artificial Intelligence 37, 8 (Jun. 2023), 10025â€“10033. https://doi.org/10.1609/
aaai.v37i8.26195
[99] A. Ustimenko and L. Prokhorenkova. 2020. StochasticRank: Global Optimization
of Scale-Free Discrete Functions. In Proc of. the 37th International Conference on
Machine Learning (ICML â€™20â€™, Vol. 119). PMLR, 9669â€“9679. https://proceedings.
mlr.press/v119/ustimenko20a.html
[100] D. Valcarce, A. BellogÃ­n, J. Parapar, and P. Castells. 2020. Assessing ranking
metrics in top-N recommendation. Information Retrieval Journal 23, 4 (01 Aug
2020), 411â€“448. https://doi.org/10.1007/s10791-020-09377-x
[101] A. Vardasbi, H. Oosterhuis, and M. de Rijke. 2020. When Inverse Propensity
Scoring Does Not Work: Affine Corrections for Unbiased Learning to Rank.
InProc of. the 29th ACM International Conference on Information & Knowledge
Management (CIKM â€™20). ACM, 1475â€“1484. https://doi.org/10.1145/3340531.
3412031
[102] F. Vasile, D. Rohde, O. Jeunen, and A. Benhalloum. 2020. A Gentle Introduction
to Recommendation as Counterfactual Policy Learning. In Proc. of the 28th ACM
Conference on User Modeling, Adaptation and Personalization (UMAP â€™20). ACM,
392â€“393. https://doi.org/10.1145/3340631.3398666
[103] K. Verstrepen and B. Goethals. 2014. Unifying Nearest Neighbors Collaborative
Filtering. In Proc. of the 8th ACM Conference on Recommender Systems (RecSys
â€™14). ACM, 177â€“184. https://doi.org/10.1145/2645710.2645731
[104] L. Yang, Y. Cui, Yuan X., C. Wang, S. Belongie, and D. Estrin. 2018. Unbiased
Offline Recommender Evaluation for Missing-not-at-random Implicit Feedback.
InProc. of the 12th ACM Conference on Recommender Systems (RecSys â€™18). ACM,
279â€“287. https://doi.org/10.1145/3240323.3240355
[105] E. Zangerle and C. Bauer. 2022. Evaluating Recommender Systems: Survey
and Framework. ACM Comput. Surv. 55, 8, Article 170 (dec 2022), 38 pages.
https://doi.org/10.1145/3556536
 
1232KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko
4.96 4.97 4.98 4.99 5.00 5.01 5.02
DCG@1000.4480.4500.4520.454nDCG@100
Empirical P(disagreement)â‰ˆ25.14%
/e.sc/a.sc/s.sc/e.scr
/k.sc/u.sc/n.sc/n.sc
Trend
Figure 2: DCG and nDCG exhibit significant disagreement
for a standard offline evaluation setup on MovieLens-1M.
A Empirical Evidence of (n)DCG Inconsistency
on Public Data
Table 1provides a formal proof that an ordering over competing rec-
ommendation (or IR) models obtained through a normalised metric
isnotguaranteed to be consistent with the original metric. Never-
theless, one might wonder whether this single example represents
a misguided pathological case, or whether metric disagreement
occurs in practice. This gives rise to the research question:
RQ5 Do DCG and normalised DCG disagree when ranking recom-
mendation models in typical offline evaluation setups?
To answer this question, we make use of the RecPack Python pack-
age [ 69] and the MovieLens-1M dataset [ 34]. We consider two types
of models, easer[90] and kunn [103], varying their hyperparame-
ters to train 192 models on a fixed 50% of the available user-item
interactions, and assess their performance on the held-out 50%.This style of evaluation setup is prevalent in the recommendation
field [ 43,89,105]. We adopt this package, dataset and methods to
provide a reproducible setup that runs in under 20 minutes on a 2021
MacBook Pro. All source code, including hyperparameter ranges,
is available at github.com/olivierjeunen/nDCG-disagreement.
We do not de-bias the interactions (as MovieLens does not pro-
vide information about exposure), and adopt the traditional loga-
rithmic discount for DCG with a cut-off at rank 100. Results are
visualised in Figure 2with DCG@100 on the ğ‘¥-axis and nDCG@100
on theğ‘¦-axis. The two metrics exhibit a linear correlation of â‰ˆ0.6
(Pearson), and a rank correlation of â‰ˆ0.5(Kendall). Whilst they
are clearly correlated, practitioners should notblindly adopt nDCG
when DCG estimates their online metric. Indeed, DCG can be formu-
lated as an unbiased estimator of the average reward per trajectory,
but nDCG cannot. As can be seen from the plot, significant disagree-
ment occurs between the two metrics: when randomly choosing
two observations, the empirical probability of nDCG inverting the
ordering implied by DCG is roughly 25% on this example. Naturally,
one would expect this type of disagreement to occur even more
frequently when considering Learning-to-Rank algorithms that
directly optimise listwise objectives such as (n)DCG [40, 64,99].
Note that this discrepancy would not occur if we would sample
the exact same number of held-out items for every user (as in Leave-
One-Out Cross-Validation). Indeed, in such cases ğ‘“â˜…(ğ‘¥)is constant
âˆ€ğ‘¥âˆˆX, simply rescaling the metric. Whilst this practice can be
common in academic scenarios, real-world use-cases typically imply
varying numbers of â€œrelevantâ€ items per user or context.
We include these results to aid in the reproducibility of the
empirical phenomena we report in this work.
 
1233