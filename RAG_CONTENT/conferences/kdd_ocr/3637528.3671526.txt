Generative Auto-bidding via Conditional Diffusion Modeling
Jiayan Guoâˆ—
guojiayan@pku.edu.cn
Peking University
Alibaba Group
Beijing, ChinaYusen Huo
huoyusen.huoyusen@alibaba-
inc.com
Alibaba Group
Beijing, ChinaZhilin Zhang
zhangzhilin.pt@alibaba-inc.com
Alibaba Group
Beijing, China
Tianyu Wang
yves.wty@@alibaba-inc.com
Alibaba Group
Beijing, ChinaChuan Yu
yuchuan.yc@alibaba-inc.com
Alibaba Group
Beijing, ChinaJian Xu
xiyu.xj@alibaba-inc.com
Alibaba Group
Beijing, China
Bo Zhengâ€ 
bozheng@alibaba-inc.com
Alibaba Group
Beijing, ChinaYan Zhang
zhyzhy001@pku.edu.cn
Peking University
Beijing, China
ABSTRACT
Auto-bidding plays a crucial role in facilitating online advertis-
ing by automatically providing bids for advertisers. Reinforcement
learning (RL) has gained popularity for auto-bidding. However,
most current RL auto-bidding methods are modeled through the
Markovian Decision Process (MDP), which assumes the Markovian
state transition. This assumption restricts the ability to perform in
long horizon scenarios and makes the model unstable when dealing
with highly random online advertising environments. To tackle this
issue, this paper introduces AI-Generated Bidding (AIGB), a novel
paradigm for auto-bidding through generative modeling. In this
paradigm, we propose DiffBid, a conditional diffusion modeling
approach for bid generation. DiffBid directly models the correlation
between the return and the entire trajectory, effectively avoiding
error propagation across time steps in long horizons. Addition-
ally, DiffBid offers a versatile approach for generating trajectories
that maximize given targets while adhering to specific constraints.
Extensive experiments conducted on the real-world dataset and
online A/B test on Alibaba advertising platform demonstrate the
effectiveness of DiffBid, achieving 2.81% increase in GMV and 3.36%
increase in ROI.
CCS CONCEPTS
â€¢Information systems â†’Computational advertising.
âˆ—Work is done during the internship at Alibaba Group.
â€ Bo Zheng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than the 
author(
s) must be honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. 
ACM ISBN 979-8-4007-0490-1/24/08. . . 
https://doi.org/10.1145/3637528.3671526KEYWORDS
Online Advertising, Auto-bidding, Generative Learning, Diffusion
Modeling
ACM Reference Format:
Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu,
Bo Zheng, and Yan Zhang. 2024. Generative Auto-bidding via Conditional
Diffusion Modeling. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671526
1 INTRODUCTION
The ever-increasing digitalization of commerce has exponentially
expanded the scope and importance of online advertising plat-
forms [ 13,18]. These ad platforms have become indispensable for
businesses to effectively target their audience and drive sales. Tra-
ditionally, advertisers need to manually adjust bid prices to opti-
mize overall ad performance. However, this coarse bidding process
becomes impractical when dealing with trillions of impression op-
portunities, requiring extensive domain knowledge [ 10] and com-
prehensive information about the advertising environments.
To alleviate the burden of bid optimization for advertisers, these
ad platforms provide auto-bidding services [ 4,5,12,39]. These
services automate the determination of bids for each impression
opportunity by employing well-designed bidding strategies. Such
strategies consider a variety of factors about advertising environ-
ments and advertisers, such as the distribution of impression oppor-
tunities, budgets, and average cost constraints [ 31]. Considering
the dynamic nature of advertising environments, it is essential to
regularly optimize the bidding strategy, typically at intervals of a
few minutes, in response to changing conditions. With advertising
episodes typically extending beyond 24 hours, auto-bidding can be
seen as a sequential decision-making process with a long planning
horizon where the bidding strategy seeks to optimize performance
throughout the entire episode.
Recently, reinforcement learning (RL) techniques have been em-
ployed to optimize auto-bidding strategies through the training of
5038
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiayan Guo et al.
1 2 3 4 5 6 7
History State Length0.720.760.80Correlation [-1,1]Pearson
Spearman
Kendall
Figure 1: Correlation Coefficients between History and the
Next State.
agents with bidding logs collected from online advertising envi-
ronments [ 6,21,27,35,44,48]. By leveraging historical realistic
bidding information, these agents can learn patterns and trends
to make informed bidding decisions. However, most existing RL
auto-bidding methods are based on the Markovian decision pro-
cess (MDP), where the next state only depends on the current state
and action. In the online auto-bidding environment, this assumption
may been challenged by our statistical analysis presented in Fig-
ure 1, which shows a significant increase in the correlation between
the sequence lengths of history states and the next bidding state.
This result indicates that solving auto-bidding considering only the
last state will encounter several problems, including instability in
the highly random online advertising environment. Additionally,
the RL methods that rely on the Bellman equation often result in
compound errors [ 15]. This issue is especially pronounced in the
auto-bidding problem characterized by sparse return and limited
data coverage. A detailed statistical analysis is provided in A.5. In
this paper, instead of employing RL-based methods, we present a
novel paradigm, AI Generated Bidding (AIGB), that regards auto-
bidding as a generative sequential decision-making problem. AIGB
directly capture the correlation between the return and the entire
bidding trajectory that consists of a sequence of states or actions,
thereby transforming the problem into learning to generate an
optimal bidding trajectory. This approach enables us to overcome
the limitations of RL when dealing with the highly random online
advertising environment, sparse returns, and limited data coverage.
In the new paradigm, we propose Diff usion auto-bid ding model
DiffBid. It gradually corrupts the bidding trajectory by injecting
scheduled Gaussian noises into the forward process. Then, it recon-
structs trajectory from corrupted ones given returns and temporal
conditions via a parameterized neural network. We further propose
a non-Markovian inverse dynamics [ 17,37,49] to more accurately
generate optimal bidding parameters. Taking one step further, Diff-
Bid provides flexibility to closely align with the specific needs of
advertisers by accommodating diverse constraints like cost-per-
click (CPC) and incorporating human feedback. Notably, DiffBid
serves as a unified model capable of mastering multiple tasks si-
multaneously, dynamically composing various bidding trajectory
components to generate sequences that efficiently maximize di-
verse targets while adhering to a range of predefined constraints.
To assess the effectiveness of DiffBid, we conducted extensive eval-
uations offline and online against baselines. Our results indicate
that DiffBid surpasses RL methods for auto-bidding. In summary:â€¢We uncover that the Markov assumptions upon which com-
mon decision-making methods rely are not applicable to
the auto-bidding problem. Therefore, we propose a novel
bidding paradigm with non-Markovian properties based on
generative learning. This paradigm represents a significant
innovation in modeling methodology compared with exist-
ing RL methods commonly used in auto-bidding.
â€¢Unlike common bidding methods, our approach captures
the correlation between the return and the entire bidding
trajectory. This design enables the method to address impor-
tant challenges, such as sparse returns, and ensures stability
in the highly random advertising environment. Finally, we
prove that the proposed diffusion modeling is equivalent
in terms of optimality to solving a non-Markovian decision
problem.
â€¢We demonstrate that the method can integrate capabilities
to handle a variety of tasks within a unified solution, tran-
scending the limitations of traditional task-specific methods.
It shows that DiffBid outperforms conventional RL methods
in auto-bidding, and achieves significant performance gain
on a leading E-commerce ad platform through both offline
and online evaluation. In specific, it achieves 2.81% increase
in GMV and 3.36% in ROI.
2 PRELIMINARY
2.1 Problem Formulation
For simplicity, we consider auto-bidding with cost-related con-
straints. During a time period, suppose there are ğ‘impression
opportunities arriving sequentially and indexed by ğ‘–. In this setting,
advertisers submit bids to compete for each impression opportunity.
An advertiser will win the impression if its bid ğ‘ğ‘–is greater than
others. Then it will incur a cost ğ‘ğ‘–for winning and getting the value.
During the period, the mission of an advertiser is to maximize
the total received valueÃ
ğ‘–ğ‘œğ‘–ğ‘£ğ‘–, whereğ‘£ğ‘–is the value of impression ğ‘–
andğ‘œğ‘–is whether the advertiser wins impression ğ‘–. Besides, we have
the budget and several constraints to control the performance of ad
deliveries. Budget constraints are simplyÃ
ğ‘–ğ‘œğ‘–ğ‘ğ‘–â‰¤ğµ, whereğ‘ğ‘–is
the cost of impression ğ‘–andğµis the budget. The other constraints
are complex and according to [ 21] we have the unified formulation:
Ã
ğ‘–ğ‘ğ‘–ğ‘—ğ‘œğ‘–Ã
ğ‘–ğ‘ğ‘–ğ‘—ğ‘œğ‘–â‰¤ğ¶ğ‘—, (1)
whereğ¶ğ‘—is the upper bound of ğ‘—â€™th constraint. ğ‘ğ‘–ğ‘—can be any
performance indicator, e.g. return, or constant. ğ‘ğ‘–ğ‘—is the cost of
constraintğ‘—. Givenğ½constraints, we have the Multi-constrained
Bidding (MCB) as:
maximizeğ‘œğ‘–âˆ‘ï¸
ğ‘–ğ‘œğ‘–ğ‘£ğ‘–
s.t.âˆ‘ï¸
ğ‘–ğ‘œğ‘–ğ‘ğ‘–â‰¤ğµ
Ã
ğ‘–ğ‘ğ‘–ğ‘—ğ‘œğ‘–Ã
ğ‘–ğ‘ğ‘–ğ‘—ğ‘œğ‘–â‰¤ğ¶ğ‘—,âˆ€ğ‘—
ğ‘œğ‘–âˆˆ{0,1},âˆ€ğ‘–(2)
5039Generative Auto-bidding via Conditional Diffusion Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ConvConvConvConvConvConvConvConv
Forward Process3$/3#/3%&#/3%/
3â€²%&#/
Reverse ProcessReturn
Conditions3%/Linear
Bid 
Generation
ConstraintsFeedbacks3â€²$/'1+"#Eq.(3),5âˆ—
Conv-.)*!+*!"#+
,$*!"#+*!+,.(+)
4.32.53.54.52.44.41.82.82235
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
7.64.662.50.98.39.80.76.18.72.25.2
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
8.89.91.401.99.39.39.18.850.76.6
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
4.32.53.54.52.44.41.82.82235
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
7.64.662.50.98.39.80.76.18.72.25.2
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
8.89.91.401.99.39.39.18.850.76.6
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
4.32.53.54.52.44.41.82.82235
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
7.64.662.50.98.39.80.76.18.72.25.2
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
8.89.91.401.99.39.39.18.850.76.6
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
4.32.53.54.52.44.41.82.82235
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
7.64.662.50.98.39.80.76.18.72.25.2
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
8.89.91.401.99.39.39.18.850.76.6
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
4.32.53.54.52.44.41.82.82235
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
7.64.662.50.98.39.80.76.18.72.25.2
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
493.162.15.84.685.160.59.4
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
4.32.53.54.52.44.41.82.82235
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
7.64.662.50.98.39.80.76.18.72.25.2
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters
493.162.15.84.685.160.59.4
T=1T=2â€¦T=TStates
Left BudgetCost SpeedBidding Parameters%'%'
Figure 2: Overall Framework for Generative Auto-bidding.
A previous study [21] has already shown the optimal solution:
ğ‘âˆ—
ğ‘–=ğœ†0ğ‘£ğ‘–+ğ¶ğ‘–ğ½âˆ‘ï¸
ğ‘—=1ğœ†ğ‘—ğ‘ğ‘–ğ‘—, (3)
whereğ‘âˆ—
ğ‘–is the predicted optimal bid for the impression ğ‘–.ğœ†ğ‘—, ğ‘—âˆˆ
{0,...,ğ½}are the optimal bidding parameters. Specifically, when
considering only the budget constraint, it is the Max Return bidding.
However, when considering both the budget constraint and the
CPC constraint, it is called Target-CPC bidding. From an alternative
perspective, the optimal strategy involves arranging all impressions
in order of their cost-effectiveness (CE) and then selecting every
impression opportunity that surpasses the optimal CE ratio ğ‘ğ‘’âˆ—.
This threshold enforces the constraint, and the optimal bidding
parameters ğœ†0=1/ğ‘ğ‘’âˆ—.
2.2 Auto-Bidding as Decision-Making
Eq.(3) gives the formation of the optimal bid ğ‘âˆ—
ğ‘–with bidding pa-
rametersğœ†ğ‘—, ğ‘—âˆˆ{0,...,ğ½}. However, in practice, the highly random
and complex nature of the advertising environment prevents di-
rect calculation of the bidding parameters. They must be carefully
calibrated to adapt to the environment and dynamically adjusted
as it evolves, This subsequently makes auto-bidding a sequential
decision-making problem. To model it with decision-making, we
introduce states ğ’”ğ‘¡âˆˆSto describe the real-time advertising status
and actions ğ’‚ğ‘¡âˆˆA to adjust the corresponding bidding param-
eters. The auto-bidding agent will take action ğ’‚ğ‘¡at the state ğ’”ğ‘¡
based on its policy ğœ‹, and then the state will transit to the next
state ğ’”ğ‘¡+1and gain reward ğ’“ğ‘¡âˆˆR according to the advertising
environment dynamics T. WhenT:ğ’”ğ‘¡Ã—ğ’‚ğ‘¡â†’ğ’”ğ‘¡+1Ã—ğ’“ğ‘¡satisfies,
it is called the Markovian decision process (MDP). Otherwise, it is
a non-Markovian decision process. We next describe the key items
of the automated bidding agent in the industrial online advertising
system:
â€¢State ğ’”ğ‘¡describes the real-time advertising status at time
periodğ‘¡, which includes 1) remaining time of the advertiser;
2) remaining budget; 3) budget spend speed; 4) real-time
cost-efficiency (CPC), 5) and average cost-efficiency (CPC).â€¢Action ğ’‚ğ‘¡indicates the adjustment to the bidding parame-
ters at the time period ğ‘¡, which has the dimension of the
number of bidding parameters ğœ†ğ‘—, ğ‘—=1,..,ğ½ and modeled as
(ğ‘ğœ†0
ğ‘¡,...,ğ‘ğœ†ğ½
ğ‘¡).
â€¢The reward ğ’“ğ‘¡is the value contributed to the objective ob-
tained within the time period ğ‘¡.
â€¢A trajectory ğœis the index of a sequence of states, actions,
and rewards within an episode.
In the online advertising system, learning policy through direct
interaction with the online environment is unfeasible due to safety
concerns. Nonetheless, access to historical bidding logs, incorpo-
rating trajectories from a variety of bidding strategies, is attainable
and provides a viable alternative. Prevalent auto-bidding methods
predominantly leverage this offline data to craft effective policies.
Our approach is aligned with this practice and will be elaborated
in detail in the subsequent chapters.
3 AIGB PARADIGM FOR AUTO-BIDDING
To thoroughly investigate the auto-bidding problem, we conducted
a series of statistical analyses of bidding trajectories, with detailed
information available in appendix ??. These analyses provide us
with the insight that devising an effective bidding strategy is es-
sentially equivalent to optimizing a state trajectory. Armed with
this insight, we propose a hierarchical paradigm for auto-bidding
that prioritizes the state trajectory optimization and subsequently
generates actions aligned with the optimized trajectory.
For state trajectory optimization, we can employ a generative
model to capture the joint distribution of the entire bidding trajec-
tory and its associated returns, subsequently generating the trajec-
tory distribution conditioned on the desired return. This approach
enables us to address key auto-bidding challenges by employing
SOTA generative algorithms. This paper presents an implementa-
tion that utilizes Denoising Diffusion Probabilistic Models (DDPM).
For action generation, several off-the-shelf methods can be utilized
to predict the proper action given the target state trajectory. In
this paper, we apply a widely used inverse dynamics model. The
hierarchical paradigm divides auto-bidding into two supervised
5040KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiayan Guo et al.
learning problems, offering several advantages that include en-
hanced interpretability and increased stability during the training
process.
4 DIFFUSION AUTO-BIDDING MODEL
In this section, we give a detailed introduction of the proposed
diffusion Auto-bidding Model (DiffBid). We will first give the mod-
eling of Auto-bidding through diffusion models in Section 4.1.1.
Then we give a detailed description of the forward process in Sec-
tion 4.1.2, the reverse process in Section 4.1.3, and the training
process in Section 4.2. Finally, we will give the complexity analysis
in Section 4.4.
4.1 Diffusion Modeling of Auto-bidding
4.1.1 Overview. We model such sequential decision-making prob-
lem through conditional generative modeling [ 2,9] by maximum
likelihood estimation (MLE):
max
ğœƒEğœâˆ¼ğ·[logğ‘ğœƒ(ğ’™0(ğœ)|ğ’š(ğœ))] (4)
whereğœis the trajectory index, ğ’™0(ğœ)is the original trajectory
of states and ğ’š(ğœ)is the corresponding property. The goal is to
estimate the conditional data distribution with ğ‘ğœƒso that the future
states of a trajectory ğ’™0(ğœ)from information ğ’š(ğœ)can be generated.
For example, in the context of online advertising, ğ’š(ğœ)can be the
constraints or the total value of the entire trajectory. Under such
a setting, we can formalize the conditional diffusion modeling for
auto-bidding:
ğ‘(ğ’™ğ‘˜+1(ğœ)|ğ’™ğ‘˜(ğœ)), ğ‘ğœƒ(ğ’™ğ‘˜âˆ’1(ğœ)|ğ’™ğ‘˜(ğœ),ğ’š(ğœ)), (5)
whereğ‘represents the forward process in which noises are gradu-
ally added to the trajectory while ğ‘ğœƒis the reverse process where a
model is used for denoising. The detailed introduction of diffusion
modeling can be found in Appendix A.2. The overall framework is
presented in Figure 2. We will make a detailed discussion about the
two modeling processes in the following sections.
4.1.2 Forward Process via Diffusion over States. We model
the forward process ğ‘(ğ’™ğ‘˜+1(ğœ)|ğ’™ğ‘˜(ğœ))via diffusion over states,
where:
ğ’™ğ‘˜(ğœ):=(ğ’”1,...,ğ’”ğ‘¡,...,ğ’”ğ‘‡)ğ‘˜, (6)
where ğ’”ğ‘¡is modeled as a one-dimensional vector. ğ’™ğ‘˜(ğ‰)is a noise
sequence of states and can be represented by a two-dimensional
array where the first dimension is the time periods and the second
dimension is the state values. Merely sampling states is not enough
for an agent. Given ğ’™ğ‘˜(ğœ), we model the diffusion process as a
Markov chain, where ğ’™ğ‘˜(ğœ)is only dependent on ğ’™ğ‘˜âˆ’1(ğœ):
ğ‘(ğ’™ğ‘˜(ğœ)|ğ’™ğ‘˜âˆ’1(ğœ))=N
ğ’™ğ‘˜(ğœ);âˆšï¸
1âˆ’ğ›½ğ‘˜ğ’™ğ‘˜âˆ’1(ğœ),ğ›½ğ‘˜ğ¼
,(7)
whenğ‘˜â†’âˆ ,ğ‘¥ğ‘˜(ğœ)approaches a sequence of standard Gauss-
ian distribution where we can make sampling through the re-
parameterization trick and then gradually denoise the trajectory to
produce the final state sequence. For the design of ğ›½ğ‘˜, ğ‘˜=1,...,ğ¾ ,
we apply cosine schedule [ 38] to assign the corresponding values
which smoothly increases diffusion noises using a cosine function
to prevent sudden changes in the noise level. The details for noise
schedule can be found in the appendix.4.1.3 Reverse Process for Bid Generation. Following [ 2,23] we
use a classifier-free guidance strategy with low-temperature sam-
pling to guide the generation of bidding, to extract high-likelihood
trajectories in the dataset. During the training phase, we jointly
train the unconditional model ğœ–ğœƒ(ğ’™ğ‘˜(ğœ),ğ‘˜)and conditional model
ğœ–ğœƒ(ğ’™ğ‘˜(ğœ),ğ’š(ğœ),ğ‘˜)by randomly dropping out conditions. During
generation, a linear combination of conditional and unconditional
score estimates is used:
Ë†ğœ–ğ‘˜:=ğœ–ğœƒ(ğ’™ğ‘˜(ğœ),ğ‘˜)+ğœ”(ğœ–ğœƒ(ğ’™ğ‘˜(ğœ),ğ’š(ğœ),ğ‘˜)âˆ’ğœ–ğœƒ(ğ’™ğ‘˜(ğœ),ğ‘˜)),(8)
where the scale ğœ”is applied to extract the most suitable portion
of the trajectory in the dataset that coappeared with ğ’š(ğœ). After
that, we can sample from DiffBid to produce bidding parameters
through sampling from ğ‘ğœƒ(ğ’™ğ‘˜âˆ’1(ğœ)|ğ’™ğ‘˜(ğœ),ğ’š(ğœ)):
ğ’™ğ‘˜âˆ’1(ğœ)âˆ¼N ğ’™ğ‘˜âˆ’1(ğœ)|ğğœƒ(ğ‘¥ğ‘˜(ğœ),ğ’š(ğœ),ğ‘˜),ğšºğœƒ(ğ’™ğ‘˜(ğœ),ğ‘˜)(9)
where a widely used parameterization here is ğğœƒ(ğ’™ğ‘˜(ğœ),ğ’š(ğœ),ğ‘˜)=
1âˆšğ›¼ğ‘˜(ğ’™ğ‘˜(ğœ)âˆ’ğ›½ğ‘˜âˆš
1âˆ’ğ›¼ğ‘˜Ë†ğœ–ğ‘˜)andÎ£ğœƒ(Â·)=ğ›½ğ‘˜, in whichğ›¼ğ‘˜=1âˆ’ğ›½ğ‘˜and
ğ›¼ğ‘˜=Ãğ‘˜
ğ‘–=1ğ›¼ğ‘˜. When serving at time period ğ‘¡, the agent first sample
a initial trajectory ğ‘¥â€²
ğ¾(ğœ)âˆ¼N( 0,ğ¼)and assign the history states
ğ’”0:ğ‘¡into it. Then, we can sample predicted states with the reverse
process recursively by
ğ’™â€²
ğ‘˜âˆ’1(ğœ)=ğğœƒ(ğ’™â€²
ğ‘˜(ğœ),ğ’š(ğœ),ğ‘˜)+âˆšï¸
ğ›½ğ‘˜ğ’› (10)
where ğ’›âˆ¼N( 0,ğ¼). Given ğ’™â€²
0(ğœ), we can extract the next predicted
stateğ‘ â€²
ğ‘¡+1, and determine how much it should bid to achieve that
state. In this setting, we apply widely used inverse dynamics [ 1,40]
with non-Markovian state sequence to determine current bidding
parameters at time period ğ‘¡:
Ë†ğ’‚ğ‘¡=ğ‘“ğœ™(ğ’”ğ‘¡âˆ’ğ¿:ğ‘¡,ğ’”â€²
ğ‘¡+1), (11)
where Ë†ğ’‚ğ‘¡âˆˆRğ½contains predicted bidding parameters (i.e. ğœ†ğ‘–, ğ‘–=
1,...,ğ‘› ) at timeğ‘¡.ğ¿is the length of history states. The inverse dy-
namic function ğ‘“ğœ™can be trained with the same offline logs as the
reverse process. This design disentangles the learning of states and
actions, making it easier to learn the connection between states
thus achieving better empirical performance. The overall procedure
is summarized in Algorithm 1.
4.2 DiffBid Training
Following [ 22], we train DiffBid to approximate the given noise
and the returns in a supervised manner. Given a bidding trajectory
ğ’™0(ğœ), we have its corresponding returns e.g., values the advertiser
received, the constraint the model should obey and the history
statesğ‘ ğ‘™,ğ‘™=1,...,ğ‘¡ before time ğ‘¡+1. Then we just train the reverse
process model ğ‘ğœƒwhich is parameterized through the noise model
ğœ–ğœƒand the inverse dynamics ğ‘“ğœ™through:
L(ğœƒ,ğœ™)=Eğ‘˜,ğœâˆˆD
||ğœ–âˆ’ğœ–ğœƒ(ğ’™ğ‘˜(ğœ),ğ’š(ğœ),ğ‘˜)||2
+E(ğ’”ğ‘¡âˆ’ğ¿:ğ‘¡,ğ’‚ğ‘¡,ğ‘ â€²
ğ‘¡+1)âˆˆD
||ğ’‚ğ‘¡âˆ’ğ‘“ğœ™(ğ’”ğ‘¡âˆ’ğ¿:ğ‘¡,ğ’”â€²
ğ‘¡+1)||2
,(12)
In the training process, we randomly sample a bidding trajectory ğœ
and a time step ğ‘˜, then we construct a noise trajectory ğ’™ğ‘˜(ğœ)and
predict the noise through Eq (8). Following [ 23], we randomly drop
conditions ğ’š(ğœ)with probability ğ‘to train DiffBid to enhance the
robustness. The process is presented in Algorithm 2.
5041Generative Auto-bidding via Conditional Diffusion Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.3 Design of Conditions.
In this section, we present approaches transforming industrial met-
rics into conditions of DiffBid.
4.3.1 Generation with Returns. For each trajectory ğœwe have
the total value the advertiser received as the the return ğ‘…(ğœ)=Ãğ‘‡
ğ‘¡=1ğ‘Ÿğ‘¡. We normalize the return by:
ğ‘…=ğ‘…(ğœ)âˆ’ğ‘…min
ğ‘…maxâˆ’ğ‘…min, (13)
whereğ‘…minandğ‘…maxare the smallest and the largest return in the
dataset. Through Eq. 13 we normalize the return into [0,1]and
merge it into ğ‘¦(ğœ). Subsequently, we train the model to generate
trajectories conditioned on the normalized returns. It should be
noted that trajectories with more values received have higher nor-
malized returns. Thus ğ‘…=1indicates the best trajectory with the
highest values which will better fit the advertisersâ€™ needs. When
generation, we just set ğ‘…=1and generate the trajectory under the
max return condition to the advertiser.
4.3.2 Generation with Constraints or Human Feedback .In
MCB, the cumulative performance related to the constraints within
a given episode should be controlled so as not to exceed the adver-
tisersâ€™ expectations. In such a setting, we can design ğ’š(ğœ)to control
the generation process. For example, in the Target-CPC setting, we
can maintain a binary variable ğ¸to indicate whether the final CPC
exceeds the given constraint ğ¶:
ğ¸=Iğ‘¥â‰¤ğ¶(ğ‘¥) (14)
whereğ‘¥=Ã
ğ‘–ğ‘ğ‘–ğ‘œğ‘–Ã
ğ‘–ğ‘ğ‘–ğ‘œğ‘–is defined in Eq (2). We can then normalize ğ‘¥into
[0,1]through min-max normalization for simplification. ğ¸can be
used to indicate whether trajectory ğœbreak the CPC constraint. We
can also design ğ’š(ğœ)to includeğ¸=1to make the model generate
bids that do not break the CPC constraint. Sometimes it is also
important to adjust the bidding parameters given real-time feedback
provided by the advertiser to enable flexibility. Here we use two
example indicators that reflect the experience of advertisers:
(1)Smoothness: an advertiser may expect the cost curve as
smooth as possible to avoid sudden change. By defining ğ‘¥=
1
ğ‘‡Ã
ğ‘¡|ğ‘ğ‘œğ‘ ğ‘¡ğ‘¡âˆ’ğ‘ğ‘œğ‘ ğ‘¡ğ‘¡âˆ’1|, we can model it as a binary variable
ğ‘†indicating whether the max cost change between adjacent
time period exceeds a threshold as in Eq (14).
(2)Early/Late Spend: an advertiser may expect the budget to
be cost in the morning or in the evening when there are
promotions. Here we model the ratio of cost in the early
half day through ğ‘¥=Ãğ‘‡/2
ğ‘¡=0ğ‘ğ‘œğ‘ ğ‘¡ğ‘–Ãğ‘‡
ğ‘¡=0ğ‘ğ‘œğ‘ ğ‘¡ğ‘–, and use a binary variable to
indicate whether the spend in the early half day exceeds a
certain threshold ğ¶as in Eq (14).
We can also compose several constraints together to form ğ’š(ğœ)to
guide the model to generate bid parameters that adhere to different
constraints. In this setting, ğ’š(ğœ)will be a vector.
4.4 Complexity Analysis
The complexity analysis for training DiffBid consists of the training
process and the inference process. For training, given the time com-
plexity of the noise prediction model ğœ–ğœƒisO(ğ‘‡1), the complexityfor the inverse dynamic model ğ‘“ğœ™isO(ğ‘‡2), the complexity for a
training epoch isO(|B|(ğ‘‡1+ğ‘‡2)). It can be seen that the training
complexity is linear with the input given ğ‘‡1andğ‘‡2are relatively
fixed. Thus the training of DiffBid is efficient. For generation, given
the total diffusion step ğ¾, the trajectory length ğ¿, then the time
complexity for inference is O(ğ¾ğ¿(ğ‘‡1+ğ‘‡2)). We can observe that the
time complexity for inference is linearly scaled with the diffusion
stepğ¾. In image generation, ğ¾is usually very large to ensure good
generation quality, which brings the problem of non-efficiency.
However, for bidding generation, we find ğ¾needs not to be very
large. Relatively small ğ¾has already generated promising results.
Moreover, in auto-bidding, a higher tolerance for latency is accept-
able, enabling the use of relatively larger ğ¾.
5 THEORETICAL ANALYSIS
In this section, we theoretically analyze the property of DiffBid. In
specific, we show that DiffBid that utilize MLE as the objective has
a corresponding non-Markovian decision problem [ 16,33,36,41].
The detailed proofs can be found in the Appendix A.7.
Lemma 5.1 (MLE as non-Markovian decision-making). As-
suming the Markovian transition ğ‘ğ›¾âˆ—(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡)is known, the ground-
truth conditional state distribution ğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)for demonstration
sequences is accessible, we can construct a non-Markovian sequential
decision-making problem, based on a reward function ğ‘Ÿğ›¼(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡):=
logâˆ«
ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)ğ‘ğ›¾âˆ—(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡)ğ‘‘ğ‘ğ‘¡for an arbitrary energy-based
policyğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡). Its objective is
ğ‘‡âˆ‘ï¸
ğ‘¡=0Eğ‘âˆ—(ğ‘ 0:ğ‘¡)
ğ‘‰ğ‘ğ›¼(ğ‘ 0:ğ‘¡)
=Eğ‘âˆ—(ğ‘ 0:ğ‘‡)"ğ‘‡âˆ‘ï¸
ğ‘¡=0ğ‘‡âˆ‘ï¸
ğ‘˜=ğ‘¡ğ‘Ÿğ›¼(ğ‘ ğ‘˜+1;ğ‘ 0:ğ‘˜)#
(15)
ğ‘‰ğ‘ğ›¼(ğ‘ 0:ğ‘¡):=Eğ‘âˆ—(ğ‘ ğ‘¡+1:ğ‘‡|ğ‘ 0:ğ‘¡)[Ãğ‘‡
ğ‘˜=1ğ‘Ÿğ›¼(ğ‘ ğ‘¡+1;ğ‘ 0:ğ‘¡)]is the value function
ofğ‘ğ›¼. This objective yields the save optimal policy as the Maximum
Likelihood Estimation Eğ‘âˆ—(ğ‘ 0:ğ‘‡)[logğ‘ğœƒ(ğ‘ 0:ğ‘‡)].
Remarks. This analysis shows that DiffBid utilizing MLE objective
has its corresponding non-Markovian decision problem, and their
optimal are equivalent. It means that DiffBid does not require the
MDP assumption of problems and thus is more powerful in handling
randomness and sparse return like in the advertising environment.
6 EXPERIMENTS
6.1 Experimental Setup
6.1.1 Experimental Environment. The simulated experimental
environment is conducted in a manually built offline real advertising
system (RAS) as in [ 35]. Specifically, the RAS is composed of two
consecutive stages, where the auction mechanisms resemble those
in the RAS. We consider the bidding process in a day, where the
episode is divided into 96 time steps. Thus, the duration between
any two adjacent time steps ğ‘¡andğ‘¡+1is 15 minutes. The number
of impression opportunities between time step ğ‘¡andğ‘¡+1fluctuates
from 100 to 500. Detailed parameters in the RAS are shown in
Table 5. We keep the parameters the same for all experiments.
6.1.2 Data Collection. We use the widely applied auto-bidding
RL method USCB in the online environment to generate the bidding
logs for offline RL training. This results in a total 5,000trajectories
for the based dataset and 50,000for a larger one. To increase the
5042KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiayan Guo et al.
Table 1: Performance Comparison with baselines in different settings, including different data scales, and budgets in Max
Return bidding. improv indicates the relative improvement of DiffBid against the most comparative baseline. The best results
are bolded and the best second results are underlined.
Training Dataset Budget USCB BCQ CQL IQL DT DiffBid improv
USCB-5K1.5K 454.25 454.72 461.82 456.80 477.39 480.76 0.71%
2.0K 482.67 483.50 475.78 486.56 507.30 511.17 0.76%
2.5K 497.66 498.77 481.37 518.27 527.88 531.29 0.65%
3.0K 500.60 501.86 491.36 549.19 550.66 556.32 1.03%
USCBEx-5K1.5K 454.25 453.74 358.43 464.69 378.64 475.62 2.35%
2.0K 482.67 487.63 356.80 529.36 439.03 544.38 2.84%
2.5K 497.66 510.75 356.41 613.67 505.43 624.29 1.73%
3.0K 500.60 512.18 355.42 670.65 574.79 678.73 1.17%
USCBEx-50K1.5K 454.25 458.64 435.06 446.23 396.24 495.57 8.05%
2.0K 482.67 491.72 431.49 533.58 478.29 551.73 3.40%
2.5K 497.66 513.23 428.39 592.32 554.48 606.34 2.37%
3.0K 500.60 526.21 425.29 633.26 611.50 644.88 1.83%
diversity of the action space, we also randomly make explorations
to generate a dataset with more noise. The above process results
in three datasets: USCB-5k, USCBEx-5k, and USCBEx-50k, where
USCBEx indicates USCB logs with random exploration data.
6.1.3 Baselines. We use the state-of-the-art auto-bidding method
USCB as well as other 4 recently proposed offline RL methods as
our baselines. The details of the baselines are as follows:
â€¢USCB [21] an RL method designed for real-time bidding to
dynamically adjust parameters to achieve the optimum. It
has outperformed many RL baselines and is also the base
policy that is used to collect the data for offline training.
â€¢BCQ [14] a classic offline RL method without interaction
with the environment.
â€¢CQL [30] address the limitations of offline RL methods by
learning a conservative Q-function such that the expected
value of a policy under it lower-bounds its true value.
â€¢IQL [29] an offline RL method that does not require evaluat-
ing actions outside of the dataset, yet it enables substantial
improvement of the learned policy beyond the best behavior
in the data through generalization
â€¢DT[9] a prevalent generative method based on the trans-
former architecture for sequential decision-making.
6.1.4 Implementation Details. For the implementation of base-
lines, we use the default hyper-parameters suggested from their
papers and also tune through our best effort. For DiffBid, the diffu-
sion steps is searched within {5,10,20,30,50}.ğ›¾is set to 0.008. ğ¿is
searched in{1,2,3}.ğœ”for noise schedule is set to 0.2 empirically.
The batch size is set to 2% of all training trajectories. Total training
epochs is set to 500. For the implementation of ğ‘ğœƒ, we adopt the
most widely used model U-Net for diffusion modeling with hidden
sizes of 128 and 256. We use Adam optimizer with a learning rate
1ğ‘’âˆ’4to optimize the model. The condition dropout ratio is set to
0.2 during training. We update the model with momentum updates
over a period of 4 steps.6.1.5 Evaluation. For evaluation, we randomly initialize a multi-
agent advertising environment with USCB as the base auto-bidding
agents and use other methods to compete with these agents. We test
the performance under 4 different budgets, 1500, 2000, 2500, and
3000, to test the generalization under different budget scales. We
use the cumulative reward as the evaluation metric, which reflects
the total gain received by the target agent. For each method, we
randomly initialize 50 times and report the average of top-5 scores.
6.2 Performance Evaluation
The performance against baselines is shown in Table 1. In this
table, we show the cumulative reward from different budgets of
all the models. We have the following discoveries. One of the key
takeaways from the performance comparison presented in Table 1
is that offline RL methods consistently outperform the state-of-
the-art auto-bidding method, USCB. This finding underscores the
advantages of leveraging historical bidding data to train RL agents.
Offline RL methods, such as BCQ, IQL, and DT, exhibit superior
performance in terms of cumulative rewards across various budget
scenarios. The superiority of offline RL methods can be attributed to
their ability to learn from past bidding experiences without interac-
tion with a simulation environment. This mitigates the challenges
associated with inconsistencies between the online bidding envi-
ronment and the offline bidding environment, leading to policies
that are better aligned with real-world scenarios. Notably, DiffBid
stands out as the top-performing approach among all the methods
evaluated. In all budget scenarios and training datasets, DiffBid
consistently achieves the highest cumulative rewards. This remark-
able performance highlights the efficacy of the DiffBid approach in
optimizing bidding strategies by directly modeling the correlation
with the returns and entire trajectories. By decoupling the compu-
tational complexity from horizon length, DiffBid achieves superior
decision-making capabilities, outperforming traditional RL meth-
ods in both foresight and strategy. Another important observation
5043Generative Auto-bidding via Conditional Diffusion Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Ablation Study
Mo
del USCBEx-5K USCBEx-50K
DiffBid
2280.12 2395.60
DiffBid
w/o cond 1812.64 1852.21
DiffBid w/o non-mkv 2254.78 2287.41
from the results is the impact of training dataset size on model per-
formance. When comparing the "USCB-5K" and "USCBEx-50K" set-
tings, it becomes evident that a larger training dataset consistently
leads to improved cumulative rewards. This finding underscores
the significance of data size in training RL models for automated
bidding. A richer dataset allows the models to capture more diverse
bidding scenarios and make more informed decisions, ultimately
resulting in better performance. One intriguing aspect of DiffBidâ€™s
performance is its resilience to noise. In real-world advertising
environments, there can be inherent uncertainty and variability
in the bidding process due to factors like market dynamics and
competitor behavior. DiffBid appears to handle such noise more
effectively than the RL baselines. This means that even in situations
where bidding outcomes are less predictable, DiffBid manages to
maintain competitive performance.
6.3 Ablation Study
To study different parts of the proposed DiffBid, we run the model
without a certain module to see if the removed corresponding mod-
ule will result in a performance drop. The result of the ablation
study is shown in Table 2. Due to the space limitation, we only
provide the results on USCBEx-5K and USCBEx-50K. w/o cond
refers to the DiffBid with the condition set to 0.0 (rather than 1.0).
w/o non-mkv refers to the situation where we only use the current
state and the predicted next state to generate the bidding coeffi-
cient. From the table, we find both of the two parts contribute to
the final result, and removing either of them will result in a perfor-
mance drop. It verifies the effectiveness of the proposed methods
in boosting DiffBidâ€™s performance for auto-bidding.
6.4 In-depth Analysis
6.4.1 Study of State Transition. Here we compare the state
transition of the baseline method USCB and our proposed method
DiffBid. The result for grouped and non-grouped state transition
during a day is shown in Figure 3. In this figure, we plot the budget
left ratio with time steps in one day. From the figure, we can observe
that under USCB, most of the advertisersâ€™ consumption does not
exhaust their budget. This is attributed to the inconsistency between
the offline virtual environment and the real online environment
faced by USCB. On the contrary, the budget completion situation
improves under DiffBid, where most of the advertisers spend more
than 80% of their budgets. One possible reason is that DiffBid finds
trajectories with a high budget completion ratio will also have a
high cumulative reward, and thus tend to generate trajectories with
a high budget completion ratio. Moreover, advertisers with small
budgets undertend to spend money in the afternoon. This is because
the impressions in the afternoon offer a higher cost-effectiveness,
albeit with a limited quantity.
(a) USCB
 (b) DiffBid
Figure 3: State Transition in One Episode.
0.1 0.3 0.5 0.7 0.9
/glyph1197ormalized CPC0.00.20.40.60.81.01.2Exceeding Ratio (%)CPC
0.9
0.7
0.5
0.3
0.1
1000120014001600180020002200
Return
Return
(a) IQL
0.1 0.3 0.5 0.7 0.9
/glyph1197ormalized CPC0.00.20.40.60.81.01.2Exceeding Ratio (%)CPC
0.9
0.7
0.5
0.3
0.1
10001200140016001800200022002400
Return
Return (b) DiffBid
Figure 4: Performance under CPC constraint.
0.0 0.2 0.4 0.6 0.8 1.00.02.55.07.510.012.515.017.5DensityLow
High
True
(a) Smoothness
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35024681012DensityLow
High
True (b) Early Spend
Figure 5: Performance of Human Feedback.
6.4.2 Performance under Constraints and Feedbacks. We
additionally investigate DiffBidâ€™s multi-objective optimization ca-
pability under specific constraints, comparing its performance with
Offline RL. Specifically, we choose CPC ratio and overall return as
metrics and examine the ability of DiffBid and IQL to control the
overall CPC exceeding ratio while maximizing the overall return.
During training, we set different thresholds of CPC as in Eq (14).
Then when testing, we make DiffBid generating trajectories un-
der the expected CPC. In Figure 4, we show the exceeding ratio
and overall return under different CPC constraints and training
settings. From the figure, we find that DiffBid has the ability to
control diverse levels of exceeding ratio while maintaining an in-
tact return, surpassing IQL by a significant margin. Consequently,
DiffBid holds a distinct advantage in effectively addressing MCB
problems. We also study the performance under different advertiser
5044KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiayan Guo et al.
10 20 30
K500600700750Return
Budget
1500
2000
2500
3000
(a) Impact of Diffusion Steps
CQL IQL DiffBid
Methods5001000150020002500Returns
 (b) Stability
Figure 6: In-depth Analysis.
Table 3: Online A/B Test Result.
Metrics #P
lan Budget Cost Buycnt GMV ROI
Baseline 2068
886744 834426.104 23584.6836 1853823 2.221
DiffBid 2068
886744 829992.384 24078.6883 1905954 2.296
compare -
- -0.53% +2.09% +2.81% +3.36%
feedbacks. During training we split the trajectories through thresh-
olds of Eq. (14) into high and low levels, and learn the conditional
distribution under different levels. During generation, we adjust the
condition and generate corresponding samples and summarize the
metrics. The results for the statistic distribution of metrics for low
level, high level and the original trajectories are shown in Figure 5.
We find that the trajectory obtained from deploying DiffBid is well
controlled by the condition.
6.4.3 Impact of Diffusion Steps. We also study the overall per-
formance under different diffusion steps, which is an important
factor in influencing the efficiency and performance. The overall im-
pact of diffusion steps with respect to different budgets is illustrated
in Figure 6. From the figure, we have the following discoveries. First
of all, we observe that diffusion steps have a larger impact on ad-
vertisers with small budgets (1500 yuan). Secondly, larger budgets
are not sensitive to the diffusion steps, where we can get the best
result in most situations within 30 diffusion steps.
6.4.4 Stability. In this study, we randomly initialized the parame-
ters of three models - CQL, IQL, and DiffBid - and conducted thirty
training trials for each to examine stability in performance. As de-
picted in Figure 3(b), the RL-based models, CQL and IQL, showed a
tendency towards instability under varying random seeds. Notably,
IQL demonstrated slightly better performance than CQL, which
may be attributed to its design optimized for conservative regu-
larization. Contrasting with these, the generative model DiffBid
exhibited remarkable stability, with significantly fewer instances
of failure compared to its RL counterparts.
6.5 Online A/B Test
To further substantiate the effectiveness of DiffBid, we have de-
ployed it on Alibaba advertising platform for comparison against
the baseline IQL [ 29] method, which performs best among various
auto-bidding methods. The online A/B test is conducted from Febru-
ary 01, 2024, to February 08, 2024. The results are shown in Table 3.
It shows that DiffBid can significantly improve the Buycnt by 2.09%,the GMV by 2.81%, the ROI by 3.36%, showing its effectiveness in
optimizing the overall performance. For efficiency, DiffBid takes
0.2s per request with GPU acceleration while the baseline is 0.07s,
which means latency can be well guaranteed.
7 RELATED WORKS
Offline-Reinforcement Learning. Offline reinforcement learn-
ing is a research direction that has gained significant attention in
recent years. The primary goal of offline RL is to learn effective
policies from a fixed dataset without additional online interaction
with the environment. This approach is particularly beneficial when
online interaction is costly, risky, or otherwise not feasible.Notable
works include Conservative Q-learning (CQL) by Kumar et al. [ 30],
and Batch-Constrained deep Q-learning (BCQ) by Fujimoto et al.
[14]. Both algorithms aim to tackle overestimation bias which tends
to occur in offline RL settings. Kostrikov et al. [ 29] propose an
implicit q-learning approach to address the training instability for
CQL. Chen et al. [ 9] propose to use transformers for offline RL to
increase the model capability. Hansen-Estruch et al. [ 19] proposes
a diffusion-based approach with implicit Q-learning for offline RL.
Diffusion Models. They recently have shown the capability of
high-quality generation [ 11], unconditional generation [ 3] and con-
ditional generation [ 7,25]. It has shown promising performance in
decision-making. Hansen-Estruch et al. [ 19] proposes a diffusion-
based approach with implicit q-learning for offline RL. Wang et
al. [45] propose a expressive policy though diffusion modeling.
Chen et al. [ 8] propose to use diffusion models for behavior mod-
eling. Hu et al. [ 24] introduce temporal conditions for trajectory
generation. Despite these preliminary explorations, no work has
been payed for diffusion based auto-bidding which requires the
model to adapt to the random advertising environment. Li et al. [ 32]
utilize diffusion model in anti-money laundering.
Auto-bidding. Auto-bidding systems are widely used in program-
matic advertising, where they are employed to automatically place
bids on ad spaces. The main focus of such systems is to optimise a
given key performance indicator (KPI), such as the number of clicks
or conversions, while maintaining a certain budget [ 44]. Cai et al. [ 6]
proposed an RL-based approach to the problem of auto-bidding
for display advertising. They designed a bidding environment and
applied a deep RL algorithm to learn the optimal bidding strategy.
He et al. [ 21] propose a unified solution with RL to enable multi-
ple constraints for auto-bidding. Jin et al. extend the RL to enable
multi-agent competition [ 27]. Zhang et al. [ 35] also adopted the
RL framework for auto-bidding and showed that their approach
can outperform traditional bidding strategies. Wen et al. [ 46] pro-
pose a multi-agent-based approach for auto-bidding, which enables
the modeling of multiple auto-bidding agents at the same time to
include more information and also has been deployed online.
8 CONCLUSION
In this paper, we design a new paradigm for auto-bidding through
the lens of generative modeling. To achieve this goal, we propose
a decision-denoising diffusion approach to generate conditional
bidding trajectories and at the same time control the generated
samples under certain constraints. This new generative modeling
approach enables integrating different kinds of industrial metrics,
5045Generative Auto-bidding via Conditional Diffusion Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
which is the first unified model for bidding. Extensive experiments
on real-world simulation environments demonstrate the effective-
ness of the newly proposed approach. In the future, we will consider
developing new methods to accelerate the generation process and
new methods to ensure the robustness of DiffBid.
REFERENCES
[1]Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine.
2016. Learning to poke by poking: Experiential learning of intuitive physics.
Advances in neural information processing systems 29 (2016).
[2]Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola,
and Pulkit Agrawal. 2022. Is Conditional Generative Modeling all you need for
Decision Making?. In The Eleventh International Conference on Learning Represen-
tations.
[3]Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van
Den Berg. 2021. Structured denoising diffusion models in discrete state-spaces.
Advances in Neural Information Processing Systems 34 (2021), 17981â€“17993.
[4]Santiago Balseiro, Yuan Deng, Jieming Mao, Vahab Mirrokni, and Song Zuo. 2021.
Robust auction design in the auto-bidding world. Advances in Neural Information
Processing Systems 34 (2021), 17777â€“17788.
[5]Santiago R Balseiro, Yuan Deng, Jieming Mao, Vahab S Mirrokni, and Song Zuo.
2021. The landscape of auto-bidding auctions: Value versus utility maximization.
InProceedings of the 22nd ACM Conference on Economics and Computation. 132â€“
133.
[6]Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and
Defeng Guo. 2017. Real-time bidding by reinforcement learning in display adver-
tising. In Proceedings of the tenth ACM international conference on web search and
data mining. 661â€“670.
[7]Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang,
Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, and Chun-Yi Lee. 2022. Denoising
likelihood score matching for conditional score-based data generation. arXiv
preprint arXiv:2203.14206 (2022).
[8]Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. 2022. Offline
reinforcement learning via high-fidelity generative behavior modeling. arXiv
preprint arXiv:2209.14548 (2022).
[9]Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin,
Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer:
Reinforcement learning via sequence modeling. Advances in neural information
processing systems 34 (2021), 15084â€“15097.
[10] Harry L Chiesi, George J Spilich, and James F Voss. 1979. Acquisition of domain-
related information in relation to high and low domain knowledge. Journal of
verbal learning and verbal behavior 18, 3 (1979), 257â€“273.
[11] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah.
2023. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis
and Machine Intelligence (2023).
[12] Yuan Deng, Jieming Mao, Vahab Mirrokni, and Song Zuo. 2021. Towards efficient
auctions in an auto-bidding world. In Proceedings of the Web Conference 2021.
3965â€“3973.
[13] David S Evans. 2009. The online advertising industry: Economics, evolution, and
privacy. Journal of economic perspectives 23, 3 (2009), 37â€“60.
[14] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep rein-
forcement learning without exploration. In International conference on machine
learning. PMLR, 2052â€“2062.
[15] Scott Fujimoto, David Meger, Doina Precup, Ofir Nachum, and Shixiang Shane
Gu. 2022. Why Should I Trust You, Bellman? The Bellman Error is a Poor
Replacement for Value Error. In Proceedings of the 39th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 162), Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
Sabato (Eds.). PMLR, 6918â€“6943. https://proceedings.mlr.press/v162/fujimoto22a.
html
[16] Maor Gaon and Ronen Brafman. 2020. Reinforcement learning with non-
markovian rewards. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 34. 3980â€“3987.
[17] Jiayan Guo, Yaming Yang, Xiangchen Song, Yuan Zhang, Yujing Wang, Jing
Bai, and Yan Zhang. 2022. Learning Multi-granularity Consecutive User Intent
Unit for Session-based Recommendation. In Proceedings of the Fifteenth ACM
International Conference on Web Search and Data Mining (Virtual Event, AZ, USA)
(WSDM â€™22). Association for Computing Machinery, New York, NY, USA, 343â€“352.
https://doi.org/10.1145/3488560.3498524
[18] Louisa Ha. 2008. Online advertising research in advertising journals: A review.
Journal of Current Issues & Research in Advertising 30, 1 (2008), 31â€“48.
[19] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba,
and Sergey Levine. 2023. Idql: Implicit q-learning as an actor-critic method with
diffusion policies. arXiv preprint arXiv:2304.10573 (2023).
[20] Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao, Shan
Chen, Rongquan Bai, Mingzhou Xie, Miao Xu, Zhenzhe Zheng, Chuan Yu, HanLi, Jian Xu, and Kun Gai. 2020. Dynamic Knapsack Optimization Towards Ef-
ficient Multi-Channel Sequential Advertising. In Proceedings of the 37th Inter-
national Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 4060â€“4070.
http://proceedings.mlr.press/v119/hao20b.html
[21] Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, and
Xiaoqiang Zhu. 2021. A unified solution to constrained bidding in online display
advertising. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 2993â€“3001.
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in neural information processing systems 33 (2020), 6840â€“6851.
[23] Jonathan Ho and Tim Salimans. 2021. Classifier-Free Diffusion Guidance. In
NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.
[24] Jifeng Hu, Yanchao Sun, Sili Huang, SiYuan Guo, Hechang Chen, Li Shen, Lichao
Sun, Yi Chang, and Dacheng Tao. 2023. Instructed Diffuser with Temporal Condi-
tion Guidance for Offline Reinforcement Learning. arXiv preprint arXiv:2306.04875
(2023).
[25] R Huang, MWY Lam, J Wang, D Su, D Yu, Y Ren, and Z Zhao. 2022. FastDiff: A
Fast Conditional Diffusion Model for High-Quality Speech Synthesis. In IJCAI
International Joint Conference on Artificial Intelligence. IJCAI: International Joint
Conferences on Artificial Intelligence Organization, 4157â€“4163.
[26] Edwin T Jaynes. 1957. Information theory and statistical mechanics. Physical
review 106, 4 (1957), 620.
[27] Junqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, and Weinan Zhang. 2018.
Real-time bidding with multi-agent reinforcement learning in display advertis-
ing. In Proceedings of the 27th ACM international conference on information and
knowledge management. 2193â€“2201.
[28] Durk P Kingma, Tim Salimans, and Max Welling. 2015. Variational dropout and
the local reparameterization trick. Advances in neural information processing
systems 28 (2015).
[29] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. 2021. Offline Reinforcement
Learning with Implicit Q-Learning. In International Conference on Learning Rep-
resentations.
[30] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conserva-
tive q-learning for offline reinforcement learning. Advances in Neural Information
Processing Systems 33 (2020), 1179â€“1191.
[31] Juncheng Li and Pingzhong Tang. 2022. Auto-bidding Equilibrium in ROI-
Constrained Online Advertising Markets. arXiv preprint arXiv:2210.06107 (2022).
[32] Xujia Li, Yuan Li, Xueying Mo, Hebing Xiao, Yanyan Shen, and Lei Chen. 2023.
Diga: Guided diffusion model for graph recovery in anti-money laundering. In
Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 4404â€“4413.
[33] Sultan Javed Majeed and Marcus Hutter. 2018. On Q-learning Convergence for
Non-Markov Decision Processes.. In IJCAI, Vol. 18. 2546â€“2552.
[34] Diganta Misra. 2019. Mish: A self regularized non-monotonic activation function.
arXiv preprint arXiv:1908.08681 (2019).
[35] Zhiyu Mou, Yusen Huo, Rongquan Bai, Mingzhou Xie, Chuan Yu, Jian Xu, and
Bo Zheng. 2022. Sustainable Online Reinforcement Learning for Auto-bidding.
Advances in Neural Information Processing Systems 35 (2022), 2651â€“2663.
[36] Mirco Mutti, Riccardo De Santi, and Marcello Restelli. 2022. The importance
of non-markovianity in maximum state entropy exploration. In International
Conference on Machine Learning. PMLR, 16223â€“16239.
[37] Duy Nguyen-Tuong, Jan Peters, Matthias Seeger, and Bernhard SchÃ¶lkopf. 2008.
Learning inverse dynamics: a comparison. In European symposium on artificial
neural networks.
[38] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffu-
sion probabilistic models. In International Conference on Machine Learning. PMLR,
8162â€“8171.
[39] Weitong Ou, Bo Chen, Yingxuan Yang, Xinyi Dai, Weiwen Liu, Weinan Zhang,
Ruiming Tang, and Yong Yu. 2023. Deep landscape forecasting in multi-slot real-
time bidding. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 4685â€“4695.
[40] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen,
Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell.
2018. Zero-shot visual imitation. In Proceedings of the IEEE conference on computer
vision and pattern recognition workshops. 2050â€“2053.
[41] Aoyang Qin, Feng Gao, Qing Li, Song-Chun Zhu, and Sirui Xie. 2023. Learning
non-Markovian Decision-Making from State-only Sequences. In Thirty-seventh
Conference on Neural Information Processing Systems.
[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-
tional networks for biomedical image segmentation. In Medical Image Computing
and Computer-Assisted Interventionâ€“MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234â€“241.
[43] Pascal Vincent. 2011. A connection between score matching and denoising
autoencoders. Neural computation 23, 7 (2011), 1661â€“1674.
[44] Jun Wang, Weinan Zhang, Shuai Yuan, et al .2017. Display advertising with
real-time bidding (RTB) and behavioural targeting. Foundations and TrendsÂ® in
5046KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiayan Guo et al.
Information Retrieval 11, 4-5 (2017), 297â€“435.
[45] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. 2022. Diffusion Policies
as an Expressive Policy Class for Offline Reinforcement Learning. In The Eleventh
International Conference on Learning Representations.
[46] Chao Wen, Miao Xu, Zhilin Zhang, Zhenzhe Zheng, Yuhui Wang, Xiangyu Liu, Yu
Rong, Dong Xie, Xiaoyang Tan, Chuan Yu, et al .2022. A cooperative-competitive
multi-agent framework for auto-bidding in online advertising. In Proceedings
of the Fifteenth ACM International Conference on Web Search and Data Mining.
1129â€“1139.
[47] Yuxin Wu and Kaiming He. 2018. Group normalization. In Proceedings of the
European conference on computer vision (ECCV). 3â€“19.
[48] Haoqi Zhang, Lvyin Niu, Zhenzhe Zheng, Zhilin Zhang, Shan Gu, Fan Wu, Chuan
Yu, Jian Xu, Guihai Chen, and Bo Zheng. 2023. A Personalized Automated Bidding
Framework for Fairness-aware Online Advertising. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5544â€“5553.
[49] Peiyan Zhang, Jiayan Guo, Chaozhuo Li, Yueqi Xie, Jae Boum Kim, Yan Zhang,
Xing Xie, Haohan Wang, and Sunghun Kim. 2023. Efficiently Leveraging Multi-
level User Intent for Session-based Recommendation via Atten-Mixer Network. In
Proceedings of the Sixteenth ACM International Conference on Web Search and Data
Mining (, Singapore, Singapore,) (WSDM â€™23). Association for Computing Ma-
chinery, New York, NY, USA, 168â€“176. https://doi.org/10.1145/3539597.3570445
[50] Brian D. Ziebart. 2010. Modeling purposeful adaptive behavior with the principle of
maximum causal entropy . Ph. D. Dissertation. USA. Advisor(s) Bagnell, J. Andrew.
AAI3438449.
A APPENDIX
A.1 Notations
Table 4: Definition of Notations.
Symbol Definition
ğœ The trajectory index of a serving policy.
ğ’™(ğœ)ğ‘˜ Sequence of states of trajectory ğœin diffusion step ğ‘˜.
ğ’š(ğœ) Properties or conditions for ğœ.
ğ‘… Return of a trajectory.
ğ¸ Binary indicator variable.
ğµ The budget of the advertiser.
ğ¶ğ‘– Theğ‘–â€™s constraint.
ğ’ğ‘– Whether the advertiser wins impression ğ‘–.
ğ’—ğ‘– The true value of the impression ğ‘–.
ğ’ƒâˆ—
ğ‘–The optimal bidding price for the impression ğ‘–.
ğ’”ğ‘¡ The state at time period ğ‘¡.
Ë†ğ’‚ğ‘¡ Predicted bidding parameters at time period ğ‘¡.
ğœ†ğ‘– The bidding parameters.
ğğœƒ The denoising model that predict the noise.
ğ‘“ğœ™ The model that generate bids.
ğ›¼ğ‘˜ The cumulative product of 1âˆ’ğ›½ğ‘—,ğ‘—=0,...,ğ‘˜
ğ›½ğ‘˜ Schedualing factors.
ğ›¼ğ‘˜ 1-ğ›½ğ‘˜.
A.2 Diffusion Modeling
As a kind of generative model, diffusion models [ 22,43] use the
diffusion process to gradually denoise latent samples to generate
the new sample and have been widely used in generating pictures,
videos, and audio. One of the widely used diffusion models, denois-
ing diffusion probabilistic model (DDPM), consists of two processes:
Forward process. In the forward process, the noise is gradually
added to the latent variable, which is parameterized by a Markov
chain with the transition ğ‘(ğ’™ğ‘˜|ğ’™ğ‘˜âˆ’1)=N
ğ‘¥ğ‘˜;âˆšï¸
1âˆ’ğ›½ğ‘˜ğ’™ğ‘˜,ğ›½ğ‘˜ğ¼
,whereğ‘˜âˆˆ{1,...ğ¾}refers to the diffusion step, and ğ›½ğ‘˜âˆˆ (0,1)
is a pre-defined scale that controls the noise scale at step ğ‘˜. By
definingğ›¼ğ‘˜=Ãğ‘˜
ğ‘–=1ğ›¼ğ‘–=Ãğ‘˜
ğ‘–=1(1âˆ’ğ›½ğ‘–), we can have the conditional
distribution:
ğ‘(ğ‘¥ğ‘˜|ğ‘¥0)=N
ğ‘¥ğ‘˜;âˆšï¸
ğ›¼ğ‘˜ğ‘¥0,(1âˆ’ğ›¼ğ‘˜)ğ¼
(16)
In this paper, we apply cosine noise schedule to control the noise
by:
ğ›¼ğ‘˜=ğ‘”(ğ‘¡)
ğ‘”(0)=cosğ‘˜/ğ¾+ğ›¾
1+ğ›¾Â·ğœ‹
2
cosğ›¾
1+ğ›¾Â·ğœ‹
2, (17)
whereğ›¾is a constant. When ğ¾â†’ âˆ ,ğ‘(ğ‘¥ğ¾)approaches to a
standard Gaussian distribution [ 22]. Given the original trajectory
ğ‘¥0andğœ–âˆ¼ N( 0,ğ¼), we have the noisy version at ğ‘˜byğ‘¥ğ‘˜=âˆšï¸
ğ›¼ğ‘˜ğ‘¥0+ğœ–âˆšï¸
1âˆ’ğ›¼ğ‘˜
Reverse process. In the reverse process, diffusion models plan
to remove the added noise on ğ‘¥ğ‘˜and recursively recover ğ‘¥ğ‘˜âˆ’1.
To achieve this goal, a Gaussian distribution parameterized by
ğ‘ğœƒ(ğ’™ğ‘˜âˆ’1|ğ’™ğ‘˜)=N ğ’™ğ‘˜âˆ’1|ğğœƒ(ğ’™ğ‘˜,ğ‘˜),ğšºğœƒ(ğ’™ğ‘˜,ğ‘˜)is learned, where
ğğœƒ(ğ’™ğ‘˜,ğ‘˜)is the learned mean and ğšºğœƒ(ğ’™ğ‘˜,ğ‘˜)is the learned covari-
ance of the Gaussian distribution parameterized by a neural net-
work with parameter ğœƒ. For generating new samples, we can simply
use re-parameterization trick [ 28] to sample a noise ğ’™ğ¾âˆ¼ğğ¾+ğœ–ğœğ¾
and recursively denoise the sample by ğ‘ğœƒ(ğ’™ğ‘˜âˆ’1|ğ’™ğ‘˜)for generation.
Optimization. DDPM optimizes the Evidence Lower BOund (ELBO)
of generative models. In the context of the diffusion model, we can
take the latent samples as hidden variables and rewrite ELBO in
the following form:
Eğ‘[âˆ’logğ‘ğœƒ(ğ’™0)]
â‰¤Eğ‘
âˆ’logğ‘ğœƒ(ğ’™0:ğ¾)
ğ‘(ğ’™1:ğ¾|ğ’™0)
=Eğ‘[ğ·ğ¾ğ¿(ğ‘(ğ’™ğ¾|ğ’™0)||ğ‘ğœƒ(ğ’™ğ¾))]âˆ’Eğ‘[logğ‘ğœƒ(ğ’™0|ğ’™1)]
+Eğ‘"âˆ‘ï¸
ğ‘¡>1ğ·ğ¾ğ¿(ğ‘(ğ’™ğ‘˜âˆ’1|ğ’™ğ‘˜,ğ’™0)||ğ‘ğœƒ(ğ’™ğ‘˜âˆ’1|ğ’™ğ‘˜))#
,(18)
where the first term has no learned variable given variance ğ›½ğ‘˜is
fixed to constants, thus can be ignored during training. The second
term is the reconstruction term where ğ‘ğœƒ(Â·)is trained to recover the
original sample ğ’™0from the noise sample ğ’™1. The last term is the de-
noising term where ğ‘ğœƒ(Â·)is trained to denoise ğ’™ğ‘˜to get ğ’™ğ‘˜âˆ’1, thus
we can recurrently denoise the latent samples. In the original pa-
per [ 22] the author shows that the last term can be simplified to the
noise prediction objective Eğ‘˜,ğ’™ 0,ğh
||ğâˆ’ğğœƒâˆšï¸
ğ›¼ğ‘˜ğ’™0+âˆšï¸
1âˆ’ğ›¼ğ‘˜ğ,ğ‘˜
||i
,
whereğ›¼ğ‘˜=Ãğ‘˜
ğ‘–=1ğ›¼ğ‘˜=Ãğ‘˜
ğ‘–=1(1âˆ’ğ›½ğ‘˜).
A.3 Model Configuration
We parameterize the noise model ğœ–ğœƒwith a temporal U-Net [ 42],
consisting of 3 repeated residual blocks. Each block is consisted of
two temporal convolutions, followed by group normalization [ 47],
and a final Mish activation function [ 34]. Timestamp and condition
embeddings, both 128-dimensional vectors, are produced by sep-
arate 2-layered MLP (with 256 hidden units and Mish activation
function) and are concatenated together before getting added to
5047Generative Auto-bidding via Conditional Diffusion Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold10
5
0510Odd-Even
Model
IQL
DFUSER
Condition
0
1
Figure 7: Ability of Action Control.
the activation functions of the first temporal convolution within
each block. ğ‘“ğœ™is parameterized with a 3-layer MLP.
A.4 Pseudo-code
The process of training and inference of DiffBid is shown in Algo-
rithm 2 and Algorithm 1 respectively.
Algorithm 1 Bid Generation with DiffBid.
Input: noise model ğœ–ğœƒ, inverse dynamics ğ‘“ğœ™, guidance scale ğœ”,
condition ğ’š, max diffusion step ğ¾, scalesğ›½ğ‘¡, ğ‘¡=1,...,ğ¾ .
Output: Bidding parameters ğ’‚ğ‘¡
1:Get history of states ğ’”0:ğ‘¡;
2:Sample ğ’™ğ¾(ğœ)âˆ¼N( 0,ğ›½ğ¾ğ¼);
3:forğ‘˜=ğ¾,..., 0do
4: ğ’™ğ‘˜(ğœ)[:ğ‘¡]â†ğ’”0:ğ‘¡
5: Estimating noise Ë†ğœ–through Eq. (8)
6:(ğœ‡ğ‘˜âˆ’1,Î£ğ‘˜âˆ’1)â†Denoise(ğ’™ğ‘˜(ğœ),Ë†ğœ–ğ‘˜)
7: ğ’™ğ‘˜âˆ’1âˆ¼N(ğœ‡ğ‘˜âˆ’1,ğ›¼Î£ğ‘˜âˆ’1)
8:end for
9:Extract(ğ’”ğ‘¡âˆ’ğ¿:ğ‘¡,ğ’”â€²
ğ‘¡+1)from ğ’™0(ğœ)
10:Generate Ë†ğ’‚ğ‘¡=ğ‘“ğœ™(ğ’”ğ‘¡âˆ’ğ¿:ğ‘¡,ğ’”â€²
ğ‘¡+1);
11:return Ë†ğ’‚ğ‘¡.
Algorithm 2 Training of DiffBid.
Input: randomly initialized ğœƒ,ğœ™, bidding trajectory set D
Output: optimizedğœƒ,ğœ™
1:while not converge do
2: Sample a batch of trajectories BâˆˆD ;
3: for allğœâˆˆBdo
4: Sampleğ‘˜âˆ¼Uniform(1,ğ¾),ğœ–âˆ¼N( 0,ğ¼);
5: Compute ğ’™ğ‘˜(ğœ)viağ‘(ğ’™ğ‘˜(ğœ)|ğ’™0(ğœ))in Eq (7);
6: ComputeL(ğœƒ,ğœ™)by Eq (12);
7: Perform gradient descent to optimize ğœƒandğœ™;
8: end for
9:end while
10:return optimizedğœƒ,ğœ™A.5 Analytical Results for Action Control
We analyze the ability of different models in controlling actions.
To achieve this goal, we re-define the return function to be the
summation of actions in odd time steps minus the summation of
actions in even time steps. The results are shown in Figure 7. We find
DiffBid can better control the action than IQL. The main reason
is that controlling of actions is difficult for RL in long horizons.
Instead, DiffBid directly models the correlation of trajectories and
returns, thus can well handle the long trajectory situation.
A.6 Statistical Analyses for Bidding Trajectory
The study by [ 20] indicates that CE follows a power-law decline
as the number of winning impressions increases. Our statistical
analysis confirms that this finding holds true at every discrete time
step, with decay rates varying temporally due to the heterogeneous
nature of the impressions. Figure 8(a) shows three steps sampled
from the online advertising system, From which another key insight
is that the optimal bidding strategyâ€™s ğ‘ğ‘’âˆ—is equivalent to selecting
a specific number of winning impressions per time step. We denote
the number at time step ğ‘¡asğ‘›ğ‘¡.
Another finding illustrated in Figure 8(b) is that the costs of
impressions remain relatively stable throughout the total episode,
fluctuating by less than 5%. This stability allows us to approximate
the cost of each impression ğ‘ğ‘–with the average cost Â¯ğ‘=1
ğ‘Ãğ‘
ğ‘–=0ğ‘ğ‘–,
whereğ‘represents the number of winning impressions of the
total episode. Therefore, the total cost at each time step ğ‘ğ‘¡=ğ‘›ğ‘¡Â·
Â¯ğ‘. In auto-bidding modeling, ğ‘ğ‘¡can be calculated from the state
trajectory by using the difference in the remaining budget between
two consecutive steps. Consequently, we can conclude that the
optimal strategy correlates to a specific state trajectory.
A.7 Theoretical Analysis
We first give the definition of several decision process and then
show the theoretical analysis.
Definition A.1 (Markovian Decision Process (MDP)). MDP
is a stochastic mapping from a state-action pair to state-reward pairs.
Formally,T:SÃ—Aâ†’SÃ—R , whereTdenotes a stochastic mapping.
Definition A.2 (History-based Decision Process (HDP)). HDP
is a stochastic mapping from a history-action pair to observation-
reward pairs. Formally, P:Hâˆ—Ã—Aâ†’OÃ—R , wherePdenotes a
stochastic mapping.
We show that a sequential decision-making problem can be con-
structed to maximize the same objective. The main results are given
by [41] and we put the proofs here for completeness. To start, let
the ground-truth distribution of demonstrations be ğ‘âˆ—(ğ’™0(ğœ))and
the learned marginal distributions of state sequences be ğ‘ğœƒ(ğ’™0(ğœ)).
Then Eq. (4) is an empirical estimation of
Eğ‘âˆ—(ğ’”0)
logğ‘âˆ—(ğ’”0)+Eğ‘âˆ—(ğ’”1:ğ‘‡|ğ’”0)[logğ‘ğœƒ(ğ’”1:ğ‘‡|ğ’”0)]
(19)
Suppose the MLE yields the maximum, we will have ğ‘âˆ—
ğœƒ=ğ‘âˆ—. Then
we defineğ‘‰âˆ—(ğ‘ 0):=Eğ‘âˆ—(ğ‘ 1:ğ‘‡|ğ‘ 0)[logğ‘âˆ—(ğ‘ 1:ğ‘‡|ğ‘ 0)], and generalize it
to have ağ‘‰function:
ğ‘‰âˆ—(ğ‘ 0:ğ‘¡)=Eğ‘âˆ—(ğ‘ ğ‘¡+1:ğ‘‡|ğ‘ 0:ğ‘¡)[logğ‘âˆ—(ğ‘ ğ‘¡+1:ğ‘‡|ğ‘ 0:ğ‘¡)] (20)
which comes with a Bellman optimal equation:
ğ‘‰âˆ—(ğ‘ 0:ğ‘¡):=Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)[ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡)+ğ‘‰âˆ—(ğ‘ 0:ğ‘¡+1)] (21)
5048KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiayan Guo et al.
Table 5: Parameters of the Real Advertising System.
Parameters V
alues
Numb
er of advertisers 30
Time
steps in an episode, ğ‘‡ 96
Minimum
number of impression opportunities ğ‘min 50
Maximum
number of impression opportunities ğ‘max 300
Minimum
budget 1000
Yuan
Maximum budget 4000
Yuan
Value of impression opportunities in stage 1, ğ‘£1
ğ‘—,ğ‘¡0âˆ¼1
V
alue of impression opportunities in stage 2, ğ‘£2
ğ‘—,ğ‘¡0âˆ¼1
Minimum
bid price,ğ´min 0
Yuan
Maximum bid price, ğ´max 1000
Yuan
Maximum value of impression opportunity, ğ‘£ğ‘€ 1
Maximum
market price, ğ‘ğ‘€ 1000
Yuan
(a) Cost Effectiveness Curve Samples from
Three Steps
(
b) Impression Cost Fluctuation at Differ-
ent Time Steps
Figure 8: Statistical Results From Online Advertising System.
withğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡):=logğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)=logğ‘âˆ—ğ‘(ğ‘ ğ‘¡|ğ‘ 0:ğ‘¡)ğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡)ğ‘‘ğ‘¡,
ğ‘‰âˆ—(ğ‘ 0:ğ‘‡):=0. It is worth noting that the ğ‘Ÿdefined above involves
the optimal policy, which may not be known a priori. We can re-
solve this by replacing it with ğ‘Ÿğ›¼for an arbitrary policy ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡).
All Bellman identities and updates should still hold. The entailed
Bellman update, value iteration, for arbitrary ğ‘‰andğ›¼is
ğ‘‰(ğ‘ 0:ğ‘¡)=Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)[ğ‘Ÿğ›¼(ğ‘ 0:ğ‘¡,ğ‘ ğ‘¡+1)+ğ‘‰(ğ‘ 0:ğ‘¡+1)]. (22)
We then define ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ğ‘¡,ğ‘ 0:ğ‘¡):=ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡)+logğ‘âˆ—ğ‘(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)to
construct ağ‘„function:
ğ‘„âˆ—(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡):=Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)[ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ğ‘¡,ğ‘ 0:ğ‘¡)+ğ‘‰âˆ—(ğ‘ 0:ğ‘¡+1)],(23)
which entails a Bellman update, Q backup, for arbitrary ğ›¼,ğ‘„andğ‘‰
ğ‘„(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)=Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)[ğ‘Ÿğ›¼(ğ‘ 0:ğ‘¡,ğ‘ğ‘¡,ğ‘ ğ‘¡+1)+ğ‘‰(ğ‘ 0:ğ‘¡+1)].(24)
Also note that the ğ‘‰andğ‘„in identities Eq. (23) and Eq. (25) re-
spectively are not necessarily associated with the policy ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡).
Slightly overloading the notations, we use ğ‘„ğ›¼,ğ‘‰ğ›¼to denote the
expected returns from policy ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡). By now, we finish the con-
struction of atomic algebraic components and move on to check if
the relations between them align with the algebraic structure of a se-
quential decision-making problem. We first prove the construction
above is valid at optimality.
Lemma A.1. Whenğ‘“ğ›¼(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)=ğ‘„âˆ—(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)âˆ’ğ‘‰âˆ—(ğ‘ 0:ğ‘¡),ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)
is the optimal policy.Proof. Note that the construction gives us
ğ‘„âˆ—(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)
=Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)
ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡)+logğ‘âˆ—
ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)+ğ‘‰âˆ—(ğ‘ 0:ğ‘¡+1)
=logğ‘âˆ—
ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)+Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)
ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡)+ğ‘‰âˆ—(ğ‘ 0:ğ‘¡+1)
=logğ‘âˆ—
ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)+ğ‘‰âˆ—(ğ‘ 0:ğ‘¡)(25)
â–¡
Obviously,ğ‘„âˆ—(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)lies in the hypothesis space of ğ‘“ğ›¼(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡).
It indicates that we need to either parameterize ğ‘“ğ›¼(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)orğ‘„(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡).
Whileğ‘„ğ›¼andğ‘‰ğ›¼are constructed from the optimality, the derived
ğ‘„ğ›¼andğ‘‰ğ›¼measure the performance of an interactive agent when
it executes with the policy ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡). They should be consistent.
Lemma A.2. ğ‘‰ğ›¼(ğ‘ 0:ğ‘¡)andEğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)[ğ‘„ğ›¼(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)]yield the same
optimal policy ğ‘âˆ—ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)
Proof.
Eğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)[ğ‘„ğ›¼(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)]
:=Eğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)
Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)[ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ğ‘¡,ğ‘ 0:ğ‘¡)+ğ‘‰ğ›¼(ğ‘ 0:ğ‘¡+1)]
=Eğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)
logğ‘âˆ—
ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)+ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡)+ğ‘‰ğ›¼(ğ‘ 0:ğ‘¡+1)
=Eğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)[ğ‘Ÿ(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡)âˆ’ğ»ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)+ğ‘‰ğ›¼(ğ‘ 0:ğ‘¡+1)]
âˆ’ğ‘‡âˆ’1âˆ‘ï¸
ğ‘˜=ğ‘¡+1Eğ‘âˆ—(ğ‘ ğ‘¡+1:ğ‘˜|ğ‘ 0:ğ‘¡)[ğ»ğ›¼(ğ‘ğ‘˜|ğ‘ 0:ğ‘˜)]
whereH(Â·) is the entropy term. The last line is derived by recur-
sively applying the Bellman equation in the line above until ğ‘ 0:ğ‘‡.
As an energy-based policy, ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)â€™s entropy is inherently max-
imized [ 26]. Therefore, within the hypothesis space, ğ‘âˆ—ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)
that optimizes ğ‘‰ğ›¼(ğ‘ 0:ğ‘¡)also leads to the optimal expected return
Eğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)[ğ‘„ğ›¼(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡)]. â–¡
Given the convergence proof by Ziebart [50], we have:
Lemma A.3. Ifğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)is accessible and ğ‘âˆ—ğ›¾(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡)is
known, soft policy iteration and soft ğ‘„learning both converge to
ğ‘âˆ—ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)=ğ‘âˆ—ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)âˆexp(ğ‘„âˆ—(ğ‘ğ‘¡;ğ‘ 0:ğ‘¡))under conditions.
Lemma 3 means given ğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)andğ‘âˆ—ğ›¾(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡), we can
recoverğ‘âˆ—ğ›¼through reinforcement learning methods, instead of
the proposed MLE. So ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)is a viable policy space for the
constructed sequential decision-making problem. Together, Lemma
A.1, Lemma A.2 and Lemma A.3 provide proof for a valid sequential
decision-making problem that maximizes the same objective of
MLE, by Lemma A.4.
Lemma A.4 (MLE as non-Markovian decision-making pro-
cess). Assuming the Markovian transition ğ‘ğ›¾âˆ—(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡)is known,
the ground-truth conditional state distribution ğ‘âˆ—(ğ‘ ğ‘¡+1|ğ‘ 0:ğ‘¡)for demon-
stration sequences is accessible, we can construct a non-Markovian
sequential decision-making problem, based on a reward function
ğ‘Ÿğ›¼(ğ‘ ğ‘¡+1,ğ‘ 0:ğ‘¡):=logâˆ«
ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡)ğ‘ğ›¾âˆ—(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡)ğ‘‘ğ‘ğ‘¡for an arbitrary
energy-based policy ğ‘ğ›¼(ğ‘ğ‘¡|ğ‘ 0:ğ‘¡). Its objective is
ğ‘‡âˆ‘ï¸
ğ‘¡=0Eğ‘âˆ—(ğ‘ 0:ğ‘¡)
ğ‘‰ğ‘ğ›¼(ğ‘ 0:ğ‘¡)
=Eğ‘âˆ—(ğ‘ 0:ğ‘‡)"ğ‘‡âˆ‘ï¸
ğ‘¡=0ğ‘‡âˆ‘ï¸
ğ‘˜=ğ‘¡ğ‘Ÿğ›¼(ğ‘ ğ‘˜+1;ğ‘ 0:ğ‘˜)#
(26)
ğ‘‰ğ‘ğ›¼(ğ‘ 0:ğ‘¡):=Eğ‘âˆ—(ğ‘ ğ‘¡+1:ğ‘‡|ğ‘ 0:ğ‘¡)[Ãğ‘‡
ğ‘˜=1ğ‘Ÿğ›¼(ğ‘ ğ‘˜+1;ğ‘ 0:ğ‘˜)]is the value func-
tion ofğ‘ğ›¼. This objective yields the save optimal policy as the Maxi-
mum Likelihood Estimation Eğ‘âˆ—(ğ‘ 0:ğ‘‡)[logğ‘ğœƒ(ğ‘ 0:ğ‘‡)].
5049