MISP: A Multimodal-based Intelligent Server Failure Prediction
Model for Cloud Computing Systems
Xianting Luâˆ—
Zhejiang University
Hangzhou, China
xtlu@zju.edu.cnYunong Wangâˆ—
Alibaba Cloud, Alibaba Group
Hangzhou, China
yunong.wyn@alibaba-inc.comYu Fu
ğ‘Lanzhou University
Lanzhou, China
ğ‘Zhejiang University
Hangzhou, China
yufu1994@zju.edu.cn
Qi Sun
Zhejiang University
Hangzhou, China
qisunchn@zju.edu.cnXuhua Ma
Alibaba Cloud, Alibaba Group
Hangzhou, China
xuhua.mxh@alibaba-inc.comXudong Zheng
Alibaba Cloud, Alibaba Group
Hangzhou, China
xudong.zxd@alibaba-inc.com
Cheng Zhuoâ€ 
Zhejiang University
Hangzhou, China
czhuo@zju.edu.cn
ABSTRACT
Traditional server failure prediction methods predominantly rely
on single-modality data such as system logs or system status curves.
This reliance may lead to an incomplete understanding of system
health and impending issues, proving inadequate for the complex
and dynamic landscape of contemporary cloud computing envi-
ronments. The potential of multimodal data to provide compre-
hensive insights is widely acknowledged, yet the lack of a holistic
dataset and the challenges inherent in integrating features from
both structured and unstructured data have impeded the explo-
ration of multimodal-based server failure prediction. Addressing
these challenges, this paper presents an industrial-scale, compre-
hensive dataset for server failure prediction, comprising nearly 80
types of structured and unstructured data sourced from real-world
industrial cloud systems1. Building on this resource, we intro-
duce MISP, a model that leverages multimodal fusion techniques
for server failure prediction. MISP transforms multimodal data
into multi-dimensional sequences, extracts and encodes features
both within and across the modalities, and ultimately computes
the failure probability from the synthesized features. Experiments
demonstrate that MISP significantly outperforms existing methods,
âˆ—Both authors contributed equally to this research.
â€ Corresponding author.
1This dataset will be made accessible for future research and can be found at https:
//anonymous.4open.science/r/Cloud-Multimodal-Dataset.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671568enhancing prediction accuracy by approximately 25% over previous
state-of-the-art approaches.
CCS CONCEPTS
â€¢Computer systems organization â†’Reliability ;â€¢Hardware
â†’Failure prediction .
KEYWORDS
Failure prediction; Cloud computing; Multimodal data; Time series
ACM Reference Format:
Xianting Lu, Yunong Wang, Yu Fu, Qi Sun, Xuhua Ma, Xudong Zheng,
and Cheng Zhuo. 2024. MISP: A Multimodal-based Intelligent Server Failure
Prediction Model for Cloud Computing Systems. In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671568
1 INTRODUCTION
The expansion of cloud computing platforms has been instrumen-
tal in meeting the growing demand for computing and storage
resources. However, the scale of these platforms brings inherent
challenges, notably server failures, which can lead to severe con-
sequences including data loss, business disruptions, and financial
losses [ 23]. These failures bear significant costs, impacting revenue,
customer trust, and operational efficiency. Recognizing the critical-
ity of server reliability, leading cloud providers such as Amazon
Web Services (AWS) and Microsoft Azure prioritize server availabil-
ity as part of their service commitments [47].
Historically, cloud systems have employed failure prediction
techniques to maintain system availability, as illustrated in Figure 1.
These methods typically rely on single-source data, such as system
logs or predefined thresholds, to predict server failures [ 31]. While
these methods have their merits, they often fall short in providing a
comprehensive understanding of system health due to their reactive
5509
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xianting Lu et al.
Figure 1: Workflows of the traditional and the proposed server failure prediction in cloud computing systems.
stance and narrow data focus, making them less effective in the
intricate and dynamic realm of cloud computing [49].
The pursuit of accurate and holistic server failure prediction
has turned the spotlight on the potential of multimodal data. This
type of data, offering an extensive view of system status through
diverse data modalities, holds the promise of superior prediction
capabilities. Although previous studies have acknowledged the
value of using either semantic or numeric time series for failure
prediction [ 3,4,7,9,20,32,36,50], the journey towards an effective
multimodal data-based prediction model has been fraught with
challenges. These include the scarcity of comprehensive datasets
from elite data centers, the intricacies of merging structured and un-
structured data, and the complexities of developing models adept at
handling multimodal information [ 18]. Such hurdles have restricted
in-depth investigations into multimodal-based server failure pre-
diction, often limiting the focus to single data type features and
neglecting the rich potential of cross-modality features inherent in
multimodal sequential data [18, 39, 49].
In this paper, we concentrate on server failure prediction by utiliz-
ing status data from Alibaba Cloud. We introduce a comprehensive
dataset1, encompassing 79 types of structured and unstructured
data gathered from real-world industrial cloud computing environ-
ments. This dataset, designed to address gaps in multimodal-based
failure prediction research, incorporates a diverse range of multi-
modal data. It includes semantic time series, derived from system
event logs, featuring sequences of event indexes tied to specific
exceptions. These sequences occur at irregular intervals, reflect-
ing the variability in event reporting. Complementing these are
numeric time series, which consist of multivariate sequences
capturing system metrics like CPU usage and PSU power consump-
tion at regular intervals, primarily sourced from sensors. Together,
these data types offer a comprehensive view of system health and
behavior.
The datasetâ€™s real-world nature exhibits several unique features:
â€¢The rarity of server failures results in an imbalanced data
distribution, with fewer failure instances compared to normal
ones.â€¢Not every sample from a failed server directly correlates with
the failure event. Some resemble healthy machine samples,
causing data ambiguity and complicating the analysis.
â€¢The dataset, with 79 types of data from semantic and numeric
series, poses the challenge of data diversity fusion.
Leveraging this rich, real-world multimodal dataset, this paper
introduces a Multimodal-based Intelligent Server failure Prediction
model (MISP). This model utilizes multimodal fusion techniques to
process, extract, and encode features from the data, ultimately as-
sessing failure probability. MISP not only addresses the limitations
of traditional prediction methods but also tackles the complexities
associated with multimodal data. With Alibaba Cloudâ€™s vast infras-
tructure and a user base exceeding 10,000,000 [ 1], even marginal
improvements in prediction accuracy can yield substantial benefits.
In summary, the key contributions of this paper are:
â€¢To the best of our knowledge, this is the first industrial-grade
comprehensive dataset for server failure prediction , with data
sourced from real-world industrial cloud computing systems.
This dataset tackles the ongoing issue of dataset scarcity and
will be accessible for future research.
â€¢We introduce MISP, a Multimodal-based Intelligent Server
failure Prediction model, employing advanced attention mech-
anisms to forecast server failures. MISP efficiently manages
model complexity and feature fusion by utilizing self- and
cross-attention techniques to identify significant features
within and across the modalities.
â€¢We propose a server-level loss calculation method to address
data imbalance and ambiguity. This approach dynamically
tunes training sample weights, enhancing the differentiation
between failure and non-failure instances.
Experimental results reveal that with our multimodal dataset, the
proposed MISP model can manifest substantial advancements in
server failure prediction accuracy. Compared to the previous method-
ologies, MISP demonstrates a notable improvement, increasing pre-
cision by at least 31.6%, recall by at least 15.9%, and the ğ¹1-score
by a minimum of 24.6%. Furthermore, the introduced server-level
loss exhibits significant amelioration compared to traditional loss
5510MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
formulations like cross-entropy or A-softmax, thereby amplifying
the modelâ€™s decision-making proficiency.
2 RELATED WORKS
2.1 Failure prediction
Failure prediction is essential for cloud computing systemsâ€™ sta-
bility and reliability. Various methodologies, including statistical
threshold-based approaches, nearest neighbor algorithms, time
series analysis, and Support Vector Machines (SVM), have been ex-
plored for this purpose [ 4,7,20,38]. Recently, the growth of cloud
platforms and increasing data complexity have prompted interest
in deep learning-based failure prediction techniques. While single-
modality-based methods utilizing deep learning models on logs or
time series data have been discussed [ 12,30,32,37,46], they tend
to overlook the rich information that could be gleaned from multi-
ple data modalities. Consequently, multimodal-based approaches
have been developed, aiming to harness and integrate features from
various modalities using deep learning models [ 21,40,48,49,49].
These studies underscore the potential of leveraging deep learn-
ing for failure prediction using multimodal data. However, these
approaches may not be directly applicable to our specific dataset
scenario and might overlook potential cross-modality features in
multimodal sequential data on a temporal scale. This underscores
the need for further research to effectively address failure
prediction considering these cross-modality features.
2.2 Multimodal Time Series
Our dataset encompasses semantic and numeric time series, prompt-
ing a review of relevant techniques for feature extraction and fusion
from this multimodal data. Time series analysis has attracted signif-
icant attention, with classical models like ARIMA being noteworthy.
Yet, the growing complexity of time series data sources necessitates
more sophisticated models. Deep learning and Transformer-based
models have shown promise in capturing the intricate and long-
term characteristics of time series data [ 10,14,15,26,34,42,43,45,
51].
Semantic sequence analysis, often centered around natural lan-
guage tasks, relies on the conversion of raw text into numeric
vectors. While statistical embedding techniques like One-Hot, N-
Grams, and TF-IDF provide foundational word representations,
they may not fully capture contextual nuances [ 13,16,33]. Neural-
network-based methods such as Word2Vec and GloVe, and fur-
ther, pre-training techniques in language models like ELMo, GPT,
and BERT, have advanced the field by generating contextual em-
beddings, though challenges remain in interpreting the varying
contextual meanings of words [5, 25, 27â€“29].
In scenarios involving multiple data types, feature fusion tech-
niques such as early fusion, late fusion, and hybrid fusion play a piv-
otal role. They merge features from different data sources, a strategy
employed across various domains including fashion, traffic, com-
merce, healthcare, and visual-language tasks [ 2,6,17,19,24,41,44].
The diverse technologies in these multi-modal domains significantly
enrich the approach to multimodal failure prediction. Nonethe-
less, effectively integrating and extracting cross-modality features
within specific contexts, such as our scenario, continues to be a
challenging frontier.3 DATASET
3.1 Overview
Our dataset originates from Alibaba Cloudâ€™s computing systems,
encapsulating about 5.7 million samples over 120 days from over
100,000 servers. Itâ€™s divided into positive and negative samples. The
positive samples represent data from the servers that encountered
failure events, collected from the three days before the server fail-
ures. Conversely, the negative samples are randomly selected from
failure-free servers to ensure continuity. The temporal scope of
each sample is confined to three days, encompassing semantic time
series from System Runtime Event Logs and numeric time series
from System Status Curves. Data is split into a training set (first 75
days) and a test set (last 45 days), as detailed in Table 1.
Table 1: Summary of faulty/fault-free servers and posi-
tive/negative samples for training and test sets from the pro-
posed dataset
Set# of servers # of samples
Faulty Fault-free Total Positive Negative Total
Train 2206 88820 91026 112287 3646372 3758659
Test 778 65745 66523 50442 1966329 2016771
3.2 Semantic Time Series
The semantic time series, sourced from system runtime event logs,
highlight 74 main exception events across four categories: hardware,
operating system, network, and others (un-categorizable events),
as summarized in Table 8 in Appendix A.1. Additionally, a statisti-
cal analysis revealing a disproportionate distribution of exception
events is summarized in Table 9 in Appendix A.2. The lengths of
semantic time series in the dataset vary, with approximately 80% of
the sequences being shorter than 500, as shown in Figure 8 in Ap-
pendix A.2. The variation in time intervals among elements within
the semantic sequences is evident across different samples, show-
casing the unpredictable nature of these intervals as highlighted
by the system event logs. There are instances where event logs
are densely clustered at specific moments (Figure 2(c)), whereas
in other instances, substantial gaps in time are observed between
consecutive occurrences (Figure 2(b)).
3.3 Numeric Time Series
The numeric time series, derived from system status curves, are
recorded every 5 minutes, which indicates the identical intervals
of the elements in the time series. These metrics include 5 status
curves: CPU/memory/PSU power consumptions and CPU/memory
utilization, which span three days with 864 time points each and
hence create a multivariate series when combined. Figure 2 shows
part of the curves. With approximately 5.7 million numeric time se-
ries samples, the dataset showcases variations in periodic patterns,
fluctuation magnitudes, and waveform characteristics. Numeric
time series document the systemâ€™s status over time, with failures
being infrequent occurrences. This similarity in characteristics be-
tween positive (faulty server) and negative (healthy server) samples
5511KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xianting Lu et al.
(a) Positive Sample 1.
 (b) Negative Sample 1.
(c) Positive Sample 2.
 (d) Negative Sample 2.
Figure 2: Examples of semantic and numeric time series data from positive/negative samples with different characteristics.
poses a challenge in differentiating them. For example, Positive Sam-
ple 1 in Figure 2(a) and Negative Sample 1 in Figure 2(b) exhibit
comparable trends over time.
3.4 Relative Time
When analyzing the dataset, we compute the relative time to illus-
trate the temporal relationships between elements within the same
time series. This concept is depicted in Figure 3, where the relative
time of theğ‘š-th element is the interval from the initial sampling
timepoint to the ğ‘š-th elementâ€™s occurrence in either numeric or
semantic series. Relative time proves crucial in real-world industrial
settings, enhancing analyses of regularly and irregularly intervalled
numeric and semantic time series, respectively. We also document
the absolute time, laying groundwork for future studies on how
temporal variations influence server failures. This study focuses on
predicting server failures, avoiding deep-dives into failure classifi-
cations. However, it provides detailed event categorization, setting
the stage for multi-class classification research. This could deepen
understanding of server failures in cloud computing by examining
semantic and numeric seriesâ€™ unique patterns across failure types.
Figure 3: Relative time of the ğ‘š-th input sequence element.4 METHOD
With the aforementioned dataset, a multimodal sample is denoted as
ğ‘¿=(ğ‘¿ğ‘,ğ‘»ğ‘,ğ‘¿ğ‘†,ğ‘»ğ‘†,ğ‘¦), where ğ‘¿ğ‘âˆˆRğ¿ğ‘Ã—ğ‘‘0is the numeric time
series and ğ‘»ğ‘âˆˆRğ¿ğ‘is the corresponding relative time, ğ‘¿ğ‘†âˆˆRğ¿ğ‘†
is the semantic time series and ğ‘»ğ‘†âˆˆRğ¿ğ‘†is its relative time, and
ğ‘¦is the label of the sample. In our context, the task of predicting
server failures is framed as a classification problem, determining
failure occurrence from server samples.
4.1 Overview
Figure 4 illustrates the overall architecture of the proposed MISP
model, encompassing three interlinked modules:
â€¢Embedding Module, comprising Semantic, Numeric, and
Relative Time Embeddings. Due to the distinct nature of
different series, specific embedding methods are applied to
each modality, with temporal information integrated via
Relative Time Embedding.
â€¢Feature Extractor, comprising Parallel and Cross Encoders,
plus a Fusion layer. The Parallel Encoder consists of parallel
encoder layers for independent feature extraction from each
time series type. The Cross Encoder contains cross encoder
layers for fusion of modalities. The Fusion layer is employed
post extraction to combine encoder outputs.
â€¢Prediction Module, using a classifier on Feature Extractor
outputs for server failure prediction with a server-level rule.
5512MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Figure 4: Overview of the proposed MISP model architecture.
4.2 Embedding Module
4.2.1 Semantic Embedding and Numeric Embedding. For the
semantic time series ğ‘¿ğ‘†, we utilize a dynamic embedding layer, akin
to BERT, to transform the elements into vectors. Prior to this, the
semantic time series are standardized in length through padding
with zeros. Subsequently, a dynamic embedding function maps
each element in the time series to a ğ‘‘-dimensional vector space,
producing an embedded semantic time series ğ‘¹0
ğ‘†âˆˆRğ¿ğ‘†Ã—ğ‘‘, where
ğ¿ğ‘†denotes the length. The embedding parameters of the function
are fine-tuned during training.
In contrast, Numeric Embedding adopts a distinct strategy to
incorporate continuous values from numeric time series. Treat-
ing individual elements of numeric time series as separate entities
presents a challenge. Convolution kernels are thus used to convert
each numeric row into row vectors, extending the numeric time
series dimensionality to align with the embedded semantic time
series. During this process, ğ‘‘convolution kernels glide over the
length of the numeric time series ğ‘¿ğ‘, executing convolution op-
erations. And the embedded numeric time series is represented as
ğ‘¹0
ğ‘âˆˆRğ¿ğ‘Ã—ğ‘‘, whereğ¿ğ‘†denotes the length.
After obtaining the outputs of Semantic Embedding and Numeric
Embedding, we integrate the relative time information of the ele-
ments through a relative time embedding technique. Then the final
integrated outputs serve as the inputs for the Feature Extractor.
4.2.2 Relative Time Embedding. To address the limitations of
conventional position embedding, especially in scenarios with irreg-
ular time intervals between elements in time series, we introduce
a relative time embedding technique. This approach is particu-
larly useful for capturing the temporal dynamics of events with
unpredictable reporting times, which introduces uncertainty in the
intervals between elements of semantic time series. The relative
time embedding process, illustrated in Figure 5, embeds the relative
timing of sequence elements at three resolutions: days ( ğ¸ğ·), hours
(ğ¸ğ»), and minutes ( ğ¸ğ‘€). This multi-resolution embedding captures
the nuanced temporal relationships within the data, enriching the
modelâ€™s input with temporal context.
Figure 5: The process of relative time embedding.
The comprehensive embedding of an element within the time se-
ries, incorporating both its value and its relative temporal position,
is given by:
ğ’›0,ğ‘š=ğ‘¬ğ’†(ğ‘¥ğ‘š)+ğ‘¬ğ‘«(ğ‘¡ğ‘š)+ğ‘¬ğ‘¯(ğ‘¡ğ‘š)+ğ‘¬ğ‘´(ğ‘¡ğ‘š) (1)
where ğ‘¬ğ’†represents the embedding function for either semantic or
numeric data. This unified embedding approach ensures that both
numeric and semantic time series are represented in a dimensionally
consistent manner, facilitating their integration and analysis in the
subsequent feature extraction phase.
4.3 Feature Extractor
4.3.1 Parallel Encoder. The Parallel Encoderâ€™s design, shown in
Figure 6, processes numeric and semantic time series in parallel,
employing self-attention to delve into each modalityâ€™s data relation-
ships. This approach facilitates a nuanced integration of insights,
aiming to boost prediction accuracy by maintaining separate yet
concurrent analyses for each data type.
At the core of the Parallel Encoder are ğ‘™encoder layers, each
building upon the output of its predecessor. For a given layer ğ‘–, letâ€™s
denote the hidden states of the numeric and semantic time series
asğ’ğ‘–âˆ’1
Î ,ğ‘andğ’ğ‘–âˆ’1
Î ,ğ‘†, respectively. These states undergo projection
operations to generate their respective queries, keys, and values for
5513KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xianting Lu et al.
Figure 6: The structure of the Parallel Encoder.
the self-attention computation. The process for the numeric time
series is encapsulated by:
ParaAtt(ğ‘¸ğ‘–
Î ,ğ‘,ğ‘²ğ‘–
Î ,ğ‘,ğ‘½ğ‘–
Î ,ğ‘)=Softmax(ğ‘¸ğ‘–
Î ,ğ‘ğ‘²ğ‘–
Î ,ğ‘ğ‘‡
âˆš
ğ‘‘â€²)ğ‘½ğ‘–
Î ,ğ‘(2)
where ğ‘¸ğ‘–
Î ,ğ‘,ğ‘²ğ‘–
Î ,ğ‘, and ğ‘½ğ‘–
Î ,ğ‘represent the query, key, and value
matrices derived from the preceding layerâ€™s hidden state for the
numeric time series. A parallel computation is performed for the
semantic time series, yielding its self-attention output.
Following the self-attention phase, each encoder layer applies
a sequence of transformationsâ€”a Dense Layer, LayerNorm with
residual connections, a Feedforward Layer, and another LayerNorm
operationâ€”to refine the extracted features and produce the layerâ€™s
output hidden state. The final step of the Parallel Encoderâ€™s process
involves extracting class token vectors from the output states of
the last layer for both numeric and semantic modalities, which are
then concatenated. A fully-connected layer, activated by a ğ‘‡ğ‘ğ‘›â„
function, merges these multimodal features, formalized as:
ğ’Î =Tanh([ğ’›ğ‘™
Î ,ğ‘;ğ’›ğ‘™
Î ,ğ‘†]ğ‘¾Î +ğ’ƒÎ ), (3)
where ğ’›ğ‘™
Î ,ğ‘andğ’›ğ‘™
Î ,ğ‘†represent the class token vectors for the nu-
meric and semantic time series, respectively, and ğ‘¾Î âˆˆR2ğ‘‘Ã—ğ‘‘,
ğ’ƒÎ âˆˆRğ‘‘are the fully-connected layerâ€™s weight matrix and bias
vector, respectively.
4.3.2 Cross Encoder. The Cross Encoder, depicted in Figure 7,
employs cross-attention mechanisms to integrate features from
both the semantic and numeric time series. This integration is
achieved by alternating the roles of queries, keys, and values be-
tween the two modalities across each layer of the encoder, as shown
in Figure 7. This process enables a rich interaction between the
modalities, enhancing the modelâ€™s ability to capture and leverage
the interdependencies between them.
In each layer ğ‘–, we initialize the input hidden states for the nu-
meric and semantic time series as ğ’ğ‘–âˆ’1
X,ğ‘andğ’ğ‘–âˆ’1
X,ğ‘†, respectively. For
the numeric time series, queries are generated from ğ’ğ‘–âˆ’1
X,ğ‘, while
keys and values are derived from the semantic time series ğ’ğ‘–âˆ’1
X,ğ‘†.
Figure 7: The structure of the Cross Encoder.
The cross-attention function is applied as:
CroAtt(ğ‘¸ğ‘–
X,ğ‘,ğ‘²ğ‘–
X,ğ‘†,ğ‘½ğ‘–
X,ğ‘†)=Softmax(ğ‘¸ğ‘–
X,ğ‘ğ‘²ğ‘–
X,ğ‘†ğ‘‡
âˆš
ğ‘‘â€²)ğ‘½ğ‘–
X,ğ‘†(4)
where ğ‘¸ğ‘–
X,ğ‘,ğ‘²ğ‘–
X,ğ‘†, and ğ‘½ğ‘–
X,ğ‘†are the query, key, and value matrices
for the numeric and semantic series inputs, respectively.
A similar process is executed with the roles of the numeric and
semantic series reversed, facilitating a comprehensive integration
of features from both modalities, as illustrated in Figure 7. This iter-
ative exchange of roles between the two modalities within the same
layer captures their complex interrelations. To enhance learning
efficiency and model performance, we implement weight-sharing
across the encoder layers, similar to approaches in vision-language
integration tasks. This strategy involves using the same weight ma-
trices for projection, dense, and feedforward layers within the same
cross encoder layer for both modalities. This shared architecture
not only streamlines the model but also deepens its understanding
of the intrinsic relationships between numeric and semantic data,
represented by the output vector ğ’X.
4.3.3 Fusion Layer. The Fusion Layer serves as the integration
point for features derived from both the Parallel and Cross En-
coders. This layer combines these features by first concatenating
them and then processing the combined feature vector through a
fully-connected layer, followed by a ğ‘‡ğ‘ğ‘›â„ activation function. The
process yields the final output of the Feature Extractor:
ğ’ğ¹=Tanh([ğ’Î ;ğ’X]ğ‘¾ğ¹+ğ’ƒğ¹), (5)
where ğ‘¾ğ¹andğ’ƒğ¹represent the learnable weight matrix and bias
vector within the fully-connected layer.
4.4 Prediction Module
4.4.1 Classifier. Within the Prediction Module, a linear classifier
is used to determine the confidence level of the features extracted
for an input sample. This step aligns with real-world scenarios by
adopting a server-level rule for identifying faulty servers. Specif-
ically, a server is flagged as faulty if any of its samples suggests
5514MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
a potential failure with high confidence, reflecting a practical ap-
proach to server failure detection.
4.4.2 Server-level Loss Calculation Method. Since servers gen-
erate multiple samples over time, with only some being indicative
of potential failures, we introduce a "server-level loss" calculation.
This method aims to minimize the influence of "confusing samples",
which may resemble normal operation despite being classified as
indicating failure. As summarized in Algorithm 1, the goal is to
shift focus towards misclassified samples by recalibrating server
weights based on the confidence of correctly predicted failures.
For a dataset comprising a set of servers ğ¶, whereğ‘€ğ‘is the count
of samples from server ğ‘, each sample ğ‘—from server ğ‘is denoted by
its data ğ‘¿ğ‘,ğ‘—and labelğ‘¦ğ‘,ğ‘—. The feature output for sample ğ‘¿ğ‘,ğ‘—post-
embedding and feature extraction is ğ’ğ‘,ğ‘—
ğ¹, leading to a confidence
score ğ’›ğ‘,ğ‘—. The server-level weighted cross-entropy loss is as below:
ğ¿(D;ğ’‰)=1
Ãğ¶
ğ‘=1ğ‘€ğ‘ğ¶âˆ‘ï¸
ğ‘=1ğ‘€ğ‘âˆ‘ï¸
ğ‘—=1ğ‘¤ğ‘,ğ‘—2âˆ‘ï¸
ğ‘˜=1ğ‘(ğ‘˜)
ğ‘,ğ‘—logğ›¿(ğ‘˜)
ğ‘,ğ‘—,(6)
whereğ‘(ğ‘˜)
ğ‘,ğ‘—represents the ğ‘˜-th component of the one-hot encoded
labelğ‘¦ğ‘,ğ‘—, andğ›¿(ğ‘˜)
ğ‘,ğ‘—is the softmax probability of ğ’›ğ‘,ğ‘—. The sample
weightsğ‘¤ğ‘,ğ‘—are adjusted based on:
ğ‘¤ğ‘,ğ‘—=(1âˆ’ğ‘maxğ‘)2, ğ‘¦ğ‘,ğ‘—=1
(1âˆ’ğ‘neg
ğ‘,ğ‘—)2, ğ‘¦ğ‘,ğ‘—=0(7)
where
ğ‘max
ğ‘=max
ğ‘—ğ›¿(2)
ğ‘,ğ‘—, ğ‘neg
ğ‘,ğ‘—=ğ›¿(1)
ğ‘,ğ‘—(8)
This adaptive weighting reduces the impact of confusing sam-
ples by dynamically adjusting the importance of positive samples
based on their prediction confidence. Additionally, an attenuation
mechanism moderates the influence of outliers or samples predicted
with unexpectedly high confidence, ensuring a balanced and fair
weighting across all samples. This nuanced approach addresses the
discrepancy between server-level evaluation and sample-level train-
ing, enhancing the modelâ€™s accuracy and reliability in identifying
server failures.
5 EXPERIMENTS
5.1 Experiment Setup
Implementation Details. Experiments utilized PyTorch 1.10.2 on
a platform with 8 Tesla V100 16GB GPUs. Parallel and cross en-
coders shared identical layer counts and hidden sizes. The AdamW
optimizer was used, starting with a learning rate of 1Ã—10âˆ’4and a
warm-up for the initial 10% of training.
Comparison Models.The proposed model is compared against
seven established models: LSTM [ 11], BiLSTM [ 8], AutoFusion [ 48],
Transformer [ 34], CNN-LSTM [ 35], ViLBERT [ 24], and ViLT [ 17].
These models span a range from traditional recurrent neural net-
works to recent advances in multimodal and transformer-based
architectures, each with relevance to cloud computing failure pre-
diction or multimodal data processing.Algorithm 1 Server-level Loss Calculation
Input:D={{ğ‘¿ğ‘,ğ‘—,ğ‘¦ğ‘,ğ‘—}ğ‘€ğ‘
ğ‘—=1}ğ¶
ğ‘=1, the training set; ğ’‰, the MISP model;
MAXğ‘’ğ‘ğ‘œğ‘â„ , the maximum training epoch; ğœ‚, the attenuation coefficient.
Output: MISP model ğ’‰.
Initialize ğ’‰;
Initializeğ‘maxğ‘=0, which is used to record the current maximum output
of positive samples of ğ‘-th server;
1:forğ¸ğ‘ğ‘œğ‘â„ =1to MAXğ‘’ğ‘ğ‘œğ‘â„ do
2: forğ‘=1 :ğ¶do
3: forğ‘—=1 :ğ‘€ğ‘do
4: Compute MISP output ğ’›ğ‘,ğ‘—=ğ’‰(ğ‘¿ğ‘,ğ‘—);
5: Computeğ‘neg
ğ‘,ğ‘—using (8);
6: end for
7: Computeğ‘maxğ‘using (8);
8: forğ‘—=1 :ğ‘€ğ‘do
9: Computeğ‘¤ğ‘,ğ‘—using (7);
10: end for
11: end for
Update ğ’‰by optimizing loss function (6);
12: forğ‘=1 :ğ¶do
13:ğ‘maxğ‘â†ğœ‚ğ‘maxğ‘;
14: end for
15:end for
Model Ablations. A series of ablation studies were conducted
to evaluate the impact of specific model components, including the
weight-sharing technique and the structural role of both encoders.
Loss Formulation. The server-level loss is benchmarked against
cross-entropy loss and A-Softmax [ 22]. Cross-entropy loss is a
widely used standard, while A-Softmax introduces an angular di-
mension to the softmax loss, causing tighter intra-class clustering.
Performance Metrics. Due to the data imbalance inherent in
server failure prediction, we focus on server-level Precision, Recall,
andğ¹1-Score , which offer a nuanced view of model performance
and are detailed in Appendix A.3. These metrics consider a server
as failed if any of its samples are predicted as such, addressing the
unique challenges of this application domain. Precision and recall
offer detailed insights into the modelâ€™s ability to accurately identify
positive samples. For practical industrial applications, the losses
caused by a server failure often far exceed the cost of operation for
maintenance. Therefore, we take recall into account while consid-
ering precision. Low recall may result in missed failure detections,
leading to unexpected failure and losses for customers. On the other
hand, low precision may result in false alarms, leading to exces-
sive maintenance costs and unexpected risks. Based on practical
experience in real-world applications, the ğ¹1-Score is used to gauge
the balance between precision and recall, with a higher ğ¹1-Score
indicating better overall performance.
It is crucial to acknowledge that our dataset, sourced from
Alibaba Cloudâ€™s real industrial environment, includes inher-
ent noise, presenting a significant challenge for achieving
high performance. In this context, even modest improvements
can have a meaningful impact on the efficiency of large-scale cloud
computing operations.
5515KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xianting Lu et al.
Table 2: Results of the proposed MISP model when using different hyperparameter settings
No. Ablation Layer Number Hidden Size Intermediate Size Semantic Length Numeric Length Precision Recall ğ¹1-score
1
Layer Number1 64 512 500 864 0.5288 0.5437 0.5361
2 2 64 512 500 864 0.5968 0.5193 0.5553
3 3 64 512 500 864 0.5520 0.5051 0.5275
4
Model Size2 64 128 500 864 0.5787 0.4961 0.5343
5 2 64 256 500 864 0.6189 0.5051 0.5563
6 2 80 128 500 864 0.5835 0.4987 0.5378
7 2 80 256 500 864 0.5512 0.5051 0.5272
8 2 80 512 500 864 0.5937 0.5051 0.5458
9 2 96 128 500 864 0.5608 0.4859 0.5207
10 2 96 256 500 864 0.5776 0.4974 0.5345
11 2 96 512 500 864 0.5650 0.5026 0.5320
12Semantic Length2 64 256 200 864 0.5959 0.4871 0.5361
13 2 64 256 1200 864 0.6160 0.5154 0.5612
14Numeric Length2 64 256 500 576 0.6338 0.4961 0.5566
15 2 64 256 500 288 0.6204 0.5000 0.5537
5.2 Selection of Hyperparameter
A series of ablation experiments assessed the impact of key hyper-
parameters on model performance, including hidden size, interme-
diate size, encoder layers, and the lengths of input semantic and
numeric time series. Results, detailed in Table 2, show that while
performance varies across settings, all configurations maintain a
minimum precision of 0.52, recall above 0.48, and an ğ¹1-score over
0.52, illustrating the robustness of our model compared to others
listed in Table 4.
5.3 Performance Comparison
Experimental results, as detailed in Table 3, demonstrate the ef-
fectiveness of different data modalities in server failure prediction.
When used independently, semantic time series significantly outper-
form numeric time series with the Transformer model, attributed
to the latterâ€™s increased susceptibility to noise interference in real-
world cloud computing environments. However, combining both
modalities enhances prediction accuracy, with the MISP model
achieving notable improvements of approximately 18-43% in all
performance metrics over the Transformer model. This integra-
tion leverages each data modalityâ€™s strengths, providing a more
complete system view and enhancing failure prediction accuracy.
Table 3: Performance comparison of different modalities for
server failure prediction
Modality Precision Recall ğ¹1-score
S (Transformer) 0.3637 0.4163 0.3854
N (Transformer) 0.0577 0.2506 0.0955
N + S (Transformer) 0.4124 0.4267 0.4195
N + S (MISP, Ours) 0.5937 0.5051 0.5458
Note: S denotes semantic time series. N denotes numeric time series.
Additionally, our model was compared with several multimodal
data-processing models, with findings showcased in Table 4. These
comparisons demonstrate notable enhancements in performance:Table 4: Performance comparison among different models
for server failure prediction
Model Precision Recall ğ¹1-score
LSTM [11] 0.4147 0.4280 0.4213
BiLSTM [8] 0.4381 0.4319 0.4350
AutoFusion [48] 0.3190 0.3432 0.3307
Transformer [34] 0.4124 0.4267 0.4195
CNN-LSTM [35] 0.3510 0.3483 0.3497
ViLT [17] 0.4403 0.4357 0.4380
ViLBERT [24] 0.4510 0.4254 0.4378
MISP (Ours) 0.5937 0.5051 0.5458
increases of at least 31.6% in precision, 15.9% in recall, and 24.6% in
ğ¹1-score. Such improvements underscore the MISP modelâ€™s adept-
ness at extracting and merging features from multimodal data,
reinforcing its utility in improving server failure prediction within
cloud computing environments. Furthermore, analysis revealed
that CNN and CNN-LSTM models underperformed, falling behind
even the single-modality results in Table 3. These models strug-
gle to efficiently harness joint features and, notably, to mitigate
interference between different data modalities.
5.4 Model Ablations
5.4.1 Shared Weights. Experimental results, as presented in Ta-
ble 5, compare models with and without the shared weight mech-
anism. Eliminating weight sharing results in decreased precision,
recall, andğ¹1-score, highlighting its importance in enhancing the
modelâ€™s ability to integrate multimodal data effectively.
5.4.2 Structure Ablation. Results from structural ablation stud-
ies, detailed in Table 6, reveal that omitting either the Parallel
Encoder or the Cross Encoder adversely affects the modelâ€™s perfor-
mance metrics: precision, recall, and ğ¹1-score decline by minimum
5516MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 5: Comparison between the models with and without
sharing weight in the cross encoder
Model Precision Recall ğ¹1-score
Not sharing weight 0.5595 0.5013 0.5288
Sharing weight 0.5937 0.5051 0.5458
Table 6: Performance comparison of the ablation study using
different encoders
Model Precision Recall ğ¹1-score
Only Parallel Encoder 0.5796 0.4679 0.5178
Only Cross Encoder 0.5346 0.4769 0.5041
Parallel + Cross (Ours) 0.5937 0.5051 0.5458
thresholds of 2.3%, 5.5%, and 5.1%, respectively. This underlines the
critical roles both encoders play in the modelâ€™s architecture.
The Parallel Encoder, with its multi-head self-attention mecha-
nism, excels at capturing long-term dependencies within sequences
of a single modality and extracting features independently from
each data type. Conversely, the Cross Encoder uses multi-head
cross-attention and weight-sharing to identify intermodal element
relationships effectively. Together, these encoders offer complemen-
tary strengths, boosting the modelâ€™s overall predictive performance
by leveraging the synergies between different data modalities.
5.5 Loss Calculation Methods
We explored various loss calculation methods for model training,
keeping all other experimental conditions consistent. The results,
presented in Table 7, indicate that our server-level loss strategy
significantly outperforms the traditional weighted cross-entropy
(CE) loss, with improvements of about 8.6% in precision, 12.5%
in recall, and 10.7% in ğ¹1-score. It also exceeds A-Softmax loss
performance by roughly 11.8% in precision, 13.5% in recall, and
12.7% inğ¹1-score.
Table 7: Comparison among different loss formulations
Loss Precision Recall ğ¹1-score
Weighted CE 0.5462 0.4486 0.4926
A-Softmax 0.5307 0.4447 0.4839
Server-level loss (Ours) 0.5937 0.5051 0.5458
In the context of server failure prediction, the relevance of sam-
ples to potential failures varies greatly; some are strong indicators of
failure, while others may not be. Low-confidence positive samples
closely resemble negative ones, creating potential confusion and
leading to inaccuracies. Traditional losses like weighted CE and A-
Softmax treat all positive samples uniformly, neglecting variations
within classes and across different servers.Contrastingly, our server-level loss method dynamically adjusts
the weighting of samples according to their distinctive features
and corresponding server associations. This nuanced approach
effectively counters the challenges presented by ambiguous samples,
thereby substantially improving our modelâ€™s performance.
6 DEPLOYMENT
We apply the proposed method in the online production environ-
ment. The platform monitors for exception events at one-minute
intervals. Upon detection of new exception events, data from the
corresponding server for the previous three days will be collected
to generate a multimodal sample of that minute. Subsequently, the
proposed model is applied to the sample to assess the probability
of failure occurrence. When the probability exceeds the threshold,
it indicates a potential failure in the near future. If the server status
matches the predicted outcomes within the next three days, it is
considered an accurate prediction.
During the online production stage, based on the prediction re-
sults, appropriate maintenance operations are conducted to handle
potential server failures. A machine equipped with an A100 GPU
can cover the real-time demands of the entire platform for evaluat-
ing the status of servers and failure prediction tasks, where 5000
samples from different servers are generated per minute on average.
Compared to the previous unimodal method which only employs
semantic data, the proposed multimodal method has shown a re-
markable improvement. The number of machines capable of risk
avoidance increases by 12% with the implementation of prediction
and maintenance operations.
7 CONCLUSIONS AND FUTURE WORK
Server failure prediction is essential for ensuring the reliability
of cloud computing systems. The use of multimodal data, includ-
ing semantic and numeric time series from system event logs and
hardware activity metrics, enhances monitoring capabilities and
provides a deeper insight into system operations. To aid research
in this area, we compiled a dataset from Alibaba Cloudâ€™s cloud
computing systems and have made it available for academic in-
quiry. Leveraging this dataset, we introduced MISP, an innovative
model designed for server failure prediction that effectively utilizes
multimodal data. MISP demonstrates superior performance in pre-
dicting server failures compared to existing models within cloud
computing environments. Looking ahead, the promising outcomes
with our dataset and MISP model open new paths for server failure
prediction research. Incorporating data like network traffic and
disk usage could improve prediction accuracy. Benchmarking and
feature engineering with the MISP model may reveal key failure
indicators. Additionally, this dataset is well-suited for developing
anomaly detection algorithms and extending the MISP modelâ€™s
application to various fields, enhancing its predictive capabilities.
ACKNOWLEDGMENTS
This work was supported by Alibaba Group through Alibaba Inno-
vative Research Program.
REFERENCES
[1]Alibaba Cloud 2023. About Alibaba Cloud. https://www.alibabacloud.com/about
5517KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xianting Lu et al.
[2]Pradeep K Atrey, M Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S
Kankanhalli. 2010. Multimodal fusion for multimedia analysis: a survey. Multi-
media systems 16 (2010), 345â€“379.
[3]Mirela Madalina Botezatu, Ioana Giurgiu, Jasmina Bogojeska, and Dorothea
Wiesmann. 2016. Predicting disk replacement towards reliable data centers. In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. 39â€“48.
[4]Thanyalak Chalermarrewong, Tiranee Achalakul, and Simon Chong Wee See.
2012. Failure Prediction of Data Centers Using Time Series and Fault Tree
Analysis. In 2012 IEEE 18th International Conference on Parallel and Distributed
Systems. 794â€“799. https://doi.org/10.1109/ICPADS.2012.129
[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota,
4171â€“4186. https://doi.org/10.18653/v1/N19-1423
[6]Vijay Ekambaram, Kushagra Manglik, Sumanta Mukherjee, Surya Shravan Kumar
Sajja, Satyam Dwivedi, and Vikas Raykar. 2020. Attention based Multi-Modal
New Product Sales Time-series Forecasting (KDD â€™20). Association for Computing
Machinery, New York, NY, USA, 3110â€“3118. https://doi.org/10.1145/3394486.
3403362
[7]Ilenia Fronza, Alberto Sillitti, Giancarlo Succi, Mikko Terho, and Jelena Vlasenko.
2013. Failure prediction based on log files using Random Indexing and Support
Vector Machines. Journal of Systems and Software 86, 1 (2013), 2â€“11. https:
//doi.org/10.1016/j.jss.2012.06.025
[8]Jiechao Gao, Haoyu Wang, and Haiying Shen. 2019. Task Failure Prediction in
Cloud Data Centers Using Deep Learning. In 2019 IEEE International Conference
on Big Data (Big Data). 1111â€“1116. https://doi.org/10.1109/BigData47090.2019.
9006011
[9]Shaohan Huang, Yi Liu, Carol Fung, Rong He, Yining Zhao, Hailong Yang, and
Zhongzhi Luan. 2020. HitAnomaly: Hierarchical Transformers for Anomaly
Detection in System Log. IEEE Transactions on Network and Service Management
17, 4 (2020), 2064â€“2076. https://doi.org/10.1109/TNSM.2020.3034647
[10] Michael HÃ¼sken and Peter Stagge. 2003. Recurrent neural networks for time series
classification. Neurocomputing 50 (2003), 223â€“235. https://doi.org/10.1016/S0925-
2312(01)00706-8
[11] Tariqul Islam and Dakshnamoorthy Manivannan. 2017. Predicting Application
Failure in Cloud: A Machine Learning Approach. In 2017 IEEE International
Conference on Cognitive Computing (ICCC). 24â€“31. https://doi.org/10.1109/IEEE.
ICCC.2017.11
[12] Tong Jia, Ying Li, Yong Yang, Gang Huang, and Zhonghai Wu. 2022. Augmenting
Log-based Anomaly Detection Models to Reduce False Anomalies with Human
Feedback (KDD â€™22). Association for Computing Machinery, New York, NY, USA,
3081â€“3089. https://doi.org/10.1145/3534678.3539106
[13] K. Sparck Jones. 2004. A statistical interpretation of term specificity and its
application in retrieval. Journal of Documentation 60, 5 (2004), 493â€“502.
[14] Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Shun Chen. 2018.
LSTM Fully Convolutional Networks for Time Series Classification. IEEE Access
6 (2018), 1662â€“1669. https://doi.org/10.1109/ACCESS.2017.2779939
[15] Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. 2019.
Multivariate LSTM-FCNs for time series classification. Neural networks 116 (2019),
237â€“245.
[16] S. Katz. 1987. Estimation of probabilities from sparse data for the language
model component of a speech recognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing 35, 3 (1987), 400â€“401. https://doi.org/10.1109/TASSP.1987.
1165125
[17] Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. ViLT: Vision-and-Language
Transformer Without Convolution or Region Supervision. In Proceedings of
the 38th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 5583â€“
5594.
[18] Dana Lahat, TÃ¼lay Adali, and Christian Jutten. 2015. Multimodal data fusion:
an overview of methods, challenges, and prospects. Proc. IEEE 103, 9 (2015),
1449â€“1477.
[19] Tian Lan, Ziyue Li, Zhishuai Li, Lei Bai, Man Li, Fugee Tsung, Wolfgang Ketter,
Rui Zhao, and Chen Zhang. 2023. MM-DAG: Multi-task DAG Learning for Multi-
modal Dataâ€“with Application for Traffic Congestion Analysis. arXiv preprint
arXiv:2306.02831 (2023).
[20] Yinglung Liang, Yanyong Zhang, Hui Xiong, and Ramendra Sahoo. 2007. Failure
Prediction in IBM BlueGene/L Event Logs. In Seventh IEEE International Confer-
ence on Data Mining (ICDM 2007). 583â€“588. https://doi.org/10.1109/ICDM.2007.46
[21] Qingwei Lin, Ken Hsieh, Yingnong Dang, Hongyu Zhang, Kaixin Sui, Yong Xu,
Jian-Guang Lou, Chenggang Li, Youjiang Wu, Randolph Yao, Murali Chintalapati,
and Dongmei Zhang. 2018. Predicting Node Failure in Cloud Service Systems. In
Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (Lake
Buena Vista, FL, USA) (ESEC/FSE 2018). Association for Computing Machinery,New York, NY, USA, 480â€“490. https://doi.org/10.1145/3236024.3236060
[22] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. 2017.
SphereFace: Deep Hypersphere Embedding for Face Recognition. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR).
[23] Yudong Liu, Hailan Yang, Pu Zhao, Minghua Ma, Chengwu Wen, Hongyu Zhang,
Chuan Luo, Qingwei Lin, Chang Yi, Jiaojian Wang, et al .2022. Multi-task Hi-
erarchical Classification for Disk Failure Prediction in Online Service Systems.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 3438â€“3446.
[24] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretrain-
ing task-agnostic visiolinguistic representations for vision-and-language tasks.
Advances in neural information processing systems 32 (2019).
[25] TomÃ¡s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Esti-
mation of Word Representations in Vector Space. In 1st International Conference
on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013,
Workshop Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
[26] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In
The Eleventh International Conference on Learning Representations.
[27] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:
Global Vectors for Word Representation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP). Association for
Computational Linguistics, Doha, Qatar, 1532â€“1543. https://doi.org/10.3115/v1/
D14-1162
[28] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep-
resentations. In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans,
Louisiana, 2227â€“2237. https://doi.org/10.18653/v1/N18-1202
[29] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al .2018.
Improving language understanding by generative pre-training. (2018).
[30] Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Xiaoyu Kou,
Tony Xing, Mao Yang, Jie Tong, and Qi Zhang. 2019. Time-Series Anomaly De-
tection Service at Microsoft. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD
â€™19). Association for Computing Machinery, New York, NY, USA, 3009â€“3017.
https://doi.org/10.1145/3292500.3330680
[31] Felix Salfner and Steffen Tschirpke. 2008. Error Log Processing for Accurate
Failure Prediction. WASL 8 (2008), 4.
[32] Xiaoyi Sun, Krishnendu Chakrabarty, Ruirui Huang, Yiquan Chen, Bing Zhao,
Hai Cao, Yinhe Han, Xiaoyao Liang, and Li Jiang. 2019. System-Level Hardware
Failure Prediction Using Deep Learning. In Proceedings of the 56th Annual Design
Automation Conference 2019 (Las Vegas, NV, USA) (DAC â€™19). Association for
Computing Machinery, New York, NY, USA, Article 20, 6 pages. https://doi.org/
10.1145/3316781.3317918
[33] Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word Representations:
A Simple and General Method for Semi-Supervised Learning. In Proceedings of the
48th Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics, Uppsala, Sweden, 384â€“394.
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),
Vol. 30. Curran Associates, Inc.
[35] Delu Wang, Jun Gan, Jinqi Mao, Fan Chen, and Lan Yu. 2023. Forecasting power
demand in China with a CNN-LSTM model including multimodal information.
Energy 263 (2023), 126012. https://doi.org/10.1016/j.energy.2022.126012
[36] Zhiwei Wang, Zhengzhang Chen, Jingchao Ni, Hui Liu, Haifeng Chen, and Jiliang
Tang. 2021. Multi-scale one-class recurrent neural networks for discrete event
sequence anomaly detection. In Proceedings of the 27th ACM SIGKDD conference
on knowledge discovery & data mining. 3726â€“3734.
[37] Chang Xu, Gang Wang, Xiaoguang Liu, Dongdong Guo, and Tie-Yan Liu. 2016.
Health Status Assessment and Failure Prediction for Hard Drives with Recurrent
Neural Networks. IEEE Trans. Comput. 65, 11 (2016), 3502â€“3508. https://doi.org/
10.1109/TC.2016.2538237
[38] Zhenghua Xue, Xiaoshe Dong, Siyuan Ma, and Weiqing Dong. 2007. A Sur-
vey on Failure Prediction of Large-Scale Server Clusters. In Eighth ACIS In-
ternational Conference on Software Engineering, Artificial Intelligence, Network-
ing, and Parallel/Distributed Computing (SNPD 2007), Vol. 2. 733â€“738. https:
//doi.org/10.1109/SNPD.2007.284
[39] Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar A
Bernal, and Jiebo Luo. 2017. Deep multimodal representation learning from
temporal data. In Proceedings of the IEEE conference on computer vision and pattern
recognition. 5447â€“5455.
[40] Zhe Yang, Piero Baraldi, and Enrico Zio. 2021. A multi-branch deep neural
network model for failure prognostics based on multimodal data. Journal of
Manufacturing Systems 59 (2021), 42â€“50. https://doi.org/10.1016/j.jmsy.2021.01.
5518MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
007
[41] Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L
Berg, and Ning Zhang. 2022. Commercemm: Large-scale commerce multimodal
representation learning with omni retrieval. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 4433â€“4442.
[42] Wennian Yu, Il Yong Kim, and Chris Mechefske. 2021. Analysis of different
RNN autoencoder variants for time series classification and machine prognostics.
Mechanical Systems and Signal Processing 149 (2021), 107322.
[43] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,
and Carsten Eickhoff. 2021. A transformer-based framework for multivariate time
series representation learning. In Proceedings of the 27th ACM SIGKDD conference
on knowledge discovery & data mining. 2114â€“2124.
[44] Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang,
and Junfeng Zhao. 2022. M3Care: Learning with Missing Modalities in Multimodal
Healthcare Data. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2418â€“2428.
[45] G Peter Zhang. 2003. Time series forecasting using a hybrid ARIMA and neural
network model. Neurocomputing 50 (2003), 159â€“175.
[46] Ke Zhang, Jianwu Xu, Martin Renqiang Min, Guofei Jiang, Konstantinos Pelechri-
nis, and Hui Zhang. 2016. Automated IT system failure prediction: A deep
learning approach. In 2016 IEEE International Conference on Big Data (Big Data).
1291â€“1300. https://doi.org/10.1109/BigData.2016.7840733
[47] Xu Zhang, Chao Du, Yifan Li, Yong Xu, Hongyu Zhang, Si Qin, Ze Li, Qingwei
Lin, Yingnong Dang, Andrew Zhou, et al .2021. Halo: Hierarchy-aware fault
localization for cloud systems. In Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining. 3948â€“3958.
[48] Chenyu Zhao, Minghua Ma, Zhenyu Zhong, Shenglin Zhang, Zhiyuan Tan, Xiao
Xiong, LuLu Yu, Jiayi Feng, Yongqian Sun, Yuzhi Zhang, Dan Pei, Qingwei Lin,
and Dongmei Zhang. 2023. Robust Multimodal Failure Detection for Microser-
vice Systems. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™23). Association for Computing Machinery,
New York, NY, USA, 5639â€“5649. https://doi.org/10.1145/3580305.3599902
[49] Minglu Zhao, Reo Furuhata, Mulya Agung, Hiroyuki Takizawa, and Tomoya
Soma. 2020. Failure Prediction in Datacenters Using Unsupervised Multimodal
Anomaly Detection. In 2020 IEEE International Conference on Big Data (Big Data).
3545â€“3549. https://doi.org/10.1109/BigData50022.2020.9378419
[50] Ziming Zheng, Zhiling Lan, Byung H. Park, and Al Geist. 2009. System log
pre-processing to improve failure prediction. In 2009 IEEE/IFIP International
Conference on Dependable Systems & Networks. 572â€“577. https://doi.org/10.1109/
DSN.2009.5270289
[51] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
2022. FEDformer: Frequency Enhanced Decomposed Transformer for Long-
term Series Forecasting. In Proceedings of the 39th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 162), Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
Sabato (Eds.). PMLR, 27268â€“27286.A SUPPLEMENTARY MATERIALS
A.1 Categorization of Exception Events
In our multimodal dataset, the semantic time series are constructed
from system runtime event logs, spotlighting 74 predominant ex-
ception events segmented into four categories: hardware, operat-
ing system, network, and others (un-categorizable events). Table 8
shows the detailed categories of every exception event in the dataset.
The hardware category has a relatively higher number of exception
events. In the future, we are exploring the possibility of broaden-
ing the spectrum of exception events in terms of both types and
quantity.
Table 8: Categorization of exception events in semantic time
series and the corresponding event indexes in the Dataset
Exception Category Event Index
HardwareDisk 43, 56
CPU 8, 23, 24, 36, 55, 73
Memory1, 2, 3, 4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
25, 26, 27, 29, 32, 34, 35, 39, 40, 46, 47, 48, 49, 53,
57, 60, 61, 64, 66, 67, 68, 69, 70, 71
Others 5, 7, 9, 10, 11, 44, 45, 58, 63
Operating
System6, 28, 31, 37, 38, 42, 52, 54, 62
Network 30, 33, 41, 51, 72
Others 50, 59, 65, 74
A.2 Distribution of Exception Events
The variation in time intervals among elements within the semantic
time series is evident across different samples, stemming from the
unpredictable nature of system events. This variability also results
in differences in the lengths of semantic time series when spanning
three days. A statistical analysis, summarized in Table 9, reveals
a disproportionate distribution of exception events. Additionally,
the distribution of exception event sequence lengths in the training
and test sets is depicted in Figure 8. The majority of the semantic
time series have a length shorter than 500, with approximately
80% of the time series falling within this range. This distribution is
consistent across both the training and test sets.
Figure 8: Distribution of the exception event sequence
lengths in the training and test sets.
5519KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xianting Lu et al.
Table 9: Statistics of exception events in training and test sets
Event
IndexTrain Set Test Set Event
IndexTrain Set Test Set
Neg Pos Neg Pos Neg Pos Neg Pos
1 852499 54947 544704 33291 38 216888 13466 98929 8832
2 600993 51323 410816 31862 39 5723 4209 3876 2821
3 281293 18724 195832 11319 40 7782 1541 1591 811
4 281555 17868 196074 11113 41 7044 3548 5126 1973
5 4738 415 623 527 42 76203 2878 42645 4307
6 835108 52439 533997 33116 43 2694 2120 1995 96
7 23174 7215 9684 3461 44 279 0 144 4
8 18730 6078 8979 2780 45 9682 214 5431 300
9 23037 6948 9604 3069 46 19 873 59 140
10 6554 266 1632 75 47 235588 587 126425 20
11 29490 7864 12205 4039 48 18981 11074 13502 2
12 408054 36185 360141 26703 49 26116 8748 9517 5640
13 266 54 674 0 50 474968 18674 174801 2382
14 574985 23697 468981 16943 51 200035 3191 99308 493
15 1732 1199 1911 772 52 11 29 12 94
16 204 170 357 13 53 525586 35694 376088 21666
17 8063 121 4942 42 54 231 36 60 16
18 3643 466 4112 87 55 43293 1627 20198 417
19 100 1 167 0 56 2696 1355 348 6
20 314 8 652 25 57 38990 8273 18119 4114
21 582 4 491 3 58 10432 1968 2354 35
22 21843 4647 6450 3136 59 27901 492 7913 0
23 417 914 174 60 60 1584 152 1701 212
24 160701 8220 60536 40 61 10199 1434 2016 0
25 590714 35581 420126 22456 62 35998 451 27453 16
26 53720 10829 18349 4414 63 368 304 620 118
27 69491 14838 30417 9658 64 7276 1591 2729 858
28 263 8 104 0 65 1655 94 264 0
29 334067 24892 123890 12643 66 251687 17662 113492 10266
30 173 981 205 1080 67 62027 9756 19696 5897
31 20662 3137 10725 236 68 70468 6240 12101 827
32 593070 36594 423059 22550 69 132150 10218 68010 5036
33 1587 1203 2165 252 70 27681 2868 17425 1443
34 258700 13801 188928 8268 71 16964 6334 5443 3831
35 483984 27524 346440 17671 72 1161 1109 1796 1094
36 25557 5415 20983 532 73 1860670 35654 1022523 9612
37 50179 245 17 539 7 74 1069797 11640 500577 4444
Note: Neg denotes negative samples. Pos denotes positive samples.A.3 Metrics for Model Evaluation
Given the extreme data imbalance among crashed and fault-free
machines, accuracy is not a suitable metric for evaluation. As each
server has at least one sample, a server is deemed to have a failure
if any of its samples are predicted as such. Hence, we adopt server-
level metrics more amenable to our dataset. We count the numbers
of true positive servers (TP), false positive servers (FP), and false
negative servers (FN), respectively. Then we calculate the following
metrics according to all the test results:
Precision =TP
TP+FP, (9)
Recall =TP
TP+FN, (10)
ğ¹1-Score =2Â·PrecisionÂ·Recall
Precision+Recall. (11)
5520