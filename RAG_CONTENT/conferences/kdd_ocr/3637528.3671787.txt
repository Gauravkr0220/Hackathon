Improving the Consistency in Cross-Lingual Cross-Modal
Retrieval with 1-to-K Contrastive Learning
Zhijie Nie
CCSE, Beihang University
Beijing, China
niezj@act.buaa.edu.cnRichong Zhangâˆ—
CCSE, Beihang University
Beijing, China
zhangrc@act.buaa.edu.cnZhangchi Feng
CCSE, Beihang University
Beijing, China
zcmuller@buaa.edu.cn
Hailang Huang
CCSE, Beihang University
Beijing, China
huanghl@act.buaa.edu.cnXudong Liu
CCSE, Beihang University
Beijing, China
liuxd@act.buaa.edu.cn
Abstract
Cross-lingual Cross-modal Retrieval (CCR) is an essential task in
web search, which aims to break the barriers between modality and
language simultaneously and achieves image-text retrieval in the
multi-lingual scenario with a single model. In recent years, excel-
lent progress has been made based on cross-lingual cross-modal
pre-training; particularly, the methods based on contrastive learn-
ing on large-scale data have significantly improved retrieval tasks.
However, these methods directly follow the existing pre-training
methods in the cross-lingual or cross-modal domain, leading to two
problems of inconsistency in CCR: The methods with cross-lingual
style suffer from the intra-modal error propagation, resulting in in-
consistent recall performance across languages in the whole dataset.
The methods with cross-modal style suffer from the inter-modal
optimization direction bias, resulting in inconsistent rank across
languages within each instance, which cannot be reflected by Re-
call@K. To solve these problems, we propose a simple but effective
1-to-K contrastive learning method, which treats each language
equally and eliminates error propagation and optimization bias. In
addition, we propose a new evaluation metric, Mean Rank Vari-
ance (MRV), to reflect the rank inconsistency across languages
within each instance. Extensive experiments on four CCR datasets
show that our method improves both recall rates and MRV with
smaller-scale pre-trained data, achieving the new state-of-art1.
CCS Concepts
â€¢Information systems â†’Image search; multi-lingual and
cross-lingual retrieval; Retrieval effectiveness .
âˆ—Corresbonding author: zhangrc@act.buaa.edu.cn.
1Our codes can be accessed at https://github.com/BUAADreamer/CCRK
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671787Keywords
cross-lingual cross-modal retrieval, cross-lingual cross-modal pre-
training, consistency, contrastive learning
ACM Reference Format:
Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, and Xudong Liu.
2024. Improving the Consistency in Cross-Lingual Cross-Modal Retrieval
with 1-to-K Contrastive Learning. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671787
1 Introduction
Recently, significant progress has been made in the cross-modality
[17,29,33], and the cross-lingual [ 7,9,10] domains, leading to
increased interest in the more general cross-lingual cross-modal
scenarios. In the cross-lingual cross-modal domain, Cross-lingual
Cross-modal Pre-training (CCP) [ 24,30,41,42] is first explored,
followed by Cross-lingual Cross-modal Retrieval (CCR) [ 3,12,14,27,
37] as the first downstream task independently studied. CCR aims to
achieve image-text retrieval in multi-lingual scenarios with a single
model, preventing the high latency associated with text translation
from other languages to English in real-time web searches.
In general, modern dense retrieval matches the results for a
query by a particular distance metric (e.g., Euclidean distance or
cosine similarity), which implies that the dense retrieval meth-
ods should push queries and those semantically similar candidate
items closer than other random pairs in the high-dimensional space.
Thus, the core of the retrieval task lies in aligning the semantic
spaces of queries and candidate sets, regardless of whether they
are in different languages or different modalities. Recent studies
show that contrastive learning based on pairwise data is effective
in cross-lingual and cross-modal retrieval tasks. For example, CLIP
[29], which is only pre-trained by aligning different modalities us-
ing contrastive learning, has achieved remarkable performances
in zero-shot cross-modal retrieval; on the other hand, aligning the
representations from different modalities (or different languages)
before fusing them can reduce the difficulty of fusion and signifi-
cantly improve the performance of downstream cross-modal tasks
including retrieval, question answering and reasoning [ 17]. As a re-
sult, the existing works in CCP directly pieced the alignment ideas
in cross-modal or cross-lingual domains, feeding pairwise data into
the encoder at a time, such as an image-text pair and a bi-lingual
 
2272
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, & Xudong Liu
English  French  German  Chinese  Recall@KInconsistency in Recall
(Global Problem)
Inconsistency in Rank
(Local Problem)Rank
English  French  German  Chinese  
English as Bridge Between 
Vision and Other Languages
Random Sampling a Language
Align with Vision at One TimeImage TextCheerios in a container EN
Cheerios en barquette FRCheerios in einer Dose DE
æ”¾åœ¨å®¹å™¨é‡Œçš„è½¦å˜å­ ZH
Alignment Direction
Figure 1: Two inconsistency problems exist in the current
cross-lingual cross-modal pre-training methods, leading to
inconsistent recall and ranking in cross-lingual cross-modal
retrieval separately.
text pair. Specifically, the existing methods use the following two
ideas to align different modalities: (1) considering English as the
anchor for bridging vision with other languages, which means that
the images are aligned to the English texts only, while the texts in
other languages are aligned to the English texts only [ 14,40] or (2)
considering the images being aligned with the texts in a random
language at a time during pre-training [41].
However, the desirable alignment process is more complex in
cross-lingual cross-modal scenarios. Intuitively, the semantics of
the texts in multiple languages need to be aligned jointly with those
from vision, which cannot be achieved with pairwise data. With the
theoretical derivations and empirical studies (Section 3), we find
that applying either of the two above ideas to CCP will result in two
problems of inconsistency (Figure 1). Specifically, regarding English
as the bridge in inter-modal may cause error propagation, resulting
in an inconsistent performance on Recall@K of different languages
in CCR; aligning the image with only the text in a random language
at a time may lead to the optimization direction bias, resulting
the inconsistent ranks of different languages within an instance.
Highlighting that the latter problem is more insidious since it cannot
be directly reflected by Recall@K, which is almost the only reported
evaluation metric of CCR [2, 24, 41, 42].
To solve the above problems, in this paper, we propose a sim-
ple but effective contrastive paradigm for CCP, 1-to-K contrastive
learning. Specifically, when pre-training the images and texts in
a mini-batch ratio of not 1 to 1 but 1 to K (K â‰¥2), each image is
aligned simultaneously with K texts in different languages. Under
this paradigm, all languages are aligned with vision at once, and no
language is used as the bridge between vision and other languages,eliminating intra-modal error propagation and inter-modal opti-
mization direction bias in principle. In addition, two commonly used
pre-training tasks for capturing fine-grained correlation between
modalities, Multi-lingual Image-Text Matching (MITM) [ 41,42] and
Cross-modal Masked Language Modeling (CMLM) [ 24,41], can be
easier superimposed on the novel contrastive paradigm with the
help of hard negative sampling. Based on the three pre-training
tasks, we propose a pre-trained model, CCRğ‘˜. For the evaluation of
CCR, as a complement to Recall@K, we propose a new evaluation
metric, Mean Rank Variance (MRV), to reflect the rank inconsis-
tency of the different languages in an instance. Extensive experi-
ments on four public CCR datasets demonstrate that our method
has effectively solved the above two problems and achieved new
state-of-the-art.
The contributions of this paper can be summarized as follows:
â€¢We analyze two problems of inconsistency existing in the
current CCP methods and point out their impact on the
performance of CCR for the first time.
â€¢We propose a simple but effective 1-to-K contrastive para-
digm as an alternative to the traditional 1-to-1 contrastive
paradigm in CCR to solve these problems.
â€¢We propose Mean Rank Variance (MRV) to better reflect re-
trieval performance across languages and modalities, which
is used to replenish Recall@K and evaluate the rank consis-
tency across languages in each dataset sample.
â€¢We propose CCRğ‘˜, a CCP model with the novel 1-to-K con-
trastive paradigm. We pre-train four variants of CCR with
the different language numbers and data scales. The largest
variant CCR10-E, which is still pre-trained with fewer lan-
guage numbers and data scale than all baselines, achieves
new SOTA on four CCR datasets.
2 Background
This section overviews recent advances in cross-lingual cross-modal
pre-training and cross-lingual cross-modal retrieval. Due to space
limitations, we will only focus on works related to image-text re-
trieval in the cross-lingual scenarios.
2.1 Cross-Lingual Cross-Modal Pre-Training
Cross-lingual Cross-modal Pre-training (CCP) [ 24,30,41,42] is gen-
eralized from cross-modal pre-training [ 1,17,33] and cross-lingual
pre-training [ 7,9,10], which aims to develop a representation learn-
ing model that captures the relationship in different modalities and
different languages simultaneously. Current methods can be broadly
divided into three categories based on their model architectures.
Cross-Lingual Style. The first class of methods follows the
model architecture in the cross-lingual domain, where a pre-trained
cross-modal model (e.g. CLIP [ 29]) is required. Then, the pre-trained
model is tuned to a cross-lingual version by aligning the representa-
tions of English texts and non-English texts while freezing both the
visual and English textual backbone. The representatives of these
methods are multi-lingual CLIPs [ 3,35]. The idea behind these
methods is using English as a bridge between vision and other
languages.
 
2273Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Cross-Modal Style. The second class of methods follows the
model architecture in the cross-modal domain, where multi-lingual
image-text pairs are required. Due to the difficulty of collecting
multi-lingual image-text pairs in practice, translation models are
usually used to translate the English text in the existing image-text
pairs to other languages [ 14,28,41,42]. Then, at most, one non-
English text is adapted to form an image-text pair with the image at
a time, keeping consistent with the input form of the cross-modal
model [ 17,29]. The representatives of these methods are UC2[42],
and TD-MML [ 28]. The idea behind these methods is aligning the
image with the text in a language at a time to improve the
performance across languages.
Cross-Modal Cross-Lingual Style. The third class of methods
references the architectures in both cross-lingual and cross-modal
domains. The same multi-lingual encoders are responsible for en-
coding the texts in both image-text pairs and parallel corpora for
a unified framework. The representatives of these methods are
xUNITER [ 20], M3P [24], and CCLM [ 41]. The idea behind these
methods is using a unified framework to combine the ideas
from the first and second class of methods.
2.2 Cross-Lingual Cross-Modal Retrieval
Cross-lingual Cross-modal Retrieval (CCR) [ 3,12,14,27,37] is
one of the downstream tasks that have been focused on in cross-
lingual cross-modal scenarios. MURAL [ 14] demonstrates that high
performance in CCR can be achieved through pre-training with
contrastive learning over large-scale datasets. Fei et al . [12] pre-
train only a fusion encoder for CCR using pre-extracted image
region features. More recently, IGLUE [ 2], a cross-lingual cross-
modal benchmark, was proposed with two new retrieval datasets,
xFlickr&CO and WIT. In addition, IGLUE explores several cross-
modal pre-training models (such as ViLBERT [ 23] and xUNITER
[20]), and evaluates them on two new datasets by directly translat-
ing the texts in other languages to English, demonstrating that these
models serve as strong baselines. Carlsson et al . [3] apply cross-
lingual teacher learning to transfer CLIP to other languages. Wang
et al. [37] proposed a noise robustness CCR method to improve the
performance when training on the noisy translated data.
To the best of our knowledge, our work in this paper is the first
exploration of the consistency in cross-lingual cross-modal retrieval.
In additional, our newly proposed 1-to-K contrastive learning pre-
training task and the evaluation metric MRV have not previously
appeared in CCR and related fields.
3 Problem of Inconsistency in CCR
In this section, we first explore two alignment problems in the
existing CCP methods under the perspective of contrastive learning,
then point out their impacts on the performance of CCR.
3.1 Preliminary
In the loss functions for alignment, there may be only the anchor
with its positive samples (e.g., Mean Squared Error (MSE)) and
the optional negative samples (e.g., InfoNCE Loss [ 25], which is
commonly used in contrastive learning). When these loss func-
tions are used, the anchor is optimized by the alignment direction,
which points from the anchor to the positive sample. Intuitively,the alignment direction brings the anchor and positives together in
the semantic space.
In advance, we give the required notation for the follow-up con-
tent in this section. For simplicity, we only consider the case where
one image needs to be aligned with two texts from two different
languages, and the subsequent conclusions can be easily gener-
alized to more languages. Let Ë†ğ‘–,Ë†ğ‘¡ğ‘šandË†ğ‘¡ğ‘›denote the normalized
representations of the image, the text in language ğ‘š, and the text
in language ğ‘›, respectively. We define ğ›¼=âˆ (Ë†ğ‘–,Ë†ğ‘¡ğ‘š),ğ›½=âˆ (Ë†ğ‘–,Ë†ğ‘¡ğ‘›)
andğ›¾=âˆ (Ë†ğ‘¡ğ‘š,Ë†ğ‘¡ğ‘›), where âˆ (.,.)represents the angle of two same
dimensional representations.
3.2 Inconsistency in Recall@K
Theoretical Analysis. The methods following the cross-lingual
architecture implicitly rely on English as a bridge in inter-modal
alignment between the other language and vision. In this setting,
we consider the situation in which the other language text represen-
tation is the anchor, where it is aligned to its positive sample, the
English text representation. However, in theory, it should be aligned
to the image representation. Without loss of generality, if we regard
languageğ‘šas English and language ğ‘›as another language, then
the practical alignment direction is Ë†ğ‘¡ğ‘šâˆ’Ë†ğ‘¡ğ‘›, while correct alignment
direction is Ë†ğ‘–âˆ’Ë†ğ‘¡ğ‘›(Figure 2(a)). Then we have the following results:
Lemma 3.1. Suppose that ğœƒis the angle between the practical and
correct alignment direction of Ë†ğ‘¡ğ‘›. If and only if English texts can be
aligned well with images, i.e. ğ›¼tends to 0, then ğœƒwill converge to 0.
Empirical Observation. We find the inter-modal alignment
process so tough that English texts cannot be aligned well with
images. Specifically, the loss value can drop by 5 to 6 orders of
magnitude in the text-modal (uni-modal) scenario [ 13], while it is
only 2 orders of magnitude in cross-modal contrastive learning [ 17]
(Figure 2(b)). It means that the alignment between English texts and
images is not ideal, and if English texts are used to connect images
and texts in other languages, there will be a risk of error propagation
on intra-modal alignment, resulting in a worse alignment between
non-English texts and images.
Impact of inconsistency. As this problem persists during pre-
training, the impact of this problem is global and can be revealed by
the uneven performance under the different language settings. As it
is shown by the results of M3P and UC2in Table 1, the performance
gap among different language scenarios is clear even though the
instance number per language has been kept nearly consistent
during pre-training [42].
3.3 Inconsistency in Rank
Theoretical Analysis. The methods that follow the cross-modal
architecture consider each language separately aligned to the vision,
thus avoiding error propagation in intra-modal. However, they
suffer from another local problem of inconsistency.
In this setting, we consider the situation that the image is the
anchor, where its optimal alignment coordinates should satisfy: (1)
min(âˆ (Ë†ğ‘–,Ë†ğ‘¡ğ‘š)+âˆ (Ë†ğ‘–,Ë†ğ‘¡ğ‘›))and (2) âˆ (Ë†ğ‘–,Ë†ğ‘¡ğ‘š)=âˆ (Ë†ğ‘–,Ë†ğ‘¡ğ‘›). Combining the
two conditions above, Ë†ğ‘–should be drawn to the midpoint of the
minor arc corresponding to Ë†ğ‘¡ğ‘šandË†ğ‘¡ğ‘›, i.e., the correct alignment
direction is(Ë†ğ‘¡ğ‘š+Ë†ğ‘¡ğ‘›)
âˆ¥Ë†ğ‘¡ğ‘š+Ë†ğ‘¡ğ‘›âˆ¥âˆ’Ë†ğ‘–.
 
2274KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, & Xudong Liu
(a)
0 5 10 15 20 25 30
Epoch106
105
104
103
102
101
100101102103104Contrastive LossLower bounds of cross-modal
Lower bounds of uni-modal
Cross-lingual cross-modal (b)
Figure 2: Theoretical analysis and empirical observation for
inconsistency in Recall@K. (a) An illustration of Lemma
3.1, where the green arrow represents the correct alignment
direction, while the red arrow represents the practical align-
ment direction. (b) A comparison of infoNCE loss value in
different scenarios. We pre-trained and recorded loss changes
using SimCSE [ 13] in the uni-modal setting, ALBEF [ 17] in
the cross-model setting and CCLM [ 41] in CCP, respectively,
while keeping other settings as identical as possible.
However, the image is aligned with only one of the text repre-
sentations at a time under the cross-modal setting. Without loss
of generality, if we regard Ë†ğ‘¡ğ‘šas the alignment target, the practical
alignment direction of Ë†ğ‘–can be considered as Ë†ğ‘¡ğ‘›âˆ’Ë†ğ‘–(Figure 3(a)).
Then we have the following results:
Lemma 3.2. Suppose that ğœ”is the angle between the actual align-
ment direction and the correct optimization direction of Ë†ğ‘–. If and only if
the English text can be aligned well with the text in the other language,
i.e.ğ›¾tends to 0, then ğœ”will converge to 0.
Empirical Observation. We find that the representations ob-
tained by the popular multi-lingual text encoders are not aligned
according to semantics after degenerating the representations by
t-SNE [ 36]. Instead, they remain irregularly distributed (Figure 3(b)).
As a result, the alignment direction of the image may not favor all
languages when the model only sees the texts in one language at
one time, which might result in inconsistent performance among
the semantically similar texts in different languages.
Impact of inconsistency. As this problem appears dynamically
in different instances for different languages during pre-training,
the impact of this problem is local. The very different retrieval
results will be obtained (1) when the texts in different languages
are retrieved using the same image or (2) when the same image is
retrieved using the texts in different languages but with the same
semantics. Unfortunately, Recall@K can only reflect the overall
performance of the model on each language in the whole dataset
but can not reflect the inconsistent performance across languages
of an instance.
4 Method
The section is organized as follows: some necessary notations are
first introduced in Section 4.1; a novel 1-to-K contrastive method is
then proposed to solve the inconsistency problems in Section 4.2; a
(a)
 (b)
Figure 3: Theoretical analysis and empirical observation for
inconsistency in Rank. (a) An illustration of Lemma 3.2,
where the green arrow represents the correct alignment direc-
tion, while the red arrow represents the practical alignment
direction. (b) A Visualization of T-SNE with 10 instances
randomly sampled in xFlickr&CO. The representations are
obtained by Swin Transformer [ 21] and the first half (first
six layers) of XLM-R [ 8] following the setting in CCLM [ 41].
pre-training model, CCRğ‘˜, is further presented to combine 1-to-K
contrastive learning with other common pre-training tasks in a
unified framework in Section 4.3; Finally, a new evaluation metric
called Mean Rank Variance (MRV) is proposed in Section 4.4, which
evaluates the rank consistency across languages in a instance.
4.1 Notation
Letğ·=(ğ¼,ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ¾)denote a multi-lingual image-text dataset,
consisting of the instance (ğ‘–ğ‘—,ğ‘¡ğ‘—1,ğ‘¡ğ‘—2,...,ğ‘¡ğ‘—ğ¾)âˆ¼ğ·, whereğ‘—indexes
the instance, ğ‘–ğ‘—is the image in this instance, ğ‘¡ğ‘—ğ‘˜is the text in the
ğ‘˜-th language in this instance, and ğ¾refers to the total number
of languages in the dataset. If it is clear from the context, we will
remove the subscript ğ‘—orğ‘—ğ‘˜for brevity.
4.2 1-to-K Contrastive Learning
To solve both two problems in the previous section, the key is that
the texts in all languages should be aligned with the semantically
similar images all at once. Obviously, it is not possible to do this by
aligning pairs of data. Even if uniformly sampling one from the texts
in all languages and combining it with the corresponding image to
form an image-text pair, the second problem remains. Therefore,
the effective way is to form the texts in all languages and the image
directly into a tuple as the input. We therefore propose a 1-to-K
contrastive learning approach to solve this problem. For simplicity,
letË†ğ‘¡andË†ğ‘–represent the normalized text and image representations,
respectively. Then the optimization objective of 1-to-K contrastive
learning can be formulated as follows:
Li2t
kcl=âˆ’1
ğ¾logexp(Ë†ğ‘–ğ‘‡
ğ‘—Ë†ğ‘¡ğ‘—ğ‘˜/ğœ)
Ãğ¾
ğ‘˜exp(Ë†ğ‘–ğ‘‡
ğ‘—Ë†ğ‘¡ğ‘—ğ‘˜/ğœ)+Ãğ‘
ğ‘›,ğ‘›â‰ ğ‘—Ãğ¾
ğ‘˜exp(Ë†ğ‘–ğ‘‡
ğ‘—Ë†ğ‘¡ğ‘›ğ‘˜/ğœ)(1)
 
2275Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Pre-Training Tasks Pre-Training
id ENDEFR
1
2
3
4
5
6Multi-Lingual
Text EncoderImage
Encoder
1-to-K
Contrastive
Learning  Fusion Encoder
Calculate
Mean ValueConvert
Rank
into 0/112Calculate
Mean ValueCalculate Rank Variation 1
2
Cheerios in a container
æ”¾åœ¨å®¹å™¨é‡Œçš„è½¦å˜å­Cheerios dans un rÃ©cipient
Cheerios in einer DoseImage-Text
MatchingMasked 
Language Modeling
Q & K For
Cross Attention
Calculate Recall@K for EnglishHard Sample
Mining12 3
Calculate Mean Rank VariationEvaluation
Mask
3
[MASK] in a 
containerCheerios1
DEENFR
ZHEN
ZH
DE
2 Cheerios in a 
container
ç†Ÿé€çš„è‹¹æœ
(Ripe apples)
True
False
Cheerios in a 
container
 False
Retrieval Results
Figure 4: The overview of our pre-training tasks, model architecture, and evaluation metrics.
Lt2i
kcl=âˆ’logexp(Ë†ğ‘¡ğ‘‡
ğ‘—ğ‘˜Ë†ğ‘–ğ‘—/ğœ)
exp(Ë†ğ‘¡ğ‘‡
ğ‘—ğ‘˜Ë†ğ‘–ğ‘—/ğœ)+Ãğ‘
ğ‘›,ğ‘›â‰ ğ‘—exp(Ë†ğ‘¡ğ‘‡
ğ‘—ğ‘˜Ë†ğ‘–ğ‘›/ğœ)(2)
whereğ¾is the number of languages and ğ‘is the number of negative
instances. It is worth noting that there exists literature on multiple
positive contrastive learning in other fields [ 31,34], where all posi-
tive items are accumulated in the numerator and the probability of
the overall positive terms probability is calculated to be infinitely
convergent to 1. Instead, we further set the label of each positive
item to 1/K to ensure equal contribution from each language.
Note that increasing the number of multi-lingual texts used as
input to the encoders only results in a small increase in GPU mem-
ory and training time since the text encoders are usually more
lightweight than image encoders in CCP [ 41] and most of the com-
putations involved are matrix operations that support parallelism.
The changes in memory usage and training time before and after
applying 1-to- ğ¾contrastive learning are detailed in Appendix D.
4.3 Pretraining Model: CCRğ‘˜
Based on the proposed 1-to-K contrastive learning, we further pro-
pose a CCP model named CCRğ‘˜. Specifically, we combine 1-to-K
contrastive learning with two other common CCP tasks and balance
positive and negative samples by hard sample mining. As shown
in the middle of Figure 4, we adopt the common framework in
cross-lingual cross-modal pretraining [ 24,41,42], which consists of
a multi-lingual text encoder ğ‘“(Â·), a visual encoder ğ‘”(Â·)and a fusion
encoderğœ™(Â·,Â·)with image-to-text cross-attention.
4.3.1 Hard Sample Mining. Incorporating cross-attention between
the image representation and the text representations in all lan-
guages can greatly increase the pre-training time. Therefore, we
use the hard sample mining strategy proposed by Li et al . [17] for
both positive and negative samples. This method allows the model
can only focus on how to reconstruct the hardest positive samples
in the CMLM task and distinguish the hardest negative samples in
the MITM task. In subsequent sections, we use ğ‘¡pos
ğ‘—to represent
the hard positive sample for texts and ğ‘¡neg
ğ‘—andğ‘–neg
ğ‘—to represent
the hard negative sample for texts and images, respectively. Please
refer to Appendix C.3 for sampling details.4.3.2 Multi-lingual Image-Text Matching (MITM). The MITM task
is a binary classification task that aims to identify whether the se-
mantics of a given image-text pair match. This task is often regarded
as an image-text bi-directional prediction problem. Specifically, in
the image-to-text direction, the model is trained to select the right
one from the hard positive and hard negative text samples. Let ğ‘¢cls
be the representation output by the fusion encoder, then the loss
function of MITM can be expressed as
Li2t
mitm=âˆ’logexp(ğœ“(ğ‘¢p
cls))
exp(ğœ“(ğ‘¢p
cls))+exp(ğœ“(ğ‘¢nt
cls))(3)
whereğœ“âˆˆRğ‘‘Ã—2is the binary-classification head, ğ‘‘is the repre-
sentation dimension, ğ‘¢p
clsis obtained from ğœ™(Ë†ğ‘¡pos
ğ‘—,Ë†ğ‘–ğ‘—)andğ‘¢nt
clsis
obtained from ğœ™(Ë†ğ‘¡neg
ğ‘—,Ë†ğ‘–ğ‘—). Similarly, for the text-to-image direction,
the matching objective can be expressed as
Lt2i
mitm=âˆ’logexp(ğœ“(ğ‘¢p
cls))
exp(ğœ“(ğ‘¢p
cls))+exp(ğœ™(ğ‘¢ni
cls)(4)
whereğœ“âˆˆRğ‘‘Ã—2is the same binary-classification that is used in
Eqn. (3) and ğ‘¢ni
clsis obtained from ğœ™(Ë†ğ‘¡pos
ğ‘—,Ë†ğ‘–neg
ğ‘—).
4.3.3 Cross-Modal Masked Language Modeling (CMLM). The cross-
modal masked language modeling task aims to reconstruct the
masked tokens using both textual contextual information and image
information. Let ğ‘¡mask
ğ‘—be the variant of ğ‘¡pos
ğ‘—whose partial tokens
are masked, and Ë†ğ‘¢mask
ğ‘—is the fusion encoder output corresponding
toğ‘¡mask
ğ‘—, then the loss function for this task can be expressed as
Lcmlm=âˆ’logexp(ğœŒ(Ë†ğ‘¢mask
ğ‘—,ğ‘¤+
ğ‘—))
Ã
ğ‘¤ğ‘—âˆˆWexp(ğœŒ(Ë†ğ‘¢mask
ğ‘—,ğ‘¤ğ‘—))(5)
whereğœŒ:(Rğ‘‘Ã—W)â†’ R1is a score function to evaluate the
matching degree of a given contextual representation with a given
token,ğ‘¤+
ğ‘—is the original token of the masked location and Wis
the vocabulary list. We use the special token [MASK] to replace
15% of the tokens in each text, following BERT [10].
 
2276KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, & Xudong Liu
4.3.4 Optimization Objective. Note that contrastive loss, image-
text matching, and masked language modeling have been verified
in numerous prior works [ 17,41] to converge together when co-
optimized, so we directly sum them here without the additional
hyper-parameters for weighting different losses. Thus, the final
optimization objective, which can be expressed as
L=Li2t
kcl+Lt2i
kcl+Li2t
mitm+Lt2i
mitm+L cmlm (6)
4.4 Evaluation Metric: Mean Rank Variation
While Recall@K is the common metric used in CCR, it only can
reflect the overperformance on a single language. In this section,
we introduce a new evaluation metric, Mean Rank Variation (MRV),
to measure the rank consistency in different languages within an
instance. Figure 4 illustrates the difference between MRV and Re-
call@K in their calculation methods. MRV for K languages can be
computed in both Image-to-Text Retrieval (TR) and Text-to-Image
Retrieval (IR) tasks. For example, in the TR task, given an image
ğ‘–ğ‘—and a text set in a particular language {ğ‘¡ğ‘—ğ‘˜}ğ‘
ğ‘—=1, the similarities
between the image and the text set are computed first. Then the
text set is sorted by these similarities in ascending order and the
rank ofğ‘¡ğ‘—ğ‘˜is denoted as ğ‘…ğ‘ğ‘›ğ‘˜ğ‘—ğ‘˜. For eachğ‘–ğ‘—, we can loop through
ğ‘˜from 1 toğ¾to obtain{ğ‘…ğ‘ğ‘›ğ‘˜ğ‘—ğ‘˜}ğ¾
ğ‘˜=1, and average them to obtain
ğ‘…ğ‘ğ‘›ğ‘˜ğ‘—. Similarly, in the IR task, we denote the rank of retrieving the
imageğ‘–ğ‘—using the text ğ‘¡ğ‘—ğ‘˜asğ‘…ğ‘ğ‘›ğ‘˜ğ‘—ğ‘˜and the average rank obtained
by retrieving ğ‘–ğ‘—using allğ¾languages as ğ‘…ğ‘ğ‘›ğ‘˜ğ‘—. Thus, MRV for ğ¾
languages, which is denoted as MRV K, can be expressed as
MRV K=1
ğ‘ğ¾ğ‘âˆ‘ï¸
ğ‘—ğ¾âˆ‘ï¸
ğ‘˜|ğ‘…ğ‘ğ‘›ğ‘˜ğ‘—ğ‘˜âˆ’ğ‘…ğ‘ğ‘›ğ‘˜ğ‘—|2(7)
Note that there is no trade-off between Recall@K and MRV K, which
means that when Recall@1=1 holds for all K languages, MRV K=0
also holds. MRV Kis more likely to reflect the alignment consistency
of local semantic space. Such consistency is significant in certain
scenarios, such as cross-border e-commerce to ensure consistency
in the results retrieved when the queries are in different languages
but with the same semantic.
5 Experiment
5.1 Experiment Setup
5.1.1 Pre-training Datasets. For pre-training, we mainly use Con-
ceptual Captions 3M (CC3M) [ 4], which currently has only 1.8
million image-text pairs from the web due to the inaccessibility
of image hyperlinks. To verify the scalability of our approach, we
further introduce 3 additional cross-modal web datasets, includ-
ing SBU Caption [26], Visual Genome [16] and COCO [5]. For the
translated version of the texts, we use the 6-language (English,
German, French, Czech, Japanese, and Chinese) translated texts
in CC3M provided by UC2[42] as well as the same 6-language
translated texts in the other three datasets, provided by CCLM [ 41]
for fair comparisons. To further verify the generalizability of our
method to more languages, we use the M2M-100-large model [ 11]
to translate the English text in the datasets into an additional 4
languages (Spanish, Indonesian, Russian, and Turkish), following
Qiu et al . [28] . Therefore, the total number of text languages usedfor evaluation is 10, which covers all languages in xFlickr&CO. We
plan to open-source these translated texts for research.
5.1.2 Baseline. CCRğ‘˜proposed in this paper is mainly an improve-
ment of the training optimization objective in the pre-training
phase, so we mainly compare it with other CCP models, including
xUNITER [ 20], UC2[42], M3P [24], TD-MML [ 28] and CCLM [ 41].
These methods have been briefly described in Section 2.1, while for
more details on them, please refer to Appendix B.1.
5.1.3 The Variant of CCRğ‘˜.We report the performance of four
model variants pre-trained with different data, which are as follows:
â€¢CCR6pre-trained using CC3M with 6-language texts.
â€¢CCR10pre-trained using CC3M with 10-language texts.
â€¢CCR6-Epre-trained using CC3M, COCO, VG and SBU with
6-language texts.
â€¢CCR10-Epre-trained using CC3M, COCO, VG and SBU with
10-language texts.
5.1.4 Evaluation Datasets and Protocols. We evaluate our meth-
ods on four popular CCR datasets, including xFlickr&CO [ 2], WIT
[2], Multi30K [ 39] and COCO [ 5,19,38]. Although the images in
xFlickr&CO are derived from the original Flickr30K and COCO,
the multi-lingual texts in xFlickr&CO are manually re-annotated.
Therefore, the performance on xFlickr&CO may not be strongly
correlated with that on Multi30K and COCO. For both xFlickr&CO
and WIT, we evaluate our models using two protocols: fine-tuning
on the English train set (Zero-Shot ) and fine-tuning on 100 instances
of other languages based on English fine-tuned models (Few-Shot ).
For Multi30K and COCO, we also use two evaluation protocols:
fine-tuning on the English train set (Zero-Shot ) and fine-tuning on
each language train set (Fine-Tune ). Note that the results on WIT
under the few-shot scenario are not reported because IGLUE [ 2]
does not provide the corresponding evaluation protocol. For more
details, please refer to Appendix B.2.
5.2 Implementation Details
Following [ 41], the image encoder is initialized using the 12-layer
Swin Transformer [ 21], and the multi-lingual encoder and fusion
encoder are initialized using the pre-trained XLM-R [ 8], which
consist of 6 layers for each. We provide a detailed comparison of
the model architecture and initialization sections between CCR
and other baselines in Appendix B.1. Also, keeping consistent with
[41] for a fair comparison, ğœin Eqn. (1)and(2)are set as 0.07. The
AdamW [ 22] optimizer with 1e-4 learning rate, 0.01 weight decay,
and first 3% linearly warm-up steps is used. The batch size on each
GPU is set to 64. The pre-training experiments were conducted on 2
NVIDIA A100s, while fine-tuning was done on 1 A100. We pre-train
all models for 30 epochs. With the acceleration of PyTorch DDP [ 18],
it takes approximately 4 days to pre-train for 30 epochs on CC3M
with 6 languages. In addition, we provide the hyper-parameters
used for fine-tuning all four datasets in Appendix C.2.
5.3 Main Performance
We report the performance of all four variants of CCRğ‘˜and base-
lines in Table 1. Note that the results of CCLM-3M on WIT are not
reported in Table 1 as we find that there is a significant overlap
 
2277Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Performance comparison on four retrieval datasets, where IR means text-to-Image Retrieval and TR means image-
to-Text Retrieval. Consistent with standard evaluation protocols, Recall@1 on xFlickr&CO, Accuracy on WIT, and average
Recall@K with K=1,5,10 on Multi30K and COCO are reported. We only calculate MRV on xFlickr&CO and Multi30K because
there is no one-to-many relationship between images and texts in WIT, whereas the texts in COCO are from different sources.
Model#Image+ xFlickr&CO WIT Multi30K COCO
#Text IR-R@1 TR-R@1 MRV 4â†“ IR TR EN DE FR CS MRV 4â†“ EN ZH JA
Fine-tune model on English training set (Zero-Shot)
xUNITER [20] 2.7M+100G 14.04 13.51 50.60 8.72 9.81 - - - - - - - -
M3P [24] 3.3M+100G 12.91 11.90 54.58 8.12 9.98 87.4 58.5 46.0 36.8 15.38 88.6 53.8 56.0
UC2[42] 3.3M+19.8M 20.31 17.89 21.52 7.83 9.09 87.2 74.9 74.0 67.9 6.16 88.1 82.0 71.7
TD-MML [28] 2.8M+52.0M 21.30 26.35 - 9.76 10.61 - - - - - - -
CCLM-3M [41] 2.8M+54.8M 64.47 62.74 13.27 - - 90.4 89.9 89.4 88.1 3.18 92.3 90.4 87.3
CCR61.8M+10.8M 29.16 28.72 15.02 6.78 8.73 88.5 87.1 87.8 85.7 3.06 91.6 89.6 86.0
CCR6-E 3.3M+19.8M 32.89 33.06 7.97 6.44 8.34 90.8 90.3 91.0 89.4 1.28 92.5 91.4 89.4
CCR101.8M+18.0M 55.45 54.88 18.96 9.94 11.73 84.2 82.5 81.9 80.8 4.13 90.0 88.0 81.7
CCR10-E 3.3M+33.0M 73.30 72.64 7.89 11.11 12.62 91.4 90.7 91.3 89.8 2.53 92.5 91.4 89.3
Few-shot fine-tune â€œEnglish fine-tuned modelâ€ on target languages (Few-Shot) Single-language fine-tune
xUNITER 2.7M+100G 14.30 13.54 - - - - - - - - - - -
M3P 3.3M+100G 13.21 12.26 - - - 87.4 82.1 67.3 65.0 - 88.6 75.8 80.1
UC23.3M+19.8M 19.79 17.59 - - - 87.2 83.8 77.6 74.2 - 88.1 84.9 87.3
CCLM-3M 2.8M+54.8M 65.31 63.91 12.93 - - 90.4 89.6 90.0 88.8 2.41 92.3 92.1 92.4
CCR61.8M+10.8M 29.28 28.72 15.24 - - 88.5 88.1 88.6 87.3 2.28 91.6 91.0 91.8
CCR6-E 3.3M+19.8M 33.19 33.41 7.81 - - 90.8 90.5 91.4 90.5 1.30 92.5 92.6 92.5
CCR101.8M+18.0M 55.91 55.24 18.11 - - 84.2 83.6 84.6 82.5 3.90 90.0 89.7 90.4
CCR10-E 3.3M+33.0M 73.74 73.27 7.62 - - 91.4 90.6 91.2 90.2 1.12 92.5 92.5 92.5
between the WIT test set and the pre-training data of CCLM. Un-
less otherwise noted, we use ISO 639-1 Abbreviations to represent
specific languages in subsequent tables. The table mapping the
two-letter codes to the specific language is provided in Appendix
A for convenience.
Recall Rates. With a smaller scale pre-trained data (#images
and #texts) and fewer language numbers than the baselines, CCR10-
E achieves SOTA results under both zero-shot and few-shot (or
fine-tuning) setting for all CCR datasets, demonstrating the good
generalizability and transferability of CCRğ‘˜among different lan-
guages. When comparing the performance difference among the
four variants of CCRğ‘˜, we can find that (1) CCR10use more lan-
guages compared to CCR6, causing it to improve the performance
on the newly added languages while hurting Recall@K of the orig-
inal languages existing in CCR6, possibly due to the increased
difficulty of alignment across more languages; (2) CCR6-E achieves
higher Recall@K and lower MRV on the original languages com-
pared to CCR6after introducing more pre-training data.
Consistency Evaluation of Recall@K. Recall that one of the
inconsistency problems leads to inconsistent recall@K in differ-
ent languages. As seen in Table 1, all baselines perform better in
English than in other languages on Multi30K and COCO because
English is used as a bridge between the visual and other languages
during their pre-training. Benefitting from the 1-to-K contrastive
paradigm, all four variants of CCRğ‘˜maintain significantly smaller
inter-language gaps on these two datasets. Among them, CCR10-
E maintains the smallest performance gap across languages onMulti30K and COCO in the zero-shot scenario, even though this
scenario is more favourable for English-related retrieval. More sur-
prisingly, when CCRğ‘˜is fine-tuned in each language separately, the
performance gap on various languages almost disappears, which
reflects the promising application of CCRğ‘˜in practical applications.
Consistency Evaluation of Rank. Recall that the other prob-
lem results in the inconsistency of rank. The motivation behind
proposing MRV is that Recall@K cannot reflect such differences
across languages within an instance. Therefore, we calculate MRV
for four languages (EN, DE, JA, and ZH) on xFlickr&CO and four
languages (EN, DE, FR, and CS) on Multi30K, which are denoted
as MRV 4in Table 1. We also report MRV 4of all compared models
except TD-MML based on the checkpoints obtained from the official
IGLUE GitHub repository2. It can be found that MRV 4for CCLM,
which uses 1-to-1 contrastive learning, has improved substantially
compared to M3P and UC2, while CCRğ‘˜can improve further and
achieve the lowest MRV. Similar to Recall@K, adding more lan-
guages (CCR6â†’CCR10and CCR6-Eâ†’CCR10-E) will result in a
higher MRV due to the capacity constraints of the model and the
elevated difficulty of the optimization objective.
5.4 Ablation Study
To verify the effectiveness of each model component, we conduct
ablation experiments by removing critical components. The ablated
variants we consider are as follows: w/o KCL: 1-to-K Contrastive
Learning (KCL) is replaced with 1-to-1 contrastive learning; w/o
2https://github.com/e-bug/iglue
 
2278KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, & Xudong Liu
Table 2: Ablation study on pre-training tasks. For Multi30K
and COCO, the average of all languages is reported.
ModelxFlickr&CO Multi30K COCO
Avg R@1 MRV 4Avg Lang MRV 4Avg Lang
CCR628.94 15.02 87.3 3.06 89.1
-w/o KCL 24.67 18.89 82.9 8.21 87.2
-w/o H-MITM 27.99 15.14 85.9 4.72 88.2
-w/o H-CMLM 26.20 16.51 79.2 6.83 87.5
CCR10-E 72.97 7.89 90.8 2.53 91.1
-w/o KCL 68.14 11.02 84.6 6.45 88.8
-w/o H-MITM 70.95 8.29 87.4 3.89 90.3
-w/o H-CMLM 69.48 9.45 85.9 4.78 89.6
H-MITM: Hard sample mining for MITM is replaced with random
uniform sampling from the candidate set; w/o H-CMLM: Hard
sample mining for CMLM is replaced with uniform sampling from
the candidate set.
Due to space constraints, we only report results for CCR6and
CCR10-E under the zero-shot setting in Table 2. Note that the other
two variants also show a similar trend. As can be seen from the
results, each pre-training task and sampling approach proposed to
contribute to the improvement in both Recall@K and MRV 4. More
specifically, 1-to-K contrastive learning has the largest improve-
ment for all metrics, while 1-to-1 contrastive learning is still better
than the results without contrastive learning. Hard sample mining
positively affected both MITM and CMLM downstream tasks.
5.5 Further Study
5.5.1 Pure Contrastive Learning. In fact, CCRğ‘˜is proposed to en-
sure that the modelâ€™s parameter number and pre-training tasks are
similar to other baselines. However, neither MITM and CMLM tasks
nor the fusion encoder is necessary for the retrieval task. Therefore,
we further compare the effect of 1-to-K and 1-to-1 contrastive learn-
ing on Recall@K and MRV with the fusion encoder removed, while
other settings remain consistent with CCR6. As seen from Figure
5(a), 1-to-K contrastive learning can still lead on both xFlickr&CO
and Multi30k.
5.5.2 Loss and Performance. To better understand why our method
works, we record the 1-to-1 contrastive loss and 1-to-K contrastive
loss during the pre-training process of â€œCCR6â€ and â€œCCR6-w/o
KCLâ€, respectively. In addition, we evaluate the checkpoints every
5 epochs on Multi30K under zero-shot setting and plot the results
in Figure 5(c). The figure shows that 1-to-K contrastive learning
performs better at all evaluated checkpoints. Attributed to the ab-
sence of directional bias, when pre-training with 1-to-K contrastive
learning, the corresponding loss values remain lower than those
when using 1-to-1 contrastive learning.
5.5.3 T-SNE Visualization. A T-SNE visualization similar to that in
Section 3.3 is shown in Figure 5(d) and Figure 5(e), which contains
10 instances randomly sampled in xFlickr&CO. Comparing to 1-to-
1 contrastive learning, 1-to-K contrastive learning enables higher
discrimination between instances and a more balanced distribution
IR R@1 TR R@1 MVR4051015202530 1-to-K CL
1-to-1 CL(a) The comparison of pure contrastive
learning on XFlickr&Co.
Avg Lang MVR4010203040506070801-to-K CL
1-to-1 CL(b) The comparison of pure contrastive
learning on Multi30K.
0 5 10 15 20 25 301.01.52.02.53.03.51-to-K Contrastive Loss
30405060708090
Average Recall@K
25.368.677.479.480.982.0 82.4 82.9
25.377.083.7 84.486.0 86.2 87.0 87.3
CCR6
CCR6 -w/o KCL
(c) Comparison of loss function value and average Recall@K with K=1,5,10 on Multi30K
when using 1-to-1 contrastive learning and 1-to-K contrastive learning.
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.56
4
2
0246
Image
T ext (EN)
T ext (DE)
T ext (JA)
T ext (ZH)
(d) T-SNE visualization of CCR6.
7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.06
4
2
024
Image
T ext (EN)
T ext (DE)
T ext (JA)
T ext (ZH) (e) T-SNE visualization of CCR6-w/o KCL.
Figure 5: Futher Study in Alignment Process.
within instances. In addition, a case study on failure alignment is
provided in Appendix 6 for potential further improvement.
6 Case Study
After manually analyzing the wrong cases in xFlickr&CO, which
are not correct under some language settings, we summarized two
typical causes of matching errors: fine-grained semantic match-
ing errors and pseudo-negative samples. We give some cases for
each of them in Figure 6. Since images are more presentable and
comprehensible than texts, we only use the error cases from the
text-to-image retrieval (IR) task. The first four cases demonstrate
a fine-grained semantic matching error. For example, the concept
of â€œheadbandâ€ in the first case is so specialized that the image can
match all other features when retrieved using German (DE) and
Turkish (TR). The last two cases show a pseudo-negative sample
error, where the images retrieved actually match the text semantics,
but these matching relationships are missing annotations in the
dataset. For example, in the fifth case, both images retrieved for
the "hockey game" matched the textual description, yet only one is
labelled as correct in the xFlickr&CO dataset.
 
2279Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
a group of men walk down 
the middle of a street that 
has poles lining the streetDE ES ID JA RU TR ZH EN
a living room has a couch 
and a rustic chest for a 
coffee table
a man is guiding the 
hockey puck across the ice 
while an opposing player 
races towards ityoung boy in a white 
striped shirt and headband 
holding a tennis racket
two men wearing earmuffs 
inspect a racing car s 
engine on an asphalt track
fast food displayed on a 
table with sandwich and 
soup
Figure 6: Six wrong cases of text-to-Image Retrieval (IR) on xFlickr&CO. We only provide the English text in each instance as a
reference, and the images are actually retrieved from the text corresponding to the labelled language at the top of each column.
The green and red boxes outside the images represent the correct and incorrect images.
7 Discussion
The Novelty of 1-to-K Contrastive Learning. The proposed modi-
fication is not groundbreaking but based on traditional 1-to-1 con-
trastive learning. However, recall that 1-to-1 contrastive learning,
which has been carried over from the cross-lingual or cross-modal
domains, is still the dominant paradigm in CCP. The call to change
a taskâ€™s pre-training paradigm is usually tough. Changing to 1-to-K
contrastive learning is minimal yet effective and easily applicable
to the existing CCR models based on SimSiam networks.
The Significance of the Consistency in CCR. Maintaining con-
sistency in CCR is important. For example, in a cross-border e-
commerce business, consistency in recall across languages ensures
that the entire retrieval system can be supported by a single funda-
mental model. Further, the query with the same semantics issued by
different native-speaking customers should be expected to return
the same results, meaning there needs to be good consistency in
rank across different languages within an instance. If we evaluate
the retrieval model with Recall@K on each language only, the true
performance of the CCR model will not be reflected.
Further Consistency. Ensuring equal contributions across lan-
guages in all aspects is challenging. For instance, XLM-R, CCRğ‘˜â€™s
cross-lingual encoder, is trained on the 2.5TB CommonCrawl Cor-
pus encompassing 100 languages. Discrepancies in data sizes be-
tween high-resource and low-resource languages within this cor-
pus, like the 100GB English data versus the 0.1GB Sundanese data,impede XLM-R from achieving uniform performance across lan-
guages. Balancing language contributions during pre-training could
help narrow the performance gap but would require substantial
computational resources, which we will explore in future studies.
8 Conclusion
In this paper, we first analyze the two problems of inconsistency
existing in the current CCP methods and point out their impact
on CCR via theoretical analysis and empirical studies. Then we
propose a 1-to-K contrastive paradigm and a CCP model, CCRğ‘˜,
based on it, which equally aligns all languages with vision at once,
effectively improving the consistency in CCR. In addition, a new
evaluation metric, MRV, is proposed to portray the consistency of
each language rank within each instance. Exclusive experiments on
the four CCR datasets show that our model scales well and achieves
new SOTA on both Recall@K and MRV.
Acknowledgements
This work was supported by the National Science and Technology
Major Project under Grant 2022ZD0120202, in part by the National
Natural Science Foundation of China (No. U23B2056), in part by
the Fundamental Research Funds for the Central Universities, and
in part by the State Key Laboratory of Complex & Critical Software
Environment.
 
2280KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, & Xudong Liu
References
[1]Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti
Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. 2022. VLMo: Unified
vision-language pre-training with mixture-of-modality-experts. Advances in
Neural Information Processing Systems 35 (2022), 32897â€“32912.
[2]Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott,
Edoardo Maria Ponti, and Ivan VuliÄ‡. 2022. IGLUE: A benchmark for transfer
learning across modalities, tasks, and languages. In International Conference on
Machine Learning. PMLR, 2370â€“2392.
[3]Fredrik Carlsson, Philipp Eisen, Faton Rekathati, and Magnus Sahlgren. 2022.
Cross-lingual and Multilingual CLIP. In Proceedings of the Thirteenth Language
Resources and Evaluation Conference. European Language Resources Association,
Marseille, France, 6848â€“6854. https://aclanthology.org/2022.lrec-1.739
[4]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Concep-
tual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail
Visual Concepts. In CVPR. Computer Vision Foundation / IEEE, 3558â€“3568.
[5]Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Pi-
otr DollÃ¡r, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection
and evaluation server. arXiv preprint arXiv:1504.00325 (2015).
[6]Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation
learning. In European conference on computer vision. Springer, 104â€“120.
[7]Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang,
Xia Song, Xian-Ling Mao, He-Yan Huang, and Ming Zhou. 2021. InfoXLM:
An Information-Theoretic Framework for Cross-Lingual Language Model Pre-
Training. In Proceedings of the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies.
3576â€“3588.
[8]Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-
laume Wenzek, Francisco GuzmÃ¡n, Ã‰douard Grave, Myle Ott, Luke Zettlemoyer,
and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learn-
ing at Scale. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics. 8440â€“8451.
[9]Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model
pretraining. Advances in neural information processing systems 32 (2019).
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[11] Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Sid-
dharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaud-
hary, et al .2021. Beyond english-centric multilingual machine translation. The
Journal of Machine Learning Research 22, 1 (2021), 4839â€“4886.
[12] Hongliang Fei, Tan Yu, and Ping Li. 2021. Cross-lingual Cross-modal Pretrain-
ing for Multimodal Retrieval. In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational Linguistics, Online, 3644â€“3650.
https://doi.org/10.18653/v1/2021.naacl-main.285
[13] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive
Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Em-
pirical Methods in Natural Language Processing. 6894â€“6910.
[14] Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao
Jia, Yinfei Yang, and Jason Baldridge. 2021. MURAL: Multimodal, Multitask
Representations Across Languages. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021. Association for Computational Linguistics, Punta
Cana, Dominican Republic, 3449â€“3463. https://doi.org/10.18653/v1/2021.findings-
emnlp.293
[15] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for
generating image descriptions. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 3128â€“3137.
[16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al .
2017. Visual genome: Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision 123, 1 (2017), 32â€“73.
[17] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language repre-
sentation learning with momentum distillation. Advances in neural information
processing systems 34 (2021), 9694â€“9705.
[18] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,
Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al .2020. PyTorch
distributed: experiences on accelerating data parallel training. Proceedings of the
VLDB Endowment 13, 12 (2020), 3005â€“3018.
[19] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang,
and Jieping Xu. 2019. COCO-CN for cross-lingual image tagging, captioning,
and retrieval. IEEE Transactions on Multimedia 21, 9 (2019), 2347â€“2360.
[20] Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier,
and Desmond Elliott. 2021. Visually Grounded Reasoning across Languages and
Cultures. In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing. 10467â€“10485.
[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer us-
ing shifted windows. In Proceedings of the IEEE/CVF international conference on
computer vision. 10012â€“10022.
[22] Ilya Loshchilov and Frank Hutter. [n. d.]. Decoupled Weight Decay Regularization.
InInternational Conference on Learning Representations.
[23] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretrain-
ing task-agnostic visiolinguistic representations for vision-and-language tasks.
Advances in neural information processing systems 32 (2019).
[24] Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang,
Dongdong Zhang, and Nan Duan. 2021. M3P: Learning universal representations
via multitask multilingual multimodal pre-training. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. 3977â€“3986.
[25] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[26] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011. Im2text: Describing
images using 1 million captioned photographs. Advances in neural information
processing systems 24 (2011).
[27] Maxime Portaz, Hicham Randrianarivo, Adrien Nivaggioli, Estelle Maudet,
Christophe Servan, and Sylvain Peyronnet. 2019. Image search using multi-
lingual texts: a cross-modal learning approach between image and text. Ph. D.
Dissertation. qwant research.
[28] Chen Qiu, Dan Onea t,Äƒ, Emanuele Bugliarello, Stella Frank, and Desmond Elliott.
2022. Multilingual Multimodal Learning with Machine Translated Text. In Find-
ings of the Association for Computational Linguistics: EMNLP 2022. Association
for Computational Linguistics, Abu Dhabi, United Arab Emirates, 4178â€“4193.
https://aclanthology.org/2022.findings-emnlp.308
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.2021. Learning transferable visual models from natural language supervision.
InInternational conference on machine learning. PMLR, 8748â€“8763.
[30] Bin Shan, Yaqian Han, Weichong Yin, Shuohuan Wang, Yu Sun, Hao Tian, Hua
Wu, and Haifeng Wang. 2022. ERNIE-UniX2: A Unified Cross-lingual Cross-modal
Framework for Understanding and Generation. arXiv preprint arXiv:2211.04861
(2022).
[31] Jiaming Song and Stefano Ermon. 2020. Multi-label contrastive predictive coding.
Advances in Neural Information Processing Systems 33 (2020), 8161â€“8173.
[32] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc
Najork. 2021. Wit: Wikipedia-based image text dataset for multimodal multi-
lingual machine learning. In Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 2443â€“2449.
[33] Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao,
Xiaogang Wang, Jie Zhou, and Jifeng Dai. 2022. Towards All-in-one Pre-training
via Maximizing Multi-modal Mutual Information. arXiv preprint arXiv:2211.09807
(2022).
[34] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiview
coding. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK,
August 23â€“28, 2020, Proceedings, Part XI 16. Springer, 776â€“794.
[35] Kirill Tyshchuk, Polina Karpikova, Andrew Spiridonov, Anastasiia Prutianova,
Anton Razzhigaev, and Alexander Panchenko. 2023. On Isotropy of Multimodal
Embeddings. Information 14, 7 (2023), 392.
[36] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[37] Yabing Wang, Jianfeng Dong, Tianxiang Liang, Minsong Zhang, Rui Cai, and Xun
Wang. 2022. Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning.
InProceedings of the 30th ACM International Conference on Multimedia . 422â€“433.
[38] Yuya Yoshikawa, Yutaro Shigeto, and Akikazu Takeuchi. 2017. STAIR Captions:
Constructing a Large-Scale Japanese Image Caption Dataset. In Proceedings of
the 55th Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers). Association for Computational Linguistics, Vancouver, Canada,
417â€“421. https://doi.org/10.18653/v1/P17-2066
[39] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image
descriptions to visual denotations: New similarity metrics for semantic infer-
ence over event descriptions. Transactions of the Association for Computational
Linguistics 2 (2014), 67â€“78.
[40] Yan Zeng, Xinsong Zhang, and Hang Li. 2022. Multi-Grained Vision Language
Pre-Training: Aligning Texts with Visual Concepts. In International Conference
on Machine Learning. PMLR, 25994â€“26009.
[41] Yan Zeng, Wangchunshu Zhou, Ao Luo, Ziming Cheng, and Xinsong Zhang.
2022. Cross-view language modeling: Towards unified cross-lingual cross-modal
pre-training. arXiv preprint arXiv:2206.00621 (2022).
[42] Mingyang Zhou, Luowei Zhou, Shuohang Wang, Yu Cheng, Linjie Li, Zhou Yu,
and Jingjing Liu. 2021. UC2: Universal cross-lingual cross-modal vision-and-
language pre-training. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 4155â€“4165.
 
2281Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A ISO 639 Language Codes
We give the ISO-691 codes for all the language codes that appear in
the main text and appendices in Table 3 for reference.
Table 3: Part of codes and languages in ISO 639-1.
Code Language Family Script
AR Arabic Afro-A Arabic
BG Bulgarian Indo-E Cyrillic
CS Czech Indo-E Latin
DA Danish Indo-E Latin
DE German Indo-E Latin
EL Greek Indo-E Greek
EN English Indo-E Latin
ES Spanish Indo-E Latin
ET Estonian Uralic Latin
FR French Indo-E Latin
ID Indonesian Austron Latin
JA Japanese Japonic Kanji
KO Korean Koreanic Hangul
RU Russian Indo-E Cyrillic
TR Turkish Turkic Latin
VI Vietnamese Austro-A Latin
ZH Chinese Sino-T Hanzi
B Supplement on Experiment Setup
B.1 Baseline
This section details the baselines used for comparison and com-
pares key information about their architectures and pre-training
processes in Table 4.
xUNITER [20] .is a multi-lingual variant of UNITER [ 6], which
follows the architecture of UNITER and the parameters are initial-
ized with XLM-R base[8]. It also has a twin, mUNITER, which is
initialized using mBERT [ 10]. Considering that xUNITER works
better, we ignore the results of mUNITER in this paper. xUNITER
and mUNITER are pre-trained using image-English text pairs and
parallel corpus alternately composed of batch.
UC2[42] .presents the first MT-augmented pre-training model
that pivots primarily on images and complementary on English
to learn cross-lingual cross-modal representation from large-scale
of multi-lingual image-to-text pairs. Two new pre-training tasks,
Masked Region-to-Token Language Modeling and Visual Transla-
tion Language Modeling, are proposed to facilitate the model to
obtain better alignment between vision and different languages.
M3P [24] .combines multi-lingual pre-training and multi-modal
pre-training into a unified framework via multitask Learning. multi-
modal code-switched training is proposed to further alleviate the
issue of lacking enough labeled data for non-English multi-modal
tasks and avoid the tendency to model the relationship between
vision and English text.TD-MML [28] .uses translated data for multi-lingual multi-modal
learning, which are applied in both pre-training and fine-tuning
data with the existing CCP model. In order to prevent the model
from learning from low-quality translated texts, two metrics are
proposed for automatically removing the low-quality translation
texts from the resulting datasets.
CCLM [41] .is a CCP framework that unifies cross-lingual pre-
training and cross-modal pretraining with shared architectures and
objectives. Contrastive learning is introduced for cross-modal and
cross-lingual alignment, respectively.
Table 4: The image feature source, backbone initialization
method, and the language number (#Lang) involved in pre-
training for each CCP model.
Model Image Feature Source Initialization of Backbone #Lang
xUNITER36 Rols from Faster XLM-R base104R-CNN with ResNet-101 (12 layers)
UC2 36 Rols from Faster XLM-R base104R-CNN with ResNet-101 (12 layers)
M3P10-100 Rols from Faster XLM-R base6R-CNN with ResNet-101 (12 layers)
TD-MML36 Rols from Faster XLM-R base20R-CNN with ResNet-101 (12 layers)
CCLMSwin Transformer Odd-numbered layers20(12 layers, Trainable) in XLM-R large (12 layers)
CCRğ‘˜ Swin Transformer Odd-numbered layers6-10(12 layers, Trainable) in XLM-R large (12 layers)
B.2 Evaluation Dataset
xFlickr&CO .is a novel dataset purposed by ICLUE [ 2] and
collected by combining 1000 images from Flickr30K and COCO
respectively. The existing captions from [ 5] and [ 15] are used for
English and Japanese, while the captions are from crowd-source
for the other 6 languages.
WIT .means â€œWikipedia-based Image-Textâ€ dataset [ 32] col-
lected instances from the websites of Wikipedia in 108 languages.
For training, a subset of 500K captions is randomly sampled from
the English training set of WIT. For evaluation, the WIT test data
released as part of its corresponding Kaggle competition3is used.
Multi30K .extends Flickr30K [ 39] from English to German, French
and Czech. It contains 31,783 images obtained from Flickr and pro-
vides five captions per image in English and German, and one
caption per image in French and Czech. Dataset splits are defined
as the original Flickr30K.
COCO .extends the original COCO Caption [ 5] by translating
the captions into Japanese and Chinese. The Japanese and Chinese
subsets consist of 820k and 20k captions respectively. Following
previous work, we use the same train, dev, and test splits for English
and Japanese as defined by Karpathy and Fei-Fei [15]. For Chinese,
we use the COCO-CN split [19].
 
2282KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, & Xudong Liu
Table 5: Statistics on the datasets for evaluation.
DatasetTrain TestLanguage#Text #Image #Text #Image
xFlickr&CO 145K 29K 2K 2KDE EN ES ID
JA RU TR ZH
WIT 500K 469K 9.6K 6.2KAR BG DA EL
EN ET ID JA
KO TR VI
Multi30K 29K 29K 1K 1K EN DE FR CS
COCO567K 113K 25K 5K EN JA
18K 18K 1K 1K ZH
Table 6: Hyper-parameters under the zero-shot settings.
Parameter xFlickr&CO WIT Multi30K COCO
Learning rate 1e-5 3e-5 3e-5 3e-5
Batch size 64 80 64 64
Epochs 10 10 10 10
Max input length 80 80 40 40
Table 7: Hyper-parameters under the fine-tuning settings.
Parameter xFlickr&CO Multi30K COCO
Shot number 100 - -
Learning rate 1e-5 3e-5 3e-5
Batch size 64 64 64
Epochs 60 10 10
Max input length 80 40 40
C Implementation Details
C.1 Evaluation Protocols
Zero-Shot .Only pre-training and fine-tuning on the English
train set, then evaluate the test set in each target language.
Few-Shot Fine-tune .First pre-training and fine-tuning on Eng-
lish train set. Then twice fine-tuning 100 labeled instances in a
target language and evaluating the test set of this target language.
Single-Language Fine-tune .First pre-training and fine-tuning
on English train set. Then, fine-tuning the training set of the target
language and evaluating the test set of this target language.
C.2 Hyperparameter Setting
For zero-shot xFlickr&CO and WIT, we first fine-tune the model on
the English training set, and then evaluate zero-shot and few-shot
performance in other languages. Following [ 41], for both zero-shot
and few-shot experiments, we use AdamW optimizer with ğ›½1= 0.9
andğ›½2= 0.999; weight decay is set to 0.01; learning rate scheduler
is linear. The all hyper-parameters used are shown in Table 6.
3www.kaggle.com/c/wikipedia-image-captionC.3 The Method of Hard Negative Sampling
For positive samples, given an image ğ‘–ğ‘—, its associated set of texts
(ğ‘¡ğ‘—1,ğ‘¡ğ‘—2,...,ğ‘¡ğ‘—ğ¾) can be regarded as positive samples. Among these
texts, the hardest positive sample ğ‘¡ğ‘–ğ‘˜poscan be identified as the text
that aligns worst with the image, and the degree of alignment can
be estimated by computing the cosine similarity between the image
and text representations. Accordingly, we can sample the index
ğ‘˜posof the hardest positive sample from a specific distribution ğ‘‡,
which can be expressed as
ğ‘¡pos
ğ‘—=ğ‘¡ğ‘–ğ‘˜pos, ğ‘˜posâˆ¼ğ‘‡,ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ğ‘ƒ ğ‘‡(ğ‘˜)=1âˆ’Ë†ğ‘¡ğ‘‡
ğ‘—ğ‘˜Ë†ğ‘–ğ‘—
Ãğ¾
ğ‘˜â€²Ë†ğ‘¡ğ‘‡
ğ‘—ğ‘˜â€²Ë†ğ‘–ğ‘—(8)
whereğ‘‡is a multinomial distribution.
For negative samples, if the image and the text from different
tuples are well aligned, they can be regarded as hard negative sam-
ples for each other. Also, we estimate the degree of alignment using
the cosine similarity and sample the index of the negative example
from a multinomial distribution. Thus, the process of obtaining the
hard negative image can be expressed as
ğ‘–neg
ğ‘—=ğ‘–ğ‘—neg, ğ‘—negâˆ¼ğ‘…,ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ğ‘ƒ ğ‘…(ğ‘—â€²)=Ãğ¾
ğ‘˜Ë†ğ‘¡ğ‘‡
ğ‘—ğ‘˜Ë†ğ‘–â€²
ğ‘—
Ãğ‘
ğ‘—â€²â‰ ğ‘—Ãğ¾
ğ‘˜Ë†ğ‘¡ğ‘‡
ğ‘—ğ‘˜Ë†ğ‘–ğ‘—â€²(9)
whereğ‘…is a multinomial distribution. Similarly, we can obtain the
hard negative text for each image in the batch.
C.4 The Method of Rank
We obtain the representations from the text encoder and image
encoder outputs and rank the candidates by cosine similarity. For
CCRğ‘˜and ablation models containing the fusion encoder, we re-
rank only the top ğ‘candidates using the Fusion encoder to better
adapt to the web-scale data. Specifically, we use the projection head
used for the multi-lingual image-text matching task to predict the
match probability between the query and each shortlisted candidate
and re-rank the candidates regarding this probability only. In our
experiment, ğ‘is 256 for COCO and 128 for the other three datasets.
Table 8: Time and memory comparison under different lan-
guage number settings.
Model Training Time (Per Epoch) Memory (Per A100)
CCR1137min 27,814MB
CCR6158min (15%â†‘) 31,364MB (13% â†‘)
CCR10173min (26%â†‘) 34,203MB (23% â†‘)
D Time and Memory Comparison
We compare the modelâ€™s training time and GPU memory consump-
tion for different language numbers of translated texts, which are
reported in Table 8. The results in the table are the average results
measured while keeping other external conditions constant as much
as possible. It is easy to find that both training time and memory
usage increase linearly with the number of languages. Specifically,
the training time increases by 4.2 min per language for 1 Epoch,
while the memory footprint increases by 710 MB per language per
Nvidia A100 40GB.
 
2283