Auctions with LLM Summaries
Avinava Dubeyâˆ—
avinavadubey@google.com
Google Research
Mountain View, California, USAZhe Fengâˆ—
zhef@google.com
Google Research
Mountain View, California, USARahul Kidambiâˆ—
rahulkidambi@google.com
Google Research
Mountain View, California, USA
Aranyak Mehtaâˆ—
aranyak@google.com
Google Research
Mountain View, California, USADi Wangâˆ—
wadi@google.com
Google Research
Mountain View, California, USA
ABSTRACT
We study an auction setting in which bidders bid for placement
of their content within a summary generated by a large language
model (LLM), e.g., an ad auction in which the display is a summary
paragraph of multiple ads. This generalizes the classic ad settings
such as position auctions to an LLM generated setting, which allows
us to handle general display formats. We propose a novel factorized
framework in which an auction module and an LLM module work
together via a prediction model to provide welfare maximizing
summary outputs in an incentive compatible manner. We provide
a theoretical analysis of this framework and synthetic experiments
to demonstrate the feasibility and validity of the system together
with welfare comparisons.
CCS CONCEPTS
â€¢Theory of computation â†’Computational advertising the-
ory;Computational pricing and auctions ;â€¢Computing method-
ologiesâ†’Natural language generation.
KEYWORDS
Auction Design, Computational Advertising
ACM Reference Format:
Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, and Di Wang.
2024. Auctions with LLM Summaries. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3672022
1 INTRODUCTION
The advent of large language model (LLM) technology has the po-
tential to change the user experience of online services such as
internet search, online recommendations [ 8], or shopping [ 6]. For
example, search platforms and apps, e.g., Microsoft Bing [ 16] and
Google Search [ 9], have already experimented with generative AI
tools to provide augmented search summarization to facilitate usersâ€™
âˆ—The authorlist follows alphabetical order.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672022
Figure 1: Factorized model for Auctions with LLM Summaries.
search experience. Such summarization (e.g., based on retrieval aug-
mented generation RAG [ 10]) can sometimes provide an efficient
way for users to gain useful information in a more condensed space.
For queries of a commercial nature, search platforms respond
with relevant online advertising. Online search advertising has
provided a means not only to connect buyers and sellers, but also to
support free internet services to users. Given the exciting potential
of LLMs to summarize multiple sources of content and provide
a succinct and informative output, it is natural to ask how LLM
technology can help improve online advertising.
In the ever-evolving landscape of online advertising, auction
design has been a critical component towards improving the ef-
fectiveness and efficiency of ad delivery. A well-designed auction
mechanism not only provides revenue for the platform but also
ensures relevancy and value for users and advertisers alike. It ef-
ficiently allocates ad impressions to the right audiences, creating
high value to advertisers and fostering a positive user experience.
While auctions have proved to be a flexible method in various
fixed settings such as a single slot [ 21], position auctions [ 5,20]
or list of rich ads [ 1], auctions have not been studied in general
summarization settings.
Here, we consider a general setting in which an LLM takes as
input a set of ads (and ad assets such as creatives and web pages)
and returns a summarized paragraph which can be more helpful
to the user, e.g., to compare and contrast product features, use
cases, or price. This immediately raises a few challenges compared
to the currently used static settings. For example, in a position
auction the auction directly determines the position of each ad,
and furthermore there is a fixed text creative for each ad. Since
the position based predicted click-through rates (CTRs) are known
to the auction at decision time, the auction knows an estimate of
the expected clicks, welfare, revenue for each possible allocation
(permutation of ads), and hence can make efficient decisions on
 
713
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, & Di Wang
allocation and pricing. In the LLM-based setting that we introduce,
the final summarized ad text and relative user attention to the
individual ads are all determined at run time by the LLM. Thus the
LLM black-box lies between the auctionâ€™s decision and what the
user sees, so the auction can not fully control the latter. How can
an auction then choose allocations and prices to maximize welfare
or revenue, while also providing good auction incentives in such a
setting? That is the question we study in this paper.
Our goal is to design an auction framework which can accom-
modate content summarization via LLMs in quite general summa-
rization settings. We want a system in which the auction and the
LLM work hand-in-hand to provide good properties: Firstly, the
LLM summaries should be succinct and accurate, and have good
entailment to the ad assets. Secondly, the system should provide
good incentive properties to the agents (advertisers). For example,
a higher bid should result in higher user-attention and higher click-
through rate for an ad. Formally, we want the entire mechanism
to be Incentive Compatible (IC). Thirdly, we want the mechanism
to provide efficient outcomes with high social welfare, leading to
high value to advertisers and users.
While we use ad auctions as the guiding application, we note
that our formulation and results apply more generally to any sum-
marization task where the individual content items are owned by
agents, and where (a) each agent derives a benefit to have more
prominence in the summary, (b) the platformâ€™s goal is to maximize
some welfare objective which incorporates the quality of the con-
tent items and the agentâ€™s values expressed via bids, and (c) the
quality or relevance of the content items for user queries is learned
through a prediction model. For example, one may envision a rec-
ommendation setting in which individual content providers bid
to be shown with higher prominence. In a different setting, the
content items could be represented by platform-internal systems
representing different objective functions. In such a setting, the
pricing component of the auction is not needed. An example of
such a setting is aggregation of user reviews of a product or service
in accordance with some notion of user reputation or quality score.
1.1 Our Contribution
Factorized Model for Auctions with LLM Summaries. We introduce
a general problem of running auctions to generate summaries of ğ‘˜
ads, with an LLM in the loop. By a summary of ads we mean any
collective representation of ads, ranging from an ordered list of
fixed ad assets (as in a position auction) to a combined summary of
all ads with potential images or videos.
We then develop a framework for an auction to work for such
general summary requirements. Our framework contains two main
modules, an auction module and an LLM module. The auction
module takes as input the bids, and ad qualities, as well as click
predictions from a predicted click-through rate (pCTR) module,
and outputs a prominence allocation and prices. The former is a
key abstraction that we introduce to be the interface between the
auction and LLM module. The LLM module takes as input the
prominence allocation from the auction module, and generates a
summary (in the required format). We can take the prominences to
be real numbers, although the general definition allows for abstract
prominence spaces.We then describe three sufficient properties for such a factorized
model to work effectively:
(a)The auctionâ€™s allocation function should be monotonic in bids,
i.e., a higher bid results in a higher prominence number.
(b)The LLM should have a faithfulness property which requires the
LLM output follows the auctionâ€™s instructions via the promi-
nences. One can think of the LLM as computing a function
which converts the prominences to real user attention. Faithful-
ness requires that this function is monotonic in each adâ€™s promi-
nence, i.e., as an adâ€™s prominence allocation increases it receives
higher user attention. It also requires the user-consideration
achieved by the different ads to be proportional to their promi-
nences.
(c)The CTR prediction module works with the abstract promi-
nences as features and learns the above function implemented
by the LLM module. Formally, we require that the CTR mod-
ule generate an unbiased estimation of the click-through rate
function given the prominence, under expectation of the LLM
generation.
We show that these three properties allow us to close the loop
between the three modules, and the auction can be oblivious to the
function implemented by the LLM.
Utilizing advanced prompt design techniques [ 15,27], we strate-
gically craft prompts that align the the LLM with the auctionâ€™s
instructions. We provide synthetic experiments to verify the feasi-
bility and validity of our factorized model approach for a specific
simple choice of summarization, which we call dynamic word length.
Experiments also demonstrate the efficiency of our mechanism in
that it can produce more efficient outcomes compared to a static
position auction or even a fixed-length auction that we define later.
Also for the specific example of dynamic word length summa-
rization, we show that for a simple yet very realistic family of pCTR
models, we can fully characterize the (exact) welfare-maximizing
auction in that setting. This is in contrast to the case of rich format
position auction where finding exact welfare-maximizing auction is
typically challenging due to the combinatorial nature of the feasible
space. Intuitively, LLM empowers us to expand the feasible space
to a much larger continuous space. This both increases the optimal
welfare and makes the optimization task easier, which at a high
level is analogous to relaxing integer program to linear program.
1.2 Related Work
There has been rich literature on position auctions in online adver-
tising, e.g., [ 5,20], including rich-ad formats [ 1,3], and our work
can be regarded as an extension of current position auctions to
incorporate new formats based on generative AI.
Mechanism design with Large Language Models (LLM) is a very
new but rising field. A work by Duetting et al . [4] propose a token-
based auction framework for LLM agents, where the bidders use
LLM to generate ads and the auction applies distributional aggre-
gation across different LLMs from the bidders to generate the ads
paragraph in a token-by-token. In [ 4], the allocation and payment
rules are both operated in a token-by-token manner, where the
tokens are randomly generated by an aggregation of each agentâ€™s
LLM. Whereas, in our paper, we propose a factorized framework
that contains an auction module and an LLM module, in which the
 
714Auctions with LLM Summaries KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
allocation and payment are still decided by auction module and the
LLM module will only be used to generate summaries following
the guidance from the allocated prominence by the auction mod-
ule. In another work, Feizi et al . [7] discuss general challenges and
opportunities of online advertising in the age of generative AI.
We will be using prompting to ensure LLMs stick to auctionâ€™s in-
structions and to generate the synthetic dataset containing queries
and ads. Prompting is a fast and efficient method for downstream
application of LLMs and has been extensively studied [ 14,18,22â€“
24,27]. We use Chain-of-Thought (CoT) [ 27], and provide few shot
reasoning as intermediate steps to improve performance to down-
stream goal. Multiple variants of CoT including Zero-shot CoT [ 11],
self consistency [ 23], Tree of Thoughts [ 25], Graph of Thoughts [ 2]
further extend the reasoning capability of CoT methods. Among
them, we use iterative reasoning to ensure LLMs stick to auction
instruction.
2 MODEL AND PRELIMINARIES
As mentioned in Section 1.1, we propose a factorized model con-
sisting of an auction module, an LLM module, and a click-through
rate prediction module. A high-level schematic of this factorized
model is provided in Figure 1. We next describe the input-output
characteristics of the three modules in more detail.
2.1 Auction Module
We consider ğ‘›different bidders competing for the ad summaries
shown by LLM. Each bidder ğ‘–âˆˆ[ğ‘›]has a private valueğ‘£ğ‘–âˆˆRâ‰¥0
when its ad gets clicked. Follow the standard Bayesian mechanism
design literature, we assume ğ‘£ğ‘–âˆ¼F ğ‘–and that the valuation dis-
tributionFğ‘–is known to the other bidders and to the auctioneer.
DenoteF=Ã—ğ‘–Fğ‘–be the joint distribution of valuation profile
ğ‘£=(ğ‘£1,ğ‘£2,Â·Â·Â·,ğ‘£ğ‘›). Letğ‘ğ‘–âˆˆBğ‘–âŠ†Râ‰¥0denote bidder ğ‘–â€™s bid which
can be different from the true value ğ‘£ğ‘–. Without loss of generality,
we assume value and bids are from the same space, i.e., ğ‘£ğ‘–âˆˆBğ‘–.
Denoteğ‘=(ğ‘1,Â·Â·Â·,ğ‘ğ‘›) âˆˆB =Ã—ğ‘–Bğ‘–as the bid profile of ğ‘›
bidders. Following the standard auction design literature, we de-
fineğ‘âˆ’ğ‘–as the bid profile of the other bidders except for bidder
ğ‘–. For each bidder ğ‘–, letğ‘§ğ‘–âˆˆZ ğ‘–, be a feature which contains as-
sets from the bidder, such as the ad creative and landing page. Let
ğ‘§=(ğ‘§1,Â·Â·Â·,ğ‘§ğ‘›) âˆˆ Z =Ã—ğ‘–Zğ‘–be the feature profile of all the
biddersâ€™ ads, which will be used as context in the LLM.
Relative Prominence: As usual, the auction mechanism M=
(ğ‘¥,ğ‘)contains an allocation rule ğ‘¥and payment rule ğ‘, both being
functions of the bids and pCTRs. Different from standard auction
design, the allocation rule ğ‘¥in our prominence-based auction mod-
ule does not directly specify the allocation but outputs the relative
prominence of each bidderâ€™s ad. This represents the relative impor-
tance the ad is supposed to get in the LLM generated summary, and
will be taken as input to the LLM module.
Specifically, we define Prom as an abstract space of prominences
which is the interface language between the auction and the LLM
modules â€“ it is the range space of the allocation function ğ‘¥and
the input space for the LLM. The auction also needs to know an
estimate of the click-through rate that each ad would get if it output
a particular PromâˆˆProm . This is enabled by the pCTR module
(Sec. 2.3), which gives the auction a map pctr :PromÃ—Z â†’[0,1]ğ‘›. The allocation function of the auction is now a function
ğ‘¥:BÃ—pctrâ†’Prom specifying the allocation of prominence. We
assume Prom has a well-defined tuple of order relations âª°:=(âª°1
,Â·Â·Â·,âª°ğ‘›), in whichâª°ğ‘–,âˆ€ğ‘–âˆˆ[ğ‘›]specifies the preference of each
bidderğ‘–over all possible relative prominences. The payment rule
ğ‘=(ğ‘1,Â·Â·Â·,ğ‘ğ‘›)specifies the (expected) payment for all bidders
given submitted bids, where ğ‘ğ‘–:BÃ— pctrâ†’Râ‰¥0maps the bid
profile to a non-negative payment.
Relative prominence is a general and abstract concept, as long
as it has a well-defined order relation âª°and the LLM can easily
follow its guidelines. For example, the prominence can represent
for each ad, a tuple of the space and the attractiveness of its creative.
In such a setting, the LLM could first follow the requirements of
the space and then generate the appropriately attractive creatives
using different multimodal formats for each ad. To simplify the
presentation and to formalize a concrete setting, we focus on a
simple structure of relative prominence throughout the rest of the
paper:
Definition 2.1. The Relative Prominence Prom is the set of points
Prom =(Prom 1,Â·Â·Â·,Prom ğ‘›), withÃ
ğ‘–âˆˆ[ğ‘›]Prom ğ‘–â‰¤1. Here, Prom ğ‘–
represents bidder ğ‘–â€™s allocated prominence.
2.2 LLM Module
The LLM module can be abstracted as a function to map the allo-
cated prominence of all ads to a summary. Formally, the LLM is
a function Gen :(Prom,Z)â†’S , whereSrepresents the set of
possible combined summaries of the ğ‘›bidders.
For the factorized system described in Figure 1 to work efficiently,
we would like the LLM module to satisfy the following property.
Definition 2.2 (Faithfulness). Given a set of input ad features
ğ‘§âˆˆZ and a set of input prominences PromâˆˆProm , the LLM output
summaryğ‘ =Gen(ğ‘§,Prom) should be such that when a user reads ğ‘ ,
then adğ‘–gets an amount of consideration proportional to Prom ğ‘–.
This definition is informal due to the absence of a mathematical
formulation of user consideration. However, the Faithfulness prop-
erty can be evaluated through testing on a panel of users or paid
evaluators. Informally, the property asks the LLM to implement
relative prominence instructions given by the auction. Note that
the faithfulness property implies (strict) monotonicity : as an adâ€™s
prominence increases, the user attention it gets also increases. We
will show in Sec. 3 that monotonicity is a sufficient condition to
make the system incentive compatible.
2.3 pCTR Module
As in classic ad auctions, we need a predicted click-through rate
(pCTR) module to allow the auction to make efficient allocation
decisions1. The CTR of each ad is not just a function of its intrin-
sic quality but is also modulated by its representation in the ad
summary generated by the LLM. This creates a difficulty in design-
ing the auctionâ€™s allocation function since the generated summary
is not known at auction time and the LLM is not a deterministic
function. We get around this in our factorized model by requiring
1Click-through prediction is required to enable per-click payment schemes. We note
that we can similarly incorporate other predictions such as conversion rates in our
framework.
 
715KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, & Di Wang
that the pCTR model only predict the average CTR for each ad
given the relative prominence Prom . Formally, the pCTR model
pctr :PromÃ—Zâ†’[ 0,1]ğ‘›maps the ads feature profile ğ‘§âˆˆZ and
the allocated prominence PromâˆˆProm to an average click-through
rate that takes the expectation over the randomness of the LLM
generation. Let pctrğ‘–:PromÃ—Zâ†’[ 0,1]be the average pCTR
model of bidder ğ‘–.
Given the ad summaries generated by LLM, let ctrllm:Sâ†’
[0,1]ğ‘›model the expected CTR for ğ‘›ads. Similarly, denote ctrllm
ğ‘–:
Sâ†’[ 0,1]as the expected CTR function of bidder ğ‘–.
Assumption 2.3 (Unbiased Estimation of pCTR Model). âˆ€ğ‘–âˆˆ
[ğ‘›],Prom =(Prom 1,Â·Â·Â·,Prom ğ‘›)âˆˆProm,ğ‘§âˆˆZ, we have,
pctrğ‘–(Prom,ğ‘§)= E
ğ‘ âˆˆS:ğ‘ âˆ¼Gen(Prom ,ğ‘§)
ctrllm
ğ‘–(ğ‘ )
Note that this unbiased estimation of pCTR model in auction
stage is important for our factorized model to work as intended,
given that the auction relies on this prediction model to make its
allocation and pricing decisions.
Given the above property, we are now ready to define the incen-
tive compatibility property of the factorized model end-to-end, i.e.,
truthfully reporting values are the optimal bidding strategies for
all bidders:
Definition 2.4 (Incentive Compatibility of the Factorized
Model). A factorized model with a prominence-based auction mech-
anismM=(ğ‘¥,ğ‘)and a LLM Gen is incentive compatible if and only
if,âˆ€ğ‘§âˆˆZ,âˆ€ğ‘–âˆˆ[ğ‘›],âˆ€ğ‘£ğ‘–,âˆ€ğ‘ğ‘–â‰ ğ‘£ğ‘–andâˆ€ğ‘âˆ’ğ‘–,
E
ğ‘ âˆ¼Gen(ğ‘¥(ğ‘£ğ‘–,ğ‘âˆ’ğ‘–),ğ‘§)
ctrllm
ğ‘–(ğ‘ )Â·ğ‘£ğ‘–âˆ’ğ‘ğ‘–(ğ‘£ğ‘–,ğ‘âˆ’ğ‘–)
â‰¥
E
ğ‘ â€²âˆ¼Gen(ğ‘¥(ğ‘ğ‘–,ğ‘âˆ’ğ‘–),ğ‘§)
ctrllm
ğ‘–(ğ‘ â€²)Â·ğ‘ğ‘–âˆ’ğ‘ğ‘–(ğ‘ğ‘–,ğ‘âˆ’ğ‘–)
,
which, using Assumption 2.3, is equivalent to,
pctrğ‘–(ğ‘¥(ğ‘£ğ‘–,ğ‘âˆ’ğ‘–),ğ‘§)Â·ğ‘£ğ‘–âˆ’ğ‘ğ‘–(ğ‘£ğ‘–,ğ‘âˆ’ğ‘–)â‰¥
pctrğ‘–(ğ‘¥(ğ‘ğ‘–,ğ‘âˆ’ğ‘–),ğ‘§)Â·ğ‘ğ‘–âˆ’ğ‘ğ‘–(ğ‘ğ‘–,ğ‘âˆ’ğ‘–).
Note that for simplicity on notation, we drop the dependence
of the allocation and payment functions ğ‘¥andğ‘on the pctr map.
Given this incentive compatibility property, each advertiser would
like to report their own value as bids. Therefore, for the rest of the
paper, we exchangeably use ğ‘ğ‘–andğ‘£ğ‘–to represent each advertiserâ€™s
value andğ‘ğ‘–âˆ¼Fğ‘–.
2.4 The model is a generalization of Position
Auctions
The model described in this section strictly generalizes the current
industry standard of Position Auctions [ 5,20]. This can be seen
by appropriately choosing the different model components and
variables as defined above to fit the setting of a simple position
auction (without any LLM in the loop). This is presented in Table 1
below. Besides showing that the Position Auction is a (very) special
case of our general model, the table may also help provide an
intuitive understanding of our abstract model.
In Section 4, we consider a more general instantiation of the
general model, which we call Dynamic Word Length Summary
(DWLS). In that case each of the above components are more general
that the position auction instantiation.General Model Instantiation for classic
(this paper) Position Auctions
Summary An ordered list ğ‘ of fixed ad creatives.
â€œLLM" Sort {Prom ğ‘–}ğ‘–âˆˆ[ğ‘›]and display the
ads in that order
ctrllm
ğ‘–(ğ‘ ) Average clicks for ad ğ‘–in its position in the list ğ‘ .
pctrğ‘–(Prom , ğ‘§)Predicted clicks for ad ğ‘–in the permutation Prom.
Table 1: Model instantiation for classic Position Auctions.
3 PROMINENCE-BASED AUCTION DESIGN
In this section, we consider the design problems in the factorized
prominence-based auctions. First, we provide conditions under
which we get incentive compatibility. Second, we show the fac-
torized model and above sufficient condition is indeed without
loss generality, by proving a â€œrevelation principle" type result. Fi-
nally, we discuss the welfare-maximizing auction design in general
setting.
3.1 Incentive Compatibility
Definition 3.1 (monotone Allocation). An prominence-based
auction mechanism M=(ğ‘¥,ğ‘)has a monotone allocation function
ifâˆ€ğ‘–âˆˆ[ğ‘›],ğ‘âˆ’ğ‘–andğ‘ğ‘–â‰¥ğ‘â€²
ğ‘–,ğ‘¥((ğ‘ğ‘–,ğ‘âˆ’ğ‘–))âª°ğ‘–ğ‘¥((ğ‘â€²
ğ‘–,ğ‘âˆ’ğ‘–)).
Definition 3.2 (monotonic LLM). An LLM Gen is said to be
monotonic ifâˆ€ğ‘–âˆˆ[ğ‘›],ğ‘§âˆˆZ,Prom,Promâ€², and Promâª°ğ‘–Promâ€²,
E
ğ‘ âˆˆS:ğ‘ âˆ¼Gen(Prom ,ğ‘§)
ctrllm
ğ‘–(ğ‘ )
â‰¥ E
ğ‘ âˆˆS:ğ‘ âˆ¼Gen(Promâ€²,ğ‘§)
ctrllm
ğ‘–(ğ‘ )
(1)
We next characterize the incentive compatibility of factorized
model.
Proposition 3.3 (Incentive Compatibility of Prominence-based
Auctions). Given a monotonic LLM (Def. 3.2) and an unbiased pctr
module (Def. 2.3), a prominence-based auction M=(ğ‘¥,ğ‘)is incentive
compatible (Def 2.4) if and only if
â€¢M has a monotonic allocation rule (Def. 3.1).
â€¢The payment rule ğ‘follows Myersonâ€™s Lemma [ 17], i.e.,âˆ€ğ‘–âˆˆ
[ğ‘›],ğ‘§âˆˆZğ‘›,ğ‘âˆˆB,
ğ‘ğ‘–(ğ‘)=ğ‘ğ‘–Â·pctrğ‘–(ğ‘¥(ğ‘),ğ‘§)âˆ’âˆ«ğ‘
0pctrğ‘–(ğ‘¥(ğ‘¦,ğ‘âˆ’ğ‘–),ğ‘§)ğ‘‘ğ‘¦, (2)
The proposition follows from the definitions and standard auc-
tion theory [ 17], and we omit the proof. Note that the proposition
leaves open the (impractical) possibility of an intricately woven
non-monotonic allocation and non-monotonic LLM pair which
nevertheless result in a monotonic end-to-end system.
In practice, it may be challenging to implement the exact pay-
ment due to the intrinsic complexity of integral in Eq. (2), however,
we can make the computation tractable through the discretization
of the outcome space with the cost of losing strict IC (similar to
GSP heuristics that are widely adopted in practice). Alternatively,
we can consider other payment rule, e.g., pay-your-bid auction
mechanism, which is beyond the scope of this paper.
3.2 Universality of the factorized model
We showed above that we obtain an incentive compatible factorized
system if both components are monotonic (and the pctr module is
 
716Auctions with LLM Summaries KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
unbiased). One natural question is whether there are other ways to
design an incentive compatible auction/LLM system, beyond the
factorized model in this paper. In this section, we prove a â€œrevelation
principle" type result to show that we can indeed focus on the
factorized model without loss of generality.
Consider any incentive compatible meta LLM-based mechanism
Mllm=(ğ‘¥llm,ğ‘llm)with an allocation rule ğ‘¥llm:BÃ—Zâ†’S ,
which maps the bid profile and their original ads contexts to ads
summaries directly. In addition, we assume Mllmis also scale-free:
Assumption 3.4 (Scale-freeness). A meta LLM-based mecha-
nismMllmis scale-free ifâˆ€ğ‘âˆˆB,ğ‘§âˆˆZ,ğ‘¥llm(ğ‘,ğ‘§)=ğ‘¥llm(ğ‘ğ‘,ğ‘§)
for any positive constant ğ‘.
The above scale-freeness property requires Mllmgenerates the
same ads summaries if all bids are scaled by a constant, which is a
natural assumption in practice.
Theorem 3.5. For any incentive compatible and scale-free meta
LLM-based mechanism Mllm=(ğ‘¥llm,ğ‘llm), there exists an incentive
compatible factorized model (having a prominence-based auction with
a monotonic allocation function, and a monotonic LLM Gen), which
achieves the same expected outcome as Mllm.
Proof. Consider any incentive compatible meta mechanism
Mllm=(ğ‘¥llm,ğ‘llm)with an allocation rule ğ‘¥llm:BÃ—Zâ†’S ,
which maps the bid profile (and their original contents) to ads
summaries. Now fix any prominence-based auction (ğ‘¥,ğ‘)with
allocationğ‘¥:BÃ—pctrâ†’Prom that is strictly monotonic (removing
the tie in Definition 3.1), and has an inverse ğ‘¦=ğ‘¥âˆ’1(up to scaling
factors). Note ğ‘¦is a function from a prominence to a bid profile. For
example, one can take ğ‘¥to be the proportional allocation rule. Now
we can simply construct an LLM function Gen=ğ‘¥llmâŠ™ğ‘¦, where
âŠ™represents the composition operator of two functions. Given
the scale-freeness of Mllm,Gen is well-defined. By construction,
the factorized model with the auction (ğ‘¥,ğ‘)and the LLM Gen is
identical to the given meta mechanism Mllm. The payments follow
from Myersonâ€™s lemma and are therefore also identical. Note that
since the allocation function ğ‘¥llmofMllmis monotonic (since
Mllmis given to be incentive compatible), and since we chose ğ‘¥to
be monotonic, Gen =ğ‘¥llmâŠ™ğ‘¦is also monotonic.
â–¡
3.3 Welfare Maximization
In this section, we consider the auction design problem to maximize
the total (expected) welfare of ad summaries generated by LLM,
defined as follows,
E
ğ‘âˆˆF,ğ‘§ï£®ï£¯ï£¯ï£¯ï£¯ï£°Ã•
ğ‘–âˆˆ[ğ‘›]E
ğ‘ âˆ¼Gen(ğ‘¥(ğ‘),ğ‘§)
ctrllm
ğ‘–(ğ‘ )
Â·ğ‘ğ‘–ï£¹ï£ºï£ºï£ºï£ºï£», (3)
where Eğ‘ âˆ¼Gen(ğ‘¥(ğ‘),ğ‘§)
ctrllm
ğ‘–(ğ‘ )
Â·ğ‘ğ‘–captures the expected value
achieved by LLM for a fixed bid profile ğ‘and the corresponding
allocation by the auction. Note, since the auction is incentive com-
patible, the bid is equal to the private value, therefore, ğ‘ğ‘–also rep-
resents the value of the bidder ğ‘–. Please note, the total expected
welfare doesnâ€™t depend on the payment.Proposition 3.6. Under Assumption 2.3 ( pctr unbiasedness), for
any incentive compatible prominence-based auction M=(ğ‘¥,ğ‘)and
monotone LLM Gen, we have,
E
ğ‘âˆˆF,ğ‘§ï£®ï£¯ï£¯ï£¯ï£¯ï£°Ã•
ğ‘–âˆˆ[ğ‘›]pctrğ‘–(ğ‘¥(ğ‘),ğ‘§)Â·ğ‘ğ‘–ï£¹ï£ºï£ºï£ºï£ºï£»
=E
ğ‘âˆˆF,ğ‘§ï£®ï£¯ï£¯ï£¯ï£¯ï£°Ã•
ğ‘–âˆˆ[ğ‘›]E
ğ‘ âˆ¼Gen(ğ‘¥(ğ‘),ğ‘§)
ctrllm
ğ‘–(ğ‘ )
Â·ğ‘ğ‘–ï£¹ï£ºï£ºï£ºï£ºï£».(4)
Thus, for a fixed monotone LLM Gen, the welfare-maximizing auction
mechanismM=(ğ‘¥,ğ‘)will also maximize the expected welfare of
final outcome generated LLM.
To maximize the total welfare, in general, we need to jointly
optimize the LLM Gen and the auction allocation ğ‘¥. The LLM can
be trained to provide better summaries to maximize clicks while
maintaining monotonicity, and the auction can be optimized to
provide better prominence allocation. However, the intrinsic com-
binatorial structure of this problem, e.g., the pCTR model depends
on the allocated prominence, makes this optimization problem hard
in its general setting. We instead focus on a simpler setting as a
case study in Sec. 4 where we can provide theoretical guarantees
on welfare maximization, and also provide empirical evidence for
the same in Sec. 5. We note that in this paper, we focus on wel-
fare maximization; designing revenue-optimal prominence-based
auction is an interesting future direction.
4 CASE STUDY: DYNAMIC WORD-LENGTH
SUMMARY (DWLS) AUCTIONS
The predominant UI for todayâ€™s online (text-based) advertising (e.g.,
sponsored search) is to display at most ğ‘˜(e.g. 4) text ads sequen-
tially in a position-based list [ 20] with the ordering determined
by an auction using candidates bids and pCTRs. Moreover, each
ad can come with a fixed set of optional formats (e.g. merchant
rating or phone number) to make the ad more informative, and
beyond the ordering, the auction can further optimize total welfare
with the available formats subject to total space constraint [ 1,3]. In
this section, we discuss a specific instantiation of our prominence-
based auction+LLM framework, and we denote it as the dynamic
word-lengh summary (DWLS). It is a straightforward and practi-
cal extension of todayâ€™s position-based advertising, designed as a
minimum example that retains the core elements of the generic
framework.
We consider showing up to ğ‘˜text ads subject to a limit on the
total number of words (denoted by ğ¿). In particular, the relative
prominence Prom ğ‘–for an adğ‘–would be the (intended) fraction of
theğ¿words allocated to the ad, i.e. at most Prom ğ‘–Â·ğ¿words (up
to rounding) for ad ğ‘–. We assume each ad ğ‘–as an input (i.e. the
original full-size text) has ğ‘›ğ‘–words, and if our auction allocates
Prom ğ‘–Â·ğ¿<ğ‘›ğ‘–words to it, we would use an LLM to compress
the original ad into a summary within Prom ğ‘–Â·ğ¿words. Note our
method, like the position-based UI, compresses ads individually
and puts their summaries one-by-one in order. This is a very viable
setup as existing off-the-shelf LLMs can readily perform single ad
text summarization with prompting.
 
717KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, & Di Wang
Under this setup, the auctionâ€™s task is to decide both the ordering
of the ads to be shown and the fraction of words allocated to each
ad. From a technical point of view, if we consider the length of
ad summary as an ad format, our setup generalized the existing
position-based auction with formats in the sense that we now have
access to a set of (continuous) formats dynamically generated by the
LLM instead of a fixed given set. The significance of this difference
on welfare-maximization is two-fold. Firstly, a much richer set of
formats greatly expands the feasible space we can optimize over,
and thus the optimal welfare would naturally be greater than the op-
timal welfare subject to the more restricted format space. Secondly,
a more continuous space can also make the mechanism design task
easier compared to the combinatorial case, e.g. analogous to solving
linear program versus integer program. For example, we show in
Section 4.2 under a fairly realistic CTR model, the (exact) welfare
maximizing auction becomes very simple, whereas in the combina-
torial format case, even approximate optimal auction design can be
quite challenging [1, 3].
In the rest of this section, we discuss the two modules in more
detail. In Section 4.1, we talk about the prompting strategy we could
use given the allocated prominence (i.e. word limit) to generate
good summaries such that the monotonicity property (3.2) holds.
In Section 4.2, we look at the auction design problem and show the
welfare-maximizing auction under a certain CTR model.
4.1 Prompting Strategy
LLMs are very effective in generating summaries that are preferred
by humans [ 13,26]. In DWLS we wish to summarize each ad sepa-
rately. We achieved our objective by few-shot Chain-of-Thought
[27] prompting. We provide the LLM with very few (3) hand crafted
examples of ads, number of words to summarize the ad by, and sum-
mary text. We also include key phrases mentioned in the advert as
intermediate steps, helping the LLM extract phrases corresponding
to the advert that are useful during summarization.
4.2 Auction Design
We consider welfare-maximizing auctions for DWLS, and we work
in a simplified case where we are given ğ‘›eligible ad candidates
(with their bids and pCTRs) and also the maximum number of
shown adsğ‘˜, so the auction needs to pick exactly min(ğ‘›,ğ‘˜)ads to
show, and decide the ordering as well as the relative prominence
(or equivalently the number of words) of the shown ads. As the
total welfare intricately depends on the CTRs of the ads in the way
they are shown to the user (i.e. compressed and shown along each
other), itâ€™s in general implausible to design welfare-maximizing
auctions without understanding the CTR. Thus, we start with a
fairly realistic CTR model, which is also a straightforward extension
of the widely-adopted CTR model in the position-based auction.
There are several CTR concepts in DWLS: (1)The base CTR
when the original ad (i.e. without compression) is shown to the user
without other ads. This is given as input to the auction, and in this
section we denote it by pctrğ‘–for an adğ‘–.(2)The CTR when the origi-
nal ad is shown alongside with other ads, and we denote it by pctrpos
ğ‘–.
This CTR would naturally decrease for an ad when itâ€™s shown after
other ads, so it depends on the auctionâ€™s outcome of ad ordering. (3)The CTR of the adâ€™s summary (i.e. after compression) shown along-
side other ads. This further depends on the summarization quality,
and we denote it by pctrfinal
ğ‘–. Note pctrfinal
ğ‘–would be the CTR in
the calculation of welfare for a shown ad ğ‘–, i.e. the pctrğ‘–(ğ‘¥(ğ‘),ğ‘§)
term in Equation (4). These variants are analogous to the position-
based case where there are known as position-1 pCTR, position-
normalized pCTR and position-normalized-formatted pCTR respec-
tively.
Similar to CTR models in practice for the position-based case,
we consider a factorized model where pctrpos
ğ‘–=pctrğ‘–Â·pos_norm ğ‘–
with pos_norm ğ‘–being a discount factor for the position (i.e. order)
of adğ‘–in the list of shown ads. The position discount factor only
depends on the position and is provided as input to the auction as
ğ‘˜fixed numbers (e.g. 1.0,0.9,0.81,...) for the positions respectively.
The factors are in[0,1]and decreases for later positions. Moreover,
pctrfinal
ğ‘–would be pctrpos
ğ‘–further multiplied by a compression dis-
count factor capturing the summarization quality. Since we focus on
auction design, our CTR model in this section doesnâ€™t explicitly con-
sider the LLM quality, so we assume the LLM can faithfully follow
the instructed word limit and do equally well at summarizing any
given ad conditioned on the number of allocated words, and thus
the compression discount factor is (only) a function on the number
of words. Technically this means pctrfinal
ğ‘–=pctrpos
ğ‘–Â·ğ‘“(Prom ğ‘–)for
some fixed function ğ‘“(Â·)since Prom ğ‘–is (up to scaling by ğ¿) equiva-
lent to number of words allocated to ad ğ‘–. In the rest of the section,
we focus on the family of ğ‘“(Prom ğ‘–)=Promğ›½
ğ‘–forğ›½âˆˆ(0,1]and
characterize the welfare-maximizing auction in this setting. Note
asProm ğ‘–âˆˆ[0,1], such family of polynomial functions naturally
serve as a compression discount factor in [0,1]. For simplicity, itâ€™s
helpful to think of our CTR model in a setting where the full length
of all ads are roughly the same, and are all much longer than the
number of available words to show ad summaries.
Formally speaking, given a set of ğ‘›ad candidates, we denote
Â®ğ‘as the vector of bids with bğ‘–being the bid of ad ğ‘–,âˆ’âˆ’â†’pctr as the
vector of input (base) CTRs, and ğ‘Ÿ1â‰¥...â‰¥ğ‘Ÿğ‘˜as the position
discount factor of the ğ‘˜ad positions respectively. In DWLS, we
write the auctionâ€™s allocation explicitly as two vectorsâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm
and Prom , whereâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm captures which (up to ğ‘˜) ads to show
and their position (i.e. ordering) and Prom is the vector of relative
prominence, which lies in the ğ‘›-dimensional simplex. The ğ‘–-th entry
ofâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm would beğ‘Ÿğ‘¡if adğ‘–is picked by the auction to show at
positionğ‘¡âˆˆ[1,...,ğ‘˜]or0if not picked. The total welfare is
(Â®ğ‘âŠ™âˆ’âˆ’â†’pctrâŠ™âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm)Â· ğ‘“(Prom)
whereâŠ™denotes entry-wise multiplication, Â·is dot product, and
ğ‘“(Prom) is the vector of applying ğ‘“(Â·)to each entry of Prom.
It becomes clear from the welfare formulation that the factorized
CTR model allows the auction to pick an optimal Prom conditioned
onâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm (i.e. the shown ads and their positions), and we first
characterize the optimal Prom allocation rule.
Definition 4.1 (Generalized Proportional Allocation). Given
ğ‘›ads withÂ®ğ‘,âˆ’âˆ’â†’pctr,âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm , and denoteâˆ’âˆ’âˆ’â†’ecpm as the vector with
ecpmğ‘–=bğ‘–Â·pctrğ‘–Â·pos_norm ğ‘–, the generalized proportional allocation
 
718Auctions with LLM Summaries KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
with parameter ğ›¼allocates as follows
Prom ğ‘–=(ecpmğ‘–)ğ›¼
Ãğ‘›
ğ‘—=1(ecpmğ‘—)ğ›¼,âˆ€ğ‘–=1,...,ğ‘› (5)
Note in DWLS there will be exactly min(ğ‘›,ğ‘˜)non-zero entries in
âˆ’âˆ’âˆ’â†’ecpm.
We will show that the welfare-maximizing auction correspond-
ing to any CTR model we consider with ğ‘“(Prom ğ‘–)=Promğ›½
ğ‘–for
someğ›½âˆˆ(0,1]would use a generalized allocation rule with ap-
propriately chosen ğ›¼. As a special case, consider when ğ›½=1so
ğ‘“(Prom ğ‘–)=Prom ğ‘–and we want to maximizeâˆ’âˆ’âˆ’âˆ’â†’ecpmÂ·Prom . Since
Prom lies in theğ‘›-dimensional simplex, the optimal Prom is to put
weight only on the ad(s) in the set of argmaxğ‘–âˆˆ[ğ‘›]ecpmğ‘–, and this
correspond to the generalized proportional allocation with ğ›¼=âˆ.
This is a fairly trivial case, and also a less realistic example in our
family of CTR models. In particular, the ğ‘“(Â·)function in a more
realistic CTR model would capture a diminishing return phenome-
non w.r.t. prominence (or equivalently the number of words), i.e.,
the marginal improvement of CTR vanishes as we add more and
more words to the summary. For example, the CTR increase from
a10-word ad summary to a 20-word summary is typically much
higher than the CTR increase from 40to50words. Technically, a
strictly concave ğ‘“(Â·)would capture this effect, which corresponds
toğ›½âˆˆ(0,1)with stronger diminishing return effect for smaller ğ›½.
Theorem 4.2. Whenğ‘“(Prom ğ‘–)=Promğ›½
ğ‘–forğ›½âˆˆ (0,1)in our
factorized CTR model, the optimal Prom conditioned onâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm
follows the generalized proportional allocation with ğ›¼=1/(1âˆ’ğ›½),
and the optimal welfare conditioned onâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm isâˆ¥âˆ’âˆ’âˆ’â†’ecpmâˆ¥ğ›¼, i.e. the
ğ›¼-norm of the vectorâˆ’âˆ’âˆ’â†’ecpm in Definition 4.1.
Proof. Consider the vectorâˆ’âˆ’âˆ’âˆ’â†’ecpm with ecpmğ‘–=bğ‘–Â·pctrğ‘–Â·
pos_norm ğ‘–, and denoteÂ®ğ‘¥as the vector ğ‘“(Prom) (i.e.ğ‘¥ğ‘–=Promğ›½
ğ‘–).
The constraint that Prom lies in the simplex is equivalent to the
ğ‘-norm ofğ‘¥being 1forğ‘=1/ğ›½, and the optimization of Prom is
equivalent to maximize Â®ğ‘¥Â·âˆ’âˆ’âˆ’âˆ’â†’ecpm subject toâˆ¥ğ‘¥âˆ¥ğ‘=1. By HÃ¶lderâ€™s
inequality, we know Â®ğ‘¥Â·âˆ’âˆ’âˆ’âˆ’â†’ecpmâ‰¤âˆ¥ğ‘¥âˆ¥ğ‘âˆ¥âˆ’âˆ’âˆ’âˆ’â†’ecpmâˆ¥ğ‘=âˆ¥âˆ’âˆ’âˆ’âˆ’â†’ecpmâˆ¥ğ‘where
ğ‘is the dual-norm, i.e. ğ‘=1/(1âˆ’1/ğ‘)=1/(1âˆ’ğ›½). Sinceâˆ¥âˆ’âˆ’âˆ’âˆ’â†’ecpmâˆ¥ğ‘
is fixed, the optimal welfare is achieved with the Â®ğ‘¥making the
inequality an equality. Since ğ‘,ğ‘âˆˆ(1,âˆ), we can explicitly write
theÂ®ğ‘¥that gives the equality case in HÃ¶lderâ€™s inequality, which is
ğ‘¥ğ‘
ğ‘–=(ecpmğ‘–)ğ‘/Ã
ğ‘—âˆˆ[ğ‘›](ecpmğ‘—)ğ‘. Since Prom ğ‘–=ğ‘¥ğ‘
ğ‘–, we get the
generalized proportional allocation with ğ›¼=ğ‘=1/(1âˆ’ğ›½).â–¡
With the above theorem, choosing the optimalâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm in the
welfare-maximizing auction becomes straightforward. As the op-
timal welfare conditioned onâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm is exactlyâˆ¥âˆ’âˆ’âˆ’âˆ’â†’ecpmâˆ¥ğ›¼, we
should pick the ads and their ordering to maximize âˆ¥âˆ’âˆ’âˆ’âˆ’â†’ecpmâˆ¥ğ›¼. For
anyğ›¼âˆˆ (1,âˆ)andâˆ’âˆ’âˆ’âˆ’â†’ecpm =bâŠ™âˆ’âˆ’â†’pctrâŠ™âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm , the optimal
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’pos_norm is clearly incurred by picking the top min(ğ‘›,ğ‘˜)ads with
the highest product of bid and pCTR and show them in the same or-
der according to that product. This completes the optimal allocation
of the welfare-maximizing auction. It is straightforward to check
this allocation rule is monotone since both pos_norm ğ‘–and Prom ğ‘–
are monotonic in bğ‘–. Since we consider IC mechanisms, the pricingrule follows from Myersonâ€™s Lemma, which is also straightforward
to calculate in our factorized CTR model.
We note that it may not be immediately clear from the way
we present DWLS that itâ€™s a strict instantiation of our general
prominence-based auction +LLM model. To cast it in our model,
note the auction only need to output the relative prominence Prom
as in the general model, and the set of shown ads as well as their
ordering can be inferred from Prom . That is, the LLM in our general
model would be a meta-LLM in DWLS that takes in the Prom vector,
finds the (up to ğ‘˜) ads with non-zero weights in Prom , then calls
a real LLM to compress the ads one-by-one separately with the
respective word limits according to Prom , and arrange the individual
ad summaries in the same order as their weights in Prom.
5 EXPERIMENTAL RESULTS
In this section, we present an empirical study on a synthetic data
set for the DWLS setting. Through these experiments we first verify
the validity of our proposed framework to generate ad summaries
with the auction and LLM framework. We then demonstrate the
efficiency of our mechanism, i.e., our generalized proportional allo-
cation rule and prompting strategy to the LLM can provide more
efficient allocation compared with some simple benchmarks. The
LLMs we used to generate synthetic ads data and perform summa-
rization task are both Gemini Pro.
5.1 Data Generation
Using a large LLM (Gemini Pro), we produce a set of synthetic and
anonymous advertisements. For each generated query, the LLM
generates between two and four ads. For example, we show an
example contains 3 synthetic ads generated by a LLM for the query
â€œLearning golf " in Table 2. Following the standard assumption in
ad auctions (e.g., [ 19]), we assume the bid ğ‘ğ‘–of each adğ‘–follow a
log-normal distribution LogNormal(0.5,1). In addition, the click-
through rate ğ¶ğ‘‡ğ‘… ğ‘–of each adğ‘–is independently and identically
(i.i.d) sampled from a uniform distribution Unif[0,1]. Note the
ğ¶ğ‘‡ğ‘… ğ‘–is the base click-through rate of ad ğ‘–if it is shown to the user
solely.
5.2 Evaluation Model
To evaluate the real clicks (and thus welfare), we need user feed-
back from a live production system. This requires deploying our
proposed framework in real online advertising production to get
human evaluation, which is beyond the scope of this work. For
the purpose of evaluation in this paper, we propose a synthetic
evaluation model. In particular, we first define the CTR function
used to evaluate the performance of different mechanisms,
Definition 5.1 (Click-through Rate function for evaluat-
ing welfare). For any ad summaries ğ‘ âˆˆS generated by LLM and
the associated original contexts ğ‘§, the CTR of each ad ğ‘–âˆˆ[ğ‘›]is taken
to be
ğ¶ğ‘‡ğ‘… ğ‘–(ğ‘ ,ğ‘§)=ğ¶ğ‘‡ğ‘… ğ‘–Â·ğ‘“ğ‘–(ğ‘ ,ğ‘§)Â·ğ‘›ğ‘œğ‘Ÿğ‘š ğ‘–(ğ‘ ), (6)
whereğ¶ğ‘‡ğ‘… ğ‘–is the base click-through rate of ad ğ‘–,ğ‘“ğ‘–(ğ‘ ,ğ‘§)models
the CTR multiplier due to the summary quality of each ad ğ‘–as a
function of the summaries ğ‘ and the original contexts ğ‘§, andğ‘›ğ‘œğ‘Ÿğ‘š ğ‘–(ğ‘ )
quantifies the UI normalizer for each ad ğ‘–in the summary.
 
719KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, & Di Wang
Generated Ad Generated Summary A Generated Summary B
<url1> Feeling intimidated by golf? Our beginner-friendly clinics offer a relaxed environment
to learn fundamentals and etiquette. Get comfortable with the swing, chipping, and putting
in a supportive group setting. No experience needed, just bring your enthusiasm!<url1> Golf clinics for beginners!
Learn basics, etiquette, & practice
swing, chipping, putting in a re-
laxed group setting. No experience
required, just bring your enthusiasm!
<url2> Master golf at home! PGA Pro
video lessons offer swing analysis,
drills & coaching. Learn on your own
schedule.
<url3> Fun, safe learning! Build skills,
sportsmanship, & friendships. Expert
coaching ignites love for the game.<url1> Conquer your golf fears! Our
beginner clinics in a relaxed setting
teach you the basics and proper eti-
quette. Learn swing, chipping, and
putting with supportive peers. No
experience required, just bring your
enthusiasm!
<url2> Level up your golf game
from home! Get personalized video
lessons with PGA pros. Master
swing, drills, and course strategy at
your own pace.<url2> Busy schedule? Learn from the comfort of your couch! Connect with experienced
PGA pros for tailored video lessons. Get swing analysis, drills, and course strategy guidance
at your own pace. Achieve your golfing goals on your own terms.
<url3> Spark a lifelong love for the game! Our programs provide a fun and safe space to learn
golf basics, develop sportsmanship, and make new friends. Qualified instructors guide young
golfers through proper techniques and etiquette, fostering a passion for the green.
Table 2: Synthetic ads example generated by LLMs for the query â€œLearning golfâ€ and the output summarized by LLM in ac-
cordance with the prominence provided by GPA. We used ğ›¼=2and total word length= 60. For summary A, the input ecpms
to the auction (bid times base pctr) were [0.645, 0.641, 0.617], the relative prominence output by the auction was [0.417, 0.333,
0.250] implying a target word length distribution of [25, 20, 15]. For summary B, ecpm=[0.764, 0.710, 0.113], the relative promi-
nence=[0.583, 0.409, 0.007] and the target word lengths=[35, 25, 0].
In the DWLS setting, ğ‘›ğ‘œğ‘Ÿğ‘š ğ‘–(Â·)is simply a position discount fac-
tor, i.e.,ğ‘›ğ‘œğ‘Ÿğ‘š ğ‘–(Â·)only depends on the rank of ad ğ‘–â€™s summary in
ğ‘ and this UI normalizer will be lower when the its rank is lower.
We use functionals of the ROUGE metric [ 12] to quantify summary
qualityğ‘“ğ‘–(ğ‘ ,ğ‘§). At a high level, the (variant of) ROUGE metric we
use is a score between 0and1, and captures both the length of
the summary ğ‘ and the relevance between ğ‘ andğ‘§. As the LLM
mostly can summarize very well, the main factor for the ROUGE
score in our case becomes the length we allocate to ğ‘ , and thus the
ROUGE and relative prominence roughly follow a linear relation-
ship. Consequently, the evaluation model aligns qualitatively with
our theoretical model in Section 4 when we take the ğ‘“ğ‘–(ğ‘ ,ğ‘§)to be
some concave polynomial function of ROUGE.
5.3 Benchmarks
We compare the proposed generalized proportional auction mecha-
nism against two natural baseline auction mechanisms, one that
doesnâ€™t utilize the power of LLMs for summarization, and the other
which doesnâ€™t involve optimizing the auction design.
â€¢Greedy Auction : Ads are shown in a list respecting the ğ¶ğ‘‡ğ‘… ğ‘–Â·ğ‘ğ‘–
ranking. The original creatives of ads are shown (i.e. with no
summarization) until we run out of space. If an ad doesnâ€™t fit
in the remaining space then it cannot be shown. This baseline
evaluates the performance of an auction mechanism that doesnâ€™t
utilize the power of LLMs to summarize.
â€¢Position Auction with Fixed Length : Ads are shown in a summary
paragraph which gives equal number of words for each ad. This
baseline evaluates the performance of LLMs to summarize, but
without optimizing the auction design.
We compare these baselines against generalized proportional auc-
tion with an appropriate ğ›¼(defined below) combined with the LLM
output ("GPA+LLM") by comparing the value of average welfare
realized by each of these approaches.5.4 Set-up
First, we generate 1000 random queries. Then, we use the data gen-
eration method from Section 5.1 to generate synthetic and anony-
mous ad creatives for each query. Throughout the experimental
section, we set the ğ‘›ğ‘œğ‘Ÿğ‘š ğ‘–(ğ‘ )=0.9(ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘–(ğ‘ )âˆ’1), whereğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘–(ğ‘ )is
the rank of ad ğ‘–â€™s summary in ğ‘ . We vary function ğ‘“ğ‘–in our ex-
perimental results. Specifically, we set ğ‘“ğ‘–(ğ‘ ,ğ‘§)=(ROUGE(ğ‘ ğ‘–,ğ‘§ğ‘–))ğ›½
where we set ğ›½to1/2,1/3 & 1/4. Motivated by Theorem 4.2 we
chooseğ›¼=1
1âˆ’ğ›½in our generalized proportional auction. We note
in practice when ğ‘“ğ‘–(ğ‘ ,ğ‘§)is not known, it is conceivable that some
polynomial function with ğ›½âˆˆ(0,1)qualitatively approximates it
well. Thus generalized proportional auction with the corresponding
(but unknown) ğ›¼still has good performance, and it is very practical
to tune for a good ğ›¼.
5.5 Results
5.5.1 Qualitative. In table 2, we present a qualitative result through
a sample (end to end) instantiation of our framework for two differ-
ent sets of bids. In each setting, the bids and pctrs are converted by
the auction to prominence values (number of words), and the LLM
faithfully generates a summary based on the prominence. We note
how the first ad gets a significantly larger fraction of the space in
the second (skewed) setting of bids.
5.5.2 Efficiency. In figure 2, we compare the two baselines, namely
the position auction with fixed length ("POS-FL") and the greedy
auction against GPA+LLM with ğ›¼=1
1âˆ’ğ›½. Note that the behavior
of the two baselines, Greedy and Position Auction, do not depend
on the value of ğ›½. Furthermore, the reward for Greedy also does
not depend on ğ›½because if it shows an ad then it shows the entire
ad text yielding a ROUGE score of 1. We observe from figure 2
that GPA+LLM does better than both the baselines showcasing
the value of utilizing the power of LLM along with optimizing the
auction allocation. As the total number of words increases, the
baselines catch up to the proposed GPA+LLM approach since the
LLM is largely not required for summarizing the creatives, as all the
original ad creatives can fit in many cases. Comparing across the
 
720Auctions with LLM Summaries KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 2: Total welfare with different choices of total num-
ber of words for summarization.
two baselines, when the total number of words is small, then greedy
often leaves space unused since it can only show an ad creative
in its entirety (since it does not use LLM to resize ads) and hence
performs worse than position auction which does resize ads. When
the total number of words is larger, then greedy starts doing better
than position auction, because it is often better to split the space
between fewer ads (e.g., two ads when the third adâ€™s bid is low).
6 CONCLUSIONS AND FUTURE DIRECTIONS
This paper develops a factorization that enables auction based allo-
cation for general LLM-based summarizations. The paper studies
an instantiation that shows the near-optimality of the generalized
proportional auction under a certain class of parameterized click
through rate models for the LLM-generated summaries. While the
empirical section in the paper works with a fixed LLM and the
dynamic word-length summarization interface, it is worth looking
into the use of finetuning to work in conjunction with appropriate
auction design to adapt to user behavior on other classes of flexible
user interfaces.
REFERENCES
[1]Gagan Aggarwal, Kshipra Bhawalkar, Aranyak Mehta, Divyarthi Mohan, and
Alexandros Psomas. 2022. Simple mechanisms for welfare maximization in rich
advertising auctions. Advances in Neural Information Processing Systems 35 (2022),
28280â€“28292.
[2]Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi,
Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr
Nyczyk, et al .2023. Graph of thoughts: Solving elaborate problems with large
language models. arXiv preprint arXiv:2308.09687 (2023).
[3]Ruggiero Cavallo, Prabhakar Krishnamurthy, Maxim Sviridenko, and Christo-
pher A. Wilkens. 2017. Sponsored Search Auctions with Rich Ads. In Proceedings
of the 26th International Conference on World Wide Web (Perth, Australia) (WWW
â€™17). International World Wide Web Conferences Steering Committee, Republic
and Canton of Geneva, CHE, 43â€“51. https://doi.org/10.1145/3038912.3052703
[4]Paul Duetting, Vahab Mirrokni, Renato Paes Leme, Haifeng Xu, and Song Zuo.
2023. Mechanism Design for Large Language Models. arXiv:2310.10826 [cs.GT]
[5]Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. 2007. Internet
Advertising and the Generalized Second-Price Auction: Selling Billions of DollarsWorth of Keywords. American Economic Review 97, 1 (March 2007), 242â€“259.
[6]Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang
Tang, and Qing Li. 2023. Recommender systems in the era of large language
models (llms). arXiv preprint arXiv:2307.02046 (2023).
[7]Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, and Suho Shin.
2023. Online Advertisements with LLMs: Opportunities and Challenges.
arXiv:2311.07601 [cs.CY]
[8]Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.
Recommendation as language processing (rlp): A unified pretrain, personalized
prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on
Recommender Systems. 299â€“315.
[9]Google. 2023. Supercharging Search with generative AI. https://blog.google/
products/search/generative-ai-search/.
[10] Jason D Hartline and Tim Roughgarden. 2009. Simple versus optimal mechanisms.
InProceedings of the 10th ACM conference on Electronic commerce. 225â€“234.
[11] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in
neural information processing systems 35 (2022), 22199â€“22213.
[12] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.
InText Summarization Branches Out. Association for Computational Linguistics,
Barcelona, Spain, 74â€“81. https://aclanthology.org/W04-1013
[13] Yixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han,
Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, et al .2022. Revisiting
the gold standard: Grounding summarization evaluation with robust human
evaluation. arXiv preprint arXiv:2212.07981 (2022).
[14] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,
Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
et al.2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint
arXiv:2303.17651 (2023).
[15] Aman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective
chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686 (2022).
[16] Microsoft. 2023. AI powered Bing. https://blogs.microsoft.com/blog/2023/02/07/
reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-
copilot-for-the-web/.
[17] Roger B Myerson. 1981. Optimal auction design. Mathematics of operations
research 6, 1 (1981), 58â€“73.
[18] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Ja-
cob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
Luan, et al .2021. Show your work: Scratchpads for intermediate computation
with language models. arXiv preprint arXiv:2112.00114 (2021).
[19] Michael Ostrovsky and Michael Schwarz. 2011. Reserve prices in internet adver-
tising auctions: A field experiment. In Proceedings of the 12th ACM conference on
Electronic commerce. 59â€“60.
[20] Hal R. Varian. 2006. Position auction. International Journal of Industrial Organi-
zation (2006).
[21] William Vickrey. 1961. Counterspeculation, auctions, and competitive sealed
tenders. The Journal of finance 16, 1 (1961), 8â€“37.
[22] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee,
and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-
of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091
(2023).
[23] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain
of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).
[24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
Quoc V Le, Denny Zhou, et al .2022. Chain-of-thought prompting elicits reasoning
in large language models. Advances in Neural Information Processing Systems 35
(2022), 24824â€“24837.
[25] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao,
and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving
with large language models. arXiv preprint arXiv:2305.10601 (2023).
[26] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and
Tatsunori B Hashimoto. 2024. Benchmarking Large Language Models for News
Summarization. Transactions of the Association for Computational Linguistics 12
(2024), 39â€“57.
[27] Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al .2022. Least-to-
most prompting enables complex reasoning in large language models. arXiv
preprint arXiv:2205.10625 (2022).
APPENDIX
A PROMPT USED TO GENERATE SYNTHETIC
DATA
The prompt we used to generate synthetic ads data is as below:
 
721KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, & Di Wang
Generate one to four ads randomly for the following query on the
same topic. Each ad should be on the same topic. Each ad should be
adjacent with the description of this ad with at most 40 words. The
ads should be different from each other. Stop after already generating
four ads. Do not continue to generate after a \ğ‘›character.Original: {original}
Ads:
where the "original" will be replaced by 1000 randomly-generated
commercial queries.
 
722