Practical Single Domain Generalization via Training-time and
Test-time Learning
Shuai Yang
School of Information and Artificial
Intelligence
Anhui Agricultural University
Hefei, Anhui, China
yangs@ahau.edu.cnZhen Zhang
School of Information and Artificial
Intelligence
Anhui Agricultural University
Hefei, Anhui, China
zhangz@stu.ahau.edu.cnLichuan Guâˆ—
School of Information and Artificial
Intelligence
Anhui Agricultural University
Hefei, Anhui, China
glc@ahau.edu.cn
ABSTRACT
Single domain generalization aims to learn a model that generalizes
well to unseen target domains by using a related source domain.
However, most existing methods only focus on improving the gen-
eralization performance of the model during training, making it
difficult to achieve satisfactory performance when deployed in the
target domain with large domain shifts. In this paper, we propose a
Practical Single Domain Generalization (PSDG) method, which first
leverages the knowledge in a source domain to establish a model
with good generalization ability in the training phase, and subse-
quently updates the model to adapt to target domain data using
knowledge in the unlabeled target domain during the testing phase.
Specifically, during training, PSDG leverages a newly proposed
style (e.g., background features) generator named StyIN to gener-
ate novel domain data. Moreover, PSDG introduces style-diversity
regularization to constantly synthesize distinct styles to expand the
coverage of training data, and introduces object-consistency regu-
larization to capture consistency between the currently generated
data and the original data, making the model filter style knowl-
edge during training. During testing, PSDG uses a sample-aware
and sharpness-aware minimization method to seek for a flat en-
tropy minimum surface for further model optimization by using the
knowledge in the unlabeled target domain. Using three real-world
datasets the experiments have demonstrated the effectiveness of
PSDG, in comparison with several state-of-the-art methods.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning; Transfer
learning; Learning latent representations.
KEYWORDS
Domain generalization, Data augmentation, Test-time adaptation,
Representation learning
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671806ACM Reference Format:
Shuai Yang, Zhen Zhang, and Lichuan Gu. 2024. Practical Single Domain
Generalization via Training-time and Test-time Learning. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671806
1 INTRODUCTION
Deep neural networks (DNNs) have achieved dramatic successes
in a wide range of applications [ 9,17,20]. However, these suc-
cesses rely on the assumption that the training (source domain)
data share the same distribution as the test (target domain) data.
In practice, such the assumption is often violated, since the target
domain data inevitably encounter natural variations or corruptions
such as changes in weather, scenes and sensor devices, which are
termed domain shifts. Unfortunately, DNNs often suffer from severe
performance degradation even slight domain shifts [5].
To tackle this issue, one branch of work focuses on multi-source
domain generalization, which uses only data from multiple source
domains to train a model without accessing the target domain data
[31,34,41,44]. Nevertheless, in the wild environments, acquir-
ing sufficient training data from multiple source domains is often
impractical due to data collection budgets [ 38]. As an alternative,
single domain generalization has been presented [ 15,35], with the
aim at using only a single-source domain data to train a robust
model that generalizes well to unseen target domains.
The key idea to address single domain generalization is to miti-
gate the distribution discrepancies between the source domain and
unseen target domains. In practical applications, the style features,
such as background features, are unstable across different domains.
In contrast, the content features, such as the object features, are
stable regardless of how the environment changes [ 40]. Therefore,
the core of single domain generalization is to eliminate spurious
correlations between style features and labels. Recently, numer-
ous methods for single domain generalization have been proposed
[7,15,28,35], which mainly focus on increasing the capacity of the
training data by synthesizing novel domains, and thus weaken the
attention of the model on style features due to the increase of style
diversity of training data. Existing single domain generalization
methods can be generally grouped into two different types. The first
type of method generates diverse samples by using back-propagated
gradients to perturb original samples, such as ADA [ 28], M-ADA
[24], and NCDG [ 26]. However, gradient-based perturbations are
visually imperceptible and thus these methods cannot simulate
real-world domain shifts. To alleviate this problem, the second type
3794
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Shuai Yang, Zhen Zhang, & Lichuan Gu
Figure 1: (a), (b), (c), and (d) are examples from the Photo,
Art painting, Cartoon, and Sketch domains from the PACS
dataset, respectively.
of method uses convolutional neural networks to synthesize new
style data, such as PDEN [15], L2D [35], and UDP [7].
Previous methods have made tremendous progress in single
domain generalization, but three limitations are still existed. First,
these methods pay little attention to the inter-domain discrepancies
between the currently generated data and the previously gener-
ated data, and thus cannot well constantly generate new style data,
resulting in a limited range of styles. Second, since the object fea-
tures of the original data and the corresponding generated data are
consistent, learning consistent features between them can enhance
the robustness of feature representations. However, existing meth-
ods either use contrastive learning or directly mix the generated
and original data together to learn feature representations, there
is still room for further capturing consistency in learning feature
representations. Third, these methods only focus on improving
the generalization ability of the model during training. In prac-
tice, the slight content shift is ubiquitous. For instance, the PACS
dataset contains four domains, i.e., Photo, Art painting, Cartoon,
and Sketch, obviously, the shapes of objects in the four domains
are slightly different, as shown in Fig. 1. However, these methods
are difficult to accurately classify target domain data when slight
content shifts occur [ 10,39], and thus relying solely on a model
learned during the training phase is inadequate.
Accordingly, a question naturally arises: can we boost the gen-
eralization performance of model in both the training and testing
phases? That is, in the training phase, can we further improve the
style diversity of training data and feature invariance to enhance
the generalization capability of the model? Whereas in the testing
phase, can we further optimize the model by using knowledge in
the unlabeled target domain?
Motivated by the aforementioned issues, we propose a Practical
Single Domain Generalization (PSDG) method, which performs
both training-time and test-time learning to boost the modelâ€™s
performance on the target domain. Specifically, in the training
phase, PSDG incorporates a style diversity module with an innova-
tive StyIN generator for synthesizing new style data. Additionally,
style-diversity regularization is incorporated to encourage newly
generated data to be as far away from existing data as possible in fea-
ture and pixel space. Moreover, PSDG introduces a representation
learning module that employs object-consistency regularization
to capture invariance between the currently generated data and
the original data. PSDG constantly synthesizes multiple fresh do-
mains with distinct styles by iteratively optimizing the two modules
to filter domain-specific knowledge, promoting the generalizationperformance of the model. In the testing phase, PSDG incorpo-
rates sample-aware and sharpness-aware minimization (SAM) [ 6]
method, which combines confidence-based sample reweighting and
sharpness-based optimization to pursue a flat entropy minimum
surface for further model optimization by leveraging the specific
knowledge in the target domain. Our main contributions are sum-
marized as follows:
â€¢We propose a PSDG algorithm that performs both training-
time and test-time learning to improve the performance on
an arbitrary target domain following a different distribution,
which is more practical than conventional single domain
generalization methods of only training-time learning.
â€¢To boost the generalization ability of the model during train-
ing, PSDG uses a style diversity module with a novel style
generator StyIN and a representation learning module with
object-consistency regularization for continuous generation
of new style data to filter style knowledge.
â€¢To make the model learned in the training phase adapt to tar-
get domain data during testing, PSDG combines confidence-
based sample reweighting and sharpness-based optimization
to find a flat entropy minimum surface for model optimiza-
tion by using knowledge in the target domain.
â€¢We perform extensive experiments using three public single
domain generalization datasets, and compare PSDG with
several state-of-the-art methods to validate its effectiveness.
2 RELATED WORK
Single domain generalization. The goal of single domain general-
ization is to learn a model with good generalization performance by
leveraging the knowledge in a single source domain during training.
Previous works mainly focus on expanding and diversifying the
distribution of training data by augmenting source domain data,
and they generally fall into two distinct types. The first type of
method performs adversarial data augmentation. Representatively,
ADA [ 28] disturbs original samples using back-propagated gradi-
ents obtained from the classification loss to generate new samples
with the same semantic information as the original ones. Guided
by this work, M-ADA [ 24] incorporates Wasserstein Auto-encoders
to enlarge the distance between the generated and original data
in the raw feature space, and learns feature representations with
good generalization ability via meta-learning. Along with ADA and
M-ADA, ASR-Norm uses neural networks to learn both standard-
ization and rescaling statistics, adapting them to different domains
of data. ME-ADA [ 42] generates new domains with large domain
shifts by increasing the mutual information of the source and gen-
erated data. Later on, NCDG [ 26] simultaneously maximizes the
neuron coverage of deep neural networks and the gradient sim-
ilarity between the generated and original data to enhance the
generalization performance. AdvST [ 43] augments the source do-
main data through semantic transformation, resulting in semantic
changes and new styles with significant variations.
The second type of method uses convolutional neural networks
as generators to synthesize novel domain data. For instance, PDEN
[15] progressively generates multiple domains to simulate photo-
metric and geometric transforms in unseen domains by using a
progressive domain expansion network. However, PDEN relies on
3795Practical Single Domain Generalization via Training-time and Test-time Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
two generators to preserve semantic information. L2D [ 35] intro-
duces a style-complement module to generate new style data and
optimizes them by minimizing the mutual information between
the generated and original data. Nevertheless, L2D might generate
data with distorted semantic information. Pro-RandConv [ 4] recur-
sively stacks random convolution layers with a small kernel size
to improve the style diversity while preserving class-specific se-
mantic information. UDP [ 7] minimizes the uncertainty coefficients
between the augmented and original samples from an information-
theoretic perspective.
We argue that existing single domain generalization methods
primarily concentrate on enhancing the modelâ€™s generalization
performance during the training phase. In contrast, our approach
engages in both training-time and test-time learning, ensuring the
model adapts to diverse target domains.
Test-time adaptation (TTA). The aim of test-time adaptation
is to mitigate domain shifts by using test data to optimize the
model during testing. Massive efforts have been made for TTA in
recent years, and existing methods can be broadly divided into
two categories, i.e., test-time training (TTT) and Fully TTA. The
basic paradigm of TTT methods is to additionally design a self-
supervised auxiliary task during training, and update them for
model optimization during the testing phase, such as TTT [ 25], MT3
[1], TTT++ [ 18], and OST [ 3]. Recent studies [ 2,30] have shown that
if inappropriate self-supervised tasks that are inconsistent with the
primary task are used, the performance of existing TTT methods
will deteriorate. In contrast, Fully TTA is more practical because
it does not require the addition of any auxiliary self supervised
objectives during training and can adapt to arbitrary models, which
uses entropy minimization to update the model during testing, such
as EATA [ 22], SAR [ 23], and DeYo [ 11]. However, most Fully TTA
methods need to filter out high-entropy samples to reduce the effect
of unreliable samples. In practice, the threshold for filtering samples
with high-entropy is not easy to choose.
It is worth noting that TTT methods rely on self-supervised
auxiliary tasks, and Fully TTA only focuses on test-time adaptation.
However, our method does not require the use of self-supervised
auxiliary tasks, and focuses on both training-time and test-time
learning to make the model adapt to the target domain.
3 PROPOSED METHODS
3.1 Problem Formulation and Overview
Problem Formulation. A labeled source domain ğ·ğ‘ ={ğ‘¥ğ‘–,ğ‘¦ğ‘–}ğ‘›
ğ‘–=1
withğ‘›samples is available during the training phase, where ğ‘¥ğ‘–and
ğ‘¦ğ‘–represent the ğ‘–ğ‘¡â„sample and the class label of ğ‘¥ğ‘–, respectively,
and a modelMğœƒwith parameter ğœƒtrained onğ·ğ‘ and an arbitrary
unlabeled target domain ğ·ğ‘¡=
ğ‘¥ğ‘¡
ğ‘–	ğ‘›ğ‘¡
ğ‘–=1withğ‘›ğ‘¡samples is available
during the testing phase, where ğ‘¥ğ‘¡
ğ‘–denotes the ğ‘–ğ‘¡â„sample ofğ·ğ‘¡.
The goal of our method is to first learn a model Mğœƒwith good
generalization ability by using the knowledge in ğ·ğ‘ , and then
use information from ğ·ğ‘¡to further optimize Mğœƒtoimprove the
prediction performance onğ·ğ‘¡.
Overview of PSDG. We propose the PSDG algorithm to tackle
the domain shift issue by performing training-time and test-time
learning. PSDG consists of four components, including: (1) feature
extractor Î¦(Â·;ğœƒÎ¦):Xâ†’F , whereXandFare the image space andthe feature space, respectively; (2) classifier head ğ‘“(Â·;ğœƒğ‘“):Fâ†’P ,
wherePis the prediction label space; (3) object projection head
ğ‘§(Â·;ğœƒğ‘§):F â†’Z , whereZis the low-dimensional space of F;
(4) generator ğº(Â·;ğœƒğº), which is used to synthesize new data. The
PSDG framework is illustrated in Fig. 2. In the training phase, the
training strategy commences with the pretraining of the represen-
tation learning module using original data, followed by iterative
optimization of the representation learning module and the style
diversity module, allowing the two modules to mutually enhance
each other. To be specific, Î¦(Â·;ğœƒÎ¦),ğ‘“(Â·;ğœƒğ‘“), andğ‘§(Â·;ğœƒğ‘§)are shared
across both modules. Î¦(Â·;ğœƒÎ¦),ğ‘“(Â·;ğœƒğ‘“), andğ‘§(Â·;ğœƒğ‘§)are updated,
whileğº(Â·;ğœƒğº)remains fixed when learning feature representations.
Conversely, during the generation of new data, ğº(Â·;ğœƒğº)is updated,
whereas Î¦(Â·;ğœƒÎ¦),ğ‘“(Â·;ğœƒğ‘“), andğ‘§(Â·;ğœƒğ‘§)remain fixed. PSDG itera-
tively updates these two modules and generates multiple novel
domains to expand the coverage of training data, filtering style
knowledge. For simplicity, in the training phase, the learned model
is referred asMğœƒ. During testing, PSDG employs sample-aware
and sharpness-aware minimization to obtain a flat entropy min-
imum surface using specific knowledge in the target domain for
subsequent model optimization. In the following, we provide the
details of PSDG.
3.2 Training-time Learning
1) Representation learning module. The objective of represen-
tation learning module is to capture invariant representations that
exhibit strong generalization capabilities. Before providing the de-
tails of the proposed representation learning module, we give the
following Theorem 1.
Theorem 1 [ 21].Given a finite number of domains K, with the
number of samples ğ‘›in each domain approaching infinity, the set of
representations that fulfill the conditionÃ
Î©(ğ‘–,ğ‘—)=1;ğ‘‘â‰ ğ‘‘â€²dist Î¦(ğ‘¥(ğ‘‘)
ğ‘–),
Î¦(ğ‘¥(ğ‘‘â€²)
ğ‘—)=0includes the optimal Î¦(ğ‘¥)=ğ‘¥ğ‘that minimizes the
domain generalization loss, denoted as arg minğ‘“E[â„“(ğ‘¦,ğ‘“(Î¦(ğ‘¥ğ‘)))],
whereğ‘¥ğ‘are causal features that are robust across different domains.
Here,Î©:XÃ—Xâ†’{ 0,1}is a matching function that equals 1 for
pairs of inputs across domains corresponding to the same object and 0
otherwise. The indexes ğ‘‘andğ‘‘â€²represent different domains.
Motivated by Theorem 1, we leverage the generated data to
capture invariance. Considering that the generated data ğ‘¥+
ğ‘–(the
details for generating ğ‘¥+
ğ‘–are provided in Style diversity module ) and
the original data ğ‘¥ğ‘–have the same object with distinct styles, our
expectation is that the feature representations (i.e., Î¦(ğ‘¥ğ‘–)) ofğ‘¥ğ‘–
and the feature representations (i.e., Î¦(ğ‘¥+
ğ‘–)) ofğ‘¥+
ğ‘–should exhibit
similarity, thereby mitigating spurious correlations between style
features and labels. To this end, cross-entropy coupled with object-
consistency regularization is incorporated to capture consistency
between the original data and the currently generated data. First,
the classifier ğ‘“(Â·)should accurately predict both the original and
generated data. To achieve this, we minimize the following cross-
entropy loss.
Lğ‘Ÿ=ğ‘›âˆ‘ï¸
ğ‘–=1â„“ ğ‘¦ğ‘–,ğ‘“(Î¦(ğ‘¥ğ‘–))+â„“ ğ‘¦ğ‘–,ğ‘“(Î¦(ğ‘¥+
ğ‘–)), (1)
3796KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Shuai Yang, Zhen Zhang, & Lichuan Gu
Figure 2: The framework of PSDG, mainly containing training-time learning and test-time learning. During training, PSDG
integrates a representation learning module designed to learn invariant representations with good generalization ability and a
style diversity module dedicated to generating new style data. Initially, PSDG pretrains the representation learning module
using data from a single source domain. Subsequently, PSDG iteratively optimizes these two modules to augment multiple
novel domains. During testing, PSDG first computes the confidence of the predictive results by measuring the two largest
values in the prediction logit of a given target sample, and then integrates them into sharpness-aware minimization to find a
flat entropy minimum surface for further model optimization.
whereâ„“(Â·)is the cross-entropy loss.
Second, object-consistency regularization that consists of object-
level contrastive learning loss and residual uncertainty loss is in-
troduced to capture domain invariance. To be specific, we aim to
maximize the correlation between an original sample in the source
domain and its augmented sample in the feature representation
space. Inspired by [ 13,27], we maximize the lower bound of mutual
information through contrastive learning, introducing object-level
contrastive learning to facilitate PSDG learning invariant represen-
tations. Specifically, we treat the source domain and the augmented
domain as distinct domains, considering ğ‘¥ğ‘–andğ‘¥+
ğ‘–as a paired
sample. Consequently, the object-level contrastive learning loss is
defined as follows.
Lğ‘œ=2ğ‘›âˆ‘ï¸
ğ‘–=1âˆ’logexp(ğ‘§ğ‘–Â·ğ‘§+
ğ‘–/ğœ)
Ã2ğ‘›
ğ‘—=1,ğ‘—â‰ ğ‘–exp(ğ‘§ğ‘–Â·ğ‘§ğ‘—/ğœ), (2)
whereğ‘§ğ‘–=ğ‘§(Î¦(ğ‘¥ğ‘–)),ğ‘§+
ğ‘–=ğ‘§(Î¦(ğ‘¥+
ğ‘–)).ğœis a temperature parameter.
Nevertheless, object-level contrastive learning primarily focuses
on entire features, potentially encompassing style features. There-
fore, relying solely on these features might lead the model to align
only partial aspects of the style features. Recent research has re-
vealed that an imageâ€™s prediction distribution correlates with class
activation maps, which highlight the content the model is concerned
with [ 16]. Inspired by this, we calculate the residual component,
which is the change in the original representation relative to theaugmented representation. If the original sample and its augmented
sample focus on the same region (i.e., content), then the residual
component should not contain classification information, that is,
the residual component should have the maximum prediction un-
certainty. Therefore, we propose incorporating residual uncertainty
loss to ensure that PSDG consistently focuses on the same region
when predicting labels for both the original and generated data as
follows.
Lğ‘=ğ‘›âˆ‘ï¸
ğ‘–=1ğ¶âˆ‘ï¸
ğ‘=1âˆ’ğ‘’ğ‘
ğ‘–Â·logğ‘’ğ‘
ğ‘–âˆ’ğ‘’ğ‘
ğ‘–,â˜…Â·logğ‘’ğ‘
ğ‘–,â˜…, (3)
whereğ‘’ğ‘–=ğ‘“(Î¦(ğ‘¥+
ğ‘–)âˆ’Î¦(ğ‘¥ğ‘–)),ğ‘’ğ‘–,â˜…=ğ‘“(Î¦(ğ‘¥â˜…
ğ‘–)âˆ’Î¦(ğ‘¥ğ‘–)),ğ¶is the num-
ber of categories, ğ‘’ğ‘
ğ‘–andğ‘’ğ‘
ğ‘–,â˜…are the class ğ‘prediction probabilities
of the residual component Î¦(ğ‘¥+
ğ‘–)âˆ’Î¦(ğ‘¥ğ‘–),Î¦(ğ‘¥â˜…
ğ‘–)âˆ’Î¦(ğ‘¥ğ‘–), respec-
tively.ğ‘¥â˜…
ğ‘–is an augmented sample of ğ‘¥ğ‘–(the details for generating
ğ‘¥â˜…
ğ‘–are provided in Style diversity module ).
Based on Eq. (1), Eq. (2), and Eq. (3), we formulate the objective
function of the representation learning module as Eq. (4) as follows.
Lğ‘…=Lğ‘Ÿ+Lğ‘œâˆ’Lğ‘. (4)
2) Style diversity module. The goal of style diversity module
is to yield diverse style data sharing similar semantic information
with the original one. To satisfy this requirement, we design a
generatorğºcomposed of encoder ğºğ‘’ğ‘›, StyIN, and decoder ğºğ‘‘ğ‘’to
synthesize new data. Specifically, ğºfirst adoptsğºğ‘’ğ‘›to encode the
3797Practical Single Domain Generalization via Training-time and Test-time Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
input image ğ‘¥ğ‘–to obtain latent representations ğ‘Ÿğ‘–=ğºğ‘’ğ‘›(ğ‘¥ğ‘–). Then,
ğºuses StyIN to perturb the style at the latent representation layer.
To generate data with large style shifts, StyIN performs two affine
transformations. StyIN uses two fully connected layers Fc 1(Â·)and
Fc2(Â·)to encode a Gaussian noise ğœ€âˆ¼N( 0,1)to learn variance shift
parameter Fc 1(ğœ€)and mean shift parameter Fc 2(ğœ€), respectively,
and then performs the first affine transformation on ğ‘Ÿğ‘–to obtain
style perturbation data ğ‘Ÿâ€²
ğ‘–=AdaIN(ğ‘Ÿğ‘–,ğœ€)as follows.
AdaIN(ğ‘Ÿğ‘–,ğœ€)=Fc1(ğœ€)rğ‘–âˆ’ğœ‡(rğ‘–)âˆšï¸
ğœ2(rğ‘–)+ğœ–+Fc2(ğœ€), (5)
whereğœ‡(Â·)is the mean, ğœ2(Â·)is the variance, and ğœ–is a positive
number. However, although Eq. (5) can change the styles of the
original data, we have found from the experimental results that it
only produces data with a single-color background, thereby lim-
iting the range of styles and failing to cover the target domain
with large shifts. To this end, based on AdaIN, we propose StyIN,
which conducts a second affine transformation to further enhance
diversity while ensuring safety as follows.
StyIN(ğ‘Ÿğ‘–,ğœ€,ğœ‚)= 1+Fc3(ğœ€)(AdaIN(ğ‘Ÿğ‘–,ğœ€)+ğœ‚)+Fc4(ğœ€),(6)
where Fc 3(Â·)and Fc 4(Â·)are two fully connected layers. Here, we
introduce additional Gaussian noise ğœ‚âˆ¼N( 0,1)to directly perturb
the AdaIN to further enlarge style shifts. As ğœ‚directly affects AdaIN,
potentially changing the original semantic information, we employ
constant 1 (i.e., 1+Fc3(ğœ€)) to ensure the transformation process,
including the original representations, maintaining semantic con-
sistency. Finally, ğºadoptsğºğ‘‘ğ‘’to decode StyIN(ğ‘Ÿğ‘–,ğœ€,ğœ‚)to obtain
augmented data ğ‘¥+
ğ‘–.
In summary, when the inputs ğ‘¥ğ‘–andğœ€are given,ğºcan obtain
the augmented sample ğ‘¥+
ğ‘–=ğº(ğ‘¥ğ‘–,ğœ€,ğœ‚)as the following Eq. (7).
ğº(ğ‘¥ğ‘–,ğœ€,ğœ‚)=ğºğ‘‘ğ‘’(StyIN(ğºğ‘’ğ‘›(xğ‘–),ğœ€,ğœ‚)). (7)
To ensure that the augmented data have a different style from
the original data, we introduce style-diversity regularization that
consists of feature maximization loss and pixel maximization loss
to improve the diversity of newly generated style data from both
feature-level and pixel-level. Specifically, first, the feature repre-
sentations of the augmented data and the original data need to
have slight differences. To meet this requirement, we learn feature
representations of the input image using the feature extractor Î¦,
and further map feature representations to low-dimensional space
through the object projection head z, and introduce the following
feature maximization loss Lğ‘“to enlarge the difference between
a sample in the source domain and its augmented sample (i.e., a
sample pair) inZspace.
Lğ‘“=1
Ã2ğ‘›
ğ‘–=1âˆ’log
exp(ğ‘§ğ‘–Â·ğ‘§+
ğ‘–/ğœ)Ã2ğ‘›
ğ‘—=1,ğ‘—â‰ ğ‘–exp(ğ‘§ğ‘–Â·ğ‘§ğ‘—/ğœ).(8)
Lğ‘“is the inverse of the Lğ‘œloss.Lğ‘œandLğ‘“are used in the represen-
tation learning module and the style diversity module, respectively.
We train the two modules in an adversarial manner. Note that we
find that the optimization process becomes difficult if we directly
useLğ‘“=âˆ’Lğ‘œinstead ofLğ‘“=1
Lğ‘œ. Therefore, we use Lğ‘“=1
Lğ‘œ.
Second,ğºrelies on the input ğœ€to generate new style data. To in-
crease the style diversity of augmented data, given different valuesofğœ€, the input of ğºshould be different. That is, ğ‘¥+
ğ‘–=ğº(ğ‘¥ğ‘–,ğœ€1,ğœ‚1)
should be different from ğ‘¥â˜…
ğ‘–=ğº(ğ‘¥ğ‘–,ğœ€2,ğœ‚2), whereğœ€1,ğœ‚1âˆ¼N( 0,1)
andğœ€2,ğœ‚2âˆ¼N( 0,1)are Gaussian noises. Moreover, inspired by
[15], we also progressively generate multiple domains (i.e., progres-
sively learning multiple generators) to expand the style coverage
of training data. Therefore, to improve the effectiveness of the
generated new style data, we expect to constantly generate new
styles, that is, the current generated style of ğ‘¥ğ‘–should be different
from previously generated styles. To this end, the following pixel
maximization loss is adopted.
Lğ‘=1Ãğ‘›
ğ‘–=1ğ‘¥+
ğ‘–âˆ’ğ‘¥â˜…
ğ‘–
2+1
Ãğ¾âˆ’1
ğ‘˜=1Ãğ‘›
ğ‘–=1ğ‘¥+
ğ‘–âˆ’ğ‘¥+
ğ‘–,ğ‘˜2,(9)
whereğ¾(initially set to 1) represents the number of generators.
ğ‘¥+
ğ‘–,ğ‘˜denotes an augmented sample derived from ğ‘¥ğ‘–and generated
by theğ‘˜ğ‘¡â„generator. The first term ensures that the generator pro-
duces diverse style data with distinct ğœ€. The subsequent term serves
to increase the inter-domain discrepancies between the presently
generated data and the data generated earlier.
Finally,ğ‘¥+
ğ‘–is the augmented sample derived from ğ‘¥ğ‘–, and as such,
accurate predictions by the classifier ğ‘“(Â·)are expected for them to
retain the semantic information. To achieve this, we minimize the
following cross-entropy loss.
Lğ‘‘=ğ‘›âˆ‘ï¸
ğ‘–=1â„“ ğ‘¦ğ‘–,ğ‘“(Î¦(ğ‘¥+
ğ‘–)), (10)
whereğ‘“(Î¦(ğ‘¥+
ğ‘–))is the predicted labels of ğ‘¥+
ğ‘–. Besides, we also in-
corporate residual uncertainty loss Lğ‘to ensure the generation of
data devoid of semantic information distortions.
In summary, we formulate the objective function of the style
diversity module as Eq. (11) as follows.
Lğº=Lğ‘‘âˆ’Lğ‘+ğœ†ğ‘“Lğ‘“+ğœ†ğ‘Lğ‘, (11)
whereğœ†ğ‘“andğœ†ğ‘are the balancing parameters.
In practical scenarios, the collected data exhibit inherent com-
plexity. Generating data from a single domain poses limitations on
the capacity of training data, potentially leading to the capture of
spurious correlations between style features and class labels. To ad-
dress this issue, we adopt a progressive strategy by learning ğ¾style
generators, denoted as ğº(Â·;ğœƒğº)=
ğº1(Â·;ğœƒğº1),Â·Â·Â·,ğºğ¾(Â·;ğœƒğºğ¾)	
.
These generators are employed to generate ğ¾new domain datasets,
each with distinct styles, enabling the learning of domain invari-
ant representations. Here, ğºğ‘–(Â·;ğœƒğºğ‘–)(ğ‘–=1,Â·Â·Â·,ğ¾) represents the
ğ‘–ğ‘¡â„generator. Nevertheless, real-world applications often involve
subtle variations in the shapes of objects between source and tar-
get domains. As discussed in the Introduction, relying solely on
training-time adaptation is insufficient when faced with slight con-
tent shifts.
3.3 Test-time Learning
The model learned in the training phase has good generalization
performance, but it is still difficult to classify all target domain
samples accurately due to the domain discrepancy. Recent studies
[23,30] have revealed that the domain-specific knowledge in the
target domain may facilitate the learning of model, since class la-
bels for different domains have a correlation with domain-specific
3798KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Shuai Yang, Zhen Zhang, & Lichuan Gu
knowledge. Motivated by this, we perform test-time adaptation,
that is, we aim to use the specific knowledge in the target domain to
enhance the model, making it adapt to the target domain. However,
there is no prior knowledge of what is domain-specific knowledge
in the unlabeled target domain during testing. Therefore, we di-
rectly use unlabeled target domain data to optimize the model. To
do so, we can use the domain-specific knowledge since these target
domain samples contain all domain-specific knowledge. Note that
in the testing phase, we can access the unlabeled target domain data
and the learned model Mğœƒconsists of feature extractor Î¦(Â·;ğœƒÎ¦),
and classifier ğ‘“(Â·;ğœƒğ‘“), without accessing any data from the train-
ing domain, and thus a simple and effective method is to directly
minimize the entropy of the target domain samples as follows.
ğ¸(ğ‘¥ğ‘¡
ğ‘–;ğœƒ)=âˆ’ğ‘“ Î¦(ğ‘¥ğ‘¡
ğ‘–;ğœƒ)Â·logğ‘“ Î¦(ğ‘¥ğ‘¡
ğ‘–;ğœƒ). (12)
However, in practice, it is difficult for a model learned in the
training phase to accurately classify all unlabeled target domain
samples, resulting in obtaining partial incorrect pseudo labels [ 36].
Therefore, directly minimizing the entropy of unreliable samples
(i.e., the corresponding pseudo label is incorrect or the sample
has noise) would limit the performance of the model and even
result in collapsed trivial solutions, i.e., all samples are assigned
the same class label [ 23]. Consequently, it becomes necessary to
confirm the trustworthiness of samples. To alleviate this problem,
several methods [ 22,23] are devoted to selecting samples with low
entropy to identify trustworthy samples for model optimization.
However, a recent study [ 11] highlights that there are still some
unreliable samples with low entropy samples. That is to say, it is
difficult to accurately identify unreliable samples. Therefore, it is
necessary to mitigate the impact of unreliable samples. Inspired
by [37], we leverage the confidence of the predictive results to
amplify the impact of reliable samples and diminish the impact of
unreliable ones when optimizing the model parameters. We define
"pseudo unreliable samples" as samples where the model struggles
to confidently classify the sample, leading to hesitation in assigning
its class. An intuitive reflection of this uncertainty can be observed
in the class activation map through the proximity of the prediction
logits. Hence, we classify a sample as pseudo unreliable if its two
largest prediction values in the logits are very close. For a test
sampleğ‘¥ğ‘¡
ğ‘–âˆˆğ·ğ‘¡, we first obtain the two largest values ğ‘(ğ‘¥ğ‘¡
ğ‘–)and
ğ‘(ğ‘¥ğ‘¡
ğ‘–)in logitMğœƒ(ğ‘¥ğ‘¡
ğ‘–), whereğ‘(ğ‘¥ğ‘¡
ğ‘–)denotes the largest prediction
andğ‘(ğ‘¥ğ‘¡
ğ‘–)represents the prediction ranked after ğ‘(ğ‘¥ğ‘¡
ğ‘–). Then, we
compute the confidence of ğ‘¥ğ‘¡
ğ‘–using Eq. (13) as follows.
ğ‘Š(ğ‘¥ğ‘¡
ğ‘–)= ğ‘(ğ‘¥ğ‘¡
ğ‘–)âˆ’ğ‘(ğ‘¥ğ‘¡
ğ‘–)2. (13)
We consider that the samples are far from the decision boundary
if the model has high confidence in predictions. In other words,
the modelâ€™s predictions are reliable for these samples. Therefore,
ğ‘Š(Â·)assigns high weights to these samples to encourage the model
to optimize the parameters of these samples. On the other hand,
ğ‘Š(Â·)assigns low weights to the samples with low confidence in the
prediction to prevent performance degradation. Note that if ğ‘(ğ‘¥ğ‘¡
ğ‘–)
andğ‘(ğ‘¥ğ‘¡
ğ‘–)have similar or identical values, the information from ğ‘¥ğ‘¡
ğ‘–
will not be used. To alleviate this problem, we add the constant 1
to Eq. (13) to make full use of each sample as follows.
ğ‘Š(ğ‘¥ğ‘¡
ğ‘–)=1+ ğ‘(ğ‘¥ğ‘¡
ğ‘–)âˆ’ğ‘(ğ‘¥ğ‘¡
ğ‘–)2. (14)Unfortunately, there still exists a risk that some harmful samples
may be present, where the two largest prediction values in the logits
of the sample differ significantly, yet the predicted label is incorrect,
which would mislead the modelâ€™s learning. Since the confidence
of the predictive results cannot identify these samples, we seek to
make the model insensitive to these samples. Study [ 6] reveals that
the sharpness measure has a high correlation with generalization
and a flat minimum has good generalization abilities, and thus
encouraging the model to seek to minimize the sharpness measure
of the entropy loss landscape and go to a flat area of the entropy
loss surface is one most straight forward solution to alleviate this
problem, since a flat minimum is robust to noisy or large gradients,
i.e., at the flat minimum, the model loss would not be significantly
affected by the noisy or large gradients updating [ 23]. Motivated
by this, we encourage model to go to a flat area of the entropy loss
surface for achieving the sharpness minimization by using SAM
[6] as follows.
min
ğœƒğ¸ğ‘†ğ´ğ‘€ ğ‘¥ğ‘¡
ğ‘–;ğœƒ, (15)
ğ¸ğ‘†ğ´ğ‘€ ğ‘¥ğ‘¡
ğ‘–;ğœƒâ‰œmax
ğœ–:âˆ¥ğœ–âˆ¥2â‰¤ğœŒğ¸ ğ‘¥ğ‘¡
ğ‘–;ğœƒ+ğœ–, (16)
whereğœŒis the radius of the neighbourhood, and ğœ–is the weight per-
turbation. Eq. (16) finds the weight perturbation ğœ–in the Euclidean
ball with radius ğœŒthat maximizes the empirical loss [ 32]. According
to [6], we can obtain the approximate solution ğœ–ğ‘šby invoking the
Taylor expansion of the entropy loss as follows.
ğœ–ğ‘š=max
ğœ–:âˆ¥ğœ–âˆ¥2â‰¤ğœŒğ¸ ğ‘¥ğ‘¡
ğ‘–;ğœƒ+ğœ–
â‰ˆğœŒÂ·sign âˆ‡ğ¸(ğ‘¥ğ‘¡
ğ‘–;ğœƒ)Â·âˆ‡ğ¸
ğ‘¥ğ‘¡
ğ‘–;ğœƒ
âˆ‡ğ¸(ğ‘¥ğ‘¡
ğ‘–;ğœƒ)
2.(17)
As a result, the Eq. (15) can be rewritten as
min
ğœƒğ¸ğ‘†ğ´ğ‘€(ğ‘¥ğ‘¡
ğ‘–;ğœƒ+ğœ–ğ‘š). (18)
Based on Eq. (14) and Eq. (18), we employ a sample-aware and
sharpness-aware minimization method to further optimize the
model as follows.
min
ğœƒğ‘Š(ğ‘¥ğ‘¡
ğ‘–)Â·ğ¸ğ‘†ğ´ğ‘€(ğ‘¥ğ‘¡
ğ‘–;ğœƒ+ğœ–ğ‘š), (19)
which integrates the weights learned by Eq. (14) into SAM to opti-
mize the model.
4 EXPERIMENTS
4.1 Experimental Setups
Datasets. We evaluate PSDG on three commonly used single do-
main generalization datasets, including Digits, CIFAR10-C, and
PACS. Digits [28] is used for digit classification, which consists
of 5 domains: MNIST, MNIST-M, USPS, SYN, and SVHN in from
10 categories ranging from 0 to 9. Following [ 7,15,35], we regard
the first 10,000 images from MNIST as the source domain and each
of the remaining four domains as the target domain. CIFAR10-C
[8] is created by applying 19 corruptions with 5 severity levels to
the CIFAR10 [ 12] dataset. In our experiments, CIFAR10 is used for
training and CIFAR10-C is employed for test. PACS [14] contains
9,991 images in 7 categories from four distinct domains, namely
3799Practical Single Domain Generalization via Training-time and Test-time Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Sketch, Cartoon, Art painting, and Photo. In the experiments, one
domain is used as the source domain and the other three domains
are regarded as the target domains.
Comparison Methods. PSDG is compared with 11 single do-
main generalization methods including Empirical Risk Minimiza-
tion (ERM), ADA [ 28], M-ADA [ 24], ME-ADA [ 42], L2D [ 35], PDEN
[15], NCDG [ 26], MetaCNN [ 29], Pro-RandConv [ 4], UDP [ 7], and
AdvST [ 43]. Moreover, PSDG is compared with 4 test-time adapta-
tion algorithms including Tent [ 30], CoTTA [ 33], EATA [ 22], and
SAR [ 23].For a fair comparison, we use the same pre-trained
model as [ 7,15,35] for all mentioned-above test-time meth-
ods. To fully evaluate the effectiveness of PSDG, we propose
a variant of PSDG, referred as PSDGâ€¡, which only performs
training-time learning. Similarly, we introduce a variant of
PSDG, denoted as PSDGâ¨¿, which only performs test-time
learning. In addition, we use PSDGâ€¡+(Â·)to represent the test-
time method(Â·)utilizing PSDGâ€¡as the pre-trained model. In
the following, * indicates our implementation.
Implementation Details. For Digits, the LeNet [ 28] is used as
the backbone. All images are resized to 32 Ã—32. The images from
USPS and MNIST are converted to RGB images by duplicating them
from one channel to three channels. The model is trained with batch
size 128 for 50 epochs. For optimization, we adopt Adam optimizer
with a learning rate of 1e-4. We set the parameters ğ¾=50,ğœ†ğ‘“=0.1,
andğœ†ğ‘=1. At test-time learning, we set the batchsize to 128. For
CIFAR10-C, the feature extractor is built on the WideResNet with
16 layers and the width 4. We train the network using SDG with a
learning rate of 1e-4 and momentum of 0.9 using cosine annealing
schedule. We set batch size to 128 and epoch to 30, and set the
parametersğ¾=50,ğœ†ğ‘“=0.1, andğœ†ğ‘=1. The batchsize on test-time
learning is set to 128. For PACS, we adopt the ResNet-18 network as
the backbone pre-trained on ImageNet. For optimization, we adopt
SGD optimizer with a learning rate of 1e-4 and momentum of 0.9.
We train the network with batch size 64 for 4 epochs, and set the
parametersğ¾=20,ğœ†ğ‘“=0.1, andğœ†ğ‘=2. The model is trained with
the batchsize of 32 on test-time learning.
4.2 Experiment Results and Analysis
Results on Digits. The performance on Digits is presented in Ta-
ble 1. The results highlight several important findings. Firstly, we
observe that the generalization performance of PSDGâ€¡significantly
outperforms the gradient-based data augmentation methods, ADA,
M-ADA, ME-ADA, NCDG, and AdvST, across most of the target
domains. AdvST conceptualizes a composition of several standard
data augmentations as a semantics transformation with learnable
parameters to generate augmented data. In contrast, PSDGâ€¡does
not need standard data augmentations as prior knowledge and
adopts style generation to generate data with large distribution
shifts. Secondly, when compared to the style-based data augmenta-
tion methods such as PDEN, L2D, MetaCNN, Pro-RandConv and
UDP, PSDGâ€¡also demonstrates superior performance. We reason
that PSDGâ€¡enhances its ability to capture and utilize invariant rep-
resentations, which enables it to gain strong generalization capabil-
ities. Thirdly, we observe that PSDGâ¨¿obtains the highest average
accuracy than the other TTA methods, but it still achieves poor
performance compared to single domain generalization methods,Table 1: Accuracy (%) of different methods trained on Digits.
Each column title indicates the target domain.
Mehto
ds SVHN
MNIST-M SYN USPS A
vg.
ERM 27.8
52.7 39.7 76.9 49.3
AD
A 35.5
60.4 45.3 77.3 54.6
M-
ADA 42.6
67.9 49.0 78.5 59.5
ME-
ADA 42.6
63.3 50.4 81.0 59.3
PDEN 62.2
82.2 69.4 85.3 74.8
L2D 62.9
87.3 63.7 84.0 74.5
MetaCNN 66.5 88.3 70.7
89.6 78.8
NCDG 59.7
77.4 63.8 92.6 73.4
Pr
o-RandConv 69.7
82.3 79.8 93.7 81.4
UDP 72.4
79.7 81.7 96.3 82.5
A
dvST 67.5
79.8 78.1 95.4 80.1
PSDGâ€¡73.1 86.7 83.4 95.7 84.7
T
entâˆ—29.7
52.0 41.9 82.7 51.6
Co
TTAâˆ—17.1
28.9 30.3 71.6 37.0
EA
TAâˆ—29.7
52.0 41.9 79.7 50.8
SARâˆ—29.7
52.0 41.9 83.2 51.7
PSDGâ¨¿31.4
55.1 45.2 83.0 53.7
PSDGâ€¡+T
ent 74.7
88.2 86.9 95.7 86.4
PSDGâ€¡+Co
TTA 57.0
78.1 69.4 95.7 75.1
PSDGâ€¡+EA
TA 73.7
87.4 84.1 96.0 85.3
PSDGâ€¡+SAR 75.9
88.9 88.0 95.7 87.1
PSDG 78.1
89.9 89.2 96.5 88.4
indicating that focusing only on test-time learning is insufficient.
After performing test-time learning, it can be seen that PSDG per-
forms the highest average accuracy and achieves a gain of 1.3%
compared to SAR. This is because PSDG optimizes the model by
giving different weights to the samples with different reliability,
which can help the model adapt to the target domains better than
the other test-time adaptation methods.
Results on CIFAR10-C. Table 2 shows the results on CIFAR10-
C dataset. We see that our method performs similar results to the
other methods at severity level 1. We conjecture the reason is that
CIFAR10-C is similar to CIFAR10 at level 1, which leads to stylized
samples with large distribution shifts that may not improve the gen-
eralization performance well. However, at levels 2 to 5, our method
shows excellent generalization performances. This indicates that
PSDGâ€¡prefers to complete difficult tasks with large distribution
shifts. At level 5, our method outperforms the average accuracy
by0.6%over the best baseline UDP. Besides, PSDGâ€¡also achieves
the highest average classification accuracy, which is 81.1%. More-
over, the test-time methods achieve good performance but still are
inferior to PSDGâ¨¿. At test-time learning, it is striking that PSDG
results in 89.5% performance on average accuracy for CIFAR10-C.
In addition, PSDG has a similar performance to the other test-time
adaptation methods, we conjecture the reason for the similar im-
provement is that the parameters of PSDGâ€¡may be almost enough
to extract necessary domain invariant features. However, it is still
valuable to achieve the best average accuracy on a high-quality
benchmark. The specific results of 19 corruption at severity level
5 are reported in Table 4. PSDG exhibits excellent performances
3800KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Shuai Yang, Zhen Zhang, & Lichuan Gu
Table 2: Accuracy (%) of different methods on CIFAR10 under
different corruption levels. Each column title indicates the
performance on different corruption levels.
Metho
ds Le
vel 1 Level 2 Level 3 Level 4 Level 5 A
vg.
ERM 87.8
81.5 75.5 68.2 56.1 73.8
AD
A 88.3
83.5 77.6 70.6 58.3 75.7
M-
ADA 90.5
86.8 82.5 76.4 65.6 80.4
ME-
ADA 90.0
87.5 84.6 80.7 72.7 83.1
PDEN 90.6
88.9 87.0 83.7 77.5 85.5
L2D 91.3 88.9
86.8 80.9 69.4 83.5
MetaCNN 91.2
88.9 87.0 83.4 78.2 85.7
NCDGâˆ—91.2
89.0 87.1 84.0 79.2 86.1
UDPâˆ—89.9
88.6 87.2 84.7 80.5 86.2
A
dvSTâˆ—91.1
89.0 86.9 83.1 77.5 85.5
PSDGâ€¡90.5 89.1
87.8 85.3 81.1 86.8
T
entâˆ—86.1
84.9 83.5 81.8 79.4 83.1
Co
TTAâˆ—86.5
85.3 84.1 82.5 80.3 83.7
EA
TAâˆ—86.1
84.9 83.6 81.9 79.5 83.2
SARâˆ—86.6
85.6 84.4 82.8 80.6 84.0
PSDGâ¨¿86.7
85.9 84.8 83.3 81.3 84.4
PSDGâ€¡+T
ent 91.1
90.3 89.4 88.0 86.2 89.0
PSDGâ€¡+Co
TTA 90.5
89.6 88.7 87.4 85.4 88.3
PSDGâ€¡+EA
TA 91.1
90.3 89.4 88.1 86.2 89.0
PSDGâ€¡+SAR 91.3
90.5 89.8 88.5 86.8 89.4
PSDG 91.3
90.5 89.8 88.7 87.2 89.5
compared to the other lines of methods. By assigning high weights
to reliable samples (specifically, the calibration function of Eq. (14))
to help the model update the parameters, we observe improved
performance on every corruption type compared to PSDGâ€¡, even
improving to 31% on Pixelate.
Results on PACS. Table 3 shows the results on PACS dataset.
PSDGâ€¡achieves superior performance on average accuracy. The
accuracy of PSDGâ€¡is 1.5% higher than the second best model, Pro-
RandConv, by average. We also observe that although AdvST and
Pro-RandConv gain the higher accuracy on Photo and Cartoon
compared to PSDGâ€¡, the average accuracy drops, which means that
PSDGâ€¡has the more stable generalization performance than these
methods. We see that PSDGâ¨¿is superior to the other TTA methods.
Nonetheless, its performance is worse than that of ERM, indicating
the insufficiency of only employing test-time learning. Similar to
the results on Digits dataset, the poor performance of test-time
methods indicates the insufficiency of only employing test-time
learning. At test-time learning, similar to the exhibitions of PSDGâ€¡,
PSDG consistently outperforms the existing baselines across each
scenario. We see that PSDG achieves gains against PSDGâ€¡by: +
0.3% on average. In addition, compared with the other test-time
adaptation methods, PSDG performs the highest average accuracy
since assigning reliable samples with high weights can improve
generalization performance.
4.3 Ablation Study
In this section, we provide ablation studies to exhibit where the
performance improvement of PSDG comes from.
Impact of StyIN. To analyze the impact of the StyIN, we present
a variant of PSDG, which employs the AdaIN as the generator
and does not utilize test-time learning, denoted as PSDGâ€¡-AdaIN.Table 3: Accuracy (%) of different methods on PACS. Each
column title indicates the source domain.
Metho
ds P
hoto Art Cartoon Sketch A
vg.
ERM 42.0
70.9 76.5 53.1 60.7
AD
Aâˆ—41.7
68.8 69.3 35.1 53.7
M-
ADA 43.1
68.0 71.9 33.9 54.2
ME-
ADAâˆ—42.1
69.2 70.6 36.8 54.7
PDENâˆ—58.3
77.9 74.6 57.9 67.2
L2D 52.3
76.9 77.9 53.7 65.2
MetaCNNâˆ—56.7
76.5 74.4 55.7 65.8
NCDG 49.0
76.6 76.4 53.1 63.8
Pr
o-RandConv 62.9
77.0 78.5 57.1 68.9
UDPâˆ—56.7
77.1 77.6 57.5 67.2
A
dvSTâˆ—64.1 77.9
76.0 57.1 68.8
PSDGâ€¡62.1 78.2 78.3 63.0 70.4
T
entâˆ—34.0
68.8 69.8 31.1 50.9
Co
TTAâˆ—18.7
59.4 35.6 17.5 32.8
EA
TAâˆ—34.4
70.8 74.3 31.2 52.6
SARâˆ—41.7 71.8
70.6 31.7 53.9
PSDGâ¨¿43.2 71.7 70.6
32.9 54.6
PSDGâ€¡+T
ent 62.1
78.2 78.3 63.0 70.4
PSDGâ€¡+Co
TTA 56.7
71.8 62.2 56.2 61.8
PSDGâ€¡+EA
TA 62.1
78.2 78.3 63.0 70.4
PSDGâ€¡+SAR 62.5
78.3 78.3 63.0 70.5
PSDG 63.1
78.5 78.3 63.1 70.7
According to the results in Table 5, we observe that PSDGâ€¡achieves
a higher average accuracy than PSDGâ€¡-AdaIN, i.e., 84.7% ğ‘£ğ‘ .79.5%.
Specifically, PSDGâ€¡achieves an accuracy improvement of 17.1% on
MNIST-M dataset. At test-time adaptation, PSDG also achieves the
best generalization performance.
Impact ofLğ‘œ,Lğ‘,Lğ‘“andLğ‘.Table 6 shows the validity and
necessity of each loss function. We exhibit the validity of Lğ‘œ,Lğ‘,
Lğ‘“andLğ‘. Turning onLğ‘œ, the average accuracy of the model
training withoutLğ‘œdecreased from 84.7% to 83.7%. Similarly, the
average accuracy is reduced by 1.1% when PSDGâ€¡is trained with-
outLğ‘, sinceLğ‘œandLğ‘jointly help the model extract the domain
invariant features. Note that reducing any loss function still results
in model performance degradation. Since Lğ‘“encourages the gen-
erator to generate data with different styles than the original data,
it is hard to generate samples with large distribution shifts when
Lğ‘“is not used. The experiment results without using Lğ‘“is 83.7%,
which is lower than the 84.7% obtained by using Lğ‘“. Furthermore,
the results without adopting Lğ‘is 81.1%. This result indicates that
the performance of the model is significantly improved when Lğ‘is
employed to ensure the diversity of augmented data. Moreover, we
report the results of PSDGâ€¡and its variants on PACS in Appendix
D, which are similar to the results on Digits.
Analysis of the Parameter Sensitivity. To analyze the sensi-
tivity of PSDG to changes in parameters ğ¾,ğœ†ğ‘“andğœ†ğ‘, we conduct
experiments to analyze the parameter sensitivity of PSDG w.r.t the
various ofğ¾,ğœ†ğ‘“, andğœ†ğ‘. To this end, we consider Digits dataset
here. Fig. 3 shows the sensitivity analysis of PSDG concerning ğ¾,
3801Practical Single Domain Generalization via Training-time and Test-time Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 4: Target-oriented single domain generalization accuracy (%) trained on CIFAR-10 under the corruption levels of 5 (the
most severe). Each column title indicates the performance on 19 types of corruption. * indicates our implementation.
Metho
ds W
eather Blur Noise Digital
Fog
Snow Frost Zo
om Defocus Glass Gaussian Motion Sp
eckle Shot Impulse Gaussian Jp
eg Pixelate Spatter Elastic Brightness Saturate Contrast A
vg.
ERM 65.9
74.4 61.6 60.0
53.7 49.4 30.7 63.8 41.3
35.4 25.7 29.0 69.9
41.0 75.4 72.4 91.3 89.1 36.9 56.1
AD
A 68.3
76.8 69.9 63.0
56.4 53.5 38.3 63.9 38.5
36.9 22.3 32.4 74.2
53.3 80.3 74.6 89.9 82.9 31.6 58.3
M-
ADA 69.4
80.6 76.7 68.0
61.1 61.6 47.3 64.2 60.9
60.6 45.2 56.9 77.1
52.3 80.6 75.6 90.8 87.6 29.7 65.6
ME-
ADAâˆ—61.9
80.4 79.8 74.4
70.3 68.8 60.2 68.9 76.3
76.0 71.6 74.0 85.6
73.8 82.2 78.7 87.6 83.9 27.5 72.7
PDEN 69.6
81.8 84.5 83.7
82.1 60.1 79.3 76.7 79.3
81.3 66.8 81.1 85.2
70.8 79.4 75.1 91.0 88.4 55.6 77.5
L2Dâˆ—70.4
80.3 80.2 74.5
64.0 63.3 47.7 71.5 68.8
70.7 18.5 65.7 84.4
56.0 79.8 78.3 91.6 91.6 60.9 69.4
NCDG 81.1
83.5 82.1 88.1 89.0 68.0
85.0 86.0 74.7
71.7 66.8 66.2 78.7
63.4 88.6 80.2 92.2 89.9 69.1 79.2
MetaCNNâˆ—68.1
81.6 83.0 80.0
78.7 73.1 60.5 73.9 78.7
81.4 72.6 80.5 88.5
80.8 83.4 80.1 92.7 85.8 63.3 78.2
UDPâˆ—77.5
82.3 87.2 86.1
84.8 73.5 83.5 79.4 81.5
81.1 71.6 79.7 83.1
56.4 86.8 75.5 89.8 89.0 80.8 80.5
A
dvSTâˆ—80.3
82.2 81.9 88.8 88.6
67.2 69.3 69.0 56.7
79.2 46.1 88.7 92.0
81.2 76.9 91.4 79.2 87.1 65.7 77.5
PSDGâ€¡78.6
82.5 87.3 87.1
86.1 73.0 85.1 80.5 81.4
82.0 71.3 80.5 82.4
56.7 87.4 77.7 91.2 90.5 78.9 81.1
T
entâˆ—83.5
79.5 80.4 84.2
84.7 67.3 85.3 80.2 74.3
74.7 69.9 73.5 76.3
80.6 82.0 73.4 86.9 86.8 85.4 79.4
Co
TTAâˆ—82.5
79.8 81.9 83.7
82.6 70.2 82.4 80.0 76.7
77.9 74.6 76.8 79.5
81.4 82.5 76.3 86.5 86.8 82.5 80.3
EA
TAâˆ—83.5
79.5 80.4 84.3
84.8 67.6 85.4 80.4 74.5
74.8 70.1 73.7 76.5
80.5 82.0 73.5 86.9 86.8 85.5 79.5
SARâˆ—84.2
80.5 81.4 84.8
85.1 70.2 85.8 81.5 76.6
76.9 72.4 76.3 78.1
81.3 82.9 74.7 87.1 87.0 85.9 80.7
PSDGâ¨¿84.6
81.0 82.0 85.3
85.3 71.2 85.9 82.2 77.8
78.4 73.7 77.3 78.6
82.0 83.5 75.5 87.4 86.9 86.2 81.3
PSDGâ€¡+T
ent 86.9
86.6 88.7 89.7
89.3 76.3 89.3 86.6 84.1
84.4 79.0 83.6 84.1
86.7 89.0 80.1 91.0 90.9 90.6 86.2
PSDGâ€¡+Co
TTA 84.2
84.3 87.5 88.4
87.1 77.4 86.8 85.2 84.4
84.4 81.3 83.3 85.2
86.5 88.0 80.8 89.3 90.4 88.7 85.4
PSDGâ€¡+EA
TA 87.0
86.6 88.8 89.7
89.3 76.5 89.4 86.5 84.3
84.4 79.1 83.7 84.2
86.8 89.1 80.2 91.0 91.0 90.6 86.2
PSDGâ€¡+SAR 87.8
87.2 89.0 90.2 89.5
78.1 89.5 87.4 84.8
85.0 80.6 84.5 84.8
87.5 89.4 81.0 91.3 91.3 90.8 86.8
PSDG 88.9
87.5 89.0 90.2
89.8 79.1 89.7 88.1 85.2
85.6 81.6 84.9 85.0
87.7 89.4 81.6 91.4 91.4 90.7 87.2
Table 5: Accuracy (%) achieved by different style generators.
Metho
ds SVHN
MNIST-M SYN USPS A
vg.
PSDGâ€¡-A
daIN 70.5
69.6 82.0 96.1 79.5
PSDGâ€¡73.1
86.7 83.4 95.7 84.7
PSDG-A
daIN 77.9
71.6 87.6 96.5 83.4
PSDG 78.1
89.9 89.2 96.5 88.4
Table 6: Loss ablation study on Digits.
Lğ‘œLğ‘Lğ‘“Lğ‘SVHN
MNIST-M SYN USPS A
vg.
%
" " " 71.6
86.0 82.1 95.2 83.7
"
% " " 71.6
83.8 84.5 94.6 83.6
"
" % " 71.8
86.3 82.0 94.7 83.7
"
" " % 68.7
77.5 81.8 96.6 81.1
"
" " " 73.1
86.7 83.4 95.7 84.7
ğœ†ğ‘“andğœ†ğ‘. When sensitivity analysis is performed by varying a
parameter at the time over a given range, the other parameters
we set them to their final values. From Fig. 3 (a), we see that the
generalization performance of PSDG improves rapidly during the
value ofğ¾increases. By continuously generating new distribution
domains to increase the number of stylized samples, the gener-
alization performance improves. The more generators, the more
style variation in the generated samples, and the more robust the
model to the distribution shifts. Moreover, as ğ¾increases to a cer-
tain value, the increase in generalization performance becomes flat.
This demonstrates that PSDG is almost sufficient to capture the
variability in the sample distribution by diverse stylized samples.
Moreover, the optimal value ranges of ğœ†ğ‘“andğœ†ğ‘may be [0.1, 0.2]
and [1.0, 2.0], respectively. When the values of ğœ†ğ‘“andğœ†ğ‘are small,
the generator can not synthesize sufficient diverse stylized samples.
In addition, the large values of these two parameters deteriorate
the generalization performance since the generated samples may
distort the original semantic information.
0 10 20 30 40 505060708090Accuracy (%)
MNIST-M
USPS
SYN
SVHN(a) Parameter ğ¾
0.05 0.1 0.2 0.3 0.5 1.0707580859095Accuracy (%)
MNIST-M
USPS
SYN
SVHN (b) Parameter ğœ†ğ‘“
0.5 1.0 2.0 3.0 5.0 10.0707580859095Accuracy (%)
MNIST-M
USPS
SYN
SVHN (c) Parameter ğœ†ğ‘
Figure 3: Parameter sensitivity study on Digits.
5 CONCLUSION
In this paper, we present a novel method called PSDG that performs
both training-time and test-time learning to tackle the domain
shift problem. During the training phase, PSDG utilizes StyIN to
synthesize new diversity data to increase the coverage of training
data and introduces object-consistency regularization to capture
consistency between the augmented and original data for filtering
style knowledge. During the testing phase, we use a simple sample
reweighting strategy that assigns a weight to each sample based on
the prediction difference of different classes to mitigate the impact
of unreliable samples, and employ SAM to improve the generaliza-
tion performance. We evaluate PSDG on three datasets and observe
that it consistently surpasses current methods. The success of the
proposed PSDG algorithm indicates that simultaneously perform-
ing training-time and test-time learning is beneficial for mitigating
domain shift problems.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science
Foundation of China under Grant 62306008, the Natural Science
Foundation of Anhui Province under Grant 2308085MF217, and the
University Synergy Innovation Program of Anhui Province under
Grant GXXT-2022-046.
3802KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Shuai Yang, Zhen Zhang, & Lichuan Gu
REFERENCES
[1]Alexander Bartler, Andre BÃ¼hler, Felix Wiewel, Mario DÃ¶bler, and Bin Yang.
2022. MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption.
InInternational Conference on Artificial Intelligence and Statistics, Virtual Event,
March 28-30, Vol. 151. 3080â€“3090.
[2]Liang Chen, Yong Zhang, Yibing Song, Ying Shan, and Lingqiao Liu. 2023. Im-
proved Test-Time Adaptation for Domain Generalization. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, June 17-24.
24172â€“24182.
[3]Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. 2022. OST:
Improving Generalization of DeepFake Detection via One-Shot Test-Time Train-
ing. In Advances in Neural Information Processing Systems, New Orleans, LA, USA,
November 28 - December 9. 1â€“12.
[4]Seokeon Choi, Debasmit Das, Sungha Choi, Seunghan Yang, Hyunsin Park, and
Sungrack Yun. 2023. Progressive Random Convolutions for Single Domain Gen-
eralization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
Vancouver, BC, Canada, June 17-24. 10312â€“10322.
[5]Peng Cui and Susan Athey. 2022. Stable learning establishes some common
ground between causal inference and machine learning. Nature Machine Intelli-
gence 4, 2 (2022), 110â€“115.
[6]Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2021.
Sharpness-aware Minimization for Efficiently Improving Generalization. In In-
ternational Conference on Learning Representations, Virtual Event, Austria, May
3-7. 1â€“13.
[7]Kehua Guo, Rui Ding, Tian Qiu, Xiangyuan Zhu, Zheng Wu, Liwei Wang, and
Hui Fang. 2023. Single Domain Generalization via Unsupervised Diversity Probe.
InACM International Conference on Multimedia, Ottawa, ON, Canada, October
29-November 3. 2101â€“2111.
[8]Dan Hendrycks and Thomas G. Dietterich. 2019. Benchmarking Neural Net-
work Robustness to Common Corruptions and Perturbations. In International
Conference on Learning Representations, New Orleans, LA, USA, May 6-9. 1â€“11.
[9]Julia Hirschberg and Christopher D Manning. 2015. Advances in natural language
processing. Science 349, 6245 (2015), 261â€“266.
[10] Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, and
Tongliang Liu. 2023. Harnessing Out-Of-Distribution Examples via Augmenting
Content and Style. In International Conference on Learning Representations, Kigali,
Rwanda, May 1-5. 1â€“14.
[11] Saehyung Lee Junsung Park Juhyeon Shin Uiwon Hwang Sungroh Yoon
Jonghyun Lee, Dahuin Jung. 2024. Entropy is not Enough for Test-time Adap-
tation: From the Perspective of Disentangled Factors. In The 12th International
Conference on Learning Representations, Vienna Austria, May 7-11. 1â€“26.
[12] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[13] Juliusvon KÃ¼gelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard
SchÃ¶lkopf, Michel Besserve, and Francesco Locatello. 2021. Self-Supervised
Learning with Data Augmentations Provably Isolates Content from Style. In
Conference on Neural Information Processing Systems, virtual, December 6-14.
16451â€“16467.
[14] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. 2017. Deeper,
broader and artier domain generalization. In IEEE International Conference on
Computer Vision, Venice, Italy, October 22-29. 5542â€“5550.
[15] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze Yu,
Xiaoya Li, and Boyang Xia. 2021. Progressive Domain Expansion Network for
Single Domain Generalization. In Conference on Computer Vision and Pattern
Recognition, virtual, June 19-25. 224â€“233.
[16] Shuang Li, Mixue Xie, Fangrui Lv, Chi Harold Liu, Jian Liang, Chen Qin, and
Wei Li. 2021. Semantic Concentration for Domain Adaptation. In International
Conference on Computer Vision, Montreal, QC, Canada, October 10-17. 9082â€“9091.
[17] Jian Liang, Dapeng Hu, and Jiashi Feng. 2020. Do We Really Need to Access the
Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation.
InInternational Conference on Machine Learning, Virtual Event, July 13-18, Vol. 119.
6028â€“6039.
[18] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor
Mordan, and Alexandre Alahi. 2021. TTT++: When Does Self-Supervised Test-
Time Training Fail or Thrive?. In Advances in Neural Information Processing
Systems, virtual, December 6-14. 21808â€“21820.
[19] Zirui Liu, Haifeng Jin, Ting-Hsiang Wang, Kaixiong Zhou, and Xia Hu. [n. d.].
DivAug: Plug-in Automated Data Augmentation with Explicit Diversity Maxi-
mization. In IEEE/CVF International Conference on Computer Vision, Montreal, QC,
Canada, October 10-17. 4742â€“4750.
[20] David G. Lowe. 1999. Object Recognition from Local Scale-Invariant Features. In
International Conference on Computer Vision, Kerkyra, Corfu, Greece, September
20-25. 1150â€“1157.
[21] Divyat Mahajan, Shruti Tople, and Amit Sharma. 2021. Domain Generalization
using Causal Matching. In Proceedings of International Conference on Machine
Learning, virtual, July 18-24, Vol. 139. 7313â€“7324.
[22] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin
Zhao, and Mingkui Tan. 2022. Efficient Test-Time Model Adaptation withoutForgetting. In International Conference on Machine Learning, Baltimore, Mary-
land,USA, July 17-23, Vol. 162. 16888â€“16905.
[23] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin
Zhao, and Mingkui Tan. 2023. Towards Stable Test-time Adaptation in Dynamic
Wild World. In International Conference on Machine Learning, Kigali, Rwanda,
May 1-5. 1â€“14.
[24] Fengchun Qiao, Long Zhao, and Xi Peng. 2020. Learning to Learn Single Domain
Generalization. In Conference on Computer Vision and Pattern Recognition, Seattle,
WA, USA, June 13-19. 12553â€“12562.
[25] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz
Hardt. 2020. Test-Time Training with Self-Supervision for Generalization under
Distribution Shifts. In International Conference on Machine Learning, Virtual Event,
July 13-18, Vol. 119. 9229â€“9248.
[26] Chris Xing Tian, Haoliang Li, Xiaofei Xie, Yang Liu, and Shiqi Wang. 2023. Neuron
coverage-guided domain generalization. IEEE Transactions on Pattern Analysis
and Machine Intelligence 45, 1 (2023), 1302â€“1311.
[27] AÃ¤ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning
with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018).
[28] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino,
and Silvio Savarese. 2018. Generalizing to Unseen Domains via Adversarial Data
Augmentation. In Conference on Neural Information Processing Systems, MontrÃ©al,
Canada, December 3-8. 5339â€“5349.
[29] Chaoqun Wan, Xu Shen, Yonggang Zhang, Zhiheng Yin, Xinmei Tian, Feng
Gao, Jianqiang Huang, and Xian-Sheng Hua. 2022. Meta Convolutional Neural
Networks for Single Domain Generalization. In Conference on Computer Vision
and Pattern Recognition, New Orleans, LA, USA, June 18-24. 4672â€“4681.
[30] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor
Darrell. 2021. Tent: Fully Test-Time Adaptation by Entropy Minimization. In
International Conference on Learning Representations, Virtual Event, Austria, May
3-7. 1â€“12.
[31] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu,
Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. 2023. Generalizing to Unseen
Domains: A Survey on Domain Generalization. IEEE Transactions on Knowledge
and Data Engineering 35, 8 (2023), 8052â€“8072.
[32] Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang. 2023. Sharpness-
Aware Gradient Matching for Domain Generalization. In Conference on Computer
Vision and Pattern Recognition, Vancouver, BC, Canada, June 17-24. 3769â€“3778.
[33] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. 2022. Continual Test-
Time Domain Adaptation. In IEEE Conference on Computer Vision and Pattern
Recognition, New Orleans, LA, USA, June 18-24. 7191â€“7201.
[34] Xinyi Wang, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, and
William Yang Wang. 2023. Causal Balancing for Domain Generalization. In
Causal Balancing for Domain Generalization, Kigali, Rwanda, May 1-5. 1â€“14.
[35] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and Mahsa Baktashmotlagh.
2021. Learning to Diversify for Single Domain Generalization. In International
Conference on Computer Vision, Montreal, QC, Canada, October 10-17. 814â€“823.
[36] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena,
Krishnamurthy Dvijotham, and Ali Taylan Cemgil. 2022. A Fine-Grained Analysis
on Distribution Shift. In International Conference on Learning Representations,
Virtual Event, April 25-29. 1â€“15.
[37] Hao Yang, Min Wang, Zhengfei Yu, Hang Zhang, Jinshen Jiang, and Yun Zhou.
2024. Confidence-based and sample-reweighted test-time adaptation. Knowledge-
Based Systems 283 (2024), 111164.
[38] Shuai Yang, Kui Yu, Fuyuan Cao, Lin Liu, Hao Wang, and Jiuyong Li. 2023. Learn-
ing Causal Representations for Robust Domain Adaptation. IEEE Transactions on
Knowledge and Data Engineering 35, 3 (2023), 2750â€“2764.
[39] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou,
Zhenguo Li, and Jun Zhu. 2022. OoD-Bench: Quantifying and Understanding
Two Dimensions of Out-of-Distribution Generalization. In IEEE Conference on
Computer Vision and Pattern Recognition, New Orleans, LA, USA, June 18-24. 7937â€“
7948.
[40] Kui Yu, Lin Liu, and Jiuyong Li. 2021. A Unified View of Causal and Non-causal
Feature Selection. ACM Transactions on Knowledge Discovery from Data 15, 4
(2021), 63:1â€“63:46.
[41] Kui Yu, Lin Liu, Jiuyong Li, Wei Ding, and Thuc Duy Le. 2020. Multi-Source
Causal Feature Selection. IEEE Transactions on Pattern Analysis and Machine
Intelligence 42, 9 (2020), 2240â€“2256.
[42] Long Zhao, Ting Liu, Xi Peng, and Dimitris N. Metaxas. 2020. Maximum-Entropy
Adversarial Data Augmentation for Improved Generalization and Robustness. In
Advances in Neural Information Processing Systems, virtual, December 6-12. 1â€“13.
[43] Guangtao Zheng, Mengdi Huai, and Aidong Zhang. 2024. AdvST: Revisiting
Data Augmentations for Single Domain Generalization. In Thirty-Eighth AAAI
Conference on Artificial Intelligence, February 20-27. 1â€“9.
[44] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. 2023.
Domain Generalization: A Survey. IEEE Transactions on Pattern Analysis and
Machine Intelligence 45, 4 (2023), 4396â€“4415.
3803Practical Single Domain Generalization via Training-time and Test-time Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 7: Efficiency statistics evaluated on RTX 4090.
Digits|Size:[3,32,32] T
rainingA
CC(%)Metho
ds Memor
y Time
PDEN 1.4GB 17.6min 74.8
L2D 1.1GB 8.3min 74.5
UDP 1.5GB 20.0min 82.5
A
dvST 0.9GB 1.8min 80.1
PSDGâ€¡(ğ¾=20) 1.5GB 23.5min 83.0
PSDGâ€¡(ğ¾=50) 1.6GB 70.3min 84.7
Metho
ds Time A
CC(%)
T
ent 8.2s 50.9
Co
TTA 90.3s 32.8
EA
TA 9.1s 52.6
SAR 11.8s 53.9
PSDGâ¨¿12.0s 54.6
Table 8: Accuracy (%) on Digits achieved by different compo-
nents.
Metho
ds SVHN
MNIST-M SYN USPS A
vg.
PSDGâ€¡w/o
1 71.5
82.5 82.8 95.7 83.1
PSDGâ€¡-Lğ‘ 70.8
79.5 82.4 96.1 82.2
PSDGâ€¡-ED 17.5
35.9 32.6 75.6 40.4
PSDGâ€¡73.1
86.7 83.4 95.7 84.7
Table 9: Accuracy (%) on CIFAR10-C achieved by different
components.
Metho
ds Le
vel 1 Level 2 Level 3 Level 4 Level 5 A
vg.
PSDGâ€¡w/o
1 90.4
89.1 87.7 84.8 80.2 86.4
PSDGâ€¡-Lğ‘ 90.4
89.1 87.8 85.0 80.4 86.6
PSDGâ€¡-ED 84.6
82.5 80.3 76.5 71.2 79.0
PSDGâ€¡90.5
89.1 87.8 85.3 81.1 86.8
Table 10: Accuracy (%) on PACS achieved by different compo-
nents.
Metho
ds P
hoto Art Cartoon Sketch A
vg.
PSDGâ€¡w/o
1 61.3
77.9 77.0 61.2 69.3
PSDGâ€¡-Lğ‘ 59.9
76.8 75.8 57.4 67.5
PSDGâ€¡-ED 19.2
20.0 24.6 16.6 20.1
PSDGâ€¡62.1
78.2 78.3 63.0 70.4
A COMPUTATIONAL COMPLEXITY AND
COMPUTATIONAL RESOURCES
Then, we show the computational complexity and computational
resources of our method and its main competitors on Digits in Table
7. During training, we use MNIST as the training data. Since our
method needs to learn 50 generators (i.e., ğ¾=50) to generate distinct
styles, PSDGâ€¡(ğ¾=50) has the highest computational complexity.However, 70.3 minutes is an acceptable amount of time for the high
accuracy, since it only needs to be trained once and can be deployed
to multiple different target domains. Besides, we observe PSDGâ€¡
(ğ¾=20) still achieves the highest accuracy compared to the other
methods, and its computational complexity is close to that of UDP
(23.5min v.s. 20.0min). We also see that although PSDGâ€¡requires
savingğ¾different generators, it attains comparable computational
efficiency to those of PDEN and UDP. During testing, we compute
the total inference time on four target domains, i.e., MNIST-M, USPS,
SYN, and SVHN. We find that the inference time of our method
is shorter than that of COTTA, but longer than Tent and EATA.
We also observe that the accuracy of PSDGâ¨¿is 0.7% higher than
that of SAR with a negligible additional inference time of 0.2s. In
summary, compared with its competitors, our method requires more
computational complexity and computational resources, which is a
limitation of our method.
B MORE ABLATION STUDY
To evaluate the effectiveness of several components in our method,
we propose three variants of PSDGâ€¡, referred as â€œPSDGâ€¡w/o 1â€,
â€œPSDGâ€¡-Lğ‘â€ and â€œPSDGâ€¡-EDâ€, respectively. â€œPSDGâ€¡w/o 1â€ re-
moves the constant 1 from Eq. (6) in StyIN. â€œPSDGâ€¡-Lğ‘â€ directly
maximizes the distance between original and generated images in
Lğ‘instead of minimizing their inverse, â€œPSDGâ€¡-EDâ€ uses Euclidean
distance loss between original and generated features instead of
usingLğ‘“in Eq. (8). The results on three datasets are shown in
Tables 8, 9 and 10, respectively. We see that the performance of
PSDGâ€¡is superior to â€œPSDGâ€¡w/o 1â€, as incorporating constant 1
into Eq. (6) can avoid generating new style data with large seman-
tic shifts. We observe that PSDGâ€¡achieves higher accuracy than
PSDGâ€¡-Lğ‘. We conjecture the reason is that the inverse is easier to
optimize and encourage the generative model to generate data with
more diversity compared to directly maximizing the distance. We
observe that â€œPSDGâ€¡-EDâ€ collapses on Digits and PACS datasets.
The reason is that the strict restriction of Euclidean distance would
force the representation between the original and generated data
to be exactly the same, which reduces diversity within a class and
weakens feature discriminability, thus deteriorating generalization.
C ANALYZATION OF GENERATING STYLES
To further evaluate the significance of StyIN, we quantitatively
demonstrate the diversity of the augmented data by using the vari-
ance diversity metric proposed in [ 19]. The high diversity means
a large value of diversity. Table 11 shows the variance diversity
values of different numbers of augmented domains ( ğ¾) on MNIST.
We consider that the values of variance diversity increase with the
increase in the number of augmented domains when ğ¾â‰¤40. We
also see that the diversity of the augmented data plateaus (with
a slight decrease) when ğ¾>40. We conjecture that the possible
reason is that the representation learning module might not be able
to generate samples with large shifts from the original samples
whenğ¾reaches a certain value. Moreover, we also provide the vari-
ance diversity of PSDGâ€¡-AdaIN in Table 11. The value of variance
diversity of PSDGâ€¡-AdaIN is smaller than that of PSDGâ€¡, which
confirms the effectiveness of StyIN. In addition, we note that the
variance diversity of PSDGâ€¡-AdaIN decreases when ğ¾â‰¥30and
3804KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Shuai Yang, Zhen Zhang, & Lichuan Gu
Figure 4: Example visualization PSDG â€¡and its variants,
which train on Sketch and test on Photo.
Table 11: Variance Diversity of different generators from
different numbers ( ğ¾) of augmented domains.
ğ¾ 10
20 30 40 50
PSDGâ€¡-A
daIN 0.0305
0.0334 0.0338 0.0327 0.0318
PSDGâ€¡0.1026
0.1077 0.1124 0.1163 0.1154
Table 12: Additional loss ablation study on PACS, which
trains on Sketch and test on the other three task.
Lğ‘œLğ‘Lğ‘“Lğ‘P
hoto Art Cartoon A
vg.
%
" " " 55.8
57.9 68.3 60.7
"
% " " 58.0
58.0 66.9 61.0
"
" % " 56.6
59.6 68.2 61.5
"
" " % 53.4
52.8 63.5 56.6
"
" " " 58.9
61.7 68.5 63.0
PSDGâ€¡-A
daIN 46.1
52.3 66.7 55.0
PSDGâ€¡only decreases when ğ¾â‰¥40, which means that StyIN can
continuously generate data with large shifts compared to AdaIN.
D EFFECTIVENESS OF EACH COMPONENT
To further evaluate the contributions of different components of
different PSDGâ€¡, we also present the experimental results of PSDGâ€¡and its variants on PACS in Table 12. We find that the performance
of our method PSDG is significantly influenced by the quality of the
generator StyIN. Pixel maximization loss Lğ‘also has an important
effect on the performance of PSDG by preventing the replication of
identical styles across all images within a domain and promoting the
generation of data with distinct styles compared to previously aug-
mented data at the pixel level. Compared to Lğ‘, the impact ofLğ‘“,
which emphasizes feature differences, on the model has somewhat
diminished. This is because pixel differences intuitively capture
disparities between two images more effectively than feature differ-
ences. Since the generator synthesizes ğ¾new domains, achieving
good performance simply using contrastive learning loss Lğ‘œor
residual uncertainty loss Lğ‘alone is possible. However, combining
Lğ‘œandLğ‘would lead to performance improvement. Although the
improvement is not significant, the modelâ€™s performance is already
excellent. Even a slight enhancement is meaningful. Moreover, we
also provide the class activation maps of PSDGâ€¡and its variants to
visualize the contributions of different components in Fig. 4. We
observe that the absence of any component will change the focus
of the model, decreasing the generalization performance and even
leading to model classification errors. We also see that the class
activation map of PSDGâ€¡is more comprehensive and contains less
style features than that of PSDGâ€¡-AdaIN.
E THE PRACTICAL EFFECTIVENESS AND
POTENTIAL ADVANTAGES OF PSDG
Our proposed method PSDG combines the merits of training-time
learning and test-time learning to enhance the modelâ€™s performance
on the target domain, which is more robust to domain shift com-
pared to solely performing single-domain generalization or test-
time adaptation. As a result, PSDG is expected to be deployed in
real-world environments with large domain shifts. Furthermore,
since training-time learning and test-time learning are independent
of each other, PSDG is suitable for both single domain generalization
and test-time adaptation problems. In addition, the proposed gener-
ator StyIN can produce data with diverse styles, rendering it highly
applicable for other data augmentation methods that integrate a
style generator module. Besides, the proposed residual uncertainty
loss can be extended to other methods aimed at learning feature rep-
resentations. The simple yet effective sample reweighting strategy
proposed during the test-time learning phase can also be applied
to other test-time adaptation methods. In summary, our proposed
method has ventured into new explorations in dealing with domain
shift problems, with the hope of inspiring the community.
3805