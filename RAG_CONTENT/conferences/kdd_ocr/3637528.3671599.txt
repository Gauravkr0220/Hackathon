LiMAML: Personalization of Deep Recommender Models via
Meta Learning
Ruofan Wang
LinkedIn Corporation
Sunnyvale, CA, USA
ruofwang@linkedin.comPrakruthi Prabhakar
LinkedIn Corporation
Sunnyvale, CA, USA
paprabhakar@linkedin.comGaurav Srivastava
LinkedIn Corporation
Sunnyvale, CA, USA
gsrivastava@linkedin.com
Tianqi Wang
LinkedIn Corporation
Sunnyvale, CA, USA
tianqwang@linkedin.comZeinab S. Jalali
LinkedIn Corporation
Sunnyvale, CA, USA
zejalali@linkedin.comVarun Bharill
LinkedIn Corporation
Sunnyvale, CA, USA
vbharill@linkedin.com
Yunbo Ouyang
LinkedIn Corporation
Sunnyvale, CA, USA
youyang@linkedin.comAastha Nigam
LinkedIn Corporation
Sunnyvale, CA, USA
aanigam@linkedin.comDivya Venugopalan
LinkedIn Corporation
Sunnyvale, CA, USA
dvenugopalan@linkedin.com
Aman Gupta
LinkedIn Corporation
Sunnyvale, CA, USA
amagupta@linkedin.comFedor Borisyuk
LinkedIn Corporation
Sunnyvale, CA, USA
fborisyuk@linkedin.comSathiya Keerthi
LinkedIn Corporation
Sunnyvale, CA, USA
keselvaraj@linkedin.com
Ajith Muralidharanâˆ—
Aliveo AI Corp
Sunnyvale, CA, USA
ajithm@gmail.com
ABSTRACT
In the realm of recommender systems, the ubiquitous adoption
of deep neural networks has emerged as a dominant paradigm
for modeling diverse business objectives. As user bases continue
to expand, the necessity of personalization and frequent model
updates have assumed paramount significance to ensure the de-
livery of relevant and refreshed experiences to a diverse array of
members. In this work, we introduce an innovative meta-learning
solution tailored to the personalization of models for individual
members and other entities, coupled with the frequent updates
based on the latest user interaction signals. Specifically, we lever-
age the Model-Agnostic Meta Learning (MAML) algorithm to adapt
per-task sub-networks using recent user interaction data. Given the
near infeasibility of productionizing original MAML-based models
âˆ—Work done while at LinkedIn
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671599in online recommendation systems, we propose an efficient strat-
egy to operationalize meta-learned sub-networks in production,
which involves transforming them into fixed-sized vectors, termed
meta embeddings, thereby enabling the seamless deployment of
models with hundreds of billions of parameters for online serving.
Through extensive experimentation on production data drawn from
various applications at LinkedIn, we demonstrate that the proposed
solution consistently outperforms the baseline models of those ap-
plications, including strong baselines such as using wide-and-deep
ID based personalization approach. Our approach has enabled the
deployment of a range of highly personalized AI models across di-
verse LinkedIn applications, leading to substantial improvements in
business metrics as well as refreshed experience for our members.
CCS CONCEPTS
â€¢Information systems â†’Personalization; Content ranking ;â€¢
Computing methodologies â†’Machine learning algorithms;
Neural networks.
KEYWORDS
Recommendation systems; Meta learning; Hyper-personalization;
Deployment; Few-shot learning
ACM Reference Format:
Ruofan Wang, Prakruthi Prabhakar, Gaurav Srivastava, Tianqi Wang, Zeinab
S. Jalali, Varun Bharill, Yunbo Ouyang, Aastha Nigam, Divya Venugopalan,
Aman Gupta, Fedor Borisyuk, Sathiya Keerthi, and Ajith Muralidharan. 2024.
5882
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruofan Wang et al.
LiMAML: Personalization of Deep Recommender Models via Meta Learning.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671599
1 INTRODUCTION
With the advent of deep learning, neural-network based recommen-
dation models have become very popular for modeling a variety
of objectives at large internet companies, including click-through
rate (CTR) prediction, invite prediction, and visit prediction. It is
important for these models to understand the diverse and evolving
individual needs of millions of members and provide refreshed item
recommendations everyday.
An array of approaches have been proposed to improve the per-
formance of such networks for better personalization. The first set
of approaches propose architectural improvements to learn sophis-
ticated feature interactions [ 2,6,29]. Deeper models with intricate
architectures based on factorization machines have achieved state
of the art performance on several recommender system prediction
tasks. However, they are still global models served across all mem-
bers, entities and items with limited personalization. The second set
of approaches leverage the power of embeddings to handle sparse
categorical feature inputs per user or per entity (e.g. per advertiser,
per job, per industry segment, etc.) [ 14,21,32]. Embedding based
approaches provide some degree of personalization for each user
or entity. But such approaches can only learn reliable embeddings
when there is a significant amount of data per user, limiting their
personalization scope to frequent members on the platform. In
other words, such models are data inefficient during learning.
In general, the definition of personalization varies across differ-
ent objectives and applications. As an example, for an ad - Click
Through Rate (CTR) prediction task, we may want to build person-
alized models for each advertiser ID. For a general user-item CTR
prediction task, we may want to build personalized models per user.
For a model that predicts whether or not a user would apply for a
job, a per-user and job-to-industry segment level personalization
might make more sense. In the meta learning framework1, this
would entail designing different task definitions for different use
cases.
With meta learning, the goal is to quickly and effectively learn
a new task from a small number of data samples using a model
that is learnt on a large number of different tasks. [ 4] proposes a
model-agnostic meta-learning algorithm (MAML) that modifies the
optimization approach so that it can be directly applied to any deep
learning model that is trained with a gradient descent procedure.
Only recently, MAML based approaches have started gaining popu-
larity in recommendation systems for handling cold start scenarios
of CTR prediction models [ 12,17,26]. These approaches try to
learn personalized user preferences by meta learning ID embedding
vectors for cold start items or meta learning final few layers of
the Multi-Layer Perceptron (MLP) based on few user interaction
samples.
In this paper, we extend all of these ideas to provide a general
architecture for personalizing deep recommender models via meta
learning. In our approach, we leverage MAML [ 4] to meta learn a
1See Section 3 for preliminaries on meta learning.part of the network at a per-task level, where tasks could be mem-
bers or entities or a combination of both. The rest of the network
is shared across all tasks and is kept identical to the architecture
for that application. As opposed to the original MAML which is
nearly infeasible to be productionized online in large scale recom-
mendation systems, this approach provides an easy way to deploy
personalized models with hundreds of billions of parameters in pro-
duction by converting the output of the meta-learnt sub-network
into meta embeddings, and providing these embeddings as regu-
lar features while serving the deployed neural network. By doing
meta-finetuning frequently, we present a way to keep the models
refreshed and personalized to the most recent user interactions on
the platform. We demonstrate across several real-world use cases at
LinkedIn that the proposed approach (termed LiMAML) beats the
baseline models across all use cases. Along with business impact, we
dive deep into how this approach truly uplifts model performance
even for infrequent members with limited data, thereby present-
ing a compelling case for adopting a meta learnt personalization
strategy for all recommender system models.
We go over related work in Section 2 and provide preliminar-
ies on meta learning in Section 3. In Section 4, we introduce our
proposed methodology and in Section 5, we share some insights
around LiMAML productionization. Offline experiments and online
A/B testing results are presented in Sections 6 and 7 respectively.
2 RELATED WORK
Deep recommender models usually rely on deep feature interactions
and large ID embedding tables to enable personalization. Feature in-
teraction modules such as Wide-and-Deep Network [ 2], DeepFM [ 6]
and DCN v2 [ 29] effectively fuse user features and entity features to
improve the representation power. Another stream of approaches
attempt to learn a personalized embedding per user or entity by
introducing large cardinality embedding tables [14, 19, 21, 32]. Al-
though the aforementioned recommendation approaches can pro-
vide user-level or entity-level personalization, in industrial-scale
recommender applications with billions of members and entities,
these approaches require large models to incorporate personalized
information and large data to train such models, making it less
feasible in some large scale online recommendation systems due to
limited data availability.
Meta learning, on the contrary, focuses on training models to
adapt to every user or entity quickly when only a few samples are
provided. Most literature on meta learning focuses on vision [ 4,23]
and language domain [ 8], whereas applications of recommender
systems domain have unique challenges since the number of tasks
are much larger than vision and language domain. Meta learning
can be roughly classified into 3 categories: model-based [ 24,26],
metric-based [ 25] and optimization-based [ 4]. Multiple works have
applied metric-based and model-based meta learning for recom-
mender systems [ 13], applied meta learning for scenario specific
task definition [ 3] and applied meta learning in the online set-
ting [ 10,19]. These recommendation approaches focus on small-
scale, non-industrial benchmark datasets, such as movie recommen-
dation [7], book recommendation [33], ads prediction [1].
Model Agnostic Meta Learning (MAML) [ 4] based approaches
have been applied for recommender systems [ 9,11,12,15,28]. For
5883LiMAML: Personalization of Deep Recommender Models via Meta Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
example, MAML-based recommender systems have been utilized
for ads CTR prediction problems [ 17,18]. Meta-Learned User Prefer-
ence Estimator (MeLU) [ 12] performs MAML based task adaptation
on the MLP layers of the model. However, productionizing MeLU
type architectures is challenging because the task adaptation layers
are the last layers in the model architecture, which necessitates the
storing of the personalization weights of all the members. Instead,
we perform task adaptation on a sub-network, which includes only
the first few layers of the network. This sub-network outputs a
fixed size meta learned embedding vector during the inference for
each user, which significantly facilitates model serving.
A few works have applied Meta learning based solutions for
recommender systems on production scale datasets [ 16,31]. How-
ever, these papers do not provide meta learning specific produc-
tionization strategies for industrial scale recommender systems.
G-Meta [ 30] discussed speeding up MAML-based recommender
systems by using GPU parallelism strategies, but other optimiza-
tion strategies can be utilized together with parallelism to further
accelerate meta learning model training.
3 PRELIMINARIES
The central idea of meta learning, also referred to as "learning-to-
learn", is to learn a model ğœƒacross a variety of learning tasks, such
that it is capable of adapting to any new task quickly with only
a small number of data points. There are broadly three types of
meta learning: metric based, model based and optimization based.
In this paper, we adopt optimization based meta learning from the
reference paper [ 4]. In this approach called Model-Agnostic Meta
Learning (MAML), we learn the model parameters ğœƒsuch that the
model has maximal generalization performance on a new task after
the parameters have been updated through one or more gradient
steps to a personalized ğœƒğ‘–starting from ğœƒ.
Formally, let us consider a prediction function ğ‘“ğœƒ:ğ‘¥â†’ğ‘¦which
maps observations denoted by ğ‘¥to outputs denoted by ğ‘¦.ğ‘“can be
any neural network based function approximator with parameters
ğœƒ. Then we define each task asğ‘‡ğ‘–={(ğ‘¥1,ğ‘¦1),(ğ‘¥2,ğ‘¦2),...)}where
ğ‘¥ğ‘—,ğ‘¦ğ‘—are i.i.d. samples from a specific task ğ‘‡ğ‘–. The lossLğ‘‡ğ‘–(ğ‘“ğœƒğ‘–)
provides task-specific feedback based on the problem type using the
task-specific model weights ğœƒğ‘–. For a binary classification problem,
it can be cross entropy loss, and for a regression problem, it can be
mean-squared error (MSE) loss. We want the meta learnt model to be
performing well on a distribution of learning tasks ğ‘(T). The entire
dataset is therefore constructed as a set of tasks {ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘}
whereğ‘is the number of tasks in total and each ğ‘‡ğ‘–âˆ¼ğ‘(T)refers
to a single task with all data points under task ğ‘–. Each task level
datağ‘‡ğ‘–is further split into two parts: support set and query set.
The support set is utilized for task-level personalization and query
set is utilized for maximizing generalization performance across
tasks. This will become clearer as we describe the steps of MAML
given in Algorithm 1[4].
The MAML algorithm mainly involves two stages: task adap-
tation phase (Lines 4 - 10 of Algorithm 1), henceforth called the
inner loop and meta-optimization phase (Line 11 of Algorithm 1),
henceforth called the outer loop. The goal of the inner loop is to
learn task-level personalization. It does so by minimizing the loss
on each taskâ€™s support set data by performing gradient updates ğ‘›times to obtain a set of personalized (fine-tuned) model weights
ğœƒğ‘–per task. Note that at a task-level, the learning process presents
an over-parameterized problem, with multiple solutions for ğœƒğ‘–that
can minimize the loss on the support set. However, this algorithm
restricts the solution space by bootstrapping from ğœƒas the starting
point to learn ğœƒğ‘–, creating a strong dependence of ğœƒğ‘–onğœƒ. The step
sizeğ›¼is the task learning rate. âˆ‡ğœƒrefers to the gradient w.r.t. ğœƒ. The
inner loop is also sometimes referred to as task-level fine-tuning as
we fine tune ğœƒto learn personalized model parameters ğœƒğ‘–for each
task using a few samples from it.
The goal of the outer loop is to update the model parameters
ğœƒsuch that it can maximize the generalization performance on a
wide variety of tasks. This is achieved by doing gradient update [ 4]
ofğœƒusing the losses computed on the query sets of each task from
the per-task model parameters ğœƒğ‘–. Note that this gradient update
is done using all tasks since ğœƒis shared across all tasks. ğ›½is the
learning rate used in the meta optimization step, also known as
global learning rate, and is usually different from the task learning
rateğ›¼. The minimization of the losses of different ğœƒğ‘–s computed
on the query sets represents the maximization of generalization
performance across tasks. The outer loop update involves a gradient
through a gradient computation, which requires Hessian-vector
products computation.
In essence, we learn the model parameters ğœƒsuch that the model
has maximal generalization performance on any new task after
the parameters for that task are bootstrapped from ğœƒand updated
through one or more task-level gradient steps to a personalized ğœƒğ‘–.
Algorithm 1 Original MAML Algorithm [4]
Require:ğ‘(T): distribution over tasks
Require:ğ›¼,ğ›½: step size/learning rate hyperparameters of inner and outer
loop
Require:ğ‘›: number of times to repeat the inner loop gradient updates
1:randomly initialize ğœƒ
2:while not done do
3: Sample batch of tasks ğ‘‡ğ‘–âˆ¼ğ‘(T)
4: for allğ‘‡ğ‘–do
5:ğœƒğ‘–â†ğœƒ
6: repeatğ‘›times
7: Evaluateâˆ‡ğœƒğ‘–Lğ‘‡ğ‘–(ğ‘“ğœƒğ‘–)with support set
8: ğœƒğ‘–â†ğœƒğ‘–âˆ’ğ›¼âˆ‡ğœƒğ‘–Lğ‘‡ğ‘–(ğ‘“ğœƒğ‘–)
9: end
10: end for
11: Updateğœƒâ†ğœƒâˆ’ğ›½âˆ‡ğœƒÃ
ğ‘‡ğ‘–âˆ¼ğ‘(T)Lğ‘‡ğ‘–(ğ‘“ğœƒğ‘–)with query set
12:end while
4 METHODOLOGY
In this section, we start with discussing how MAML can be designed
for applications in recommender systems, specifically choosing the
right task definitions and loss functions. We will then present the
difficulties of deploying MAML based solutions as is in production.
After that, we introduce our proposed method: LiMAML, which
preserves the benefits from meta learning while also ensuring a
scalable and production-friendly approach to personalization.
5884KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruofan Wang et al.
4.1 Task definition in MAML
Defining the task is one of the critical first steps in formulating
the problem via meta learning. The goal of personalizing a model
via meta learning is to effectively and quickly learn to produce
the fine-tuned network weights for each new task. If the learning
objective is to predict CTR on an item from a user, each user or user
segment could be a natural choice of task definition for this prob-
lem. In general, the input to the neural network in recommender
systems usually consists of a set of one or more entities and their
corresponding features. Examples of entities include job ID, adver-
tiser ID, viewer ID, etc. One or more combination of entities or
their segmentation would be a good choice in designing the task for
meta learning. If we treat each viewer ID as a task, then ğ‘‡ğ‘–includes
all data points for a particular viewer and the outcome of meta
learning would be to produce per-viewer personalized networks
based on most recent viewer interaction data.
4.2 Loss functions for a task
The next step would be to define the loss function for the learning
problem. In recommender systems, the prediction function ğ‘“ğœƒ:ğ‘¥â†’
ğ‘¦is usually a binary classifier probability predicting a user response
given a set of features. ğ‘¥is a collective feature set including several
dimensions such as user, item, context, etc. ğ‘¦represents the user
response, for example, whether a user clicked on a recommended
item.ğ‘“can be of any form. In this paper, we assume ğ‘“to be a neural
network based function approximator. Given ğ‘¦is a binary signal in
general,Lcan be cross-entropy loss as defined below. Please note
that in MAML, the losses are defined at a task level.
Lğ‘‡ğ‘–(ğ‘“)=âˆ‘ï¸
ğ‘—
ğ‘¦ğ‘—log(ğ‘“(ğ‘¥ğ‘—))+( 1âˆ’ğ‘¦ğ‘—)log(1âˆ’ğ‘“(ğ‘¥ğ‘—))
(1)
4.3 LiMAML
With the MAML algorithm, by treating each entity as a task, we
can build a personalized model where each entity has its own set of
values for the fine-tuned model parameters. In order to deploy such
a model in practice, we would either have to store the personalized
per-entity network weights and serve them during inference or
would have to do the fine-tuning during inference in real time. Both
of these solutions are infeasible to be deployed into production in
online recommendation systems mainly from two aspects.
Storage: If we treat each user or each entity as a task, the number
of tasks can be extremely large, running in the order of millions or
more. For example, there are over 1 billion members of LinkedIn
and it is extremely expensive to store 1 billion ğœƒğ‘–per user. Hence,
storing the full ğœƒğ‘–âˆ€ğ‘–would be infeasible from a storage perspective.
Latency: Recommendations are made online in near real time.
Performing task-level fine-tuning during an inference call could
cause significant inference latency with high computation cost.
Hence, doing real time fine-tuning would be infeasible from a com-
pute perspective.
These two considerations inspired us to come up with an ef-
fective variant of the original MAML for LinkedIn recommender
applications, called LiMAML. It still possesses the capability of per-
sonalization, offering well-tailored and frequently refreshed modelsper entity, but more importantly, provides a much easier frame-
work to productionize and serve at a larger scale. Algorithms 2a
and 2b provide an overview of the algorithm. Figure 1 provides the
network structure of LiMAML.
Algorithm 2a LiMAML: Training
Require:ğ‘(T): distribution over tasks
Require:ğ›¼,ğ›½: step size/learning rate hyperparameters of inner and outer
loop
Require:ğ‘›: number of times to repeat the inner loop gradient updates
1:randomly initialize ğœƒmeta,ğœƒglobal
2:while not done do
3: Sample batch of tasks ğ‘‡ğ‘–âˆ¼ğ‘(ğ‘‡)
4: for allğ‘‡ğ‘–do
5:ğœƒmetağ‘–â†ğœƒmeta
6: repeatğ‘›times
7: Evaluateâˆ‡ğœƒmetağ‘–Lğ‘‡ğ‘–(ğ‘“ğœƒmetağ‘–)with support set
8: ğœƒmetağ‘–â†ğœƒmetağ‘–âˆ’ğ›¼âˆ‡ğœƒmetağ‘–Lğ‘‡ğ‘–(ğ‘“ğœƒmetağ‘–)
9: end
10: end for
11: Updateğœƒmetaandğœƒglobal with query set:
12:ğœƒmetaâ†ğœƒmetaâˆ’ğ›½âˆ‡ğœƒmetaÃğ‘
ğ‘–=1Lğ‘‡ğ‘–(ğ‘“ğœƒmetağ‘–)
13:ğœƒglobalâ†ğœƒglobalâˆ’ğ›½âˆ‡ğœƒglobalÃğ‘
ğ‘–=1Lğ‘‡ğ‘–(ğ‘“ğœƒglobal)
14:end while
Algorithm 2b LiMAML: Meta Embedding Generation
Require:ğ‘˜: number of times to repeat the fine-tuning gradient updates
1:for eachğ‘‡ğ‘–âˆˆTdo
2:ğœƒmetağ‘–â†ğœƒmeta
3: repeatğ‘˜times
4: Evaluateâˆ‡ğœƒmetağ‘–Lğ‘‡ğ‘–(ğ‘“ğœƒmetağ‘–)with recent samples of ğ‘‡ğ‘–
5:ğœƒmetağ‘–â†ğœƒmetağ‘–âˆ’ğ›¼âˆ‡ğœƒmetağ‘–Lğ‘‡ğ‘–(ğ‘“ğœƒmetağ‘–)
6: end
7: Score the most recent sample ğ‘¥ğ‘–usingğœƒmetağ‘–to obtain the output
of the meta block (meta embedding) ğ¸ğ‘–
8:end for
Figure 1: LiMAML Model Structure
5885LiMAML: Personalization of Deep Recommender Models via Meta Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.3.1 Network Structure. In this approach, we divide the network
into two blocks - Meta Block andGlobal Block. Meta block,
whose parameters are denoted by ğœƒmeta, is the sub-network that
we will meta learn using MAML approach. Hence, for every task,
we will start with ğœƒmeta, and fine-tune it to produce personalized
sub-networks ğœƒmeta 1,ğœƒmeta 2, ...,ğœƒmeta ğ‘forğ‘tasks. Global block,
whose parameters are denoted by ğœƒglobal , is the sub-network that
is shared across all tasks and is usually equivalent in architecture
to the network structure currently deployed for an application. We
will split the input features for each task into two categories. Meta
Features are the input to the meta block, which usually contains
features specific to the entity which forms the task definition. For
example, if we are meta learning the sub-network per user, meta
features are the user features for each item in the task. Other
Features refer to all other features input to the global model. In
our applications of LiMAML, meta features can also be fed directly
into the global block as well. Global block additionally takes in
the output of the meta block, referred to as Meta Embedding, as
additional inputs along with other features. In our experiments,
we simply concatenated meta embeddings with other features and
provided that as input to the Global block. In essence, the meta
block provides the personalized embeddings per task as additional
input signals to the global block.
4.3.2 Training. Algorithm 2a provides the steps to train LiMAML.
In the inner loop (Lines 4 - 10), we only update the meta block
parameters ğœƒmeta ğ‘–for each task ğ‘‡ğ‘–using the support set data from
each task. In the outer loop (Lines 11 - 13), both meta block parame-
tersğœƒmeta and global block parameters ğœƒglobal are updated with the
query set of training data. For the meta block parameter update, the
loss for the gradient is computed using each taskâ€™s fine-tuned model
parametersğœƒmeta ğ‘–. For the global block parameter update, the loss
for the gradient is computed using the model parameters ğœƒglobal .
By the end of training, we have learned a set of model parameters
âŸ¨ğœƒmeta,ğœƒglobalâŸ©for both blocks. ğœƒmeta is obtained by training against
a variety of tasks and therefore capable of adapting quickly to any
new or old task given just a few data points.
4.3.3 Meta Embedding Generation. The central idea of LiMAML is
to decouple serving of meta block and global block in production.
Meta block is served offline, whereas Global block is served online
during inference. In order to do this, we create a separate flow
called Meta Embedding Generation, which is run on a regular
cadence (say, once per day) to do fine-tuning of the meta block to
output meta embeddings. Algorithm 2b provides the set of steps
for updating ğœƒmeta frequently via meta fine-tuning and producing
embeddings for online serving of the global block. In lines 3-6,
we use recent samples of each task to update ğœƒmeta and obtain
ğœƒmeta ğ‘–for that task by taking ğ‘˜gradient descent steps. Note that
the number of gradient steps ğ‘˜can be different from the number of
inner loop gradient steps ğ‘›taken during training. Every time we run
the meta embedding generation flow, we bootstrap the meta block
parameters with ğœƒmeta as indicated in line 2 of the algorithm. After
gettingğœƒmeta ğ‘–for each task, we then immediately score the meta
block withğœƒmeta ğ‘–as the model parameters using the most recent
sampleğ‘¥ğ‘–for that task as shown in line 7. We then store the output
of the meta block as meta embedding ğ¸ğ‘–for that task. Note that the
input to the meta block only contains entity-specific features anddo not contain any other item specific features for a task. Hence,
scoring the personalized meta block with the most recent sample ğ‘¥ğ‘–
would correspond to scoring with the latest entity specific features
and obtaining an embedding for that entity. In our experiments, we
also tried variants of this approach such as mean pooling of meta
embeddings across multiple samples of a task and found little to no
difference in the performance of our model. Instead of persisting
all the updated task-level model parameters, we only store the
output of meta block, which is a fix-sized vector, so called meta
embedding. These embedding vectors will be persisted and stored
in a feature store for retrieval during online inference. This is an
extremely important change as it reduces the required storage from
a set of model weights per task (quadrillions of parameters) to an
embedding vector per task (tens of billions of parameters). Global
Block is served online as per the usual deployment and inference
process. When a new scoring request comes in, we retrieve all
features as well as the latest version of meta embeddings from the
feature store, and score them with the global block ğœƒglobal .
We highlight the differences between the original MAML and
our approach LiMAML in Table 1. In summary, the network is
decoupled into two blocks: Meta Block and Global Block. With
LiMAML, some of the key advantages include
â€¢We are able to preserve the personalization capability, while
enabling easier deployment of large scale models.
â€¢Given that meta block is served offline using a sequence of sam-
ples per task, we can use any simple to complex architecture
for personalization such as ID embedding layer, dense MLP or a
transformer for the meta block.
â€¢We can easily refresh a part of the model at a regular cadence and
adapt it to the recent user interactions occurring on the platform.
Original MAML LiMAML
Network not decoupled decoupled into Meta Block
and Global Block
Training entire network is meta
learnedonly Meta Block is meta
learned
Serving entire network needs to be
served onlinehybrid serving, meta block
is served offline, global
block is served online
Storage required to store all model
parameters for all tasksstore an embedding vector
per task
Latency huge inference latency if
fine-tuning happens onlineno inference latency over-
head as fine-tuning hap-
pens offline
Table 1: Original MAML v.s. LiMAML.
5 PRODUCTION INSIGHTS
Serving refreshed meta embeddings via frequent fine-tuning brings
in additional complexities to the data and model pipelines in pro-
duction, especially when the number of tasks runs to the order of
hundreds of millions. In this section, we present some insights on
how we designed our pipelines and some lessons we learnt while
deploying LiMAML to production.
Every day, there are two pipelines running in production: a data
pipeline and an inference pipeline. As members are interacting
with the platform, we first needed a daily data processing pipeline
5886KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruofan Wang et al.
that collects these interactions and creates labeled data by joining
with feature datasets. We follow this with an inference pipeline
that collects the most recent ğ‘‹days of data, groups it on a task
level, and performs meta fine-tuning to produce meta embeddings
for all tasks. ğ‘‹could range from 2 weeks to 2 months depending on
the application and the scale of its dataset. These embeddings are
then pushed to a feature store for online inference usage. Global
block is the model deployed online for inference, which fetches all
features from the feature store as well as the pre-populated meta
embeddings to make the final prediction.
In terms of the cadence of refreshing both blocks, the model
parameters ğœƒmeta andğœƒglobal are usually updated as per the reg-
ular retraining schedules of different applications. For most use
cases, we do not frequently update them because we donâ€™t observe
significant performance downgrade over time. However, we run
the embedding generation flow daily, bootstrapping from ğœƒmeta to
produce a fine tuned meta block ğœƒmeta ğ‘–using the latest user inter-
action data. The primary purpose of the meta block is to capture
the fast evolution of user interest and provide personalization of
the model. We expect the meta embeddings populated on a daily
basis to provide a refreshed representation of each entity or user
adapted from their latest engagement history. In our experiments
on Push Notifications CTR prediction problem (pTap), updating
the embeddings weekly instead of daily caused a drop in offline
AUC by 0.5%, indicating that refreshed embeddings are critical to
maintaining meta learned model performance online.
6 OFFLINE EXPERIMENTS
In this section, we present offline results of LiMAML on various
applications at LinkedIn. We begin by providing an overview of
all applications and datasets in Section 6.1. Then we use one of
the applications as an example for a deep dive in Section 6.2. We
then summarize results of other applications in Section 6.3. Some
techniques to speed up LiMAML training is presented in Section
6.4 and an ablation study is provided in Section 6.5.
6.1 Applications and datasets
We evaluate LiMAML across the following applications at LinkedIn.
â€¢CTR Prediction of Push Notifications (pTap): This model
predicts the probability of members tapping on a push noti-
fication card sent to their mobile devices from LinkedIn. We
define the tasks at a user level, i.e. each recipient of the push
notification becomes a new task.
â€¢CTR Prediction of InApp Notifications (pClick): This model
predicts the probability of members clicking on a notification
shown to the members on the in-app notifications tab within
the LinkedIn app. For this model, we experiment with two
different task definitions - a per-user task definition as well
as a per-(user, notification type) task definition.
â€¢CTR Prediction of People You May Know Recommendations
(pInvite): This model predicts the probability of a user send-
ing a connection invite to the recommended person on My
Network tab within the LinkedIn App. This model takes in
two different entities and their features as input. The first
entity is the user, also called as the inviter, who sends the con-
nection invite and second entity is the invitee, who receivesthe connection invite. We experiment with both entities as
the task definitions for this problem.
Application Task Definition Number of Tasks
pTap per user tens of millions
pClick per user several millions
pClick per (user, notification type) several millions
pInvite per inviter tens of millions
pInvite per invitee tens of millions
Table 2: Application, Task Definition and Dataset Size across
all applications.
We describe the scale of these datasets in Table 2. For all appli-
cations, the date range used to derive the training data is prior to
the date range used for validation data. Similarly, the validation
data is temporally before the test data. As each task has a varying
number of samples, we also limit the number of samples per task
to an upper bound by keeping only the most recent samples. When
splitting the training dataset into support and query sets, we sort
the samples for each task chronologically over time and assign
the first 75% of the samples to the support set, and the last 25%
to the query set. This is done to prevent any form of information
leakage. Validation data is used for task-level fine-tuning and test
data is used for final evaluation and metrics reporting. They can be
regarded as the support and query set during inference time.
6.2 LiMAML on pTap
6.2.1 Experiment Setup. The baseline model for pTap uses an MLP
network. We compare three different algorithms (described below)
on this dataset. For each algorithm, we also wish to demonstrate
the value of fine-tuning tasks on recent validation data.
â€¢Vanilla Training: In this approach, we train the neural network
with regular gradient descent procedure using optimizers such
as Adam. Usually, the samples in the training data are randomly
shuffled and a mini-batch of samples are provided at a given
time for training. However, in our experiments, we group the
training data at a task-level first and then provide a mini-batch
of tasks for training. This is done to provide a fairer and stronger
baseline to compare with MAML. In the no-fine-tune scenario,
we evaluate the same trained model on all tasks on the test data.
In the fine-tune scenario, we use the trained model as bootstrap
to take a few gradient descent steps on the validation data for
each task, and evaluate this per-task model on the test data.
â€¢Entire Network MAML: In this approach, we use the MAML al-
gorithm from [ 4] on the entire MLP network. In the no-fine-tune
scenario, we evaluate the trained model output from the MAML
algorithm on all tasks on the test data. In the fine-tune scenario,
we use the trained model as bootstrap to do task adaptation on
the validation data for each task, and evaluate this per-task model
on the test data.
â€¢LiMAML: In this approach, we use the LiMAML algorithm de-
scribed in 2a to meta learn part of the network. The meta block
is set to a smaller MLP architecture in comparison to the global
block. Referring to Figure 1, meta features include all user-specific
features, and, we use both meta features as well as the rest of
the features as other features. The setting up of fine-tune and
5887LiMAML: Personalization of Deep Recommender Models via Meta Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
no-fine-tune are similar to Entire Network MAML, except that
the task-adaptation (Line 4-8 of Algorithm 2a) is done only for
the meta block on the validation data for each task.
In Section 2 we reviewed several works that utilize MAML based
optimization for recommender systems [ 9,11,12,15,17,18,28].
However, we donâ€™t consider these works as suitable baselines be-
cause they donâ€™t offer scalable productionization strategies. For
instance, MeLU [ 12] applies MAML only on the last layers of the
network, which makes it nearly infeasible to productionize. Li-
MAML specifically aims to develop a strategy to productionize
MAML algorithm for online recommender systems.
For all experiments, we report AUC (Area under the ROC Curve)
gain (or loss) in comparison to the contextual baseline on the test
dataset. Across all our experiments, we usually observe the relative
standard deviation of AUC gain over multiple runs to be less than
0.02%. For the outer loop of both variants of MAML, we use the same
optimizer as Vanilla Training baseline. In addition to the results on
all tasks in the test data, we also slice the results into tasks with
less than 25 samples and tasks with greater than 25 samples (in test
data). This is done to highlight the efficacy of our approach even
on tasks with smaller number of samples.
6.2.2 Experiment Results. We present the results of our experi-
ments in Table 3. As we can see, MAML with Fine Tuning gives
significant gains in AUC in comparison to Vanilla Training without
and with fine-tuning. This indicates that the model parameters out-
put from the MAML algorithm provides a good bootstrap starting
point to quickly and effectively adapt to any task. Additionally,
the model parameters output from MAML are not expected to be
at any optimal point, and hence we see that MAML without fine-
tuning performs the worst among all approaches. Even though
MAML gives the highest gains on tasks with larger number of
samples, tasks with smaller number of samples do see significant
gains as well. We also observe that LiMAML achieves comparable
gains with Entire Network MAML, while additionally providing
a production-friendly approach to personalization of large-scale
online recommender models.
6.2.3 Comparison with Wide-and-Deep ID embedding models. Here,
we want to compare a MAML based personalization strategy with
another popular personalization strategy in recommender models
using ID embeddings. For this experiment, we construct a base-
line pTap model with a wide-and-deep architecture [ 2] with ID
embeddings for every user ID. These embeddings are learnt via an
embedding-lookup layer and are then concatenated with other
dense features and passed through an MLP network. We com-
pare two variants of LiMAML-based models - one with a learnable
embedding-lookup layer as the meta block and the other with a
small MLP as the meta block. The global block is kept identical to
the baseline network without the ID embedding layer. In Table 4,
we present the AUC results of the baseline network as well as the
two LiMAML variants on test data. For the baseline network, we
report results without fine-tuning since ID embeddings provide the
personalization component. For the LiMAML variants, we report
results on the test data after task adaption on validation data at
a user-level. As we can observe from this experiment, LiMAML
proves to be an effective strategy for personalizing a recommendermodel, with different meta block architectures - MLP layers and ID
embeddings layers. In our future work, we plan to explore using
more complex architectures, such as transformers [ 27], as the meta
block, to understand the performance across different architectures.
6.3 LiMAML on other applications
In Table 5, we list the performance of LiMAML with different task
definitions across several other applications. Baseline model for
all experiments use the deep neural network architecture for that
application. We train the baseline model using Vanilla Training by
providing a mini-batch of tasks (samples grouped at a task level).
We observe consistent AUC gains from LiMAML (with task adap-
tion) for most applications in comparison to the baseline (without
fine-tune). This indicates that the proposed approach provides a
robust personalization paradigm for all recommender system mod-
els. Additionally, we also learn the significance of choosing the
right task definition in meta learning. As seen in pClick, a broader
task definition at a user level provides little to no gains, whereas
a fine-grained task definition at a (user, notification type) level
has a higher gain. We have observed such patterns across many
applications.
6.4 Training speed-up
LiMAML training is computationally more expensive compared to
Vanilla Training due to the additional inner loop task adaptation
step resulting in Hessian-vector product computation [ 4]. On pTap,
training with 5 inner loop iterations resulted in 221% increase in
training time over Vanilla Training (see Table 9 in Section A.1). We
present some techniques which helped us significantly reduce the
training time.
Firstly, we increased the number of tasks per batch per GPU from
32 to 128. As evidenced in [ 5], simply increasing the batch size can
potentially introduce training instability. Therefore, we employed
following techniques to mitigate the challenges with large batch
training:
â€¢We linearly scaled the global learning rate as we scaled the num-
ber of tasks per batch [ 5]. We experimented with different values
around the scaled learning rate to choose the optimal setting.
â€¢We clipped the gradient norm to 1.0during the outer loop.
â€¢We utilized global learning rate scheduling with warm-up and
decay.
With these changes, LiMAML could achieve the same offline metrics
with 46.85% reduction in training time. We observed that learning
rate scheduling and gradient clipping play a pivotal role in achieving
faster convergence speed.
Secondly, we utilized multi-GPU training in a data parallel para-
digm with gradient synchronization between different GPUs. The
gradient synchronization is done only during the outer loop but not
during the inner loop updates. Multi-GPU training along with the
large batch size optimization gave us a 64.28% reduction in training
time.
Thirdly, we realized that the number of inner loop gradient
updates,ğ‘›, significantly affects the training time. We didnâ€™t observe
a significant difference in AUC gain as we vary ğ‘›. Hence, we change
ğ‘›to 1 to achieve further speed-up in training (refer to Appendix A.1
5888KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruofan Wang et al.
Algorithm Vanilla Training Entire Network MAML LiMAML
Fine Tune No Yes No Yes No Yes
All tasks in Test Data baseline +0.75% -0.18% +1.68% -0.43% +1.47%
Tasks with Less Than 25 samples baseline +1.09% -0.29% +1.84% -0.36% +1.38%
Tasks with More Than 25 samples baseline +1.63% -0.26% +2.83% -0.40% +1.64%
Table 3: AUC Gains (over baseline) From LiMAML Experiments on pTap.
Global Block Meta Block Test AUC Gain
MLP + ID Embeddings N.A. baseline
MLP ID Embeddings +1.30%
MLP MLP +1.36%
Table 4: LiMAML compared with ID embedding based ap-
proach.
Application Task Definition Test AUC Gain
pInvite per inviter +10.56%
pInvite per invitee +0.58%
pClick per user +0.01%
pClick per (user, notification type) +0.47%
Table 5: AUC Gains (over baseline) from LiMAML Experi-
ments on all other applications.
for details). Overall we achieve 77% training speedup by applying
all the above techniques. We summarize these results in table 6.
Technique Train Time Reduction
LiMAML baseline
+Large batch size, learning rate opti-
mization, gradient clipping-46.85%
+Multi-GPU training -64.28%
+Single inner loop iteration -77.19%
Table 6: LiMAML training time reduction after applying var-
ious techniques.
6.5 Ablation study
In this section, we provide some insights on the sensitivity of differ-
ent hyper-parameters and some observations from our experiments
with LiMAML.
â€¢Among the hyper-parameter choices, we observed a relatively
higher sensitivity to global learning rate, which requires some
tuning to see gains. In Appendix A.3, we present some experi-
ments on pTap and pClick to illustrate this.
â€¢We experimented with applying dropout after each MLP layer.
However, even a dropout rate as low as 0.1 resulted in a drop
in test AUC. We postulate that this is the result of using large
training dataset with tens or hundreds of millions of tasks which
prevents the meta learn model from overfitting. More details
about this experiment is presented in Appendix A.2.
â€¢In Algorithm 2b, we score the latest sample for each task with
the fine-tuned meta block to produce meta embeddings. We also
investigated the impact of different aggregation methods such
as max, mean pooling to produce these embeddings and they all
performed poorer than using the latest sample. More details are
presented in A.4.7 ONLINE EXPERIMENTS
7.1 Online A/B test results
We have deployed LiMAML based pClick and pTap models for
evaluating the propensity of clicking and tapping on notifications.
These predicted CTRs are important components in the offline
reinforcement learning based decision making system [ 20] to make
notification send/drop decisions. Both pClick and pTap models were
experimented separately online with their respective deep neural
network as the baselines. We evaluated the performance of these
models on the following metrics.
â€¢Weekly Active Users (WAU): The number of unique members
who have visited the LinkedIn site within a seven-day period.
This is one of the most important metrics to drive long term
value on our platform.
â€¢Notifications Click-Through Rate (CTR): The average click-
through rate of notifications sent to the members on a daily
basis. This is an important metric to measure the relevance of
notifications sent to the members.
CTR WAU
pClick +2.0% +0.1%
pTap +0.6% +0.2%
Table 7: LiMAML relative metrics improvement from online
experiments.
All A/B tests have been conducted online with production traffic,
lasting a week. The results are reported in table 7. All numbers
are statistically significant (ğ‘-value <0.05). As shown in the table,
LiMAML based models have not only improved the relevance of
notifications sent to the user, but also driven long term value to
members on LinkedIn.
7.2 Analysis of online results
Frequent members of the platform are usually over-represented
in the training datasets across most applications. Hence, we have
seen modeling efforts seldom showing improved performance on
members with little to no data in the training set. However, with
a meta learning based approach, we expect to see personalization
of models even on tasks with very few (>=1) samples. We evaluate
online model performance on three select user cohorts who are
infrequent members of LinkedIn, whom we have seen in our data
to have very few samples.
â€¢Cohort 1: Members who have visited at least once a week in one
of the past four weeks. These users can be roughly regarded as
Monthly Active Users.
â€¢Cohort 2: Members who have not visited in the past 28 days but
have visited in the last 3 months.
5889LiMAML: Personalization of Deep Recommender Models via Meta Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢Cohort 3: Members who have signed up newly on our platform in
the last four weeks. These membersâ€™ data have not been included
in training the LiMAML model.
Metric Cohort 1 Cohort 2 Cohort 3
pClickCTR +7.7% +4.9% neutral
WAU +0.4% neutral neutral
pTapCTR +3.7% +11.2% +5.3%
WAU +0.5% +0.7% neutral
Table 8: For cohorts with fewer data points, meta learning
shows significant relative metrics lift.
In Table 8, we present online A/B test results for these three
selected user cohorts. From the table, we can observe that LiMAML
models have shown huge online CTR gains as well as long term
engagement (WAU) on these cohorts. These gains are large in com-
parison to what we usually observe for these cohorts in our model-
ing experiments. Additionally, in one case, we see significant gains
in Cohort 3, whose tasks are not present during training LiMAML.
This is stemming from the fact that meta learning significantly up-
lifts the model performance even for members with little to no data,
thereby enhancing user segments which were previously under-
optimized by the global models. These results also indicate that
unlike ID embedding based approaches which usually require a
decent amount of data per entity, meta learning can adapt very well
to any new or existing task given a few data points, making it a
more generalized and effective personalization strategy.
8 CONCLUSION
In this paper, we introduced a meta learning method called Li-
MAML, which provides a generalized framework for personaliza-
tion of recommender models. We have done extensive experiments
to demonstrate significant offline and online metric lifts against
state-of-the-art recommendation models when the framework is
deployed to different LinkedIn applications. We have also shown
the efficacy of our approach on tasks with very little data.
While LiMAML is a robust framework, it naturally comes with
challenges that underscore opportunities for refinement:
â€¢Task Definitions: The assumption that tasks are related and can
be learned using the same meta block is fundamental to LiMAML.
While this has generally yielded strong generalization capabili-
ties, it poses challenges when tasks are not well-defined or overly
broad, as this can affect model performance. This observation
invites further exploration into how task definitions influence
outcomes, providing a path for optimizing task granularity.
â€¢Number of Tasks: The effectiveness of LiMAML is contingent
on the diversity and number of tasks it is trained on. In cases
where there is only a limited number of tasks (e.g. geo based co-
hort of users as tasks), LiMAMLâ€™s performance has not surpassed
traditional training methods.
For future work, we want to explore different extensions of
this approach, along with new applications. Firstly, we want to
integrate LiMAML with our in-house foundation models, such as
Large Language Models, Reinforcement Learning Agents and Graph
Neural Networks. Secondly, many applications contain a flavor of
multiple entities for which we need to achieve personalizationsimultaneously. For example, an ad CTR prediction problem might
want a model personalized per user as well as per advertiser, but a
task definition of user-advertiser pair may not make sense. Similarly,
for a notifications CTR prediction problem, we might want a user
task as well as a user-notification type task simultaneously. We
are exploring ways to extend LiMAML for multiple simultaneous
task definitions, either via multiple meta-blocks or via intelligently
combining different task distributions. Thirdly, we are exploring
different architectures such as a transformer for the meta block.
Given that the meta block is trained on a chronological sequence of
user interaction data, a sequential architecture might give higher
gains while personalizing on such a data.
9 ACKNOWLEDGMENTS
The authors would like to thank Mohsen Jamali, Shipeng Yu, Angus
Qiu, Viral Gupta, Akashnil Dutta, Parag Agrawal, Xiaobing Xue
and others who collaborated with us, and Kenneth Tay, Ruoying
Wang, Ankan Saha for reviewing the paper and providing insightful
suggestions.
REFERENCES
[1]Yi Wang Aden. 2012. KDD Cup 2012, Track 2. https://kaggle.com/competitions/
kddcup2012-track2
[2]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7â€“10.
[3]Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and Jie Tang. 2019.
Sequential scenario-specific meta learner for online recommendation. In Proceed-
ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining. 2895â€“2904.
[4]Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-
Learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th In-
ternational Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 1126â€“1135.
https://proceedings.mlr.press/v70/finn17a.html
[5]Priya Goyal, Piotr DollÃ¡r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate,
large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677
(2017).
[6]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[7]F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History
and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015),
1â€“19.
[8]Nathan Hu, Eric Mitchell, Christopher D Manning, and Chelsea Finn. 2023. Meta-
Learning Online Adaptation of Language Models. arXiv preprint arXiv:2305.15076
(2023).
[9]TIANZE HU. 2021. Hybrid Meta-Learning for Cold-Start Recommendation.
(2021).
[10] Minseok Kim, Hwanjun Song, Yooju Shin, Dongmin Park, Kijung Shin, and Jae-
Gil Lee. 2022. Meta-learning for online update of recommender systems. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 4065â€“4074.
[11] Minchang Kim, Yongjin Yang, Jung Hyun Ryu, and Taesup Kim. 2023. Meta-
Learning with Adaptive Weighted Loss for Imbalanced Cold-Start Recommenda-
tion. arXiv preprint arXiv:2302.14640 (2023).
[12] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019.
Melu: Meta-learned user preference estimator for cold-start recommendation.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 1073â€“1082.
[13] Mi Luo, Fei Chen, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Jiashi Feng,
and Zhenguo Li. 2020. Metaselector: Meta-learning for recommendation with
user-level adaptive model selection. In Proceedings of The Web Conference 2020.
2507â€“2513.
[14] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-
Jean Wu, Alisson G Azzolini, et al .2019. Deep learning recommendation model
for personalization and recommendation systems. arXiv preprint arXiv:1906.00091
(2019).
5890KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruofan Wang et al.
[15] Krishna Prasad Neupane, Ervine Zheng, Yu Kong, and Qi Yu. 2022. A dynamic
meta-learning model for time-sensitive cold-start recommendations. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, Vol. 36. 7868â€“7876.
[16] Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Li Li, Kun Zhang, Jinmei Luo, Zhaojie
Liu, and Yanlong Du. 2021. Learning graph meta embeddings for cold-start ads in
click-through rate prediction. In Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 1157â€“1166.
[17] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm
up cold-start advertisements: Improving ctr predictions via learning to learn id
embeddings. In Proceedings of the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval. 695â€“704.
[18] Haoyu Pang, Fausto Giunchiglia, Ximing Li, Renchu Guan, and Xiaoyue Feng.
2022. PNMTA: A pretrained network modulation and task adaptation approach
for user cold-start recommendation. In Proceedings of the ACM Web Conference
2022. 348â€“359.
[19] Danni Peng, Sinno Jialin Pan, Jie Zhang, and Anxiang Zeng. 2021. Learning
an adaptive meta model-generator for incrementally updating recommender
systems. In Proceedings of the 15th ACM Conference on Recommender Systems.
411â€“421.
[20] Prakruthi Prabhakar, Yiping Yuan, Guangyu Yang, Wensheng Sun, and Ajith
Muralidharan. 2022. Multi-objective Optimization of Notifications Using Offline
Reinforcement Learning. arXiv:2207.03029 [cs.LG]
[21] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.
2016. Product-based neural networks for user response prediction. In 2016 IEEE
16th international conference on data mining (ICDM). IEEE, 1149â€“1154.
[22] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. 2019. Rapid
learning or feature reuse? towards understanding the effectiveness of maml.
arXiv preprint arXiv:1909.09157 (2019).
[23] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. 2019.
Meta-learning with implicit gradients. Advances in neural information processing
systems 32 (2019).
[24] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy
Lillicrap. 2016. Meta-learning with memory-augmented neural networks. In
International conference on machine learning. PMLR, 1842â€“1850.
[25] Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for
few-shot learning. Advances in neural information processing systems 30 (2017).
[26] Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, and Hugo
Larochelle. 2017. A meta-learning perspective on cold-start recommendations
for items. Advances in neural information processing systems 30 (2017).
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[28] Chunyang Wang, Yanmin Zhu, Haobing Liu, Tianzi Zang, Jiadi Yu, and Fei-
long Tang. 2022. Deep Meta-learning in Recommendation Systems: A Survey.
arXiv:2206.04415 [cs.IR]
[29] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In Proceedings of the web conference 2021.
1785â€“1797.
[30] Youshao Xiao, Shangchun Zhao, Zhenglei Zhou, Zhaoxin Huan, Lin Ju, Xiaolu
Zhang, Lin Wang, and Jun Zhou. 2023. G-Meta: Distributed Meta Learning
in GPU Clusters for Large-Scale Recommender Systems. In Proceedings of the
32nd ACM International Conference on Information and Knowledge Management .
4365â€“4369.
[31] Runsheng Yu, Yu Gong, Xu He, Yu Zhu, Qingwen Liu, Wenwu Ou, and Bo
An. 2021. Personalized adaptive meta learning for cold-start user preference
prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.
10772â€“10780.
[32] Kui Zhao, Yuechuan Li, Zhaoqian Shuai, and Cheng Yang. 2018. Learning and
transferring ids representation in e-commerce. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. 1031â€“
1039.
[33] Cai-Nicolas Ziegler, Sean M McNee, Joseph A Konstan, and Georg Lausen. 2005.
Improving recommendation lists through topic diversification. In Proceedings of
the 14th international conference on World Wide Web. 22â€“32.
A INFORMATION FOR REPRODUCIBILITY
A.1 Results for different inner loop iterations
Table 9 illustrates the increase in training time for pTap applica-
tion when we introduce meta learning over vanilla training. When
increasing the inner loop iterations ğ‘›, the training time increases
roughly linearly while AUC gains remain the same. We have similar
observation on pClick applications.Inner Loop
IterationsTrain Time Increase Test AUC Gain
Baseline baseline baseline
1 +104.98% +1.47%
2 +130.35% +1.50%
3 +162.40% +1.50%
4 +185.77% +1.50%
5 +221.16% +1.51%
Table 9: LiMAML training time increase and Test AUC gains
for different values of inner loop updates on pTap.
A.2 Results with different dropout rates
In this experiment on pTap, we apply dropout to every layer of
the neural network. With dropout, we observe a drop in Test AUC
gains. As presented in Table 10, test AUC gains drop from +1.47%
to +1.44% with a minimal dropout rate of 0.1. As the dropout rate
is increased, we observed the drop in AUC consistently increasing.
Dropout rate Test AUC Gain
Vanilla training Baseline
0. +1.47%
0.1 +1.44%
0.2 +1.40%
0.4 +1.26%
0.6 +0.88%
Table 10: pTap LiMAML Test AUC gains for different dropout
rates with respect to vanilla training baseline.
A.3 Hyperparameter tuning experiments
On the pTap LiMAML model, we fix the hyperparameter config-
uration and experiment with different task learning rate values.
As evident from Table 12, tweaking task learning rate (0.005 to 10)
around the best model configuration has insignificant drop on the
test AUC gains. We have similar observation for pClick LiMAML
(user, notification type) model as shown in Table 11. We perform
similar experiment over global learning rate values to observe the
effects on test AUC gains. The resulting test AUC gains are shown
in Table 13.
We observe that in comparison, the task learning rate requires
less tuning and is usually set to 100x or 1000x orders of magnitude
higher than global learning rate. This observation is consistent with
high task learning rates values used in other MAML works [ 4,22].
A.4 Aggregation strategies
In our experiments, we score the latest sample for each task with the
fine-tuned meta block to produce meta embeddings. Alternatively,
we also tried the following strategies (Max, Mean, Cos) to produce
these embeddings. :
â€¢Max: Max pooling across all samples for each task.
â€¢Mean: Mean pooling across all samples for each task.
â€¢Cos: Cosine similarity weighted mean pooling, where we
take a weighted mean across all the samples for each task
with the weights set to the cosine similarity between that
sample and the latest sample for the task.
5891LiMAML: Personalization of Deep Recommender Models via Meta Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Task Learning Rate Test AUC Gain
Vanilla training Baseline
0.001 0.44%
0.005 0.46%
0.01 0.41%
0.05 0.47%
0.1 0.45%
0.5 0.45%
1 0.44%
5 0.45%
10 0.46%
20 0.45%
50 0.42%
Table 11: pClick (user, notification type) LiMAML task learn-
ing rate tuning. Global learning rate is set to .0006 for the
above experiments. Test AUC gain is over vanilla training
baseline.
Task Learning Rate Test AUC Gain
Vanilla training Baseline
0.005 1.44%
0.01 1.45%
0.05 1.47%
0.1 1.47%
0.5 1.45%
1 1.45%
10 1.46%
Table 12: pTap LiMAML task learning rate tuning. Global
learning rate is set to .0012 for the above experiments. Test
AUC gain is over vanilla training baseline.
Global Learning Rate Test AUC Gain
Vanilla training Baseline
0.00095 +1.47%
0.0009 +1.47%
0.0012 +1.47%
0.0015 +1.46%
0.003 +1.43%
0.004 +1.41%
0.005 +1.37%
0.006 +1.40%
Table 13: pTap LiMAML global learning rate tuning. Tasks
learning rate is set to 0.1 for these experiments. Test AUC
gain is over vanilla training baseline.
As we can see from Table 14, all these aggregation strategies
performed poorer in comparison to deriving the meta embeddings
from the latest sample for each task.Max Mean Cos
AUC -0.06% -0.12% -0.10%
Table 14: AUC Gains using different pooling methods for
Meta Embedding Generation.
5892