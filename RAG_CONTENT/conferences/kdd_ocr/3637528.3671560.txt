Temp
oral Uplift Modeling for Online Marketing
Xin Zhangâˆ—â€ 
Wuhan University
Wuhan, China
zhangxin@whu.edu.cnKai Wangâˆ—
NetEase Fuxi AI Lab
Hangzhou, China
wangkai02@corp.netease.comZengmao Wangâ€¡â€ 
Wuhan University
Wuhan, China
wangzengmao@whu.edu.cn
Bo Duâ€¡â€ 
Wuhan University
Wuhan, China
dubo@whu.edu.cnShiwei Zhao
NetEase Fuxi AI Lab
Hangzhou, China
zhaoshiwei@corp.netease.comRunze Wuâ€¡
NetEase Fuxi AI Lab
Hangzhou, China
wurunze1@corp.netease.com
Xudong Shen
NetEase Fuxi AI Lab
Hangzhou, China
hzshenxudong@corp.netease.comTangjie Lv
NetEase Fuxi AI Lab
Hangzhou, China
hzlvtangjie@corp.netease.comChangjie Fan
NetEase Fuxi AI Lab
Hangzhou, China
fanchangjie@corp.netease.com
ABSTRACT
In recent years, uplift m odeling, a lso k nown a s i ndividual treat-
ment effect (ITE) estimation, has seen wide applications in online 
marketing, such as delivering one-time issuance of coupons or dis-
counts to motivate usersâ€™ purchases. However, complex yet more 
realistic scenarios involving multiple interventions over time on 
users are still rarely explored. The challenges include handling the 
bias from time-varying confounders, determining optimal treat-
ment timing, and selecting among numerous treatments. In this 
paper, to tackle the aforementioned challenges, we present a tem-
poral point process-based uplift model (TPPUM) that utilizes usersâ€™ 
temporal event sequences to estimate treatment effects via coun-
terfactual analysis and temporal point processes. In this model, 
marketing actions are considered as treatments, user purchases as 
outcome events, and how treatments alter the future conditional 
intensity function of generating outcome events as the uplift. Em-
pirical evaluations demonstrate that our method outperforms ex-
isting baselines on both real-world and synthetic datasets. In the 
online experiment conducted in a discounted bundle recommenda-
tion scenario involving an average of 3 to 4 interventions per day 
and hundreds of treatment candidates, we demonstrate how our 
model outperforms current state-of-the-art methods in selecting 
the appropriate treatment and timing of treatment, resulting in a 
3.6% increase in application-level revenue.
âˆ—Co-first authors
â€ Also with the National Engineering Research Center for Multimedia Software, Insti-
tute of Artificial Intelligence of Wuhan University, and the Hubei Key Laboratory of 
Multimedia and Network Communication Engineering.
â€¡Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than 
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, or republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. 
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671560CCS CONCEPTS
â€¢Information systems â†’Personalization; â€¢ Applied comput-
ingâ†’Online shopping .
KEYWORDS
online marketing, uplift modeling, individual treatment effect
ACM Reference Format:
Xin Zhang, Kai Wang, Zengmao Wang, Bo Du, Shiwei Zhao, Runze Wu,
Xudong Shen, Tangjie Lv, and Changjie Fan. 2024. Temporal Uplift Mod-
eling for Online Marketing. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29,
2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/
10.1145/3637528.3671560
1 INTRODUCTION
In modern online marketing, uplift modeling has emerged as a
pivotal technique for enhancing user engagement and platform
revenue by offering targeted incentives (i.e., treatment) such as
coupons [31] and discounts [15]. Traditional marketing strategies
often overlook the likelihood of a customer purchasing a product
without any intervention (i.e., giving treatment). This oversight has
prompted a paradigm shift towards uplift-based strategies that pri-
oritize marketing actions to induce a significant uplift in user pur-
chases, rather than merely focusing on items with the highest prob-
ability of purchase. Consider the following scenario: during a mar-
keting campaign, the system predicts a 90% probability of a user
purchasing bottled water and a 40% probability for an energy drink.
Traditional strategies would favor bottled water due to its higher
purchase probability. However, without intervention, bottled wa-
ter might still have an 80% purchase likelihood due to its essential
nature, while the energy drink might only have a 10% likelihood. In
this case, recommending the energy drink could be more strategic,
as it represents a greater increase in purchase probability (30% vs.
10%), potentially leading to higher profits. Considering that these
incentives carry costs and that diverse users respond differently
to them â€” where some only purchase with coupons, while others
would purchase regardless â€” accurately identifying the sensitive
6247
KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain Xin Zhang, et al.
user segments for each incentive is crucial for maximizing market-
ing efficacy. Unlike traditional supervised learning, this involves
a typical causal inference problem, as we cannot observe both the
factual outcome (what actually happened) and the counterfactual
outcome (what would have happened under a different decision)
for an individual simultaneously. To achieve this objective, online
marketing employs estimation of individual treatment effects (ITE)
or uplift to accurately capture the differential response of users to
various incentives compared to no incentives. Uplift modeling has
been validated across multiple scenarios [3, 6].
Existing uplift modeling methods for online marketing can be
categorized into the meta-learner method [13, 20], the tree-based
approach [22, 26], and neural network-based approach [23, 25].
The main idea is to predict unbiased outcomes of different treat-
ments by establishing a connection between treated and controlled
groups [11], accomplished through the implementation of various
data utilization methods [20] and subpopulation segmentation [22].
While existing uplift modeling methods have demonstrated promis-
ing results in simple scenarios, such as one-time treatment, most
cannot be applied in scenarios where users receive more than one
treatment across a continuous timeline. A typical online marketing
scenario is depicted in Figure 1(a), where users have historical tem-
poral data on treatments and outcomes, posing three challenges:
â€¢Highly biased data. Usersâ€™ responses to past treatments
shape future treatments, resulting in a higher bias in sequen-
tial scenarios compared to static single-intervention scenar-
ios. For instance, based on learning from past data, the sys-
tem perpetually recommends high-value discounts to high-
spending users, and users develop strong expectations re-
garding discount values. The direct use of sequence learning
methods [12, 27] will be biased by the highly biased data and
will not be able to make accurate estimations of counterfac-
tuals for different treatments.
â€¢Handling time-varying confounders. In Figure 1(a), the
user is given the treatment ğ‘“3atğ‘¡3, assuming the userâ€™s co-
variate itself was influenced by past treatment ğ‘“2, failing to
adjust for time-varying confounders (the impact of ğ‘“2on
user covariates) would lead to erroneous conclusions about
the effectiveness of treatment ğ‘“2if the user exhibits pur-
chase behavior after ğ‘“3rather than after ğ‘“2. Existing uplift-
based methods [16, 20] solely considering treatments and
outcomes dependent on static user covariate values are inad-
equate for estimating the effect of a sequence of treatments
on user outcomes.
â€¢Continuous-time modeling. In online marketing, the oc-
currence of multiple events within a time window, includ-
ing treatments and outcomes, is common. A userâ€™s outcome
event can be attributed to multiple treatments within a time
window. Time gap is a critical factor in estimating the mu-
tual influence of treatment and outcome effects, with effects
generally diminishing over time. Since uplift is defined as
the change in the distribution of future outcomes due to
treatment, the impact of treatment on future outcomes at
different time points needs to be estimated and integratedover time. Point process models [5, 7] are considered effec-
tive techniques for handling the mutual influence of events
on continuous-time event sequences.
In this paper, to tackle the aforementioned challenges, we pro-
pose a novel Temporal Point Process-based Uplift Model, denoted
as TPPUM. As shown in Figure 1(b), it models the individual treat-
ment effect of a treatment given at timestamp ğ‘¡5on a userâ€™s fu-
ture outcome through counterfactual reasoning on temporal point
processes. Initially, we define a causal model that characterizes
the uplift as the effect of the treatment on altering the intensity
function of user purchases. Here, the marketing action is consid-
ered the treatment, and the userâ€™s future conditional intensity func-
tion is regarded as the outcome. To address bias stemming from
time-varying confounders in the point process, we draw inspira-
tion from CRN [4] and employ domain adversarial training to es-
tablish balancing representations of the userâ€™s past event sequence.
Furthermore, we extend this approach from discrete time-step event
sequences to continuous time event sequences. Given an event oc-
curs at timestamp ğ‘¡, TPPUM first constructs a time-aware treatment-
invariant representation for the userâ€™s historical events preceding
that particular event. The resulting balancing representation, de-
noted as H, is then utilized to separately model the intensity func-
tion ğœ†(ğ‘¡|ğ»)and the eventâ€™s feature distribution ğ‘(ğ‘“|ğ»)using tem-
poral point processes and Multi-layer Perceptron (MLP), respec-
tively. The parameters ğœ†(ğ‘¡|ğ»)and ğ‘(ğ‘“|ğ»)are learned by maxi-
mizing the likelihood of each event given the userâ€™s past event se-
quence. By directly learning the integrated intensity function, our
model allows for obtaining a closed-form solution for the expecta-
tion of the outcome distribution, thereby enhancing efficiency in
model inference.
We summarize the main contributions as follows:
â€¢We first explore temporal uplift modeling for online mar-
keting, extending uplift modeling scenarios from single-step
intervention to multiple interventions across a continuous
timeline. We introduce a novel causal model that quantifies
the causal effect of treatments at specific timestamp ğ‘¡. Addi-
tionally, we develop a neural temporal process model that
enables unbiased estimation of these effects.
â€¢We deploy the model in a challenging real-world online mar-
keting application. This scenario is significant and innova-
tive due to its realistic setting, allowing for multiple inter-
ventions over time and requiring decision-making from hun-
dreds of treatment options, including timing and content.
The sophisticated estimation of uplift enables an optimal
equilibrium between bundle GMV and shop GMV, thereby
achieving a 3.7% uplift in application-level revenue.
2 RELATED WORK
2.1 Uplift Modeling For Online Marketing
The uplift modeling has evolved along three distinct research lines:
1) meta-learner method leverages existing techniques to construct
6248Temporal Uplift Modeling for Online Marketing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
timelineObserved HistoryCounterfactualexpected  user purchaseswith treatmentwithout treatmentğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%ğ‘¡&Tuser purchases(i.e., outcome) marketing actions(i.e., treatment)(ğ‘“!,ğ‘¡!)(ğ‘“",ğ‘¡")(ğ‘“#,ğ‘¡#)(ğ‘“$,ğ‘¡$)(ğ‘“%,ğ‘¡%)(ğ‘“&,ğ‘¡&)timeline
(
a)
timelineObserved HistoryCounterfactualexpected  user purchaseswith treatmentwithout treatmentğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%ğ‘¡&Tuser purchases(i.e., outcome) marketing actions(i.e., treatment)(ğ‘“!,ğ‘¡!)(ğ‘“",ğ‘¡")(ğ‘“#,ğ‘¡#)(ğ‘“$,ğ‘¡$)(ğ‘“%,ğ‘¡%)(ğ‘“&,ğ‘¡&)timeline (
b)
Figure 1: (a) depicts a typical scenario of online marketing. On one hand, purchasing (i.e., outcomes) trigger each other on
the timeline based on their individual dynamics. On the other hand, marketing actions (i.e., treatment) impact the future
purchasing within a certain time window. (b) depicts changes in the expected value of user purchases over time. At a current
time ğ‘¡5, the model can predict the counterfactual dynamic (the colored dashed branches) with planned treatments and the
dynamic (the black dashed branches) without treatments.
potential outcome models separately for the treatment and con-
trol groups [13, 20]. 2) Tree-based methods [22, 26] explicitly di-
vide sensitive users corresponding to each treatment using differ-
ent metrics as splitting criteria, offering interpretability. 3) Neu-
ral network-based methods leverage the advantages of neural net-
works to capture more complex and flexible relationships between
treatments and outcomes. With neural networks, models could learn
more unbiased information from observed data [23, 25]. Further-
more, some studies focus on utilizing deep learning to eliminate
the effect of confounders [17, 28]. Some studies pay more attention
to utilizing extra information for uplift estimation. For example,
DESCN [32] introduces the entire space from multi-task learning
to address distribution imbalance. EFIN [16] encodes not only user
and contextual features but treatment features. These methods can
only be applied to single-intervention scenarios in a static setting.
2.2 Causal Inference Over Time
Beyond the realm of uplift in online marketing, a few studies pay
attention to the sequence-to-sequence methods for estimating the
treatment effect in a dynamic view. Here, RMSN [14] uses RNN to
predict the inverse probability of treatment for training networks.
CRN [4] uses adversarial learning to produce balanced representa-
tions, which are simultaneously predictive of the outcome but non-
predictive of the current treatment assignment. DCRN [2] disen-
tangles the balancing representations into treatment, outcome and
confounder factor. CT [19] utilizes a Transformer into sequence en-
coding. These models are mainly intended for medical situations
where treatment and outcome events occur concurrently. While
we have adapted these algorithms to support temporal sequence
in the experimental section, due to their architecture, calculating
the uplift value of treatment on future outcomes remains a chal-
lenge for these algorithms.
2.3 Temporal Point Process
Temporal point processes (TPP) have gained attention as versatile
tools for generating sequences of discrete events in a continuous
timeline, finding applications across various domains [18, 24]. Re-
cently, the applications of neural networks in point process model-
ing have attracted significant attention. RMTPP [7] introduces re-
current neural networks instead of specifically fixed assumptionsto view the intensity function of a temporal point process. JUMP
[33] uses point process and sequential modeling techniques to pre-
dict dwell time based on three-layered RNN. CNTPP [30] incor-
porates the neural point process and Gaussian mixture models to
estimate the ITE of misinformation on social media.
3 PRELIMINARY
3.1 Temporal Uplift Problem Definition
Consider an observational dataset ğ·=(ğ‘‹ğ‘–, ğ‘ğ‘–,ğ‘¦ğ‘–,ğ‘‡ğ‘–)ğ‘
ğ‘–=1consisting
of history events about ğ‘samples. For ğ‘–-th sample (each sample
is an outcome event), ğ‘‹ğ‘–=[ğ‘“ğ‘–
1, ğ‘“ğ‘–
2, ..., ğ‘“ğ‘–
ğ¿]andğ‘‡ğ‘–=[ğ‘¡ğ‘–
1, ğ‘¡ğ‘–
2, ..., ğ‘¡ğ‘–
ğ¿]is
the sequence of history events features and corresponding times-
tamp. ğ‘“ğ‘–
ğ‘™âˆˆğ‘…ğ‘‘ğ‘“is a vector indicates ğ‘‘ğ‘“features of the ğ‘™-th event.
ğ¿means the total number of events in sequence. ğ‘ğ‘–âˆˆğ‘…ğ‘‘ğ‘is a
vector that indicates which treatment option (also including the
option of no treatment) has been employed in ğ‘–-th sample. ğ‘¦ğ‘–in-
dicates the actual outcomes. Choosing what feature of outcome
events as outcome ğ‘¦ğ‘–usually depends on the actual task. As de-
picted in Figure 1b, our goal is to estimate the causal effect of the
previous treatment event on the outcome event over time. Conse-
quently, followed by [30], the employed treatment option ğ‘ğ‘–is de-
cided by the last event. Specifically, if the previous event is a treat-
ment event, we consider that the current outcome event receives
the corresponding treatment. If the previous event is an outcome
event, we consider that the current event receives no treatment.
For simplicity, the superscript ğ‘–will be omitted in the later part.
Following the potential outcome framework [8, 21], let ğ‘¦(Ë†ğ‘)and
ğ‘¦(0)be the potential outcomes for each possible option of treat-
ment Ë†ğ‘and for no treatment respectively. Then the objective of
this paper is to estimate the uplift of the treatment Ë†ğ‘for each out-
come event at the target timestamp which is defined as ğœ(Ë†ğ‘, ğ‘¡âˆ—)=
ğ‘¦(Ë†ğ‘, ğ‘¡âˆ—)âˆ’ğ‘¦(0, ğ‘¡âˆ—). However, we only have access to a single ob-
served outcome, which poses a challenge for uplift modeling as it
differs from traditional supervised learning. Thus, for each user,
the goal of uplift modeling is to estimate the expected ITE ğœ(Ë†ğ‘, ğ‘¡âˆ—)
of each treatment Ë†ğ‘given history events ğ‘‹and timestamp ğ‘‡. It can
be formulated as:
ğœ(Ë†ğ‘, ğ‘¡âˆ—)=ğ¸(ğ‘¦|ğ‘=Ë†ğ‘, ğ‘¡=ğ‘¡âˆ—, ğ‘‹,ğ‘‡)âˆ’ğ¸(ğ‘¦(0)|ğ‘=0, ğ‘¡=ğ‘¡âˆ—, ğ‘‹,ğ‘‡)(1)
6249KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain Xin Zhang, et al.
After obtaining all the ITE ğœ(Ë†ğ‘, ğ‘¡âˆ—)for each of the treatment op-
tions Ë†ğ‘and the candidate target timestamp ğ‘¡âˆ—, we can rank these
estimated ITEs and select the optimal treatment timestamp to as-
sign an effective treatment.
3.2 Marked Temporal Point Process
The marked temporal point process is a highly effective framework
for capturing the underlying mechanisms that govern the observed
patterns of random events over time [7]. By considering the tempo-
ral dependencies among events, we can develop models that explic-
itly account for the influence of past events on the timing of future
events. More formally, a marked temporal point process represents
a random process characterized by a sequence of discrete events,
each associated with a specific timing, denoted as ğ‘¡, and a marker,
denoted as ğ‘“. In this paper, we assign the marker of an event (both
treatment and outcome) as its associated features. In this paper, the
marker ğ‘“can be regarded same as feature ğ‘“in section 3.1.
By considering the historical sequence of events, we can explic-
itly characterize the marked temporal point process using the con-
ditional intensity function ğœ†(ğ‘“ , ğ‘¡|ğ»)that the next event will hap-
pen at time ğ‘¡with marker ğ‘“where ğ»=(ğ‘‹,ğ‘‡)is the historical
sequence of events and corresponding timestamp before ğ‘¡. The in-
tensity function ğœ†(ğ‘“ , ğ‘¡|ğ»)means the instantaneous probability of
the event occurring at time ğ‘¡with marker ğ‘“[5]. Based on the in-
tensity function, we can derive the probability density function for
theğ‘—-th event, which occurs at time ğ‘¡ğ‘—with marker ğ‘“ğ‘—:
ğ‘(ğ‘“ğ‘—, ğ‘¡ğ‘—|ğ»ğ‘—)=ğœ†(ğ‘“ğ‘—, ğ‘¡ğ‘—|ğ»ğ‘—)ğ‘’ğ‘¥ğ‘(âˆ’âˆ«ğ‘¡ğ‘—
ğ‘¡ğ‘—âˆ’1ğœ†(ğ‘“ğ‘—, ğ‘¡ğ‘—|ğ»ğ‘—)ğ‘‘ğ‘¡) (2)
In practical scenarios, to alleviate the complexities associated with
jointly and explicitly modeling both the timing and marker infor-
mation, several studies [5, 7] will factorize the intensity function
ğœ†(ğ‘“ , ğ‘¡|ğ»)toğœ†(ğ‘¡|ğ»)ğ‘(ğ‘“|ğ»).
4 METHOD
In this section, we present our proposed TPPUM and provide an
overview of its architecture. As depicted in Figure 3, given a se-
quence of previous events, the model first encodes both treatment
events and outcome events jointly to obtain their representation ğœ™
which includes the sequential relations among these events. Sub-
sequently, we leverage the representation ğœ™to model the intensity
function ğœ†(ğ‘¡|ğ»)and the markerâ€™s probability ğ‘(ğ‘“|ğ»)by neural net-
work. The intensity function will be further used to predict the tim-
ing of the next outcome event. For features of the next outcome
event, we construct an outcome predictor to predict them. During
the training process, we introduce adversarial learning techniques
to ensure that the embedding representation ğœ™strikes a balance
between being predictive of outcomes and not being predictive of
treatments. Finally, we can utilize the estimated intensity function
and markerâ€™s probability to predict the timing and marker of the
next event.
4.1 Balancing Representation Encoding
In order to formulate the process of treatment and outcome events
as a unified marked temporal point process, we need to capture the
complex dependencies within the historical event sequences basedon the sequence encoder. Specifically, we first design two separate
mapping functions because of the inherent differences between the
features of treatment and outcome events. These functions are re-
sponsible for mapping the timing and features of events (ğ‘“ , ğ‘¡)into
the embedding(f,t). Then we feed them into the sequence encoder
as depicted in Figure 3 to generate the representation ğœ™. To ensure
unbiased estimation, it is crucial to obtain the balancing representa-
tion ğœ™in a manner that is invariant to treatments. We will provide
a detailed explanation of how we achieve the balancing encoding
in Section 4.4.
4.2 Point Process Learning
As discussed in Section 2.3, we model the marked temporal point
process (MTPP) by learning the intensity function ğœ†(ğ‘¡|ğ»)and mark-
ersâ€™ probability ğ‘(ğ‘“|ğ»)based on balancing representation ğœ™sepa-
rately. MTPP stands out for its ability in modeling events within
continuous timelines, which is instrumental in accurately captur-
ing the stochastic nature of event timings along with the under-
lying intensity function. This capability is particularly beneficial
for our application, which aims not only to determine the most
appropriate treatment options but also to identify the optimal tim-
ing for presenting these recommendations to players. The unique
ability of MTPP to model time accurately provides flexibility in
making treatment decisions about when to offer treatments and
what treatments bring in the future, which is particularly critical
in addressing the challenges presented by the irregular timing of
treatment and outcome events observed in online marketing.
4.2.1 Intensity Learning. For the intensity function ğœ†(ğ‘¡|ğ»), previ-
ous studies [29, 30] have demonstrated the effectiveness of a multi-
layer perceptron (MLP) in modeling intensity functions. Building
upon these findings, we employ an MLP architecture in this paper
to explicitly learn the intensity function. In addition, to avoid the
complex calculation of the integral involved in the intensity func-
tion, we choose to directly fit the integral of the intensity with
neural networks. By obtaining the gradient of the fitted integral
with respect to the time, we can effectively estimate the intensity
function. The process can be formulated as follows:
ğ¼ğ‘¡=âˆ«ğ‘¡
ğ‘¡âˆ’ğœ†(ğ‘¡|ğ»)â† ğœ(ğ‘¤2(ğœ(ğ‘¤1[ğœ™;(ğ‘¡âˆ’ğ‘¡âˆ’)]+ğ‘1))+ğ‘2)
ğœ†(ğ‘¡|ğ»)=ğ‘‘ğ¼ğ‘¡
ğ‘‘
ğ‘¡(3)
where ğ‘¤1, ğ‘¤2, ğ‘1, ğ‘2are trainable parameters of MLP and ğœis ac-
tivation function set as Elu. ğ‘¡âˆ’means the last eventâ€™s timestamp
before the current event. Note that we concatenate ğœ™with the time
interval ğ‘¡âˆ’ğ‘¡âˆ’to feed into MLP because definite integralâˆ«ğ‘¡
ğ‘¡âˆ’ğœ†(ğ‘¡|ğ»)
involves the accumulation from the previous event to the current
event in time.
4.2.2 Marker Learning. For the markerâ€™s probability, a straightfor-
ward solution is to assume the markersâ€™ distribution as Gaussian
distribution and apply a neural network to fit the parameters of a
Gaussian distribution. However, the marker of both treatment and
outcome events is not guaranteed to conform to Gaussian distri-
bution. Thus we apply representation learning to derive markersâ€™
6250Temporal Uplift Modeling for Online Marketing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figur
e 2: The architecture of TPPUM.
probability from the balancing representation, which can be for-
mulated as:
ğ‘’ğ‘‘=ğœ(ğ‘¤4(ğœ(ğ‘¤3ğœ™+ğ‘3))+ğ‘4) (4)
where ğ‘’ğ‘‘means the representation of markers.
4.3 Marker Prediction
By building an outcome predictor ğ‘‚(ğœ™, ğœ†, ğ‘’ ğ‘‘), the prediction of mark-
ers of the next outcome event can be formulated as follows:
Ë†ğ‘“=ğ‘‚(ğœ™, ğœ†(ğ‘¡|ğ»), ğ‘’ğ‘‘) (5)
where Ë†ğ‘“âˆˆğ‘…ğ‘‘ğ‘“means the predicted markers of the next outcome
event. Since we focus on the ITE estimation, we only need to use
the outcome Ë†ğ‘¦included in Ë†ğ‘“which will be elaborated in section
4.5.
4.4 Training
4.4.1 Point Process Training. Based on Equation (2), the log-likelihood
of the marked event (ğ‘“ , ğ‘¡)and point process loss can be formulated
as follows:
ğ‘™ğ‘œğ‘”ğ‘(ğ‘“ğ‘™, ğ‘¡ğ‘™|ğ»)=ğ‘™ğ‘œğ‘”(ğœ†(ğ‘“ğ‘™, ğ‘¡ğ‘™)ğ‘’ğ‘¥ğ‘(âˆ’âˆ«ğ‘¡ğ‘™
ğ‘¡ğ‘™âˆ’1ğœ†(ğ‘“ , ğ‘¡|ğ»)ğ‘‘ğ‘¡))
=ğ‘™ğ‘œğ‘”ğœ†(ğ‘“ğ‘™, ğ‘¡ğ‘™|ğ»)âˆ’âˆ«ğ‘¡ğ‘™
ğ‘¡ğ‘™âˆ’1ğœ†(ğ‘“ , ğ‘¡|ğ»)ğ‘‘ğ‘¡
=ğ‘™ğ‘œğ‘”ğœ†(ğ‘¡ğ‘™|ğ»)+ğ‘™ğ‘œğ‘”ğ‘(ğ‘“ğ‘™|ğ»)âˆ’ğ‘(ğ‘“ğ‘™|ğ»)âˆ«ğ‘¡ğ‘™
ğ‘¡ğ‘™âˆ’1ğœ†(ğ‘¡|ğ»)ğ‘‘ğ‘¡
ğ¿ğ‘=ğ‘âˆ‘
ğ‘–=1ğ‘(ğ‘“ğ‘™|ğ»)âˆ«ğ‘¡ğ‘™
ğ‘¡ğ‘™âˆ’1ğœ†(ğ‘¡|ğ»)ğ‘‘ğ‘¡âˆ’ğ‘™ğ‘œğ‘”ğœ†(ğ‘¡ğ‘™|ğ»)âˆ’ğ‘™ğ‘œğ‘”ğ‘(ğ‘“ğ‘™|ğ»)
(6)
As discussed in Section 4.2, we implicitly learn markersâ€™ probabil-
ityğ‘(ğ‘“|ğ»)by integrating it into the training of outcome predictor
which will be elaborated in Section 4.3.2.
4.4.2 Adversarial Training. As discussed in Section 1, the history
of events contains the time-varying confounders that bias the treat-
ment assignment in the observational data. Merely minimizing theloss function during model training could result in biased uplift pre-
dictions. To tackle this issue, we integrate the domain adversarial
technique into the training process with the aim of acquiring a bal-
ancing representation denoted ğœ™. The objective is to eliminate the
future treatment information from ğœ™, ensuring that the probability
of receiving any specific treatment remains the same for any two
users with different historical events. Formally, it can be expressed
asğ‘(ğ‘|ğ»0)=ğ‘(ğ‘|ğ»1),âˆ€ğ‘âˆˆğ‘¡ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘šğ‘’ğ‘›ğ‘¡ğ‘œğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  . Specifically, the ad-
versarial training involves the min-max game:
ğ‘šğ‘–ğ‘› ğœ™ğ‘šğ‘ğ‘¥ Ë†ğ‘Eğ»ğ‘™ğ‘œğ‘”Ë†ğ‘(ğ‘|ğœ™(ğ»)) (7)
where ğœ™represents the representation of historical sequence ğ»and
Ë†ğ‘(ğ‘|ğœ™(ğ»)is the prediction of treatment given representation ğœ™. Ac-
cording to [4, 30], the solution ğœ™ensures ğ‘(ğœ™(ğ»)|ğ‘1)=ğ‘(ğœ™(ğ»)|ğ‘2)
for any ğ‘1, ğ‘2if it reaches Nash balanced.
In practice, the adversarial training approach aims to simulta-
neously minimize the outcome loss and maximize the treatment
loss. To achieve this, we introduce an outcome predictor denoted
asğ‘‚(ğœ™, ğœ†, ğ‘’ ğ‘‘). Additionally, we construct a treatment classifier de-
noted as ğ¶(ğœ™). Furthermore, we leverage the Gradient Reversal
Layer (GRL) [9] between the balancing representation and the treat-
ment predictor to reverse the sign of the gradient. GRL could facil-
itate adversarial training during backpropagation. The treatment
loss and outcome loss are defined as follows:
ğ¿ğ‘=ğ‘‘ğ‘âˆ‘
ğ‘—=1I[ğ‘=ğ‘ğ‘—]ğ‘™ğ‘œğ‘”(ğ¶(ğºğ‘…ğ¿(ğœ™)))
ğ¿ğ‘¦=||ğ‘¦âˆ’Ë†ğ‘¦||2
ğ¿ğ‘ğ‘ğ‘™ğ‘ğ‘›ğ‘ğ‘’ =ğ‘âˆ‘
ğ‘–=1ğ¿ğ‘¦âˆ’ğ›¼ğ¿ğ‘(8)
where ğ›¼is a hyper-parameter for domain adversarial and ğ‘is the
number of samples in the dataset. Thereby, the time-varying con-
founding bias will be removed from balancing representation. Fi-
nally, we train the model with the summarization of the loss func-
tion including ğ¿ğ‘andğ¿ğ‘ğ‘ğ‘™ğ‘ğ‘›ğ‘ğ‘’ .
6251KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain Xin Zhang, et al.
Table 1: Statistics of datasets used in this experiment. The
statistics of Hawkes-Tumor include all subsets with varying
bias coefficients.
Dataset
#Treatment #Outcome Avg. Sqe. Len
GAME 1,826,871
432,397 225.9214
Synthetic GAME 1,473,719 1,516,143 158.2913
Hawkes-Tumor 3,691,583 3,581,632 101.0168
4.5
Inference
During the evaluation stage in offline experiments, to estimate the
uplift about the next outcome event given current treatment event
Ë†ğ‘, we follow a two-step process. Firstly, we predict the potential
outcomes Ë†ğ‘¦(Ë†ğ‘)and Ë†ğ‘¦(0)based on the sequence of historical events
ğ», considering the presence or absence of the current treatment
event. To estimate the counterfactual outcomes, we conduct this in
two scenarios. If the factual treatment option is treated, we remove
the last treatment event and construct the counterfactual historical
sequence ğ»ğ‘ğ‘“. If the factual treatment option is untreated, we add a
simulated treatment event to construct ğ»ğ‘ğ‘“. Its features are consis-
tent with the last factual treatment event and its occurrence time
is set to 24 hours after the last event. The counterfactual outcome
Ë†ğ‘¦(0)orË†ğ‘¦(Ë†ğ‘)is predicted given ğ»ğ‘ğ‘“Subsequently, we calculate the
uplift as the difference between these predicted outcomes, denoted
asğœ(Ë†ğ‘)=Ë†ğ‘¦(Ë†ğ‘)âˆ’Ë†ğ‘¦(0).
On the other hand, in real-world applications, the accurate tim-
ing of the next outcome event is not always knowable, that is, we
cannot directly predict the outcome as described in Equation 4. To
address this limitation, we propose to predict the expected out-
come within a specific time window, which can be formulated as
follows:
ğ¸(Ë†ğ‘¦)=âˆ«ğ‘¡+ğ‘Š
ğ‘¡ğœ†(ğ‘¡|ğ»)ğ‘‚(ğœ™, ğœ†(ğ‘¡|ğ»))ğ‘‘ğ‘¡ (9)
where ğ‘Šis a hyper-parameter means the length of the time win-
dow. Similarly, we can obtain uplift ğœ(Ë†ğ‘)by calculating the differ-
ence expected outcome ğ¸(Ë†ğ‘¦(Ë†ğ‘))andğ¸(Ë†ğ‘¦(0)).
5 EXPERIMENTS
5.1 Experimental Settings
5.1.1 Baselines. The study of uplift modeling over time applied
to temporal event sequences remains unexplored in previous re-
search efforts. To address this research gap, we carefully select a
set of methods that demonstrate proficiency in estimating treat-
ment effects based on event sequences over time, including CRN
[4], DCRN [2], and CT [19].
â€¢CRN: CRN uses adversarial training to construct treatment
invariant representation, which aims to address the bias from
time-varying confounders.
â€¢DCRN: DCRN disentangles the representation of event se-
quences into three latent factors to allow better time-varying
confounder modeling.
â€¢CT: CT designs the encoder based on the Transformer and
cross-attention network which can model more complex and
long-range time-varying confounders.Since DCRN is only applied to binary treatment scenarios, to eval-
uate it in multi-treatment scenarios, we make some extendence.
Specifically, the origin binary-setting discrepancy measure in the
loss function involves the calculation ğ‘‘ğ‘–ğ‘ ğ‘(Oğ‘=0
ğ‘¡,Oğ‘=1
ğ‘¡), we extend
it to the multi-treatment scenario by calculating the distance be-
tween two of the multiple Oğ‘
ğ‘¡and add them together. Furthermore,
in order to enable the baselines to leverage temporal information,
we have incorporated the time gap from the previous event as an
additional input for each time step of the baselines.
5.1.2 Datasets. We conduct the experiments on both the synthetic
and real-world data. The statistics of real-world data are presented
in Table 1. The generation of synthetic datasets will be described
in the Appendix in detail. The datasets include:
â€¢GAME: This is real-world industrial data collected from an
online game. It includes bundle recommendation logs and
usersâ€™ in-shop purchase logs. Bundle recommendations are
treated as the events triggering treatment, while user pur-
chases in the shop are considered as the outcome events.
â€¢Synthetic GAME: Motivated by [30], we create a simula-
tion of an online game recommendation scenario. The fea-
ture is the same as used in the â€GAMEâ€ dataset while the
distribution is more balanced. Events in the simulation were
modeled using a Hawkes process.
â€¢Hawkes-Tumor: Followed by [2, 4, 19], we generated Tu-
mor data based on a state-of-the-art biomedical model [10]
to simulate the effects of tumor cancer treatments over time.
Furthermore, to enhance the consistency of the data with
event sequences, the outcome event was simulated as the
observation event on tumor growth based on the Hawkes
process.
5.1.3 Data Preparation. For GAME datasets, the treatment assign-
ment is selective with high selection bias due to the recommenda-
tion strategy, and we use these data as the training set. We also
choose another set of users whose treatment assignments are un-
der randomized controlled trials (RCT) and we use them as the test
set. For Synthetic GAME datasets, we generated event sequences of
8,000 users for training and another 1,000 users for validation and
testing respectively. In this dataset, the feature sets in the GAME
for recommendation and purchase events are the same. Consequently,
we utilized a single embedding layer to transform these two event
types. We choose the prediction of shop revenue to evaluate the
prediction performance. In the case of Hawkes-Tumor, we select
the time-varying coefficient in ğ›¾âˆˆ {0,1,3,5,7,10}. For each ğ›¾,
we first simulated 10,000 patientsâ€™ treatment trajectories and cor-
responding tumor size data for training, and 1000 patientsâ€™ data
for validation and testing respectively. To enhance this data with
time information, we extended the outcome event by introducing
observational events linked to tumor size. Subsequently, we sam-
pled timestamps using the Hawkes process to construct an out-
come event sequence. The outcome values (tumor size) and treat-
ment were determined based on previous and subsequent know-
able events in the sequence. We will elaborate the process in the
Appendix.
5.1.4 Metrics. In line with the experimental setup adopted in CRN [4],
for the synthetic dataset, we employ two evaluation metrics, namely
6252Temporal Uplift Modeling for Online Marketing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Root Mean Square Error (RMSE) and Precision in Estimation of
Heterogeneous Effect (pEHE) [1], to assess the performance of mod-
els on outcome and uplift predictions. We begin by predicting the
outcome value and uplift for each userâ€™s outcome event. Subse-
quently, we calculate the RMSE by comparing the predicted out-
come values with the true outcome values. Similarly, we compute
pEHE by comparing the predicted uplift values with the true up-
lift values. For the real-world dataset, the unobservability of the
counterfactual outcome prevents us from accessing the true uplift.
Consequently, we evaluate the uplift ranking performance using
a widely adopted metric in uplift modeling, namely the area un-
der the Qini curve (QINI). Higher Qini indicates the better perfor-
mance of the model in the uplift task. It should be noticed that al-
though RMSE can be calculated without counterfactual outcomes
and measures the prediction of factual outcomes in real-world datasets,
it does not provide insights into the effectiveness of uplift model-
ing, particularly in data with inherent selection bias. This limita-
tion arises because RMSE fails to account for how outcomes vary
across different treatment options at the individual level. Conse-
quently, few studies employ RMSE in real-world datasets.
5.1.5 Implementation Details. We implement our model in PyTorch
1.10.1. We use an Adam optimizer and set the number of epochs to
100. During validation, we use RMSE as the evaluation metric for
searching the best hyper-parameters. To ensure the robustness and
reliability of our evaluations, we conducted each experiment five
times and computed the average results.
(
a)
 (
b)
Figure 3: Results on Hawkes-Tumor dataset with varying de-
gree of bias coefficient ğ›¾. A higher ğ›¾value indicates a greater
presence of bias within the dataset.
5.2 Performance Evaluation
Because the Hawkes Tumor dataset has 6 subsets with different
bias coefficients ğœ†, we depict the comparison results on the Hawkes
Tumor dataset in Figure 3 for direct visualization and show results
on the GAME and synthetic GAME datasets in Table 2. In real-
world scenarios, we are often limited to observing only one poten-
tial outcome. However, using synthetic data offers a distinct advan-
tage by providing access to counterfactual outcomes. This unique
capability empowers us to explicitly evaluate the predictions of
counterfactual outcomes, enabling a comprehensive assessment of
models. Note that the RMSE in Figure 3 is normalized by the maxi-
mum tumor volume 1150. From the results in Figure 3, we can have
the following observations:Table 2: The results of the comparison methods on Synthetic
GAME dataset and real-world GAME dataset. The best per-
formance on each dataset is highlighted with bold font.
Dataset Synthetic
GAME GAME
Metric A
verage RMSE A
verage pEHE Qini
CRN 27.3875 45.4682 0.0190
DCRN 32.3839 54.6399 0.0157
CT 29.1003 44.5813 0.0097
TPP
UM w/o T 25.1069 40.5218 -0.0111
TPP
UM w/o I 26.8344 43.4867 0.0031
TPP
UM 23.2204 36.8538 0.0205
Impr
ov 8.8470% 17.3335% 7.8947%
â€¢CT
performs poorly among all baselines in most cases. It
may be attributed to the complicated architecture of its attention-
based encoder. Especially in Synthetic GAME, The time-dependent
confounding introduced by complex, feature-laden treatments
and treatment sequences leads to a higher risk of overfitting
for CT.
â€¢Our TPPUM consistently outperforms all baselines on two
Synthetic datasets in most cases. Especially when bias coef-
ficient ğ›¾=10, TPPUM improves by 34.57%on RMSE. While
CRN and DCRN also employ RNN for encoding event se-
quences within the counterfactual framework, TPPUM sur-
passes them by adopting a distinctive approach that treats
usersâ€™ event sequences as uniform marked point processes.
This unique methodology effectively alleviates the challenges
associated with the distribution of continuous-time events
in complex scenarios of online marketing. Furthermore, our
model excels in enhancing predictions of the next events
on the timeline, resulting from its incorporation of intensity
function and markersâ€™ distribution representations. This aug-
mentation provides a distinct advantage compared to rely-
ing solely on the neural network-based predictor.
For the real-world GAME dataset, the Qini score measures the
ranking performance according to predicted uplift and is applied in
areas where ground truth uplift is unknown. As shown in Table 2,
our proposed TPPUM surpasses the best baselines by 8.8470% ,17.3335%
and 7.8947% on RMSE, pEHE and Qini respectively. Similar to the
performance in Synthetic data, CT still suffers from its complicated
encoder and achieves the worst performance. Our TPPUM con-
sistently outperforms other baselines in the GAME dataset. This
achievement stems from the modelâ€™s ability to efficiently capture
the dynamics of usersâ€™ events along the timeline through point pro-
cess modeling. The results of previous experiments validate the sig-
nificant effectiveness of TPPUM in the uplift task.
5.3 Ablation Study
Moreover, we conduct ablation studies of our TPPUM to analyze
the importance of each part. We consider the following variants of
TPPUM.
â€¢TPPUM w/o T: A model variant that removes adversarial
balancing representation learning.
6253KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain Xin Zhang, et al.
â€¢TPPUM w/o I: A model variant that removes intensity learn-
ing from outcome prediction.
The results are shown in Figure 3 and Table 2. Specifically, when
removing adversarial training to conduct balancing encoding of
event sequences, the model performance has a decline. It is mainly
due to the bias from time-varying confounders that confuse the
model on the treatment effect. A surprising phenomenon is that
even after removing the balancing encoding, TPPUM w/o T still
outperforms baselines in the synthetic GAME dataset. We specu-
late that the reason is a relatively lower treatment bias in the Syn-
thetic GAME dataset compared to other datasets and thus the vari-
ant could retain a great performance via point process learning.
When the model removes the intensity from point process learn-
ing and only preserves the marker for outcome prediction, it will
also bring performance degradation. This validates the effective-
ness of intensity learning, i.e., the estimated intensity can provide
a mathematical characterization of the distribution of future event
occurrences, which is crucial for counterfactual outcome predic-
tion.
6 ONLINE DEPLOYMENT
We applied our proposed TPPUM in one of the most popular games1
released by NetEase Games2for providing online marketing service
by recommending discounted bundles to users. Figure 4 presents
a graphical process of the application. There are two types of user
purchase events: one involves users purchasing the recommended
discounted bundle at a reduced price, while the other involves users
making purchases at the original price in the in-game shop. The
latter can be considered as user-initiated purchases resulting from
spontaneous demand. User interaction with the application is di-
vided into four parts. Firstly, during the gameplay, users may trig-
ger bundle treatment due to various predefined conditions with
an approximate frequency of 30 times per day (Step 1). The strat-
egy will determine whether to make an intervention (Step 2) and
then decide the contents of the discounted bundle (Step 3). The
bundle consists of multiple item IDs and their corresponding item
quantities, and the discount is determined by manual rules. Note
that bundle treatments are limited to an average of 3~4 opportuni-
ties per day, not every user-triggered event will result in a recom-
mendation. This necessitates the need for decision-making regard-
ing the timing of interventions. Then user will receive a pop-up
window and have 30 minutes to decide whether to make a pur-
chase. Undoubtedly, the recommendation of discounted bundles
(i.e., treatment) will influence user spending in the in-game shop
(i.e., outcomes) through substitution effects and demand stimula-
tion effects. Therefore, for this online marketing problem, we need
to dynamically select the optimal treatment and timing based on
real-time user context to maximize the total Gross Merchandise
Volume (GMV) from bundle sales and shop transactions. This relies
on accurately estimating the counterfactual uplift of treatments.
During a two-week online A/B testing, we conducted a compar-
ative analysis of Transformer [27], CRN [4], and TPPUM. Trans-
former is the existing baseline without uplift prediction. It was
1The
game is a real-world production game. Its name is kept anonymous during the
review stage
2http://leihuo.163.com/en/index.html
Usertrigger eventRecommenderSystem30 timesper daydecide to recommend or not&  recommend what
Bundlerecommend a discounted bundle3~4 timesper day0.5 timesper day
In-game shoppurchase in shop
Records:time         event  id            scene10:00:00  buy    item_id   shop11:00:00  click  item_ids  bundle11:02:00  buy    item_ids  bundle11:05:00  buy    item_ids  bundle11:10:00  buy    item_id   shop
Figur
e 4: A graphic description of the discounted bundle rec-
ommendation service. The application includes two scenar-
ios: discounted bundle and in-game shop.
Figur
e 5: Results of the two-week online A/B testing. The left
and right parts illustrate the revenue from discounted bun-
dles and the application-level revenue, respectively.
previously employed in live deployment, with a primary focus on
maximizing revenue in gift bundle scenarios. On the other hand,
CRN represents a widely-used causal inference algorithm for se-
quential treatment but is limited to discrete time series and 1-step
outcome prediction. To enable compatibility with CRN, we defined
a time step as three hours and consolidated multiple treatments oc-
curring within the same time step. In the uplift inference phase of
CRN, the objective was formulated to maximize the revenue from
the current time stepâ€™s treatment, combined with the outcome rev-
enue (i.e., the in-game shop). The uplift inference process of TP-
PUM has been described in Section 4.5. We set the time window
ğ‘Što be 24 hours. The TPPUM networkâ€™s single inference time
is 100-150 milliseconds with a GPU device and 300-400 millisec-
onds with a CPU device. As depicted in Figure 5, the results from
a 14-day A/B test indicate that, compared to the baseline scenario
of only optimizing the bundle, TPPUM suffered a certain loss in
GMV within the bundle scenario. This loss, however, was compen-
sated by a growth in the overall GMV of bundle plus shop (3.6%).
On the other hand, CRN adopted a more aggressive strategy. How-
ever, a higher loss in bundle GMV (-8.9%) did not lead to a higher
shop GMV, resulting in a decrease in the overall GMV of bundle
plus shop by 1.8%. On Sundays (the 6th and the 13th day), TPPUM
achieved significant growth in both bundle and shop compared to
other baselines by providing bundles with only smaller discounts.
This demonstrates that TPPUM accurately estimated the uplift of
discount bundle sequence on usersâ€™ purchase behavior in in-game
6254Temporal Uplift Modeling for Online Marketing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
shop, thus enabling the selection of the best treatment and recom-
mendation timing, ultimately leading to an increase in overall user
purchases in both scenarios.
7 CONCLUSION
In this paper, to address the task of uplift estimation in complex
marketing scenarios, where users receive multiple interventions
over time, we propose a temporal point process-based uplift mod-
eling network, named TPPUM. By modeling the historical events
sequence as a marked temporal point process, we estimate the
future intensity and the markersâ€™ based on balancing representa-
tion. Here, the model could evaluate the treatment effect on in-
dividual users by event prediction. We deployed the model into
a real-world online marketing application with a significant im-
provement. The combination of causal inference and more sophis-
ticated time-series point process methods will be one of the future
research directions.
ACKNOWLEDGMENTS
This work was supported by the National Key Research and Devel-
opment Program of China 2023YFC2705700, National Natural Sci-
ence Foundation of China under Grants 62225113, 62271357, Natu-
ral Science Foundation of Hubei Province under Grants 2023BAB072,
the Innovative Research Group Project of Hubei Province under
Grants 2024AFA017, and the Fundamental Research Funds for the
Central Universities under Grants 2042023kf0134.
REFERENCES
[1] Susan Athey, Guido W Imbens, and Stefan Wager. 2018. Approximate residual
balancing: debiased inference of average treatment effects in high dimensions.
Journal of the Royal Statistical Society Series B: Statistical Methodology 80, 4 (2018),
597â€“623.
[2] Jeroen Berrevoets, Alicia Curth, Ioana Bica, Eoin McKinney, and Mihaela van der
Schaar. 2021. Disentangled counterfactual recurrent networks for treatment ef-
fect inference over time. arXiv preprint arXiv:2112.03811 (2021).
[3] Artem Betlei, Eustache Diemert, and Massih-Reza Amini. 2021. Uplift modeling
with generalization guarantees. In Proceedings of the 27th ACM SIGKDD Confer-
ence on Knowledge Discovery & Data Mining. 55â€“65.
[4] Ioana Bica, Ahmed M Alaa, James Jordon, and Mihaela van der Schaar. 2019. Esti-
mating counterfactual treatment outcomes over time through adversarially bal-
anced representations. In International Conference on Learning Representations .
[5] Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. 2020. Neural spatio-
temporal point processes. arXiv preprint arXiv:2011.04583 (2020).
[6] Floris Devriendt, Jente Van Belle, Tias Guns, and Wouter Verbeke. 2020. Learn-
ing to rank for uplift modeling. IEEE Transactions on Knowledge and Data Engi-
neering 34, 10 (2020), 4888â€“4904.
[7] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-
Rodriguez, and Le Song. 2016. Recurrent marked temporal point processes: Em-
bedding event history to vector. In Proceedings of the 22nd ACM SIGKDD inter-
national conference on knowledge discovery and data mining. 1555â€“1564.
[8] Garrett Fitzmaurice, Marie Davidian, Geert Verbeke, and Geert Molenberghs.
2008. Estimation of the causal effects of time-varying exposures. In Longitudi-
nal Data Analysis . Chapman and Hall/CRC, 567â€“614.
[9] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, FranÃ§ois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The journal of machine learn-
ing research 17, 1 (2016), 2096â€“2030.
[10] Changran Geng, Harald Paganetti, and Clemens Grassberger. 2017. Prediction
of treatment response for combined chemo-and radiation therapy for non-small
cell lung cancer patients using a bio-mathematical model. Scientific reports 7, 1
(2017), 13542.[11] Maciej Jaskowski and Szymon Jaroszewicz. 2012. Uplift modeling for clinical
trial data. In ICML Workshop on Clinical Data Analysis, Vol. 46. 79â€“95.
[12] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197â€“206.
[13] SÃ¶ren R KÃ¼nzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. Metalearners
for estimating heterogeneous treatment effects using machine learning. Proceed-
ings of the national academy of sciences 116, 10 (2019), 4156â€“4165.
[14] Bryan Lim. 2018. Forecasting treatment responses over time using recurrent
marginal structural networks. Advances in neural information processing systems
31 (2018).
[15] Ying-Chun Lin, Chi-Hsuan Huang, Chu-Cheng Hsieh, Yu-Chen Shu, and Kun-Ta
Chuang. 2017. Monetary discount strategies for real-time promotion campaign.
InProceedings of the 26th International Conference on World Wide Web. 1123â€“
1132.
[16] Dugang Liu, Xing Tang, Han Gao, Fuyuan Lyu, and Xiuqiang He. 2023. Explicit
Feature Interaction-aware Uplift Network for Online Marketing. arXiv preprint
arXiv:2306.00315 (2023).
[17] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and
Max Welling. 2017. Causal effect inference with deep latent-variable models.
Advances in neural information processing systems 30 (2017).
[18] Hongyuan Mei and Jason M Eisner. 2017. The neural hawkes process: A neurally
self-modulating multivariate point process. Advances in neural information pro-
cessing systems 30 (2017).
[19] Valentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. 2022. Causal trans-
former for estimating counterfactual outcomes. In International Conference on
Machine Learning. PMLR, 15293â€“15329.
[20] Xinkun Nie and Stefan Wager. 2021. Quasi-oracle estimation of heterogeneous
treatment effects. Biometrika 108, 2 (2021), 299â€“319.
[21] Donald B Rubin. 2005. Causal inference using potential outcomes: Design, mod-
eling, decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322â€“331.
[22] Piotr Rzepakowski and Szymon Jaroszewicz. 2010. Decision trees for uplift mod-
eling. In 2010 IEEE International Conference on Data Mining . IEEE, 441â€“450.
[23] Uri Shalit, Fredrik D Johansson, and David Sontag. 2017. Estimating individual
treatment effect: generalization bounds and algorithms. In International confer-
ence on machine learning. PMLR, 3076â€“3085.
[24] Karishma Sharma, Yizhou Zhang, Emilio Ferrara, and Yan Liu. 2021. Identify-
ing coordinated accounts on social media through hidden influence and group
behaviours. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 1441â€“1451.
[25] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting neural networks for
the estimation of treatment effects. Advances in neural information processing
systems 32 (2019).
[26] Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A Levine, and Xin Yan. 2012.
Facilitating score and causal inference trees for large observational studies. Jour-
nal of Machine Learning Research 13 (2012), 2955.
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. In Advances in neural information processing systems. 6000â€“6010.
[28] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. 2018. GANITE: Esti-
mation of individualized treatment effects using generative adversarial nets. In
International conference on learning representations .
[29] Kanghoon Yoon, Youngjun Im, Jingyu Choi, Taehwan Jeong, and Jinkyoo Park.
2023. Learning Multivariate Hawkes Process via Graph Recurrent Neural Net-
work. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining. 5451â€“5462.
[30] Yizhou Zhang, Defu Cao, and Yan Liu. 2022. Counterfactual neural temporal
point process for estimating causal influence of misinformation on social media.
Advances in Neural Information Processing Systems 35 (2022), 10643â€“10655.
[31] Kui Zhao, Junhao Hua, Ling Yan, Qi Zhang, Huan Xu, and Cheng Yang. 2019.
A unified framework for marketing budget allocation. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
1820â€“1830.
[32] Kailiang Zhong, Fengtong Xiao, Yan Ren, Yaorong Liang, Wenqing Yao, Xiaofeng
Yang, and Ling Cen. 2022. DESCN: Deep Entire Space Cross Networks for In-
dividual Treatment Effect Estimation. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 4612â€“4620.
[33] Tengfei Zhou, Hui Qian, Zebang Shen, Chao Zhang, Chengwei Wang, Shichen
Liu, and Wenwu Ou. 2018. JUMP: a joint predictor for user click and dwell time.
InProceedings of the 27th International Joint Conference on Artificial Intelligence.
AAAI Press . 3704â€“3710.
6255KDDâ€™24, August 25â€“29, 2024, Barcelona, Spain Xin Zhang, et al.
Algorithm
1:Generation process of synthetic GAME
Randomly
generate embedding of users and items and the
parameters of ğ‘“ğ‘”ğ‘–ğ‘“ ğ‘¡(), ğ‘“ğ‘ â„ğ‘œğ‘(), ğ‘“ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’()
Set hyper-parameter ğ‘andğ‘to control the ratio between
two events and counterfactual treatment options
respectively
foreach user ğ‘¢do
ğ»=âˆ…
while ğ‘¡ğ‘
ğ‘¢ğ‘Ÿ< ending time ğ‘‡do
Draw
an in-shop purchase event timestamp ğ‘¡ğ‘ and
features ğ‘“ğ‘ based on ğœ†ğ‘ (ğ‘“ , ğ‘¡|ğ»)between ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ and
ğ‘‡
Draw a gift bundle recommendation event
timestamp ğ‘¡ğ‘”and features ğ‘“ğ‘”based on ğœ†ğ‘”(ğ‘“ , ğ‘¡|ğ»)
between ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ andğ‘‡
ifğ‘¡ğ‘”<ğ‘¡ğ‘ + athen
flag
= â€™Treatmentâ€™
Draw a gift bought label ğ‘™ğ‘ğ‘ğ‘’ğ‘™ based on
ğ‘ğ‘”(ğ‘“ğ‘”|ğ»)
ğ»=ğ»âˆª(ğ‘“ğ‘”, ğ‘¡ğ‘”, ğ‘™ğ‘ğ‘ğ‘’ğ‘™)
ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ=ğ‘¡ğ‘”
else
flag
= â€™Outcomeâ€™
iflast event in ğ»is gift recommendation event
then
ğ»ğ‘
ğ‘“=ğ»âˆ’ğ‘™ğ‘ğ‘ ğ‘¡ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡
else
ğ‘“ğ‘
ğ‘“=ğ‘“ğ‘ except discount in ğ‘“ğ‘ğ‘“is 0.5
ğ‘¡ğ‘ğ‘“=ğ‘šğ‘ğ‘¥(ğ‘™ğ‘ğ‘ ğ‘¡ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡â€²ğ‘ ğ‘¡ğ‘–ğ‘šğ‘’ğ‘ ğ‘¡ğ‘ğ‘šğ‘, ğ‘¡ ğ‘ âˆ’ğ‘)
ğ»ğ‘ğ‘“=ğ»âˆª(ğ‘“ğ‘ğ‘“, ğ‘¡ğ‘ğ‘“,1)
ğ‘ğ‘ğ‘“(ğ‘¦|ğ‘¡ğ‘ğ‘“, ğ»ğ‘ğ‘“)=
ğœ†ğ‘ (ğ‘¦, ğ‘¡ğ‘ğ‘“|ğ»ğ‘ğ‘“)/âˆ«
ğœ†ğ‘ (ğ‘¦, ğ‘¡ğ‘ğ‘“|ğ»ğ‘ğ‘“)ğ‘‘ğ‘¦
Sample counterfactual outcome ğ‘¦ğ‘ğ‘“based
on distribution ğ‘ğ‘ğ‘“(ğ‘¦|ğ‘¡ğ‘ğ‘“, ğ»ğ‘ğ‘“)
A
ctual outcome ğ‘¦set as price in ğ‘“ğ‘ 
ğ‘™ğ‘ğ‘ğ‘’ğ‘™ =1ğ»=ğ»âˆª(ğ‘“ğ‘ , ğ‘¡ğ‘ , ğ‘™ğ‘ğ‘ğ‘’ğ‘™,ğ‘¦,ğ‘¦ ğ‘ğ‘“)ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ=ğ‘¡ğ‘ 
Algorithm
2:Generation process of Hawkes-Tumor
Simulate
Initial Tumor data with counterfactual outcomes
foreach user ğ‘¢do
ğ»=âˆ…
while ğ‘¡ğ‘
ğ‘¢ğ‘Ÿ< ending time ğ‘‡do
Draw
an observation event timestamp ğ‘¡ğ‘œbased on
ğœ†ğ‘œ(ğ‘¡|ğ»)between ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ andğ‘‡
denote last time step before ğ‘¡ğ‘œasğ‘-th step
ğ‘¦ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ=ğ‘¦ğ‘+(ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿâˆ’ğ‘¡ğ‘)(ğ‘¦ğ‘+1âˆ’ğ‘¦ğ‘)
ğ‘¦ğ‘ğ‘“
ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ=ğ‘¦ğ‘+(ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿâˆ’ğ‘¡ğ‘)(ğ‘¦ğ‘ğ‘“
ğ‘+1âˆ’ğ‘¦ğ‘ğ‘“
ğ‘)
ğ»=ğ»âˆª(ğ‘ğ‘, ğ‘¡ğ‘)âˆª( ğ‘¦ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ,ğ‘¦ğ‘ğ‘“
ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ, ğ‘¡ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ)A
SYNTHETIC DATASET GENERATION
In the experiment, we use two synthetic datasets including the syn-
thetic GAME dataset and the Hawkes-Tumor dataset. In this sec-
tion, we introduce how to generate these two datasets based on the
Hawkes process.
A.1 Synthetic GAME
In the generation of Synthetic GAME, motivated by [30] that uses
hidden vectors to represent users and news and simulate the se-
quence of users by modeling Hawkes process. On the basis of this
idea, we first randomly initialize the embeddings of users ğ‘¢and
items ğ‘–and the parameters of various functions and then define the
intensity function of the gift bundle recommendation event and in-
shop purchase event as:
ğœ†ğ‘”(ğ‘“ğ‘”, ğ‘¡ğ‘”|ğ»)=ğ‘¢ğ‘“ğ‘”ğ‘–ğ‘“ ğ‘¡(ğ‘“ğ‘”)+âˆ‘
(ğ‘“ğ‘™,ğ‘¡ğ‘™)âˆˆğ»ğ‘’ğ‘¡ğ‘™âˆ’ğ‘¡ğ‘“ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘“ğ‘”ğ‘–ğ‘“ ğ‘¡(ğ‘“ğ‘™))
ğœ†ğ‘ (ğ‘“ğ‘ , ğ‘¡ğ‘ |ğ»)=ğ‘¢ğ‘“ğ‘ â„ğ‘œğ‘(ğ‘“ğ‘ )+âˆ‘
(ğ‘“ğ‘™,ğ‘¡ğ‘™)âˆˆğ»ğ‘’ğ‘¡ğ‘™âˆ’ğ‘¡ğ‘“ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘“ğ‘ â„ğ‘œğ‘(ğ‘“ğ‘™âˆ—ğ‘™ğ‘ğ‘ğ‘’ğ‘™))
(10)
where ğ‘“ğ‘”, ğ‘“ğ‘ are features of gift bundle recommendation and in-
shop purchase events including itemsâ€™ embedding, number of items
and discount. ğ»denotes the historical events including both gift
bundle recommendations and in-shop purchases. ğ‘™ğ‘ğ‘ğ‘’ğ‘™ indicates
if items are purchased. ğ‘“ğ‘”ğ‘–ğ‘“ ğ‘¡()and ğ‘“ğ‘ â„ğ‘œğ‘()transform the eventâ€™s
features to the representation of values of items. ğ‘“ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’()ensures
itemsâ€™ value in history events are measurable in Hawkes process.
Additionally, unlike in-shop purchase events, the results of gift
bundle recommendations are classified as bought and not bought.
We extra define the probability of recommended gifts bought by
users as:
ğ‘ğ‘”(ğ‘“ğ‘”|ğ»)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘¢ğ‘“ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘“ğ‘”)+1
ğ¿âˆ‘
(ğ‘“ğ‘™,
ğ‘¡ğ‘™)âˆˆğ»(ğ‘“ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘“ğ‘™))+ğ‘›ğ‘œğ‘–ğ‘ ğ‘’
(11)
The process is described in Algorithm 1.
A.2 Hawkes-Tumor
In the generation of Hawkes-Tumor, we first simulate patientsâ€™
data following [4]. For ğ‘™-th time step ğ‘¡ğ‘™, a patient has the corre-
sponding tumor size ğ‘¦ğ‘™, treatments ğ‘ğ‘™and counterfactual tumor
ğ‘¦ğ‘ğ‘“
ğ‘™size with other treatment options. Then we define the outcome
event as the observation of tumor size. The intensity function of the
outcome event can be defined as:
ğœ†ğ‘œ(ğ‘¡|ğ»)=ğ¼ğ‘ğ‘ğ‘ ğ‘’+âˆ‘
ğ‘¡ğ‘™âˆˆğ»ğ›¼(ğ‘’ğ›½(ğ‘¡ğ‘™âˆ’ğ‘¡))) (12)
where ğ¼ğ‘ğ‘ğ‘ ğ‘’ is a randomly generate base intensity and ğ›¼, ğ›½ are
hyper-parameters. The process is described in Algorithm 2.
6256