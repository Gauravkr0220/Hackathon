FRNet: Frequency-based Rotation Network for Long-term Time
Series Forecasting
Xinyu Zhangâˆ—
Harbin Institute of Technology
Shenzhen, China
zhangxinyu45@stu.hit.edu.cnShanshan Fengâˆ—
Centre for Frontier AI Research,
A*STAR
Institute of High Performance
Computing, A*STAR
Singapore, Singapore
victor_fengss@foxmail.comJianghong Maâ€ 
Harbin Institute of Technology
Shenzhen, China
majianghong@hit.edu.cn
Huiwei Lin
Harbin Institute of Technology
Shenzhen, China
linhuiwei@stu.hit.edu.cnXutao Liâ€ 
Harbin Institute of Technology
Shenzhen, China
lixutao@hit.edu.cnYunming Ye
Harbin Institute of Technology
Shenzhen, China
yeyunming@hit.edu.cn
Fan Li
Hong Kong Polytechnic University
Hong Kong, China
fan-5.li@polyu.edu.hkYew Soon Ong
Centre for Frontier AI Research,
A*STAR
Nanyang Technological University
Singapore, Singapore
asysong@ntu.edu.sg
Abstract
Long-term time series forecasting (LTSF) aims to predict future val-
ues for a long time based on historical data. The period term is an
essential component of the time series, which is complex yet impor-
tant for LTSF. Although existing studies have achieved promising
results, they still have limitations in modeling dynamic complicated
periods. Most studies only focus on static periods with fixed time
steps, while very few studies attempt to capture dynamic periods
in the time domain. In this paper, we dissect the original time series
in time and frequency domains and empirically find that changes
in periods are more easily captured and quantified in the frequency
domain. Based on this observation, we propose to explore dynamic
period features using rotation in the frequency domain. To this end,
we develop the frequency-based rotation network (FRNet), a novel
LTSF method to effectively capture the features of the dynamic
complicated periods. FRNet decomposes the original time series
into period and trend components. Based on the complex-valued
linear networks, it leverages a period frequency rotation module
to predict the period component and a patch frequency rotation
âˆ—Both authors contributed equally to this research.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671713module to predict the trend component, respectively. Extensive
experiments on seven real-world datasets consistently demonstrate
the superiority of FRNet over various state-of-the-art methods. The
source code is available at https://github.com/SiriZhang45/FRNet.
CCS Concepts
â€¢Applied computing â†’Forecasting; â€¢Computing method-
ologiesâ†’Neural networks.
Keywords
Long-term time series forecasting; rotation networks; frequency
domain
ACM Reference Format:
Xinyu Zhang, Shanshan Feng, Jianghong Ma, Huiwei Lin, Xutao Li, Yunming
Ye, Fan Li, and Yew Soon Ong. 2024. FRNet: Frequency-based Rotation
Network for Long-term Time Series Forecasting. In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671713
1 Introduction
Long-term time series forecasting (LTSF) involves estimating future
data points over a period based on past observations. It has wide
applications in various fields such as traffic forecasting [ 7], electric
load forecasting [ 53], energy management [ 13], and finance [ 49].
The solution for LTSF has evolved from traditional statistical ap-
proaches (e.g., ARIMA[ 12]) and machine learning approaches (e.g.,
GBRT [ 9]) to deep learning-based techniques [ 43,50,52]. These
methods have made significant advancements in LTSF by effectively
capturing long-term dependencies.
3586
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinyu Zhang et al.
Day 2
Day 3
Day 2Day 1Day 1
Time \
hourValue
0 24 12 6 18
Figure 1: Examples of the dynamic complicated period on the
Traffic dataset. The upper reflects the phenomenon of value
decline in the same period, and the lower shows a shift of
the period, where the peak time on the second day is earlier
than the first day.
As an important component of time series, the period term has
been proven to play an important role in LTSF. For example, DLin-
ear [46] confirms that a simple linear model can outperform previ-
ous sophisticated models by a large margin. Similarly, other stud-
ies [42] have achieved considerable performance by capturing fea-
tures of periods. These studies demonstrate that the periodicity of
time series is the dominant factor for LTSF, and great performance
can be achieved by solely learning the dominant periodicity.
Although existing studies have achieved promising results, they
still have shortcomings in modeling the dynamic complicated peri-
ods. For one thing, most of them only consider the case of the static
period with a fixed time step, which is not practical. The period of
the time series is not strictly static over time. Real-time series may
have multiple periods [ 3,39], and more importantly, each period
component dynamically evolves over time. For instance, we analyze
some cases from the Traffic dataset [ 16] and illustrate them in Fig-
ure 1. The upper figure shows the decline of traffic occupancy over
three consecutive days, suggesting that amplitude patterns across
successive periods may exhibit fluctuations. The lower figure re-
veals a shift in the period phase, where the peak time occurs earlier
on the second day, thereby indicating the presence of a frequency
shift within these periods. Such situations of dynamically evolving
periods widely exist in various time series datasets [3].
For another thing, a few studies try to capture the dynamic com-
plicated periods. To handle those complicated dynamic periodic
patterns, they introduce rotation in the time domain, where rota-
tion is a technique that can describe the changes in periods. The
rolling operation based on the auto-correlation mechanism in Aut-
oformerâ€™s [ 43] can be seen as a rotation operation, as shown in
2(a). Moreover, the evolution of periods can be characterized in a
simple and compact manner through the rotation of quaternions in
Quatformer [3], as shown in 2(b).
In this paper, we dissect the original time series in time and
frequency domains and empirically find that the change of periods
in the frequency domain is easier to capture and quantify by the
ğ‘…ğ‘œğ‘™ğ‘™(ğœ1)
ğ‘…ğ‘œğ‘™ğ‘™(ğœ2)(a) Rotation in Autoformer [43]
ğ‘ = ğ‘ + ğ‘i + ğ‘j + ğ‘‘kğ‘ b c d
ğ‘ à·¤ğ‘
ğœƒ (b) Rotation in Quatformer [3]
ğœ½
RealImaginary
Frequency at 
time step 
ğ‘»ğ’Šâˆ’ğŸ
Frequency 
at time step 
ğ‘»ğ’Š
Frequency at 
time step 
ğ‘»ğ’Š+ğŸ
(c) Frequency rotation in FRNet
Figure 2: (a) The Autoformer implements a head-to-tail ro-
tation on temporally correlated subsequences, which effec-
tively integrates pertinent periodic patterns within the time
series data. (b) Quatformer converts time series data into
quaternion form, which enables a simple and compact repre-
sentation of the changes in periodic patterns over time. (c)
Different from existing studies, our model conducts rotation
operations in the frequency domain to extract the evolving
complexities of periods in time series data.
model than in the time domain. We conduct an in-depth exam-
ination of existing rotation methods, including Autoformer [ 43]
and Quatformer [ 3]. Our analysis reveals that these approaches
do not effectively represent time series data in the time domain.
First, the time-domain representation cannot effectively distinguish
the weak dynamic evolution process over periods. Second, the
high-frequency noise in the data can disrupt the time-domain mor-
phology of the periods, causing interference to model the period
dynamically through rotation. Our proposed idea posits that the
sub-sequences of each complete period can be more aptly repre-
sented in the frequency domain as the real and imaginary parts,
encapsulating both phase and amplitude information. The evolving
dynamics of these periods can therefore be conceptualized as a
spectral rotation process occurring within the frequency domain.
Compared to the time domain, the frequency domain can better
reflect the period changes of time series. Hence, we propose to
explore the features of dynamic complicated periods using rotation
in the frequency domain.
To achieve this, we present the Frequency-based Rotation Net-
work (FRNet), a novel technique for Long-term Time Series Fore-
casting (LTSF). FRNet forecasts the rotational behavior of individual
spectral components within periods to effectively model intricate
and evolving temporal patterns. As illustrated in Figure 2(c), our
proposed method uniquely performs rotation operations in the
3587FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
frequency domain, differing from existing methods that operate
primarily in the time domain. The FRNet comprises two main mod-
ules: a Period Frequency Rotation Module and a Patch Frequency
Rotation Module. These modules serve to predict period and trend
components, respectively.
The Period Frequency Rotation Module specifically targets at
predicting the rotational dynamics of periodical components within
the frequency domain. It endeavors to forecast the spectral char-
acteristics of future periods based on the historical variations in
the frequency spectrum of past periods. In essence, this module
transforms the prediction task into forecasting the rotation angles
and amplitude scales of periodic frequency elements as a complex-
valued multivariate time series. This innovative approach helps us
better understand how periods evolve. We will provide a feasibility
analysis of specific motivations in Section 3.
For trend components, conventional decomposition-based mod-
els such as Autoformer [ 43], FedFormer [ 52], and MICN [ 36] rely
on linear layers in the time domain to directly forecast the entire
trend component. However, this approach would be insufficient
because slight variations in similar trend patterns often result in
significantly different future trends. Hence, a simple linear layer is
likely to produce overly simplistic and flat predictions when han-
dling the whole trend sequence represented in the time domain. To
overcome this limitation, we introduce the Patch Frequency module
for trend forecasting. Drawing inspiration from patch-based archi-
tectures [ 26,35,48] that partition long time series into semantically
meaningful patches, we extend this concept further by transform-
ing each patch into its frequency representation and conducting
predictions within the frequency domain.
The final output forecast combines the trend prediction gener-
ated by the patch-based trend frequency rotation module with the
seasonal prediction obtained from the Period Frequency Rotation
Module. In this way, FRNet effectively captures the nuanced varia-
tions and dynamics inherent in both trend and period components.
To summarize, the main contributions of this work are as follows:
â€¢We analyze the changes of time series in time and frequency
domains and find that the dynamic complicated periods of
time series can be characterized in the frequency domain.
To the best of our knowledge, this is the first work to model
dynamic complicated periods in the frequency domain.
â€¢We propose a novel LTSF method based on rotation in the
frequency domain, namely FRNet. In FRNet, we design a
period frequency rotation module and a patch frequency
rotation module to capture the period and trend components
of the time series, respectively.
â€¢We conducted extensive experiments on seven real-world
datasets in various fields, and empirical results consistently
demonstrate the superiority of our FRNet over various state-
of-the-art methods. We also investigate and analyze the ben-
efits of each module by ablation studies.
2 Related Work
With the vigorous development of deep learning, advanced deep
learning models in LTSF have rapidly developed [ 17,33]. Deep
learning methods have been widely designed for LSFT, including
RNN-based methods [ 16,29], CNN-based methods [ 21,31,36] andTransformer-based methods [ 10,18,19,22,50]. In addition, some
works obtain the representation of time series by self-supervised
learning and make predictions through fine-tuning [ 4,6,40,45].
In this section, we will describe the existing studies on LTSF tasks
related to our work.
Decomposition-Based Models. Autoformer [ 43] uses a moving
average kernel time to extract the seasonal-trend component of
the time series. They propose the auto-correlation mechanism to
discover the period-based dependencies and aggregate similar sub-
series to expand the information utilization. MICN [ 36] also designs
a multi-scale hybrid decomposition and adopts a multi-scale branch
structure to extract different potential patterns separately with
down-sampled convolution and isometric convolution. N-HiTS [ 2]
refines the N-BEATS [ 27] architecture by improving input decompo-
sition with multi-rate data sampling and output synthesis through
multi-scale interpolation. CoST [ 40] utilizes inductive biases in the
model architecture to learn unraveled seasonal and trend represen-
tations and combines new frequency domain comparison losses to
encourage differentiated seasonal representations.
Frequency-Domaind Models. FEDformer [ 52] proposes a frequency-
enhanced decomposed Transformer architecture with a mixture of
experts for seasonal-trend decomposition and computes attention
by enhancing and aggregating frequency domain representations
of time series. ETSformer [ 41] develops the Exponential Smoothing
Attention (ESA) and Frequency Attention (FA) mechanisms to ex-
tract latent growth representations and dominant seasonal patterns.
TDformer [ 47] suggests applying seasonal-trend decomposition as
an initial and utilizes an MLP for predicting trend components and a
Fourier attention Transformer for predicting seasonal components.
FiLM [ 51] applies Legendre polynomial projection to preserve his-
torical information and uses Fourier projection to approximate his-
torical information to eliminate noise. The recent models FreTS [ 37]
and FITS [ 44] perform frequency-domain representations on en-
tire time series and utilize frequency-domain MLPs for time series
prediction. By working in the frequency domain, they can better
understand and predict patterns that might be obscured or difficult
to discern in the original format. These methods not only reduce
the number of model parameters but also showcase competitive
predictive capabilities.
Period-aware Models. The DLinear [ 46] challenges the effective-
ness of the Transformer-based models. They use one-layer linear
models to forecast future time series, which achieves surprisingly
excellent performance. TimesNet [ 42] transforms 1D time series
into 2D tensors based on different periods and employs various
vision backbones to interact with information within and across
these periods. To handle complicated periodical patterns of time
series, Quatformer [ 3] proposes learning-to-rotate attention based
on quaternions which introduces learnable period and phase infor-
mation to depict intricate periodical patterns.
Patch-based Models. PatchTST [ 26] and Crossformer [ 48] intro-
duce the idea of local semantics to models by dividing time series
into patches. The difference is that PatchTST uses the channel in-
dependence strategy to isolate negative impacts between different
variable time series. TSMixer [ 35] draws inspiration from the suc-
cess of MLP-Mixer models in computer vision and adopts the same
3588KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinyu Zhang et al.
patching scheme. By stacking MLPs that interact with time dimen-
sions and MLPs that interact between variables, Mixer provides an
efficient alternative to time series Transformers.
3 Problem Statement and Analysis
3.1 Problem Definition
The long-term time series forecasting task aims to predict the long-
term future data based on historical time series observations. The
historical observations are denoted as ğ‘‹=[ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ¿]âˆˆRğ¶Ã—ğ¿,
whereğ¶is the number of variables of the data and ğ¿is the length
of the historical look-back window. The future data for next ğ»time
steps are defined as ğ‘Œ=[ğ‘¥ğ¿+1,ğ‘¥ğ¿+2,...,ğ‘¥ğ¿+ğ»]âˆˆRğ¶Ã—ğ»andğ»is
also called forecasting horizon. To accurately predict ğ‘Œfromğ‘‹in
the consecutive time series, LTSF is required to learn a mapping
functionF:ğ‘‹ğ¶Ã—ğ¿â†¦â†’ğ‘Œğ¶Ã—ğ».
3.2 Revisit Existing Methods
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949501234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495
(a) DLinear Period Layer
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949501234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495 (b) DLinear Trend Layer
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949501234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495 (c) TSMixer Layer
Figure 3: visualizes the weights ( ğ‘‡Ã—ğ¿) of the linear layers
in DLinear [ 46] and TSMixer [ 20] on the ETTH2 dataset. (a)
displays the period layer weights of DLinear. (b) displays
the trend layer weights of DLinear. (c) displays the temporal
layer weights of TSMixer. In all subfigures, the X-axis repre-
sents the look-back window size ( ğ¿), while the Y-axis denotes
different forecast time steps ( ğ‘‡).
Recently, DLinear [ 46], a simple yet highly potent approach,
achieves remarkable results with only a single linear layer. To gain
deeper insights into the learning patterns of DLinear in time series
forecasting, we conduct an analysis by decomposing the weights
of its linear layer on the ETTH2 dataset [ 16] into period and trend
components, as visualized in Figure 3(a) 3(b). Upon comparing the
trend component against the period component, it is evident that
the period layer exhibits a more consistent and regular distribution
pattern, which strongly suggests the presence of significant period-
icity. This weight visualization offers intuitive evidence highlight-
ing the pivotal role of seasonal elements in enhancing forecasting
performance. In a similar context, Li et al. [ 20] analyze the learning
patterns of the mapping matrix in the Mixer linear model for the
time series forecasting task. The study reveals that various Mixer
variants consistently learn a similar mapping matrix with periodic
characteristics for forecasting tasks, as depicted in Figure3(c). These
linear models, including both DLinear and Mixers, achieve superior
performance by effectively capturing fixed-period seasonal patterns.
Despite their notable achievements, itâ€™s important to note that these
existing linear models [ 20,46] primarily focus on static periodic
characteristics. They do not inherently incorporate dynamic period
information, which can be a significant aspect of real-world time
series datasets where periodicity may vary over time. This suggestsa potential limitation in these methods, highlighting the need for
future research to address the integration of dynamic periodical
variations for enhanced forecasting performance.
There have been some attempts to study periodic dynamic pro-
cesses, such as Autoformer [ 43] and Quadformer [ 3]. In the Auto-
former, the rolling of a series based on periodic time delay can be
seen as a kind of rotation in the time domain. In the Quadformer,
time series are encoded into quaternion form, and the evolution of
periods can be characterized as the rotation of quaternions. The
rotation techniques [ 8] are natural for the periodic temporal infor-
mation, which can effectively capture the dynamic complicated peri-
ods. However, there are still several limitations of existing rotation-
based methods. (1) These methods perform rotation operations
based on the transformer [ 34], which restricts their performance.
Recent studies point out that the cross-attention mechanism [ 20]
and channel-dependent prediction strategies [ 11,26] may nega-
tively impact LTSF outcomes. (2) Performing rotations in the time
domain is less effective due to redundancy and noise sensitivity.
The representation of periods in the time domain tends to be more
intricate and less discriminative, making it difficult to discern sub-
tle changes between periods. Moreover, high-frequency noise can
mask or distort the extraction of dynamic period patterns. (3) Exist-
ing methods typically do not incorporate global period knowledge
across the entire dataset. They rely on the self-discovery of periods
for aggregation, which might not be optimal for extracting intricate
and evolving periods. Diverging from previous works, our study
aims to explore rotation techniques for LTSF from a fresh perspec-
tive, to address the above limitations and model dynamic periodic
patterns in time series data.
3.3 Feasibility Analysis
The dynamic periodic features of time series are more pronounced
and easier to measure in the frequency domain compared to the time
domain. To illustrate this, we use various forms of a sine-periodic
time series as examples and present their characteristics across
both domains in Figure 4. Figure 4 (a) represents the original time
series, while others are changed states with different phenomenons.
Figure 4 (b) depicts a shift in the time series, which is a leftward
movement in the time domain and corresponds to a change in
phase angle in the frequency domain. Figure 4 (c) shows changes
in amplitude, with variations in the range of values in the time
domain corresponding to proportional scaling in the frequency
domainâ€™s amplitude. Figure 4 (d) shows a change in frequency,
where adjustments in the time range correspond to shifts in the
frequency domainâ€™s amplitude spectrum. Figure 4 (e) depicts the
phenomenon of periodic interruptions in frequency, visualized as
spectral leakage, which refers to the spreading or diffusion of am-
plitude across frequencies in the frequency domain. Figure 4 (f)
shows the period with a trend component, evident as a complex
sum of periodic and trend frequencies in the frequency domainâ€™s
amplitude.
Consequently, the evolution of periodic changes over time, such
as frequency rotation, scaling, and interaction, can be effectively
captured by rotations and scalings of each component in the pe-
riodic frequency spectrum. Moreover, non-stationary trend com-
ponents can potentially obscure the detection of these periodic
3589FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
0 5
Time1.0
0.5
0.00.51.0Value
0 5
Frequency010203040Amplitude0Â°45Â°90Â°
135Â°
180Â°
225Â°
270Â°315Â°
(a) Normal Period
0 5
Time1.0
0.5
0.00.51.0Value
0 5
Frequency010203040Amplitude0Â°45Â°90Â°
135Â°
180Â°
225Â°
270Â°315Â° (b) Time Shift Period
0 5
Time1.0
0.5
0.00.51.0Value
0 5
Frequency010203040Amplitude0Â°45Â°90Â°
135Â°
180Â°
225Â°
270Â°315Â° (c) Amplitude Changed Period
0 5
Time1.0
0.5
0.00.51.0Value
0 5
Frequency010203040Amplitude0Â°45Â°90Â°
135Â°
180Â°
225Â°
270Â°315Â°
(d) Frequency Changed Period
0 5
Time1.0
0.5
0.00.51.0Value
0 5
Frequency010203040Amplitude0Â°45Â°90Â°
135Â°
180Â°
225Â°
270Â°315Â° (e) Interruption of Periodic Process
0 5
Time1.0
0.5
0.00.51.0Value
0 5
Frequency010203040Amplitude0Â°45Â°90Â°
135Â°
180Â°
225Â°
270Â°315Â° (f) Period with Trend Component
Figure 4: Different variants of a sine-period and their states in both time and frequency domains. Each sub-figure consists of
three parts, including the time domain, the amplitude of the frequency domain, and the phases of the frequency domain.
patterns. The issue arises where the trend componentâ€™s frequency
in Figure 4(f) complicates the representation of the periodâ€™s fre-
quency, impairing the Frequency Rotation modelâ€™s ability to focus
on seasonal fluctuations. This motivates us to decompose the time
series into seasonal and trend parts, and then solve them separately.
4 Model Framework
RevINDecompositionRevINLinear & ReLU Layer
Significant Periods 
Information
Training DatasetPeriod Frequency 
Rotation Module
Variable Interaction 
Attention
Significant 
Periods PatchingPatch Frequency 
Rotation Module
Variable Interaction 
Attention
PatchingSeasonal 
Frequency 
Rotation 
EncoderTrend 
Frequency 
Rotation 
Encoder
Figure 5: Overview of the Frequency Rotation Network (FR-
Net) Framework.
The overall structure of the FRNet is illustrated in Figure 5. Our
model consists of four main components: Decomposition modules,
Variable Interaction Attention for improving variable interaction,
Period Frequency Rotation module for seasonal component predic-
tion, and Patch Frequency Rotation module for trend component
prediction. Additionally, we extract the global significant periods
from the entire training dataset to enhance the overall effectiveness
of the proposed FRNet model.
4.1 Significant Periods of Time Series Data
When dealing with a time series that exhibits seasonality, itâ€™s easy to
extract multiple periods for each channel by applying Fast FourierTransform (FFT) or Wavelet Transform across the entire training
dataset. These significant periods encapsulate the long-term depen-
dencies in the time series; for instance, the transformer temperature
typically has one period per day.
Since different channels within the same dataset often stem from
related events, they tend to have similar significant periods. This
shared information serves as a global context for segmenting time
series in frequency rotation models. To harness this, we perform
FFT on the complete training dataset as follows:
ğ´ğ‘›=ğ´ğ‘šğ‘
ğ¹ğ¹ğ‘‡(ğ‘‹ğ‘›
ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›)
, ğ‘›âˆˆ1,...,ğ¶, (1)
whereğ‘‹ğ‘›
ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›represents the data of the ğ‘›-th channel in the training
dataset.ğ¹ğ¹ğ‘‡(Â·)andğ´ğ‘šğ‘(Â·)denote the Fast Fourier Transform and
the calculation of amplitude values.
ğ‘“ğ‘›
1,...,ğ‘“ğ‘›
ğ¾=ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘œğ‘ğ¾(ğ´ğ‘›), ğ‘›âˆˆ1,...,ğ¶
ğ‘ğ‘›
ğ‘–=âŒˆğ‘‡
ğ‘“ğ‘›
ğ‘–âŒ‰, ğ‘›âˆˆ1,...,ğ¶, ğ‘–âˆˆ1,...,ğ¾(2)
ğ‘‡ğ‘œğ‘ğ¾ significant periodsâ€™ lengths of the ğ‘›-th channel are ğ‘1,...,ğ‘ğ‘˜.
âŒˆÂ·âŒ‰denotes the upward rounding operation.
ğ‘ğ‘ ğ‘–ğ‘”ğ‘›ğ‘–ğ‘“ğ‘–ğ‘ğ‘ğ‘›ğ‘¡ 1,...,ğ‘ğ‘ ğ‘–ğ‘”ğ‘›ğ‘–ğ‘“ğ‘–ğ‘ğ‘ğ‘›ğ‘¡ ğ‘
=ğ‘€ğ‘œğ‘‘ğ‘’ğ‘„(ğ‘ğ‘›
ğ‘–ğ‘–ğ‘“ ğ‘ğ‘›
ğ‘–<ğ¿).ğ‘›âˆˆ1,...,ğ¶, ğ‘–âˆˆ1,...,ğ¾(3)
To integrate significant periods across all channels, we use an oper-
ationğ‘€ğ‘œğ‘‘ğ‘’ğ‘„(Â·)to identify the top ğ‘„recurring periods from a total
ofğ¶Â·ğ¾periods as the significant periods for the dataset. Impor-
tantly, only periods with lengths shorter than the input sequence
lengthğ¿are taken into account for this selection process.
4.2 Instance Normalization and Decomposition
The initial module follows the normalization and decomposition
adopted in previous works [ 26,46]. We preprocess the input data
ğ‘‹âˆˆRğ¶Ã—ğ¿using RevIN [ 15], which helps minimize differences
between training and testing data distributions. Subsequently, the
decomposition module separates the normalized time series into
3590KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinyu Zhang et al.
Period Frequency 
Rotation Block
Period Frequency 
Rotation Block
FFT
(C, N, E)iFFT
(C, N, F)(C, N, F)(C, N, E)â€¦
Time MixingFrequency 
MixingResidual ResidualComplex -valued Mixer
Frequency 
Norm  inverse 
Frequency 
Norm  
ğ’Ã—blocks
Figure 6: The period frequency rotation module architecture
in FRNet.
its seasonal and trend components:
ğ‘‹ğ‘¡=ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™ ğ‘ƒğ‘ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”(ğ‘‹)
ğ‘˜ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™ =ğ‘ğ‘šğ‘ğ‘¥,
ğ‘‹ğ‘ =ğ‘‹âˆ’ğ‘‹ğ‘¡,(4)
whereğ‘‹ğ‘¡,ğ‘‹ğ‘ âˆˆRğ¶Ã—ğ¿denote the trend part and seasonal part,
respectively. The kernel size for the ğ´ğ‘£ğ‘”ğ‘ğ‘œğ‘œğ‘™(Â·)is typically set
manually. Since we have identified the significant periods within
the time series, we utilize the most prominent period ğ‘ğ‘šğ‘ğ‘¥ as the
kernel size, which corresponds to the one obtained when setting
ğ‘„=1in Equation 3.
4.3 Seasonal Frequency Rotation Encoder
4.3.1 Significant Periods Patching Strategy. Taking significant pe-
riods as global information, we separate time series into period
segments and arrange them in chronological order. For normalized
seasonal component time series ğ‘‹ğ‘ , the reshaping operations are
defined as:
ğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”=ğ‘…ğ‘’ğ‘ â„ğ‘ğ‘ğ‘’ğ‘ğ‘–
ğ‘‡ğ‘Ÿğ‘¢ğ‘›ğ‘(ğ‘‹ğ‘ )
, ğ‘–âˆˆ1,...,ğ‘„, (5)
whereğ‘‡ğ‘Ÿğ‘¢ğ‘›ğ‘(Â·)is to truncate the time series along the time dimen-
sion to the maximum length divisible by the significant period ğ‘ğ‘–.
Theğ‘…ğ‘’ğ‘ â„ğ‘ğ‘ğ‘’(Â·)divides the time series into segments. âŒŠÂ·âŒ‹is the
rounding down operation. And ğ‘‹ğ‘–ğ‘ ğ‘’ğ‘”âˆˆRğ¶Ã—ğ‘Ã—ğ‘ğ‘–,ğ‘=âŒŠğ¿
ğ‘ğ‘–âŒ‹. Then
a linear embedding layer converts each segment of the period inde-
pendently into an embedded representation ğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”ğ¸ğ‘šğ‘âˆˆRğ¶Ã—ğ‘Ã—ğ¸:
ğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”ğ¸ğ‘šğ‘=ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(ğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”) (6)
4.3.2 Variable Interaction Attention. A recent study [ 23] has ob-
served that encoding each variableâ€™s time series into a tensor can
improve predictions when using cross-variable Transformers, effec-
tively reducing overfitting in variable interactions. Based on this,
the Variable Interaction Attention operation reshapes ğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”ğ¸ğ‘šğ‘into
(ğ¶,ğ‘Ã—ğ¸)for calculating attention between variables across the
ğ¶dimension. The standard attention calculation is employed as
follows:ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„,ğ¾,ğ‘‰)=softmaxğ‘„ğ¾ğ‘‡
âˆš
ğ¶
, whereğ‘„represents
queries,ğ¾denotes keys, ğ‘‰stands for values, and ğ¶is number of
variables. Once the attention operation is applied, the resulting
tensor is reshaped back to its original form (ğ¶,ğ‘,ğ¸).4.3.3 Period Frequency Rotation Module. In this module, we design
a channel-independent period frequency rotation model to predict
the seasonal components. The detailed structure is shown in Fig-
ure 6, featuring a series of ğ‘™frequency rotation blocks connected
together.
First, we apply ğ¹ğ¹ğ‘‡(Â·)on each periodic sub-sequence, transform-
ing them into their corresponding frequency domain segments:
ğ¹ğ‘Ÿğ‘’ğ‘ğ‘–
ğ‘ ğ‘’ğ‘”=ğ¹ğ¹ğ‘‡(ğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”ğ¸ğ‘šğ‘), ğ‘–âˆˆ1,...,ğ‘„, (7)
whereğ¹ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘’ğ‘”âˆˆCğ¶Ã—ğ‘Ã—ğ¹,ğ¹=âŒŠğ¸
2âŒ‹+1. Next, we treat the predic-
tion of rotations for multiple frequency components as a forecasting
task for a complex-valued multivariate time series. As shown in
Figure 2(c), the frequency spectra of period segments are organized
chronologically and considered as a multivariate time series with
continuous time steps, where each distinct frequency component
is viewed as an individual variable. We apply the Frequency Norm
to normalize these various frequency components and then re-
store them back to their original scales. To process these frequency
segments, we use the Mixer architecture [ 20,35], which has demon-
strated strong performance in time series forecasting tasks recently.
Note that each layer in the Mixer block is implemented by complex
values.
ğ¹ğ‘Ÿğ‘’ğ‘ğ‘™,ğ‘–
ğ‘ ğ‘’ğ‘”=ğ‘€ğ‘–ğ‘¥ğ‘’ğ‘Ÿ(ğ¹ğ‘Ÿğ‘’ğ‘ğ‘™âˆ’1,ğ‘–
ğ‘ ğ‘’ğ‘”) (8)
Each complex-valued Mixer consists of temporal dimensional in-
teraction, inter-component dimensional interaction, and residual
connections. The temporal dimension interaction learns how in-
dividual frequency components change over time, while the inter-
component interaction learns the spread and transfer between dif-
ferent frequency components, as visualized in Figure 4.
For the first ğ‘™âˆ’1blocks, both input and output shapes are
ğ¹ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘’ğ‘”âˆˆCğ¶Ã—âŒŠğ¿
ğ‘ğ‘–âŒ‹Ã—ğ¹. The final block outputs a shape of ğ¹ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘’ğ‘”âˆˆ
Cğ¶Ã—âŒˆğ»
ğ‘ğ‘–âŒ‰Ã—ğ¹to align with the prediction length. In summary, the
rotation blocks here leverage past âŒŠğ¿
ğ‘ğ‘–âŒ‹periodsâ€™ frequency compo-
nent rotation and scaling data to predict the amplitude and phase
of frequency components for the upcoming âŒˆğ»
ğ‘ğ‘–âŒ‰periods.
Next, we use the inverse Fast Fourier Transform (iFFT) to convert
these predictions back into the time domain, yielding representa-
tions of the forecasted periods:
eğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”ğ¸ğ‘šğ‘=ğ‘–ğ¹ğ¹ğ‘‡(ğ¹ğ‘Ÿğ‘’ğ‘ğ‘–
ğ‘ ğ‘’ğ‘”),ğ‘–âˆˆ1,...,ğ‘„. (9)
Finally, we obtain the forecasting result for the ğ‘–-th significant
period by reversing the reshaping and applying the linear decoder
to generate predictions:
ğ‘‹ğ‘–
ğ‘ =ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ
ğ‘…ğ‘’ğ‘ â„ğ‘ğ‘ğ‘’ğ‘ğ‘–(eğ‘‹ğ‘–
ğ‘ ğ‘’ğ‘”ğ¸ğ‘šğ‘)
, ğ‘–âˆˆ1,...,ğ‘„. (10)
Moreover, a complex-valued linear learns to map the frequency
spectrum of the input sequence to that of the prediction sequence [ 44].
This mapping can be viewed as a special case of frequency rotation,
akin to rotating with a period length equal to ğ¿, and it contributes
to prediction as global frequency domain information.
eğ‘‹ğ‘ =ğ´ğ‘£ğ‘”(ğ‘‹ğ‘–
ğ‘ ).ğ‘–âˆˆ1,...,ğ‘„+1 (11)
The final prediction result eğ‘‹ğ‘ , is obtained by averaging the outcomes
of multiple significant periods.
3591FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
4.4 Trend Frequency Rotation Encoder
Similarly, we introduce a Trend Frequency Rotation Encoder for
predicting trend components. Following the normalization, each
independent channel trend component ğ‘‹ğ‘¡is segmented into ğ‘
patches.
ğ‘‹ğ‘ğ‘ğ‘¡ğ‘â„ =ğ‘ƒğ‘ğ‘¡ğ‘â„(ğ‘‹ğ‘¡) (12)
For each channel, ğ‘ƒğ‘ğ‘¡ğ‘â„(Â·)generates a patch sequence ğ‘‹ğ‘ğ‘ğ‘¡ğ‘â„âˆˆ
Rğ¶Ã—ğ‘Ã—ğ‘ƒ,ğ‘=âŒŠ(ğ¿âˆ’ğ‘ƒ)
ğ‘†+1âŒ‹,ğ‘ƒis the patch length and ğ‘†is the step
size. Since the significant periodic information has been removed,
there are no specific values for ğ‘ƒandğ‘†, so we follow PatchTSTâ€™s set-
tings in this paper. Subsequently, each patch ğ‘‹ğ‘ğ‘ğ‘¡ğ‘â„ is transformed
into a frequency representation following the Variable Interaction
Attention module.
ğ¹ğ‘Ÿğ‘’ğ‘ğ‘ğ‘ğ‘¡ğ‘â„ =ğ¹ğ¹ğ‘‡(ğ‘‹ğ‘ğ‘ğ‘¡ğ‘â„) (13)
Similar to the Period Frequency Rotation module, complex-valued
Mixer blocks will encode these frequency patches and forecast the
frequency representation of upcoming patches. Following, each
patch will be processed by ğ‘–ğ¹ğ¹ğ‘‡(Â·),
eğ‘‹ğ‘ğ‘ğ‘¡ğ‘â„ =ğ‘–ğ¹ğ¹ğ‘‡
ğ‘€ğ‘–ğ‘¥ğ‘’ğ‘Ÿ(ğ¹ğ‘Ÿğ‘’ğ‘ğ‘ğ‘ğ‘¡ğ‘â„)
(14)
where eğ‘‹ğ‘ğ‘ğ‘¡ğ‘â„âˆˆRğ¶Ã—ğ‘€Ã—ğ‘ƒandğ‘€=âŒŠ(ğ»âˆ’ğ‘ƒ)
ğ‘†+1âŒ‹is the number
of output patches. Then a flatten linear layer is used to obtain
the normalized prediction result of future trend components. This
process involves concatenating the patches first and then passing
them through a linear layer to generate the trend prediction result
of lengthğ».
eğ‘‹ğ‘¡=ğ¹ğ‘™ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘›ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(eğ‘‹ğ‘ğ‘ğ‘¡ğ‘â„) (15)
Finally, the trend prediction results are added to the seasonal pre-
diction results, and after passing a linear layer and a ReLU layer, the
final prediction result is obtained through a reverse normalization
layer (RevIN) as shown in Figure 5.
5 Experiments
5.1 Experimental Setup
5.1.1 Dataset. We conduct extensive experiments on seven real-
world datasets for evaluation. These datasets are all multivariate
time series from various fields and their description and statistics
are summarized in A.1.
5.1.2 Baselines. To evaluate the effectiveness of the proposed FR-
Net, we compare it with the following four methodological cate-
gories. (1) Standard Transformer-based models: FEDformer [ 52]
and Autoformer [ 43]; (2) Patch Transformers: PatchTST [ 26] and
GPT2(6) [ 48]; (3) CNN-based models: TimesNet [ 42], MICN [ 36]
and ModernTCN [ 24]; (4) MLP-based models: CI-TSMixer [ 35],
DLinear [46] and FreTS [37].
5.1.3 Metrics. To fairly compare performance, the Mean Squared
Error (MSE) and Mean Absolute Error (MAE) are adopted as evalu-
ation metrics for all experiments.5.1.4 Implementation details. For a fair comparison, we follow the
same data loading parameters (train/val/test split ratio) as previous
works [ 26]. We set the look-back window ğ¿of our model as 512 for
any forecasting horizon and test various lengths of the historical
time series with ğ¿âˆˆ{96,192,336,512}for other baselines to choose
the best as the final result. The prediction horizons ğ»are fixed with
ğ»âˆˆ{96,192,336,720}for all datasets. All experiments are repeated
5 times. In each table, the best-performing results are emphasized
in bold, while the runners-up are underlined.
5.1.5 Seasonal Intensity and simplified Model. Without clear struc-
tural information, decomposing time series into trend and season-
ality might not always be beneficial [ 38,40]. To assess the intensity
of seasonality, we employ an STL-based method [ 47]. For datasets
where trend decomposition is unnecessary, FRNet streamlines by
including only Periodic Frequency Rotation and normalization mod-
ules. More information can be found in Appendix A.4.
5.2 Overall Performance
In this section, we conduct experiments to compare FRNet with
top state-of-the-art baselines in LTSF. Table 1 presents the average
MAE and MSE across 5 runs on seven datasets. The results show
that DLinear, a simple model using fixed period projection and
decomposition, remains competitive. Recent methods like PatchTST
and ModernTCN perform well due to strong encoders.
Nevertheless, our proposed FRNet still outperforms these base-
lines, demonstrating its effectiveness. Specifically, FRNet has the
lowest MSE (0.296) and MAE (0.322) under all experimental scenar-
ios, better than PatchTSTâ€™s scores of 0.304 (MSE) and 0.328 (MAE)
and ModernTCNâ€™s scores of 0.301 (MSE) and 0.325 (MAE). Com-
pared with DLinear, FRNet achieves a 0.032 MSE improvement and
a 0.026 MAE improvement.
Compared to approaches like FITS [ 44] and FreTS [ 37] that
predict directly in the frequency domain using MLP, our method
achieves substantial improvements. The main reason is that our
encoding technique maintains temporal information effectively. In
contrast, their methods use too long time segments for FFT, caus-
ing different frequency contents to blend together and become
indistinguishable.
5.3 Ablation studies
We break down FRNet into three components and delve deeper into
their roles.
Theperiod frequency rotation module is designed to handle
dynamic complicated periods. As this module is the core of FRNet,
it is not easy to disassemble for ablation experiments. Dlinear and
TimeNet, which capture static periods, serve as ablation versions
of our method. As evidenced in Table 1, FRNet surpasses these
methods largely due to the period frequency rotation module, thus
confirming its significance.
Furthermore, we find that the completeness of periods is crucial
in this module. To validate this, we conduct an experiment on
ETTM1 data with a primary period of 96 and 48 time steps. We
examine different sequence lengths to determine their significance
for prediction accuracy. Results from Table 2 show that predictions
perform better when using sequence lengths equivalent to 0.5, 1,
or 2 full periods. This result supports our initial motivation that
3592KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinyu Zhang et al.
Table 1: Results of all methods (lower is better). The best scores are in boldface and the second best scores are underlined.
Mo
delsFRNet
(
Ours)DLinear
(2023)PatchTST
(2023)TimesNet
(2023)Mo
dernTCN
(2024)CI-
TSMixer
(2023)FEDformer
(2022)A
utoformer
(2021)GPT4TS
(2023)Fr
eTS
(2024)Cr
ossformer
(2023)MICN
(2023)
Metric MSE
MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
ET
TH196 0.360
0.391 0.375 0.399 0.370 0.400 0.384 0.402 0.368 0.394 0.368
0.398 0.376 0.416 0.449 0.455 0.376 0.397 0.392 0.415 0.386 0.429 0.396 0.427
192 0.395
0.411 0.405 0.416 0.413 0.429 0.436 0.429 0.405 0.413 0.400
0.418 0.422 0.445 0.468 0.465 0.416 0.418 0.440 0.451 0.419 0.444 0.430 0.430
336 0.409 0.422 0.439
0.443 0.422 0.440 0.491 0.469 0.391 0.412 0.422 0.437 0.452 0.463 0.506 0.494 0.442 0.433 0.477 0.482 0.440 0.461 0.433 0.458
720 0.440
0.456 0.472 0.490 0.447 0.468
0.521 0.500 0.450 0.461 0.450
0.467 0.483 0.496 0.516 0.513 0.477 0.456 0.572 0.551 0.519 0.524 0.474 0.508
ET
TH296 0.263
0.330 0.282 0.346 0.274 0.337 0.340 0.374 0.263 0.332 0.276
0.339 0.343 0.385 0.372 0.406 0.285 0.342 0.313 0.375 0.628 0.563 0.289 0.357
192 0.325 0.373 0.350
0.396 0.341 0.382 0.402 0.414 0.320 0.374 0.330
0.374 0.429
0.438 0.441 0.442 0.354 0.389 0.424 0.449 0.703 0.624 0.409 0.438
336 0.320 0.380 0.410
0.437 0.329 0.384 0.452 0.452 0.313 0.376 0.357 0.401 0.489 0.485 0.479 0.478 0.373 0.407 0.500 0.498 0.827 0.675 0.417 0.452
720 0.377
0.420 0.587 0.544 0.379 0.422 0.462
0.468 0.392 0.433 0.395 0.436 0.463 0.481 0.488 0.496 0.406 0.441 0.809 0.550 1.181 0.840 0.426 0.473
ET
TM196 0.287
0.340 0.299 0.343 0.293 0.346 0.338
0.375 0.292 0.346 0.291
0.346 0.356
0.406 0.498 0.474 0.292 0.346 0.325
0.365 0.316 0.373 0.314 0.360
192 0.327
0.360 0.335 0.365 0.333 0.370 0.374 0.387 0.332 0.368 0.334
0.369 0.391 0.424 0.586 0.513 0.332 0.372
0.353 0.385 0.377 0.411 0.359 0.387
336 0.362
0.382 0.369 0.386 0.369 0.392 0.410 0.411 0.365 0.391
0.367 0.387 0.441
0.453 0.657 0.543 0.366 0.394 0.383 0.405 0.431 0.442 0.398 0.413
720 0.408
0.411 0.425 0.421 0.416 0.420
0.478 0.450 0.416 0.417 0.421 0.415 0.482
0.476 0.719 0.572 0.417 0.421 0.440 0.436 0.600 0.547 0.459 0.464
ET
TM296 0.161
0.251 0.167 0.260 0.166 0.256 0.187 0.267 0.166 0.256 0.165 0.255 0.189
0.281 0.254 0.325 0.173 0.262 0.173 0.268 0.421 0.461 0.178 0.273
192 0.219
0.292 0.224 0.303 0.233 0.296 0.249 0.309 0.222 0.293 0.225
0.299 0.257 0.324 0.280 0.336 0.229 0.301 0.247 0.324 0.503 0.519 0.245 0.316
336 0.267
0.324 0.281 0.342 0.274 0.329 0.321 0.351 0.272 0.324 0.273
0.329 0.325 0.364 0.350 0.382 0.286 0.341 0.319 0.363 0.611 0.580 0.295 0.350
720 0.347
0.377 0.397 0.421 0.362 0.385 0.408 0.403 0.351 0.381 0.361
0.383 0.429 0.424 0.437 0.429 0.378 0.401 0.440 0.448 0.996 0.750 0.389 0.406
Ele
ctricity96 0.129 0.225 0.140
0.237 0.129 0.222 0.168
0.272 0.129 0.226
0.129 0.225 0.189
0.305 0.199 0.314 0.139 0.238 0.134 0.232 0.187 0.283 0.159 0.267
192 0.138
0.238 0.153 0.249 0.147 0.249 0.184 0.289 0.143 0.239 0.146
0.242 0.205 0.320 0.215 0.325 0.153 0.251 0.150 0.250 0.258 0.330 0.168 0.279
336 0.161
0.259 0.169 0.267 0.163 0.259 0.198
0.300 0.161 0.259 0.164
0.261 0.212 0.327 0.234 0.340 0.169 0.266 0.168 0.268 0.323 0.369 0.196 0.308
720 0.194
0.289 0.203 0.301 0.197 0.290 0.220 0.320 0.191 0.286 0.186
0.282 0.245 0.352 0.289 0.380 0.206 0.297 0.203 0.302 0.404 0.423 0.203 0.312
T
raffic96 0.353
0.248 0.410 0.282 0.360 0.249 0.593
0.321 0.368 0.253 0.369 0.264 0.577 0.360 0.642 0.408 0.388 0.282 0.405 0.386 0.512 0.290 0.508 0.301
192 0.375
0.256 0.423 0.287 0.379 0.256 0.617
0.336 0.379 0.261
0.393 0.278 0.607 0.374 0.640 0.405 0.407 0.290 0.429 0.297 0.523 0.297 0.536 0.315
336 0.385
0.264 0.436 0.296 0.392 0.264 0.629
0.336 0.397 0.270 0.406 0.285 0.624 0.384 0.621 0.384 0.412 0.294 0.450 0.307 0.530 0.300 0.525 0.310
720 0.423
0.284 0.466 0.315 0.432 0.286 0.640
0.350 0.440 0.296 0.445 0.304 0.625 0.381 0.709 0.435 0.450 0.312 0.490 0.330 0.573 0.313 0.571 0.323
W
eather96 0.143
0.195 0.176 0.237 0.149 0.198 0.172 0.220 0.149 0.200 0.146 0.197 0.221
0.304 0.271 0.341 0.162 0.212 0.163 0.224 0.153 0.217 0.161 0.226
192 0.184
0.235 0.220 0.282 0.194 0.241 0.219 0.261 0.196 0.245 0.191 0.240 0.325
0.372 0.320 0.374 0.204 0.248 0.199 0.256 0.197 0.269 0.220 0.283
336 0.235
0.275 0.265 0.319 0.245 0.282 0.280 0.306 0.238 0.277 0.244
0.280 0.386 0.408 0.350 0.387 0.254 0.286 0.247 0.297 0.252 0.311 0.275 0.328
720 0.306
0.330 0.323 0.362 0.314 0.334 0.365
0.359 0.314 0.334 0.316 0.333 0.415 0.423 0.428 0.434 0.326 0.337 0.308 0.343 0.318 0.363 0.311 0.356
A
vg. 0.296
0.322 0.328 0.348 0.304 0.328 0.376 0.362 0.301 0.325 0.306
0.327 0.391 0.394 0.443 0.423 0.317 0.336 0.355 0.362 0.485 0.453 0.358 0.362
successful prediction depends on rotations based on entire periodic
patterns. Moreover, combining significant periods across multiple
resolutions further enhances prediction results.
Table 2: Period Completeness Ablation Study.
Significant
Period Length 96
192 336 720
12 0.294
0.329 0.363 0.420
20 0.300
0.333 0.364 0.323
24 0.287 0.329 0.362 0.423
44 0.302
0.333 0.366 0.423
48 0.288 0.331
0.363 0.418
92 0.297
0.335 0.368 0.422
96 0.288 0.331
0.363 0.422
192 0.288 0.332
0.365 0.421
200 0.297
0.335 0.367 0.423
512(Input length) 0.327
0.353 0.395 0.482
96 + 48 + 24 + 12 + 512 0.287
0.327 0.362 0.408
Time series decomposition is to separate the trend component
from the original time series and process it separately. We choose
ETTM2 and Weather datasets for the ablation experiment of de-
composition structure, and these two datasets have different trend
component proportions as shown in Table 8. We compare predic-
tions from models with and without decomposition for each dataset.
The results in Table 3 reveal that model performance depends on
the significance of trend components. Models incorporating decom-
position perform better when trends are prominent, whereas those
without decomposition are better when trends are less significant.
Thepatch frequency rotation module is designed to efficiently
predict trend components within time series. To validate its effec-
tiveness, we substitute our trend prediction module with a three-
layer linear model and the PatchTST model for predicting theWeather dataset. The results presented in Table 3 show that our
patch-based frequency prediction module outperforms these alter-
natives in forecasting trend components. We provide a visualization
of the trend prediction layer weights for the Weather dataset in
Appendix A.3.
Table 3: Decomposition Structure and Patch Frequency Rota-
tion Module Ablation Study.
Datasets w/o
Decomposition w
Decomposition
PFR
PatchTST Linear
ET
TM296 0.161 0.169 0.170
0.170
192 0.219 0.226
0.226 0.225
336 0.267 0.271 0.277
0.278
720 0.347 0.353
0.360 0.350
W
eather96
0.169 0.143 0.150 0.150
192
0.212 0.184 0.196 0.200
336
0.258 0.235 0.245 0.249
720
0.317 0.306 0.310 0.317
5.4 Model Effenciency
Running efficiency plays a crucial role in time series forecasting.
The complexity of the period\patch frequency rotation module is
ğ‘‚(ğ‘€Â·ğ‘),ğ‘€=ğ‘ƒ/2+1,ğ‘=ğ¿/ğ‘ +1. To assess efficiency, we use
the weather dataset to measure model speed and memory usage. In
Table 3â€™s left column, we list the number of trainable parameters
for various TSF models with a look-back window of 512 steps and a
forecast horizon of 336 steps. The right side compares their training
times using a batch size of 16. Our modelâ€™s number of parameters
and training time are only slightly larger than the simple linear
model DLinear and are much smaller than other time-domain and
frequency-domain models. These results demonstrate that PRNet
offers real-world deployment benefits due to its efficiency.
3593FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 4: Running time per epoch and Memory usages.
Mo
dels Parameters(M)
Training Time(s/epoch)
FEDformer 15.288
158.385
TimesNet 75.743
179.785
Autoformer 8.996
58.730
PatchTST 1.432
47.488
DLinear 0.345
17.670
FRNet 0.510 27.380
5.5 Experiment for Anomaly Detection
This exploration aims to demonstrate FRNetâ€™s potential as a broadly
applicable time series encoder, transcending its core forecasting
capabilities. We conduct anomaly detection experiments on five
widely-used datasets: MSL, PMS [ 1], SMD [ 32], SWaT [ 25], and
SMAP [ 14]. Following the standard approach [ 42], we employ the
reconstruction task and utilize the reconstruction error as a measure
to detect anomalies. The results in Table 5 indicate that FRNet
delivers competitive performance compared to the previous state-
of-the-art model, TimesNet.
Table 5: Anomaly detection result of F1-scores on 5 datasets.
Mo
dels MSL
PMS SDM SWaT SMAP Avg F1
A
utoformer 79.05
93.29 85.11 92.74 71.12 84.26
DLinear 84.88
93.55 77.10 87.52 69.26 82.46
Crossformer 84.19
93.30 79.70 90.92 69.14 83.45
TimesNet 85.15 97.47
85.81 91.74 71.52 86.34
FEDformer 78.57
97.23 85.08 93.19 70.76 84.97
Anomaly Trans 83.31
79.40 85.49 83.10 71.18 80.50
PatchTST 85.14
96.37 84.44 87.24 70.91 84.82
FRNet 85.16 97.41 85.60 93.29 69.41
86.19
6 Conclusion
In this study, we aim to enhance the modeling of dynamically com-
plex periods in time series. Inspired by the analyses that period
dynamics can be represented as a spectral rotation process in the
frequency domain, we introduce a new frequency-based rotation
network, which is the first study to capture dynamic periods in
the frequency domain. The proposed FRNet mainly consists of two
novel modules: Period Frequency Rotation for seasonal compo-
nent prediction and Patch Frequency Rotation for trend component
prediction. Extensive experimental results show that FRNet consis-
tently outperforms a wide range of time-series forecasting models
across seven real-world datasets.
Acknowledgment
This work was supported in part by NSFC under Grants 62376072,
62272130, 62202122, 62073272, 62202124 and in part by Shenzhen
Science and Technology Program Nos.JCYJ20210324120208022, KCX
FZ20211020163403005 and GXWD20231130110308001, and the Guang-
dong Basic and Applied Basic Research Foundation under Grant
No.2024A1515011949.
References
[1]Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. 2021. Practical ap-
proach to asynchronous multivariate time series anomaly detection and localiza-
tion. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery
& data mining. 2485â€“2494.[2]Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez,
Max Mergenthaler Canseco, and Artur Dubrawski. 2023. Nhits: Neural hierarchi-
cal interpolation for time series forecasting. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 37. 6989â€“6997.
[3]Weiqi Chen, Wenwei Wang, Bingqing Peng, Qingsong Wen, Tian Zhou, and Liang
Sun. 2022. Learning to rotate: Quaternion transformer for complicated periodical
time series forecasting. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 146â€“156.
[4]Mingyue Cheng, Qi Liu, Zhiding Liu, Zhi Li, Yucong Luo, and Enhong Chen. 2023.
FormerTime: Hierarchical Multi-Scale Representations for Multivariate Time
Series Classification. In Proceedings of the ACM Web Conference 2023. 1437â€“1445.
[5]Robert B Cleveland, William S Cleveland, Jean E McRae, and Irma Terpenning.
1990. STL: A seasonal-trend decomposition. J. Off. Stat 6, 1 (1990), 3â€“73.
[6]Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Ming-
sheng Long. 2024. Simmtm: A simple pre-training framework for masked time-
series modeling. Advances in Neural Information Processing Systems 36 (2024).
[7]Weiwei Fang, Wenhao Zhuo, Jingwen Yan, Youyi Song, Dazhi Jiang, and Teng
Zhou. 2022. Attention meets long short-term memory: A deep learning network
for traffic flow forecasting. Physica A: Statistical Mechanics and its Applications
587 (2022), 126485.
[8]Shanshan Feng, Lisi Chen, Kaiqi Zhao, Wei Wei, Xuemeng Song, Shuo Shang,
Panos Kalnis, and Ling Shao. 2022. ROLE: Rotated Lorentzian graph embedding
model for asymmetric proximity. IEEE Transactions on Knowledge and Data
Engineering (2022).
[9]Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting
machine. Annals of statistics (2001), 1189â€“1232.
[10] Matt Gorbett, Hossein Shirazi, and Indrakshi Ray. 2023. Sparse Binary Trans-
formers for Multivariate Time Series Modeling. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 544â€“556.
[11] Lu Han, Han-Jia Ye, and De-Chuan Zhan. 2024. The capacity and robustness
trade-off: Revisiting the channel independent strategy for multivariate time series
forecasting. IEEE Transactions on Knowledge and Data Engineering (2024).
[12] Siu Lau Ho and Min Xie. 1998. The use of ARIMA models for reliability forecasting
and analysis. Computers & industrial engineering 35, 1-2 (1998), 213â€“216.
[13] Tao Hong, Pierre Pinson, Shu Fan, Hamidreza Zareipour, Alberto Troccoli, and
Rob J Hyndman. 2016. Probabilistic energy forecasting: Global energy forecasting
competition 2014 and beyond. , 896â€“913 pages.
[14] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and
Tom Soderstrom. 2018. Detecting spacecraft anomalies using lstms and nonpara-
metric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD interna-
tional conference on knowledge discovery & data mining. 387â€“395.
[15] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and
Jaegul Choo. 2021. Reversible instance normalization for accurate time-series
forecasting against distribution shift. In International Conference on Learning
Representations.
[16] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling
long-and short-term temporal patterns with deep neural networks. In The 41st
international ACM SIGIR conference on research & development in information
retrieval. 95â€“104.
[17] Pedro Lara-BenÃ­tez, Manuel Carranza-GarcÃ­a, and JosÃ© C Riquelme. 2021. An
experimental review on deep learning architectures for time series forecasting.
International journal of neural systems 31, 03 (2021), 2130001.
[18] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,
and Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-
neck of transformer on time series forecasting. Advances in neural information
processing systems 32 (2019).
[19] Yiduo Li, Shiyi Qi, Zhe Li, Zhongwen Rao, Lujia Pan, and Zenglin Xu. [n.d.].
SMARTformer: Semi-Autoregressive Transformer with Efficient Integrated Win-
dow Attention for Long Time Series Forecasting. ([n. d.]).
[20] Zhe Li, Zhongwen Rao, Lujia Pan, and Zenglin Xu. 2023. Mts-mixers: Multivariate
time series forecasting via factorized temporal and channel mixing. arXiv preprint
arXiv:2302.04501 (2023).
[21] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and
Qiang Xu. 2022. Scinet: Time series modeling and forecasting with sample
convolution and interaction. Advances in Neural Information Processing Systems
35 (2022), 5816â€“5828.
[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and
Schahram Dustdar. 2021. Pyraformer: Low-complexity pyramidal attention for
long-range time series modeling and forecasting. In International conference on
learning representations.
[23] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and
Mingsheng Long. 2023. itransformer: Inverted transformers are effective for time
series forecasting. arXiv preprint arXiv:2310.06625 (2023).
[24] Donghao Luo and Xue Wang. 2024. Moderntcn: A modern pure convolution
structure for general time series analysis. In The Twelfth International Conference
on Learning Representations.
3594KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinyu Zhang et al.
[25] Aditya P Mathur and Nils Ole Tippenhauer. 2016. SWaT: A water treatment
testbed for research and training on ICS security. In 2016 international workshop
on cyber-physical systems for smart water networks (CySWater). IEEE, 31â€“36.
[26] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In
The Eleventh International Conference on Learning Representations.
[27] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N-
BEATS: Neural basis expansion analysis for interpretable time series forecasting.
InInternational Conference on Learning Representations.
[28] Jinwen Qiu, S Rao Jammalamadaka, and Ning Ning. 2018. Multivariate Bayesian
Structural Time Series Model. J. Mach. Learn. Res. 19, 1 (2018), 2744â€“2776.
[29] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.
DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-
national Journal of Forecasting 36, 3 (2020), 1181â€“1191.
[30] Steven L Scott and Hal R Varian. 2015. Bayesian variable selection for nowcasting
economic time series. In Economic analysis of the digital economy. University of
Chicago Press, 119â€“135.
[31] Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. 2019. Think globally, act locally:
A deep neural network approach to high-dimensional time series forecasting.
Advances in neural information processing systems 32 (2019).
[32] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust
anomaly detection for multivariate time series through stochastic recurrent
neural network. In Proceedings of the 25th ACM SIGKDD international conference
on knowledge discovery & data mining. 2828â€“2837.
[33] JosÃ© F Torres, Dalil Hadjout, Abderrazak Sebaa, Francisco MartÃ­nez-Ãlvarez, and
Alicia Troncoso. 2021. Deep learning for time series forecasting: a survey. Big
Data 9, 1 (2021), 3â€“21.
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[35] E Vijay, Arindam Jati, Nam Nguyen, Gift Sinthong, and Jayant Kalagnanam. 2023.
TSMixer: lightweight MLP-mixer model for multivariate time series forecasting.
InACM SIGKDD International Conference on Knowledge Discovery and Data
Mining.
[36] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao.
2022. Micn: Multi-scale local and global context modeling for long-term series
forecasting. In The Eleventh International Conference on Learning Representations.
[37] S Wang, K Yi, Q Zhang, W Fan, C Wang, H He, D Lian, L Cao, and Z Niu. 2023.
Frequency-domain MLPs are More Effective Learners in Time Series Forecasting.
In37th Conference on Neural Information Processing Systems (NeurIPS 2023).
[38] Qingsong Wen, Jingkun Gao, Xiaomin Song, Liang Sun, Huan Xu, and Shenghuo
Zhu. 2019. RobustSTL: A robust seasonal-trend decomposition algorithm for
long time series. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 5409â€“5416.
[39] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.
RobustPeriod: Robust time-frequency mining for multiple periodicity detection.
InProceedings of the 2021 international conference on management of data. 2328â€“
2337.
[40] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2021.
CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for
Time Series Forecasting. In International Conference on Learning Representations.
[41] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022.
Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv
preprint arXiv:2202.01381 (2022).
[42] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2022. TimesNet: Temporal 2D-Variation Modeling for General Time Series
Analysis. In The Eleventh International Conference on Learning Representations.
[43] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-
composition transformers with auto-correlation for long-term series forecasting.
Advances in Neural Information Processing Systems 34 (2021), 22419â€“22430.
[44] Zhijian Xu, Ailing Zeng, and Qiang Xu. 2023. FITS: Modeling Time Series with
10ğ‘˜Parameters. In The Twelfth International Conference on Learning Representa-
tions.
[45] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,
Yunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation of
time series. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.
8980â€“8987.
[46] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are transformers
effective for time series forecasting?. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 37. 11121â€“11128.
[47] Xiyuan Zhang, Xiaoyong Jin, Karthick Gopalswamy, Gaurav Gupta, Youngsuk
Park, Xingjian Shi, Hao Wang, Danielle C Maddix, and Bernie Wang. 2022. First
De-Trend then Attend: Rethinking Attention for Time-Series Forecasting. In
NeurIPSâ€™22 Workshop on All Things Attention: Bridging Different Perspectives on
Attention.
[48] Yunhao Zhang and Junchi Yan. 2023. Crossformer: Transformer Utilizing Cross-
Dimension Dependency for Multivariate Time Series Forecasting. In The Eleventh
International Conference on Learning Representations. https://openreview.net/forum?id=vSVLM2j9eie
[49] Dawei Zhou, Lecheng Zheng, Yada Zhu, Jianbo Li, and Jingrui He. 2020. Domain
adaptive multi-modality neural attention network for financial forecasting. In
Proceedings of The Web Conference 2020. 2230â€“2240.
[50] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-
quence time-series forecasting. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 35. 11106â€“11115.
[51] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin,
et al.2022. Film: Frequency improved legendre memory model for long-term
time series forecasting. Advances in Neural Information Processing Systems 35
(2022), 12677â€“12690.
[52] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
2022. Fedformer: Frequency enhanced decomposed transformer for long-term
series forecasting. In International Conference on Machine Learning. PMLR, 27268â€“
27286.
[53] Yihong Zhou, Zhaohao Ding, Qingsong Wen, and Yi Wang. 2022. Robust load
forecasting towards adversarial attacks via bayesian learning. IEEE Transactions
on Power Systems 38, 2 (2022), 1445â€“1459.
A Appendix
A.1 Dataset Details
We conduct extensive experiments on six real-world datasets for
evaluation. These datasets are all multivariate time series from
various fields and their statistics are summarized in Table 6. (1)
ETT [ 50]consists of two sub-datasets, which are named ETTh and
ETTm. They record the temperature changes of electric transform-
ers from July 2016 to July 2018, with frequencies of 1 hour and 15
minutes, respectively. (2) Traffic [ 16]describes the hourly road
occupancy rates measured by different sensors on San Francisco
Bay area freeways. (3) Electricity [ 16]contains the electricity con-
sumption of 321 customers recorded hourly from 2012 to 2014. (4)
Weather [ 50]records the values of 21 meteorological indicators
every 10 minutes for the entire year of 2020.
Table 6: Statistics of datasets in our experiments.
Dataset Sequence Length Features Frequency
ETTH1 17,420 7 1h
ETTH2 17,420 7 1h
ETTM1 69,680 7 15 min
ETTM2 69,680 7 15 min
Traffic 17,544 862 1h
Electricity 26,304 321 1h
Weather 52,696 21 10 min
A.2 Existing Rotation Models for LTSF
Recently, there have been some attempts to simulate periodic dy-
namic processes. The implementation of the auto-correlation mech-
anism in Autoformer [ 43] can be considered as a rotation based on
periodicity. Auto-correlation discovers the period-based dependen-
cies by calculating the series auto-correlation and aggregates similar
sub-series by time delay aggregation to expand the periodic infor-
mation utilization. The rolling of a series based on periodic time
delay can be seen as a rotation in the time domain. Quatformer [ 3]
innovatively proposes an attention mechanism based on a hidden
space rotation, which encodes the period and phase information
specifically to handle complicated periodical patterns. Specifically,
Quatformer encodes the time series into quaternion form and uses
3595FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 7: Hyerparamter Configurations on Long-term Forecasting Benchmarks.
Datasets QSignificant Periods Period Rotation Layers Patch Rotation Layers w/wo Variable Interaction Attention
ETTH1 1 24,48 1 0 wo
ETTH2 1 24,48,12 1 0 wo
ETTM1 1 96,48,24,12 1 0 wo
ETTM2 1 96,48,24,12 1 0 wo
Electricity 3 24,48,12 2 0 wo
Traffic 3 24,12 3 0 w
Weather 3 144,72,48 3 3 wo
(a) Trend Prediction Layer of DLinear
(b) Flatten Linear Layer of FRNet
Figure 8: Visualization of Weights for Trend Prediction Lay-
ers on the Weather Dataset
ğ‘…ğ‘œğ‘™ğ‘™(ğœ1)
ğ‘…ğ‘œğ‘™ğ‘™(ğœ2)
(a) Rotation in Autoformer
ğ‘ = ğ‘ + ğ‘i + ğ‘j + ğ‘‘kğ‘ b c d
ğ‘ à·¤ğ‘
ğœƒ
(b) Rotation in Quatformer
Figure 7: Existing Rotation Models for LTSF1-D convolution to obtain the angle and phase to rotate the quater-
nion. After being encoded into unit quaternions, the evolution of
periods can be characterized simply and compactly through the
three-dimensional rotation of quaternions. Quatformer[3] has the
most related motivation compared to ours. The core-rotation oper-
ation of the above works are shown in Figure 7.
A.3 Visualization of Weights for Trend
Prediction Layers
We visualized the weights of the trend linear layer in DLinear and
the flatten liner layer in the Patch Frequency Rotation module,
respectively on the Weather dataset, As shown in Figure 8. For
the trend layer of DLinear, most network parameters only learn
noise, but the trend layer shows a higher weight of information
closer to the output, which enables the beginning of the prediction
sequence to align with the end of the input sequence. For the flatten
layer of the Patch Frequency module, there are significant banded
weights can be seen. This indicates that our module can capture
more important details and give more attention, rather than just
focusing on tail information.
A.4 Seasonal Intensity and simplified Model
According to standard structure time series analysis, time series is
the sum of trend, season, and remainder error variables [28, 30],
ğ‘‹ğ‘¡=ğ‘‡ğ‘¡+ğ‘†ğ‘¡+ğ‘…ğ‘¡ (16)
whereğ‘‡ğ‘¡,ğ‘†ğ‘¡, andğ‘…ğ‘¡represent the trend, seasonal, and remainder
error components, respectively. Many statistical prediction mod-
els and deep learning models adopt the time series decomposition
scheme [ 36,43,47,52], which decomposes the time series into more
predictable trend and seasonal patterns. However, without suffi-
cient structure prior, it is difficult to achieve complete decoupling
between the trend and seasonal patterns [ 38,40], which leads to
that the trend decomposition prediction methods are not always
necessary for different data. For datasets with very weak trend
components, the trend decomposition module not only increases
the model size, but improper decomposition may disrupt the dy-
namic process of real seasonal content. Therefore we use a seasonal
intensity quantification method based on STL decomposition [ 5]
to quantify the strength of seasonality for each dataset, which was
3596KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinyu Zhang et al.
Table 8: Seasonality intensity of datasets.
Dataset Electricity Traffic Weather ETT
Seasonality Strength 0.998 0.998 0.476 0.993
proposed in [47].
ğ‘†=ğ‘šğ‘ğ‘¥(0,ğ‘‰ğ‘ğ‘Ÿ(ğ‘…ğ‘¡)
ğ‘‰ğ‘ğ‘Ÿ(ğ‘†ğ‘¡)+ğ‘‰ğ‘ğ‘Ÿ(ğ‘…ğ‘¡)) (17)
Following Equation 17, we summarize the seasonality intensity
of benchmarks in Table 8. For Electricity, traffic, and ETT which
are strongly seasonal data, there is no need for trend decompo-
sition. While the weather dataset demonstrates less seasonality
and more trend, requiring separate predictions after decomposition.For datasets that do not require trend decomposition, the overall
model architecture will be simplified to include only the Periodic
Frequency Rotation module and the instance normalization module.
A.5 Implementation Details
In this section, we provide the hyperparameters corresponding to
the experimental results. The specific hyperparameters are shown
in Table 7, where ğ‘„indicates the number of significant periods
selected for the corresponding dataset. Significant Periods give
the step size of these Q periods. Period Rotation Layers and Patch
Rotation Layers refer to the number of corresponding rotation
blocks. Variable Interaction Attention indicates whether the model
includes or excludes this feature (with 'w'signifying inclusion and
'wo' indicating exclusion).
3597