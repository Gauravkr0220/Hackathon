Improving Robustness of Hyperbolic Neural Networks by
Lipschitz Analysis
Yuekang Liâˆ—
Applied Mathematics and Computational Sciences,
DNAS, Duke Kunshan Univerity
Kunshan, China
yuekang.li@duke.eduYidan Maoâˆ—
Applied Mathematics and Computational Sciences,
DNAS, Duke Kunshan Univerity
Kunshan, China
ym188@duke.edu
Yifei Yang
Electronic Information School,
Wuhan Univerity
Wuhan, China
yfyang@whu.edu.cnDongmian Zouâ€ 
Zu Chongzhi Center and Data Science Research Center,
DNAS, Duke Kunshan Univerity
Kunshan, China
dongmian.zou@duke.edu
Abstract
Hyperbolic neural networks (HNNs) are emerging as a promising
tool for representing data embedded in non-Euclidean geometries,
yet their adoption has been hindered by challenges related to sta-
bility and robustness. In this work, we conduct a rigorous Lipschitz
analysis for HNNs and propose using Lipschitz regularization as a
novel strategy to enhance their robustness. Our comprehensive in-
vestigation spans both the PoincarÃ© ball model and the hyperboloid
model, establishing Lipschitz bounds for HNN layers. Importantly,
our analysis provides detailed insights into the behavior of the
Lipschitz bounds as they relate to feature norms, particularly dis-
tinguishing between scenarios where features have unit norms
and those with large norms. Further, we study regularization using
the derived Lipschitz bounds. Our empirical validations demon-
strate consistent improvements in HNN robustness against noisy
perturbations.
CCS Concepts
â€¢Computing methodologies â†’Neural networks; Regulariza-
tion; â€¢Mathematics of computing â†’Graphs and surfaces.
Keywords
Hyperbolic neural networks, robustness, Lipschitz bounds, noisy
data
ACM Reference Format:
Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou. 2024. Improving
Robustness of Hyperbolic Neural Networks by Lipschitz Analysis. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671875
âˆ—Both authors contributed equally to this research.
â€ Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718751 Introduction
In recent years, the emergence of datasets with complex geometric
structures has underscored the limitations of traditional Euclidean
spaces for accurate data representation. To capture the essence
of numerous tree-like and hierarchical structures encountered in
various applications such as product recommendation [ 53], drug dis-
covery [ 54] and image recognition [ 1], a growing body of research
has embraced hyperbolic spaces as a natural way of embedding
such data with low distortion [ 4,7,43]. Among other successes of
deep learning methods, hyperbolic neural networks (HNNs) offer
a geometric and expressive data-driven approach to representing
such data, outperforming Euclidean or graph neural networks in
numerous applications [40].
Although HNNs have demonstrated significant potential, their
practical implementation is challenged by the complex geometry of
the hyperbolic space in which they operate. Firstly, as features move
away from the origin, gradients can grow exponentially, leading to
the â€œgradient explosion problemâ€ in training [36, 41, 43, 55]. Addi-
tionally, perturbing a feature in the hyperbolic space may alter its
local properties, thereby compromising network robustness. This
can lead to more severe vulnerabilities compared to the Euclidean
case that has been studied in [ 13,22,31,45,59,60]. To enhance
the robustness of HNNs, we leverage the concept of Lipschitz con-
tinuity, a fundamental mathematical property that characterizes
the smoothness of functions. In the context of neural networks,
Lipschitz constants provide an upper bound on how significant
the output of a network can change in response to changes in its
input. Moreover, by maintaining lower Lipschitz constants, neu-
ral networks can achieve greater robustness, rendering them less
vulnerable to the effects of adversarial perturbations [48].
Research on Lipschitz analysis of neural networks has primarily
centered on linear layers [ 16,26,28,32,52] or convolutional layers
[48, 50, 57], both of which exhibit linear structures. In these cases,
a neural network layer has a uniform Lipschitz constant across
input locations and variations in Lipschitz constants arise only
when nonlinear activation functions are introduced. In contrast,
for HNNs, linearity typically holds only in tangent spaces and the
â€œlinearâ€ layers involve nonlinear operations [ 8,19,23,46]. There-
fore, each HNN layer has different Lipschitz constants for different
 
1713
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
input features. This fact poses unique analytical challenges and
necessitates discussion considering the diversity of input features.
In this study, we present an extensive Lipschitz analysis for
HNNs. Our contribution lies in the development of Lipschitz bounds
and the application of regularization techniques tailored specifically
for HNNs. By rigorously analyzing and deriving Lipschitz bounds
for HNN layers, we gain valuable insights into their behavior corre-
sponding to different input features. We then leverage these bounds
to regularize the network during training, effectively constraining
its output fluctuations. We summarize our contribution as follows:
â€¢We derive Lipschitz bounds for HNN layers, including weight
matrix transformation and bias translation, using both the
PoincarÃ© ball model and the hyperboloid model.
â€¢We extend our analysis to examine the effects of the derived
Lipschitz bounds concerning input features under various
conditions. This investigation leads to simplification of the
expressions of the Lipschitz bounds, making them more prac-
tical for regularizing HNNs with normalized input features.
â€¢We empirically validate the effectiveness of Lipschitz reg-
ularization in enhancing the robustness of HNNs against
noise. We establish that a small Lipschitz bound contributes
to superior performance for classification tasks.
2 Related Works
2.1 Hyperbolic Neural Networks
HNNs were first introduced in [ 19], where fundamental neural oper-
ations, including linear and recurrent layers, were executed within
the tangent spaces of hyperbolic features. Later, it was generalized
to HNN++ [ 46], which incorporated concatenation, convolution
and attention mechanisms. Both works used the PoincarÃ© ball model
to represent the hyperbolic space, while other works also used the
hyperboloid model and the Klein model [23, 38].
Using tangent spaces to construct operations in the hyperbolic
space could be prone to gradient explosion and numerical issues
[38]. Consequently, recent works started to consider operations
fully performed within the hyperbolic space itself [ 9,14]. Other
works concentrated on hyperbolic embedding techniques and sub-
sequently devised neural network models according to such em-
beddings [ 24,39]. Due to their proficiency in capturing the un-
derlying geometry of diverse data types, HNNs have also been
applied to graph models [ 5,8,11,33,34,42,56] and generative
models [ 11,34,35,41]. However, all the aforementioned works
have primarily concentrated on effective feature extraction and
numerical considerations, while largely overlooking the critical
aspect of robustness in the presence of noise and perturbations.
2.2 Robustness of Neural Networks
The issue of vulnerability of neural networks has been a significant
concern within deep learning, as originally noted in [ 48]. Prior
research efforts also addressed vulnerability challenges in neural
networks from the views of perturbation [ 15], noisy labels [ 21] and
response to noise [ 13]. Moreover, neural networks are also subject
to designed attacks [ 22,31,45,59,60]. Regarding HNNs, we remark
that previous works on â€œstabilityâ€ of HNNs mainly concerned with
numerical instability and gradient explosion issues [ 36,41,43], butnot vulnerability to noise perturbation and our work represents the
pioneering effort to address this specific issue for HNN.
The vulnerability of neural networks can be attributed to their
large Lipschitz constants [ 48]. Recent works [ 16,26,28,32,52] de-
veloped diverse optimization techniques aimed at tighter Lipschitz
bounds. These investigations have extended to specific neural net-
work architectures, including those with convolutional or attention
layers [ 3,30,50,57] and graph neural networks [ 12,17,18,20,27,
29,58]. To our best knowledge, there is no prior investigation into
Lipschitz analysis specifically for HNNs.
3 Lipschitz Bounds of HNN
3.1 Preliminaries
Hyperbolic geometry is a non-Euclidean geometry with a constant
negative curvature [ 2]. It has several coordinate systems, or â€œmod-
elsâ€, that provide concrete elementwise representation of features.
The most frequently used models in HNNs are the PoincarÃ© ball
model and the hyperboloid (Lorentz) model. While these models
are isometric to each other, they have distinct numerical properties
and work differently in neural networks. We review the operations
in both models as follows.
3.1.1 The PoincarÃ© ball model. Theğ‘›-dimensional PoincarÃ© ball
model with a constant negative curvature of âˆ’ğ‘is formally defined
as
Dğ‘›ğ‘,ğ‘”D
, where Dğ‘›ğ‘=
xâˆˆRğ‘›|ğ‘âˆ¥xâˆ¥2<1	is an open ball in
Rğ‘›, andğ‘”Dis the metric tensor defined as ğ‘”D= ğœŒğ‘x2Iğ‘›. It is worth
noting that Iğ‘›is a Euclidean metric tensor, and the hyperbolic
metric tensor is conformal with it. In this context, Dğ‘›ğ‘represents an
open ball with a radius of ğ‘âˆ’1
2, andğœŒğ‘xis a conformal factor given
byğœŒğ‘x=2 1âˆ’ğ‘âˆ¥xâˆ¥2âˆ’1. Then the induced distance function on Dğ‘›ğ‘,ğ‘”ğ‘is given by:
ğ‘‘ğ‘(x,y)=(2/âˆšğ‘)tanhâˆ’1âˆšğ‘âˆ¥âˆ’xâŠ•ğ‘yâˆ¥
,
whereâŠ•ğ‘is the MÃ¶bius addition, defined by
xâŠ•ğ‘y= 1+2ğ‘âŸ¨x,yâŸ©+ğ‘âˆ¥yâˆ¥2x+ 1âˆ’ğ‘âˆ¥xâˆ¥2y
1+2ğ‘âŸ¨x,yâŸ©+ğ‘2âˆ¥xâˆ¥2âˆ¥yâˆ¥2. (1)
LetTxDğ‘›ğ‘denote the tangent space of xinDğ‘›ğ‘. For x,yâˆˆDğ‘›ğ‘,vâˆˆ
TxDğ‘›ğ‘such that yâ‰ xandvâ‰ 0, the exponential map expğ‘x(v)
transforms vintoDğ‘›ğ‘and the logarithmic map logğ‘
x(y)transforms
yintoTxDğ‘›ğ‘. Their expressions are given by:
expğ‘
x(v)=xâŠ•ğ‘
tanhâˆšğ‘ğœŒğ‘xâˆ¥vâˆ¥
2vâˆšğ‘âˆ¥vâˆ¥
;
logğ‘
x(y)=2âˆšğ‘ğœŒğ‘xtanhâˆ’1âˆšğ‘âˆ¥âˆ’xâŠ•ğ‘yâˆ¥âˆ’xâŠ•ğ‘y
âˆ¥âˆ’xâŠ•ğ‘yâˆ¥.
3.1.2 The hyperboloid model. Write each point in Rğ‘‘+1asx=
[ğ‘¥ğ‘¡,xâŠ¤ğ‘ ]âŠ¤whereğ‘¥ğ‘¡âˆˆRandxğ‘ âˆˆRğ‘›. The Minkowski inner product
âŸ¨Â·,Â·âŸ©Lis defined byâŸ¨x,yâŸ©L:=âˆ’ğ‘¥ğ‘¡ğ‘¦ğ‘¡+xâŠ¤ğ‘ yğ‘ . Theğ‘›-dimensional
hyperboloid manifold with constant negative curvature âˆ’1/ğ¾is
formally defined as Hğ‘›,ğ¾={xâˆˆRğ‘›+1|âŸ¨x,xâŸ©L=âˆ’ğ¾,ğ‘¥ğ‘¡>0}. By
the convention of special relativity, we call ğ‘¥ğ‘¡the time axis and
xğ‘ the spatial axes. In this context, the tangent space at xis the
orthogonal space of Hğ‘›,ğ¾with respect to the Minkowski inner
product, and is represented as TxHğ‘›,ğ¾:={vâˆˆRğ‘›+1:âŸ¨v,xâŸ©L=0}.
 
1714Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The geodesic distance between two points x,yinHğ‘›,ğ¾is given by
ğ‘‘ğ¾
L(x,y)=âˆš
ğ¾coshâˆ’1(âˆ’âŸ¨x,yâŸ©L/ğ¾).
Forx,yâˆˆHğ‘›,ğ¾andvâˆˆTxHğ‘›,ğ¾such that xâ‰ yandvâ‰ 0, the
exponential and the logarithmic maps of the hyperboloid model
are given by
expğ¾
x(v)=coshâˆ¥vâˆ¥Lâˆš
ğ¾
x+âˆš
ğ¾sinhâˆ¥vâˆ¥Lâˆš
ğ¾v
âˆ¥vâˆ¥L;
logğ¾
x(y)=ğ‘‘ğ¾
L(x,y)y+1
ğ¾âŸ¨x,yâŸ©Lx
âˆ¥y+1
ğ¾âŸ¨x,yâŸ©Lxâˆ¥L.
3.2 Derivation of Lipschitz Bounds
In this work, we focus on the linear layers of HNN in both the
PoincarÃ© ball model [ 19] and the hyperboloid model [ 8]. Hyper-
bolic linear transformations involve a weight matrix multiplication
followed by a bias translation, which are defined by leveraging the
exponential and logarithmic maps. The product of their Lipschitz
bounds can be used to estimate the Lipchitz bound for each HNN
layer. In the following analysis, we consider ğ‘=1for the PoincarÃ©
ball model and ğ¾=1for the hyperboloid model for simplicity,
which are in line with the common convention of HNN literature.
While hyperbolic linear transformations are called â€œlinearâ€, they
exhibit linearity solely within the tangent spaces and show non-
linear structures once the exponential maps are applied. Since the
behavior of exponential maps varies with the position of the hy-
perbolic feature, we estimate Lipschitz constants locally.
Before analyzing Lipschitz constants of HNNs, we revisit the
definition of the Lipschitz constant. A function ğ‘“:Rğ‘›â†’Rğ‘šis
said to be Lipschitz continuous on an input set XâŠ†Rğ‘›if there
exists a constant ğ¿â‰¥0such that for all x,yâˆˆX,ğ‘“satisfies the
following inequality:
âˆ¥ğ‘“(x)âˆ’ğ‘“(y)âˆ¥â‰¤ğ¿âˆ¥xâˆ’yâˆ¥,âˆ€x,yâˆˆX. (2)
The smallest possible ğ¿for which (2) holds is called the Lipschitz
constant ofğ‘“. For neural networks, it is in general difficult to derive
the smallest ğ¿and a Lipschitz bound is widely adopted. Let Jbe the
Jabobian of ğ‘“atx. Since
ğ‘“(x)âˆ’ğ‘“(y)=J(xâˆ’y)+ğ‘œ(âˆ¥xâˆ’yâˆ¥),
given anğœ–>0, there exists a neighborhood Uofxfor which
âˆ¥ğ‘“(x)âˆ’ğ‘“(y)âˆ¥â‰¤âˆ¥Jâˆ¥âˆ¥xâˆ’yâˆ¥+ğœ– (3)
as long as yâˆˆU. Here,âˆ¥Jâˆ¥is the spectral norm of J. In view
of (3), we can use âˆ¥Jâˆ¥as an upper bound for the local Lipschitz
constant of ğ‘“atx. Since obtaining the precise constant for neural
networks is in general difficult, instead of seeking for an exactly
tight bound, we focus on a bound that can be more easily expressed
and implemented.
3.2.1 Lipschitz bounds for the PoincarÃ© ball model.
Weight matrix transformation. LetMbe ağ‘šÃ—ğ‘›weight matrix.
For all xâˆˆDğ‘›andMxâ‰ 0, the weight matrix transformation can
be performed using the MÃ¶bius matrix-vector multiplication [ 51],
which is equivalent to matrix-vector multiplication in the tangent
space [19]. Specifically,
MâŠ—x:=tanhâˆ¥Mxâˆ¥
âˆ¥xâˆ¥tanhâˆ’1(âˆ¥xâˆ¥)Mx
âˆ¥Mxâˆ¥.To derive the Lipschitz bound for y=MâŠ—x, denote its Jacobian
matrix of this operation by J1=ğœ•y/ğœ•x. We have the following
result, with the proof provided in Appendix A.1.
Theorem 3.1. The spectral norm of J1satisfies
âˆ¥J1âˆ¥â‰¤tanhâˆ¥Mxâˆ¥
âˆ¥xâˆ¥tanhâˆ’1(âˆ¥xâˆ¥)
Â·M
âˆ¥Mxâˆ¥âˆ’MxxâŠ¤MâŠ¤M
âˆ¥Mxâˆ¥3+
âˆ¥Mxâˆ¥sech2 
âˆ¥Mxâˆ¥
âˆ¥xâˆ¥Â·tanhâˆ’1(âˆ¥xâˆ¥)!
Â·
tanhâˆ’1(âˆ¥xâˆ¥)MâŠ¤M
âˆ¥Mxâˆ¥2+tanhâˆ’1(âˆ¥xâˆ¥)
âˆ¥xâˆ¥2+1
âˆ¥xâˆ¥(1âˆ’âˆ¥xâˆ¥2)
.
(4)
Analysis ofâˆ¥J1âˆ¥.The upper bound of âˆ¥J1âˆ¥in (4) is complicated
and depends on x. To gain insights of the bound, we consider the
following two cases based on the magnitude of âˆ¥J1âˆ¥. The first case
isâˆ¥xâˆ¥â†’ 1, where the input feature lies close to the boundary of
the PoincarÃ© ball. This corresponds to points that are infinitely far
away from the origin in hyperbolic space and thus large norms of
the input. The second case is âˆ¥xâˆ¥=tanh(1), it represents a position
at a unit hyperbolic distance from the input feature to the origin in
hyperbolic space. We simplify the upper bound in (4) in these cases
and derive the following results.
Theorem 3.2. (1)Asâˆ¥xâˆ¥â†’ 1, the upper bound of âˆ¥J1âˆ¥can be
simplified depending on the value of ğœš:=âˆ¥Mxâˆ¥/âˆ¥xâˆ¥. There
are three cases:
(a)ifğœš>1,
âˆ¥J1âˆ¥â‰¤2âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥. (5)
(b)ifğœš=1,
âˆ¥J1âˆ¥â‰¤2âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥+1. (6)
(c)ifğœš<1,âˆ¥J1âˆ¥diverges.
(2)Whenâˆ¥xâˆ¥=tanh(1), the upper bound of âˆ¥J1âˆ¥can be simpli-
fied as
âˆ¥J1âˆ¥â‰¤2âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥+âˆ¥MâŠ¤Mâˆ¥
âˆ¥Mxâˆ¥+4.86âˆ¥Mxâˆ¥. (7)
Proof. (1) Asâˆ¥xâˆ¥â†’ 1,tanhâˆ’1(âˆ¥xâˆ¥)â†’âˆ. Thus we have
tanhâˆ¥Mxâˆ¥
âˆ¥xâˆ¥tanhâˆ’1(âˆ¥xâˆ¥)
â†’1.
Consider
sech2âˆ¥Mxâˆ¥
âˆ¥xâˆ¥tanhâˆ’1(âˆ¥xâˆ¥)
.
Denoteğ‘Ÿ=âˆ¥xâˆ¥. Since
sech2(ğœštanhâˆ’1(ğ‘Ÿ))=4
1+ğ‘Ÿ
1âˆ’ğ‘Ÿğœš
+
1âˆ’ğ‘Ÿ
1+ğ‘Ÿğœš
+2,
we have
sech2(ğœštanhâˆ’1(ğ‘Ÿ))
1âˆ’ğ‘Ÿ2=4
(1+ğ‘Ÿ)ğœš+1
(1âˆ’ğ‘Ÿ)ğœšâˆ’1+(1âˆ’ğ‘Ÿ)ğœš+1
(1+ğ‘Ÿ)ğœšâˆ’1+2(1âˆ’ğ‘Ÿ)(1+ğ‘Ÿ)
ğ‘Ÿâ†’1âˆ’
â†’4(1âˆ’ğ‘Ÿ)ğœšâˆ’1
(1+ğ‘Ÿ)ğœš+1=:ğ‘„.
 
1715KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
Therefore, as ğ‘Ÿâ†’1âˆ’, there are three cases: (a) ğœš>1: thenğ‘„â†’0,
the same as the conclusion in the paper; (b) ğœš=1: thenğ‘„â†’1, we
haveâˆ¥ğ½1âˆ¥â‰¤2âˆ¥ğ‘€âˆ¥
âˆ¥ğ‘€ğ‘¥âˆ¥+âˆ¥ğ‘€ğ‘¥âˆ¥; (c)ğœš<1:ğ‘„diverges. Therefore, a
necessary condition for a bounded âˆ¥ğ½1âˆ¥asâˆ¥ğ‘¥âˆ¥â†’ 1is thatâˆ¥ğ‘€ğ‘¥âˆ¥â‰¥
âˆ¥ğ‘¥âˆ¥. Consequently,
âˆ¥J1âˆ¥â‰¤M
âˆ¥Mxâˆ¥âˆ’MxxâŠ¤MâŠ¤M
âˆ¥Mxâˆ¥3=
ğ‘°âˆ’MxxâŠ¤MâŠ¤
âˆ¥Mxâˆ¥2
Â·M
âˆ¥Mxâˆ¥
â‰¤ğ‘°âˆ’MxxâŠ¤MâŠ¤
âˆ¥Mxâˆ¥2Â·âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥â‰¤
âˆ¥ğ‘°âˆ¥+âˆ¥MxxâŠ¤MâŠ¤âˆ¥
âˆ¥Mxâˆ¥2
Â·âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥.
Since
âˆ¥MxxâŠ¤MâŠ¤âˆ¥â‰¤tr(MxxâŠ¤MâŠ¤)=tr(xâŠ¤MâŠ¤Mx)=âˆ¥Mxâˆ¥2,
we have
âˆ¥J1âˆ¥â‰¤
âˆ¥ğ‘°âˆ¥+âˆ¥Mxâˆ¥2
âˆ¥Mxâˆ¥2
Â·âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥=2âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥.
(2) Whenâˆ¥xâˆ¥=tanh(1),tanhâˆ’1(âˆ¥xâˆ¥)=1. Thus (4) simplifies to
âˆ¥J1âˆ¥â‰¤tanhâˆ¥Mxâˆ¥
âˆ¥xâˆ¥
Â·M
âˆ¥Mxâˆ¥âˆ’MxxâŠ¤MâŠ¤M
âˆ¥Mxâˆ¥3+
âˆ¥Mxâˆ¥sech2âˆ¥Mxâˆ¥
âˆ¥xâˆ¥
Â· MâŠ¤M
âˆ¥Mxâˆ¥2+1
âˆ¥xâˆ¥2+1
âˆ¥xâˆ¥(1âˆ’âˆ¥xâˆ¥2)!
â‰¤M
âˆ¥Mxâˆ¥âˆ’MxxâŠ¤MâŠ¤M
âˆ¥Mxâˆ¥3+âˆ¥Mxâˆ¥Â·
 MâŠ¤M
âˆ¥Mxâˆ¥2+1
âˆ¥xâˆ¥2+1
âˆ¥xâˆ¥(1âˆ’âˆ¥xâˆ¥2)!
â‰¤2âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥+âˆ¥Mxâˆ¥Â· MâŠ¤M
âˆ¥Mxâˆ¥2+1
âˆ¥xâˆ¥2+1
âˆ¥xâˆ¥(1âˆ’âˆ¥xâˆ¥2)!
.
We compute1
âˆ¥tanh(1)âˆ¥2+1
âˆ¥tanh(1)âˆ¥(1âˆ’âˆ¥tanh(1)âˆ¥2)â‰¤4.86. Hence,
âˆ¥J1âˆ¥â‰¤2âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥+âˆ¥Mxâˆ¥Â·âˆ¥MâŠ¤Mâˆ¥
âˆ¥Mxâˆ¥2+4.86
â‰¤2âˆ¥Mâˆ¥
âˆ¥Mxâˆ¥+âˆ¥MâŠ¤Mâˆ¥
âˆ¥Mxâˆ¥+4.86âˆ¥Mxâˆ¥.
â–¡
We conclude that to bound âˆ¥J1âˆ¥in both cases, we can focus on
boundingâˆ¥Mâˆ¥and we need to ensure the smallest singular value of
Mis sufficiently large as well. The upper bounds of (5)â€“(7) can be
used as subsequent guidelines for regularized training to improve
the robustness of the HNNs embedded in the PoincarÃ© ball model,
which are simpler than (4) and can be more efficiently implemented.
Bias translation. The bias translation in the PoincarÃ© ball model
is performed by moving along geodesics, which coincides with the
MÃ¶bius addition [ 51]. Translating yâˆˆDğ‘šby a bias bâˆˆDğ‘šis
defined according to (1) as
yâŠ•b:= 1+2âŸ¨y,bâŸ©+âˆ¥ bâˆ¥2y+ 1âˆ’âˆ¥yâˆ¥2b
1+2âŸ¨y,bâŸ©+âˆ¥ yâˆ¥2âˆ¥bâˆ¥2.
To derive the Lipschitz bound for z=yâŠ•b, denote the Jacobian
matrix of this operation by J2=ğœ•z/ğœ•y. We have the following
result (due to page limit, we omit the proof which resembles that
of Theorem 3.1).Theorem 3.3. The spectral norm of J2satisfies
âˆ¥J2âˆ¥â‰¤
1+4yâŠ¤b+3âˆ¥bâˆ¥2+2âˆ¥yâˆ¥âˆ¥bâˆ¥+4yâŠ¤b2+
2yâŠ¤bâˆ¥bâˆ¥2+5âˆ¥yâˆ¥2âˆ¥bâˆ¥2+4yâŠ¤bâˆ¥yâˆ¥âˆ¥bâˆ¥+
4âˆ¥yâˆ¥âˆ¥bâˆ¥3+6yâŠ¤bâˆ¥yâˆ¥2âˆ¥bâˆ¥2+3âˆ¥yâˆ¥2âˆ¥bâˆ¥4+
2âˆ¥yâˆ¥3âˆ¥bâˆ¥3
Â·
1+2yâŠ¤b+âˆ¥yâˆ¥2âˆ¥bâˆ¥2âˆ’2
.
3.2.2 Lipschitz bounds for the hyperboloid model.
Weight matrix transformation. LetWbe anğ‘šÃ—(ğ‘›+1)weight
matrix. In the hyperboloid model, the hyperbolic matrix multiplica-
tion is defined as yâ‰¡WâŠ—ğ¾xğ»:=expğ¾o(Wlogğ¾
o(xğ»))[8], where
odenotes the hyperbolic origin. In the hyperboloid model, the coor-
dinate of the time axis is completely determined by the coordinates
of the spatial axes. Therefore, when considering robustness it suf-
fices to look at the spatial axes. Denote the Jacobian matrix of this
operation by J1=ğœ•yğ‘ /ğœ•xğ‘ . We have the following result, with the
proof detailed in Appendix A.2.
Theorem 3.4. Letğ‘=coshâˆ’1(ğ‘¥ğ‘¡)
âˆ¥xğ‘ âˆ¥=coshâˆ’1(ğ‘¥ğ‘¡)âˆšï¸ƒ
ğ‘¥2
ğ‘¡âˆ’1, and let W[2:]
denote the matrix comprising columns from the second column to the
last column of W. Then the spectral norm of J1satisfies
âˆ¥J1âˆ¥â‰¤ 
sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥âˆ¥W[2:]xğ‘ âˆ¥âˆ’1
âˆ¥W[2:]xğ‘ âˆ¥2+
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥ğ‘
âˆ¥W[2:]xğ‘ âˆ¥âˆ’
ğ‘
âˆ¥xğ‘ âˆ¥+1
ğ‘¥ğ‘¡âˆ¥xğ‘ âˆ¥!
W[2:].(8)
Analysis ofâˆ¥J1âˆ¥.To gain insights of the bound (8), we consider
the following two cases. The first case is âˆ¥xğ‘ âˆ¥=1, which corre-
sponds to performing normalization for the input features before
entering each hyperbolic linear layer. We simplify the expression
of the Lipschitz bound in the following theorem.
Theorem 3.5. Whenâˆ¥xğ‘ âˆ¥=1, the upper bound of âˆ¥J1âˆ¥can be
simplified as
âˆ¥J1âˆ¥â‰¤exp(coshâˆ’1(âˆš
2)âˆ¥W[2:]xğ‘ âˆ¥)
âˆ¥W[2:]xğ‘ âˆ¥âˆ¥W[2:]âˆ¥. (9)
Proof. Whenâˆ¥xğ‘ âˆ¥=1,ğ‘=coshâˆ’1(âˆš
2) â‰ˆ 0.88, andğ‘¥ğ‘¡=âˆšï¸
âˆ¥xğ‘ âˆ¥2+1=âˆš
2. Thus (8) simplifies to
âˆ¥J1âˆ¥â‰¤ 
sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥ 
1
âˆ¥W[2:]xğ‘ âˆ¥âˆ’1
âˆ¥W[2:]xğ‘ âˆ¥2!
+
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥ 
ğ‘
âˆ¥W[2:]xğ‘ âˆ¥âˆ’ğ‘+âˆš
2
2! !
âˆ¥W[2:]âˆ¥
= 
sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥1
âˆ¥W[2:]xğ‘ âˆ¥+
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥ğ‘
âˆ¥W[2:]xğ‘ âˆ¥!
âˆ¥W[2:]âˆ¥+
 
1716Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
 
âˆ’sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥1
âˆ¥W[2:]xğ‘ âˆ¥2+
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥ âˆš
2
2âˆ’ğ‘! !
âˆ¥W[2:]âˆ¥.
Sinceâˆ’sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
1
âˆ¥W[2:]xğ‘ âˆ¥2and
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥ âˆš
2
2âˆ’ğ‘
are both negative,
âˆ¥J1âˆ¥< 
sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥1
âˆ¥W[2:]xğ‘ âˆ¥+
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥ğ‘
âˆ¥W[2:]xğ‘ âˆ¥!
âˆ¥W[2:]âˆ¥
â‰¤ 
sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥1
âˆ¥W[2:]xğ‘ âˆ¥+
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥1
âˆ¥W[2:]xğ‘ âˆ¥!
âˆ¥W[2:]âˆ¥ (ğ‘<1)
=exp(coshâˆ’1(âˆš
2)âˆ¥W[2:]xğ‘ âˆ¥)
âˆ¥W[2:]xğ‘ âˆ¥âˆ¥W[2:]âˆ¥.
â–¡
For normalized features, it is much easier to implement (9) than
(8) as a regularization term in loss functions of learning tasks.
The second case is âˆ¥xğ‘ âˆ¥â†’âˆ , which corresponds to the case of
input features with a considerably large norm. Due to symmetry in
all spatial axes, it suffices to consider a simple case where only the
first coordinate of xğ‘ goes to infinity and other dimensions are all
zero. We will first state the result in the following lemma, with the
proof presented in Appendix A.3.
Lemma 3.6. Letxğ‘ =[ğœ0Â·Â·Â·0]âŠ¤. Asğœâ†’âˆ , a necessary condi-
tion for a finiteâˆ¥J1âˆ¥isâˆ¥W[2]âˆ¥<1, where W[2]denotes the second
column vector of W.
Next, we extend our result to the general case for a general xğ‘ 
withâˆ¥xğ‘ âˆ¥â†’âˆ in the following theorem.
Theorem 3.7. Asâˆ¥xğ‘ âˆ¥â†’âˆ , a necessary condition for a finite
âˆ¥J1âˆ¥isâˆ¥W[2:]âˆ¥F<1, whereâˆ¥Â·âˆ¥Fdenotes the Frobenius norm.
Proof. For any given xğ‘ withâˆ¥xğ‘ âˆ¥â†’âˆ , there exists a unitary
matrix PâˆˆRğ‘›Ã—ğ‘›such that Pxğ‘ =[ğœ0Â·Â·Â·0]âŠ¤whereğœâ†’âˆ . Since
W[2:]xğ‘ =W[2:]PâŠ¤Pxğ‘ , according Lemma 3.6, a necessary condi-
tion for a finiteâˆ¥J1âˆ¥isâˆ¥W[2:]PâŠ¤
[1]âˆ¥<1. Here we use W[2:]PâŠ¤
[1]
to denote the first column of W[2:]PâŠ¤. Let e=[1 0Â·Â·Â·0]âŠ¤, then
W[2:]PâŠ¤
[2]=W[2:]PâŠ¤e. Since Pis unitary, PâŠ¤is also unitary. Then,
1>âˆ¥W[2:]âˆ¥F=âˆ¥W[2:]PâŠ¤âˆ¥Fâ‰¥âˆ¥W[2:]PâŠ¤âˆ¥
=sup
âˆ¥uâˆ¥=1âˆ¥W[2:]PâŠ¤uâˆ¥â‰¥âˆ¥ W[2:]PâŠ¤eâˆ¥=âˆ¥W[2:]PâŠ¤
[1]âˆ¥.
Hence, a necessary condition for a finite âˆ¥J1âˆ¥in the case of any
âˆ¥xğ‘ âˆ¥â†’âˆ isâˆ¥W[2:]âˆ¥F<1. â–¡
From the theorem above, we can conclude that when âˆ¥xğ‘ âˆ¥â†’âˆ ,
ifâˆ¥W[2:]âˆ¥ğ¹<1, then it is feasible to bound âˆ¥J1âˆ¥. Therefore, toboundâˆ¥J1âˆ¥in all cases, we need to focus on bounding âˆ¥W[2:]âˆ¥.
However, in the case of âˆ¥xğ‘ âˆ¥â†’âˆ , it is difficult to derive a simple
upper bound, unlike the case of the PoincarÃ© ball model.
Bias translation. To perform bias translation in hyperboloid,
we define bas a Euclidean vector located at ToHğ‘‘â€²,ğ¾={bâˆˆ
Rğ‘‘â€²+1|âŸ¨b,oâŸ©L=0}and then parallel transport bto the point in
hyperboloid and map it to hyperboloid. Let ğ‘ƒğ¾
oâ†’xğ»(Â·)denote the
parallel transport from ToHğ‘‘â€²,ğ¾toTxğ»Hğ‘‘â€²,ğ¾, we have xğ»âŠ•ğ¾b:=
expğ¾
xğ»(ğ‘ƒğ¾
oâ†’xğ»(b))[8]. Similar to the weight matrix multiplication,
for Lipschitz analysis we only need to look at the spatial axes. To
derive the Lipschitz bound for z=yâŠ•1b, denote its Jacobian by
J2=ğœ•zğ‘ /ğœ•yğ‘ . We have the following result (due to page limit, we
omit the proof which resembles that of Theorem 3.4).
Theorem 3.8. The spectral norm of J2satisfies
âˆ¥J2âˆ¥â‰¤cosh(âˆ¥bğ‘ âˆ¥)+sinh(âˆ¥bğ‘ âˆ¥)
âˆ¥bğ‘ âˆ¥(1+ğ‘¦ğ‘¡)
yâŠ¤
ğ‘ bğ‘ +
âˆ¥yğ‘ âˆ¥+yâŠ¤ğ‘ bğ‘ 
ğ‘¦ğ‘¡(1+ğ‘¦ğ‘¡)âˆ¥yğ‘ âˆ¥
.
3.2.3 Discussion. While the PoincarÃ© ball and the hyperboloid
models exhibit isometric equivalence, their distinct Euclidean rep-
resentations lead to different numerical characteristics for HNNs.
Given that the Lipschitz bounds we consider fundamentally rely on
the Euclidean representation, it is comparatively more manageable
to control robustness in the PoincarÃ© ball model. In its Euclidean
representation, the features are bounded in a ball while it is more
difficult for the hyperboloid model since features could possess
large coordinates. This has to be understood numerically rather
than geometrically. Further, comparing (5) and (9), it is evident that
discussing Lipschitz analysis for the hyperboloid model is more in-
tricate, especially when the feature vector is distant from the origin.
Our experimental results will demonstrate that HNNs employing
the PoincarÃ© model indeed exhibit greater robustness than the hy-
perboloid model in most cases. Nevertheless, the PoincarÃ© model
is susceptible to gradient explosion in certain scenarios. In cases
where we need to use the hyperboloid, (9) is an effective expression
for a simplified representation of Lipschitz bounds.
4 Experiments
4.1 Datasets and Settings
To show how Lipschitz regularization improves HNN robustness,
we consider node classification tasks with noisy features. We utilize
the following datasets: the WebKB datasets [ 10] including Texas
and Wisconsin, where nodes correspond to web pages and edges
represent hyperlinks; the Actor dataset [ 49], where nodes represent
actors and two nodes are connected if they co-occur on the same
Wikipedia page; the citation dataset Cora [ 37] and Pubmed [ 44],
where nodes are publications and edges represent citations. These
graphs are sparse with Gromov hyperbolicity [ 47] closely resembles
that of a hyperbolic space. This makes HNNs particularly advanta-
geous for classification tasks on these datasets. Table 1 summarizes
their statistics.
We perform Lipschitz regularized training for HNN using the
PoincarÃ© ball model and the hyperboloid model. Specifically, we use
the sum of Lipschitz bounds at all node features as a regularization
 
1717KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
Table 1: Statistics of datasets
Datasets Nodes Edges Features Class Hyperbolicity Training Validating Testing
Texas 183 280 1,703 5 1.0 244 44 44
Wisconsin 251 466 1,703 5 1.0 314 54 54
Actor 7,600 26,752 931 5 1.5 9,254 1,878 1,878
Cora 2,708 5,278 1,433 6 3.0 140 500 1,000
Pubmed 19,717 44,327 500 3 2.5 60 500 1,000
term added to the loss function for classification. The multiplier for
the regularization term is a hyperparameter ğœ†, chosen by validation
from {1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9}.
In our experiments, we compare the following methods: â€œNor-
malâ€, which represents non-regularized training; â€œLip-Reg1â€, which
refers to the computation of the Lipschitz bounds following (4) and
(8); â€œLip-Reg2â€, which refers to the computation of the Lipschitz
bounds following (7) and (9). We also compare with the baseline
method of â€œClipâ€ [ 25], where clipping is applied for regularization.
In all the experiments, we take an HNN with two layers. The
first layer is a hyperbolic linear layer with zero bias term either
in the PoincarÃ© ball model or the hyperboloid model, with output
dimension 16. The second layer is a Euclidean linear layer, the
output dimension of which depends on the number of classes. We
use Riemannian Adam [ 6] for optimization, where the learning rate
is chosen from {0.05, 0.01, 0.005, 0.001} by validation. The dropout
rate is also validated from {0, 0.2, 0.5}. By virtue of the discussion
in Section 3.2, the input features are always normalized.
The implementation of our proposed methods can be accessed
at https://github.com/YidanM/HNN_Lipshitz.
4.2 Results
We consider datasets in Table 1 poisoned by noise added to hyper-
bolic node features exponentially mapped from Euclidean inputs.
Specifically, we add independent Gaussian noise to all the normal-
ized input node features with various standard deviations. The
standard deviation (std) is from {0.001, 0.005, 0.01}.
Tables 2â€“4 present the test accuracies. The results indicate the
effectiveness of both expressions of Lipschitz bounds, contributing
to enhanced test accuracies in comparison to non-regularized train-
ing, or the Clip regularization, across most scenarios. Although
there is clear advantage of using Lipschitz regularization, there is
no clear conclusion for using â€œLip-Reg1â€ or â€œLip-Reg2â€.
We also consider the scenario when the noise level escalates
with a large std from {0.1, 1}. In this case, the PoincarÃ© ball model
experiences issues with gradient explosion, necessitating the ex-
clusive use of the hyperboloid model. Such substantial increases
in standard deviation result in markedly diminished accuracy for
HNNs. Nevertheless, Table 5 clearly demonstrates that Lipschitz
regularization still consistently enhances performance in most cases
under these challenging conditions.
For clear comparison, we also present the test accuracies for
clean data in Table 6. Interestingly, Lipschitz regularization still
improves the performance in some scenarios. We attribute this
to the improved generalizability. Also, although HNNs using the
PoincarÃ© ball model and the hyperboloid model behave similarlyfor clean data, the PoincarÃ© ball is in general much better when
noises are injected. This implies that the PoincarÃ© ball model is less
subject to perturbation vulnerability.
To study how Lipschitz bounds evolve with training epochs, we
plot their relationships when training on the Wisconsin, Actor and
Cora datasets. Figure 1 shows the results for the PoincarÃ© ball model
and Figure 4 in Appendix B shows the results for the hyperboloid
model.
(a)
 (b)
(c)
Figure 1: Change of Lipschitz bounds with epochs for the
PoincarÃ© model. (a) Wisconsin; (b) Actor; (c) Cora.
4.3 Sensitivity Analysis
We perform sensitivity analysis on the influence of the multiplier ğœ†
of the regularization term. In Figures 2 and 3, we show the change
of test accuracy with different ğœ†values. Each subfigure shows a
different noise level used in the experiments.
From the results, we find that the test accuracy is relatively stable
with the change of ğœ†. Nevertheless, in most cases, there is a sudden
decline when ğœ†exceeds a certain threshold. This phenomenon
underscores a trade-off between model performance and robustness,
where an excessive emphasis on regularization compromises the
predictive accuracy. This decay is less significant when the noise
level is high, indicating that robustness is more important under
large noise.
 
1718Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: The test accuracy of two-layer HNN for noisy data with std=0.001
Model Method Texas Wisconsin Actor Cora Pubmed
PoincarÃ© ballNormal 0.9621Â±0.0347 0.9134Â±0.0280 0.4531Â±0.0175 0.5167Â±0.0175 0.6917Â±0.0083
Clip 0.9621Â±0.0347 0.9134Â±0.0280 0.4531Â±0.0175 0.5167Â±0.0175 0.6917Â±0.0083
Lip-Reg1 0.9621Â±0.0132 0.9568Â±0.0214 0.4613Â±0.0111 0.6040Â±0.0010 0.7427Â±0.0031
Lip-Reg2 0.9469Â±0.0131 0.9506Â±0.0283 0.4580Â±0.0125 0.5973Â±0.0119 0.7400Â±0.0040
HyperboloidNormal 0.9242Â±0.0107 0.9630Â±0.0151 0.4313Â±0.0095 0.5190Â±0.0057 0.6920Â±0.0051
Clip 0.9242Â±0.0107 0.9506Â±0.0231 0.4384Â±0.0035 0.5160Â±0.0054 0.6893Â±0.0082
Lip-Reg1 0.9318Â±0.0000 0.9692Â±0.0087 0.4450Â±0.0013 0.5860Â±0.0049 0.7330Â±0.0016
Lip-Reg2 0.9318Â±0.0000 0.9815Â±0.0000 0.4485Â±0.0009 0.5887Â±0.0063 0.7320Â±0.0029
Table 3: The test accuracy of two-layer HNN for noisy data with std=0.005
Model Method Texas Wisconsin Actor Cora Pubmed
PoincarÃ© ballNormal 0.9015Â±0.0730 0.8827Â±0.0534 0.4329Â±0.0019 0.4803Â±0.0086 0.6847Â±0.0042
Clip 0.9015Â±0.0730 0.8827Â±0.0534 0.4329Â±0.0019 0.4803Â±0.0086 0.6847Â±0.0042
Lip-Reg1 0.9167Â±0.0860 0.9136Â±0.0283 0.4413Â±0.0059 0.5727Â±0.0057 0.7303Â±0.0138
Lip-Reg2 0.9091Â±0.0787 0.9074Â±0.0490 0.4482Â±0.0132 0.5843Â±0.0104 0.7233Â±0.0068
HyperboloidNormal 0.9015Â±0.0386 0.9321Â±0.0231 0.4177Â±0.0033 0.4957Â±0.0102 0.6820Â±0.0112
Clip 0.9015Â±0.0386 0.9012Â±0.0231 0.4347Â±0.0013 0.4950Â±0.0114 0.6810Â±0.0126
Lip-Reg1 0.9015Â±0.0386 0.9321Â±0.0231 0.4277Â±0.0035 0.5437Â±0.0111 0.7007Â±0.0109
Lip-Reg2 0.9015Â±0.0284 0.9321Â±0.0231 0.4262Â±0.0036 0.5443Â±0.0048 0.6990Â±0.0156
Table 4: The test accuracy of two-layer HNN for noisy data with std=0.01
Model Method Texas Wisconsin Actor Cora Pubmed
PoincarÃ© ballNormal 0.7727Â±0.1202 0.7716Â±0.0566 0.4073Â±0.0098 0.4360Â±0.0052 0.6490Â±0.0089
Clip 0.7727Â±0.1202 0.7716Â±0.0566 0.4073Â±0.0098 0.4360Â±0.0052 0.6490Â±0.0089
Lip-Reg1 0.8106Â±0.0572 0.7901Â±0.0595 0.4180Â±0.0102 0.5010Â±0.0066 0.6673Â±0.0029
Lip-Reg2 0.8258Â±0.0347 0.7963Â±0.0641 0.4269Â±0.0074 0.5053Â±0.0142 0.6783Â±0.0159
HyperboloidNormal 0.7803Â±0.0284 0.8395Â±0.0486 0.3987Â±0.0042 0.4500Â±0.0135 0.6590Â±0.0091
Clip 0.7803Â±0.0284 0.8148Â±0.0303 0.3990Â±0.0031 0.4483Â±0.0139 0.6590Â±0.0091
Lip-Reg1 0.8258Â±0.0283 0.8395Â±0.0486 0.4130Â±0.0010 0.4780Â±0.0073 0.6693Â±0.0031
Lip-Reg2 0.8106Â±0.0467 0.8395Â±0.0486 0.4054Â±0.0035 0.4580Â±0.0036 0.6613Â±0.0057
Table 5: The test accuracy of two-layer HNN for noisy data with large std using the hyperboloid model
Noise std Method Texas Wisconsin Actor Cora Pubmed
0.1Normal 0.4924Â±0.0107 0.4506Â±0.0315 0.2744Â±0.0035 0.1627Â±0.0057 0.3533Â±0.0009
Clip 0.5379Â±0.0429 0.5432Â±0.0486 0.2744Â±0.0035 0.1723Â±0.0041 0.3610Â±0.0014
Lip-Reg1 0.5757Â±0.0386 0.5123Â±0.0174 0.2924Â±0.0033 0.1643Â±0.0045 0.3697Â±0.0225
Lip-Reg2 0.5909Â±0.0491 0.5185Â±0.0262 0.2875Â±0.0122 0.1650Â±0.0029 0.3563Â±0.0005
1Normal 0.3257Â±0.0934 0.2222Â±0.0920 0.2201Â±0.0020 0.1553Â±0.0125 0.3250Â±0.0211
Clip 0.3333Â±0.0467 0.3086Â±0.0231 0.2178Â±0.0028 0.1523Â±0.0127 0.3283Â±0.0009
Lip-Reg1 0.3560Â±0.0214 0.3704Â±0.0000 0.2924Â±0.0033 0.1537Â±0.0079 0.3457Â±0.0094
Lip-Reg2 0.3485Â±0.0772 0.3025Â±0.0087 0.2875Â±0.0122 0.1633Â±0.0196 0.3427Â±0.0133
 
1719KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
(a)
 (b)
(c)
 (d)
Figure 2: Sensitivity analysis for ğœ†using the PoincarÃ© model.
Evidently, both Lipschitz regularization techniques proficiently
reduce the Lipschitz bounds of the HNNs. Notably, the Lipschitz
bounds continue to increase with epochs even when employing
Lipschitz regularization. This phenomenon shows that it is nec-
essary to maintain a Lipschitz bound of sufficient magnitude to
ensure that the HNN retains the capacity for meaningful feature
discrimination.
4.4 Complexity Analysis
Since in both PoincarÃ© and hyperboloid models, the linear opera-
tion is dominated by matrix vector multiplication, with complexity
ğ‘‚(ğ‘šğ‘›). Givenğ‘nodes in the graph, the complexity is ğ‘‚(ğ‘šğ‘›ğ‘).
Moreover, in both models, the complexity for calculating the Lips-
chitz bounds is dominated by the matrix multiplication, which is
ğ‘›-by-ğ‘šmultiplied with ğ‘š-by-ğ‘›, thusğ‘‚(ğ‘›2ğ‘š).
In Table 7, we report the average run time needed to train one
epoch on the Pubmed dataset using the four methods: Normal, Clip,
Lip-Reg1, and Lip-Reg2. We find that it takes similar time to train
all the models.
4.5 Application to Other Architectures
Our proposed Lipschitz regularization can be generally adopted
in any neural networks that contain HNN layers. We consider the
following two examples: the first is an HNN with two hyperbolic
layers and a Euclidean layer, where the Lipschitz bound is taken
to be the product of the bounds of the two layers; the second is an
HGCN [ 8] with a hyperbolic message passing layer and a Euclidean
layer. We report the numerical results, namely the test accuracy of
both models for noisy input features with std=0.01 in Tables 8 and
9. The results show that our Lipschitz regularization also improves
the three-layer HNN and the HGCN models.
(a)
 (b)
(c)
 (d)
(e)
 (f)
Figure 3: Sensitivity analysis for ğœ†using the hyperboloid
model.
5 Conclusion
In this paper, we have performed a rigorous Lipschitz analysis for
the linear layers within HNNs. We derived and had a comprehen-
sive discussion of Lipschitz bounds, particularly in the context of
varied input amplitudes. Subsequently, we applied Lipschitz reg-
ularization during the training of HNNs, consistently enhancing
their robustness against noise perturbations.
One limitation of the current work is that we only have consid-
ered a very simple form of hyperbolic layers, which might result
in restricted expressivity. We will explore realistic approaches to
incorporating the Lipschitz bounds for other common hyperbolic
operators. Another future work is extending the current progress to
other types of manifold where neural networks use Euclidean-like
neural operators. We will also explore how to use our analysis to
devise adversarial defense approaches.
Acknowledgments
This work is supported by National Natural Science Foundation of
China (Grant No. 12301117). YL and YM have been supported by
the Summer Research Scholars (SRS) program of Duke Kunshan
University.
 
1720Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 6: The test accuracy of two-layer HNN for clean data
Model Method Texas Wisconsin Actor Cora Pubmed
PoincarÃ© ballNormal 0.9394Â±0.0131 0.9012Â±0.0283 0.4556Â±0.0091 0.5197Â±0.0115 0.6920Â±0.0070
Clip 0.9394Â±0.0131 0.9012Â±0.0283 0.4556Â±0.0091 0.5197Â±0.0115 0.6920Â±0.0070
Lip-Reg1 0.9469Â±0.0131 0.9444Â±0.0186 0.4622Â±0.0078 0.6060Â±0.0026 0.7367Â±0.0006
Lip-Reg2 0.9469Â±0.0131 0.9383Â±0.0214 0.4604Â±0.0102 0.6030Â±0.0087 0.7397Â±0.0081
HyperboloidNormal 0.9318Â±0.0185 0.8889Â±0.0151 0.4297Â±0.0012 0.5203Â±0.0034 0.6883Â±0.0094
Clip 0.9318Â±0.0185 0.8889Â±0.0151 0.4466Â±0.0033 0.5227Â±0.0025 0.6847Â±0.0024
Lip-Reg1 0.9318Â±0.0185 0.8889Â±0.0151 0.4475Â±0.0013 0.5853Â±0.0125 0.7300Â±0.0051
Lip-Reg2 0.9318Â±0.0185 0.9444Â±0.0303 0.4384Â±0.0011 0.5860Â±0.0016 0.7343Â±0.0042
Table 7: Average runtime for training one epoch on the Pubmed dataset (mean and std over 3 trials)
Model Noise std 0 0.001 0.005 0.01 0.1 1
PoincarÃ© ballNormal 0.8091Â±0.0200 0.8120Â±0.0172 0.8006Â±0.0212 0.8157Â±0.0103 - -
Clip 0.8201Â±0.0089 0.8223Â±0.0060 0.8248Â±0.0098 0.8179Â±0.0113 - -
Lip-Reg1 0.8145Â±0.0272 0.8070Â±0.0266 0.8314Â±0.0075 0.8200Â±0.0170 - -
Lip-Reg2 0.8233Â±0.0223 0.8284Â±0.0166 0.8234Â±0.0176 0.8173Â±0.0342 - -
HyperboloidNormal 0.9734Â±0.0157 0.9949Â±0.0123 1.0059Â±0.0118 1.0011Â±0.0036 0.9926Â±0.0169 1.0060Â±0.0070
Clip 1.0172Â±0.0309 0.9865Â±0.0125 0.9965Â±0.0107 1.0049Â±0.0163 1.0148Â±0.0151 0.9818Â±0.0179
Lip-Reg1 0.9975Â±0.0057 1.0036Â±0.0066 1.0222Â±0.0083 1.0232Â±0.0095 1.0143Â±0.0087 1.0281Â±0.0061
Lip-Reg2 0.9907Â±0.0072 0.9891Â±0.0307 0.9866Â±0.0267 1.0196Â±0.0053 1.0198Â±0.0043 0.9648Â±0.0334
Table 8: The test accuracy of three-layer HNN for noisy data with std=0.01
Model Method Texas Wisconsin Actor Cora Pubmed
PoincarÃ© ballNormal 0.8561Â±0.0263 0.7901Â±0.0428 0.7041Â±0.0102 0.2650Â±0.0111 0.5657Â±0.0081
Lip-Reg1 0.8712Â±0.0347 0.8025Â±0.0566 0.7069Â±0.0056 0.2757Â±0.0121 0.5983Â±0.0031
Lip-Reg2 0.8712Â±0.0347 0.8333Â±0.0186 0.7069Â±0.0046 0.2753Â±0.0105 0.5817Â±0.0257
HyperboloidNormal 0.7803Â±0.0347 0.7099Â±0.0566 0.4354Â±0.0092 0.2623Â±0.0240 0.5517Â±0.0074
Lip-Reg1 0.8636Â±0.0228 0.8395Â±0.0466 0.4404Â±0.0088 0.2690Â±0.0240 0.5757Â±0.0090
Lip-Reg2 0.8788Â±0.0473 0.8272Â±0.0650 0.4386Â±0.0068 0.2700Â±0.0218 0.5843Â±0.0047
Table 9: The test accuracy of HGCN for noisy data with std=0.01
Model Method Texas Wisconsin Actor Cora Pubmed
PoincarÃ© ballNormal 0.7500Â±0.0455 0.7840Â±0.0651 0.6777Â±0.0213 0.6240Â±0.0393 0.6457Â±0.0205
Lip-Reg1 0.8030Â±0.0572 0.8025Â±0.0466 0.6894Â±0.0109 0.6907Â±0.0451 0.7107Â±0.0260
Lip-Reg2 0.8106Â±0.0525 0.7901Â±0.0566 0.6807Â±0.0121 0.7320Â±0.0062 0.7263Â±0.0150
HyperboloidNormal 0.7348Â±0.0429 0.7531Â±0.0231 0.6876Â±0.0091 0.6343Â±0.0066 0.6747Â±0.0213
Lip-Reg1 0.7955Â±0.0371 0.7531Â±0.0231 0.6922Â±0.0077 0.7223Â±0.0063 0.7183Â±0.0116
Lip-Reg2 0.7955Â±0.0371 0.7716Â±0.0349 0.6903Â±0.0058 0.7270Â±0.0086 0.7257Â±0.0063
 
1721KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
References
[1]Ola Ahmad and Freddy Lecue. 2022. FisheyeHDK: Hyperbolic Deformable Ker-
nel Learning for Ultra-Wide Field-of-View Image Recognition. In 36th AAAI
Conference on Artificial Intelligence.
[2]James W Anderson. 2006. Hyperbolic geometry. Springer Science & Business
Media.
[3]Alexandre Araujo, Benjamin Negrevergne, Yann Chevaleyre, and Jamal Atif. 2021.
On Lipschitz regularization of convolutional layers using toeplitz matrix theory.
In35th AAAI Conference on Artificial Intelligence.
[4]Mina Ghadimi Atigh, Julian Schoep, Erman Acar, Nanne van Noord, and Pascal
Mettes. 2022. Hyperbolic Image Segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition.
[5]Gregor Bachmann, Gary BÃ©cigneul, and Octavian Ganea. 2020. Constant cur-
vature graph convolutional networks. In International Conference on Machine
Learning.
[6]Gary Becigneul and Octavian-Eugen Ganea. 2019. Riemannian Adaptive Op-
timization Methods. In International Conference on Learning Representations.
https://openreview.net/forum?id=r1eiqi09K7
[7]Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christo-
pher RÃ©. 2020. Low-Dimensional Hyperbolic Knowledge Graph Embeddings.
InProceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. https://doi.org/10.18653/v1/2020.acl-main.617
[8]Ines Chami, Zhitao Ying, Christopher RÃ©, and Jure Leskovec. 2019. Hyperbolic
graph convolutional neural networks. Advances in neural information processing
systems 32 (2019), 4868â€“4879.
[9]Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong
Sun, and Jie Zhou. 2022. Fully Hyperbolic Neural Networks. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). 5672â€“5686.
[10] Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell,
Kamal Nigam, and SeÃ¡n Slattery. 2000. Learning to construct knowledge bases
from the World Wide Web. Artificial intelligence 118, 1-2 (2000), 69â€“113.
[11] Jindou Dai, Yuwei Wu, Zhi Gao, and Yunde Jia. 2021. A Hyperbolic-to-Hyperbolic
Graph Convolutional Network. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition.
[12] George Dasoulas, Kevin Scaman, and Aladin Virmaux. 2021. Lipschitz normal-
ization for self-attention layers with application to graph neural networks. In
International Conference on Machine Learning.
[13] N Benjamin Erichson, Dane Taylor, Qixuan Wu, and Michael W Mahoney. 2021.
Noise-response analysis of deep neural networks quantifies robustness and finger-
prints structural malware. In Proceedings of the 2021 SIAM International Conference
on Data Mining (SDM). SIAM.
[14] Xiran Fan, Chun-Hao Yang, and Baba C Vemuri. 2022. Nested Hyperbolic Spaces
for Dimensionality Reduction and Hyperbolic NN Design. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition.
[15] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2016.
Robustness of classifiers: from adversarial to random noise. Advances in neural
information processing systems 29 (2016).
[16] Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George
Pappas. 2019. Efficient and accurate estimation of Lipschitz constants for deep
neural networks. Advances in Neural Information Processing Systems 32 (2019).
[17] Fernando Gama, Joan Bruna, and Alejandro Ribeiro. 2020. Stability properties
of graph neural networks. IEEE Transactions on Signal Processing 68 (2020),
5680â€“5695.
[18] Fernando Gama and Somayeh Sojoudi. 2022. Distributed linear-quadratic control
with graph neural networks. Signal Processing 196 (2022), 108506.
[19] Octavian Ganea, Gary BÃ©cigneul, and Thomas Hofmann. 2018. Hyperbolic neural
networks. Advances in neural information processing systems 31 (2018), 5345â€“
5355.
[20] Feng Gao, Guy Wolf, and Matthew Hirn. 2019. Geometric scattering for graph
data analysis. In International Conference on Machine Learning.
[21] Amirmasoud Ghiassi, Robert Birke, and Lydia Y Chen. 2023. Robust Learning via
Golden Symmetric Loss of (un) Trusted Labels. In Proceedings of the 2023 SIAM
International Conference on Data Mining (SDM). SIAM.
[22] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining
and harnessing adversarial examples. In International Conference on Learning
Representations.
[23] Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pas-
canu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam
Santoro, and Nando de Freitas. 2019. Hyperbolic Attention Networks. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
rJxHsjRqFQ
[24] Yunhui Guo, Haoran Guo, and Stella X Yu. 2022. Co-SNE: Dimensionality re-
duction and visualization for hyperbolic data. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition.
[25] Yunhui Guo, Xudong Wang, Yubei Chen, and Stella X Yu. 2022. Clipped hyper-
bolic classifiers are super-hyperbolic classifiers. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 11â€“20.
[26] Yujia Huang, Huan Zhang, Yuanyuan Shi, J Zico Kolter, and Anima Anandkumar.
2021. Training Certifiably Robust Neural Networks with Efficient Local Lipschitz
Bounds. Advances in Neural Information Processing Systems 34 (2021).
[27] Yaning Jia, Dongmian Zou, Hongfei Wang, and Hai Jin. 2023. Enhancing Node-
Level Adversarial Defenses by Lipschitz Regularization of Graph Neural Net-
works. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
[28] Matt Jordan and Alexandros G Dimakis. 2020. Exactly computing the local
Lipschitz constant of relu networks. Advances in Neural Information Processing
Systems 33 (2020), 7344â€“7353.
[29] Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. 2020. Convergence and
stability of graph convolutional networks on large random graphs. Advances in
Neural Information Processing Systems 33 (2020), 21512â€“21523.
[30] Hyunjik Kim, George Papamakarios, and Andriy Mnih. 2021. The Lipschitz
constant of self-attention. In International Conference on Machine Learning.
[31] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. 2018. Adversarial examples
in the physical world. In Artificial intelligence safety and security. Chapman and
Hall/CRC, 99â€“112.
[32] Fabian Latorre, Paul Rolland, and Volkan Cevher. 2020. Lipschitz constant esti-
mation of neural networks via sparse polynomial optimization. In International
Conference on Learning Representations.
[33] Marc Law. 2021. Ultrahyperbolic neural networks. Advances in Neural Information
Processing Systems 34 (2021), 22058â€“22069.
[34] Qi Liu, Maximilian Nickel, and Douwe Kiela. 2019. Hyperbolic Graph Neural
Networks. Advances in Neural Information Processing Systems 32 (2019), 8230â€“
8241.
[35] Emile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye
Teh. 2019. Continuous hierarchical representations with poincarÃ© variational
auto-encoders. Advances in neural information processing systems 32 (2019).
[36] Gal Mishne, Zhengchao Wan, Yusu Wang, and Sheng Yang. 2023. The numerical
stability of hyperbolic representation learning. In International Conference on
Machine Learning.
[37] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. 2012. Query-
driven active surveying for collective classification. In 10th International Workshop
on Mining and Learning with Graphs, Vol. 8. 1.
[38] Maximillian Nickel and Douwe Kiela. 2018. Learning continuous hierarchies in
the lorentz model of hyperbolic geometry. In International Conference on Machine
Learning.
[39] Giannis Nikolentzos, Michail Chatzianastasis, and Michalis Vazirgiannis. 2023.
Weisfeiler and Leman go Hyperbolic: Learning Distance Preserving Node Rep-
resentations. In International Conference on Artificial Intelligence and Statistics.
1037â€“1054.
[40] W. Peng, T. Varanka, A. Mostafa, H. Shi, and G. Zhao. 2021. Hyperbolic Deep
Neural Networks: A Survey. IEEE Transactions on Pattern Analysis & Machine
Intelligence (December 2021). https://doi.org/10.1109/TPAMI.2021.3136921
[41] Eric Qu and Dongmian Zou. 2022. Lorentz Direct Concatenation for Stable
Training in Hyperbolic Neural Networks. In NeurIPS 2022 Workshop on Symmetry
and Geometry in Neural Representations.
[42] Eric Qu and Dongmian Zou. 2023. Hyperbolic Convolution via Kernel Point
Aggregation. arXiv preprint arXiv:2306.08862 (2023).
[43] Frederic Sala, Chris De Sa, Albert Gu, and Christopher RÃ©. 2018. Representation
tradeoffs for hyperbolic embeddings. In International Conference on Machine
Learning.
[44] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93â€“93.
[45] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label
poisoning attacks on neural networks. Advances in neural information processing
systems 31 (2018).
[46] Ryohei Shimizu, YUSUKE Mukuta, and Tatsuya Harada. 2021. Hyperbolic Neural
Networks++. In International Conference on Learning Representations.
[47] Rishi Sonthalia and Anna Gilbert. 2020. Tree! I am no tree! I am a low dimensional
hyperbolic embedding. Advances in Neural Information Processing Systems 33
(2020), 845â€“856.
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
InInternational Conference on Learning Representations.
[49] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. 2009. Social influence analysis
in large-scale networks. In Proceedings of the 15th ACM SIGKDD international
conference on Knowledge discovery and data mining.
[50] Matthieu Terris, Audrey Repetti, Jean-Christophe Pesquet, and Yves Wiaux. 2020.
Building firmly nonexpansive convolutional neural networks. In ICASSP 2020-
2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 8658â€“8662.
[51] Abraham Ungar. 2022. A gyrovector space approach to hyperbolic geometry.
Springer Nature.
 
1722Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[52] Aladin Virmaux and Kevin Scaman. 2018. Lipschitz regularity of deep neural
networks: analysis and efficient estimation. Advances in Neural Information
Processing Systems 31 (2018).
[53] Xiao Wang, Yiding Zhang, and Chuan Shi. 2019. Hyperbolic heterogeneous in-
formation network embedding. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 33.
[54] Zhenxing Wu, Dejun Jiang, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Dong-
sheng Cao, and Tingjun Hou. 2021. Hyperbolic relational graph convolution
networks plus: a simple but highly efficient QSAR-modeling method. Briefings
in Bioinformatics 22, 5 (2021), bbab112.
[55] Tao Yu and Christopher M De Sa. 2019. Numerically accurate hyperbolic em-
beddings using tiling-based models. Advances in Neural Information Processing
Systems 32 (2019).
[56] Yiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. 2021. Lorentzian
graph convolutional networks. In Proceedings of the Web Conference 2021. 1249â€“
1261.
[57] Dongmian Zou, Radu Balan, and Maneesh Singh. 2019. On Lipschitz bounds of
general convolutional neural networks. IEEE Transactions on Information Theory
66, 3 (2019), 1738â€“1759.
[58] Dongmian Zou and Gilad Lerman. 2020. Graph convolutional neural networks via
scattering. Applied and Computational Harmonic Analysis 49, 3 (2020), 1046â€“1074.
[59] Daniel ZÃ¼gner, Amir Akbarnejad, and Stephan GÃ¼nnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining.
[60] Daniel ZÃ¼gner, Oliver Borchert, Amir Akbarnejad, and Stephan GÃ¼nnemann.
2020. Adversarial attacks on graph neural networks: Perturbations and their
patterns. ACM Transactions on Knowledge Discovery from Data (TKDD) 14, 5
(2020), 1â€“31.
A Additional Proofs
A.1 Proof of Theorem 3.1
First, we compute
J1=ğœ•y
ğœ•x=ğœ•h
tanhâˆ¥Mxâˆ¥
âˆ¥xâˆ¥tanhâˆ’1(âˆ¥xâˆ¥)
Mx
âˆ¥Mxâˆ¥i
ğœ•x.
Letğ´:=âˆ¥Mxâˆ¥,ğµ:=âˆ¥xâˆ¥,ğ¶:=tanhâˆ’1(âˆ¥xâˆ¥),ğ‘£(x)=tanh(ğ´
ğµÂ·ğ¶),
u(x)=Mx
ğ´. Then,
ğœ•y
ğœ•x=ğ‘£Â·ğœ•u
ğœ•x+uÂ·ğœ•ğ‘£
ğœ•xâŠ¤
. (10)
We proceed to calculateğœ•ğ‘£(x)
ğœ•ğ‘¥andğœ•u(x)
ğœ•x, respectively as follows.
ğœ•ğ‘£(x)
ğœ•xâŠ¤
=tanhâ€²ğ´
ğµÂ·ğ¶ğœ•
ğ´
ğµÂ·ğ¶
ğœ•x
=sech2ğ´
ğµÂ·ğ¶ ğ¶
ğµÂ·ğœ•ğ´
ğœ•xâŠ¤
âˆ’ğ´ğ¶
ğµ2Â·ğœ•ğµ
ğœ•xâŠ¤
+ğ´
ğµÂ·ğœ•ğ¶
ğœ•xâŠ¤
;
ğœ•u(x)
ğœ•x=1
ğ´2ğœ•Mx
ğœ•xÂ·ğ´âˆ’MxÂ·ğœ•ğ´
ğœ•xâŠ¤
=M
ğ´âˆ’Mx
ğ´2Â·ğœ•ğ´
ğœ•xâŠ¤
.
(11)
Combining (10)â€“(11) yields
ğœ•y
ğœ•x=tanhğ´
ğµÂ·ğ¶ M
ğ´âˆ’Mx
ğ´2ğœ•ğ´
ğœ•xâŠ¤
+
Mxsech2ğ´
ğµÂ·ğ¶ ğ¶
ğ´ğµğœ•ğ´
ğœ•xâŠ¤
âˆ’ğ¶
ğµ2ğœ•ğµ
ğœ•xâŠ¤
+1
ğµğœ•ğ¶
ğœ•xâŠ¤
.
Computingğœ•ğ´
ğœ•x,ğœ•ğµ
ğœ•x,ğœ•ğ¶
ğœ•xyields
ğœ•y
ğœ•x=tanhâˆ¥Mxâˆ¥
âˆ¥xâˆ¥tanhâˆ’1(âˆ¥xâˆ¥)
Â·M
âˆ¥Mxâˆ¥âˆ’MxxâŠ¤MâŠ¤M
âˆ¥Mxâˆ¥3
+
Mxsech2âˆ¥Mxâˆ¥
âˆ¥xâˆ¥tanhâˆ’1(âˆ¥xâˆ¥)
Â·"
tanhâˆ’1(âˆ¥xâˆ¥)xâŠ¤MâŠ¤M
âˆ¥Mxâˆ¥2âˆ¥xâˆ¥âˆ’
tanhâˆ’1(âˆ¥xâˆ¥)xâŠ¤
âˆ¥xâˆ¥3+xâŠ¤
âˆ¥xâˆ¥2(1âˆ’âˆ¥xâˆ¥2)#
.The spectral norm of this Jacobian matrix J1thus satisfies (4). â–¡
A.2 Proof of Theorem 3.4
Lety=WâŠ—1x=exp1o(Wlog1
o(x)). Writing exp1oandlog1
oexplic-
itly yields
yğ‘ =sinhÂ©Â­Â­
Â«coshâˆ’1(ğ‘¥ğ‘¡)âˆšï¸ƒ
ğ‘¥2
ğ‘¡âˆ’1âˆ¥W[2:]xğ‘ âˆ¥ÂªÂ®Â®
Â¬W[2:]xğ‘ 
âˆ¥W[2:]xğ‘ âˆ¥.
Leth(xğ‘ )=sinh 
coshâˆ’1(ğ‘¥ğ‘¡)âˆšï¸ƒ
ğ‘¥2
ğ‘¡âˆ’1âˆ¥W[2:]xğ‘ âˆ¥!
xğ‘ 
âˆ¥W[2:]xğ‘ âˆ¥, andğ‘£(xğ‘ )=
sinh 
coshâˆ’1(ğ‘¥ğ‘¡)âˆš
ğ‘¥2ğ‘¡âˆ’1âˆ¥W[2:]xğ‘ âˆ¥!
âˆ¥W[2:]xğ‘ âˆ¥,u(xğ‘ )=xğ‘ . Then,
ğœ•yğ‘ 
ğœ•xğ‘ =W[2:]ğœ•h
ğœ•xğ‘ =W[2:]
ğ‘£ğœ•u
ğœ•xğ‘ +uğœ•ğ‘£
ğœ•xğ‘ 
=W[2:]Â©Â­Â­Â­Â­Â­Â­
Â«sinh 
coshâˆ’1(ğ‘¥ğ‘¡)âˆšï¸ƒ
ğ‘¥2
ğ‘¡âˆ’1âˆ¥W[2:]xğ‘ âˆ¥!
âˆ¥W[2:]xğ‘ âˆ¥I+xğ‘ ğœ•ğ‘£
ğœ•xğ‘ âŠ¤ÂªÂ®Â®Â®Â®Â®Â®
Â¬.(12)
To compute
ğœ•ğ‘£
ğœ•xğ‘ âŠ¤
, we rewrite ğ‘£=sinh 
coshâˆ’1(âˆš
xâŠ¤ğ‘ xğ‘ +1)âˆš
xâŠ¤ğ‘ xğ‘ âˆš
xâŠ¤ğ‘ ğ‘¼xğ‘ !
âˆš
xâŠ¤ğ‘ ğ‘¼xğ‘ 
where we use ğ‘¥ğ‘¡=âˆšï¸
âˆ¥xğ‘ âˆ¥2+1=âˆšï¸
xâŠ¤ğ‘ xğ‘ +1and denote ğ‘¼as
ğ‘¼=WâŠ¤
[2:]W[2:]. Letğ‘“(xğ‘ ):=xâŠ¤ğ‘ xğ‘ ,ğ‘”(xğ‘ ):=xâŠ¤ğ‘ ğ‘¼xğ‘ , thenğ‘£=
sinh
coshâˆ’1(âˆš
ğ‘“+1)âˆš
ğ‘“âˆšğ‘”
âˆšğ‘”, and
ğœ•ğ‘£
ğœ•xğ‘ âŠ¤
=ğœ•ğ‘£
ğœ•ğ‘“ğœ•ğ‘“
ğœ•xğ‘ âŠ¤
+ğœ•ğ‘£
ğœ•ğ‘”ğœ•ğ‘”
ğœ•xğ‘ âŠ¤
. Let
ğ‘=coshâˆ’1(ğ‘¥ğ‘¡)
âˆ¥xğ‘ âˆ¥. Computingğœ•ğ‘£
ğœ•ğ‘“,ğœ•ğ‘£
ğœ•ğ‘”,ğœ•ğ‘“
ğœ•xğ‘ âŠ¤
, andğœ•ğ‘”
ğœ•xğ‘ âŠ¤
yields
ğœ•ğ‘£
ğœ•xğ‘ âŠ¤
=
1
ğ‘¥ğ‘¡âˆ’ğ‘
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥xğ‘ âˆ¥2xâŠ¤
ğ‘ +
Â©Â­Â­
Â«ğ‘cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥2âˆ’sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥3ÂªÂ®Â®
Â¬xâŠ¤
ğ‘ WâŠ¤
[2:]W[2:].
Substituting into (12) yields that J1=
sinh ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥W[2:]+
1
ğ‘¥ğ‘¡âˆ’ğ‘
cosh ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥xğ‘ âˆ¥2W[2:]xğ‘ xâŠ¤
ğ‘ +
 
ğ‘cosh ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥2âˆ’sinh ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥3!
W[2:]xğ‘ xâŠ¤
ğ‘ WâŠ¤
[2:]W[2:].
The spectral norm of J1thus satisfies
âˆ¥J1âˆ¥â‰¤ 
sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥âˆ¥W[2:]xğ‘ âˆ¥âˆ’1
âˆ¥W[2:]xğ‘ âˆ¥2+cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
ğ‘
âˆ¥W[2:]xğ‘ âˆ¥âˆ’ğ‘
âˆ¥xğ‘ âˆ¥+1
ğ‘¥ğ‘¡âˆ¥xğ‘ âˆ¥!
âˆ¥W[2:]âˆ¥.â–¡
 
1723KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
A.3 Proof of Lemma 3.6
Letxğ‘ =[ğœ0Â·Â·Â·0]âŠ¤. Thenâˆ¥xğ‘ âˆ¥=ğœ,âˆ¥W[2:]xğ‘ âˆ¥=ğœâˆ¥W[2]âˆ¥. As
ğœâ†’âˆ ,ğ‘¥ğ‘¡=âˆš
ğœ2+1=ğœ+ğ‘‚
1
ğœ
,ğ‘=coshâˆ’1(ğ‘¥ğ‘¡)
âˆ¥xğ‘ âˆ¥=ln(2ğœ+ğ‘‚(1
ğœ))
ğœ.
Next, we continue with the following simplifications:
sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥W[2:]
=1
2ğœâˆ¥W[2]âˆ¥Â©Â­Â­
Â«
2ğœ+ğ‘‚1
ğœâˆ¥W[2]âˆ¥
âˆ’1

2ğœ+ğ‘‚
1
ğœâˆ¥W[2]âˆ¥ÂªÂ®Â®
Â¬W[2:];

1
ğ‘¥ğ‘¡âˆ’ğ‘
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥xğ‘ âˆ¥2W[2:]xğ‘ xâŠ¤
ğ‘ 
=Â©Â­Â­
Â«1
ğœ+ğ‘‚
1
ğœâˆ’ln
2ğœ+ğ‘‚
1
ğœ
ğœÂªÂ®Â®
Â¬Â·

2ğœ+ğ‘‚
1
ğœâˆ¥W[2]âˆ¥
+1
(2ğœ+ğ‘‚(1
ğœ))âˆ¥W[2]âˆ¥
2W[2:]ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1 0Â·Â·Â· 0
0 0Â·Â·Â· 0
............
0 0Â·Â·Â· 0ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Â©Â­Â­
Â«1
ğœ+ğ‘‚
1
ğœâˆ’ln
2ğœ+ğ‘‚
1
ğœ
ğœÂªÂ®Â®
Â¬Â·

2ğœ+ğ‘‚
1
ğœâˆ¥W[2]âˆ¥
+1
(2ğœ+ğ‘‚(1
ğœ))âˆ¥W[2]âˆ¥
2ğ‘³,
where ğ‘³=W[2]0Â·Â·Â· 0
;
Â©Â­Â­
Â«ğ‘cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥2âˆ’sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥3ÂªÂ®Â®
Â¬Â·
W[2:]xğ‘ xâŠ¤
ğ‘ WâŠ¤
[2:]W[2:]
= ln(2ğœ+ğ‘‚(1
ğœ))
ğœcosh
ln
2ğœ+ğ‘‚
1
ğœ
âˆ¥W[2:]xğ‘ âˆ¥
ğœ2âˆ¥W[2]âˆ¥2âˆ’
sinh
ln
2ğœ+ğ‘‚
1
ğœ
âˆ¥W[2:]xğ‘ âˆ¥
ğœ3âˆ¥W[2]âˆ¥3!
ğœ2ğ‘³WâŠ¤
[2:]W[2:]
=ln
2ğœ+ğ‘‚
1
ğœ
2ğœâˆ¥W[2]âˆ¥2Â©Â­Â­
Â«
2ğœ+ğ‘‚1
ğœâˆ¥W[2]âˆ¥
+1

2ğœ+ğ‘‚
1
ğœâˆ¥W[2]âˆ¥ÂªÂ®Â®
Â¬Â·
ğ‘³WâŠ¤
[2:]W[2:]âˆ’1
2ğœâˆ¥W[2]âˆ¥3 
2ğœ+ğ‘‚1
ğœâˆ¥W[2]âˆ¥
âˆ’
1

2ğœ+ğ‘‚
1
ğœâˆ¥W[2]âˆ¥!
ğ‘³WâŠ¤
[2:]W[2:].Since limğœâ†’âˆğ‘‚
1
ğœ
=0,
limğœâ†’âˆJ1=limğœâ†’âˆsinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥W[2:]+
limğœâ†’âˆ
1
ğ‘¥ğ‘¡âˆ’ğ‘
cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥xğ‘ âˆ¥2W[2:]xğ‘ xâŠ¤
ğ‘ +
limğœâ†’âˆÂ©Â­Â­
Â«ğ‘cosh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥2âˆ’sinh
ğ‘âˆ¥W[2:]xğ‘ âˆ¥
âˆ¥W[2:]xğ‘ âˆ¥3ÂªÂ®Â®
Â¬Â·
W[2:]xğ‘ xâŠ¤
ğ‘ WâŠ¤
[2:]W[2:]
=limğœâ†’âˆ1
âˆ¥W[2]âˆ¥ 
(2ğœ)âˆ¥W[2]âˆ¥âˆ’1âˆ’1
(2ğœ)âˆ¥W[2]âˆ¥+1!
W[2:]+
limğœâ†’âˆ(1âˆ’ln(2ğœ)) 
(2ğœ)âˆ¥W[2]âˆ¥âˆ’1+1
(2ğœ)âˆ¥W[2]âˆ¥+1!
ğ‘³+
limğœâ†’âˆ  
ln(2ğœ)
âˆ¥W[2]âˆ¥2âˆ’1
âˆ¥W[2]âˆ¥3!
(2ğœ)âˆ¥W[2]âˆ¥âˆ’1+
 
ln(2ğœ)
âˆ¥W[2]âˆ¥2+1
âˆ¥W[2]âˆ¥3!
1
(2ğœ)âˆ¥W[2]âˆ¥+1!
ğ‘³WâŠ¤
[2:]W[2:].
Since we haveâˆ¥W[2]âˆ¥âˆ’1on the exponent, a necessary condition
for a finiteâˆ¥J1âˆ¥isâˆ¥W[2]âˆ¥<1. â–¡
B Additional Results
We plot Lipschitz bounds versus training epochs in Figure 4 corre-
sponding to Section 4.2.
(a)
 (b)
(c)
Figure 4: Change of Lipschitz bounds with epochs for the
hyperboloid model. (a) Wisconsin; (b) Actor; (c) Cora.
 
1724