Conformal Counterfactual Inference under Hidden Confounding
Zonghao Chenâˆ—â€ 
University College London
London, United Kingdom
zonghao.chen.22@ucl.ac.ukRuocheng Guoâ€ 
ByteDance Research
London, United Kingdom
ruocheng.guo@bytedance .com
Jean-Francois Ton
ByteDance Research
London, United Kingdom
jeanfrancois@bytedance .comYang Liu
ByteDance Research
San Jose, United States
yang.liu01@bytedance .com
ABSTRACT
Personalized decision making requires the knowledge of poten-
tial outcomes under different treatments, and confidence intervals
about the potential outcomes further enrich this decision-making
process and improve its reliability in high-stakes scenarios. Pre-
dicting potential outcomes along with its uncertainty in a coun-
terfactual world poses the foundamental challenge in causal in-
ference. Existing methods that construct confidence intervals for
counterfactuals either rely on the assumption of strong ignorabil-
ity that completely ignores hidden confounders, or need access
to un-identifiable lower and upper bounds that characterize the
difference between observational and interventional distributions.
In this paper, to overcome these limitations, we first propose a
novel approach wTCP-DR based on transductive weighted con-
formal prediction, which provides confidence intervals for coun-
terfactual outcomes with marginal converage guarantees, even
under hidden confounding. With less restrictive assumptions, our
approach requires access to a fraction of interventional data (from
randomized controlled trials) to account for the covariate shift
from observational distributoin to interventional distribution. The-
oretical results explicitly demonstrate the conditions under which
our algorithm is strictly advantageous to the naive method that
only uses interventional data. Since transductive conformal pre-
diction is notoriously costly, we propose wSCP-DR, a two-stage
variant of wTCP-DR, based on split conformal prediction with same
marginal coverage guarantees but at a significantly lower compu-
tational cost. After ensuring valid intervals on counterfactuals, it
is straightforward to construct intervals for individual treatment
effects (ITEs). We demonstrate our method across synthetic and
real-world data, including recommendation systems, to verify the
superiority of our methods compared against state-of-the-art base-
lines in terms of both coverage and efficiency. Our code can be
found at https://github .com/rguo12/KDD24-Conformal.
âˆ—Work done during an internship at ByteDance Research
â€ Equal contribution
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528 .3671976CCS CONCEPTS
â€¢Mathematics of computing â†’Causal networks; Hypothesis
testing and confidence interval computation.
KEYWORDS
Conformal Prediction, Causal Inference, Uncertainty Quantification
ACM Reference Format:
Zonghao Chen, Ruocheng Guo, Jean-Francois Ton, and Yang Liu. 2024. Con-
formal Counterfactual Inference under Hidden Confounding. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Min-
ing (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi .org/10.1145/3637528 .3671976
1 INTRODUCTION
Estimating the heterogeneous causal effects of an intervention
(e.g., a medicine) on an important outcome (e.g., health status)
of different individuals is a fundamental problem in a variety of
influential research areas, including economics, healthcare and
education [ 2â€“4]. In the growing area of machine learning for causal
inference, this problem has been casted as estimating individual
treatment effect (ITE) and most existing work focuses on developing
machine learning models to improve the point estimate of ITE [ 5â€“
14]. However, point estimates is not enough to ensure safe and
reliable decision-making in high-stake applications where failures
are costly or may endanger human lives, and hence uncertainty
quantification and confidence intervals allow machine learning
models to express confidence in the correctness of their predictions.
Pioneering work [ 6,15] provides confidence intervals for ITEs
through Bayesian machine learning models such as Bayesian Ad-
ditive Regression Trees [ 5] and Gaussian Process [ 16]. However,
these approaches cannot be easily generalized to popular machine
learning models for causal inference on various input data types,
including but not limited to text [17, 18] and graphs [19, 20].
Recently, built upon conformal prediction [ 21,22], Lei and Can-
des [ 1] propose the first conformal prediction method for counter-
factual outcomes and ITEs, which can provide confidence intervals
with guaranteed marginal coverage in a model-agnostic fashion.
This means that, given any machine learning model that estimates
the potential outcomes under treatment, conformal prediction acts
as a post-hoc wrapper that provides confidence intervals guaran-
teed to contain the ground truth of potential outcomes and ITEs
above a specified probability under marginal distribution. Unfortu-
nately however, Lei and Candes [ 1] require the assumption of strong
 
397
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-FranÃ§ois Ton, & Yang Liu
Figure 1: Under hidden confounding, our proposed methods wTCP-
DR and wSCP-DR incorporate a small set of interventional data for
density ratio based weighted conformal prediction, which provides
marginal coverage guarantee along with high efficiency (small con-
fidence interval). In contrast, WCP [ 1] cannot guarantee coverage as
hidden confounding leads to biased estimate of propensity scores.
The Naive method suffers from low efficiency as it only uses the
small set of interventional data.
ignorability that excludes the possibility of hidden confounders,
which cannot be verified given data [ 23,24] and can be violated
in many real-world applications. For example, the socio-economic
status of a patient, which is likely to be unavailable due to privacy
concerns, is a common unobserved confounding factor that affects
both patientâ€™s access to treatment and oneâ€™s health condition. Sim-
ilarly, under the strong ignorability assumption, [ 25] propose to
use meta-learners [ 11,26,27] in conformal prediction of ITEs. Re-
cently, Jin et al. [ 28] take hidden confounding into consideration
for conformal prediction of ITEs from a sensitivity analysis aspect.
However, their method needs access to the upper and lower bounds
of the density ratio between the observational distribution and the
interventional distribution to characterize the covariate shift from
observational to interventional distribution.
To address these limitations and provide confidence intervals
that have finite-sample guarantees even without the strong ignor-
ability assumption, we propose weighted Transductive Conformal
Prediction with Density Ratio estimation (wTCP-DR) that is based on
weighted transductive conformal prediction. With less restrictive
assumptions, wTCP-DR needs access to both observational and a
fraction of interventional data (e.g., data collected from random-
ized control trials) [ 29,30]. In contrast to the weighted conformal
prediction method proposed by [ 1] which uses propensity score as
the reweighting function, our algorithm computes the reweighting
function by learning the density ratio of the interventional and
observational distribution using the data provided. The benefits of
our proposed method are as follows: (i) wTCP-DR does not require
strong ignorability assumption and provides a confidence interval
with coverage guarantee even under the presence of confounding.
(ii) wTCP-DR works well under an imbalanced number of inter-
ventional and observational data, i.e., when interventional data
is of smaller size than observational data due to the higher cost
of collecting interventional data. Although wTCP-DR is compu-
tationally expensive due to the nature of transductive conformal
prediction, we also propose a variant of wTCP-DR, called weighted
SplitConformal Prediction with Density Ratio estimation (wSCP-DR)ğ‘‡ ğ‘Œğ‘ˆ ğ‘‹
Figure 2: Example causal graph with hidden confounding. ğ‘‹: Ob-
served covariates, ğ‘ˆ: Hidden confounders, ğ‘‡: Treatment, ğ‘Œ: Outcome.
Direct edges denote causal relations and the bidirectional edge signi-
fies possible correlation.
which preserves all the advantages of wTCP-DR but at a lower com-
putational cost. We briefly describe how our methods are different
from the method proposed by [1] and the Naive method in Fig. 1.
The paper is organized as follows. Section 2 gives a description of
the problem setting and provides necessary background on confor-
mal prediction. Section 3 describes our novel algorithm wTCP-DR
which provides a confidence interval on counterfactual outcomes
at an individual level with marginal coverage guarantee. Section
4 proposes wSCP-DR which is a more implementable variant of
wTCP-DR. Section 5 applies wTCP-DR and wTCP-DR to provide
confidence intervals for estimating individual treatment effects.
Section 6 demonstrates our method across synthetic and real-world
data, including recommendation systems, to verify our methods in
terms of both coverage and efficiency. Section 7 discusses related
work in the literature. Section 8 concludes the paper.
2 PRELIMINARIES
2.1 Problem setting
We consider the standard potential outcome (PO) framework [ 31,
32] with a binary treatment. Let ğ‘‡âˆˆ{0,1}be the treatment indi-
cator,ğ‘¥âˆˆXâŠ‚ Rğ‘‘be the observed covariates, and ğ‘¦âˆˆYâŠ‚ R
be the outcome of interest. We use ğ‘‹,ğ‘Œ to denote random vari-
ables inX,Y. For each subject ğ‘–, let(ğ‘Œğ‘–(0),ğ‘Œğ‘–(1))be the pair of
potential outcomes under control ğ‘‡=0and treatment ğ‘‡=1, re-
spectively. We assume that the data generating process satisfies
the following widely used assumptions: 1) Consistency: ğ‘Œğ‘–=ğ‘Œğ‘–(ğ‘‡ğ‘–),
which means the observed outcome ğ‘Œğ‘–is the same as the poten-
tial outcome ğ‘Œğ‘–(ğ‘‡ğ‘–)with the observed treatment ğ‘‡ğ‘–. (2) Positivity:
0<P(ğ‘‡=1|ğ‘‹=ğ‘¥)<1,âˆ€ğ‘¥âˆˆX, which means that any subject
has a positive chance to get treated and controlled. We would like to
emphasize that we are notassuming strong ignorability, i.e., there
might exist potential hidden confounding ğ‘ˆthat affects treatment
ğ‘‡and outcome ğ‘Œat the same time. See Fig. 2 for an example causal
graph.
Under this framework, the joint distribution under intervention
ğ‘‘ğ‘œ(ğ‘‡=ğ‘¡)isğ‘ƒğ‘‹,ğ‘Œ(ğ‘¡)=ğ‘ƒğ‘Œ(ğ‘¡)|ğ‘‹Ã—ğ‘ƒğ‘‹and that for observational data
isğ‘ƒğ‘‹,ğ‘Œ|ğ‘‡=ğ‘¡=ğ‘ƒğ‘Œ|ğ‘‹,ğ‘‡=ğ‘¡Ã—ğ‘ƒğ‘‹|ğ‘‡=ğ‘¡. Note that the difference between
conditional distribution ğ‘ƒğ‘Œ(ğ‘¡)|ğ‘‹andğ‘ƒğ‘Œ|ğ‘‹,ğ‘‡=ğ‘¡is due to potential
hidden confounding, and the difference between ğ‘ƒğ‘‹andğ‘ƒğ‘‹|ğ‘‡=ğ‘¡is
due to intervention. Throughout this work, we stick to the nota-
tion of probability density (mass) functions instead of probability
measures. We use superscript ğ¼for interventional distribution and
ğ‘‚for observational distribution. For a given treatment ğ‘¡âˆˆ{0,1},
 
398Conformal Counterfactual Inference under Hidden Confounding KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
we assume there are ğ‘›observational and ğ‘šinterventional samples:
(ğ‘¥ğ‘‚,ğ‘‡=ğ‘¡
ğ‘–,ğ‘¦ğ‘‚,ğ‘‡=ğ‘¡
ğ‘–)ğ‘›
ğ‘–=1âˆ¼ğ‘ğ‘‚
ğ‘¡(ğ‘¥,ğ‘¦)=ğ‘ğ‘‚(ğ‘¦|ğ‘¥,ğ‘¡)ğ‘(ğ‘¥|ğ‘¡)
(ğ‘¥ğ¼,ğ‘‡=ğ‘¡
ğ‘–,ğ‘¦ğ¼,ğ‘‡=ğ‘¡
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1âˆ¼ğ‘ğ¼
ğ‘¡(ğ‘¥,ğ‘¦)=ğ‘ğ¼(ğ‘¦|ğ‘¥,ğ‘¡)ğ‘(ğ‘¥)(1)
Given a predetermined target coverage rate of 1âˆ’ğ›¼, our goal is to
construct confidence interval ğ¶for potential outcome under treat-
mentğ‘¡at a new test sample ğ‘¥ğ‘›+ğ‘š+1âˆ¼ğ‘(ğ‘¥), such thatğ¶(ğ‘¥ğ‘›+ğ‘š+1)
ensures marginal coverage: P(ğ‘¦ğ‘›+ğ‘š+1âˆˆğ¶(ğ‘¥ğ‘›+ğ‘š+1))â‰¥1âˆ’ğ›¼, where
the probability is over (ğ‘¥ğ‘›+ğ‘š+1,ğ‘¦ğ‘›+ğ‘š+1)âˆ¼ğ‘ğ¼
ğ‘¡(ğ‘¥,ğ‘¦).
2.2 Background: Conformal Prediction
Conformal prediction (CP) is a distribution-free framework that
provides finite-sample marginal coverage guarantees. Transductive
and split CP are two approaches to conformal prediction and we
briefly introduce both since we will be using them in Section 3.
Split Conformal Prediction (SCP). Given a datasetD=(ğ‘¥ğ‘–,ğ‘¦ğ‘–)ğ‘›
ğ‘–=1âˆ¼
ğ‘ƒğ‘‹,ğ‘Œ, SCP starts by splitting Dinto two disjoint subsets: a train-
ing setDğ‘¡, and a calibration set Dğ‘. Then, a regression estimator
bğœ‡is trained onDğ‘¡and conformity scores ğ‘ (ğ‘¥,ğ‘¦)are computed
for(ğ‘¥,ğ‘¦) âˆˆ Dğ‘where typically ğ‘ (ğ‘¥,ğ‘¦)=|ğ‘¦âˆ’bğœ‡(ğ‘¥)|. The em-
pirical distribution of the conformity scores are defined as bğ¹=
1
|Dğ‘|Ã|Dğ‘|
ğ‘–=1ğ›¿ğ‘ (ğ‘¥ğ‘–,ğ‘¦ğ‘–)and the confidence interval for the target sam-
pleğ‘¥ğ‘›+1is
ğ¶SCP(ğ‘¥ğ‘›+1)=[bğœ‡(ğ‘¥ğ‘›+1)âˆ’ğ‘bğ¹,bğœ‡(ğ‘¥ğ‘›+1)+ğ‘bğ¹] (2)
whereğ‘bğ¹=Quantile((1âˆ’ğ›¼)(1+1
|Dğ‘|);bğ¹). [33] has proved that
under exchangeability of D,ğ¶SCP(ğ‘¥ğ‘›+1)is guaranteed to satisfy
marginal coverage. Futhermore, if ties between conformity scores
occur with probability zero, then
1âˆ’ğ›¼â‰¤P(ğ‘¦ğ‘›+1âˆˆğ¶SCP(ğ‘¥ğ‘›+1))â‰¤1âˆ’ğ›¼+1
|Dğ‘|(3)
Note that the upper bound ensures that the confidence interval is
nonvacuuous, i.e., the interval width does not go to infinity.
Transductive Conformal Prediction (TCP). Given a same dataset
Das above, TCP takes a different approach by looping over all pos-
sible values ğ‘¦in the domainY. Forğ‘¦âˆˆY, TCP first constructs an
augmented dataset D(ğ‘¥ğ‘›+1,ğ‘¦)=Dâˆª{ğ‘¥ğ‘›+1,ğ‘¦}. Then, a regression
estimator bğœ‡ğ‘¦is trained onD(ğ‘¥ğ‘›+1,ğ‘¦)and the conformity scores read
ğ‘ ğ‘¦
ğ‘–=|ğ‘¦ğ‘–âˆ’bğœ‡ğ‘¦(ğ‘¥ğ‘–)|forğ‘–=1,Â·Â·Â·,ğ‘›andğ‘ ğ‘¦
ğ‘›+1=|ğ‘¦âˆ’bğœ‡ğ‘¦(ğ‘¥ğ‘›+1)|. With
empirical distribution defined as bğ¹=1
ğ‘›+1Ãğ‘›
ğ‘–=1ğ›¿ğ‘ ğ‘¦
ğ‘–+1
ğ‘›+1ğ›¿âˆ, the
interval for the target sample ğ‘¥ğ‘›+1is
ğ¶TCP(ğ‘¥ğ‘›+1)={ğ‘¦âˆˆY :ğ‘ ğ‘¦
ğ‘›+1â‰¤ğ‘bğ¹} (4)
whereğ‘bğ¹=Quantile((1âˆ’ğ›¼);bğ¹). The same lower and upper bound
guarantee as (3) has been proved in [33].
TCP is computationally more expensive as it requires fitting bğœ‡
for every fixed ğ‘¦âˆˆY. The discretization of Ycomes as a tradeoff
between computational costs and accuracy of the conformal inter-
val. For these reasons, SCP is more widely used due to its simplicity,
however, SCP is less sample efficient by splitting the dataset into a
training set and a calibration set. Cross-conformal prediction can
be used to improve efficiency for SCP [34].2.3 Weighted Conformal Prediction
When calibration and test data are independent yet not drawn from
the same distribution, [ 35] propose a weighted version of conformal
prediction. In this section, we discuss a more specific setting of [ 35]
where the dataset are merged from two different distributions, D=
{(ğ‘¥ğ‘–,ğ‘¦ğ‘–)ğ‘›
ğ‘–=1âˆ¼ğ‘ƒğ‘‹,ğ‘Œ}âˆª{(ğ‘¥ğ‘–,ğ‘¦ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1âˆ¼ğ‘ƒâ€²
ğ‘‹,ğ‘Œ}and the test sample
ğ‘¥ğ‘›+ğ‘š+1is sampled from ğ‘ƒâ€²
ğ‘‹. Define the density ratio as ğ‘Ÿ(ğ‘¥,ğ‘¦)=
ğ‘‘ğ‘ƒâ€²
ğ‘‹,ğ‘Œ
ğ‘‘ğ‘ƒğ‘‹,ğ‘Œ(ğ‘¥,ğ‘¦), then(ğ‘¥ğ‘–,ğ‘¦ğ‘–)ğ‘›+ğ‘š+1
ğ‘–=1are weighted exchangeable with
weight functions ğ‘¤(ğ‘¥,ğ‘¦)=1if(ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒğ‘‹,ğ‘Œandğ‘¤(ğ‘¥,ğ‘¦)=ğ‘Ÿ(ğ‘¥,ğ‘¦)
if(ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒâ€²
ğ‘‹,ğ‘Œ. Forğ‘¦âˆˆY, define the normalized weights ğ‘ğ‘–as:
ğ‘ğ‘–=Ã
ğœ:ğœ(ğ‘›+ğ‘š+1)=ğ‘–ğ‘›+ğ‘š+1Ã
ğ‘—=ğ‘›+1ğ‘Ÿ(ğ‘¥ğœ(ğ‘—),ğ‘¦ğœ(ğ‘—))
Ã
ğœğ‘›+ğ‘š+1Ã
ğ‘—=ğ‘›+1ğ‘Ÿ(ğ‘¥ğœ(ğ‘—),ğ‘¦ğœ(ğ‘—))(5)
where the summations are taken over permutations ğœof1,Â·Â·Â·,ğ‘›+
ğ‘š+1(see [ 35, Lemma 3]). Here in Eq. (5), we use an abuse of notation
thatğ‘¦ğ‘›+ğ‘š+1=ğ‘¦for symmetry reason. With the conformity scores
ğ‘ ğ‘¦
ğ‘–computed in the same way as TCP and the weighted empirical
distribution of the conformity scores defined as bğ¹=Ãğ‘›+ğ‘š
ğ‘–=1ğ‘ğ‘–ğ›¿ğ‘ ğ‘¦
ğ‘–+
ğ‘ğ‘›+ğ‘š+1ğ›¿âˆ, the conformal interval for the target sample is:
ğ¶w-TCP(ğ‘¥ğ‘›+ğ‘š+1)={ğ‘¦âˆˆY :ğ‘ ğ‘¦
ğ‘›+ğ‘š+1â‰¤ğ‘bğ¹} (6)
whereğ‘bğ¹=Quantile(1âˆ’ğ›¼;bğ¹). The lower bound guarantee is
proven in [ 35] and the upper bound is proven in [ 1] under extra
assumptions. When ğ‘š=0,ğ‘ğ‘–becomesğ‘Ÿ(ğ‘¥ğ‘–,ğ‘¦ğ‘–)/Ãğ‘›+1
ğ‘—=1ğ‘Ÿ(ğ‘¥ğ‘—,ğ‘¦ğ‘—),
which is more commonly used in the literature [ 1,11,36]. When
ğ‘š>1, the computational cost of ğ‘ğ‘–isğ‘šğ¶ğ‘š
ğ‘›+ğ‘š+1=O(ğ‘šğ‘›ğ‘š).
3 CONFORMAL PREDICTION OF
COUNTERFACTUALS: WTCP-DR
In this section, we formally introduce our proposed method weighted
Transductive Conformal Prediction with Density Ratio estimation
(wTCP-DR). Since our method considers ğ‘‡=0andğ‘‡=1sepa-
rately, we fix ğ‘‡=ğ‘¡in this section and drop the dependence on ğ‘‡in
Eq.(1)for simplicity of notations. Recall there are ğ‘›observational
andğ‘šinterventional samples and the test sample is ğ‘¥ğ‘›+ğ‘š+1.
(ğ‘¥ğ‘‚
ğ‘–,ğ‘¦ğ‘‚
ğ‘–)ğ‘›
ğ‘–=1âˆ¼ğ‘ğ‘‚(ğ‘¥,ğ‘¦)=ğ‘ğ‘‚(ğ‘¦|ğ‘¥,ğ‘¡)ğ‘(ğ‘¥|ğ‘¡)
(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1âˆ¼ğ‘ğ¼(ğ‘¥,ğ‘¦)=ğ‘ğ¼(ğ‘¦|ğ‘¥,ğ‘¡)ğ‘(ğ‘¥)(7)
The Naive Method. We first introduce a straightforward method:
constructing confidence interval for the potential outcome only
from interventional data (ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1using standard split confor-
mal prediction of Eq. (2)as(ğ‘¥ğ¼
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1come from the same dis-
tribution as the test sample ğ‘¥ğ¼
ğ‘›+ğ‘š+1. The algorithm is detailed in
Algorithm 1. From Eq. (3) we know that
1âˆ’ğ›¼+1
ğ‘š+1â‰¥P(ğ‘¦âˆˆğ¶naive(ğ‘¥))â‰¥ 1âˆ’ğ›¼ (8)
This approach can be inefficient because it completely ignores ğ‘›
observational data and typically ğ‘›is larger than ğ‘š.
To combine both ğ‘šinterventional data and ğ‘›observational data,
it is necessary to take distribution shift into consideration. There-
fore, weighted conformal prediction of Eq. (6)is naturally suitable
 
399KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-FranÃ§ois Ton, & Yang Liu
Algorithm 1 Naive algorithm
Require: levelğ›¼, interventional data Dğ¼=(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1split into
a training foldDğ¼
ğ‘¡and a calibration fold Dğ¼ğ‘, target sample
ğ‘¥ğ¼
ğ‘›+ğ‘š+1.
1:Fit regression model Ë†ğœ‡onDğ¼
ğ‘¡.
2:foreach sample(ğ‘¥ğ‘–,ğ‘¦ğ‘–)âˆˆDğ¼ğ‘do
3: Compute the conformity score ğ‘ ğ‘–=|Ë†ğœ‡(ğ‘¥ğ‘–)âˆ’ğ‘¦ğ‘–|.
4:end for
5:Construct empirical distribution of conformity scores bğ¹=
1
|Dğ¼ğ‘|Ã|Dğ¼
ğ‘|
ğ‘–=1ğ›¿ğ‘ ğ‘–.
6:Computeğ‘bğ¹=Quantile((1âˆ’ğ›¼)(1+1
|Dğ‘|);bğ¹).
Ensure:ğ¶ğ‘›ğ‘ğ‘–ğ‘£ğ‘’(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)=[Ë†ğœ‡(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)âˆ’ğ‘bğ¹,Ë†ğœ‡(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)+ğ‘bğ¹]
for such tasks, and the key challenge is to identify the normalized
weights in Eq. (5), i.e., to identify the density ratio
ğ‘Ÿ(ğ‘¥,ğ‘¦):=ğ‘ğ¼(ğ‘¥,ğ‘¦)
ğ‘ğ‘‚(ğ‘¥,ğ‘¦)=ğ‘ğ¼(ğ‘¦|ğ‘¥,ğ‘¡)ğ‘(ğ‘¥)
ğ‘ğ‘‚(ğ‘¦|ğ‘¥,ğ‘¡)ğ‘(ğ‘¥|ğ‘¡)(9)
Under the unconfoundedness assumption of [ 1],ğ‘ğ¼(ğ‘¦|ğ‘¥,ğ‘¡)
equalsğ‘ğ‘‚(ğ‘¦|ğ‘¥,ğ‘¡)soğ‘Ÿ(ğ‘¥,ğ‘¦)is as simple as estimating the propen-
sity scoreğ‘(ğ‘¥)/ğ‘(ğ‘¥|ğ‘¡). When hidden confouding exists, propen-
sity score is not enough to account for the distribution shift. Our
method proposes to learn ğ‘Ÿ(ğ‘¥,ğ‘¦)from data, as detailed next.
Weighted Transductive Conformal Prediction with Density
Ratio estimation (wTCP-DR). The key of weighted conformal
prediction is the density ratio ğ‘Ÿ(ğ‘¥,ğ‘¦), and fortunately there exists a
rich literature of density ratio estimation [ 37], including moment
matching [ 38], probabilistic classification and ratio matching. Since
probabilistic classification using neural networks is more flexible
and better exploits nonlinear relations in the data [ 39], so we only
introduce probabilistic classification here and refer the readers to
[37] for a comprehensive review.
By assigning labels ğ‘§=1to observational data (ğ‘¥ğ‘‚
ğ‘–,ğ‘¦ğ‘‚
ğ‘–)and
assigning labels ğ‘§=0to interventional data (ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–), we construct
a new dataset for learning the density ratio.
DDR={(ğ‘¥ğ‘‚
ğ‘–,ğ‘¦ğ‘‚
ğ‘–,ğ‘§ğ‘–)ğ‘›
ğ‘–=1,(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–,ğ‘§ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1}
For any nonlinear binary classification algorithm like logistic re-
gression with nonlinear features, random forests or neural net-
works that output estimated probabilities of class membership
Ë†ğ‘(ğ‘§=1|ğ‘¥,ğ‘¦)and Ë†ğ‘(ğ‘§=0|ğ‘¥,ğ‘¦), the density ratio can be ap-
proximated by:
ğ‘ğ¼(ğ‘¥,ğ‘¦)
ğ‘ğ‘‚(ğ‘¥,ğ‘¦)=ğ‘(ğ‘¥,ğ‘¦|ğ‘§=0)
ğ‘(ğ‘¥,ğ‘¦|ğ‘§=1)=ğ‘(ğ‘§=0|ğ‘¥,ğ‘¦)/ğ‘(ğ‘§=0)
ğ‘(ğ‘§=1|ğ‘¥,ğ‘¦)/ğ‘(ğ‘§=1)
â‰ˆğ‘(ğ‘§=1)
ğ‘(ğ‘§=0)Ë†ğ‘(ğ‘§=0|ğ‘¥,ğ‘¦)
Ë†ğ‘(ğ‘§=1|ğ‘¥,ğ‘¦)(10)
Sinceğ‘(ğ‘§=1)
ğ‘(ğ‘§=0)is a constant and will cancel out when computing the
normalized weights in Eq. (5), we denote Ë†ğ‘Ÿ(ğ‘¥,ğ‘¦)=Ë†ğ‘(ğ‘§=0|ğ‘¥,ğ‘¦)
Ë†ğ‘(ğ‘§=1|ğ‘¥,ğ‘¦)as the
estimated density ratio, so the corresponding estimated normalizedweights of Eq. (5) are:
Ë†ğ‘ğ‘–=Ã
ğœ:ğœ(ğ‘›+ğ‘š+1)=ğ‘–ğ‘›+ğ‘š+1Ã
ğ‘—=ğ‘›+1Ë†ğ‘Ÿ(ğ‘¥ğœ(ğ‘—),ğ‘¦ğœ(ğ‘—))
Ã
ğœğ‘›+ğ‘š+1Ã
ğ‘—=ğ‘›+1Ë†ğ‘Ÿ(ğ‘¥ğœ(ğ‘—),ğ‘¦ğœ(ğ‘—))(11)
Unfortunately, Eq. (11)requiresğ‘šğ¶ğ‘š
ğ‘›+ğ‘š+1=O(ğ‘šğ‘›ğ‘š)times of eval-
uating Ë†ğ‘Ÿwhich is computationally impractical for ğ‘š>1. As a result,
we only use observational data when computing the normalized
weights (i.e. ğ‘š=1) and use interventional data for computing the
density ratio Ë†ğ‘Ÿ, so the estimated normalized weights become
Ë†ğ‘ğ‘–=Ë†ğ‘Ÿ(ğ‘¥ğ‘–,ğ‘¦ğ‘–)Ãğ‘›
ğ‘—=1Ë†ğ‘Ÿ(ğ‘¥ğ‘—,ğ‘¦ğ‘—)+Ë†ğ‘Ÿ(ğ‘¥ğ‘›+ğ‘š+1,ğ‘¦ğ‘›+ğ‘š+1)(12)
forğ‘–={1,Â·Â·Â·,ğ‘›}âˆª{ğ‘›+ğ‘š+1}. See Algorithm 2 for a complete
description of our method.
By using estimated normalized weights Ë†ğ‘ğ‘–rather than the oracle
normalized weights ğ‘ğ‘–to reweight the empirical distribution of
conformity scores bğ¹, our approach introduces an extra source of
error, as quantified below.
Proposition 1 (Prosample 4.2 from [ 36]).Under the assump-
tions thatğ‘ğ‘‚(ğ‘¥,ğ‘¦)andğ‘ğ¼(ğ‘¥,ğ‘¦)are absolutely continuous with each
other and that[Eğ‘ğ‘‚(ğ‘¥,ğ‘¦)Ë†ğ‘Ÿ(ğ‘¥,ğ‘¦)2]1/2<ğ‘€then the confidence inter-
valğ¶wTCP-DR constructed from Algorithm 2 satisfies
1âˆ’ğ›¼+ğ‘ğ‘›âˆ’1/2+Î”ğ‘Ÿâ‰¥P(ğ‘¦âˆˆğ¶wTCP-DR(ğ‘¥))â‰¥1âˆ’ğ›¼âˆ’Î”ğ‘Ÿ (13)
whereğ‘is a constant and Î”ğ‘Ÿ=Eğ‘ğ‘‚(ğ‘¥,ğ‘¦)|ğ‘Ÿ(ğ‘¥,ğ‘¦)âˆ’Ë†ğ‘Ÿ(ğ‘¥,ğ‘¦)|is the
approximation error of the density ratio.
Algorithm 2 Weighted Transductive Conformal Prediction with
Density Ratio Estimation (wTCP-DR)
Require: levelğ›¼, observational data Dğ‘‚=(ğ‘¥ğ‘‚
ğ‘–,ğ‘¦ğ‘‚
ğ‘–)ğ‘›
ğ‘–=1and inter-
ventional dataDğ¼=(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1, test sample ğ‘¥ğ¼
ğ‘›+ğ‘š+1.
1:Initializeğ¶wTCP-DR(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)=âˆ….
2:Estimate the density ratio Ë†ğ‘ŸusingDğ‘‚andDğ¼.
3:forğ‘¦âˆˆYdo
4: Construct augmented dataset Dğ‘¦=Dğ‘‚âˆª{ğ‘¥ğ¼
ğ‘›+ğ‘š+1,ğ‘¦}.
5: Fit a regression model Ë†ğœ‡onDğ‘¦.
6: Compute conformity scores ğ‘ ğ‘¦
ğ‘–=|Ë†ğœ‡(ğ‘¥ğ‘‚
ğ‘–)âˆ’ğ‘¦ğ‘‚
ğ‘–|forğ‘–=
1,Â·Â·Â·,ğ‘›andğ‘ ğ‘¦
ğ‘›+ğ‘š+1=|Ë†ğœ‡(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)âˆ’ğ‘¦|.
7: Compute the normalized weights Ë†ğ‘ğ‘–as in Eq. (12)(ğ‘¦ğ‘›+ğ‘š+1
is replace with ğ‘¦).
8: Construct weighted empirical distribution of conformity
scoresbğ¹=Ãğ‘›
ğ‘–=1Ë†ğ‘ğ‘–ğ›¿ğ‘ ğ‘¦
ğ‘–+Ë†ğ‘ğ‘›+ğ‘š+1ğ›¿âˆ.
9: Compute quantile ğ‘bğ¹=Quantile(1âˆ’ğ›¼;bğ¹).
10: ifğ‘ ğ‘›+ğ‘š+1â‰¤ğ‘bğ¹then
11:ğ¶wTCP-DR(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)=ğ¶wTCP-DR(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)âˆª{ğ‘¦}.
12: end if
13:end for
Ensure:ğ¶wTCP-DR(ğ‘¥ğ¼
ğ‘›+ğ‘š+1).
By comparing Eq. (13)and Eq. (8), we can see that when we have
access to the oracle density ratio ğ‘Ÿ(ğ‘¥,ğ‘¦), i.eÎ”ğ‘Ÿ=0, then wTCP-DR
 
400Conformal Counterfactual Inference under Hidden Confounding KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
obtains a tighter upper bound than the naive method, as typically
the number of observational data ğ‘›is much larger than the number
of interventional data ğ‘šin causal inference, due to the higher cost
of randomized controlled trails. Unfortunately, oracle density ratio
ğ‘Ÿ(ğ‘¥,ğ‘¦)is usually unavailable, and the estimation error of density
ratio is of order min(ğ‘›,ğ‘š)âˆ’1/2=ğ‘šâˆ’1/2for moment matching
or ratio matching [ 37,39] and of order ğ‘šâˆ’1/2for probabilistic
classification [ 40]. It seems that wTCP-DR has spent a huge amount
of effort while achieving a worse result in the end.
However, we would like to emphasize that the efficiency of con-
formal prediction methods is quantified by the width of the con-
fidence interval, not by the difference between the probability
upper and lower bound. An upper bound strictly lower than 1guar-
antees that the confidence interval is not arbitrarily large, however
there is no guarantee that a smaller upper bound results in a smaller
confidence interval. Intuitively, our method has a smaller interval
compared to the naive method, because the regression model Ë†ğœ‡
of wTCP-DR is trained on ğ‘›observational data while the regres-
sion model Ë†ğœ‡of naive method is trained on ğ‘šinterventional data.
Intuitively, there is a higher chance that the conformity scores
of wTCP-DR are smaller than the conformity scores of the naive
method, which means that ğ¶wTCP-DR is a smaller interval than
ğ¶naive. We formalize the above intuition in the following section
for additive Gaussian noise model.
3.1 Case Study: Additive Gaussian Noise Model
In this section, we consider an additive Gaussian noise model, which
is a simple yet popular setting in causal inference [ 41]. Recall that
we fixğ‘‡=ğ‘¡and drop the dependence on ğ‘‡for simplicity of nota-
tions. Specifically, we make the following assumptions:
A1Additive Gaussian noise. ğ‘¦ğ‘‚âˆ¼N(ğœƒğ‘‚âŠ¤ğœ‘(ğ‘¥ğ‘‚),ğœ2)andğ‘¦ğ¼âˆ¼
N(ğœƒğ¼âŠ¤ğœ‘(ğ‘¥ğ¼),ğœ2), whereğœ‘represents the (learned) features of
interventional and observational data.
A2 Gaussian features. ğœ‘(ğ‘¥ğ‘‚)âˆ¼N( 0,Î£ğ‘‚)andğœ‘(ğ‘¥ğ¼)âˆ¼N( 0,Î£ğ¼).
A3Upper bounds on the difference between oracle density ratio
ğ‘Ÿ(ğ‘¥,ğ‘¦)and estimated density ratio Ë†ğ‘Ÿ(ğ‘¥,ğ‘¦).
Eğ‘ğ‘‚(ğ‘¥,ğ‘¦)(ğ‘Ÿ(ğ‘¥,ğ‘¦)âˆ’Ë†ğ‘Ÿ(ğ‘¥,ğ‘¦))2<âˆ
Î”ğ‘Ÿ:=Eğ‘ğ‘‚(ğ‘¥,ğ‘¦)|ğ‘Ÿ(ğ‘¥,ğ‘¦)âˆ’Ë†ğ‘Ÿ(ğ‘¥,ğ‘¦)|<1âˆ’ğ›¼
ğ›¼
A4 Bounded ğœ’2divergence between ğ‘ğ¼(ğ‘¥,ğ‘¦)andğ‘ğ‘‚(ğ‘¥,ğ‘¦).
ğœ’2(ğ‘ğ¼âˆ¥ğ‘ğ‘‚)=âˆ«ğ‘ğ¼(ğ‘¥,ğ‘¦)
ğ‘ğ‘‚(ğ‘¥,ğ‘¦)âˆ’12
ğ‘ğ‘‚(ğ‘¥,ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦ <âˆ
Under these assumptions, the effect of hidden confounding is re-
flected from the difference of ğ‘ğ‘‚(ğ‘¦|ğ‘¥,ğ‘¡)ğ‘ğ¼(ğ‘¦|ğ‘¥,ğ‘¡)through the
difference of ğœƒğ‘‚andğœƒğ¼:ğœƒğ‘‚is dependent of hidden confounding ğ‘¢
whereasğœƒğ¼is independent of ğ‘¢due to intervention. Before showing
our main theoretical result, let us first discuss the implications of
these assumptions.
A1We assume that interventional and observational data share the
same feature ğœ‘, a commonly used setting in causal inference
especially when ğœ‘is learned with neural networks [ 12]. We
assume the same noise scale for observational and interven-
tional data only for simplicity, which can be relaxed to the more
general case that ğ‘¦ğ‘‚andğ‘¦ğ¼have different noise scales ğœğ‘‚,ğœğ¼.A2This assumption is satisfied when either the features are de-
signed to have Gaussian distribution, or the features are learned
from wide enough neural networks [42].
A3This assumption requires that the error of density ratio estima-
tion is upper bounded, and given that ğ›¼is typically 0.1or0.05,
this assumption is usually satisfied in practice.
A4This assumption ensures that ğ‘ğ¼andğ‘ğ‘‚share the same support
overXÃ—Y , and is required such that the central limit theorem
can be used in the proof.
Now we give the main theoretical result of this paper.
Theorem 1. Assume the above assumptions hold, with probability
at least 1âˆ’ğ›¿1âˆ’ğ›¿2âˆ’ğ›¿3âˆ’ğ›¿4, the interval ğ¶wTCP-DR(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)obtained
from Algorithm 2 will be smaller than the interval ğ¶naive(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)
obtained from Algorithm 1 up to O(âˆšï¸
ğ‘™ğ‘œğ‘”ğ‘›/ğ‘›), withğ›¿1,ğ›¿2,ğ›¿3,ğ›¿4being
the following:
ğ›¿1=Â©Â­
Â«2
ğ‘›1âˆ’ğ›¼âˆ’Î”ğ‘Ÿ
Î”ğ‘Ÿ+1
ğ›¼+Î”ğ‘Ÿ
Î”ğ‘Ÿ+1ğ‘ğ‘‚(ğ‘¥)
ğ‘ğ¼(ğ‘¥)ÂªÂ®
Â¬4ğœ2âˆšï¸ƒ
ğ¶1
ğ¶2
,ğ›¿2=2
ğ‘›,
ğ›¿3=exp
âˆ’1
2ğ¿2
1âˆ’ğ›¼
erfâˆ’1(1âˆ’ğ›¼)2(ğ‘‘âˆ’1)2
ğ‘šâˆ’1
,
ğ›¿4=exp
âˆ’ğ¶2
ğ›¼ğ‘›eff
(ğ‘šâˆ’ğ‘‘)2
whereğ¶1
ğ¶2=(ğœƒğ¼+ğœƒğ‘‚)âŠ¤Î£ğ¼(ğœƒğ¼+ğœƒğ‘‚)
(ğœƒğ¼âˆ’ğœƒğ‘‚)âŠ¤Î£ğ¼(ğœƒğ¼âˆ’ğœƒğ‘‚)represent the dissimilarity distance
betweenğœƒğ¼andğœƒğ‘‚;erfâˆ’1is the inverse error function [ 43],ğ¿1âˆ’ğ›¼
andğ¶ğ›¼are constants that only depend on ğ›¼; andğ‘›effis the effective
sample size defined as below
ğ‘›eff= ğ‘›âˆ‘ï¸
ğ‘–=1Ë†ğ‘Ÿ(ğ‘¥ğ‘‚
ğ‘–)!2.ğ‘›âˆ‘ï¸
ğ‘–=1Ë†ğ‘Ÿ(ğ‘¥ğ‘‚
ğ‘–)2(14)
The proof of Theorem 1 can be found in the Appendix A and B
of https://arxiv.org/abs/2405.12387. The implications of Theorem 1
is summarized as below.
(1)ğ›¿1quantifies the number of observational data needed to con-
tain sufficient information about the interventional distribution.
Ifğœƒğ¼andğœƒğ‘‚are very close, which means that the distribu-
tionsğ‘ğ¼(ğ‘¥,ğ‘¦)andğ‘ğ‘‚(ğ‘¥,ğ‘¦)are very similar, the exponenetğ¶1
ğ¶2
is bigger so fewer observational data (smaller ğ‘›) would contain
sufficient information of the interventional distributions.
(2)ğ›¿2quantifies the stability of the estimator used. Since we are
using the least squared estimator which is known to be stable
whenğ‘›>ğ‘‘andğ‘š>ğ‘‘, having more ğ‘›would entail smaller ğ›¿2.
(3)ğ›¿3andğ›¿4quantifies the ratio of the effective sample size ğ‘›eff
and the interventional sample size ğ‘š.ğ‘›effwas first defined
by [38] in covariate shift literature and [ 35] gives an intuition
that the performance of weighted conformal prediction should
depend onğ‘›eff, our theorem is the first to quantitatively show
thatğ‘›effrather thanğ‘›is the key to measure the performance of
weighed conformal prediction when compared against standard
conformal prediction.
From Theorem 1, we can see that our method in Algorithm 2
is more efficient than the naive method in Algorithm 1 in terms
of width of confidence interval provided, when the interventional
distribution is close to the observational distribution, when the
 
401KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-FranÃ§ois Ton, & Yang Liu
dimensionğ‘‘is relatively high compared to the number of inter-
ventional data ğ‘š, and when the effective sample size ğ‘›effis larger
thanğ‘š. The theoretical result is further corroborated by empirical
findings in Section 6.
Algorithm 3 Two-stage wSCP-DR (Inexact)
Require: Levelğ›¼, observational data Dğ‘‚=(ğ‘¥ğ‘‚
ğ‘–,ğ‘¦ğ‘‚
ğ‘–)ğ‘›
ğ‘–=1and inter-
ventional dataDğ¼=(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1, test sample ğ‘¥ğ¼
ğ‘›+ğ‘š+1.
1:UseDğ‘‚andDğ¼to estimate the density ratio Ë†ğ‘Ÿ.
2:# First stage.
3:forğ‘¥ğ¼
ğ‘—,ğ‘¦ğ¼
ğ‘—âˆˆDğ¼do
4: Fit a regression model Ë†ğœ‡onDğ‘‚âˆª(ğ‘¥ğ¼
ğ‘—,ğ‘¦ğ¼
ğ‘—).
5: Compute conformity scores ğ‘ ğ‘–=|Ë†ğœ‡(ğ‘¥ğ‘‚
ğ‘–)âˆ’ğ‘¦ğ‘‚
ğ‘–|.
6: Compute the normalized weights Ë†ğ‘ğ‘–as in Eq. (12).
7: Construct weighted empirical distribution of conformity
scoresbğ¹=Ãğ‘›
ğ‘–=1Ë†ğ‘ğ‘–ğ›¿ğ‘ ğ‘–+Ë†ğ‘ğ‘—ğ›¿âˆ.
8: Compute quantile ğ‘bğ¹=Quantile(1âˆ’ğ›¼;bğ¹).
9:ğ¶ğ¿
ğ‘—=Ë†ğœ‡(ğ‘¥ğ¼
ğ‘—)âˆ’ğ‘bğ¹andğ¶ğ‘…
ğ‘—=Ë†ğœ‡(ğ‘¥ğ¼
ğ‘—)+ğ‘bğ¹
10:end for
11:# Second stage.
12:Fit regressor Ë†ğ‘šğ¿on(ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ¿
ğ‘›+1),Â·Â·Â·,(ğ‘¥ğ¼ğ‘›+ğ‘š,ğ¶ğ¿ğ‘›+ğ‘š), and fit re-
gressor Ë†ğ‘šğ‘…on(ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ‘…
ğ‘›+1),Â·Â·Â·,(ğ‘¥ğ¼ğ‘›+ğ‘š,ğ¶ğ‘…ğ‘›+ğ‘š).
Ensure:ğ¶ğ¼ğ‘›ğ‘’ğ‘¥ğ‘ğ‘ğ‘¡
ğ‘¤ğ‘†ğ¶ğ‘ƒâˆ’ğ·ğ‘…(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)=[Ë†ğ‘šğ¿(ğ‘¥ğ¼
ğ‘›+ğ‘š+1),Ë†ğ‘šğ‘…(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)]
4 PRACTICAL ALGORITHM: WSCP-DR
In practice, although transductive conformal prediction in Algo-
rithm 2 is theoretically well-grounded, it is notoriously expensive
to compute, compared to split conformal prediction. The reason
that split conformal prediction cannot be used in Algorithm 2 is the
density ratio Ë†ğ‘Ÿevaluated at test sample, which requires the knowl-
edge of both test covariate ğ‘¥ğ‘›+ğ‘š+1and test target value ğ‘¦ğ‘›+ğ‘š+1but
unfortunately ğ‘¦ğ‘›+ğ‘š+1is inaccessible to us. In this section, we show
that we can do two-stage split conformal prediction which is com-
putationally more efficient than transductive conformal prediction
Algorithm 2 and achieves the same marginal coverage guarantee.
In the first stage, recall that interventional labels ğ‘¦ğ¼
ğ‘›+1,Â·Â·Â·,ğ‘¦ğ¼ğ‘›+ğ‘š
are accessible, so the density ratios Ë†ğ‘Ÿ(ğ‘¥ğ¼
ğ‘›+1,ğ‘¦ğ¼
ğ‘›+1),Â·Â·Â·,Ë†ğ‘Ÿ(ğ‘¥ğ¼ğ‘›+ğ‘š,ğ‘¦ğ¼ğ‘›+ğ‘š)
and the normalized conformal weights in Eq. (5)can be computed
forğ‘›+1,Â·Â·Â·,ğ‘›+ğ‘š. Therefore, split weighted conformal prediction
can be used to construct intervals (ğ¶ğ¿
ğ‘›+1,ğ¶ğ‘…
ğ‘›+1),Â·Â·Â·,(ğ¶ğ¿ğ‘›+ğ‘š,ğ¶ğ‘…ğ‘›+ğ‘š)
for interventional data (ğ‘¥ğ¼
ğ‘›+1,ğ‘¦ğ¼
ğ‘›+1),Â·Â·Â·,(ğ‘¥ğ¼ğ‘›+ğ‘š,ğ‘¦ğ¼ğ‘›+ğ‘š)with mar-
ginal coverage guarantee. In the second stage, by noticing that the
test sampleğ‘¥ğ¼
ğ‘›+ğ‘š+1shares the same distribution as ğ‘¥ğ¼
ğ‘›+1,Â·Â·Â·,ğ‘¥ğ¼ğ‘›+ğ‘š,
a standard split conformal prediction can be used to construct con-
fidence interval[ğ¶ğ¿ğ‘¥ğ‘›+ğ‘š+1,ğ¶ğ‘…
ğ‘›+ğ‘š+1]for the test sample ğ‘¥ğ‘›+ğ‘š+1with
marginal coverage guarantee. Details of this method are presented
in Algorithm 4. Additionally, we can further reduce the computa-
tional cost of Algorithm 4 by directly fitting a regressor Ë†ğœ‡ğ¿over the
interval lower bounds (ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ¿
ğ‘›+1),Â·Â·Â·,(ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ¿ğ‘›+ğ‘š)and fitting
a regressor Ë†ğœ‡ğ‘…over the interval upper bounds (ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ‘…
ğ‘›+1),Â·Â·Â·,
(ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ‘…ğ‘›+ğ‘š)in the second stage. Therefore, we call Algorithm 4the exact two-stage method which has marginal coverage guaran-
tee and call Algorithm 3 the inexact two-stage method which does
not have marginal coverage guarantee but is more efficient.
Algorithm 4 Two-stage wSCP-DR (Exact)
Require: Levelğ›¼, observational data Dğ‘‚=(ğ‘¥ğ‘‚
ğ‘–,ğ‘¦ğ‘‚
ğ‘–)ğ‘›
ğ‘–=1and inter-
ventional dataDğ¼=(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘›+ğ‘š
ğ‘–=ğ‘›+1, test sample ğ‘¥ğ¼
ğ‘›+ğ‘š+1.
1:UseDğ‘‚andDğ¼to estimate the density ratio Ë†ğ‘Ÿ.
2:# First stage.
3:Same as the first stage in Algorithm 3
4:# Second stage.
5:SplitDğ¼into a training set of size ğ‘š1:Dğ¼
ğ‘¡ğ‘Ÿ=(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘›+ğ‘š1
ğ‘–=ğ‘›+1and
calibration set of size ğ‘šâˆ’ğ‘š1:Dğ¼
ğ‘ğ‘ğ‘™=(ğ‘¥ğ¼
ğ‘–,ğ‘¦ğ¼
ğ‘–)ğ‘š
ğ‘–=ğ‘š1+1.
6:Fit regressor Ë†ğ‘šğ¿on(ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ¿
ğ‘›+1),Â·Â·Â·,(ğ‘¥ğ¼ğ‘›+ğ‘š1,ğ¶ğ¿ğ‘›+ğ‘š1)and Ë†ğ‘šğ‘…
on(ğ‘¥ğ¼
ğ‘›+1,ğ¶ğ‘…
ğ‘›+1),Â·Â·Â·,(ğ‘¥ğ¼ğ‘›+ğ‘š1,ğ¶ğ‘…ğ‘›+ğ‘š1).
7:Compute conformity scores on Dğ¼
ğ‘ğ‘ğ‘™:ğ‘ ğ‘–=max{Ë†ğ‘šğ¿(ğ‘¥ğ¼
ğ‘–)âˆ’
ğ¶ğ¿
ğ‘–,ğ¶ğ‘…
ğ‘–âˆ’Ë†ğ‘šğ‘…(ğ‘¥ğ¼
ğ‘–)}forğ‘–={ğ‘š1+1,Â·Â·Â·,ğ‘š}.
8:Construct empirical distribution of conformity scores bğ¹=
1
ğ‘šâˆ’ğ‘š1Ãğ‘š
ğ‘–=ğ‘š1+1ğ›¿ğ‘ ğ‘–.
9:Computeğ‘bğ¹=Quantile((1âˆ’ğ›¼)(1+1
ğ‘šâˆ’ğ‘š1);bğ¹).
Ensure:ğ¶ğ¸ğ‘¥ğ‘ğ‘ğ‘¡
ğ‘¤ğ‘†ğ¶ğ‘ƒâˆ’ğ·ğ‘…(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)=[Ë†ğ‘šğ¿(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)âˆ’ğ‘bğ¹,Ë†ğ‘šğ‘…(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)+ğ‘bğ¹]
Conformal Inference of Individual Treatment Effect. In Section
3 and 4, we focus on conformal inference for counterfactual out-
comesğ‘Œ(1)andğ‘Œ(0). However, offering confidence intervals for
individual treatment effects may hold greater practical significance.
Our algorithms wTCP-DR and wSCP-DR can predict confidence
intervals[ğ¶ğ¿
ğ‘¡(ğ‘¥ğ¼
ğ‘›+ğ‘š+1),ğ¶ğ‘…
ğ‘¡(ğ‘¥ğ¼
ğ‘›+ğ‘š+1)],ğ‘¡âˆˆ{0,1}that has marginal
coverage guarantee for the potential outcome ğ‘¦ğ‘›+ğ‘š+1under treat-
mentğ‘¡=1(or under control ğ‘¡=0). The naive way of construcing
intervals for ITE is to use bonferroni correction, i.e., ğ¶ğ¿
ğ¼ğ‘‡ğ¸=ğ¶ğ¿
1âˆ’ğ¶ğ‘…
0
andğ¶ğ‘…
ğ¼ğ‘‡ğ¸=ğ¶ğ‘…
1âˆ’ğ¶ğ¿
0. We demonstrate the empirical result using
the naive way in Section 6 for fair comparison among methods that
infer counterfactual outcomes, and we also include the results in
Appendix A.1 where intervals for ITE are constructed using the
nested methods from [1, Section 4].
5 EXPERIMENTS
5.1 Experiment on Synthetic Data
Here, we conduct experiments for counterfactual outcome and ITE
estimation on synthetic data with hidden confounding and focus
on the setting where the number of observational data ğ‘›is larger
than the number of interventional data ğ‘š. We aim to answer the
following research questions: RQ1: Can our proposed methods
achieve the specified level of coverage (0.9) for potential outcomes
under the setting with hidden confounding and ğ‘›larger than ğ‘š
for counterfactual outcomes and ITEs? RQ2: Can our proposed
methods have better efficiency (smaller confidence interval) than
the Naive method which only uses interventional data? RQ3: How
does hidden confounding strength impact the coverage of our meth-
ods? RQ4: How does the size of interventional data ( ğ‘š) impact the
efficiency of our methods?
 
402Conformal Counterfactual Inference under Hidden Confounding KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Description for synthetic data, Yahoo and Coat
Dataset ğ‘›ğ‘¡ğ‘Ÿğ‘›ğ‘ğ‘ğ‘™ğ‘šğ‘¡ğ‘Ÿğ‘šğ‘ğ‘ğ‘™ğ‘šğ‘¡ğ‘ 
Synthetic 5,000 5,000 125 125 200
Yahoo 103,343 25,706 10,800 10,800 32,399
Coat 5,568 1,385 928 928 2,784
Dataset. For synthetic data, we use the following data-generating
process for the observables ğ‘‹,ğ‘‡,ğ‘Œ with hidden confounding ğ‘ˆ.
ğ‘ˆ,ğ‘âˆ¼N( 0,I),ğœ–1,ğœ–0âˆ¼N( 0,1)
ğ‘‹=ğ‘âŠ™(ğ‘2(1âˆ’ğ‘ˆ)+ğ‘2ğ‘ˆ)+ğ‘ˆ
ğœŒ=ğ‘Â¯ğ‘ˆ+(1âˆ’ğ‘)(1âˆ’Â¯ğ‘ˆ), ğ‘‡âˆ¼Bern(ğœŒ)
ğ‘Œ(1)=1
1+exp(âˆ’3(Â¯ğ‘ˆ+2))+0.1ğœ–1
ğ‘Œ(0)=1
1+exp(âˆ’3(Â¯ğ‘ˆâˆ’2))+0.1ğœ–0
ğ‘Œ=ğ‘‡ğ‘Œ(1)+( 1âˆ’ğ‘‡)ğ‘Œ(0)(15)
Iisğ‘‘Ã—ğ‘‘identity matrix, ğ‘‘is the dimensionality of ğ‘‹,âŠ™is the
hadamard product, Â¯ğ‘ˆis the mean of each dimension of ğ‘ˆ, and
ğ‘=5,ğ‘=3,ğ‘=0.9. Whenğ‘is close to 1,ğœŒis close to 0asÂ¯ğ‘ˆis close
to0, leading to more controlled samples (less treated samples) in
the observational data.
Baselines. Naive : it uses interventional data for standard split con-
formal prediction, as detailed in Algorithm 1. WCP : the algorithm
proposed in [ 1] that uses propensity score as the reweighting func-
tion in WCP. For all the methods we use the same Gradient Boosting
Tree from scikit-learn as the base model Ë†ğœ‡.
Data Splitting Details. We split the observational and interven-
tional data into training Dğ‘‚
ğ‘¡ğ‘Ÿ,Dğ¼
ğ‘¡ğ‘Ÿ, calibrationDğ‘‚
ğ‘ğ‘ğ‘™,Dğ¼
ğ‘ğ‘ğ‘™, and test
Dğ‘¡ğ‘ . For the Naive method, we train the base model Ë†ğœ‡onDğ¼
ğ‘¡ğ‘Ÿ
and compute conformity scores on Dğ¼
ğ‘ğ‘ğ‘™. For WCP, we train the
base model Ë†ğœ‡onDğ‘‚
ğ‘¡ğ‘Ÿand compute conformity scores on Dğ‘‚
ğ‘ğ‘ğ‘™. The
propensity model is trained on Dğ‘‚
ğ‘¡ğ‘Ÿ. For our methods, we train the
base model Ë†ğœ‡onDğ‘‚
ğ‘¡ğ‘Ÿand compute conformity scores on Dğ‘‚
ğ‘ğ‘ğ‘™. The
density ratio estimator Ë†ğ‘Ÿis trained onDğ‘‚
ğ‘¡ğ‘ŸâˆªDğ¼
ğ‘¡ğ‘Ÿ. The size of each
split can be found in Table 1.
Evaluation Metrics. We use the evaluation metrics from [ 1,25]
for both counterfactual outcomes and ITEs. Coverage measures the
probability of the true counterfactual outcome falling in predicted
confidence interval , where 1is the indicator function. Interval
width is the average size of the confidence interval ğ¶(ğ‘¥ğ‘–)on test
samplesğ‘–âˆˆ Dğ‘¡ğ‘ , which represents the efficiency of conformal
inference methods.
Comparison Results (RQ1-2). Table 2 shows results under the
setting ofğ‘›=10,000andğ‘š=250under strong hidden confounding
(ğ‘‘=1). We make the following observations:
â€¢In terms of coverage, our methods wSCP-DR (Exact) and wTCP-
DR achieve the specified level of coverage (0 .9) forğ‘Œ(0),ğ‘Œ(1)
and ITE. wSCP-DR (Inexact) has coverage slightly lower than 0.9
forğ‘Œ(1)andğ‘Œ(0)as it trades coverage guarantee for lower com-
putational cost. The coverage results verify that our proposed
reweighting function based on density ratio estimation can accu-
rately adapt the conformity scores computed on observational
data to the interventional distribution even under hidden con-
founding. In contrast, coverage of WCP is much lower than 0.9,because WCP does not take hidden confounding into considera-
tion, which leads to biased estimates of propensity scores so even
after reweighting, the interventional data is not exchangeable
with the observational data. Therefore, the confidence interval
constructed by WCP does not have coverage guarantee.
â€¢Considering interval width, wSCP-DR (Inexact) achieves much
better efficiency (narrower interval widths) than Naive for coun-
terfactual outcomes and ITE. As wSCP-DR (Exact) expands the
confidence interval to gain guaranteed coverage and has slightly
smaller interval width than the Naive method. WCP has the
smallest interval width, however, its confidence intervals cannot
contain the ground truth with 0.9probability as desired. In prac-
tice, we recommend using wSCP-DR (Inexact) for its enhanced
efficiency, if there is no strict requirement on coverage.
â€¢There is a imbalance of the number of treated and controlled
samples in the observational data. Notice that ğ‘=0.9in Eq. (15)
means that the size of controlled group is larger than the size
of treated group in observational data. As a result, compared to
Naive method, wTCP-DR has smaller interval width for ğ‘Œ(0), but
it has a similar interval width for ğ‘Œ(1), due to the fact that only
the number of controlled samples is larger than ğ‘šwhile the num-
ber of treated samples is at the same scale as ğ‘š. This observation
verifies the theory of Theorem 1. Nevertheless, wTCP-DRâ€™s ITE
interval is still smaller than Naive.
Impact of Hidden Confounding Strength on Coverage (RQ3).
Here, we modify the dimensionality of observed covariates ğ‘‘âˆˆ
{1,3,5,10}where larger ğ‘‘means weaker hidden confounding. Fig. 3
shows the results with varying hidden confounding strengths. We
make the following observations. At varying levels of hidden con-
founding strength, wSCP-DR (Exact) and Naive can maintain the
specified level of coverage. In contrast, coverage of wSCP-DR (In-
exact) is slightly lower than the specified level. When hidden con-
founding is stronger ( ğ‘‘is lower), WCP has lower coverage be-
cause it ignores hidden confounders and hence its propensity score
reweighted conformal prediction does not have guaranteed cover-
age. When hidden confounding gets weaker (larger ğ‘‘), the coverage
of WCP starts to improve, because propensity scores gets closer to
the true density ratio that accounts for the distribution shift.
Impact of Interventional Data Size ğ‘šon Interval Width (RQ4).
Here, we study the impact of the size of interventional data ğ‘š=
ğ‘šğ‘¡ğ‘Ÿ+ğ‘šğ‘ğ‘ğ‘™on interval width, under strong hidden confounding
ğ‘‘=1. Fig. 4 shows results with different ğ‘š. The interval width
(efficiency) of the Naive method benefit the most from increasing
ğ‘šas its has more training samples and also a larger calibration set
for split conformal prediction, which agrees with Eq. (8). Increas-
ingğ‘šhas no significant impact on the efficiency of our methods,
which agrees with Eq. (13). The reason is that our methods only
use interventional data for density ratio estimation, so larger ğ‘š
only improves the quality of estimated density ratios, which does
not impact the conformity scores because the scores are computed
on the observational data. For WCP, it does not use interventional
data at all, so increasing ğ‘šalso has no impact. As we discussed
before, due to the sample size difference between treatment group
and control group, wTCP-DRâ€™s efficiency is worse for ğ‘Œ(1)but its
interval width for ITE can still be narrower than that of Naive.
 
403KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-FranÃ§ois Ton, & Yang Liu
Table 2: Results for counterfactual outcomes and ITEs on the synthetic data. We compare our methods wSCP-DR (Inexact), wSCP-DR (Inexact),
and wTCP-DR with baselines. Results are shown for coverage and confidence interval width on the synthetic data with ğ‘›=10,000andğ‘š=250.
Boldface and underlining are used to highlight the top and second-best interval width among the methods with coverage close to 0.9.
Method Coverageğ‘Œ(0)â†‘Interval Width ğ‘Œ(0)â†“Coverageğ‘Œ(1)â†‘Interval Width ğ‘Œ(1)â†“Coverage ITE â†‘Interval Width ITE â†“
wSCP-DR(Inexact) 0.891Â±0.026 0.414Â±0.008 0.889Â±0.019 0.421Â±0.013 0.942Â±0.017 0.835Â±0.016
wSCP-DR(Exact) 0.934Â±0.026 0.496Â±0.010 0.935Â±0.023 0.503Â±0.010 0.957Â±0.018 0.998Â±0.015
wTCP-DR 0.899Â±0.028 0.386Â±0.013 0.923Â±0.015 0.576Â±0.066 0.953Â±0.015 0.962Â±0.074
WCP 0.572Â±0.039 0.222Â±0.007 0.608Â±0.042 0.227Â±0.009 0.710Â±0.027 0.449Â±0.012
Naive 0.932Â±0.018 0.508Â±0.042 0.930Â±0.023 0.560Â±0.049 0.952Â±0.018 1.068Â±0.098
(a) Coverage of ğ‘Œ(0)
 (b) Coverage of ğ‘Œ(1)
 (c) Coverage of ITE
Figure 3: Coverage results of counterfactual outcomes and ITE with varying hidden confounding strength. Higher dimensional ğ‘‹carries more
information of the hidden confounders, leading to weaker hidden confounding. Their interval width results are in Fig. 5 of Appendix A.1.
(a) Interval width of ğ‘Œ(0)with different ğ‘š
(b) Interval width of ğ‘Œ(1)with different ğ‘š
Figure 4: Impact of interventional data size ğ‘šon efficiency of con-
formal inference methods. See Appendix A.1 for coverage results.
5.2 Counterfactual Outcome Estimation on
Real-world Recommendation System Data
Causal recommendation datasets Yahoo!R31(Yahoo) and Coat2
can benchmark counterfactual outcome estimation under hidden
confounding [ 44â€“46]. Note that we use these datasets for coun-
terfactual regression, leaving ranking based evaluation for future
work. Following the formulation of [ 44,45], we define each sample
as a user-item pair, define treatment as whether the item is exposed
to the user, and define outcome as the userâ€™s rating from 1 to 5. The
1https://webscope.sandbox.yahoo.com/
2https://www.cs.cornell.edu/~schnabts/mnar/goal is to predict potential outcome ğ‘Œ(1)for the user-item pairs
in the test setDğ‘¡ğ‘ given the learned embeddings of a user-item
pairğ‘‹. Available information include massive observational data
fromğ‘ƒğ‘‹,ğ‘Œ|ğ‘‡=1and a small set of interventional data from ğ‘ƒğ‘‹,ğ‘Œ(1).
We run conformal inference on the top of the classic matrix fac-
torization model [ 47] trained onDğ¼
ğ‘¡ğ‘Ÿfor Naive andDğ‘‚
ğ‘¡ğ‘Ÿfor other
methods. The size of dataset split can be found in Table 1.
Methods for Comparison. In addition to wSCP-DR (Inexact and
Exact), we introduce their variants wSCP-DR* (Inexact and Exact)
that estimate the density ratio by learned embeddings asğ‘ğ¼(ğ‘¥)
ğ‘ğ‘‚(ğ‘¥).
This is a favorable setting in practice because randomized controlled
trail is costly, whereas randomly assigned users without requiring
their outcomes under treatment is much cheaper and easier to
implement. We aim to illustrate our methods can perform well even
when there is no access to labeled interventional data. Here, we
do not consider wTCP-DR due to its high computational cost. For
baselines, we use Naive andWCP-NB â€“ A variant of WCP which
uses interventional data with labels to train a Naive Bayes classifier
for estimating propensity scores as in [30, 46, 48].
Table 3: Coverage and interval width results on Yahoo and Coat.
Boldface and underlining are used to highlight the top and second-
best interval width among the methods with coverage close to 0.9.
Yahoo Coat
Method Coverage â†‘ Interval Width â†“ Coverage â†‘ Interval Width â†“
wSCP-DR(Inexact) 0.892Â±0.019 4.353Â±0.019 0.919Â±0.008 3.787Â±0.045
wSCP-DR(Exact) 0.952Â±0.001 5.140Â±0.001 0.959Â±0.001 4.565Â±0.228
wSCP-DR*(Inexact) 0.892Â±0.020 4.353Â±0.020 0.919Â±0.008 3.789Â±0.046
wSCP-DR*(Exact) 0.952Â±0.001 5.140Â±0.001 0.960Â±0.001 4.571Â±0.233
WCP-NB 0.825Â±0.002 4.036Â±0.002 0.912Â±0.005 3.635Â±0.040
Naive 0.899Â±0.001 6.047Â±0.001 0.896Â±0.003 7.725Â±0.018
Comparison Results (RQ1-2). We fixğ‘šğ‘¡ğ‘Ÿ=ğ‘šğ‘ğ‘ğ‘™for Yahoo and
Coat to ensure ğ‘›larger thanğ‘šandğ‘šğ‘¡ğ‘ is large enough (see Table 1).
Studies onğ‘šğ‘¡ğ‘Ÿandğ‘šğ‘ğ‘ğ‘™can be found in Appendix A.1. Table 3
shows results on these two datasets. Our methods achieve 0.9cover-
age and have significantly smaller intervals than the Naive method.
Surprisingly, even when the density ratio is estimated only from
 
404Conformal Counterfactual Inference under Hidden Confounding KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
the learned embeddings without using interventional labels, our
method can still achieve 0.9coverage and small intervals. Therefore,
our method has the potential to completely replace randomized
controlled trail with randomized assignation of users when the
dimension of the covariate ğ‘‹is higher than the dimension of target
ğ‘¦, saving huge amounts of resources in practice. In contrast, even
with interventional data, WCP-NB fails to maintain 0.9coverage
on the Yahoo dataset because does not take hidden confounding
into consideratin. As expected, Naive has the widest intervals on
both datasets while maintaining 0.9 coverage most of the time.
6 RELATED WORK
Estimation of individual treatment effect has been the key for in-
dividual decision making in economics [ 49], healthcare [ 3] and
education [ 2]. Construcing confidence intervals for ITE provides
additional information for decision making process to improve
its reliability in high-stake situations [ 50,51]. Previous methods
that aim at constructing confidence intervals for the estimation of
counterfactual outcomes and individual treatment effects include
Bayesian inference [ 6], bootstrapping [ 52], kernel smoothing [ 53],
etc. These methods are known to have aymptotic coverage guaran-
tees (i.e. they require infinite number of samples) and depend on
the specific choice of regression models.
Recently, conformal prediction [ 33,35] becomes increasingly
popular because it has marginal coverage guarantee with finite
number of samples and it is also agnostic to the regression model
used. [ 1] has proposed to use weighted conformal prediction to con-
struct intervals for counterfactuals and ITE, and [ 25] also proposes
to use conformal prediction along with meta-learners to construct
intervals for ITE. However, both [ 1,54] require strong ignorability
assumption and completely ignores the existence of confounding
variables, which is unverifiable and unrealistic in practice. Recently,
[28] conducts sensitivity analysis of conformal prediction for ITE
under hidden confounding, but their method assumes marginal
selection condition, another unverifiable assumption in practice.
7 CONCLUSION
In this paper, we propose a novel algorithm WTCP-DR that provides
confidence intervals for predicting counterfactual outcomes and in-
dividual treatment effects with guaranteed marginal coverage, even
under hidden confounding. Our theory explicitly demonstrates the
conditions under which wTCP-DR is strictly advantageous to the
naive method that only uses interventional data. We also propose
a two stage variant called wSCP-DR with the same guarantee at
a lower computational cost than wTCP-DR. We demonstrate that
wTCP-DR and wSCP-DR achieve superior performances against
state-of-the-art baselines in terms of both coverage and efficiency
across synthetic and real-world datasets.
REFERENCES
[1]Lihua Lei and Emmanuel J CandÃ¨s. Conformal inference of counterfactuals
and individual treatment effects. Journal of the Royal Statistical Society Series B:
Statistical Methodology, 83(5):911â€“938, 2021.
[2]Xiang Zhou. Attendance, completion, and heterogeneous returns to col-
lege: A causal mediation approach. Sociological Methods & Research, page
00491241221113876, 2022.
[3]Thierry Wendling, Kenneth Jung, Alison Callahan, Alejandro Schuler, Nigam H
Shah, and Blanca Gallego. Comparing methods for estimation of heterogeneous
treatment effects using observational data from health care databases. Statistics
in medicine, 37(23):3309â€“3324, 2018.[4]Richard Breen, Seongsoo Choi, and Anders Holm. Heterogeneous causal effects
and sample selection bias. Sociological Science, 2:351â€“369, 2015.
[5]Hugh A Chipman, Edward I George, and Robert E McCulloch. Bart: Bayesian
additive regression trees. 2010.
[6] Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal
of Computational and Graphical Statistics, 20(1):217â€“240, 2011.
[7]Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treat-
ment effects using random forests. Journal of the American Statistical Association,
113(523):1228â€“1242, 2018.
[8]Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for
counterfactual inference. In International conference on machine learning, pages
3020â€“3029, 2016.
[9]Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treat-
ment effect: generalization bounds and algorithms. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 3076â€“3085. JMLR.
org, 2017.
[10] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and
Max Welling. Causal effect inference with deep latent-variable models. Advances
in neural information processing systems, 30, 2017.
[11] SÃ¶ren R KÃ¼nzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for
estimating heterogeneous treatment effects using machine learning. Proceedings
of the national academy of sciences, 116(10):4156â€“4165, 2019.
[12] Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the
estimation of treatment effects. Advances in neural information processing systems,
32, 2019.
[13] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang.
Representation learning for treatment effect estimation from observational data.
Advances in neural information processing systems, 31, 2018.
[14] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of hetero-
geneous treatment effects: From theory to learning algorithms. In International
Conference on Artificial Intelligence and Statistics, pages 1810â€“1818. PMLR, 2021.
[15] Ahmed M Alaa and Mihaela Van Der Schaar. Bayesian inference of individual-
ized treatment effects using multi-task gaussian processes. Advances in neural
information processing systems, 30, 2017.
[16] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for
machine learning, volume 2. MIT press Cambridge, MA, 2006.
[17] Victor Veitch, Dhanya Sridhar, and David Blei. Adapting text embeddings for
causal inference. In Conference on Uncertainty in Artificial Intelligence, pages
919â€“928. PMLR, 2020.
[18] Amir Feder, Katherine A Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar,
Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E
Roberts, et al. Causal inference in natural language processing: Estimation, predic-
tion, interpretation and beyond. Transactions of the Association for Computational
Linguistics, 10:1138â€“1158, 2022.
[19] Ruocheng Guo, Jundong Li, and Huan Liu. Learning individual causal effects from
networked observational data. In Proceedings of the 13th international conference
on web search and data mining, pages 232â€“240, 2020.
[20] Jing Ma, Mengting Wan, Longqi Yang, Jundong Li, Brent Hecht, and Jaime Teevan.
Learning causal effects on hypergraphs. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, pages 1202â€“1212, 2022.
[21] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning
in a random world, volume 29. Springer, 2005.
[22] Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman. On-line predictive linear
regression. The Annals of Statistics, pages 1566â€“1590, 2009.
[23] Eric J Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi, and Wang Miao. An
introduction to proximal causal learning. arXiv preprint arXiv:2009.10982, 2020.
[24] Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and
effect. Basic books, 2018.
[25] Ahmed Alaa, Zaid Ahmad, and Mark van der Laan. Conformal meta-
learners for predictive inference of individual treatment effects. arXiv preprint
arXiv:2308.14895, 2023.
[26] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling
without replacement from a finite universe. Journal of the American statistical
Association, 47(260):663â€“685, 1952.
[27] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous
causal effects. arXiv preprint arXiv:2004.14497, 2020.
[28] Ying Jin, Zhimei Ren, and Emmanuel J CandÃ¨s. Sensitivity analysis of individual
treatment effects: A robust conformal inference approach. Proceedings of the
National Academy of Sciences, 120(6):e2214889120, 2023.
[29] Ye Li, Hong Xie, Yishi Lin, and John CS Lui. Unifying offline causal inference
and online bandit learning for data driven decision. In Proceedings of the Web
Conference 2021, pages 2291â€“2303, 2021.
[30] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli
Lin, and Keping Yang. Autodebias: Learning to debias for recommendation.
InProceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 21â€“30, 2021.
[31] Jerzy S Neyman. On the application of probability theory to agricultural experi-
ments. essay on principles. section 9.(tlanslated and edited by dm dabrowska and
 
405KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-FranÃ§ois Ton, & Yang Liu
tp speed, statistical science (1990), 5, 465-480). Annals of Agricultural Sciences,
10:1â€“51, 1923.
[32] Donald B Rubin. Causal inference using potential outcomes: Design, modeling,
decisions. Journal of the American Statistical Association, 100(469):322â€“331, 2005.
[33] Jing Lei, Max Gâ€™Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman.
Distribution-free predictive inference for regression. Journal of the American
Statistical Association, 113(523):1094â€“1111, 2018.
[34] Vladimir Vovk. Cross-conformal predictors. Annals of Mathematics and Artificial
Intelligence, 74:9â€“28, 2015.
[35] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas.
Conformal prediction under covariate shift. Advances in neural information
processing systems, 32, 2019.
[36] Muhammad Faaiz Taufiq, Jean-Francois Ton, Rob Cornish, Yee Whye Teh, and
Arnaud Doucet. Conformal off-policy prediction in contextual bandits. Advances
in Neural Information Processing Systems, 35:31512â€“31524, 2022.
[37] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation
in machine learning. Cambridge University Press, 2012.
[38] Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borg-
wardt, Bernhard SchÃ¶lkopf, et al. Covariate shift by kernel mean matching.
Dataset shift in machine learning, 3(4):5, 2009.
[39] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and
Masashi Sugiyama. Relative density-ratio estimation for robust distribution
comparison. Neural computation, 25(5):1324â€“1370, 2013.
[40] Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification,
and risk bounds. Journal of the American Statistical Association, 101(473):138â€“156,
2006.
[41] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard
SchÃ¶lkopf. Nonlinear causal discovery with additive noise models. Advances in
neural information processing systems, 21, 2008.
[42] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pen-
nington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes.
arXiv preprint arXiv:1711.00165, 2017.
[43] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions
with formulas, graphs, and mathematical tables, volume 55. US Government
printing office, 1948.[44] Yixin Wang, Dawen Liang, Laurent Charlin, and David M Blei. The deconfounded
recommender: A causal inference approach to recommendation. arXiv preprint
arXiv:1808.06581, 2018.
[45] Yixin Wang, Dawen Liang, Laurent Charlin, and David M Blei. Causal infer-
ence for recommender systems. In Proceedings of the 14th ACM Conference on
Recommender Systems, pages 426â€“431, 2020.
[46] Qing Zhang, Xiaoying Zhang, Yang Liu, Hongning Wang, Min Gao, Jiheng Zhang,
and Ruocheng Guo. Debiasing recommendation by learning identifiable latent
confounders. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, 2023.
[47] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques
for recommender systems. Computer, 42(8):30â€“37, 2009.
[48] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and
Thorsten Joachims. Recommendations as treatments: Debiasing learning and
evaluation. In international conference on machine learning , pages 1670â€“1679.
PMLR, 2016.
[49] Colin Camerer. Individual decision making. The handbook of experimental
economics, 1:587â€“704, 1995.
[50] Brent R Logan, Rodney Sparapani, Robert E McCulloch, and Purushottam W Laud.
Decision making and uncertainty quantification for individualized treatments
using bayesian additive regression trees. Statistical methods in medical research,
28(4):1079â€“1093, 2019.
[51] Andrew Jesson, SÃ¶ren Mindermann, Uri Shalit, and Yarin Gal. Identifying causal-
effect inference failure with uncertainty-aware models. Advances in Neural
Information Processing Systems, 33:11637â€“11649, 2020.
[52] Wanzhu Tu and Xiao-Hua Zhou. A bootstrap confidence interval procedure for
the treatment effect using propensity score subclassification. Health Services and
Outcomes Research Methodology, 3:135â€“147, 2002.
[53] Nathan Kallus, Xiaojie Mao, and Angela Zhou. Interval estimation of individual-
level causal effects under unobserved confounding. In The 22nd international
conference on artificial intelligence and statistics, pages 2281â€“2290. PMLR, 2019.
[54] Mingzhang Yin, Claudia Shi, Yixin Wang, and David M Blei. Conformal sensitivity
analysis for individual treatment effects. Journal of the American Statistical
Association, pages 1â€“14, 2022.
 
406Conformal Counterfactual Inference under Hidden Confounding KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Appendix A EXPERIMENTS
A.1 Experiments on Synthetic Data
Implementation Details. For WCP, the propensity model is implemented as a logistic regression model, which is widely adopted in the
causal inference literature. For density ratio estimation, we use the MLP model from scikit-learn3to classify whether a given data point
(ğ‘¥,ğ‘¦)is from observational or interventional distribution.
Results of Nested Methods for ITE. We skipped the experiment for wTCP-DR as the nested methods from [ 1] for ITE requires inferring
confidence intervals of potential outcomes on the massive Dğ‘‚
ğ‘ğ‘ğ‘™, leading to extremely heavy computational cost. Table 4 shows results on
ITE with nested inexact and exact methods which can construct ITE intervals from intervals of counterfactual outcomes. As we can see,
under the nested inexact method, none of the methods achieve 0.9 coverage, as this method does not guarantee coverage. While the nested
exact method can significantly expand the confidence interval, leading to low efficiency.
Ablation Study on Density Estimation Method: MLP vs Density Estimator (DR). We compare two different density estimators, i.e.,
MLP from scikit-learn and density estimator densratio4(DR) on the synthetic dataset, where we adopt the same setting as the results shown
in Table 2. Intuitively, directly modeling the density of the joint distribution (DR) is more challenging than classifying whether a data point
is from the observational or the interventional distribution (MLP). We can observe that the coverage of wTCP-DR drops significantly when
DR is used, because an inaccurate estimate of density ratio would result in worse coverage of wTCP-DR. wSCP-DR (Exact and Inexact) are
more robust against inaccurate density ratios due to the correction taken from the second-stage inference.
Results with Different Settings. Here, we illustrate the results for different dimensionalities of the observed features ( ğ‘‘ğ‘–ğ‘š(ğ‘‹)) in Fig. 5 and
results for different sample size of interventional data ( ğ‘š) in Fig. 6. In Fig. 5, we can observe that the coverage rates of all methoeds increase
asğ‘‘ğ‘–ğ‘š(ğ‘‹)grows, which corresponds to less hidden confounding. At the same time, the interval widths of most of the methods become
narrower when ğ‘‘ğ‘–ğ‘š(ğ‘‹)increases due to the decrease of calibration error of the underlying regression models given more informative
observed features ğ‘‹. For WCP, it only provides expected coverage guarantees when ğ‘‘ğ‘–ğ‘š(ğ‘‹)is large, which leads to weak hidden confounding
and accurate estimates of propensity scores. Its interval widths increase with ğ‘‘ğ‘–ğ‘š(ğ‘‹)such that the coverage can be guaranteed. In Fig. 6, we
show the coverage and interval width with ğ‘šranging within{10,20,50,100,250,500,750,1,000}. For all methods, the coverage is increasing
withğ‘šand the interval width is decreasing with ğ‘š, as expected. This is because, for small ğ‘š,ğ‘š<50, wTCP-DR cannot achieve the specified
level of coverage (0.9) because the density ratio estimator has high variance. As ğ‘šincreases, wTCP-DR reaches the coverage of 0.9 and the
smallest interval width.
Table 4: Results of ITE on synthetic data under the nested inexact and exact methods [1].
Method Coverage ITE (Nested Inexact) Interval Width ITE (Nested Inexact) Coverage ITE (Nested Exact) Interval Width ITE (Nested Exact)
wSCP-DR(Inexact) 0.749Â±0.055 0 .422Â±0.011 0 .938Â±0.012 0 .767Â±0.011
wSCP-DR(Exact) 0.819Â±0.033 0 .504Â±0.009 0 .948Â±0.016 0 .847Â±0.008
WCP 0.458Â±0.062 0 .224Â±0.007 0 .865Â±0.027 0 .602Â±0.006
Naive 0.850Â±0.060 0 .558Â±0.095 0 .945Â±0.019 0 .943Â±0.104
Table 5: Comparison of MLP and DR as density estimators with wTCP-DR and wSCP-DR (Inexact and Exact). The setting is the same as Table 2.
Method Coverage ğ‘Œ(0)â†‘ Interval Width ğ‘Œ(0)â†“ Coverageğ‘Œ(1)â†‘ Interval Width ğ‘Œ(1)â†“ Coverage ITE â†‘ Interval Width ITE â†“
MLP wSCP-DR(Inexact) 0.891 Â±0.026 0.414 Â±0.008 0.889 Â±0.019 0.421 Â±0.013 0.942 Â±0.017 0.835 Â±0.016
MLP wSCP-DR(Exact) 0.934 Â±0.026 0.496 Â±0.010 0.935 Â±0.023 0.503 Â±0.010 0.957 Â±0.018 0.998 Â±0.015
MLP wTCP-DR 0.899 Â±0.028 0.386 Â±0.013 0.923 Â±0.015 0.576 Â±0.066 0.953 Â±0.015 0.962 Â±0.074
DR wSCP-DR(Inexact) 0.899 Â±0.024 0.423 Â±0.013 0.874 Â±0.014 0.411 Â±0.011 0.946 Â±0.020 0.834 Â±0.015
DR wSCP-DR(Exact) 0.936 Â±0.014 0.503 Â±0.009 0.934 Â±0.004 0.493 Â±0.017 0.966 Â±0.014 0.996 Â±0.009
DR wTCP-DR 0.847 Â±0.022 0.363 Â±0.011 0.853 Â±0.031 0.372 Â±0.013 0.910 Â±0.020 0.735 Â±0.016
A.2 Experiments on Recommendation System Data
Implementation Details. We use MSE loss to train matrix factorization (MF) models [ 47] with 64 dimensional embeddings as the base
model for rating prediction, which is one of the most popular approaches in recommendation systems [ 44,48]. In this setting, the features
(user/item embeddings) are learned from the factual outcomes ğ‘Œ, leading to their capability to capture part of hidden confounding. We
use the Python version of the package densratio for density ratio estimation of our method to handle the high dimensional. For WCP-NB,
following [ 30,48], we fit a Naive Bayes classifier to model the propensity ğ‘ƒ(ğ‘‡=1|ğ‘‹,ğ‘,ğ‘Œ). It is simplified as ğ‘ƒ(ğ‘‡=1|ğ‘Œ)=ğ‘ƒ(ğ‘Œ|ğ‘‡=1)ğ‘ƒ(ğ‘‡=1)
ğ‘ƒ(ğ‘Œ).
Asğ‘ƒ(ğ‘Œ|ğ‘‡=0)is not available in the observational data, ğ‘ƒ(ğ‘Œ)can only be estimated from the interventional data where treatment is
randomized ( ğ‘ƒ(ğ‘Œ)=ğ‘ƒğ¼(ğ‘Œ)=ğ‘ƒğ¼(ğ‘Œ|ğ‘‡)). So, WCP-NB needs to use interventional data with outcomes. In this case, WCP-NB can be seens as a
variant of our method using a different density ratio estimator based on propensity scores.
Impact ofğ‘šğ‘ğ‘ğ‘™.We maintain ğ‘šğ‘¡ğ‘Ÿ=0.2ğ‘š,ğ‘šğ‘¡ğ‘ =0.6ğ‘šand modify ğ‘šğ‘ğ‘ğ‘™âˆˆ{0.05ğ‘š,0.1ğ‘š,0.15ğ‘š,0.2ğ‘š}. Results are shown in Fig. 7. All the
methods maintain coverage close or above 0.9 for all cases. In terms of efficiency, we can observe that the efficiency of Naive gets slightly
improved with increasing ğ‘šğ‘ğ‘ğ‘™.
Impact ofğ‘šğ‘¡ğ‘Ÿ.We maintain ğ‘šğ‘ğ‘ğ‘™=0.2ğ‘š,ğ‘šğ‘¡ğ‘ =0.6ğ‘šand modifyğ‘šğ‘¡ğ‘Ÿâˆˆ{0.05ğ‘š,0.1ğ‘š,0.15ğ‘š,0.2ğ‘š}. Fig. 7 shows results on Coat where ğ‘šis
small. We make the following observations. First, the efficiency of Naive is improved because its base model has lower MSE with more
3https://scikit-learn .org/stable/
4https://github .com/hoxo-m/densratio_py
 
407KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-FranÃ§ois Ton, & Yang Liu
(a) Coverage of ğ‘Œ(0)
 (b) Coverage of ğ‘Œ(1)
 (c) Coverage of ITE
(d) Interval width of ğ‘Œ(0)
 (e) Interval width of ğ‘Œ(1)
 (f) Interval width of ITE
Figure 5: Coverage and interval width results of counterfactual outcomes and ITE with varying hidden confounding strength. Higher
dimensional ğ‘‹carries more information of the hidden confounders, leading to weaker hidden confounding.
(a) Coverage of ğ‘Œ(0)with different ğ‘š
 (b) Coverage of ğ‘Œ(1)with different ğ‘š
(c)ğ‘Œ(0)interval width with different ğ‘š
(d)ğ‘Œ(1)interval width with different ğ‘š
Figure 6: Impact of interventional data size ğ‘šon coverage and efficiency of conformal inference methods.
(a) Test empirical coverage with differ-
entğ‘šğ‘ğ‘ğ‘™on Coat
(b) Test interval width with different
ğ‘šğ‘ğ‘ğ‘™on Coat
(c) Test empirical coverage with differ-
entğ‘šğ‘¡ğ‘Ÿon Coat
(d) Test interval width with different
ğ‘šğ‘¡ğ‘Ÿon Coat
Figure 7: Results on Coat with different ğ‘šğ‘¡ğ‘Ÿand different ğ‘šğ‘ğ‘ğ‘™.
training data, leading to smaller confidence intervals. Second, the coverage of all methods are improved, as more trainig samples from the
interventional distribution can improve the base model for the Naive method, density ratio estimators for our methods and the propensity
model for WCP-NB.
 
408