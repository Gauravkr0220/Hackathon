ProtoMix: Augmenting Health Status Representation Learning via
Prototype-based Mixup
Yongxin Xuâˆ—
Key Laboratory of High
Confidence Software
Technologies, Ministry of
Education
Peking University
Beijing, China
xuyx@stu.pku.edu.cnXinke Jiangâˆ—
Key Laboratory of High
Confidence Software
Technologies, Ministry of
Education
Peking University
Beijing, China
xinkejiang@stu.pku.edu.cnXu Chuâ€ 
Key Laboratory of High
Confidence Software
Technologies, Ministry of
Education
SCS & CFCS, Peking
University
Beijing, China
chu_xu@pku.edu.cnYuzhen Xiao
Chaohe Zhang
Hongxin Ding
Key Laboratory of High
Confidence Software
Technologies, Ministry of
Education
Peking University
Beijing, China
Junfeng Zhao
Key Laboratory of High
Confidence Software
Technologies, Ministry of
Education
Peking University
Beijing, China
Nanhu Laboratory
Jiaxing, ChinaYasha Wang
Key Laboratory of High
Confidence Software
Technologies, Ministry of
Education
Peking University
Beijing, ChinaBing Xie
Key Laboratory of High
Confidence Software
Technologies, Ministry of
Education
Peking University
Beijing, China
Abstract
With the widespread adoption of electronic health records (EHR)
data, deep learning techniques have been broadly utilized for vari-
ous health prediction tasks. Nevertheless, the labeled data scarcity
issue restricts the prediction power of these deep models. To en-
hance the generalization capability of deep learning models when
faced with such situations, a common trend is to train generative
adversarial networks (GANs) or diffusion models for data augmen-
tation. However, due to limitations in sample size and potential
label imbalance issues, these methods are prone to mode collapse
problems. This results in the generation of new samples that fail to
preserve the subtype structure within EHR data, thereby limiting
their practicality in health prediction tasks that generally require
detailed patient phenotyping. Aiming at the above problems, we pro-
pose a Proto type-based Mixup method, dubbed ProtoMix , which
combines prior knowledge of intrinsic data features from subtype
centroids (i.e., prototypes) to guide the synthesis of new samples.
Specifically, ProtoMix employs a prototype-guided mixup training
task to shift the decision boundary away from the subtypes. Then,
âˆ—Both authors contributed equally to this research.
â€ Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671937ProtoMix optimizes the sampling weights in different areas of the
data manifold via a prototype-guided mixup sampling strategy.
Throughout the training process, ProtoMix dynamically expands
the training distribution using an adaptive mixing coefficient com-
putation method. Experimental evaluations on three real-world
datasets demonstrate the efficacy of ProtoMix.
CCS Concepts
â€¢Applied computing â†’Health informatics; â€¢Information
systemsâ†’Data mining.
Keywords
Healthcare Informatics; Representation Learning; Electronic Health
Records; Labeled Data Scarcity
ACM Reference Format:
Yongxin Xu, Xinke Jiang, Xu Chu, Yuzhen Xiao, Chaohe Zhang, Hongxin
Ding, Junfeng Zhao, Yasha Wang, and Bing Xie. 2024. ProtoMix: Augment-
ing Health Status Representation Learning via Prototype-based Mixup. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671937
1 Introduction
With the digitization of hospital information systems, the avail-
ability of electronic health records (EHR) data has significantly im-
proved [ 45]. EHR provide valuable data resources for researchers by
comprehensively documenting patientsâ€™ health trajectories, includ-
ing their medical conditions, diagnoses, and treatment plans [ 18].
Therefore, developing powerful computational methods to mine
3633
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongxin Xu et al.
(a) GRACE
 (b) EHRDiff
 (c)ProtoMix
Figure 1: t-Distributed Stochastic Neighbor Embedding (t-
SNE) visualization of real samples (blue points) and generated
samples (red points) on the MIMIC-III dataset (e.g., mortality
prediction task with label imbalance). Left: GAN-based rep-
resentative method GRACE; Middle: Diffusion model-based
representative method EHRDiff; Right: ProtoMix.
patient health patterns from EHR data, and assist doctors in as-
sessing the patient health status and making targeted treatment
decisions, has garnered significant interest from researchers [ 8,9].
Among them, deep learning techniques, which can capture complex
input-output relationships [ 21,54], have been widely applied to
various health prediction tasks [ 22,23,25,35,55,63,64,80,82,88].
However, the EHR datasets often suffer from limited labeled sam-
ples due to the sensitivity and privacy of the data [ 34,62,70]. In
this situation of insufficient supervising, these data-hungry deep
models are prone to over-fitting [ 27], resulting in poor generaliza-
tion abilities [ 68,77], especially for minority classes in imbalanced
label distributions [ 29,78,79]. Data augmentation techniques en-
hance the generalization capability of the model without explic-
itly collecting new data, by pushing the decision boundary away
from the data scatter in the feature space, effectively preventing
over-fitting [ 19,46]. To address the impact of labeled data scarcity
and learn reliable representations, some studies have begun to ex-
plore the application of data augmentation techniques, especially
training generative adversarial networks (GANs) [26] or diffusion
models [ 31] to synthesize new samples, thereby improving decision
boundaries [ 5,12,17,62,86]. Despite such fruitful progress, they
still suffer from the following limitations:
These methods risk disrupting the subtype structure in
the synthesized new samples. Subtypes represent finer-grained
patient subgroups that are not captured by the standard labels,
and they exhibit diverse disease progression trajectories [ 13,66].
Furthermore, mining and identifying patient subtypes from EHR
data at a more granular level is essential for accurate diagnosis and
treatment, and critical for improving the performance of health
prediction tasks [ 6,25,89]. As shown in Figure 1, GANs and dif-
fusion models face severe mode collapse issues when applied to
real-world EHR datasets, which are characterized by a scarcity of
labeled samples and an imbalanced label distribution [ 44,60]. Con-
sequently, the generated data tends to capture only a narrow slice
of the original data distribution. This limitation hampers the ex-
pected performance gains from increasing the sample size through
data augmentation. In this work, we consider identifying patient
subtypes hidden under labels through clustering analysis. By lever-
aging these subtypes, we guide the interpolation process based on
the original data distribution to synthesize new samples, therebybypassing the instability, high costs, and mode-collapse issues com-
monly associated with training generative models. As a result, the
augmented samples effectively capture and preserve the diversity
of subtypes. Although seemingly straightforward, implementing
this intuition will encounter the following challenges:
C1. How to synthesize training samples that make the deci-
sion boundary far from subtypes? The patient subtypes corre-
spond to the sub-manifolds in geometric structures [ 40]. Moving
the decision boundary away from these sub-manifolds is crucial
for improving the recognition of subtypes and subsequent predic-
tion tasks [ 71]. Popular interpolation methods, such as Mixup [ 90],
generate convex combinations of training samples and their corre-
sponding labels, encouraging the model to have better predictive
capabilities in the interpolation region between samples [ 91]. These
methods show potential in moving the decision boundary away
from the training samples. However, how to ensure that the deci-
sion boundary moves away from subtypes when generating new
samples remains unknown.
C2. How to maintain a high signal-to-noise ratio when se-
lecting samples during data augmentation? In the process of
randomly combining pairs of samples, the vanilla mixup method
assigns equal weights to both highly concentrated and less concen-
trated areas on the data manifold [ 7,65] (e.g., the typical samples of
the minority classes and the noisy samples of the majority classes).
The resulting augmented data might include noise, which biases
the decision boundary too close to large clusters.
C3. How to choose a reasonable interpolation coefficient?
The vanilla mixup method randomly selects mixing coefficients
from prior distributions, like the beta distribution, to determine
interpolation strength. Improper values can cause under-fitting
and limit diversity [ 28]. The correlations among patient subtypes
can hint at sample dependence and potentially guide interpolation
strength [92], but has been marginally investigated.
By jointly considering the above issues, we propose ProtoMix ,
aProto type-based Mixup method. It is a variant of the manifold
mixup method [ 75], utilizing the prior knowledge about the in-
trinsic features of the data from the centroids of the subtypes (i.e.,
prototypes) to guide the synthesis of new samples. This includes
improving decision boundaries and mixing strategies to achieve
robust patient health representation learning. Specifically, our main
contributions are summarized as follows:
â€¢To solve the challenge C1, we propose a prototype-guided mixup
training task. By assigning new subtype labels to each sam-
ple based on prototypes and generating new training samples
through interpolation of features and subtype labels from differ-
ent samples, we aim to shift the decision boundary away from
each subtype, thereby better preserving the subtype structure
throughout the data augmentation process.
â€¢Addressing challenge C2, we propose a prototype-guided mixup
sampling strategy to choose the appropriate sample pairs by
considering both label frequency and sample typicality (The dis-
tance between the sample and the prototype) simultaneously,
thereby achieving better representation recalibration. Address-
ing challenge C3, we propose an adaptive mixing coefficient
computation method to decide the mixing strength based on the
learned correlations among patient subtypes.
3634ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
prototypePrototype-guidedMixup
â€¦464.0571.4240.9571.4ğ‘£!"ğ‘£!#ğ‘£!$EHR BackboneEncoderâ€¦285.9690.1285.9274.9ğ‘£%"ğ‘£%#ğ‘£%$â„!â„%!ğ’‰=ğœ†!,#ğ’‰!+1âˆ’ğœ†!,# ğ’‰#(ğ’š$=ğœ†!,#ğ’š!$+1âˆ’ğœ†!,#ğ’š#$Guidingğ¿*+forPrototype-guidedMixupTraining
Prototype-basedSamplerLabelfrequencyTypicalityscoreSampleâ€¦494.0748.4521.0748.4ğ‘£!"ğ‘£!#ğ‘£!$EHR BackboneEncoderâ€¦477.0464.1011.3540.0ğ‘£%"ğ‘£%#ğ‘£%$â„!â„%ManifoldMixup!ğ’‰=ğœ†%,&ğ’‰%+1âˆ’ğœ†%,& ğ’‰&,ğ‘¦=ğœ†%,&ğ‘¦%+1âˆ’ğœ†%,&ğ‘¦&Decisionboundary
Patient i's EHRPatient j's EHRPatient k's EHRPatient l's EHRğ¿*.forMixup Training under Prototype-guided Sampling
(a) Prototype-guided Mixup Training and Mixup Training under Prototype-guided Sampling
EHR BackboneEncoderBiaffineModelMixingcoefficient ğœ†!,# forFigure2(a)
ğœ™!ğœ™"
â€¦285.9690.1285.9274.9ğ‘£#$ğ‘£#%ğ‘£#&Patient j's EHRâ€¦464.0571.4240.9571.4ğ‘£'$ğ‘£'%ğ‘£'&Patient i's EHR (b) Adaptive Coefficient Computation
Figure 2: The Framework of ProtoMix.
â€¢We construct extensive experiments on three real-world EHR
datasets. Experimental results, complemented by in-depth ab-
lation studies, reveal that ProtoMix consistently outperforms
all state-of-the-art models across various evaluation metrics on
these datasets. Besides, the medical findings uncovered are in har-
mony with established medical literature, delivering significant
insights and clarifications in the medical field.
2 Problem Formulation
EHR Dataset. In longitudinal EHR data, each patientâ€™s informa-
tion is organized as a sequence of visits in chronological order,
with multiple medical codes accompanying each visit. Let C=
ğ‘1,ğ‘2,...,ğ‘|C|	
be the entire set of codes used in an EHR dataset,
where|C|is the number of unique medical codes, each of which
representing a diagnosis, a medication, or a procedure. For in-
stance, the ICD-9 Code 240.9 in Figure 2 corresponds to Goiter,
unspecified. For the ğ‘–-th patient, the visit sequence is defined as
Xğ‘–=
x1
ğ‘–,x2
ğ‘–,Â·Â·Â·,xğ‘‡
ğ‘–, whereğ‘‡is the number of visit records of
theğ‘–-th patient, and the ğ‘¡-th visit is denoted by a binary vector
xğ‘¡
ğ‘–âˆˆ{0,1}|C|. Theğ‘›-th element is 1 if it contains the code ğ‘ğ‘›.
Mortality prediction. In this paper, our predictive objective is
presented as a mortality prediction task. It is formulated as a binary
classification task to predict the survival status ğ‘¦ğ‘–âˆˆ{0,1}(i.e.,
survive or expire) of the ğ‘–-th patient given the medical visit history
Xğ‘–. Besides, the necessary notations used in the paper are listed in
Table 1.
3 Methodology
3.1 Overview
Figure 2 illustrates the general framework of ProtoMix , which
comprises the following modules:
â€¢Prototype-guided Mixup Training. As shown in Figure 2 (a),
at the end of each training epoch, ProtoMix performs clustering
based on the representations of the patients in the training set
and extracts the centroid of each cluster to form a prototype
patient cohort. In the prototype-guided mixup training process,Table 1: Notations for ProtoMix
Notation Definition
C The
list of high-dimensional medical codes
Xğ‘– The
visit sequence of the ğ‘–-th patient
Ë†ğ‘¦ğ‘–âˆˆ[0,1] Pr
edicted outcome of the ğ‘–-th patient
ğ‘¦ğ‘–âˆˆ{0,1} Gr
ound truth of the ğ‘–-th patient
yğ‘
ğ‘–âˆˆ{0,1}ğ‘ƒThe
subtype label of the ğ‘–-th patient
Ëœyğ‘The
mixed subtype label
Ëœğ‘¦ The
mixed label
ğµ Batch
size
ğ‘ T
raining set size
ğº The
number of patients (i.e., patient pairs) sampled by the
pr
ototype-based sampler
vğ‘¡
ğ‘–âˆˆRğ‘œLearne
d representation of the ğ‘¡-th visit for the ğ‘–-th patient
hğ‘–âˆˆRğ‘’Learne
d overall representation of the ğ‘–-th patient
Ëœh The
mixed representations
ğ‘…(ğ‘–) Distribution
of the inverse of the distance
b
etween the patient sample and prototypes
ğ´(ğ‘–) The
typicality score of the ğ‘–-th patient
LF(ğ‘Œ(ğ‘–)) The
label frequency of class ğ‘Œ(ğ‘–)
ğ‘ƒ(ğ‘–) The
probability of being sampled for the ğ‘–-th patient
Î¦ The
matrix of prototypes
ğ‘ƒ The
number of prototypes
ğœ™ğ‘  The
representation vector of the ğ‘ -th prototype
ğœ†ğ‘–,ğ‘— The
mixing coefficient
ğ›¼ The
re-weighting coefficient for Lpg
ğ›½ The
re-weighting coefficient for Lps
ProtoMix uses the learned prototypes to infer the subtype la-
bels (e.g., yğ‘
ğ‘–,yğ‘
ğ‘—) of patients. By interpolating the hidden repre-
sentations of the patients (e.g., hğ‘–,hğ‘—) with their subtype labels,
ProtoMix creates virtual samples, aiming to smooth the decision
boundary for patient subtypes.
â€¢Mixup Training under Prototype-guided Sampling. As shown
in Figure 2 (a), to optimize the sampling weights in different areas
of the data manifold, ProtoMix employs a Prototype-based Sam-
plerthat assigns new sampling weights to patients based on their
distance distribution to prototypes as well as label frequency.
3635KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongxin Xu et al.
Then, ProtoMix samples based on these weights and conducts
manifold mixup training among these patients (e.g., Patient k,l).
â€¢Adaptive Coefficient Computation. As shown in Figure 2
(b), During mixup training in Figure 2 (a), ProtoMix calculates
the mixup coefficient based on the relevance of the subtype to
which the patient belongs, expanding the training distribution
dynamically.
During the training process, ProtoMix integrates the loss Lpg
from the prototype-guided mixup training task with the loss Lps
from the manifold mixup training task. During inference, ProtoMix
first employs a trained EHR encoder to obtain patient representa-
tions. It then identifies patient subtypes, followed by the completion
of downstream tasks. The details are presented in the following
subsections.
3.2 EHR Backbone Encoder
Given a sparse binary visit vector xğ‘¡
ğ‘–, we first obtain a linear visit
embedding of it as follows:
vğ‘¡
ğ‘–=Wğ‘£xğ‘¡
ğ‘–+bğ‘£, (1)
where Wğ‘£âˆˆRğ‘œÃ—|C|is the learnable weight matrix, and bğ‘£âˆˆRğ‘œ
is the bias vector. Consequently, the data of the ğ‘–-th patient can
be represented by
v1
ğ‘–,v2
ğ‘–,Â·Â·Â·,vğ‘‡
ğ‘–, whereğ‘¡=1,...,ğ‘‡ is the time
step of the visits. Next, ProtoMix attempts to extract temporal
patterns and learn representations from the patientâ€™s historical visit
sequence via a backbone model (e.g., Transformer [74], etc.):
h
h1
ğ‘–,h2
ğ‘–,Â·Â·Â·,hğ‘‡
ğ‘–i
=Backboneh
v1
ğ‘–,v2
ğ‘–,Â·Â·Â·,vğ‘‡
ğ‘–i
, (2)
where hğ‘¡
ğ‘–âˆˆRğ‘’is the hidden state at the ğ‘¡-th time step. The final
hidden state hğ‘‡
ğ‘–of the last layer encompasses historical visit in-
formation and can serve as a representation for a specific patient,
frequently used in various prediction tasks. For convenience, we
drop the superscript ğ‘‡inhğ‘‡
ğ‘–to represent the overall representation
of theğ‘–-th patient in the rest of this paper.
3.3 Prototype-guided Mixup Training
To boost model capability in identifying subtypes during data aug-
mentation, we propose a prototype-guided mixup training task.
3.3.1 Prototype Learning and Subtype Label Assignment. For the
training setDtrain={(X1,ğ‘¦1),...,(Xğ‘,ğ‘¦ğ‘)}, at the end of each
training epoch, ProtoMix clusters the representations of patients
using an improved K-Means method [ 33] based on Euclidean dis-
tance. Specifically, in the process of clustering based on the K-Means
method, embeddings belonging to the same cluster (e.g., ğ‘ ) are usu-
ally aggregated using an averaging operator to compute an updated
representation of the centroid (e.g., ğœ™ğ‘ ). However, during the early
stages of training, before model convergence, the distributions of
representations and clusters are vigorously changing, causing in-
stability [ 89]. To smooth out these drastic changes, inspired by [ 30],
we propose a momentum-based centroid updating method:
ğœ™ğ‘ â†ğ‘Ÿğœ™ğ‘ +(1âˆ’ğ‘Ÿ)hğ‘–,âˆ€ğ‘–âˆˆ{ğ‘–|hğ‘–belongs to the cluster ğ‘ },(3)
whereğ‘Ÿâˆˆ[0,1)is a momentum coefficient. In the first training
epoch, we randomly initialize the centroids. In subsequent training
epochs, we randomly select ğ‘ƒsamples and the representations of
the centroids to which these ğ‘ƒsamples belonged (i.e., the nearestones) in the previous epoch are used for initialization. Through
this gradual evolution, ProtoMix captures the training dynamics
of deep neural networks and provides a robust estimate of the
centroids, thus preventing oscillation and enhancing stability.
In this paper, we consider the learned prototypes as the cen-
triods of several basic patient subtypes (i.e., patient groups with
similar disease progression pathways) [ 6,25,89]. Driven by this
hypothesis, based on the matrix of center points (i.e., prototypes)
Î¦=[ğœ™1,ğœ™2,Â·Â·Â·,ğœ™ğ‘ƒ]âˆˆRğ‘ƒÃ—ğ‘’(ğ‘ƒis the number of prototypes) ob-
tained from the previous epoch through the improved clustering
algorithm, ProtoMix assigns a pseudo subtype label yğ‘âˆˆ{0,1}ğ‘ƒto
each patient in the training set. This label indicates which prototype
the patient sample is closest to, with the corresponding index being
1. In this manner, we redefine the training set as:
Dtrain=n
X1,ğ‘¦1,yğ‘
1
,...,
Xğ‘,ğ‘¦ğ‘,yğ‘
ğ‘o
. (4)
3.3.2 Virtual Samples Generation. Recently, Manifold Mixup trains
the neural network on linear combinations of the hidden represen-
tations of training samples, enhancing feature space exploration
and making the decision boundaries smoother [ 75]. Inspired by
this, we consider combining this simple yet efficient method with
the above learned patient prototypes to better preserve the subtype
structure in the synthesized new samples. Specifically, for the cur-
rently loaded mini-batch and its randomly shuffled permutation,
we can obtain multiple pairs of samples. For each pair of samples,
such as
Xğ‘–,ğ‘¦ğ‘–,yğ‘
ğ‘–
,
Xğ‘—,ğ‘¦ğ‘—,yğ‘
ğ‘—
, and the representations of these
two samples, hğ‘–andhğ‘—,ProtoMix performs data augmentation by
constructing virtual samples with convex combinations of their
overall representations and corresponding subtype labels:
Ëœh=ğœ†ğ‘–,ğ‘—hğ‘–+ 1âˆ’ğœ†ğ‘–,ğ‘—hğ‘—,
Ëœyğ‘=ğœ†ğ‘–,ğ‘—yğ‘
ğ‘–+ 1âˆ’ğœ†ğ‘–,ğ‘—yğ‘
ğ‘—,(5)
whereğœ†ğ‘–,ğ‘—âˆˆ[0,1]is the mixing coefficient calculated based on
patient subtype correlations, which will be introduced in Section 3.5.
Unlike the original sample, which uses hard labels, the generated
virtual data uses soft labels.
3.3.3 Prediction Layer. To simulate the real-world process where
doctors first conduct a meticulous phenotyping and comprehen-
sive characterization of the patientâ€™s condition before proceeding
with further diagnosis [ 2], we design the execution of downstream
tasks to first identify patient subtypes and then complete further
predictions through a two-layer multilayer perceptron (MLP):
Ë†yğ‘
ğ‘–=Softmax Wğ‘hğ‘–+bğ‘, (6)
Ë†ğ‘¦ğ‘–=ğœ Wğ‘¦ğ›¿ Wğ‘hğ‘–+bğ‘+bğ‘¦, (7)
whereğ‘ƒis the number of prototypes, Wğ‘¦âˆˆR1Ã—ğ‘ƒ,Wğ‘âˆˆRğ‘ƒÃ—ğ‘’,
bğ‘¦âˆˆR1, and bğ‘âˆˆRğ‘ƒare learnable parameters, ğœrefers to the
sigmoid functon, ğ›¿is the ReLU function. It is worth noting that
Ë†yğ‘
ğ‘–represents the predicted probability of subtype classification,
which is obtained by applying softmax to the logits output from
the first fully-connected (FC) layer. Based on this, we construct a
prototype-guided mixup training task that teaches the model to
identify subtypes according to the patientâ€™s health status during the
data augmentation process. Ë†ğ‘¦ğ‘–represents the predicted probability
3636ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
of the model in the final downstream task, which is made based on
the potential patient subtypes through the second FC layer.
3.3.4 Mixup Training Task Formulation. After creating a virtual
sample
Ëœh,Ëœyğ‘
(consisting of the mixed representation and the
mixed subtype label) through interpolation, ProtoMix formulates
the patient subtype classification task as a multi-class classification
task, and obtains the probability Ë†yğ‘based on the mixed representa-
tion Ëœhthrough the Prediction Layer (cf., Section 3.3.3):
Ë†yğ‘=Softmax
Wğ‘Ëœh+bğ‘
. (8)
Finally, we train the EHR Backbone Encoder and the first FC layer of
thePrediction Layer using cross-entropy loss with the mixed labels:
Lpg=âˆ’1
ğµğµâˆ‘ï¸
ğ‘¢=1Ëœyğ‘
ğ‘¢logË†yğ‘
ğ‘¢, (9)
whereğµis the batch size and Ëœyğ‘
ğ‘¢is the mixed label of the ğ‘¢-th
virtual sample. As training progresses, patient representations of the
same subtype are closely clustered together. ProtoMix can further
enhance within-cluster compactness and between-cluster scatter,
strengthening the modelâ€™s ability to identify patient subtypes during
mixup.
3.4 Mixup Training under Prototype-guided
Sampling
Under an ill-posed isotropic assumption, the vanilla mixup method
assigns equal weights to both highly concentrated and less con-
centrated areas on the data manifold, especially for the typical
samples of the minority classes and the noisy samples of the ma-
jority classes [ 29], stemming from the prevalent and significant
label imbalance problem in EHR data [ 52,79], which introduces
bias in the learning process and limits the generalization perfor-
mance of the minority classes [ 1,84]. To further address this po-
tential problem, we design a prototype-based sampler, which as-
signs new sampling weights to the patient samples by simultane-
ously considering label frequency and sample typicality. Specifi-
cally, we calculate the typicality scores based on the distribution
ğ‘…(ğ‘–)=Softmax 
1/ğ‘‘ğ‘–,1,1/ğ‘‘ğ‘–,2,Â·Â·Â·,1/ğ‘‘ğ‘–,ğ‘ƒâˆˆRğ‘ƒof the inverse of
the distances (e.g., Euclidean distance) between the patient samples
and prototypes, where ğ‘‘ğ‘–,ğ‘ denotes the distance of the ğ‘–-th patient
from theğ‘ -th prototype. We argue that the closer a sample is to
a certain prototype, i.e., the sparser and sharper ğ‘…(ğ‘–)is, the more
typical that sample is. The Gini index is the most widely used metric
for measuring impurity [ 69]. Inspired by the decision tree that uses
the Gini index as a split criteria [ 61], we calculate the typicality
score of a sample based on the Gini index:
ğ´(ğ‘–)=1
1âˆ’Ãğ‘ƒ
ğ‘ =1ğ‘…(ğ‘–)[ğ‘ ]2, (10)
whereğ‘…(ğ‘–)[ğ‘ ]is theğ‘ -th element of ğ‘…(ğ‘–). The sparser the distribu-
tionğ‘…(ğ‘–)is, the higher the typicality score ğ´(ğ‘–). For theğ‘–-th patient,
his/her probability of being sampled is defined as follows:
ğ‘ƒ(ğ‘–)âˆğ´(ğ‘–)
LF(ğ‘Œ(ğ‘–)), (11)where LF(ğ‘Œ(ğ‘–))denotes the label frequency of ground truth class
ğ‘Œ(ğ‘–)(i.e., survive or expire) and ğ´(ğ‘–)denotes the typicality score
of theğ‘–-th patient. In this manner, ProtoMix allocates probabili-
ties to each sample inversely proportional to its label frequency,
making samples from minority classes more likely to be selected.
At the same time, ProtoMix increases the probability of sampling
typical samples. Next, ProtoMix randomly draws samples from the
training set based on the newly assigned sampling weights and
performs mixup in pairs by linear interpolation in latent space, and
by reassigning the ground truth label correspondingly:
Ëœh=ğœ†ğ‘˜,ğ‘™hğ‘˜+ 1âˆ’ğœ†ğ‘˜,ğ‘™hğ‘™,
Ëœğ‘¦=ğœ†ğ‘˜,ğ‘™ğ‘¦ğ‘˜+ 1âˆ’ğœ†ğ‘˜,ğ‘™ğ‘¦ğ‘™,(12)
where hğ‘˜andhğ‘™are the overall representations of the ğ‘˜-th andğ‘™-th
samples obtained from sampling, respectively, and ğ‘¦ğ‘˜andğ‘¦ğ‘™are
their ground truth labels. ğœ†ğ‘˜,ğ‘™âˆˆ[0,1]is the mixing coefficient that
will be introduced in Section 3.5. Then, the binary cross-entropy
loss is applied as the loss function:
Lps=âˆ’1
ğºğºâˆ‘ï¸
ğ‘¢=1 Ëœğ‘¦âŠ¤
ğ‘¢log(Ë†ğ‘¦ğ‘¢)+(1âˆ’Ëœğ‘¦ğ‘¢)âŠ¤log(1âˆ’Ë†ğ‘¦ğ‘¢), (13)
whereğºis the number of patients (i.e., patient pairs) sampled
from the training set by the prototype-based sampler, and Ë†ğ‘¦ğ‘¢âˆˆ
[0,1]is the predicted probability of the mixed representation Ëœhğ‘¢
obtained through the Prediction Layer (cf., Section 3.3.3). With the
prototype-guided mixup sampling strategy, ProtoMix enhances the
generalization performance with higher signal-to-noise ratio.
3.5 Adaptive Coefficient Computation
To avoid the under-fitting problem caused by unreasonable mix-
ing coefficients (e.g., improper beta distribution hyperparameter
choices might lead to interpolation intensities near 0 or 1, harm-
ing sample quality [ 56]), we introduce the learned correlations
adaptively [ 47,51] between patient subtypes as priors to control
the strength of interpolation throughout the mixup training pro-
cess. Specifically, given two input patient samples
Xğ‘–,ğ‘¦ğ‘–,yğ‘
ğ‘–
and

Xğ‘—,ğ‘¦ğ‘—,yğ‘
ğ‘—
, we obtain the corresponding prototype vectors ğœ™ğ‘¤
andğœ™ğ‘§based on their patient subtype labels yğ‘
ğ‘–andyğ‘
ğ‘—. For better
modeling the correlation between patient prototypes, we employ
the deep biaffine attention model [ 20] which effectively captures
the interaction between text tokens and achieves impressive results
in some natural language processing tasks:
ğœ†ğ‘–,ğ‘—=ğœ ğœ™âŠ¤
ğ‘¤Wğ‘ğœ™ğ‘§+Wğ‘[ğœ™ğ‘¤âŠ•ğœ™ğ‘§]+bğ‘, (14)
where Wğ‘âˆˆRğ‘’Ã—ğ‘’,Wğ‘âˆˆR1Ã—2ğ‘’, and bğ‘âˆˆR1are trainable param-
eters,ğœis the sigmoid function, âŠ•denotes concatenation. Based on
Eq.(14), we incorporate the interdependency between patient sub-
types to determine the coefficients in Eq. (5)and Eq. (12), thereby
reducing the learning difficulty.
3.6 Training
Notice that ProtoMix uses a standard binary cross-entropy loss
Lcls=âˆ’1
ğµÃğµ
ğ‘¢=1 ğ‘¦âŠ¤ğ‘¢log(Ë†ğ‘¦ğ‘¢)+(1âˆ’ğ‘¦ğ‘¢)âŠ¤log(1âˆ’Ë†ğ‘¦ğ‘¢)based on
the original data by default, where ğµis the batch size, Ë†ğ‘¦ğ‘¢âˆˆ[0,1]is
the predicted probability obtained based on the original data, the
3637KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongxin Xu et al.
Algorithm 1: ProtoMix(ğ‘¿)
Initialization:
Randomly initialize all model parameters and the matrix of
prototypes Î¦.
1forğ‘’ğ‘ğ‘œğ‘â„ =1â†’EPOCHs do
2 forğµin mini-batches do
3 Calculate the overall representation hğ‘–by the EHR
Backbone Encoder;
4 Obtain the patient subtype label yğ‘
ğ‘–;
5 Construct virtual samples (Ëœh,Ëœyğ‘);
6 Calculate the cross-entropy loss Lpg;
7 Calculate the distribution ğ‘…(ğ‘–);
8 Calculate the typicality score ğ´(ğ‘–)and the new
sampling weight ğ‘ƒ(ğ‘–);
9 Sampleğºpatients from the training set by the
prototype-based sampler;
10 Construct virtual samples (Ëœh,Ëœğ‘¦)from sampled
patients;
11 Calculate the binary cross-entropy loss Lps;
12 Calculate the binary cross-entropy loss Lclsbased
on the original data;
13 Calculate the total loss Lall;
14 Update parameters of ProtoMix by optimizingLall;
15 end
16 Update the matrix of prototypes Î¦with the improved
K-Means method based on momentum moving
average;
17end
Table 2: Statistics of the datasets.
Dataset MIMIC-III MIMIC-IV eICU
# of patients 7,537 42,095 27,389
# of visits 19,993 157,650 62,139
Avg. # of visits per patient 2.65 3.75 2.25
Max. # of visits per patient 42 195 17
# of unique medical codes 5,681 28,256 1,568
Avg. # of medical codes per visit 41.44 33.73 52.43
Max. # of medical codes per visit 153 157 176
EHR Backbone Encoder (cf., Section 3.2) and the Prediction Layer
(cf., Section 3.3.3), andğ‘¦ğ‘¢âˆˆ{0,1}is the ground truth. This is bene-
ficial for the network to provide more stable clustering results in
the early stages, thereby accelerating the convergence speed. Com-
bining the prototype-guided mixup training task and the manifold
mixup training task based on the prototype-guided mixup sampling
strategy, the total loss used to train the model is as follows:
Lall=ğ›¼Lpg+ğ›½Lps+Lcls, (15)
whereğ›¼andğ›½are the re-weighting coefficients, and ğ›¼+ğ›½=1. The
training procedure for ProtoMix is detailed in Algorithm 1.4 Experiment
In this section, we construct experiments to demonstrate the effec-
tiveness of ProtoMix. The code is provided in1.
4.1 Experimental Setup
4.1.1 Datasets. We conduct experiments on the following three
real-world EHR datasets with mortality prediction task for evalua-
tion. The data statistics are listed in Table 2.
â€¢MIMIC-III Dataset. We use the ICU data from the open-source
Medical Information Mart for Intensive Care (MIMIC-III) data-
base [ 38]. MIMIC-III contains deidentified health-related data
of patients between 2001 and 2012. We formulate the mortality
prediction as a binary classification problem to predict the fi-
nal outcome (i.e., survive or expire) of the patient. In this study,
we select patients with at least two visits. We use the outcome
of the last visit as labels and adopt the rest visits as features.
The MIMIC-III dataset is imbalanced, whose resulting mortality
(positive) rate is 19.34%.
â€¢MIMIC-IV Dataset. Another dataset we use is the publicly avail-
able MIMIC-IV dataset [ 37]. MIMIC-IV contains comprehensive
information of patients admitted to an ICU or the emergency
department between 2008 and 2019. Since there is an overlapped
time range between MIMIC-III and MIMIC-IV, following [ 48], we
filtered out the duplicates. The cleaned dataset consists of 42,095
patients. Then, we adopt the same pre-processing method as
used for the MIMIC-III dataset. The MIMIC-IV dataset is highly
imbalanced, with only 5.48% of samples labeled positive.
â€¢eICU Dataset. The third dataset we use is the eICU dataset [ 59].
We adopt the same cleaning method as for MIMIC-III, and the
cleaned dataset includes 27,389 patients. The eICU dataset is
imbalanced, with a resulting mortality (positive) rate of 6.38%.
4.1.2 Baselines. To evaluate the prediction performance of our
proposed model, we select several state-of-the-art methods as our
baselines:
â€¢GRU [14] is a typical recurrent neural network architecture.
â€¢Deepr [57] extracts patientsâ€™ clinical patterns via convolutional
neural networks (CNNs).
â€¢Dr.Agent [24] uses policy gradient agents in neural networks.
â€¢StageNet [25] incorporates stage-related patterns in prediction.
â€¢GRASP [89] integrates ancillary information from similar pa-
tients and considers correlations between different patient groups.
â€¢T-ContextGGAN [81] extracts temporal features and correla-
tions of clinical event variables through time-aware meta-paths.
â€¢DHCE [93] captures the fine-grained high-order relationships
between diagnoses through dynamic hypergraphs.
â€¢Manifold Mixup [75] leverages linear interpolations of hidden
representations as additional training signals.
â€¢GRACE [62] proposes an item-relation-aware GAN to capture
data pattern with two pre-training tasks.
â€¢EHRDiff [86] uses the diffusion model to generate EHR data.
â€¢TITAN [83] generates new samples by sampling from the distri-
butions of prototypes and mixing them with the original data.
1https://github.com/jiangxinke/ProtoMix
3638ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Performance comparisons on the MIMIC-III, MIMIC-IV, and eICU datasets. The number in () denotes the standard
deviation.
MethodsDataset MIMIC-III MIMIC-IV eICU
Metric AUPRC AUROC F1-Score AUPRC AUROC F1-Score AUPRC AUROC F1-Score
BaselinesGRU 0.2842(.0195) 0.6137(.0212) 0.2344(.0270) 0.1289(.0132) 0.6913(.0234) 0.1178(.0174) 0.2031(.0311) 0.7885(.0342) 0.1421(.0305)
Deepr 0.2663(.0216) 0.6270(.0123) 0.1319(.0413) 0.1527(.0252) 0.7511(.0071) 0.0687(.0231) 0.1989(.0443) 0.7889(.0492) 0.1111(.0789)
Dr.Agent 0.2827(.0280) 0.6024(.0267) 0.2356(.0253) 0.1266(.0139) 0.6997(.0128) 0.1176(.0153) 0.1922(.0320) 0.7581(.0447) 0.1631(.0493)
StageNet 0.2645(.0255) 0.5870(.0400) 0.2436(.0608) 0.1233(.0082) 0.6888(.0143) 0.1417(.0169) 0.1711(.0223) 0.7349(.0239) 0.1832(.0447)
GRASP 0.3274(.0380) 0.6092(.0215) 0.3083(.0266) 0.1364(.0042) 0.7003(.0183) 0.1333(.0210) 0.2157(.0149) 0.7968(.0133) 0.2125(.0348)
T-ContextGGAN 0.3113(.0204) 0.6044(.0145) 0.3044(.0354) 0.1609(.0213) 0.7142(.0115) 0.1442(.0200) 0.2142(.0998) 0.7941(.0134) 0.2029(.0327)
DHCE 0.3215(.0406) 0.6197(.0630) 0.3177(.0075) 0.1611(.0078) 0.7205(.0027) 0.1433(.0053) 0.2133(.0042) 0.7907(.0066) 0.2055(.0111)
Manifold Mixup 0.3075(.0443) 0.6219(.0291) 0.2891(.0321) 0.1439(.0301) 0.7365(.0297) 0.1388(.0285) 0.2111(.0234) 0.7963(.0253) 0.1922(.0240)
GRACE 0.3241(.0157) 0.6137(.0122) 0.3127(.0248) 0.1742(.0049) 0.7667(.0120) 0.1544(.0132) 0.2269(.0244) 0.8097(.0198) 0.2165(.0187)
EHRDiff 0.2757(.0234) 0.6120(.0280) 0.2369(.0462) 0.1416(.0180) 0.7236(.0292) 0.1230(.0163) 0.2056(.0188) 0.7977(.0140) 0.1656(.0138)
TITAN 0.3259(.0154) 0.6106(.0241) 0.3099(.0296) 0.1387(.0098) 0.7146(.0140) 0.1189(.0126) 0.2118(.0187) 0.7981(.0203) 0.1984(.0214)
AblationProtoMixğ‘šâˆ’ 0.3095(.0227) 0.6228(.0195) 0.3151(.0263) 0.1635(.0122) 0.7528(.0311) 0.1304(.0169) 0.2315(.0376) 0.7937(.0242) 0.2188(.0305)
ProtoMixğ‘ âˆ’ 0.3184(.0226) 0.6339(.0237) 0.3028(.0276) 0.1759(.0259) 0.7508(.0257) 0.1345(.0248) 0.2198(.0422) 0.8011(.0397) 0.2103(.0318)
ProtoMixğœ†âˆ’0.3350(.0331) 0.6385(.0301) 0.3197(.0349) 0.1805(.0281) 0.7825(.0193) 0.1511(.0178) 0.2395(.0388) 0.8136(.0320) 0.2301(.0407)
Ours ProtoMix 0.3522(.0507) 0.6813(.0372) 0.3498(.0375) 0.1977(.0229) 0.8110(.0073) 0.1679(.0322) 0.2526(.0388) 0.8278(.0979) 0.2406(.0167)
Table 4: Comparisons of algorithm execution efficiency of
data augmentation between ProtoMix and some baselines.
Avg. Generate data and training with it Time (s) / Epoch
Methods GRACE EHRDiff ProtoMix Speedup
MIMIC-III 16.30 10.94 6.19 1.77Ã—
MIMIC-IV 88.60 72.01 16.35 4.40Ã—
eICU 56.41 39.46 13.51 2.92Ã—
Note that we do not compare with baselines incorporating ex-
ternal medical knowledge, which is not used in this study and is
different from our setting. For EHRDiff, we use it to generate data
and pre-train with the two self-supervised tasks of GRACE. Based
on the pre-trained model, we then train and evaluate. Additionally,
to ensure absolute fairness in the experimental results, we employ
Gated Recurrent Units (GRU) as the EHR encoder backbone for
both ProtoMix and the baseline models that utilize data augmenta-
tion techniques, including Manifold Mixup, GRACE, EHRDiff, and
TITAN. GRU is a basic and widely-adopted encoder, without con-
sidering additional information for refined modeling. Our method
is versatile and allows for flexible selection of EHR encoders. In
our future research, we will explore the design of more sophisti-
cated encoders to further enhance performance. We also compare
ProtoMix with three reduced variants to conduct ablation studies:
â€¢ProtoMixğ‘šâˆ’removes the Prototype-guided Mixup Training mod-
ule (i.e., the lossLpgin Eq. (9)) from ProtoMix.
â€¢ProtoMixğ‘ âˆ’removes the Mixup Training under Prototype-guided
Sampling module from ProtoMix and performs vanilla manifold
mixup to calculate Lps.
â€¢ProtoMixğœ†âˆ’updates the calculation of the mixing coefficient
by replacing the Adaptive Coefficient Computation module with
random sampling from the beta distribution.
4.1.3 Evaluation Metrics and Strategy. We assess the performance
with three widely used evaluation metrics: the area under theprecision-recall curve (AUPRC), the area under the receiver op-
erating characteristic curve (AUROC), and F1-score. AUPRC serves
as the most informative and the primary evaluation metric when
dealing with highly imbalanced and skewed datasets like ours [ 52].
These datasets are randomly divided into the training, validation,
and testing sets with a proportion of 8:1:1. Each model is trained 5
times with different parameter initializations. We report the aver-
age performance and the standard deviation for both baselines and
our method. More implementation details are listed in Appendix B.
Additionally, we also conduct experimental analyses on different
amounts of augmented data, which can be found in Appendix D.
4.2 Experimental Results
The experimental outcomes of both the baseline methods and
ProtoMix on the three datasets are showcased in Table 3. The num-
ber in () denotes the standard deviation. We observe that ProtoMix
shows outstanding performance and achieves the state-of-the-art
scores on almost all metrics, especially AUPRC, which is the most in-
formative and the primary evaluation metric. We divide the baseline
models into two groups. The first group consists of models without
data augmentation techniques, while the second group includes
models that employ data augmentation techniques. Comparing the
performance of the first group of models, we find that GRASP ex-
hibits excellent performance, which validates the effectiveness of
identifying patient subtypes. The second group of models demon-
strates strong competitiveness, underlining the significance of data
augmentation techniques. ProtoMix outperforms Manifold Mixup,
GRACE, EHRDiff, and TITAN on these datasets with a large margin
in all cases. It proves the effectiveness of capturing and preserving
subtype structure in the generated samples. Meanwhile, as shown
in Table 4, ProtoMix significantly reduces the time cost of data aug-
mentation compared to GAN- or Diffusion Model-based methods
under identical experimental settings, proving that ProtoMix can
perform better effectiveness and efficiency trade-off.
We further conduct the ablation studies to investigate the design
elements of ProtoMix . The performance of ProtoMixğ‘šâˆ’drops,
which demonstrates the effectiveness of smoothing the decision
3639KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongxin Xu et al.
(a) GRACE
 (b) Manifold Mixup
 (c)ProtoMix
Figure 3: Visualization of patient representations learned by GRACE, Manifold Mixup, and ProtoMix on the MIMIC-III dataset
with t-SNE. Different colors denote the different ground-truth class labels. Purple: positive samples (expired samples) in the
testing set; Green: negative samples (survived samples) in the testing set.
10 15 30 50 80
Training Data Size (%)0.100.150.200.250.300.35AUPRC
MIMIC-III
GRU
GRASP
T-ContextGRACE
ProtoMix
10 15 30 50 80
Training Data Size (%)0.020.060.100.140.18AUPRC
MIMIC-IV
GRU
GRASP
T-ContextGRACE
ProtoMix
Figure 4: Performance comparisons of ProtoMix and several
baselines with different amounts of training data.
boundaries of patient subtypes. Moreover, the superior performance
ofProtoMix than the ProtoMixğ‘ âˆ’confirms the benefits of optimiz-
ing sampling weights across the data manifold. Finally, ProtoMix
outperforms ProtoMixğœ†âˆ’, demonstrating that ProtoMix can learn
the appropriate mixing coefficients according to the scenario based
on the learned correlations among patient subtypes.
4.3 Analysis
4.3.1 Robustness Against Data Insufficiency. To investigate the im-
pact of data sufficiency on model performance, we conduct the
following experiments on the MIMIC-III and MIMIC-IV datasets
respectively. Specifically, we simulate the scenario of label scarcity
by reducing the sample size of the training set from 80% to 50%,
30%, 15%, and 10%, while the testing set remains constant. We
conduct experiments under these conditions, and the AUPRC is
plotted in Figure 4. As illustrated in Figure 4, ProtoMix consistently
surpasses the performance of all chosen baselines across various
settings. Intriguingly, with a diminishing size of the training set, the
efficacy of other methodologies deteriorates at a swifter pace than
ProtoMix . Even when we train with only 10% of the data, ProtoMix
still achieves an AUPRC of 0.2688 and 0.1309 on the MIMIC-III and
MIMIC-IV datasets, respectively, which is comparable to or even
surpasses the performance of most baselines using more training
data. These results suggest that even in situations with insufficient
labeled data, ProtoMix can still learn reasonable representations,
improving prediction outcomes and exhibiting stronger robustness.4.3.2 Visualization of Learned Representations. In this section, we
use t-SNE [ 36,73] to plot the representations learned by ProtoMix
on MIMIC-III in a 2-D space, providing an intuitive understanding
ofProtoMix â€™s ability to identify patient subtypes. For comparison,
we also show the t-SNE plots on the strongest results of GRACE and
Manifold Mixup. The visualization results can be found in Figure 3.
Each point represents a patient from the testing set, and the color
of the point indicates the patientâ€™s final outcome (i.e., survive or
expire). Specifically, purple points represent positive samples, while
green points represent negative samples. As shown in Figure 3, in
the representation space learned by GRACE and Manifold Mixup,
the distribution of patients lacks discernible patterns, with signif-
icant overlap between different outcomes, posing challenges for
classification. In contrast, the representations learned by ProtoMix
form tighter clusters with clearer boundaries, with more distance
between clusters and closer distance between nodes within clus-
ters. This demonstrates that patients with different outcomes are
well separated based on effective identification of patient subtypes.
According to [ 3,58,67], well separation in the representation space
leads to better generalization bounds.
4.3.3 Patient subtype analysis. In this experiment, we perform
patient subtyping on the three datasets to investigate the expres-
sive power of patient representations learned by ProtoMix . As we
mentioned before, identifying patient subtypes can help clinicians
develop targeted treatment plans and prevent adverse outcomes [ 6].
Concretely, we use the representations before the output layer to
cluster the patients by K-Means algorithm [ 33]. During this process,
we determine the hyperparameter value for the number of clusters
(i.e., the number of prototypes) through evaluation on the validation
set, utilizing the Calinski-Harabasz score (C-H score). C-H score is
an important criterion for assessing the optimal number of clusters
and quantitatively evaluating subtyping performance [ 10,25,32,49].
The details of calculating the C-H score are described in Appen-
dixC. A higher C-H score relates to a better method. In assessing
subtyping performance with the C-H score, we conducted a thor-
ough grid search on the number of prototypes for both baseline
methods and ProtoMix . The results indicate that when this hyper-
parameter is set to 6 (for MIMIC-III and MIMIC-IV) and 10 (for
eICU), the C-H score reaches the highest value for all the methods,
butProtoMix still works best as shown in Table 5, demonstrating
3640ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 5: Calinski-Harabasz scores on the MIMIC-III, MIMIC-
IV, and eICU datasets.
Method MIMIC-III MIMIC-IV eICU
GRU 20.46 225.72 96.85
GRASP 138.03 635.10 187.92
Manifold Mixup 53.55 405.78 468.39
GRACE 47.81 436.08 795.22
EHRDiff 20.23 400.11 281.72
ProtoMix 233.58 981.88 969.23
Table 6: Performance comparisons of ProtoMix equipped
with different clustering algorithms on the MIMIC-III,
MIMIC-IV, and eICU datasets.
Dataset MIMIC-III MIMIC-IV eICU
Methods AUPRC AUROC AUPRC AUROC AUPRC AUROC
ProtoMixğ‘£ğ‘˜0.3453 0.6629 0.1867 0.7942 0.2501 0.8156
ProtoMix 0.3522 0.6813 0.1977 0.8110 0.2526 0.8278
that ProtoMix learns more distinguishable representations that
enhance within-cluster compactness and between-cluster scatter.
4.3.4 Analysis For different Clustering Algorithms. We also evalu-
ate the efficacy of our proposed improved K-Means method with
momentum-based centroid updating in enhancing learning effec-
tiveness and stability. We compare it with ProtoMix equipped with
the vanilla K-Means method (denoted as ProtoMixğ‘£ğ‘˜). The experi-
mental results are presented in Table 6. We can observe that our
proposed improved K-Means method consistently outperforms the
vanilla K-Means across all datasets. This indicates that through the
momentum-based centroid updating method, ProtoMix can achieve
more robust estimation of centroids, thereby further enhancing the
effectiveness and stability of learning.
5 Related Work
Deep Learning on Modeling Longitudinal EHR Data. With the
widespread utilization of EHR data, numerous deep learning models
have been proposed to model temporal patient visits and conduct
clinical predictions. Given the longitudinal nature of EHR data, one
mainstream approach involves employing sequence-based deep
learning models. For example, RETAIN [ 16] captures influential
visits and critical diagnoses through a two-level attention process.
T-LSTM [ 6], Timeline [ 4], HiTANet [ 50], StageNet [ 25], and Con-
Care [ 52] incorporate temporal information into sequence model-
ing. Deepr [ 57] models local patterns using one-dimensional CNNs.
GRASP [ 89] enhances the representation learning by extracting
knowledge from patients with similar conditions. T-ContextGGAN [ 81]
and DHCE [ 93] employ graph neural networks to derive the cor-
relations of clinical event variables. To boost representation learn-
ing in sparse data, another mainstream method incorporates ex-
ternal medical knowledge. For example, methods represented by
GRAM [ 15] impose attention mechanisms on medical ontology to
learn robust medical concept representations. Methods represented
by MedPath [ 85] utilize the personalized knowledge graph to as-
sist prediction. However, these methods are strongly dependenton precise task-specific knowledge, which is time-consuming to
obtain and not always available. In response to the aforementioned
issue, there is a category of methods that utilizes GANs or diffu-
sion models to generate EHR data, tackling labeled data scarcity
through data augmentation. EHRGAN [ 12] uses GAN to produce
realistic samples and bolsters representations via semi-supervised
learning. MedGAN [ 17] combines autoencoders to capture discrete
feature distributions. GRACE [ 62] generates higher quality EHR
data through an item-relation-aware GAN architecture and designs
two pre-training tasks to fully exploit the information in the gener-
ated data. EHRDiff [ 86] uses the diffusion model to generate EHR
data. Nevertheless, they face challenges with intricate training pro-
cesses, and the newly generated samples fail to retain the subtype
structure in EHR data.
Mixup Methods. Training with Mixup [ 90] and its variants has
become a simple and popular data augmentation method in com-
puter vision. Mixup improves the robustness of neural networks
by creating and training on convex combinations of training sam-
ples and labels, preventing overconfident predictions and over-
fitting [ 11,53]. Manifold Mixup [ 75] extends Mixup to the hidden
space of neural networks, obtaining intermediate representations by
linear interpolation of the hidden states. CutMix [ 87] introduces an
image patch-based mixing strategy. AttentiveMix [ 76] and Salien-
cyMix [ 72] guide mixup using salient regions in the images. Puzzle
Mix [ 42] and Co-Mixup [ 41] obtain appropriate masks through
solving optimization problems. Guided Mixup [ 39] smoothly in-
terpolates between two images using a greedy pairing algorithm.
TITAN [ 83] enhances the representation of original samples by sam-
pling virtual samples from the distributions of different prototypes.
However, they marginalized the study of identifying patient sub-
types, selecting appropriate sample pairs and mixing coefficients
when dealing with under-labeled EHR data.
6 Conclusions and Future Works
In this work, we propose a novel prototype-based mixup method
named ProtoMix , which leverages prior knowledge about the dataâ€™s
intrinsic features from learned prototypes to guide the entire Mixup
process for the labeled data scarcity issue in EHR-based data analy-
sis. Specifically, ProtoMix enhances the modelâ€™s ability to identify
patient subtypes through a prototype-guided mixup training task.
Then, ProtoMix adjusts the sampling weights across various areas
of the data manifold by a prototype-guided mixup sampling method.
It also incorporates learned correlations among patient subtypes
throughout the training process to dynamically expand the training
distribution. Experimental results on three real-world EHR datasets
demonstrate that ProtoMix consistently outperforms the state-of-
the-art baselines. Besides, the extracted findings are consistent with
medical literature and clinical experiences. In the future, we will
explore mixup methods at the input level and consider other data
types and modalities in EHR, such as lab test values, images, etc.
Acknowledgments
This work is supported by the National Key R&D Program of China
(2021ZD0110303), and the National Natural Science Foundation of
China (No.U23A20468).
3641KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongxin Xu et al.
References
[1]Hisham Al Majzoub, Islam Elgedawy, Ã–ykÃ¼ AkaydÄ±n, and Mehtap KÃ¶se UlukÃ¶k.
2020. HCAB-SMOTE: A hybrid clustered affinitive borderline SMOTE approach
for imbalanced data binary classification. Arab J Sci Eng 45, 4 (2020), 3205â€“3222.
[2]Pierre Amarenco, J Bogousslavsky, LR Caplan, GA Donnan, and MG Hennerici.
2009. Classification of stroke subtypes. Cerebrovasc. Dis. 27, 5 (2009), 493â€“501.
[3]Amiran Ambroladze, Emilio Parrado-HernÃ¡ndez, and John Shawe-Taylor. 2006.
Tighter pac-bayes bounds. In NeurIPS, Vol. 19.
[4]Tian Bai, Shanshan Zhang, Brian L Egleston, and Slobodan Vucetic. 2018. Inter-
pretable representation learning for healthcare via capturing disease progression
through time. In SIGKDD. 43â€“51.
[5]Mrinal Kanti Baowaly, Chia-Ching Lin, Chao-Lin Liu, and Kuan-Ta Chen. 2019.
Synthesizing electronic health records using improved generative adversarial
networks. J AM MED INFORM ASSN 26, 3 (2019), 228â€“241.
[6]Inci M Baytas, Cao Xiao, Xi Zhang, Fei Wang, Anil K Jain, and Jiayu Zhou. 2017.
Patient subtyping via time-aware LSTM networks. In SIGKDD. 65â€“74.
[7]Richard L Bishop and Richard J Crittenden. 2011. Geometry of manifolds: Geometry
of Manifolds. Academic press.
[8]Mathias Carl Blom, Awais Ashfaq, Anita Santâ€™Anna, Philip D Anderson, and
Markus Lingman. 2019. Training machine learning models to predict 30-day
mortality in patients discharged from the emergency department: a retrospective,
population-based registry study. BMJ open 9, 8 (2019), e028015.
[9]Xiongcai Cai, Oscar Perez-Concha, Enrico Coiera, Fernando Martin-Sanchez,
Richard Day, David Roffe, and Blanca Gallego. 2016. Real-time prediction of
mortality, readmission, and length of stay using electronic health record data.
JAMIA 23, 3 (2016), 553â€“561.
[10] Tadeusz CaliÅ„ski and Jerzy Harabasz. 1974. A dendrite method for cluster analysis.
COMMUN STAT-THEOR M 3, 1 (1974), 1â€“27.
[11] Luigi Carratino, Moustapha CissÃ©, Rodolphe Jenatton, and Jean-Philippe Vert.
2022. On mixup regularization. JMLR 23, 1 (2022), 14632â€“14662.
[12] Zhengping Che, Yu Cheng, Shuangfei Zhai, Zhaonan Sun, and Yan Liu. 2017.
Boosting deep learning risk prediction with generative adversarial networks for
electronic health records. In ICDM. IEEE, 787â€“792.
[13] Zhengping Che, David Kale, Wenzhe Li, Mohammad Taha Bahadori, and Yan Liu.
2015. Deep computational phenotyping. In Proceedings of the 21th ACM SIGKDD
international conference on knowledge discovery and data mining. 507â€“516.
[14] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
InEMNLP.
[15] Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and Jimeng
Sun. 2017. GRAM: graph-based attention model for healthcare representation
learning. In SIGKDD. 787â€“795.
[16] Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy
Schuetz, and Walter Stewart. 2016. Retain: An interpretable predictive model for
healthcare using reverse time attention mechanism. In NeurIPS, Vol. 29.
[17] Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart,
and Jimeng Sun. 2017. Generating multi-label discrete patient records using
generative adversarial networks. In MLHC. PMLR, 286â€“305.
[18] Martin R Cowie, Juuso I Blomster, Lesley H Curtis, Sylvie Duclaux, Ian Ford, Fleur
Fritz, Samantha Goldman, Salim Janmohamed, JÃ¶rg Kreuzer, Mark Leenay, et al .
2017. Electronic health records to facilitate clinical research. Clin Res Cardiol
106 (2017), 1â€“9.
[19] Terrance DeVries and Graham W. Taylor. 2017. Improved Regularization of
Convolutional Neural Networks with Cutout. arXiv:1708.04552
[20] Timothy Dozat and Christopher D Manning. 2016. Deep biaffine attention for
neural dependency parsing. arXiv preprint arXiv:1611.01734 (2016).
[21] Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-Ming Wu. 2023. Towards
LLM-driven Dialogue State Tracking. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing. 739â€“755.
[22] Yujie Feng, Jiangtao Wang, Yasha Wang, and Xu Chu. 2022. Spatial-attention
and demographic-augmented generative adversarial imputation network for
population health data reconstruction. IEEE Transactions on Big Data (2022).
[23] Yujie Feng, Jiangtao Wang, Yasha Wang, and Sumi Helal. 2021. Completing
missing prevalence rates for multiple chronic diseases by jointly leveraging both
intra-and inter-disease population health data correlations. In Proceedings of the
Web Conference 2021. 183â€“193.
[24] Junyi Gao, Cao Xiao, Lucas M Glass, and Jimeng Sun. 2020. Dr. Agent: Clinical
predictive model via mimicked second opinions. JAMIA 27, 7 (2020), 1084â€“1091.
[25] Junyi Gao, Cao Xiao, Yasha Wang, Wen Tang, Lucas M Glass, and Jimeng Sun.
2020. Stagenet: Stage-aware neural networks for health risk prediction. In WWW.
530â€“540.
[26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In NeurIPS, Vol. 27.
[27] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration
of modern neural networks. In ICML. PMLR, 1321â€“1330.[28] Hongyu Guo, Yongyi Mao, and Richong Zhang. 2019. Mixup as locally linear
out-of-manifold regularization. In AAAI, Vol. 33. 3714â€“3722.
[29] Haibo He and Yunqian Ma. 2013. Imbalanced learning: foundations, algorithms,
and applications. John Wiley & Sons (2013).
[30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-
mentum contrast for unsupervised visual representation learning. In CVPR. 9729â€“
9738.
[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. In NeurIPS, Vol. 33. 6840â€“6851.
[32] Anna Inguanzo, Konstantinos Poulakis, Rosaleena Mohanty, Christopher G
Schwarz, Scott A Przybelski, Patricia Diaz-Galvan, Val J Lowe, Bradley F Boeve,
Afina W Lemstra, Marleen van de Beek, et al .2023. MRI data-driven clustering
reveals different subtypes of Dementia with Lewy bodies. npj Parkinsonâ€™s Disease
9, 1 (2023), 5.
[33] Anil K Jain. 2010. Data clustering: 50 years beyond K-means. Pattern Recognit
Lett31, 8 (2010), 651â€“666.
[34] Xinke Jiang, Zidi Qin, Jiarong Xu, and Xiang Ao. 2024. Incomplete Graph Learning
via Attribute-Structure Decoupled Variational Auto-Encoder. In WSDM.
[35] Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang,
Jinyi Tang, Hongxin Ding, Xu Chu, Junfeng Zhao, and Yasha Wang. 2024. HyKGE:
A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable
Medical LLMs Responses.
[36] Xinke Jiang, Dingyi Zhuang, Xianghui Zhang, Hao Chen, Jiayuan Luo, and
Xiaowei Gao. 2023. Uncertainty Quantification via Spatial-Temporal Tweedie
Model for Zero-inflated and Long-tail Travel Demand Prediction. In CIKM.
[37] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout,
Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al .
2023. MIMIC-IV, a freely accessible electronic health record dataset. Sci. Data 10,
1 (2023), 1.
[38] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng,
Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and
Roger G Mark. 2016. MIMIC-III, a freely accessible critical care database. Sci.
Data 3, 1 (2016), 1â€“9.
[39] Minsoo Kang and Suhyun Kim. 2023. GuidedMixup: An Efficient Mixup Strategy
Guided by Saliency Maps. In AAAI, Vol. 37. 1096â€“1104.
[40] Aparajita Khan and Pradipta Maji. 2021. Multi-manifold optimization for multi-
view subspace clustering. IEEE Trans Neural Netw Learn Syst 33 (2021), 3895â€“3907.
[41] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. 2021. Co-mixup:
Saliency guided joint mixup with supermodular diversity. In ICLR.
[42] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. 2020. Puzzle mix: Exploiting
saliency and local statistics for optimal mixup. In ICML. PMLR, 5275â€“5285.
[43] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-
mization. In ICLR.
[44] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. 2017. On conver-
gence and stability of gans. arXiv:1705.07215
[45] Rajiv Kohli and Sharon Swee-Lin Tan. 2016. Electronic health records. Mis
Quarterly 40, 3 (2016), 553â€“574.
[46] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. In NeurIPS, Vol. 25.
[47] Rongfan Li, Ting Zhong, Xinke Jiang, Goce Trajcevski, Jin Wu, and Fan Zhou. 2022.
Mining Spatio-Temporal Relations via Self-Paced Graph Contrastive Learning.
[48] Chang Lu, Tian Han, and Yue Ning. 2022. Context-aware health event prediction
via transition functions on dynamic disease graphs. In AAAI, Vol. 36. 4567â€“4574.
[49] Szymon Åukasik, Piotr A Kowalski, MaÅ‚gorzata Charytanowicz, and Piotr Kulczy-
cki. 2016. Clustering using flower pollination algorithm and Calinski-Harabasz
index. In CEC. IEEE, 2724â€“2728.
[50] Junyu Luo, Muchao Ye, Cao Xiao, and Fenglong Ma. 2020. Hitanet: Hierarchical
time-aware attention networks for risk prediction on electronic health records.
InSIGKDD. 647â€“656.
[51] Jiayuan Luo, Wentao Zhang, Yuchen Fang, Xiaowei Gao, Dingyi Zhuang, Hao
Chen, and Xinke Jiang. 2024. Time Series Supplier Allocation via Deep Black-
Litterman Model. arXiv:2401.17350 [cs.LG]
[52] Liantao Ma, Chaohe Zhang, Yasha Wang, Wenjie Ruan, Jiangtao Wang, Wen Tang,
Xinyu Ma, Xin Gao, and Junyi Gao. 2020. Concare: Personalized clinical feature
embedding via capturing the healthcare context. In AAAI, Vol. 34. 833â€“840.
[53] Xinyu Ma, Xu Chu, Yasha Wang, Yang Lin, Junfeng Zhao, Liantao Ma, and
Wenwu Zhu. 2023. Fused Gromov-Wasserstein Graph Mixup for Graph-level
Classifications. In Advances in Neural Information Processing Systems, A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36.
Curran Associates, Inc., 15252â€“15276. https://proceedings.neurips.cc/paper_files/
paper/2023/file/3173c427cb4ed2d5eaab029c17f221ae-Paper-Conference.pdf
[54] Xinyu Ma, Xu Chu, Zhibang Yang, Yang Lin, Xin Gao, and Junfeng Zhao. 2024.
Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. arXiv
preprint arXiv:2404.04316 (2024).
[55] Xinyu Ma, Yasha Wang, Xu Chu, Liantao Ma, Wen Tang, Junfeng Zhao, Ye
Yuan, and Guoren Wang. 2023. Patient Health Representation Learning via
Correlational Sparse Prior of Medical Features. IEEE Transactions on Knowledge
and Data Engineering 35, 11 (2023), 11769â€“11783. https://doi.org/10.1109/TKDE.
2022.3230454
3642ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[56] Zhijun Mai, Guosheng Hu, Dexiong Chen, Fumin Shen, and Heng Tao Shen. 2021.
Metamixup: Learning adaptive interpolation policy of mixup with metalearning.
IEEE transactions on neural networks and learning systems 33, 7 (2021), 3050â€“3064.
[57] Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, and Svetha Venkatesh.
2017. Deepr: a convolutional net for medical records (2016). JBHI (2017).
[58] Emilio Parrado-HernÃ¡ndez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang
Sun. 2012. PAC-Bayes bounds with data dependent priors. JMLR 13, 1 (2012),
3507â€“3531.
[59] Tom J Pollard, Alistair EW Johnson, Jesse D Raffa, Leo A Celi, Roger G Mark,
and Omar Badawi. 2018. The eICU Collaborative Research Database, a freely
available multi-center database for critical care research. Sci. Data 5, 1 (2018),
1â€“13.
[60] Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang.
2023. Class-balancing diffusion models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 18434â€“18443.
[61] Laura Elena Raileanu and Kilian Stoffel. 2004. Theoretical comparison between
the gini index and information gain criteria. Ann. Math. Artif 41 (2004), 77â€“93.
[62] Houxing Ren, Jingyuan Wang, and Wayne Xin Zhao. 2022. Generative Adversar-
ial Networks Enhanced Pre-training for Insufficient Electronic Health Records
Modeling. In SIGKDD. 3810â€“3818.
[63] Houxing Ren, Jingyuan Wang, and Wayne Xin Zhao. 2022. Rsd: a reinforced
Siamese network with domain knowledge for early diagnosis. In Proceedings of
the 31st ACM International Conference on Information & Knowledge Management.
1675â€“1684.
[64] Houxing Ren, Jingyuan Wang, Wayne Xin Zhao, and Ning Wu. 2021. Rapt: Pre-
training of time-aware transformer for learning robust healthcare representation.
InProceedings of the 27th ACM SIGKDD conference on knowledge discovery & data
mining. 3503â€“3511.
[65] Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction
by locally linear embedding. science 290, 5500 (2000), 2323â€“2326.
[66] Clark D Russell and J Kenneth Baillie. 2017. Treatable traits and therapeutic
targets: goals for systems biology in infectious disease. Current opinion in systems
biology 2 (2017), 140â€“146.
[67] Yevgeny Seldin and Naftali Tishby. 2009. PAC-Bayesian generalization bound
for density estimation with application to co-clustering. In Artificial Intelligence
and Statistics. PMLR, 472â€“479.
[68] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. JMLR 15, 1 (2014), 1929â€“1958.
[69] S Sivagama Sundhari. 2011. A knowledge discovery using decision tree by Gini
coefficient. In ICBEIA. IEEE, 232â€“235.
[70] Yanchao Tan, Carl Yang, Xiangyu Wei, Chaochao Chen, Weiming Liu, Longfei Li,
Jun Zhou, and Xiaolin Zheng. 2022. Metacare++: Meta-learning with hierarchical
subtyping for cold-start diagnosis prediction in healthcare data. In SIGIR. 449â€“
459.
[71] Thomas Tanay and Lewis Griffin. 2016. A boundary tilting persepective on the
phenomenon of adversarial examples. arXiv preprint arXiv:1608.07690 (2016).
[72] AFM Uddin, Mst Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae,
et al.2021. Saliencymix: A saliency guided data augmentation strategy for better
regularization. In ICLR.
[73] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
JMLR 9, 11 (2008).
[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS, Vol. 30.
[75] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,
David Lopez-Paz, and Yoshua Bengio. 2019. Manifold mixup: Better representa-
tions by interpolating hidden states. In ICML. PMLR, 6438â€“6447.[76] Devesh Walawalkar, Zhiqiang Shen, Zechun Liu, and Marios Savvides. 2020.
Attentive cutmix: An enhanced data augmentation approach for deep learning
based image classification. In ICASSP.
[77] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. 2013. Regu-
larization of neural networks using dropconnect. In ICML. PMLR, 1058â€“1066.
[78] Jiaqi Wang, Junyu Luo, Muchao Ye, Xiaochen Wang, Yuan Zhong, Aofei Chang,
Guanjie Huang, Ziyi Yin, Cao Xiao, Jimeng Sun, et al .2024. Recent Ad-
vances in Predictive Modeling with Electronic Health Records. arXiv preprint
arXiv:2402.01077 (2024).
[79] Yongxin Xu, Xu Chu, Kai Yang, Zhiyuan Wang, Peinie Zou, Hongxin Ding,
Junfeng Zhao, Yasha Wang, and Bing Xie. 2023. SeqCare: Sequential Training
with External Medical Knowledge Graph for Diagnosis Prediction in Healthcare
Data. In WWW. 2819â€“2830.
[80] Yongxin Xu, Kai Yang, Chaohe Zhang, Peinie Zou, Zhiyuan Wang, Hongxin Ding,
Junfeng Zhao, Yasha Wang, and Bing Xie. 2023. VecoCare: visit sequences-clinical
notes joint learning for diagnosis prediction in healthcare data. In Proceedings of
the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23.
4921â€“4929.
[81] Yuyang Xu, Haochao Ying, Siyi Qian, Fuzhen Zhuang, Xiao Zhang, Deqing
Wang, Jian Wu, and Hui Xiong. 2022. Time-aware context-gated graph attention
network for clinical risk prediction. TKDE (2022).
[82] Kai Yang, Yongxin Xu, Peinie Zou, Hongxin Ding, Junfeng Zhao, Yasha Wang,
and Bing Xie. 2023. KerPrint: local-global knowledge graph enhanced diagnosis
prediction for retrospective and prospective interpretations. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 37. 5357â€“5365.
[83] Xinlong Yang, Haixin Wang, Jinan Sun, Shikun Zhang, Chong Chen, Xian-Sheng
Hua, and Xiao Luo. 2023. Prototypical Mixing and Retrieval-based Refinement for
Label Noise-resistant Image Retrieval. In Proceedings of the IEEE/CVF International
Conference on Computer Vision. 11239â€“11249.
[84] Yuzhe Yang and Zhi Xu. 2020. Rethinking the value of labels for improving
class-imbalanced learning. In NeurIPS, Vol. 33. 19290â€“19301.
[85] Muchao Ye, Suhan Cui, Yaqing Wang, Junyu Luo, Cao Xiao, and Fenglong Ma.
2021. Medpath: Augmenting health risk prediction via medical knowledge paths.
InWWW. 1397â€“1409.
[86] Hongyi Yuan, Songchi Zhou, and Sheng Yu. 2023. EHRDiff: Exploring Realistic
EHR Synthesis with Diffusion Models. arXiv:2303.05656
[87] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and
Youngjoon Yoo. 2019. Cutmix: Regularization strategy to train strong classifiers
with localizable features. In ICCV. 6023â€“6032.
[88] Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang,
and Junfeng Zhao. 2022. M3care: Learning with missing modalities in multimodal
healthcare data. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2418â€“2428.
[89] Chaohe Zhang, Xin Gao, Liantao Ma, Yasha Wang, Jiangtao Wang, and Wen Tang.
2021. GRASP: generic framework for health status representation learning based
on incorporating knowledge from similar patients. In AAAI, Vol. 35. 715â€“723.
[90] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018.
mixup: Beyond empirical risk minimization. In ICLR.
[91] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou.
2021. How does mixup help with robustness and generalization?. In ICLR.
[92] Xi Zhang, Jingyuan Chou, Jian Liang, Cao Xiao, Yize Zhao, Harini Sarva, Claire
Henchcliffe, and Fei Wang. 2019. Data-driven subtyping of Parkinsonâ€™s disease
using longitudinal clinical records: a cohort study. Sci. Rep. 9, 1 (2019), 797.
[93] Xin Zhang, Xueping Peng, Hongjiao Guan, Long Zhao, Xinxiao Qiao, and Wen-
peng Lu. 2023. Fusion of Dynamic Hypergraph and Clinical Event for Sequential
Diagnosis Prediction. In IEEE International Conference on Parallel and Distributed
Systems.
3643KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongxin Xu et al.
2 4 6 8 10 20
# of Prototypes0.3200.3250.3300.3350.3400.3450.3500.3550.360AUPRC
AUPRC
AUROC
0.630.640.650.660.670.680.69
AUROC
Performance on MIMIC-III Dataset
2 4 6 8 10 20
# of Prototypes0.1600.1650.1700.1750.1800.1850.1900.1950.200AUPRC
AUPRC
AUROC
0.750.760.770.780.790.800.810.82
AUROC
Performance on MIMIC-IV Dataset
Figure 5: Performance comparisons of ProtoMix equipped
with different numbers of prototypes on the MIMIC-III (left)
and MIMIC-IV (right) datasets. AUPRC (blue, left axis) and
AUROC (red, right axis) are plotted.
20% 40% 60% 80%
Sampling Ratio0.3200.3250.3300.3350.3400.3450.3500.3550.360AUPRC
AUPRC
AUROC
0.630.640.650.660.670.680.69
AUROC
Performance on MIMIC-III Dataset
20% 40% 60% 80%
Sampling Ratio0.1700.1750.1800.1850.1900.1950.200AUPRC
AUPRC
AUROC
0.770.780.790.800.810.82
AUROC
Performance on MIMIC-IV Dataset
Figure 6: Performance comparisons of ProtoMix equipped
with different sampling ratios on the MIMIC-III (left) and
MIMIC-IV (right) datasets. AUPRC (blue, left axis) and AU-
ROC (red, right axis) are plotted.
0.9 0.95 0.99 0.999
Momentum Coefficient0.3300.3350.3400.3450.3500.3550.360AUPRC
AUPRC
AUROC
0.6500.6550.6600.6650.6700.6750.6800.6850.690
AUROC
Performance on MIMIC-III Dataset
0.9 0.95 0.99 0.999
Momentum Coefficient0.18000.18250.18500.18750.19000.19250.19500.19750.2000AUPRC
AUPRC
AUROC
0.7900.7950.8000.8050.8100.8150.820
AUROC
Performance on MIMIC-IV Dataset
Figure 7: Performance comparisons of ProtoMix equipped
with different momentum coefficients on the MIMIC-III (left)
and MIMIC-IV (right) datasets. AUPRC (blue, left axis) and
AUROC (red, right axis) are plotted.
A Effect of Hyperparameters
Here we conduct experiments under different settings of hyper-
parameters on the MIMIC-III and MIMIC-IV datasets. We focus on
three hyper-parameters, i.e., ğ‘ƒ,ğº, andğ‘Ÿ. As shown in Figure 5, Fig-
ure 6, and Figure 7, we find that the optimal number of prototypes
ğ‘ƒis 6, which can drive the model to better enhance within-cluster
compactness and between-cluster scatter. Furthermore, ğºcontrols
the number of patients (i.e., patient pairs) sampled from the training
set using our proposed prototype-guided mixup sampling strategythrough the sampling ratio. A ğºthat is too small will lead to in-
sufficient model learning, while an excessively large ğºwill likely
cause the model to overfit. Finally, we can observe that the optimal
value of the momentum coefficient ğ‘Ÿis about 0.99.
B Implementation Details
All methods are implemented using Python 3.8.13 and PyTorch
1.10.0 with CUDA 11.1 on a machine with 256GB memory and
NVIDIA GeForce RTX 3090. Furthermore, we meticulously tune
the hyper-parameters of the baselines on the validation set through
a grid-search approach to guarantee optimal performance. For all
models, we set the max training epoch as 400. Specifically for the
proposed ProtoMix , we use the Adam [ 43] optimization and set the
learning rate as 2ğ‘’âˆ’4. Besides, we set ğ‘œ=256,ğ‘’=128,ğ‘Ÿ=0.99,
ğ›¼=0.5,ğ›½=0.5, andğµ=64. For the MIMIC-III and MIMIC-IV
datasets, the number of prototypes is set to 6. For the eICU dataset,
the number of prototypes is set to 10. The number of sampled
patients (i.e., patient pairs) ğºis set to 60% of the total number of
training set samples, and we conduct sampling with replacement.
We set the layer of the EHR base encoder as 1. Finally, we employ
Dropout methods [ 68] with dropout rate=0.5 in the final prediction
layer.
C C-H Score Calculation
A higher C-H score relates to a model with better-defined clusters.
The C-H score is calculated as:
Calinski-Harabasz score =tr(ğµğ‘˜)
tr(ğ‘Šğ‘˜)ğ‘šâˆ’ğ‘˜
ğ‘˜âˆ’1, (16)
whereğ‘šis the sample size, ğ‘˜is the number of clusters, ğµğ‘˜is the
covariance matrix between clusters, ğ‘Šğ‘˜is the covariance matrix
within clusters, and tris the trace of matrix.
Table 7: Performance comparisons for different amounts
of augmented data on the MIMIC-III, MIMIC-IV, and eICU
datasets.
Dataset MIMIC-III MIMIC-IV eICU
Methods AUPRC AUROC AUPRC AUROC AUPRC AUROC
ProtoMix-20% 0.3245 0.6333 0.1689 0.7791 0.2369 0.8015
ProtoMix-40% 0.3352 0.6435 0.1782 0.7823 0.2403 0.8083
ProtoMix-60% 0.3407 0.6601 0.1798 0.7978 0.2419 0.8091
ProtoMix-80% 0.3495 0.6779 0.1857 0.8043 0.2497 0.8252
ProtoMix 0.3522 0.6813 0.1977 0.8110 0.2526 0.8278
DAnalysis for different amounts of augmented
data
We assess the impact of varying amounts of augmented data on
model performance. Specifically, we set the number of virtual sam-
ples in Eq. (9) to 20%, 40%, 60%, 80% of the original (denoted as
ProtoMix -20%, ProtoMix -40%, ProtoMix -60%, ProtoMix -80%, re-
spectively), and conduct additional experiments on three datasets in
Table 7. As illustrated in Table 7, we find that the original method of
using the entire mini-batch data for mixup training yielded the best
results. This might result from generating more unique sample com-
binations, creating diverse and representative data that enriches
information and improves network generalization.
3644