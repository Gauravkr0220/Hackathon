Efficient and Effective Implicit Dynamic Graph Neural Network
Yongjian Zhong
Department of Computer Science
University of Iowa
Iowa City, IA, USA
yongjian-zhong@uiowa.eduHieu Vu
Department of Computer Science
University of Iowa
Iowa City, IA, USA
hieu-vu@uiowa.edu
Tianbao Yang
Department of Computer Science and Engineering
Texas A&M University
College Station, TX, USA
tianbao-yang@tamu.eduBijaya Adhikari
Department of Computer Science
University of Iowa
Iowa City, IA, USA
bijaya-adhikari@uiowa.edu
ABSTRACT
Implicit graph neural networks have gained popularity in recent
years as they capture long-range dependencies while improving
predictive performance in static graphs. Despite the tussle between
performance degradation due to the oversmoothing of learned em-
beddings and long-range dependency being more pronounced in
dynamic graphs, as features are aggregated both across neighbor-
hood and time, no prior work has proposed an implicit graph neural
model in a dynamic setting.
In this paper, we present Implicit Dynamic Graph Neural Net-
work (IDGNN) a novel implicit neural network for dynamic graphs
which is the first of its kind. A key characteristic of IDGNN is that
it demonstrably is well-posed, i.e., it is theoretically guaranteed to
have a fixed-point representation. We then demonstrate that the
standard iterative algorithm often used to train implicit models is
computationally expensive in our dynamic setting as it involves
computing gradients, which themselves have to be estimated in an
iterative manner. To overcome this, we pose an equivalent bilevel
optimization problem and propose an efficient single-loop training
algorithm that avoids iterative computation by maintaining moving
averages of key components of the gradients. We conduct exten-
sive experiments on real-world datasets on both classification and
regression tasks to demonstrate the superiority of our approach
over the state-of-the-art baselines. We also demonstrate that our
bi-level optimization framework maintains the performance of the
expensive iterative algorithm while obtaining up to 1600x speed-
up.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks.
KEYWORDS
Dynamic Graphs, Implicit Graph Neural Networks, Graph Convo-
lutional Networks
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672026ACM Reference Format:
Yongjian Zhong, Hieu Vu, Tianbao Yang, and Bijaya Adhikari. 2024. Efficient
and Effective Implicit Dynamic Graph Neural Network. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3672026
1 INTRODUCTION
Graph Convolution Network (GCN) [ 12] and its subsequent vari-
ants [ 16,32] have achieved the state-of-the-art performance in pre-
dictive tasks in various applications including molecular prediction
[23], recommendation [ 17], and hyperspectral image classification
[9]. GCNs have also been extended to the dynamic setting, where
the graph changes over time. Even in the dynamic setting, GCNs
have achieved state-of-the-art results for numerous tasks including
rumor detection [31] and traffic prediction [13].
Despite numerous advantages, a major limitation of existing
GCNs is that they can only aggregate information up to ğ‘˜-hops,
whereğ‘˜is the depth of the graph convolution operation. Hence,
standard graph neural networks [ 12,32] cannot capture long-range
dependencies beyond a radius imposed by the number of convo-
lution operations used. Trivial solutions like setting ğ‘˜to a large
number fail in overcoming this issue as empirical evidence [ 15]
suggests that deepening the layers of GCN, even beyond a few
(2-4) layers, can lead to a notable decline in their performance.
This is because the stacked GCN layers gradually smooth out the
node-level features which eventually results in non-discriminative
embeddings (aka oversmoothing). This creates a dilemma where,
on the one hand, we would like to capture dependencies between
nodes that are far away in the network by stacking multiple layers
of GCN together. On the other hand, we also would like to maintain
the predictive performance by only using a few layers. To tackle
this dilemma in the static setting, Gu et al. [ 7] proposed an implicit
graph neural network (IGNN), which iterates the graph convolution
operator until the learned node representations converge to a fixed-
point representation. Since there is no a priori limitation on the
number of iterations, the fixed-point representation potentially con-
tains information from all neighbors in the graph. Evidence shows
that it is able to capture long-range dependency while maintaining
predictive performance. Following this, other recent works [ 18,22]
also have addressed the problem in the static setting.
4595
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongjian Zhong, Hieu Vu, Tianbao Yang, and Bijaya Adhikari
In the case of dynamic graphs, where graphs evolve over time,
dynamic graph neural networks aggregate information over the
current graph topology and historical graphs to learn meaningful
representations [ 31,36]. Note the architecture of the graph neural
networks within each time stamp in dynamic graph neural net-
works is similar to existing static GCNs. Hence, the information
aggregated for each node in a given time stamp is still limited to a
radius ofğ‘˜âˆ’â„ğ‘œğ‘ğ‘  within the time stamp. Increasing the depth of the
GCN operator in each time stamp to ensure long-range dependency
exacerbates the performance degradation of dynamic graph neural
networks as these models convolve both over the time stamps and
within the time stamps, hence oversmoothing the features even
faster. Therefore, capturing long-range dependency while improv-
ing (or even maintaining) the performance is a big challenge for
dynamic graph neural networks. Despite its importance, very few
prior works have studied this phenomenon in dynamic graphs: Yang
et al. [ 36] propose an L2 feature normalization process to alleviate
the smoothing of features in dynamic graphs and Wang et al. [ 33]
mitigate the oversmoothing problem by emphasizing the impor-
tance of low-order neighbors via a node-wise encoder. However,
these approaches either rescale features or forget neighborhood
information, both of which are not ideal.
To address the challenges mentioned above, we propose IDGNN,
an implicit neural network for dynamic graphs derived from the
first principles. In designing IDGNN, we encountered multiple chal-
lenges including i)uncertainty on whether fixed-point (converged)
representations exist for implicit neural models defined over dy-
namic graphs; and ii)efficiently training a model to find these
fixed-point representations. In this paper, we overcome the first
challenge by providing theoretical guarantees on the existence
of the fixed-point representations on a single dynamic graph by
leveraging a periodic model and generalizing this result to a set
of dynamic graphs. For the second challenge, we notice that the
stochastic gradient descent via implicit differentiation (often used
by other implicit models [ 7,14]) is too inefficient in our problem set-
ting. As such, we reformulate our problem as an equivalent bilevel
optimization problem and design an efficient optimization strategy.
The key contributions of the paper are as follows:
â€¢We propose a novel dynamic graph neural network IDGNN,
which ensures long-range dependency while providing theo-
retical guarantees on the existence of fixed-point representa-
tions. IDGNN is the first approach to leverage implicit graph
neural network framework for dynamic graphs.
â€¢We present a bilevel optimization formulation of our problem
and propose a novel stochastic optimization algorithm to
efficiently train our model. Our experiments show that the
proposed optimization algorithm is faster than the naive
gradient descent by up to 1600 times.
â€¢We conduct comprehensive comparisons with existing meth-
ods to demonstrate that our method captures the long-range
dependency and outperforms the state-of-the-art dynamic
graph neural models on both classification and regression
tasks.2 RELATED WORK
Dynamic Graph Representation Learning: GNN has been suc-
cessful for static graphs, leading to the development of GNN-based
algorithms for dynamic graphs [ 11]. DyGNN [ 19] comprises two
components: propagation and update, which enable information
aggregation and propagation for new interactions. EvolveGCN [ 21]
uses an RNN to update GCN parameters and capture dynamic graph
properties. Sankar et. al. [ 29] propose a Dynamic Self-Attention
Network (DySAT) with structural and temporal blocks to capture
graph information. TGN [ 26] models edge streaming to learn node
embeddings using an LSTM for event memory. TGAT [ 35] consid-
ers the time ordering of node neighbors. Gao et al. [ 5] explores the
expressiveness of temporal GNN models and introduces a time-then-
graph framework for dynamic graph learning, leveraging expressive
sequence representations like RNN and transformers.
Implicit Graph Models: The implicit models or deep equilib-
rium models define their output using fixed-point equations. [ 1].
propose an equilibrium model for sequence data based on the fixed-
point solution of an equilibrium equation. El et al. [ 4] introduce
a general implicit deep learning framework and discuss the well-
posedness of implicit models. Gu et al. [ 7] demonstrate the potential
of implicit models in graph representation learning, specifically
with their implicit model called IGNN, which leverages a few layers
of graph convolution network (GCN) to discover long-range de-
pendencies. Park et al. [ 22] introduce the equilibrium GNN-based
model with a linear transition map, and they ensure the transition
map is contracting such that the fixed point exists and is unique.
Liu et al. [ 18] propose an infinite-depth GNN that captures long-
range dependencies in the graph while avoiding iterative solvers
by deriving a closed-form solution. Chen et al. [ 2] employ the dif-
fusion equation as the equilibrium equation and solve a convex
optimization problem to find the fixed point in their model.
Implicit Models Training: Efficiently training implicit models
has always been a key challenge. Normally, the gradient of im-
plicit models is obtained by solving an equilibrium equation using
fixed-point iteration or reversing the Jacobian matrix [ 7]. However,
training these models via implicit deferential introduces additional
computational overhead. Geng et al. [ 6] propose a phantom gra-
dient to accelerate the training of implicit models based on the
damped unrolling and Neumann series. Li et al. [ 14] leverage sto-
chastic proximal gradient descent and its variance-reduced version
to accelerate the training.
3 METHODOLOGY
3.1 Preliminaries
Dynamic Graphs: We are given a set of ğ‘dynamic graphs{Gğ‘–}ğ‘
ğ‘–=1.
Each dynamic graph Gğ‘–={ğº1
ğ‘–,...,ğºğ‘¡
ğ‘–,...,ğºğ‘‡
ğ‘–}is a collection of ğ‘‡
snapshots. LetVdenote the union set of nodes that appear in
any dynamic graph and ğ‘›:=|V|be the total number of nodes.
Without loss of generality, each snapshot can be represented as
ğºğ‘¡
ğ‘–={V,Eğ‘¡
ğ‘–,ğ‘‹ğ‘¡
ğ‘–}since we can assume each graph is built on the
union setVand treat the absent nodes as isolated. Eğ‘¡
ğ‘–is the set
of edges at time ğ‘¡inG.ğ‘‹ğ‘¡
ğ‘–âˆˆRğ‘™Ã—ğ‘›represents the node attribute
matrix, where ğ‘™is the dimension of the node attributes. Let ğ´ğ‘¡
ğ‘–be
the adjacency matrix of ğºğ‘¡
ğ‘–.
4596Efficient and Effective Implicit Dynamic Graph Neural Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 1: Model overview. This figure indicates the forward
process of our model.
In this paper, we focus on node-level tasks (e.g. classification
and regression) for dynamic graphs. We consider the dataset as
{(Gğ‘–,ğ’šğ‘–)}ğ‘
ğ‘–=1, whereGğ‘–is theğ‘–-th dynamic graph, and ğ’šğ‘–âˆˆRğ‘›is
the node-level labels assigned to the nodes in the last snapshot of
dynamic graphGğ‘–.
Permutation: Here, we define a permutation function to simplify
the notation in the rest of the paper. Let ğœ(ğ‘¡):=ğ‘‡âˆ’[(ğ‘‡âˆ’ğ‘¡+1)
modğ‘‡], whereğ‘‡refers to the number of snapshots. This function
mapsğ‘¡â†’ğ‘¡âˆ’1forğ‘¡âˆˆ[2,ğ‘‡]and mapsğ‘¡â†’ğ‘‡forğ‘¡=1.
Implicit Models: In general, implicit models [ 4,6,7] have the form
ğ‘=ğ‘“(ğ‘,ğ‘‹), whereğ‘“is a function that can be parameterized by a
neural network, ğ‘‹is the data and ğ‘is the learned representation.
We can obtain the fixed-point representation via iteration ğ‘âˆ—=
limğ‘˜â†’âˆğ‘ğ‘˜+1=limğ‘˜â†’âˆğ‘“(ğ‘ğ‘˜,ğ‘‹)=ğ‘“(ğ‘âˆ—,ğ‘‹).
Thus, the key to designing an implicit model for dynamic graphs
is to provide a function ğ‘“that leads to converged representations.
3.2 Implicit Model for Dynamic Graphs
We first consider a single dynamic graph G={ğº1,...,ğºğ‘‡}withğ‘‡
snapshots and discover its well-posedness condition in this section.
We then generalize the well-posedness conclusion for a set of dy-
namic graphs later. All the proofs are provided in the Appendix.
Now, let us consider the following stacked GCN model:
ğ‘1
ğ‘˜+1=ğœ(ğ‘Š1ğ‘ğœ(1)
ğ‘˜ğ´1+ğ‘‰ğ‘‹1)
ğ‘2
ğ‘˜+1=ğœ(ğ‘Š2ğ‘ğœ(2)
ğ‘˜ğ´2+ğ‘‰ğ‘‹2)
Â·Â·Â·
ğ‘ğ‘‡
ğ‘˜+1=ğœ(ğ‘Šğ‘‡ğ‘ğœ(ğ‘‡)
ğ‘˜ğ´ğ‘‡+ğ‘‰ğ‘‹ğ‘‡) (1)
In the model presented above, the embeddings ğ‘2
ğ‘˜+1of the nodes
in the second time stamp in the (ğ‘˜+1)-th layer depend on the
embeddings ğ‘1
ğ‘˜of nodes in the first time stamp learned in the ğ‘˜-
th layer and the feature of the nodes in the second time stamp
ğ‘‹2. This design enables us to propagate information between time
stamps when stacking layers. The parameters for the ğ‘¡-th layer ofthe model are denoted as ğ‘Šğ‘¡âˆˆRğ‘‘Ã—ğ‘‘andğ‘‰âˆˆRğ‘‘Ã—ğ‘™withğ‘‰being a
shared weight across all layers. Note that the proposed model and
the corresponding theory still hold when ğ‘‰is not shared. We opt
for a shared ğ‘‰for simplicity (thorough empirical discussion on this
choice is presented in the Experiment section). Following the prin-
ciple of the implicit model [ 1,4,7], we apply our model iteratively
infinite times. If the process converges, we consider the converged
result{ğ‘1âˆ,...,ğ‘ğ‘‡âˆ}as the final embeddings. Consequently, the
final embeddings have to satisfy the system of equations in (1) and
can be considered a fixed-point solution to (1). However, at this
point, it is not clear whether the fixed-point solution always exists
for arbitrary graph G.
Well-posedness is a property that an implicit function, such as
in (1), possesses a unique fixed point solution. While Gu et al. [ 7]
demonstrated the well-posedness of a single-layer implicit GCN
on a single static graph, the question remains open for dynamic
graphs. To establish the well-posedness property for our model, we
first introduce its vectorized version as follows.
ğ‘§1
ğ‘˜+1=ğœ(ğ‘€1ğ‘§ğœ(1)
ğ‘˜+vec(ğ‘‰ğ‘‹1))
ğ‘§2
ğ‘˜+1=ğœ(ğ‘€2ğ‘§ğœ(2)
ğ‘˜+vec(ğ‘‰ğ‘‹2))
Â·Â·Â·
ğ‘§ğ‘‡
ğ‘˜+1=ğœ(ğ‘€ğ‘‡ğ‘§ğœ(ğ‘‡)
ğ‘˜+vec(ğ‘‰ğ‘‹ğ‘‡)) (2)
whereğ‘§=vec(ğ‘)is column-wise vectorization of ğ‘, andğ‘€ğ‘–=
(ğ´ğ‘–)âŠ¤âŠ—ğ‘Šğ‘–whereâŠ—is the Kronecker product. Note that Equations
(2) can also be expressed in a single matrix form. This transforma-
tion involves sequentially connecting the shared nodes between
the graphs. Thus, the formula (2) can be reformulated as follows:
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘§1
ğ‘§2
ğ‘§3
...
ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ğœÂ©Â­Â­Â­Â­Â­Â­
Â«ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0 0Â·Â·Â· 0ğ‘€1
ğ‘€20Â·Â·Â· 0 0
0ğ‘€3Â·Â·Â· 0 0
...............
0 0Â·Â·Â·ğ‘€ğ‘‡0ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘§1
ğ‘§2
ğ‘§3
...
ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»+ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°vec(ğ‘‰ğ‘‹1)
vec(ğ‘‰ğ‘‹2)
vec(ğ‘‰ğ‘‹3)
...
vec(ğ‘‰ğ‘‹ğ‘‡)ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ÂªÂ®Â®Â®Â®Â®Â®
Â¬
(3)
Here, we omit the subscript for simplicity. Equation (3) represents
a single equilibrium form of Equation (2). It can also be viewed as
the time-expanded static version [] of our original dynamic graph
G. Based on the Banach fixed-point theorem [ 27], the Equation
(3) admits a unique fixed-point if the right-hand side is a contrac-
tive mapping w.r.t. ğ‘§. Therefore, we express the well-posedness
condition for our model as follows,
Theorem 3.1. For any element-wise non-expansive function ğœ(Â·),
the coupled equilibrium equations in (2) have a unique fixed point
solution ifâˆ¥Mâˆ¥ğ‘œğ‘<1, whereMdefine as
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0Â·Â·Â· 0ğ‘€1
ğ‘€2Â·Â·Â· 0 0
............
0Â·Â·Â·ğ‘€ğ‘‡0ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
andâˆ¥Mâˆ¥ğ‘œğ‘is the operator norm of M, which is the largest absolute
eigenvalue. Furthermore, this is equivalent to âˆ¥ğ‘€ğ‘¡âˆ¥ğ‘œğ‘<1for any
ğ‘¡=1,...,ğ‘‡ .
4597KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongjian Zhong, Hieu Vu, Tianbao Yang, and Bijaya Adhikari
In order to maintain âˆ¥ğ‘€ğ‘¡âˆ¥ğ‘œğ‘<1,âˆ€ğ‘¡=1,...,ğ‘‡ , it is necessary to
ensure that the condition ğœ†pr(|ğ‘Šğ‘¡|)ğœ†pr(ğ´ğ‘¡)<1is satisfied, where
ğœ†pr(Â·)represents the Perron-Frobenius eigenvalue. However, satis-
fying the condition is challenging in general as it is hard to track
the eigenvalue as the underlying matrix changes. To overcome this
challenge, we impose a more stringent requirement on ğ‘Šwhich
is more easily enforceable by leveraging a convex projection. We
formally state this in the following theorem.
Theorem 3.2. Letğœbe an element-wise non-expansive function.
If the coupled equilibrium equations satisfy the well-posedness con-
dition, namelyâˆ¥Mğ‘¡âˆ¥ğ‘œğ‘â‰¤âˆ¥ğ‘Šğ‘¡âˆ¥ğ‘œğ‘âˆ¥ğ´ğ‘¡âˆ¥ğ‘œğ‘<1,âˆ€ğ‘¡=1,...,ğ‘‡ , then
there exists rescale coupled equilibrium equations, which satisfy the
conditionâˆ¥ğ‘Šğ‘¡âˆ¥âˆâˆ¥ğ´ğ‘¡âˆ¥ğ‘œğ‘<1,âˆ€ğ‘¡=1,...,ğ‘‡ , and the solutions of these
two equations are equivalent.
Proof. Suppose{ğ‘Šğ‘¡}satisfyâˆ¥ğ‘Šğ‘¡âˆ¥ğ‘œğ‘âˆ¥ğ´ğ‘¡âˆ¥ğ‘œğ‘<1for allğ‘¡, then
the following equation has a unique fixed point.
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘§1
ğ‘§2
ğ‘§3
...
ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ğœÂ©Â­Â­Â­Â­Â­Â­
Â«ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0 0Â·Â·Â· 0ğ‘€1
ğ‘€20Â·Â·Â· 0 0
0ğ‘€3Â·Â·Â· 0 0
...............
0 0Â·Â·Â·ğ‘€ğ‘‡0ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘§1
ğ‘§2
ğ‘§3
...
ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»+ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°vec(ğ‘‰ğ‘‹1)
vec(ğ‘‰ğ‘‹2)
vec(ğ‘‰ğ‘‹3)
...
vec(ğ‘‰ğ‘‹ğ‘‡)ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ÂªÂ®Â®Â®Â®Â®Â®
Â¬
and this condition implies âˆ¥Mâˆ¥ğ‘œğ‘â‰¤1. Based on Theorem 4.3, there
exists a set of diagonal matrices {ğ‘†ğ‘¡}such that
Ë†ğ‘Šğ‘¡=ğ‘†ğ‘¡ğ‘Šğ‘¡(ğ‘†ğ‘¡)âˆ’1,Ë†ğ‘‰=ğ‘†ğ‘¡ğ‘‰
Then the fixed-point of following equations
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Ë†ğ‘§1
Ë†ğ‘§2
Ë†ğ‘§3
...
Ë†ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ğœÂ©Â­Â­Â­Â­Â­Â­Â­
Â«ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0 0Â·Â·Â· 0 Ë†ğ‘€1
Ë†ğ‘€20Â·Â·Â· 0 0
0 Ë†ğ‘€3Â·Â·Â· 0 0
...............
0 0Â·Â·Â· Ë†ğ‘€ğ‘‡0ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Ë†ğ‘§1
Ë†ğ‘§2
Ë†ğ‘§3
...
Ë†ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»+ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°vec(Ë†ğ‘‰ğ‘‹1)
vec(Ë†ğ‘‰ğ‘‹2)
vec(Ë†ğ‘‰ğ‘‹3)
...
vec(Ë†ğ‘‰ğ‘‹ğ‘‡)ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ÂªÂ®Â®Â®Â®Â®Â®Â®
Â¬
satisfies the following relation
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Ë†ğ‘§1
...
Ë†ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£»=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°(ğ‘†1)âˆ’1Â·Â·Â· 0
.........
0Â·Â·Â· (ğ‘†ğ‘‡)âˆ’1ï£¹ï£ºï£ºï£ºï£ºï£ºï£»ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Ë†ğ‘§1
...
Ë†ğ‘§ğ‘‡ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
â–¡
As previously stated, the theorem is formulated for a single
dynamic graph. In the following Remark, we present a broader and
more general conclusion for a set of dynamic graphs.
Remark 1. Considering a set of dynamic graphs denoted as {Gğ‘–}ğ‘
ğ‘–=1,
achieving fixed-point representations for all dynamic graphs using
our model is guaranteed if the condition âˆ¥ğ‘Šğ‘¡âˆ¥âˆâˆ¥ğ´ğ‘¡
ğ‘–âˆ¥ğ‘œğ‘<1holds
for all time steps ğ‘¡=1,...,ğ‘‡ and all graph indices ğ‘–=1,...,ğ‘ .
Proof. Given a set of dynamic graphs {Gğ‘–}ğ‘
ğ‘–=1, we can construct
a single dynamic graph by merging the snapshots that are from the
same time stamp, then we obtain
Ë†G={[ğº1
ğ‘–,...,ğº1
ğ‘],...,[ğºğ‘‡
ğ‘–,...,ğºğ‘‡
ğ‘]} (4)
Let Ë†ğ´ğ‘¡denote the adjacency matrix of [ğºğ‘¡
ğ‘–,...,ğºğ‘¡
ğ‘]. By theorem
3.2, we need to ensure âˆ¥ğ‘Šğ‘¡âˆ¥âˆâˆ¥Ë†ğ´ğ‘¡âˆ¥ğ‘œğ‘<1. Since Ë†ğ´ğ‘¡containsğ‘disconnected graphs, âˆ¥Ë†ğ´ğ‘¡âˆ¥ğ‘œğ‘â‰¤maxğ‘–âˆ¥ğ´ğ‘¡
ğ‘–âˆ¥ğ‘œğ‘, which means
âˆ¥ğ‘Šğ‘¡âˆ¥âˆâˆ¥ğ´ğ‘¡
ğ‘–âˆ¥ğ‘œğ‘<1needed to be satisfied for all ğ‘–. Sinceğ‘¡is ar-
bitrary, the remark holds. â–¡
In practice, we can track the matrices with the largest operator
norms ( maxğ‘–âˆ¥ğ´ğ‘¡
ğ‘–âˆ¥ğ‘œğ‘) at each time step. Focusing on these "critical"
matrices and their corresponding constraints ensures our model
meets the required conditions.
Incorporating Downstream Tasks: Based on the established con-
ditions, we can obtain the fixed-point representation by iteratively
applying our model. Now, we want the fixed-point representations
suited for specific downstream tasks with their own optimization
objective. Let us now introduce the comprehensive objective which
incorporates both the application loss and the convergence require-
ments mentioned above. To this end, we utilize a neural network
ğ‘“ğœƒ(.), parameterized by ğœƒ, to map graph embeddings to their respec-
tive targets. LetW:={ğ‘Š1,...,ğ‘Šğ‘‡}. The comprehensive objective
can now be summarized as follows:
min
ğœƒ,W,ğ‘‰L(ğœƒ,W,ğ‘‰)=ğ‘âˆ‘ï¸
ğ‘–=1â„“(ğ‘“ğœƒ(ğ‘§ğ‘‡
ğ‘–),ğ’šğ‘–) (5)
s.t.ğ‘§1
ğ‘–=ğœ
(ğ´1
ğ‘–)âŠ¤âŠ—ğ‘Š1
ğ‘§ğœ(1)
ğ‘–+vec(ğ‘‰ğ‘‹1
ğ‘–)
Â·Â·Â·
ğ‘§ğ‘‡
ğ‘–=ğœ
(ğ´ğ‘‡
ğ‘–)âŠ¤âŠ—ğ‘Šğ‘‡
ğ‘§ğœ(ğ‘‡)
ğ‘–+vec(ğ‘‰ğ‘‹ğ‘‡
ğ‘–)
,
âˆ¥ğ‘Šğ‘¡âˆ¥âˆâ‰¤ğœ…
âˆ¥ğ´ğ‘¡
ğ‘–âˆ¥ğ‘œğ‘,âˆ€ğ‘–=1,...,ğ‘,ğ‘¡ =1,...,ğ‘‡
Whereâ„“is a loss function ( e.g. cross entropy loss, mean square
error), andğœ…is a positive number that is close to 1, which is added
for numerical stability.
To find the fixed-point representation (i.e. forward pass), we can
use fixed-point iteration or other root-finding algorithms. Backprop-
agation requires storing intermediate results, which is infeasible
since there might be hundreds of iterations in discovering the rep-
resentation. Thus, the key challenge in solving Equation (5) lies in
determining how to perform backpropagation effectively, especially
in the context of dynamic graphs.
4 TRAINING
In this section, we provide two algorithms to solve Equation (5). We
first explore the stochastic gradient descent (SGD) method where
we estimate the gradients leveraging the Implicit Function Theorem.
This approach offers several advantages, such as eliminating the
need to store intermediate results during the forward pass and en-
abling direct backpropagation through the equilibrium point. While
widely used in various techniques [ 1,7], this approach presents
certain drawbacks when applied to our specific model, particu-
larly regarding computational overhead. Subsequently, we intro-
duce an efficient training algorithm for our model, which adopts a
bilevel viewpoint of our problem. This novel approach allows us to
overcome the limitations of the vanilla SGD method, resulting in
improved computational efficiency during training.
4598Efficient and Effective Implicit Dynamic Graph Neural Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.1 SGD with Implicit Differentiation
The algorithm operates as follows: it first finds the fixed-point
embedding through iteration and then computes the gradient based
on this embedding. It then updates the weights using SGD. The
main obstacle in our way is estimating the gradient.
The parameters that need to be updated are ğ‘Š,ğ‘‰ (from GCN
layers), and ğœƒ(from the classifier). The gradient with respect to
parameterğœƒcan be obtained asğœ•L
ğœ•ğœƒ, which can be computed using
autograd functions given the fixed point. However, computing the
gradient for other parameters presents a greater challenge. Letğœ•L
ğœ•ğ‘ƒğ‘–
represent the gradient with respect to ğ‘Šğ‘–orğ‘‰ğ‘–. Then, the gradient
is computed asğœ•L
ğœ•ğ‘ƒğ‘–=Ãğ‘‡
ğ‘—=1ğœ•L
ğœ•ğ‘§ğ‘—ğœ•ğ‘§ğ‘—
ğœ•ğ‘ƒğ‘–. The computation ofğœ•L
ğœ•ğ‘§ğ‘—can be
achieved through the autograd mechanism. However, determining
ğœ•ğ‘§ğ‘—
ğœ•ğ‘ƒğ‘–is non-trivial due to the cyclic definition of ğ‘§ğ‘—.
By the definition of our model, the learned embeddings must
satisfy the following equations:
ğ‘§1âˆ’ğœ(ğ‘€1ğ‘§ğœ(1)+vec(ğ‘‰ğ‘‹1))=0
ğ‘§2âˆ’ğœ(ğ‘€2ğ‘§ğœ(2)+vec(ğ‘‰ğ‘‹2))=0
...
ğ‘§ğ‘‡âˆ’ğœ(ğ‘€ğ‘‡ğ‘§ğœ(ğ‘‡)+vec(ğ‘‰ğ‘‹ğ‘‡))=0 (6)
We apply column-wise vectorization on matrices ğ‘Šandğ‘‰respec-
tively to obtain ğ‘¤andğ‘£. We can then calculate the gradientsğœ•ğ‘§
ğœ•ğ‘¤
andğœ•ğ‘§
ğœ•ğ‘£using the implicit function theorem. Here, we will show the
details of the derivation.
Letğ‘ğ‘¡
ğ‘–ğ‘—,ğ´ğ‘¡
ğ‘–ğ‘—,ğ‘‹ğ‘¡
ğ‘–ğ‘—andğ‘Šğ‘¡
ğ‘–ğ‘—denote the element at the ğ‘–-th row and ğ‘—-
th column of ğ‘ğ‘¡,ğ´ğ‘¡,ğ‘‹ğ‘¡andğ‘Šğ‘¡, respectively. Taking the derivative
of element-wise form of Equation (6), ğ‘ğ‘¡
ğ‘–ğ‘—âˆ’ğœ(Ã
ğ‘™Ã
ğ‘›ğ‘Šğ‘¡
ğ‘–ğ‘™ğ‘ğœ(ğ‘¡)
ğ‘™ğ‘›ğ´ğ‘¡
ğ‘›ğ‘—+Ã
ğ‘™ğ‘‰ğ‘–ğ‘™ğ‘‹ğ‘¡
ğ‘™ğ‘—)=0, with respect to ğ‘Šğ‘
ğ‘ğ‘we obtain
ğœ•ğ‘ğ‘¡
ğ‘–ğ‘—
ğœ•ğ‘Šğ‘
ğ‘ğ‘âˆ’Î£ğ‘¡
ğ‘–ğ‘—Â©Â­
Â«âˆ‘ï¸
ğ‘›ğ›¿ğ‘ğ‘¡ğ›¿ğ‘ğ‘–ğ‘ğœ(ğ‘¡)
ğ‘ğ‘›ğ´ğ‘¡
ğ‘›ğ‘—+âˆ‘ï¸
ğ‘™âˆ‘ï¸
ğ‘›ğ‘Šğ‘¡
ğ‘–ğ‘™ğœ•ğ‘ğœ(ğ‘¡)
ğ‘™ğ‘›
ğœ•ğ‘Šğ‘
ğ‘ğ‘ğ´ğ‘¡
ğ‘›ğ‘—ÂªÂ®
Â¬=0
whereğ›¿ğ‘ğ‘¡is the indicator function which equals 1 only when ğ‘=ğ‘¡,
andâŠ™denotes element-wise multiplication. Let ğœâ€²(.)represent the
derivativeğœ(.), and we define
Î£ğ‘¡
ğ‘–ğ‘—:=ğœâ€²(âˆ‘ï¸
ğ‘™âˆ‘ï¸
ğ‘›ğ‘Šğ‘¡
ğ‘–ğ‘™ğ‘ğœ(ğ‘¡)
ğ‘™ğ‘›ğ´ğ‘¡
ğ‘›ğ‘—+âˆ‘ï¸
ğ‘™ğ‘‰ğ‘–ğ‘™ğ‘‹ğ‘¡
ğ‘™ğ‘—)
which means Î£ğ‘¡is a matrix and has a shape like ğ‘ğ‘¡. Letğ»ğ‘¡:=
(ğ‘ğœ(ğ‘¡)ğ´ğ‘¡)ğ‘‡, andğ»ğ‘¡Â·ğ‘denotes the ğ‘-th column of ğ»ğ‘¡. Letğ‘’ğ‘be a col-
umn vector with value 1 at ğ‘-th entry and 0 elsewhere. Enumerating
ğ‘ğ‘¡
ğ‘–ğ‘—for allğ‘–,ğ‘—, we have
ğœ•ğ‘§ğ‘¡
ğœ•ğ‘Šğ‘
ğ‘ğ‘âˆ’vec(Î£ğ‘¡)âŠ™ 
ğ›¿ğ‘ğ‘¡ğ‘’ğ‘âŠ—ğ»ğ‘¡
Â·ğ‘+ğ‘€ğ‘¡ğœ•ğ‘§ğœ(ğ‘¡)
ğœ•ğ‘Šğ‘
ğ‘ğ‘!
=0
Therefore, for any time stamp ğ‘¡, the gradient of ğ‘§ğ‘¡with respect to
theğ‘-th layer of GCN, ğ‘¤ğ‘, can be expressed as:
ğœ•ğ‘§ğ‘¡
ğœ•ğ‘¤ğ‘âˆ’Îğ‘¡âŠ™ 
ğ›¿ğ‘ğ‘¡ğ»ğ‘¡âŠ—ğ¼+ğ‘€ğ‘¡ğœ•ğ‘§ğœ(ğ‘¡)
ğœ•ğ‘¤ğ‘!
=0 (7)Where Îğ‘¡is a matrix that has identical column vectors, and each col-
umn vector is the vec(Î£ğ‘¡). Similarly, we can compute the gradient
ofğ‘ğ‘¡
ğ‘–ğ‘—w.r.t.ğ‘‰ğ‘ğ‘.
ğœ•ğ‘ğ‘¡
ğ‘–ğ‘—
ğœ•ğ‘‰ğ‘ğ‘âˆ’Î£ğ‘¡
ğ‘–ğ‘—Â©Â­
Â«ğ›¿ğ‘ğ‘–ğ‘‹ğ‘¡
ğ‘ğ‘—+ğ‘Šğ‘¡
ğ‘–ğ‘™ğœ•ğ‘ğœ(ğ‘¡)
ğ‘™ğ‘›
ğœ•ğ‘Šğ‘
ğ‘ğ‘ğ´ğ‘¡
ğ‘›ğ‘—ÂªÂ®
Â¬=0
Therefore,
ğœ•ğ‘§ğ‘¡
ğœ•ğ‘£âˆ’Îğ‘¡âŠ™ 
(ğ‘‹ğ‘¡)âŠ¤âŠ—ğ¼+ğ‘€ğ‘¡ğœ•ğ‘§ğœ(ğ‘¡)
ğœ•ğ‘£!
=0 (8)
While these equations provide a path for gradient computation,
it is essential to note that all gradients are interconnected within
a system of equations. The resolution of such a system entails a
substantial computational overhead.
Per-iteration Complexity of naive gradient descent: Equa-
tions (7) and (8) reveal that a set of equilibrium equations determines
the gradients. Consequently, to compute the gradients, we need to
solve these equations using fixed-point iteration. Each layer neces-
sitates one round of fixed-point iteration, and in total, including
ğ‘‰, we need to perform fixed-point iteration ğ‘‡+1times. The ma-
jor computational overhead arises from the multiplication of ğ‘€
with the derivatives, resulting in a complexity of ğ‘‚((ğ‘›ğ‘‘)2ğ‘‘2). Each
fixed-point iteration involves ğ‘‡instances of such computations.
Consequently, the overall runtime for each update is ğ‘‚(ğ‘‡2ğ‘›2ğ‘‘4).
Although the adjacency matrix is sparse, it only reduces the com-
plexity toğ‘‚(ğ‘‡2ğ‘›ğ‘‘4). This limits the application of our model in
large-scale dynamic graphs and hampers our ability to utilize large
embeddings.
4.2 Efficient Update via Bilevel Optimization
To address the previously mentioned challenges, we turn to Bilevel
Optimization [ 3] as a potential solution. We reformulate the prob-
lem presented in Equation (5) as the following standard bilevel
optimization problem.
min
ğœƒ,W,ğ‘‰L(ğœƒ,W,ğ‘‰)=ğ‘âˆ‘ï¸
ğ‘–=1â„“(ğ‘“ğœƒ(ğ‘§ğ‘‡
ğ‘–,ğ’šğ‘–) (9)
s.t.ğ‘§ğ‘‡
ğ‘–=arg minğ‘§âˆ¥ğ‘§âˆ’ğœ™(ğ‘§,W,ğ‘‰;Gğ‘–)âˆ¥2
2
âˆ¥ğ‘Šğ‘¡âˆ¥âˆâ‰¤ğœ…
âˆ¥ğ´ğ‘¡
ğ‘–âˆ¥ğ‘œğ‘,âˆ€ğ‘–=1,...,ğ‘,ğ‘¡ =1,...,ğ‘‡
Whereğœ™(ğ‘§,W,ğ‘‰;Gğ‘–)=ğœ(ğ‘€ğ‘‡
ğ‘–...ğœ(ğ‘€1
ğ‘–ğ‘§+vec(ğ‘‰ğ‘‹1
ğ‘–))...+vec(ğ‘‰ğ‘‹ğ‘‡
ğ‘–)).
The main differences between these problems lie in the constraints.
Equation (9) introduces explicit constraints solely on the last snap-
shot, leading to a multi-block bilevel optimization problem. This
type of problem has been investigated recently by [ 25] and [ 10].
[25] focus on top-K NDCG optimization, formulating it as a com-
positional bilevel optimization with a multi-block structure. Their
approach simplifies updates by sampling a single block batch in
each iteration and only updating the sampled blocks. [ 10] employs
a similar technique but addresses a broader range of multi-block
min-max bilevel problems.
However, these state-of-the-art bilevel optimization algorithms
are designed to address problems with strongly convex lower-level
4599KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongjian Zhong, Hieu Vu, Tianbao Yang, and Bijaya Adhikari
Algorithm 1 Bilevel Optimization for IGDNN
Require:D={(Gğ‘–,ğ’šğ‘–)}ğ‘
ğ‘–=1,ğœ‚1,ğœ‚2,ğ›¾
Ensure:ğœ”,ğœƒ
1:Randomly initialize ğ‘§0
ğ‘—andğ‘£0
ğ‘—forğ‘—=1,...,ğ‘
2:forğ‘˜=0,1,...,ğ¾ do
3: Sample a batch data B
4: Ë†ğ‘§ğ‘˜+1
ğ‘—=(
(ğ¼âˆ’ğœ‚1)Ë†ğ‘§ğ‘˜
ğ‘—+ğœ‚1ğœ™(Ë†ğ‘§ğ‘˜
ğ‘—,ğœ”ğ‘¡;Gğ‘–)ğ‘—âˆˆB
Ë†ğ‘§ğ‘¡
ğ‘—o.w.
5: Ë†ğ‘£ğ‘˜+1
ğ‘—=(
(ğ¼âˆ’ğœ‚2âˆ‡2ğ‘§ğ‘§ğ‘”(Ë†ğ‘§ğ‘˜
ğ‘—,ğœ”ğ‘˜))Ë†ğ‘£ğ‘˜
ğ‘—+ğœ‚2âˆ‡ğ‘§â„“ğ‘—(Ë†ğ‘§ğ‘˜
ğ‘—,ğœ”ğ‘˜)ğ‘—âˆˆB
Ë†ğ‘£ğ‘¡
ğ‘—o.w.
6: Update gradient estimator
Î”ğ‘˜+1=1
|B|âˆ‘ï¸
ğ‘—âˆˆBh
âˆ‡ğœ”â„“ğ‘—(Ë†ğ‘§ğ‘˜
ğ‘—,ğœ”ğ‘˜)âˆ’âˆ‡2
ğœ”ğ‘§ğ‘”ğ‘—(Ë†ğ‘§ğ‘˜
ğ‘—,ğœ”ğ‘˜)Ë†ğ‘£ğ‘˜
ğ‘—i
7:ğ‘šğ‘˜+1=(1âˆ’ğ›¾)ğ‘šğ‘˜+ğ›¾Î”ğ‘˜+1
8:ğœ”ğ‘˜+1=Î Î©
ğœ”ğ‘˜âˆ’ğœ‚0ğ‘šğ‘˜+1
9:end for
problems, which does not hold true for our problem. It is evident
that our lower-level problems in 9 are generally nonconvex with
respect toğ‘§since they involve highly nonlinear neural networks.
Additionally, these methods utilize stochastic gradient descent on
the lower level in each iteration, which may lead to potential extra
computation in estimating the gradient. Nevertheless, it is crucial
to note that the optimal solution to our lower-level problem cor-
responds to the fixed point of Eq (2). Leveraging this insight, we
employ a fixed-point iteration to update the lower-level solution.
We propose a single loop algorithm (see Algorithm 1) with fixed-
point updates.
To better illustrate our algorithm, let ğœ”={W,ğ‘‰}, andğ‘”ğ‘–(ğ‘§,ğœ”)
represents the ğ‘–-th-block lower-level problem, defined as âˆ¥ğ‘§âˆ’
ğœ™(ğ‘§,ğœ”;Gğ‘–)âˆ¥2
2, and letâ„“ğ‘–(ğ‘§,ğœ”):=â„“(ğ‘“ğœƒ(ğ‘§),ğ‘¦ğ‘–). The key to solving
the bilevel optimization problem in (9) is to estimate the hyper-
gradientâˆ‡L(ğœƒ,ğœ”)and backpropagate. The hypergradient of our
objective in (9) with respect to ğœ”as follows:
âˆ‡L(ğœƒ,ğœ”)=1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1âˆ‡â„“ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)
âˆ’âˆ‡2
ğœ”ğ‘§ğ‘”ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)h
âˆ‡2
ğ‘§ğ‘§ğ‘”ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)iâˆ’1
âˆ‡ğ‘§â„“ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)
Explicitly computing the inverted Hessian matrix
âˆ‡2ğ‘§ğ‘§ğ‘”ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)âˆ’1
in computation of our hypergradient is computationally expen-
sive. Inspired by [ 10] and [ 25], we instead directly approximate
âˆ‡2ğ‘§ğ‘§ğ‘”ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)âˆ’1âˆ‡ğ‘§â„“ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)usingğ‘£ğ‘–for each block by moving
average estimation (line 5). More specifically, we track the optimal
point of minğ‘£1
2ğ‘£âŠ¤âˆ‡2ğ‘§ğ‘§ğ‘”ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)ğ‘£âˆ’ğ‘£âŠ¤âˆ‡ğ‘§â„“ğ‘–(ğ‘§ğ‘‡
ğ‘–,ğœ”)for each block by
maintaining ğ‘£ğ‘–. Let Ë†ğ‘§ğ‘–be a moving average approximation to the
optimal lower-level solution ğ‘§ğ‘‡
ğ‘–. We use a single fixed-point update
forË†ğ‘§ğ‘–(line 4). We do not want to update all blocks in every iteration
since this is impractical when the number of blocks is large. To
address this issue, we use stochastic training. For sampled blocks,
we update their Ë†ğ‘§andË†ğ‘£, and we compute the hypergradient (line 6).
Note that the multiplication âˆ‡2ğœ”ğ‘§ğ‘”ğ‘—(Ë†ğ‘§ğ‘˜
ğ‘—,ğœ”ğ‘˜)Ë†ğ‘£ğ‘˜
ğ‘—(in line 6) can alsobe efficiently computed using Hessian vector product [ 24]. .As a
result, the training time for our algorithm is proportional to normal
backpropagation, eliminating the need for fixed-point iterations.
Note that in cases where the lower-level problem is strongly
convex, the errors introduced by these approximations are well-
contained [ 10]. Our lower-level problem admits a unique fixed
point, hence, employing fixed-point iteration becomes an efficient
means of attaining the optimal lower-level solution, akin to the
effectiveness of gradient descent under strong convexity. Therefore,
it is justifiable to assert that our approximations are effective in this
scenario, with empirical evidence robustly endorsing their practical
efficacy.
Per-iteration Complexity of bilevel optimization: The main
computational overheads are updating ğ‘£and estimating gradient.
Both steps are involved with estimating a huge Hassian matrix,
but, in practice, we use Hessian vector product to avoid explicitly
computing the Hessian matrix. Therefore, the dominant runtime
of bilevel optimization is three times backpropagation. Each back-
propagation takes ğ‘‚(ğ‘‡ğ‘›ğ‘‘2+ğ‘‡ğ‘›2ğ‘‘).
5 EXPERIMENTS
In this section, we present the performance of IDGNN in various
tasks, including effectiveness in capturing long-range dependencies
and avoiding oversmoothing on a synthetic dataset. We benchmark
IDGNN against nine state-of-the-art baselines on multiple real-
world datasets, comprising three node classification and four node
regression datasets. Key dataset statistics are summarized in Table
2, with further details available in section 5.1. Due to the space
constraints, specifics on experimental setup and hyperparameters
are provided in Appendices A.1 and A.2 respectively. Our code is
publicly available for reproducibility.1.
5.1 Datasets
Brain dataset is derived from a real-world fMRI brain scans dataset
2. In this dataset, nodes represent brain tissues, and edges capture
nodesâ€™ activation time similarity in the examined period. The node
attributes are generated from the fMRI signals [ 34]. Given the tem-
poral graph, our goal is to predict the functionality of each node,
which is in one of the 10categories.
DBLP is an extracted co-author network from DBLP website3,
where nodes are authors and edges represent co-authorship rela-
tionships. The extracted authors are from 5research areas [ 34],
serving as our class labels. Note attributes are word2vec represen-
tations of titles and abstracts from related papers.
Reddit dataset is extracted from a popular online platform Red-
dit4[8], where nodes represent posts, and edges are constructed
between nodes having similar keywords. The node attributes are
word2vec representations of the comments of a post [ 34], and node
labels correspond to one of the 4communities or â€œsubreddits" to
which the post belongs.
PeMS04 & PeMS08 These two datasets represent traffic sensor net-
works in two distinct districts of California during various months.
1https://github.com/yongjian16/IDGNN
2https://tinyurl.com/y4hhw8ro
3https://dblp.org/
4https://www.reddit.com/
4600Efficient and Effective Implicit Dynamic Graph Neural Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Performance for classification task (ROCAUC) and regression task (MAPE (%)). Performances on Brain10, England-
COVID, PeMS04, and PeMS08 for baseline methods are taken from [ 5]. The best performance for each dataset is highlighted in
bold, while the second-best performance is underlined.
Classification Regr
ession
Model Brain10
DBLP5 Reddit4 England-CO
VID PeMS04 PeMS08
T
rans. Induc. Trans. Induc. Trans. Induc.
Ev
olveGCN-O 0.58Â±0.10 0.639Â±0.207 0.513Â±0.008 4.07Â±0.73 3.88Â±0.47 3.20Â±0.25 2.61Â±0.42 2.65Â±0.12 2.40Â±0.27
Ev
olveGCN-H 0.60Â±0.11
0.510Â±0.013 0.508Â±0.008 4.14Â±1.14
3.50Â±0.42 3.34Â±0.14 2.84Â±0.31 2.81Â±0.28 2.81Â±0.23
GCN-GRU 0.87Â±0.07 0.878Â±0.017 0.513Â±0.010 3.56Â±0.26 2.97Â±0.34 1.60Â±0.14 1.28Â±0.04 1.40Â±0.26 1.07Â±0.03
D
ySAT-H 0.77Â±0.07 0.917Â±0.007 0.508Â±0.003 3.67Â±0.15
3.32Â±0.76 1.86Â±0.08 1.58Â±0.08 1.49Â±0.08 1.34Â±0.03
GCRN-M2 0.77Â±0.04 0.894Â±0.009 0.546Â±0.020 3.85Â±0.39 3.37Â±0.27 1.70Â±0.20 1.20Â±0.06 1.30Â±0.17 1.07Â±0.03
DCRNN 0.84Â±0.02
0.904Â±0.013 0.535Â±0.007 3.58Â±0.53
3.09Â±0.24 1.67Â±0.19 1.27Â±0.06 1.32Â±0.19 1.07Â±0.03
T
GAT 0.80Â±0.03 0.895Â±0.003 0.510Â±0.011 5.44Â±0.46 5.13Â±0.26 3.11Â±0.50 2.25Â±0.27 2.66Â±0.27 2.34Â±0.19
T
GN 0.91Â±0.03 0.887Â±0.004
0.521Â±0.010 4.15Â±0.81
3.17Â±0.23 1.79Â±0.21 1.19Â±0.07 1.49Â±0.26 0.99Â±0.06
GRU-GCN 0.91Â±0.03 0.906Â±0.008 0.525Â±0.006 3.41Â±0.28 2.87Â±0.19 1.61Â±0.35 1.13Â±0.05 1.27Â±0.21 0.89Â±0.07
IDGNN 0.94Â±0.01 0.907Â±0.005 0.556Â±0.017 2.65Â±0.25 3.05Â±0.25 0.53Â±0.05
0.63Â±0.04 0.45Â±0.11 0.50Â±0.05
Figure 2: The left and middle are accuracy and loss curves when using 10 layers. The x-axis is epochs, and the y-axis is accuracy
and cross entropy loss, respectively. The right plot represents the accuracy results of all baselines under different layer settings.
Table 2: Statistics of datasets. ğ‘: number of dynamic graphs,
|ğ‘‰|: number of nodes, min|ğ¸|: minimum number of edges,
max|ğ¸|: maximum number of edges, ğ‘‡: window size, ğ‘‘: feature
dimension, ğ‘¦label dimension
ğ‘|ğ‘‰|min|ğ¸|max|ğ¸|ğ‘‡
ğ‘‘ ğ‘¦
Brain10 1 5000 154094 167944 12 20 10
DBLP5 1 6606 2912 5002 10 100 5
Re
ddit4 1 8291 12886 56098 10 20 4
PeMS04 16980 307 680 680 12 5 3
PeMS08 17844 170 548 548 12 5 3
English-CO
VID 54 129 836 2158 7 1 1
In this context, each node symbolizes a road sensor, while the edge
weights denote the geographical distance between two sensors
(with uniform weights across all snapshots). Every sensor captures
average traffic statistics such as flow, occupancy, and speed over a
5-minute interval. The datasets utilized in this study are identical
to those in [ 5], where we exclusively forecast attribute values for a
single future snapshot.England-COVID This temporal graph dataset is taken from [ 5],
which is created by aggregating information from the mobility log
released by Facebook under Data For Good program5for England
[20]. The nodes are cities, and the edges are transportation statistics
between them. Given a temporal graph of length 7, which represents
the a week, we need to predict the infection rate for the next day.
5.2 Regression
For node-level tasks, there are two evaluation paradigms: trans-
ductive and inductive. Transductive evaluation allows the model
to access identities of the nodes and edges in testing data during
training (i.e., only the labels are hidden), while inductive evaluation
involves testing the model on new nodes and edges. In simpler
terms, transductive evaluation separates training and testing sets
based on nodes, while inductive evaluation separates them based on
time stamps (i.e., models are trained on past time stamps and tested
on the future). Here, we conduct experiments on both settings.
The datasets we used for the regression task are England-COVID,
PeMS04, and PeMS08. We use the mean average percentage error
5https://dataforgo
od.fb.com/tools/disease-prevention-maps/
4601KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongjian Zhong, Hieu Vu, Tianbao Yang, and Bijaya Adhikari
Table 3: Memory and runtime comparison results for all
methods on reddit4 and DBLP5 datasets. We report the mem-
ory usage using MB and runtime using seconds per window
.
Reddit4 DBLP5
Mem. Time Mem. Time
EvolveGCN-O 42 0.0649Â±0.017 52 0.0672Â±0.014
EvolveGCN-H 52 0.0904Â±0.020 82 0.0997Â±0.037
GCN-GRU 221 0.0733Â±0.012 200 0.1142Â±0.045
DySAT-H 181 0.1613Â±0.056 165 0.1343Â±0.012
GCRN-M2 322 0.4345Â±0.080 319 0.4934Â±0.076
DCRNN 223 0.1697Â±0.019 278 0.2121Â±0.039
TGAT 793 0.0750Â±0.014 338 0.0770Â±0.015
TGN 450 0.0417Â±0.004 233 0.0454Â±0.012
GRU-GCN 4116 0.0199Â±0.008 580 0.0161Â±0.007
IDGNN 89 0.0291Â±0.007 75 0.0302Â±0.002
(MAPE) as our evaluation metric. The results are reported in Table
1 with mean MAPE and standard deviation. Our proposed method
outperforms other methods in both transductive and inductive
settings, with the exception of the inductive case in England-COVID.
Our method demonstrates a significant improvement for PeMS04
and PeMS08, particularly in the transductive learning scenario. In
comparison to the second-best method, our proposed model reduces
the error by over 1% of MAPE, but our model on the inductive
learning scenario does not enjoy such improvement. We attribute
this difference to our modelâ€™s tendency to separate nodes, even
when they have the same labels and topology. We delve into this
phenomenon in the Appendix B.
5.3 Classification
We conduct classification experiments on Brain10, DBLP5, and
Reddit4 datasets. Since these datasets consist of only one dynamic
graph, we focused on testing the transductive case. The evaluation
was done using the Area under the ROC Curve (AUC) metric. The
average prediction AUC values and their corresponding standard
deviations are presented in Table 1. Our proposed model achieved
the top rank in 2 out of 3 datasets and was the second best in the
remaining dataset. These results demonstrate that our model suc-
cessfully captures the long-range dependencies within the dynamic
graphs, as reflected in the learned embeddings.
5.4 Long-range Dependency and
Oversmoothing
Long-range Dependency: Here we first consutrct a toy data that
aims to test the ability of all approaches to capture long-range
dependencies. Our toy data consists of {5,10,15}snapshots, with
each snapshot being a clique of 10nodes. Each node has 10asso-
ciated attributes. The task is to classify nodes at the last snapshot,
where each node represents its own class (i.e., there are a total
of 10 classes). The node attributes consist of randomly generated
numbers, except for the first snapshot, which uses the one-hot
representation of the class. Successful classification of this dataset
requires effective information aggregation starting in the initialtime stamp, propagating the class label information over time, and
avoiding oversmoothing as the information is propagated. In this
dataset, there are no testing nodes; all nodes are used for training.
The training results are presented on Figure 2. Our model is
compared with TGCN [ 37]. We also propose two more modified
baselines: IGNN-GRU and TIGNN, which are obtained by replacing
the GCNs within GCN-GRU [ 30] and TGCN by IGNN. We ensure
the comparison is fair by ensuring a similar number of parameters
are used, and we test all models on {5,10,15}layers. All meth-
ods are trained for a maximum of 2000 epochs, followed by the
hyper-parameter selection approach described in the Appendix. As
shown in the figure, we explored the impact of varying the number
of layers on both baseline models and our proposed model. We
documented the resulting accuracies accordingly. Notably, TGCN
exhibits a discernible pattern of over-smoothing, evidenced by a per-
formance decline with increasing layers. Conversely, IGNN-GRU
and TIGNN do not demonstrate such susceptibility. Additionally,
we observed challenges in data fitting for both methods, whereas
our model consistently achieved 100% accuracy across different
layer configurations. This experiment underscores the robustness
of our architecture in fully unleashing the potential of implicit
models in dynamic graph learning.
Moreover, we perform an alternative experiment, wherein we
shift label information from snapshot 1to snapshot 5. This adjust-
ment ensures consistent difficulty in leveraging label information
across all models. Due to space constraints, we provide the detailed
results in the Appendix; however, the overall conclusion remains
unaffected.
Oversmoothing: Here, we employ Dirichlet energy to assess the
smoothness of the learned embeddings [ 28]. The Dirichlet energy
measures the mean distance between nodes and is defined as fol-
lows:
ğ·ğ¸(ğ‘)=âˆšï¸„1
|V|âˆ‘ï¸
ğ‘–âˆˆVâˆ‘ï¸
ğ‘—âˆˆNğ‘–âˆ¥ğ‘ğ‘–âˆ’ğ‘ğ‘—âˆ¥2
Here,Nğ‘–denotes the neighbors of node ğ‘–. Low Dirichlet energy
means smooth embedding. We compare our model with two differ-
ent baselines:
â€¢Static: We create a static graph by averaging the adjacency
matrices of all snapshots and using the feature matrix of
the last snapshot as its feature. Subsequently, we learn the
embeddings using a vanilla multi-layer GCN.
â€¢W/o loop: This model shares the structure of IDGNN but
does not enforce the learned embeddings to be a fixed point.
We evaluate these methods on the Brain10 dataset, and the results
are presented in Table 4. The "static" method exhibits oversmooth-
ing as the layer count increases, and a similar issue is observed for
the "W/o loop" method, despite stacking one layer for each snap-
shot. These findings strongly indicate that our model effectively
addresses oversmoothing in dynamic graph learning by leveraging
the implicit model structure.
5.5 Efficiency
We compare runtime and performance between SGD and bilevel
optimization algorithms. To this end, we compare Brain10, England-
COVID, PeMS04, and PeMS08. The results are summarized on the
4602Efficient and Effective Implicit Dynamic Graph Neural Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 4: Evaluating the smoothness of embeddings and the
AUC on Brain10.
Dirichlet Energyâ†‘AUCâ†‘
Static (3 layers) 10.660 89.79
Static (4 layers) 8.781 88.67
Static (5 layers) 3.399 81.49
W/o looping 8.957 85.61
IDGNN 35.299 94.87
Table 5: Runtime and performance comparison between SGD
and Bilevel Methods.
Runtime (s/win) Performance
SGD Bilevel SGD Bilevel
Brain10 624 0.390 94.7 94.9
PeMS04 0.72 0.049 0.63 0.63
PeMS08 0.29 0.046 0.56 0.50
England-COVID 0.092 0.030 2.97 3.05
Table 5. The results are computed by averaging the runtime of a
whole epoch with the number of dynamic graphs ğ‘.
These methods have similar performance, but the runtime results
show that the bilevel optimization algorithm is much faster than the
SGD. Especially, in the Brain10 dataset, bilevel algorithm achieves
1600 times of speedup compared with SGD. Furthermore, we notice
that the ratio of runtimes in PeMS04 and PeMS08 is0.72
0.29=2.48, and
the squared ratio of their number of nodes is (307
170)2=3.26. This
confirms our complexity result for SGD, which is quadratic regard-
ing the number of nodes. On the other hand, the bilevel method
exhibits only linear dependency. We also present the memory usage
and runtimes of all methods on Reddit4 and DBLP5. The memory ef-
ficiency of implicit models comes from the fact that implicit models
can use few parameters and do not need to store the intermediate
results. However, we need to store intermediate results and back-
propagate for our bi-level method. Due to the simple RNN-free
architecture of our method, our approach is competitive in runtime
and memory. We provide a memory and runtime comparison on
DBLP5 and Reddit4. The results are summarized in Table 3.
5.6 Ablation Study
In this section, we delve into the various configurations of our
model. Drawing from the properties mentioned earlier, our model
can be represented by the formula (1). We have deliberately decided
to assign different weights ( ğ‘Š) to each timestamp while maintain-
ing weight-tied for ğ‘‰. Alternatively, the model comprises ğ‘‡layers of
Table 6: Evaluating different variants of our model. Present-
ing AUC results on Brain10 datasets
Share both IDGNN Share ğ‘‰Not share W/o loop
AUC 94.29 94.87 94.87 94.90 85.62GCN and one linear layer. The linear layer serves to aggregate static
information, while the GCNs handle dynamic information. To vali-
date our architecture choice, we conducted a thorough comparison
of our model against other configurations:
â€¢Share both: Both ğ‘Šandğ‘‰are shared across layers.
â€¢Shareğ‘‰: Onlyğ‘‰is shared across layers.
â€¢Not share:ğ‘Šandğ‘‰are not shared.
â€¢W/o loop: as defined in Section 5.4.
The results are presented in Table 6. As observed, the share-
both model exhibits the poorest performance. We believe this is
due to the limited number of free variables available for learning,
which makes the training process challenging. In our approach and
the share-ğ‘‰method, we achieve very similar results. Our model
utilizesğ‘‡layers of GCN for dynamic information and one linear
layer for static information, while the share- ğ‘‰method employs
one GCN and ğ‘‡linear layers. The not-share method achieves the
best result, although the improvement is negligible. However, it
increases the parameter size, resulting in significant computational
overhead. Hence, we opt for the current configuration, as it delivers
good performance while minimizing parameter size, especially
since the number of attributes may exceed the hidden dimension.
We additionally evaluate an alternative baseline, denoted as "w/o
loop," wherein we eliminate the looping structure from IDGNN.
The obtained results reveal that this model exhibits the lowest
performance, underscoring the efficacy of our proposed approach.
6 CONCLUSIONS
In this paper, we proposed a novel implicit graph neural network
for dynamic graphs. As far as we know, this is the first implicit
model on dynamic graphs. We demonstrate that the implicit model
we proposed has the well-posedness characteristic. We proposed a
standard optimization algorithm using the Implicit Function Theo-
rem. However, the optimization was too computationally expensive
for our model. Hence, we proposed a novel bilevel optimization
algorithm to train our proposed model. We conducted extensive
experiments on 6 real-world datasets and one toy dataset. The re-
gression and classification tasks show that the proposed approach
outperforms all the baselines in most settings. Finally, we also
demonstrated that the proposed bilevel optimization algorithm
obtains significant speedup over standard optimization while main-
taining the same performance. A key limitation of our proposed
approach is that it can only predict the consecutive snapshot. In the
future, we plan on addressing this issue and also provide a diffusion
model-based training algorithm.
7 ACKNOWLEDGEMENT
We are grateful to the anonymous reviewers for their constructive
comments and suggestions. This work was supported in part by
the NSF Cybertraining 2320980, SCH 2306331, CAREER 1844403
and the CDC MInD Healthcare group under cooperative agreement
U01-CK000594.
4603KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongjian Zhong, Hieu Vu, Tianbao Yang, and Bijaya Adhikari
REFERENCES
[1]Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2019. Deep equilibrium models.
Advances in Neural Information Processing Systems 32 (2019).
[2]Qi Chen, Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. 2022.
Optimization-induced graph implicit nonlinear diffusion. In International Confer-
ence on Machine Learning. PMLR, 3648â€“3661.
[3]BenoÃ®t Colson, Patrice Marcotte, and Gilles Savard. 2007. An overview of bilevel
optimization. Annals of operations research 153 (2007), 235â€“256.
[4]Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Tsai.
2021. Implicit deep learning. SIAM Journal on Mathematics of Data Science 3, 3
(2021), 930â€“958.
[5]Jianfei Gao and Bruno Ribeiro. 2022. On the equivalence between temporal and
static equivariant graph representations. In International Conference on Machine
Learning. PMLR, 7052â€“7076.
[6]Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin.
2021. On training implicit models. Advances in Neural Information Processing
Systems 34 (2021), 24247â€“24260.
[7]Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui.
2020. Implicit graph neural networks. Advances in Neural Information Processing
Systems 33 (2020), 11984â€“11995.
[8]William L. Hamilton, Rex Ying, and Jure Leskovec. 2018. Inductive Representation
Learning on Large Graphs. arXiv:1706.02216 [cs.SI]
[9]Danfeng Hong, Lianru Gao, Jing Yao, Bing Zhang, Antonio Plaza, and Joce-
lyn Chanussot. 2020. Graph convolutional networks for hyperspectral image
classification. IEEE Transactions on Geoscience and Remote Sensing 59, 7 (2020),
5966â€“5978.
[10] Quanqi Hu, Yongjian Zhong, and Tianbao Yang. 2022. Multi-block min-max
bilevel optimization with applications in multi-task deep auc maximization. arXiv
preprint arXiv:2206.00260 (2022).
[11] Shima Khoshraftar and Aijun An. 2022. A survey on graph representation
learning methods. arXiv preprint arXiv:2204.01855 (2022).
[12] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[13] Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin,
and Yong Li. 2023. Dynamic graph convolutional recurrent network for traffic
prediction: Benchmark and solution. ACM Transactions on Knowledge Discovery
from Data 17, 1 (2023), 1â€“21.
[14] Mingjie Li, Yifei Wang, Yisen Wang, and Zhouchen Lin. 2022. Unbiased Stochastic
Proximal Solver for Graph Neural Networks with Equilibrium States. In The
Eleventh International Conference on Learning Representations.
[15] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph
convolutional networks for semi-supervised learning. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[16] Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. 2018. Adaptive Graph
Convolutional Neural Networks. arXiv:1801.03226 [cs.LG]
[17] Jie Liao, Wei Zhou, Fengji Luo, Junhao Wen, Min Gao, Xiuhua Li, and Jun Zeng.
2022. SocialLGN: Light graph convolution network for social recommendation.
Information Sciences 589 (2022), 595â€“607.
[18] Juncheng Liu, Kenji Kawaguchi, Bryan Hooi, Yiwei Wang, and Xiaokui Xiao.
2021. Eignn: Efficient infinite-depth graph neural networks. Advances in Neural
Information Processing Systems 34 (2021), 18762â€“18773.
[19] Yao Ma, Ziyi Guo, Zhaocun Ren, Jiliang Tang, and Dawei Yin. 2020. Stream-
ing graph neural networks. In Proceedings of the 43rd international ACM SIGIR
conference on research and development in information retrieval. 719â€“728.
[20] George Panagopoulos, Giannis Nikolentzos, and Michalis Vazirgiannis. 2021.
Transfer Graph Neural Networks for Pandemic Forecasting. In Proceedings of the35th AAAI Conference on Artificial Intelligence.
[21] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,
Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:
Evolving graph convolutional networks for dynamic graphs. In Proceedings of
the AAAI conference on artificial intelligence, Vol. 34. 5363â€“5370.
[22] Junyoung Park, Jinhyun Choo, and Jinkyoo Park. 2021. Convergent Graph Solvers.
InInternational Conference on Learning Representations.
[23] Junhui Park, Gaeun Sung, SeungHyun Lee, SeungHo Kang, and ChunKyun Park.
2022. ACGCN: graph convolutional networks for activity cliff prediction between
matched molecular pairs. Journal of Chemical Information and Modeling 62, 10
(2022), 2341â€“2351.
[24] Barak A Pearlmutter. 1994. Fast exact multiplication by the Hessian. Neural
computation 6, 1 (1994), 147â€“160.
[25] Zi-Hao Qiu, Quanqi Hu, Yongjian Zhong, Lijun Zhang, and Tianbao Yang. 2022.
Large-scale stochastic optimization of ndcg surrogates for deep learning with
provable convergence. arXiv preprint arXiv:2202.12183 (2022).
[26] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal graph networks for deep learning
on dynamic graphs. arXiv preprint arXiv:2006.10637 (2020).
[27] Halsey Lawrence Royden and Patrick Fitzpatrick. 1968. Real analysis. Vol. 2.
Macmillan New York.
[28] T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. 2023. A survey
on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993
(2023).
[29] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2020.
Dysat: Deep neural representation learning on dynamic graphs via self-attention
networks. In Proceedings of the 13th international conference on web search and
data mining. 519â€“527.
[30] Youngjoo Seo, MichaÃ«l Defferrard, Pierre Vandergheynst, and Xavier Bresson.
2018. Structured sequence modeling with graph convolutional recurrent net-
works. In Neural Information Processing: 25th International Conference, ICONIP
2018, Siem Reap, Cambodia, December 13-16, 2018, Proceedings, Part I 25. Springer,
362â€“373.
[31] Mengzhu Sun, Xi Zhang, Jiaqi Zheng, and Guixiang Ma. 2022. Ddgcn: Dual
dynamic graph convolutional networks for rumor detection on social media. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 4611â€“4619.
[32] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks.
arXiv:1710.10903 [stat.ML]
[33] Zehong Wang, Qi Li, and Donghua Yu. 2022. TPGNN: Learning High-order
Information in Dynamic Graphs via Temporal Propagation. arXiv preprint
arXiv:2210.01171 (2022).
[34] Dongkuan Xu, Wei Cheng, Dongsheng Luo, Yameng Gu, Xinyu Liu, Jingchao Ni,
Bo Zong, Haifeng Chen, and Xiang Zhang. 2019. Adaptive Neural Network for
Node Classification in Dynamic Networks. 2019 IEEE International Conference on
Data Mining (ICDM) (2019), 1402â€“1407.
[35] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
2020. Inductive representation learning on temporal graphs. arXiv preprint
arXiv:2002.07962 (2020).
[36] Menglin Yang, Ziqiao Meng, and Irwin King. 2020. Featurenorm: L2 feature nor-
malization for dynamic graph embedding. In 2020 IEEE International Conference
on Data Mining (ICDM). IEEE, 731â€“740.
[37] Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and
Haifeng Li. 2019. T-gcn: A temporal graph convolutional network for traffic
prediction. IEEE transactions on intelligent transportation systems 21, 9 (2019),
3848â€“3858.
4604Efficient and Effective Implicit Dynamic Graph Neural Network KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Layers GCN-GRU T-GCN IDGNN
8 0.6217 0.9934 1.1113
16 0.5204 0.8719 1.0982
32 0.0077 0.7176 1.0019
Table 7: Smoothness of embeddings. (The larger the better)
A EXPERIMENT
A.1 Experiment Setup
We follow the procedure from [ 5] and utilize the provided code
as the code base to compare all the baselines and our method.
Specifically, we first split the dataset into three portions with ratios
70%-10%-20% for training, validation, and testing, respectively. Split-
ting is based on nodes for transductive tasks and time for inductive
tasks. We then normalize node attributes and edge attributes with
the 0-1 normalization method. We train on the training portion,
find the best hyperparameters using the validation set, and report
the performance on the test set. We also use ROCAUC score to
evaluate classification tasks and mean average percentage error
(MAPE) for regression tasks.
A.2 Hyperparameter
For detailed baselinesâ€™ architecture, please refer to [ 5]. Notice that,
for all the methods and all task, we fixed the embedding size as
16, and we searched the learning rates from 0.1,0.01, and 0.001.
For Brain10, we observed that our method converged slowly, then
we let it run for 1000 epochs. For DBLP5 and Reddit4, we let all
the methods run 500epochs for 5times for each learning rate and
report the performance on the test set where the performance of the
validation set is the best. For regression datasets, we run 100 epochs
for England-COVID and 10 epochs for PeMS04/PeMS08. The hyper-
parameter our model ğœ‚1âˆˆ{0.5,0.7,0.9,1},ğœ‚2âˆˆ{0.001,0.01,0.1}.
PROOFS
Lemma A.1. Ifâˆ¥.âˆ¥ğ‘œğ‘is the matrix operator norm on Rğ‘›Ã—ğ‘›and
ğœ†PF(ğ´)outputs the largest absolute eigenvalue of ğ´, then, for any
matrixğ´âˆˆRğ‘›Ã—ğ‘›,
ğœ†PF(ğ´)â‰¤âˆ¥ğ´âˆ¥ğ‘œğ‘
Proof. Letğœ†be the eigenvalue of ğ´, and letğ‘¥â‰ 0be the corre-
sponding eigenvector. From ğ´ğ‘¥=ğœ†ğ‘¥, we have
ğ´ğ‘‹=ğœ†ğ‘‹
where each column in ğ‘‹isğ‘¥. Further, we have
|ğœ†|âˆ¥ğ‘‹âˆ¥ğ‘œğ‘=âˆ¥ğœ†ğ‘‹âˆ¥ğ‘œğ‘=âˆ¥ğ´ğ‘‹âˆ¥ğ‘œğ‘â‰¤âˆ¥ğ´âˆ¥ğ‘œğ‘âˆ¥ğ‘‹âˆ¥ğ‘œğ‘
Sinceâˆ¥ğ‘‹âˆ¥>0, taking the maximal ğœ†gives the result. â–¡
Lemma A.2. The equilibrium equation ğ‘§=ğœ(ğ‘€ğ‘§+ğ‘)has a unique
fixed point solution if âˆ¥|ğ‘€|âˆ¥ğ‘œğ‘<1, whereâˆ¥.âˆ¥ğ‘œğ‘is the operator norm,
andğœ(Â·)is an element-wise non-expansive function.
Proof. Based on Lemma A.1 and Theorem 4.1 in [ 7], this lemma
is immediately followed. â–¡Proof of Theorem 3.1
Proof. By Lemma. A.2, the well-posedness requirement for For-
mula. (3) isâˆ¥Mâˆ¥ğ‘œğ‘|â‰¤1. Since Formula. (3) and (2) are equivalent,
the well-posedness requirement for Formula. (2) is also âˆ¥|M|âˆ¥ğ‘œğ‘<
1.
Letğ‘€:=0ğ‘€1
Ë†ğ‘€ 0
where Ë†ğ‘€:=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘€2...
0...0
0... ğ‘€ğ‘¡ï£¹ï£ºï£ºï£ºï£ºï£ºï£». Let Ëœğ‘€:=
Ë†ğ‘€ 0
0ğ‘€1
. Then
âˆ¥|ğ‘€|âˆ¥ğ‘œğ‘=âˆ¥|Ëœğ‘€|0ğ¼ğ‘š
ğ¼ğ‘› 0
âˆ¥ğ‘œğ‘â‰¤âˆ¥| Ëœğ‘€|âˆ¥ğ‘œğ‘Â·âˆ¥0ğ¼ğ‘š
ğ¼ğ‘› 0
âˆ¥ğ‘œğ‘
=âˆ¥|Ëœğ‘€|âˆ¥ğ‘œğ‘=max{âˆ¥|ğ‘€1|âˆ¥ğ‘œğ‘,...,âˆ¥|ğ‘€ğ‘¡|âˆ¥ğ‘œğ‘}
This means if all subsystems satisfy the largest eigenvalue con-
straint, then the coupled equilibrium equation has a fixed point
solution by Lemma A.2. â–¡
B EMBEDDING VISUALIZATION
In this section, we explore an interesting aspect of our method
that can provide empirical insights into its ability to mitigate over-
smoothing. We conduct experiments on a synthetic dataset that
bears resemblance to toy datasets.
The dataset comprises 10 snapshots, with each snapshot rep-
resenting a clique of 10 nodes. Each node is associated with 10
attributes. The nodes fall into two distinct classes, but we delib-
erately conceal the label information in the attributes of the first
snapshot. Specifically, the first two dimensions of the attributes
represent the one-hot encoding of the labels, while the remaining
dimensions are set to zero. Additionally, we assign unique IDs to
the nodes in sequential order. Nodes with IDs ranging from 0 to 4
belong to class 0, while those with IDs ranging from 5 to 9 belong
to class 1. To assess the effectiveness of our method, we visually
compare the embedding results with those obtained from TGCN.
Upon examining the visualizations, we observe that our modelâ€™s
embeddings exhibit gradual changes, whereas TGCNâ€™s embeddings
remain consistent for nodes belonging to the same class. From a
node-centric perspective, TGCNâ€™s embeddings seem reasonable.
Nodes of the same class possess identical features and exhibit the
same topological structure. Therefore, it is logical for them to share a
common embedding. However, our embeddings tend to differentiate
each individual node. We believe that this characteristic plays a
role in mitigating the oversmoothing problem within our model.
Furthermore, we conducted an additional experiment to quanti-
tatively evaluate our modelâ€™s ability to tackle over-smoothing. This
experiment is conducted on the binary toy dataset: the toy data
we constructed consists of a dynamic graph with a maximum of
64 snapshots (adapt to layers), with each snapshot being a clique
of 10 nodes. Each node has 10 associated attributes. The task is
binary classification where each nodeâ€™s class information is hidden
in its first time-stampâ€™s attributes. Attributes of other time stamps
are randomly sampled from the normal distribution. All methods
are trained with a maximum of 2000 epochs and a learning rate
of 0.001. Finally, we evaluate their smoothness using the Mean
Average Distance (MAD). Results are summarized in Table 7.
4605KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yongjian Zhong, Hieu Vu, Tianbao Yang, and Bijaya Adhikari
Figure 4: Additional result.
(a) Our model
(b) TGCN
Figure 3: The embedding visualization of our method and
TGCNC HESSIAN VECTOR PRODUCT
To compute the product of Hassian and a vector: ğ»ğ‘£, andğ»=ğœ•2ğ‘“
ğœ•ğ‘¥2.
We compute the product by ğ»ğ‘£=ğœ•(ğœ•ğ‘“
ğœ•ğ‘¥)ğ‘‡ğ‘£
ğœ•ğ‘¥. In this way, we are not
explicitly computing the Hessian.6
D ADDITIONAL RESULT ON TOY DATA
This synthetic experiment shifts label information from time stamp
1 to time stamp 5. This adjustment ensures uniform difficulty in
utilizing label information across all models. Implementing this
change postpones our method towards achieving 100% accuracy
by approximately 50 epochs. However, even with this modification,
the baselines still struggle to fit the data.
D.1 Complexity
To contrast with the emprical runtime, here we present the theoret-
ical time complexities for our approach and the baselines. For an
arbitrary temporal graph, we denote the total number of snapshots
asğ‘‡, the total number of nodes as ğ‘›, the number of edges of time ğ‘¡
asğ¸ğ‘¡, andğ¸ğ‘ğ‘”ğ‘”as the number of aggregated edges, which satisfies
ğ¸ğ‘ğ‘”ğ‘”â‰ªÃ
ğ‘¡ğ¸ğ‘¡orğ¸ğ‘ğ‘”ğ‘”â‰ˆÃ
ğ‘¡ğ¸ğ‘¡. Some basic complexities: GRU, self-
attention, and LSTM have complexity ğ‘‚(ğ‘‡2)if the input sequence
6https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html
has lengthğ‘‡; GCN and SpectralGCN have complexity ğ‘‚(ğ‘›ğ‘‘2+ğ¸ğ‘‘);
GAT has complexity ğ‘‚(ğ‘›ğ‘‘+ğ¸ğ‘‘2). The complexities of all models
are summarized in Table 8. Based on the complexity results, IDGNN
is faster than EvolveGCN-O, EvolveGCN-H, GCN-GRU, GCRN-M2,
and DCRNN due to the absence of RNN in our model.
EvolveGCN-O ğ‘‚(ğ‘‡ğ‘‘2+ğ‘‡ğ‘›ğ‘‘2+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘)
EvolveGCN-H ğ‘‚(ğ‘‡ğ‘‘2+ğ‘‡ğ‘›ğ‘‘2+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘)
GCN-GRU ğ‘‚(ğ‘‡ğ‘‘2+ğ‘‡ğ‘›ğ‘‘2+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘)
DySAT ğ‘‚(ğ‘‡ğ‘‘2+ğ‘‡ğ‘›ğ‘‘+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘2)
GCRN-M2 ğ‘‚(ğ‘‡ğ‘‘2+ğ‘‡ğ‘›ğ‘‘2+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘)
DCRNN ğ‘‚(ğ‘‡ğ‘‘2+ğ‘‡ğ‘›ğ‘‘2+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘)
TGAT ğ‘‚(ğ‘‡ğ‘›ğ‘‘+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘2)
TGN ğ‘‚(ğ¸ğ‘ğ‘”ğ‘”ğ‘‘+ğ‘‡ğ‘›ğ‘‘2+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘2)
GRU-GCN ğ‘‚(ğ¸ğ‘ğ‘”ğ‘”ğ‘‘+ğ‘‡ğ‘›ğ‘‘2+ğ‘‡ğ¸ğ‘ğ‘”ğ‘”ğ‘‘2)
IDGNN ğ‘‚(ğ‘‡ğ‘›ğ‘‘2+Ã
ğ‘¡ğ¸ğ‘¡ğ‘‘)
Table 8: Summary of complexities of all models
4606