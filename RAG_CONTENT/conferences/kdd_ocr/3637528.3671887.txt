URRL-IMVC: Unified and Robust Representation Learning for
Incomplete Multi-View Clustering
Ge Tengâˆ—
12115044@zju.edu.cn
Zhejiang University
Hangzhou, ChinaTing Mao
maoting.mao@alibaba-inc.com
Alibaba Cloud
Hangzhou, ChinaChen Shenâ€ 
jason.sc@alibaba-inc.com
Alibaba Cloud
Hangzhou, China
Xiang Tianâ€ 
tianx@zju.edu.cn
Zhejiang University
Hangzhou, China
Zhejiang University Embedded
System Engineering Research Center,
Ministry of Education of China
Hangzhou, ChinaXuesong Liu
11015006@zju.edu.cn
Zhejiang University
Hangzhou, China
Zhejiang University Embedded
System Engineering Research Center,
Ministry of Education of China
Hangzhou, ChinaYaowu Chen
yaowuchen@zju.edu.cn
Zhejiang University
Hangzhou, China
Zhejiang University Embedded
System Engineering Research Center,
Ministry of Education of China
Hangzhou, China
Jieping Ye
yejieping.ye@alibaba-inc.com
Alibaba Cloud
Hangzhou, China
Abstract
Incomplete multi-view clustering (IMVC) aims to cluster multi-view
data that are only partially available. This poses two main chal-
lenges: effectively leveraging multi-view information and mitigat-
ing the impact of missing views. Prevailing solutions employ cross-
view contrastive learning and missing view recovery techniques.
However, they either neglect valuable complementary information
by focusing only on consensus between views or provide unreliable
recovered views due to the absence of supervision. To address these
limitations, we propose a novel Unified and Robust Representa-
tion Learning for Incomplete Multi-View Clustering (URRL-IMVC).
URRL-IMVC directly learns a unified embedding that is robust to
view missing conditions by integrating information from multiple
views and neighboring samples. Firstly, to overcome the limitations
of cross-view contrastive learning, URRL-IMVC incorporates an
attention-based auto-encoder framework to fuse multi-view infor-
mation and generate unified embeddings. Secondly, URRL-IMVC
directly enhances the robustness of the unified embedding against
view-missing conditions through KNN imputation and data aug-
mentation techniques, eliminating the need for explicit missing
view recovery. Finally, incremental improvements are introduced
âˆ—This work was done during a research internship at Alibaba Cloud.
â€ Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671887to further enhance the overall performance, such as the Clustering
Module and the customization of the Encoder. We extensively eval-
uate the proposed URRL-IMVC framework on various benchmark
datasets, demonstrating its state-of-the-art performance. Further-
more, comprehensive ablation studies are performed to validate the
effectiveness of our design.
CCS Concepts
â€¢Computing methodologies â†’Artificial intelligence; Learn-
ing latent representations; Cluster analysis.
Keywords
Deep Learning; Representation Learning; Self-supervised Learning;
Multi-view Learning; Incomplete Multi-view Clustering
ACM Reference Format:
Ge Teng, Ting Mao, Chen Shen, Xiang Tian, Xuesong Liu, Yaowu Chen,
and Jieping Ye. 2024. URRL-IMVC: Unified and Robust Representation Learn-
ing for Incomplete Multi-View Clustering. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671887
1 Introduction
Multi-view data [ 9] is commonly collected and utilized in various
domains, making multi-view clustering (MVC) a crucial tool for ana-
lyzing such data and uncovering its underlying structures [ 4,5]. Pre-
vious research has proposed several approaches [ 36,37] achieving
promising performance by exploiting consensus or complementary
information between views. However, in real-world applications,
some views may be partially unavailable due to sensor malfunctions
or other practical reasons. Existing MVC methods heavily rely on
 
2888
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ge Teng et al.
complete views to learn a comprehensive representation for clus-
tering, making them inadequate under such conditions. To address
this issue, Incomplete Multi-view Clustering (IMVC) methods have
been introduced to reduce the impact of missing views [ 33]. Various
IMVC approaches have been proposed, including matrix decompo-
sition [ 17], kernel-based [ 22], and graph-based [ 10] methods. With
the superior feature representation ability demonstrated by deep
learning, some IMVC methods have integrated deep learning tech-
niques, known as Deep Incomplete Multi-view Clustering (DIMVC)
methods, which we will mainly discuss below. The key challenges
in the IMVC task revolve around two problems: i) effectively utiliz-
ing multi-view information, and ii) mitigating the impact of missing
views. Previous DIMVC works [ 15,19â€“21,29] have employed two
mainstream strategies to address these problems: 1) cross-view con-
trastive learning, and 2) missing view recovery. However, these
strategies have inherent drawbacks.
A general framework for cross-view contrastive learning is il-
lustrated in Fig 1a, which originates from the MVC approaches. In
this framework, Deep Neural Network (DNN) auto-encoders are
employed to extract embeddings for each view. The embeddings
are then aligned using a contrastive loss, aiming to minimize the
distance between embeddings from the same sample across differ-
ent views while simultaneously maximizing the distance with other
samples [ 15,20,39]. However, this framework primarily focuses
on extracting consensus information in multi-view data, overlook-
ing the valuable complementary information present. Additionally,
the efficiency of the pair-wise contrastive strategy suffers as the
number of views increases, and the effectiveness of this strategy
diminishes due to less overlapped information between views (See
Table 5 for experimental analysis). Theoretical analysis by Trosten
et al. [25] supports these observations, highlighting that contrastive
alignment can reduce the number of separable clusters in the repre-
sentation space, with this effect worsening as the number of views
increases.
The missing view recovery framework, as depicted in 1b, is com-
monly adopted in IMVC approaches. Typically, a DNN is employed
to recover the missing view, either in the data or latent space. Subse-
quently, MVC methods or another view fusion network are utilized
for clustering based on the recovered views. However, the reliabil-
ity of the recovered views is a concern since the recovery ability
of DNNs relies on unsupervised training. Meanwhile in some in-
stances, [ 21] for example, missing views are recovered by a fused
embedding in the first stage, and subsequently used to generate
another fused embedding for clustering in the second stage, intro-
ducing unnecessary complexity and inefficiency to the pipeline.
We propose that a well-designed recovery-free method can achieve
comparable performance to recovery-based methods while offering
the advantages of simplicity and reduced computational overhead.
To address the aforementioned challenges, we propose a Unified
and Robust Representation Learning framework for Incomplete
Multi-View Clustering (URRL-IMVC). Our framework, depicted in
Fig 1c, is designed to be cross-view contrastive learning-free and
missing view recovery-free. First, to overcome the limitations of
cross-view contrastive learning, we propose a new framework that
fuses multi-view information into a unified embedding instead of
contrasting each viewâ€™s information. We achieve this by design-
ing an attention-based auto-encoder network, which captures bothconsensus and complementary information and intelligently fuses
them. Moreover, it is naturally scalable to different numbers of
views. Second, to tackle the issue of missing views, we aim to di-
rectly enhance the robustness of the unified embedding against
view-missing conditions without explicitly recovering the missing
views. We introduce two strategies to achieve this robustness. 1) We
treat view missing as a form of noise and draw inspiration from suc-
cessful applications of denoising and masked auto-encoders [ 12,28].
Our proposed approach randomly drops out existing views as a
form of data augmentation to simulate the view missing condition.
By reconstructing denoised input data from the unified embed-
ding and imposing constraints between the augmented and un-
augmented embeddings, we enhance the robustness of the unified
representation. 2) As the old saying goes, â€œOne cannot make bricks
without strawâ€, it is hard to learn to reconstruct a dropped-out view
directly. We introduce k-nearest neighbors (KNN) as additional
inputs, with a cross-view imputation strategy to fill in the missing
or dropped-out views, providing valuable hints for reconstruction.
We want to highlight that while previous methods have focused
on either fusing multi-view information [ 18,30] or incorporating
neighborhood information [ 23,26,32,38] for clustering, our ap-
proach represents one of the initial endeavors to fuse both aspects.
Finally, we conduct experiments based on this framework and make
incremental improvements to enhance clustering performance and
stability. Some of the key enhancements include the customization
of the Transformer-based Encoder to filter out noise and emphasize
critical information, and the introduction of the Clustering Module
to learn clustering-friendly representations.
To summarize, our main contributions are:
â€¢Unified: We propose a unified representation learning frame-
work that efficiently fuses both multi-view and neighbor-
hood information, allowing for better capturing of consensus
and complementary information while avoiding the limita-
tions of cross-view contrastive learning.
â€¢Robust: We proposed novel strategies, including KNN im-
putation and data augmentation, to directly learn a robust
representation capable of handling view-missing conditions
without explicit missing view recovery.
â€¢Improvements: Multiple incremental improvements are
introduced for better clustering performance and stability,
including the extra Clustering Module and the customization
of the Transformer-based Encoder.
â€¢Experiments: Through comprehensive experiments on di-
verse benchmark datasets, we demonstrate the state-of-the-
art performance of our unified representation learning frame-
work. Thorough ablation studies are also conducted to pro-
vide valuable insights for future research in this field.
2 Related works
Deep neural networks (DNNs) have shown good performance in
learning feature representation, which is beneficial for the IMVC
task. Various IMVC approaches have integrated DNNs into their
framework, denoted as DIMVC approaches. In terms of network
architecture, DIMVC approaches can be divided into four categories.
(1) Auto-encoder-based approaches [ 15,18â€“20]. These approaches
utilize auto-encoders to extract high-level features of each view,
 
2889URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) Cross-view contrastive learning framework
 (b) Missing view recovery framework
(c) Our unified and robust learning framework
Figure 1: A comparison between our learning framework and commonly used cross-view contrastive learning and missing view
recovery framework. The key difference lies in how the unified embedding for clustering is obtained. Our design (1c) directly
fuses multi-view information and utilizes KNN imputation and data augmentation to obtain unified and robust embedding
under view-missing conditions, avoiding the drawbacks of (1a) and (1b).
which are usually combined with contrastive learning or cross-view
prediction to handle the incompleteness problem. (2) Generative
network-based approaches. For the IMVC task, an intuitive solution
is to complete the missing views with generative models, trans-
forming it into an MVC task. Adversarial learning [ 11] is commonly
adopted by generative IMVC approaches including AIMVC Xu et al .
[35], PMVC-CG Wang et al . [29] , and GP-MVC Wang et al . [30]
to improve data distribution learning in the context of IMVC. (3)
Graph Neural Network-based (GNN-based) approaches [ 29,31].
These approaches aim to learn consensus representations from
the structure information contained in the graphs constructed for
each view. (4) Transformer [ 27] or attention-based approaches. The
Transformer network has gained attention in recent years due to
its successful application in various domains. Its architecture, along
with its Multi-head Attention mechanism, has been particularly
effective in capturing complex relationships. In the field of DIMVC,
RecFormer [ 21] proposed a Transformer auto-encoder with a mask
to recover missing views, while MCAC [ 40] and IMVC-PBI [ 16]
incorporated attention mechanisms into their frameworks. In this
paper, we leverage an auto-encoder architecture based on the Trans-
former framework to address the challenges of the IMVC task.3 The Proposed Method
Notations. An incomplete multi-view dataset with ğ‘samples and
ğ‘‰views is denoted as ğ‘‹={ğ‘‹(1),ğ‘‹(2),Â·Â·Â·,ğ‘‹(ğ‘‰)},ğ‘‹(ğ‘£)âˆˆRğ‘Ã—ğ‘‘ğ‘£,
whereğ‘‘ğ‘£denotes the dimension of ğ‘£-th view. The view missing
condition can be described by a binary missing indicator matrix
ğ‘€âˆˆ{0,1}ğ‘Ã—ğ‘‰, whereğ‘€ğ‘–ğ‘—=0indicates the ğ‘—-th view of the ğ‘–-th
sample is missing and ğ‘€ğ‘–ğ‘—=1just the opposite. An extra restriction
is imposed:Ã
ğ‘—ğ‘€ğ‘–ğ‘—â‰¥1, ensuring that at least one view is available
for each sample, which is essential for the clustering task.
3.1 Framework
Unlike many prior approaches in the field of MVC that employ
view-specific auto-encoders for each view, we propose a novel
framework using a unified auto-encoder that effectively fuses multi-
view data. The network architecture, depicted in Fig 2, consists of
three key modules: the Encoder ğ‘“, the Decoder ğ‘”, and the Clus-
tering Module â„. To provide a formal description, the framework
operates as follows. Given an incomplete multi-view data sample
ğ’™={ğ’™(1),ğ’™(2),Â·Â·Â·,ğ’™(ğ‘‰)},ğ’™(ğ‘£)âˆˆRğ‘‘ğ‘£from dataset ğ‘‹with its miss-
ing indicator vector ğ’âˆˆ{0,1}ğ‘‰, we apply KNN Imputation and
Data Augmentation (KIDA), as described in section 3.2, to obtain
 
2890KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ge Teng et al.
Figure 2: The overall architecture of URRL-IMVC. During training, the input data is augmented to simulate view-missing
conditions, and KNN Imputation provides hints for missing views, forming an input batch with both neighbor and view
dimensions. This batch is fed into the auto-encoder network, consisting of the Encoder (including the Neighbor Dimensional
Encoder and View Dimensional Encoder), the Decoder, and the Clustering Module. The Encoders fuse information from the
neighbor and view dimensions to generate a unified embedding. The Decoder reconstructs the augmented input, and the
Clustering Module produces clustering results. Additionally, an un-augmented embedding is obtained by passing the original
input data through the shared Encoders. Three loss functions, including Reconstruction loss, Robustness loss, and Clustering
loss, enhance robustness against view-missing conditions and encourage learning clustering-friendly embeddings.
the input for the auto-encoder network,
Â¯ğ’™,Â¯ğ’™â€²,Â¯ğ’,Â¯ğ’â€²=ğ¾ğ¼ğ·ğ´(ğ’™,ğ’,ğ‘‹,ğ‘€)
Â¯ğ’™(ğ‘£),Â¯ğ’™â€²(ğ‘£)âˆˆRğ‘˜Ã—ğ‘‘ğ‘£;Â¯ğ’,Â¯ğ’â€²âˆˆ{0,1}ğ‘˜Ã—ğ‘‰(1)
where Â¯ğ’™,Â¯ğ’is the data and mask after KNN Imputation, while Â¯ğ’™â€²,
Â¯ğ’â€²is the augmented version of Â¯ğ’™,Â¯ğ’, andğ‘˜is the hyperparameter
ğ‘˜in KNN. Note that though KNN Imputation is widely applied in
prior IMVC works, it is mainly used for recovering missing views,
which is different from our usage as a pre-process. Next, these
inputs are fed into the Encoder network to obtain the augmented
and un-augmented embeddings, denoted as ğ’›â€²andğ’›respectively,
ğ’›=ğ‘“(Â¯ğ’™,Â¯ğ’;ğœ½ğ¸),ğ’›â€²=ğ‘“(Â¯ğ’™â€²,Â¯ğ’â€²;ğœ½ğ¸);ğ’›,ğ’›â€²âˆˆRğ‘‘ğ‘’ (2)
where ğœ½ğ¸represents the Encoderâ€™s parameters and ğ‘‘ğ‘’is the dimen-
sion of the embedding. Then, the Decoder maps the augmented
embedding back to the data space to reconstruct the data sample,
Ë†ğ’™â€²=ğ‘”(ğ’›â€²;ğœ½ğ·),Ë†ğ’™â€²(ğ‘£)âˆˆRğ‘‘ğ‘£ (3)
where ğœ½ğ·represents the parameters of the Decoder. Simultaneously,
clustering is performed using the un-augmented embedding,
ğ’„=â„(ğ’›;ğœ½ğ¶),ğ’„âˆˆ[0,1]ğ‘‘ğ‘ (4)
where ğ’„is the clustering result, and represents the probabilities of
the data sample belonging to ğ‘‘ğ‘cluster centers. During training,
the loss function defined in equation 19 is computed to optimize
parameters ğœ½ğ¸,ğœ½ğ·,ğœ½ğ¶; During testing, ğ’„is regarded as the final
clustering result.In the following sections, we will introduce the Encoder module,
including its two submodules: the Neighbor Dimensional Encoder
(NDE) and the View Dimensional Encoder (VDE), the Decoder
module, and the Clustering Module respectively.
3.1.1 Neighbor Dimensional Encoder. KNN Imputation (Section 3.2)
provides additional information for missing views, but the retrieved
nearest neighbors may contain noise and be unreliable. To address
this issue, we propose the Neighbor Dimensional Encoder (NDE),
which is a series of customized Transformer Encoders [ 27], with
each one dedicated to a view to fuse its KNN input and filter out
noise, formulated as:
ğ’™ğ‘ğ·ğ¸={ğ’™(1)
ğ‘ğ·ğ¸,ğ’™(2)
ğ‘ğ·ğ¸,Â·Â·Â·,ğ’™(ğ‘‰)
ğ‘ğ·ğ¸},ğ’™(ğ‘£)
ğ‘ğ·ğ¸âˆˆRğ‘‘ğ‘£ (5)
ğ’™(ğ‘£)
ğ‘ğ·ğ¸=ğ‘“(ğ‘£)
ğ‘ğ·ğ¸(ğ¶ğ·ğ‘ƒğ¸(Â¯ğ’™(ğ‘£)),Â¯ğ’;ğœ½(ğ‘£)
ğ‘ğ·ğ¸)[0,:] (6)
Theğ‘£th Transformer Encoder corresponding to the ğ‘£th view
is represented with ğ‘“(ğ‘£)
ğ‘ğ·ğ¸(Â·;ğœ½(ğ‘£)
ğ‘ğ·ğ¸)in equation 6, and ğœ½(ğ‘£)
ğ‘ğ·ğ¸ is its
parameters. The input KNN sequence from the ğ‘£th view Â¯ğ’™(ğ‘£)is first
processed to add the Cosine Distance-based Positional Encoding
(CDPE), then it is passed through the Transformer Encoder with
the KNN mask Â¯ğ’. Finally, only the first vector from the output
sequence is chosen as the output, denoted as [0,:].
Below we introduce the two key customizations of the Trans-
former Encoders in NDE: the CDPE and the output choice.
Cosine Distance-based Positional Encoding (CDPE). The order or
distance of the KNN instances contains vital information regarding
the reliability of the inputs, with farther neighbors noisier and less
 
2891URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
reliable. To capture this information for the permutation invariant
Transformer structure, we introduce Positional Encoding (PE) to
provide this extra KNN order information. We explored various
positional encoding (PE) designs considering the data sources and
their combination with data. Among these configurations, concate-
nating cosine distance-based (inspired by Nguyen et al . [23] ) or
learnable PE with the input yielded the best results. For better in-
terpretability, Cosine Distance-based Positional Encoding (CDPE)
is chosen as our final design. The CDPE can be explained as,
ğ¶ğ·ğ‘ƒğ¸(Â¯ğ’™(ğ‘£))=Â¯ğ’™(ğ‘£)âŠ•ğ‘‘(Â¯ğ’™(ğ‘£)), ğ‘‘(Â¯ğ’™(ğ‘£))âˆˆRğ‘˜Ã—ğ‘˜(7)
in whichğ‘‘()is the function calculating the pair-wise distance of ğ‘˜
vectors and return a ğ‘˜Ã—ğ‘˜distance matrix, and âŠ•stands for matrix
concatenation. Given two input vectors ğ’™1andğ’™2, the pair-wise
cosine distance is formulated as,
ğ‘‘ğ‘ğ‘œğ‘ (ğ’™1,ğ’™2)=1âˆ’ğ’™1Â·ğ’™2
||ğ’™1||Â·||ğ’™2||(8)
We conjecture that CDPE is the most suitable for two reasons.
First, it contains the KNN distance information rather than simply
providing order information. Second, its value range is 0-2, which is
more stable than other distance functions, e.g., Euclidean distance.
Output choice. (Figure 3a) Generally, for fusing information with
a Transformer Encoder, an additional token like [CLS] can be added
[6]. However, in our unsupervised task, adding such a meaningless
token can introduce noise and lead to performance degradation.
Instead, we adopt the first vector of the output sequence. This
design not only avoids extra noise but also introduces a bias on the
first input. The first input is always the most reliable sample, i.e.,
the center sample for an existing view or the nearest neighbor for
a missing view. By introducing this bias, important information is
emphasized while fusing KNN information.
3.1.2 View Dimensional Encoder. The View Dimensional Encoder
(VDE) is designed to fuse view representations and obtain unified
embedding. As depicted in Figure 2, it consists of two parts, with
firstly a Feed-Forward Network (FFN) to map the representations of
different dimensions to the same latent space, and then followed by
a Transformer Encoder for fusion. The FFN consists of three fully
connected (FC) layers, without normalization or dropout layers,
which can be detrimental to the stability of training. The FFN of
VDE can be formulated as:
ğ’™ğ‘‰ğ·ğ¸âˆ’ğ¹=ğ’™(1)
ğ‘‰ğ·ğ¸âˆ’ğ¹âŠ•ğ’™(2)
ğ‘‰ğ·ğ¸âˆ’ğ¹âŠ•Â·Â·Â·âŠ• ğ’™(ğ‘‰)
ğ‘‰ğ·ğ¸âˆ’ğ¹,ğ’™ğ‘‰ğ·ğ¸âˆ’ğ¹âˆˆRğ‘‰Ã—ğ‘‘ğ‘’(9)
ğ’™(ğ‘£)
ğ‘‰ğ·ğ¸âˆ’ğ¹=ğœ(ğœ(ğ’™(ğ‘£)
ğ‘ğ·ğ¸ğ‘¾(ğ‘£)
1+ğ’ƒ(ğ‘£)
1)ğ‘¾(ğ‘£)
2+ğ’ƒ(ğ‘£)
2)ğ‘¾(ğ‘£)
3+ğ’ƒ(ğ‘£)
3(10)
In the equation,âŠ•represents the concatenate operation, ğœis the
activation function, ğ‘¾andğ’ƒare the weight matrix and bias vector
of the FC layer respectively.
The Transformer Encoder part of the VDE can be explained as,
ğ’›=ğ‘‰âˆ‘ï¸
ğ‘£=1ğ‘“ğ‘‰ğ·ğ¸âˆ’ğ‘‡(ğ’™ğ‘‰ğ·ğ¸âˆ’ğ¹,ğ‘‡ğ´ğ‘€(Â¯ğ’,ğ’);ğœ½ğ‘‰ğ·ğ¸âˆ’ğ‘‡)/ğ‘‰ (11)
in whichğ‘“ğ‘‰ğ·ğ¸âˆ’ğ‘‡(Â·;ğœ½ğ‘‰ğ·ğ¸âˆ’ğ‘‡)is the VDE Transformer Encoder struc-
ture. The view representations ğ’™ğ‘‰ğ·ğ¸âˆ’ğ¹are passed through the Trans-
former Encoder along with the generated Three-level Adaptive
Mask (TAM) ğ‘‡ğ´ğ‘€(Â¯ğ’,ğ’). The output sequence is averaged for
fusion. The Transformer Encoder in VDE is also customized butdifferent from that in NDE. The key difference lies in that views are
permutation invariant, i.e., changing the order of views should yield
the same output, while KNN has an order. Based on this difference,
the two key customizations of the VDE Transformer are designed
as:
Three-level Adaptive Masking (TAM). We employ a masking mech-
anism in VDE to emphasize the reliability of the inputs, instead
of the positional encoding used in NDE to maintain permutation
invariant. The self-attention in Transformer is formulated as,
ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„,ğ¾,ğ‘‰,ğ‘€ ğ´)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘„ğ¾ğ‘‡
âˆšï¸
ğ‘‘ğ‘˜+ğ‘€ğ´)ğ‘‰ (12)
whereğ‘€ğ´is the mask applied, with negative infinity for masking
and 0 for not. The input view representations can be roughly divided
into 3 categories based on data completeness: (1) complete, (2)
missing view with KNN imputation, and (3) missing view without
imputation. Therefore, instead of the original binary mask, we
design a Three-level Adaptive Mask (TAM) for the 3 categories, the
mask values range from completely unmasked (1) to fully masked
(3), with an intermediate level (2) in between, formulated as,
ğ‘€ğ´=ğ‘‡ğ´ğ‘€(Â¯ğ’,ğ’)=ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³0,ğ’ğ‘—=1
ğ›¾,ğ‘˜âˆ‘ï¸
ğ‘–=1Â¯ğ’ğ‘–ğ‘—>0 &ğ’ğ‘—=0
âˆ’âˆ,ğ‘˜âˆ‘ï¸
ğ‘–=1Â¯ğ’ğ‘–ğ‘—=0 &ğ’ğ‘—=0(13)
in which ğ’and Â¯ğ’are the original and KNN imputation generated
missing matrix respectively. ğ›¾is a negative hyperparameter to
control the emphasizing intensity.
Output choice. (Figure 3b) To ensure permutation invariance
and avoid bias towards any views, the embedding is generated by
averaging all Transformer output vectors. The output choices of
NDE and VDE are depicted in Figure 3 for intuitive understanding.
3.1.3 Decoder. The Decoder in our model is designed as a com-
pact 4-layer FFN, to reconstruct the input from the unified embed-
ding. Similar to the FFN in VDE, we removed its normalization and
dropout layer for better stability. Through our experiments, we have
observed that a deep and complex Decoder does not necessarily
improve the clustering performance and may even have negative
effects in certain cases. One possible explanation for this phenome-
non is that a shallow and simple Decoder serves as a regularization
technique on the embedding space, and prevents it from collaps-
ing. This regularization effect is similar to the Locality-preserving
Constraint proposed by Huang et al . [14] , which helps preserve the
local structure of the data. The process of Decoder is formulated as,
Ë†ğ’™={Ë†ğ’™(1),Ë†ğ’™(2),Â·Â·Â·,Ë†ğ’™(ğ‘‰)},Ë†ğ’™(ğ‘£)âˆˆRğ‘‘ğ‘£ (14)
Ë†ğ’™(ğ‘£)=ğ‘”(ğ‘£)(ğ’›;ğœ½(ğ‘£)
ğ·) (15)
3.1.4 Clustering Module. The auto-encoder we have designed ex-
tracts robust representations and captures the inherent structures
of data. However, these inherent structures may not necessarily
follow a cluster-oriented distribution. To enhance the clustering
performance, we introduce a Clustering Module inspired by DEC
 
2892KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ge Teng et al.
(a) Neighbor Dimensional Encoder
(b) View Dimensional Encoder
Figure 3: An intuitive visualization of the output choice of
the Neighbor Dimensional Encoder (NDE) and View Dimen-
sional Encoder (VDE). In NDE, the first vector of the output
sequence is chosen to provide a bias on the most reliable in-
put. In VDE, the outputs are averaged to provide an unbiased
representation of all views.
[34]. Below we describe its procedures. First, after auto-encoder pre-
training, a traditional clustering method is adopted to initialize the
cluster centers ğœ½ğ¶âˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘’from embedding ğ’›. Then, during each
iteration of joint training, a similarity matrix ğ’’âˆˆ[0,1]ğ‘Ã—ğ‘‘ğ‘is gen-
erated between ğœ½ğ¶and embedding ğ’›with Studentâ€™s t-distribution,
in whichğ‘ğ‘–ğ‘—represents the possibility sample ğ‘–belongs to cluster ğ‘—,
andğ’„âˆˆRğ‘‘ğ‘defined in equation 4 is a row of ğ’’. With the similarity
matrix ğ’’, we calculate the target distribution ğ’‘as,
ğ‘ğ‘–ğ‘—=ğ‘2
ğ‘–ğ‘—/ğ‘“ğ‘—
Ãğ‘‘ğ‘
ğ‘—=1(ğ‘2
ğ‘–ğ‘—/ğ‘“ğ‘—)(16)
in whichğ‘“ğ‘—is the soft cluster sizeÃğ‘âˆ’1
ğ‘–=0ğ‘ğ‘–ğ‘—. The training target is
the KL-Divergence between ğ’‘andğ’’(equation 22) which is defined
in Section 3.3.1. Finally, after joint training, the clustering result
can be obtained by finding the maximum possibility in each row of
ğ’’. The detailed training procedures are described in Section 3.3.2.
3.2 Data Augmentation and Imputation
In this section, we introduce Data Augmentation and KNN Imputa-
tion (ğ¾ğ¼ğ·ğ´(Â·)in equation 1), which are two key data processing
strategies of our approach for handling incomplete multi-view data.
3.2.1 KNN Imputation. The KNN search is conducted separately
in each view, taking into account the incomplete condition. Forexisting views, the KNN is directly obtained, while for missing
views, existing views from the same sample are used for searching
KNN. Specifically, for a missing view ğ’™(ğ‘£)
ğ‘–of sample ğ’™ğ‘–, we first
find all existing views ğ’™(ğ‘)
ğ‘–of the same sample. Then we iterate
through the KNN samples ğ’™(ğ‘)
ğ‘—of these existing views to check
ifğ’™(ğ‘£)
ğ‘—exists. If it does, we appended it to the KNN list of the
missing view. Finally, we select the top ğ‘˜samples from the KNN
list as the imputation for the missing view. If the length of the
KNN list is less than ğ‘˜, then the remaining positions are filled with
zeros. Along with the KNN imputation Â¯ğ’™, a missing indicator matrix
Â¯ğ’âˆˆRğ‘˜Ã—ğ‘‰is generated, where 1 represents a position filled with
a KNN sample and 0 represents a position filled with zeros. The
detailed procedures are listed in Algorithm 1, Appendix A.
3.2.2 Data Augmentation. Our framework is inspired by the de-
noising auto-encoder, which helps to learn robust representations
by introducing noise during training. Three types of noise are
designed, including Gaussian noise, random dropout, and view
dropout. Gaussian noise helps prevent overfitting by introducing
variability in the input data. Random dropout, functional as a regu-
larization technique, encourages the model to learn more robust
features by forcing it to rely on different subsets of the input data.
View dropout is a noise specifically designed for the IMVC task. It
randomly drops out one or more views from the input data during
training, encouraging the model to learn representations that are
more robust to view missing conditions.
Below are the formulations of the three kinds of augmentation
we used. By combining equation 17 and 18, the KIDA operation in
equation 1 is obtained.
Â¯ğ’™â€²,Â¯ğ’â€²=ğ¾ğ‘ğ‘ğ¼(ğ’™,ğ’âŠ™ğ’ğ‘‰ğ·,ğ‘‹,ğ‘€), ğ‘ƒ(ğ’ğ‘‰ğ·=0)=ğœ™1 (17)
Â¯ğ’™â€²=(Â¯ğ’™â€²+ğœ™2ğ’)âŠ™ğ’ğ‘…ğ·,ğ’âˆ¼N( 0,1), ğ‘ƒ(ğ’ğ‘…ğ·=0)=ğœ™3 (18)
The view dropout augmentation is shown in equation 17, where
random views are dropped out with a possibility of ğœ™1, before the
KNN Imputation ğ¾ğ‘ğ‘ğ¼(Â·)(Algorithm 1) step. The dropout mask
is applied with element-wise multiplication, denoted by âŠ™. The
masked views are regarded as missing views in both the KNN Impu-
tation and the auto-encoder network. After the KNN Imputation, in
equation 18, Gaussian noise is added to the input data to introduce
variability, whose intensity is controlled by hyperparameter ğœ™2.
After that, random values in the input are set to zero with a proba-
bility ofğœ™3, which is the random dropout augmentation. Finally, the
augmented input Â¯ğ’™â€²along with the corresponding missing indicator
matrix Â¯ğ’â€²can be used for training.
3.3 Training Strategy and Loss Function
3.3.1 Loss Function. In the training process, we utilize a combina-
tion of 3 loss functions formulated as follows:
ğ¿(ğ’™,Ë†ğ’™â€²,ğ’›,ğ’›â€²,ğ’„)=ğ¿ğ‘Ÿğ‘’ğ‘(ğ’™,Ë†ğ’™â€²)+ğœ†1ğ¿ğ‘ğ‘¢ğ‘”(ğ’›,ğ’›â€²)+ğœ†2ğ¿ğ‘ğ‘™ğ‘¢(ğ’„)(19)
ğ¿ğ‘Ÿğ‘’ğ‘corresponds to the reconstruction loss of the auto-encoder
for learning meaningful latent representations, formulated as:
ğ¿ğ‘Ÿğ‘’ğ‘(ğ’™,Ë†ğ’™â€²)=ğ‘‰âˆ‘ï¸
ğ‘£=1(||Ë†ğ’™â€²(ğ‘£)âˆ’ğ’™(ğ‘£)||2âŠ™ğ’ğ‘£) (20)
 
2893URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The missing indicator ğ’ğ‘£is element-wisely multiplied so only the
mean square errors of existing views are calculated. Note that dur-
ing training, the network output is Ë†ğ’™â€²as the input is the augmented
input Â¯ğ’™â€²(equation 2 and 3), so the network learns to reconstruct
dropped out views with KNN hints and cross-view correlation and
thus, learns to predict missing viewsâ€™ information implicitly.
ğ¿ğ‘ğ‘¢ğ‘”is the embedding robustness loss, which encourages the
learned representations to be consistent when augmentations are
applied, promoting the robustness of the learned representations,
formulated as:
ğ¿ğ‘ğ‘¢ğ‘”(ğ’›,ğ’›â€²)=âˆ’ğ‘™ğ‘œğ‘”ğ‘’âˆ’||ğ’›â€²
ğ‘–âˆ’ğ’›ğ‘–||
Ãğµ
ğ‘—=1ğ‘’âˆ’||ğ’›â€²
ğ‘–âˆ’ğ’›ğ‘—||(21)
Though our embedding robustness target is equivalent to mini-
mizing the distance between ğ’›â€²andğ’›(equation 2), we design ğ¿ğ‘ğ‘¢ğ‘”
based on cross-entropy loss within the training mini-batch. ğµin the
equation represents the training batch size. This design simultane-
ously minimizes the distance between ğ’›â€²
ğ‘–andğ’›ğ‘–and maximizes the
distances between ğ’›â€²
ğ‘–and embeddings of other samples ğ’›ğ‘—,ğ‘—â‰ ğ‘–,
preventing the embedding space from collapsing.
ğ¿ğ‘ğ‘™ğ‘¢is the DEC-based clustering loss [ 34] for the Clustering
Module (Section 3.1.4), which optimizes embeddings for clustering
with gradients from high-confidence samples.
ğ¿ğ‘ğ‘™ğ‘¢(ğ’„)=ğ¾ğ¿(ğ’‘â€²||ğ’’â€²)=ğµâˆ‘ï¸
ğ‘–=1ğ‘‘ğ‘âˆ‘ï¸
ğ‘—=1ğ‘â€²
ğ‘–ğ‘—ğ‘™ğ‘œğ‘”ğ‘â€²
ğ‘–ğ‘—
ğ‘â€²
ğ‘–ğ‘—(22)
It is formulated as the KL-divergence between distribution ğ’‘â€²and
ğ’’â€²computed from augmented input ğ’™â€²during training.
Hyperparameters ğœ†1andğœ†2control the balance between different
loss components. By jointly minimizing the three loss terms, our
network can learn representations that are both informative and
clustering-friendly.
3.3.2 Training Strategy. The training process is divided into two
stages. In the first stage, the auto-encoder is pre-trained using ğ¿ğ‘Ÿğ‘’ğ‘
andğ¿ğ‘ğ‘¢ğ‘”, focusing on learning robust representations. Once the
pre-training is complete, the Clustering Module is initialized. In the
second stage, ğ¿ğ‘ğ‘™ğ‘¢is added for joint training to learn a clustering-
friendly representation.
The training process is controlled by 2 hyperparameters: ğ¸ğ‘,
which represents the number of pre-training epochs, and ğ¸ğ‘—, which
represents the number of joint training epochs. For a detailed de-
scription of the training process, please refer to Algorithm 2 in
Appendix A.
4 Experiments
Please refer to Appendix B.1for the hyperparameter settings and
design details in our experiments.
4.1 Datasets and Metrics
Experiments were performed on 6 multi-view datasets varying in
number of views and modal to validate the effectiveness of our
method. The dataset characteristics are summarized in Table 1. We
report the widely used metrics Clustering Accuracy (Acc), Normal-
ized Mutual Information (NMI), and Adjusted Rand Index(ARI) as
results. We run each experiment 10 times and report the averagevalue and standard deviation (after Â±). Details about our experiment
and view-missing settings can be found in Appendix B.2.
4.2 Comparison with State-of-the-arts
We compare our approach with several state-of-the-art DIMVC
approaches listed in Table 2. Other comparisons about different
numbers of views, traditional IMVC methods, model parameters,
and computational costs can be found in Appendix B.3.
Comparison on different datasets. URRL-IMVC achieved state-
of-the-art performance on the 6 benchmark datasets, surpassing
most existing approaches, as indicated in Table 2. Our approach
consistently outperformed other SOTA methods across all evalu-
ation metrics, except for the BDGP dataset and NMI on Scene15
and Reuters, where our approach is sub-optimal. This excellent
clustering performance and stability can be attributed to our rep-
resentation learning framework, which effectively captures the
underlying data structure while remaining robust in the presence
of missing views. Additionally, URRL-IMVC exhibited stability com-
pared to other SOTA methods, with a relatively lower standard
deviation across 10 experiments, thanks to the tailored components
in the network to filter out noise. Notably, our approach excelled on
datasets with more views, such as Handwritten and Caltech101-7.
Together with experimental results in Table 5, it showed that our
framework successfully overcomes the drawbacks of cross-view
contrastive learning.
Comparison with different missing rates. As depicted in Figure
4, URRL-IMVC consistently outperformed other approaches, es-
tablishing an upper bound for clustering performance regardless
of the missing rate ( ğ‘šğ‘Ÿ). Our approach displayed better stability
compared to other methods, with a gradual decrease in accuracy as
the missing rate increased. In contrast, other approaches exhibited
more fluctuation, rendering their results less predictable. Notably,
DCP and Completer experienced a significant decline in perfor-
mance when the missing rate reached 0.75, as they only trained
their cross-view contrastive and recovery networks using complete
samples. Insufficient training samples led to unsatisfactory recovery
outcomes and fragile representations for clustering. In contrast, our
approach focused on the robustness of the unified representation,
allowing us to circumvent these limitations and achieve stable and
high performance across varying missing rates.
4.3 Ablation Studies
Unless otherwise specified, the experiments were conducted on
the Caltech101-7 dataset with the missing rate ğ‘šğ‘Ÿ=0.5. In certain
experiments, the Clustering Module was disabled to provide clearer
observations of specific phenomena. Additional ablation studies
Table 1: The statistic of 6 datasets used in our experiments.
Name
Views Clusters Samples Dimensions Type
Handwritten
[7] 6 10 2000 240/76/216/47/64/6 Image
Caltech101-7 [2] 5 7 1400 40/254/1984/512/928 Image
ALOI_Deep [21] 3 100 10800 2048/4096/2048 Image
Scene15 [2, 8] 2 15 4485 20/59 Image
BDGP [3, 24] 2 5 2500 1750/79 Image/Text
Reuters [1, 39] 2 6 18758 10/10 Text
 
2894KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ge Teng et al.
Table 2: Comparison of our method with state-of-the-art approaches on 6 benchmark datasets. The results are averaged on
missing rates ğ‘šğ‘Ÿ={0,0.25,0.5,0.75}. The best result is highlighted in bold while the sub-optimal is underlined .
Datasets Handwritten Calte
ch101-7 ALOI_Deep
Metrics A
cc(%) NMI(%) ARI(%) A
cc(%) NMI(%) ARI(%) A
cc(%) NMI(%) ARI(%)
Completer
[20] 52.19Â±5.14
54.67Â±3.60 28.77Â±4.72 62.89Â±8.02
57.75Â±6.50 41.05Â±9.33 44.53Â±2.53
75.45Â±1.18 26.47Â±2.10
DSIMVC [24] 75.76Â±4.04
71.32Â±2.77 63.17Â±4.11 70.06Â±3.95
59.56Â±2.58 52.06Â±3.52 72.58Â±2.24
91.21Â±0.67 70.15Â±2.10
SURE [39] 66.46Â±6.81
61.74Â±4.59 50.37Â±6.38 66.97Â±5.94
54.37Â±4.92 46.86Â±6.71 50.08Â±5.96
86.39Â±1.83 40.66Â±7.56
DCP [19] 59.80Â±6.32
62.73Â±3.92 45.40Â±8.21 55.26Â±10.75
54.44Â±9.59 40.75Â±13.03 56.89Â±5.34
86.82Â±2.01 50.66Â±9.00
CPSPAN [15] 84.19Â±5.43
81.49Â±2.65 76.06Â±4.79 82.07Â±4.41 73.04Â±4.19 68.55Â±5.73 73.01Â±4.27
92.40Â±1.07 71.79Â±3.78
RecFormer [21] 90.46Â±1.33 82.67Â±0.99 80.12Â±1.81 71.07Â±2.53
63.87Â±2.49 56.77Â±3.09 86.57Â±1.54 96.84Â±0.33 86.01Â±1.55
URRL-IMV
C (ours) 93.86Â±2.51
89.90Â±1.38 88.93Â±2.58 93.19Â±1.54
86.94Â±1.46 86.45Â±1.98 91.48Â±2.21
97.50Â±1.04 90.91Â±2.70
Datasets Scene15
BDGP Reuters
Metrics A
cc(%) NMI(%) ARI(%) A
cc(%) NMI(%) ARI(%) A
cc(%) NMI(%) ARI(%)
Completer
[20] 38.39Â±1.96 42.09Â±1.54 22.86Â±1.78 54.91Â±5.99
46.89Â±4.62 22.20Â±6.30 38.68Â±4.02
22.04Â±4.44 8.26Â±4.12
DSIMVC [24] 31.63Â±1.22
35.50Â±0.74 17.48Â±0.67 94.63Â±1.53
85.62Â±2.29 87.53Â±2.74 44.07Â±2.91 33.27Â±2.10 23.69Â±2.20
SURE
[39] 37.83Â±1.83
37.62Â±0.80 21.03Â±0.93 60.48Â±9.91
40.41Â±9.37 34.68Â±10.60 46.68Â±3.63 26.26Â±3.32
20.57Â±2.16
DCP [19] 38.28Â±1.63
41.69Â±1.23 22.22Â±1.70 50.98Â±5.90
44.50Â±6.50 18.67Â±6.87 38.60Â±3.29
21.79Â±4.84 7.12Â±3.80
CPSPAN [15] 37.71Â±2.33 41.38Â±2.04 22.68Â±1.84 76.93Â±9.26
63.25Â±7.62 59.91Â±10.32 39.78Â±2.02
14.55Â±2.07 12.47Â±1.54
RecFormer [21] 33.37Â±1.39
35.31Â±0.94 17.45Â±0.79 51.89Â±2.92
40.46Â±2.69 19.52Â±2.33 41.43Â±3.59
18.38Â±2.25 15.91Â±2.23
URRL-IMVC (ours) 41.18Â±1.77 41.87Â±0.95 24.09Â±1.09 89.15Â±5.07 77.14Â±6.25 77.64Â±8.03 48.63Â±2.64 28.94Â±1.65 24.78Â±2.59
(
a)
 (
b)
 (
c)
Figure 4: Comparison with state-of-the-art approaches under different missing conditions on the Caltech101-7 dataset. The
performance of each approach is reported using fold lines.
regarding detailed designs and hyperparameters (e.g., output choice,
ğ‘˜in KNN Imputation, view dropout probability) can be found in
Appendix B.4.
4.3.1 Ablation on Modules. In Table 3, we present the results of
our ablation study on the main modules we designed. First of all,
our Unified auto-encoder framework sets a solid baseline. Then,
our designed robustness strategies, KNN Imputation (KNNI), and
Data Augmentation (Aug) significantly improve clustering perfor-
mance and have approximately equal contributions. The Clustering
Module (CM) also plays a vital role in some datasets, by learn-
ing clustering-friendly representations. However, directly applying
it can result in unstable performance, as the DEC-based training
is sensitive to initialization. While our tailored components, i.e.,
CDPE&TAM help stabilize the learning. To summarize, the ablation
study on modules reveals that the KNN Imputation, Augmentation,
and Clustering Module are the three key components for improving
clustering performance, while CDPE&TAM is essential for stability.
4.3.2 Visualization. Figure 5 presents a T-SNE visualization of the
embeddings during one training process. Initially in Figure 5a, theTable 3: Ablation study on our designed modules. We begin
with the baseline model, which is a simple Transformer-
based auto-encoder. Then different combinations of modules
are incorporated to evaluate their contributions. â€œKNNIâ€:
KNN Imputation; â€œAugâ€: data augmentation and robustness
loss; â€œCDPE&TAMâ€: CDPE and TAM described in NDE and
VDE; â€œCMâ€: clustering module and clustering loss.
KNNI
Aug CDPE&TAM CM A
cc(%) NMI(%) ARI(%)
76.93Â±4.35 64.79Â±2.18
56.52Â±3.99
âœ“ 83.68Â±2.79 72.76Â±2.21
68.60Â±4.48
âœ“ 83.96Â±3.11
74.39Â±2.24 70.41Â±3.63
âœ“ 85.60Â±6.52
78.10Â±4.46 75.29Â±7.14
âœ“
âœ“ 84.77Â±3.59 74.92Â±2.54
71.76Â±3.69
âœ“ âœ“ 82.05Â±3.71
70.74Â±3.44 63.92Â±6.82
âœ“ âœ“ 89.95Â±0.74
80.01Â±0.96 79.55Â±1.19
âœ“ âœ“ 89.39Â±3.12
81.74Â±1.81 80.60Â±2.76
âœ“
âœ“ âœ“ 85.46Â±1.42 75.25Â±2.02
71.69Â±2.44
âœ“ âœ“ âœ“ 90.22Â±5.20
83.21Â±3.89 82.15Â±5.67
âœ“ âœ“ âœ“ 91.90Â±2.99 84.29Â±2.06 83.76Â±3.96
âœ“
âœ“ âœ“ âœ“ 93.35Â±0.37
86.50Â±0.61 86.25Â±0.64
 
2895URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) Raw data (38.43)
 (b) 200 iteration (73.43)
 (c) 1600 iteration (89.86)
(d) 2200 iteration (87.86)
 (e) 2400 iteration (90.14)
 (f) 4400 iteration (94.14)
Figure 5: T-SNE visualization of the embeddings during the training process on the Caltech101-7 dataset. The iteration number
and corresponding accuracy are recorded below each sub-figure. The training process consists of 4400 iterations, with the
Clustering Module initialized at 2200 iterations.
multi-view raw data are concatenated as embeddings, and the visu-
alization appears to be disorganized. After 200 iterations of training,
in Figure 5b, inherent structures start to be captured, and the pre-
training peak accuracy (89.86) occurs at 1600 iterations 5c. At 2200
iterations 5d, the Clustering Module is initialized, and joint training
with DEC-based clustering loss commences. The clusters become
more compact after 200 iterations of joint training, as depicted in
Figure 5e. Finally, at the end of the training, as shown in Figure
5f, the clusters become very compact, numerous samples initially
incorrectly clustered with low confidence are now corrected, and
the accuracy reaches 94.14.
5 Conclusion
In this paper, we proposed URRL-IMVC, a novel unified and robust
representation learning framework for the incomplete multi-view
clustering task. By leveraging an attention-based auto-encoder
framework, we successfully fuse the multi-view information into a
unified embedding, offering a more comprehensive solution com-
pared to potentially limiting cross-view contrastive learning. Through
the utilization of KNN imputation and data augmentation strate-
gies, we directly acquire robust embeddings that effectively handle
the view-missing condition, eliminating the need for explicit miss-
ing view recovery and its associated computation and unreliability.
Furthermore, incremental improvements, such as the Clustering
Module and customization of the Encoder, enhance the clustering
stability and performance, achieving state-of-the-art results. Thisimproved robust and unified representation learning framework
acts as a powerful tool for addressing the challenges of IMVC and
provides valuable insights for future research in this domain.
Acknowledgments
This work was supported by the Zhejiang Provincial Natural Sci-
ence Foundation of China under Grant No. LDT23F01013F01, and
by the Fundamental Research Funds for the Central Universities.
References
[1]Massih R. Amini, Nicolas Usunier, and Cyril Goutte. 2009. Learning from Multiple
Partially Observed Views - an Application to Multilingual Text Categorization.
InAdvances in Neural Information Processing Systems, Y. Bengio, D. Schuurmans,
J. Lafferty, C. Williams, and A. Culotta (Eds.), Vol. 22. Curran Associates, Inc.
[2]Xiao Cai, Feiping Nie, and Heng Huang. 2013. Multi-view k-means clustering on
big data. In Twenty-Third International Joint conference on artificial intelligence.
[3]Xiao Cai, Hua Wang, Heng Huang, and Chris Ding. 2012. Joint stage recognition
and anatomical annotation of drosophila gene expression patterns. Bioinformatics
28, 12 (2012), i16â€“i24.
[4]Guoqing Chao, Shiliang Sun, and Jinbo Bi. 2021. A Survey on Multiview
Clustering. IEEE Transactions on Artificial Intelligence 2, 2 (2021), 146â€“168.
https://doi.org/10.1109/TAI.2021.3065894
[5]Man-Sheng Chen, Jia-Qi Lin, Xiang-Long Li, Bao-Yu Liu, Chang-Dong Wang,
Dong Huang, and Jian-Huang Lai. 2022. Representation learning in multi-view
clustering: A literature review. Data Science and Engineering 7, 3 (2022), 225â€“241.
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 [cs.CL]
[7]Robert Duin. 2023. Multiple Features. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C5HC70.
[8]L. Fei-Fei and P. Perona. 2005. A Bayesian hierarchical model for learning natural
scene categories. In 2005 IEEE Computer Society Conference on Computer Vision
 
2896KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ge Teng et al.
and Pattern Recognition (CVPRâ€™05), Vol. 2. 524â€“531 vol. 2. https://doi.org/10.1109/
CVPR.2005.16
[9]Lele Fu, Pengfei Lin, Athanasios V Vasilakos, and Shiping Wang. 2020. An
overview of recent multi-view clustering. Neurocomputing 402 (2020), 148â€“161.
[10] Hang Gao, Yuxing Peng, and Songlei Jian. 2016. Incomplete Multi-view Clustering.
InIntelligent Information Processing VIII, Zhongzhi Shi, Sunil Vadera, and Gang
Li (Eds.). Springer International Publishing, Cham, 245â€“255.
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial
Nets. In Advances in Neural Information Processing Systems, Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.), Vol. 27. Curran
Associates, Inc.
[12] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.
2022. Masked Autoencoders Are Scalable Vision Learners. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 16000â€“
16009.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving Deep
into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.
InProceedings of the IEEE International Conference on Computer Vision (ICCV).
[14] Peihao Huang, Yan Huang, Wei Wang, and Liang Wang. 2014. Deep Embed-
ding Network for Clustering. In 2014 22nd International Conference on Pattern
Recognition. 1532â€“1537. https://doi.org/10.1109/ICPR.2014.272
[15] Jiaqi Jin, Siwei Wang, Zhibin Dong, Xinwang Liu, and En Zhu. 2023. Deep
Incomplete Multi-View Clustering With Cross-View Partial Sample and Prototype
Alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR). 11600â€“11609.
[16] Haobin Li, Yunfan Li, Mouxing Yang, Peng Hu, Dezhong Peng, and Xi Peng. 2023.
Incomplete Multi-view Clustering via Prototype-based Imputation. In Proceedings
of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-
23, Edith Elkind (Ed.). International Joint Conferences on Artificial Intelligence
Organization, 3911â€“3919. https://doi.org/10.24963/ijcai.2023/435 Main Track.
[17] Shao-Yuan Li, Yuan Jiang, and Zhi-Hua Zhou. 2014. Partial Multi-View Clustering.
Proceedings of the AAAI Conference on Artificial Intelligence 28, 1 (Jun. 2014).
https://doi.org/10.1609/aaai.v28i1.8973
[18] Fangfei Lin, Bing Bai, Kun Bai, Yazhou Ren, Peng Zhao, and Zenglin Xu.
2022. Contrastive multi-view hyperbolic hierarchical clustering. arXiv preprint
arXiv:2205.02618 (2022).
[19] Yijie Lin, Yuanbiao Gou, Xiaotian Liu, Jinfeng Bai, Jiancheng Lv, and Xi Peng.
2023. Dual Contrastive Prediction for Incomplete Multi-View Representation
Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 4
(2023), 4447â€“4461. https://doi.org/10.1109/TPAMI.2022.3197238
[20] Yijie Lin, Yuanbiao Gou, Zitao Liu, Boyun Li, Jiancheng Lv, and Xi Peng. 2021.
COMPLETER: Incomplete Multi-View Clustering via Contrastive Prediction. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 11174â€“11183.
[21] Chengliang Liu, Jie Wen, Zhihao Wu, Xiaoling Luo, Chao Huang, and Yong
Xu. 2023. Information Recovery-Driven Deep Incomplete Multiview Clustering
Network. IEEE Transactions on Neural Networks and Learning Systems (2023),
1â€“11. https://doi.org/10.1109/TNNLS.2023.3286918
[22] Xinwang Liu, Miaomiao Li, Lei Wang, Yong Dou, Jianping Yin, and En Zhu. 2017.
Multiple Kernel k-Means with Incomplete Kernels. Proceedings of the AAAI
Conference on Artificial Intelligence 31, 1 (Feb. 2017). https://doi.org/10.1609/aaai.
v31i1.10893
[23] Xuan-Bac Nguyen, Duc Toan Bui, Chi Nhan Duong, Tien D. Bui, and Khoa Luu.
2021. Clusformer: A Transformer Based Clustering Approach to Unsupervised
Large-Scale Face and Visual Landmark Recognition. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). 10847â€“10856.
[24] Huayi Tang and Yong Liu. 2022. Deep Safe Incomplete Multi-view Clustering:
Theorem and Algorithm. In Proceedings of the 39th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 162), Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
Sabato (Eds.). PMLR, 21090â€“21110.
[25] Daniel J. Trosten, Sigurd LÃ¸kse, Robert Jenssen, and Michael C. Kampffmeyer.
2023. On the Effects of Self-Supervision and Contrastive Alignment in Deep
Multi-View Clustering. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). 23976â€“23985.
[26] Wenxuan Tu, Sihang Zhou, Xinwang Liu, Xifeng Guo, Zhiping Cai, En Zhu,
and Jieren Cheng. 2021. Deep Fusion Clustering Network. Proceedings of the
AAAI Conference on Artificial Intelligence 35, 11 (May 2021), 9978â€“9987. https:
//doi.org/10.1609/aaai.v35i11.17198
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),
Vol. 30. Curran Associates, Inc.
[28] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
2008. Extracting and Composing Robust Features with Denoising Autoencoders.
InProceedings of the 25th International Conference on Machine Learning (Helsinki,Finland) (ICML â€™08). Association for Computing Machinery, New York, NY, USA,
1096â€“1103. https://doi.org/10.1145/1390156.1390294
[29] Qianqian Wang, Zhengming Ding, Zhiqiang Tao, Quanxue Gao, and Yun Fu. 2018.
Partial Multi-view Clustering via Consistent GAN. In 2018 IEEE International
Conference on Data Mining (ICDM). 1290â€“1295. https://doi.org/10.1109/ICDM.
2018.00174
[30] Qianqian Wang, Zhengming Ding, Zhiqiang Tao, Quanxue Gao, and Yun Fu.
2021. Generative Partial Multi-View Clustering With Adaptive Fusion and Cycle
Consistency. IEEE Transactions on Image Processing 30 (2021), 1771â€“1783. https:
//doi.org/10.1109/TIP.2020.3048626
[31] Yiming Wang, Dongxia Chang, Zhiqiang Fu, Jie Wen, and Yao Zhao. 2022. Graph
Contrastive Partial Multi-View Clustering. IEEE Transactions on Multimedia
(2022), 1â€“12. https://doi.org/10.1109/TMM.2022.3210376
[32] Zhongdao Wang, Liang Zheng, Yali Li, and Shengjin Wang. 2019. Linkage Based
Face Clustering via Graph Convolution Network. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR).
[33] Jie Wen, Zheng Zhang, Lunke Fei, Bob Zhang, Yong Xu, Zhao Zhang, and Jinxing
Li. 2023. A Survey on Incomplete Multiview Clustering. IEEE Transactions on
Systems, Man, and Cybernetics: Systems 53, 2 (2023), 1136â€“1149. https://doi.org/
10.1109/TSMC.2022.3192635
[34] Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016. Unsupervised Deep Embedding
for Clustering Analysis. In Proceedings of The 33rd International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 48), Maria Flo-
rina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, New York, USA,
478â€“487.
[35] Cai Xu, Ziyu Guan, Wei Zhao, Hongchang Wu, Yunfei Niu, and Beilei Ling. 2019.
Adversarial Incomplete Multi-view Clustering. In Proceedings of the Twenty-
Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. Inter-
national Joint Conferences on Artificial Intelligence Organization, 3933â€“3939.
https://doi.org/10.24963/ijcai.2019/546
[36] Jie Xu, Yazhou Ren, Huayi Tang, Xiaorong Pu, Xiaofeng Zhu, Ming Zeng, and
Lifang He. 2021. Multi-VAE: Learning Disentangled View-Common and View-
Peculiar Visual Representations for Multi-View Clustering. In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV). 9234â€“9243.
[37] Jie Xu, Huayi Tang, Yazhou Ren, Liang Peng, Xiaofeng Zhu, and Lifang He.
2022. Multi-Level Feature Learning for Contrastive Multi-View Clustering. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 16051â€“16060.
[38] Lei Yang, Dapeng Chen, Xiaohang Zhan, Rui Zhao, Chen Change Loy, and Dahua
Lin. 2020. Learning to Cluster Faces via Confidence and Connectivity Estima-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR).
[39] Mouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. 2023.
Robust Multi-View Clustering With Incomplete Information. IEEE Transactions
on Pattern Analysis and Machine Intelligence 45, 1 (2023), 1055â€“1069. https:
//doi.org/10.1109/TPAMI.2022.3155499
[40] Yanhao Zhang and Changming Zhu. 2023. Incomplete multi-view clustering via
attention-based contrast learning. International Journal of Machine Learning and
Cybernetics (2023), 1â€“17.
A Method Appendix
The detailed KNN Imputation algorithm is described in Algorithm
1, and the training procedure is described in Algorithm 2.
B Experiments Appendix
B.1 Implementation Details
We set most of the hyperparameters empirically with grid search,
and the same setting is used for all experiments if not specifically
mentioned. ğ›¾in equation 13 is set to -10. The hyperparameter ğ‘˜
in KNN Imputation is set to 4. ğœ™2andğœ™3, the data augmentation
hyperparameter in equation 18, are fixed at 0.05, while ğœ™1in equa-
tion 17 which controls view dropout possibility is set to be growing
with the actual missing rate of the dataset, defined as,
ğœ™1=ğœ–+(1âˆ’ğœ–)Ã—( 1âˆ’Ãğ‘
ğ‘–=0Ãğ‘‰
ğ‘—=0ğ‘€ğ‘–ğ‘—
ğ‘Ã—ğ‘‰)2(23)
in which we set ğœ–=0.15. The loss weight hyperparameters ğœ†1and
ğœ†2are set to 0.001 and 0.1 respectively. The embedding dimension
ğ‘‘ğ‘’is set to 256. Batch size ğµis set to 64 for both training and
 
2897URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1 Procedure of KNN Imputation
Input: Target ğ’™(ğ‘£)
ğ‘–, which is the ğ‘£th view of the ğ‘–th sample, dataset
ğ‘‹, missing indicator matrix ğ‘€, hyperparameter ğ‘˜.
1:ifğ‘€ğ‘–ğ‘£=1then
2: # The view exists
3: Return ğ’™(ğ‘£)
ğ‘–â€™s KNN
4:else
5: # The view is missing
6:ğ‘=0, create empty KNN list
7: whileğ‘<ğ‘˜do
8: # Traverseğ‘˜neighbors
9:ğ‘=1
10: whileğ‘<=ğ‘‰do
11: # Traverse all views
12: ifğ‘=ğ‘£orğ‘€ğ‘–ğ‘=0then
13: #ğ‘th view of target sample ğ’™ğ‘–is missing
14: pass
15: else
16: #ğ‘th view of target sample ğ’™ğ‘–exists
17: Findğ‘th nearest neighbor of ğ’™(ğ‘)
ğ‘–, denoted as ğ’™(ğ‘)
ğ‘—
18: ifğ‘€ğ‘—ğ‘£=0then
19: #ğ‘£th view of neighbor ğ’™ğ‘—is missing
20: pass
21: else
22: #ğ‘£th view of neighbor ğ’™ğ‘—exists
23: Addğ’™(ğ‘£)
ğ‘—to KNN list
24: end if
25: end if
26:ğ‘=ğ‘+1
27: end while
28:ğ‘=ğ‘+1
29: end while
30: ifKNN list length <ğ‘˜then
31: Pad KNN list with zeros to length ğ‘˜
32: else
33: Choose top ğ‘˜from KNN list
34: end if
35: Return KNN list
36:end if
Output: KNN Imputation Â¯ğ’™(ğ‘£)
ğ‘–
testing and the learning rate is fixed at 3e-4 throughout training. A
small weight decay of 4e-5 is used for less over-fitting. The training
epoch parameter ğ¸ğ‘(Section 3.3.2) is set to 100, 100, 15, and 50
respectively for the four datasets in Table 1 to maintain roughly
the same training iteration. As for ğ¸ğ‘—, we found that training with
DEC-based loss on some datasets (ALOI_Deep and Scene15 in this
paper) can diverge, possibly due to imbalanced cluster size. For
these datasets, we simply skip the second stageâ€™s joint training, i.e.,
ğ¸ğ‘—=0, while for other datasets ğ¸ğ‘—=ğ¸ğ‘.
PReLU [ 13] is used as the activation function in the VDE and
the Decoder. Dropout is not used in any modules of our network.Algorithm 2 Training process of URRL-IMVC
Input: Datasetğ‘‹, missing indicator matrix ğ‘€, hyperparameters.
1:Initialize model parameters ğœ½ğ¸,ğœ½ğ·, and ğœ½ğ¶. Pre-compute
KNN-search results. epoch = 0, iteration per epoch = ğ¼ğ‘’
2:whileğ‘’ğ‘ğ‘œğ‘â„ <ğ¸ğ‘do
3: # Stage 1: Pre-training
4: iteration = 0
5: whileğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› <ğ¼ğ‘’do
6: Pre-process: KNN Imputation and Data Augmentation
by equation 17 and 18, and obtain processed data Â¯ğ’™,Â¯ğ’™â€²
and processed mask Â¯ğ’,Â¯ğ’â€².
7: Forward: network forward by equation 2, 3, and obtain
embedding ğ’›â€²,ğ’›and reconstruction Ë†ğ’™â€²
8: Loss: Compute loss by equation 19, in which ğ¿ğ‘ğ‘™ğ‘¢is
ignored, i.e., ğœ†2=0.
9: Backward: Loss backward and update model parameters
ğœ½ğ¸andğœ½ğ·.
10: iteration = iteration + 1
11: end while
12: epoch = epoch + 1
13:end while
14:Initialize cluster centers ğœ½ğ¶.
15:whileğ‘’ğ‘ğ‘œğ‘â„ <ğ¸ğ‘+ğ¸ğ‘—do
16: # Stage 2: Joint Training
17: iteration = 0
18: whileğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› <ğ¼ğ‘’do
19: Pre-process: KNN Imputation and Data Augmentation
by equation 17 and 18, and obtain processed data Â¯ğ’™,Â¯ğ’™â€²
and processed mask Â¯ğ’,Â¯ğ’â€².
20: Forward: network forward by equation 2, 3, and obtain
embedding ğ’›â€²,ğ’›, reconstruction Ë†ğ’™â€², and clustering result
ğ’„â€²
21: Loss: Compute loss by equation 19.
22: Backward: Loss backward and update model parameters
ğœ½ğ¸,ğœ½ğ·, and ğœ½ğ¶.
23: iteration = iteration + 1
24: end while
25: epoch = epoch + 1
26:end while
Output: Model parameters ğœ½ğ¸,ğœ½ğ·,ğœ½ğ¶, final clustering result ğ’„
Agglomerative clustering with â€œwardâ€ linkage is used to initialize
cluster centers in the Clustering Module.
B.2 Datasets and Experiments Setting
Our chosen datasets vary in views (2â€“6), clusters (7â€“100), samples
(1400â€“18758), modal (image/text), and feature types (deep/hand-
crafted), providing a comprehensive evaluation of approaches. Two
parameters missing number ğ‘šğ‘›and missing rate ğ‘šğ‘Ÿare defined
to control the missing conditions. We first select ğ‘Ã—ğ‘šğ‘Ÿsamples
as incomplete samples, then randomly select ğ‘šğ‘›views of each
incomplete sample as missing views. We fix ğ‘šğ‘›and varyğ‘šğ‘Ÿin
our experiments, ğ‘šğ‘›are fixed at 4, 3, 2, 1, 1, 1 for the 6 datasets
respectively. Importantly, it should be noted that within the same set
 
2898KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ge Teng et al.
Table 4: The ablation study on view dropout augmentation probability ğœ™1from equation 17. We use grid search to determine
the best value range of ğœ™1under different missing rates ğ‘šğ‘Ÿand try to design a mapping function from the actual missing rate
to the desired ğœ™1. Generally speaking, a larger missing rate requires a larger view dropout probability for augmentation, the ğœ™1
value from the designed mapping function, equation 23, is listed in the last column of the table.
Parameter ğœ™1=0ğœ™1=0.15ğœ™1=0.3ğœ™1=0.45ğœ™1=0.6 Equation
23
ğ‘šğ‘Ÿ=0.00 89.30Â±1.86
89.50Â±1.77 88.91Â±1.89 89.71Â±1.19 86.94Â±2.12ğœ™1=0.15
ğ‘šğ‘Ÿ=0.25 86.15Â±2.42
88.12Â±1.90 88.56Â±1.55 86.51Â±3.33
83.57Â±4.74ğœ™1=0.17
ğ‘šğ‘Ÿ=0.50 83.35Â±1.94 86.12Â±1.57 85.61Â±2.08 83.25Â±3.86
82.25Â±4.82ğœ™1=0.23
ğ‘šğ‘Ÿ=0.75 81.41Â±2.79
81.78Â±3.86 82.89Â±4.51 80.82Â±3.58
77.17Â±5.63ğœ™1=0.32
ğ‘šğ‘Ÿ=1.00 76.31Â±3.00
77.53Â±3.63 77.02Â±3.91 80.21Â±2.94 79.19Â±3.67ğœ™1=0.46
Table 5: Comparison between our approach and cross-
view contrastive learning-based approach (CPSPAN) on
Caltech101-7 dataset with different numbers of views. The
best results of CPSPAN are achieved with 4 views, while with
5 views for our approach.
Vie
ws CPSP
AN [15] URRL-IMV
C (ours)
A
cc(%) NMI(%) ARI(%) A
cc(%) NMI(%) ARI(%)
2 50.88Â±1.87
45.27Â±2.50 35.79Â±2.25 58.36Â±3.01
47.16Â±2.50 39.40Â±2.71
3 73.17Â±4.27
61.40Â±4.29 55.37Â±5.46 77.60Â±0.88 67.61Â±0.98
63.97Â±1.33
4 84.89Â±2.15
75.37Â±2.45 71.79Â±3.26 91.73Â±0.47 83.57Â±0.68 83.26Â±0.76
5 77.62Â±4.74 69.70Â±4.04 63.23Â±5.51 92.95Â±2.60
86.29Â±1.76 86.02Â±2.91
of experiments, we ensured that the input data and missing indicator
matrix remained consistent across different methods, ensuring fair
comparisons. For comparison with state-of-the-art methods in Table
2 we reproduce the results with their published code. Several prior
works are difficult to adapt to different numbers of views, which
could hinder real applications. We randomly select views when the
dataset has more views than the model requires.
B.3 Comparison with State-of-the-art Methods
B.3.1 Comparison with a different number of views. As we men-
tioned in the introduction, the effectiveness of the cross-view con-
trastive learning strategy diminishes due to less overlapped infor-
mation between views. Observing from Table 5, adding more views
may harm the clustering performance of the cross-view contrastive
learning-based approach, proving this point of view, and also being
consistent with the theoretical analysis from [ 25]. On the other
hand, our approach stably benefits from more views in the dataset,
overcoming this drawback.
B.4 Ablation Studies
B.4.1 Ablation on Output Choice. We conducted the ablation test
in Table 6 to find the best output choice of both the Neighbor Di-
mensional Encoder (NDE, section 3.1.1) and the View Dimensional
Encoder (VDE, section 3.1.2). It can be observed that choosing the
first vector of the Transformer output sequence significantly out-
performs other choices, and using the latter output vectors results
in worse and worse performance. It is consistent with our point of
view that NDE needs a bias on the most confident input (the center
sample or the nearest neighbor), and further neighbors contain
more noise to harm the final performance. For VDE the situation is
different, using the average of all output vectors outperforms other
choices, which is consistent with our point of view that VDE needsTable 6: Ablation test on the output choice of VDE and NDE.
"Mean" represents using the average of all output vectors
from the Transformer as output. The 1ğ‘ ğ‘¡represents using the
first output vector, 2ğ‘›ğ‘‘represents the second output vector,
and so on. "Concat+Linear" represents first concatenating
the output vectors and then using a linear layer to map the
new vector to the desired dimension.
Output/Mo
dule NDE
VDE
1ğ‘ 
ğ‘¡93.36Â±0.89 89.73Â±3.26
2ğ‘›
ğ‘‘91.87Â±1.01
88.75Â±3.56
3ğ‘Ÿğ‘‘90.47Â±3.15
87.23Â±3.67
4ğ‘¡â„89.36Â±4.33
89.94Â±4.15
5ğ‘¡â„-
87.14Â±3.88
Mean 90.09Â±3.61 93.36Â±0.98
Concat+Linear 83.23Â±7.16
91.13Â±3.38
to be unbiased. Concatenation with linear layer does not perform
well in both Encoders, possibly due to lack of supervision.
B.4.2 Ablation on View Dropout Probability ğœ™1.We conducted a
grid search to determine the best value range of view dropout
augmentation probability ğœ™1under different missing rates ğ‘šğ‘Ÿ, and
the results are shown in Table 4. For view complete condition,
ğœ™1<0.6have similar performance, while for view incomplete
condition, the desired ğœ™1ascends as the missing rate ğ‘šğ‘Ÿincreases.
According to this observation, we designed the mapping function
in equation 23 to follow this ascending trend, and its value is listed
in the last column of the table.
Table 7: Ablation test on the hyperparameter ğ‘˜in KNN im-
putation. The result is unimodal with the best ğ‘˜=4. Largerğ‘˜
values tend to provide more stable results (smaller standard
deviation).
ğ‘˜=1ğ‘˜=2ğ‘˜=4ğ‘˜=8ğ‘˜=16
83.84Â±3.38
84.84Â±2.82 87.31Â±2.01 87.00Â±2.46 86.01Â±1.28
B.4.3 Ablation on hyperparameter ğ‘˜for KNN. We conduct an ab-
lation study on ğ‘˜in KNN Imputation (3.2.1) to examine its effect.
A large increment can be observed comparing ğ‘˜=4withğ‘˜=1.
However, the performance starts to drop as ğ‘˜>4, which we infer
can be caused by the noise brought by further neighbors. On the
other hand, larger ğ‘˜also seems to benefit the stability of clustering.
 
2899