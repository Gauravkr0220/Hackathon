RJUA-MedDQA: A Multimodal Benchmark for Medical Document
Question Answering and Clinical Reasoning
Congyun Jin‚àó
Ant Group
Shanghai, China
jincongyun.jcy@antgroup.comMing Zhang‚àó
Shanghai Jiao Tong University
School of Medicine Affiliated
Renji Hospital
Shanghai, China
zhangming@renji.comWeixiao Ma
Shanghai Jiao Tong University
School of Medicine Affiliated
Renji Hospital
Shanghai, China
maxiaowei@renji.comYujiao Li
Ant Group
Shanghai, China
liyujiao.lyj@antgroup.com
Yingbo Wang
Ant Group
Shanghai, China
wangyingbo.wyb@antgroup.comYabo Jia
Ant Group
Shanghai, China
jiayabo.jyb@antgroup.comYuliang Du
Ant Group
Shanghai, China
duyuliang.dyl@antgroup.comTao Sun
Ant Group
Shanghai, China
suntao.sun@antgroup.com
Haowen Wang
Ant Group
Shanghai, China
wanghaowen.whw@antgroup.comCong Fan
Ant Group
Shanghai, China
fancong.fan@antgroup.comJinjie Gu
Ant Group
Hangzhou, China
jinjie.gujj@antgroup.comChenfei Chi
Shanghai Jiao Tong University
School of Medicine Affiliated
Renji Hospital
Shanghai, China
chichenfei@renji.com
Xiangguo Lv
Shanghai Jiao Tong University
School of Medicine Affiliated
Renji Hospital
Shanghai, China
chnlvsc@163.comFangzhou Li
Shanghai Jiao Tong University
School of Medicine Affiliated
Renji Hospital
Shanghai, China
renjilfzh@163.comWei Xue
Shanghai Jiao Tong University
School of Medicine Affiliated
Renji Hospital
Shanghai, China
xuewei@renji.comYiran Huang
Shanghai Jiao Tong University
School of Medicine Affiliated
Renji Hospital
Shanghai, China
huangyiran@renji.com
ABSTRACT
Recent advancements in Large Language Models (LLMs) and Large
Multi-modal Models (LMMs) have shown potential in various med-
ical applications, such as Intelligent Medical Diagnosis. Although
impressive results have been achieved, we find that existing bench-
marks do not reflect the complexity of real medical reports and spe-
cialized in-depth reasoning capabilities. In this work, we establish
a comprehensive benchmark in the field of medical specialization
and introduced RJUA-MedDQA, which contains 2000 real-world
Chinese medical report images poses several challenges: compre-
hensively interpreting imgage content across a wide variety of
challenging layouts, possessing the numerical reasoning ability to
identify abnormal indicators and demonstrating robust clinical rea-
soning ability to provide the statement of disease diagnosis, status
and advice based on a collection of medical contexts. We carefully
design the data generation pipeline and proposed the Efficient Struc-
tural Restoration Annotation (ESRA) Method, aimed at restoring
textual and tabular content in medical report images. This method
‚àóEqual Contribution
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671644substantially enhances annotation efficiency, doubling the produc-
tivity of each annotator, and yields a 26.8% improvement in accuracy.
We conduct extensive evaluations, including few-shot assessments
of 5 LMMs which are capable of solving Chinese medical QA tasks.
To further investigate the limitations and potential of current LMMs,
we conduct comparative experiments on a set of strong LLMs by
using image-text generated by ESRA method. We report the perfor-
mance of baselines and offer several observations: (1) The overall
performance of existing LMMs is still limited; however LMMs more
robust to low-quality and diverse-structured images compared to
LLMs. (3) Reasoning across context and image content present sig-
nificant challenges. We hope this benchmark helps the community
make progress on these challenging tasks in multi-modal medical
document understanding and facilitate its application in healthcare.
Our dataset will be publicly available for noncommercial use at
https://github.com/Alipay-Med/medDQA_benchmark.git
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíMachine learning; ‚Ä¢Informa-
tion systems‚ÜíData mining.
KEYWORDS
Large Multi-modal Model, Medical Dataset, Benchmark, Visual
Question Answering, Medical Document Understanding, Contex-
tual Reasoning
5218
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Congyun Jin et al.
ACM Reference Format:
Congyun Jin, Ming Zhang, Weixiao Ma, Yujiao Li, Yingbo Wang, Yabo
Jia, Yuliang Du, Tao Sun, Haowen Wang, Cong Fan, Jinjie Gu, Chenfei Chi,
Xiangguo Lv, Fangzhou Li, Wei Xue, and Yiran Huang. 2024. RJUA-MedDQA:
A Multimodal Benchmark for Medical Document Question Answering and
Clinical Reasoning . In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671644
1 INTRODUCTION
Breakthroughs in large language models (LLMs) are producing
generalist AI models that can solve a variety of medical challenges,
such as the automated diagnostic agent [ 10,20]. Large multimodal
models (LMMs) aim to achieve even stronger general intelligence
by augmenting LLMs with multimodal inputs, such as Medical
Imaging Analysis [ 24,33]. Consequently, advances in multimodal
vision-language research can help support clinical decision making,
improve patient engagement, reduce the burden on the healthcare
system and increase the efficiency of medical staff [16].
Although these efforts have led to remarkable progress, little
attention has been paid to understanding medical reports, despite
its great potential for practical use, as medical report images are
proving to be a powerful tool that provides a clear, concise, and
accessible way to convey important clinical information, including
the patient‚Äôs basic information, examination details, abnormal test
indicators, and disease diagnoses.
In general, there are several available datasets with document-
based VQA or reasoning-based VQA. Despite these important ad-
vances, there are limitations: (1) In the currently available document-
based datasets [ 8,23,27], most do not require reasoning skills. A
few require discrete reasoning based on the image content such as
TAT-DQA [ 36] A dataset for contextual reasoning is missing. (2)
The currently available document VQAs and contextual reasoning
datasets lack real-world medical visual documents and specialized
medical data relevant to disease diagnosis, status, and treatments.
Meanwhile, current LMM models still struggle to understand the
information in a real-world image, which is usually misaligned,
skewed and blurred. In addition, compared to humans, large LMMs
are still unable to make numerical or contextual inferences with
successive logical connections within the whole image.
Based on these observations, we introduce a new benchmark,
RJUA-MedDQA, which was extracted from the database of the
Urology Department of Shanghai Renji Hospital. The raw medi-
cal data was then processed and anonymized to create synthetic,
multimodal clinical data for virtual patients, ensuring that medical
confidentiality and patient privacy are not compromised. While ex-
isting benchmarks mainly concentrate on basic question-answering
or image-based reasoning, RJUA-MedDQA challenges models to
perform complex contextual reasoning, mirroring the diagnostic
process of real-world doctors. We hope that this benchmark will
help bridge the gap between academic research and practical scenar-
ios to facilitate future studies on understanding medical document
understanding. The dataset is collaboratively constructed by a team
of urological experts from Shanghai Renji Hospital. It includes over
2,000 images of real medical reports with a series of complex lay-
outs containing both textual and tabular information (Figure 1).
Figure 1: Complex Page Layouts in Various Medical Report Cate-
gories: Four Illustrative Examples
RJUA-MedDQA is a high-level document comprehension task in
which a visually rich medical report and a relevant natural language
question are given, which may be linked to a set of facts or evidence
from official medical guidelines or clinical experience. The model
is required to provide the correct answer to the question based on
the image.
Our main contributions can be summarized as follows:
‚Ä¢The Largest Benchmark for Medical Reports in Chinese:
RJUA-MedDQA is a comprehensive benchmark for understand-
ing visual medical reports in Chinese with a focus on urology. To
the best of our knowledge, this is also the largest VQA dataset for
medical reports in the real world. We decided to publish a part
of the dataset with high-quality OCR results and annotations.
In addition, we define three tasks that correspond to different
application scenarios. The primary goal of RJUA-MedDQA is
to improve LMMs to understand medical reports. This enables
accurate interpretation of content across different layouts and
strong logical reasoning given a list of medical knowledge.
‚Ä¢Large Layout Variability: The dataset includes a variety of
image types, including photographs, scanned PDFs and screen-
shots, which are characterized not only by their diverse and
complex layouts from numerous public sources, but also by vary-
ing image quality. In particular, the photographed images and
screenshots may show signs of degraded quality due to factors
such as rotation, skew, blurring of text or missing information,
highlighting the challenges in real-world scenarios.
‚Ä¢Efficient Structural Restoration Annotation (ESRA): In con-
trast to conventional layout annotation methods, we propose an
efficient structure-aware annotation method to reconstruct both
textual and tabular content in medical reports. It significantly
reduces human labeling errors and improves the efficiency of
5219RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Comparison of VQAs. Answer types can be broken down into single-span(SS), multi-span(MS), and non-span(NS). "S/P" denotes the
"Scanned/Photographed" modality of images.
Dataset Domain #ImagesImage
TypeImages Modal
Typew/o Context
Reasoningw/ Context
Reasoning#QAsAnswer
Type
DocVQA General 12k S Document 50k SS
TAT-DQA Finance 2k S Document ‚úì 1.4k SS,MS,NS
InfographicVQA General 5k S Document ‚úì 30k SS,MS,NS
SlideVQA General 52k S Document ‚úì 14.5k SS,MS,NS
MultiModalQA General 57k S Document ‚úì 30k SS,MS,NS
ScienceQA General 6.5k S Non-document ‚úì 6.5k NS
A-OKVQA General 2.9k S Non-document ‚úì 2.9k NS
SLAKE Medical 642 S Non-document ‚úì 14k NS
RJUA-MedDQA Medical 2k S/P Document ‚úì ‚úì 72k SS,MS,NS
the annotation process. Statistically, this method has increased
the accuracy rate from 70% to 96.8%.
‚Ä¢Synonym-aware Automatic QA Generator: Based on the
ESRA method, we have integrated an synonym-aware auto-
matic QA generator with an extensive range of templates that
can be used to process tasks ranging from simple fact to more
complex inference-based questions. In addition, these templates
are highly flexible and can be customized based on given anno-
tations, allowing for fine-tuning or evaluation of the models for
specific research inquiries.
‚Ä¢Clinical Expert Annotation: The dataset is also precisely
annotated by urology specialists for contextual reasoning task.
This dataset also provides a fact base that contains a logical chain
to diagnose the disease and its stage, as well as treatment and
advises, mainly extracted from clinical experience and official
Urological Disease Diagnosis and Treatment Guideline [ 12].
This fact base attempts to mitigate the gap between urological
disease diagnosis and research communities.
2 RELATED WORK
In this section, we provide a brief overview of previous research in
the area of datasets for Document VQA (DQA), Reasoning over VQA
and Medical VQA, with a particular focus on the work most related
to ours. The characteristics of these datasets and the comparison
with ours are shown in Table 1.
Document VQA Document VQA is a high-level document un-
derstanding task in which a model must answer a question in natu-
ral language using a visually rich document. Several useful datasets
have already been published. DocVQA was created using different
types of industrial documents for extractive question answering,
where the answers can always be extracted from the text of the
document [ 23]. The VisualMRC dataset was created for abstract
answering of questions where the answers cannot be extracted
directly from the text of the document [ 31]. Meanwhile, there are
more and more datasets that go beyond extractive question answer-
ing and increasingly focus on inference capabilities over a given
image, including InfographicVQA [ 22], SlideVQA [ 30], TAT-DQA
[36], and MultiModalQA [ 29]. All of these datasets require single-
hop or multi-hop inference capabilities. InfographicVQA focuses on
infographics instead of documents; SlideVQA focuses on slide decks
instead of documents; the TAT-DQA dataset includes real-world,high-quality business documents and MultiModalQA requires joint
reasoning about textual and visual information in Wikipedia, while
our RJUA-MedDQA dataset includes real-world medical documents
of varying quality.
Contextual Reasoning VQA The previous task assumes that
the datasets contain a relevant image that contains all the facts
needed to answer the question. In the Contextual Reasoning VQA
task, the image may not contain all the facts to answer a ques-
tion. Instead, a rationale or paragraph is given to supplement the
knowledge for each question: Context-VQA [ 25], ScienceQA [ 21]
and A-OKVQA [ 28]. In Context-VQA, context refers to the type
of website an image comes from, which is different from our use
of the term to describe additional data that helps answer visual
questions. The context provided in ScienceQA, on the other hand,
is artificially generated. However, all existing contextual reasoning
VQAs contain visual or simple textual information, but no layout
information.
Medical VQA There are several publicly available medical VQA
datasets that are up-to-date. VQA-MED-2018 [ 9], VQA-RAD [ 14],
VQAMED-2019 [ 4], RadVisDial [ 13], PathVQA [ 11], VQA-MED-
2020 [ 3], SLAKE [ 18], and VQA-MED-2021 [ 5]. The selected datasets
differ in terms of imaging modality and question categories. The
imaging modality of these datasets includes chest X-ray, CT, MRI
and pathology. The questions include closed questions and open
questions on a variety of topics [ 17]. SLAKE is most similar to us
as it is a comprehensive dataset containing both semantic labels
and a structural medical knowledge base. The medical knowledge
base is represented as a knowledge graph instead of being specific
to each question. Furthermore, all of these medical datasets consist
of visual images only.
Therefore, we introduce RJUA-MedDQA for document content
understanding and contextual diagnosis. Our goal is to advance
research in multimodal document VQA in the academic community
and to support the medical community in developing applications
that improve their clinical reasoning capabilities.
5220KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Congyun Jin et al.
Figure 2: Statistics of Report Types
3 THE DATASET
3.1 Overview
The RJUA-MedDQA dataset contains a total of 2000 images, of
which 402 are screenshots, 619 are scanned PDFs, and the remain-
ing 979 are photos taken by patients during real doctor visits. Re-
ports in screenshot and scanned PDF format ensure the integrity
and clarity of the information. On the other hand, reports taken
with photos may show some degree of quality degradation, such as
rotated or skewed angles, blurred text, or incomplete information,
indicating real problems. Medical reports can be divided into two
main categories, namely laboratory reports and clinical reports. Fig-
ure 2 shows the overall frequency and distribution of the different
report types. Overall, there are 386 low-quality images based on
the criteria of low quality image in Appendix B, shown in Table 2
Table 2: Statistics of Image Type
Image Type Quality # Images # Key-value pairs
PhotoHigh 5937603Low 386
PDF High 619 6895
Screenshot High 402 2351
Overall 2000 16849
Laboratory reports: mainly include a complete blood count
(CBC), liver function tests (LFTs), urinalysis and urological screen-
ing tests such as prostate specific antigen (PSA), which include
a well-structured table and some relevant text. Tables are highly
variable and range from single-column to multi-column formats
with or without lines. They primarily convey laboratory results
and the corresponding reference range.
Clinical reports: mainly include MRI, ultrasound, pathology,
digital radiography (DR), plain CT, contrast-enhanced CT (CECT)
and other diagnostic modalities such as PET-CT, PET-MRI, en-
doscopy report and renogram.
Knowledge Fact Base: The context base provides essential
medical knowledge to support clinical reasoning task within the
domain of urology and laboratory. Every context is made up of
three components: Title, Type andContent.Title It refers to the name or label given to a specific disease,
examination finding, or health-related topic, such as "Mild
Anemia" or "Renal Cysts Treatment"
Type It refers to the type of a context. For example, "Disease-
Advice" offers advice for controlling or alleviating a health
issue, and "Disease-Treatment" provides specifics on the
method to addressing a medical disease.
Content It offers detailed medical knowledge to arrive at the conclu-
sion mentioned in the title, such as the criteria for diagnosing
"Mild Anemia".
Overall, the research team compiled statistics for the year 2023
from the urology department of Renji Hospital in Shanghai, which
included initial diagnoses of urological diseases in 319,401 patients.
They performed 934,675 major laboratory tests and a total of 401,615
examinations and pathological assessments. The dataset includes 95
(48/50) of the 50 most common laboratory tests and 98% (18/20) of
the 20 most common examinations and pathology procedures. In ad-
dition, the dataset covers over 334 different diseases and diagnoses.
Detailed statistics can be found in the Appendix A. Characteris-
tically, these reports vary widely in their formatting and often
contain a wide range of medical terminology and related synonyms,
resulting in a high degree of variability and generalizability.
3.2 Task Overview and Definition
We introduce the RJUA-MedDQA dataset for the problem of medical
report understanding and question answering, where models must
have the ability to interpret textual and tabular content in images,
as well as the ability to draw conclusions from a given context.
Considering a medical report ùê∑, which contains textual content
and possibly a table, we propose two main tasks: (1) Image Content
Recognition VQA (Without Context); (2) Clinical Reasoning VQA
(With Context)
3.2.1 Task 1: Image Content Recognition VQA (Without Context).
This task tests the models‚Äô ability to accurately extract the content
presented in medical reports, which includes both textual and tab-
ular data given a question ùëÑ. Formally, the task is formulated as
follows
F(ùê∑,ùëÑ)=ùëé (1)
Subtask 1 Entity Recognition This involves accurately extracting key
information, such as age, examination descriptions and con-
clusions.
Subtask 2 Table Interpretation This requires the model to parse tabular
data within laboratory reports (e.g. test results and reference
intervals).
Subtask 3 Table Numerical Reasoning This requires the model to apply
quantitative reasoning to identify and interpret abnormal
indicators of laboratory reports.
3.2.2 Task 2: Clinical Reasoning VQA (With Context). This task
requires models to not only precisely extract details from the images
but also to apply advanced clinical diagnostic abilities. Models must
integrate the information from the reports with relevant medical
knowledge (context) to support their reasoning. Given a question
ùëÑ, and a piece of context ùê∂containing the golden facts and the
distracting facts necessary to answer ùëÑ, which is formulated as
5221RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Figure 3: Data Generation Pipeline
follows
F(ùê∑,ùê∂,ùëÑ)=ùëé (2)
Subtask 1 Disease Diagnosis This requires the model to perform disease
diagnosis based on abnormal indicators in laboratory tests
(e.g. blood tests) and medical knowledge.
Subtask 2 Disease Status Diagnosis This requires the model to assess
the severity and stage of disease such as tumor staging based
on findings in report and provided medical knowledge.
Subtask 3 Advice or Treatment This requires the model to generate
advice such as further examinations or treatment plans.
3.3 More Statistics of RJUA-MedDQA
In this section, we present more statistics of RJUA-MedDQA. We
have listed the types and quantities of annotated contexts corre-
sponding to the laboratory reports and clinical reports, 178 contexts
in total. Note that the laboratory reports do not include contexts
for the treatment of diseases, as it is difficult to extract accurate
treatment plans from a single laboratory report. At the same time,
the contexts for clinical reports are longer than those for laboratory
reports. The reason for this is that the conclusions of laboratory
reports usually refer to specific indicators and contain shorter de-
ductive chains, as shown in Table 3. In contrast, clinical reports
contain more detailed content based on descriptive observations,
such as the dimensions and condition of the observed tissues, which
contributes to a variety of therapeutic approaches.
Overall, the task of clinical reasoning requires models to be able
to integrate both the content of images and specific medical infor-
mation in given contexts in order to derive the correct statement.
4 DATA GENERATION PIPELINE
The data generation pipeline was conducted in three phases. In
phase one, we first identify and prepare the data sources and then ap-
ply the ESRA (Efficient Structural Restoration Annotation) method
to structurally restore textual and tabular content based on the
OCR results. In phrase two, we carefully design the annotation
guidelines to achieve high efficiency and maximum consistency,
and then upload each image together with the corresponding image
text to an online interface. The annotation process is mainly divided
into two parts: (1) Image Content Annotation; (2) Clinical Reason-
ing Annotation. Both parts include an efficient and detailed guideTable 3: Statistics of Annotated Contexts
Report
TypeContext
Type# ContextsAverage
Context Length
LaboratoryExamination-Disease 55 63.1
Examination-Status 7 86.3
Disease-Status 1 86
Disease-Advice 46 71.5
ClinicalExamination-Disease 3 29
Disease-Examination 23 100.8
Disease-Status 3 408
Disease-Treatment 40 310.5
Overall 178 135.1
with the format of the data, the specific annotation task and the
examples of different types of annotations. We have performed con-
tinuous quality checks. In phrase three, the reviewed annotations
are refined by an automatic generator to create the final dataset.
An illustration can be found in Figure 3.
4.1 Efficient Structural Restoration Annotation
Given OCR results, the information of coordinates are converted
to appropriate spaces and line breaks which connect all chunks
of texts, resulting in structural text that is similar to the original
medical report image(see Figure 4).
Mathematically, given a medical report image ùêº, an OCR tool is
applied to extract textual content. The resulting OCR results consist
of extracted text segments and their corresponding bounding boxes
(bbox) are denoted as ùëá={ùë°1,ùë°2,...,ùë° ùëõ}andùêµ={ùëè1,ùëè2,...,ùëè ùëõ},
whereùëõrepresents the number of text segments recognized.
Step 1 Based on the bbox ùêµ={ùëè1,ùëè2,...,ùëè ùëõ}, partition the boxes
that belong to the same text line. Specifically, the coordinates
withinùêµare sorted orderly from left to right, and then top to
bottom, resulting new text segments ùëá‚àó={ùë°‚àó
1,ùë°‚àó
2,...,ùë°‚àóùëõ}and
new bboxùêµ‚àó={ùëè‚àó
1,ùëè‚àó
2,...,ùëè‚àóùëõ}, whereùëè‚àó
ùëñ=[ùë•ùëñ,0,ùë•ùëñ,1,ùë¶ùëñ,0,ùë¶ùëñ,1]
Step 2 The bbox in ùêµare traversed and converted into lines denoted
asùëôùëñùëõùëíùëñby line flag.
ùëìùëôùëéùëî=ùëèùëúùëúùëô[(ùë¶ùëñ+1,0+ùúÄ1<ùë¶ùëñ,1‚àíùë¶ùëñ,0
2)|
(ùë¶ùëñ,0+ùúÄ2<ùë¶ùëñ+1,1‚àíùë¶ùëñ+1,0
2<ùë¶ùëñ,1‚àíùúÄ2)](3)
5222KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Congyun Jin et al.
Figure 4: A Demonstration of ESRA method
whereùúÄ1=ùëü‚àó(ùë¶ùëñ,1‚àíùë¶ùëñ,0)andùúÄ2=ùëü‚àó(ùë¶ùëñ+1,1‚àíùë¶ùëñ+1,0).ùëü‚àóis
a discount coefficient ranging from 0 to 1. By setting this
hyper-parameter ùëü‚àó, the adhesion of text segments between
lines can be mitigated, thereby increasing readability.
Ifùëìùëôùëéùëî=ùëáùëüùë¢ùëí , bboxùëèùëñandùëèùëñ+1are on the same line.
Ifùëìùëôùëéùëî=ùêπùëéùëôùë†ùëí , a new line will be added, resulting bbox
ùêµ‚àó={ùëôùëñùëõùëí 1,ùëôùëñùëõùëí 2,...,ùëôùëñùëõùëí ùëõ} (4)
whereùëôùëñùëõùëíùëñ={ùëè‚àó
ùëôùëñùëõùëí ùëñ1,ùëè‚àó
ùëôùëñùëõùëí ùëñ2,...,ùëè‚àó
ùëôùëñùëõùëí ùëñùëõ}.
Step 3 Define a setHrepresenting the heights of all bbox of ùëôùëñùëõùëíùëñ.
Apply k-means clustering to Hon generate clusters. Within
each cluster ùëêùëñ, determine the character count ùëÅùëñof bbox.
Identify the cluster ùëêùëöùëéùë• that has the maximum character
count. The total width of bbox within ùëêùëöùëéùë• is denoted as
ùë§‚àó
ùë°ùëúùë°ùëéùëôand total characters in ùëêùëöùëéùë• is denoted as ùëÅ‚àó
ùë°ùëúùë°ùëéùëô. The
average character width ùëê‚àócan be calculated as:
ùëê‚àó=ùë§‚àó
ùë°ùëúùë°ùëéùëô
ùëÅ‚àó
ùë°ùëúùë°ùëéùëô(5)
Step 4 Join text segments in the same line by spaces. Given two
adjacent text segments ùëè‚àó
ùëôùëñùëõùëí ùëñ ùëó,ùëè‚àó
ùëôùëñùëõùëí ùëñùëò
number_of_spaces =max(‚Ñéùëñ,ùëó,ùëò
ùëê‚àóùëô,1) (6)
where‚Ñéùëñ,ùëó,ùëò is the horizontal distance between the bbox
ùëè‚àó
ùëôùëñùëõùëí ùëñ ùëó,ùëè‚àó
ùëôùëñùëõùëí ùëñùëò. Additionally, ùëôis an expansion coefficient from
0 to 1. By setting the hyper-parameter ùëô, we control the
coefficient that determines the number of spaces per line in
the overall layout output, thereby increasing integrity.
A demonstration of the impact of different ùëüon the output struc-
ture of ESRA method and detailed hyper-parameters setting can be
found in Appendix C.
4.2 Annotation Process
All images are uploading to a online platform which provides a
visual annotation interface and allows for dataset inspection and
analysis.
4.2.1 Image Content Annotation. A group of 10 dedicated annota-
tors with basic medical knowledge were responsible for this section
to ensure the accuracy and professionalism of the data annotation.
We review the collected reports, identify the most common struc-
tural features they exhibit, and select two representative aspects:‚Ä¢Key-Value Pairs : Annotators are required to identify and extract
key information pairs from the images, e.g. age, date of examina-
tion, clinical pre-test findings, etc., and label them as structured
fields.
‚Ä¢ùëÑùë¢ùëéùëëùëüùë¢ùëùùëôùëíùë°ùë† <item, result, range, is_normal>: Annotators are
required to restore the structure of the tables, including the
name of the item and the corresponding result and reference
interval, and mark anomalies. Each indicator can be marked
as normal, abnormal or indeterminate(if no reference range
is specified). For abnormal results, commentators must also
indicate whether the value is high or low.
All content in the report screen is labeled with either Key-Value
Pairs or Quadruplets. During the labeling process, annotators must
appropriately fill in incomplete information such as missing keys
and partially missing element names to increase the generalization
capability of the dataset.
Overall, the ESRA approach significantly reduces the possibility
of human input errors and increases the efficiency of the annota-
tion. It avoids the use of traditional typewriters by the annotators,
significantly reducing the amount of error that can occur during
typing and saving a lot of time typing obscure medical terminology
and descriptions. In addition, the annotators refine the OCR results
to further improve accuracy. Consequently, the working efficiency
of the standard annotators can be increased from 5 images per hour
to 10 images per hour. The accuracy rate has improved from 70% to
96.8% by manually inspecting the annotation quality of 100 images.
4.2.2 Quality Check. The production phase took around three
weeks. A peer validation was then carried out to correct any errors
in the annotations. In addition, we performed continuous quality
checks during the data quality assurance process by developing an
automated quality check tool based on this annotation framework.
It includes features to detect issues such as missing keys or values,
discrepancies in the number of table elements and the potential
for error in the assessment of abnormal indicators. Problematic
samples are sent back for re-checking and manual corrections.
4.2.3 Clinical Reasoning Annotation. A small team of 5 urological
experts is assembled for the Clinical Reasoning Annotation. Even
with the support of ESRA, this process is more time consuming
as it demands specialized knowledge and clinical experience. To
accelerate this process and streamline the subsequent generation
of the QA dataset, we firstly established a context database. To
5223RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 4: QA Generators: Diverse Task and Difficulty Levels. Synonyms : synonyms for medical terminology; Annotations: labeled data
Task Sub-task Question Template Answer Template
EntitySingle What is key(impression) ? keyisvalue.
Multi What are key1 (age) andkey2 (date) ? value1; value2
TableSingle Cell What is the (laboratory result) ofitem1 The (laboratory result) ofitem1 isresult1.
Single Row What is the (result) and(reference range) ofitem1 ?The (result) ofitem1 isresult1,
and the (reference range) isrange1.
Multi-RowWhat are the (results) and(standard intervals)
ofitem1 anditem2 correspondingly?item1, result1, range1; item2, result2, range2;
TableNRComparison Is item1 within (normal range) ? The (result) ofitem1 isresult1 and the (range) isrange1, hence is_normal1
Multi-Comparison Is there any (abnormal indicators) in this report? is_normal1, is_normal3, is_normal9.
Customized SummarizationWhat key elements should be
noticed in this medical report?There are len([items]) in this report.
len(is_normal==0) are not in standard reference, which are ...
reflect the real-life diagnostic process of physicians when reviewing
reports, the contexts were categorized into different categories:
1.Examination-disease : Diseases that can be diagnosed on the basis
of report results, such as a total bilirubin level >17.1ùúámol/L,
which indicates hyperbilirubinemia.
2.Examination-status : The disease status that can be determined
from test results, e.g. in patients with prostate cancer: a low
serum PSA level indicates a good response to treatment and a
lower tumor burden.
3.Disease-status : Medical knowledge about the status of diseases,
such as the staging of bladder cancer.
4.Disease-advice : Recommendations derived from a disease diag-
nosis, e.g. in hyperbilirubinemia it is recommended to consider
the patient‚Äôs condition together with other liver enzyme tests
and to perform further ultrasound and abdominal CT.
5.Disease-examination : The recommended follow-up examina-
tions after diagnosing a disease. For example, for neurogenic
bladder, it is required to check renal function and urinary rou-
tine after catheterization has been maintained.
6.Disease-treatment : Treatment plans based on a disease diagnosis,
like for ureteral stones, should be treated differently depending
on the size and condition of the stones.
In this way, physicians can efficiently and consistently label
medical data by giving each piece of information a descriptive title.
This allows them to easily identify and mark repeated information.
Labels are clear, fair, and do not include any doubtful information.
A report can have no diagnosis, one diagnosis, or several diagnoses,
which are clearly stated or implied in the report and its context.
4.3 VQA Generation
We propose a KG-based schema Question-Answering (QA) gen-
erator based on the structurally annotation format, as shown in
Section 4.2.3.
4.3.1 Medical Knowledge-based Synonym-aware Schema. Reports
from different healthcare information systems often use different
forms of expression, and these differences are often reinforced in
Chinese medical terminology. For example, the presentation of test
results in non-laboratory test reports may vary and be described as
"impressions", "pathological diagnosis" or "conclusions"; similarly,
reference intervals in laboratory test reports may be described as
"normal ranges", "normal intervals" or "standard values". To reduce
the complexity of parsing annotation results and increase the diver-
sity of question templates, we analyzed all keys from annotations.Together with urological experts, we created a knowledge-based
schema that integrates the validated list of synonyms into a com-
prehensive framework containing common vocabularies and their
synonyms from different types of reports. Based on this schema,
mapping of data can be easily performed, leading not only to a
reduction of ambiguities and errors in the interpretation of medical
reports due to the use of synonyms, but also to a greater variety in
the generation of questions and thus to improved interoperability
of data.
4.3.2 VQA Generator.
In RJUA-MedDQA dataset, we proposed two tasks: Image Content
VQA and Clinical Reasoning VQA.
Image Content Recognition VQA Generation: The core of
this process is task generators. Each generator corresponds to sev-
eral subtask question templates that vary in difficulty, as shown in
Table 4. The templates contain annotated variables from reports
and(vocabulary) in the synonym schema. Using these templates,
we generate questions of varying difficulty that can be enriched
with annotations to include complex queries with double-entity,
single-line or multi-line questions. When formulating the questions,
we use synonym substitutions to improve the generalization ability
of our dataset. We have developed templates based on our schema
and the specific annotation format.
Customization: In addition to existing QA pairs in the dataset,
we provide key-value pair annotations and quadruplets, enabling
the construction of customized templates to meet diverse model
requirements such as summarization task shown in Table 4.
Clinical Reasoning VQA Generation: This task comprises
multiple choice (MC) and short answer (SA) questions. In each
report, diagnosis, status or advice can be displayed explicitly or
inferred from the image content and the corresponding context.
For the latter, we retrieve the title from the context fact base and
use bert-base-chinese [ 7] embedding and cosine similarity to find
the three most similar titles as distracting options. We evenly dis-
tributed the proportion of each option in the question bank. The
answers to the short answers consist of one or more of the correct
answers from the multiple-choice questions.
5 EXPERIMENTS
5.1 Baseline Models
There are very few LMM models that have been proposed to effec-
tively solve Chinese medical QA tasks over images containing both
tabular data and text, where contextual inference is particularly
5224KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Congyun Jin et al.
required. We selected 5 different multimodal models and compared
them with the dataset. The models we selected cover a wide range
of strategies and architectures and illustrate the current state of the
art in multimodal understanding, i.e. LLaVA-v1.5-7B [ 19], mPLUG-
Owl2-9.8B [ 35], Qwen-VL-Chat-9.6B, Qwen-VL-Plus-[API] [ 2] and
GPT-4v-[API] [ 34]. This selection process ensures the credibility of
our experimental results.
In addition to the LMMs, we perform comparative experiments
with a set of strong LLMs by using image-texts generated with the
ESRA method to further investigate the limitations and potential of
the current LMMs. We have conducted comprehensive research and
chosen models based on their performance in medical domains as
indicated by the Apollo assessment [ 32]. We used GPT-4-[API] [ 26]
for its best performance in English medical tasks and Qwen-[API]
[1] for Chinese. This selection process ensures the credibility of our
experimental results.
5.2 Evaluation metrics
The dataset contains various tasks that are formulated either as
short answers or as single-choice questions. Our observations indi-
cate that LMMs have a preference for certain response techniques.
For example, with short answers, LMMs tend to rephrase the ques-
tion before presenting their answer. For single-choice questions,
models often produce a complete sentence that either directly
matches or is semantically similar to one of the options and is
sometimes even accompanied by an explanation.
Consequently, relying on a single scoring metric or simply com-
paring the similarity between the model-generated responses and
the ground truth (ANLS [ 6]) can potentially lead to scoring bias.
Therefore, to achieve a fairer assessment, we implemented task-
specific metrics in the dataset that allow us to capture the different
nuances of LMM performance. For Table Numerical Reasoning (NR)
QA and Clinical Reasoning Multiple Choice (MC), we use Soft Accu-
racy, which means that the predicted answer is considered correct
if it contains the ground truth. For Entity QA, Table QA and Clinical
Reasoning Short Answer (SA) we will use both Soft Accuracy and
ROUGE-L [15] for comparison.
5.3 Results and Analysis
5.3.1 Overall Results. We exhaustively evaluate the five models on
the existing 5 sub-tasks of RJUA-MedDQA. In Table 5, we present
the models‚Äô overall performance on various QA tasks namely En-
tity, Table, NR, Clinical Reasoning MC and SA. Among five LMMs,
Qwen-VL-Plus and GPT-4v yield superior results on all tasks, while
LLaVA-v1.5-7B and mPLUG-Owl2 demonstrate lower overall per-
formance on Chinese VQA tasks compared to the other models.
The performance patterns of Qwen-VL-Plus and GPT-4v on Entity
and Table tasks are intriguingly divergent when evaluated using
two metrics. One explanation is that Qwen often deviates from the
prompt instructions, resulting in longer sentences that negatively
impact RougeL score; while GPT-4v tends to change output format
such as the decimal place of floats, leading to lower accuracy but
higher RougeL score. However, both responses are generally accu-
rate. For entities, where longer text is involved, we prefer to rely on
RougeL, while for tables, we opt for Soft Accuracy as a reference.
Overall, Qwen-VL-Plus is more proficient in image-content relatedQAs; while GPT-4v shows stronger in-context reasoning ability and
perform the best in Clinical Reasoning Tasks.
Figure 5: Results of 5 LMMs across 5 tasks
Compared to LMMs, ESRA+LLMs yields superior results. ESRA+
GPT4‚Äôs performance on most tasks is far better than that of LMMs.
Meanwhile, the performance of all models in Clinical Reasoning
SA is not satisfactory.
5.3.2 Impact of image Quality. For a deeper understanding, we
provide a comprehensive analysis of image quality (high vs. low) on
model performance. Although the capabilities of current LMMs are
still quite limited and do not outperform traditional image-text with
LLMs, analyzing the impact of image quality on model performance
with tabular content (Table 6) revealed an interesting fact. LMMs
are more robust to poor quality and diversely structured images.
Table QA requires high image quality, as even minor distortions
can lead to a significant drop in model performance. The disparity
in GPT-4 to process tables is evident by a significant performance
difference of about 0.3 between high and low quality images, while
for Qwen the difference is 0.22. In contrast, such a large difference
in performance is not observed for the LMMs. Overall, LMMs often
show better performance, even on low quality images, suggesting
a certain robustness of multimodal methods that uni-modal OCR-
based models may not possess. In Figure 8, we present several
cases of low-quality images that demonstrate robustness to quality
degradation.
5.3.3 Clinical Reasoning. Studies indicates that the existence of
context definitely improves model performance (Table 8). However,
the evaluation results reveals that cross-instance understanding
and logic reasoning pose a significant challenge for existing LMMs
(Table 7). GPT-4v exhibits the most advanced reasoning capabili-
ties among all LMMs, yet it falls notably behind ESRA+GPT4 by
approximately 0.3. Similar pattern appears in the comparison be-
tween QwenVL+ and QwenVL+ESRA, with an even larger gap of
0.45. It is observed that disease diagnosis tends to be a less com-
plex task than providing clinical advice in table reasoning tasks,
as the latter demands multi-step reasoning - a challenge evidently
not met by any model except for ESRA+GPT-4. Meanwhile, the
SA performance for laboratory and clinical diagnosis by existing
5225RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 5: Performance of Baseline Models on RJUA-MedDQA. Best results are marked in bold
Image Content Clinical Reasoning
Entity Table TableNR MC SA Method
RougeL Acc RougeL Acc Acc Acc RougeL
LLaVA-v1.5-7B 0.08 0.04 0.08 0.04 0.36 0.15 0.11
mPLUG-Owl2-9.8B 0.17 0.09 0.22 0.02 0.31 0.28 0.13
Qwen-VL-Chat-9.6B 0.28 0.20 0.28 0.14 0.33 0.30 0.11
Qwen-VL-Plus-[API] 0.46 0.51 0.38 0.53 0.52 0.32 0.15LMMs
GPT-4v-[API] 0.45 0.38 0.68 0.47 0.50 0.46 0.23
ESRA+Qwen-[API] 0.77 0.80 0.50 0.73 0.68 0.66 0.18 ImageText+
LLMs ESRA+GPT-4-[API] 0.79 0.76 0.62 0.72 0.63 0.79 0.29
Table 6: Impact of Photo Quality on Model Performance. "High",
"Low" denotes the "High-quality", "Low-quality"
MethodEntity Table TableNR
High Low High Low High Low
RougeL RougeL Acc Acc Acc Acc
LLaVA-v1.5 0.08 0.07 0.03 0.06 0.39 0.38
mPLUG-Owl2 0.19 0.08 0.02 0.01 0.27 0.21
Qwen-VL-Chat 0.33 0.29 0.08 0.09 0.33 0.26
Qwen-VL-Plus 0.47 0.42 0.35 0.36 0.44 0.40
GPT-4v 0.51 0.44 0.46 0.34 0.48 0.39
ESRA+Qwen 0.83 0.79 0.75 0.58 0.70 0.52
ESRA+GPT-4 0.82 0.77 0.71 0.40 0.61 0.48
Table 7: Performance of Baseline Models on Clinical Reasoning. "D",
"S", "A" denotes the "Disease", "Status", "Advice"
MethodMultiple Choice (Acc) Short Answer (RougeL)
Table Clinical Table Clinical
D A S A D A S A
LLaVA-v1.5 0.10 0.13 0 0.05 0.14 0.07 0.17 0.01
mPLUG-Owl2 0.18 0.17 0 0.27 0.10 0.03 0.01 0.00
Qwen-VL-Chat 0.24 0.21 0 0.29 0.10 0 0.01 0.01
Qwen-VL-Plus 0.30 0.25 0.25 0.33 0.11 0.02 0.01 0.01
GPT-4v 0.50 0.47 0.33 0.59 0.21 0.02 0.19 0.04
ESRA+Qwen 0.75 0.65 0.33 0.52 0.06 0 0.10 0.04
ESRA+GPT-4 0.86 0.87 0.5 0.67 0.22 0.05 0.33 0.06
models is remarkably poor, with the highest RougeL score being
only 0.33. The results indicate that improving the medical report
understanding and contexual reasoning capabilities of LLMs can
be a significant and promising direction.
5.4 Case study
We analyze three common mistakes of LMMs and provide illustra-
tive examples in Figure 8.
Incorrect Extraction A large proportion of errors are caused by
incorrect extraction, a problem that frequently occurs in tableQA.
For example, failing to find the correct item name can lead to mis-
takes when getting reference ranges. A subset of these errors is due
to incomplete extraction, where the model correctly identifies the
correct entity but fails to include all the necessary information. It
is particularly difficult for models to correctly extract longer text
that spans multiple lines.Hallucination is another common issue, where the model confi-
dently generates text that is not present in the image. We randomly
selected 84 questions in Entity QA and 50 questions in Table QA
that are unanswerable. Statistics show that models exhibit halluci-
nation issues to varying degrees, with LMMs facing more severe
problems. For instance, in Entity Tasks, GPT-4v produced incorrect
responses for 54 out of 84 questions; in Table Tasks, the error rate
was 35 out of 50.
Faulty Reasoning Compared to LLMs, LMMs need to initially
locate the correct content, which entails a certain degree of difficulty.
In SA tasks where there are no cues provided by options, the models
tend to extract the text that is directly relevant to the semantics
of the images, rather than fully understanding the test description,
conclusions, and the relevant context. For example, take the phrase
"preclinical diagnosis" as the final diagnosis for the report. Qwen
exhibits the same problem, resulting in even worse performance
than GPT-4v.
6 CONCLUSION AND FUTURE STUDY
In this paper, we present a new benchmark RJUA-MedDQA to mea-
sure the progress of image content extraction and clinical reasoning
from visually rich medical documents in a real-world application.
We also proposed a comprehensive data generation pipeline that
includes an image-text restoration method, a well-designed annota-
tion policy, and an automatic QA generator, which greatly improves
efficiency and accuracy. Evaluation results reveals that LMMs are
robust even under poor image quality, but still have difficulty with
complex reasoning tasks involving cross-instance understanding
and strong logical link. Our research aims to improve multimodal
understanding of medical documents and support healthcare ap-
plications. In future studies, we will also focus on the three areas
discovered in this work and explore approaches that can improve
contextual reasoning.
7 ETHICAL CONSIDERATION
We strictly adhere to ethical guidelines when collecting data for
scientific research and use medical reports sourced from an online
hospital platform with informed patient consent. To protect pri-
vacy, we have implemented a thorough anonymization protocol
where identifiable information is removed and ages are converted to
ranges, which are monitored by doctors at Shanghai Renji Hospital
to ensure that no sensitive information remains.
5226KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Congyun Jin et al.
REFERENCES
[1]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng
Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,
Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng
Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical
Report. arXiv:2309.16609 [cs.CL]
[2]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Jun-
yang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-
Language Model for Understanding, Localization, Text Reading, and Beyond.
arXiv:2308.12966 [cs.CV]
[3]Asma Ben Abacha, Vivek V. Datla, Sadid A. Hasan, Dina Demner-Fushman, and
Henning M√ºller. 2020. Overview of the VQA-Med Task at ImageCLEF 2020:
Visual Question Answering and Generation in the Medical Domain. In CLEF
2020 Working Notes (CEUR Workshop Proceedings). CEUR-WS.org <http://ceur-
ws.org >, Thessaloniki, Greece.
[4]Asma Ben Abacha, Sadid A. Hasan, Vivek V. Datla, Joey Liu, Dina Demner-
Fushman, and Henning Muller. 2019. VQA-Med: Overview of the Medical Visual
Question Answering Task at ImageCLEF 2019. In Working Notes of CLEF 2019
(CEUR Workshop Proceedings, Vol. 2380) . CEUR-WS.org, Lugano, Switzerland.
https://ceur-ws.org/Vol-2380/paper_272.pdf
[5]Asma Ben Abacha, Mourad Sarrouti, Dina Demner-Fushman, Sadid A. Hasan,
and Henning M√ºller. 2021. Overview of the VQA-Med Task at ImageCLEF 2021:
Visual Question Answering and Generation in the Medical Domain. In CLEF
2021 Working Notes (CEUR Workshop Proceedings). CEUR-WS.org, Bucharest,
Romania.
[6]Ali Furkan Biten, Rub√®n Tito, Andres Mafla, Lluis Gomez, Mar√ßal Rusi√±ol,
Minesh Mathew, C. V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas.
2019. ICDAR 2019 Competition on Scene Text Visual Question Answering.
arXiv:1907.00490 [cs.CV]
[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 [cs.CL]
[8]Yihao Ding, Siwen Luo, Hyunsuk Chung, and Soyeon Caren Han. 2023. PDFVQA:
A New Dataset for Real-World VQA on PDF Documents. arXiv:2304.06447 [cs.CV]
[9]Sadid A. Hasan, Yuan Ling, Oladimeji Farri, Joey Liu, Henning M√ºller, and
Matthew P. Lungren. 2018. Overview of ImageCLEF 2018 Medical Domain
Visual Question Answering Task. In Conference and Labs of the Evaluation Forum.
https://api.semanticscholar.org/CorpusID:51943124
[10] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng,
and Erik Cambria. 2023. A Survey of Large Language Models for Health-
care: from Data, Technology, and Applications to Accountability and Ethics.
arXiv:2310.05694 [cs.CL]
[11] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie.
2020. PathVQA: 30000+ Questions for Medical Visual Question Answering.
arXiv:2003.10286 [cs.CL]
[12] Xu Zhang Jian Huang. 2022. Chinese Urological and Andrological Diseases
Diagnosis and Treatment Guidelines: 2022 Edition.
[13] Olga Kovaleva, Chaitanya Shivade, Satyananda Kashyap, Karina Kanjaria, Joy
Wu, Deddeh Ballah, Adam Coy, Alexandros Karargyris, Yufan Guo, David Beymer
Beymer, Anna Rumshisky, and Vandana Mukherjee Mukherjee. 2020. Towards
Visual Dialog for Radiology. In Proceedings of the 19th SIGBioMed Workshop on
Biomedical Language Processing, Dina Demner-Fushman, Kevin Bretonnel Cohen,
Sophia Ananiadou, and Junichi Tsujii (Eds.). Association for Computational
Linguistics, Online, 60‚Äì69. https://doi.org/10.18653/v1/2020.bionlp-1.6
[14] Jason J. Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. 2018.
A dataset of clinically generated visual questions and answers about radiology
images (Scientific Data, Vol. 180251). https://doi.org/10.1038/sdata.2018.251
[15] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.
InText Summarization Branches Out. Association for Computational Linguistics,
Barcelona, Spain, 74‚Äì81. https://aclanthology.org/W04-1013
[16] Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi
Wu, Mingguang He, and Zongyuan Ge. 2023. Medical visual question answering:
A survey. Artificial Intelligence in Medicine 143 (2023), 102611. https://doi.org/
10.1016/j.artmed.2023.102611
[17] Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi
Wu, Mingguang He, and Zongyuan Ge. 2023. Medical visual question answering:
A survey. Artificial Intelligence in Medicine 143 (Sept. 2023), 102611. https:
//doi.org/10.1016/j.artmed.2023.102611
[18] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. 2021. SLAKE:
A Semantically-Labeled Knowledge-Enhanced Dataset for Medical Visual Ques-
tion Answering. arXiv:2102.09542 [cs.CV]
[19] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved Baselines
with Visual Instruction Tuning. arXiv:2310.03744 [cs.CV][20] Wenge Liu, Yi Cheng, Hao Wang, Jianheng Tang, Yafei Liu, Ruihui Zhao, Wenjie
Li, Yefeng Zheng, and Xiaodan Liang. 2022. "My nose is running.""Are you also
coughing?": Building A Medical Diagnosis Agent with Interpretable Inquiry
Logics. arXiv:2204.13953 [cs.CL]
[21] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun
Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to Explain:
Multimodal Reasoning via Thought Chains for Science Question Answering.
arXiv:2209.09513 [cs.CL]
[22] Minesh Mathew, Viraj Bagal, Rub√®n P√©rez Tito, Dimosthenis Karatzas, Ernest
Valveny, and C. V Jawahar. 2021. InfographicVQA. arXiv:2104.12756 [cs.CV]
[23] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2021. DocVQA: A
Dataset for VQA on Document Images. arXiv:2007.00398 [cs.CV]
[24] Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, and Edward
Choi. 2022. Multi-Modal Understanding and Generation for Medical Images
and Text via Vision-Language Pre-Training. IEEE Journal of Biomedical and
Health Informatics 26, 12 (Dec. 2022), 6070‚Äì6080. https://doi.org/10.1109/jbhi.
2022.3207502
[25] Nandita Naik, Christopher Potts, and Elisa Kreiss. 2023. Context-
VQA: Towards Context-Aware and Purposeful Visual Question Answering.
arXiv:2307.15745 [cs.CL]
[26] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[27] Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar.
2022. DocLayNet: A Large Human-Annotated Dataset for Document-Layout
Segmentation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ‚Äô22). ACM. https://doi.org/10.1145/3534678.
3539043
[28] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi. 2022. A-OKVQA: A Benchmark for Visual Question Answer-
ing using World Knowledge. arXiv:2206.01718 [cs.CV]
[29] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari
Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. 2021. Mul-
tiModalQA: Complex Question Answering over Text, Tables and Images.
arXiv:2104.06039 [cs.CL]
[30] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito,
and Kuniko Saito. 2023. SlideVQA: A Dataset for Document Visual Question
Answering on Multiple Images. arXiv:2301.04883 [cs.CL]
[31] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021. VisualMRC: Machine
Reading Comprehension on Document Images. arXiv:2101.11272 [cs.CL]
[32] Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu,
Anningzhe Gao, Xiang Wan, Haizhou Li, and Benyou Wang. 2024. Apollo: An
Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B
People. arXiv:2403.03640 [cs.CL]
[33] Li Xu, Bo Liu, Ameer Hamza Khan, Lu Fan, and Xiao-Ming Wu. 2023. Multi-
modal Pre-training for Medical Vision-language Understanding and Generation:
An Empirical Study with A New Benchmark. arXiv:2306.06494 [cs.CV]
[34] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng
Liu, and Lijuan Wang. 2023. The Dawn of LMMs: Preliminary Explorations with
GPT-4V(ision). arXiv:2309.17421 [cs.CV]
[35] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi
Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023. mPLUG-Owl2: Revolu-
tionizing Multi-modal Large Language Model with Modality Collaboration.
arXiv:2311.04257 [cs.CL]
[36] Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng
Chua. 2022. Towards Complex Document Understanding By Discrete Reasoning.
InProceedings of the 30th ACM International Conference on Multimedia (MM ‚Äô22).
ACM. https://doi.org/10.1145/3503161.3548422
A STATISTICS OF DISEASE
RJUA-MedDQA covers approximately 166 unique diseases and diag-
noses for laboratory report, 334 for clinical report. Of these, 80% are
urological-related diseases, and we have listed the top 25 diseases
for both report types, shown in Figure 6.
B CRITERIA OF LOW QUALITY IMAGE
The RJUA-MedDQA dataset contains 2,000 medical images divided
into three categories: 402 screenshots, 619 scanned PDFs, and 979
patient-taken photos. Among the photos, they are further catego-
rized by quality, assessed using two sensors‚Äîthe Completeness
Sensor and Angle Perception Sensor.
1.Completeness Sensor identifies the paper‚Äôs four corners in an im-
age. If less than three corners are detected, the image is deemed
incomplete.
5227RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Figure 6: Top 25 Diseases in Laboratory Reports (Left) and clinical reports (Right)
Figure 7: The Impact of Different ron the output structure of ESRA Structure: An Illustration
Table 8: Impact of Contexts on Model Performance. "Q" denotes "Image input only", "C+Q" denotes "Image+context"
MethodMultiple Choice Short Answer
Table Clinical Table Clinical
Q C+Q Œî Q C+Q Œî Q C+Q Œî Q C+Q Œî
Qwen-VL-Plus 0.29 0.3 +0.01 0.28 0.31 +0.03 0.08 0.09 +0.01 0.01 0.01 +0.00
GPT-4v 0.40 0.48 +0.08 0.38 0.45 +0.07 0.13 0.21 +0.08 0.1 0.19 +0.09
ESRA+Qwen 0.65 0.72 +0.07 0.45 0.48 +0.03 0.04 0.05 +0.01 0.06 0.01 +0.02
ESRA+GPT-4 0.70 0.86 +0.16 0.62 0.66 +0.04 0.14 0.22 +0.08 0.22 0.30 +0.08
2.Angle Perception Sensor captures skewness. Given Four points
of the image[ùë•0,0,ùë•0,1,ùë¶1,0,ùë¶1,1], if the angle between lineùë•0,0,ùë•0,1
andlineùë•1,0,ùë•1,1is greater than 15 degrees, it is deemed skewness.
C IMPLEMENT DETAILS OF ESRA METHOD
As mentioned in Section 4.1, ùëü‚àóis a discount coefficient ranging
from 0 to 1. By setting this hyper-parameter ùëü‚àó, the adhesion of
text segments between lines can be mitigated, thereby increasingreadability. We have tested various settings for ùëüranging from
0 to 1 (see Figure 7). It can be observed that for electronic raw
images (PDF), the impact of ùëüis negligible, whereas for photographs,
ùëüresults in a more distinct segmentation of each line. We have
determined that ùëü‚àó=0.15is the optimal value for processing both
electronic and photographed images.
ùëôis an expansion coefficient from 0 to 1. By setting the hyper-
parameterùëô, we control the coefficient that determines the number
of spaces per line in the overall layout output. ùëô‚àó=0.7is optimal.
5228KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Congyun Jin et al.
Figure 8: RJUA-MedDQA cases
5229