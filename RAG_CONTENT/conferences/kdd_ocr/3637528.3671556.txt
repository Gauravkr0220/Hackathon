Metric Decomposition in A/B Tests
Alex Dengâˆ—
Airbnb
Seattle, WA, USA
alex.deng@airbnb.comLuke Hagar
University of Waterloo
Waterloo, ON, Canada
lmhagar@uwaterloo.ca
Nathaniel T. Stevens
University of Waterloo
Waterloo, ON, Canada
nstevens@uwaterloo.caTatiana Xifara
Airbnb
San Francisco, CA, USA
tatiana.xifara@airbnb.comAmit Gandhiâ€ 
University of Pennsylvania
Philadelphia, PA, USA
agandhi@upenn.edu
Abstract
Morethanadecadeago,CUPED(ControlledExperimentsUtilizing
Pre-Experiment Data) mainstreamed the idea of variance reduction
leveraging pre-experiment covariates. Since its introduction, it has
been implemented, extended, and modernized by major online ex-
perimentation platforms. Despite the wide adoption, it is known by
practitioners that the variance reduction rate from CUPED utilizing
pre-experimental data varies case by case and has a theoretical limit.
In theory, CUPED can be extended to augment a treatment effect
estimator utilizing in-experiment data, but practical guidance on
how to construct such an augmentation is lacking. In this article, we
fill this gap by proposing a new direction for sensitivity improve-
ment via treatment effect augmentation whereby a target metric of
interest is decomposed into components with high signal-to-noise
disparity. Inference in the context of this decomposition is devel-
oped using both frequentist and Bayesian theory. We provide three
real world applications demonstrating different flavors of metric
decomposition; these applications illustrate the gain in agility metric
decomposition yields relative to an un-decomposed analysis.
CCS Concepts
â€¢Mathematics of computing â†’Probabilistic inference prob-
lems; Hypothesis testing and confidence interval computa-
tion; Bayesian computation ;â€¢Appliedcomputing â†’E-commerce
infrastructure.
Keywords
A/B testing, online experimentation, variance reduction, Bayesian
analysis, causal surrogate, counterfactual
ACM Reference Format:
Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, and Amit
Gandhi. 2024. Metric Decomposition in A/B Tests. In Proceedings of Proceed-
ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
âˆ—Corresponding author.
â€ Work completed while employed by Airbnb.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671556Mining (KDD â€™24). ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671556
1 Introduction
Online controlled experiments, also referred to as â€œA/B testsâ€, are
an invaluable tool used by companies to test and evaluate changes to
theironlineproducts.Withrespecttosomemetric(s)ofinterest,these
experiments facilitate causal conclusions about the efficacy of such
changes. Large tech companies collectively run tens of thousands
of these experiments each year, engaging millions of users [23].
An A/B test typically compares two versions of a product: a new
treatment version to the existing control version. Interest lies in
understanding the treatment effect ğ›¿, which quantifies the poten-
tial improvement (with respect to some metric of interest) induced
by the treatment relative to the control. Denoting the metric of in-
terestğ‘€, the treatment effect ğ›¿is commonly estimated using the
difference in metric values observed in the treatment and control
groups Î”(ğ‘€):=ğ‘€ğ‘‡âˆ’ğ‘€ğ¶. Assuming the users are independent of
one another and randomized to the treatment and control groups,
this estimator is unbiased for ğ›¿. In some contexts, a treatment ef-
fect defined on the percent scale is preferred for ease of business
communication. This is referred to as lift, and is estimated by
Î”%(ğ‘€):=ğ‘€ğ‘‡âˆ’ğ‘€ğ¶
ğ‘€ğ¶.
In A/B tests, such metrics are often defined as averages ğ‘€:=ğ‘‹of
some measurement ğ‘‹ğ‘–observed on each user ğ‘–=1,2,...,ğ‘› in the
treatment (or control) group. However, ratio and percentile metrics
may also be relevant [ 9,22,23]. In this paper, we focus on average
and ratio metrics as they account for the appreciable majority of
metrics used in practice.
Thus, inference (by way of hypothesis tests and statistical inter-
vals) forğ›¿using Î”(ğ‘€)(orÎ”%(ğ‘€)) is of interest. However, such
inference is complicated by the noisiness of these metrics in practice;
inference quality hinges critically on the sampling variances
Var[Î”(ğ‘€)]and Var[Î”%(ğ‘€)].
Although sample sizes in online A/B tests are typically very largeâ€”
often at least thousands up to millionsâ€”it is widely documented by
practitioners that metrics of interest are highly variable and that
hypothesis tests for ğ›¿lack statistical power [ 20]. Consequently, false
negativesâ€”when experimenters cannot detect a non-zero treatment
effectâ€”are prevalent. Moreover, in the face of statistically significant
(abbreviated as stat. sig. henceforth) results, the estimated treatment
effect Î”(ğ‘€)(orÎ”%(ğ‘€)) often over exaggerates the true treatment
effectğ›¿yielding false discoveries [18, 21].
4885
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, and Amit Gandhi
Therefore, there is great interest in increasing sensitivity of met-
rics; for a given metric ğ‘€, determining how to construct an estimator
of the treatment effect ğ›¿with low bias and small variance remains
one of the most critical statistical challenges for A/B testing re-
search [ 3,20,23]. Assuming an unbiased estimator, the method most
widely applied in industry to reduce variability is CUPED (Controlled
experiments Utilizing Pre-Experimental Data) or its variants and
extensions [ 7,12,13]. The general idea with this class of methods
is to use in place of ğ‘€an alternative version of the metric that is
augmented by a second metric highly correlated with ğ‘€. Another
class of methods recently gaining traction is the use of surrogate
metrics in place ofğ‘€. Such surrogates are chosen or designed to be
proxies forğ‘€with higher sensitivity [15].
In this paper, we propose novel methodology for increasing the
sensitivity of metrics and hence treatment effect estimators that
represents a new direction on this problem. In particular, we propose
decomposing the metric of interest into two or more components in
an attempt to isolate those with high signal and low noise from those
with low signal and high noise. The paper demonstrates both empir-
ically and theoretically the value of this practice in both frequentist
and Bayesian settings.
1.1 Metric Decomposition
Consider an additive decomposition of a metric ğ‘€as follows
ğ‘€=ğ‘€1+ğ‘€2. This decomposition implies the following decomposi-
tion of the estimator
Î”(ğ‘€)=Î”(ğ‘€1)+Î”(ğ‘€2), (1)
where the true effect to be estimated also has the decomposition
ğ›¿=ğ›¿1+ğ›¿2. Multiplicative decompositions such as ğ‘€=ğ‘€1Ã—ğ‘€2
may also be of interest. In a treatment vs. control comparison, if
we observe percent lifts Î”%(ğ‘€1)andÎ”%(ğ‘€2), by the multiplicative
decomposition, we have
ğ‘€ğ‘‡=ğ‘€1,ğ‘‡Ã—ğ‘€2,ğ‘‡
=[1+Î”%(ğ‘€1)]ğ‘€1,ğ¶Ã—[1+Î”%(ğ‘€2)]ğ‘€2,ğ¶
=ğ‘€ğ¶Ã—[1+Î”%(ğ‘€1)][1+Î”%(ğ‘€2)].
Dividing both sides by ğ‘€ğ¶and expanding the right hand side, we see
ğ‘€ğ‘‡
ğ‘€ğ¶âˆ’1=Î”%(ğ‘€1)+Î”%(ğ‘€2)+Î”%(ğ‘€1)Â·Î”%(ğ‘€2).
The left hand side is the percent treatment effect Î”%(ğ‘€), and the last
term on the right hand side is an ignorable second order term. When
bothÎ”%(ğ‘€1)andÎ”%(ğ‘€2)are relatively small, which is often the case
in A/B tests where even a 10% change is commonly deemed extreme,
the following approximate additive decomposition is appropriate
Î”%(ğ‘€)â‰ˆÎ”%(ğ‘€1)+Î”%(ğ‘€2). (2)
With a unified (though slight abuse of) notation, we let ğ›¿â‰ˆğ›¿1+ğ›¿2
represent the ground truth multiplicative treatment effect. Thus de-
compositions of Î”(ğ‘€)andÎ”%(ğ‘€)will both be treated as additive no
matter whether the decomposition of ğ‘€is additive or multiplicative.
Note that the above decompositions assume the metric ğ‘€decom-
poses intoğ‘˜=2components, but context and/or engineered solutions
may dictate a decomposition into any number of components, e.g.,
ğ‘€=ğ‘€1+Â·Â·Â·+ğ‘€ğ‘˜orğ‘€=ğ‘€1Ã—Â·Â·Â·Ã—ğ‘€ğ‘˜.
We address (and develop theory for) this more general case in this
paper.Where does a decomposition come from?
Context may dictate a natural decomposition. If ğ‘€is a simple average
ğ‘‹, an additive decomposition can come from breaking each observa-
tionğ‘‹ğ‘–into two (or more) parts. Similarly, when ğ‘€is a ratio metric
ğ‘‹/ğ‘Œ, an additive decomposition can be constructed from a decompo-
sition of the numerator ğ‘‹. Multiplicative decompositions such as ğ‘‹=
ğ‘‹
ğ‘ŒÃ—ğ‘Œalso occur naturally. For example, multiplicative metric chain-
ing is common in conditional funnels; if a conversion funnel involves
multiple steps, then a conversion rate ğ‘Œat an intermediate step can
be used to decompose the overall conversion rate ğ‘‹multiplicatively.
Similarly, common revenue metrics such as revenue per user can be
decomposed into revenue per purchase, and purchases per user.
More generally, both additive and multiplicative decompositions
can be engineered by defining one of the components and then taking
the second component to be the additive or multiplicative comple-
ment. Specifically, let ğ‘€1be any arbitrary metric, we can define
ğ‘€2asğ‘€âˆ’ğ‘€1(in the additive case), or ğ‘€/ğ‘€1(in the multiplicative
case). With this construction, we can always get a synthetic metric
decomposition. In Section 3, we illustrate real-world examples of
both contextual and engineered decompositions.
Why is metric decomposition useful?
In Sections 2 and 4 we respectively develop frequentist and Bayesian
theory for how to leverage decompositions to improve the sensitivity
of treatment effect estimators relative to the standard approach with-
out decomposition. Then, in Section 3, we provide three real-world
applications of metric decomposition to illustrate how these meth-
ods can be employed in practice. Here, we motivate at a high-level
the value of metric decomposition from both the frequentist and
Bayesian perspectives.
In the frequentist setting, we propose defining a new treatment
effect estimator as a function of the components. Illustrating the
basic idea with ğ‘˜=2components, we have
Î”âˆ—(ğœƒ):=Î”1+ğœƒÂ·Î”2 (3)
where Î”1andÎ”2are suppressed notation for Î”(ğ‘€1)andÎ”(ğ‘€2).
Clearly, the original estimator from (1) arises as a special case when
ğœƒ=1, but the formulation in (3) allows for optimization of different
objectives with respect to ğœƒ. Such objectives may include variance
reduction, mean squared error reduction, or power boosting. As we
demonstrate in Section 2.2, the proposed framework is flexible to a
variety of different objectives.
Keen readers familiar with variance reduction and CUPED [ 12]
will recognize this form of regression adjustment. When the compo-
nentÎ”2has no treatment effect, i.e., ğ›¿2=0, we knowğ›¿=ğ›¿1. Instead
of using Î”=Î”1+Î”2, we can directly use Î”1as an estimator if it has
a smaller variance than Î”. Or, more generally, we can find ğœƒthat
minimizes variance in the family of (3). However, in general, beyond
using pre-experiment data as suggested by CUPED, it is hard to
construct a component Î”2with theoretically 0treatment effect. Nev-
ertheless, as we demonstrate in this paper, it is commonly possible
to define a decomposition in which the components have drastically
different signal-to-noise ratios (SNRs), with one component captur-
ing the majority of the treatment effect and the other component(s)
capturing much less treatment effect and a large proportion of noise.
With metric decomposition, we can exploit this kind of SNR disparity.
In this way, the estimator in (3)based on metric decomposition can
4886Metric Decomposition in A/B Tests KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
be seen as a generalization of CUPED, where rather than adjusting
by a null-effect term (i.e., mean-zero augmentation) [ 13], we adjust
by an almost null-effect term. We refer to the adjustment made with
estimators in the form of (3)asapproximately null augmentation, or
ANA. This perspective will be formalized in Section 2.
From a Bayesian perspective, inference for ğ›¿is carried out via pos-
terior analyses. Of interest here are the two posterior distributions
ğ‘(ğ›¿|Î”)andğ‘(ğ›¿|Î”1,Î”2),
where the first would be used in a standard analysis and the second
exploits the decomposition. As we formalize in Section 4, we can ex-
pect the posterior distribution ğ‘(ğ›¿|Î”1,Î”2)to have smaller posterior
variance than ğ‘(ğ›¿|Î”). Moreover, certain prior information can also
lead toğ‘(ğ›¿|Î”1,Î”2)being concentrated more closely around ğ›¿than
ğ‘(ğ›¿|Î”). Thus, from a Bayesian perspective, sensitivity and hence the
quality of inference can also be improved by metric decomposition.
1.2 Setup and Notation
We assume the target of inference is the treatment effect ğ›¿, which
quantifies the additive (or percent) difference between treatment
and control with respect to some metric ğ‘€. We further assume ğ‘€
decomposes into a sum (or product) of components ğ‘€1,...,ğ‘€ğ‘˜. In
either case, as discussed in Section 1.1, we assume that Î”(ğ‘€)=
Î”1(ğ‘€)+Â·Â·Â·+ Î”ğ‘˜(ğ‘€)estimates the treatment effect ğ›¿which similarly
decomposes: ğ›¿=ğ›¿1+Â·Â·Â·+ğ›¿ğ‘˜. Although in this paper we will illus-
trate metric decomposition for the basic ğ‘˜=2component version,
we develop theory for the general ğ‘˜>2component case as well.
Throughout weâ€™ll use the vector notation ğš«=(Î”1,...,Î”ğ‘˜)to repre-
sent observed treatment effects and ğœ¹=(ğ›¿1,...,ğ›¿ğ‘˜)to represent true
treatment effects. We adopt the following random effect model
ğš«=ğœ¹+ğœº, (4)
where ğœ¹andğœºare both random vectors which are assumed to be
independent of one another. This model is meant to characterize the
variation in observed treatment effects across a population of A/B
tests (e.g., across all the A/B tests run by a given organization). The
random vector ğœ¹reflects variation in true treatment effects across
these experiments and the random vector ğœºreflects noise in treatment
effect estimation. For large scale A/B tests, it is common to exploit the
central limit theorem and assume that ğœºÂ¤âˆ¼N(0,ğšº). We further follow
industry convention and assume the covariance matrix ğšºis fixed and
known, where the â€œknownâ€ values are found using sample variances
and covariances based on past experiments. For a given experiment,
this covariance matrix is a function of sample size ğ‘›though we
suppress notation and do not notate this dependence explicitly.
We also posit that ğœ¹follows a distribution with mean E[ğœ¹]=0and
covariance matrix Var[ğœ¹]=ğš². Note that unlike ğœº, which follows a nor-
mal distribution due to the central limit theorem, we do not in general
assume ğœ¹follows a normal distribution1. The zero-mean assumption
reflects the reality that across an organizationâ€™s population of A/B
tests, results will be positive, negative, null, and likely null on average.
We remark that ğš²can be estimated empirically from data. For exam-
ple, suppose we observe ğ‘equal sample-sized experiment results
each with the observed vector ğš«ğ‘ ,ğ‘ =1,...,ğ‘ . By the independence of
ğœ¹andğœº, the covariance of the observed ğš«has a trivial decomposition
Var[ğš«]=Var[ğœ¹]+Var[ğœº]=ğš²+ğšº.
1A normality assumption for ğœ¹is however made in Section 4 when we take a Bayesian
view of the problem.This leads to a sample estimate of ğš²defined as the difference between
the sample covariance matrix of ğš«and the noise covariance matrix
ğšº. When sample sizes for the set of experiments are different, we
can instead use a sample average of ğšºğ‘ ,ğ‘ =1,...,ğ‘ . There exist other
and more robust ways to estimate ğš², but this line of research is or-
thogonal to the metric decomposition methods we propose here. In
this paper weâ€™ll simply assume an estimate of ğš²is already available.
We end this section by emphasizing the symbol ğ›¿will be used to de-
scribe the true unknown treatment effect in a single experiment, but
also the random variable representing variation in true treatment ef-
fects across a population of experiments. Though we take care to dis-
tinguish this, context should dictate which version of ğ›¿is being used.
1.3 Contributions
This paper makes the following contributions to the online exper-
imentation and measurement science literature:
(1)A new framework for treatment effect estimation that exploits
metric decomposition from both frequentist (see Section 2) and
Bayesian (see Section 4) perspectives. We also share the code to
implement and reproduce our simulation studies2.
(2)Real-world applications of this new methodology in three dif-
ferent flavors: (i) engineered decomposition, (ii) natural funnel
decomposition, and (iii) adjustment of a surrogate metric. These
applications are detailed Section 3.
2 Frequentist View of Metric Decomposition
Here we overview the frequentist motivation for metric decom-
position. See Section 4 for an elaboration of the Bayesian motivation
for decomposition.
2.1 Approximately Null Augmentation
In Section 1.1 we argued that metric decomposition can exploit
disparity in signal-to-noise ratios (SNRs). We define the SNR as
Var[ğ›¿]
Var[ğœ€]. (5)
Given a decomposition with effect variance ğš²and noise variance ğšº, if
one component, say the first component (without loss of generality),
has an SNR Î›11/Î£11that is much larger than the other components,
the intuition is that Î”1is the most useful component for estimating
ğ›¿. And although the other components provide much less signal,
they can still be useful as an (approximately null augmentation)
adjustment to Î”1. This leads us to the following definition.
Definition 2.1 (Approximately Null Augmentation). ANA refers
to the family of estimators Î”âˆ—(ğ’„):=ğ’„âŠºğš«where ğ’„âŠºğ’†1=1for the
standard basis vector ğ’†1âˆˆRğ‘˜. Note that when ğ‘˜=2, this reduces to
(3) with ğ’„=(1,ğœƒ).
The theoretical results developed in this section address the follow-
ing question of theoretical and practical importance. For the purpose
of estimating ğ›¿=1âŠº
ğ‘˜ğœ¹in a manner that leverages the SNR disparity
and approximately null augmentation, what vector ğ’„should we use?
In Section 2.2, we define five potential objectives that may be used
to find the optimal augmentation vector ğ’„, and for each we state the
optimal coefficients. In Section 2.3 we explore how the proposed
metric decomposition method is related to and different from exist-
ing variance reduction methods like CUPED and the use of more
sensitive surrogate metrics.
2https://github.com/lmhagar/MetricDecomp
4887KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, and Amit Gandhi
2.2 ANA Objectives
Note that for brevity and consistency with the examples in Section
3, we consider the ğ‘˜=2case here, and hence define the optimal ğœƒ
for each objective. In Appendix A we provide the corresponding
derivations and also consider the more general ğ‘˜>2case.
Minimizing Mean Squared Error. We consider minimizing the
MSE of the ANA estimator: E[(ğ›¿âˆ’Î”âˆ—)2]. Doing so balances bias and
variance for better point estimation of effect size at the organiza-
tional level. This is a standard regression objective, except that the
responseğ›¿is not directly observed. However, because the solution
for the regression coefficients involves only the covariance of ğ›¿and
the regressors ğš«, which can all be estimated, we are still able to
compute the regression coefficients. See also [ 5,26,28]. With respect
to model (4), the value of ğœƒthat minimizes E[(ğ›¿âˆ’Î”âˆ—)2]is
ğœƒ=Î›22âˆ’Î£12
Î›22+Î£22. (6)
Maximizing Correlation. Tripuraneni et al . [28] in a slightly
different context suggest maximizing Corr[ğ›¿,Î”âˆ—], the correlation
betweenğ›¿andÎ”âˆ—. This criterion is useful from the perspective of
treating Î”âˆ—as a surrogate metric not just for estimating ğ›¿but also
for understanding the direction (i.e., sign) of the effect. With respect
to model (4), the value of ğœƒthat maximizes Corr [ğ›¿,Î”âˆ—]is
ğœƒ=(Î›12+ğšº12)(Î›11+Î›12)âˆ’(Î›12+Î›22)(Î›11+Î£11)
(Î›12+Î£12)(Î›12+Î›22)âˆ’(Î›11+Î›12)(Î›22+Î£22). (7)
It is also interesting to point out that the ANA maximizing correlation
is just a rescaled version of the posterior mean E[ğ›¿|ğš«].
Minimizing Error Variance. Whereas minimizing MSE will
inherently address the bias-variance trade-off associated with ap-
proximately null augmentation, another sensible objective would
be to directly minimize the error variance Var[ğ’„âŠºğœº]. This serves as a
lower bound for what variance reduction is possible, as it corresponds
to the optimal adjustment in CUPED. With respect to model (4), the
value ofğœƒthat minimizes Var[ğ’„âŠºğœº]is
ğœƒ=âˆ’Î£12
Î£22. (8)
Maximizing Expected Squared Z-Score. The test statistic as-
sociated with ğ»0:ğ›¿=0in an ANA analysis is the following Z-score:
Î”âˆ—/âˆšï¸
Var[ğœ€1+ğœƒğœ€2]. To increase the sensitivity of this test we may
seek to find the augmentation that maximizes the expected magni-
tude of this test statistic. We operationalize this by maximizing the
expected square of this test statistic, which based on model (4)is
Var[Î”âˆ—]/Var[ğœ€1+ğœƒğœ€2]. The optimal value of ğœƒfor this objective is
ğœƒ=âˆ’ğ‘âˆ’âˆš
ğ‘2âˆ’4ğ‘ğ‘
2ğ‘(9)
withğ‘=Î›22Î£12âˆ’Î›12Î£22,ğ‘=Î›22Î£11âˆ’Î›11Î£22,ğ‘=Î›12Î£11âˆ’Î›11Î£12.
Maximizing Power. While maximizing the expected Z-score
seeks to increase sensitivity when testing ğ»0:ğ›¿=0, this may be
achieved more directly by maximizing power. Whereas the previous
objective marginalizes over the distribution of possible ğ›¿values, we
may seek to maximize the Z-score for a specifically selected (posi-
tive)Â¤ğ›¿that reflects (for instance) an anticipated effect size of interest.
Exploiting the decomposition Â¤ğ›¿=Â¤ğ›¿1+Â¤ğ›¿2, the test statistic for this
anticipated effect is (Â¤ğ›¿1+ğœƒÂ¤ğ›¿2)/âˆšï¸
Var[ğœ€1+ğœƒğœ€2]. The value of ğœƒthat
maximizes this test statistic (and hence power) is
ğœƒ=Â¤ğ›¿1Î£12âˆ’Â¤ğ›¿2Î£11
Â¤ğ›¿2Î£12âˆ’Â¤ğ›¿1Î£22. (10)Note that rather than specifying a single effect of interestÂ¤ğ›¿3, a con-
tinuum ofÂ¤ğ›¿values could be specified and we could maximize an
â€œintegratedâ€ test statistic that aggregates across the plausible Â¤ğ›¿values.
In this case the optimal ğœƒis given by (10)but withÂ¤ğ›¿1andÂ¤ğ›¿2replaced
byÂ¯ğ›¿1andÂ¯ğ›¿2which denote the average of the Â¤ğ›¿1andÂ¤ğ›¿2values across
the continuum of interest.
We acknowledge that maximizing power and the expected squared
Z-score will lead to an increase in test rejection, but they may lead
to increasingly biased point estimates of the true effect for the un-
decomposed metric, especially when ğœƒis far away from the region
[0,1]. Therefore, we recommend bounding ğœƒwithin [0,1] when op-
timizing for power or the expected squared Z-score.
When choosing among objectives, one must recognize that there
is no uniformly superior objective; which is appropriate depends on
a practitionerâ€™s goals. If interest lies in accurately and precisely esti-
mating the treatment effect, minimizing MSE is a sensible objective; a
practitioner primarily interested in determining the sign of the effect
may seek to maximize correlation; and a practitioner interested in
increasing test sensitivity may seek to maximize power. That said,
in practice, if we are able to find decompositions with very high
SNR disparities, ANA with different objectives will not be materially
different. This is illustrated in the first two applications in Section 3.
Itâ€™s also important to emphasize that all of these objectives are
defined with respect to model (4). This means that the optimal aug-
mentations are optimal at the organizational level. This does not
necessarily imply that the objectives are satisfied at the individual
experiment level. In future work, we plan to use simulation to in-
vestigate the extent to which the objectives are/ are not satisfied for
individual experiments.
2.3 Relation to Exisiting Work
Metric decomposition follows from existing work aimed at in-
creasing metric sensitivity and statistical power. It is closely related
to CUPED, in that it augments the treatment effect estimator in the
interest of improving sensitivity. Metric decomposition can also be
viewed as a more sensitive surrogate metric of the original metric
of interest. Procedures to find the optimal augmentation ğ’„using a
set of historical experiment results is also a form of meta-analysis,
and is related to an empirical Bayesian analysis of experiments. In
the subsections below, we describe these connections to existing
methodology in more detail.
2.3.1 CUPED The CUPED method [ 12] was inspired by the method
of control variates from stochastic simulation [ 1,25]. CUPED is a
model-free method that relies only on the key observation that any
pre-experiment difference between two randomized groups is pure
noise due to randomization and should be 0in expectation as it esti-
mates a null effect. Deng et al . [13] formulated CUPED as mean-zero
augmentation:
Î”âˆ—=Î”+ğœƒÂ·Î”0. (11)
This is similar to the two-component ANA in (3), but it differs in
that the augmentation term Î”0in CUPED is assumed to be ex-
actly zero in expectation. From equation (6)we see that the optimal
ANA that minimizes MSE contains the optimal CUPED adjustment
3In the applications in Section 3, we take Â¤ğ›¿1to be the 95th percentile of N(0,Î›11)
(calibrating to detect reasonably large effects) and Â¤ğ›¿2=Î›12Â¤ğ›¿1/Î›11(the mean of the
conditional distribution of ğ›¿2|ğ›¿1=Â¤ğ›¿1).
4888Metric Decomposition in A/B Tests KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(âˆ’Cov[ğœ€1,ğœ€2]/Var[ğœ€2]) as a special case when the ANA is in fact an ex-
act mean zero augmentation (i.e., when Î›22=Var[ğ›¿2]=0). This is the
ANA that minimizes error variance in (8). Importantly, the augmen-
tation term in ANA also need not come from pre-experimental data.
ANAisanontrivialextensionandafundamentallydifferentwayto
construct augmentations, often with a much greater variance reduc-
tion possible. Beyond using pre-experiment period data or relying
on triggering conditions [ 8,13], there arenâ€™t many ways to construct
true mean-zero augmentations. It is documented by various sources
(e.g., [ 4,7]) that the amount of variance reduction elicited by CUPED
varies and in many cases can be as little as 10% or less. Recently, it
has also been shown that CUPED using pre-experiment data has a
variance reduction limit [ 27]. Greater variance reduction can only
be achieved with augmentation terms from in-experiment signals.
Metric decomposition stems from the idea of using in-experiment
observations to directly construct approximately null components
with low SNRs. As we have seen, the theory of ANA gives the optimal
adjustment to suit a variety of objectives.
2.3.2 Surrogate Metrics Instead of restricting attention to unbiased
estimators for a target metric ğ‘€, the surrogate metric literature
(e.g., [ 2,5,7,15,28]) aims to use another metricâ€”which may be an
existing metric, a functional combination of a set of metrics, or a
model prediction of the target metricâ€”as a proxy. Surrogate metrics
can often achieve greater variance reduction and greatly improve
experimentation agility when the target metric has low statistical
power. However, one drawback is that surrogate metrics are gener-
ally biased, with the degree of bias varying case by case. Choosing
and evaluating surrogate metrics is an active research area in the
A/B testing community [23].
Metric decomposition shares the similar goal of using a potentially
biased estimator in pursuit of increased sensitivity. ANA differs from
methodologies in the surrogate metric literature, however, because
we define explicitly how these potentially biased estimators arise
by breaking the target metric into components, instead of choosing
from a cohort of existing metrics or linear combinations thereof.
Moreover, the metric decomposition approach leads to further im-
provement for anysurrogate metric, because the target metric can
be decomposed by the surrogate and its residual (additive or mul-
tiplicative). In this way, ANA can be applied to further adjust any
surrogate metric by its residual. We elaborate on this in more detail,
and provide a real example in Section 3.3.
2.3.3 Meta Analysis and Empirical Bayes The way we use a set of
historical experiments to aid metric development is a form of meta-
analysis [11,17,28]. The framework of estimating the parameters of
the distribution of the treatment effect ğœ¹is also a form of empirical
Bayes [ 6,10,14,16,24,29] and multilevel (hierarchical) modeling
[19]. But to the authorsâ€™ knowledge, we are the first to study the im-
plications of replacing an observed metric value with a decomposed
vector.
3 Real-world Applications
To apply metric decomposition with approximate null augmenta-
tion, we require one or more approximately null components (ANCs)
(i.e.,Î”2in(3)orÎ”2,...,Î”ğ‘˜in Definition 2.1). This requires leverag-
ing domain knowledge and additional information to answer the
question what part of the measured outcomes is not attributed to thetreatment intervention? In this section we illustrate three applications
of ANA where bivariate metric decompositions arise by engineering
an ANC (Section 3.1), identifying an ANC that arises naturally in
a funnel decomposition (Section 3.2), and defining an ANC in the
context of a surrogate metric (Section 3.3). In each of these sections
we find that ANA improves sensitivity and increases the number
of stat. sig. results. Through A/A tests and type I error control, in
Section 3.4 we demonstrate that ANA does not arbitrarily increase
sensitivity, it increases sensitivity to non-null effects only.
3.1 Engineering an Approximately Null
Component via Counterfactual Reasoning
We applied approximately null augmentation to 39 early-stage
ranking experiments at Airbnb. The goal of these experiments was to
compare two versions of the ranking algorithm that determines the
order of displayed search results. These early-stage experiments run
for roughly 1 week taking a small percentage of total traffic. The main
target metric of interest is bookings per guest. For each search, a user
is given the ranked results which determines both (i) the list of results
shown in the feed view and on the map view, and (ii) the order of the
results listed in the feed view. See Figure 1 for an example of the feed
view and the map view together in the desktop browser experience.
In the iOS and Android apps, users can switch between the two views.
Figure 1: Example Airbnb Search Results. Feed View (left) and Map View (right).
To construct the ANC, we leverage counterfactual ranking results.
That is, for treated users (those for whom the treatment ranker gen-
erated their ranked feeds and corresponding map view), we also
compute the ranked list that would have been shown to them if they
were assigned to the control group. For control users we similarly
computed the ranked list that would have been shown to them had
they been assigned to the treatment group. We then construct the
approximately null component as described in the following steps:
(1)For each booking conversion, we used attribution logic to at-
tribute the booking to click actions from various search result
pages. The attribution is additive such that the sum of the attrib-
uted values is 1 for every booking. In this way, the attributed
values provide information on the relative importance of var-
ious click actions on the booking. Attribution methods are an
important research area in their own right. See Deng et al . [7]
for more discussion of the attribution logic and the method used
in our application.
(2)We then select a subset of attributed search result clicks leading
to every booking. A click is selected if:
(a)the clicked result is ranked among the top 4 results by both
the treatment and the control ranker, with a ranked position
difference no more than 2, or
4889KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, and Amit Gandhi
(b)both treatment and control rankers show the clicked result
on the map view, and the click happens only on the map (i.e.,
there was no click on the feed view).
(3)For each booking, define the ANC (Component 2, Î”2) as the
sum of attributed values for all selected clicks from the last step.
The signal component (Component 1, Î”1) is straightforwardly
defined as the complement of the decomposition such that the
two components sum to 1. In other words, Component 1 is the
sum of all attributed values from clicks that were notincluded
in the last step.
(4)Aggregate decomposed bookings to the user level and then to
the treatment/control group level.
The heuristics behind this process can be explained as follows. The
criterion in 2(a) considers cases where the booked listing was highly
ranked by both the factual and the counterfactual rankers. This kind
of booked listing is considered â€œeasyâ€ in the sense that any sensible
ranker would put this listing within the first few results. Moreover,
we require the ranked position difference to be no more than 2 to fur-
ther restrict the proximity of the two rankers on this booked listingâ€™s
position. The intuition is that this type of booking would have hap-
pened regardless of which ranker was used, and thus the treatment
effect should be approximately null. The criterion in 2(b) is based on
the assumption that if search results are clicked on the map (i.e., not
the feed view), and both rankers put this listing on the map, then the
booking would happen regardless of which ranker was used. Thus
the treatment effect for such clicks should be approximately null.
Note that these heuristics ignore second order effects like the possibil-
ity that a userâ€™s booking behavior also depends on the the whole set
of the results, not just the ranked position of the booked listing. How-
ever, we do not aim nor do we need to guarantee zero treatment effect
on Component 2, we only aspire to limit the treatment effect on this
component so it has a much smaller effect compared to Component 1.
The effect covariance ğš²and the average covariance of the noise
ğœºwere estimated to be (after scaling by the same constant)
ğš²=3.479âˆ’0.979
âˆ’0.979 0.672
andğšº=0.779 0.162
0.162 4.096
.
We find the approximately null component Î”2displays a noise vari-
ance 5 times larger than the signal component Î”1(4.096vs.0.779),
while the variance of the treatment effects for Î”2is less than 1/5 of
that of Î”1(0.672vs.3.479). This means the SNR of Component 2 is
much smaller than that of Component 1 (see equation (5)).
Table 1 summarizes these results and shows that the SNRs of the
two components differ by almost a factor of 30. Component 1â€™s SNR
is also more than 10 times greater than the SNR of the original metric
without decomposition. This suggests that if Component 2â€™s effect is
truly much smaller than Component 1, Î”1alone can be an estimate
for the target metricâ€™s treatment effect, with smaller noise variance
and hence much greater statistical power. Indeed, among the 39 early-
stage experiments, Component 2 was stat. sig. at a 5% level only twice
(i.e., 5.1% of the time). This is very close to the 5% significance level,
indicating that Component 2 is approximately null. Component 1,
on the other hand, was stat. sig. in 13 of the 39 experiments. Without
metric decomposition, the booking metric was only stat. sig. 6 times.
We applied the five versions of ANA adjustment discussed in
Section 2.2 in this example: ANA to maximize correlation (denoted
ANA c), to minimize mean squared error (denoted ANA e), to mini-
mize variance (denoted ANA v), to maximize the expected squaredZ-score (denoted ANA z) and to maximize power (denoted ANA p).
Table 1 demonstrates that all these adjustment methods yield similar
results to analyses using Component 1 alone, though ANA ehas one
less stat. sig. result out of 39 experiments. This is because the adjust-
ment coefficient ğœƒfor all objectives tends to be relatively small for
these experiments. Figure 2 plots the optimal ğœƒvalues. We see these
values range between -0.2 to 0.2. Figure 3 shows the five ANA esti-
mates Î”âˆ—. Despite different objectives, estimates in this application
donâ€™t differ materially, aligning with the similar test results in Table 1.
Finally, Figure 4 compares the variances of these ANA estimators.
We see that minimum variance objective (ANA v) provides the lower
bound of what variance is achievable through augmentation. In gen-
eral, minimizing variance directly could lead to more bias relative
to using the high SNR component ( Î”1) alone, since the second com-
ponent ( Î”2) is only approximately null. However, in this application
Component 2â€™s SNR is so low (relative to Component 1), it suggests
augmentation by Component 2 is essentially a null augmentation.
Figure 2: Optimal ğœƒfor various ANA objectives in Application 1.
Figure 3: Comparison of ANA estimates in Application 1.
Figure 4: Comparison of ANA estimator variances in Application 1.
4890Metric Decomposition in A/B Tests KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Comp
. 1 (Î”1) Comp. 2 ( Î”2) No Decomposition ( Î”) ANA c(Î”âˆ—ğ‘) ANA e(Î”âˆ—ğ‘’) ANA v(Î”âˆ—ğ‘£) ANA z(Î”âˆ—ğ‘§) ANA p(Î”âˆ—ğ‘)
Signal: V
ar[ğ›¿] 3.479 0.672 2.193
Noise: Var[ğœ€] 0.779 4.096 5.198
Signal-Noise-Ratio 4.466 0.164 0.422
Proportion of Stat. Sig. 13/39 (33.3%) 2/39 (5.1%) 6/39 (15.4%) 13/39 (33.3%) 12/39 (30.8%) 13/39 (33.3%) 13/39 (33.3%) 13/39 (33.3%)
Table 1: Results of Application 1 (Engineering an ANC via Counterfactual Reasoning).
3.2 A Metric
with Natural Multiplicative Decomposition
In the last section we exploited domain knowledge and context-
specific information to engineer an approximately null component.
Here we consider a context in which an ANC arises naturally in a con-
version funnel where the treatment intervention mainly impacts one
step of the funnel and has close to zero impact on the other steps. To
illustrate this, we study the metric nights per guest which quantifies
the number of nights booked per guest. This metric naturally decom-
poses into nights per booking, and bookings per guest. As explained
in Section 1.1, a multiplicative decomposition of percent treatment
effects can be treated as an additive decomposition when the lifts
are expected to be small. In this study, we use the decomposition
Î”%(Nights/Guest)â‰ˆ Î”%(Nights/Booking)+ Î”%(Bookings/Guest)
where Î”%(Nights/Booking) is the approximately null component
(Î”2), and Î”%(Bookings/Guest) is the signal component ( Î”1).
We analyze 116 past A/B tests separately with each of the three
metrics: nights per guest, nights per booking, and bookings per
guest. The results are summarized in Table 2. Generally speaking,
the results are very similar to those from the previous application
in Table 1. First, Component 2 (nights per booking) has close to
5% empirical stat. sig. rate (5 out of 116) with a very low SNR of
0.014. Second, Component 1 (bookings per guest) has a much larger
SNR, and higher empirical stat. sig. rate. (30 out of 116). Analysis
with Component 1 also shows better performance than an analysis
without decomposition (i.e., analyzing with respect to nights per
booking), which has just 11 out of 116 stat. sig. results. Further, anal-
yses with all five ANA adjustments give similar results to analyses
with Component 1 alone. As with the previous application, this is
because the optimal ğœƒvalues (though not pictured here) are close to 0.
3.3 Adjustment of a Surrogate Metric
As discussed in Section 2.3, an important area of related work
that also seeks to increase metric sensitivity is to build a surrogate
or proxy metric. The goal is to use one or more candidate metrics
to form an index to better track the treatment effect of a metric of
interest, or construct a model-based prediction for the metric of
interest using a set of predictors [2, 7, 15, 28].
Letğ‘†be a surrogate metric for a metric ğ‘€, then this implies a
decomposition
Î”(ğ‘€)=Î”(ğ‘†)+Î”(ğ‘…),
whereğ‘…=ğ‘€âˆ’ğ‘†is the residual. Therefore any surrogate metric is
always associated with a decomposition, and the surrogate metric
framework can therefore be seen as a special case of metric decom-
position. Moreover, if a surrogate metric is unbiased, then
E[Î”(ğ‘€)]=E[Î”(ğ‘†)],andE[Î”(ğ‘…)]=0. This means a surrogate without bias is also a decom-
position with null augmentation!4However, in practice we donâ€™t
expect to achieve an unbiased surrogate and instead aim for small
bias; this of course lends itself well to the benefits of approximately
null augmentation. Thus, we advocate for the general use of the
metric decomposition and ANA framework for two reasons:
(1)Defining surrogate metrics and then verifying their small bias is
often harder than directly constructing a decomposition that has
an approximately null effect. The latter is more straightforward
because we can leverage natural decompositions from conver-
sion funnels or leverage domain knowledge and counterfactual
information, as demonstrated in the previous two applications.
(2)Even when a surrogate metric is available, we can always ap-
ply an ANA adjustment to the decomposition implied by the
surrogate and its residual.
This latter point is illustrated in this application where we took
a surrogate metric for booking-per-guest and studied its decompo-
sition across 133 experiments. This surrogate metric utilizes a set of
upper funnel signals to predict a future conversion. It is known that
this type of surrogate metric is only unbiased under strong surrogacy
assumptions [ 2]. Table 3 summarizes the results. Comparing to the
previous two applications, one distinct difference is that Component
2 (the residual) has a noticeably greater SNR (1.044), and a higher
(12.8%, 17 out of 133) stat. sig. rate than the nominal 5% significance
level. This indicates that Component 2 may still contain some treat-
ment effect not fully captured by the surrogate metric (Component
1). Nevertheless, relative to the original (un-decomposed) metric, the
surrogate component has a greater SNR (6.456 vs. 1.794) and higher
statistical power (55 out of 133 stat. sig. results compared to 34 out
of 133).
ANA in this case would create an adjusted estimate of the form
Î”(ğ‘†)+ğœƒ[Î”(ğ‘€)âˆ’Î”(ğ‘†)], that pulls the surrogate metric closer to the
original metric by the factor of ğœƒ. Figure 5 displays the optimal ANA
ğœƒacross these experiments for each of the 5 objectives discussed in
Section 2.2. We see that maximizing correlation and minimizing er-
ror resulted in larger ğœƒranging from 0.4 to 0.8, and a smaller number
of stat. sig. results (45 out of 133). On the other hand, minimizing
variance, maximizing expected squared Z-score, and maximizing
power resulted in smaller values of ğœƒ(ranging between 0 and 0.3) so
the ANA estimator is closer to using the surrogate metric (Compo-
nent 1, Î”1) directly. Interestingly, these latter 3 objectives resulted
in slightly more stat. sig. results than the surrogate metric alone.
3.4 ANA in A/A Tests
In the previous three subsections, we have celebrated an increase
in the number of stat. sig. results when using ANA versus an unde-
composed analysis. However, itâ€™s important to consider whether this
increase in stat. sig. results coincides with an increase in type I error.
4We can also use multiplicative decomposition with ğ‘…=ğ‘€/ğ‘†andÎ”%(ğ‘€)=
Î”%(ğ‘†)+Î”%(ğ‘…).
4891KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, and Amit Gandhi
Comp
. 1 (Î”1) Comp. 2 ( Î”2) No Decomposition ( Î”) ANA c(Î”âˆ—ğ‘) ANA e(Î”âˆ—ğ‘’) ANA v(Î”âˆ—ğ‘£) ANA z(Î”âˆ—ğ‘§) ANA p(Î”âˆ—ğ‘)
Signal: V
ar[ğ›¿] 6.508 0.074 5.198
Noise: Var[ğœ€] 2.810 5.321 8.133
Signal-Noise-Ratio 2.316 0.014 0.639
Proportion of Stat. Sig. 30/116 (25.9%) 5/116 (4.3%) 11/116 (9.5%) 30/116 (25.9%) 30/116 (25.9%) 30/116 (25.9%) 30/116 (25.9%) 30/116 (25.9%)
Table 2: Results of Application 2 (Natural Multiplicative Decomposition).
Comp
. 1 (Î”1) Comp. 2 ( Î”2) No Decomposition ( Î”) ANA c(Î”âˆ—ğ‘) ANA e(Î”âˆ—ğ‘’) ANA v(Î”âˆ—ğ‘£) ANA z(Î”âˆ—ğ‘§) ANA p(Î”âˆ—ğ‘)
Signal: V
ar[ğ›¿] 1.011 0.352 0.585
Noise: Var[ğœ€] 0.157 0.337 0.326
Signal-Noise-Ratio 6.456 1.044 1.794
Proportion of Stat. Sig. 55/133 (41.4%) 17/133 (12.8%) 34/133 (25.6%) 45/133 (33.8%) 45/133 (33.8%) 57/133 (42.9%) 58/133 (43.6%) 58/133 (43.6%)
Table 3: Results of Application 3 (Adjustment of a Surrogate Metric).
Figure 5: Optimal ğœƒfor various ANA objectives in Application 3.
Figure 6: Empirical distribution of p-values of an ANA estimator from 1000
A/A tests.
In this section we emphasize that ANA does not increase sensitivity
in general, it increases sensitivity to non-null effects. To demonstrate
that ANA does not inflate type I error, we simulated 1000 A/A tests
(where the treatment effect is truly null) and performed an ANA-
based analysis on each. In particular, we randomly split one experi-
mentâ€™s data into pseudo treatment and control groups 1000 times and
computed p-values for ğ»0:ğ›¿=0when the estimator is taken to be Î”âˆ—ğ‘
(i.e., ANA to maximize correlation). Figure 6 shows that the p-values
for these tests were uniformly distributed as expected. The empirical
proportions of p-values less than 5% and 10% were respectively 0.05
and 0.105 and hence close to nominal. Though not shown here, other
augmentations yielded similar behavior. This should provide assur-
ance that ANA is not arbitrarily increasing the number of stat. sig.
results; it is instead increasing sensitivity to truly non-null effects.
4 Bayesian View on Metric Decomposition
Here we elaborate on the Bayesian motivation for metric decom-
position. In Section 4.1 we demonstrate theoretically and through
an example that metric decomposition reduces posterior variance.And in Section 4.2, we use simulation to explore the circumstances
under which the variation reduction elicited by decomposition is
large or small.
4.1 Posterior Variance Reduction
The methodology in Section 2, which is predicated on the random
effects model (4), is closely related to a Bayesian analysis where we
posit a prior distribution for ğ›¿and perform inference via posterior
analyses [ 6,10,14,16,24,29]. It is well-known that the Bayesian
posterior mean will shrink the observed frequentist point estimate
towards the global mean of the prior, where the shrinkage factor is re-
lated to the signal-to-noise ratio. Given a metric decomposition with
SNR disparity, the Bayesian posterior mean should shrink each com-
ponent very differently, resulting in a posterior mean that depends
more on high SNR components. Of interest is to investigate whether
this new posterior distribution exhibits reduced posterior variance.
We prove here that when we assume ğœ¹has a multivariate normal
prior with covariance matrix ğš², at least for the two-component case,
the posterior variance of ğ›¿conditioned on the bivariate vector ğš«
cannot exceed the posterior variance of ğ›¿conditioned only on the
univariate Î”. This is also true for the general ğ‘˜>2case when the
noise covariance matrices ğšºandğš²are co-linear. These results are
established in Theorem 1 below.
Theorem 1. Metric decomposition naturally leads to variance re-
duction under the Bayesian framework with a Gaussian prior for ğœ¹.
The posterior variances of ğ›¿conditioned on ğš«=(Î”1,Î”2)andÎ”are
respectively
Var[ğ›¿|ğš«]=1âŠº
ğ‘˜(ğš²âˆ’ğš²(ğš²+ğšº)âˆ’1ğš²)1ğ‘˜,
Var[ğ›¿|Î”]=1âŠº
ğ‘˜ğš²1ğ‘˜Ã—1âŠº
ğ‘˜ğšº1ğ‘˜
1âŠº
ğ‘˜(ğš²+ğšº)1ğ‘˜.
Whenğ‘˜=2, the posterior variance of ğ›¿=ğ›¿1+ğ›¿2under bivariate
decomposition cannot exceed the univariate posterior variance, i.e.,
Var[ğ›¿|ğš«]â‰¤Var[ğ›¿|Î”]. (12)
Whenğ‘˜â‰¥2âˆˆN, the above inequality holds strictly when ğšº=ğ‘ğš²for
some scalar constant ğ‘âˆˆR.
The proof is provided in Appendix B. Even though we have not
provedtheinequalityin (12)forgeneralğ‘˜(withoutthestrongcollinear-
ity assumption), we conjecture the result holds under much milder
assumptions and leave this investigation for future theoretical de-
velopment.
Next we demonstrate this posterior variance reduction in the con-
text of Application 1 from Section 3.1. The results in that section
4892Metric Decomposition in A/B Tests KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
corresponded to frequentist analyses, but we also analyzed each of
the 39 experiments from the Bayesian perspective, in line with Theo-
rem 1. As the theory suggests, the left panel in Figure 7 demonstrates
that with a Gaussian prior, the posterior variance is greatly reduced
with the bivariate decomposition compared to the univariate analy-
sis without decomposition. Furthermore, the right panel in Figure 7
also illustrates that the Bayesian Z-score (posterior mean divided by
posterior standard deviation) tends to have a larger magnitude un-
der the bivariate decomposition. However, this result does not hold
uniformly; 1 of the 39 experiments has a larger Z-score with the non-
decomposed analysis. Thus, Theorem 1 guarantees a variance reduc-
tion, but it does not guarantee an increase in power; sometimes the
reduction in the size of the posterior mean may be substantial enough
to offset the variance reduction achieved with the decomposition.
Figure 7: Comparison of bivariate decomposed vs. univariate non-decomposed
models with respect to posterior variances (left) and Bayesian Z-scores (right).
4.2 Simulation
Illustrating the Benefit of Decomposition
In Section 4.1 we established that the posterior variance of ğ›¿when
conditioned on ğš«cannot be larger than when conditioned on Î”.
However, we did not explore what variation reduction is achievable
by decomposition, nor did we explore when the variation reduction
is negligible. The numerical study presented in this section provides
insights into this. Here we use a more helpful parameterization of
ğš²andğšº:
ğš²=ğœ†111âˆš
ğ¾ğœŒğœ†âˆš
ğ¾ğœŒğœ†ğ¾
andğšº=ğœ†11"
1/ğ‘†1âˆšï¸
ğ¾/(ğ‘†1ğ‘†2)ğœŒÎ£âˆšï¸
ğ¾/(ğ‘†1ğ‘†2)ğœŒÎ£ğ¾/ğ‘†2#
,
(13)
whereğœ†11=Var[ğ›¿1],ğ¾=Var[ğ›¿2]/ğœ†11,ğœŒğœ†=Corr[ğ›¿1,ğ›¿2],ğœŒÎ£=
Corr[ğœ€1,ğœ€2], andğ‘†1=Var[ğ›¿1]/Var[ğœ€1]andğ‘†2=Var[ğ›¿2]/Var[ğœ€2]
are signal-to-noise ratios. We can freely vary these parameters and
still satisfy the Cauchy-Schwarz inequality.
Here we compare the posterior variances in the decomposed and
un-decomposed models for each combination of the following pa-
rameter values:
â€¢ğ¾,ğ‘†1,ğ‘†2={0.01,0.11,...,0.91,1.01,2,3,4,5}
â€¢ğœŒğœ†,ğœŒÎ£={âˆ’0.975,âˆ’0.925,...,0.975}
Because changing the value for ğœ†11just scales ğš²andğšºby the same
constant, we do not consider it in our simulations. For each of these
5.4Ã—106combinations, we computed the ratio of variances in the
posteriors that do and do not account for the bivariate decomposi-
tion. As expected, we found the variances to be equal when ğœŒğœ†=ğœŒÎ£
andğ‘†1=ğ‘†2. Under these conditions, ğšº=ğ‘ğš².Across the 5.4Ã—106parameter combinations, we found that vari-
ance reduction is greatest when three conditions are satisfied: the
signal-to-noise ratios ğ‘†1andğ‘†2differ substantially, |ğœŒğœ†|is large, and
|ğœŒÎ£|is large. To summarize, we visualize the magnitude of the vari-
ancereductionfactorundervariousscenarioswheretheseconditions
are and are not satisfied. Figure 8 plots the density curves of the vari-
ance reduction factor under several scenarios with small and large
|ğœŒğœ†|and|ğœŒÎ£|when|ğ‘†1âˆ’ğ‘†2|>2. In all such scenarios, the SNRs differ
substantially. As expected, the median reduction factor is largest in
the top left plot, where |ğœŒğœ†|and|ğœŒÎ£|are large. The plots on the off-
diagonals consider scenarios where only one of |ğœŒğœ†|or|ğœŒÎ£|is large.
Figure 8 suggests that strong correlation between ğ›¿1andğ›¿2is more
beneficial than strong correlation between ğœ€1andğœ€2. The median vari-
ance reduction factor is smallest in the bottom right plot, where |ğœŒğœ†|
and|ğœŒÎ£|are small. Moreover, we find that when SNRs are relatively
similar (i.e.,|ğ‘†1âˆ’ğ‘†2|<0.1), analogous plots (See Appenix C) indi-
cate minimal variance reduction, no matter the values of |ğœŒğœ†|or|ğœŒÎ£|.
These results suggest that discrepancies (or the lack thereof) between
the SNRs play a greater role than |ğœŒğœ†|or|ğœŒÎ£|in variance reduction.
Figure 8: Density curves of the variance reduction factor for several conditions
when signal-to-noise ratios differ substantially. Median reduction factors are
given by the dashed vertical lines and annotated text.
5 Conclusion & Discussion
In this paper we have proposed metric decomposition as a novel
means to improve metric sensitivity in online A/B tests. The idea
is premised upon the decomposition of a target metric into two or
more components that differ with respect to their signal-to-noise
ratios. We show through theory, simulation, and empirical examples
that if such a decomposition exists (or can be engineered), sensi-
tivity may be increased via approximately null augmentation (in a
frequentist setting) and posterior variance is reduced (in a Bayesian
setting). We provide practical guidance for, and discuss the implica-
tions of, metric decomposition in both settings. We also contrast it
with industry-favorite alternatives like CUPED, and in doing so high-
light its broad utility. An important extension to this work would be
to next consider sample size determination in both the frequentist
or Bayesian contexts; while a boost in sensitivity typically means
less data is required for a given analysis, a methodology that deter-
mines the smallest sample size required to control various operating
characteristics in this context would be of practical value.
4893KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, and Amit Gandhi
References
[1] Soren Asmussen and Peter Glynn. 2008. Stochastic Simulation. Springer-Verlag.
[2]Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. The surro-
gate index: Combining short-term proxies to estimate long-term treatment effects more
rapidly and precisely. Technical Report. National Bureau of Economic Research.
[3]Iavor Bojinov and Somit Gupta. 2022. Online Experimentation: Benefits,
Operational and Methodological Challenges, and Scaling Guide. Harvard Data
Science Review 4, 3 (jul 28 2022). https://hdsr.mitpress.mit.edu/pub/aj31wj81.
[4]Laura Cosgrove, Jen Townsend, and Jonathan Litz. [n. d.]. Deep Dive Into Variance
Reduction. https://www.microsoft.com/en-us/research/group/experimentation-
platform-exp/articles/deep-dive-into-variance-reduction/.
[5]Tom Cunningham. 2023. Experiment Interpretation and Extrapolation.
https://tecunningham.github.io/posts/2023-04-18-experiment-interpretation-
extrapolation.html
[6]Alex Deng. 2015. Objective Bayesian Two Sample Hypothesis Testing for Online
Controlled Experiments. In Proceedings of the 24th International Conference on
World Wide Web Companion. 923â€“928.
[7]Alex Deng, Michelle Du, Anna Matlin, and Qing Zhang. 2023. Variance Reduction
Using In-Experiment Data: Efficient and Targeted Online Measurement for Sparse
and Delayed Outcomes. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 3937â€“3946.
[8]Alex Deng and Victor Hu. 2015. Diluted treatment effect estimation for trigger
analysis in online controlled experiments. In Proceedings of the Eighth ACM
International Conference on Web Search and Data Mining. 349â€“358.
[9]Alex Deng, Ulf Knoblich, and Jiannan Lu. 2018. Applying the Delta Method in
Metric Analytics: A Practical Guide with Novel Ideas. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(London, United Kingdom) (KDD â€™18). ACM, New York, NY, USA, 233â€“242.
[10] Alex Deng, Yicheng Li, Jiannan Lu, and Vivek Ramamurthy. 2021. On Post-
selection Inference in A/B Testing. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 2743â€“2752.
[11] Alex Deng and Xiaolin Shi. 2016. Data-driven metric development for online
controlled experiments: Seven lessons learned. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
[12] Alex Deng, Ya Xu, Ron Kohavi, and Toby Walker. 2013. Improving the sensitivity
of online controlled experiments by utilizing pre-experiment data. In Proceedings
of the 6th ACM WSDM Conference. 123â€“132.
[13] Alex Deng, Lo-Hua Yuan, Naoya Kanai, and Alexandre Salama-Manteau. 2023.
Zero to hero: Exploiting null effects to achieve variance reduction in experiments
with one-sided triggering. In Proceedings of the Sixteenth ACM International
Conference on Web Search and Data Mining. 823â€“831.
[14] Drew Dimmery, Eytan Bakshy, and Jasjeet Sekhon. 2019. Shrinkage Estimators
in Online Experiments. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2914â€“2922.
[15] Weitao Duan, Shan Ba, and Chunzhe Zhang. 2021. Online Experimentation with
Surrogate Metrics: Guidelines and a Case Study. In Proceedings of the 14th ACM
International Conference on Web Search and Data Mining. 193â€“201.
[16] Bradley Efron. 2010. Large-scale Inference: Empirical Bayes Methods for Estimation,
Testing and Prediction. Cambridge University Press.
[17] Michael R Elliott, Anna SC Conlon, Yun Li, Nico Kaciroti, and Jeremy MG Taylor.
2015. Surrogacy marker paradox measures in meta-analytic settings. Biostatistics
16, 2 (2015), 400â€“412.
[18] Andrew Gelman and John Carlin. 2014. Beyond power calculations: Assessing
type S (sign) and type M (magnitude) errors. Perspectives on Psychological Science
9, 6 (2014), 641â€“651.
[19] Andrew Gelman and Jennifer Hill. 2006. Data analysis using regression and
multilevel/hierarchical models. Cambridge University Press.
[20] Somit Gupta et al .2019. Top Challenges from the First Practical Online Controlled
Experiments Summit. SIGKDD Explor. Newsl. 21, 1 (May 2019), 20â€“35.
[21] Ron Kohavi, Alex Deng, and Lukas Vermeer. 2022. A/B Testing Intuition Busters:
Common Misunderstandings in Online Controlled Experiments. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
3168â€“3177.
[22] Ron Kohavi, Diane Tang, and Ya Xu. 2020. Trustworthy online controlled
experiments: A practical guide to a/b testing. Cambridge University Press.
[23] Nicholas Larsen, Jonathan Stallrich, Srijan Sengupta, Alex Deng, Ron Kohavi, and
Nathaniel T Stevens. 2023. Statistical challenges in online controlled experiments:
A review of a/b testing methodology. The American Statistician (2023), 1â€“15.
[24] Sareh Nabi, Houssam Nassif, Joseph Hong, Hamed Mamani, and Guido Imbens.
2022. Bayesian meta-prior learning using Empirical Bayes. Management Science
68, 3 (2022), 1737â€“1755.
[25] Art B Owen. 2013. Monte Carlo theory, methods and examples. (2013).
[26] Alexander Peysakhovich and Dean Eckles. 2018. Learning causal effects from
many randomized experiments using regularized instrumental variables. In
Proceedings of the 2018 World Wide Web Conference. 699â€“707.
[27] Daniel Ting and Kenneth Hung. 2023. On the Limits of Regression Adjustment.
arXiv preprint arXiv:2311.17858 (2023).[28] Nilesh Tripuraneni, Lee Richardson, Alexander Dâ€™Amour, Jacopo Soriano, and
Steve Yadlowsky. 2023. Choosing a Proxy Metric from Past Experiments. arXiv
preprint arXiv:2309.07893 (2023).
[29] Runzhe Wan, Yu Liu, James McQueen, Doug Hains, and Rui Song. 2023. Exper-
imentation platforms meet reinforcement learning: Bayesian sequential decision-
making for continuous monitoring. arXiv preprint arXiv:2304.00420 (2023).
Appendix
A ANA Derivations
Consider ANA estimators Î”âˆ—:=ğ’„âŠºğš«, where ğ’„âŠºğ’†1=1for the stan-
dard basis vector ğ’†1. Letğ’„âˆ—be the finalğ‘˜âˆ’1components of ğ’„andğš²âˆ—
be the(ğ‘˜âˆ’1)Ã—(ğ‘˜âˆ’1)submatrix of ğš²corresponding to(ğ›¿2,...,ğ›¿ğ‘˜).
Letğšºâˆ—and be the(ğ‘˜âˆ’1)Ã—(ğ‘˜âˆ’1)submatrix of ğšºcorresponding to
(ğœ€2,...,ğœ€ğ‘˜). Letğšº1=(Î£12,...,Î£1ğ‘˜).
Minimizing Mean Squared Error. We have that
E
(ğ›¿âˆ’Î”âˆ—)2
=Eh
(1âŠº
ğ‘˜ğœ¹âˆ’ğ’„âŠº(ğœ¹+ğœº))2i
=E
((1ğ‘˜âˆ’ğ’„)âŠºğœ¹âˆ’ğ’„âŠºğœº)2
=E[((1ğ‘˜âˆ’ğ’„)âŠºğœ¹ğœ¹âŠº(1ğ‘˜âˆ’ğ’„)âˆ’2(1ğ‘˜âˆ’ğ’„)âŠºğœ¹ğœºâŠºğ’„+ğ’„âŠºğœºğœºâŠºğ’„]
=(1ğ‘˜âˆ’ğ’„)âŠºğš²(1ğ‘˜âˆ’ğ’„)+ğ’„âŠºğšºğ’„
=(1ğ‘˜âˆ’1âˆ’ğ’„âˆ—)âŠºğš²âˆ—(1ğ‘˜âˆ’1âˆ’ğ’„âˆ—)+Î£11+2ğ’„âŠº
âˆ—ğšº1+ğ’„âŠº
âˆ—ğšºâˆ—ğ’„âˆ—.(A.1)
The penultimate step follows because ğœ¹andğœºare independent, and
the final equality holds because ğ’„âŠºğ’†1=1. The derivative of (A.1) with
respect to ğ’„âˆ—is
ğœ•
ğœ•ğ’„âˆ—E
(ğ›¿âˆ’Î”âˆ—)2
=âˆ’2ğš²âˆ—(1ğ‘˜âˆ’1âˆ’ğ’„âˆ—)+2ğšº1+2ğšºâˆ—ğ’„âˆ—. (A.2)
The value for ğ’„âˆ—that equates the expression in (A.2) to 0and hence
minimizes mean squared error is
ğ’„âˆ—=
ğš²âˆ—+ğšºâˆ—âˆ’1
ğš²âˆ—1ğ‘˜âˆ’1âˆ’ğšº1
. (A.3)
The value of ğœƒgiven in (6)is obtained as a special case when ğ‘˜=2
andğ’„=(1,ğœƒ)âŠº. â–¡
Maximizing Correlation. We have that
Corr ğ›¿,Î”âˆ—=Cov(1âŠº
ğ‘˜ğœ¹,ğ’„âŠºğš«)
âˆšï¸ƒ
Var(1âŠº
ğ‘˜ğœ¹)âˆšï¸
Var(ğ’„âŠºğš«)
=1âŠº
ğ‘˜ğš²ğ’„
âˆšï¸ƒ
1âŠº
ğ‘˜ğš²1ğ‘˜âˆšï¸
ğ’„âŠº(ğš²+ğšº)ğ’„.(A.4)
To maximize (A.4), we take the partial derivative
ğœ•
ğœ•ğ’„(1âŠº
ğ‘˜ğš²ğ’„)2
ğ’„âŠº(ğš²+ğšº)ğ’„=2(1âŠº
ğ‘˜ğš²ğ’„)ğš²1ğ‘˜ğ’„âŠº(ğš²+ğšº)ğ’„âˆ’2(ğš²+ğšº)ğ’„(1âŠº
ğ‘˜ğš²ğ’„)2
(ğ’„âŠº(ğš²+ğšº)ğ’„)2.
(A.5)
Equating the numerator of (A.5) to 0prompts the following result:
(ğš²+ğšº)âˆ’1ğš²1ğ‘˜=1âŠº
ğ‘˜ğš²ğ’„
ğ’„âŠº(ğš²+ğšº)ğ’„Ã—ğ’„. (A.6)
Since the scaling constant to the left of the Ã—sign in (A.6) is ap-
plied to each component of ğ’„, we have that ğ‘ºâŠº1ğ‘˜âˆğ’„. To enforce the
constraint that ğ’„âŠºğ’†1=1, we require
ğ’„=1
ğ’†âŠº
1ğ‘ºâŠº1ğ‘˜ğ‘ºâŠº1ğ‘˜. (A.7)
This is the augmentation that maximizes correlation. The value of ğœƒ
given in (7)is obtained as a special case when ğ‘˜=2andğ’„=(1,ğœƒ)âŠº.â–¡
Minimizing Error Variance. We have that
Var[ğ’„âŠºğœº]=ğ’„âŠºğšºğ’„
=Î£11+2ğ’„âŠº
âˆ—ğšº1+ğ’„âŠº
âˆ—ğšºâˆ—ğ’„âˆ—(A.8)
4894Metric Decomposition in A/B Tests KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The final equality holds because ğ’„âŠºğ’†1=1. The derivative of (A.8)
with respect to ğ’„âˆ—is
ğœ•
ğœ•ğ’„âˆ—Var[ğ’„âŠºğœº]=2(ğšº1+ğšºâˆ—ğ’„âˆ—). (A.9)
The value for ğ’„âˆ—that equates the expression in (A.9) to 0and hence
minimizes the error variance is
ğ’„âˆ—=âˆ’ğšºâˆ’1
âˆ—ğšº1. (A.10)
The value of ğœƒgiven in (8)is obtained as a special case when ğ‘˜=2
andğ’„=(1,ğœƒ)âŠº. â–¡
Maximizing Expected Squared Z-Score. The expected squared
Z-score has the form:
Var[Î”âˆ—]
Var[ğ’„âŠºğœº]=ğ’„âŠº(ğš²+ğšº)ğ’„
ğ’„âŠºğšºğ’„
=[ğšº1/2ğ’„]âŠº[ğšºâˆ’1/2(ğš²+ğšº)ğšºâˆ’1/2][ğšº1/2ğ’„]
[ğšº1/2ğ’„]âŠº[ğšº1/2ğ’„].(A.11)
Letğ’™=ğšº1/2ğ’„, and (A.11) is a Rayleigh quotient maximized when ğ’™is
any multiple of the first eigenvector of the matrix ğšºâˆ’1/2(ğš²+ğšº)ğšºâˆ’1/2.
Letğ’™âˆ—be this eigenvector. Then it is easy to see
ğ’„=1
ğ’†âŠº
1ğšºâˆ’1/2ğ’™âˆ—ğšºâˆ’1/2ğ’™âˆ—. (A.12)
The value of ğœƒgiven in (9)is obtained as a special case when ğ‘˜=2
andğ’„=(1,ğœƒ)âŠº. â–¡
Maximizing Power. For a specific value Â¤ğœ¹=(Â¤ğ›¿1,...,Â¤ğ›¿âŠº
ğ‘˜)from the
ğœ¹distribution specified by model (4), the test statistic for testing
ğ»0:ğ›¿=0is given by
ğ’„âŠºÂ¤ğœ¹âˆš
ğ’„âŠºğšºğ’„=Â¤ğ›¿1+ğ’„âŠº
âˆ—Â¤ğœ¹âˆ—âˆšï¸
Î£11+2ğ’„âŠº
âˆ—ğšº1+ğ’„âŠº
âˆ—ğšºâˆ—ğ’„âˆ—(A.13)
whereÂ¤ğœ¹âˆ—is the finalğ‘˜âˆ’1components ofÂ¤ğœ¹. This equality holds
because ğ’„âŠºğ’†1=1. The derivative of (A.13) with respect to ğ’„âˆ—is
Â¤ğœ¹âŠº
âˆ—(Î£11+2ğ’„âŠº
âˆ—ğšº1+ğ’„âŠº
âˆ—ğšºâˆ—ğ’„âˆ—)âˆ’(Â¤ğ›¿1+ğ’„âŠº
âˆ—Â¤ğœ¹âˆ—)(ğšºâŠº
1+ğ’„âŠº
âˆ—ğšºâˆ—)
(Î£11+2ğ’„âŠº
âˆ—ğšº1+ğ’„âŠº
âˆ—ğšºâˆ—ğ’„âˆ—)3/2(A.14)
The value for ğ’„âˆ—that equates the expression in (A.14) to 0and hence
maximizes power is
ğ’„âˆ—=âˆ’
ğšº1Â¤ğœ¹âŠº
âˆ—âˆ’Â¤ğ›¿1ğšºâˆ—âˆ’1
Î£11Â¤ğœ¹âˆ—âˆ’Â¤ğ›¿1ğšº1
. (A.15)
The value of ğœƒgiven in (10)is obtained as a special case when ğ‘˜=2
andğ’„=(1,ğœƒ)âŠº. â–¡
B Proof of Theorem 1
For this proof, we use the following parameterization for ğš²andğšº:
ğš²=ğ¿2ğ¿âˆšğœ†22ğœŒğœ†
ğ¿âˆšğœ†22ğœŒğœ†ğœ†22
andğšº=Î£11Î£12
Î£12Î£22
.
That is,ğ¿=âˆšğœ†11. This parameterization allows us to freely vary ğ¿
across R+while satisfying the Cauchy-Schwarz inequality. We now
show that each side of the inequality in (12) can be expressed as the
ratio of two quadratic functions of ğ¿.
For the left side of (12), we show that 1âŠº
2(ğš²âˆ’ğš²(ğš²+ğšº)âˆ’1ğš²)12takes
the form
ğ‘1ğ¿2+ğ‘2ğ¿+ğ‘3
ğ‘4ğ¿2+ğ‘5ğ¿+ğ‘6. (B.1)
Through simple algebra, we can show that ğ‘1=ğœ†22(1âˆ’ğœŒ2
ğœ†)(Î£11+
2Î£12+Î£22)+Î£11Î£22âˆ’Î£2
12,ğ‘2=2âˆšğœ†22ğœŒğœ†(Î£11Î£22âˆ’Î£2
12), andğ‘3=
ğœ†22(Î£11Î£22âˆ’Î£2
12). The denominator of (B.1) is the determinant of
ğš²+ğšº; it takes the form ğ‘4ğ¿2+ğ‘5ğ¿+ğ‘6, whereğ‘4=ğœ†22(1âˆ’ğœŒ2
ğœ†)+Î£22,
ğ‘5=âˆ’2âˆšğœ†22ğœŒğœ†Î£12, andğ‘6=Î£11ğœ†22+Î£11Î£22âˆ’Î£2
12.The algebra is simpler to show the right side of (12) takes the form
ğ‘7ğ¿2+ğ‘8ğ¿+ğ‘9
ğ‘10ğ¿2+ğ‘11ğ¿+ğ‘12. (B.2)
That numerator 1âŠº
2ğš²12Ã—1âŠº
2ğšº12is such that ğ‘7=Î£11+2Î£12+Î£22,
ğ‘8=2âˆšğœ†22ğœŒğœ†(Î£11+2Î£12+Î£22), andğ‘9=ğœ†22(Î£11+2Î£12+Î£22). That
denominator 1âŠº
2(ğš²+ğšº)12can be expressed as ğ‘10ğ¿2+ğ‘11ğ¿+ğ‘12,
whereğ‘10=1,ğ‘11=2âˆšğœ†22ğœŒğœ†, andğ‘12=ğœ†22+Î£11+2Î£12+Î£22. The de-
nominator of (B.1) must be non-negative due to the Cauchy Schwarz
inequality:ğ‘4ğ¿2+ğ‘5ğ¿+ğ‘6â‰¥0for allğ¿â‰¥0. Moreover, the denominator
of (B.2) is a variance, so ğ‘10ğ¿2+ğ‘11ğ¿+ğ‘12â‰¥0for allğ¿â‰¥0.
We can therefore cross multiply the fractions in (B.1) and (B.2) to
obtain an equivalent inequality to (12) that is a quartic equation of ğ¿:
ğ‘ğ¿4+ğ‘ğ¿3+ğ‘ğ¿2+ğ‘‘ğ¿+ğ‘’â‰¥0, (B.3)
whereğ‘=(Î£12+Î£22)2â‰¥0,ğ‘=2âˆšğœ†22ğœŒğœ†(Î£22âˆ’Î£11)(Î£12+Î£22),
ğ‘=ğœ†22(ğœŒ2
ğœ†(Î£11âˆ’Î£22)2âˆ’2(Î£11+Î£12)(Î£12+Î£22)),ğ‘‘=2ğœ†3/2
22ğœŒğœ†(Î£11âˆ’
Î£22)(Î£11+Î£12),andğ‘’=ğœ†2
22(Î£11+Î£12)2.If(B.3)holdstrueforall ğ¿â‰¥0,
then the bivariate variance of ğ›¿cannot exceed the univariate variance
(i.e., the inequality in (12) also holds true). It can be shown via algebra
that the coefficients in (B.3) satisfy ğ·=64ğ‘3ğ‘’âˆ’16ğ‘2ğ‘2+16ğ‘ğ‘2ğ‘âˆ’
16ğ‘2ğ‘ğ‘‘âˆ’3ğ‘4=0. This result implies that (B.3) has two double roots.
Because the leading coefficient ğ‘â‰¥0, the quartic equation in (B.3) is
non-negative for all ğ¿â‰¥0. Theorem 1 follows directly from this result.
â–¡
C Additional Simulation Plot
Figure 9: Density curves of the variance reduction factor for several conditions
when signal-to-noise ratios do not differ substantially. Median reduction
factors are given by the dashed vertical lines and annotated text.
4895