Dataset Regeneration for Sequential Recommendation
Mingjia Yin
mingjia-yin@mail.ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, ChinaHao Wangâˆ—
wanghao3@ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, ChinaWei Guo
Yong Liu
guowei67@huawei.com
liu.yong6@huawei.com
Huawei Singapore Research Center
Singapore
Suojuan Zhang
Sirui Zhao
sjzsj@ustc.edu.cn
sirui@mail.ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, ChinaDefu Lian
liandefu@ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, ChinaEnhong Chen
cheneh@ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, China
Abstract
The sequential recommender (SR) system is a crucial component
of modern recommender systems, as it aims to capture the evolv-
ing preferences of users. Significant efforts have been made to
enhance the capabilities of SR systems. These methods typically
follow the model-centric paradigm, which involves developing
effective models based on fixed datasets. However, this approach of-
ten overlooks potential quality issues and flaws inherent in the data.
Driven by the potential of data-centric AI, we propose a novel
data-centric paradigm for developing an ideal training dataset using
a model-agnostic dataset regeneration framework called DR4SR.
This framework enables the regeneration of a dataset with excep-
tional cross-architecture generalizability. Additionally, we intro-
duce the DR4SR+ framework, which incorporates a model-aware
dataset personalizer to tailor the regenerated dataset specifically
for a target model. To demonstrate the effectiveness of the data-
centric paradigm, we integrate our framework with various model-
centric methods and observe significant performance improvements
across four widely adopted datasets. Furthermore, we conduct in-
depth analyses to explore the potential of the data-centric par-
adigm and provide valuable insights. The code can be found at
https://github.com/USTC-StarTeam/DR4SR.
CCS Concepts
â€¢Information systems â†’Recommender systems.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671841Keywords
Recommendation System; Sequential Recommendation; Data-Centric
AI; Data Generation
ACM Reference Format:
Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu
Lian, and Enhong Chen. 2024. Dataset Regeneration for Sequential Recom-
mendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671841
1 INTRODUCTION
The sequential recommender (SR) system is a critical component of
modern recommender systems, aiming to capture the evolving pref-
erences of users through their sequential interaction records [ 48,54].
In recent years, significant efforts have been made to enhance rec-
ommender systems. This includes the development of sophisticated
deep models [ 10,14,18,25,40,44,45,55], effective training strate-
gies [ 28,34], and the refinement of representation space through
self-supervised learning [ 4,36,57,64,65,76], among others. These
methods follow the model-centric paradigm, which specifically
aims to develop more effective models given fixed datasets.
Despite the remarkable achievements, these methods often over-
look potential quality issues inherent within the data [ 68], which
may lead to overfitting or amplification of data errors [ 23,69]. To
address these challenges, the data-centric paradigm [ 23,68,74]
has been proposed, focusing on developing high-quality data with
fixed models. For example, graph structure learning [ 17,30,61,78]
has been introduced to uncover valuable graph structures from data
[74]. Additionally, generative models such as GAN [ 6], VAE [ 21],
and Diffusion model [ 11] have been employed to synthesize new
training samples [ 12,15]. Inspired by the potential of data-centric
AI, we aim to acquire an informative and generalizable training
dataset for sequential recommender systems. We define this as the
training data development problem. To the best of our knowl-
edge, we are the first to investigate this problem in the context of
sequential recommendation systems.
3954
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Mingjia Yin et al.
Data-centricparadigm: personalized datasets for eachmodelModel-centricparadigm: one dataset for all modelsRegenerateOriginal Sequenceğƒğšğ­ğšğ‘¹ğ‘µğ‘µ	ğƒğšğ­ğšğ‘¨ğ’•ğ’•ğ’	ğƒğšğ­ğšğ‘®ğ’“ğ’‚ğ’‘ğ’‰	RNNAttentionGraphAll Models
Personalize123524134561246
2345â€¦â€¦1231212324345â€¦FeedIndiscriminate Data Feeding
RNNAttentionGraph
Figure 1: Model-centric paradigm v.s. Data-centric pradigm.
To address the issue, dataset generation is the most relevant topic.
However, to the best of our knowledge, it has been rarely studied
in the field of recommender systems. UPC-SDG[ 29] proposed to
generate a synthetic dataset, but its primary goal is to preserve
privacy rather than recommendation performance. Another poten-
tial solution is dataset distillation (DD), aiming to derive a smaller
synthetic dataset and enable trained models to attain comparable
performance[ 37,43,52,53,66]. However, they prioritize training
efficiency over effectiveness, which differs from our motivation.
Notably, recent advancements in DD [ 7,75] have reported that even
increasing synthetic dataset size, the performance will ultimately
plateau around the optimal performance achieved with the original
dataset. Yet another viable approach is denoising sequential recom-
mendation, which involves the removal of noisy information from
the original data [ 35,41,67,70,77]. However, denoising is just one
aspect of the larger issue concerning training data development.
Our objective is to excavate innovative data content and formats
within the developed data, further enhancing model training.
To acquire the optimal training data, our key idea is to learn a
new dataset that explicitly contains item transition patterns. In de-
tail, we decompose the modeling process of a recommender into two
stages: extracting transition patterns Xâ€²from the original dataset
Xand learning user preferences Ybased onXâ€². The learning of the
mappingXâ†’Y is challenging since it involves two implicit map-
pings:Xâ†’Xâ€²andXâ€²â†’Y . Therefore, we explore the possibility
of developing a dataset that explicitly represents item transition
patterns ofXâ€². This allows us to explicitly decompose the learning
process into two phases, with Xâ€²â†’Y being intuitively easier to
learn. Hence, our main focus is on learning an effective mapping
function forXâ†’Xâ€², which is a one-to-many mapping. We define
the learning process as a dataset regeneration paradigm as de-
picted in Figure 1, where "re-" indicates that we do not incorporate
any additional information but solely rely on the original dataset.
To realize dataset regeneration, we propose a novel data-centric
paradigm Dataset Regeneration for Sequential Recommendation
(DR4SR), to regenerate the original dataset into a highly infor-
mative and generalizable dataset. In detail, we first construct a
pre-training task, which makes it possible to perform dataset re-
generation. Subsequently, a diversity-promoted regenerator is pro-
posed to model the one-to-many relationship between sequences
and patterns in the regeneration process. Lastly, we propose a hy-
brid inference strategy to regenerate a new dataset with balanced
exploration and exploitation. To confirm the superiority of DR4SR,
we integrated our framework with various model-centric methods
and conducted experiments on four widely adopted datasets. Theexperimental results can demonstrate the ideal cross-architecture
generalizability of DR4SR and the high complementarity of data-
centric and model-centric paradigms.
Upon regenerating the dataset, we encounter a novel challenge:
the dataset regeneration process operates independently of the
target model. While it demonstrates promising cross-architecture
generalizability, the regenerated dataset may be sub-optimal for
a particular target model. Consequently, our objective is to fur-
ther customize the regenerated dataset to a particular target model.
Nevertheless, the non-differentiable nature of the hybrid inference
process poses a difficulty, as optimizing the dataset regenerator
based on the downstream recommendation performance via gradi-
ent backpropagation becomes unfeasible. To mitigate the aforemen-
tioned challenges, we augment DR4SR to a model-aware dataset
regeneration process, denoted as DR4SR+. DR4SR+ takes into ac-
count the unique attributes of each target model and modifies the
regenerated dataset accordingly, as depicted in Figure 1. In par-
ticular, we have implemented a dataset personalizer that assigns
a score to every pattern within the regenerated dataset. To pre-
vent model collapse, we formulate the optimization of the dataset
personalizer as a bi-level optimization problem, which can be effi-
ciently addressed using implicit differentiation. Empirical results
substantiated that DR4SR+ can enhance the regenerated dataset
further. Additionally, our investigations suggest that the data forms
amenable to regeneration are not confined to sequences. Our con-
tributions are summarized as follows:
â€¢To the best of our knowledge, this study is the first to address
the issue of training data development for sequential recommen-
dations from a data-centric perspective, intending to create an
informative and generalizable training dataset.
â€¢We propose a model-agnostic dataset regeneration framework
DR4SR to tackle the problem. The framework comprises a pre-
training task, a diversity-promoted regenerator, and a hybrid
inference strategy, all of which contribute to the regeneration of
an informative and generalizable dataset.
â€¢We further develop DR4SR into a model-aware version, denoted
as DR4SR+, to tailor the regenerated dataset to various target
models. This is accomplished through the use of a dataset per-
sonalizer, which assigns scores to each regenerated pattern and
can be optimized efficiently using implicit differentiation.
â€¢We integrated DR4SR and DR4SR+ with various model-centric
methods, resulting in substantial improvements on four widely
adopted datasets. Additionally, we conducted comprehensive
analyses to explore the potential of the data-centric paradigm.
2 RELATED WORKS
2.1 Sequential Recommendation
To reveal the dynamic preferences embedded within usersâ€™ inter-
action sequences, sequential recommendation (SR) has become a
prominent branch within the realm of recommendation systems[ 48].
Significant efforts have been directed towards enhancing the ca-
pabilities of sequential recommender systems. Initial endeavors
concentrated on developing intricate sequential deep models to
capture complex sequential preferences, encompassing CNN[ 40,
59], RNN[ 10,24], GNN[ 51,55,58], and Transformer[ 42]-based
approaches[ 18,39]. Beyond model design, ongoing efforts aim to
3955Dataset Regeneration for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
augment the efficacy of SR by devising superior training strate-
gies, such as RSS[ 34] and CT4Rec[ 28]. Additionally, self-supervised
learning (SSL) has been incorporated into SR to refine the repre-
sentation space with diverse SSL tasks, including sequence-level
tasks[4, 57], model-level tasks[36], and hybrid tasks[76].
Recognizing the presence of noise in sequential data, denoising
sequential recommendation has emerged as an effective method
to filter out noisy information from the original data (e.g., [ 8,27,
35,41,67,70,77]). One approach achieves this implicitly in the
representation space through elaborate model architectures, such as
the sparse attention network in DSAN[ 67], and the filter-enhanced
MLP in FMLP[ 77]. Another approach explicitly filters out irrelevant
items for a target item, including CLEA[35] and RAP[41].
Denoising-based methods partially align with our objective, but
denoising alone falls short of producing an optimal dataset as it
only focuses on removing information from the data. Dataset gen-
eration should produce diverse and novel samples to alleviate the
constraints imposed by the original dataset [2].
2.2 Data-Centric AI in Recommender System
Recently, the importance of data in the field of AI has been signifi-
cantly magnified, culminating in the emergence of the Data-Centric
AI (DCAI) paradigm. DCAI represents an all-encompassing concept
that comprises three principal components: training data develop-
ment, inference data development, and data maintenance [ 23,68].
Within these significant themes, our primary emphasis is on the
development of training data, specifically, the creation of an optimal
training dataset for sequential recommendations.
In pursuit of the objective of training data development, dataset
generation is the most relevant topic. In the field of recommender
systems, UPC-SDG[ 29] proposed to develop a privacy-preserving
synthetic dataset. GTN [ 5] generated adaptive graph data for graph
collaborative filtering. For sequential recommendation, ASReP [ 31]
and METL [ 19] focused on generating fabricated data for long-
tailed sequences. Another promising technique in DCAI is Dataset
Distillation (DD). DD aims to distill a condensed dataset from
the original dataset, enabling trained models to attain compara-
ble performance[ 66]. DD is predominantly popular in the field of
computer vision (CV) and can be categorized into three main classes:
performance matching[ 49], parameter matching[ 72], and distribu-
tion matching[ 71]. Specifically, DD has been introduced to the field
of recommender systems by some pioneer works[ 37,38,43,52,53].
âˆ-AE[ 37] adopted neural tangent kernel (NTK) to approximate
an infinitely wide autoencoder and synthesized fake users with
sampling-based reconstruction. DConRec[ 52] proposed to distill a
synthesized dataset by sampling user-item pairs from a learnable
probabilistic matrix. Farzi [ 38] achieved efficient sequential data
distillation within the latent space. Different from the previous two
performance matching methods, CGM[ 43] followed a parameter
matching[ 73] paradigm to condense categorical recommendation
data in the CTR scenario.
While these methods have achieved remarkable success, they
diverge from our specific objectives. Our goal is to utilize the gen-
erated dataset to enhance performance, whereas these methods
primarily prioritize privacy or efficiency concerns.3 PROBLEM DEFINITION
In the domain of sequential recommendation, the primary objective
is to model user preferences based on their interaction records and
provide the next recommended item for them. We can formally
define the problem as follows:
Definition1. (Sequential Recommendation). In a sequential
recommendation system, we denote Uas the user set andVas the
item set, and|U|and|V|is the respective number of users and items.
The interaction sequences of users are ordered chronologically. We
define them as ğ‘ ğ‘¢=[ğ‘£1,ğ‘£2,...,ğ‘£ğ‘¡,...,ğ‘£|ğ‘ ğ‘¢|], whereğ‘¢âˆˆU is some
user,ğ‘£ğ‘¡âˆˆV is one item interacted by the user at the time step ğ‘¡,
ğ‘ ğ‘¢represents the sequence, and |ğ‘ ğ‘¢|denotes its length, which has a
maximum value of ğ‘. Then the next item prediction task aims to
predict the item at the next time step ğ‘£|ğ‘ ğ‘¢|+1for each user ğ‘¢âˆˆU.
In deviating from conventional model-centric approaches, which
concentrate on enhancing recommendation performance with fixed
datasets, we endeavor to construct an optimal training dataset from
a data-centric perspective, which is formalized as a training data
development problem as follows:
Definition2. (Training Data Development). Given an origi-
nal datasetX, traditional methods aim to learn a target model ğ‘“â€²
satisfyingY=ğ‘“â€²(X), whereYis the user preferences. Training
data development aims to learn a new informative and generalizable
datasetXâ€²satisfyingY=ğ‘“(Xâ€²), where the target model ğ‘“can be
easier to learn than ğ‘“â€²based onXâ€².
Then the next crucial issue is how to guarantee an optimal Xâ€².
We aim to accomplish this through a model-agnostic dataset regen-
eration paradigm, which is explained in detail below:
Definition3. (Dataset Regeneration). Given a sequence ğ‘ ğ‘¢inX,
dataset regeneration aims to learn a one-to-many dataset regenerator
ğ‘“â€²satisfying{ğ‘ğ‘¢1,ğ‘ğ‘¢2,...,ğ‘ğ‘¢ğ¾}=ğ‘“â€²(ğ‘ ğ‘¢), where{ğ‘ğ‘¢1,ğ‘ğ‘¢2,...,ğ‘ğ‘¢ğ¾}
are K informative patterns and ğ‘“â€²is learned independently from the
target model ğ‘“. By collecting all regenerated patterns, we can obtain
the new datasetXâ€².
Upon regenerating a new dataset, there is an imperative need to
tailor the regenerated dataset specifically for a target recommen-
dation model, as the regeneration process remains independent of
the target model. We hence propose a model-aware dataset regen-
eration process that can evaluate the score of each data sample for
the target model. The formal definition is as follows:
Definition4. (Model-aware Dataset Regeneration). Given the
regenerated dataset Xâ€², a dataset personalizer ğ‘”with parameter ğœ™is
used to generate a scoring matrix WâˆˆR|Xâ€²|to evaluate the score of
each data sample. The learning of ğ‘”will be guided by a target model
ğ‘“, thereby achieving dataset personalization. The dataset-score pair
(Xâ€²,W)will serve as the personalized dataset.
In the next section, we will illustrate the details of dataset regen-
eration and model-aware dataset regeneration.
4 METHODOLOGY
4.1 Overall Framework
In this paper, we propose a data-centric framework named Data
Regeneration for Sequential Recommendation (DR4SR), aiming to
regenerate the original dataset into a highly informative and gener-
alizable dataset, as shown in Figure 2. Since the data regeneration
process is independent of target models, the regenerated dataset
3956KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Mingjia Yin et al.
1234512EncoderDiversityPromoterDecoder12
Decoderğ’ğŸ" ğ’ğŸ" â€¦ğ’ğ‘²"ğ…ğŸğ…ğŸ â€¦ğ…ğ‘² 
ğ’ğŸ"ğ’ğ‘²"Personalizer1?26ğ›¾1âˆ’ğ›¾RestrictiveGenerative
34?57ğ›¾1âˆ’ğ›¾RestrictiveGenerativeâ€¦â€¦
12
127
â€¦â€¦â€¦â€¦1212324127â€¦TargetModel(A)
(B)(C)0.70.2 0.8 	â‹¯0.5 Ã—Ã—Ã—Ã—
âˆ‘
ğ‘ !in ğ’³ğ‘"in ğ’³â€²Items not in ğ‘ !ForwardImplicit gradient712121212324127â€¦1212324127â€¦12<BOS><EOS>
Figure 2: The framework of the proposed data-centric paradigm: (A) The Pre-training stage of DR4SR involves training a
diversity-promoted data regenerator utilizing the curated pre-training dataset. (B) The inference stage of DR4SR regenerates
each source sequence into multiple target patterns with a hybrid inference strategy. (C) Model-aware dataset regeneration with
a personalizer further tailors the regenerated dataset to each target model.
121323123121312312345ğ’³!"#1234512345123451231312345â‹¯â‹¯
Figure 3: Pre-training task construction. Assuming two given
sequences (1,2,3,4,5), (1,2,3) with a window size 3 and a thresh-
old 2, the following patterns can be extracted: (1,2), (1,3), (2,3),
(1,2,3). This is because these patterns appear twice within the
sliding window. Then the regenerator is supposed to regen-
erate each sequence into multiple corresponding patterns.
may not necessarily align with their requirements. Therefore, we
extend DR4SR to a model-aware version DR4SR+ in Section 4.3,
which tailors the regenerated dataset to a particular target model.
4.2 Model-agnostic Dataset Regeneration
To develop an informative and generalizable dataset, we aim to
construct a dataset regenerator to facilitate the automated dataset
regeneration. However, the original dataset lacks supervised in-
formation for learning a dataset regenerator. Therefore, we must
implement it in a self-supervised learning manner. To achieve this,
we introduce a pre-training task in Section 4.2.1 that can guide the
learning of the proposed diversity-promoted regenerator detailed
in Section 4.2.2. After completing the pre-training, we proceed to re-
generate a new dataset using a hybrid inference strategy in Section
4.2.3. The pipeline is detailed in Algorithm 1.
4.2.1 Construction of Pre-training Task. To construct a pre-training
task, we first obtain item transition patterns through rule-based
methods. Then, the pre-training task is formalized as training the
regenerator to learn how to regenerate these patterns from the
sequences in the original dataset.Specifically, we utilize a sliding window strategy to extract pat-
terns. Within a specified sliding window of size ğ›¼, we count the
occurrence of item transition patterns. Patterns with an occurrence
count exceeding a predetermined threshold ğ›½will be selected to
form the pre-training task. A toy example is depicted in Figure 3,
where the window size is set to 3 and the threshold is set to 2.
After obtaining the pattern set, one pre-training instance pair
can be constructed as (ğ‘ ğ‘¢,ğ‘ğ‘–), whereğ‘ ğ‘¢is the interaction sequence
of userğ‘¢,ğ‘ğ‘–isğ‘–-th pattern extracted from ğ‘ ğ‘¢. Then the regenerator
ğ‘“â€²is supposed to regenerate ğ‘ ğ‘¢into the corresponding pattern ğ‘ğ‘–
for each pre-training instance pair (ğ‘ ğ‘¢,ğ‘ğ‘–). We denote the entire
pre-training dataset as Xpre.
Note: This strategy is related to sequential pattern mining, which
aims to uncover patterns for guiding sequential recommendations
[47,63]. However, these methods are known to generate redundant
information and neglect infrequent patterns, thus failing to meet
the requirements of a dataset regenerator.
4.2.2 Diversity-promoted Regenerator. With the aid of the pre-
training task, we can now pre-train a dataset regenerator. In this
paper, we employ the Transformer[ 42] model as the main architec-
ture of our regenerator, whose generation capabilities have been
extensively validated. The dataset regenerator consists of three
modules: an encoder to obtain representations of sequences in the
original dataset, a decoder to regenerate the patterns, and a diversity
promoter to capture the one-to-many mapping relationship. Next,
we will proceed to introduce each of these modules individually.
The encoder consists of several stacked layers of multi-head
self-attention (MHSA) and feed-forward (FFN) layers. Considering
a sequence inX, we can get the sequence representation by:
H(ğ‘™)=ğ¹ğ¹ğ‘(ğ‘€ğ»ğ‘†ğ´(H(ğ‘™âˆ’1))), (1)
where H(ğ‘™âˆ’1)âˆˆRğ‘Ã—ğ‘‘is the output at the (ğ‘™âˆ’1)-th layer, and H(0)
is item embeddings with added learnable positional encoding. As
3957Dataset Regeneration for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
for the decoder, it takes patterns in the regenerated dataset Xâ€²as
input. The objective of the decoder is to reconstruct the pattern
given the sequence representations from the encoder:
ğ¿recon=âˆ’|Xpre|âˆ‘ï¸
(ğ‘ ğ‘¢,ğ‘ğ‘–)ğ‘‡âˆ‘ï¸
ğ‘¡=1logğ‘ƒ(ğ‘ğ‘–ğ‘¡|h(ğ‘™)
ğ‘¢,Ë†ğ‘<ğ‘¡), (2)
where(ğ‘ ğ‘¢,ğ‘ğ‘–)is a sequence-pattern pair in Xpre,ğ‘¡is the position in
the target pattern, h(ğ‘™)
ğ‘¢is the representation of the original sequence
ğ‘ ğ‘¢,Ë†ğ‘<ğ‘¡is the pattern generated before ğ‘¡, andğ‘ƒis the prediction
probability for the target item ğ‘ğ‘–ğ‘¡.
However, as mentioned in Section 4.2.1, multiple patterns can
be extracted from a sequence, which presents challenges during
the training process. For instance, a sequence like (1, 2, 3, 4, 5) may
yield two distinct patterns, namely (1, 2) and (4, 5). Consequently,
this can introduce conflicts during training, thereby impeding the
convergence of the dataset regenerator. To address this one-to-many
mapping issue, we further propose a diversity promoter.
Specifically, we employ a more aggressive approach to adaptively
regulate the impact of the original sequence during the decoding
stage by incorporating information about the target pattern. First,
we project the memory mâˆˆRğ·(representations of the original
sequence) generated by the encoder into K distinct vector spaces,
i.e.,{mâ€²
1,mâ€²
2,..., mâ€²
ğ¾}andmâ€²
ğ‘˜âˆˆRğ·. This projection enables the
acquisition of memories with diverse semantic information. Ideally,
different target patterns should match distinct memories. To achieve
this, we additionally introduce a Transformer encoder to encode the
target patterns and obtain h(ğ‘™)
patternâˆˆRğ·. Notably, it is imperative
to exercise caution when incorporating h(ğ‘™)
patterninto the decoder,
otherwise the model may inadvertently collapse into an identity
mapping function by simply duplicating the input. Therefore, we
compress h(ğ‘™)
patterninto a probability vector by:
ğœ‹=Softmax(MLP(h(ğ‘™)
pattern)), (3)
whereğœ‹={ğœ‹1,ğœ‹2,...,ğœ‹ğ‘˜,...,ğœ‹ğ¾}andğœ‹ğ‘˜is the probability of
choosing the ğ‘˜-th memory. To ensure that each memory space re-
ceives sufficient training, we do not perform hard selection. Instead,
we obtain the final memory through a weighted sum:
mfinal=ğ¾âˆ‘ï¸
ğ‘˜=1ğœ‹ğ‘˜mâ€²
ğ‘˜. (4)
Ultimately, we can leverage the acquired memory to facilitate the
decoding process and effectively capture the intricate one-to-many
relationship between sequences and patterns.
4.2.3 Dataset Regeneration with Hybrid Inference Strategy. After
pre-training, we utilize the regenerator for inference to obtain a
regenerated dataset Xâ€². Specifically, we re-feed the sequences in
Xinto the regenerator to obtain new patterns. Due to the unavail-
ability of target patterns, we cannot generate a category vector
ğœ‹for selecting memories. As a result, we respectively input each
memory directly into the decoder, leading to the generation of K
patterns. Besides, we consider the following two decoding modes:
Restrictive mode. Decoding is limited to selecting items from
the input sequence, which focuses on the exploitation of existing
information from the current sequence. Generative mode. Thereare no restrictions, which allows for the exploration of patterns that
may not exist inX. For instance, suppose the original dataset con-
tains item transition patterns like (1, 2) and (2, 6). When decoding
a sequence like (1, 2, 3, 4, 5), the model may identify a high-order
pattern like (1, 2, 6), which was not explicitly present in X.
To achieve a harmonious balance between exploitation and explo-
ration, we propose a hybrid strategy: When decoding an item, there
is a probability of ğ›¾to adopt the generative mode and a probability
of1âˆ’ğ›¾to adopt the restrictive mode.
4.3 Model-aware Dataset Regeneration
Since the previous regeneration process is independent of target
models, the regenerated dataset may be sub-optimal for a particular
target model. Hence, we extend the model-agnostic dataset regener-
ation process to a model-aware one. To achieve this, built upon the
dataset regenerator, we further introduce a dataset personalizer to
evaluate the scores of each data sample in the regenerated dataset
in Section 4.3.1. Then the dataset personalizer can be efficiently
optimized by implicit differentiation introduced in Section 4.3.2.
The pipeline is detailed in Algorithm 2.
4.3.1 Dataset personalizer. As outlined in Section 3, our objective is
to train a dataset personalizer ğ‘”with parameter ğœ™that can evaluate
the score of each data sample Wfor the target model. Specifically,
given a data sample of pattern ğ‘–in the position ğ‘¡, the scoreğ‘¤ğ‘–,ğ‘¡for
this data sample can be computed as follows:
z=ğ‘”ğœ™(hğ‘–
ğ‘¡),
Ë†z=Softmax((z+G)/ğœ),
ğ‘¤ğ‘–,ğ‘¡=Ë†z1,(5)
whereğ‘”ğœ™is the dataset personalizer implemented as an MLP, hğ‘–
ğ‘¡
is the representation of pattern ğ‘–at position ğ‘¡generated by the
target model ğ‘“â€²,zâˆˆR2is the computed logits vector, Gumbel noise
Gis sampled from a Gumbel distribution[ 16],ğœis a temperature
parameter to control the smoothness of Gumbel distribution, and
the subscript 1means selecting the first element of Ë†zas the score.
Since the personalizer takes pattern representations as input, the
personalizer possesses a receptive field over the entire pattern, en-
abling a comprehensive evaluation of the pattern. Additionally, the
Gumbel noise is introduced to encourage the scores to approximate
a discrete selection operation, which is a desirable property[16].
To ensure the generality of the framework, we utilize the calcu-
lated score to adjust the weighting of training losses, which does
not necessitate any additional modifications to target models. We
start by defining the original next-item prediction loss:
Lğ‘Ÿğ‘’ğ‘âˆ’ğ‘œğ‘Ÿğ‘–=|Xâ€²|âˆ‘ï¸
ğ‘–=1|ğ‘ğ‘–|âˆ‘ï¸
ğ‘¡=2Lğ‘›ğ‘’ğ‘¥ğ‘¡âˆ’ğ‘–ğ‘¡ğ‘’ğ‘š(ğ‘–,ğ‘¡), (6)
Lğ‘›ğ‘’ğ‘¥ğ‘¡âˆ’ğ‘–ğ‘¡ğ‘’ğ‘š(ğ‘–,ğ‘¡)=âˆ’log(ğœ(hğ‘–
ğ‘¡âˆ’1Â·vğ‘–
ğ‘¡))âˆ’âˆ‘ï¸
ğ‘£ğ‘—âˆ‰ğ‘ğ‘–log(1âˆ’ğœ(hğ‘–
ğ‘¡âˆ’1Â·vğ‘—)),
(7)
whereğ‘ğ‘–represents one regenerated pattern, hğ‘–
ğ‘¡âˆ’1is the represen-
tation of pattern ğ‘–at positionğ‘¡âˆ’1,vğ‘–
ğ‘¡is the embedding of the target
item inğ‘ğ‘–,ğœis the sigmoid function, and we randomly sample one
negative item ğ‘£ğ‘—at each time step ğ‘¡for each pattern. The purpose
3958KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Mingjia Yin et al.
of Equation 7 is to maximize the probability of recommending the
expected next item ğ‘£ğ‘–
ğ‘¡given the pattern representation hğ‘–
ğ‘¡âˆ’1.
Subsequently, the training loss function on the personalized
dataset can be defined as follows:
Lğ‘Ÿğ‘’ğ‘=|Xâ€²|âˆ‘ï¸
ğ‘–=1|ğ‘ğ‘–|âˆ‘ï¸
ğ‘¡=2wğ‘–,ğ‘¡Lğ‘›ğ‘’ğ‘¥ğ‘¡âˆ’ğ‘–ğ‘¡ğ‘’ğ‘š(ğ‘–,ğ‘¡). (8)
4.3.2 Efficient optimization of dataset personalizer. The optimiza-
tion of the dataset personalizer presents a nontrivial challenge.
Should we attempt to optimize it concurrently with the target model
using Equation 8, we are likely to confront a model collapse issue,
wherein the personalizer assigns zero to each data sample. To con-
front this challenge, we formalize the optimization process as a
bi-level optimization problem.
Specifically, our goal is to train an optimal dataset personalizer to
obtain an optimal personalized dataset, on which the target model
can achieve the best recommendation performance:
ğœ™âˆ—=arg min
ğœ™ğ¿dev(ğœƒâˆ—(ğœ™)),
ğ‘ .ğ‘¡. ğœƒâˆ—(ğœ™)=arg min
ğœƒğ¿train(ğœƒ,ğœ™),(9)
whereğ¿train(ğœƒ,ğœ™)is defined in Equation 8. The lower optimization
aims to get an optimal target model with parameter ğœƒâˆ—while fixing
the dataset personalizer. Besides, ğœƒâˆ—is an implicit function of ğœ™, so
it can be denoted as ğœƒâˆ—(ğœ™).ğ¿dev(ğœƒâˆ—(ğœ™))is defined in Equation 6,
which is the unweighted recommendation loss of the target model
on a validation dataset. In the context of batch training, we can
shuffle the original dataset to create a validation set[3].
Then we can elaborate on how to optimize the bi-level opti-
mization problem. Theoretically, achieving the optimal point of the
target model through training is a prerequisite before conducting
a one-step upper-level optimization. For the sake of efficiency, we
conduct a one-step upper optimization after only ğ‘‡lower rounds of
lower-level optimization in our implementation. The lower opti-
mization can be directly optimized with gradient descent. As for
the upper optimization, we need to compute âˆ‡ğœ™ğ¿dev(ğœƒâˆ—(ğœ™)). Given
thatğœƒâˆ—(ğœ™)is an implicit function of ğœ™, we can derive:
âˆ‡ğœ™ğ¿dev=âˆ’âˆ‡ğœƒğ¿devÂ·ğ¾âˆ‘ï¸
ğ‘›=0(ğ¼âˆ’âˆ‡2
ğœƒğ¿train)ğ‘›Â·âˆ‡ğœ™âˆ‡ğœƒğ¿train. (10)
The detailed derivations can be found in Appendix A.2. Equation
10 can be efficiently calculated with vector-Jacobi product[ 33].
Through this, we can personalize the regenerated dataset with-
out incurring a substantial time overhead.
5 EXPERIMENTAL EVALUATION
5.1 Experimental Settings
5.1.1 Datasets. To validate the effectiveness of our proposed ap-
proach, we follow previous works[ 4,57] to conduct experiments
on four commonly used and publicly available datasets:
â€¢Beauty, Sports, Toys1: The Amazon-review dataset has emerged
as a prominent resource for evaluating recommendation systems.
For our study, we specifically choose three categories, namely
"Beauty," "Sports and Outdoors," and "Toys and Games."
1http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/Table 1: Statistics of the datasets.
Dataset Beauty Sports Toys Yelp
|U| 22,363 35,598 19,412 30,431
|V| 12,101 18,357 11,924 20,033
# Interactions 0.2m 0.3m 0.17m 0.3m
Avg. length 8.9 8.3 8.6 8.3
Sparsity 99.95% 99.95% 99.93% 99.95%
â€¢Yelp2: The Yelp dataset is a widely used business review dataset
that contains information about businesses, users, and reviews.
We follow the preprocessing in [ 4,57] to guarantee a minimum
of 5 interactions associated with each user and item. Statistics of
the preprocessed datasets are summarized in Table 1.
5.1.2 Compared Baselines and Target Models. To verify the superi-
ority of DR4SR, we select two representative data-centric baselines:
â€¢âˆ-AE [ 37]: an AutoEncoder-based framework, synthesizing user
interaction data with sampling-based reconstruction.
â€¢MELT [ 19]: it focused on the long-tail problem through data
generation for long-tail users and items, respectively.
To fully demonstrate the cross-architecture generalizability of DR4SR,
we select target models from various categories:
â€¢RNN-based: GRU4Rec[ 10] is a sequential recommender system
that applies GRU layers to capture dynamic user preferences.
â€¢Transformer-based: SASRec[ 18] is a sequential recommendation
model that leverages self-attention mechanisms[ 42] to make item
recommendations for the next user action.
â€¢Denoising-based: FMLP[ 77] is one of the most representative
denoising-based SR methods, which adopts a filter-enhanced
MLP to remove noises in interaction sequences.
â€¢GNN-based: GCE-GNN[ 51] constructs an item graph to capture
global context information facilitating recommendation.
â€¢Contrastive-learning-based: CL4SRec[ 57] is one of the most pow-
erful contrastive sequential recommenders, which introduces
three sequence-level augmentation strategies.
The proposed framework is a data-centric framework, which is
complementary to those model-centric methods. Therefore, we
integrate DR4SR with all of them to validate the cross-architecture
generalizability of DR4SR:
â€¢Backbone: We train one target model on the original dataset.
â€¢DR4SR: We integrate DR4SR with the target model.
â€¢DR4SR+: We integrate DR4SR+ with the target model.
5.1.3 Evaluation Protocols. To assess the performance of the next-
item recommendations, we adopt the widely-used leave-one-out
strategy[ 4,18,57]. Considering evaluation metrics, we employ two
ranking metrics Recall@{10, 20} and NDCG@{10, 20}. Furthermore,
we follow the suggestion of Krichene and Rendle[22] to adopt the
whole item set as the candidate item set during evaluation.
5.1.4 Implementation Details. To ensure a fair comparison envi-
ronment, we implement DR4SR and all target models based on
an open-source recommender system library RecStudio[ 26]. The
training process is configured with a maximum of 1000 epochs, and
early stopping is employed if the NDCG@20 on the validation data
2https://www.yelp.com/dataset
3959Dataset Regeneration for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Statistics of the regenerated datasets.
Dataset Beauty Sport Toys Yelp
#users 22,363 35598 19412 30431
#items 12101 18357 11924 20033
#interactions 0.32m 0.45m 0.30m 0.52m
#Avg.length 4.0 4.2 3.7 4.1
Sparsity 99.87% 99.93% 99.87% 99.91%
does not improve for 20 consecutive epochs. The training batch size
ğµis fixed to 256, Adam[ 20] with a learning rate of 1e-3 is used as
the optimizer, the embedding size ğ‘‘is set to 64, and the maximum
sequence length ğ‘is set to 50 for all datasets. BCE loss is used for
all baselines. Detailed hyper-parameter settings of each model can
be found in Appendix A.4. Notably, we fixed the hyper-parameters
of each target model when integrating DR4SR with them.
5.2 Overall Performance
We compared the performance of each target model with "DR4SR"
and "DR4SR+" variants to verify the efficacy of the proposed frame-
work. The statistics of regenerated datasets can be found in Table
2 in the Appendix. From the overall performance presented in
Table 3, we can draw the following conclusions: (1) DR4SR can
regenerate an informative and generalizable dataset: Across all
datasets and target models, we can observe a considerable perfor-
mance enhancement when comparing the "DR4SR" variant with
target models. (2) Different target models prefer different datasets:
An additional improvement can be observed when comparing the
"DR4SR" variant with the "DR4SR+" variant. This is because DR4SR
is independent of target models, which makes the regenerated
dataset sub-optimal. (3) Denoising is just one aspect of the broader
training data development problem: We can find that FMLP can
yield significant performance improvement when integrated with
DR4SR and DR4SR+. This is because dataset regeneration can ad-
ditionally capture the one-to-many relationship of sequences and
patterns. Moreover, dataset regeneration has the potential to dis-
cover latent transition patterns with the hybrid inference mode.
(4) Data-centric and model-centric paradigms are complementary:
The proposed data-centric paradigm has consistently demonstrated
benefits across various datasets and target models, emphasizing the
complementarity of the data-centric and model-centric paradigms.
This underscores the potential for advancing recommender systems
from a data-centric perspective.
5.3 Ablation Study
In this section, we aim to evaluate the efficacy of each module
within DR4SR. An ablation study is conducted using SASRec as
the target model. We have designed the following model variants:
(A) "-diversity": This aims to confirm the significance of capturing
the one-to-many relationship by replacing the diversity-promoted
regenerator with a vanilla transformer. (B) "pattern": This seeks
to substantiate the need for a trainable regenerator by employing
patterns extracted based on predefined rules as the regenerated
dataset. (C) "end-to-end": This endeavors to illustrate the impor-
tance of bi-level optimization by optimizing the dataset personalizer
in an end-to-end fashion. Results are presented in Table 4.From the table, we can obtain the following conclusions: (1) Com-
paring (A) with "DR4SR+", we can observe significant performance
loss, which demonstrates the necessity of modeling the one-to-
many mapping relationship between the original sequences and
patterns. (2) Comparing (B) and "SASRec", we can observe that
using only the pattern dataset leads to a decrease in performance
since many valuable infrequent patterns are ignored. Thus high-
lighting the superiority of the proposed powerful regenerator. (3)
Comparing (C) and "DR4SR+", we find that the end-to-end learning
approach yields poor performance, which is unsurprising as the
end-to-end training may readily converge to a trivial solution where
the dataset personalizer assigns zero scores to all data samples.
5.4 Advanced Study
To demonstrate the superiority of the proposed paradigm, we have
conducted in-depth analysis experiments from various aspects. The
obtained experimental results and analysis are as follows:
5.4.1 Analysis of efficiency of dataset personalization. Bi-
level optimization is widely recognized for its inefficiency and high
memory consumption[ 33], which hinders its practical applicability.
Consequently, we conducted an efficiency analysis to validate the
efficiency of the adopted bi-level optimization process. The exper-
imental results are presented in Table 5. It can be observed from
the results that the training of the dataset personalizer using the
implicit differentiation method does not incur significant additional
time and space overhead. This demonstrates that the regenerated
dataset can be efficiently tailored to different target models.
5.4.2 Analysis of data sample scores of different target mod-
els.We randomly selected 25 sequences from the regenerated
dataset and visualized the sample scores learned by different tar-
get models, as depicted in Figure 4. The figure reveals noticeable
variations in dataset preferences among different models. Notably,
GRU4Rec demonstrates a higher degree of score variance compared
to other methods, which exhibit more uniform weight assignments.
This is because the powerful self-attention mechanism or filter-
enhanced MLP can adaptively determine the utilization of items in
the sequence to some extent. However, even among these methods,
discernible preference differences can still be observed. These obser-
vations further validate the crucial role of the dataset personalizer.
5.4.3 Analysis of data forms that should be regenerated.
After regenerating a dataset, we can construct a new graph for GNN
and perform data augmentation based on the new dataset. This
naturally raises the question of which dataset should be adopted
for graph construction or data augmentation: the original or the
regenerated one. To shed light on this matter, we further conducted
two experiments. Firstly, we compared the effects of using different
graphs, and the results are shown in Figure 5a. The results are
subtle, with the original graph performing better on half of the
dataset, while the regenerated graph yields superior results on
the other half. Subsequently, we compared the effects of using
different augmentation data, and the results are shown in Figure
5b. From the table, we can observe that DR4SR yields superior data
augmentation samples for CL4SRec on the first two datasets. This is
attributed to the regenerated sequences containing richer semantic
3960KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Mingjia Yin et al.
Table 3: The overall performance. Considering a target model, the best result is bolded while the second-best result is underlined.
Superscript * means improvements are statistically significant with p<0.05 while ** meaning p<0.01.
Dataset Beauty Sp
orts T
oys Y
elp
Metric R@10
R@20 N@10 N@20 R@10
R@20 N@10 N@20 R@10
R@20 N@10 N@20 R@10
R@20 N@10 N@20
âˆ-
AE 0.0478
0.0661 0.0262 0.0308 0.0256
0.0373 0.0144 0.0173 0.0450
0.0593 0.0268 0.0304 0.0252
0.0424 0.0121 0.0162
MELT 0.0577
0.0879 0.0303 0.0379 0.0311
0.0488 0.0163 0.0208 0.0709
0.0987 0.0401 0.0473 0.0293
0.0497 0.0143 0.0195
GRU4Re
c 0.0204
0.0382 0.0107 0.0150 0.0160
0.0279 0.0085 0.0115 0.0212
0.0357 0.0099 0.0136 0.0215
0.0364 0.0105 0.0143
DR4SR 0.0252âˆ—âˆ—0.0448âˆ—âˆ—0.0128âˆ—âˆ—0.0177âˆ—âˆ—0.0208âˆ—âˆ—0.0341âˆ—âˆ—0.0102âˆ—âˆ—0.0135âˆ—âˆ—0.0252âˆ—âˆ—0.0418âˆ—âˆ—0.0124âˆ—âˆ—0.0165âˆ—âˆ—0.0235âˆ—âˆ—0.0403âˆ—âˆ—0.0114âˆ—âˆ—0.0156âˆ—âˆ—
Impr
ov 23.5%
17.3% 19.6% 18.0% 30.0%
22.2% 20.0% 17.4% 18.9%
22.4% 25.3% 21.3% 9.30%
10.7% 8.57% 9.09%
DR4SR+ 0.0292âˆ—âˆ—0.0473âˆ—âˆ—0.0149âˆ—âˆ—0.0194âˆ—âˆ—0.0223âˆ—âˆ—0.0360âˆ—âˆ—0.0116âˆ—âˆ—0.0151âˆ—âˆ—0.0274âˆ—âˆ—0.0456âˆ—âˆ—0.0134âˆ—âˆ—0.0179âˆ—âˆ—0.0243âˆ—âˆ—0.0415âˆ—âˆ—0.0120âˆ—âˆ—0.0164âˆ—âˆ—
Impr
ov 43.1%
23.8% 39.3% 29.3% 39.4%
29.0% 36.5% 31.3% 29.2%
27.7% 35.4% 31.6% 13.0%
14.0% 14.3% 14.7%
SASRe
c 0.0553
0.0847 0.0291 0.0368 0.0297
0.0449 0.0156 0.0194 0.0682
0.0951 0.0381 0.0448 0.0289
0.0488 0.0143 0.0193
DR4SR 0.0595âˆ—âˆ—0.0906âˆ—âˆ—0.0317âˆ—âˆ—0.0395âˆ—âˆ—0.0330âˆ—âˆ—0.0512âˆ—âˆ—0.0174âˆ—âˆ—0.0220âˆ—âˆ—0.0762âˆ—âˆ—0.1049âˆ—âˆ—0.0432âˆ—âˆ—0.0504âˆ—âˆ—0.0304âˆ—0.0512âˆ—0.0151âˆ—0.0202âˆ—
Impr
ov 7.59%
6.97% 8.93% 7.34% 11.1%
14.0% 11.5% 13.4% 11.7%
10.3% 13.4% 12.5% 5.19%
4.92% 5.59% 4.66%
DR4SR+ 0.0619âˆ—âˆ—0.0919âˆ—âˆ—0.0337âˆ—âˆ—0.0412âˆ—âˆ—0.0349âˆ—âˆ—0.0525âˆ—âˆ—0.0191âˆ—âˆ—0.0235âˆ—âˆ—0.0773âˆ—âˆ—0.1068âˆ—âˆ—0.0453âˆ—âˆ—0.0527âˆ—âˆ—0.0317âˆ—âˆ—0.0523âˆ—âˆ—0.0159âˆ—âˆ—0.0211âˆ—âˆ—
Impr
ov 11.9%
8.50% 15.8% 12.0% 17.5%
16.9% 22.4% 21.1% 13.3%
12.3% 18.9% 17.6% 9.69%
7.17% 11.2% 9.33%
FMLP 0.0602
0.0934 0.0311 0.0394 0.0323
0.0524 0.0166 0.0217 0.0676
0.0982 0.0377 0.0447 0.0297
0.0495 0.0143 0.0197
DR4SR 0.0635âˆ—âˆ—0.0993âˆ—âˆ—0.0332âˆ—âˆ—0.0421 0.0345 0.0559 0.0177âˆ—âˆ—0.0230âˆ—âˆ—0.0717âˆ—âˆ—0.1061âˆ—âˆ—0.0400âˆ—âˆ—0.0486âˆ—âˆ—0.0316âˆ—âˆ—0.0524âˆ—âˆ—0.0158âˆ—âˆ—0.0210âˆ—âˆ—
Impr
ov 5.48%
6.32% 6.75% 6.85% 6.81%
6.68 6.63% 5.99% 6.07%
8.04% 6.10% 8.72% 6.40%
5.86% 10.5% 6.60%
DR4SR+ 0.0687âˆ—âˆ—0.1056âˆ—âˆ—0.0357âˆ—âˆ—0.0449âˆ—âˆ—0.0384âˆ—âˆ—0.0597âˆ—âˆ—0.0198âˆ—âˆ—0.0253âˆ—âˆ—0.0788âˆ—âˆ—0.1136âˆ—âˆ—0.0437âˆ—âˆ—0.0524âˆ—âˆ—0.0353âˆ—âˆ—0.0582âˆ—âˆ—0.0171âˆ—âˆ—0.0231âˆ—âˆ—
Impr
ov 14.1%
13.1% 14.8% 14.0% 18.9%
13.9% 19.3% 16.6% 16.6%
15.7% 15.9% 17.2% 18.9%
17.6% 19.6% 17.3%
GNN 0.0570
0.0859 0.0311 0.0384 0.0311
0.0476 0.0167 0.0211 0.0697
0.0958 0.0403 0.0469 0.0242
0.0430 0.0118 0.0166
DR4SR 0.0611âˆ—âˆ—0.0926âˆ—âˆ—0.0324âˆ—0.0406âˆ—0.0336âˆ—âˆ—0.0525âˆ—âˆ—0.0182âˆ—âˆ—0.0230âˆ—âˆ—0.0736âˆ—âˆ—0.1031âˆ—âˆ—0.0424âˆ—âˆ—0.0498âˆ—âˆ—0.0268âˆ—âˆ—0.0451âˆ—0.0129âˆ—âˆ—0.0175âˆ—
Impr
ov 7.19%
7.80% 4.18% 5.73% 8.04%
10.3% 8.98% 9.00% 5.60%
7.62% 5.21% 6.18% 10.7%
4.88% 9.32% 5.42%
DR4SR+ 0.0637âˆ—âˆ—0.0953âˆ—âˆ—0.0334âˆ—âˆ—0.0414âˆ—âˆ—0.0351âˆ—âˆ—0.0545âˆ—âˆ—0.0189âˆ—âˆ—0.0238âˆ—âˆ—0.0771âˆ—âˆ—0.1082âˆ—âˆ—0.0442âˆ—âˆ—0.0521âˆ—âˆ—0.0272âˆ—âˆ—0.0471âˆ—âˆ—0.0134âˆ—âˆ—0.0184âˆ—âˆ—
Impr
ov 11.8%
10.9% 7.40% 7.81% 12.9%
14.5% 13.2% 12.8% 10.6%
12.9% 9.68% 11.1% 12.4%
9.53% 13.6% 10.8%
CL4SRe
c 0.0653
0.0947 0.0370 0.0441 0.0381
0.0559 0.0215 0.0259 0.0781
0.1075 0.0456 0.0530 0.0322
0.0535 0.0159 0.0212
DR4SR 0.0732âˆ—âˆ—0.1016âˆ—âˆ—0.0423âˆ—âˆ—0.0495âˆ—âˆ—0.0401âˆ—âˆ—0.0600âˆ—âˆ—0.0227âˆ—âˆ—0.0274âˆ—âˆ—0.0821âˆ—âˆ—0.1113âˆ—0.0481âˆ—âˆ—0.0551âˆ—0.0344âˆ—âˆ—0.0561âˆ—âˆ—0.0174âˆ—âˆ—0.0229âˆ—âˆ—
Impr
ov 12.1%
7.29% 14.3% 12.2% 5.25%
7.33% 5.58% 5.79% 5.12%
3.53% 5.48% 3.96% 6.83%
4.86% 9.43% 8.02%
DR4SR+ 00756âˆ—âˆ—0.1062âˆ—âˆ—0.0440âˆ—âˆ—0.0517âˆ—âˆ—0.0448âˆ—âˆ—0.0655âˆ—âˆ—0.0247âˆ—âˆ—0.0299âˆ—âˆ—0.0829âˆ—âˆ—0.1140âˆ—âˆ—0.0489âˆ—âˆ—0.0567âˆ—âˆ—0.0363âˆ—âˆ—0.0598âˆ—âˆ—0.0183âˆ—âˆ—0.0241âˆ—âˆ—
Impr
ov 15.8%
1.12% 18.9% 17.2% 17.6%
17.2% 14.8% 15.4% 6.15%
6.05% 7.24% 6.98% 12.7%
11.8% 15.1% 13.7%
0 10 200
5
10
15
20GRU4Rec
0 10 20SASRec
0 10 20GNN
0 10 20CL4SRec
0 10 20FMLP
0.00.20.40.60.8
Position of items in the sequenceSequence ID
Figure 4: Scores assigned by the dataset personalizer for different target models on Toys. From left to right, the variances of the
scores for each target model are 0.0280, 0.0244, 0.0241, 0.0248, and 0.0247.
Table 4: Abalation study of DR4SR on NDCG@20.
Dataset Beauty
Sport Toys Yelp
SASRe
c 0.0368
0.0194 0.0448 0.0193
DR4SR+ 0.0412
0.0235 0.0527 0.0211
(
A) -diversity 0.0365
0.0211 0.0470 0.0196
(B) pattern 0.0181
0.0184 0.0407 0.0141
(C) end-to-end 0.0026
0.0029 0.0067 0.0035
T
able 5: Time and Space Efficiency Analysis.
Dataset Metric Beauty
Sport Toys Yelp
BASERuntime(
s/epoch) 7.618
15.345 13.370 17.738
GP
U memory (MB) 1930
2194 1968 2254
w/
bi-level optimizationRuntime(
s/epoch) 9.476
18.952 14.213 22.41
GP
U memory (MB) 2342
2626 2382 2688
information. However, for the last two datasets, we notice that there
is no significant improvement.
Considering these results, we can observe that while DR4SR
can provide curated sequence training datasets for diverse target
models, certain advanced data formats such as graphs and data used
Beauty Sport T oys Yelp0.800.850.900.951.001.051.10Relative NDCG@20 improvementOriginal
Regenerated(
a) Different graphs
Beauty Sport T oys Yelp0.80.91.01.11.21.3Relative NDCG@20 improvementOriginal
Regenerated (
b) Different augmentations
Figure 5: Relative NDCG@20 improvement of graphs and
data augmentations on different datasets.
for augmentation still need to be constructed based on the original
dataset in certain scenarios. Therefore, we conclude that for the
sequential recommendation, the scope of data regeneration can be
extended beyond the interaction sequences. For instance, graph
structure learning[ 78] can be employed to construct a more suitable
correlation graph, and leverage learnable data augmentation[ 13,46,
3961Dataset Regeneration for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
1 3 5 7 90.08200.08450.08700.08950.0920Recall@20
Recall@20
NDCG@20
0.0320.0340.0360.0380.040
NDCG@20
(a) Beauty
1 3 5 7 90.09400.09750.10100.10450.1080Recall@20
Recall@20
NDCG@20
0.0450.0470.0490.0510.053
NDCG@20
 (b) Toys
Figure 6: Recommendation Performance w.r.t different K.
0 0.1 0.3 0.5 10.081000.084250.087500.090750.09400Recall@20
Recall@20
NDCG@20
0.034000.035750.037500.039250.04100
NDCG@20
(a) Beauty
0 0.1 0.3 0.5 10.09800.10050.10300.10550.1080Recall@20
Recall@20
NDCG@20
0.04500.04750.05000.05250.0550
NDCG@20
 (b) Toys
Figure 7: Recommendation Performance w.r.t different ğ›¾.
60,62,79] can be utilized for contrastive learning. In this study, we
primarily focus on regenerating sequences, and the exploration of
regenerating other data forms will be addressed in future work.
5.5 Hyper-parameter Sensitivity
In this subsection, we investigate the hyper-parameter sensitivity
of the proposed paradigm. Specifically, we focus on the impact of
two parameters on model performance, the diversity factor ğ¾and
the generative decoding probability ğ›¾. For the sake of simplicity,
we conducted experiments on Toys and Beauty based on SASRec.
5.5.1 The diversity factor K. We set K among[1,3,5,7,9]and the
results are depicted in Figure 6, where the performance exhibits an
increasing trend as K increases. This is because by increasing K, we
can effectively reduce confusion during training and simultaneously
enhance the diversity of the regenerated data. Furthermore, once
K exceeded a certain threshold, further increasing did not lead to
significant changes. This indicates that the regenerator had already
captured all available patterns, reaching a plateau phase.
5.5.2 The generative decoding probability ğ›¾.We set the parameter
ğ›¾within the range of [0,0.1,0.3,0.5,1]and visualized the results in
Figure 7. The performance initially shows an increasing trend fol-
lowed by a decrease. This observation highlights the importance of
striking a balance between exploration and exploitation during the
regeneration process. When ğ›¾is too low, the regenerator is unable
to sufficiently explore higher-order information. Conversely, when
ğ›¾is set too high, the model underutilizes the existing information,
leading to a decrease in the reliability of the generated results.
6 CONCLUSIONS
In this paper, we have investigated a novel problem concerning
the development of training data for sequential recommendations.
To tackle this problem, we have introduced a data-centric datasetregeneration framework called DR4SR. Our framework has dis-
played exceptional cross-architecture generalizability and has un-
derscored the complementary relationship between data-centric
and model-centric paradigms. Moreover, we have extended DR4SR
to a model-aware version, dubbed DR4SR+, which allows for per-
sonalized adaptation of the regenerated datasets to various target
models. Additionally, we have conducted in-depth analysis to inves-
tigate the potential of the data-centric approach, providing valuable
insights. For our future work, we intend to propose a more compre-
hensive framework that can regenerate various forms of data, such
as sequences, graphs, augmented data. We also plan to explore the
integration of Large Language Models to guide the dataset regener-
ation process, enabling the generation of data that maintains both
collaborative and semantic information.
Acknowledgments
This work is supported by the National Natural Science Foundation
of China (No. 62202443, U23A20319, and 62207031) and the Anhui
Provincial Science and Technology Major Project (No. 2023z020006).
Besides, we thank MindSpore[1] for the support of this work.
References
[1] 2020. MindSpore. https://www.mindspore.cn
[2]AndrÃ© Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev,
Mark Leznik, Kyle Chard, and Ian T. Foster. 2024. Comprehensive Exploration of
Synthetic Data Generation: A Survey. CoRR abs/2401.02524 (2024).
[3]Hong Chen, Xin Wang, Ruobing Xie, Yuwei Zhou, and Wenwu Zhu. 2023. Cross-
domain Recommendation with Behavioral Importance Perception. In Proceedings
of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4
May 2023. 1294â€“1304.
[4]Yongjun Chen, Zhiwei Liu, Jia Li, Julian J. McAuley, and Caiming Xiong. 2022.
Intent Contrastive Learning for Sequential Recommendation. In WWW â€™22: The
ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. ACM,
2172â€“2182.
[5]Wenqi Fan, Xiaorui Liu, Wei Jin, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2022.
Graph Trend Filtering Networks for Recommendation. In SIGIR â€™22: The 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval, Madrid, Spain, July 11 - 15, 2022. 112â€“121.
[6]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative
Adversarial Networks. CoRR abs/1406.2661 (2014).
[7]Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You.
2023. Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory
Matching. CoRR abs/2310.05773 (2023).
[8]Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu,
Defu Lian, and Enhong Chen. 2024. END4Rec: Efficient Noise-Decoupling for
Multi-Behavior Sequential Recommendation. CoRR abs/2403.17603 (2024).
[9]Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network
for Recommendation. In Proceedings of the 43rd International ACM SIGIR con-
ference on research and development in Information Retrieval, SIGIR 2020, Virtual
Event, China, July 25-30, 2020. 639â€“648.
[10] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2016. Session-based Recommendations with Recurrent Neural Networks. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto
Rico, May 2-4, 2016, Conference Track Proceedings.
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Proba-
bilistic Models. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual.
[12] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi,
and Tim Salimans. 2022. Cascaded Diffusion Models for High Fidelity Image
Generation. J. Mach. Learn. Res. 23 (2022), 47:1â€“47:33.
[13] Chengkai Hou, Jieyu Zhang, and Tianyi Zhou. 2023. When to Learn What: Model-
Adaptive Data Augmentation Curriculum. In IEEE/CVF International Conference
on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. 1717â€“1728.
[14] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong
Wen. 2022. Towards Universal Sequence Representation Learning for Recom-
mender Systems. In KDD â€™22: The 28th ACM SIGKDD Conference on Knowledge
3962KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Mingjia Yin et al.
Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022. ACM,
585â€“593.
[15] Wei-Ning Hsu, Yu Zhang, and James R. Glass. 2017. Unsupervised domain
adaptation for robust speech recognition via variational autoencoder-based data
augmentation. In 2017 IEEE Automatic Speech Recognition and Understanding
Workshop, ASRU 2017, Okinawa, Japan, December 16-20, 2017. 16â€“23.
[16] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization with
Gumbel-Softmax. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
[17] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang.
2020. Graph Structure Learning for Robust Graph Neural Networks. In KDD
â€™20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
Virtual Event, CA, USA, August 23-27, 2020. 66â€“74.
[18] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec-
ommendation. In IEEE International Conference on Data Mining, ICDM 2018,
Singapore, November 17-20, 2018. IEEE Computer Society, 197â€“206.
[19] Kibum Kim, Dongmin Hyun, Sukwon Yun, and Chanyoung Park. 2023. MELT:
Mutual Enhancement of Long-Tailed User and Item for Sequential Recommenda-
tion. In SIGIR. ACM, 68â€“77.
[20] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.
[21] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In
2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings.
[22] Walid Krichene and Steffen Rendle. 2021. On Sampled Metrics for Item Recom-
mendation (Extended Abstract). In Proceedings of the Thirtieth International Joint
Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada,
19-27 August 2021. 4784â€“4788.
[23] Riwei Lai, Li Chen, Rui Chen, and Chi Zhang. 2024. A Survey on Data-Centric
Recommender Systems. arXiv preprint arXiv:2401.17878 (2024).
[24] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.
Neural Attentive Session-based Recommendation. In Proceedings of the 2017 ACM
on Conference on Information and Knowledge Management, CIKM 2017, Singapore,
November 06 - 10, 2017. 1419â€“1428.
[25] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian J.
McAuley. 2023. Text Is All You Need: Learning Language Representations for
Sequential Recommendation. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August
6-10, 2023. ACM, 1258â€“1267.
[26] Defu Lian, Xu Huang, Xiaolong Chen, Jin Chen, Xingmei Wang, Yankai Wang,
Haoran Jin, Rui Fan, Zheng Liu, Le Wu, and Enhong Chen. 2023. RecStudio:
Towards a Highly-Modularized Recommender System. In SIGIR. ACM, 2890â€“
2900.
[27] Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, and Wanyu Wang. 2023.
AutoDenoise: Automatic Data Instance Denoising for Recommendations. In
WWW. ACM, 1003â€“1011.
[28] Chong Liu, Xiaoyang Liu, Rongqin Zheng, Lixin Zhang, Xiaobo Liang, Juntao
Li, Lijun Wu, Min Zhang, and Leyu Lin. 2023. CT4Rec: Simple yet Effective
Consistency Training for Sequential Recommendation. In KDD. ACM, 3901â€“
3913.
[29] Fan Liu, Zhiyong Cheng, Huilin Chen, Yinwei Wei, Liqiang Nie, and Mohan S.
Kankanhalli. 2022. Privacy-Preserving Synthetic Data Generation for Recom-
mendation Systems. In SIGIR â€™22: The 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15,
2022. 1379â€“1389.
[30] Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan.
2022. Towards Unsupervised Deep Graph Structure Learning. In WWW â€™22:
The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022.
1392â€“1403.
[31] Zhiwei Liu, Ziwei Fan, Yu Wang, and Philip S. Yu. 2021. Augmenting Sequential
Recommendation with Pseudo-Prior Items via Reversely Pre-training Trans-
former. In SIGIR â€™21: The 44th International ACM SIGIR Conference on Research
and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021.
1608â€“1612.
[32] Jonathan Lorraine, Paul Vicol, and David Duvenaud. 2020. Optimizing Millions of
Hyperparameters by Implicit Differentiation. In The 23rd International Conference
on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online
[Palermo, Sicily, Italy]. 1540â€“1552.
[33] Aviv Navon, Idan Achituve, Haggai Maron, Gal Chechik, and Ethan Fetaya. 2021.
Auxiliary Learning by Implicit Differentiation. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
[34] Aleksandr V. Petrov and Craig Macdonald. 2022. Effective and Efficient Training
for Sequential Recommendation using Recency Sampling. In RecSys. ACM, 81â€“91.
[35] Yuqi Qin, Pengfei Wang, and Chenliang Li. 2021. The World is Binary: Contrastive
Learning for Denoising Next Basket Recommendation. In SIGIR â€™21: The 44th
International ACM SIGIR Conference on Research and Development in Information
Retrieval, Virtual Event, Canada, July 11-15, 2021. 859â€“868.[36] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive Learn-
ing for Representation Degeneration Problem in Sequential Recommendation. In
WSDM â€™22: The Fifteenth ACM International Conference on Web Search and Data
Mining, Virtual Event / Tempe, AZ, USA, February 21 - 25, 2022. ACM, 813â€“823.
[37] Noveen Sachdeva, Mehak Preet Dhaliwal, Carole-Jean Wu, and Julian J. McAuley.
2022. Infinite Recommendation Networks: A Data-Centric Approach. In Advances
in Neural Information Processing Systems 35: Annual Conference on Neural Infor-
mation Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November
28 - December 9, 2022.
[38] Noveen Sachdeva, Zexue He, Wang-Cheng Kang, Jianmo Ni, Derek Zhiyuan
Cheng, and Julian J. McAuley. 2023. Farzi Data: Autoregressive Data Distillation.
CoRR abs/2310.09983 (2023).
[39] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre-
sentations from Transformer. In Proceedings of the 28th ACM International Con-
ference on Information and Knowledge Management, CIKM 2019, Beijing, China,
November 3-7, 2019. 1441â€“1450.
[40] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation
via Convolutional Sequence Embedding. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining, WSDM 2018, Marina
Del Rey, CA, USA, February 5-9, 2018. ACM, 565â€“573.
[41] Xiaohai Tong, Pengfei Wang, Chenliang Li, Long Xia, and Shaozhang Niu. 2021.
Pattern-enhanced Contrastive Policy Learning Network for Sequential Recom-
mendation. In Proceedings of the Thirtieth International Joint Conference on Artifi-
cial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021.
1593â€“1599.
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA. 5998â€“6008.
[43] Cheng Wang, Jiacheng Sun, Zhenhua Dong, Ruixuan Li, and Rui Zhang. 2023.
Gradient Matching for Categorical Data Distillation in CTR Prediction. In Proceed-
ings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore,
Singapore, September 18-22, 2023. 161â€“170.
[44] Hao Wang, Defu Lian, Hanghang Tong, Qi Liu, Zhenya Huang, and Enhong
Chen. 2022. HyperSoRec: Exploiting Hyperbolic User and Item Representations
with Multiple Aspects for Social-aware Recommendation. ACM Trans. Inf. Syst.
40, 2 (2022), 24:1â€“24:28.
[45] Hao Wang, Tong Xu, Qi Liu, Defu Lian, Enhong Chen, Dongfang Du, Han Wu,
and Wen Su. 2019. MCNE: An End-to-End Framework for Learning Multiple
Conditional Network Representations of Social Network. In KDD. ACM, 1064â€“
1072.
[46] Mengzhu Wang, Yuehua Liu, Jianlong Yuan, Shanshan Wang, Zhibin Wang, and
Wei Wang. 2024. Inter-Class and Inter-Domain Semantic Augmentation for
Domain Generalization. IEEE Transactions on Image Processing (2024).
[47] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, and
Mehmet A. Orgun. 2019. Sequential Recommender Systems: Challenges, Progress
and Prospects. In Proceedings of the Twenty-Eighth International Joint Conference
on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019 . 6332â€“6338.
[48] Shoujin Wang, Qi Zhang, Liang Hu, Xiuzhen Zhang, Yan Wang, and Charu Aggar-
wal. 2022. Sequential/Session-based Recommendations: Challenges, Approaches,
Applications and Opportunities. In SIGIR â€™22: The 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval, Madrid, Spain,
July 11 - 15, 2022. ACM, 3425â€“3428.
[49] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. 2018.
Dataset Distillation. CoRR abs/1811.10959 (2018).
[50] Xin Wang, Amin Hosseininasab, Pablo Colunga, Serdar KadÄ±oÄŸlu, and Willem-Jan
van Hoeve. 2022. Seq2Pat: Sequence-to-pattern generation for constraint-based
sequential pattern mining. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 36. 12665â€“12671.
[51] Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xianling Mao, and Minghui Qiu.
2020. Global Context Enhanced Graph Neural Networks for Session-based Rec-
ommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval, SIGIR 2020, Virtual Event,
China, July 25-30, 2020. 169â€“178.
[52] Jiahao Wu, Wenqi Fan, Shengcai Liu, Qijiong Liu, Rui He, Qing Li, and Ke Tang.
2023. Dataset Condensation for Recommendation. CoRR abs/2310.01038 (2023).
[53] Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing Li, Xiao-
Ming Wu, and Ke Tang. 2023. Leveraging Large Language Models (LLMs) to
Empower Training-Free Dataset Condensation for Content-Based Recommenda-
tion. CoRR abs/2310.09874 (2023).
[54] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,
Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. 2023.
A Survey on Large Language Models for Recommendation. CoRR abs/2305.19860
(2023).
[55] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan.
2018. Session-based Recommendation with Graph Neural Networks. CoRR
3963Dataset Regeneration for Sequential Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
abs/1811.00855 (2018).
[56] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu
Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking Efficiency in Large Language
Model Inference: A Comprehensive Survey of Speculative Decoding. CoRR
abs/2401.07851 (2024).
[57] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin
Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation.
In38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala
Lumpur, Malaysia, May 9-12, 2022. IEEE, 1259â€“1273.
[58] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie Xu, Fuzhen
Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph Contextualized Self-
Attention Network for Session-based Recommendation. In Proceedings of the
Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019,
Macao, China, August 10-16, 2019. 3940â€“3946.
[59] An Yan, Shuo Cheng, Wang-Cheng Kang, Mengting Wan, and Julian J. McAuley.
2019. CosRec: 2D Convolutional Neural Networks for Sequential Recommen-
dation. In Proceedings of the 28th ACM International Conference on Information
and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019.
2173â€“2176.
[60] Xihong Yang, Jin Jiaqi, Siwei Wang, Ke Liang, Yue Liu, Yi Wen, Suyuan Liu, Sihang
Zhou, Xinwang Liu, and En Zhu. 2023. Dealmvc: Dual contrastive calibration for
multi-view clustering. In Proceedings of the 31st ACM International Conference on
Multimedia. 337â€“346.
[61] Xihong Yang, Yue Liu, Sihang Zhou, Siwei Wang, Wenxuan Tu, Qun Zheng,
Xinwang Liu, Liming Fang, and En Zhu. 2023. Cluster-guided Contrastive Graph
Clustering Network. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 37. 10834â€“10842.
[62] Xihong Yang, Cheng Tan, Yue Liu, Ke Liang, Siwei Wang, Sihang Zhou, Jun
Xia, Stan Z Li, Xinwang Liu, and En Zhu. 2023. Convert: Contrastive graph
clustering with reliable augmentation. In Proceedings of the 31st ACM International
Conference on Multimedia. 319â€“327.
[63] Ghim-Eng Yap, Xiaoli Li, and Philip S. Yu. 2012. Effective Next-Items Recom-
mendation via Personalized Sequential Pattern Mining. In Database Systems for
Advanced Applications - 17th International Conference, DASFAA 2012, Busan, South
Korea, April 15-19, 2012, Proceedings, Part II. 48â€“64.
[64] Mingjia Yin, Hao Wang, Xiang Xu, Likang Wu, Sirui Zhao, Wei Guo, Yong
Liu, Ruiming Tang, Defu Lian, and Enhong Chen. 2023. APGL4SR: A Generic
Framework with Adaptive and Personalized Global Collaborative Information
in Sequential Recommendation. In Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management, CIKM 2023, Birmingham,
United Kingdom, October 21-25, 2023. ACM, 3009â€“3019.
[65] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2024.
Self-Supervised Learning for Recommender Systems: A Survey. IEEE Trans.
Knowl. Data Eng. 36, 1 (2024), 335â€“355.
[66] Ruonan Yu, Songhua Liu, and Xinchao Wang. 2024. Dataset Distillation: A
Comprehensive Review. IEEE Trans. Pattern Anal. Mach. Intell. 46, 1 (2024),
150â€“170.
[67] Jiahao Yuan, Zihan Song, Mingyou Sun, Xiaoling Wang, and Wayne Xin Zhao.
2021. Dual Sparse Attention Network For Session-based Recommendation. In
Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third
Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The
Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,
Virtual Event, February 2-9, 2021. 4635â€“4643.
[68] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang,
Shaochen Zhong, and Xia Hu. 2023. Data-centric Artificial Intelligence: A Survey.
CoRR abs/2303.10158 (2023).
[69] An Zhang, Wenchang Ma, Jingnan Zheng, Xiang Wang, and Tat-Seng Chua.
2023. Robust Collaborative Filtering to Popularity Distribution Shift. CoRR
abs/2310.10696 (2023).
[70] Chi Zhang, Rui Chen, Xiangyu Zhao, Qilong Han, and Li Li. 2023. Denoising and
Prompt-Tuning for Multi-Behavior Recommendation. In Proceedings of the ACM
Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023.
1355â€“1363.
[71] Bo Zhao and Hakan Bilen. 2023. Dataset Condensation with Distribution Match-
ing. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV
2023, Waikoloa, HI, USA, January 2-7, 2023. 6503â€“6512.
[72] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2021. Dataset Condensation
with Gradient Matching. In 9th International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
[73] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2021. Dataset Condensation
with Gradient Matching. In 9th International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
[74] Xin Zheng, Yixin Liu, Zhifeng Bao, Meng Fang, Xia Hu, Alan Wee-Chung Liew,
and Shirui Pan. 2023. Towards Data-centric Graph Machine Learning: Review
and Outlook. CoRR abs/2309.10979 (2023).
[75] Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan Zhang,
Yang You, and Jiashi Feng. 2023. Dataset Quantization. In IEEE/CVF InternationalConference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. 17159â€“
17170.
[76] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,
Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for se-
quential recommendation with mutual information maximization. In Proceedings
of the 29th ACM international conference on information & knowledge management .
1893â€“1902.
[77] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Filter-enhanced
MLP is All You Need for Sequential Recommendation. In WWW â€™22: The ACM
Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. 2388â€“2399.
[78] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Qiang Liu, Shu Wu, and Liang Wang.
2021. Deep Graph Structure Learning for Robust Representations: A Survey.
CoRR abs/2103.03036 (2021).
[79] Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng, and
Yuguang Chen. 2021. LearnDA: Learnable knowledge-guided data augmentation
for event causality identification. arXiv preprint arXiv:2106.01649 (2021).
A APPENDIX
A.1 Pseudo Code of DR4SR and DR4SR+
The pipeline of DR4SR is detailed in Algorithm 1, and the pipeline
of DR4SR+ is detailed in Algorithm 2.
A.2 Detailed Derivations of Equation 10
By applying the chain rule, we can compute calculate the implicit
gradient ofğ¿dev(ğœƒâˆ—(ğœ™))as follows:
âˆ‡ğœ™ğ¿dev(ğœƒâˆ—(ğœ™))=âˆ‡ğœƒğ¿dev(ğœƒâˆ—(ğœ™))âˆ‡ğœ™ğœƒâˆ—(ğœ™). (11)
Algorithm 1 Pseudo code of DR4SR
Input: The original sequence dataset X.
Output: A regenerated dataset Xâ€²and scores for each data sample
WâˆˆR|Xâ€²|.
1:Construct a pre-training dataset Xpreas in Section 4.2.1
2:while not converged do âŠ²Pre-training
3: Extract a batch of sequence-pattern pairs (ğ‘ ğ‘¢,ğ‘ğ‘–)fromXpre
4: Get K memories{mâ€²
1,mâ€²
2,..., mâ€²
ğ¾}=Encoder(ğ‘ ğ‘¢)
5: Get a category vector ğœ‹=DiversityPromoter(ğ‘ğ‘–)
6: Get a mixed memory by mfinal=Ãğ¾
ğ‘˜=1ğœ‹ğ‘˜mâ€²
ğ‘˜
7: Get a decoded sequence ğ‘â€²
ğ‘–=Decoder(ğ‘ğ‘–,mfinal)
8: Optimize the dataset generator with Equation 2
9:end while
10:while not converged do âŠ²Inference
11: Extract a sequence ğ‘ ğ‘¢from the original dataset X
12: Get K memories{mâ€²
1,mâ€²
2,..., mâ€²
ğ¾}=Encoder(ğ‘ ğ‘¢)
13: Init the regenerated dataset Xâ€²as an empty set
14: formâ€²
ğ‘˜in{mâ€²
1,mâ€²
2,..., mâ€²
ğ¾}do
15: Init the regenerated pattern ğ‘â€²
ğ‘–as the <BOS> token
16: whileğ‘£â‰ <EOS> token do
17: Get a probability vector q=Decoder(ğ‘â€²
ğ‘–,mâ€²
ğ‘˜)
18: ifGenerative decoding then
19: Get itemğ‘£âˆˆV with the highest probability
20: else
21: Get itemğ‘£âˆˆğ‘ ğ‘¢with the highest probability
22: end if
23: Update the regenerated pattern ğ‘â€²
ğ‘–â†ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘â€²
ğ‘–,ğ‘£)
24: end while
25:Xâ€²â†Xâ€²âˆªğ‘â€²
ğ‘–
26: end for
27:end while
3964KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Mingjia Yin et al.
Table 6: The overall performance on two additional datasets.
Metric GRU4Re
c DR4SR DR4SR+ SASRe
c DR4SR DR4SR+ FMLP
DR4SR DR4SR+ GNN
DR4SR DR4SR+ CL4SRe
c DR4SR DR4SR+
Go
walla R@20 0.1297
0.1325 0.1392 0.1672
0.1786 0.1856 0.1710
0.1811 0.1833 0.1274
0.1331 0.1348 0.2012
0.2064 0.2095
N@20 0.0582
0.0615 0.0634 0.0833
0.0912 0.0934 0.0851
0.0903 0.0922 0.0627
0.0653 0.0662 0.0987
0.1032 0.1048
Bo
ok R@20 0.0223
0.0238 0.0245 0.0308
0.0343 0.0372 0.0388
0.0432 0.0444 0.0301
0.0332 0.0346 0.0367
0.0378 0.0386
N@20 0.0087
0.0098 0.0112 0.0125
0.0140 0.0154 0.0162
0.0181 0.0188 0.0129
0.0141 0.0148 0.0153
0.0161 0.0165
The first termâˆ‡ğœƒğ¿dev(ğœƒâˆ—(ğœ™))can be obtained with automatic dif-
ferentiation (autograd) tools. As for the second term, we notice that
ğœƒâˆ—(ğœ™)is the optimal point of ğ¿train(ğœƒ;ğœ™), so we have:
âˆ‡ğœƒğ¿train(ğœƒâˆ—(ğœ™),ğœ™)=0. (12)
Then, we calculate the gradient with respect to ğœ™in Equation 12:
âˆ‡2
ğœƒğ¿train(ğœƒâˆ—(ğœ™),ğœ™)âˆ‡ğœ™ğœƒâˆ—(ğœ™)+âˆ‡ğœ™âˆ‡ğœƒğ¿train(ğœƒâˆ—(ğœ™),ğœ™)=0,(13)
which leads to:
âˆ‡ğœ™ğœƒâˆ—(ğœ™)=âˆ’(âˆ‡2
ğœƒğ¿train(ğœƒâˆ—(ğœ™),ğœ™))âˆ’1Â·âˆ‡ğœ™âˆ‡ğœƒğ¿train(ğœƒâˆ—(ğœ™),ğœ™),(14)
where the Hessian inverse (âˆ‡2
ğœƒğ¿train(ğœƒâˆ—(ğœ™),ğœ™))âˆ’1can be approxi-
mated toÃğ¾
ğ‘›=0(ğ¼âˆ’âˆ‡2
ğœƒğ¿train(ğœƒâˆ—(ğœ™))ğ‘›with K-truncated Neumann
series[32, 33]. Therefore, we can obtain the following results:
âˆ‡ğœ™ğœƒâˆ—(ğœ™)=âˆ’ğ¾âˆ‘ï¸
ğ‘›=0(ğ¼âˆ’âˆ‡2
ğœƒğ¿train(ğœƒâˆ—(ğœ™))ğ‘›Â·âˆ‡ğœ™âˆ‡ğœƒğ¿train(ğœƒâˆ—(ğœ™),ğœ™).
(15)
Finally, the implicit gradient of ğ¿dev(ğœƒâˆ—(ğœ™))is:
âˆ‡ğœ™ğ¿dev=âˆ’âˆ‡ğœƒğ¿devÂ·ğ¾âˆ‘ï¸
ğ‘›=0(ğ¼âˆ’âˆ‡2
ğœƒğ¿train)ğ‘›Â·âˆ‡ğœ™âˆ‡ğœƒğ¿train, (16)
where the parameters are omitted for the sake of brevity.
A.3 Computational Complexity
The scale of datasets can be measured with the number of sequences
N, maximum sequence length L, maximum pattern length M, and
Algorithm 2 Pseudo code of DR4SR+
Input: The regenerated sequence dataset Xâ€², a target model ğ‘“with
parametersğœƒ.
Output: A personalizer ğ‘”with parameters ğœ™that can assign scores
WâˆˆR|Xâ€²|for each data sample in Xâ€².
1:Generate a dev dataset Xâ€²
devby shufflingXâ€²
2:while not converged do
3: // Lower optimization
4: forround = 1â†’ğ‘‡lower do
5: Extract a batch ğ‘fromXâ€²
6: Get data sample scores W=ğ‘”(ğ‘)
7: Optimizeğ‘“based on Equation 8
8: end for
9: // Upper optimization
10: Extract a batch ğ‘devfromXâ€²
dev
11: Getâˆ‡ğœƒğ¿dev(ğœƒâˆ—(ğœ™))withğ‘devand Equation 6
12: Getâˆ‡ğœ™ğœƒâˆ—(ğœ™)by implicit differentiation
13: Getâˆ‡ğœ™ğ¿dev(ğœƒâˆ—(ğœ™))as in Equation 11
14: updateğœ™withâˆ‡ğœ™ğ¿dev(ğœƒâˆ—(ğœ™))
15:end whilethe number of extracted patterns q (empirically, q is of the same
order of magnitude as N). We denote the diversity factor as K.
The pre-training dataset is constructed with sequential pattern
mining, which is implemented with Seq2Pat [ 50] and its complex-
ity isğ‘‚(ğ‘ğ‘€ğ‘). Empirically, this process can be accelerated with
offline pre-computation or batching technique [ 50]. During pre-
training, the regenerator needs to process each sequence. Consider-
ing the quadratic complexity of the self-attention mechanism, the
overall complexity is ğ‘‚(ğ‘ğ¿2ğ‘‘). During inference, the complexity is
ğ‘‚(ğ¾ğ‘ğ¿2ğ‘‘), as we need to regenerate K patterns from each sequence.
Meanwhile, autoregressive decoding can be further accelerated
with efficient decoding [ 56]. As for dataset personalization, denote
the complexity of the target model as ğ‘. The additional overhead
incurred by the bi-level optimization stems from the upper opti-
mization conducted every ğ‘‡lower rounds. In the upper optimization,
complexity mainly results from the implicit gradient computation,
which necessitates (K+2) backward for vector-Jacobi product[ 33].
Herein, K represents the truncated number of Neumann series,
which is fixed at 3. Consequently, the complexity merely amounts
toğ‘‚
1+5
ğ‘‡lower
ğ‘
. Therefore, with some acceleration strategies,
DR4SR and DR4SR+ is scalable when applied to large datasets.
A.4 Detailed Hyper-parameters
We first introduce the hyper-parameter settings of target mod-
els: For GRU4Rec, we search the layer number among [2, 3], and
dropout probability among [0.2, 0.5]. For SASRec, the layer number
is 2, the dimension of hidden layers is 128, and the dropout prob-
ability is 0.5. For FMLP, the layer number is 2, the dimension of
hidden layers is 256, and the dropout probability is 0.5. For GCE-
GNN, we first replace its graph encoder with LightGCN[ 9] and
its sequential encoder with SASRec[ 18], which can significantly
improve its performance. Then, we search the number of graph
convolution layers among [2, 3]. For CL4SRec, we set the weight of
contrastive learning loss to 0.1, the crop/mask/reorder rate in data
augmentation is set to 0.2/0.7/0.2, and other hyper-parameters are
the same as SASRec.
Then we consider particular hyper-parameters of DR4SR. For
dataset regeneration, we set sliding window size ğ›¼to 5 for Beauty
and 10 for others, and we set the threshold ğ›½to 2 for all datasets.
We fix the diversity number ğ¾to 5, and the generative decoding
probability ğ›¾to 0.1. For DR4SR+, we set ğ‘‡lower to 30, and adopt
SGD as the upper optimizer with learning rate and weight decay
searched among [1e-3, 1e-2, 1e-1].
A.5 Generalization to More Datasets
We provide the results on two additional datasets Amazon-book and
Gowalla in Table 6, where we can find that the proposed method can
be generalized to datasets with large-scale or different distribution.
3965