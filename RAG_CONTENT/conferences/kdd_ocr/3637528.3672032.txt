InLN: Knowledge-aware Incremental Leveling Network for
Dynamic Advertising
Xujia Li
Hong Kong University of Science and
Technology
Hong Kong SAR, China
xligm@connect.ust.hkJingshu Pengâˆ—
Hong Kong University of Science and
Technology
Hong Kong SAR, China
jpengab@connect.ust.hkLei Chenâ€ 
Hong Kong University of Science and
Technology (Guangzhou)
Guangzhou, China
leichen@cse.ust.hk
Abstract
In todayâ€™s fast-paced world, advertisers are increasingly demanding
real-time and accurate personalized ad delivery based on dynamic
preference modeling, which emphasizes the temporality existing in
both user preference and product characteristics. Meanwhile, with
the development of graph neural networks (GNNs), E-commerce
knowledge graphs (KG) with rich semantic relatedness are invoked
to improve accuracy and provide appropriate explanations to en-
courage advertisersâ€™ willingness to invest in ad expenses. However,
it is still challenging for existing methods to comprehensively con-
sider both time-series interactions and graph-structured knowledge
triples in a unified model, i.e., the case in knowledge-aware dynamic
advertising. The interaction graph between users and products
changes rapidly over time, while the knowledge in KG remains
relatively stable. This results in an uneven distribution of temporal
and semantic information, causing existing GNNs to fail in this sce-
nario. In this work, we quantitatively define the above phenomenon
as temporal unevenness and introduce the Incremental Leveling
Network (InLN) with three novel techniques: the periodic-focusing
window for node-level dynamic modeling, the biased temporal walk
for subgraph-level dynamic modeling and the incremental leveling
mechanism for KG updating. Verified by comprehensive and inten-
sive experiments, InLN outperforms nine baseline models in three
tasks by substantial margins, reaching up to a 9.9% improvement
and averaging a 5.7% increase.
CCS Concepts
â€¢Mathematics of computing â†’Graph algorithms; â€¢Informa-
tion systemsâ†’Computational advertising.
Keywords
Computational Advertising, Knowledge Graph, Temporal Graph
Neural Network.
âˆ—Corresponding author.
â€ Also with Hong Kong University of Science and Technology, Hong Kong SAR, China.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672032ACM Reference Format:
Xujia Li, Jingshu Peng, and Lei Chen. 2024. InLN: Knowledge-aware Incre-
mental Leveling Network for Dynamic Advertising. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3672032
1 Introduction
â€œHalf the money I spend on advertising is wasted; the trouble is I
donâ€™t know which half, " said John Wanamaker, the pioneer of mod-
ern advertising [ 27]. To this day, this quote still highlights two core
problems in computational advertising. On one hand, advertisers
seek to enhance dynamic preference modeling for more accurate
personalized ad delivery in the ever-evolving market, which in turn
promotes user consumption willingness and justifies their advertis-
ing expenses. On the other hand, both advertisers and users express
concern about the rationale behind each ad delivery. To answer
what to advertise and when to advertise [ 3,17], the cutting-edge
approach is to dynamically embed the users and products via deep-
learning techniques and rank their similarity in the latent space
over time. In addition to only using time-series data in traditional re-
current neural networks (RNNs) [ 12,25,36], temporal graph neural
networks show strength by additionally considering the topological
structure of user-product interaction graph, where relevant prod-
ucts or users with similar preference are located closely [ 22,38].
However, current temporal GNNs still face two problems: Firstly,
current temporal GNNs only model the user-product interactions
as independent samples, neglecting the relatedness between prod-
ucts [ 32]. This insufficient exploitation of user intention behind
their purchase history may lead to sub-optimal performance, e.g., a
user keeps buying some products mainly because of his preference
for the brand rather than the product itself. The second issue is
that, comparing the embedding similarity in the latent space cannot
provide clear insights about why to recommend certain products
and further promote consumer willingness [31].
Considering the successful application of the knowledge-graph
technique in various intelligent systems [ 21], we believe its po-
tential to resolve the mentioned bottlenecks of temporal GNNs.
E-commerce KGs, as an extendable and flexible carriage of knowl-
edge, contain product nodes and rich semantic entities, such as
brands and categories [ 40]. The product relevance can significantly
improve the model performance [ 43]. Meanwhile, the path with
usersâ€™ purchase records, timestamps and KG relations can be trans-
lated into the reason for this advertisement[31].
Although many KG-enhanced advertising systems have been
adopted on a pure static graph [ 2,28,29,31,33,37], it is still
challenging to incorporate edge timing and mine user dynamic
 
1679
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xujia Li, Jingshu Peng, and Lei Chen
Figure 1: Temporal uneven graph and a toy example of
knowledge-aware advertising
preference in a unified graph learning model [ 4,16]. A toy example
of dynamic advertising with KG is highlighted in Fig. 1. The rea-
soning path with edge time indicates not only the semantic reason
"Product-Category-Brand" but also the explanation from the tempo-
ral aspect "Snowy season around December". The input information
for the dynamic and knowledge-aware advertising is visualized
in this temporal uneven graph: Compared with the frequently-
changing interaction graph where every edge owns a timestamp,
the KG remains relatively static without edge timing. We define
this uneven distribution of temporal information on a single
graph as Temporal Unevenness (refer to Definition 1).
There are three technical challenges for a graph-based frame-
work to fully leverage the uneven-distributed temporal information
and semantic knowledge in knowledge-aware dynamic advertising:
Periodic Preference: nodes exhibit different periodic character-
istics, which is commonly observed in dynamic preference. Specif-
ically, products can be classified into seasonal, trendy and daily
goods as the hashtags in Fig. 1. These periodic features cause users
to purchase numerous products with similar properties in a short
period or consistently adopt some goods all year round. E.g., for sea-
sonal products like swimming glass, the most expressive purchase
records were gathered in the summer period. However, existing
graph-based methods [ 14,22,38] deliver the whole nodesâ€™ history
interactions directly into RNNs or normal attention layers, which
cannot focus on the significant period slices and precisely capture
the periodic patterns, so the model performance is limited by the
inferior representation of the single-node history.
Exploding Bi-neighborhood: in dynamic knowledge-aware
advertising, each user and product node not only accumulates a
large number of interactions at different times, but also owns numer-
ous neighbors in KG. With the increase in the network exploration
depth, the explosion of neighborhoods can soon lead to unafford-
able computational costs. Ideally, according to different timestamps
and locations in a graph, the nodes should have varying proba-
bilities of being sampled for GNN aggregation. For example, the
embedding of a fashion-related node depends more on the temporal
edges during a certain period, which means its neighbors in theinteraction graph should be more frequently sampled. While a daily
product may rely more on its relatedness in KG, e.g., a loyal Adidas
fan keeps purchasing brand-related goods. However, existing sam-
pling strategies of temporal GNNs fail in such cases: the random
sampling strategy neglects edge timing and loses most of the tem-
poral information [ 9,38]. While the most-recent sampling strategy
only selects neighbors via temporal interactions and totally passes
the static KG corpus [22].
Temporal Unevenness: dynamic advertising requires dynamic
node representation, not only for the user and product nodes, the
representation of the rest of KG entities should also keep evolving
over time. E.g., the entities like NBA should own differentiated
embeddings during the game period or during the off-season. The
difference of KG embedding on the time-axis helps to adjust the
distance between users and products in the embedding space, so that
the model can advertise products according to a specific time period.
However, the existing KG-enhanced solutions [ 28,33,34] adopt
the stationary pre-trained embeddings throughout the entire GNN
training, while temporal GNN solutions can only consume dynamic
interactions sequentially as training samples. In both solutions, this
temporal unevenness and the relative stationarity in KG result in
outdated and ineffective knowledge materials.
To address the above challenges, we present the Incremental lev-
eling network (InLN) in this paper, which consumes both temporal
information and knowledge in a unified graph learning framework.
To dynamically capture the periodic characteristics of users and
products, we propose a novel multi-head periodic-focusing win-
dow, which can adaptively mask out the irrelevant time periods
from the complete sequence of the node history. Then, the layer
will only allocate attention weights on the valid interactions, which
restricts the model to only focus on the informative time slice. Then,
we design the adaptive sub-graph sampler with the biased tem-
poral walks to capture the most expressive bi-neighborhood and
save computational resources for GNN aggregation. To identify
the most active neighbors at each moment, the sampler executes
multiple temporal walks from the input node guided by a biased
probability distribution. This distribution allows the neighbors with
higher correlation in both temporal and topological dimensions
to get a higher possibility of being sampled. Afterward, the most
frequently-arrived neighbors can constitute the adaptive sub-graph
for aggregation. To tackle the temporal unevenness, we propose
theincremental leveling mechanism to continually update the
KG representation and balance the distribution of temporal infor-
mation. Each semantic entity in KG owns an indicator to represent
its temporal activity in a local graph area in the recent time period.
Every time an entity is reached by a temporal walk, its activity indi-
cator gets a raise. Once its activity level exceeds a certain threshold,
an incremental batch for these semantic entities is automatically
inserted into the training process and gets continually updated.
The main contributions of this work are summarized as:
â€¢We propose InLN with three novel techniques, which is the first
GNN-based framework to achieve dynamic preference modeling
in knowledge-aware advertising.
â€¢We quantitatively define the phenomenon of Temporal Uneven-
ness and verify the importance of incorporating both temporal
information and KG corpus into computational advertising.
 
1680InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
â€¢Intensive experiments including performance evaluation, abla-
tion study, sensitivity analysis, efficiency test, and case study
verify InLN superiority over SOTA baselines.
2 Related Work
2.1 Temporal Graph Neural Networks
The first direction in this area is considering the discrete-time
dynamic graphs as a sequence of static graph snapshots, so that
the static GNN models can be applied to produce the embedding
separately and then generate as series data [ 23,36,39]. However,
these methods are not precise enough to capture the local events
and trends. Thus, the latest research put more attention on the
continue-time dynamic graphs, usually recorded as a list of edge
events. TGAT [ 38] proposes a self-attention mechanism in GNN
aggregation to collect temporal-topological features from node
neighborhoods. But TGAT ignores the temporal features on the
node level, i.e. the node history. TGNs [ 22] combines a time-series
RNN with a graph aggregator. However, it merely deals with the
most-recent interactions and ignores the inactive parts of the graph,
which limits its expressivity to capture temporal locality. To our best
knowledge, none of the temporal GNNs are compatible with the
static KGs to achieve knowledge-aware advertising [ 21], because
of the exploding bi-neighborhood and the temporal unevenness.
2.2 Advertising with Knowledge Graph
There are two main research directions in this area [ 43]. Firstly,
GNN-based embedding methods compute the similarity between la-
tent vectors of users and product ads by converting the whole graph
into the embedding space. KGAT [ 31] firstly constructs a static hy-
brid graph of KG and user-product interactions and introduces
the graph attention technique. KGNN-LS [ 28] further transforms
the KG into a weighted graph to compute user-specific product
embedding, while KGIN [ 33] focuses on the user intents behind
each purchase behavior by investigating the relation sequences
in KG. In the scenario of sequential advertising, SHCF [ 14] and
THIGE [ 11] incorporate an attention mechanism on static product
attributes as heterogeneous information input. PDKR [ 25] reforms
KG into sequential format and captures the patterns with the Gated
Recurrent Unit. The second category is the path-based methods.
Xian et al. [ 37] utilizes reinforcement learning to train an agent,
which can walk starting from a user node and ending at a prod-
uct node, whereas Lei et al. [ 13] generate the reasoning path on
KG with the human-in-loop question and answering. Notably, all
SOTA works utilize static knowledge and have consensus on their
future direction: the knowledge-aware dynamic advertising is a
promising but challenging topic [6].
3 Preliminary
3.1 Problem Definition
The temporal uneven graph Gconsists of the temporal interaction
graphGğ¼ğºand the E-commerce knowledge graph Gğ¾ğº. The two
graphs are connected together through the shared product nodes.
On the temporal uneven graph, the set of nodes ğ‘£ğ‘–âˆˆEincludes
user nodes, product nodes, KG entities; and the set of undirected
relationsğ‘ŸâˆˆR includes user-product interactions, KG relations.
Specifically, in the temporal interaction graph, the user behaviorson E-commerce platforms (i.e., browse, purchase, click, etc.) can be
represented as{(ğ‘£ğ‘–,ğ‘Ÿ,ğ‘£ğ‘—,ğ‘¡)|ğ‘£ğ‘–âˆˆU,ğ‘ŸâˆˆRğ¼ğº,ğ‘£ğ‘—âˆˆP} , whereUâŠ‚
Eis the set of user nodes, PâŠ‚E is the set of product nodes, ğ‘¡is the
timestamp of this interaction and each ğ‘ŸâˆˆRğ¼ğºis the embedding
of the specific behavior such as the textual review or rating. As for
the static KG, every triple as the basic carrier of knowledge can
be represented as{(ğ‘£ğ‘–,ğ‘Ÿ,ğ‘£ğ‘—)|ğ‘£ğ‘–âˆˆEâ€²,ğ‘ŸâˆˆRğ¾ğº,ğ‘£ğ‘—âˆˆEâ€²}, where
Eâ€²=Eâˆ’U , andRğ¾ğº=Râˆ’Rğ¼ğºis the set of KG relations. Major
notations of InLN are summarized in Appendix A.1.
Input: An temporal uneven graph G=Gğ¼ğºâˆªGğ¾ğº, a specific
userğ‘£ğ‘–âˆˆU and the current timestamp ğ‘‡.
Output: A set of product ads, which the user has a higher prob-
ability to adopt; The temporal path from the user node ğ‘£ğ‘–âˆˆU to
each product node ğ‘£ğ‘—âˆˆP as the rationale for this advertisement.
3.2 Temporal Unevenness
The temporal unevenness is to measure how unbalanced the tem-
poral information is distributed on a graph:
Definition 1. (Temporal Unevenness)
Letğ‘ğ‘–represent the total number of temporal interactions of the
nodeğ‘£ğ‘–with others throughout the graph evolution. The distribution C
records the frequency ğ‘ğ‘–of all nodes on the graph G. And the temporal
unevennessğœŒGis the coefficient of variation of this distribution, where
ğœCandğœ‡Care the standard deviation and mean value of C:
ğœŒG=ğœC
ğœ‡Cğ‘¤â„ğ‘’ğ‘Ÿğ‘’Câˆ¼(ğ‘ğ‘–,âˆ€ğ‘£ğ‘–âˆˆE) (1)
The coefficient of variation is utilized to non-dimensionalize, so
that different graphs can be compared with each other. A larger ğœŒG
indicates a more unbalanced distribution of temporal information.
Visualized in Fig. 2, the temporal uneven graph has an extremely
largeğœŒG, since the entities in KG do not own any temporal neigh-
bors and only have one initial semantic relation, i.e., ğ‘ğ‘–=1. When
only considering temporal graphs, the temporal unevenness may
also exist: the graph suffering from long-tail effect generally has a
large temporal unevenness, because a large number of tail-products
have few temporal edges [ 41]. The existing temporal GNNs repeti-
tively update the embedding of the nodes located in the hot zone,
because their training batches consist only of temporal interactions.
The neglect of the quiet parts will make KG ineffective and the
cold-start problem of inactive users and products [ 15]. Hence, it is
worthwhile to design a leveling mechanism against the discrimina-
tion of the existing methods on the few-interacted nodes.
Figure 2: Temporal unevenness on Amazon Book Dataset via
3D surface interpolation. The horizontal coordinates are the
node position. Vertically, the logarithm operation is applied
on the frequency ğ‘ğ‘–for a gradual change of heights.
 
1681KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xujia Li, Jingshu Peng, and Lei Chen
We calculate temporal unevenness based on the formulation
(1), to test whether the proposed definition can objectively reflect
the unbalanced distribution of temporal information. As shown in
the following table, the temporal unevenness of Last-FM graph is
much higher than that of others, because all temporal information is
compressed in the 3.4% nodes in the interaction graph, which is very
unbalanced comparing 42.2% in Beauty and Book dataset. Then, if
we remove all the static KG parts, the temporal unevenness of a pure
dynamic graphGğ¼ğºdecreases but still exists (i.e., 2.04 for Beauty
and 1.81 for Book, respectively). We also visualize the leveling effect
of InLN to reduce the temporal unevenness by updating KG nodes
incrementally in Appendix A.5.
Table 1: The temporal unevenness of different graph datasets
Dataset Amazon Beauty Amazon Book Last-FM Music
Ratio of # Node inGğ¼ğºtoG 42.17% 42.17% 3.42%
Temporal Unevenness of G 4.93 3.10 12.58
Temporal Unevenness of Gğ¼ğº 2.04 1.81 1.47
4 Methodology
In this section, we introduce the methodology of InLN in details.
4.1 The Overall InLN Framework
Following the convention of most temporal GNN methods [ 22,
38], each training sample is a specific interaction (ğ‘£ğ‘–,ğ‘Ÿ,ğ‘£ğ‘—,ğ‘‡) âˆˆ
Gğ¼ğº. Therefore, two embedding processes are conducted, one for
the user node and one for the product node respectively. And the
last prediction layer examines the cosine similarity between their
embeddings and back propagates the loss. To generate high-quality
temporal embedding for each node, four major modules constitute
InLN, shown in Fig. 3:
Firstly, the input of Periodic Focusing Module âŸ¨1âŸ©is all interac-
tions of node ğ‘£ğ‘–before the current timestamp ğ‘‡. Then, the sequence
is fed into the focusing windows to generate the node-level repre-
sentationğ‘ ğ‘–
ğ‘‡as the output of this module.
In parallel, the Adaptive Sub-graph Sampler âŸ¨2âŸ©performs the
biased temporal walks starting from the input node ğ‘£ğ‘–. Multiple
temporal paths are sampled under the guidance of a biased proba-
bility distribution. In principle, the neighbors that have frequent
interactions with others or the hub neighbors with large degrees
are assigned a higher probability of being sampled. Based on the
sampling frequency, the selected neighbors form the adaptive sub-
graphGâŸ¨ğ‘£ğ‘–,ğ‘‡âŸ©surrounding node ğ‘£ğ‘–as the output. This sub-graph
contains both the graph-level temporal features and the topological
information of bi-neighborhood.
The latest node-level representation from âŸ¨1âŸ©and the sampled
adaptive sub-graph from âŸ¨2âŸ©is the input of the GNN Module âŸ¨3âŸ©.
The GNN Module aggregates the neighborhood information and
outputs the final node embedding at the current timestamp ğ‘’ğ‘–
ğ‘‡.
In order to propagate the temporal influence from the interaction
graph into the static KG, the Incremental Leveling Buffer âŸ¨4âŸ©reuses
the temporal walks in âŸ¨3âŸ©as its input. Each node in KG owns an
activity indicator to represent its temporal locality, shown as the
battery inâŸ¨4âŸ©. If an entity node arrives with any temporal walks,
its activity level increases. Contrastively, a constant decay rate isapplied to all entities to gently discharge the activity level over
time. Once this accumulated activity exceeds a certain limit, the
node is added to the incremental batch. As long as the size of the
incremental batch reaches a standard batch size, this extra batch of
KG entities is inserted into the training process and gets updated.
4.2 Periodic Focusing Windows
To precisely capture the periodical preference, we introduce the
Periodic Focusing Module âŸ¨1âŸ©. Different from current RNN-based
or attention-based approaches, we design a trainable multi-head
focusing window. By considering the node features and the current
time point, this layer can adaptively mask out the irrelevant time
periods from the complete sequence of the node history and restrict
the model to only focus on the informative time slice. Meanwhile,
the focusing mechanism optimizes the computational complexity
and improves model efficiency compared to normal RNN or self-
attention layer, proved in the efficiency evaluation section 4.6.
First of all, the Periodic Focusing Module retrieves all interac-
tions related to the input node ğ‘£ğ‘–before the current timestamp ğ‘‡,
formulated as zğ‘–
ğ‘‡={ğ‘§ğ‘–
0,ğ‘§ğ‘–
2,...,ğ‘§ğ‘–
ğ‘¡,...,ğ‘§ğ‘–
ğ‘‡|âˆ€ğ‘¡â‰¤ğ‘‡}.ğ‘§ğ‘–
ğ‘¡is the embed-
ding of each interaction between node ğ‘£ğ‘–and its neighbor node ğ‘£ğ‘—
happening at the previous timestamp ğ‘¡. Refer to TGNs [ 22], each
interactionğ‘§ğ‘–
ğ‘¡contains the following information: 1)the raw fea-
tures of the user node and the product node. User nodes can be
initialized with one-hot encoding, while the nodes and relations in
KG are pre-trained by the static distance-based methods, such as
TransE [ 1] and TransH [ 35];2)the edge feature of the interaction
graph records the user review, which can be encoded by LIWC tech-
nique [ 26];3)the time difference embedding Time(Î”ğ‘¡)is a generic
time encoding method in [ 38] to vectorize the edge timing, where
Î”ğ‘¡=ğ‘‡âˆ’ğ‘¡. Then, these components are concatenated as the embed-
dingğ‘§ğ‘–
ğ‘¡of a single temporal interaction. With the set of all historical
interactions zğ‘–
ğ‘‡, the Periodic Focusing Module starts to aggregate
the latest node-level representation ğ‘ ğ‘–
ğ‘‡, summarizing the temporal
features from the node history. Inspired by the time-series feature
extraction technique from natural language processing [ 5,24], the
self-attention mechanism is utilized as the basis of the Periodic
Focusing Module. In order to effectively capture the periodical char-
acteristics, we design a focusing window shown in Fig. 4 to prune
the sequence and narrow the range of attention allocation.
Forğ‘¡âˆˆ[0,ğ‘‡], one focusing window is formulated as:
ğ‘¤ğ›¼,ğ›½(ğ‘¡)=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³ğ‘šğ‘ğ‘¥h
ğ‘šğ‘–ğ‘›h
1âˆ’1
ğ‘€(ğ›¼âˆ’ğ›½
2âˆ’ğ‘¡),1i
,0i
,ğ‘¡<ğ›¼
ğ‘šğ‘ğ‘¥h
ğ‘šğ‘–ğ‘›h
1+1
ğ‘€(ğ›¼+ğ›½
2âˆ’ğ‘¡),1i
,0i
,ğ‘¡â‰¥ğ›¼(2)
where Softnessğ‘€is a hyperparameter to ensure a smooth transition.
And Positionğ›¼controls which period the model should focus on,
while the Spanğ›½controls the duration of this period, e.g., for sea-
sonal products such as the ski-helmet, the position of the window is
located at the winter time and the span is shorter than the common
good such as the running shoes. We equip the same mechanism
for the user nodes to capture their periodic preferences. The range
of the focusing window is ğ‘¤ğ›¼,ğ›½(ğ‘¡)âˆˆ[ 0,1], which forces the atten-
tion to become zero when the interaction token is masked out, as
computed in (5). According to different nodes at different times-
tamps, the focusing window can adaptively change the Span and
 
1682InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Figure 3: Framework of Incremental Leveling Network
Position and mask out unnecessary slices correspondingly. At the
current timestamp ğ‘‡, two parameters of the window are computed
according to the current input interaction ğ‘§ğ‘–
ğ‘‡:
ğ›¼(ğ‘§ğ‘–
ğ‘‡)=ğ‘‡Â·ğœ(ağ›¼TÂ·ğ‘§ğ‘–
ğ‘‡+ğ‘ğ›¼)
ğ›½(ğ‘§ğ‘–
ğ‘‡)=ğ‘‡Â·ğœ(ağ›½TÂ·ğ‘§ğ‘–
ğ‘‡+ğ‘ğ›½)(3)
whereğœis the sigmoid activation function. The vector ağ›¼,ağ›½and
the scalarğ‘ğ›¼,ğ‘ğ›½are learnable parameters, which are jointly opti-
mized together with the model loss function.
Afterward, the multi-head periodic-focusing layer is proposed
to allocate attention weights on each interaction. For each token
ğ‘§ğ‘–
ğ‘¡, the attention weight is computed by its similarity to the latest
interactionğ‘§ğ‘–
ğ‘‡in(4). For those interactions outside of the focusing
window, i.e., ğ‘¤ğ›¼,ğ›½(ğ‘¡)=0, they gain zero attention and are masked
out by the focusing window in (5).
SIM(ğ‘§ğ‘–
ğ‘¡)=ğ‘§ğ‘–
ğ‘‡TÂ·QTÂ·h
KÂ·ğ‘§ğ‘–
ğ‘¡+TIME(ğ‘‡âˆ’ğ‘¡)i
(4)
ATTN(ğ‘§ğ‘–
ğ‘¡)=ğ‘¤ğ›¼,ğ›½(ğ‘¡)exp(SIM(ğ‘§ğ‘–
ğ‘¡))
Ã
ğ‘§ğ‘–
ğ‘¡ğ‘›âˆˆZğ‘–
ğ‘‡ğ‘¤ğ›¼,ğ›½(ğ‘¡ğ‘›)exp(SIM(ğ‘§ğ‘–
ğ‘¡ğ‘›))(5)
where Q,K,Vare the trainable key, query and value matrices.
TIME(ğ‘‡âˆ’ğ‘¡)is utilized to vectorize the time difference, which has
similar principles to the position embedding in a common self-
attention layer [ 38]. With the modified attention weights, the mod-
ule can update its node-level representation ğ‘ ğ‘–
ğ‘‡:
ğ‘ ğ‘–
ğ‘‡=âˆ‘ï¸
ğ‘§ğ‘–
ğ‘¡âˆˆZğ‘–
ğ‘‡ATTN(ğ‘§ğ‘–
ğ‘¡)VÂ·ğ‘§ğ‘–
ğ‘¡ (6)
We employ the multi-head technique and build one-to-one cor-
respondence between each head and each focusing window, so
that the multiple windows are capable of mining the features from
Figure 4: Function of the periodic-focusing windowdifferent periods and enhancing the capture of repetitive periodic
patterns, such as seasonal products.
4.3 Adaptive Sub-graph Sampling with Biased
Temporal Walks
To capture both temporal and topological features on the graph level
and tackle the problem of exploding bi-neighborhood, we propose
the biased temporal walk. Unlike traditional random walk methods
[8,20], for each step on the temporal uneven graph, the walk should
consider both temporal interactions and knowledge relatedness.
Therefore, we design a probability distribution, which can measure
the temporal proximity and semantic relevance between the current
node and its successors, so that the biased temporal walks tend to
sample the informative neighbors instead of random selection.
The set of all accumulated temporal neighbors of node ğ‘£ğ‘–be-
fore the current timestamp ğ‘‡is notated as Nğ¼ğº
ğ‘–={ğ‘£ğ‘—|âˆ€ğ‘¡ğ‘–ğ‘—<
ğ‘‡,(ğ‘£ğ‘–,ğ‘Ÿ,ğ‘£ğ‘—,ğ‘¡ğ‘–ğ‘—)âˆˆGğ¼ğº}. Specifically, ğ‘¡ğ‘–ğ‘—here is the latest edge times-
tamp between ğ‘£ğ‘–andğ‘£ğ‘—in case multiple times of interactions exist.
The probability of choosing a specific temporal neighbor ğ‘£ğ‘—in the
interaction graph is calculated by:
PRğ¼ğº(ğ‘£ğ‘—)=exp[1/(ğ‘‡âˆ’ğ‘¡ğ‘–ğ‘—)]Ã
ğ‘£ğ‘›âˆˆNğ¼ğº
ğ‘–exp[1/(ğ‘‡âˆ’ğ‘¡ğ‘–ğ‘›)](7)
This exponential probability distribution prefers more-recent in-
teractions, which accords with the conclusion in [ 12,22], that the
most-recent sampling is more effective than the uniform sampling.
For the KG, we define the set of KG neighbors of node ğ‘£ğ‘–as
Nğ¾ğº
ğ‘–={ğ‘£ğ‘—|(ğ‘£ğ‘–,ğ‘Ÿ,ğ‘£ğ‘—)âˆˆGğ¾ğº}. In order to fully mine the semantic
relevance in KG, we use uniform sampling, i.e., each KG neighbor
shares the same probability as 1/|Nğ¾ğº
ğ‘–|. Although sampling with
temporal walks already considers the graph structure, InLN also
allows a weighted sampling on KG based on the node topological
importance [ 18]. E.g., the node with a large degree takes a higher
sampling probability. Especially, when stepping on an interlinking
product node ğ‘£ğ‘–âˆˆP, which owns both neighbors from temporal
interaction graph and KG, Nğ‘–=Nğ¼ğº
ğ‘–âˆªNğ¾ğº
ğ‘–, the probability of
choosing a specific neighbor ğ‘£ğ‘—is calculated by:
PR(ğ‘£ğ‘—)=(
ğœ‚PRğ¼ğº(ğ‘£ğ‘—)ğ‘£ğ‘—âˆˆNğ¼ğº
ğ‘–
(1âˆ’ğœ‚)/|Nğ¾ğº
ğ‘–|ğ‘£ğ‘—âˆˆNğ¾ğº
ğ‘–(8)
 
1683KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xujia Li, Jingshu Peng, and Lei Chen
whereğœ‚is a hyperparameter to weigh Gğ¾ğºandGğ¼ğº. Instead of
tuning manually, we let ğœ‚be equal to the ratio of the number of
nodes inGğ¼ğºto the number inGğ¾ğº, so that the biased temporal
walks can adapt to different graphs.
To avoid leaking the temporal information, only the edge before
the current timestamp ğ‘‡can be sampled. Based on the probability
distribution, the walks are terminated at a certain length or at the
dead end. For each node ğ‘£ğ‘–at timestamp ğ‘‡, the Sampler generates
ğ·paths to construct the adaptive sub-graph GâŸ¨ğ‘£ğ‘–,ğ‘‡âŸ©. Based on the
frequency of occurrence in all paths, this module can sample the
most active neighbors in each hop and induce the adaptive sub-
graph. The sample size at each hop is defined as the hyperparameter
ğ‘š. The effect of various sample sizes on the model performance is
empirically analyzed in the section 5.6. The algorithm procedures
of the biased temporal walks are shown in Appendix A.2.
4.4 Incremental Leveling Mechanism
To address the temporal unevenness in knowledge-aware dynamic
advertising, we propose the Incremental Leveling Buffer. The input
batch of existing temporal GNNs only contains temporal inter-
actions, which means the static KG part can not be updated in
the normal training phase. Hence, the knowledge information will
be outdated and lose its influence on the representation learning
over time. Inspired by the incremental learning to address the cata-
strophic forgetting problem in processing series data [ 30], we design
the incremental leveling mechanism to reuse the results of biased
temporal walks and create additional batches for KG updating.
After sampling sub-graph for each input node, the Sampler pack-
ages all generated paths and sends them to the Buffer. Since product
nodes can be updated by regular training batches, for all other en-
tities in KG excluding product nodes, we define the node activity
level Act(ğ‘£ğ‘–)as the total number of arrivals by all temporal walks
since its last update. Once the activity level of the node exceeds
a threshold ğœ–, this node is added to the incremental batch, and
its activity level is reset to zero correspondingly. We present the
procedure of generating incremental batches in Algorithm 1.
Once the IncrementalFlag isğ‘‡ğ‘Ÿğ‘¢ğ‘’ , the latest incremental batch
is inserted into training phase. It should be noted that the hyper-
parameterğ›¾implies the decay rate of node activity level. Together
with the activity threshold ğœ–, the generation rate of the incremental
batches can be controlled. In general, we set the decay rate ğ›¾in pos-
itive proportion to the temporal unevenness ğœŒG, while the activity
thresholdğœ–in negative proportion, so that the more unbalanced dis-
tribution of temporal information is, the more incremental batches
are generated. However, one can imagine that incremental batches
often lead to more computational cost. Thus, it is significant to bal-
ance the trade-off between the overall efficiency and effectiveness
by fine-tuning the hyperparameter ğ›¾andğœ–. In addition, to build a
consistent input format (ğ‘£ğ‘–,ğ‘Ÿ,ğ‘£ğ‘—,ğ‘‡)as the regular training batch,
for a KG node ğ‘£ğ‘–in the incremental batch, we set ğ‘‡as its activated
timestamp and the vertex ğ‘£ğ‘—as its neighbor on the temporal path
which eventually stimulates ğ‘£ğ‘–into an activated state.
4.5 Graph Aggregation and Loss Function
The GNN Module is in charge of aggregating the temporal features
and topological structure with the graph neural networks. Thanks
to the modularization of InLN, most GNN aggregation methods areAlgorithm 1 Incremental Batch Generation
Input: Temporal Walks{ğ‘ƒğ‘ğ‘¡â„}, Incremental Batch B, the set of
Node Activity Level {ğ´ğ‘ğ‘¡(ğ‘£ğ‘–)|âˆ€ğ‘£ğ‘–âˆˆEâ€²âˆ’P}
1:forğ‘ƒğ‘ğ‘¡â„ in{ğ‘ƒğ‘ğ‘¡â„}do
2:fornodeğ‘£ğ‘–inğ‘ƒğ‘ğ‘¡â„ do
3: ifğ‘£ğ‘–âˆˆEâ€²âˆ’Pthen
4: Act(ğ‘£ğ‘–)â†Act(ğ‘£ğ‘–)+1
5: ifAct(ğ‘£ğ‘–)â‰¥ğœ–then
6: Incremental Batch Bappendğ‘£ğ‘–
7: Act(ğ‘£ğ‘–)â† 0
8:end for
9:end for
10:For allğ‘£ğ‘–âˆˆEâ€²âˆ’P,Act(ğ‘£ğ‘–)â†ğ›¾Act(ğ‘£ğ‘–)
11:if|B|â‰¥ğµğ‘ğ‘¡ğ‘â„ _ğ‘ ğ‘–ğ‘§ğ‘’then
12: returnğ¼ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ğ‘™ğ¹ğ‘™ğ‘ğ‘” =ğ‘‡ğ‘Ÿğ‘¢ğ‘’
compatible with the adaptive sub-graph GâŸ¨ğ‘£ğ‘–,ğ‘‡âŸ©. In this work, we
implement the GNN Module on the top of SOTA work temporal
graph sum method in [22]:
Ëœâ„(ğ‘™)
ğ‘–=ğœ“n
[â„(ğ‘™âˆ’1)
ğ‘—âˆ¥TIME(ğ‘‡âˆ’ğ‘¡ğ‘—)]:âˆ€ğ‘£ğ‘—âˆˆGâŸ¨ğ‘£ğ‘–,ğ‘‡âŸ©o
(9)
â„(ğ‘™)
ğ‘–=ğœ™
WÂ·[â„(ğ‘™âˆ’1)
ğ‘–âˆ¥Ëœâ„(ğ‘™)
ğ‘–]
(10)
whereğœ“is the aggregate function, ğœ™is ReLu activation function
andWis the weight matrix. Initially, â„(0)
ğ‘—(ğ‘‡)=ğ‘ ğ‘—
ğ‘‡, which is the
latest node-level representation generated by the Periodic Focusing
Module. And at the output layer ğ¿, the final embedding of node ğ‘£ğ‘–
at current timestamp ğ‘‡isğ‘’ğ‘–
ğ‘‡=â„(ğ¿)
ğ‘–.
We design a compound loss function to train the complete InLN.
For each training sample (ğ‘£ğ‘–,ğ‘Ÿ,ğ‘£ğ‘—,ğ‘‡), we sample a negative node
ğ‘£ğ‘›ğ‘’ğ‘”âˆˆG, which has no edges to the node ğ‘£ğ‘–on the complete graph.
After computing the embedding ğ‘’ğ‘–
ğ‘‡,ğ‘’ğ‘—
ğ‘‡,ğ‘’ğ‘›ğ‘’ğ‘”
ğ‘‡, the interaction proba-
bility of positive pairs ğ‘ğ‘ğ‘œğ‘ betweenğ‘£ğ‘–andğ‘£ğ‘—, and the interaction
probability of negative pairs ğ‘ğ‘›ğ‘’ğ‘”betweenğ‘£ğ‘–andğ‘£ğ‘›ğ‘’ğ‘”will be com-
puted based on the cosine similarity between node embeddings.
Then, the complete loss function is formulated as:
L(ğ‘’ğ‘–
ğ‘‡,ğ‘’ğ‘—
ğ‘‡,ğ‘’ğ‘›ğ‘’ğ‘”
ğ‘‡)=MSE(ğ‘’ğ‘–
ğ‘‡,ğ‘’ğ‘—
ğ‘‡)+ğœ†1BCE(ğ‘ğ‘ğ‘œğ‘ ,ğ‘ğ‘›ğ‘’ğ‘”)+ğœ†2âˆ¥Î˜âˆ¥2
2(11)
where we apply ğ¿2regularization with ğœ†2on the trainable parame-
tersÎ˜to prevent overfitting. The Binary Cross-Entropy Loss (BCE)
is to distinguish positive and negative samples, which distances
irrelevant nodes. The Mean-Squared Error Loss (MSE) is to improve
the similarity of the related user and product nodes. The hyper-
parameterğœ†1scales both losses to a similar range. A decline in
performance is observed when only one loss is used, indicating that
improving the similarity of positive pairs (MSE) and creating larger
distances between unrelated nodes (BCE) are both important.
4.6 Model Efficiency Analysis
Except the graph aggregation, the computational cost of InLN is
mainly caused by the periodic-focusing window and sampling tem-
poral walks. First, the computational cost of the periodic-focusing
layer isğ‘‚(ğ›½2Â·DIM(ğ‘§ğ‘–
ğ‘¡)). Itâ€™s related to the span of window ğ›½and
the embedding size DIM(ğ‘§ğ‘–
ğ‘¡). This cost is smaller than a normal
self-attention layer with complexity of ğ‘‚(L2Â·DIM(ğ‘§ğ‘–
ğ‘¡)), where the
 
1684InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
length of input sequence Lis larger than ğ›½due to the pruning
effect of the focusing window. The efficiency of this time-series
embedding in InLN is also better than the SOTA temporal GNNs
[22,23], which utilizes recurrent neural network with the layer
complexity of ğ‘‚(LÂ·DIM(ğ‘§ğ‘–
ğ‘¡)2). Second, the procedures of sampling
temporal walks can be processed in data preprocessing or in par-
allel with other calculations. All temporal walks are cached and
reused in the Incremental Leveling Buffer. Additionally, the number
of incremental batches can be controlled by the decay rate and the
activity threshold to balance the optimization between effectiveness
and efficiency. Therefore, InLN shows fair scalability and efficiency
on a larger temporal uneven graph instead of a pure interaction
graph or a static one.
5 Experiments
In this section, we exhibit the model overall performance, ablation
studies, efficiency tests, sensitivity analysis, and two real cases.
5.1 Experimental Settings
5.1.1 Dataset. We select three most-used benchmarks in different
product categories with totally different purchase patterns: Book,
Beauty and Music [ 12,31,33], showing various graph properties
in the average node degree, number of interactions, number of
knowledge triples, density and temporal unevenness, shown in
Table 2. These diversities help show the generalization ability of
InLN. The Beauty and Book datasets are collected from Amazon
E-commerce Platform [ 10], which contain the usersâ€™ reviews and
ratings towards products. And Last-FM, the music dataset, records
the temporal interactions between songs and listeners, which is
widely used in dynamic preference modeling tasks. The dataset
dividing and KG construction are kept consistent with all previous
works [22, 31, 38], which are stated in the Appendix A.4.
Table 2: Statistics of datasets
Dataset StatisticsAmazon
BeautyAmazon
BookLast-FM
Music
# Users 22,363 21,577 992
# Products 12,101 24,881 1,000
# Interactions 149,844 540,221 1,293,103Gğ¼ğº
Aver. Degree 8.70 23.26 1298.30
# Entities 59,353 88,572 58,266
# Relations 28 39 9
# Triplets 1,915,657 2,557,746 464,567Gğ¾ğº
Aver. Degree 64.56 57.76 15.94
5.1.2 Metrics. We perform three tasks of temporal advertising to
evaluate the models. For the future link prediction, i.e., to distin-
guish positive and negative pairs at a given timestamp, we employ
Average Precision (%) as previous works [ 22,38] for both transduc-
tive and inductive tasks. The transductive task is conducted for the
nodes that have been observed in the training phase. The inductive
task evaluates model performance when encountering new nodes.
For the top@K recommendation task, we set ğ¾=10as previous
work [ 7,12] and utilize mean reciprocal rank (MRR) to test whether
the positive pair is ranked in the top@K among all products.5.1.3 Baselines and Implementation. We compare InLN with nine
SOTA baselines. CTDNE, JODIE, TGAT, TGN, DGRS are the base-
lines of dynamic models, which utilize various methods to gather
information from continuous dynamic graphs without KG. While,
GraphSage, KGAT, KGIN, MetaKG are the baselines focusing on a
static graph. The details of baselines are shown in Appendix A.3.
The implementation details are also listed in Appendix A.4.
5.2 Overall Performance Comparison
The comparison of overall performance is shown in Table 3. All the
results are averaged over 10 runs for the significant test. For the
three tasks, in all three benchmarks, InLN consistently improves
the advertising performance by different margins (up to 9.9% and
5.7% on average). The results show that InLN can bridge the gap
between static knowledge and dynamic interactions, while other
SOTA models are incapable of processing them simultaneously.
We conclude Table 3 from three aspects as follows:
1)From the aspect of extracting temporal information, InLN
and other dynamic baselines generally outperform the static KG-
enhanced methods. This observation verifies the importance of
temporal factors in computational advertising. InLN achieves rela-
tively high performance gain of InLN in the Music dataset, because
both the Periodic Focusing Module and the temporal walks prefer
ample history behaviors. As shown in Table 2, compared to the av-
erage user interaction times in the Book domain (i.e., 23.26 per user),
the Music domain has a high interaction frequency at 1298.3. The
scale of the interaction graph in Last-FM is much larger than the KG,
which means the most influential information to represent node
characteristics lies in the large amount of temporal interactions.
2)From the aspect of utilizing KG, in the Beauty and Book
datasets, the static KG-enhanced methods maintain an ordinary
performance even without any temporal information. These two
domains have abundant relatedness information between products
in KG, where the number of knowledge triples is about 5 times more
than the number in the Music dataset. This relational information
from KG can better represent the user preference behind the prod-
uct itself, which accords with the conclusions of previous works
[29,31,33]. However, the general performance of the static base-
lines is inferior, which is highly related to the ignoring of temporal
features and the over-smoothing issue of GNNs when bringing in a
huge KG without any effective pruning mechanisms [44].
3)From the aspect of different tasks, InLN is more effective
in the transductive task of the link prediction than the inductive
one, because the periodic-focusing layer with multiple windows
increases the model complexity with more hidden parameters to
enhance the model expressiveness. Hence, the model needs more
input data for a better prediction when new nodes come in.
5.3 Ablation Study on Model Performance
Because of the modular design of InLN, we can remove each module
and build three ablation models to compare their influence, called
InLN-rmF (remove Periodic Focusing), InLN-rmA (remove Adaptive
Sampler), InLN-rmI (remove Incremental Leveling Buffer). Their
performances compared to the full InLN framework are shown in
Table 4. In general, the drop in performance clearly reflects the
effectiveness of each module. The drop without Periodic Focusing
 
1685KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xujia Li, Jingshu Peng, and Lei Chen
Table 3: Overall performance comparison
Dataset Beauty Book Music
Metrices MRR AP-Trans. (%) AP-Induc. (%) MRR AP-Trans. (%) AP-Induc. (%) MRR AP-Trans. (%) AP-Induc. (%)
GraphSage 0.0183 Â±1.4e-4 84.2 Â±0.5 79.8 Â±0.6 0.0129 Â±1.2e-4 82.1 Â±0.7 80.3 Â±0.7 0.070 Â±1.9e-3 65.0 Â±1.6 62.4 Â±1.8
KGAT 0.0201 Â±3.3e-4 82.3 Â±0.4 - 0.0133 Â±0.4e-4 86.5 Â±0.1 - 0.084 Â±0.2e-3 67.5 Â±0.2 -
KGIN 0.0212 Â±4.9e-4 85.0 Â±0.2 - 0.0156 Â±0.5e-4 88.5 Â±0.3 - 0.083 Â±0.7e-3 63.5 Â±0.5 -
MetaKG 0.0197 Â±7.4e-4 82.5 Â±2.0 81.0 Â±2.6 0.0148 Â±3.2e-4 86.1 Â±2.7 82.9 Â±1.7 0.103 Â±3.9e-3 70.0 Â±3.6 68.8 Â±2.8
CTDNE 0.0094 Â±2.5e-4 70.7 Â±2.0 - 0.0081 Â±3.8e-4 58.2 Â±2.9 - 0.012 Â±0.5e-3 62.8 Â±2.8 -
TGAT 0.0242 Â±2.7e-4 84.9 Â±0.9 83.3 Â±0.7 0.0155 Â±0.5e-4 90.2 Â±0.2 86.2 Â±0.8 0.143 Â±1.4e-3 72.9 Â±0.7 71.1 Â±0.7
JODIE 0.0308 Â±6.0e-4 77.2 Â±1.6 72.1 Â±2.0 0.0207 Â±3.2e-4 73.0 Â±1.2 71.9 Â±1.8 0.192 Â±2.5e-3 70.1 Â±1.2 68.4 Â±2.7
TGNs 0.0288 Â±3.1e-4 86.0 Â±0.3 84.2 Â±0.8 0.0167 Â±0.2e-4 91.6 Â±0.2 87.5 Â±0.5 0.189 Â±0.5e-3 71.5 Â±0.2 73.6 Â±0.2
DGRS 0.0267 Â±0.1e-4 86.2 Â±0.0 84.8 Â±0.1 0.0151 Â±0.0e-4 88.1 Â±0.2 85.0 Â±0.1 0.169 Â±0.5e-3 73.1 Â±0.0 72.3 Â±0.2
InLN (Ours) 0.0326 Â±1.7e-4 93.6 Â±0.5 89.0 Â±1.0 0.0210 Â±0.6e-4 94.5 Â±0.3 90.1 Â±0.3 0.205 Â±1.0e-3 80.3 Â±0.4 79.1 Â±0.3
Improv. â†‘5.84% â†‘8.58% â†‘4.95% â†‘1.45% â†‘3.17% â†‘2.97% â†‘6.77% â†‘9.85% â†‘7.47%
Module or Adaptive Sub-graph Sampler indicates that, the tempo-
ral information on both node-level and graph-level is significant
for the advertising quality. The incremental updating of KG ev-
idently improves the performance in Beauty and Book datasets,
which depend more on the KG corpus as discussed in section 5.2.
In addition, we separately replace the focusing window with a
normal self-attention layer and find that the pruning effect of the
focusing window is notable in Last-FM. The performance difference
reaches 7.5%. The reason is that there is a much longer interaction
sequence of each user in Last-FM, with an average length of 1298.3.
Thus, plenty of periodical characteristics are available for focusing
windows to capture. We measure the length of focusing windows
according to the relative span length, defined as the average length
of the window span divided by the average graph degree, which
is 0.40, 0.18, and 0.04 for Beauty, Book, and Music. Notably, with
similar graph properties, the span length of Beauty is larger than
that of Book. This could be attributed to the more pronounced
long-term preference for beauty products than books. Due to the
smaller number of transaction records for each user in Beauty, a
larger span can capture user preferences more effectively.
Table 4: Performance comparison on ablation models
Dataset Amazon-Beauty Amazon-Book Last-FM
Metrices MRR AP-Trans. MRR AP-Trans. MRR AP-Trans.Aver.
Drop
InLN 0.0326 93.6 0.0210 94.5 0.205 80.3 -
InLN-rmF 0.0299 89.8 0.0200 93.2 0.177 74.8 6.5%
InLN-rmA 0.0260 85.0 0.0185 91.0 0.183 77.0 10.0%
InLN-rmI 0.0286 88.1 0.0181 89.1 0.200 78.9 7.0%
5.4 Model Efficiency Evaluation
Here we show the evaluation results of InLN efficiency against
two dynamic models in Table 5. Compared with Jodie [ 12], which
utilizes the coupled RNN only on time-series data, the temporal
GNNs achieve a shorter total training time because of the smaller
computation complexity. Although InLN spends more time in each
epoch because of the extra batches for incremental learning, InLNTable 5: Evaluation of Model Efficiency
Evaluation of Efficiency of Dynamic Models
on Beauty DatasetJodie
(w/o KG)TGNs
(w/o KG)InLN
with KG
Training PhaseNumber of epochs until convergence 22.0 13.2 11.6
Time cost per epoch 526.29 s 618.49 s 819.27 s
Total time cost until convergence 3.22 h 2.23 h 2.63 h
Inference Phase Time cost for all testing samples 130.6 s 257.20 s 205.71 s
shows its strength in the time cost for the inference phase, which is
more critical in the real-time application on E-commerce platforms.
To verify model scalability, after merging two Amazon datasets
into a single multi-category dataset, despite doubling the size of
the dataset, InLN maintains SOTA recommendation performance,
with an MRR value of 0.025, while the second-best performance is
0.020. The first reason lies in the adaptive sampling mechanism in
InLN, which ensures the selection of the most informative neigh-
boring nodes at the current time point, even as the graph scales
up. This control over the aggregation range of GNNs prevents
over-smoothing of the embeddings due to increased degrees and
neighboring nodes. Additionally, the periodic focusing windows
allow for the pruning of the userâ€™s historical behavior sequence,
ensuring an optimal selection of a limited number of interactions
along the timeline for updating the node-level representation.
5.5 Cases Study
Shown in Fig. 5, we demonstrate two concrete cases to show the
rationale of each advertising as previous works did [ 7,31,33]. In
case 1, we extract the path between the user and the target song
as the explanation in the Last-FM dataset, which clearly illustrates
the relation including userâ€™s history behavior and KG triples. In
addition, the embedding distance between the user and each node
shows that InLN can discover the user preference located at the KG
entity â€“ the band ğ‘„ğ‘¢ğ‘’ğ‘’ğ‘› rather than any specific songs. The song
that the user truly listened to at the current timestamp (the ground
truth of testing data) is ranked at 7 among all 1000 candidates by
InLN. In case 2, we list the five top-recommended products of the
 
1686InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
same user at two different timestamps in a time interval of around
6 months. The product with a red frame is the ground truth that
the user purchased at the corresponding timestamp. The distances
between the same user-product pair but at two periods are very
different, which implies that the dynamical embedding using InLN
can distinguish user preference based on time information. Notably,
ğ¼ğ‘¡ğ‘’ğ‘š 10443 owns a relatively stable embedding compared to others.
In fact, the user purchased this face wash 12 times, which indicates
the long-term preference or the daily property of this product.
Another case shows the filtering effect of irrelevant information
by the adaptive span: we observed a Sunscreen product in the
testing set. When the testing timestamp is in 2013.04, two focusing
windows are located at 2011.03-2011.08 and 2012.03-2012.07, which
indicates that the adaptive span tends to choose periods closer to
the testing timestamp and filters out the autumn and winter months,
which is less informative for this sunscreen product.
Figure 5: Explanations of periodical preference and real cases
5.6 Sensitivity Analysis
We plot the training curves of different sub-graph sampling strate-
gies in Fig. 6. Two phenomenons are concluded: Firstly, the most-
recent sampling and the adaptive sampling with biased temporal
walk converge much faster than the uniform one, which evidences
that the time information helps to generate more accurate node
embeddings [ 22], especially in Last-FM, sampling uniformly from
large amounts of temporal neighbors makes little contribution to
the model performance. Secondly, the training curves reflect the
dependence of Beauty and Book on KG information, where adap-
tive sampling reaches a lower optimal loss than the most-recent
sampling, since the most-recent one only considers the interaction
graph, while the biased temporal walks can incorporate both the
most-recent user behaviors and the active entities in KG. However,
InLN needs more training data to digest the information from KG,
which slightly limits its convergence speed compared to the most-
recent sampling. In the lower part of Fig. 6, we plot the effect of
varying sample sizes with different strategies. Adaptive sampling
achieves the best performance, which can be seen as combining the
advantage of most-recent strategy in capturing temporal features
and the advantage of uniform sampling in discovering KG.
We also conduct sensitivity analysis on the activity threshold ğœ–,
which is the core hyper-parameter of Incremental Leveling Buffer
and controls the updating frequency for the KG nodes. As shown
in Table 6, lower thresholds with more incremental batches for
updating KG nodes generally improve the model performance, at
Figure 6: Comparison between the sampling strategies
the price of more training time. This correlation also verifies the
necessity of updating KG with dynamic information.
Table 6: Average precision with different activity thresholds
DatasetThreshold
40 60 80 100 120
Amazon-Beauty 93.5 93.6 92.5 90.7 88.4
Amazon-Book 94.5 94.5 94.2 90.0 89.3
Last-FM 80.0 80.0 80.3 79.6 79.2
6 Conclusion
In this work, we investigate the phenomenon of temporal uneven-
ness wildly existing in dynamic advertising. Then, we propose the
InLN model and appeal the possibility of GNNs to better process
time-series data and topological graph structure in a unified frame-
work. According to comprehensive experiments, InLN outperforms
the baselines in various tasks and each module is effective in line
with our original design intention. In future work, we will apply
this framework to various scenarios. E.g., in the knowledge-aware
stock market prediction.
Acknowledgement
Lei Chenâ€™s work is partially supported by National Key Research
and Development Program of China Grant No. 2023YFF0725100,
National Science Foundation of China (NSFC) under Grant No.
U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project
R6020-19, AOE Project AoE/E-603/18, Theme-based project TRS
T41-603/20R, CRF Project C2004-21G, Guangdong Province Science
and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF
grants MHX/078/21 and PRP/004/22FX, Zhujiang scholar program
2021JC02X170, Microsoft Research Asia Collaborative Research
Grant and HKUST-Webank joint research lab grants.
 
1687KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xujia Li, Jingshu Peng, and Lei Chen
References
[1]Antoine Bordes, Nicolas Usunier, Alberto Garcia-DurÃ¡n, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-Relational
Data. In NIPS.
[2]Chong Chen, Min Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2020. Jointly
non-sampling learning for knowledge graph enhanced recommendation. In Pro-
ceedings of the 43rd International ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval. 189â€“198.
[3]Justin Cheng, Caroline Lo, and Jure Leskovec. 2017. Predicting intent using
activity logs: How goal specificity and temporal range affect user behavior. The
Web Conference (2017).
[4]Janneth Chicaiza and Priscila Valdiviezo-Diaz. 2021. A comprehensive survey of
knowledge graph-based recommender systems: Technologies, development, and
contributions. Information (2021).
[5]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan
Salakhutdinov. 2020. Transformer-XL: Attentive language models beyond a
fixed-length context. ACL (2020).
[6]Ronky Francis Doh, Conghua Zhou, John Kingsley Arthur, Isaac Tawiah, and
Benjamin Doh. 2022. A Systematic Review of Deep Knowledge Graph-Based
Recommender Systems, with Focus on Explainable Embeddings. Data (2022).
[7]Yuntao Du, Xinjun Zhu, Lu Chen, Ziquan Fang, and Yunjun Gao. 2022. MetaKG:
Meta-learning on Knowledge Graph for Cold-start Recommendation. TKDE
(2022).
[8]Francois Fouss, Alain Pirotte, Jean-michel Renders, and Marco Saerens. 2007.
Random-Walk Computation of Similarities between Nodes of a Graph with
Application to Collaborative Recommendation. TKDE (2007).
[9]William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. NIPS (2017).
[10] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evo-
lution of fashion trends with one-class collaborative filtering. the Web Conference
(2016).
[11] Yugang Ji, MingYang Yin, Yuan Fang, Hongxia Yang, Xiangwei Wang, Tianrui
Jia, and Chuan Shi. 2020. Temporal heterogeneous interaction graph embedding
for next-item recommendation. In ECML PKDD.
[12] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-
bedding trajectory in temporal interaction networks. In SIGKDD.
[13] Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang
Chen, and Tat Seng Chua. 2020. Interactive Path Reasoning on Graph for Con-
versational Recommendation. SIGKDD (2020).
[14] Chen Li, Linmei Hu, Chuan Shi, Guojie Song, and Yuanfu Lu. 2021. Sequence-
aware heterogeneous graph neural collaborative filtering. In SDM.
[15] Jiawei Liu, Chuan Shi, Cheng Yang, Zhiyuan Lu, and S Yu Philip. 2022. A survey
on heterogeneous information network based recommender systems: Concepts,
methods, applications and resources. AI Open 3 (2022), 40â€“57.
[16] Jiawei Liu, Chuan Shi, Cheng Yang, Zhiyuan Lu, and Philip S. Yu. 2022. A survey
on heterogeneous information network based recommender systems: Concepts,
methods, applications and resources. AI Open (2022).
[17] Xin Liu. 2015. Modeling usersâ€™ dynamic preference for personalized recommen-
dation. In IJCAI.
[18] Yao Ma, Suhang Wang, and Jiliang Tang. 2017. Network Embedding with Cen-
trality Information. In ICDM.
[19] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
estimation of word representations in vector space. ICLR (2013), 1â€“12.
[20] Giang Hoang Nguyen, John Boaz Lee, Ryan A. Rossi, Nesreen K. Ahmed, Eunyee
Koh, and Sungchul Kim. 2018. Continuous-Time Dynamic Network Embeddings.
The Web Conference (2018).
[21] Ciyuan Peng, Feng Xia, Mehdi Naseriparsa, and Francesco Osborne. 2023. Knowl-
edge graphs: Opportunities and challenges. Artificial Intelligence Review (2023),
1â€“32.
[22] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal Graph Networks for Deep Learning
on Dynamic Graphs. In ICML.
[23] Joakim Skarding, Bogdan Gabrys, and Katarzyna Musial. 2021. Foundations
and Modeling of Dynamic Networks Using Dynamic Graph Neural Networks: A
Survey. IEEE Access (2021).
[24] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
2020. Adaptive attention span in transformers. ACL (2020).
[25] Hao Sun, Zijian Wu, Yue Cui, Liwei Deng, Yan Zhao, and Kai Zheng. 2021. Per-
sonalized dynamic knowledge-aware recommendation with hybrid explanations.
InDatabase Systems for Advanced Applications: 26th International Conference,
DASFAA 2021, Taipei, Taiwan, April 11â€“14, 2021, Proceedings, Part III 26. Springer,
148â€“164.
[26] Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of
words: LIWC and computerized text analysis. Journal of language and social
psychology (2010).
[27] John Wanamaker. 1911. Golden Book of the Wanamaker Stores. Jubilee year,
1861-1911.[28] Hongwei Wang, Jure Leskovec, Fuzheng Zhang, Miao Zhao, Wenjie Li, Mengdi
Zhang, and Zhongyuan Wang. 2019. Knowledge-aware Graph Neural Networks
with Label Smoothness Regularization for Recommender Systems. SIGKDD
(2019), 968â€“977.
[29] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge
graph convolutional networks for recommender systems. The Web Conference
(2019).
[30] Junshan Wang, Guojie Song, Yi Wu, and Liang Wang. 2020. Streaming Graph
Neural Networks via Continual Learning. CIKM (2020).
[31] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:
Knowledge graph attention network for recommendation. In SIGKDD.
[32] Xiang Wang, Xiangnan He, and Tat-Seng Chua. 2020. Learning and reasoning
on graph for recommendation. In WSDM.
[33] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,
Xiangnan He, and Tat Seng Chua. 2021. Learning intents behind interactions with
knowledge graph for recommendation.
[34] Yu Wang, Zhiwei Liu, Ziwei Fan, Lichao Sun, and Philip S. Yu. 2021. DSKReG: Dif-
ferentiable Sampling on Knowledge Graph for Recommendation with Relational
GNN. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management (CIKM â€™21). 3513â€“3517.
[35] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge
Graph Embedding by Translating on Hyperplanes. In AAAI.
[36] Chao Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J. Smola, and How Jing.
2017. Recurrent recommender networks. WSDM (2017).
[37] Yikun Xian, Zuohui Fu, S. Muthukrishnan, Gerard De Melo, and Yongfeng Zhang.
2019. Reinforcement knowledge graph reasoning for explainable recommenda-
tion. SIGIR (2019), 285â€“294.
[38] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
2020. Inductive representation learning on temporal graphs. In ICLR.
[39] Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial temporal graph convolu-
tional networks for skeleton-based action recognition. In AAAI.
[40] Mengqiu Yao, Liqiang Song, Ye Bi, Wei Wang, Kun Deng, Jianming Wang, Jing
Xiao, Xiaoyun Lin, and Zhaojun Gui. 2021. ADKGN: An Attentive Dynamic
Knowledge Graph Network for Sequential Recommendation. In IJCNN.
[41] Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, and Chen Chen. 2012. Challenging the
long tail recommendation. VLDB (2012).
[42] Mengqi Zhang, Shu Wu, Xueli Yu, Qiang Liu, and Liang Wang. 2022. Dynamic
graph neural networks for sequential recommendation. TKDE (2022).
[43] Yongfeng Zhang and Xu Chen. 2020. Explainable recommendation: A survey
and new perspectives. Foundations and Trends in Information Retrieval (2020).
[44] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI Open (2020).
A Appendix
A.1 Notation Summary
Here we summarize the major notation in this paper.
A.2 Algorithm of Biased Temporal Walk
The procedures of the biased temporal walk for an input node ğ‘£ğ‘–
are shown in the following algorithm:
Algorithm 2 Biased Temporal Walk
Require: GraphG, Nodeğ‘£ğ‘–, Timestamp ğ‘‡, Path Length ğ‘‘
1:Initialize temporal walk ğ‘ƒğ‘ğ‘¡â„ =[ğ‘£ğ‘–]
2:for1toğ‘‘do
3: Retrieve neighborhood Nğ‘–of current node ğ‘£ğ‘–:
4:if|Nğ‘–|>0then
5:âˆ€ğ‘£ğ‘—âˆˆNğ‘–, calculate the probability PR(ğ‘£ğ‘—)
6: Sample a node ğ‘£ğ‘—based on probability distribution
7:ğ‘ƒğ‘ğ‘¡â„ append node ğ‘£ğ‘—
8:ğ‘£ğ‘–â†ğ‘£ğ‘—
9:else break
10:returnğ‘ƒğ‘ğ‘¡â„
 
1688InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 7: Major notations of InLN
Notation Description
G T
emporal uneven graph
Gğ¼
ğº User-pr
oduct interaction graph
Gğ¾
ğº E-commer
ce knowledge graph
G
âŸ¨ğ‘£ğ‘–,ğ‘‡âŸ©The adaptive sub-graph surrounding node ğ‘£ğ‘–at the current times-
tampğ‘‡
ğ‘£ğ‘–,
ğ‘£ğ‘— No
des including user, product and KG entities
ğ‘Ÿ Edges
including user-product interactions and KG relations
ğ‘¡ The
timestamp for a user-item interaction
ğ‘‡ The
current timestamp for a training or testing sample
ğ‘¡ğ‘–
ğ‘— The latest interaction time between user ğ‘£ğ‘–and product ğ‘£ğ‘—beforeğ‘‡
if multiple historical interactions exist
ğœŒG T
emporal unevenness of a graph G
ğ‘ğ‘– The
total number of interactions of the node ğ‘£ğ‘–
ğ‘§ğ‘–
ğ‘¡ The raw embedding of each interaction between node ğ‘£ğ‘–and its
neighbor node ğ‘£ğ‘—happening at the previous timestamp ğ‘¡
zğ‘–
ğ‘‡The set of historical interactions related to the node ğ‘£ğ‘–before the
current timestamp ğ‘‡
ğ‘ ğ‘–
ğ‘‡The node-level representation of node ğ‘£ğ‘–at timestamp ğ‘‡generated
by the Periodic Focusing Windows
Ëœâ„(ğ‘™)
ğ‘–The hidden embedding of node ğ‘£ğ‘–at theğ‘™level during the GNN
aggregation
ğ‘’ğ‘–
ğ‘‡The
final embedding of node ğ‘£ğ‘–at current timestamp ğ‘‡
ğ›¼ The
position of a focusing window
ğ›½ The
span of a focusing window
ğ‘ğ¼
ğº
ğ‘–The set of all accumulated temporal neighbors of node ğ‘£ğ‘–before the
current timestamp ğ‘‡
ğ‘ğ¾
ğº
ğ‘–The
set of KG neighbors of node ğ‘£ğ‘–
ğ· The
number of the path sampled by temporal random walk
ğ‘‘ The
length of each path
A
ct(ğ‘£ğ‘–)The node activity level defined by the total number of arrivals by
all temporal walks since its last update
ğœ– The
activity threshold
ğ›¾ The
decay rate of node activity level
A.3 Baselines
We compare InLN with a total of nine SOTA baselines. CTDNE,
JODIE, TGAT, TGN, DGRS are the baselines of dynamic models.
While, GraphSage, KGAT, KGIN, MetaKG are the baselines focusing
on a static graph:
â€¢KGAT [ 31] firstly constructs a collaborative knowledge graph
including static user-item interactions and introduces graph at-
tention technique to generate the path between user and product
as a post-hoc explanation.
â€¢KGIN [ 33] is the SOTA static KG-enhanced model focusing on
advertising with user purchase intents. With this baseline, it is
expected to investigate the influence of temporal information on
the KG-enhanced advertising.
â€¢MetaKG [ 7] introduces meta-learning into KG-enhanced rec-
ommender system, which designs a collaborative-aware and a
knowledge-aware meta learner to address the cold-start problem.
â€¢GraphSage [ 9] propose a random sampling strategy with GNN
to collect neighborhood features. Its performance is especially
remarkable for the unseen nodes. Hence, it functions as the
baseline for the inductive task.â€¢CTDNE [ 20] utilizes random walks to directly collect corpus
from temporal neighbors, and passes the corpus to the Skip-
Gram encoder [19] to generate dynamic embedding.
â€¢JODIE [ 12] introduces a coupled RNN to predict the future edges
between users and items by modeling the trajectory of the node
embedding over time. JODIE is the baseline only considering the
temporal information without KG.
â€¢TGAT [ 38] designs a time encoding function to add timestamp in-
formation into the node representation, so that GNN can measure
the temporal similarity between nodes and perform weighted
aggregation from the nodeâ€™s neighborhood. However, TGAT
ignores the history of each node, i.e. the node-level temporal
information.
â€¢TGN [ 22] proposes a RNN-based memory module to capture
node-wise temporal features and well combines the memory
with graph-wise GNN to achieve strong performance on contin-
uous dynamic graphs. TGN represents the SOTA temporal GNN
baselines, where KG is still out of scope.
â€¢DGSR [ 42] explores a dynamic graph consisting of interaction
sequences and achieves SOTA performance in the sequential
recommendation. However, the black-box embedding pipeline
makes the model and internal sampling methods totally uninter-
pretable.
In addition, the static baselines do not consider the temporal
information on interaction edges. The dynamic baselines also utilize
the pre-trained product embeddings, but all static KG triples are
ignored, because their methods can only process temporal edges
during training phase.
A.4 Implementation Details
A.4.1 Data Prepossessing. To build the corresponding KGs, we
project the product nodes to the Freebase entities as the way in
[31] and keep two-hop neighbors. To ensure the dataset quality, we
apply the convention in other KG-enhanced advertising methods
[29,37] by filtering out the nodes, whose number of relations is
less than 10 times. The details of the datasets are shown in Table 2.
Keep consistent with all previous works [ 22,31,38], the datasets
are divided as into training, validation and testing parts with the
ratio of 70%, 15%, 15%.
A.4.2 Model Implementation. The hyperparameters of baselines
are set referring to original papers and optimized with the grid
search and the same termination condition is applied to all base-
lines. The architecture details of InLN are listed here for easier
reproduction of the results in Table 3. For a fair comparison, the
batch size is set to 400. The embedding size of node feature, edge
feature and timestamp are equally fixed to 128for all baselines.
All hyper-parameters are optimized with the grid search: a single
periodic-focusing layer with 512hidden size and 2attention heads
is applied after tuning. The softness of focusing windows ğ‘€is tuned
to0for Beauty and Book and 10for Last-FM. About the optimized
parameters of random walk, the path length ğ‘‘is5and total number
of pathsğ·is15. And the size of adaptive sampling is 10,10,20for
Beauty, Book and Last-FM respectively. In the Incremental Leveling
Buffer, the activity threshold ğœ–is tuned to(100,80,50)for each
dataset respectively. And the decay rate ğ›¾is tuned in the range of
 
1689KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xujia Li, Jingshu Peng, and Lei Chen
[0.8,1.0]. The setting of GNN aggregation is followed by the previ-
ous work [ 22]. The weight ğœ†1on BCE loss is 10âˆ’2andğœ†2=0.01for
the regularization. For the training process, the dropout is set to 0.2
and the learning rate is searched in {10âˆ’3,10âˆ’4,10âˆ’5}. The training
will be terminated if no improvement is achieved in 5epochs or
at the final 30ğ‘¡â„epoch. This termination condition is also applied
to all baselines. For the convenience of model evaluation, we store
each version of KG embeddings and the corresponding timestamp
after every incremental batch during the training phase. Then, to
compute the user and product embeddings in the testing set, the
most-recent version of KG embeddings can be directly retrieved
according to the timestamp of this testing sample. The experiments
run on the GPU GeForce RTX 2080 Ti 8 and use Pytorch 1.4.A.5 Visualization of Leveling Effect with InLN
The updating frequency of InLN without Incremental Leveling
Mechanism is shown in the left figure. The one with full packages
of InLN is shown in the right figure, where the side KG nodes get a
considerable number of updates with the time evolving.
Figure 7: Visualization of Leveling Effect with InLN
 
1690