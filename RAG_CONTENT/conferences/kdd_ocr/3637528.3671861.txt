Hierarchical Linear Symbolized Tree-Structured Neural Processes
Jin yang Taiâˆ—
School of Computer Engineering and Science
Shanghai University
Shanghai, China
1203146411@shu.edu.cnYi ke Guo
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong, China
yikeguo@ust.hk
Abstract
Traditional Neural Processes (NPs) and their variants aim to learn
relationshipsbetweencontextsamplepointsbutdonotconsider
multi-level information, resulting in a limited ability to learn com-
plex distributions.This paper draws inspiration from features such
as the hierarchical nature and interpretability of tree-like struc-
tures. This paper proposes a Hierarchical Linear Symbolized Tree-
structured Neural Processes (HLNPs) architecture. This framework
utilizes variables to build a top-down hierarchical linear symbol-
ized tree-structured network architecture, enhancing positional
representationinformationinahierarchicalmanneralongthede-
terministicpath.Inthelatentdistribution,thehierarchicallinear
symbolized tree-structured network approximates functions dis-cretely through a layered approach. By decomposing the latent
complex distribution into several simpler sub-problems using sum
andproductsymbols,theupperboundofoptimizationisthereby
increased. The tree structure discretizes variables to capture model
uncertainty inthe formof entropy.This approach alsoimparts a
causaleffecttotheHLNPsmodel.Finally,wedemonstratetheeffec-
tiveness of the HLNPsmodels for 1D data, Bayesian optimization,
and 2D data.
CCS Concepts
â€¢Computing methodologies â†’Neural networks.
Keywords
Neural Processes,Bayesian Network,Hierarchical Linear Symbol-
ized Tree-structured
ACM Reference Format:
Jin yang Tai and Yi ke Guo. 2024. Hierarchical Linear Symbolized Tree-
StructuredNeuralProcesses.In Proceedingsofthe30thACMSIGKDDCon-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671861
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718611 Introduction
Thelimitationofneuralnetworksisthattheycanonlylearnspe-
cificfixedmappingfunctions[ 10],whilemeta-learningcanacquire
arange offunctiondistributions,enabling ittopossessthe ability
to quickly adapt to new tasks and enhance computational effi-
ciency.NeuralProcesses(NPs)[ 11]representanimplementation
methodology within the realm of meta-learning [ 23]. It constructs
implicitprobabilisticfunctionmodelsbasedonobservedcontext
samplepointstopredictthefunctionvaluesoftargetsamplepoints.
Thisformofimplicitprobabilityfunctionintegratestheinherent
flexibility of neural networks with the uncertainty inherent in sto-
chasticprocesses,therebyfacilitatingtheacquisitionofmapping
relationships between input and output data. The above imple-
mentation stands apart from conventional approaches, such asGaussian Processes (GPs) [
4], where a predefined mathematical
modelisemployed,andmodelparametersaresubsequentlyfine-
tuned to align with the data distribution. The fitting results of this
approach are overly reliant on the selection of the kernel function
[18]. In contrast, NPs usually operate with minimal inductive bias
andaredrivenbydata,enablingthemtoautonomouslyshapetheir
structure based on the inherent characteristics of the data.
The design goal of the NPs model is to fit context sample points
withimplicitfunctionsandcaptureuncertainty.TheNPs[ 11]im-
plementationutilizes fixed-dimensionallatentvariables toencode
contextsamplepoints,therebyenhancingtheexpressivecapacity
for implicit variable functions. However, this design is prone to theproblem of underfitting the context sample points [
16]. Integrating
attention mechanisms addresses this concern by assigning varying
weightstocontextsamplepoints.Nevertheless,whendealingwitha
highlyintricateoriginaldistributionofinputcontextsamplepoints,
the model encounters a significant increase incomputational cost
andtime[ 12].Additionally,inthecontextofNPsmodelsdealing
with complex distributions, there is a substantial gap between the
Gaussian posterior approximation using Variational Inference (VI)
and the true distribution. This form of approximation introduces a
significantbiasinrepresentingtheuncertaintyofthelatentfunc-
tion. The introduction of importance-weighted methods reduces
thisdisparity,resultinginatighterlowerboundfortheEvidence
Lower Bound (ELBO) [ 30]. It is worth exploring the redesign of an
effective NPs model that not only incorporates sufficient hierarchi-
cal information when dealing with context sample points but also
provides amorereasonableapproximationto theGaussian poste-
riordistribution.Thiswouldenableittobetteradapttohandling
complex distributions.
Inthispaper,weproposeaHierarchicalLinearSymbolizedTree-
structuredNeuralProcesses.Weintroduceatop-downtreestruc-
turetofacilitatethelearningofinformationatdifferenthierarchical
levels. When dealing with complex latent distributions, the tree
 
2818
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
structuresymbolizesthemthroughsumandproduct,breakingthem
down into simpler sub-problems for resolution in a recursive man-
ner.Intheprocessofsolvingthroughtree-structureddiscretization,
the formation of a â€™coefficient pathâ€™ enhances interpretability. Fi-
nally,entropyisemployedasameanstolearnuncertaintyinthe
constructionprocessofHLNPs.Wedemonstratetheeffectiveness
oftheHLNPsmodelsfor1Ddata,Bayesianoptimization,and2D
data.
2 Preliminaries
2.1 Notations
Wehavetheprimarytaskofdefiningthesetupforthemeta-learning
problem. Suppose a set of input spaces is represented as Xand
output space is represented as Y. We have Dğ‘=(X Ã—Y)ğ‘as
the collection of ğ‘pairs of input-output data. Dâ‰¤ğ‘=/uniontext.1ğ‘
ğ‘›=1Dğ‘›
representsacollectionthatcontainsatmost ğ‘pairsofinput-output
data,whereas Dâˆ=/uniontext.1âˆ
ğ‘›=1Dğ‘›representsacollectionconsistingof
afinite numberof pairsof input-outputdata. Definethe elements
inğ·âˆˆDthe data set is denoted as ğ·=(ğ‘¥,ğ‘¦), where ğ‘¥âˆˆXğ‘
representstheelementsetoftheinputspaceand ğ‘¦âˆˆYğ‘represents
the element set of the output space. A collection of datasets is
denotedas ğ·ğ‘€
ğ‘š=1.Inmeta-learningcalleachindividualdataset ğ·ğ‘š
as a task [ 29]. For each task data ğ·ğ‘šis divided into two categories
represented as ğ·ğ¶ğ‘š=/parenleftBig
ğ‘¥ğ¶ğ‘š,ğ‘¦ğ¶ğ‘š/parenrightBig
context sample points and ğ·ğ‘‡ğ‘š=
/parenleftBig
ğ‘¥ğ‘‡ğ‘š,ğ‘¦ğ‘‡ğ‘š/parenrightBig
targetsamplepointsrespectively[ 3].Theultimategoal
istotrainamodelfromthedataset ğ·ğ¶ğ‘štoobtaintheoutputbest
prediction ğ‘¦ğ‘‡ğ‘šgiven the target inputs ğ‘¥ğ‘‡ğ‘š.
2.2 Neural Processes
Our task is to implement an implicit function ğ‘“:Xâ†’Y that
efficiently captures the relationship between input and output
data (includinguncertainty). TheNPs modelacquires anycontext
sample points as conditions and predicts target sample points as
ğ‘/parenleftBig
ğ‘¦ğ‘‡ğ‘š|ğ‘¥ğ‘‡ğ‘š,ğ·ğ¶ğ‘š/parenrightBig
:=ğ‘/parenleftBig
ğ‘¦ğ‘‡ğ‘š|ğ‘¥ğ‘‡ğ‘š;/parenleftBig
ğ‘¥ğ¶ğ‘š,ğ‘¦ğ¶ğ‘š/parenrightBig/parenrightBig
. More specifically, we
focusedonlyonasimpleheteroscedasticGaussianmeasurement
noise to restrict the scope of the study.
ğ‘ğœƒ/parenleftBig
ğ‘¦ğ‘‡
ğ‘š|ğ‘¥ğ‘‡
ğ‘š,ğ·ğ¶
ğ‘š/parenrightBig
=/productdisplay.1
(ğ‘¥ğ‘‡ğ‘š,ğ‘¦ğ‘‡ğ‘š)âˆˆğ·ğ‘‡ğ‘šN/parenleftBig
ğ‘¦ğ‘‡
ğ‘š;ğœ‡ğœƒ/parenleftBig
ğ‘¥ğ‘‡
ğ‘š,ğ·ğ¶
ğ‘š/parenrightBig
,
ğœ2
ğœƒ/parenleftBig
ğ‘¥ğ‘‡
ğ‘š,ğ·ğ¶
ğ‘š/parenrightBig/parenrightBig
.(1)
where ğœ‡ğœƒ:Xâ†’Yandğœ2
ğœƒ:Xâ†’R+denotethemappingofinputs
into the form of averages and variances [ 18].ğœƒis a parameter in
the function ğ‘“that can be learned. In the Bayesian optimization
processes, the NPs [ 11] model represents the overall structure and
uncertainty as a global latent Gaussian variable ğ‘is denoted as
ğ‘âˆ¼ğ‘ğœƒ/parenleftBig
ğ‘|ğ‘¥ğ‘‡ğ‘š,ğ·ğ¶ğ‘š/parenrightBig
. We further express Eq (19) as follows:
ğ‘ğœƒ/parenleftBig
ğ‘¦ğ‘‡
ğ‘š|ğ‘¥ğ‘‡
ğ‘š,ğ‘/parenrightBig
=/productdisplay.1
(ğ‘¥ğ‘‡ğ‘š,ğ‘¦ğ‘‡ğ‘š)âˆˆğ·ğ‘‡ğ‘šN/parenleftBig
ğ‘¦ğ‘‡
ğ‘š;ğœ‡ğœƒ/parenleftBig
ğ‘,ğ‘¥ğ‘‡
ğ‘š,ğ·ğ¶
ğ‘š/parenrightBig
,
ğœ2
ğœƒ/parenleftBig
ğ‘,ğ‘¥ğ‘‡
ğ‘š,ğ·ğ¶
ğ‘š/parenrightBig/parenrightBig
.(2)
The log-likelihood of the above posterior distribution is difficult
tosolve,andVIisintroducedtooptimizetheframework[ 17].Wecompute the ELBO of logğ‘ğœƒ/parenleftBig
ğ‘¦ğ‘‡ğ‘š|ğ‘¥ğ‘‡ğ‘š,ğ‘/parenrightBig
, which is represented as
two parts as the reconstruction part and the KL scatter measure
part:
Eğ‘âˆ¼ğ‘ğœ™(ğ‘|ğ·ğ‘‡ğ‘š)/bracketleftBig
logğ‘ğœƒ/parenleftBig
ğ‘¦ğ‘‡
ğ‘š|ğ‘,ğ‘¥ğ‘‡
ğ‘š,ğ·ğ¶
ğ‘š/parenrightBig/bracketrightBig
âˆ’ğ·ğ¾ğ¿/bracketleftBig
ğ‘ğœ™/parenleftBig
ğ‘|ğ·ğ‘‡
ğ‘š/parenrightBig
||ğ‘ğœ“/parenleftBig
ğ‘|ğ‘¥ğ‘‡
ğ‘š,ğ·ğ¶
ğ‘š/parenrightBig/bracketrightBig
.(3)
Where ğœ“representstheconditionalpriorparameter[ 12].ğ‘ğœ™/parenleftBig
ğ‘|ğ·ğ‘‡ğ‘š/parenrightBig
representsaposteriordistributionusedduringthetrainingofthe
model to aid in data fitting. However, this posterior distribution is
no longer used in the actual inference processes.
2.3 Bayesian Network
Webuildaprobabilisticmodel ğºoftheinputcontextsamplepoints
(ğ‘¥ğ‘‡ğ‘š,ğ‘¦ğ‘‡ğ‘š). In the probability model distribution, the context sample
pointssatisfyconditionalindependence,representedinconditional
probability form as ğ‘/parenleftBig
ğ·ğ¶ğ‘š|pağº/parenleftBig
ğ‘¥ğ¶ğ‘š,ğ‘¦ğ¶ğ‘š/parenrightBig/parenrightBig
, resulting in a joint dis-
tribution ğ‘(ğ·ğ¶ğ‘š|ğº)=/producttext.1
ğ‘–ğ‘/parenleftBig
ğ·ğ¶
ğ‘–|pağº/parenleftBig
ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–/parenrightBig/parenrightBig
. In the above
joint distribution, pağºdenotes the parent class node of the context
samplepoint.InthisBayesianmodel,weimplementthecontinu-
ous function/parenleftBig
ğ‘¥ğ¶ğ‘š,ğ‘¦ğ¶ğ‘š/parenrightBig
=(ğ‘¥ğ¶ğ‘š,ğ‘¦ğ¶ğ‘š)ğµ+ğœ–using the existing linear
Gaussiandistribution[ 19],where ğµâˆˆRğ‘šÃ—ğ‘šisaparametermatrix
with learnable weights and ğœ–âˆ¼N( ğ‘,Î£). Meanwhile, ğ‘âˆˆRğ‘šand
Î£âˆˆRğ‘šÃ—ğ‘š
â‰¥0adiagonalmatrixinthenoisevariable.Inparticular,for
ğ‘–not a parent of ğ‘—inğº, then ğµğ‘–,ğ‘—=0.
We partition the graph ğºinto subgraphs connected by multi-
plication ğºğ‘–.ğºğ‘–representstheparentclasssetofthe contextsam-
plepoint (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–).Weindirectlyobtaintheposteriordistribution
ğ‘ğº(ğº|ğ·ğ¶ğ‘š)ofgraph ğºthroughthe posteriordistributionsofits
subgraphs ğºğ‘–.Tosimplifythecomputationalprocess,weintroduce
the correlation ğŸ™in the subgraphs ğºğ‘–. Meanwhile, ğ‘ğº(ğº|ğ·ğ¶ğ‘š)
calculation also requires the definition of prior ğ‘ğœ“(ğºğ‘–)and log-
likelihood ğ‘ğœƒ(ğºğ‘–)forthesubgraphs.Theposteriordistributionof
graph ğºisexpressedas ğ‘ğº(ğº|ğ·ğ¶ğ‘š)=ğŸ™ğº/producttext.1
ğ‘–ğ‘ğœ“(ğºğ‘–)ğ‘ğœƒ/parenleftBig
ğ·ğ¶
ğ‘–|ğºğ‘–/parenrightBig
.
During the process of computing the posterior distribution, obtain-
ing a reasonable estimate of model parameters is achieved through
theuseofpriordistributionandlog-likelihood,whiletakinginto
full consideration the data and domain knowledge [26].
3 Method
3.1 Hierarchical Linear Symbolized
Tree-structured Network
We employ the observed context sample points ğ·ğ¶ğ‘što construct a
hierarchical linear symbolized tree-structured network in graph ğº.
At the same time, we consider preserving the NPs model Permu-
tationinvariance[ 11]fortreestructureconstruction.Weadopta
top-downapproachtoimplementthehierarchicallinearsymbol-
izedtree-structurednetwork.Inadditiontotheleafnodes ğ¿,our
treestructurecomprisestwotypesofsymbolizednodes:Product
nodes ğ‘€andsumnodes ğ‘‡.Thesenodescanberegardedasdistri-
butionsofsubsetsoftherootnode,withtherootnoderepresenting
the overall distribution. The product node aims to aggregate the
distribution of child nodes, i.e., ğ‘€=/producttext.1
ğ·ğ¶
ğ‘–âˆˆğ‘â„(ğ‘€)ğ·ğ¶
ğ‘–. The sum
 
2819Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
node is determined by weighting and summing its child nodes, i.e.,
ğ‘‡=/summationtext.1
ğ·ğ¶
ğ‘–âˆˆğ‘â„(ğ‘‡)ğœ†ğ‘–ğ·ğ¶
ğ‘–(ğœ†ğ‘–>0,/summationtext.1ğœ†ğ‘–=1). The node ğ·ğ¶
ğ‘–scope de-
scribesthesetofvariablesthateachnodeaffectsintheprobabilistic
graph model. For example, the node ğ·ğ¶
ğ‘–has a scope ğ‘ ğ‘(ğ·ğ¶
ğ‘–)={ğ‘‰}
,where ğ‘‰isthecoverageofthespecifieddistribution.Thescope
ofasumorproductnodedependsonthescopeofitschildnodes
ğ‘ ğ‘(ğ·ğ¶
ğ‘–)=/uniontext.1
ğ·ğ¶
ğ‘—âˆˆğ‘â„(ğ·ğ¶
ğ‘–)ğ‘ ğ‘(ğ·ğ¶
ğ‘—).
Theadvantageofalinearsymbolizedtree-structurednetworkis
itsabilitytocapturecontextinformationatdifferenthierarchical
levels,simplifyingtheposteriordistributioncalculationforgraph
ğº.Whenconstructingtree-structurednetworks,itisessentialto
possess completeness and decomposability.
(i) The completeness of linear symbolized tree-structured net-
works. For each sum node ğ‘‡in the tree, for any two nodes ğ·ğ¶
1
andğ·ğ¶
2, they should satisfy the condition ğ‘ ğ‘/parenleftBig
ğ·ğ¶
1/parenrightBig
=ğ‘ ğ‘/parenleftBig
ğ·ğ¶
2/parenrightBig
.I n
other words, the two child nodes have the same scope, and their
distributions involve the same variables. This property also applies
tothesumnode ğ‘‡itself,asitsscopeisaconcatenationofthescopes
of its children.
(ii) The decomposability of linear symbol tree-structured net-
works.Foreachproductnode ğ‘€inthetreeandanytwochildnodes
ğ·ğ¶
1andğ·ğ¶
2,theyshouldsatisfythecondition ğ‘ ğ‘/parenleftBig
ğ·ğ¶
1/parenrightBig
âˆ©ğ‘ ğ‘/parenleftBig
ğ·ğ¶
2/parenrightBig
=âˆ….
Inotherwords,thescopeoftheproductnode ğ‘€canbepartitioned
into disjoint subsets by the scopes of its child nodes.
Whenthesepropertiesarepresent,itbecomesfeasibletocom-
putetheprobabilisticmodelgraph ğº,whichisinferredusingthe
MostProbableExplanation(MPE)posteriordistribution[ 1].Therea-
soningprocessofthehierarchicallinearsymbolizedtree-structured
network is only related to the number of its edges, so it also has
the advantage of linear time growth.
3.2 Processable Representation of
Tree-Structured Network
We process context sample points in a top-down manner, intro-
ducing sum and product symbolization to construct a hierarchical
linear tree-structured network[5].
Each processed context sample point (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)during the tree
constructionisdenotedas ğœ‹<ğ‘–.Ifğºğ‘–âŠ†ğœ‹<ğ‘–signifiesthattheparent
nodesetofeachnode ğ·ğ¶
ğ‘–intheprobabilisticgraph ğºğ‘–isconsistent
withtheordering ğœ‹<ğ‘–,thenweconsiderthegraph ğºtobeconsis-
tent(ğº|=ğœ‹). We use the joint distribution ğ‘(ğœ‹,ğº|ğ·ğ¶ğ‘š)=ğ‘ğœ“(ğº)
ğ‘ğœƒ/parenleftBig
ğ·ğ¶ğ‘š|ğº/parenrightBig/producttext.1
ğ‘–ğŸ™ğºğ‘–âŠ†ğœ‹<ğ‘–.Itâ€™simportanttonotethatthemarginal
probability distribution ğ‘(ğº|ğ·ğ¶ğ‘š)is distinct from the conditional
probability ğ‘ğº(ğº|ğ·ğ¶ğ‘š). The key distinction lies in the fact that
ğ‘(ğº|ğ·ğ¶ğ‘š)considers a broad spectrum of ordering ğœ‹, while the
conditional probability ğ‘ğº(ğº|ğ·ğ¶ğ‘š)is limited to specific graph
structures corresponding to a particular ordering ğœ‹. Graph ğºis
representable as decomposable by ğ‘ğº(ğº)=/producttext.1
ğ‘–ğ‘ğºğ‘–(ğºğ‘–). We refor-
mulate the joint distribution: ğ‘(ğœ‹,ğº)=/producttext.1
ğ‘–ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğœ‹<ğ‘–.
3.3 Hierarchical Bayesian Conditional
Independence Inference
This approach is disastrous for joint distributions because the pro-
cess of constructing a tree for context sample points. We cannotachieve the computation of the posterior distribution through sam-
pling,aswellasthemarginalprobabilityofspecificnodeedges.For
the computation of this posterior distribution, we transform it into
a feasible decomposed approximation approach for representation.
Weareinspiredbythedivideandconquermethodtodecompose
the computation of the target distribution into precise conditional
independence,hierarchicallybreakingitdownintosmaller,simpler
problems for approximate solutions. We define a subset of context
samplepointsas ğ·ğ¶ğ‘ ,where ğœ‹<ğ‘ representsthetopologicalordering
involved in tree construction with ğ·ğ¶ğ‘ .ğºğ‘ /defines{ğºğ‘–:ğ‘–âˆˆğ‘ }denotes
the set of parent class nodes ğ·ğ¶
ğ‘–inğ·ğ¶ğ‘ . We partition the data set
into two subsets (ğœ‹ğ‘ 1,ğœ‹ğ‘ 2).ğ·ğ¶ğ‘ 1denotes completed tree network
constructioncontext samplepointsand ğ·ğ¶ğ‘ 2denotespendingcon-
text sample points. The conditional distribution for this case is
rewritten as:
ğ‘/parenleftbigğœ‹,ğº|ğœ‹=/parenleftbigğœ‹ğ‘ 1,ğœ‹ğ‘ 2/parenrightbig/parenrightbig=/productdisplay.1
ğ‘–âˆˆğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†(ğœ‹ğ‘ 1<ğ‘–)
/productdisplay.1
ğ‘–âˆˆğ‘ 2ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ‘ 1âˆª(ğœ‹ğ‘ 2<ğ‘–),(4)
From the above formulation it can be noticed that we have divided
thecomputationalprocessintotwoparts:/parenleftbigğœ‹ğ‘ 1,ğºğ‘ 1/parenrightbigand/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig.
Now, we extend the processing approach to the entire sampling
context sample points, also denoted as ğ·ğ¶ğ‘ 1andğ·ğ¶ğ‘ 2, respectively.
We focus on computing the distribution of/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig. The corre-
sponding distribution can be expressed as: Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/defines
/producttext.1
ğ‘–âˆˆğ‘ 2ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ‘ 1âˆª(ğœ‹ğ‘ 2<ğ‘–).Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2denotestheunnormalized
originaldistribution.Now,weconditionallydecomposethepend-
ing context sample points ğ·ğ¶ğ‘ 2into two categories: the operational
contextsamplepoints ğ·ğ¶ğ‘ 21andthecandidatesamplepoints ğ·ğ¶ğ‘ 22.So,
werecalculate Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2|ğœ‹ğ‘ 2=/parenleftbigğœ‹ğ‘ 21,ğœ‹ğ‘ 22/parenrightbig/parenrightbig,forwhich the
approximation Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 21,ğºğ‘ 21/parenrightbigËœğ‘ğ·ğ¶ğ‘ 1âˆªğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22/parenleftbigğœ‹ğ‘ 22,ğºğ‘ 22/parenrightbig.This
independence is employed for approximating probability distribu-
tions by partitioning the context sample points into two parts ğ·ğ¶ğ‘ 1
andğ·ğ¶ğ‘ 2,independentlyapproximatingtheprobabilitydistributions
withineachpart Ëœğ‘âˆ…,ğ·ğ¶ğ‘ 1/parenleftbigğœ‹ğ‘ 1,ğºğ‘ 1/parenrightbig,Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig.Thisisdone
recursively during the implementation of the hierarchical linear
symbolized tree-structured network, continuing until it results in a
single node (leaf node) remaining in ğ·ğ¶ğ‘ 2(Proposition 1).
3.4 Constructing a Hierarchical Linear
Symbolized Tree-structured Network
Wetaketherandomlysampledcontextsamplepoints ğ·ğ¶ğ‘šasawhole
toserveastherootnode.Duringtheconstructionofthetree-like
network, we alternate between processing subsets ğ·ğ¶ğ‘ 1andğ·ğ‘ 2ğ¶.
Weformallydefineahierarchicallinearsymbolizedtree-structured
network.
Definition1.Thehierarchicallinearsymbolizedtree-structured
network(ğœ‹,ğº)has the following structure:
(i)Whentheleafnode ğ¿belongsto (ğ·ğ¶ğ‘ 1,(ğ‘¥ğ‘–ğ¶,ğ‘¦ğ‘–ğ¶))and(ğ‘¥ğ‘–ğ¶,ğ‘¦ğ‘–ğ¶)
âˆ‰ğ·ğ¶ğ‘ 1, its scope is defined as ğ‘ ğ‘(ğ¿)=(ğœ‹ğ‘–,ğºğ‘–). The distribution of
the leaf node ğ¿is considered only with respect to the graph ğºğ‘–
corresponding to the parent node set consistent with subset ğ·ğ¶ğ‘ 1.
 
2820KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
+
Ñ«
(0.4,2.3) (1.2,0.8)
(1.6,1.9) (2.4,1.0)
1D Gaussian Distribution Root Node
  0.2 0.5 0.10.2
+
Ñ« (0.4,2.3)
(1.2,0.8) (1.6,1.9)
(2.4,1.0)+ +
Ñ« (1.2,0.8)
(1.6,1.9) (2.4,1.0)
(0.4,2.3)Ñ« (1.6,1.9)
(2.4,1.0) (0.4,2.3)
(1.2,0.8)+
Ñ« (2.4,1.0)
(0.4,2.3)
(1.6,1.9)(1.2,0.8)0.7 0.10.10.1
Figure1:Wesamplefourcontextsamplepointsfroma1D
Gaussiandistribution:(0.4,2.3),(1.2,0.8),(1.6,1.9),(2.4,1.0).
Based on the above results, we construct the hierarchical
linear symbolized tree-structured network in a top-downmanner. Meanwhile, the root node also includes the sum
symbol, ğ·ğ¶ğ‘ 1=âˆ….Wesettheallocationparameters ğ¾=(1,2),
correspondingto ğ·ğ¶ğ‘ 2,andpartitioning( ğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22).Thefirst
round of product nodes and sum nodes are alternately recur-
sively propagated, as shown in the above diagram .
(ii) A symbolic sum node ğ‘‡exists with a joint subset (ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2)/parenleftBig
ğ·ğ¶ğ‘ 1âˆ©ğ·ğ¶ğ‘ 2=âˆ…/parenrightBig
, and/barex/barexğ·ğ¶ğ‘ 2/barex/barex>1, and its scope is ğ‘ ğ‘(ğ‘‡)=(ğœ‹ğ‘ 2,ğºğ‘ 2).
It has child nodes contain weights ğœ†ğ‘–, If the ğ‘–th child node is a
product node ğ‘€, partition ğ·ğ¶ğ‘ 2into(ğ·ğ¶ğ‘ 21,ğ‘–,ğ·ğ¶ğ‘ 22,ğ‘–).
(iii) A symbolic product node ğ‘€exists with/parenleftBig
ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22/parenrightBig
/parenleftBig
ğ·ğ¶ğ‘ 1âˆ©ğ·ğ¶ğ‘ 21âˆ©ğ·ğ¶ğ‘ 22=âˆ…/parenrightBig
,andğ‘ ğ‘(ğ‘€)=(ğœ‹ğ‘ 21âˆªğœ‹ğ‘ 22,ğºğ‘ 21âˆªğºğ‘ 22).I n
general, the product node construction process contains two kinds
of child nodes, the first child node (ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 22)and the second child
node/parenleftBig
ğ·ğ¶ğ‘ 1âˆªğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22/parenrightBig
.
We construct hierarchical linear symbolized tree-structured net-
work sum nodes ğ‘‡for(ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2)to be interpreted as distributed
over ğ·ğ¶ğ‘ 2, with their parent nodes stored in ğ·ğ¶ğ‘ 1. Based on condi-
tions that involve a subset of context sample points represented
byasetofdataset ğ·ğ¶ğ‘ 2andtheirparentnodesin ğ·ğ¶ğ‘ 1,wecanuse
sum or product symbols to infer the dependencies among nodes in
ğ·ğ¶ğ‘ 2.Subsequently,theoverallstructureofthehierarchicallinear
symbolizedtree-structurednetworkisestablished.Intheprocess
of constructing the symbolized tree-structured network, it was ob-
servedthatsumnodes ğ‘‡andproductnodes ğ‘€appearalternately,
startingwithasumrootnode.Further,foreachchild (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)of
eachsumnode ğ‘‡,weprocesstheset ğ·ğ¶ğ‘ 2bysettingallocationpa-
rameters ğ¾pairs of pending context sample points partitions ( ğ·ğ¶ğ‘ 21,
ğ·ğ¶ğ‘ 22),whileensuring ğ·ğ¶ğ‘ 21,ğ‘–âˆ©ğ·ğ¶ğ‘ 22=âˆ….Wedescribeahierarchical
linear symbolized tree-structured network structure through an
example(asshowninFigure1).Therootnodeofthehierarchical
linearsymbolizednetwork,inadditiontosymbolicaddition,also
samplesfourcontextsamplepointsfroma1DGaussiandistribution,withcoordinatesbeing(0.4,2.3),(1.2,0.8),(1.6,1.9),(2.4,1.0).The
sum nodes ğ‘‡at different levels in a hierarchical linear symbolized
tree-structurednetworkareassignedthevalue ğ¾=(1,2).Thesum
and product layers alternate in a top-down loop until the leaf node
ğ¿.
From the above defined hierarchical linear symbolized tree-
structured network. If the leaf node ğ¿is associated with ( ğ·ğ‘ 1ğ¶,
ğ·ğ¶ğ‘ 21,ğ‘–),thenthe ğ‘–thvariablerepresentsthedistributionoverthepar-
entclassnodeset ğºğ‘–ofoperationalcontextsamplepoints (ğ‘¥ğ‘–ğ¶,ğ‘¦ğ‘–ğ¶).
Thedistribution overthesetof parentsofnode ğ·ğ‘–ğ¶isa subsetof
ğ·ğ‘ 1ğ¶. Here ğ·ğ‘ 1ğ¶is a subset representing possible parent node sets.
The restriction of ğ·ğ‘ 1ğ¶is that this distribution has support only if
ğºğ‘–âŠ†ğ·ğ‘ 1ğ¶, i.e., it only considers the case where the set of possible
parentnodesiswithin ğ·ğ‘ 1ğ¶.Theaboverestrictionindicatescon-
sistencyinthehierarchicallineartree-structurednetwork (ğœ‹,ğº).
The model is valid across the range of distributions.
Proposition1. Inhierarchicallinearsymbolizedtree-structured
networks,theprocessingofeachpairofcontextdatapointscon-
forms to the logical implication of ğºonğœ‹(where ğº|=ğœ‹).
For Proposition 1, the nodes in the graph structure ğºcontain
all the nodes in the topological ordering ğœ‹. One of the key require-
ments is that the distributions over leaf nodes are tractable, in
ordertoachievetheoverallapproximationofthehierarchicallin-
ear symbolized tree-structured network distribution (ğœ‹,ğº). Our
completion of the above tasks requires the introduction of MPE
inferenceandconditionalinferenceinprobabilisticmodeling.We
introducetheBooleanoperation ğµğ‘–ğ‘—tosimplifycalculations,indi-
catingwhether ğ·ğ¶
ğ‘–istheparentclassof ğ·ğ¶
ğ‘—.Further,lettheiterative
relationbedenotedbythelogicalconnective ğ¶ğ‘–,i.e., ğµğ‘–,ğ‘—orÂ¬ğµğ‘–,ğ‘—,
i.e.wecanrepresentthenodesas ğ¶ğ‘–=ğµğ¶
ğ‘–,1âˆ§ğµğ¶
ğ‘–,2âˆ§Â¬ğµğ¶
ğ‘–,3thatnodes
(ğ‘¥ğ¶
1,ğ‘¦ğ¶
1)and(ğ‘¥ğ¶
2,ğ‘¦ğ¶
2)are parents of ğ·ğ¶
ğ‘–, while node (ğ‘¥ğ¶
3,ğ‘¦ğ¶
3)is
not.Then,accordingtothejointsubset ğ·ğ¶ğ‘ 1onecanrepresentits
edge inference probability task as ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğ¶ğ‘–=1). On the basis
of the conditional distribution ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğºğ‘–|ğ¶ğ‘–=1), conditional
samplingneedstobedonefromittofinallycompletethemaximiza-
tioninference maxğºğ‘–ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğºğ‘–|ğ¶ğ‘–=1).Evengiventhestatesof
some edges, computing the edge probability distributions of other
edges is challenging and requires a lotof search and computation.
Thesechallengesmakeitverydifficulttoperforminferencequeries
in a Bayesian network. There has been previous work on this [ 28],
andwedealwithitbylimitingthenumberofcontextsamplepoints
ineachnode ğ¶ğ‘–âŠ‚ğ·ğ¶ğ‘ ğ‘š\ğ·ğ¶
ğ‘–.Weinheritfrompreviousworkbycon-
straining the parent nodes of context sample point (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)to a
fixed-size set of candidate parent nodes, where the size of ğ¶ğ‘–(cho-
senhereastheaveragenumberofcontextsamplepointsusedto
construct the tree) is selected to participate in the maximization of
MPEinference.Forthis Ëœğ‘ğ·ğ¶ğ‘ 1,{ğ·ğ¶
ğ‘–}(ğºğ‘–)approximaterepresentation,
we have ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1âˆ©ğ·ğ¶
ğ‘–.
Proposition 2. Any hierarchical linear tree-structured network
possesses completeness and decomposability.
In general, the hierarchical linear symbolized tree-structured
networksatisfiesthestandarddecomposabilityandcompleteness
properties.
 
2821Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
 
 Deterministic path
Latent  path
DecoderMLP
ZxiT
Z r#xiCyiCHierarchical 
Linear 
Symbolized
Tree 
xiCyiCCross -Tree
x1CxiC xiT
r#
synthesis
 y1Cx1C
yiCxiC
synthesisHierarchical
Linear
Symbolized  
Tree
yiT
Encoderx1Cy1C
x1CxiC 
KeyValue
Query
Hierarchical 
Symbolized 
Recursive 
Decomposition
Figure 2: The HLNPs model encoder structures information
from both the deterministic path and the latent path, while
the decoder predicts target sample points. The deterministic
path can learn hierarchical information through a hierar-
chicallinearsymbolizedtree-structurednetworkusingpa-
rameter ğ›¾. The latent path manages intricate distributions
by modeling them in the form of a discrete distribution. The
above-mentionedcomplexdistributionisdecomposedinto
several simpler sub-problems for resolution through hier-
archical linear symbolization, involving sum and productnodes, thereby inferring the overall posterior distribution
ofthecomplexlatentdistribution.Thedecoder,havingac-
quired knowledge of latent complex representations and po-
sitional information encoding, predicts target sample points
through an MLP.
3.5 Tree-Structured Neural Processes
Inthissection,thepreviouslyintroducedhierarchicallinearsymbol-izedtree-structurednetworkiscombinedwithNPstolearndifferenthierarchicalinformationinatree-structuredmanner,achievingbet-
ter fitting of complex distributions. At the same time, the model
gainsinterpretability.Therefore,basedontheabovedescription,we
propose a named Hierarchical Linear Symbolized Tree-structured
Neural Process (HLNPs).
In previous NPs approaches, learning from sampling to context
samplepointscontrolsthegenerationofcomplexdistributionsby
either considering a single latent variable [ 11] or incorporating
autoregressivestackedlatentvariables[ 27].WeredesignedtheNPs
model based on the advantages of tree structure, such as hierar-
chy, efficient retrieval and search, and interpretability. The HLNPs
model, generated through the fusion of both, also possesses dis-tinct levels of information, interpretability, scalability, and other
characteristics.NPshavePermutationinvariance:themodelshould
be able to produce the same output or have the same properties
regardless of the order in which the input data are arranged. Weadoptatop-downsamplingapproachtoconstructatree-structured
network, implementing both a deterministic path and a latent path
to preserve the aforementioned characteristics. Compared to de-
terministicpathsandthecurrentlypopularself-attention( O(ğ‘›2))
mechanisms, a tree structure allows for searching and retrieval
within a lower time O(ğ‘™ğ‘œğ‘”ğ‘›)complexity [ 16]. Learning complex
distributions in latent paths is challenging. We leverage tree struc-
ture and symbolization to decompose it into several simple sub-
problemsforresolution,thusachievingafunctionapproximation
for complex distributions.
ThestructureoftheHLNPsmodelisillustratedinFigure2.From
thestructuralperspectiveinFigure2,itisobservedthattheencoder
oftheHLNPsmodelisalsoimplementedthroughtwopathways:the
deterministic path and the latent path. In the deterministic path of
the encoder, ğ‘¥ğ‘‡
ğ‘–serves as the query, while context sample contexts
tokensactaskeysandvalues.Thisprocessresultsinthegeneration
ofquery-specificrepresentation ğ‘Ÿ#(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–,ğ‘¥ğ‘‡
ğ‘–)throughcross-tree
learning. Unlike simple averaging or the self-attention mechanism
used to assign weights, the values in the hierarchical linear sym-bolizedtree-structurednetworkaredistinct.The lattercanlearn
weight information for context sample points at different levels.
The hierarchical linear symbolized tree comprises alternating sum
andproductnodes,commencingfromtherootnode.Eachsumnode
represents a sub-problem requiring Bayesian inference for a set of
contextsamplepoints.Foreachsumnode,thereexistsasetofprod-uct nodes that combine different context sample points to facilitate
information sharing between various sum nodes. Consequently,
the hierarchical linear tree structure efficiently learns information
aboutthehierarchicalnodelocations.Bayesianconditioning,de-
notedby ğ›¾,mergeslocationinformationbetweendifferentlevels.
Forinstance,therootnodeinFig1isrepresentedas ğ›¾1((1.2,0.8))+
ğ›¾2((1.2,0.8),(1.6,1.9))+ğ›¾3((1.2,0.8),(1.6,1.9),(0.4,2.3),(2.4,1.0))
(ğ›¾ğ‘–>0,/summationtext.1ğ›¾ğ‘–=1). By applying the same processing approach to
the latent distribution, approximating the latent distribution of
Bayesiannetworkstructuresgivenobservedcontextsamplepoints.
The latent distribution is generally complex and challenging to
calculate directly,especiallyin the case of largenetwork structuresandlimiteddata.Therefore,ahierarchicaldecompositionapproach
is proposed to approximate the posterior distribution. It involves a
tree-structuredhierarchicaldecompositionofthejointdistribution,
breakingitdownintosimplersub-problems,eachcorresponding
to a subset of variables in the network. The decomposition process
startswiththeentiresetofvariablesandrecursivelypartitionsthemintosmallersubsetsuntileachsubsetcontainsonlyonevariable.In
this process, the tree structure is represented using sum nodes and
product nodes to denote order and structure. In the above process,
marginal distribution, conditional distribution, and sampling infer-
enceareappliedtotheleafnodes(Refertoappendixfordetails).
The decoder follows the traditional NPs model in predicting the
target sample points [4].
ELBOParameterLearning. Aftercompletingtheconstruction
of the HLNPs model, we need to address the parameter estimation
problem using VI. During the VI, the hierarchical linear symbol-
ized tree-structured network is discretized. Give a non-normalized
distribution Ëœğ‘(ğœ‹,ğº)=ğ‘ğº(ğº)ğŸ™ğº|=ğœ‹, the ELBO is given by:
Eğ‘ğœ™(ğœ‹,ğº)[logËœğ‘(ğœ‹,ğº)]+ğ»/parenleftBig
ğ‘ğœ™(ğœ‹,ğº)/parenrightBig
(5)
 
2822KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
where ğ»/parenleftBig
ğ‘ğœ™(ğœ‹,ğº)/parenrightBig
=âˆ’Eğ‘ğœ™(ğœ‹,ğº)/bracketleftbig
logğ‘ğœ™(ğœ‹,ğº)/bracketrightbigistheentropyof
thehierarchicallinearsymbolizedtree-structurednetwork ğ‘.The
goalofVIistomaximizetheELBOwithrespectto ğœ™.DiscretizedVI
ischallengingduetotherequirementofcomputing(gradientsof)
theexpectationunderhigh-varianceconditions,suchasthoseen-
counteredwithmethodslikeREINFORCE,whicharenotapplicable
duetothediscretenature.Fortunately,thisproblemisaddressed
by the Processable Representation of a Tree-Structured Network.
Proposition3. AnyHierarchicalLinearSymbolicTree-Structured
Network ğ‘ğœ™and a non-normalized distribution Ëœğ‘(ğœ‹,ğº)=ğ‘ğº(ğº)
can be computed in linear time.
Proof.Weassumetheexistenceofahierarchicallinearsymbol-
ized tree-structured Ëœğ‘(ğœ‹,ğº)=/producttext.1
ğ‘–ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğº|=ğœ‹. Define the ğ·ğ¶ğ‘ 2
asËœğ‘ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig=/producttext.1
(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)âˆˆğ·ğ¶ğ‘ 2ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘ 2=ğœ‹ğ‘ 2. In the con-
text of a hierarchical linear symbolized tree structure network,
for any node ğ‘with scope (ğœ‹ğ‘ 2,ğºğ‘ 2, we will define the evidence
lower-bound when using the distribution ğ‘(ğœ‹ğ‘ 2,ğºğ‘ 2)to approxi-
mateËœğ‘ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig:
ELBO(ğ‘)=Eğ‘/bracketleftBig
Ëœğ‘ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightBig
+ğ»/parenleftbigğ‘/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/parenrightbig(6)
Now we will demonstrate that the ELBO for the distribution ğ‘ğœ™
can be efficiently computed. This means the calculation can bedone in linear time with respect to the size of the hierarchical
linearsymbolizedtree-structurednetwork.Theefficiencyofthis
computation relies on the ELBO of the leaf node distributions.
Symbolizedsumnodes ğ‘‡areassociatedwith (ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2),withthe
weights denoted as ğ›¾1, ...,ğ›¾ğ‘–for their child nodes. We can represent
the expectation and entropy of Ëœğ‘ğ·ğ¶ğ‘ 2by decomposing it into the
distributions of its children. The formula is expressed as follows:
Eğ‘‡/bracketleftBig
logËœğ‘ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightBig
=/summationdisplay.1
ğ‘–ğ›¾ğ‘–Eğ‘â„(ğ‘‡ğ‘–)/bracketleftBig
logËœğ‘ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightBig
(7)
where ğ‘â„(ğ‘‡ğ‘–)representthe ğ‘–thchildnode.Theformulaforuncer-
tainty in entropy calculation is as follows:
ğ»/parenleftbigğ‘‡/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/parenrightbig=âˆ’Eğ‘‡/bracketleftbig
logğ‘‡/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightbig
=âˆ’/summationdisplay.1
ğ‘–ğ›¾ğ‘–Eğ‘â„(ğ‘‡ğ‘–)/bracketleftbig
logğ›¾ğ‘–ğ‘â„(ğ‘‡ğ‘–)/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightbig
=âˆ’/summationdisplay.1
ğ‘–ğ›¾ğ‘–logğ›¾ğ‘–+/summationdisplay.1
ğ‘–ğ›¾ğ‘–Eğ‘â„(ğ‘‡ğ‘–)/bracketleftbig
âˆ’ğ‘â„(ğ‘‡ğ‘–)/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightbig
=âˆ’/summationdisplay.1
ğ‘–ğ›¾ğ‘–logğ›¾ğ‘–+/summationdisplay.1
ğ‘–ğ›¾ğ‘–ğ»/parenleftbigğ‘â„(ğ‘‡ğ‘–)/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/parenrightbig
(8)
Combining the above 7 and 8, it is represented as follows:
ğ¸ğ¿ğµğ‘‚(ğ‘‡)=Eğ‘‡/bracketleftBig
logËœğ‘ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightBig
+ğ»/parenleftbigğ‘‡/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/parenrightbig
=âˆ’/summationdisplay.1
ğ‘–ğ›¾ğ‘–logğ›¾ğ‘–+/summationdisplay.1
ğ‘–ğ›¾ğ‘–ğ¸ğ¿ğµğ‘‚(ğ‘â„(ğ‘‡ğ‘–))(9)
From the above formula, it can be observed that we decompose
the complex expected distribution into the expectations of several
sub-distributions, and then sum them with different weights. Inthe computation of entropy uncertainty, the entropy of the sum
nodeweightsdecomposesintoaweightedsumofentropiesfrom
distinct sub-distributions. Just so you know, an important assump-
tion exists in the decomposition of entropy. The children nodesof the ğ‘‡weighted sum symbol node have disjoint support sets,
henceEğ‘â„(ğ‘‡ğ‘–)/bracketleftbig
ğ‘â„(ğ‘‡ğ‘—)(ğœ‹,ğº)/bracketrightbig
=0(ğ‘–â‰ ğ‘—). From the above, we can
conclude that the ELBO of a sum node can be decomposed into the
ELBO of its individual child nodes.
Symbolproductnode ğ‘€,associatedwithdataset (ğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22).The
symbol ğ‘€represents a distribution over/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig,where ğ·ğ¶ğ‘ 2=
ğ·ğ¶ğ‘ 21âˆªğ·ğ¶ğ‘ 22. We can obtain the following:
ğ¸ğ¿ğµğ‘‚(ğ‘€)=Eğ‘€/bracketleftBig
logËœğ‘ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/bracketrightBig
+ğ»/parenleftbigğ‘€/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/parenrightbig
=Eğ‘€/bracketleftBig
logËœğ‘ğ·ğ¶ğ‘ 21/parenleftbigğœğ‘†21,ğºğ‘†21/parenrightbig/bracketrightBig
+ğ»/parenleftbigğ‘â„(ğ‘€1)/parenleftbigğœ‹ğ‘ 21,ğºğ‘ 21/parenrightbig/parenrightbig
+Eğ‘€/bracketleftBig
logËœğ‘ğ·ğ¶ğ‘ 22/parenleftbigğœ‹ğ‘ 22,ğºğ‘†22/parenrightbig/bracketrightBig
+ğ»/parenleftbigğ‘â„(ğ‘€2)/parenleftbigğœğ‘ 22,ğºğ‘ 22/parenrightbig/parenrightbig
=ğ¸ğ¿ğµğ‘‚(ğ‘â„(ğ‘€1))+ğ¸ğ¿ğµğ‘‚(ğ‘â„(ğ‘€2))
(10)
The feasibility mentioned above comes from decomposability,en-
suringthenon-intersectionbetweenchilddistributions.Byrecur-
sivelyapplyingtheaboveequations,wecanexpresstheELBOof
the HLNPs model using the weights and ELBO of the leaf node
distributions.Symbolsumandproductareutilizedforeachchild
node.
Besides,itisnecessarytocomputetheELBOfortheleafnode
distributions after the decomposition process. For the leaf node
L associated with (ğ·ğ¶ğ‘ 1,(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–))and the context sample point
(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–),itsparentnodedistributionisrepresentedby ğ¿(ğºğ‘–).We
can express it as follows:
ğ¸ğ¿ğµğ‘‚(ğ¿)=Eğ¿/bracketleftBig
logËœğ‘/parenleftBig
ğœ‹{ğ‘–},ğºğ‘–/parenrightBig/bracketrightBig
+ğ»/parenleftBig
ğ¿/parenleftBig
ğœ‹{ğ‘–},ğºğ‘–/parenrightBig/parenrightBig
=Eğ¿/bracketleftbig
logğ‘ğºğ‘–(ğºğ‘–)/bracketrightbig
+ğ»(ğ¿(ğºğ‘–))(11)
Werequirethat ğ¿hassupportonlyover ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1.Itsupport ğ¿(ğºğ‘–)
approximating ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1. The aforementioned ELBO leaf
node distribution can be rewritten as follows:
ğ¸ğ¿ğµğ‘‚(ğ¿)=Eğ¿/bracketleftbig
logğ‘ğºğ‘–(ğºğ‘–)/bracketrightbig
+ğ»(ğ¿(ğºğ‘–))/parenrightbig
=Eğ¿/bracketleftbig
logğ‘ğºğ‘–(ğºğ‘–)/bracketrightbig
âˆ’Eğ¿[logğ¿(ğºğ‘–)]
=âˆ’ğ¾ğ¿/parenleftbigğ¿/bardblğ‘ğºğ‘–/parenrightbig(12)
KLrepresentstheKLdivergencemeasure.Maximizingthesim-
plified ELBO above is essentially minimizing the distribution diver-
gence. We continue to simplify the above results:
ğ¸ğ¿ğµğ‘‚(ğ¿)=Eğ¿/bracketleftbig
logğ‘ğºğ‘–(ğºğ‘–)/bracketrightbig
+ğ»(ğ¿(ğºğ‘–))/parenrightbig
=Eğ¿/bracketleftbig
logğ‘ğºğ‘–(ğºğ‘–)/bracketrightbig
âˆ’Eğ¿[logğ¿(ğºğ‘–)]
=âˆ’Eğ¿/bracketleftBigg
logğ¿(ğºğ‘–)
ğ‘ğºğ‘–(ğºğ‘–)//summationtext.1
ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)/bracketrightBigg
+log/summationdisplay.1
ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)(13)
 
2823Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
It support ğ¿(ğºğ‘–)approximating ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1. The formula 13
mentioned above can be rewritten as follows:
ğ¸ğ¿ğµğ‘‚(ğ¿)=âˆ’ğ¾ğ¿/parenleftBigg
ğ¿/bardblğ‘ğºğ‘–ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1/summationtext.1
ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)/parenrightBigg
+log/summationdisplay.1
ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)
=log/summationdisplay.1
ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)(14)
Weobservethat,withthedeterminationoftheLnode,theELBO
becomes a constant.
InterpretabilityofTree-StructuredNeuralProcesses. The
hierarchical linear symbolized tree-structured network utilizes
Bayesian inference for the weights of edges. We find that causalinference relationships form between different nodes during thetree construction processes. The product node acts as a role in
weight allocation, analogous to a method of path coefficients (1934,
Wright). i.e. We consider the deterministic path in a given graph ğº
withweights ğ›¾.andcausalinferencebetween (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)and(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)
, represented by ğ¶ğ¼ğ‘–ğ‘—(ğ›¾). The inference from (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)to(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)
is a directed path where the weights are added. If the nodes arenot in an ancestor relationship, then
ğ¶ğ¼ğ‘–ğ‘—(ğ›¾)=0. When utilizing
weights in ğ·ğ¶
ğ‘†1and the weights ğ›¾of the graph is unknown, the
representation of its expected inference is as follows:
ğ¶ğ¼ğ‘–ğ‘—=/summationdisplay.1
ğ¿ğ‘ƒâˆˆğ·ğ¶ğ‘ ğ‘š\{(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–),(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)})ğ›¾{ğ‘–,ğ¿ğ‘ƒ1}ğ›¾{ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|,ğ‘—}
|ğ¿ğ‘ƒ|âˆ’1/productdisplay.1
ğ‘–=1ğ›¾{ğ¿ğ‘ƒğ‘–,ğ¿ğ‘ƒğ‘–+1}(15)
where ğ›¾{ğ‘–,ğ¿ğ‘ƒ1}represents the weight of the edge from context sam-
ple points (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)to(ğ‘¥ğ¶
ğ¿ğ‘ƒ1,ğ‘¦ğ¶
ğ¿ğ‘ƒ1),and ğ›¾{ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|,ğ‘—}represents the
weight ofthe edgefrom (ğ‘¥ğ¶
ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|,ğ‘¦ğ¶
ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|)to(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—). Theproduct
of these two weights represents the multiplication of edge weights
along the path, used to consider the contribution of that path to
the causal effect. The entire formula calculates the expected causal
effect by summing over all possible paths, considering all directed
pathsfrom contextsample points (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)to(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—).In thecon-
text of the HLNPs model, uncertainty is treated as a part of the
probability output. What we need to adopt is Bayesian model aver-
aging,wheretheestimateofeachmodelisweightedbyitsposteriorprobability,resultinginacomprehensiveestimation.Theaforemen-
tioned process is known as the average causal effect. This is as
follows:
BCE((ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–),(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—))/definesEğºâˆ¼ğ‘ğœ™(ğº)/bracketleftbig
Eğ›¾âˆ¼ğ‘(ğ›¾|ğº)/bracketleftbig
ğ¸ğ‘–ğ‘—(ğ›¾)/bracketrightbig/bracketrightbig
(16)
Therefore, the entire formula signifies calculating the causal effect
of(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)on(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)given observed pairs of context sample
pointsandthecausalgraph.Itinvolvestakingtheexpectationover
allpossiblecausalgraphsandcausalparameters,resultinginthe
Bayesian averaged causal effect.
Figure3:ResultsforbayesianoptimizationonHLNPsmodels.
we measured normalized simple regret and its cumulative
value for an iteration.
4 Experiment
4.1 meta-regression (1D data)
WeevaluateLatentHierarchicalLinearSymbolTree-structuredNet-
workNeuralProcesses(HLNPs)onseveraltasks:meta-regression
(1Ddata)andimagecompletion.WecomparethevariantsofNPs
as follows: Conditional Neural Processes (CNPs)[ 10]1, Neural Pro-
cesses (NPs)[ 11], Attentive Neural Processes( ANPs)[ 16]2, Boot-
strapping Neural Processes (BNPs)[ 20]3, Transformer Neural Pro-
cesses(TNPs)[ 24]4,AutoregressiveNeuralProcesses(AENPs)[ 27],
ConditionalAutoregressiveNeuralProcesses(CAENPs)[ 27],Ver-
satile Neural Processes (VNPs) [13]5.
Table 1: The mean and standard deviation of the five runs
are reported (MSE measures).
Method RBF kernels MatÃ©rn 5/2 Periodic
CNPs 0.278 Â±0.003 0.310 Â±0.003 0.652 Â±0.001
NPs 0.282 Â±0.003 0.315 Â±0.003 0.650 Â±0.002
ANPs 0.193 Â±0.001 0.230 Â±0.000 0.703 Â±0.002
BNPs 0.269 Â±0.003 0.301 Â±0.003 0.649 Â±0.002
TNPs 0.177 Â±0.001 0.222 Â±0.000 0.670 Â±0.009
VNPs 0.162 Â±0.003 0.201 Â±0.000 0.642 Â±0.007
AENPs 0.152 Â±0.003 0.217 Â±0.001 0.593 Â±0.005
CAENPs 0.149 Â±0.001 0.199 Â±0.003 0.524 Â±0.001
HLNPs 0.103Â±0.002 0.127 Â±0.001 0.391 Â±0.003
We need to perform bayesian optimization on the benchmark
functions used in training with the RBF kernel [ 2]. To assess the
modelâ€™s optimal performance, we employ the best simple regret,
whichmeasuresthedifferencebetweenthebestvalueinthemodel
andtheglobaloptimum.Fig3showstheaveragenormalizedregret
andcumulativenormalizedregretafter100trialsofthefunction.
TheresultsinthefiguredemonstratethattheHLNPsmodelexhibits
the best performance in both normalized regret and cumulative
normalized regret.
1https://github.com/stratisMarkou/conditional-neural-processes
2https://github.com/soobinseo/Attentive-Neural-Process
3https://github.com/juho-lee/bnp
4https://github.com/tung-nd/TNP-pytorch
5https://github.com/ZongyuGuo/Versatile-NP
 
2824KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
Inthissection,thecurvesinthedatasetaregeneratedusingfour
different configurations of 1D Gaussian processes: i) RBF kernel, ii)
Matern5/2kernel,iii)Periodickernel.TheHLNPsmodelobserves ğ‘›
contextsamplepointsasthetrainingsetand ğ‘›â€ samplepointsfrom
the same distribution as the test set. Furthermore, we estimate the
MSEmeasurebasedonthesampledtargetsamplepoints.HLNPs
experimental results are shown in table 1. The results from table 1
reveal that HLNPs outperform the variants of NPs by a significant
margin.
4.2 Image Completion
In this section, the model uses a portion of pixel values from the
observed images as context sample points to predict the remaining
pixel values in the images. Therefore, the above ğ‘¥ğ¶
ğ‘–represents the
pixelposition,andthecorresponding ğ‘¦ğ¶
ğ‘–representsthepixelvalue.
Duringtheexperimentalprocess,foreaseofhandling,wescaled
the pixel position value ğ‘¥ğ¶
ğ‘–to [-1, 1] and the pixel value ğ‘¦ğ¶
ğ‘–to [-0.5,
0.5][9].Theaboveexperimentinvolvedrandomlysamplingcontext
sample points and target sample points. We use two datasets for
experimentation: MNIST [7], CelebA [27].
TheimagesintheMNISTdatasethaveasizeof28 Ã—28pixels.We
samplecontextsamplepoints ğ‘›âˆ¼U[3,197),and ğ‘›â€ âˆ¼U[3,200âˆ’ğ‘›)
targetdatapoints.ForMNIST16,theimagesaredownsampledto
16Ã—16 and ğ‘›âˆ¼U[3,97), and ğ‘›â€ âˆ¼U[3,100âˆ’ğ‘›). The images in
the CelebA dataset have a size of 178 Ã—218 pixels. For CelebA32,
the images are down sampled to 32 Ã—32 and ğ‘›âˆ¼U[3,197), and
ğ‘›â€ âˆ¼U[3,200âˆ’ğ‘›).ForCelebA64,theimagesaredownsampledto
64Ã—64 and ğ‘›âˆ¼U[3,797), and ğ‘›â€ âˆ¼U[3,800âˆ’ğ‘›). For CelebA128,
theimagesaredownsampledto128 Ã—128and ğ‘›âˆ¼U[3,1597),and
ğ‘›â€ âˆ¼U[3,1600âˆ’ğ‘›). The above results are shown in table 2 and
3.Fromtheabovetable,itcanbeobservedthattheresultsofthe
HLNPs model are better than variants of other NPs models.
Table 2: Each method is evaluated with 5 different seeds
according to the log-likelihood (CelebA dataset).
Method CelebA
32Ã—32 64 Ã—64 128 Ã—128
CNPs 2.15 Â±0.01 2.43 Â±0.00 2.55 Â±0.02
NPs 2.48 Â±0.02 2.60 Â±0.01 2.67 Â±0.01
ANPs 2.90 Â±0.00 3.12 Â±0.00 3.72 Â±0.03
BNPs 2.76 Â±0.01 2.97 Â±0.00 3.12 Â±0.02
TNPs 2.37 Â±0.01 2.42 Â±0.03 3.10 Â±0.00
VNPs 3.14 Â±0.01 3.28 Â±0.01 3.31 Â±0.01
AENPs 3.02 Â±0.00 3.21 Â±0.00 3.85 Â±0.01
CAENPs 3.16 Â±0.02 3.43 Â±0.02 3.86 Â±0.00
HLNPs (ours) 3.41Â±0.01 3.98 Â±0.00 4.03 Â±0.00
4.3 Ablation Study
ğ¾factor allocation of pending context sample points. Forthe
HL-NPs, the selection of the number of pending context sample
points ğ·ğ¶ğ‘ 2duringtheconstructionprocesssignificantlyimpactstheTable 3: Each method is evaluated with 5 different seeds
according to the log-likelihood(MNIST dataset).
Method MNIST
28Ã—28 16 Ã—16
CNPs 4.06 Â±0.01 3.76 Â±0.01
NPs 4.13 Â±0.00 3.81 Â±0.00
ANPs 4.27 Â±0.02 3.95 Â±0.02
BNPs 4.34 Â±0.01 3.88 Â±0.01
TNPs 4.22 Â±0.02 3.79 Â±0.02
VNPs 4.57 Â±0.03 3.84 Â±0.03
AENPs 4.78 Â±0.01 3.85 Â±0.01
CAENPs 4.83 Â±0.00 3.96 Â±0.00
HLNPs (ours) 5.14Â±0.01 4.20 Â±0.01
inferenceresults.Forinstance,therootnodeinFig1isrepresentedas
ğ›¾1((1.2,0.8))+ğ›¾2((1.2,0.8),(1.6,1.9),(0.4,2.3))+ğ›¾3((1.2,0.8),(1.6,
1.9),(0.4,2.3),(2.4,1.0))(ğ›¾ğ‘–>0,/summationtext.1ğ›¾ğ‘–=1). We allocate ğ¾=(1,ğ‘›âˆ’
1),ğ¾=(ğ‘›/4,3ğ‘›/4),ğ¾=(ğ‘›/2,ğ‘›/2),ğ¾=(3ğ‘›/4,ğ‘›/4),ğ¾=(ğ‘›âˆ’1,1)
in order to achieve the distribution. The results of the above al-location are shown in table 4 and 5 . From the above results, it
isobservedthatdifferentallocationmethodsforpendingsample
points ğ·ğ¶ğ‘ 2in table 4 and 5 show different performances. When the
HLNPs model samples more context sample points, the greater the
numberassigned by ğ·ğ¶ğ‘ 2toğ·ğ¶ğ‘ 21,so choosedifferent ğ¾parameters
to construct a HLNPs according to different scenarios.
Table4:Differentpartitionmethodsfor ğ¾pendingcontext
sample points (CelebA dataset).
ğ¾ CelebA
32Ã—32 64 Ã—64 128 Ã—128
ğ¾=(1,ğ‘›âˆ’1)3.41Â±0.013.98Â±0.00 4.03 Â±0.02
ğ¾=(ğ‘›/4,3ğ‘›/4)3.18Â±0.014.05Â±0.014.00Â±0.01
ğ¾=(ğ‘›/2,ğ‘›/2)3.27Â±0.01 3.86 Â±0.004.18Â±0.01
ğ¾=(3ğ‘›/4,ğ‘›/4)3.24Â±0.00 3.82 Â±0.01 3.94 Â±0.02
ğ¾=(ğ‘›âˆ’1,1)3.17Â±0.01 3.78 Â±0.01 3.89 Â±0.02
Table5:Differentpartitionmethodsfor ğ¾pendingcontext
sample points (MNIST dataset).
ğ¾ MNIST
28Ã—28 16 Ã—16
ğ¾=(1,ğ‘›âˆ’1)5.14Â±0.014.20Â±0.00
ğ¾=(ğ‘›/4,3ğ‘›/4)5.07Â±0.01 4.11 Â±0.01
ğ¾=(ğ‘›/2,ğ‘›/2)4.98Â±0.024.57Â±0.00
ğ¾=(3ğ‘›/4,ğ‘›/4)4.92Â±0.00 4.16 Â±0.01
ğ¾=(ğ‘›âˆ’1,1)4.31Â±0.01 4.09 Â±0.01
 
2825Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Tree structure model. Thetree-likestructuregreatlyenhances
themodelâ€™sinterpretability,handlingofnonlinearrelationships,ro-
bustness,andsoon[ 22,25].Drozdov[ 8]employstreestructuresto
implement inside-outside recursive autoencoding, achieving unsu-
pervised learning of noun phrases and entities from unlabeled text.
Zeng[31]addressesthepermutationofclasslabelsinsupervised
classification problems by leveraging tree structures to preserve
classrelationships,whilealsoprovidingametricintheformoftree
nodes.
4.4 Related Work
Neural processes and variations. Garnelloetal[ 10]havetheprob-
lem of high computing cost and difficulty in finding a prior in GPs.
Nowadays popular neural networks can make accurate predictions
by gradient descent[ 14,21]. These authors combine the two to pro-
duceConditionalNeuralProcesses(CNPs)andcanscaletolargedatasets by observing only a few sample points. CNPs use fixed-
dimensioninputinthedataencodingprocess,resultinginalack
of flexibility in output. On this basis, NPs[ 12] enrich their encoded
representation by introducing latent variables. The aggregation
approach of encoders in NPs causes sample point underfitting. At-
tentive Neural Processes(ANPs)[ 3] dynamically assign power to
sample points in thisaggregation with multi-headed attention. In
practice, NPs have serious flaws in sequential decision-making,combined with the temporal function in the current transformerto produce Transformer Neural Processes (TNPs)[
24]. Convolu-
tionalConditionalNeuralProcesses(CCNPs)solveNPstomodel
the translational equivariance in the data (ie, time series, spatial
data, text data). This model extends the data processing from finite
to infinite dimensionality.
Tree structure model. Thetree-likestructuregreatlyenhances
themodelâ€™sinterpretability,handlingofnonlinearrelationships,ro-bustness,andsoon[
22,25].Drozdov[ 8]employstreestructuresto
implement inside-outside recursive autoencoding, achieving unsu-
pervised learning of noun phrases and entities from unlabeled text.
Zeng[31]addressesthepermutationofclasslabelsinsupervised
classification problems by leveraging tree structures to preserve
classrelationships,whilealsoprovidingametricintheformoftree
nodes.
Symbolic Network. Recently, the SGCN work leverages atten-
tion to design a signed attention network, representing the links in
thenetworkwithpositiveandnegativeattention.Buildinguponthis, the paper combines the ideas with ANPs [
16] to design a
PositiveandNegativeSignAttentionHierarchicalTreeStructure
Neural Processes (PSNPs)[ 6,15] model. However, compared to the
approach in this paper, this method oversimplifies the approxima-
tionofcomplexdistributions(representingpositiveandnegativeat-
tention with Gaussian distributions) and yields lower performance
than the proposed HLNPs model in this paper.
5 Conclusion
Inthispaper,weproposetheHLNPsmodelasavariantofNPs.The
HLNPs constructs a hierarchical linear symbolized tree networkin a top-down manner. In the deterministic path, this approachallows for richer positional information representations and re-
duces time complexity. In the latent path, complex distributionsarediscretized,andthrougharecursiveprocessinvolvingsumor
product, symbolized nodes are transformed into a set of simpler
sub-problemsforresolution.HLNPsimpartinterpretabilitytothe
causalrelationships betweencontextsample pointsinthe formof
â€™coefficientpathsâ€™.Thetreestructurediscretizesvariablestocapture
model uncertainty in the form of entropy. In future work, we will
apply the HLNPs model to 3D scenarios.
5.1 Acknowledgements
This work was supported by the National Natural Science Founda-
tion of China under grant No. 61936001.
References
[1]Guangji Bai, Chen Ling, and Liang Zhao. 2023. Temporal Domain General-
ization with Drift-Aware Dynamic Neural Networks. In The Eleventh Interna-
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id=
sWOsRj4nT1n
[2]EricBrochu,VladMCora,andNandoDeFreitas.2010. AtutorialonBayesian
optimizationofexpensivecostfunctions,withapplicationtoactiveusermodeling
and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599 (2010).
[3]WesselBruinsma,StratisMarkou,JamesRequeima,AndrewY.K.Foong,Tom
Andersson,AnnaVaughan,AnthonyBuonomo,ScottHosking,andRichardE
Turner. 2023. Autoregressive Conditional Neural Processes. In The Eleventh
International Conference on Learning Representations. https://openreview.net/
forum?id=OAsXFPBfTBh
[4]Wenlin Chen, Austin Tripp, and JosÃ© Miguel HernÃ¡ndez-Lobato. 2023. Meta-
learningAdaptiveDeep KernelGaussianProcessesforMolecular PropertyPre-
diction. In The Eleventh International Conference on Learning Representations .
https://openreview.net/forum?id=KXRSh0sdVTP
[5]Tristan Deleu, Mizu Nishikawa-Toomey, Jithendaraa Subramanian, Nikolay
Malkin,LaurentCharlin,andYoshuaBengio.2023. JointBayesianInferenceof
GraphicalStructure andParameters witha SingleGenerative FlowNetwork.In
ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling.
https://openreview.net/forum?id=4NMp0QFqwH
[6]Mingyu Ding, Yikang Shen, Lijie Fan, Zhenfang Chen, Zitian Chen, Ping Luo,
JoshuaB.Tenenbaum,andChuangGan.2023. VisualDependencyTransform-
ers:DependencyTreeEmergesFromReversedAttention.In Proceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 14528â€“
14539.
[7]AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An Image is Worth 16x16 Words: Trans-
formers for Image Recognition at Scale. In International Conference on Learning
Representations.
[8]Andrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, and Andrew Mc-
Callum. 2019. Unsupervised Latent Tree Induction with Deep Inside-Outside
RecursiveAuto-Encoders.In Proceedingsofthe2019ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and
ThamarSolorio(Eds.).AssociationforComputationalLinguistics,Minneapolis,
Minnesota, 1129â€“1141. https://doi.org/10.18653/v1/N19-1116
[9]LeoFeng,HosseinHajimirsadeghi,YoshuaBengio,andMohamedOsamaAhmed.
2023. LatentBottleneckedAttentiveNeuralProcesses.In TheEleventhInterna-
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id=
yIxtevizEA
[10]Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David
Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami.
2018. Conditional Neural Processes. In Proceedings of the 35th International
Conference on Machine Learning, Jennifer Dy and Andreas Krause (Eds.), Vol. 80.
1704â€“1713.
[11]Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J.
Rezende, S. M. Ali Eslami, and Yee Whye Teh. 2018. Neural Processes. ICML
2018workshop abs/1807.01622(2018). arXiv:1807.01622 http://arxiv.org/abs/1807.
01622
[12]Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, and Zhibo Chen. 2023.
VersatileNeuralProcessesforLearningImplicitNeuralRepresentations.In ICLR.
[13]Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, and Zhibo Chen. 2023.
Versatile Neural Processes for Learning Implicit Neural Representations. In The
EleventhInternationalConferenceonLearningRepresentations. https://openreview.
net/forum?id=2nLeOOfAjK
[14]Ayoub El Hanchi, David Stephens, and Chris Maddison. 2022. Stochastic
Reweighted Gradient Descent. In Proceedings of the 39th International Conference
 
2826KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
on Machine Learning (Proceedings of Machine Learning Research, Vol. 162) , Kama-
likaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,GangNiu,andSivan
Sabato (Eds.). PMLR, 8359â€“8374. https://proceedings.mlr.press/v162/hanchi22a.
html
[15]Junjie Huang, HuaweiShen, Liang Hou, and Xueqi Cheng.2019. Signed Graph
Attention Networks. CoRRabs/1906.10958 (2019). arXiv:1906.10958 http://arxiv.
org/abs/1906.10958
[16]Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, S. M. Ali Eslami,
Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. 2019. Attentive Neural
Processes. InInternationalConferenceonLearningRepresentations abs/1901.05761.
[17]DiederikP.KingmaandMaxWelling.2014. Auto-EncodingVariationalBayes.In
2ndInternationalConferenceonLearningRepresentations,ICLR2014,Banff,AB,
Canada, April 14-16, 2014, Conference Track Proceedings.
[18]Hyungi Lee, Eunggu Yun, Giung Nam, Edwin Fong, and Juho Lee. 2023. Mar-
tingalePosteriorNeuralProcesses.In TheEleventhInternationalConferenceon
Learning Representations. https://openreview.net/forum?id=-9PVqZ-IR_
[19]HyungiLee,EungguYun,HongseokYang,andJuhoLee.2022. ScaleMixtures
ofNeuralNetworkGaussianProcesses.In InternationalConferenceonLearning
Representations. https://openreview.net/forum?id=YVPBh4k78iZ
[20]JuhoLee,YoonhoLee,JungtaekKim,EunhoYang,SungJuHwang,andYeeWhye
Teh. 2020. Bootstrapping neural processes. NeurIPS.
[21]Wu Lin, Frank Nielsen, Khan Mohammad Emtiyaz, and Mark Schmidt. 2021.
Tractable structured natural-gradient descent using local parameterizations. In
Proceedingsofthe38thInternationalConferenceonMachineLearning (Proceedings
of Machine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.).
PMLR, 6680â€“6691. https://proceedings.mlr.press/v139/lin21e.html
[22]SaschaMarton,ChristianBartelt,andStefanLÃ¼dtke.2023. LearningAxis-Aligned
Decision Trees with Gradient Descent. https://openreview.net/forum?id=
gwizseh-Iam
[23]IvonaNajdenkoska,XiantongZhen,andMarcelWorring.2023. MetaLearning
to Bridge Vision and Language Models for Multimodal Few-Shot Learning. In
The Eleventh International Conference on Learning Representations.
[24]Tung Nguyen and Aditya Grover. 2022. Transformer Neural Processes:
Uncertainty-Aware Meta Learning Via Sequence Modeling. In Proceedings of
the39thInternationalConferenceonMachineLearning (ProceedingsofMachine
Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 16569â€“16594.
https://proceedings.mlr.press/v162/nguyen22b.html
[25]AlizÃ©e Pace, Alex Chan, and Mihaela van der Schaar. 2022. POETREE: Inter-
pretable Policy Learning with Adaptive Decision Trees. In International Confer-
enceonLearningRepresentations. https://openreview.net/forum?id=AJsI-ymaKn_
[26]FabioSigrist.2023. LatentGaussianModelBoosting. IEEETransactionsonPattern
Analysis and Machine Intelligence 45, 2 (2023), 1894â€“1905. https://doi.org/10.
1109/TPAMI.2022.3168152
[27]Jinyang Tai. 2023. Global Perception Based Autoregressive Neural Processes. In
Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
10487â€“10497.
[28]Jussi Viinikka, Antti Hyttinen, Johan Pensar, and Mikko Koivisto. 2020. To-
wards scalable bayesian learning of causal dags. Advances in Neural Information
Processing Systems 33 (2020), 6584â€“6594.
[29]Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al .2016. Match-
ing networks for one shot learning. In Advances in Neural Information Processing
Systems. 3630â€“3638.
[30]Qi Wang, Marco Federici, and Herke van Hoof. 2023. Bridge the Inference
Gaps of Neural Processes via Expectation Maximization. In The Eleventh Interna-
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id=
A7v2DqLjZdq
[31]SiqiZeng,RemiTachetdesCombes,andHanZhao.2023. LearningStructured
Representations by Embedding Class Hierarchy. In The Eleventh International
Conference on Learning Representations. https://openreview.net/forum?id=7J-
30ilaUZM
A APPENDIX
A.1The decomposability of Hierarchical Linear
Symbolized Tree-Structured Network
In the third section, we represent the construction process table
of the context sample point dataset ğ·ğ¶ğ‘šas graph (ğœ‹,ğº).W er e p -
resent the above context sample points divided into two distri-
butions as/parenleftbigğœ‹ğ‘ 1,ğºğ‘ 1/parenrightbigand/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig. Our construction condition is
ğœ‹=(ğœ‹ğ‘ 1,ğœ‹ğ‘ 2),with ğ·ğ‘ 1processedbefore ğ·ğ‘ 2duringthesamestage.
We have now demonstrated that the results obtained through com-
putation exhibit broad applicability, allowing us to decompose thedistribution hierarchically and thereby giving rise to the proposed
hierarchical tree structure.
Weconstruct ğ‘ğº(ğœ‹,ğº)=ğŸ™ğºğ‘–|=ğœ‹usingahierarchicallinearsym-
bolizedtree-structurednetworkoncontextsamplepointsin ğ·ğ¶ğ‘š.As-
suming ğ·ğ¶ğ‘ 1andğ·ğ¶ğ‘ 2arejointsubsetsof ğ·ğ¶ğ‘š,and ğ·ğ¶ğ‘ 2canbedecom-
posed into/parenleftBig
ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 22/parenrightBig
. We represent the above distribution on the
unnormalized distribution as Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2|ğœ‹ğ‘ 2=/parenleftbigğœ‹ğ‘ 21,ğœ‹ğ‘ 22/parenrightbig/parenrightbig,
forwhichtheapproximationbreaksdowninto Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 21,ğºğ‘ 21/parenrightbig
Ëœğ‘ğ·ğ¶ğ‘ 1âˆªğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22/parenleftbigğœ‹ğ‘ 22,ğºğ‘ 22/parenrightbig
Proof.By the above definition, we have Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2/parenrightbig/defines/producttext.1
ğ‘–âˆˆğ‘ 2ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ‘ 1âˆª(ğœ‹ğ‘ 2<ğ‘–).Intheconstructionprocess,condi-
tional operations ğœ‹ğ‘ 2=(ğœ‹ğ‘ 21,ğœ‹ğ‘ 22). We have that:
Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 2,ğºğ‘ 2|ğœ‹ğ‘ 2=/parenleftbigğœ‹ğ‘ 21,ğœ‹ğ‘ 22/parenrightbig/parenrightbig
âˆ/productdisplay.1
ğ‘–âˆˆğ‘ 2ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1âˆª(ğœ‹ğ‘ 2<ğ‘–)
=/productdisplay.1
ğ‘–âˆˆğ‘ 21ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1âˆª(ğœ‹ğ‘ 21<ğ‘–)
/productdisplay.1
ğ‘–âˆˆğ‘ 22ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1âˆªğ·ğ¶ğ‘ 21âˆª(ğœ‹ğ‘ 22<ğ‘–)
=Ëœğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2/parenleftbigğœ‹ğ‘ 21,ğºğ‘ 21/parenrightbigËœğ‘ğ·ğ¶ğ‘ 1âˆªğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22/parenleftbigğœ‹ğ‘ 22,ğºğ‘ 22/parenrightbig(17)
Proof complete.
A.2 Consistency in Hierarchical Linear
Symbolized Tree-Structured Network
For any context sample point dataset ğ·ğ¶ğ‘šas graph (ğœ‹,ğº), then we
consider the graph ğºto be consistent (ğº|=ğœ‹).
Proof.Contextsamplepointsconstructa completehierarchical
linearsymbolizedtree-structurednetworktop-down:i)Selecting
onechild fromeachsumnode forprocessing.ii) Selectingallchil-
drenfromeachproductnodeforprocessing.iii)Selectinguntilall
leafnodes(contextsamplepoints)havebeenprocessed.Throughan
orderedrecursiveprocess,contextdatapointsformahierarchical
linearsymbolizedtree-structurednetwork.Itâ€™sworthnotingthatin
constructingthistreenetwork,thefinalleafnodesaredeterministic.
Duringtheconstructionprocess, ğ·ğ¶ğ‘ 1determinestheorderofthe
nextpendingcontextsamplepointsbasedonthecontextsample
pointsaboveandbelowit.Inotherwords,fortheabove-mentioned
network, differentpartitioned context data points (ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22)
are connected through the product node ğ‘€. Simultaneously, the
node ğ‘€determines ğœ‹ğ‘ 21andğœ‹ğ‘ 22. Finally, the sum node in this
structureselectsachildnode.Theallocationoftheaforementioned
context sample points is represented by ğœ‹.
Now, construct the tree structure from the root node to the leaf
nodes in a top-down fashion. We associate pending context sample
points ğ·ğ¶ğ‘ 2through the sum node ğ‘‡. We prove by induction that,
for each sum node ğ‘‡ğ‘–in the hierarchical linear symbolized tree-
structured network, the ordering ğœ‹of context sample points in
handling context sample points from ğ·ğ¶ğ‘ 1obtained from ğ·ğ¶ğ‘ 2.
i)Therootnodeistheunionofallcontextsamplepointsdatasets
(ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2)=(âˆ…,ğ·ğ¶ğ‘š), so all conditions are satisfied.
ii)Now,withinthehierarchicallinearsymbolizedtree-structured
network, consider a product symbol node ğ‘€ğ‘–, such that ğ‘€ğ‘–+1could
 
2827Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
be either the first child ğ·ğ¶ğ‘ 21or the second child ğ·ğ¶ğ‘ 22ofğ‘€ğ‘–. The
objectivehere istoassociate ğ‘€ğ‘–with(ğ·ğ¶ğ‘ 1,ğ‘–,ğ·ğ¶ğ‘ 21,ğ‘–,ğ·ğ¶ğ‘ 22,ğ‘–).Then,(i)
Ifğ‘€ğ‘–+1isthefirstchild ğ·ğ¶ğ‘ 21inthehierarchicallinearsymbolized
tree-structurednetwork,then (ğ·ğ¶ğ‘ 1,ğ‘–+1ğ·ğ¶ğ‘ 21,ğ‘–+1)=(ğ·ğ¶ğ‘ 1,ğ‘–ğ·ğ¶ğ‘ 21,ğ‘–)holds.
Similarly,(ii)if ğ‘€ğ‘–+1isthesecondchild ğ·ğ¶ğ‘ 22,then(ğ·ğ¶ğ‘ 1,ğ‘–+1,ğ·ğ¶ğ‘ 2,ğ‘–+1)=
(ğ·ğ¶ğ‘ 1,ğ‘–âˆªğ·ğ¶ğ‘ 21,ğ‘–,ğ·ğ¶ğ‘ 22,ğ‘–).N o w , ğœ‹possesses the property of such or-
dering, where all processed context sample points in ğ·ğ¶ğ‘ 21,ğ‘–come
before those in ğ·ğ¶ğ‘ 22,ğ‘–. Building on the aforementioned inductive
assumptionthat ğ·ğ¶ğ‘ 1,ğ‘–precedes ğ·ğ¶ğ‘ 2,ğ‘–intheordering,astraightfor-
ward conclusion can be drawn in both cases (i) and (ii): during the
processingof contextsample points,all processedcontext sample
points in ğ·ğ¶ğ‘ 1,ğ‘–+1precede those in ğ·ğ¶ğ‘ 2,ğ‘–+1.
Theabovedescriptionimpliesthat,atanyleafnodeassociated
withthe pendingcontext samplepoint (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–),ğ·ğ¶ğ‘ 1precedesthe
variable ğ‘–in the construction of the ğœ‹rule context sample point.
Since the leaf distribution only has support over graphs with ğºğ‘–âŠ†
ğ·ğ¶ğ‘ 1, Therefore, all the graphs ğºbelow satisfy ğº|=ğœ‹.
Proposition2.Anyhierarchicallineartree-structurednetwork
possesses completeness and decomposability.
Proof.Inhierarchicallinearsymbolizedtree-structurednetworks,
theğ‘–th product symbol node has a scope (ğœ‹ğ‘ 21,ğ‘–âˆªğ‘ 22,ğ‘–,ğºğ‘ 21,ğ‘–âˆªğ‘ 22,ğ‘–),
ensuring its completeness. This is because, by definition, ğ·ğ¶ğ‘ 21,ğ‘–and
ğ·ğ¶ğ‘ 22,ğ‘–partition ğ·ğ¶ğ‘ 2. This networkâ€™s decomposability is obtained
from the scope ranges of the product symbol node ğ‘€and its child
nodes, where thevariable (ğœ‹ğ‘ 21âˆªğ‘ 22,ğºğ‘ 21âˆªğ‘ 22)are decomposed into
sum(orleaf)nodeswithranges (ğœ‹ğ‘ 21,ğºğ‘ 21)and(ğœ‹ğ‘ 22,ğºğ‘ 22),while
ğ·ğ¶ğ‘ 21andğ·ğ¶ğ‘ 21are disjoint.
A.3 Leaf Node Distribution
Wenowexplainhowtoperformmarginal,conditional,andmost
probableexplanation(MPE),andsamplinginferenceforleafnode
distributions in hierarchical linear symbolized tree-structured neu-
ralprocesses.Inthisnetwork,theleafnodedistributionforcontext
samplepoint (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)iscalculated usingthefollowing probability
density:
ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğºğ‘–)=ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1/summationtext.1
ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)(18)
where pağº/parenleftBig
ğ·ğ¶
ğ‘–/parenrightBig
âŠ†ğ·ğ¶ğ‘ 1is the set of potential parent contexts for
thevariablecontextsamplepoint,andatthesametime,thenormal-
izationisappliedwithinit.Asthenumberofconstructedcontext
samplepointsincreasesintherecursiveprocessofbuildingthetree-
structured network, the computation process becomes challenging.
During computation, simplifications require a global restriction on
thecandidatenodeset ğ·ğ¶
ğ‘–.Inotherwords,foreachleafnodecor-
responding to context sample point ğ·ğ¶
ğ‘–, with distribution ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–,
wereplacethepotentialparentnodeset ğ·ğ¶ğ‘ ğ‘–withtheintersectionof
ğ·ğ¶ğ‘ 1and the candidate set pağº/parenleftBig
ğ·ğ¶
ğ‘–/parenrightBig
. This restricts the range of the
distribution in the tree-structured network, and we can choose the
candidateparentcontextsamplepoints pağº/parenleftBig
ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–/parenrightBig
topreserve
as much posterior distribution as possible. As mentioned above, ef-
ficientinferencequeriesforany pağº/parenleftBig
ğ·ğ¶
ğ‘–/parenrightBig
âŠ†ğ·ğ¶ğ‘ 1canbeconducted
to obtain ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğºğ‘–).Thehierarchicallinearsymbolizedtree-structurednetworkstruc-
ture is presented in a normalized manner, demonstrating logical
symbolinferenceforms.Therefore,wedefine ğµğ‘–ğ‘—astheassociation,
i.e.,ğ·ğ¶
ğ‘—âˆˆğºğ‘–,indicating that ğ‘—is the parent node of ğ‘–. Then, the
computation for the key component is as follows:
ğ‘“ğ‘–/parenleftbigğ´ğ‘–,ğ´/prime
ğ‘–/parenrightbig=/summationdisplay.1
ğºğ‘–|=/parenleftbigg/parenleftbigg/logicalandtext.1
(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ´ğ‘–ğµğ‘–,ğ‘—/parenrightbigg
âˆ§/parenleftbigg/logicalandtext.1
(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ´/prime
ğ‘–Â¬ğµğ‘–,ğ‘—/parenrightbigg/parenrightbigg
ğ‘ğºğ‘–(ğºğ‘–)(19)
where ğ´ğ‘–,ğ´/prime
ğ‘–aredisjointsubsetsof pağº(ğ·ğ¶
ğ‘–).Theentireexpression
requires that the context sample points in ğ´ğ‘–are all parents of ğ‘–,
and the context sample points in ğ´/prime
ğ‘–are none of the parents of ğ‘–.
Basedontheabove,where ğ´ğ‘–andğ´/prime
ğ‘–aresubsetsof pağº(ğ·ğ¶
ğ‘–),w e
further express it as:
ğ‘“ğ‘–/parenleftbigğ´ğ‘–,ğ´/prime
ğ‘–/parenrightbig=ğ‘ğºğ‘–(ğ´ğ‘–) (20)
Because ğ´ğ‘–andğ´/prime
ğ‘–determinetherelevantparentnodesin ğ·ğ¶
ğ‘–.In
the construction process of the hierarchical linear symbolized tree-
structured network, the representation is implemented recursively
as follows:
ğ‘“ğ‘–/parenleftbigğ´ğ‘–,ğ´/prime
ğ‘–/parenrightbig=ğ‘“ğ‘–/parenleftBig
ğ´ğ‘–âˆª{(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)},ğ´/prime
ğ‘–/parenrightBig
+
ğ‘“ğ‘–/parenleftBig
ğ´ğ‘–,ğ´/prime
ğ‘–âˆª{(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)}/parenrightBig (21)
where(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)âˆˆ ğ·ğ¶
ğ‘–. From the representation of ğ‘“ğ‘–, it can be
observed that the processing involves determining whether the
context sample point (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)is a parent of pağº/parenleftBig
ğ·ğ¶
ğ‘–/parenrightBig
.
We use ğ¶ğ‘–to represent the conjunction of literals in/braceleftbig
ğµğ‘–,ğ‘—/bracerightbig
, thus
enabling logical representation and inference:
i)Marginal distribution: Now, for distribution ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–, the mar-
ginal distribution of the logical expression is represented as:
ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğ¶ğ‘–=1)=/summationtext.1
ğºğ‘–|=ğ·ğ¶
ğ‘–ğ‘ğºğ‘–(ğºğ‘–)ğŸ™ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1/summationtext.1
ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1ğ‘ğºğ‘–(ğºğ‘–)
=/summationtext.1
ğºğ‘–|=/parenleftbigg
ğ·ğ¶
ğ‘–âˆ§/logicalandtext.1
(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ·ğ¶
ğ‘–\ğ·ğ¶ğ‘ 1Â¬ğµğ‘–,ğ‘—/parenrightbiggğ‘ğºğ‘–(ğºğ‘–)
/summationtext.1
ğºğ‘–|=/logicalandtext.1
(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ·ğ¶
ğ‘–\ğ·ğ¶ğ‘ 1Â¬ğµğ‘–,ğ‘—ğ‘ğºğ‘–(ğºğ‘–)(22)
where We represent the condition of ğºğ‘–âŠ†ğ·ğ¶ğ‘ 1with the logical for-
mula/logicalandtext.1
(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ·ğ¶
ğ‘–\ğ·ğ¶ğ‘ 1Â¬ğµğ‘–,ğ‘—.ByobservingFormula19,wecanex-
press ğ‘“ğ‘–asamarginalprobabilitycalculation ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–/parenleftbigğ¶ğ‘–=1|ğ¶/prime
ğ‘–=1/parenrightbig:
ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–/parenleftbigğ¶ğ‘–=1|ğ¶/prime
ğ‘–=1/parenrightbig=ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–/parenleftbigğ¶ğ‘–âˆ§ğ¶/prime
ğ‘–=1/parenrightbig
ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–/parenleftBig
ğ¶/prime
ğ‘–=1/parenrightBig(23)
ii)MPE:Now,fordistribution ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–,theMPEfor ğ¶ğ‘–=1the
logical expression is represented as:
ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğ¶ğ‘–=1)=max
ğºğ‘–ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğºğ‘–|ğ¶ğ‘–=1)
=max
ğºğ‘–|=ğ¶ğ‘–ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğºğ‘–|ğ¶ğ‘–=1)
=maxğºğ‘–|=ğ·ğ¶
ğ‘–âˆ§/logicalandtext.1
(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ·ğ¶
ğ‘–\ğ·ğ¶ğ‘ 1Â¬ğµğ‘–,ğ‘—ğ‘ğºğ‘–(ğºğ‘–)
/summationtext.1
ğºğ‘–|=ğ·ğ¶
ğ‘–âˆ§/logicalandtext.1
(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ·ğ¶
ğ‘–\ğ·ğ¶ğ‘ 1Â¬ğµğ‘–,ğ‘—ğ‘ğºğ‘–(ğºğ‘–)(24)
 
2828KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
The process of the above computation involves the selection of
ğºğ‘–satisfying the logical conjunction. The summation from the
function ğ‘“ğ‘–, where there is a summation over ğºğ‘–satisfying the
logicalconjunction.Therefore,intherecursiveprocess,wecompute
anotherfunction ğ‘“max
ğ‘–,whichperformsthe sameas ğ‘“ğ‘–exceptthat
the recurrence is rewritten as follows:
ğ‘“max
ğ‘–/parenleftbigğ´ğ‘–,ğ´/prime
ğ‘–/parenrightbig=max/parenleftBig
ğ‘“max
ğ‘–/parenleftBig
ğ´ğ‘–âˆª{(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)},ğ´/prime
ğ‘–/parenrightBig
,
ğ‘“max
ğ‘–/parenleftBig
ğ´ğ‘–,ğ´/prime
ğ‘–âˆª{(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)}/parenrightBig/parenrightBig(25)
The function ğ‘“ğ‘–is similar to a function ğ‘“ğ‘šğ‘ğ‘¥
ğ‘–. Its purpose is to
calculatethemaximumprobability ğ‘ğºğ‘–(ğºğ‘–)amongall ğºğ‘–satisfying
a certain logical formula. Specifically, for each ğºğ‘–that meets the
logical conditions, the function ğ‘“ğ‘šğ‘ğ‘¥
ğ‘–computes the corresponding
probability ğ‘ğºğ‘–(ğºğ‘–),andthenidentifiesthemaximumvalueamong
these probabilities.
iii)Sampling:Inthegivencondition ğ¶ğ‘–,wewishtosample ğºğ‘–from
ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–(ğ¶ğ‘–=1).ğµğ‘–,ğ‘—âŠ†ğ¶ğ‘–contains unprocessed context sample
points ğ·ğ¶
ğ‘–.
Then,Wehavedependencieson ğµğ‘–,ğ‘—andcan sequentially eval-
uate the pending sample points (ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)âˆˆğ·ğ¶
ğ‘–. We sample from
ğ·ğ¶
ğ‘–using conjunctions represented by ğ¶ğ‘–=ğµğ‘–,1âˆ§ğµğ‘–,2âˆ§...ğµğ‘–,ğ‘—.
Thisapproachenablesprocessingcontextsamplepoints (ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)
inğ¶ğ‘–based on context dependencies, with each step considering
previouslyprocessed samples.Thefollowing canbeexpressed as:
ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–/parenleftbigğµğ‘–,ğ‘—=1|ğ‘‘ğ‘–=1,ğ¶ğ‘–=1/parenrightbig
=ğ‘ğ·ğ¶ğ‘ 1,ğ·ğ¶
ğ‘–/parenleftbigğµğ‘–,ğ‘—=1|ğ‘‘ğ‘–âˆ§ğ¶ğ‘–=1/parenrightbig (26)
where ğ‘‘ğ‘–=ğµ1,2âˆ§Â¬ğµ1,3âˆ§...ğµğ‘–âˆ’1,ğ‘—.ğ‘‘ğ‘–representsthesetofpreviously
processed context sample points (Whether it is a parent class).
A.4 Interpretability of Tree-Structured Neural
Processes
Thissectiondemonstrateshowourmodelcomputesinterpretability
for the average causal effects in the hierarchical linear symbolized
tree-structuredneuralprocesses.Thecomputationof ğµğ¶ğ¸((ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—),
(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–))differs from the traditional approach that is limited to
only leaf nodes. We construct a tree-structured probabilistic graph,encompassing all relationships between context sample points
(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)and(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—).
Proposition 4. In constructing a hierarchical linear symbolized
network, interpretability is achieved as all pairwise sample points
contribute to the BCE through coefficient paths.
Proof.In a hierarchical linear symbolized tree-structured net-
work,allnodescanbeassociatedwithcontextsamplepoints (ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 2)
andrepresentthedistributioncomputationoverthemarginal ğºğ‘ 2.
We define causal effects as follows:
ğ¶ğ¼ğ‘–,ğ‘—=/summationdisplay.1
ğ¿ğ‘ƒâˆˆğ·ğ¶ğ‘ ğ‘š\{(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–),(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)})ğ›¾{ğ‘–,ğ¿ğ‘ƒ1}ğ›¾{ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|,ğ‘—}
|ğ¿ğ‘ƒ|âˆ’1/productdisplay.1
ğ‘–=1ğ›¾{ğ¿ğ‘ƒğ‘–,ğ¿ğ‘ƒğ‘–+1}(27)
where ğ›¾{ğ‘–,ğ¿ğ‘ƒ1}represents the weight of the edge from context sam-
ple points (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)to(ğ‘¥ğ¶
ğ¿ğ‘ƒ1,ğ‘¦ğ¶
ğ¿ğ‘ƒ1),and ğ›¾{ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|,ğ‘—}represents the
weight of the edge from (ğ‘¥ğ¶
ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|,ğ‘¦ğ¶
ğ¿ğ‘ƒ|ğ¿ğ‘ƒ|)to(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—).
We define the Bayesian causal effects of the tree structure in the
form of expectations:
BCE((ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–),(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—))/definesEğºâˆ¼ğ‘ğœ™(ğº)/bracketleftbig
Eğ›¾âˆ¼ğ‘(ğ›¾|ğº)/bracketleftbig
ğ¶ğ¼ğ‘–,ğ‘—(ğ›¾)/bracketrightbig/bracketrightbig
(28)
Basedontheaboveresults,wenowneedtocompletethestructural
decomposition calculationof ğµğ¶ğ¸(ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)for thehierarchical lin-
ear symbolized tree-structured Network. At a ğ‘‡symbol sum node,
the weights of its child nodes are represented as ğ›¾1, ...,ğ›¾ğ‘–.
BCE((ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–),(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—))/definesEğºâˆ¼ğ‘ğœ™(ğº)/bracketleftbig
Eğ›¾âˆ¼ğ‘(ğ›¾|ğº)/bracketleftbig
ğ¶ğ¼ğ‘–,ğ‘—(ğ›¾)/bracketrightbig/bracketrightbig
=/summationdisplay.1
ğ‘–ğ›¾ğ‘–Eğºâˆ¼ğ‘ğœ™(ğº)/bracketleftbig
Eğ›¾âˆ¼ğ‘(ğ›¾|ğº)/bracketleftbig
ğ¶ğ¼ğ‘–,ğ‘—(ğ›¾)/bracketrightbig/bracketrightbig
=/summationdisplay.1
ğ‘–ğ›¾ğ‘–BCE(ğ‘–, ğ‘—)
(29)
We represent linear path information in a weighted manner.
If the node is a product node ğ‘€, it processes two child nodes,
each handling context-associated subsets (ğ·ğ¶ğ‘ 1,ğ·ğ¶ğ‘ 21)and(ğ·ğ¶ğ‘ 1âˆª
ğ·ğ¶ğ‘ 21,ğ·ğ¶ğ‘ 22)respectively. We now consider three different cases, de-
pending on the partition of (ğ‘¥ğ¶
ğ‘–,ğ‘¦ğ¶
ğ‘–)and(ğ‘¥ğ¶
ğ‘—,ğ‘¦ğ¶
ğ‘—)into distinct
subsets.
 
2829