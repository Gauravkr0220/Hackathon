Reimagining Graph Classification from a Prototype View with
Optimal Transport: Algorithm and Theorem
Chen Qian
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
qianchen2022@ruc.edu.cnHuayi Tang
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
huayitang@ruc.edu.cn
Hong Liang
School of Electronic and Computer Engineering
Peking University
Shenzhen, China
lho@stu.pku.edu.cnYong Liuâˆ—
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
liuyonggsai@ruc.edu.cn
Abstract
Recently, Graph Neural Networks (GNNs) have achieved inspiring
performances in graph classification tasks. However, the message
passing mechanism in GNNs implicitly utilizes the topological infor-
mation of the graph, which may lead to a potential loss of structural
information. Furthermore, the graph classification decision process
based on GNNs resembles a black box and lacks sufficient trans-
parency. The non-linear classifier following the GNNs also defaults
to the assumption that each class is represented by a single vector,
thereby limiting the diversity of intra-class representations.
To address these issues, we propose a novel prototype-based
graph classification framework that introduces the Fused Gromov-
Wasserstein (FGW) distance in Optimal Transport (OT) as the simi-
larity measure. In this way, the model explicitly exploits the struc-
tural information on the graph through OT while leading to a
more transparent and straightforward classification process. The
introduction of prototypes also inherently addresses the issue of
limited within-class representations. Besides, to alleviate the widely
acknowledged computational complexity issue of FGW distance cal-
culation, we devise a simple yet effective NN-based FGW distance
approximator, which can enable full GPU training acceleration
with a marginal performance loss. In theory, we analyze the gen-
eralization performance of the proposed method and derive an
O(1
ğ‘)generalization bound, where the proof techniques can be
extended to a broader range of prototype-based classification frame-
works. Experimental results show that the proposed framework
achieves competitive and superior performance on several widely
used graph classification benchmark datasets. The code is avaliable
at https://github.com/ChnQ/PGOT.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671696CCS Concepts
â€¢Computing methodologies â†’Neural networks; â€¢Theory
of computationâ†’Sample complexity and generalization
bounds.
Keywords
Graph Neural Networks; Optimal Transport; Generalization Bound
ACM Reference Format:
Chen Qian, Huayi Tang, Hong Liang, and Yong Liu. 2024. Reimagining
Graph Classification from a Prototype View with Optimal Transport: Al-
gorithm and Theorem. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671696
1 Introduction
Graph classification is a ubiquitous and significant task across ap-
plication domains ranging from Bioinformatics and Chemoinfor-
matics [ 2,40,45] to social network analysis [ 34,55], to name a
few. In recent years, numerous approaches have been proposed
for graph classification. In the early stage, a series of kernel-based
algorithms [ 1,47â€“49] have been proposed to solve the graph classi-
fication task by designing graph kernels that leverage the topologi-
cal properties of the observed graphs. Although the kernel-based
methods are well-defined and easy to train, the expressive power
of which is severely limited due to the hand-crafted input fea-
tures [ 16,52]. Moreover, the graph kernel design relies heavily on
prior human knowledge. In contrast, as a kind of deep learning-
based method, Graph Neural Networks (GNNs) [ 29,41,70,75] are
becoming a dominant paradigm for modeling graph classification
tasks, offering unprecedented expressive power. Most of the cur-
rent GNNs follow a message-passing scheme [20] where the node
representation is learned by aggregating and transforming from
its neighbors to capture the attributes and topological information.
After that, a graph-level representation is obtained via a READOUT
function (a simple permutation invariant function such as summa-
tion or a more sophisticated graph-level pooling [ 6,39,70])w.r.t.
the node embeddings and then feed to another dense layer, e.g.,an
MLP, for a downstream graph classification scenario.
 
2444
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Chen Qian, Huayi Tang, Hong Liang, and Yong Liu
3.2Â·Â·Â·
3.5Â·  Â·  Â·
5.7Â·Â·Â·
6.9-softmaxÂ·Â·Â· Â·Â·Â·Probability
FGW3.2 Â·  Â·  Â·
5.1 min
distancemin
GNN Â·  Â·  Â·Prototype11 Prototype1K
PrototypeC1 PrototypeCKÂ·Â·Â·
Â·Â·Â·Class 1
Class C
Figure 1: The classification procedure of PGOT.
Despite the inspiring success that has been achieved by GNNs,
however, they may share the following potential weaknesses. First,
few of them exploit the structural information on the graph explic-
itly. One can see that in GNNs, the topological information of the
graph is only implicitly utilized in the neighbor aggregation process
of the message-passing scheme. Besides, in practical applications,
due to the problem of over-smoothing [ 36], GNNs often cannot
stack too many layers. As such, the topological receptive field of
each node in GNNs is limited. Second, the non-linear classifier
followed by the GNN module naturally ignores the diversity of
intra-class features, thereby limiting the modelâ€™s expressive power.
Taking the widely utilized MLP as an example, its last layer maps
ğ‘‘-dimensional features to ğ¾classes, suggesting that each class
could be associated with a ğ‘‘-dimensional latent embedding [ 64,78].
However, this form of single vector-based prototype representation
may not fully capture the diverse characteristics within each class.
Third, the aforementioned embedding-based graph classification
paradigm may lack transparency. For one thing, this classification
decision process tends to be more opaque, lacking a mechanism to
offer clear rationales for its predictions [ 77]. For another thing, it
is challenging to distinguish whether the model has learned better
node feature representations or a more powerful classifier.
Tackling these issues can provide new insights into graph classi-
fication model design. Motivated by the first weakness, the Fused
Gromov-Wasserstein (FGW) distance [ 56] in Optimal Transport
(OT) can be introduced as a helpful tool to provide meaningful dis-
tances between graphs by capturing the nodesâ€™ attributed features
and topological structure information simultaneously. By comput-
ing an optimal matching of topological connectivity between two
graphs globally, FGW distance explicitly exploits the geometrical
properties of the graph. Driven by the last two issues, we are moti-
vated to reconsider the paradigm of graph classification from the
perspective of prototypes. The concept of prototype-based classifi-
cation is well-established and more in line with human cognitive
intuition [ 31,51], in which data samples are classified based on their
similarity to the representative prototypes of classes [ 78]. Prototype-
based classification inherently possesses the advantage of simplicity
and transparency, which can also effectively address the limitation
of single-modal representation within classes. Note that, although
recent studies [ 8,62] have incorporated the OT distance into GNNs,
they still face the last challenge and lack a theoretical insight into
graph classification tasks.In light of the above discussion, in this paper, we propose a novel
graph classification framework named Prototype-based Graph clas-
sification via Optimal Transport (PGOT), which blends OT and
prototype-based classification elegantly in a synergy. Concretely,
by dismantling the final non-linear classifier, PGOT performs predic-
tion on a new input graph based on its similarity to the prototypes,
where the similarity is measured under the FGW metric space. For
the selection and updating scheme of prototypes, we provide two
possible solutions. On the one hand, we randomly sample graphs
from each class of the given training sets and never update them
during the training procedure. On the other hand, we instantiate
the prototypes as FGW barycenters and update them via a mo-
mentum mechanism. To verify the effectiveness and rationality
of the proposed method, in theory, we study the generalization
performance of the PGOT from the perspective of algorithmic sta-
bility, deriving anO(1
ğ‘)generalization upper bound. Notably, our
proof techniques can be generalized to a broader range of sce-
narios, providing new insights for the generalization analysis of
prototype-based classification algorithms. Moreover, to address the
high computational complexity issue of FGW distance calculation,
we design an FGW distance approximator based on simple siamese
neural networks to replace the typical FGW solver [ 18], which
can benefit from complete GPU acceleration only with a slight
performance loss.
To conclude, PGOT enjoys the following advantages. First, bene-
fiting from the introduction of FGW distance, the proposed method
explicitly utilizes topological structure information on the global
graph. Second, the prototype-based classification procedure makes
the classification process more transparent, and multiple prototypes
for each class improve the diversity of intra-class representations.
Also, decoupling the final nonlinear classifier allows only the GNN
parameters to be learned, which enables the model to focus more
on learning powerful features. Third, to the best of our knowledge,
we have introduced the first NN-based FGW disance approximator,
which significantly alleviates the computational overhead associ-
ated with FGW distance calculations and can serve as an effec-
tive alternative to the traditional FGW solver. Fourth, theoretical
guarantees ensure that our algorithm has a good generalization
performance, where the proof techniques may shed new insight
into the generalization analysis of the prototype-based classifica-
tion paradigm. Comprehensive experiments on real-world datasets
demonstrate the effectiveness of our framework.
 
2445Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
2 Related Work
In this section, we provide a brief overview of related work here
due to space constraints, and we encourage readers to refer to the
Appendix.A1for more detailed information.
Graph classification. Graph classification is a significant task with
practical applications in many different domains. Several works
in the literature have studied the graph classification problem by
designing graph kernels that leverage topological properties of the
observed graphs-based subgraphs [ 24,49], subtree patterns [ 44,47],
walks and paths on the graph [ 1,28] and so on. For instance, the
work [ 48] proposes the Weisfeiler-Lehman kernel by iteratively
aggregating the features of nodesâ€™ neighbors. In recent years, graph
neural networks (GNNs) have shown excellence in graph mining.
They are widely applied to graph classification tasks [ 41,59,70,72,
75], which broadly follow a recursive neighborhood aggregation
(or message passing) scheme, where each node aggregates feature
vectors of its neighbors to compute its new feature vector.
OT-based graph learning. Recently, the theory of Optimal Trans-
port (OT) [ 46,60] has been used as a tool in graph machine learn-
ing algorithms to consider the geometry of the data. For example,
Gromov-Wasserstein (GW) distance [ 38] extends Wasserstein dis-
tance [ 27,57] to consider edges transferring between graphs and
has been performed in graph matching and partition [ 67,69], un-
supervised graph dictionary learning [ 61,63,66] and barycenter
estimation [ 43,68]. Motivated by the previous work, the Fused
Gromov-Wasserstein (FGW) distance [ 56] has been proposed re-
cently as a trade-off combination of the Wasserstein distance and
GW distance. By associating each graph node with a discrete prob-
ability measure, OT can be applied to many downstream tasks such
as graph classification [8, 33, 62, 76].
Prototype learning. Prototype learning is a form of case-based
reasoning [ 32] representing classes by prototypes and then making
predictions for new instances by comparing them with the proto-
types [ 77]. The earliest prototype learning originated from the near-
est neighbors rule [ 12], and while ğ¾-nearest neighbors do not use
prototypes, it is similar to prototype methods like ğ¾-means cluster-
ing [22]. In the past few decades, prototype learning has been widely
applied to unsupervised learning [ 7,35,65], supervised classifica-
tion, few-shot learning [ 50], and semantic segmentation [ 37,78],
to name a few. More relevant to the task concerned in this article,
several works have proposed combining prototype learning with
GNN, concentrating on the interpretability of GNNs [13, 77].
3 Preliminary
In this section, we first give the notations used in this paper. Then
we briefly introduce the FGW distance and FGW barycenter.
3.1 Notations
Letğ‘¨(ğ‘–)andğ‘¨(ğ‘–,ğ‘—)denote theğ‘–-th row and(ğ‘–,ğ‘—)-th entry of ma-
trixğ‘¨, respectively. Let ğºdenote an undirected attributed graph
composed of a node set ğ‘‰and an edge set ğ¸with|ğ‘‰|standing for
the number of nodes. A graph ğºis represented by a triplet (ğ‘ª,ğ‘­,ğ’‰),
where ğ‘ª=ğ‘ªâŠ¤âˆˆR|ğ‘‰|Ã—|ğ‘‰|contains the topological information of
the graph, e.g.,the adjacency matrix or the shortest-path-distance
1Due to space limitations, the appendix of this paper is available at https://github.com/
ChnQ/PGOT.matrix; ğ‘­âˆˆR|ğ‘‰|Ã—ğ‘‘is the node feature matrix with ğ‘‘standing for
the dimension of latent space; ğ’‰âˆˆR|ğ‘‰|
+is a vector satisfying the
probability distribution, which indicates the relative importance
of each node in the graph. Empirically, ğ’‰is determined by prior
knowledge. In this work, we fix ğ’‰to a uniform distribution for the
sake of simplicity. In this paper, we focus on the graph classification
task, in which a set of labeled graphs D={ğºğ‘–,ğ‘¦ğ‘–}|D|
ğ‘–=1with totally
ğ¶classes will be given, where |D|is the size of the training set and
ğ‘¦ğ‘–âˆˆ{1,...,ğ¶}stands for the label of graph ğºğ‘–.
3.2 FGW distance
Considering two graphs represented by ğº1=(ğ‘ª1,ğ‘­1,ğ’‰1),ğº2=
(ğ‘ª2,ğ‘­2,ğ’‰2)and with the number of nodes |ğ‘‰1|,|ğ‘‰2|respectively.
Theğ‘-order FGW distance [56] is defined as:
ğ‘‘ğ¹ğºğ‘Š ğ›¼,ğ‘(ğº1,ğº2)= min
ğ…âˆˆÎ (ğ’‰1,ğ’‰2)âˆ‘ï¸
ğ‘–,ğ‘˜,ğ‘—,ğ‘™
(1âˆ’ğ›¼)âˆ¥ğ‘­1(ğ‘–)âˆ’ğ‘­2(ğ‘—)âˆ¥ğ‘
+ğ›¼|ğ‘ª1(ğ‘–,ğ‘˜)âˆ’ğ‘ª2(ğ‘—,ğ‘™)|ğ‘
ğ…ğ‘–,ğ‘—ğ…ğ‘˜,ğ‘™,
(1)
whereğ‘–,ğ‘˜âˆˆ{1,...,|ğ‘‰1|},ğ‘—,ğ‘™âˆˆ{1,...,|ğ‘‰2|},
Î (ğ’‰1,ğ’‰2)={ğ…âˆˆR|ğ‘‰1|Ã—|ğ‘‰2|
+ğ‘ .ğ‘¡.ğ…1|ğ‘‰2|=ğ’‰1,ğ…1|ğ‘‰1|=ğ’‰2}(2)
is the set of all possible coupling, the matrix ğ…describes a proba-
bilistic matching of the nodes of the two graphs, which is called
the transport plan with ğ…âˆ—standing for the optimal transport plan
(let the Eq. (1) reach the minimum), the ğ›¼âˆˆ[0,1]is the trade-off
parameter that governs the contribution of the Wasserstein cost
and GW cost.
3.3 FGW barycenter
Based on the FGW distance defined before, the ğ‘-order FGW barycen-
ter [56] is formulated as
ğº=arg min
ğºÃ
ğ‘–ğœ†ğ‘–ğ‘‘FGW ğ›¼,ğ‘
ğºğ‘–,ğº
, (3)
where{ğœ†ğ‘–âˆˆ(0,1)}|D|
ğ‘–=1denotes the predefined weights of observed
graphs,ğºis the barycenter graph with a predefined number of
nodes. Without prior knowledge, the {ğœ†ğ‘–}|D|
ğ‘–=1is endowed with
uniform weights. The barycenter graph works as a â€œreferenceâ€
connecting with the observed graphs by minimizing the weighted
average to observed graphs aligned by their FGW distances, which
inherits the good properties of FGW, e.g., the invariance under
isomorphism [5].
3.4 Prototype-based Classification
We recap here the process of prototype-based classification. Given
a datasetğ·={ğ‘¥ğ‘–}ğ‘›
ğ‘–=1belongs toğ¶classes where each class is
represented by ğ¾prototypes. We denote ğ‘ğ‘ğ‘˜to be theğ‘˜-th proto-
type of theğ‘-th class. Each individual data sample ğ‘¥ğ‘–is associated
with a label ğ‘¦ğ‘–âˆˆ{1,...,ğ¶}. Let Sim(ğ‘,ğ‘)denotes a similarity be-
tween sample ğ‘andğ‘. Recall the exemplar-driven idea of prototype
learning, the classification result for sample ğ‘¥ğ‘–can be formulated
as
Ë†ğ‘¦ğ‘–=arg max
ğ‘âˆˆ[ğ¶]
max
ğ‘˜âˆˆ[ğ¾]Sim(ğ‘¥ğ‘–,ğ‘ğ‘ğ‘˜)
, (4)
 
2446KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Chen Qian, Huayi Tang, Hong Liang, and Yong Liu
FGW distance 
approximation
GCN
GCNshare2 Prototype
InputshareMLP
MLP
*
*
*
**
*
*
*
2*
*
*
**
*
*
*Ã—1âˆ’ğ›¼
Ã—ğ›¼
Figure 2: Architecture diagram for the FGW Approximator.
or an average form:
Ë†ğ‘¦ğ‘–=arg max
ğ‘âˆˆ[ğ¶](
1
ğ¾ğ¾âˆ‘ï¸
ğ‘˜=1Sim(ğ‘¥ğ‘–,ğ‘ğ‘ğ‘˜))
. (5)
Intuitively, the input sample is endowed with the same label as the
closest prototype(s). With this classification process in mind, we
continue the following discussion.
4 Methodology
In this section, we will first give an overview of PGOT. Then we
elaborate on the similarity measure and prototype instantiation
of the prototype-based classification. After that, the formal graph
classification procedure of PGOT will be described.
4.1 Overview
Figure 1 demonstrates the main classification procedure of PGOT.
As plotted, we take the classification of organic compounds in
chemistry as an example. Organic compounds can be broadly cate-
gorized into cyclic compounds (closed chain, Class 1) and acyclic
compounds (open chain, Class 2).
Given an input graph belongs to cyclic compounds, we first pass
it through a GNN to obtain latent node representations and then
compute its FGW distance to all ğ¶Â·ğ¾prototypes ( ğ¶classes totally,
per classğ¾prototypes). Subsequently, we assign the input graph
to the class of the prototype with the closest FGW distance. Finally,
we optimize the model parameters through supervised loss.
One can see that the proposed PGOT enjoys a simple and trans-
parent classification procedure thanks to the introduction of pro-
totypes. Nevertheless, throughout the framework, only the GNN
module and the hyperparameter ğ›¼in Eq. (1) are set to be learnable,
which enables PGOT to focus on learning better node represen-
tations without being influenced by the parametric classifier. The
visualizations of the features in Appendix.D may serve as evidence
to support our claim. In the upcoming parts of this section, we
will deconstruct our framework and provide detailed explanations
individually.
4.2 Similarity Measure
In order to mine explicit topological characteristics, FGW distance [ 56]
is introduced as the similarity measure in PGOT. Formulated by
Eq. (1), FGW distance defines a metric when ğ‘=1and a semi-metric
whenğ‘>1for the attributed graph [ 58, Theorem 3.2]. To simplifythe notations, we denote ğ‘‘ğ¹ğºğ‘Š ğ›¼,ğ‘asğ‘‘ğ¹ğºğ‘Š in the following. Fur-
ther, the FGW distance has a good property of the invariance under
isomorphism ( ğ‘‘ğ¹ğºğ‘Š =0implies that the two graphs are isomor-
phic) [ 58], which is of great significance for the discrimination of
protein isomers in the Biochemical field.
FGW Approximator. The computation of FGW distance often en-
tails significant time overhead. At present, the most widely adopted
technique for FGW distance computation relies on [ 56, Algorithm 1].
This iterative method exhibits a time complexity of ğ‘‚(ğ‘š2ğ‘›+ğ‘šğ‘›2)
for a critical sub-problem under the specific setting (i.e., p=2, which
is also the practical configuration used in this work ), where ğ‘š,ğ‘›
denotes the number of nodes and edges, respectively. Moreover,
the official toolkit [ 18] designed for this solution operates on CPU
architecture [ 62]. This means that, despite endeavors to enhance
computational speed through multi-threading on CPUs, it continues
to face challenges concerning overall performance.
To alleviate this issue, we design a Siamese NN-based FGW dis-
tance approximator as shown in Figure 2, which can be regarded
as an extension of [ 11] in structural data. Specifically, the input
graphs are first passed through a Siamese MLP and a Siamese GCN
to obtain node features representation and graph structural rep-
resentation, respectively. Then, the Euclidean distance between
the representations is computed to approximate the Wasserstein
distance and the GW distance, respectively. Notably, this Euclidean
distance-based distance approximation serves to preserve the nat-
ural symmetry of the FGW distance. Finally, under the control of
the parameter ğ›¼, the ultimate approximation of the FGW distance
is obtained. One can first pre-train the FGW approximator on a
specific dataset and then surrogate the standard FGW solver [ 18]
the actual utilization.
In contrast to the traditional iterative-based solver, our FGW Ap-
proximator could fully benefit from GPU acceleration, significantly
alleviating the efficiency concerns related to FGW distance compu-
tation. As we will see in Section 6, the approximator significantly
speeds up the training process with slight performance loss.
4.3 Prototype Instantiation
Fixed prototypes. A straightforward and simple solution for se-
lecting prototypes is to directly sample graphs within each class
from the training set. Specifically, we specify the size (number of
nodes) of prototype graphs to be the median size of the training
set. Then, we randomly sample ğ¾graphs that meet the size require-
ment from each class to serve as prototypes and never update them.
As we will see later in Section 6, interestingly, this simple proto-
type setting scheme could achieve comparable and even superior
performance to modern GNNs.
Updated prototypes. Due to the introduction of FGW distance,
we are motivated to instantiate the prototypes as FGW barycen-
ters. To emphasize the diversity of the intra-class representation,
barycenters under each class should reflect different attributes. A
natural way is to control the trade-off coefficient ğ›¼in Eq. (3) to ini-
tialize different barycenters, i.e.,the largerğ›¼is, the more topology
information the prototype will contain while paying less attention
to the node features.
Concretely, denote by ğœ¶ğµğ¶={ğ›¼1,...,ğ›¼ğ¾}a customized ğ›¼list
for barycenters, we initialize ğ¾FGW barycenters for each class
 
2447Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
denoted as{ğºğ‘ğ‘˜}ğ¾
ğ‘˜=1, where theğºğ‘ğ‘˜is computed by Eq. (3) with ğ›¼ğ‘˜.
To utilize the latest advancements in node feature representations,
we propose to update the barycenters by aggregating the latest
learned features via a momentum mechanism. Formally, Let Î¦ğ‘¡=n
(ğ‘ª(ğ‘¡)
11,ğ‘­(ğ‘¡)
11,ğ’‰(ğ‘¡)
11),...,(ğ‘ª(ğ‘¡)
ğ¶ğ¾,ğ‘­(ğ‘¡)
ğ¶ğ¾,ğ’‰(ğ‘¡)
ğ¶ğ¾)o
stand for all barycenter
parameters at iteration ğ‘¡, then the barycenter update strategy is
formulated as
Î¦ğ‘¡=ğ›½Î¦ğ‘¡âˆ’1+(1âˆ’ğ›½)Î¦â€²
ğ‘¡, (6)
whereğ›½is the customized coefficient, Î¦â€²
ğ‘¡denotes the FGW barycen-
ters computed by Eq. (3) with the latest node features in ğ‘¡-th itera-
tion. As such, barycenters are dynamically updated based on the
latest node features to better describe the intrinsic properties and
topological characteristics of input graphs.
Algorithm 1: Training Procedure of PGOT 1(fixed proto-
type)
Input: Training setDğ‘†={(ğºğ‘–,ğ‘¦ğ‘–)}ğ‘
ğ‘–=1, GNNğ‘“ğœ”(Â·),ğ›¼in
Eq. (1), total classes ğ¶, #prototypes per class ğ¾, max
iterationsğ‘‡
Output: Theğ¶Â·ğ¾prototypes{ğºğ‘“ğ‘–ğ‘¥
11,...,ğºğ‘“ğ‘–ğ‘¥
ğ¶ğ¾},ğ‘“ğœ”(Â·),ğ›¼
1Randomly sample ğ¾graphs with the median size of Dğ‘†
from each class to serve as prototypes, obtaining
{ğºğ‘“ğ‘–ğ‘¥
11,...,ğºğ‘“ğ‘–ğ‘¥
ğ¶ğ¾};
2forğ‘¡=1toğ‘‡do
3 formini-batch dataDğ‘–inDğ‘†do
4 Employing GNN ğ‘“ğœ”(Â·)to process all graphs in Dğ‘–to
generate latent node embeddings;
5 Computeğ‘‘ğ¹ğºğ‘Š(ğºğ‘—,ğºğ‘“ğ‘–ğ‘¥
ğ‘ğ‘˜)between each graph
ğºğ‘—âˆˆDğ‘–and each barycenter
ğºğ‘“ğ‘–ğ‘¥
ğ‘ğ‘˜âˆˆ{ğºğ‘“ğ‘–ğ‘¥
11,...,ğºğ‘“ğ‘–ğ‘¥
ğ¶ğ¾}withğ›¼by Eq. (1);
6 Get the predicted label by Eq. (5);
7 Compute cross-entropy loss Lbetween predicted
labels and true labels;
8 Optimize the trainable parameters ğœ”,ğ›¼via standard
backpropagation to minimize L;
4.4 Prototype-based Classification Procedure
With the aforementioned discussion in mind, we now formally
elaborate on the graph classification procedure of the PGOT. Given
an observed graph ğºğ‘–to be classified with original features ğ‘¿ğ‘–,
along withğ¶Â·ğ¾prototypes{ğºğ‘
11,...,ğºğ‘
ğ¶ğ¾}instantiated using the
method described in Subsection 4.3. The input graph will first pass
through a GNN ğ‘“ğœ”(Â·)parameterized by ğœ”to obtain latent node
representations ğ‘­ğ’Š=ğ‘“ğœ”(ğ‘¿ğ’Š). Then, the prediction label of the ğ‘–-th
graph is formulated as
Ë†ğ‘¦ğ‘–=arg min
ğ‘âˆˆ[ğ¶]
min
ğ‘˜âˆˆ[ğ¾]ğ‘‘ğ¹ğºğ‘Š
ğºğ‘–(ğ‘­ğ‘–),ğºğ‘
ğ‘ğ‘˜
, (7)
whereğ‘‘ğ¹ğºğ‘Š is defined in Eq. (1), ğºğ‘–(ğ‘­ğ‘–)denotes the graph with
latent node features. Intuitively, the label of the class with the clos-
est FGW distance to the observation graph is taken as the final
classification result. Finally, the whole framework is optimized ina supervised end-to-end fashion by the Cross-Entropy loss. The
overall learning procedures are summarized in Algorithm 1 and
Algorithm 2 (corresponding to fixed prototypes and updated proto-
types described in Subsection 4.3, respectively).
Complexity Analysis. The computational complexity of the pro-
posed algorithms is primarily concentrated on the FGW distance
calculation. For simplicity, we assume both input graphs have ğ‘›
nodes,ğ‘’edges, and a feature dimension of ğ‘‘. Then we have the
following computational complexities:
â€¢PGOT with OT Solver [ 56, Algorithm 1]: ğ‘‚(ğ‘›ğ‘–ğ‘¡ğ‘’ğ‘Ÿ(ğ‘›3+ğ‘›2ğ‘‘)),
whereğ‘›ğ‘–ğ‘¡ğ‘’ğ‘Ÿrepresents the number of iterations in the algorithm.
â€¢PGOT with FGW Approximator: ğ‘‚(ğ‘’ğ‘‘+ğ‘›ğ‘‘2).
From the perspective of graph scale, we can see that PGOT with OT
Solver scales with ğ‘‚(ğ‘›3)whereas PGOT with FGW Approximator
scales withğ‘‚(ğ‘›+ğ‘’). In practice, note that the OT solver primarily
operates iteratively on the CPU, while the FGW Approximator could
benefit from the acceleration of GPU parallel computing. Table 3
records the differences in actual runtime speeds between the two,
which implies that the proposed method with FGW Approximator
is capable of extending to large-scale graphs (see Table 2).
Algorithm 2: Training Procedure of PGOT 2(updated pro-
totype)
Input: Training setDğ‘†={(ğºğ‘–,ğ‘¦ğ‘–)}ğ‘
ğ‘–=1, GNNğ‘“ğœ”(Â·),ğ›¼in
Eq. (1),ğ›½in Eq. (4), total classes ğ¶, #prototypes per
classğ¾,ğœ¶ğµğ¶={ğ›¼1,...,ğ›¼ğ¾}, max iterations ğ‘‡
Output: Theğ¶Â·ğ¾prototypes{ğº11,...,ğºğ¶ğ¾},ğ‘“ğœ”(Â·),ğ›¼
1Initialize the ğ¶Â·ğ¾prototypes{ğº11,...,ğºğ¶ğ¾}onDğ‘†with
ğœ¶ğµğ¶by Eq. (3) ;
2forğ‘¡=1toğ‘‡do
3 formini-batch dataDğ‘–inDğ‘†do
4 Employing GNN ğ‘“ğœ”(Â·)to process all graphs in Dğ‘–to
generate latent node embeddings ;
5 Computeğ‘‘ğ¹ğºğ‘Š(ğºğ‘–,ğºğ‘ğ‘˜)between each graph
ğºğ‘—âˆˆDğ‘–and each barycenter
ğºğ‘ğ‘˜âˆˆ{ğº11,...,ğºğ¶ğ¾}withğ›¼by Eq. (1) ;
6 Get the predicted label by Eq. (5) ;
7 Compute cross-entropy loss Lbetween predicted
labels and true labels ;
8 Optimize the trainable parameters ğœ”,ğ›¼via standard
backpropagation to minimize L;
9 Update the prototypes {ğº11,...,ğºğ¶ğ¾}with ğœ¶ğµğ¶by
Eq. (4) ;
5 Theoretical Analysis
To evaluate the rationality of the proposed method theoretically, in
this section, we study the PGOT from the perspective of generaliza-
tion. We first briefly describe some symbol conventions, then detail
several assumptions throughout this section, and finally present
our main results.
 
2448KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Chen Qian, Huayi Tang, Hong Liang, and Yong Liu
5.1 Preliminary
Studying algorithm generalization is crucial for ensuring that ma-
chine learning models can perform well on new and unseen data
beyond the training set. In the following, we employ algorithmic sta-
bility tools [ 3,4,21,30] to analyze the generalization performance
of the proposed method.
Denote byXthe instance space and Ythe output space, let
ğ‘†={(ğ‘¥ğ‘–,ğ‘¦ğ‘–)}ğ‘
ğ‘–=1be a training set from some space Z=XÃ—Y
withğ‘§ğ‘–=(ğ‘¥ğ‘–,ğ‘¦ğ‘–)the instance-label pair. Let A:Zâ†¦â†’Y stands
for the learning algorithm and A(ğ‘†)be the output of the learning
algorithm run on dataset ğ‘†. Without loss of generality, we are going
to analyze the generalization of Algorithm 1. Assume we take the
GNN in Algorithm 1 as a 1-layer GCN [ 29] denoted as ğœ(eğ‘¨ğ‘¿ğ‘¾),
whereğœis the ELU activation function [ 10] with hyperparame-
terğ›¾,eğ‘¨represents the normalized adjacency matrix of the input
graph, ğ‘¿is the original features of the input graph, and ğ‘¾is the
parameters of GCN. Besides, we set one prototype per class, i.e.,
totallyğ¶prototypical graphs, each with initial latent node features
ğ‘·ğ‘,ğ‘âˆˆ[ğ¶]. Letğ‘š1,ğ‘š2represent the maximum number of nodes of
input graphs and prototypical graphs, respectively. Suppose we run
ğ‘‡iterations with monotonically non-increasing step sizes ğ›¼ğ‘¡â‰¤ğ‘Ÿ/ğ‘¡
for the Stochastic Gradient Method [ 21] to optimize the model. The
loss function â„“:YÃ—Yâ†¦â†’ R+is non-negative. Then the popula-
tion risk and empirical risk are defined as ğ‘…(A(ğ‘†))=Eğ‘§[â„“(A,ğ‘§)],
ğ‘…ğ‘†(ğ´(ğ‘†))=1
ğ‘Ãğ‘
ğ‘–=1[â„“(Ağ‘†,ğ‘§ğ‘–)], respectively. As such, the general-
ization gap can be denoted as ğ‘…(A(ğ‘†))âˆ’ğ‘…ğ‘†(A(ğ‘†)).
5.2 Assumptions
Assumption 1. Assume that there exists a constant ğ‘ğ‘‹>0such
thatâˆ¥ğ‘¿âˆ¥â‰¤ğ‘ğ‘‹holds for all ğ‘¿âˆˆX. Assume that there exists constants
ğ‘ğ‘ƒ>0such thatâˆ¥ğ‘·ğ‘âˆ¥â‰¤ğ‘ğ‘ƒholds forğ‘âˆˆ[ğ¶].
Remark 1. Assumption 1 means that the origin features of the
input graph and the initial latent features of prototype graphs are
bounded, respectively. Widely used graph classification benchmark
datasets [ 2,14,23] naturally satisfy that the original features of
graphs are bounded. As Batch Normalization [ 26] is commonly
applied to features in modern GNN training [ 29,71], the latter
assumption is always satisfied.
Assumption 2. Let{ğ‘¾(ğ‘¡)}ğ‘‡
ğ‘¡=1denote the parameter matrices of
GNN during the iteration in Algorithm 1. Assume that there exists a
constantğ‘ğ‘Š>0such thatâˆ¥ğ‘¾âˆ¥â‰¤ğ‘ğ‘Šhold for all ğ‘¾âˆˆ{ğ‘¾(ğ‘¡)}ğ‘‡
ğ‘¡=1.
Remark 2. Assumption 2 states that the parameters of the model
during the training process are bounded [ 17]. This assumption can
be easily satisfied since L2 regularization is commonly used in
model training.
Assumption 3. Denote by ğ‘»âˆ—ğ‘the optimal transport matrix w.r.t.
input graph and the ğ‘-th prototype graph. Assume that vec ğ‘»âˆ—ğ‘sat-
isfies
vec ğ‘»âˆ—
ğ‘(ğ‘¾)âˆ’vec ğ‘»âˆ—
ğ‘(ğ‘¾â€²)â‰¤ğœ–ğ‘‡ğ‘¾âˆ’ğ‘¾â€². (8)
Remark 3. Assumption 3 implies that the output of the OT
solver should not change significantly when the input is subjected
to small perturbations. Intuitively, the stability of the solution to
the optimal transport matrix is crucial for the stability of the entirealgorithm. The experimental results in Appendix.D on the Lipschitz
continuity of the optimal transport matrix across several datasets
show that Assumption 3 is reasonable.
5.3 Main Result
In this part, we give the main theoretical results, i.e.,the general-
ization upper bound of PGOT.
Theorem 1. Suppose Assumptions 1, 2, and 3 hold. Assume the
loss function â„“âˆˆ[0,ğ‘€]is bounded. Then for any ğ‘â‰¥1, and any
ğ›¿âˆˆ(0,1), the following bound holds over the random draw of the
sampleğ‘†:
Eğ‘†Eğ´[ğ‘…(ğ´(ğ‘†))âˆ’ğ‘…ğ‘†(ğ´(ğ‘†))]â‰¤1+1/ğ›½Fğ‘Ÿ
ğ‘âˆ’1 2ğ‘Ÿğ¿2
F
ğ‘€! 1
ğ›½Fğ‘Ÿ+1
ğ‘‡ğ›½Fğ‘Ÿ
ğ›½Fğ‘Ÿ+1,
whereğ¿Fandğ›½Fare the Lipschitz constant and Smoothness constant
of the PGOT model with 1-layer GCN w.r.t. learnable parameters ğ‘¾2.
Remark 4. Theorem 1 demonstrates that the generalization
gap depends on the maximum degree of the input graph âˆ¥ğ‘¨âˆ¥âˆ,
network architecture related Lipschitz continuity constant ğ¿Fand
smoothness constant ğ›½F, iteration rounds ğ‘‡, and OT solver-related
Lipschitz continuity constant ğœ–ğ‘‡. Omitting constant factors, our
upper bound is of order ğ‘‚(1
ğ‘)under expectation. To facilitate the
analysis, we only consider a 1-layer of GCN and one prototype per
class. It is worth noting that our analysis technique can be extended
to the cases of multiple layers of GCN and multiple prototypes per
class, introducing only a constant factor change without affecting
the order of the bound.
Remark 5. It is important to note that our theoretical analy-
sis framework can be adapted to various GNNs. In the course of
our theoretical analysis, the part related to GNNs involves deriv-
ing the specific Lipschitz constant for a given GNN, which is not
intertwined with the entire proof process. Existing research that
analyzes the Lipschitz constants of GNNs [ 54] further supports
the analysis within the PGOT theoretical framework. Here, we ini-
tially choose one of the most representative GNNs, GCN [ 29], for
exemplary theoretical analysis.
Remark 6. We analyze the generalization performance of PGOT
by the tool of algorithmic stability [ 21]. To our knowledge, it is the
first generalization bound for the prototype-based graph classifica-
tion paradigm, and the proof techniques of which can be extended
to a more general prototype-based classification framework for
the generalization performance analysis. We further discuss the
distinctions and connections between Theorem 1 and the other
associated generalization bounds in the Appendix.C.
6 Experiments
In this section, we conduct extensive experiments to evaluate the
PGOT comprehensively.
6.1 Experimental Setup
Graph classification benchmarks. The experiments are first
conducted on several widely used graph classification benchmark
datasets [ 40], including MUTAG [ 14], BZR, COX2 [ 53], PROTEINS [ 2],
2Please refer to Eq. (26) and Eq. (41) in the Appendix.B
 
2449Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Test Accuracy (%, mean Â±std) of graph classification of different methods on several widely used benchmarks with the
best bolded and the runner-up underlined .
Method MUTAG BZR PTC COX2 PROTEINS IMDB-B IMDB-M
SP-Kernel 69.01 Â±5.23 77.51 Â±1.01 55.87 Â±6.19 78.72 Â±1.74 68.55 Â±3.06 38.00 Â±5.93 28.74 Â±3.94
WL-Kernel 85.26 Â±7.37 78.29 Â±1.71 52.29 Â±8.48 78.51 Â±1.15 59.82 Â±0.00 66.70 Â±3.29 48.53 Â±4.18
GCN 87.89 Â±5.29 86.83 Â±4.25 62.57 Â±4.13 84.26 Â±2.89 74.11 Â±1.79 60.60 Â±3.44 50.47 Â±4.38
GIN 83.68 Â±8.95 82.68 Â±3.53 54.86 Â±7.86 80.85 Â±5.47 72.05 Â±3.57 61.30 Â±1.85 47.93 Â±2.99
GAT 87.37 Â±4.21 83.66 Â±4.09 63.14 Â±5.92 85.11 Â±2.13 74.38 Â±1.33 61.70 Â±1.49 49.93 Â±3.09
DropGIN 85.79 Â±9.72 77.80 Â±2.55 55.14 Â±8.86 78.72 Â±0.95 55.14 Â±8.86 55.00 Â±3.71 37.33 Â±3.16
DIFFPOOL 87.50 Â±3.35 51.83 Â±2.17 45.75 Â±5.25 59.67 Â±3.32 70.60 Â±2.74 58.71 Â±1.48 42.08 Â±1.93
ChebyNet 85.79 Â±7.08 85.61 Â±3.69 60.57 Â±4.20 81.06 Â±5.58 72.68 Â±3.25 61.80 Â±2.32 50.20 Â±2.21
APPNP 87.37 Â±3.49 82.44 Â±2.13 61.71 Â±8.40 78.95 Â±0.67 71.16 Â±2.68 60.50 Â±2.69 50.13 Â±1.73
GPRGNN 91.05 Â±2.41 84.88 Â±2.39 62.57 Â±5.49 78.72 Â±0.00 68.30 Â±4.76 61.90 Â±2.84 49.80 Â±2.78
OT-GNN 94.74 Â±5.77 85.85 Â±3.75 62.00 Â±9.39 84.26 Â±2.89 72.59 Â±3.75 61.50 Â±3.77 49.00 Â±1.48
TFGW 93.68 Â±2.11 84.39 Â±2.93 61.14 Â±5.74 83.19 Â±3.08 74.29 Â±2.14 62.20 Â±2.18 50.67 Â±2.92
PGOT 1(GIN) 95.79 Â±3.16 84.15 Â±4.26 63.43 Â±7.65 85.74 Â±3.44 71.52 Â±2.27 60.60 Â±4.94 43.07 Â±5.54
PGOT 2(GIN) 92.63 Â±2.58 87.32 Â±3.90 63.43 Â±3.79 82.98 Â±5.21 73.21 Â±2.59 62.90 Â±3.05 51.33 Â±1.76
PGOT 1(GIN) + Appro. 92.11 Â±4.85 83.47 Â±4.57 62.29 Â±9.96 84.47 Â±3.30 70.76 Â±1.24 62.50 Â±3.32 48.00 Â±2.67
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048/uni00000056/uni00000003/uni00000052/uni00000051/uni00000003/uni00000025/uni0000003d/uni00000035
/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c
/uni00000024/uni00000026/uni00000026
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000017/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048/uni00000056/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000030/uni00000027/uni00000025/uni00000010/uni00000025
/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018
/uni00000024/uni00000026/uni00000026
(
a) Convergence on PGOT 1
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048/uni00000056/uni00000003/uni00000052/uni00000051/uni00000003/uni00000025/uni0000003d/uni00000035
/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c
/uni00000024/uni00000026/uni00000026
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048/uni00000056/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000030/uni00000027/uni00000025/uni00000010/uni00000025
/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018
/uni00000024/uni00000026/uni00000026 (
b) Convergence on PGOT 2
Figure 3: The loss value and test accuracy (%) value with training iterations.
and PTC [ 23], along with two social network datasets: IMDB-
BINARY, IMDB-MULTI [ 73]. Please refer to Appendix.D for more
details of the datasets.
Baseline methods. To verify the effectiveness and competitiveness
of PGOT, we compare the proposed framework with the following
baselines: i) Kernel-based methods, i.e.,SP kernel [ 1], WL subtree
kernel [ 48]; ii) OT-based methods, i.e.,OT-GNN [ 8], TFGW [ 62];
iii) GNN-based methods, i.e.,GCN [ 29], GIN [ 70], GAT [ 59], Drop-
GIN [ 41], DIFFPOOL [ 74], ChebyNet [ 15], APPNP [ 19], GPRGNN [ 9].
Implementation details. The proposed framework is implemented
with PyTorch [ 42]. The number of training epochs is 100. We set the
ğ‘in Eq. (1) and Eq. (3) to 2. We use the POT toolbox [ 18] to compute
the FGW-related items (when the approximation is not utilized).
Following [ 62], we evaluate the performance of a 10-fold cross-
validation (CV) with a fixed holdout test set, where the best average
accuracy on the test fold is reported. For the FGW Approximator,
we sample 5000 pairs of graphs for pre-training on each dataset.
All experiments are conducted on either an Intel(R) Xeon(R) Gold
5218R CPU or a single NVIDIA GeForce RTX 3090. More details
and codes are accessible in the supplemental materials.6.2 Main Results
Performance on graph classification benchmarks. The test
accuracy comparison on several widely used graph classification
benchmarks is presented in Table 1, where PGOT 1and PGOT 2
denote Algorithm 1 and Algorithm 2, respectively. From the table,
we obtain the following observation: i) Overall, for all datasets, the
deep neural network based methods outperform the kernel-based
methods, which indicates that deep models are more powerful
in representing complex biochemical graph-structured data, e.g.,
molecules and proteins. ii) The selection of either fixed prototypes
or updated prototypes, as outlined in Algorithm 1 and Algorithm 2,
respectively, can yield varying benefits depending on the unique
characteristics of the datasets. For example, the relatively simple
MUTAG dataset may enable effective representation of each class
through direct prototype selection. iii) The PGOT demonstrates
strong competitiveness and even superior performance compared
to GNNs and kernel-based methods. Furthermore, employing the
simple-designed FGW approximator yields comparable results com-
pared to other baselines. These observations underscore the effec-
tiveness of PGOT in graph classification.
Evaluation on FGW Approximator. 1) Approximation capability.
To assess the approximation capability of the FGW Approximator,
 
2450KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Chen Qian, Huayi Tang, Hong Liang, and Yong Liu
Table 2: Test Accuracy (%, mean Â±std) of several methods on large-scale graph classification datasets with the best bolded and
the runner-up underlined .
Metho
d ogbg-molsider
ogbg-molclintox ogbg-molbace ogbg-molbbbp ogbg-moltoxcast ogbg-molhiv ogbg-molpcba
#
Graphs 1427
1478 1513 2039 8575 41127 437929
GCN 58.20 Â±2.59 75.56 Â±3.12
71.90 Â±4.04 65.60 Â±1.43 56.78 Â±3.04 72.90 Â±1.12 23.64 Â±0.91
GIN 57.73 Â±3.46
76.10 Â±3.00 72.43 Â±4.36 65.76 Â±1.91 60.33 Â±2.00
76.75 Â±1.36 29.45 Â±0.52
PGO
T1(GIN) + Appro. 60.22 Â±3.18
87.62 Â±4.94 72.89 Â±6.83 65.96 Â±3.17 59.68 Â±0.86 74.40 Â±3.13 30.13 Â±2.68
0 2 4 6 8 10
True FGW Distance02468Appro. FGW Distance
T est on COX2, MSE=0.3195, Corr.=0.9743
0 5 10 15 20
True FGW Distance05101520Appro. FGW Distance
T est on IMDB-B, MSE=2.3427, Corr.=0.9621
Figur
e 4: Predicted performance of FGW Approximator.
we calculate the real and predicted FGW distances between mul-
tiple pairs of graphs in the test set with random ğ›¼sampled from
Uniform(0, 1), and report the correlation coefficient and MSE in Fig-
ure 4. From the figure, we can observe that the FGW Approximator
demonstrates a high correlation and relatively small MSE between
its predicted values and the ground truth. 2) Computational per-
formance. Table 3 provides a comparison of the computational
performance of the traditional OT solver [ 18] and our designed
FGW Approximator. From the table, we can observe that on the
CPU, our FGW Approximator outperforms the OT solver by an
average of two orders of magnitude in terms of FGW distance com-
putation (per second). Notably, the speed gains are further amplified
when leveraging GPU acceleration.
Convergence visualization and analysis. In Figure 3, we plot the
accuracy and loss curves after training PGOT within 100 epochs
on dataset BZR and IMDB-B to verify the modelâ€™s convergence.
We can observe that the loss value decreases rapidly in the first
several epochs and then continuously decrease in a fluctuation
way until convergence. Also, the acc curve firstly increases with
iterations and then keep fluctuation in a narrow range. After a
certain epochs of training, both of them tend to be stable. These
results demonstrate the convergence of the proposed PGOT.
6.3 Ablation Studies
Impact of #prototypes. Table 5 and Table 6 present the impact
of varying the number of prototypes on the modelâ€™s classification
performance under Algorithm 1 and Algorithm 2, respectively. From
the tables, we can see that despite the design of multiple prototypes
per class contributing to enhancing the expressive power of the
model, however, increasing the number of prototypes may not
necessarily yield better results.Table 3: Computation efficiency of FGW Approximator
(#FGW/Second).
De
vice Method MU
TAG BZR IMDB-B
CP
UOT Solver [18] 177
138 85
FGW Appro. 23,384
7,062 12,190
GP
UOT Solver [18] -
- -
FGW Appro. 37,300
39,384 37,925
Impact of ğ›¼.To observe the effect of different settings of ğ›¼in
Eq. (1), we set ğ›¼to a learnable variable along with invariable scalars
0,0.5,1, the result is presented as a boxplot and illustrated in Fig-
ure 5. For all three datasets, we can see that setting the value of
ğ›¼to either 0or1consistently leads to suboptimal performance,
showing the limitation of partially considering node features or
topological information. In contrast, the learnable ğ›¼leads to su-
perior experimental performance by effectively adjusting to the
unique characteristics of the datasets.
6.4 Case Studies
Performance on large-scale datasets. To further validate the
soundness of PGOT in handling large-scale graph classification
challenges, we further conduct experiments to encompass a suite
of datasets from the OGBG dataset [ 25]. As shown in Table 2,
the proposed PGOT outperforms the baselines across the initial
four datasets and ogbg-molpcba, particularly on ogbg-molclintox,
achieving impressive performance. In the remaining datasets, PGOTâ€™s
performance remains on par with these baselines. These results
firmly establish the scalability and efficacy of PGOT when applied to
large-scale graph classification tasks, demonstrating its consistent
performance across extensive datasets.
Performance on low-resource settings. To verify the quick adap-
tation ability of PGOT, we conducted low-resource experiments on
three different partitions of the original training sets (10/30/50%
ratio) across three datasets. Table 4 shows the performance of graph
classification under three low-resource settings. We observe that
PGOT still achieves significantly better performance compared to
some of the baselines with low ratio data. Intuitively, this is owing to
our well-designed prototype-based classification paradigm, where
the graph-structured barycenters serve as desirable prototypes that
further lead to stronger generalization performances.
Visualization of FGW distances. In order to verify that the pro-
totype obtained after PGOT training can effectively complete the
graph classification task, we report in Figure 6 the distribution of
FGW distances between graphs and prototypes on datasets MUTAG
 
2451Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 4: Low-resource results on datasets MUTAG, PTC and BZR.
Metho
dMU
TAG PT
C BZR
10%
(15) 30% (45) 50% (76) 10%
(27) 30% (83) 50% (139) 10%
(32) 30% (98) 50% (163)
GCN 72.63 Â±5.16
86.32 Â±6.74 87.37 Â±4.21 54.00 Â±8.71 58.86 Â±6.15 57.71 Â±6.49 79.27 Â±2.25
83.17 Â±2.55 81.46 Â±2.49
GIN 73.68 Â±7.44
77.37 Â±11.30 77.89 Â±10.99 54.86 Â±4.00
56.00 Â±9.05 58.00 Â±5.12 76.10 Â±6.71
77.07 Â±2.93 79.02 Â±1.19
PGOT 2(GIN) 82.63 Â±5.79
88.95 Â±5.98 88.42 Â±4.59 58.00 Â±7.78 58.00 Â±8.76 59.37 Â±5.18 80.98 Â±4.73
83.66 Â±2.68 84.63 Â±3.09
Table 5: Effect of #prototypes in Algorithm 1.
#
Prototypes MU
TAG BZR PTC COX2
1 88.95 Â±5.49
82.44 Â±2.13 60.86 Â±5.72 84.04 Â±3.95
2 95.79 Â±3.16 84.15 Â±4.26 63.43 Â±7.65 85.74 Â±3.44
3 90.53 Â±6.57 85.37 Â±1.99 59.43 Â±5.97
86.38 Â±2.72
4 90.53 Â±6.57
84.39 Â±4.52 59.43 Â±8.16 86.81 Â±2.29
MUTAG BZR COX25060708090100Accuracies
: 0
: 0.5
: 1
: learnable
Figure 5: Effective of different ğ›¼setting strategies.
and BZR. The prototype number of each class is 1. The ordinate
of the figure represents the distance value, and the abscissa repre-
sents the class of the graph instance. In general, the FGW distance
distribution from the observed graphs of a particular class to their
corresponding prototypes tends to be smaller compared to the
distances to other prototypes. From the perspective of distance dis-
tribution, PGOT can discriminatively classify graphs by calculating
the FGW distance between the graphs and prototypes.
7 Conclusion
This work addresses the weaknesses of the current mainstream
GNN-based graph classification paradigm, which implicitly ex-
ploits the structural information of graphs and lacks sufficient
decision transparency. To overcome these limitations, we propose
a prototype-based graph classification framework called PGOT,
which models the OT with prototype learning jointly. Experimen-
tal results verify the generality and effectiveness of the proposed
PGOT. Moreover, we design the first NN-based FGW distance ap-
proximator to alleviate the computational overhead associated with
FGW distance calculations. In theory, we provide an upper bound
of the generalization gap of the proposed algorithm, and the proof
technique employed can be applied to more general prototype clas-
sification tasks. We hope our work can bring new insights to the
graph classification learning paradigm.Table 6: Effect of ğœ¶ğµğ¶in Algorithm 2.
ğœ¶ğµ
ğ¶ MU
TAG BZR PTC COX2
[0] 88.42 Â±5.67
84.63 Â±3.28 58.86 Â±10.08 82.13 Â±3.04
[0.5] 92.63 Â±2.58
87.32 Â±3.90 63.43 Â±3.79 82.98 Â±5.21
[1] 86.32 Â±6.32
85.85 Â±5.32 62.00 Â±8.19 80.43 Â±2.66
[0.3,0.7] 91.05 Â±4.74
85.37 Â±2.44 58.57 Â±2.93 84.26 Â±4.17
[0.1,0.3,0.5,0.7,0.9] 88.42 Â±5.16
85.37 Â±2.89 63.43 Â±6.36 84.68 Â±3.27
/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni00000014 /uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni00000015/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000030/uni00000038/uni00000037/uni00000024/uni0000002a
/uni00000053/uni00000055/uni00000052/uni00000057/uni00000052/uni00000057/uni0000005c/uni00000053/uni00000048/uni00000003/uni00000014
/uni00000053/uni00000055/uni00000052/uni00000057/uni00000052/uni00000057/uni0000005c/uni00000053/uni00000048/uni00000003/uni00000015
/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni00000014 /uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni00000015/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000025/uni0000003d/uni00000035
/uni00000053/uni00000055/uni00000052/uni00000057/uni00000052/uni00000057/uni0000005c/uni00000053/uni00000048/uni00000003/uni00000014
/uni00000053/uni00000055/uni00000052/uni00000057/uni00000052/uni00000057/uni0000005c/uni00000053/uni00000048/uni00000003/uni00000015
Figure 6: Distribution of FGW distances between input graphs
and prototypes.
Acknowledgements
We thank the anonymous reviewers for their constructive sugges-
tions to improve the quality of this paper. This work is supported by
the Beijing Natural Science Foundation (No.4222029); the National
Natural Science Foundation of China (NO.62076234); the National
Key Research and Development Project (No.2022YFB2703102); the
â€œIntelligent Social Governance Interdisciplinary Platform, Major
Innovation & Planning Interdisciplinary Platform for the â€œDouble-
First Classâ€ Initiative, Renmin University of Chinaâ€; the Beijing Out-
standing Young Scientist Program (NO.BJJWZYJH012019100020098);
the Public Computing Cloud, Renmin University of China; the Fun-
damental Research Funds for the Central Universities, and the Re-
search Funds of Renmin University of China (NO.2021030199); the
Huawei-Renmin University joint program on Information Retrieval;
and the Unicom Innovation Ecological Cooperation Plan.
Limitations and Outlooks
There are several limitations of this work. Firstly, we only consider
the vanilla SGD during the generalization analysis of PGOT. Ex-
panding the stability analysis to include more practical optimizers
(e.g., employs adaptive learning rates and momentum techniques)
beyond the native SGD algorithm is a valuable direction for future
work. Secondly, we attempt to provide two possible strategies for
prototype initialization and updating. However, directly selecting
prototypes from the dataset may overlook the extraction of certain
substructures present in the dataset. We aspire to pursue more
exquisite and interpretable prototype designs in the future.
 
2452KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Chen Qian, Huayi Tang, Hong Liang, and Yong Liu
References
[1]Karsten M. Borgwardt and Hans-Peter Kriegel. 2005. Shortest-Path Kernels on
Graphs. In Proceedings of the 5th IEEE International Conference on Data Mining.
74â€“81.
[2]Karsten M Borgwardt, Cheng Soon Ong, Stefan SchÃ¶nauer, SVN Vishwanathan,
Alex J Smola, and Hans-Peter Kriegel. 2005. Protein function prediction via graph
kernels. Bioinformatics 21, suppl_1 (2005), i47â€“i56.
[3]Olivier Bousquet and AndrÃ© Elisseeff. 2002. Stability and generalization. The
Journal of Machine Learning Research 2 (2002), 499â€“526.
[4]Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. 2020. Sharper Bounds
for Uniformly Stable Algorithms. In Conference on Learning Theory, Vol. 125. 610â€“
626.
[5]Luc Brogat-Motte, RÃ©mi Flamary, CÃ©line Brouard, Juho Rousu, and Florence
dâ€™AlchÃ© Buc. 2022. Learning to predict graphs with fused Gromov-Wasserstein
barycenters. In International Conference on Machine Learning. 2321â€“2335.
[6]David Buterez, Jon Paul Janet, Steven J Kiddle, Dino Oglic, and Pietro LiÃ². 2022.
Graph Neural Networks with Adaptive Readouts. In Advances in Neural Informa-
tion Processing Systems.
[7]Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
Armand Joulin. 2020. Unsupervised learning of visual features by contrasting
cluster assignments. Advances in Neural Information Processing Systems (2020),
9912â€“9924.
[8]Benson Chen, Gary BÃ©cigneul, Octavian-Eugen Ganea, Regina Barzilay, and
Tommi Jaakkola. 2020. Optimal transport graph neural networks. arXiv preprint
arXiv:2006.04804 (2020).
[9]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2020. Adaptive Universal
Generalized PageRank Graph Neural Network. In International Conference on
Learning Representations.
[10] Djork-ArnÃ© Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2016. Fast and
Accurate Deep Network Learning by Exponential Linear Units (ELUs). In 4th
International Conference on Learning Representations.
[11] Nicolas Courty, RÃ©mi Flamary, and MÃ©lanie Ducoffe. 2018. Learning Wasserstein
Embeddings. In International Conference on Learning Representations.
[12] Thomas Cover and Peter Hart. 1967. Nearest neighbor pattern classification.
IEEE Transactions on Information Theory 13, 1 (1967), 21â€“27.
[13] Enyan Dai and Suhang Wang. 2022. Towards Prototype-Based Self-Explainable
Graph Neural Network. arXiv preprint arXiv:2210.01974 (2022).
[14] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shus-
terman, and Corwin Hansch. 1991. Structure-activity relationship of mutagenic
aromatic and heteroaromatic nitro compounds. correlation with molecular or-
bital energies and hydrophobicity. Journal of medicinal chemistry 34, 2 (1991),
786â€“797.
[15] MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-
tional neural networks on graphs with fast localized spectral filtering. Advances
in Neural Information Processing Systems 29 (2016), 3837â€“3845.
[16] Simon S. Du, Kangcheng Hou, Ruslan Salakhutdinov, BarnabÃ¡s PÃ³czos, Ruosong
Wang, and Keyulu Xu. 2019. Graph Neural Tangent Kernel: Fusing Graph Neural
Networks with Graph Kernels. In Advances in Neural Information Processing
Systems 32. 5724â€“5734.
[17] Pascal Mattia Esser, Leena C. Vankadara, and Debarghya Ghoshdastidar. 2021.
Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Net-
works. In Advances in Neural Information Processing Systems 34. 27043â€“27056.
[18] RÃ©mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z Alaya, AurÃ©lie
Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras,
Nemo Fournier, et al .2021. POT: Python Optimal Transport. Journal of Machine
Learning Research 22, 78 (2021), 1â€“8.
[19] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2019.
Combining Neural Networks with Personalized PageRank for Classification on
Graphs. In International Conference on Learning Representations.
[20] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
Conference on Machine Learning. 1263â€“1272.
[21] Moritz Hardt, Ben Recht, and Yoram Singer. 2016. Train faster, generalize better:
Stability of stochastic gradient descent. In Proceedings of the 33nd International
Conference on Machine Learning, Vol. 48. 1225â€“1234.
[22] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means
clustering algorithm. Applied Statistics 28, 1 (1979), 100â€“108.
[23] Christoph Helma, Ross D. King, Stefan Kramer, and Ashwin Srinivasan. 2001. The
predictive toxicology challenge 2000â€“2001. Bioinformatics 17, 1 (2001), 107â€“108.
[24] TamÃ¡s HorvÃ¡th, Thomas GÃ¤rtner, and Stefan Wrobel. 2004. Cyclic pattern kernels
for predictive graph mining. In Proceedings of the Tenth ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining. 158â€“167.
[25] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for
machine learning on graphs. Advances in Neural Information Processing Systems
33 (2020), 22118â€“22133.
[26] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating
Deep Network Training by Reducing Internal Covariate Shift. In Proceedings ofthe 32nd International Conference on Machine Learning, Vol. 37. 448â€“456.
[27] Leonid V Kantorovich. 1960. Mathematical methods of organizing and planning
production. Management Science 6, 4 (1960), 366â€“422.
[28] Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. 2003. Marginalized kernels
between labeled graphs. In Proceedings of the 20th International Conference on
Machine Learning (ICML-03). 321â€“328.
[29] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In 5th International Conference on Learning
Representations.
[30] Yegor Klochkov and Nikita Zhivotovskiy. 2021. Stability and Deviation Optimal
Risk Bounds with Convergence Rate $O(1/n)$. In Advances in Neural Information
Processing Systems 34. 5065â€“5076.
[31] Barbara J Knowlton and Larry R Squire. 1993. The learning of categories: Parallel
brain systems for item memory and category knowledge. Science 262, 5140 (1993),
1747â€“1749.
[32] Janet L Kolodner. 1992. An introduction to case-based reasoning. Artificial
Intelligence Review 6, 1 (1992), 3â€“34.
[33] Soheil Kolouri, Navid Naderializadeh, Gustavo K. Rohde, and Heiko Hoffmann.
2021. Wasserstein Embedding for Graph Learning. In International Conference on
Learning Representations.
[34] Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang.
2019. Semi-supervised graph classification: A hierarchical graph perspective. In
The World Wide Web Conference. 972â€“982.
[35] Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. 2021. Prototypi-
cal Contrastive Learning of Unsupervised Representations. In 9th International
Conference on Learning Representations.
[36] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper Insights Into Graph
Convolutional Networks for Semi-Supervised Learning. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence. 3538â€“3545.
[37] Jie Liu, Yanqi Bao, Guo-Sen Xie, Huan Xiong, Jan-Jakob Sonke, and Efstratios
Gavves. 2022. Dynamic prototype convolution network for few-shot semantic
segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition.
11553â€“11562.
[38] Facundo MÃ©moli. 2011. Gromovâ€“Wasserstein distances and the metric approach
to object matching. Foundations of Computational Mathematics 11, 4 (2011),
417â€“487.
[39] Diego Mesquita, Amauri Souza, and Samuel Kaski. 2020. Rethinking pooling in
graph neural networks. Advances in Neural Information Processing Systems 33
(2020), 2220â€“2231.
[40] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,
and Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for
learning with graphs. In ICML 2020 Workshop on Graph Representation Learning
and Beyond.
[41] PÃ¡l AndrÃ¡s Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. 2021.
Dropgnn: random dropouts increase the expressiveness of graph neural networks.
Advances in Neural Information Processing Systems 34 (2021), 21997â€“22009.
[42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
in Neural Information Processing Systems 32 (2019).
[43] Gabriel PeyrÃ©, Marco Cuturi, and Justin Solomon. 2016. Gromov-wasserstein
averaging of kernel and distance matrices. In International Conference on Machine
Learning. 2664â€“2672.
[44] Jan Ramon and Thomas GÃ¤rtner. 2003. Expressivity versus efficiency of graph
kernels. In Proceedings of the First International Workshop on Mining Graphs, Trees
and Sequences. 65â€“74.
[45] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang,
and Junzhou Huang. 2020. Self-supervised graph transformer on large-scale
molecular data. Advances in Neural Information Processing Systems 33 (2020),
12559â€“12571.
[46] Filippo Santambrogio. 2015. Optimal transport for applied mathematicians.
BirkÃ¤user, NY 55, 58-63 (2015), 94.
[47] Nino Shervashidze and Karsten M. Borgwardt. 2009. Fast subtree kernels on
graphs. In Advances in Neural Information Processing Systems 22. 1660â€“1668.
[48] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn,
and Karsten M. Borgwardt. 2011. Weisfeiler-Lehman Graph Kernels. Journal of
Machine Learning Research 12 (2011), 2539â€“2561.
[49] Nino Shervashidze, S. V. N. Vishwanathan, Tobias Petri, Kurt Mehlhorn, and
Karsten M. Borgwardt. 2009. Efficient graphlet kernels for large graph comparison.
InProceedings of the Twelfth International Conference on Artificial Intelligence and
Statistics, Vol. 5. 488â€“495.
[50] Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for
few-shot learning. Advances in Neural Information Processing Systems (2017),
4077â€“4087.
[51] Robert L Solso, M Kimberly MacLin, and Otto H MacLin. 2005. Cognitive psy-
chology. Pearson Education New Zealand.
[52] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2020. InfoGraph:
Unsupervised and Semi-supervised Graph-Level Representation Learning via
 
2453Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Mutual Information Maximization. In International Conference on Learning Rep-
resentations.
[53] Jeffrey J Sutherland, Lee A Oâ€™brien, and Donald F Weaver. 2003. Spline-fitting with
a genetic algorithm: A method for developing classification structure- activity
relationships. Journal of Chemical Information and Computer Sciences 43, 6 (2003),
1906â€“1915.
[54] Huayi Tang and Yong Liu. 2023. Towards Understanding the Generalization of
Graph Neural Networks. arXiv preprint arXiv:2305.08048 (2023).
[55] Lei Tang and Huan Liu. 2010. Graph Mining Applications to Social Network
Analysis. In Managing and Mining Graph Data. Vol. 40. 487â€“513.
[56] Vayer Titouan, Nicolas Courty, Romain Tavenard, and RÃ©mi Flamary. 2019. Op-
timal transport for structured data with application on graphs. In International
Conference on Machine Learning. 6275â€“6284.
[57] Leonid Nisonovich Vaserstein. 1969. Markov processes over denumerable prod-
ucts of spaces, describing large systems of automata. Problemy Peredachi Infor-
matsii 5, 3 (1969), 64â€“72.
[58] Titouan Vayer, Laetitia Chapel, RÃ©mi Flamary, Romain Tavenard, and Nicolas
Courty. 2020. Fused gromov-wasserstein distance for structured objects. Algo-
rithms 13, 9 (2020), 212.
[59] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations.
[60] CÃ©dric Villani. 2009. Optimal transport: old and new. Vol. 338. Springer.
[61] CÃ©dric Vincent-Cuaz, RÃ©mi Flamary, Marco Corneli, Titouan Vayer, and Nicolas
Courty. 2022. Semi-relaxed Gromov-Wasserstein divergence and applications on
graphs. In International Conference on Learning Representations.
[62] CÃ©dric Vincent-Cuaz, RÃ©mi Flamary, Marco Corneli, Titouan Vayer, and Nicolas
Courty. 2022. Template based Graph Neural Network with Optimal Transport
Distances. In Advances in Neural Information Processing Systems.
[63] CÃ©dric Vincent-Cuaz, Titouan Vayer, RÃ©mi Flamary, Marco Corneli, and Nicolas
Courty. 2021. Online graph dictionary learning. In International Conference on
Machine Learning. 10564â€“10574.
[64] Wenguan Wang, Cheng Han, Tianfei Zhou, and Dongfang Liu. 2023. Visual
recognition with deep nearest centroids. In International Conference on Learning
Representations.
[65] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervised
feature learning via non-parametric instance discrimination. In Proceedings of
the IEEE conference on Computer Vision and Pattern Recognition. 3733â€“3742.
[66] Hongteng Xu. 2020. Gromov-Wasserstein Factorization Models for Graph Clus-
tering. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, Vol. 34.6478â€“6485.
[67] Hongteng Xu, Dixin Luo, and Lawrence Carin. 2019. Scalable Gromov-
Wasserstein Learning for Graph Partitioning and Matching. In Advances in Neural
Information Processing Systems. 3046â€“3056.
[68] Hongteng Xu, Dixin Luo, Lawrence Carin, and Hongyuan Zha. 2021. Learning
graphons via structured gromov-wasserstein barycenters. In The Thirty-Fifth
AAAI Conference on Artificial Intelligence, Vol. 35. 10505â€“10513.
[69] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. 2019.
Gromov-wasserstein learning for graph matching and node embedding. In Inter-
national Conference on Machine Learning. 6932â€“6941.
[70] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful
are Graph Neural Networks?. In International Conference on Learning Representa-
tions.
[71] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Pow-
erful are Graph Neural Networks?. In 7th International Conference on Learning
Representations.
[72] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In International Conference on Machine Learn-
ing. 5453â€“5462.
[73] Pinar Yanardag and S. V. N. Vishwanathan. [n. d.]. Deep Graph Kernels. In
Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. 1365â€“1374.
[74] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure
Leskovec. 2018. Hierarchical graph representation learning with differentiable
pooling. Advances in Neural Information Processing Systems 31 (2018).
[75] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor
Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive Learning Method.
InInternational Conference on Learning Representations.
[76] Tong Zhang, Yun Wang, Zhen Cui, Chuanwei Zhou, Baoliang Cui, Haikuan
Huang, and Jian Yang. 2021. Deep Wasserstein Graph Discriminant Learning for
Graph Classification. In Thirty-Fifth AAAI Conference on Artificial Intelligence.
10914â€“10922.
[77] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. 2022. Protgnn:
Towards self-explaining graph neural networks. In Thirty-Sixth AAAI Conference
on Artificial Intelligence, Vol. 36. 9127â€“9135.
[78] Tianfei Zhou, Wenguan Wang, Ender Konukoglu, and Luc Van Gool. 2022. Re-
thinking semantic segmentation: A prototype view. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 2582â€“2593.
 
2454