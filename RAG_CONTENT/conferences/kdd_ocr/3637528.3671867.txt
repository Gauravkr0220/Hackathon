Label Shift Correction via Bidirectional Marginal Distribution
Matching
Ruidong Fanâˆ—
fanruidong1996@hotmail.com
National University of Defense Technology
Changsha, Hunan, ChinaXiao Ouyang
ouyangxiao98@hotmail.com
National University of Defense Technology
Changsha, Hunan, China
Hong Taoâ€ 
taohong.nudt@hotmail.com
National University of Defense Technology
Changsha, Hunan, ChinaChenping Houâ€ 
hcpnudt@hotmail.com
National University of Defense Technology
Changsha, Hunan, China
ABSTRACT
Due to the timeliness and uncertainty of data acquisition, label
shift, which assumes that the source (training) and target (test) la-
bel distributions differ, occurs with the changing environment and
reduces the generalization ability of traditional models. To correct
the label shift, existing methods estimate the true label distribution
by prediction of target data from a source classifier, which results
in high variance, especially with large label shift. In this paper, we
tackle this problem by proposing a novel approach termed as Label
Shift Correction via Bidirectional Marginal Distribution Matching
(BMDM). Our approach matchs the label and feature marginal dis-
tributions simultaneously to ensure the stability of estimated class
proportions. We prove theoretically that there is a unique optimal
solution, i.e., true target label distribution, for our approach under
mild conditions, and an efficient optimization strategy is also pro-
posed. On this basis, in multi-shot scenario where label distribution
changes continuously, we extend BMDM by designing a new distri-
bution matching mechanism and constructing a regularization term
that constrains the direction of label distribution change. Extensive
experimental results validate the effectiveness of our approach over
existing state-of-the-arts methods.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning.
KEYWORDS
Label shift, Distribution matching, Bidirectional
ACM Reference Format:
Ruidong Fan, Xiao Ouyang, Hong Tao, and Chenping Hou. 2024. Label Shift
Correction via Bidirectional Marginal Distribution Matching. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671867
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718671 INTRODUCTION
The success of traditional machine learning methods relies on the
fundamental assumption that the training and test data are drawn
from the same distribution independently and identically [ 12]. How-
ever, in some open environment applications, data is gathered in a
dynamic way [ 8,24,27,28]. Apart from the timeliness and cumula-
tiveness of data streams, the test (target) distribution always differs
from the training (source) distribution [9, 11, 22, 23]. For example,
in medical image analysis [ 7,14], even if there is not much differ-
ence in symptoms, disease prevalences can also vary significantly in
different seasons. Besides, in plant classification [ 1,16], the types of
popular plants change in different regions, while there is not much
difference in plant characteristics. Specifically, as shown in Figure
1, poplars are mostly found in plain areas, while cacti are mostly
found in desert areas, thus the corresponding optimal classifier
will change accordingly. This phenomenon is an important case of
distribution shift, that is, label shift, based on the assumption that
the source and target distributions have different proportions of cat-
egories(ğ‘ğ‘ (ğ‘Œ)â‰ ğ‘ğ‘¡(ğ‘Œ)), while the feature distribution is the same
for each category(ğ‘ğ‘ (ğ‘‹|ğ‘Œ)=ğ‘ğ‘¡(ğ‘‹|ğ‘Œ)). Researches show that the
change of label marginal distribution can significantly reduce the
generalization performance of traditional models [ 5,13,19,20,26].
Thus, such a shift poses significant challenges for machine learning
systems deployed in the wild.
To overcome the impact of label shift and build a robust model
capable of generalizing well to test samples, current mainstream
methods employ two strategies: confusion matrix and maximum
likelihood estimation [ 3,6,15,25]. For the first strategy, Lipton
et al. [ 10] estimate the important weight using target pseudo la-
bels predicted by the source classifier and the confusion matrix
derived from the predictions and labels of source samples, while a
reweighted framework is used to train the target classifier. On this
basis, Azizzadenesheli et al. [ 4] design a regularized weight estima-
tor to mitigate the influence of large label shift and limited target
samples, and derive the generalization error bound in the finite sam-
ple case. Regarding the second strategy, under the premise of source
classifier calibration, Alexandari et al. [ 2] propose the maximum
likelihood function to estimate the target prior and design a simple
iterative procedure through expectation-maximization algorithm.
Sulc et al. combine the maximum likelihood estimation framework
 
735
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruidong Fan, Xiao Ouyang, Hong Tao, and Chenping Hou
Figure 1: Illustration of the label shift scenario. In the con-
text of plant classification, the source data originates from
desert areas with a predominant presence of cacti, while the
target data comes from plain areas where poplars are more
prevalent. A shift in the position of the black line signifies a
change in the optimal classifier.
with Dirichlet hyper-prior, and an effective projected gradient de-
scent algorithm is adopted to solve the simplex constraint [ 18]. To
combine the advantages of above two strategies, Sipka et al. employ
confusion matrix to obtain the calibrated target predictions, which
is used in maximum likelihood estimation [17].
Although these approaches have demonstrated remarkable per-
formance in their respective applications, there remain at least two
challenges to be addressed. (1) Most traditional methods heavily
rely on the performance of the source classifier, as the estimation
of weights depends on the confusion matrix and target predic-
tions derived from the source classifier. Consequently, if the source
classifier performs poorly, the bias in weight estimation will be
substantial. (2) The majority of label shift methods are designed to
handle one-shot scenarios. If the label distribution changes over
time, not just once, it is difficult for existing methods to solve this
problem since successive utilization of one-shot method will ignore
the weights estimated in the previous stage and waste information
in the process. Again take plant classification as an example, the
change from desert to plain is gradual rather than sudden, thus the
label distribution of plant changes gradually, which is an urgent
and important challenge.
To effectively address the aforementioned challenges, we pro-
pose a novel method called Label Shift Correction via Bidirectional
Marginal Distribution Matching (BMDM), as depicted in Figure 2.
For the one-shot scenario, we begin by matching the label marginal
distribution between the source and target domains. To ensure the
sufficiency and uniqueness of the solution, we construct a vicarious
convex function. To reduce reliance on the source classifier, feature
marginal distribution is matched through the Bayes formula based
on kernel mean matching and label shift. Subsequently, we integrate
the two matching strategies into a comprehensive framework and
utilize a projected gradient descent algorithm to efficiently handle
the simplex constraint. We further extend BMDM to the multi-shot
scenario, introducing Multi-shot BMDM (MBMDM),which incor-
porates an effective diversity constraint to ensure the direction of
label distribution deviation. The key aspect of this strategy lies ineffectively utilizing the estimated weights from the intermediate
stages, with theoretical guarantees. Finally, extensive experiment
results are presented to verify the effectiveness of our model. In
summary, the contributions of our paper are listed as follows:
â€¢We propose the BMDM approach to address the label shift
problem and extend it to the multi-shot scenario. To the
best of our knowledge, this may be the first study to de-
velop a multi-shot label shift method that effectively utilizes
intermediate information.
â€¢We propose a novel approach that tackles the label shift prob-
lem by simultaneously matching the label and feature mar-
ginal distributions. This not only reduces the performance
requirements of the source classifiers but also guarantees a
unique optimal solution for the objective function.
â€¢We demonstrate the effectiveness of our methods on multiple
datasets. The experimental results consistently show that our
methods outperform other compared methods in most cases,
especially on the large label shift or multi-shot scenario.
2 THE BMDM ALGORITHM
2.1 Notations
We use random variables ğ‘‹âˆˆX,ğ‘ŒâˆˆY to represent features
and labels respectively, where X=Rğ‘‘andYis a discrete domain
equivalent to[ğ¾]. Here,ğ‘‘is the feature dimension, and ğ¾is the
total number of classes. Let ğ‘ƒğ‘ andğ‘ƒğ‘¡denote the source and target
distributions defined on XÃ—Y . Additionally, ğ‘ğ‘ andğ‘ğ‘¡are used to
denote the probability density or mass function associated with ğ‘ƒğ‘ 
andğ‘ƒğ‘¡. We possess labeled data (ğ‘‹ğ‘ ,ğ‘Œğ‘ )={ğ‘¥ğ‘–,ğ‘¦ğ‘–}ğ‘›
ğ‘–=1drawn i.i.d.
from the source distribution ğ‘ƒğ‘ and unlabeled data ğ‘‹ğ‘¡={ğ‘¥ğ‘–}ğ‘›+ğ‘š
ğ‘–=ğ‘›+1
drawn i.i.d. from the target distribution ğ‘ƒğ‘¡. We assume access to an
arbitrary classifier ğ‘“:Xâ†¦â†’Î”ğ¾âˆ’1, which is trained to approximate
the source class posterior probability, i.e., ğ‘“ğ‘ (ğ‘¥)â‰ˆğ‘ğ‘ (ğ‘Œ|ğ‘‹=ğ‘¥), by
sigmoid function, cross-entropy minimization or whatever. Here
Î”ğ¾âˆ’1denotes the standard ğ¾-dimensional probability simplex.
Under the label shift assumption, i.e., ğ‘ğ‘ (ğ‘‹|ğ‘Œ)=ğ‘ğ‘¡(ğ‘‹|ğ‘Œ)and
ğ‘ğ‘ (ğ‘Œ)â‰ ğ‘ğ‘¡(ğ‘Œ), if we give a loss function ğ‘™:XÃ—Yâ†’ R1, the goal
of label shift setting is to find an optimal target classifier ğ‘“ğ‘¡which
minimizes the following reweighted loss [10]:
L(ğ‘“)=E(ğ‘‹,ğ‘Œ)âˆ¼ğ‘ƒğ‘¡[ğ‘™(ğ‘“ğ‘¡(ğ‘‹),ğ‘Œ)]=E(ğ‘‹,ğ‘Œ)âˆ¼ğ‘ƒğ‘ ğ‘ğ‘¡(ğ‘Œ)
ğ‘ğ‘ (ğ‘Œ)ğ‘™(ğ‘“ğ‘¡(ğ‘‹),ğ‘Œ)
.
(1)
In practice, since the source label distribution ğ‘ğ‘ (ğ‘Œ)can naturally
be estimated by directly computing the proportion of source labels,
denoted asğ‘ğ‘ (ğ‘Œ=ğ‘˜)=1
ğ‘›ğ‘˜Ãğ‘›ğ‘˜
ğ‘–=11{ğ‘Œğ‘–=ğ‘˜}, whereğ‘›ğ‘˜the number of
samples inğ‘˜-th class. Therefore, the key challenge lies in estimating
the target label distribution ğ‘ğ‘¡(ğ‘Œ).
2.2 Core Algorithm
First, let us analyze the relationship between the distribution of
features and the distribution of labels. Utilizing Bayes formula, we
can express this relationship as follows:
ğ‘ğ‘¡(ğ‘Œ|ğ‘‹)=ğ‘ğ‘¡(ğ‘Œ,ğ‘‹)
ğ‘ğ‘¡(ğ‘‹)=ğ‘ğ‘¡(ğ‘‹|ğ‘Œ)ğ‘ğ‘¡(ğ‘Œ)
ğ‘ğ‘¡(ğ‘‹)=ğ‘ğ‘ (ğ‘‹|ğ‘Œ)ğ‘ğ‘¡(ğ‘Œ)
ğ‘ğ‘¡(ğ‘‹)
=ğ‘ğ‘ (ğ‘‹,ğ‘Œ)ğ‘ğ‘¡(ğ‘Œ)
ğ‘ğ‘¡(ğ‘‹)ğ‘ğ‘ (ğ‘Œ)=ğ‘ğ‘ (ğ‘Œ|ğ‘‹)ğ‘ğ‘¡(ğ‘Œ)ğ‘ğ‘ (ğ‘‹)
ğ‘ğ‘¡(ğ‘‹)ğ‘ğ‘ (ğ‘Œ).(2)
 
736Label Shift Correction via Bidirectional Marginal Distribution Matching KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 2: Illustration of BMDM. On the left side, a calibrated source classifier is utilized to predict labels for the target data,
enabling label matching. Conversely, on the right side, the feature matching is performed using only the source and target
data without labels, ensuring that the performance is independent of the quality of the classifier. By combining these two
components, optimal weight estimation is achieved, leading to the acquisition of the weighted target classifier (depicted in red)
through the Empirical Risk Minimization (ERM) framework.
According to the properties of conditional distribution, the equationÃğ¾
ğ‘˜=1ğ‘ğ‘¡(ğ‘Œ=ğ‘˜|ğ‘‹)=1always holds. Combined with Eq. (2), the
following formulas remain valid.
ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³ğ‘ğ‘¡(ğ‘‹)
ğ‘ğ‘ (ğ‘‹)=ğ¾Ã
ğ‘–=1ğ‘ğ‘¡(ğ‘Œ=ğ‘–)ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–),
ğ‘ğ‘¡(ğ‘Œ=ğ‘˜|ğ‘‹)=ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
ğ¾Ã
ğ‘–=1ğ‘ğ‘¡(ğ‘Œ=ğ‘–)ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–),ğ‘˜âˆˆ[ğ¾].(3)
Building upon this foundation, we will demonstrate how to estimate
the importance weight through bidirectional marginal distribution
matching.
2.2.1 Label Marginal Distribution Matching. Given the weight
variablesğœƒ=[ğœƒ1,ğœƒ2,...,ğœƒğ¾] âˆˆRğ¾,âˆ€ğ‘˜âˆˆ [ğ¾],0â‰¤ğœƒğ‘˜â‰¤1and
ğœƒâˆ—:=ğ‘ğ‘¡(ğ‘Œ), we define the weighted class posterior distribution as
follows:
ğ‘ğœƒ
ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)=ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–), ğ‘˜âˆˆ[ğ¾].(4)
According to Eq. (3), if ğœƒ=ğœƒâˆ—=ğ‘ğ‘¡(ğ‘Œ),ğ‘ğœƒâˆ—
ğ‘ (ğ‘Œ|ğ‘‹)=ğ‘ğ‘¡(ğ‘Œ|ğ‘‹)is
always holds. Then, we define the weight class marginal distribution
as follows:
ğ‘ğœƒ
ğ‘ (ğ‘Œ=ğ‘˜)=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)h
ğ‘ğœƒ
ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)i
=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)#
.(5)
Ifğœƒ=ğœƒâˆ—=ğ‘ğ‘¡(ğ‘Œ), The following conclusion is valid.
ğ‘ğœƒâˆ—
ğ‘ (ğ‘Œ)=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)h
ğ‘ğœƒâˆ—
ğ‘ (ğ‘Œ|ğ‘‹)i
=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)[ğ‘ğ‘¡(ğ‘Œ|ğ‘‹)]
=âˆ«
ğ‘ğ‘¡(ğ‘Œ|ğ‘‹)ğ‘ğ‘¡(ğ‘‹)ğ‘‘ğ‘‹=ğ‘ğ‘¡(ğ‘Œ)=ğœƒâˆ—.(6)This observation motivates us to transform the estimation of ğ‘ğ‘¡(ğ‘Œ)
into a weighted marginal distribution matching problem. Specifi-
cally, we aim to find a weighted distribution ğ‘ğœƒğ‘ (ğ‘Œ)=ğœƒ, which can
be expressed using the following formula.
ğœƒğ‘˜=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)#
:=[ğ‘‡(ğœƒ;ğ‘‹)]ğ‘˜, ğ‘˜âˆˆ[ğ¾],(7)
where[Â·]ğ‘˜denotes the ğ‘˜-th component of a vector. The equation
ğœƒ=ğ‘‡(ğœƒ;ğ‘‹)can be considered as a non-linear equation. One com-
monly used approach to obtain a solution for this non-linear equa-
tion is through fixed-point iteration. However, it should be noted
that sinceğ‘‡is not a contraction mapping, fixed-point iteration alone
is not enough to guarantee convergence to the optimal solution.
To address the aforementioned issue, we construct an auxiliary
optimization function with the following specific form:
ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³min
ğœƒEğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)h
âˆ’logÃğ¾
ğ‘˜=1ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)i
,
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾].(8)
Due to the concavity of the logarithmic function and the linearity
of the constraint, Eq. (8) is a convex problem. It possesses a unique
solution, denoted as ğœƒâˆ—=ğ‘ğ‘¡(ğ‘Œ), as demonstrated in Theorem 2.1.
Theorem 2.1. The optimal solution of Eq. (8) must satisfy the non-
linear Eq. (7). Further, if the set of distributions {ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘˜),ğ‘˜âˆˆ
[ğ¾]}are linearly independent, Eq. (8) has the unique solution ğœƒâˆ—=
ğ‘ğ‘¡(ğ‘Œ).
Please refer to the Appendix B.1 for a detailed proof. It is impor-
tant to note that the linearly independent property is commonly
utilized as a fundamental condition in label shift problems. Thus,
this is reasonable to utilize Eq. (8) to match the label distribution.
 
737KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruidong Fan, Xiao Ouyang, Hong Tao, and Chenping Hou
2.2.2 Feature Marginal Distribution Matching. Building on
the Bayes formula, ğ‘ğ‘ (ğ‘‹)=Ãğ¾
ğ‘–=1ğ‘ğ‘ (ğ‘‹,ğ‘Œ=ğ‘–)=Ãğ¾
ğ‘–=1ğ‘ğ‘ (ğ‘‹|ğ‘Œ=
ğ‘–)ğ‘ğ‘ (ğ‘Œ=ğ‘–)always hold forâˆ€ğ‘‹âˆˆX. We now define a family of
weighted feature marginal distribution over ğ‘‹parameterized by ğœƒ
as
ğ‘ğœƒ
ğ‘ (ğ‘‹)=ğ¾âˆ‘ï¸
ğ‘–=1ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘–)ğœƒğ‘–. (9)
Due to the label shift assumption ğ‘ğ‘ (ğ‘‹|ğ‘Œ)=ğ‘ğ‘¡(ğ‘‹|ğ‘Œ), ifğœƒ=ğ‘ğ‘¡(ğ‘Œ),
we have that ğ‘ğœƒğ‘ (ğ‘‹)=ğ‘ğ‘¡(ğ‘‹). Thus one efficient strategy to match
the feature marginal distribution is to find a vector ğœƒsuch that
ğ¾âˆ‘ï¸
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘–)=ğ‘ğ‘¡(ğ‘‹),âˆ€ğ‘‹âˆˆX. (10)
We show that the optimal solution of Eq. (10) is unique under the
same mild condition as in Theorem 2.1.
Theorem 2.2. If the set of distributions {ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘˜),ğ‘˜âˆˆ[ğ¾]}
are linearly independent, then for any ğœƒâˆ—that satisfies Eq. (10), it
follows that ğœƒâˆ—=ğ‘ğ‘¡(ğ‘Œ).
Please refer to the Appendix B.2 for a detailed proof. To enforce
the matching between ğ‘ğœƒğ‘ (ğ‘‹)andğ‘ğ‘¡(ğ‘‹)effectively, we employ the
Kernel Mean Matching (KMM) for two distributions and obtain the
squared Maximum Mean Discrepancy (MMD) loss:
min
ğœƒEğ‘‹âˆ¼ğ‘ğœƒğ‘ (ğ‘‹)[ğœ‘(ğ‘‹)]âˆ’Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)[ğœ‘(ğ‘‹)]2
, (11)
whereğœ‘(Â·)is a kernel mapping. On this basis, we have
Eğ‘‹âˆ¼ğ‘ğœƒğ‘ (ğ‘‹)[ğœ‘(ğ‘‹)]=âˆ«
ğœ‘(ğ‘‹)ğ‘ğœƒ
ğ‘ (ğ‘‹)ğ‘‘ğ‘‹
=âˆ¬
ğœ‘(ğ‘‹)ğ‘ğ‘ (ğ‘‹|ğ‘Œ)ğœƒğ‘‘ğ‘‹ğ‘‘ğ‘Œ =âˆ¬
ğ‘ğ‘ (ğ‘‹,ğ‘Œ)ğœƒğœ‘(ğ‘‹)
ğ‘ğ‘ (ğ‘Œ)ğ‘‘ğ‘‹ğ‘‘ğ‘Œ
=E(ğ‘‹,ğ‘Œ)âˆ¼ğ‘ğ‘ (ğ‘‹,ğ‘Œ)ğœƒğœ‘(ğ‘‹)
ğ‘ğ‘ (ğ‘Œ)
.(12)
Combined with Eq. (12), the loss Eq. (11) is the same as
min
ğœƒE(ğ‘‹,ğ‘Œ)âˆ¼ğ‘ğ‘ (ğ‘‹,ğ‘Œ)ğœƒğœ‘(ğ‘‹)
ğ‘ğ‘ (ğ‘Œ)
âˆ’Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)[ğœ‘(ğ‘‹)]2
. (13)
Therefore, we adopt Eq. (13) to match the feature marginal distri-
bution. It is worth noting that the feature marginal distribution
matching is independent of the class posterior distribution ğ‘ƒğ‘ (ğ‘Œ|ğ‘‹).
Hence, this estimation is not influenced by the quality of the source
classifier.
2.2.3 Overall Framework. Based on the above information, we
can estimate the true target label distribution ğ‘ğ‘¡(ğ‘Œ)by matching
the feature and label distribution at the same time. The overall loss
function is
ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³min
ğœƒğœ†Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)h
âˆ’logÃğ¾
ğ‘˜=1ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)i
+(1âˆ’ğœ†)E(ğ‘‹,ğ‘Œ)âˆ¼ğ‘ğ‘ (ğ‘‹,ğ‘Œ)hğœƒğœ‘(ğ‘‹)
ğ‘ğ‘ (ğ‘Œ)i
âˆ’Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)[ğœ‘(ğ‘‹)]2
,
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾].
(14)
whereğœ†âˆˆ[0,1]is the parameter that tradeoffs the loss between
two types of distribution matching. It is obvious that ğœƒ=ğ‘ğ‘¡(ğ‘Œ)is
the optimal solution of Eq. (14) according to Theorem 2.1 and 2.2.Using the labeled source samples {ğ‘¥ğ‘–,ğ‘¦ğ‘–}ğ‘›
ğ‘–=1and the unlabeled
target samples{ğ‘¥ğ‘–}ğ‘›+ğ‘š
ğ‘–=ğ‘›+1, we approximate the expectation with its
empirical average and obtain the following optimization problem:
ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³min
ğœƒğœ†
ğ‘šÃğ‘›+ğ‘š
ğ‘—=ğ‘›+1
âˆ’logÃğ¾
ğ‘˜=1ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹=ğ‘¥ğ‘—)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
+(1âˆ’ğœ†)1
ğ‘›Ãğ‘›
ğ‘–=1ğœ‘(ğ‘¥ğ‘–)ğœƒğ‘¦ğ‘–
ğ‘ğ‘ (ğ‘¦ğ‘–)âˆ’1
ğ‘šÃğ‘›+ğ‘š
ğ‘—=ğ‘›+1ğœ‘(ğ‘¥ğ‘—)2
,
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾].(15)
Sinceğ‘ğ‘ (ğ‘Œ|ğ‘‹)is unknown, let us approximate it by the output of
the source classifier. In addition, we construct the representation
vectorsğ‘‘ğ‘–=hI(ğ‘¦ğ‘–=1)
ğ‘ğ‘ (ğ‘Œ=1),I(ğ‘¦ğ‘–=2)
ğ‘ğ‘ (ğ‘Œ=2),...,I(ğ‘¦ğ‘–=ğ¾)
ğ‘ğ‘ (ğ‘Œ=ğ¾)i
,ğ‘–âˆˆ{1,...,ğ‘›}. Thus,
we design the transformation matrix DâˆˆRğ‘›Ã—ğ‘›, where the ğ‘–-th
row of Disğ‘‘ğ‘–. Then,ğœƒğ‘¦ğ‘–
ğ‘ğ‘ (ğ‘¦ğ‘–)=D[ğ‘¦ğ‘–,:]ğœƒand the MMD loss can be
reparametrized as
1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğœ‘(ğ‘¥ğ‘–)ğœƒğ‘¦ğ‘–
ğ‘ğ‘ (ğ‘¦ğ‘–)âˆ’1
ğ‘šğ‘›+ğ‘šâˆ‘ï¸
ğ‘—=ğ‘›+1ğœ‘(ğ‘¥ğ‘—)2
=ğœƒTDTKğ‘ Dğœƒ
ğ‘›2âˆ’21TKğ‘¡,ğ‘ Dğœƒ
ğ‘šğ‘›+1TKğ‘¡1
ğ‘š2,(16)
where Kğ‘ andKğ‘¡are the kernel matrix of the source and target data,
andKğ‘¡,ğ‘ is the cross kernel matrix. we adopt the Gaussian kernel in
this paper, i.e., Kğ‘–,ğ‘—=exp
âˆ’âˆ¥ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—âˆ¥2
2ğ›¿2
, whereğ›¿is the bandwidth.
Therefore, our final optimization problem is as follows:
ï£±ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£³min
ğœƒğ¿(ğœƒ)=min
ğœƒ(1âˆ’ğœ†)
ğœƒTDTKğ‘ Dğœƒ
ğ‘›2âˆ’21TKğ‘¡,ğ‘ Dğœƒ
ğ‘šğ‘›
+ğœ†
ğ‘šÃğ‘›+ğ‘š
ğ‘—=ğ‘›+1
âˆ’logÃğ¾
ğ‘˜=1ğœƒğ‘˜[ğ‘“ğ‘ (ğ‘‹=ğ‘¥ğ‘—)]ğ‘˜
Ë†ğ‘(ğ‘Œ=ğ‘˜)
,
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾].(17)
where the constant term of Eq. (16) is ignored. Optimization strate-
gies for the above problem are shown in the Appendix A.1.
2.3 Reweighted Framework
After get the estimated parameter Ë†ğœƒ, we can now correct the source
classifier. Specifically, Our goal is to find a new calssifier ğ‘“ğ‘¡(Â·)which
minimizes
ğ¿(ğ‘“ğ‘¡)=ğ¸(ğ‘‹,ğ‘Œ)âˆ¼ğ‘ƒğ‘¡(ğ‘‹,ğ‘Œ)[ğ‘™(ğ‘Œ,ğ‘“ğ‘¡(ğ‘‹))]
=ğ¸(ğ‘‹,ğ‘Œ)âˆ¼ğ‘ƒğ‘ (ğ‘‹,ğ‘Œ)[ğ‘ğ‘¡(ğ‘Œ)ğ‘™(ğ‘Œ,ğ‘“ğ‘¡(ğ‘‹))/ğ‘ğ‘ (ğ‘Œ)].(18)
In the typical finite sample scenario, we lack knowledge of the dis-
tribution of the source domain and only have access to the observed
source data{ğ‘¥ğ‘–,ğ‘¦ğ‘–}ğ‘›
ğ‘–=1. As a result, we need to seek the minimizer of
the empirical loss using the importance weighted ERM framework.
ğ¿(ğ‘“ğ‘¡;Ë†ğœƒ)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1Ë†ğœƒ(ğ‘¦ğ‘–)
ğ‘ğ‘ (ğ‘¦ğ‘–)ğ‘™(ğ‘¦ğ‘–,ğ‘“ğ‘¡(ğ‘¥ğ‘–)). (19)
3 MULTI-SHOT BMDM
In the previous subsection, we presented our basic strategy for
the one-shot case. In this section, we extend our approach to the
multi-shot scenario and propose the Multi-shot BMDM (MBMDM)
method to address the challenge of effectively utilizing the esti-
mated weights from intermediate processes. In many real-world
applications, the label shift occurs gradually over multiple stages,
which is often overlooked by existing label shift methods. Figure
 
738Label Shift Correction via Bidirectional Marginal Distribution Matching KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 3: Illustration of multi-shot label shift scenario. In
plant classification, the data comes in the form of a block
stream, and correspondingly, the label distribution changes.
3 provides a brief overview of the notations used in the multi-
shot case, following a similar structure as Figure 1. Assume there
areğ‘…stages of gradual change, and the corresponding probability
density or mass functions are denoted as {ğ‘ğ‘¡1(Â·),...,ğ‘ğ‘¡ğ‘…(Â·)}, where
ğ‘ğ‘¡1(Â·)=ğ‘ğ‘ (Â·). We make the following assumption:
Assumption 1. For progressive label shift, the class-conditional
distribution remains unchanged, i.e., ğ‘ğ‘ (ğ‘‹|ğ‘Œ)=ğ‘ğ‘¡1(ğ‘‹|ğ‘Œ)=...=
ğ‘ğ‘¡ğ‘…(ğ‘‹|ğ‘Œ), and the label marginal distribution changes gradually, i.e.,
ğ‘ğ‘¡ğ‘Ÿ(ğ‘Œ)=ğ›¾ğ‘Ÿğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)+(1âˆ’ğ›¾ğ‘Ÿ)ğ‘ğ‘¡ğ‘Ÿ+1(ğ‘Œ), whereğ‘Ÿâˆˆ[2,3,...,ğ‘…âˆ’1]
andğ›¾ğ‘Ÿâˆˆ(0,1).
Our goal is to obtain the corrected labels in the final stage, which
is equivalent to estimate ğ‘ğ‘¡ğ‘…(ğ‘Œ|ğ‘‹). However, directly scaling the
one-shot BMDM formulation from Eq. (17) poses several challenges:
(1) It is challenging to effectively utilize the properties of the inter-
mediate label distributions. (2) Storing data at all stages may lead to
an excessive burden. (3) Carrying out multi-stage transformations
can result in high computational complexity.
To overcome the above difficulties, we propose the following
strategy. In the ğ‘Ÿ-th stage, assuming that we can only utilize data
from the previous and current stages {ğ‘‹ğ‘ ,ğ‘‹ğ‘Ÿ}, the pseudo labels
obtained from the previous stage Ë†ğ‘Œğ‘Ÿâˆ’1, the estimated label distribu-
tions{Ë†ğ‘ğ‘ (ğ‘Œ),Ë†ğ‘ğ‘Ÿâˆ’1(ğ‘Œ)}and the source classifier ğ‘“(Â·). Similar to the
one-shot case, we try to solve the above problem from the perspec-
tive of bidirectional distributed matching. Our estimator consists
of three main components:
(1). In theğ‘Ÿ-th stage, to match the label marginal distribution,
we employ a similar approach to Eq. (8), yielding:
ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³min
ğœƒEğ‘‹âˆ¼ğ‘ğ‘¡ğ‘Ÿ(ğ‘‹)h
âˆ’logÃğ¾
ğ‘˜=1ğœƒğ‘˜ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ=ğ‘˜)i
,
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾],(20)
where the optimal solution is ğœƒâˆ—=ğ‘ğ‘¡ğ‘Ÿ(ğ‘Œ). However, it is tedious
to estimateğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ=ğ‘˜|ğ‘‹âˆ¼ğ‘ğ‘¡ğ‘Ÿ(ğ‘‹))due to the absence of (ğ‘Ÿâˆ’1)-
th stage classifier. In order to address this issue, we propose the
following theorem:Theorem 3.1.âˆ€ğ‘‹âˆ¼ğ‘ğ‘¡ğ‘Ÿ(ğ‘‹), the following equation holds
ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ|ğ‘‹)
ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)=ğ‘ğ‘Ÿâˆ’1(ğ‘‹)ğ‘ğ‘ (ğ‘Œ|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ),âˆ€ğ‘Ÿâˆˆ{2,3,...,ğ‘…}, (21)
whereğ‘(ğ‘‹)is a cluster of constants independent of ğ‘Œ.
Please refer to the Appendix B.3 for a detailed proof. Building
upon Theorem 3.1, the optimization problem Eq.(20) is transformed
as follows:
ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³min
ğœƒEğ‘‹âˆ¼ğ‘ğ‘¡ğ‘Ÿ(ğ‘‹)h
âˆ’logÃğ¾
ğ‘˜=1ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
âˆ’logğ‘ğ‘Ÿâˆ’1(ğ‘‹)i
,
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾].
(22)
SinceEğ‘‹âˆ¼ğ‘ğ‘ (ğ‘‹)[logğ‘ğ‘Ÿâˆ’1(ğ‘‹)]is a constant which is not related to
the optimization of ğœƒ, we can ignore it.
(2). In theğ‘Ÿ-th stage, to match the feature marginal distribution,
similar to Eq. (16), we need to optimize the following problem,
min
ğœƒğœƒTDTKğ‘ Dğœƒ
ğ‘›2âˆ’21TKğ‘Ÿ,ğ‘ Dğœƒ
ğ‘›ğ‘›ğ‘Ÿ. (23)
(3). To reuse the previous label distributions ğ‘ğ‘ (ğ‘Œ),ğ‘ğ‘Ÿâˆ’1(ğ‘Œ), we
aim to design a regularization term under Assumption 1. Prior to
that, we assert the following theorem to be valid.
Theorem 3.2.âˆƒğ›¾âˆˆRwhich is determined by {ğ›¾ğ‘—}ğ‘Ÿâˆ’1
ğ‘—=1, the fol-
lowing equation holds
ğ‘ğ‘¡ğ‘Ÿ(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ)=ğ›¾ ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ). (24)
Please refer to the Appendix B.4 for a detailed proof. Hence, we
observe that{ğ‘ğ‘¡ğ‘–(ğ‘Œ)}ğ‘Ÿ
ğ‘–=1lies on a straight line in the ğ¾-dimensional
space. However, as the parameter ğ›¾is unknown, it is not possible to
directly obtain ğ‘ğ‘¡ğ‘Ÿ(ğ‘Œ)by Eq. (24). Therefore, we propose optimizing
the angle between pairwise vectors, i.e.,
arg min
ğœƒ 
1âˆ’
ğœƒâˆ’ğ‘ğ‘ (ğ‘Œ),ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ)
âˆ¥ğœƒâˆ’ğ‘ğ‘ (ğ‘Œ)âˆ¥ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ)!
. (25)
Ifğœƒ=ğ‘ğ‘¡ğ‘Ÿ(ğ‘Œ), the above loss reaches the minimum value 0.
To sum up, we propose the following loss function for MBMDM:
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³min
ğœƒğ¿ğ‘€(ğœƒ)=min
ğœƒ(1âˆ’ğœ†)
ğœƒTDTKğ‘ Dğœƒ
ğ‘›2âˆ’21TKğ‘Ÿ,ğ‘ Dğœƒ
ğ‘›ğ‘›ğ‘Ÿ
+ğœ†
ğ‘›ğ‘ŸÃğ‘›ğ‘Ÿ
ğ‘—=1
âˆ’logÃğ¾
ğ‘˜=1ğœƒğ‘˜[ğ‘“ğ‘ (ğ‘‹=ğ‘¥ğ‘—)]ğ‘˜
Ë†ğ‘(ğ‘Œ=ğ‘˜)
âˆ’ğœ”âŸ¨ğœƒâˆ’Ë†ğ‘ğ‘ (ğ‘Œ),Ë†ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’Ë†ğ‘ğ‘ (ğ‘Œ)âŸ©
âˆ¥ğœƒâˆ’Ë†ğ‘ğ‘ (ğ‘Œ)âˆ¥âˆ¥Ë†ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’Ë†ğ‘ğ‘ (ğ‘Œ)âˆ¥,
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾],(26)
whereğœ†andğœ”are the parameters to balance various losses. The
optimization strategy for the aforementioned loss function is similar
to Eq. (17), and further details can be found in the Appendix A.2.
4 EXPERIMENTS
4.1 Experimental Setup
Datasets.We conduct performance evaluations of BMDM and
MBMDM using artificial shifts on the MNIST, CIFAR10, and CI-
FAR100 datasets. In the first, each dataset is divided into two parts:
the source and target domain randomly. Then, we consider two
types of shift in our experiments: (1) Tweak-One shift, which makes
the probability of a certain source class change to ğœŒ, and the prob-
ability of other source classes keep the same ratio. (2) Dirichlet
 
739KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruidong Fan, Xiao Ouyang, Hong Tao, and Chenping Hou
shift, which generates a Dirichlet distribution by the concentration
parameterğ›¼and makes the source label scale consistent with it.
We then sample the source and target domain data based on the
aforementioned proportions, creating the source and target sets,
respectively. For the MNIST and CIFAR10 datasets, we use 10,000
source and 8,000 target samples. Considering the larger number
of categories, we set the number of source and target samples as
20,000 and 10,000, respectively, for the CIFAR100 dataset.
Compared methods. In the experiment, we compare BMDM
and MBMDM with several advanced label shift methods to show-
case their performance. The methods we consider are as follows:
(1) BBSE (2018) and RLLS (2019): These label shift methods are
based on hard confusion matrix. Additionally, we have BBSE-soft
and RLLS-soft, which are extensions of the former methods based
on soft confusion matrix. (2) EM (2020): A representative method
based on maximum likelihood estimation. Moreover, based on this
approach, we have MAP (2019), which assumes that the source
label distribution follows a given prior. (3) SML (2022): This method
combines the EM method with a soft confusion matrix. (4) WW: A
variant that demonstrates unweighted performance.
Network architecture and evaluation indicators. All meth-
ods have the flexibility to utilize any classifier as the base classifier
for estimating their weights. Additionally, when other conditions
remain unchanged, a more accurate base (source) classifier leads to
more accurate estimated weights and a reweighted target classifier.
In our experiment, we employ a two-layer fully connected neural
network for the MNIST dataset, and ResNet18 for the CIFAR10
and CIFAR100 datasets. For each shift type, we randomly sample
10 times for each distribution parameter to evaluate the F-score,
Accuracy (Acc), and Mean Square Error (MSE) of the estimated
weights. To be specific, MSE is defined as follows:
MSE(Ë†ğœƒ)=1
len(ğœƒ)Ë†ğœƒ
Ë†ğ‘ğ‘ (ğ‘Œ)âˆ’ğ‘ğ‘¡(ğ‘Œ)
ğ‘ğ‘ (ğ‘Œ)2
. (27)
Parameter setting. For the compared methods, all the parame-
ters are determined according to the strategies provided by their
respective references. In the case of BMDM, the bandwidth parame-
terğ›¿of the kernel matrices is fixed at 1, and the balance parameter
ğœ†is selected from the set {0,0.2,0.4,0.6,0.8,1}. As for MBMDM,
we set the parameter ğœ”from the set{10âˆ’3,10âˆ’2,10âˆ’1,1,10,100}.
The optimal combination of parameters is chosen based on the
validation set. Additionally, we choose the values of ğ›¼from the set
{0.5,1,5}(where smaller values of ğ›¼result in more extreme label
shift), and the values of ğœŒfrom the set{0.5,0.7,0.9}(where larger
values ofğœŒresult in more extreme label shift).
4.2 BMDM Performance Comparison
Firstly, to verify the validity of the proposed BMDM, we compare
it with advanced label shift methods, and investigate F-score, Acc
and MSE on the Dirichlet and Tweak-One shift datasets. Itâ€™s worth
noting that we make the proportion of 10 classes sum to ğœŒon
Tweak-One shift CIFAR100 dataset due to the large number of
classes. All methods run on framework with Python 3.7. The exper-
imental results are displayed in Table 1 and we have the following
observations.(1) The results presented in Table 1 demonstrate that in most
cases, BMDM outperforms other approaches in terms of both mean
and variance. This indicates an improvement in effectiveness and
stability. (2) Generally, BMDM performs better with larger label
shifts, thanks to its feature matching strategy. With increased label
shift, the performance of the source classifier tends to decrease,
leading to a decline in the accuracy of estimated weights. However,
since feature matching is not affected by the source classifierâ€™s
performance, the accuracy and stability of weight estimation can
be enhanced. (3) The MSE solely depends on the estimated weights,
whereas Acc and F-score are influenced not only by the estimated
weights but also by the reweighting framework. Therefore, MSE can
provide a more intuitive measure of the effectiveness of the methods.
It can be observed that although our method may show only slight
improvements in Acc and F-score for some datasets, there is a
significant improvement in the MSE. This further illustrates the
effectiveness of our approach.
4.3 MBMDM Performance Comparison
In this case, in addition to the previous compared approaches, we
also make a comparison with BMDM. We divide each data into
source, middle and target sets, which is equivalent to a three-
shot scenario. The amount of data in the source and target sets
is consistent with that in one-shot scenario and the data amount
of middle set is 8000 for all datasets. Compared approaches that
include BMDM are trained only on the source and target sets with
Dirichlet parameter ğ›¼âˆˆ{0.1,0.3}. To make sure the data meets
our multi-stage assumption, we set the middle distribution to be
ğ‘ƒğ‘Ÿ(ğ‘Œ)=0.5ğ‘ğ‘ (ğ‘Œ)+0.5ğ‘ƒğ‘¡(ğ‘Œ)and sample accordingly. We report the
experimental results in Table 2 and there are several observations
as follows.
(1) The performance of our BMDM and MBMDM is still better
than other approaches in most cases. (2) Compared with BMDM,
MBMDM delivers improvements across all datasets, which shows
the validity of the regularization constraint we designed. Interest-
ingly, it can be seen that Acc and F-score show a trend of reverse
improvement with the increase of label shift degree.
4.4 Performance Analyses
We further analyze the performance of our proposed BMDM and
MBMDM from different aspects in this subsection.
4.4.1 The influence of base classifiers. As observed in Figure
4, we employed MLP and ResNet18 as the base classifiers on the
CIFAR10 dataset to investigate their impact on model performance.
Through our analysis, we discovered that as the base classifierâ€™s
performance improves, the estimated weight becomes more accu-
rate, subsequently leading to an enhancement in the performance
of the target classifier as well.
4.4.2 Convergence analysis. As depicted in Algorithm 1, we
employ a linear search technique to guarantee the convergence of
the gradient descent method. For illustrative purposes, we showcase
the objective function values in Figure 5, and the experimental
results validate the convergence of both BMDM and MBMDM. It is
evident that our optimization algorithm converges rapidly, typically
requiring fewer than 30 iterations.
 
740Label Shift Correction via Bidirectional Marginal Distribution Matching KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Performance(mean(std)) comparison on Dirichlet and Tweak-One shift datasets. The best results are boldfaced.
Methodsğ›¼=0.5 ğ›¼=1 ğ›¼=5
F-score Acc MSE F-score Acc MSE F-score Acc MSE
MNIST
WW .7014(.052) .7482(.042) 8.23e+3(4.36e+3) .8471(.037) .8625(.026) 1.79e+2(8.23e+1) .9128(.007) .9057(.009) 2.52e-1(7.93e-2)
BBSE .8573(.014) .8655(.012) 1.37e+1(5.82e+0) .9121(.005) .9114(.004) 2.19e+0(4.65e-1) .9331(.003) .9334(.003) 8.96e-3(4.01e-4)
RLLS .8625(.018) .8741(.015) 1.02e+1(3.34e+0) .9277(.004) .9207(.004) 1.83e+0(2.39e-1) .9342(.003) .9345(.003) 6.34e-3(3.45e-4)
BBSE-soft .8557(.013) .8644(.011) 1.07e+1(4.13e+0) .9127(.003) .9120(.003) 7.44e-1(1.02e-1) .9332(.002) .9335(.002) 1.67e-3(2.16e-4)
RLLS-soft .8670(.010) .8749(.009) 8.32e+0(2.76e+0) .9281(.003) .9221(.003) 5.71e-1(4.39e-2) .9342(.004) .9344(.004) 2.39e-3(3.82e-4)
EM .8811(.007) .8815(.007) 6.74e-1(8.43e-2) .9271(.002) .9263(.002) 3.51e-1(6.39e-2) .9345(.001) .9346(.001) 2.08e-4(7.61e-5)
MAP .8821(.008) .8825(.008) 6.48e-1(1.83e-1) .9312(.003) .9311(.003) 3.43e-1(3.58e-2) .9355(.002) .9356(.002) 1.53e-3(4.97e-4)
SML .8768(.011) .8744(.008) 3.55e+0(6.02e-1) .9233(.002) .9270(.002) 4.06e-1(3.58e-2) .9338(.002) .9342(.002) 3.82e-4(5.26e-5)
BMDM .8772(.008) .8862(.009) 3.47e-1(3.51e-2) .9338(.003) .9340(.003) 2.98e-1(4.84e-2) .9373(.003) .9375(.003) 1.85e-4(3.69e-5)
CIFAR10
WW .3213(.048) .5096(.031) 6.79e+3(3.68e+3) .4471(.031) .5946(.027) 3.96e+2(1.17e+2) .6306(.013) .6717(.011) 3.88e-1(4.12e-2)
BBSE .5239(.017) .6435(.012) 2.14e+1(3.28e+0) .5942(.007) .6704(.004) 9.73e+0(2.18e+0) .6897(.003) .7137(.002) 2.02e-2(5.87e-3)
RLLS .5959(.016) .6569(.011) 1.61e+1(5.80e+0) .5709(.006) .6711(.004) 8.33e+0(8.72e-1) .6699(.002) .7065(.002) 2.14e-2(3.28e-3)
BBSE-soft .5138(.023) .6029(.015) 2.69e+1(7.84e+0) .5194(.012) .5743(.009) 2.26e+1(8.69e+0) .6504(.006) .6873(.004) 2.62e-1(6.58e-2)
RLLS-soft .5388(.015) .6441(.011) 2.35e+1(5.11e+0) .5887(.008) .6702(.004) 1.39e+1(4.16e+0) .6898(.003) .7248(.003) 3.21e-2(4.14e-3)
EM .6296(.011) .6409(.011) 1.46e+1(2.59e+0) .6478(.003) .6586(.003) 9.39e-1(2.72e-1) .6772(.004) .7116(.002) 1.03e-2(2.65e-3)
MAP .6124(.012) .6335(.012) 1.11e+1(4.03e+0) .6478(.004) .6609(.003) 2.42e+0(3.81e-1) .6625(.002) .7071(.002) 1.51e-1(5.06e-2)
SML .6112(.011) .6285(.009) 1.49e+1(1.86e+0) .6381(.007) .6644(.004) 5.21e+0(9.15e-1) .6895(.004) .7191(.002) 1.48e-2(3.81e-3)
BMDM .6443(.011) .6658(.011) 7.26e+0(1.85e+0) .6641(.003) .6903(.002) 4.63e-1(2.79e-2) .7028(.003) .7324(.002) 8.10e-3(9.02e-4)
CIFAR100
WW .1193(.072) .1949(.065) 5.42e+5(2.71e+5) .2812(.067) .3494(.039) 2.55e+2(8.37e+1) .3669(.035) .4478(.031) 3.69e-1(9.88e-2)
BBSE .3008(.016) .3719(.013) 2.30e+1(7.96e+0) .3915(.014) .4401(.012) 6.17e+0(8.95e-1) .4217(.009) .4967(.011) 1.36e-1(3.68e-2)
RLLS .3142(.013) .3911(.011) 9.35e+0(5.88e+0) .3941(.015) .4416(.011) 5.94e+0(7.79e-1) .4343(.010) .4934(.009) 2.83e-1(6.67e-2)
BBSE-soft .3279(.012) .3914(.007) 1.35e+1(4.92e+0) .3692(.011) .4069(.009) 1.01e+1(2.23e+0) .3969(.011) .4778(.011) 7.34e-1(2.24e-2)
RLLS-soft .3193(.013) .3969(.007) 1.05e+1(6.38e+0) .3625(.013) .4104(.012) 7.99e+0(1.57e+0) .4161(.012) .4905(.011) 1.91e-1(4.63e-2)
EM .3260(.010) .3897(.009) 4.19e+0(1.31e+0) .3756(.011) .4268(.008) 2.49e-1(8.47e-2) .4211(.007) .4752(.006) 7.12e-2(6.91e-3)
MAP .3314(.007) .3932(.007) 4.33e+0(8.20e-1) .3828(.008) .4234(.007) 2.32e-1(5.30e-2) .4243(.007) .4756(.007) 6.45e-2(7.75e-3)
SML .3395(.012) .3927(.010) 7.28e+0(1.58e+0) .3869(.009) .4217(.007) 4.51e-1(6.61e-2) .4262(.009) .4415(.010) 2.83e-1(9.40e-2)
BMDM .3650(.011) .4182(.012) 1.34e+0(3.91e-1) .4072(.009) .4491(.009) 1.77e-1(8.64e-2) .4441(.007) .4934(.007) 5.08e-2(6.03e-3)
ğœŒ=0.5 ğœŒ=0.7 ğœŒ=0.9
MNIST
WW .8713(.003) .8865(.002) 2.92e+0(0.00e+0) .8394(.008) .8528(.004) 8.12e+0(0.00e+0) .7313(.012) .7620(.009) 7.29e+1(0.00e+0)
BBSE .9363(.002) .9364(.002) 2.04e-3(2.97e-4) .9119(.003) .9134(.003) 5.05e-3(6.74e-4) .8375(.011) .8419(.009) 3.17e+0(5.88e-1)
RLLS .9366(.003) .9367(.003) 2.05e-3(4.81e-4) .9118(.003) .9134(.003) 3.48e-3(8.71e-4) .8384(.008) .8423(.007) 2.04e+0(5.26e-1)
BBSE-soft .9346(.002) .9341(.002) 2.82e-3(1.65e-4) .9089(.003) .9094(.003) 8.71e-3(1.25e-3) .8378(.007) .8421(.006) 2.63e+0(7.42e-1)
RLLS-soft .9344(.003) .9347(.003) 3.12e-3(3.48e-4) .9118(.003) .9134(.003) 8.87e-3(2.14e-3) .8352(.007) .8412(.006) 2.68e+0(4.79e-1)
EM .9085(.004) .9287(.003) 5.06e-3(7.81e-4) .9081(.003) .9082(.003) 7.19e-3(5.83e-4) .8467(.007) .8466(.007) 2.45e+0(2.76e-1)
MAP .9078(.004) .9189(.004) 3.99e-1(7.41e-2) .9025(.005) .9048(.005) 1.16e+0(4.96e-1) .8268(.008) .8303(.008) 8.06e+0(2.61e+0)
SML .9082(.002) .9184(.002) 9.68e-3(2.35e-3) .9061(.002) .9059(.002) 2.11e-2(4.57e-3) .8472(.007) .8455(.007) 2.31e+0(4.18e-1)
BMDM .9327(.003) .9328(.003) 1.85e-3(4.97e-4) .9221(.002) .9226(.002) 2.25e-3(2.17e-4) .8605(.004) .8613(.004) 1.06e+0(2.72e-1)
CIFAR10
WW .5675(.009) .6243(.007) 2.92e+0(0.00e+0) .3425(.047) .4395(.041) 8.12e+0(0.00e+0) .1525(.071) .3115(.058) 7.29e+1(0.00e+0)
BBSE .6119(.006) .7002(.005) 4.83e-2(9.65e-3) .5722(.011) .6208(.007) 5.45e-1(1.99e-1) .1862(.023) .4345(.010) 3.31e+1(7.66e+0)
RLLS .6454(.005) .6970(.005) 4.17e-2(1.24e-2) .5534(.010) .6205(.005) 6.81e-1(7.83e-2) .3046(.012) .4265(.008) 2.97e+1(6.80e+0)
BBSE-soft .5772(.004) .5939(.004) 5.98e-1(6.63e-2) .4111(.008) .5078(.006) 2.97e+0(6.03e-1) .2352(.016) .2673(.014) 5.51e+1(1.21e+1)
RLLS-soft .6559(.003) .6984(.003) 1.09e-1(2.11e-2) .5309(.007) .6164(.006) 7.41e-1(2.88e-1) .2303(.018) .4769(.007) 3.92e+1(9.64e+0)
EM .6573(.002) .6755(.002) 2.61e-2(2.19e-3) .5743(.004) .5848(.003) 2.20e-1(3.37e-2) .3503(.007) .4811(.007) 1.82e+1(3.71e+0)
MAP .6355(.004) .6833(.004) 3.68e-1(7.55e-2) .4810(.007) .6147(.005) 2.24e+0(8.99e-2) .2442(.013) .4753(.008) 4.23e+1(1.08e+0)
SML .6755(.002) .6785(.002) 1.27e-1(3.27e-2) .5567(.006) .5943(.006) 7.07e-1(8.52e-2) .3054(.009) .4927(.008) 2.89e+1(5.47e+0)
BMDM .6832(.006) .7054(.002) 8.59e-3(7.60e-4) .6067(.003) .6280(.003) 1.42e-1(1.88e-2) .3633(.006) .5138(.005) 8.06e+0(7.24e-1)
CIFAR100
WW .1879(.037) .2514(.026) 2.92e+0(0.00e+0) .0948(.052) .1659(.042) 8.12e+0(0.00e+0) .0679(.052) .1035(.042) 7.29e+1(0.00e+0)
BBSE .2614(.012) .3169(.011) 5.21e-1(6.72e-2) .1698(.015) .2252(.019) 2.77e+0(7.81e-1) .0904(.026) .1608(.028) 3.86e+0(1.44e+0)
RLLS .2621(.012) .3278(.011) 5.34e-1(7.03e-2) .1496(.015) .2427(.014) 2.69e+0(6.56e-1) .1549(.019) .2334(.021) 3.07e+0(6.19e-1)
BBSE-soft .2082(.014) .2562(.014) 2.83e+0(5.91e-1) .1173(.015) .1778(.017) 8.17e+0(3.32e+0) .0974(.022) .1354(.023) 1.03e+1(2.16e-4)
RLLS-soft .2146(.011) .2606(.011) 1.36e+0(4.17e-1) .1229(.013) .1969(.009) 4.92e+0(1.85e+0) .1108(.015) .1489(.015) 4.43e+0(3.82e-4)
EM .2236(.007) .3261(.009) 7.57e-1(1.88e-1) .1607(.009) .2406(.011) 2.31e+0(4.83e-1) .1386(.012) .2401(.013) 4.13e+0(7.61e-5)
MAP .2387(.005) .3321(.004) 5.25e-1(4.33e-2) .1694(.011) .2596(.010) 3.58e+0(6.17e-1) .1494(.011) .2353(.012) 3.32e+0(4.97e-4)
SML .2628(.006) .3072(.007) 4.39e-1(6.67e-2) .1442(.009) .2497(.009) 3.29e+0(4.98e-1) .1497(.014) .2342(.016) 3.29e+0(5.26e-5)
BMDM .2705(.011) .3461(.007) 2.61e-1(5.47e-2) .1777(.008) .2727(.005) 1.66e+0(2.47e-1) .1688(.012) .2496(.009) 2.26e+0(3.69e-5)
 
741KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruidong Fan, Xiao Ouyang, Hong Tao, and Chenping Hou
Table 2: Performance(mean(std)) comparison on Tweak-One and Dirichlet shift datasets. The best results are boldfaced.
Methodsğ›¼=0.1 ğ›¼=0.3
F-score Acc MSE F-score Acc MSE
MNIST
BBSE .7563(.023) .8295(.012) 1.10e+2(2.21e+1) .8326(.015) .8597(.016) 7.78e+1(8.81e+0)
RLLS .7513(.017) .8273(.015) 1.02e+2(8.65e+0) .8340(.012) .8606(.009) 7.65e+1(5.45e+0)
BBSE-soft .7421(.019) .8271(.017) 1.07e+2(1.48e+1) .8371(.014) .8615(.017) 5.22e+1(7.74e+0)
RLLS-soft .7564(.012) .8261(.014) 1.13e+2(9.55e+0) .8369(.013) .8618(.010) 5.07e+1(6.96e+0)
EM .8111(.011) .8156(.010) 1.92e+1(1.14e+0) .8639(.009) .8639(.009) 1.44e+1(2.25e+0)
MAP .8023(.014) .8218(.012) 4.85e+1(6.73e+0) .8594(.009) .8665(.008) 2.27e+1(2.54e+0)
SML .8047(.012) .8234(.009) 4.92e+1(5.13e+0) .8535(.010) .8563(.010) 5.56e+1(3.77e+0)
BMDM .7913(.013) .8389(.011) 1.48e+1(8.76e-1) .8514(.009) .8732(.009) 9.89e+0(5.58e-1)
MBMDM .8316(.007) .8517(.006) 3.71e+0(6.34e-2) .8829(.004) .8866(.004) 1.45e+0(1 .77e-2)
CIFAR10
BBSE .3162(.016) .5281(.014) 1.80e+2(2.27e+1) .5451(.013) .6198(.014) 6.77e+1(4.83e+0)
RLLS .3402(.014) .5144(.015) 1.68e+2(1.68e+1) .5694(.011) .6296(.009) 6.03e+1(3.24e+0)
BBSE-soft .2506(.013) .3998(.021) 1.85e+2(3.79e+1) .4693(.014) .6200(.012) 7.71e+1(7.38e+0)
RLLS-soft .3421(.010) .3811(.017) 1.56e+2(3.91e+1) .5818(.009) .6397(.010) 5.87e+1(4.65e+0)
EM .4134(.012) .5222(.010) 6.54e+1(4.26e+0) .6023(.011) .6395(.010) 2.06e+1(1.85e+0)
MAP .3758(.014) .4704(.018) 8.56e+1(1.15e+1) .5829(.012) .6307(.011) 2.56e+1(4.77e+0)
SML .3802(.012) .4781(.011) 1.06e+2(9.80e+0) .6006(.012) .6367(.011) 5.58e+1(6.06e+0)
BMDM .4390(.009) .5066(.008) 4.17e+1(2.36e+0) .6132(.010) .6423(.011) 1.28e+1(1.74e+0)
MBMDM .4461(.006) .5458(.005) 2.07e+1(4.73e-1) .6338(.007) .6682(.007) 2.61e+0(8.49e-2)
CIFAR100
BBSE .1423(.013) .2422(.016) 2.30e+1(5.11e+0) .2566(.012) .3549(.010) 1.23e+1(2.14e+0)
RLLS .1658(.011) .2841(.013) 1.07e+1(2.74e+0) .2908(.011) .4079(.010) 7.42e+0(9.47e-1)
BBSE-soft .1713(.016) .1993(.023) 1.67e+1(4.23e+0) .2218(.014) .3549(.012) 1.37e+1(2.08e+0)
RLLS-soft .1636(.010) .2688(.011) 1.36e+1(2.08e+0) .2615(.011) .3481(.016) 9.01e+0(1.84e+0)
EM .1904(.012) .2725(.009) 6.96e+0(9.65e-1) .3006(.010) .3951(.011) 5.73e+0(7.07e-1)
MAP .2080(.010) .3018(.008) 5.12e+0(4.15e-1) .3106(.008) .3858(.010) 4.27e+0(3.91e-1)
SML .1813(.014) .3176(.011) 1.38e+1(2.66e+0) .2438(.014) .4016(.009) 9.05e+0(1.18e-1)
BMDM .2279(.012) .3086(.011) 5.83e+0(6.59e-1) .3209(.009) .3947(.010) 3.12e+0(4.00e-1)
MBMDM .2433(.008) .3201(.006) 2.75e+0(1.43e-1) .3668(.007) .4188(.007) 9.61e-1(3.06e-2)
0.5 1 50.30.40.50.60.70.8Acc
BMDM-RES
BMDM-MLP
(a) CIFAR10 (Acc)
0.5 1 5051015MSEBMDM-RES
BMDM-MLP (b) CIFAR10 (MSE)
Figure 4: The impact of different base classifiers on the per-
formance of BMDM on CIFAR10 dataset.
20 40 60 80 100
Iteration Times-0.6-0.5-0.4-0.3-0.2-0.10Obeject Value=0.5
=1
=5
(a) MNIST (BMDM)
20 40 60 80 100
Iteration Times-4-3-2-10Obeject Value=0.3
=0.1 (b) CIFAR10 (MBMDM)
Figure 5: The convergence behavior of BMDM and MBMDM.4.4.3 Target Data Volume Influence Analysis. From the obser-
vations in Figure 6, where we set the target data volume as ğ‘šfrom
the set 2000,400,8000, we note the following: (1) As the degree
0.1 0.3 0.50.60.70.80.91Acc
m=2000
m=4000
m=8000
(a) MNIST (BMDM)
0.1 0.3 0.50.30.40.50.60.7Acc
m=2000
m=4000
m=8000 (b) CIFAR10 (BMDM)
0.1 0.30.60.70.80.91Acc
m=2000
m=4000
m=8000
(c) MNIST (MBMDM)
0.1 0.30.30.40.50.60.7Acc
m=2000
m=4000
m=8000 (d) CIFAR10 (MBMDM)
Figure 6: The impact of target data volume on BMDM and
MBMDM performance on MNIST and CIFAR10 datasets.
of label shift increases, the impact of target data volume on Acc
 
742Label Shift Correction via Bidirectional Marginal Distribution Matching KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
becomes more pronounced. This can be attributed to the fact that
larger label shifts lead to less accurate weight estimation, requiring
a greater amount of target data to compensate for the estimation
error. (2) MBMDM is less affected by the amount of target data
compared to BMDM. This is due to the fact that the target weight
estimation in MBMDM is influenced by the intermediate weight,
making it less sensitive to variations in the volume of target data.
4.4.4 Parameter analysis. In our BMDM method, we have a
parameterğœ†, while in MBMDM, we have another parameter ğœ”. In
this subsection, we illustrate the impact of different values of ğœ†
andğœ”on our BMDM and MBMDM methods, as shown in Figure 7.
Based on our observations, we make the following remarks:
(1) In the case of BMDM, we observe that the parameter ğœ†sig-
nificantly influences the experimental results. This finding demon-
strates the effectiveness of our bidirectional matching strategy. (2)
In the case of MBMDM, the method is not highly sensitive to the
parameterğœ”within a certain range. This characteristic simplifies
the adjustment of the parameter ğœ”.
0 0.2 0.4 0.6 0.8 1012345MSE
(a) MNIST (BMDM)
0 0.2 0.4 0.6 0.8 100.20.40.60.81MSE (b) CIFAR100 (BMDM)
10-310-210-1 1 10 1000
0.2
0.4
0.6
0.8
1
49.75 131.3 5.512
7.776
8.201
8.504
8.712
8.7533.704
7.42
8.12
8.306
8.558
8.6794.205
6.074
6.942
7.548
7.837
7.9574.948
5.019
5.279
5.437
5.854
5.84134.45
23.89
5.442
5.521
5.09737.02
24.12
5.831
5.65
5.69
20406080100120
(c) MNIST (MBMDM)
10-310-210-1 1 10 1000
0.2
0.4
0.6
0.8
1
41.76
48.81
58.1
61.92
63.72
64.4341.76
43.33
53.7
58.25
60.62
61.6141.76
39.35
43.01
45.7641.76 41.76
28.33
34.9132.57
21.63
22.78
23.88
24.93
25.9320.77
20.91
21.04
21.15
21.2820.83
20.83
20.83
20.83
20.79
2530354045505560 (d) CIFAR10 (MBMDM)
Figure 7: The impact of parameters on the MSE indicator of
BMDM and MBMDM.
4.4.5 The impact of the number of stages. To assess the im-
pact of the number of stages R, we conducted 4-shot and 5-shot
experiments on the MNIST and CIFAR10 dataset with ğ›¼=0.3,
where the label distributions are defined as follows: For 4-shot:
ğ‘ğ‘¡(ğ‘Œ)=1
3ğ‘ƒğ‘ (ğ‘Œ)+1
3ğ‘ƒ1(ğ‘Œ)+1
3ğ‘ƒ2(ğ‘Œ); For 5-shot: ğ‘ğ‘¡(ğ‘Œ)=1
4ğ‘ƒğ‘ (ğ‘Œ)+
1
4ğ‘ƒ1(ğ‘Œ)+1
4ğ‘ƒ2(ğ‘Œ)+1
4ğ‘ƒ3(ğ‘Œ). From the experimental results as shown
in Figure 8, it can be observed that as the number of stages R
increases, the estimation of the label distribution becomes more
accurate, leading to improved model performance.
4.4.6 Confusion matrix .As illustrated in Figure 9, we visualize
the confusion matrix for both BMDM and MBMDM to demonstrate
the accuracy achieved for each class. It is evident that we have
obtained favorable results for the majority of classes. However, in
some cases where performance is subpar, label shift appears to be
(a) MNIST
 (b) CIFAR10
Figure 8: The impact of the number of stages.
the primary contributing factor. Furthermore, we observed that the
confusion matrices contain significant values in several off-diagonal
positions, indicating a potential correlation among misclassified
classes.
12345678910
Target Class1
2
3
4
5
6
7
8
9
10Output Class Confusion Matrix
782
9.8%
0
0.0%
1
0.0%
0
0.0%
1
0.0%
6
0.1%
3
0.0%
1
0.0%
6
0.1%
0
0.0%
97.8%
2.2%0
0.0%
778
9.7%
2
0.0%
2
0.0%
1
0.0%
7
0.1%
1
0.0%
0
0.0%
8
0.1%
1
0.0%
97.3%
2.7%5
0.1%
2
0.0%
759
9.5%
5
0.1%
3
0.0%
3
0.0%
6
0.1%
5
0.1%
10
0.1%
2
0.0%
94.9%
5.1%1
0.0%
2
0.0%
21
0.3%
706
8.8%
0
0.0%
33
0.4%
1
0.0%
8
0.1%
15
0.2%
13
0.2%
88.3%
11.8%6
0.1%
4
0.1%
7
0.1%
4
0.1%
663
8.3%
2
0.0%
12
0.1%
4
0.1%
6
0.1%
92
1.1%
82.9%
17.1%16
0.2%
9
0.1%
5
0.1%
50
0.6%
11
0.1%
630
7.9%
32
0.4%
7
0.1%
33
0.4%
7
0.1%
78.8%
21.3%6
0.1%
5
0.1%
3
0.0%
1
0.0%
5
0.1%
2
0.0%
778
9.7%
0
0.0%
0
0.0%
0
0.0%
97.3%
2.7%2
0.0%
22
0.3%
10
0.1%
6
0.1%
8
0.1%
2
0.0%
0
0.0%
708
8.8%
1
0.0%
41
0.5%
88.5%
11.5%3
0.0%
38
0.5%
16
0.2%
74
0.9%
7
0.1%
78
1.0%
15
0.2%
1
0.0%
536
6.7%
32
0.4%
67.0%
33.0%10
0.1%
9
0.1%
2
0.0%
20
0.3%
17
0.2%
9
0.1%
1
0.0%
84
1.1%
16
0.2%
632
7.9%
79.0%
21.0%94.1%
5.9%
89.5%
10.5%
91.9%
8.1%
81.3%
18.7%
92.6%
7.4%
81.6%
18.4%
91.6%
8.4%
86.6%
13.4%
84.9%
15.1%
77.1%
22.9%
87.2%
12.8%
(a) MNIST (BMDM)
12345678910
Target Class1
2
3
4
5
6
7
8
9
10Output Class Confusion Matrix
565
7.1%
36
0.4%
8
0.1%
32
0.4%
12
0.1%
4
0.1%
6
0.1%
51
0.6%
66
0.8%
20
0.3%
70.6%
29.4%6
0.1%
707
8.8%
0
0.0%
8
0.1%
0
0.0%
1
0.0%
3
0.0%
21
0.3%
17
0.2%
37
0.5%
88.4%
11.6%79
1.0%
7
0.1%
276
3.5%
62
0.8%
85
1.1%
50
0.6%
63
0.8%
149
1.9%
28
0.4%
1
0.0%
34.5%
65.5%16
0.2%
12
0.1%
12
0.1%
489
6.1%
36
0.4%
95
1.2%
36
0.4%
90
1.1%
11
0.1%
3
0.0%
61.1%
38.9%14
0.2%
0
0.0%
5
0.1%
60
0.8%
526
6.6%
13
0.2%
33
0.4%
137
1.7%
10
0.1%
2
0.0%
65.8%
34.3%3
0.0%
7
0.1%
10
0.1%
196
2.5%
24
0.3%
376
4.7%
14
0.2%
164
2.1%
5
0.1%
1
0.0%
47.0%
53.0%7
0.1%
15
0.2%
10
0.1%
83
1.0%
85
1.1%
20
0.3%
506
6.3%
56
0.7%
7
0.1%
11
0.1%
63.2%
36.8%18
0.2%
13
0.2%
3
0.0%
28
0.4%
100
1.3%
26
0.3%
8
0.1%
591
7.4%
2
0.0%
11
0.1%
73.9%
26.1%54
0.7%
31
0.4%
1
0.0%
19
0.2%
4
0.1%
1
0.0%
4
0.1%
11
0.1%
655
8.2%
20
0.3%
81.9%
18.1%15
0.2%
69
0.9%
1
0.0%
9
0.1%
3
0.0%
2
0.0%
2
0.0%
41
0.5%
13
0.2%
645
8.1%
80.6%
19.4%72.7%
27.3%
78.8%
21.2%
84.7%
15.3%
49.6%
50.4%
60.1%
39.9%
63.9%
36.1%
75.0%
25.0%
45.1%
54.9%
80.5%
19.5%
85.9%
14.1%
66.7%
33.3% (b) CIFAR10 (MBMDM)
Figure 9: The confusion matrix of BMDM and MBMDM.
5 CONCLUSION
In this paper, we have tackled the challenge of classification under
label shift and introduced two bidirectional marginal distribution
matching methods for both scenarios. Our methods prove to be
particularly effective in scenarios with large label shifts. We have
provided theoretical assurance that the objective function possesses
a unique optimal solution, namely the true target label distribution.
This insight offers valuable guidance for the development of label
shift methods. The experimental results, both in the one-shot and
multi-shot settings, have demonstrated the superiority of our pro-
posed methods. Moving forward, we plan to extend our approach to
the generalized label shift setting, which imposes stringent require-
ments on aligning conditional distributions. Furthermore, exploring
the integration of classifier learning and weight estimation is among
the directions for future research.
ACKNOWLEDGMENTS
This work was partially supported by the National Key Research
and Development Program (No. 2022ZD0114803), the Key NSF of
China under Grant No. 62136005, the NSF of China under Grant No.
61922087 and 62006238, the NSF of Hunan Province under Grant
2023JJ20052, and the Research Innovation Project for Postgraduate
Students under Grant No. CX20230012.
 
743KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruidong Fan, Xiao Ouyang, Hong Tao, and Chenping Hou
REFERENCES
[1]Raffi Al-Qurran, Mahmoud Al-Ayyoub, and Ali M. Shatnawi. 2022. Plant classifi-
cation in the wild: Energy evaluation for deep learning models. Multimedia Tools
and Applications 81, 21 (2022), 30143â€“30167.
[2]Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. 2020. Maximum like-
lihood with bias-corrected calibration is hard-to-beat at label shift adaptation.
InProceedings of the 37th International Conference on Machine Learning(ICML).
222â€“232.
[3]Kamyar Azizzadenesheli. 2022. Importance Weight Estimation and Generalization
in Domain Adaptation Under Label Shift. IEEE Transactions on Pattern Analysis
and Machine Intelligence 44, 10 (2022), 6578â€“6584.
[4]Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar.
2019. Regularized Learning for Domain Adaptation under Label Shifts. In 7th
International Conference on Learning Representations(ICLR).
[5]Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J. Gordon.
2020. Domain Adaptation with Conditional Distribution Matching and General-
ized Label Shift. In Advances in Neural Information Processing Systems 33(NeurIPS).
[6]Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C. Lipton. 2020.
A Unified View of Label Shift Estimation. In Advances in Neural Information
Processing Systems 33(NeurIPS).
[7]Hao Guan and Mingxia Liu. 2022. Domain Adaptation for Medical Image Analysis:
A Survey. IEEE Transactions on Biomedical Engineering 69, 3 (2022), 1173â€“1185.
[8]Chenping Hou, Ruidong Fan, Ling-Li Zeng, and Dewen Hu. 2023. Adaptive
Feature Selection With Augmented Attributes. IEEE Transactions on Pattern
Analysis and Machine Intelligence 45, 8 (2023), 9306â€“9324.
[9]Chenping Hou and Zhi-Hua Zhou. 2017. One-pass learning with incremental
and decremental features. IEEE Transactions on Pattern Analysis and Machine
Intelligence 40, 11 (2017), 2776â€“2792.
[10] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. 2018. Detecting and
correcting for label shift with black box predictors. In Proceedings of the 35th
International Conference on Machine Learning (ICML). 3122â€“3130.
[11] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou.
2022. Test-Time Adaptation with Calibration of Medical Image Classification
Nets for Label Distribution Shift. In Medical Image Computing and Computer
Assisted Intervention - MICCAI 2022 - 25th International Conference, Vol. 13433.
313â€“323.
[12] Kevin P Murphy. 2012. Machine learning: a probabilistic perspective. MIT press.
[13] Aleksandr Podkopaev and Aaditya Ramdas. 2021. Distribution-free uncertainty
quantification for classification under label shift. In Proceedings of the Thirty-
Seventh Conference on Uncertainty in Artificial Intelligence(UAI), Vol. 161. 844â€“853.
[14] Meghavi Rana and Megha Bhushan. 2023. Machine learning and deep learning
approach for medical image analysis: diagnosis to detection. Multimedia Tools
and Applications 82, 17 (2023), 26731â€“26769.
[15] Marco Saerens, Patrice Latinne, and Christine Decaestecker. 2002. Adjusting the
outputs of a classifier to new a priori probabilities: a simple procedure. Neural
computation 14, 1 (2002), 21â€“41.
[16] Viney Sharma, Amit Kumar Mishra, and Shweta Paliwal. 2023. Machine Learning
Framework for Recognition and Classification of Plant Species: A Study using
Digital Database. In 24th International Conference on Distributed Computing and
Networking (ICDCN). 407â€“413.
[17] TomÃ¡s Sipka, Milan Sulc, and JirÃ­ Matas. 2022. The Hitchhikerâ€™s Guide to Prior-
Shift Adaptation. In IEEE/CVF Winter Conference on Applications of Computer
Vision(WACV). 2031â€“2039.
[18] Milan Sulc and Jiri Matas. 2019. Improving cnn classifiers by estimating test-time
priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision
Workshops. 0â€“0.
[19] Dirk Tasche. 2017. Fisher Consistency for Prior Probability Shift. Journal of
Machine Learning Research 18, 95 (2017), 1â€“32.
[20] Afonso Fernandes Vaz, Rafael Izbicki, and Rafael Bassi Stern. 2019. Quantification
Under Prior Probability Shift: the Ratio Estimator and its Extensions. Journal of
Machine Learning Research 20, 79 (2019), 1â€“33.
[21] Weiran Wang and Miguel A Carreira-PerpinÃ¡n. 2013. Projection onto the prob-
ability simplex: An efficient algorithm with a simple proof, and an application.
arXiv preprint arXiv:1309.1541 (2013).
[22] Qingyao Wu, Hanrui Wu, Xiaoming Zhou, Mingkui Tan, Yonghui Xu, Yuguang
Yan, and Tianyong Hao. 2017. Online Transfer Learning with Multiple Homo-
geneous or Heterogeneous Sources. IEEE Transactions on Knowledge and Data
Engineering 29, 7 (2017), 1494â€“1507.
[23] Ruihan Wu, Chuan Guo, Yi Su, and Kilian Q Weinberger. 2021. Online adaptation
to label distribution shift. Advances in Neural Information Processing Systems
34(NeurIPS) 34 (2021), 11340â€“11351.
[24] Dianlong You, Jiawei Xiao, Yang Wang, Huigui Yan, Di Wu, Zhen Chen, Limin
Shen, and Xindong Wu. 2023. Online Learning From Incomplete and Imbalanced
Data Streams. IEEE Transactions on Knowledge and Data Engineering 35, 10 (2023),
10650â€“10665.
[25] Kun Zhang, Bernhard SchÃ¶lkopf, Krikamol Muandet, and Zhikun Wang. 2013.
Domain adaptation under target and conditional shift. In Proceedings of the 30thInternational Conference on Machine Learning(ICML). 819â€“827.
[26] Min-Ling Zhang, Yu-Kun Li, Hao Yang, and Xu-Ying Liu. 2022. Towards Class-
Imbalance Aware Multi-Label Learning. IEEE Transactions on Cybernetics 52, 6
(2022), 4459â€“4471.
[27] Zhi-Hua Zhou. 2022. Open-environment machine learning. National Science
Review 9, 8 (2022), nwac123.
[28] Lei Zhu, Shaoning Pang, Abdolhossein Sarrafzadeh, Tao Ban, and Daisuke Inoue.
2016. Incremental and Decremental Max-Flow for Online Semi-Supervised
Learning. IEEE Transactions on Knowledge and Data Engineering 28, 8 (2016),
2115â€“2127.
A OPTIMIZATION
A.1 BMDM Optimization
For BMDM, since we have utilized both negative log loss and con-
strained regularizers, it is difficult to derive a closed-form solution.
A natural iterative approach to this problem is Projected Gradient
Descent(PGD):
ğœƒğ‘¡+1=Î Î” ğœƒğ‘¡âˆ’ğœ‚ğ‘¡âˆ‡ğ¿ ğœƒğ‘¡, (28)
whereğ‘¡is the number of iterations, ğœ‚ğ‘¡is step size determined
by inexact linear search, âˆ‡ğ¿ ğœƒğ‘¡is the gradient of function ğ¿at
pointğœƒğ‘¡andÎ Î”(.)is a projection onto the standard simplex Î”=
ğœƒâˆˆRğ¾:Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1andğœƒâ‰¥0	
. To be specific, we have
âˆ‡ğ¿ ğœƒğ‘¡=2(1âˆ’ğœ†)DTKğ‘ Dğœƒğ‘¡
ğ‘›2âˆ’DTKğ‘ ,ğ‘¡1
ğ‘šğ‘›
âˆ’ğœ†
ğ‘šÂ©Â­
Â«1âˆ’ğ‘›+ğ‘šâˆ‘ï¸
ğ‘—=ğ‘›+1"
ğ‘“ğ‘ (ğ‘‹=ğ‘¥ğ‘—)/ğ‘ğ‘ (ğ‘Œ)
[ğ‘“ğ‘ (ğ‘‹=ğ‘¥ğ‘—)/ğ‘ğ‘ (ğ‘Œ)]Tğœƒğ‘¡#
ÂªÂ®
Â¬.(29)
Assume that ğ‘§ğ‘¡+1=ğœƒğ‘¡âˆ’ğœ‚ğ‘¡âˆ‡ğ¿ ğœƒğ‘¡, then Eq .(29) can be replaced
by the following optimization problem:
(
ğœƒğ‘¡+1=arg minğœƒğœƒâˆ’ğ‘§ğ‘¡+12
ğ‘ .ğ‘¡.Ãğ¾
ğ‘˜=1ğœƒğ‘˜=1,ğœƒâ‰¥0.(30)
Due to the strong convexity of the objective function and the linear-
ity of the constraints, there is a unique solution. We adopt Lagrange
multiplier method to solve the above optimization problem with
constraints [21] and the Lagrangian function is
L(ğœƒ,ğ›¼,ğ›½)=1
2ğœƒâˆ’ğ‘§ğ‘¡+12âˆ’ğ›¼
ğœƒT1âˆ’1
âˆ’ğ›½Tğœƒ, (31)
whereğœƒandğ›½=[ğ›½1;...;ğ›½ğ¾]are the Lagrange multipliers for the
equality and inequality constraints, respectively. The KKT condi-
tions which need to be satisfied by optimal solutions are shown as
follows:
ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³ğœƒğ‘˜âˆ’ğ‘§ğ‘¡+1
ğ‘˜âˆ’ğ›¼âˆ’ğ›½ğ‘˜=0,âˆ€ğ‘˜âˆˆ[ğ¾]
ğœƒğ‘˜â‰¥0, ğ›½ğ‘˜â‰¥0, ğ›½ğ‘˜ğœƒğ‘˜=0,âˆ€ğ‘˜âˆˆ[ğ¾]Ãğ¾
ğ‘–=1ğœƒğ‘–=1.(32)
From the second complementarity condition of Eq. (32), it is obvious
that ifğœƒğ‘˜=0, thenğ›½ğ‘–â‰¥0, which is equal to ğ‘§ğ‘¡+1
ğ‘˜+ğ›¼=âˆ’ğ›½ğ‘–â‰¤0;
ifğœƒğ‘˜>0, thenğ›½ğ‘–=0andğœƒğ‘˜=ğ‘§ğ‘¡+1
ğ‘˜+ğ›¼>0. Thus, the zero com-
ponents in the optimal solution correspond to the smaller partial
components of ğ‘§ğ‘¡+1. For clarity, we sort the components of ğ‘§ğ‘¡+1
andğœƒin the same order, i.e.,
ğ‘§ğ‘¡+1
1â‰¥...â‰¥ğ‘§ğ‘¡+1
ğœŒ>ğ‘§ğ‘¡+1
ğœŒ+1â‰¥...â‰¥ğ‘§ğ‘¡+1
ğ¾,
ğœƒ1â‰¥...â‰¥ğœƒğœŒ>ğœƒğœŒ+1=...=ğœƒğ¾=0,(33)
 
744Label Shift Correction via Bidirectional Marginal Distribution Matching KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
whereğœŒis the number of positive components. Now we apply the
last condition of Eq. (32) and have
1=ğ¾âˆ‘ï¸
ğ‘–=1ğœƒğ‘–=ğœŒâˆ‘ï¸
ğ‘–=1ğœƒğ‘–=ğœŒâˆ‘ï¸
ğ‘–=1
ğ‘§ğ‘¡+1
ğ‘–+ğ›¼
. (34)
In other word, we obtain the optimal value of the Lagrange multi-
plierğ›¼, i.e.,
ğ›¼=1
ğœŒ 
1âˆ’ğœŒâˆ‘ï¸
ğ‘–=1ğ‘§ğ‘¡+1
ğ‘–!
. (35)
HenceğœŒis the key of this question. Once we obtain ğœŒ, it is natural to
get the Lagrange multiplier ğ›¼, and then obtain the optimal solution
ğœƒğ‘˜=max{0,ğ‘§ğ‘¡+1
ğ‘˜+ğ›¼},âˆ€ğ‘˜âˆˆ[ğ¾]. Sinceğ‘§ğ‘¡+1is ordered, we find that
the parameter ğœŒneeds to satisfy the following properties:
âˆ€ğ‘–â‰¤ğœŒ,ğœƒğ‘–=ğ‘§ğ‘¡+1
ğ‘–+ğ›¼=ğ‘§ğ‘¡+1
ğ‘–+1
ğœŒÂ©Â­
Â«1âˆ’ğœŒâˆ‘ï¸
ğ‘—=1ğ‘§ğ‘¡+1
ğ‘—ÂªÂ®
Â¬>0;
âˆ€ğ‘–>ğœŒ,ğœƒğ‘–=0andğ‘§ğ‘¡+1
ğ‘–+ğ›¼=ğ‘§ğ‘¡+1
ğ‘–+1
ğœŒÂ©Â­
Â«1âˆ’ğœŒâˆ‘ï¸
ğ‘—=1ğ‘§ğ‘¡+1
ğ‘—ÂªÂ®
Â¬â‰¤0.(36)
It is obvious that the optimal ğœŒis as follows:
ğœŒ=maxï£±ï£´ï£´ ï£²
ï£´ï£´ï£³ğ‘–âˆˆ[ğ¾]:ğ‘§ğ‘¡+1
ğ‘–+1
ğ‘–Â©Â­
Â«1âˆ’ğ‘–âˆ‘ï¸
ğ‘—=1ğ‘§ğ‘¡+1
ğ‘—ÂªÂ®
Â¬>0ï£¼ï£´ï£´ ï£½
ï£´ï£´ï£¾. (37)
In summary, the procedure in optimizing Eq. (17) is summarized in
Algorithm 1.
A.2 MBMDM Optimization
For MBMDM, Eq. (26) can also be sovled by Algorithm 1, if we
replace the gradient âˆ‡ğ¿ ğœƒğ‘¡with
âˆ‡ğ¿ğ‘€ ğœƒğ‘¡=2(1âˆ’ğœ†)DTKğ‘ Dğœƒğ‘¡
ğ‘›2âˆ’DTKğ‘ ,ğ‘Ÿ1
ğ‘›ğ‘›ğ‘Ÿ
âˆ’ğœ†
ğ‘›ğ‘Ÿğ‘›ğ‘Ÿâˆ‘ï¸
ğ‘—=1 
1âˆ’"
ğ‘“ğ‘ (ğ‘‹=ğ‘¥ğ‘—)/ğ‘ğ‘ (ğ‘Œ)
[ğ‘“ğ‘ (ğ‘‹=ğ‘¥ğ‘—)/ğ‘ğ‘ (ğ‘Œ)]Tğœƒğ‘¡#!
+ğœ”
âŸ¨a,bâŸ©aâˆ’âˆ¥aâˆ¥2b
âˆ¥aâˆ¥3âˆ¥bâˆ¥,(38)
where a=ğœƒğ‘¡âˆ’ğ‘ğ‘ (ğ‘Œ)andb=Ë†ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ).
B PROOF OF THEOREM
B.1 Proof of Theorem 1
Proof. Letâ€™s first skip constraint {ğœƒğ‘˜â‰¥0,âˆ€ğ‘˜âˆˆ[ğ¾]}during the
proof since if we can prove ğœƒâˆ—=ğ‘ğ‘¡(ğ‘Œ)is the unique solution of Eq.
(8), it must be true.
(1) Sufficiency. Since Eq. (8) belongs to constraint optimization,
we change it to an unconstrained form by the Lagrange multiplier
method:
ğ¿(ğœƒ,ğ›¾):=
Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
âˆ’log ğ¾âˆ‘ï¸
ğ‘˜=1ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)!#
+ğ›¾ ğ¾âˆ‘ï¸
ğ‘˜=1ğœƒğ‘˜âˆ’1!
,(39)Algorithm 1 Algorithm in Solving BMDM problem in Eq. (17).
Input: D,Kğ‘ ,Kğ‘¡,ğ‘ ,Ë†ğ‘”â—¦Ë†ğ‘“(ğ‘¥),ğ‘¥âˆˆ{ğ‘‹ğ‘ ,ğ‘‹ğ‘¡},ğœ†.
Initialization: ğœ‚=0.1,ğœƒ1=1,ğ›¿=0.9.
Repeat not converged do
1: Compute the gradient âˆ‡ğ¿(ğœƒğ‘¡)by Eq. (31).
2:Whileğ¿(ğœƒğ‘¡âˆ’ğœ‚ğ‘¡âˆ‡ğ¿ ğœƒğ‘¡)â‰¥ğ¿(ğœƒğ‘¡),do
3:ğœ‚ğ‘¡=ğ›¿ğœ‚ğ‘¡
4: Define the iteration point ğ‘§ğ‘¡+1=ğœƒğ‘¡âˆ’ğœ‚ğ‘¡âˆ‡ğ¿ ğœƒğ‘¡.
5: Sortğ‘§ğ‘¡+1intoğ‘£:ğ‘£1â‰¥ğ‘£2â‰¥...â‰¥ğ‘£ğ¾.
6: FindğœŒ=maxn
ğ‘–âˆˆ[ğ¾]:ğ‘£ğ‘–+1
ğ‘–
1âˆ’Ãğ‘–
ğ‘—=1ğ‘£ğ‘—
>0o
.
7: Compute ğ›¼=1
ğœŒ 1âˆ’ÃğœŒ
ğ‘–=1ğ‘£ğ‘–.
8: Obtain ğœƒğ‘Ÿ
ğ‘˜=max
0,ğ‘§ğ‘¡+1
ğ‘˜+ğ›¼	
,âˆ€ğ‘˜âˆˆ[ğ¾].
9:End while
10:ğœƒğ‘¡+1=ğœƒğ‘Ÿ.
10:ğ‘¡=ğ‘¡+1.
Until meeting the stopping criterion
Output: The optimal ğœƒ.
End procedure
whereğ›¾is a Lagrange multiplier and the Karush-Kuhn-Tucke (KKT)
conditions for the Lagrange function is given as follows:
ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)
=ğ›¾,âˆ€ğ‘˜âˆˆ[ğ¾],
Ãğ¾
ğ‘˜=1ğœƒğ‘˜âˆ’1=0.(40)
From above equations, we can determine ğ›¾as
ğ›¾=ğ›¾ğ¾âˆ‘ï¸
ğ‘˜=1ğœƒğ‘˜
=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)#ğ¾âˆ‘ï¸
ğ‘˜=1ğœƒğ‘˜
=1.(41)
Thus,âˆ€ğ‘˜âˆˆ[ğ¾],Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)
=1always
holds. Furthermore,
[ğ‘‡(ğœƒ;ğ‘‹)]ğ‘˜=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
ğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)#
=ğœƒğ‘˜, ğ‘˜âˆˆ[ğ¾],(42)
Therefore, the optimal solution of Eq. (8) must satisfy the fixed
point equation Eq. (7). that is, the sufficiency of solution is proved.
(2) Uniqueness. In the first, we prove that ğœƒ=ğ‘ğ‘¡(ğ‘Œ)is one
of the optimal solutions of Eq. (8). Specifically, we prove that it
satisfies the KKT conditions Eq. (40). For the first condition, we
 
745KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruidong Fan, Xiao Ouyang, Hong Tao, and Chenping Hou
have
Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
Ãğ¾
ğ‘–=1ğœƒğ‘–ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)#
=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
Ãğ¾
ğ‘–=1ğ‘ğ‘¡(ğ‘Œ=ğ‘–)ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)#
=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)
ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘˜)Ãğ¾
ğ‘–=1ğ‘ğ‘¡(ğ‘Œ=ğ‘–)ğ‘ğ‘ (ğ‘Œ=ğ‘–|ğ‘‹)/ğ‘ğ‘ (ğ‘Œ=ğ‘–)
ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)
1â—‹=1
ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)
=1,(43)
where 1â—‹holds due to the optimal distribution matching Eq. (6). In
addition, for the second condition, we have
ğ¾âˆ‘ï¸
ğ‘˜=1ğœƒğ‘˜âˆ’1=ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)âˆ’1=0. (44)
Thus,ğœƒ=ğ‘ğ‘¡(ğ‘Œ)satisfies the KKT conditions. Next, we prove that
ğœƒâˆ—=ğ‘ğ‘¡(ğ‘Œ)is the unique solution of Eq. (8). If there exists another
optimal soulution Ëœğœƒâ‰ ğœƒâˆ—, the objective function values of both are
equal, i.e.,
Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
âˆ’log ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)!#
=Eğ‘‹âˆ¼ğ‘ğ‘¡(ğ‘‹)"
âˆ’log ğ¾âˆ‘ï¸
ğ‘˜=1Ëœğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)!#
.(45)
Thus, we have
ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)âˆ’ğ¾âˆ‘ï¸
ğ‘˜=1Ëœğœƒğ‘˜ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)
=ğ¾âˆ‘ï¸
ğ‘˜=1
ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)âˆ’Ëœğœƒğ‘˜
ğ‘ğ‘ (ğ‘Œ=ğ‘˜)ğ‘ğ‘ (ğ‘Œ=ğ‘˜|ğ‘‹)
=0.(46)
Since the set of distributions {ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘˜),ğ‘˜âˆˆ[ğ¾]}are linearly
independent, we have
ğ‘ğ‘¡(ğ‘Œ=ğ‘˜)âˆ’Ëœğœƒğ‘˜=0,âˆ€ğ‘˜âˆˆ[ğ¾]. (47)
This contradicts the assumption that Ëœğœƒâ‰ ğ‘ğ‘¡(ğ‘Œ), thus Eq. (8) has a
unique solution. â–¡
B.2 Proof of Theorem 2
Proof. If there exists ğœƒâ‰ ğœƒâˆ—that satisfies Eq. (10), then we
haveÃğ¾
ğ‘–=1(ğœƒğ‘–âˆ’ğœƒâˆ—
ğ‘–)ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘–)=0. Since the set of distribu-
tions{ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘˜),ğ‘˜âˆˆ[ğ¾]}are linearly independent,Ãğ¾
ğ‘–=1(ğœƒğ‘–âˆ’
ğœƒâˆ—
ğ‘–)ğ‘ğ‘ (ğ‘‹|ğ‘Œ=ğ‘–)=0is equal toğœƒâˆ’ğœƒâˆ—=0. This contradicts the previ-
ous conclusion. Thus Eq. (10) has a unique solution ğœƒâˆ—=ğ‘ğ‘¡(ğ‘Œ).â–¡
B.3 Proof of Theorem 3
Proof. Assume that the data flow is in phase ğ‘Ÿ, based on the
label shift assumption ğ‘ğ‘ (ğ‘‹|ğ‘Œ)=ğ‘ğ‘¡1(ğ‘‹|ğ‘Œ)=...=ğ‘ğ‘¡ğ‘…(ğ‘‹|ğ‘Œ), wehave the following conclusion
ğ‘ğ‘Ÿâˆ’1(ğ‘Œ|ğ‘‹)=ğ‘ğ‘Ÿâˆ’1(ğ‘Œ,ğ‘‹)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹)=ğ‘ğ‘Ÿâˆ’1(ğ‘‹|ğ‘Œ)ğ‘ğ‘Ÿâˆ’1(ğ‘Œ)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹)
=ğ‘ğ‘Ÿâˆ’2(ğ‘‹|ğ‘Œ)ğ‘ğ‘Ÿâˆ’1(ğ‘Œ)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹)=ğ‘ğ‘Ÿâˆ’2(ğ‘‹,ğ‘Œ)ğ‘ğ‘Ÿâˆ’1(ğ‘Œ)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹)ğ‘ğ‘Ÿâˆ’2(ğ‘Œ)
=ğ‘ğ‘Ÿâˆ’2(ğ‘‹)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹)ğ‘ğ‘Ÿâˆ’1(ğ‘Œ)
ğ‘ğ‘Ÿâˆ’2(ğ‘Œ)ğ‘ğ‘Ÿâˆ’2(ğ‘Œ|ğ‘‹)
=...
=ğ‘ğ‘Ÿâˆ’2(ğ‘‹)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹)...ğ‘ğ‘ (ğ‘‹)
ğ‘2(ğ‘‹)ğ‘ğ‘Ÿâˆ’1(ğ‘Œ)
ğ‘ğ‘Ÿâˆ’2(ğ‘Œ)ğ‘2(ğ‘Œ)
ğ‘ğ‘ (ğ‘Œ)ğ‘ğ‘ (ğ‘Œ|ğ‘‹)
=ğ‘ğ‘ (ğ‘‹)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹)ğ‘ğ‘Ÿâˆ’1(ğ‘Œ)
ğ‘ğ‘ (ğ‘Œ)ğ‘ğ‘ (ğ‘Œ|ğ‘‹).(48)
If we setğ‘ğ‘Ÿâˆ’1(ğ‘‹)=ğ‘ğ‘ (ğ‘‹)
ğ‘ğ‘Ÿâˆ’1(ğ‘‹), this theorem is proved. â–¡
B.4 Proof of Theorem 4
Proof. First of all, through Assumption 1, we have
ğ‘ğ‘¡ğ‘Ÿ(ğ‘Œ)âˆ’ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)
=ğ›¾ğ‘Ÿâˆ’1
1âˆ’ğ›¾ğ‘Ÿâˆ’1 ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘¡ğ‘Ÿâˆ’2(ğ‘Œ)
=ğ›¾ğ‘Ÿâˆ’1ğ›¾ğ‘Ÿâˆ’2
(1âˆ’ğ›¾ğ‘Ÿâˆ’1)(1âˆ’ğ›¾ğ‘Ÿâˆ’2) ğ‘ğ‘¡ğ‘Ÿâˆ’2(ğ‘Œ)âˆ’ğ‘ğ‘¡ğ‘Ÿâˆ’3(ğ‘Œ)
=...
=Ãğ‘Ÿâˆ’1
ğ‘–=1ğ›¾ğ‘–Ãğ‘Ÿâˆ’1
ğ‘–=1(1âˆ’ğ›¾ğ‘–) ğ‘ğ‘¡2(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ).(49)
On this basis, we further have
ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ)
= ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘¡ğ‘Ÿâˆ’2(ğ‘Œ)+ ğ‘ğ‘¡ğ‘Ÿâˆ’2(ğ‘Œ)âˆ’ğ‘ğ‘¡ğ‘Ÿâˆ’3(ğ‘Œ)
+...+ ğ‘ğ‘¡2(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ)
=Â©Â­Â­Â­Â­
Â«1+ğ‘Ÿâˆ‘ï¸
ğ‘—=2ğ‘—âˆ’1Ã
ğ‘–=1ğ›¾ğ‘–
ğ‘—âˆ’1Ã
ğ‘–=1(1âˆ’ğ›¾ğ‘–)ÂªÂ®Â®Â®Â®
Â¬ ğ‘ğ‘¡2(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ).(50)
By combining Eq. (49) and Eq. (50), âˆƒğ›¾âˆˆR, whose specific value is
ğ›¾=1+Ãğ‘Ÿâˆ’1
ğ‘–=1ğ›¾ğ‘–
Ãğ‘Ÿâˆ’1
ğ‘–=1(1âˆ’ğ›¾ğ‘–)Â©Â­Â­
Â«1+ğ‘ŸÃ
ğ‘—=2ğ‘—âˆ’1Ã
ğ‘–=1ğ›¾ğ‘–
ğ‘—âˆ’1Ã
ğ‘–=1(1âˆ’ğ›¾ğ‘–)ÂªÂ®Â®
Â¬,
(51)
the following equation holds
ğ‘ğ‘¡ğ‘Ÿ(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ)=ğ›¾ ğ‘ğ‘¡ğ‘Ÿâˆ’1(ğ‘Œ)âˆ’ğ‘ğ‘ (ğ‘Œ). (52)
Thus the theorem is proved. â–¡
 
746