Provable Adaptivity of Adam under Non-uniform Smoothness
Bohan Wangâˆ—
bhwangfy@gmail.com
University of Science and Technology
of China & Microsoft Research Asia
Beijing, ChinaYushun Zhangâˆ—
yushunzhang@link.cuhk.edu.cn
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, Guangdoug, ChinaHuishuai Zhangâ€ 
zhanghuishuai@pku.edu.cn
Peking University
Beijing, China
Qi Meng
meq@amss.ac.cn
Chinese Academy of Mathematics
and Systems Science
Beijing, ChinaRuoyu Sun
sunruoyu@cuhk.edu.cn
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, Guangdong, ChinaZhi-Ming Ma
mazm@amt.ac.cn
Chinese Academy of Mathematics
and Systems Science
Beijing, China
Tie-Yan Liu
tie-yan.liu@microsoft.com
Microsoft
Beijing, ChinaZhi-Quan Luo
luozq@cuhk.edu.cn
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, Guangdong, ChinaWei Chenâ€ 
chenwei2022@ict.ac.cn
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
Abstract
Adam is widely adopted in practical applications due to its fast con-
vergence. However, its theoretical analysis is still far from satisfac-
tory. Existing convergence analyses for Adam rely on the bounded
smoothness assumption, referred to as the L-smooth condition. Un-
fortunately, this assumption does not hold for many deep learning
tasks. Moreover, we believe that this assumption obscures the true
benefit of Adam, as the algorithm can adapt its update magnitude
according to local smoothness. This important feature of Adam
becomes irrelevant when assuming globally bounded smoothness.
This paper studies the convergence of randomly reshuffled Adam
(RR Adam) with diminishing learning rate, which is the major ver-
sion of Adam adopted in deep learning tasks. We present the first
convergence analysis of RR Adam without the bounded smooth-
ness assumption. We demonstrate that RR Adam can maintain its
convergence properties when smoothness is linearly bounded by
the gradient norm, referred to as the (ğ¿0,ğ¿1)-smooth condition. We
further compare Adam to SGD when both methods use diminishing
learning rate. We refine the existing lower bound of SGD and show
that SGD can be slower than Adam. To our knowledge, this is the
first time that Adam and SGD are rigorously compared in the same
setting and the advantage of Adam is revealed.
âˆ—Both authors contributed equally to this research.
â€ Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671718CCS Concepts
â€¢Mathematics of computing â†’Nonconvex optimization.
Keywords
Adaptive Optimizer, Convergence Analysis, Non-uniform smooth-
ness
ACM Reference Format:
Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-
Ming Ma, Tie-Yan Liu, Zhi-Quan Luo, and Wei Chen. 2024. Provable Adaptiv-
ity of Adam under Non-uniform Smoothness. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 35 pages.
https://doi.org/10.1145/3637528.3671718
1 Introduction
Machine learning tasks are often formulated as solving the follow-
ing finite-sum problem:
min
ğ’˜âˆˆRğ‘‘ğ‘“(ğ’˜)=1
ğ‘›ğ‘›âˆ’1âˆ‘ï¸
ğ‘–=0ğ‘“ğ‘–(ğ’˜), (1)
whereğ‘›denotes the number of samples or mini-batches, and ğ’˜
denotes the trainable parameters. Recently, it is noted that adaptive
gradient methods including Adaptive Moment estimation (Adam)
[22] are widely used to train modern deep neural networks in-
cluding GANs [ 3], BERTs [ 21], GPTs [ 4] and ViTs [ 11]. It is often
observed that Adam converges considerably faster than vanilla Sto-
chastic Gradient Descent (SGD) for the training of Transformers,
as seen in Figure 1(a). Similar phenomena are also reported in [ 45].
Despite its practical success, the theoretical analysis of Adam is
less than satisfactory. Existing analyses rely on bounded smooth-
ness assumption, i.e., the Lipschitz coefficient of gradients (or the
spectrum norm of the Hessian) is globally upper bounded by con-
stantğ¿, referred to as ğ¿-smooth condition. However, recent studies
 
2960
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bohan Wang et al.
show that the ğ¿-smooth condition does nothold in practical deep
learning tasks such as LSTM [43] and Transformers [8].
Moreover, such an assumption hides the benefit of Adam. Intu-
itively, Adam can overcome the issue of unbounded smoothness
using adaptive learning rate. First, Adam uses the reciprocal of the
square root of the exponential moving averages of past squared
gradients as an effective learning rate (see Algorithm 1 for the
update rule). Thus, the effective learning rate would be adapted
to the local gradient norm. Second, there is a strong correlation
between the Lipschitz coefficient and the gradient norm of deep
neural networks [ 7,8,43]. As a result, Adam can adapt the update
magnitude to the local Lipschitz coefficient and is empirically ob-
served to converge fast (Figure 1(a) and [ 43]). Unfortunately, such
benefit is hidden because existing theories of Adam are built upon
ğ¿-smooth condition.
To reveal the theoretical benefit of Adam, we analyze its conver-
gence under a relaxed smoothness condition called (ğ¿0,ğ¿1)-smooth
condition [43]:
âˆ¥âˆ‡2ğ‘“ğ‘–(ğ’˜)âˆ¥â‰¤ğ¿0+ğ¿1âˆ¥âˆ‡ğ‘“ğ‘–(ğ’˜)âˆ¥. (2)
Whenğ¿1=0, Eq. (2)degenerates into classical ğ¿-smooth condition.
The(ğ¿0,ğ¿1)-smooth condition allows the spectral norm of the Hes-
sian (Lipschitz coefficient of gradients) to linearly grow with the
gradient norm of ğ’˜, so it is a relaxed version of ğ¿-smooth condition.
The(ğ¿0,ğ¿1)-smooth condition is empirically observed to hold in
LSTM [42, 43] and Transformers (Figure 1(b) and [8]).
Our Contribution: Under the(ğ¿0,ğ¿1)-smooth condition, we es-
tablish the convergence of randomly-reshuffled Adam. Specifically,
our contributions are summarized as follows.
â€¢We establish the first convergence result of Adam without
â€œğ¿-smoothness". We prove that Adam converges under the
(ğ¿0,ğ¿1)-smooth condition.
â€¢Our convergence result enjoys several good properties. First,there
is no need for the bounded gradient assumption (i.e. âˆ¥âˆ‡ğ‘“(ğ’˜)âˆ¥â‰¤
ğ¶). Eliminating this assumption is essential since the (ğ¿0,ğ¿1)-
smooth condition would otherwise degenerate to the ğ¿-
smooth condition. Second, our result does not rely on other
assumptions such as a bounded adaptor or a large regular-
izer for numerical stability. Lastly, the convergence holds
for every possible trajectory, which is not only technically
demanding but also much stronger than â€œconvergence in
expectationâ€.
â€¢We further compare Adam to SGD when both methods use
diminishing learning rate. We present an improved lower
bound for (S)GD under the (ğ¿0,ğ¿1)-smooth condition. In
this lower bound, there is a factor related to the gradient
norm of the initial point, which does not exist in the upper
bound of Adam. This indicates that (S)GD can converge slow
under the(ğ¿0,ğ¿1)-smooth condition, showing the advantage
of Adam over (S)GD. To our knowledge, this is the first time
that Adam and SGD are rigorously compared in the same
setting where the advantage of Adam can be revealed. We
believe these results shed new light on understanding the
benefit of Adam.Organization of this paper. The rest of this paper is organized
as follows: In Section 2, we review related works on the conver-
gence analysis for Adam, the relaxed smoothness assumption, and
the variants of Adam. In Section 3, we define notations, present the
psedocode of Adam, and provide the assumptions that our result
rests on. In Section 4, we provide our main result on the conver-
gence of RR Adam under non-uniform smoothness together with
explanations regarding the result. In Section 5, we then state the
proof ideas of the main result. In Section 7, we provide discussions
on intuitions of why non-adaptive optimizers can be used for fine-
tuning tasks, comparison of Adam and Clipped SGD, insights for
practioners and limitations of Theorem 4.1.
2 Related works
Convergence analysis for Adam. Adam is firstly proposed in
Kingma and Ba [23] with a convergence proof. However, the proof
is pointed out to have flaws by [ 30] and [ 30] further provide simple
counterexamples with which Adam diverges. This discovery caused
the convergence analysis of Adam to stagnate for a while and
motivated a series of works developing variants of Adam without
divergent issues (see discussion latter in this section). On the other
hand, vanilla Adam works well in practice and divergence is not
empirically observed. This phenomenon motivates researchers to
rethink the counterexamples. The counterexamples states â€œfor every
ğ›½1<âˆšï¸
ğ›½2, there exists a problem that Adam diverges". That is to
say, the divergence statement requires picking (ğ›½1,ğ›½2)before fixing
the problem, while in practice, the algorithmic parameters are often
picked according to the problem. Based on this observation, a recent
work [ 46] proves that Adam can converge with (ğ›½1,ğ›½2)picked after
the problem is given.
We categorize the existing results of Adam into two classes based
on the sampling strategy: with-replacement sampling (a.k.a.,
i.i.d. sampling, abbreviated as â€œWR") and RR Adam. We believe
both sampling strategies are worth studying: WR is more favored
among the theory community due to its simple form, whereas RR
is widely used among practitioners because it is easy to implement.
Further, RR guarantees to pass each data at least once and brings
good performance [1, 2].
The first line of work analyzes WR Adam. For instance, [ 40]
shows that WR RMSProp (a simplified version of Adam with ğ›½1=0)
converges to the neighborhood of the stationary points. [ 9] prove
the convergence of WR RMSProp by assuming the signs of the
gradients to remain the same along the trajectory. However, this
condition is not guaranteed to hold in practice. [ 10] prove the con-
vergence of WR Adam with ğ›½1<ğ›½2. However, their convergence
bound is inversely proportional to ğœ‰, which is the hyperparameter
for numerical stability. Consequently, their bound becomes vacuous
asğœ‰approaches zero. This result does not match practical observa-
tions because small values of ğœ‰, like 10âˆ’8, often yield satisfactory
performance. Moreover, employing large values of ğœ‰obscures the
effect ofâˆšğ‘£ğ‘˜, and thus the proof is largely reduced to the proof of
SGD. [ 18,19] provide simple convergence proof for WR Adam with
ğ›½1close to 1. However, their results require theâˆšğ‘£ğ‘˜to be bounded
in a certain interval [ğ¶ğ‘™,ğ¶ğ‘¢]. This condition changes Adam into
AdaBound [ 27]. In summary, all the above works require certain
strong conditions such as boundedâˆšğ‘£ğ‘˜or largeğœ‰. Further, they all
 
2961Provable Adaptivity of Adam under Non-uniform Smoothness KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) Training loss
 (b) Gradient vs. smoothness
Figure 1: Experiments on the WMT 2014 dataset trained with the transformer. (a): The training loss of SGD and Adam. (b):
The gradient norm vs. the local smoothness on the training trajectory. The blue line in (b) stands for log(local smoothness) =
log(gradient norm)+ 1.4. It can be observed that (ğ‘’1.4,0)-smooth condition holds in this task. Similar results can be seen in
Zhang et al. [43].
require bounded gradient ( âˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ¥â‰¤ğ¶) and bounded smoothness
(ğ¿-smooth) condition.
Our analysis falls into the second line of works, which focus on
RR Adam. [ 33] prove the trajectory-wise convergence of RR RM-
SProp and [ 46] prove the in-expectation convergence of RR Adam.
However, these works both require ğ¿-smooth condition. Our anal-
ysis follows this line of works and provides the first convergence
result of RR Adam under relaxed smoothness condition.
Relaxed smoothness assumption. There are several attempts on
relaxingğ¿-smooth condition. Zhang et al . [43] proposes(ğ¿0,ğ¿1)-
smooth condition to theoretically explain the acceleration effect of
clipped SGD over SGD. Similar results are also extended to clipped
SGD with momentum [ 42], distributionally-robust optimization
[20] , differentially-private SGD [ 39] and generalized SignSGD [ 8].
However, they did not theoretically analyze Adam in this setting.
Considering the great empirical impact of Adam, we believe it is
important to study Adam in its original form.
One concurrent work [ 24] studies the convergence of WR Adam
under(ğ¿0,ğ¿1)-smooth condition by cleverly constructing certain
stopping time. They also propose a variance-reduced variant with
better convergence rate. However, their bound on Adam not only
assumes the noise is deterministically bounded, but also has poly-
nomial dependence over 1/ğœ‰(the hyperparameter for numerical
stability). Similarly to [ 9], this result does not match practice ob-
servations, since Adam performs well even when ğœ‰is as small as
10âˆ’8.
Variants of Adam. Ever since the counter-example of the conver-
gence of Adam raised by [ 30], many new variants of Adam have
been designed. For instance, Chen et al . [5,6], Gadat and Gavra
[14], Zou et al . [49] replaced the constant hyperparameters by
iterate-dependent ones e.g. ğ›½1ğ‘¡orğ›½2ğ‘¡. AMSGrad [ 31] and AdaFom
[6] enforced{ğ‘£ğ‘¡}to be non-decreasing. Similarly, AdaBound [ 27]imposed constraints ğ‘£ğ‘¡âˆˆ[ğ¶ğ‘™,ğ¶ğ‘¢]to prevent the learning rate from
vanishing or exploding. Similarly, [ 48] adopted a new estimate of ğ‘£ğ‘¡
to correct the bias. In addition, there are attempts to combine Adam
with Nesterov momentum [ 12] as well as warm-up techniques [ 25].
There are also some works providing theoretical analysis on the
variants of Adam. For instance, Zhou et al . [47] studied the con-
vergence of AdaGrad and AMSGrad. Gadat and Gavra [14] studied
the asymptotic behavior of a subclass of adaptive gradient methods
from landscape point of view. Their analysis applies to RMSprop-
variants with iterate-dependent ğ›½2ğ‘¡. In summary, all these works
study variants of Adam, which is different from our work since we
focus on vanilla Adam.
3 Preliminaries
This section introduces notations, definitions, and assumptions that
are used throughout this work.
Notations. We list the notations that are used in the formal defi-
nition of the randomly-shuffled Adam and its convergence analysis.
â€¢(Vector) We define ğ’‚âŠ™ğ’ƒas the Hadamard product (i.e.,
component-wise product) between two vectors ğ’‚andğ’ƒwith
the same dimension. We also define âŸ¨ğ’‚,ğ’ƒâŸ©as theâ„“2inner
product between ğ’‚andğ’ƒ. We define 1ğ‘‘as an all-one vector
with dimension ğ‘‘.
â€¢(Array) We define [ğ‘š1,ğ‘š2]â‰œ{ğ‘š1,Â·Â·Â·,ğ‘š2},âˆ€ğ‘š1,ğ‘š2âˆˆ
N,ğ‘š1â‰¤ğ‘š2. Specifically, we use [ğ‘š]â‰œ{1,Â·Â·Â·,ğ‘š}.
â€¢(Asymptotic notation) We define ğ´1(ğ‘¥)=Oğ‘¥â†’ğ‘(ğ´2(ğ‘¥))
ifğ´1(ğ‘¥)
ğ´2(ğ‘¥)is bounded when ğ‘¥â†’ğ‘. We define ğ´2(ğ‘¥)=
Î©ğ‘¥â†’ğ‘(ğ´1(ğ‘¥))whenğ´1(ğ‘¥)=Oğ‘¥â†’ğ‘(ğ´2(ğ‘¥)). We use ËœOto
denoteOwith logarithmic factors hidden, i.e., ğ´1(ğ‘¥)=
ËœOğ‘¥â†’ğ‘(ğ´2(ğ‘¥))ifğ´1(ğ‘¥)=Oğ‘¥â†’ğ‘(ğ´2(ğ‘¥)log|ğ´2(ğ‘¥)|). When
the context is clear, we hide " ğ‘¥â†’ğ‘" and only useO,Î©,ËœO.
 
2962KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bohan Wang et al.
Pseudocode. To facilitate the analysis, we provide the pseudocode
of Adam in Algorithm 1.
Algorithm 1 Randomly reshuffled Adam (RR-Adam)
Input: Objective function ğ‘“(ğ’˜):=1
ğ‘›Ãğ‘›âˆ’1
ğ‘–=0ğ‘“ğ‘–(ğ’˜), learning rate
series{ğœ‚ğ‘˜}ğ‘‡
ğ‘˜=1and hyperparameters (ğ›½1,ğ›½2)âˆˆ[ 0,1)2. Initialize
the parameter ğ’˜1,0âˆˆRğ‘‘, the conditioner ğ‚1,âˆ’1âˆˆRğ‘‘,â‰¥0, and the
momentum ğ’1,âˆ’1âˆˆRğ‘‘.
forğ‘˜=1toğ‘‡do
Randomly shuffle[0,ğ‘›âˆ’1]to get{ğœğ‘˜,ğ‘—}ğ‘›âˆ’1
ğ‘—=0
forğ‘–=0toğ‘›âˆ’1do
Calculateğ‘”ğ‘˜,ğ‘–=âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğœğ‘˜,ğ‘–)
Update ğ‚ğ‘˜,ğ‘–=ğ›½2ğ‚ğ‘˜,ğ‘–âˆ’1+(1âˆ’ğ›½2)ğ‘”âŠ™2
ğ‘˜,ğ‘–,
Update ğ’ğ‘˜,ğ‘–=ğ›½1ğ’ğ‘˜,ğ‘–âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘˜,ğ‘–
Update ğ’˜ğ‘˜,ğ‘–+1=ğ’˜ğ‘˜,ğ‘–âˆ’ğœ‚ğ‘˜1âˆšğ‚ğ‘˜,ğ‘–+ğœ‰âŠ™ğ’ğ‘˜,ğ‘–
end for
Update ğ‚ğ‘˜+1,âˆ’1=ğ‚ğ‘˜,ğ‘›âˆ’1,ğ’ğ‘˜+1,âˆ’1=ğ’ğ‘˜,ğ‘›âˆ’1,ğ’˜ğ‘˜+1,0=ğ’˜ğ‘˜,ğ‘›
end for
ğ’ğ‘˜,ğ‘–andğ‚ğ‘˜,ğ‘–are weighted averages with hyperparamter ğ›½1âˆˆ
[0,1)andğ›½2âˆˆ[0,1), respectively. ğœ‰is adopted for numerical sta-
bility and it is often chosen to be 10âˆ’8in practice. In our theory,
we allowğœ‰to be an arbitrary non-negative constant including 0.
Algorithm 1 follows a without-replacement sampling strategy
(also known as shuffling), which is the default strategy used in CV,
NLP, GANs, etc. However, it is not necessarily easy to analyze shuf-
fling strategy, because the stochastic gradients sampled by random-
shuffling lack statistical unbiasedness, i.e. E
âˆ‡ğ‘“ğ‘˜,ğ‘–(ğ‘¥ğ‘˜,ğ‘–)|ğ‘¥ğ‘˜,ğ‘–
â‰ 
âˆ‡ğ‘“(ğ‘¥ğ‘˜,ğ‘–). This bias requires a much different analysis from its with-
replacement counterpart. Even for SGD, the analysis for shuffling is
often known to be â€œmore challenging" [ 28,36]. However, we choose
to study this version as it is closer to the practice.
Assumptions. Here we state the assumptions that our result will
rest on. The first one is the (ğ¿0,ğ¿1)-smooth condition introduced
in Section 1.
Assumption 3.1 ((ğ¿0,ğ¿1)-smooth condition). We assume that
ğ‘“ğ‘–(ğ’˜)is lower bounded by 0, andğ‘“ğ‘–(ğ’˜)satisfies(ğ¿0,ğ¿1)-smooth
condition, i.e., there exist positive constants ( ğ¿0,ğ¿1), such that,
âˆ€ğ’˜1,ğ’˜2âˆˆRğ‘‘satisfyingâˆ¥ğ’˜1âˆ’ğ’˜2âˆ¥â‰¤1
ğ¿1,
âˆ¥âˆ‡ğ‘“ğ‘–(ğ’˜1)âˆ’âˆ‡ğ‘“ğ‘–(ğ’˜2)âˆ¥â‰¤(ğ¿0+ğ¿1âˆ¥âˆ‡ğ‘“ğ‘–(ğ’˜1)âˆ¥)âˆ¥ğ’˜1âˆ’ğ’˜2âˆ¥.(3)
Eq. (3) is firstly introduced by Zhang et al . [42] , and is the weakest
version of(ğ¿0,ğ¿1)-smooth condition to our best knowledge since
it does not require ğ‘“ğ‘–(ğ’˜)to be twice differentiable. When ğ‘“ğ‘–(ğ’˜)is
twice differentiable, Eq. (3) is equivalent to Eq. (2)[42].(ğ¿0,ğ¿1)-
smooth condition generalizes the ğ¿-smooth condition (i.e., (ğ¿0,ğ¿1)-
smooth condition with ğ¿0=ğ¿andğ¿1=0) in classical non-convex
optimization literature [ 16,26] and allows the smoothness to be
unbounded globally.
Assumption 3.2 (Affine Noise Variance). âˆ€ğ’˜âˆˆRğ‘‘, the gradients
of{ğ‘“ğ‘–(ğ’˜)}ğ‘›âˆ’1
ğ‘–=0has the following connection with the gradient of
ğ‘“(ğ’˜):
1
ğ‘›ğ‘›âˆ’1âˆ‘ï¸
ğ‘–=0âˆ¥âˆ‡ğ‘“ğ‘–(ğ’˜)âˆ¥2â‰¤ğ·1âˆ¥âˆ‡ğ‘“(ğ’˜)âˆ¥2+ğ·0.Assumption 3.2 is one of the weakest assumption on gradient
noise in existing literature. It not only generalizes the â€œbounded
variance" assumption (which requires ğ·1=1/ğ‘›, and thus further
generalizes the "bounded gradient" assumption [ 10]) [17,19,41],
but also is weaker than the â€œstrongly growth condition" (which
requiresğ·0=0) [32,38]. Assumption 3.2 allows flexible choices of
ğ·0&ğ·1and thus it is among the weakest assumption of this kind.
4 Adam Converges under the (ğ¿0,ğ¿1)-smooth
condition
In this section, we provide our main result on the convergence of
RR Adam under(ğ¿0,ğ¿1)-smooth condition. As discussed in Section
1, even for the simpler with-replacement sampling Adam, the es-
tablishment of the convergence under (ğ¿0,ğ¿1)-smooth condition
requires restrictive assumptions such as a large ğœ‰(the constant in-
troduced for numerical stability and is as small as 10âˆ’8in practice),
and deterministically bounded noise [ 24]. Such assumptions make
the corresponding results hard to apply to practical setting. As for
the harder randomly-reshuffled setting, there is no convergence
result for Adam under non-uniform smoothness. Our result tackles
the limitation in existing works and propose the first convergence
result for RR Adam under non-uniform smoothness, provided as
follows.
Theorem 4.1. Consider RR Adam defined as Algorithm 1 with di-
minishing learning rate ğœ‚ğ‘˜=ğœ‚1âˆš
ğ‘˜. Let Assumptions 3.1 and 3.2 hold.
Suppose the hyperparamters satisfy: 0â‰¤ğ›½2
1<ğ›½2<1andğ›½2is larger
than a threshold ğ›¾(ğ·1). Then, we have
min
ğ‘˜âˆˆ[1,ğ‘‡]âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥âˆšğ·1,âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥2
âˆšğ·0
â‰¤ËœOğ‘“(ğ’˜1,0)âˆ’minğ’˜ğ‘“(ğ’˜)âˆš
ğ‘‡
+O(( 1âˆ’ğ›½2)2âˆšï¸
ğ·0). (4)
For simplicity, we defer the concrete form of ğ›¾to Appendix B.2.
We provide some remarks on the results as follows, and state the
proof idea in the next section.
Explanation for Theorem 4.1. Theorem 4.1 is pioneering in
demonstrating that RR Adam is capable of converging under the
non-uniform smoothness condition, a finding that is novel to our
best knowledge. Observing the right-hand side of inequality (4), one
can see that as ğ‘‡â†’âˆ , it approachesO((1âˆ’ğ›½2)2âˆšğ·0). This sug-
gests that Adamâ€™s convergence to the vicinity of stationary points
is inversely related to the proximity of ğ›½2to 1. This theoretical
insight corroborates the common practice of choosing ğ›½2close to
0.99. A counterexample provided later will further illustrate that
convergence to a neighborhood, rather than an exact point, is an
intrinsic characteristic of the algorithm.
Beyond the(ğ¿0,ğ¿1)-smooth condition, Theorem 4.1 presupposes
only that the gradient noise exhibits affine variance as per Assump-
tion 3.2, which is a relatively mild constraint that eschews the
need for a bounded gradient norm. This is crucial, as imposing a
bound would reduce the (ğ¿0,ğ¿1)-smooth condition to an (ğ¿0+ğ¿1ğ‘€)-
smooth condition with a gradient norm capped by ğ‘€. Additionally,
we do not require the adaptive learning rate ğœ‚ğ‘˜/âˆšï¸
Ë†ğœˆğ‘˜to be upper
bounded, nor do we stipulate a large regularizer ğœ‰â€”aligning with
common practices in deep learning libraries where a small ğœ‰such
as10âˆ’8is often effective. Our theorem permits any non-negative
ğœ‰, including zero. Finally, Theorem 4.1 asserts convergence for
 
2963Provable Adaptivity of Adam under Non-uniform Smoothness KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
every possible trajectory, a guarantee that exceeds the typical "con-
vergence in expectation" results and poses a significant technical
challenge.
On the Comparison to Existing Analyses of RR Adam. Our
analysis extends the applicability of RR Adam by ensuring con-
vergence under the (ğ¿0,ğ¿1)smooth condition, which inherently
encompasses the traditional ğ¿-smooth condition. This broadened
perspective allows our results to guarantee convergence for RR
Adam even under the more general ğ¿-smooth scenario. When jux-
taposed with the state-of-the-art analysis of RR Adam under the
ğ¿-smooth condition by Zhang et al . [46] , our findings advance the
field in two significant ways. Firstly, we elevate the notion of con-
vergence from the expected sense to the more stringent trajectory-
wise convergence. Secondly, we refine the estimated convergence
neighborhood, tightening it from (1âˆ’ğ›½2)âˆšğ·0to(1âˆ’ğ›½2)2âˆšğ·0.
Collectively, our analysis not only operates under a less restric-
tive assumptionâ€”the (ğ¿0,ğ¿1)-smooth conditionâ€”but also delivers
substantively enhanced convergence results.
On the range of hyperparameters. Theorem 4.1 indicates
that Adam can work when ğ›½2is close enough to 1. This matches
the practical choice of ğ›½2(e.g., 0.999in default setting, 0.95in the
GPT-3 training [ 4]). Note that our result does not contradict the
counterexamples of Adamâ€™s non-convergence [ 30,46], as these
divergence results require ğ›½2to be small and thus not close to 1.
Rather, these counterexamples suggest that large ğ›½2is necessary
for convergence. As for ğ›½1, Theorem 4.1 needs ğ›½2
1<ğ›½2. Whenğ›½2
is large, Theorem 4.1 allows a wide range of candidates of ğ›½1(e.g.,
0.9in default setting and 0.5in GAN [29]).
Figure 2: Reconduct of experimental results from [ 46]. The
objective function is defined in Eq. (5). One can observe that
while letting ğ›½2closer to 1can make the limiting gradient
norm smaller, the limiting gradient norm always stabilizes
beyond 0.
On the neighborhood of stationary points. Whenğ·0â‰ 0,
Theorem 4.1 only ensures that Adam converges to a neighborhood
of stationary points {ğ’˜:min{âˆ¥âˆ‡ğ‘“(ğ’˜))âˆ¥âˆšğ·1,âˆ¥âˆ‡ğ‘“(ğ’˜)âˆ¥2
âˆšğ·0}â‰¤O(( 1âˆ’
ğ›½2)âˆšğ·0)}. Since SGD converges to the stationary points with di-
minishing learning rate, one may wonder if Theorem 4.1 can be
improved to obtain the same conclusion as SGD. Unfortunately,
there is a counterexample in the existing literature ( function (9)
in Zhang et al . [46] ) showing that Adam does notconverge to
stationary points even if all the conditions in Theorem 4.1 aresatisfied. Specifically, [46] consider the following function:
ğ‘“(ğ‘¥)=9âˆ‘ï¸
ğ‘—=0ğ‘“ğ‘—(ğ‘¥)=1
10ğ‘¥2âˆ’1,
whereğ‘“ğ‘—(ğ‘¥)=((ğ‘¥âˆ’1)2ifğ‘—=0
âˆ’0.1
ğ‘¥âˆ’10
92
if1â‰¤ğ‘—â‰¤9.(5)
One can easily verify such an example satisfies Assumptions 3.2
and 3.1 with ğ·0>0. As shown in Figure 2, when running Adam
(withğ›½1=0.9,ğœ‚ğ‘˜=0.1/âˆš
ğ‘˜,ğ‘=3,ğ‘¥0=âˆ’2), it does not converge to
exact stationary points. Instead, it converges to a neighborhood of
stationary points with size inversely proportional to ğ›½2. Therefore,
the non-vanishing term in Theorem 4.1 is notdue to the limitation
of the proof. Rather, it is an intrinsic property of Adam.
Why cannot Adam converge to exact stationary points when
ğ·0>0? Intuitively, this is because even with diminishing ğœ‚ğ‘˜,
the effective learning rateğœ‚ğ‘˜
ğœ‰1ğ‘‘+âˆšğ‚ğ‘˜,ğ‘–may not diminish due to the
potentially decreasingâˆšğ‚ğ‘˜,ğ‘–. The good news is that O((1âˆ’ğ›½2)âˆšğ·0)
approaches 0asğ›½2gets close to 1. This means that the neighborhood
shrinks asğ›½2â†’1(this is also observed in Figure 2). As discussed
above, the practical use of ğ›½2is close to 1, and thusğ‘‚((1âˆ’ğ›½2)âˆšğ·0)
is tolerable.
On the Diminishing Learning Rate. In Theorem 4.1, we con-
sider a diminishing learning rate of the form ğœ‚ğ‘˜=ğœ‚1âˆš
ğ‘˜to maintain
consistency with existing works on RR Adam, such as those by
[33,46], which also employ diminishing learning rates. Nonethe-
less, our results can be readily extended to RR Adam with a constant
learning rate. By adhering to the same proof strategy outlined in
Theorem 1, one can demonstrate that with a constant learning rate
ğœ‚, the conclusion (as given in Eq. (4)) of Theorem 1 is modified to
minğ‘˜âˆˆ[1,ğ‘‡]{âˆ¥âˆ‡ğ‘“(ğ‘¤ğ‘˜,0)âˆ¥âˆšğ·0,âˆ¥âˆ‡ğ‘“(ğ‘¤ğ‘˜,0)âˆ¥2âˆšğ·1}â‰¤ ËœOğ‘“(ğ’˜1,0)âˆ’minğ’˜ğ‘“(ğ’˜)
ğœ‚ğ‘‡
+
O(âˆšğ·0(1âˆ’ğ›½2)2)+O(ğœ‚). In essence, while Adam may converge
more rapidly to a neighborhood (with the rate improving from 1/âˆšğ‘¡
to1/ğ‘¡), the size of this neighborhood is increased by an additional
termO(ğœ‚), which is attributable to the constant step size.
5 Proof sketch
In this section, we briefly explain our proof idea for Theorem 4.1,
which can be divided into two stages. In Stage I, we will prove
Theorem 4.1 for Adam with ğ›½1=0to show the challenge brought
by(ğ¿0,ğ¿1)-smooth condition and how we tackle it. In Stage II, we
then show the additional difficulty when adding the momentum
and our corresponding intuition to solve it.
Stage I: Convergence of Adam with ğ›½1=0.By the descent
lemma,
ğ‘“(ğ’˜ğ‘˜+1,0)âˆ’ğ‘“(ğ’˜ğ‘˜,0)â‰¤âŸ¨ğ’˜ğ‘˜+1,0âˆ’ğ’˜ğ‘˜,0,âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âŸ©
|                           {z                           }
First Order
+ğ¿ğ‘™ğ‘œğ‘
2âˆ¥ğ’˜ğ‘˜+1,0âˆ’ğ’˜ğ‘˜,0âˆ¥2,
|                        {z                        }
Second Order(6)
 
2964KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bohan Wang et al.
whereğ¿ğ‘™ğ‘œğ‘is the local smoothness. We bound the first-order and
the second-order term respectively. The upper bound on second-
order term is relatively simple. Due to the limited space, we only
show the idea of bounding first-order term here.
The ever-changing adaptive learning rate poses a challenge
on deriving the bound. It is even noted that with small ğ›½2, the
first order term can be positive [ 30]. However, we notice that
ifğ‚ğ‘˜,ğ‘–is stationary, i.e., RMSProp degenerates to SGD with pre-
conditioning, the first order term equals to âˆ’ğœ‚ğ‘˜âŸ¨Ã
ğ‘–1
ğœ‰1ğ‘‘+âˆšğ‚ğ‘˜,0âŠ™
âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,ğ‘–),âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âŸ©â‰ˆâˆ’ğœ‚ğ‘˜âŸ¨Ã
ğ‘–1
ğœ‰1ğ‘‘+âˆšğ‚ğ‘˜,0âŠ™âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,0),âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âŸ©,
which is indeed negative. While that " ğ‚ğ‘˜,ğ‘–is stationary" is too good
to be true, we prove that ğ‚ğ‘˜,ğ‘–changes little when ğ›½2is close to 1,
assuming that the gradient is large. Below we denote ğ‚ğ‘™,ğ‘˜,ğ‘–as the
ğ‘™-th component of ğ‚ğ‘˜,ğ‘–.
Lemma 5.1 (Informal). For anyğ‘™âˆˆ [ğ‘‘]andğ‘–âˆˆ [0,ğ‘›âˆ’1], if
maxğ‘âˆˆ[0,ğ‘›âˆ’1]|ğœ•ğ‘™ğ‘“ğ‘(ğ’˜ğ‘˜,0)|=Î©(Ãğ‘˜âˆ’1
ğ‘Ÿ=1ğ›½(ğ‘˜âˆ’1âˆ’ğ‘Ÿ)
2
2ğœ‚ğ‘Ÿâˆ¥âˆ‡ğ‘“(ğ’˜ğ‘Ÿ,0)âˆ¥+ğœ‚ğ‘˜),
then|ğ‚ğ‘™,ğ‘˜,ğ‘–âˆ’ğ‚ğ‘™,ğ‘˜,0|=O((1âˆ’ğ›½2)ğ‚ğ‘™,ğ‘˜,0).
The idea of Lemma 5.1 is simple: since ğ‚ğ‘˜,ğ‘–=ğ›½2ğ‚ğ‘˜,ğ‘–âˆ’1+(1âˆ’
ğ›½2)âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğœğ‘˜,ğ‘–)âŠ™2, the change of ğ‚ğ‘˜,ğ‘–w.r.t.ğ‘–should be small when
ğ›½2is large. However, we need to check that the relative size of
âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğœğ‘˜,ğ‘–)âŠ™2w.r.t. ğ‚ğ‘˜,ğ‘–âˆ’1is uniformly bounded across varying ğ›½2,
otherwise the term (1âˆ’ğ›½2)âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğœğ‘˜,ğ‘–)âŠ™2may not go to zero when
ğ›½2â†’1. We resolve this challenge by expanding ğ‚ğ‘˜,ğ‘–in terms of
squared gradients and bounding the gap between each of the terms
andâˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğœğ‘˜,ğ‘–)âŠ™2by echoing(ğ¿0,ğ¿1)-smooth condition. We defer
a detailed proof to Corollary B.9 for details.
As a conclusion, if we denote those dimensions with large gradi-
ents (i.e., satisfying the requirement of Lemma 5.1) as Lğ‘˜
ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’and
the rest as Lğ‘˜
ğ‘ ğ‘šğ‘ğ‘™ğ‘™, Lemma 5.1 indicates that the Lğ‘˜
ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’part (i.e.,Ã
ğ‘™âˆˆLğ‘˜
ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’(ğ’˜ğ‘™,ğ‘˜+1,0âˆ’ğ’˜ğ‘™,ğ‘˜,0)ğœ•ğ‘™ğ‘“(ğ’˜ğ‘˜,0)) in the first order term can be
bounded as
âˆ’ğœ‚ğ‘˜âˆ‘ï¸
ğ‘™âˆˆLğ‘˜
ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ğœ•ğ‘™ğ‘“(ğ’˜ğ‘˜,0)
âˆšğ‚ğ‘™,ğ‘˜,ğ‘–+ğœ‰âˆ‘ï¸
ğ‘–ğœ•ğ‘™ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,ğ‘–)
â‰ˆâˆ’ğœ‚ğ‘˜âˆ‘ï¸
ğ‘™âˆˆLğ‘˜
ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ 
ğœ•ğ‘™ğ‘“(ğ’˜ğ‘˜,0)2
âˆšğ‚ğ‘™,ğ‘˜,0+ğœ‰+O 
(1âˆ’ğ›½2)ğœ•ğ‘™|ğ‘“(ğ’˜ğ‘˜,0)|Ã
ğ‘–|ğœ•ğ‘™ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,ğ‘–)|
âˆšğ‚ğ‘™,ğ‘˜,0+ğœ‰!!
=âˆ’Î©
ğœ‚ğ‘˜minâˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥âˆšğ·1,âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥2
âˆšğ·0
+ğ‘‚(ğœ‚ğ‘˜(1âˆ’ğ›½2)âˆšï¸
ğ·0).
The last equation uses the affine noise assumption (Assumption
3.2), and we defer a detailed proof to Appendix B.4. A remain-
ing problem is how to deal with those components in Lğ‘˜
ğ‘ ğ‘šğ‘ğ‘™ğ‘™. We
treat them as error terms. Concretely, ğ‘™âˆˆLğ‘˜
ğ‘ ğ‘šğ‘ğ‘™ğ‘™indicates that
ğœ•ğ‘™ğ‘“(ğ’˜ğ‘˜,0)=O(Ãğ‘˜âˆ’1
ğ‘Ÿ=1ğ›½(ğ‘˜âˆ’1âˆ’ğ‘Ÿ)
2
2ğœ‚ğ‘Ÿâˆ¥âˆ‡ğ‘“(ğ’˜ğ‘Ÿ,0)âˆ¥+ğœ‚ğ‘˜). Applying it di-
rectly intoÃ
ğ‘™âˆˆLğ‘˜
ğ‘ ğ‘šğ‘ğ‘™ğ‘™(ğ’˜ğ‘™,ğ‘˜+1,0âˆ’ğ’˜ğ‘™,ğ‘˜,0)ğœ•ğ‘™ğ‘“(ğ’˜ğ‘˜,0), we have
âˆ’ğœ‚ğ‘˜âˆ‘ï¸
ğ‘™âˆˆLğ‘˜
ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ğœ•ğ‘™ğ‘“(ğ’˜ğ‘˜,0)
âˆšğ‚ğ‘™,ğ‘˜,ğ‘–+ğœ‰âˆ‘ï¸
ğ‘–ğœ•ğ‘™ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,ğ‘–)
=O 
ğœ‚ğ‘˜ ğ‘˜âˆ’1âˆ‘ï¸
ğ‘Ÿ=1ğ›½(ğ‘˜âˆ’1âˆ’ğ‘Ÿ)
2
2ğœ‚ğ‘Ÿâˆ¥âˆ‡ğ‘“(ğ’˜ğ‘Ÿ,0)âˆ¥+ğœ‚ğ‘˜!!
,where the equation is becauseğœ•ğ‘™ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,ğ‘–)
âˆšğ‚ğ‘™,ğ‘˜,ğ‘–+ğœ‰is bounded (proved by
Lemma B.3).
In order to upper bound the first order term, we then need to
prove thatâˆ’Î©(ğœ‚ğ‘˜min{âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥âˆšğ·1,âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥2
âˆšğ·0})dominates
O(ğœ‚ğ‘˜(Ãğ‘˜âˆ’1
ğ‘Ÿ=1ğ›½(ğ‘˜âˆ’1âˆ’ğ‘Ÿ)
2
2ğœ‚ğ‘Ÿâˆ¥âˆ‡ğ‘“(ğ’˜ğ‘Ÿ,0)âˆ¥+ğœ‚ğ‘˜)). This is not necessarily
true, as the historical gradient norms in the latter term can be large.
Remark 5.2.We recognize this as the challenge brought by (ğ¿0,ğ¿1)-
smooth condition, since the latter term degenerates to O(ğœ‚2
ğ‘˜)with
ğ¿-smooth condition, which is minor (Ãğ‘‡
ğ‘˜=1ğœ‚2
ğ‘˜is only in order logğ‘‡).
We address this challenge by noting that what we need to bound
is the sum of the first order term. Fortunately, although we can-
not upper bound the first order term in one single epoch, we
can bound the sum of it across epochs. By a sum order change,
the sum ofO(ğœ‚ğ‘˜(Ãğ‘˜âˆ’1
ğ‘Ÿ=1ğ›½(ğ‘˜âˆ’1âˆ’ğ‘Ÿ)
2
2ğœ‚ğ‘Ÿâˆ¥âˆ‡ğ‘“(ğ’˜ğ‘Ÿ,0)âˆ¥+ğœ‚ğ‘˜))overğ‘˜equals
toO(Ãğ‘‡
ğ‘˜=1ğœ‚2
ğ‘˜âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥+ lnğ‘‡). This is smaller by the sum of
âˆ’Î©(ğœ‚ğ‘˜min{âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥âˆšğ·1,âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥2
âˆšğ·0})by order ofğœ‚ğ‘˜except a lnğ‘‡
term due to the mean value inequality, i.e.,
ğœ‚2
ğ‘˜âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥â‰¤O(ğœ‚2
ğ‘˜)+O 
ğœ‚2
ğ‘˜âˆšï¸‚
ğ·1
ğ·0âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥2!
.
We then conclude the sum of the first order term can be bounded
byâˆ’Î©(ğœ‚ğ‘˜min{âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥âˆšğ·1,âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥2
âˆšğ·0})+O( lnğ‘‡).
Remark 5.3 (Difficulty compared to the analysis under ğ¿-smooth
condition). Here we illustrate the challenge brought by stepping
beyondğ¿-smooth condition. First of all, the change of ğ‚ğ‘˜,ğ‘–is easier
to bound without the historical gradient term due to the absence
of the gradient norm in the bound of local smoothness. Secondly,
underğ¿-smooth condition, the error does not contain historical
gradient information and is only in order of O(ğœ‚2
ğ‘˜), which is easy
to bound.
Stage II: adding the momentum. The second order term
of Adam can be bounded similarly. However, the analysis of the
first order term becomes more challenging even though we still
have ğ‚ğ‘˜,ğ‘–â‰ˆğ‚ğ‘˜,0. Specifically, even with constant ğ‚ğ‘˜,ğ‘–=ğ‚ğ‘˜,0,
âˆ’ğœ‚ğ‘˜âŸ¨Ã
ğ‘–ğ’ğ‘˜,ğ‘–âˆšğ‚ğ‘˜,ğ‘–+ğœ‰,âˆ’âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âŸ©>0is not necessarily correct, as the
momentum ğ’ğ‘˜,ğ‘–contains a heavy historical signal, and may push
the update away from the negative gradient direction.
We resolve this challenge by observing that the alignment of
ğ’˜ğ‘˜+1,0âˆ’ğ’˜ğ‘˜,0andâˆ’âˆ‡ğ‘“(ğ’˜ğ‘˜,0)is required due to that our analysis
is based on the potential function ğ‘“(ğ’˜ğ‘˜,0). However, while this
potential function is suitable for the analysis of RMSProp, it is
no longer appropriate for Adam based on the above discussion.
We need to construct another potential function. Our construc-
tion of the potential function is based on the following observa-
tion: we revisit the update rule in Algorithm 1 and rewrite it as
ğ’ğ‘˜,ğ‘–âˆ’ğ›½1ğ’ğ‘˜,ğ‘–âˆ’1
1âˆ’ğ›½1=âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,ğ‘–).
Notice that the right-hand-side of the above equation contains
no historical gradients but only the gradient of the current step! By
 
2965Provable Adaptivity of Adam under Non-uniform Smoothness KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
dividing(âˆšğ‚ğ‘˜,ğ‘–+ğœ‰)/ğœ‚ğ‘˜above,
ğ’˜ğ‘˜,ğ‘–+1âˆ’ğ’˜ğ‘˜,ğ‘–âˆ’ğ›½1(ğ’˜ğ‘˜,ğ‘–âˆ’ğ’˜ğ‘˜,ğ‘–âˆ’1)
1âˆ’ğ›½1â‰ˆâˆ’ğœ‚ğ‘˜âˆšğ‚ğ‘˜,0+ğœ‰1ğ‘‘âŠ™ğ’ğ‘˜,ğ‘–âˆ’ğ›½1ğ’ğ‘˜,ğ‘–âˆ’1
1âˆ’ğ›½1
=âˆ’ğœ‚ğ‘˜âˆšğ‚ğ‘˜,0+ğœ‰1ğ‘‘âŠ™âˆ‡ğ‘“ğœğ‘˜,ğ‘–(ğ’˜ğ‘˜,ğ‘–).
After simple rearrangement, one can see that the sequence {ğ’˜ğ‘˜,ğ‘–âˆ’ğ›½1ğ’˜ğ‘˜,ğ‘–âˆ’1
1âˆ’ğ›½1}
are (approximately) doing SGD within one epoch (with coordinate-
wise but constant learning rate ğ‚ğ‘˜,ğ‘–)! We define
ğ’–ğ‘˜,ğ‘–â‰œğ’˜ğ‘˜,ğ‘–âˆ’ğ›½1ğ’˜ğ‘˜,ğ‘–âˆ’1
1âˆ’ğ›½1.
Then, further notice that the distance between ğ’–ğ‘˜,ğ‘–=ğ’˜ğ‘˜,ğ‘–+ğ›½1ğ’˜ğ‘˜,ğ‘–âˆ’ğ’˜ğ‘˜,ğ‘–âˆ’1
1âˆ’ğ›½1
andğ’˜ğ‘˜,ğ‘–is in order of one stepâ€™s update, and thus ğ’–ğ‘˜,ğ‘–â‰ˆğ’˜ğ‘˜,ğ‘–.
Therefore, we choose our potential function as ğ‘“(ğ’–ğ‘˜,ğ‘–). The Tay-
lorâ€™s expansion of ğ‘“atğ’–ğ‘˜,0then provides a new descent lemma,
i.e.,
ğ‘“(ğ’–ğ‘˜+1,0)âˆ’ğ‘“(ğ’–ğ‘˜,0)â‰¤âŸ¨ğ’–ğ‘˜+1,0âˆ’ğ’–ğ‘˜,0,âˆ‡ğ‘“(ğ’–ğ‘˜,0)âŸ©
|                             {z                             }
First Order
+ğ¿0+ğ¿1âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âˆ¥
2âˆ¥ğ’˜ğ‘˜+1,0âˆ’ğ’˜ğ‘˜,0âˆ¥2,
|                                                   {z                                                   }
Second Order(7)
By noticing ğ’˜ğ‘˜,ğ‘–â‰ˆğ’–ğ‘˜,ğ‘–â‰ˆğ’–ğ‘˜,0, the first order term can be fur-
ther approximated by âˆ’âŸ¨ğœ‚ğ‘˜âˆšğ‚ğ‘˜,0+ğœ‰1ğ‘‘âŠ™âˆ‡ğ‘“(ğ’˜ğ‘˜,0),âˆ‡ğ‘“(ğ’˜ğ‘˜,0)âŸ©which
is negative. The rest of the proof is the same as that of Stage I.
Remark 5.4 (On Why State-of-the-Art Results Do Not Achieve Tra-
jectory-Wise Convergence as Ours). The state-of-the-art analysis
of RR Adam under the ğ¿-smooth condition, as presented by Zhang
et al. [46] , also addresses the misalignment between ğ’˜ğ‘˜+1,0âˆ’ğ’˜ğ‘˜,0
andâˆ’âˆ‡ğ‘“(ğ’˜ğ‘˜,0). However, their approach does not employ a poten-
tial function, resulting in convergence results that are restricted
to in-expectation guarantees. Specifically, Zhang et al . [46] man-
age this misalignment by assuming a uniform distribution over
all possible shuffling orders and demonstrating that, under this as-
sumption, the expected value of ğ’˜ğ‘˜+1,0âˆ’ğ’˜ğ‘˜,0is approximately equal
toâˆ’âˆ‡ğ‘“(ğ’˜ğ‘˜,0). In contrast, our methodology introduces an auxiliary
function,ğ‘“(ğ’–ğ‘˜,ğ‘–), and examines the dynamics of ğ’–ğ‘˜,ğ‘–. This approach
shifts the challenge from aligning ğ’˜ğ‘˜+1,0âˆ’ğ’˜ğ‘˜,0withâˆ’âˆ‡ğ‘“(ğ’˜ğ‘˜,0)to
aligning ğ’–ğ‘˜+1,0âˆ’ğ’–ğ‘˜,0withâˆ’âˆ‡ğ‘“(ğ’˜ğ‘˜,0). Such a strategy simplifies the
analytical process and facilitates the demonstration of trajectory-
wise convergence.
Remark 5.5 (Similar potential functions in the existing literature.).
We notice that similar potential functions have already been applied
in the analysis of other momentum-based optimizers, e.g., momen-
tum (S)GD in [ 15] and [ 26]. However, extending the proof to Adam
is highly-nontrivial. The key difficulty lies in showing that the first-
order expansion of ğ‘“(ğ’–ğ‘˜,0)is positive, which further requires that
the adaptive learning rate does not change much within one epoch.
This is hard for Adam as the adaptive learning rate of Adam can be
non-monotonic. The lack of L-smooth condition makes the proof
even challenging due to the unbounded error brought by gradient
norms.6 Comparison Between Adam and SGD
Now we compare the convergence rate of Adam with SGD. To do
so, we need a lower bound of SGD in the same setting as Theorem
4.1. There are several existing lower bounds of SGD under (ğ¿0,ğ¿1)
smoothness condition (e.g., [ 8,43]). However, we find these lower
bounds cannot be directly applicable for comparison with Adam.
This is because:
â€¢1) In the lower bound of [ 8,43], they pick the learning rate
before the construction of the objective function and initial-
ization point (we restate their lower bound in Appendix B.1
for completeness). In other words, it is possible that if we fix
the objective function and tune the learning rate (which is a
common practice in the training of deep neural networks),
SGD can converge very fast. For rigorous comparison with
Adam, we need a lower bound with reversed ordering. That
is, we need the following statement: â€œconsider a fixed objec-
tive function and initialization point, then no matter how
we pick the learning rate, SGD suffers from a certain rate. "
â€¢2) The lower bounds of SGD in [ 8,43] require constant
learning rate. However, since Adam in Theorem 4.1 uses
diminishing-learning-rate, we aim to establish a lower bound
of SGD with diminishing learning rate.
Unfortunately, there is no existing lower bound that satisfies
the above two properties. In the following theorem, we provide a
refined lower bound of SGD in the setup that we desired.
Theorem 6.1. For anyğ¿0,ğ¿1,ğ‘‡>0, there exists an objective function
ğ‘“obeying Assumption 3.1, and an initialized parameter ğ’˜0satisfying
ğ‘€=sup{âˆ¥âˆ‡ğ‘“(ğ’˜)âˆ¥:ğ‘“(ğ’˜)â‰¤ğ‘“(ğ’˜0)}, such thatâˆ€ğœ‚1>0, the itera-
tions of SGD{ğ’˜ğ‘¡}âˆ
ğ‘¡=0satsifies minğ‘¡âˆˆğ‘‡âˆ¥âˆ‡ğ‘“(ğ’˜ğ‘¡)âˆ¥2=Î©(ğ‘€(ğ‘“(ğ’˜0)âˆ’
minğ’˜âˆˆRğ‘‘ğ‘“(ğ’˜))/âˆš
ğ‘‡).
The proof can be in Appendix A. The proof idea is mainly moti-
vated by [ 44]. We highlight some differences when we try to reach
the two properties mentioned previously.
â€¢To reverse the ordering of â€œpicking learning rate and func-
tions & initialization", we simply augment the worst-case
example in [ 44] into 2 dimensional space. It turns out this
simple trick is effective in the proof.
â€¢To change constant learning rate into diminishing learning
rate, we show that: when the initial learning rate ğœ‚0is larger
than a certain threshold, the decay rate of the learning rate
cannot offset the curvature explosion along the iteration,
causing divergence; on the other hand, when initial ğœ‚0is
small, it would lead to slow convergence. This is a new find-
ing in(ğ¿0,ğ¿1)setting. We prove this result by mathematical
induction. This part of the discussion is not required in the
lower bound of [44] with constant learning rate.
Comparison between Adam and SGD.. Finally, we discuss the
implication the lower bound of SGD (Theorem 6.1) and the upper
bound of Adam (Theorem 4.1). In the lower bound of SGD, there is
an extra constant ğ‘€which does not appear in the upper bound of
Adam. This allows us to compare the convergence rates of these
two algorithms.
We summarize our findings as follows. We emphasize that The-
orem 4.1 and Theorem 6.1 share exactly the same setting: both
 
2966KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bohan Wang et al.
consider function class under the same assumptions; both SGD
and Adam use diminishing learning rate. Therefore, the following
comparison is rigorous.
Finding 1: Whenğ·0=0. Adam converges to stationary point
with rateO
1
ğ‘‡
while GD converges with rate O
1âˆš
ğ‘‡
. So Adam
converges (to stationary points) faster.
Finding 2: Whenğ·0>0. There exists a set of ğ’˜with infinite
Lebesgue measure, such that, when starting at any ğ’˜in this set,
Adam converges (to the neighborhood of stationary points) faster
than SGD.
Note that the above statement â€œalgorithm 1 converges faster than
algorithm 2â€ does not mean that algorithm 1 always converges faster
than algorithm 2. For sure, rarely can anyone make such a strong
statement. The above statement actually means that â€œthe worst-case
complexity of algorithm 1 is faster than that of algorithm 2, and both
complexity bounds can be simultaneously achieved when working
on the same function and starting at the same initializationâ€. This
definition is adopted from [ 35], and it is a widely accepted definition
in the optimization field.
Proof. Finding 1 can be directly proved by plugging ğ·0=
0into Theorem 4.1 and squaring the inequality. We now prove
Finding 2. First, we state an important fact from the proof of
Theorem 6.1.
Fact 1: For the counter-example we constructed in Theorem 6.1.
ğ‘€=sup{âˆ¥âˆ‡ğ‘“(ğ’˜)âˆ¥:ğ‘“(ğ’˜)â‰¤ğ‘“(ğ’˜0)}goes to infinity as âˆ¥ğ’˜âˆ¥goes
to infinity. Further, for any ğ¶>0, the set{ğ’˜:ğ‘€>ğ¶}is of infinite
Lebesgue measure.
Based on Fact 1, for the worst-case example in Theorem 6.1, there
must exist a region in Rğ‘‘whereğ‘€is larger than all the constant
terms in the upper bound of Adam in Theorem 4.1. Further, Such
region is of infinite Lebesgue measure. When running Adam and
SGD simultaneously on this worst-case example starting from any
ğ’˜in this region, the constants in the upper bound of Adam is smaller
than the constants in the lower bound of SGD. Since the upper and
lower bounds share the same rate, so Adam is faster. Note that
there is an additional constant term in the upper bound of Adam
(4), so we conclude that Adam converges to the neighborhood of
stationary points faster than SGD. â–¡
Note that when ğ·0>0, Adam is still guaranteed to converge
faster, but only to the neighborhood in lieu of the exact stationary
points. We emphasize that this â€œneighborhood" cannot be elimi-
nated since there is a counter-example showing that Adam cannot
reach 0 gradient when ğ·0>0(see Figure 2). So this is an in-
trinsic property of Adam, rather than the limitation of the theory.
Nevertheless, we believe the effect of â€œnot converging to exact sta-
tionary points" is minor in practice. This is because: 1) As shown
in Theorem 4.1 and Figure 2, the size of the â€œambiguity zone" is
inversely proportional to ğ›½2. Sinceğ›½2is often chosen to be close to
1, the ambiguity zone shrinks and becomes negligible. 2) Machine
learning tasks do not pursue high-precision solutions (as much as
other fields like PDE). Practitioners usually aim to efficiently find
approximate solutions, rather than exact solutions that over-fit the
training data.To our knowledge, the discussion above is the first time that
Adam and SGD are rigorously compared in the same setting where
the advantage of Adam can be revealed. We believe these results
shed new light on understanding the benefit of Adam.
Finally, we briefly explain why the upper bound of Adam is
independent of ğ‘€. Intuitively, this is because: (1) it uses different
learning rates for different components of ğ’˜. (2) For each component
ofğ’˜, the effective learning rate adjusts according to the gradient
norm (thus according to the local smoothness). Even though the
initial effective learning rate is small, it gets larger when moving
in a flat landscape. Combining together, the initial learning rate of
Adam can be independent of ğ‘€, and so is its convergence rate.
7 Discussion
Adamâ€™s Advantage over Gradient Descent with Gradient Clip-
ping. Zhang et al . [43] established that gradient descent (GD) and
stochastic gradient descent (SGD) with gradient clipping are con-
vergent under the (ğ¿0,ğ¿1)smooth condition. A natural inquiry
arises concerning the benefits of Adam over GD/SGD when gradi-
ent clipping is employed. While we lack robust theoretical backing
to fully answer this question, one discernible advantage of Adam, as
inferred from our results, is its capability to manage more intricate
noise profiles that adhere to the affine variance noise assumption.
In contrast, the current analyses of GD/SGD with gradient clip-
ping within the(ğ¿0,ğ¿1)-smooth framework presuppose that the
deviation between the stochastic gradient and the true gradient is
uniformly bounded with certaintyâ€”an assumption more stringent
than the one we consider. Indeed, a recent work [ 13] demonstrates
that there exists a counterexample satisfying Assumption 3.2 over
which SGD with gradient clipping fails to converge. Together with
Theorem 4.1, their result demonstrate that a wide range of applica-
tion scenario of Adam than SGD with gradient clipping.
Insights for Practitioners. Here we discuss the insights our
Theorem 4.1 can provide to practitioners. Firstly, the widespread
adoption of Adam among practitioners, evidenced by its exten-
sive citation record, underscores the importance of a theoretical
understanding of the algorithm.
Secondly, our findings offer theoretical support for a prevalent
practice among practitioners: for tasks involving architectures like
Transformers and LSTMs, Adam is often favored over SGD.
Lastly, in light of the convergence criteria delineated in Theorem
4.1, we propose practical guidance for hyperparameter selection
when employing Adam. Specifically, we recommend increasing ğ›½2
and experimenting with various ğ›½1values that satisfy ğ›½1<âˆšï¸
ğ›½2.
This heuristic could potentially reduce the computational burden
associated with exhaustive hyperparameter exploration across the
(ğ›½1,ğ›½2)space.
Limitations of Theorem 4.1. While Theorem 4.1 represents
a significant advancement in establishing the convergence of RR
Adam under non-uniform smoothness conditions, it is not without
its limitations. Specifically, Theorem 4.1 applies to cases where
ğ›½1=0, implying the absence of momentum. The theorem does not
distinguish between the convergence rates when ğ›½1=0and when
ğ›½1>0, thus not demonstrating the benefits of momentum. The
theoretical elucidation of momentumâ€™s advantage within Adamâ€™s
 
2967Provable Adaptivity of Adam under Non-uniform Smoothness KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
convergence analysis remains a complex question. The role of mo-
mentum is not fully understood even in momentum SGD for non-
convex optimization [ 26], much less so for Adam. We acknowledge
the importance of this question but consider it beyond the scope of
this paper, leaving it for future research.
8 Conclusions and Future directions
In this paper, we have taken a pioneering step towards a theoretical
understanding of the adaptivity inherent in the Adam optimization
algorithm. We present the first convergence results for RR Adam
under the (ğ¿0,ğ¿1)-smooth condition, which is both realistic and
closely aligned with practical scenarios. In contrast to existing
analyses of RR Adam under the stronger ğ¿-smooth condition, our
results further demonstrate a more robust form of convergence,
specifically trajectory-wise convergence, and indicate a reduced
distance to the stationary point.
Future Directions. An intriguing avenue for future research lies
in delineating the advantages of incorporating momentum in Adam.
Our Theorem 4.1 indicates an identical convergence rate for both
ğ›½1=0(RMSProp) and ğ›½1>0(Adam), implying that the current
analysis does not differentiate between the iteration complexities of
Adam and RMSProp. Consequently, the specific benefits of momen-
tumin Adam remain elusive. This presents a substantial challenge,
given that the impact of momentum is not yet fully understood
even in the context of SGD with momentum. A possible strategy
could be to first establish a theoretical foundation for the advan-
tages of momentum in SGD, followed by extending these insights to
the analysis of Adam. Moreover, it would be compelling to explore
whether Adam can effectively manage more severe smoothness
conditions, such as those bounded by a higher-order polynomial of
the gradient norm.
Supplementary materials. Supplementary materials including
proofs can be found at https://arxiv.org/abs/2208.09900.
Acknowledgement. This work is founded by the Strategic Prior-
ity Research Program of the Chinese Academy of Sciences under
Grant No. XDB0680101, CAS Project for Young Scientists in Basic
Research under Grant No. YSBR-034, Innovation Project of ICT CAS
under Grants No. E261090, NSFC under Grant No. 12326608, Hetao
Shenzhen-Hong Kong Science and Technology Innovation Cooper-
ation Zone Project under Grant No.HZQSWS-KCCYB-2024016.
References
[1]LÃ©on Bottou. 2009. Curiously fast convergence of some stochastic gradient
descent algorithms. In Proceedings of the symposium on learning and data science,
Paris, Vol. 8. 2624â€“2633.
[2]LÃ©on Bottou. 2012. Stochastic gradient descent tricks. In Neural networks: Tricks
of the trade. Springer, 421â€“436.
[3]Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large Scale GAN
Training for High Fidelity Natural Image Synthesis. In International Conference
on Learning Representations.
[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877â€“1901.
[5]Congliang Chen, Li Shen, Fangyu Zou, and Wei Liu. 2021. Towards Practical
Adam: Non-Convexity, Convergence Theory, and Mini-Batch Acceleration. arXiv
preprint arXiv:2101.05471 (2021).
[6]Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. 2018. On the convergence
of a class of Adam-type algorithms for non-convex optimization. arXiv preprint
arXiv:1808.02941 (2018).[7]Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar.
2021. Gradient descent on neural networks typically occurs at the edge of stability.
arXiv preprint arXiv:2103.00065 (2021).
[8]Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun
Zhuang. 2022. Robustness to Unbounded Smoothness of Generalized SignSGD.
arXiv preprint arXiv:2208.11195 (2022).
[9]Soham De, Anirbit Mukherjee, and Enayat Ullah. 2018. Convergence guarantees
for RMSProp and Adam in non-convex optimization and an empirical comparison
to Nesterov acceleration. arXiv preprint arXiv:1807.06766 (2018).
[10] Alexandre DÃ©fossez, LÃ©on Bottou, Francis Bach, and Nicolas Usunier. 2020. A
Simple Convergence Proof of Adam and AdaGrad. arXiv preprint arXiv:2003.02395
(2020).
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An Image is Worth 16x16 Words: Trans-
formers for Image Recognition at Scale. In International Conference on Learning
Representations.
[12] Timothy Dozat. 2016. Incorporating Nesterov momentum into Adam. (2016).
[13] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay
Shakkottai, and Rachel Ward. 2022. The Power of Adaptivity in SGD: Self-
Tuning Step Sizes with Unbounded Gradients and Affine Variance. arXiv preprint
arXiv:2202.05791 (2022).
[14] SÃ©bastien Gadat and Ioana Gavra. 2020. Asymptotic study of stochastic adaptive
algorithm in non-convex landscape. arXiv preprint arXiv:2012.05640 (2020).
[15] Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. 2015.
Global convergence of the heavy-ball method for convex optimization. In 2015
European control conference (ECC). IEEE, 310â€“315.
[16] Saeed Ghadimi and Guanghui Lan. 2013. Stochastic first-and zeroth-order meth-
ods for nonconvex stochastic programming. SIAM Journal on Optimization 23, 4
(2013), 2341â€“2368.
[17] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. 2016. Mini-batch stochas-
tic approximation methods for nonconvex stochastic composite optimization.
Mathematical Programming 155, 1 (2016), 267â€“305.
[18] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. 2021. A Novel
Convergence Analysis for Algorithms of the Adam Family. arXiv preprint
arXiv:2112.03459 (2021).
[19] Feihu Huang, Junyi Li, and Heng Huang. 2021. Super-Adam: faster and universal
framework of adaptive gradients. Advances in Neural Information Processing
Systems 34 (2021), 9074â€“9085.
[20] Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. 2021. Non-convex
distributionally robust optimization: Non-asymptotic analysis. Advances in Neural
Information Processing Systems 34 (2021), 2771â€“2782.
[21] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of NAACL-HLT. 4171â€“4186.
[22] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[23] Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In International Conference on Learning Representations.
[24] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. 2023. Convergence of Adam
Under Relaxed Assumptions. arXiv preprint arXiv:2304.13972 (2023).
[25] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
Gao, and Jiawei Han. 2020. On the Variance of the Adaptive Learning Rate
and Beyond. In International Conference on Learning Representations . https:
//openreview.net/forum?id=rkgz2aEKDr
[26] Yanli Liu, Yuan Gao, and Wotao Yin. 2020. An Improved Analysis of Stochastic
Gradient Descent with Momentum. Advances in Neural Information Processing
Systems 33 (2020).
[27] Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. 2019. Adaptive gradient
methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843
(2019).
[28] Konstantin Mishchenko, Ahmed Khaled Ragab Bayoumi, and Peter RichtÃ¡rik.
2020. Random reshuffling: Simple analysis with vast improvements. Advances in
Neural Information Processing Systems 33 (2020).
[29] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised representa-
tion learning with deep convolutional generative adversarial networks. arXiv
preprint arXiv:1511.06434 (2015).
[30] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2018. On the Convergence of
Adam and Beyond. In International Conference on Learning Representations.
[31] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2019. On the convergence of
Adam and beyond. arXiv preprint arXiv:1904.09237 (2019).
[32] Mark Schmidt and Nicolas Le Roux. 2013. Fast convergence of stochastic gradient
descent under a strong growth condition. arXiv preprint arXiv:1308.6370 (2013).
[33] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. 2021. RMSprop con-
verges with proper hyper-parameter. In International Conference on Learning
Representations.
[34] Suvrit Sra. 2014. Advanced Optimization: Lecture 18 Proximal methods, Mono-
tone operators. https://www.cs.cmu.edu/ suvrit/teach/lect18.pdf (2014).
 
2968KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bohan Wang et al.
[35] Ruoyu Sun and Yinyu Ye. 2021. Worst-case complexity of cyclic coordinate
descent: O (nË† 2) O (n 2) gap with randomized version. Mathematical Programming
185 (2021), 487â€“520.
[36] Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. 2021. Smg: A shuffling
gradient-based method with momentum. In International Conference on Machine
Learning. PMLR, 10379â€“10389.
[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998â€“6008.
[38] Sharan Vaswani, Francis Bach, and Mark Schmidt. 2019. Fast and faster conver-
gence of SGD for over-parameterized models and an accelerated perceptron. In
The 22nd International Conference on Artificial Intelligence and Statistics. PMLR,
1195â€“1204.
[39] Xiaodong Yang, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. 2022. Normal-
ized/Clipped SGD with Perturbation for Differentially Private Non-Convex Opti-
mization. arXiv e-prints (2022), arXivâ€“2206.
[40] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Ku-
mar. 2018. Adaptive methods for nonconvex optimization. Advances in neural
information processing systems 31 (2018).
[41] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and San-
jiv Kumar. 2018. Adaptive Methods for Nonconvex Optimization. In Ad-
vances in Neural Information Processing Systems, S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31.
Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/file/
90365351ccc7437a1309dc64e4db32a3-Paper.pdf[42] Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. 2020. Improved analysis of
clipping algorithms for non-convex optimization. Advances in Neural Information
Processing Systems 33 (2020), 15511â€“15521.
[43] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. 2019. Why Gradient
Clipping Accelerates Training: A Theoretical Justification for Adaptivity. In
International Conference on Learning Representations.
[44] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim,
Sashank J Reddi, Sanjiv Kumar, and Suvrit Sra. 2019. Why Adam beats SGD for
attention models. (2019).
[45] Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhi-Quan
Luo. 2024. Why Transformers Need Adam: A Hessian Perspective. arXiv preprint
arXiv:2402.16788 (2024).
[46] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo.
2022. Adam Can Converge Without Any Modification on Update Rules. Advances
in Neural Information Processing Systems (2022).
[47] Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan
Gu. 2018. On the convergence of adaptive gradient methods for nonconvex
optimization. arXiv preprint arXiv:1808.05671 (2018).
[48] Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang,
and Yong Yu. 2018. Adashift: Decorrelation and convergence of adaptive learning
rate methods. arXiv preprint arXiv:1810.00143 (2018).
[49] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. 2019. A sufficient
condition for convergences of Adam and rmsprop. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 11127â€“11135.
 
2969