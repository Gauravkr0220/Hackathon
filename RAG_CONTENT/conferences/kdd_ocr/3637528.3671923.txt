Unraveling Block Maxima Forecasting Models with
Counterfactual Explanation
Yue Deng
Michigan State University
East Lansing, Michigan, USA
dengyue1@msu.eduAsadullah Hill Galib
Michigan State University
East Lansing, Michigan, USA
galibasa@msu.edu
Pang-Ning Tan
Michigan State University
East Lansing, Michigan, USA
ptan@msu.eduLifeng Luo
Michigan State University
East Lansing, Michigan, USA
lluo@msu.edu
ABSTRACT
Disease surveillance, traffic management, and weather forecasting
are some of the key applications that could benefit from block max-
ima forecasting of a time series as the extreme block maxima values
often signify events of critical importance such as disease outbreaks,
traffic gridlock, and severe weather conditions. As the use of deep
neural network models for block maxima forecasting increases, so
does the need for explainable AI methods that could unravel the in-
ner workings of such black box models. To fill this need, this paper
presents a novel counterfactual explanation framework for block
maxima forecasting models. Unlike existing methods, our proposed
framework, DiffusionCF, combines deep anomaly detection with
a conditional diffusion model to identify unusual patterns in the
time series that could help explain the forecasted extreme block
maxima. Experimental results on several real-world datasets demon-
strate the superiority of DiffusionCF over other baseline methods
when evaluated according to various metrics, particularly their
informativeness and closeness. Our data and codes are available at
https://github.com/yue2023cs/DiffusionCF.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks.
KEYWORDS
Explainable AI, counterfactual explanation, time series forecasting
ACM Reference Format:
Yue Deng, Asadullah Hill Galib, Pang-Ning Tan, and Lifeng Luo. 2024. Unrav-
eling Block Maxima Forecasting Models with Counterfactual Explanation.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM,
Barcelona, Spain, 12 pages. https://doi.org/10.1145/3637528.3671923
1 INTRODUCTION
Block maxima forecasting is the task of predicting the maximum
value of a time series for a future time window. Such a forecasting
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671923task has widespread applicability in many practical domains such as
weather forecasting, disease monitoring, traffic management, and
financial risk assessment. Block maxima forecasting models play
a crucial role in these domains as the predicted block maxima can
provide early warning to stakeholders about an impending severe
event. Despite their growing importance, there is a noticeable gap
in current research regarding the explainability of these models.
Explainability is important for block maxima forecasting models
as it enables stakeholders to comprehend and trust the modelâ€™s
predictions, fostering transparency and informed decision-making,
particularly in critical scenarios. The growing field of explainable AI
therefore plays a crucial role in this context, offering methodologies
and tools that can enhance the explainability of these models.
Explainable AI involves two primary methodologies: feature
attribution and counterfactual explanation methods [ 23]. Feature
attribution methods, such as LIME [ 25], SHAP [ 21], Grad-CAM [ 26],
CRP [ 1], and adversarial examples [ 10,28], focus on elucidating the
conditions behind a modelâ€™s decision, shedding light on the influ-
ential features or input values. Differently, counterfactual explana-
tion methods [ 13] seek to discover the smallest modification (i.e.,
changes) to the input that leads to a completely opposite forecast,
a.k.a., counterfactual target, by the black box model. Counterfactual
explanation methods are appealing as they offer a powerful means
to explore alternative scenarios and assess the impact of different
conditions on forecasting outcomes. For block maxima forecast-
ing, the counterfactual instances enable us to identify historical
patterns in the time series that may help explain the forecasted ex-
treme block maxima so actions can be taken to prevent their future
occurrence. In the example depicted in Figure 1, a prior incident
of epidemic outbreak may likely explain the forecasted next wave
by the black box model. The forecast is juxtaposed against a coun-
terfactual scenario which assumes preventative intervention had
been taken to mitigate the likelihood of the subsequent outbreak.
For time series, a good counterfactual instance must be (1) in-
formative, i.e., identifies the contrastive segment in the time series
that explains the generated prediction by the black box model, (2)
closely mimics the original time series, and (3) realistic, i.e., drawn
from the same distribution as the majority of the time series data.
However, striking a balance among the three criteria can be tricky.
For instance, neighborhood-based methods [ 8,20,33] consider
the nearest training instance whose prediction matches the coun-
terfactual target and utilize or partially modify them to form the
 
562
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
Figure 1: An illustration of counterfactual explanation for
block maxima forecast of a disease outbreak. The blue dot
denotes the forecasted epidemic outbreak while the red dot
represents a counterfactual scenario devoid of any outbreak.
counterfactual instance. While the counterfactual instance found
by these methods is quite realistic since it corresponds or is close
to an actual training instance, it may not resemble the original
time series. On the other hand, gradient-based methods [ 5,30,32]
learn a counterfactual instance by perturbing the time series in
such a way that maintains closeness to their original time series.
Yet these methods may induce modification across the entire time
series, making it difficult to pinpoint exactly the segment in the
time series that helps explain the model forecast, thereby reducing
its informativeness.
In this paper, we present a novel counterfactual explanation
framework for block maxima forecasting models. Unlike counter-
factual explanation for time series classification [ 3,8,18], choosing
the right counterfactual target value for block maxima is non-trivial
since the block maxima are continuous-valued, which means, there
are infinitely many possible counterfactual targets to choose from.
To address this challenge, we propose a principled way to create the
counterfactual target by leveraging the generalized extreme value
(GEV) distribution [ 7], which governs the distribution of block max-
ima values of a time series. Next, to ensure that the counterfactual
instance is informative, we constrain the area for modification by
identifying abnormal segments within the original time series, de-
parting from the conventional practice of considering the entire
time series for perturbation when constructing the counterfactual
instance. Specifically, we apply anomaly detection to each segment
of the time series and extract a subset of the segments with the high-
est anomaly scores as possible candidates for replacement. For each
candidate, we employ a conditional diffusion model [ 29] to gener-
ate a new time series segment to replace the identified anomalous
segment. This strategy of constructing a counterfactual instance by
replacing only its anomalous segment helps create counterfactual
instances that are informative, yet close to the original time series.
Our overall proposed framework, named DiffusionCF, encapsu-
lates this comprehensive approach. The primary contributions of
this work can be summarized as follows:
(1)We introduce the novel problem of counterfactual expla-
nation for block maxima forecasting models in time series,
where the counterfactual instances help identify anomalouspatterns in the time series that lead to extreme values in the
forecasted block maxima.
(2)We propose a method to create a counterfactual target for
the block maxima by leveraging the generalized extreme
value (GEV) distribution.
(3)We present DiffusionCF, a framework that balances the trade-
off between generating counterfactual instances that are
informative, yet realistic and close to the original time series.
(4)We perform extensive experiments comparing DiffusionCF
against other baseline methods under different experimental
settings. We demonstrate the versatility and effectiveness of
DiffusionCF across different real-world domains.
2 RELATED WORK
Extreme value theory (EVT) [ 7] offers a well-grounded approach
for modeling and forecasting extreme values in time series. The
theory has recently been incorporated into various deep-learning
formulations. For instance, Nishino et al. [ 24] proposed to predict
the maximum value in a forecast window using GRU with the gen-
eralized extreme value (GEV) distribution. DeepExtrema [ 12] is
another approach that uses deep learning to estimate parameters of
the GEV distribution for block maxima forecasting. The GEV distri-
bution has also been used to impute missing values in time series
for block maxima forecasting task [ 11]. Despite these advances, the
forecasts generated by the black box models can be hard to explain.
Explainable AI, as described by Molnar [ 23], encompasses two
main methodologies: feature attribution methods, such as LIME
[25], SHAP [ 21], and Grad-CAM [ 26], or counterfactual explanation
methods. LIME [ 25], a representative feature attribution method,
explains the predictions of complex machine learning models by
approximating them locally with simpler models such as linear
regression or decision trees. Differently, counterfactual explanation
methods [ 13] focus on identifying the smallest modifications to
the input features that would alter the modelâ€™s decision, offering
insights into how different inputs could lead to different outcomes.
Such methods have been applied to various domains, including
recommender systems [ 6,31], computer vision [ 9,16], and natural
language processing [ 4]. For time series prediction, there are several
ways to generate the counterfactual explanation. First, gradient-
based methods [ 3,18,22,30,32] can be used to create counterfactual
instances by minimizing the loss between the model prediction and
the desired counterfactual target while maintaining the similarity
between the perturbed and original instances. Attribution analy-
sis methods [ 19] leverage domain knowledge to select the input
features and quantify their impact on the counterfactual target.
Our work differs from past research in several key aspects. First,
we emphasize deriving counterfactual explanations for regression
instead of classification tasks, which have been the focus of many
previous studies. Second, we utilize generative AI to produce realis-
tic counterfactual instances, which is a challenge for gradient-based
methods. Third, unlike attribution analysis methods that require
domain knowledge with a limited number of factors, our method
can automatically identify the explanatory factors responsible for
the prediction.
Generative models [ 2,17] have emerged as a key machine learn-
ing paradigm in recent years due to their capacity to synthesize
 
563Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 2: Comparison of counterfactual instances under the informativeness (left), closeness (middle), and realisticness (right)
criteria. The blue dot in each figure denotes the forecasted extreme block maxima by a black box model for the period between
January and September in 2023, while the red dot denotes its corresponding counterfactual target block maxima.
realistic samples replicating the underlying data distribution. Diffu-
sion models [ 14,27,29] have recently emerged as a cutting-edge
approach due to their ability to produce high-quality samples. These
models have found success in diverse applications, including time
series imputation [ 29] and counterfactual explanation for images
[15]. However, their use for counterfactual explanations of extreme
values in time series remains largely unexplored.
3 PRELIMINARIES
3.1 Problem formulation
Consider a time series Z=ğ‘§1ğ‘§2...ğ‘§ğ‘‡, whereğ‘‡denotes the length
of the time series. Assume the time series is partitioned into a
set of distinct time windows, each denoted as ğ‘¤ğ‘¡=[ğ‘¡âˆ’ğ›¼,ğ‘¡+ğ›½],
respectively, where ğ‘¡denotes the current time step. Each window
encompasses a predictor time series, ğ‘‹ğ‘¡=ğ‘§ğ‘¡âˆ’ğ›¼ğ‘§ğ‘¡âˆ’ğ›¼+1...ğ‘§ğ‘¡, where
ğ›¼+1is the length of the predictor window, and a forecast time series,
ğ‘Œğ‘¡=ğ‘§ğ‘¡+1ğ‘§ğ‘¡+2...ğ‘§ğ‘¡+ğ›½, whereğ›½is the length of the forecast window.
The predictor window [ğ‘¡âˆ’ğ›¼,ğ‘¡]contains historical data or other
input variables employed by the forecasting model to generate its
predictions, whereas the forecast window (ğ‘¡,ğ‘¡+ğ›½]contains the
future values to be predicted by the model. Let ğ‘¦ğ‘¡=maxğœâˆˆ1,...,ğ›½ğ‘§ğ‘¡+ğœ
be the block maxima of ğ‘Œğ‘¡,i.e., the maximum value over the forecast
window. Furthermore, we denote ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ=[ğ‘¡âˆ’ğœ+1,ğ‘¡âˆ’ğœ+ğ‘˜]as a sub-
interval within ğ‘¤ğ‘¡such thatğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)=ğ‘§ğ‘¡âˆ’ğœ+1ğ‘§ğ‘¡âˆ’ğœ+2Â·Â·Â·ğ‘§ğ‘¡âˆ’ğœ+ğ‘˜
is the corresponding length- ğ‘˜time series segment of ğ‘‹ğ‘¡, where
ğ‘˜â‰¤ğœâ‰¤ğ›¼+1.
Letğ‘“be a black box model that generates a block maxima fore-
cast, Ë†ğ‘¦ğ‘¡, for any given input ğ‘‹ğ‘¡,i.e.,Ë†ğ‘¦ğ‘¡=ğ‘“(ğ‘‹ğ‘¡). Our primary goal
is to construct a counterfactual predictor, ğ‘‹â€²
ğ‘¡, such thatğ‘“(ğ‘‹â€²
ğ‘¡)â‰ˆË†ğ‘¦â€²
ğ‘¡,
where Ë†ğ‘¦â€²
ğ‘¡â‰ Ë†ğ‘¦ğ‘¡is the desired counterfactual target. For example, if Ë†ğ‘¦ğ‘¡
corresponds to an extreme block maxima generated by the model
ğ‘“, then the counterfactual target Ë†ğ‘¦â€²
ğ‘¡would be a non-extreme block
maxima value. In this paper, we employ the Generalized Extreme
Value (GEV) Distribution to define whether a block maxima value
is extreme or non-extreme. If the forecasted block maxima Ë†ğ‘¦ğ‘¡is
extreme, then a counterfactual instance (ğ‘‹â€²
ğ‘¡,Ë†ğ‘¦â€²
ğ‘¡)is generated. To
do so, the counterfactual predictor, ğ‘‹â€²
ğ‘¡, forğ‘‹ğ‘¡should satisfy the
following three desirable criteria:
(1)Informativeness: ğ‘‹â€²
ğ‘¡is(ğœŒ,ğ‘˜)-informative ifâˆƒğ‘¤ğ‘˜
ğ‘¡âˆ’ğœâŠ‚ğ‘¤ğ‘¡,ğ‘˜â‰ª
ğ›¼,ğœŒ>0 :âˆ¥ğ‘‹â€²
ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ’ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ¥>ğœŒandğ‘‹ğ‘¡\ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)â‰ˆğ‘‹â€²
ğ‘¡\ğ‘‹â€²
ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ), whereğ‘‹ğ‘¡\ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)is the corresponding time
series inğ‘‹ğ‘¡after excluding the segment ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ).
(2)Closeness: ğ‘‹â€²
ğ‘¡isğœ–-close toğ‘‹ğ‘¡ifâˆ¥ğ‘‹ğ‘¡âˆ’ğ‘‹â€²
ğ‘¡âˆ¥<ğœ–forğœ–>0.
(3)Realisticness: ğ‘‹â€²
ğ‘¡isğ›¿-realistic ifğ‘ƒ(ğ‘‹â€²
ğ‘¡)â‰¥ğ›¿, whereğ‘ƒ(Â·)is
the probability that ğ‘‹â€²
ğ‘¡is drawn from the same distribution
as any randomly chosen segment from the time series Z.
Figure 2 illustrates examples of counterfactual instances eval-
uated using the 3 criteria above when applied to a temperature
block maxima forecasting model. In this hypothetical example, as-
sume the forecast model predicts an extreme temperature value,
say, 105â—¦F (depicted as a blue dot), for the forecast window between
January and September in 2023. Suppose a counterfactual instance
with the counterfactual target of around 80â—¦F (shown as a red dot)
is to be constructed. The left panel shows a comparison between
an informative and uninformative counterfactual instance. Even
though both counterfactual instances yield the same counterfactual
target, the informative counterfactual, shown as a red line, is nearly
identical to the original time series, shown as a blue line, except for
the larger deviation in the interval highlighted in green shadow. The
counterfactual is informative as it pinpoints the segment within the
predictor window whose anomalous values lead to the unusually
extreme block maxima forecasted by the model. In contrast, the
uninformative counterfactual, shown by the green line, exhibits
deviations from the original time series throughout the entire time
period, offering little information that could explain the forecasted
extreme block maxima. The middle panel of Figure 2 shows the
distinction between counterfactual instances with high (red line)
and low (green line) closeness in terms of their proximity to the
original time series. Finally, the right panel of Figure 2 illustrates
the difference between realistic (red line) and unrealistic (green
line) counterfactual instances. Specifically, the temperature profile
of the unrealistic counterfactual is counter-intuitive as it exhibits
higher temperatures in winter than in summer.
Unfortunately, balancing the trade-off among the criteria can
be tricky. For example, the following theorem demonstrates the
impossibility of satisfying both informative and closeness criteria
whenğœŒ>ğœ–.
Theorem 1. Letğ‘‹â€²
ğ‘¡be a(ğœŒ,ğ‘˜)-informative counterfactual predic-
tor ofğ‘‹ğ‘¡. IfğœŒ>ğœ–, thenğ‘‹â€²
ğ‘¡must not be ğœ–-close.
 
564KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
Proof. Sinceğ‘‹â€²
ğ‘¡is(ğœŒ,ğ‘˜)-informative, using the additive prop-
erty of vector norm, we have:
âˆ¥ğ‘‹â€²
ğ‘¡âˆ’ğ‘‹ğ‘¡âˆ¥=âˆ¥ğ‘‹â€²
ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ’ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ¥+âˆ¥ğ‘‹â€²
ğ‘¡\ğ‘‹â€²
ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ’ğ‘‹ğ‘¡\ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ¥
â‰¥ğœŒ+âˆ¥ğ‘‹â€²
ğ‘¡\ğ‘‹â€²
ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ’ğ‘‹ğ‘¡\ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ¥
Furthermore, given that ğœŒ>ğœ–, this impliesâˆ¥ğ‘‹â€²
ğ‘¡âˆ’ğ‘‹ğ‘¡âˆ¥>ğœ–, which
meansğ‘‹â€²
ğ‘¡must not be ğœ–-close. â–¡
Similarly, ifğ‘‹â€²
ğ‘¡isğœ–-close andğœŒ>ğœ–, thenâˆ¥ğ‘‹â€²
ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ’ğ‘‹ğ‘¡(ğ‘¤ğ‘˜
ğ‘¡âˆ’ğœ)âˆ¥â‰¤
ğœ–, which means finding a (ğœŒ,ğ‘˜)-informative counterfactual predic-
tor would be impossible. An analogous impossibility theorem for
realisticness is more challenging as it depends on the probability
model of the time series. The difficulty of balancing the 3 criteria
can be illustrated with an example. Let ğ‘‹â€²
ğ‘¡be the counterfactual
predictor obtained via the nearest neighbor approach [ 8]. Sinceğ‘‹â€²
ğ‘¡
is an existing time series, ğ‘ƒ(ğ‘‹â€²
ğ‘¡)must be large. However, it may
not be close unless ğ‘‹â€²
ğ‘¡is in theğœ–-neighborhood of ğ‘‹ğ‘¡. Even if it is
ğœ–-close,ğ‘‹â€²
ğ‘¡may not be(ğœŒ,ğ‘˜)-informative if ğœŒ>ğœ–, as shown above.
3.2 Generalized Extreme Value Distribution
Consider a time series Zof lengthğ‘‡that is partitioned into ğ‘š
sequences, each of length ğ‘›(i.e.,ğ‘›ğ‘š=ğ‘‡). For each sequence, let
ğ‘Œğ‘›be its block maxim. The generalized extreme value (GEV) dis-
tribution [ 7] is often used to describe the probability distribution
governing the block maxima values. The distribution is character-
ized by its shape ( ğœ‰), location (ğœ‡), and scalar ( ğœ) parameters, with
the following cumulative distribution function (CDF):
ğ‘ƒ(ğ‘Œğ‘›â‰¤ğ‘¦)=exp
âˆ’h
1+ğœ‰ğ‘¦âˆ’ğœ‡
ğœiâˆ’1
ğœ‰
, (1)
subject to the constraint:
âˆ€ğ‘¦: 1+ğœ‰ğ‘¦âˆ’ğœ‡
ğœ>0 (2)
Theğ‘thquantile of the distribution, ğ‘¦ğ‘, can be calculated as follows:
ğ‘¦ğ‘=ğœ‡+ğœ
ğœ‰
(âˆ’logğ‘)âˆ’ğœ‰âˆ’1
. (3)
Given the shape, location, and scale parameters of the GEV distri-
bution1, the preceding equation is used to determine the threshold
for extreme block maxima and the counterfactual target for our
proposed framework by setting the appropriate quantile values, ğ‘.
3.3 Deep Block Maxima Forecasting Model
Recent years have witnessed a growing number of research focus-
ing on the use of extreme value theory to enhance the forecasting
of extreme events in time series [ 12,24,34]. For example, Wilson
et al. [ 34] introduced the DeepGPD framework with Generalized
Pareto (GP) distribution to forecast excess values over some pre-
specified threshold while Galib et al. [ 12] and Nishino et al. [ 24]
utilized the Generalized Extreme Value (GEV) distribution for block
maxima forecasting problems. In this work, we choose DeepEx-
trema [ 12] as our black box model due to its superior performance
in block maxima forecasting compared to other baselines. Never-
theless, our framework is model-agnostic, and thus, applicable to
other block maxima forecasting models such as [24].
1These parameters will be estimated by the block maxima forecast model.DeepExtrema employs a combination of LSTM with fully con-
nected layers to estimate the GEV parameters {ğœ‡(ğ‘‹ğ‘¡),ğœ(ğ‘‹ğ‘¡),ğœ‰(ğ‘‹ğ‘¡)}
of an input time series ğ‘‹ğ‘¡in a way that preserves the inequality
constraints given in (2). It then utilizes a fully connected network
(FCN) layer to generate the forecast of block maxima, Ë†ğ‘¦ğ‘¡, from the
estimated GEV parameters. The framework is trained end-to-end
to simultaneously learn both the GEV parameters and its block
maxima forecast by minimizing the following loss function [12]:
ğ¿=ğœ†1Ë†ğ¿ğºğ¸ğ‘‰+(1âˆ’ğœ†1)ğ‘âˆ‘ï¸
ğ‘–=1(ğ‘¦(ğ‘–)
ğ‘¡âˆ’Ë†ğ‘¦(ğ‘–)
ğ‘¡)2, (4)
where Ë†ğ¿ğºğ¸ğ‘‰ is a regularized negative log-likelihood of the GEV
distribution while the second term is the least-square loss between
the forecasted and actual block maxima of the training instances.
Note that the aim of our study is not to assess the predictive per-
formance of DeepExtrema or other similar forecast models. Instead,
it focuses on providing counterfactual explanations to elucidate the
forecasts generated by the model, regardless of their accuracy.
3.4 Counterfactual Explanation
Existing studies on counterfactual explanation have mostly cen-
tered around binary classification problems [ 8,30]. Specifically, let
ğ‘‹ğ‘¡denote the predictor time series and ğ‘denote the predicted class
label of the block maxima in the forecast time series ğ‘Œğ‘¡, according
to a binary classifier ğ‘“,i.e.,ğ‘“(ğ‘‹ğ‘¡)=ğ‘. The primary objective of
counterfactual explanation is to find ğ‘‹â€²
ğ‘¡, a modified counterpart of
ğ‘‹ğ‘¡that will lead to the alternative class label ğ‘â€²by the model ğ‘“,i.e.,
ğ‘“(ğ‘‹â€²
ğ‘¡)=ğ‘â€²â‰ ğ‘. This objective is typically achieved by minimizing
the following loss function [30]:
ğ¿(ğ‘‹ğ‘¡,ğ‘‹â€²
ğ‘¡,ğ‘â€²,ğœ†)=ğœ† ğ‘“(ğ‘‹â€²
ğ‘¡)âˆ’ğ‘â€²2+ğ‘‘(ğ‘‹ğ‘¡,ğ‘‹â€²
ğ‘¡), (5)
whereğœ†is a tuning parameter that balances the components of the
loss function. The first term quantifies the difference between the
modelâ€™s prediction for the modified input ğ‘‹â€²
ğ‘¡and the counterfactual
target classğ‘â€². The second term measures the dissimilarity between
the inputğ‘‹ğ‘¡and its counterfactual ğ‘‹â€²
ğ‘¡.
In this work, we will adapt the approach to generate a coun-
terfactual explanation for extreme block maxima, a continuous
value instead of a class label, forecasted by models such as DeepEx-
trema [12].
4 PROPOSED FRAMEWORK
To generate the counterfactual explanation for block maxima fore-
casting, our DiffusionCF framework performs the following steps.
First, a continuous-valued counterfactual target Ë†ğ‘¦â€²
ğ‘¡associated with
the forecasted block maxima Ë†ğ‘¦ğ‘¡is constructed, as described in Sec-
tion 4.1. Next, the corresponding counterfactual predictors ğ‘‹â€²
ğ‘¡for
the target Ë†ğ‘¦â€²
ğ‘¡is learned using the approach described in Section 4.2.
Finally, in Section 4.3, we demonstrate how to use ğ‘‹â€²
ğ‘¡to explain
the extreme block maxima forecast generated by the model ğ‘“.
4.1 Constructing Counterfactual Target Ë†ğ‘¦â€²
ğ‘¡
Our goal is to generate a counterfactual target that satisfies the
following two conditions. First, given an input ğ‘‹ğ‘¡, if the forecasted
block maxima Ë†ğ‘¦ğ‘¡is an extreme value, then the counterfactual target
must be non-extreme. This requires setting a threshold Ë†ğ‘¦â€²
ğ‘ˆthat
 
565Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 3: A schematic illustration of the proposed DiffusionCF framework.
determines whether the forecasted block maxima is extreme or
non-extreme. Second, as the distribution of block maxima values
is governed by the GEV distribution, the sampled counterfactual
target should be drawn from the same distribution. Adhering to
the GEV distribution enables us to construct the continuous-valued
counterfactual target in a more principled fashion.
To achieve both conditions, we utilize the GEV parameters gen-
erated by the DeepExtrema model2. Specifically, given an input
ğ‘‹ğ‘¡, DeepExtrema will generate both the block maxima forecast Ë†ğ‘¦ğ‘¡
along with parameters of its associated GEV distribution, ğœ‡(ğ‘‹ğ‘¡),
ğœ‰(ğ‘‹ğ‘¡), andğœ(ğ‘‹ğ‘¡). Letğ‘âˆˆ(0,1)be a hyperparameter corresponding
to the desired quantile for defining an extreme block maxima. The
extreme value threshold is computed by setting Ë†ğ‘¦â€²
ğ‘ˆ=ğ‘§ğ‘using the
quantile formula for GEV distribution given by (3). If the predicted
block maxima Ë†ğ‘¦ğ‘¡â‰¥Ë†ğ‘¦â€²
ğ‘ˆ, then Ë†ğ‘¦ğ‘¡is considered an extreme value.
IfË†ğ‘¦ğ‘¡is an extreme block maxima, then a counterfactual target
will be constructed using the forecasted GEV distribution by setting
a quantileğ‘â€²<ğ‘to ensure the counterfactual target is below the
extreme threshold. Analogous to (3), the counterfactual target is
computed as follows:
Ë†ğ‘¦â€²
ğ‘¡=ğœ‡(ğ‘‹ğ‘¡)+ğœ(ğ‘‹ğ‘¡)
ğœ‰(ğ‘‹ğ‘¡)h
(âˆ’logğ‘â€²)âˆ’ğœ‰(ğ‘‹ğ‘¡)âˆ’1i
, (6)
whereğœ‡(ğ‘‹ğ‘¡),ğœ(ğ‘‹ğ‘¡), andğœ‰(ğ‘‹ğ‘¡)are the learned parameters produced
by the DeepExtrema model.
2For other black box models, we can fit a GEV distribution to all the block maxima
values first to learn their GEV parameters and use them to generate the threshold Ë†ğ‘¦â€²
ğ‘ˆ.While our framework can be adapted to explain non-extreme
block maxima forecasts, our current approach is specifically de-
signed to generate counterfactual instances for extreme block max-
ima in this work. This is due to their significant practical impli-
cations and the valuable insights they offer for prevention and
mitigation efforts.
4.2 Constructing Counterfactual Predictor ğ‘‹â€²
ğ‘¡
After identifying the counterfactual target Ë†ğ‘¦â€²
ğ‘¡, the next step is to
construct its corresponding counterfactual predictor, ğ‘‹â€²
ğ‘¡, such that
ğ‘“(ğ‘‹â€²
ğ‘¡)â‰ˆ Ë†ğ‘¦â€²
ğ‘¡while ensuring that ğ‘‹â€²
ğ‘¡is informative, realistic, and
close to the original time series ğ‘‹ğ‘¡. Unlike previous approaches (as
examples in Section 5.2) that often resorted to searching or random
(gradient-based) perturbations of the input data to produce the
counterfactual predictors, our proposed DiffusionCF framework is
designed to balance the tradeoff between informativeness, closeness,
and realisticness of the counterfactual explanation. There are 3 main
components in DiffusionCF, as shown in Figure 3.
4.2.1 Constructor for the Counterfactual Predictor. The key assump-
tion guiding our approach is that when the black box model predicts
an exceptionally extreme block maxima value, the time series seg-
ment(s) within ğ‘‹ğ‘¡most likely contributing to the extreme forecast
are those exhibiting a high level of anomaly. Our confidence in this
assumption stems from the fact that the black box model derives
its prediction solely from ğ‘‹ğ‘¡. If the segments in ğ‘‹ğ‘¡align with typi-
cal observations in the time series, it would be improbable for the
forecast model to produce an extreme value output. While there
may exist other scenarios that could lead to the forecasted extreme
 
566KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
block maxima, our assumption provides a highly plausible and com-
putationally feasible way to pinpoint the explanatory factor behind
the modelâ€™s forecast.
Based on this assumption, our constructor for the counterfactual
predictor needs to identify the most anomaly segment(s) in the time
series and replace them with more typical patterns, while leaving
the rest of the time series intact. This strategy enables our approach
to create more informative yet realistic counterfactual instances.
The counterfactual predictors are constructed as follows:
(1)Extraction of Time Series Segments: Given a time series
ğ‘‹ğ‘¡of lengthğ›¼+1for the predictor window [ğ‘¡âˆ’ğ›¼,ğ‘¡], our algo-
rithm first extracts all the time series segmentsn
ğ‘‹ğ‘–
ğ‘¡oğ›¼âˆ’ğ‘‘+2
ğ‘–=1
withinğ‘‹ğ‘¡by using a sliding window of fixed length, ğ‘‘.
(2)Detection of Anomalous Segments: For each extracted
segment,ğ‘‹ğ‘–
ğ‘¡, the algorithm computes the probability ğ‘ğ‘‹ğ‘–
ğ‘¡that conforms to the underlying distribution of the time
series data. The probability is estimated using a detector
functionğ‘”(to be described in Section 4.2.2), where ğ‘ğ‘‹ğ‘–
ğ‘¡=
ğ‘”(ğ‘‹ğ‘–
ğ‘¡).
(3)Counterfactual Generation: Starting from the most anoma-
lous segment, ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ =arg minğ‘ âˆˆ{ğ‘‹ğ‘–
ğ‘¡}ğ‘ğ‘ , the algorithm
would remove this segment from ğ‘‹ğ‘¡and replaces it with
a more â€œtypical" segment. This is achieved by using a sam-
plerâ„(see Section 4.2.3) to generate a set of ğ‘šcandidate
replacements, denoted as ğ¶ğ‘š
ğ‘¡=â„(ğ‘‹ğ‘¡,ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡), where each
candidateğ‘‹ğ‘ 
ğ‘¡âˆˆğ¶ğ‘š
ğ‘¡has a higher probability to be drawn
from the time series than ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ ,i.e.,ğ‘ğ‘‹ğ‘ 
ğ‘¡>ğ‘ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡. The
best candidate ğ‘‹min
ğ‘¡is then selected based on its distance to
the counterfactual target Ë†ğ‘¦â€²
ğ‘¡:
ğ‘‹min
ğ‘¡=arg min
ğ‘‹ğ‘ 
ğ‘¡âˆˆğ¶ğ‘š
ğ‘¡ğ‘“
(ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡)âŠ•ğ‘‹ğ‘ 
ğ‘¡
âˆ’Ë†ğ‘¦â€²
ğ‘¡1(7)
Here(ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡)âŠ•ğ‘‹ğ‘ 
ğ‘¡denotes the resulting times series
after replacing the anomalous segment ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ with the can-
didateğ‘‹ğ‘ 
ğ‘¡. The counterfactual predictor ğ‘‹â€²
ğ‘¡is obtained by
replacing the anomalous segment with the best candidate,
i.e.,ğ‘‹â€²
ğ‘¡=(ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡)âŠ•ğ‘‹min
ğ‘¡. As the forecast for the counter-
factual predictor ğ‘‹â€²
ğ‘¡generated from the anomalous segment,
ğ‘“(ğ‘‹â€²
ğ‘¡)may not always be close to the counterfactual target,
Ë†ğ‘¦â€²
ğ‘¡, this step is repeated using the next ğ¾most anomalous
segment(s) until the following conditions are met:
Counterfactual Target Condition:
ğ‘“(ğ‘‹â€²
ğ‘¡)<Ë†ğ‘¦â€²
ğ‘ˆandğ‘“(ğ‘‹â€²
ğ‘¡)âˆˆ[ Ë†ğ‘¦â€²
ğ‘¡âˆ’ğœ–,Ë†ğ‘¦â€²
ğ‘¡+ğœ–] (8)
For efficiency reasons, this process of removal and replace-
ment of the anomalous segment is repeated for at most ğ¾
times. If no viable counterfactual predictor ğ‘‹â€²
ğ‘¡is found, the
algorithm will return the best ğ‘‹â€²
ğ‘¡it has discovered.
4.2.2 Detector ğ‘”.Our DiffusionCF framework uses a variational
auto-encoder (VAE) for anomaly detection [ 2] as its detector ğ‘”. TheVAE for anomaly detection is trained to learn the underlying distri-
bution of all time series segments {ğ‘‹ğ‘–
ğ‘¡}ğ›¼âˆ’ğ‘‘+2
ğ‘–=1of lengthğ‘‘extracted
from the predictor time series ğ‘‹ğ‘¡of the training data. It comprises
two main componentsâ€”an encoder ğ‘‰encoder and a decoder ğ‘‰decoder .
The encoder ğ‘‰encoder takes eachğ‘‹ğ‘–
ğ‘¡as input and maps it to a latent
Gaussian distribution with mean vector ğœ‡and isotropic covariance
ğœ2I. The Gaussian distribution is used to draw ğ¿samples from the
â„-dimensional latent space, denoted as {ğ¼ğ‘–âˆˆğ‘…â„|ğ‘–=1,2,...,ğ¿}. The
decoderğ‘‰decoder would attempt to reconstruct the original time
series segment, ğ‘‹ğ‘–
ğ‘¡, from each sampled latent instance. The VAE for
anomaly detection is trained to minimize the average reconstruc-
tion error of the time series segments.
During the detection step, for each input ğ‘‹ğ‘–
ğ‘¡, the VAE would
compute the parameters of its latent distribution, which are used to
calculate the probability ğ‘ğ‘‹ğ‘–
ğ‘¡. The probability determines whether
ğ‘‹ğ‘–
ğ‘¡is anomalous. The higher the probability, the more likely ğ‘‹ğ‘–
ğ‘¡
belongs to the same distribution as the majority of the time series
segments, so the less anomalous it is. Details of the Detector module
are shown in Figure 3.
4.2.3 Sampler â„.Our framework uses a conditional diffusion model
as its sampler â„to construct a candidate replacement, ğ‘‹ğ‘ 
ğ‘¡, for
an anomalous segment, ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ . Specifically, it will first mask the
anomalous segment from the input time series ğ‘‹ğ‘¡and provide the
masked input to CSDI [ 29], a conditional diffusion model that is
adept at imputing missing segments of a time series (see Appen-
dixA.1 for details). We chose CSDI because it can leverage the
unmasked portion of the time series (i.e., ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ ) to create a
new candidate ğ‘‹ğ‘ 
ğ‘¡for imputing the masked segment. This allows
DiffusionCF to produce imputed segments that are consistent with
the rest of the time series, leading to more realistic counterfac-
tual predictors. Nevertheless, our framework is flexible and can
incorporate other samplers such as DDPM [14] and VAE [17].
4.3 Using Counterfactual Predictor to Explain
Block Maxima Forecast
The counterfactual predictor, ğ‘‹â€²
ğ‘¡, generated by DiffusionCF can be
used to elucidate the specific segment within the input time series
ğ‘‹ğ‘¡that largely contributes to the forecasted extreme block maxima.
LetÎ”ğ‘‹=ğ‘‹ğ‘¡âˆ’ğ‘‹â€²
ğ‘¡=(Î”ğ‘§ğ‘¡âˆ’ğ›¼,Î”ğ‘§ğ‘¡âˆ’ğ›¼+1,...,Î”ğ‘§ğ‘¡)be a vector of abso-
lute difference between the counterfactual and original predictor,
i.e.,Î”ğ‘§ğ‘–=|ğ‘‹ğ‘¡,ğ‘–âˆ’ğ‘‹â€²
ğ‘¡,ğ‘–|. The time steps within the predictor window,
[ğ‘¡âˆ’ğ›¼,ğ‘¡]can be sorted in decreasing magnitude of their Î”ğ‘§ğ‘–. If
|Î”ğ‘§ğ‘–|exceeds some threshold, then the segment can be considered
a notable contributor to the extreme block maxima forecast by the
black box model ğ‘“.
5 PERFORMANCE EVALUATION
5.1 Datasets
We use the following datasets for our experiments: (1) Global
Surface Summary of the Day (GSOD), a dataset that contains
daily observations of precipitation and temperature from 79 weather
stations in the Mobile/Pensacola area in the southwestern United
States. The dataset spans a time period from August 1, 1929, to
 
567Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Summary of datasets, where |ğ‘‹ğ‘¡|and|ğ‘Œğ‘¡|denote the
length of the predictor and forecast windows, respectively.
Dataset#Training
samples#Validation
samples# Testing
samples|ğ‘‹ğ‘¡| |ğ‘Œğ‘¡|
GSOD 14798 548 511 24 6
S&P 500 7022 251 251 25 5
Dodgers sensor 846 248 241 40 8
November 22, 2023. (2) S&P 500, a dataset comprises of daily closing
prices of the S&P-500 index from January 13, 1994 to January 12,
2024. (3) Dodgers loop sensor, a dataset containing traffic volume
data from April 10, 2005, to October 2, 2005, at the Glendale ramp
of the 101 North freeway in Los Angeles, near the Dodgers stadium.
Summary statistics of the datasets are given in Table 1 while their
pre-processing steps are described in Appendix A.3.
5.2 Baseline Methods
We compare the performance of DiffusionCF against the following
baseline methods:
â€¢BaseNN, a baseline used in [ 32] to identify the nearest-
unlike neighbor ğ‘‹â€²
ğ‘¡from the training set, whose true block
maxima value aligns with the counterfactual target.
â€¢ğœ”-CF[30] learnsğ‘‹â€²
ğ‘¡by minimizing the loss between ğ‘“(ğ‘‹â€²
ğ‘¡)
and Ë†ğ‘¦â€²
ğ‘¡, as well as the distance between ğ‘‹ğ‘¡andğ‘‹â€²
ğ‘¡.
â€¢Native guide (NG-CF) [8] constructs ğ‘‹â€²
ğ‘¡by identifying a
nearest-unlike neighbor to ğ‘‹ğ‘¡and modifying it to produce a
model forecast close to the counterfactual target.
â€¢ForecastCF [32] employs a mask objective function to learn
ğ‘‹â€²
ğ‘¡, aiming at minimizing the loss between ğ‘“(ğ‘‹â€²
ğ‘¡)and Ë†ğ‘¦â€²
ğ‘¡.
â€¢SPARCE [18] generates a counterfactual explanation for
time series by using a generative adversarial network (GAN).
As some baseline methods were developed for classification tasks,
they were adapted to generate counterfactual instances for block
maxima forecasts. Details are given in Appendix A.4.
5.3 Evaluation Metrics
As noted in Section 3.1, a good counterfactual predictor should be
realistic, informative, and close to the original time series. Let ğ‘‹ğ‘¡=
ğ‘§ğ‘¡âˆ’ğ›¼ğ‘§ğ‘¡âˆ’ğ›¼+1...ğ‘§ğ‘¡be the original predictors and ğ‘‹â€²
ğ‘¡=ğ‘§â€²
ğ‘¡âˆ’ğ›¼ğ‘§â€²
ğ‘¡âˆ’ğ›¼+1...ğ‘§â€²
ğ‘¡
be the counterfactual predictors. We employ the following metrics
to assess the performance of the various methods:
â€¢Informativeness. As noted in Section 3.1, an informative
counterfactual should modify only a small segment of ğ‘‹ğ‘¡,
keeping the rest of the predictor time series intact. We use a
combination of sparsity andconsecutiveness metrics to deter-
mine whether a counterfactual predictor ğ‘‹â€²
ğ‘¡is informative.
Sparsity(ğ‘‹â€²
ğ‘¡)=1
ğ›¼+1count{ğ‘–:ğ‘§ğ‘¡âˆ’ğ‘–â‰ ğ‘§â€²
ğ‘¡âˆ’ğ‘–,ğ‘–=0,Â·Â·Â·,ğ›¼}(9)
A lower value of sparsity means fewer modifications to ğ‘‹ğ‘¡.
Next, we construct the following sequence of binary values:
Î”(ğ‘§ğ‘¡âˆ’ğ‘–,ğ‘§â€²
ğ‘¡âˆ’ğ‘–)=(
1,if|ğ‘§ğ‘¡âˆ’ğ‘–âˆ’ğ‘§â€²
ğ‘¡âˆ’ğ‘–|>ğœŒ,
0,otherwise,(10)whereğœŒ>0is a threshold. Let ğ¿maxrepresent the maxi-
mum length of consecutive 1â€™s in the binary sequence. The
consecutiveness metric is defined as
Consecutiveness(ğ‘‹â€²
ğ‘¡)=ğ¿maxÃğ›¼
ğ‘–=1Î”(ğ‘§ğ‘¡âˆ’ğ‘–,ğ‘§â€²
ğ‘¡âˆ’ğ‘–). (11)
A higher consecutiveness implies the notable difference be-
tweenğ‘‹ğ‘¡andğ‘‹â€²
ğ‘¡is mostly concentrated in a local segment of
the time series. Thus, an informative counterfactual should
have low sparsity but high consecutiveness values.
â€¢Closeness. The proximity metric below is used to determine
the extent to which ğ‘‹ğ‘¡is close toğ‘‹â€²
ğ‘¡:
Proximity(ğ‘‹ğ‘¡,ğ‘‹â€²
ğ‘¡)=1
ğ›¼+1ğ›¼âˆ‘ï¸
ğ‘–=0|ğ‘§ğ‘¡âˆ’ğ‘–âˆ’ğ‘§â€²
ğ‘¡âˆ’ğ‘–|. (12)
The lower the proximity, the closer the counterfactual pre-
dictor is to the original predictor.
â€¢Realisticness. The negative likelihood function ofğ‘‹â€²
ğ‘¡is used
to determine whether a counterfactual instance is realistic,
NLL(ğ‘‹â€²
ğ‘¡)=âˆ’logğ‘ğ‘‹â€²
ğ‘¡. Here,ğ‘ğ‘‹â€²
ğ‘¡represents the probability
assigned to ğ‘‹â€²
ğ‘¡by detector ğ‘”. The lower the NLL, the more
realistic the counterfactual predictor.
Finally, we use the precision metric to ascertain how well ğ‘‹â€²
ğ‘¡will
ensure that ğ‘“(ğ‘‹â€²
ğ‘¡)is classified as a non-extreme block maxima.
5.4 Experimental Results
5.4.1 Performance Comparison. Experiments results are summa-
rized in Table 2 based on the setup discussed in Appendix A.5.
Figure 4 provides an illustrated example of the counterfactual in-
stances found by the different methods. In general, DiffusionCF
generates the most informative counterfactual instances in all 4
datasets, achieving the best sparsity and consecutiveness scores. It
also appears among the top 2 approaches with the best proximity
and precision in at least 3 of the 4 datasets. These results suggest
that DiffusionCF generally demonstrate superior performance in
explaining the extreme block maxima forecast generated by the
black box model compared to other baselines. Though its NLL score
is slightly worse than BaseNN and NG-CF, this is not surprising as
the latter two approaches create their counterfactuals by sampling
from the training instances.
Specifically, (1) For BaseNN, its superior performance in terms of
NLL is attributable to its strategy of using training instances as ğ‘‹â€²
ğ‘¡.
However, since BaseNN does not optimize for closeness between
ğ‘‹ğ‘¡andğ‘‹â€²
ğ‘¡, it performs poorly in terms of proximity andsparsity
metrics, as illustrated in Figure 4(left). BaseNN also has lower pre-
cision because the forecasted counterfactual block maxima ğ‘“(ğ‘‹â€²
ğ‘¡)
may not be consistent with the desired counterfactual target since
the nearest-unlike neighbor is chosen based on the true block max-
ima value instead of its forecasted block maxima. (2) For ğœ”-CF and
ForecastCF, their precision is perfect in all 4 datasets, which is not
surprising as they both employ optimization-based approaches to
ensureğ‘“(ğ‘‹â€²
ğ‘¡)is close to the counterfactual target, Ë†ğ‘¦â€²
ğ‘¡. Nevertheless,
ğœ”-CF has a better proximity score compared to ForecastCF. This
is because the loss function used by ğœ”-CF includes the distance
betweenğ‘‹ğ‘¡andğ‘‹â€²
ğ‘¡to encourage smaller modifications to the input.
In contrast, ForecastCF does not consider such a factor, allowing it
to make larger modifications. Hence, its proximity score is higher.
 
568KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
Table 2: Evaluation of performance on 4 real-world datasets conducted for scenarios where the forecast Ë†ğ‘¦ğ‘¡is extreme, and the
counterfactual target Ë†ğ‘¦â€²
ğ‘¡is non-extreme. Within the comparative results, redentries indicate the top-performing result, while
blue entries signify the second-best performance for each metric.
GSOD-precipitation GSOD-temperature
Method Spars.â†“Consecu.â†‘Proxi.â†“ NLLâ†“ Prec.â†‘Spars.â†“Consecu.â†‘Proxi.â†“ NLLâ†“ Prec.â†‘
BaseNN1.00
(Â±0.00)0.86
(Â±0.18)1.12
(Â±0.19)1.18
(Â±0.15)0.271.00
(Â±0.00)0.86
(Â±0.17)1.13
(Â±0.21)1.08
(Â±0.36)0.48
ğœ”-CF1.00
(Â±0.00)0.91
(Â±0.13)0.17
(Â±0.07)1.07
(Â±0.18)1.001.00
(Â±0.00)0.65
(Â±0.18)0.18
(Â±0.08)1.12
(Â±0.07)1.00
NG-CF0.83
(Â±0.26)0.67
(Â±0.26)0.57
(Â±0.24)0.99
(Â±0.17)0.380.94
(Â±0.17)0.65
(Â±0.25)0.56
(Â±0.22)1.09
(Â±0.08)0.56
ForecastCF1.00
(Â±0.00)0.92
(Â±0.18)0.45
(Â±0.17)1.13
(Â±0.37)1.001.00
(Â±0.00)0.90
(Â±0.15)0.35
(Â±0.13)1.21
(Â±0.09)1.00
SPARCE1.00
(Â±0.00)0.72
(Â±0.22)1.10
(Â±0.21)1.13
(Â±0.17)0.681.00
(Â±0.00)0.74
(Â±0.21)1.10
(Â±0.22)1.18
(Â±0.07)0.87
DiffusionCF0.25
(Â±0.00)0.93
(Â±0.13)0.16
(Â±0.07)1.06
(Â±0.22)0.980.12
(Â±0.00)0.96
(Â±0.13)0.08
(Â±0.04)1.20
(Â±0.08)1.00
S&P 500 Dodgers loop sensor
Method Spars.â†“Consecu.â†‘Proxi.â†“ NLLâ†“ Prec.â†‘Spars.â†“Consecu.â†‘Proxi.â†“ NLLâ†“ Prec.â†‘
BaseNN1.00
(Â±0.00)0.88
(Â±0.17)1.43
(Â±0.40)0.84
(Â±0.49)0.981.00
(Â±0.01)0.75
(Â±0.22)1.12
(Â±0.39)0.36
(Â±0.17)0.66
ğœ”-CF1.00
(Â±0.00)0.46
(Â±0.17)0.59
(Â±0.11)1.13
(Â±0.26)1.001.00
(Â±0.00)0.76
(Â±0.25)0.02
(Â±0.01)0.45
(Â±0.11)1.00
NG-CF1.00
(Â±0.00)0.82
(Â±0.21)0.92
(Â±0.23)0.96
(Â±0.29)0.990.99
(Â±0.03)0.50
(Â±0.17)0.52
(Â±0.18)0.56
(Â±0.24)0.61
ForecastCF1.00
(Â±0.00)0.85
(Â±0.19)0.81
(Â±0.16)1.10
(Â±0.23)1.001.00
(Â±0.00)0.56
(Â±0.24)0.09
(Â±0.04)0.44
(Â±0.12)1.00
SPARCE0.98
(Â±0.00)0.75
(Â±0.22)1.43
(Â±0.38)1.71
(Â±0.20)1.000.97
(Â±0.00)0.66
(Â±0.21)1.14
(Â±0.35)0.46
(Â±0.12)1.00
DiffusionCF0.40
(Â±0.00)0.96
(Â±0.10)0.51
(Â±0.16)1.68
(Â±0.19)0.970.12
(Â±0.00)0.81
(Â±0.18)0.05
(Â±0.03)0.43
(Â±0.13)1.00
Figure 4: A comparative study of ğ‘‹â€²
ğ‘¡, generated by BaseNN, ForecastCF, ğœ”-CF, NGCF, SPARCE, and DiffusionCF, when applied to
precipitation forecasting between 2021-06 and 2023-09 for a weather station in Pensacola, Florida. The blue dot represents the
forecasted block maxima, Ë†ğ‘¦ğ‘¡, while the red dot represents the counterfactual target, Ë†ğ‘¦â€²
ğ‘¡.
This difference in proximity betweenğœ”-CF and ForecastCF can
be clearly seen in the middle two plots of Figure 4. Furthermore,
in terms of informativeness, both approaches perform poorly in
terms of their sparsity metric, though ForecastCF demonstrates a
relatively higher consecutiveness score compared to ğœ”-CF. (3) For
SPARCE, which utilizes GAN for optimization, it outperforms ğœ”-CF
and ForecastCF in terms of sparsity on the S&P 500 and Dodgers
loop sensor datasets. However, its sparsity is similar to other base-
lines and worse than NG-CF on the GSOD datasets. SPARCE also
struggles in terms of informativeness, closeness, and realisticness
metrics. This underscores the difficulty of using GAN to optimizemultiple objectives simultaneously. ForecastCF, ğœ”-CF, and SPARCE
all fall short compared to BaseNN in terms of NLL, suggesting the
counterfactual predictors produced by these optimization-based
methods are less realistic. (4) For NG-CF, which is a combination of
neighbor searching and perturbation-based methods, can balance
the trade-off in terms of proximity, precision, and NLL but falls short
in terms of producing informative explanation due to the generally
lowconsecutiveness scores. Finally, (5) for DiffusionCF, it excels in
terms of sparsity, consecutiveness, proximity, and precision. This
advantage is crucial as it renders the counterfactual explanation
more informative, as shown in Figure 4(right).
 
569Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Ablation study results on GSOD-precipitation data.
Spars.â†“Consecu.â†‘Proxi.â†“ NLLâ†“ Prec.â†‘
CSDI+30.12
(Â±0.00)0.97
(Â±0.13)0.06
(Â±0.06)1.14
(Â±0.21)0.87
CSDI+60.25
(Â±0.00)0.93
(Â±0.13)0.16
(Â±0.07)1.06
(Â±0.22)0.98
CSDI+90.38
(Â±0.00)0.90
(Â±0.17)0.29
(Â±0.13)1.11
(Â±0.28)1.00
CSDI+120.50
(Â±0.00)0.91
(Â±0.16)0.36
(Â±0.10)1.08
(Â±0.24)1.00
VAE+30.12
(Â±0.00)0.98
(Â±0.15)0.02
(Â±0.01)1.13
(Â±0.19)0.36
VAE+60.25
(Â±0.00)0.90
(Â±0.17)0.07
(Â±0.03)1.08
(Â±0.21)0.42
VAE+90.38
(Â±0.00)0.84
(Â±0.17)0.13
(Â±0.04)1.04
(Â±0.21)0.56
VAE+120.50
(Â±0.00)0.87
(Â±0.17)0.25
(Â±0.05)1.02
(Â±0.16)0.49
5.4.2 Ablation study of DiffusionCF. We conduct an ablation study
forDiffusionCF by varying the window size ğ‘‘for segment extrac-
tion, as described in Section 4.2.2 and employing VAE as our alter-
native sampling technique. A detailed breakdown of the results is
given in Table 3. Our key conclusions are as follows:
â€¢Increasing the window size ğ‘‘from 3 to 12 generally degrades
the performance of DiffusionCF in terms of sparsity, prox-
imity, and consecutiveness, while enhancing its realisticness
andprecision. This is because a larger ğ‘‘increases the num-
ber of time steps available for adding perturbation, allowing
greater deviation from the original time series, thus reducing
sparsity and increasing their dissimilarity. Larger window
size also reduces the percentage of consecutive time steps
that were perturbed, as shown in Table 3, with the excep-
tion ofğ‘‘=12. Nevertheless, it also gives more flexibility
forDiffusionCF to construct counterfactual instances that
are close to the desired target, thus enhancing its precision.
Interestingly, the NLL values for VAE exhibit a decreasing
trend with larger ğ‘‘, warranting future investigation.
â€¢In terms of informativeness andprecision, DiffusionCF based
on CSDI outperforms the one based on VAE. However, the
latter is superior in proximity andrealisticness. As shown in
Table 3, the CSDI-based method slightly surpasses the VAE-
based method in consecutiveness. The CSDI-based methodâ€™s
advantage in precision is evident as the diffusion model used
for imputation generates samples that, while deviating more
from the original time series compared to the VAE-based
method, still remain realistic, thereby contributing to more
accurate counterfactual targets. However, this increased de-
viation results in worse performance in proximity andNLL
compared to the VAE-based method.
5.4.3 Case study of DiffusionCF. Our case study on the Dodgers
loop sensor dataset focuses on the specific example when there are
two consecutive game days at the Dodgers stadium, on May 31 and
June 1, 2005. Figure 5 depicts the average 3-hourly traffic flow at a
ramp near the stadium from Friday, May 27 to Wednesday, June 1,
Figure 5: Observed average traffic flow (May 27-Jun 01) vs.
counterfactual instance found by CSDI-based DiffusionCF.
2005. In this example, the model utilizes traffic flow data from the
preceding five days to forecast the maximum traffic flow for June 1st.
The resulting block maxima value, approximately 1300, is deemed
extreme, surpassing 80% of the average 3-hourly traffic flows within
the dataset. This extreme value is attributed to a baseball game held
at the stadium on that day. The figure also depicts a counterfactual
target, with the block maxima value set around 1150, representing
the typical traffic flow if the baseball match had not occurred. Based
on the counterfactual target, a counterfactual instance is generated
by the CSDI-based DiffusionCF model.
The counterfactual instance generated by DiffusionCF adjusts the
traffic pattern for Tuesday, May 31st, depicting a decrease in traffic
flow for that day, indicative of the absence of a baseball game at the
stadium. The absence of a game on Tuesday suggests the likelihood
of no game the following day. In essence, the increase in traffic
flow on Tuesday attributed to the baseball game could elucidate the
extreme block maxima forecasted for Wednesday. The result shown
in Figure 5 thus underscores DiffusionCF â€™s capability in identifying
anomalies (i.e., pattern of elevated traffic flow on game days) and
modify them towards a normative state (i.e., pattern of reduced
traffic flow) as its counterfactual instance.
6 CONCLUSIONS AND FUTURE WORK
This paper introduces the novel problem of counterfactual explana-
tion for block maxima forecasting models in time series. We propose
a methodology for creating counterfactual block maxima and intro-
duce the DiffusionCF framework to balance the trade-off between
generating counterfactual explanations that are informative, close
to the original time series, and realistic. Experimental results show
that DiffusionCF generates better counterfactual instances com-
pared to other baselines. Nevertheless, the current framework has
two potential limitations. First, it considers only univariate time
series. For future endeavors, we plan to extend our methodology to
the multivariate case. Second, DiffusionCF is biased towards con-
structing its counterfactual predictor by modifying only one of
the anomalous segments in the time series. We plan to investigate
approaches that could modify multiple segments instead.
7 ACKNOWLEDGMENT
This research is supported by the U.S. National Science Foundation
under grant IIS-2006633. Any use of trade, firm, or product names
is for descriptive purposes only and does not imply endorsement
by the U.S. Government.
 
570KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
REFERENCES
[1]Reduan Achtibat, Maximilian Dreyer, Ilona Eisenbraun, Sebastian Bosse, Thomas
Wiegand, Wojciech Samek, and Sebastian Lapuschkin. 2023. From attribution
maps to human-understandable explanations through concept relevance propa-
gation. Nature Machine Intelligence 5, 9 (2023), 1006â€“1019.
[2]Jinwon An and Sungzoon Cho. 2015. Variational autoencoder based anomaly
detection using reconstruction probability. Special Lecture on IE 2, 1 (2015), 1â€“18.
[3]Emre Ates, Burak Aksar, Vitus J Leung, and Ayse K Coskun. 2021. Counterfactual
explanations for multivariate time series. In 2021 International Conference on
Applied Artificial Intelligence (ICAPAI). IEEE, 1â€“8.
[4]Lorenzo Betti, Carlo Abrate, Francesco Bonchi, and Andreas Kaltenbrunner. 2023.
Relevance-based infilling for natural language counterfactuals. In Proceedings of
the 32nd ACM International Conference on Information and Knowledge Manage-
ment. 88â€“98.
[5]Dieter Brughmans, Pieter Leyman, and David Martens. 2023. Nice: an algorithm
for nearest instance counterfactual explanations. Data Mining and Knowledge
Discovery (2023), 1â€“39.
[6]Ziheng Chen, Fabrizio Silvestri, Jia Wang, Yongfeng Zhang, and Gabriele Tolomei.
2023. The dark side of explanations: poisoning recommender systems with
counterfactual examples. In Proceedings of the 46th International ACM SIGIR
conference on Research and Development in Information Retrieval. 2426â€“2430.
[7]Stuart Coles, Joanna Bawa, Lesley Trenner, and Pat Dorazio. 2001. An introduction
to statistical modeling of extreme values. Vol. 208. Springer.
[8]Eoin Delaney, Derek Greene, and Mark T Keane. 2021. Instance-based counter-
factual explanations for time series classification. In International Conference on
Case-Based Reasoning. Springer, 32â€“47.
[9]Bhat Dittakavi, Bharathi Callepalli, Aleti Vardhan, Sai Vikas Desai, and Vineeth N
Balasubramanian. 2024. CARE: counterfactual-based algorithmic recourse for
explainable pose correction. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision. 4902â€“4911.
[10] Gil Fidel, Ron Bitton, and Asaf Shabtai. 2020. When explainability meets adver-
sarial learning: detecting adversarial examples using shap signatures. In 2020
International Joint Conference on Neural Networks (IJCNN). IEEE, 1â€“8.
[11] Asadullah Hill Galib, Andrew McDonald, Pang-Ning Tan, and Lifeng Luo. 2023.
Self-recover: forecasting block maxima in time series from predictors with dis-
parate temporal coverage using self-supervised learning. In Proceedings of the
Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI 2023).
[12] Asadullah Hill Galib, Andrew McDonald, Tyler Wilson, Lifeng Luo, and Pang-
Ning Tan. 2022. DeepExtrema: a deep learning approach for forecasting block
maxima in time series data. In Proceedings of the Thirty-First International Joint
Conference on Artificial Intelligence, IJCAI (2022).
[13] Riccardo Guidotti. 2022. Counterfactual explanations and how to find them:
literature review and benchmarking. Data Mining and Knowledge Discovery
(2022), 1â€“55.
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851.
[15] Guillaume Jeanneret, LoÃ¯c Simon, and FrÃ©dÃ©ric Jurie. 2022. Diffusion models for
counterfactual explanations. In Proceedings of the Asian Conference on Computer
Vision. 858â€“876.
[16] Guillaume Jeanneret, LoÃ¯c Simon, and FrÃ©dÃ©ric Jurie. 2023. Adversarial counterfac-
tual visual explanations. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 16425â€“16435.
[17] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
ArXiv Preprint ArXiv:1312.6114 (2013).
[18] Jana Lang, Martin A Giese, Winfried Ilg, and Sebastian Otte. 2023. Generating
sparse counterfactual explanations for multivariate time series. In International
Conference on Artificial Neural Networks. Springer, 180â€“193.
[19] Nicholas J Leach, Antje Weisheimer, Myles R Allen, and Tim Palmer. 2021.
Forecast-based attribution of a winter heatwave within the limit of predictability.
Proceedings of the National Academy of Sciences 118, 49 (2021), e2112087118.
[20] Peiyu Li, SoukaÃ¯na Filali Boubrahimi, and Shah Muhammad Hamdi. 2022. Motif-
guided time series counterfactual explanations. In International Conference on
Pattern Recognition. Springer, 203â€“215.
[21] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in Neural Information Processing Systems 30 (2017).
[22] Han Meng, Christian Wagner, and Isaac Triguero. 2023. Explaining time se-
ries classifiers through meaningful perturbation and optimisation. Information
Sciences (2023), 119334.
[23] Christoph Molnar. 2020. Interpretable machine learning. Lulu.com.
[24] Kaneharu Nishino, Ken Ueno, and Ryusei Shingaki. 2022. Deep learning-based
block maxima distribution predictor for extreme value prediction. In 8th SIGKDD
International Workshop on Mining and Learning from Time Series â€“ Deep Forecast-
ing: Models, Interpretability, and Applications.
[25] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should I
trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
1135â€“1144.[26] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: visual explanations from
deep networks via gradient-based localization. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision. 618â€“626.
[27] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
Ermon, and Ben Poole. 2020. Score-based generative modeling through stochastic
differential equations. ArXiv Preprint ArXiv:2011.13456 (2020).
[28] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. 2019. One pixel
attack for fooling deep neural networks. IEEE Transactions on Evolutionary
Computation 23, 5 (2019), 828â€“841.
[29] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. CSDI:
conditional score-based diffusion models for probabilistic time series imputation.
Advances in Neural Information Processing Systems 34 (2021), 24804â€“24816.
[30] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual
explanations without opening the black box: automated decisions and the GDPR.
Harv. JL & Tech. 31 (2017), 841.
[31] Xiangmeng Wang, Qian Li, Dianer Yu, Qing Li, and Guandong Xu. 2024. Coun-
terfactual explanation for fairness in recommendation. ACM Transactions on
Information Systems 42, 4 (2024), 1â€“30.
[32] Zhendong Wang, Ioanna Miliou, Isak Samsten, and Panagiotis Papapetrou.
2023. Counterfactual explanations for time series forecasting. ArXiv Preprint
ArXiv:2310.08137 (2023).
[33] James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda
ViÃ©gas, and Jimbo Wilson. 2019. The what-if tool: interactive probing of machine
learning models. IEEE Transactions on Visualization and Computer Graphics 26, 1
(2019), 56â€“65.
[34] Tyler Wilson, Pang-Ning Tan, and Lifeng Luo. 2022. DeepGPD: a deep learning
approach for modeling geospatio-temporal extreme events. Proceedings of the
AAAI Conference on Artificial Intelligence 36, 4 (Jun. 2022), 4245â€“4253.
A APPENDICES
A.1 Details of CSDI model
CSDI [ 29] is a conditional score-based diffusion model for imput-
ing missing values in time series by leveraging the available ob-
served values. Specifically, CSDI is designed to estimate the true
conditional data distribution ğ‘(ğ‘¥ğ‘¡ğ‘|ğ‘¥ğ‘ğ‘œ)via the model distribution
ğ‘ğœƒ(ğ‘¥ğ‘¡ğ‘|ğ‘¥ğ‘ğ‘œ), whereğ‘¥ğ‘¡ğ‘denotes the missing values to be imputed
andğ‘¥ğ‘ğ‘œdenotes the observed values.
Diffusion models such as DDPM [ 14] typically follow a two-step
training process. First, during the forward process, the model starts
from an initial input ğ‘¥ğ‘¡ğ‘
0and iteratively perturbs the time series
values toğ‘¥ğ‘¡ğ‘
1,ğ‘¥ğ‘¡ğ‘
2,Â·Â·Â·by adding random noise until it converges to
ğ‘¥ğ‘¡ğ‘
ğ‘‡, whereğ‘¥ğ‘¡ğ‘
ğ‘‡is of known, simple distribution. Next, during the
reverse process, a neural network is trained to convert ğ‘¥ğ‘¡ğ‘
ğ‘‡back to
the original values ğ‘¥ğ‘¡ğ‘
0. CSDI modifies the reverse process of DDPM
with a conditional model defined as follows:
ğ‘ğœƒ(ğ‘¥ğ‘¡ğ‘
0:ğ‘‡|ğ‘¥ğ‘ğ‘œ
0)=ğ‘(ğ‘¥ğ‘¡ğ‘
ğ‘‡)ğ‘‡Ã–
ğ‘¡=1ğ‘ğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡âˆ’1|ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¥ğ‘ğ‘œ
0), ğ‘¥ğ‘¡ğ‘
ğ‘‡âˆ¼N( 0,I),
whereğ‘ğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡âˆ’1|ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¥ğ‘ğ‘œ
0)=N ğ‘¥ğ‘¡ğ‘
ğ‘¡âˆ’1;ğœ‡ğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¡|ğ‘¥ğ‘ğ‘œ
0),ğœğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¡|ğ‘¥ğ‘ğ‘œ
0)I.
Here,ğœ‡ğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¡|ğ‘¥ğ‘ğ‘œ
0)=1
ğ›¼ğ‘¡ ğ‘¥ğ‘¡âˆ’ğ›½ğ‘¡âˆš1âˆ’ğ›¼ğ‘¡ğœ–ğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¡|ğ‘¥ğ‘ğ‘œ
0), whereğ›½ğ‘¡is a
small positive constant representing the noise level, ğ›¼ğ‘¡=Ãğ‘¡
ğ‘–=1Ë†ğ›¼ğ‘–,
Ë†ğ›¼ğ‘¡=1âˆ’ğ›½ğ‘¡, andğœ–ğœƒ:(Xğ‘¡ğ‘Ã—R|Xğ‘ğ‘œ) â†’ Xğ‘¡ğ‘is a conditional
denoising function. Furthermore, ğœğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¡|ğ‘¥ğ‘ğ‘œ
0)=Ëœğ›½1/2
ğ‘¡, where
Ëœğ›½ğ‘¡=(1âˆ’ğ›¼ğ‘¡âˆ’1
1âˆ’ğ›¼ğ‘¡ğ›½ğ‘¡ifğ‘¡>1,
ğ›½1 ifğ‘¡=1.(13)
Givenğ‘¥ğ‘ğ‘œ
0andğ‘¥ğ‘¡ğ‘
0, noisy samples for diffusion step ğ‘¡is given by:
ğ‘¥ğ‘¡ğ‘
ğ‘¡=âˆšğ›¼ğ‘¡ğ‘¥ğ‘¡ğ‘
0+(1âˆ’ğ›¼ğ‘¡)ğœ–, whereğœ–is the added noise. During the
reverse process, ğœ–ğœƒis estimated by minimizing the loss function
min
ğœƒL(ğœƒ):=min
ğœƒğ¸ğ‘¥0âˆ¼ğ‘(ğ‘¥0),ğœ–âˆ¼N( 0,I),ğ‘¡âˆ¥(ğœ–âˆ’ğœ–ğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¡|ğ‘¥ğ‘ğ‘œ
0))âˆ¥2
2,(14)
 
571Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
whereğ‘(ğ‘¥0)is the data distribution of ğ‘¥0. During training, the
choice of imputation target is important, either by random strategy,
historical strategy, mix strategy, or test pattern strategy. See [ 29] for
details. Once trained, ğ‘¥ğ‘¡ğ‘
0can be sampled from ğ‘ğœƒ(ğ‘¥ğ‘¡ğ‘
ğ‘¡âˆ’1|ğ‘¥ğ‘¡ğ‘
ğ‘¡,ğ‘¥ğ‘ğ‘œ
0).
A.2 The DiffusionCF algorithm
The pseudocode of the DiffusionCF algorithm is summarized in
Algorithm 1. In this context, ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ denotes the remaining part
ofğ‘‹ğ‘¡after excluding ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ . The expression(ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡)âŠ•ğ‘‹ğ‘ 
ğ‘¡
represents the time series obtained after replacing ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ inğ‘‹ğ‘¡
withğ‘‹ğ‘ 
ğ‘¡.
Algorithm 1 DiffusionCF
Input: Time series ğ‘=ğ‘‹ğ‘¡âˆªğ‘Œğ‘¡, quantilesğ‘andğ‘â€², sliding window length
ğ‘‘, sampling epochs ğ¿, searching epochs ğ¾
Output: Counterfactual instances (ğ‘‹â€²
ğ‘¡,Ë†ğ‘¦â€²
ğ‘¡)
1:train, validation, test â†dataset.
2:fâ†forecast(train, validation).
3:gâ†detector(train, validation).
4:hâ†sampler(train, validation).
5:probs = []; Cm
t= [].
6:foreachğ‘=ğ‘‹ğ‘¡âˆªğ‘Œğ‘¡in test do
7: Ë†ğ‘¦ğ‘¡,Ë†ğ‘¦â€²
ğ‘ˆ= f(ğ‘‹ğ‘¡).
8: ifË†ğ‘¦ğ‘¡>Ë†ğ‘¦â€²
ğ‘ˆthen
9: Ë†ğ‘¦â€²
ğ‘¡â†ğœ‡(ğ‘‹ğ‘¡)+ğœ(ğ‘‹ğ‘¡)
ğœ‰(ğ‘‹ğ‘¡)
(âˆ’logğ‘â€²)âˆ’ğœ‰(ğ‘‹ğ‘¡)âˆ’1
10: end if
11: foreach of{ğ‘‹ğ‘–
ğ‘¡}ğ›¼âˆ’ğ‘‘+2
ğ‘–=1within sliding windows of length ğ‘‘onğ‘‹ğ‘¡
do
12: probs.append(g( ğ‘‹ğ‘–
ğ‘¡)).
13: end for
14:ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡â†arg minğ‘‹ğ‘–
ğ‘¡probs.
15: removeğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ fromğ‘‹ğ‘¡.
16: while searching epoch< ğ¾and conditions ğ‘“(ğ‘‹â€²
ğ‘¡) <
Ë†ğ‘¦â€²
ğ‘ˆandğ‘“(ğ‘‹â€²
ğ‘¡)âˆˆ[ Ë†ğ‘¦â€²
ğ‘¡âˆ’ğœ–,Ë†ğ‘¦â€²
ğ‘¡+ğœ–]does not meet do
17: while sampling epoch <ğ¿do
18: Cm
t.append(h(ğ‘‹ğ‘¡,ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡ )).
19: end while
20:ğ‘‹min
ğ‘¡=arg minğ‘‹ğ‘ 
ğ‘¡âˆˆğ¶ğ‘š
ğ‘¡âˆ¥ğ‘“ (ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡)âŠ•ğ‘‹ğ‘ 
ğ‘¡âˆ’Ë†ğ‘¦â€²
ğ‘¡âˆ¥1.
21:ğ‘‹â€²
ğ‘¡â†(ğ‘‹ğ‘¡\ğ‘‹ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡)âŠ•ğ‘‹min
ğ‘¡.
22: end while
23:end for
A.3 Data Preprocessing
Global Surface Summary of the Day (GSOD)3. Prior to analysis, the
GSOD dataset undergoes a series of pre-processing steps. Initially,
the daily weather data are converted into monthly aggregates by
averaging each monthâ€™s daily recordings, excluding any missing or
invalid entries (e.g., missing or invalid daily precipitation (PRCP)
was recorded as 99.99while that of daily temperature (TEMP) was
recorded as 999.99). To mitigate seasonal influences, standardiza-
tion is applied on a monthly basis. The datasets are then divided
temporally into training, validation, and test sets: data before Janu-
ary 1, 2022, forms the training set; data between January 1, 2022,
3https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-
day?pageNum=1and January 1, 2023, forms the validation set; data after January 1,
2023, forms the test set. Further, sliding windows of 30time steps
(months) are applied to the series. Within each sliding window, the
first 24time steps (months) are used as predictors and the last 6
time steps (months) are used as forecasts.
S&P 5004. Its pre-processing involves two primary steps. First, to
eliminate long-term trends, the differencing method is applied. This
technique computes the difference between consecutive data points,
effectively detrending the time series. Following differencing, the
entire time series is standardized. Data with the end time step before
January 13, 2023, forms the training set; data between January 13,
2023, and January 13, 2024, forms the validation set; data after
January 13, 2024, forms the test set. Sliding windows of 30time
steps (days) are applied to the series. Within each sliding window,
the first 25time steps (days) are used as predictors and the last 5
time steps (days) are used as forecasts.
Dodgers loop sensor5. The pre-processing of this dataset involves
several steps. First, approximately 5.76% of the dataset contains
missing values, which are addressed using the Forward Fill tech-
nique. Here, each missing value is replaced with the most recent
observed data point. Second, traffic data are aggregated in 3-hour in-
tervals, with each interval represented as a single time step, marked
by the final timestamp of each 3-hour period. Third, the entire time
series is standardized, normalizing the data to ensure uniformity in
scale. Data before August 1, 2005 forms the training set; data be-
tween August 1, 2005, and September 1, 2005, forms the validation
set; data after September 1, 2005, forms the test set. Additionally,
sliding windows of 48time steps (equivalent to 144hours or 6days)
are applied to the series. Within each sliding window, the first 40
time steps (120 hours or 5days) are used as predictors and the last
8time steps (24 hours or 1day) are used as forecasts.
A.4 Details for Baseline Algorithms
A.4.1 BaseNN. BaseNN, which is used as a baseline in the study
by Wang et al. [ 32], identifies the closest instance in the training set
that aligns with the desired target outcome. Specifically, BaseNN
selects from the training set the predictor time series ğ‘‹ğ‘¡whose
subsequent block maxima ğ‘¦ğ‘¡is closest to the desired target Ë†ğ‘¦â€²
ğ‘¡.
Once identified, it is used as the counterfactual predictor ğ‘‹â€²
ğ‘¡.
A.4.2ğœ”-CF. Although it was originally developed for classifica-
tion tasks,ğœ”-CF [ 30] can be adapted to a regression scenario. The
counterfactual predictor ğ‘‹â€²
ğ‘¡can be learned through an optimization
process defined as
ğ‘‹â€²
ğ‘¡=arg min
ğ‘‹âˆ—
ğ‘¡max
ğœ†ğœ†ğ‘“(ğ‘‹âˆ—
ğ‘¡)âˆ’Ë†ğ‘¦â€²
ğ‘¡+ğ‘‘(ğ‘‹ğ‘¡,ğ‘‹âˆ—
ğ‘¡),
whereğœ†is a tuning parameter that balances the two components
of the objective function and ğ‘‘(ğ‘‹ğ‘¡,ğ‘‹âˆ—
ğ‘¡)is the Manhattan distance.
The learning of ğœ”-CFis based on the Adam optimizer.
A.4.3 Native guide (NG-CF). Native guide (NG-CF) [ 8] encom-
passes a two-step process. Initially, it identifies the nearest unlikely
neighbor of original instance ğ‘‹ğ‘¡. Then, by leveraging the Dynamic
Time Warping (DTW) algorithm, NG-CF modifies the identified
4https://finance.yahoo.com/quote/%5EGSPC/history?period1=1673481600&period2=
1705017600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true
5https://archive.ics.uci.edu/dataset/157/dodgers+loop+sensor
 
572KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
neighbor of ğ‘‹ğ‘¡to construct ğ‘‹â€²
ğ‘¡, which is engineered to yield a
predictionğ‘“(ğ‘‹â€²
ğ‘¡)closely aligned with the desired outcome Ë†ğ‘¦â€²
ğ‘¡.
A.4.4 ForecastCF. ForecastCF [ 32], which was initially designed
to explain trend forecasting in time series, can be adapted for ex-
plaining block maxima prediction by replacing the time series in
the forecast window of its original formulation with block maxima
value. Specifically, in the real world, assume that the forecasted
block maxima Ë†ğ‘¦ğ‘¡corresponding to a ğ‘‹ğ‘¡is extreme. In the counter-
factual world, if Ë†ğ‘¦ğ‘¡is expected to be non-extreme instead, denoted
byË†ğ‘¦â€²
ğ‘¡, how should the ğ‘‹ğ‘¡be changed to ğ‘‹â€²
ğ‘¡? Given a forecasting
modelğ‘“, takeğ›¼=Ë†ğ‘¦â€²
ğ‘¡âˆ’ğœ–andğ›½=Ë†ğ‘¦â€²
ğ‘¡+ğœ–as the lower and upper
boundaries where ğœ–is the tolerance, ForecastCF [32] learnsğ‘‹â€²
ğ‘¡s.t.
ğ‘“(ğ‘‹â€²
ğ‘¡)â‰ˆË†ğ‘¦â€²
ğ‘¡:ğ‘…|ğ‘‹â€²
ğ‘¡|â†’ğ‘…1by minimizing the loss function
ğ¿=(
0,ifâˆ¥ğ‘“(ğ‘‹â€²
ğ‘¡)âˆ’Ë†ğ‘¦â€²
ğ‘¡âˆ¥â‰¤ğœ–,
âˆ¥ğ‘“(ğ‘‹â€²
ğ‘¡)âˆ’ğ›¼âˆ¥+âˆ¥ğ›½âˆ’ğ‘“(ğ‘‹â€²
ğ‘¡)âˆ¥,otherwise,(15)
The learning of ForecastCF is based on the Adam optimizer.
A.4.5 SPARCE. SPARCE [ 18] is a GAN-based method used to gen-
erate sparse counterfactual explanations for time series, employing
both a discriminator and a generator. The discriminator attempts
to distinguish between a real instance ğ‘‹ğ‘¡from its counterfactual in-
stanceğ‘‹â€²
ğ‘¡while the generator attempts to generate counterfactual
instances that could fool the discriminator into misclassifying them
as real instances. In our implementation, the generator initially
receives a real instance ğ‘‹ğ‘¡linked to an extreme forecasted block
maxima Ë†ğ‘¦ğ‘¡and then generates a counterfactual instance ğ‘‹â€²
ğ‘¡that
yields the counterfactual target Ë†ğ‘¦â€²
ğ‘¡, which is non-extreme. This setup
enables the generator to learn how to modify (or perturb) its initial
inputğ‘‹ğ‘¡to construct ğ‘‹â€²
ğ‘¡, which yields a non-extreme block max-
ima Ë†ğ‘¦â€²
ğ‘¡, achieving the desired target. The generator-discriminator
architecture is jointly trained to minimize the difference between
Ë†ğ‘¦â€²
ğ‘¡andğ‘“(ğ‘‹â€²
ğ‘¡), which is also constrained to guarantee the sparsity
ofğ‘‹â€²
ğ‘¡.
A.5 Experimental Setup
For a fair comparison, all the experiments were conducted using the
same trained DeepExtrema model on each dataset to generate block
maxima forecasts. For the predictor and forecast windows set for
the DeepExtrema model, their lengths were chosen as the ones that
yielded the best performance or were used in previous works. The
RMSE of the trained DeepExtrema model evaluated on the test set
varies between 0.40 and 0.75 on the given datasets. For each block
maxima forecast, a counterfactual target is then constructed to be
utilized by both DiffusionCF and all the baselines. To define the
counterfactual target, we first identify a counterfactual threshold
Ë†ğ‘¦â€²
ğ‘ˆto determine whether the forecasted block maxima is extreme.
Towards this end, we set the quantile ğ‘of the GEV distributionin such a way that around 10% (8.8%âˆ’11.1%) of the forecasted
block maxima will be considered as extreme values. Based on this
threshold, a counterfactual target Ë†ğ‘¦â€²
ğ‘¡is subsequently constructed
using the quantile ğ‘â€²=ğ‘âˆ’0.2. It is worth noting, however, that our
framework operates independently of the DeepExtrema modelâ€™s
performance.
On the GSOD precipitation dataset, the DeepExtrema model is
configured with the following hyperparameters: batch size is set to
128, learning rate to 0.005, dimension of hidden layer to 32, number
of hidden layers to 2,ğœ†1to 0.8, andğœ†2to 0.5. For ForecastCF and
ğœ”âˆ’CF, when Ë†ğ‘¦ğ‘¡is extreme and Ë†ğ‘¦â€²
ğ‘¡is not, the maximum iterations
for learning is set to 300. For ForecastCF, tolerance is set to 0.05. For
SPARCE, we use the default parameters. Considering the balance
between time and accuracy for DiffusionCF, the sampling window
length is searched in[3,6,9,12]. For each of them, the searching
epochsğ¾(as shown in line 16 in Algorithm 1) is set to 5. In each of
the search windows, the sampling epochs ğ¿(as shown in line 17 in
Algorithm 1) is set to 300. This involves determining the least likely
sub-time series ğ‘‹ğ‘¡ofsampling window length inğ‘‹ğ‘¡, removing it,
and then sampling a new ğ‘‹â€²
ğ‘¡of the same length from the learned
latent distribution for imputation. The parameter balancing the two
components of the loss function in VAE is set to 10âˆ’8
On the GSOD temperature dataset, the DeepExtrema model is
configured with the following hyperparameters: batch size is set to
128,learning rate to 0.001, dimension of hidden layer to 10, number of
hidden layers to 2,ğœ†1to 0.1, andğœ†2to 0.5. The parameter balancing
the two components of the loss function in the VAE is set to 10âˆ’9,
and for anomaly detection within the VAE, it is set to 1.0. Other
parameters are identical to those used for the GSOD precipitation
data.
On the S&P 500 dataset, the DeepExtrema model is configured
with the following hyperparameters: batch size is set to 128, learning
rateto 0.0001, dimension of hidden layer to 8, number of hidden layers
to 2,ğœ†1to 0.2, andğœ†2to 0.5. The sampling window length is searched
in[5,10,15]. Other parameters are identical to those used for the
GSOD temperature data.
On the Dodgers loop sensor traffic forecasting dataset, the Deep-
Extrema model is configured with the following hyperparameters:
batch size is set to 128, learning rate to 0.0005, dimension of hidden
layer to 16, number of hidden layers to 2,ğœ†1to 0.1, andğœ†2to 1.5. The
sampling window length is searched in[5,10,15,20]. Additionally,
the parameter balancing the two components of the loss function
in the VAE for anomaly detection is set to 1.0 when its sampling
size is 40. For smaller sampling sizes, specifically 5, 10, 15, or 20, the
balancing parameter is adjusted to 5.0. This variation in settings
is aimed at optimizing the modelâ€™s performance for different data
granularity and anomaly detection contexts. Other parameters are
identical to those used for the GSOD temperature data.
 
573