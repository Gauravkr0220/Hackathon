Tensorized Unaligned Multi-view Clustering with Multi-scale
Representation Learning
Jintian Ji
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
22120385@bjtu.edu.cnSonghe Feng
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
shfeng@bjtu.edu.cnYidong Liâˆ—
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
ydli@bjtu.edu.cn
ABSTRACT
The Unaligned Multi-view Clustering (UMC) problem is currently
receiving widespread attention, focusing on clustering unaligned
multi-view data generated in real-world applications. Although
some algorithms have emerged to address this issue, there still exist
the following drawbacks: 1) The fully unknown correspondence of
samples across views can significantly limit the exploration of con-
sistent clustering structure. 2) The fixed representation space makes
it difficult to mine the comprehensive information in the original
data. 3) Unbiased tensor rank approximation is desired to capture
the high-order correlation among different views. To address these
issues, we proposed a novel UMC framework termed Tensorized
Unaligned Multi-view Clustering with Multi-scale Representation
Learning (TUMCR). Specifically, TUMCR designs a multi-scale rep-
resentation learning and alignment framework, which constructs
multi-scale representation spaces to comprehensively explore the
unknown correspondence across views. Then, a tensorial multi-
scale fusion module is proposed to fuse multi-scale representations
and explore the high-order correlation hidden in different views,
which utilizes the Enhanced Tensor Rank (ETR) to learn the low-
rank structure. Furthermore, TUMCR is solved by an efficient algo-
rithm with good convergence. Extensive experiments on different
types of datasets demonstrate the effectiveness and superiority of
our TUMCR compared with state-of-the-art methods. Our code is
publicly available at: https://github.com/jijintian/TUMCR.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning approaches;
â€¢Information systems â†’Clustering.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671689KEYWORDS
Unaligned Multi-view Clustering; Low-rank Tensor Regularization;
Multi-scale Representations.
ACM Reference Format:
Jintian Ji, Songhe Feng, and Yidong Li. 2024. Tensorized Unaligned Multi-
view Clustering with Multi-scale Representation Learning . In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671689
1 INTRODUCTION
Clustering is a basic task in unsupervised learning and data min-
ing, which aims to partition samples into certain categories. Many
algorithms such as ğ‘˜-means [ 25] and spectral clustering [ 31] have
been able to achieve satisfactory performances. However, these
methods are limited to single-view data. With the development of
data science, multi-view data is rapidly becoming a mainstream
focus in data analysis, which portrays the samples from different
perspectives. For example, human fingerprints can be recorded by
different sensors, and sculptures can be photographed from mul-
tiple views. So, multi-view clustering [ 3] aims to utilize the rich
information hidden in different views to address the shortcomings
of traditional clustering and improve clustering performance.
Many multi-view clustering methods have emerged, such as
kernel-based multi-view clustering algorithms [ 17,20,35], graph-
based multi-view clustering algorithms [ 8,16,41,44], and subspace-
based multi-view clustering algorithms[ 4,7,47], etc. These algo-
rithms can achieve satisfactory performance in some areas. How-
ever, they are carried out under ideal multi-view data (e.g., Fig. 1.
(a)). In real-world applications, due to the instability of the data
acquisition and transmission process, multi-view data often face
different degrees of problems, such as the missing problem and
the unaligned problem. The missing problem is also called the In-
complete Multi-view Clustering problem (IMC) [ 15,21,22,42,48],
which has attracted a lot of attention and brought many innova-
tive solutions. In this paper, we focus on the unaligned problem,
which is also called the Unaligned Multi-view Clustering (UMC)
problem. In most of the current studies, the UMC problem is usually
set up in two different ways and thus divided into two types of
 
1246
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
(a)
 (b)
 (c)
Figure 1: The motivation of this paper. In the figure, different
colors denote different samples and different shapes indicate
different categories, the dotted lines represent the desired
correspondence. (a) Traditional multi-view data. (b) Partially
view-aligned problem: only a portion of data is with the
known correspondence; (c) Fully unaligned problem: The
correspondence of all samples between views is unknown
due to data collection and transmission errors.
problems. The first type is called the partially view-aligned prob-
lem [ 9,37,39,45](e.g., Fig. 1. (b)), which assumes that a portion
of the samples in the multi-view data have known inter-view cor-
respondence and the rest are unknown. These problems typically
utilize known correspondence as supervised signals to predict the
correspondence of unobserved samples. For example, Zhang et al.
[45] utilize the existing inter-view constraints to obtain the mutual
information among views. Huang et al. [9] take advantage of the
aligned data to learn a neural network to establish the category-
level correspondence of the unaligned data. Yang et al. [39] propose
a noise-robust contrastive loss to ensure that aligned samples in
the representation space are pulled together while the unaligned
samples are pushed away from each other.
The second type is termed the fully unaligned problem [ 14,36,
40] (e.g., Fig. 1. (c)), which is more challenging to solve as compared
with the previous type. In this setting, the inter-view correspon-
dence of all samples is unknown. In order to solve such problems,
the hidden information in the multi-view data is usually exploited
to establish connections between views. For instance, Yu et al. [40]
explore the graph-structure consistency for all views based on the
non-negative matrix factorization framework. Lin et al. [14] use
a view-specific coupling matrix to establish the inter-view corre-
spondence in the self-representation space and adopt the low-rank
tensor learning to explore the high-order correlation among views.
Wen et al. [36] propose a novel parameter-free graph clustering
framework to refine cross-view correspondence. Although someprogress has been made by these algorithms, there are still two
main issues that need to be addressed. The first issue is that all these
methods explore the inter-view correspondence under a fixed latent
dimension, which affects the expressive ability of the model and the
establishment of intrinsic correspondence between views. The sec-
ond issue lies in devising an effective low-rank tensor constraint to
capture the high-order correlation among views in existing view fu-
sion frameworks since the Tensor Nuclear Norm (TNN) constraint
has trouble in suppressing noisy information in the representation
tensor.
Being aware of the above issues, we proposed a novel framework
to tackle the fully unaligned problem, which is termed Tensorized
Unaligned Multi-view Clustering with Multi-scale Representation
Learning (TUMCR) (Fig. 2). Specifically, TUMCR obtains multi-
scale representation matrices under the Matrix Factorization (MF)
framework, and the cross-view mapping matrices can be well estab-
lished in multi-scale spaces. Then we propose a tensorial multi-scale
fusion module to integrate representation matrices in multi-scale
spaces and explore high-order correlation among views. Specifically,
the Enhanced Tensor Rank is adopted for the low-rank structural
constraint, which leads to more compact clustering representations.
We summarize the main contributions as follows:
(1)Different from most of the existing methods, TUMCR ex-
plores the inter-view correspondence in multi-scale spaces,
which can adequately integrate the information of unaligned
multi-view data from different views, thus improving the
accuracy of the alignment and clustering.
(2)To integrate multi-scale representations, TUMCR proposes
a tensorial multi-scale fusion module, which fuses the in-
formation of different scales by assigning adaptive weights.
Then a low-rank tensor learning framework with Enhanced
Tensor Rank is adopted to explore the high-order correlation
among views.
(3)We develop an alternative optimization algorithm with good
convergence. To validate the effectiveness, we conduct ex-
tensive experiments on various datasets. The results demon-
strate the efficiency and excellent performance of TUMCR.
The remainder of this paper is organized as follows. Section 2
presents notations and preliminaries. Section 3 details our proposed
method and framework. In Section 4, we present optimization,
complexity analysis, convergence analysis, and complexity analysis.
The experimental results and analysis are shown in Section 5. At
last, Section 6 concludes this paper.
2 NOTATIONS AND PRELIMINARIES
2.1 Notations
In this paper, bold-lowercase ğ’™and lowercase letters ğ‘¥represent
a vector and a scalar, respectively. Bold-uppercase Xdenotes the
matrix. Iğ‘›is theğ‘›Ã—ğ‘›identity matrix. Calligraphy letter denotes the
tensorXâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3.Xğ‘˜is theğ‘˜-th frontal slice ofX.Xğ‘“means
the Fast Fourier Transformation (FFT) along the third dimension
of tensorX. And some tensor-related operations[ 13] are described
as follows. We first define two three-order tensors AâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3
andBâˆˆRğ‘›2Ã—ğ‘›4Ã—ğ‘›3.
â€¢Transposition of tensor: Ağ‘‡âˆˆRğ‘›2Ã—ğ‘›1Ã—ğ‘›3.
 
1247Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Figure 2: The framework of our proposed TUMCR. The method consists primarily of two parts: the multi-scale alignment
module and the tensorial multi-scale fusion model. The multi-scale alignment module learns the inter-view correspondence
in the multi-scale spaces. The tensorial multi-scale fusion model integrates the information captured in different scales and
adopts a low-rank tensor learning framework to enhance the consistency among different views.
â€¢Cyclic expansion of the tensor ğ‘ğ‘–ğ‘Ÿğ‘(A)âˆˆ Rğ‘›1ğ‘›3Ã—ğ‘›2ğ‘›3:
ğ‘ğ‘–ğ‘Ÿğ‘(A)=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°A1Ağ‘›3Â·Â·Â· A2
A2A1Â·Â·Â· A3
............
Ağ‘›3Ağ‘›3âˆ’1Â·Â·Â· A1ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£». (1)
â€¢Tensor unfolding and folding:
ğ‘¢ğ‘›ğ‘“ğ‘œğ‘™ğ‘‘(A)=
A1,A2,Â·Â·Â·,Ağ‘›3ğ‘‡âˆˆRğ‘›1ğ‘›3Ã—ğ‘›2.
A=ğ‘“ğ‘œğ‘™ğ‘‘(ğ‘¢ğ‘›ğ‘“ğ‘œğ‘™ğ‘‘(A)).(2)
â€¢t-productAâˆ—Bâˆˆ Rğ‘›1Ã—ğ‘›4Ã—ğ‘›3:
Aâˆ—B =ğ‘“ğ‘œğ‘™ğ‘‘(ğ‘ğ‘–ğ‘Ÿğ‘(A)Â·ğ‘¢ğ‘›ğ‘“ğ‘œğ‘™ğ‘‘(B)). (3)
â€¢Orthogonal tensor: The tensor Ais orthogonal ifAğ‘‡âˆ—A=
Aâˆ—Ağ‘‡=I.
Based on the above tensor operations, the definition of the tensor-
Singular Value Decomposition (t-SVD) and Tensor Nuclear Norm
(TNN) are defined as follows:
Definition 1. (t-SVD)[ 12] Given tensorAâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3, then the
t-SVD ofAis:
A=Uâˆ—Sâˆ—Vğ‘‡(4)
whereUâˆˆRğ‘›1Ã—ğ‘›1Ã—ğ‘›3andVâˆˆRğ‘›2Ã—ğ‘›2Ã—ğ‘›3are orthogonal tensors,
SâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3is ağ‘“-diagonal tensor.
Definition 2. (TNN)[ 23,24,49] Given a tensorAâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3,
then the tensor nuclear norm is defined as:
âˆ¥Aâˆ¥âŠ›=1
ğ‘›3ğ‘›3âˆ‘ï¸
ğ‘˜=1â„âˆ‘ï¸
ğ‘–=1Sğ‘˜
ğ‘“(ğ‘–,ğ‘–) (5)whereâ„=min(ğ‘›1,ğ‘›2)andSğ‘“is from the t-SVD of A=Uâˆ—Sâˆ—Vğ‘‡
in Fourier domain.
Multi-view dataset with ğ‘›samples and ğ‘šviews is denoted as
{Xğ‘£}ğ‘š
ğ‘£=1, where Xğ‘£âˆˆRğ‘›Ã—ğ‘‘ğ‘£,ğ‘‘ğ‘£is the feature dimension of ğ‘£-th
view.ğ‘is the cluster number of this dataset.
2.2 Multi-view Matrix Factorization Clustering
The Matrix Factorization (MF) technique [ 2,5,26] is an effective
method for clustering, especially for high-dimensional datasets. In
single-view clustering problem, it aims to decompose the original
feature Xğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’âˆˆRğ‘›Ã—ğ‘‘into two parts, denoted as representation
matrix Zğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’âˆˆRğ‘›Ã—ğ‘and base matrix Hğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’âˆˆRğ‘Ã—ğ‘‘.
min
{Zğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’,Hğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’}L(Xğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’,Zğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’ Hğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’), (6)
whereL(Â·) is the loss function that is usually measured by the
Frobenius norm [ 18]. To improve the distinguishability of the repre-
sentation matrix Zğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’ , Non-negative Matrix Factorization (NMF)
[1,5] regularizes Zğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’ andHğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’ to be non-negative, [ 6,32]
further impose orthogonality constraints on the base matrix or
representation matrix.
Extending Eq. (6) to the multi-view clustering problem, we can
obtain the general formula of MF-based multi-view clustering meth-
ods:
min
{Zğ‘£,Hğ‘£}ğ‘š
ğ‘£=1L({Xğ‘£,Zğ‘£Hğ‘£}ğ‘š
ğ‘£=1)+ğœ†R({Zğ‘£}ğ‘š
ğ‘£=1,{Hğ‘£}ğ‘š
ğ‘£=1)(7)
whereğœ†is a trade-off parameter, {Hğ‘£}ğ‘š
ğ‘£=1represents a group of
base matrices,R(Â·) denotes some constraint terms on {Zğ‘£}ğ‘š
ğ‘£=1and
{Hğ‘£}ğ‘š
ğ‘£=1. For example, [ 32] imposes orthogonality constraints to
improve the discriminability of representation matrices. The graph
 
1248KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
Laplacian regularization is adopted in [ 46] to preserve the geomet-
ric structure. After learning the representation matrix Zğ‘£of each
view, the consensus representation matrix Zğ‘¢ğ‘›ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ is obtained by
fusion strategies (i.e., linear weighted fusion method [ 34] or spectral
rotation fusion method [ 43]), which is then fed into ğ‘˜-means or
spectral clustering to acquire the final clustering results.
3 PROPOSE METHOD
3.1 Multi-scale Representation Learning and
Alignment
Existing unaligned multi-view clustering methods usually first map
the original features of various views into a unified space with a
group of base matrices, then explore the inter-view correspondence
in the fixed space. Thus the alignment is extremely reliant on the
representation matrices of the mapping space. However, in multi-
view data, data matrices from multiple sources usually belong to
specific latent dimensions, which means that just one feature space
alone makes it difficult to accurately characterize all views. Most
works ignore this shortcoming and simply equate it to the number
of clusters to avoid selecting the dimensions. Others tackle the
problem by introducing a hyperparameter, which increases the
computational burden during the optimization stage. To this end,
our proposed TUMCR tries to map the original features of different
views into various spaces and explore the cross-view mapping
matrices with these multi-scale representations. Specifically, we
chooseğ‘Ÿgroups of basis matrices {Hğ‘£ğ‘}ğ‘£=1,...,ğ‘š
ğ‘=1,...,ğ‘Ÿto map the original
feature into the ğ‘Ÿspaces with dimensions {ğ‘‘1,Â·Â·Â·,ğ‘‘ğ‘Ÿ}, so we can
obtain the multi-scale representations by solving the following
problem:
min
{Zğ‘£ğ‘,Hğ‘£ğ‘}ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Xğ‘£âˆ’Zğ‘£
ğ‘Hğ‘£
ğ‘2
ğ¹
ğ‘ .ğ‘¡.âˆ€ğ‘£,ğ‘,Zğ‘£
ğ‘ğ‘‡Zğ‘£
ğ‘=Iğ‘‘ğ‘,(8)
where Zğ‘£ğ‘âˆˆRğ‘›Ã—ğ‘‘ğ‘denotes the ğ‘£-th view representation matrix
inğ‘-th space. Then we need to establish the cross-view mapping
matrices. Since the complexity of searching for mapping matrices
for any two views is extremely high, a common operation is to
assign a central view to align the others. To avoid introducing
redundant complexity, an arbitrary view is randomly selected as
the central view in our proposed TUMCR, denoted as Zğ‘ğ‘œğ‘Ÿğ‘’ğ‘. So we
only need to build ğ‘šcross-view mapping matrices {Pğ‘£}ğ‘š
ğ‘£=1, when
ğ‘£=ğ‘ğ‘œğ‘Ÿğ‘’,Pğ‘ğ‘œğ‘Ÿğ‘’=Iğ‘›. Then the multi-scale alignment module can
be formalized as:
min
{Pğ‘£}ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Zğ‘ğ‘œğ‘Ÿğ‘’
ğ‘âˆ’Pğ‘£Zğ‘£
ğ‘2
ğ¹
ğ‘ .ğ‘¡.âˆ€ğ‘£,Pğ‘£âˆˆ{0,1}ğ‘›Ã—ğ‘›,Pğ‘£1=1,Pğ‘£ğ‘‡1=1,(9)
The problem in Eq. (9) is a classical NP-hard problem, so we adopt
the spectral relaxation [ 30] to simplify the problem, and taking the
multi-scale representation construction into account, the objectivefunction becomes:
min
{Pğ‘£,Zğ‘£ğ‘,Hğ‘£ğ‘}ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Xğ‘£âˆ’Zğ‘£
ğ‘Hğ‘£
ğ‘2
ğ¹+ğœƒğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Zğ‘ğ‘œğ‘Ÿğ‘’
ğ‘âˆ’Pğ‘£Zğ‘£
ğ‘2
ğ¹
ğ‘ .ğ‘¡.âˆ€ğ‘£,ğ‘,Zğ‘£
ğ‘ğ‘‡Zğ‘£
ğ‘=Iğ‘‘ğ‘,Pğ‘£Pğ‘£ğ‘‡=Iğ‘›,
(10)
whereğœƒis a hyperparameter.
3.2 Tensorial Multi-scale Fusion Module
The multi-scale representation matrices {Zğ‘£ğ‘}ğ‘£=1,...,ğ‘š
ğ‘=1,...,ğ‘Ÿand the cross-
view mapping matrix Pğ‘£can be obtained by solving the Eq. (10),
but the representation matrices in different scales cannot be fused
directly, so we first map them into a ğ‘-dimension space by a group
of matrices{Wğ‘}ğ‘Ÿ
ğ‘=1. Since the information characterized by dif-
ferent scales has varying importance, we further introduce scale
weights ğœ·âˆˆRğ‘ŸÃ—1to automatically balance the contributions of
different scales and improve the discriminative properties of the
fused representations {Gğ‘£}ğ‘š
ğ‘£=1âˆˆRğ‘›Ã—ğ‘,
min
{Gğ‘£,Wğ‘}âˆ’ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·ğ‘ğ‘‡ğ‘Ÿ(Gğ‘£Wğ‘(Pğ‘£Zğ‘£
ğ‘)ğ‘‡)
ğ‘ .ğ‘¡.âˆ€ğ‘£,ğ‘,Wğ‘Wğ‘ğ‘‡=Iğ‘,Gğ‘£ğ‘‡Gğ‘£=Iğ‘,
ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·2
ğ‘=1,ğœ·ğ‘â‰¥0.(11)
To capture the high-order correlation among views, the low-
rank tensor framework is usually adopted. In traditional tensor-
based works, the approximation of tensor rank often employs the
commonly used Tensor Nuclear Norm (TNN), which results in
under-penalizing the smaller singular values and over-penalizing
the larger singular values. Note that the large (small) singular values
usually represent the main structural information (the noise) of
the tensor, thus TNN often derives noise residuals that disrupt the
clustering structure. Being aware of this, we adopt a noise-robust
Enhanced Tensor Rank (ETR) (Definition 3) [ 10] to approximate the
true rank of representation tensor G=Î¦(G1,Â·Â·Â·,Gğ‘š)âˆˆRğ‘›Ã—ğ‘Ã—ğ‘š,
where Î¦denotes the merging operation.
Definition 3. [10] Given a tensorGâˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3, then the En-
hanced Tensor Rank (ETR) is defined as:
âˆ¥Gâˆ¥ETR=1
ğ‘›3ğ‘›3âˆ‘ï¸
ğ‘˜=1Gğ‘˜
ğ‘“ETR
=1
ğ‘›3ğ‘›3âˆ‘ï¸
ğ‘˜=1â„âˆ‘ï¸
ğ‘–=1Â©Â­
Â«ğ‘’ğ›¿2Sğ‘˜
ğ‘“(ğ‘–,ğ‘–)
ğ›¿+Sğ‘˜
ğ‘“(ğ‘–,ğ‘–)ÂªÂ®
Â¬,(12)
where 0<ğ›¿â‰¤1,â„=min(ğ‘›1,ğ‘›2)andSğ‘“is obtained by t-SVD of
Gğ‘“=Uğ‘“Sğ‘“Vğ‘‡
ğ‘“in Fourier domain.Gğ‘˜
ğ‘“means theğ‘˜-th frontal slice
of the tensorZğ‘“.
By simultaneously considering Eq. (10), Eq. (11) and Eq. (12), the
final objective function of our TUMCR is formulated as:
 
1249Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
min
{Gğ‘£,Pğ‘£,Zğ‘£ğ‘,Hğ‘£ğ‘,Wğ‘}âˆ¥Gâˆ¥ETRâˆ’ğ›¼ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·ğ‘ğ‘‡ğ‘Ÿ(Gğ‘£Wğ‘(Pğ‘£Zğ‘£
ğ‘)ğ‘‡)
+ğ›¾ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Xğ‘£âˆ’Zğ‘£
ğ‘Hğ‘£
ğ‘2
ğ¹+ğœƒğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Zğ‘ğ‘œğ‘Ÿğ‘’
ğ‘âˆ’Pğ‘£Zğ‘£
ğ‘2
ğ¹
ğ‘ .ğ‘¡.âˆ€ğ‘£,ğ‘,Zğ‘£
ğ‘ğ‘‡Zğ‘£
ğ‘=Iğ‘‘ğ‘,Wğ‘£
ğ‘Wğ‘£
ğ‘ğ‘‡=Iğ‘,Pğ‘£Pğ‘£ğ‘‡=Iğ‘›,
G=Î¦(G1,Â·Â·Â·,Gğ‘š),Gğ‘£ğ‘‡Gğ‘£=Iğ‘,ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·2
ğ‘=1,ğœ·ğ‘â‰¥0(13)
whereğ›¼,ğ›¾,ğœƒare the trade-off parameters.
Once the optimal representation matrices {Gğ‘£}ğ‘š
ğ‘£=1are obtained,
the unified affinity matrix can be fused as SG=1
ğ‘šÃğ‘š
ğ‘£=1Gğ‘£Gğ‘£ğ‘‡âˆˆ
Rğ‘›Ã—ğ‘›. Inspired by [ 11], the spectral embedding FâˆˆRğ‘›Ã—ğ‘can be
obtained by directly imposing singular value decomposition (SVD)
onË†G=1âˆšğ‘š[G1,Â·Â·Â·,Gğ‘š]âˆˆRğ‘›Ã—ğ‘šğ‘according to Theorem 1, which
enjoys a linear time complexity O(ğ‘›ğ‘š2ğ‘2).
Theorem 1. The left singular vectors of Ë†G=1âˆšğ‘š[G1,Â·Â·Â·,Gğ‘š]âˆˆ
Rğ‘›Ã—ğ‘šğ‘is the same as the eigenvectors of S=Ë†GË†Gğ‘‡.
Proof The singular value decomposition (SVD) of Ë†Gis denoted
asUÎ£Vğ‘‡, then
S=Ë†GË†Gğ‘‡=(UÎ£Vğ‘‡)(UÎ£Vğ‘‡)ğ‘‡=UÎ£2Uğ‘‡. (14)
Thereby, the left singular vectors of Ë†Gare the same as eigenvectors
ofS. â–¡
4 OPTIMIZATION
To solve problem (13), we first introduce the auxiliary tensor vari-
ableJ, and obtain the following augmented Lagrangian function,
L({Zğ‘£
ğ‘}ğ‘£=1,...,ğ‘š
ğ‘=1,...,ğ‘Ÿ,{Gğ‘£}ğ‘š
ğ‘£=1,{Hğ‘£
ğ‘}ğ‘£=1,...,ğ‘š
ğ‘=1,...,ğ‘Ÿ,J,Pğ‘£,I)
=âˆ¥Jâˆ¥ğ¸ğ‘‡ğ‘…âˆ’ğ›¼ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·ğ‘ğ‘‡ğ‘Ÿ(Gğ‘£Wğ‘(Pğ‘£Zğ‘£
ğ‘)ğ‘‡)
+ğ›¾ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Xğ‘£âˆ’Zğ‘£
ğ‘Hğ‘£
ğ‘2
ğ¹+ğœƒğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Zğ‘ğ‘œğ‘Ÿğ‘’
ğ‘âˆ’Pğ‘£Zğ‘£
ğ‘2
ğ¹
+âŸ¨I,Gâˆ’JâŸ©+ğœŒ
2âˆ¥Gâˆ’Jâˆ¥2
ğ¹,(15)
whereIis Lagrange multiplier and ğœŒis penalty parameter. Then,
we solve the variables in Eq. (15) through the following five sub-
problems.
4.1 Update Zğ‘£
ğ‘
When fixing other variables, the problem with Zğ‘£ğ‘is formulated as
arg min
Zğ‘£ğ‘ğ‘‡Zğ‘£ğ‘=Iğ‘‘ğ‘âˆ’ğ›¼ğœ·ğ‘ğ‘‡ğ‘Ÿ(Gğ‘£Wğ‘(Pğ‘£Zğ‘£
ğ‘)ğ‘‡)+ğ›¾Xğ‘£âˆ’Zğ‘£
ğ‘Hğ‘£
ğ‘2
ğ¹
+ğœƒZğ‘ğ‘œğ‘Ÿğ‘’
ğ‘âˆ’Pğ‘£Zğ‘£
ğ‘2
ğ¹
â‡’Zğ‘£âˆ—
ğ‘=arg max
Zğ‘£ğ‘ğ‘‡Zğ‘£ğ‘=Iğ‘‘ğ‘ğ‘‡ğ‘Ÿ(Zğ‘£
ğ‘ğ‘‡Mğ‘),(16)where Mğ‘=ğ›¼ğ›½ğ‘Pğ‘£ğ‘‡Gğ‘£Wğ‘+2ğ›¾Xğ‘£Hğ‘£ğ‘ğ‘‡+2ğœƒPğ‘£ğ‘‡Zğ‘ğ‘œğ‘Ÿğ‘’ğ‘. The optimal
solution of Zğ‘£ğ‘isUğ‘Vğ‘ğ‘‡, where Uğ‘andVğ‘are the left and right
singular matrix of Mğ‘.
4.2 Update Gğ‘£
Fixing the other variables leads to the following problem for Gğ‘£,
Gğ‘£âˆ—=arg max
Gğ‘£ğ‘‡Gğ‘£=Iğ‘ğ‘‡ğ‘Ÿ(Gğ‘£ğ‘‡Mğº),(17)
where Mğº=Ãğ‘Ÿ
ğ‘=1ğ›¼ğœ·ğ‘Pğ‘£Zğ‘£ğ‘Wğ‘ğ‘‡âˆ’Iğ‘£âˆ’Jğ‘£. The optimal solution
ofGğ‘£isUğºVğºğ‘‡, where UğºandVğºare the left and right singular
matrix of Mğº.
4.3 Update Pğ‘£
Fixing the other variables, we obtain the problem for Pğ‘£,
Pğ‘£âˆ—=arg max
Pğ‘£ğ‘‡Pğ‘£=Iğ‘›ğ‘‡ğ‘Ÿ(Pğ‘£ğ‘‡Mğ‘ƒ),(18)
where Mğ‘ƒ=Ãğ‘Ÿ
ğ‘=1(ğ›¼ğœ·ğ‘Gğ‘£Wğ‘Zğ‘£ğ‘ğ‘‡+2ğœƒZğ‘ğ‘Zğ‘£ğ‘ğ‘‡)The optimal solu-
tion of Pğ‘£isUğ‘ƒVğ‘ƒğ‘‡, where Uğ‘ƒandVğ‘ƒare the left and right singular
matrix of Mğ‘ƒ.
4.4 UpdateJ
When other variables are fixed, the subproblem for Gis formulated
as,
arg min
J1
ğœŒâˆ¥Jâˆ¥ğ¸ğ‘‡ğ‘…+1
2Jâˆ’(G+I
ğœŒ)2
ğ¹. (19)
We refer to this problem as the Enhanced Tensorial Rank Mini-
mization problem (ETRM) [ 10], which can be solved with the fol-
lowing theorem.
Theorem 2. [10] SupposeA âˆˆ Rğ‘›1Ã—ğ‘›2Ã—ğ‘›3with t-SVDA=
Uâˆ—Sâˆ—Vğ‘‡andğ›½>0. The Enhanced Tensorial Rank Minimization
problem (ETRM) can be described as follows,
arg min
Gğ›½âˆ¥Gâˆ¥ğ¸ğ‘‡ğ‘…+1
2âˆ¥Gâˆ’Aâˆ¥2
ğ¹. (20)
Then, optimal solution Gâˆ—is obtained as,
Gâˆ—=Uâˆ—ğ‘–ğ‘“ğ‘“ğ‘¡(ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘“,ğ›½(Sğ‘“),[],3)âˆ—Vğ‘‡, (21)
whereğ‘–ğ‘“ğ‘“ğ‘¡(ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘“,ğ›½(Sğ‘“),[],3)âˆˆRğ‘›1Ã—ğ‘›2Ã—ğ‘›3is a f-diagonal tensor,
andğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘“,ğ›½(Sğ‘˜
ğ‘“(ğ‘–,ğ‘–))satisfies the following equation,
ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘“,ğ›½(Sğ‘˜
ğ‘“(ğ‘–,ğ‘–))=arg min
ğ‘¥â‰¥01
2(ğ‘¥âˆ’Sğ‘˜
ğ‘“(ğ‘–,ğ‘–))2+ğ›½ğ‘“(ğ‘¥), (22)
whereğ‘“(ğ‘¥)=ğ‘’ğ›¿2ğ‘¥
ğ›¿+ğ‘¥.
The proof of Theorem 2 is given in [ 10]. Then we adopt the
difference of convex (DC) programming [ 29] to obtain the closed-
form solution,
ğœğ‘–ğ‘¡ğ‘’ğ‘Ÿ+1=
Sğ‘˜
ğ‘“(ğ‘–,ğ‘–)âˆ’ğœ•ğ‘“(ğœğ‘–ğ‘¡ğ‘’ğ‘Ÿ)
ğœŒ
+, (23)
whereğœ=ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘“,ğ›½(Sğ‘˜
ğ‘“(ğ‘–,ğ‘–)),ğ‘“(ğ‘¥)=ğ‘’ğ›¿2ğ‘¥
ğ‘¥+ğ›¿andğ‘–ğ‘¡ğ‘’ğ‘Ÿis the number
of iterations.
 
1250KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
4.5 Update Hğ‘£
ğ‘
Fixing the other variables, Hğ‘£ğ‘can be updated by,
Hğ‘£âˆ—
ğ‘=arg minâˆ¥Xğ‘£âˆ’Zğ‘£
ğ‘Hğ‘£
ğ‘âˆ¥2
ğ¹. (24)
Differentiating the objective function respecting Hğ‘£ğ‘and setting
the derivative to zero, it is obtained that Hğ‘£ğ‘is updated by
Hğ‘£âˆ—
ğ‘=Zğ‘£
ğ‘ğ‘‡Xğ‘£. (25)
4.6 Update Wğ‘
Fixing the other variables, Wğ‘can be updated by,
Wâˆ—
ğ‘=arg max
Wğ‘Wğ‘ğ‘‡=Iğ‘ğ‘‡ğ‘Ÿ(Wğ‘ğ‘‡Mğ‘Š),(26)
where Mğ‘Š=Ãğ‘š
ğ‘£=1Gğ‘£ğ‘‡Pğ‘£Zğ‘£ğ‘. The optimal solution of Wğ‘isUğ‘ŠVğ‘Šğ‘‡,
where Uğ‘ŠandVğ‘Šare the left and right singular matrix of Mğ‘Š.
4.7 Update ğœ·ğ‘
With other variables fixed in Eq. (15), ğœ·ğ‘can be updated by the
following formula
maxğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·ğ‘ğœ‚ğ‘, ğ‘ .ğ‘¡.ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·2
ğ‘=1,ğœ·ğ‘â‰¥0, (27)
whereğœ‚ğ‘=Ãğ‘š
ğ‘£=1ğ‘‡ğ‘Ÿ(Gğ‘£Wğ‘Zğ‘£ğ‘ğ‘‡Pğ‘£). The optimal solution is
ğœ·ğ‘=ğœ‚ğ‘âˆšï¸ƒÃğ‘Ÿ
ğ‘=1ğœ‚2ğ‘(28)
At last, the Lagrange multipliers and penalty parameters are
updated as follows,
I=I+ğœŒ(Gâˆ’J)
ğœŒ=ğœğœŒğœŒ, ğœŒ=min(ğœŒ,ğœŒmax)(29)
whereğœğœŒ>1is used to accelerate convergence. The complete
procedure is summarized in Algorithm 1.
Algorithm 1 Optimization Algorithm of TUMCR
Input: Multi-view data{X1,...,Xğ‘š}, cluster number ğ‘, trade-off
parametersğ›¾,ğ›¼andğœƒ.
Initialize: SetHğ‘£ğ‘,Zğ‘£ğ‘,Gğ‘£,Wğ‘to zero matrix ,J=I=0,ğœ‡=
10âˆ’5,ğœŒ=10âˆ’4,ğœğœŒ=2,ğœŒğ‘šğ‘ğ‘¥=1010,ğœ–=10âˆ’7.
1:while not converge do
2: Update Zğ‘£ğ‘by Eq. (16);
3: Update Gğ‘£by Eq. (17);
4: Update Pğ‘£by Eq. (18);
5: UpdateJby Eq. (19);
6: Update Hğ‘£ğ‘by Eq. (25);
7: Update Wğ‘by Eq. (26);
8: Update ğœ·ğ‘by Eq. (28);
9: UpdateğœŒandIby Eq.(29);
10: Check the convergence conditions:
âˆ¥Zâˆ’Gâˆ¥âˆ<ğœ–
11:end while
12:Output the clustering results via performing ğ‘˜-means clustering
onF.4.8 Convergence Analysis
The convergence of Algorithm 1 is theoretically guaranteed by the
following Theorem 3, the proof of this theorem is similar to the
convergence analysis in the work [38].
Theorem 3. Let{Yğ‘˜=(G(ğ‘˜),I(ğ‘˜),J(ğ‘˜))}âˆ
ğ‘˜=1be the sequence
generated by Algorithm 1, then the sequence {Yğ‘˜}âˆ
ğ‘˜=1satisfies the
following two principles:
1).{Yğ‘˜}âˆ
ğ‘˜=1is bounded.
2).Any accumulation point of {Yğ‘˜}âˆ
ğ‘˜=1is a KKT point of Eq. (15).
4.9 Time Complexity Analysis
The mean computational complexity of TUMCR focuses on updat-
ing the variables (i.e., Zğ‘£ğ‘,Hğ‘£ğ‘,Pğ‘£,Gğ‘£,Wğ‘,J) in problem (15), which
takeO(ğ‘›ğ‘‘2ğ‘),O(ğ‘›ğ‘‘ğ‘£ğ‘‘ğ‘),O(ğ‘›3),O(ğ‘›ğ‘2),O(ğ‘‘ğ‘ğ‘2)andO(ğ‘›ğ‘šğ‘ğ‘™ğ‘œğ‘”(ğ‘›)+
ğ‘›ğ‘š2ğ‘), respectively. So, the time complexity of TUMCR is O(ğ‘›3+
ğ‘›ğ‘šğ‘ğ‘™ğ‘œğ‘”(ğ‘›)+ğ‘›ğ‘‘ğ‘£ğ‘‘ğ‘).
Table 1: Details of the used datasets.
Dataset Sample Cluster View Type
Yale 165 15 3 Image
MSRCv1 210 7 4 Image
NGs 500 5 3 Text
BBCSport 544 5 2 Text
Caltech101-7 1474 7 6 Object
100leaves 1600 100 3 Image
Caltech101-20 2386 20 6 Object
BDGP 2500 5 2 Genome
CCV 6773 20 3 Video
5 EXPERIMENT
5.1 Experimental Settings
Datasets: Nine challenging datasets are adopted to validate our
TUMCR, including Yale1,MSRCv1, NGs2,BBCSport3,Caltech101-
7,100leaves4,Caltech101-205,BDGP6andCCV7. More details
can be found in Table 1. Then we randomly arrange samples in all
views excluding the central view to construct the fully unaligned
multi-view datasets.
Baselines: To validate the superiority of TUMCR, nine state-
of-the-art multi-view clustering methods are selected for com-
parison, including, AWP (2018) [ 27],GMC(2019) [ 33],EOMSC-
CA(2022) [ 19],SMVSC (2021) [ 28],AWMVC (2023)[ 32],T-UMC
(2022) [ 14] and three variations of UPMGC (i.e.,CoMSC+UPMGC,
LSR+UPMGC andGMC+UPMGC) (2023) [36], respectively.
Evaluation Metrics: We employ three commonly used met-
rics to measure the clustering quality, including accuracy (ACC),
normalized mutual information (NMI), and purity (PUR). For all
metrics, the larger values indicate better performances.
1http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html
2https://lig-membres.imag.fr/grimal/data.html
3http://mlg.ucd.ie/datasets/segment.html
4https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+data+set.
5http://www.vision.caltech.edu/Image_Datasets/Caltech101/
6https://www.fruitfly.org/
7http://www.ee.columbia.edu/ln/dvmm/CCV/
 
1251Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 2: Results of our proposed method and other compared methods on nine datasets. NAN means the program has a â€œNANâ€
or â€œINFâ€ problem.
Dataset
Metric AWP GMC EOMSC-CA SMVSC AWMVC T-UMC CoMSC+UPMGC LSR+UPMGC GMC+UPMGC Ours
Y
aleACC 0.3636 0.3152 0.3394 0.2909 0.5455 0.6509 0.6381 0.6728 0.5042 0.7152
NMI
0.3903 0.3402 0.3490 0.3486 0.5409 0.6858 0.6694 0.6982 0.5374 0.7887
P
UR 0.3818 0.3333 0.3515 0.3091 0.5455 0.6521 0.6474 0.6913 0.5088 0.7333
MSRCv1A
CC 0.2571 0.1905 0.3762 0.3476 0.3048 0.5200 0.7022 0.4075
0.3659 0.8714
NMI 0.0808 0.0478 0.2223 0.1612 0.1551 0.4392 0.6126 0.3322
0.2497 0.7722
PUR 0.2810 0.1952 0.3762 0.3524 0.3238 0.5295 0.7135 0.4726
0.3898 0.8714
BBCSp
ortACC 0.4853 0.4099 0.3548 0.4265 0.4504 0.954 0.9206
0.2658 0.5242 0.9669
NMI 0.1633 0.0824 0.0585 0.1003 0.2483 0.8600 0.7913
0.0183 0.3780 0.8904
PUR 0.4982 0.4154 0.4228 0.4522 0.5147 0.9540 0.9229
0.3655 0.6007 0.9669
NGsA
CC 0.4180 0.2320 0.2800 0.3140 0.3200 0.7544 0.8686 0.3682
0.3423 0.8820
NMI 0.1293 0.0270 0.0215 0.0345 0.0967 0.5631 0.6790 0.1203
0.1193 0.7097
PUR 0.4360 0.2340 0.2860 0.3140 0.3240 0.7544 0.8686 0.3761
0.3439 0.8820
Calte
ch101-7ACC 0.4389 0.5421 0.5434 0.3467 0.3426 0.3847 0.5951 0.3807
0.3337 0.6927
NMI 0.0378 0.0077 0.0075 0.0171 0.2143 0.3243 0.5586 0.2619
0.2212 0.1764
PUR 0.5461 0.5434 0.5448 0.5421 0.7001 0.7757 0.8472 0.7630
0.6671 0.6974
100leav
esACC 0.3913 0.1606 0.1313 0.1594 0.1875 0.6202 0.6307 0.1636
0.1839 0.7631
NMI 0.5813 0.2671 0.3268 0.4322 0.4781 0.7999 0.8200 0.4685
0.4843 0.8988
PUR 0.4031 0.1713 0.1475 0.1688 0.1981 0.6467 0.6653 0.1733
0.1947 0.7919
Calte
ch101-20ACC 0.1220 0.3336 0.3550 0.1572 0.2578 0.3449 0.3726 0.2644
0.2399 0.5448
NMI 0.0525 0.0182 0.0616 0.0411 0.2476 0.3782 0.5297 0.3171
0.2395 0.6969
PUR 0.3391 0.3399 0.3822 0.3378 0.4904 0.6169 0.7036 0.5490
0.4742 0.8407
BDGPA
CC 0.3624 0.1444 0.3216 0.4352 0.3996 NAN 0.5853 NAN
0.3542 0.9140
NMI 0.1002 0.1933 0.0923 0.1748 0.1870 NAN 0.3742 NAN
0.1214 0.7835
PUR 0.3624 0.7012 0.3268 0.4412 0.4040 NAN 0.6089 NAN
0.3804 0.9140
CCVA
CC 0.1000 0.1059 0.1078 0.2252 0.1084
NAN 0.2086 0.2011 0.1620 0.4310
NMI 0.0224 0.0085 0.0338 0.1618 0.0477 NAN 0.1839 0.1699
0.1227 0.5249
PUR 0.1255 0.1078 0.1236 0.2541 0.1410
NAN 0.2386 0.2268 0.1905 0.4859
Parameter Setting: For our TUMCR, five parameters need to be
tuned,ğ›¼,ğ›¾, andğœƒare trade-off parameters to balance the importance
of different terms in the objective function, which take the range of
{10âˆ’3,Â·Â·Â·,1},{10âˆ’3,Â·Â·Â·,103}and{10âˆ’3,Â·Â·Â·,101}, respectively.
The parameter ğ›¿in ETR has a search range of {0.001,0.01,0.1,1}.
The dimensions of scale space are selected from {2ğ‘,3ğ‘,4ğ‘}. For all
datasets, we use the first view as the central view. For the baselines,
we follow the parameter settings in the corresponding literature
and report the best results. All methods use the same random seed
to obtain stable results. All the experiments are implemented on
a computer with a 2.50GHz i7-11700 CPU and 64GB RAM, Matlab
R2021a.
5.2 Clustering Performance
Table 2 displays the clustering performance of all methods on nine
challenging datasets. The best and second best results in the table
are denoted by bold value and underline value. From Table 2, we
can conclude the following interesting observations.
1) From a global perspective, our TUMCR can achieve the best
or comparable clustering performance on all nine datasets com-
pared with the other nine comparison methods. Furthermore, the
improvement of clustering performance on some datasets is also
remarkable. For example, on the MSRCv1 dataset, TUMCR out-
performs the second-best CoMSC+UPMGC methods by 16.92%,
15.96%, and 15.79%, in terms of the three metrics respectively. For
the BDGP dataset, TUMCR gains improvements around 38.39%,
36.74%, and 31.15%, respectively. These results validate its cluster-
ing ability when processing the fully unaligned data. Meanwhile,
TUMCR achieves satisfactory clustering performance for variousdata domains (e.g., text data, object data, video data, etc.), which
further shows the clustering robustness under different scenarios.
2) Not surprisingly, the multi-view clustering baselines based on
the ideal dataset (i.e., AWP, GMC, EOMSC-CA, SMVSC, AWMVC)
obtain worse clustering results than the unaligned multi-view clus-
tering methods. This is because complementarity and consensus
among views are difficult to exploit in the fully unaligned case,
and the unaligned sample information may misrepresent these
multi-view clustering baselines, producing inferior results.
3) The clustering result obtained by TUMCR is much better than
that obtained by the unaligned multi-view clustering (i.e., T-UMC,
CoMSC+UPMGC, LSR+UPMGC, GMC+UPMGC), all of which are
proposed for fully unaligned problem. From the results, we can
infer that it is better to conduct the establishment of cross-view
correspondence on the multi-scale space than on a fixed space,
the multi-scale representations can provide a more complete pic-
ture of different views, which allows the model to better capture
complementary and consistent information among views.
Visualization: To demonstrate the advantages of TUMCR in
unaligned multi-view clustering more intuitively, we provide visual-
ization graphs on the BBCSport dataset, including the ideal original
data structure of the 2-th view, the unaligned data structure of the
2-th view, the data structure of aligned representation G2, and the
unified graphs learned by the TUMCR. As seen from Fig. 3, with the
data unalignment, the real data structure in the 2-th view is severely
damaged, which makes it difficult for traditional multi-view cluster-
ing methods to mine the consistency and complementarity among
views. Then, from Fig. 3.(c), it can be noticed that the important
diagonal block structure in the original data has been recovered,
which indicates that TUMCR can effectively recover the unaligned
 
1252KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.10.20.30.40.50.60.70.8
(a) Ideal structure
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.10.20.30.40.50.60.70.8 (b) Unaligned structure
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.0050.010.0150.020.025 (c) Aligned representation
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.0020.0040.0060.0080.010.0120.0140.0160.0180.020.022 (d) Unified graph
Figure 3: Visualization on 2-th view of BBCSport dataset. (a) The ideal original data structure. (b) The unaligned data structure.
(c) The data structure of aligned representation. (d) The unified graph of TUMCR.
structure. We present the clustering graphs learned by the TUMCR,
as can be seen from the Fig. 3.(d), it has a clearer diagonal block
structure as compared to previous structures, whereas a clearer
diagonal block structure implies stronger interclass variance and
intraclass compactness. These visualization results validate that our
approach is not only effective in recovering the unaligned structure
but also in mining the intrinsic similarity between multiple views.
5.3 Model Analysis
Scale Analysis: In our TUMCR, the number of the scale spaces
ğ‘Ÿ={1,2,3}, and the corresponding dimensions of the scale spaces
are{2ğ‘,3ğ‘,4ğ‘}. In order to verify the effectiveness of different scales
on clustering performance, we show the clustering results on nine
datasets using different numbers of scales, as shown in Table 3.
we can see that TUMRC achieves the best clustering results when
ğ‘Ÿ=2,3in most cases, which means that the perception of the
full picture of multi-view data is favored by increasing the scale
space, providing a good understanding of the consensus and com-
plementary information among views. Note that for the 100leaves
dataset, the clustering performance deteriorates with increasing ğ‘Ÿ.
This phenomenon is probably caused by the fact that a single-scale
space characterizes all views well, and the multi-scale spaces may
introduce more noise to perturb the original structure.
Ablation Studies: To evaluate the effectiveness of the ETR
in TUMCR, we derive a variant, i.e.,TUMCR-TNN. TUMCR-TNN
replaces the Enhanced Tensor Rank (ETR) by tensor nuclear norm
(TNN), and the loss functions are as follows:
min
{Gğ‘£,Pğ‘£,Zğ‘£ğ‘,Eğ‘,Hğ‘£ğ‘,Wğ‘}âˆ¥Gâˆ¥TNNâˆ’ğ›¼ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·ğ‘ğ‘‡ğ‘Ÿ(Gğ‘£Wğ‘(Pğ‘£Zğ‘£
ğ‘)ğ‘‡)
+ğ›¾ğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Xğ‘£âˆ’Zğ‘£
ğ‘Hğ‘£
ğ‘2
ğ¹+ğœƒğ‘šâˆ‘ï¸
ğ‘£=1ğ‘Ÿâˆ‘ï¸
ğ‘=1Zğ‘ğ‘œğ‘Ÿğ‘’
ğ‘âˆ’Pğ‘£Zğ‘£
ğ‘2
ğ¹
ğ‘ .ğ‘¡.âˆ€ğ‘£,ğ‘,Zğ‘£
ğ‘ğ‘‡Zğ‘£
ğ‘=Iğ‘‘ğ‘,Wğ‘£
ğ‘Wğ‘£
ğ‘ğ‘‡=Iğ‘,G=Î¦(G1,Â·Â·Â·,Gğ‘š).
Gğ‘£ğ‘‡Gğ‘£=Iğ‘,Pğ‘£Pğ‘£ğ‘‡=Iğ‘›,ğ‘Ÿâˆ‘ï¸
ğ‘=1ğœ·2
ğ‘=1,ğœ·ğ‘â‰¥0
(30)
Fig. 5.(a) shows the ACC result of TUMCR and its variant on
nine datasets (i.e., Yale, MSRCv1, BBCSport, NGs, Caltech101-7,
100leaves, Caltech101-20, BDGP and CCV). We can observe thatTUMCR is superior to TUMCR-TNN on all datasets, which demon-
strates the effectiveness of ETR. With the benefit of ETR, we can
learn a representation tensor with a satisfactory clustering struc-
ture, which greatly facilitates subsequent clustering performance.
Parameters Analysis: Our TUMCR has four parameters ğ›¼,ğ›¾,ğœƒ,
TUMCR TUMCR-TNN00.20.40.60.81Clustering Performanceyale
MSRCv1
WebKB
NGs
BBCSport
Caltech101-7
100leaves
Caltech101-20
BDGP
CCV
(a)
Figure 4: The comparison of TUMCR and TUMCR-TNN on
nine datasets.
andğ›¿that need to be tuned. Here, we investigate the sensitiv-
ity of TUMCR on these parameters. We first define the search
range set{10âˆ’3,Â·Â·Â·,1}forğ›¼andğ›¿,{10âˆ’3,Â·Â·Â·,103}forğ›¾and
{10âˆ’3,Â·Â·Â·,101}forğœƒ. Then, we show the clustering performance
(ACC) of the Caltech101-7 dataset in Fig. 5. We can observe that ğ›¾,
ğœƒandğ›¼can reach a more stable performance than ğ›¿. Specifically,
our method is sensitive to ğ›¿becauseğ›¿needs to adjust the strength
of the penalty for singular values in the ETR, which is essential for
learning the low-rank structure of the tensor.
Convergence Analysis: The convergence of our TUMCR is
guaranteed by the Theorem 3. Fig. 6 shows the values of stop criteria
in each iteration on eight datasets, the stop criteria used here is
Match Error (ME): ME=âˆ¥Jâˆ’Gâˆ¥âˆ. We can observe that the
values of ME rapidly tend to 0 within 20 steps and remain stable,
which indicates the excellent convergence property of our TUMCR.
 
1253Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 3: Scale analysis on nine datasets.
ğ‘ŸYale MSRCv1 BBCSport NGs Caltech101-7 100leaves Caltech101-20 BDGP CCV
ACC(%)1 38.20 60.00 28.10 53.80 54.30 76.30 39.70 91.80 41.40
271.50 60.00 96.70 71.20 64.90 66.30 54.50 91.40 43.10
3 30.30 87.10 56.10 88.20 69.30 29.70 49.50 91.20 39.80
(a)ğ›¾&ğ›¼vs ACC
 (b)ğ›¾&ğ›¼vs ACC
 (c)ğ›¾&ğ›¼vs ACC
10-310-210-110010-310-210-100.20.40.60.81Clustering Performance
ACC (d)ğ›¿vs ACC
Figure 5: The parameters analysis of our proposed TUMCR on the Caltech101-7 dataset.
0 5 10 15 20 25 30
Iterations00.511.522.5Stop CriteriaMatch Error
(a) Yale
0 5 10 15 20 25
Iterations00.10.20.30.40.50.60.70.80.91Stop CriteriaMatch Error (b) MSRCv1
0 5 10 15 20 25
Iterations00.20.40.60.811.21.4Stop CriteriaMatch Error (c) NGs
0 5 10 15 20 25 30
Iterations00.20.40.60.811.2Stop CriteriaMatch Error (d) BBCSport
0 5 10 15 20 25 30
Iterations00.511.522.533.544.55Stop CriteriaMatch Error
(e) 100leaves
0 5 10 15 20 25
Iterations00.511.522.5Stop CriteriaMatch Error (f) Caltech101-20
0 5 10 15 20 25
Iterations00.10.20.30.40.50.60.70.80.91Stop CriteriaMatch Error (g) BDGP
0 5 10 15 20 25 30
Iterations00.511.522.53Stop CriteriaMatch Error (h) CCV
Figure 6: Convergence Analysis: The stop criteria variation curves on eight datasets.
6 CONCLUSION
In this paper, we propose a novel unaligned multi-view clustering
method, termed tensorized unaligned multi-view clustering with
multi-scale representation learning (TUMCR). Distinct from exist-
ing unaligned multi-view clustering methods, TUMCR explores
the cross-view mapping matrices in multi-scale spaces, which are
constructed by the matrix factorization framework. The multi-scale
representations can characterize the full picture of multi-view data,
thus improving the accuracy of view alignment and clustering. To
fuse the information in different scales and views, TUMCR designs
a tensorial multi-scale fusion module, which employs an adaptive
weighting approach to integrate different scales and adopts thelow-rank tensor framework with Enhanced Tensor Rank to ex-
plore the high-order correlation among views. Furthermore, an
efficient optimization method is designed to address the objective
function. Extensive experiments are performed on nine challenging
datasets with different sizes. Compared with several state-of-the-art
methods, our proposed algorithm achieves superior performance
consistently.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Founda-
tion of China under Grant U2268203.
 
1254KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
REFERENCES
[1]Michael W Berry, Murray Browne, Amy N Langville, V Paul Pauca, and Robert J
Plemmons. 2007. Algorithms and applications for approximate nonnegative
matrix factorization. Computational Statistics & Data Analysis 52, 1 (2007), 155â€“
173.
[2]Thomas Blumensath. 2014. Sparse matrix decompositions for clustering. In 2014
22nd European Signal Processing Conference (EUSIPCO). IEEE, 1163â€“1167.
[3]Guoqing Chao, Shiliang Sun, and Jinbo Bi. 2021. A survey on multiview clustering.
IEEE Transactions on Artificial Intelligence 2, 2 (2021), 146â€“168.
[4]Man-Sheng Chen, Chang-Dong Wang, Dong Huang, Jian-Huang Lai, and Philip S
Yu. 2022. Efficient orthogonal multi-view subspace clustering. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
127â€“135.
[5]Chris Ding, Xiaofeng He, and Horst D Simon. 2005. On the equivalence of
nonnegative matrix factorization and spectral clustering. In Proceedings of the
2005 SIAM International Conference on Data Mining. SIAM, 606â€“610.
[6]Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonnegative
matrix t-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 126â€“135.
[7]Jicong Fan. 2021. Large-scale subspace clustering via k-factorization. In Proceed-
ings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining .
342â€“352.
[8]Shudong Huang, Ivor W Tsang, Zenglin Xu, and Jiancheng Lv. 2021. Measur-
ing diversity in graph learning: A unified framework for structured multi-view
clustering. IEEE Transactions on Knowledge and Data Engineering 34, 12 (2021),
5869â€“5883.
[9]Zhenyu Huang, Peng Hu, Joey Tianyi Zhou, Jiancheng Lv, and Xi Peng. 2020.
Partially view-aligned clustering. Advances in Neural Information Processing
Systems 33 (2020), 2892â€“2902.
[10] Jintian Ji and Songhe Feng. 2023. Anchor Structure Regularization Induced Multi-
view Subspace Clustering via Enhanced Tensor Rank Minimization. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 19343â€“19352.
[11] Zhao Kang, Wangtao Zhou, Zhitong Zhao, Junming Shao, Meng Han, and Zenglin
Xu. 2020. Large-scale Multi-view Subspace Clustering in Linear Time. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, Vol. 34. 4412â€“4419.
[12] Misha E Kilmer, Karen Braman, Ning Hao, and Randy C Hoover. 2013. Third-order
tensors as operators on matrices: A theoretical and computational framework
with applications in imaging. SIAM J. Matrix Anal. Appl. 34, 1 (2013), 148â€“172.
[13] Misha E Kilmer and Carla D Martin. 2011. Factorization strategies for third-order
tensors. Linear Algebra Appl. 435, 3 (2011), 641â€“658.
[14] Jia-Qi Lin, Man-Sheng Chen, Chang-Dong Wang, and Haizhang Zhang. 2024.
A tensor approach for uncoupled multiview clustering. IEEE Transactions on
Cybernetics 54, 2 (2024), 1236â€“1249.
[15] Yijie Lin, Yuanbiao Gou, Zitao Liu, Boyun Li, Jiancheng Lv, and Xi Peng. 2021.
Completer: Incomplete multi-view clustering via contrastive prediction. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
11174â€“11183.
[16] Yawen Ling, Jianpeng Chen, Yazhou Ren, Xiaorong Pu, Jie Xu, Xiaofeng Zhu,
and Lifang He. 2023. Dual label-guided graph refinement for multi-view graph
clustering. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37.
8791â€“8798.
[17] Jiyuan Liu, Xinwang Liu, Jian Xiong, Qing Liao, Sihang Zhou, Siwei Wang, and
Yuexiang Yang. 2020. Optimal neighborhood multiple kernel clustering with
adaptive local kernels. IEEE Transactions on Knowledge and Data Engineering 34,
6 (2020), 2872â€“2885.
[18] Jiyuan Liu, Xinwang Liu, Yuexiang Yang, Li Liu, Siqi Wang, Weixuan Liang, and
Jiangyong Shi. 2021. One-pass multi-view clustering for large-scale data. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 12344â€“
12353.
[19] Suyuan Liu, Siwei Wang, Pei Zhang, Kai Xu, Xinwang Liu, Changwang Zhang,
and Feng Gao. 2022. Efficient one-pass multi-view subspace clustering with
consensus anchors. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 36. 7576â€“7584.
[20] Xinwang Liu, Li Liu, Qing Liao, Siwei Wang, Yi Zhang, Wenxuan Tu, Chang
Tang, Jiyuan Liu, and En Zhu. 2021. One pass late fusion multi-view clustering.
InInternational Conference on Machine Learning. PMLR, 6850â€“6859.
[21] Xinwang Liu, Xinzhong Zhu, Miaomiao Li, Chang Tang, En Zhu, Jianping Yin,
and Wen Gao. 2019. Efficient and effective incomplete multi-view clustering. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 4392â€“4399.
[22] Xinwang Liu, Xinzhong Zhu, Miaomiao Li, Lei Wang, Chang Tang, Jianping
Yin, Dinggang Shen, Huaimin Wang, and Wen Gao. 2019. Late Fusion Incom-
plete Multi-View Clustering. IEEE Transactions on Pattern Analysis and Machine
Intelligence 41, 10 (2019), 2410â€“2423.
[23] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng
Yan. 2016. Tensor robust principal component analysis: exact recovery of cor-
rupted low-rank tensors via convex optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 5249â€“5257.[24] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng Yan.
2019. Tensor robust principal component analysis with a new tensor nuclear
norm. IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 4 (2019),
925â€“938.
[25] J. Macqueen. 1967. Some methods for classification and analysis of multivariate
observations. Proc. Symp. Math. Statist. and Probability, 5th 1 (1967).
[26] Feiping Nie, Shenfei Pei, Rong Wang, and Xuelong Li. 2020. Fast clustering with
co-clustering via discrete non-negative matrix factorization for image identifica-
tion. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 2073â€“2077.
[27] Feiping Nie, Lai Tian, and Xuelong Li. 2018. Multiview clustering via adap-
tively weighted procrustes. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 2022â€“2030.
[28] Mengjing Sun, Pei Zhang, Siwei Wang, Sihang Zhou, Wenxuan Tu, Xinwang Liu,
En Zhu, and Changjian Wang. 2021. Scalable multi-view subspace clustering
with unified anchors. In Proceedings of the 29th ACM International Conference on
Multimedia. 3528â€“3536.
[29] Pham Dinh Tao and LT Hoai An. 1997. Convex analysis approach to DC pro-
gramming: theory, algorithms and applications. Acta Mathematica Vietnamica
22, 1 (1997), 289â€“355.
[30] Shinji Umeyama. 1988. An eigendecomposition approach to weighted graph
matching problems. IEEE Transactions on Pattern Analysis and Machine Intelligence
10, 5 (1988), 695â€“703.
[31] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
Computing 17, 4 (2007), 395â€“416.
[32] Xinhang Wan, Xinwang Liu, Jiyuan Liu, Siwei Wang, Yi Wen, Weixuan Liang,
En Zhu, Zhe Liu, and Lu Zhou. 2023. Auto-weighted multi-view clustering for
large-scale data. arXiv preprint arXiv:2303.01983 (2023).
[33] Hao Wang, Yan Yang, and Bing Liu. 2019. GMC: Graph-based multi-view cluster-
ing.IEEE Transactions on Knowledge and Data Engineering 32, 6 (2019), 1116â€“1129.
[34] Jing Wang, Feng Tian, Hongchuan Yu, Chang Hong Liu, Kun Zhan, and Xiao
Wang. 2017. Diverse non-negative matrix factorization for multiview data repre-
sentation. IEEE Transactions on Cybernetics 48, 9 (2017), 2620â€“2632.
[35] Qiang Wang, Yong Dou, Xinwang Liu, Fei Xia, Qi Lv, and Ke Yang. 2018. Local
kernel alignment based multi-view clustering using extreme learning machine.
Neurocomputing 275 (2018), 1099â€“1111.
[36] Yi Wen, Siwei Wang, Qing Liao, Weixuan Liang, Ke Liang, Xinhang Wan, and
Xinwang Liu. 2023. Unpaired multi-view graph clustering with cross-view struc-
ture matching. IEEE Transactions on Neural Networks and Learning Systems (2023),
1â€“15.
[37] Like Xin, Wanqi Yang, Lei Wang, and Ming Yang. 2023. Selective Contrastive
Learning for Unpaired Multi-View Clustering. IEEE Transactions on Neural
Networks and Learning Systems (2023), 1â€“15.
[38] Haizhou Yang, Quanxue Gao, Wei Xia, Ming Yang, and Xinbo Gao. 2022. Mul-
tiview spectral clustering with bipartite graph. IEEE Transactions on Image
Processing 31 (2022), 3591â€“3605.
[39] Mouxing Yang, Yunfan Li, Zhenyu Huang, Zitao Liu, Peng Hu, and Xi Peng. 2021.
Partially view-aligned representation learning with noise-robust contrastive
loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 1134â€“1143.
[40] Hong Yu, Jia Tang, Guoyin Wang, and Xinbo Gao. 2021. A novel multi-view clus-
tering method for unknown mapping relationships between cross-view samples.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining. 2075â€“2083.
[41] Kun Zhan, Changqing Zhang, Junpeng Guan, and Junsheng Wang. 2017. Graph
learning for multiview clustering. IEEE Transactions on Cybernetics 48, 10 (2017),
2887â€“2895.
[42] Chao Zhang, Huaxiong Li, Wei Lv, Zizheng Huang, Yang Gao, and Chunlin
Chen. 2023. Enhanced tensor low-rank and sparse representation recovery
for incomplete multi-view clustering. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 37. 11174â€“11182.
[43] Chen Zhang, Siwei Wang, Jiyuan Liu, Sihang Zhou, Pei Zhang, Xinwang Liu,
En Zhu, and Changwang Zhang. 2021. Multi-view clustering via deep matrix
factorization and partition alignment. In Proceedings of the 29th ACM International
Conference on Multimedia. 4156â€“4164.
[44] Pei Zhang, Siwei Wang, Liang Li, Changwang Zhang, Xinwang Liu, En Zhu,
Zhe Liu, Lu Zhou, and Lei Luo. 2023. Let the data choose: flexible and diverse
anchor graph fusion for scalable multi-view clustering. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 11262â€“11269.
[45] Xianchao Zhang, Linlin Zong, Xinyue Liu, and Hong Yu. 2015. Constrained
NMF-based multi-view clustering on unmapped data. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 29. 3174â€”-3180.
[46] Handong Zhao, Zhengming Ding, and Yun Fu. 2017. Multi-view clustering via
deep matrix factorization. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 31. 2921â€“2927.
[47] Shuping Zhao, Jie Wen, Lunke Fei, and Bob Zhang. 2023. Tensorized incomplete
multi-view clustering with intrinsic graph completion. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 11327â€“11335.
 
1255Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
[48] Peng Zhou, Liang Du, Xinwang Liu, Zhaolong Ling, Xia Ji, Xuejun Li, and Yi-
Dong Shen. 2023. Partial Clustering Ensemble. IEEE Transactions on Knowledge
and Data Engineering (2023), 1â€“14.[49] Pan Zhou, Canyi Lu, Jiashi Feng, Zhouchen Lin, and Shuicheng Yan. 2019. Tensor
low-rank representation for data recovery and clustering. IEEE Transactions on
Pattern Analysis and Machine Intelligence 43, 5 (2019), 1718â€“1732.
 
1256