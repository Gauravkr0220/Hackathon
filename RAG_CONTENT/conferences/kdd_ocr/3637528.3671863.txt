Latent Diffusion-based Data Augmentation for Continuous-Time
Dynamic Graph Model
Yuxing Tian
tianyxx@gmail.com
International Digital Economy
Academy, IDEA Research
Shenzhen, ChinaAiwen Jiang
jiangaiwen@jxnu.edu.cn
Jiangxi Normal University
Nanchang, ChinaQi Huang
huangqi@jxnu.edu.cn
Jiangxi Normal University
Nanchang, China
Jian Guoâ€¡
guojian@idea.edu.cn
International Digital Economy
Academy, IDEA Research
Shenzhen, ChinaYiyan Qiâ€¡
qiyiyan@idea.edu.cn
International Digital Economy
Academy, IDEA Research
Shenzhen, China
ABSTRACT
Continuous-Time Dynamic Graph (CTDG) precisely models evolv-
ing real-world relationships, drawing heightened interest in dy-
namic graph learning across academia and industry. However, ex-
isting CTDG models encounter challenges stemming from noise and
limited historical data. Graph Data Augmentation (GDA) emerges
as a critical solution, yet current approaches primarily focus on
static graphs and struggle to effectively address the dynamics inher-
ent in CTDGs. Moreover, these methods often demand substantial
domain expertise for parameter tuning and lack theoretical guaran-
tees for augmentation efficacy. To address these issues, we propose
Conda, a novel latent diffusion-based GDA method tailored for CT-
DGs. Conda features a sandwich-like architecture, incorporating a
Variational Auto-Encoder (VAE) and a conditional diffusion model,
aimed at generating enhanced historical neighbor embeddings for
target nodes. Unlike conventional diffusion models trained on en-
tire graphs via pre-training, Conda requires historical neighbor
sequence embeddings of target nodes for training, thus facilitating
more targeted augmentation. We integrate Conda into the CTDG
model and adopt an alternating training strategy to optimize perfor-
mance. Extensive experimentation across six widely used real-world
datasets showcases the consistent performance improvement of
our approach, particularly in scenarios with limited historical data.
CCS CONCEPTS
â€¢Theory of computation â†’Dynamic graph algorithms; â€¢
Information systems â†’Social networks.
â€¡Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671863KEYWORDS
Dynamic graph, data augmentation, diffusion model
ACM Reference Format:
Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guoâ€¡, and Yiyan Qiâ€¡. 2024. Latent
Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph
Model. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, NewYork, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671863
1 INTRODUCTION
Continuous-Time Dynamic Graphs (CTDGs), with every edge (event)
having a timestamp to denote its occurrence time, are prevalent
in real-world applications such as social networks [ 1,11], physical
systems [ 18] and e-commerce [ 46]. Recently, CTDG models [ 2,16,
17,23,24,26,31,37,48] have gained increasing attention due to
their significant representation capacity by directly learning the
representation of the continuously occurring events in CTDGs. De-
spite the rapid advancements in CTDG models, they encounter two
primary challenges. Firstly, the "observed" CTDG often falls short
of accurately representing the true underlying process it intends
to model, mainly due to various factors such as measurement in-
accuracies, thresholding errors, or human mistakes [ 22]. Secondly,
most CTDG methods typically rely on extensive historical data for
effective training [ 26]. However, in many applications, obtaining
such data is impractical, particularly in scenarios with a cold start.
For instance, a nascent trading platform may only possess a few
daysâ€™ worth of user-asset interactions, rendering existing CTDG
models trained on such limited data inadequate and resulting in
sub-optimal performance.
Graph Data Augmentation (GDA) has emerged as a promising
solution, with existing methods falling into two main categories [ 7]:
structure-oriented and feature-oriented methods. Structure-oriented
methods, such as [ 15,28,42], typically involve adjusting graph con-
nectivity by adding or removing edges or nodes. On the other hand,
feature-oriented methods, exemplified by works from [ 12,14,15,
21,32], directly modify or create raw features of nodes or edges in
graphs.
2900
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
However, most existing GDA methods focus on static graphs and
it is challenging to directly apply on CTDG. (1) Existing structure-
oriented GDA methods heavily rely on domain knowledge, necessi-
tating the selection of diverse augmentation strategy combinations
tailored to specific graph datasets, as highlighted in [ 45]. Further-
more, structure-oriented GDA methods present inherent challenges
in calibrating the extent of data augmentation, potentially leading
to either over-augmentation or under-augmentation [ 3]. This is-
sue arises from input data processed by mapping-agnostic deep
neural networks, which generate features without tailored calibra-
tion. (2) Existing feature-oriented GDA methods concentrate on
transforming node/edge features and have demonstrated significant
optimization improvements with appropriate feature augmenta-
tions. However, they cannot handle graphs lacking node features or
even edge features. (3) Although Wang et al. [ 36] propose MeTA for
CTDG, this method is tailored for CTDG models [ 29,31] with mem-
ory modules, limiting its compatibility with other state-of-the-art
models such as DyGFormer [44] and GraphMixer [5].
To address these above challenges, we introduce a novel latent
cond itional diffusion-based dataaugmentation method for CTDG
models, named Conda . Conda adopts a sandwich-like architecture,
comprising a Variational Auto-Encoder (VAE) and a conditional
diffusion model. This design aims to create new historical neighbor
embeddings derived from existing neighbor sequences, enhancing
subsequent training of the CTDG model. Unlike traditional diffusion
models trained via pre-training on the entire graphs, Conda neces-
sitates historical neighbor sequence embeddings of target nodes for
training. Consequently, we integrate Conda into the CTDG model
and implement an alternating training strategy. Our contributions
can be summarized as follows:
â€¢We present Conda, an innovative data augmentation technique
aimed at enhancing the CTDG model. This method leverages a
latent conditional diffusion model to generate historical neighbor
embeddings for the target node during the training phase.
â€¢Rather than directly manipulating the raw graph structure, Conda
operates within the latent space, where it is more likely to en-
counter authentic samples.
â€¢We extensively evaluate our method on six widely used real-
world datasets and compare it against seven baselines. Our re-
sults demonstrate that Conda enhances the performance of link
prediction tasks on these baselines by up to 5%. Notably, this
improvement is achieved without the need for domain-specific
knowledge.
2 PRELIMINARIES
2.1 Continuous-Time Dynamic Graph
A CTDGğºcan be represented as a chronological sequence of inter-
actions between specific node pairs: ğº={(ğ‘¢0,ğ‘£0,ğ‘¡0),...,(ğ‘¢ğ‘›,ğ‘£ğ‘›,ğ‘¡ğ‘›)},
whereğ‘¡ğ‘–denotes the timestamp and the timestamps are ordered
as(0â‰¤ğ‘¡0â‰¤ğ‘¡1â‰¤...â‰¤ğ‘¡ğ‘›).ğ‘¢ğ‘–,ğ‘£ğ‘–âˆˆğ‘‰denote the node IDs of the
ğ‘–âˆ’ğ‘¡â„interaction at timestamp ğ‘¡ğ‘–,ğ‘‰is the entire node set. Each
nodeğ‘£âˆˆğ‘‰is associated with node feature ğ‘£ğ‘¢, and each interaction
(ğ‘¢,ğ‘—,ğ‘¡)has edge feature ğ‘’ğ‘¡
ğ‘¢,ğ‘£âˆˆğ‘…ğ‘‘ğ‘’, whereğ‘‘ğ‘£andğ‘‘ğ‘’denote the
dimensions of the node and link feature respectively.2.2 Diffusion Model
The diffusion model encompasses both forward and reverse pro-
cesses.
Forward process. In general, given an input data point ğ’™0drawn
from the distribution ğ‘(ğ’™0), the forward process involves gradually
introducing Gaussian noise to ğ’™0, generating a sequence of increas-
ingly noisy variables ğ’™1,ğ’™2,..., ğ’™ğ‘in a Markov chain. The final
noisy output, ğ’™ğ‘, follows a Gaussian distribution N(0,ğ‘°)and car-
ries no discernible information about the original data point. Specif-
ically, the transition from one point to the next is determined by
a conditional probability ğ‘(ğ’™ğ‘›|ğ’™ğ‘›âˆ’1)=N(ğ’™ğ‘›;âˆšï¸
1âˆ’ğ›½ğ‘›ğ’™ğ‘›âˆ’1,ğ›½ğ‘›ğ‘°),
whereğ›½ğ‘›âˆˆ(0,1)controls the scale of noise added at step ğ‘›.
Reverse process. The reverse process reverses the effects of the
forward process by learning to eliminate the added noise and tries
to gradually reconstruct the original data ğ‘¥0via sampling from ğ‘¥ğ‘
by learning a neural network ğ‘“ğœƒ.
Inference. Once trained, the diffusion model can produce new data
by sampling a point from the final distribution ğ’™ğ‘âˆ¼N( 0,ğ‘°)and
then iteratively denoising it using the aforementioned model ğ’™ğ‘â†¦â†’
ğ’™ğ‘âˆ’1â†¦â†’Â·Â·Â·â†¦â†’ ğ’™0to obtain a sample from the data distribution.
3 METHODOLOGY
Existing structure-oriented GDA methods like MeTA augment the
CTDGs by modifying the initial interactions through edge addi-
tion/deletion and time perturbation. However, these methods in-
troduce coarse-grained augmentations and substantially alter the
original transition patterns within CTDGs. Conversely, simply in-
troducing noise to either the raw or hidden feature space often lacks
theoretical bound. In this section, we introduce a novel fine-grained
GDA model based on a conditional diffusion model and establish
robust theoretical guarantees.
3.1 CTDG model
As mentioned above, the paradigm of the CTDG model ğœ‰can be
divided into two parts: the encoder module and the backbone mod-
ule. Mathematically, given an interaction (ğ‘¢,ğ‘£,ğ‘¡)and historical
interactions before timestamp ğ‘¡, the computation flow unfolds as
follows:
ğ’”ğ‘¡
ğ‘¢=ğ‘’ğ‘›ğ‘({ğ‘£ğ‘¤ğ‘™âˆ¥ğ‘’ğ‘¡ğ‘™
(ğ‘¢,ğ‘¤ğ‘™)âˆ¥ğ‘¡ğ‘™}),ğ‘¤ğ‘™âˆˆN<ğ‘¡(ğ‘¢) (1)
where ğ’”ğ‘¡
ğ‘¢âˆˆğ‘…ğ¿Ã—ğ·denotes the historical neighbor embedding se-
quence,ğ·represents the embedding dimension. N<ğ‘¡(ğ‘¢)denotes
the sampled set of neighbors that interacted with node ğ‘¢before
timestamps ğ‘¡,ğ‘¤ğ‘™âˆˆN<ğ‘¡(ğ‘¢)is theğ‘™-th neighbor, and âˆ¥denotes
the concatenation operation. ğ‘’ğ‘›ğ‘(Â·)represents the general encoder
module of CTDG model.
ğ’‰ğ‘¡
ğ‘¢=ğ‘ğ‘ğ‘ğ‘˜ğ‘ğ‘œğ‘›ğ‘’(ğ’”ğ‘¡
ğ‘¢) (2)
ğ’‰ğ‘¡
ğ‘¢âˆˆğ‘…ğ·denotes the representation of node ğ‘¢at timestamp ğ‘¡.
ğ‘ğ‘ğ‘ğ‘˜ğ‘ğ‘œğ‘›ğ‘’(Â·)represents the backbone of CTDG model.
3.2 Conda
Due to the extensive resource requirement of the diffusion process,
to reduce the costs, we first utilize a VAE encoder to conduct di-
mension compression and then conduct diffusion processes in the
latent space.
2901Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Figure 1: The Alternating Training Process of Conda and the CTDG model. While the Conda module is in the training phase,
the other modules are frozen. Conversely, when the Conda module is frozen, other modules are in the training phase
VAE Encoder. Given the historical neighbor embedding se-
quence ğ’”of any node1, computed by the CTDG encoder module,
we use a variational encoder parameterized by ğœ™to compress ğ’”
to a low-dimensional vector ğ’›âˆˆğ‘…ğ¿Ã—ğ‘‘, whereğ‘‘<<ğ·, the VAE
encoderğœ™predicts ğğœ™andğœ2
ğœ™ğ‘°as the mean and covariance of the
variational distribution ğ‘ğœ™(ğ’›|ğ’”)=N(ğ’›;ğğœ™(ğ’”),ğœ2
ğœ™(ğ’”)ğ‘°).
Forward Process with Partial Noising. It is worth noting that,
different from the conventional diffusion model that corrupts the
whole variable without distinction, we conduct partial noising in
order to control the magnitude of data augmentation and prepare
for the reverse process with conditional denoising. Specifically,
given the low-dimensional vector ğ’›=[ğ‘§ğ‘¤1,ğ‘§ğ‘¤2,...,ğ‘§ğ‘¤ğ¿], we can set
ğ’™0=ğ’›as the initial state. Then we divide ğ’™0into two part: diffused
part and conditional part, ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
0âˆˆğ‘…ğ‘‘ğ‘–ğ‘“ğ‘“Ã—ğ‘‘andğ’™ğ‘ğ‘œğ‘›ğ‘‘
0âˆˆğ‘…ğ‘ğ‘œğ‘›ğ‘‘Ã—ğ‘‘,
whereğ‘‘ğ‘–ğ‘“ğ‘“+ğ‘ğ‘œğ‘›ğ‘‘ =ğ¿andğ’™0=ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
0||ğ’™ğ‘ğ‘œğ‘›ğ‘‘ğ‘
0. In the forward
process, we only add noise on ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
0. Then the forward process is
parameterized by
ğ‘(ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
ğ‘›|ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
ğ‘›âˆ’1)=N(ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
ğ‘›;âˆšï¸
1âˆ’ğ›½ğ‘›ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
ğ‘›âˆ’1,ğ›½ğ‘›ğ‘°), (3)
whereğ›½ğ‘›âˆˆ(0,1)controls the Gaussian noise scales added at each
stepğ‘›. Since the transition kernel is Gaussian, the value at any step
ğ‘›can be sampled directly from ğ’™0in practice. Let ğ›¼ğ‘›=1âˆ’ğ›½ğ‘›and
Â¯ğ›¼ğ‘›=ğ‘›Ã–
ğ‘›â€²=1ğ›¼â€²
ğ‘›, then we can write:
ğ‘(ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
ğ‘›|ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
0)=N(ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
ğ‘›;âˆšÂ¯ğ›¼ğ‘›ğ’™ğ‘‘ğ‘–ğ‘“ğ‘“
0,(1âˆ’Â¯ğ›¼ğ‘›)ğ‘°). (4)
where we can reparameterize ğ’™ğ‘›=âˆšÂ¯ğ›¼ğ‘›ğ’™0+âˆš1âˆ’Â¯ğ›¼ğ‘›ğwith ğâˆ¼
N(0,ğ‘°). To regulate the added noises in ğ’™1:ğ‘, follow [ 35], we ultilize
the linear noise schedule for 1âˆ’Â¯ğ›¼ğ‘›:
1âˆ’Â¯ğ›¼ğ‘›=ğ‘˜Â·
ğ›¼min+ğ‘›âˆ’1
ğ‘âˆ’1(ğ›¼maxâˆ’ğ›¼min)
, ğ‘›âˆˆ{1,...,ğ‘}(5)
1For brevity, we omit the subscript node ğ‘¢and superscript timestamp ğ‘¡inğ’”forğ’”ğ‘¡
ğ‘¢
unless necessary to avoid ambiguity.where a hyper-parameter ğ‘˜âˆˆ[0,1]controls the noise scales, and
two hyper-parameters ğ›¼min<ğ›¼maxâˆˆ(0,1)indicating the upper
and lower bounds of the added noises.
Reverse Process with Conditional Denoising. The reverse
process is used to reconstruct the original ğ’™0by denoising ğ’™ğ‘. With
the partial nosing strategy adopted in the forward process, we can
naturally employ the part without noise as the conditional input
when denoising.
Starting from ğ’™ğ‘, the reverse process is parameterized by the
denoising transition step:
ğ‘ğœƒ(ğ’™ğ‘›âˆ’1|ğ’™ğ‘›)=N(ğ’™ğ‘›âˆ’1;ğğœƒ(ğ’™ğ‘›,ğ‘›),ğˆğœƒ(ğ’™ğ‘›,ğ‘›)) (6)
where ğğœƒ(ğ’™ğ‘›,ğ‘›)andğˆğœƒ(ğ’™ğ‘›,ğ‘›)are parameterization of the pre-
dicted mean and standard deviation outputted by any neural net-
worksğ‘“ğœƒ. Then the whole reverse process can be written as follows:
ğ‘ğœƒ(ğ‘¥ğ‘:0)=ğ‘(ğ‘¥ğ‘)ğ‘›Ã–
ğ‘›=1ğ‘ğœƒ(ğ’™ğ‘›âˆ’1|ğ’™ğ‘›) (7)
The reconstructed output is denoted as Ë†ğ’™0.
VAE Decoder. To keep the notations consistent, we set Ë†ğ’›=Ë†ğ’™0,
Ë†ğ’›is then fed into the VAE decoder parameterized by ğœ“to predict ğ’”
viağ‘ğœ“(ğ’”|Ë†ğ’™0).
3.3 Optimization and Alternating Training
In this section, we first present the optimization objective for the
Conda and the CTDG model, respectively. Then we introduce the
training and inference process of CTDG model with conda.
Although our ultimate goal is to learn a CTDG model ğœ‰, but
we also have to learn the parameters of the VAE encoder ğ‘ğœ™(ğ’›|ğ’”),
the conditional diffusion model logğ‘ğœƒ(ğ’™0)and the VAE decoder
ğ‘ğœ“(ğ’”|Ë†ğ’™0)for providing positive augmentation to the CTDG model.
CTDG model. For training the CTDG model ğœ‰, the Loss func-
tionLğ‘ğ‘¡ğ‘‘ğ‘” depends on the downstream task. For example, if the
downstream task is link prediction, then the loss function is binary
cross-entropy.
2902KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
Variational Auto Encoder. Theğ‘ğœ™(ğ’›|ğ’”)andğ‘ğœ“(ğ’”|Ë†ğ’™0)jointly con-
stitute a VAE that bridges the embedding space and the latent space.
We optimize the VAE by directly maximizing the ELBO:
Lğ‘£ğ‘ğ‘’(ğ’”;ğœ™,ğœ“)=Eğ‘ğ“(ğ’›|ğ’”)
logğ‘(ğ’”,ğ’›)
ğ‘ğ“(ğ’›|ğ’”)
=Eğ‘ğ“(ğ’›|ğ’”)
logğ‘ğ(ğ’”|ğ’›)ğ‘(ğ’›)
ğ‘ğ“(ğ’›|ğ’”)
=Eğ‘ğ“(ğ’›|ğ’”)
logğ‘ğ(ğ’”|ğ’›)
+Eğ‘ğ“(ğ’›|ğ’”)
logğ‘(ğ’›)
ğ‘ğ“(ğ’›|ğ’”)
=Eğ‘ğ“(ğ’›|ğ’”)
logğ‘ğ(ğ’”|ğ’›)
|                         {z                         }
reconstruction termâˆ’ğ·KL(ğ‘ğ“(ğ’›|ğ’”)ğ‘(ğ’›))
|                    {z                    }
prior matching term
â‰¥Eğ‘ğ“(ğ’›|ğ’”)
logğ‘ğ(ğ’”|ğ’›)
(8)
where the first term measures the reconstruction likelihood of the
decoder from variational distribution, the second term measures
how similar the learned variational distribution is to a prior belief
held over latent variables. Maximizing the ELBO is thus equivalent
to maximizing its first term and minimizing its second term. Since
the KL divergence term of the ELBO can be computed analytically,
and the reconstruction term can be approximated using a Monte
Carlo estimate:
arg max
ğ“,ğEğ‘ğ“(ğ’›|ğ’”)
logğ‘ğ(ğ’”|ğ’›)
âˆ’ğ·KL(ğ‘ğ“(ğ’›|ğ’”)ğ‘(ğ’›))
â‰ˆarg max
ğ“,ğğ‘€âˆ‘ï¸
ğ‘š=1logğ‘ğ(ğ’”|ğ’›(ğ‘š))âˆ’ğ·KL(ğ‘ğ“(ğ’›|ğ’”)ğ‘(ğ’›))(9)
where latents{ğ’›(ğ‘š)}ğ‘€
ğ‘š=1are sampled from ğ‘ğ“(ğ’›|ğ’”), for every ob-
servation ğ’”in the traning sample.
Conditional diffusion model. To optimize the conditional
diffusion model ğœƒ, the training objective is to use the Variational
Lower Bound (VLB) to optimize the log-likelihood of ğ’™0:
Lğ‘‰ğ¿ğµ(ğ’™0;ğœƒ)=logE[ğ‘ğœƒ(ğ’™0)]
=logE[âˆ«
ğ‘ğœƒ(ğ’™0:ğ‘)dğ’™1:ğ‘]
â‰¤logEğ‘(ğ’™1:ğ‘|ğ’™0)[ğ‘(xğ‘|x0)
ğ‘ğœƒ(xğ‘)
|       {z       }
Lğ‘+ğ‘âˆ‘ï¸
ğ‘›=2logğ‘(xğ‘›âˆ’1|x0,xğ‘›)
ğ‘ğœƒ(xğ‘›âˆ’1|xğ‘›)
|                   {z                   }
Lğ‘›âˆ’1]
(10)
Next we will provide detailed to show how we estimate VLB.
The termLğ‘›âˆ’1makesğ‘ğœƒ(ğ’™ğ‘›âˆ’1|ğ’™ğ‘›)to approximate the tractable
distribution ğ‘(ğ’™ğ‘›âˆ’1|ğ’™0,ğ’™ğ‘›). Through Bayes rules, we can derive
the probability of any intermediate value ğ’™ğ‘›âˆ’1given its successor
ğ’™ğ‘›and initial ğ’™0as:
ğ‘(ğ’™ğ‘›âˆ’1|ğ’™ğ‘›,ğ’™0)=ğ‘(ğ’™ğ‘›|ğ’™ğ‘›âˆ’1,ğ’™0)ğ‘(ğ’™ğ‘›âˆ’1|ğ’™0)
ğ‘(ğ’™ğ‘›|ğ’™0)(11)
ğ‘(ğ’™ğ‘›âˆ’1|ğ’™ğ‘›,ğ’™0)=N(ğ’™ğ‘›âˆ’1;Ëœğğ‘›,Ëœğ›½ğ‘›ğ‘°) (12)
where Ëœğœ‡ğ‘›(ğ’™ğ‘›,ğ’™0)=âˆšğ›¼ğ‘›(1âˆ’Â¯ğ›¼ğ‘›âˆ’1)
1âˆ’Â¯ğ›¼ğ‘›ğ’™ğ‘›+âˆšÂ¯ğ›¼ğ‘›âˆ’1ğ›½ğ‘›
1âˆ’Â¯ğ›¼ğ‘›ğ’™0,
Ëœğ›½ğ‘›=1âˆ’Â¯ğ›¼ğ‘›âˆ’1
1âˆ’Â¯ğ›¼ğ‘›ğ›½ğ‘›.(13)
Ëœğœ‡ğ‘›denotes the reparameterized mean of ğ‘(ğ’™ğ‘›âˆ’1|ğ’™0,ğ’™ğ‘›). There-
after, for 1â‰¤ğ‘›â‰¤ğ‘âˆ’1, the the parameterization of Lğ‘‰ğ¿ğµ at stepğ‘›can be calculated by pushing ğğœƒ(ğ’™ğ‘›,ğ‘›)to be close to Ëœğğ‘›(ğ’™ğ‘›,ğ’™0).
Then, we can similarly factorize ğğœƒ(ğ’™ğ‘›,ğ‘›)via
ğğœƒ(ğ’™ğ‘›,ğ‘›)=âˆšğ›¼ğ‘›(1âˆ’Â¯ğ›¼ğ‘›âˆ’1)
1âˆ’Â¯ğ›¼ğ‘›ğ’™ğ‘›+âˆšÂ¯ğ›¼ğ‘›âˆ’1(1âˆ’ğ›¼ğ‘›)
1âˆ’Â¯ğ›¼ğ‘›ğ’‡ğœƒ(ğ’™ğ‘›,ğ‘›)(14)
whereğ‘“ğœƒ(ğ’™ğ‘›,ğ‘›)is the predicted ğ’™0based on ğ’™ğ‘›and diffusion step
ğ‘›. And theLğ‘‰ğ¿ğµ at stepğ‘›can be formula as :
Lğ‘‰ğ¿ğµğ‘›=Ex0
logğ‘(xğ‘›|x0,xğ‘›+1)
ğ‘ğœƒ(xğ‘›|xğ‘›+1)
=Ex01
2||ğœğœƒ||2||Ëœğœ‡ğ‘›(xğ‘›,x0)âˆ’ğœ‡ğœƒ(xğ‘›,ğ‘›)||2
=Ex01
2||ğœğœƒ||2||âˆšğ›¼ğ‘›(1âˆ’Â¯ğ›¼ğ‘›âˆ’1)
1âˆ’Â¯ğ›¼ğ‘›xğ‘›+âˆšÂ¯ğ›¼ğ‘›âˆ’1ğ›½ğ‘›
1âˆ’Â¯ğ›¼ğ‘›x0âˆ’
(âˆšğ›¼ğ‘›(1âˆ’Â¯ğ›¼ğ‘›âˆ’1)
1âˆ’Â¯ğ›¼ğ‘›xğ‘›+âˆšÂ¯ğ›¼ğ‘›âˆ’1ğ›½ğ‘›
1âˆ’Â¯ğ›¼ğ‘›ğ‘“ğœƒ(xğ‘›,ğ‘›))||2
=âˆšÂ¯ğ›¼ğ‘›âˆ’1ğ›½ğ‘›
1âˆ’Â¯ğ›¼ğ‘›
2||ğœğœƒ||2Ex0[||x0âˆ’ğ‘“ğœƒ(xğ‘›,ğ‘›)||2],(15)
In practice, to keep training stability and simplify the calculation, we
following the previous work [ 35], ignore the learning of ğˆğœƒ(ğ’™ğ‘›,ğ‘›)
inğ‘ğœƒ(ğ’™ğ‘›âˆ’1|ğ’™ğ‘›)of Eq. (6) and directly set ğˆğœƒ(ğ’™ğ‘›,ğ‘›)=Ëœğ›½ğ‘›.Then the
optimization of training loss can be further simplified as:
minLVLB(ğ’™0;ğœƒ)=min"
||Ëœğœ‡(xğ‘)||2+ğ‘âˆ‘ï¸
ğ‘›=2||x0âˆ’ğ‘“ğœƒ(xğ‘›,ğ‘›)||2#
â†’min"ğ‘âˆ‘ï¸
ğ‘›=2||x0âˆ’ğ‘“ğœƒ(xğ‘›,ğ‘›)||2# (16)
Optimization of Conda. In conclusion, we combine and min-
imize the loss of the conditional diffusion model and VAE via
Lğ‘‰ğ¿ğµ(ğ’™0;ğœƒ)+ğœ†Â·Lğ‘£ğ‘ğ‘’(ğ’”;ğœ™,ğœ“)to optimize Conda, where the hyper-
parameterğœ†ensures the two terms in the same magnitude.
Alternating training. Unlike the common diffusion models that
are trained for the direct generation of raw graph data through
pre-training, Conda requires the historical neighbor sequence em-
beddings of nodes obtained through the CTDG encoder before
performing the diffusion process. Therefore, we utilize alternative
training method to alternatively train the CTDG model and Conda.
Here we briefly describe the training process. Initially, the CTDG
modelğœ‰is trained by minimizing the Lğ‘ğ‘¡ğ‘‘ğ‘” forğ‘…ğ‘ğ‘¡ğ‘‘ğ‘”rounds. Then
we insert Conda into the intermediate layer of the CTDG model and
train theğœƒ,ğœ™,ğœ“ according toLğ‘‰ğ¿ğµ(ğ’™0;ğœƒ)+ğœ†Â·Lğ‘£ğ‘ğ‘’(ğ’”;ğœ™,ğœ“)with
theğœ‰frozen forğ‘…ğ‘ğ‘œğ‘›ğ‘‘ğ‘ rounds. Next, we train the CTDG model ğœ‰
again forğ‘…ğ‘ğ‘¡ğ‘‘ğ‘”rounds with the ğœƒ,ğœ™,ğœ“ frozen. At this point, Conda
is in the inference phase, used to generate augmented historical
neighbor embeddings via the reverse process. The above process
will be repeated several times.
4 EXPERIMENTS
In this section, we evaluate the performance of our method on link
prediction task across various CTDG models. All the experiments
are conducted on open CTDG datasets.
2903Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
MethodWiki_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1 Social.Evo_0.1
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
JODIE 85.57Â±1.77 89.16Â±0.51 94.01Â±1.02 94.82Â±0.81 70.57Â±0.15 75.44Â±0.09 84.43Â±0.62 87.44Â±0.25 64.94Â±1.22 66.17Â±1.14 77.11Â±0.90 82.86Â±0.76
JODIE+Conda 88.63Â±0.93 90.52Â±0.40 94.55Â±0.82 94.98Â±0.65 72.11Â±0.47 75.79Â±0.42 84.57Â±0.70 87.71Â±0.59 65.09Â±1.24 66.25Â±1.17 78.74Â±0.71 83.46Â±0.66
DyRep 93.86Â±0.06 93.37Â±0.09 94.24Â±0.75 94.64Â±0.29 71.45Â±1.36 76.81Â±1.33 69.19Â±0.90 73.70Â±0.71 65.86Â±0.54 66.61Â±0.59 77.57Â±0.64 82.99Â±0.60
DyRep+Conda 94.05Â±0.13 93.89Â±0.22 94.84Â±0.64 94.98Â±0.22 72.88Â±1.12 77.02Â±1.07 69.47Â±0.94 73.91Â±0.85 66.37Â±0.51 66.94Â±0.48 78.97Â±0.43 83.62Â±0.37
TGAT 93.16Â±0.29 93.25Â±0.30 94.02Â±0.12 94.01Â±0.13 74.44Â±1.27 74.13Â±1.92 73.54Â±1.05 73.01Â±1.22 69.93Â±0.30 70.42Â±0.33 80.03Â±0.46 85.09Â±0.41
TGAT+Conda 94.02Â±0.27 94.11Â±0.29 94.88Â±0.10 94.90Â±0.09 76.15Â±1.43 76.97Â±1.85 73.99Â±1.10 73.74Â±1.18 69.94Â±0.28 70.48Â±0.41 81.79Â±0.60 85.83Â±0.57
TGN 93.01Â±1.22 92.77Â±1.05 94.94Â±0.09 94.23Â±0.12 77.30Â±0.50 75.62Â±0.72 88.00Â±0.99 87.73Â±1.14 73.61Â±0.74 72.75Â±0.68 78.79Â±0.64 78.60Â±0.48
TGN+Conda 93.47Â±1.16 92.93Â±1.09 95.50Â±0.14 95.06Â±0.18 78.63Â±0.75 76.52Â±0.81 88.22Â±0.92 87.79Â±1.06 74.24Â±0.80 73.31Â±0.75 80.20Â±0.57 80.03Â±0.48
TCL 93.48Â±0.27 92.98Â±0.23 95.05Â±0.11 94.92Â±0.12 74.12Â±0.74 78.10Â±0.65 85.14Â±1.54 84.87Â±1.50 60.07Â±0.15 58.97Â±0.17 83.57Â±1.22 85.41Â±1.04
TCL+Conda 94.04Â±0.20 93.75Â±0.17 95.96Â±0.08 95.79Â±0.07 76.31Â±1.20 79.55Â±1.13 85.79Â±1.37 85.40Â±1.28 60.15Â±0.44 59.21Â±0.45 86.09Â±1.25 88.45Â±1.17
GraphMixer 94.02Â±0.13 93.73Â±0.12 94.93Â±0.06 94.62Â±0.06 74.15Â±1.92 78.07Â±1.67 90.10Â±1.51 89.83Â±1.42 71.11Â±0.07 70.33Â±0.09 82.40Â±1.09 86.24Â±0.92
GraphMixer+Conda 94.61Â±0.19 94.26Â±0.20 95.17Â±0.09 94.85Â±0.08 75.24Â±1.66 78.79Â±1.42 90.33Â±1.70 90.00Â±1.64 71.70Â±0.18 70.85Â±0.19 83.89Â±0.84 88.31Â±0.77
DyGFormer 95.74Â±0.11 95.54Â±0.09 96.01Â±0.08 96.00Â±0.07 75.47Â±0.96 77.49Â±0.81 91.02Â±0.85 90.74Â±0.67 77.86Â±0.14 77.01Â±0.10 84.25Â±0.73 87.64Â±0.62
DyGFormer+Conda 96.68Â±0.14 96.32Â±0.13 96.68Â±0.14 96.55Â±0.12 76.12Â±1.41 77.98Â±1.33 91.09Â±1.00 90.97Â±0.92 78.84Â±0.35 78.11Â±0.27 85.64Â±0.58 88.78Â±0.40
Table 1: Experiments results on dataset with 0.1 ratio of train set
MethodWiki_0.3 Reddit_0.3 MOOC_0.3 UCI_0.3 LastFM_0.3 Social.Evo_0.3
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
JODIE 90.23Â±0.41 91.47Â±0.35 95.63Â±0.37 95.72Â±0.33 72.28Â±0.64 75.41Â±0.59 85.17Â±0.88 87.45Â±0.27 65.62Â±1.54 67.04Â±1.44 78.65Â±0.58 83.43Â±0.54
JODIE+Conda 91.54Â±0.55 92.01Â±0.48 96.55Â±0.36 96.60Â±0.28 73.75Â±0.85 75.97Â±0.54 85.67Â±1.33 87.79Â±1.01 66.13Â±1.87 67.75Â±1.60 79.42Â±0.78 84.05Â±0.70
DyRep 94.11Â±0.16 93.96Â±0.13 95.78Â±0.34 95.90Â±0.15 73.46Â±1.17 78.04Â±1.11 71.40Â±0.75 75.71 Â±1.57 67.41Â±0.86 68.02Â±0.91 77.12Â±0.50 78.51Â±0.48
DyRep+Conda 94.42Â±0.33 94.15Â±0.29 96.58Â±0.28 96.63Â±0.21 74.23Â±1.07 77.69Â±1.05 70.33Â±0.27 74.31Â±1.40 67.94Â±0.75 68.35Â±0.92 80.65Â±0.63 81.00Â±0.57
TGAT 94.33Â±0.25 94.10Â±0.20 95.96Â±0.08 96.02Â±0.06 78.31Â±0.97 80.11Â±0.78 71.04Â±0.43 72.43Â±0.63 70.65Â±0.38 70.22Â±0.40 82.59Â±0.78 86.97Â±0.74
TGAT+Conda 94.99Â±0.20 94.58Â±0.19 96.58Â±0.10 96.77Â±0.09 80.04Â±1.23 80.85Â±1.10 71.44Â±0.50 72.69Â±0.65 71.64Â±0.27 71.08Â±0.31 83.22Â±0.80 87.41Â±0.79
TGN 95.90Â±0.32 95.66Â±0.30 96.25Â±0.13 96.12Â±0.10 82.31Â±1.86 81.49Â±2.03 88.96Â±0.57 88.02Â±0.51 75.68Â±1.74 74.95Â±1.55 81.85Â±0.32 85.73Â±0.28
TGN+Conda 96.52Â±0.37 96.24Â±0.35 96.97Â±0.16 96.78Â±0.15 82.55Â±1.65 81.63Â±1.79 88.92Â±0.68 88.09Â±0.64 76.61Â±1.66 75.13Â±1.47 84.20Â±0.44 88.36Â±0.35
TCL 95.75Â±0.10 95.01Â±0.11 96.50Â±0.06 96.42Â± 0.05 80.20Â±0.14 81.98Â±0.21 88.62Â±0.24 87.84Â±0.19 62.23Â±0.88 59.83Â±0.75 91.57Â±0.25 93.64Â±0.22
TCL+Conda 96.23Â±0.20 95.58Â±0.24 97.02Â±0.12 96.87Â±0.11 80.47Â±0.35 82.24Â±0.32 88.69Â±0.30 87.93Â±0.27 62.79Â±1.14 60.56Â±1.02 91.88Â±0.23 93.95Â±0.19
GraphMixer 95.72Â±0.05 95.47Â±0.02 96.46Â±0.03 96.12Â±0.01 79.72Â±0.37 82.22Â±0.26 91.14Â±1.03 90.68Â±0.94 72.17Â±0.14 72.05Â±0.13 87.86Â±0.63 90.73Â±0.58
GraphMixer+Conda 95.99Â±0.05 95.75Â±0.03 96.89Â±0.04 96.65Â±0.03 80.01Â±0.58 82.52Â±0.51 91.84Â±1.00 90.87Â±0.93 72.62Â±0.16 72.17Â±0.16 89.05Â±0.57 91.89Â±0.53
DyGFormer 96.50Â±0.06 96.36Â±0.05 97.14Â±0.03 97.07Â±0.03 79.42Â±0.49 83.09Â±0.31 92.87Â±0.62 92.45Â±0.53 82.57Â±0.48 81.79Â±0.40 88.91Â±0.47 91.54Â±0.43
DyGFormer+Conda 97.11Â±0.09 96.96Â±0.10 97.45Â±0.04 97.31Â±0.04 81.00Â±0.83 83.97Â±0.53 92.94Â±0.48 92.50Â±0.40 83.17Â±0.36 82.31Â±0.32 90.02Â±0.53 92.79Â±0.50
Table 2: Experiments results on the dataset with 0.3 ratio of train set
4.1 Experiment settings
4.1.1 Datasets. We utilize six open CTDG datasets: Wiki, RED-
DIT, MOOC, LastFM, Social Evo, and UCI in the experiments. The
detailed description and the statistics of the datasets are shown in
Table 7 in Appendix A.1. The sparsity of the graphs is quantified us-
ing the density score, calculated as2|ğ¸|
|ğ‘‰|(|ğ‘‰|âˆ’1), where|ğ¸|and|ğ‘‰|
represent the number of links and nodes in the training set, respec-
tively. These datasets are split into three chronological segments
for training, validation, and testing with ratios of 10%-10%-80% and
30%-20%-50%. To differentiate the datasets with different splitting
ratios, the dataset names are written with suffix 0.1 and 0.3.
4.1.2 Baselines. Since Conda is agnostic to model structure, to
evaluate the performance of our GDA method, we conduct experi-
ments on several state-of-the-art CTDG models, including JODIE [ 23],DyRep [ 31], TGAT [ 6], TGN [ 29], TCL [ 34], GraphMixer [ 5], DyG-
Former [ 44]. We also combine our method with other data aug-
mentation methods: DropEdge [ 28], DropNode [ 8], and MeTA [ 36].
Detailed descriptions of these baselines and GDA methods can be
found in Appendix A.2 and Appendix A.3, respectively.
4.1.3 Evaluation and hyper-parameter settings. We evaluate
Conda on the task of link prediction. As for the evaluation met-
rics, we follow the previous works [ 36,44], employing Average
Precision (AP) and Area Under the Receiver Operating Character-
istic Curve (A-R) as the evaluation metrics. We perform the grad
search to find the best settings of some critical hyper-parameters.
We vary the learning rates of all baselines in {1ğ‘’âˆ’4,1ğ‘’âˆ’3}, the
dropout rate of dropout layer in {0.0,0.1,0.2,0.3,0.4,0.5}, the num-
berğ¿of sampled neighbors and the diffusion length ğ‘‘ğ‘–ğ‘“ğ‘“ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›
2904KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
MethodWiki_0.3 Reddit_0.3 MOOC_0.3 UCI_0.3 LastFM_0.3 Social.Evo_0.3
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
TGN 95.90Â±0.32 95.66Â±0.30 96.25Â±0.13 96.12Â±0.10 82.31Â±1.86 81.49Â±2.03 88.96Â±0.57 88.02Â±0.51 75.68Â±1.74 74.95Â±1.55 81.85Â±0.32 85.73Â±0.28
TGN+DropEdge 96.11Â±0.69 95.69Â±0.92 96.79Â±0.86 96.72Â±0.72 82.46Â±1.91 81.54Â±1.54 88.93Â±0.90 88.09Â±0.89 75.91Â±1.92 75.02Â±1.50 82.15Â±0.51 86.20Â±0.48
TGN+DropNode 96.03Â±0.73 95.97Â±0.76 96.87Â±0.87 96.63Â±0.63 82.35Â±1.35 81.62Â±1.62 88.96Â±0.95 88.05Â±0.50 75.77Â±1.76 75.00Â±1.00 82.31Â±0.67 86.55Â±0.60
TGN+MeTA 96.20Â±0.91 95.97Â±0.97 96.75Â±0.75 96.61Â±0.61 82.31Â±1.31 81.60Â±1.60 88.92Â±0.92 88.05Â±0.54 76.47Â±1.47 75.12Â±1.12 83.27Â±0.63 86.94Â±0.56
TGN+Conda 96.52Â±0.37 96.24Â±0.35 96.97Â±0.16 96.78Â±0.15 82.55Â±1.65 81.63Â±1.79 88.92Â±0.68 88.09Â±0.64 76.61Â±1.66 75.13Â±1.47 84.20Â±0.44 88.36Â±0.35
GraphMixer 95.72Â±0.05 95.47Â±0.02 96.46Â±0.03 96.12Â±0.01 79.72Â±0.37 82.22Â±0.26 91.14Â±1.03 90.68Â±0.94 72.17Â±0.14 72.05Â±0.13 87.86Â±0.63 90.73Â±0.58
GraphMixer+DropEdge 95.98Â±0.27 95.70Â±0.24 96.63Â±0.62 96.49Â±0.49 79.74Â±0.73 82.48Â±0.47 91.45Â±0.45 90.83Â±0.83 72.31Â±0.30 72.08Â±0.18 88.91Â±0.90 0.84Â±0.83
GraphMixer+DropNode 95.90Â±0.32 95.60Â±0.30 96.72Â±0.72 96.32Â±0.31 79.75Â±0.74 82.42Â±0.41 91.47Â±0.47 90.74Â±0.74 72.25Â±0.25 72.17Â±0.16 88.76Â±0.76 90.88Â±0.87
GraphMixer+MeTA 95.97Â±0.97 95.47Â±0.47 96.59Â±0.59 96.49Â±0.48 79.87Â±0.86 82.44Â±0.43 91.29Â±0.29 90.81Â±0.81 72.18Â±0.17 72.15Â±0.14 88.34Â±0.94 90.97Â±0.96
GraphMixer+Conda 95.99Â±0.05 95.75Â±0.03 96.89Â±0.04 96.65Â±0.03 80.01Â±0.58 82.52Â±0.51 91.84Â±1.00 90.87Â±0.93 72.62Â±0.16 72.17Â±0.16 89.05Â±0.57 91.89Â±0.53
DyGFormer 96.50Â±0.06 96.36Â±0.05 97.14Â±0.03 97.07Â±0.03 79.42Â±0.49 83.09Â±0.31 92.87Â±0.62 92.45Â±0.53 82.57Â±0.48 81.79Â±0.40 88.91Â±0.47 91.54Â±0.43
DyGFormer+DropEdge 96.71Â±0.24 96.48Â±0.21 97.35Â±0.14 97.12Â±0.12 80.37Â±0.63 83.94Â±0.50 92.88Â±0.55 92.46Â±0.32 82.84Â±0.62 82.17Â±0.20 89.09Â±0.62 91.70Â±0.49
DyGFormer+DropNode 96.72Â±0.24 96.51Â±0.21 97.43Â±0.15 97.20Â±0.15 79.52Â±0.64 83.86Â±0.46 92.93Â±0.19 92.48Â±0.58 82.83Â±0.64 82.14Â±0.32 89.04Â±0.84 92.24Â±0.51
DyGFormer+MeTA 96.57Â±0.33 96.38Â±0.28 97.26Â±0.79 97.19Â±0.62 79.53Â±1.05 83.40Â±0.82 92.60Â±0.94 92.49Â±0.05 83.09Â±0.51 81.94Â±0.14 89.93Â±0.53 91.64Â±0.21
DyGFormer+Conda 97.11Â±0.09 96.96Â±0.10 97.45Â±0.04 97.31Â±0.04 81.00Â±0.83 83.97Â±0.53 92.94Â±0.48 92.50Â±0.40 83.17Â±0.36 82.31Â±0.32 90.02Â±0.53 92.79Â±0.50
Table 3: Performance comparison of baseline and baseline with different GDA methods on the dataset with 0.3 ratio of train set
MethodWiki_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1 Social.Evo_0.1
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
TGN 93.01Â±1.22 92.77Â±1.05 94.94Â±0.09 94.23Â±0.12 77.30Â±0.50 75.62Â±0.72 88.00Â±0.99 87.73Â±1.14 73.61Â±0.74 72.75Â±0.68 78.79Â±0.64 78.60Â±0.48
TGN+DropEdge 93.18Â±1.40 92.90Â±1.51 95.12Â±0.58 94.38Â±0.33 76.62Â±1.98 75.60Â±1.62 87.95Â±1.69 87.73Â±1.25 74.12Â±1.05 72.85Â±1.77 78.66Â±1.58 78.06Â±1.10
TGN+DropNode 92.82Â±1.42 92.71Â±1.48 95.26Â±0.69 94.44Â±0.41 76.90Â±1.54 75.93Â±1.91 88.04Â±1.28 87.77Â±1.90 73.86Â±1.69 73.23Â±1.28 79.21Â±1.37 79.50Â±1.21
TGN+MeTA 92.96Â±1.45 92.85Â±1.37 95.20Â±0.84 93.83Â±0.50 78.03Â±1.88 76.38Â±1.88 87.93Â±1.87 87.73Â±1.47 74.15Â±1.03 73.11Â±1.19 79.43Â±1.31 78.57Â±1.08
TGN+Conda 93.47Â±1.16 92.93Â±1.09 95.50Â±0.14 95.06Â±0.18 78.63Â±0.75 76.52Â±0.81 88.22Â±0.92 87.79Â±1.06 74.24Â±0.80 73.31Â±0.75 80.20Â±0.57 80.03Â±0.48
GraphMixer 94.02Â±0.13 93.73Â±0.12 94.93Â±0.06 94.62Â±0.06 74.15Â±1.92 78.07Â±1.67 90.10Â±1.51 89.83Â±1.42 71.11Â±0.07 70.33Â±0.09 82.40Â±1.09 86.24Â±0.92
GraphMixer+DropEdge 94.34Â±0.71 93.61Â±0.43 94.97Â±0.31 94.84Â±0.24 75.02Â±1.72 78.06Â±1.11 90.14Â±1.37 89.77Â±1.30 70.75Â±0.68 70.51Â±0.40 82.88Â±1.16 85.90Â±1.02
GraphMixer+DropNode 93.97Â±0.74 93.89Â±0.45 94.90Â±0.39 94.82Â±0.40 73.54Â±1.98 78.15Â±1.06 90.06Â±1.64 89.61Â±1.50 71.28Â±0.73 70.41Â±0.70 82.67Â±1.94 86.12Â±1.82
GraphMixer+MeTA 93.72Â±0.96 94.21Â±0.72 94.98Â±0.39 94.58Â±0.33 74.35Â±1.55 77.97Â±1.44 90.11Â±2.00 89.86Â±1.64 71.70Â±0.41 70.75Â±0.58 82.74Â±1.88 87.02Â±1.61
GraphMixer+Conda 94.61Â±0.19 94.26Â±0.20 95.17Â±0.09 94.85Â±0.08 75.24Â±1.66 78.79Â±1.42 90.33Â±1.70 90.00Â±1.64 71.70Â±0.18 70.85Â±0.19 83.89Â±0.84 88.31Â±0.77
DyGFormer 95.74Â±0.11 95.54Â±0.09 96.01Â±0.08 96.00Â±0.07 75.47Â±0.96 77.49Â±0.81 91.02Â±0.85 90.74Â±0.67 77.86Â±0.14 77.01Â±0.10 84.25Â±0.73 87.64Â±0.62
DyGFormer+DropEdge 96.05Â±0.57 95.68Â±0.33 96.22Â±0.42 95.90Â±0.30 75.92Â±1.15 77.76Â±1.98 91.05Â±1.41 90.87Â±1.26 76.93Â±1.30 77.91Â±1.81 85.18Â±1.55 88.71Â±1.54
DyGFormer+DropNode 96.02Â±0.61 95.68Â±0.52 95.54Â±0.45 95.50Â±0.36 74.94Â±1.87 77.20Â±1.30 90.89Â±1.88 90.74Â±1.06 78.20Â±1.15 77.17Â±1.63 83.76Â±1.75 88.16Â±1.34
DyGFormer+MeTA 96.24Â±0.58 95.83Â±0.33 96.05Â±0.61 96.00Â±0.49 75.70Â±1.21 77.64Â±1.00 90.76Â±1.64 90.96Â±1.45 77.96Â±1.27 77.99Â±1.25 85.48Â±1.72 88.59Â±1.55
DyGFormer+Conda 96.68Â±0.14 96.32Â±0.13 96.68Â±0.14 96.55Â±0.12 76.12Â±1.41 77.98Â±1.33 91.09Â±1.00 90.97Â±0.92 78.84Â±0.35 78.11Â±0.27 85.64Â±0.58 88.78Â±0.40
Table 4: Performance comparison of baseline and baseline with different GDA methods on the dataset with 0.1 ratio of train set
in{10,20,32,64,128,256,512}and{1,ğ¿
16,ğ¿
8,ğ¿
4,ğ¿
3}, respectively. The
number of diffusion steps ğ‘is fixed at 50, respectively. Besides, the
noise scaleğ‘˜is tuned in{1ğ‘’âˆ’5,1ğ‘’âˆ’4,1ğ‘’âˆ’3,1ğ‘’âˆ’2}. Regarding GDA
methods used for comparison, we vary the drop rate for Dropedge
and Dropnode in{0.1,0.2,0.3,0.4,0.5}. As for MeTA, we control
the magnitude of the three DA strategies with a unified ğ‘, vary in
{0.1,0.2,0.3}, and follow the setting in its paper. More details are
listed in the Appendix.
The configurations of the baselines align with those specified
in their respective papers. The model that achieves the highest
performance on the validation set is selected for testing. We conduct
five runs of each method with different seeds and report the averageperformance to eliminate deviations. All experiments are performed
on a server with 8 NVIDIA A100-SXM4 40GB GPUs.
4.2 Performance Comparison and Discussion
In this section, in order to verify the effectiveness of Conda, we
integrate it into each baseline across six datasets with different
ratios of the train set for the link prediction task. As shown in
Table 1 and Table 2, mean and standard deviations of five runs are
reported, and the best results are highlighted in bold font. The
experiment results clearly demonstrate that Conda improves the
performance of all the baselines with respect to all datasets with
different ratios of train sets.
2905Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
From the results, we can observe that with the ratio of train set
decreasing, the performance of each baseline also decreases. Specif-
ically, when the training data is relatively sufficient, all baselines
achieve great performance. However, when training data is more
limited, the performance of most baselines drops significantly (e.g.
JODIE on Wiki_0.1, all baselines on Reddit_0.1, MOOC_0.1 and
SocialEvo_0.1). The possible reason is that the paradigm of CTDG
models is to use historical data to obtain target node embeddings.
When historical data is limited, the quality of the obtained embed-
dings cannot be guaranteed. In addition, the data distribution of the
testing set could be diverse from the training set. This would lead to
the model overfitting the historical data and cannot be generalized
to future data. By using Conda, the modelâ€™s performance improves.
It is achieved by utilizing the conditional diffusion model to gener-
ate augmented historical neighbor embeddings of the target node
during the training of the CTDG model. In Conda, the mechanism
of partial noise addition and conditional inputs ensures that the
newly generated embeddings are not random. Instead, they closely
resemble the embeddings of recently interacted neighbors. Con-
sequently, this guarantees high-quality embeddings of the nodeâ€™s
historical neighbors following augmentation.
In addition, we also compare Conda with three GDA methods
to show the superiority of our GDA method. Overall, we find that
Conda can consistently outperform competing GDA methods. Fur-
thermore, the variance in the results indicates that Conda provides
stable improvements in model performance, unlike other GDA
methods that rely on random augmentations and thus yield erratic
results. This stability is particularly evident in the setting with more
sparse training data (e.g. 0.1 train set ratio). By analyzing the experi-
ments on datasets with a 0.3 train set ratio, itâ€™s obvious that all GDA
methods can improve baselinesâ€™ performance on most datasets to
some extent, but the improvement on UCI_0.3 is relatively minor
compared to SocialEvo_0.3. The reason may be that SocialEvo has
a smaller sparsity and longer interaction sequences than UCI. This
phenomenon suggests that training the CTDG model indeed re-
quires sufficient historical interaction data. Moreover, we notice
that MeTA improves baselinesâ€™ performance than DropEdge and
DropNode. This might be because MeTA considers the time pertur-
bation, which is crucial for dynamic graph learning. Additionally,
unlike Dropedge and DropNode, which essentially remove edges,
MeTA maintains or even increases the number of interaction data
samples before and after augmentation by simultaneously adding
and removing edges. However, due to the combination of multiple
augmentation strategies, the results of MeTA also introduce a larger
variance, indicating that it is hard to control.
Next, we analyze the results of the experiment on datasets with a
0.1 train set ratio. Table 4 clearly shows that, apart from our method,
which still achieves stable performance improvements, the other
three methods frequently resulted in outcomes even worse than
the original baseline. The main reason is that at such a low ratio
of train sets, the datasets become extremely sparse. At this level,
employing random perturbations like edge deletion further reduces
the already insufficient data samples and risks removing essential
interaction. However, our method maintains stable performance
gains by controlling the diffused sequence length. Even though
the dataset becomes sparser and the historical neighbors of nodes
decrease, by simultaneously reducing the length of the diffusedWIKI_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1
GraphMixer 94.02Â±0.13 94.93Â±0.06 74.15Â±1.92 90.10Â±1.51 71.11Â±0.07
+Conda w/o VAE 94.58Â±0.17 95.12Â±0.08 74.90Â±2.17 90.30Â±1.67 71.65Â±0.18
+Conda w/o diffusion 93.87Â±0.25 94.80Â±0.13 74.77Â±1.80 90.15Â±1.91 71.43Â±0.30
+Conda 94.61Â±0.1995.17Â±0.0975.24Â±1.6690.33Â±1.7071.70Â±0.18
Table 5: Experiment results on AP of different variants
WIKI_0.1 MOOC_0.1 WIKI_0.3 MOOC_0.3
GraphMixer 94.02Â±0.13 74.15Â±1.92 95.72Â±0.05 79.72Â±0.37
+Conda (E2E) 94.09 Â±0.58 73.62Â± 1.90 95.49Â±0.13 78.53 Â±0.90
+Conda (AT) 94.61Â±0.19 75.24Â±1.66 95.99Â±0.05 80.01Â±0.58
Table 6: AP of different training approaches
sequence, we still ensure a stable, albeit somewhat reduced, level of
improvement compared to the results on datasets with a 0.3 ratio of
the train set. The effect of the diffused length on model performance
will be analyzed in detail in the following section.
4.3 Ablation analysis
We conduct an ablation study to assess the contributions of the
VAE and diffusion components within the Conda module. The
results, summarized in Table 5, compare the baseline GraphMixer,
add Conda without VAE (+Conda w/o VAE), add Conda without
diffusion (+Conda w/o diffusion), and add the full Conda module
(+Conda).
It is obvious that the full Conda module achieves the highest AP
scores on all datasets, and consistently outperforms all variants,
indicating the importance of both VAE and diffusion components.
Removing the diffusion component results in a performance drop,
particularly on WIKI_0.1 (from 94.61 to 93.87), highlighting the dif-
fusionâ€™s role in generating effective latent representation. Similarly,
removing the VAE component also decreases AP scores, especially
on MOOC_0.1 (from 75.24 to 74.77). In conclusion, the combination
of the VAE and diffusion model results in superior performance, as
shown by consistently higher AP scores compared to the ablated
variants. This synergy is crucial for optimal model performance.
In addition to the ablation study of the Conda module, we also
explore different training approaches to further understand their
impact on model performance. As shown in Table 6, we conduct
experiments on GraphMixer+Conda with two different training
approaches: end-to-end training (E2E) and alternative training (AT).
The results indicate that the E2E training approach results in a
significant performance decline across all datasets. For example,
on the MOOC_0.1 and MOOC_0.3, the AP drops from 75.24.61
(AT) to 73.62 (E2E) and from 80.01 (AT) to 78.53 E2E), respectively.
This decline can be attributed to conflicting objectives between the
Conda module and the CTDG model. The Conda module aims to
generate embeddings similar to the original data, while the CTDG
model seeks to learn from diverse augmented data close to reality
to enhance performance. When integrated into end-to-end training,
these conflicting goals prevent the model from optimally achiev-
ing both objectives, leading to suboptimal or even diminishing
performance.
2906KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
Figure 2: Performance comparison of different diffused length ğ‘‘ğ‘–ğ‘“ğ‘“ in DyGFormer+Conda on Reddit, MOOC, SocialEvo with
different train set ratios. The blue and orange dashed lines respectively represent the baselineâ€™s AP and A-R values.
Figure 3: Performance comparison of different noise scale ğ‘˜in GraphMixer+Conda on Reddit_0.3, MOOC_0.3, SocialEvo_0.3.
ğ‘˜=0equivalents to the baseline.
4.4 Sensitivity of Hyper-Parameters
In this section, we conduct experiments to investigate the Sensitiv-
ity of two important hyper-parameters in our proposed method:
diffused sequence length ğ‘‘ğ‘–ğ‘“ğ‘“ and noise scale ğ‘˜.
We conducted experiments with DyGFormer on Reddit, MOOC,
and SocialEvo datasets, as DyGFormer tends to yield better results
from longer historical neighbor sequences on these datasets, which
can effectively show the effect on the model performance of using
varying diffused lengths across the historical neighbor sequence.
We also provide the optimal configurations of the number ğ¿of
sampled neighbors and diffused sequence length ğ‘‘ğ‘–ğ‘“ğ‘“ of different
baselines on different datasets in Appendix A.4. Specifically, we first
setğ¿, the number of sampled historical neighbors by DyGFormeron each dataset, to the optimal settings which can be found in the
Appendix A.4. Note that in practice, if the target nodeâ€™s historical
neighbors are fewer than ğ¿, we use zero-padding to fill the gap.
Then, we vary ğ‘‘ğ‘–ğ‘“ğ‘“ in the set 1,ğ¿
16,ğ¿
8,ğ¿
4,ğ¿
3, with results as shown
in the Figure 2. From the Figure, we can observe that the model per-
formance is best when ğ‘‘ğ‘–ğ‘“ğ‘“ =ğ¿
8. However, as ğ‘‘ğ‘–ğ‘“ğ‘“ increases, such
as whenğ‘‘ğ‘–ğ‘“ğ‘“ =ğ¿
3, the model performance significantly decreases,
even falling below the baseline. This phenomenon is particularly
pronounced when the training set ratio of the dataset is 0.1. The
underlying reason is likely due to the fact that there are few actual
neighbors in the sampled historical neighbor sequence of the node,
2907Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
with the latter part of the sequence being filled by zero-padding.
Therefore, the majority of the conditional inputs of the reverse
process are meaningless, resulting in the generated augmented
embeddings being too distant from the current node state repre-
sentation, leading to a decline in model performance. When ğ‘‘ğ‘–ğ‘“ğ‘“
is less thanğ¿
8, the model performance is slightly worse than atğ¿
8
but still outperforms the baseline. This phenomenon indicates that
Conda can consistently generate positive augmentation when it
controls the length of diffused sequence embedding to be relatively
short.
For the noise scale, we conduct experiments on GraphMixer+Conda
with Reddit_0.3, MOOC_0.3, and SocialEvo_0.3 . As illustrated in
Figure 3, we can observe that, as the noise scale increases, the per-
formance first rises compared to training without noise ( ğ‘˜=0),
verifying the effectiveness of denoising training. However, enlarg-
ing noise scales degrades the performance due to corrupting the
pattern of interaction sequence. Hence, we also should carefully
choose a relatively small noise scale (e.g. 1ğ‘’âˆ’4).
5 RELATED WORK
5.1 Graph Data Augmentation
There is a growing interest among researchers in graph data aug-
mentation (GDA) methods since they offer an attractive solution in
denoising and generally augmenting graph data. GDA methods can
be categorized into structure-oriented and feature-oriented meth-
ods by the data modality that they aim to manipulate. Structure-
oriented GDA methods often modify the graph structure via adding
or removing edges and nodes. Zhao et al . [47] and Gasteiger et al .
[9]modify the graph structure and used the modified graph for
training/inference, Feng et al . [8], Rong et al . [28] randomly drop
edges/nodes from the observed training graph. Wang et al . [38]
utilizes a node-centric strategy to crop a subgraph from the origi-
nal graph while maintaining its connectivity. However, these GDA
methods are usually used in static graphs or DTDG and can not be
directly applied to CTDG due to the lack of consideration of time,
Although Wang et al . [36] introduces MeTA for CTDG model, which
augments CTDG combining three structure-oriented GDA methods
including perturbing time, removing edges, and adding edges with
perturbed time. However, it is limited to apply on CTDG models
with memory modules [ 29,31] because it needs to incorporate a
multi-level memory module to process augmented graphs of vary-
ing magnitudes at different levels. Furthermore, It is widely noticed
that the effectiveness structure-oriented GDA methods requires a
great of specific domain knowledge, necessitating the selection of
diverse augmentation strategy combinations tailored to different
graph datasets.
Feature-oriented methods directly modify or create raw fea-
tures. Hou et al . [15] uses Attribute Masking that randomly mask
node features, Kong et al . [21] augments node features with gradient-
based adversarial perturbations. Itâ€™s worth noting that structure-
oriented and feature-oriented augmentation are also sometimes
combined in some GDA methods. For example, You et al . [43] sum-
marizes four types of graph augmentations to learn the invariant
representation across different augmented view. Wang et al . [39]
changes both the node feature and the graph structure for differentnodes individually and separately, to coordinate DA for different
nodes. However, most of these methods require original features for
nodes or edges. Meanwhile, Most CTDG datasets are attribute-free
graphs. Additionally, there are only a few structure-oriented and
feature-oriented GDA methods that offer rigorous proofs or theoret-
ical bounds (e.g., Evidence Lower Bound). Most rely predominantly
on empirical intuition or constraints from contrastive learning to
achieve positive augmentation.
5.2 Generative Models
Generative models [ 10,19] are powerful tools for learning data
distribution. Recently, researchers have proposed several interest-
ing generative models for graph data generation. Variational graph
auto-encoder (VGAE) [ 20] exploits the latent variables to learn in-
terpretable representations for undirected graphs. Salha et al . [30]
make use of a simple linear model to replace the GCN encoder in
VGAE and reduce the complexity of encoding schemes. Xu et al .
[40]propose a generative GCN model to learn node representations
for growing graphs. ConDgen [ 41] exploits the GCN encoder to han-
dle the invariant permutation for conditional structure generation.
Besides, diffusion-based generative models are shown to be power-
ful in generating high-quality graphs [ 4,13,27,33]. DiGress [ 33],
one of the most advanced graph generative models, employs a dis-
crete diffusion process to progressively add discrete noise to graphs
by either adding or removing edges and altering node categories.
However, these diffusion models rely on continuous Gaussian noise
and do not align well with graph structure. In addition, they are
limited to generating small graphs and can not scale up to large
graphs. Contrary to these approaches mainly focusing on structure
generation, [ 25] pretrains a VAE for node feature generation, which
can serve as a DA method for the downstream backbone models.
However, VAE often uses over-simplified prior and decoder, which
suffers from the trade-off between tractability and representation
ability.
6 CONCLUSION
In this paper, we propose Conda, a novel GDA method designed
to integrate seamlessly into any CTDG model. Conda utilizes a
latent conditional diffusion model to enhance the embeddings of
nodesâ€™ historical neighbor sequences during the training phase of
CTDG models. Unlike structure-oriented and feature-oriented GDA
methods, Conda operates within the latent space rather than the
input space, thereby enabling more subtle modeling of transition in
dynamic graphs. More importantly, Conda employs a conditional
diffusion model to generate high-quality historical neighbor em-
beddings with solid theoretical foundations. Extensive experiments
conducted on various baseline models using real-world datasets
demonstrate the efficacy of our method Conda. In the future, we
aim to extend our method to CTDG with edge deletions.
7 ACKNOWLEDGMENTS
The research presented in this paper is supported in part by the
National Natural Science Foundation of China (Grant No. 62372362)
and the National Natural Science Foundation of China (Grant No.
62366021).
2908KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
REFERENCES
[1]Tanya Y. Berger-Wolf and Jared Saia. 2006. A Framework for Analysis of Dynamic
Social Networks. In Proceedings of the 12th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (Philadelphia, PA, USA) (KDD â€™06).
Association for Computing Machinery, New York, NY, USA, 523â€“528.
[2]Xiaofu Chang, Xuqin Liu, Jianfeng Wen, Shuang Li, Yanming Fang, Le Song, and
Yuan Qi. 2020. Continuous-time dynamic graph learning via neural interaction
processes. In Proceedings of the 29th ACM International Conference on Information
& Knowledge Management. 145â€“154.
[3]Nan Chen, Zemin Liu, Bryan Hooi, Bingsheng He, Rizal Fathony, Jun Hu, and
Jia Chen. 2024. Consistency Training with Learnable Data Augmentation for
Graph Anomaly Detection with Limited Supervision. In The Twelfth International
Conference on Learning Representations.
[4]Xiaohui Chen, Yukun Li, Aonan Zhang, and Li-Ping Liu. 2023. NVDiff: Graph
Generation through the Diffusion of Node Vectors. arXiv:2211.10794 [cs.LG]
[5]Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang
Tong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated Model
Architectures For Temporal Networks? arXiv:2302.11636 (2023).
[6]da Xu, chuanwei ruan, evren korpeoglu, sushant kumar, and kannan achan. 2020.
Inductive representation learning on temporal graphs. In International Conference
on Learning Representations (ICLR).
[7]Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. 2022. Data Augmentation
for Deep Graph Learning: A Survey. SIGKDD Explor. Newsl. 24, 2 (dec 2022),
61â€“77. https://doi.org/10.1145/3575637.3575646
[8]Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang
Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph random neural networks
for semi-supervised learning on graphs. In Proceedings of the 34th International
Conference on Neural Information Processing Systems (Vancouver, BC, Canada)
(NIPSâ€™20). Curran Associates Inc., Red Hook, NY, USA, Article 1853, 12 pages.
[9]Johannes Gasteiger, Stefan WeiÃŸenberger, and Stephan GÃ¼nnemann. 2019. Diffu-
sion improves graph learning. Curran Associates Inc., Red Hook, NY, USA.
[10] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
networks. In Advances in Neural Information Processing Systems.
[11] Derek Greene, DÃ³nal Doyle, and PÃ¡draig Cunningham. 2010. Tracking the
Evolution of Communities in Dynamic Social Networks. In 2010 International
Conference on Advances in Social Networks Analysis and Mining. 176â€“183. https:
//doi.org/10.1109/ASONAM.2010.17
[12] Hongyu Guo and Yongyi Mao. 2021. ifmixup: Towards intrusion-free graph
mixup for graph classification. arXiv e-prints (2021), arXivâ€“2110.
[13] Kilian Konstantin Haefeli, Karolis Martinkus, NathanaÃ«l Perraudin, and Roger
Wattenhofer. 2022. Diffusion Models for Graphs Benefit From Discrete State
Spaces. In The First Learning on Graphs Conference.
[14] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. 2022. G-mixup: Graph
data augmentation for graph classification. In International Conference on Machine
Learning. PMLR, 8230â€“8248.
[15] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang,
and Jie Tang. 2022. GraphMAE: Self-Supervised Masked Graph Autoencoders. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (Washington DC, USA) (KDD â€™22). Association for Computing Machinery,
New York, NY, USA, 594â€“604. https://doi.org/10.1145/3534678.3539321
[16] Zijie Huang, Yizhou Sun, and Wei Wang. 2020. Learning continuous system
dynamics from irregularly-sampled partial observations. Advances in Neural
Information Processing Systems 33 (2020), 16177â€“16187.
[17] Ming Jin, Yuan-Fang Li, and Shirui Pan. 2022. Neural Temporal Walks: Motif-
Aware Representation Learning on Continuous-Time Dynamic Graphs. In Ad-
vances in Neural Information Processing Systems.
[18] Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi,
Peter Forsyth, and Pascal Poupart. 2020. Representation learning for dynamic
graphs: A survey. The Journal of Machine Learning Research 21, 1 (2020), 2648â€“
2720.
[19] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[20] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv
preprint arXiv:1611.07308 (2016).
[21] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem,
Gavin Taylor, and Tom Goldstein. 2020. Flag: Adversarial data augmentation for
graph neural networks. arXiv preprint arXiv:2010.09891 (2020).
[22] Srijan Kumar and Neil Shah. 2018. False Information on Web and Social Media:
A Survey. arXiv:1804.08559 [cs.SI]
[23] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting Dynamic Em-
bedding Trajectory in Temporal Interaction Networks. (08 2019).
[24] Bo Liang, Lin Wang, and Xiaofan Wang. 2022. Autoregressive GNN-ODE GRU
Model for Network Dynamics. arXiv:2211.10594 (2022).
[25] Songtao Liu, Rex Ying, Hanze Dong, Lanqing Li, Tingyang Xu, Yu Rong, Peilin
Zhao, Junzhou Huang, and Dinghao Wu. 2022. Local Augmentation for Graph
Neural Networks. In Proceedings of the 39th International Conference on MachineLearning, Vol. 162. PMLR, 14054â€“14072.
[26] Linhao Luo, Reza Haffari, and Shirui Pan. 2022. Graph Sequential Neural ODE
Process for Link Prediction on Dynamic and Sparse Graphs. (11 2022). https:
//doi.org/10.48550/arXiv.2211.08568
[27] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and
Stefano Ermon. 2020. Permutation Invariant Graph Generation via Score-Based
Generative Modeling. In Proceedings of the Twenty Third International Conference
on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research,
Vol. 108), Silvia Chiappa and Roberto Calandra (Eds.). PMLR, 4474â€“4484.
[28] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. Dropedge: To-
wards deep graph convolutional networks on node classification. In International
Conference on Learning Representation.
[29] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal Graph Networks for Deep Learning
on Dynamic Graphs. (06 2020).
[30] Guillaume Salha, Romain Hennequin, and Michalis Vazirgiannis. 2019. Keep
it simple: Graph autoencoders without graph convolutional networks. arXiv
preprint arXiv:1910.00942 (2019).
[31] Rakshit S. Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.
2019. DyRep: Learning Representations over Dynamic Graphs. In International
Conference on Learning Representations.
[32] Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho
Kannala, and Jian Tang. 2021. Graphmix: Improved training of gnns for semi-
supervised learning. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 35. 10024â€“10032.
[33] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher,
and Pascal Frossard. 2023. DiGress: Discrete Denoising diffusion for graph
generation. In The Eleventh International Conference on Learning Representations.
[34] Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng
He, Le Song, Jingren Zhou, and Hongxia Yang. 2021. Tcl: Transformer-based
dynamic graph modelling via contrastive learning. arXiv:2105.07944 (2021).
[35] Wenjie Wang, Yiyan Xu, Fuli Feng, Xinyu Lin, Xiangnan He, and Tat-Seng Chua.
2023. Diffusion Recommender Model. arXiv:2304.04971 [cs.IR]
[36] Yiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, Siddharth
Bhatia, and Bryan Hooi. 2021. Adaptive data augmentation on temporal graphs.
Advances in Neural Information Processing Systems 34 (2021), 1440â€“1452.
[37] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. 2021.
Inductive representation learning in temporal networks via causal anonymous
walks. arXiv preprint arXiv:2101.05974 (2021).
[38] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2020.
GraphCrop: Subgraph Cropping for Graph Classification. CoRR abs/2009.10564
(2020).
[39] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi.
2020. NodeAug: Semi-Supervised Node Classification with Data Augmentation.
207â€“217.
[40] Da Xu, Chuanwei Ruan, Kamiya Motwani, Evren Korpeoglu, Sushant Kumar,
and Kannan Achan. 2019. Generative graph convolutional network for growing
graphs. In International Conference on Acoustics, Speech and Signal Processing.
[41] Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, and Pan Li. 2019. Conditional
Structure Generation through Graph Variational Generative Adversarial Nets..
InAdvances in Neural Information Processing Systems.
[42] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph Con-
trastive Learning Automated. In Proceedings of the 38th International Conference
on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina
Meila and Tong Zhang (Eds.). PMLR, 12121â€“12132.
[43] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
Neural Information Processing Systems (2020).
[44] Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. 2023. Towards Better Dynamic
Graph Learning: New Architecture and Unified Library. arXiv:2303.13047 (2023).
[45] Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. 2022. Label-invariant
Augmentation for Semi-Supervised Graph Classification. In Advances in Neural
Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 29350â€“29361.
[46] Shilei Zhang, Toyotaro Suzumura, and Li Zhang. 2021. DynGraphTrans: Dynamic
Graph Embedding via Modified Universal Transformer Networks for Financial
Transaction Data. In 2021 IEEE International Conference on Smart Data Services
(SMDS). 184â€“191. https://doi.org/10.1109/SMDS53860.2021.00032
[47] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil
Shah. 2020. Data Augmentation for Graph Neural Networks. CoRR abs/2006.06830
(2020).
[48] Yanping Zheng, Zhewei Wei, and Jiajun Liu. 2023. Decoupled Graph Neural
Networks for Large Dynamic Graphs. arXiv:2305.08273
2909Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Dataset Nodes Edges Unique Edges Node/Link Feature Time Granularity Duration density
WIKI 9227 157474 18257 0/172 Unix timestamp 1 month 4.30E-03
REDDIT 10984 672447 78516 0/172 Unix timestamp 1 month 8.51E-03
MOOC 7144 411749 178443 0/4 Unix timestamp 17 month 1.26E-02
LastFM 1980 1293103 154993 0/0 Unix timestamp 1 month 5.57E-01
Social Evo. 74 2099519 4486 0/2 Unix timestamp 8 months 5.36E+02
UCI 1899 59835 20296 0/0 Unix timestamp 196 days 3.66E-02
Table 7: Dataset statistics
A APPENDIX
A.1 Detail Descriptions of Datasets
â€¢Wiki: is a bipartite interaction graph that contains the edits on
Wikipedia pages over a month. Nodes represent users and wiki
pages, and links denote the editing behaviors with timestamps.
Each link is associated with a 172-dimensional Linguistic Inquiry
and Word Count (LIWC) feature.
â€¢Reddit: consists of one month of posts made by users on sub-
reddits. Users and subreddits are the nodes, and links are the
timestamped posting requests. Each link has a 172-dimensional
LIWC feature.
â€¢MOOC: is a bipartite interaction network of online sources,
where nodes are students and course content units (e.g., videos
and problem sets). Each link denotes a studentâ€™s access behavior
to a specific content unit and is assigned a 4-dimensional feature.
â€¢UCI: is a Facebook-like, unattributed online communication net-
work among students of the University of California at Irvine,
along with timestamps with the temporal granularity of seconds.
â€¢LastFM: is an interaction network where users and songs are
nodes and each edge represents a user-listens-to-song relation.
The dataset consists of the relations of 1000 users listening to
the 1000 most listened songs over a period of one month. The
dataset contains no attributes.
â€¢SocialEvo: is a mobile phone proximity network that tracks the
everyday life of a whole undergraduate dormitory from October
2008 to May 2009. Each edge has 2 features.
A.2 Detail Descriptions of baselines
â€¢JODIE is a RNN-based method. Denote â„ğ‘¢(ğ‘¡)as the embedding
of nodeğ‘¢at timestamp ğ‘¡,ğ‘’ğ‘¡
ğ‘¢,ğ‘£as the link feature between ğ‘¢,
ğ‘£at timestamp ğ‘¡, andğ‘šğ‘¢as the timestamp that node ğ‘¢latest
interact with other nodes. When an interaction between node ğ‘¢,
ğ‘£happens at timestamp ğ‘¡, JODIE updates the node embedding
using RNN by â„ğ‘¢(ğ‘¡)=ğ‘…ğ‘ğ‘(â„ğ‘¢(ğ‘šğ‘¢),â„ğ‘£(ğ‘šğ‘£),ğ‘’ğ‘¡
ğ‘¢,ğ‘£,ğ‘¡âˆ’ğ‘šğ‘¢). Then,
the embedding of node ğ‘¢at timestamp ğ‘¡0is computed as â„ğ‘¢(ğ‘¡0)=
(1+(ğ‘¡0âˆ’ğ‘šğ‘¢)ğ‘¤)Â·â„ğ‘¢(ğ‘šğ‘¢).
â€¢TGAT is a self-attention-based method that could capture spa-
tial and temporal information simultaneously. TGAT first con-
catenates the raw feature ğ‘¥ğ‘¢with a trainable time encoding
ğ‘§(ğ‘¡), i.e.,ğ‘¥ğ‘¢(ğ‘¡)=[ğ‘¥ğ‘¢||ğ‘§(ğ‘¡)]andğ‘§(ğ‘¡)=ğ‘ğ‘œğ‘ (ğ‘¡ğ‘¤+ğ‘). Then, self-
attention is applied to produce node representation â„ğ‘¢(ğ‘¡0)=
ğ‘†ğ´ğ‘€(ğ‘¥ğ‘¢(ğ‘¡0),ğ‘¥ğ‘£(ğ‘šğ‘£)|ğ‘£âˆˆğ‘ğ‘¡0(ğ‘¢)), whereğ‘ğ‘¡0(ğ‘¢)denotes the neigh-
bors of node ğ‘¢at timeğ‘¡0andğ‘šğ‘¢denotes the timestamp of thelatest interaction of node ğ‘¢. Finally, the prediction on any node
pair at time ğ‘¡0is computed by ğ‘€ğ¿ğ‘ƒ([â„ğ‘¢(ğ‘¡0)||â„ğ‘£(ğ‘¡0)]).
â€¢TGN is a mixture of RNN- and self-attention based method. TGN
utilizes a memory module to store and update the (memory) state
ğ‘ ğ‘¢(ğ‘¡)of nodeğ‘¢. The state of node ğ‘¢is expected to represent
ğ‘¢â€™s history in a compressed format. Given the memory updater
as mem, when an link ğ‘’ğ‘¢ğ‘£(ğ‘¡)connecting node ğ‘¢is observed,
nodeğ‘¢â€™s state is updated as ğ‘ ğ‘¢(ğ‘¡)=ğ‘šğ‘’ğ‘š(ğ‘ ğ‘¢(ğ‘¡âˆ’),ğ‘ ğ‘£(ğ‘¡âˆ’)||ğ‘’ğ‘¢ğ‘£(ğ‘¡)).
whereğ‘ ğ‘¢(ğ‘¡âˆ’)is the memory state of node ğ‘–just before time
ğ‘¡.||is the concatenation operator, and node ğ‘£isğ‘¢â€™s neighbor
connected by ğ‘’ğ‘¢,ğ‘£(ğ‘¡). The implementation of ğ‘šğ‘’ğ‘š is a recurrent
neural network (RNN), and node ğ‘¢â€™s embedding is computed by
aggregating information from its L-hop temporal neighborhood
using self-attention.
â€¢DyRep is an RNN-based method that updates node states upon
each interaction. It also includes a temporal-attentive aggregation
module to consider the temporally evolving structural informa-
tion in dynamic graphs.
â€¢TCL is a contrastive learning-based method. It first generates
each nodeâ€™s interaction sequence by performing a breadth-first
search algorithm on the temporal dependency interaction sub-
graph. Then, it presents a graph transformer that considers both
graph topology and temporal information to learn node repre-
sentations. It also incorporates a cross-attention operation for
modeling the inter-dependencies of two interaction nodes.
â€¢GraphMixer is a simple MLP-based architecture. It uses a fixed
time encoding function that performs rather than the trainable
version and incorporates it into a link encoder based on MLP-
Mixer to learn from temporal links. A node encoder with neighbor
mean-pooing is employed to summarize node features.
â€¢DyGFormer is a self attetion-based method. Specifically, for
nodeğ‘›ğ‘–, DyGFormer just retrieves the features of involved neigh-
bors and links based on the given features to represent their en-
codings. DyGFormer is equipped with a neighbor co-occurrence
encoding scheme, which encodes the appearing frequencies of
each neighbor in the sequences of the source node and destina-
tion node, and can explicitly explore the correlations between
two nodes. Instead of learning at the interaction level, DyG-
Former splits each source/destination nodeâ€™s sequence into mul-
tiple patches and then feeds them to the transformer.
A.3 Descriptions of GDA Methods in
experiments
â€¢DropEdge which randomly drops edges according to the drop
possibilityğ‘at data pre-process phase. This slight modification
2910KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
of the original graph results in the GNN observing a different
graph at each epoch.
â€¢DropNode Similar to DropEdge, DropNode drops nodes accord-
ing to the drop possibility ğ‘at the data pre-process phase.
â€¢MeTA a dynamic graph data augmentation module that stacks a
few levels of memory modules to augment dynamic graphs of
different magnitudes on separate levels with three data augmen-
tation strategies, including perturbing time, removing edges, and
adding edges with perturbed time.
A.4 Detail configurations
Dataset Wiki_0.3 Reddit_0.3 MOOC_0.3 UCI_0.3 LastFM_0.3 SocialEvo_0.3
JODIE 10/ğ¿
410/ğ¿
410/ğ¿
410/1 10/ğ¿
310/ğ¿
3
DyRep 10 /ğ¿
410/ğ¿
410/ğ¿
410/1 10/ğ¿
310/ğ¿
3
TGAT 20 /ğ¿
420/ğ¿
420 /ğ¿
820/ğ¿
820/ğ¿
820/ğ¿
4
TGN 10 /ğ¿
410 /ğ¿
310/ğ¿
410/ğ¿
410/ğ¿
410/ğ¿
3
TCL 20/ğ¿
820/ğ¿
820/ğ¿
820/ğ¿
820/ğ¿
820/ğ¿
4
GraphMixer 32/ğ¿
810/ğ¿
320/ğ¿
420/ğ¿
810/1 20/ğ¿
4
DyGFormer 32/ğ¿
464/ğ¿
8128/ğ¿
832/ğ¿
8256/ğ¿
832/ğ¿
4
Table 8: The optimal configurations of the number ğ¿of sam-
pled neighbors and length ğ‘‘ğ‘–ğ‘“ğ‘“ of diffused sequenceDataset Wiki_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1 SocialEvo_0.1
JODIE 10/ğ¿
410/ğ¿
410/ğ¿
410/1 10/1 10/ğ¿
4
DyRep 10/1 10/1 10/1 10/1 10/1 10/1
TGAT 20/ğ¿
820/ğ¿
820/ğ¿
820/ğ¿
820/ğ¿
820/ğ¿
8
TGN 10/1 10/ğ¿
410/ğ¿
410/1 10/1 10/ğ¿
4
TCL 20/ğ¿
820/ğ¿
820/ğ¿
820/1 20/ğ¿
820/ğ¿
4
GraphMixer 20/ğ¿
810/ğ¿
810/ğ¿
410/ğ¿
810/ğ¿
820/ğ¿
8
DyGFormer 32/ğ¿
820/ğ¿
832/ğ¿
832/1 64/ğ¿
1632/ğ¿
8
Table 9: The optimal configurations of the number ğ¿of sam-
pled neighbors and length ğ‘‘ğ‘–ğ‘“ğ‘“ of diffused sequence
2911