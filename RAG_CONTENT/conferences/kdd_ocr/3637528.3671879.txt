BadSampler : Harnessing the Power of Catastrophic Forgetting
to Poison Byzantine-robust Federated Learning
Yi Liu
City University of Hong Kong
Hong Kong, China
yiliu247-c@my.cityu.edu.hkCong Wangâˆ—
City University of Hong Kong
Hong Kong, China
congwang@cityu.edu.hkXingliang Yuan
The University of Melbourne
Melbourne, Australia
xingliang.yuan@unimelb.edu.au
ABSTRACT
Federated Learning (FL) is susceptible to poisoning attacks, wherein
compromised clients manipulate the global model by modifying
local datasets or sending manipulated model updates. Experienced
defenders can readily detect and mitigate the poisoning effects
of malicious behaviors using Byzantine-robust aggregation rules.
However, the exploration of poisoning attacks in scenarios where
such behaviors are absent remains largely unexplored for Byzantine-
robust FL. This paper addresses the challenging problem of poi-
soning Byzantine-robust FL by introducing catastrophic forgetting.
To fill this gap, we first formally define generalization error and
establish its connection to catastrophic forgetting, paving the way
for the development of a clean-label data poisoning attack named
BadSampler. This attack leverages only clean-label data (i.e., with-
out poisoned data) to poison Byzantine-robust FL and requires the
adversary to selectively sample training data with high loss to feed
model training and maximize the modelâ€™s generalization error. We
formulate the attack as an optimization problem and present two
elegant adversarial sampling strategies, Top- ğœ…sampling, and meta-
sampling, to approximately solve it. Additionally, our formal error
upper bound and time complexity analysis demonstrate that our
design can preserve attack utility with high efficiency. Extensive
evaluations on two real-world datasets illustrate the effectiveness
and performance of our proposed attacks.
CCS CONCEPTS
â€¢Security and privacy â†’Distributed systems security ;â€¢
Computing methodologies â†’Machine learning.
KEYWORDS
Federated Learning, Data Poisoning Attack, Clean Label, Reinforce-
ment Learning
ACM Reference Format:
Yi Liu, Cong Wang, and Xingliang Yuan. 2024. BadSampler : Harnessing the
Power of Catastrophic Forgetting to Poison Byzantine-robust Federated
âˆ—Corresponding author. This work was supported by CityU of HK under Grants 9678146
and 9678126, in part by HK RGC under Grants CityU 11218521, 11218322, R6021-20F,
R1012-21, RFS2122-1S04, C2004-21G, C1029-22G, and N_CityU139/21.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671879Learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671879
1 INTRODUCTION
Federated Learning (FL) [ 26], as an emerging distributed machine
learning framework, has provided users with many privacy-friendly
promising applications [ 28,30,43,52,54]. However, it is known
that FL is vulnerable to poisoning [33,34,39]: a small fraction of
compromised FL clients, who are either owned or controlled by an
adversary, may act maliciously during the training process to cor-
rupt the jointly trained global model, i.e., degrading global model
performance or causing misclassification in prediction. In this con-
text, various attack strategies aimed at perturbing local training
datasets or generating poisoned gradients have been proposed, so-
called Data Poisoning Attacks (DPA) [32] orModel Poisoning Attacks
(MPA) [34]. However, prior arts [ 34,40] have shown that existing
attacks are difficult to deploy in practice because they either rely on
unrealistic assumptions (e.g., manipulating too many clients), or the
effectiveness of these attacks is greatly suppressed by state-of-the-
art defensive Byzantine-robust aggregation rules (e.g., FLTrust [8]).
In this paper, we endeavor to investigate the FL poisoning prob-
lem in a more practical scenario, where the adversary needs to
achieve successful poisoning of FL without resorting to establishing
unrealistic attack assumptions or deploying advanced defensive ag-
gregation rules. In addition, aligning with existing work [ 8,13,21],
we likewise need to push the frontiers of FL poisoning attacks by
considering the deployment of strong defenses. In particular, we
target Byzantine-robust FL [ 8,25], which represents a widely em-
braced collaborative learning framework known for its resilience
against attacks. In the Byzantine-robust FL paradigm, the central
server employs robust aggregation rules for the purpose of model
update aggregation. This facilitates the filtering of poisoned up-
dates or malicious gradients, thereby preserving the overall model
utility [ 2,8,21]. As a result, implementing poisoning attacks in
the context of Byzantine-robust FL without relying on unrealistic
attack assumptions presents a formidable challenge.
Our Contributions. To tackle the aforementioned challenges, we
present BadSampler, which stands as an effective clean-label data
poisoning attack designed exclusively for Byzantine-robust FL. Un-
like existing attacks, BadSampler leverages only clean-label data
to achieve its goals. In the FL training pipeline, the widely used
Stochastic Gradient Descent (SGD) typically employs uniform ran-
dom sampling of training instances from the dataset. However, this
randomness is seldom tested or enforced in practical implementa-
tions [ 33,37]. This situation presents an opportunity for adversaries
to exploit the vulnerability, allowing them to devise an adversarial
 
1944
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yi Liu, Cong Wang, & Xingliang Yuan
sampler that induces catastrophic forgetting (a phenomenon where
models lose information from previous tasks while learning new
ones [ 15]) in the FL model. BadSampler is specifically designed to
address this issue, with two primary goals in mind: (1)adaptively
select training samples to poison FL, causing catastrophic forgetting
while maintaining good training behavior, and (2)bypass the latest
defenses within a practical FL parameter range. Next, we focus on
responding to the following three challenges:
â€¢(C1.) How to introduce catastrophic forgetting under Byzantine
robust aggregation rules?
(S1.) â€“Not all benign updates are beneficial. In FL, benign training
behavior means that the updates generated by iteration are bene-
ficial to the model to obtain high training accuracy [ 26]. Inspired
by the phenomenon of catastrophic forgetting [ 15], well-designed
benign updates (i.e., achieving higher training accuracy) may cause
the global model to forget knowledge about old tasks, thus dam-
aging the modelâ€™s generalization ability. Furthermore, prevailing
Byzantine robust aggregation protocols are limited to mitigating
the adverse effects stemming from adversarial training behaviors,
specifically poisoned updates and gradients [ 2,8], while encounter-
ing challenges in discerning the nature of benign training behaviors.
Therefore, BadSampler is designed to circumvent advanced defenses
by maintaining good training behavior. The key to maintaining
good training behavior is to maintain a low training error but
slowly increase the validation error. To achieve this, the proposed
attack strategy endeavors to accomplish high-frequency sampling
of training samples characterized by elevated losses through the
construction of adversarial samplers. This approach is designed
to uphold the modelâ€™s attainment of high training accuracy while
concurrently inducing catastrophic forgetting phenomena.
â€¢(C2.) How to design an effective poisoning attack under strict
parameter constraints?
(S2.) â€“Not all poisoning attacks are subject to parameter constraints.
It is also known that most of the prior poisoning attacks are less
effective under a practical FL parameter range [ 34,40], such as a
realistic compromised client ratio ( ğ‘€<20%), participation ratio
(ğ‘=10%), and data poisoning size ( |ğ·ğ‘ğ‘œğ‘–ğ‘ ğ‘œğ‘›|â‰«|ğ·|). Under these
settings, enabling a stealthy poisoning attack becomes even more
difficult. For this reason, BadSampler is designed to no longer use
poisoned data but to use clean-labeled data to implement the attack.
More specifically, BadSampler can cause catastrophic forgetting in
the global model simply by changing the order in which local clean
data is fed to the model. Furthermore, BadSampler does not rely on
changing parameter settings to improve attack performance. The
key information that our attack relies on is the generalization error
representation (refer to Sec. 4.1), so it is data-agnostic and model-
agnostic and independent of external FL parameters (i.e., controlled
by the server). Furthermore, our theoretical analysis confirms that
the error upper bound of our attack is related to the internal FL
parameters (i.e., controlled by the clients).
â€¢(C3.) How to maintain excellent attack effectiveness under FLâ€™s
dynamic training principles?
(S3.) â€“Heuristic poisoning attacks are feasible. FLâ€™s dynamic training
principle is reflected in the way that the server re-selects different
clients to participate in training each iteration [ 26]. Existing poi-
soning attacks often require compromising a fixed set of clients to
participate in each round of training rather than randomly selectingavailable clients as per the FL training pipeline. Such a requirement
heavily reduces attack practicality. BadSampler demonstrates adapt-
ability by enabling different compromised clients at each training
iteration to leverage the training state, e.g., error information, to
make effective adversarial sampling decisions. The attack formu-
lates the attack target as an optimization problem and leverages
the Top-ğœ…and meta-sampling strategies to approximately solve it.
Moreover, our attack formalizes a model-agnostic generalization
error representation, and the objective function differs from the
main task, allowing optimization in each iteration.
The contributions of this paper are listed as follows:
(1)We design a new clean-label data poisoning attack, i.e.,Bad-
Sampler, against Byzantine-robust FL models under practical pa-
rameter range constraints by introducing catastrophic forgetting.
(2)We design two optimizable adaptive adversarial sampling
strategies to maintain the utility of the proposed attack in practical
FL scenarios.
(3)We conduct extensive experiments on two public datasets for
convex, and non-convex models, and evaluate over prior defenses to
demonstrate the advantages of the proposed attacks. In particular,
our attack can lead to a 8.98%drop in the accuracy of FLTrust [8])
compared to the baselines.
2 RELATED WORK
Existing Poisoning Attacks and Their Limitations. The cur-
rent poisoning attacks in FL primarily follow two common strate-
gies: (1)Directly modifying the local data [ 13,14,39];(2)Building
adversarially-crafted gradients [ 3,9,33]. Although these attack
tactics indeed pose real threats, they have evident limitations in
practical scenarios. Here we explore the limitations of these attack
strategies in a more realistic FL scenario where training operates
within a practical range of parameters (refer to Table 8). Firstly,
DPAs necessitate the adversary possessing knowledge of the dataset
or model and the capability to perturb the training dataset [ 33,34].
Fulfilling this requirement proves challenging in practice. Experi-
enced defenders, especially those equipped with anomaly detection
techniques, can swiftly identify corrupted datasets [ 2,35]. Moreover,
studies have shown that DPAs struggle to bypass well-designed
defenses, such as Byzantine robust aggregation rules [ 8,25], even if
the adversary successfully acquires dataset and model knowledge
while circumventing the anomaly detection mechanism. Secondly,
existing MPAs heavily rely on sophisticated and high-cost joint
optimization techniques among compromised clients, as described
in references [ 3,9]. These attacks also rely on irrational parameter
values, such as an excessive percentage of compromised clients
(typically >20%) [ 9]. These limitations motivate us to explore a
poisoning attack that does not require dataset perturbation, utilizes
a clean-label dataset, and bypasses Byzantine robust aggregation.
Defenses Against Poisoning Attacks in FL. Existing defenses
against poisoning attacks can be categorized into two main strate-
gies. Firstly, there are defenses aimed at detecting and removing
malicious clients with anomalous data or model updates [ 10,38,44,
46,53]. Secondly, there are defenses aimed at limiting the negative
impact of such attacks, such as using Byzantine-robust aggregation
methods that aggregate local model parameters using the Geomet-
ric mean instead of the mean [ 8,21,29,50]. In this paper, our focus
 
1945BadSampler : Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
is on Byzantine robust FL, and these defenses serve as commonly
used methods against both data poisoning attacks and model poi-
soning attacks [ 49]. Specifically, the goal of poisoning attacks is
to manipulate a limited number of malicious clients to poison the
aggregated model, resulting in an inevitable drop in model accuracy.
To counter this, the key idea behind Byzantine-robust Aggregation
(AGR) is to identify and remove anomalous updates before updating
the aggregated model by comparing the clientsâ€™ local model up-
dates. For instance, Li et al. in [18] proposed a new aggregation rule
termed RSA, which employs the geometric median of the clientâ€™s lo-
cal update as the global model update. The current state-of-the-art
defense in this context is FLTrust [8], which leverages trust scores
to determine which model updates can be aggregated.
3 BACKGROUND AND THREAT MODEL
3.1 Definition of the Generalization Error
Here, we focus on elaborating on the formal definition of gener-
alization error. In the machine learning domain, the bias-variance
trade-off is the basic theory for qualitative analysis of generalization
error [11, 22, 55], which can be defined as follows:
Definition 1. (Generalization Error). Let ğ·={ğ’™,ğ’š}={ğ‘¥ğ‘–,ğ‘¦ğ‘–}ğ‘›
ğ‘–
that contains ğ‘›training samples denote the training dataset, where
ğ‘¥ğ‘–denotes theğ‘–-th training sample, and ğ‘¦ğ‘–is the associated label. Let
ğ‘“ğœƒ(ğ‘¥)denote the predictor and let â„“(Â·,Â·)denote the loss function. Thus,
the formal expression of the generalization error is as follows:
Err=Eğ’™,ğ’šEğ·h
âˆ¥ğ’šâˆ’ğ‘“ğœƒ(ğ’™|ğ·)âˆ¥2
2i
â‰ˆEğ’™,ğ’šh
âˆ¥ğ’šâˆ’Â¯ğ‘“ğœƒ(ğ’™)âˆ¥2
2i
|                    {z                    }
Bias+Eğ’™,ğ’šEğ·h
âˆ¥ğ‘“ğœƒ(ğ’™|ğ·)âˆ’Â¯ğ‘“ğœƒ(ğ’™)âˆ¥2
2i
|                                  {z                                  }
Variance,(1)
where Â¯ğ‘“ğœƒ(ğ’™)=Eğ·[ğ‘“ğœƒ(ğ’™|ğ·)]. According to the above definition, the
bias may be smaller when the variance is larger, which means that
the training error of the model is small and the verification error may
be large. When the above situation occurs, the model is likely to fall
into the catastrophic forgetting problem.
Definition 2. (Generalization Error of Sample ğ‘¥ğ‘–). Similarly, we
define the generalization error of a single sample ğ‘¥ğ‘–as
errğ‘–=Eğ·
â„“
ğ‘“ğœƒ ğ‘¥ğ‘–|ğ·,ğ‘¦ğ‘–
â‰ˆğµğ‘–ğ‘ğ‘ (ğ‘¥ğ‘–)+ğ‘‰ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥ğ‘–),(2)
whereğµğ‘–ğ‘ğ‘ (ğ‘¥ğ‘–)andğ‘‰ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥ğ‘–)are the bias and variance of ğ‘¥ğ‘–.
Obviously, when the generalization error of the sample ğ‘¥ğ‘–approaches
1, we can call it a hard sample; on the contrary, we can call the sample
ğ‘¥ğ‘–an easy sample. Therefore, for the sample ğ‘¥ğ‘–, the definition of the
generalization error can be used to represent the difficulty of sample
learning, i.e., the sample difficulty [11, 22, 55].
3.2 Threat Model
We consider a threat model in which an adversary could exploit
well-crafted adversarial samplers to poison the global model. Dur-
ing training, the adversary forges a sampler that embeds a malicious
sampling function. This adversarial sampler is then incorporated
into the target FL system through system development or main-
tenance [ 36]. At inference time, the target FL system has indis-
criminately high error rates for testing examples. Next, we discussour threat model with respect to the attackerâ€™s goals, background
knowledge, and capabilities.
Attackerâ€™s Goal. The attackerâ€™s goal is to manipulate the learned
global model to have an indiscriminately high error rate for testing
examples, which is similar to prior studies on poisoning attacks [ 6,9,
10,35]. Given that there are many sophisticated defenders against
poisoning attacks today [ 8,31], the attackerâ€™s second goal is to
successfully evade the tracks of these defenders.
Attackerâ€™s Background Knowledge. We assume that the at-
tacker holds a model (called surrogate model ) that is not related to
the primary task. It should be noted that the surrogate model is
utilized by the attacker to build an adversarial sampler but has no
impact on the training of the primary task. Furthermore, to make
the attack more stealthy and practical, we assume that the attacker
only needs to access and read the local dataset. Although previous
literature [ 7,32,33] has shown that attackers can modify and add
perturbations to local datasets, our experimental results show that
such operations are easily detected by experienced defenders.
Attackerâ€™s Capability. We assume that the attacker controls the
proportionğ‘€of all clients ğ¾, called compromised clients. Following
the production FL settings (see Appendix A), we assume that the
number of compromised clients is much less than the number of
benign clients, i.e.,ğ‘€â©½10%.
4 BADSAMPLER ATTACK
4.1 Primer on BadSampler Attack
Following [ 33], we first demonstrate that the order of local training
batches provided to model training affects model behavior. In FL, the
central server and clients cooperatively run a ğ‘‡-round training pro-
tocol to perform federated optimization. Referring to FedAvg [ 26],
the formal definition of federated optimization is as follows:
minğœ”ğ‘“(ğœ”),whereğ‘“(ğœ”):=ğ¾Ã•
ğ‘˜=1ğ‘ğ‘˜ğ¹ğ‘˜(ğœ”), (3)
whereğ‘ğ‘˜â©¾0,Ã
ğ‘˜ğ‘ğ‘˜=1is a user-defined term that indicates the
relative influence of each client on the global model. In Eq. (3),
we mainly focus on the local optimization termğ¹ğ‘˜(ğœ”)which de-
termines the performance of the global model. In fact, this local
optimization term can be regarded as solving a non-convex opti-
mization problem with respect to the parameter ğœ”ğ‘˜, corresponding
to the minimization of a given local loss function ğ¹ğ‘˜(ğœ”ğ‘˜). For the
ğ‘˜-th client, we consider that the batch size of local training is ğµand
letğ‘Â·ğµbe the total number of items for training, then in a single
epoch one aims to optimize: ğ¹ğ‘˜(ğœ”ğ‘˜)=1
ğ‘Ãğ‘
ğ‘–=1â„“ğ‘–(ğ‘“ğ‘˜(ğ‘¥ğ‘–),ğ‘¦ğ‘–).
In general, we use the SGD algorithm to solve the above optimiza-
tion problem, i.e., the following weight update rule is implemented
in a training epoch: ğœ”ğ‘˜
ğ‘¡+1=ğœ”ğ‘˜
ğ‘¡+ğœ‚Î”ğœ”ğ‘˜
ğ‘¡,Î”ğœ”ğ‘˜
ğ‘¡=âˆ’âˆ‡ğœ”â„“(ğœ”ğ‘˜),where
ğœ‚is the learning rate. Secondly, we delve into the sampling proce-
dure of the SGD algorithm and analyze the impact of its batching
on model performance. In fact, the stochasticity of SGD comes
from its sampling process, thus, in SGD, sampling affects how well
the mini-batch gradient approximates the true gradient. Given the
local datağ·ğ‘˜, we assume an unbiased sampling procedure, and
the expectation of the batch gradient matches the true gradient as
 
1946KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yi Liu, Cong Wang, & Xingliang Yuan
follows:
E[âˆ‡â„“ğ‘˜
ğ‘–ğ‘¡(ğœ”ğ‘˜)]=ğ‘Ã•
ğ‘–=1P(ğ‘–ğ‘¡=ğ‘–)âˆ‡â„“ğ‘˜
ğ‘–(ğœ”ğ‘˜)=âˆ‡ğ¹ğ‘˜(ğœ”ğ‘˜), (4)
where P(ğ‘–ğ‘¡=ğ‘–)=1
ğ‘andğ‘¡is the number of SGD steps.
Observations. We emphasize that Eq. (4)occurs only as expected,
and conversely, for isolated training batches, the approximation
error may be large and affect model performance. Inspired by this
fact, we investigate how adversaries exploit local sampler to disrupt
training. This means that the classical stochastic assumption in SGD
opens up a new attack surface for adversaries to affect the learning
performance and process [ 36,37]. To this end, we revisit the impact
ofğ‘SGD steps during local training on model performance:
ğœ”ğ‘˜
ğ‘¡=ğœ”ğ‘˜
0âˆ’ğœ‚âˆ‡â„“0(ğœ”ğ‘˜
0)âˆ’ğœ‚âˆ‡â„“1(ğœ”ğ‘˜
1)âˆ’ğœ‚âˆ‡â„“2(ğœ”ğ‘˜
2)âˆ’Â·Â·Â·
=ğœ”ğ‘˜
0âˆ’ğœ‚ğ‘âˆ’1Ã•
ğ‘—=0âˆ‡â„“ğ‘—(ğœ”ğ‘˜
0)+ğœ‚2ğ‘âˆ’1Ã•
ğ‘—=0Ã•
ğ‘–<ğ‘—âˆ‡âˆ‡â„“ğ‘—(ğœ”ğ‘˜
0)â„“ğ‘–(ğœ”ğ‘˜
0)+O(ğ‘3ğœ‚3)
=ğœ”ğ‘˜
0âˆ’ğ‘ğœ‚âˆ‡â„“(ğœ”ğ‘˜
0)+ğœ‚2ğœ(ğœ”ğ‘˜
0)+O(ğ‘3ğœ‚3),
(5)
whereO(ğ‘3ğœ‚3)is the error caused by the Taylor expansion and
second order correction term ğœ(ğœ”ğ‘˜
0)=ğ‘âˆ’1Ã
ğ‘—=0Ã
ğ‘–<ğ‘—âˆ‡âˆ‡â„“ğ‘—(ğœ”ğ‘˜
0)â„“ğ‘–(ğœ”ğ‘˜
0)is
the stochastic error determined by the mini-batches. This means
that we can simply manipulate local training mini-batches to harm
learning performance and convergence. Hence, our poisoning at-
tack aims to manipulate local samplers to change the order of the
local training batch without using poisoned data. By doing so, we
cause the first and second derivatives to be misaligned with the
true gradient step, thereby undermining the modelâ€™s generalization
ability.
4.2 Formulating Optimization Problems
Our idea is to poison Byzantine-robust FL by utilizing adversarial
training batches sampled by a carefully crafted adversarial sampler
at each iteration. More specifically, we aim to use local training
batches sampled by adversarial samplers to increase the generaliza-
tion error of the FL global model so that FL falls into catastrophic
forgetting. Therefore, we expect to find a model-agnostic represen-
tation that can provide generalization error information for adver-
sarial samplers. Inspired by the theory of bias-variance tradeoff
(see Defi. 1), the attackerâ€™s goal translates to performing adversarial
sampling among compromised clients to obtain the most appro-
priate adversarial training batches to maximize Eq. (1). Without
loss of generality, we consider that the first ğ‘šclients are compro-
mised. Our generalization error maximization objective is to craft
locally adversarial training batches Bâ€²
1,Bâ€²
2,Â·Â·Â·,Bâ€²ğ‘šfor the com-
promised clients via solving the following optimization problem at
each iteration:
max
Bâ€²
1,Bâ€²
2,Â·Â·Â·,Bâ€²ğ‘šErr,
subject toBâ€²=A(Bâ€²
1,Bâ€²
2,Â·Â·Â·,Bâ€²
ğ‘š),
(ğ‘¥,ğ‘¦)âˆˆğ·ğ‘š,
min
(ğ‘¥,ğ‘¦)âˆˆğ·ğ‘šEğ‘š
ğ·ğ‘¡, (6)whereBdenotes that the after-attack adversarial training batch
on the compromised client, (ğ‘¥,ğ‘¦)âˆˆğ·ğ‘šdenotes that only clean-
label data can be used, and min
(ğ‘¥,ğ‘¦)âˆˆğ·ğ‘šEğ‘š
ğ·ğ‘¡denotes that the training
error is minimized (aim to bypass advanced defenses). From the
above optimization goals, it can be seen that the attacker needs to
maximize the generalization error on the one hand and minimize the
training error on the other to prevent being filtered by experienced
defenders.
4.3 Attack Implementation
The attackerâ€™s goal is formulated as an optimization problem in
Eq.(6). However, due to the non-convex and nonlinear nature of
Eq.(6), solving it exactly is impractical. To make it amenable to
optimization, we propose an effective adversarial sampler called
BadSampler, which guides hard sampling to optimize Eq. (6). The
workflow and taxonomy of BadSampler attacks are illustrated in Fig.
1. The proposed attack strategies consist of (1)the top-ğœ…sampling
strategy and (2)the meta-sampling strategy. The core idea of the
first attack strategy is to construct a training sample candidate
pool with high loss and sample the hard training samples from this
pool to feed the model. The insight of the second attack strategy is
to treat the training and validation error distribution as the meta-
state of the error optimization process, thus using meta-training to
maximize the generalization error.
Top-ğœ…Sampling Strategy. As mentioned above, the core idea of
this strategy is the construction of a hard sample candidate pool. To
select those samples that are conducive to maximizing the general-
ization error, we use the sample difficulty (see Defi. 2) as a selection
metric to select appropriate samples. Specifically, we select the top
ğœ…âˆ—ğµ(ğœ…is a constant and ğµis the size of the training batch) difficult-
to-learn samples as the hard sample candidate pool and instruct
the adversarial sampler to sample data from this candidate pool to
feed the model training. Note that sample difficulty is calculated by
the surrogate model because sample difficulty is model-agnostic and
independent of dataset size and sample space [ 33].For simplicity, let
Hğ‘ğ‘ğ‘‘denote the hard sample pool, then the selection process can
be formally defined as follows:
Hğ‘ğ‘ğ‘‘=ğ‘‡ğ‘œğ‘âˆ’ğœ…(ğ‘’ğ‘Ÿğ‘Ÿğ‘–,ğ·ğ‘š,ğµ). (7)
Then the process of sampling hard samples from Hğ‘ğ‘ğ‘‘to feed the
model training can be expressed as: ğ‘“(ğœƒ,ğ‘¥ğ‘–|Hğ‘ğ‘ğ‘‘). Note that we
resample the samples in this candidate pool and the candidate pool
is also updated iteratively.
Meta Sampling Strategy. In this strategy, we adopt the concept of
meta-learning to seek a model-agnostic representation that provides
generalization error information for adversarial meta-samplers. By
leveraging the bias-variance trade-off, we can express the general-
ization error in a form independent of task-specific properties, such
as dataset size and feature space [ 1,47]. Consequently, this repre-
sentation enables adversarial samplers to perform across different
tasks. Drawing inspiration from Definition 1 and the idea of â€œgradi-
ent/hardness distributionâ€ [ 11,23,24], we introduce the histogram
distribution of training and validation errors as the meta-state of
the adversarial sampler optimization process. Next, we delve into
the construction of the histogram error distribution.
 
1947BadSampler : Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Top-Îºsampling
Selected hard 
sampleEasy sample Hard sample
Underfitting
(high bias)Overfitting
(high variance)Training 
errorValidation 
error
Err
Attack insightTraining error
distribution
Validation error
distributionMeta -state
]Ë† [Ë†,
tD Dv Es E=RL Agent
Meta sampling()Î¾â‹…Attack Point
Figure 1: Workflow and taxonomy of our BadSampler attack.
Meta Sampling â€“ Histogram Error Distribution. First, refer to
reference [ 11], we give a formal definition of the histogram error
distribution as follows:
Ë†ğ¸ğ‘–
ğ·=|{(ğ‘¥,ğ‘¦)|ğ‘–âˆ’1
ğ‘â©½â„“(ğ‘“ğ‘ (ğœƒ,ğ‘¥)âˆ’ğ‘¦)<ğ‘–
ğ‘,(ğ‘¥,ğ‘¦)âˆˆğ·}|
|ğ·|, (8)
where 1â©½ğ‘–â©½ğ‘and Ë†ğ¸ğ·âˆˆRğ‘is a vector that denotes the error
distribution ğ¸ğ·approximated by the histogram on dataset ğ·,Ë†ğ¸ğ‘–
ğ·
denotes the ğ‘–-th component of vector Ë†ğ¸ğ·,ğ‘“ğ‘ is the surrogate model,
andğ‘is the number of bins in the histogram. Gvien tranining set
ğ·ğ‘¡and validation set ğ·ğ‘£, we have the meta-state:
ğ‘ =h
bğ¸ğ·ğ‘¡:bğ¸ğ·ğ‘£i
âˆˆR2ğ‘. (9)
In practice, the histogram error distribution Ë†ğ¸ğ·reflects the fit of a
given classifier to the dataset ğ·. By adjusting ğ‘in a fine-grained
manner, it captures the distribution information of hard and easy
samples, enriching the adversarial sampling process with more
generalization error information. Moreover, in conjunction with
Defi. 1, the meta-state provides the meta-sampler with insights into
the bias/variance of the current local model, thus supporting its
decision-making. However, directly maximizing the generalization
error solely based on this meta-state is impractical. Sampling from
large datasets for sample-level decisions incurs exponential com-
plexity and time consumption. Additionally, blindly maximizing
training and validation errors can be easily detected by an experi-
enced defender.
Meta Sampling â€“ Gaussian Sampling. To make updating the
meta-state more efficient, we leverage a Gaussian function trick to
simplify the meta-sampling process and the sampler itself, reduc-
ing the sampling complexity from O(|ğ·|)toO(1). Specifically,
we aim to construct a mapping such that an adversarial meta-
samplerğœ‰(Â·)outputs into a scalar interval ğœ‡âˆˆ[0,1]for a given
input meta-state ğ‘ ,i.e.,ğœ‡âˆ¼ğœ‰(ğœ‡|ğ‘ ). In doing so, we apply a Gauss-
ian function ğ‘”ğœ‡,ğœ(ğ‘¥)to the classification error of each instance
to determine its sampling weights, where ğ‘”ğœ‡,ğœ(ğ‘¥)is defined as:
ğ‘”ğœ‡,ğœ(ğ‘¥)=1
ğœâˆš
2ğœ‹ğ‘’âˆ’1
2ğ‘¥âˆ’ğœ‡
ğœ2
,whereğ‘’is the Eulerâ€™s number and ğœis a
hyperparameter. Therefore, for the i-th sample, its sampled weight
ğ‘¤ğ‘–=ğ‘”ğœ‡,ğœ(â„“(ğ‘“ğ‘ (ğ‘¥ğ‘–|ğ·),ğ‘¦ğ‘–)Ã
(ğ‘¥ğ‘—,ğ‘¦ğ‘—)âˆˆğ·ğ‘”ğœ‡,ğœ (â„“(ğ‘“(ğ‘¥ğ‘—|ğ·),ğ‘¦ğ‘—).
Meta Sampling Strategy â€“ Meta Training. In this section, we
present the training process of the adversarial meta-sampler and
its strategy to maximize generalization error. The adversarial meta-
sampler is designed to iteratively and adaptively select adversar-
ial training batches, aiming to maximize the generalization er-
ror of the local model. As mentioned earlier, the sampler takes
the current meta-state ğ‘ as input and outputs Gaussian functionparameters to determine the sampling probability for each sam-
ple. By learning and adjusting its selection strategy through the
state(ğ‘ )â€“action(ğœ‡)â€“state(new ğ‘ ) interaction, the adversarial meta-
sampler seeks to maximize the generalization error. Consequently,
the non-convex and nonlinear nature of the objective function
is naturally addressed through Reinforcement Learning (RL) [ 12].
We consider the generalization error maximization process as an
environment (ENV) in the RL setting. Therefore, in the RL set-
ting, we treat the above optimization process as a Markov deci-
sion process (MDP), where the MDP can be defined by the tuple
(S,Î©,ğ‘ƒğ‘,ğ‘…ğ‘). The definition of parameters in this tuple can be
seen as follows:Sis a set of state ğ‘ called state space, Î©is a set
of actionsğ‘called the action space (where ğ‘ğ‘¡âˆˆÎ©âˆˆ [0,1]is
continuous), ğ‘ƒğ‘ ğ‘ ,ğ‘ â€²=Pr ğ‘ ğ‘¡+1=ğ‘ â€²|ğ‘ ğ‘¡=ğ‘ ,ğ‘ğ‘¡=ğ‘is the prob-
ability that action ğ‘ğ‘¡in stateğ‘ ğ‘¡at timeğ‘¡will lead to state ğ‘ ğ‘¡+1
at timeğ‘¡+1, andğ‘…ğ‘(ğ‘ ,ğ‘ â€²)is the immediate reward received af-
ter transitioning from state ğ‘ to stateğ‘ â€², due to action ğ‘. More
specifically, at each environment step, ENV provides the meta-state
ğ‘ ğ‘¡=[Ë†ğ¸ğ·ğ‘¡:Ë†ğ¸ğ·ğ‘£], and then the action ğ‘ğ‘¡is selected by ğ‘ğ‘¡âˆ¼ğœ‰(ğœ‡ğ‘¡|ğ‘ ğ‘¡).
The new state ğ‘ ğ‘¡+1will be sampled ğ‘¤.ğ‘Ÿ.ğ‘¡ğ‘ ğ‘¡+1âˆ¼ğ‘ƒğ‘(ğ‘ ,ğ‘ â€²)in the
next round of local training. To maximize the generalization er-
ror (i.e., high validation error) while maintaining good learning
behavior (i.e., low training error), we design a generalization
error quantization function ğº(ğœƒ,ğ·)=|ğ·|Ã
ğ‘–=1â„“(ğ‘“(ğ‘¥ğ‘–|ğ·),ğ‘¦ğ‘–), where
ğº(ğœƒ,ğ·)=|ğ·|Ã
ğ‘–=1â„“(ğ‘“ğœƒ(ğ‘¥ğ‘–|ğ·),ğ‘¦ğ‘–)is the sum of losses on the validation
setğ·ğ‘£, then the reward ğ‘Ÿcan be defined as the difference in gener-
alization performance of the surrogate model before and after the
update, i.e.,ğ‘Ÿğ‘¡=|ğ·ğ‘£|Ã
ğ‘–=1â„“(ğ‘“ğ‘¡+1
ğ‘˜(ğ‘¥ğ‘–âˆˆğ·ğ‘£),ğ‘¦ğ‘–)âˆ’|ğ·ğ‘£|Ã
ğ‘–=1â„“(ğ‘“ğ‘¡
ğ‘˜(ğ‘¥ğ‘–âˆˆğ·ğ‘£),ğ‘¦ğ‘–).
Therefore, the above optimization objective (i.e., cumulative reward)
of the adversarial meta-sampler is to maximize the generalization
error of the surrogate model. To achieve this goal, we utilize the
Soft Actor-Critic (SAC) theory in [ 12], to optimize our meta-sampler
ğœ‰(Â·). The overview of the proposed attack algorithms are shown in
Algo. 1â€“3 (refer to Appendix B).
5 THEORETICAL ANALYSIS
5.1 Error Upper Bound Analysis for
BadSampler
In analyzing the error lower bound of the proposed attack, it is cru-
cial to consider that this bound can be influenced by factors such as
the experimental environment, communication status, and network
delay. As a consequence, the practical significance of conducting
 
1948KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yi Liu, Cong Wang, & Xingliang Yuan
such an analysis is limited. Therefore, while understanding the
theoretical lower bound is informative, it is essential to interpret
the results within the context of specific experimental conditions
and practical considerations. Thus, this paper provides a detailed
error upper bound analysis. For simplicity, we make the required
assumptions as follows.
Assumption 1. (Bounded Gradients). For any model parameter
ğœ”and in the sequence [ğœ”0,ğœ”1,...,ğœ”ğ‘¡,...], the norm of the gradi-
ent at every sample is bounded by a constant ğœ€2
0, i.e.,âˆ€ğœ”, we have:
||âˆ‡ğ¹(ğœ”)||2â©½ğœ€2
0.
Assumption 2. (Bounded BadSamplerâ€™s Gradients). For any model
parameterğœ”and in the sequence [ğœ”0ğ‘,ğœ”1ğ‘,...,ğœ”ğ‘¡ğ‘,...], the norm
of the bad batchâ€™s gradient at every sample is bounded by a constant
ğœ€2
ğ‘, i.e.,âˆ€ğœ”ğ‘, we have:||âˆ‡ğ¹(ğœ”ğ‘)||2â©½ğœ€2
ğ‘.
Assumption 3. (Unbiased Estimation Sampling). For any training
epoch and any model parameter ğœ”, given an unbiased sampling, the
expectation of the batch gradient matches the true gradient, i.e., âˆ€bâˆ¼
Band(ğ‘¥,ğ‘¦)âˆ¼ğ·, we have: E[âˆ‡â„“ğ‘–ğ‘¡(ğœ”)]=1
ğ‘ğ‘Ã
ğ‘–=1âˆ‡â„“ğ‘–(ğœ”)=âˆ‡ğ¹(ğœ”),
andP(ğ‘–ğ‘¡=ğ‘–)=1
ğ‘.
To analyze the error upper bound of BadSampler attack, we give
Lemma 1 and Lemma 2 as follows:
Lemma 1. If Assumptions 1 and 3 hold, the expectation of the
stochastic second-order correction term, i.e., the expectation of the
bias, is formally expressed as follows:
E[ğœ‰(ğœ”)]=ğ‘2
4âˆ‡(||âˆ‡ğ¹(ğœ”)||2âˆ’1
ğ‘2Ã•ğ‘âˆ’1
ğ‘—=0||â„“ğ‘—(ğœ”)||2). (10)
Proof. Full proofs can be found in the Appendix E. â–¡
Lemma 2. If Assumptions 1, 3 and Lemma 1 hold, the expectation
of the local model parameter term is formally expressed as follows:
E[ğœ”ğ‘¡]=ğœ”0âˆ’ğ‘ğœ‚âˆ‡ğ¹(ğœ”0)+ğ‘2ğœ‚2
4âˆ‡(||âˆ‡ğ¹(ğœ”0)||2
âˆ’1
ğ‘2Ã•ğ‘âˆ’1
ğ‘—=0||â„“ğ‘—(ğœ”0)||2)+ğ‘‚(ğ‘3ğœ‚3).(11)
Proof. Full proofs can be found in the Appendix E. â–¡
Therefore, the following Theorem 1 gives the bias upper bound
for our BadSampler attack:
Theorem 1. We useğœ”âˆ—to denote the model converged without any
attack, andğœ”ğ‘to denote the final model obtained under the BadBacth
attack. If Assumptions 1â€“3 and Lemma 1â€“2 hold, the expectation of
our attackâ€™s error is formally expressed as follows:
E[||âˆ‡ğ¹(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”ğ‘)||2]â©½ğµ(1âˆ’1
ğ‘)2ğœ€2
0âˆ’ğµâˆ’1
ğ‘ğµğœ€2
ğ‘. (12)
Proof. Full proofs can be found in the Appendix E. â–¡
5.2 Complexity Analysis
We give a formal complexity analysis of the proposed attacks. We
denote the feature dimensions of the input sample as ğ‘‘, the num-
ber of training batches as ğ‘, the size of the training batch as ğµ,
and the total computation per sample (depending on the gradient
computation formula) as ğ¶.Complexity of Top- ğœ…Sampling. LetO(ğ‘logğœ…)be the time
complexity of the Top- ğœ…operation and let O(ğ‘‘ğµğ¶)(orO(ğ‘‘ğ¶))
be the time complexity of computing the gradient of a single
batch (or sample), then the total time complexity of the BadSam-
pler attack to complete the sorting and compute the difficulty is
O(ğ‘logğœ…)+O(ğ‘‘ğ‘ğµğ¶). For the adversary, we assume that it ma-
nipulatesğ‘compromised clients, then the overall time complexity
of the proposed attack is O(ğ‘ğ‘logğ‘(logğœ…+ğ‘‘ğµğ¶)).
Complexity of Meta Sampling. Compared with the Top- ğœ…sam-
pling attack strategy, this attack strategy only brings additional
overhead of performing meta-training. Therefore, we next give
the required complexity for meta-training. Likewise, we assume
that the cost of the meta-sampler to perform a single gradient
update step is ğ¶ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ (depending on the gradient computation
formula). In our attack implementation, following the SAC op-
timization strategy, we need to perform ğ‘›ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘š actions before
updating the meta-sampler, and then need ğ‘›ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ steps to col-
lect online transitions and perform gradient updates for ğœ‰(Â·).
Therefore, the overall cost of meta-training can be expressed as
ğ‘‚((ğ‘›ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘š+ğ‘›ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’)ğ‘ğµğ¶)+ğ‘‚(ğ‘›ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ğ¶ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’). For the adver-
sary, we assume that it manipulates ğ‘compromised clients, then the
overall time complexity of the proposed attack is O(ğ‘ğ‘logğ‘(logğœ…+
ğ‘‘ğµğ¶))+ğ‘‚(ğ‘logğ‘((ğ‘›ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘š+ğ‘›ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’)ğ‘ğµğ¶+ğ‘›ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ğ¶ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’)).
6 EXPERIMENTS
6.1 Experiment Setup
Datasets. We adopt two datasets for evaluations, i.e.,Fashion-
MNIST (F-MNIST) [45] and CIFAR-10 [16]. The datasets cover dif-
ferent attributes, dimensions, and number of categories, allowing us
to explore the poisoning effectiveness of BadSampler. To simulate
theIIDsetting of FL, we evenly distribute the two training datasets
to all clients, and to simulate non-IID setting of FL, we follow [ 26] to
distribute the two training datasets to all clients (see Appendix C).
Models. In this experiment, we validate our BadSampler attack in
both convex and non-convex settings. For convex problems, we
use the Logistic Regression (LR) model as our local model; for non-
convex problems, we use a simple CNN model, i.e., CNN with 2
convolutional layers followed by 1 fully connected layer and the
ResNet-18 model. Furthermore, we use a common CNN model, i.e.,
LeNet-5 model as our surrogate model.
Hyperparameters. In our attacks, we consider the cross-device FL
scenario and we set the number of clients ğ¾=100and the propor-
tion of client participation ğ‘=10%. Unless otherwise mentioned,
we set local training epoch ğ¸ğ‘™ğ‘œğ‘ğ‘ğ‘™=5, the learning rate ğœ‚=0.001,
the total number of training rounds ğ‘‡=250, the Gaussian function
parameterğœ=0.2, the meta-state size is 10 (i.e., ğ‘=5), the propor-
tion of the compromised clients ğ‘€={5%,10%}, the mini-batch size
ğµâˆˆ{8,16,32,64}and constant ğœ…âˆˆ{1,2,4,8}. Table 8 summarizes
our practical parameters/settings for comparing the previous work.
Attacks for Comparison. We compare the following attacks with
BadSampler :Label Flipping Attack (LFA) [ 10,39],Adversarial Attack
(AA) [ 4],Gaussian Attack (GA) [ 9,44],Zero-update Attack (ZA) [ 18,
21],Local Model Poisoning Attack (LMPA) [ 9],OBLIVION [ 51], and
Data Ordering Attack (ODA) [ 36]. As mentioned above, we chose
both the classic data poisoning attack and the model poisoning
attack to compare the proposed attacks fairly.
 
1949BadSampler : Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 1: Attack impact in the production federated learning
settings.
AttacksLR
(F-MNIST) CNN (CIFAR-10) ResNet-18 (CIFAR-10)
T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘
None
86.03 85.08 0 63.73
59.10 0 92.34
67.11 0
LFA 85.41 83.94 1.14 60.65
57.21 1.89 91.88
65.58 1.53
AA 83.06 82.41 2.67 58.96
55.07 4.03 88.36
63.88 3.29
GA 82.63 79.46 6.34 59.24
53.96 5.44 85.63
60.85 6.26
ZA 86.01 84.99 0.09 62.97
58.41 0.69 91.06
65.32 1.79
LMPA 79.31 75.64 9.44 57.96
52.11 6.99 84.69
61.64 5.47
DOA 85.42 81.76 3.32 62.88
55.41 3.69 90.86
65.42 1.69
OBLIVION 80.42 77.83 7.25 60.85
53.32 5.78 83.55
60.67 6.44
Ours
T
op-ğœ… 76.07 75.87 9.21 43.60
40.92 18.18 72.73
56.38 10.73
Meta 80.78 72.12 12.96 49.17
39.80 19.30 87.92
56.74 10.37
Defenses. To further validate the performance of BadSampler, we
evaluate attacks under three anomaly-detection-based defenses, i.e.,
PCA-based defense [ 39],FoolsGold [10], and FLDetector [ 53], and
three Byzantine robust AGRs, i.e.,Trimmed Mean [50],Krum [5],
andFLTrust [8].Note that here we do not consider the verification set
defense scheme because the verification set is already used in FLTrust.
Attack Impact Metric. We denote by ğ´ğ‘ğ‘âˆ—
ğ‘¡ğ‘’ğ‘ ğ‘¡the maximum ac-
curacy that the global model converges without any attack. We
useğ´ğ‘ğ‘ğ‘
ğ‘¡ğ‘’ğ‘ ğ‘¡to denote the maximum accuracy the global model can
achieve under a given attack. Here, we define attack impact Î”as
the reduction in global model accuracy due to the attack, thus for a
given attack, i.e.,Î”=ğ´ğ‘ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡âˆ—âˆ’ğ´ğ‘ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘.
6.2 Evaluation
6.2.1 Attack Performance under Practical Settings. In the first eval-
uation, we evaluate the effectiveness of our proposed BadSampler
attack under a realistic setting, considering practical parameter
ranges (refer to Table 8) in FL. We set the batch size ğµ=32, the
number of candidate adversarial training batches ğœ…=2, and use
FedAvg [ 26] as AGR. For classification tasks on the Fashion-MNIST
dataset, we employ the LR model, and for the CIFAR-10 dataset,
we use the CNN model and ResNet-18 model. The adversaryâ€™s ca-
pabilities are restricted; they do not have access to global model
parameters and must carry out the attack within strict parameters.
We employ the LeNet model as the surrogate model to compute
sample difficulty. Table 1 demonstrates that despite these limita-
tions, our designed attack scheme outperforms selected baselines,
particularly the adversarial meta-sampling attack, where our attack
achieves a remarkable performance of 19.3%. The key to our success
lies in adaptively sampling and feeding the model hard samples
during the learning process, which can destroy the modelâ€™s gener-
alization performance and cause the model to fall into catastrophic
forgetting.
6.2.2 Attack Performance under Byzantine Robust Aggregation
based Defenses. To validate the effectiveness of our BadSampler
attack, we evaluate its performance against Trimmed Mean [50],
Krum [5], and FLTrust [8], which are server-side Byzantine ro-
bust aggregation-based defenses. These defenses have previously
demonstrated robustness against existing DPAs and MPAs. In this
experiment, we conduct comparative evaluations using the CNN
model on the CIFAR-10 dataset. Table 2 presents the attack impact
Î”of our attack against the three defenses on CIFAR-10. Notably,Table 2: Attack impact under different Byzantine robust
AGRs.
AttacksT
rimmed Mean Krum FLTrust
T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘
None
63.73 59.10 0 63.73
59.10 0 63.73
59.10 0
LFA 63.12 58.89 0.21 63.54
58.99 0.11 63.50
59.00 0.10
AA 63.01 58.68 0.42 63.25
59.01 0.09 63.64
59.07 0.03
GA 63.45 58.24 0.86 63.32
58.71 0.39 63.65
59.04 0.06
ZA 63.70 59.10 0 63.66
59.07 0.03 63.64
59.10 0
LMPA 59.24 54.97 4.13 59.98
55.21 3.89 61.24
56.65 2.45
DOA 61.57 56.21 2.89 61.89
56.42 2.68 62.10
57.43 1.67
OBLIVION 57.32 53.96 5.14 57.54
52.32 5.78 57.41
56.17 2.93
Ours
T
op-ğœ… 55.65 52.24 7.86 54.60
50.12 8.98 55.70
51.21 7.89
Meta 54.24 49.04 10.06 54.27
50.09 9.09 55.04
50.12 8.98
Table 3: Attack impact under anomaly detection based de-
fenses.
AttacksPCA
FoolsGold FLDetector
T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘
None
63.73 59.10 0 63.73
59.10 0 63.73
59.10 0
LFA 62.44 57.90 1.20 62.14
57.54 1.56 63.50
58.63 0.47
AA 62.35 57.68 1.42 62.78
57.65 1.45 63.64
59.07 0.03
GA 63.52 58.36 0.74 63.57
58.75 0.35 63.65
59.05 0.05
ZA 63.70 59.10 0 63.66
59.07 0.03 63.64
59.10 0
LMPA 56.11 53.21 5.89 56.54
54.12 4.95 62.25
57.01 2.09
DOA 62.14 56.87 2.23 62.64
57.16 1.94 63.10
57.54 1.56
OBLIVION 57.12 53.32 5.78 57.21
52.01 7.09 58.55
53.17 5.93
Ours
T
op-ğœ… 54.21 49.29 9.81 54.77
50.25 8.85 55.65
52.22 6.88
Meta 50.54 46.25 12.85 51.98
48.02 11.08 55.41
50.65 8.45
our attack successfully leads to a substantial drop in global model
accuracy, even in the presence of strong defense measures. For in-
stance, when confronted with the state-of-the-art defense FLTrust,
our attack reduces the convergence accuracy of the CNN model by
8.98%. Byzantine robust aggregation defenses are effective against
attacks involving malicious model updates due to their reliance on
specific aggregation rules. However, these defenses face challenges
in countering our attack, as they disrupt the generalization ability
of the model, making it difficult for the aggregation rules to enhance
model generalization.
6.2.3 Attack Performance under Anomaly Detection Based Defenses.
To evaluate the attack performance of BadSampler and baselines
under Byzantine-robust FL with anomaly detection defenses, we use
thePCA-based method [ 39],FoolsGold [10], and FLDetector [ 53]
defenses against the proposed attacks. Specifically, we evaluate the
attack impact Î”ofBadSampler on CNN models on CIFAR-10 dataset.
We fix the local training epoch ğ¸ğ‘™ğ‘œğ‘ğ‘ğ‘™=5, the batch size ğµ=32, and
the number of candidate adversarial training batches ğœ…=2. Table
3 shows that our attacks still effectively increase the classification
error of the aggregated model. For example, under the PCA-based
defense, our attack using the CNN model as a classifier can cause
a 12.85% accuracy loss on the CIFAR-10 dataset. The main reason
is that anomaly detection-based defenses are less effective when
the compromised client is not behaving maliciously. In addition,
these defenses will also bring accuracy loss to the model due to the
dimensionality reduction operation involved.
6.2.4 Attack Performance under Different Hyperparameters. We
summarize the experimental results as follows:
 
1950KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yi Liu, Cong Wang, & Xingliang Yuan
Table 4: Attack impact under different hyperparameters.
AttacksHyp
erparameters
ğ‘€ ğœ… ğµ
5%
10% 1
2 4 8 8
16 32 64
T
op-ğœ…(%) 7.3 18.18 17.5
18.18 13.8 10.2 8.3
9.8 18.18 9.4
Meta (%) 8.6 19.3 N/A
N/A N/A N/A 12.6
13.8 19.3 11.3
Table 5: Attack impact under the non-IID setting.
AttacksLR
(F-MNIST) CNN (CIFAR-10) ResNet-18 (CIFAR-10)
T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘
None
82.51 81.99 0 52.34
47.45 0 36.67
34.15 0
Top-ğœ…(Ours) 71.88 71.29 10.7 32.99
31.20 16.25 22.82
22.34 12.81
Meta (Ours) 80.27 68.84 13.15 48.36
35.47 11.98 31.91
20.29 13.86
Impact of the Parameter ğ‘€inBadSampler Attacks. Table 4
shows the attack impact of different attack strategies as the percent-
age of compromised clients increases on the CIFAR-10 dataset using
CNN model. Experimental results show that our attack significantly
degrades the aggregation modelâ€™s performance as the proportion
of compromised clients increases. In particular, our attack can still
effectively attack the aggregated model when the proportion of
compromised clients is merely 5%. Compared to other common
data poisoning attacks, our attack is more robust and practical.
Impact of the Parameter ğœ…inBadSampler Attacks. We ex-
plore the effect of the poisoned sample size on Top- ğœ…sampling
attack performance. In this experiment, we change the poisoned
sample size by adjusting the number of adversarial training batches
ğœ…={1,2,4,8}. Table 4 shows that within a certain number range,
the more adversarial training batches ğœ…there are, the better the
attack performance is. For example, when ğœ…=4, our BadSampler
attack performance does not change significantly compared to it
whenğœ…=1. The reason is that the error caused by our attack has
an upper bound, i.e., the number of adversarial training batches
cannot be increased infinitely to enhance the attack performance.
Impact of the Parameter ğµinBadSampler Attacks. Here we
show that if we fix the dataset size, the larger the batch size, the
fewer adversarial training batches we can choose. Table 4 reports
the attack impact Î”whenğµ={16,32,64}. The experimental results
show that the batch size gradually increases, and the impact of
the attack gradually weakens, but it is still stronger than the latest
poisoning attacks. Furthermore, we emphasize that small batch sizes
are commonly used for training in resource-constrained scenarios
[20, 26]. Therefore, our BadSampler attack is practical.
6.2.5 Attack Performance under Non-IID Setting. Last, we evaluate
the attack performance under the non-IID data setting ( ğ›¼=0.5).
In this experiment, we fix the local training epoch ğ¸ğ‘™ğ‘œğ‘ğ‘ğ‘™=5, the
batch sizeğµ=32, and the number of adversarial training batches
ğœ…=2. We use the LR model for classification tasks on the FMNIST
dataset and the CNN model and ResNet-18 model for classification
tasks on the CIFAR-10 dataset. Table 5 shows that our attacks are
still effective in the non-IID setting.
6.2.6 Effectiveness of BadSampler. Here, we will demonstrate why
BadSampler is effective from the perspective of training behavior,
generalization ability evaluation, and generalization ability visual-
ization. To this end, we use the following three metrics [ 17,27]: theTable 6: Numerical results (Group 1: Benign and Benign;
Group 2: Benign and Malicious).
AttacksDğ‘
ğ‘œğ‘  ğ»ğ‘ ğ»ğ·
Gr
oup 1 Gr
oup 2 Gr
oup 1 Gr
oup 2 Gr
oup 1 Gr
oup 2
T
op-ğœ… 1.09 0.73 8456 13256 0.98 0.67
Meta 1.12 0.89 9687 15428 0.97 0.96
Table 7: Numerical result of running time.
Attacks OBLI
VION LMPA DOA Top- ğœ… Meta
Running
Time (s) 185.24
246.32 154.54 86.22 156.48
gradient cosine distance: Dcos, the difference in Hessian norm ( ğ»ğ‘),
and the difference in Hessian direction ( ğ»ğ·). We conducted five
sets of experiments and recorded the mean value of the experimen-
tal results of these five sets of experiments in Table 6. The results
show that the training behavior of our attack is not very different
from that of benign clients, especially the meta-sampling attack
strategy. This means that it is difficult for sophisticated defenders
to quickly determine which clients are malicious, which brings new
challenges to current advanced defenses. Furthermore, we use the
parametric loss landscape [ 17] visualization with Hessian feature
eigenvectors ( ğœ†ğ‘šğ‘ğ‘¥ğ‘¥andğœ†ğ‘šğ‘ğ‘¥ğ‘¦) for the resulting global model be-
fore and after the attack to show the impact of our attack on the
model generalization ability. From Fig. 2, we can clearly see that
the loss landscape of the poisoned model changes more drastically
and its optimization route is not smooth, even the route optimized
to the local optimum is extremely uneven. This means that the
generalization ability of the poisoned model is greatly damaged.
On the contrary, in the absence of attacks, the loss landscape of the
global model changes steadily and its optimization route is smooth
and flat. Please find more details in the Appendix D.
6.2.7 Overhead of BadSampler. We recorded the running time of
the attacks over ten rounds to underscore the low attack-effort char-
acteristic of our approach. Table 7 reveals that the proposed attackâ€™s
running time is lower than LMPA and comparable to DOAs, which
signifies the efficiency of our attack. This efficiency is attributed to
the utilization of Top- ğœ…and RL techniques, which enable us to cir-
cumvent complex optimization processes. Additional experimental
results can be found in Appendix C.
7 CONCLUSIONS
In this work, we introduced BadSampler, a clean-label data poison-
ing attack algorithm targeting Byzantine-robust FL. Such attacks
are designed to maximize the generalization error of the model by
performing adversarial sampling on clean-label data while main-
taining good training behavior during training. Furthermore, we
emphasize that the attack is learnable and adaptive so that it can
be deployed to real FL scenarios and remains effective within strict
parameter constraints. The proposed BadSampler attack undergoes
theoretical analysis and experimental evaluation, demonstrating
its effectiveness on moderate-scale public datasets with various
parameter ranges, defense strategies, and FL deployment scenarios,
highlighting its significant threat to real FL systems.
 
1951BadSampler : Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
REFERENCES
[1]Kareem Amin, Alex Kulesza, Andres Munoz, and Sergei Vassilvtiskii. 2019. Bound-
ing user contributions: A bias-variance trade-off in differential privacy. In Proc.
of ICML.
[2]Sana Awan, Bo Luo, and Fengjun Li. 2021. CONTRA: Defending against Poisoning
Attacks in Federated Learning. In Proc. of ESORICS.
[3]Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov. 2020. How to backdoor federated learning. In Proc. of AISTATS.
[4]Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
2019. Analyzing federated learning through an adversarial lens. In Proc. of ICML.
[5]Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. 2017.
Machine learning with adversaries: Byzantine tolerant gradient descent. In Proc.
of NeurIPS.
[6]Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al .2017. Machine learning
with adversaries: Byzantine tolerant gradient descent. In Proc. of NeurIPS.
[7]Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub KoneÄn `y, Stefano Mazzocchi,
Brendan McMahan, et al .2019. Towards federated learning at scale: System
design. In Proc. of MLSys.
[8]Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2021. FLTrust:
Byzantine-robust Federated Learning via Trust Bootstrapping. In Proc. of NDSS.
[9]Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local model
poisoning attacks to Byzantine-robust federated learning. In Proc. of USENIX
Security.
[10] Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2020. The limitations of
federated learning in sybil settings. In Proc. of RAID.
[11] Stuart Geman, Elie Bienenstock, and RenÃ© Doursat. 1992. Neural networks and
the bias/variance dilemma. Neural computation 4, 1 (1992), 1â€“58.
[12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. In Proc. of ICML.
[13] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,
and Bo Li. 2018. Manipulating machine learning: Poisoning attacks and counter-
measures for regression learning. In Proc. of IEEE S&P.
[14] Malhar S Jere, Tyler Farnan, and Farinaz Koushanfar. 2020. A taxonomy of attacks
on federated learning. IEEE Security & Privacy 19, 2 (2020), 20â€“28.
[15] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al .2017. Overcoming catastrophic forgetting in neural
networks. Proceedings of the national academy of sciences 114, 13 (2017), 3521â€“
3526.
[16] A Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.
Masterâ€™s thesis, University of Tront (2009).
[17] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018.
Visualizing the loss landscape of neural nets. In Proc. of NeurIPS.
[18] Liping Li, Wei Xu, Tianyi Chen, Georgios B Giannakis, and Qing Ling. 2019. RSA:
Byzantine-robust stochastic aggregation methods for distributed learning from
heterogeneous datasets. In Proc. of AAAI.
[19] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2022. Federated learning
on non-iid data silos: An experimental study. In Proc. of ICDE.
[20] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. Federated
learning: Challenges, methods, and future directions. IEEE Signal Processing
Magazine 37, 3 (2020), 50â€“60.
[21] Feng Lin, Qing Ling, and Zhiwei Xiong. 2019. Byzantine-resilient distributed
large-scale matrix completion. In Proc. of ICASSP.
[22] Chen Liu, Zhichao Huang, Mathieu Salzmann, Tong Zhang, and Sabine SÃ¼sstrunk.
2021. On the impact of hard adversarial instances on overfitting in adversarial
training. arXiv preprint arXiv:2112.07324 (2021).
[23] Zhining Liu, Wei Cao, Zhifeng Gao, Jiang Bian, Hechang Chen, Yi Chang, and
Tie-Yan Liu. 2020. Self-paced ensemble for highly imbalanced massive data
classification. In Proc. of ICDE.
[24] Zhining Liu, Pengfei Wei, Jing Jiang, Wei Cao, Jiang Bian, and Yi Chang. 2020.
MESA: boost ensemble imbalanced learning with meta-sampler. In Proc. of
NeurIPS.
[25] Yunlong Mao, Xinyu Yuan, Xinyang Zhao, and Sheng Zhong. 2021. Romoa:
Robust model aggregation for the resistance of federated learning to model
poisoning attacks. In Proc. of ESORICS.
[26] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Proc. of AISTATS.
[27] Matias Mendieta and et al. 2022. Local learning matters: Rethinking data hetero-
geneity in federated learning. In Proc. of CVPR.
[28] Chuizheng Meng, Sirisha Rambhatla, and Yan Liu. 2021. Cross-node federated
graph neural network for spatio-temporal data modeling. In Proc. of KDD.
[29] Jung Wuk Park, Dong-Jun Han, Minseok Choi, and Jaekyun Moon. 2021. Sageflow:
Robust Federated Learning against Both Stragglers and Adversaries. In Proc. of
NeurIPS.[30] Md Mahmudur Rahman and Sanjay Purushotham. 2023. FedPseudo: Privacy-
Preserving Pseudo Value-Based Deep Learning Models for Federated Survival
Analysis. In Proc. of KDD.
[31] Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, and Ahmad-Reza Sadeghi.
2022. DeepSight: Mitigating Backdoor Attacks in Federated Learning Through
Deep Model Inspection. In Proc. of NDSS.
[32] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label
poisoning attacks on neural networks. In Proc. of NeurIPS.
[33] Virat Shejwalkar and Amir Houmansadr. 2021. Manipulating the Byzantine:
Optimizing Model Poisoning Attacks and Defenses for Federated Learning. In
Proc. of NDSS.
[34] Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. 2022.
Back to the drawing board: A critical evaluation of poisoning attacks on federated
learning. In Proc. of IEEE S&P.
[35] Shiqi Shen, Shruti Tople, and Prateek Saxena. 2016. Auror: Defending against
poisoning attacks in collaborative deep learning systems. In Proc. of ACSAC.
[36] Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Paper-
not, Murat A Erdogdu, and Ross Anderson. 2021. Manipulating SGD with data
ordering attacks. In Proc. of NeurIPS.
[37] Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. 2021. On the
Origin of Implicit Regularization in Stochastic Gradient Descent. In Proc. of ICLR.
[38] Qiheng Sun, Xiang Li, Jiayao Zhang, Li Xiong, Weiran Liu, Jinfei Liu, Zhan Qin,
and Kui Ren. 2023. Shapleyfl: Robust federated learning based on shapley value.
InProc. of KDD.
[39] Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. 2020. Data
poisoning attacks against federated learning systems. In Proc. of ESORICS.
[40] Fei Wang, Ethan Hugh, and Baochun Li. 2023. More than Enough is Too Much:
Adaptive Defenses against Gradient Leakage in Production Federated Learning.
InProc. of INFOCOM.
[41] Yansheng Wang, Yongxin Tong, Zimu Zhou, Ziyao Ren, Yi Xu, Guobin Wu, and
Weifeng Lv. 2023. Fed-LTD: Towards cross-platform ride hailing via federated
learning to dispatch. In Proc. of KDD.
[42] Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2023. Fe-
dAttack: Effective and covert poisoning attack on federated recommendation via
hard sampling. In Proc. of KDD.
[43] Xidong Wu, Zhengmian Hu, Jian Pei, and Heng Huang. 2023. Serverless federated
auprc optimization for multi-party collaborative imbalanced data mining. In Proc.
of KDD.
[44] Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. 2020. Feder-
ated variance-reduced stochastic gradient descent with robustness to byzantine
attacks. IEEE Transactions on Signal Processing 68 (2020), 4583â€“4596.
[45] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[46] Gang Yan, Hao Wang, Xu Yuan, and Jian Li. 2023. Criticalfl: A critical learning
periods augmented client selection framework for efficient federated learning. In
Proc. of KDD.
[47] Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. 2020. Re-
thinking bias-variance trade-off for generalization of neural networks. In Proc. of
ICML.
[48] Jingwei Yi, Fangzhao Wu, Bin Zhu, Jing Yao, Zhulin Tao, Guangzhong Sun, and
Xing Xie. 2023. UA-FedRec: untargeted attack on federated news recommendation.
InProc. of KDD.
[49] Dong Yin, Yudong Chen, and Ramchandran Kannan. 2019. Defending against
saddle point attack in Byzantine-robust distributed learning. In Proc. of ICML.
[50] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018.
Byzantine-robust distributed learning: Towards optimal statistical rates. In Proc.
of ICML.
[51] Chen Zhang, Boyang Zhou, Zhiqiang He, Zeyuan Liu, Yanjiao Chen, Wenyuan
Xu, and Baochun Li. 2023. Oblivion: Poisoning Federated Learning by Inducing
Catastrophic Forgetting. In Proc. of INFOCOM.
[52] Xiaoli Zhang, Fengting Li, Zeyu Zhang, Qi Li, Cong Wang, and Jianping Wu. 2020.
Enabling execution assurance of federated learning at untrusted participants. In
Proc. of INFOCOM.
[53] Zaixi Zhang, Xiaoyu Cao, Jinayuan Jia, and Neil Zhenqiang Gong. 2022. FLDe-
tector: Defending Federated Learning Against Model Poisoning Attacks via
Detecting Malicious Clients. In Proc. of KDD.
[54] Yifeng Zheng, Shangqi Lai, Yi Liu, Xingliang Yuan, Xun Yi, and Cong Wang.
2023. Aggregation Service for Federated Learning: An Efficient, Secure, and More
Resilient Realization. IEEE Transactions on Dependable and Secure Computing 20,
2 (2023), 988â€“1001. https://doi.org/10.1109/TDSC.2022.3146448
[55] Xiaoling Zhou, Ou Wu, Weiyao Zhu, and Ziyang Liang. 2022. Understanding
Difficulty-based Sample Weighting with a Universal Difficulty Measure. In Proc.
of ECML-PKDD.
 
1952KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yi Liu, Cong Wang, & Xingliang Yuan
Table 8: Summary of practical parameters/settings for pro-
duction FL [34]. Note that we use green vsred to distinguish
whether parameters/settings are practical or not.
Parameters/Settings What
we argue to be practicalUse
d in previous
data poisoning works
FL
type + Attack typeCr
oss-silo + DPAs
Cross-device + DPAsCr
oss-silo + MPAs
T
otal number of FL
clients,ğ¾Or
der of[101,1010]for cross-device
[2,10]for cross-silo[50,
100]
Numb
er of clients
chosen per round, ğ‘˜Small
fractionğ‘ofğ¾for cross-device
All for cross-siloAll
%
of compromised
clients,ğ‘€ğ‘€â‰¤10%
for DPAs [20,50]%
A
verage size of benign
clientsâ€™ data,|ğ·|avg[50,1000] for
cross-deviceNot
studied for cross-device
[50,1000] for cross-silo
Maximum
size of
local poisoning dataUp
to100Ã—|ğ·|ğ‘˜for DPAs âˆ¼
|ğ·|ğ‘˜
Table 9: Attack impact with different surrogate models.
AttacksLeNet-5
DenseNet AlexNet
T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘
None
63.73 59.10 0 63.73
59.10 0 63.73
59.10 0
Top-ğœ…(Ours) 43.60 40.92 18.18 44.80
41.32 17.78 45.13
44.20 14.9
Meta (Ours) 49.17 39.80 19.30 48.36
40.47 18.63 50.98
43.49 15.61
A PRODUCTION FEDERATED LEARNING
Production FL Settings. Prior works [ 3,4,9,42,48] make several
unrealistic assumptions in FL poisoning attacks in terms of the
total number of clients, the percentage of compromised clients,
and the knowledge held by the adversary. This means that the
current literature is still far from practice. Our approach focuses
on creating a new poisoning attack in this realistic environment
to close this gap. Briefly, the difference between the production FL
environment and the FL environment in the literature is the range of
practical FL parameters. Following the work [ 34], we summarize the
differences between the practical FL parameters (called production
FL parameters [ 34]) and the ones used in existing attacks in Table 8.
Nonetheless, in this paper, our attack still needs to effectively attack
the FL system in such a setting. Notably, to confirm the efficacy of
our approach, our experimental setup adheres to every practical
parameter setting in Table 8.
BBADSAMPLER ATTACK ALGORITHMS
Here, we provide a detailed version of the pseudocode for the de-
signed BadSampler attack algorithm, where Algorithm 2 demon-
strates the Top-k sampling attack strategy, and Algorithms 3 demon-
strates the meta-sampling attack strategy.
C ADDITIONAL EXPERIMENTS
IID and Non-IID setting: (i)IID: Here we detail our IID settings.
For the CIFAR-10 dataset, we allocate 860 samples to each client
on average; that is, a total of 48,000 data samples are used as the
training set, and the server stores 12,000 samples as the test set.
For the F-MNIST dataset, we allocate 1,020 data samples to each
client on average, that is, a total of 56,000 data as the local training
set, and the server stores 14,000 samples as the test set. (ii)Non-
IID: We adopt the classic non-IID setting [ 19,26,41] where each
client can only hold at most two classes of data samples. To do this,
we distribute the dataset equally to each client by data class. For
example, for client ğ‘˜and CIFAR-10 dataset, its local dataset only
has two kinds of data samples with category code â€œ0â€ and â€œ1â€.Algorithm 1: Description of the attack steps of the Bad-
Sampler attack algorithm
/* â€“ Step 1: Reading Data Index â€“ */
1do
2 read data index from local clients in the form of training
samples
3 get a new data point and add it to a list of unseen data
index
4while complete step 1
5while local training do
/* â€“ Step 2: Computing Error Value â€“ */
6 Calculate sample difficulty or Ë†ğ¸ğ·using a surrogate
model;
/* â€“ Step 3: Replacing Sampling Function â€“ */
7 ifATK == "Top-ğœ…Sampling" then
8 sort each sample with the sample difficulty
9 obtain the hard sample pool Hğ‘ğ‘ğ‘‘â–·Refer to Eq. (7)
10 perform local training on the adversarial training
batch fromHğ‘ğ‘ğ‘‘
11 else
12 construct meta-state and optimize it using SAC
strategy
13 obtain the trained adversarial sampler ğœ‰(Â·)
14 replace the sampling function and perform
adversarial sampling
15 train the local model on these adversarial training
batches
16end
/* â€“ Step 4: Uploading Model Updates â€“ */
17upload the model updates to the server
Impact of the Non-IID Degree in BadSampler Attacks. Here,
we verify the performance of BadSampler on the CIFAR-10 dataset
and set the Dirichlet parameter ğ›¼={0.1,0.5,1}. The experimental
results are shown in Table 10.
Impact of the Surrogate Models in BadSampler Attacks.
Here, we add a set of additional experiments to explore the im-
pact of different surrogate models on the attack effectiveness of
the proposed attack. We set the batch size ğµ=32, the number of
candidate adversarial training batches ğœ…=2, and use FedAvg [ 26]
as AGR. We use the CNN model to conduct classification tasks
on the CIFAR-10 dataset, and we use the LeNet-5, DenseNet, and
AlexNet models as our surrogate models. Table 9 summarizes the
experimental results showing that the surrogate model does not
significantly affect the performance of the proposed attack.
D WHY BADSAMPLER WORK?
Here, we will demonstrate why BadSampler is effective from the
perspective of training behavior, generalization ability evaluation,
and generalization ability visualization.
Training Behavior: To demonstrate the training behavior of the
designed attack, we use the cosine distance of the gradient between
clients to represent it. Specifically, we divide the clients into two
groups, benign and malicious, and then we calculate the cosine
distances of gradients between benign clients and between benign
 
1953BadSampler : Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Algorithm 2: Top-ğœ…sampling attack algorithm
Input: Surrogate model ğ‘“ğ‘ , the function ğ‘”ğ‘’ğ‘¡_ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥()to get
the next index of real data, the function ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘“ğ‘˜,B)
that trains local model ğ‘“ğ‘˜on a batch of dataB, and
the function ğ‘’ğ‘Ÿğ‘Ÿğ‘–(ğ‘“ğ‘ ,ğ‘¥ğ‘–|ğ·)to compute sampleâ€™s
generalization error.
Output: Global model ğœ”â€²
/* Step 1: Reading Data Index */
1data_index =[]
2do
3ğ¼ğ‘–=ğ‘”ğ‘’ğ‘¡_ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥(ğ‘¥ğ‘–)
4 if|I|==ğµthen
5 pass batchBto modelğ‘“ğ‘ 
6 else
7 add individual sample index ğ¼ğ‘–into data_index
8 train(ğ‘“ğ‘ ,B)
9while complete step 1
10while local training do
/* Step 2: Computing Error Value */
11 compute sample difficulty ğ‘’ğ‘Ÿğ‘Ÿğ‘–viağ‘’ğ‘Ÿğ‘Ÿğ‘–(ğ‘“ğ‘ ,ğ‘¥ğ‘–|ğ·)
12 build a hard sample candidate pool Hğ‘ğ‘ğ‘‘via Eq. (7)
/* â€“ Step 3: Replacing Sampling Function â€“ */
13 get adversarial training batch Bâ€²âˆ¼Hğ‘ğ‘ğ‘‘
14 passBâ€²to the local model ğ‘“ğ‘˜
15 conduct local training, i.e.,ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘“ğ‘˜,Bâ€²)
16end
/* â€“ Step 4: Uploading Model Updates â€“ */
17upload the model updates to the server
and malicious clients. Next, we give the formal definition of the
gradient cosine distance between clients as follows:
Dğ‘–,ğ‘—
cos=1âˆ’cosğœƒ=1âˆ’<gğ‘–,gğ‘—>
||gğ‘–||Â·||gğ‘—||, (13)
where grepresents the gradient vector of the client. Here, we stip-
ulate that the larger the cosine distance is, the closer the direction
of gradient descent between clients is, which means the closer the
training behavior is. In the experiment, we conducted five sets of
experiments and recorded the mean value of the experimental re-
sults of these five sets of experiments in Table 6. The results show
that the training behavior of our attack is not very different from
that of benign clients, especially the meta-sampling attack strategy.
This means that it is difficult for sophisticated defenders to quickly
determine which clients are malicious, which brings new challenges
to current advanced defenses.
Generalization Ability Visualization: In the machine learning
generalization research domain [ 17,27], top Hessian eigenvalues
(ğœ†ğ‘šğ‘ğ‘¥) and Hessian trace ( ğ»ğ‘‡) are generally used as key indicators
for evaluating model generalization capabilities. In practice, a net-
work with lower ğœ†ğ‘šğ‘ğ‘¥ andğ»ğ‘‡is generally a network with stronger
generalization ability, that is, the network is less sensitive to small
perturbations. This is because lower ğœ†ğ‘šğ‘ğ‘¥ andğ»ğ‘‡indicate a more
balanced loss space during training and a flatter route to the mini-
mum point. Therefore, we use the parametric loss landscape [ 17]
visualization with Hessian feature eigenvectors ( ğœ†ğ‘šğ‘ğ‘¥ğ‘¥andğœ†ğ‘šğ‘ğ‘¥ğ‘¦)
for the resulting global model before and after the attack to showAlgorithm 3: Meta sampling attack algorithm
Input: Surrogate model ğ‘“ğ‘ , the function ğ‘”ğ‘’ğ‘¡_ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥()to get
the next index of real data, and the function
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘“ğ‘˜,B)that trains local model ğ‘“ğ‘˜on a batch of
dataB.
Output: Global model ğœ”â€²
/* Step 1: Reading Data Index */
1data_index =[]
2do
3ğ¼ğ‘–=ğ‘”ğ‘’ğ‘¡_ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥(ğ‘¥ğ‘–)
4 if|I|==ğµthen
5 pass batchBto modelğ‘“ğ‘ 
6 else
7 add individual sample index ğ¼ğ‘–into data_index
8 train(ğ‘“ğ‘ ,B)
9while complete step 1
/* Step 2: Computing Error Value */
10while meta training do
11 train(ğ‘“ğ‘ ,Bâ€²) at timeğ‘¡
12 use surrogate model ğ‘“ğ‘ to compute Ë†ğ¸ğ·ğ‘¡and Ë†ğ¸ğ·ğ‘£via Eq.
(8)
13 build the meta-state via Eq. (9)
14ğœ‡ğ‘¡âˆ¼ğœ‰(ğœ‡ğ‘¡|ğ‘ ğ‘¡)
15 sampleBâ€²via Gaussian function trick â–·Refer to Eq. (??)
16 train(ğ‘“ğ‘ ,Bâ€²) at time(ğ‘¡+1)
17end
18while local training do
19 update the adversarial meta-sampler ğœ‰(Â·)via SAC
/* â€“ Step 3: Replacing Sampling Function â€“ */
20 deploy the adversarial meta sampler ğœ‰(Â·)
21 get adversarial training batches Bâ€²âˆ¼ğœ‰(Â·)
22 passBâ€²to the local model ğ‘“ğ‘˜
23 conduct local training, i.e.,ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘“ğ‘˜,Bâ€²)
24end
/* â€“ Step 4: Uploading Model Updates â€“ */
25upload the model updates to the server
Table 10: Attack impact under the non-IID setting.
Attacksğ›¼=0.1 ğ›¼=0.5 ğ›¼=1
T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘T
rain
acc (%)Test
acc (%)Î”â†‘
None
49.14 47.45 0 52.34
47.45 0 54.67
48.74 0
Top-ğœ…(Ours) 42.85 28.98 18.47 51.88
32.14 16.60 22.82
22.34 12.81
Meta (Ours) 37.75 26.25 21.20 48.36
35.47 11.98 52.87
21.34 27.40
the impact of our attack on the model generalization ability. From
Fig. 2, we can clearly see that the loss landscape of the poisoned
model changes more drastically and its optimization route is not
smooth, even the route optimized to the local optimum is extremely
uneven. This means that the generalization ability of the poisoned
model is greatly damaged. On the contrary, in the absence of an
attack, the loss landscape of the global model changes steadily and
its optimization route is smooth and flat.
Generalization Ability Evaluation: To further quantify the
modelâ€™s generalization error, we follow reference [ 27] to compute
 
1954KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Yi Liu, Cong Wang, & Xingliang Yuan
1
Without 
Attack
With 
Top-k Sampling AttackWith 
Meta Sampling Attack
Figure 2: Visual overview of Hessian eigenvalue distributions for benign versus poisoned models.
the difference in Hessian norm ( ğ»ğ‘) and the difference in Hessian
direction (ğ»ğ·) across clients. More specifically, ğ»ğ‘can be used
to represent the closeness of model generalization ability between
clients andğ»ğ·can be used to represent the correlation of training
performance. We consider that the model generalization ability of
benign clients is generally good, so the above indicators can be used
to indirectly quantify the model generalization ability of malicious
clients. The formal definition of the above indicators is as follows:
ğ»ğ‘–,ğ‘—
ğ‘= 
Diag(Hğ‘–)ğ¹âˆ’Diag
Hğ‘—ğ¹!2
, (14)
ğ»ğ‘–,ğ‘—
ğ·=Diag(Hğ‘–)âŠ™Diag
Hğ‘—
Diag(Hğ‘–)ğ¹Â·Diag
Hğ‘—ğ¹, (15)
whereâŠ™is the dot product, Diag(Hğ‘–)is the diagonal matrix of the
Hessian matrix, and ||Â·||ğ¹is the Frobenius norm. In the experiment,
we conducted five sets of experiments and recorded the mean value
of the experimental results of these five sets of experiments in Table
6. From the experimental results, we know that our attack strategy
can seriously damage the generalization ability of the model, but
the correlation between the training performance of the Top- ğœ…
sampling strategy and the benign client is relatively different. This
may be the reason why the Top- ğœ…sampling strategy is easier to
defend than the meta-sampling attack strategy.
E PROOFS
E.1 Proof of Lemma 1
Proof. According to Eq. (5), we have: ğœ‰(ğœ”) =
ğ‘âˆ’1Ã
ğ‘—=0Ã
ğ‘–<ğ‘—âˆ‡âˆ‡â„“ğ‘—(ğœ”)â„“ğ‘–(ğœ”). If assumption 3 holds, thus, we have:
E[ğœ‰(ğœ”)]=1
2 Ã•ğ‘âˆ’1
ğ‘—=0Ã•
ğ‘˜â‰ ğ‘—âˆ‡âˆ‡â„“ğ‘—(ğœ”)âˆ‡â„“ğ‘˜!
=ğ‘2
2âˆ‡âˆ‡ğ¹(ğœ”)âˆ‡ğ¹(ğœ”)âˆ’1
2Ã•ğ‘âˆ’1
ğ‘—=0âˆ‡âˆ‡â„“ğ‘—(ğœ”)â„“ğ‘—(ğœ”)
=ğ‘2
4âˆ‡ 
||âˆ‡ğ¹(ğœ”)||2âˆ’1
ğ‘2Ã•ğ‘âˆ’1
ğ‘—=0||â„“ğ‘—(ğœ”)||2!
.
â–¡E.2 Proof of Lemma 2
Proof. Recall that,ğœ”ğ‘¡=ğœ”0âˆ’ğ‘ğœ‚âˆ‡â„“(ğœ”0)+ğœ‚2ğœ‰(ğœ”0)+O(ğ‘3ğœ‚3),
thus, we have: E(ğœ”ğ‘¡)=ğœ”0âˆ’ğ‘ğœ‚âˆ‡â„“(ğœ”0)+ğœ‚2E(ğœ‰(ğœ”0))+ğ‘‚(ğ‘3ğœ‚3).
Then we according to Lemma 2 have:
E(ğœ”ğ‘¡)=ğœ”0âˆ’ğ‘ğœ‚âˆ‡ğ¹(ğœ”0)
+ğ‘2ğœ‚2
4(||âˆ‡ğ¹(ğœ”0)||2âˆ’1
ğ‘2Ã•ğ‘âˆ’1
ğ‘—=0||â„“ğ‘—(ğœ”0)||2)+ğ‘‚(ğ‘3ğœ‚3).
â–¡
E.3 Proof of Theorem 1
Proof. First, we expand the error term expec-
tation as follows: E[||âˆ‡ğ¹(ğœ”âˆ—) âˆ’ âˆ‡ğ¹(ğœ”ğ‘)||2] =
1
ğµ2EğµÃ
ğ‘–=1ğµÃ
ğ‘—=1(âˆ‡ğ¹ğ‘–(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”âˆ—))Â·(âˆ‡ğ¹ğ‘—(ğœ”ğ‘) âˆ’ âˆ‡ğ¹(ğœ”ğ‘))
. To
keep the notation clean, we let ğ‘‹ğ‘–=(âˆ‡ğ¹ğ‘–(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”âˆ—))and
ğ‘‹ğ‘—=(âˆ‡ğ¹ğ‘–(ğœ”ğ‘)âˆ’âˆ‡ğ¹(ğœ”ğ‘)). Thus, we have:
E[||âˆ‡ğ¹(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”ğ‘)||2]=1
ğµ2E(ğµÃ•
ğ‘–=1ğµÃ•
ğ‘—=1(ğ‘‹ğ‘–Â·ğ‘‹ğ‘—))
=1
ğµE(ğ‘‹ğ‘–Â·ğ‘‹ğ‘—)+ğµâˆ’1
ğµE(ğ‘‹ğ‘–Â·ğ‘‹ğ‘—â‰ ğ‘–)
Since Lemma 1 and Lemma 2 hold, we have:
E[||âˆ‡ğ¹(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”ğ‘)||2]=1
ğ‘ğµğ‘Ã•
ğ‘–=1ğ‘‹ğ‘–Â·ğ‘‹ğ‘–+(ğµâˆ’1)
ğµğ‘(ğ‘âˆ’1)ğ‘Ã•
ğ‘–=1Ã•
ğ‘—â‰ ğ‘–ğ‘‹ğ‘–Â·ğ‘‹ğ‘—
Next, we recall thatÃğ‘
ğ‘–=1ğ‘‹ğ‘–=Ãğ‘
ğ‘–=1
âˆ‡ğ¹ğ‘–(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”)
=0, thus
we have:
E[||âˆ‡ğ¹(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”ğ‘)||2]â©½1
ğ‘ğµğ‘Ã•
ğ‘–=1ğ‘‹ğ‘–Â·ğ‘‹ğ‘–âˆ’(ğµâˆ’1)
ğµğ‘(ğ‘âˆ’1)ğ‘Ã•
ğ‘–=1ğ‘‹ğ‘—Â·ğ‘‹ğ‘—
=1
ğ‘ğµ(ğ‘Ã•
ğ‘–=1ğ‘‹ğ‘–Â·ğ‘‹ğ‘–âˆ’ğµâˆ’1
ğ‘âˆ’1ğ‘Ã•
ğ‘–=1ğ‘‹ğ‘—Â·ğ‘‹ğ‘—).
If Assumptions 1â€“3 hold, thenÃğ‘
ğ‘–=1ğ‘‹ğ‘–Â·ğ‘‹ğ‘–â©½ğ‘(1âˆ’1
ğ‘)2ğœ€2
0andÃğ‘
ğ‘–=1ğ‘‹ğ‘—Â·ğ‘‹ğ‘—â©½ğ‘(1âˆ’1
ğ‘)2ğœ€2
ğ‘, thus we have:
E[||âˆ‡ğ¹(ğœ”âˆ—)âˆ’âˆ‡ğ¹(ğœ”ğ‘)||2]â©½ğµ(1âˆ’1
ğ‘)2ğœ€2
0âˆ’ğµâˆ’1
ğ‘ğµğœ€2
ğ‘.
â–¡
 
1955