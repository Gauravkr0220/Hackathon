Model-Agnostic Random Weighting for Out-of-Distribution
Generalization
Yue Heâˆ—
Tsinghua University
Beijing, China
hy865865@gmail.comPengfei Tianâˆ—
Tsinghua University
Beijing, China
e9tian@gmail.comRenzhe Xu
Tsinghua University
Beijing, China
xrz@199721gmail.com
Xinwei Shen
ETH ZÃ¼rich
ZÃ¼rich, Switzerland
xinwei.shen@stat.math.ethz.chXingxuan Zhang
Tsinghua University
Beijing, China
xingxuanzhang@hotmail.comPeng Cuiâ€ 
Tsinghua University
Beijing, China
cuip@tsinghua.edu.cn
Abstract
Despite the encouraging successes in numerous applications, ma-
chine learning methods grounded on the i.i.d. assumption often
experience performance deterioration when confronted with the
distribution shift between training and test data. This challenge has
instigated recent research endeavors focusing on out-of-distribution
(OOD) generalization. A particularly pervasive and intricate OOD
problem is to enhance the modelâ€™s generalization ability by training
it on samples drawn from a single environment. In response to
the problem, we propose a simple model-agnostic method tailored
for a practical OOD scenario in this paper. Our approach centers
on pursuing robust weighted empirical risks, utilizing randomly
shifted training distributions derived through a specific sample-
based weighting strategy. Furthermore, we theoretically establish
that the expected risk of the shifted training distribution can bound
the expected risk of the test distribution. This theoretical foundation
ensures the improved prediction performance of our method when
employed in uncertain test distributions. Extensive experiments
conducted on diverse real-world datasets affirm the effectiveness
of our method, highlighting its potential to address the distribution
shifts in machine learning applications.
CCS Concepts
â€¢Computing methodologies â†’Machine learning.
Keywords
Out-of-Distribution Generalization, Distribution shift, Sample Weight-
ing
âˆ—Both authors contributed equally to this research.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671762ACM Reference Format:
Yue He, Pengfei Tian, Renzhe Xu, Xinwei Shen, Xingxuan Zhang, and Peng
Cui. 2024. Model-Agnostic Random Weighting for Out-of-Distribution Gen-
eralization. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671762
1 Introduction
In past years, the field of machine learning has experienced un-
precedented growth, attributed to advanced data-driven modeling
techniques and powerful computational capabilities. Traditional
machine learning methods often rely on the assumption that train-
ing and test data are independent and identically distributed (i.i.d.).
The model, that achieves empirical risk minimization (ERM) on
training samples, is expected to approximately attain the minimal
expected risk on test samples under the i.i.d. assumption. However,
real-world applications often involve distribution shifts between
training and test data due to the heterogeneity and uncertainty of
real data. This results in the test risk of models based on the i.i.d. as-
sumption being unguaranteed, and prone to deterioration, which is
very detrimental for high-stake scenarios [ 8,39,48]. To address the
challenge of generalizing a model to data drawn out-of-distribution
(OOD) [ 23], researchers have intensively studied the problem of
OOD generalization recently.
To solve the OOD problem, various approaches from differ-
ent paradigms have been investigated. Dai et al . [9], Ganin et al .
[16], Sun and Saenko [45] are designed to transition the model
from a source domain to a target domain by leveraging test data
information. However, having access to such test information prior
is often not feasible. To enhance the non-targeted generalization
ability, a strand of methods proposes to utilize the labels of multi-
ple heterogeneous data environments that training samples come
from. They aim to learn the invariant representation [ 3,6,24,36]
or domain-agnostic model [ 17,27,28] that is irrelevant to the envi-
ronment labels, so that the model can be deployed in the unknown
environment well in virtue of invariance [ 38]; or directly learn
a model that can generalize to the combinations of these envi-
ronments [ 25], enabling the model with improved precision and
reliability across environments. Nonetheless, the performances of
these methods heavily depend on the availability and heterogeneity
of multi-environments [2], which are difficult to guarantee in real
applications, limiting their practical deployment.
 
1050
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue He et al.
In contrast, another strand focuses on a more general setting of
the OOD problem, that is to enhance the generalization ability of
the model upon a single training environment. The distribution-
ally robust optimization (DRO) [ 12,35,44] minimizes the worst
expected risk of the distributions in a pre-defined uncertainty set
of test distributions. Creager et al . [7], Liu et al . [32] assume the
training data is a mixture of multi-source data, iteratively mine the
heterogeneity inside data for environment partition and learn an
invariant model from them. Recently, some model-agnostic meth-
ods are proposed to deal with the distribution shift in common
tasks. JTT [ 31] and LfF [ 37] up-weight the samples that are falsely
identified by a biased classifier to train another unbiased classifier.
RWY [ 22] adjusts the imbalanced ratio of positive and negative
samples. Focal loss [ 29] and CVaR [ 47] concentrate on the training
samples of high risks. Despite the advantages in simplicity and
universality, these methods are either designed for a special type
of data bias or lack the theoretical guarantee, resulting in their
unstable efficacy.
Considering both impacts of the changed population of samples
and the distinct system biases1[34,49] in data environments, we
propose a novel model-agnostic model towards better OOD gener-
alization in this paper. Suppose ğ‘‹andğ‘Œdenote the observational
covariates and outcome variable, respectively. Our method MARW
(Model- Agnostic Random Weighting) randomly shifts the training
distribution through reweighting the training samples according
to the function ğ‘¤(ğ‘‹,ğ‘Œ)from a specific functional space W, and
pursues the robust weighted empirical risks of the shifted distribu-
tions. We further theoretically prove that the expected risk of test
distribution can be upper bounded by the expected risk of train-
ing distribution shifted by ğ‘¤(ğ‘‹,ğ‘Œ). As a result, our method can
facilitate better performance of the prediction model in potential
test distributions, then shows its consistent advantages on OOD
generalization in a range of actual tasks empirically.
In summary, our contributions are highlighted as follows:
â€¢We propose a novel model-agnostic method called MARW
for the out-of-distribution (OOD) generalization problem,
which pursues the robust weighted empirical risks of the ran-
domly shifted training distributions derived from a specific
sample-based weighting, to achieve the better predictions in
the uncertain test distributions.
â€¢We theoretically demonstrate that the expected risk of the
shifted training distribution can bound the expected risk of
the non-i.i.d. test distribution, which proves the rationality
of our method for OOD generalization.
â€¢We conduct extensive experiments on various real-world
datasets to validate the effectiveness of MARW in withstand-
ing the distribution shifts in common tasks.
The rest of this paper is organized as follows. Section 2 gives the
formulation of the OOD problem that we study. Section 3 introduces
our proposed Model-Agnostic Random Weighting method. Section 4
provides the theoretical analysis of our approach. Section 5 reviews
the related work. Section 6 presents the experimental results. Finally,
Section 7 concludes the paper.
1The system bias is widely seen in real applications. The batch effect often appears
in the results of the same biomedical experiment conducted by different experts. It is
also prone to introduce the individual prior biases when labeling the images.2 Preliminary
2.1 Problem Setup
2.1.1 Notations. We defineğ‘‹âˆˆ X âŠ† Rğ‘‘andğ‘Œâˆˆ Y âŠ† Ras
random variables representing the covariates with dimension ğ‘‘and
the outcome, respectively. XandYare the space of the covariates
and outcome, respectively. The focus of our paper is on the Out-of-
Distribution (OOD) problem, with Eğ‘ğ‘™ğ‘™denoting the set of potential
environments. The distribution of data within each environment
ğ‘’âˆˆEğ‘ğ‘™ğ‘™is expressed as ğ‘ƒğ‘’(ğ‘‹,ğ‘Œ). The predictive model is denoted
asË†ğ‘“ğœƒ:Xâ†’Y with parameters ğœƒâˆˆÎ˜, and the loss function, given
model parameters ğœƒand a sample with covariates and outcome
(ğ‘¥,ğ‘¦), is represented as â„“(ğ‘¥,ğ‘¦;ğœƒ).
Givenğ‘training samples{(ğ‘¥ğ‘–,ğ‘¦ğ‘–)}ğ‘
ğ‘–=1from a specific training
environment ğ‘’ğ‘¡ğ‘ŸâˆˆEğ‘ğ‘™ğ‘™, distributed according to ğ‘ƒğ‘’ğ‘¡ğ‘Ÿ(ğ‘‹,ğ‘Œ), our
task is to ensure the good performance in an unseen test envi-
ronmentğ‘’ğ‘¡ğ‘’âˆˆEğ‘ğ‘™ğ‘™, characterized by the distribution ğ‘ƒğ‘’ğ‘¡ğ‘’(ğ‘‹,ğ‘Œ).
In other words, we aim to minimize the expected loss in the test
environment, Eğ‘ƒğ‘’ğ‘¡ğ‘’[â„“(ğ‘‹,ğ‘Œ;ğœƒ)]. To simplify the notations, we use
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘‹,ğ‘Œ)andğ‘ƒğ‘¡ğ‘’(ğ‘‹,ğ‘Œ)to denote the training distribution and test
distribution, respectively.
2.1.2 OOD setting setup. Since the general OOD problem is in-
tractable without further assumptions on the set of all possible
environmentsEğ‘ğ‘™ğ‘™[30], we consider a practical setting in this pa-
per, which assumes the distribution ğ‘ƒğ‘’(ğ‘‹,ğ‘Œ)for each environment
satisfies the following generation function: âˆ€ğ‘’âˆˆEğ‘ğ‘™ğ‘™,
ğ‘‹âˆ¼ğ‘ƒğ‘’(ğ‘‹), ğ‘Œ =ğ‘“(ğ‘‹+ğœ‚ğ‘’)+ğœ‡ğ‘’+ğœ–, ğœ–âˆ¼N( 0,ğœ2
ğ‘’).(1)
Hereğ‘ƒğ‘’(ğ‘‹)is the marginal distribution on ğ‘‹andğœ‚ğ‘’âˆˆRğ‘‘,ğœ‡ğ‘’âˆˆ
R,ğœğ‘’âˆˆR+are parameters for each environment ğ‘’âˆˆEğ‘ğ‘™ğ‘™.ğ‘“:Rğ‘‘â†’
Ris an invariant labeling function across all environments.
In this setting, we employ the parameters ğœ‚ğ‘’andğœ‡ğ‘’to repre-
sent the system biases affecting the observed values of covariates
and outcomes, respectively, for each environment ğ‘’. System biases
often change across different data batches due to distinct mea-
surement conditions [ 15,49]. Additionally, we characterize the
diverse populations denoted by ğ‘ƒğ‘’(ğ‘‹)[42] and the varying signal-
to-noise ratios represented by N(0,ğœ2ğ‘’)[1] across environments,
that are frequently encountered in reality. The goal of this paper is
to achieve out-of-distribution generalization for environments spec-
ified by Equation (1), where the data distribution ğ‘ƒğ‘’(ğ‘‹,ğ‘Œ)shifts
with changes in ğ‘ƒğ‘’(ğ‘‹),ğœğ‘’,ğœ‚ğ‘’, andğœ‡ğ‘’.
2.2 Weighted Empirical Risk Minimization
Our method is based on weighted empirical risk minimization.
In i.i.d. settings, the empirical loss is given by Eğ‘ƒğ‘¡ğ‘Ÿ[â„“(ğ‘‹,ğ‘Œ;ğœƒ)]â‰ˆ
1/ğ‘Ãğ‘
ğ‘–=1â„“(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğœƒ). However, for a non-i.i.d. test distribution where
ğ‘ƒğ‘¡ğ‘’(ğ‘‹,ğ‘Œ)â‰ ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘‹,ğ‘Œ), we employ the weighted empirical loss to
approximate the population-level test loss as follows:
Eğ‘ƒğ‘¡ğ‘’[L(ğ‘‹,ğ‘Œ;ğœƒ)]=âˆ«ğ‘ƒğ‘¡ğ‘’(ğ‘¥,ğ‘¦)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)â„“(ğ‘¥,ğ‘¦;ğœƒ)ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦
â‰ˆ1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ‘¤(ğ‘¥ğ‘–,ğ‘¦ğ‘–)â„“(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğœƒ),(2)
 
1051Model-Agnostic Random Weighting for Out-of-Distribution Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
whereğ‘¤(ğ‘¥,ğ‘¦)=ğ‘ƒğ‘¡ğ‘’(ğ‘¥,ğ‘¦)/ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)is the importance sampling
weight [ 14,19]. Notably, we can also leverage a suitable weight-
ing function ğ‘¤(ğ‘¥,ğ‘¦)to simulate test environments [ 46], provided
that the weighting function satisfies the normalization constraint
Eğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¤(ğ‘‹,ğ‘Œ)]=1.
3 Model-Agnostic Random Weighting
In this section, we present our proposed method, Model-Agnostic
Random Weighting, abbreviated as MARW. The MARW methodol-
ogy comprises two primary components: random weighting and
variance-based optimization. These components are discussed in
the following two subsections.
3.1 Characterizing the Weighting Function
Class
In this subsection, we provide a proper way to parametrize the
space of feasible weighting functions to simulate unknown test
environments.
All possible weighting functions. We first formulate the ground-
truth weighting function class for all environments in Eğ‘ğ‘™ğ‘™. Let
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘‹),ğœ‚ğ‘¡ğ‘Ÿ,ğœ‡ğ‘¡ğ‘Ÿ,ğœğ‘¡ğ‘Ÿbe the parameters in the training distribution
ğ‘ƒğ‘¡ğ‘Ÿas shown in Equation (1). Then the probability density ratio
between the distribution ğ‘ƒğ‘’(ğ‘¥,ğ‘¦)of any environment ğ‘’âˆˆEğ‘ğ‘™ğ‘™and
the training distribution ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)can be formulated as âˆ€ğ‘¥âˆˆX,ğ‘¦âˆˆ
Y,
ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)â‰œğ‘ƒğ‘’(ğ‘¥,ğ‘¦)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)=exp
ğ´ğ‘’ğ‘¦2+ğµğ‘’ğ‘¦
â„ğ‘’(ğ‘¥)ğ‘”ğ‘’(ğ‘¥,ğ‘¦),(3)
where
ğ´ğ‘’=1
2ğœ2
ğ‘¡ğ‘Ÿâˆ’1
2ğœ2ğ‘’, ğµğ‘’=ğœ‡ğ‘’
ğœ2ğ‘’âˆ’ğœ‡ğ‘¡ğ‘Ÿ
ğœ2
ğ‘¡ğ‘Ÿ,
ğ‘”ğ‘’(ğ‘¥,ğ‘¦)=exph ğ‘“(ğ‘¥+ğœ‚ğ‘’)
ğœ2ğ‘’âˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)
ğœ2
ğ‘¡ğ‘Ÿğ‘¦i
,
â„ğ‘’(ğ‘¥)=ğœğ‘¡ğ‘Ÿ
ğœğ‘’Â·ğ‘ƒğ‘’(ğ‘¥)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)
Â·exph(ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ)2
2ğœ2
ğ‘¡ğ‘Ÿâˆ’(ğ‘“(ğ‘¥+ğœ‚ğ‘’)+ğœ‡ğ‘’)2
2ğœ2ğ‘’i
.(4)
The detailed derivation is available in Appendix, where we addi-
tionally illustrate that the density ratios adhere to the formulation
described in Equation 3 when considering a broader assumption of
exponential distribution noise.
Let the space of all possible weighting functions for all environ-
ments inEğ‘ğ‘™ğ‘™beWâˆ—, i.e.,Wâˆ—={ğ‘¤ğ‘’(ğ‘¥,ğ‘¦):ğ‘’âˆˆEğ‘ğ‘™ğ‘™}. We note
that the density ratio ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)in Equation (3)can be expressed as
the product of three terms, corresponding to functions on ğ‘¦,ğ‘¥, and
(ğ‘¥,ğ‘¦)respectively.
Parametrized weighting function class. Our goal is to establish a
parametrized weighting function class derived from Equation (3).
At first glance, one might consider parametrizing all three terms
in Equation (3)and formulating the function class in the following
manner:
Wâ€²=
ğ‘¤(ğ‘¥,ğ‘¦)=exp(ğ´ğ‘¦2+ğµğ‘¦)ğ‘”(ğ‘¥,ğ‘¦;ğœ‰)â„(ğ‘¥;ğœ™):
ğ´,ğµâˆˆR,ğœ‰âˆˆÎ,ğœ™âˆˆÎ¦,Eğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¤(ğ‘‹,ğ‘Œ)]=1	
,(5)
<ğœŸ
MinDistanceğ‘ƒ!"ğ‘ƒ!"!ğ‘ƒ!"!ğ‘ƒ!"!Risk!"Risk!"!Risk!"!Risk!"!Risk!#Risk!#Risk!#!!(#,%)!!(#,%)
!!(#,%)Figure 1: The diagram of how our model works. At each op-
timization step, MARW first randomly samples weighting
functions from a specific space to approximate the excepted
risk in the potential test distributions using weighted empir-
ical risk. Then it minimizes the variance of weighted empir-
ical risks to learn a prediction model that performs stably
(having bounded test risk) in different distributions.
whereğ‘”(ğ‘¥,ğ‘¦;ğœ‰)andâ„(ğ‘¥;ğœ™)are parametrized functions with param-
etersğœ‰âˆˆÎandğœ™âˆˆÎ¦respectively. The constraint Eğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¤(ğ‘‹,ğ‘Œ)]=
1is the normalization requirement to guarantee that the weighting
function is feasible to model the probability density ratio of two
distributions. However, we argue that parametrizing the space of
ğ‘”ğ‘’(ğ‘¥,ğ‘¦)in Equation (3)as{ğ‘”(ğ‘¥,ğ‘¦;ğœ‰):ğœ‰âˆˆÎ}is inherently prob-
lematic for a couple of key reasons.
Firstly, the ground-truth weighting function space of ğ‘”ğ‘’(ğ‘¥,ğ‘¦)
in Equation (3)is intrinsically dependent on only two parameters,
ğœ‚ğ‘’andğœğ‘’, as all other parameters ğ‘“,ğœ‚ğ‘¡ğ‘Ÿ, andğœğ‘¡ğ‘Ÿremain consis-
tent across different environments. By contrast, the function space
ofğ‘”(ğ‘¥,ğ‘¦;ğœ‰)often spans a much larger domain, given that typi-
cally parametrized functions (e.g., linear functions or multi-layer
perceptrons) carry parameters with dimension at least ğ‘‘(i.e., the
dimension of ğ‘¥), which tends to be large in practical scenarios (e.g.,
images and texts). Secondly, the function space of ğ‘”ğ‘’(ğ‘¥,ğ‘¦)is heav-
ily influenced by the labeling function ğ‘“in Equation (1). Given
the usual lack of prior knowledge about ğ‘“, modeling the space for
ğ‘”ğ‘’(ğ‘¥,ğ‘¦)with{ğ‘”(ğ‘¥,ğ‘¦;ğœ‰):ğœ‰âˆˆÎ}is a significant challenge, espe-
cially considering the dimensional constraints that require the size
of the space Îto be small.
Considering these constraints, we propose an alternate weight-
ing function class by excluding the ğ‘”(ğ‘¥,ğ‘¦;ğœ‰)term from Equation
(5), i.e.,
W=
ğ‘¤(ğ‘¥,ğ‘¦)=exp(ğ´ğ‘¦2+ğµğ‘¦)â„(ğ‘¥;ğœ™):
ğ´,ğµâˆˆR,ğœ™âˆˆÎ¦,Eğ‘ƒğ‘¡ğ‘Ÿ[ğ‘¤(ğ‘‹,ğ‘Œ)]=1	
.(6)
It is noteworthy that while the â„ğ‘’(ğ‘¥)term in Equation (1)incor-
porates the ground-truth labeling function ğ‘“, the existence of the
covariate-shift term ğ‘ƒğ‘’(ğ‘¥)/ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)expands the function space for
â„ğ‘’(ğ‘¥)considerably in real-world scenarios, making it viable to
model it with a parametrized function space {â„(ğ‘¥;ğœ™):ğœ™âˆˆÎ¦}.
Furthermore, we prove that the exclusion of the ğ‘”ğ‘’(ğ‘¥,ğ‘¦)term has a
 
1052KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue He et al.
Algorithm 1 Out-of-Distribution Generalization via Model Agnos-
tic Random Weighting (MARW)
Input: the training dataset {(ğ‘¥ğ‘–,ğ‘¦ğ‘–)}ğ‘
ğ‘–=1, the number of sample
weightingğ¸, the randomness ğ‘ƒğœ™of weighting function â„(ğ‘¥;ğœ™),
and the randomness ğ‘ƒ(ğ´,ğµ)of weighting parameters (ğ‘,ğ‘).
Initialize the parameter set ğœƒof prediction model Ë†ğ‘“(ğ‘¥;ğœƒ).
repeat
Sample{ğœ™1,Â·Â·Â·,ğœ™ğ¸}âˆ¼ğ‘ƒğœ™,{(ğ‘1,ğ‘1),Â·Â·Â·,(ğ‘ğ¸,ğ‘ğ¸)}âˆ¼ğ‘ƒ(ğ´,ğµ).
ComputeL0=Ãğ‘
ğ‘–=1â„“(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğœƒ)
forğ‘’=1toğ¸do
ComputeLğ¸=Ãğ‘
ğ‘–=1exp(ğ‘ğ‘’ğ‘¦2
ğ‘–+ğ‘ğ‘’ğ‘¦ğ‘–)Â·â„(ğ‘¥ğ‘–;ğœ™ğ‘’)Â·â„“(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğœƒ)
end for
ComputeL=1/(ğ¸+1)Ãğ¸
ğ‘’=0Lğ¸+ğœ†Â·ğ‘‰ğ‘ğ‘Ÿ({L 0,Â·Â·Â·,Lğ¸}).
Optimizeğœƒâ†ğœƒâˆ’ğœ‚Â·â–½L .
until convergence
return: the prediction model Ë†ğ‘“(ğ‘¥;ğœƒ).
moderate impact on the weighting function space to be acceptable,
as detailed in Section 4.1.
3.2 OOD Optimization with Random Sample
Weights
Building upon the preceding subsection, we propose the use of
randomly sampled weighting functions from the class Wto simu-
late novel environments, and we incorporate a variance regularizer
to constrain the model performance within these synthetic envi-
ronments. The generation of environments is independent of the
model optimization, making MARW both simple and stable.
In detail, we first design distributions ğ‘ƒğ´,ğ‘ƒğµ,ğ‘ƒğœ™for all three
parameters ğ´,ğµ,ğœ™ in the weighting function class W. At each
training iteration, we sample ğ¸environments ğ‘’1,ğ‘’2,...,ğ‘’ğ¸, where
each environment ğ‘’ğ‘—is characterized by the weighting function
ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)=exp(ğ´ğ‘—ğ‘¦2+ğµğ‘—ğ‘¦)â„(ğ‘¥;ğœ™ğ‘—)withğ´ğ‘—âˆ¼ğ‘ƒğ´,ğµğ‘—âˆ¼
ğ‘ƒğµ, andğœ™ğ‘—âˆ¼ğ‘ƒğœ™. Employing these weighting functions, we generate
ğ¸environments and apply variance regularization to minimize the
variance of the weighted empirical risks, thereby mitigating risk
ascent in non-i.i.d. test distributions. This strategy is illustrated
to provide more stable and effective optimization in practice [ 25].
Formally, we optimize ğœƒMARWas the value to minimize following
formula:
1
ğ¸+1ğ¸âˆ‘ï¸
ğ‘—=0Lğ‘—(ğœƒ)+ğœ†Â·Var(L0(ğœƒ),L1(ğœƒ),...,Lğ¸(ğœƒ)),(7)
whereL0(ğœƒ)=Ãğ‘
ğ‘–=1â„“(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğœƒ), andâˆ€ğ‘—âˆˆ{1,2,...,ğ¸},
Lğ‘—(ğœƒ)=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘¤ ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—â„“(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğœƒ). (8)
In essence,L0(ğœƒ)corresponds to the empirical loss in the training
distribution andLğ‘—(ğœƒ)represents the weighted empirical loss in
the distribution constituted by ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—). Here,ğœ†is a hyper-
parameter that controls the intensity of the variance penalty.
The pseudo-code of MARW can be found in Algorithm 1. In
practice, we ensure the sum of weights equals 1by normalizing
each weight ğ‘¤(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)through division by the total sumÃğ‘›
ğ‘–=1ğ‘¤(ğ‘¥ğ‘–,ğ‘¦ğ‘–;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—). When adopting our models for deep learn-
ing tasks, we can utilize the representation of the data instead of the
original input sample to compute the sample weights, that benefits
from the dense and separable representation space [20].
4 Theoretical Analysis
In this section, we first examine the distance between the ground-
truth weighting function for a specific environment ğ‘’and the
weighting functions within our proposed class W, considering
both the best-case and worst-case scenarios (Section 4.1). Leverag-
ing these results, we investigate the effects of our method on the
OOD performance in Section 4.2. It is worth noting that when we
consider scenarios where the training and test distributions have
identical noise variance (i.e., ğœ2ğ‘’=ğœ2
ğ‘¡ğ‘Ÿ), we can obtain a tighter
bound (Section 4.3).
Further assumptions. If the test distribution is entirely unknown,
the prediction tasks become intractable [ 30]. Therefore, we assume
that the test distribution lies within the vicinity of the training
distribution. More specifically, we consider test sets to be in the
following set controlled by ğ‘Ÿandğ›¿,âˆ€ğ‘Ÿâ‰¥1,ğ›¿â‰¥0:
E(ğ‘Ÿ,ğ›¿)=
ğ‘’âˆˆEğ‘ğ‘™ğ‘™:âˆ€ğ‘¥âˆˆX,|ğœ‚ğ‘’âˆ’ğœ‚ğ‘¡ğ‘Ÿ|â‰¤ğ›¿,
1
ğ‘Ÿâ‰¤ğœğ‘’
ğœğ‘¡ğ‘Ÿ,ğœ‡ğ‘’
ğœ‡ğ‘¡ğ‘Ÿ,ğ‘ƒğ‘’(ğ‘¥)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)â‰¤ğ‘Ÿ	
.(9)
The parameters ğ‘Ÿandğ›¿regulate the potential range of the distri-
bution. As these values increase, the probability of the test distri-
bution being significantly divergent from the training distribution
also rises. Particularly, when considering E(1,0), it solely encom-
passes the training distribution, thereby reducing the task to the i.i.d.
scenario. Furthermore, we give an assumption on our generating
function to bound the range and smoothness of the ground-truth
labeling function.
Assumption 4.1.|ğ‘“(ğ‘¥)|â‰¤ğ‘€,ğ‘¥âˆˆXand|ğ‘“(ğ‘¥1)âˆ’ğ‘“(ğ‘¥2)|â‰¤ğ¿|ğ‘¥1âˆ’
ğ‘¥2|,âˆ€ğ‘¥1,ğ‘¥2âˆˆX.
4.1 Justification of the Chosen Weighting
Function Class
Based on the weighting function class in Equation (6), we addition-
ally add constraints on the range of the parameters as follows.
W(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™)=
exp(ğ´ğ‘¦2+ğµğ‘¦)â„(ğ‘¥;ğœ™):
|ğ´|â‰¤ğœ…ğ´,|ğµ|â‰¤ğœ…ğµ,âˆ€ğ‘¥âˆˆX,|lnâ„(ğ‘¥;ğœ™)|â‰¤ğœ…ğœ™	
.(10)
For any environment ğ‘’inE(ğ‘Ÿ,ğ›¿), we could develop upper bounds
on the distance between the ground-truth weighting function and
the worst (Theorem 4.1) and best (Theorem 4.2) weighting function
inW(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™).
 
1053Model-Agnostic Random Weighting for Out-of-Distribution Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Theorem 4.1. Under Assumption 4.1, we have âˆ€ğ‘’âˆˆE(ğ‘Ÿ,ğ›¿),âˆ€ğ‘¦âˆˆY,
sup
ğ‘¤âˆˆW(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™)sup
ğ‘¥âˆˆX|lnğ‘¤(ğ‘¥,ğ‘¦)âˆ’lnğ‘¤ğ‘’(ğ‘¥,ğ‘¦)|
â‰¤
ğœ…ğ´+ğ‘Ÿ2âˆ’1
2ğœ2
ğ‘¡ğ‘Ÿ
ğ‘¦2+
ğœ…ğµ+1
ğœ2
ğ‘¡ğ‘Ÿ((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿ2ğ¿ğ›¿)
+|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ3âˆ’1)
ğœ2
ğ‘¡ğ‘Ÿ
|ğ‘¦|+ğœ…ğœ™+2 lnğ‘Ÿ
+(ğ‘Ÿ+1)ğ‘€+(ğ‘Ÿ2+1)|ğœ‡ğ‘¡ğ‘Ÿ|
2ğœ2
ğ‘¡ğ‘Ÿ
(ğ‘Ÿâˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿+|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ2âˆ’1)
.(11)
Remark. This theorem provides an upper bound on the worst-case
distance between the ground-truth weighting function ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)
and any function within W(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™). Since the weight function
is complex, we choose to fix ğ‘¦and measure the maximum distance
over allğ‘¥to quantify the gap between weights. In addition, we note
that whenğ‘Ÿ=1andğ›¿=0, the scenario reduces to i.i.d. tasks. In
such instances, the right-hand side retains only the worst sample
error,ğœ…ğ´ğ‘¦2+ğœ…ğµ|ğ‘¦|+ğœ…ğœ™.
Furthermore, if we have additional information about the range
of the test environment characterized by ğ‘Ÿ,ğ›¿, we can obtain the
best-case distance as follows.
Theorem 4.2. Under Assumption 4.1, by setting
ğœ…ğ´â‰¥ğ‘Ÿ2âˆ’1
2ğœ2
ğ‘¡ğ‘Ÿ,ğœ…ğµâ‰¥|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ3âˆ’1)
ğœ2
ğ‘¡ğ‘Ÿ,
ğœ…ğœ™â‰¥2 lnğ‘Ÿ+(ğ‘Ÿ+1)ğ‘€+(ğ‘Ÿ2+1)|ğœ‡ğ‘¡ğ‘Ÿ|
2ğœ2
ğ‘¡ğ‘Ÿ
Â·[(ğ‘Ÿâˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿+|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ2âˆ’1)],(12)
we haveâˆ€ğ‘’âˆˆE(ğ‘Ÿ,ğ›¿),âˆ€ğ‘¦âˆˆY,
inf
ğ‘¤âˆˆW(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™)sup
ğ‘¥âˆˆX|lnğ‘¤(ğ‘¥,ğ‘¦)âˆ’lnğ‘¤ğ‘’(ğ‘¥,ğ‘¦)|
â‰¤|ğ‘¦|1
ğœ2
ğ‘¡ğ‘Ÿ
(ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿ2ğ¿ğ›¿
.(13)
Remark. Similar to the remark of Theorem 4.1, when ğ‘Ÿ=1and
ğ›¿=0, the scenario reduces to i.i.d. tasks. In such instances, the
right-hand side of Equation (13)becomes zero since W(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™)
contains the function ğ‘¤(ğ‘¥,ğ‘¦)â‰¡1. We note that there can be nu-
merous weighting functions that satisfy Equation (13), given that
it merely establishes an upper limit for the distance.
Theorems 4.1 and 4.2 highlight a trade-off in choosing small
or large values for the parameters ğœ…ğ´,ğœ…ğµ, andğœ…ğœ™. Small values
hinder accurate ground-truth weighting function approximation,
while large values increase the worst-case distance, as shown in
Theorem 4.1.
4.2 Analysis of Model Performance on Test
Distribution
In this subsection, we further analyze how randomly generated
weighting functions and our optimization target in Equation (7)
can help improve performances on unknown test distributions.Consider a specific test environment ğ‘’ğ‘¡ğ‘’âˆˆEğ‘ğ‘™ğ‘™. The probability
density ratio between the test environment and training environ-
ment,ğ‘¤ğ‘’ğ‘¡ğ‘’(ğ‘¥,ğ‘¦), can be obtained using Equation (3). We show that if
we can randomly generate a weighting function ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)
that is sufficiently close to the true function ğ‘¤ğ‘’ğ‘¡ğ‘’(ğ‘¥,ğ‘¦), then the test
performance can be bounded by the weighted training performance
with respect to the function ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—).
Theorem 4.3. Under Assumption 4.1, if there exists sampled envi-
ronmentğ‘—such that the weighting function ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)sat-
isfies Equation (13) (i.e.,âˆ€ğ‘¦âˆˆY,supğ‘¥âˆˆX|lnğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)âˆ’
lnğ‘¤ğ‘’ğ‘¡ğ‘’(ğ‘¥,ğ‘¦)|â‰¤|ğ‘¦|((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿ2ğ¿ğ›¿)/ğœ2
ğ‘¡ğ‘Ÿ), then for any ğ›¼>0, there
existğ¶ğ›¼=exp[((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿ2ğ¿ğ›¿)(ğ‘€+|ğœ‡ğ‘¡ğ‘Ÿ|+ğ‘§1âˆ’ğ‘(ğ›¼,ğ‘Ÿ,ğ›¿)/2ğœğ‘¡ğ‘Ÿ)/ğœ2
ğ‘¡ğ‘Ÿ]
such thatâˆ€ğœƒâˆˆÎ˜,
Eğ‘¡ğ‘’hğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
â‰¤ğ¶ğ›¼Eğ‘¡ğ‘Ÿh
ğ‘¤(ğ‘‹,ğ‘Œ;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
+ğ›¼,(14)
whereğ‘§1âˆ’ğ‘(ğ›¼,ğ‘Ÿ,ğ›¿)/2represents 1âˆ’ğ‘(ğ›¼,ğ‘Ÿ,ğ›¿)/2quantile of Gaussian
distribution and ğ‘(ğ›¼,ğ‘Ÿ,ğ›¿)is increasing about ğ›¼and decreasing about
ğ‘Ÿ,ğ›¿.
Remark. Note thatğ¶ğ›¼â‰¥1. Whenğ‘Ÿ=1,ğ›¿=0,ğ¶ğ›¼=1, the task
degenerates to i.i.d. tasks. If potential test distribution range is small
(i.e.,ğ‘Ÿ,ğ›¿is small), the coefficient of the training risk ğ¶ğ›¼is small, and
then the performance guarantee on test distribution is sufficient.
Although we do not have prior knowledge about the true func-
tionğ‘¤ğ‘’ğ‘¡ğ‘’(ğ‘¥,ğ‘¦), there hopefully exists a function ğ‘¤ğ‘—(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)âˆˆ
W(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™)that satisfies Equation (13)to approximate the op-
timal weight relative to the test distribution well. In addition, as
demonstrated by [ 25], the optimization target in Equation (7)can ef-
fectively guarantee the performances on each environment ğ‘’1,ğ‘’2,...,ğ‘’ğ¸,
i.e., make Eğ‘¡ğ‘Ÿ[ğ‘¤(ğ‘‹,ğ‘Œ;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|]small. Hence, Theo-
rem 4.3 can demonstrate that our method can effectively optimize
the performances in unknown test distribution. Moreover, under the
same-variance-noise assumption, by employing techniques such
as covering numbers, we can establish that when a sufficient num-
ber of weight functions are sampled, there is a high probability of
ensuring the modelâ€™s performance in test domain.
4.3 Case Analysis: when all the distributions
share the same noise variance
In addition to the general situation, a sound assumption that all
the potential data environments share the same noise variance
ğœğ‘’=ğœis also widely considered. In this section, we discuss the
situation under the equal variance assumption, where we can see
the favorable property of our method in terms of out-of-distribution
generalization.
All Possible Weighting Functions. Here we formulate the ground-
truth weighting function class for the same noise variance (SNV)
environments inEğ‘ ğ‘›ğ‘£. Then the probability density ratio between
the distribution ğ‘ƒğ‘’(ğ‘¥,ğ‘¦)of any environment ğ‘’âˆˆ Eğ‘ ğ‘›ğ‘£and the
training distribution ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)can be formulated as
âˆ€ğ‘¥âˆˆX,ğ‘¦âˆˆY, ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)â‰œğ‘ƒğ‘’(ğ‘¥,ğ‘¦)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)=exp(ğµğ‘’ğ‘¦)â„ğ‘’(ğ‘¥)ğ‘”ğ‘’(ğ‘¥,ğ‘¦),
 
1054KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue He et al.
Table 1: Empirical results in US-Wide PUMS Data in a sub-population generalization setting. We report the precision accuracy
on test data (higher value is better). The bold and underline denote the best and the second best results, respectively. MARW
shows a consistent advantage in all tasks compared to other baselines.
Metho
dA
CSPublicCoverage A
CSEmployment A
CSTravelTime A
CSIncome A
CSMobility
Mean
Worst Mean
Worst Mean
Worst Mean
Worst Mean
Worst
ERM 75.25%
64.00% 77.25%
74.90% 60.03%
50.20% 78.24%
73.10% 70.88%
58.62%
DRO 75.54%
62.68% 77.13%
75.00% 60.14%
50.62% 77.80%
72.70% 71.05%
58.62%
RWY 64.97%
47.20% 77.16%
74.60% 52.33%
48.40% 77.21% 74.50% 55.70%
41.10%
Focal 75.19%
63.50% 77.16%
74.60% 60.01%
50.40% 78.09%
73.00% 70.90%
58.62%
CVaR 75.44%
62.90% 77.51%
75.60% 60.90% 52.90% 78.07%
73.03% 71.77%
62.07%
LfF 74.27%
61.90% 73.24%
69.40% 59.94%
51.70% 77.42%
72.40% 70.99%
58.62%
JTT 74.53%
64.06% 77.34%
75.40% 56.71%
51.24% 78.14%
73.30% 69.00%
62.07%
MARW 75.85% 64.50% 77.96% 76.14% 60.22%
51.52% 78.11%
73.94% 71.53% 63.45%
Table 2: Empirical results in CelebA and Civilcomments
dataset. We report the precision accuracy on test data. MARW
and JTT show the superior performances in these two
datasets.
Metho
dCeleb
A (Image Dataset) Civilcomments
(Text Dataset)
Mean
Worst Mean
Worst
ERM 95.60%
47.20% 92.60%
57.40%
DRO 94.68%
53.89% 92.44%
55.08%
RWY 93.63%
72.23% 90.39%
68.99%
Focal 95.45%
49.45% 92.39%
55.23%
CVaR 94.72%
56.11% 92.22%
55.00%
LfF 85.10%
77.20% 92.50%
58.80%
JTT 88.00% 81.10% 91.10%
69.30%
MARW 92.47%
78.33% 91.11% 70.87%
where
ğµğ‘’=ğœ‡ğ‘’âˆ’ğœ‡ğ‘¡ğ‘Ÿ
ğœ2, ğ‘”ğ‘’(ğ‘¥,ğ‘¦)=expğ‘“(ğ‘¥+ğœ‚ğ‘’)âˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)
ğœ2
ğ‘¦
,
â„ğ‘’(ğ‘¥)=Â·ğ‘ƒğ‘’(ğ‘¥)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)Â·exp(ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ)2âˆ’(ğ‘“(ğ‘¥+ğœ‚ğ‘’)+ğœ‡ğ‘’)2
2ğœ2
.
Then we introduce the covering number to measure the space
W(0,ğœ…ğµ,ğœ…ğœ™)size.
Distance on Weight Function Space. We can define a distance on
W(0,ğœ…ğµ,ğœ…ğœ™), for anyğ‘¤1,ğ‘¤2âˆˆW( 0,ğœ…ğµ,ğœ…ğœ™), we can define
ğ‘‘Î©(ğ‘¤1,ğ‘¤2)=sup
ğ‘¥sup
|ğ‘¦|â‰¥Î©|lnğ‘¤1(ğ‘¥,ğ‘¦)âˆ’lnğ‘¤2(ğ‘¥,ğ‘¦)|
|ğ‘¦|,âˆ€Î©>0.
Following proposition guarantees the definition is reasonable.
Proposition 4.4. ğ‘‘Î©(Â·,Â·)is a well-defined distance, and W(0,ğœ…ğµ,ğœ…ğœ™)
is a bounded set based on ğ‘‘Î©.
Then we introduce the covering number to measure the space
W(0,ğœ…ğµ,ğœ…ğœ™)size.
Covering Number [4]. We denoteN(W( 0,ğœ…ğµ,ğœ…ğœ™),ğœ‰,ğ‘‘Î©)as the
minimal number of ğœ‰-radiusğ‘‘Î©-ball to cover the W(0,ğœ…ğµ,ğœ…ğœ™).When there are no confusing notations, we use Nto represent
N(W( 0,ğœ…ğµ,ğœ…ğœ™),ğœ‰,ğ‘‘Î©).
Assumption 4.2. Our sampling methods are uniform distribution
on theW(0,ğœ…ğµ,ğœ…ğœ™)based onğ‘‘Î©.
Remark. That uniform distribution on the W(0,ğœ…ğµ,ğœ…ğœ™)based on
ğ‘‘Î©means for any ball ğµ(ğ‘‚1,ğ‘…,ğ‘‘Î©),ğµ(ğ‘‚2,ğ‘…,ğ‘‘Î©)âŠ‚W( 0,ğœ…ğµ,ğœ…ğœ™)
based onğ‘‘Î©, we haveğ‘ƒ(ğµ(ğ‘‚1,ğ‘…,ğ‘‘Î©))=ğ‘ƒ(ğµ(ğ‘‚2,ğ‘…,ğ‘‘Î©)). To satisfy
Assumption 4.2, we can uniformly sample ğµâˆˆ [âˆ’ğœ…ğµ,ğœ…ğµ], and
chooseğœ™âˆˆğ‘ƒğœ™, such that lnâ„(ğ‘¥;ğœ™) âˆˆ [âˆ’ğœ…ğœ™,ğœ…ğœ™]. For example,
â„(ğ‘¥,ğœ™)=ğœ™andlnğœ™uniformly sampled from [âˆ’ğœ…ğœ™,ğœ…ğœ™].
Theorem 4.5. Under Assumption 4.2, if we independently sample ğ‘†
weight functions and our sampling method satisfies Assumption 4.2,
and
ğœ…ğµâ‰¥|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿâˆ’1)
ğœ2,ğœ…ğœ™â‰¥lnğ‘Ÿ+2ğ‘€+(ğ‘Ÿ+1)|ğœ‡ğ‘¡ğ‘Ÿ|
2ğœ2[ğ¿ğ›¿+|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿâˆ’1)],
then with probability 1âˆ’(1âˆ’N(W( 0,ğœ…ğµ,ğœ…ğœ™),ğœ‰,ğ‘‘Î©)âˆ’1)ğ‘†, there
exists a weight function ğ‘¤(ğ‘¥,ğ‘¦)âˆˆW( 0,ğœ…ğµ,ğœ…ğœ™), such that for any
ğ›¼>0, there exist ğ¶ğ›¼=exp[(2ğœ‰+ğ¿ğ›¿/ğœ2)(ğ‘€+|ğœ‡ğ‘¡ğ‘Ÿ|+ğ‘§1âˆ’ğ‘(ğ›¼)/2ğœ)]+
exp(6ğœ‰ğ¾+ğ¿ğ›¿/ğœ2)such that
âˆ€ğœƒâˆˆÎ˜,Eğ‘¡ğ‘’hğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
â‰¤ğ¶ğ›¼Eğ‘¡ğ‘Ÿh
ğ‘¤(ğ‘‹,ğ‘Œ;ğµ,ğœ™)ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
+ğ›¼,
whereğ‘§1âˆ’ğ‘(ğ›¼)/2represents 1âˆ’ğ‘(ğ›¼)/2quantile of Gaussian distribu-
tion andğ‘(ğ›¼)is increasing about ğ›¼.
Remark. Compared to Theorem 4.3, Theorem 4.5 states a more clear
version in the description for explaining the probability of taking
enough good weight. Actually, there is no need for our weight
to take the projection of the ğ‘¤ğ‘’(ğ‘¥,ğ‘¦), here, we extend the limits
to a broad range, and the range is controlled by ğœ‰. The larger ğœ‰
implies the larger range. Then it results in the smaller the covering
number and exists the weighting function with higher probability.
But the performance guarantee on test distribution turns weaker.
The difficulty of extending this theorem to the general situation is
on the choice of ğ‘‘Î©, since in a general situation, W(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™)is
unbounded set based on defined ğ‘‘Î©here. Choosing ğ‘‘Î©(ğ‘¤1,ğ‘¤2)=
sup|ğ‘¦|â‰¥ğ¾supğ‘¥âˆˆX|lnğ‘¤1âˆ’lnğ‘¤2|/|ğ‘¦|2can similarly gain the similar
but complicated result, (since quadratic term existence).
 
1055Model-Agnostic Random Weighting for Out-of-Distribution Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Empirical results in US-Wide PUMS Data in a domain generalization setting. We report the precision accuracy on test
data. The bold and underline denote the best and the second best results, respectively. MARW shows a consistent advantage.
Metho
dA
CSPublicCoverage A
CSEmployment A
CSTravelTime A
CSIncome A
CSMobility
Mean
Worst Mean
Worst Mean
Worst Mean
Worst Mean
Worst
ERM 64.55%
48.80% 76.25%
71.60% 56.31%
39.10% 76.83%
70.70% 62.44%
46.93%
DRO 65.04% 48.74% 76.41%
71.64% 56.09%
41.50% 76.98%
70.84% 62.08%
48.04%
RWY 61.54%
38.82% 76.19%
71.40% 53.92%
41.20% 75.79%
70.60% 60.56%
50.00%
Fo
cal 64.36%
48.30% 76.23%
71.20% 56.19%
39.10% 77.17% 70.80% 62.37%
47.49%
CVaR 64.98%
49.33% 76.52% 71.83% 57.88% 41.70% 77.28% 70.70% 62.53%
48.23%
LfF 63.90%
47.20% 75.09%
71.40% 55.87%
39.50% 76.38%
70.52% 62.68% 48.04%
JT
T 63.93%
47.28% 76.10%
71.00% 52.88%
40.30% 76.91%
70.50% 62.47%
48.78%
MARW 67.36%
55.63% 76.70%
73.18% 58.25%
43.76% 76.83% 71.76% 63.16%
52.28%
5 Related Works
In this section, we review and compare the works related to our
proposal. According to the difference in assumptions of OOD gen-
eralization from single training environment, the methods fall into
three main categories we discuss below.
5.1 Heterogeneity Identification
This type of methods [ 7,32] put forward to find the environments
with heterogeneity a mixture of multi-source data. Then they learn
an invariant model across the heterogeneous environments. How-
ever, the invariant constraints [ 3] on model parameters limits their
deployment in over-parameterized models. In constrast, MARW is
a model-agnostic method.
5.2 Distributionary Robust Optimization
Distributionary Robust Optimization (DRO) supposes that all the
test distributions come from an uncertainty set of distributions Q,
each of which has a limited distance to the training distribution
[26,40]. Thereby, it aims to minimize the worst excepted risk in Q
to improve the OOD performance. To characterize the distribution
setQ, Duchi and Namkoong [12], Michel et al . [35] consider the
ğœ™-divergence metric as a distance measure, Sinha et al . [43] , Staib
and Jegelka [44] concern about the Wasserstein ball or MMD ball
around the training distribution, Delage and Ye [10] utilizes the
moments of distribution to constrain the choice of uncertainty set,
and etc. However, these methods often suffer from hard implemen-
tation of distributed distance calculation, and are easily affected
by the noise of sample. And the uncertainty set is usually overpes-
simistic, leading to the prediction model stuck in the unreasonable
distributions. In contrast, MARW directly approximates the test dis-
tributions utilizing an alternative shifted training distributions, that
promotes the generalization ability upon much sound distributions.
5.3 Debias by Simple Weighting
Recently, some model-agnostic methods are proposed to solve dis-
tribution shift by sample weighting, typically including:
â€¢RWY [ 22] balances the ratio of positive and negative samples
by adding weights to minority class.
â€¢Focal loss [29] up-weights the harder samples based on the
prediction confidence during training.â€¢CVaR [ 47] minimizes the empirical risks on the most at-risk
portion of training samples.
â€¢LfF [ 37] optimizes a pair of models synchronously, making
the training of unbiased model focus on the samples go
against the prejudice of biased model.
â€¢JTT [ 31] first trains a prediction model using ERM, then up-
weights the samples that the first model misclassifies when
training the second unbiased model.
Despite the competitive results in special cases, these methods
perform unstable in different OOD scenarios. In constrast, MARW
can address more general OOD problems with a theoretical support.
6 Experiments
To evaluate the effectiveness of our proposed method, we conduct
extensive experiments in two popular out-of-distribution settings,
namely sub-population generalization and domain generalization,
on various types of real datasets.
6.1 Baselines
In this paper, we compare the generalization performance of MARW
with other model-agnostic methods that can deal with the distri-
bution shift between the training and test data, including ERM,
DRO, RWY, Focal loss, CVaR, LfFandJTT. For a fair compari-
son, we adopt the same backbone model for all the methods in each
experiment.
6.2 Datasets
In experiments, we utilize various types of real datasets for the full
validation of methods, including:
â€¢US-Wide ACS PUMS Data [11]: This large tabular dataset2
based on US Census sources contains individual records.
Each sample provides the profile features, outcome label, as
well as an extra sub-group label. Five prediction tasks are
pre-defined in this dataset, each of which aims to determine
a personal event outcome. For example, the outcome of AC-
SPublicCoverage task is whether an individual is covered by
public health insurance.
2https://github.com/zykls/folktables
 
1056KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue He et al.
Table 4: Empirical results in VLCS and OGB dataset. 1) In VLCS, we report the precision accuracy on test data. Ours achieves
the best average performance. 2) In OGB, we report the ROC metric on test data (higher value is better) in classification tasks
(molhiv, moltoxcast, molbace), and RMSE metric (lower value is better) in regression tasks (molfreesolv, mollipo). Ours shows
its advantage in both types of tasks.
Metho
dVLCS
(Image Dataset) OGB
(Graph Dataset)
P
ASCAL Lab
elMe Calte
ch Sun A
verage molhiv molto
xcast molbace molfr
eesolv mollip
o
ERM 74.6% 64.3% 97.7% 73.4% 77.5% 76.06% 63.54% 79.15% 2.640 0.797
DRO 75.4% 64.6% 99.1% 70.8% 77.5% 77.56% 63.41% 79.86% 2.709 0.840
RW
Y 75.2% 61.7% 97.9% 70.3% 76.3% 76.80% 63.57% 80.72% - -
Fo
cal 77.0% 63.6% 98.6% 68.2% 76.9% 76.54% 63.77% 80.88% - -
CV
aR 77.0% 63.4% 98.6% 69.3% 77.1% 76.78% 63.35% 81.01% 2.663 0.795
LfF 43.8% 46.5% 61.6% 38.4% 47.6% 72.67% 60.59% 78.38% - -
JT
T 75.5% 62.4% 98.6% 69.1% 76.4% 76.93% 63.35% 80.85% - -
MARW 76.7% 63.5% 99.0% 73.1% 78.1% 77.16% 64.23% 81.44% 2.529 0.785
â€¢CelebA [33]: This dataset3contains the face images of celebri-
ties with attribute annotations. We consider the task of pre-
dicting if hair color of a celebrity is blond [ 41], which is
spuriously related to gender attribute.
â€¢CivilComments [5]: This dataset4collects the user-generated
public comments online. We consider the task of predicting
whether a comment is toxic [ 23], which is spuriously related
to 8 demographic identities.
â€¢VLCS [13]: This dataset5gathers the images from 4 separate
datasets (PASCAL VOC, LabelMe, Caltech, and Sun). Distri-
bution shift comes from the heterogeneity between these
data sources.
â€¢OGB [21]: The Open Graph Benchmark (OGB)6is a collec-
tion of realistic graph datasets with pre-defined tasks. In our
experiments, we consider 5 graph-level tasks, which aim to
predict some properties of molecules. The molecules are split
based on their scaffolds, causing the data distribution shift.
6.3 Sub-population Generalization
6.3.1 Experimental Setting. In this section, we first validate the
methods in the sub-population generalization setting, where the
training data composes of a group of sub-populations, but the por-
tion of each sub-population in training data is different. As a result,
the prediction model is prone to overlook the minor groups, while
falsely learns the spurious correlations in the major groups. In test
stage, the model is evaluated in each sub-population separately
to verify if it is trapped by the imbalance training data. Referring
to [31], we take the worst-case result among all the sub-populations
as the main evaluation metric.
In this setting, we conduct experiments in 3 datasets. For all 5
tasks in US-Wide ACS PUMS Data, we partition out 8 sub-populations
according to the hybird of group and state, and collect training sam-
ples in different data size from each sub-population. For CelebA and
CivilComments dataset, we keep the same configuration with [ 31].
3https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8?resourcekey=0-
5BR16BdXnb8hVj6CNHKzLg&usp=sharing
4https://github.com/p-lambda/wilds/
5http://www.mediafire.com/file/7yv132lgn1v267r/vlcs.tar.gz/file
6https://github.com/snap-stanford/ogb6.3.2 Experimental Result. From the results in ACS PUMS Data in
Table 1, we see that:
(1)ERM is sensitive to distribution shift. Its performance sharply
descends in the minor sub-population due to the substantial
imbalance in training data.
(2)DRO achieves the similar performances with ERM. The sam-
ple noise is easy to interfere the correct finding of minor
sub-populations in this dataset.
(3)RWY, Focal loss, CVaR and LfF perform unstably in differ-
ent tasks relying on if their assumptions of model failureâ€™s
reasons hold in the corresponding task.
(4)JTT presents competitive results against sub-population shift.
(5)Compared to baselines, MARW shows superior performance
in all tasks. It reaches the best two results of worst accu-
racy in most cases, that verifies its effectiveness for sub-
population generalization.
The results in CelebA and Civilcomments dataset are reported in
Table 2. Besides ERM, each of the methods can promote the OOD
performance from different perspective. Among them, MARW and
JTT gain the best two results in both two datasets, suggesting their
advances in the sub-population generalization problem.
6.4 Domain Generalization
6.4.1 Experimental Setting. In this section, we validate the meth-
ods in the domain generalization setting. Different from the sub-
population generalization, domain generalization considers that
the test samples come from distinct data environments that are
unseen during training, instead of a sub-population of training data.
The core of domain generalization is whether the model can learn
a prediction function from the limited training environment that
can generalize to more data environments. Hence, we also concern
about the modelâ€™s average performance in all test environments, in
addition to the worst-case result.
In this setting, we also conduct experiments in 3 datasets. For
US-Wide ACS PUMS Data, we simulate different environments
according to the group label and state, select training samples of
group 1 from partial states, and test samples of other groups from
whole states. For VLCS dataset, we choose one data domain for test
 
1057Model-Agnostic Random Weighting for Out-of-Distribution Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) The curve from the number of weighting functions to
modelâ€™s OOD performance.
ï¼ˆÃ—10 ï¼‰5(b) The curve from the intensity of variance penalty to
modelâ€™s OOD performance.
Figure 2: Empirical model analysis unfolded in ACSMobility task. Green and red lines denote mean accuracy and the worst
accuracy, respectively. We see that: 1) as more shifted training distributions appear, MARWâ€™s generalization ability is gradually
enhanced until convergence (a); 2) although the reduction of risk variance benefits the OOD performance, it will hurt model
optimization if the intensity is too large (b).
and the others for training in turn. For OGB dataset, the training
and test data is divided depending on the scaffolds [21].
6.4.2 Experimental Result. From the results in ACS PUMS Data in
Table 3, we observe that:
(1)The unseen test environments in domain generalization usually
imply the heightened distribution shifts, leading to a larger drop
in the performance of traditional ERM.
(2)The performance of DRO is not stable, depending on the mag-
nitude of distribution shift. It performs better upon small shift.
(3)RWY, Focal loss, LfF, and JTT can improve the OOD perfor-
mance in some cases, but still suffer from the unstable problem.
(4) CVaR presents competitive results in this setting.
(5)MARW achieves the best results on both average and worst
accuracy in almost all the tasks. And its advantage is more
significant in domain generalization than that in sub-population
generalization, demonstrating the superiority of our method to
address distribution shift.
Table 4 shows the results in VLCS dataset. Although ERM has
shown farily strong performance [ 18], MARW can still outperforms
the baselines with a higher average performance. The results in
OGB dataset are reported in Table 4 as well. This dataset contains
both classification tasks and regression tasks. Because RWY, Fo-
cal loss, LfF, and JTT are designed for classification, we compare
MARW with DRO, CVaR and ERM in regression tasks (molfreesolv
and mollipo). Compared to the baselines, MARW works well in
both types of tasks thanks to its way of shifting distribution that is
free to the prediction model.
6.5 Model Analysis
Further, we investigate how our method works empirically by the
experiments in US-Wide PUMS Data.
6.5.1 Impact of the Number of Weighting Functions. Firstly, we
change the number of random weighting at each optimization step,
and observe the performances of MARW. The results (Figure 2(a))
points that as the number increases, the modelâ€™s generalizationability rises up owing to more potential distributions being ac-
cessed. However, the performance convergences after the number
exceeds a threshold (5 in Figure 2(a)), that says the finite weighting
functions can cover the representative test distributions for OOD
generalization.
6.5.2 Role of Variance Penalty. Secondly, we change the coeffi-
cient of variance penalty to see how it drives the prediction model
towards better generalization. From Figure 2(b), we find that the
stronger variance penalty will force the model to be more stable
in different distributions, thus eliminating the biased correlations.
However, the high-intensity penalty would affect the learning abil-
ity of the model, because the variance of noise term is distinct in
different environments.
7 Conclusion
In this paper, we present a novel model-agnostic approach, the
Model-Agnostic Random Weighting (MARW) method, designed
for out-of-distribution (OOD) generalization from single training
environment. MARW leverages sample-based weighting within a
designated function space to stochastically generate shifted training
environments. This method enables the subsequent training of a
prediction model with emphasis on robust weighted empirical risks.
We provide theoretical evidence supporting the approximation of
test distributions by the shifted training data, ensuring the modelâ€™s
guaranteed OOD performance in uncertain test distributions. Fi-
nally, we conduct comprehensive experiments to demonstrate the
advanced capabilities of our model, particularly its effectiveness in
various OOD generalization tasks.
Acknowledgments
This work was supported in part by National Natural Science Foun-
dation of China (No. 62141607), China National Postdoctoral Pro-
gram for Innovative Talents (BX20230195), Beijing Natural Science
Foundation (QY23081). All opinions in this paper are those of the
authors and do not necessarily reflect the views of the funding
agencies. We would like to all the anonymous reviewers for the
helpful feedback.
 
1058KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue He et al.
References
[1]Alekh Agarwal and Tong Zhang. 2022. Minimax regret optimization for robust
machine learning under distribution shift. In Conference on Learning Theory.
PMLR, 2704â€“2729.
[2]Kartik Ahuja, Jun Wang, Amit Dhurandhar, Karthikeyan Shanmugam, and Kush R
Varshney. 2020. Empirical or invariant risk minimization? a sample complexity
perspective. arXiv preprint arXiv:2010.16412 (2020).
[3]Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[4]Ben Babcock and Adam Van Tuyl. 2011. Revisiting the spreading and covering
numbers. arXiv preprint arXiv:1109.5847 (2011).
[5]Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasser-
man. 2019. Nuanced metrics for measuring unintended bias with real data for text
classification. In Companion proceedings of the 2019 world wide web conference.
491â€“500.
[6]Fabio M Carlucci, Antonio Dâ€™Innocente, Silvia Bucci, Barbara Caputo, and Tatiana
Tommasi. 2019. Domain generalization by solving jigsaw puzzles. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2229â€“
2238.
[7]Elliot Creager, JÃ¶rn-Henrik Jacobsen, and Richard Zemel. 2021. Environment
inference for invariant learning. In International Conference on Machine Learning.
PMLR, 2189â€“2200.
[8]Brandon Da Silva and Sylvie Shang Shi. 1906. Towards improved generalization
in financial markets with synthetic data generation. arXiv preprint arXiv (1906).
[9]Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu. 2007. Boosting for
transfer learning. In Proceedings of the 24th international conference on Machine
learning. 193â€“200.
[10] Erick Delage and Yinyu Ye. 2010. Distributionally robust optimization under mo-
ment uncertainty with application to data-driven problems. Operations research
58, 3 (2010), 595â€“612.
[11] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. 2021. Retiring
adult: New datasets for fair machine learning. Advances in neural information
processing systems 34 (2021), 6478â€“6490.
[12] John Duchi and Hongseok Namkoong. 2019. Variance-based regularization with
convex objectives. Journal of Machine Learning Research 20, 68 (2019), 1â€“55.
[13] Chen Fang, Ye Xu, and Daniel N Rockmore. 2013. Unbiased metric learning:
On the utilization of multiple datasets and web images for softening bias. In
Proceedings of the IEEE International Conference on Computer Vision. 1657â€“1664.
[14] Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R Arabnia. 2021.
A brief review of domain adaptation. Advances in data science and information
engineering: proceedings from ICDATA 2020 and IKE 2020 (2021), 877â€“894.
[15] Dailin Gan and Jun Li. 2023. SCIBER: a simple method for removing batch effects
from single-cell RNA-sequencing data. Bioinformatics 39, 1 (2023), btac819.
[16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, FranÃ§ois Laviolette, Mario March, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. Journal of machine learning
research 17, 59 (2016), 1â€“35.
[17] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi.
2015. Domain generalization for object recognition with multi-task autoencoders.
InProceedings of the IEEE international conference on computer vision. 2551â€“2559.
[18] Ishaan Gulrajani and David Lopez-Paz. 2020. In search of lost domain generaliza-
tion. arXiv preprint arXiv:2007.01434 (2020).
[19] Negar Hassanpour and Russell Greiner. 2019. CounterFactual Regression with
Importance Sampling Weights.. In IJCAI. 5880â€“5887.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[21] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for
machine learning on graphs. Advances in neural information processing systems
33 (2020), 22118â€“22133.
[22] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-
Paz. 2022. Simple data balancing achieves competitive worst-group-accuracy. In
Conference on Causal Learning and Reasoning. PMLR, 336â€“351.
[23] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin
Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
Phillips, Irena Gao, et al .2021. Wilds: A benchmark of in-the-wild distribution
shifts. In International conference on machine learning. PMLR, 5637â€“5664.
[24] Masanori Koyama and Shoichiro Yamaguchi. 2020. Out-of-distribution general-
ization with maximal invariant predictor. (2020).
[25] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021. Out-of-
distribution generalization via risk extrapolation (rex). In International Conference
on Machine Learning. PMLR, 5815â€“5826.[26] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh
Shafieezadeh-Abadeh. 2019. Wasserstein distributionally robust optimization:
Theory and applications in machine learning. In Operations research & manage-
ment science in the age of analytics. Informs, 130â€“166.
[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. 2018. Learning to
generalize: Meta-learning for domain generalization. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[28] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. 2017. Deeper,
broader and artier domain generalization. In Proceedings of the IEEE international
conference on computer vision. 5542â€“5550.
[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. 2017.
Focal loss for dense object detection. In Proceedings of the IEEE international
conference on computer vision. 2980â€“2988.
[30] Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. 2022. Zin: When and how to
learn invariance without environment partition? Advances in Neural Information
Processing Systems 35 (2022), 24529â€“24542.
[31] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh,
Shiori Sagawa, Percy Liang, and Chelsea Finn. 2021. Just train twice: Improving
group robustness without training group information. In International Conference
on Machine Learning. PMLR, 6781â€“6792.
[32] Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. 2021. Heterogeneous
risk minimization. In International Conference on Machine Learning. PMLR, 6804â€“
6814.
[33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep learning
face attributes in the wild. In Proceedings of the IEEE international conference on
computer vision. 3730â€“3738.
[34] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan. 2021. A survey on bias and fairness in machine learning. ACM com-
puting surveys (CSUR) 54, 6 (2021), 1â€“35.
[35] Paul Michel, Tatsunori Hashimoto, and Graham Neubig. 2021. Modeling the sec-
ond player in distributionally robust optimization. arXiv preprint arXiv:2103.10282
(2021).
[36] Krikamol Muandet, David Balduzzi, and Bernhard SchÃ¶lkopf. 2013. Domain
generalization via invariant feature representation. In International conference on
machine learning. PMLR, 10â€“18.
[37] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. 2020.
Learning from failure: De-biasing classifier from biased classifier. Advances in
Neural Information Processing Systems 33 (2020), 20673â€“20684.
[38] Jonas Peters, Peter BÃ¼hlmann, and Nicolai Meinshausen. 2016. Causal inference
by using invariant prediction: identification and confidence intervals. Journal of
the Royal Statistical Society Series B: Statistical Methodology 78, 5 (2016), 947â€“1012.
[39] Andrea Piazzoni, Jim Cherian, Martin Slavik, and Justin Dauwels. 2020. Modeling
perception errors towards robust decision making in autonomous vehicles. arXiv
preprint arXiv:2001.11695 (2020).
[40] Hamed Rahimian and Sanjay Mehrotra. 2019. Distributionally robust optimiza-
tion: A review. arXiv preprint arXiv:1908.05659 (2019).
[41] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. 2019.
Distributionally robust neural networks for group shifts: On the importance
of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731
(2019).
[42] Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate
shift by weighting the log-likelihood function. Journal of statistical planning and
inference 90, 2 (2000), 227â€“244.
[43] Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. 2017. Certi-
fying some distributional robustness with principled adversarial training. arXiv
preprint arXiv:1710.10571 (2017).
[44] Matthew Staib and Stefanie Jegelka. 2019. Distributionally robust optimization
and generalization in kernel methods. Advances in Neural Information Processing
Systems 32 (2019).
[45] Baochen Sun and Kate Saenko. 2016. Deep coral: Correlation alignment for
deep domain adaptation. In Computer Visionâ€“ECCV 2016 Workshops: Amsterdam,
The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14. Springer,
443â€“450.
[46] Renzhe Xu, Xingxuan Zhang, Zheyan Shen, Tong Zhang, and Peng Cui. 2022.
A Theoretical Analysis on Independence-driven Importance Weighting for
Covariate-shift Generalization. In International Conference on Machine Learning.
PMLR, 24803â€“24829.
[47] Ziyu Xu, Chen Dan, Justin Khim, and Pradeep Ravikumar. 2020. Class-weighted
classification: Trade-offs and robust approaches. In International conference on
machine learning. PMLR, 10544â€“10554.
[48] Chaoqi Yang, M Brandon Westover, and Jimeng Sun. 2023. ManyDG: many-
domain generalization for healthcare applications. arXiv preprint arXiv:2301.08834
(2023).
[49] Xu Zhang, Zhiqiang Ye, Jing Chen, and Feng Qiao. 2022. AMDBNorm: an ap-
proach based on distribution adjustment to eliminate batch effects of gene ex-
pression data. Briefings in Bioinformatics 23, 1 (2022), bbab528.
 
1059Model-Agnostic Random Weighting for Out-of-Distribution Generalization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Appendix
A Proof of Equation 3 and 4
Proof. We denote Î“ğ‘’=ğœ‡ğ‘’+ğœ€for any environment ğ‘’âˆˆEğ‘ğ‘™ğ‘™and
consider the map Î¨:(ğ‘‹,Î“)â†’(ğ‘‹,ğ‘Œ).Denoteğ‘¥,ğ‘¦,ğ›¾ as the value
of the correspondingly random variables. Then the Jacobi matrix
of the map Î¨is
ğœ•(ğ‘¥,ğ‘¦)
ğœ•(ğ‘¥,ğ›¾)=1 0
ğ‘“â€²(ğ‘¥+ğœ‚ğ‘’)1
.
Thereforeğœ•(ğ‘¥,ğ‘¦)
ğœ•(ğ‘¥,ğ›¾)=1.Note thatğ‘‹andÎ“are independent,
ğ‘ƒğ‘’(ğ‘¥)ğ‘ƒğ‘’(ğ›¾)=ğ‘ƒğ‘’(ğ‘¥,ğ›¾)=ğ‘ƒğ‘’(ğ‘¥,ğ‘¦)ğœ•(ğ‘¥,ğ‘¦)
ğœ•(ğ‘¥,ğ›¾)=ğ‘ƒğ‘’(ğ‘¥,ğ‘¦),
for allğ‘’âˆˆEğ‘ğ‘™ğ‘™. Then
ğ‘ƒğ‘’(ğ‘¥,ğ‘¦)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)=ğ‘ƒğ‘’(ğ‘¥)ğ‘ƒğ‘’(ğ›¾)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)ğ‘ƒğ‘¡ğ‘Ÿ(ğ›¾)=ğ‘ƒğ‘’(ğ‘¥)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)1âˆš
2ğœ‹ğœ2ğ‘’exp(âˆ’1
2ğœ2ğ‘’(ğ›¾âˆ’ğœ‡ğ‘’)2)
1âˆšï¸ƒ
2ğœ‹ğœ2
ğ‘¡ğ‘Ÿexp(âˆ’1
2ğœ2
ğ‘¡ğ‘Ÿ(ğ›¾âˆ’ğœ‡ğ‘¡ğ‘Ÿ)2)
=ğ‘ƒğ‘’(ğ‘¥)ğœğ‘¡ğ‘Ÿ
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)ğœğ‘’exp[1
2ğœ2
ğ‘¡ğ‘Ÿ(ğ‘¦âˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)âˆ’ğœ‡ğ‘¡ğ‘Ÿ)2âˆ’1
2ğœ2ğ‘’(ğ‘¦âˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘’)âˆ’ğœ‡ğ‘’)2]
=exp[(1
2ğœ2
ğ‘¡ğ‘Ÿâˆ’1
2ğœ2ğ‘’)ğ‘¦2+(ğœ‡ğ‘’
ğœ2ğ‘’âˆ’ğœ‡ğ‘¡ğ‘Ÿ
ğœ2
ğ‘¡ğ‘Ÿ)ğ‘¦]
Â·exp[(1
ğœ2ğ‘’ğ‘“(ğ‘¥+ğœ‚ğ‘’)âˆ’1
ğœ2
ğ‘¡ğ‘Ÿğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ))ğ‘¦].
We can obtain Equation 3 and 4. â–¡
B Proof of Theorem 4.1
Proof. Actually,
|ğ´ğ‘’|=1
2ğœ2
ğ‘¡ğ‘Ÿ|ğœ2
ğ‘¡ğ‘Ÿ
ğœ2ğ‘’âˆ’1|â‰¤ğ‘Ÿ2âˆ’1
2ğœ2
ğ‘¡ğ‘Ÿ,|ğµğ‘’|=|ğœ‡ğ‘¡ğ‘Ÿ|
ğœ2
ğ‘¡ğ‘Ÿ|ğœ‡ğ‘’
ğœ‡ğ‘¡ğ‘Ÿğœ2
ğ‘¡ğ‘Ÿ
ğœ2ğ‘’âˆ’1|â‰¤|ğœ‡ğ‘¡ğ‘Ÿ|
ğœ2
ğ‘¡ğ‘Ÿ(ğ‘Ÿ3âˆ’1)
|lnâ„ğ‘’|=|lnğœğ‘¡ğ‘Ÿ
ğœğ‘’+lnğ‘ƒğ‘’(ğ‘¥)
ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥)+[(ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ)2
2ğœ2
ğ‘¡ğ‘Ÿâˆ’(ğ‘“(ğ‘¥+ğœ‚ğ‘’)+ğœ‡ğ‘’)2
2ğœ2ğ‘’]|
â‰¤2 lnğ‘Ÿ+1
2|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ
ğœğ‘¡ğ‘Ÿâˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘’)+ğœ‡ğ‘’
ğœğ‘’|
Â·|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ
ğœğ‘¡ğ‘Ÿ+ğ‘“(ğ‘¥+ğœ‚ğ‘’)+ğœ‡ğ‘’
ğœğ‘’|
Since
|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ
ğœğ‘¡ğ‘Ÿ+ğ‘“(ğ‘¥+ğœ‚ğ‘’)+ğœ‡ğ‘’
ğœğ‘’|
â‰¤|ğ‘€+ğœ‡ğ‘¡ğ‘Ÿ
ğœğ‘¡ğ‘Ÿ+ğ‘€+ğ‘Ÿğœ‡ğ‘¡ğ‘Ÿ
ğœğ‘¡ğ‘Ÿ/ğ‘Ÿ|
=(ğ‘Ÿ+1)ğ‘€+(ğ‘Ÿ2+1)|ğœ‡ğ‘¡ğ‘Ÿ|
ğœğ‘¡ğ‘Ÿ,
and
|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ
ğœğ‘¡ğ‘Ÿâˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘’)+ğœ‡ğ‘’
ğœğ‘’|
â‰¤|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)
ğœğ‘¡ğ‘Ÿâˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘’)
ğœğ‘’|+|ğœ‡ğ‘¡ğ‘Ÿ
ğœğ‘¡ğ‘Ÿâˆ’ğœ‡ğ‘’
ğœğ‘’|
â‰¤|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)|
ğœğ‘¡ğ‘Ÿ|ğœğ‘¡ğ‘Ÿ
ğœğ‘’ğ‘“(ğ‘¥+ğœ‚ğ‘’)
ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)âˆ’1|+|ğœ‡ğ‘¡ğ‘Ÿ|
ğœğ‘¡ğ‘Ÿ|ğœ‡ğ‘’
ğœ‡ğ‘¡ğ‘Ÿğœğ‘¡ğ‘Ÿ
ğœğ‘’âˆ’1|
â‰¤ğ‘€
ğœğ‘¡ğ‘Ÿ(ğ‘Ÿğ‘€+ğ¿ğ›¿
ğ‘€âˆ’1)+|ğœ‡ğ‘¡ğ‘Ÿ|
ğœğ‘¡ğ‘Ÿ(ğ‘Ÿ2âˆ’1)
=1
ğœğ‘¡ğ‘Ÿ((ğ‘Ÿâˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿+|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ2âˆ’1)).Hence
|lnâ„ğ‘’|â‰¤2 lnğ‘Ÿ+(ğ‘Ÿ+1)ğ‘€+(ğ‘Ÿ2+1)|ğœ‡ğ‘¡ğ‘Ÿ|
2ğœ2
ğ‘¡ğ‘Ÿ((ğ‘Ÿâˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿+|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ2âˆ’1)).
|lnğ‘”ğ‘’|â‰¤|ğ‘“(ğ‘¥+ğœ‚ğ‘’)
ğœ2ğ‘’âˆ’ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)
ğœ2
ğ‘¡ğ‘Ÿ||ğ‘¦|
=|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)|
ğœ2
ğ‘¡ğ‘Ÿ(ğ‘Ÿ2|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)|+ğ¿ğ›¿
|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)|âˆ’1)|ğ‘¦|
=1
ğœ2
ğ‘¡ğ‘Ÿ((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿)|ğ‘¦|.
Then sinceğ‘¤âˆˆW(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™), then by definition, we have
|lnğ‘¤(ğ‘¥,ğ‘¦)|â‰¤ğœ…ğ´ğ‘¦2+ğœ…ğµ|ğ‘¦|+ğœ…ğœ™.
Then we can obtain the conclusion. â–¡
C Proof of Theorem 4.2
Proof. Since we have acquired the concrete upper bound in
Proof of Theorem 4.1, then as long as
ğœ…ğ´â‰¥ğ‘Ÿ2âˆ’1
2ğœ2
ğ‘¡ğ‘Ÿ, ğœ…ğµâ‰¥|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ3âˆ’1)
ğœ2
ğ‘¡ğ‘Ÿ,
and
ğœ…ğœ™â‰¥2 lnğ‘Ÿ+(ğ‘Ÿ+1)ğ‘€+(ğ‘Ÿ2+1)|ğœ‡ğ‘¡ğ‘Ÿ|
2ğœ2
ğ‘¡ğ‘Ÿ[(ğ‘Ÿâˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿+|ğœ‡ğ‘¡ğ‘Ÿ|(ğ‘Ÿ2âˆ’1)].
For any environment ğ‘’âˆˆE(ğ‘Ÿ,ğ›¿), there exist ğ‘¤(ğ‘¥,ğ‘¦)=exp(ğ´ğ‘¦2+
ğµğ‘¦)â„(ğ‘¥;ğœ™)âˆˆW(ğœ…ğ´,ğœ…ğµ,ğœ…ğœ™), such that
ğ´=ğ´ğ‘’, ğµ=ğµğ‘’, â„(ğ‘¥;ğœ™)=â„ğ‘’(ğ‘¥).
Then for this ğ‘¤(ğ‘¥,ğ‘¦)and forâˆ€ğ‘¦âˆˆY, we have
sup
ğ‘¥âˆˆX|lnğ‘¤(ğ‘¥,ğ‘¦)âˆ’lnğ‘¤ğ‘’(ğ‘¥,ğ‘¦)|=sup
ğ‘¥âˆˆX|lnğ‘”ğ‘’(ğ‘¥,ğ‘¦)|â‰¤|ğ‘¦|1
ğœ2
ğ‘¡ğ‘Ÿ((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿).
â–¡
D Proof of Theorem 4.3
Proof. First, we can find that âˆ€ğœƒâˆˆÎ˜,
Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|
=Eğ‘¡ğ‘’Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|â‰¥ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‹
+Eğ‘¡ğ‘’Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|<ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‹
.
Since
lim
ğ‘â†’âˆEğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|â‰¥ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‹
=0.
Then by Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|â‰¥ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)
â‰¤Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|
<
âˆand Lebesgue Dominated Convergence theorem, we have
lim
ğ‘â†’0Eğ‘¡ğ‘’Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|â‰¥ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‹
=0.
Hence for small positive ğ›¼>0, there exist ğ‘, such that
Eğ‘¡ğ‘’Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|â‰¥ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‹
=ğ›¼.
We can observe that the large ğ‘=ğ‘(ğ›¼)is, the large ğ›¼is. From the
conclusion of the inverse function, ğ‘(ğ›¼)is an increasing function.
 
1060KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yue He et al.
We next consider the second term, for any ğ‘¥âˆˆX and small
ğ›¼âˆˆR+,âˆ€|ğœ€|<ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘(ğ›¼)/2, since we collect data from training
distribution, hence
|ğ‘¦|=|ğ‘“(ğ‘¥+ğœ‚ğ‘¡ğ‘Ÿ)+ğœ‡ğ‘¡ğ‘Ÿ+ğœ–|â‰¤ğ‘€+|ğœ‡ğ‘¡ğ‘Ÿ|+ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘(ğ›¼)/2,
and
exp{|ğ‘¦|1
ğœ2
ğ‘¡ğ‘Ÿ((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿)}
â‰¤exp{(ğ‘€+|ğœ‡ğ‘¡ğ‘Ÿ|+ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘(ğ›¼)/2)1
ğœ2
ğ‘¡ğ‘Ÿ((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿)}.
We consider
Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|<ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‹
=âˆ«
ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)|ğ‘¦âˆ’Ë†ğ‘“ğœƒ(ğ‘¥)|
ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)
ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¦|ğ‘¥)ğ¼(|ğœ€|<ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‘ğ‘¦
â‰¤ğ¶ğ›¼Eğ‘¡ğ‘’
ğ‘¤(ğ‘¥,ğ‘¦;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ‘‹
,
where
ğ¶ğ›¼=exp{(ğ‘€+|ğœ‡ğ‘¡ğ‘Ÿ|+ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘(ğ›¼)/2)1
ğœ2
ğ‘¡ğ‘Ÿ((ğ‘Ÿ2âˆ’1)ğ‘€+ğ‘Ÿğ¿ğ›¿)}.
Hence
Eğ‘¡ğ‘’Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğœ€|<ğœğ‘¡ğ‘Ÿğ‘§1âˆ’ğ‘/2)ğ‘‹
â‰¤ğ¶ğ›¼Eğ‘¡ğ‘Ÿ
ğ‘¤(ğ‘‹,ğ‘Œ;ğ´ğ‘—,ğµğ‘—,ğœ™ğ‘—)ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)
Then we obtain the result.
â–¡
E Proof of THEOREM 4.5
Proof. We have
Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|
=Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğ‘Œâ‰¥ğ¾|)
+Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğ‘Œ<ğ¾|)
Then
Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğ‘Œ<ğ¾|)
=âˆ«
|ğ‘¦âˆ’Ë†ğ‘“ğœƒ(ğ‘¥)|ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™)ğœ‹(ğ‘¤ğ‘’(ğ‘¥,ğ‘¦))
ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™)ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)
ğœ‹(ğ‘¤ğ‘’(ğ‘¥,ğ‘¦))ğ¼(|ğ‘¦|<ğ¾)ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦.
Denoteğœ‹(ğ‘¤ğ‘’)=arg infğ‘¤âˆˆW( 0,ğœ…ğµ,ğœ…ğœ™)ğ‘‘Î©(ğ‘¤,ğ‘¤ğ‘’)as the projection
ofğ‘¤ğ‘’, for|ğ‘¦|â‰¤ğ¾,
ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)
ğœ‹(ğ‘¤ğ‘’(ğ‘¥,ğ‘¦))â‰¤exp(ğ¿ğ›¿
ğœ2|ğ‘¦|)â‰¤exp(ğ¿ğ›¿
ğœ2ğ¾),
then from definition of covering number, we can know that with
probability 1âˆ’(1âˆ’N(W( 0,ğœ…ğµ,ğœ…ğœ™),ğœ‰,ğ‘‘Î©)âˆ’1)ğ‘†,
ğ‘‘Î©(ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™),ğœ‹(ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)))â‰¤ 2ğœ‰,
i.e.,
âˆ€|ğ‘¦|â‰¥ğ¾, sup
ğ‘¥âˆˆX|lnğ‘¤âˆ’lnğœ‹(ğ‘¤ğ‘’)
ğ‘¦|â‰¤2ğœ‰,We takeğ‘¤=exp(ğµğ‘¦)â„(ğ‘¥,ğœ™)andğœ‹(ğ‘¤ğ‘’)=exp(ğµğœ‹ğ‘¦)â„(ğ‘¥,ğœ™ğœ‹)do
simple computation,
sup
ğ‘¥âˆˆX|(ğµâˆ’ğµğœ‹)ğ‘¦+lnâ„(ğ‘¥,ğœ™)âˆ’lnâ„(ğ‘¥,ğœ™ğœ‹)|â‰¤ 2ğœ‰|ğ‘¦|,âˆ€|ğ‘¦|â‰¥ğ¾,
hence|ğµâˆ’ğµğœ‹|â‰¤2ğœ‰, and takeğ‘¦=ğ¾, we have
2ğœ‰ğ¾â‰¥sup
ğ‘¥âˆˆX|(ğµâˆ’ğµğœ‹)ğ¾+lnâ„(ğ‘¥,ğœ™)âˆ’lnâ„(ğ‘¥,ğœ™ğœ‹)|
â‰¥sup
ğ‘¥âˆˆX|lnâ„(ğ‘¥,ğœ™)âˆ’lnâ„(ğ‘¥,ğœ™ğœ‹)|âˆ’|(ğµâˆ’ğµğœ‹)ğ¾|
â‰¥sup
ğ‘¥âˆˆX|lnâ„(ğ‘¥,ğœ™)âˆ’lnâ„(ğ‘¥,ğœ™ğœ‹)|âˆ’2ğœ‰ğ¾.
So
sup
ğ‘¥âˆˆX|lnâ„(ğ‘¥,ğœ™)âˆ’lnâ„(ğ‘¥,ğœ™ğœ‹)|â‰¤ 4ğœ‰ğ¾.
Then we know that for |ğ‘¦|â‰¤ğ¾,
ğœ‹(ğ‘¤ğ‘’)
ğ‘¤â‰¤exp(|(ğµâˆ’ğµğœ‹)ğ‘¦+lnâ„(ğ‘¥,ğœ™)âˆ’lnâ„(ğ‘¥,ğœ™ğœ‹)|)â‰¤ exp(6ğœ‰ğ¾).
So
Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğ‘Œ<ğ¾|)
â‰¤exp(ğ¿ğ›¿
ğœ2+6ğœ‰ğ¾)Eğ‘¡ğ‘Ÿ
ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™)|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|
Then consider another term Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğ‘Œâ‰¥ğ¾|)
, since
Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğ‘Œâ‰¥ğ¾|)
=âˆ«
|ğ‘¦âˆ’Ë†ğ‘“ğœƒ(ğ‘¥)|ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™)ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)
ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™)ğ¼(|ğ‘¦|â‰¥ğ¾)ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦
From definition of covering number, with probability 1âˆ’(1âˆ’
N(W( 0,ğœ…ğµ,ğœ…ğœ™),ğœ‰,ğ‘‘Î©)âˆ’1)ğ‘†,
ğ‘‘Î©(ğ‘¤,ğ‘¤ğ‘’)â‰¤ğ¿ğ›¿
ğœ2+2ğœ‰,
hence for|ğ‘¦|â‰¥ğ¾,
ğ‘¤ğ‘’(ğ‘¥,ğ‘¦)
ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™)â‰¤exp((ğ¿ğ›¿
ğœ2+2ğœ‰)|ğ‘¦|),
we have
Eğ‘¡ğ‘’
|ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)|ğ¼(|ğ‘Œâ‰¥ğ¾|)
â‰¤âˆ«
|ğ‘¦âˆ’Ë†ğ‘“ğœƒ(ğ‘¥)|ğ‘¤(ğ‘¥,ğ‘¦;ğµ,ğœ™)exp((ğ¿ğ›¿
ğœ2+2ğœ‰)|ğ‘¦|)ğ¼(|ğ‘¦|â‰¥ğ¾)ğ‘ƒğ‘¡ğ‘Ÿ(ğ‘¥,ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦.
Then repeat the similar proof of Theorem 4.3, and with probability
1âˆ’(1âˆ’N(W( 0,ğœ…ğµ,ğœ…ğœ™),ğœ‰,ğ‘‘Î©)âˆ’1)ğ‘†,
Eğ‘¡ğ‘’hğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
â‰¤exp((ğ¿ğ›¿
ğœ2+2ğœ‰)(ğ‘€+|ğœ‡ğ‘¡ğ‘Ÿ|+ğœğ‘§1âˆ’ğ‘(ğ›¼)/2))
Â·Eğ‘¡ğ‘Ÿh
ğ‘¤(ğ‘‹,ğ‘Œ;ğµ,ğœ™)ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
+ğ›¼.
Above all, with probability 1âˆ’(1âˆ’N(W( 0,ğœ…ğµ,ğœ…ğœ™),ğœ‰,ğ‘‘Î©)âˆ’1)ğ‘†,
there exist ğ¶ğ›¼=exp[(2ğœ‰+ğ¿ğ›¿/ğœ2)(ğ‘€+|ğœ‡ğ‘¡ğ‘Ÿ|+ğ‘§1âˆ’ğ‘(ğ›¼)/2ğœ)]+
exp(6ğœ‰ğ¾+ğ¿ğ›¿/ğœ2)such that
âˆ€ğœƒâˆˆÎ˜,Eğ‘¡ğ‘’hğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
â‰¤ğ¶ğ›¼Eğ‘¡ğ‘Ÿh
ğ‘¤(ğ‘‹,ğ‘Œ;ğµ,ğœ™)ğ‘Œâˆ’Ë†ğ‘“ğœƒ(ğ‘‹)i
+ğ›¼,
whereğ‘§1âˆ’ğ‘(ğ›¼)/2represents 1âˆ’ğ‘(ğ›¼)/2quantile of Gaussian distri-
bution andğ‘(ğ›¼)is increasing about ğ›¼. â–¡
 
1061