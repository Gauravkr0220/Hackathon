Business Policy Experiments using Fractional Factorial Designs:
Consumer Retention on DoorDash
Yixin Tang
yixin@doordash.com
DoorDash, Inc.
San Francisco, CA, USAYicong Lin
nicole.lin@doordash.com
DoorDash, Inc.
San Francisco, CA, USANavdeep S. Sahni
navdeep.sahni@stanford.edu
Stanford GSB
Stanford, CA, USA
ABSTRACT
This paper investigates an approach to both speed up business
decision-making and lower the cost of learning through experimen-
tation by factorizing business policies and employing fractional
factorial experimental designs for their evaluation. We illustrate
how this method integrates with advances in the estimation of
heterogeneous treatment effects, elaborating on its advantages and
foundational assumptions. We empirically demonstrate the imple-
mentation and benefits of our approach and assess its validity in
evaluating consumer promotion policies at DoorDash, which is one
of the largest delivery platforms in the US. Our approach discovers
a policy with 5% incremental profit at 67% lower implementation
cost.
CCS CONCEPTS
â€¢Mathematics of computing â†’Hypothesis testing and confidence
interval computation .
KEYWORDS
Experimentation; Machine Learning; Causal Inference; Customer
Relationship Management; Notifications; Factorial Design; Hetero-
geneous Treatment Effect
ACM Reference Format:
Yixin Tang, Yicong Lin, and Navdeep S. Sahni. 2024. Business Policy Experi-
ments using Fractional Factorial Designs: Consumer Retention on DoorDash.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671574
1 INTRODUCTION
Businesses commonly employ randomized experiments to select
the optimal policy among several options. This approach involves
evaluating outcomes across randomly chosen units exposed to ex-
perimental policies to infer the most effective policy. While the
simplicity of this method is appealing, it can also be tedious and
delay the policy evaluation process, and thus the business inno-
vation process. This is because setting up counterfactual policies
concurrently entails setup costs that increase with the number of
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671574test policies, and it can take time to gather the necessary sample
size to obtain statistically conclusive results.1
This paper investigates an approach to solve these challenges
by considering the factorization of business policies and using fac-
torial experimental designs to evaluate policies. Using a model we
show how this approach can be combined with advances in the
estimation of heterogeneous treatment effects, and discuss its ben-
efits and its underlying assumptions. We empirically demonstrate
our approach and its benefits and assess its validity in evaluating
consumer promotion policies at DoorDash, which is one of the
largest delivery platforms in the US.
Our motivation comes from two observations.
First, since business policies touch upon several different func-
tions of the business, testing them involves significant setup costs,
which distinguishes experimenting with them from typical web ex-
periments [ 14]. Consider a simple scenario in which a digital retail
platform is considering spreading out the promotional incentives it
gives to consumers over time, as opposed to providing them upfront
which is the status quo. Implementing the test scenario realistically
requires engineering several aspects of the platform, including but
not limited to random assignment of the users into experimental
buckets; generating separate promo codes for the test and control
policies; making the users-promo codes mapping visible to the sys-
tems that use them (e.g., CRM, platform UI, Analytics); designing
relevant customer-facing messages (emails, promo banners shown
on the platform home page); designing matching user browsing
experience (platform ranking of options may change accordingly);
specifying the matching call center customer experience and train-
ing the sales support team accordingly. Merely identifying such
touchpoints takes effort. Then each of these aspects needs to be
configured correctly, coordinated across different teams owning
them, and tested thoroughly before the experiment starts. A bug
in the workflow can deter the user experience, harm the business,
and render the test useless.
Further, note that the effort required in such implementations in-
creases with scale of the number of variants, which increases more
if offline experience change is also involved. Indeed the literature
has documented several instances in which complex experiment
designs led to mistakes in implementation [16, 18].
1The literature has discussed several other aspects of this strategy including designing
experiments for decision-making [ 6], estimating long-term effects [ 1][21], presence of
network effects [5], and parallel experimentation [20].
5793
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yixin Tang, Yicong Lin, and Navdeep S. Sahni
Second, business policies are often regarded as monolithic units
and not as combinations of distinct components at the testing stage.
Consequently, businesses often implement separate policies and
compare them via A/B testing [ 15]. This approach, while tedious,
can work satisfactorily in the early design stages when the policy
space is relatively unexplored and the magnitude of improvements
(effect sizes) is large. However, the statistical power of experimen-
tation can be a constraint challenging the ability to identify im-
provements in later stages when significant advancements have
been made and the experimentsâ€™ effect sizes tend to be lower. Also,
given the rate of true improvement becomes increasingly lower,
companies that simply follow a 0.05 ğ‘-value shipping criterion will
have a higher false positive risk [ 13].2This can lead to incorrect
learning and unnecessary production costs.
In this paper, we use an analytical model to show the impact
of factorization of policies, and how one can use fractional facto-
rial designs to test them. We specify the underlying assumption
it makes, characterize the value of factorization relative to other
approaches, and integrate it with advances in estimation of hetero-
geneous treatment effects.
Next, we implement this method at DoorDash to demonstrate
its usage, assess its validity and quantify its value. In our context,
we take on the platformâ€™s problem of re-engaging inactive users
using promotions. Our objective is to find the promo configuration
that will have the maximum impact, holding constant the monetary
cost of giving the promo. In other words, given the companyâ€™s will-
ingness to spend per customer retention, what is the best structure
of the promo? How does the optimal promo structure vary across
customers? Our setting is typical because experimenting with pro-
motional policies is tedious and costly.
Following the above approach, we factorized retention policies
into four factors and nine levels in total, which is a 2Ã—2Ã—2Ã—3
design. In our implementation, we use a fractional factorial orthog-
onal experiment design with eight experimental arms. Using this
experiment, we are able to estimate the effect of each of the 24 pos-
sible policies, assuming no interaction between the factorâ€™s effects.
Given the availability of individual-level covariates, we are able
to estimate heterogeneous effects which enable us to recommend
optimal personalized policies.
Additionally, to test the modeling assumption of no interactions,
we launch an additional â€œheld outâ€ experimental arm that is held
out in estimation and used only for the purpose of evaluation.
In our application we are able to evaluate personalized effects
of 24 policies at the cost of implementing an experiment with 8
policies, showcasing a 67% reduction in implementation cost. Given
the orthogonality of the assigned factors, we gain statistical effi-
ciency because each factorâ€™s impact is estimated by leveraging the
whole sample. Overall, we are able to recover a policy that increases
2For example, when the rate of actual improvement is 5%the false positive rate can be
as high as 37%.profits by 5%.
To test our model assumptions we predict the impact of the held-
out experimental arm relative to the other eight arms and compare
this prediction to the actual difference. We repeat this procedure
separately for different individual groups. We detect no significant
differences between the model prediction and the actual differences
in these cases, showing that our data support the modeling assump-
tions.
Overall, this paper shows a practical and rigorous way of speed-
ing up business decision making by factorizing the business policy
space. As a consequence of our approach, the company might re-
duce the number of conducted experiments but increase the amount
of learning per experiment because the learning is at the factor
level and not at the policy level. This paper draws on the extensive
body of research on multivariate testing in statistics [ 2] to combine
the statistical efficiency of fractional factorial designs with mod-
ern methods for estimating heterogeneous treatment effects, and
illustrating its benefit in testing business policies. It is also related
to Conjoint Analysis work in Marketing [ 8,9] which typically in-
volves evaluating products by characterizing them as bundles of
attributes. Our approach is inspired by this literature and extends
the approach to settings such as digital platforms where cross-
sectional user behavioral data is available (as opposed to a smaller
longitudinal survey data in Conjoint), and some of the assumptions
made to speed up experimentation can be tested. While the most
commonly used Conjoint Analysis designs ask respondents to com-
pare profiles, our policy experiments expose one profile to each user.
Therefore, although the objective of estimating marginal effects
and heterogeneous treatment effects is similar to Conjoint Analysis
[7], our methodology is different. Our approach is also related to
the more recent literature on the estimation of the heterogeneity of
treatment effects [ 10,19] as we apply that approach to multivari-
ate testing. More generally, our approach is directionally related
to structural econometric methods that make assumptions for a
more efficient policy evaluation; the difference is that assumptions
in structural models are more tightly grounded in economic and
consumer theories [3, 17].
The remainder of the paper is organized as follows. In Section 2,
we break down the framework step by step and discuss its advan-
tages compared to some other commonly used experiment method-
ologies. In Section 3, we provide the business context of this paper
and discuss challenges that are not solvable by traditional A/B tests.
Then we describe the key steps to applying this framework in this
business context. In Section 4, we give an overview of the proposed
framework and how one can apply it step by step. In Section 5, we
discuss the design of the experiment, including campaign factor-
ization, and some practical considerations about sample size calcu-
lation. In Section 6, we present our results, including end-to-end
framework validation with out-of-sample variants, test the exis-
tence of heterogeneity, and discuss the benefits of this framework
in terms of experimentation velocity, business impact, and how it
creates unique opportunity for optimization using the HTE model.
Section 7 concludes this paper by summarizing the challenges and
solutions proposed in this paper.
5794Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
2 FRAMEWORK
2.1 Break down the policy space into factors
Policy factorization is one of the most important steps which sets a
foundation for reducing the experimental space and concurrently
testing different policies in later steps. First, we will discuss what
constraints the selected factors need to satisfy and how to validate
the model. We will also cover how we practically select the factors
later in sections 3 and 4.
Here, we introduce some terminology and notation used through-
out the paper.
â€¢Variant: One variant is a unique candidate business policy
â€¢Factor: One factor is a feature or attribute of a policy
â€¢Level: Value of a factor. A factor often has two or more levels
â€¢ğ‘›: Total number of potential variants
â€¢ğ¼: Total number of experimentation units
â€¢ğ‘–: Unit of experimentation; ğ‘–=1,2,...,ğ¼
â€¢ğ‘Œğ‘–: The primary business metric of unit ğ‘–
â€¢ğ¹: Total number of factors
â€¢ğ¿ğ‘“,ğ‘“=1,2,3...,ğ¹: The number of levels of the factor ğ‘“
â€¢ğ‘Šğ‘–ğ‘“ğ‘™,ğ‘“=1,2,3...ğ¹,ğ‘™=1,2,3...ğ¿ğ‘“: The binary variable denot-
ing if the unit ğ‘–â€™s factorğ‘“is on levelğ‘™
Given the notation, we can express the total number of potential
variants as the product of the number of levels of each factor,
ğ‘›=ğ¹Ã–
ğ‘™=1ğ¿ğ‘“ (1)
Our framework allows us to write ğ‘Œğ‘–as a function of factors and
levels. For example, a linear function looks like below
ğ‘Œğ‘–=ğ¹âˆ‘ï¸
ğ‘“=1ğ¿ğ‘“âˆ‘ï¸
ğ‘™=1ğ‘Šğ‘–ğ‘“ğ‘™ğ›½ğ‘“ğ‘™
+ğ¹âˆ‘ï¸
ğ‘“1=1ğ¿ğ‘“1âˆ‘ï¸
ğ‘™1=1Â©Â­
Â«ğ¹âˆ‘ï¸
ğ‘“2=1,ğ‘“2â‰ ğ‘“1ğ¿ğ‘“2âˆ‘ï¸
ğ‘™2=1ğ‘Šğ‘–ğ‘“1ğ‘™1Ã—ğ‘Šğ‘–ğ‘“2ğ‘™2ğ›½ğ‘“1ğ‘™1,ğ‘“2ğ‘™2ÂªÂ®
Â¬
+ğ¹âˆ‘ï¸
ğ‘“1=1ğ¿ğ‘“1âˆ‘ï¸
ğ‘™1=1Â©Â­
Â«ğ¹âˆ‘ï¸
ğ‘“2=1,ğ‘“2â‰ ğ‘“1ğ¿ğ‘“2âˆ‘ï¸
ğ‘™2=1Â©Â­
Â«ğ¹âˆ‘ï¸
ğ‘“3=1,ğ‘“3â‰ ğ‘“1,ğ‘“2ğ¿ğ‘“3âˆ‘ï¸
ğ‘™3=1(2)
ğ‘Šğ‘–ğ‘“1ğ‘™1Ã—ğ‘Šğ‘–ğ‘“2ğ‘™2Ã—ğ‘Šğ‘–ğ‘“3ğ‘™3ğ›½ğ‘“1ğ‘™1,ğ‘“2ğ‘™2,ğ‘“3ğ‘™3
...
+ğœ–ğ‘– (3)
whereğ‘–=1,2,3...,ğ¼,ğ¼is the total number of units(users), ğœ–ğ‘–is
the error independent of ğ‘Šğ‘–ğ‘—ğ‘˜. Note that this model allows every
combination of factor levels to have its own unique impact on ğ‘Œ.
Consequently, the number of unknowns ( ğ›½s) here will be the same
as the number of the total number of potential variants which is
ğ‘›, so to estimate this â€œfullâ€ model we will need an experiment that
implements all potential variants.
This framework allows us to make systematic assumptions about
the interaction among factors that appear in lines two and three inequation (3). Given our purpose to simplify the implementation of
the experiment, we make assumptions about the interaction terms
and assume the factors to be additive with no interactions, that is,
ğ‘Œğ‘–=ğ¹âˆ‘ï¸
ğ‘“=1ğ¿ğ‘“âˆ‘ï¸
ğ‘™=1ğ‘Šğ‘–ğ‘“ğ‘™ğ›½ğ‘“ğ‘™+ğœ–ğ‘–(ğ‘–=1,2,3...ğ¼) (4)
2.2 Factorial Design
After selecting the factors, we can encode policies as unique combi-
nations of factor levels. We can launch a full factorial experiment to
measure the treatment effect of each combination of factor values,
as in equation (3). However, in some instances, this can take un-
realistic effort for the business to prepare all policies, for example
in the context of testing customer promotions we consider in the
empirical section.
We are hence interested in fractional factorial designs that re-
duce the number of the policies needed to be implemented in the
experiment while still being able to estimate the treatment effect of
all potential policies. There are multiple fractional factorial designs
we can choose from, giving us the flexibility to trade off between
the number of implemented policies and the estimated interaction
between the factors.
In cases where we assume no interaction effect among factors
as in the model (4), all we need to do is identify the main effects.
In such cases, we are good with a Resolution III experiment design
[2] that aliases the main effects with two-factor interactions, i.e.
the main effects are indistinguishable from the effects of two-factor
interactions.
In cases where we expect significant interaction effects, we would
choose a higher resolution design, with a goal of not aliasing effects
which we think are important with effects of less importance. We
might end up with more runs when there exist higher-order inter-
actions. Although using a lower resolution design yields a lower
number of experimentally implemented variants, it is important to
keep in mind the potential risk brought by aliasing. In the example
used in this paper, we assume that there is no interaction effects be-
tween factors. To validate this, we launch another new experiment
at the end to test the validity of the framework end-to-end.
2.3 Advantages of the framework
The main advantage of the factorial framework is to improve the
sensitivity or velocity of experimentation compared to the tradi-
tional way of testing policies using ğ´/ğµ/ğ‘›testing, which makes it
suitable for environments with time-varying effects. Furthermore,
the framework allows us to make systematic assumptions about
the nonexistence of interaction across factors.
2.3.1 Improve sensitivity and velocity compared to policy A/B testing.
In this section, we analyze the increase in the speed of experimen-
tation that our framework provides.
5795KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yixin Tang, Yicong Lin, and Navdeep S. Sahni
Without breaking down the ğ‘›policies into factors, we typically
assign allğ¼units equally under each policy, and one of the ğ‘›poli-
cies is used as the control or the baseline policy. In this setting, a
natural way to estimate the treatment effect of any given policy is
via a difference-in-means estimator, where we use the average of all
units whose policy is the given policy minus the average of all units
whose policy is the control policy. The variance of the estimator is
2ğœ2
ğ¼ğ‘›, whereğœis the standard deviation of the outcome metric as-
sumed to be the same across experimental groups in this discussion.
After breaking down policies into ğ¹factors, we can estimate
the treatment effect of any policy over the control policy as the
sum of the difference between their associated factors. For the ğ‘“th
factor, the variance of the difference-in-means estimator between
two different levels is2ğœ2
ğ¼/ğ¿ğ‘“. Hence, the variance of the sum of these
difference-in-means estimators is equal to2ğœ2
ğ¼(Ãğ¹
ğ‘“=1ğ¿ğ‘“).
From the above argument, we can get the ratio of the variances
of the two estimators as follows:
ğ‘›
Ãğ¹
ğ‘“=1ğ¿ğ‘“=Ãğ¹
ğ‘“=1ğ¿ğ‘“
Ãğ¹
ğ‘“=1ğ¿ğ‘“(5)
Using the fact that the Minimum Detectable Effect (MDE) is lin-
early proportional to the standard error of the estimator, the ratio
of the MDE of our framework to a typical policy A/B testing frame-
work isâˆšï¸„Ãğ¹
ğ‘“=1ğ¿ğ‘“Ãğ¹
ğ‘“=1ğ¿ğ‘“. The ratio is less than 1when a factor has at least
2different levels, which means that our framework allows us to
detect a much smaller effect than the policy A/B testing framework.
In our use case where we break policies into 4factors with 2,3,2,2
levels, respectively, we are able to detect an MDE which is 38%of
the MDE the the A/B testing framework.
2.3.2 Reduce experiment/policy setup cost. After we factorize the
policy space, one way is to launch a full factorial experiment that
tests allÃğ¹
ğ‘“=1ğ¿ğ‘“combinations. However, practically this can in-
volve high setup effort costs to select users targeting groups and
implement them. In our framework, we reduce the number of poli-
cies needed to be actually tested by using fractional factorial design.
This allows for a reduction in set-up costs, while still enabling
estimation of the effects of all potential policies.
2.4 Estimate heterogeneous treatment effect
The above framework can be extended beyond estimating the aver-
age treatment effect for each policy to estimating the heterogeneous
treatment effects of each factor level and, therefore, estimating the
heterogeneous treatment effect of each candidate policy. When it is
possible to conduct policy targeting based on the available experi-
mental unit characteristics, this add-on benefit allows the business
to assess and formulate personalized optimal policies. Let ğ‘‹ğ‘–denote
a vector of unit ğ‘–â€™s pre-experiment characteristics. We can then
extend our model to a more generic form. The model below uses
a linear HTE model; as previously mentioned, a non-linear model
can also be used.ğ‘Œğ‘–=ğ¹âˆ‘ï¸
ğ‘“=1ğ¿ğ‘“âˆ‘ï¸
ğ‘™=1ğ‘Šğ‘–ğ‘“ğ‘™
ğ›½ğ‘“ğ‘™+ğœ†â€²
ğ‘“ğ‘™ğ‘‹ğ‘–
+ğ›¾â€²ğ‘‹ğ‘–+ğœ–ğ‘– (6)
Here, we introduce two new parameters compared to (4):ğ›¾rep-
resents heterogeneity in the outcome ğ‘Œacross units with different
ğ‘‹s; andğœ†ğ‘“ğ‘™represents the corresponding heterogeneity in the treat-
ment effect of policy factors.
2.4.1 Validate if HTE model is needed. If there are no strong priors
ofğ‘‹ğ‘–s impacting the factorâ€™s treatment effects, one might statisti-
cally test the existence of systematic heterogeneity in the effects.
Specifically, we can test the null hypothesis that all interaction
termsğœ†â€™s are equal to 0;
ğ»0:ğœ†11=ğœ†12=...=ğœ†ğ¹ğ¿ğ¹=0 (7)
If the null hypothesis is not rejected, there may not be detectable
heterogeneous effects across the feature space. This may indicate
that going from a ğ´ğ‘‡ğ¸ toğ»ğ‘‡ğ¸ model may not lead to much im-
provement. However, this test can be conservative in practice [ 6],
and firms might still use HTE if there are quantitatively significant
benefits from it.
2.4.2 Build HTE model. There are a variety of models available for
estimating heterogeneous treatment effects. On a high level, they
can be classified into direct and indirect estimation methods [11].
Indirect estimation models are trained to minimize the loss func-
tion based on the observed and predicted metric values, such as the
squared error. In our application that is:
ğ¿(Ë†ğ›½)=ğ¸[(ğ‘Œğ‘–âˆ’Ë†ğ‘Œğ‘–(ğ‘‹ğ‘–,{ğ‘Šğ‘–ğ‘“ğ‘™,ğ‘“=1,2...ğ¹,ğ‘™=1,2,...ğ¿ğ‘“}))2]
=ğ¸ï£®ï£¯ï£¯ï£¯ï£¯ï£°Â©Â­
Â«ğ‘Œğ‘–âˆ’ğ¹âˆ‘ï¸
ğ‘“=1ğ¿ğ‘“âˆ‘ï¸
ğ‘™=1ğ‘Šğ‘–ğ‘“ğ‘™
ğ›½ğ‘“ğ‘™+ğœ†â€²
ğ‘“ğ‘™ğ‘‹ğ‘–
âˆ’ğ›¾â€²ğ‘‹ğ‘–ÂªÂ®
Â¬2ï£¹ï£ºï£ºï£ºï£ºï£»(8)
Various approaches within the indirect approach are either para-
metric, putting different assumptions on sample distribution about
the error terms ğœ–and regularization (for example a linear or logit
Lasso regression model), or search for parameter values to minimize
the loss function non-parametrically.
Having obtained an estimate of the regression above, we can pre-
dict the conditional average treatment effect based on the predicted
expected outcome difference as:
Ë†ğœ(ğ‘‹ğ‘–,ğ‘Šğ´,ğ‘Šğµ)=ğ¹âˆ‘ï¸
ğ‘“=1ğ¿ğ‘“âˆ‘ï¸
ğ‘™=1(ğ‘Šğ´
ğ‘“ğ‘™âˆ’ğ‘Šğµ
ğ‘“ğ‘™)(Ë†ğ›½ğ‘“ğ‘™+Ë†ğœ†ğ‘“ğ‘™â€²ğ‘‹ğ‘–) (9)
whereğ‘Šğ´andğ‘Šğµare two sets of indicators representing two
different policies ğ´andğµin the factor-level space; Ë†ğ›¾, and Ë†ğœ†are the
estimated parameters; Ë†ğœ(ğ‘‹ğ‘–,ğ‘Šğ´,ğ‘Šğµ)denotes the estimated effect
of switching from policy ğ´toğµfor a unitğ‘–with characteristics ğ‘‹ğ‘–.
5796Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The direct estimation models predict the conditional average
treatment effect (CATE) directly. Say, we want to estimate the treat-
ment effect of policy ğµrelative toğ´. We can use machine-learning
methods to predict this effect. For example, we can use causal K-
nearest-neighbors (KNN); for any user characteristics ğ‘‹ğ‘–, we find a
set ofğ¾nearest neighbors in the ğ‘‹space which are given policy ğ´
represented by ğ‘Šğ´in the factor-level space in the experiment, and
similar set what was given the policy ğµrepresented by ğ‘Šğµ. We
then estimate the CATE using the difference-in-means estimator:
Ë†ğœ(ğ‘‹ğ‘–,ğ‘Šğ´,ğ‘Šğµ)=1
ğ¾âˆ‘ï¸
ğ‘¢âˆˆğ‘ğ¾(ğ‘‹ğ‘–,ğ‘Šğ´)ğ‘Œğ‘¢âˆ’1
ğ¾âˆ‘ï¸
ğ‘¢âˆˆğ‘ğ¾(ğ‘‹ğ‘–,ğ‘Šğµ)ğ‘Œğ‘¢(10)
Hereğ‘ğ¾(ğ‘‹ğ‘–,ğ‘Šğ´)is the set of the K nearest neighbors with
the policyğ´. Note that the estimator above is unbiased due to the
independence between user characteristics ğ‘‹ğ‘–and the policy as-
signment. Also, note that when the hyper-parameter ğ¾uses the
largest possible value, ğ‘ğ¾(ğ‘‹ğ‘–,ğ‘Šğ´)becomes all units received ğ´,
similarly,ğ‘ğ¾(ğ‘‹ğ‘–,ğ‘Šğµ)becomes all units that received ğµ. The esti-
mator becomes equivalent to the estimator given by (9).
2.5 Evaluate the framework
There are two parts of evaluations that we conduct, one to evaluate
the correctness of our framework to test the ATE of each policy,
which essentially evaluate the correctness of our additive main
effects and no-interaction assumptions; the other one to evaluate
the HTE framework. In the evaluation of the HTE framework, we
not only want to evaluate the prediction accuracy but more impor-
tantly, we want to evaluate how much the key business metrics
such as profit can be improved using the framework.
2.5.1 Evaluating Model Assumptions with Out-of-Sample Variant .
To evaluate the correctness of our framework, we launch an addi-
tional variant formed by the same set of factors but not belonging
to the other experimental test policies (we refer to this as the out-of-
sample or held-out variant). From the experiment, we can estimate
the treatment effect of the out-of-sample variant relative to each in-
sample variant via a t-test. From there, we can jointly test whether
the observed treatment effects and our predicted treatment effects
from our framework are statistically different.
2.5.2 Evaluating Model Assumptions with Out-of-Sample Variant
on User Segments. To get more data points for comparison, espe-
cially since the noise-signal ratio is high, we propose the following
additional comparisons. First, we can compare the out-of-sample
variant with each of the in-sample-variant and compare the pre-
dicted differences with actual observed differences.
Additionally, we repeat this procedure across various distinct
consumer groups. The idea is to split users into heterogeneous
groups. Once the split is done, we can compare the predicted treat-
ment effects with the observed ones for each group. If our frame-
work holds, we expect the observed and predicted effects to match
across all groups.
Here is an overview of the procedure:(1)Select user features ğ‘‹ğ‘–which are historically known to pre-
dict the effects of similar treatments, or those which are
highly predictive of metric ğ‘Œğ‘–
(2)Train a regression or another machine learning model ğ‘“:
ğ‘‹ğ‘–â†’ğ‘Œğ‘–using historical data and apply ğ‘“to predict on the
experimental sample as well as the sample corresponding to
the out-of-sample variant.
(3)Chooseğ¾as the number of groups, and discretize the sam-
ple into equal-sized ğ¾groups based value of the predicted
outcome. The number of groups ğ¾can be chosen by any
cluster analysis algorithm such as partitioning clustering
like K-means or density-based clustering like DBSCAN.
(4)For each sample subgroup and each experimental variant,
we calculate the difference between the average predicted
outcome value for those sample units assigned the experi-
mental variant and those assigned the out-of-sample variant.
This is the predicted relative treatment effect.
(5)Similarly, for each sample subgroup and each experimental
variant, we calculate the difference between the average
observed outcome value for those sample units assigned the
experimental variant and those assigned the out-of-sample
variant. This is the observed relative treatment effect.
(6)Compare the predicted relative treatment effects with the
"true" observed relative treatment effect across all the groups
and variants.
3 BUSINESS CONTEXT
We focus on an application of our approach at DoorDashâ€™s Con-
sumer Retention Marketing. Doordashâ€™s primary offering â€œDoor-
Dash Marketplaceâ€ provides a suite of services that enable mer-
chants to establish an online presence, generate demand, seamlessly
transact with consumers, and fulfill orders primarily through inde-
pendent contractors who use its platform to deliver orders.3The
DoorDash Consumer Retention Marketing team aims to build a
lasting relationship with customers as soon as they engage with
DoorDash by providing them useful personalized experiences and
driving them to return to DoorDash to find the relevant merchants
and products.
3.1 Status Quo Experimentation Approach and
Limitations
Incremental value of a marketing policy, in general, is estimated us-
ing a randomized field experiment whenever possible at DoorDash.
When optimizing a marketing program, analysts iteratively exper-
iment on hypotheses, ship winning variant, conduct dimension
analysis based on the experimentâ€™s data â€“ analyze how different
segments within the sample respond differently, based on which
analysts hypothesize further improvements, and propose the next
round of experiments.
There are some key challenges that significantly limit the exper-
imentation and innovation speed
3DoorDash Inc. (2023) Form 10-Q. U.S. Securities and Exchange Commis-
sion. https://d18rn0p25nwr6d.cloudfront.net/CIK-0001792789/83885643-4f88-4a45-
a869-cd2fef24524e.pdf
5797KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yixin Tang, Yicong Lin, and Navdeep S. Sahni
â€¢Experiment duration for running sequential testing can be
long due to factors such as limited testable population, lim-
ited marketing budget which limits the total sample for test-
ing, and long term success metrics which require long mea-
surement period.
â€¢If we were to conduct a randomized experiment testing all
possible combinations in one experiment, both the direct
implementation cost (i.e. labor cost and marketing spend)
and indirect opportunity cost of exposing consumers with
sub-optimal policies could be extremely high [ 11]. On labor
cost, implementing a multi-variant marketing experiment
takes an average 1 week per variant to complete. Key steps
of implementation includes 1/ Determining target audience;
2/ Configure the promo code 3/ Design content copy for off-
app channels and in-app placements 4/ Build an automated
workflow to send off-app notifications. This roughly means 7-
8 operator week for setting up an experiment with 8 variants,
24 operator week for 24 variants. For example, after setting
up 4 distinct promo codes, a marketer would need to insert
the correct promo code in all the email templates in the
CRM platform. If we were to test into 2 different triggering
timing (which corresponds to distinct â€œworkflowsâ€) and 2
different content copies (which corresponds to distinct email
â€œtemplatesâ€) at the same time, these 4 distinct promo codes
will have to be manually inserted in 4*2*2 numbers of places.
â€¢Marketing spend is also a key consideration. It is fair to
assume marketing cost will linearly scale with the number
of variants we test holding statistical power constant. The
indirect cost refers to the opportunity cost of offering less
performing treatment during the entire experiment duration.
4 EXPERIMENT DESIGN
4.1 Break down campaign design into factors
In order to design our treatments in a more methodical way, we
identified four main factors that are at the core of our hypotheses:
promo spread, discount, triggering timing and message.
â€¢Promo Spread: There are many ways one can deliver a long-
duration, say a 4 week, promotion. A straightforward and
commonly used way is to provide and communicate the
whole promotion incentive upfront to consumers via email or
mobile push notifications. We call this "Upfront". As opposed
to Upfront, Spread promo is where the same offered incentive
is divided into two halves that are given in sequence. So
consumers receive two 2-week long offers; one in week 1, and
another in week 3. This can change the amount of attention
consumers give to the incentive and times when they do so.
â€¢Discount: Discount is the type of monetary incentive we
can offer. It can take various formats such as â€œfree deliveryâ€,
â€œpercentages offâ€, â€œdollars offâ€ or a combination of them.
â€¢Triggering Timing: Triggering timing is when we trigger
the marketing campaign to consumers. Campaigns may be
triggered by consumer events, for example, when consumers
experience a bad delivery, or consumers stop ordering for an
extended period of time. There are two types of triggering
timing of interest here:â€“Ongoing: Start the campaign as soon as the consumer
becomes eligible
â€“Weekday trigger only: trigger only on weekdays. For con-
sumers who happen to become eligible over the weekends,
we trigger the campaign on the next Monday.
â€¢Messaging: Email and push notifications are the primary
channels used to communicate offers to consumers. While
push notification usually contains short messages, email
content can be rich, which gives two options:
â€“Generic promo messaging: the email speaks only about
the promotion being offered, which is a relatively plain
email.
â€“Merchant Recommendation: the email provides some rec-
ommendations to the consumer in addition to the offer.
4.2 Apply Fractional Factorial Design
After creating these four factors, three of which have two levels and
one has three levels, we have 24 combinations. So a full factorial
design testing all combinations will require 24 experimental arms.
There are major practical operational challenges in setting up such
a 24-arm marketing campaign, as mentioned in section 3.1.
To solve this problem, we apply fractional factorial design shown
in figure 1 to shrink the number of variants from 24 to 8, which
makes the execution manageable while retaining the ability to make
inferences about the untested variants by making modeling assump-
tions. In addition, when factors are independently randomized, we
are able to effectively use the whole sample to analyze the impact
of each factor.4
Figure 1: Experimental Treatment Arms Implemented
4Additionally, we include in our design a randomly chosen Control group of users
who are not given any promo. The sole purpose of the Control group is to provide a
benchmark to estimate the overall impact of the program.
5798Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.3 Additional Experimental Arm for Validation
Evaluation of the 16 policies excluded from the experiment relies
on modeling assumptions of no significant interaction terms. Such
effects are likely to be small, according to the teamâ€™s priors but no
theory guarantees this. To empirically test this, we launch a 9th
experimental arm, referred to as the â€œout of sampleâ€ arm, which is
randomly chosen from the excluded 16. The idea is to compare the
observed effect of this variant relative to others, and the correspond-
ing model predicted effect to validate our frameworkâ€™s assumptions.
The 9th arm was implemented after the other eight, and ran concur-
rently for a subset of the time period. So for validation comparisons
below, we will use a subset of the data when all arms were concur-
rently active.
5 RESULTS
5.1 Testing Model Assumptions
5.1.1 Joint Test on Out-of-Sample Variant. As we described in Sec-
tion 2.5.1, we conducted a joint test to check if there is a statistically
significant difference between our frameworkâ€™s predictions and ob-
served data. Given that we have a total of eight in-sample variants
and one out-of-sample variant, there are eight differences in total;
âˆ€ğ‘–predicted profit change going from arm ğ‘–to out-of-sample arm,
denoted as arm 9, versus the observed change. As a result of the
joint test of differences, we obtain a p-value of 0.80, which means
that there is no detectable difference between the predictions of our
framework and the observed data. This supports the validity of our
framework end-to-end, especially our assumption of no interaction
effects.
Figure 2 plots the predicted and observed effects for a visual
comparison, showing that the predicted and observed effects are
significantly correlated. For robustness, we repeat this analysis us-
ing orders as our dependent metric. Figure 3 shows a similar high
degree of correlation between the actual observed and predicted ef-
fects. Overall, this analysis supports our assumption that the model
is capable of predicting results in policy configurations not included
in the experiment.
5.1.2 Evaluate across User Segments. To conduct further compar-
isons using heterogeneous user populations, we grouped the sample
by features such as average spend per order (avg-order-spend), pro-
motion lifetime orders, orders rate during pre-churn period, churn
tenure, and number of visits during the pre-churn period. In to-
tal, we chose the number of groups ğ¾=10, so in total we have
10Ã—8=80predicted effects to compare with the observed effects.
From the plots in figures 4 and 5 we can visually see that the
predicted and the observed user-segment level treatment effects
are positively correlated. The slopes in both these figures are statis-
tically significant (corresponding ğ‘-values equal to 0.02 and <0.01
respectively) which further supports our framework.
It is also interesting to note that the variance of the predicted
treatment effects is less than the variance of the observed ones,
Figure 2: Design Validation - Metric: VP
Figure 3: Design Validation - Metric: Order Rate 14d
Figure 4: Compare predicted HTE to observed HTE
- Metric: Profit
which occurs due to the additional sampling noise in the observed
data.
5799KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yixin Tang, Yicong Lin, and Navdeep S. Sahni
Figure 5: Compare predicted HTE to observed HTE
- Metric: Orders
This suggests that this evaluation method may have a higher
false-positive rate when the unexplained variance is high.5
5.2 Factorial Experiment Results
We begin our analysis by estimating the impact of our factor lev-
els on the business outcome metric, average profits per user, by
estimating the Ë†ğ›½ğ‘“ğ‘™â€™s that are the estimates of the parameters in (4)
using a linear regression. Table 1 shows these estimates, with the
first level of each factor as the omitted baseline.
We assess the importance of each factor, which is the maximum
impact the factor can have on the outcome metric. Specifically, for
a factorğ‘“, we calculate maxğ‘™{Ë†ğ›½ğ‘“ğ‘™âˆ’minğ‘™Ë†ğ›½ğ‘“ğ‘™}. By this measure, we
note that Discount is the most important factor, followed by Promo
Spread, Messaging, and Trigger Timing. Within the Discount factor,
Level 3, which stands for a â€œ%offâ€ with a limited redemption count
promo representation of the discount has the largest and most sta-
tistically significant coefficient.
Overall, these estimates tell us that the way the discount is com-
municated in a promotion is the most important factor among those
considered here, and, specifically, a unified â€œ%offâ€ with a limited
max redemption count representation of the discount has the great-
est impact on profit for the target audience of the program we
optimize for.
Using Table 1, we can estimate a profit gain using this framework
against the traditional sequential a/b to be about 5%. We computed
that as a percentage difference between the best ATE and worst ATE.
Specifically, the profit from the best treatment = 7.35231+0.4195
= 7.77181; the worst treatment = 7.35231. It is important to note
that the â€œworstâ€ possible treatment may happen not so rarely in
practice if we were to test sequentially and decided to control for
promo level 1 and test into different variations of triggering time
and messaging.
5In other contexts, researchers have proposed adjustments of the prediction variance
by accounting for the prediction error [4].Variable Name Coef Std err t P >|t|
Intercept 7.35231 23.08625 61.032 0.000
Promo Spread [Upfront] 0.12905 0.40522 1.414 0.157
Discount [Level2] 0.40757 1.27977 3.648 0.000
Discount [Level3] 0.41950 1.31723 3.253 0.001
Trigger Timing [weekday] -0.01350 -0.04239 -0.147 0.883
Messaging [Merchant Recs] -0.02732 -0.08578 -0.298 0.766
Notes: Variables are represented as â€œFactor[Level]â€.
We use data on the eight experimental arms for this estimation.
Table 1: Regression of Average profits per user on factor
levels.
Variable Name coef std err t P >|t|
Intercept 1.25977 0.21980 5.734 0.000
Promo [upfront] 0.26784 0.16642 1.602 0.109
Discount[level3] 0.93321 0.23550 3.951 0.000
Discount[level2] 0.63302 0.20410 3.089 0.002
Trigger Timing[weekday only] 0.36361 0.16642 2.176 0.030
Messaging[mxrec] 0.01256 0.16642 0.076 0.940
avg-order-spend 0.23864 0.00628 33.103 0.000
Promo[upfront]Ã—avg-order-spend -0.00534 0.00628 -0.998 0.318
avg-order-spendÃ—Discount[level3] -0.01947 0.00628 -2.520 0.012
avg-order-spendÃ—Discount[level2] -0.00848 0.00628 -1.237 0.216
Trigger Timing[weekday only] Ã—avg-order-spend -0.01476 0.00628 -2.681 0.007
Messaging[mxrec]Ã—avg-order-spend -0.00126 0.00628 -0.212 0.832
Notes: Variables are represented as â€œFactor[Level]â€.
We use data on the eight experimental arms for this estimation.
Table 2: Regression of Average profits per user on factor levels
interacting with avg-order-spend.
avg-or
der-spend Factors Predicted Profit
Promo Spread Discount Trigger Timing Messaging
[0,10] upfr
ont level3 weekday only mxrex 2.837+0.1975Ã—ğ‘ğ‘£ğ‘”âˆ’ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâˆ’ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘
[11,24] upfront level3 weekday only generic 2.824+0.1986Ã—ğ‘ğ‘£ğ‘”âˆ’ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâˆ’ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘
[25,26] upfront level3 ongoing generic 2.461+0.2134Ã—ğ‘ğ‘£ğ‘”âˆ’ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâˆ’ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘
[27,48] upfront level2 ongoing generic 2.160+0.2247Ã—ğ‘ğ‘£ğ‘”âˆ’ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâˆ’ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘
[49,75] spread level2 ongoing generic 1.893+0.2301Ã—ğ‘ğ‘£ğ‘”âˆ’ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâˆ’ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘
[76,âˆ] spread level1 ongoing generic 1.260+0.2385Ã—ğ‘ğ‘£ğ‘”âˆ’ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâˆ’ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘
Table 3: Optimal Policies given avg-order-spend
5.3 Data
5.3.1 Metric selection. The marketing campaignâ€™s goal is to encour-
age more usage of the product hence driving higher sales volume
while spending promotion budget efficiently. Therefore, our main
success metric is â€œaverage profits per userâ€ which is sales revenue
minus costs, such as promotional costs within 28 days after first
exposure to the promo. For the purpose of validating our frame-
work we use an additional metric, order volume or order rate, for
robustness. While these are the two metrics we use for analysis,
the company tracks other metrics such as customer retention and
promo redemption rate, and guardrail metrics such as delivery
quality, and manual support ticket volume (which would surge if a
promotion is not implemented properly).
Viable Policy. The main success metrics are used to make pro-
jection about the payback period: how long it will take for the
promotion spend to be paid back in full. A marketing policy is
considered to be viable for scaling if its payback period is within
an acceptable range, provided no statistically nor economically sig-
nificant change in guardrail metrics. For the results section of the
paper, we have used this metric.
5.3.2 Sample Size Calculation. As in a traditional A/B test, we
assess the required sample size by calculating the baseline of our
5800Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
outcome metric, estimating the minimum detectable effect (MDE)
and calculating the smallest sample size required at the chosen type
I (5%) and type II error rate (20%).
Our experimental design enables us to test the comparative effec-
tiveness of multiple factors, each of which has two or more variants
[12]. In other words, we are testing different levels within each
factor against the base level of that factor. For example, how does
a Spread promo perform compared to Upfront. Therefore, when
estimating the MDE, we want to estimate the MDE for each of the
four factors, as we expect different sensitivity for different factors
based on our historical experiments and domain knowledge.
Using the Promo Spread factor as an example, we first use busi-
ness tradeoffs to choose a minimum detectable effect (d ) of the
success metric and calculate the sample size needed for each level
of this factor using the following formula:
ğ‘›=(ğ‘§1âˆ’ğ›¼+ğ‘§1âˆ’ğ›½)ğœ
ğ‘‘2
(11)
whereğœis the standard deviation of the dependent variable.
Given that Promo Spread has two levels, the total sample size
needed is nÃ—2. We repeat this sample-size calculation process
for the other three factors and go with the largest sample size
required.
5.3.3 Covariates. Past experiments and customer data analysis
show customer heterogeneity in response to promotional offers.
In the teamâ€™s experience, a consumerâ€™s previous average behavior
is also predictive of his future behavior. On the basis of this un-
derstanding, we chose covariates to assess heterogeneity in effecs.
Here are some examples of chosen covariates:
â€¢Lifetime orders. The amount of orders a consumer previously
placed on DoorDash typically serves as a signal of product
adoption and loyalty. Consumers who have placed orders
beyond a certain threshold are more likely than others to
adopt new product features, and react to marketing messages
and redeem promotions.
â€¢Promotion usage. Historical promotion usage is an indicator
of customer price sensitivity and the likelihood of promo
redemption when receiving one.
â€¢App visit pattern. Frequent app visit without placing orders
could indicate promotion seeking or price comparison.
â€¢Basket size. Historical basket size (average spend per oder) is
observed to correlate with profitability. Customers who tend
to place bigger sized orders could redeem more promotions
especially when given "%off" type of promotions. This is an
important feature to consider when analyzing a promotionâ€™s
benefit.
â€¢Order day of the week. Whether the order arrives during
weekdays or weekends indicates the role DoorDash plays in
the customerâ€™s life style. Weekday orderers may view food
delivery service as a way to bring convenience into their fast
paced lifestyle while customers who tend to place orders
over the weekend may crave for something but donâ€™t want
to leave their home and wait in line. Such customer types
may respond differently to promo configurations.6 CONCLUSION
Businesses have begun to rigorously use A/B testing to optimize
policies. However, the path to optimization using A/B testing re-
quires effort, is time-consuming, and necessitates prioritization over
potential hypotheses to test in typical settings with low statistical
power. This paper presents a case study with empirical experiment
data that conducts and validates a fractional factorial design in
the marketing policy optimization space. This paper presents a
framework that breaks down the business policy space into factors,
accelerating learning and optimization velocity by improving sta-
tistical power given limited testable sample. Subsequently, we use a
fractional factorial design to reduce the number of variants required
to be implemented for experimentation, significantly reducing the
implementation costs. Additionally, we continue to build on this
methodology to leverage heterogeneous treatment effects and im-
prove business outcomes by enabling optimal personalized policies.
Furthermore, we have devised a robust evaluation procedure that
facilitates the validation of model assumptions when dividing the
policy space into factors.
In our business context, our framework enables us to discover a
policy with 5% incremental profit, with a 267% higher experimenta-
tion speed and 67% lower setup cost, relative to the status quo. This
framework also presents a rare opportunity to run an HTE model
on a randomized experimental sample. Exploiting the heterogeneity
in treatment effects we can further improve the business impact by
2%. Overall, we believe this framework can be applied to a broad
category of experiments where the cardinality of the policy space
is high and implementation costs are prohibitive.
ACKNOWLEDGMENTS
We are grateful to Elea Feit and Seenu Srinivasan for their com-
ments and suggestions on this paper. We also express our gratitude
to our partners at DoorDash, Kristin Mendez, Meghan Bender, Will
Stone, and Taryn Riemer for helping us configure and launch the
experiments and supporting us throughout this research. We also
acknowledge the contributions of the data science and engineer-
ing community at DoorDash, especially Qiyun Pan, Caixia Huang,
and Zhe Mai. Finally, we thank Gunnard Johnson, Jason Zheng,
Bhawana Goel, and Sudhir Tonse. The completion of this research
would not have been possible without your contribution and sup-
port.
REFERENCES
[1]Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. The
surrogate index: Combining short-term proxies to estimate long-term treatment
effects more rapidly and precisely. Technical Report. National Bureau of Economic
Research.
[2]George EP Box, J Stuart Hunter, and William G Hunter. 2005. Statistics for
experimenters. In Wiley series in probability and statistics. Wiley Hoboken, NJ.
[3]Pradeep K Chintagunta. 2018. Structural models in marketing. Handbook of
Marketing Analytics (2018), 200â€“224.
[4]Weitao Duan, Shan Ba, and Chunzhe Zhang. 2021. Online Experimentation
with Surrogate Metrics: Guidelines and a Case Study. In Proceedings of the 14th
ACM International Conference on Web Search and Data Mining. ACM. https:
//doi.org/10.1145/3437963.3441737
[5]Dean Eckles, RenÃ© F Kizilcec, and Eytan Bakshy. 2016. Estimating peer effects in
networks with peer encouragement designs. Proceedings of the National Academy
of Sciences 113, 27 (2016), 7316â€“7322.
5801KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yixin Tang, Yicong Lin, and Navdeep S. Sahni
[6]Elea McDonnell Feit and Ron Berman. 2019. Test & Roll: Profit-Maximizing A/B
Tests. Marketing Science 38, 6 (2019), 1038â€“1058. https://doi.org/10.1287/mksc.
2019.1194 arXiv:https://doi.org/10.1287/mksc.2019.1194
[7]Max Goplerud, Kosuke Imai, and Nicole Pashley. 2022. Estimating Heterogeneous
Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis.
(01 2022).
[8]Paul E Green. 1978. Conjoint analysis in consumer research: issues and outlook.
Journal of consumer research 5, 2 (1978), 103â€“123.
[9]Paul E Green and V Srinivasan. 1990. Conjoint analysis in marketing research:
New developments and directions. Journal of Marketing 54, 4 (1990), 3.
[10] GÃ¼nter J Hitsch and Sanjog Misra. 2018. Heterogeneous treatment effects and
optimal targeting policy evaluation. Available at SSRN 3111957 (2018).
[11] GÃ¼nter J. Hitsch and Sanjog Misra. 2018. Heterogeneous Treatment Effects and
Optimal Targeting Policy Evaluation. (2018). Ã¢Ä‚ÅƒÃ¢Ä‚Åƒhttps://papers.ssrn.com/
sol3/papers.cfm?abstract_id=3111957
[12] Randy Brown Jelena Zurovac. 2012. Orthogonal Design: A Powerful Method for
Comparative Effectiveness Research with Multiple Interventions. (4 2012).
[13] Ron Kohavi, Alex Deng, and Lukas Vermeer. 2022. A/B Testing Intuition Busters:
Common Misunderstandings in Online Controlled Experiments. https://doi.org/
10.1145/3534678.3539160[14] Ron Kohavi, Roger Longbotham, Dan Sommerfield, and Randal M Henne. 2009.
Controlled experiments on the web: survey and practical guide. Data mining and
knowledge discovery 18 (2009), 140â€“181.
[15] Ron Kohavi and Stefan Thomke. 2017. The surprising power of online experi-
ments. Harvard business review 95, 5 (2017), 74â€“82.
[16] Klaus M Miller, Navdeep S Sahni, and Avner Strulov-Shlain. 2022. Sophisti-
cated Consumers with Inertia: Long-Term Implications from a Large-Scale Field
Experiment. Available at SSRN 4065098 (2022).
[17] Peter C Reiss and Frank A Wolak. 2007. Structural econometric modeling: Ra-
tionales and examples from industrial organization. Handbook of econometrics 6
(2007), 4277â€“4415.
[18] Navdeep S Sahni, Sridhar Narayanan, and Kirthi Kalyanam. 2019. An experimen-
tal investigation of the effects of retargeted advertising: The role of frequency
and timing. Journal of Marketing Research 56, 3 (2019), 401â€“418.
[19] Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneous
treatment effects using random forests. J. Amer. Statist. Assoc. 113, 523 (2018),
1228â€“1242.
[20] Caio Waisman, Navdeep S Sahni, Harikesh S Nair, and Xiliang Lin. 2019. Parallel
experimentation on advertising platforms. arXiv preprint arXiv:1903.11198 (2019).
[21] Jeremy Yang, Dean Eckles, Paramveer Dhillon, and Sinan Aral. 2023. Targeting
for long-term outcomes. Management Science (2023).
5802