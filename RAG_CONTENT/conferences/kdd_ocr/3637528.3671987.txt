HiGPT: Heterogeneous Graph Language Model
Jiabin Tang
University of Hong Kong
Hong Kong, China
jiabintang77@gmail.comYuhao Yang
University of Hong Kong
Hong Kong, China
yuhao-yang@outlook.comWei Wei
University of Hong Kong
Hong Kong, China
weiwei1206cs@gmail.com
Lei Shi
Baidu Inc.
Beijing, China
harryshi.cs@gmail.comLong Xia
Baidu Inc.
Beijing, China
long.phil.xia@gmail.comDawei Yin
Baidu Inc.
Beijing, China
yindawei@acm.org
Chao Huangâˆ—
University of Hong Kong
Hong Kong, China
chaohuang75@gmail.com
ABSTRACT
Heterogeneous graph learning aims to capture complex relation-
ships and diverse relational semantics among entities in a hetero-
geneous graph to obtain meaningful representations for nodes and
edges. Recent advancements in heterogeneous graph neural net-
works (HGNNs) have achieved state-of-the-art performance by
considering relation heterogeneity and using specialized message
functions and aggregation rules. However, existing frameworks
for heterogeneous graph learning have limitations in generalizing
across diverse heterogeneous graph datasets. Most of these frame-
works follow the "pre-train" and "fine-tune" paradigm on the same
dataset, which restricts their capacity to adapt to new and unseen
data. This raises the question: â€œCan we generalize heterogeneous
graph models to be well-adapted to diverse downstream learning
tasks with distribution shifts in both node token sets and relation
type heterogeneity?â€ To tackle those challenges, we propose HiGPT,
a general large graph model with Heterogeneous graph instruction-
tuning paradigm. Our framework enables learning from arbitrary
heterogeneous graphs without the need for any fine-tuning pro-
cess from downstream datasets. To handle distribution shifts in
heterogeneity, we introduce an in-context heterogeneous graph
tokenizer that captures semantic relationships in different hetero-
geneous graphs, facilitating model adaptation. We incorporate a
large corpus of heterogeneity-aware graph instructions into our
HiGPT, enabling the model to effectively comprehend complex rela-
tion heterogeneity and distinguish between various types of graph
tokens. Furthermore, we introduce the Mixture-of-Thought (MoT)
instruction augmentation paradigm to mitigate data scarcity by
âˆ—Chao Huang is the Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671987generating diverse and informative instructions. Through compre-
hensive evaluations conducted in various settings, our proposed
framework demonstrates exceptional performance in terms of gen-
eralization performance, surpassing current leading benchmarks.
We make our model implementation openly available, along with
comprehensive details at: https://github.com/HKUDS/HiGPT.
CCS CONCEPTS
â€¢Information systems â†’Data mining; Language models; â€¢
Mathematics of computing â†’Graph algorithms.
KEYWORDS
Large Language Models, Graph Learning
ACM Reference Format:
Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, and Chao
Huang. 2024. HiGPT: Heterogeneous Graph Language Model. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, Barcelona,
Spain, 12 pages. https://doi.org/10.1145/3637528.3671987
1 INTRODUCTION
Heterogeneous graphs have garnered extensive popularity and
adoption in various domains, including recommendation systems [ 5],
knowledge graphs [ 26], social network analysis [ 4], and biological
networks [ 16]. These graphs encompass entities of diverse types
that engage in a multitude of interactions, enabling a comprehen-
sive representation of complex systems [ 3]. The focus of hetero-
geneous graph learning is to derive meaningful representations
for the nodes and edges within such graphs [ 11,15,42]. These
representations aim to capture the intricate relationships and di-
verse relational semantics that exist within the graph, facilitating a
deeper understanding of the underlying structural heterogeneity.
In recent years, there has been a growing recognition of the sig-
nificant potential of heterogeneous graph neural networks (HGNNs)
in capturing the intricate and diverse information that resides
within heterogeneous graph structures [ 28,36]. HGNNs leverage
the expressive capabilities of high-order message passing tech-
niques, enabling them to effectively model the complex relation-
ships, diverse entity types, and heterogeneous semantics present in
 
2842
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiabin Tang et al.
these graphs. By aggregating and propagating information across
various node and edge types, HGNNs facilitate a deeper under-
standing and analysis of the intricate inter-dependencies that exist
within heterogeneous graph structures. Some notable examples
of HGNNs include: i) Metapath-based GNNs such as HAN [ 29]
and MAGNN [ 5]; ii) Transformer-enhanced GNNs like HGT [ 9].
Furthermore, heterogeneous graph self-supervised learning, in-
cluding contrastive methods (e.g., DMGI [ 18] and HeCo [ 31]), and
generative (e.g., HGMAE [ 24]) methods, showcase effectiveness in
alleviating data scarcity in real-world heterogeneous graph data.
Despite the demonstrated effectiveness of current frameworks
for heterogeneous graph learning, they possess limitations when it
comes to generalizing across diverse heterogeneous graph datasets.
These frameworks commonly adopt the "pre-train" and "fine-tune"
paradigm, where they are initially trained on a specific dataset and
subsequently fine-tuned on the same dataset [ 9,24,31]. However,
this approach presents challenges in adapting and achieving optimal
performance on new and unseen data. The heavy reliance on the
characteristics and patterns of the original training dataset hinders
their ability to effectively handle the intricacies and complexities in-
herent in different heterogeneous graph datasets. As a result, these
frameworks may encounter difficulties in effectively addressing the
diverse nuances and variations present in various heterogeneous
graph datasets, particularly when applied to downstream tasks.
This research aims to push the boundaries of heterogeneous
graph models by addressing a fundamental question: "Can we de-
velop highly adaptable and versatile heterogeneous graph models
capable of effectively addressing diverse downstream learning tasks,
even when faced with distribution shifts in node token sets and
relation type heterogeneity?" To tackle this challenge, we introduce
HiGPT as a novel and general solution. Our model is specifically de-
signed to overcome key challenges associated with generalization
across various downstream heterogeneous graph learning tasks.
C1. Relation Type Heterogeneity Shift. One of the primary chal-
lenges we focus on in this research is the shift in relation type het-
erogeneity within various heterogeneous graph structures. In such
graphs, entities are connected by various types of relations, and
these relation types can differ significantly across diverse datasets.
To illustrate this, letâ€™s consider two examples. In a recommendation
system, a heterogeneous graph may involve node-wise relationships
between users and items. The relation types in this scenario could in-
clude "click," "favorite," "review," and "purchase." On the other hand,
in an academic graph, the relations could involve "paper-paper,"
"author-paper," and "paper-venue." These examples demonstrate
how different heterogeneous graphs can exhibit diverse relation
heterogeneity with distinct semantics across domains.
Solution: In-Context Heterogeneous Graph Tokenizer. To
achieve adaptability in a wide range of heterogeneous graph sce-
narios with varying node and edge types, we introduce the in-
context heterogeneous graph tokenizer. This tokenizer captures the
diverse semantic relationships found in different heterogeneous
graphs, providing a unified approach. It comprises two essential
components: the in-context parameterized heterogeneity projector,
which utilizes language to encode distinct node and edge types,
facilitating effective model adaptation, and the parameter allocator,
which dynamically assigns tailored parameters to the tokenizer. Tooptimize performance and integrate the tokenizer seamlessly into
the HiGPT framework, we employ pre-training with a lightweight
text-graph contrastive alignment paradigm. This pre-training pro-
cess directly incorporates the tokenizer into HiGPT, enhancing its
capabilities and ensuring smooth functionality within the overall
model architecture, including integration with the language model.
C2. Complex Heterogeneous Graph Structures. The primary
focus of this study is to tackle the challenge of integrating large
language models (LLMs) into heterogeneous graph learning, with
the goal of enhancing model generalization. Our specific objective
is to develop a graph-oriented language model that excels in com-
prehending the intricate structural information inherent in complex
heterogeneous graph structures. In doing so, we strive to equip the
graph model with the ability to not only recognize the heterogene-
ity of relations among different types of nodes, but also capture the
distinct characteristics of entities belonging to the same type.
Solution: Heterogeneous Graph Instruction-Tuning. We intro-
duce a novel heterogeneous graph instruction-tuning framework
that integrates inter-type and intra-type token matching tasks to
fine-tune large language models (LLMs). Our framework specifically
targets the enhancement of LLMsâ€™ understanding of both hetero-
geneous relation awareness and homogeneous relation awareness.
By utilizing these tasks, our aim is to bolster the LLMsâ€™ capabilities
in the following areas: (i) distinguishing between different types
of graph tokens, (ii) comprehending intricate relationships within
heterogeneous graphs, (iii) preserving the distinctive attributes of
entities within homogeneous graphs, and (iv) effectively harnessing
diverse graph instructions during the training process.
C3. Data Scarcity for Model Fine-Tuning. In practical scenarios
involving heterogeneous graph learning, one of the key challenges
is the limited availability of data [ 10,37]. This scarcity of data poses
a significant obstacle when fine-tuning models for real-world ap-
plications. For instance, when utilizing heterogeneous graphs to
model cold-start users or items in recommendation systems, the
sparse nature of user interaction data restricts the availability of
supervised signals [ 33,35]. This data scarcity hampers the effec-
tiveness of task-specific model fine-tuning and necessitates the
development of novel techniques to overcome this challenge.
Solution: Mixture-of-Thought Augmentation. Our approach
introduces a novel mechanism for augmenting graph instructions,
emphasizing the use of Mixture-of-Thought (MoT) combined with
various prompting techniques. This integration enables us to gen-
erate a diverse and comprehensive set of informative task-specific
instructions. By seamlessly incorporating these augmented graph
instructions into our framework, we anticipate that our model en-
hancement will effectively address the challenge of data sparsity.
Experiments. To assess the efficacy of HiGPT, we perform a com-
prehensive set of experiments to showcase the impressive gener-
alization capabilities of our heterogeneous graph language model
across diverse settings. We thoroughly investigate the design ratio-
nales, effectiveness, and efficiency of the key model components.
2 PRELIMINARIES
Heterogeneous Graph. A heterogeneous graph is a graph denoted
asG(V,E,A,T,R,X). It consists of nodes represented by V, edges
 
2843HiGPT: Heterogeneous Graph Language Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
represented byE, and an adjacency matrix Athat captures the
relationships between nodes. The sets TandRsignify the types
of nodes and edges, respectively. Additionally, the feature matrix
X={ğ‘‹ğ‘‡ğ‘–âˆˆR|Vğ‘‡ğ‘–|Ã—ğ‘‘ğ‘‡ğ‘–}contains attributes associated with each
node. Here, ğ‘‡ğ‘–refers to a specific node type, while ğ‘‘ğ‘‡ğ‘–represents
the dimensionality of the corresponding node attributes.
Meta Relation. In a heterogeneous graph, a meta relation is a
representation of the relationship between different types of nodes
connected by an edge. Specifically, for an edge ğ‘’that links a source
nodeğ‘¢of typeğ‘‡ğ‘–to a target node ğ‘£of typeğ‘‡ğ‘—, the meta relation
ofğ‘’=(ğ‘¢,ğ‘£)is denoted as <ğœ(ğ‘¢),ğœŒ(ğ‘’),ğœ(ğ‘£)>. Here,ğœ(ğ‘¢)andğœ(ğ‘£)
represent the node types of ğ‘¢andğ‘£respectively, drawn from the
set of node typesT, whileğœŒ(ğ‘’)denotes the relation type of the
edgeğ‘’, which is selected from the set of relation types R. This meta
relation provides a concise representation of the heterogeneous
connections in the graph, capturing the types of nodes involved.
Heterogeneous Graph Neural Networks (HGNNs). In the con-
text of a heterogeneous graph G, Heterogeneous Graph Neural
Networks (HGNNs) employ message passing and aggregation tech-
niques to incorporate neighbor information based on different node
and edge categories. This enables the modeling of heterogeneous
structural semantic relationships, as expressed below:
â„(ğ‘™)
ğ‘£= Aggregate
âˆ€ğ‘¢âˆˆN(ğ‘£),âˆ€ğ‘’âˆˆE(ğ‘¢,ğ‘£)
Propagate
â„(ğ‘™âˆ’1)
ğ‘¢ ;â„(ğ‘™âˆ’1)
ğ‘£,ğ‘’
(1)
Here,N(ğ‘£)represents all the source nodes connected to node
ğ‘£, andE(ğ‘¢,ğ‘£)denotes the set of edges connecting node ğ‘¢and
nodeğ‘£. In most HGNNs, the parameters of the Propagate(Â·)and
Aggregate(Â·)functions depend on the types of nodes ğ‘¢andğ‘£, as
well as the edge ğ‘’. However, this implies that HGNNs are limited
to modeling the specific heterogeneous graph they were trained
on and cannot be effectively applied to new heterogeneous graphs
with different node and edge types. This limitation greatly ham-
pers the generalization ability of HGNNs in capturing the diverse
semantic relationships across various heterogeneous graphs.
3 METHODOLOGY
3.1 In-Context Heterogeneous Graph Tokenizer
To make our HiGPT adaptable to a wide range of heterogeneous
graph scenarios with varying node and edge types, we propose an
in-context heterogeneous graph tokenizer. This method captures
the diverse semantic relationships present in different heteroge-
neous graphs, ensuring a unified approach. It comprises two es-
sential components: the in-context parameterized heterogeneity
projector and the parameter allocator. The adaptive in-context pro-
jector utilizes language to encode the distinct node and edge types
within the heterogeneous graphs, facilitating model adaptation.
Meanwhile, the parameter allocator dynamically assigns param-
eters tailored specifically for the tokenizer. To optimize the tok-
enizerâ€™s performance and seamlessly integrate it within the HiGPT
framework, we conduct pre-training using a simplified text-graph
contrastive learning framework. This pre-training process directly
incorporates the tokenizer into the HiGPT framework and effec-
tively integrates it with the Large Language Model (LLM). This ap-
proach enhances the tokenizerâ€™s capabilities and ensures its smooth
functioning within the overall model architecture.3.1.1 Graph Tokenization with Meta Projector. Given a het-
erogeneous graphGwith a feature matrix X={ğ‘‹ğ‘‡ğ‘–âˆˆR|Vğ‘‡ğ‘–|Ã—ğ‘‘ğ‘‡ğ‘–,ğ‘‡ğ‘–âˆˆ
T}and an adjacency matrix A, the goal of the heterogeneous
graph tokenizer is to encode the hidden representations of the
heterogeneous graph, denoted as H={ğ»ğ‘‡ğ‘–âˆˆR|Vğ‘‡ğ‘–|Ã—ğ‘“ğ‘‡ğ‘–,ğ‘‡ğ‘–âˆˆT} .
This is achieved through the function H=HG-Tokenizer(X,A),
whereğ‘“ğ‘‡ğ‘–represents the hidden dimension of node type ğ‘‡ğ‘–. The
HG-Tokenizer(Â·)can be implemented using various backbone HGNN
architectures, such as HetGNN [41], HAN [30], or HGT [9].
However, the generalization capability of these heterogeneous
GNNs is constrained by their inherent design, which includes
pre-defined parameter learning tailored to specific heterogeneous
graphs. As a result, the trained HGNNs cannot be readily applied
to other unseen heterogeneous graphs, which goes against the ob-
jective of achieving unified encoding with the HG-Tokenizer. To
illustrate, letâ€™s consider HGT as an example. In HGT, the calcula-
tion ofâ„(ğ‘™)
ğ‘£involves utilizing functions such as Attention(Â·)and
Message(Â·)to process information from the source nodes:
eâ„(ğ‘™)
ğ‘£=âŠ•
âˆ€ğ‘¢âˆˆN(ğ‘£)(Attention(ğ‘¢,ğ‘’,ğ‘£)Â·Message(ğ‘¢,ğ‘’,ğ‘£))
â„(ğ‘™)
ğ‘£=Fğœ(ğ‘£)
Î˜1
ğœ
eâ„(ğ‘™)
ğ‘£
+â„(ğ‘™âˆ’1)
ğ‘£
=Wğœ(ğ‘£)
1Â·
ğœ
eâ„(ğ‘™)
ğ‘£
+bğœ(ğ‘£)
1+â„(ğ‘™âˆ’1)
ğ‘£ (2)
The notationFğœ(ğ‘£)
Î˜1(Â·)represents a fully-connected layer with pa-
rameters Î˜1={Wğœ(ğ‘£),bğœ(ğ‘£)}. Here,ğœ(ğ‘£)denotes the node type of
ğ‘£, andğœ(Â·)represents the activation function. The specific formula-
tion of the Attention(Â·)andMessage(Â·)functions, with â„heads:
Attention(ğ‘¢,ğ‘’,ğ‘£)
=Softmax
âˆ€ğ‘¢âˆˆN(ğ‘£) 
âˆ¥
ğ‘–âˆˆ[1,â„]Fğœ(ğ‘¢)
Î˜2
â„(ğ‘™âˆ’1)
ğ‘¢
WğœŒ(ğ‘’)
1Fğœ(ğ‘£)
Î˜3
â„(ğ‘™âˆ’1)
ğ‘£!
Message(ğ‘¢,ğ‘’,ğ‘£)=âˆ¥
ğ‘–âˆˆ[1,â„]Fğœ(ğ‘¢)
Î˜4
â„(ğ‘™âˆ’1)
ğ‘¢
WğœŒ(ğ‘’)
2(3)
Adaptive Parameterized Heterogeneity Projector. To make
our HiGPT adaptable to a wide range of heterogeneous graphs
with varying graph heterogeneity settings, and to eliminate the
requirement of pre-defining the number of type-specific projections
in advance, we propose the design of a type-aware parameterized
projector. This projector dynamically and automatically encodes the
relation heterogeneity into latent representations. More specifically,
the type-aware projectors with the parameters Fğœ(ğ‘£)
Î˜ğ‘–andWğœŒ(ğ‘’)
ğ‘–
are generated automatically according to the following procedure:
Î˜ğ‘–={Wğœ(ğ‘£)
ğ‘–;bğœ(ğ‘£)
ğ‘–}=FÎ©
Tğœ(ğ‘£)
;WğœŒ(ğ‘’)
ğ‘–=FÎ©
TğœŒ(ğ‘’)
(4)
FÎ©is a fully-connected layer with parameters Î©, while Tğœ(ğ‘£)and
TğœŒ(ğ‘’)are the features associated with node type ğœ(ğ‘£)and edge type
ğœŒ(ğ‘’), respectively. It is important to note that the example provided
showcases the usage of the in-context parameterized heterogeneity
projector within the heterogeneous graph transformer framework.
However, our HiGPT is designed to be versatile and adaptable,
allowing for the integration of diverse heterogeneous GNNs.
 
2844KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiabin Tang et al.
Heterogeneous Graph TransformerHuman instructions with graph tokens <Graph>â€œThisnoderepresentsamovieâ€LMsSentence-BERTâ€œThemovieisdirectedbythedirectorâ€Language-Enriched Heterogeneity RepresentationThisnoderepresentsamovieThisnodeiscategorizedasadirectorThisnodeisanactorThisnodeindicatesStanleyKubrickThisnoderepresentsAngLeeThisnodeisAlfredHitchcockHeterogeneous Graph TokensNodes of type 1Nodes of type 2â€¦(a) Heterogeneous Relation Awareness(b) Homogenous Relation Awareness(c) Heterogeneity-aware Fine-Tuningğ‘Š
Graph Projectorâ€¦â€¦
LLMsVicuna
HGT LayersÃ—ğ¿â€¦â€¦Adaptive Parameterized Heterogeneity Projectorğ¹!ğ‘‡
Instructa. Node-level Inferenceb. Edge-level InferenceHiGPTHiGPTHiGPTInput Heterogeneous Graph
Heterogeneous Graph Corpus
Figure 1: The overall architecture of our HiGPT.
Language-Enriched Heterogeneity Representation . We lever-
age natural language as a means to generate universal heterogeneity
representations for nodes and edges based on their respective types.
For instance, in the heterogeneous IMDB dataset, we can describe
a "movie" node as "This node represents a movie" using natural
language. Similarly, the edge ("movie", "to", "director") can be ex-
pressed as "The movie is directed by the director". To encode these
natural language descriptions of nodes and edges, we employ a
pre-trained language model such as Sentence-BERT [ 21] to obtain
type representations. To ensure distinguishability and diversity
among different types, we utilize multiple languages to describe
the same type. The encoded representations from the pre-trained
language models are averaged to derive the final representation.
This process can be defined as follows:
Tğœ(ğ‘£)=Mean-Pooling
Sentence-BERT
Sğœ(ğ‘£)
TğœŒ(ğ‘’)=Mean-Pooling
Sentence-BERT
SğœŒ(ğ‘’)
(5)
Sğœ(ğ‘£)andSğœŒ(ğ‘’)represent sets of descriptions for node type ğœ(ğ‘£)
and edge type ğœŒ(ğ‘’), respectively. For instance, consider the example
of the edge ("movie", "to", "director"). One possible description is:
S("movie","to","director")={
"The movie is directed by the director" ,
"The film features direction by the director" ,Â·Â·Â·} (6)
For comprehensive descriptions featuring text-enriched heterogene-
ity representations of various datasets, please consult the appendix.
3.1.2 Lightweight Text-Graph Contrastive Alignment. Build-
ing upon recent advancements in aligning cross-modality seman-
tics [ 19,34], we draw inspiration to employ a text-graph contrastive
alignment paradigm for pre-training the proposed heterogeneous
graph tokenizer. This approach aims to align the modeling capa-
bilities of language and heterogeneous structures, enabling bet-
ter collaboration between the tokenizer and the language mod-
els. To begin, we consider raw textual contents represented as
C=ğ‘ğ‘–âˆˆRğ‘™ğ‘–Ã—ğ‘‘,1â‰¤ğ‘–â‰¤ğ‘, whereğ‘denotes the total number of
heterogeneous graph nodes X=ğ‘‹ğ‘‡ğ‘–âˆˆR|Vğ‘‡ğ‘–|Ã—ğ‘‘ğ‘‡ğ‘–. Here,ğ‘™ğ‘–repre-
sents the length of the textual content associated with the ğ‘–-th node.
In our approach, we adopt a lightweight text-graph contrastivealignment paradigm formally presented as follows:
Ë†H=norm(HG-Tokenizer(X)),Ë†T=norm(LM-Tokenizer(C))
L=1
2 CE(Î›,y)+ CE(Î›âŠ¤,y),Î›=(Ë†HË†TâŠ¤)Â·exp(ğœ) (7)
We use the contrastive label y=(0,1,Â·Â·Â·,ğ‘›âˆ’1)âŠ¤and the Cross-
Entropy function CE(Â·). Our implementation employs the multi-
layer vanilla transformer for LM-Tokenizer (Â·).
3.2 Heterogeneous Graph Instruction Tuning
The objective of HiGPT is to empower language models to directly
generate predictions for downstream tasks with the unseen het-
erogeneous graph and corresponding instructions. The natural
language instruction is first encoded by a tokenizer into text em-
beddings, denoted as XI=LM-tokenizer(instruction) . To align
the dimensions, we employ a projector that maps graph tokens to
the same dimension as the text embeddings, given by XG=ğ‘“P(H),
which can be as simple as a linear layer. For a sequence of length ğ¿,
we determine the probability of generating the target output XO:
ğ‘(XO|XG,XI)=ğ¿Ã–
ğ‘–=1ğ‘Î¦(ğ‘¥ğ‘–|XG,XI,<ğ‘–,XO,<ğ‘–) (8)
where Î¦represents the learnable parameters within HiGPT.
3.2.1 Instruction Tuning with Heterogeneous Graph Corpus.
To enable the Language Model (LLM) to effectively differentiate be-
tween different types of input heterogeneous graph tokens and the
specific nodes within each type, based on natural language instruc-
tions, we propose instruction pre-training using a large "corpus"
consisting of heterogeneous graph-instruction pairs. This approach
equips the fine-tuned HiGPT with a comprehensive understanding
of both homogeneous and heterogeneous graph structures.
â€¢Heterogeneous Relation Awareness. Our objective is to en-
hance the language modelâ€™s proficiency in distinguishing be-
tween specific types of nodes within a heterogeneous context,
taking into account the intricate relationships. This is achieved
by leveraging the information encoded in the graph tokens.
â€¢Homogeneous Relation Awareness. Our aim is to equip the
model with the ability to establish a significant correspondence
between sequences of graph tokens that belong to the same
category and their corresponding natural language descriptions.
 
2845HiGPT: Heterogeneous Graph Language Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Prompts for the three tasks of heterogeneous graph instruction-tuning.
Heter
oGraph Human Question HiGPT Response
(
a) Heterogeneous Instruction Pre-training
central_nodes: ("movie": [1, ...,
n]), num_neighbors:[10, 10]Given a heterogeneous graph about movies, there are 3 types of nodes: <DESC>. By performing
random sampling, a heterogeneous subgraph is obtained. Separately nodes of different types
are: 1. <graph>, 2. <graph>... Please sequentially provide the types for the node sequences.Based on graph tokens,
types of the graph tokens
should be 1. movie, 2. ....
(
b) Homogenous Instruction Pre-training
central_nodes: ("paper": [1, ...,
n]), num_neighbors: [10, 10]Given a heterogeneous graph about papers, there are 4 types of nodes: <DESC>. ..., a heteroge-
neous subgraph is obtained. The nodes for "paper" are:<graph>. Also, a list of textual descriptions
for the papers are: <DESC>. Please reorder the text list based on the order of graph tokens.The matching of graph to-
kens and papers should be:
<ANSWER>.
(
c) Heterogeneous Supervised Fine-Tuning
central_nodes: ("movie": [i]),
num_neighbors: [10, 10]Given a heterogeneous graph about movies, there are 3 types of nodes: <DESC>. ..., a heteroge-
neous subgraph is obtained. There are nodes of different types: "movie" nodes: <graph>, <DESC>
where the 0-th node is the central node. "actor" nodes: <graph>; "director" nodes: <graph>. Which
of the following classes does this movie belong to: action, comedy, drama?Based on the given infor-
mation, the likely category
formovie isAction.
Heterogeneous Graph Instruction. In our graph instruction, we
incorporate a heterogeneous subgraph generated through random
neighbor sampling, accompanied by a question generated by a
human. To enhance the diversity of the heterogeneous graph corpus,
we conduct multiple samplings focusing on nodes from different
types. Additionally, we introduce the <graph> token as a graph
indicator within the human question. i) To achieve heterogeneous
relation awareness, we introduce the inter-type token matching
task. This task involves providing the Language Model (LLM) with
encoded sequences of graph tokens from different types, enabling
it to differentiate between the various types. ii) For homogeneous
relation awareness, we design the intra-type matching task, where
the LLM receives encoded sequences of graph tokens from a specific
type, allowing it to establish correspondence with the relevant
descriptions. Further details regarding the instruction template at
this stage are illustrated in Table 1 and Table 2.
3.2.2 Heterogeneity-aware Fine-Tuning. To customize the rea-
soning abilities of the language model for specific downstream
tasks on heterogeneous graphs, we propose Heterogeneity-aware
Fine-Tuning. This approach entails conducting supervised learning
with task-specific instructions following the initial instruction pre-
training phase with heterogeneous graph corpus. It allows us to
refine the LLMâ€™s performance and adapt it to the specific require-
ments of the targeted tasks on heterogeneous graphs.
In this stage, we incorporate a randomly sampled heterogeneous
subgraph centered around the target node, along with a human-
generated question. Given that the previous phase of instruction
pre-training has already equipped the model with heterogeneous
and homogeneous relation awareness, we design human questions
that are rich in heterogeneity. These questions contain sequences
of graph tokens from different types, indicated by multiple occur-
rences of the <graph> token. Additionally, the human question
includes pertinent auxiliary information pertaining to the target
node. The designs of these instructions are presented in Figure 1.
3.3 Mixture-of-Thought (MoT) for Graph
Instruction Augmentation
In practical scenarios of heterogeneous graph learning, data scarcity
often poses a challenge. This is especially true when using hetero-
geneous graphs to model cold-start users/items in recommendationTable 2: Instruction template for IMDB.
Node Classification
Input: Given a heterogeneous graph about internet movie, there are three types
of nodes, namely: movie, actor, director. The relationships (meta paths) between
different nodes include: [movie is directed by director], [movie has actor]. By per-
forming random sampling of 2-hop 10 neighbors centered on the target movie
node, a heterogeneous subgraph is obtained. In the subgraph, "movie" nodes:
<graph> , where the 0-th node is the central node that represents a moive with
the following information: Name: {movie name} Directorâ€™s name: {director
name} Actorsâ€™ names: {actor name} Plot keywords: {plot keywords} "actor"
nodes: <graph> ; "director" nodes: <graph> . Question: Which of the following
classes does this movie belong to: action, comedy, drama? {Prompting Suffix}
Output: {Answer (& Reasoning)}.
Instruction for Heterogeneous Relation Awareness
Input: Given a heterogeneous graph about internet movie, there are three
types of nodes, namely: movie, actor, director. The relationships (meta paths)
between different nodes include: [movie is directed by director], [movie has
actor]. By performing random sampling of 2-hop 10 neighbors centered on
the target movie node, a heterogeneous subgraph is obtained. In the subgraph,
there are several sequences of heterogeneous graph nodes of different types as
follows:1. <graph> 2.<graph> 3.<graph> Question: please sequentially provide
the categories for the node sequences. Example: 1. actor 2. movie 3. director.
Output: {The correct sequence of node types}.
Instruction for Homogeneous Relation Awareness
Given a heterogeneous graph about internet movie, there are three types of
nodes, namely: movie, actor, director. The relationships (meta paths) between
different nodes include: [movie is directed by director], [movie has actor]. By
performing random sampling of 2-hop 10 neighbors centered on the target
movie node, a heterogeneous subgraph is obtained. In the subgraph, there are
"movie" nodes: <graph>. Here is a list of moviesâ€™ names, their directorsâ€™ names,
their actorsâ€™ names and their plot keywords: {movie lists} Question: please
reorder the list of movies according to the order of graph tokens (i.e., complete
the matching of graph tokens and movies)
Output: {The correct sequence of moviesâ€™ names}.
systems, where sparse user interaction data limits the availabil-
ity of supervised signals. To address the issue of data sparsity, we
propose enhancing our HiGPT by incorporating augmented graph
 
2846KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiabin Tang et al.
Human QuestionImaginethatyoumadethecorrectchoiceandproceedwithstep-by-stepreasoning...Imaginethreedifferentexpertsareansweringquestion,thenshareitwiththegroup...Imaginethatthreeexpertsarediscussingthequestionwithapaneldiscussion...
Pleasegeneratesomeknowledgethatcanassistinformulatingtheanswer.Imaginethatyouareatthecorrectanswerbasedontheprovidedinformationandknowledge,andpresentstep-by-stepreasoning.Ground TruthCoT SimulationToT SimulationPanel SimulationHuman QuestionGround TruthReasoningGKP SimulationGround TruthReasoningHuman QuestionKnowledge
Figure 2: Mixture-of-Thought (MoT) Augmentation
instructions. Drawing inspiration from previous works [ 22], we in-
troduce a novel method for instruction augmentation in the context
of heterogeneous graph instruction tuning. This method utilizes
prompt engineering techniques, particularly Mixture-of-Thought
(MoT), to generate diverse and informative instructions. The goal
is to effectively overcome the challenges posed by data scarcity. By
incorporating augmented graph instructions, we expect our model
enhancement to effectively handle data sparsity.
3.3.1 Mixture-of-Thought (MoT) Prompting. Our focus is on
designing and optimizing prompts to effectively utilize language
models [ 14,23,32,38]. We employ several techniques to enhance
language models: i) Chain-of-Thought (CoT) [32]: CoT prompts
introduce intermediate steps, enabling complex reasoning and so-
phisticated capabilities. ii) Tree-of-Thought (ToT) [38]: ToT main-
tains a tree structure of coherent language sequences called thoughts.
These thoughts serve as systematic intermediate steps for problem-
solving. iii) PanelGPT [23]: PanelGPT incorporates panel discus-
sions among language models, enhancing the prompt engineer-
ing process through collaboration. iv) Generated Knowledge
Prompting (GKP) [14]: GKP involves incorporating additional
knowledge into prompts for enhancement. By leveraging these
techniques, our objective is to augment heterogeneous graph in-
structions, especially in scenarios with limited data availability.
3.3.2 Instruction Augmentation with Priori Knowledge. We
utilize seven instruction augmentation strategies, each generating
seven augmented instructions for every question-answer pair, incor-
porating the characteristics of Mixture-of-Thought (MoT). However,
closed-source language models such as ChatGPT may produce in-
correct answers, resulting in flawed guidance. To overcome this
issue, we propose incorporating prior knowledge, specifically the
correct answer, into the prompt. It enables LLM to simulate gener-
ating the correct answer and produce intermediate reasoning steps
using different MoT methods, as shown in Figure 1.
4 EVALUATION
To assess the effectiveness of our HiGPT model, our experiments
are designed to address the following research questions:
â€¢RQ1: How does the performance of our HiGPT compare to that
of baseline methods in both few-shot and zero-shot scenarios?
â€¢RQ2: To what extent do the key components of HiGPT contribute
to its overall performance across various settings?â€¢RQ3: Can the HiGPTâ€™s universal heterogeneity knowledge be
leveraged to achieve graph in-context learning solely through
graph instruction examples, without any model optimization?
4.1 Experimental Settings
4.1.1 Experimental Datasets. The experiments were conducted
on three benchmark datasets, i.e., IMDB [ 7], DBLP [ 7], and ACM [ 30].
IMDB is an extensive dataset that focuses on online movies and
television programs. It encompasses 4278 movies, 2081 directors,
and 5257 actors. Each movie is categorized into one of three classes:
Action, Comedy, or Drama. DBLP, on the other hand, consists of
4057 authors, 14328 papers, 7723 terms, and 20 publication venues.
This dataset was gathered from a computer science bibliography
website and the authors are distributed among four research ar-
eas: Database, Data Mining, Artificial Intelligence, and Information
Retrieval. Lastly, the ACM dataset comprises 3025 papers, 5835
authors, and 56 subjects. The papers are classified into three classes:
Database, Wireless Communication, and Data Mining. In Table 3,
we present the statistical information of the datasets used in our
experiments, where the types of the target nodes in each heteroge-
neous graph are highlighted with an underline .
Table 3: Detailed statistics of utilzed datasets.
Dataset #
Nodes #
Edges Metapaths #
Classes
A
CMPap
er:
3025 P-
A: 9949 P
AP
3A
uthor: 5959 P-S:
3025 PSP
Subje
ct: 56 P-
T: 255619 PTP
T
erm: 1902 P-P:
5343
DBLPA
uthor :
4057 P-
A: 19645 P
AP
4Pap
er: 14328 P-C:
14328 APCP
A
T
erm: 7723 P-
T: 85810 APTP
A
Confer
ence: 20
IMDBMo
vie:
4278 M-D:
4278 MAM
3 Dir
ector: 2081 M-
A: 12828 MDM
A
ctor: 5257
4.1.2 Evaluation Protocols. To ensure consistency in the feature
dimension of nodes across all datasets, we utilize a pre-trained
Sentence-BERT to encode nodes of all types from each dataset
into a standardized dimension. For the supervised few-shot node
classification, we randomly select 1, 3, 5, 10, 20, 40, or 60 labeled
nodes per class as our training set. Additionally, we reserve 1,000
nodes for validation and another 1,000 nodes for testing purposes.
Our evaluation metrics encompass Micro-F1, Macro-F1, and AUC.
4.1.3 Compared Baseline Methods. For our comprehensive per-
formance comparison, we evaluate various state-of-the-art methods
from three different categories: i) The first category consists of repre-
sentative homogeneous graph neural networks, including SAGE [8]
andGAT [25]. ii) The second category includes approaches uti-
lizing message-passing mechanisms in heterogeneous graph neu-
ral networks. This category features models such as HAN [30],
HGT [9], and HetGNN [41]. iii) The third category focuses on
self-supervised techniques for heterogeneous graph learning. This
category incorporates generative strategies like HGMAE [24], as
well as contrastive schemes such as DMGI [18] and HeCo [31].
4.1.4 Implementation Details. In the implementation of our
HiGPT, we employ heterogeneous graph data with corresponding
 
2847HiGPT: Heterogeneous Graph Language Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
textual contents from IMDB and DBLP to conduct text-graph con-
trastive alignment and obtain a heterogeneous graph tokenizer. In
the first phase, we utilize a heterogeneous graph corpus consisting
of IMDB and DBLP for instruction tuning. We set the batch size to
1 per GPU and train for 1 epoch with the learning rate 2ğ‘’âˆ’5, the
warmup ratio 3ğ‘’âˆ’2and the weight decay 1ğ‘’âˆ’4. The projector ob-
tained from the first phase training is used as the initial state for the
second phase, where we set the epochs to 15 and further perform
instruction tuning on downstream tasks. The base model used in
both stages is vicuna-7B-v1.5, with the maximum context length set
to 2048. And For the evaluation of most baselines, we utilize their
publicly available code. We employ a grid-search strategy based
on default hyperparameter settings to ensure a fair evaluation. For
further details, please refer to our released source code.
4.2 Overall Performance Comparison (RQ1)
We performed node classification tasks on three datasets, explor-
ing both few-shot and zero-shot settings. In the few-shot settings,
our model was trained on the IMDB dataset with shot numbers
ranging from 1 to 60, and evaluated on the IMDB test set of 1,000
samples [ 24,31]. For the zero-shot settings, the model was trained
on the IMDB dataset with the same shot numbers, and tested on
separate test sets from the DBLP and ACM datasets, each containing
1,000 samples. To enable cross-dataset transferability in supervised
heterogeneous Graph Neural Networks (GNNs), we unified node
and edge categories, and utilized a classifier trained with transfer
data to accommodate variations in class quantities across datasets.
For self-supervised methods focused on learning embeddings
for downstream heterogeneous graph nodes, we excluded the zero-
shot settings. The overall performance is partially shown in Table 4.
"-std" and "-cot" notations represent the standard test prompt with
direct answers and the prompt with a Chain-of-Thought (CoT)
feature, respectively. These details provide insights into our node
classification experiments in both supervised and zero-shot settings.
Superiority of HiGPT in Few-Shot Settings. HiGPT outper-
forms state-of-the-art baselines consistently in supervised settings,
even with only one sample per class. The success can be attributed
to our effective instruction-tuning on a large-scale heterogeneous
graph corpus. This enables the LLM to extract valuable and trans-
ferable heterogeneous structural information from graph tokens,
resulting in a significant performance boost in downstream tasks.
Additionally, our proposed MoT graph instruction augmentation
method enhances the LLM with diverse mixed reasoning capabili-
ties without the need for additional supervision signals. As a result,
it effectively mitigates the data scarcity in few-shot scenarios.
Zero-shot Superiority of HiGPT. In zero-shot settings, our HiGPT
surpasses baselines with significant improvements. Unlike tradi-
tional models constrained by training graph types, our approach
leverages an in-context heterogeneous graph tokenizer. This tok-
enizer adapts tokenization based on the input graph, allowing the
LLM to seamlessly combine graph tokens that capture higher-order
structural features with its semantic understanding. As a result, our
model effectively overcomes the limitation of graph heterogeneity
shift, performing exceptionally well even in cross-domain scenarios.
This showcases the remarkable adaptability of our HiGPT.Effectiveness of Mixture-of-Thought Augmentation. Through
the implementation of the MoT approach, our model harnesses the
varied reasoning capabilities of the formidable LLM (specifically,
GPT-3.5) and seamlessly integrates them into our more compact
language model. This integration serves to bolster our modelâ€™s
ability to effectively navigate data scarcity and elevate its perfor-
mance in situations characterized by limited supervised signals.
The MoT technique assumes a pivotal role in generating dynamic
and diverse instructions, thereby offsetting the dearth of data and
empowering our model to make notably precise predictions across
both supervised and zero-shot settings.
4.3 Model Ablation Test (RQ2)
To evaluate the proposed modulesâ€™ effectiveness, we individually
remove the key techniques in HiGPT. The results are summarized
in Table 5. Here are the ablated variants and the key conclusions:
â€¢Effect of Heterogeneous Graph Instruction-Tuning. To vali-
date the effectiveness of instruction tuning in the tuning stage on
the large heterogeneous graph corpus, we generated the "w/o S1"
variant by directly tuning the instructions solely on the down-
stream task data. Through experiments with different epoch set-
tings (15, 50, and 100), we observed that models tuned solely on
the downstream task data failed to provide complete and accurate
answers in all cases. However, our HiGPT achieved state-of-the-
art performance within just 15 epochs across all settings. This
success can be attributed to the fact that our HiGPT learns from
a vast heterogeneous graph context corpus, enabling it to under-
stand and extract crucial structural information. As a result, in
the second stage, our HiGPT requires only a minimal amount of
supervised data (even in a 1-shot scenario) to quickly align with
the downstream task. Conversely, directly aligning LLMs with
sparse supervised data proves to be challenging.
â€¢Effect of In-Context Heterogeneous Graph Tokenizer. We
tested the necessity of incorporating heterogeneous graph struc-
tural information from our in-context tokenizer. By excluding
the introduction of heterogeneous graph tokens and solely train-
ing the LLMâ€™s embeddings weights on the downstream data,
we obtained a variant called "w/o HG". Our HiGPT consistently
outperformed this variant across different shot settings, espe-
cially in scenarios with limited samples (e.g., 1 or 3 shots). This
improvement is attributed to the introduction of graph tokens,
which enable the LLM to extract high-dimensional heterogeneous
structural information from the in-context graph tokenizer. This
enhanced understanding significantly improves the LLMâ€™s accu-
racy, particularly with sparse supervised signals.
â€¢Effect of MoT Instruction Augmentation. To verify the effec-
tiveness of the MoT graph instruction augmentation strategy, we
trained the variant "-IA" using only direct-answer instructions.
Results showed a significant drop in model performance without
instruction augmentation, highlighting its importance in tackling
the scarcity of labels in downstream tasks. Additionally, HiGPTâ€™s
superior performance in zero-shot settings can be attributed to
its enhanced reasoning ability, acquired through training with
diverse reasoning instructions. This improved capacity enables
effective cross-dataset and cross-domain transfer.
 
2848KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiabin Tang et al.
Table 4: Performance comparison on node classification tasks in both few-shot and zero-shot settings. However, since SSL
methods focus on learning embeddings from downstream graphs, we excluded the zero-shot settings for them ("-").
Datasets Metric train-on test-on SA
GE GAT HAN HGT HetGNN DMGI HGMAE HeCo HiGPT
-std HiGPT-cot
Sup
ervisedMi-F1IMDB-1 IMDB-1000 0.4663Â±0.0025
0.4567Â±0.0122 0.4890Â±0.0271 0.4977Â±0.0186 0.4790Â±0.0134 0.4570Â±0.0126 0.3609Â±0.0145 0.3874Â±0.0159 0.5090Â±0.0073 0.5360Â±0.0065
IMDB-5 IMDB-1000 0.5010Â±0.0051
0.5170Â±0.0029 0.4840Â±0.0094 0.5003Â±0.0093 0.5020Â±0.0045 0.4413Â±0.0173 0.3652Â±0.0062 0.3385Â±0.0169 0.6180Â±0.0027 0.6320Â±0.0085
IMDB-20 IMDB-1000 0.5930Â±0.0093
0.6117Â±0.0012 0.5763Â±0.0046 0.5750Â±0.0065 0.5957Â±0.0054 0.5497Â±0.0256 0.4107Â±0.0106 0.3781Â±0.0148 0.6090Â±0.0255 0.6440Â±0.0075
IMDB-40 IMDB-1000 0.6170Â±0.0112
0.6261Â±0.0015 0.6198Â±0.0025 0.5923Â±0.0040 0.6177Â±0.0046 0.5813Â±0.0033 0.3946Â±0.0067 0.3927Â±0.0134 0.6260Â±0.0057 0.6280Â±0.0071
Ma-F1IMDB-1 IMDB-1000 0.4425Â±0.0068
0.3974Â±0.0183 0.4229Â±0.0104 0.4020Â±0.0112 0.4456Â±0.0036 0.4083Â±0.0288 0.3573Â±0.0117 0.4023Â±0.0137 0.4986Â±0.0141 0.5247Â±0.0061
IMDB-5 IMDB-1000 0.4613Â±0.0086
0.4767Â±0.0098 0.4695Â±0.0037 0.4676Â±0.0153 0.4677Â±0.0145 0.4254Â±0.0124 0.3500Â±0.0080 0.3468Â±0.0213 0.6111Â±0.0091 0.6243Â±0.0060
IMDB-20 IMDB-1000 0.5953Â±0.0095
0.6121Â±0.0024 0.5756Â±0.0051 0.5723Â±0.0056 0.5969Â±0.0055 0.5495Â±0.0270 0.4065Â±0.0089 0.3904Â±0.0172 0.6068Â±0.0146 0.6398Â±0.0083
IMDB-40 IMDB-1000 0.6182Â±0.0107
0.6254Â±0.0009 0.6224Â±0.0057 0.5909Â±0.0068 0.6234Â±0.0038 0.5786Â±0.0064 0.3866Â±0.0072 0.3988Â±0.0147 0.6265Â±0.0090 0.6237Â±0.0059
AUCIMDB-1 IMDB-1000 0.6079Â±0.0061
0.6151Â±0.0065 0.6234Â±0.0252 0.6249Â±0.0170 0.6107Â±0.0075 0.5780Â±0.0130 0.5274Â±0.0058 0.5712Â±0.0099 0.6565Â±0.0146 0.6685Â±0.0037
IMDB-5 IMDB-1000 0.6309Â±0.0049
0.6372Â±0.0012 0.6102Â±0.0059 0.6197Â±0.0152 0.6290Â±0.0022 0.5832Â±0.0132 0.5262Â±0.0041 0.5067Â±0.0228 0.7308Â±0.0125 0.7310Â±0.0086
IMDB-20 IMDB-1000 0.6976Â±0.0064
0.7122Â±0.0020 0.6815Â±0.0052 0.6801Â±0.0048 0.7005Â±0.0030 0.6657Â±0.0179 0.5766Â±0.0064 0.5541Â±0.0145 0.7227Â±0.0034 0.7424Â±0.0113
IMDB-40 IMDB-1000 0.7171Â±0.0069
0.7210Â±0.0014 0.7204Â±0.0015 0.6970Â±0.0060 0.7145Â±0.0035 0.6860Â±0.0027 0.5488Â±0.0049 0.5653Â±0.0105 0.7323Â±0.0036 0.7331Â±0.0074
Zer
o-shotMi-F1IMDB-1 DBLP-1000 0.2353Â±0.0372
0.1893Â±0.0373 0.2653Â±0.0203 0.2573Â±0.0519 0.2900Â±0.0638 - - - 0.3180Â±0.0072 0.3500Â±0.0073
IMDB-5 DBLP-1000 0.2607Â±0.0082
0.2737Â±0.0176 0.2577Â±0.0094 0.2453Â±0.0458 0.2427Â±0.0452 - - - 0.3180Â±0.0044 0.3620Â±0.0047
IMDB-20 DBLP-1000 0.2810Â±0.0289
0.2780Â±0.0033 0.2710Â±0.0000 0.2803Â±0.0208 0.2333Â±0.0353 - - - 0.3840Â±0.0088 0.4180Â±0.0083
IMDB-40 DBLP-1000 0.2400Â±0.0324
0.2847Â±0.0053 0.2710Â±0.0000 0.2937Â±0.0005 0.2027Â±0.0345 - - - 0.3320Â±0.0087 0.3630Â±0.0045
Ma-F1IMDB-1 DBLP-1000 0.0963Â±0.0132
0.1169Â±0.0089 0.1047Â±0.0063 0.1016Â±0.0169 0.1778Â±0.0629 - - - 0.2048Â±0.0068 0.2472Â±0.0070
IMDB-5 DBLP-1000 0.1042Â±0.0028
0.1291Â±0.0145 0.1024Â±0.0030 0.1138Â±0.0296 0.0971Â±0.0148 - - - 0.1917Â±0.0046 0.2773Â±0.0085
IMDB-20 DBLP-1000 0.1448Â±0.0573
0.1274Â±0.0060 0.1066Â±0.0000 0.1143Â±0.0116 0.1008Â±0.0191 - - - 0.3142Â±0.0074 0.3733Â±0.0051
IMDB-40 DBLP-1000 0.1068Â±0.0060
0.1588Â±0.0078 0.1066Â±0.0000 0.1268Â±0.0105 0.0984Â±0.0161 - - - 0.2331Â±0.0069 0.2912Â±0.0056
AUCIMDB-1 DBLP-1000 0.4999Â±0.0001
0.4513Â±0.0295 0.5000Â±0.0000 0.5000Â±0.0000 0.5206Â±0.0306 - - - 0.5222Â±0.0069 0.5406Â±0.0040
IMDB-5 DBLP-1000 0.4978Â±0.0030
0.4908Â±0.0078 0.5000Â±0.0000 0.5031Â±0.0043 0.4998Â±0.0003 - - - 0.5184Â±0.0081 0.5493Â±0.0091
IMDB-20 DBLP-1000 0.5154Â±0.0213
0.4918Â±0.0020 0.5000Â±0.0000 0.5011Â±0.0016 0.4957Â±0.0060 - - - 0.5669Â±0.0041 0.5907Â±0.0089
IMDB-40 DBLP-1000 0.5027Â±0.0031
0.4976Â±0.0021 0.5000Â±0.0000 0.5008Â±0.0006 0.4884Â±0.0164 - - - 0.5296Â±0.0070 0.5508Â±0.0086
Mi-F1IMDB-1 A
CM-1000 0.3293Â±0.0418
0.3567Â±0.0053 0.3407Â±0.0111 0.3240Â±0.0014 0.3743Â±0.0434 - - - 0.4160Â±0.0106 0.4540Â±0.0089
IMDB-5 A
CM-1000 0.3820Â±0.0113
0.3787Â±0.0057 0.3630Â±0.0086 0.3160Â±0.0169 0.3583Â±0.0198 - - - 0.4580Â±0.0173 0.4880Â±0.0131
IMDB-20 A
CM-1000 0.2807Â±0.0074
0.3013Â±0.0188 0.3133Â±0.0031 0.3530Â±0.0000 0.2840Â±0.0226 - - - 0.5080Â±0.0129 0.5030Â±0.0064
IMDB-40 A
CM-1000 0.3173Â±0.0005
0.2393Â±0.0144 0.2697Â±0.0194 0.3560Â±0.0099 0.3180Â±0.0016 - - - 0.4750Â±0.0149 0.5050Â±0.0077
Ma-F1IMDB-1 A
CM-1000 0.2647Â±0.0269
0.2908Â±0.0131 0.2250Â±0.0416 0.1631Â±0.0005 0.3139Â±0.0468 - - - 0.3949Â±0.0078 0.4177Â±0.0124
IMDB-5 A
CM-1000 0.3208Â±0.0130
0.3009Â±0.0137 0.2782Â±0.0026 0.1969Â±0.0301 0.3087Â±0.0225 - - - 0.4336Â±0.0085 0.4510Â±0.0114
IMDB-20 A
CM-1000 0.2694Â±0.0091
0.2422Â±0.0098 0.2412Â±0.0050 0.2094Â±0.0501 0.2715Â±0.0181 - - - 0.4964Â±0.0075 0.4877Â±0.0070
IMDB-40 A
CM-1000 0.3117Â±0.0017
0.2141Â±0.0071 0.2313Â±0.0132 0.2749Â±0.0122 0.3144Â±0.0017 - - - 0.4176Â±0.0116 0.4585Â±0.0089
AUCIMDB-1 A
CM-1000 0.4934Â±0.0247
0.5248Â±0.0038 0.5128Â±0.0086 0.5000Â±0.0000 0.5318Â±0.0295 - - - 0.5672Â±0.0040 0.5969Â±0.0082
IMDB-5 A
CM-1000 0.5433Â±0.0082
0.5415Â±0.0047 0.5282Â±0.0073 0.4950Â±0.0134 0.5256Â±0.0145 - - - 0.5991Â±0.0103 0.6224Â±0.0054
IMDB-20 A
CM-1000 0.4601Â±0.0048
0.4772Â±0.0137 0.4877Â±0.0029 0.5038Â±0.0053 0.4625Â±0.0163q - - - 0.6352Â±0.0094 0.6318Â±0.0068
IMDB-40 A
CM-1000 0.4867Â±0.0013
0.4320Â±0.0108 0.4545Â±0.0146 0.5148Â±0.0043 0.4872Â±0.0006 - - - 0.6138Â±0.0047 0.6360Â±0.0051
Table 5: Ablation study of our HiGPT.
Datasets Metric train-on test-on w/o
S1 w/o HG w/o IA HiGPT
Sup
ervisedMi-F1IMDB-1 IMDB-1000 fail
0.3740 0.4260 0.5360
IMDB-3 IMDB-1000 fail
0.5000 0.4540 0.5730
IMDB-10 IMDB-1000 fail
0.5660 0.4380 0.5810
IMDB-20 IMDB-1000 fail
0.5640 0.5620 0.6440
Ma-F1IMDB-1 IMDB-1000 fail
0.2433 0.3978 0.5247
IMDB-3 IMDB-1000 fail
0.4969 0.4289 0.5591
IMDB-10 IMDB-1000 fail
0.5619 0.3966 0.5762
IMDB-20 IMDB-1000 fail
0.5636 0.5364 0.6398
AUCIMDB-1 IMDB-1000 fail
0.5195 0.6023 0.6685
IMDB-3 IMDB-1000 fail
0.6340 0.6186 0.6935
IMDB-10 IMDB-1000 fail
0.6790 0.5903 0.6875
IMDB-20 IMDB-1000 fail
0.6891 0.6840 0.7424
Zer
o-shotMi-F1IMDB-1 DBLP-1000 fail
0.2980 0.2800 0.3500
IMDB-3 DBLP-1000 fail
0.3430 0.3180 0.3660
IMDB-10 DBLP-1000 fail
0.3640 0.3140 0.4020
IMDB-20 DBLP-1000 fail
0.3920 0.2800 0.4180
Ma-F1IMDB-1 DBLP-1000 fail
0.2444 0.2145 0.2472
IMDB-3 DBLP-1000 fail
0.2768 0.2503 0.2814
IMDB-10 DBLP-1000 fail
0.3211 0.2581 0.3386
IMDB-20 DBLP-1000 fail
0.3689 0.1836 0.3733
AUCIMDB-1 DBLP-1000 fail
0.5275 0.5035 0.5406
IMDB-3 DBLP-1000 fail
0.5422 0.5286 0.5524
IMDB-10 DBLP-1000 fail
0.5636 0.5269 0.5777
IMDB-20 DBLP-1000 fail
0.5834 0.4995 0.5907
4.4 Graph In-Context Learning (RQ3)
In-context learning (ICL)[ 17] is a method for adapting large lan-
guage models (LLMs) to new tasks without gradient updates, using
a prompt with task examples. In this subsection, we explore the
impact of Graph In-Context Learning on improving HiGPTâ€™s per-
formance. We conduct comprehensive tests by adding prefatory
examples from the training set to models trained with different
shots of IMDB data. We randomly sampled training examples cor-
responding to the test data. "-ICL-1" and "-ICL-2" denote one andtwo prefatory examples, respectively. "-ICL-DBLP" signifies the in-
clusion of DBLP examples before the ACM test prompt. The results,
depicted in Figure3, reveal the following observations:
1-shot Beat 60-shot with Graph ICL in HiGPT. Results show
that, even with just a single example, most 1-shot models using
Graph ICL consistently outperform 60-shot models without further
training in both supervised and zero-shot settings. Increasing the
number of examples enhances the effect of in-context learning. This
improvement can be attributed to HiGPTâ€™s two-stage instruction
tuning process, which enables it to understand and analyze hetero-
geneous graph tokens, benefiting downstream tasks. By providing
question-and-answer examples with graph tokens, the model gains
a deeper understanding of the graph-text relationship. Analyzing
and emulating these examples leads to more accurate responses.
Enhanced Transferability with our Graph ICL. The advantages
of our Graph ICL in HiGPT are particularly evident in zero-shot
transfer scenarios. This indicates that the Graph ICL approach
significantly improves HiGPTâ€™s transferability without the need to
optimize model parameters. Our HiGPT does not simply overfit to
a single dataset but develops the ability to analyze text alongside
heterogeneous graph tokens. By incorporating graph examples
from other datasets, the model effortlessly transfers this analytical
capability, demonstrating strong transfer learning capacity.
Benefit of Irrelevant Graph Examples. We experimented with
HiGPT with irrelevant graph examples, like using DBLP Q&A ex-
amples for testing on the ACM dataset. Surprisingly, using DBLP
graph examples yielded the best results. Despite different target
tasks, our HiGPT effectively leverages in-context information from
heterogeneous graph tokens, enhancing downstream tasks. This
 
2849HiGPT: Heterogeneous Graph Language Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
1 3 5 10 20 40 60
Number of Training Shots on IMDB50%55%60%65%70%T esting Mi-F1 on IMDB0.655 0.6530.6460.654 0.6550.6640.658Ours Ours-ICL-1 Ours-ICL-2
1 3 5 10 20 40 60
Number of Training Shots on IMDB50%55%60%65%70%T esting Ma-F1 on IMDB0.650 0.6480.6410.6490.6530.6610.654Ours Ours-ICL-1 Ours-ICL-2
(a)IMDB-IMDB@Mi-F1, Ma-F1
1 3 5 10 20 40 60
Number of Training Shots on IMDB35%40%45%50%55%T esting Mi-F1 on DBLP0.3870.4690.4750.542
0.5100.5330.540Ours Ours-ICL-1 Ours-ICL-2
1 3 5 10 20 40 60
Number of Training Shots on IMDB35%40%45%50%55%T esting Ma-F1 on DBLP0.3740.4610.4670.535
0.5050.5280.539Ours Ours-ICL-1 Ours-ICL-2
(b)IMDB-DBLP@Mi-F1, Ma-F1
1 3 5 10 20 40 60
Number of Training Shots on IMDB50%55%60%65%70%T esting Mi-F1 on ACM0.6340.6270.679
0.620
0.5170.5780.634Ours Ours-ICL-1 Ours-ICL-DBLP
1 3 5 10 20 40 60
Number of Training Shots on IMDB50%55%60%65%70%T esting Ma-F1 on ACM0.631 0.6270.679
0.619
0.5050.5700.629Ours Ours-ICL-1 Ours-ICL-DBLP
(c)IMDB-ACM@Mi-F1, Ma-F1
Figure 3: Graph In-Context Learning of our HiGPT.
confirms that our HiGPT learns valuable information from hetero-
geneous graph structures, rather than relying solely on text. Using
ACMâ€™s own examples did not perform as well due to a deficiency
in encoding the ACM graph in the alignment and stage 1 process.
However, the DBLP examples mitigated this issue to some extent.
4.5 Case Study
We perform a case study to showcase our HiGPTâ€™s robust gener-
alization in understanding complex graph structures with diverse
nodes and connections. Our model generates graph-aware predic-
tions and responses, demonstrating its profound comprehension
and awareness of graph-related aspects. Furthermore, we validate
the positive impact of our MoT instruction augmentation. For more
comprehensive details, please refer to Appendix Section A.4.1.
5 RELATED WORK
Heterogeneous Graph Neural Networks. Heterogeneous Graph
Neural Networks (HGNNs) capture complex relationships and di-
verse semantics among entities in a heterogeneous graph [ 5,41].
They use specialized message functions and aggregation rules to
model relation heterogeneity. Existing models, such as MAGNN
[5] and HetGNN [ 41], leverage metapaths to capture composite
relations and guide neighbor selection. Heterogeneous graph con-
volution frameworks like HeteGCN [ 20] and MHGCN [ 40] draw
inspiration from graph convolutional networks. Heterogeneous
graph attention networks, including HAN [ 29], HGT [ 9], and HGAT[13], use attention mechanisms to effectively capture and aggre-
gate information from different node types. However, most HGNNs
require sufficient labels to learn accurate graph representations.
Heterogeneous Graph Self-Supervised Learning. To address
the limited availability of labeled data, recent research has focused
on incorporating self-supervised learning techniques into the mod-
eling of heterogeneous graphs [ 12,27]. In this direction, heteroge-
neous graph models use the labels that are automatically generated
from the input graphs for self-supervision. Notably, contrastive
and generative approaches for heterogeneous graphs have proven
effective in augmenting data. Specifically, contrastive learning ap-
proaches like DMGI [ 18] and HeCo [ 31] employ self-supervised data
augmentation by bringing similar instances (positive pairs) closer
together and pushing dissimilar instances (negative pairs) apart in a
latent space. This encourages the model to capture relevant patterns
and structure in the data. On the other hand, generative learning
approaches for heterogeneous graphs, such as HGMAE [ 24], uti-
lize a masked autoencoder to encode complex heterogeneity-aware
information for the purpose of reconstructing the heterogeneous
graph. While these works propose leveraging self-supervised learn-
ing for data augmentation, their inherent model design still presents
limitations in handling the relation heterogeneity shift across down-
stream tasks. Thus, they may exhibit poor generalization ability.
Large Language Models for Graph Data. Recent research has
focused on combining large language models (LLMs) and graph
models to enhance our understanding of complex relationships
and structures within graph data. Currently, there are two primary
approaches to integrating graph structural information into LLMs:
utilizing textual prompts and incorporating graph embeddings as in-
put tokens. Leading researchers, such as Chen et al. [ 2], have made
significant contributions by developing a diverse range of care-
fully crafted textual prompts specifically tailored for graph learning
tasks. In contrast, InstructGLM [ 39] proposes an alternative tech-
nique that combines prompt instructions with node embeddings.
Moreover, recent advancements have introduced LLMs to improve
the reasoning capabilities of models when working with graph-
structured data. Prominent examples include the works of Fatemi
et al.[ 6] and Chai et al.[ 1]. However, existing LLM-enhanced graph
models have primarily focused on homogeneous graphs, overlook-
ing the inherent heterogeneity in real-world graphs. This calls for
further exploration of heterogeneous graph language models with
strong generalization abilities across diverse downstream tasks.
6 CONCLUSION
This work introduce HiGPT, a general and versatile graph model
that offers the ability to learn from diverse heterogeneous graphs
without the need for downstream fine-tuning processes. To ad-
dress distribution shifts in heterogeneity, we propose an in-context
heterogeneous graph tokenizer that captures semantic relation-
ships across different heterogeneous graphs, facilitating seamless
model adaptation. By incorporating the heterogeneity-aware graph
instructions into our HiGPT, the model becomes proficient in com-
prehending intricate relation heterogeneity and accurately discern-
ing between various types of graph tokens. Our proposed frame-
work has undergone extensive evaluations across diverse scenarios,
demonstrating outstanding generalization performance.
 
2850KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiabin Tang et al.
REFERENCES
[1]Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang. Graphllm:
Boosting graph reasoning ability of large language model. arXiv preprint
arXiv:2310.05845, 2023.
[2]Z. Chen, H. Mao, H. Li, et al. Exploring the potential of large language models
(llms) in learning on graphs. CoRR, abs/2307.03393, 2023.
[3]Y. Dong, N. V. Chawla, and A. Swami. metapath2vec: Scalable representation
learning for heterogeneous networks. In KDD, pages 135â€“144, 2017.
[4]A. El-Kishky, T. Markovich, S. Park, C. Verma, B. Kim, R. Eskander, Y. Malkov,
F. Portman, S. Samaniego, Y. Xiao, et al. Twhin: Embedding the twitter heteroge-
neous information network for personalized recommendation. In KDD, pages
2842â€“2850, 2022.
[5]S. Fan, J. Zhu, X. Han, C. Shi, L. Hu, B. Ma, and Y. Li. Metapath-guided het-
erogeneous graph neural network for intent recommendation. In KDD, pages
2478â€“2486, 2019.
[6]B. Fatemi, J. Halcrow, and B. Perozzi. Talk like a graph: Encoding graphs for
large language models. arXiv preprint arXiv:2310.04560, 2023.
[7]X. Fu, J. Zhang, Z. Meng, and I. King. MAGNN: metapath aggregated graph
neural network for heterogeneous graph embedding. In WWW, pages 2331â€“2341.
ACM / IW3C2, 2020.
[8]W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on
large graphs. In NeurIPS, pages 1024â€“1034, 2017.
[9]Z. Hu, Y. Dong, K. Wang, and Y. Sun. Heterogeneous graph transformer. In
WWW, pages 2704â€“2710. ACM / IW3C2, 2020.
[10] D. Hwang, J. Park, S. Kwon, K. Kim, J.-W. Ha, and H. J. Kim. Self-supervised
auxiliary learning with meta-paths for heterogeneous graphs. NeurIPS, 33:10294â€“
10305, 2020.
[11] D. Jin, C. Huo, C. Liang, and L. Yang. Heterogeneous graph neural network via
attribute completion. In WWW, pages 391â€“400, 2021.
[12] B. Jing, S. Feng, Y. Xiang, X. Chen, Y. Chen, and H. Tong. X-goal: multiplex
heterogeneous graph prototypical contrastive learning. In CIKM, pages 894â€“904,
2022.
[13] H. Linmei, T. Yang, C. Shi, H. Ji, and X. Li. Heterogeneous graph attention
networks for semi-supervised short text classification. In EMNLP, pages 4821â€“
4830, 2019.
[14] J. Liu, A. Liu, X. Lu, S. Welleck, P. West, R. L. Bras, Y. Choi, and H. Hajishirzi.
Generated knowledge prompting for commonsense reasoning. In ACL (1), pages
3154â€“3169. Association for Computational Linguistics, 2022.
[15] Q. Lv, M. Ding, Q. Liu, Y. Chen, W. Feng, S. He, C. Zhou, J. Jiang, Y. Dong, and
J. Tang. Are we really making much progress? revisiting, benchmarking and
refining heterogeneous graph neural networks. In KDD, pages 1150â€“1160, 2021.
[16] A. Ma, X. Wang, J. Li, C. Wang, T. Xiao, Y. Liu, H. Cheng, J. Wang, Y. Li, Y. Chang,
et al. Single-cell biological network inference using a heterogeneous graph
transformer. Nature Communications, 14(1):964, 2023.
[17] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettle-
moyer. Rethinking the role of demonstrations: What makes in-context learning
work? In EMNLP, pages 11048â€“11064, 2022.
[18] C. Park, D. Kim, J. Han, and H. Yu. Unsupervised attributed multiplex network
embedding. In AAAI, volume 34, pages 5371â€“5378, 2020.
[19] A. Radford, J. W. Kim, C. Hallacy, et al. Learning transferable visual models from
natural language supervision. In International Conference on Machine Learning
(ICML), pages 8748â€“8763. PMLR, 2021.[20] R. Ragesh, S. Sellamanickam, A. Iyer, R. Bairi, and V. Lingam. Hetegcn: hetero-
geneous graph convolutional networks for text classification. In WSDM, pages
860â€“868, 2021.
[21] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese
bert-networks. In EMNLP. Association for Computational Linguistics, 11 2019.
[22] K. Shridhar, A. Stolfo, and M. Sachan. Distilling reasoning capabilities into smaller
language models. In ACL, pages 7059â€“7073, 2023.
[23] H. Sun, A. HÃ¼yÃ¼k, and M. van der Schaar. Query-dependent prompt evaluation
and optimization with offline inverse rl. arXiv e-prints, pages arXivâ€“2309, 2023.
[24] Y. Tian, K. Dong, C. Zhang, C. Zhang, and N. V. Chawla. Heterogeneous graph
masked autoencoders. In AAAI, volume 37, pages 9997â€“10005, 2023.
[25] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, et al. Graph attention
networks. In ICLR (Poster). OpenReview.net, 2018.
[26] H. Wang, H. Ren, and J. Leskovec. Relational message passing for knowledge
graph completion. In KDD, pages 1697â€“1707, 2021.
[27] P. Wang, K. Agarwal, C. Ham, S. Choudhury, and C. K. Reddy. Self-supervised
learning of contextual embeddings for link prediction in heterogeneous networks.
InWWW, pages 2946â€“2957, 2021.
[28] X. Wang, D. Bo, C. Shi, S. Fan, Y. Ye, and S. Y. Philip. A survey on heterogeneous
graph embedding: methods, techniques, applications and sources. Transactions
on Big Data (TBD), 9(2):415â€“436, 2022.
[29] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu. Heterogeneous graph
attention network. In WWW, pages 2022â€“2032, 2019.
[30] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, et al. Heterogeneous graph attention
network. In WWW, pages 2022â€“2032. ACM, 2019.
[31] X. Wang, N. Liu, H. Han, and C. Shi. Self-supervised heterogeneous graph neural
network with co-contrastive learning. In KDD, pages 1726â€“1736, 2021.
[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le,
and D. Zhou. Chain-of-thought prompting elicits reasoning in large language
models. In NeurIPS, 2022.
[33] W. Wei, C. Huang, L. Xia, Y. Xu, J. Zhao, and D. Yin. Contrastive meta learning
with behavior multiplicity for recommendation. In WSDM, pages 1120â€“1128,
2022.
[34] Z. Wen and Y. Fang. Augmenting low-resource text classification with graph-
grounded pre-training and prompting. In SIGIR, 2023.
[35] H. Xuan, Y. Liu, B. Li, and H. Yin. Knowledge enhancement for contrastive
multi-behavior recommendation. In WSDM, pages 195â€“203, 2023.
[36] C. Yang, Y. Xiao, Y. Zhang, Y. Sun, and J. Han. Heterogeneous network represen-
tation learning: A unified framework with survey and benchmark. Transactions
on Knowledge and Data Engineering (TKDE), 34(10):4854â€“4873, 2020.
[37] Y. Yang, Z. Guan, Z. Wang, W. Zhao, C. Xu, W. Lu, and J. Huang. Self-supervised
heterogeneous graph pre-training based on structural clustering. NeurIPS,
35:16962â€“16974, 2022.
[38] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree
of thoughts: Deliberate problem solving with large language models. CoRR,
abs/2305.10601, 2023.
[39] R. Ye, C. Zhang, R. Wang, S. Xu, and Y. Zhang. Natural language is all a graph
needs. arXiv preprint arXiv:2308.07134, 2023.
[40] P. Yu, C. Fu, Y. Yu, C. Huang, Z. Zhao, and J. Dong. Multiplex heterogeneous
graph convolutional network. In KDD, pages 2377â€“2387, 2022.
[41] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla. Heterogeneous graph
neural network. In KDD, pages 793â€“803. ACM, 2019.
[42] J. Zhao, X. Wang, C. Shi, B. Hu, G. Song, and Y. Ye. Heterogeneous graph structure
learning for graph neural networks. In AAAI, volume 35, pages 4697â€“4705, 2021.
 
2851HiGPT: Heterogeneous Graph Language Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A APPENDIX
A.1 Detailed Descriptions of Baselines
To conduct a thorough evaluation, our baseline set includes the
following methods, which are presented below:
1) Homogeneous Graph Neural Networks
â€¢SAGE [8]:It was developed to facilitate the learning of inductive
representations on large-scale homogeneous graphs, allowing for
the generation of highly effective node embeddings for unseen
data. However, we made adaptations and modifications to tailor
it specifically to the unique demands of heterogeneous graphs.
â€¢GAT [25]: It utilizes graph attention networks, which overcome
the limitations of graph convolutional networks (GCN) by in-
corporating masked self-attention layers. What sets this method
apart is its ability to selectively weigh the aggregated information
from different nodes, thereby enhancing the message passing
mechanism and refining the overall process.
2) Heterogeneous Graph Neural Encoders.
â€¢HAN [30]: It is a heterogeneous graph neural encoder that incor-
porates hierarchical attention mechanisms, including node-level
and semantic-level attentions, to address the complexity of het-
erogeneous graphs with various types of nodes and links, thereby
improving the representation and interpretability of node embed-
dings through feature aggregation based on meta-path neighbors.
â€¢HGT [9]: HGT is an advanced graph neural network framework
designed to model the complexities of large-scale heterogeneous
graphs, featuring type-dependent parameters for nodes and edges
to enable heterogeneous attention, a relative temporal encoding
to capture dynamic relationships, and an efficient graph sampling
algorithm for scalable training.
â€¢HetGNN [41]: HetGNN is a powerful heterogeneous graph neu-
ral network model that seamlessly integrates both the structural
information and diverse content attributes of nodes. It achieves
this by employing a two-module architecture for feature aggre-
gation and incorporating a well-designed random walk sampling.
This comprehensive approach enables HetGNN to generate mean-
ingful and informative node embeddings.
3) SSL-enhanced Heterogeneous Graph Learning Approaches.
â€¢HGMAE [24]: HGMAE is a generative SSL approach that ad-
dresses the challenges of capturing complex structures, incor-
porating diverse node attributes, and encoding node positions
in heterogeneous graphs. It achieves this through innovative
masking techniques and tailored training strategies, utilizing a
heterogeneous graph masked autoencoder. HGMAE efficiently
learns to generate meaningful representations while effectively
preserving the rich information present in heterogeneous graphs.
â€¢DMGI [18]: It is an effective unsupervised network embedding
method for attributed multiplex networks that maximizes mutual
information between graph patches and a global representation,
integrating multiple relation-type embeddings with a consensus
regularization framework and a universal discriminator, further
enhanced by an attention mechanism to weigh relation types.
â€¢HeCo [31]: This is a self-supervised heterogeneous graph neu-
ral network framework that employs a co-contrastive learningmechanism across two views (network schema and meta-path) to
capture both local and high-order structures, with a view mask
mechanism for effective cross-view supervision, enhanced by
extensions for generating higher-quality negative samples.
A.2 Detailed Descriptions of Text-Enriched
Heterogeneity Representations
We present diverse descriptions for different node and edge types
in Table 6 for IMDB as an example.
A.3 Instruction construction template for graph
in-context learning
In Figure 4, we illustrate the construction of instructions for our
graph in-context learning tests, which includes "-ICL-1" with one
example, "-ICL-2" with two examples, and "-ICL-DBLP" where ex-
amples from ACM are concatenated with DBLP examples. We repre-
sent the examples and the final question using a Q:...A:...Q:...
sequence.
IMDB-ICL-1:Q:Given a heterogeneous graph about internet movieâ€¦ {Human Question}A:{Ground Truth Answer&Reasoning}Q:Given a heterogeneous graph about internet movieâ€¦ {Human Question}IMDB-ICL-2:Q:Given a heterogeneous graph about internet movieâ€¦ {Human Question}A:{Ground Truth Answer&Reasoning}Q:Given a heterogeneous graph about internet movieâ€¦ {Human Question}A:{Ground Truth Answer&Reasoning}Q:Given a heterogeneous graph about internet movieâ€¦ {Human Question}ACM-ICL-DBLP:Q:Given a heterogeneous academic network graph about computer science from DBLPwebsite â€¦ {Human Question}A:{Ground Truth Answer&Reasoning}Q:Given a heterogeneous academic network graph about computer science collected from ACMwebsiteâ€¦ {Human Question}
Figure 4: Instruction construction template for graph in-
context learning, including "ICL-1", "ICL-2" and "ICL-DBLP".
A.4 Supplementary Experimental Results
A.4.1 Model Case Study. In this subsection, we explore the be-
havior of our HiGPT under different prompting techniques. Specifi-
cally, we utilize various prompting techniques to prompt the 10-shot
IMDB model, obtaining six different responses, and the prediction
cases for different categories of HiGPT are shown in 7, respectively.
The parts showing the final answers are highlighted in pink. We
make the following observations: Obs.1 Our HiGPT, after instruc-
tion tuning with the MoT graph instruction augmentation strategy,
can dynamically respond accurately to different prompts. Obs.2 The
CoT prompt in Table 7, which is unformatted, also shows a certain
format (highlighted in yellow), which is attributed to the fact that
mixing a variety of instructions can also benefit different prompting
techniques. Obs.3 As highlighted in green in multiple cases, our
HiGPT, after our designed two-stage graph instruction tuning, is
consciously considering issues from a graph perspective, further
proving that our model is not only solving downstream problems
from a textual perspective but has also developed a certain level of
graph-awareness.
 
2852KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiabin Tang et al.
Table 6: Detailed Descriptions of Text-Enriched Heterogeneity Representations for IMDB and DBLP.
No
de (Edge) Type Sets of Descriptions
(
a) IMDB
"Mo
vie" {"This node represents a movie","This is an action movie","This is a comedy movie","This is a drama movie"}
"Director" {"This node represents a director", "This is an action film director", "The director specializes in action", "This is a comedy
film director", "The director specializes in comedy", "This is a drama film director", "The director specializes in dram"}
"Actor" {"This node represents an actor", "This is an action film actor", "The actor specializes in action", "This is a comedy film
actor", "The actor specializes in comedy", "This is a drama film actor", "The actor specializes in drama"}
("movie", "to", "director") {"The movie is directed by the director", "The film features direction by the director", "The movieâ€™s direction was in the
hands of the director", "The movie was helmed by the director", "The film is a directorial effort by the director", "The
movie bears the directorial signature of the director"}
("movie", "to", "actor") {"The movie has the actor", "The movie features the actor", "The film includes the actor in its lineup", "The movie
showcases the talent of the actor", "The filmâ€™s cast includes the actor", "The movie presents the actor on its roster"}
("director", "to", "movie") {"This director is responsible for the filmâ€™s direction", "The director take the helm for the movie", "The director steers
the production of the movie", "The director in question crafts the narrative of the film", "The director provides the
creative direction for the film", "The director orchestrates the making of the movie"}
("actor", "to", "movie") {"The actor appears in the movie", "The actor is part of the movieâ€™s cast", "The actor stars in the movie", "The actor is
featured in the film", "The actor has a role in the movie"}
Table 7: Visualization of our HiGPTâ€™s response with different prompting engineering techniques on IMDB for drama genre.
Question: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths)
between different nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the
target movie node, a heterogeneous subgraph is obtained. In the subgraph, "movie" nodes: <graph>, where the 0-th node is the central node that represents a
movie with the following information: Name: The Life of David Gale; Directorâ€™s name: Alan Parker; Actorsâ€™ names: Kevin Spacey|Kate Winslet|Matt Craven;
Plot keywords: activist|death|death row|murder|texas"actor" nodes: <graph>; "director" nodes: <graph>. Question: Which of the following classes does this
movie belong to: action, comedy, drama?
Ground-Truth Category: Drama
+ Give likely categories directly. HiGPT (Direct Response): Drama.
+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.
HiGPT (CoT Response without Format Constraint): Based on the plot keywords provided, the movie "The Life of David Gale" belongs to the drama
genre. The plot keywords include "activist," "death," "death row," "murder," and "texas," which suggest a serious and emotional storyline. These keywords are
commonly associated with drama films, rather than action or comedy films. There fore, themovie "The LifeofDavid Gale" belongs tothedrama genre.
+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning. Using the following
format: Answer: [The answer] Reason: ...
HiGPT (CoT Response with Format Constraint): Answer: Drama Reason: The movie "The Life of David Gale" is a drama, as it deals with serious and
emotional themes such as activism, death row, and murder. The plot keywords also suggest a dramatic tone.
+ Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all
experts will go on to the next step, etc. If any expert realizes theyâ€™re wrong at any point then they leave.
HiGPT (ToT Response): Expert 1: Based on the plot keywords, this movie seems to be a drama. Expert 2: I agree with Expert 1. The plot keywords such as
"activist," "death," "death row," and "murder" suggest a serious and emotional theme. Expert 3: I also agree with Expert 1 and 2. The plot keywords and the fact
that the movie is directed by Alan Parker, who is known for his dramatic films, ... Finalanswer: Themovie belongs tothedrama class.
+ 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penalty
HiGPT (Panel Discussion Response): 1. Expert 1: Letâ€™s start by analyzing the plot keywords. We have "activist," "death," "death row," "murder," and "texas."
Based on these keywords, I would say this movie belongs to the drama genre. 2. Expert 2: I agree with Expert 1. The plot keywords suggest a serious and
emotional storyline. However, we also have "actor" nodes in the graph. 3. Expert 3: I would like to add that we should also consider the directorâ€™s name, Alan
Parker. He has directed both dramatic and comedic movies. 4. Panel Discussion: Based on the analysis so far, wecanconclude that themovie "The Lifeof
David Gale" belongs tothedrama genre, but it may alsohave comedic elements duetothepresence ofactornodes inthegraph.
 
2853