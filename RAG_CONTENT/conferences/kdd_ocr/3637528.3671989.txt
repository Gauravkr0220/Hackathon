Scalable Rule Lists Learning with Sampling
Leonardo Pellegrina
Dept. of Information Engineering, University of Padova
Padova, Italy
leonardo.pellegrina@unipd.itFabio Vandin
Dept. of Information Engineering, University of Padova
Padova, Italy
fabio.vandin@unipd.it
Abstract
Learning interpretable models has become a major focus of ma-
chine learning research, given the increasing prominence of ma-
chine learning in socially important decision-making. Among in-
terpretable models, rule lists are among the best-known and easily
interpretable ones. However, finding optimal rule lists is compu-
tationally challenging, and current approaches are impractical for
large datasets.
We present a novel and scalable approach to learn nearly opti-
mal rule lists from large datasets. Our algorithm uses sampling to
efficiently obtain an approximation of the optimal rule list with rig-
orous guarantees on the quality of the approximation. In particular,
our algorithm guarantees to find a rule list with accuracy very close
to the optimal rule list when a rule list with high accuracy exists.
Our algorithm builds on the VC-dimension of rule lists, for which
we prove novel upper and lower bounds. Our experimental evalua-
tion on large datasets shows that our algorithm identifies nearly
optimal rule lists with a speed-up up to two orders of magnitude
over state-of-the-art exact approaches. Moreover, our algorithm is
as fast as, and sometimes faster than, recent heuristic approaches,
while reporting higher quality rule lists. In addition, the rules re-
ported by our algorithm are more similar to the rules in the optimal
rule list than the rules from heuristic approaches.
CCS Concepts
â€¢Mathematics of computing â†’Probabilistic algorithms ;â€¢
Information systems â†’Data mining; â€¢Theory of compu-
tationâ†’Sketching and sampling; Sample complexity and
generalization bounds.
Keywords
Rule list learning, Interpretable Machine Learning, VC-dimension,
Sample complexity bounds
ACM Reference Format:
Leonardo Pellegrina and Fabio Vandin. 2024. Scalable Rule Lists Learning
with Sampling. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671989
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36719891 Introduction
Interpretability is one of the characteristics of machine learning
models that has become a major topic of research due to the ever
increasing impact of machine learning models in socially important
decision-making [ 22]. The goal is to build models that are easily
understood by humans, while achieving high predictive power, in
contrast to black-box models (e.g., large deep-learning models) that
are highly predictive but are not transparent nor interpretable.
Rule lists [ 21], and more in general rule-based models such as
decision trees, are among the best-known and easily interpretable
models. A rule list is a sequence of rules, and the prediction from
such a model is obtained by applying the first rule in the list whose
condition is satisfied for the given input.
Rule-based models often display predictive power comparable
to black-box models in several critical applications [ 22]. While ad-
vanced techniques have been recently developed to speed-up the
discovery of optimal rule lists [ 1,14,23,32], it remains a computa-
tionally challenging task, in particular for large datasets where dif-
ferentiable models (such as neural networks) can be easily learned
with gradient descent and its variants.
Fast heuristic methods for rule lists and other rule-based models
have been developed [ 7,30], but they do not provide guarantees in
terms of the accuracy of the rule list they report. In several appli-
cations identifying an optimal, or close to optimal rule, is crucial,
since the goal is to find models that are not only interpretable,
but also have high predictive power and can therefore be used as
interpretable alternatives to black-box models.
A natural solution to speed up the training on a large dataset,
and to obtain an approximation of the best rule list, is to reduce the
size of the training set, e.g., by only considering a small random
sample of the dataset. On the other hand, it is clear that there is
an intrinsic trade-off between the accuracy of this approximation
and the size of the random sample, i.e., the computational cost
incurred by the learning algorithm. This accuracy depends directly
on the fluctuations of the losses estimated on the sample w.r.t. the
losses measured on the entire dataset: a sample too small can be
analyzed quickly but may provide inaccurate estimates. Moreover, it
is possible that the best rule list, learned from a random sample, may
be qualitatively different, thus not representative, of the optimal
solution computed on the whole dataset.
Contributions. To address these challenges, we precisely quantify
the maximum deviation between the accuracies of rule lists that
are estimated on the random sample w.r.t. the respective exact
counterparts. As a consequence, we obtain sufficient sample sizes
that guarantee that an accurate rule list can be found from a small
random sample. We then propose SamRuLe (Sam pling for Rules
listLearning), a scalable approach to approximate optimal rule lists
from large datasets with rigorous guarantees on the quality of the
approximation.
 
2352
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
â€¢SamRuLe is the first scalable approach to approximate optimal
rule lists from large datasets while providing rigorous guar-
antees on the accuracy of the reported rule list. In particular,
SamRuLe guarantees to find a rule list with accuracy very close
to the optimal one when a rule list with high accuracy exists.
As a result, SamRuLe makes it possible to assess the existence
of high accuracy rule lists for large datasets.
â€¢SamRuLe uses sampling to efficiently obtain an approximation
of the optimal rule list with guarantees. We derive novel bounds
relating the accuracy of a rule lists in the sample with its accu-
racy in the whole dataset andwith the accuracy of the optimal
rule list. Our bounds build on the VC-dimension of rule lists,
for which we prove novel upper and lower bounds, and that
allow us to define sample sizes leading to the desired quality
guarantees.
â€¢Our experimental evaluation shows that SamRuLe enables the
identification of nearly optimal rule lists from large datasets,
with a speed-up of up to two orders of magnitude over state-of-
the-art exact approaches. Moreover, SamRuLe is as fast as, or
faster than, recent heuristic approaches, while reporting higher
quality rule lists, for which it can also certify their quality with
respect to the optimal rule list. The rules reported by SamRuLe
are also more similar to the rules in the optimal rule list than
the rules from heuristic approaches.
2 Related Works
We now discuss the works most related to ours. We focus on meth-
ods to learn rule lists, in particular sparse rule lists. Sparsity is
a crucial requirement for the interpretability of models. For an
overview on the topic of interpretability in machine learning, we
point the interested reader to the survey from Rudin et al. [22].
Finding optimal sparse rule lists is NP-hard, and several methods
have been proposed to learn sparse rule lists. Such methods fall
under two categories: exact methods, that guarantee to find the
optimal rule list (i.e., with highest accuracy), and heuristic methods,
that are faster but provide no guarantees on the accuracy of the
reported rule list w.r.t. the optimal.
In the category of exact methods, Angelino et al. [ 1] propose
CORELS, a branch-and-bound algorithm that achieves several or-
ders of magnitude speedup in time and a significant reduction of
memory consumption, by leveraging algorithmic bounds, efficient
data structures, and computational reuse. Rudin and Ertekin [ 23]
develop a mathematical programming approach to building rule
lists. Okajima and Sadamasa [ 14] define a continuous relaxed ver-
sion of a rule list, and propose an algorithm that optimizes rule lists
based on such continuous relaxed versions. Yu et al. [ 32] propose
a SAT-based approach to find optimal rule sets and lists. While
highly optimized, exact methods such as the aforementioned ones
only apply to problems of moderate size, and do not scale to large
datasets.
In the category of heuristic methods, Rivest [ 21] proposed greedy
splitting techniques, also used in subsequent heuristic algorithms
for learning decision trees [ 5,19]. RIPPER [ 7] builds rule sets in a
greedy fashion, and a similar greedy strategy has been used for find-
ing sets of robust rules in terms of minimum description length [ 9].Other works considered different problems and classification mod-
els, such as probabilistic rules for multiclass classification [ 18], and,
more recently, rule lists with preferred variables [ 15], sets of locally
optimal rules [ 11], and multi-label rule sets [ 6]. Yang et al. [ 30] uses,
instead, a Bayesian approach with Monte-Carlo search to explore
the rule list solution space. All such approaches focus on improving
efficiency with respect to exact approaches, but do not provide
guarantees on the quality of the reported rule lists with respect to
the optimal one. In several challenging instances, the gap with the
optimal solution may be substantial, resulting in a predictive model
with suboptimal performance.
A different line of research, related to the identification of almost-
optimal models, is provided by the exploration of the Rashomon
set, that is the set of almost-optimal models for a machine learning
problem [ 4,10,26]. Works in this area have focused on the explo-
ration or full enumeration of the Rashomon set, including the case
of rule-based models [ 29], for example to study variable impor-
tance in well-performing models [ 8]. While some of our theoretical
results may be useful for such applications as well, our focus is
not the exploration or enumeration of allalmost optimal models,
but rather on efficiently finding one nearly optimal model while
providing guarantees on its quality.
Our algorithm builds on novel upper bounds to the VC-
dimension [ 28] of rule lists, a fundamental combinatorial measure of
the complexity of classification models. We combine these bounds
with sharp concentration inequalities to bound the maximum devi-
ation between the prediction accuracy of rule lists from a random
sample w.r.t. the entire dataset. Our VC-dimension upper bounds
generalize previous results that can be derived from rule-based
classification models [ 24], as they provide a more granular depen-
dence on the rule list parameters compared to known results (see
Section 4.1 for a detailed discussion). We establish the tightness of
our analysis by proving almost matching lower bounds, that signif-
icantly improves over the best known results from Yildiz [ 31]. The
VC-dimension has been used to develop generalization bounds and
sampling algorithms for other data mining tasks, including associa-
tion rules mining [ 20] and sequential pattern mining [ 25]. To the
best of our knowledge, our work is the first to use VC-dimension
in designing efficient sampling approaches for learning rule lists.
3 Preliminaries
We define a datasetDas a collectionD={(ğ‘ 1,ğ‘¡1),...,(ğ‘ ğ‘›,ğ‘¡ğ‘›)}ofğ‘›
training instances, where each training instance is a pair (ğ‘ ,ğ‘¡)âˆˆD
composed by a set ğ‘ ofğ‘‘features, and a categorical label ğ‘¡. We
denote with ğ‘¥ğ‘–theğ‘–-th feature, for 1â‰¤ğ‘–â‰¤ğ‘‘. For simplicity, in this
work we will focus on datasets with binary features, ğ‘¥ğ‘–âˆˆ{0,1}
for1â‰¤ğ‘–â‰¤ğ‘‘, and binary labels ğ‘¡âˆˆ{0,1}, but most of our results
extend to the general case. We use Dğ‘ to denote the collection of
theğ‘‘features of each training instance, ignoring the labels, that is
Dğ‘ ={ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘›}.
We define a rule list ğ‘…=[ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿğ‘˜,ğ‘ŸâŠº]of length|ğ‘…|=ğ‘˜as a
sequence of ğ‘˜â‰¥0rulesğ‘Ÿğ‘–,1â‰¤ğ‘–â‰¤ğ‘˜, plus a default rule ğ‘ŸâŠº. Each
ruleğ‘Ÿâˆˆğ‘…is defined as a pair ğ‘Ÿ=(ğ‘,ğ‘)whereğ‘is a condition on a
subset of the ğ‘‘features of the dataset, and ğ‘âˆˆ{0,1}is a prediction
of the label. Similarly to previous rule-based models [ 1,23,30], we
only consider conditions composed by conjunctions of monotone
 
2353Scalable Rule Lists Learning with Sampling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
variables of the type ğ‘=Ã“
ğ‘–â€œğ‘¥ğ‘–=1â€, i.e., we do not consider the
negation of binary features (e.g., â€œğ‘¥ğ‘–=0â€). Note that negations
can be included easily by considering additional binary features.
The default rule ğ‘ŸâŠº=(ğ‘âŠº,ğ‘)is composed by the condition ğ‘âŠºthat
is always true. Given an instance (ğ‘ ,ğ‘¡)âˆˆD , the predicted label
ğ‘ƒ(ğ‘…,ğ‘ )by the rule list ğ‘…onğ‘ is computed as the predicted label ğ‘
of the first rule ğ‘Ÿwhose condition ğ‘is satisfied by ğ‘ ; Figure 1 shows
an example of a dataset with ğ‘›=5training instances over ğ‘‘=4
features, and a rule list ğ‘…of lengthğ‘˜=3with conjunctions with at
mostğ‘§=2terms.
Dğ‘¥1ğ‘¥2ğ‘¥3ğ‘¥4ğ‘¡
ğ‘ 1 0 1 0 0 1
ğ‘ 2 1 1 0 0 1
ğ‘ 3 0 0 1 1 1
ğ‘ 4 0 0 0 0 0
ğ‘ 5 1 0 1 1 0
(a)ifğ‘¥2=1â†’1
else
ifğ‘¥1=1âˆ§ğ‘¥3=1â†’0
else
ifğ‘¥4=1â†’1
elseâ†’0
(b)
Figure 1: (a): example of a dataset Dwithğ‘›=5instances and
ğ‘‘=4features. (b): example of a rule list ğ‘…with length ğ‘˜=3
with conjunctions with at most ğ‘§=2terms. The rule list ğ‘…
perfectly classifies the instances of D.
The regularized binary loss â„“(ğ‘…,D)of a rule list ğ‘…is defined, for
some constant ğ›¼â‰¥0, as
â„“(ğ‘…,D)=1
ğ‘›âˆ‘ï¸
(ğ‘ ,ğ‘¡)âˆˆD1[ğ‘ƒ(ğ‘…,ğ‘ )â‰ ğ‘¡]+ğ›¼|ğ‘…|.
The use of the regularized loss is common practice in rule list
learning [ 1], allowing to prefer shorter (i.e., simpler) rule lists over
longer ones when their accuracy is similar, as quantified by the
parameterğ›¼.
DefineRğ‘§
ğ‘˜as the set of rule lists with length â‰¤ğ‘˜, where each
rule inğ‘…is given by the conjunction of at most ğ‘§terms. The rule
list learning problem is to identify a rule ğ‘…â˜…such that
ğ‘…â˜…=arg min
ğ‘…âˆˆRğ‘§
ğ‘˜{â„“(ğ‘…,D)}.
The values of ğ‘˜andğ‘§are usually fairly small, since large values
would compromise the interpretability of the rule list [ 22]. Finding
ğ‘…â˜…is NP-hard, and to develop efficient algorithms one has to resort
to approximations.
Our goal is to compute, for given accuracy parameters ğœ€,ğœƒâˆˆ
(0,1], a rule list Ëœğ‘…that provides an(ğœ€,ğœƒ)-approximation of the
optimal rule list ğ‘…â˜…, defined as follows.
Definition 3.1. A rule list Ëœğ‘…provides an(ğœ€,ğœƒ)-approximation of
the optimal rule list ğ‘…â˜…if it holds
â„“(Ëœğ‘…,D)â‰¤â„“(ğ‘…â˜…,D)+ğœ€max{â„“(ğ‘…â˜…,D),ğœƒ}.
Our definition of(ğœ€,ğœƒ)-approximation is motivated by the fact
that the optimal loss â„“(ğ‘…â˜…,D)is unknown a priori, and often ex-
tremely expensive to compute or even estimate from large datasets.
Therefore, we need to design approximation guarantees that are
sharp in all situations, i.e., for the all possible values of the optimalloss. The(ğœ€,ğœƒ)-approximation of Definition 3.1 allows to interpo-
late between an additive approximation, with small absolute error
ğœ€ğœƒ, when the optimal loss â„“(ğ‘…â˜…,D)is small (i.e.,â‰¤ğœƒ), and a relative
approximation (with parameter ğœ€) when the optimal loss â„“(ğ‘…â˜…,D)
is large (i.e., >ğœƒ). This flexible design avoids statistical bottlenecks
that are incurred when the loss of the best model is large, a well
known issue in statistical learning theory [ 3]. Taking this effect
into account allows SamRuLe to be extremely efficient in terms of
sample sizes.
As we will formalize in Section 4, the main goal of SamRuLe is
to compute an(ğœ€,ğœƒ)-approximation of the optimal rule list from a
randomly drawn samples with high probability.
4SamRuLe Algorithm
As introduced in previous sections, the main idea of our approach
is to compute an approximation of the best rule list performing
the analysis of a small random sample, instead of processing a
large dataset, which may be extremely expensive and unfeasible in
practice. In this section we formally define our algorithm.
Define a sampleSas a collection of ğ‘šinstances of the dataset
Dtaken uniformly and independently at random. The regularized
loss of a rule list ğ‘…computed on the sample Sis defined as
â„“(ğ‘…,S)=1
ğ‘šâˆ‘ï¸
(ğ‘ ,ğ‘¡)âˆˆS1[ğ‘ƒ(ğ‘…,ğ‘ )â‰ ğ‘¡]+ğ›¼|ğ‘…|.
Intuitively,â„“(ğ‘…,S)provides an estimate ofâ„“(ğ‘…,D). More precisely,
the loss of any rule ğ‘…computed onSis an unbiased estimator of
the loss computed on D; in fact, it holds
E
S[â„“(ğ‘…,S)]=â„“(ğ‘…,D).
We remark, however, that the convergence of â„“(ğ‘…,S)towards
â„“(ğ‘…,D)in expectation is not sufficient to derive rigorous approxi-
mations for the best rule list; instead, it is necessary to take into
account the variance of the deviations, that indirectly depends the
complexity of the class of rule list models. Therefore, our approach is
based on properly bounding the deviations|â„“(ğ‘…,S)âˆ’â„“(ğ‘…,D)| with
high probability, in order to guarantee that the best rule identified
from the sampleSis a high-quality approximation of the optimal
solution fromD. To obtain a scalable method, we wish to identify
the smallest possible sample size that yields the above-mentioned
guarantees. We address this challenging problem combining ad-
vanced VC-dimension bounds and sharp concentration inequalities.
We now describe our algorithm SamRuLe. The input to SamRuLe
is: a datasetDofğ‘štraining instances over ğ‘‘features; the values
ğ‘˜andğ‘§, representing the maximum number of rules in a rule list
and the maximum number of conditions for each rule; constants
ğœ€,ğœƒâˆˆ(0,1], that are the values defining the quality of the (ğœ€,ğœƒ)-
approximation (see Section 3); and a constant ğ›¿âˆˆ(0,1], that defines
the required confidence, that is the probability that the output by
SamRuLe is a(ğœ€,ğœƒ)-approximation. (Note that the set Rğ‘§
ğ‘˜of rules
lists that can be produced in output by SamRuLe is defined byD,ğ‘˜,
andğ‘§.) Given the input, SamRuLe computes the quantity Ë†ğ‘š(Rğ‘§
ğ‘˜,D),
defined as the minimum value of ğ‘šâ‰¥1such that it holds
âˆšï¸„
3ğœƒln(2
ğ›¿)
ğ‘š+vuut
2
ğœƒ+âˆšï¸ƒ
3ğœƒln(2
ğ›¿)
ğ‘š ğœ”+ln(2
ğ›¿)
ğ‘š+2 ğœ”+ln(2
ğ›¿)
ğ‘šâ‰¤ğœ€ğœƒ,
 
2354KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
whereğœ”=ğ‘˜ğ‘§ln(2ğ‘’ğ‘‘/ğ‘§)+2. Note that Ë†ğ‘š(Rğ‘§
ğ‘˜,D)can be easily com-
puted (e.g., with a binary search over ğ‘š). We also show analytical
bounds to Ë†ğ‘š(Rğ‘§
ğ‘˜,D)in our analysis (Section 4.2).
SamRuLe than draws a sample SofË†ğ‘š(Rğ‘§
ğ‘˜,D)instances taken
uniformly at random from D, and outputs Ëœğ‘…=arg minğ‘…âˆˆRğ‘§
ğ‘˜â„“(ğ‘…,S)
by finding the optimal rule list on the sample S. Note that SamRuLe
can leverage any exact algorithm, such as CORELS [ 1], to find Ëœğ‘…
fromS.
In the following sections, we prove that SamRuLe pro-
vides an(ğœ€,ğœƒ)-approximation of the optimal rule list ğ‘…â˜…=
arg minğ‘…âˆˆRğ‘§
ğ‘˜â„“(ğ‘…,S)with probabilityâ‰¥1âˆ’ğ›¿. Our proof lever-
age novel bounds on the VC-dimension of rule lists, that we present
in Section 4.1, while the approximation guarantees from SamRuLe
are proved in Section 4.2.
4.1 Complexity of Rule Lists
In this section we provide analytical results on the complexity of
rule lists, by studying their VC-dimension [ 28]. Intuitively, the VC-
dimension measures the capacity of a class of predicion models of
predicting arbitrary labels assignments with perfect accuracy. As
discussed in previous sections, these bounds will be instrumental to
derive accurate approximation bounds and prove that the reported
rule by SamRuLe is an(ğœ€,ğœƒ)-approximation.
We start by providing the main concepts related to VC-dimension
(see [12, 13, 27] for a more in depth introduction).
Given a datasetD={(ğ‘ 1,ğ‘¡1),...,(ğ‘ ğ‘›,ğ‘¡ğ‘›)}, remember thatDğ‘ =
{ğ‘ 1,...,ğ‘ ğ‘›}. Given the setRğ‘§
ğ‘˜of rule lists with length â‰¤ğ‘˜, where
each condition contains conjunctions with at most ğ‘§terms, for
each rule list ğ‘…âˆˆRğ‘§
ğ‘˜we define the projection ofğ‘…on the dataset
Dasğ‘‹(ğ‘…,D)={ğ‘ âˆˆ Dğ‘ :ğ‘ƒ(ğ‘…,ğ‘ )=1}. We define the range
space associated to the dataset Dand toRğ‘§
ğ‘˜as(D,ËœRğ‘§
ğ‘˜), where
ËœRğ‘§
ğ‘˜={ğ‘‹(ğ‘…,D):ğ‘…âˆˆRğ‘§
ğ‘˜}is the range set. Note that in general
|ËœRğ‘§
ğ‘˜|can be smaller than |Rğ‘§
ğ‘˜|, since two rule lists ğ‘…1andğ‘…2may
provide the same predictions for all instances in a dataset D, which
impliesğ‘‹(ğ‘…1,D)=ğ‘‹(ğ‘…2,D). The projection of the range set ËœRğ‘§
ğ‘˜
on a subsetDâ€²âŠ†D of the datasetDis defined asP(ËœRğ‘§
ğ‘˜,Dâ€²)=
{ğ‘‹(ğ‘…,Dâ€²):ğ‘…âˆˆRğ‘§
ğ‘˜}.
IfP(ËœRğ‘§
ğ‘˜,Dâ€²)=2Dâ€², i.e., the projection of ËœRğ‘§
ğ‘˜onDâ€²produces all
possible dichotomies of the elements of Dâ€², thenDâ€²isshattered by
ËœRğ‘§
ğ‘˜. The VC-dimension of the range space (D,ËœRğ‘§
ğ‘˜)is the cardinality
of the largest subset of Dthat is shattered by ËœRğ‘§
ğ‘˜. In what follows
we will useğ‘‰ğ¶(Rğ‘§
ğ‘˜)to denote the VC-dimension of the range space
(D,ËœRğ‘§
ğ‘˜)associated to the set of rule lists Rğ‘§
ğ‘˜.
4.1.1 Upper bounds to the VC-dimension. We now prove refined
upper bounds to the VC-dimension of rule lists. We start by provid-
ing upper bounds to the growth function (the maximum cardinality
of distinct projections of a range set, see below) of rule lists R1
ğ‘˜,
for whichğ‘§=1(i.e., each rule consists of a condition on exactly
1 feature). From such bound we derive an upper bound to the
VC-dimension of the range space associated to R1
ğ‘˜, that we will
generalize to derive an upper bound to the VC-dimension of Rğ‘§
ğ‘˜for
general values of ğ‘§. Due to space constraints, some of the proofs
are in Appendix.Define the growth function Î›(Rğ‘§
ğ‘˜,ğ‘š)of the rule listsRğ‘§
ğ‘˜as the
maximum number of distinct projections of rule lists in Rğ‘§
ğ‘˜over
anydatasetDwithğ‘šinstances, that is:
Î›(Rğ‘§
ğ‘˜,ğ‘š)= max
Dâ€²âŠ†D:|Dâ€²|=ğ‘š|P(ËœRğ‘§
ğ‘˜,Dâ€²)|.
Theorem 4.1. It holds
Î›(R1
ğ‘˜,ğ‘š)â‰¤2+ğ‘˜âˆ‘ï¸
ğ‘—=1"
2ğ‘—ğ‘—âˆ’1Ã–
ğ‘–=0(ğ‘‘âˆ’ğ‘–)#
.
Proof. For0â‰¤ğ‘—â‰¤ğ‘˜, we define the set Gğ‘—of rule lists with
length exactly ğ‘—asGğ‘—={ğ‘…âˆˆR1
ğ‘˜:|ğ‘…|=ğ‘—}, and define ËœGğ‘—=
{ğ‘‹(ğ‘…,D):ğ‘…âˆˆGğ‘—}the range set forGğ‘—, with ËœR1
ğ‘˜=âˆªğ‘—ËœGğ‘—. From
an union bound and Jensenâ€™s inequality, it holds
Î›(R1
ğ‘˜,ğ‘š)= max
Dâ€²âŠ†D:|Dâ€²|=ğ‘š|P(âˆªğ‘—ËœGğ‘—,Dâ€²)|
â‰¤ max
Dâ€²âŠ†D:|Dâ€²|=ğ‘šğ‘˜âˆ‘ï¸
ğ‘—=0|P( ËœGğ‘—,Dâ€²)|â‰¤ğ‘˜âˆ‘ï¸
ğ‘—=0Î›(Gğ‘—,ğ‘š).
We proceed to bound each Î›(Gğ‘—,ğ‘š)separately, while excluding
rule listsğ‘…âˆˆ Gğ‘—that have the same projection, on any Dâ€², of
other rules ğ‘…â€²âˆˆGğ‘—â€², for someğ‘—â€²â‰ ğ‘—. First, considerG0; it holds
Î›(G0,ğ‘š)=2sinceG0containsğ‘…1=[(ğ‘âŠº,1)]andğ‘…2=[(ğ‘âŠº,0)].
Then, considerGğ‘—for any 1â‰¤ğ‘—â‰¤ğ‘˜, and let any rule list
ğ‘…âˆˆ Gğ‘—, withğ‘…=[(ğ‘1,ğ‘1),...,(ğ‘ğ‘—,ğ‘ğ‘—),(ğ‘âŠº,ğ‘)]. There are two
possibilities: ğ‘ğ‘—=ğ‘orğ‘ğ‘—â‰ ğ‘. In the first case, denote the rule
ğ‘…â€²=[(ğ‘1,ğ‘1),...,(ğ‘ğ‘—âˆ’1,ğ‘ğ‘—âˆ’1),(ğ‘âŠº,ğ‘)] âˆˆGğ‘—âˆ’1. It is easy to ob-
serve that the projections ğ‘‹(ğ‘…,Dâ€²)andğ‘‹(ğ‘…â€²,Dâ€²)are equal, for
anyDâ€². Therefore, any rule list âˆˆ Gğ‘—withğ‘ğ‘—=ğ‘can be ig-
nored from the upper bound to Î›(Gğ‘—,ğ‘š)as it is already covered
by at least one element of the set Gğ‘—âˆ’1. Then, consider a rule list
ğ‘…=[(ğ‘1,ğ‘1),...,(ğ‘ğ‘—,ğ‘ğ‘—),(ğ‘âŠº,ğ‘)]âˆˆGğ‘—such that there exist two
indices 1â‰¤ğ‘¦<ğ‘–â‰¤ğ‘—withğ‘ğ‘¦=ğ‘ğ‘–. We observe that the rule
ğ‘…â€²=[(ğ‘1,ğ‘1),...,(ğ‘ğ‘–âˆ’1,ğ‘ğ‘–âˆ’1),(ğ‘ğ‘–+1,ğ‘ğ‘–+1)...,(ğ‘ğ‘—,ğ‘ğ‘—),(ğ‘âŠº,ğ‘)] âˆˆ
Gğ‘—âˆ’1is equivalent to ğ‘…, sinceğ‘‹(ğ‘…,Dâ€²)=ğ‘‹(ğ‘…â€²,Dâ€²)for anyDâ€².
From these observations, the number of rules lists ğ‘…âˆˆGğ‘—with
distinct projections on a dataset with ğ‘šelements cannot exceed
2ğ‘—Ãğ‘—âˆ’1
ğ‘–=0(ğ‘‘âˆ’ğ‘–), as we consider all possible ordered choices of ğ‘—
distinct elements from the set of ğ‘‘features{ğ‘¥1,...,ğ‘¥ğ‘‘}, and all 2ğ‘—
possible assignments of the predicted values {ğ‘1,...,ğ‘ğ‘—}âˆˆ{ 0,1}ğ‘—,
whileğ‘is set to the unique value â‰ ğ‘ğ‘—. The sum of these bounds
yields the statement. â–¡
A consequence of Theorem 4.1 is the following upper bound to
the VC-dimension ğ‘‰ğ¶(R1
ğ‘˜)ofR1
ğ‘˜.
Corollary 4.2. The VC-dimension ğ‘‰ğ¶(R1
ğ‘˜)ofR1
ğ‘˜is
ğ‘‰ğ¶(R1
ğ‘˜)â‰¤
ğ‘˜log2(2ğ‘‘)+2
.
Proof. We first note that the VC-dimension of a range set ğ‘„is
at mostğ‘šif it holds log2(Î›(ğ‘„,ğ‘š))<ğ‘š. Using the upper bound to
 
2355Scalable Rule Lists Learning with Sampling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Î›(R1
ğ‘˜,ğ‘š)from Theorem 4.1, we have the following:
log2(Î›(R1
ğ‘˜,ğ‘š))â‰¤ log2Â©Â­
Â«2+ğ‘˜âˆ‘ï¸
ğ‘—=1"
2ğ‘—ğ‘—âˆ’1Ã–
ğ‘–=0(ğ‘‘âˆ’ğ‘–)#
ÂªÂ®
Â¬
â‰¤log2Â©Â­
Â«2+ğ‘˜âˆ‘ï¸
ğ‘—=1(2ğ‘‘)ğ‘—ÂªÂ®
Â¬â‰¤log2 
1+(2ğ‘‘)ğ‘˜+1
2ğ‘‘âˆ’1!
â‰¤log2 
(2ğ‘‘)ğ‘˜+1
2ğ‘‘âˆ’1!
+log2(ğ‘’)2ğ‘‘âˆ’1
(2ğ‘‘)ğ‘˜+1,
where in the last inequality we have used the fact that log2(1+ğ‘¥)â‰¤
log2(ğ‘¥)+log2(ğ‘’)/ğ‘¥,âˆ€ğ‘¥â‰¥0. Then, we have
log2 
(2ğ‘‘)ğ‘˜+1
2ğ‘‘âˆ’1!
+log2(ğ‘’)2ğ‘‘âˆ’1
(2ğ‘‘)ğ‘˜+1
â‰¤(ğ‘˜+1)log2(2ğ‘‘)âˆ’log2(2ğ‘‘âˆ’1)+log2(ğ‘’)(2ğ‘‘)âˆ’ğ‘˜
=ğ‘˜log2(2ğ‘‘)+log22ğ‘‘
2ğ‘‘âˆ’1
+log2(ğ‘’)(2ğ‘‘)âˆ’ğ‘˜
â‰¤ğ‘˜log2(2ğ‘‘)+1+log2(ğ‘’)(2ğ‘‘)âˆ’ğ‘˜<ğ‘˜log2(2ğ‘‘)+2,
proving the statement. â–¡
Using the above upper bound to ğ‘‰ğ¶(R1
ğ‘˜), we prove the following
upper bound to the VC-dimension ğ‘‰ğ¶(Rğ‘§
ğ‘˜)ofRğ‘§
ğ‘˜.
Corollary 4.3. The VC-dimension ğ‘‰ğ¶(Rğ‘§
ğ‘˜)ofRğ‘§
ğ‘˜is
ğ‘‰ğ¶(Rğ‘§
ğ‘˜)â‰¤
ğ‘˜ğ‘§log22ğ‘’ğ‘‘
ğ‘§
+2
.
The bounds we derived in Corollary 4.2 and Corollary 4.3 scale
linearly with ğ‘˜andğ‘§, and logarithmically with ğ‘‘. This implies that,
since we are interested in sparse models with small values of ğ‘˜and
ğ‘§, the resulting complexity will not be large. This guarantees that
accurate models will generalize well, and that the random sample
Sshould be representative of the dataset D. Moreover, the weak
dependence of the bounds on ğ‘‘allows SamRuLe to be extremely
effective even on datasets with large sets of features.
Rudin et al. [ 24] study generalization bounds for classes of binary
classification models built from sequences of association rules; this
general model includes rule lists. Moreover, they show that the
VC-dimension of the set of rule lists created using pre-mined rules
is exactly the size of the set of pre-mined rules, which is O(ğ‘‘ğ‘§)(see
Theorem 3 in [ 24]). However, we remark that this result only holds
for unconstrained rule lists (e.g., without any bound ğ‘˜on their
length), and cannot be adapted to our setting. Our O(ğ‘˜ğ‘§ log(ğ‘‘/ğ‘§))
upper bound on the VC-dimension is more accurate, since it offers a
more granular dependence on the parameters defining the rule list
search space. More precisely, it explicitly depends on the maximum
numberğ‘˜of rules in the list, the number of features ğ‘‘, and the
maximum number ğ‘§of conditions in each rule, while the bound
O(ğ‘‘ğ‘§)from [ 24] is not sensible to such constraints (as it assumes
ğ‘˜=ğ‘‘ğ‘§). Since in most cases ğ‘˜â‰ªğ‘‘ğ‘§, our upper bounds are orders
of magnitude smaller.4.1.2 Lower bounds to the VC-dimension. In this section we prove
almost matching lower bounds to the VC-dimension of rule lists,
confirming the tightness of our analysis. Our first result involves a
lower bound to ğ‘‰ğ¶(R1
ğ‘˜).
Theorem 4.4. The VC-dimension ğ‘‰ğ¶(R1
ğ‘˜)ofR1
ğ‘˜is
ğ‘‰ğ¶(R1
ğ‘˜)â‰¥
ğ‘˜log2ğ‘‘+ğ‘˜
ğ‘˜
.
We may observe that the upper bound O(ğ‘˜ log(ğ‘‘))given by
Corollary 4.2 is almost tight, since Theorem 4.4 implies that
ğ‘‰ğ¶(R1
ğ‘˜)âˆˆÎ©(ğ‘˜log(ğ‘‘+ğ‘˜
ğ‘˜)). Reducing this gap is an interesting open
problem. We remark that the lower bound from Theorem 4.4 im-
proves the best known result, which provides a (weaker) lower
bound of rate Î©(ğ‘˜+log(ğ‘‘âˆ’ğ‘˜))(see Theorem 3 of [31]).
We now prove a lower bound for the general case ğ‘§â‰¥1.
Theorem 4.5. The VC-dimension ğ‘‰ğ¶(Rğ‘§
ğ‘˜)ofRğ‘§
ğ‘˜is
ğ‘‰ğ¶(Rğ‘§
ğ‘˜)â‰¥
ğ‘˜ğ‘§log2ğ‘‘
ğ‘§ğ‘§âˆš
ğ‘˜
.
From Theorem 4.5 and Corollary 4.3 we conclude that ğ‘‰ğ¶(Rğ‘§
ğ‘˜)âˆˆ
O(ğ‘˜ğ‘§ log(ğ‘‘/ğ‘§))andğ‘‰ğ¶(Rğ‘§
ğ‘˜) âˆˆÎ©(ğ‘˜ğ‘§log(ğ‘‘/ğ‘§ğ‘§âˆš
ğ‘˜)), which are al-
most matching. A further refinement of either the upper or lower
bound is an interesting direction for future research.
4.2 Approximation guarantees
In this section we prove sharp approximation bounds for the es-
timated losses â„“(ğ‘…,S)of rulesğ‘…on a random sample Sw.r.t. the
lossesâ„“(ğ‘…,D)on the datasetD, and derive sufficient sample sizes
to compute(ğœ€,ğœƒ)-approximations for the optimal rule list ğ‘…â˜…. Due
to space constraints, we provide most of the proofs in the Appendix.
We defineğœ”asğœ”=ğ‘˜ğ‘§ln(2ğ‘’ğ‘‘/ğ‘§)+2, a complexity parameter of our
bounds derived from the results of Section 4.1 (Corollary 4.3).
Theorem 4.6. LetSbe an i.i.d. random sample of size ğ‘šâ‰¥1of
the datasetD. Then, forğ›¿âˆˆ(0,1), the following inequalities hold
simultaneously with probability â‰¥1âˆ’ğ›¿:
â„“(ğ‘…,D)â‰¤â„“(ğ‘…,S)+vt
2â„“(ğ‘…,S)
ğœ”+ln 2
ğ›¿
ğ‘š+2
ğœ”+ln 2
ğ›¿
ğ‘š,âˆ€ğ‘…âˆˆRğ‘§
ğ‘˜,
â„“(ğ‘…â˜…,S)â‰¤â„“(ğ‘…â˜…,D)+âˆšï¸„
3â„“(ğ‘…â˜…,D)ln 2
ğ›¿
ğ‘š.
From the concentration bounds of Theorem 4.6 we conclude
that, with high probability, all estimates â„“(ğ‘…,S)from the sampleS
provide accurate upper bounds to the losses â„“(ğ‘…,D)in the dataset,
while the loss â„“(ğ‘…â˜…,S)of the optimal rule list strongly concentrates
to its expectation â„“(ğ‘…â˜…,D). We now combine these inequalities to
obtain an easy-to-check condition; this condition yields a sufficient
number of samples to obtain a high-quality approximation of the
optimal rule list ğ‘…â˜…from a random sample. Define Ë†ğ‘š(Rğ‘§
ğ‘˜,D)as the
minimum value of ğ‘šâ‰¥1such that it holds
âˆšï¸„
3ğœƒln(2
ğ›¿)
ğ‘š+vuut
2
ğœƒ+âˆšï¸ƒ
3ğœƒln(2
ğ›¿)
ğ‘š ğœ”+ln(2
ğ›¿)
ğ‘š+2 ğœ”+ln(2
ğ›¿)
ğ‘šâ‰¤ğœ€ğœƒ.
(1)
 
2356KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
We prove that a sample Sof size at least Ë†ğ‘š(Rğ‘§
ğ‘˜,D)provides an
(ğœ€,ğœƒ)-approximation of ğ‘…â˜…with high probability.
Theorem 4.7. LetSbe an i.i.d. random sample of size â‰¥
Ë†ğ‘š(Rğ‘§
ğ‘˜,D)of the datasetD. The output Ëœğ‘…=arg minğ‘…âˆˆRğ‘§
ğ‘˜â„“(ğ‘…,S)of
SamRuLe provides an(ğœ€,ğœƒ)-approximation of ğ‘…â˜…with probability
â‰¥1âˆ’ğ›¿.
Proof. Our proof is based on obtaining an upper bound to
â„“(Ëœğ‘…,D), where Ëœğ‘…is the rule list reported by SamRuLe, that is a
function of the optimal loss â„“(ğ‘…â˜…,D), and then showing that it is
sufficiently small to guarantee an (ğœ€,ğœƒ)-approximation. First, we
observe that â„“(ğ‘…â˜…,D) â‰¤â„“(Ëœğ‘…,D), asğ‘…â˜…is one of the rule lists
with minimum loss. Therefore, from the first set of inequalities of
Theorem 4.6 setting ğ‘…=Ëœğ‘…, we have the upper bound to â„“(ğ‘…â˜…,D)
â„“(ğ‘…â˜…,D)â‰¤â„“(Ëœğ‘…,S)+vt
2â„“(Ëœğ‘…,S)
ğœ”+ln 2
ğ›¿
ğ‘š+2
ğœ”+ln 2
ğ›¿
ğ‘š.(2)
We now prove that, since Ëœğ‘…is chosen as the rule list with minimum
loss on the sample, its estimated loss â„“(Ëœğ‘…,S)should be concentrated
towardsâ„“(ğ‘…â˜…,D). We haveâ„“(Ëœğ‘…,S) â‰¤â„“(ğ‘…â˜…,S), as Ëœğ‘…is one of
the rule list with minimum loss in S. Using the last inequality of
Theorem 4.6 it holds
â„“(Ëœğ‘…,S)â‰¤â„“(ğ‘…â˜…,S)â‰¤â„“(ğ‘…â˜…,D)+âˆšï¸„
3â„“(ğ‘…â˜…,D)ln 2
ğ›¿
ğ‘š.(3)
Replacing all occurences of â„“(Ëœğ‘…,S)in(2)with its upper bound given
by the r.h.s. of (3), we obtain
â„“(Ëœğ‘…,D)â‰¤â„“(ğ‘…â˜…,D)+ğ‘¢(â„“(ğ‘…â˜…,D),ğ‘š), (4)
where the function ğ‘¢(ğ‘£,ğ‘š)is
ğ‘¢(ğ‘£,ğ‘š)=âˆšï¸„
3ğ‘£ln 2
ğ›¿
ğ‘š+vuuut
2
ğ‘£+âˆšï¸‚
3ğ‘£ln 2
ğ›¿
ğ‘š
ğœ”+ln 2
ğ›¿
ğ‘š+2
ğœ”+ln 2
ğ›¿
ğ‘š.
We now seek to identify the minimum ğ‘šsuch thatâ„“(Ëœğ‘…,D)is guar-
anteed to beâ‰¤â„“(ğ‘…â˜…,D)+ğœ€max{ğœƒ,â„“(ğ‘…â˜…,D)}. Note that, from the
derivations above, it is sufficient to verify that ğ‘¢(â„“(ğ‘…â˜…,D),ğ‘š)â‰¤
ğœ€max{ğœƒ,â„“(ğ‘…â˜…,D)}, for anyâ„“(ğ‘…â˜…,D)âˆˆ[ 0,1]. First, consider the
caseâ„“(ğ‘…â˜…,D)â‰¤ğœƒ. We have that
ğ‘¢(â„“(ğ‘…â˜…,D),ğ‘š)â‰¤ğ‘¢(ğœƒ,ğ‘š)â‰¤ğœ€ğœƒ
is true when ğ‘šâ‰¥Ë†ğ‘š(Rğ‘§
ğ‘˜,D)by definition of Ë†ğ‘š(Rğ‘§
ğ‘˜,D)given in
the statement, since ğ‘¢(ğœƒ,ğ‘š)â‰¤ğœ€ğœƒis guaranteed by (1). We now
consider the case â„“(ğ‘…â˜…,D)â‰¥ğœƒ. We need to verify that
ğ‘¢(ğ‘£,ğ‘š)â‰¤ğœ€ğ‘£,âˆ€ğœƒâ‰¤ğ‘£â‰¤1. (5)
From the definition of Ë†ğ‘š(Rğ‘§
ğ‘˜,D), we know that ğ‘¢(ğœƒ,ğ‘š)â‰¤ğœ€ğœƒholds
for allğ‘šâ‰¥Ë†ğ‘š(Rğ‘§
ğ‘˜,D), therefore (5)is verified for ğ‘£=ğœƒ. By taking
the derivative w.r.t. ğ‘£of both sides of (5), it is simple to check that
the derivative of the r.h.s. is constant, while the derivative of the
l.h.s. is monotonically decreasing with ğ‘£. Therefore, (5)also holds
for allğœƒ<ğ‘£â‰¤1, obtaining the statement. â–¡
We note that, while Ë†ğ‘š(Rğ‘§
ğ‘˜,D)can be easily computed (e.g., with
a binary search over ğ‘š), from (1)it may not be simple to interpretits dependence on the several parameters. To do so, we prove the
following upper bound to Ë†ğ‘š(Rğ‘§
ğ‘˜,D).
Theorem 4.8. It holds Ë†ğ‘š(Rğ‘§
ğ‘˜,D)âˆˆO ğœ”+ln(1
ğ›¿)
ğœ€2ğœƒ.
Proof. First, we prove that Ë†ğ‘š(Rğ‘§
ğ‘˜,D)â‰¥ 3ln(2/ğ›¿)/ğœƒ. This fol-
lows easily fromâˆšï¸ƒ
3ğœƒln(2/ğ›¿)
ğ‘šâ‰¤ğœ€ğœƒ, i.e., only considering the left-
most term of (1). Then, it holds
âˆšï¸„
3ğœƒln(2
ğ›¿)
ğ‘š+vuuut2
ğœƒ+âˆšï¸ƒ
3ğœƒln(2
ğ›¿)
ğ‘š ğœ”+ln(2
ğ›¿)
ğ‘š+2 ğœ”+ln(2
ğ›¿)
ğ‘š
â‰¤âˆšï¸„
3ğœƒln(2
ğ›¿)
ğ‘š+âˆšï¸„
4ğœƒ ğœ”+ln(2
ğ›¿)
ğ‘š+2 ğœ”+ln(2
ğ›¿)
ğ‘š
â‰¤âˆšï¸„
14ğœƒ ğœ”+ln(2
ğ›¿)
ğ‘š+2 ğœ”+ln(2
ğ›¿)
ğ‘š,
where the second-last inequality holds for all ğ‘šâ‰¥3ln(2/ğ›¿)/ğœƒ. By
solving the quadratic inequality
âˆšï¸
14ğ‘šğœƒ(ğœ”+ln(2/ğ›¿))+2(ğœ”+ln(2/ğ›¿))â‰¤ğ‘šğœ€ğœƒ,
we observe that (1) holds for all ğ‘šwith
ğ‘šâ‰¥ ğœ”+ln(2
ğ›¿)âˆš
14âˆš
8ğœ€+14
2ğœ€2ğœƒ+2 ğœ”+ln(2
ğ›¿)
ğœ€ğœƒ+7 ğœ”+ln(2
ğ›¿)
ğœ€2ğœƒ.(6)
The r.h.s. of (6)isâˆˆ O(ğœ”+ln(1
ğ›¿)
ğœ€2ğœƒ); moreover, it upper bounds
Ë†ğ‘š(Rğ‘§
ğ‘˜,D), as(6)provides a value of ğ‘šthat make (1)true (not
necessarily the minimum), obtaining the statement. â–¡
Interestingly, the sample size Ë†ğ‘š(Rğ‘§
ğ‘˜,D)is completely indepen-
dent of the size ğ‘›of the datasetD: it only depends on the parameters
ğœ”=ğ‘˜ğ‘§ln(2ğ‘’ğ‘‘/ğ‘§)+2of the rule lists search space, and the desired
approximation accuracy ğœ€,ğœƒand confidence ğ›¿. This characteristic
allows SamRuLe to be applied to massive datasets, of arbitrarily
large sizeğ‘›.
5 Experiments
This section presents the results of our experiments. The main
goal of our experimental evaluation is to test the scalability of
SamRuLe in analyzing large datasets compared to exact approaches
(Section 5.1). To do so, we measure the number of samples used by
SamRuLe to obtain an accurate approximation of the best rule, its
running time, and the accuracy of the reported rule list compared
to the optimal solution. Then, we also compare SamRuLe with
state-of-the-art heuristics for rule list training (Section 5.2), as such
methods, while not offering theoretical guarantees, may still provide
accurate rule lists in practice. Finally, we quantify the performance
ofSamRuLe under several settings of the rule list parameters ğ‘§and
ğ‘˜(Section 5.3).
Datasets. We tested SamRuLe on8benchmark datasets from
UCI1. We binarized the datasets containing countinous features
by considering 4thresholds at equally spaced quantiles: for each
countinous feature ğ‘“and each threshold ğ‘¡, we created two binary
features â€œğ‘“â‰¥ğ‘¡â€andâ€œğ‘“<ğ‘¡â€. The statistics of the resulting binary
1https://archive.ics.uci.edu/
 
2357Scalable Rule Lists Learning with Sampling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
datasets are described in Table 1. Since some of these datasets are
quite small, we replicate them ğ‘Ÿtimes, i.e., each training sample of
Dis copiedğ‘Ÿtimes (see the column ğ‘Ÿof Table 1). Note that this pre-
processing allows to obtain larger datasets, all with approximately
106training instances, while preserving the rule list distribution
and search space structure, as the losses and covered instances of
all rule lists are the same on the original and replicated datasets.
Compared methods. Our main goal is to compare SamRuLe with
CORELS [1], the state-of-the-art exact method to identify the rule
list with minimum loss from a dataset D.SamRuLe, as described in
Section 4, generates the random sample Sand then runs an exact
algorithm to search for the rule list with minimum loss on S(using
the same settings and parameters). Note that by using CORELS as
exact algorithm within SamRuLe, all differences between SamRuLe
and CORELS are due to the use of sampling, i.e., there are no other
confounding factors in the comparison between the two methods.
We also compare SamRuLe with SBRL [ 30] and RIPPER [ 7], two
state-of-the-art heuristic approaches for rule list learning. SBRL
uses a scalable Monte-Carlo approach to approximately search for
accurate rule lists, while RIPPER leverages a greedy selection of
conditions. Our comparison is motivated by the fact that, even if
these methods do not provide guarantees in terms of solution qual-
ity, they are often the methods of choice for practitioners since they
obtain good solutions and are designed to scale to large datasets.
Rule Lists Parameters. To compare SamRuLe with exact and
heuristic methods (Sections 5.1 and 5.2), for each dataset we set the
parameterğ‘˜as shown in Table 1 and ğ‘§=1. In Section 5.3 we evalu-
ate the impact of different choices of ğ‘˜andğ‘§. Regarding SamRuLe,
we compute(ğœ€,ğœƒ)-approximations for several combinations of the
parameters ğœ€andğœƒ. We consider ğœ€âˆˆ{1,0.5,0.25}, and varyğœƒin
the interval[0.005,0.05]. For all experiments we fix ğ›¿=0.05, as we
did not observe significant differences for other values (given the
exponential dependence of the bounds w.r.t. ğ›¿).
Experimental setup. We implemented SamRuLe in Python. The
code and the scripts to reproduce all experiments are available
online2. For CORELS, we have used the implementation available
online3. We made minor modifications to the original implemen-
tation to limit its exploration to rule lists of length at most ğ‘˜(i.e.,
pruning all rules of length >ğ‘˜instead of performing an unbounded
search). We made similar minor changes to the implementation
of SBLR4. We evaluated RIPPER with a recent efficient implemen-
tation5. All the code was compiled and executed on a machine
equipped with 2.30 GHz Intel Xeon CPU, 1TB of RAM, on Ubuntu
20.04. We repeated all experiments 10times, and report averages Â±
stds over the 10repetitions.
5.1 Comparison to exact method
In this section we describe the experimental comparison between
SamRuLe and CORELS, the state-of-the-art method to identify the
optimal rule list ğ‘…â˜…with minimum loss. Our main goal is to eval-
uate the scalability of SamRuLe in terms of number of samples
and running time required to obtain an accurate approximation of
the best rule. Furthermore, we evaluate, both quantitatively and
2https://github.com/VandinLab/SamRuLe
3https://github.com/corels/corels
4https://github.com/Hongyuy/sbrlmod/
5https://github.com/imoscovitz/wittgensteinTable 1: Statistics of the datasets considered in our experi-
ments.ğ‘›is the number of transactions, ğ‘‘is the number of
binary features, ğ‘Ÿis the replication factor, ğ‘˜is the maximum
rule list length.
D ğ‘› ğ‘‘ ğ‘Ÿ ğ‘˜
a9a 32561 124 100 4
adult 32561 175 100 4
bank 41188 152 100 4
higgs 11000000 531 1 3
ijcnn1 91701 35 100 8
mushroom 8124 118 200 5
phishing 11050 69 100 8
susy 5000000 179 1 5
qualitatively, the accuracy of the rule list found by SamRuLe w.r.t.
the optimal solution returned by CORELS. We ran both methods on
all datasets, setting ğ‘§=1andğ‘˜as in Table 1. For SamRuLe, we vary
the parameters ğœƒandğœ€as described at the beginning of Section 5.
Figure 2 shows the results for these experiments. In Figure 2.(a)
and (b) we compare the running time of SamRuLe with the time
needed by CORELS on the adult and bank datasets. The results
for other datasets are very similar, and shown in Figure 6 (in the
Appendix). From these results, we can immediately conclude that
SamRuLe requires a small fraction of the time needed by CORELS,
with an improvement of up to 2orders of magnitude. The reason
for this significant speedup is that SamRuLe searches for the rule
with minimum loss on a small sample S, which is all cases orders
of magnitude smaller than the size of original dataset D. We show
the number of samples Ë†ğ‘š(Rğ‘§
ğ‘˜,D)used by SamRuLe for all datasets
and for all parameters in Figure 5 (in the Appendix).
Then, we evaluated the quality of the solutions returned by Sam-
RuLe with the optimal rule list computed by CORELS. Figures 2.(c)
and (d) show the average deviations |â„“(Ëœğ‘…,D)âˆ’â„“(ğ‘…â˜…,D)|of the
loss of the rule list Ëœğ‘…found by SamRuLe w.r.t. the optimal rule list
ğ‘…â˜…found by CORELS on the dataset D. To verify the validity of
SamRuLeâ€™s theoretical guarantees, the plots also show the loss of
the optimal solution â„“(ğ‘…â˜…,D)found by CORELS (purple horizontal
line), and upper error bars ( +std) for the average deviations (see
Figures 6 and 7 in the Appendix for the plots for all datasets with
both bars). From these results, we observe that the rule lists re-
ported by SamRuLe are extremely accurate in terms of prediction
accuracy, since the deviations |â„“(Ëœğ‘…,D)âˆ’â„“(ğ‘…â˜…,D)|areorders of
magnitude smaller than â„“(ğ‘…â˜…,D), and smaller than guaranteed by
our theoretical analysis. This confirms that SamRuLe outputs ex-
tremely accurate rule lists, even when trained on random samples
that are orders of magnitude smaller than the entire dataset. Further-
more, it is likely that the guarantees of the (ğœ€,ğœƒ)-approximations
hold for samples smaller than what guaranteed by our analysis;
this leaves significant opportunities for further improvements of
our algorithm. Then, we quantify the deviations between the loss
â„“(Ëœğ‘…,S)ofËœğ‘…estimated on the sample w.r.t. to the loss â„“(Ëœğ‘…,D)on
the dataset, i.e., the approximation error incurred by SamRuLe due
to analyzingSinstead of the entire dataset D. In Figure 8 we show
the average loss approximation error |â„“(Ëœğ‘…,D)âˆ’â„“(Ëœğ‘…,S)|, which is
 
2358KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
102
105
104
Average deviations
Average deviations (susy)
SamRuLe (=0.25)
SamRuLe (=0.5)
SamRuLe (=1.0)
 CORELS
102
103104Running time (s)
Running time (adult)
(a)
102
102103104Running time (s)
Running time (bank) (b)
102
104
103
102
101
Average deviations
Average deviations (adult) (c)
102
104
103
102
101
Average deviations
Average deviations (bank) (d)
Figure 2: Performance and accuracy comparison between SamRuLe and CORELS on adult and bank datasets, for different
values ofğœ€andğœƒ. (a)-(b): running times of CORELS and SamRuLe. (c)-(d): average deviations |â„“(Ëœğ‘…,D)âˆ’â„“(ğ‘…â˜…,D)|of the loss of
the rule list Ëœğ‘…found by SamRuLe with the optimal rule list ğ‘…â˜…found by CORELS (purple horizontal line drawn at ğ‘¦=â„“(ğ‘…â˜…,D)).
The deviation plots only show upper errors bars at +std to improve readability. See Figures 6 and 7 in the Appendix for the
plots for all datasets and with Â±std bars.
ifcapital-gainâ‰¥2Â·104â†’1
else if capital-lossâ‰¥1742â†’1
else
ifeducation-num <13â†’0
else if marital-status=married â†’1
elseâ†’0
(a)ifcapital-gainâ‰¥2Â·104â†’1
else if capital-lossâ‰¥1742â†’1
else
ifeducation-num <13â†’0
else if marital-status=married â†’1
elseâ†’0ifcapital-gainâ‰¥2Â·104â†’1
else
ifage<26â†’0
else
ifeducation-num <13â†’0
else
ifmarital-status=married â†’1
elseâ†’0ifcapital-gainâ‰¥2Â·104â†’1
else
ifhours-per-week <35â†’0
else
ifeducation-num <11â†’0
else
ifmarital-status=married â†’1
elseâ†’0
(b)
Figure 3: (a): optimal rule ğ‘…â˜…computed by CORELS on the adult dataset ( â„“(ğ‘…â˜…,D)=0.176) to predict high income (the label 1
denotes â€œâ‰¥50ğ¾â€). (b): set of rule lists computed by SamRuLe over 10runs. SamRuLe identified the optimal rule and slight
variations Ëœğ‘…1and Ëœğ‘…2that differ in the second rule of the list: they predict a lower outcome using the age ( Ëœğ‘…1) and the per-week
work hours features ( Ëœğ‘…2) with respective loss â„“(Ëœğ‘…1,D)=0.1763 andâ„“(Ëœğ‘…2,D)=0.1775.
also extremely small (i.e., 1to2orders of magnitude smaller than
â„“(Ëœğ‘…,D)), confirming that the conclusions that can be drawn from
the sample using SamRuLe, e.g., from the estimated loss â„“(Ëœğ‘…,S), are
very close to the corresponding exact ones. Finally, we compared
the logical conditions in the rule lists returned by SamRuLe with
the ones in the best rule list computed by CORELS. Our goal is to
verify that the insights gained from the approximated prediction
models from SamRuLe were similar to the optimal ones, i.e., that
SamRuLe allows a qualitative interpretation of the reported rule
list that was stable over the different experimental runs and similar
to what obtainable from the exact analysis. Figure 3 reports the
optimal rule list computed by CORELS on the dataset adult (a),
and the set of rule lists computed by SamRuLe (b) over all runs.
Interestingly, we observe that the approximations from SamRuLe
either match the optimal solution, or are very similar to it. In fact,
all rules reported by SamRuLe either share the same conditions
found in the optimal rule list, or replace one of the feature with
alternative reasonable insights (e.g., predicting a lower income for
young individuals and limited weekly working hours). In general,
we found the solutions reported by SamRuLe to be extremely stable
and similar to the respective optimal solutions also for all other
dataset.
From these observations we conclude that SamRuLe computes
extremely accurate rule lists using a fraction of the resources needed
by exact approaches, therefore scaling effectively to large datasets.5.2 Comparison to heuristic methods
In this set of experiments, we compare SamRuLe with two state-of-
the-art heuristic methods SBRL and RIPPER. For SamRuLe we fix
ğœƒ=0.025andğœ€=0.5. We ran SBRL for 104iterations using default
parameters (as suggested by [ 1,30]). Also for these experiments,
for each dataset we fix ğ‘§=1andğ‘˜to the values in Table 1.
We show the results of these experiments in Figure 4.
Figure 4.(a) shows the running times for the three methods. We
observe that in all but one case, RIPPER is the slowest method.
On two datasets (ijcnn1 and higgs) we stopped it as it could not
complete after more than 12hours, while requiring a very large
memory footprint (e.g., more than 400GB of memory for higgs).
Regarding SBRL and SamRuLe, both methods are fast, e.g., requiring
always less than 18minutes. More precisely, SamRuLe is faster on 3
datasets, up to 3orders of magnitude for the susy dataset. In other
2datasets, the running times of the two methods are comparable,
while for phishing and adult SBRL is faster by a factor at most 4. This
experiment confirms that SamRuLe is very practical, requiring a
lower or comparable amount of resources of state-of-the-art scalable
heuristics.
Figure 4.(b) compares all methods in terms of accuracy. The plots
show the values of the loss â„“(Ëœğ‘…,D)for the rule list Ëœğ‘…reported
by all runs of the methods. We may observe that RIPPER is the
worst approach, as it always reports rule lists with the highest loss
 
2359Scalable Rule Lists Learning with Sampling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
susy
ijcnn1
higgs
mushroom
bank
a9a
phishing
adult101
100101102103104Running time (s)Running time
SamRuLe SBRL RIPPER
susy
ijcnn1
higgs
mushroom
bank
a9a
phishing
adult101
100101102103104Running time (s)Running time
(
a)
susy
ijcnn1
higgs
mushroom
bank
a9a
phishing
adult103
102
101
Rule list lossRule list loss (b)ifageâ‰¥26â†’0
else if education-num <9â†’0
else if marital-status=married â†’1
else if education-num <13â†’0
elseâ†’0
(c)
Figure 4: Comparison in terms of running time (a) and accuracy (b) between SamRuLe, SBRL, and RIPPER. (c): rule ğ‘…computed
by SBRL on adult with loss â„“(ğ‘…,D)=0.238over all 10runs.
(results for ijcnn1 and higgs are not shown as RIPPER could not
complete in reasonable time, as discussed before), while SamRuLe
always provides a rule list with the smallest loss. Regarding SBRL,
we observe that it reports a rule list with the same, or almost the
same, loss of SamRuLe on4datasets, while it provides suboptimal
solutions for other cases, with losses up to 30%higher than Sam-
RuLe. This suggests that, while SBRL scales to large datasets, it
often provides solutions that are sensibly less accurate than the
optimal one. Instead, as discussed previously, SamRuLe outputs a
rule list with guaranteed gap with the optimal solution, and always
very close to it in practice. We remark that providing a suboptimal
solution may also impact the interpretation for the predictions, e.g.,
missing relevant factors for the model. In fact, we report the rule
list computed by SBRL on adult (see Figure 4.(c)) for all the 10runs:
such rule list does not involve the capital-gain feature, a key condi-
tion for the optimal solution to predict high income (Figure 3.(a)),
thus obtaining a higher loss (0 .238). In contrast, this feature was
always found in the rule lists reported by SamRuLe (Figure 3.(b)),
which have loss always very close to the optimal (0 .176).
Overall, compared to RIPPER and SBRL, SamRuLe outputs an
equally accurate solution in less time, or a sensibly better rule list
using comparable resources. We conclude that SamRuLe provides
an excelled combination of scalability and high accuracy for rule
list learning from large datasets, achieving a better trade-off than
state-of-the-art heuristic methods with no theoretical guarantees.
5.3 Impact of rule list parameters
In this final set of experiments, we evaluate the impact to the per-
formance of SamRuLe of the rule list search space parameters ğ‘§and
ğ‘˜. We focus on the datasets mushroom and phishing, as the results
for other datasets were similar. We test all values of 1â‰¤ğ‘§â‰¤3and
1â‰¤ğ‘˜â‰¤5, fixingğœƒ=0.025andğœ€=0.5, measuring the number of
samples required by SamRuLe, its running time, and the accuracy
of the rule lists provided in output.
We show these results in Figure 9. When increasing ğ‘˜andğ‘§,
the number of samples Ë†ğ‘š(Rğ‘§
ğ‘˜,D)considered by SamRuLe grows
following the expect trend Ë†ğ‘š(Rğ‘§
ğ‘˜,D)âˆˆO((ğœ”+log(1/ğ›¿))/(ğœ€2ğœƒ))
proved in our analysis (Theorem 4.8), resulting in sample sizes thatare always a small fraction of the size of the dataset. Consequently,
the running time of SamRuLe increases roughly linearly with the
sample size, remaining practical for all settings. Regarding the
accuracy of the rule lists, we observed the parameter ğ‘˜to have the
largest impact on the loss, that remains fairly stable w.r.t. ğ‘§.
These results demonstrate that SamRuLe is applicable to complex
analysis involving larger values of ğ‘˜andğ‘§while scaling to large
datasets, that are in most cases out of reach of exact approaches.
6 Conclusions
We introduced SamRuLe, a novel and scalable algorithm to find
nearly optimal rule lists. SamRuLe uses sampling to scale to large
datasets, and provides rigorous guarantees on the quality of the
rule lists it reports. Our approach builds on the VC-dimension of
rule lists, for which we proved novel upper and lower bounds. Our
experimental evaluation shows that SamRuLe enables learning
highly accurate rule lists on large datasets, is up to two orders of
magnitude faster than state-of-the-art exact approaches, and is as
fast as, and sometimes faster than, recent heuristic approaches,
while reporting higher quality rule lists.
Our work opens several interesting directions for future research,
including the use of sampling to scale approaches for learning other
rule-based models while providing rigorous guarantees on the qual-
ity of the learned model. Moreover, the efficient computation of ad-
vanced data-dependent complexity measures, such as Rademacher
averages [ 2,16,17], may be useful to obtain even sharper approxi-
mation guarantees for our problem.
Acknowledgments
This work was supported by the â€œNational Center for HPC, Big
Data, and Quantum Computingâ€, project CN00000013, and by the
PRIN Project n. 2022TS4Y3N - EXPAND: scalable algorithms for
EXPloratory Analyses of heterogeneous and dynamic Networked
Data, funded by the Italian Ministry of University and Research
(MUR), and by the project BRAINTEASER (Bringing Artificial In-
telligence home for a better care of amyotrophic lateral sclerosis
and multiple sclerosis), funded by European Unionâ€™s Horizon 2020
(grant agreement No. GA101017598).
 
2360KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
References
[1]Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia
Rudin. 2018. Learning certifiably optimal rule lists for categorical data. Journal
of Machine Learning Research 18, 234 (2018), 1â€“78.
[2]Peter L. Bartlett and Shahar Mendelson. 2002. Rademacher and Gaussian complex-
ities: Risk bounds and structural results. Journal of Machine Learning Research 3,
Nov (2002), 463â€“482.
[3]StÃ©phane Boucheron, Olivier Bousquet, and GÃ¡bor Lugosi. 2005. Theory of
classification: A survey of some recent advances. ESAIM: probability and statistics
9 (2005), 323â€“375.
[4]Leo Breiman. 2001. Statistical modeling: The two cultures (with comments and a
rejoinder by the author). Statistical science 16, 3 (2001), 199â€“231.
[5]L Breiman, J Friedman, R A Olshen, and C J Stone. 1984. Classification and
Regression Trees. Routledge.
[6]Martino Ciaperoni, Han Xiao, and Aristides Gionis. 2023. Concise and inter-
pretable multi-label rule sets. Knowledge and Information Systems 65, 12 (2023),
5657â€“5694.
[7]William W Cohen. 1995. Fast effective rule induction. In Machine learning
proceedings 1995. Elsevier, 115â€“123.
[8]Jiayun Dong and Cynthia Rudin. 2020. Exploring the cloud of variable importance
for the set of all good models. Nature Machine Intelligence 2, 12 (2020), 810â€“824.
[9]Jonas Fischer and Jilles Vreeken. 2019. Sets of robust rules, and how to find them.
InJoint European Conference on Machine Learning and Knowledge Discovery in
Databases. Springer, 38â€“54.
[10] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models are
Wrong, but Many are Useful: Learning a Variableâ€™s Importance by Studying an
Entire Class of Prediction Models Simultaneously. J. Mach. Learn. Res. 20, 177
(2019), 1â€“81.
[11] Van Quoc Phuong Huynh, Johannes FÃ¼rnkranz, and Florian Beck. 2023. Efficient
learning of large sets of locally optimal classification rules. Machine Learning
112, 2 (2023), 571â€“610.
[12] Michael Mitzenmacher and Eli Upfal. 2017. Probability and computing: Random-
ization and probabilistic techniques in algorithms and data analysis. Cambridge
university press.
[13] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations
of machine learning. MIT press.
[14] Yuzuru Okajima and Kunihiko Sadamasa. 2019. Decision list optimization based
on continuous relaxation. In Proceedings of the 2019 SIAM International Conference
on Data Mining. SIAM, 315â€“323.
[15] Ioanna Papagianni and Matthijs van Leeuwen. 2023. Discovering Rule Lists
with Preferred Variables. In International Symposium on Intelligent Data Analysis.
Springer, 340â€“352.
[16] Leonardo Pellegrina. 2023. Efficient Centrality Maximization with Rademacher
Averages. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™23). Association for Computing Machinery,
New York, NY, USA, 1872â€“1884. https://doi.org/10.1145/3580305.3599325
[17] Leonardo Pellegrina, Cyrus Cousins, Fabio Vandin, and Matteo Riondato. 2022.
MCRapper: Monte-Carlo Rademacher averages for poset families and approx-
imate pattern mining. ACM Transactions on Knowledge Discovery from Data
(TKDD) 16, 6 (2022), 1â€“29.
[18] Hugo M ProenÃ§a and Matthijs van Leeuwen. 2020. Interpretable multiclass
classification by MDL-based rule lists. Information Sciences 512 (2020), 1372â€“
1393.
[19] J. Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA. http://portal.acm.org/citation.cfm?id=
152181
[20] Matteo Riondato and Eli Upfal. 2014. Efficient Discovery of Association Rules and
Frequent Itemsets through Sampling with Tight Performance Guarantees. ACM
Trans. Knowl. Disc. from Data 8, 4 (2014), 20. https://doi.org/10.1145/2629586
[21] Ronald L Rivest. 1987. Learning decision lists. Machine learning 2 (1987), 229â€“246.
[22] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and
Chudi Zhong. 2022. Interpretable machine learning: Fundamental principles and
10 grand challenges. Statistics Surveys 16, none (2022), 1 â€“ 85.
[23] Cynthia Rudin and Åeyda Ertekin. 2018. Learning customized and optimized
lists of rules with mathematical programming. Mathematical Programming
Computation 10 (2018), 659â€“702.
[24] Cynthia Rudin, Benjamin Letham, and David B Madigan. 2013. Learning theory
analysis for association rules and sequential event prediction. (2013).
[25] Diego Santoro, Andrea Tonon, and Fabio Vandin. 2020. Mining Sequential Pat-
terns with VC-Dimension and Rademacher Complexity. Algorithms 13, 5 (2020),
123.
[26] Lesia Semenova, Cynthia Rudin, and Ronald Parr. 2022. On the existence of
simpler machine learning models. In Proceedings of the 2022 ACM Conference on
Fairness, Accountability, and Transparency. 1827â€“1858.
[27] Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding Machine Learning:
From Theory to Algorithms. Cambridge University Press.[28] V. N. Vapnik and A. Ya. Chervonenkis. 1971. On the Uniform Convergence of
Relative Frequencies of Events to Their Probabilities. Theory of Probability & Its
Applications 16, 2 (1971), 264. https://doi.org/10.1137/1116025
[29] Rui Xin, Chudi Zhong, Zhi Chen, Takuya Takagi, Margo Seltzer, and Cynthia
Rudin. 2022. Exploring the whole rashomon set of sparse decision trees. Advances
in Neural Information Processing Systems 35 (2022), 14071â€“14084.
[30] Hongyu Yang, Cynthia Rudin, and Margo Seltzer. 2017. Scalable Bayesian rule
lists. In International conference on machine learning. PMLR, 3921â€“3930.
[31] Olcay Taner Yildiz. 2014. VC-Dimension of Rule Sets. In 2014 22nd International
Conference on Pattern Recognition. IEEE, 3576â€“3581.
[32] Jinqiang Yu, Alexey Ignatiev, Peter J Stuckey, and Pierre Le Bodic. 2021. Learning
optimal decision sets and lists with sat. Journal of Artificial Intelligence Research
72 (2021), 1251â€“1279.
A Appendix
In this Appendix we provide proofs and additional experimental
results that could not fit in the main paper due to space constraints.
Some figures, and some of the proofs for the results of Section 4.1
are deferred to the online extended version, that is available at
https://arxiv.org/abs/2406.12803.
A.1 Proofs of Section 4.1
Proof of Corollary 4.3. Given a datasetDwithğ‘‘features, we
create a new dataset Dğ‘§built as follows. Let ğ¶={â€œğ‘¥1=1â€,â€œğ‘¥2=
1â€,..., â€œğ‘¥ğ‘‘=1â€}be the set of all ğ‘‘possible conditions on the
ğ‘‘binary features of Dğ‘ ; for any non-empty subset ğ´âŠ†ğ¶with
|ğ´|â‰¤ğ‘§, we add toDğ‘§the binary feature ğ‘¥â€²
ğ´that is equal to 1for
all training instances of Dsuch thatÃ“
ğ‘âˆˆğ´ğ‘is true. Equivalently,
the feature values of ğ‘¥â€²
ğ´are obtained from the logical AND of the
evaluations of the conditions in ğ´. It follows that the total number
of features ofDğ‘§isğ‘‘â€²=Ãğ‘§
ğ‘–=1 ğ‘‘
ğ‘–.
We now observe that the set P(ËœRğ‘§
ğ‘˜,D)of projections of rule lists
inRğ‘§
ğ‘˜onDis contained in the set P(ËœR1
ğ‘˜,Dğ‘§)of projections of rules
inR1
ğ‘˜onDğ‘§, since we can replace each rule with a conjunctions
withğ‘¡terms, with 1â‰¤ğ‘¡â‰¤ğ‘§, in any rule listâˆˆRğ‘§
ğ‘˜onDby a rule
with a single condition on one of the features of Dğ‘§, obtaining an
equivalent rule list from R1
ğ‘˜. Therefore, from Corollary 4.2 applied
toR1
ğ‘˜over a dataset with ğ‘‘â€²features, and the fact ğ‘‘â€²=Ãğ‘§
ğ‘–=1 ğ‘‘
ğ‘–â‰¤
(ğ‘’ğ‘‘
ğ‘§)ğ‘§, we obtain
ğ‘‰ğ¶(Rğ‘§
ğ‘˜)â‰¤
ğ‘˜log2 2ğ‘‘â€²+2
=$
ğ‘˜log2 
2ğ‘§âˆ‘ï¸
ğ‘–=1ğ‘‘
ğ‘–!
+2%
â‰¤
ğ‘˜log2
2ğ‘’ğ‘‘
ğ‘§ğ‘§
+2
â‰¤
ğ‘˜ğ‘§log22ğ‘’ğ‘‘
ğ‘§
+2
,
and the statement follows. â–¡
A.2 Proofs of Section 4.2
To prove our results we use the following Chernoff bounds (see
Theorem 4.4 and 4.5 of [12]).
Theorem A.1. Letğ‘‹1,...ğ‘‹ğ‘šbe independent Poisson trials such
that Pr(ğ‘‹ğ‘–=1)=ğ‘ğ‘–. Letğ‘‹=Ãğ‘›
ğ‘–=1ğ‘‹ğ‘–andğœ‡=E[ğ‘‹]. Then the
following Chernoff bounds hold for any 0<ğ›¾<1:
Pr(ğ‘‹â‰¥(1+ğ›¾)ğœ‡)â‰¤exp(âˆ’ğœ‡ğ›¾2/3),
Pr(ğ‘‹â‰¤(1âˆ’ğ›¾)ğœ‡)â‰¤exp(âˆ’ğœ‡ğ›¾2/2).
 
2361Scalable Rule Lists Learning with Sampling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Proof of Theorem 4.6. We prove the first set of inequalities.
Letğ‘…be an arbitrary rule list from Rğ‘§
ğ‘˜, and define the functions
ğ‘”(ğ‘…,S)=â„“(ğ‘…,S)âˆ’ğ›¼|ğ‘…|, (7)
ğ‘”(ğ‘…,D)=â„“(ğ‘…,D)âˆ’ğ›¼|ğ‘…|. (8)
Note thatğ‘”(ğ‘…,D)andğ‘”(ğ‘…,S)are the non-regularized variants
of the loss functions â„“(ğ‘…,D)andâ„“(ğ‘…,S). It easy to show that
ES[ğ‘”(ğ‘…,S)]=ğ‘”(ğ‘…,D) â‰¤â„“(ğ‘…,D), and thatğ‘”(ğ‘…,S)is an aver-
age ofğ‘šbinary random variables with expectation ğ‘”(ğ‘…,D). From
an application of the Chernoff bound (Theorem A.1) to the random
variableğ‘=ğ‘šğ‘”(ğ‘…,S)withES[ğ‘]=ğ‘šğ‘”(ğ‘…,D), we have that, for
any0<ğ›¾<1,
Pr
ğ‘â‰¤(1âˆ’ğ›¾)E
S[ğ‘]
â‰¤exp
âˆ’E
S[ğ‘]ğ›¾2/2
. (9)
Fixing 0<ğ›¿â€²<1, imposing the r.h.s. of (9)to beâ‰¤ğ›¿â€², and solving
forğ›¾, gives
Pr 
ğ‘”(ğ‘…,S)+âˆšï¸‚
2ğ‘”(ğ‘…,D)ln(1/ğ›¿â€²)
ğ‘šâ‰¤ğ‘”(ğ‘…,D)!
â‰¤ğ›¿â€².
Define the events
ğ¸ğ‘…=â€œğ‘”(ğ‘…,S)+âˆšï¸‚
2ğ‘”(ğ‘…,D)ln(1/ğ›¿â€²)
ğ‘šâ‰¤ğ‘”(ğ‘…,D)â€,âˆ€ğ‘…âˆˆRğ‘§
ğ‘˜,
and the event ğ¸=â€œâˆƒğ‘…:ğ¸ğ‘…is trueâ€ . Note that Pr(ğ¸ğ‘…)â‰¤ğ›¿â€². We
want to prove that Pr(ğ¸)â‰¤ğ›¿/2. First, we observe that
Pr(ğ¸)=PrÃ˜
ğ‘…âˆˆRğ‘§
ğ‘˜ğ¸ğ‘…
.
Denote with ğ‘…1andğ‘…2two rule listsâˆˆRğ‘§
ğ‘˜such that the projec-
tionsğ‘‹(ğ‘…ğ‘–,D)ofğ‘…ğ‘–onDare equal:ğ‘‹(ğ‘…1,D)=ğ‘‹(ğ‘…2,D). Con-
sequently, it holds ğ‘”(ğ‘…1,D)=ğ‘”(ğ‘…2,D). Moreover, for all possible
samplesS, it holdsğ‘‹(ğ‘…1,S)=ğ‘‹(ğ‘…2,S)andğ‘”(ğ‘…1,S)=ğ‘”(ğ‘…2,S).
The observations above imply that the events ğ¸ğ‘…1andğ¸ğ‘…2are iden-
tical.
We now define the set C(Rğ‘§
ğ‘˜,D)âŠ†Rğ‘§
ğ‘˜as a cover of the rangeset
ËœRğ‘§
ğ‘˜as follows: fix an arbitrary, deterministic total order over all
elements ofRğ‘§
ğ‘˜; then, for every distinct projection ğ»âˆˆËœRğ‘§
ğ‘˜, let
{ğ‘…:ğ‘‹(ğ‘…,D)=ğ»,ğ‘…âˆˆRğ‘§
ğ‘˜}be the set of rule lists with projection
equal toğ». From this set, we pick the minimum element in the total
order overRğ‘§
ğ‘˜and we include it in C(Rğ‘§
ğ‘˜,D). Therefore,C(Rğ‘§
ğ‘˜,D)
contains a unique rule list for each distinct projection of the rangeset
ËœRğ‘§
ğ‘˜over the datasetD. This implies that, for any ğ‘…âˆˆRğ‘§
ğ‘˜, there is
an unique rule list ğ‘…â€²âˆˆC(Rğ‘§
ğ‘˜,D)such thatğ‘‹(ğ‘…,D)=ğ‘‹(ğ‘…â€²,D)and that the events ğ¸ğ‘…andğ¸ğ‘…â€²are identical. It follows
PrÃ˜
ğ‘…âˆˆRğ‘§
ğ‘˜ğ¸ğ‘…
=PrÃ˜
ğ‘…âˆˆC(Rğ‘§
ğ‘˜,D)ğ¸ğ‘…
,
as we can replace the union over all ğ‘…âˆˆRğ‘§
ğ‘˜with the union over
the coverC(Rğ‘§
ğ‘˜,D)containing rule lists with unique projections
onD. From the definitions of Î›(Rğ‘§
ğ‘˜,ğ‘›)andC(Rğ‘§
ğ‘˜,D), it holds
|C(Rğ‘§
ğ‘˜,D)|=|ËœRğ‘§
ğ‘˜(D)|â‰¤ Î›(Rğ‘§
ğ‘˜,ğ‘›).
From an union bound, we have
Pr(ğ¸)=PrÃ˜
ğ‘…âˆˆC(Rğ‘§
ğ‘˜,D)ğ¸ğ‘…
â‰¤âˆ‘ï¸
ğ‘…âˆˆC(Rğ‘§
ğ‘˜,D)Pr(ğ¸ğ‘…)â‰¤Î›(Rğ‘§
ğ‘˜,ğ‘›)ğ›¿â€².
Choosingğ›¿â€²=ğ›¿/(2Î›(Rğ‘§
ğ‘˜,ğ‘›)), we obtain that
ğ‘”(ğ‘…,D)â‰¤ğ‘”(ğ‘…,S)+vt
2ğ‘”(ğ‘…,D)
ğœ”+ln 2
ğ›¿
ğ‘š(10)
holds for all ğ‘…âˆˆRğ‘§
ğ‘˜with probabilityâ‰¥1âˆ’ğ›¿/2, since ln(ğ›¿â€²)â‰¤
ğœ”+ln 2
ğ›¿(following analogous derivations for the proof of Corol-
lary 4.2). The first set of inequalities is obtained from (10)after
addingğ›¼|ğ‘…|on both sides, from the fact that ğ‘”(ğ‘…,D)â‰¤â„“(ğ‘…,D),
after solving the quadratic inequality ğ‘¦â‰¤ğ‘¢+âˆšğ‘£ğ‘¦w.r.t.ğ‘¦, and after
straightforward computations.
We now prove the last inequality and the statement. Let ğ‘…â˜…
be an arbitrary rule list with â„“(ğ‘…â˜…,D)=minğ‘…âˆˆRğ‘§
ğ‘˜â„“(ğ‘…,D). Note
thatğ‘…â˜…is fixed and independent of the choice of S. We apply
the Chernoff bound to the random variable ğ‘=ğ‘šğ‘”(ğ‘…â˜…,S)with
ES[ğ‘]=ğ‘šğ‘”(ğ‘…â˜…,D), obtaining for any 0<ğ›¾<1
Pr
ğ‘â‰¥(1+ğ›¾)E
S[ğ‘]
â‰¤exp
âˆ’E
S[ğ‘]ğ›¾2/3
. (11)
Setting the r.h.s. of (11)â‰¤ğ›¿/2, and solving for ğ›¾usingğ›¼â‰¥0, gives
Pr 
ğ‘”(ğ‘…â˜…,S)â‰¥ğ‘”(ğ‘…â˜…,D)+âˆšï¸‚
3ğ‘”(ğ‘…â˜…,D)ln(2/ğ›¿)
ğ‘š!
â‰¤ğ›¿/2,
and, equivalently,
ğ‘”(ğ‘…â˜…,S)â‰¤ğ‘”(ğ‘…â˜…,D)+âˆšï¸‚
3ğ‘”(ğ‘…â˜…,D)ln(2/ğ›¿)
ğ‘š
with probabilityâ‰¥1âˆ’ğ›¿/2. We obtain the last inequality of the
statement after adding ğ›¼|ğ‘…â˜…|to both sides, and from the fact
ğ‘”(ğ‘…â˜…,D) â‰¤â„“(ğ‘…â˜…,D). From an union bound, all inequalities of
the theorem are simultaneously valid with probability â‰¥1âˆ’ğ›¿,
obtaining the statement. â–¡
 
2362KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
102
 6Ã—103
2Ã—102
104105Number of samples
Number of samples (=1.0)
a9a adult bank higgs ijcnn1 mushroom phishing susy
102
 6Ã—103
2Ã—102
104105Number of samples
Number of samples (=1.0)
(
a)
102
 2Ã—102
3Ã—102
4Ã—102
104105Number of samples
Number of samples (=0.5)
 (b)
102
 2Ã—102
3Ã—102
4Ã—102
104105Number of samples
Number of samples (=0.25)
 (c)
Figure 5: Number of samples Ë†ğ‘š(Rğ‘§
ğ‘˜,D)used by SamRuLe varyingğœ€andğœƒfor all datasets. ğ‘˜is set as in Table 1 and ğ‘§=1.
102
105
104
Average deviations
Average deviations (susy)
SamRuLe (=0.25)
SamRuLe (=0.5)
SamRuLe (=1.0)
 CORELS
102
102103104Running time (s)
Running time (a9a)
102
103104Running time (s)
Running time (adult)
102
102103104Running time (s)
Running time (bank)
102
102103104Running time (s)
Running time (higgs)
102
102103104Running time (s)
Running time (ijcnn1)
102
102103Running time (s)
Running time (mushroom)
102
103104Running time (s)
Running time (phishing)
102
102
101
100101Running time (s)
Running time (susy)
Figure 6: Running times of SamRuLe and CORELS for different values of ğœ€andğœƒon all datasets.
102
105
104
103
102
101
Average deviations
Average deviations (a9a)
102
104
103
102
101
Average deviations
Average deviations (adult)
102
105
104
103
102
101
Average deviations
Average deviations (bank)
102
104
103
102
101
Average deviations
Average deviations (higgs)
102
104
103
102
101
Average deviations
Average deviations (ijcnn1)
102
105
104
103
Average deviations
Average deviations (mushroom)
102
105
104
103
102
101
Average deviations
Average deviations (phishing)
102
105
104
Average deviations
Average deviations (susy)
Figure 7: Average deviations |â„“(Ëœğ‘…,D)âˆ’â„“(ğ‘…â˜…,D)|over all runs for different values of ğœ€andğœƒand for all datasets. The purple
horizontal line is drawn at ğ‘¦=â„“(ğ‘…â˜…,D)(the loss of the optimal rule list ğ‘…â˜…found by CORELS). (same legend of Figure 6.)
 
2363