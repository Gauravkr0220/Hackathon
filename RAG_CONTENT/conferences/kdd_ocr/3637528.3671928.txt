Fredformer: Frequency Debiased Transformer for Time Series
Forecasting
Xihao Piao*
SANKEN, Osaka University
Osaka, Japan
park88@sanken.osaka-u.ac.jpZheng Chen*
SANKEN, Osaka University
Osaka, Japan
chenz@sanken.osaka-u.ac.jpTaichi Murayama
SANKEN, Osaka University
Osaka, Japan
taichi@sanken.osaka-u.ac.jp
Yasuko Matsubara
SANKEN, Osaka University
Osaka, Japan
yasuko@sanken.osaka-u.ac.jpYasushi Sakurai
SANKEN, Osaka University
Osaka, Japan
yasushi@sanken.osaka-u.ac.jp
ABSTRACT
The Transformer model has shown leading performance in time se-
ries forecasting. Nevertheless, in some complex scenarios, it tends
to learn low-frequency features in the data and overlook high-
frequency features, showing a frequency bias. This bias prevents
the model from accurately capturing important high-frequency
data features. In this paper, we undertake empirical analyses to
understand this bias and discover that frequency bias results from
the model disproportionately focusing on frequency features with
higher energy. Based on our analysis, we formulate this bias and
propose Fredformer , a Transformer-based framework designed
to mitigate frequency bias by learning features equally across dif-
ferent frequency bands. This approach prevents the model from
overlooking lower amplitude features important for accurate fore-
casting. Extensive experiments show the effectiveness of our pro-
posed approach, which can outperform other baselines in differ-
ent real-world time-series datasets. Furthermore, we introduce a
lightweight variant of the Fredformer with an attention matrix
approximation, which achieves comparable performance but with
much fewer parameters and lower computation costs. The code is
available at: https://github.com/chenzRG/Fredformer
CCS CONCEPTS
â€¢Computing methodologies â†’Artificial intelligence; Neural
networks.
KEYWORDS
Time series forecasting, Deep learning
ACM Reference Format:
Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Ya-
sushi Sakurai. 2024. Fredformer: Frequency Debiased Transformer for
* Indicates corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671928Time Series Forecasting . In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671928
1 INTRODUCTION
Time series data are ubiquitous in everyday life. Forecasting time
series could provide insights for decision-making support, such
as potential traffic congestion [ 10] or changes in stock market
trends [ 33]. Accurate forecasting typically involves discerning vari-
ous informative temporal variations in historical observations, e.g.,
trends, seasonality, and fluctuations, which are consistent in fu-
ture time series [ 41]. Benefiting from the advancements in deep
learning, the community has seen great progress, particularly with
Transformer-based methods [ 38,40,48]. Successful methods of-
tentokenize time series with multiresolution, such as time points
[42] or sub-series [ 47], and model their dependencies leveraging the
self-attention mechanism. Several state-of-the-art (SOTA) baselines
have been proposed, namely PatchTST [ 28], Crossformer [ 47], and
iTransformer [23], and demonstrate impressive performance.
Despite their success, the effectiveness with which we can cap-
ture informative temporal variations remains a concern. From a data
perspective, a series of time observations is typically considered a
complex set of signals or waves that varies over time [ 13,17]. Vari-
ous temporal variations, manifested as different frequency waves,
such as low-frequency long-term periodicity or high-frequency
fluctuation, often co-occur and are intermixed in the real world
[19,20,41]. While tokenizing a time series may provide fine-grained
information for the model, the temporal variations in resulting
tokens or sub-series are also entangled. This issue may complicate
the feature extraction and forecasting performance. Existing works
have proposed frequency decomposition to represent the time se-
ries and deployed Transformers on new representation to explicitly
learn eventful frequency features [ 41,42]. Learning often incorpo-
rates feature selection strategies in the frequency domain, such as
top-K or random-K [ 40,49], to help Transformers better identify
more relevant frequencies. However, such heuristic selection may
introduce spurious, sub-optimal frequency correlations into the
model (seen in Figure 1(a)), inadvertently misleading the learning
process.
 
2400
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
(b) PatchTST (c) Ours (a) FEDformer
F
 F
A
FInput Ground Truth Forecasting
Figure 1: In contrast to a frequency modeling-based work FEDformer [ 49] and a SOTA work PatchTST [ 28], our model can
accurately capture more significant mid-to-high frequency components.
From a model perspective, researchers have recently noticed a
learning bias issue that is common in the Transformer. That is, the self-
attention mechanism often prioritizes low-frequency features at the
expense of high-frequency features [ 14,29,34,35]. This subtle issue
may also appear in time series forecasting, potentially biasing model
outcomes and leading to information losses. Figure 1(b) shows an
electricity case where the forecasting result successfully captures
low-frequency features, neglecting some consistent mid-to-high
frequencies. In practice, such high frequencies represent short-
term variations, e.g., periodicities over short durations, which serve
as good indicators for forecasting [ 10,16,33]. However, the low-
frequencies typically carry a substantial portion of the energy in the
spectrum and are dominant in time series. The amplitude of these
low-frequency components far exceeds that of higher frequencies
[50], which provides the Transformer with more observations. This
may raise the possibility of frequency bias in time series forecasting,
as the model might disproportionately learn from these dominant
low-frequency components.
This work explores one direction of capturing informative, com-
plex variations by frequency domain modeling for accurate time se-
ries forecasting. We introduce Fredformer , aFrequency-debiased
Transformer model. Fredformer follows the line of frequency
decomposition but further investigates how to facilitate the uses
of Transformers in learning frequency features. To improve the
effectiveness of our approach, we provide a comprehensive analy-
sis of frequency bias in time series forecasting and a strategy for
debiasing it. Our main contributions lie in three folds.
- Problem definition. We undertake empirical studies to investi-
gate how this bias is introduced into time series forecasting Trans-
formers. We observe that the main cause is the proportional dif-
ference between key frequency components. Notably, these key
components should be consistent in the historical and ground truth
of the forecasting. We also investigate the objective and key designs
that affect debiasing.
- Algorithmic design. OurFredformer has three pivotal compo-
nents: patching for the frequency band, sub-frequency-independent
normalization to mitigate proportional differences, and channel-
wise attention within each sub-frequency band for fairness learningof all frequencies and attention debiasing.
- Applicability. Fredformer undertakes NystrÃ¶m approximation
to reduce the computational complexity of the attention maps, thus
achieving a lightweight model with competitive performance. This
attempt opens new opportunities for efficient time series forecast-
ing.
Remark. This is the first paper to study the frequency bias issue
in time series forecasting. Extensive experimental results on eight
datasets show the effectiveness of Fredformer , which achieves
superior performance with 60 top-1 and 20 top-2 cases out of 80.
2 PRELIMINARY ANALYSIS
We present two cases to show (i)how frequency attributes of time
series data introduce bias into forecasting with the Transformer
model and(ii)an empirical analysis of the potential debiasing
strategy. This section introduces the notation and a metric for the
case studies in Sec. 2.1. The case analyses are detailed in Sec. 2.2.
2.1 Preliminary
Time Series Forecasting. LetX={ğ’™(ğ‘)
1,..., ğ’™(ğ‘)
ğ¿}ğ¶
ğ‘=1denote
a multivariate time series consisting of ğ¶channels, where each
channel records an independent ğ¿length look-back window. For
simplicity, we omit channel index ğ‘in subsequent discussions. The
forecasting task is to predict ğ»time steps in the future data Ë†X:
Ë†Xğ¿+1:ğ¿+ğ»=ğ‘“(X1:ğ¿)
whereğ‘“(Â·)denotes the forecasting function, which is a Transformer-
based model in this work. Our objective is to mitigate the learning
bias in the Transformer and enhance the forecasting outcome Xâ€²,
that is, to minimize the error between Xâ€²and Ë†X.
Discrete Fourier Transform (DFT). We use DFT to analyze the
frequency content of X,Ë†X, andXâ€². For example, given the input
sequence{ğ’™1,...,ğ’™ğ¿}, the DFT can be formulated as
ğ’‚ğ‘˜=1
ğ¿ğ¿âˆ‘ï¸
ğ‘™=1ğ’™ğ‘™Â·ğ‘“ğ‘˜, ğ‘˜=1,...,ğ¿
 
2401Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Relative Error0.9 0.1Ground Truth Input
 Forecasting
(b)Time domain model
+ Non-normalizationTime domain model
+ NormalizationFrequency domain  
+ NormalizationF F F
(a)#epochk2k1
k31 50 #epoch 1 50k2k1
k3
Fk2k1
k3
k2
k1k3
F
Figure 2: Figure (a) shows the learning dynamics and results
for two synthetic datasets, employing line graphs to illus-
trate amplitudes in the frequency domain and heatmaps to
represent training epoch errors. Figure (b) explores the in-
fluence of amplitude and domain on learning by comparing
Transformers in the time and frequency domains, both with
and without frequency local normalization.
whereğ‘“ğ‘˜=ğ‘’âˆ’ğ‘–2ğœ‹ğ‘˜/ğ¿denotes the ğ‘˜-th frequency component. The
DFT coefficients A={ğ’‚1,ğ’‚2,..., ğ’‚ğ¿}represent the amplitude in-
formation of these frequencies. As illustrated in Figure 2 (b, left),
four components are observed to have higher amplitudes in the
historical observations (X) and the forecasting data ( Ë†X). We refer
to such consistent components as â€™key components â€™ (defined in Sec.
3.1). Here, the inverse DFT (i.e., IDFT) is ğ’™ğ‘™=Ãğ¿
ğ‘˜=1ğ’‚ğ‘˜Â·ğ‘“âˆ’1
ğ‘˜, which
reconstructs the time series data from the DFT coefficients.
Frequency Bias Metric. Inspired by the work of [ 5,45], this study
employs a Fourier analytic metric of relative error Î”ğ‘˜to determine
the frequency bias. Given the model outputs Aâ€²and the ground
truth Ë†A, the mean-square error (MSE) for the ğ‘˜-th component is
calculated as follows: MSEğ‘˜=|ğ’‚â€²ğ‘˜âˆ’Ë†ğ’‚ğ‘˜|, where|Â·|denotes the L2
norm of a complex number. Then, the relative error is applied to
mitigate scale differences. In other words, the error may become
larger as the proportion of amplitude increases.
Î”ğ‘˜=|ğ’‚â€²ğ‘˜âˆ’Ë†ğ’‚ğ‘˜|/|Ë†ğ’‚ğ‘˜|
This metric is used in case study analyses and the experiments
detailed in Section 5.2.2.2 Case Studies
We first generate single-channel time series data with a total length
of10000 timestamps and then employ a Transformer model [ 28]
to forecast the data. The details are in Appendix A. For the first
case study (Case 1), we generate two datasets with three key fre-
quency components ( {ğ‘˜1,ğ‘˜2,ğ‘˜3}). Each dataset contains a different
proportion of these three components, as illustrated in the DFT
visualization in Figure 2. On the left side of the figure, their ampli-
tudes are arranged as ğ’‚ğ‘˜1<ğ’‚ğ‘˜2<ğ’‚ğ‘˜3, whereas on the right side,
the arrangement is ğ’‚ğ‘˜1>ğ’‚ğ‘˜2>ğ’‚ğ‘˜3. We maintain these propor-
tions so that they are consistent between the observed Aand the
ground truth Ë†A(i.e.,Aâ‰ˆË†A). Then, we assess the bias for different
ğ‘˜in the Transformer outputs Aâ€². Meanwhile, we track how Î”ğ‘˜
changes during the model training to show the learning bias, using
heatmap values to represent the numerical values of Î”ğ‘˜.
Here, we generate a dataset with four key frequency components
for the second case study (Case 2). This study analyzes different
modeling strategies to investigate their flexibility for debiasing.
2.2.1 Investigating the Frequency Bias of Transformer (Case
1).As shown in Figure 2(a) (left), after 50 epochs of training, the
model successfully captures the amplitude of low-frequency com-
ponentğ‘˜1but fails to capture ğ‘˜2andğ‘˜3. Meanwhile, the heatmap
values show that the model predominantly focuses on learning
theğ‘˜1component. In other words, the relative error decreases
to around 0.01 (red codes) during the training. But, it lacks opti-
mization for ğ‘˜3, resulting in a high relative error of almost 0.95.
These observations indicate that signals in the time domain can be
represented by a series of frequency waves, typically dominated by
low-frequency components [ 19,26,30]. When the Transformer is
deployed on this mixed-frequency collection, the dominant propor-
tion of frequencies experiences a learning bias. A similar result is
also evident in the control experiment in the right Subfigure. Here,
we introduce synthetic data with higher amplitudes in the mid and
high-frequency ranges (resulting in ğ’‚ğ‘˜1<ğ’‚ğ‘˜2<ğ’‚ğ‘˜3). In response,
the model shifts its focus towards the key component ğ‘˜3, leading to
Î”ğ‘˜1>Î”ğ‘˜2>Î”ğ‘˜3. This learning bias aligns with recent theoretical
analyses of the Transformer model [ 29,34,35]. In addition, Sec. 3.1
provides a formal definition of this frequency bias.
2.2.2 Debiasing the Frequency Learning for Transformer
(Case 2). Based on the above discussion, we initially use the same
experimental settings for a new dataset, as shown in Figure 2(b)
(Left). We then perform two feasibility analyses for debiasing by (1)
mitigating the influence of high proportionality and (2) providing
the transformer with fine-grained frequency information.
(1) Frequency normalization: We first decompose the frequency
domain and normalize the amplitudes of the frequencies to elimi-
nate their proportional differences. Specifically, we apply the DFT,
normalize the amplitudes, and then use the IDFT to convert the fre-
quency representation back into the time domain before inputting
it into the Transformer, formulated as Xâ€²=(IDFT(Ağ‘›ğ‘œğ‘Ÿğ‘š)).
As depicted in Figure 2(b) (middle and right), the four input
components are adjusted so that they have the same amplitude
value, shown by a blue dashed line. The middle subfigure shows
that frequency normalization enhances the forecasting performance
for the latter three frequencies, but relative errors remain high.
 
2402KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
(2) Frequency domain modeling: We further directly deploy the
Transformer on the frequency domain to model the DFT matrix.
Subsequently, we apply the IDFT to return the forecasting outcome
to the time domain. Here, the purpose is to provide the transformer
with more refined and disentangled frequency features. Formally,
Xâ€²=IDFT((Ağ‘›ğ‘œğ‘Ÿğ‘š)). As shown in Figure 2(b) (right), there is a
marked improvement in forecasting accuracy for the latter three
frequency components. Notably, the bias in the second frequency
component (60-75 Hz) is effectively eliminated. These findings sug-
gest the potential for direct frequency domain modeling with
proportion mitigation in achieving the debiasing.
3 FREQUENCY BIAS FORMULATION
This section defines the frequency bias in Sec.3.1, then describes
the research problem in Sec.3.2.
3.1 Frequency Bias Definitions
Given the aforementioned empirical analyses, which demonstrate
that a frequency bias exists in key frequency components, we first
define these key components in terms of two properties: 1) a key
component should have a relatively high amplitude within the
spectrum, and 2) it should be consistent in historical observations
and future time series, as well as robust to time shifts [4, 30].
Definition 1.Key Frequency Components. Given a frequency
spectrum Awith lengthğ¿,Acan be segmented into ğ‘sub-frequency
bands{ğ’˜1,ğ’˜2,..., ğ’˜ğ‘}by a sliding window, where ğ’˜ğ‘›âˆˆR1Ã—ğ‘ƒ. The
maximum amplitude in the ğ‘›-th window is determined as follows:
max(ğ’˜ğ‘›)=max|ğ’‚ğ‘˜|:ğ’‚ğ‘˜âˆˆğ’˜ğ‘›forğ‘›=1,2,...,ğ‘ (1)
where ğ’˜ğ‘›denotesğ‘ƒamplitudes in the ğ‘›-th window. If ğ’‚ğ‘˜is a key
component in the ğ‘–-th window, then:
ğ’‚ğ‘˜=max(ğ’˜ğ‘›)and Ë†ğ’‚ğ‘˜=max(Ë†ğ’˜ğ‘›)
ËœAis a collection of all key components. Notably, ËœAshould be present
in historical Aand ground truth Ë†Afor accurate forecasting.
Definition 2.Frequency Bias in Transformer. Given that
a time series Xcontainsğ‘key frequency components amplitudes
ËœA={Ëœğ’‚1,..., Ëœğ’‚ğ‘}, for theğ‘˜-th component Ëœğ’‚ğ‘˜âˆˆËœA, we haveğ‘ƒ(Ëœğ’‚ğ‘˜)=
|Ëœğ’‚ğ‘˜|Ãğ‘
ğ‘›=1|Ëœğ’‚ğ‘›|, which refers to the proportion of Ëœğ’‚ğ‘˜in the total sum of
amplitudes of ËœA. Frequency bias can be defined as relative error Î”ğ‘˜.
Here, a larger proportion ğ‘ƒ(Ëœğ’‚ğ‘˜)leads to a smaller Î”ğ‘˜and exhibits a
higher ranking:
âˆ’|Î”ğ‘˜|âˆğ‘ƒ(Ëœğ’‚ğ‘˜) (2)
Eventually, the Transformer pays more attention to high-ranked
components during the training, as seen in Figure 2 (a) heatmaps.
3.2 Problem Statement
Based on the discussions in Sec. 2, we argue that if the Transformer
assigns attention to all key frequency components ËœAequally during
learning, then the frequency bias could be mitigated.
Problem 1.Debiasing Frequency Learning for Transformer.
Given a Transformer forecasting ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ (ğ‘‹), whereğ‘‹contains several
key frequency component Ëœğ’‚ğ‘˜, our goal is to debias ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  and improveforecasting performance by making the relative error Î”Ëœğ’‚ğ‘˜independent
ofğ‘ƒ(Ëœğ’‚ğ‘˜):
âˆ’|Î”ğ‘˜|Ì¸âˆğ‘ƒ(Ëœğ’‚ğ‘˜) (3)
thereby ensuring a balanced response by the Transformer to different
key frequency components.
4 FREDFORMER
Here, we discuss how to tackle the problem formulated in Sec.3.2
and propose Fredformer , aFrequency debiased Transformer model
for accurate time series forecasting.
Architecture Overview. Fredformer consists of four principal
components:(i)a DFT-to-IDFT backbone, (ii)frequency domain
refinement,(iii)local frequency independent learning, and (iv)
global semantic frequency summarization. Figure 3 shows an ar-
chitectural overview. The DFT-to-IDFT backbone breaks down the
input time series Xinto its frequency components using DFT and
learns a debiased representation of key frequency components by
modules(ii)(iii)and(iv). Based on the discussion in Sec. 2.2.2
(2), where we noted the significant potential of frequency modeling
for debiasing, we first refine the overall frequency spectrum into
sub-frequencies, which we achieve through a patching operation
on DFT coefficients. Patches from different channels within the
same sub-frequency band are embedded as tokens. That is, each
sub-frequency band is encoded independently, which avoids the
influence of other frequency components, as discussed in Section
2.2.2 (1). We deploy the Transformer to extract local frequency
features for each sub-band across all channels. This mitigates the
higher proportion crux defined in Def. 2. Finally, we summarize all
the frequency information, which serves as IDFT for forecasting.
Below, we provide a description of each module.
4.1 Backbone
Given X, we first use DFT to decompose Xinto frequency coeffi-
cients A1for all channels. We then extract the debiased frequency
features by using a Transformer encoder to AâˆˆRğ¶Ã—ğ¿. The fre-
quency outputs are subsequently reconstructed to the time domain
signal Xâ€²by IDFT.
Xâ€²=IDFT(ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ (A)),A=DFT(X))
4.2 Frequency Refinement and Normalization
From the observations described in Sec. 2.2.2, we conclude that if
there are significant proportional differences between different ËœAğ‘˜
values in the input data, it will lead to the model overly focusing
on components with larger amplitudes. To address this issue, we
propose frequency refinement and normalization. Specifically, a
non-overlapping patching operation is applied to Aalong theğ¶-
axis (i.e., channel), resulting in a sequence of local sub-frequencies
as follows:
W={W1,W2,...,Wğ‘}=Patching(A),Wğ‘›âˆˆRğ¶Ã—ğ‘ƒ
whereğ‘is the total number of the patches, while ğ‘ƒrepresents
the length of each patch. Mitigating information redundancy over
1Aconsists of two coefficient matrices: a real part RâˆˆRğ¶Ã—ğ¿and an imaginary matrix
IâˆˆRğ¶Ã—ğ¿. Since all operations are conducted synchronously for these two matrices,
we will refer to them as Ain our subsequent discussions.
 
2403Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
DFTIDFT
Frequency Domain
Modeling Transformer
EncoderFrequency
Summarization
Frequency
Channel-W ise
Attention
NormLinear Linear
Figure 3: Overview of our framework. Fredformer employs
DFT to transform input sequences into the frequency do-
main, normalizes locally, and segments into patches before
employing channel-wise attention, yielding final predictions
through a frequency-wise summarizing layer and IDFT.
fine-grained frequency bands, such as neighboring 1 Hz and 2 Hz,
allows the model to learn the local features in each sub-frequency.
Parameterğ‘†is adaptable to the requirements of real-world scenarios,
for example, an hourly sampling of daily recordings or the alpha
waveform typically occurring at 8-12 Hz [8].
Since patching operation allows the model to manage each Mğ‘›
independently, we further normalize each Wğ‘›along theğ‘-axis:
Wâˆ—ğ‘›=ğœ(Wğ‘›)ğ‘›=1,2,...,ğ‘, whereğœ(Â·)denotes the normaliza-
tion, and it further projects the numerical value of each ËœAğ‘˜into a
range of 0-1. This operation eliminates proportionate differences
in the maximum values within sub-frequency bands, thereby main-
taining an equal Î”across all key components ËœA.
Lemma 1.Frequency-wise Local Normalization: Given fre-
quency patchesâˆ€Wğ‘›,Wğ‘šâˆˆWformax(Wğ‘›)>max(Wğ‘š)and
ğœ(Â·), the normalization strategy is defined by:
Wâˆ—={ğœ(W1),...,ğœ(Wğ‘)}
This ensures that within each localized frequency patch Wğ‘›, the am-
plitude differences between key frequency components are minimized,
promoting equal attention to all key frequencies by the model:
max(Wâˆ—
ğ‘›)=max(Wâˆ—
ğ‘š)
Some studies also introduce patching operations in the time do-
main and perform normalization within these time domain patches
[28]. However, according to Parsevalâ€™s theorem [ 31], normalization
within time domain patches is equivalent to normalizing across
all frequencies. This could not address the issue of amplitude bias
among key frequency components.
4.3 Frequency Local Independent Modeling
Given the normalized Wâˆ—, we deploy frequency local independent
Transformer encoders to learn the importance of each Wâˆ—ğ‘›inde-
pendently. For W(1:ğ¶)
ğ‘› ={ğ’˜(1)
ğ‘›,ğ’˜(2)
ğ‘›,..., ğ’˜(ğ¶)
ğ‘›}ğ‘
ğ‘›=1, a Transformerencoderğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ (Â·)accepts each ğ’˜âˆ—(ğ‘)
ğ‘›as an input token:
{Wâ€²(1:ğ¶)
ğ‘›}=ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ (Wâˆ—(1:ğ¶)
ğ‘›)
where Wâ€²(1:ğ¶)
ğ‘› is encoded by a channel-wise self-attention encoder,
formally:
Attention(Qğ‘›,Kğ‘›,Vğ‘›)=
Softmax 
Wâˆ—(1:ğ¶)
ğ‘›Wğ‘
ğ‘›(Wâˆ—(1:ğ¶)
ğ‘›Wğ‘˜ğ‘›)ğ‘‡
âˆš
ğ‘‘!
Wâˆ—(1:ğ¶)
ğ‘›Wğ‘£
ğ‘›
where Wğ‘
ğ‘›,Wğ‘˜ğ‘›,Wğ‘£ğ‘›âˆˆRğ‘†Ã—ğ‘€are the weight matrices for generat-
ing the query matrix Qğ‘›, key matrix Kğ‘›, and value matrix Vğ‘›.âˆš
ğ‘‘
denotes a scaling operation. The attention module also includes
normalization and a feed-forward layer with residual connections
[12], and Attention(Qğ‘›,Kğ‘›,Vğ‘›)âˆˆRğ¶Ã—ğ‘€weights the correlations
amongğ¶channels for the ğ‘›-th sub-frequency band Mğ‘›. This de-
sign ensures that the features of each sub-frequency are calculated
independently, preventing learning bias.
Lemma 2.Given Wâˆ—(1:ğ¶)
ğ‘› ={ğ’˜âˆ—(1)
ğ‘›,ğ’˜âˆ—(2)
ğ‘›,..., ğ’˜âˆ—(ğ¶)
ğ‘›}ğ‘
ğ‘›=1, if
Wâ€²ğ‘›=ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ (Wâˆ—(1:ğ¶)
ğ‘›), then by modeling the relationships of iden-
tical frequencies ğ’˜ğ‘ğ‘›across different channels, for the ğ‘˜-th key com-
ponent Ëœğ’‚ğ‘˜presents in ğ’˜(ğ‘)
ğ‘›, we haveâˆ’|Î”(ğ‘)
ğ‘˜|âˆ{|Î”(ğ‘)
ğ‘˜|}ğ¶
ğ‘=1. The
Transformer encoders will focus on channel-wise correlations instead
of the{|Î”(ğ‘)
ğ‘˜|}ğ¾
ğ‘˜=1, i.e., debiasingâˆ’|Î”(ğ‘)
ğ‘˜|Ì¸âˆğ‘ƒ(Ëœğ’‚ğ‘˜).
Lemma 2, which indicates a lower ğ‘ƒ(Ëœğ’‚ğ‘˜)does not necessarily lead
to an increase in|Î”Ëœğ’‚ğ‘˜|, thus avoiding disproportionate attention
to frequency components. Channel-wise attention is proposed in
the work of [ 23,47]. We include these studies as the baselines and
the results in Sec. 5.2. In this work, we have different modeling
purposes; we deploy self-attention on the aligned local features,
i.e., in the same frequency bands across channels, for frequency
debiasing.
4.4 Frequency-wise Summarization
Given the learned features of the sub-frequencies Wâ€²={ğ’˜â€²
1,ğ’˜â€²
2,..., ğ’˜â€²
ğ‘}
of the historical time series X, the frequency-wise summarizing
operation contains linear projections and IDFT:
Xâ€²=IDFT(Aâ€²)Aâ€²=Linear(Wâ€²)
where Xâ€²âˆˆRğ¶Ã—ğ»is the final output of the framework.
Table 1: Benchmark dataset summary
Datasets W
eather Electricity ETTh1 ETTh2 ETTm1 ETTm2 Solar Traffic
#Channel 21
321 7 7 7 7 137 862
#Timesteps 52969
26304 17420 17420 69680 69680 52179 17544
5 EXPERIMENTS
5.1 PROTOCOLS
- Datasets. We conduct extensive experiments on eight real-world
benchmark datasets: Weather, four ETT datasets (ETTh1, ETTh2,
ETTm1, ETTm2), Electricity (ECL), Traffic, and the Solar-Energy
 
2404KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
Table 2: Multivariate forecasting results with prediction lengths ğ‘†âˆˆ{96,192,336,720}for all datasets and fixed look-back length
ğ‘‡=96. The best and second best results are highlighted. The full results of four selected datasets* will be shown in Figure 3.
Results are averaged from all prediction lengths. Full results for all datasets are listed in Appendix C.
Mo
delsFredformer iT
ransformer RLinear PatchTST Cr
ossformer TiDE TimesNet DLinear SCINet FEDformer Stationar
yA
utoformer
(
Ours) ([23]) [21] [28] [47] [11] [41] [46] [25] [49] [24] [42]
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
ECL 0.175 0.269 0.178 0.270 0.219 0.298 0.216 0.304 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 0.268 0.365 0.214 0.327 0.193 0.296 0.227 0.338
ET
Th1 0.435 0.426 0.454 0.447 0.446 0.434 0.469 0.454 0.529 0.522 0.541 0.507 0.548 0.450 0.456 0.452 0.747 0.647 0.440 0.460 0.570 0.537 0.496 0.487
ET
Th2* 0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459
ET
Tm1* 0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517
ET
Tm2 0.279 0.324 0.288 0.332 0.286 0.327 0.281 0.326 0.757 0.610 0.358 0.404 0.291 0.333 0.350 0.401 0.571 0.537 0.305 0.349 0.306 0.347 0.327 0.371
T
raffic 0.431 0.287 0.428 0.282 0.626 0.378 0.555 0.362 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 0.804 0.509 0.610 0.376 0.624 0.340 0.628 0.379
W
eather * 0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382
Solar-Energy *0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711
Table 3: Full results of four selected datasets, with the best and second best results are highlighted. We compare extensive
competitive models under different prediction lengths following the setting of iTransformer [ 23]. The input sequence length is
set to 96 for all baselines. Avg means the average results from all four prediction lengths.
Mo
delsFredformer iT
ransformer RLinear PatchTST Cr
ossformer TiDE TimesNet DLinear SCINet FEDformer Stationar
yA
utoformer
(
Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021]
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEET
Th296 0.293 0.342 0.297 0.349 0.288 0.338 0.302 0.348 0.745 0.584 0.400 0.440 0.340 0.374 0.333 0.387 0.707 0.621 0.358 0.397 0.476 0.458 0.346 0.388
192 0.371 0.389 0.380 0.400 0.374 0.390 0.388 0.400 0.877 0.656 0.528 0.509 0.402 0.414 0.477 0.476 0.860 0.689 0.429 0.439 0.512 0.493 0.456 0.452
336 0.382 0.409 0.428 0.432 0.415 0.426 0.426 0.433 1.043 0.731 0.643 0.571 0.452 0.452 0.594 0.541 1.000 0.744 0.496 0.487 0.552 0.551 0.482 0.486
720 0.415 0.434 0.427 0.445 0.420 0.440 0.431 0.446 1.104 0.763 0.874 0.679 0.462 0.468 0.831 0.657 1.249 0.838 0.463 0.474 0.562 0.560 0.515 0.511
A
vg0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459ET
Tm196 0.326 0.361 0.334 0.368 0.355 0.376 0.329 0.367 0.404 0.426 0.364 0.387 0.338 0.375 0.345 0.372 0.418 0.438 0.379 0.419 0.386 0.398 0.505 0.475
192 0.363 0.380 0.377 0.391 0.391 0.392 0.367 0.385 0.450 0.451 0.398 0.404 0.374 0.387 0.380 0.389 0.439 0.450 0.426 0.441 0.459 0.444 0.553 0.496
336 0.395 0.403 0.426 0.420 0.424 0.415 0.399 0.410 0.532 0.515 0.428 0.425 0.410 0.411 0.413 0.413 0.490 0.485 0.445 0.459 0.495 0.464 0.621 0.537
720 0.453 0.438 0.491 0.459 0.487 0.450 0.454 0.439 0.666 0.589 0.487 0.461 0.478 0.450 0.474 0.453 0.595 0.550 0.543 0.490 0.585 0.516 0.671 0.561
A
vg0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517W
eather96 0.163 0.207 0.174 0.214 0.192 0.232 0.177 0.218 0.158 0.230 0.202 0.261 0.172 0.220 0.196 0.255 0.221 0.306 0.217 0.296 0.173 0.223 0.266 0.336
192 0.211 0.251 0.221 0.254 0.240 0.271 0.225 0.259 0.206 0.277 0.242 0.298 0.219 0.261 0.237 0.296 0.261 0.340 0.276 0.336 0.245 0.285 0.307 0.367
336 0.267 0.292 0.278 0.296 0.292 0.307 0.278 0.297 0.272 0.335 0.287 0.335 0.280 0.306 0.283 0.335 0.309 0.378 0.339 0.380 0.321 0.338 0.359 0.395
720 0.343 0.341 0.358 0.349 0.364 0.353 0.354 0.348 0.398 0.418 0.351 0.386 0.365 0.359 0.345 0.381 0.377 0.427 0.403 0.428 0.414 0.410 0.419 0.428
A
vg0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382Solar-Energy96 0.185 0.233 0.203 0.237 0.322 0.339 0.234 0.286 0.310 0.331 0.312 0.399 0.250 0.292 0.290 0.378 0.237 0.344 0.242 0.342 0.215 0.249 0.884 0.711
192 0.227 0.253 0.233 0.261 0.359 0.356 0.267 0.310 0.734 0.725 0.339 0.416 0.296 0.318 0.320 0.398 0.280 0.380 0.285 0.380 0.254 0.272 0.834 0.692
336 0.246 0.284 0.248 0.273 0.397 0.369 0.290 0.315 0.750 0.735 0.368 0.430 0.319 0.330 0.353 0.415 0.304 0.389 0.282 0.376 0.290 0.296 0.941 0.723
720 0.247 0.276 0.249 0.275 0.397 0.356 0.289 0.317 0.769 0.765 0.370 0.425 0.338 0.337 0.356 0.413 0.308 0.388 0.357 0.427 0.285 0.295 0.882 0.717
A
vg0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711
1stCount 17 17 0 2 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
dataset [ 19], with all datasets being published in [ 23]2. The infor-
mation these datasets provide is summarized in Table 1. And the
full results of four selected datasets* will be shown in Figure 3.
- Baselines. We select 11 SOTA baseline studies. Since we are focus-
ing on Transformer, we first add seven proposed Transformer-based
2https://github.com/thuml/iTransformerbaselines, including iTransformer [ 23], PatchTST [ 28], Crossformer
[47], Stationary [ 24], Fedformer [ 49], Pyraformer [ 22], Autoformer
[42]. We also add 2 MLP-based and 2 TCN-based methods, including
RLinear [21], DLinear [46], TiDE [11], TimesNet [41].
- Setup and Evaluation. All baselines use the same prediction
length with ğ»âˆˆ{96,192,336,720}for all datasets. The look-back
 
2405Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
FEDformer#epoch 1 50
F 65 115 0k1
k2k3k4
k1
k2
k3
k4
PatchTST#epoch 1 50F 65 115 0k1
k2k3k4
k1
k2
k3
k4
iTransformer#epoch 1 50k1
k2
k3
k4
F 65 115 0k1
k2k3k4
Fredformer(Ours)k1
k2
k3
k4
#epoch 1 50650
 F115k1
k2k3k4Input Ground Truth Forecasting Relative Error0.9 0.1
Figure 4: Visualizations of the learning dynamics and results for Fredformer and baselines on the ETTh1 dataset, employing
line graphs to illustrate amplitudes in the frequency domain and heatmaps to represent training epoch errors.
windowğ¿= 96 was used in our setting for fair comparisons, refer-
ring to [ 23,49]. We used MSE and MAE as the forecasting metrics.
We further analyzed the forecasting results between the model
outputs and the ground truth in the time and frequency domains.
Using heatmaps, we tracked the way in which Î”ğ‘˜changes during
training to show the debiased results of Fredformer compared
with various SOTA baselines.
5.2 Results
Forecasting Results. Table 2 shows the average forecasting per-
formance across four prediction lengths. The best results are high-
lighted in bold, and the second-best results are underlined . With a
default look-back window of ğ¿=96, our approach realizes leading
performance levels on most datasets, securing 14 top-1 and 2 top-
2 positions across two metrics over eight datasets. More detailed
results for 4 of the eight datasets are shown in Table 3, where our
method achieves 34 top-1 and 6 top-2 rankings out of 40 possible
outcomes across the four prediction lengths. More comprehensive
results regarding the different prediction length settings on all
datasets are detailed in Appendix C.
Frequency Bias Evaluation. Figure 4 is a case study visualization
in the frequency domain, i.e., the DFT plot. The input, forecast out-
put, and ground truth data series are shown in blue, red, and green,
respectively. Similar to Section 2.2, the heat map shows the relative
error for four selected mid-to-high frequency components over
increasing epochs. After training, Fredformer accurately identifies
ğ‘˜1,ğ‘˜2, andğ‘˜3, with uniformly decreasing relative errors. Despite
a larger learning error for ğ‘˜4,Î”ğ‘˜4consistently diminishes. This
performance contrasts with all the baselines, demonstrating a lack
of effectiveness in capturing these frequency components, with
unequal reductions in relative errors. In contrast, PatchTST demon-
strates a sudden improvement in component accuracy ( ğ‘˜2,ğ‘˜3) dur-
ing the final stages of training. FEDformer fails to capture these
frequency components, possibly because its strategy of selecting
and learning weights for only a random set of ğ‘˜components over-
looks all unselected components. Notably, iTransformer overlooks
mid-to-high frequency features, partially learning components ğ‘˜1Table 4: Averaged results for each setting in the ablation
study. "No-CW" refers to removing channel-wise attention,
and "No-FR" refers to removing frequency refinement.
SettingFull No-CW No-FR
MSE
MAE MSE
MAE MSE
MAE
ET
Tm1 0.384
0.396 0.418
0.419 0.539
0.485
Weather 0.246
0.273 0.262
0.290 0.293
0.322
Table 5: The average forecasting accuracy (MSE) on ETTh1
dataset under 4 patch length settings.
Patch
length 8 16 32 Non
MSE 0.417 0.425 0.440 0.449
andğ‘˜3while ignoring ğ‘˜2andğ‘˜4, indicating a clear frequency bias.
This may stem from its use of channel-wise attention alongside
global normalization in the time domain, as discussed in Lemma 1
and further supported by our ablation study 5.3. This highlights
the effectiveness of frequency refinement and normalization.
5.3 Ablation Study
Channel-wise Attention and Frequency Refinement. We eval-
uate the effectiveness of channel-wise attention and frequency
refinement. To this end, we remove each component by ablation
and compare it with the original Fredformer . Table 4 shows that
our method consistently outperforms others in all experiments,
highlighting the importance of integrating channel-wise attention
with frequency local normalization in our design. Interestingly, em-
ploying frequency local normalization alone yields better accuracy
than channel-wise attention alone. This suggests that minimizing
proportional differences in amplitudes across various key frequency
components is crucial for enhancing accuracy.
 
2406KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
ECL ETTh1
MSE MSE
Mb MbiTransformer
PatchTSTFedformer
OursCrossformer
Ours*iTransformerPatchTSTCrossformer
Fedformer
Ours
Figure 5: Comparation of forecasting accuracy and com-
putational complexity (VRAM usage) among Transformer-
based methods, Fredformer (Ours), and its optimized variant,
NystrÃ¶m-Fredformer (Ours*).
Table 6: The theoretical computational complexity of
Transformer-based methods.
Method FEDformer PatchTST Crossformer
iTransformer Ours Ours*(NystrÃ¶m)
Complexity ğ‘‚(ğ¿ğ¶)ğ‘‚
ğ¿2
ğ‘ƒ2ğ¶
ğ‘‚
ğ¿2
ğ‘ƒ2ğ¶
ğ‘‚(ğ¶2)ğ‘‚
ğ¿
ğ‘ƒğ¶2
ğ‘‚
ğ¿
ğ‘ƒğ¶
Effect of Patch Length. This ablation evaluates the impact of
patch length using the ETTh1 dataset. We conduct four experi-
ments with ğ‘ƒ=[8,16,32,48]patch lengths and corresponding
patch numbers ğ‘=[6,3,2,1]. In this context, ğ‘=1means fre-
quency normalization and channel-wise attention are applied to the
entire spectrum without a patching operation. Table 5 shows the
forecasting accuracy for each setting. As the patch length increases,
the granularity of the frequency features extracted by the model
becomes coarser, decreasing forecasting accuracy.
5.4 Discussion of Applicability
Beyond algorithmic considerations, we further discuss the practi-
cal deployment of Fredformer in real-world scenarios, with the
primary challenge being memory consumption during model train-
ing. Theğ‘‚(ğ‘›2)complexity of self-attention limits the use of longer
historical time series for forecasting, generating the need for in-
novations to reduce computational demands [ 20,28,48]. Through
patching operations, we decrease the complexity from ğ‘‚(ğ¿ğ¶2)to
ğ‘‚(ğ¿
ğ‘ƒğ¶2), as shown in Table 6. However, our channel-wise attention
increases the computational costs with the number of channels,
potentially limiting practical applicability with many channels. To
address this, we propose a lightweight Fredformer , inspired by
NystrÃ¶mFormer [ 44], which applies a matrix approximation to the
attention map. This design allows us to further reduce our com-
plexity toğ‘‚(ğ¿
ğ‘ƒğ¶)without the need to modify the feature extraction
(attention computation) or the data stream structure within the
Transformer, unlike with previous methods [ 22,42,48,49]. Figure
5 shows a tradeoff between the model efficiency (VRAM usage) and
accuracy in our method and the baselines. The plain Fredformer
achieves high accuracy with low computational costs with fewer
channels, such as ETTh1 with 7 channels. However, as shown in theECL dataset (321 channels), the computational costs increase while
maintaining high accuracy as the channel number increases. Here,
NystrÃ¶m- Fredformer further reduces computational requirements
without compromising accuracy (the right sub-figure), showing
that our model can realize computational efficiency and forecasting
accuracy. Further details and derivations are in Appendix B.
6 RELATED WORKS
Transformer for Time Series Forecasting. Forecasting is impor-
tant in time series analysis [ 1,15]. Transformer has significantly
progressed in time series forecasting[ 18,28,47]. Earlier attempts
focused on improving the computational efficiency of Transform-
ers for time series forecasting tasks[ 3,22,48]. Several studies have
used Transformers to model inherent temporal dependencies in
the time domain of time series[ 20,22,23,28,48]. Various studies
have integrated frequency decomposition and spectrum analysis
with the Transformer in modeling temporal variations [ 40,42] to
improve the capacity for temporal-spatial representation. In [ 49],
attention layers are designed that directly function in the frequency
domain to enhance spatial or frequency representation.
Modeling Short-Term Variation in Time Series. Short-term
variations are intrinsic characteristics of time series data and play
a crucial role in effective forecasting [ 10,24]. Numerous deep
learning-based methods have been proposed to capture these tran-
sient patterns [ 2,7,9,27,32,36,37,39,42]. Here, we summarize
some studies closely aligned with our proposed method. Pyraformer
[22] applies a pyramidal attention module with inter-scale and intra-
scale connections to capture various temporal dependencies. FED-
former [ 49] incorporates a Fourier spectrum within the attention
computation to identify pivotal frequency components. Beyond
Transformers, TimesNet [ 41] employs Inception blocks to capture
intra-period and inter-period variations.
Channel-wise Correlation. Understanding the cross-channel cor-
relation is also critical for time series forecasting. Several studies
aimed to capture intra-channel temporal variations and model the
inter-channel correlations using Graph Neural Networks (GNNs)
[6,43]. Recently, Crossformer [ 47] and iTransformer [ 23] both
adopted channel-wise Transformer-based frameworks, and exten-
sive experimental results have demonstrated the effectiveness of
channel-wise attention for time series forecasting.
7 CONCLUSION
In this paper, we first empirically analyzed frequency bias, delving
into its causes and exploring debiasing strategies. We then provided
a formulation of this bias based on our analytical insights. We pro-
posed the Fredformer framework with three critical designs to
tackle this bias and thus ensure unbiased learning across frequency
bands. Our extensive experiments across eight datasets confirmed
the excellent performance of our proposed method. Visual anal-
ysis confirmed that our approach effectively mitigates frequency
bias. The model analysis further illustrated how our designs aid
frequency debiasing and offered preliminary guidelines for future
model design. Additionally, a lightweight variant of our model ad-
dresses computational efficiency, facilitating practical application.
 
2407Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
8 ACKNOWLEDGMENTS
We thank anonymous reviewers for their insightful comments
and discussions. This work is supported by JSPS KAKENHI Grant-
in-Aid for Scientific Research Number JP21H03446, JP23K16889,
JP24K20778, NICT JPJ012368C03501, JST-AIP JPMJCR21U4, JST-
CREST JPMJCR23M3, JST-RISTEX JPMJRS23L4.
REFERENCES
[1]Rob J. Hyndman Alysha M. De Livera and Ralph D. Snyder. 2011. Forecasting
Time Series With Complex Seasonal Patterns Using Exponential Smoothing. J.
Amer. Statist. Assoc. (2011), 1513â€“1527.
[2]Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. Convolutional Sequence
Modeling Revisited. (2018).
[3]Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-
Document Transformer. (2020). arXiv:2004.05150 [cs.CL]
[4]S.A. Broughton and K. Bryan. 2011. Discrete Fourier Analysis and Wavelets:
Applications to Signal and Image Processing. (2011).
[5]Daniela Calvetti. 1991. A Stochastic Roundoff Error Analysis for the Fast Fourier
Transform. (1991).
[6]Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Conguri Huang,
Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2021. Spectral Tem-
poral Graph Neural Network for Multivariate Time-series Forecasting. (2021).
[7]Yen-Yu Chang, Fan-Yun Sun, Yueh-Hua Wu, and Shou-De Lin. 2018. A Memory-
Network Based Solution for Multivariate Time-Series Forecasting. (2018).
arXiv:1809.02105 [cs.LG]
[8]Zheng Chen, Ziwei Yang, Lingwei Zhu, Wei Chen, Toshiyo Tamura, Naoaki
Ono, Md Altaf-Ul-Amin, Shigehiko Kanaya, and Ming Huang. 2023. Automated
Sleep Staging via Parallel Frequency-Cut Attention. IEEE Transactions on Neural
Systems and Rehabilitation Engineering (2023), 1974â€“1985.
[9]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.
(2014). arXiv:1412.3555 [cs.NE]
[10] Jesus Crespo Cuaresma, Jaroslava Hlouskova, Stephan Kossmeier, and Michael
Obersteiner. 2004. Forecasting Electricity Spot-Prices Using Linear Univariate
Time-Series Models. Applied Energy 77 (2004), 87â€“106.
[11] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and
Rose Yu. 2023. Long-term Forecasting with TiDE: Time-series Dense Encoder.
Transactions on Machine Learning Research (2023).
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is
Worth 16x16 Words: Transformers for Image Recognition at Scale. In International
Conference on Learning Representations.
[13] Filip Elvander and Andreas Jakobsson. 2020. Defining Fundamental Frequency
for Almost Harmonic Signals. IEEE TRANSACTIONS ON SIGNAL PROCESSING
(2020).
[14] Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. 2023. ContraNorm: A
Contrastive Learning Perspective on Oversmoothing and Beyond. (2023).
[15] James D Hamilton. 2020. Time series analysis. (2020).
[16] Nicholas W. Hammond, FranÃ§ois Birgand, Cayelan C. Carey, Bethany Bookout,
Adrienne Breef-Pilz, and Madeline E. Schreiber. 2023. High-frequency Sensor Data
Capture Short-term Variability In Fe and Mn Concentrations Due to Hypolimnetic
Oxygenation and Seasonal Dynamics in a Drinking Water Reservoir. Water
Research 240 (2023).
[17] Long Steven R. Wu Manli C. Shih Hsing H. Zheng Quanan Yen Nai-Chyuan
Tung Chi Chao Huang Norden E. Shen Zheng and Liu Henry H. 1998. The
Empirical Mode Decomposition and the Hilbert Spectrum for Nonlinear and
Non-stationary Time Series Analysis. Proceedings of the Royal Society of London.
Series A: mathematical, physical, and engineering sciences (1998), 903â€“995.
[18] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.
PDFormer: Propagation Delay-aware Dynamic Long-range Transformer for Traf-
fic Flow Prediction. (2023), 4365â€“4373.
[19] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling
Long- and Short-Term Temporal Patterns with Deep Neural Networks. (2018),
95â€“104.
[20] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,
and Xifeng Yan. 2019. Enhancing the Locality and Breaking the Memory Bottle-
neck of Transformer on Time Series Forecasting. (2019).
[21] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. 2023. Revisiting Long-term Time
Series Forecasting: An Investigation on Linear Mapping. (2023).
[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and
Schahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for
Long-Range Time Series Modeling and Forecasting. (2022).[23] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and
Mingsheng Long. 2024. iTransformer: Inverted Transformers Are Effective for
Time Series Forecasting. In The Twelfth International Conference on Learning
Representations.
[24] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary
Transformers: Exploring the Stationarity in Time Series Forecasting. (2022).
[25] Liu M., Zeng A., Chen M., Xu Z., Lai Q., Ma L., and Q. Xu. 2022. SCINet: Time
Series Modeling and Forecasting with Sample Convolution and Interaction. (2022),
5816â€“5828.
[26] Sobhan Moosavi, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan
Parthasarathy, and Rajiv Ramnath. 2019. Short and Long-Term Pattern Discovery
Over Large-Scale Geo-Spatiotemporal Data. (2019), 2905â€“2913.
[27] Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. 2016. Phased LSTM: Acceler-
ating Recurrent Network Training for Long or Event-based Sequences. (2016).
arXiv:1610.09513 [cs.LG]
[28] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.
(2023).
[29] Namuk Park and Songkuk Kim. 2022. How Do Vision Transformers Work?
(2022).
[30] John G. Proakis and Dimitris G. Manolakis. 1996. Digital Signal Processing (3rd
Ed.): Principles, Algorithms, and Applications. (1996).
[31] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
Hamprecht, Yoshua Bengio, and Aaron Courville. 2019. On the Spectral Bias of
Neural Networks. 97 (2019), 5301â€“5310.
[32] Daniel Stoller, Mi Tian, Sebastian Ewert, and Simon Dixon. 2019. Seq-U-Net:
A One-Dimensional Causal U-Net for Efficient Sequence Modelling. (2019).
arXiv:1911.06393 [cs.LG]
[33] James R. Thompson and James R. Wilson. 2016. Multifractal Detrended Fluctua-
tion Analysis: Practical Applications to Financial Time Series. Mathematics and
Computers in Simulation 126 (2016), 63â€“88.
[34] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. 2023. Scan and Snap:
Understanding Training Dynamics and Token Composition in 1-layer Trans-
former. (2023).
[35] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. 2022. Anti-
Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis:
From Theory to Practice. (2022).
[36] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and
Fan Zhou. 2022. Learning Latent Seasonal-Trend Representations for Time Series
Forecasting. (2022).
[37] Qingsong Wen, Zhe Zhang, Yan Li, and Liang Sun. 2020. Fast RobustSTL: Effi-
cient and Robust Seasonal-Trend Decomposition for Time Series with Complex
Patterns. (2020), 2203â€“2213.
[38] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,
and Liang Sun. 2023. Transformers in Time Series: A Survey. (2023).
[39] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022.
CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for
Time Series Forecasting. (2022).
[40] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H.
Hoi. 2022. ETSformer: Exponential Smoothing Transformers for Time-series
Forecasting. (2022).
[41] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series
Analysis. (2023).
[42] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer:
Decomposition Transformers with Auto-Correlation for Long-Term Series Fore-
casting. (2021).
[43] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi
Zhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with
Graph Neural Networks. (2020).
[44] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn
Fung, Yin Li, and Vikas Singh. 2021. NystrÃ¶mformer: A NystrÃ¶m-based Algorithm
for Approximating Self-Attention. (2021).
[45] Zhi-Qin John Xu. 2020. Frequency Principle: Fourier Analysis Sheds Light on
Deep Neural Networks. Communications in Computational Physics 28 (2020),
1746â€“1767.
[46] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers
Effective for Time Series Forecasting? (2023).
[47] Yunhao Zhang and Junchi Yan. 2023. Crossformer: Transformer Utilizing Cross-
Dimension Dependency for Multivariate Time Series Forecasting. (2023).
[48] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long
Sequence Time-Series Forecasting. (2021).
[49] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.
FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series
Forecasting. (2022), 1â€“12.
[50] Yunyue Zhu and Dennis Shasha. 2002. StatStream: Statistical Monitoring of
Thousands of Data Streams in Real Time. (2002), 358â€“369.
 
2408KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
The full appendix can be found at: http://arxiv.org/abs/2406.09009
A DETAILS OF THE CASE STUDIES
Here, we illustrate the details of how we generated the data for
case study 2 in Sec.2.2.2: The generation of data for Case Study
2 from the original time series involves a sequence of steps to
emphasize certain frequency components by manipulating their
positions in the frequency domain. This process not only constructs
a dataset with distinct frequency characteristics but also preserves
the inherent noise and instability of the real data, enhancing the
robustness and credibility of subsequent analyses. Specifically, the
steps are as follows:
(1)Apply the Discrete Fourier Transform (DFT) to the original
time series data to obtain its frequency components, exclud-
ing columns irrelevant for Fourier analysis (e.g., dates).
(2)Select four prominent low-frequency components from the
entire frequency spectrum and move them to the mid-frequency
part. This modification aims to reduce the impact of fre-
quency bias typically seen between low and high frequencies
by placing important components in a non-low and non-high
frequency position.
(3) Split the frequency components into three equal parts.
(4)Rearrange these parts according to a predefined order for
frequency emphasis, ensuring that the first part is moved to
the end while keeping the original second and third parts in
their order.
(5)Apply the Inverse Discrete Fourier Transform (IDFT) to the
rearranged frequency data to convert it back into the time
domain, thereby generating the modified "mid" frequency
data.
(6)Reinsert any excluded columns (e.g., dates) to maintain the
original structure of the data.
Through the operations described above, we have constructed
a dataset with clearly high amplitude frequency components in
the middle of the frequency domain. By moving significant low-
frequency components to the mid-frequency section, we aim to
mitigate the effects of frequency differences that arise from the
dominance of low and high frequencies. The advantage of creating
artificial data through these simple modifications to real data lies
in its ability to preserve the inherent noise and instability present
in the real data, thereby enhancing the robustness and credibility
for subsequent analysis.
B NYSTRÃ–M APPROXIMATION IN
TRANSFORMER SELF-ATTENTION
MECHANISM
Overview: To streamline the attention computation, we select ğ‘š
landmarks by averaging rows or columns of the attention matrix,
simplifying the matrices Qğ‘›andKğ‘›into ËœQğ‘›and ËœKğ‘›. The NystrÃ¶m
approximation for the ğ‘›-th channel-wise attention Ağ‘›is then cal-
culated as Ağ‘›â‰ˆËœAğ‘›=ËœFğ‘›ËœAğ‘›ËœBğ‘›, where ËœFğ‘›=softmax(Qğ‘›ËœKğ‘›ğ‘‡),
ËœAğ‘›=softmax(ËœQğ‘›ËœKğ‘›ğ‘‡)+,ËœBğ‘›=softmax(ËœQğ‘›Kğ‘›ğ‘‡). Here, ËœAğ‘›+is the
Moore-Penrose inverse of ËœAğ‘›[44]. This significantly reducing the
computational load from ğ‘‚(ğ¿
ğ‘ƒğ¶2)toğ‘‚(ğ¿
ğ‘ƒğ¶). Specifically:Details: We reduce the computational cost of self-attention in
the Transformer encoder using the NystrÃ¶m method. Following,
we describe how to use the NystrÃ¶m method to approximate the
softmax matrix in self-attention by sampling a subset of columns
and rows.
Consider the softmax matrix in self-attention, defined as:
ğ‘†=softmax 
ğ‘„ğ¾ğ‘‡
âˆšï¸ğ‘‘ğ‘!
This matrix can be partitioned as:
ğ‘†=ğ´ğ‘†ğµğ‘†
ğ¹ğ‘†ğ¶ğ‘†
Whereğ´ğ‘†is derived by sampling ğ‘šcolumns and rows from ğ‘†.
By employing the NystrÃ¶m method, the SVD of ğ´ğ‘†is given by:
ğ´ğ‘†=ğ‘ˆÎ›ğ‘‰ğ‘‡
Using this, an approximation Ë†ğ‘†ofğ‘†can be constructed:
Ë†ğ‘†=ğ´ğ‘†ğµğ‘†
ğ¹ğ‘†ğ¹ğ‘†ğ´+
ğ‘†ğµğ‘†
Whereğ´+
ğ‘†is the Moore-Penrose inverse of ğ´ğ‘†.
To further elaborate on the approximation, given a query ğ‘ğ‘–and
a keyğ‘˜ğ‘—, let:
K(ğ‘ğ‘–,ğ¾)=softmax 
ğ‘ğ‘–ğ¾ğ‘‡
âˆšï¸ğ‘‘ğ‘!
K(ğ‘„,ğ‘˜ğ‘—)=softmax ğ‘„ğ‘˜ğ‘‡
ğ‘—âˆšï¸ğ‘‘ğ‘!
From the above, we can derive:
ğœ™(ğ‘ğ‘–,ğ¾)=Î›âˆ’1
2ğ‘‰ğ‘‡K(ğ‘ğ‘–,ğ¾)ğ‘šÃ—1
ğœ™(ğ‘„,ğ‘˜ğ‘—)=Î›âˆ’1
2ğ‘ˆğ‘‡K(ğ‘„,ğ‘˜ğ‘—)ğ‘šÃ—1
Thus, the NystrÃ¶m approximation for a particular entry in Ë†ğ‘†is:
Ë†ğ‘†ğ‘–ğ‘—=ğœ™(ğ‘ğ‘–,ğ¾)ğ‘‡ğœ™(ğ‘„,ğ‘˜ğ‘—)
In matrix form, Ë†ğ‘†can be represented as:
Ë†ğ‘†=softmax 
ğ‘„ğ¾ğ‘‡
âˆšï¸ğ‘‘ğ‘!
ğ‘›Ã—ğ‘šğ´+
ğ‘†softmax 
ğ‘„ğ¾ğ‘‡
âˆšï¸ğ‘‘ğ‘!
ğ‘šÃ—ğ‘›
This method allows for the approximation of the softmax matrix
in self-attention, potentially offering computational benefits.
C DETAILED RESULTS OF ALL DATASETS
Here, we show the detailed forecasting results of full datasets in
the Table. 7. The best andsecond best results are highlighted. With
a default look-back window of ğ¿=96, our proposal shows lead-
ing performance on most datasets and different prediction length
settings, with 60 top-1 (29 + 31) cases out of 80 in total.
 
2409Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 7: Full results of the long-term forecasting task
. We compare extensive competitive models under different prediction lengths following the setting of iTransformer [23]. The input
sequence length is set to 96 for all baselines. Avg means the average results from all four prediction lengths.
ModelsFredformer iT
ransformer RLinear PatchTST Cr
ossformer TiDE TimesNet DLinear SCINet FEDformer Stationar
yA
utoformer
(
Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021]
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEET
Tm196 0.326 0.361 0.334 0.368 0.355 0.376 0.329 0.367 0.404 0.426 0.364 0.387 0.338 0.375 0.345 0.372 0.418 0.438 0.379 0.419 0.386 0.398 0.505 0.475
192 0.363 0.380 0.377 0.391 0.391 0.392 0.367 0.385 0.450 0.451 0.398 0.404 0.374 0.387 0.380 0.389 0.439 0.450 0.426 0.441 0.459 0.444 0.553 0.496
336 0.395 0.403 0.426 0.420 0.424 0.415 0.399 0.410 0.532 0.515 0.428 0.425 0.410 0.411 0.413 0.413 0.490 0.485 0.445 0.459 0.495 0.464 0.621 0.537
720 0.453 0.438 0.491 0.459 0.487 0.450 0.454 0.439 0.666 0.589 0.487 0.461 0.478 0.450 0.474 0.453 0.595 0.550 0.543 0.490 0.585 0.516 0.671 0.561
A
vg0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517ET
Tm296 0.177 0.259 0.180 0.264 0.182 0.265 0.175 0.259 0.287 0.366 0.207 0.305 0.187 0.267 0.193 0.292 0.286 0.377 0.203 0.287 0.192 0.274 0.255 0.339
192 0.243 0.301 0.250 0.309 0.246 0.304 0.241 0.302 0.414 0.492 0.290 0.364 0.249 0.309 0.284 0.362 0.399 0.445 0.269 0.328 0.280 0.339 0.281 0.340
336 0.302 0.340 0.311 0.348 0.307 0.342 0.305 0.343 0.597 0.542 0.377 0.422 0.321 0.351 0.369 0.427 0.637 0.591 0.325 0.366 0.334 0.361 0.339 0.372
720 0.397 0.396 0.412 0.407 0.407 0.398 0.402 0.400 1.730 1.042 0.558 0.524 0.408 0.403 0.554 0.522 0.960 0.735 0.421 0.415 0.417 0.413 0.433 0.432
A
vg0.279 0.324 0.288 0.332 0.286 0.327 0.281 0.326 0.757 0.610 0.358 0.404 0.291 0.333 0.350 0.401 0.571 0.537 0.305 0.349 0.306 0.347 0.327 0.371ET
Th196 0.373 0.392 0.386 0.405 0.386 0.395 0.414 0.419 0.423 0.448 0.479 0.464 0.384 0.402 0.386 0.400 0.654 0.599 0.376 0.419 0.513 0.491 0.449 0.459
192 0.433 0.420 0.441 0.436 0.437 0.424 0.460 0.445 0.471 0.474 0.525 0.492 0.436 0.429 0.437 0.432 0.719 0.631 0.420 0.448 0.534 0.504 0.500 0.482
336 0.470 0.437 0.487 0.458 0.479 0.446 0.501 0.466 0.570 0.546 0.565 0.515 0.491 0.469 0.481 0.459 0.778 0.659 0.459 0.465 0.588 0.535 0.521 0.496
720 0.467 0.456 0.503 0.491 0.481 0.470 0.500 0.488 0.653 0.621 0.594 0.558 0.521 0.500 0.519 0.516 0.836 0.699 0.506 0.507 0.643 0.616 0.514 0.512
A
vg0.435 0.426 0.454 0.447 0.446 0.434 0.469 0.454 0.529 0.522 0.541 0.507 0.458 0.450 0.456 0.452 0.747 0.647 0.440 0.460 0.570 0.537 0.496 0.487ET
Th296 0.293 0.342 0.297 0.349 0.288 0.338 0.302 0.348 0.745 0.584 0.400 0.440 0.340 0.374 0.333 0.387 0.707 0.621 0.358 0.397 0.476 0.458 0.346 0.388
192 0.371 0.389 0.380 0.400 0.374 0.390 0.388 0.400 0.877 0.656 0.528 0.509 0.402 0.414 0.477 0.476 0.860 0.689 0.429 0.439 0.512 0.493 0.456 0.452
336 0.382 0.409 0.428 0.432 0.415 0.426 0.426 0.433 1.043 0.731 0.643 0.571 0.452 0.452 0.594 0.541 1.000 0.744 0.496 0.487 0.552 0.551 0.482 0.486
720 0.415 0.434 0.427 0.445 0.420 0.440 0.431 0.446 1.104 0.763 0.874 0.679 0.462 0.468 0.831 0.657 1.249 0.838 0.463 0.474 0.562 0.560 0.515 0.511
A
vg0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459ECL96 0.147 0.241 0.148 0.240 0.201 0.281 0.195 0.285 0.219 0.314 0.237 0.329 0.168 0.272 0.197 0.282 0.247 0.345 0.193 0.308 0.169 0.273 0.201 0.317
192 0.165 0.258 0.162 0.253 0.201 0.283 0.199 0.289 0.231 0.322 0.236 0.330 0.184 0.289 0.196 0.285 0.257 0.355 0.201 0.315 0.182 0.286 0.222 0.334
336 0.177 0.273 0.178 0.269 0.215 0.298 0.215 0.305 0.246 0.337 0.249 0.344 0.198 0.300 0.209 0.301 0.269 0.369 0.214 0.329 0.200 0.304 0.231 0.338
720 0.213 0.304 0.225 0.317 0.257 0.331 0.256 0.337 0.280 0.363 0.284 0.373 0.220 0.320 0.245 0.333 0.299 0.390 0.246 0.355 0.222 0.321 0.254 0.361
A
vg0.175 0.269 0.178 0.270 0.219 0.298 0.216 0.304 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 0.268 0.365 0.214 0.327 0.193 0.296 0.227 0.338T
raffic96 0.406 0.277 0.395 0.268 0.649 0.389 0.544 0.359 0.522 0.290 0.805 0.493 0.593 0.321 0.650 0.396 0.788 0.499 0.587 0.366 0.612 0.338 0.613 0.388
192 0.426 0.290 0.417 0.276 0.601 0.366 0.540 0.354 0.530 0.293 0.756 0.474 0.617 0.336 0.598 0.370 0.789 0.505 0.604 0.373 0.613 0.340 0.616 0.382
336 0.432 0.281 0.433 0.283 0.609 0.369 0.551 0.358 0.558 0.305 0.762 0.477 0.629 0.336 0.605 0.373 0.797 0.508 0.621 0.383 0.618 0.328 0.622 0.337
720 0.463 0.300 0.467 0.302 0.647 0.387 0.586 0.375 0.589 0.328 0.719 0.449 0.640 0.350 0.645 0.394 0.841 0.523 0.626 0.382 0.653 0.355 0.660 0.408
A
vg 0.431 0.287 0.428 0.282 0.626 0.378 0.555 0.362 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 0.804 0.509 0.610 0.376 0.624 0.340 0.628 0.379W
eather96 0.163 0.207 0.174 0.214 0.192 0.232 0.177 0.218 0.158 0.230 0.202 0.261 0.172 0.220 0.196 0.255 0.221 0.306 0.217 0.296 0.173 0.223 0.266 0.336
192 0.211 0.251 0.221 0.254 0.240 0.271 0.225 0.259 0.206 0.277 0.242 0.298 0.219 0.261 0.237 0.296 0.261 0.340 0.276 0.336 0.245 0.285 0.307 0.367
336 0.267 0.292 0.278 0.296 0.292 0.307 0.278 0.297 0.272 0.335 0.287 0.335 0.280 0.306 0.283 0.335 0.309 0.378 0.339 0.380 0.321 0.338 0.359 0.395
720 0.343 0.341 0.358 0.349 0.364 0.353 0.354 0.348 0.398 0.418 0.351 0.386 0.365 0.359 0.345 0.381 0.377 0.427 0.403 0.428 0.414 0.410 0.419 0.428
A
vg0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382Solar-Energy96 0.185 0.233 0.203 0.237 0.322 0.339 0.234 0.286 0.310 0.331 0.312 0.399 0.250 0.292 0.290 0.378 0.237 0.344 0.242 0.342 0.215 0.249 0.884 0.711
192 0.227 0.253 0.233 0.261 0.359 0.356 0.267 0.310 0.734 0.725 0.339 0.416 0.296 0.318 0.320 0.398 0.280 0.380 0.285 0.380 0.254 0.272 0.834 0.692
336 0.246 0.284 0.248 0.273 0.397 0.369 0.290 0.315 0.750 0.735 0.368 0.430 0.319 0.330 0.353 0.415 0.304 0.389 0.282 0.376 0.290 0.296 0.941 0.723
720 0.247 0.276 0.249 0.275 0.397 0.356 0.289 0.317 0.769 0.765 0.370 0.425 0.338 0.337 0.356 0.413 0.308 0.388 0.357 0.427 0.285 0.295 0.882 0.717
A
vg0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711
1stCount 29 31 4 8 1 1 2 1 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0
 
2410