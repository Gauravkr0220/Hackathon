ERASE: Benchmarking Feature Selection Methods for Deep
Recommender Systems
Pengyue Jiaâˆ—
jia.pengyue@my.cityu.edu.hk
City University of Hong Kong
Hong Kong SAR, ChinaYejing Wangâˆ—
yejing.wang@my.cityu.edu.hk
City University of Hong Kong
Hong Kong SAR, ChinaZhaocheng Duâˆ—
zhaochengdu@huawei.com
Huawei Noahâ€™s Ark Lab
Shenzhen, China
Xiangyu Zhaoâ€ 
xianzhao@cityu.edu.hk
City University of Hong Kong
Hong Kong SAR, ChinaYichao Wang
wangyichao5@huawei.com
Huawei Noahâ€™s Ark Lab
Shenzhen, ChinaBo Chen
chenbo116@huawei.com
Huawei Noahâ€™s Ark Lab
Shenzhen, China
Wanyu Wang
wanyuwang4-c@my.cityu.edu.hk
City University of Hong Kong
Hong Kong SAR, ChinaHuifeng Guoâ€ 
huifeng.guo@huawei.com
Huawei Noahâ€™s Ark Lab
Shenzhen, ChinaRuiming Tang
tangruiming@huawei.com
Huawei Noahâ€™s Ark Lab
Shenzhen, China
ABSTRACT
Deep Recommender Systems (DRS) are increasingly dependent on
a large number of feature fields for more precise recommendations.
Effective feature selection methods are consequently becoming
critical for further enhancing the accuracy and optimizing storage
efficiencies to align with the deployment demands. This research
area, particularly in the context of DRS, is nascent and faces three
core challenges. Firstly, variant experimental setups across research
papers often yield unfair comparisons, obscuring practical insights.
Secondly, the existing literatureâ€™s lack of detailed analysis on selec-
tion attributes, based on large-scale datasets and a thorough com-
parison among selection techniques and DRS backbones, restricts
the generalizability of findings and impedes deployment on DRS.
Lastly, research often focuses on comparing the peak performance
achievable by feature selection methods. This approach is typically
computationally infeasible for identifying the optimal hyperpa-
rameters and overlooks evaluating the robustness and stability of
these methods. To bridge these gaps, this paper presents ERASE, a
comprehensive bEnchmaRk for feAture SElection for DRS. ERASE
comprises a thorough evaluation of eleven feature selection meth-
ods, covering both traditional and deep learning approaches, across
four public datasets, private industrial datasets, and a real-world
commercial platform, achieving significant enhancement. Our code
is available online1for ease of reproduction.
âˆ—Authors contributed equally to this research.
â€ Corresponding author.
1https://github.com/Applied-Machine-Learning-Lab/ERASE
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671571CCS CONCEPTS
â€¢Information systems â†’Recommender systems.
KEYWORDS
Benchmark, Feature Selection, Deep Recommender System
ACM Reference Format:
Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang,
Bo Chen, Wanyu Wang, Huifeng Guo, and Ruiming Tang. 2024. ERASE:
Benchmarking Feature Selection Methods for Deep Recommender Systems.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671571
1 INTRODUCTION
Recommender systems have become indispensable in many sectors
ranging from e-commence to content streaming in the realm of in-
formation explosion [ 37,38]. With the application of deep learning
techniques, Deep Recommender System (DRS) exhibits amplified
prediction ability of user preference, providing personalized expe-
rience and dominating the deployment landscape [7, 16, 43].
To improve the accuracy of recommendations, DRS is progres-
sively integrating an expanding array of feature fields into their
predictive models, which can number in the hundreds or even thou-
sands [ 19]. The significance of each feature varies, resulting in the
accumulation of superfluous or extraneous features. Consequently,
feature selection, which concentrates on pinpointing and leverag-
ing the most critical features, is becoming increasingly crucial in
modern DRS [ 5,52]. One immediate benefit of feature selection is
the enhancement of prediction performance, achieved by eliminat-
ing non-contributory features that could otherwise adversely affect
predictions. From an industrial viewpoint, selecting predictive fea-
tures is also essential for meeting deployment criteria regarding
memory usage since unnecessary storage demands can often be
inflated by the presence of redundant features.
Hand-crafted feature selection usually requires lots of expert
knowledge and labor efforts, which is usually infeasible to achieve
5194
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pengyue Jia, et al.
optimal results when the candidate set contains thousands of fea-
tures. Researchers have designed various methods to automatically
select predictive features, including statistical methods [ 31,41,42],
learning methods [ 4,6,13], and agent-based methods [ 10,34,50].
Despite the satisfactory results these methods achieved, they also
reveal the following issues hindering the development of this field:
â€¢Experimental Differences. Recent years have witnessed a va-
riety of methods designed for DRS [ 17,26,29,35,44â€“46]. How-
ever, these works conduct experiments with varied settings. For
example, MultiSFS [ 44] is designed to select features for multi-
task DRS. SHARK [ 46] suggests the feature selection method
F-permutation and a quantization method. Optfs [ 35] selects the
feature from the value-level while others mainly select from the
field-level. These experimental differences usually lead to unfair
or unavailable comparisons, making the subsequent researchers
struggle to generate practical insights.
â€¢Insufficiency. Existing feature selection benchmarks are pre-
dominantly tailored for conventional downstream tasks, such as
classification [ 2,3]. They are primarily built upon synthetic or
domain-specific datasets [ 1,9,12], which diverges significantly
from the complex, large-scale datasets encountered in DRS. This
divergence results in a notable deficiency in guiding feature se-
lection specifically for DRS, due to the benchmarksâ€™ disparate
scope and focus. DeepLasso [ 8], while being among the bench-
marks most closely related to DRS, primarily addresses tabular
learningâ€”a context that, despite similarities, does not fully align
with the intricacies of recommendation tasks. Furthermore, such
benchmarks often limit their exploration to traditional selection
methods and rely on datasets of a much smaller scale, thereby
omitting crucial, in-depth analysis from an industrial perspective
for DRS. Moreover, while related literature surveys [ 5,52] pro-
vide comprehensive reviews of the field, they fall short in offering
empirical evidence or experimental results that could motivate
practical deployments in DRS, lacking the necessary data-driven
support to inform and inspire future research directions.
â€¢Assessment Deficit. A pivotal hyperparameter for feature se-
lection methods is the number of features to be selected, denoted
asğ‘˜in this study. Feature selection methods often exhibit signifi-
cant variability in performance across different values of ğ‘˜, and
the optimal ğ‘˜is not the same for all methods [ 29,35,45]. This
variability on ğ‘˜introduces two significant challenges in evalu-
ating feature selection methods. First, the direct comparison of
methods at their respective optimal ğ‘˜may not constitute a fair
assessment. Such comparisons fail to account for the different
memory requirements associated with varying optimal ğ‘˜val-
ues for different selection methods. Furthermore, this approach
neglects the performance variability under sub-optimal hyperpa-
rameter settings, thereby obscuring insights into the methodsâ€™
robustness and stability across a range of ğ‘˜values. Second, the
exhaustive search for the optimal ğ‘˜across the entire spectrum
of possible values is time-consuming and computationally in-
tensive. This necessitates an evaluation methodology capable of
effectively assessing a methodâ€™s performance based on partial ğ‘˜
values, offering a more efficient means to gauge feature selection
effectiveness without finishing complete iterations.To address these issues, we propose ERASE, a comprehensive
bEnchmaRk for feAture Selection for DRS. ERASE initiates a uni-
fied and fair experimental framework, minimizing experimental
discrepancies across various selection methods. Remarkably, ERASE
pioneers as the first feature selection benchmark with a focus on
DRS tasks, incorporating both prevalent DRS feature selection tech-
niques and conventional methods. It introduces a novel taxonomy to
classify these methods and unearth intrinsic patterns among groups
of methods. By evaluating the performance on widely used public
datasets and authentic industrial production datasetsâ€”through both
offline comparison and online testingâ€”ERASE furnishes strong em-
pirical support, facilitating the generation of actionable insights.
In its endeavor to provide a comprehensive assessment of feature
selection methods, ERASE contrasts the optimal performance of
compared selection methods alongside their outcomes under spe-
cific deployment prerequisites. Additionally, we introduce a novel
metric, AUKC, specifically crafted for assessing the robustness and
stability of feature selection methods across a range of feature
quantitiesğ‘˜, thereby addressing the critical lack of such evaluative
metrics. We summarize our major contributions as follows:
â€¢We present ERASE, a comprehensive benchmark for DRS feature
selection methods, providing a fair comparison for emerging
selection techniques with various datasets and DRS backbones.
To the best of our knowledge, we are the first to focus on bench-
marking feature selection methods for recommendation tasks.
â€¢We recognize the assessment shortfall linked to the substantial
dependency of selection efficacy on the hyperparameter ğ‘˜, and
in response, we introduce a novel evaluation metric, AUKC. This
metric is designed to evaluate the robustness and stability of fea-
ture selection methods, bridging the existing gap in assessment.
â€¢We carry out thorough experiments across four widely used
public datasets and real-world industrial production datasets,
yielding insights from various angles. Notably, our experimen-
tal findings have guided optimizations in our online platform,
achieving a 20% reduction in latency without compromising ef-
fectiveness, validating the practical utility of our benchmark.
2 BECNCHMARK DESIGN
In this section, we will give an overview of our benchmark. As
shown in Figure 1, the benchmark consists of four components:
dataset, feature selection methods, backbone models, and metrics.
2.1 Datasets
To comprehensively evaluate the effectiveness of different feature
selection methods, we select four public datasets for our exper-
iments. Avazu2andCriteo3are selected because they are fre-
quently used dataset for studying feature selection in DRS [ 29,
35,45]. To compare the performance of different methods with a
small feature set, we choose the popular Movielens-1M4dataset
in the recommendation field. Additionally, to compare the per-
formance of different methods in scenarios closer to real-world
recommender systems, we supplement with the AliCCP5dataset,
2https://www.kaggle.com/competitions/avazu-ctr-prediction
3https://ailab.criteo.com/ressources/
4https://grouplens.org/datasets/movielens/1m/
5https://tianchi.aliyun.com/dataset/408
5195ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ERASEDatasets
(Â§2.1)A
vazu
Crite
o
Mo
vielens-1M
AliCCP
Featur
e
Selection
Methods
(Â§2.2)Shallo
w
Feature
SelectionLasso
[41]
GBD
T [13]
RF
[4]
X
GBoost [6]
Gate-base
d
Feature
SelectionA
utoField [45]
A
daFS [29]
OptFS
[35]
LPFS
[17]
Sensitivity
-base
d
Feature
SelectionPermutation
[11]
SHARK
[46]
SFS
[44]
Backb
one
Models
(Â§2.3)Wide&De
ep [7]
De
epFM [16]
DCN
[43]
FibiNet
[20]
Metrics
(Â§2.4)AUC
Logloss
AUK
C
Figur
e1:Benchmark Overview.
which possesses user anditem featur esandincludes arichsetof
85,316,519 interaction samples. Thestatistics ofdatasets andthe
detaile dintroduction areillustrate dinAppendix A.
2.2 Featur eSelection Metho ds
Welistallfeatur eselection metho dsinthisworkandtheir attributes
inTable 1.Ther earethreefeasible dimensions toclassify these
metho ds.1)Training strategy .Base donthetraining strategy ,
themetho dscanbesingle-stage ortwo-stage .Single-stage usually
directlyintegrates thefeatur eselection module into theoriginal
modelwithout changing thetraining logic. Incontrast, two-stage
metho dscontain searching andretraining phases. Informativ efea-
turefields orfeatur evalues areselectedinthesearching phase ,
andthebackb onemodelistraine dwith these selectedfeatur es
intheretraining phase .2)Selection type.Ther earetwotypes
ofselection infeatur eselection metho ds:softselection andhard
selection. Thesoftselection offers amask toaffectinputs. Featur es
multiplie dwith 0areconsider edfilteredout.Forthehardselection,
featur esareremoveddirectlyfromtheinputs. 3)Selection tech-
nique .Depending onthetechniques usedforselection, asshown
inFigur e2,wedivide metho dscontaine dinourbenchmark intoTable 1:Metho dsOverview.Forthetypecolumn, "Shallo w"
represents shallo wfeatur eselection metho ds,"Gate "repre-
sents gate-base dfeatur eselection metho ds,and"Sensitivity"
represents thesensitivity-base dfeatur eselection metho ds.
Forthesingle-stage ,two-stage ,softselection, andhardse-
lection columns, âœ”represents applicable ,âœ˜represents not
applicable ,andâœ”â˜…represents that thismetho dcanbeappli-
cable after appr opriate modifications.
Metho
ds Type Single-stage Two-stage SoftSelection HardSelection
Lasso
Shallo w âœ˜ âœ” âœ˜ âœ”
GBD T Shallo w âœ˜ âœ” âœ˜ âœ”
RF Shallo w âœ˜ âœ” âœ˜ âœ”
XGBoost Shallo w âœ˜ âœ” âœ˜ âœ”
AutoField Gate âœ˜ âœ” âœ˜ âœ”
AdaFS Gate âœ” âœ˜ âœ” âœ˜
OptFS Gate âœ˜ âœ” âœ” âœ”â˜…
LPFS Gate âœ” âœ”â˜… âœ” âœ”â˜…
Permutation Sensitivity âœ˜ âœ” âœ˜ âœ”
SHARK Sensitivity âœ˜ âœ” âœ˜ âœ”
SFS Sensitivity âœ˜ âœ” âœ˜ âœ”
threecategories: shallo wfeatur eselection, gate-base dselection,
andsensitivity-base dselection.
Due totheunbalance ddistribution offeatur eselection metho ds
classifying ontraining strategy andselection type,weelaborate
ourworkbasedonselection technique classification. Specifically ,
ourbenchmark contains thefollowing metho ds:
1.Shallo wFeatur eSelection
â€¢Lasso [41].Theleast absolute shrinkage andselection operator
(Lasso) algorithm isatraditional anduseful metho dinmachine
learning. Itperforms bothvariable selection andregularization
toimpr ovethemodelperformance .
â€¢GBD T[13].Gradient-b oosteddecision tree(GBD T)achie ves
superior performance bycontinually adding treestofitresiduals.
Byaggregating thefeatur eimportance scoresfromeach tree,it
alsoservesasaneffectivemetho dforfeatur eselection.
â€¢RandomFor est[4](RF). RandomFor estderiv esthefeatur eim-
portance bymeasuring howmuch each featur edecreases the
impurity inatree,commonly using Gini impurity orentropy.
â€¢XGBoost[6].Extreme Gradient Boosting (XGBoost)ranks the
importance offeatur esbycalculating theimpr ovement ofevery
featur eonthefinal performance .
2.Gate-base dFeatur eSelection
â€¢AutoField [45].AutoField isthefirst gate-base dfeatur eselec-
tion metho dinDRS. Itdesigns anovelcontr oller netw orkto
generate a2-dimensional vectordetermining whether tochoose
thisfeatur efield. Itisatwo-stage metho dandselectsfeatur eson
thefield level.
â€¢AdaFS [29].AdaFS isanalgorithm that cangenerate agate
foreach featur efield adaptiv ely.Itisasingle-stage metho dand
selectsfeatur esonthefield level.
â€¢OptFS [35].OptFS focuses onthefeatur evalue levelselection. It
trains ascalar foreach featur evalue andcontinuously strength-
enstheconstraints onsparsity astraining progresses.
â€¢LPFS [17].LPFS argues that theconclusion that featur eswith
smaller weights arelessimportant than those with larger weights
may notbecorrect.Itproposes akind ofsmoothed-ğ‘™0function
thatcaneffectivelyselectinformativ efeatur es.Itisasingle-stage
metho dandfocuses onthefield level.
5196KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pengyue Jia, et al.
...Forward
...Backward
...Sensitivity-Based Feature Selection 
...
......Gate-Based Feature Selection 
...
...Shallow Feature Selection 
Importance Score Feature Feature Embedding Forward Backward Sensitivity Score
Figure 2: Overview of three categories of feature selection methods in deep recommender systems. Shallow methods typically
use statistical algorithms to assign feature importance to each field. Gate-based methods, on the other hand, assign gates to
feature embeddings and consider the gate values as the importance of the corresponding features. Sensitivity-based methods
derive parameter sensitivity from backward propagation steps and calculate feature importance accordingly.
3. Sensitivity-based Feature Selection
â€¢Permutation [ 11].Permutation works by randomly permut-
ing the feature values at a time and measuring how much this
permutation affects the performance of the model.
â€¢SHARK [ 46].Shark takes the novel first-order component of
Taylor expansion as the feature importance score for model pre-
diction. It then prunes those features with lower scores from the
embedding table to improve model performance and efficiency.
â€¢SFS [ 44].SFS takes the gradients of the gate for each feature
field as the feature importance score. It is a two-stage method
and selects on the feature field level.
2.3 Backbone Models
To comprehensively evaluate the effectiveness of feature selection
methods, we choose four popular DRS models as backbones: 1)
Wide&Deep : A classical model contains shallow and deep net-
works to capture feature interactions. 2) DeepFM : A model with
FM module to automatically learn feature interactions. 3) DCN : A
model with cross layers to study feature interactions. 4) FibiNet : A
model equipped with SENet and bilinear layer to adaptively learn
feature importance and capture high-order feature interactions.
The detailed introduction of these models is in Appendix B.
2.4 Metrics
In this paper, we focus on the Click-Through Rate (CTR) prediction
task, so we take AUC and Logloss as the two main metrics in our
benchmark. The detailed introduction is in Appendix C. Addition-
ally, since there is currently no metric that comprehensively evalu-
ates the performance of feature selection methods across varying
numbers of selected features, we propose a new evaluation metric
named â€œAUKCâ€ to address this challenge.
AUKC. Current metrics cannot assess the robustness and stability
of a specific feature selection method across different numbers of
selections. Therefore, we introduce a new metric Area Under theğ¾-performance Curve (AUKC) to fill this gap. AUKC measures the
area under the performance curve of feature selection methods
with different ğ¾. Specifically, AUKC is formalized as follows:
ğ´ğ‘ˆğ¾ğ¶ =1
|ğ¾||ğ¾|âˆ‘ï¸
ğ‘˜=1(ğ´ğ‘ˆğ¶ğ‘˜+ğ´ğ‘ˆğ¶ğ‘˜âˆ’1âˆ’1) (1)
where|ğ¾|denotes the number of all feature fields, ğ´ğ‘ˆğ¶ğ‘˜is the
AUC score when selecting ğ‘˜feature fields follow a specific feature
selecting method. If there are no input features, the model would
make random predictions so that ğ´ğ‘ˆğ¶ 0=0.5.
In practice, due to resource constraints, it is generally not feasible
to conduct experiments with every possible number of selections.
Therefore, we further propose a more general form of AUKC to
accommodate this change:
ğ´ğ‘ˆğ¾ğ¶ =1
|ğ¾||ğ‘|âˆ‘ï¸
ğ‘›=1(ğ´ğ‘ˆğ¶ğ‘›,ğ‘™+ğ´ğ‘ˆğ¶ğ‘›,ğ‘Ÿâˆ’1)Ã—Î”ğ‘™ğ‘› (2)
where|ğ¾|denotes the number of all feature fields and |ğ‘|is the
number of segments across the entire length of the feature fields set.
ğ´ğ‘ˆğ¶ğ‘›,ğ‘™is the AUC corresponding to the number of features selected
at the left endpoint of the ğ‘›-th segment, and ğ´ğ‘ˆğ¶ğ‘›,ğ‘Ÿdenotes the
AUC with the number of selections at the right endpoint of the
ğ‘›th segment. Î”ğ‘™ğ‘›represents the length of the ğ‘›-th segment. The
detailed process of the formula derivation is in Appendix D.
AUKC and AUC share a similar conceptual basis, representing
the area under a curve, and both have a value range from 0 to 1.
However, unlike AUC, the endpoint of AUKC does not necessar-
ily equal 1, and the values in the middle of the curve can exceed
the endpoint value. This occurs because eliminating features with
less information can result in a model that performs better than
one using all available features. The AUKC metric considers the
effectiveness of feature selection at different numbers of selected
features,ğ‘˜, thereby providing a more comprehensive reflection of
the efficacy of feature selection methods.
5197ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
3 EXPERIMENTS
In this section, we will provide extensive experimental results to
answer following research questions:
â€¢RQ1: In a fair and unified environment, how do different feature
selection methods perform?
â€¢RQ2: How do feature selection methods perform in terms of
robustness and stability across various numbers of selections?
â€¢RQ3: How is the efficiency of feature selection methods?
â€¢RQ4: Do the performances of feature selection methods align
between large-scale industrial datasets and public datasets?
â€¢RQ5: Do the performances of feature selection methods remain
consistent online and offline?
â€¢RQ6: What is the relationship between feature rankings of dif-
ferent feature selection methods?
3.1 Experimental Details
We implement the benchmark framework based on Pytorch 1.11.
In the training phase, we utilize the Adam optimizer with ğ›½1=0.9,
ğ›½2=0.999, andğœ–=1Ã—10âˆ’8. We set the learning rate as 0.001, the
batch size as 4,096, and the embedding size as 8. For the activation
function, if the original papers do not emphasize a specific one,
we use ReLU as the activation function. We release the repository
of our benchmark online6. We conduct each experimental setting
three times and record its average metrics to mitigate the impact
of experimental fluctuations.
3.2 Overall Performance (RQ1)
In this subsection, we compare the performance of feature selection
methods with different backbone models. The complete experimen-
tal results for all four backbone models are illustrated in Appen-
dix E. For the two-stage approaches (Lasso, GBDT, RF, XGBoost,
AutoField, Permutation, SHARK, and SFS), we experiment with
different values of selected features during the retraining stage and
adopt the best results. In the case of the single-stage approach, if it
can also be modified to a two-stage method (OptFS and LPFS), we
conduct experiments on both ways and record the optimal results;
if it only supports single-stage (i.e., AdaFS), we directly document
its results. From Table 2, we can observe the following points:
â€¢In general, gate-based methods show better performance com-
pared to shallow feature selection and sensitivity-based feature
selection. The likely reason is that compared to using traditional
machine learning methods or solely relying on gradient informa-
tion, leveraging the powerful expression and learning capabilities
of deep neural networks allows for a more thorough exploration
of feature importance.
â€¢The effectiveness of feature selection methods remains relatively
consistent across different backbone models. This is because the
information contained in feature combinations is objective and
factual and is not affected by the backbone model.
â€¢The validity of feature selection methods is distinct across dif-
ferent datasets. The shallow feature selection methods perform
better in Criteo and Aliccp. The possible reason is the feature
value distribution in the other two datasets is very imbalanced,
making shallow models prone to overfitting. Gate-based feature
6https://github.com/Applied-Machine-Learning-Lab/ERASEselection methods are good at dealing with limited data, while
sensitivity-based feature selection methods achieve the best per-
formance among all methods in datasets with rich samples. This
finding can be attributed to the fact that in the data-scarce sce-
nario, gradients are too sensitive to judge feature importance.
However, with sufficient data support, the stability of gradient
information will be greatly improved, allowing for more accurate
feature importance rankings.
â€¢The two-stage approach yields more stable results compared
to the single-stage method across different backbone models.
This is because the feature combinations filtered out by the two-
stage method can exist independently after the search phase,
whereas the single-stage method requires retraining the feature
importance scores with each training session.
3.3 Stability Evaluation (RQ2)
In this section, we conduct experiments on each backbone model
with different number of selected features ğ‘˜to investigate their
robustness and stability. Specifically, we iterate a specific list for ğ‘˜
(e.g.,[5,10,15,17,19,21]for Avazu) and record the corresponding
performance of feature selection methods. Then, the stability of
feature selection methods can be revealed from two perspectives,
visual patterns of the performance curve (Figure 3) and AUKC
values calculated as Equation (1) (Table 3).
In Figrue 3, the x-axis represents the number of selected features
ğ‘˜and the y-axis represents the evaluation metrics AUC or Logloss.
We only visualize the results of DeepFM, for we find that the trends
for different backbone models remain consistent. The detailed ex-
perimental results for the other backbone models are listed in our
released repository. In addition, it is worth noting that to make
OptFS applicable in this experiment, we assign importance scores
to features based on the drop rates of feature values in each feature
field. From Figure 3, we can find:
â€¢The effectiveness of features selected by different methods varies
in ranking when the number of selections changes. Specifically,
when the number of features selected is smaller (i.e., 5 or 10),
Autofield, SHARK, and SFS perform better. They obtain infor-
mation through the values or gradients of gate vectors, guiding
the sorting of feature importance. They are good at selecting the
most informative features from the candidate feature sets. OptFS
doesnâ€™t initially perform well, as it is inherently a single-stage
method. Modifying it to a two-stage method based on different
feature value drop rates may not align with the goal of enhanc-
ing effectiveness through feature selection. When the number
of features approaches the full set of features, the Permutation
method performs well. This is because the permutation method
calculates feature importance scores based on the loss of effect
caused by dropping a feature from the full set. It tends to identify
redundant features with less information.
â€¢The XGBoost and GBDT methods show different trends on the
Avazu and Criteo datasets. Specifically, they perform poorly on
the Avazu dataset but well on the Criteo dataset. This is be-
cause the Avazu datasetâ€™s feature values are concentrated in a
few feature fields, leading to an imbalance that makes XGBoost
and GBDT prone to overfitting, which affects the assessment of
feature importance [40].
5198KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pengyue Jia, et al.
Table 2: Overall experimental results of feature selection for DeepFM and Wide&Deep backbones.
Backb
one model MethodsAvazu Criteo Movielens-1M AliCCP
AUC
Logloss AUC Logloss AUC Logloss AUC Logloss
De
epFMno_selection 0.78576 0.37680 0.80024 0.45321 0.79027 0.54124 0.61813 0.16176
Lasso 0.78587 0.37654 0.79832 0.45483 0.80683 0.52498
0.61887 0.16174
GBDT 0.76930 0.38571 0.80011 0.45329 0.78942 0.54234 0.61864 0.16150
RF 0.78642
0.37630 0.79920 0.45427 0.78920 0.54217 0.61772 0.16172
XGBoost 0.76953 0.38548 0.80022 0.45333 0.78974 0.54190 0.61822 0.16172
AutoField 0.78611 0.37641 0.80022 0.45325 0.80722
0.52471 0.61894
0.16174
AdaFS 0.78278 0.37922 0.79950 0.45405 0.78590 0.54676 0.61726 0.16158
OptFS
0.78624 0.37627
0.79934 0.45505 0.79027 0.54124 0.61551 0.16195
LPFS 0.78840 0.37601 0.80080 0.45364 0.78885 0.54276 0.61941 0.17864
Permutation 0.78624 0.37640 0.80006 0.45349 0.78974 0.54207 0.61844 0.16174
SHARK 0.78611 0.37643 0.80035
0.45329 0.78888 0.54342 0.61840 0.16174
SFS 0.78626 0.37631 0.80020 0.45330 0.78982 0.54194 0.61835 0.16168
Wide&De
epno_selection 0.78574 0.37679 0.79971 0.45346 0.79041 0.54163 0.62137 0.16154
Lasso 0.78578 0.37665 0.79761 0.45526 0.80750 0.52400 0.62096 0.16159
GBDT 0.76900 0.38583 0.79979 0.45337 0.78926 0.54252 0.62187
0.16146
RF 0.78571 0.37664 0.79907 0.45400 0.78992 0.54170 0.62202 0.16132
XGBoost 0.76886 0.38587 0.80023
0.45320 0.78893 0.54266 0.62174 0.16142
AutoField 0.78601 0.37653 0.79971 0.45348 0.80736 0.52496
0.62176 0.16134
AdaFS 0.78607 0.37665 0.79952 0.45394 0.78703 0.54467 0.62106 0.16123
OptFS
0.78582 0.37654 0.80111 0.45284 0.79041 0.54163 0.62053 0.16144
LPFS 0.78606 0.37680 0.79953 0.45477 0.79072 0.54136 0.61916 0.17177
Permutation 0.78607 0.37647
0.79988 0.45342 0.78936 0.54226 0.62156 0.16138
SHARK 0.78592 0.37655 0.80019 0.45313
0.78980 0.54146 0.62148 0.16154
SFS 0.78592 0.37646 0.80010 0.45326 0.78993 0.54139 0.62165 0.16122
5 10 15171921
The number of selected features K0.6250.6500.6750.7000.7250.7500.7750.800AUC
0.78576AUC on Avazu
5 10 15171921
The number of selected features K0.370.380.390.400.410.420.430.44Logloss
0.37680Logloss on Avazu
510152025303336
The number of selected features K0.700.720.740.760.780.80AUC
0.80024AUC on Criteo
510152025303336
The number of selected features K0.450.460.470.480.490.500.510.520.53Logloss
0.45321Logloss on Criteo
Baseline
LassoGBDT
RFXGBoost
AutoFieldOptFS
LPFSPermutation
SHARKSFS
Figur
e 3: Experimental results of feature selection methods with DeepFM backbone, evaluated by AUC and Logloss.
To offer a more direct comparison of the stability, we evaluate
the selection results using the AUKC metric. Table 3 shows AUKC
results on Avazu and Criteo. We can conclude:
â€¢Overall, sensitivity-based feature selection methods outperform
gate-based feature selection methods, both of which are superior
to shallow feature selection methods. The possible reason is that
gradient-based methods calculate feature importance based on
partial batches, which helps to prevent overfitting. The inferior
performance of shallow methods may be attributed to their ex-
cessive simplicity, which cannot effectively capture the complex
relationships between features to determine feature importance.â€¢Regarding specific methods, AutoField, SHARK, and SFS exhibit
the best performance, demonstrating leading effects across dif-
ferent datasets. It is noteworthy that AutoField significantly sur-
passes other gate-based feature selection methods. This may be
attributed to AutoField being trained in a bi-level optimization
manner, which reduces the risk of overfitting, making the selec-
tion results more stable and reliable.
3.4 Efficiency Analysis (RQ3)
In this section, we answer the RQ3 from two aspects: experiments
with performance limitations and with memory limitations.
5199ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Experimental results of AUKC on Avazu and Criteo.
Metho
ds Lasso GBDT RF XGBoost AutoField OptFS LPFS Permutation SHARK SFS
A
vazuDeepFM 0.43896 0.44430 0.49814 0.43264 0.50526 0.40455 0.49337 0.48526 0.50267
0.50146
WideDeep 0.43845 0.44392 0.49780 0.43271 0.50537 0.40464 0.49349 0.48521 0.50254
0.50120
DCN 0.43909 0.44419 0.49920 0.43311 0.50658 0.40534 0.49472 0.48673 0.50363
0.50223
FibiNet 0.44410 0.44626 0.50475 0.43544 0.50877
0.40974 0.49866 0.49246 0.50918 0.50691
Crite
oDeepFM 0.52193 0.49894 0.52498 0.49388 0.53862 0.50410 0.53634 0.50311 0.53886 0.53998
WideDe
ep 0.52142 0.49821 0.52473 0.49338 0.53826 0.50369 0.53598 0.50279 0.53857 0.53972
DCN
0.52245 0.49898 0.52582 0.49410 0.53926 0.50449 0.53690 0.50374 0.53952 0.54077
FibiNet
0.52822 0.50459 0.53155 0.50006 0.54528 0.51048 0.54329 0.50964 0.54580 0.54693
Table 4: Experimental results with performance limitations.
FS
MethodsAvazu Criteo
memory remain ğ‘˜ memory remain ğ‘˜
Lasso
99.99963% 17 68.73103% 25
GBDT 100.00000% 23 23.22062% 30
RF 99.99161% 10 99.56109% 20
XGBoost 100.00000% 23 75.20371% 33
AutoField 99.91556% 10 8.17985% 20
AdaFS 100.00000% 23 100.00000% 39
OptFS 99.73494% 17 79.05157% 25
LPFS 99.96204% 15 0.00100% 15
Permutation 71.56409% 15 48.77971% 25
SHARK 99.98986% 10 7.31463% 15
SFS 99.98345% 10 8.16199% 15
3.4.1 Experiments with performance limitations. In real-world sce-
narios, an important consideration is reducing the memory usage of
a model as much as possible while ensuring its effectiveness. To test
the memory-saving capabilities of different methods under the con-
straint of maintaining effectiveness, experiments were conducted
on two classic DRS datasets, Avazu and Criteo, using DCN as the
backbone model. Firstly, we calculate an acceptable threshold for
effectiveness based on the baseline performance of the backbone
model (without feature selection, using the full set of features),
considering a 1% loss in AUC as acceptable. The threshold is AUC
0.77873 for Avazu and AUC 0.79245 for Criteo. Based on the feature
importance ranking list obtained in the search phase by different
feature selection methods, we select features from highest to lowest
importance until the modelâ€™s performance exceeds the threshold.
We record the number of features and feature values required to
reach this lower threshold of effectiveness for each feature selection
method. Since most parameters in DRS models are concentrated in
the embedding table, we only considered the memory usage of the
embedding table when calculating the memory models required.
From Table 4, we can conclude that:
â€¢From the perspective of memory usage, methods like AutoField,
LPFS, Permutation, SHARK, and SFS perform better. This indi-
cates that even in DRS scenarios rich in high-dimensional sparse
features, these methods do not rely solely on ID-type features
(e.g., userid). Instead, they effectively select informative features.
Itâ€™s noteworthy that AdaFS shows 100% memory usage on both
datasets. This is because AdaFS is an instance-level selectionTable 5: Experimental results with memory limitations.
FS
Methods25% memory 50% memory 75% memory
AUCğ‘˜ AUCğ‘˜ AUCğ‘˜
Lasso
0.76513 11 0.76509 12 0.79465 25
GBDT 0.79981 32 0.80003 33 0.80008 34
RF 0.72374 6 0.75961 8 0.76520 10
XGBoost 0.76282 16 0.77942 21 0.78809 30
AutoField 0.79997 30 0.80037 32 0.80037 32
OptFS 0.79162 21 0.79263 22 0.79297 23
LPFS 0.79892 25 0.80015 30 0.80031 32
Permutation 0.79462 24 0.79782 30 0.79782 30
SHARK 0.79748 20 0.79977 25 0.79977 25
SFS 0.79577 18 0.80027 28 0.80027 28
method (where gate weights vary with each input sample), and
therefore cannot save memory usage.
â€¢In terms of the number of features, RF, AutoField, LPFS, SHARK,
and SFS only require half or fewer of the original feature set
to achieve 99% of the backbone modelâ€™s performance. On the
other hand, GBDT and XGBoost need more features. A possible
reason for this is that GBDT and XGBoost are prone to overfitting
in high-dimensional sparse DRS datasets, which can affect the
judgment of feature importance.
â€¢From the perspective of the datasets, the memory-saving effec-
tiveness of various methods is less pronounced on the Avazu
dataset compared to the Criteo dataset. This is because most
feature values in the Avazu dataset are concentrated in just a
few feature fields, whereas in Criteo, the feature values are more
evenly distributed. Therefore, when these particular feature fields
are indispensable sources of information for CTR prediction, the
memory-saving effect becomes less noticeable.
3.4.2 Experiments with memory limitations. Maximizing effective-
ness within limited memory is also an important application sce-
nario for feature selection. To test the capability of different methods
in feature selection under memory constraints, experiments were
conducted on the Criteo dataset using DCN as the backbone model.
The memory limits were categorized into three levels: 25% memory,
50% memory, and 75% memory. Specifically, since the parameters
in DRS are mostly concentrated in the embedding table, we used
the memory usage of the full feature setâ€™s embedding table as a
reference. We set thresholds at 25%, 50%, and 75% of the total mem-
ory to record the optimal performance achievable under memory
constraints by different feature selection methods. Since AdaFS is
incapable of performing a hard selection of partial feature fields, it
5200KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pengyue Jia, et al.
Table 6: Overall experimental results on industrial dataset
Metho
ds Base AutoField LPFS Permutation SHARK SFS
AUC
0.933920 0.933915 0.933858 0.933946 0.933985 0.933894
AUK
C N/A 0.819053 0.817445 0.818387
0.818169 0.817084
17 37 57 77 97 117 137 155
The number of selected features K0.9270.9280.9290.9300.9310.9320.9330.9340.935AUC
0.93392
Baseline
AutoField
SHARK
Permutation
LPFS
SFS
Figure 4: K-performance figure on industrial dataset
is not included in this experiment. Based on the results presented
in Table 5, we can draw the following conclusion:
â€¢Overall, gate-based feature selection and sensitivity-based feature
selection methods perform better. Even when restricted to using
only 25% of the available memory, they can achieve results close
to those obtained using the full set of features. This is because
these methods rely on more powerful deep learning networks
that can better model feature importance and are not dependent
on high-dimensional sparse ID-type features in DRS.
â€¢Specifically, AutoField achieves the best results because its de-
signed controller effectively learns the importance of each field
for the prediction outcome. The DARTS (Differentiable Architec-
ture Search) [ 32] parameter updating manner made the learning
of the gate vector more robust and capable of generalization. In
contrast, Random Forest (RF) performs the worst. This is because,
in the context of high-dimensional sparse DRS, RF tends to assign
high weights to ID-type features with numerous feature values.
Such feature fields offer more opportunities for splitting, which
facilitates the training of RF. However, this approach increases
the risk of model overfitting. This tendency of RF to favor features
with many splitting points can lead to a bias towards complex,
less generalizable models that donâ€™t necessarily capture the most
predictive or relevant features for the task at hand.
3.5 Offline Results on Industrial Dataset (RQ4)
To find whether our conclusions for feature selection methods align
between large-scale industrial datasets and public datasets, we
conduct comparison experiments on large-scale industrial datasets.
3.5.1 Industrial Dataset. The dataset is constructed with sampled
CVR records from 2023-11-29 to 2023-11-30 containing approxi-
mately seven hundred million samples. The feature used in thisoffline dataset is aligned with the online service model, which con-
tains over 157 features. All of these features have been roughly
proved useful in offline leave-one-out experiments.
3.5.2 Experiment. Considering the industry dataset is relatively
huge, we only select the top 5 elite methods for comparison ac-
cording to their performance on public datasets, including two
gate-based methods (AutoField and LPFS) and three sensitivity-
based methods (SHARK, Permutation, and SFS).
The feature preprocessing is slightly different from the public
dataset. Specifically, we utilize AutoDis [ 15] technique to directly
convert dense features into embeddings with the same size as sparse
features. As for multi-hot features, we adopt the reduce-mean oper-
ator to aggregate multiple embeddings into one feature embedding.
Finally, the preprocessed features are fed into the FibiNet backbone.
After training, we grab each methodâ€™s feature importance and
perform top-k experiments. The experiment result is demonstrated
in Table 6 and Figure 4. From the redundant feature elimination
perspective, SHARK, AutoField, and Permutation perform the best.
They eliminate about half of the feature sets (80 features) without
decreasing model accuracy. However, from an effective feature
mining perspective, AutoField performs best for small feature sets,
aligning with our observations from the public datasets. We can
also find that AutoField achieves the highest AUKC in Table 6,
demonstrating its robustness and stability across different numbers
of selections. LPFSâ€™s performance is slightly misaligned with the
one on the public dataset. We attribute this misalignment to the
sensitive learnable polarization module.
3.6 Online Experiments (RQ5)
To validate the effectiveness of our benchmark in online environ-
ments, we reconfigure partial benchmarks to adapt to our feature
factory, package them into a new toolkit, and conduct online tasks
with this toolkit for redundant feature elimination tasks.
Specifically, we create an experiment group that reduces 30% fea-
tures with our toolkit in a latency-sensitive online recommendation
platform. Then, we deploy the experimental group for the A/B test
with 5% traffic (approximately 1 million users). After one weekâ€™s
observation, the inference latency was reduced by approximately
20% while the number of video views (vv) and average play duration
remained the same. This experiment proves the effectiveness of our
benchmark toolkits in the online environment.
3.7 Similarity Visualization (RQ6)
In this section, we visualize the similarity between feature rank-
ings of different feature selection methods in DRS across 4 public
datasets. Figure 5 shows the heatmap of pair-wise Spearman corre-
lation similarities. In the heatmap, the x-axis and y-axis represent
different feature selection methods. The color of each square indi-
cates the level of similarity. The more the color leans towards dark
green, the more similar between the feature rankings derived from
the two feature selection methods. Conversely, the more the color
leans towards light green, the greater the difference in the feature
rankings selected by the two methods.
We can discern the following information from the heatmap:
1) The heatmap displays three darker clusters in the upper left,
middle, and lower right. This is because the features selected by
5201ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Lasso
GBDT
RF
XGBoost
AutoField
OptFS
LPFS
Permutation
SHARK
SFSLasso
GBDT
RF
XGBoost
AutoField
OptFS
LPFS
Permutation
SHARK
SFSPair-wise Spearman Correlation Heatmap
âˆ’0.4âˆ’0.20.00.20.40.60.81.0
Figure 5: Similarity between feature selection methods.
the three different category methods are more similar within each
category. 2) The rankings of LPFS and OptFS are highly similar. This
is because they are both gate-based methods and use the L0 regular-
ization term to control the sparsity of the gates. 3) The rankings of
AutoField and Permutation are the most dissimilar. This aligns with
our conclusion in Section 3.3. AutoField, as a gate-based method,
determines feature importance by learning the gate weights of dif-
ferent feature fields during the modelâ€™s training process, aiding in
uncovering features rich in information. On the other hand, Per-
mutation, as a sensitivity-based feature selection method, judges
feature importance based on the loss of effect caused by shuffling a
particular feature. This approach is more conducive to identifying
features with less information. Therefore, their rankings show sig-
nificant differences. 4) The rankings of SHARK and SFS are quite
similar because they both determine feature importance based on
the gradient of the features, sharing the same source of information.
4 RELATED WORKS
Feature Selection Methods. Machine learning has shown supe-
rior results in data mining [ 21â€“25,28,47]. The modeling perfor-
mance relies heavily on input features, making feature selection
a crucial part of feature engineering. Traditional feature selection
methods fall into three categories: filter, wrapper, and embedded
methods [ 18]. Filter methods utilize criteria to identify predictive
feature fields, exemplified by the Chi2 score [ 31] and mutual infor-
mation [ 42]. Wrapper methods employ black-box models to select
predictive features, assessing the utility of feature subsets through,
often, generic algorithms [ 14,39]. Embedded methods, on the other
hand, integrate the feature selection process within the prediction
model, thereby evaluating the effectiveness of feature subsets in
conjunction. Notable embedded methods include LASSO [ 41] and
Gradient Boosting Machine [ 13]. The advent of deep learning has
spurred novel approaches [ 5,27,30,33,48,49,51], such as gate-
based methods, where learnable gates determine the significance
of feature fields [ 26,29,35,45], and sensitivity-based methods,leveraging gradient techniques to identify critical features by their
sensitivity [ 11,44,46]. Furthermore, some studies explored rein-
forcement learning to automate feature selection, using agents to
pinpoint predictive features [10, 34, 50].
Considering their appliability to DRS, our benchmark focus on
traditional embedded methods (termed â€œshallow methodsâ€), along-
side gate-based and sensitivity-based methods. Other techniques
often fall short in terms of effectiveness or efficiency within the
DRS context. Specifically, filter methods might overlook the DRS
modelâ€™s nuances by relying solely on data-intrinsic relationships,
leading to suboptimal outcomes. Besides, wrapper and reinforce-
ment learning methods, typically designed for smaller datasets,
become impractically time-consuming for the expansive datasets
characteristic of DRS, which can encompass millions of samples.
Benchmarks for Feature Selection. Most existing benchmarks
are tailored for classical downstream models [ 2,3] and rely on syn-
thetic or domain-specific datasets [ 1,9,12]. Due to their constrained
relevance, these benchmarks frequently fall short in providing the
necessary guidance to propel research forward in the domain of
DRS feature selection. The most related work, DeepLasso [ 8], con-
fines its evaluation to shallow methods and a narrow set of back-
bone models, primarily targeting tabular learning tasks. Despite
utilizing real-world datasets, the scale of these datasets pales in com-
parison to those encountered in DRS scenarios, thus limiting the
provision of actionable insights for DRS feature selection strategies.
ERASE undertakes a thorough evaluation of shallow, gate-based,
and sensitivity-based methods across an array of representative DRS
backbones, leveraging both large-scale public datasets and private
industrial datasets. Furthermore, ERASE proposes a novel metric
designed to assess feature selection efficacy comprehensively.
5 CONCLUSION
In this study, we introduce ERASE, a comprehensive bEnchmaRk
for fe Ature SElection for deep recommender systems (DRS). ERASE
integrates a broad spectrum of feature selection methods pertinent
to DRS and ensures fair and comprehensive experimentation across
four public datasets as well as real-world industrial datasets. Fur-
thermore, ERASE pioneers the adoption of the AUKC metric, de-
vised to address the shortcomings of existing metrics by offering
a thorough evaluation of the robustness and stability of feature
selection methods over various numbers of selected features. The
empirical validation provided by both offline and online analyses on
industrial datasets underscores the applicability of our findings in
practical settings, offering valuable perspectives on the deployment
of feature selection methods within DRS environments.
6 ACKNOWLEDGMENTS
This research was partially supported by Huawei (Huawei Inno-
vation Research Program), Research Impact Fund (No.R1015-23),
APRC - CityU New Research Initiatives (No.9610565, Start-up Grant
for New Faculty of CityU), CityU - HKIDS Early Career Research
Grant (No.9360163), Hong Kong ITC Innovation and Technology
Fund Midstream Research Programme for Universities Project (No.
ITS/034/22MS), Hong Kong Environmental and Conservation Fund
(No. 88/2022), and SIRG - CityU Strategic Interdisciplinary Research
Grant (No.7020046, No.7020074).
5202KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pengyue Jia, et al.
REFERENCES
[1]VerÃ³nica BolÃ³n-Canedo, Noelia SÃ¡nchez-MaroÃ±o, and Amparo Alonso-Betanzos.
2013. A review of feature selection methods on synthetic data. Knowledge and
information systems 34 (2013), 483â€“519.
[2]VerÃ³nica BolÃ³n-Canedo, Noelia SÃ¡nchez-Marono, Amparo Alonso-Betanzos,
JosÃ© Manuel BenÃ­tez, and Francisco Herrera. 2014. A review of microarray
datasets and applied feature selection methods. Information sciences 282 (2014).
[3]Andrea Bommert, Xudong Sun, Bernd Bischl, JÃ¶rg RahnenfÃ¼hrer, and Michel Lang.
2020. Benchmark for filter methods for feature selection in high-dimensional
classification data. Computational Statistics & Data Analysis 143 (2020), 106839.
[4] Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5â€“32.
[5]Bo Chen, Xiangyu Zhao, Yejing Wang, Wenqi Fan, Huifeng Guo, and Ruim-
ing Tang. 2024. A comprehensive survey on automated machine learning for
recommendations. ACM Transactions on Recommender Systems 2, 2 (2024), 1â€“38.
[6]Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785â€“794.
[7]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7â€“10.
[8]Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping,
C. Bruss, Andrew Wilson, Tom Goldstein, and Micah Goldblum. 2023. A
Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning.
InNeurIPS 2023 Second Table Representation Learning Workshop.
[9] SL Shiva Darshan and CD Jaidhar. 2018. Performance evaluation of filter-based
feature selection techniques in classifying portable executable files. Procedia
Computer Science 125 (2018), 346â€“356.
[10] Wei Fan, Kunpeng Liu, Hao Liu, Pengyang Wang, Yong Ge, and Yanjie Fu. 2020.
AutoFS: Automated Feature selection via diversity-aware interactive reinforce-
ment learning. In 2020 IEEE International Conference on Data Mining (ICDM).
IEEE, 1008â€“1013.
[11] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models are
Wrong, but Many are Useful: Learning a Variableâ€™s Importance by Studying an
Entire Class of Prediction Models Simultaneously. Journal of machine learning
research: JMLR 20 (2019).
[12] George Forman. 2003. An Extensive Empirical Study of Feature Selection Metrics
for Text Classification. Journal of Machine Learning Research 3 (2003), 1289â€“1305.
[13] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting
machine. Annals of statistics (2001), 1189â€“1232.
[14] Pablo M Granitto, Cesare Furlanello, Franco Biasioli, and Flavia Gasperi. 2006.
Recursive feature elimination with random forest for PTR-MS analysis of agroin-
dustrial products. Chemometrics and intelligent laboratory systems (2006).
[15] Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang
He. 2021. An embedding learning framework for numerical features in ctr
prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 2910â€“2918.
[16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[17] Yi Guo, Zhaocheng Liu, Jianchao Tan, Chao Liao, Daqing Chang, Qiang Liu,
Sen Yang, Ji Liu, Dongying Kong, Zhi Chen, et al .2022. LPFS: Learnable Po-
larizing Feature Selection for Click-Through Rate Prediction. arXiv preprint
arXiv:2206.00267 (2022).
[18] Isabelle Guyon and AndrÃ© Elisseeff. 2003. An introduction to variable and feature
selection. Journal of machine learning research 3, Mar (2003), 1157â€“1182.
[19] Jeff Heaton. 2016. An empirical analysis of feature engineering for predictive
modeling. In SoutheastCon 2016. IEEE, 1â€“6.
[20] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-
ture importance and bilinear feature interaction for click-through rate prediction.
InProceedings of the 13th ACM Conference on Recommender Systems. 169â€“177.
[21] Pengyue Jia, Ling Chen, and Dandan Lyu. 2024. Fine-Grained Population Mobility
Data-Based Community-Level COVID-19 Prediction Model. Cybernetics and
Systems 55, 1 (2024), 184â€“202.
[22] Pengyue Jia, Yiding Liu, Xiaopeng Li, Xiangyu Zhao, Yuhao Wang, Yantong
Du, Xiao Han, Xuetao Wei, Shuaiqiang Wang, and Dawei Yin. 2024. G3: An
Effective and Adaptive Framework for Worldwide Geolocalization Using Large
Multi-Modality Models. arXiv preprint arXiv:2405.14702 (2024).
[23] Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao, Shuaiqiang
Wang, and Dawei Yin. 2023. Mill: Mutual verification with large language models
for zero-shot query expansion. arXiv preprint arXiv:2310.19056 (2023).
[24] Pengyue Jia, Yichao Wang, Shanru Lin, Xiaopeng Li, Xiangyu Zhao, Huifeng Guo,
and Ruiming Tang. 2024. D3: A Methodological Exploration of Domain Division,
Modeling, and Balance in Multi-Domain Recommendations. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 38. 8553â€“8561.
[25] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for
generating image descriptions. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 3128â€“3137.[26] Youngjune Lee, Yeongjong Jeong, Keunchan Park, and SeongKu Kang. 2023. MvFS:
Multi-view Feature Selection for Recommender System. In Proceedings of the
32nd ACM International Conference on Information and Knowledge Management.
[27] Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze
Wu, and Ruocheng Guo. 2023. Automlp: Automated mlp for sequential recom-
mendations. In Proceedings of the ACM Web Conference 2023. 1190â€“1198.
[28] Xiaopeng Li, Lixin Su, Pengyue Jia, Xiangyu Zhao, Suqi Cheng, Junfeng Wang,
and Dawei Yin. 2023. Agent4Ranking: Semantic Robust Ranking via Personalized
Query Rewriting Using Multi-agent LLM. arXiv preprint arXiv:2312.15450 (2023).
[29] Weilin Lin, Xiangyu Zhao, Yejing Wang, Tong Xu, and Xian Wu. 2022. AdaFS:
Adaptive Feature Selection in Deep Recommender System. In Proceedings of the
28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD).
[30] Dugang Liu, Chaohua Yang, Xing Tang, Yejing Wang, Fuyuan Lyu, Weihong Luo,
Xiuqiang He, Zhong Ming, and Xiangyu Zhao. 2024. MultiFS: Automated Multi-
Scenario Feature Selection in Deep Recommender Systems. In Proceedings of the
17th ACM International Conference on Web Search and Data Mining. 434â€“442.
[31] Huan Liu and Rudy Setiono. 1995. Chi2: Feature selection and discretization of
numeric attributes. In Proceedings of 7th IEEE international conference on tools
with artificial intelligence. IEEE, 388â€“391.
[32] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS: Differentiable
Architecture Search. In International Conference on Learning Representations.
[33] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020.
Automated embedding size search in deep recommender systems. In Proceedings
of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval. 2307â€“2316.
[34] Kunpeng Liu, Yanjie Fu, Pengfei Wang, Le Wu, Rui Bo, and Xiaolin Li. 2019.
Automating feature subspace exploration via multi-agent reinforcement learning.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 207â€“215.
[35] Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, and Xue Liu.
2023. Optimizing Feature Set for Click-Through Rate Prediction. In Proceedings
of the ACM Web Conference 2023. 3386â€“3395.
[36] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire space multi-task model: An effective approach for estimating
post-click conversion rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval. 1137â€“1140.
[37] Steffen Rendle. 2010. Factorization Machines. In Proceedings of the 2010 IEEE
International Conference on Data Mining. 995â€“1000.
[38] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. 452â€“461.
[39] Shital C Shah and Andrew Kusiak. 2004. Data mining and genetic algorithm
based gene/SNP selection. Artificial intelligence in medicine 31, 3 (2004), 183â€“196.
[40] Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007.
Bias in random forest variable importance measures: Illustrations, sources and a
solution. BMC bioinformatics 8, 1 (2007), 1â€“21.
[41] Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal
of the Royal Statistical Society: Series B (Methodological) (1996).
[42] Jorge R Vergara and Pablo A EstÃ©vez. 2014. A review of feature selection methods
based on mutual information. Neural computing and applications 24 (2014).
[43] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDDâ€™17. 1â€“7.
[44] Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Bo Chen, Huifeng Guo, Ruiming
Tang, and Zhenhua Dong. 2023. Single-shot Feature Selection for Multi-task
Recommendations. In Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 341â€“351.
[45] Yejing Wang, Xiangyu Zhao, Tong Xu, and Xian Wu. 2022. AutoField: Automating
Feature Selection in Deep Recommender Systems. In Proceedings of the ACM Web
Conference.
[46] Beichuan Zhang, Chenggen Sun, Jianchao Tan, Xinjun Cai, Jun Zhao, Mengqi
Miao, Kang Yin, Chengru Song, Na Mou, and Yang Song. 2023. SHARK: A Light-
weight Model Compression Approach for Large-Scale Recommender Systems.
InProceedings of the 32nd ACM International Conference on Information and
Knowledge Management (CIKM â€™23).
[47] Zijian Zhang, Shuchang Liu, Jiaao Yu, Qingpeng Cai, Xiangyu Zhao, Chunxu
Zhang, Ziru Liu, Qidong Liu, Hongwei Zhao, Lantao Hu, et al .2024. M3oE: Multi-
Domain Multi-Task Mixture-of Experts Recommendation Framework. arXiv
preprint arXiv:2404.18465 (2024).
[48] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, and Chong Wang.
2021. Autoloss: Automated loss function search in recommendations. In Pro-
ceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining.
[49] Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida
Wang, Huiji Gao, and Bo Long. 2021. Autodim: Field-aware embedding dimension
searchin recommender systems. In Proceedings of the Web Conference 2021.
[50] Xiaosa Zhao, Kunpeng Liu, Wei Fan, Lu Jiang, Xiaowei Zhao, Minghao Yin, and
Yanjie Fu. 2020. Simplifying reinforced feature selection via restructured choice
strategy of single agent. In 2020 IEEE International Conference on Data Mining
5203ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(ICDM). IEEE, 871â€“880.
[51] Xiangyu Zhaok, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong Wang,
Ming Chen, Xudong Zheng, Xiaobing Liu, and Xiwang Yang. 2021. Autoemb:
Automated embedding dimensionality search in streaming recommendations. In
2021 IEEE International Conference on Data Mining (ICDM). IEEE, 896â€“905.
[52] Ruiqi Zheng, Liang Qu, Bin Cui, Yuhui Shi, and Hongzhi Yin. 2023. Automl for
deep recommender systems: A survey. ACM Transactions on Information Systems
41, 4 (2023), 1â€“38.
Table 7: Dataset statistics
Dataset
Avazu Criteo Movielens-1M AliCCP
Interactions
40,428,967 45,850,617 1,000,209 85,316,519
Users N/A N/A 6,040 238,635
Items N/A N/A 3,706 467,298
Interaction Type Click Click Rating (1-5) Click
Feature Num 23 39 9 23
A DATASETS
In this subsection, we introduce the datasets utilized in our bench-
mark. The statistics of these datasets are listed in Table 7. We select
the following four datasets for use:
â€¢Avazu. Avazu is a commonly used dataset in the field of feature
selection for deep recommendation systems. It consists of 23
features and 40,428,967 interaction records. We divide the dataset
into training, validation, and test sets in a 7:2:1 ratio.
â€¢Criteo. Criteo is another frequently used dataset for studying
feature selection in Deep Recommendation Systems. The Criteo
dataset contains 45,850,617 samples and 39 features, offering a
larger number of features. We also adopt a 7:2:1 ratio for dividing
the data into training, validation, and test sets to train the model.
â€¢Movielens-1m. The Movielens dataset is a well-known public
movie dataset in the field of recommendation systems, featuring
multiple variants with different data volumes to meet diverse
research needs. We choose the Movielens-1M dataset, which
comprises 9 features. The limited number of features presents a
greater challenge for feature selection methods. In our experi-
ments with Movielens, we also divide the dataset into training,
validation, and test sets according to a 7:2:1 ratio. The interactions
with a rating bigger than 3 are considered as positive samples.
â€¢AliCCP. Alibaba Click and Conversion Prediction (AliCCP) dataset
is extracted from the real-world e-commerce platform Taobao. It
is a popular dataset used for Click-Through Rate (CTR) estima-
tion. It consists of 23 features and 85,316,519 samples, making
it an effective tool for evaluating feature selection methods in
scenarios closely resembling real advertising contexts. We follow
the original AliCCP splitting approach [ 36], allocating 50% of
the data for training, with the remaining data split equally into
validation and test sets in a 1:1 ratio.
B BACKBONE MODELS
we apply a variety of feature selection methods to the following
four popular deep recommendation models in this work.
â€¢Wide&Deep [ 7].Wide and Deep (Wide&Deep) model is devel-
oped by Google to improve the recommender systems. It com-
bines wide and deep models to fit specific feature combinations in
large-scale datasets and previously unseen feature combinations
through low-dimensional dense embeddings.â€¢DeepFM [ 16].DeepFM is an effective model in recommender
systems. It combines the strengths of FM models and deep neural
networks to extract both low-order feature interactions and high-
order feature interactions.
â€¢DCN [ 43].The Deep & Cross Network (DCN) model is proposed
by Google to capture both explicit and implicit feature interac-
tions for prediction tasks. The cross layers have cross operations
that learn complex feature interactions to improve the prediction
performance.
â€¢FibiNet [ 20].The Feature Importance and Bilinear feature Inter-
action NETwork (FibiNet) is an innovative model in CTR predic-
tion. It introduces the SENET and bilinear layer. SENET learns
feature importance adaptively and the bilinear layer captures
high-order feature interactions.
C METRICS
In this section, we detail the two frequently used metrics in recom-
mender systems: AUC and Logloss.
1)AUC. Area Under the ROC Curve (AUC) measures the area
under the ROC curve. The ROC curve is one graph shows that the
classification performance of the model under different thresholds.
In addition, in random positive-negative pairs, AUC also represents
the probability that the model ranks the positive sample with a
higher score than the negative one. It can be formalized as follows:
ğ´ğ‘ˆğ¶ =Ãğ‘€
ğ‘–ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘–âˆ’ğ‘€(1+ğ‘€)/2
ğ‘€Ã—ğ‘, ğ‘–âˆˆğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ¶ğ‘™ğ‘ğ‘ ğ‘  (3)
whereğ‘€denotes the number of positive samples and ğ‘is the
number of negative samples.
2)Logloss. Logloss is also named Binary Cross-Entropy Loss,
it is always used in binary classification tasks. The formula for
Logloss is described as follows:
ğ¿ğ‘œğ‘”ğ‘™ğ‘œğ‘ ğ‘  =âˆ’1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1[ğ‘¦ğ‘–ğ‘™ğ‘œğ‘”(Ë†ğ‘¦ğ‘–)+(1âˆ’ğ‘¦ğ‘–)ğ‘™ğ‘œğ‘”(1âˆ’Ë†ğ‘¦ğ‘–)] (4)
whereğ‘is the number of samples, ğ‘¦ğ‘–denotes the predicted label
forğ‘–th sample, and Ë†ğ‘¦ğ‘–denotes the ground truth for ğ‘–th sample.
D FORMULA DERIVATION OF AUKC
In this section, we detail the formula derivation process of AUKC.
AUKC is a comprehensive metric for assessing the AUC perfor-
mance of a given feature selection method across different numbers
of feature selections. Under ideal circumstances, experiments can
be conducted for each number of selections to obtain the corre-
sponding AUC metrics, and then derive AUKC:
ğ´ğ‘ˆğ¾ğ¶ =Ã|ğ¾|
ğ‘˜=1((ğ´ğ‘ˆğ¶ğ‘˜âˆ’0.5)+(ğ´ğ‘ˆğ¶ğ‘˜+1âˆ’0.5))/2
|ğ¾|/2(5)
=1
|ğ¾||ğ¾|âˆ‘ï¸
ğ‘˜=1(ğ´ğ‘ˆğ¶ğ‘˜+ğ´ğ‘ˆğ¶ğ‘˜+1âˆ’1) (6)
where|ğ¾|denotes the number of all features, ğ´ğ‘ˆğ¶ğ‘˜is the AUC
metric with ğ‘˜features selected. When there is no feature selected,
the model will predict labels randomly (i.e., ğ´ğ‘ˆğ¶ =0.5). Therefore,
we subtract 0.5 from ğ´ğ‘ˆğ¶ğ‘˜to obtain the improvement.
5204KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pengyue Jia, et al.
Table 8: Overall experimental results of feature selection for deep recommender systems.
Backb
one model MethodsAvazu Criteo Movielens-1M AliCCP
AUC
Logloss AUC Logloss AUC Logloss AUC Logloss
De
epFMno_selection 0.78576 0.37680 0.80024 0.45321 0.79027 0.54124 0.61813 0.16176
Lasso 0.78587 0.37654 0.79832 0.45483 0.80683 0.52498
0.61887 0.16174
GBDT 0.76930 0.38571 0.80011 0.45329 0.78942 0.54234 0.61864 0.16150
RF 0.78642
0.37630 0.79920 0.45427 0.78920 0.54217 0.61772 0.16172
XGBoost 0.76953 0.38548 0.80022 0.45333 0.78974 0.54190 0.61822 0.16172
AutoField 0.78611 0.37641 0.80022 0.45325 0.80722
0.52471 0.61894
0.16174
AdaFS 0.78278 0.37922 0.79950 0.45405 0.78590 0.54676 0.61726 0.16158
OptFS
0.78624 0.37627
0.79934 0.45505 0.79027 0.54124 0.61551 0.16195
LPFS 0.78840 0.37601 0.80080 0.45364 0.78885 0.54276 0.61941 0.17864
Permutation 0.78624 0.37640 0.80006 0.45349 0.78974 0.54207 0.61844 0.16174
SHARK 0.78611 0.37643 0.80035
0.45329 0.78888 0.54342 0.61840 0.16174
SFS 0.78626 0.37631 0.80020 0.45330 0.78982 0.54194 0.61835 0.16168
Wide&De
epno_selection 0.78574 0.37679 0.79971 0.45346 0.79041 0.54163 0.62137 0.16154
Lasso 0.78578 0.37665 0.79761 0.45526 0.80750 0.52400 0.62096 0.16159
GBDT 0.76900 0.38583 0.79979 0.45337 0.78926 0.54252 0.62187
0.16146
RF 0.78571 0.37664 0.79907 0.45400 0.78992 0.54170 0.62202 0.16132
XGBoost 0.76886 0.38587 0.80023
0.45320 0.78893 0.54266 0.62174 0.16142
AutoField 0.78601 0.37653 0.79971 0.45348 0.80736 0.52496
0.62176 0.16134
AdaFS 0.78607 0.37665 0.79952 0.45394 0.78703 0.54467 0.62106 0.16123
OptFS
0.78582 0.37654 0.80111 0.45284 0.79041 0.54163 0.62053 0.16144
LPFS 0.78606 0.37680 0.79953 0.45477 0.79072 0.54136 0.61916 0.17177
Permutation 0.78607 0.37647
0.79988 0.45342 0.78936 0.54226 0.62156 0.16138
SHARK 0.78592 0.37655 0.80019 0.45313
0.78980 0.54146 0.62148 0.16154
SFS 0.78592 0.37646 0.80010 0.45326 0.78993 0.54139 0.62165 0.16122
DCNno_sele
ction 0.78660 0.37615 0.80045 0.45297 0.78966 0.54228 0.62315
0.16134
Lasso 0.78628 0.37638 0.79860 0.45456 0.80830 0.52367
0.62289 0.16124
GBDT 0.76926 0.38578 0.80049 0.45299 0.79106 0.54048 0.62318 0.16131
RF 0.78644 0.37614 0.79978 0.45342 0.79058 0.54058 0.62310 0.16156
XGBoost 0.76934 0.38565 0.80067 0.45264
0.79102 0.54050 0.62321 0.16131
AutoField 0.78663 0.37620 0.80051 0.45286 0.80932 0.52226 0.62294 0.16114
AdaFS 0.78706 0.37597 0.80018 0.45336 0.78792 0.54366 0.62264 0.16122
OptFS 0.78663 0.37618 0.80164 0.45222 0.78966 0.54228 0.62083 0.16183
LPFS 0.78686
0.37631 0.80048 0.45349 0.79006 0.54123 0.62202 0.16536
Permutation 0.78651 0.37616 0.80052 0.45336 0.79047 0.54086 0.62311 0.16116
SHARK
0.78656 0.37614
0.80063 0.45300 0.79116 0.54019 0.62302 0.16127
SFS 0.78649 0.37620 0.80066 0.45273 0.79066 0.54067 0.62299 0.16118
FibiNetno_sele
ction 0.79039 0.37401 0.80528 0.44854 0.78490 1.44348 0.62138 0.16138
Lasso 0.79057 0.37399 0.80345 0.45012 0.80350 0.56017 0.62147 0.16162
GBDT 0.77222 0.38403 0.80537 0.44846 0.78213 1.44155 0.62257
0.16145
RF 0.79053 0.37377 0.80451 0.44917 0.78421 0.61107 0.62166 0.16133
X
GBoost 0.77202 0.38436 0.80554 0.44828
0.78356 1.25796 0.62132 0.16147
AutoField 0.79068 0.37371 0.80523 0.44864 0.80091 0.54683 0.62272 0.16137
A
daFS 0.78959 0.37441 0.80439 0.44953 0.78478 0.54911 0.61938 0.16158
OptFS 0.79059 0.37383 0.80246 0.45159 0.78490 1.44348 0.61958 0.16154
LPFS 0.79093 0.37385 0.80559 0.44898 0.78790 0.54625 0.61750 0.16797
Permutation 0.79071 0.37405 0.80515 0.44861 0.78457 0.67354 0.62136 0.16161
SHARK 0.79078 0.37364 0.80550
0.44833 0.78574 1.33867 0.62198 0.16123
SFS 0.79068 0.37369 0.80557 0.44826 0.78507
1.20824 0.62219 0.16140
When the number of selected features is unevenly distributed
across the length of the set of features (e.g., in practice, the gran-
ularity of the feature selection number often becomes finer as it
approaches the total number of features, and coarser when there
are fewer features), AUKC has a more general format:
ğ´ğ‘ˆğ¾ğ¶ =Ã|ğ‘|
ğ‘›=1((ğ´ğ‘ˆğ¶ğ‘›ğ‘™âˆ’0.5)+(ğ´ğ‘ˆğ¶ğ‘›ğ‘Ÿâˆ’0.5))Ã—Î”ğ‘™ğ‘›/2
|ğ¾|/2(7)
=1
|ğ¾||ğ‘|âˆ‘ï¸
ğ‘›=1(ğ´ğ‘ˆğ¶ğ‘›,ğ‘™+ğ´ğ‘ˆğ¶ğ‘›,ğ‘Ÿâˆ’1)Ã—Î”ğ‘™ğ‘› (8)
where|ğ‘|represents the number of segments for the number of
features selected across the entire length of the feature set. Forexample, with 6 features, if experiments are conducted at feature
counts of 2,4,5, and 6, then |ğ‘|=4.ğ´ğ‘ˆğ¶ğ‘›,ğ‘™is the AUC correspond-
ing to the number of features selected at the left endpoint of the
ğ‘›th segment, and ğ´ğ‘ˆğ¶ğ‘›,ğ‘Ÿdenotes the AUC with the number of
selections at the right endpoint of the ğ‘›th segment. Î”ğ‘™ğ‘›represents
the length of the ğ‘›th segment.
E COMPLETE OVERALL PERFORMANCE
Due to page limitations, we can not include the complete overall
experimental results in the main content. We list the complete
results in Table 8 for reference.
5205