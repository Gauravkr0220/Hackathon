CAFO: Feature-Centric Explanation on Time Series Classification
Jaeho Kim
kjh3690@unist.ac.kr
Ulsan National Institute of Science
and Technology (UNIST)
Artificial Intelligence Graduate School
Ulsan, South KoreaSeok-Ju Hahn
seokjuhahn@unist.ac.kr
Ulsan National Institute of Science
and Technology (UNIST)
Department of Industrial Engineering
Ulsan, South KoreaYoontae Hwang
yoontae@unist.ac.kr
Ulsan National Institute of Science
and Technology (UNIST)
Department of Industrial Engineering
Ulsan, South Korea
Junghye Lee
junghye@snu.ac.kr
Seoul National University (SNU)
Technology Management, Economics
and Policy Program &
Graduate School of Engineering
Practice &
Institute of Engineering Research
Seoul, South KoreaSeulki Leeâ€ 
seulki.lee@unist.ac.kr
Ulsan National Institute of Science
and Technology (UNIST)
Computer Science and Engineering &
Artificial Intelligence Graduate School
Ulsan, South Korea
ABSTRACT
In multivariate time series (MTS) classification, finding the impor-
tant features (e.g., sensors) for model performance is crucial yet chal-
lenging due to the complex, high-dimensional nature of MTS data,
intricate temporal dynamics, and the necessity for domain-specific
interpretations. Current explanation methods for MTS mostly focus
on time-centric explanations, apt for pinpointing important time
periods but less effective in identifying key features. This limita-
tion underscores the pressing need for a feature-centric approach,
a vital yet often overlooked perspective that complements time-
centric analysis. To bridge this gap, our study introduces a novel
feature-centric explanation and evaluation framework for MTS,
named CAFO (Channel Attention and Feature Orthgonalization).
CAFO employs a convolution-based approach with channel atten-
tion mechanisms, incorporating a depth-wise separable channel
attention module (DepCA) and a QR decomposition-based loss for
promoting feature-wise orthogonality. We demonstrate that this or-
thogonalization enhances the separability of attention distributions,
thereby refining and stabilizing the ranking of feature importance.
This improvement in feature-wise ranking enhances our under-
standing of feature explainability in MTS. Furthermore, we develop
metrics to evaluate global and class-specific feature importance.
Our frameworkâ€™s efficacy is validated through extensive empirical
analyses on two major public benchmarks and real-world datasets,
both synthetic and self-collected, specifically designed to highlight
class-wise discriminative features. The results confirm CAFOâ€™s ro-
bustness and informative capacity in assessing feature importance
in MTS classification tasks. This study not only advances the un-
derstanding of feature-centric explanations in MTS but also sets a
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671724foundation for future explorations in feature-centric explanations.
The codes are available at https://github.com/eai-lab/CAFO.
CCS CONCEPTS
â€¢Computing methodologies â†’Learning to rank ;Temporal
reasoning.
KEYWORDS
Time Series Classification, Feature-Centric Explanation, Explain-
able AI
ACM Reference Format:
Jaeho Kim, Seok-Ju Hahn, Yoontae Hwang, Junghye Lee, and Seulki Leeâ€ .
2024. CAFO: Feature-Centric Explanation on Time Series Classification. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671724
1 INTRODUCTION
With the advancement of Internet of Things (IoT) technologies, time
series classification (TSC) tasks have proliferated in recent years.
A notable characteristic of TSC data derived from these sources is
that they are usually 1) multivariate; that is, they contain multiple
measurements or sensors and 2) characterized by patterns that
are complex and intertwined, which poses challenges for semantic
interpretation [ 3,34] by humans, a stark contrast to more intuitively
graspable domains of image and text data. These multivariate time
series (MTS) data have found practical applications ranging from
the classification of human activities to the detection of industrial
faults [10, 22], demonstrating its broad applicability.
As MTS data find broader applications, an essential need emerges
in the phase of model development. Engineers and domain ex-
perts seek not just to use these models but to understand how
they process data. This understanding is vital; it can drastically re-
duce computational and manufacturing costs and foster confidence
â€ S. Lee is the corresponding author
 
1372
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jaeho Kim, Seok-Ju Hahn, Yoontae Hwang, Junghye Lee, and Seulki Leeâ€ 
Figure 1: Overview of CAFO: (A) End-to-end training. Raw time series are converted into images using image encoding methods, followed by
the extraction of channel-wise attention scores using the DepCA+QR Module. These attention scores are element-wise multiplied to image
features for end-to-end model training. (B) DepCA assesses feature contributions, while QR-Ortho Loss minimizes feature redundancy through
orthogonality regularization. (C) Feature Importance Calculation. The calculated attention scores are utilized to explain MTS data via Global
Importance (GI) and Class-Wise Relative Importance (CWRI) metrics.
in the modelâ€™s deployment, ensuring it leverages features recog-
nized as important [ 47]. In this context, the role of explainable
AI (XAI) is not just beneficial but indispensable. Yet, a concern-
ing observation arises: XAI research in MTS has predominantly
concentrated on generating time-step-specific or instance-specific
explanations [ 7,19,32,40,41], focusing narrowly on segments of
time critical to the modelâ€™s decision-making in a given instance.
Such local explanations, while invaluable in contexts like healthcare,
reveal a significant gap for a more comprehensive, feature-centric
overview that can provide a broader understanding of the data.
In MTS, a â€˜featureâ€™ is commonly identified as a separate channel or
measurement variable, which is independent of the time axis. This
need is particularly acute in both industry and academia, where
a global understanding of TSC tasks, at both the class level and
across the model as a whole, is crucial. However, previous MTS
XAI works [ 2,17] have addressed this topic in a limited manner,
leaving room for more extensive exploration and discussion.
Herein lies the main theme of our paper: we address the
imperative need for a feature-centric explanation - a perspec-
tive that is not just complementary but essential to the time-centric
explanation in the MTS classification task.
To illustrate the practical impact of our approach, consider the
example of a smart shoe manufacturer employing a deep learning
model for classifying user activities based on sensor data (accelerom-
eters, force sensors, etc.). Unlike previous methods that used four-
teen sensors (or features) for 95% accuracy [ 22], our feature-centric
analysis achieves a near-comparable 94% accuracy with just three
key sensors (See Fig. 4 where we sequentially dropped important
and unimportant features and measured the model performance).
This insightful identification of key sensors would not have been
easily attainable through previous time-step or local instance-based
explanation methods. Conversely, relying on the three least im-
portant sensors, as identified by our model, drops accuracy to 71%,
highlighting the critical nature of sensor selection. Moreover, this
revelation goes beyond mere accuracy; it offers manufacturers and
engineers invaluable insights. Manufacturers can reduce costs by fo-
cusing on essential sensors, potentially eliminating redundant ones.Engineers gain insights for optimizing model performance. These
scenarios, spanning industrial and academic fields, introduce the
broader application of feature-centric explanation in MTS classifica-
tion tasks. We present detailed use cases motivating feature-centric
explanation throughout this paper and in Appendix A.
While a number of studies have explored time-step based expla-
nations in depth, feature-based explanations have typically been
addressed as a secondary focus or in a limited scope [ 2,17]. To our
knowledge, there has been a lack of discussion regarding feature-
centric explanations in TSC in deep learning, especially with regard
to multivariate data (i.e., MTS)1, presented as a comprehensive re-
search paper. The lack of explanation strategy, appropriate MTS
datasets, characterized by clearly defined feature importance, along-
side matching evaluation protocols, presents a considerable chal-
lenge [ 32] in the realm of MTS XAI. Our paper bridges this gap by
presenting the following contributions:
(1)Methodologies. Introduction of Channel Attention and Fea-
ture Orthogonalization (CAFO): (1) DepCA, a novel convolution-
based framework that utilizes channel attention, which mea-
sures feature importance and (2) QR-Ortho, a QR decomposition-
based regularizer that ensures feature separability for im-
proved feature-centric explanation.
(2)Datasets. Compilation of both synthetic and real world
datasets collected with known class-discriminative features.
(3)Metrics. Development of a comprehensive set of metrics
aimed at quantifying global and class-specific feature impor-
tance, complete with a corresponding evaluation protocol.
(4)Experiments. Extensive number of empirical evaluations
confirming the practical value of the proposed work.
2 PRELIMINARIES AND RELATED WORKS
We first formalize the notation used in our work. To better under-
stand, it is beneficial to have a big overview of CAFO depicted in
Fig. 1. Subsequently, the rest of the section illustrates prior works
that are helpful in understanding our method.
1Our focus is on multivariate time series as we provide feature-centric explanations.
 
1373CAFO: Feature-Centric Explanation on Time Series Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
2.1 Preliminaries
Givenğ‘number of MTS samples, the ğ‘–-th MTS instance X(ğ‘–)=h
x(ğ‘–)
1,...,x(ğ‘–)
ğ·i
âˆˆRğ‘‡Ã—ğ·encompasses ğ‘‡times steps and ğ·features.
In this context, a univariate time series is defined as a single ğ‘—-th fea-
ture sequence x(ğ‘–)
ğ‘—=[ğ‘¥1,...,ğ‘¥ğ‘‡]âŠ¤, and an aggregation of such uni-
variate sequences constitutes an MTS. Traditional time-step-based
explanations offer insights along the temporal ( ğ‘‡) dimension, but
our research pivots towards elucidating the feature ( ğ·) dimension.
Consequently, our primary interest lies in discerning the signifi-
cance of each feature, establishing a hierarchy of feature importance
that is both global and class-specific. Specific to our problem setting,
rather than using the raw MTS as-is, we transform each feature xğ‘—
into a single-channel image of size {0,1}ğ‘‡Ã—ğ‘‡using image encoding
methods. As a whole, the raw MTS input X(ğ‘–)âˆˆRğ‘‡Ã—ğ·is converted
into an image of size X(ğ‘–)âˆˆRğ·Ã—ğ‘‡Ã—ğ‘‡. Employing a channel at-
tention (CA) module, we compute a set of attention scores along
theğ·dimension CA(X(ğ‘–))=a(ğ‘–)=[ğ‘1,...,ğ‘ğ·]âŠ¤within the range
[0,1]ğ·, where each ğ‘ğ‘—represents the attention allocated to the ğ‘—-th
feature channelXğ‘—(equivalent to xğ‘—) .
2.2 Image Encoding of MTS
Image encoding of MTS involves transforming time series data into
image formats, such as a Recurrence Plot (RP) [ 9] or Gramian Angu-
lar Fields (GAF) [ 44]. Encoding time series into images offers several
benefits in analyzing feature-wise importance. First, image encod-
ing operates independently of specific standardization methods [ 9],
which is often crucial due to the heterogeneity of features, e.g., the
varying scales of accelerometers and gyroscopes. This can even
significantly impact the end performance of a modeling [ 52]. Image
encoding, however, primarily relies on point-wise relations (e.g.,
inner products with threshold) to represent time series, thereby
liberating the feature representation from the implicit biases of any
particular scaling method. This aspect ensures a more equitable
comparison and ranking of features. Second, image encoding en-
hances the representation of temporal dependency within features.
By converting the original feature xğ‘—âˆˆRğ‘‡into aXğ‘—âˆˆRğ‘‡Ã—ğ‘‡image,
recurrent patterns become more explicit and discernible [ 9], which
allows for the use of well-curated vision models, e.g., ViT [ 8]. Our
empirical results (Appendix C) suggest these models more effec-
tively discern feature importance in our study, potentially owing
to image encoding or inherent model capabilities. We use Recur-
rence Plot to capture the recurrence patterns in MTS, and explore
alternative encoding techniques such as GAF in Appendix B.
2.3 Channel Attention (CA) Modules
CA is primarily used in the image classification domain to improve
model performance by emphasizing relevant feature channels. The
pioneering SENet [ 18], BAM [ 30], CBAM [ 46], and SIMAM [ 48] har-
ness CA by collating channel specific statistics (e.g. global average),
passing them through parametrized functions to obtain channel or
spatial attention. In contrast to the use of CA in the image domain,
which integrates CA at multiple points within the latent space, our
approach applies CA singularly and directly to the input rep-
resentation, to obtain the attention scores for each feature.
We note that in the time series domain, the joint usage of imageencoding and CA has been previously explored in temporal [ 24],
frequency [ 21], and wavelet [ 12]-based literature, often to augment
model performance, and occasionally to offer interpretative insights
via raw attention visualizations. Our research pioneers the use of
CA scores to systematically evaluate feature importance on a global
and class-specific scale in MTS data.
2.4 Multivariate Time Series Explanation
Post-hoc explanation in multivariate time series (MTS) eluci-
date model decisions by deriving explanations from their output,
making them generally agnostic to the underlying model. In MTS
explanation, several post-hoc methods employed in the image do-
main have been repurposed for MTS by viewing the raw time series
as ağ‘‡Ã—ğ·image. A recent study by TurbÃ© et al. [ 41] has undertaken
a comprehensive assessment of various post-hoc interpretability
methodsâ€” Integrated Gradients [ 36], GradientSHAP [ 26], and Shap-
ley value sampling [ 6]â€”in the context of TSC, highlighting substan-
tial discrepancies in time-centric explanation across methods, also
noted by Schlegel et al [ 32]. Our research extends these observa-
tions, confirming these inconsistencies in feature-centric explana-
tions of MTS, and first identifying the impact of train/validation
distribution on feature importance inconsistency. The study [ 41]
also highlights the limitations on the use of synthetic data in prior
time-centric explanation research [ 19], emphasizing the need for
real-world datasets with clear discriminative features for validating
MTS explanation methods. In the pursuit of enhancing these meth-
ods, past works have applied these post-hoc methods on LSTM [ 15],
TCN [ 23], and Transformer[ 43] models for time-based explana-
tions. Orthogonal to these approaches, DynaMask [ 7] is a post-hoc
method, providing an explanation based on optimizing perturbation
masks for MTS. However, its requirement for numerous optimiza-
tion steps per instance presents a challenge, limiting its efficiency
in global and class-specific importance calculation.
Model-based explanation for time series rely on specific neural
architecture such as recurrent neural networks (RNNs), as these
models inherently handle sequential inputs. Nevertheless, recent
works show that they suffer from saliency vanishing [ 14] and may
have limitations in explaining time series data [ 20]. For example,
TimeSHAP [ 2] is a recurrent explainer extending KernelSHAP [ 26]
to the temporal domain by grouping sequential data into coali-
tions. Shapley-based methods are known to be computationally-
intensive [ 26], while TimeSHAP provides efficient pruning methods
to overcome this. However, pruning relies on the assumption that
recent events have a predominant influence on model outcome
might not apply universally, such as in continuous event recording
like human activity monitoring. Another recurrent-based approach,
FIT [ 40], assigns significance to events using counterfactuals within
a generative model. Unfortunately, training a generator adds an
extra cost, and the explanation depends on the generatorâ€™s perfor-
mance. LAXCAT [ 17] is another model-based explanation method
utilizing both temporal and variable attention scores using 1D con-
volution methods. Our CAFO method, however, distinguishes itself
by employing 2D convolutions and channel attention (CA), offering
a unique structural approach to derive attention scores.
 
1374KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jaeho Kim, Seok-Ju Hahn, Yoontae Hwang, Junghye Lee, and Seulki Leeâ€ 
Figure 2: Visualization of the channel attention (CA) values using t-SNE [ 42] for CBAM [ 46], SE [ 18], SIMAM [ 48], and the proposed DepCA
module on the GILON dataset [ 22]. The Calinski-Harabasz score [ 5] at the bottom indicates their clustering performance (the higher, the better).
As observed, DepCA effectively captures sample and class-specific information even though the CA scores are computed in the early layer of
the network, in contrast to existing methods [18, 46] that compute the CA scores in latent channel spaces (middle layers of the network)
.
3 CAFO: CHANNEL ATTENTION AND
FEATURE ORTHOGONALIZATION
Fig. 1 provides an overview of CAFO for extracting feature-centric
importances from MTS data. Starting with image encoding (specif-
ically recurrence plot (RP) [ 9]), the raw MTS is transformed into
image-like data, where our DepCA module (Sec. 3.1) is used to
compute the channel attention score aâˆˆRğ·. These scores are then
element-wise multiplied with their respective channels, which are
further processed for end-to-end training with downstream
backbone models such as a ResNet [ 13]. Throughout the training,
we employ a novel QR-Ortho loss (Sec. 3.2) to ensure the orthogonal-
ity along the feature dimension of the attention, effectively reducing
feature redundancy through orthogonalization. Upon completion
of the training, we harness these attention scores to compute the
Global Importance (GI) and Class-Wise Relative Importance (CWRI)
metrics, with further details regarding the metric described in Sec. 4.
3.1 Depthwise Channel Attention (DepCA)
In our work, a raw time series XâˆˆRğ‘‡Ã—ğ·is encoded into an image-
like formatXâˆˆRğ·Ã—ğ‘‡Ã—ğ‘‡, with each channel representing a dis-
tinct feature. The CA (Channel Attention) module evaluates these
channels, employing attention scores a=[ğ‘1,...,ğ‘ğ·]âŠ¤for global
and class-specific metric computation. Itâ€™s essential that attention
scores, a, precisely captures each featureâ€™s unique information. To
achieve this, we introduce the Depthwise Channel Attention
(DepCA) module, which surpasses traditional CA techniques in
extracting comprehensive information from each channel. Where
previous CA methods [ 46] rely on simple statistics like global max-
imaâ€”apt for latent channel spaces for efficiencyâ€”our methodol-
ogy, with its depthwise convolution, allows the model to learn
informative statistics from each feature representation. By apply-
ing depthwise convolutions, we treat each feature independently,
capturing distinct details without inter-feature interference. It effi-
ciently extracts clear, differentiated information, as shown by our
t-SNE visualizations in Fig. 2.
The DepCA module begins by applying a set of depthwise con-
volutional filters to the input X. We useğ›¾number of filters per
each feature channel (with ğ›¾=3in all experiments). This yields the
feature descriptor as Convğ›¾(X)=FoutâˆˆR(ğ›¾Ã—ğ·)Ã—HÃ—W, whereğ·
is the number of channels, HandWdenotes the height and width
of the output channels, respectively. Following this, DepCA per-
forms two pooling operations on Fout: an average and max pooling,
executed channel-wise (CW) [ 46] to produce Favg,FmaxâˆˆRğ›¾Ã—ğ·.
Figure 3: Orthogonal regularization on the feature-dimension of the
attentions enhances separability. Using QR-Ortho loss, we demon-
strate an enhanced distinction between previously overlapping at-
tentions in the Gilon dataset [22], consistent across five-fold CV.
These pooled features are then averaged across the channels com-
ing from the same original feature (i.e., feature-wise; FW). As we
aim to provide a feature-centric explanation, we performed FW
pooling to get an attention score for each feature. Finally, the two
output features are combined through element-wise summation to
obtain the aggregated feature representation and then passed to
the sigmoid function ğœ(ğ‘¥)=1/(1+exp(âˆ’ğ‘¥)), to constrain the CA
score between zero and one. Putting it all together, the CA score
of the imageX, denoted as CA(X)=[ğ‘1,...,ğ‘ğ·]âŠ¤âˆˆ[0,1]ğ·where
ğ‘ğ‘—is the attention score of Xğ‘—, is computed as:
Fout=Convğ›¾(X)
Favg=CWAvgPool(Fout),Fmax=CWMaxPool(Fout)
CA(X)â‰œğœ(FWAvgPool(Favg)+FWAvgPool(Fmax))â‰¡ a(1)
The CA score, a, is element-wise multiplied with the image represen-
tationX, expressed as aâŠ™X . Here, attention values determine
the retention or suppression of features; values near 1 retain
features, while those close to 0 suppress them. Consequently, these
attention scores are crucial in constructing feature importance met-
rics and determining the relevance of different features.
3.2 Enhancing Feature Separability: QR-Ortho
During the development of DepCA, we observed an overlapping
distribution of CA (channel attention) scores between each fea-
ture (see Fig. 3). This overlap complicates the derivation of precise
feature importance rankings, which are essential for computing
both global and class-specific metrics. To address this issue, we en-
force an orthogonal regularization on the feature average of the CA
scores to obtain distinguished CA distribution, leading to enhanced
and distinct feature rankings of MTS data. Fig. 3 illustrates the
clear separation of previously overlapping CA distributions when
orthogonality is enforced. This enhanced separation contributes to
 
1375CAFO: Feature-Centric Explanation on Time Series Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 4: RemOve And Retrain (ROAR) with Gilon [ 22]. The feature ranks of the Gilon task were first identified by our CAFO using the whole
14 feature set, with potential rank variations across figures. To assess the importance of each feature, we systematically removed them from
the train and test datasets, ensuring consistency in distribution. This process involved the progressive subtraction of more important (red as
â€˜Truthâ€™) and less important (blue as â€˜Inverseâ€™) features. After each removal, the model was retrained, and its accuracy was evaluated. The X-axis
represents the number of features removed (with zero indicating no removal), while the Y-axis shows the modelâ€™s accuracy. A notable decline
in accuracy is observed with the removal of key features, in contrast to a minimal impact when less important features are omitted. The area
between the curve (ABC) metric quantifies the gap between the two curves, where a higher ABC indicates superior feature-wise ranking. The
first row exhibits the modelâ€™s performance using cross-entropy (CE) alone, while the second row shows integration of QR-Ortho (our approach)
with CE. A marked improvement in ABC scores across all models is evident, underscoring QR-Orthoâ€™s efficacy in identifying pivotal features.
three key outcomes: (1) distinct CA distributions for each feature,
(2) improved ranking measures for global and class-wise feature
importance, and as a result (3) overall better explainability of the
MTS data. In our evaluation, we observe a substantial improvement
in the explainability of MTS data with feature-wise orthogonality,
empirically verifying its critical role in TSC model explanations.
To this end, we propose QR-Ortho Loss that enforces feature-
wise orthogonality along the feature dimension of the CA scores
through QR decomposition [ 11] that factorizes a given matrix Â¯Ainto
an orthonormal matrix Qand a residual upper triangular matrix R,
i.e.,Â¯A=QR . Here, the class prototype matrix Â¯Ais constructed by
stacking Â¯ağ‘in a row-wise manner, where row-wise means arranging
the class-prototype vectors into a matrix format where each class
prototype occupies a single row. Given ğ¶classes in a TSC task,
MTS data can be represented as K={(X(1),ğ‘¦(1)),...,(X(ğ‘),ğ‘¦(ğ‘))},
whereğ‘¦(ğ‘–)âˆˆ{1,...,ğ¶}is the corresponding class label. Then, the
class prototype of class ğ‘denoted as Â¯ağ‘, is defined as the average
CA scores of samples belonging to class ğ‘, given by:
Â¯ağ‘=1
|Kğ‘|âˆ‘ï¸
ğ‘–âˆˆKğ‘a(ğ‘–)(2)
where Kğ‘={(X,ğ‘¦)|ğ‘¦âˆˆğ‘}denotes the MTS instances in class ğ‘.
As such, we denote Â¯A:,ğ‘—(ğ‘—=1,...,ğ·)as the column (feature) vector
ofÂ¯Aused to perform QR decomposition as Â¯A=QR, given by:
ï£®ï£¯ï£¯ï£¯ï£¯ï£°| | |
q1q2Â·Â·Â· qğ·
| | |ï£¹ï£ºï£ºï£ºï£ºï£»|                     {z                     }
Qï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âŸ¨q1,Â¯A:,1âŸ© âŸ¨q 1,Â¯A:,2âŸ© Â·Â·Â· âŸ¨q 1,Â¯A:,ğ·âŸ©
0âŸ¨q2,Â¯A:,2âŸ© Â·Â·Â· âŸ¨q 2,Â¯A:,ğ·âŸ©
............
0 0 Â·Â·Â· âŸ¨qğ·,Â¯A:,ğ·âŸ©ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»|                                                  {z                                                  }
R
(3)
TheQmatrix embodies the orthogonal basis of the feature di-
mension of Â¯A, and the upper diagonal elements of the Rmatrix,
i.e.,Rğ‘–ğ‘—(ğ‘–â‰¤ğ‘—)=âŸ¨qğ‘–,Â¯A:,ğ‘—âŸ©, signify the dot products between class
feature representation Â¯A:,ğ‘—and the orthonormal basis qğ‘–. The decom-
position process ensures direct orthogonality, as the orthonormal
columns of Qinherently exhibit orthogonal properties. Also, by
leveraging the widely-used Gram-Schmidt [ 4] or Householder [ 33]algorithms, it maintains numerical stability while guaranteeing a
unique set of orthogonal vectors.
Thus, by penalizing the upper off-diagonals of R, i.e., Rğ‘–ğ‘—(ğ‘–<ğ‘—),
feature-wise orthogonality of the channel attentions can be effec-
tively regularized. From this, we define QR-Ortho loss as:
LQR=âˆ‘ï¸
ğ‘–<ğ‘—|Rğ‘–ğ‘—| (4)
which is to be minimized in addition to the cross-entropy (CE)
lossLCEwith the hyperparameter ğœ†that controls the strength of
orthogonality asL=LCE+ğœ†LQR. The loss jointly optimizes the
DepCA module and the downstream model end-to-end. In practice,
the class prototype matrix Â¯Ais formed in a mini-batch, with QR-
Ortho Loss details in Appendix I.
4 FEATURE EXPLANATION MEASURES
We present two feature importance measures: (1) Global Impor-
tance (GI) and (2) Class-Wise Relative Importance (CWRI),
which provide reliable feature-wise explanations of MTS data (Fig. 5).
While we elucidate GI and CWRI in terms of the attention scores,
the computation of both measures does not favor or rely on at-
tention scores. Rather, both measures can be effectively applied
in conjunction with any attribution method capable of produc-
ing instance-wise attributions, as in prior studies [ 7,40]. This is
achieved by averaging the scores across the time dimension, align-
ing with Eq. (5). This comprehensiveness ensures that our measures
are broadly applicable across various attribution methodologies.
Global Importance (GI). The GI score quantifies the significance
of each feature in relation to classification performance over the
entire data and thus simplifies the interpretation and comparison
between features. A feature with a high GI score is globally essential
for accurate classifications, while a low GI score suggests negligible
influence on the overall model performance. We denote the GI score
of theğ‘—-th feature xğ‘—asGI(xğ‘—), and it is calculated by averaging the
ğ‘—-th channel attention (CA) scores ğ‘ğ‘—âˆˆ[0,1]of all data samples
over all classes, as shown in Eq. (5).
GI(xğ‘—)=1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ‘(ğ‘–)
ğ‘—(5)
 
1376KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jaeho Kim, Seok-Ju Hahn, Yoontae Hwang, Junghye Lee, and Seulki Leeâ€ 
Figure 5: (A) The GI (Global Importance) score for the MS Dataset [ 28]
is provided. x0tox5denotes the feature index. (B) An example
of WhichFingerâ€™s CWRI (Class-Wise Relative Importance) score:
columns represent sensors (features), rows denote classes, and cell
values convey CWRI scores. Red indicates the higher relative impor-
tance of the feature for the class, whereas blue denotes features of
lesser importance in the context of the specific class.
GI Evaluation. The evaluation on the GI score focuses on two
aspects: (1) the removal of high GI ranked feature should have a
higher drop in model performance compared to low GI ranked
feature, and (2) the order of GI ranks should be consistent within
models. For model performance, we employ the renowned RemOve
And Retrain (ROAR) method [ 16], which sequentially eliminates
the most important (truth) and least important (inverse) features
before retraining the model to maintain consistent train and test
distributions (See Fig. 4). Based on ROAR, we report the Drop-in
Accuracy (DA), which is the drop in model performance after ex-
cluding 20% of the important features. Second, to complement the
manual selection of K% of features to be removed, we introduce
theWeighted Drop in Accuracy (WDA) metric, which measures the
decrease in accuracy when important features are removed sequen-
tially, giving greater weight to scenarios where a smaller fraction
of features is dropped. In real-world scenarios, practitioners and
developers are often more interested in discerning the most or least
important features rather than knowing mediocre features. Such
scenarios arise when we have to extensively reduce the number of
features to fit in a small memory budget or remove features that
do not contribute to model performance. This weighting scheme
ensures that the impact of removing high ranked GI feature is more
pronounced (see Appendix C.1). Third, as an enhancement to the
ROAR, we introduce the Area Between Curves (ABC) metric (Eq. (6))
based on the trapezoid rule [ 49], quantifying the area enclosed
by the inverse ( ğ‘“(ğ‘¥)) and truth curve ( ğ‘”(ğ‘¥)) between the interval
[ğ‘,ğ‘]; a larger ABC value denotes a more precise feature ranking
assessment, highlighted as gray areas in Fig. 4.
ABC â‰œâˆ«ğ‘
ğ‘(ğ‘“(ğ‘¥)âˆ’ğ‘”(ğ‘¥))ğ‘‘ğ‘¥
â‰ƒ1
2ğ‘›âˆ’1âˆ‘ï¸
ğ‘–=1(ğ‘“(ğ‘¥ğ‘–)âˆ’ğ‘”(ğ‘¥ğ‘–)+ğ‘“(ğ‘¥ğ‘–+1)âˆ’ğ‘”(ğ‘¥ğ‘–+1))Î”ğ‘¥ğ‘–(6)
To assess the consistency in GI rankings produced for differ-
ent model runs, we utilize the Spearman correlation ( ğœŒğ‘†) [29] and
Kendall correlation ( ğœŒğ¾) [1] based on a 5-fold cross-validation (CV).Class-Wise Relative Importance (CWRI). While GI offers a
global view of feature importance, CWRI provides detailed, class-
specific insights into the role of each feature for every class ğ‘âˆˆ
{1,...,ğ¶}. This approach is particularly valuable because a feature
with a high GI score may not necessarily be of high importance
for each individual class. CWRI, therefore, provides a class-centric
perspective of feature importance in MTS data. We define the CWRI
score for class ğ‘, denoted as CWRI(ğ‘)âˆˆRğ·, as outlined in Eq. (7).
For instance, the rows of Fig. 5-B shows the CWRI score for each
class in the WhichFinger dataset. This class-specific score is derived
by evaluating the deviation in the class prototype of class ğ‘in
comparison to other classes ğ‘˜â‰ ğ‘.
CWRI(ğ‘)â‰œÂ¯ağ‘âˆ’1
ğ¶âˆ’1âˆ‘ï¸
ğ‘˜â‰ ğ‘Â¯ağ‘˜,Â¯ağ‘: see Eq. (2)(7)
A key aspect of CWRI is its relative computation across differ-
ent classes, ensuring that the sum of CWRI scores for any given
feature is zero. This method naturally produces both positive and
negative CWRI values for the same feature across various classes.
To illustrate, in a situation with three classes, the CWRI for feature
xğ‘—might be +1.7 for Class 1, -1.4 for Class 2, and -0.3 for Class 3.
Here, a positive CWRI of feature xğ‘—for Class 1 indicates higher
relative importance of this feature in classifying Class 1 compared
to Classes 2 and 3. The negative scores in Classes 2 and 3 indicate
that these features were relatively unimportant. This approach in
relatively calculating the difference is advantageous over simple
class average scoring, which can be ambiguous and less informative,
particularly when classes exhibit similar scores.
CWRI Evaluation. We utilize the CWRI scores to categorize each
class and feature into relatively important (positive scores, xğ‘—â‰¥0;
red cells in Fig. 5-B) and relatively unimportant sets (negative scores,
xğ‘—<0; blue cells). Our evaluation compares these categorized fea-
ture sets against the established ground truth to determine the
accuracy of feature importance identification. Given the lack of
public datasets with known class discriminative feature importance,
we created both real-world and synthetic datasets specifically for
this purpose (detailed in Sec. 5). The comparison between our iden-
tified important/unimportant sets and the ground truths employs
binary classification metrics like the F1 score, Jaccard index, and
accuracy (we use the term interpretative accuracy (IACC) for clarity
over standard model accuracy). The methodology for establishing
these ground truths is elaborated in Appendix E.
5 DATASET AND BASELINE
GI Datasets. For GI measure, we curated two large public datasets,
chosen for their substantial size and relevance (Appendix F). The
Gilon Activity (7-class) comprises 14 features collected from smart
insoles utilized by 72 users [ 22]. The Microsoft (MS) Activity (10-
class) contains 6 features from armbands worn by 92 users [ 28].
The details of all the datasets can be found in Appendix F.
CWRI Datasets. Evaluating the CWRI measure on existing public
datasets is challenging due to: (1) lacking in-depth comprehension
of the features that influence the performance for each class [ 41],
and (2) unmet requirements for ample classes ( ğ¶â‰¥3), features
(ğ·â‰¥3), and samples ( ğ‘â‰¥10,000) for generalization. Thus, we
introduce synthetic and real-world MTS datasets, as follows.
 
1377CAFO: Feature-Centric Explanation on Time Series Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
GILON MSModels MethodsABC(â†‘) DA(â†‘) WDA(â†‘)ğœŒS(â†‘)ğœŒK(â†‘) ACC(â†‘) ABC(â†‘) DA(â†‘) WDA(â†‘)ğœŒS(â†‘)ğœŒK(â†‘) ACC(â†‘)
GS 0.152 -0.451 0.195 0.105 0.081 -0.159 7.014 0.166 0.051 0.066
SVS 0.819 0.092 0.331 0.100 0.068 0.225 11.062 0.329 -0.068 -0.066
Saliency 1.192 4.015 0.546 0.371 0.257 0.039 -4.861 0.142 0.102 0.146
FA 0.714 0.313 0.331 0.148 0.107 0.491 4.934 0.287 -0.091 -0.040
IG 0.210 -0.118 0.193 0.045 0.033 -0.015 -2.084 0.241 0.074 0.066
CE 0.684 3.325 0.478 0.207 0.1420.958
0.355 7.781 0.475 0.062 0.0130.846ShuffleNet
CE+QR(Ours) 1.227 8.157 0.760 0.352 0.270 0.945 0.337 21.03 0.450 0.640 0.546 0.810
GS 1.284 3.935 0.671 0.453 0.318 0.059 8.438 0.356 0.051 0.013
SVS 0.577 0.835 0.281 0.134 0.116 0.160 4.520 0.344 0.360 0.280
Saliency 1.338 2.652 0.663 0.364 0.287 0.154 8.327 0.345 0.251 0.200
FA 0.847 0.118 0.440 -0.014 -0.024 0.350 7.037 0.373 0.108 0.066
IG 1.289 2.137 0.620 0.453 0.318 -0.005 7.935 0.244 0.051 0.013
CE 0.801 1.553 0.420 0.370 0.2700.960
0.175 -0.667 0.007 -0.062 -0.0660.752ResNet
CE+QR(Ours) 1.163 6.393 0.615 0.581 0.441 0.940 0.117 1.266 0.254 0.440 0.333 0.787
GS -0.307 0.494 0.112 0.230 0.156 0.294 2.572 0.218 -0.040 0.013
SVS -0.708 3.330 0.063 0.164 0.112 0.328 3.983 0.226 -0.137 -0.120
Saliency 0.986 3.446 0.490 0.525 0.411 0.321 6.642 0.263 0.040 0.040
FA 0.457 1.041 0.269 -0.065 -0.046 0.300 3.209 0.226 -0.045 -0.013
IG -0.301 0.787 0.115 0.235 0.169 0.326 2.788 0.263 -0.040 0.013
CE 0.920 8.117 0.531 0.330 0.2390.920
0.125 0.730 0.165 -0.142 -0.0930.736MLP-Mixer
CE+QR(Ours) 1.144 8.105 0.697 0.165 0.129 0.922 0.276 6.093 0.289 0.908 0.813 0.726
GS -0.197 1.244 0.116 0.218 0.147 0.169 5.219 0.361 0.360 0.280
SVS -0.607 2.830 0.083 0.131 0.081 0.182 5.227 0.318 -0.040 -0.066
Saliency 0.587 1.216 0.376 0.120 0.098 0.122 5.219 0.329 0.051 0.040
FA 0.244 0.367 0.223 -0.054 -0.024 0.182 5.227 0.318 0.141 0.120
IG -0.176 1.244 0.119 0.200 0.138 0.169 5.219 0.361 0.177 0.173
CE 0.553 3.512 0.387 0.407 0.2920.922
-0.054 -1.109 0.151 -0.097 -0.0660.703ViT
CE+QR(Ours) 0.636 2.046 0.456 0.502 0.389 0.924 0.128 10.962 0.530 0.120 0.093 0.706
Table 1: Performance Evaluation of GI Metrics on Gilon and MS datasets. Each performance metric is explained in Sec. 4. Optimal performance
is indicated by values in bold red, while the second-highest performance is marked in bold black. See Appendix C for full comparison (including
standard deviation from five-fold cross validation) of explainers based on raw MTS data such as LSTM, and TCN.
Figure 6: (A) Class-specific signals in Circle, Triangle, and Square
masks; grey regions are filled with Gaussian noise. (B-1) The smart
glove has 10 sensors, two per finger. (B-2, 3) We depict the data ac-
quisition process for Class 1 and 2 for the WhichFinger. Specifically,
Class 1 involves folding and unfolding movements of the thumb. In
contrast, Class 2 is the complement of Class 1, focusing on the fold-
ing and unfolding of the remaining four fingers (See Appendix G).
Dataset 1: SquidGame (ğ¶=3;ğ·=10;ğ‘=54,000). We designed
the SquidGame dataset (Fig. 6-A), a 3-class synthetic MTS data,
comprising of 30 features, which are divided into sets Gcircle=
{1,...,10},Gtriangle ={11,...,20}, and Gsquare ={21,...,30}. For
Class 1, distinct time signals, such as sine waves, are produced
within the circular mask in the feature set Gcircle. Meanwhile, Gauss-
ian noise fills the remaining areas (grey region) outside the circular
mask in Gcircle. This process mirrors the approach taken by Ismail
et al. [ 19]. Similarly, Classes 2 and 3 have unique time signals within
their respective feature sets, Gtriangle andGsquare . For each MTS
instance, the location and size of the masks are randomly generated
within the three feature sets to increase complexity.
Dataset 2: WhichFinger (ğ¶=10;ğ·=10;ğ‘=18,010). We gath-
ered a real-world MTS dataset using a smart glove [ 25] from 19users, called WhichFinger (Fig. 6-B), to validate the CWRI measure.
The smart glove incorporated with two sensors for each finger
measures the resistance change in response to the tensile force
exerted by each finger. We capture ten unique finger movements,
achieved by either flexing and extending a single finger or a group
of four fingers. Owing to the interlinked nature of hand muscles,
we observe realistic correlations among features, making the task
both intricate and non-trivial, providing a valuable MTS dataset for
XAI applications. Detailed descriptions of the task design and data
collection methodologies are provided in Appendix G.
Implementation and Baselines. We compare CAFO to several
post-hoc explanation methods i.e., Gradient Shap (GS) [ 26], Shapley
Value Sampling (SVS) [ 6], Saliency [ 35], Feature Ablation (FA) [ 37],
Integrated Gradients (IG) [ 36], and DynaMask (DM) [ 7]. We uti-
lized several deep architectures including vision-based deep models:
ShuffleNet [ 51], ResNet [ 13], MLP-Mixer [ 39], and Vision Trans-
former (ViT) [ 8], and sequence based deep models like LSTM [ 15]
and TCN. We adopt FIT [ 40], an explainer designed for recurrent
models. We also employ LAXCAT [ 17], a 1-D CNN based MTS
explainer. A detailed description is in Appendix H.
6 RESULTS
6.1 Evaluation of Global Importance
We evaluated CAFOâ€™s performance in the GI measure using Gilon
and MS datasets. The results for vision-based models like ShuffleNet,
ResNet, MLP-Mixer, and ViT are in Tab. 1. Models using raw MTS
format (e.g., LSTM, TCN, LAXCAT) and post-hoc methods relying
on raw MTS (e.g., FIT, DynaMask) are detailed in Appendix C.
Generally, vision-based models excel in most scenarios.
CAFO consistently demonstrates superior performance across
key metrics (ABC, DA, WDA), highlighted in bold red in the Gilon
 
1378KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jaeho Kim, Seok-Ju Hahn, Yoontae Hwang, Junghye Lee, and Seulki Leeâ€ 
Figure 7: (a) Pairwise Spearman correlation ( ğœŒğ‘†) of GI ranks from
four models with cross-entropy (CE) loss in five-fold CV, revealing
lower correlations and inconsistent GI ranks. (b) Enhanced GI rank
consistency observed with QR-Ortho integration, demonstrated by
higherğœŒğ‘†values in both inter and intra-model comparisons.
dataset: notably, ShuffleNet (1.227), MLP-Mixer (1.144), and ViT (0.636)
in the ABC metric. The use of QR-Ortho with cross-entropy (CE)
significantly improved GI metric performance in most cases: 10
of 12 in Gilon and 9 of 12 in MS datasets. Notably, several base-
lines showed negative ABC scores, indicating a mismatch between
critical feature identification and the drop in model accuracy, as
seen in ROARâ€™s inverse and truth lines (Fig. 4). This suggests some
baseline explainers inadequately measure GI rankings. Our find-
ings show minimal difference in model accuracy between with and
without QR-Ortho integration. The regularization parameter ğœ†in
L=LCE+ğœ†LQRwas selected based on dataset characteristics, not
through exhaustive tuning. We believe a more tailored selection of
ğœ†for each model may offer additional performance enhancements.
6.2 Consistency in GI Ranks
6.2.1 Within Models. Establishing consistency in model explana-
tions is imperative for fostering user trust, as highlighted by Riberio
et al. [ 31]. Conversely, models that yield divergent feature rank-
ings across multiple runs undermine confidence in userâ€™s decision-
making processes. Although prior research [ 41] has underscored
the variability in time-step importance produced with post-hoc
explanation methods, our study is the initial effort to identify and
quantitatively evaluate such variability in the context of feature-
based importance. Our evaluation involves executing each model
through 5 iterations of CV, with each fold serving once as a valida-
tion set and the remaining as training. This process yields 5 distinct
feature rankings for the same left-out test set, from which we com-
pute the pairwise Spearmanâ€™s ğœŒğ‘ and Kendallâ€™s ğœŒğ‘˜coefficients to
gauge rank consistency, with the findings presented in Tab. 1. No-
tably, CAFO demonstrates the highest consistency in 11 out of 16
instances. Across the board, our results reveal there exists a huge
variability in feature rankings, even under constant model architec-
tures and explanatory methods. Alarmingly, certain explanatory
methods yield negative correlations, indicating inverted ranking
orders across different runs. These findings raise the need for more
robust explanatory frameworks that can deliver dependable and
stable feature rankings.
Figure 8: The GI metric at each training epoch is visualized, with
bold red lines representing the irrelevant signal, while the thin grey
lines correspond to the actual variables in the Gilon task. Over the
course of training, the pseudo signals (i.e., bold red lines) consistently
converge to the lowest GI values.
6.2.2 Between Models. Our analysis extends to assessing feature
rank consistency across different models. The Spearman correlation
coefficients, visualized as a heatmap in Fig. 7, reveal that the use of
QR-Ortho significantly improves feature ranking consistency across
models compared to CE alone, demonstrating CAFOâ€™s robustness
in providing consistent feature rankings, independent of model
architecture. While different models naturally prioritize varying
features for optimal performance, a degree of ranking consistency
is a robustness indicator, fostering model trust. Additionally, ana-
lyzing ranking discrepancies offers deep insights. For example, as
indicated by the yellow arrow in Fig. 7, we observe an anomaly
where a single run from the ViT model presents an entirely reversed
feature ranking relative to all other runs. Such an outlier warrants
further investigation by developers to ascertain the presence of po-
tential errors or anomalies in the model training or data processing.
These insights can prove invaluable in enhancing model reliability.
6.3 Robustness to Irrelevant Signals
In real world MTS problems, the overabundance of data often results
in the accumulation of measurements from superfluous sensors [ 45].
Eliminating such irrelevant feature is, therefore, a critical task for
practitioners. To assess the efficacy of CAFO in filtering out in-
significant variables during model training, we generate pseudo
signals [ 27] from time series processes: White noise, Sinusoidal,
and Gaussian Process (detailed in Appendix J). The GI measure
from each training epoch is visualized in Fig. 8. Initially, the pseudo-
variableâ€™s GI value is near 0.5, but it converges to the lowest GI
ranking as training advances. This demonstrates CAFOâ€™s robust-
ness against non-significant variables and its potential to identify
and discard non-significant features.
6.4 Class-Wise Relative Importance
Assessing feature relevance for specific classes is critical in applica-
tions like predictive maintenance in Heating, Ventilating, and Air
Conditioning (HVAC) systems, where sensor importance varies by
fault (class) type [ 38,50] (Appendix A). Our CWRI methodology
offers valuable information for sensor prioritization for each class.
In this study, we evaluate the ability of CAFO to identify crit-
ical class-wise features using the CWRI metric on two datasets:
SquidGame and WhichFinger. As discussed in Sec. 5, these datasets
come with ground truth labels indicating the relevance of features
 
1379CAFO: Feature-Centric Explanation on Time Series Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
SquidGame WhichFingerModels MethodsF1(â†‘) Jaccard(â†‘) IACC(â†‘) F1(â†‘) Jaccard(â†‘) IACC(â†‘)
GS 0.689 0.531 0.649 0.590 0.424 0.596
SVS 0.811 0.699 0.853 0.635 0.466 0.636
Saliency 0.533 0.371 0.489 0.601 0.436 0.644
FA 0.765 0.624 0.818 0.552 0.383 0.588
IG 0.690 0.531 0.649 0.582 0.414 0.588
CE 0.561 0.394 0.518 0.429 0.273 0.530ShuffleNet
CE+QR(Ours) 0.983 0.967 0.978 0.679 0.514 0.606
GS 0.747 0.598 0.693 0.630 0.462 0.636
SVS 0.758 0.621 0.800 0.626 0.456 0.634
Saliency 0.489 0.329 0.467 0.588 0.419 0.604
FA 0.739 0.603 0.789 0.670 0.503 0.696
IG 0.746 0.597 0.693 0.632 0.463 0.638
CE 0.524 0.362 0.580 0.357 0.232 0.530ResNet
CE+QR(Ours) 0.987 0.974 0.982 0.724 0.570 0.698
GS 0.929 0.877 0.918 0.859 0.753 0.862
SVS 0.994 0.988 0.996 0.778 0.641 0.784
Saliency 0.514 0.349 0.453 0.726 0.571 0.744
FA 0.987 0.975 0.991 0.732 0.586 0.752
IG 0.929 0.877 0.918 0.861 0.756 0.864
CE 0.596 0.427 0.736 0.496 0.333 0.600MLP-Mixer
CE+QR(Ours) 0.949 0.904 0.933 0.709 0.551 0.660
GS 0.861 0.763 0.842 0.685 0.527 0.690
SVS 0.883 0.803 0.902 0.811 0.686 0.812
Saliency 0.526 0.359 0.484 0.722 0.570 0.750
FA 0.779 0.640 0.809 0.674 0.512 0.684
IG 0.861 0.763 0.842 0.702 0.544 0.710
CE 0.700 0.546 0.796 0.520 0.352 0.536ViT
CE+QR(Ours) 0.925 0.863 0.898 0.531 0.363 0.572
Table 2: Performance Evaluation of CWRI Metrics. Here, the fea-
tures identified as important by the model against the established
ground truth importance is evaluated using binary metrics, including
F1 score, Jaccard, and Accuracy (distinguished from model accuracy.)
on a class-wise basis. As detailed in Tab. 2, CAFO surpasses other ex-
planatory models in accurately identifying class-specific features in
13 out of 24 scenarios. We also note that integrating QR-Ortho Loss
consistently enhances the discernment of class-wise relevant fea-
tures across the table, compared to the standalone use of CE (cross-
entropy) loss. Moreover, we observe a general performance degra-
dation of all explainers in the WhichFinger dataset compared to
SquidGame dataset, which may be attributed to the increased com-
plexity inherent in real-world data. Such observation underscores
the need for the development of real-world data oriented for XAI,
especially in the time series domain.
6.5 Additional Experiments
Due to the space constraint, additional experimental results are
presented in the appendix, with key highlights summarized below.
6.5.1 Other Image Encoding Methods. We provide several main
experiment results with the Gramian Angular Field image encoding
method in Appendix B.
6.5.2 Effect of ğœ†.We evaluated the effect of ğœ†-a key hyperparam-
eter in our model which modulates the QR-Ortho Loss (Eq. (4))-
(ranging from 0 to 1) on two tasks: SquidGame and WhichFinger.
Results indicate that increasing ğœ†improves CWRI-related metrics,
but excessively high values can reduce model accuracy. Detailed
findings are in Appendix K.
6.5.3 Alignment with Domain Knowledge. Using Gilon and MS
datasets, we demonstrated CAFOâ€™s alignment with established do-
main insights. For the Gilon task, accelerometer features were cru-
cial for speed differentiation, and in the MS task, similar activities
yielded similar attention scores. Visual evidence of these correla-
tions is provided in Appendix L.
6.6 Limitations and Discussions
We discuss the following limitations of CAFO. As our evaluation
strategy for the GI method inherits the ROAR method [ 16], theretraining and re-evaluation cost is computationally intensive. Con-
sequently, there is a need for alternative explanation methodolo-
gies that either do not rely on model accuracy or employ more
computationally efficient evaluation techniques for the GI method.
Additionally, our CAFO leverages image encoding to represent a
time series into an image-like representation. While this approach
has its merits, it also restricts the type of models used. As such,
our research agenda includes the development of evaluation meth-
ods that are not only less demanding in terms of computational
resources but also architecture-agnostic.
7 CONCLUSION
In this paper, we introduce CAFO, a feature-centric explanation
framework for MTS classification. An in-depth discussion regarding
the feature-centric explanation for MTS has been missing in much
of the previous literature despite its huge importance, due to the
lack of evaluation protocols, pertinent benchmarks, and method-
ologies. Addressing these problems, our contribution is threefold:
First, we present CAFO, a channel attention-based feature explainer
which combines a novel depth-wise channel attention module, De-
pCA, with QR-Ortho regularization for feature explanation in time
series. Second, we curate a collection of both real-world and syn-
thetic datasets, each annotated with known discriminative feature
importance. Third, we introduce a set of feature importance met-
rics designed to quantify both global and class-specific importance,
complete with corresponding evaluation schemes. We believe that
our work will serve as a new groundwork for understanding feature
importance within MTS classification.
ACKNOWLEDGMENTS
This work was supported by the National Research Foundation of
Korea(NRF) grant funded by the Korea government(MSIT) (RS-2023-
00277383), and Institute of Information & communications Technol-
ogy Planning & Evaluation(IITP) grant funded by the Korea govern-
ment(MSIT) (No.RS-2020-II201336, Artificial Intelligence graduate
school support(UNIST)). The authors extend their gratitude to the
Gilon Corporation for inspiring this work. Special thanks are due
to Prof. Sunghoon Lim, Gyeongho Kim, Sujin Jeon, and Jae Gyeong
Choi for providing the smart glove essential for the WhichFinger
data collection and for their valuable insights into our research. We
are also grateful to MyoungHoon Lee, Prof. Suhyeon Kim, Wonho
Sohn, and Hyewon Kang for their initial discussions that shaped our
paper. Appreciation is further extended to Yeonjoo Kim, Solang Kim,
Bosung Kim, Isu Jeong, Jaewook Lee, and Changhyeon Lee for their
thorough review of our manuscript. Lastly, we thank the numerous
anonymous reviewers whose constructive feedback significantly
enhanced our work.
REFERENCES
[1]HervÃ© Abdi. 2007. The Kendall rank correlation coefficient. Encyclopedia of
Measurement and Statistics. Sage, Thousand Oaks, CA (2007), 508â€“510.
[2]JoÃ£o Bento, Pedro Saleiro, AndrÃ© F Cruz, MÃ¡rio AT Figueiredo, and Pedro Bizarro.
2021. Timeshap: Explaining recurrent models through sequence perturbations.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining. 2565â€“2573.
[3]Jonas Beuchert, Friedrich Solowjow, Sebastian Trimpe, and Thomas Seel. 2020.
Overcoming bandwidth limitations in wireless sensor networks by exploitation
of cyclic signal patterns: An event-triggered learning approach. Sensors 20, 1
(2020), 260.
 
1380KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jaeho Kim, Seok-Ju Hahn, Yoontae Hwang, Junghye Lee, and Seulki Leeâ€ 
[4]Ã…ke BjÃ¶rck. 1994. Numerics of gram-schmidt orthogonalization. Linear Algebra
and Its Applications 197 (1994), 297â€“316.
[5]Tadeusz CaliÅ„ski and Jerzy Harabasz. 1974. A dendrite method for cluster analysis.
Communications in Statistics-theory and Methods 3, 1 (1974), 1â€“27.
[6]Javier Castro, Daniel GÃ³mez, and Juan Tejada. 2009. Polynomial calculation of
the Shapley value based on sampling. Computers & Operations Research 36, 5
(2009), 1726â€“1730.
[7]Jonathan CrabbÃ© and Mihaela Van Der Schaar. 2021. Explaining time series
predictions with dynamic masks. In International Conference on Machine Learning.
PMLR, 2166â€“2177.
[8]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[9]Jean-Pierre Eckmann, S Oliffson Kamphorst, David Ruelle, et al .1995. Recurrence
plots of dynamical systems. World Scientific Series on Nonlinear Science Series A
16 (1995), 441â€“446.
[10] Pavel Filonov, Andrey Lavrentyev, and Artem Vorontsov. 2016. Multivariate
industrial time series with cyber-attack simulation: Fault detection using an
lstm-based predictive data model. arXiv preprint arXiv:1612.06676 (2016).
[11] Colin R Goodall. 1993. 13 Computation using the QR decomposition. (1993).
[12] Jianing He, Xiaolong Gong, and Linpeng Huang. 2021. Wavelet-temporal neu-
ral network for multivariate time series prediction. In 2021 International Joint
Conference on Neural Networks (IJCNN). IEEE, 1â€“8.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[14] Sepp Hochreiter. 1998. The vanishing gradient problem during learning recurrent
neural nets and problem solutions. International Journal of Uncertainty, Fuzziness
and Knowledge-Based Systems 6, 02 (1998), 107â€“116.
[15] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735â€“1780.
[16] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A
benchmark for interpretability methods in deep neural networks. Advances in
neural information processing systems 32 (2019).
[17] Tsung-Yu Hsieh, Suhang Wang, Yiwei Sun, and Vasant Honavar. 2021. Explainable
multivariate time series classification: a deep neural network which learns to
attend to important variables as well as time intervals. In Proceedings of the 14th
ACM international conference on web search and data mining. 607â€“615.
[18] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition. 7132â€“7141.
[19] Aya Abdelsalam Ismail, Mohamed Gunady, Hector Corrada Bravo, and Soheil
Feizi. 2020. Benchmarking deep learning interpretability in time series predictions.
Advances in neural information processing systems 33 (2020), 6441â€“6452.
[20] Aya Abdelsalam Ismail, Mohamed Gunady, Luiz Pessoa, Hector Corrada Bravo,
and Soheil Feizi. 2019. Input-cell attention reduces vanishing saliency of recurrent
neural networks. Advances in Neural Information Processing Systems 32 (2019).
[21] Maowei Jiang, Pengyu Zeng, Kai Wang, Huan Liu, Wenbo Chen, and Haoran Liu.
2022. FECAM: Frequency Enhanced Channel Attention Mechanism for Time
Series Forecasting. arXiv preprint arXiv:2212.01209 (2022).
[22] Jaeho Kim, Hyewon Kang, Jaewan Yang, Haneul Jung, Seulki Lee, and Junghye
Lee. 2023. Multi-task Deep Learning for Human Activity, Speed, and Body
Weight Estimation using Commercial Smart Insoles. IEEE Internet of Things
Journal (2023).
[23] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager.
2017. Temporal convolutional networks for action segmentation and detection.
Inproceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
156â€“165.
[24] Jiyoon Lee, Hyungrok Do, Mingu Kwak, Hyungu Kahng, and Seoung Bum Kim.
2021. Hierarchical segment-channel attention network for explainable multi-
channel signal classification. Information Sciences 567 (2021), 312â€“331.
[25] Minhyuk Lee and Joonbum Bae. 2020. Deep learning based real-time recognition
of dynamic finger gestures using a data glove. IEEE Access 8 (2020), 219923â€“
219933.
[26] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in neural information processing systems 30 (2017).
[27] JR Maat, A Malali, and P Protopapas. 2017. Timesynth: A multipurpose library
for synthetic time series in python.
[28] Dan Morris, T Scott Saponas, Andrew Guillory, and Ilya Kelner. 2014. RecoFit:
using a wearable sensor to find, recognize, and count repetitive exercises. In
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
3225â€“3234.
[29] Leann Myers and Maria J Sirois. 2004. Spearman correlation coefficients, differ-
ences between. Encyclopedia of statistical sciences 12 (2004).
[30] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So Kweon. 2018. Bam:
Bottleneck attention module. arXiv preprint arXiv:1807.06514 (2018).
[31] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. " Why should i
trust you?" Explaining the predictions of any classifier. In Proceedings of the 22ndACM SIGKDD international conference on knowledge discovery and data mining.
1135â€“1144.
[32] Udo Schlegel, Hiba Arnout, Mennatallah El-Assady, Daniela Oelke, and Daniel A
Keim. 2019. Towards a rigorous evaluation of XAI methods on time series. In
2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW).
IEEE, 4197â€“4201.
[33] Robert Schreiber and Charles Van Loan. 1989. A storage-efficient WY represen-
tation for products of Householder transformations. SIAM J. Sci. Statist. Comput.
10, 1 (1989), 53â€“57.
[34] Shoaib Ahmed Siddiqui, Dominique Mercier, Mohsin Munir, Andreas Dengel,
and Sheraz Ahmed. 2019. Tsviz: Demystification of deep learning models for
time-series analysis. IEEE Access 7 (2019), 67027â€“67040.
[35] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside
convolutional networks: Visualising image classification models and saliency
maps. arXiv preprint arXiv:1312.6034 (2013).
[36] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. In International conference on machine learning. PMLR, 3319â€“
3328.
[37] Harini Suresh, Nathan Hunt, Alistair Johnson, Leo Anthony Celi, Peter Szolovits,
and Marzyeh Ghassemi. 2017. Clinical intervention prediction and understanding
using deep networks. arXiv preprint arXiv:1705.08498 (2017).
[38] Saman Taheri, Amirhossein Ahmadi, Behnam Mohammadi-Ivatloo, and Somayeh
Asadi. 2021. Fault detection diagnostic for HVAC systems via deep learning
algorithms. Energy and Buildings 250 (2021), 111275.
[39] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob
Uszkoreit, et al .2021. Mlp-mixer: An all-mlp architecture for vision. Advances in
neural information processing systems 34 (2021), 24261â€“24272.
[40] Sana Tonekaboni, Shalmali Joshi, Kieran Campbell, David K Duvenaud, and
Anna Goldenberg. 2020. What went wrong and when? Instance-wise feature
importance for time-series black-box models. Advances in Neural Information
Processing Systems 33 (2020), 799â€“809.
[41] Hugues TurbÃ©, Mina Bjelogrlic, Christian Lovis, and Gianmarco Mengaldo. 2023.
Evaluation of post-hoc interpretability methods in time-series classification.
Nature Machine Intelligence 5, 3 (2023), 250â€“260.
[42] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[44] Zhiguang Wang, Tim Oates, et al .2015. Encoding time series as images for
visual inspection and classification using tiled convolutional neural networks. In
Workshops at the twenty-ninth AAAI conference on artificial intelligence, Vol. 1.
AAAI Menlo Park, CA, USA.
[45] Samuel R West, Ying Guo, X Rosalind Wang, and Joshua Wall. 2011. Automated
fault detection and diagnosis of HVAC subsystems using statistical machine
learning. (2011).
[46] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. Cbam:
Convolutional block attention module. In Proceedings of the European conference
on computer vision (ECCV). 3â€“19.
[47] Feiyu Xu, Hans Uszkoreit, Yangzhou Du, Wei Fan, Dongyan Zhao, and Jun Zhu.
2019. Explainable AI: A brief survey on history, research areas, approaches
and challenges. In Natural Language Processing and Chinese Computing: 8th CCF
International Conference, NLPCC 2019, Dunhuang, China, October 9â€“14, 2019,
Proceedings, Part II 8. Springer, 563â€“574.
[48] Lingxiao Yang, Ru-Yuan Zhang, Lida Li, and Xiaohua Xie. 2021. Simam: A
simple, parameter-free attention module for convolutional neural networks. In
International conference on machine learning. PMLR, 11863â€“11874.
[49] Shi-Tao Yeh et al .2002. Using trapezoidal rule for the area under a curve calcu-
lation. Proceedings of the 27th Annual SASÂ® User Group International (SUGIâ€™02)
(2002), 1â€“5.
[50] Liang Zhang and Jin Wen. 2019. A systematic feature selection procedure for
short-term data-driven building energy forecasting model development. Energy
and Buildings 183 (2019), 428â€“442.
[51] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. 2018. Shufflenet: An ex-
tremely efficient convolutional neural network for mobile devices. In Proceedings
of the IEEE conference on computer vision and pattern recognition. 6848â€“6856.
[52] Zhibin Zhao, Tianfu Li, Jingyao Wu, Chuang Sun, Shibin Wang, Ruqiang Yan,
and Xuefeng Chen. 2020. Deep learning algorithms for rotating machinery
intelligent diagnosis: An open source benchmark study. ISA transactions 107
(2020), 224â€“255.
 
1381CAFO: Feature-Centric Explanation on Time Series Classification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A MOTIVATION REGARDING
FEATURE-BASED IMPORTANCE
For our appendix materials, please refer to the extended manuscript,
which is available in https://arxiv.org/abs/2406.01833v1.B IMAGE ENCODING METHODS
C GI FULL RESULTS
C.1 GI Metric Calculation
D CWRI FULL RESULTS
E EVALUATION OF CWRI METRICS
F DATASETS
G WHICHFINGER DATASET
H IMPLEMENTATION DETAILS
H.1 Hardware and Software
I QR FEATURE ORTHOGONALITY
REGULARIZATION ALGORITHM
J PSEUDO VARIABLES
K EFFECT OF ğœ†
LALIGNMENT WITH DOMAIN KNOWLEDGE
 
1382