Ranking with Slot Constraints
Wentao Guoâˆ—
Cornell University
Department of Computer Science
Ithaca, New York, USA
wg247@cornell.eduAndrew Wangâˆ—
Cornell University
Department of Computer Science
Ithaca, New York, USA
azw7@cornell.edu
Bradon Thymes
Cornell University
Department of Computer Science
Ithaca, New York, USA
bmt63@cornell.eduThorsten Joachims
Cornell University
Department of Computer Science
Ithaca, New York, USA
tj@cs.cornell.edu
ABSTRACT
Rankings are increasingly used as part of human decision-making
processes to most effectively allocate reviewing resources. Many
of these processes have complex constraints, and we identify slot
constraints as a model for a wide range of application problems â€“
from college admission with limited slots for different majors, to
composing a stratified cohort of eligible participants in a medical
trial. In this paper, we formalize the slot-constrained ranking prob-
lem as producing a ranking that maximizes the number of filled
slots if candidates are evaluated by a human decision maker for
slot eligibility in the order of the ranking. We show that naive adap-
tations of the Probability Ranking Principle (PRP) can be highly
sub-optimal for slot-constrained ranking problems, and we devise a
new ranking algorithm, called MatchRank. MatchRank generalizes
the PRP, and it subsumes the PRP as a special case when there are
no slot constraints. Our theoretical analysis shows that MatchRank
has a strong approximation guarantee without any independence
assumptions between slots or candidates. Furthermore, we show
how MatchRank can be implemented efficiently. Beyond the theo-
retical guarantees, empirical evaluations show that MatchRank can
provide substantial improvements over a range of synthetic and
real-world tasks.
CCS CONCEPTS
â€¢Information systems â†’Top-k retrieval in databases.
KEYWORDS
ranking; slot constraints; maximum bipartite matching
ACM Reference Format:
Wentao Guoâˆ—, Andrew Wangâˆ—, Bradon Thymes, and Thorsten Joachims.
2024. Ranking with Slot Constraints. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $15.00
https://doi.org/10.1145/3637528.367200025â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3672000
1 INTRODUCTION
Rankings have become a ubiquitous interface whenever there is
a need to focus attention among an otherwise impractically or
intractably large number of options. Beyond their conception as
an interface for query-based retrieval [e.g., 31], rankings are now
widely used in related tasks like recommendation and advertising.
In addition, a substantial number of new ranking applications come
with new types of constraints, and we identify the notion of slot
constraints as a frequent requirement. With slot constraints we refer
to capacity constraints for different types of relevant candidates.
For example, there is only a certain number of slots for each major
in a college admissions task; or the cohort of a medical trial may
need to fulfill constraints on gender and race to be representative
[7]; and in a multi-stage retrieval pipeline we may have assortment
constraints [ 41]. In these applications, the goal is to fill all slots
with relevant candidates, and each candidate can have a different
probability of relevance for each slot.
For rankings without constraints, the Probability Ranking Prin-
ciple (PRP) [ 30] has long been understood to provide the ranking
that maximizes the number of relevant candidates that are found
in the top-ğ‘˜of the ranking, for any ğ‘˜. However, the PRP does not
apply to ranking problems with slot constraints, and naive exten-
sions of the PRP can be highly sub-optimal by disparately spending
human evaluation effort on candidates for which there are no open
slots while ignoring other candidates that are relevant for unfilled
slots. Furthermore, ranking with slot constraints is different from
both intrinsically and extrinsically diversified ranking [ 28], since
it involves a vector of relevance probabilities for each candidate
and it allows us to put explicit constraints on the set of relevant
results. Conventional diversification methods cannot handle rele-
vance vectors, and they typically focus on the composition of the
ranking instead of on the composition of the relevant results (e.g.,
demographic parity [ 17]). Furthermore, they typically do not allow
the specification of explicit constraints (e.g., [9, 10, 43]).
To remedy this shortcoming, we formalize and address the prob-
lem of ranking with slot constraints in this paper. In our formulation,
a human decision maker can define an arbitrary set of slots (e.g.,
âˆ—Both authors contributed equally to this research.
 
956
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wentao Guoâˆ—, Andrew Wangâˆ—, Bradon Thymes, and Thorsten Joachims
admission slots for each major) that need to be filled with relevant
candidates (i.e., qualified students). The ranker then supports the
human decision maker in allocating the evaluation effort while
leaving the relevance decisions to the human. Specifically, given a
relevance model that estimates the relevance probability of each
candidate for each slot, the goal is to compute a ranking under
which the expected number of filled slots is maximized. This model
is very general, as candidates can be qualified for any number of
slots, and we can use any probabilistic model with arbitrary depen-
dencies between the slot relevances of all candidates.
Under this model, we show that there is a connection between
slot-constrained ranking and bipartite matching, and we derive a
general ranking algorithm, called MatchRank, that merely requires
efficient sampling from the relevance model. We theoretically ana-
lyze MatchRank and show that it provides a strong approximation
guarantee. In particular, we prove that any top- ğ‘˜of the ranking
computed by MatchRank guarantees an expected number of filled
slots that is asymptotically at most (1âˆ’1/ğ‘’)away from the optimal
set with high probability. Furthermore, we show how MatchRank
can be implemented efficiently to handle ranking problems of sub-
stantial size. Finally, we provide an extensive empirical evaluation
of MatchRank, showing that it can outperform heuristic baselines
by a substantial margin, and perform accurately on a real-world
college-admission problem.
2 RELATED WORK
In the following we detail how our new setting of ranking with slot
constraints differs from existing research areas.
Search result diversification is a widely studied problem in
IR in which one aims to cover multiple intents or aspects of an
ambiguous or composite query. Specifically, in extrinsic diversi-
fication [ 28] the goal is to cover all intents of a query to ensure
that the user finds at least one relevant result despite the uncer-
tainty about the query intent. This typically leads to coverage-style
objectives [ 9,42], not matching problems like in slot-constrained
ranking. For intrinsic diversification [ 28], the goal to put together
a portfolio of items, but none of the existing methods provides the
same flexibility in specifying complex systems of slots with arbi-
trary dependencies. Another difference of our setting to existing
diversification approaches is that we do not need to model simi-
larity between items, either explicitly through predefined aspects
[32] or implicitly through similarity metrics [ 36]. However, one
could use our slot-constrained ranking framework as a method for
diversification, especially in high-stakes selection problems where
practitioners require both full transparency and control over the
composition of the set of selected items, and the ability to specify
complex real-world matching constraints.
Beyond the standard diversity settings, Agarwal et al . [2] con-
sider how to maximize user engagement while satisfying a min-
imum impression requirement per search result. However, their
target objective is not a matching problem, since serving an item to
one user does not exclude the use of such item with another user.
Also related but different from our setting is the approach of Dang
and Croft [13]. Their â€œseatsâ€ allocation approach is relevant to our
slot constraints, except that we directly treat the slots as a constraintinstead of a normalization factor. Therefore, we use the maximum
bipartite matching algorithm [19] for the ranking objective.
Fairness in ranking is also frequently implemented by adding
constraints to the ranking, since ranking by predicted merit can
lead to both poor representation [ 4] and suboptimal performance
[29] of admitted cohorts. In many cases, these fairness constraints
ensure a certain amount of representation from different groups
in various positions of the ranking (e.g., demographic parity) [ 17,
22]. This is critically different from our slot constraints, since slot
constraints act on the relevant items, not all items. Note that under
differential estimation accuracy between groups, merely ensuring
representational fairness can still be unfair to the relevant items [ 16,
33]. Furthermore, slot constraints are different from independent
diversity constraints [ 8], as slot constraints are always mutually
exclusive (e.g., college applicants can only be admitted by and
matriculate in 1 major of studies).
Matching problems have a wide range of applications in job
markets, dating, and resource allocation in online clouds [ 14,21,38].
The typical setting in stochastic matching is that each edge in a graph
is realized independently with (predicted) probability ğ‘[1,5], which
is analogous to the college admission scenario where we only have
a calibrated regression model to know the predicted probability
of a candidate ğ¶being relevant to a slot ğ‘†. Dickerson et al . [14]
consider online bipartite matching to improve the diversity and
relevance of search results by maximizing a multilinear objective
over the set of matched edges. Ahmed et al . [3] propose a quadratic
programming based objective for the diversity of a matching and
propose a scalable greedy algorithm to trade off efficiency and
diversity. Instead of evaluating an objective on top of matching on
a sampled graph, we use the size of the bipartite matching on a
sampled graph to derive a ranking of candidates that maximizes the
size of the bipartite matching in the true relevance matrix during the
human evaluation phase. We are the first to formulate this ranking
problem to maximize the size of the bipartite matching, and this is a
core contribution of this paper.
3 PROBLEM SETTING
Consider that we have ğ‘candidatesC={ğ¶1,...,ğ¶ğ‘}, and we have
ğ‘ <ğ‘slotsS={ğ‘†1,...,ğ‘†ğ‘ }that we need to fill with relevant can-
didates. Each candidate can be relevant to any number of slots, or
no slots at all. We denote whether candidate ğ¶ğ‘–is relevant to slot
ğ‘†ğ‘—via the matrix entry ğ‘…ğ‘–ğ‘—âˆˆ{0,1}. We use the generic concept
of â€œrelevanceâ€ to indicate whether a candidate matches a slot. This
allows us to model a broad range of selection problems as follows:
Hiring: A company has a number of openings for different roles,
with a specific number of interview slots budgeted for each
role. Applicants may be qualified for some subset of roles.
An applicant is relevant for an interview slot if qualified and
interested in the particular opening.
College Admission: Slots correspond to seats in various majors
(100 slots for CS, 50 slots for Math, etc.), and in each major a
certain number of slots is reserved for low socioeconomic
status students. A student is relevant for any slot in a major
if both qualified for and interested in that major.
Medical Trial: Researchers need to find qualifying participants for
a medical trial among millions of electronic health records.
 
957Ranking with Slot Constraints KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The trial is designed with a certain number of slots by gender,
race and disease severity. Patients are relevant for a slot if
they match demographic requirements and disease severity
determined by further testing.
In all of these application scenarios our goal is to fill all slots with rele-
vant candidates. Throughout this paper we assume that relevance is
binary, but we conjecture that many of our results can be extended
to non-binary relevance values.
If the relevance matrix ğ‘…was fully known, the problem of finding
relevant candidates to fill all slots would be solved by the maximum
bipartite matching algorithm [ 19]. In practice, however, we only
have uncertain information ğ‘ƒ(ğ‘…)about the true relevances. ğ‘ƒ(ğ‘…)
can be approximated by a probabilistic relevance prediction model
learned from data ( Ë†ğ‘ƒ(ğ‘…)). Furthermore, accurately revealing the
true relevance vector ğ‘…ğ‘–âˆˆ{0,1}ğ‘ of any particular candidate ğ¶ğ‘–
for allğ‘ slots comes at substantial cost. In the admission and hiring
example, assessing relevance requires detailed human review of
the application, and in the medical example it requires additional
medical testing. We would thus like to avoid evaluating candidates
that do not contribute to filling more slots, either because these
candidates are not relevant or because we already have identified
other relevant candidates for these slots.
To achieve this goal, we would like to compute a ranking ğœof can-
didates so that evaluating the candidates ğœ[1],...,ğœ[ğ‘]from top to
bottom maximizes the number of filled slots given the information
contained in ğ‘ƒ(ğ‘…). Without slot constraints (i.e., for any candidate
ğ¶ğ‘–:âˆ€ğ‘—,ğ‘˜:ğ‘…ğ‘–ğ‘—=ğ‘…ğ‘–ğ‘˜) the ranking problem has a well-known so-
lution that follows from the Probability Ranking Principle (PRP)
[30]: simply ranking candidates by their probability of relevance is
optimal under most sensible metrics. However, this PRP ranking
can be highly sub-optimal under general slot constraints, as the
following toy example shows.
Example 1 (Suboptimality of PRP for Ranking with Slot
Constraints). Consider a problem with ğ‘=1000 candidates and
ğ‘ =10slots. Candidates ğ¶1,...,ğ¶ 500have a probability of relevance of
0.5 for slotsğ‘†1,...,ğ‘† 5, and 0 for the other slots. Analogously, candidates
ğ¶501,...,ğ¶ 1000have a probability of relevance of 0.4 for slots ğ‘†6,...,ğ‘† 10,
and 0 for the other slots. Any heuristic based on sorting candidates
by a score computed from their probability of relevance would either
produce a ranking equivalent to ğ¶1,...,ğ¶ 500,ğ¶501,...,ğ¶ 1000 or equiva-
lent toğ¶501,...,ğ¶ 1000,ğ¶1,...,ğ¶ 500. However, either ranking would be
highly suboptimal, since one type of slots would not be filled until
after reviewing at least 500 candidates.
Note that this high degree of suboptimality already surfaces in
this particularly simplistic example, where there are only two types
of slots and candidates are relevant to at most one type of slots. In
the more general case, where we can have complex systems of slots
where each candidate can have dependent probabilities of being rel-
evant to multiple slots, it is not even clear how to heuristically apply
the conventional PRP. This motivates the need for a new algorithm
that goes beyond sorting candidates by some heuristic function of
relevance, but that explicitly takes the slot constraints into account.
In the following we develop the MatchRank algorithm that does
not have the inefficiencies of the PRP ranking and that provides
provable guarantees on the quality of the ranking for arbitrary
S1S2S3
SlotsC5
C4
C3
C2
C1Candidates0 0 0
0 0 1
1 1 0
1 1 0
1 1 0Ras Biadjacency Matrix
C1C2C3C4C5
S1S2S3MBM Solution of RFigure 1: Example showing how MBM computes an optimal
assignment of candidates to slots for a known relevance ma-
trixğ‘…. In this figure, ğ¶5is not relevant for any slots. ğ¶1,ğ¶2
andğ¶4are relevant and can be matched with available slots.
ğ¶3is relevant for ğ‘†1andğ‘†2, but both are already occupied.
slot constraints and relevance models. To start the derivation, the
following begins with a formal definition of the ranking objective.
3.1 Ranking Objective
We formalize the problem of ranking under slot constraints in two
steps. We first define the problem of finding a candidate set ğ‘‹ğ‘˜of
a given size ğ‘˜that is optimal. In the second step we show how to
construct a nested sequence
ğ‘‹0âŠ‚ğ‘‹1âŠ‚ğ‘‹2âŠ‚... (1)
of such candidate sets that naturally forms a ranking ğœ. In particular,
any two consecutive candidate sets ğ‘‹ğ‘˜andğ‘‹ğ‘˜+1differ by only one
candidateğ‘‹ğ‘˜+1\ğ‘‹ğ‘˜=ğ¶ğœ[ğ‘˜+1], which corresponds to the element
ğœ[ğ‘˜+1]of rankingğœ. Note thatğ‘‹0is always the empty set.
To evaluate a given candidate set ğ‘‹âŠ†C, we use the size of the
maximal matching between relevant candidates in ğ‘‹and slotsS.
This is illustrated in Figure 1, where the candidates in ğ‘‹and the
slots inSform a bipartite graph (right panel). The correspond-
ing submatrix of the relevance matrix ğ‘…(left panel) defines the
biadjacency matrix of the graph, where an edge exists whenever
a candidate ğ¶ğ‘–is relevant for slot ğ‘†ğ‘—. The right panel of Figure 1
shows the (not necessarily unique) maximum bipartite matching
{ğ¶1â†’ğ‘†1,ğ¶2â†’ğ‘†2,ğ¶4â†’ğ‘†3}, which corresponds to the largest
number of slots that can be filled with relevant candidates from ğ‘‹.
We denote this maximum bipartite matching size as MBM(ğ‘‹,S|ğ‘…).
While MBM(ğ‘‹,S|ğ‘…)gives us the optimal solution for a known
relevance matrix ğ‘…, we need to evaluate candidates sets ğ‘‹under
uncertainty about what the correct relevance matrix is. A natural
metric for evaluating a candidate set ğ‘‹underğ‘ƒ(ğ‘…)is to measure the
expected size of the matching, which corresponds to the expected
number of slots that can be filled with candidates from ğ‘‹.
ğ‘€(ğ‘‹)=Eğ‘…âˆ¼ğ‘ƒ(ğ‘…)
MBM(ğ‘‹,S|ğ‘…)
(2)
=âˆ‘ï¸
ğ‘…MBM(ğ‘‹,S|ğ‘…)ğ‘ƒ(ğ‘…) (3)
 
958KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wentao Guoâˆ—, Andrew Wangâˆ—, Bradon Thymes, and Thorsten Joachims
When evaluating a ranking ğœ, we will apply this metric ğ‘€(ğ‘‹)to
each top-ğ‘˜prefixğ‘‹ğ‘˜of the ranking ğœ.
3.2 MatchRank Algorithm
Given the metric ğ‘€(ğ‘‹)from Equation (3), our goal is to find a
rankingğœof the candidates in Cso thatğ‘€(ğ‘‹ğ‘˜)for any top-ğ‘˜prefix
is as large as possible under ğ‘ƒ(ğ‘…). We split this goal into three
steps. First, we show how to estimate ğ‘€(ğ‘‹)for any candidate set ğ‘‹
and for any ğ‘ƒ(ğ‘…)that permits sampling. Second, we show how to
construct a single candidate set ğ‘‹ğ‘˜of sizeğ‘˜that has a large value
forğ‘€(ğ‘‹ğ‘˜). Third, we show that our construction of the ğ‘‹ğ‘˜in the
previous step naturally produces a ranking.
Estimatingğ‘€(ğ‘‹).We do not make any structural assumptions
on the distribution ğ‘ƒ(ğ‘…), andğ‘ƒ(ğ‘…)can contain arbitrary depen-
dencies among the entries. The following is a general method for
evaluating a given candidate set ğ‘‹, where we merely require that
we can sample relevance matrices ğ‘…fromğ‘ƒ(ğ‘…). With such samples,
we can compute Monte-Carlo estimates of ğ‘€(ğ‘‹)as follows.
LetR=[ğ‘…1,...,ğ‘…ğ‘›]beğ‘›samples of relevance matrices drawn
i.i.d. fromğ‘ƒ(ğ‘…). The Monte-Carlo estimate of ğ‘€(ğ‘‹)is
Ë†ğ‘€(ğ‘‹)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1MBM(ğ‘‹,S|ğ‘…ğ‘–). (4)
By the weak law of large numbers, Ë†ğ‘€(ğ‘‹)converges to ğ‘€(ğ‘‹)for
largeğ‘›. We will later characterize how the number of samples ğ‘›
affects the algorithm.
Constructing the ranking. Now that we know how to estimate the
quality Ë†ğ‘€(ğ‘‹)of any particular candidate set ğ‘‹, we can think about
finding a candidate set that maximizes Ë†ğ‘€(ğ‘‹). However, naively
enumerating all subsets ğ‘‹âŠ‚C of size|ğ‘‹|=ğ‘˜and evaluating Ë†ğ‘€(ğ‘‹)
would not be efficient. Furthermore, it would not be clear how to
make sure that the candidate set is nested and forms a ranking.
To avoid this combinatorial enumeration, we instead construct
Ë†ğ‘€(ğ‘‹)using the following greedy algorithm, which we will prove to
enjoy strong approximation guarantees. Since this algorithm adds
one candidate in each iteration, it naturally constructs a ranking
and we show that the approximation guarantees hold for any top- ğ‘˜
of the ranking.
Algorithm 1: MatchRank
Input: candidatesC; slotsS; sampled relevances R=[ğ‘…1,...,ğ‘…ğ‘›];
ğ‘‹0â†âˆ…;ğ´â†C ;ğœ=[];ğ‘˜â†1
whileğ´â‰ âˆ…do
ğ¶bestâ†argmaxğ¶âˆˆğ´1
ğ‘›Ãğ‘›
ğ‘–=1MBM(ğ‘‹ğ‘˜âˆ’1âˆª{ğ¶},S|ğ‘…ğ‘–)
ğœ[ğ‘˜]=ğ¶best
ğ‘‹ğ‘˜â†ğ‘‹ğ‘˜âˆ’1âˆª{ğ¶best};ğ´â†ğ´âˆ’{ğ¶best};ğ‘˜â†ğ‘˜+1
end while
Output: rankingğœ
In each iteration ğ‘˜, the MatchRank algorithm finds the candi-
dateğ¶bestthat most improves Ë†ğ‘€(ğ‘‹ğ‘˜âˆ’1âˆª{ğ¶best})for the current
candidate set ğ‘‹ğ‘˜âˆ’1. It then places ğ¶bestinto position ğ‘˜of the rank-
ing. Furthermore, it adds ğ¶bestto the current top- ğ‘˜setğ‘‹ğ‘˜, and it
removesğ¶bestfrom the set of remaining candidates ğ´. These itera-
tions continue until all candidates have been added to the ranking.If only a top- ğ‘˜ranking is desired, one could also stop early. Note
that Algorithm 1 is optimized for clarity, but Section 3.4 presents
several efficiency improvements.
3.3 Theoretical Analysis
We now analyze theoretically how effective MatchRank is on con-
structing a ranking that optimizes the objective ğ‘€(ğ‘‹)given in
Equation (3). We start by stating our main result, which we will
then prove subsequently. The main result states that for any ğ‘˜,
the top-ğ‘˜candidate set ğ‘‹ğ‘˜constructed by MatchRank enjoys an
approximation guarantee compared to the optimal candidate set
ğ‘‹âˆ—
ğ‘˜=argmax
ğ‘‹âŠ†Câˆ§|ğ‘‹|=ğ‘˜ğ‘€(ğ‘‹).
Note that these optimal ğ‘‹âˆ—
ğ‘˜may not be nested and may not form a
ranking, unlike the ğ‘‹ğ‘˜constructed by MatchRank.
Theorem 3.1. The ranking ğœproduced by MatchRank when given
ğ‘ slots andğ‘›Monte-Carlo samples R=[ğ‘…1,...,ğ‘…ğ‘›]fromğ‘ƒ(ğ‘…)enjoys
the following approximation guarantee for each top- ğ‘˜setğ‘‹ğ‘˜inğœ:
with probability 1âˆ’ğ›¿(where 0<ğ›¿<1/2),
ğ‘€(ğ‘‹ğ‘˜)â‰¥
1âˆ’1
ğ‘’
ğ‘€(ğ‘‹âˆ—
ğ‘˜)âˆ’2ğ‘ âˆšï¸‚
ğ‘‚(ğ‘˜lnğ‘˜)+ln(2/ğ›¿)
2ğ‘›,
whereğ‘‹âˆ—
ğ‘˜=argmaxğ‘‹âŠ†Câˆ§|ğ‘‹|=ğ‘˜ğ‘€(ğ‘‹)is the optimal set.
The proof of Theorem 3.1 is given in Appendix A. Its main
steps are to first show that Ë†ğ‘€(ğ‘‹)is monotone submodular, which
implies that the greedy nature of MatchRank provides a (1âˆ’1/ğ‘’)
approximation guarantee for Ë†ğ‘€(ğ‘‹). We then show that optimizing
Ë†ğ‘€(ğ‘‹)provides a solution that is close to optimizing ğ‘€(ğ‘‹)directly.
3.4 Computational Efficiency of MatchRank
and Improvements
The MatchRank algorithm as written in Algorithm 1 is optimized
for clarity, but there are a number of improvements that can sub-
stantially speed up computation. To motivate these improvements,
we first analyze the runtime complexity of Algorithm 1.
For computing the top ğ‘˜positions of the ranking when there
areğ‘candidates, ğ‘ slots, andğ‘›Monte-Carlo samples, the greedy
maximizer inside Algorithm 1 will evaluate ğ‘‚(ğ‘˜ğ‘)sets. For each
such set, it will find the MBM solutions of all bipartite graphs from
R, which takes ğ‘‚(ğ‘›ğ‘ğ‘ âˆšğ‘+ğ‘ )time per set when using the classic
Hopcroft-Karp MBM algorithm [ 19]. So, a naive implementation
will takeğ‘‚(ğ‘˜ğ‘›ğ‘2ğ‘ âˆšğ‘+ğ‘ )time. However, this implementation is
unnecessarily slow.
We can improve the time efficiency of MatchRank by following
the principle that any unmatched candidate can increase the match-
ing size by at most 1. So, if we are given a candidate set ğ‘‹âŠ†C and
an unmatched candidate ğ¶, finding the MBM(ğ‘‹âˆª{ğ¶},S|ğ‘…)can be
reduced to determining if there is an augmenting path to the match-
ing of MBM(ğ‘‹,S|ğ‘…)starting from ğ¶. If such an augmenting path
exists, then we can extend the matching and MBM(ğ‘‹âˆª{ğ¶},S|ğ‘…)=
MBM(ğ‘‹,S|ğ‘…)+1. It no augmenting path exists, then we will know
that MBM(ğ‘‹âˆª{ğ¶},S|ğ‘…)=MBM(ğ‘‹,S|ğ‘…)and the matching re-
mains unchanged. Therefore, it will take ğ‘‚(ğ‘ğ‘ )time (a BFS) per
each unmatched candidate per ranking step, and we obtain an
ğ‘‚(ğ‘˜ğ‘›ğ‘(ğ‘ğ‘ ))=ğ‘‚(ğ‘˜ğ‘›ğ‘2ğ‘ )algorithm.
 
959Ranking with Slot Constraints KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Ranking
algorithms
5. obtain a ranking
shortlist  over all
candidates3. obtain Monte-Carlo samples of
biadjacency relevance matrices 
via i.i.d. sampling from 4. obtain slot
constraints 
Regression models
1. train regression models to
predict relevances between any
given candidate and any given slotCandidate features
2. obtain predicted
probability of 
relevances6. obtain ground-
truth relevances  
from human reviews
6. Find the minimum  such
that all slots  are filled
by relevant candidates in Predicting relevances Ranking Evaluation
Figure 2: Application and evaluation pipeline used in ranking experiments.
If we consider the typical scenario where ğ‘>ğ‘ , we can improve
the time complexity by keeping a list of unmatched slots instead
of candidates for each bipartite graph, and on each ranking step
we start from each unmatched slot and follow the BFS to find all
augmenting paths that end on unmatched candidates. We need
ğ‘‚(ğ‘ğ‘ 2)time per ranking step, and in total only ğ‘‚(ğ‘˜ğ‘›ğ‘ğ‘ 2)time. We
also note that on each ranking step each Monte-Carlo estimate ğ‘…ğ‘–
is independent and with perfect parallelism, we could eliminate the
dependency over ğ‘›to getğ‘‚(ğ‘˜ğ‘ğ‘ 2)time complexity.
A further improvement in runtime can be achieved by exploiting
that MatchRank is a greedy algorithm for maximizing a submodular
objective. For any such algorithm, we can use lazy evaluation [ 23] to
accelerate the ranking process in practice1. Lazy greedy maintains
a priority queue of stale marginal gains to reduce unnecessary
computation of marginal gains for many examples per step. Since
marginal gains can never increase due to the submodularity of the
objective, the stale marginal gains provide an upper bound on the
improvement. Thus, if a stale marginal gain is not large enough
to propose a candidate as a greedy maximizer, then recomputing
its marginal gain is not necessary. This is particularly effective in
our matching scenarios, since many candidates are not relevant
for some slots. This means they will have small marginal gains
even in the first step of ranking, and we can significantly reduce
computation of their marginal gains during subsequent iterations
when using lazy greedy.
Finally, we could replace the greedy algorithm with some approx-
imate version of greedy that is substantially faster, such as stochastic
greedy [ 24] and threshold greedy [ 6] for monotone submodular
maximization problem. We have performed initial evaluations of
these methods with promising results, but did not find a need for
them for our experiments. Generally, we found the exact greedy
algorithm to be tractable for datasets with up to 50,000 candidates.
Note that theâˆš.term approaches zero as we increase the number
ğ‘›of sampled relevance matrices. This means that for large Monte-
Carlo samples, the approximation factor approaches 1âˆ’1/ğ‘’, where
ğ‘’is Eulerâ€™s number.
The monotone submodularity of Ë†ğ‘€(ğ‘‹)opens a large arsenal of
submodular optimization methods for constructing candidate sets
ğ‘‹ğ‘˜with provable approximation guarantees. We opt for the greedy
algorithm, since it naturally constructs a ranking.
1We run all of our experiments with lazy greedy.4 EMPIRICAL EVALUATION
We now evaluate the MatchRank algorithm on three types of data.
The first is fully synthetic data, where we can control all aspects of
the ranking problems to understand the conditions under which
MatchRank improves over baseline heuristics. Second, we evalu-
ate MatchRank on a number of benchmark datasets. And finally,
we verify the applicability of MatchRank on a real-world college-
admissions problem. We provide software for reproducing the em-
pirical results and to enable followup work on MatchRank2.
Evaluation Process and Metric. For each problem, our evaluation
follows the process depicted in Figure 2. We first use a training set
to learn a model â€” usually a calibrated regression model â€“ which we
can use to infer the relevance probabilities Ë†ğ‘ƒ(ğ‘…)for the candidates
in the test set. We apply MatchRank and other baseline rankers to
rank this test set, which only requires sampling from Ë†ğ‘ƒ(ğ‘…).
To evaluate any ranking ğœ, we use the following process and
metric. For each top- ğ‘˜prefixğœğ‘˜ofğœ, we reveal the ground-truth
relevance labels ğ‘…of theseğ‘˜candidates and compute how many
slots can be filled when optimally matching these ğ‘˜candidates to
the slots. This is precisely the size of the matching ğ‘€ğµğ‘€(ğœğ‘˜,S|ğ‘…).
Our final evaluation metric for ğœis the smallest ğ‘˜for which the
prefixğœğ‘˜fills all|S|slots with relevant candidates.
ğ‘˜min=argmin
ğ‘˜âˆˆ[ğ‘]{ğ‘€ğµğ‘€(ğœğ‘˜,S|ğ‘…)=|S|} (5)
This means that ğ‘˜minis the number of candidates in ğœthat need to
be reviewed before all slots are filled.
Sinceğ‘˜minscales with the total number of slots, we report the
normalized ğ‘˜min/|S|, so that the best possible score is 1.
Baseline Rankers. We compare MatchRank against the following
baselines rankers. These methods compute a score for each candi-
date, and then rank by this score. The baseline rankers differ by
how they aggregate the estimated marginal relevance probabilities
Ë†ğ‘ƒ(ğ‘…ğ¶,ğ‘†)(estimated from the Monte-Carlo samples) for each can-
didateğ¶across all slots ğ‘†. The first heuristic is motivated by the
softAND ruleÃ
ğ‘†âˆˆSË†ğ‘ƒ(ğ‘…ğ¶,ğ‘†). The second uses the soft ORrule
1âˆ’Ã
ğ‘†âˆˆS(1âˆ’Ë†ğ‘ƒ(ğ‘…ğ¶,ğ‘†)). Both the AND and the ORrule skip probabil-
ities that are zero when computing the product. The third heuristic,
called Total Relevance (TR) merely sums the relevance probabilitiesÃ
ğ‘†âˆˆSË†ğ‘ƒ(ğ‘…ğ¶,ğ‘†)across all slots. The final heuristic is a normalized
version of the total relevances, called NTR, that normalizes with
the competition for each slot (number of relevant candidates for
2https://github.com/GarlGuo/ranking_with_slot_constraints.git
 
960KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wentao Guoâˆ—, Andrew Wangâˆ—, Bradon Thymes, and Thorsten Joachims
Table 1: Synthetic Datasets: Performance of MatchRank in comparison to the heuristic baselines, reporting mean and standard
deviation in the format of â€œ mean stdâ€ (for standard error divide byâˆš
1000) ofğ‘˜min/|S| over 1000 random draws of the true
relevance matrix ğ‘…fromğ‘ƒ(ğ‘…).
Default # Slots Per Group # Group Memberships # Samples ğ‘ƒ(ğ‘…)
30 70 1 3 100 1000 S L
MatchRank 1.27 0.061.26 0.071.29 0.05 2.05 0.20 1.12 0.03 1.32 0.101.25 0.031.52 0.131.14 0.03
AND 5.010.296.060.64 4.290.20 11.260.31 2.730.24 5.010.295.000.285.840.394.450.25
OR 4.200.334.560.53 3.850.22 11.260.31 2.290.21 4.210.344.200.335.310.413.290.29
TR 4.410.315.480.39 4.000.23 10.730.44 2.570.17 4.140.314.730.315.460.413.900.26
NTR 1.350.071.450.12 1.300.04 3.690.14 1.12 0.02 1.450.071.390.091.660.131.200.05
Random 1.690.111.780.18 1.680.02 3.700.40 1.230.04 1.690.111.690.112.510.251.350.07
each slot in each sampled relevance matrix)Ã
ğ‘†âˆˆSË†ğ‘ƒ(ğ‘…ğ¶,ğ‘†)Ã
ğ¶â€²âˆˆCË†ğ‘ƒ(ğ‘…ğ¶â€²,ğ‘†).
We use the same sampled relevance matrices R=[ğ‘…1,...,ğ‘…ğ‘›]as
what we use for MatchRank when computing the ranking shortlist
for TR and NTR. Note that the conventional diversification methods
we discussed in the related work section cannot handle relevance
signals that are multivariate vectors to optimize the number of filled
slots, and thus an empirical comparison is not sensible.
4.1 Synthetic Datasets
We first focus on a synthetic dataset where we can control the
structure of ğ‘ƒ(ğ‘…), so that we can investigate all problem dimensions
that affect the MatchRank algorithm. Furthermore, we can directly
useğ‘ƒ(ğ‘…)instead of Ë†ğ‘ƒ(ğ‘…), which avoids confounding the algorithmâ€™s
behavior with potential inaccuracies in a learned Ë†ğ‘ƒ(ğ‘…).
Experiment Setup. To construct synthetic ğ‘ƒ(ğ‘…), we defineğ‘”groups
(defaultğ‘”=10) and each group has ğ‘ slots (default ğ‘ =50). We
then create 10,000 candidates, where each candidate is randomly
assigned to ğ‘groups (default ğ‘=2). If candidate ğ¶is a member
of groupğ‘—âˆˆ{1,...,ğ‘”}, we first sample ğ‘from the Gaussian distri-
butionN(ğ‘base+0.03âˆ—ğ‘—,0.1)with default ğ‘baseas 0.3, and clip ğ‘
to the range of (0.0001, 0.9999). We then sample from a Bernoulli
distribution with probability ğ‘, and the success outcome means ğ¶
is relevant for all slots associated with group ğ‘—. If candidate ğ¶is not
member of group ğ‘—, thenğ¶is not relevant for any slots associated
with group ğ‘—, or formally ğ‘ƒ(ğ‘…ğ¶,ğ‘†)=0if slotğ‘†belongs to group ğ‘—.
If not mentioned otherwise, parameters are at their default value.
We draw 200 Monte-Carlo i.i.d. samples R=[ğ‘…1,...,ğ‘… 200]as
input to the ranking algorithms. For evaluation, we draw a ground-
truth relevance matrix ğ‘…and compute ğ‘˜min/|S|, repeat this evalua-
tion 1000 times, and report the mean and standard deviation.
Comparing MatchRank against the Baselines. The first column
of Table 1 shows the performance of MatchRank and the baselines
for the default values of the synthetic data generator. MatchRank
achieves a performance of 1.27, which means that on average only
an additional 27% of candidates need to be reviewed beyond the
number of slots. The best heuristic is NTR, which averages 35%.
Most heuristics do worse than random, which requires 69%. The
reason is that the heuristics systematically miss candidates of some
group, such that those slots cannot be filled.
Effects of the Number of Slots Per Group. We now vary the number
of slots per group from the default of 50 to 30 and 70. The resultsfor 30 and 70 are in the third main column of Table 1, while the
results for 50 are in the default column. All other parameters are
at their default values. MatchRank again show stable performance
over all three settings and dominates most baselines. Only NTR
comes close for larger numbers of slots.
Effects of Number of Group Memberships. We now assign each
candidate to be a member of 1, 2 (default) and 3 groups respec-
tively. All other parameters are at their default. The fourth main
column of Table 1 shows that the problem becomes easier for all
methods, when each candidate can be in more groups. We see that
the advantage of MatchRank is greater for harder problems.
Effect of Number of Monte-Carlo Samples. We now investigate
how important the number ğ‘›of Monte-Carlo samples Ris, as we
varyğ‘›among 100, 200 (default), and 1000. The results can be found
in the fifth main column of Table 1. We find that MatchRank already
performs well for ğ‘›=100, although having larger samples still
improve the results. A larger ğ‘›may be even more important for
ğ‘ƒ(ğ‘…)with stronger dependencies between candidates and slots.
Effect of Overall Relevance Level. We now vary the overall proba-
bility of relevance. In our model, this is controlled by the parameter
ğ‘base, and the higher ğ‘basethe greater the overall probability of
relevance. We vary ğ‘basebetween 0.2 (S), 0.3 (default), and 0.4 (L).
The results can be found in the last major column of Table 1. As
ğ‘baseincreases, the problem of finding relevant candidates to fill
the slots becomes easier. Both MatchRank and the baselines benefit
from this, but MatchRank maintains a consistent advantage.
Analyzing the Variability of MatchRank. We want to further un-
derstand the variability of ğ‘˜min/|S| of MatchRank across ground-
truth relevance matrices. So we prepare a histogram of ğ‘˜min/|S|
for MatchRank evaluating on Rtestin default settings as shown in
Figure 3. This figure demonstrates that the distribution of ğ‘˜min/|S|
is slightly right-tailed as the sample mean is higher than the sam-
ple median. This could be explained as MatchRank being prone to
overestimating candidatesâ€™ probability of relevances. Therefore, it
is likely that some slots are not filled by relevant candidates even
when MatchRank believes all slots have been filled. Such phenom-
enon is illustrated by the fact that there are still 24.9% ğ‘…samples
for which slots have not been filled after Î”Rbecomes 0 (the purple
line) in the Figure 3.
 
961Ranking with Slot Constraints KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Real-World Benchmarks: Performance of MatchRank in comparison to heuristic baselines in terms of ğ‘˜min/|S|. For
each trial, we repeat for 3 random seeds and report the mean and standard deviation of ğ‘˜min/|S|in the format of â€œmean stdâ€.
Datasets Medical Bibtex Delicious
Slots Per Label 5 10 15 10 20 30 10 30 50
MatchRank 2.17 0.262.00 0.102.23 0.372.62 0.532.33 0.512.07 0.121.09 0.071.05 0.011.07 0.01
AND 4.120.723.000.162.570.237.741.004.920.364.200.231.680.061.940.171.690.03
OR 4.820.993.360.172.800.256.420.434.400.173.680.042.490.112.100.092.050.08
TR 4.400.503.270.432.750.296.350.444.330.153.680.082.210.292.110.101.980.05
NTR 4.830.432.870.212.080.095.960.403.350.172.480.051.520.251.520.261.500.01
Random 4.560.593.950.423.490.228.100.066.480.176.410.271.370.021.420.061.390.04
Datasets TMC2007 Mediamill Bookmarks
Slots Per Label 30 50 70 10 30 50 10 30 50
MatchRank 1.24 0.091.30 0.041.28 0.021.03 0.001.07 0.031.11 0.014.66 0.293.60 0.443.70 0.13
AND 13.681.479.711.097.981.081.840.112.070.212.060.0368.601.8326.954.5724.810.92
OR 3.180.42 3.310.163.590.392.770.292.660.292.220.1020.121.5610.950.62 8.930.68
TR 8.710.25 6.260.255.520.303.120.172.270.112.110.0519.632.0111.640.60 9.230.43
NTR 1.820.09 1.480.081.370.072.630.241.670.031.490.0422.712.4010.130.73 7.020.23
Random 4.370.05 3.790.053.550.122.740.482.460.312.800.1413.780.6911.930.5013.140.57
1.2 1.3 1.4 1.5
kmin/|S|0.000.040.080.120.16Frequencysample mean
sample median
sample meanÂ±sample std
âˆ†R= 0
Figure 3: Synthetic Datasets: Histogram of ğ‘˜min/|S| for
MatchRank under default settings for 1000 draws of the
ground-truth relevances from ğ‘ƒ(ğ‘…). The vertical lines are
the mean, median, and standard deviation of the histogram.
Î”R=0is the position in the ranking when all slots in the
Monte-CarloRsamples are filled and the marginal gain of
adding another candidate becomes 0.
Analyzing the Robustness of MatchRank to Ë†ğ‘ƒ(ğ‘…)Misspecification.
We now investigate how robust MatchRank is against an inaccu-
rately learned Ë†ğ‘ƒ(ğ‘…). We draw the ground-truth relevance labels
from a model ğ‘ƒ(ğ‘…)withğ‘base=0.3(Ref), but draw the Monte-
Carlo samples from misspecified models Ë†ğ‘ƒ(ğ‘…)withğ‘baseset to 0.1
(XS), 0.2 (S), 0.4 (L), and 0.5 (XL). Figure 4 shows that MatchRank
performs best for the correctly specified Ë†ğ‘ƒ(ğ‘…)as expected. Among
the misspecified Ë†ğ‘ƒ(ğ‘…), we can observe that MatchRank is more
vulnerable to overestimation of relevance probabilities than un-
derestimation. This can be explained as follows. If MatchRank is
erroneously convinced that a slot is filled with high probability, it
will not add an alternative candidate for this slot to the ranking.
This fact is illustrated in the middle subfigure of Figure 4, where
XS SRef LXL1.001.251.501.752.00kmin/|S|
500 750 1000460470480490|S|Number of Filled SlotsMonte-CarloRSamples
XS
S
Ref
L
XL
500 750 1000460470480490|S|Avg Number of Filled SlotsGround-Truth RFigure 4: Effect from Ë†ğ‘ƒ(ğ‘…)misspecification, where Ref is the
correct model. The left subfigure shows mean and standard
deviation of ğ‘˜min/|S|.
MatchRank fills all slots in the Monte-Carlo Rsamples faster as the
overall relevance level increases. However, the average number of
filled slots using the ground-truth relevances ğ‘…is not necessarily
higher, and in fact both L and XL have fewer filled slots per ranking
step than S and XS for ğ‘…. So it may be advisable to clip Ë†ğ‘ƒ(ğ‘…)to
a maximum value that is well below 1 to increase robustness to
misspecification.
4.2 Real-World Benchmark Datasets
We now evaluate MatchRank on six benchmark datasets, where
we now learn the relevance model Ë†ğ‘ƒ(ğ‘…)from training data. These
benchmark datasets are constructed from the multi-label datasets
Medical [ 26], Bibtex [ 20], Delicious [ 39], TMC2007 [ 35], Mediamill
[34], and Bookmarks [ 20] from the Mulan data repository [ 40]. Our
data source is the scikit-multilearn library [37].
 
962KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wentao Guoâˆ—, Andrew Wangâˆ—, Bradon Thymes, and Thorsten Joachims
Experiment Setup. Each dataset comes with a train/test split. We
consider each example in test set as a candidate, and each label
as a group. We first select 10 labels that cover different numbers
of relevant candidates (the specific label choices are included in
Table 4 in Appendix B), and then allocate a certain number of slots
for each label. We train Ë†ğ‘ƒ(ğ‘…)on the training set using a calibrated
binary logistic regression for each label. For the Medical, Bibtex,
Delicious, Mediamill, and Bookmarks dataset, we use Platt scaling
method [ 27] to calibrate the probabilistic predictions while for the
TMC2007 dataset, we use isotonic regression. To raise the noise in
the relevance prediction to a more challenging and realistic level, we
mask 20% of the label in both the train and test set. We then draw 100
Monte-Carlo samples from Ë†ğ‘ƒ(ğ‘…)as input to the ranking algorithm.
All rankers are evaluated against the true masked relevance labels
in the test set, meaning that a test example matches a slot if it
contains the corresponding label.
Results. The result are shown in Table 2. For all benchmark
datasets and all numbers of slots per label, MatchRank delivers the
best ranking performance in terms of ğ‘˜min/|S|. The most competi-
tive heuristic ranker is NTR, but the results show that its heuristic
can fail on some datasets and provide substantially worse rank-
ing performance than MatchRank (e.g. Bookmarks). These results
demonstrate that MatchRank performs robustly over a wide range
of datasets where the Ë†ğ‘ƒ(ğ‘…)is learned from training data.
Analysis. To provide additional insights into the behavior of
MatchRank in comparison to the heuristic rankers, Figure 5 shows
their behavior on each dataset with medium amount of slots per
label. On each subfigure, left panels show the average number of
filled slots over the Monte-Carlo samples as the shortlist size ğ‘˜
grows. Across all datasets, MatchRank finds a close to optimal rank-
ing while all heuristics require a substantially longer shortlist. This
indicates that the heuristics optimize a fundamentally wrong objec-
tive. Right panels of each subfigure in Figure 5 show the number
of filled slots when using the ground-truth labels for matching.
These plots generally show a gap between MatchRank and the
optimal ranking one can compute from the ground-truth labels.
Since MatchRank performs close to optimal on the Monte-Carlo
samples, this gap can be attributed to the inherent inaccuracy and
uncertainty of Ë†ğ‘ƒ(ğ‘…)in predicting the ground-truth relevance. This
suggests that MatchRank performs close to optimal in terms of its
optimization performance, and that the remaining suboptimality is
largely a result of an imperfect Ë†ğ‘ƒ(ğ‘…).
4.3 College Admission Dataset
To verify the effectiveness of MatchRank under real-world condi-
tions, we consider an anonymized undergraduate admission dataset
from a selective US university. The groups in this datasets are ma-
jors, and we posit that each major has only a fixed number of slots
for admitting qualified students. We consider all majors that ad-
mitted at least 50 students, which leaves us with 13 majors and
19421 applicants in the test set. On the admission decisions from
the prior year we train a boosted tree model with XGBoost [ 11]
using a logistic loss objective and L2-regularization. This model is
used to predict each applicantâ€™s probability of being admitted, and
we clip the maximum Ë†ğ‘ƒ(ğ‘…)by 0.3. Applicants can indicate theirTable 3: College Admission: Performance of MatchRank in
comparison to heuristics in terms of ğ‘˜min/|S|. For each trial,
we repeat for 3 random seeds and report the mean and stan-
dard deviation of ğ‘˜min/|S|in the format of â€œmean stdâ€.
ğ‘ max 30 50 70 100
MatchRank 5.74 0.26 7.65 3.08 7.34 2.87 7.52 3.07
AND 12.635.98 8.693.48 10.644.45 9.433.16
OR 22.8412.1319.8710.1417.658.6416.447.81
TR 22.5811.07 16.897.58 14.076.0312.575.02
NTR 7.022.73 8.323.58 8.103.49 7.733.38
Random 30.7816.1323.1111.6119.189.3217.017.83
interests in majors, and the probability of admission to a major that
the applicant did not indicate interest in is set to zero. This provides
us with Ë†ğ‘ƒ(ğ‘…)for all test applicants.
We create 100 Monte-Carlo sampled relevance matrices from this
Ë†ğ‘ƒ(ğ‘…)as input to all ranking algorithms. To set the number of avail-
able slots for each member, we use min(âŒŠ0.7âˆ—|relevant applicants
for this major|âŒ‹,ğ‘ max)to get an interesting relationship between
supply and demand, and we vary ğ‘ maxin the following experiments.
We then run our ranking algorithms, and during evaluation we
reveal the applicants ground-truth admissions decisions (i.e. the
true relevance matrix ğ‘…) for each candidate. To evaluate, we again
find the minimum shortlist size ğ‘˜for each algorithm at which all
slots are filled with relevant applicants.
The results are provided in Table 3. This dataset is substantially
more challenging for all methods, as the density of relevant candi-
dates is small and we need to find a larger fraction of the relevant
candidates to fill all slots. However, even in this challenging setting,
we see that MatchRank is more effective than the heuristic base-
lines. This holds particularly when the number of slots per major
is smaller, as the performance gap between MatchRank and other
heuristic algorithms becomes larger. For larger ğ‘ maxthe majority of
relevant candidates need to be found, such that even a small degree
of inaccuracy in Ë†ğ‘ƒ(ğ‘…)can have large impact.
5 EXTENSIONS AND FUTURE WORK
Instead of binary relevances as assumed in this paper, some ap-
plications may require real-valued relevances (e.g., star-ratings)
where the decision maker aims to maximize the sum of relevances
under slot constraints. In this setting, the optimal solution is given
by the Maximum Weight Bipartite Matching ğ‘€ğ‘Šğµğ‘€(ğ‘‹,S|ğ‘…)for
the weighted biadjacency matrix ğ‘…. If the MWBM objective is
also monotone submodular3, we could simply replace the MBM in
MatchRankwith the MWBM and provide a similar approximation
guarantee for this weighted version of MatchRank.
Another extension is the use of high-dimensional matching in-
stead of the bipartite matching considered in this paper. This could
model slot constraints over more than one category. For example,
college admission may have slot constraints not only for majors but
also for extracurricular teams (e.g., orchestra, athletics). Since slots
for majors are orthogonal to the slots for extracurricular teams, this
3We find a submodularity proof for maximum generalized flow problem in Fleis-
cher [15], and we would refer readers to that paper as such discussion is beyond the
scope of this paper.
 
963Ranking with Slot Constraints KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
100 200 300 400
Ranking Shortlist Size406080|S|Number of Filled SlotsMonte-CarloRSamples
OPT
MatchRank
AND
OR
TR
NTR
Random
kminfor MatchRank
100 200 300 400
Ranking Shortlist Sizekmin= 1.860|S|Ground-Truth R
(a) Medical (10 slots per label)
100 300 500 700
Ranking Shortlist Size50100150|S|Number of Filled SlotsMonte-CarloRSamples
OPT
MatchRank
AND
OR
TR
NTR
Random
kminfor MatchRank
100 300 500 700
Ranking Shortlist Sizekmin= 2.265|S|Ground-Truth R (b) Bibtex (20 slots per label)
250 300 350 400 450
Ranking Shortlist Size250260270280290|S|Number of Filled SlotsMonte-CarloRSamples
OPT
MatchRank
AND
OR
TR
NTR
Random
kminfor MatchRank
250 300 350 400 450
Ranking Shortlist Sizekmin= 1.053|S|Ground-Truth R
(c) Delicious (30 slots per label)
400 600 800 1000 1200
Ranking Shortlist Size200300400|S|Number of Filled SlotsMonte-CarloRSamples
OPT
MatchRank
AND
OR
TR
NTR
Random
kminfor MatchRank
400 600 800 1000 1200
Ranking Shortlist Sizekmin= 1.322|S|Ground-Truth R (d) TMC2007 (50 slots per label)
200 300 400 500
Ranking Shortlist Size150200250|S|Number of Filled SlotsMonte-CarloRSamples
OPT
MatchRank
AND
OR
TR
NTR
Random
kminfor MatchRank
200 300 400 500
Ranking Shortlist Sizekmin= 1.043|S|Ground-Truth R
(e) Mediamill (30 slots per label)
200 600 1000 1400 1800
Ranking Shortlist Size0100200|S|Number of Filled SlotsMonte-CarloRSamples
OPT
MatchRank
AND
OR
TR
NTR
Random
kminfor MatchRank
200 600 1000 1400 1800
Ranking Shortlist Sizekmin= 4.090|S|Ground-Truth R (f) Bookmarks (30 slots per label)
Figure 5: Real-World Benchmarks: number of filled slots on the Monte-Carlo samples versus the expected number of filled
slots w.r.t. the ground-truth relevances ğ‘…. On the right plot of each subfigure, â€œOPTâ€ is the optimal ranking if the ground-truth
ğ‘…was known, which achieves ğ‘€ğµğ‘€(ğœğ‘‚ğ‘ƒğ‘‡
|S|,S|ğ‘…)=|S|. The dashed blue line represents the ğ‘˜minresult for MatchRank, with the
exact ratio over|S|illustrated nearby. Each subfigure represents a single seed result of the described experiment setting.
corresponds to a three-dimensional matching (3-DM) problem. The
3-DM problem is NP-hard and even APX-hard [ 18], and the best
approximation algorithm so far achieves an error bound of (4/3 +
ğœ–) [12]. In general, Hazan et al . [18] show that all ğ‘‘-DM (ğ‘‘â‰¥3)
problems cannot be approximated within a factor of ğ‘‚(ğ‘‘/lnğ‘‘)un-
less P = NP. But even if this matching problem cannot be solved
exactly any more, the approximate solutions could give rise to an
approximate ranking algorithm analogous to MatchRank.
6 CONCLUSION
We introduce the problem of ranking under slot constraints, which
allows practitioners to specify conditions that arise in a wide va-
riety of applications. To solve this ranking problem, we develop
the MatchRank algorithm and show that it provides a theoretical
guarantee on its ranking performance. A key insight is that the
ranking objective can be related to the maximum bipartite matching
problem, and that it is monotone submodular. We also show how
MatchRank can be implemented efficiently so that it can efficiently
handle real-world ranking problems of substantial size. Beyond itstheoretical guarantees, MatchRank shows superior ranking perfor-
mance across extensive experiments compared to several heuristic
baselines. This holds across a wide range of datasets and experiment
conditions, and MatchRank shows robustness to sample size and
misspecified relevance distributions. We conclude that the ability
to model complex problems and provide accurate rankings across a
wide range of domains, backed by theoretical guarantees, makes
the slot constraint framework a promising paradigm for tackling
complex real-world ranking problems.
ACKNOWLEDGMENTS
This research was supported in part by NSF Awards IIS-2008139,
IIS-2312865, and OAC-2311521. All content represents the opinion
of the authors, which is not necessarily shared or endorsed by their
respective employers and/or sponsors.
 
964KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wentao Guoâˆ—, Andrew Wangâˆ—, Bradon Thymes, and Thorsten Joachims
REFERENCES
[1]Marek Adamczyk, Brian Brubach, Fabrizio Grandoni, Karthik A Sankararaman,
Aravind Srinivasan, and Pan Xu. 2020. Improved approximation algorithms for
stochastic-matching problems. arXiv:2010.08142
[2]Deepak Agarwal, Shaunak Chatterjee, Yang Yang, and Liang Zhang. 2015. Con-
strained Optimization for Homepage Relevance. In Proceedings of the 24th In-
ternational Conference on World Wide Web (Florence, Italy) (WWW â€™15 Com-
panion). Association for Computing Machinery, New York, NY, USA, 375â€“384.
https://doi.org/10.1145/2740908.2745398
[3]Faez Ahmed, John P Dickerson, and Mark Fuge. 2017. Diverse weighted bipartite
b-matching. arXiv:1702.07134
[4]Peter Arcidiacono, Michael Lovenheim, and Maria Zhu. 2015. Affirmative action
in undergraduate education. Annu. Rev. Econ. 7, 1 (2015), 487â€“518.
[5]Sepehr Assadi, Sanjeev Khanna, and Yang Li. 2019. The stochastic matching prob-
lem with (very) few queries. ACM Transactions on Economics and Computation
(TEAC) 7, 3 (2019), 1â€“19.
[6]Ashwinkumar Badanidiyuru and Jan VondrÃ¡k. 2014. Fast algorithms for maximiz-
ing submodular functions. In Proceedings of the twenty-fifth annual ACM-SIAM
symposium on Discrete algorithms. SIAM, 1497â€“1514.
[7]Claudia R Baquet, Patricia Commiskey, C Daniel Mullins, and Shiraz I Mishra.
2006. Recruitment and participation in clinical trials: socio-demographic, ru-
ral/urban, and health care access predictors. Cancer detection and prevention 30,
1 (2006), 24â€“33.
[8]Robert Bredereck, Piotr Faliszewski, Ayumi Igarashi, Martin Lackner, and Piotr
Skowron. 2018. Multiwinner elections with diversity constraints. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 32.
[9]Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based
reranking for reordering documents and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference on Research and development
in information retrieval. 335â€“336.
[10] Olivier Chapelle, Shihao Ji, Ciya Liao, Emre Velipasaoglu, Larry Lai, and Su-
Lin Wu. 2011. Intent-based diversification of web search results: metrics and
algorithms. Information Retrieval 14 (2011), 572â€“592.
[11] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD â€™16).
ACM, New York, NY, USA, 785â€“794. https://doi.org/10.1145/2939672.2939785
[12] Marek Cygan. 2013. Improved approximation for 3-dimensional matching via
bounded pathwidth local search. In 2013 IEEE 54th Annual Symposium on Foun-
dations of Computer Science. IEEE, 509â€“518.
[13] Van Dang and W. Bruce Croft. 2012. Diversity by Proportionality: An Election-
Based Approach to Search Result Diversification. In Proceedings of the 35th In-
ternational ACM SIGIR Conference on Research and Development in Information
Retrieval (Portland, Oregon, USA) (SIGIR â€™12). Association for Computing Ma-
chinery, New York, NY, USA, 65â€“74. https://doi.org/10.1145/2348283.2348296
[14] John P Dickerson, Karthik Abinav Sankararaman, Aravind Srinivasan, and Pan
Xu. 2019. Balancing relevance and diversity in online bipartite matching via
submodularity. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 1877â€“1884.
[15] Lisa Fleischer. 2010. Data center scheduling, generalized flows, and submodu-
larity. In 2010 Proceedings of the Seventh Workshop on Analytic Algorithmics and
Combinatorics (ANALCO). SIAM, 56â€“65.
[16] Nikhil Garg, Hannah Li, and Faidra Monachou. 2021. Dropping Standardized
Testing for Admissions Trades Off Information and Access. arXiv:2010.04396
[cs] http://arxiv.org/abs/2010.04396
[17] Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. 2019. Fairness-
Aware Ranking in Search & Recommendation Systems with Application to
LinkedIn Talent Search. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD
â€™19). Association for Computing Machinery, New York, NY, USA, 2221â€“2231.
https://doi.org/10.1145/3292500.3330691
[18] Elad Hazan, Shmuel Safra, and Oded Schwartz. 2003. On the complexity of
approximating k-dimensional matching. In Approximation, Randomization, and
Combinatorial Optimization.. Algorithms and Techniques. Springer, 83â€“97.
[19] John E Hopcroft and Richard M Karp. 1973. An nË†5/2 algorithm for maximum
matchings in bipartite graphs. SIAM Journal on computing 2, 4 (1973), 225â€“231.
[20] Ioannis Katakis, Grigorios Tsoumakas, and Ioannis Vlahavas. 2008. Multilabel
text classification for automated tag suggestion. In Proceedings of the ECML/PKDD,Vol. 18. Citeseer, 5.
[21] Samir Khuller, Stephen G Mitchell, and Vijay V Vazirani. 1994. On-line algorithms
for weighted bipartite matching and stable marriages. Theoretical Computer
Science 127, 2 (1994), 255â€“267.
[22] Jon Kleinberg and Manish Raghavan. 2018. Selection Problems in the Presence
of Implicit Bias. arXiv:1801.03533 [cs, stat] http://arxiv.org/abs/1801.03533
[23] Michel Minoux. 1978. Accelerated greedy algorithms for maximizing submodular
set functions. In Optimization techniques. Springer, 234â€“243.
[24] Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Von-
drÃ¡k, and Andreas Krause. 2015. Lazier than lazy greedy. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 29.
[25] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. 1978. An analysis
of approximations for maximizing submodular set functionsâ€”I. Mathematical
programming 14, 1 (1978), 265â€“294.
[26] John Pestian, Chris Brew, Pawel Matykiewicz, Dj J Hovermale, Neil Johnson,
K Bretonnel Cohen, and Wlodzislaw Duch. 2007. A shared task involving multi-
label classification of clinical free text. In Biological, translational, and clinical
language processing. 97â€“104.
[27] John Platt et al .1999. Probabilistic outputs for support vector machines and com-
parisons to regularized likelihood methods. Advances in large margin classifiers
10, 3 (1999), 61â€“74.
[28] Filip Radlinski, Paul N. Bennett, Ben Carterette, and Thorsten Joachims. 2009.
Redundancy, Diversity and Interdependent Document Relevance. SIGIR Forum
43, 2 (dec 2009), 46â€“52. https://doi.org/10.1145/1670564.1670572
[29] Leonard Ramist, Charles Lewis, and Laura McCamley-Jenkins. 1994. Student
Group Differences in Predicting College Grades: Sex, Language, and Ethnic
Groups. ETS Research Report Series 1994 (1994), 41.
[30] Stephen E Robertson. 1977. The probability ranking principle in IR. Journal of
documentation 33, 4 (1977), 294â€“304.
[31] Gerard Salton (Ed.). 1971. The SMART Retrieval System: Experiments in Automatic
Document Processing. Prentice-Hall, Englewood Cliffs, NJ.
[32] Rodrygo Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting query
reformulations for web search result diversification. In Proceedings of the 19th
International Conference on World Wide Web, WWW â€™10. 881â€“890. https://doi.
org/10.1145/1772690.1772780
[33] A. Singh, D. Kempe, and T. Joachims. 2021. Fairness in Ranking under Uncertainty.
InNeural Information Processing Systems (NeurIPS).
[34] Cees GM Snoek, Marcel Worring, Jan C Van Gemert, Jan-Mark Geusebroek, and
Arnold WM Smeulders. 2006. The challenge problem for automated detection of
101 semantic concepts in multimedia. In Proceedings of the 14th ACM international
conference on Multimedia. 421â€“430.
[35] Ashok N Srivastava and Brett Zane-Ulman. 2005. Discovering recurring anom-
alies in text reports regarding complex space systems. In 2005 IEEE aerospace
conference. IEEE, 3853â€“3862.
[36] Zhan Su, Zhicheng Dou, Yutao Zhu, Xubo Qin, and Ji-Rong Wen. 2021. Modeling
Intent Graph for Search Result Diversification. In Proceedings of the 44th Interna-
tional ACM SIGIR Conference on Research and Development in Information Retrieval
(Virtual Event, Canada) (SIGIR â€™21). Association for Computing Machinery, New
York, NY, USA, 736â€“746. https://doi.org/10.1145/3404835.3462872
[37] P. SzymaÅ„ski and T. Kajdanowicz. 2017. A scikit-based Python environment for
performing multi-label classification. arXiv:1702.01460 [cs.LG]
[38] Joseph Thekinen and Jitesh H Panchal. 2017. Resource allocation in cloud-
based design and manufacturing: A mechanism design approach. Journal of
Manufacturing Systems 43 (2017), 327â€“338.
[39] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2008. Effective and
efficient multilabel classification in domains with large number of labels. In Proc.
ECML/PKDD 2008 Workshop on Mining Multidimensional Data (MMDâ€™08), Vol. 21.
53â€“59.
[40] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2009. Mining multi-
label data. Data mining and knowledge discovery handbook (2009), 667â€“685.
[41] Lequn Wang, Thorsten Joachims, and Manuel Gomez Rodriguez. 2022. Improving
Screening Processes via Calibrated Subset Selection. arXiv:2202.01147 [cs, stat]
http://arxiv.org/abs/2202.01147
[42] Yisong Yue and T. Joachims. 2008. Predicting Diverse Subsets Using Structural
SVMs. In International Conference on Machine Learning (ICML). 271â€“278.
[43] ChengXiang Zhai, William W Cohen, and John Lafferty. 2015. Beyond indepen-
dent relevance: methods and evaluation metrics for subtopic retrieval. In Acm
sigir forum, Vol. 49. ACM New York, NY, USA, 2â€“9.
 
965Ranking with Slot Constraints KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
In Section A we will provide a proof of Theorem 3.1. In Sec-
tion B, we report the selected labels in our real-world benchmark
experiments.
A PROOF OF THEOREM 3.1
The first step in proving Theorem 3.1 is to prove that the maximum
bipartite matching is monotone submodular.
Lemma A.1. For any relevance matrix ğ‘…and any set of slotsS, the
size of the maximum bipartite matching is monotone in ğ‘‹
âˆ€ğ‘‹âŠ†C,âˆ€ğ¶âˆˆC:MBM(ğ‘‹âˆª{ğ¶},S|ğ‘…)â‰¥MBM(ğ‘‹,S|ğ‘…) (6)
and also submodular in ğ‘‹, which means that âˆ€ğ‘‹âŠ†C,âˆ€ğ¶,ğ¶â€²âˆˆC
MBM(ğ‘‹âˆª{ğ¶},S|ğ‘…)âˆ’MBM(ğ‘‹,S|ğ‘…)
â‰¥MBM(ğ‘‹âˆª{ğ¶,ğ¶â€²},S|ğ‘…)âˆ’MBM(ğ‘‹âˆª{ğ¶â€²},S|ğ‘…).
Proof. By a matchingMfromğ‘‹toS, we refer to a set of
candidate-slot pairs MâŠ‚ğ‘‹Ã—S such that all pairs are relevant
(ğ‘…ğ‘ğ‘ =1for all(ğ‘,ğ‘ )âˆˆM ) and no candidate or slot appears in more
than one pair inM. We call a matching Mfromğ‘‹toSmaximum
if|M| is maximized over all matchings from ğ‘‹toS.
For monotonicity, we know that every single matching from ğ‘‹
toSis also a possible matching from ğ‘‹âˆª{ğ¶}toS, thus the size
of the maximum matching from ğ‘‹âˆª{ğ¶}toSmust be at least the
size of the maximum matching from ğ‘‹toS. So Equation 6 holds.
For submodularity, if we denote
ğ‘€0=MBM(ğ‘‹,S|ğ‘…) (7)
ğ‘€1=MBM(ğ‘‹âˆª{ğ¶},S|ğ‘…) (8)
ğ‘€2=MBM(ğ‘‹âˆª{ğ¶â€²},S|ğ‘…) (9)
ğ‘€12=MBM(ğ‘‹âˆª{ğ¶,ğ¶â€²},S|ğ‘…) (10)
We want to show that
ğ‘€1âˆ’ğ‘€0â‰¥ğ‘€12âˆ’ğ‘€2. (11)
Let us denote ğ‘€1âˆ’ğ‘€0as the LHS and ğ‘€12âˆ’ğ‘€2as the RHS
for Equation (11). Equation (6)says both the LHS and RHS are
nonnegative. Additionally, the LHS is upper bounded by 1, as given
any matchingMof sizeğ‘€1fromğ‘‹âˆª{ğ¶}toS, we can construct
a matchingMâ€²fromğ‘‹toSof size at least ğ‘€1âˆ’1by removing
the unique pair involving ğ¶fromM, if it exists, which shows
ğ‘€0â‰¥|Mâ€²|â‰¥ğ‘€1âˆ’1. By the same reasoning, the RHS is upper
bounded by 1, and indeed we also have 0â‰¤ğ‘€2âˆ’ğ‘€0â‰¤1. Then
since both the LHS and RHS can only take on values 0or1, in order
to prove equation (11)it suffices to show that if the RHS equals 1,
then the LHS equals 1.
Assume the RHS equals 1. Then any maximum matching M
fromğ‘‹âˆª{ğ¶,ğ¶â€²}toSmust include a pair (ğ¶,ğ‘†)for someğ‘†âˆˆS, or
elseMwould also be a valid matching from ğ‘‹âˆª{ğ¶â€²}toS, which
would imply that ğ‘€2â‰¥|M| =ğ‘€12, violating the assumption. We
split on two cases, which as we noted earlier are exhaustive.
Case :ğ‘€2âˆ’ğ‘€0=0. LetM0be some maximum matchings from
ğ‘‹toS. Sinceğ‘€2=ğ‘€0by assumption,M0is also a maximum
matching from ğ‘‹âˆª{ğ¶â€²}toS. Indeed,M0is also a matching
fromğ‘‹âˆª{ğ¶,ğ¶â€²}toS, but not a maximum matching, as ğ‘€12>
ğ‘€2by assumption. Therefore, by Bergeâ€™s theorem, there must
exist an â€œaugmenting pathâ€ ğ‘ƒconsisting of candidate-slot pairs(ğ‘1,ğ‘ 1),...,(ğ‘ğ‘‡,ğ‘ ğ‘‡) âˆˆ (ğ‘‹âˆª{ğ¶,ğ¶â€²})Ã—S for allğ‘¡such that all
pairs are relevant, i.e. ğ‘…ğ‘ğ‘¡,ğ‘ ğ‘¡=1for allğ‘¡, and the path starts and
ends on unmatched edges and alternates between matched and
unmatched edges, i.e. (ğ‘ğ‘¡,ğ‘ ğ‘¡)âˆ‰M0for allğ‘¡and(ğ‘ğ‘¡,ğ‘ ğ‘¡âˆ’1)âˆˆM 0for
allğ‘¡âˆˆ{2,...,ğ‘‡}. But then since ğ‘ƒcontains one more unmatched
than matched edge, â€œapplyingâ€ ğ‘ƒtoM0via the set difference op-
eration gives a maximum matching from ğ‘‹âˆª{ğ¶,ğ¶â€²}toS, since
M:=(M0\ğ‘ƒ)âˆª(ğ‘ƒ\M 0)satisfies|M|=ğ‘€0+1=ğ‘€2+1=ğ‘€12.
But sinceMis a maximum matching from ğ‘‹âˆª{ğ¶,ğ¶â€²}toS, we
must have thatMcontains candidate ğ¶as shown earlier, thus it
cannot contain ğ¶â€²by the alternating edges property of ğ‘ƒas bothğ¶
andğ¶â€²are unmatched inM0. But thenMis a valid matching from
ğ‘‹âˆª{ğ¶}toS, henceğ‘€1â‰¥|M| =ğ‘€12=ğ‘€0+1as desired.
Case :ğ‘€2âˆ’ğ‘€0=1. LetMbe a maximum matching from ğ‘‹âˆª
{ğ¶,ğ¶â€²}toS; then we must have |M|=ğ‘€12=ğ‘€0+2by assumption.
If candidate ğ¶â€²does not appear in M, thenMis a valid matching
fromğ‘‹âˆª{ğ¶}toS, soğ‘€1â‰¥|M| =ğ‘€2+1â‰¥ğ‘€0+1and we are done.
Otherwise, assume Mincludes a pair(ğ¶â€²,ğ‘†â€²)for someğ‘†â€²âˆˆS. But
thenMâ€²â€²:=M\{(ğ¶â€²,ğ‘†â€²)}is a valid matching from ğ‘‹âˆª{ğ¶}toS
with|Mâ€²â€²|=ğ‘€12âˆ’1=ğ‘€0+1, henceğ‘€1â‰¥ğ‘€0+1as desired.
â–¡
The monotone submodularity of MBM(ğ‘‹,S|ğ‘…)implies that our
estimate Ë†ğ‘€(ğ‘‹)is also monotone submodular, since this property is
closed under addition.
Lemma A.2. For any sample of relevance matrices R=[ğ‘…1,...,ğ‘…ğ‘›]
and set of slotsS,Ë†ğ‘€(ğ‘‹)from Equation (4)is monotone in ğ‘‹
âˆ€ğ‘‹âŠ†C,âˆ€ğ¶âˆˆC :Ë†ğ‘€(ğ‘‹âˆª{ğ¶})â‰¥ Ë†ğ‘€(ğ‘‹)
and also submodular in ğ‘‹, which means that âˆ€ğ‘‹âŠ†C,âˆ€ğ¶,ğ¶â€²âˆˆC
Ë†ğ‘€(ğ‘‹âˆª{ğ¶})âˆ’ Ë†ğ‘€(ğ‘‹)â‰¥ Ë†ğ‘€(ğ‘‹âˆª{ğ¶,ğ¶â€²})âˆ’ Ë†ğ‘€(ğ‘‹âˆª{ğ¶â€²}).
Proof. We know from Lemma A.1 that each each MBM(ğ‘‹,S|ğ‘…ğ‘–)
is monotone and submodular. This means that Ë†ğ‘€(ğ‘‹)is a sum of
monotone submodular function, and it is well known that sum is
monotone submodular as well. â–¡
We are now in a position to complete the proof of Theorem 3.1.
Proof of Theorem 3.1. Our goal is to bound with high proba-
bility the suboptimality ğ‘€(ğ‘‹ğ‘˜)âˆ’ğ‘€(ğ‘‹âˆ—
ğ‘˜)between the candidate set
ğ‘‹ğ‘˜output by the model and the optimal candidate set ğ‘‹âˆ—
ğ‘˜.
First, note that the approximation guarantee of the greedy algo-
rithm for monotone submodular maximization with a cardinality
constraint [25] guarantees
Ë†ğ‘€(ğ‘‹ğ‘˜)â‰¥( 1âˆ’1/ğ‘’)Ë†ğ‘€(Ë†ğ‘‹âˆ—
ğ‘˜)â‰¥( 1âˆ’1/ğ‘’)Ë†ğ‘€(ğ‘‹âˆ—
ğ‘˜), (12)
where Ë†ğ‘‹âˆ—
ğ‘˜=argmax|ğ‘‹|=ğ‘˜Ë†ğ‘€(ğ‘‹)is the optimal set on the Monte-
Carlo samples. The second inequality follows from Ë†ğ‘€(ğ‘‹âˆ—
ğ‘˜)â‰¤ Ë†ğ‘€(Ë†ğ‘‹âˆ—
ğ‘˜).
To get a bound in terms of ğ‘€(.)instead of Ë†ğ‘€(.), we need to
bound the error due to Monte-Carlo sampling. We start with the
following equivalent expansion of Equation (12).
ğ‘€(ğ‘‹ğ‘˜)â‰¥
1âˆ’1
ğ‘’
ğ‘€(ğ‘‹âˆ—
ğ‘˜)
Ë†ğ‘€(ğ‘‹âˆ—
ğ‘˜)âˆ’ğ‘€(ğ‘‹âˆ—
ğ‘˜)
+
ğ‘€(ğ‘‹ğ‘˜)âˆ’Ë†ğ‘€(ğ‘‹ğ‘˜)
First, we upper bound Ë†ğ‘€(ğ‘‹âˆ—
ğ‘˜)âˆ’ğ‘€(ğ‘‹âˆ—
ğ‘˜), for which we can use
Hoeffdingâ€™s inequality since for any ğ‘‹it holds that ğ¸ğ‘…[Ë†ğ‘€(ğ‘‹)]=
 
966KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wentao Guoâˆ—, Andrew Wangâˆ—, Bradon Thymes, and Thorsten Joachims
Table 4: Selected Labels in Real-world Benchmark Experi-
ments.
Dataset
namesSelected
label
indicesSelected label names
Medical 0, 23,
41, 44,
32, 24,
31, 9,
4, 31Class-0-593_70, Class-23-786_50,
Class-41-591, Class-44-786_07,
Class-32-486, Class-24-596_54,
Class-31-780_6, Class-9-599_0,
Class-4-753_0, Class-31-780_6
Bibtex 44, 134,
63, 10,
14, 104,
131, 52,
117, 83TAG_electrochemistry,
TAG_statphys23,
TAG_immunoassay, TAG_apob,
TAG_bibteximport, TAG_ontology,
TAG_software, TAG_evolution,
TAG_requirements,
TAG_mathematics
Delicious 924, 452,
809, 99,
941, 733,
540, 897,
946, 700TAG_video, TAG_howto,
TAG_software, TAG_blog,
TAG_web, TAG_reference,
TAG_linux, TAG_tutorial,
TAG_webdesign,
TAG_programming
TMC2007 13, 7,
21, 5,
18, 4,
1, 12,
11, 17class14, class08,
class22, class06,
class19, class05,
class02, class13,
class12, class18
Mediamill 78, 24,
84, 94,
65, 96,
51, 2,
67, 66Class79, Class25,
Class85, Class95,
Class66, Class97,
Class52, Class3,
Class68, Class67
Bookmarks 20, 163,
151, 144,
145, 109,
57, 89,
92, 87TAG_books, TAG_shipyard,
TAG_rssfeedek, TAG_recept,
TAG_recipe, TAG_medical,
TAG_firefox, TAG_journal,
TAG_kultur, TAG_java
ğ‘€(ğ‘‹)and theğ‘€ğµğ‘€(ğ‘‹,S|ğ‘…ğ‘–)(ğ‘–âˆˆ{1,...,ğ‘›}) are i.i.d. Monte-Carlosamples with 0â‰¤ğ‘€ğµğ‘€(ğ‘‹,S|ğ‘…ğ‘–)â‰¤ğ‘ .
ğ‘ƒ(ğ‘€(ğ‘‹)âˆ’ Ë†ğ‘€(ğ‘‹)>ğœ–)â‰¤exp(âˆ’2ğ‘›ğœ–2/ğ‘ 2).
We thus get for our particular ğ‘‹âˆ—
ğ‘˜that with probability 0â‰¤ğ›¿1â‰¤1/2
Ë†ğ‘€(ğ‘‹âˆ—
ğ‘˜)âˆ’ğ‘€(ğ‘‹âˆ—
ğ‘˜)â‰¤ğ‘ âˆšï¸‚
ln(1/ğ›¿1)
2ğ‘›:=ğœ–1.
Second, we need to upper bound ğ‘€(ğ‘‹ğ‘˜)âˆ’ Ë†ğ‘€(ğ‘‹ğ‘˜). Sinceğ‘‹ğ‘˜is
selected on the same Monte-Carlo sample we evaluate it on, we
need to ensure uniform convergence over all ğ‘‹. We thus take the
union bound over the set Hğ‘˜of all possible candidate sets of size ğ‘˜:
ğ‘ƒ(max
ğ‘‹(ğ‘€(ğ‘‹)âˆ’ Ë†ğ‘€(ğ‘‹))>ğœ–)â‰¤|Hğ‘˜|exp(âˆ’2ğ‘›ğœ–2/ğ‘ 2)
â‰¤ 
âˆš
2ğœ‹ğ‘˜ğ‘˜
ğ‘’ğ‘˜
ğ‘’1
12ğ‘˜!
expâˆ’2ğ‘›ğœ–2
ğ‘ 2
The second step uses Stirlingâ€™s inequality, since |Hğ‘˜|=ğ‘˜!. By
letting the final expression equal ğ›¿2and solving for ğœ–, we get that
with probability 0â‰¤ğ›¿2â‰¤1/2it holds for all ğ‘‹(and therefore also
for anyğ‘‹ğ‘˜our algorithm picks) that
ğ‘€(ğ‘‹ğ‘˜)âˆ’ Ë†ğ‘€(ğ‘‹ğ‘˜)â‰¤ğ‘ âˆšï¸ƒ
(ğ‘˜lnğ‘˜âˆ’ğ‘˜+ğ‘‚(lnğ‘˜))+ln(1/ğ›¿2)
2ğ‘›:=ğœ–2.
Putting these bounds together, we get that for all 0<ğ›¿1,ğ›¿2<1/2,
we have with probability 1âˆ’ğ›¿1âˆ’ğ›¿2,
ğ‘€(ğ‘‹ğ‘˜)â‰¥
1âˆ’1
ğ‘’
(ğ‘€(ğ‘‹âˆ—
ğ‘˜)âˆ’ğœ–1)âˆ’ğœ–2
â‰¥
1âˆ’1
ğ‘’
ğ‘€(ğ‘‹âˆ—
ğ‘˜)âˆ’(ğœ–1+ğœ–2).
Settingğ›¿1=ğ›¿2=ğ›¿/2gives the claimed bound.
â–¡
B SELECTED LABELS IN REAL-WORLD
BENCHMARK EXPERIMENTS
We report the selected labels for our real-world benchmark experi-
ments in Table 4. We pick these labels with a consideration of both
prediction precision (these labels are not hard to predict by logistic
regression models) and sufficient competition among candidates
(the positive occurrences of these labels should be larger than the
number of slots). We do not change the selected labels when we
change the number of slots per label in Table 2.
 
967