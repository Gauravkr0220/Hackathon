On the Convergence of Zeroth-Order Federated Tuning for
Large Language Models
Zhenqing Ling
Sun Yat-sen University
Shenzhen, Guangdong, China
lingzhq@mail2.sysu.edu.cnDaoyuan Chen
Alibaba Group
Hangzhou, Zhejiang, China
daoyuanchen.cdy@alibaba-inc.comLiuyi Yao
Alibaba Group
Hangzhou, Zhejiang, China
yly287738@alibaba-inc.com
Yaliang Li
Alibaba Group
Bellevue, Washington, United States
yaliang.li@alibaba-inc.comYing Shenâˆ—
Sun Yat-sen University
Shenzhen, Guangdong, China
Pazhou Lab
Guangzhou, Guangdong, China
Guangdong Provincial Key
Laboratory of Fire Science and
Intelligent Emergency Technology
Guangzhou, Guangdong, China
sheny76@mail.sysu.edu.cn
ABSTRACT
The confluence of Federated Learning (FL) and Large Language
Models (LLMs) is ushering in a new era in privacy-preserving nat-
ural language processing. However, the intensive memory require-
ments for fine-tuning LLMs pose significant challenges, especially
when deploying on clients with limited computational resources.
To circumvent this, we explore the novel integration of Memory-
efficient Zeroth-Order Optimization within a federated setting, a
synergy we term as FedMeZO. Our study is the first to examine
the theoretical underpinnings of FedMeZO in the context of LLMs,
tackling key questions regarding the influence of large parameter
spaces on optimization behavior, the establishment of convergence
properties, and the identification of critical parameters for conver-
gence to inform personalized federated strategies. Our extensive
empirical evidence supports the theory, showing that FedMeZO not
only converges faster than traditional first-order methods such as
FedAvg but also significantly reduces GPU memory usage during
training to levels comparable to those during inference. Moreover,
the proposed personalized FL strategy that is built upon the theoreti-
cal insights to customize the client-wise learning rate can effectively
accelerate loss reduction. We hope our work can help to bridge
theoretical and practical aspects of federated fine-tuning for LLMs,
thereby stimulating further advancements and research in this area.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671865CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning; â€¢Mathe-
matics of computing â†’Mathematical optimization.
KEYWORDS
Federated Learning, Zeroth-order Optimization, Convergence Anal-
ysis, Large Language Models
ACM Reference Format:
Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, and Ying Shen. 2024.
On the Convergence of Zeroth-Order Federated Tuning for Large Language
Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671865
1 INTRODUCTION
Federated Learning (FL) has become an important approach in
modern machine learning, particularly in scenarios where data
decentralization and privacy-preserving are crucial [ 28,42,43,56,
58]. Central to this learning paradigm is the corroborative training
of a global model through the aggregation of updates from multiple
clients, without sharing their raw data [31, 41].
In parallel, Large Language Models (LLMs) have radically ad-
vanced the field of natural language processing [ 6,17,52]. The
fine-tuning of LLMs that are already pre-trained on vast corpora,
has proven to be a highly effective strategy for numerous tasks,
yielding models that are both versatile and capable of adapting to
specific domain narratives or aligning with human values [6, 39].
The tuning of LLMs requires suitable alignment data, which are
often costly to acquire [ 11,16]. Due to the abundance of private data
that remains largely isolated and underutilized, the intersection of
FL and LLMs has sparked increasing interest among researchers
[20,32,46,61]. Notably, this integration presents significant com-
putational challenges, especially for clients with limited resources
[10,62]. The scaling up of LLMs further compounds this issue, as
 
1827
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, & Ying Shen
the computation of gradients for backpropagation incurs substan-
tial memory costs, frequently surpassing the practical capabilities
of these clients [39].
Addressing this challenge, we turn our attention to Zeroth-Order
Optimization (ZOO), an algorithm that computes gradient approxi-
mations without explicit gradient information, thus significantly
reducing memory consumption [ 25]. However, the combination of
ZOO and FL â€”a research direction we refer to as ZOO-FL â€”remains
unexplored in the literature in the context of LLMs [ 47]. Our work
intends to bridge this gap by harnessing the memory efficiency of
ZOO within the context of federated fine-tuning of LLMs, especially
on the following theoretical foundations:
(Q1) How does the vast parameter space of LLMs influence
the behavior of ZOO-FL? (Q2) Can we establish the convergence
properties of ZOO-FL for LLMs? (Q3) Which model parameters are
critical for convergence, and how can we leverage them to optimize
FL performance, such as via personalization?
In this paper, we focus on incorporating a memory-efficient ZOO
method, MeZO [ 39] into FL, a synergy we denote as FedMeZO,
and establishing its convergence properties under the large-scale
parameter space of LLMs. We analyze and present precise conver-
gence rates characterized by the low effective rank ğ‘Ÿof the modelsâ€™
Hessian matrices [ 3,33], and other typical FL parameters such as
number of clients ğ‘, number of FL rounds ğ‘‡, iteration steps of local
trainingğ»and heterogeneity constants ğ‘â„andğœâ„. Moreover, we
reveal the learning rate to be a crucial variable for convergence.
Building on theoretical insights, we further propose a strategy that
tailors the learning rate to each clientâ€™s specific data characteristics.
To validate our theoretical results, we conduct extensive exper-
iments on real-world FL datasets for LLM tuning, which cover
diverse data distributions and application tasks. Our empirical find-
ings corroborate the theoretical analysis, validating effective conver-
gence even when scaling up to models with billions of parameters.
Compared with first-order methods such as FedAvg, FedMeZO
converges faster meanwhile remarkably reducing GPU memory
requirements. The personalized strategies guided by our theoretical
insights, empirically show a more rapid loss reduction, as opposed
to non-personalized or random learning rate assignments.
In summary, our theoretical and empirical exploration validates
FedMeZO in the fine-tuning process of LLMs, providing a rigorous
framework and practical insights for future applications. Our key
contributions are threefold:
â€¢We advance the understanding of FedMeZO for LLMs, extending
the two-point gradient estimation to federated tuning and es-
tablishing theoretical convergence rate O
ğ‘Ÿ3/2(ğ‘ğ»ğ‘‡)âˆ’1/2
and
O
ğ‘Ÿ3/2(eğ‘â„ğ‘ğ»ğ‘‡)âˆ’1/2
âˆ’O
ğœ2
â„(ğ‘â„ğ‘)âˆ’1
for the i.i.d. setting and
non-i.i.d. setting, respectively.
â€¢We analyze the impact of various hyperparameters of FedMeZO
and explore a theory-informed strategy for personalized learning
rate adjustment, providing practical guidance for ZOO-FL.
â€¢Through extensive empirical evidence with LLMs, we verify the
proposed theoretical results and show that FedMeZO yields ef-
fective convergence with substantially reduced memory over-
head compared to FedAvg. Our codes are publicly available at
https://github.com/alibaba/FederatedScope/tree/FedMeZO.2 PRELIMINARIES
2.1 Background and Related Works
Federated Fine-Tuning of Large Language Models. Large Lan-
guage Models (LLMs) have demonstrated remarkable capabilities
that enable a variety of real-world applications [ 52,64,65]. The fed-
erated fine-tuning of LLMs has recently attracted attention, focused
on adapting these models to domain-specific tasks while preserv-
ing the privacy of the training data. Chen et al. [ 8] investigated
the integration of LLMs within federated settings, highlighting the
inherent challenges and potential opportunities. Zhang et al. [ 62]
furthered this research by examining instruction tuning of LLMs
in a federated context, marking progress in applying FL to the spe-
cialized training of LLMs. Notable frameworks such as FATE-LLM
by Fan et al. [ 21] and FederatedScope-LLM by Kuang et al. [ 32]
offer industrial-grade and comprehensive solutions for federated
fine-tuning. Our work, in contrast, investigates the fusion of Zeroth-
Order Optimization (ZOO) with FL for the fine-tuning of LLMs, an
area that has yet to be fully investigated, thereby addressing a gap
in the literature and providing fundamental theoretical insights.
Zeroth-Order Optimization in Federated Learning. ZOO has
emerged as a viable method to address the difficulties of computing
gradients in FL, especially in settings limited by computational
resources. Zhang et al. [ 63] proposed a ZOO algorithm tailored for
vertical FL, focusing on privacy preservation. Yi et al. [ 60] and Li
et al. [ 34] studied ZOO-FL algorithms, with discussions on conver-
gence properties with single-point perturbation and local updates
in decentralized FL, respectively. The convergence analysis is a
critical aspect of FL, as illustrated by Li et al. [ 37] for the FedAvg
algorithm and further developed by Fang et al. [ 22] for mini-batch
stochastic ZOO-FL in wireless networks. Moreover, Shu et al. [ 50]
proposed enhancements to query efficiency for ZOO within the FL
framework. Our research sets itself apart by formulating theoret-
ical convergence bounds for ZOO-FL, specifically tailored to the
large-scale parameter space of LLMs. This builds on the preliminary
work by Malladi et al. [ 39], which confirmed the feasibility of ZOO
for LLMs in a centralized setting.
2.2 Problem Formulation
Federated Learning. We consider the general FL setting as of
FedAvg [ 41], with a central server and a collection of ğ‘clients,
indexed by 1,2,...,ğ‘ . The central server coordinates the training of a
global model through the collaborative efforts of these clients, each
holding local data samples drawn from their respective distributions
Dğ‘–. The optimization problem can be formulated as:
min
ğœƒâˆˆRğ‘‘ğ‘“(ğœƒ)â–³=1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ‘“ğ‘–(ğœƒğ‘–), ğ‘“ğ‘–(ğœƒ)â–³=EBğ‘–âˆ¼Dğ‘–
ğ¹ğ‘–(ğœƒ,Bğ‘–)
,(1)
whereğœƒâˆˆRğ‘‘denotes the ğ‘‘-dimension parameter of the model,
andğ‘“(ğœƒ)andğ‘“ğ‘–(ğœƒ)denote the global loss function on the central
server and local loss function on ğ‘–ğ‘¡â„client, respectively. Typically,
the clients are assumed with equal importance [ 54], and the data is
randomly sampled for efficiency [ 35].ğ¹ğ‘–(ğœƒ,Bğ‘–)represents the local
loss function w.r.t a specific mini-batch Bğ‘–drawn fromDğ‘–.
 
1828On the Convergence of Zeroth-Order Federated Tuning for Large Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Zeroth-Order Optimization. Zeroth-order optimization (ZOO)
is a prominent technique in scenarios where gradients are difficult to
obtain, which estimates gradients by forward propagations. Given
a random vector ğ‘§and a smoothing constant ğœ‡, a typical one-point
gradient estimator [18] is defined as:
eâˆ‡ğ¹(ğœƒ,ğ‘§,B,ğœ‡)=ğ‘§
2ğœ‡ ğ¹(ğœƒ+ğœ‡ğ‘§,B)âˆ’ğ¹(ğœƒ,B), (2)
However, Eq. (2)provides a biased gradient estimation, leading to
a certain degree of information loss [ 38]. Hence our work employs
the ZOO paradigm with a two-point gradient estimator proposed
by [39] in a federated setting:
Definition 2.1. (Two-point gradient estimator) Given a set of
parametersğœƒâˆˆRğ‘‘for an LLM and a mini-batch Bğ‘–, the two-point
zeroth-order gradient estimator is formulated as:
eâˆ‡ğ¹ğ‘–(ğœƒ,ğ‘§ğ‘–,Bğ‘–,ğœ‡)=ğ‘§ğ‘–
2ğœ‡ ğ¹ğ‘–(ğœƒ+ğœ‡ğ‘§ğ‘–,Bğ‘–)âˆ’ğ¹ğ‘–(ğœƒâˆ’ğœ‡ğ‘§ğ‘–,Bğ‘–),(3)
whereğ‘§ğ‘–âˆ¼N( 0,ğ¼ğ‘‘)is a Gaussian random variable and ğœ‡is the
perturbation scale. The two-point gradient estimator in Eq. (3)
requires only two forward passes through the model to compute
the estimation of gradient, which serves as a memory-efficient
alternative to backpropagation (BP).
The FedMeZO Algorithm. In this paper, we study and ana-
lyze the proprieties of a practical synergy of MeZO [ 39] and Fe-
dAvg [ 41], which is designed to fine-tune LLMs in an efficient,
privacy-preserving and personalized manner. We term this ZOO-FL
approach as FedMeZO, depicted with the following processes:
In a single communication round, the central server first broad-
casts the global model parameters to available clients. Once the
clients have completed their local updates and uploaded their mod-
els, the server aggregates the updates according to Eq. (1), forming
the basis for the subsequent round.
Upon receiving the global model parameters, clients perform the
following steps, distinguishing FedMeZO from traditional BP-based
FedAvg algorithms in two-fold:
(1) Training Memory Reduction: Clients update their models using
the two-point ZOO gradient estimator defined in Eq. (3) as:
ğ‘’(ğ‘¡,ğ‘˜)
ğ‘–=eâˆ‡ğ¹ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–,ğ‘§(ğ‘¡,ğ‘˜)
ğ‘–,B(ğ‘¡,ğ‘˜)
ğ‘–,ğœ‡), (4)
where(ğ‘¡,ğ‘˜)denotes theğ‘˜ğ‘¡â„iteration within the ğ‘¡ğ‘¡â„communication
round. Unlike standard ZO-SGD algorithms that require storing
the perturbation vector ğ‘§at each iteration, FedMeZO resamples
ğ‘§using random seeds in in-place implementation, thus reducing
memory usage to a level equivalent to inference [39].
(2) Communication Cost Reduction: To mitigate the high commu-
nication overhead associated with LLMs, FedMeZO leverages Low-
Rank Adaptation (LoRA) [ 4,29], which introduces reparametriza-
tion to tune two small delta matrix on the linear layers instead of
the whole LLM weights, based on the assumption that well pre-
trained LLM possess a low â€œintrinsic dimensionâ€ when adapted to
new tasks. Introducing it can help us further reduce the number
of parameters to be updated and uploaded, thereby aligning with
the practical constraints of federated settings. Detailed analysis of
communication cost is available in Appendix C.1.2.3 Lemmas and Assumptions
Lemma 2.2. (Unbiased Gradient Estimator) The two-point zeroth-
order gradient estimator described in Eq. (3)is an unbiased estimator
of the true gradient, that is,
E[eâˆ‡ğ¹ğ‘–(ğœƒ,ğ‘§ğ‘–,Bğ‘–,ğœ‡)]=âˆ‡ğ‘“ğ‘–(ğœƒ). (5)
The Hessian matrix, which is the square matrix of second-order
partial derivatives of the loss w.r.t the model parameters, character-
izes the curvature of the loss surface [ 24]. Although the size of a
modelâ€™s loss Hessian is often associated with the rate of fine-tuning,
studies suggest that the large-scale parameters of LLMs do not
necessarily impede convergence [ 2,30]. This paradox is addressed
by recognizing that the loss Hessian often exhibits a small local
effective rank [ 39], which we capture in the following assumption:
Assumption 1. There exist a Hessian matrix H(ğœƒğ‘¡)satisfying:
â€¢âˆ‡2ğ‘“(ğœƒ)âª¯H(ğœƒğ‘¡)for allğœƒsuch thatâˆ¥ğœƒâˆ’ğœƒğ‘¡âˆ¥â‰¤ğœ‚ğ‘‘ğº(ğœƒğ‘¡), where
ğº(ğœƒğ‘¡)=maxBâˆ¼Dâˆ¥âˆ‡ğ‘“(ğœƒğ‘¡,B)âˆ¥.
â€¢The effective rank of H(ğœƒğ‘¡), denoted as tr(H(ğœƒğ‘¡))/âˆ¥H(ğœƒğ‘¡)âˆ¥op,
is at mostğ‘Ÿ. Here trdenotes the trace of the matrix, and âˆ¥Â·âˆ¥opdenotes
the operator norm.
Assumption 1 characterizes a low effective rank ğ‘Ÿin the Hessian
matrix, which demonstrates that LLM fine-tuning can occur in a
low dimensional subspace ( â‰¤200parameters) [ 3,33]. With this
insight, [ 39] identified the bound of loss descent at each step of
centralized ZOO, which is partially influenced by ğ‘Ÿ:
Lemma 2.3. (Bounded Centralized Descent) Assumeğ‘“(ğœƒ)isğ¿-
smooth and let eâˆ‡ğ¹(ğœƒ,ğ‘§,B,ğœ‡)be the unbiased zeroth-order gradient
estimator from Eq. (3). If the Hessian matrix H(ğœƒ)exhibits a local
effective rank of ğ‘Ÿ, and constants ğ›¾=Î˜(ğ‘Ÿ/ğ‘›)andğœ=Î˜(1/ğ‘Ÿğ‘‘)exist,
then the expected decrease in loss can be bounded as follows:
E
ğ‘“(ğœƒğ‘¡+1)
â‰¤ğ‘“(ğœƒğ‘¡)âˆ’ğœ‚
ğ›¾âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡)âˆ¥2
+ğœ‚2ğ¿ğœ
2E
âˆ¥eâˆ‡ğ¹(ğœƒ,ğ‘§,B,ğœ‡)âˆ¥2
, (6)
whereğ›¾=ğ‘‘ğ‘Ÿ+ğ‘‘âˆ’2
ğ‘›(ğ‘‘+2),ğœ=(ğ‘‘+2)ğ‘›2
(ğ‘‘ğ‘Ÿ+ğ‘‘âˆ’2)(ğ‘‘+ğ‘›âˆ’1), andğ‘›is the number of
randomizations.
From Eq. (6), we observe that the rate of descent at a single step
depends on the gradient related to ğ›¾and the gradient estimation
related toğœ. Following [ 39], we setğ‘›to 1 in this paper. Detailed
proofs of Lemmas are available in Appendix C.1 and C.2 of our
extended version [1].
Besides, to facilitate the analysis in FL setting, we introduce four
assumptions, including Bounded Loss (Assumption 2), ğ¿-smoothness
(Assumption 3), mini-batch gradient error bound (Assumption 4),
global-local disparities in i.i.d. andnon-i.i.d. settings (Assumptions 5
and 6 respectively). These assumptions are standard in optimization
and FL literature [5, 36, 37, 55], which we detail in Appendix A.
3 MAIN RESULTS
3.1 Convergence Analysis in i.i.d. Case
In this subsection, we examine the convergence properties of Fed-
MeZO within the i.i.d. data distribution setting. We establish the
conditions under which the algorithm guarantees loss reduction at
each iteration and provide a global convergence rate.
 
1829KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, & Ying Shen
Theorem 3.1. (Stepwise Loss Descent in i.i.d. Setting) Under
Assumptions 1-5 and with a learning rate ğœ‚satisfying
ğœ‚â‰¤minn1
3ğ»ğ¿âˆšï¸ğ‘ğ‘”ğ‘‘,ğ‘
3ğ»ğ¿ğ‘ğ‘”,1
ğ»2o
, (7)
the expected decrease in loss at each step for FedMeZO under the i.i.d.
scenario is bounded as
Eğ‘¡
ğ‘“(ğœƒğ‘¡+1)
â‰¤ğ‘“(ğœƒğ‘¡)âˆ’2
ğ›¾âˆ’2ğœ
ğ‘‘
ğœ‚Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+2ğœ2ğ‘”ğœğœ‚ğ¿
ğ‘ğ»ğ‘‘+ğœğœ‚ğœ‡2ğ¿3
2ğ‘ğ», (8)
whereğ›¾andğœquantify the effective low-rank properties of the gradi-
ent and its estimator, respectively.
In Theorem 3.1, the term (âˆ’2ğœ‚/ğ›¾)Eğ‘¡|âˆ‡ğ‘“(ğœƒğ‘¡)|2serves as a critical
factor that drives the decrease in the loss function, as it is the sole
negative contributor in Eq. (8)such that Eğ‘¡
ğ‘“(ğœƒğ‘¡+1)âˆ’ğ‘“(ğœƒğ‘¡)
â‰¤0.
Note that the presence of the factor ğ›¾âˆ’1=Î˜(ğ‘Ÿâˆ’1)underscores
the impact of the low effective rank ğ‘Ÿon the convergence rate
(under Assumption 1), revealing that a reduction in ğ‘Ÿcan accelerate
convergence independently of the high-dimensional parameter
spaceğ‘‘. Consequently, even for LLMs with expansive parameter
spaces, FedMeZO can attain convergence. This addresses our first
foundational question â€œQ1: How does the vast parameter space of
LLMs influence the behavior of ZOO-FL? â€.
Moreover, the terms (2ğœ‚ğœ/ğ‘‘)Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2and(2ğœ2ğ‘”ğœğœ‚ğ¿/ğ‘ğ»ğ‘‘)
are scaled by ğœ/ğ‘‘,i.e., in Î˜(1/ğ‘Ÿğ‘‘2), contributing a relatively smaller
effect on the convergence speed compared to negative term. This
demonstrates that the influence on convergence speed from the
zeroth-order gradient estimation is moderated by the modelâ€™s effec-
tive low rank and dimensionality. As for the last term, (ğœğœ‚ğœ‡2ğ¿3/2ğ‘ğ»),
it acts as a factor slowing down the convergence rate, and we can
observe that when ğ‘andğ»are larger, this term becomes smaller.
This suggests that the effect of slowing down the convergence rate
is not as pronounced, and simultaneously, the perturbation step ğœ‡
should not be excessively large. Specifically, this term indicates that
increasing the number of clients and the number of local rounds
can enhance convergence, while also emphasizing the importance
of keeping the perturbation step ğœ‡moderate.
After gaining intuitive insights in each round of training through
the analysis of Theorem 3.1, it is necessary to assess the convergence
performance of FedMeZO from a global perspective. We utilize the
squared magnitude of the gradient Eğ‘¡âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡)âˆ¥2as a measure to
assess the suboptimality of each iterate. The rapidity with which
the algorithm approaches a stationary point serves as a crucial
metric for determining its efficacy in the context of non-convex
optimization problems [44].
Corollary 3.2. (Global Convergence in i.i.d. Setting) Assum-
ing the conditions of Theorem 3.1 hold, the global convergence for
FedMeZO in the i.i.d. case, characterized by Î“=ğ‘‘âˆ’ğœğ›¾
ğ‘‘ğ›¾, is given by
min
ğ‘¡âˆˆ[ğ‘‡]Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
â‰¤ğ‘“(ğœƒ0)âˆ’ğ‘“âˆ—
2ğœ‚ğ‘‡Î“+ğœ2ğ‘”ğœğ¿
ğ‘ğ»ğ‘‘Î“+ğœğœ‡2ğ¿3
4ğ‘ğ»Î“(9)
whereğ‘“âˆ—denotes the optimal loss value.The upper bound on the minimum squared gradient norm across
iterations is composed of three terms in Corollary 3.2. The first term
indicates that the distance to the optimal loss relies on the initial
state of optimality, while the second and third terms elucidate the
influences of stochastic mini-batch errors and the perturbation scale
ğœ‡inherent to ZOO, respectively. Specifically, they both reflect the
impact of the model parameters ğ‘‘and the low effective rank ğ‘Ÿon
the optimal loss. As pointed out in [ 44], by choosing an appropriate
step size, we can obtain the desired accuracy.
Given thatğ›¾=Î˜(ğ‘Ÿ)andğœ=Î˜
1
ğ‘Ÿğ‘‘
, we haveğœğ›¾=Î˜
ğ‘Ÿ
ğ‘Ÿğ‘‘
=
Î˜
1
ğ‘‘
. As the parameter ğ‘‘is large,ğ‘‘âˆ’ğœğ›¾=ğ‘‘âˆ’Î˜
1
ğ‘‘
is domi-
nated byğ‘‘. Consequently, the dominant term of Î“isğ‘‘ğ›¾
ğ‘‘âˆ’ğœğ›¾, which
simplifies to ğ›¾=Î˜(ğ‘Ÿ). Therefore, Î“=Î˜
1
ğ›¾
=Î˜
1
ğ‘Ÿ
. Building on
this relationship, we have the following corollary that articulates
the convergence rate of FedMeZO.
Corollary 3.3. (Convergence Rate in i.i.d. Setting) Assuming the
conditions of Corollary 3.2 hold and given ğœ‚=(ğ‘ğ»)1/2(ğ‘Ÿğ‘‡)âˆ’1/2and
ğœ‡=(ğ‘ğ»)1/4ğ‘Ÿâˆ’1/2, we have
min
ğ‘¡âˆˆ[ğ‘‡]Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
â‰¤O
ğ‘Ÿ3/2(ğ‘ğ»ğ‘‡)âˆ’1/2
+O
ğ‘‘âˆ’1(ğ‘Ÿğ‘ğ»)âˆ’1/2
.
(10)
The expression on the right-hand side of Eq. (10)is dominated
byO
ğ‘Ÿ3/2(ğ‘ğ»ğ‘‡)âˆ’1/2
. Consequently, we have derived the con-
vergence rate for FedMeZO. The low effective rank ğ‘Ÿsignificantly
contributes to lowering the convergence rate, which is also influ-
enced by the number of clients ğ‘, the steps of local training iteration
ğ», and the total number of communication rounds ğ‘‡. Moreover,
to satisfy the learning rate condition in Eq. (7), the values of ğ‘,ğ»,
andğ‘‡must be suitably large.
It is important to note that FedMeZO does not primarily aim to
accelerate convergence speed but rather to identify the convergence
rate under assumptions pertinent to LLMs. This is intended to
demonstrate that FedMeZO can achieve convergence even within
a vast parameter space. In a series of studies on federated ZOO,
Federated Zeroth-Order Optimization (FedZO) presents the most
comprehensive and complete analysis with a convergence rate of
Oâˆšï¸
ğ‘‘/(ğ‘ğ»ğ‘‡ğ‘ 1ğ‘2)
[22], which exhibits a lower rate compared
toO ğ‘‘3/ğ‘‡of ZONE-S [ 26] and accounts for the impact of ğ»
compared toOâˆšï¸
ğ‘‘/ğ‘ğ‘‡
of DZOPA [ 60]. In contrast to FedZO, our
method, FedMeZO, theoretically supports a faster convergence by
replacingğ‘‘1/2withğ‘Ÿ3/2and settingğ‘1=ğ‘2=1. These comparisons
show that FedMeZO addresses the challenges posed by large models,
offering an efficient convergence rate that relies on ğ‘Ÿ.
This advancement signifies progress in optimizing federated
learning algorithms, particularly for LLMs, where the scalability
of parameters and data heterogeneity are major challenges. By
emphasizing the low effective rank, our approach enhances both
the theoretical understanding of convergence behavior in complex
settings and the guidance insights into the settings of learning rates
and other parameters to achieve efficient convergence outcomes.
However, i.i.d. data is typically encountered in idealized envi-
ronments. In real-world applications, non-i.i.d. conditions are more
 
1830On the Convergence of Zeroth-Order Federated Tuning for Large Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
common and challenging. Next, we further discuss and analyze the
convergence of FedMeZO under non-i.i.d. settings.
3.2 Convergence Analysis in Non-i.i.d Case
Analyzing convergence in the context of non-i.i.d. data distributions
is crucial for understanding the behavior of FL algorithms in real-
world scenarios. In this section, we extend our convergence analysis
to the case where client data distributions are heterogeneous.
Theorem 3.4. (Stepwise Loss Descent in Non-i.i.d Setting) Let
Assumptions 1-4 and Assumption 6 hold and learning rate ğœ‚satisfy
ğœ‚â‰¤minn1
3ğ»ğ¿âˆšï¸ğ‘ğ‘”ğ‘‘,ğ‘
3ğ»ğ¿ğ‘ğ‘”,1
ğ»2o
. (11)
Then, the expected loss at each step for FedMeZO in the non-i.i.d.
setting is bounded as
Eğ‘¡
ğ‘“(ğœƒğ‘¡+1)
â‰¤ğ‘“(ğœƒğ‘¡)âˆ’2
ğ›¾ğ‘âˆ’2ğœeğ‘â„
ğ‘‘
ğœ‚Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+2eğœ2ğœğ¿ğœ‚
ğ‘ğ»ğ‘‘+ğœğœ‚ğœ‡2ğ¿3
2ğ‘ğ»âˆ’2
ğ›¾ğ‘ğœ‚ğœ2
â„, (12)
whereeğ‘â„=ğ‘â„+ğ‘andeğœ2=3ğ‘ğ‘”ğœ2
â„+ğœ2ğ‘”.
Comparing Eq. (12)with its i.i.d. counterpart Eq. (8), the non-i.i.d.
setting introduces additional terms reflecting data heterogeneity.
Firstly, an additional term eğ‘â„appears before Eğ‘¡âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡)âˆ¥2; secondly,
the original ğœ2ğ‘”change into eğœ2; thirdly, a new term related to ğœ2
â„
is added at the end. The term eğ‘â„amplifies the effect of the gradi-
ent norm, while eğœ2encapsulates both the intrinsic stochasticity
and data heterogeneity. The presence of ğœ2
â„indicates the impact
of client data divergence on the convergence behavior, in a degree
dependent on Î˜(1/ğ‘Ÿğ‘‘2). Given that the contribution of the negative
term accelerates the rate of decline in each round, it can be con-
cluded that heterogeneity is positively correlated with convergence.
Considering all the above changes, appropriate heterogeneity can
aid in the model convergence.
Experimental results in Section 5.3.3 confirm that a more random-
ized dataset distribution leads to improved convergence, supporting
our theoretical insights. Next, we present the global convergence
result for the non-i.i.d. setting building upon Theorem 3.4.
Corollary 3.5. (Global Convergence in Non-i.i.d. Setting) As-
suming the conditions of Theorem 3.4 hold, denote eÎ“=ğ‘‘âˆ’ğ‘ğ›¾ğœ
ğ‘‘ğ›¾ğ‘, Fed-
MeZO satisfies:
min
ğ‘¡âˆˆ[ğ‘‡]Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2â‰¤ğ‘“(ğœƒ0)âˆ’ğ‘“âˆ—
2eÎ“eğ‘â„ğœ‚ğ‘‡+eğœ2ğœğ¿
eÎ“eğ‘â„ğ‘ğ»ğ‘‘
+ğœğœ‡2ğ¿3
4eÎ“eğ‘â„ğ‘ğ»âˆ’ğœ2
â„
eÎ“eğ‘â„ğ›¾ğ‘. (13)
Givenğ›¾=Î˜(ğ‘Ÿ)andğœ=Î˜
1
ğ‘Ÿğ‘‘
, the expression ğ‘‘âˆ’ğ‘ğœğ›¾ sim-
plifies toğ‘‘âˆ’Î˜
ğ‘
ğ‘‘
which is dominated by ğ‘‘. Consequently, eÎ“
simplifies to Î˜
1
ğ‘Ÿ
as in the non-i.i.d. case. Compared to Corollary
3.2, Corollary 3.5 introduces two changes: first, all terms on the
right side of Eq. 13 include a denominator ğ‘ğ‘”, and second, there is
an additional term associated with non-i.i.d. heterogeneity, scaledwithÎ˜(ğœ2
â„/ğ‘â„ğ‘). This further demonstrates the constraining ef-
fect of data heterogeneity in FedMeZO. Similar to Corollary 3.3,
by setting appropriate values for ğœ‚andğœ‡, we obtain the following
convergence rate.
Corollary 3.6. (Convergence Rate in Non-i.i.d. Setting) Assum-
ing the conditions of Corollary 3.5 hold, with ğœ‚=(ğ‘ğ»)1/2(ğ‘Ÿeğ‘â„ğ‘‡)âˆ’1/2
andğœ‡=(eğ‘â„ğ‘ğ»)1/4ğ‘Ÿâˆ’1/2, FedMeZO has convergence rate as follows:
min
ğ‘¡âˆˆ[ğ‘‡]Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
â‰¤O
ğ‘Ÿ3/2(eğ‘â„ğ‘ğ»ğ‘‡)âˆ’1/2
+O
ğ‘‘âˆ’1(ğ‘Ÿeğ‘â„ğ‘ğ»)âˆ’1/2
âˆ’O
ğœ2
â„(ğ‘â„ğ‘)âˆ’1
.(14)
The convergence rate in Eq. (14)is primarily driven by the term
O
ğ‘Ÿ3/2(eğ‘â„ğ‘ğ»ğ‘‡)âˆ’1/2
, indicating that optimizing the balance be-
tweeneğ‘â„,ğ‘â„, andğ‘is crucial, which reflects a complex interplay
of heterogeneity. Specifically, we observe that to achieve better
convergence, a smaller O
ğ‘Ÿ3
2(eğ‘â„ğ‘ğ»ğ‘‡)âˆ’1
2
is preferred while the
O
ğœ2
â„(ğ‘â„ğ‘)âˆ’1
term need to increase at the same time. Conse-
quently, the balance between ğ‘ğ‘”,ğ‘â„, andğ‘becomes a dynamic
trade-off process, i.e., the heterogeneity among different clients
directly influences the overall convergence performance.
For now, we have answered the question â€œQ2: Can we establish
the convergence properties of ZOO-FL for LLMs? â€ via theorems and
corollaries mentioned in this section. We also validated the nature of
convergence under different scenarios and tasks through empirical
experiments in Section 5.2.
3.3 Implications
The aforementioned theoretical results offer numerous insights into
parameter tuning. A critical revelation from our analysis pertains
to the constraints imposed on the learning rate, as delineated in
Eq.(7)and Eq. (11), which suggests that an optimal learning rate
magnitude is anchored at 1/âˆš
ğ‘‘. Larger learning rates are not only
ineffectual but also pose a risk of destabilizing the training dynamic.
In Appendix C.3, our empirical experiments corroborate this hy-
pothesis, demonstrating that excessive learning rates precipitate
abrupt increases in loss.
Furthermore, our insights regarding the learning rate open up
prospects for personalized FL, a compelling approach that uses
client-specific configurations to address heterogeneity and has at-
tracted increasing interest [ 9,12,19,40,49]. Specifically, we inves-
tigate theory-guided personalized strategies by dynamically adjust-
ing the learning rate ğœ‚ğ‘–in proportion to a quantifiable measure
of data heterogeneity among clients. In light of Theorem 3.4 that
a larger heterogeneity is more conducive to model convergence,
it is feasible to appropriately increase the learning rate, allowing
specific clients to contribute more to the overall convergence, we
propose the following tailored adjustment strategy:
Proposition 3.7. (Adaptive Learning Rate Adjustment) Let As-
sumption 6 hold, the learning rate ğœ‚ğ‘–can be adjusted according to the
formula to better accommodate the varied learning landscapes than
non-personalized FL:
ğœ‚ğ‘–=ğœ‚0(1+ğ›¼Â·Î¦ğ‘–), (15)
 
1831KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, & Ying Shen
whereğœ‚0represents a default learning rate applicable in a i.i.d.
setting,ğ›¼is a scaling factor that determines the sensitivity of the
learning rate and Î¦ğ‘–is the heterogeneity index, representing the
extent ofğ‘ğ‘”andğœ2
â„. This proposition underscores the importance
of considering data heterogeneity in the design of the learning rate
strategy within personalized FL, offering a structured approach to
enhance learning outcomes across diverse client datasets.
In Section 5.4, we empirically confirm that a particular implemen-
tation of this strategy facilitates faster convergence. It is important
to note that the data heterogeneity index Î¦ğ‘–cannot be determined
a priori; therefore, we utilize several proxy measures during the
training process to estimate it. Our goal is not to prescribe an exact
solution to this strategy, but rather, through analysis and empirical
investigation, to enlighten further research and development of
personalized FedMeZO for more effective training of LLMs.
These discussions and corresponding empirical support address
the question â€œQ3: Which model parameters are critical for conver-
gence, and how can we leverage them to optimize FL performance,
such as via personalization? â€.
Besides, recall that we adopt LoRA to mitigate the communi-
cation burden associated with LLMs for practical FL scenarios.
Nonetheless, the influence of LoRA on the modelâ€™s low effective
rank, remains an open question. We thus advance the following con-
jecture under Assumption 1, predicated on existing literature[ 48,
59], to facilitate further validations:
Conjecture 3.8 (Rank Correlation). The optimal reparametriza-
tion rankğ‘ŸLoRA used in Low-Rank Adaptation (LoRA) is positively
proportional to the effective rank ğ‘Ÿof the Hessian matrix H(ğœƒğ‘¡)of
the tuned LLM. The ğ‘ŸLoRA is lower-bounded by ğ‘Ÿ, and can serve as an
empirical proxy for ğ‘Ÿ.
4 PROOF OUTLINE
This section provides an outline of the derivations presented in
Section 3, emphasizing the key analytical techniques and concepts
employed. Detailed proofs are available in Appendix D of [1].
We begin by taking expectations on both sides of Eq. (6), consid-
ering a federated learning setting. The equation is split into two
main parts for further analysis:
Eğ‘¡
ğ‘“(ğœƒğ‘¡+1)
â‰¤ğ‘“(ğœƒğ‘¡)âˆ’1
ğ›¾Â·ğœ‚Eğ‘¡1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1âˆ‡ğ‘“ğ‘–(ğœƒğ‘¡)2
+1
2ğœ‚2ğ¿Â·ğœÂ·Eğ‘¡1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1ğ‘’(ğ‘¡,ğ‘˜)
ğ‘–2
. (16)
For simplicity, we denote the two expectation terms as ğ‘‡1andğ‘‡2,
which pertain to the expected squared norms of the gradient and
the zeroth-order gradient estimator, respectively.
4.1 Proof of Theorem 3.1
For termğ‘‡1in Eq. (16), we utilize the Cauchy-Schwarz inequality
âˆ¥ğ‘+ğ‘âˆ¥2â‰¤2âˆ¥ğ‘âˆ¥2+2âˆ¥ğ‘âˆ¥2to decompose it into two parts, with
the first representing the discrepancy between local and global
gradients. By invoking Assumption 5, we establish:
ğ‘‡1â‰¤2
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1Eğ‘¡âˆ‡ğ‘“ğ‘–(ğœƒğ‘¡)âˆ’âˆ‡ğ‘“(ğœƒğ‘¡)2
+2Â·Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2.For termğ‘‡2, Jensenâ€™s inequality allows us to bound the expected
squared norm of the zeroth-order gradient estimator as follows:
ğ‘‡2â‰¤1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1Eğ‘¡ğ‘’(ğ‘¡,ğ‘˜)
ğ‘–2
. (17)
By substituting Eq. (3)into Eq. (17), we proceed to use the
Cauchy-Schwarz inequality to decompose this gradient estimator
into two parts, each of which is a biased estimator. Recall that in our
gradient estimator, ğ‘§ğ‘–follows a Gaussian distribution. Therefore,
the impact on the norm caused by a forward step and a backward
step of the estimator is identical. Consequently, we ascertain that
the termğ‘‡2is bounded by a single-point gradient estimation as
Eğ‘¡ğ‘§(ğ‘¡,ğ‘˜)
ğ‘–
ğœ‡
ğ¹ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–+ğœ‡ğ‘§(ğ‘¡,ğ‘˜)
ğ‘–,B(ğ‘¡,ğ‘˜)
ğ‘–)âˆ’ğ¹ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–,B(ğ‘¡,ğ‘˜)
ğ‘–)2
.
Following Lemma 4.1 in [ 23], we can bound the expectation term
as:
Eğ‘¡ğ‘’(ğ‘¡,ğ‘˜)
ğ‘–2
â‰¤1
ğ‘‘2"
2ğ‘‘Â·Eğ‘¡âˆ‡ğ¹ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–,B(ğ‘¡,ğ‘˜)
ğ‘–)2
+ğœ‡2
2ğ¿2ğ‘‘2#
â‰¤1
ğ‘‘2"
2ğ‘ğ‘”ğ‘‘Â·Eğ‘¡âˆ‡ğ‘“ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–)2
+2ğ‘‘ğœ2
ğ‘”+ğœ‡2
2ğ¿2ğ‘‘2#
,(18)
where the second inequality is derived based on Assumption 4. Sub-
sequently, we bound the expectation term by applying the Cauchy-
Schwartz inequality to divide it into three parts:
Eğ‘¡âˆ‡ğ‘“ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–)2
=Eğ‘¡âˆ‡ğ‘“ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–)âˆ“âˆ‡ğ‘“ğ‘–(ğœƒğ‘¡)âˆ“âˆ‡ğ‘“(ğœƒğ‘¡
ğ‘–)2
â‰¤3ğ¿2Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
+3Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
.(19)
The first part represents the gradient difference between stages
(ğ‘¡,ğ‘˜)and(ğ‘¡,0), which can be computed using Assumption 3, i.e.,
theğ¿-smooth condition. The second part signifies the disparity
between local and global aspects, calculated using Assumption 5.
The third part is retained as is.
Combining Equations (17),(18)and(19), we boundğ‘‡2as follow:
ğ‘‡2â‰¤6ğ‘ğ‘”ğ¿2
ğ‘ğ‘‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
+6ğ‘ğ‘”ğ»
ğ‘‘Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+2ğ»ğœ2ğ‘”
ğ‘‘+ğœ‡2ğ»ğ¿2
2. (20)
Next, combining Equations (16), (17) and (20), we have:
Eğ‘¡
ğ‘“(ğœƒğ‘¡+1)
âˆ’ğ‘“(ğœƒğ‘¡)â‰¤ 
3ğ‘ğ‘”ğœğœ‚2ğ»ğ¿
ğ‘‘âˆ’2ğœ‚
ğ›¾!
Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+3ğ‘ğ‘”ğœğœ‚2ğ¿3
ğ‘ğ‘‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
+ğœ2ğ‘”ğœğœ‚2ğ»ğ¿
ğ‘‘+ğœğœ‚2ğœ‡2ğ»ğ¿3
4. (21)
In Eq. (21),Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
remains unknown and we need to
constrain it further. The key idea is to transform this expectation
term into a form related to Eğ‘˜
ğ‘¡âˆ¥ğ‘’(ğ‘¡,ğ‘˜)
ğ‘–âˆ¥2and then utilize the con-
clusion of Eq. (18)and Eq. (19)for computation, we can have the
 
1832On the Convergence of Zeroth-Order Federated Tuning for Large Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
bounded result:
1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
â‰¤ğ¶1
ğ¶0, (22)
whereğ¶0=1âˆ’3ğ‘ğ‘”ğ‘‘ğœ‚2ğ»2ğ¿2andğ¶1=2ğ‘ğ‘”ğ‘‘ğ»3ğœ‚2Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+2
3ğ‘‘ğœ2ğ‘”ğ»3ğœ‚2+ğœ‡2ğ¿2ğ‘‘2ğ»3ğœ‚2
6.
Finally, by substituting Eq. (22)into Eq. (21), we obtain the fi-
nal result of the stepwise descent. After simplifying and appropri-
ately setting the learning rate, we arrive at the result presented in
Theorem 3.1. The detailed derivation of this result is provided in
Appendix D.1 of [1].
4.2 Proof of Theorem 3.4
The proof for the non-i.i.d. case follows a similar structure to that of
thei.i.d. case, with adjustments made for the heterogeneity between
local and global models as captured by ğ‘â„andğœ2
â„(Assumption 6).
In particular, we redefine ğ‘‡1in Eq. (16)aseğ‘‡1to reflect the increased
variance due to non-i.i.d. data:
eğ‘‡1â‰¤2
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1Eğ‘¡âˆ‡ğ‘“ğ‘–(ğœƒğ‘¡)âˆ’âˆ‡ğ‘“(ğœƒğ‘¡)2
+2Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
â‰¤2(1+ğ‘â„)Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2+2ğœ2
â„, (23)
where the second inequality follows Assumption 6.
Subsequently, eğ‘‡2is computed similarly, with the heterogeneity
terms incorporated. The difference from the i.i.d. case lies in the
bounding of expectation term in Eq. (19):
Eğ‘¡âˆ‡ğ‘“ğ‘–(ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–)2
â‰¤3ğ¿2Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
+3(ğ‘â„+1)Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+3ğœ2
â„, (24)
where the inequality employs Assumption 6.
Combining Equations (17),(18)and(24), we bound eğ‘‡2as follow:
eğ‘‡2â‰¤6ğ‘ğ‘”ğ¿2
ğ‘ğ‘‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
+6ğ‘ğ‘”(ğ‘â„+1)ğ»
ğ‘‘Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+6ğ‘ğ‘”ğœ2
â„ğ»
ğ‘‘+2ğ»ğœ2ğ‘”
ğ‘‘+ğœ‡2ğ»ğ¿2
2. (25)
By substituting eğ‘‡1in Eq. (23)andeğ‘‡2in Eq. (25)into Eq. (16), we
get the result under the non-i.i.d. condition as follow:
Eğ‘¡
ğ‘“(ğœƒğ‘¡+1)
âˆ’ğ‘“(ğœƒğ‘¡)â‰¤ 
3ğ‘ğ‘”eğ‘â„ğœğœ‚2ğ»ğ¿
ğ‘‘âˆ’2eğ‘â„ğœ‚
ğ›¾!
Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+3ğ‘ğ‘”ğœğœ‚2ğ¿3
ğ‘ğ‘‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
+eğœ2ğœğœ‚2ğ»ğ¿
ğ‘‘+ğœğœ‚2ğœ‡2ğ»ğ¿3
4âˆ’2
ğ›¾ğœ2
â„ğœ‚, (26)
whereeğ‘â„denotes(ğ‘â„+1)andeğœ2denotes(3ğ‘ğ‘”ğœ2
â„+ğœ2ğ‘”). We then
need to make the expectation term bounded. Unlike Eq. (22), due to
the variations introduced by Eq. (24), two additional terms relatedtoeğ‘â„andeğœ2emerge, yielding the following result:
1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1Eğ‘¡ğœƒ(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœƒğ‘¡2
â‰¤ğ¶2
ğ¶0, (27)
whereğ¶1=1âˆ’3ğ‘ğ‘”ğ‘‘ğœ‚2ğ»2ğ¿2andğ¶2=2ğ‘ğ‘”ğ‘‘eğ‘â„ğ»3ğœ‚2Eğ‘¡âˆ‡ğ‘“(ğœƒğ‘¡)2
+2
3eğœ2ğ‘‘ğ»3ğœ‚2+ğœ‡2ğ¿2ğ‘‘2ğ»3ğœ‚2
6.
Finally, we can substitute Eq. (27)into Eq. (26)and have the
result of Theorem 3.4. The detailed derivations about these steps of
Theorem 3.4 are provided in Appendix D.3 of [1].
5 EMPIRICAL SUPPORT
This section aims to empirically validate our theoretical findings
through a series of experiments.
5.1 Experimental Setup
We utilize LLaMA-3B [ 52] as the foundational model and employ
four datasets covering a range of tasks and data distribution types
to provide comprehensive validation of our theoretical results [ 32].
Given that our theory centers on the loss function, we primarily
focus on analyzing loss descent in our experiments. More details of
the used datasets are in the Appendix (Table 2).
We set the total number of communication rounds to 500. By
default, BP-based baselines undertake local training for one epoch,
whereas FedMeZO conducts local training for 30 steps. We repeat
our experiments with three seeds and plot the error bars. For more
detailed implementation specifics, please refer to Appendix B.
5.2 Convergence Study
To assess the convergence of FedMeZO, we perform experiments on
three datasets using different data splitters, as specified in Table 2,
with test loss serving as the convergence metric. Our objective is to
evaluate the generalization and stability of FedMeZO across diverse
datasets and heterogeneity scenarios. For benchmarking purposes,
we also measure the performance of BP-based FedAvg on the same
datasets. Additionally, we document the GPU memory usage during
training in Table 1. Representative findings are illustrated in Figure
1, while the comprehensive results are available in Appendix F.11
of [1] due to the space limitation.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001a/uni00000014/uni00000011/uni0000001b/uni00000014/uni00000011/uni0000001c/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000027/uni00000052/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000030/uni00000048/uni00000057/uni00000044
/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000055
/uni00000029/uni00000048/uni00000047/uni00000030/uni00000048/uni0000003d/uni00000032
/uni00000025/uni00000033/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000029/uni00000048/uni00000047/uni00000024/uni00000059/uni0000004a
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000026/uni00000052/uni00000047/uni00000048/uni00000010/uni0000002f/uni00000027/uni00000024
/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000055
/uni00000029/uni00000048/uni00000047/uni00000030/uni00000048/uni0000003d/uni00000032
/uni00000025/uni00000033/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000029/uni00000048/uni00000047/uni00000024/uni00000059/uni0000004a
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni0000002a/uni00000036/uni00000030/uni00000010/uni0000002c/uni0000002c/uni00000027
/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000055
/uni00000029/uni00000048/uni00000047/uni00000030/uni00000048/uni0000003d/uni00000032
/uni00000025/uni00000033/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000029/uni00000048/uni00000047/uni00000024/uni00000059/uni0000004a
Figure 1: Convergence comparison of FedMeZO and BP-based
FedAvg.
Two main conclusions emerge from the convergence experi-
ments: First, when the learning rate complies with the require-
ments discussed in Section 3.3, as stipulated by Theorem 3.1 and
 
1833KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, & Ying Shen
Table 1: The GPU Memory of BP-based FedAvg and FedMeZO.
Task BP-based FedAvg (MiB) FedMeZO (MiB)
Dolly-Meta 26571 10061
GSM8K-IID 17771 9733
CodeAlpaca-LDA 15287 9569
Theorem 3.4, FedMeZO consistently diminishes loss with each step,
ultimately achieving stable convergence. Second, under equiva-
lent learning rate configurations, FedMeZO decreases loss more
rapidly than BP-based FedAvg, indicating a swifter convergence
rate. For instance, in the Dolly-Meta figure, FedMeZO stabilizes
and converges around 300 rounds, whereas BP-based FedAvgâ€™s loss
is still declining at this juncture. Notably, from Table 1, we observe
that the GPU memory demand for FedMeZO is roughly one-half of
that required by BP-based FedAvg, suggesting that FedMeZO can
achieve a speedier convergence with fewer resources.
5.3 Hyper-parameters Study
In this subsection, we perform a series of experiments to ascertain
the influence of various hyper-parameters, as intimated by our
theoretical findings.
5.3.1 Impact of Perturbation Scale ğœ‡.To corroborate the theoretical
impacts of the perturbation step on convergence, we examine ğœ‡
values of 5Ã—10âˆ’3and2Ã—10âˆ’4, in addition to the default ğœ‡=
1Ã—10âˆ’3. We leverage the same datasets and splitters as in Section
5.2 for robustness. Figure 2 shows representative outcomes, with
comprehensive results in Appendix F.9 of [1].
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001a/uni00000014/uni00000011/uni0000001b/uni00000014/uni00000011/uni0000001c/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000027/uni00000052/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000030/uni00000048/uni00000057/uni00000044
/uni00000033/uni00000048/uni00000055/uni00000057/uni00000058/uni00000055/uni00000045/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000015
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000026/uni00000052/uni00000047/uni00000048/uni00000010/uni0000002f/uni00000027/uni00000024
/uni00000033/uni00000048/uni00000055/uni00000057/uni00000058/uni00000055/uni00000045/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000015
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni0000002a/uni00000036/uni00000030/uni00000010/uni0000002c/uni0000002c/uni00000027
/uni00000033/uni00000048/uni00000055/uni00000057/uni00000058/uni00000055/uni00000045/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000015
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018
Figure 2: Effects of different perturbation scales ğœ‡.
The results confirm that, consistent with Equations (10)and(14),
a smallerğœ‡marginally expedites model convergence. Figure 2 exem-
plifies that the training trajectory with ğœ‡=2Ã—10âˆ’4descends more
rapidly than the others. However, given that ğœ‡appears as a second-
order term inğœğœ‡2ğ¿3
4eÎ“eğ‘â„ğ‘ğ»and its absolute value is relatively small, its
overall influence is modest. This is evident in Figure 2, where mod-
ifications to ğœ‡within a specific range yield only slight variations.
Thus, a smaller ğœ‡proves advantageous for model convergence.
5.3.2 Impact of Local Iteration Step ğ».To validate the theoretical
impact of local iteration steps on convergence, we contrast ğ»=10
andğ»=50with the standard ğ»=30. Utilizing identical datasets
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001a/uni00000014/uni00000011/uni0000001b/uni00000014/uni00000011/uni0000001c/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000027/uni00000052/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000030/uni00000048/uni00000057/uni00000044
/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056
/uni0000002b/uni00000016/uni00000013
/uni0000002b/uni00000014/uni00000013
/uni0000002b/uni00000018/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000026/uni00000052/uni00000047/uni00000048/uni00000010/uni0000002f/uni00000027/uni00000024
/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056
/uni0000002b/uni00000016/uni00000013
/uni0000002b/uni00000014/uni00000013
/uni0000002b/uni00000018/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni0000002a/uni00000036/uni00000030/uni00000010/uni0000002c/uni0000002c/uni00000027
/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056
/uni0000002b/uni00000016/uni00000013
/uni0000002b/uni00000014/uni00000013
/uni0000002b/uni00000018/uni00000013Figure 3: Effects of different local iteration steps ğ».
and splitters from Section 5.2, we present typical findings in Figure
3, with all the detailed results forthcoming in Appendix F.10 of [ 1].
These experimental results suggest that a lower ğ»engenders a
more sluggish convergence pace, whereas a higher ğ»somewhat
propels convergence, mirroring the impact of ğ»as a denominator in
the theoretical convergence rate analysis. Nonetheless, an excessive
ğ»may lead to instability, as depicted by the curve of ğ»=50, which
exhibits a surge endwise in the figure. Hence, an appropriate choice
ofğ»facilitates efficient model convergence.
5.3.3 Analysis of Other Hyper-parameters. We also explore the
ramifications of learning rate, data splitters, the number of clients
ğ‘and the model size on the convergence rate. Due to the space
limit, details pertaining to these experimental settings and results
are presented in Appendices C.3, C.4, C.5, and C.6 respectively.
In a nutshell, as shown in Fig. 5, a suitable learning rate anchored
at1/âˆš
ğ‘‘leads to stabilized training dynamics, while larger ones ex-
hibit divergence as suggested by our analysis (Section 3.3). Besides,
dissimilar splitters symbolize varying extents of data heterogeneity,
and we have observations revealing that augmented heterogeneity
culminates in lower stabilized loss values (as shown in Fig. 6). This
intimates that a moderate degree of data heterogeneity can elevate
the modelâ€™s convergence proficiency. Moreover, Fig. 7 verifies our
theoretical analysis in Section 3 that an increase in ğ‘helps stabilize
the global convergence.
5.4 Personalization Study
To reconcile Proposition 3.7 with practical scenarios, we conduct
the following subsequent experiments. To account for each clientâ€™s
heterogeneity during model updates in each round, we derive three
signal quantities: (1) Round-wise Train Loss Difference : The discrep-
ancy between each clientâ€™s loss in the preceding training round and
the global loss. (2) Five-round Average Train Loss Difference : The
average loss deviation for each client relative to the global loss over
the antecedent five rounds. (3) Model Parameter Update Difference :
The disparity between each clientâ€™s previous round parameter up-
dates and the global update magnitude. We normalize them to the
range of(âˆ’1,1), serving as empirical estimates for Î¦.
For the setting of the scaling factor ğ›¼, following the guidance
of the learning rate in Section 3.3, we designate 1.5Ã—10âˆ’5as the
maximal learning rate, potentially leading to surges as per the
learning rate search network. Symmetrically, we posit the minimal
value at 5Ã—10âˆ’6, anchored on the default learning rate of 1Ã—10âˆ’5,
thereby assigning ğ›¼a value of 5Ã—10âˆ’6.
 
1834On the Convergence of Zeroth-Order Federated Tuning for Large Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001b/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000027/uni00000048/uni00000049/uni00000044/uni00000058/uni0000004f/uni00000057
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050
/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001b/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000029/uni0000004c/uni00000059/uni00000048/uni00000010/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000027/uni00000048/uni00000049/uni00000044/uni00000058/uni0000004f/uni00000057
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050
/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001b/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000027/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048
/uni00000027/uni00000048/uni00000049/uni00000044/uni00000058/uni0000004f/uni00000057
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050
/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c
Figure 4: Comparison of different strategies of learning rate
adjustment. â€œDefaultâ€ indicates non-personalized case, and
â€œRound-wise Lossâ€, â€œFive-round Lossâ€ and â€œModel Update
Differenceâ€ indicate three signal quantities leveraged.
As counterpoints, we furnish two configuration strategies for
the learning rate adjustment: one uniformly applies a default learn-
ing rate of 1Ã—10âˆ’5, while the other randomly selects within
[5Ã—10âˆ’6,1.5Ã—10âˆ’5]for each round. The conclusive results are
depicted in Figure 4. Based on the experimental results, we observe
that our method achieves faster loss convergence with the second
and third types of signal quantities compared to the default and
random settings, with the third type yielding the most impressive
performance. In contrast, the first type of signal quantity had a
negligible impact. These results suggest that while round-wise loss
exhibits some degree of randomness, aggregating losses over multi-
ple rounds can approximate heterogeneity to a meaningful extent,
thus serving as an indicator to expedite model convergence.
It is also noteworthy that the third type of signal quantity aligns
with the expression âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡)âˆ’Ãğ»
ğ‘˜=0ğœ‚ğ‘–âˆ‡ğ‘“ğ‘–(ğœƒ(ğ‘¡,ğ‘˜))âˆ¥2, which most
closely reflects Assumption 6. Consequently, it demonstrates the
most effective performance in the experiments, not only achiev-
ing the fastest convergence but also the lowest stable loss. This
case study experiment substantiates the efficacy of Proposition 3.7,
offering valuable insights for parameter tuning in personalized FL.
6 CONCLUSION
This work investigates the convergence of FedMeZO, a practical
approach integrating Memory-efficient Zeroth-Order Optimization
within a federated learning setting for Large Language Models
(LLMs). Extensive empirical results verified our analyses and in-
dicated that FedMeZO achieves fast convergence with reduced
GPU memory requirements, offering a promising alternative to
traditional optimization methods. The incorporation of a person-
alized learning rate adjustment, derived from theoretical analysis,
has been shown to effectively enhance loss reduction. Through
this study, we aim to enlighten more research and development
of memory-efficient optimization techniques to address practical
challenges associated with the fine-tuning of LLMs, particularly in
resource-constrained scenarios [29].
REFERENCES
[1] Extended version of this paper. arXiv preprint arXiv:2402.05926.
[2]Alekh Agarwal, Martin J Wainwright, Peter Bartlett, and Pradeep Ravikumar.
2009. Information-theoretic lower bounds on the oracle complexity of convex
optimization. Advances in Neural Information Processing Systems 22 (2009).
[3]Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021. Intrinsic Di-
mensionality Explains the Effectiveness of Language Model Fine-Tuning. InProceedings of the 59th Annual Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers). 7319â€“7328.
[4]Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li. 2024. Fed-
erated Fine-tuning of Large Language Models under Heterogeneous Language
Tasks and Client Resources. arXiv preprint arXiv:2402.11505 (2024).
[5]LÃ©on Bottou, Frank E Curtis, and Jorge Nocedal. 2018. Optimization methods for
large-scale machine learning. SIAM review 60, 2 (2018), 223â€“311.
[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877â€“1901.
[7]Sahil Chaudhary. 2023. Code alpaca: An instruction-following llama model for
code generation.
[8]Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, and Xiaolin Zheng. 2023.
Federated large language model: A position paper. arXiv preprint arXiv:2307.08925
(2023).
[9]Daoyuan Chen, Dawei Gao, Weirui Kuang, Yaliang Li, and Bolin Ding. 2022.
pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning.
InNeurIPS.
[10] Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao Li, Yaliang Li, Bolin
Ding, and Jingren Zhou. 2023. FS-REAL: Towards Real-World Cross-Device Fed-
erated Learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 3829â€“3841.
[11] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge,
Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding,
and Jingren Zhou. 2024. Data-Juicer: A One-Stop Data Processing System for
Large Language Models. In International Conference on Management of Data.
[12] Daoyuan Chen, Liuyi Yao, Dawei Gao, Bolin Ding, and Yaliang Li. 2023. Efficient
Personalized Federated Learning via Sparse Model-Adaptation. In International
Conference on Machine Learning, ICML, Vol. 202. 5234â€“5256.
[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
et al.2021. Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374 (2021).
[14] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
et al.2021. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168 (2021).
[15] Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan,
Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, et al .2023. Free dolly:
Introducing the worldâ€™s first truly open instruction-tuned llm.
[16] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah,
Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free Dolly:
Introducing the Worldâ€™s First Truly Open Instruction-Tuned LLM.
[17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
Jie Tang. 2021. Glm: General language model pretraining with autoregressive
blank infilling. arXiv preprint arXiv:2103.10360 (2021).
[18] John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. 2015.
Optimal rates for zero-order convex optimization: The power of two function
evaluations. IEEE Transactions on Information Theory 61, 5 (2015), 2788â€“2806.
[19] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized feder-
ated learning: A meta-learning approach. In NeurIPS 2020.
[20] Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and
Qiang Yang. 2023. FATE-LLM: A Industrial Grade Federated Learning Framework
for Large Language Models. CoRR abs/2310.10049 (2023).
[21] Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and
Qiang Yang. 2023. Fate-llm: A industrial grade federated learning framework for
large language models. arXiv preprint arXiv:2310.10049 (2023).
[22] Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N Jones, and Yong
Zhou. 2022. Communication-efficient stochastic zeroth-order optimization for
federated learning. IEEE Transactions on Signal Processing 70 (2022), 5058â€“5073.
[23] Xiang Gao, Bo Jiang, and Shuzhong Zhang. 2018. On the information-adaptive
variants of the ADMM: an iteration complexity perspective. Journal of Scientific
Computing 76 (2018), 327â€“363.
[24] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. 2019. An investigation
into neural net optimization via hessian eigenvalue density. In International
Conference on Machine Learning. PMLR, 2232â€“2241.
[25] Jiaqi Gu, Chenghao Feng, Zheng Zhao, Zhoufeng Ying, Ray T Chen, and David Z
Pan. 2021. Efficient on-chip learning for optical neural networks through power-
aware sparse zeroth-order optimization. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 35. 7583â€“7591.
[26] Davood Hajinezhad, Mingyi Hong, and Alfredo Garcia. 2019. ZONE: Zeroth-
order nonconvex multiagent optimization over networks. IEEE Trans. Automat.
Control 64, 10 (2019), 3995â€“4010.
[27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt. 2020. Measuring massive multitask language under-
standing. arXiv preprint arXiv:2009.03300 (2020).
 
1835KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, & Ying Shen
[28] Junyuan Hong, Zhuangdi Zhu, Shuyang Yu, Zhangyang Wang, Hiroko Dodge,
and Jiayu Zhou. 2021. Federated Adversarial Debiasing for Fair and Transferable
Representations. In KDD.
[29] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large
Language Models. In International Conference on Learning Representations. https:
//openreview.net/forum?id=nZeVKeeFYf9
[30] Kevin G Jamieson, Robert Nowak, and Ben Recht. 2012. Query complexity of
derivative-free optimization. Advances in Neural Information Processing Systems
25 (2012).
[31] Peter Kairouz, H. Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, Rafael G. L. Dâ€™Oliveira, Hubert Eichner, Salim El Rouayheb,
David Evans, Josh Gardner, Zachary Garrett, AdriÃ  GascÃ³n, Badih Ghazi, Phillip B.
Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo,
Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Kho-
dak, Jakub KonecnÃ½, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
TancrÃ¨de Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer
Ã–zgÃ¼r, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova,
Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha
Suresh, Florian TramÃ¨r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng
Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. 2021. Advances and Open
Problems in Federated Learning. Foundations and TrendsÂ® in Machine Learning
14, 1â€“2 (2021), 1â€“210.
[32] Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan,
Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. Federatedscope-
LLM: A comprehensive package for fine-tuning large language models in feder-
ated learning. arXiv preprint arXiv:2309.00363 (2023).
[33] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. 2018. Measuring
the Intrinsic Dimension of Objective Landscapes. In International Conference on
Learning Representations.
[34] LeiLai Li, Jianzong Wang, Xiaoyang Qu, and Jing Xiao. 2021. Communication-
memory-efficient decentralized learning for audio representation. In 2021 Inter-
national Joint Conference on Neural Networks (IJCNN). IEEE, 1â€“8.
[35] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. 2014. Efficient mini-
batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining. 661â€“670.
[36] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of Machine learning and systems 2 (2020), 429â€“450.
[37] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2019.
On the Convergence of FedAvg on Non-IID Data. In ICLR.
[38] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III,
and Pramod K Varshney. 2020. A primer on zeroth-order optimization in signal
processing and machine learning: Principals, recent advances, and applications.
IEEE Signal Processing Magazine 37, 5 (2020), 43â€“54.
[39] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi
Chen, and Sanjeev Arora. 2023. Fine-Tuning Language Models with Just Forward
Passes. Advances in Neural Information Processing Systems (2023).
[40] Othmane Marfoq, Chuan Xu, Giovanni Neglia, and Richard Vidal. 2020.
Throughput-Optimal Topology Design for Cross-Silo Federated Learning. In
NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.),
Vol. 33. Curran Associates, Inc., 19478â€“19487. https://proceedings.neurips.cc/
paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf
[41] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In AISTATS. PMLR, 1273â€“1282.
[42] Chuizheng Meng, Sirisha Rambhatla, and Yan Liu. 2021. Cross-node federated
graph neural network for spatio-temporal data modeling. In KDD. 1202â€“1211.
[43] Khalil Muhammad, Qinqin Wang, Diarmuid Oâ€™Reilly-Morgan, Elias Tragos, Barry
Smyth, Neil Hurley, James Geraci, and Aonghus Lawlor. 2020. Fedfast: Going
beyond average for faster training of federated recommender systems. In KDD.
1234â€“1242.
[44] Yurii Nesterov and Vladimir Spokoiny. 2017. Random gradient-free minimization
of convex functions. Foundations of Computational Mathematics 17 (2017), 527â€“
566.
[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems 32 (2019).[46] Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, and Shuiguang
Deng. 2024. Federated Full-Parameter Tuning of Billion-Sized Language Models
with Communication Cost under 18 Kilobytes. arXiv:2312.06353 [cs.LG]
[47] Xinchi Qiu, Javier Fernandez-Marques, Pedro PB Gusmao, Yan Gao, Titouan Par-
collet, and Nicholas Donald Lane. 2022. ZeroFL: Efficient On-Device Training for
Federated Learning with Local Sparsity. In International Conference on Learning
Representations.
[48] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. 2017.
Empirical analysis of the hessian of over-parametrized neural networks. arXiv
preprint arXiv:1706.04454 (2017).
[49] Felix Sattler, Klaus-Robert MÃ¼ller, and Wojciech Samek. 2020. Clustered Federated
Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy
Constraints. TNNLS (2020).
[50] Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, and Bryan Kian Hsiang Low. 2023.
Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gra-
dients. arXiv preprint arXiv:2308.04077 (2023).
[51] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An
instruction-following llama model.
[52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[54] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMa-
han, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly,
Deepesh Data, et al .2021. A field guide to federated optimization. arXiv preprint
arXiv:2107.06917 (2021).
[55] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. 2021. A
novel framework for the analysis and design of heterogeneous federated learning.
IEEE Transactions on Signal Processing 69 (2021), 5234â€“5249.
[56] Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and
Jingren Zhou. 2022. FederatedScope-GNN: Towards a Unified, Comprehensive
and Efficient Package for Federated Graph Learning. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4110â€“4120.
[57] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, et al .
2020. Transformers: State-of-the-art natural language processing. In Proceedings
of the 2020 conference on empirical methods in natural language processing: system
demonstrations. 38â€“45.
[58] Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope: A Flexible Fed-
erated Learning Platform for Heterogeneity. Proceedings of the VLDB Endowment
16, 5 (2023), 1059â€“1072.
[59] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. 2020. Pyhes-
sian: Neural networks through the lens of the hessian. In 2020 IEEE international
conference on big data (Big data). IEEE, 581â€“590.
[60] Xinlei Yi, Shengjun Zhang, Tao Yang, and Karl H Johansson. 2022. Zeroth-order
algorithms for stochastic distributed nonconvex optimization. Automatica 142
(2022), 110353.
[61] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin
Wang, and Yiran Chen. 2023. Towards Building the Federated GPT: Federated
Instruction Tuning. arXiv preprint arXiv:2305.05644 (2023).
[62] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong
Yu, Guoyin Wang, and Yiran Chen. 2023. Towards Building the FederatedGPT:
Federated Instruction Tuning. In International Workshop on Federated Learning
in the Age of Foundation Models in Conjunction with NeurIPS 2023. https://
openreview.net/forum?id=TaDiklyVps
[63] Qingsong Zhang, Bin Gu, Zhiyuan Dang, Cheng Deng, and Heng Huang. 2021.
Desirable companion for vertical federated learning: New Zeroth-order gradient
based algorithm. In Proceedings of the 30th ACM International Conference on
Information & Knowledge Management. 2598â€“2607.
[64] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al .2022. Opt:
Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068
(2022).
[65] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey
of large language models. arXiv preprint arXiv:2303.18223 (2023).
 
1836On the Convergence of Zeroth-Order Federated Tuning for Large Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
APPENDIX
A DETAILED ASSUMPTIONS
Assumption 2. (Bounded Loss) The global loss function ğ‘“(ğœƒ)is
bounded below by a scalar ğ‘“âˆ—, i.e.,ğ‘“âˆ—â‰¥ğ‘“(ğœƒ)>âˆ’âˆfor allğœƒ.
Assumption 3. (ğ¿-smoothness) The local and global loss functions
ğ¹ğ‘–(ğœƒ,Bğ‘–),ğ‘“ğ‘–(ğœƒ), andğ‘“(ğœƒ)are L-smooth. Mathematically, for any
ğœƒ1,ğœƒ2âˆˆRğ‘‘, it holds that
âˆ¥âˆ‡ğ‘“ğ‘–(ğœƒ2)âˆ’âˆ‡ğ‘“ğ‘–(ğœƒ1)âˆ¥â‰¤ğ¿âˆ¥ğœƒ2âˆ’ğœƒ1âˆ¥,
ğ‘“ğ‘–(ğœƒ2)â‰¤ğ‘“ğ‘–(ğœƒ1)+âŸ¨âˆ‡ğ‘“ğ‘–(ğœƒ),ğœƒ2âˆ’ğœƒ1âŸ©+ğ¿
2âˆ¥ğœƒ2âˆ’ğœƒ1âˆ¥2.
Assumption 4. (Mini-batch Gradient Error Bound) For any
ğœƒâˆˆRğ‘‘, the second-order moment of the stochastic gradient is bounded
byEBğ‘–âˆ¥âˆ‡ğ¹ğ‘–(ğœƒ,Bğ‘–)âˆ¥2â‰¤ğ‘ğ‘”âˆ¥âˆ‡ğ‘“ğ‘–(ğœƒ)âˆ¥2+ğœ2ğ‘”, whereğ‘ğ‘”â‰¥1.
Assumption 5. (Global-Local Disparities in i.i.d. Setting) For
anyğœƒâˆˆRğ‘‘, the discrepancy between the local and global gradient is
negligible, i.e., Eğ‘–âˆ¥âˆ‡ğ‘“(ğœƒ)âˆ’âˆ‡ğ‘“ğ‘–(ğœƒ)âˆ¥2=0.
Assumption 6. (Global-Local Disparities in Non-i.i.d. Setting)
For anyğœƒâˆˆRğ‘‘, the discrepancy between the local and global gradient
is bounded byâˆ¥âˆ‡ğ‘“(ğœƒ)âˆ’âˆ‡ğ‘“ğ‘–(ğœƒ)âˆ¥2â‰¤ğ‘â„âˆ¥âˆ‡ğ‘“(ğœƒ)âˆ¥2+ğœ2
â„,whereğ‘â„is
a positive constant.
Assumptions 2-4 are well-established in the literature on large-
scale stochastic optimization [ 5]. Assumption 5 describes an ideal
i.i.d. setting where each clientâ€™s gradient is aligned with the global
gradient. Assumption 6 accounts for the heterogeneity of client
data distributions that is typical in non-i.i.d. settings [36, 37, 55].
B IMPLEMENTATION DETAILS
B.1 Datasets
We adopt several federated tuning datasets tailored for LLMs from
[32], with different splitting strategies to simulate the heterogeneity
typical of different federated learning (FL) scenarios, including a
uniform distribution of data, a Dirichlet distribution of data, and a
splitter based on meta-information. The proportion of test data for
each dataset is 1%.
Table 2: Datasets and Basic Information.
Name #Sample Domain
Fed-Alpaca [51] 52.0k Generic Language
Fed-Dolly [15] 15.0k Generic Language
Fed-GSM8K [14] 7.5k CoT
Fed-CodeAlpaca [7] 8.0k Code Generation
B.2 Experimental Platforms
We implement our approaches using PyTorch [ 45] v1.10.1, coupled
with PEFT v0.3.0 and the Transformers library [ 57] v4.29.2. Exper-
iments with LLaMA-3B are conducted on a computing platform
equipped with four NVIDIA A100 GPUs (40GB), with pre-trained
LLMs loaded as 16-bit floating-point numbers.B.3 Default Implementation Settings
Following the guidelines in [ 32,39], all approaches perform local
training with a batch size of 1. To standardize the experimental
conditions, both BP-based and FedMeZO train locally with specific
learning rates: ğœ‚=1Ã—10âˆ’5for the Fed-Dolly and Fed-Alpaca
datasets,ğœ‚=2Ã—10âˆ’5for the Fed-CodeAlpaca dataset, and ğœ‚=2.5Ã—
10âˆ’5for the Fed-GSM8K dataset. The rank and alpha parameters
for LoRA are set to 128 and 256. As per [ 39], the perturbation
scaleğœ‡for FedMeZO is set to 1Ã—10âˆ’3. We employed the early
stopping mechanism following previous works [ 9]. The training
was stopped if there was no improvement in the validation loss
within 30 epochs. The best model was selected from the epoch with
the lowest validation loss. We conducted three sets of experiments
with randomly selected seeds, and calculated the mean as the line
plot with a 90% confidence interval as the error bar.
C SUPPLEMENTARY EXPERIMENTS
C.1 Communication Cost of FedMeZO
In Section 2.2, we mention using LoRA [ 29] to reduce the substantial
communication overhead. We theoretically prove this methodâ€™s
equivalence to full-parameter transmission as follows:
ğœƒğ‘¡+1,0
ğ‘–=1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğœƒğ‘¡,ğ»
ğ‘–=1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1"
ğœƒğ‘¡+1,0
ğ‘–+ğ»âˆ‘ï¸
ğ‘˜=1âˆ‡ğ‘™ğ‘œğ‘Ÿğ‘ğœƒğ‘¡,ğ‘˜
ğ‘–#
=ğœƒğ‘¡,0
ğ‘–+1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ»âˆ‘ï¸
ğ‘˜=1âˆ‡ğ‘™ğ‘œğ‘Ÿğ‘ğœƒğ‘¡,ğ‘˜
ğ‘–(28)
Note that the left side represents the clientâ€™s parameters with
FedAvg, while the right side corresponds to FedMeZO, and they
are equivalent. By utilizing LoRA, FedMeZO achieves the same
effect with just 1.23% of parameters in our setting. Specifically, full
parameter transmission needs 6.39GB, while LoRA demands merely
80.45MB.
C.2 Evaluations with Common LLMâ€™s Metrics
We examined FedMeZO with some commonly used metrics for
LLMs evaluations: Dolly with MMLU metrics [ 27], Code with Ope-
nAI HumanEval metrics [ 13], and GSM8K with CoT metrics[ 14].
We evaluated FedMeZO and BP-based FedAvg at model checkpoints
of rounds 0, 100, 200 and present the results in Table 3. The results
verified FedMeZOâ€™s effectiveness again:
â€¢Fine-tuning LLMs with FedMeZO effectively improves their
performance on specific tasks;
â€¢FedMeZO gains better performance compared to BP-based
FedAvg.
Table 3: Evaluations of FedMeZO with LLMâ€™s Metrics
RoundsDolly (%) Code (%) GSM8K (%)
FedMeZO BP FedMeZO BP FedMeZO BP
0 26 26 8.53 8.53 3.41 3.41
100 26.6 26.1 8.66 8.53 4.32 4.25
200 26.3 26.2 8.41 8.53 4.62 4.40
 
1837KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, & Ying Shen
C.3 Conditions for the Learning Rate
We disabled the early-stop mechanism and sequentially selected
four learning rates: 1Ã—10âˆ’5,3Ã—10âˆ’5,5Ã—10âˆ’5, and 1Ã—10âˆ’4on
the GSM-IID dataset, then conducted training for 500 rounds each.
The results are presented in Figure 5, which indicate that the loss
function exhibits a sharp increase when the learning rate exceeds
the range supported by theory. Meanwhile, it is noteworthy that
our theory suggests that an optimal learning rate magnitude is
anchored at 1/âˆš
ğ‘‘in Section 3.3. In our chosen model, LLaMA-3B,
whereğ‘‘can be set to 3Ã—109, this gives 1/âˆš
ğ‘‘=1.826Ã—10âˆ’5, which
is approximately the learning rate we aim to use.
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni0000002a/uni00000036/uni00000030/uni00000010/uni0000002c/uni0000002c/uni00000027
/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000014/uni00000048/uni00000010/uni00000013/uni00000018
/uni00000016/uni00000048/uni00000010/uni00000013/uni00000018
/uni00000018/uni00000048/uni00000010/uni00000013/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014
Figure 5: Phenomenon of loss surge due to larger learning
rates.
C.4 The Impact of Heterogeneity on
Convergence
We present the results of processing the same dataset with different
splitters in Figure 6. It is observable that in the Dolly and CodeAl-
paca datasets, the LDA and Meta splitters perform better than IID,
with Meta being the best. Noting that the data classified by Meta
and LDA are non-i.i.d. , this indicates that higher data heterogeneity
is more conducive to model convergence.
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001a/uni00000014/uni00000011/uni0000001b/uni00000014/uni00000011/uni0000001c/uni00000015/uni00000011/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000027/uni00000052/uni0000004f/uni0000004f/uni0000005c
/uni00000036/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000057/uni00000048/uni00000055
/uni0000002c/uni0000002c/uni00000027
/uni0000002f/uni00000027/uni00000024
/uni00000030/uni00000048/uni00000057/uni00000044
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000014/uni00000011/uni00000014/uni00000018/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000026/uni00000052/uni00000047/uni00000048/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044
/uni00000036/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000057/uni00000048/uni00000055
/uni0000002c/uni0000002c/uni00000027
/uni0000002f/uni00000027/uni00000024
/uni00000030/uni00000048/uni00000057/uni00000044
Figure 6: Effects of different splitters on the same dataset.C.5 The Impact of Client Number
In Figure 7, we showcase the outcomes of federated learning on
the same Alpaca dataset with 3 clients and 8 clients, respectively.
This demonstrates that the model converges more stably with more
clients participating in the training. This conclusion also corre-
sponds to the theoretical results regarding the number of clients
ğ‘discussed in Section 3, i.e., an increase in ğ‘is beneficial for
reducing global convergence.
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni00000056/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001b/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000002c/uni0000002c/uni00000027
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000026/uni0000004f/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000056
/uni00000016/uni00000010/uni00000046/uni0000004f/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000056
/uni0000001b/uni00000010/uni00000046/uni0000004f/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000056
Figure 7: Effects of the number of clients.
C.6 The Impact of Model Size
To investigate the and versatility of FedMeZO across different model
sizes, based on LLaMA2-7B [ 53], we conducted experiments us-
ing the same experimental setup on three datasets: Dolly-Meta,
CodeAlpaca-LDA and GSM8K-IID. The experimental results are
shown in Table 4. The results show that FedMeZO on LLaMA2-7B
retains similar trends, while starts with lower Loss than 3B-model
by 0.2 and 0.12 in Dolly-Meta and GSM8K-IID. For all tasks, 7B-
modelâ€™s loss decreases more slowly, with reductions at 300 rounds
being only 34%, 67%, and 41% of the 3B-modelâ€™s in Dolly-Meta,
Code-LDA, and GSM8K-IID.
Table 4: Test loss on LLaMA-3B and LLaMA2-7B.
RoundsDolly-Meta Code-LDA GSM8K-IID
3B 7B 3B 7B 3B 7B
9 1.87 1.67 0.93 0.94 1.26 1.14
109 1.68 1.59 0.91 0.93 1.10 1.09
209 1.52 1.55 0.87 0.91 1.04 1.05
309 1.40 1.51 0.87 0.90 0.94 1.01
 
1838