Optimizing Novelty of Top-k Recommendations using Large
Language Models and Reinforcement Learning
Amit Sharma
Microsoft Research
Bengaluru, India
amshar@microsoft.comHua Li
Microsoft Bing Ads
Redmond, USA
huli@microsoft.com
Xue Li
Microsoft Bing Ads
Mountain View, USA
xeli@microsoft.comJian Jiao
Microsoft Bing Ads
Redmond, USA
jiajia@microsoft.com
ABSTRACT
Given an input query, a recommendation model is trained using
user feedback data (e.g., click data) to output a ranked list of items.
In real-world systems, besides accuracy, an important consideration
for a new model is novelty of its top-k recommendations w.r.t. an ex-
isting deployed model. However, novelty of top-k items is a difficult
goal to optimize a model for, since it involves a non-differentiable
sorting operation on the modelâ€™s predictions. Moreover, novel items,
by definition, do not have any user feedback data. Given the seman-
tic capabilities of large language models, we address these problems
using a reinforcement learning (RL) formulation where large lan-
guage models provide feedback for the novel items. However, given
millions of candidate items, the sample complexity of a standard RL
algorithm can be prohibitively high. To reduce sample complexity,
we reduce the top-k list reward to a set of item-wise rewards and
reformulate the state space to consist of âŸ¨query, itemâŸ©tuples such
that the action space is reduced to a binary decision; and show that
this reformulation results in a significantly lower complexity when
the number of items is large. We evaluate the proposed algorithm
on improving novelty for a query-ad recommendation task on a
large-scale search engine. Compared to supervised finetuning on
recent <query, ad>pairs, the proposed RL-based algorithm leads
to significant novelty gains with minimal loss in recall. We obtain
similar results on the ORCAS query-webpage matching dataset and
a product recommendation dataset based on Amazon reviews.
CCS CONCEPTS
â€¢Information systems â†’Novelty in information retrieval;
Online advertising.
KEYWORDS
Recommendation System, Novelty, Large Language Models, Rein-
forcement Learning
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671618ACM Reference Format:
Amit Sharma, Hua Li, Xue Li, and Jian Jiao. 2024. Optimizing Novelty of
Top-k Recommendations using Large Language Models and Reinforcement
Learning . In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671618
1 INTRODUCTION
Given a userâ€™s profile or an input query, the recommendation prob-
lem is to fetch a ranked list of top-k items based on a task-specific
goal. We consider the retrieval layer of a recommendation system,
where the input is typically millions of candidate items and output
is hundreds of ranked items (e.g., k=200). For the retrieval layer,
semantic relevance is a common goal and models are trained to
rank relevant items higher [ 9]. A common way to train such recom-
mendation models is to use supervised learning on user feedback
data such as clicks, using losses such as contrastive learning that
encourage representations of clicked query-item pairs to be closer
to each other than negative (or random) query-item pairs [8].
However, in addition to relevance, real-world recommendation
systems typically have additional goals to optimize for the pre-
dicted top-k items. For instance, an important goal is that a can-
didate model predicts top-k items that are novel w.r.t. the existing
deployed models in the system [ 15]. Novelty is a desirable property
since it can avoid showing repetitive or redundant items to a user,
enhance global coverage over all available items, and help avoid
any systematic bias towards previously clicked items in the sys-
tem [ 11]. However, it is difficult to train a model to optimize novelty
of top-k items since evaluating novelty requires a non-differentiable
sorting operation to obtain the top-k list. Moreover, novel items,
by definition, do not have any user feedback data corresponding
to the particular query. As a result, getting relevance feedback on
novel items requires conducting an online experiment showing
exploratory, novel items to the user, which can be a costly and
infeasible procedure for many systems.
To obtain scalable feedback for novel items, we utilize recent
results that show that large language models (LLMs) like GPT-3.5
or GPT-4 can match or surpass quality of crowd-sourced relevance
feedback for recommendation tasks, such as matching user input
to ad keywords [ 10] or matching user history to suggested movies
and games [ 12]. Specifically, we propose that LLMs can be used as
reward models to provide relevance feedback for novel items, using
5669
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Amit Sharma, Hua Li, Xue Li, and Jian Jiao
Figure 1: Our goal is to optimize retrieval models for novelty. Left: Supervised learning with a contrastive loss is unable to
optimize novelty directly since it involves a non-differentiable top-k sorting operation. Right: the proposed RL approach with
explicit optimization for the novelty of top- ğ‘˜retrieved items (w.r.t. to a base model) using LLM-based relevance feedback.
an appropriate task-specific prompt. As a result, (noisy) relevance
feedback for novel items can be obtained at scale and be used for
both offline model training and evaluation, with human feedback
restricted to only evaluating the final model in an A/B test.
For handling non-differentiability of the novelty metric, we con-
sider a reinforcement learning (RL) formulation, as in past work on
optimizing metrics such as diversity [ 20] and popularity [ 32,33]
of top-k predicted items. Specifically, the retrieval problem can be
framed as a one-step RL problem where the input query is the state,
the top-k predicted items constitute the action, and their relevance
and novelty constitute the environment reward. However, the large
action space in retrieval tasksâ€”equal to the number of available
items that can be predicted for an input query (typically millions)â€”
presents a challenge due to the data requirements for RL algorithms.
While we use a policy gradient RL algorithm that does not explic-
itly parameterize the action space and hence is better suited for
large action spaces, the finite sample convergence results of policy
gradient algorithms [ 23] show a direct dependence of policy error
on the square of the action space size. Consequently, the resultant
algorithms are reported to not work well beyond tens of items [ 22].
To address the issue of optimizing over large action spaces, we
make two contributions. First, rather than the standard practice
of training RL algorithms from scratch for recommendation sys-
tems [ 16,32,33,38], we argue that it is more practical to consider
RL as a tool to finetune an existing supervised learning model that
has been trained on user feedback data. Such an approach has led to
substantial gains in training image [ 28] and language [ 26] models
and we extend it for recommendation systems. We also provide a
formal justification for this line of work: assuming that the exist-
ing model has a higher probability of selecting the optimal action
(item) for a query than a uniformly random policy, we show that
initializing from an existing model reduces the sample complexity
requirements for a policy gradient algorithm.
Second, we propose a reformulation of the problem that further
reduces the sample complexity. Rather than considering a query as
the state, we consider the (query, item) pair as a state. As a result,
the action space is converted to a binary space: whether the item
is preferred by the task-specific goal (e.g., relevance and novelty)
or not. Theoretically, our formulation reduces the dependence of
convergence error from quadratic to linear in the action space,which is a substantial reduction whenever the action space is large.
Crucially, even though the action space is binary given a query-
item pair, policy optimization can use rewards based on the top- ğ‘˜
retrieved list from the current policy, which was not possible in
supervised learning. At the same time, the reduction leads to an
empirical advantage: we can develop a scalable, batch-wise training
algorithm for optimizing the policy using loss functions that are
well-suited for large-scale recommendation, such as those based on
contrastive learning. We call the resultant algorithm, PG-Ret, Policy
Gradient for large-scale Retrieval.
We evaluate PG-Ret on three datasets: a proprietary dataset for
query-ad recommendation task, a public dataset for query-webpage
matching task, and a product recommendation dataset based on
Amazon reviews. In the first dataset, the task is produce top-k items
that are novel compared to a given production model on a commer-
cial search engine. The training data consists of recently collected
<query, ad>pairs from the search engineâ€™s logs and has 33M actions.
We find that PG-Ret leads to significant improvement (2X-5X) in
novelty of top-k retrieved items compared to a supervised model
trained on the same training data, while incurring minimal drop in
precision and recall. Online A/B test shows a statistically significant
gain of 1% in retrieval density, the average number of relevant ad
keywords fetched per query. The second dataset is a public dataset,
ORCAS, for matching queries to webpage titles. We obtain a similar
result: PG-Ret leads to over 2X improvement in novelty for the
top-20 items while incurring a small drop in recall and precision.
Finally, PG-Ret can also be applied to general recommendation
scenarios beyond query-based tasks. We apply it to an Amazon
user-product recommendation task and find significant gains in
novelty compared to supervised finetuning.
To summarize, we propose a technique based on the policy gra-
dient algorithm that is simple to implement (see Algorithm 1) and
shows the benefit of finetuning existing models with RL for task-
specific goals such as novelty. Our contributions include,
â€¢A simple, model-agnostic algorithm to finetune retrieval
models that works for large action spaces (millions of actions)
and arbitrary task-specific goals.
â€¢Leveraging relevance feedback from LLMs; thus allowing
for optimization of novel items directly since novel items,
by definition, do not have user feedback data.
5670Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢Theoretical and empirical justification of the method for
increasing novelty in recommendation systems, including
evaluation of the method for predicting ad keywords on a
commercial search engine.
2 RELATED WORK
2.1 Enhancing novelty of top-k results
Outputting novel items compared to existing algorithms is an impor-
tant goal for recommendation systems [ 31]. A common approach is
to consider a multi-objective optimization problem [ 29] with rele-
vance and novelty as the two objectives [ 6,14]. Recently, reinforce-
ment learning has been used for novelty optimization since RL can
be applied to non-differentiable objectives like novelty [ 16,33,38].
However, past work uses approximations of novelty, such as an
item being less popular [ 32â€“34] or belonging to less popular cate-
gories [ 38], to enhance novelty, since truly novel itemsâ€”those that
have not been shown by the current systemâ€”would not have any
user feedback data for training. To ascertain relevance of truly novel
items, heuristics like estimating the curiosity of a user (more curios-
ity admits more novel items as relevant) have been proposed [ 40],
but no systematic way to estimate relevance exists. Consequently,
without relevance feedback, there exists a direct tradeoff between
optimizing novelty and relevance of a top-k list. In this work, we
use the semantic capabilities of LLMs to propose a general, scal-
able method to estimate relevance of novel items. As a result, we
can directly optimize for the novelty objective without sacrificing
relevance.
Note that encouraging novelty in top-k items is different from
addressing the cold-start problem [ 30,37]. Novel items are defined
wrt. a query whereas cold-start items are defined globally for a
system. In many cases, a novel item wrt. a query may be a popular
item globally. As a result, methods for the cold-start problem may
not directly apply to novelty optimization.
2.2 RL with large action spaces
Given a query, information retrieval can be divided into two stages [ 13];
1) retrieval of relevant items from a large candidate pool of items;
2) ranking the retrieved items to select a smaller set of items that
are shown to the user. RL algorithms find many applications in the
ranking phase, including contextual bandits [ 19], markov decision
processes [ 35], and policy gradient algorithms [ 27,39]. Many of
these algorithms introduce a task-specific loss to the ranking prob-
lem. However, applying these techniques to the retrieval phase is
challenging because of the large number of candidate items (ac-
tions).
Large action spaces is a general challenge for RL beyond retrieval
models [ 7], even when using policy gradient, a method that does
not explicitly parameterize the action space and thus is better suited
for higher number of actions. For sequential recommendation prob-
lems, appropriately parameterizing the policy and using off-policy
correction helps in scaling to large actions [ 2]. For one-step RL
problems, recent theoretical work in contextual bandits [ 21,41]
tries to address the large actions problem. However, the focus of
these approaches is on obtaining an optimal solution from scratch,
which may be difficult in practice and misses the complementarybenefits of supervised learning over user feedback. Instead, we fine-
tune an existing supervised model, as done by [ 28] for computer
vision models.
2.3 LLMs for information retrieval
Recently, LLMs like GPT-3.5 have been applied to retrieval tasks in
a zero-shot setting with encouraging results [ 1,5,12]. Instead of
using compute-intensive LLMs directly for the retrieval task, here
we aim to use LLMs as reward models to train an efficient, small
retrieval model.
3 OPTIMIZING NOVELTY W/ LLM FEEDBACK
3.1 Problem statement: Novelty optimization
Commonly used retrieval models use a bi-encoder architecture [ 8],
where the same neural network model embeds a query and item
into a common representation space. The top-k items are selected
based on the nearest neighbors to a query, as measured by a suitable
distance function over the embeddings (e.g., cosine similarity). The
encoderğœ™is typically optimized using a variant of contrastive learn-
ing, encouraging that positive <query,item >pairs in the training
set should be closer in embedding space than non-positive pairs.
Non-positive pairs may be random pairs [ 8] or mined from train
data[ 4]. Thus, given a set of queries ğ‘‹and itemsğ‘, a train dataset
ğ·âˆ¼D with positive query-item pairs, and a similarity function
sim, the trained retriever Ë†ğœ™can be written as (using the InfoNCE
contrastive loss function [25]),
Ë†ğœ™=arg min
ğœ™âˆ’logâˆ‘ï¸
(ğ‘¥,ğ‘§)âˆˆDexp(sim(ğœ™(ğ‘¥),ğœ™(ğ‘§)))Ã
ğ‘§â€²âˆˆğ‘›ğ‘’ğ‘”(ğ‘¥)exp(sim(ğœ™(ğ‘¥),ğœ™(ğ‘§â€²)))
y(ğ‘¥)=Topkğ‘§âˆˆğ‘sim(Ë†ğœ™(ğ‘¥),Ë†ğœ™(ğ‘§)) (1)
where y=[ğ‘¦1,ğ‘¦2,Â·Â·Â·,ğ‘¦ğ‘˜]refers to the sequence of top-k items
returned by a trained encoder Ë†ğœ™andTopk is a non-differentiable
ranking operation. At inference time, given a query, the trained
encoder Ë†ğœ™is used to compute the similarity with each item and the
top-k closest items are returned as the prediction.
Given the trained encoder Ë†ğœ™, our goal is to finetune the encoder
such that the novelty of its top-k items compared to a base model
is optimized, while ensuring that the accuracy of the encoder does
not decrease substantially. Specifically, we assume that there is an
existing base retrieval model ğœ“deployed in the production recom-
mendation system and we want to deploy another retrieval model
that introduces novel items. Both the base model and the new, fine-
tuned model would run in parallel and their results can be merged
for the downstream ranking layer. Formally, novelty@k is defined
as,
Definition 1. Novelty: Given a query ğ‘¥, Novelty@k(ğ‘¥,ğœ™,ğœ“,ğ¿)
is defined as the number of items in top-k predictions of a candidate
modelğœ™that do not exist in top-L predictions of the base model ğœ“.
Typically,ğ¿is set as the number of items that the retrieval layer
sends downstream to the ranking layer (e.g., ğ¿=200in our exper-
iments). If a candidate model outputs an item that is beyond the
top-L of the base model, then it will result in a new item added to
the recommendation systemâ€™s ranking layer. Note that ğ‘˜need not
be the same as ğ¿. For efficiency reasons, we may be interested in
5671KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Amit Sharma, Hua Li, Xue Li, and Jian Jiao
novelty at lower ğ‘˜(e.g.,ğ‘˜=50), under a setting where the candidate
model only adds ğ‘˜additional items to be sent to the downstream
ranking layer. While our definition assumes a single base model,
it can be easily extended to multiple deployed models by pooling
their top-ğ¿predictions for novelty comparison.
Note that novelty of top-k items of a candidate algorithm is
different from its diversity [ 31], which measures the distance of
items within the top-k list produced by the candidate algorithm.
3.2 RL formulation using policy gradient
Novelty@k , as defined above, cannot be optimized directly since it
includes a non-differentiable component w.r.t. the encoderâ€™s param-
eters, the top-k sorting operation over items. As in past work on
recommendation systems [ 16,33,38], a natural solution for non-
differentiable rewards is to use reinforcement learning. Formally,
our problem can be formulated as a one-step RL setting. The query
is considered as the state and the set of top-k items as the action. A
policyğœ‹:ğ‘‹â†’{ğ‘ğ‘˜:ğ‘ğ‘˜âŠ†ğ‘}is a function that outputs a set of
top-k items given a query. For each action selected by the policy,
the environment provides reward feedback on the <state,action
>pair. For instance, in the query-ad recommendation task, userâ€™s
input query is the state, the top-k predicted ads are the action by the
policy, and the environment provides reward based on the query
and its top-k predicted ads, e.g., based on the novelty and relevance
of the predicted ads. Given a reward function R, the task to learn a
policyğœ‹ğœƒ(parameterized by ğœƒ), that optimizes,
max
ğœƒEğ‘¥âˆ¼DEyâˆ¼ğœ‹ğœƒ(ğ‘¥)R(ğ‘¥,y) (2)
In our work, we consider a pre-trained encoder ğ‘“ğœƒ:ğ‘‹âˆªğ‘â†’Rğ‘‘
to correspond to the initial policy ğœ‹0
ğœƒ. For simplicity, given an en-
coder/policy at any stage of training, the top-k items yare assumed
to be independently sampled from the discrete action probability
distribution, ğœ‹ğœƒ:ğ‘‹â†’ğ‘;ğ‘¦ğ‘—âˆ¼ğœ‹ğœƒ(ğ‘§|ğ‘¥)âˆ€ğ‘—âˆˆ{1,2,3..ğ‘˜}; where
ğœ‹ğœƒ(ğ‘§|ğ‘¥)=softmaxğ‘sim(ğ‘“(ğ‘¥),ğ‘“(ğ‘§)). We useğœ‹ğœƒ(y|ğ‘¥)to denote the
top-k items generated using the ğœ‹ğœƒpolicy.
As in past work on using RL in recommendation systems [ 2,24,
27], we use a policy gradient algorithm to optimize the novelty re-
ward. Policy gradient algorithms are well-suited for recommender
systems since they do not explicitly parameterize the action space
(thus allowing a large number of actions) and can directly update
the parameters of the encoder ğ‘“ğœƒ. Specifically, we use the REIN-
FORCE [36] algorithm that depends on a Monte Carlo approxima-
tion of the reward expectation from Eqn. 2.
Eğ‘¥âˆ¼DEyâˆ¼ğœ‹ğœƒ(ğ‘¥)R(ğ‘¥,y)â‰ˆ1
ğµğµâˆ‘ï¸
ğ‘–=1R(ğ‘¥(ğ‘–),y(ğ‘–)) (3)
whereğ‘¦(ğ‘–)
ğ‘—âˆ¼ğœ‹ğœƒ(ğ‘¥(ğ‘–))âˆ€ğ‘—âˆˆ{1,2,3,Â·Â·Â·,ğ‘˜}and B is the batch size.
The loss gradient is given by,
âˆ‡Eğ‘¥âˆ¼DEyâˆ¼ğœ‹ğœƒ(ğ‘¥)R(ğ‘¥,y)=Eğ‘¥âˆ¼DEyâˆ¼ğœ‹ğœƒ(ğ‘¥)R(ğ‘¥,y)âˆ‡logğœ‹ğœƒ(y|ğ‘¥)
â‰ˆ1
ğµğµâˆ‘ï¸
ğ‘–=1R(ğ‘¥(ğ‘–),y(ğ‘–))âˆ‡logğœ‹ğœƒ(y(i)|ğ‘¥(ğ‘–)) (4)
whereğ‘¦(ğ‘–)
ğ‘—âˆ¼ğœ‹ğœƒ(ğ‘¥(ğ‘–))âˆ€ğ‘—âˆˆ{1,2,3,Â·Â·Â·,ğ‘˜}. Since the reward is one-
step, the above optimization can be interpreted to have a simplegoal: increase the probability of items that occur in a k-sequence
with high reward, and decrease the probability of items that occur
in a k-sequence that obtains low reward. Note that we use the
REINFORCE algorithm for simplicity, but any other policy gradient
algorithm can be used.
3.3 LLMs make novelty optimization practical
While the formulation is reasonable, there is a key limitation: novel
items, by definition, do not have any user feedback data since they
were never shown to users by the base production model for that
query. Hence, if we use only the log-based training data for Eqn. 4,
relevance feedback for the novel items would be missing and thus
no novel itemâ€™s reward can be computed.
In this paper, we show that LLMs help avoid this limitation by
providing relevance feedback for novel query-item pairs. LLMs
such as GPT-3.5 and GPT-4 have been shown to provide a substan-
tial improvement in semantic understanding compared to existing
encoder-based models. For instance, recent work shows that GPT-
3.5 can match or surpass crowd-sourced human labellers in their
accuracy on labelling [ 10] or ranking [ 12] retrieval outputs for
relevance. In addition, even though the relevance criterion for a
correct recommendation may differ for different tasks, we need not
train separate relevance models. For example, while the criteria for
matching books to other books may be different from matching
queries to ads, which in turn may be slightly different for matching
queries to webpages; a single LLM like GPT-3.5 can be used for pro-
viding relevance feedback for all these domains by simply changing
the prompt. Hence, the accuracy and universality of LLMs make
it possible to obtain relevance feedback for arbitrary query-item
pairs and optimize novelty directly for retrieval tasks.
3.4 The challenge with large action spaces
While LLMs provide a solution to the problem of relevance feed-
back for novel query-item pairs, another key challenge is the large
number of potential query-item pairs to label since available items
are typically in the millions. Below we show that the number of
relevance reward samples needed for a policy gradient algorithm
to obtain a desired accuracy (sample complexity ) increases propor-
tional to the square of the number of actions. The result is based on
applying finite sample convergence bounds [ 23] for policy gradient
algorithms to the retrieval setup.
Letğœ‹âˆ—denote the optimal policy that maximizes Eqn 2 and ğ‘¡refer
to the steps of the optimization. We consider a one-step Markov
Decision Process where each query corresponds to a state and the
actions are the top-k predicted items. Hence, for each state, we
only need to optimize the current reward. We assume a uniform
probability for sampling each state (query).
Assumption 1 (from [ 23]).(Sufficient exploration). The initial state
ğœ‡distribution satisfies minğ‘ ğœ‡(ğ‘ )>0.
Assumption 2. For each state, the initial policyâ€™s probability for se-
lecting the optimal action is better than random (uniform probability)
within some multiplicative constant ğœŒâ‰¥1:ğœ‹ğœƒ0(yâˆ—(ğ‘¥)|ğ‘¥)>1
ğœŒğ´âˆ€ğ‘¥âˆˆ
ğ·where yâˆ—(ğ‘¥):=arg max yğœ‹âˆ—(y|ğ‘¥).
Proposition 1. Let Assumptions 1 and 2 hold and let {ğœƒğ‘¡}ğ‘¡â‰¥1
be generated using the standard policy gradient update: ğœƒğ‘¡+1â†
5672Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ğœƒğ‘¡+ğœ‚ğœ•ğ‘‰ğœ‹ğœƒğ‘¡(ğœ‡)
ğœ•ğœƒğ‘¡whereğ‘‰ğœ‹ğœƒğ‘¡(ğœ‡)=Eğ‘¥âˆ¼DEyâˆ¼ğœ‹ğœƒğ‘¡(.|ğ‘¥)R(ğ‘¥,y). Then,
for allğ‘¡â‰¥1, withğœ‚=1/8,
Eğ‘¥âˆ¼D[(ğœ‹âˆ—(ğ‘¥)âˆ’ğœ‹ğœƒğ‘¡(ğ‘¥))ğ‘‡r(ğ‘¥)]â‰¤16ğ‘†ğ´2ğœŒ2
ğ‘¡1
ğœ‡âˆ(5)
whereğ‘†=|ğ‘‹|is the number of states and ğ´is the number of actions.
Proof is in Appendix. The proof uses Thm. 4 from [ 23] for a
Markov Decision Process and adapts it to the single-step problem
and additionally uses Assumption 2 to express the bound in terms
ofğœŒandğ´. Since the policy outputs a list of top-k items, the total
number of possible actions is ğ´=ğ¶(|ğ‘|,ğ‘˜), indicating a high sample
complexity. In the next section, we describe how we reduce the
effective sample complexity and derive a practical algorithm.
4 LARGE ACTION POLICY GRADIENT
Proposition 1 shows that a naive application of the policy gra-
dient algorithm will be slow to converge to the optimal reward-
maximizing solution due to a combinatorially large action space.
First, we show that the combinatorial action space ğ´=ğ¶(|ğ‘|,ğ‘˜)can
be decomposed into item-wise action space, ğ´=|ğ‘|whenever the
reward over top-k items can be decomposed as an additive sum over
item-wise rewards. We show that most common novelty reward
functions satisfy this property. Second, we provide a reformulation
of the RL problem that reduces the action space to a binary decision
and further increases the rate of convergence.
4.1 Reduction from Top-k to item-wise rewards
As stated in Section 3, the reward function is a combination of
novelty and relevance rewards. We assume that both novelty and
relevance rewards compose additively. That is, given a query ğ‘¥,
ğ‘ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦ @ğ‘˜(ğ‘¥,ğœ™,ğœ“,ğ¿)=Ãğ‘˜
ğ‘—=1ğ‘ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦(âŸ¨ğ‘¥,ğ‘§ğ‘—âŸ©,ğœ™,ğœ“,ğ¿), and we have
ğ‘…ğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’ @ğ‘˜(ğ‘¥,ğœ™)=Ãğ‘˜
ğ‘—=1ğ‘…ğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’(âŸ¨ğ‘¥,ğ‘§ğ‘—âŸ©,ğœ™); whereğ‘§ğ‘—are the
individual item predictions and ğ‘…ğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’ @ğ‘˜is typically recall@k
or precision@k. As a result, we can reformulate the action space
to consist of individual items, ğ´=|ğ‘|. In expectation, maximizing
the novelty and relevance reward function for ğ‘—âˆˆ[1,ğ‘˜]separately
would imply maximizing the top-k reward.
While the action space is reduced in size, a key benefit of our
formulation is that the reward can still be a function of the top-k
retrieved items from some model. This is because the environment
can decide the reward based on whether the item is a part of the
top-k items for the query. Recall that, given a state (query) ğ‘¥and an
action (item) ğ‘§âˆˆğ‘, the novelty reward is dependent on whether
theğ‘§is part of the top-k items returned by the base model ğœ“. As an
example, we provide a simple reward function combining item-wise
relevance and novelty w.r.t. ğœ“. Given any query ğ‘¥and itemğ‘§, and a
relevance oracle, Rel(i.e., an LLM), the reward is given by,
Rğ‘(ğ‘¥,ğ‘§)=ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³1 ifRel(ğ‘¥,ğ‘§)=1andğ‘§âˆ‰ğ‘¡ğ‘œğ‘ğ‘˜(ğ‘¥,ğœ“)
âˆ’1 ifRel(ğ‘¥,ğ‘§)=0andğ‘§âˆˆğ‘¡ğ‘œğ‘ğ‘˜(ğ‘¥,ğœ“)
âˆ’0.5ifRel(ğ‘¥,ğ‘§)=1andğ‘§âˆˆğ‘¡ğ‘œğ‘ğ‘˜(ğ‘¥,ğœ“)
âˆ’1 ifRel(ğ‘¥,ğ‘§)=0andğ‘§âˆ‰ğ‘¡ğ‘œğ‘ğ‘˜(ğ‘¥,ğœ“)(6)
We can see that the reward function penalizes irrelevant items
and relevant items that are not novel w.r.t. the base model, while
encouraging relevant items that are novel. However, in practice, theLLM-based relevance function may be noisy. In particular, as higher
relevance items are likely to be sampled more, false negatives can
be a problem. Therefore, for noisy oracles, we propose a simpler
reward function that is only activated for relevant items but not for
irrelevant items predicted by the oracle.
Rğ‘(ğ‘¥,ğ‘§)=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³1 ifRel(ğ‘¥,ğ‘§)=1andğ‘§âˆ‰ğ‘¡ğ‘œğ‘ğ‘˜(ğ‘¥,ğœ“)
âˆ’0.5ifRel(ğ‘¥,ğ‘§)=1andğ‘§âˆˆğ‘¡ğ‘œğ‘ğ‘˜(ğ‘¥,ğœ“)
0 otherwise(7)
4.2 Reduction to binary action space
To further improve the convergence rate, we consider a different
RL formulation where the state is âŸ¨query, itemâŸ©pair and the pol-
icy outputs the probability of selecting the item. The item-wise
reward function changes slightly to accommodate the two actions
of selecting the item (1) or not (0). Assuming the same reward
logic from Eqn. 6, the reward function becomes, R((ğ‘¥,ğ‘§),ğ‘)=
ğ‘Rğ‘(ğ‘¥,ğ‘§)+(1âˆ’ğ‘)(âˆ’Rğ‘(ğ‘¥,ğ‘§)). Intuitively, if the policy selects the
item for a query, then its reward for the action is proportional to
Rğ‘. Otherwise, if it does not select the item, then its reward is
proportional to negative of Rğ‘. The corresponding gradient is,
âˆ‡Eğ‘¥,ğ‘§âˆ¼DEaâˆ¼ğœ‹â€²
ğœƒ(ğ‘¥,ğ‘§)R((ğ‘¥,ğ‘§),ğ‘) (8)
â‰ˆ1
ğµğµâˆ‘ï¸
ğ‘–=1[ğ‘(ğ‘–)Rğ‘(ğ‘¥(ğ‘–),ğ‘§(ğ‘–))âˆ’( 1âˆ’ğ‘(ğ‘–))Rğ‘(ğ‘¥(ğ‘–),ğ‘§(ğ‘–))]âˆ‡logğœ‹â€²
ğœƒ(ğ‘(ğ‘–)|ğ‘¥(ğ‘–),ğ‘§(ğ‘–))
whereğœ‹â€²
ğœƒ(ğ‘=1|ğ‘¥,ğ‘§)=ğœ‹ğœƒ(ğ‘¥,ğ‘§)=softmaxğ‘sim(ğ‘“ğœƒ(ğ‘¥),ğ‘“ğœƒ(ğ‘§))and
ğœ‹â€²
ğœƒ(ğ‘=0|ğ‘¥,ğ‘§)=1âˆ’ğœ‹ğœƒ(ğ‘¥,ğ‘§)=1âˆ’softmaxğ‘sim(ğ‘“ğœƒ(ğ‘¥),ğ‘“ğœƒ(ğ‘§)).
With this formulation, the number of states increases to ğ‘†ğ´
but the number of actions reduces to 2. As we show below, the
convergence rate is significantly faster since the error now grows
linearly with A rather than quadratic.
Proposition 2. With the new formulation, under the assumptions
of Proposition 1, for all ğ‘¡â‰¥1,
Eğ‘¥âˆ¼D(ğœ‹âˆ—(ğ‘¥)âˆ’ğœ‹ğœƒğ‘¡(ğ‘¥))ğ‘‡r(ğ‘¥)â‰¤64ğ‘†ğ´ğœŒ2
ğ‘¡1
ğœ‡âˆ(9)
Note that in practice, ğœŒmay be higher for the binary action for-
mulation since there are only 2 actions. Assuming a â€œgood enough"
supervised policy, conservative value for ğœŒmay beâˆ¼50, implying
that probability of the optimal action under supervised policy is
alwaysâ‰¥1/(2Ã—50)=0.01. Even under this conservative estimate,
as long as the number of actions is of the order of millions, ğœŒ2<<ğ´
and hence the convergence rate in Proposition 2 would be signifi-
cantly faster. In other words, as long as ğœŒ2<<ğ´, the binary-action
policy gradient algorithm will converge at a faster rate.
For maximizing a reward based on novelty and relevance such
as Eqn. 6, the ğœŒparameter in Proposition 2 also implies that using
the base policy as the initial policy may not be the best choice. The
base policy is expected to be significantly better than a random
policy at relevance, so it will assign higher than random probability
to relevant itemsâ€”both novel and not novel items. However, within
relevant items, by definition, its probability for novel items will be
lower than that of the not novel items. To increase the probability
of the optimal action under the initial policy even further (and
decreaseğœŒ), we can use additional training data to finetune the base
policy using supervised learning (e.g., InfoNCE loss from Eq. 1).
We call this model the Supervised Model. As long as the relevance
5673KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Amit Sharma, Hua Li, Xue Li, and Jian Jiao
accuracy does not decrease, randomness in the training procedure
should shift the order of top-k items under the supervised model.
As a result, the supervised model may retain the property of being
better than a random policy at relevance (just like the base policy)
but may also yield higher probability to some novel items under
the base policy. Therefore, whenever additional training data is
available, such a supervised model is expected to have a lower ğœŒ
and we recommend to use it as the initial policy.
4.3 Proposed algorithm: PG-Ret
The above discussion indicates the following conditions for a fast
convergence rate with policy gradient, 1)the action space should
be small; and 2)the initial policy should be as close to the optimal
as possible. We addressed the first condition through a binary-
action formulation of the problem. For the second condition, we
proposed initializing the policy optimization with a policy trained
using supervised learning (i.e., using Eq. 1).
Algorithm 1 shows the resultant training algorithm. In each
batch,ğµquery-item pairs (states) are sampled and the reward is
computed for each state. Using Eqn. 8, we compute the gradient
and update the policy parameters ğœƒat the end of the batch. For each
application, one needs to specify the sampling procedure for query-
item pairs, the reward function, and the estimation procedure for
ğœ‹â€²(ğ‘|ğ‘¥,ğ‘§).
Sampling query-item states. Since items are also a part of the
state in our proposed formulation, a key question is how to sample
the statesâŸ¨ğ‘¥,ğ‘§âŸ©. To map to the original policy gradient formulation,
we can sample queries ğ‘¥randomly from the train dataset for each
batch. For each query, we compute the similarity scores with all
items using the encoder ğ‘“ğœƒcorresponding to the current policy and
then sample items proportional to their score. Note that we are not
restricted to only sampling items proportional to their similarity
score (since items are not actions now). Therefore, we also add
exploration by sampling items from another retrieval model (trained
independently). For example, for novelty, a natural choice is to
sample from predictions of the base model, restricting the sampling
only to items that are ranked beyond top-L. Such items will be
novel by definition and thus we only need to check for relevance.
More generally, we may use any pretrained encoder suitable for the
task. Finally, there is a risk that the policy overfits to the reward
from the relevance oracle and â€œforgetsâ€ the true user feedback data
on which the supervised policy was trained [ 26]. Thus, we should
also add the user feedback data (ğ‘¥,ğ‘§)âˆˆğ·during training. To this
end, in practice, all three sources are combined stochastically: we
sample a query ğ‘¥randomly from the dataset and then sample either
ğ‘§âˆ¼ğœ‹ğœƒğ‘¡(ğ‘¥)with probability ğ›¼;ğ‘§âˆ¼ğœ‹ğœƒğ‘’ğ‘¥ğ‘ğ‘™ğ‘œğ‘Ÿğ‘’ğ‘¡(ğ‘¥)with probability
ğ›½; orğ‘§âˆ¼ğ‘ƒD(ğ‘¥)with probability 1âˆ’ğ›¼âˆ’ğ›½.
Estimating ğœ‹â€²(ğ‘|ğ‘¥,ğ‘§).Whileğœ‹â€²(ğ‘|ğ‘¥,ğ‘§)is defined as a softmax op-
eration, computing the softmax over all items is a compute-intensive
procedure. Moreover, training procedures for recommendation sys-
tems have benefitted from using contrastive losses. Therefore, we
we implement an approximation of the softmax using contrastive
losses. A straightforward approximation is to only use the items in
the current batch as negatives (random in-batch negatives). How-
ever, since we initialize with a well-trained supervised policy, wemay find that most of the contrastive losses are zero since the cho-
sen item is already closer to the query than random negatives. To
speed up training, we use negatives that are aimed at optimizing
novelty. Specifically, for each query, we use the top-M items re-
turned by the base model as negatives. ğ‘€is a hyperparameter; too
highğ‘€may destroy all relevance. Therefore, we define two kinds of
losses: 1)Aggressive-Novelty: InfoNCE loss from Eqn. 1 with top-M
negatives (typically M is small, e.g., ğ‘€=5);2)Conservative-Novelty:
Triplet loss [ 17], boundedâˆˆ[0,1]and margin=0 (effectively ğ‘€=1).
Efficiency considerations. Note that to compute top-k items or
sample items for a given query, the entire set of actions have to be
encoded by the current encoder ğ‘“ğœƒ. For computationally efficiency,
we fix the item encoder to be the initial encoder and only update the
query encoder. This avoids having to recompute the item embed-
dings for each query in the batch. That is, only the query encoder
is updated during policy optimization.
Algorithm 1 Large action PG
1:Input: Initial Policy ğœ‹ğœƒ, Base Model ğœ“, Training dataset ğ·, Rele-
vance Oracle rel, Number of epochs ğ‘, Batch sizeğµ, Learning
rateğœ‚,ğ›¼,ğ›½âˆˆ[0,1]
2:Output: Trained policy Ë†ğœ‹ğœƒ.
3:forepoch=1,2,3..N do
4:forğ·ğ‘ğ‘ğ‘¡ğ‘â„âˆ¼ğ·do
5:ğ¿=0
6:ğ‘–=0
7:(ğ‘¥,ğ‘§)âˆ¼ğ·
8: whileğ‘–<ğµdo
9:ğ‘âˆ¼ğœ‹â€²
ğœƒ(ğ‘¥,ğ‘§)// Sample Action
10:ğ‘Ÿğ‘=Rğ‘(ğ‘¥,ğ‘§,topk(ğ‘¥,ğœ“),rel(ğ‘¥,ğ‘§))// Reward depends
on top-k items and relevance oracle
11:ğ‘Ÿ=ğ‘Ÿğ‘ğ‘+(âˆ’ğ‘Ÿğ‘)(1âˆ’ğ‘)
12:ğ¿=ğ¿âˆ’ğ‘Ÿlogğœ‹â€²
ğœƒ(ğ‘|ğ‘¥,ğ‘§)
13:ğ‘–=ğ‘–+1
14:ğ‘¥âˆ¼ğ·
15:ğ‘§âˆ¼ğœ‹ğœƒ(ğ‘¥)(wProbğ›¼),âˆ¼ğœ‹ğœƒğ‘’ğ‘¥ğ‘ğ‘™ğ‘œğ‘Ÿğ‘’(ğ‘¥)(wProbğ›½),orâˆ¼
ğ·(wProb 1âˆ’ğ›¼âˆ’ğ›½)
16: end while
17:ğœƒ=ğœƒâˆ’ğœ‚âˆ‡ğ¿// Can use any gradient optimizer
18: end for
19:end for
5 EVALUATION
We evaluate PG-Ret on increasing novelty of the top-k items of a
retrieval model compared to an existing modelâ€™s output.
5.1 Setup: Datasets, Metrics, and Baselines
Setup. We consider the production setup for a recommendation
system, wherein there is an existing retrieval algorithm in produc-
tion. We call this model the base model. Typically, this model is
trained on millions of query-item clicked pairs collected from log
data. We assume access to a small amount of new training data
(e.g., query-item pairs collected from log data after the base model
has been deployed) and evaluate different ways to produce a new
retrieval model with high novelty compared to the base model. We
5674Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Dataset Train inputs Test inputs # Actions
Query-Keyword 1.23M 6.2K 33.2M
ORCAS 1M 8.5K 1.07M
Table 1: Statistics for the Query-Keyword datasets from a
commercial search engine and the public ORCAS dataset.
consider an independent test set for evaluation. As in prior work [ 4],
the candidate pool of items remains the same across train and test
datasets, typically of the order of millions of items.
Datasets. On the query-document matching task, we use a dataset
from a commercial search engine and a public dataset (see Table 1).
We also use a Amazon dataset for product recommendation.
â€¢Query-keyword recommendation. This dataset contains queries
and ad keywords from a commercial search engineâ€™s logs. Given
a query, the goal is to predict ad keywords that share the same in-
tent or a more general intent than the query (i.e., â€˜phrase matchâ€™ ).
The base model is a 4-layer bi-encoder model that has been fine-
tuned with contrastive loss on more than 20M query-keyword
pairs. The training data consists of 1M new query-item pairs,
along with a candidate pool of over 33 million items.
â€¢ORCAS [3]. This public dataset contains clicked webpages from
Bing for queries from the TREC deep learning challenge. For our
evaluation, we consider the search query as the user input and
the webpage title as the item. We filter the dataset to remove click
data where either the query or the webpage title are empty. The
dataset contains 17.5 million query-webpage pairs. To simulate a
production setup, we utilize the majority (16.5M) of the dataset
for training a supervised model, that acts as the base model. The
base model is initialized with SimCSE [ 8] and trained for 5 epochs.
The remaining 1M are used as new training data for optimizing
novelty. We also reserve a separate, randomly sampled test set
consisting of 8.5K query-keyword pairs.
â€¢AmazonReviews [18]. This dataset contains usersâ€™ product re-
view histories on Amazon. Based on a userâ€™s previous reviews,
the goal is to predict the next product that they would review. Li
et al. [18] convert it to text-based problem by represent items as
a text sequence using their metadata (e.g., â€œTitle: Philips motor
Category: Home Appliances Color: Blackâ€). Users are represented
as a text concatenation of each item in their profile. For our exper-
iments, we consider the Industrial & Scientific domain consisting
of over 11K user histories and 5K products. Li et al . [18] sort each
userâ€™s history by time and break it down into a train set, valida-
tion set (second-last product), and a test set (most recent product
in the history). We use the train set for training the base model,
which is initialized with the pre-trained RecFormer model from
[18]. To simulate the production scenario where users review
additional items over time, we use augmented review history
for finetuning novelty models that includes both the train and
validation set products (Finetuning dataset). In both cases, the
test set remains identical and we ensure that the test set is never
used during training (since the finetuning stage does not have
a validation set for early stopping; we train models for a fixed
number of epochs).Relevance Feedback. For all datasets, we use GPT-3.5 as the re-
ward model for providing relevance feedback during training. For
the first dataset where the goal is produce keywords with more
general intent than the query, we use the prompt,
Given the query â€{Query}â€, is the following query ex-
pressing a similar but more general intent, â€œ{Keyword}â€?
Please answer in a single word: Yes/No.
For the ORCAS dataset where the goal is to predict the top search
result titles, we use the prompt,
Given the query, â€œ{Query}â€, is it possible that the docu-
ment titled, â€œ{WebPageTitle}â€, is relevant for the userÅ›
intent. Please answer in a single word: Yes/No.
The system prompt is the same in both cases, â€œYou are an expert
in understanding user intent from search engine queries.â€. For the
AmazonReviews dataset, we use the system prompt, â€œYou are an
expert in understanding user interests from Amazon.com product
browsing data. â€. The user prompt is given by,
A user is browsing Amazon.com for the following prod-
ucts:
{ListOfProducts}
Is this a relevant product that targets the same scientific
interest?
Recommendation: {CandidateProduct}
Provide a brief reasoning and then the final answer
within â€œâŸ¨Yes/NoâŸ© â€ tag.
Metrics. We evaluate PG-Ret on the following offline metrics.
â€¢Novelty@k: Novelty of top-k items compared to the base model,
as defined in Definition 1. For the query-keyword dataset, we use
L=200 for the base model since each retrieval model sends roughly
200 keywords to the downstream ranking layer. For ORCAS and
AmazonReviews dataset, we use L=50 since we observe that the
relevance of predicted items decreases significantly beyond 50
items.
â€¢Recall@k is the number of clicked <query, item>pairs from test
data that are in the top-k predictions. As novelty is optimized,
recall over the top-k items should not decrease substantially.
â€¢Precision@k: While recall is an important metric, it is depen-
dent on the available clicks in the test data for each query. It is
possible that a model makes relevant predictions but they are
not counted since those <query, item>pairs do not exist in the
test data. Hence, we use an advanced LLM, GPT-4 as the rele-
vance evaluator for top-k items. Note that we use a different
LLM for evaluation than the one used in training because 1) A
more capable model like GPT-4 can provide more reliable rele-
vance feedback; 2) using a different model ensures fairness of
evaluation when compared to other baselines. Moreover, we use
standardized prompts that are used in the production system for
evaluating retrieval models. For each task, these prompts have
been validated against human-labelled data and they achieve
more than 85% accuracy, thus serving as a reliable approxima-
tion of human feedback. The first task uses a prompt tuned for
estimating phrase-match relevance of keyword and the second
5675KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Amit Sharma, Hua Li, Xue Li, and Jian Jiao
Model Novelty@50 Novelty@100 Novelty@200 Recall@100 Recall@200 Precision@3
Base Model â€“ â€“ â€“ 45.0 54.1 63.7
Supervised Finetuning 6.6 22.7 79.1 49.1 53.5 65.5
PG-Ret (Conservative) 14.5 39.6 107.8 46.8 56.4 70.0
PG-Ret (Aggressive) 32.8 70.2 151.8 27.6 37.7 51.8
Table 2: Novelty, recall and precision for PG-Ret compared to supervised finetuning. PG-Ret (Conservative) obtains substantially
high novelty with equal or better precision. PG-Ret (Aggressive) provides even higher novelty but with reduced accuracy.
Query Top-5 Ad Keywords (Base Model) Top-5 Ad Keywords (PG-Ret (Conservative))
tankless water heater electric home
depothome depot tankless water heater
residential electric tankless water heater
electrical tankless water heater
electric tankless water heater
water heater electric tanklessinstant water heater electric
instant electric water heater
domestic electrical appliances
instant water heaters electric
instant electric water heaters
file management pc computer file management
file management in windows
file management
file management software
file management systemcomputer file management
files management software
software to organize files
file management software
files software
hayan sunrise senior living sunrise senior living home
sunrise seniors living
sunrise senior living wheaton il
sunrise senior care living
who owns sunrise senior livingliving facilities for seniors
senior living facilities
elder living
elderly living
elderly living facilities
can i finance my childâ€™s college with
a home equity loanhome equity loan years
dcu home equity loan
home equity financing
10 year home equity loan
home equity loan in texashome equality loans
educational financing
household finance
home equity laons
home equity laon
Table 3: Example predictions from base model and PG-Ret. Bolded items are novel compared to top-200 from the base model.
task uses a prompt tuned for estimating the general relevance of
a query and a webpage title.
In addition, we evaluate PG-Ret using an A/B experiment for the
first task of matching query to relevant ad keywords.
Baselines. For each dataset, we compare PG-Ret to a supervised
model initialized with the base model and trained on the same
training data using the InfoNCE loss with random negatives (Eq. 1).
All models are trained using Adam optimizer with a learning rate
of10âˆ’5and batch size of 128. PG-Ret (Aggressive) uses ğ‘€=5. For
the AmazonReviews dataset, we use the same hyperparameters as
in [18].
5.2 Query-keyword recommendation task
Novelty. For the query-keyword dataset, the goal is to increase
the novelty of policyâ€™s recommendations. For training PG-Ret, we
use the reward from Equation 7. The results are shown in Table 2.
Compared to supervised finetuning on the same training set, PG-
Ret leads to 2X-5X gains in novel keywords in top-50. In top-200,
PG-Ret (Conservative) and PG-Ret (Aggressive) obtain 108 and 152
novel keywords respectively compared to 79 from the supervisedfinetuned model. At the same time, recall of PG-Ret (Conserva-
tive) is almost the same as the base model. In fact, recall@200 is
slighly higher than the base model. To check the quality of the PG-
Ret models, we also evaluate precision@3 as evaluated by GPT-4.
While we would expect the precision to decrease due to the novelty
loss, we find that PG-Ret (Conservative) has a substantially higher
precision than the base model. This may be possible because the
novelty loss can encourage the model to move away from local
minima and sometimes find a more optimal solution. In comparison,
PG-Ret (Aggressive) suffers a significant drop in both recall and
precision, indicating that novelty optimization has led to a decrease
in the modelâ€™s accuracy. The offline results indicate that PG-Ret
(Conservative) is a good balance between novelty and accuracy.
Table 3 shows qualitative results for a sample of the queries
where PG-Ret led to novel keywords in the top-5 predictions. The
base model tends to match keywords based on lexical overlap whear-
eas PG-Ret is able to find rephrases with the same meaning.
A/B test. Finally, we evaluate PG-Ret (Conservative) on real user
traffic using an A/B experiment over a 10 day period. As mentioned
before, the retrieval system is engineered such that keywords from
a new algorithm (e.g., PG-Ret) are selected for downstream ranking
5676Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Model Novelty@10 Novelty@20 Novelty@50 Recall@50 Precision@1
Base Pretrained Model â€“ â€“ â€“ 81.0 70.0
Supervised Finetuning (3 epochs) 0.007 0.04 0.65 79.7 69.9
Supervised Finetuning (10 epochs) 0.07 0.33 2.82 78.7 68.4
PG-Ret (Conservative) 0.17 0.47 2.64 79.8 68.7
PG-Ret (Aggressive) 0.33 0.85 4.03 78.3 67.9
Table 4: Novelty, Recall and Precision on ORCAS. PG-Ret obtains substantial gains in novelty with minimal accuracy drop.
Model Novelty@10 Novelty@20 Novelty@50 Recall@50
Base Pretrained Model â€“ â€“ â€“ 21.9
Supervised Finetuning 1.9 5.8 24.4 22.6
PG-Ret (Conservative) 3.6 9.5 32.4 22.4
Table 5: Novelty and Recall on the AmazonReviews dataset. Compared to supervised finetuning, PG-Ret obtains substantial
gains in novelty with almost the same recall.
layer (which in turn, may lead to user impressions) only if they
are novel compared to the existing algorithm. By including PG-
Ret in the retrieval pipeline, we observe a 1% increase in query-ad
matching density, the average number of relevant ads selected per
query (as determined by the downstream ranker). We also observe
a0.14% increase in coverage, the fraction of queries for which
relevant ads are shown to users. This indicates that the novelty
optimization can help to match ads to the queries that other algos
are not able to match relevant ads with. Finally, we also observe
a 0.26% increase in click yield, the number of ad clicks per search
query, indicating the new keywords recommended by the novelty
optimization are well received by the real-world users. While the
absolute number may look small, an increase of 0.26%can lead to a
substantial impact when scaled to millions of users.
5.3 ORCAS: Query-webpage matching
To simulate the production setting, we train a supervised model on
over 16M <query, webpage title>pairs as the base model. Our goal
is to produce top-k webpages that are novel wrt. top-50 webpages
predicted by the base model, using a training set of 1M pairs. We first
use the InfoNCE loss to finetune the base model over the 1M training
dataset (Supervised Finetuning). PG-Ret model is finetuned using
our proposed method using the base model as the initialization.
Table 4 shows the results. We report results for two supervised
finetuning models (epochs 3 and 10). In both models, the recall and
precision decreases compared to base model, indicating that the
1M train set may lead to overfitting. PG-Ret (Conservative) also
leads to a drop in recall but the corresponding novelty is signifi-
cantly higher than that of a supervised model with similar recall.
At comparable recall, PG-Ret (Conservative) obtains Novelty@50
of 2.64 compared to 0.65 for the supervised model (3 epochs). Over-
all, PG-Ret (Aggressive) obtains the highest noveltyâ€”on average,
there are 4.03webpages in top-50 predictions of PG-Ret (Conserva-
tive) that did not exist in top-50 predictions of the base model. The
novelty is significantly higher than a supervised model (2.82, 10
epochs) with similar recall. Recall and precision of PG-Ret modelsdecrease slightly (up to 3%) compared to base model, which can be
a reasonable tradeoff given the gains in novelty.
5.4 Amazon: User-Product recommendation
Table 5 shows the novelty and recall metrics for the AmazonRe-
views dataset. Both Supervised Finetuning and PG-Ret models are
initialized with the base pretrained RecFormer model and trained on
the finetuning set. Since these models are trained on an additional
recent product from the userâ€™s history, both models obtain slighly
higher recall than the base model. However, at the same recall,
novelty of the PG-Ret (Conservative) model is significantly higher
than the supervised model. On average, PG-Ret recommends 3.6
products in its top-10 list that are novel wrt. the top-50 recommen-
dations from the base model, compared to 1.9 novel products for
the supervised model. As in the previous datasets, PG-Ret leads to
improved novelty while incurring a minimal loss in recall compared
to the supervised model.
6 CONCLUSION
We presented a technique to optimize a non-differentiable, task-
specific loss in information retrieval applications. We justified the
binary-action formulation of the problem through theoretical and
empirical results. On empirical recommendation datasets, the pro-
posed technique leads to substantial gains in novelty of top-k items.
While we used the simple REINFORCE-based RL algorithm, fu-
ture work can consider Actor-critic or proximal policy optimization
algorithms for optimizing novelty and how they can be extended to
large action spaces. Exploring robust reward functions in the pres-
ence of noisy LLM feedback is also an important future direction.
REFERENCES
[1]Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gau-
rav Sinha, and Amit Sharma. 2023. GAR-meets-RAG Paradigm for Zero-Shot
Information Retrieval. arXiv preprint arXiv:2310.20158 (2023).
[2]Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and
Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender
system. In Proceedings of the Twelfth ACM International Conference on Web Search
and Data Mining. 456â€“464.
5677KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Amit Sharma, Hua Li, Xue Li, and Jian Jiao
[3]Nick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz, and Bodo Billerbeck.
2020. ORCAS: 20 million clicked query-document pairs for analyzing search. In
Proceedings of the 29th ACM International Conference on Information & Knowledge
Management. 2983â€“2989.
[4]Kunal Dahiya, Ananye Agarwal, Deepak Saini, K Gururaj, Jian Jiao, Amit Singh,
Sumeet Agarwal, Purushottam Kar, and Manik Varma. 2021. Siamesexml: Siamese
networks meet extreme classifiers with 100m labels. In International Conference
on Machine Learning. PMLR, 2330â€“2340.
[5]Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot
dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).
[6]Jorge DÃ­ez, David MartÃ­nez-Rego, Amparo Alonso-Betanzos, Oscar Luaces, and
Antonio Bahamonde. 2019. Optimizing novelty and diversity in recommendations.
Progress in Artificial Intelligence 8 (2019), 101â€“109.
[7]Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy
Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and
Ben Coppin. 2015. Deep reinforcement learning in large discrete action spaces.
arXiv preprint arXiv:1512.07679 (2015).
[8]Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive
Learning of Sentence Embeddings. In 2021 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2021. Association for Computational
Linguistics (ACL), 6894â€“6910.
[9]Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng.
2022. Semantic models for the first-stage retrieval: A comprehensive review.
ACM Transactions on Information Systems (TOIS) 40, 4 (2022), 1â€“42.
[10] Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian
Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen, et al .2023. Annollm: Making
large language models to be better crowdsourced annotators. arXiv preprint
arXiv:2303.16854 (2023).
[11] Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl.
2004. Evaluating collaborative filtering recommender systems. ACM Transactions
on Information Systems (TOIS) 22, 1 (2004), 5â€“53.
[12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,
and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for
recommender systems. In European Conference on Information Retrieval. Springer,
364â€“381.
[13] Jiri Hron, Karl Krauth, Michael Jordan, and Niki Kilbertus. 2021. On component
interactions in two-stage recommender systems. Advances in neural information
processing systems 34 (2021), 2744â€“2757.
[14] Neil Hurley and Mi Zhang. 2011. Novelty and diversity in top-n recommendationâ€“
analysis and evaluation. ACM Transactions on Internet Technology (TOIT) 10, 4
(2011), 1â€“30.
[15] Marius Kaminskas and Derek Bridge. 2016. Diversity, serendipity, novelty, and
coverage: a survey and empirical analysis of beyond-accuracy objectives in
recommender systems. ACM Transactions on Interactive Intelligent Systems (TiiS)
7, 1 (2016), 1â€“42.
[16] Ee Yeo Keat, Nurfadhlina Mohd Sharef, Razali Yaakob, Khairul Azhar Kasmiran,
Erzam Marlisah, Norwati Mustapha, and Maslina Zolkepli. 2022. Multiobjective
Deep Reinforcement Learning for Recommendation Systems. IEEE Access 10
(2022), 65011â€“65027.
[17] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. 2020. Contrastive represen-
tation learning: A framework and review. Ieee Access 8 (2020), 193907â€“193934.
[18] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian
McAuley. 2023. Text is all you need: Learning language representations for
sequential recommendation. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 1258â€“1267.
[19] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-
bandit approach to personalized news article recommendation. In Proceedings of
the 19th international conference on World wide web. 661â€“670.
[20] Yong Liu, Zhiqi Shen, Yinan Zhang, and Lizhen Cui. 2021. Diversity-promoting
deep reinforcement learning for interactive recommendation. In 5th International
Conference on Crowd Science and Engineering. 132â€“139.
[21] Romain Lopez, Inderjit S Dhillon, and Michael I Jordan. 2021. Learning from
extreme bandit feedback. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 8732â€“8740.
[22] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay
Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022. Dynamic Prompt Learningvia Policy Gradient for Semi-structured Mathematical Reasoning. In The Eleventh
International Conference on Learning Representations.
[23] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. 2020. On
the global convergence rates of softmax policy gradient methods. In International
Conference on Machine Learning. PMLR, 6820â€“6829.
[24] Ali Montazeralghaem, Hamed Zamani, and James Allan. 2020. A reinforcement
learning framework for relevance feedback. In Proceedings of the 43rd international
acm sigir conference on research and development in information retrieval. 59â€“68.
[25] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022.
Training language models to follow instructions with human feedback. Advances
in Neural Information Processing Systems 35 (2022), 27730â€“27744.
[27] Feiyang Pan, Qingpeng Cai, Pingzhong Tang, Fuzhen Zhuang, and Qing He.
2019. Policy gradients for contextual recommendations. In The World Wide Web
Conference. 1421â€“1431.
[28] AndrÃ© Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas Beyer, and Xiaohua
Zhai. 2023. Tuning computer vision models with task rewards. In International
Conference on Machine Learning. PMLR, 33229â€“33239.
[29] Mario Rodriguez, Christian Posse, and Ethan Zhang. 2012. Multiple objective
optimization in recommender systems. In Proceedings of the sixth ACM conference
on Recommender systems. 11â€“18.
[30] Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, and Lucas Dixon.
2023. Large language models are competitive near cold-start recommenders for
language-and item-based preferences. In Proceedings of the 17th ACM conference
on recommender systems. 890â€“896.
[31] Guy Shani and Asela Gunawardana. 2011. Evaluating recommendation systems.
Recommender systems handbook (2011), 257â€“297.
[32] Xiaoyu Shi, Quanliang Liu, Hong Xie, Di Wu, Bo Peng, MingSheng Shang, and
Defu Lian. 2023. Relieving popularity bias in interactive recommendation: A
diversity-novelty-aware reinforcement learning approach. ACM Transactions on
Information Systems 42, 2 (2023), 1â€“30.
[33] Dusan Stamenkovic, Alexandros Karatzoglou, Ioannis Arapakis, Xin Xin, and
Kleomenis Katevas. 2022. Choosing the best of both worlds: Diverse and novel
recommendations through multi-objective reinforcement learning. In Proceedings
of the Fifteenth ACM International Conference on Web Search and Data Mining.
957â€“965.
[34] SaÃºl Vargas and Pablo Castells. 2011. Rank and relevance in novelty and diversity
metrics for recommender systems. In Proceedings of the fifth ACM conference on
Recommender systems. 109â€“116.
[35] Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Reinforce-
ment Learning to Rank with Markov Decision Process. In Proceedings of the 40th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (Shinjuku, Tokyo, Japan) (SIGIR â€™17). Association for Computing Ma-
chinery, New York, NY, USA, 945â€“948. https://doi.org/10.1145/3077136.3080685
[36] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8 (1992), 229â€“256.
[37] Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ning-
hao Liu. 2024. Could Small Language Models Serve as Recommenders? Towards
Data-centric Cold-start Recommendation. In Proceedings of the ACM on Web
Conference 2024. 3566â€“3575.
[38] Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin. 2021. Hierar-
chical reinforcement learning for integrated recommendation. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 35. 4521â€“4528.
[39] Jun Xu, Zeng Wei, Long Xia, Yanyan Lan, Dawei Yin, Xueqi Cheng, and Ji-
Rong Wen. 2020. Reinforcement Learning to Rank with Pairwise Policy Gra-
dient. In Proceedings of the 43rd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (Virtual Event, China) (SI-
GIR â€™20). Association for Computing Machinery, New York, NY, USA, 509â€“518.
https://doi.org/10.1145/3397271.3401148
[40] Pengfei Zhao and Dik Lun Lee. 2016. How much novelty is relevant? it depends
on your curiosity. In Proceedings of the 39th International ACM SIGIR conference
on Research and Development in Information Retrieval. 315â€“324.
[41] Yinglun Zhu, Dylan J Foster, John Langford, and Paul Mineiro. 2022. Contextual
bandits with large action spaces: Made practical. In International Conference on
Machine Learning. PMLR, 27428â€“27453.
5678Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A APPENDIX: PROOF OF PROPOSITIONS
A.1 Proposition 1
To prove Proposition 1, we restate a result from [ 23] for a MDP.
Hereğ›¾is the discount factor, ğ‘¡refers to steps of the optimization,
andğ‘‘ğœ‹ğœ‡(ğ›¾)=Eğ‘ 0âˆ¼ğœ‡(1âˆ’ğ›¾)Ãâˆ
ğ‘¡=0ğ›¾ğ‘¡ğ‘ƒ(ğ‘ ğ‘¡=ğ‘ |ğ‘ 0,ğœ‹)is the discounted
state distribution.
Theorem 2 ([23]).Let Assumption 1 hold and let {ğœƒğ‘¡}ğ‘¡â‰¥1be gen-
erated using ğœƒğ‘¡+1â†ğœƒğ‘¡+ğœ‚ğœ•ğ‘‰ğœ‹ğœƒğ‘¡(ğœ‡)
ğœ•ğœƒğ‘¡whereğ‘‰ğœ‹ğœƒğ‘¡(ğœ‡)=Eğ‘ âˆ¼ğœ‡ğ‘‰ğœ‹ğœƒğ‘¡(ğ‘ ).
Letğœ‚=(1âˆ’ğ›¾)3/8,ğ‘be the positive constant ğ‘:=infğ‘ âˆˆğ‘†,ğ‘¡â‰¥1ğœ‹ğœƒğ‘¡(ğ‘âˆ—(ğ‘ )|ğ‘ )>
0. Then, for all ğ‘¡â‰¥1,
Eğ‘ âˆ¼ğœ‡[ğ‘‰âˆ—(ğ‘ )âˆ’ğ‘‰ğœ‹ğœƒğ‘¡(ğ‘ )]â‰¤16ğ‘†
ğ‘2(1âˆ’ğ›¾)6ğ‘¡ğ‘‘ğœ‹âˆ—ğœ‡(ğ›¾)
ğœ‡2
âˆ1
ğœ‡âˆ(10)
Proposition 1. Let Assumptions 1 and 2 hold and let {ğœƒğ‘¡}ğ‘¡â‰¥1
be generated using the standard policy gradient update: ğœƒğ‘¡+1â†
ğœƒğ‘¡+ğœ‚ğœ•ğ‘‰ğœ‹ğœƒğ‘¡(ğœ‡)
ğœ•ğœƒğ‘¡whereğ‘‰ğœ‹ğœƒğ‘¡(ğœ‡)=Eğ‘¥âˆ¼DEyâˆ¼ğœ‹ğœƒğ‘¡(.|ğ‘¥)R(ğ‘¥,y). Then,
for allğ‘¡â‰¥1, withğœ‚=1/8,
Eğ‘¥âˆ¼D[(ğœ‹âˆ—(ğ‘¥)âˆ’ğœ‹ğœƒğ‘¡(ğ‘¥))ğ‘‡r(ğ‘¥)]â‰¤16ğ‘†ğ´2ğœŒ2
ğ‘¡1
ğœ‡âˆ(5)
whereğ‘†=|ğ‘‹|is the number of states and ğ´is the number of actions.
Proof. Note that our RL setup is one-step, hence we can assume
ğ›¾=0. Thenğ‘‰ğœ‹(ğ‘ )from Theorem 1 simplifies to,
ğ‘‰ğœ‹(ğ‘ )=Eğ‘ğ‘¡âˆ¼ğœ‹(.|ğ‘ ğ‘¡)âˆâˆ‘ï¸
ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ ğ‘¡,ğ‘ğ‘¡)=Eğ‘ğ‘¡âˆ¼ğœ‹(.|ğ‘ ğ‘¡)ğ‘Ÿ(ğ‘ 0,ğ‘0)=ğœ‹(ğ‘ )ğ‘‡r(ğ‘ )
whereğœ‹andğ‘Ÿin the last equation refer to the vectors of action
probabilities and their rewards given a state. Further, since ğ›¾=0,
ğ‘‘ğœ‹âˆ—ğœ‡(ğ›¾)=ğœ‡. Hence, we can write Theorem 1 as,Eğ‘ âˆ¼ğœ‡[ğ‘‰âˆ—(ğ‘ )âˆ’ğ‘‰ğœ‹ğœƒğ‘¡(ğ‘ )]=Eğ‘ âˆ¼ğœ‡[ğœ‹âˆ—(ğ‘ )ğ‘‡r(ğ‘ )âˆ’ğœ‹ğœƒt(ğ‘ )ğ‘‡r(ğ‘ )](11)
=Eğ‘ âˆ¼ğœ‡[(ğœ‹âˆ—(ğ‘ )âˆ’ğœ‹ğœƒt(ğ‘ ))ğ‘‡r(ğ‘ )] (12)
=Eğ‘¥âˆ¼D[(ğœ‹âˆ—(ğ‘¥)âˆ’ğœ‹ğœƒt(ğ‘¥))ğ‘‡r(ğ‘¥)] (13)
â‰¤16ğ‘†
ğ‘2(1âˆ’ğ›¾)6ğ‘¡ğ‘‘ğœ‹âˆ—ğœ‡(ğ›¾)
ğœ‡2
âˆ1
ğœ‡âˆ(14)
â‰¤16ğ‘†
ğ‘2ğ‘¡1
ğœ‡âˆ(15)
where the third equality is because the initial state distribution
is the distribution of queries in the training data.
Now, using Assumption 2, the minimum initial probability for
the optimal action ğ‘âˆ—is1
ğœŒğ´for all states. Assuming that the gradient
updates do not decrease the probability of the optimal action, ğ‘=
infğ‘ âˆˆğ‘†,ğ‘¡â‰¥1ğœ‹ğœƒğ‘¡(ğ‘âˆ—(ğ‘ )|ğ‘ )=1
ğœŒğ´. Substituting c in the above equation,
we obtain the result.
Eğ‘¥âˆ¼D[(ğœ‹âˆ—(ğ‘¥)âˆ’ğœ‹ğœƒt(ğ‘¥))ğ‘‡r(ğ‘¥)]â‰¤16ğ‘†ğ´2ğœŒ2
ğ‘¡1
ğœ‡âˆ
â–¡
A.2 Proposition 2
Proposition 2. With the new formulation, under the assumptions
of Proposition 1, for all ğ‘¡â‰¥1,
Eğ‘¥âˆ¼D(ğœ‹âˆ—(ğ‘¥)âˆ’ğœ‹ğœƒğ‘¡(ğ‘¥))ğ‘‡r(ğ‘¥)â‰¤64ğ‘†ğ´ğœŒ2
ğ‘¡1
ğœ‡âˆ(9)
Proof. Using the equation from Proposition 1 and substituting
ğ‘†=ğ‘†ğ´andğ´=2leads us to the result. â–¡
5679