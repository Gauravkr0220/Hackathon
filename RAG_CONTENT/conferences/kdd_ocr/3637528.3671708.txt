Make Your Home Safe: Time-aware Unsupervised User Behavior
Anomaly Detection in Smart Homes via Loss-guided Mask
Jingyu Xiaoâˆ—
Peng Cheng Laboratory
Tsinghua Shenzhen International
Graduate School
Shenzhen, China
jy-xiao21@mails.tsinghua.edu.cnZhiyao Xuâˆ—
Xiâ€™an University of Electronic Science
and Technology
Xiâ€™an, China
21009200843@stu.xidian.edu.cnQingsong Zouâˆ—
Tsinghua Shenzhen International
Graduate School
Peng Cheng Laboratory
Shenzhen, China
zouqs21@mails.tsinghua.edu.cn
Qing Liâ€ 
Peng Cheng Laboratory
Shenzhen, China
liq@pcl.ac.cnDan Zhao
Peng Cheng Laboratory
Shenzhen, China
zhaod01@pcl.ac.cnDong Fang
Tencent
Shenzhen, China
victordfang@tencent.com
Ruoyu Li
Tsinghua Shenzhen International
Graduate School
Shenzhen, China
liry19@mails.tsinghua.edu.cnWenxin Tang
Tsinghua Shenzhen International
Graduate School
Shenzhen, China
vinsontang2126@gmail.comKang Li
Tsinghua Shenzhen International
Graduate School
Shenzhen, China
lk26603878@gmail.com
Xudong Zuo
Tsinghua Shenzhen International
Graduate School
Shenzhen, China
zuoxd20@mails.tsinghua.edu.cnPenghui Hu
Tsinghua University
Beijing, China
huph22@mails.tsinghua.edu.cnYong Jiang
Tsinghua Shenzhen International
Graduate School
Peng Cheng Laboratory
Shenzhen, China
jiangy@sz.tsinghua.edu.cn
Zixuan Weng
Beijing Jiaotong University
Beijing, China
20722027@bjtu.edu.cnMichael R.Lyu
The Chinese University of Hong Kong
Hong Kong, China
lyu@cse.cuhk.edu.hk
ABSTRACT
Smart homes, powered by the Internet of Things, offer great conve-
nience but also pose security concerns due to abnormal behaviors,
such as improper operations of users and potential attacks from
malicious attackers. Several behavior modeling methods have been
proposed to identify abnormal behaviors and mitigate potential
risks. However, their performance often falls short because they
do not effectively learn less frequent behaviors, consider temporal
context, or account for the impact of noise in human behaviors. In
this paper, we propose SmartGuard, an autoencoder-based unsuper-
vised user behavior anomaly detection framework. First, we design
âˆ—The first three authors have equal contribution.
â€ Qing Li is the corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671708a Loss-guided Dynamic Mask Strategy (LDMS) to encourage the
model to learn less frequent behaviors, which are often overlooked
during learning. Second, we propose a Three-level Time-aware Po-
sition Embedding (TTPE) to incorporate temporal information into
positional embedding to detect temporal context anomaly. Third,
we propose a Noise-aware Weighted Reconstruction Loss (NWRL)
that assigns different weights for routine behaviors and noise behav-
iors to mitigate the interference of noise behaviors during inference.
Comprehensive experiments demonstrate that SmartGuard consis-
tently outperforms state-of-the-art baselines and also offers highly
interpretable results.
CCS CONCEPTS
â€¢Security and privacy â†’Human and societal aspects of se-
curity and privacy.
KEYWORDS
User Behavior Modeling, Anomaly Detection, Transformer, Smart
Homes
3551
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jingyu Xiao et al.
ACM Reference Format:
Jingyu Xiao, Zhiyao Xu, Qingsong Zou, Qing Li, Dan Zhao, Dong Fang,
Ruoyu Li, Wenxin Tang, Kang Li, Xudong Zuo, Penghui Hu, Yong Jiang,
Zixuan Weng, and Michael R.Lyu. 2024. Make Your Home Safe: Time-aware
Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-
guided Mask. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671708
1 INTRODUCTION
The rapid growth of IoT solutions has led to an unprecedented
increase in smart devices within homes, expected to reach approx-
imately 5 billion by 2025 [ 24]. However, the abnormal behaviors
pose substantial security risks within smart homes. These abnor-
mal behaviors usually originate from two primary sources. First,
improper operations by users can cause abnormal behaviors, such
as inadvertently activating the air conditionerâ€™s cooling mode dur-
ing winter or forgetting to close a water valve. Second, malicious
attackers can exploit vulnerabilities within IoT devices and plat-
forms, taking unauthorized control of these devices. For example,
hackers can compromise IoT platforms, allowing them to disable
security cameras and manipulate home automation systems, creat-
ing opportunities for burglary. These security concerns emphasize
the urgency of robust behavioral modeling methods and enhanced
security measures to safeguard smart home environments.
Deep learning has been employed across various domains to
mine correlations between behaviors for modeling user behav-
ior sequences [ 22,31,32] and address security issues [ 2,13â€“15].
DeepMove[ 9] leverages RNNs to model both long and short-term
mobility patterns of users for human mobility prediction. To cap-
ture the dynamics of userâ€™s behaviors, SASRec [ 20] proposes a
self-attention based model to achieve sequential recommendation.
More recent efforts[ 4,8,30] primarily focus on transformer-based
models for their superior ability to handle sequential behavior data.
However, we cannot borrow the above models to directly apply
them in our scenarios, because of the following three challenges of
user behavior modeling in smart homes.
0 10 20 30 40 50 60 70
Behavior ID0200400600Number of Behavior
0.0000.0050.0100.0150.020
Reconstruction Loss
Figure 1: Reconstruction losses for behaviors with different
occurrence frequencies.
First, the occurrence frequencies of different user behaviors may
be imbalanced, leading to challenges in learning the semantics of
these behaviors. This user behavior imbalance can be attributed
to individualsâ€™ living habits. For example, cook-related behaviors(e.g., using microwave and oven) of office workers may be infre-
quent, because they dine at their workplace on weekdays and only
cook on weekends. On the other hand, some daily behaviors like
turning on lights and watching TV of the same users can be more
frequent. Behavior imbalance complicates the learning process for
models: some behaviors, which occur frequently in similar contexts,
can be easily inferred, while others that rarely appear or manifest
in diverse contexts can be more challenging to infer. We train an
autoencoder model on AN dataset (shown in Table 1), record the oc-
currences and reconstruction loss of different behaviors. As shown
in Figure 1, with the number of occurrences of behavior decreases,
the reconstruction loss tends to increase.
Second, temporal context, e.g., the timing and duration of user
behaviors, plays a significant role in abnormal behavior detection
but is overlooked by existing solutions. For example, turning on
the cooling mode of the air conditioner in winter is abnormal, but
is normal in summer. Showering for 30-40 minutes is normal, but
exceeding 2 hour suggests a user accident. Ignoring timing informa-
tion hinders the identification of abnormal behavior patterns. As
shown in Figure 2, sequence 1 represents a userâ€™s normal laundry-
related behaviors. Sequences 2 and 3 follow the same order as
sequence 1. However, in sequence 2, the water valve were opens at
2 oâ€™clock in the night. In sequence 3, the duration between opening
and closing the water valve is excessively long. Therefore these
two sequences should be identified as abnormal behaviors possibly
conducted by attackers intending to induce water leakage.
ProblemDefinition2
open9amclose10amopen9:10amnotification9:50am
open2amclose3amopen2:10amnotification2:50am
open9amclose12amopen9:10amnotification9:50am
â€¦â€¦
Figure 2: Example of three user behaviors with the same
behavior order. Sequence 2 and 3 are abnormal due to their
inappropriate timing and excessive duration.
Third, arbitrary intents and passive device actions can cause
noise behaviors in user behavior sequences, which interfere modelâ€™s
inference. Figure 3 shows noise behaviors in a behavior sequence
related to a userâ€™s behaviors after getting up. The user do some
routine behaviors like â€œturn on the bed lightâ€, â€œopen the curtainsâ€,
â€œswitch off the air conditionerâ€, â€œopen the refrigeratorâ€, â€œclose the
refrigeratorâ€ and â€œswitch on the ovenâ€. However, there are also
some sporadic actions which are not tightly related to the behavior
sequence, including 1) active behaviors, e.g., suddenly deciding to
â€œturn on the network audioâ€ to listen to music; 2) passive behavior
from devices, e.g., the â€œself-refreshâ€ of the air purifier. These noise
3552Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
behaviors may also occur in other sequences with varying patterns.
These noise behaviors introduces uncertainty that can disrupt the
learning process and lead the model to misclassify sequences con-
taining noise behaviors as anomalies. Therefore, treating noise
behaviors on par with normal behaviors could potentially harm the
modelâ€™s performance, leading to increased losses.
on
open
openğ‘¡!switch onclose
openoffself-refreshnoise behaviorsnetwork audioair purifiercurtainroutine behaviorsair conditionerbed light
ovenrefrigeratorğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%ğ‘¡&ğ‘¡'ğ‘¡(
Figure 3: An example of noise behaviors.
In this paper, we propose SmartGuard to solve above challenges.
SmartGuard is an autoencoder-based architecture, which learns to
reconstruct normal behavior sequences during training and identify
the behavior sequences with high reconstruction loss as anomaly.
Firstly, we devise a Loss-guided Dynamic Mask Strategy (LDMS)
to promote the modelâ€™s learning of infrequent hard-to-learn behav-
iors. Secondly, we introduce a Three-level Time-aware Position
Embedding (TTPE) to integrate temporal information into posi-
tional embedding for detecting temporal context anomalies. Lastly,
we propose a Noise-aware Weighted Reconstruction Loss (NWRL)
to assign distinct weights to routine behaviors and noise behaviors,
thereby mitigating the impact of noise behaviors. Our codes are
released to the GitHub1. Our contributions can be summarized as
follows:
â€¢We design LDMS to mask the behaviors with high recon-
struction loss, thus encouraging the model to learn these
hard-to-learn behaviors.
â€¢We propose TTPE to consider the order-level, moment-level
and duration-level information of user behaviors meanwhile.
â€¢We design NWRL to treat noisy behaviors and normal behav-
iors differently for learning robust behavior representations.
2 RELATED WORK
2.1 User Behavior Modeling in Smart Homes
Some works propose to model user behavior (i.e., user device in-
teraction) based on deep learning. [ 16] uses event transition graph
to model IoT context and detect anomalies. In [ 34], authors build
device interaction graph to learn the device state transition rela-
tionship caused by user actions. [ 12] detects anomalies through cor-
relational analysis of device actions and physical environment. [ 29]
infers user behavior through readings from various sensors installed
in the userâ€™s home. IoTBeholder [ 38] utilizes attention-based LSTM
to predict the user behavior from history sequences. SmartSense
[18] leverages query-based transformer to model contextual infor-
mation of user behavior sequences. DeepUDI [ 35] and SmartUDI
[36] use relational gated graph neural networks, capsule neural
networks and contrastive learning to model usersâ€™ routines, intents
1https://github.com/xjywhu/SmartGuardand multi-level periodicities. However, above methods aim at pre-
dicting next behavior of user accurately, they can not be applied
into abnormal behavior detection.
2.2 Attacks and Defenses in Smart Homes
An increasing number of attack vectors have been identified in
smart homes in recent years. In addition to cyber attacks, it is also a
concerning factor that IoT devices are often close association with
the userâ€™s physical environment and they have the ability to alter
physical environment. In this context, the automation introduces
more serious security risks. Prior research has revealed that ad-
versaries can leak personal information, and gain physical access
to the home [ 3,19]. In [ 10], spoof attack is employed to exploit
automation rules and trigger unexpected device actions. [ 7,11]
apply delay-based attacks to disrupt cross-platform IoT informa-
tion exchanges, resulting in unexpected interactions, rendering IoT
devices and smart homes in an insecure state. This series of attacks
aim at causing smart home devices to exhibit expected actions,
thereby posing significant security threats. Therefore, designing an
effective mechanism to detect such attacks is necessary. 6thSense
[27] utilizes Naive Bayes to detect malicious behavior associated
with sensors in smart homes. Aegis [ 28] utilizes a Markov Chain to
detect malicious behaviors. ARGUS [ 26] designed an Autoencoder
based on Gated Recurrent Units (GRU) to detect infiltration attacks.
However, these methods ignore the behavior imbalance, temporal
information and noise behaviors.
3 PROBLEM FORMULATION
LetDdenote a set of devices, Cdenote a set of device controls and
Sdenote a set of behavior sequences.
Definition 1. (Behavior) A behavior ğ‘=(ğ‘¡,ğ‘‘,ğ‘), is a 3-tuple
consisting of time stamp ğ‘¡, deviceğ‘‘âˆˆD and device control ğ‘âˆˆC.
For example, behavior b = (2022-08-04 18:30, air conditioner, air
conditioner:switch on) describes the behavior â€œswich on the air con-
ditioner â€ at 18:30 on 2022-08-04.
Definition 2. (Behavior Sequence) A behavior sequence ğ‘ =
[ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›]âˆˆS is a list of behaviors,ordered by their timestamps,
andğ‘›is the length of ğ‘ .
We define the User Behavior Sequence (UBS) anomaly detection
problem as follows.
Problem 1. (UBS Anomaly Detection) Given a behavior se-
quenceğ‘ , determine whether ğ‘ is an anomaly event or a normal event.
In this paper, we consider four types of abnormal behaviors:
â€¢(SD) Single Device context anomaly (Figure 4(a)), defined as
unusual high frequency operations on a single device, e.g.,
frequently switching light on and off to break the light.
â€¢(MD) Multiple Devices context anomaly (Figure 4(b)), de-
fined as the simultaneous occurrence of behaviors on mul-
tiple devices that are not supposed to occur in the same
sequence, e.g., turning off the camera and opening the win-
dow for burglary.
â€¢(DM) Device control-Moment context anomaly (Figure 4(c)),
defined as a device control occurring at an inappropriate
3553KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jingyu Xiao et al.
Challenge1
onoff
broken
theftcloseopen
three hoursopen
flood
coldwintercooling
(a) Attention Weight(a) Single Device Context Anomaly.(b) Multiple Devices Context Anomaly.
(c) Device Control-Moment Context Anomaly.(d) Device Control-Duration Context Anomaly.
Figure 4: Different types of anomaly behaviors.
time, e.g., turning on the cooling mode of an air conditioner
in winter, potentially causing the user to catch a cold.
â€¢(DD) Device control-Duration context anomaly (Figure 4(d)),
defined as device controls that last for an inappropriate du-
ration, e.g., leaving a water valve open for 3 hours for flood
attack.
4 METHODOLOGY
4.1 Solution Overview
To achieve accurate user behavior sequence anomaly detection in
smart homes, we propose SmartGuard, depicted in Figure 5. The
workflow of SmartGuard can be summarized as follows. During
training, the Loss-guided Dynamic Mask Strategy (Â§4.2) is initially
employed to mask hard-to-learn behaviors based on the loss vec-
torLvecfrom the previous epoch. Subsequently, the Three-level
Time-aware Positional Encoder (Â§4.3.1) is applied to capture order-
level, moment-level, and duration-level temporal information of
the behaviors, producing the positional embedding ğ‘ƒğ¸. This embed-
ding is then added to the device control embedding â„ğ‘to form the
behavior embedding h. Finally, his fed into an ğ¿-layer attention-
based encoder and decoder to extract contextual information for
reconstructing the source sequence. During the inference phase, the
Noise-aware Weighted Reconstruction Loss Noise-aware Weighted
Reconstruction Loss (Â§4.4) is utilized to assign different weights to
various behaviors, determined by the loss vector from the training
dataset, resulting in the final reconstruction loss ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ . If theğ‘ ğ‘ğ‘œğ‘Ÿğ‘’
surpasses the threshold ğ‘¡â„, SmartGuard triggers an alarm.
4.2 Loss-guided Dynamic Mask Strategy
Autoencoders [ 37], which take complete data instances as input
and target to reconstruct the entire input data, are widely used in
anomaly detection. Different from traditional autoencoders, masked
autoencoders randomly mask a portion of input data, encoding the
partially-masked data and aiming to reconstruct the masked tokens.
By introducing a more meaningful self-supervised task, masked au-
toencoders have recently excelled in images [ 17] learning. However,
such reconstruction tasks without mask and with random mask are
sub-optimal in our scenarios because they do not emphasize the
learning of hard-to-learn behaviors that occur rarely.
We conduct experiments to verify the performance of autoen-
coders trained with three mask options: 1) w/o mask: no mask
strategy is used, the objective function is to reconstruct the input;2) random mask: masking behaviors at every epoch randomly to
reconstruct the masked behaviors; 3) top- ğ‘˜loss mask: masking
topğ‘˜behaviors with higher reconstruction loss to reconstruct the
masked behaviors. We set mask ratio as 20% for the latter two.
Figure 6 shows the changing trends of the reconstruction loss and
itâ€™s variance of different behavior during training on SP dataset
(described in Table 1). First, as shown in Figure 6(a), the model
without mask shows the fastest convergence trend, whereas the
loss of the model with mask fluctuates. Model without mask can
simultaneously learn all behaviors, facilitating rapid convergence.
In contrast, the mask strategy only encourages the model to focus
on learning masked behaviors, which may hinder initial-stage con-
vergence. Second, the model with top- ğ‘˜loss mask strategy shows
lowest variance towards the end of training as shown in Figure 6(b),
because the top- ğ‘˜loss mask strategy effectively encourages the
model to learn hard-to-learn behaviors (i.e., the behaviors with high
reconstruction loss), thereby reducing the variance of behavior
reconstruction losses.
In this paper, we design a Loss-guided Dynamic Mask Strategy.
Intuitively, at the beginning of training, we encourage the model
to learn the relatively easy task to accelerate convergence, i.e.,
behavior sequence reconstruction without mask. After training ğ‘
epochs without mask, we adopt the top- ğ‘˜loss mask strategy to
encourage the model to learn the masked behaviors with
high reconstruction loss. We continuously track the modelâ€™s
reconstruction loss of different behaviors by updating a loss vector
in each epoch to guide the mask strategy in the next epoch. In
epochğ‘’ğ‘, the loss vectorLğ‘’ğ‘
vecis calculated as:
Lğ‘’ğ‘
vec=
â„“1,â„“2,...,â„“ğ‘,...,â„“|C|	
,ğ‘âˆˆC, (1)
â„“ğ‘=1
ğ‘›ğ‘ğ‘›ğ‘âˆ‘ï¸
ğ‘–=1â„“ğ‘–
ğ‘, (2)
whereğ‘›ğ‘is the number of times the device control ğ‘occurs in epoch
ğ‘’ğ‘, andâ„“ğ‘is the average reconstruction loss of the device control
ğ‘. In epochğ‘’ğ‘+1, the mask vector for behavior sequence sample
ğ‘ =[ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›]is obtained as:
ğ‘šğ‘ğ‘ ğ‘˜(ğ‘–)=n1,ifğ‘–âˆˆğ‘ ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥[:âŒŠğ‘›Â·ğ‘ŸâŒ‹]
0,ifğ‘–âˆ‰ğ‘ ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥[:âŒŠğ‘›Â·ğ‘ŸâŒ‹],ğ‘–âˆˆ[1,ğ‘›],(3)
ğ‘ ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ =argsortn
Lğ‘’ğ‘
vec(ğ‘1),Lğ‘’ğ‘
vec(ğ‘2),....Lğ‘’ğ‘
vec(ğ‘ğ‘›)o
,(4)
where argsort gets the sorted index with descending order of the
elements in the vector, ğ‘Ÿâˆˆ[0,1]is the mask ratio, and ğ‘›is the
length of behavior sequence ğ‘ .
4.3 Autoencoder with Temporal Information
4.3.1 Three-level Time-aware Positional Encoder. The tem-
poral information in user behavior sequence data primarily resides
in the timing of control behaviors, which can be examined from
two perspectives: the absolute timing of each individual control
behavior, and the relative timing gap between control actions on
the same device. On the one hand, the relative timing gap between
control actions on the same device reflects the duration the device
is in some specific state and the operation frequency of the user.
On the one hand, user behaviors are usually time-regulated, and
the functionalities a device carried can determine the absolute tim-
ing users operate on it. For example, users usually operate lights
3554Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Three-level Time-aware Positional EncoderDevice Control EncoderDayEmbeddingğ‘·ğ‘¬(ğ‘«ğ’‚ğ’š)HourEmbeddingğ‘·ğ‘¬(ğ‘¯ğ’ğ’–ğ’“)Sequence EncoderLoss-guided Dynamic Masked Strategy
OrderEmbeddingğ‘·ğ‘¬(ğ‘¶ğ’“ğ’…ğ’†ğ’“)Sequence DecoderPrediction Layer
Multi-head AttentionAdd & NormAdd & NormFeed Forwardğ¿Ã—Encoder & Decoder Architecture
MaskMaskMask
durationğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%ğ‘¡&ğ‘¡'ğ‘¡(momentorder01234567duration
DurationEmbeddingğ‘·ğ‘¬(ğ‘«ğ’–ğ’“ğ’‚ğ’•ğ’Šğ’ğ’)UpdateDeviceControlEmbedding ğ’‰ğ’„BehaviorEmbedding h
Noise-aware Weighted Reconstruction Loss<ğ’•ğ’‰?YesNoLossBehaviorhard-to-learneasy-to-learnLoss Vector â„’ğ’—ğ’†ğ’„
â€¦â€¦Reconstruction Loss â„’ğ’“ğ’†ğ’„â€¦â€¦Behavior Weight ğ‘¾ğ’“ğ’†ğ’„Noise Behaviors
Figure 5: The overview of SmartGuard.
123456789101112131415
Training Step0.00.20.40.60.81.01.2Mean of Reconstruction Lossw/o Mask
Random Mask
T op-k Loss Mask
(a) Mean of reconstruction loss.
123456789101112131415
Training Step0246810Variance of Reconstruction Lossw/o Mask
Random Mask
T op-k Loss Mask910 11 12 13 14 150.000.250.500.751.001.251.50 (b) Variance of reconstruction loss.
Figure 6: Autoencoder training process on SP dataset under
different mask strategies.
in the morning and evening, and operate the microwave and the
oven at meal time. Since certain operations frequently take place
nearly simultaneously, we will also consider the order of behaviors
to provide a more comprehensive characterization of behaviors
that occur successively. Therefore, we incorporate three types of
temporal information into our model. (1) Order-level temporal
information: we use integer ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâˆˆ [0,ğ‘›âˆ’1]to denotes the
order-level information of the behavior, ğ‘›is the length of behaviors
sequenceğ‘ . (2)Moment-level temporal information: we repre-
sent the moment as hour of day â„ğ‘œğ‘¢ğ‘Ÿ and day of week ğ‘‘ğ‘ğ‘¦based on
behaviorâ€™s timestamp. (3) Duration-level temporal information:
the duration for behavior ğ‘is calculated as:
ğ‘‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘=ğ‘¡(ğ‘)âˆ’ğ‘¡(ğ‘ğ‘›ğ‘’ğ‘¥ğ‘¡) (5)
whereğ‘andğ‘ğ‘›ğ‘’ğ‘¥ğ‘¡ are the behaviors on the same device and ğ‘ğ‘›ğ‘’ğ‘¥ğ‘¡
is the first behavior after ğ‘that operates on the device, and ğ‘¡(ğ‘)
represents the occurrence time of behavior ğ‘.Then, the positional embedding is calculated as:
ğ‘ƒğ¸=ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘ŸÂ·ğ‘ƒğ¸(ğ‘ğ‘œğ‘ )+ğ‘¤â„ğ‘œğ‘¢ğ‘ŸÂ·ğ‘ƒğ¸(â„ğ‘œğ‘¢ğ‘Ÿ)+
ğ‘¤ğ‘‘ğ‘ğ‘¦Â·ğ‘ƒğ¸(ğ‘‘ğ‘ğ‘¦)+ğ‘¤ğ‘‘ğ‘¢ğ‘ŸÂ·ğ‘ƒğ¸(ğ‘‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›),(6)
whereğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ ,ğ‘¤â„ğ‘œğ‘¢ğ‘Ÿ,ğ‘¤ğ‘‘ğ‘ğ‘¦andğ‘¤ğ‘‘ğ‘¢ğ‘Ÿare learnable weights. ğ‘ƒğ¸(Â·)
is a positional encoding function [33] defined as:
ğ‘ƒğ¸(Â·,2ğ‘–)=sin
Â·/100002ğ‘–/ğ‘‘
,
ğ‘ƒğ¸(Â·,2ğ‘–+1)=cos
Â·/100002ğ‘–/ğ‘‘
,(7)
whereğ‘–denotes the ğ‘–-th dimension of the positional embedding, ğ‘‘
is the dimension of temporal embedding.
To learn the representation â„ğ‘for device control ğ‘âˆˆC, we first
encode device control ğ‘into a low-dimensional latent space through
device control encoder, i.e., an embedding layer. Finally, we add
positional embedding to the device control embedding as following
to get the behavior embedding:
h=ğ‘ƒğ¸+â„ğ‘. (8)
4.3.2 Sequence Encoder. To learn the sequence embedding, we
employ transformer encoder [ 33] consisting of multi-head attention
layer, residual connections and position-wise feed-forward network
(FNN). Given an input behavior representation h, the self-attention
layer can effectively mine global semantic information of behavior
sequence context by learning query Q, key Kand value Vmatrices
of different variables, which are calculated as:
Q=hWğ‘„,K=hWğ¾,V=hWğ‘‰, (9)
3555KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jingyu Xiao et al.
where Wğ‘„,Wğ¾,Wğ‘‰are the transformation matrices. The attention
score Ais computed by:
A=Attention(ğ‘„,ğ¾,ğ‘‰)=softmax 
ğ‘„ğ¾ğ‘‡
âˆšï¸
ğ‘‘ğ‘˜!
ğ‘‰, (10)
whereğ‘‘ğ‘˜is the dimension of ğ¾. Multi-head attention is applied to
improve the stability of the learning process and achieve higher
performance. Then, the position-wise feed-forward network (FNN)
and residual connections are adopted:
h=Trans(h)=h+Ah+FNN(h+Ah), (11)
where Trans(Â·)is the transformer and FNN(Â·)is a 2-layered position-
wise feed-forward network [33].
4.3.3 Sequence Decoder. The decoder has the same architecture
as the encoder. We input hinto the decoder to reconstruct the entire
sequence, probabilities of target device controls are calculated as:
ehi=decoder
hi
, (12)
Ë†yi=softmax
Wâ„ehi
, (13)
where Ë†yiis the predicted probabilities of the ğ‘–-th device control and
Wâ„âˆˆR|C|Ã—ğ‘™ğ‘’ğ‘›(â„)is the learnable transformation matrix, |C|is the
number of device controls, and ğ‘™ğ‘’ğ‘›(â„)is the length of â„.
4.3.4 Objective Function. We optimize the model to minimize
the average reconstruction loss measured by cross-entropy loss:
Lğ‘Ÿğ‘’ğ‘=(
âˆ’1
|S|Ã
ğ‘ âˆˆSÃ|ğ‘ |
ğ‘–=1yğ‘–logË†yğ‘–, ifğ‘’ğ‘ğ‘œğ‘â„ <=ğ‘
âˆ’1
|S|Ã
ğ‘ âˆˆSÃ|ğ‘ |
ğ‘–=1ğ‘šğ‘ğ‘ ğ‘˜ğ‘ (ğ‘–)yğ‘–logË†yğ‘–,ifğ‘’ğ‘ğ‘œğ‘â„ >ğ‘,
(14)
whereSis the behavior sequences set, |ğ‘ |is the length of sequence
ğ‘ ,yğ‘–is the one-hot vector of the ground-truth label, ğ‘šğ‘ğ‘ ğ‘˜ğ‘ is the
mask vector for sequence ğ‘ , andğ‘is the training steps w/o mask.
4.4 Noise-aware Weighted Reconstruction Loss
Although LDMS encourages the model to focus on learning be-
haviors with high reconstruction losses, it remains challenging to
reconstruct noise behaviors due to their inherent uncertainty. The
significant reconstruction loss associated with noise behaviors can
overshadow other aspects during anomaly detection, potentially
leading to the misclassification of normal sequences containing
noise behaviors as anomalies.
To eliminate the interference of noise behaviors, we propose a
Noise-aware Weighted Reconstruction Loss as the anomaly score.
We can get the final loss vector after training:
Lvec=
â„“1,â„“2,...,â„“ğ‘,...,â„“|C|	
,ğ‘âˆˆC, (15)
which is converted into the corresponding weight vector:
Wğ‘£ğ‘’ğ‘=
ğ‘¤1,ğ‘¤2,...,ğ‘¤ğ‘,...,ğ‘¤|ğ¶|	
,ğ‘¤ğ‘˜âˆˆ(0,1), (16)
by the following equation:
Wğ‘£ğ‘’ğ‘=sigmoid 
âˆ’relu(Lğ‘£ğ‘’ğ‘âˆ’E(Lğ‘£ğ‘’ğ‘))âˆšï¸
Var(Lğ‘£ğ‘’ğ‘)Â·ğœ‡!
, (17)
whereğœ‡is a coefficient to adjust the input for sigmoid function,
EandVarcalculate the expectation and variance of the loss dis-
tribution, respectively. Relu function ensures that behaviors withTable 1: Datasets Description.
Name
Time period (Y-M-D) Sizes # Devices# Device controls
AN
2022-07-31âˆ¼2022-08-31 1,765 36 141
FR 2022-02-27âˆ¼2022-03-25 4,423 33 222
SP 2022-02-28âˆ¼2022-03-3015,665 34 234
losses less than E(Lğ‘£ğ‘’ğ‘)(routine behaviors) are equally weighted.
The sigmoid function assigns small weights to behaviors with high
losses (potential noise behaviors). For each behavior ğ‘ğ‘–in a se-
quenceğ‘ ={ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›}, we compute the weight ğ‘ğ‘–as follows:
ğ‘ğ‘–=Wğ‘£ğ‘’ğ‘(ğ‘ğ‘–)Ãğ‘›
ğ‘—=1Wğ‘£ğ‘’ğ‘(ğ‘ğ‘—). (18)
Then, we can get the anomaly score of ğ‘ as the weighted sum of
the reconstruction losses of behaviors in ğ‘ :
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘ )=âˆ’1
|ğ‘ ||ğ‘ |âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–yğ‘–logË†yğ‘–. (19)
SmartGuard can inference whether a behavior sequence ğ‘ ğ‘–is normal
or abnormal based on the anomaly score:
ğ‘ ğ‘–=nNormal, ifğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘ ğ‘–)<ğ‘¡â„
Abnormal,ifğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘ ğ‘–)>ğ‘¡â„, (20)
whereğ‘¡â„is the anomaly threshold. We take the 95% quantile of the
reconstruction loss distribution on the validation set as ğ‘¡â„.
5 EXPERIMENTS
In this section, we conduct comprehensive experiments on three
real-world datasets to answer the following key questions:
â€¢RQ1. Performance. Compared with other methods, does
SmartGuard achieve better anomaly detection performance?
â€¢RQ2. Ablation study. How will model performance change
if we remove key modules of SmartGuard?
â€¢RQ3. Parameter study. How do key parameters affect the
performance of SmartGuard?
â€¢RQ4. Interpretability study. Can SmartGuard give reason-
able explanations for the detection results?
â€¢RQ5. Embedding space analysis. Does SmartGuard suc-
cessfully learn useful embeddings of behaviors and correct
correlations between device controls and time?
5.1 Experimental Setup
5.1.1 Datasets. We train SmartGuard on three real-world datasets
consisting of only normal samples, two (FR/SP) from public datasets2
and one anonymous dataset (AN) collected by ourselves. The datasets
description is shown in Table 1. All datasets are split into training,
validation and testing sets with a ratio of 7:1:2. To evaluate the
performance of SmartGuard, we construct ten categories of abnor-
mal behaviors as shown in Table 2 and insert them among normal
behaviors for simulating real anomaly scenarios.
2https://github.com/snudatalab/SmartSense
3556Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Anomaly Behaviors.
Anomaly Type Anomaly Type
Light flickering SDOpen the airconditionerâ€™s
cool mode in winterDM
Camera flickering SDOpen the window
at midnightDM
TV flickering SDOpen the watervalve
at midnightDM
Open the window
while smartlock lockMD Shower for long time DD
Close the camera
while smartlock lockMDMicrowave runs
for long timeDD
5.1.2 Baselines. We compare SmartGuard with existing general
unsupervised anomaly detection methods and unsupervised anom-
aly behaviors detection methods in smart homes:
â€¢Local Outiler Factor (LOF) [6] calculates the density ratio
between each sample and its neighbors to detect anomaly.
â€¢Isolation Forest (IF) [23] builds binary trees, and instances
with short average path lengths are detected as anomaly.
â€¢6thSense [27] utilizes Naive Bayes to detect malicious be-
havior associated with sensors in smart homes.
â€¢Aegis[ 28] utilizes a Markov Chain-based machine learning
technique to detect malicious behavior in smart homes.
â€¢OCSVM [1] build a One-Class Support Vector Machine
model to prevent malicious control of smart home systems.
â€¢Autoencoder [5] learns to reconstruct normal data and then
uses the reconstruction error to determine whether the input
data is abnormal.
â€¢ARGUS[ 26] designed an Autoencoder based on Gated Re-
current Units (GRU) to detect IoT infiltration attacks.
â€¢TransformerAutoencoder (TransAE) [33] uses self-attention
mechanism in the encoder and decoder to achieve context-
aware anomaly detection.
5.1.3 Evaluation metrics. We use common metrics such as False
Positive rate, False Negative Rate, Recall, and F1-Score to evaluate
the performance of SmartGuard.
5.1.4 Complexity analysis. Suppose the embedding size is ğ‘’ğ‘š,
and the behavior sequence length is ğ‘›. The computational complex-
ity of SmartGuard is mainly due to the self-attention layer and the
feed-forward network, which is ğ‘‚(ğ‘›2ğ‘‘+ğ‘›ğ‘‘2). The dominant term
is typicallyğ‘‚(ğ‘›2ğ‘‘)from the self-attention layer. SmartGuard only
takes 0.0145s, which shows that it can detect abnormal behaviors
in real time.
5.2 Performance Comparison (RQ1)
We use grid search to adjust the parameters of SmartGuard and
report the overall performance of SmartGuard and all baselines
in Table 3. Bold values indicate the optimal performance among
all schemes, and underlined values indicate the second best per-
formance. First, SmartGuard outperforms all competitors in most
cases. This is because SmartGuardsimultaneously considers the
temporal information, behavior imbalance and noise behaviors.Second, SmartGuard significantly improves the performance on
DM and DD type anomalies detection. We ascribe this superiority to
our TTPEâ€™s effective mining of temporal information of behaviors.
Third, the LOF, IF and 6thSense show the worst performance. Aegis
and OCSVM outperforms LOF, IF and 6thSense, which benifits
from the Markov Chainâ€™s modeling of behavior transitions and
SVMâ€™s powerful kernel function. The Autoencoder outperform the
traditional models because of stronger sequence modeling capabil-
ity. ARGUS outperforms Aueocoder because of stronger sequence
modeling capability of GRU. By exploiting transformer to mine
contextual information, TransAE achieves better performance than
all other baselines, but is still inferior to our proposed scheme.
5.3 Ablation Study (RQ2)
SmartGuard mainly consists of three main components: Loss-guided
Dynamic Mask Strategy (LDMS), Three-level Time-aware Position
Embedding (TTPE) and Noise-aware Weighted Reconstruction
Loss (NWRL). To investigate different componentsâ€™ effectiveness
in SmartGuard, we implement 5 variants of SmartGuard for abla-
tion study (ğ¶0-ğ¶4). Y represents adding the corresponding compo-
nents, Xrepresents removing the corresponding components. ğ¶4is
SmartGuard with all three components. As shown in Table 4, each
component of SmartGuard has a positive impact on results. The
combination of all components brings the best results, which is
much better than using any subset of the three components.
5.4 Parameter Study (RQ3)
5.4.1 The mask ratio ğ‘Ÿand the training step ğ‘without mask.
Figure 7 illustrates that SmartGuard achieves the optimal perfor-
mance when ğ‘Ÿ=0.4andğ‘=5. The parameter ğ‘Ÿ(Equation 3)
determines the difficulty of the model learning task. A smaller
ğ‘Ÿfails to effectively encourage the model to learn hard-to-learn
behaviors, while a larger ğ‘Ÿincreases the learning burden on the
model, consequently diminishing performance. As for training steps
without a mask, a smaller ğ‘hinders the model from converging
effectively at the beginning stage, whereas a larger ğ‘impedes the
modelâ€™s ability to learn hard-to-learn behaviors towards the end,
resulting in degraded performance.
5.4.2ğœ‡of Noise-aware Weighted Reconstruction Loss. The
parameterğœ‡(Equation 17) controls the weights assigned to poten-
tial noise behaviors. A smaller ğœ‡results in a smaller weight for noise
behaviors, while a larger ğœ‡leads to a greater weight for noise behav-
iors. As illustrated in Figure 8(a), the False Positive Rate gradually
decreases as ğœ‡decreases, benefiting from the reduced loss weight
assigned to noise behaviors. However, as depicted in Figure 8(b),
the False Negative Rate slightly increases as ğœ‡decreases. When
ğœ‡=0.1, SmartGuard achieves a balance, minimizing both the False
Positive Rate and the False Negative Rate.
5.4.3 The embedding size ğ‘’ğ‘š.We fine-tune the embedding size
for time and device control, ranging from 8 to 512. As depicted
in Figure 9(a), an initial increase in the embedding dimension re-
sults in a notable performance improvement, which is attributed
to the larger dimensionality enabling behavior embedding to cap-
ture more comprehensive information about the context, thereby
3557KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jingyu Xiao et al.
Table 3: Performance comparison on three real world datasets.
Dataset Type Metric LOF IF 6thSense Aegis OCSVM Autoencoder ARGUS TransAE SmartGuard
ANSDRecall 0.0275 0.4105 0.4680 0.2902 0.5399 0.9832 0.9858 0.9882 0.9986
F1 Score 0.0519 0.4972 0.5196 0.3672 0.5862 0.9915 0.9928 0.9908 0.9967
MDRecall 0.0745 0.4039 0.5941 0.4431 0.6039 0.5156 0.5666 0.6216 0.9745
F1 Score 0.1357 0.4824 0.6215 0.4718 0.6553 0.6692 0.7135 0.7557 0.9832
DMRecall 0.0784 0.4373 0.3745 0.5647 0.3510 0.5196 0.5313 0.6078 0.9961
F1 Score 0.1418 0.5174 0.4817 0.5647 0.4257 0.6725 0.6843 0.7452 0.9941
DDRecall 0.0961 0.3451 0.1980 0.7804 0.4961 0.5137 0.5117 0.5294 0.9980
F1 Score 0.1713 0.4282 0.3108 0.7044 0.5967 0.6675 0.6675 0.6818 0.9951
FRSDRecall 0.3541 0.2444 0.2907 0.3915 0.5918 0.9816 0.9796 0.9864 0.9979
F1 Score 0.4804 0.3655 0.4167 0.4542 0.6612 0.9907 0.9897 0.9921 0.9932
MDRecall 0.4275 0.2980 0.6567 0.7098 0.4384 0.9726 0.9875 0.9782 0.9984
F1 Score 0.5192 0.4230 0.6092 0.3827 0.5534 0.9861 0.9783 0.9874 0.9907
DMRecall 0.3825 0.3191 0.5461 0.7619 0.3920 0.4952 0.6676 0.6529 0.9985
F1 Score 0.4830 0.4494 0.6124 0.6822 0.4940 0.6508 0.7867 0.7779 0.9912
DDRecall 0.3572 0.1850 0.5358 0.9743 0.6267 0.4397 0.7329 0.6098 0.9981
F1 Score 0.4375 0.2806 0.5880 0.4481 0.6422 0.6013 0.8382 0.7479 0.9921
SPSDRecall 0.2197 0.2643 0.6979 0.1618 0.5332 0.9824 0.9795 0.9172 0.9862
F1 Score 0.3350 0.3857 0.7248 0.2164 0.6155 0.9911 0.9896 0.9489 0.9831
MDRecall 0.2786 0.3399 0.6317 0.7445 0.3840 0.5645 0.9696 0.9936 0.9961
F1 Score 0.3916 0.4632 0.6440 0.6636 0.5026 0.7095 0.9845 0.9866 0.9830
DMRecall 0.2780 0.3465 0.6080 0.8121 0.5351 0.3074 0.5297 0.5451 0.9198
F1 Score 0.4112 0.4918 0.6935 0.7758 0.6341 0.4649 0.6847 0.6962 0.9498
DDRecall 0.2109 0.1763 0.5449 0.8001 0.8293 0.6455 0.6455 0.6456 0.9961
F1 Score 0.3052 0.2627 0.6343 0.6545 0.7311 0.7685 0.7658 0.7653 0.9788
Table 4: The F1-Score of 5 variants ( ğ¶0-ğ¶4) on AN dataset.
LDMS TTPE NWRL SD MD DM DD
X X X ğ¶00.9908 0.7557 0.7452 0.6818
Y Y X ğ¶10.9877 0.9708 0.9767 0.9817
Y X Y ğ¶20.9883 0.6716 0.6783 0.6799
X Y Y ğ¶30.9902 0.9766 0.9835 0.9855
Y Y Y ğ¶40.9967 0.9832 0.9941 0.9951
furnishing valuable representations for other modules in Smart-
Guard. Nevertheless, excessively large sizes (e.g., > 256) can lead to
performance degradation due to over-fitting.
5.4.4 Number of layers ğ¿of encoder and decoder. Figure 9(b)
shows the performance of SmartGuard with different numbers
of layers. When ğ¿increases, F1-Score first increases and then de-
creases, reaching the optimal value at 3 layers, because fewer layers
leads to under-fitting, and too many layers leads to over-fitting.
5.5 Case Study (RQ4)
To assess the interpretability of SmartGuard, we select a behavior
sequence from the test set of the AN dataset and visualize its atten-
tion weights and reconstruction loss. Illustrated in Figure 10, the
user initiated a sequence of actions: turning off the TV, stopping the
sweeper, closing the curtains, switching off the bedlight, and lock-
ing the smart lock before going to sleep. Subsequently, an attacker
took control of IoT devices, turning off the camera, and opening
the window for potential theft. Examining Figure 10(a), we observe
3 4 5 6
Step w/o Mask0.2 0.4 0.6 0.8Mask Ratio0.9948 0.9929 0.9935 0.9948
0.9948 0.9961 0.9967 0.9948
0.9948 0.9954 0.9948 0.9954
0.9935 0.9941 0.9922 0.9941
0.9930.9940.9950.996(a) SD Anomaly.
3 4 5 6
Step w/o Mask0.2 0.4 0.6 0.8Mask Ratio0.9803 0.9804 0.9814 0.9813
0.9803 0.9842 0.9832 0.9833
0.9803 0.9802 0.9803 0.9843
0.9783 0.9793 0.9764 0.9803
0.9770.9780.9790.9800.9810.9820.9830.984 (b) MD Anomaly.
3 4 5 6
Step w/o Mask0.2 0.4 0.6 0.8Mask Ratio0.9912 0.9883 0.9893 0.9912
0.9912 0.9932 0.9941 0.9912
0.9912 0.9912 0.9902 0.9922
0.9883 0.9893 0.9874 0.9893
0.9880.9890.9900.9910.9920.9930.994
(c) DM Anomaly.
3 4 5 6
Step w/o Mask0.2 0.4 0.6 0.8Mask Ratio0.9922 0.9893 0.9903 0.9922
0.9922 0.9941 0.9951 0.9922
0.9922 0.9932 0.9922 0.9932
0.9903 0.9912 0.9883 0.9912
0.9890.9900.9910.9920.9930.9940.995 (d) DD Anomaly.
Figure 7: Performance under different mask ratio and step
w/o mask on AN dataset.
that the attention weights between behaviors ğ‘6,ğ‘7,ğ‘8, and other
behaviors in the sequence are relatively smaller. This suggests that
ğ‘6,ğ‘7,ğ‘8, and other behaviors lack contextual relevance and are
likely abnormal. Turning to Figure 10(b), the reconstruction losses
3558Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
SD MD DM DD
Anomaly Type0.0000.0050.0100.0150.0200.0250.030False Positive RateÎ¼= 100 Î¼= 10 Î¼= 0.1 Î¼= 0.01
(a) False Positive Rate.
SD MD DM DD
Anomaly Type0.0150.0160.0170.0180.0190.0200.0210.022False Negative RateÎ¼= 100 Î¼= 10 Î¼= 0.1 Î¼= 0.01 (b) False Negative Rate.
Figure 8: False Positive Rate and False Negative Rate on AN
dataset under different ğœ‡.
0 100 200 300 400 500
Dimension of Embeddings0.950.960.970.980.991.00F1-Score
SD
MD
DM
DD
(a) Embedding dimension.
1 2 3 4
layers of Encoder/Decoder0.9840.9860.9880.9900.9920.9940.996F1-Score
SD
MD
DM
DD (b) Layers of encoder/decoder.
Figure 9: The influence of embedding dimension and en-
coder/decoder layer number on AN dataset.
for behaviors ğ‘6,ğ‘7, andğ‘8are notably high. SmartGuard identifies
these anomalies in the sequence, triggering an immediate alarm.
Case Study1ğ‘¡!offğ‘(ğ‘¡#ğ‘¡$ğ‘¡%
â¾ä¸ºåˆ†åˆ«æ˜¯ï¼šå…³ç”µè§†æœºã€å…³åºŠç¯ã€å…³çª—å¸˜ã€å…³ç©ºâ½“å‡€åŒ–å™¨ã€å…³â»”é”ï¼ˆä¸»â¼ˆâ¼Šç¡ï¼‰ï¼ˆè¿‡äº†â¼€ä¼šï¼‰ã€å…³æ‘„åƒå¤´ï¼ˆç›—è´¼å¼€å§‹æ½œâ¼Šï¼ˆå‡†å¤‡â¼¯ä½œï¼‰ï¼‰ã€å¼€çª—å¸˜ï¼ˆæ½œâ¼Šï¼‰ã€å…³çª—å¸˜ï¼ˆè¿˜åŸç°åœºï¼‰ã€å…³åºŠç¯ï¼ˆå¼€å§‹â¾çªƒï¼‰
SmartGuard!$ # " #    !"! $$$$! #    !

			
		





		
stopcloseoffrefreshlockoffopencloserefresh
(b) Reconstruction Loss(a) Attention Weight(c) Event
	
	

ğ‘'ğ‘"ğ‘!ğ‘#ğ‘$ğ‘%ğ‘&ğ‘)ğ‘(*
Figure 10: (a) Attention weights, (b) reconstrution loss and
(c) the corresponding events.
5.6 Embedding Space Analysis (RQ5)
We visualize the similarity between device embeddings and time
embeddings (i.e., hour embedding, day embedding and duration
embeddings) to analyze whether the model effectively learns the
relationship between behaviors. As shown in Figure 11(a), opening
the curtains usually occurs between 6-9 and 9-12 oâ€™clock because
users usually get up during this period, while closing the curtains
generally occurs between 21-24 oâ€™clock because the user usuallygo to bed during this period. The dishwasher usually runs between
12-15 and 18-21 oâ€™clock, which means that the user has lunch and
dinner during this period, and then washes the dishes. As shown
in Figure 11(b), users generally watch TV and do laundry on Satur-
days and Sundays. As shown in Figure 11(c), users usually take a
bath for about 1-2 hours, bath time longer than this may indicate
abnormality occurs.
Case Study1	


(a) Similarity between device control embedding and hour embedding
openclosewash
	

	
(b) Similarity between device control embedding and day embedding		

	

(c) Similarity between device control embedding and duration embedding
openclosewash		

	
		

	

shower	


Figure 11: Similarity between device control embedding and
time embedding.
6 CONCLUSION
In this paper, we introduce SmartGuard for unsupervised user be-
havior anomaly detection. We first devise a Loss-guided Dynamic
Mask Strategy (LDMS) to encourage the model to learn less frequent
behaviors that are often overlooked during the learning process.
Additionally, we introduce Three-level Time-aware Position Embed-
ding (TTPE) to integrate temporal information into positional em-
bedding, allowing for the detection of temporal context anomalies.
Furthermore, we propose a Noise-aware Weighted Reconstruction
Loss (NWRL) to assign distinct weights for routine behaviors and
noise behaviors, thereby mitigating the impact of noise. Compre-
hensive experiments conducted on three datasets encompassing
ten types of anomaly behaviors demonstrate that SmartGuard con-
sistently outperforms state-of-the-art baselines while delivering
highly interpretable results.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their constructive feedback
and comments. This work is supported by the Major Key Project of
PCL under grant No. PCL2023A06-4, the National Key Research and
Development Program of China under grant No. 2022YFB3105000,
and the Shenzhen Key Lab of Software Defined Networking under
grant No. ZDSYS20140509172959989. The first author, Jingyu
Xiao, in particular, wants to thank his parents Yingchao Xiao,
Aiping Li and his girlfriend Liudi Shen for their kind support.
He also thanks for all his friends who are the antidote during
his tired period.
3559KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jingyu Xiao et al.
REFERENCES
[1]Noureddine Amraoui and Belhassen Zouari. 2021. An ml behavior-based security
control for smart home systems. In Risks and Security of Internet and Systems: 15th
International Conference, CRiSIS 2020, Paris, France, November 4â€“6, 2020, Revised
Selected Papers 15. Springer, 117â€“130.
[2]Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, and Wei Liu.
2024. BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP.
InCVPR.
[3]Z. Berkay Celik, Leonardo Babun, Amit Kumar Sikder, Hidayet Aksu, Gang Tan,
Patrick D. McDaniel, and A. Selcuk Uluagac. 2018. Sensitive Information Tracking
in Commodity IoT. In 27th USENIX Security Symposium, USENIX Security 2018,
Baltimore, MD, USA, August 15-17, 2018, William Enck and Adrienne Porter Felt
(Eds.). USENIX Association, 1687â€“1704. https://www.usenix.org/conference/
usenixsecurity18/presentation/celik
[4]Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior
sequence transformer for e-commerce recommendation in alibaba. In Proceedings
of the 1st international workshop on deep learning practice for high-dimensional
sparse data. 1â€“4.
[5]Zhaomin Chen, Chai Kiat Yeo, Bu Sung Lee, and Chiew Tong Lau. 2018.
Autoencoder-based network anomaly detection. In 2018 Wireless telecommu-
nications symposium (WTS). IEEE, 1â€“5.
[6]Zhangyu Cheng, Chengming Zou, and Jianwei Dong. 2019. Outlier detection
using isolation forest and local outlier factor. In Proceedings of the conference on
research in adaptive and convergent systems. 161â€“168.
[7]Haotian Chi, Chenglong Fu, Qiang Zeng, and Xiaojiang Du. 2022. Delay Wreaks
Havoc on Your Smart Home: Delay-based Automation Interference Attacks. In
43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco, CA, USA,
May 22-26, 2022. IEEE, 285â€“302. https://doi.org/10.1109/SP46214.2022.9833620
[8]Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and
Even Oldridge. 2021. Transformers4rec: Bridging the gap between nlp and
sequential/session-based recommendation. In Proceedings of the 15th ACM Con-
ference on Recommender Systems (RecSys). 143â€“153.
[9]Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and
Depeng Jin. 2018. DeepMove: Predicting Human Mobility with Attentional
Recurrent Networks. In Proceedings of the 2018 World Wide Web Conference
(Lyon, France) (WWW â€™18) . International World Wide Web Conferences Steer-
ing Committee, Republic and Canton of Geneva, CHE, 1459â€“1468. https:
//doi.org/10.1145/3178876.3186058
[10] Earlence Fernandes, Jaeyeon Jung, and Atul Prakash. 2016. Security Analysis
of Emerging Smart Home Applications. In Proceedings of IEEE Symposium on
Security and Privacy, SP 2016, San Jose, CA, USA.
[11] Chenglong Fu, Qiang Zeng, Haotian Chi, Xiaojiang Du, and Siva Likitha Valluru.
2022. IoT Phantom-Delay Attacks: Demystifying and Exploiting IoT Timeout
Behaviors. In 52nd Annual IEEE/IFIP International Conference on Dependable
Systems and Networks, DSN 2022, Baltimore, MD, USA, June 27-30, 2022. IEEE,
428â€“440. https://doi.org/10.1109/DSN53405.2022.00050
[12] Chenglong Fu, Qiang Zeng, and Xiaojiang Du. 2021. HAWatcher: Semantics-
Aware Anomaly Detection for Appified Smart Homes. In 30th USENIX Security
Symposium, USENIX Security 2021, August 11-13, 2021, Michael D. Bailey and
Rachel Greenstadt (Eds.). USENIX Association, 4223â€“4240. https://www.usenix.
org/conference/usenixsecurity21/presentation/fu-chenglong
[13] Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, and
Wei Liu. 2024. Inducing High Energy-Latency of Large Vision-Language Models
with Verbose Images. In ICLR.
[14] Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, and Shu-Tao Xia. 2023. Backdoor
Defense via Adaptively Splitting Poisoned Dataset. In CVPR.
[15] Kuofeng Gao, Jindong Gu, Yang Bai, Shu-Tao Xia, Philip Torr, Wei Liu, and
Zhifeng Li. 2024. Energy-Latency Manipulation of Multi-modal Large Language
Models via Verbose Samples. arXiv preprint arXiv:2404.16557 (2024).
[16] Tianbo Gu, Zheng Fang, Allaukik Abhishek, Hao Fu, Pengfei Hu, and Prasant
Mohapatra. 2020. IoTGaze: IoT Security Enforcement via Wireless Context
Analysis. In 39th IEEE Conference on Computer Communications, INFOCOM 2020,
Toronto, ON, Canada, July 6-9, 2020. IEEE, 884â€“893. https://doi.org/10.1109/
INFOCOM41043.2020.9155459
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.
2022. Masked autoencoders are scalable vision learners. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 16000â€“16009.
[18] Hyunsik Jeon, Jongjin Kim, Hoyoung Yoon, Jaeri Lee, and U Kang. 2022. Accurate
action recommendation for smart home via two-level encoders and commonsense
knowledge. In Proceedings of the 31st ACM International Conference on Information
and Knowledge Management (CIKM). 832â€“841.
[19] Yunhan Jack Jia, Qi Alfred Chen, Shiqi Wang, Amir Rahmati, Earlence Fer-
nandes, Zhuoqing Morley Mao, and Atul Prakash. 2017. ContexloT: To-
wards Providing Contextual Integrity to Appified IoT Platforms. In 24th
Annual Network and Distributed System Security Symposium, NDSS 2017,
San Diego, California, USA, February 26 - March 1, 2017. The Internet So-
ciety. https://www.ndss-symposium.org/ndss2017/ndss-2017-programme/contexlot-towards-providing-contextual-integrity-appified-iot-platforms/
[20] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197â€“206.
[21] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[22] Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, Guorui
Zhou, Yang Song, and Hechang Chen. 2024. Contextual Distillation Model for
Diversified Recommendation. arXiv preprint arXiv:2406.09021 (2024). https:
//arxiv.org/abs/2406.09021
[23] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation forest. In 2008
eighth ieee international conference on data mining. IEEE, 413â€“422.
[24] Knud Lasse Lueth. 2018. State of the IoT 2018: Number of IoT devices now at 7B
â€“ Market accelerating. https://iot-analytics.com/state-of-the-iot-update-q1-q2-
2018-number-of-iot-devices-now-7b/.
[25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
in Neural Information Processing Systems (NIPS) 32 (2019).
[26] Phillip Rieger, Marco Chilese, Reham Mohamed, Markus Miettinen, Hossein
Fereidooni, and Ahmad-Reza Sadeghi. 2023. ARGUS: Context-Based Detection of
Stealthy IoT Infiltration Attacks. In Proceedings of the 32nd USENIX Conference on
Security Symposium (Anaheim, CA, USA) (SEC â€™23). USENIX Association, USA,
Article 241, 18 pages.
[27] Amit Kumar Sikder, Hidayet Aksu, and A Selcuk Uluagac. 2017. {6thSense}: A
context-aware sensor-based attack detector for smart devices. In 26th USENIX
Security Symposium (USENIX Security 17). 397â€“414.
[28] Amit Kumar Sikder, Leonardo Babun, Hidayet Aksu, and A. Selcuk Uluagac.
2019. Aegis: A Context-Aware Security Framework for Smart Home Systems. In
Proceedings of the 35th Annual Computer Security Applications Conference (San
Juan, Puerto Rico, USA) (ACSAC â€™19). Association for Computing Machinery,
New York, NY, USA, 28â€“41. https://doi.org/10.1145/3359789.3359840
[29] Vijay Srinivasan, John A. Stankovic, and Kamin Whitehouse. 2008. Protecting
your daily in-home activity information from a wireless snooping attack. In
UbiComp 2008: Ubiquitous Computing, 10th International Conference, UbiComp
2008, Seoul, Korea, September 21-24, 2008, Proceedings (ACM International Confer-
ence Proceeding Series, Vol. 344), Hee Yong Youn and We-Duke Cho (Eds.). ACM,
202â€“211. https://doi.org/10.1145/1409635.1409663
[30] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-
resentations from transformer. In Proceedings of the 28th ACM International
Conference on Information and Knowledge Management (CIKM). 1441â€“1450.
[31] Shisong Tang, Qing Li, Xiaoteng Ma, Ci Gao, Dingmin Wang, Yong Jiang, Qian
Ma, Aoyang Zhang, and Hechang Chen. 2022. Knowledge-based temporal fusion
network for interpretable online video popularity prediction. In Proceedings of
the ACM Web Conference 2022. 2879â€“2887.
[32] Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, Yong
Jiang, Qian Ma, and Aoyang Zhang. 2023. Counterfactual Video Recommendation
for Duration Debiasing. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 4894â€“4903.
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in Neural Information Processing Systems (NIPS) 30 (2017).
[34] Jincheng Wang, Zhuohua Li, Mingshen Sun, Bin Yuan, and John C. S. Lui. 2023.
IoT Anomaly Detection Via Device Interaction Graph. In 53rd Annual IEEE/IFIP
International Conference on Dependable Systems and Network, DSN 2023, Porto,
Portugal, June 27-30, 2023. IEEE, 494â€“507. https://doi.org/10.1109/DSN58367.
2023.00053
[35] Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Wenxin Tang, Runjie
Zhou, and Yong Jiang. 2023. User Device Interaction Prediction via Relational
Gated Graph Attention Network and Intent-aware Encoder. In Proceedings of
the 2023 International Conference on Autonomous Agents and Multiagent Systems
(AAMAS). 1634â€“1642.
[36] Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Zixuan Weng, Ruoyu Li,
and Yong Jiang. 2023. I Know Your Intent: Graph-enhanced Intent-aware User
Device Interaction Prediction via Contrastive Learning. Proceedings of the ACM
on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMUWT/UbiComp)
7, 3 (2023), 1â€“28.
[37] Junhai Zhai, Sufang Zhang, Junfen Chen, and Qiang He. 2018. Autoencoder and
its various variants. In 2018 IEEE international conference on systems, man, and
cybernetics (SMC). IEEE, 415â€“419.
[38] Qingsong Zou, Qing Li, Ruoyu Li, Yucheng Huang, Gareth Tyson, Jingyu Xiao,
and Yong Jiang. 2023. IoTBeholder: A Privacy Snooping Attack on User Habitual
Behaviors from Smart Home Wi-Fi Traffic. Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies (IMWUT/UbiComp) 7, 1 (2023),
1â€“26.
3560Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 7: Device information on FR dataset.
No. Device No. Device No. Device
0 AirConditioner 11 Fan 22 Refrigerator
1 AirPurifier 12 GarageDoor 23 RemoteController
2 Blind 13 Light 24 RobotCleaner
3 Camera 14 Microwave 25 Siren
4 ClothingCareMachine 15 MotionSensor 26 SmartLock
5 Computer 16 NetworkAudio 27 SmartPlug
6 ContactSensor 17 None 28 Switch
7 CurbPowerMeter 18 Other 29 Television
8 Dishwasher 19 Oven 30 Thermostat
9 Dryer 20 PresenceSensor 31 Washer
10 Elevator 21 Projector 32 WaterValve
Table 8: Device information on SP dataset.
No. Device No. Device No. Device
0 AirConditioner 12 GarageDoor 24 RobotCleaner
1 AirPurifier 13 Light 25 SetTop
2 Blind 14 Microwave 26 Siren
3 Camera 15 MotionSensor 27 SmartLock
4 ClothingCareMachine 16 NetworkAudio 28 SmartPlug
5 Computer 17 None 29 Switch
6 ContactSensor 18 Other 30 Television
7 CurbPowerMeter 19 Oven 31 Thermostat
8 Dishwasher 20 PresenceSensor 32 Washer
9 Dryer 21 Projector 33 WaterValve
10 Elevator 22 Refrigerator
11 Fan 23 RemoteController
A APPENDICES
A.1 Notations
Key notations used in the paper and their definitions are summa-
rized in Table 5.
Table 5: Main notations and their definitions.
Notation Definition
ğ‘‘,D a device/set of devices
ğ‘,C a device control/set of device controls
ğ‘ ,S a sequence/set of sequences
ğ‘› the lenghth of a sequence
ğ‘ a behavior
ğ‘¡ the timestamp of behavior
â„ğ‘œğ‘¢ğ‘Ÿ the hour of day of a behavior
ğ‘‘ğ‘ğ‘¦ the day of week of a behavior
ğ‘‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› the duration of a behavior
ğ‘ƒğ¸ the positional embedding function
ğ‘ƒğ¸(ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ),ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ the positional embedding and itâ€™s weight
ğ‘ƒğ¸(â„ğ‘œğ‘¢ğ‘Ÿ),ğ‘¤â„ğ‘œğ‘¢ğ‘Ÿ the hour embedding and itâ€™s weight
ğ‘ƒğ¸(ğ‘‘ğ‘ğ‘¦),ğ‘¤ğ‘‘ğ‘ğ‘¦ the day embedding and itâ€™s weight
ğ‘ƒğ¸(ğ‘‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›),ğ‘¤ğ‘‘ğ‘¢ğ‘Ÿ the duration embedding and itâ€™s weight
ğ‘ƒğ¸ the positional embedding
â„ğ‘ the device control embedding
h the behavior embedding
Lğ‘Ÿğ‘’ğ‘ the reconstruction loss
â„“ğ‘–,Lvec the loss ofğ‘–-th behavior and loss vector
ğ‘¤ğ‘–,Wvec the weight of ğ‘–-th behavior and weight vector
ğ‘ğ‘– the normalized weight of ğ‘–-th behavior
ğ‘šğ‘ğ‘ ğ‘˜ the mask vector
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘ ) the anomaly score of sequence ğ‘ 
ğ‘¡â„ the thresholdA.2 Device information of different dataset
The AN, FR and SP data sets contain 36, 33, and 34 devices respec-
tively, as shown in Table 6, Table 7, and Table 8.
Table 6: Device information on AN dataset.
No. Device No. Device No. Device
0 AC 12 LED 24 projector
1 heater 13 locker 25 washing_machine
2 dehumidifier 14 bathheater 26 kettle
3 humidifier_1 15 water_cooler 27 dishwasher
4 fan 16 curtains 28 bulb_1
5 standheater 17 outlet 29 TV
6 aircleaner 18 audio 30 pet_feeder
7 humidifier_2 19 plug 31 hair_dryer
8 desklight 20 bulb_2 32 window_cleaner
9 bedight_1 21 soundbox_1 33 bedlight_2
10 camera 22 soundbox_2 34 bedlight_3
11 sweeper 23 refrigerator 35 cooler
A.3 Data collection
Testbed and Participants. To create a practical and viable smart
home model, we implemented our experimental platform within
an apartment setting to gather user usage data of various devices,
forming our smart home user behavior dataset (AN). Three vol-
unteers were recruited to simulate the typical daily activities of
a standard family, assuming the roles of an adult male, an adult
female, and a child. The experimental platform comprises a compre-
hensive selection of 36 popular market-available devices, detailed
in Table 6, with their deployment illustrated in Figure 12.
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Figure 12: Overview of testbed setup.
Normal Behavior Collection. We enlisted volunteers to re-
side in apartments and encouraged them to utilize equipment in
accordance with their individual habits. Throughout the designated
period of occupancy, we refrained from actively or directly inter-
vening in the usersâ€™ behavior. However, we implemented a system
where users consistently logged their activities. Following the con-
clusion of the data collection phase, we reviewed the device usage
logs via the smart home app, amalgamating these logs with the
usersâ€™ behavior records to compile a comprehensive user behavior
dataset. To mitigate potential biases arising from acclimating to
3561KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jingyu Xiao et al.
a new living environment, participants were required to inhabit
the experimental setting for a minimum of two weeks before the
formal commencement of data collection. All users possessed com-
prehensive knowledge of the IoT devices and applications in use.
Subsequent to check-in, control of all devices was relinquished to
the users, who were duly informed in advance that their device
usage would be subsequently reviewed and analyzed by our team
Anomaly Behavior Injection. We insert abnormal behaviors
in Table 2 into normal behavior sequences to construct abnormal
behavior sequences. Then the abnormal behavior sequences. Then
the abnormal behavior sequence and the normal behavior sequence
together form the test dataset. The anomaly behavior sequences
examples are shown in Figure 13.
light flickering
up
wash
on
offon
off
on
off
off
onon
off
on
off
down
on
off
stop
on
onTV flickeringonon
light up
strong
on
off
on
off
camera flickering
off
stopcloseoffrefreshlockoffopencloserefreshcamera turn offwhile smartlock lockopen the windowwhile smartlock lock
offlight up
off
refresh
on
off
on
off
refreshrefresh
ononmore than three hoursshower for long timemicrowave runsfor long time
on
down
strong
sound down
open
on
onmore than three hours
down
off
off
open
open
washon
onsound up
down
on
up
offmidnightopen the windowat midnight
refreshoff
open
close
refresh
on
off
refreshon
midnightopen the watervalveat midnight
Figure 13: Examples of anomaly behavior sequences.
A.4 Detailed experimental settings
All models (including baselines and SmartGuard) are implemented
by PyTorch [ 25] and run on a graphic card of GeForce RTX 3090
Ti. All models are trained with Adam optimizer [ 21] with learning
rate 0.001. We train SmartGuard to minimize Lğ‘Ÿğ‘’ğ‘in Equation (14).
During training, we monitor reconstruction loss and stop training
if there is no performance improvement on the validation set in10 steps. For model hyperparameters of SmartGuard, we set the
batch size to 512 and the initial weights of TTPE are ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ =
0.1,ğ‘¤â„ğ‘œğ‘¢ğ‘Ÿ=0.4,ğ‘¤ğ‘‘ğ‘ğ‘¦=0.4, andğ‘¤ğ‘‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› =0.7. For mask ratio
and step without step, we search in {0.2,0.4,0.6,0.8}and{3,4,5,6},
respectively. We chose the number of encoder and decoder layers
in{1,2,3,4}, and the embedding size in {8,16,32,64,128,256,512}.
A.5 Mask strategy deep dive
To verify the effectiveness of LDMS, we compared it with the three
baselines (w/o mask, random mask and top- ğ‘˜loss mask) mentioned
in the section 4.2. As illustrated in Figure 14(a), LDMS consistently
outperforms all other mask strategies across four types of anom-
alies. The results presented in Figure 14(b) further demonstrate that
LDMS exhibits the smallest variance in reconstruction loss through-
out the training process, which demonstrates that SmartGuard
learns both easy-to-learn behaviors and hard-to-learn behaviors
very well. We also plotted the loss distribution diagram under dif-
ferent mask strategies. As shown in Figure 15, LDMS shows the
smallest reconstruction loss and variance, which demonstrates that
our mask strategy can better learn hard-to-learn behaviors. We can
still observe behaviors with high reconstruction loss as pointed by
the red dashed arrow after applying LDMS, which is likely to be
noise behaviors, thus itâ€™s necessary to assign small weights for these
noise behaviors during anomaly detection for avoiding identifying
normal sequences containing noise behaviors as abnormal.
SD MD DM DD
Anomaly Type0.98250.98500.98750.99000.99250.99500.99751.0000F1-ScoreLDMS
T op-k Loss MaskRandom Mask
w/o Mask
(a) F1-Score.
10 11 12 13
Epoch0.00.10.20.30.40.50.60.7Reconstruction Loss VarianceLDMS
T op-k Loss Mask
Random Mask
w/o Mask (b) Variance.
Figure 14: Performance and reconstruction loss variance on
AN dataset under different mask strategy.
off
stopcloseoffrefreshlockoffopencloserefreshwindow opens while smartlocklock
	(#$	
'!$#(#$		
#"%&$'&#"#%%)#% "#!% #  #%%% #%%'*"!% noise behaviors
Figure 15: Reconstruction loss distribution under different
mask strategy on SP dataset.
3562