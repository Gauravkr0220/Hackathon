Unveiling Vulnerabilities of Contrastive Recommender Systems
to Poisoning Attacks
Zongwei Wang
zongwei@cqu.edu.cn
Chongqing University
Chongqing, ChinaJunliang Yu
jl.yu@uq.edu.au
The University of Queensland
Brisbane, AustraliaMin Gaoâˆ—
gaomin@cqu.edu.cn
Chongqing University
Chongqing, China
Hongzhi Yinâˆ—
h.yin1@uq.edu.au
The University of Queensland
Brisbane, AustraliaBin Cui
bin.cui@pku.edu.cn
Peking University
Beijing, ChinaShazia Sadiq
shazia@eecs.uq.edu.au
The University of Queensland
Brisbane, Australia
ABSTRACT
Contrastive learning (CL) has recently gained prominence in the
domain of recommender systems due to its great ability to enhance
recommendation accuracy and improve model robustness. Despite
its advantages, this paper identifies a vulnerability of CL-based
recommender systems that they are more susceptible to poisoning
attacks aiming to promote individual items. Our analysis indicates
that this vulnerability is attributed to the uniform spread of repre-
sentations caused by the InfoNCE loss. Furthermore, theoretical and
empirical evidence shows that optimizing this loss favors smooth
spectral values of representations. This finding suggests that attack-
ers could facilitate this optimization process of CL by encouraging
a more uniform distribution of spectral values, thereby enhancing
the degree of representation dispersion. With these insights, we
attempt to reveal a potential poisoning attack against CL-based
recommender systems, which encompasses a dual-objective frame-
work: one that induces a smoother spectral value distribution to
amplify the InfoNCE lossâ€™s inherent dispersion effect, named disper-
sion promotion; and the other that directly elevates the visibility of
target items, named rank promotion. We validate the threats of our
attack model through extensive experimentation on four datasets.
By shedding light on these vulnerabilities, our goal is to advance
the development of more robust CL-based recommender systems.
The code is available at https://github.com/CoderWZW/ARLib.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems.
KEYWORDS
Contrastive Learning, Recommender Systems, Poisoning Attacks,
Self-Supervised Learning
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671795ACM Reference Format:
Zongwei Wang, Junliang Yu, Min Gao, Hongzhi Yin, Bin Cui, and Shazia
Sadiq. 2024. Unveiling Vulnerabilities of Contrastive Recommender Systems
to Poisoning Attacks. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671795
1 INTRODUCTION
In recent years, contrastive learning (CL) [ 13,14] has emerged as a
promising self-supervised learning paradigm in deep representa-
tion learning, demonstrating its significant potential across various
domains [ 30,50]. This approach encourages models to learn infor-
mative features and invariance from vast amounts of unlabeled
data, achieving enhancement in the modelâ€™s generalization abil-
ity and leading to improved accuracy and robustness in different
tasks. When applied to recommender systems, CL has been exten-
sively studied and proven effective in enhancing recommendation
quality [ 51,54]. Additionally, its utilization in this domain exhibits
a remarkable level of robustness comparable to that observed in
computer vision [ 8,40] and natural language processing [ 4,6], con-
firming its ability to uphold recommendation accuracy even in the
presence of noisy data scenarios [39, 43, 47].
However, it is noteworthy that existing studies predominantly
concentrate on the overall robustness of CL-based recommenda-
tion, overlooking the vulnerabilities affecting individual items. For
example, in the context of product recommendations, there have
been cases where certain low-quality and unpopular products are
intentionally promoted to gain profits [ 38]. Similarly, in the domain
of news recommendations, misleading information is disseminated
to specific groups on purpose [ 48]. It is conceivable that the suc-
cessful promotion of target items with ulterior motives can have a
significantly negative impact on user experience and even social
stability. Within the realm of recommender systems, one prominent
type of attack, referred to poisoning attacks, can focus on individ-
ual items [ 17,49,57,60,63]. This type of attack fulfills deliberate
manipulation of recommendation outcomes through strategic in-
jection of malicious profiles, whose ultimate goal is to amplify the
exposure of target items. As poisoning attacks are widespread and
pose a genuine threat in many real-world recommender systems, a
natural question arises: Can CL-based recommendation effectively
withstand the poisoning attacks on individual target items?
3311
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zongwei Wang et al.
0 0.01 0.02 0.03 0.04 0.05
Attack Size0.1200.1250.1300.1350.1400.1450.1500.1550.160Recall@50
DouBan
0 0.01 0.02 0.03 0.04 0.05
Attack Size0.1850.1900.1950.2000.2050.2100.2150.220Recall@50
Epinions
0.01 0.02 0.03 0.04 0.05
Attack Size0.0000.0020.0040.0060.0080.0100.0120.014Hit Ratio@50DouBan
0.01 0.02 0.03 0.04 0.05
Attack Size0.0000.0050.0100.0150.0200.025Hit Ratio@50EpinionsLightGCN SSL4Rec SGL SimGCL XSimGCL LightGCN SSL4Rec SGL SimGCL XSimGCL
Figure 1: The comparison of LightGCN, SSL4Rec, SGL, SimGCL, and XSimGCL on DouBan and Epinions under RandomAttack.
Attack size represents the ratio of the number of malicious users to the total number of users.
To address our concerns, we first conducted a series of exper-
iments specifically focused on poisoning attacks against recom-
mender systems, comparing the performance of recommendation
models with and without CL. For our experiments, we employed
the widely used model, LightGCN [ 10], as the recommendation
encoder, and evaluated four popular and easily reproducible CL-
based recommendation methods: SSL4Rec [ 46], SGL [ 43], SimGCL
[53], and XSimGCL [ 51], which differ in the way of constructing
contrastive views. To initially explore the impact of poisoning at-
tacks, we adopted a common attack approach (RandomAttack) [ 16]
to inject randomly constructed malicious user profiles into public
datasets. Regrettably, as illustrated in Figure 1, despite the ability of
CL-based recommendation methods to maintain stability in overall
recommendation accuracy (measured by Recall@50), the results
revealed that they were all more susceptible to poisoning attacks
that aim to increase the exposure of target individual items (mea-
sured by Hit Ratio@50). This unexpected outcome has sparked a
profound sense of worry and urgency within us, as it implies that
the widespread adoption of CL-based recommendation may have
caught the considerable attention of potential attackers.
To identify the underlying causes of CL-based recommenda-
tionâ€™s vulnerability against poisoning attacks, we investigate the
embedding space learned by non-CL and CL-based recommenda-
tion methods. By visualizing the distributions of the representations,
we find that the objective function of CL, InfoNCE [ 24], is the core
factor. In the absence of CL, the representations show local cluster-
ingcharacteristics, where users and popular items tend to gather
and hinder cold items from reaching more users. In contrast, the
addition of CL provides repulsion between any two nodes, thus
suppressing the aggregation of users and popular items and lead-
ing to globally dispersed representation distribution. Our findings
reveal that CL is a double-edged sword. On the one hand, it has an
inhibitory effect on popular items and then alleviates the popularity
bias problem [ 1]. On the other hand, non-popular items could be
easier to enter recommendation lists. As target items in poisoning
attacks are often unpopular, CL becomes a booster to increase the
exposure of target items. This inherent flaw gives rise to a new
question: Does there exist a potential poisoning attack that poses a
greater threat to CL-based recommendation than current poisoning
attacks?In this paper, we give an affirmative answer to this question.
Given that the CL objective serves as an auxiliary task relative to
the primary recommendation objective, the representation distri-
bution from the joint optimization (i.e., the attraction endowed by
the recommendation objective and the repulsion induced by the CL
objective) might not be sufficiently uniform to meet the attackerâ€™s
criteria. Potential adversaries may thus choose to amplify the dis-
persion degree of representation distribution, ensuring users are
more broadly dispersed across the embedding space. This increased
dispersion creates a conducive attack environment, providing target
items with expanded opportunities to reach more diverse users.
Based on the above insights, we theoretically and empirically
establish that the fine-tuning of CL correlates with smooth spec-
tra of representations. This revelation supports the notion that
attackers can facilitate the optimization of CL loss by promoting a
smoother spectral value distribution, consequently increasing the
degree of dispersion. On this basis, an attack model is proposed to
uncover the poisoning mechanism against CL-based recommen-
dation, aiming to deepen the understanding of the robustness of
CL-based recommender systems and safeguard them from manip-
ulation. Specifically, we explore a bi-level optimization based at-
tack framework against CL-based recommendation, named the
Contrastive Learning Recommendation Attack (CLeaR). The CLeaR
pursues two primary objectives: Firstly, the dispersion promotion
objective focuses on achieving a smoother spectral distribution of
representations to facilitate optimizing CL loss, thereby enhancing
the extent of their dispersion across the vector space. Secondly, the
rank promotion objective is dedicated to optimizing the visibility
of target items, ensuring their maximized exposure to an extensive
user base.
Our contributions can be summarized as follows:
â€¢We delve into the vulnerability of CL-based recommender sys-
tems to poisoning attacks, which has still remained unexplored
despite the popularity of CL-based recommendation.
â€¢We propose a potential poisoning attack strategy to examine the
detrimental impact of poisoning attacks on CL-based recommen-
dation.
â€¢Extensive experiments on several datasets illustrate that the pro-
posed attack method carries a greater threat than existing attack
methods to both general and CL-based recommendation.
3312Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Figure 2: Representation distribution on Epinions under poisoning attacks (RandomAttack). Recommendation methods without
CL show a local clustering pattern, while the ones with CL show a global dispersion pattern.
2 PRELIMINARIES
2.1 Recommendation Models
Recommendation Task. LetUandIrespectively be the sets of
users and items, and Ddenote the user interaction data. A recom-
mendation model aims to learn a low-dimensional representation
setZfor predicting the preference of user ğ‘¢âˆˆU to itemğ‘–âˆˆI,
which can be calculated by zğ‘¢andzğ‘–âˆˆZ. A widely used recommen-
dation objective function is Bayesian Personalized Ranking (BPR)
[27], which is formulated as:
Lğ‘Ÿğ‘’ğ‘= E
(ğ‘¢,ğ‘–,ğ‘—)âˆ¼ğ‘ƒDâˆ’logğœ™(zğ‘‡
ğ‘¢zğ‘–âˆ’zğ‘‡
ğ‘¢zğ‘—), (1)
whereğœ™is the sigmoid function, ğ‘ƒD(Â·)refers to the distribution
of the interaction data, and the tuple (ğ‘¢,ğ‘–,ğ‘—)includes a user ğ‘¢, a
positive sample item ğ‘–with observed interactions, and a negative
sample item ğ‘—without observed interactions with ğ‘¢.
Contrastive Learning Task. CL [3] aims to maximize the consis-
tency between augmentations of the original representations (e.g.,
learned from perturbed data). The most commonly used contrastive
loss is InfoNCE [24] formulated as follows:
Lğ‘ğ‘™=âˆ’âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)logexp
zâ€²
ğ‘ğ‘‡zâ€²â€²
ğ‘/ğœ
Ã
ğ‘›âˆˆ(UâˆªI) exp
zâ€²
ğ‘ğ‘‡zâ€²â€²
ğ‘›/ğœ, (2)
where zğ‘›represents the negative sample from UorI,zis the L2
normalized representations of z,zâ€²
ğ‘andzâ€²â€²
ğ‘are learned from differ-
ent augmentations of the entity ğ‘, andğœis the hyper-parameter,
known as the temperature coefficient.
Generally, CL-based recommendation models optimize a joint
loss where the recommendation is the primary task while the CL
task plays a secondary role whose magnitude is controlled by a
coefficientğœ”. The joint loss is formulated as follows:
L=Lğ‘Ÿğ‘’ğ‘+ğœ”Lğ‘ğ‘™. (3)
2.2 Poisoning Attacks
Attackerâ€™s Goal. In recommender systems, poisoning attacks are
of two kinds: non-targeted attacks aiming to degrade overall per-
formance, and targeted attacks intended to promote specific items.
Our attack falls under the category of targeted promotion, wherewe aim to maximize the likelihood of target items Iğ‘‡appearing in
as many usersâ€™ top-K recommendation lists as possible.
Attackerâ€™s Prior Knowledge. There are two cases for attackers to
know about victim models. In the white-box setting, attackers have
full access to the recommendation model. While in the black-box
setting, attackers only have access to interaction information D
and do not possess any prior knowledge about the recommenda-
tion model. To figure out the attack mechanism against CL-based
recommender systems and investigate the theoretical upper bound
of potential damage, our attack strategy is primarily concentrated
on the white-box setting.
Attackerâ€™s Capability. Attackers can inject a set of malicious users
Uğ‘€and generate well-designed interactions Dğ‘€. However, due to
limited resources, the attacker can only register a restricted number
of malicious users with limited interactions. If not stressed, we
assume that the attacker can register malicious users equivalent to
1% of the total number of genuine users, and the interaction count
of each malicious user can not be more than the average number
of interactions per normal user. This setting is commonly used in
related studies [9, 38, 60].
3 HOW POISONING ATTACKS AFFECT
CL-BASED RECOMMENDATION
Given the fact that CL-based recommender systems are more vul-
nerable to poisoning attacks aimed at target item promotion, this
section will explore the cause behind the susceptibility.
3.1 Global Dispersion Affects Local Clustering
Drawing inspiration from prior research [ 34,36,53], we first visu-
alize the representation distributions learned by recommendation
models with and without CL when subjected to a poisoning attack.
The visualization results on Epinions dataset, as shown in Figure 2,
reveal two distinct patterns in the absence and presence of CL:
â€¢Local Clustering (without CL): recommendation without CL
(LightGCN) exhibits a tendency to cluster popular items and
users together, while cold items are positioned distantly from
the user clusters.
â€¢Global Dispersion (with CL): in contrast, recommendation with
CL (SSL4Rec, SGL, SimGCL, and XSimGCL) exhibits a more
balanced distribution, extending throughout the entire vector
3313KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zongwei Wang et al.
space. Consequently, this distribution brings popular items and
cold items into closer proximity.
The details on the design of visualization experiments, along
with additional visualization results on the DouBan [ 62] dataset
are available in Appendix A.1. Furthermore, the emergence of two
distinct patterns (i.e., local clustering and global dispersion) from
empirical observations can be explained from the loss optimization
perspective.
BPR Loss Causes Local Clustering. The local clustering pattern
has been discovered in many studies [ 53,60]. The gradient update
formulas of BPR loss from Equation (1) are as follows:
âˆ‡zğ‘¢=âˆ’(zğ‘–âˆ’zğ‘—)Â·exp(zğ‘‡ğ‘¢zğ‘–âˆ’zğ‘‡ğ‘¢zğ‘—)
1+exp(zğ‘‡ğ‘¢zğ‘–âˆ’zğ‘‡ğ‘¢zğ‘—). (4)
By examining Equation (4), the gradient update direction ( âˆ’âˆ‡ğ‘§ğ‘¢)
pulls users towards positive item ğ‘–while distancing themselves
from negative item ğ‘—. Given the long-tail distribution characteristic
of recommendation data, popular items have a higher likelihood
of being sampled. Consequently, users are more inclined to be
attracted to popular items, leading to a clustering effect between
users and popular items. In contrast, cold items are often treated as
negative samples and are pushed away from the user clusters.
CL Loss Causes Global Dispersion. Recall Equation (2), we can
derive the following format:
Lğ‘ğ‘™=âˆ’(âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)(zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘/ğœ)âˆ’âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)logâˆ‘ï¸
ğ‘›âˆˆ(UâˆªI)exp(zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘›/ğœ)),(5)
and we can deduce that minimizing CL loss involves maximizing
the similarity between zâ€²ğ‘andzâ€²â€²ğ‘, while simultaneously minimizing
the similarity between zâ€²ğ‘andzâ€²â€²ğ‘›. We attribute global dispersion
to the second term of the formula: this process of CL loss opti-
mization can be perceived as pushing zâ€²ğ‘andzâ€²â€²ğ‘›away from each
other. Consequently, the CL loss inherently generates a natural
repulsion among the representations, causing the representation to
be roughly uniformly distributed.
Discussion. The CL loss appears to be a double-edged sword in
recommendation. On the one hand, it helps to mitigate the popular-
ity bias problem. The global dispersion pattern caused by CL loss
provides cold items with an opportunity to be exposed to a larger
user base. This observation is supported by Figure 2 and 6, which
show that many cold items are in close proximity to more users in
CL-based recommendation methods. On the other hand, the CL loss
breaks down the local clustering pattern, inadvertently facilitating
the targeting of specific items through poisoning attacks. Partic-
ularly, considering that the target items are often cold items, this
property of CL gives these items more exposure to a larger num-
ber of users through the poisoning attack. This raises a pertinent
question of whether attackers can intensify the global dispersion
patterns caused by CL loss. In a scenario where such optimization
is accentuated, target items would have a higher visibility in the
recommendation lists of a wider user base. In the following subsec-
tion, we will explicate the correlation between the optimization of
CL loss and the spectral characteristics of representations.3.2 Analysis of CL Loss from a Spectral
Perspective
In this section, we first theoretically show that the optimization of
CL loss favors smooth representationsâ€™ spectral values (also called
singular values). Then, we empirically show the distribution of
singular values decomposed by representation learned by recom-
mendation models with and without CL.
3.2.1 Theoretical Analysis. Inspired by [ 21], we consider the
decomposition of representations and use Singular Value Decom-
position (SVD) [ 11] to get the decomposition of Z, i.e., Z=LÎ£Rğ‘‡,
where L=(l1,...,lğ‘‘)representsğ‘‘-order orthogonal matrix and
R=(r1,...,rğ‘)representsğ‘-order orthogonal matrix. The terms
landrcorrespond to left singular vector and right singular vec-
tor, respectively. Here, ğ‘‘is the dimensional length of the repre-
sentation vector, while ğ‘is the number of all users and items.
Î£=ğ‘‘ğ‘–ğ‘ğ‘”(ğœ1,...,ğœğ‘‘)is theğ‘‘Ã—ğ‘rectangular diagonal matrix, with
non-negative singular values ğœarranged in descending order. Then,
we can get an upper bound of the CL loss:
Proposition 1 .Give the representations Zâ€²andZâ€²â€²which are learned
on augmented views and the corresponding singular values Î£â€²=
ğ‘‘ğ‘–ğ‘ğ‘”(ğœâ€²
1,...,ğœâ€²
ğ‘‘)andÎ£â€²â€²=ğ‘‘ğ‘–ğ‘ğ‘”(ğœâ€²â€²
1,...,ğœâ€²â€²
ğ‘‘),an upper bound of the
CL loss is given by:
Lğ‘ğ‘™<ğ‘max
ğ‘—ğœâ€²
ğ‘—ğœâ€²â€²
ğ‘—âˆ’âˆ‘ï¸
ğ‘–ğœâ€²
ğ‘–ğœâ€²â€²
ğ‘–+ğ‘logğ‘.(6)
Proof. See Appendix A.2.
Since minimizing the CL loss is equivalent to minimizing this
upper bound, Proposition 1 suggests that the minimization of the CL
loss can be transformed into a dual-objective optimization problem:
minimizing the first term maxğ‘—ğœâ€²
ğ‘—ğœâ€²â€²
ğ‘—, which corresponds to the
largest singular value product; and maximizing the second term
Â®ğˆâ€²ğ‘‡Â®ğˆâ€²â€², which corresponds to the sum of the product of singular
values. This approach targets the reduction of the highest singular
value similarities while enhancing the overall similarity of the
singular values between Zâ€²andZâ€²â€². Such a minimization strategy
is inclined to yield a smoother distribution of singular values. Next,
we will empirically show the difference in singular values derived
from CL-based and non-CL recommendation models.
3.2.2 Empirical Observations. In the experimental setup anal-
ogous to prior configurations, representations are derived from
LightGCN, SSL4Rec, SGL, SimGCL, and XSimGCL. Subsequent to
this, SVD is employed to decompose these representations into
singular values, which are then sorted.
Figure 3 illustrates the distribution of singular values for both
non-CL and CL-based recommendations. Both distributions obey
the long-tail distribution. However, notable distinctions can be ob-
served: the LightGCN curves appear sharper across both datasets,
whereas the curves corresponding to CL-based recommendation
maintain smooth. Our theoretical analysis can explain this phenom-
enon, i.e., the optimization of CL loss tends to gravitate towards
diminishing the maximal spectral value, concurrently facilitating a
smoother distribution of spectral values. Therefore, we can observe
that the CL-based recommendation models tend to temper larger
singular values, simultaneously advocating for a smooth transition
between singular values.
3314Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
0 10 20 30 40 50 60
Sorted Singular Value Index20406080100120Singular ValueDouBan
LightGCN
SSL4Rec
SGL
SimGCL
XSimGCL
0 10 20 30 40 50 60
Sorted Singular Value Index50100150200250Epinions
LightGCN
SSL4Rec
SGL
SimGCL
XSimGCL
Figure 3: Singular Value Distributions on two datasets.
Discussion. Through empirical and theoretical demonstration,
we conclude that the optimization of CL loss can contribute to a
smooth spectral value distribution. This view prompts us to wonder
whether a poisoning attack can become more powerful by addition-
ally promoting smoother spectral values in representations. Such
an approach will favor the optimization of CL loss and, in turn,
disperse user representations. In the next section, we will introduce
a novel poisoning attack method that focuses on spectral values of
representations to enhance attack effectiveness.
4 PROPOSED POISONING ATTACKS
In this section, we reveal a more powerful poisoning attack, CLeaR,
which aims to homogenize spectral values in representations, thereby
impacting a broader user base effectively.
4.1 Attack as a Bi-Level Optimization Problem
We define the poisoning attack as the bi-level optimization setting:
the inner optimization is to derive the recommendation model
details based on real user interactions and fixed malicious user
interactions set. On the other hand, the outer optimization is to
adjust the interactions between malicious users and items, with
the goal of increasing the presence of target items in more user
recommendation lists. The bi-level optimization process is defined
by the following equations:
max
Dğ‘€Lğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜(D,Dğ‘€,Zâˆ—
U,Zâˆ—
I),
s.t.,Zâˆ—
U,Zâˆ—
I=arg min
ZU,ZILğ‘Ÿğ‘’ğ‘(ğ‘“(D,Dğ‘€)),(7)
whereğ‘“is the function of the recommendation model, Zâˆ—
Uand
Zâˆ—
Iare the optimal user representations and item representations,
andLğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ is the loss function for evaluating attack utility. It is
worth noting that there are no interactions for the initialization
ofDğ‘€, and the interactions are generated as the inner and outer
optimizations are alternately performed.
4.2 CLeaR Overview
To explore the worst-case scenario, we provide the white-box im-
plementation of CLeaR under the bi-level optimization framework.
This framework is designed to be adaptable to black-box scenarios
through the integration of proxy models. The simple overview can
be seen in Figure 4, encompassing the following steps.
4.2.1 Inner Optimization. Initially, it is essential to obtain the
representations of both users ZUand items ZI, as this serves as
the fundamental prior knowledge for subsequent steps. To achieve
Poisoning DataInner Optimization (Recommendation)
â€¦â€¦11210Spectral Value VectorUsers and ItemsBalanced Spectral Value Vector1112UserTarget ItemÃ—>UserLast Item (Item3)Item1>Item2>Item3Item1>Item2>Target ItemÃ—â€¦â€¦â€¦Normal UserMalicious UserNormal ItemTarget ItemOuter Optimization (Attack)EncodeintoRepresentations
Dispersion PromotionRank PromotionUpdate Poisoning DataInput RepresentationsMalicious UserNormal ItemTarget ItemNormal UserFigure 4: The overview of CLeaR.
this, we train the recommendation model to obtain optimal repre-
sentations as follows:
Zâˆ—
U,Zâˆ—
I=arg min
ZU,ZIL(ğ‘“(D,Dğ‘€)).(8)
4.2.2 Outer Optimization. In this step, by establishing interac-
tions between malicious users and target items, as well as between
malicious users and the candidate items, the goal of the attack is
to promote target items to enter more recommendation lists. To
achieve this goal, the CleaR combines two pivotal objective func-
tions: a dispersion promotion objective that indirectly augments
the dispersion of representations and a rank promotion objective
to pull items into recommendation lists directly.
Dispersion Promotion Objective: This way aims to promote
spectral values of representations to be smoother, thereby facili-
tating the optimization of CL loss and a comprehensive dispersion
across the entirety of the vector space. In this case, target items
naturally gain augmented opportunities to optimize proximity to
broader users. In alignment with this concept, a common way is to
first decompose a representation Zto get Â®ğˆby SVD, then align the
spectral values Â®ğˆof a smooth and flat distribution. Therefore, we
initially formulate the dispersion promotion objective Lğ·as:
Lğ·=ğ‘ ğ‘–ğ‘š(Â®ğˆ,ğ‘ğ‘¥âˆ’ğ›½), (9)
whereğ‘ ğ‘–ğ‘š(Â·)calculates the similarity of two distribution, ğ‘ğ‘¥âˆ’ğ›½is
the power-law distribution, and ğ‘andğ›½are the hyper-parameters.
A smaller value of ğ›½would lead to a gradual transition into the
long-tail part of the distribution.
However, the applicability of this method encounters limitations
because obtaining the spectrum of a matrix is time-consuming and
3315KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zongwei Wang et al.
the backpropagation of SVD is unstable, which often results in nu-
merical inaccuracies. To address this challenge, we adopt a spectral
approximation approach, drawing inspiration from rank-1 approxi-
mation techniques as discussed in [ 55,61], which directly calculates
approximated representations Ë†Z, thereby bypassing the need for
the SVD process. The equation of the approximated representation
is as follows:
Ë†Z=Zâˆ’ZVâ€²Vâ€²ğ‘‡
||Vâ€²||2
2, (10)
where Vâ€²=Zğ‘‡ZV, and Vis randomly re-initialized in each itera-
tion. This approach can approximately adjust the representation Z
by applying a scaling mechanism where larger singular values are
diminished using smaller scaling factors, whereas smaller singular
values are amplified with larger factors. Consequently, this leads
to the approximated representation Ë†Zexhibiting a more uniform
singular value distribution following its decomposition.
After getting the Ë†Z, we can avoid repetitive computations and
backpropagation of SVD. The Equation (9) is transformed into cal-
culating a similarity between the original representation Zand
the approximated representation Ë†Z, and in our method, we use ğ¿1
distance as ğ‘ ğ‘–ğ‘š(Â·). The formulation of the dispersion promotion
objective is as follows:
Lğ·=ğ‘ ğ‘–ğ‘š(Z,Ë†Z)=âˆ’||ZVâ€²Vâ€²ğ‘‡||
||Vâ€²||2
2. (11)
Rank Promotion Objective: The dispersion promotion objective
is strategically crafted to amplify the dispersion of representations.
From the perspective of the user, such dispersion merely estab-
lishes a level playing field for all items, which reduces the difficulty
for target items to be included in the userâ€™s recommendation list.
Nonetheless, it is important to note that this equalization does not
inherently guarantee that the target item will surpass others in
terms of recommendation priority. Consequently, there is a need
for an objective tailored to boost the ranking of target items in user
recommendations, facilitating their presence in the userâ€™s list of
preferences. Taking inspiration from the latest target attack meth-
ods in the recommendation, we optimize the differentiable CW loss
[29] as the rank promotion objective. We aim to find the last item
in the recommended list and increase the probability of the target
items more than such the last item. The formulation of the CW loss
is:
Lğ‘…=âˆ‘ï¸
ğ‘¢âˆˆ(U)âˆ‘ï¸
ğ‘¡âˆˆIğ‘‡âˆ§(ğ‘¢,ğ‘¡)âˆ‰Dğ‘”(zğ‘‡
ğ‘¢zğ‘¡âˆ’ min
ğ‘–âˆˆIrecğ‘¢âˆ§ğ‘–âˆ‰Iğ‘‡n
zğ‘‡
ğ‘¢zğ‘–o
),
ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘”(ğ‘¥)=(
ğ‘¥, ğ‘¥â‰¥0,
ğ‘’ğ‘¥âˆ’1, ğ‘¥<0,(12)
where minğ‘–âˆˆIrecğ‘¢âˆ§ğ‘–âˆ‰Iğ‘‡
zğ‘‡ğ‘¢zğ‘–	
means the last item in the recommen-
dation list of user ğ‘¢, andIğ‘Ÿğ‘’ğ‘ğ‘¢is the set of items recommended for
userğ‘¢. Finally, we combine the two promotion objectives, and then
theğ¿ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ is shown as follows:
Lğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ =Lğ·+ğ›¼Lğ‘…. (13)
whereğ›¼is the weight for adjusting the balance of Lğ·andLğ‘….
Based on the aforementioned description, we present the attack
algorithm of CLeaR in Appendix A.3.5 EXPERIMENTS
5.1 Experimental Settings
Datasets. Four public datasets with different scales: DouBan [ 62],
Epinions [ 25], ML-1M [ 7], and Yelp [ 53], are used in our experi-
ments. The dataset statistics are shown in Appendix A.4.1.
Evaluation Protocol. We split the datasets into three parts (train-
ing set, validation set, and test set) in a ratio of 7:1:2. The poisoning
data only affects the model training process; therefore, we exclu-
sively utilize the training set data as the known data. Two metrics
are used for measuring attack effictiveness, i.e., Hit Ratio@50 and
NDCG@50, which calculate the frequency and ranking of the target
items in recommendation lists. Each experiment in this section is
conducted 10 times, and then we report the average results.
Recommendation Methods. We choose LightGCN [ 10] as the
base model and four recent CL-based recommendation models as
the subjects of study:
â€¢SSL4Rec [ 46] introduces a contrastive objective to enhance item
representation by utilizing the inherent relationships among
item features.
â€¢SGL [ 43] adopts node/edge dropout to augment graph data and
conducts CL between representations learned over different
augmentations to enhance recommendation.
â€¢SimGCL [ 53] simplifies data augmentations in CL and optimizes
representations towards more uniformity.
â€¢XSimGCL [ 51] further streamlines the forward/backward pass
in SimGCL for more effective and efficient CL.
Attack Comparison Methods. To examine the effectiveness of the
proposed attack method, we compare it with some representative
poisoning attacks.
â€¢RandomAttack [ 16] randomly interacts with available items to
construct attacks with minimum expenses.
â€¢BandwagonAttack [ 18] chooses to interact with popular items
to expose target items to more users.
â€¢AUSH [ 19] leverages GANs [ 5] to generate malicious users based
on real user profiles.
â€¢DLAttack [ 12] formulates the attack as an optimization problem
in the deep learning-based recommendation scenario.
â€¢PoisonRec [ 31] leverages reinforcement learning to dynamically
generate interactive behavior for fake user profiles.
â€¢FedRecAttack [ 29] utilizes a bi-level optimization approach to
generate the interactions between malicious users and items.
â€¢A-hum [ 28] identifies hard users who consider the target items
as negative samples, and manipulates malicious interactions to
elevate these usersâ€™ preferences for the targeted items.
â€¢GSPAttack [ 23] targets graph neural network-based recommen-
dation by exploiting the vulnerabilities of graph propagation,
implementing attacks through the generation of fake users and
user-item interaction data.
It should be noted that FedRecAttack and A-hum are designed
under model poisoning setting which manipulates continuous gra-
dients in federated RS. To ensure fairness, we have adapted Fe-
dRecAttack and A-hum to data poisoning that manipulates discrete
interactions in Centralized RS.
3316Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 1: Performance comparison of different attack methods on the CL-based RS. The best results are in bold, and the runners-
up are with underlines. H and N refer to Hit Ratio and NDCG, respectively. The corresponding values for Yelp should be
multiplied by 10âˆ’3.
Dataset Mo
delRandomAttack BandwagonAttack AUSH DLAttack PoisonRe
c Fe
dRecAttack A_hum GSP
Attack CLeaR
H@50
N@50 H@50
N@50 H@50
N@50 H@50
N@50 H@50
N@50 H@50
N@50 H@50
N@50 H@50
N@50 H@50
N@50
DouBanLightGCN 0.0010
0.0037 0.0011
0.0100 0.0013 0.0106 0.0012 0.0100 0.0016
0.0083 0.0013
0.0033 0.0014
0.0026 0.0014
0.0025 0.0023 0.0039
SSL4Re
c 0.0019
0.0082 0.0034
0.0095 0.0058
0.0094 0.0138
0.0094 0.0146
0.0088 0.0176
0.0095 0.0177 0.0108 0.0163
0.0100 0.0181
0.0129
SGL 0.0038
0.0108 0.0020
0.0109 0.0018
0.0112 0.0017
0.0115 0.0044 0.0145 0.0051
0.0007 0.0069
0.0086 0.0047
0.0102 0.0101
0.0218
SimGCL 0.0058
0.0186 0.0119
0.0214 0.0067 0.0367 0.0055
0.0172 0.0109
0.0224 0.0095
0.0248 0.0089
0.0209 0.0108
0.0199 0.0159 0.0264
XSimGCL 0.0059
0.0127 0.0027
0.0147 0.0030
0.0121 0.0072
0.0124 0.0066
0.0105 0.0066
0.0148 0.0055 0.0154 0.0067
0.0135 0.0100
0.0158
EpinionsLightGCN 0.0042
0.0187 0.0023 0.0362 0.0011
0.0106 0.0091
0.0312 0.0033
0.0235 0.0074
0.0342 0.0087
0.0303 0.0093
0.0256 0.0095
0.0380
SSL4Re
c 0.0082
0.0269 0.0055
0.0198 0.0095
0.0289 0.0325
0.0389 0.0278
0.0306 0.0499
0.0569 0.0512 0.0582 0.0465
0.0542 0.0556
0.0658
SGL 0.0151
0.0560 0.0039
0.0220 0.0075
0.0393 0.0186
0.0478 0.0164
0.0356 0.0249
0.0642 0.0289 0.0753 0.0222
0.0579 0.0415
0.1223
SimGCL 0.0117
0.0560 0.0037
0.0194 0.0061
0.0328 0.0297
0.0219 0.0288
0.0465 0.0311 0.0716 0.0310
0.0710 0.0299
0.0345 0.0342
0.0857
XSimGCL 0.0161
0.0826 0.0048
0.0398 0.0089
0.0499 0.0159
0.0468 0.0244
0.0568 0.0361
0.0941 0.0368
0.0962 0.0458 0.1262 0.0470
0.1490
ML-1MLightGCN 0.0020
0.0091 0.0023
0.0101 0.0025 0.0102 0.0022
0.0098 0.0021
0.0088 0.0021
0.0056 0.0023
0.0068 0.0020
0.0063 0.0024 0.0111
SSL4Re
c 0.0038
0.0100 0.0039 0.0111 0.0036
0.0086 0.0029
0.0078 0.0033
0.0108 0.0030
0.0049 0.0041
0.0089 0.0043
0.0099 0.0046
0.0137
SGL 0.0021
0.0102 0.0029
0.0108 0.0033 0.0118 0.0030
0.0111 0.0036
0.0106 0.0041
0.0103 0.0031
0.0100 0.0035
0.0111 0.0052
0.0120
SimGCL 0.0040
0.0138 0.0072 0.0174 0.0039
0.0134 0.0048
0.0098 0.0048
0.0112 0.0036
0.0116 0.0033
0.0112 0.0055 0.0147 0.0084 0.0142
XSimGCL 0.0034 0.0132 0.0033
0.0110 0.0035
0.0130 0.0034
0.0111 0.0032
0.0109 0.0033
0.0112 0.0036
0.0117 0.0032
0.0101 0.0050
0.0138
Y
elpLightGCN 0.0188
0.0114 0.0205 0.0122 0.0
0.0 0.0158
0.0094 0.0132
0.0111 0.0210
0.0083 0.0125
0.0041 0.0112
0.0038 0.0262
0.0138
SSL4Re
c 0.1563
0.0153 0.1500
0.0540 0.2061
0.0190 0.1118
0.0018 0.1886
0.0645 0.0812
0.0276 0.1938 0.0716 0.1311
0.0447 0.2563
0.0932
SGL 0.0937
0.0318 0.1438
0.0494 0.1561 0.0525 0.0954
0.0348 0.1005
0.0426 0.0268
0.0123 0.1000
0.0337 0.1135
0.0467 0.2001
0.0674
SimGCL 0.0250
0.0122 0.0237
0.0141 0.0187
0.0163 0.0258 0.0187 0.0232
0.0156 0.0225 0.0209 0.0262
0.0180 0.0198
0.0164 0.0375 0.0119
XSimGCL 0.0197
0.0165 0.0270
0.0132 0.0125
0.0143 0.0226
0.0186 0.0202
0.0145 0.0287 0.0264 0.0165
0.0043 0.0305
0.0113 0.0365
0.0282
5.2 Attack Performance Comparison
We begin by comparing CLeaR with existing attack methods on
four distinct datasets. The results are presented in Table 1. Upon
examining Table 1, we can derive the following observations and
conclusions:
â€¢The proposed CLeaR effectively enhances the likelihood of tar-
get items being included in the recommendation list generated
by CL-based recommendation models. CLeaR demonstrates the
best performance among all these methods in most cases. These
performances can be attributed to CLeaRâ€™s consideration of rep-
resentation spectra. Throughout the attack process, CLeaR pro-
motes a smooth spectra distribution, while other attack methods
fail to take this factor into account in their approach.
â€¢Conventional attack methods, such as BandwagonAttack and
RandomAttack, typically fall short of the efficacy demonstrated
by learning-based attack approaches in most cases.
â€¢CLeaR proves to be effective not just within CL-based RS but
also in those non-CL counterparts. This strategic attack focus
extends CLeaRâ€™s applicability and increases its potential threat
across various recommendation methods.
5.3 Impact of Attack Size
We assess the performance of CLeaR in situations with a growing
number of malicious users. To maintain attack practicality, we limit
the attack size to specific percentages, specifically [1%, 2%, 3%, 4%,
5%]. Figure 5 displays the experimental results on two datasets,
and other datasets exhibit patterns analogous to those identified in
these two datasets. The following insights and conclusions can be
extrapolated:
â€¢CL-based recommendation models indicate variable degrees of
sensitivity to different attack sizes. Specifically, SSL4Rec exhibits
a pronounced increase in vulnerability, while models like SGL,
SimGCL, and XSimGCL show a moderate, upward trend in their
0.01 0.02 0.03 0.04 0.05
Attacksize0.0000.0100.0200.0300.0400.0500.060Hit Ratio@50
DouBan
0.01 0.02 0.03 0.04 0.05
Attacksize0.0000.0200.0400.0600.080
EpinionsLightGCN SSL4Rec SGL SimGCL XSimGCLFigure 5: Attack performance w.r.t. attack size.
hit ratios corresponding to incremental increases in attack size.
Parallel observations are evident in the Epinions dataset.
â€¢LightGCN demonstrates a relatively stable performance over the
range of attack sizes, exhibiting robustness that renders it less
susceptible to attacks compared to other CL-based recommenda-
tion methods. This observation aligns with the initial findings
presented earlier in the paper.
â€¢CLeaR consistently exhibits upward trends in performance as the
attack size increases across all cases. Itâ€™s crucial to highlight that
these upward trends have not reached a plateau, indicating that
CLeaRâ€™s full attack potential remains untapped. This highlights
the need to closely monitor CLeaR for potential threats.
5.4 The Effect of Two Promotion Objectives
In our experiments, we assess the effect of the dispersion promo-
tion objective and rank promotion objective on CLeaR. Table 2
illustrates the efficacy of two objectives as individual and com-
bined attack strategies. CLeaRğ·,CLeaRğ‘…, and CLeaRğ·+ğ‘…denote
the CLeaR attack method configured with solely the dispersion
promotion objective, exclusively the rank promotion objective, and
a combination of both objectives, respectively. From Table 2, we
can find the observations as follows:
3317KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zongwei Wang et al.
Table 2: The effect of two promotion objectives of CLeaR on
DouBan and Epinions (measured by Hit Ratio@50).
Dataset Cases SSL4Re
c SGL SimGCL XSimGCL
DouBanNone
Attack 0.0 0.0001 0.0002 0.0001
CLeaRğ· 0.0057 0.0047 0.0051 0.0035
CLeaRğ‘… 0.0136 0.0051 0.0076 0.0064
CLeaRğ·+ğ‘… 0.0181 0.0101 0.0099 0.0100
EpinionsNone
Attack 0.0 0.0001 0.0 0.0
CLeaRğ· 0.0158 0.0168 0.0094 0.0245
CLeaRğ‘… 0.0359 0.0219 0.0303 0.0301
CLeaRğ·+ğ‘… 0.0556 0.0415 0.0342 0.0470
â€¢Without any attack (NoneAttack), the Hit Ratio@50 is notably
low across all the models, indicating a minimal presence of target
items in the absence of manipulation.
â€¢The implementations of the dispersion promotion ( CLeaRğ·) and
rank promotion (CLeaR ğ‘…) as a standalone attack strategy result
in a discernible improvement in Hit Ratio@50 compared to the
NoneAttack scenario. This suggests that CLeaRğ·passively re-
duces the difficulty for an attacker to place target items within
the top recommendations successfully, and CLeaRğ‘…actively pro-
motes target items closer to the user in the recommendation list.
The combined use of dispersion promotion objective and rank
promotion objective ( CLeaRğ·+ğ‘…) demonstrates the most substan-
tial increase in Hit Ratio@50, highlighting a synergistic effect
where the conjunction of balance and rank promotion objectives
culminates in an optimized attack impact.
5.5 Attack Analysis on More Basic Encoders
From the data results and analysis in Table 1, we can conclude
that CLeaR not only works on CL-based recommendation methods.
Therefore, we design additional experiments to explore whether
CLeaR can have a stronger attack effect on a wider range of basic
recommendation encoders. Specifically, we select GMF [ 15] and
NGCF [ 37] as the basic recommendation encoders to be attacked.
The results are shown in Table 3, where CLeaR still shows the
strongest attack. This advantage is most obvious when attacking
NGCF on the Epinions data set. These experimental results show
that CLeaRâ€™s consideration of promoting smooth spectral distribu-
tion can indeed be widely used to attack various recommendation
systems. This has sounded the alarm for us, and we should pay at-
tention to this feature when designing recommendation algorithms.
Table 3: Comparison of different attack methods with basic
encoders (measured by Hit Ratio@50).
Dataset DouBan Epinions
Mo
dels GMF NGCF GMF NGCF
RandomAttack 0.0017 0.0019 0.0032 0.0037
AUSH 0.0019 0.0021 0.0040 0.0063
DLAttack 0.0021 0.0021 0.0039 0.0058
Fe
dRecAttack 0.0017 0.0021 0.0059 0.0068
A_hum 0.0016 0.0020 0.0060 0.0073
CLeaR 0.0023 0.0023 0.0066 0.01005.6 Attack Analysis on Black-Box Setting
In this experiment, we demonstrat the adaptability of our CLeaR
to black-box attacks by using surrogate models on the DouBan
dataset. Specifically, all victim models utilize LightGCN as the base
encoder, and we select NGCF and LightGCN as surrogate models,
respectively. To simulate a realistic attack scenario, we train both
the victim models and surrogate models independently, even when
they share the same model structure. As can be seen from table 4,
our attack method exceeds the performance of existing baseline
methods in most cases. This demonstrates that our approach adapts
effectively to black-box settings when using a general recommenda-
tion model as the surrogate model. Furthermore, our experiments
also led to two noteworthy findings:
â€¢Our method shows superior attack performance when the model
structure of the surrogate model is similar to that of the victim
model, compared to when it differs. This suggests that if we can
obtain prior knowledge about the model structure of the victim
model, it can be effectively incorporated into our surrogate model
training process to facilitate a more effective attack;
â€¢While possessing knowledge of the victim modelâ€™s structure aids
the attack process, it does not achieve the effectiveness level of a
white-box attack. This is due to the fact that, although the mod-
els have an identical structure, they are trained independently,
leading to variations in the trained model parameters.
Table 4: Comparison between the black-box baseline and
CLeaR variants (measured by Hit Ratio@50).
Mo
dels PoisonRe
c CLeaR CLeaR CLeaR
-LightGCN -NGCF -
White
LightGCN 0.0016 0.0021 0.0019 0.0023
SSL4Re
c 0.0146 0.0171 0.0166 0.0181
SGL 0.0044 0.0089 0.0084 0.0101
SimGCL 0.0109 0.0132 0.0128 0.0159
XSimGCL 0.0066 0.0082 0.0077 0.0100
5.7 Sensitivity of Hyperparameters
We examine the sensitivity of the CLeaR model to the hyperparam-
eterğ›¼on DouBan and Epinions datasets. We vary ğ›¼within a set [0,
0.01, 0.1, 0.5, 1, 5, 10]. The results are illustrated in Fig. 7, and we can
derive the following observations: CLeaR indicates consistent stabil-
ity in performance across all datasets as ğ›¼varies. LightGCN on the
DouBan dataset shows a consistent increase in the Hit Ratio@50 as
ğ›¼increases, reaching its peak at ğ›¼=0.5. SSL4Rec displays an initial
increase and then plateaus, maintaining performance across higher
ğ›¼values. Both SGL and SimGCL experience a modest rise before
reaching stability, while XSimGCL displays a gentle upward trend
but largely maintains a steady performance. A similar pattern is
observed from the results conducted on Epinions.
5.8 Analysis of Time Complexity
In this part, we report the real running time of compared meth-
ods for a one-time attack on the DouBan dataset. The results in
Table 5 are collected on an Intel(R) Core(TM) i9-10900X CPU and
a GeForce RTX 3090Ti GPU. Table 5 presents the running time in
seconds for a one-time attack on the DouBan dataset across various
3318Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
attack methods and recommender system models: LightGCN, SGL,
SimGCL, and XSimGCL. The observation can be found as follows:
â€¢Among all evaluated models, RandomAttack consistently exhibits
the lowest execution times, primarily due to its training-free and
heuristic-based methodology. CLeaR, on the other hand, presents
moderate execution times when compared to other attacks, but
is significantly more time-consuming than RandomAttack. This
indicates that CLeaR strikes a computational balance, effectively
navigating the trade-off between operational efficiency and algo-
rithmic complexity.
â€¢RandomAttack and AUSH maintain stable execution times across
models, reflecting their model-independent nature. In contrast,
other strategies exhibit variable time costs contingent on the rec-
ommendation model in use. This variability is attributed to the
necessity for these methods to train attack models tailored to spe-
cific recommendation model structures, in contrast to the model-
agnostic approach employed by RandomAttack and AUSH.
Table 5: Running time (s) for a one-time attack on DouBan.
Mo
dels LightGCN SSL4Re
c SGL SimGCL XSimGCL
RandomAttack 7.7167 7.5945 7.3935 7.4895 7.6484
AUSH 1340.6 1348.1 1379.3 1356.3 1378.1
DLAttack 1736.3 4537.2 7654.1 5671.0 4876.2
Fe
dRecAttack 777.36 899.82 1222.6 957.94 929.64
A_hum 768.28 884.62 1273.4 950.75 912.15
CLeaR 345.43 428.64 885.43 735.64 454.54
6 RELATED WORK
CL-Based Recommendation. CL-based recommendation meth-
ods are specifically designed to maximize the agreement between
positive pair representations while minimizing the similarity be-
tween negative pair representations. Through this process, the
recommendation model can learn the essential information from
the user-item interactions.
For instance, ğ‘†3-Rec [ 64], CL4SRec [ 45], SSL4Rec [ 46] and DuoRec
[26] introduce CL into sequence augmentations by randomly mask-
ing attributes and items, emphasizing the consistency between
different augmentations. During the same period, CL is also intro-
duced to various graph-based recommendation scenarios. HHGR
[59] proposes a double-scale augmentation approach for group
recommendation. CrossCBR [ 22] has explored the application of
CL in cross-domain recommendation. NCL [ 20] introduces a pro-
totypical contrastive objective that captures the correlations be-
tween a user/item and its context. SEPT [ 52] and COTREC [ 44] go
a step further and propose semi-supervised learning approaches
for social/session-based recommendation. SGL [ 43] generates two
additional views of graphs through node/edge dropout for CL. In
a similar way, SimGCL [ 53] directly creates an additional view at
the representation level. To streamline the propagation process,
XSimGCL [ 51] simplifies the forward/backward pass of SimGCL
for both recommendation and contrastive tasks.
Poisoning Attacks. In the past, traditional attacks on recom-
mender systems relied on heuristic manipulations of user behaviordata. RandomAttack [ 16] involved users randomly rating items,
while BandwagonAttack [ 18] focused on interacting with popular
items to boost the target itemâ€™s visibility. However, as recommender
systems have become more sophisticated and robust, these con-
ventional attack methods have become less effective. In response,
researchers have turned to machine learning techniques to develop
more advanced attacks [35, 56].
AUSH [ 19], GOAT [ 42], and TrialAttack [ 41] use generative ad-
versarial networks [ 5] to create fake user profiles based on existing
real user configurations. This kind of technique employs a dis-
criminator to differentiate between real and fake users, providing
guidance to the attacker. PoisonRec [ 31], LOKI [ 58], and KGAttack
[2] propose reinforcement learning-based attack strategies to gen-
erate interactive behavior sequences for fake user profiles. All the
aforementioned methods are typical black-box attacks, assuming
that the attacker has no knowledge of the internal structure or
parameter information of the recommendation model. However,
in certain attack scenarios, the attacker may possess complete or
partial information about the recommendation model. For instance,
DLAttack [ 12] assumes that the attacker know all the details of
recommendation models and explores poisoning attack in the deep
learning-based recommendation scenario. FedRecAttack [ 29] uti-
lizes a bi-level optimization approach to optimize interactions be-
tween malicious users and items. A-hum [ 28] identifies "hard users"
who consider the target items as negative samples. By manipulat-
ing these hard users to interact with target items, the attacker can
ensure the inclusion of the target items in their recommendation
lists.
7 CONCLUSION
In this study, we identify the susceptibility of CL-based recom-
mendation methods to poisoning attacks that specifically promote
target items. We also investigate the underlying reasons for this
sensitivity and demonstrate that the global dispersion of embedding
distributions induced by the CL loss is the core factor. Addition-
ally, we introduce a novel attack approach called CLeaR, which
manipulates the spectral value distribution of representations to be
smoother and simultaneously optimizes the visibility of target items
in usersâ€™ preference lists. The extensive experiments conducted on
four real-world datasets demonstrate the potential threats. Our
future research endeavors will concentrate on developing more nu-
anced defense approaches within CL-based recommender systems
to tackle the specific challenges associated with poisoning attacks.
ACKNOWLEDGMENTS
This research is funded by the National Natural Science Foundation
of China (Grant No. 62176028), the Australian Research Council
through the Future Fellowship (Grant No. FT210100624), Discov-
ery Project (Grants No. DP240101108), Industrial Transformation
Training Centre (Grant No. IC200100022), and the Science and
Technology Research Program of Chongqing Municipal Education
Commission (Grant No. KJZD-K202204402). Additionally, this work
is supported by the ARC Training Centre for Information Resilience
(CIRES).
3319KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zongwei Wang et al.
REFERENCES
[1]Himan Abdollahpouri. 2019. Popularity bias in ranking and recommendation. In
Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 529â€“530.
[2]Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing
Li, and Yihua Huang. 2022. Knowledge-enhanced black-box attacks for recom-
mendations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 108â€“117.
[3]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning.
[4]Seungtaek Choi, Myeongho Jeong, Hojae Han, and Seung-won Hwang. 2022.
C2l: Causally contrastive learning for robust text classification. In Proceedings of
the AAAI Conference on Artificial Intelligence.
[5]Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sen-
gupta, and Anil A Bharath. 2018. Generative adversarial networks: An overview.
IEEE signal processing magazine (2018).
[6]Lijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, and Chuang Gan. 2021. When
does contrastive learning preserve adversarial robustness from pretraining to
finetuning? Advances in neural information processing systems (2021).
[7]Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, and Jia Liu. 2018. Poisoning
Attacks to Graph-Based Recommender Systems. Annual Computer Security
Applications Conference.
[8]Aritra Ghosh and Andrew Lan. 2021. Contrastive learning improves model ro-
bustness under label noise. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2703â€“2708.
[9]Ihsan Gunes, Cihan Kaleli, Alper Bilge, and Huseyin Polat. 2014. Shilling attacks
against recommender systems: A comprehensive survey. Artificial Intelligence
Review (2014).
[10] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval. 639â€“648.
[11] Andreas Hoecker and Vakhtang Kartvelishvili. 1996. SVD approach to data unfold-
ing. Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
Spectrometers, Detectors and Associated Equipment 372, 3 (1996), 469â€“481.
[12] Hai Huang, Jiaming Mu, Neil Zhenqiang Gong, Qi Li, Bin Liu, and Mingwei Xu.
2021. Data poisoning attacks to deep learning based recommender systems. arXiv
(2021).
[13] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Baner-
jee, and Fillia Makedon. 2020. A survey on contrastive self-supervised learning.
Technologies (2020).
[14] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive
learning. Advances in neural information processing systems (2020).
[15] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer (2009).
[16] Shyong K Lam and John Riedl. 2004. Shilling recommender systems for fun
and profit. In Proceedings of the 13th international conference on World Wide Web.
393â€“402.
[17] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data poi-
soning attacks on factorization-based collaborative filtering. Advances in neural
information processing systems 29 (2016).
[18] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data poi-
soning attacks on factorization-based collaborative filtering. Advances in neural
information processing systems (2016).
[19] Chen Lin, Si Chen, Hui Li, Yanghua Xiao, Lianyun Li, and Qian Yang. 2020.
Attacking recommender systems with augmented user profiles. In Proceedings of
the 29th ACM international conference on information & knowledge management.
[20] Zihan Lin, Changxin Tian, Yupeng Hou, and Wayne Xin Zhao. 2022. Improving
graph collaborative filtering with neighborhood-enriched contrastive learning.
InProceedings of the ACM Web Conference 2022.
[21] Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. 2022. Revisiting graph
contrastive learning from the perspective of graph spectrum. Advances in Neural
Information Processing Systems 35 (2022), 2972â€“2983.
[22] Yunshan Ma, Yingzhi He, An Zhang, Xiang Wang, and Tat-Seng Chua. 2022.
CrossCBR: cross-view contrastive learning for bundle recommendation. In Pro-
ceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining.
[23] Toan Nguyen Thanh, Nguyen Duc Khang Quach, Thanh Tam Nguyen,
Thanh Trung Huynh, Viet Hung Vu, Phi Le Nguyen, Jun Jo, and Quoc Viet Hung
Nguyen. 2023. Poisoning GNN-based recommender systems with generative
surrogate-based attacks. ACM Transactions on Information Systems (2023).
[24] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[25] Yiteng Pan, Fazhi He, and Haiping Yu. 2020. Learning social representations
with deep autoencoder for recommender system. World Wide Web 23 (2020),
2259â€“2279.[26] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive
learning for representation degeneration problem in sequential recommendation.
InProceedings of the fifteenth ACM international conference on web search and
data mining.
[27] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint
arXiv:1205.2618 (2012).
[28] Dazhong Rong, Qinming He, and Jianhai Chen. 2022. Poisoning Deep Learning
based Recommender Model in Federated Learning Scenarios. arXiv preprint
arXiv:2204.13594 (2022).
[29] Dazhong Rong, Shuai Ye, Ruoyan Zhao, Hon Ning Yuen, Jianhai Chen, and
Qinming He. 2022. Fedrecattack: Model poisoning attack to federated recom-
mendation. In International Conference on Data Engineering.
[30] Jie Shuai, Kun Zhang, Le Wu, Peijie Sun, Richang Hong, Meng Wang, and Yong Li.
2022. A review-aware graph contrastive learning framework for recommendation.
InACM SIGIR Conference on Research and Development in Information Retrieval.
[31] Junshuai Song, Zhao Li, Zehong Hu, Yucheng Wu, Zhenpeng Li, Jian Li, and
Jun Gao. 2020. Poisonrec: an adaptive data poisoning framework for attacking
black-box recommender systems. In 2020 IEEE 36th International Conference on
Data Engineering.
[32] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[33] Chenyang Wang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu,
and Shaoping Ma. 2022. Towards Representation Alignment and Uniformity in
Collaborative Filtering. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 1816â€“1825.
[34] Feng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastive
loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition.
[35] Qinyong Wang, Hongzhi Yin, Tong Chen, Junliang Yu, Alexander Zhou, and
Xiangliang Zhang. 2021. Fast-adapting and privacy-preserving federated recom-
mender system. The VLDB Journal (2021).
[36] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation
learning through alignment and uniformity on the hypersphere. In International
Conference on Machine Learning. PMLR, 9929â€“9939.
[37] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In Proceedings of the 42nd international ACM
SIGIR conference on Research and development in Information Retrieval.
[38] Zongwei Wang, Min Gao, Jundong Li, Junwei Zhang, and Jiang Zhong. 2022.
Gray-Box Shilling Attack: An Adversarial Learning Approach. ACM Transactions
on Intelligent Systems and Technology (2022).
[39] Zongwei Wang, Min Gao, Wentao Li, Junliang Yu, Linxin Guo, and Hongzhi Yin.
2023. Efficient bi-level optimization for recommendation denoising. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
2502â€“2511.
[40] Zekai Wang and Weiwei Liu. 2022. Robustness verification for contrastive learn-
ing. In International Conference on Machine Learning. 22865â€“22883.
[41] Chenwang Wu, Defu Lian, Yong Ge, Zhihao Zhu, and Enhong Chen. 2021. Triple
adversarial learning for influence based poisoning attack in recommender sys-
tems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining. 1830â€“1840.
[42] Fan Wu, Min Gao, Junliang Yu, Zongwei Wang, Kecheng Liu, and Xu Wang. 2021.
Ready for emerging threats to recommender systems? A graph convolution-based
generative shilling attack. Information Sciences 578 (2021), 683â€“701.
[43] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and
Xing Xie. 2021. Self-supervised graph learning for recommendation. In Proceed-
ings of the 44th international ACM SIGIR conference on research and development
in information retrieval. 726â€“735.
[44] Xin Xia, Hongzhi Yin, Junliang Yu, Yingxia Shao, and Lizhen Cui. 2021. Self-
supervised graph co-training for session-based recommendation. In Proceedings
of the 30th ACM international conference on information & knowledge management .
2180â€“2190.
[45] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin
Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In
IEEE international conference on data engineering.
[46] Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya
Menon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al .2021. Self-supervised
learning for large-scale item recommendations. In Proceedings of the 30th ACM
International Conference on Information & Knowledge Management. 4321â€“4330.
[47] Haibo Ye, Xinjie Li, Yuan Yao, and Hanghang Tong. 2023. Towards robust neural
graph collaborative filtering via structure denoising and embedding perturbation.
ACM Transactions on Information Systems (2023).
[48] Jingwei Yi, Fangzhao Wu, Bin Zhu, Yang Yu, Chao Zhang, Guangzhong Sun, and
Xing Xie. 2022. UA-FedRec: Untargeted Attack on Federated News Recommen-
dation. arXiv preprint arXiv:2202.06701 (2022).
[49] Hongzhi Yin, Liang Qu, Tong Chen, Wei Yuan, Ruiqi Zheng, Jing Long, Xin
Xia, Yuhui Shi, and Chengqi Zhang. 2024. On-Device Recommender Systems: A
Comprehensive Survey. arXiv preprint arXiv:2401.11441 (2024).
3320Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
[50] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
neural information processing systems (2020).
[51] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and
Hongzhi Yin. 2023. XSimGCL: Towards extremely simple graph contrastive
learning for recommendation. IEEE Transactions on Knowledge and Data Engi-
neering (2023).
[52] Junliang Yu, Hongzhi Yin, Min Gao, Xin Xia, Xiangliang Zhang, and Nguyen Quoc
Viet Hung. 2021. Socially-aware self-supervised tri-training for recommendation.
InProceedings of the 27th ACM SIGKDD conference on knowledge discovery & data
mining.
[53] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung
Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive
learning for recommendation. In Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 1294â€“1303.
[54] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2023.
Self-supervised learning for recommender systems: A survey. IEEE Transactions
on Knowledge and Data Engineering (2023).
[55] Tan Yu, Yunfeng Cai, and Ping Li. 2020. Toward faster and simpler matrix
normalization via rank-1 update. In Computer Visionâ€“ECCV 2020: 16th European
Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XIX 16. Springer,
203â€“219.
[56] Wei Yuan, Quoc Viet Hung Nguyen, Tieke He, Liang Chen, and Hongzhi Yin.
2023. Manipulating Federated Recommender Systems: Poisoning with Synthetic
Users and Its Countermeasures. arXiv preprint arXiv:2304.03054 (2023).
[57] Wei Yuan, Shilong Yuan, Chaoqun Yang, Nguyen Quoc Viet hung, and Hongzhi
Yin. 2023. Manipulating Visually Aware Federated Recommender Systems and
Its Countermeasures. ACM Transactions on Information Systems (2023).
[58] Hengtong Zhang, Yaliang Li, Bolin Ding, and Jing Gao. 2020. Practical data
poisoning attack against next-item recommendation. In Proceedings of The Web
Conference 2020. 2458â€“2464.
[59] Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021.
Double-scale self-supervised hypergraph learning for group recommendation. In
Proceedings of the 30th ACM international conference on information & knowledge
management. 2557â€“2567.
[60] Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen,
and Lizhen Cui. 2022. Pipattack: Poisoning federated recommender systems for
manipulating item promotion. In Proceedings of the Fifteenth ACM International
Conference on Web Search and Data Mining. 1415â€“1423.
[61] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. 2023. Spectral
feature augmentation for graph contrastive learning and beyond. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 37. 11289â€“11297.
[62] Guoshuai Zhao, Xueming Qian, and Xing Xie. 2016. User-service rating prediction
by exploring social usersâ€™ rating behaviors. IEEE Transactions on Multimedia
(2016).
[63] Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, and Hongzhi Yin. 2024.
Poisoning Decentralized Collaborative Recommender System and Its Counter-
measures. Proceedings of the 47rd International ACM SIGIR conference on research
and development in Information Retrieval (2024).
[64] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,
Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for
sequential recommendation with mutual information maximization. In ACM
international conference on information & knowledge management.
[65] Daniel ZÃ¼gner, Amir Akbarnejad, and Stephan GÃ¼nnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining. 2847â€“2856.
A APPENDIX
A.1 Visualization Experiment of
Representation Distributions Under Attacks
We conducted experiments for visualizing the representation dis-
tributions learned by recommendation models with and without
CL when subjected to a poisoning attack. The experimental setup
employed is as follows: We use RandomAttack as the chosen attack
approach, and for each malicious user, the number of interactions
is set to match the average number of user interactions. To ensure
clear and concise experimental results, we employed a systematic
sampling strategy for each dataset. Initially, items were ranked
based on their popularity, and we sampled 500 popular items from
the top 20% group, along with 500 cold items and 5 target itemsfrom the remaining items. This sampling approach was chosen to
accommodate the presence of a long-tail distribution (also named
power-law distribution) in item popularity. Additionally, we ran-
domly selected 500 users for the experiment. Then, we used t-SNE
[32] to map the optimal representations to a two-dimensional space.
Our experimental results were executed on a selection of preva-
lent datasets, including Epinions [ 25] and DouBan [ 62]. Figure 2 and
Figure 6 demonstrate two patterns: local clustering in the absence
of CL and global dispersion with CL. Local clustering aggregates
popular items and users, isolating cold items. In contrast, global
dispersion achieves a balanced spread across the vector space, re-
ducing the distance between popular and cold items.
A.2 Proof of Proposition 1
Proposition 1 .Give the representations Zâ€²andZâ€²â€²which are learned
on augmented views and the corresponding singular values Î£â€²=
ğ‘‘ğ‘–ğ‘ğ‘”(ğœâ€²
1,...,ğœâ€²
ğ‘‘)andÎ£â€²â€²=ğ‘‘ğ‘–ğ‘ğ‘”(ğœâ€²â€²
1,...,ğœâ€²â€²
ğ‘‘),an upper bound of the
CL loss is given by:
Lğ‘ğ‘™<ğ‘max
ğ‘—ğœâ€²
ğ‘—ğœâ€²â€²
ğ‘—âˆ’âˆ‘ï¸
ğ‘–ğœâ€²
ğ‘–ğœâ€²â€²
ğ‘–+ğ‘logğ‘.(14)
Proof. For simplification, we first set ğœ=1in Equation (5), and we
can get that:
Lğ‘ğ‘™=âˆ’âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘+âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)logâˆ‘ï¸
ğ‘›âˆˆ(UâˆªI)exp(zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘›)
<âˆ’âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘+âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)log(ğ‘ max
ğ‘›âˆˆ(UâˆªI)exp(zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘›))
<âˆ’âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘+âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)(logğ‘+max
ğ‘›âˆˆ(UâˆªI)zâ€²ğ‘‡
ğ‘›zâ€²â€²
ğ‘›).
(15)
In the provided equation, the transformation from the first to
the second row applies the log-sum-exp trick, consolidating all
zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘›terms to their maximum within the set, thus simplifying the
computation. Proceeding to the third row, drawing inspiration from
the findings of [ 33,36], we suppose a near alignment of the two
augmented representations. This implies that the maximum simi-
larity between any negative pair zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘›is less than the maximum
similarity within the augmented representations themselves zâ€²ğ‘‡
ğ‘›zâ€²â€²
ğ‘›.
Consequently, the optimization of the CL loss can be reformulated
as follows:
Lğ‘ğ‘™<âˆ’âˆ‘ï¸
ğ‘âˆˆ(UâˆªI)zâ€²ğ‘‡
ğ‘zâ€²â€²
ğ‘+ğ‘ max
ğ‘›âˆˆ(UâˆªI)(zâ€²ğ‘‡
ğ‘›zâ€²â€²
ğ‘›)+ğ‘logğ‘
=âˆ’ğ‘‡ğ‘Ÿ(Zâ€²ğ‘‡Zâ€²â€²)+ğ‘ max
ğ‘›âˆˆ(UâˆªI)(zâ€²ğ‘‡
ğ‘›zâ€²â€²
ğ‘›)+ğ‘logğ‘,(16)
whereğ‘‡ğ‘Ÿ(Â·)means the trace of matrix.
Considering Zâ€²andZâ€²â€²as augmented representations derived
from the original representations, we hypothesize a minimal vari-
ance in their respective singular vectors. Therefore, we constrain
them to be identical. Following this constraint, we get that Zâ€²=
LÎ£â€²Rğ‘‡andZâ€²â€²=LÎ£â€²â€²Rğ‘‡, leading to:
Zâ€²ğ‘‡Zâ€²â€²=RÎ£â€²ğ‘‡Lğ‘‡LÎ£â€²â€²Rğ‘‡
=ğœâ€²
1ğœâ€²â€²
1r1r1ğ‘‡+ğœâ€²
2ğœâ€²â€²
2r2r2ğ‘‡+Â·Â·Â·+ğœâ€²
ğ‘‘ğœâ€²â€²
ğ‘‘rğ‘‘rğ‘‘ğ‘‡.(17)
3321KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zongwei Wang et al.
Figure 6: Representation distribution on DouBan dataset under poisoning attacks (RandomAttack).
Then, we plug this equation into Equation (16), and due to rğ‘‡
ğ‘—rğ‘—=
1, we can get that:
Lğ‘ğ‘™<âˆ’ğ‘‡ğ‘Ÿ(Zâ€²ğ‘‡Zâ€²â€²)+ğ‘ max
ğ‘›âˆˆ(UâˆªI)(zâ€²ğ‘‡
ğ‘›zâ€²â€²
ğ‘›)+ğ‘logğ‘
=âˆ’âˆ‘ï¸
ğ‘–ğœâ€²
ğ‘–ğœâ€²â€²
ğ‘–+ğ‘max
ğ‘—ğœâ€²
ğ‘—ğœâ€²â€²
ğ‘—rğ‘‡
ğ‘—rğ‘—+ğ‘logğ‘
=ğ‘max
ğ‘—ğœâ€²
ğ‘—ğœâ€²â€²
ğ‘—âˆ’âˆ‘ï¸
ğ‘–ğœâ€²
ğ‘–ğœâ€²â€²
ğ‘–+ğ‘logğ‘.(18)
A.3 Algorithm
We present the attack algorithm of CLeaR in Algorithm 1.
Algorithm 1: The White-Box Algorithm of CLeaR.
Input: The recommendation model ğ‘“, malicious users set
Uğ‘€, target items setIğ‘‡, interactions dataD.
Output: Malicious DataDğ‘€.
while not converged do
// inner optimization:
// recommendation phase
1 sample tuples(ğ‘¢,ğ‘–,ğ‘—)from the synthetic dataset Dand
Dğ‘€;
2 calculateğ¿ğ‘Ÿğ‘’ğ‘to obtain optimal user representation Zâˆ—
U
and item representation Zâˆ—
I;
// outer optimization:
// attack phase
3 Dispersion Promotion Objective: traverse all
representations Zâˆ—
UâˆªZâˆ—
I, calculate and accumulate
every||ZVâ€²Vâ€²ğ‘‡||
||Vâ€²||2
2, i.e., calculateLğ·;
4 Rank Promotion Objective: traverse all users ğ‘¢âˆˆU,
and find the last item in the recommendation list of
userğ‘¢to calculateLğ‘…;
5 updateDğ‘€, i.e.,Dğ‘€=arg max(Lğ·+ğ›¼Lğ‘…);
Details of Approximate Solution. The CLeaR utilizes a bi-level
optimization framework. In outer optimization, we face the chal-
lenge of discrete domains and constraints, as interactions can only
take values of 0 or 1. To address this, we adopt a greedy approxima-
tion scheme [ 65]. We treat all interactions as continuous values for
optimization and then select the interactions that yield the highestvalues among them. The number of choices is limited by the maxi-
mum number of interactions of malicious users minus the number
of target items. The remaining interactions of each malicious user
are fixed to interact with the target items.
A.4 Experimental Supplements
Table 6: Statistics of datasets.
Dataset #Users #Items #Interactions #Density
DouBan 2,831 36,821 805,611 0.772%
Epinions 75,887 75,881 508,837 0.009%
ML-1M 6,038 3,492 575,281 2.728%
Yelp 31,668 38,048 1,561,406 0.129%
A.4.1 Statistics of Dataset.
A.4.2 Parameter Settings. As for the specific hyperparameters
of the baseline models, we initialize them with the recommended
settings in the original papers and then perform a grid search
around them for optimal performance. For all attack models, the
injected malicious user population is 1% of the number of normal
users, and each malicious user is assigned an interaction count
equivalent to the average number of interactions per normal user.
As for the general settings, we set ğœ”, batch size, learning rate, em-
bedding size, and the number of LightGCN layers to 0.1, 1024, 0.001,
64, and 2, respectively. Following previous work [ 16,18], we ran-
domly choose 10 cold items from the bottom 80% group as target
items.
0 0.01 0.1 0.5 1 5 10
0.0000.0030.0050.0070.0100.0120.0150.018Hit Ratio@50
DouBan
0 0.01 0.1 0.5 1 5 10
0.0100.0200.0300.0400.050
EpinionsLightGCN SSL4Rec SGL SimGCL XSimGCL
Figure 7: Parameter sensitivity with regard to ğ›¼on DouBan
and Epinions.
3322