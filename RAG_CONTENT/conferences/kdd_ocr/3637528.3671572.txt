Controllable Multi-Behavior Recommendation for In-Game Skins
with Large Sequential Model
Yanjie Gouâˆ—
Common Data Platform, Tencent
Shenzhen, China
yanjiegou@tencent.comYuanzhou Yaoâˆ—
Institute of Computing Technology,
Chinese Academy of Sciences
University of Chinese Academy of
Sciences
Beijing, China
yaoyuanzhou21s@ict.ac.cnZhao Zhangâ€ 
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
zhangzhao2021@ict.ac.cn
Yiqing Wu
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
wuyiqing20s@ict.ac.cnYi Huâ€ 
Common Data Platform, Tencent
Shenzhen, China
sunracerhu@tencent.comFuzhen Zhuang
Institute of Artificial Intelligence,
Beihang University
Zhongguancun Laboratory
Beijing, China
zhuangfuzhen@buaa.edu.cn
Jiangming Liu
School of Information Science and
Engineering, Yunnan University
Kunming, China
jiangmingliu@ynu.edu.cnYongjun Xu
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
xyj@ict.ac.cn
ABSTRACT
Online games often house virtual shops where players can acquire
character skins. Our task is centered on tailoring skin recommenda-
tions across diverse scenarios by analyzing historical interactions
such as clicks, usage, and purchases. Traditional multi-behavior
recommendation models employed for this task are limited. They ei-
ther only predict skins based on a single type of behavior or merely
recommend skins for target behavior type/task. These models lack
the ability to control predictions of skins that are associated with
different scenarios and behaviors. To overcome these limitations,
we utilize the pretraining capabilities of Large Sequential Models
(LSMs) coupled with a novel stimulus prompt mechanism and build
a controllable multi-behavior recommendation (CMBR) model. In
our approach, the pretraining ability is used to encapsulate usersâ€™
multi-behavioral sequences into the representation of usersâ€™ general
interests. Subsequently, our designed stimulus prompt mechanism
stimulates the model to extract scenario-related interests, thus gen-
erating potential skin purchases (or clicks and other interactions)
for users. To the best of our knowledge, this is the first work to
provide controlled multi-behavior recommendations, and also the
first to apply the pretraining capabilities of LSMs in game domain.
âˆ—Yanjie Gou and Yuanzhou Yao are co-first authors with equal contributions.
â€ Zhao Zhang and Yi Hu are the corresponding authors.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671572Through offline experiments and online A/B tests, we validate our
method significantly outperforms baseline models, exhibiting about
a tenfold improvement on various metrics during the offline test.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems.
KEYWORDS
Recommendation Systems, Large Sequential Model, Prompt Mech-
anism
ACM Reference Format:
Yanjie Gou, Yuanzhou Yao, Zhao Zhang, Yiqing Wu, Yi Hu, Fuzhen Zhuang,
Jiangming Liu, and Yongjun Xu. 2024. Controllable Multi-Behavior Recom-
mendation for In-Game Skins with Large Sequential Model. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671572
1 INTRODUCTION
In online multiplayer competitive games, such as â€˜King of Gloryâ€™1
and â€˜League of Legendsâ€™2, in-game skins offer users a unique visual
experience, serving as a significant revenue stream and playing a
pivotal role in enhancing user engagement. As shown in Figure 1,
skins in the game are cosmetic options that players can select for
their hero characters. While skins alter the appearance of the char-
acter, including clothing and accessories, they do not change the
fundamental abilities and attributes of the character. This offers a
1https://en.wikipedia.org/wiki/Honor_of_Kings
2https://wildrift.leagueoflegends.com/en-gb/
4986
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yanjie Gou et al.
way for players to personalize their gaming experience while main-
taining game balance. In-game skin sequential recommendation is
an essential module in our platform, which aims to recommend ap-
propriate skins to users based on user historical behavior sequence.
Although existing recommendation models have achieved great
success in various real-world platforms [ 37], [12,17]. In-game skin
recommendation is not a trivial task. Different from the traditional
recommendation that only involves a single user behavior type
and a single scenario, our in-game skin recommendation involves
multi-behavior and multi-scenario/task.
In our platform, users produce various interactive behaviors with
these skins within the game, such as usage, clicks, and purchases,
collectively forming their multi-behavioral historical sequences.
Each item in these sequences distinctly identifies the skin and the
associated interaction type. Besides, there are multi-scenario in
in-game skin recommendation, different scenarios involve different
tasks. For instance, in the scenario of the in-game store, we aim
to predict which skins a user is likely to purchase. Conversely, in
the scenario of distributing skin trial cards, the objective shifts
to predict the skins a user might use. The complex dependencies
across different types of behavior and different demands across
different scenario brings challenges to the traditional recommen-
dation. Unfortunately, there remains a lack of effort to solve this
issue. Therefore, in this study, we aim to solve this problem by
acontrollable skin recommendation system , which are based
on usersâ€™ multi-behavioral sequences, tailored specifically to the
nuances of distinct in-game scenarios.
Recent studies on artificial intelligence and knowledge graph-
related research have provided valuable insights and advancements
[21,30,35]. Actually, some efforts have been explored for multi-
behavior recommendation. Traditional single-behavior recommen-
dation models predominantly focus on resolving the singular as-
sociation between users and items [ 14,18], often overlooking the
distinctions among various interaction behaviors, which may re-
sult in the loss of fine-grained information and the nuances of
diverse interactions. To address these challenges, researchers have
ventured into multi-behavior recommendation models. For exam-
ple, the MB-GMN model [ 26] employs meta-learning to extract
personalized information from users, injecting it into a graph trans-
fer learning framework to comprehend usersâ€™ complex interests
and provide more accurate recommendations. Additionally, var-
ious multi-behavior recommendation models such as NMTR [ 7],
MBGCN [ 3], and MBSSL [ 27] have demonstrated superior model
performance against various baseline methods, validating the effec-
tiveness of multi-behavior related modeling.
Despite the success of existing multi-behavior studies. There still
are two limitations:
(1)Overlooking complex temporal relationships between
multiple behaviors. The temporal information contained in the
userâ€™s behavior sequence is crucial for recommender systems. Sig-
nificant efforts have been conducted to model this temporal in-
formation [ 12,17,37]. While existing multi-behavior studies usu-
ally are graph-based without considering temporal information
[2,3,29]. Furthermore, a minority of sequential-based methods just
split multi-behavior into several sub-sequences and model them
separately [ 22]. However, userâ€™s multi-behaviors are intertwined
over time. For example, a user may free trial a skin at time 1, thenfree trial another skin at time 2. By comparing the two skins, they
eventually purchased Skin 3 at time 3. Separately modeling meth-
ods fail to capture such complex dependency across different types
of behaviors.
Figure 1: Examples of game character skins. The figure dis-
plays different skins for two heroes: Sun Ce and Mai Shiranui.
Each hero has multiple skins, which retain the characterâ€™s
abilities and attributes, but differ in appearance, including
attire and dialogue.
(2)Lack of scenario transferability. Existing models typically
constrain predictions solely to the interactions under a specific
task/scenario. This fixed predictive framework limits the modelsâ€™
adaptability and flexibility across different scenarios, preventing
them from adjusting the recommendation strategy according to
the specific needs of diverse scenarios. For example, in the in-game
Store scenario, the target behavior of the recommendation system
is purchasing, while in the skin trial card distribution scenario, the
target behavior is usage. This lack of scenario transferability ren-
ders traditional multi-behavior recommendation models ineffective
in providing efficient scenario-specific recommendations across
different gaming scenarios.
To address the limitations of traditional multi-behavior recom-
mendation models in the context of diverse gaming scenarios,
we propose a novel controllable multi-behavior recommendation
(CMBR) model, which is uniquely designed to adapt its recommen-
dation strategies based on specific scenarios. By leveraging the
pretraining capabilities of Large Sequential Models (using LLM
architectures like GPT to treat user sequences as natural language)
and the flexibility of the prompt mechanism in the NLP commu-
nity, CMBR can deeply understand complex temporal relationships
between multiple behaviors and simultaneously acquire the ca-
pability of scenario transferring. Specifically, by harnessing the
pretraining capabilities of LSMs, i.e., self-supervised autoregressive
modeling, along with a carefully designed embedding layer, CMBR
can effectively grasp the intricate interdependence among various
behavior types across time, which contributes to a profound un-
derstanding of usersâ€™ preferences. Furthermore, we also introduce
a unique mechanism called â€˜Behavioral Stimulus Promptâ€™. These
prompts serve as advanced scene embeddings that encapsulate core
motivations and key actions within each specific scenario.
For varying needs across different scenarios, we employ differ-
ent Behavioral Stimulus Prompts. Those prompts guide the model
4987Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
generating diverse recommendation results, serving as â€˜catalystsâ€™,
which allow the model to â€˜activateâ€™ specific sub-spaces within the
multi-dimensional user profile that the CMBR constructs. They
dynamically steer the CMBR to focus on aspects that are partic-
ularly relevant to distinct scenarios, thereby making our CMBR
controllable.
Above all, compared with traditional multi-behavior recommen-
dation models, our model has the following advances: (1) Our CMBR
comprehensively models the temporal relations in multi-behavior
sequence, where the userâ€™s multi-behaviors are interlaced over the
time. (2) Our CMBR is under control, which means we can control
the model to adopt appropriate recommendation strategies based
on specific scenarios. Compared with previous works that focus on
a single scenario or task, our CMBR can achieve a unified model
for all scenarios.
To the best of our knowledge, this work represents the first
application of pretraining capabilities of LSMs to multi-behavior
recommendation tasks and is pioneering in offering a controllable
multi-behavior recommendation model. Specifically, our approach
offers the following contributions:
â€¢We are the first to introduce a controllable multi-behavior
recommendation model that effectively addresses recom-
mendations across multiple scenarios.
â€¢We innovatively incorporate the capability of pretraining of
LSMs into multi-behavior recommendation tasks, enhanced
by our newly proposed â€˜Behavioral Stimulus Promptâ€™ mech-
anism for scenario-specific control.
â€¢Through both offline experiments and online A/B tests, our
approach has demonstrated remarkable effectiveness, where
the proposed model significantly outperformed baseline mod-
els, showing about a tenfold improvement in various metrics
during offline tests.
2 RELATED WORK
2.1 Multi-behavior Recommendation
In the realm of multi-behavior analysis, initial approaches enhanced
matrix factorization techniques, like CMF [ 36], and utilized spe-
cialized sampling methods, as seen in MC-BPR [ 8], to incorporate
various behavioral data. Recent advancements focus on deeper con-
nections between users and items, employing neural networks and
graph neural networks (GNNs) to model complex relationships.
Techniques such as MATN [ 25], MGNN [ 34], MBGCN [ 3], GHCF
[1], and MBGMN [ 26] leverage transformers, GNNs, and contrastive
learning to understand multi-behavior dynamics.
Additionally, Multi-Task Learning (MTL) approaches, like NMTR
[7] and EHCF [ 2], use diverse structures to model interactive infor-
mation, categorizing MTL models into two types: one extracting
expert information from coupled representations and another using
transfer-based paradigms for task associations.
Despite these advancements, existing models, including MGNN
[34] and GHCF [ 1], show limitations in online multiplayer com-
petitive games, lacking scenario-specific adaptability. To address
this, we propose a novel approach integrating pretraining with a
â€˜Behavioral Stimulus Promptâ€™ mechanism for a controllable multi-
behavior recommendation model, aiming for more flexible and
scenario-aware recommendations.2.2 Large Language/Sequential Models for
Recommendation
Large Language/Sequential Models(LLMs) significantly enhance
recommender systems by providing sophisticated interpretations
of user preferences and generating more refined recommendations,
surpassing traditional collaborative and content-based filtering
methods which struggle with issues like cold start and data sparsity
[5]. The recent shift towards utilizing LLMs for direct recommen-
dation generation represents a departure from their traditional role
as mere feature extractors [ 12,17], moving towards a generative
approach that simplifies the recommendation process by eliminat-
ing the need for multi-stage filtering [ 15,16]. This shift is part of
a broader movement from discriminative to generative AI models,
characterized by the ability of LLMs to autonomously generate rel-
evant recommendations without the need for explicit item ranking,
thereby redefining the concept of IDs in recommender systems
to align with the LLMsâ€™ natural language processing capabilities
[9,28,33]. Inspired by the success of prompt technology in NLP,
PPR, and SFR introduce personalized prompt-tuning for fairness
and cold-start problems[23, 24].
However, traditional generative models often lack control over
the content they generate, which can be problematic in scenarios
that require precise recommendations, such as suggesting in-game
skins based on usersâ€™ purchasing behaviors rather than general
in-game activities. To address this, we propose a novel approach
that leverages the pretraining of LLMs with a â€˜Behavioral Stimulus
Promptâ€™ mechanism to guide the generation of recommendations
towards more relevant and context-specific outcomes, particularly
suited for complex scenarios like online multiplayer games. This
method ensures that recommendations are closely aligned with
usersâ€™ likely interests and purchase intentions, enhancing the effec-
tiveness and precision of the recommender system.
3 PROBLEM FORMULATION
In our scenario, a myriad of skins are available for players to interact
with. Users engage with these skins in various manners, including
using, clicking, and purchasing, forming a historical sequence of
interactions. We aim to predict the skins a user is likely to interact
with in different scenarios based on their historical sequence. Each
scenario corresponds to a specific type of interaction. For instance,
in the in-game Store scenario, the target behavior is purchasing,
and the task is to predict which skin the user is likely to purchase
next. Similarly, in the skin trial card distribution scenario, the target
behavior is usage, and the objective is to predict which skin the
user is likely to use next.
Denote the user set as U=
ğ‘¢1,ğ‘¢2,...,ğ‘¢|U|	, where|U|is the
total number of users, and the skin set as S=
ğ‘ 1,ğ‘ 2,...,ğ‘ |S|	,
where|S|is the total number of skins. The historical interaction
sequence of user ğ‘¢ğ‘–is represented as hğ‘¢ğ‘–, where each interaction is
denoted by an item combining the skin ID and the interaction type.
Formally, the interaction sequence can be represented as follows:
[uğ‘–]
|{z}[2003211_use]
|           {z           }[2001452_buy]
|            {z            }[2004562_click]
|             {z             }Â·Â·Â· [ 2004588_use]
|           {z           }
User Item 1 Item 2 Item 3Â·Â·Â· Itemğ¿
|                                                                                                         {z                                                                                                         }
Interaction Seq
4988KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yanjie Gou et al.
Each item within the interaction sequence is composed of a skin ID
and an interaction type, concatenated together with an underscore.
For instance, the item â€˜2003211_useâ€™ is composed of the skin ID
â€˜2003211â€™ and the interaction type â€˜useâ€™.
4 METHODOLOGY
4.1 Stimulus Prompt Mechanism
In this paper, we focus on sequence recommendation based on usersâ€™
multi-behavior sequence. In our CMBR, usersâ€™ behavior sequence
is inputted into the sequence generation model, then the sequence
generation model generates the recommended items. This process
can be formulated as:
Ë†ğ‘ ğ‘›ğ‘’ğ‘¥ğ‘¡=ğ¶ğ‘€ğµğ‘…(â„ğ‘¢), (1)
where Ë†ğ‘ ğ‘›ğ‘’ğ‘¥ğ‘¡ is the predicted skin, â„ğ‘¢is the user multi-behavior
sequence and ğ¶ğ‘€ğµğ‘… denotes our sequence generation model. The
details of our ğ¶ğ‘€ğµğ‘… will introduced in section 4.2.
The inherent nature of the employed generative model enables
the prediction of a variety of interaction items, not solely restricted
to scenario interactions. To achieve controlled prediction of specific
interaction types, we propose a Stimulus Prompt Mechanism. This
mechanism is designed to provide contextual cues to the model by
injecting carefully crafted stimulus tokens into the userâ€™s historical
interaction sequence during both the training and inference phases.
4.1.1 Stimulus Prompt Mechanism Illustration. To further illustrate
the essence and operation of the Stimulus Prompt Mechanism,
we provide an example that demonstrates how different stimulus
prompts can be employed to guide the sequence generation model
toward making scenario-specific predictions.
Consider a user ğ‘¢ğ‘–with a historical interaction sequence hğ‘¢ğ‘–=
[2003211_use ,2001452_buy ,2004562_click] . Suppose we have two
distinct scenarios: the in-game Store scenario and the Skin Trial
scenario. In the in-game Store scenario, the aim is to predict the
likelihood of purchase interactions, while in the Skin Trial scenario,
the aim is to predict the likelihood of use interactions.
For the in-game Store scenario, the stimulus token [stimulus_buy]
is inserted before the purchase interaction item in the userâ€™s inter-
action sequence, resulting in the modified sequence:
hbuy
ğ‘¢ğ‘–=[2003211_use]
|           {z           }[stimulus_buy]
|            {z            }[2001452_buy]
|            {z            }[2004562_click]
|             {z             }
Item 1 Stimulus prompt Item 2 Item 3
|                                                                                                     {z                                                                                                     }
Modified In-Game Store Scenario Interaction Seq
This modification is aimed at informing the model that upon en-
countering the [stimulus_buy] token, the subsequent interaction
to be predicted should ideally be a purchase interaction ( ???_buy ).
The numerous instances of such supervisory signals within the
sequence serve as a learning guide for the model to associate the
[stimulus_buy] token with a higher probability of the following
interaction being a purchase interaction.
How Does the CMBR Model Transfer Scenarios? When the
scenario shifts to the Skin Trial scenario, we simply replace the
token [stimulus_buy] with [stimulus_use] , which is inserted
before the use interaction item, resulting in the modified sequence:
Figure 2: The framework of our model. It illustrates the user
interaction sequence, embedding layer, dual-level interaction
attention mechanism, and inference process.
huse
ğ‘¢ğ‘–=[stimulus_use]
|            {z            }[2003211_use]
|           {z           }[2001452_buy]
|            {z            }[2004562_click]
|             {z             }
Stimulus prompt Item 1 Item 2 Item 3
|                                                                                                     {z                                                                                                     }
Modified Skin Trial Scenario Interaction Seq
In this scenario, the [stimulus_use] token serves as a cue for
the model to predict a use interaction ( ???_use ) as the subsequent
interaction. Through repeated instances of such supervisory sig-
nals within the training data, the model learns to associate the
[stimulus_use] token with a higher likelihood of the next inter-
action being a use interaction.
Through this illustration, it becomes evident how the Stimulus
Prompt Mechanism effectively tailors the modelâ€™s predictive focus
in alignment with the specific objectives of different scenarios by
leveraging strategically inserted stimulus tokens.
4.1.2 Training and Inference. The training and inference proce-
dures are extensions of the methodologies previously described,
adapted to accommodate the Stimulus Prompt Mechanism for scenario-
specific recommendations.
During the training phase, we add the stimulus prompt corre-
sponding to the target behavior of each token in front of it in the
sequence (e.g., buy, click, use, etc.). This allows for the unified train-
ing of stimulus prompts for all behaviors. Of course, we can also
adjust the model to be trained separately for each scenario. For the
in-game Store scenario, we replace the original interaction sequence
hğ‘¢ğ‘–with the modified sequence hbuy
ğ‘¢ğ‘–in the sequence generation
model. Similarly, for the Skin Trial scenario, huseğ‘¢ğ‘–replaces hğ‘¢ğ‘–in
the same model. This adaptation ensures that the model is trained
to recognize and respond to the scenario-specific stimulus prompts,
thereby enabling controlled multi-behavior recommendations.
During the inference phase, the goal is to predict the next in-
teraction item for a given scenario based on the userâ€™s historical
interaction sequence. To achieve this, a scenario-specific stimulus
4989Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
prompt is appended to the end of the modified interaction sequence,
acting as the scenario information ğ‘ ğ‘in our sequence generation
model. Specifically, the modified input to the model is given by ap-
pending the stimulus prompt to the modified interaction sequence
as follows:
hmod
ğ‘¢ğ‘–=hbuy
ğ‘¢ğ‘–âŠ•[stimulus_buy] orhuse
ğ‘¢ğ‘–âŠ•[stimulus_use] ,(2)
whereâŠ•denotes the concatenation operation. For instance, in the
in-game Store scenario, the stimulus prompt [stimulus_buy] is ap-
pended to hbuy
ğ‘¢ğ‘–, and in the Skin Trial scenario, the stimulus prompt
[stimulus_use] is appended to huseğ‘¢ğ‘–. The modified interaction se-
quence hmodğ‘¢ğ‘–is then used as the input to the model in Equation 8
to obtain the predicted item Ë†ğ‘–ğ‘¡ğ‘’ğ‘š :
Ë†ğ‘–ğ‘¡ğ‘’ğ‘š=arg max
ğ‘–ğ‘¡ğ‘’ğ‘šğ‘ƒ(ğ‘–ğ‘¡ğ‘’ğ‘š|hmod
ğ‘¢ğ‘–.Î˜). (3)
This approach guides the model to generate predictions that are
aligned with the intended interaction behavior for the given sce-
nario, enabling controlled multi-behavior recommendations that
are attuned to the specific objectives of different scenarios.
4.2 Sequence Generation Model Architecture
The architectural design of the proposed sequence generation model
is meticulously crafted to accommodate the fine-grained prediction
tasks as emphasized in the problem formulation. It processes the
historical interaction sequence hğ‘¢ğ‘–to predict the next interaction
itemğ‘ ğ‘›ğ‘’ğ‘¥ğ‘¡ for a userğ‘¢ğ‘–in a given scenario ğ‘ ğ‘. The architecture
comprises several integral components, namely the Embedding
Layer, Dual-Level Interaction Attention Mechanism, Position-wise
Feed-Forward Networks, and the Prediction Layer. Each of these
components plays a vital role in encoding the interaction history,
capturing sequential dependencies, and making accurate predic-
tions. See Figure 2 for a detailed illustration of the model architec-
ture.
4.2.1 Embedding Layer. The embedding layer transforms discrete
interaction items into continuous vectors through four sub-components:
Token, Skin, Position, and Time Embeddings.
Token Embedding: Maps each interaction item in the sequence
hğ‘¢ğ‘–to a vector using a token embedding matrix Eğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› .
Skin Embedding: Utilizes a skin embedding matrix Eğ‘ ğ‘˜ğ‘–ğ‘› to
assign common vectors to different interactions of the same skin,
enhancing consistency across interactions.
Position and Time Embeddings: Derived from respective
matrices Eğ‘ğ‘œğ‘ andEğ‘¡ğ‘–ğ‘šğ‘’, these embeddings capture the unique po-
sition and temporal context of interactions, with Time Embedding
providing uniform temporal information for interactions occurring
on the same day.
The final embedding for an item ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘—combines these compo-
nents:
eğ‘—=eğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›,ğ‘—+eğ‘ ğ‘˜ğ‘–ğ‘›,ğ‘—+eğ‘ğ‘œğ‘ ,ğ‘—+eğ‘¡ğ‘–ğ‘šğ‘’,ğ‘—, (4)
serving as input for the modelâ€™s subsequent layers to capture user
preferences and behaviors effectively.
4.2.2 Dual-Level Interaction Attention Mechanism. The Dual-Level
Interaction Attention Mechanism utilizes two types of embeddings:
interaction item embeddings, E, and skin item embeddings, Eskin, toprocess an interaction sequence. It incorporates two attention com-
ponents: Item-level Attention and Skin-level Attention, employing
a masked self-attention framework for causality. The attention out-
puts, OitemandOskin, are generated from the same self-attention
formula applied separately to EandEskin. This process is captured
by:
O=softmax 
QKğ‘‡
âˆšï¸
ğ‘‘ğ‘˜âŠ™V!
, (5)
where Q,K, and Vrepresent the query, key, and value matrices
obtained from the input embeddings. Specifically, Oitemresults
from applying this operation to E, and Oskinfrom applying it to
Eskin. The final output, Oatt, combines these two attention outputs
through concatenation and linear transformation:
Oatt=Wğ‘œÂ·concat(Oitem,Oskin)+bğ‘œ, (6)
with Wğ‘œandbğ‘œas the parameters of the concluding linear layer.
This mechanism allows the model to simultaneously consider in-
teractions at both the item and skin levels, thus enhancing its pre-
dictive performance in gaming scenarios.
4.2.3 Training and Prediction Procedure. The training procedure
for the model focuses on predicting the next interaction item in a
sequence based on previous interactions, employing an autoregres-
sive approach. For a sequence hğ‘¢ğ‘–=[ğ‘–ğ‘¡ğ‘’ğ‘š 1,ğ‘–ğ‘¡ğ‘’ğ‘š 2,...,ğ‘–ğ‘¡ğ‘’ğ‘š ğ¿], the
model maximizes the likelihood:
L(Î˜)=ğ¿âˆ‘ï¸
ğ‘¡=1logğ‘ƒ(ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘¡|ğ‘–ğ‘¡ğ‘’ğ‘š 1,ğ‘–ğ‘¡ğ‘’ğ‘š 2,...,ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘¡âˆ’1,ğ‘ ğ‘,Î˜),(7)
whereğ‘¡is the sequence position, ğ‘ ğ‘is the scenario, and Î˜are the
model parameters. This process aims to accurately predict the next
item based on preceding interactions.
Skin Prediction Procedure. Given a scenario ğ‘ ğ‘and the historical
interaction sequence hğ‘¢ğ‘–of a userğ‘¢ğ‘–, the model computes the
probability distribution over all possible next interaction items.
The interaction item with the highest probability is selected as the
predicted next interaction item. Formally, the predicted interaction
item Ë†ğ‘–ğ‘¡ğ‘’ğ‘š is obtained as:
Ë†ğ‘–ğ‘¡ğ‘’ğ‘š=arg max
ğ‘–ğ‘¡ğ‘’ğ‘šğ‘ƒ(ğ‘–ğ‘¡ğ‘’ğ‘š|hğ‘¢ğ‘–,ğ‘ ğ‘,Î˜). (8)
Subsequently, the skin ID is extracted from the predicted interac-
tion item to obtain the predicted skin. For instance, if the predicted
interaction item is â€˜2003211_useâ€™, the predicted skin ID is â€˜2003211â€™.
4.3 CTCVR Modeling
In the realm of online services, the sequence of user interactions
leading to a purchase often commences with a click. Therefore, the
historical sequences encountered during online service differ from
those used during training. To address this discrepancy and model
Click-Through rate (CTR) and Conversion Rate (CVR) effectively,
our model constructs the sequences hbuy
ğ‘¢ğ‘–andhclickğ‘¢ğ‘–akin to the
methodology delineated in previous sections.
During the final prediction step, our model computes scores
across all items in the vocabulary, emanating from the generative
model. These scores represent the probability distributions of CTR
4990KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yanjie Gou et al.
and CVR across all items. Formally, let pctrandpcvrdenote the
probability distribution vectors for CTR and CVR respectively, over
the entire vocabulary. The elements of these vectors are computed
as:
ğ‘ctr,ğ‘˜=ğ‘ƒ(ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘˜|hclick
ğ‘¢ğ‘–,Î˜), ğ‘ cvr,ğ‘˜=ğ‘ƒ(ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘˜|hbuy
ğ‘¢ğ‘–,Î˜).(9)
For each item ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘˜in the vocabulary, where ğ‘˜ranges from 1 to
the size of the vocabulary. To model the Click-Through Conversion
Rate (CTCVR), we perform an element-wise multiplication of the
two probability distributions, resulting in the CTCVR score for each
item in the vocabulary:
ğ‘ctcvr ,ğ‘˜=ğ‘ctr,ğ‘˜Â·ğ‘cvr,ğ‘˜, (10)
for eachğ‘˜, the item with the highest CTCVR score is selected as the
predicted item, embodying the skin recommendation for the user:
Ë†ğ‘–ğ‘¡ğ‘’ğ‘š=arg max ğ‘˜ğ‘ctcvr ,ğ‘˜. This procedure integrates the propensity
of a user both to click and to buy, ensuring a more nuanced and
effective recommendation that aligns with the userâ€™s interaction
tendencies in the online environment.
5 EXPERIMENTS
5.1 Experiment Setup
Table 1: Statistical information of the KingGame and
HeroGame datasets.
Dataset Num. of Users Avg. Seq. Length Num. of Behav
KingGame 35,524,021 110 14
HeroGame 39,730,461 66 11
5.1.1 Datasets. To evaluate the effectiveness of our proposed model,
we have constructed two real-world multi-behavior game recom-
mendation datasets, named KingGame andHeroGame.In our
study, the KingGame dataset includes 762 skins for 156 characters,
with each character having a varying number of skins influenced
by their popularity and longevity. Users can only select characters
and their corresponding skins that they have previously purchased.
In Table 1, â€˜Num. of Usersâ€™ indicates the volume of data by speci-
fying the number of users. â€˜Avg. Seq. Lengthâ€™ describes the average
sequence length per user, and â€˜Num. of Behavâ€™ gives the number of
different types of behaviors present in each dataset, representative
behaviors in the datasets include:
â€¢skinbuy - Skin purchase.
â€¢skinuse - Skin usage during gameplay.
â€¢skinclick - Clicking on a skin in-store.
â€¢wishadd - Adding a skin to a wishlist.
For data collection, we have utilized user interaction data up to 180
days prior to August 28, 2023, as historical sequences for each user,
which also serve as the training set. Subsequently, the interaction
data on August 29, 2023, is used to predict future interactions and
serve as labels, based on the 180-day historical data preceding
August 28, 2023.
5.1.2 Baseline Models. For a thorough verification of model effec-
tiveness, we compared various types of baseline models:â€¢GRU4Rec: [ 10] utilizes Gated Recurrent Units for session-based
recommendations.
â€¢Bert4Rec: [ 17] leverages Bidirectional Encoder Representations
from Transformers for sequential recommendation.
â€¢SASRec: [ 12] proposes a self-attention based sequential model
aiming to capture long-term semantics.
â€¢NextItNet: [ 32] employs holed convolutional layers and residual
blocks for next item recommendation.
â€¢CORE: [ 11] designs a framework that unifies the representa-
tion space for encoding and decoding processes in session-based
recommendation.
â€¢SINE: [ 19] proposes a Sparse Interest Network for sequential
recommendation.
â€¢STAMP: [ 13] captures both long-term and short-term user pref-
erences within a sequence for session-based recommendation.
â€¢LightSANS: [ 6] introduces Low-Rank Decomposed Self-Attention
Networks for next-item recommendation.
5.1.3 Parameter Settings. For a fair comparison, we maintain a con-
sistent set of hyper-parameters across all models. Our models are
trained in parallel on five NVIDIA V100 GPUs with 32GB memory
each. The batch size is fixed at 2560 for both training and inference
phases. The learning rate is set to 0.001. The sequence length is con-
figured at 128, with gradient accumulation steps set to 1 to ensure
the accuracy of gradient descent over a single step. The optimizer
employed for model optimization is LAMB [ 31]. The number of
warm-up steps is 10,000, and the steps per epoch are 5,000 to ensure
a smooth learning curve. For the model architecture, the number of
hidden layers and the number of attention heads are both set to 12,
with an embedding size, hidden size, and intermediate size of 256,
256, and 512, respectively, which are substantial for capturing com-
plex patterns in the data. These settings are determined to achieve
a balanced trade-off between computational efficiency and model
performance, while the reported results are averaged over multiple
runs to ensure reliability.
5.2 Overall Experiment
In our main experiment, we aim to validate our modelâ€™s predictive
prowess across two distinct scenarios: the purchasing environment
within the in-game Store and the intricate realm of click behaviors.
In the purchasing context, our objective focuses on anticipating
the skins a user might be inclined to buy. Meanwhile, for the click-
centric scenario, our model is tuned to discern and forecast the
skins users would be most likely to click on. We propose and em-
pirically evaluate three specialized variants of our model, each
designed to address unique aspects of user interaction and scenario
requirements.
â€¢Buy Prompt Only (BPO): In this variant, the Stimulus Prompt
Mechanism is applied with only the [stimulus_buy] token in-
serted into the userâ€™s historical sequence before every purchase
(â€˜buyâ€™) interaction. See Section 4.1 for more.
â€¢Buy and Click Prompts (BCP): This variant is an extension
of BPO. In addition to inserting the [stimulus_buy] token be-
fore purchase interactions, the [stimulus_click] token is also
inserted before every click interaction in the historical sequence.
â€¢CTCVR Modeling: This model is based on the approach dis-
cussed in Section 4.3.
4991Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Performance comparison of different models for in-game store purchase behavior prediction on HeroGame and
KingGame datasets.
ModelHeroGame Dataset KingGame Dataset
HR NDCG MRR HR NDCG MRR
@5 @10 @5 @10 @5 @10 @5 @10 @5 @10 @5 @10
GRU4Rec 0.0277 0.0566 0.0153 0.0246 0.0113 0.0151 0.0773 0.1462 0.045 0.0671 0.0346 0.0436
Bert4Rec 0.0315 0.0598 0.0182 0.0273 0.0139 0.0176 0.0865 0.1503 0.0525 0.0729 0.0413 0.0497
SASRec 0.0381 0.0688 0.0221 0.0319 0.0169 0.0209 0.0894 0.1542 0.0587 0.0781 0.0473 0.0551
NextItNet 0.0409 0.0776 0.0231 0.0349 0.0173 0.0221 0.0819 0.1458 0.0488 0.0693 0.0379 0.0464
CORE 0.0285 0.0551 0.0162 0.0247 0.0122 0.0156 0.0939 0.1539 0.0572 0.0765 0.0453 0.0532
SINE 0.0187 0.0422 0.0099 0.0174 0.0070 0.0100 0.0792 0.1442 0.0459 0.0667 0.0351 0.0435
STAMP 0.0250 0.0522 0.0138 0.0225 0.0102 0.0137 0.0764 0.1410 0.0438 0.0645 0.0332 0.0417
LightSANS 0.0384 0.0710 0.0222 0.0326 0.0169 0.0199 0.0899 0.1526 0.0557 0.0759 0.0445 0.0528
CMBR-BPO 0.3033 0.4099 0.2086 0.2430 0.1774 0.1916 0.2856 0.4182 0.1938 0.2366 0.1638 0.1813
CMBR-BCP 0.3556 0.4600 0.2571 0.2909 0.2246 0.2385 0.5162 0.6357 0.3855 0.4242 0.3422 0.3582
CMBR-CTCVR 0.3732 0.4789 0.2710 0.3052 0.2373 0.2514 0.5222 0.6423 0.3973 0.4362 0.3557 0.3719
Purchase prediction in the in-game Store scenario. Table 2 presents
the performance metrics of all models compared across the two
datasets used for our experiments. Based on the results outlined in
Table 2, we arrive at the following observations:
(1)Effectiveness of BCP over BPO: The BCP model shows superior
performance compared to BPO, suggesting that introducing
prompts for other interactions like clicks during the training
process can enhance the modelâ€™s predictive accuracy for buying
behaviors. The likely reason for this could be that the additional
[stimulus_click] tokens provide the model with richer con-
textual information, allowing it to learn more complex, multi-
faceted user behaviors that contribute to the ultimate buying
decision.
(2)Performance on Different Datasets: The model performs better
on KingGame dataset than HeroGame dataset. A likely expla-
nation is that KingGameâ€™s average user sequence length (110)
is greater than that of HeroGame (66), enabling the model to
better capture the userâ€™s diverse interests and preferences, thus
enhancing prediction accuracy. Additionally, the buying be-
havior in KingGame is more skewed towards popular skins,
simplifying the modelâ€™s task of predicting popular items.
(3)Superiority of CTCVR Modeling: The CTCVR model performs
the best among the variants, indicating its effectiveness over
pure CVR models. This is likely because CTCVR takes into ac-
count not just the conversion rate but also the click-through
rate, providing a more holistic understanding of user engage-
ment and intention. This dual-aspect scoring enables the model
to recommend items that users are both likely to click and to
buy, enhancing the recommendation quality.
Click prediction in the click-centric scenario Drawing in-
sights from table 3, and with an aim to validate the efficacy of our
model in predicting click behaviors, we delved into a deeper ex-
amination of the click-centric scenario. Based on our analysis, the
following observations stand out:
(1)In both KingGame and HeroGame datasets, the accuracy met-
rics for click behavior predictions surpass those of purchasebehavior predictions. One primary reason is the richer abun-
dance of click actions within the usersâ€™ historical data, thereby
providing a larger pool of training samples. This ample data
allows our model to gain a more comprehensive understand-
ing of the various factors influencing a userâ€™s decision to click,
offering a more refined predictive capability.
(2)Our model exhibits a significant improvement over baseline
models, and this enhancement is particularly pronounced in the
click prediction task compared to the purchase prediction task.
This suggests that our proposed â€˜Behavioral Stimulus Promptâ€™
mechanism becomes even more effective as the target behaviors
become more diverse. With a broader set of target behaviors,
the mechanism can be trained more thoroughly, leading to more
noticeable improvements in its predictive capabilities.
5.3 Ablation Experiments
To further understand the effectiveness of each component in our
model, we conduct the following ablation experiments:
â€¢No-Prompt CMBR (NPC): This setup operates on the base
CMBR but omits the utilization of our proposed â€˜Behavioral Stim-
ulus Promptâ€™ mechanism.
â€¢Skin-ID Only (SIO): In this setting, each userâ€™s historical se-
quence comprises only the IDs of the skins interacted with, Which
the historical sequence ğ»={ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘›}, and eachğ‘ ğ‘–represents
only the skin ID.
â€¢Buy-Only History (BOH): This configuration exclusively uses
purchase behaviors to form the usersâ€™ historical sequence. which
the historical sequence ğ»={ğ‘1,ğ‘2,...,ğ‘ ğ‘›}, and eachğ‘ğ‘–repre-
sents a buy action associated with a skin ID.
There are several insightful observations emerge from our ab-
lation experiments, the results of which are summarized in Table
4. (1) Skin-ID Only (SIO) significantly outperforms the NPC on
both datasets. This is likely due to the long-tail distribution of be-
haviors in our data, where a majority of the actions are â€˜useâ€™, and
â€˜buyâ€™ actions are significantly less frequent. Without the stimulus
4992KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yanjie Gou et al.
Table 3: Performance comparison of different models for Click behavior prediction on HeroGame and KingGame datasets.
ModelHeroGame Dataset KingGame Dataset
HR NDCG MRR HR NDCG MRR
@5 @10 @5 @10 @5 @10 @5 @10 @5 @10 @5 @10
GRU4Rec 0.0292 0.0579 0.0162 0.0254 0.0120 0.0157 0.0219 0.0421 0.0127 0.0192 0.0097 0.0124
Bert4Rec 0.0322 0.0601 0.0185 0.0275 0.0141 0.0177 0.0260 0.0480 0.0154 0.0222 0.0120 0.0148
SASRec 0.0381 0.0693 0.0219 0.0319 0.0166 0.0207 0.0226 0.0428 0.0132 0.0196 0.0101 0.0128
NextItNet 0.0417 0.0779 0.0235 0.0352 0.0177 0.0224 0.0267 0.0482 0.0160 0.0228 0.0151 0.0228
CORE 0.0280 0.0526 0.0159 0.0238 0.0120 0.0152 0.0232 0.0434 0.0138 0.0203 0.0137 0.0196
SINE 0.0199 0.0403 0.0105 0.0180 0.0075 0.0105 0.0189 0.0404 0.0101 0.0174 0.0079 0.0106
STAMP 0.0278 0.0550 0.0156 0.0240 0.0112 0.0141 0.0164 0.0312 0.0100 0.0147 0.0079 0.0124
LightSANS 0.0384 0.0710 0.0222 0.0326 0.0169 0.0199 0.0208 0.0386 0.0127 0.0184 0.0101 0.0124
CMBR-BCP 0.4370 0.5856 0.3926 0.4392 0.2668 0.2866 0.3021 0.4401 0.1920 0.2367 0.1560 0.1745
Table 4: Ablation Study on Datasets HeroGame and
KingGame.
Dataset Metric NPC SIO BOH CMBR
HeroGameHR@5 0.0248 0.0394 0.1147 0.3732
HR@10 0.0468 0.0710 0.1819 0.4789
NDCG@5 0.0236 0.0353 0.0951 0.2710
NDCG@10 0.0236 0.0353 0.0951 0.3052
MRR@5 0.0139 0.0207 0.0604 0.2373
MRR@10 0.0168 0.0248 0.0692 0.2514
KingGameHR@5 0.0389 0.0649 0.2025 0.5250
HR@10 0.0756 0.1229 0.3504 0.6377
NDCG@5 0.0355 0.0575 0.1742 0.3952
NDCG@10 0.0355 0.0575 0.1742 0.4326
MRR@5 0.0189 0.0304 0.1019 0.3530
MRR@10 0.0236 0.0380 0.1214 0.3685
prompt, the model has a propensity to predict these frequent be-
haviors, reducing the overall effectiveness of the system. (2) The
Buy-Only History (BOH) configuration exhibits the best perfor-
mance among the three ablation settings. This finding suggests that
focusing solely on the target behavior can enhance the modelâ€™s
predictive power for that specific behavior. However, this approach
also comes with limitations: it makes the model less versatile for dif-
ferent scenarios. Whenever the target behavior changes, the userâ€™s
history sequence needs to be reconstructed, and the model has to
be retrained. Additionally, this focused approach fails to capitalize
on the possible informational gains from other behaviors. (3) Our
full model outperforms all the ablated variants, emphasizing its
robustness and adaptability for generating scenario-specific recom-
mendations. These observations reaffirm the utility of our Stimulus
Prompt Mechanism for controllable multi-behavior recommenda-
tions, as elaborated in Section 4.1.
5.4 Visualization of User Interests
In our analysis, we aim to explore the distribution of user inter-
ests within two distinct datasets: HeroGame and KingGame. Each
Figure 3: User interest distribution across skins on HeroGame
and KingGame datasets.
dataset consists of user interactions with various game skins. By
applying the t-Distributed Stochastic Neighbor Embedding (t-SNE)
method [ 20], we reduce the dimension of the interaction embed-
dings to a two-dimensional space for visualization purposes. The
resulting plots provide a graphical representation of how users are
aligned with different skins, which can be interpreted as a proxy
for their interests.
The left plot in Figure 3 illustrates the user interest distribution
across skins in the HeroGame dataset. Each gray dot represents
a skin, and the colored stars represent individual users. The posi-
tioning of the stars within the plot indicates the concentration of
interest around certain skins. It is observed that some users have
a unique interest profile, as indicated by the isolated placement
of their corresponding stars. Similarly, the right plot shows the
distribution of interests in KingGame dataset. Despite the distinct
nature of the two datasets, common patterns emerge. Certain users
display a broad interest, engaging with a variety of skins scattered
throughout the space, while others show a more focused interest,
with stars clustered closely in specific regions of the plot.
Through these visualizations, we gain valuable insights into
the distribution and diversity of user interests, forming the basis
for our modelâ€™s scenario-aware recommendations. This affirms
the effectiveness of our CMBR model enhanced with Behavioral
Stimulus Prompts.
4993Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) KingGame: ARPU +1.06% over 7 days.
(b) HeroGame: ARPU +2.79% over 7 days.
Figure 4: Online experiment ARPU improvements for
KingGame and HeroGame
5.5 Online A/B Tests
In this section, we discuss the online experiments conducted for
both KingGame and HeroGame, focusing on the enhancements in
Average Revenue Per User (ARPU) implemented by our model.
Our model has been designed to generate top-k relevant results
based on the user historical sequence. These results are then used
as features for downstream recommendation models, such as the
Deep Interest Network (DIN) [ 37]. Instead of real-time recommen-
dations, our CMBR pre-generates recommendations for each user
and stores them in a Key-Value (KV) table. This design ensures that
during online requests, traditional lightweight models can quickly
retrieve our pre-stored results, addressing the real-time processing
demands effectively. Specifically, for KingGame, we utilized the
top-250 recommendations as features, while for HeroGame, we
employed the top-300.
For the KingGame platform, our online experiment spanned 7
days, from October 2nd to October 8th, 2023. As shown in the ac-
companying Figure 4, there was an average uplift of +1.06% in ARPU
over these 7 days, underscoring our modelâ€™s ability to boost user
engagement and revenue generation. In contrast, the HeroGame ex-
periment, which ran from November 18th to November 21th, 2023,
over 7 days, saw a more pronounced increase, with an average
ARPU improvement of +2.79%. This substantial growth highlights
the modelâ€™s versatility across diverse platforms and user groups.
6 CONCLUSION
In this paper, we introduce a controllable multi-behavior recommen-
dation system leveraging stimulus prompt mechanism, an initiative
that distinguishes itself by addressing the nuanced demands of vari-
ous in-game scenarios. By harnessing the pretraining capabilities ofLSMs, i.e., self-supervised autoregressive modeling, and incorporat-
ing Behavioral Stimulus Prompts, the model can selectively capture
scenario-specific user interactions to generate tailored recommen-
dation. The proposed controllable multi-behavior recommendation
model, validated through comprehensive testing, has demonstrated
a significant performance leap over traditional models. For future
work, we could pivot towards integrating a series of attributes re-
lated to game skins, as well as their images, into a Multimodal Large
Sequential Model tailored for game skin recommendations. Further-
more, the potential of cross-domain recommendation techniques
could be explored to utilize the historical behaviors of the same user
across different games. Such an approach could effectively address
challenges related to cold starts and data sparsity for certain users,
thereby enhancing the overall robustness and accuracy of the game
skin recommendation system.
ACKNOWLEDGMENTS
This research work is supported by the National Key Research and
Development Program of China under Grant No. 2021ZD0113602,
the National Natural Science Foundation of China under Grant
Nos.62206266, 62176014, the Fundamental Research Funds for the
Central Universities.
REFERENCES
[1]Chong Chen, Weizhi Ma, Min Zhang, Zhaowei Wang, Xiuqiang He, Chenyang
Wang, Yiqun Liu, and Shaoping Ma. 2021. Graph heterogeneous multi-relational
recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence ,
Vol. 35. 3958â€“3966.
[2]Chong Chen, Min Zhang, Yongfeng Zhang, Weizhi Ma, Yiqun Liu, and Shaoping
Ma. 2020. Efficient heterogeneous collaborative filtering without negative sam-
pling for recommendation. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 34. 19â€“26.
[3]Zhiyong Cheng, Sai Han, Fan Liu, Lei Zhu, Zan Gao, and Yuxin Peng. 2023.
Multi-Behavior Recommendation with Cascading Graph Convolution Networks.
InProceedings of the ACM Web Conference 2023. 1181â€“1189.
[4] George H Dunteman. 1989. Principal components analysis. Vol. 69. Sage.
[5]Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang
Tang, and Qing Li. 2023. Recommender Systems in the Era of Large Language
Models (LLMs). (Jul 2023).
[6]Xinyan Fan, Zheng Liu, Jianxun Lian, Wayne Xin Zhao, Xing Xie, and Ji-Rong
Wen. 2021. Lighter and better: low-rank decomposed self-attention networks for
next-item recommendation. In Proceedings of the 44th international ACM SIGIR
conference on research and development in information retrieval. 1733â€“1737.
[7]Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-
Seng Chua, and Depeng Jin. 2019. Neural Multi-task Recommendation from Multi-
behavior Data. In 2019 IEEE 35th International Conference on Data Engineering
(ICDE). 1554â€“1557. https://doi.org/10.1109/ICDE.2019.00140
[8]Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-
Seng Chua, and Depeng Jin. 2019. Neural multi-task recommendation from
multi-behavior data. In 2019 IEEE 35th international conference on data engineering
(ICDE). IEEE, 1554â€“1557.
[9]Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.
Recommendation as language processing (rlp): A unified pretrain, personalized
prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on
Recommender Systems. 299â€“315.
[10] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based recommendations with recurrent neural networks. arXiv
preprint arXiv:1511.06939 (2015).
[11] Yupeng Hou, Binbin Hu, Zhiqiang Zhang, and Wayne Xin Zhao. 2022. Core:
simple and effective session-based recommendation within consistent represen-
tation space. In Proceedings of the 45th international ACM SIGIR conference on
research and development in information retrieval. 1796â€“1801.
[12] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197â€“206.
[13] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: short-
term attention/memory priority model for session-based recommendation. In
4994KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yanjie Gou et al.
Proceedings of the 24th ACM SIGKDD international conference on knowledge dis-
covery & data mining. 1831â€“1839.
[14] Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wil-
fred Ng. 2019. SDM: Sequential deep matching model for online large-scale
recommender system. In Proceedings of the 28th ACM International Conference on
Information and Knowledge Management. 2635â€“2643.
[15] AleksandrV. Petrov and Craig Macdonald. 2023. Generative Sequential Recom-
mendation with GPTRec. (Jun 2023).
[16] Anima Singh, Trung Vu, Raghunandan Keshavan, Nikhil Mehta, Xinyang Yi,
Lichan Hong, Lukasz Heldt, Li Wei, Ed Chi, and Maheswaran Sathiamoorthy.
2023. Better Generalization with Semantic IDs: A case study in Ranking for
Recommendations. (Jun 2023).
[17] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-
resentations from transformer. In Proceedings of the 28th ACM international
conference on information and knowledge management. 1441â€“1450.
[18] Qiaoyu Tan, Jianwei Zhang, Ninghao Liu, Xiao Huang, Hongxia Yang, Jingren
Zhou, and Xia Hu. 2021. Dynamic memory based attention network for sequential
recommendation. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 35. 4384â€“4392.
[19] Qiaoyu Tan, Jianwei Zhang, Jiangchao Yao, Ninghao Liu, Jingren Zhou, Hongxia
Yang, and Xia Hu. 2021. Sparse-interest network for sequential recommendation.
InProceedings of the 14th ACM international conference on web search and data
mining. 598â€“606.
[20] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using
t-SNE. Journal of Machine Learning Research 9, 86 (2008), 2579â€“2605. http:
//jmlr.org/papers/v9/vandermaaten08a.html
[21] Qi Wang, Tingting Li, Yongjun Xu, Fei Wang, Boyu Diao, Lei Zheng, and Jincai
Huang. 2023. How to prevent malicious use of intelligent unmanned swarms?
The Innovation 4, 2 (2023).
[22] Yiqing Wu, Ruobing Xie, Yongchun Zhu, Xiang Ao, Xin Chen, Xu Zhang, Fuzhen
Zhuang, Leyu Lin, and Qing He. 2022. Multi-view multi-behavior contrastive
learning in recommendation. In International Conference on Database Systems for
Advanced Applications. Springer, 166â€“182.
[23] Yiqing Wu, Ruobing Xie, Yongchun Zhu, Fuzhen Zhuang, Ao Xiang, Xu Zhang,
Leyu Lin, and Qing He. 2022. Selective fairness in recommendation via prompts.
InProceedings of the 45th international ACM SIGIR conference on research and
development in information retrieval. 2657â€“2662.
[24] Yiqing Wu, Ruobing Xie, Yongchun Zhu, Fuzhen Zhuang, Xu Zhang, Leyu Lin,
and Qing He. 2024. Personalized Prompt for Sequential Recommendation. IEEE
Transactions on Knowledge and Data Engineering (2024).
[25] Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Bo Zhang, and Liefeng Bo.
2020. Multiplex behavioral relation learning for recommendation via memoryaugmented transformer network. In Proceedings of the 43rd international ACM
SIGIR conference on research and development in information retrieval. 2397â€“2406.
[26] Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, and Liefeng Bo. 2021. Graph
meta network for multi-behavior recommendation. In Proceedings of the 44th
international ACM SIGIR conference on research and development in information
retrieval. 757â€“766.
[27] Jingcao Xu, Chaokun Wang, Cheng Wu, Yang Song, Kai Zheng, Xiaowei Wang,
Changping Wang, Guorui Zhou, and Kun Gai. 2023. Multi-behavior Self-
supervised Learning for Recommendation. arXiv preprint arXiv:2305.18238 (2023).
[28] Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. 2023. OpenP5: Benchmarking
Foundation Models for Recommendation. arXiv preprint arXiv:2306.11134 (2023).
[29] Weijun Xu, Han Li, and Meihong Wang. 2023. Multi-behavior Guided Temporal
Graph Attention Network for Recommendation. In Pacific-Asia Conference on
Knowledge Discovery and Data Mining. Springer, 297â€“309.
[30] Yongjun Xu, Fei Wang, Zhulin An, Qi Wang, and Zhao Zhang. 2023. Artificial
intelligence for scienceâ€”bridging data to wisdom. The Innovation 4, 6 (2023).
[31] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bho-
janapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2019.
Large batch optimization for deep learning: Training bert in 76 minutes. arXiv
preprint arXiv:1904.00962 (2019).
[32] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, and Xi-
angnan He. 2019. A simple convolutional generative network for next item
recommendation. In Proceedings of the twelfth ACM international conference on
web search and data mining. 582â€“590.
[33] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan,
and Yongxin Ni. 2023. Where to Go Next for Recommender Systems? ID- vs.
Modality-based recommender models revisited. (Mar 2023).
[34] Weifeng Zhang, Jingwen Mao, Yi Cao, and Congfu Xu. 2020. Multiplex graph
neural networks for multi-behavior recommendation. In Proceedings of the 29th
ACM international conference on information & knowledge management. 2313â€“
2316.
[35] Zhao Zhang, Fuwei Zhang, Fuzhen Zhuang, and Yongjun Xu. 2023. Knowledge
Graph Error Detection with Hierarchical Path Structure. In Proceedings of the
32nd ACM International Conference on Information and Knowledge Management.
4430â€“4434.
[36] Xiaodong Zheng, Hao Ding, Hiroshi Mamitsuka, and Shanfeng Zhu. 2013. Collab-
orative matrix factorization with multiple similarities for predicting drug-target
interactions. In Proceedings of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining. 1025â€“1033.
[37] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059â€“1068.
4995Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) 2D, HeroGame
 (b) 3D angle 0, HeroGame
 (c) 3D angle 120, HeroGame
 (d) 3D angle 240, HeroGame
(e) 2D, KingGame
 (f) 3D angle 0, KingGame
 (g) 3D angle 120, KingGame
 (h) 3D angle 240, KingGame
Figure 5: Cluster visualizations of different actions for the HeroGame and KingGame datasets. The top row represents the
clustering results for HeroGame, while the bottom row illustrates the clustering for KingGame. The first column provides 2D
representations, and the following columns display 3D representations from varied perspectives. Each color corresponds to a
unique action, and the spatial grouping of points highlights the similarity in embeddings for these actions.
A BEHAVIORS VISUALIZATION ANALYSIS
In the process of examining the modelâ€™s capability to capture diverse
behavioral patterns from multi-behavioral sequences, we conduct
a series of visualization experiments to understand how effectively
our model embeds different behaviors. This not only provides an
intuitive understanding of our modelâ€™s embedding mechanism but
also establishes the modelâ€™s ability to differentiate between distinct
behaviors based on their relevance. We focus on plotting the em-
beddings corresponding to different behaviors extracted from the
tokens. To make these high-dimensional embeddings interpretable,
we employ Principal Component Analysis (PCA) [ 4], a popular
dimensionality reduction technique, to project the embeddings into
both two-dimensional and three-dimensional spaces.
The visualization for HeroGame, as shown in Figure 5, show-
cases a clear distinction among various behaviors. Behaviors like
use_match (â€˜matchâ€™, â€˜mix_battleâ€™, â€˜matchv1â€™, â€˜matchv3â€™, â€˜peakâ€™, â€˜qual-
ifyâ€™ and â€˜otherâ€™ are different game modes). and use_mix_battleappear to be closely knit in the embedded space, possibly indicat-
ing similarities in the context or motivation when these behav-
iors are executed in the game. In contrast, behaviors like skinbuy
andwashbuy are positioned further away, underlining their dis-
tinctive nature compared to other behaviors. For the KingGame
game, behaviors such as skinuse_match ,skinuse_matchv1 , and
skinuse_matchv3 form a tight cluster, hinting at their similar con-
text or importance within the gameplay. Conversely, herodetail
and secretbuy stand out, suggesting their unique relevance or
context within the game.
While distinct clusters are evident in both games, certain be-
havior points do overlap, suggesting that these behaviors share a
contextual similarity or importance within their respective games.
For instance, skinuse_peak andskinuse_qualify in KingGame
oruse_other anduse_rank in HeroGame bear resemblance in
their embedded space. Spread of Behavior Embeddings: HeroGame
exhibits a broader spread of behavior points compared to KingGame.
This might indicate a more diversified player behavior in HeroGame,
whereas KingGame player behaviors are possibly more concen-
trated or limited.
4996