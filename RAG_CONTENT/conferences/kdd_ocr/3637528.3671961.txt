Heterogeneity-Informed Meta-Parameter Learning for
Spatiotemporal Time Series Forecasting
Zheng Dongâˆ—
Southern University of
Science and Technology
Shenzhen, China
zhengdong00@outlook.comRenhe Jiangâˆ—
The University of Tokyo
Tokyo, Japan
jiangrh@csis.u-tokyo.ac.jpHaotian Gao
The University of Tokyo
Tokyo, Japan
gaoht6@outlook.comHangchen Liu
Southern University of
Science and Technology
Shenzhen, China
liuhc3@outlook.com
Jinliang Deng
Hong Kong University of
Science and Technology
Hong Kong, China
dengjinliang@ust.hkQingsong Wen
Squirrel AI
Seattle, USA
qingsongedu@gmail.comXuan Songâ€ 
Jilin University
Changchun, China
Southern University of
Science and Technology
Shenzhen, China
songx@sustech.edu.cn
ABSTRACT
Spatiotemporal time series forecasting plays a key role in a wide
range of real-world applications. While significant progress has
been made in this area, fully capturing and leveraging spatiotem-
poral heterogeneity remains a fundamental challenge. Therefore,
we propose a novel Heterogeneity-Informed Meta-Parameter
Learning scheme. Specifically, our approach implicitly captures
spatiotemporal heterogeneity through learning spatial and tem-
poral embeddings, which can be viewed as a clustering process.
Then, a novel spatiotemporal meta-parameter learning paradigm is
proposed to learn spatiotemporal-specific parameters from meta-
parameter pools, which is informed by the captured heterogeneity.
Based on these ideas, we develop a Heterogeneity- Informed Spa-
tiotemporal Meta-Network (HimNet) for spatiotemporal time se-
ries forecasting. Extensive experiments on five widely-used bench-
marks demonstrate our method achieves state-of-the-art perfor-
mance while exhibiting superior interpretability. Our code is avail-
able at https://github.com/XDZhelheim/HimNet .
CCS CONCEPTS
â€¢Information systems â†’Spatial-temporal systems; â€¢Com-
puting methodologies â†’Artificial intelligence.
KEYWORDS
spatiotemporal time series, heterogeneity, meta-parameter learning
âˆ—Equal contribution.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671961
Figure 1: Spatiotemporal heterogeneity. (a): Locations of two
selected areas. ğ´1is in the downtown region, while ğ´2is
a residential area. (b): Illustration of spatial heterogeneity,
with differing vehicle speed distributions between ğ´1and
ğ´2. (c): Demonstration of temporal heterogeneity, showing
distinct patterns between different time periods.
ACM Reference Format:
Zheng Dong, Renhe Jiang, Haotian Gao, Hangchen Liu, Jinliang Deng, Qing-
song Wen, and Xuan Song. 2024. Heterogeneity-Informed Meta-Parameter
Learning for Spatiotemporal Time Series Forecasting. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671961
1 INTRODUCTION
The rise of sensor networks has brought the widespread collec-
tion of spatiotemporal time series in urban environments. As these
datasets grow quickly, accurate spatiotemporal time series fore-
casting is increasingly important for many real-world applications
such as transportation [ 29,31], weather [ 1], and economics [ 32].
While considerable efforts [ 16,17,21,50] have been made, existing
 
631
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zheng Dong et al.
methods still face a major challenge: fully capturing and leverag-
ingspatiotemporal heterogeneity . Spatial heterogeneity refers
to the phenomenon that distinct patterns are observed across dif-
ferent location types during the same time period, while temporal
heterogeneity is reflected by the unique patterns observed at the
same location across different time periods. Figure 1 provides a real-
world example. As shown in Figure 1(a), we analyze two areas in
different spatial contexts: Area ğ´1in downtown LA and Area ğ´2in
a residential region. Figure 1(b) reveals substantial differences in ve-
hicle speed distributions between them: ğ´1shows a lower average
speed due to daily downtown congestion, while ğ´2exhibits higher
speeds with less commuting pressure. This demonstrates spatial
heterogeneity. Figure 1(c) illustrates their time series in different
temporal contexts. The patterns are stable during the holiday (Sun.)
but fluctuate violently during the workday (Mon.). Furthermore,
morning peak hours and nighttime off-peak hours also exhibit op-
posing trends, showing temporal heterogeneity. These phenomena
occur consistently throughout spatiotemporal data. Thus, effec-
tively modeling spatiotemporal heterogeneity is critical to accurate
forecasting. As time series from diverse spatial locations and time
periods display distinct characteristics, proper capturing of het-
erogeneity relies on identifying and distinguishing such varied
spatiotemporal contexts.
To account for different contexts, graph-based approaches [ 35,
52,62] attempt to capture heterogeneity by utilizing handcrafted
features such as graph topology and similarity metrics between
time series. However, depending heavily on pre-defined repre-
sentations limits their adaptability and generalizability, as these
features cannot encompass the full complexity of diverse contexts.
To enhance adaptability, other works introduce meta-learning tech-
niques [ 46,47] that apply multiple parameter sets across different
spatial locations. However, they also rely on auxiliary characteris-
ticsincluding Points-of-Interests (POIs) and sensor placements. Ad-
ditionally, the high computational and memory costs limit their
applicability to large-scale datasets. Recent representation learning
methods [ 38,51] effectively identify heterogeneity through input
embeddings. However, their oversimplified downstream processing
structures fail to fully leverage the representational power. While
self-supervised learning methods [ 18,24] also succeed in capturing
spatiotemporal heterogeneity by designing additional tasks, fully
end-to-end joint optimization for spatiotemporal forecasting con-
tinues to present difficulties for these approaches. In short, the key
drawbacks are: (1) reliance on auxiliary features; (2) high compu-
tational and memory costs; (3) failure to fully leverage captured
heterogeneity; and (4) difficulties with end-to-end optimization.
To address the above limitations in prior works, we propose
a novel Heterogeneity-Informed Meta-Parameter Learning
scheme and a Heterogeneity- Informed Meta-Network (HimNet)
for spatiotemporal time series forecasting. In detail, we first implic-
itly characterize heterogeneity by learning spatial and temporal
embeddings from a clustering view. The representations gradually
differentiate and form distinct clusters during training, capturing
underlying spatiotemporal contexts. Next, a novel spatiotemporal
meta-parameter learning paradigm is proposed to enhance model
adaptability and generalizability, which is flexible for various do-
mains. Specifically, we learn a unique parameter set for each spa-
tiotemporal context via querying a small meta-parameter pool.Based on these, we further propose Heterogeneity-Informed Meta-
Parameter Learning that uses the characterized heterogeneity to
inform meta-parameter learning. Thus, our approach can not only
capture but explicitly leverage spatiotemporal heterogeneity to
improve forecasting. Finally, we design an end-to-end network
called HimNet, implementing these techniques for spatiotemporal
forecasting. In summary, our contributions are three-fold:
â€¢Methodologically, we present a novel HimNet model for
spatiotemporal time series forecasting. It captures inherent
spatiotemporal heterogeneity through learnable embeddings,
which then inform spatiotemporal meta-parameter learning
to enhance model adaptability and generalizability.
â€¢Theoretically, to the best of our knowledge, our proposed
Heterogeneity-Informed Meta-Parameter Learning is the
first method that not only captures but also fully leverages
spatiotemporal heterogeneity. This enables our model to
distinguish and adapt to different spatiotemporal contexts.
â€¢Empirically, HimNet significantly outperforms state-of-the-
art methods on five benchmarks based on extensive experi-
ments, demonstrating superior performance with competi-
tive efficiency. Visualization experiments further illustrate
its strong interpretability through meta-parameters.
2 RELATED WORK
2.1 Spatiotemporal Forecasting
Spatiotemporal forecasting has seen extensive research as it plays a
key role in many real-world applications [ 14,15,27,37,39,65,69â€“
71]. Early approaches relied on traditional time series analysis
methods such as ARIMA [ 45] and VAR [ 53], as well as machine
learning techniques including ğ‘˜-NN [ 10] and SVM [ 55], but they
often fail to capture complex spatiotemporal dependencies inherent
to the data. Recent years have witnessed remarkable progress in
deep learning methods. Recurrent Neural Networks (RNNs) like
LSTMs [ 42,43] and GRUs [ 7] achieved performance gains by effec-
tively modeling the temporal dynamics. Convolutional networks
such as WaveNet [ 44] also found success via their long receptive
fields. However, these models do not fully represent spatial de-
pendencies critical to networked urban systems. To address this
limitation, Graph Convolutional Networks (GCNs) have been exten-
sively explored for spatiotemporal forecasting. Pioneering works
such as STGCN [ 64] and DCRNN [ 36] achieved better performance
by integrating GCNs with temporal models [ 2,4,59,60]. Building
on this foundation, many innovative methods have been proposed
in recent years [ 12,13,28,30,33,34,49,61]. Moreover, Trans-
former [ 56] has also revolutionized the field, motivating time series
transformers [ 6,57] that expertly capture spatiotemporal correla-
tions [ 23,26,38,66] or handle long sequences [ 40,58,67,68]. While
showing impressive performance, existing work still lacks further
exploration of spatiotemporal heterogeneity. Our study aims to fill
this gap through a novel HimNet that captures and leverages the
heterogeneity for improved spatiotemporal forecasting.
2.2 Meta-Parameter Learning
One of the earliest works [ 48] proposed predicting network param-
eters for temporal modeling. More recently, Hypernetworks [ 20]
applied this idea to recurrent networks by generating adaptive
 
632Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
weights, acting as a form of weight-sharing across layers. Lear-
net [ 3] was constructed as a second deep network to predict the
parameters of another deep model. Dynamic Filter Networks [ 25]
dynamically generate convolutional filters conditioned on input, in-
creasing flexibility due to their adaptive nature. For meta multi-task
learning, MetaMTL [ 5] introduced a shared Meta-LSTM to gen-
erate basic LSTM parameters based on the current input context.
In spatiotemporal applications, ST-GFSL [ 41] proposed learning
non-shared parameters for cross-city transfer using learned spa-
tiotemporal meta-knowledge from the input city. While effective
in other domains, these methods were not designed specifically for
spatiotemporal forecasting. To address this, recent works have intro-
duced meta-parameter learning techniques for spatiotemporal data.
ST-MetaNet [ 46] first applied meta-learning across spatial locations,
using auxiliary POI and road network information as spatial meta-
knowledge to learn multiple parameter sets. AGCRN [ 2] proposed
node adaptive parameter learning to generate node-specific param-
eters from a learned embedding matrix, which can be interpreted
as learning node specific patterns from a set of candidate patterns
discovered from all spatiotemporal time series. ST-WA [ 8] proposes
a spatiotemporal aware approach to jointly learn location-specific
and time-varying model parameters from encoded stochastic vari-
ables. In contrast to these approaches, as shown in Table 1, Him-
Net learns meta-parameters using only the spatiotemporal time
series itself, without depending on auxiliary features. Critically, we
propose a concurrent learning scheme that operates on the tempo-
ral, spatial, and spatiotemporal joint dimensions simultaneously.
Therefore, HimNet achieves maximum adaptability to any kind of
spatiotemporal context. This makes our approach highly flexible
and capable of extracting the full informational value from learned
spatiotemporal heterogeneity.
Table 1: Comparison of related meta-parameter learning
methods for spatiotemporal forecasting.
Method Data-Independent Temporal Spatial Joint
ST-MetaNet [46] Ã—Ã— âœ“Ã—
AGCRN [2] âœ“Ã— âœ“Ã—
ST-WA [8] âœ“Ã— Ã— âœ“
HimNet âœ“ âœ“ âœ“ âœ“
3 PROBLEM DEFINITION
Given a spatiotemporal time series ğ‘‹ğ‘¡âˆ’(ğ‘‡âˆ’1):ğ‘¡over the past ğ‘‡time
steps, our goal is to forecast the values over the future ğ‘‡â€²time
steps. That is, we aim to map [ğ‘‹ğ‘¡âˆ’(ğ‘‡âˆ’1),...,ğ‘‹ğ‘¡]â†’[ğ‘‹ğ‘¡+1,...,ğ‘‹ğ‘¡+ğ‘‡â€²],
where eachğ‘‹ğ‘–âˆˆRğ‘represents the observations at the ğ‘–-th time step
forğ‘time series, typically from sensors deployed at ğ‘locations.
4 METHODOLOGY
In this section, we present details of our proposed Heterogeneity-
Informed Meta-parameter Learning scheme along with the HimNet
model, where the key components are depicted in Figure 2. To better
illustrate our proposed method, we consider a minibatch dimension
ğµin the following discussions. For ease of understanding, ğµcan be
assumed to be 1 without loss of generality.4.1 Spatiotemporal Heterogeneity Modeling
A key aspect of modeling spatiotemporal heterogeneity is identify-
ing and distinguishing input contexts across both temporal and spa-
tial dimensions [ 11]. Rather than relying on auxiliary data [ 46,52],
we employ learnable embeddings [ 38,51], which assign a unique
representation to each spatiotemporal context.
For the temporal dimension, we construct a time-of-day embed-
ding dictionary ğ·ğ‘¡ğ‘œğ‘‘âˆˆRğ‘ğ‘‘Ã—ğ‘‘ğ‘¡ğ‘œğ‘‘and a day-of-week dictionary
ğ·ğ‘‘ğ‘œğ‘¤âˆˆRğ‘ğ‘¤Ã—ğ‘‘ğ‘‘ğ‘œğ‘¤, whereğ‘‘ğ‘¡ğ‘œğ‘‘andğ‘‘ğ‘‘ğ‘œğ‘¤ are the embedding di-
mensions,ğ‘ğ‘‘denotes the number of timesteps per day, and ğ‘ğ‘¤
represents the number of days in a week. Given a minibatch of
input samples ğ‘‹ğ‘âˆˆRğµÃ—ğ‘‡Ã—ğ‘with historical length ğ‘‡, the last
stepâ€™s timestamp of each sample in the minibatch is used as the
time index to extract the corresponding time-of-day embedding
ğ¸ğ‘¡ğ‘œğ‘‘âˆˆRğµÃ—ğ‘‘ğ‘¡ğ‘œğ‘‘and day-of-week embedding ğ¸ğ‘‘ğ‘œğ‘¤âˆˆRğµÃ—ğ‘‘ğ‘‘ğ‘œğ‘¤
from the dictionaries. The two embeddings are concatenated to
obtain the overall temporal embedding ğ¸ğ‘¡âˆˆRğµÃ—ğ‘‘ğ‘¡:
ğ¸ğ‘¡=ğ¸ğ‘¡ğ‘œğ‘‘||ğ¸ğ‘‘ğ‘œğ‘¤, (1)
whereğ‘‘ğ‘¡=ğ‘‘ğ‘¡ğ‘œğ‘‘+ğ‘‘ğ‘‘ğ‘œğ‘¤ represents the temporal embedding dimen-
sion and||denotes concatenation operation. Time-of-day identifies
periodic patterns with fine-grained time intervals, capturing phe-
nomena like peak and off-peak hours. Day-of-week identifies mean-
ingful longer-term patterns, such as the distinction in time series
between weekdays and weekends. Leveraging these dictionaries,
the temporal embedding aims to learn representations that can
implicitly recognize and account for such temporal heterogeneity
over multiple timescales.
For the spatial dimension, we utilize a spatial embedding matrix
ğ¸ğ‘ âˆˆRğ‘Ã—ğ‘‘ğ‘ , whereğ‘denotes the total number of time series or
spatial locations in the dataset (e.g. number of sensors) and ğ‘‘ğ‘ is
the dimension of the embedding vector assigned to each location.
Unlike the temporal dimension where the embeddings are extracted
from learned dictionaries, here each of the ğ‘spatial locations is di-
rectly associated with a unique learnable spatial embedding vector
initialized randomly in the embedding matrix. The goal of introduc-
ingğ¸ğ‘ is to account for the inherent functional differences between
locations that can influence time series patterns, such as factors
related to infrastructure and urban design. It aims to learn latent
representations that can help distinguish such diverse contexts,
without the need for auxiliary geographical data. This facilitates
modeling the spatial heterogeneity present in each location.
Furthermore, learning these embedding vectors essentially per-
forms a dynamic clustering process. During model training, the
representations within the embedding matrices gradually differen-
tiate based on the input time series. Embedding vectors belonging
to spatiotemporal contexts that exhibit similar trends in their time
series will move closer in the latent space, and vice versa. As a re-
sult, this dynamic adjustment of the embeddings can be viewed as a
clustering process, where representations organically form clusters.
Each distinct cluster captures a typical context in the spatiotempo-
ral time series. Analogous to the NLP models where the learned
embeddings of "man" and "woman" can naturally be very close,
this clustering process could be successfully achieved without in-
volving any additional constraints like the way did in [ 24,28]. This
clustering perspective is what empowers the model to distinguish
 
633KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zheng Dong et al.
Figure 2: The overall illustration of the proposed Heterogeneity-Informed Meta-Parameter Learning and HimNet model.
different contexts and thereby accurately model the heterogeneity
across space and time. Through the learned embeddings, spatiotem-
poral heterogeneity is modeled in an intrinsic, data-driven manner,
without requiring manual feature engineering or domain expertise
to describe it in advance.
4.2 Spatiotemporal Meta-Parameter Learning
To fully leverage the heterogeneity encoded in the spatiotemporal
embeddings, we propose Heterogeneity-Informed Meta-Parameter
Learning. It provides a simple yet effective way to improve model
adaptability and generalizability.
We begin by introducing the general meta-parameter learning
paradigm. For the modelâ€™s original parameter space Î˜with sizeğ‘†,
our approach first extends it along a specific dimension to create
an enlarged parameter space Î˜â€². Specifically, we can build three en-
larged spaces for the temporal and spatial dimensions: temporal pa-
rameter space Î˜ğ‘¡âˆˆRTÃ—ğ‘†, spatial parameter space Î˜ğ‘ âˆˆRğ‘Ã—ğ‘†, and
spatiotemporal joint space Î˜ğ‘ ğ‘¡âˆˆRTÃ—ğ‘Ã—ğ‘†. Here,Trepresents the
total number of timesteps and ğ‘is the number of spatial locations
in the dataset. This extension aims to assign a unique parameter
set to each spatiotemporal context, rather than sharing Î˜. Directly
optimizing these enlarged parameter spaces is prohibitively expen-
sive, as they become very huge, especially when Tandğ‘are large,
which will lead to exploding computational and memory costs. To
address this challenge, we maintain a small meta-parameter pool
for each parameter space, which contains ğ‘˜parameter candidates.
The pool size ğ‘˜is defined to be much smaller than Tandğ‘, but
sufficiently large to encapsulate the variety of different contexts.
For an incoming query representing a specific spatiotemporal con-
text, we dynamically generate its meta-parameter as a weighted
combination of the pool candidates.
For a meta-parameter pool ğ‘ƒâˆˆRğ‘˜Ã—ğ‘†, given a queryQâˆˆRğ‘˜, the
general formulation of our meta-parameter learning paradigm is:
ğœ—=QÂ·ğ‘ƒ, (2)whereğœ—âˆˆRğ‘†is the generated meta-parameter. By maintaining
meta-parameter pools, our approach alleviates this computational
burden by optimizing within the compact pools rather than the
full enlarged parameter spaces. Thus, the complexity is reduced
fromğ‘‚(Tğ‘)toğ‘‚(ğ‘˜), whereğ‘˜â‰ªTğ‘. This makes the learning
computationally feasible for large spatiotemporal datasets.
Specifically, we leverage the embeddings proposed earlier that
encode spatiotemporal heterogeneity as queries to adaptively gen-
erate the meta-parameters for each spatiotemporal context. This
results in a Heterogeneity-Informed Meta-Parameter Learning
approach. Based on Equation 2, three types of meta-parameters are
generated for the respective enlarged parameter spaces.
Temporal Meta-Parameters. We maintain a small temporal meta-
parameter pool ğ‘ƒğ‘¡âˆˆRğ‘‘ğ‘¡Ã—ğ‘†containingğ‘‘ğ‘¡candidates. Using the
temporal embedding ğ¸ğ‘¡âˆˆRğµÃ—ğ‘‘ğ‘¡as queries, the temporal meta-
parameters ğœ—ğ‘¡âˆˆRğµÃ—ğ‘†are generated by:
ğœ—ğ‘¡=ğ¸ğ‘¡Â·ğ‘ƒğ‘¡. (3)
Spatial Meta-Parameters. We maintain a small spatial meta-
parameter pool ğ‘ƒğ‘ âˆˆRğ‘‘ğ‘ Ã—ğ‘†withğ‘‘ğ‘ candidates. Using the spatial
embedding ğ¸ğ‘ âˆˆRğ‘Ã—ğ‘‘ğ‘ as queries, the spatial meta-parameters
ğœ—ğ‘ âˆˆRğ‘Ã—ğ‘†for each location are generated by:
ğœ—ğ‘ =ğ¸ğ‘ Â·ğ‘ƒğ‘ . (4)
ST-Mixed Meta-Parameters. To directly create a joint spatiotem-
poral embedding is prohibitive, as its size would also be too large
to optimize effectively. Instead, we utilize the rich information in-
herently contained within the input data itself [ 8], where the time
series patterns naturally reflect spatiotemporal heterogeneity. For
a minibatch of input samples ğ‘‹ğ‘âˆˆRğµÃ—ğ‘‡Ã—ğ‘, we encode it into a
spatiotemporal embedding ğ¸ğ‘ ğ‘¡âˆˆRğµÃ—ğ‘Ã—ğ‘‘ğ‘ ğ‘¡as:
ğ¸ğ‘ ğ‘¡=ğ¹ğ‘’ğ‘›ğ‘(ğ‘‹ğ‘), (5)
 
634Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
whereğ‘‘ğ‘ ğ‘¡is the embedding dimension, and ğ¹ğ‘’ğ‘›ğ‘(Â·)represents an
encoding function such as linear projection, TCN [ 64], or GRU [ 36]
designed to encode the input into a hidden representation.
Accordingly, we maintain a spatiotemporal meta-parameter pool
ğ‘ƒğ‘ ğ‘¡âˆˆRğ‘‘ğ‘ ğ‘¡Ã—ğ‘†havingğ‘‘ğ‘ ğ‘¡candidates. Leveraging the encoded spa-
tiotemporal embedding ğ¸ğ‘ ğ‘¡as queries, the spatiotemporal-mixed
(ST-mixed) meta-parameters ğœ—ğ‘ ğ‘¡âˆˆRğµÃ—ğ‘Ã—ğ‘†are generated by:
ğœ—ğ‘ ğ‘¡=ğ¸ğ‘ ğ‘¡Â·ğ‘ƒğ‘ ğ‘¡. (6)
While we generate the meta-parameters based on the proposed
embeddings, the underlying meta-parameter learning technique
itself is not limited specifically to these embeddings. We present
a purely data-driven approach using only the intrinsic spatiotem-
poral heterogeneity in the time series itself. However, other aux-
iliary spatiotemporal features could also be utilized if available in
the dataset, such as POIs, weather patterns, or urban structural
characteristics. These supplemental contextual attributes could be
encoded into the query representation to provide further instruc-
tion for meta-parameter learning. Therefore, the method is flexible
and can leverage diverse contextual features to learn improved
meta-parameters tailored to various application domains.
4.3 Heterogeneity-Informed Spatiotemporal
Meta-Network
Leveraging our proposed techniques, we implement an end-to-
end model called Heterogeneity-Informed Spatiotemporal Meta-
Network (HimNet) for accurate spatiotemporal forecasting.
Taking inspiration from GCNs, researchers have developed graph
convolutional recurrent networks [ 36,49,63] which leverage both
graph convolutions and recurrent transformations. By integrating
graph convolutions within recurrent cells, these models can effec-
tively capture temporal dynamics while also accounting for the
spatial relations in the graph. Therefore, our proposed HimNet ar-
chitecture uses variants of the widely adopted Graph Convolutional
Recurrent Unit (GCRU) [ 2,28] as the fundamental building block.
GCRU is formulated by:
ğ‘Ÿğ‘¡=ğœ(Î˜ğ‘Ÿâ˜…G[ğ‘‹ğ‘¡,ğ»ğ‘¡âˆ’1]+ğ‘ğ‘Ÿ)
ğ‘¢ğ‘¡=ğœ(Î˜ğ‘¢â˜…G[ğ‘‹ğ‘¡,ğ»ğ‘¡âˆ’1]+ğ‘ğ‘¢)
ğ‘ğ‘¡=tanh(Î˜ğ‘â˜…G[ğ‘‹ğ‘¡,(ğ‘Ÿğ‘¡âŠ™ğ»ğ‘¡âˆ’1)]+ğ‘ğ‘)
ğ»ğ‘¡=ğ‘¢ğ‘¡âŠ™ğ»ğ‘¡âˆ’1+(1âˆ’ğ‘¢ğ‘¡)âŠ™ğ‘ğ‘¡,(7)
whereğ‘‹ğ‘¡âˆˆRğ‘andğ»ğ‘¡âˆˆRğ‘Ã—â„denote the input and output at
timestepğ‘¡.â„is the hidden size. ğ‘Ÿğ‘¡andğ‘¢ğ‘¡are the reset and update
gates.Î˜ğ‘Ÿ,Î˜ğ‘¢,Î˜ğ‘are parameters for the corresponding filters in the
graph convolution operation â˜…G, which is defined as:
ğ‘=ğ‘ˆâ˜…G=ğ¾âˆ‘ï¸
ğ‘˜=0Ëœğ´ğ‘ˆğ‘Šğ‘˜, (8)
whereğ‘ˆâˆˆRğ‘Ã—ğ¶is the input, ğ‘âˆˆRğ‘Ã—â„is the output, and ğ¶
denotes the input channels. Ëœğ´represents the topology of graph G,
andğ‘ŠâˆˆRğ¾Ã—ğ¶Ã—â„is the kernel parameter Î˜.
By applying temporal meta-parameter learning to GCRU, we
propose a temporal meta-encoder. Referring to Equation 3, the
generated temporal meta-parameters in this encoder take the form
ğ‘Šğ‘¡âˆˆRğµÃ—ğ¾Ã—ğ¶Ã—â„. Similarly, we build a spatial meta-encodervia spatial meta-parameter learning, with generated spatial meta-
parametersğ‘Šğ‘ âˆˆRğ‘Ã—ğ¾Ã—ğ¶Ã—â„by applying Equation 4. For the graph
Gused in the encoders, rather than relying on a static pre-defined
adjacency matrix as in prior works [ 19,36,64], we follow the adap-
tive graph learning design [ 59,60]. Specifically, we leverage our
learned spatial embeddings ğ¸ğ‘ to dynamically generate the adja-
cency matrix Ëœğ´âˆˆRğ‘Ã—ğ‘as:
Ëœğ´=Softmax(ReLU(ğ¸ğ‘ Â·ğ¸âŠ¤
ğ‘ )). (9)
In short, given an input ğ‘‹ğ‘âˆˆRğµÃ—ğ‘‡Ã—ğ‘, the two encoders operate
in parallel to encode it into two hidden representations based on
their respective meta-parameters. These representations are then
summed to yield the final combined hidden encoding ğ»âˆˆRğµÃ—ğ‘Ã—â„.
To decode the latent representation ğ»into predictions, we pro-
pose a spatiotemporal meta-decoder that leverages ST-mixed
meta-parameters. Specifically, we first generate the corresponding
spatiotemporal embedding ğ¸ğ‘ ğ‘¡âˆˆRğµÃ—ğ‘Ã—ğ‘‘ğ‘ ğ‘¡fromğ»:
ğ¸ğ‘ ğ‘¡=ğ»Â·ğ‘Šğ¸+ğ‘ğ¸, (10)
whereğ‘Šğ¸âˆˆRâ„Ã—ğ‘‘ğ‘ ğ‘¡andğ‘ğ¸âˆˆRğ‘‘ğ‘ ğ‘¡are the linear projection pa-
rameters. According to Equation 6, we then produce the ST-mixed
meta-parameters for GCRU as ğ‘Šğ‘ ğ‘¡âˆˆRğµÃ—ğ‘Ã—ğ¾Ã—ğ¶Ã—â„using query
ğ¸ğ‘ ğ‘¡. Moreover, inspired by [28], we apply a time-varying adaptive
graphGğ‘ ğ‘¡inferred from ğ¸ğ‘ ğ‘¡:
Ëœğ´ğ‘ ğ‘¡=Softmax(ReLU(ğ¸ğ‘ ğ‘¡Â·ğ¸âŠ¤
ğ‘ ğ‘¡)), (11)
where Ëœğ´ğ‘ ğ‘¡âˆˆRğµÃ—ğ‘Ã—ğ‘. The decoder takes the initial hidden state
fromğ»and iteratively generates predictions Ë†ğ‘‹for each future
timestep by applying the GCRU cell parameterized by ğ‘Šğ‘ ğ‘¡.
Given ground truth ğ‘‹ğ‘¡+1:ğ‘¡+ğ‘‡â€², we mainly adopt the Mean Abso-
lute Error (MAE) loss as our training objective to optimize multi-
step predictions jointly. The loss function for HimNetâ€™s multi-step
spatiotemporal time series forecasting can be formulated as:
L=ğ‘€ğ´ğ¸(Ë†ğ‘‹ğ‘¡+1:ğ‘¡+ğ‘‡â€²,ğ‘‹ğ‘¡+1:ğ‘¡+ğ‘‡â€²)=1
ğ‘‡â€²ğ‘ğ‘‡â€²âˆ‘ï¸
ğ‘–=1ğ‘âˆ‘ï¸
ğ‘—=1|Ë†ğ‘‹ğ‘–,ğ‘—âˆ’ğ‘‹ğ‘–,ğ‘—|.(12)
5 EXPERIMENT
5.1 Experimental Setup
Datasets. Our proposed model is evaluated on the five most com-
monly used spatiotemporal forecasting benchmarks. The METRLA
dataset and PEMSBAY dataset [ 36] contain traffic speed data recorded
by traffic sensors in Los Angeles and the Bay Area, respectively. The
PEMS04, PEMS07, and PEMS08 datasets [ 52] are three traffic flow
datasets collected from the California Transportation Performance
Management System (PeMS). The raw data has a fine temporal
Table 2: Summary of datasets.
Dataset #Sensors #Timesteps Time Range
METRLA 207 34,272 03/2012 - 06/2012
PEMSBAY 325 52,116 01/2017 - 06/2017
PEMS04 307 16,992 01/2018 - 02/2018
PEMS07 883 28,224 05/2017 - 08/2017
PEMS08 170 17,856 07/2016 - 08/2016
 
635KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zheng Dong et al.
Table 3: Performance on METRLA, PEMSBAY, PEMS04, PEMS07, and PEMS08 datasets.
Dataset
Metric HI
GRU STGCN DCRNN GWNet AGCRN GTS STNorm STID ST-WA PDFormer MegaCRN HimNetMETRLAStep
3
15 minMAE 6.80
3.07 2.75 2.67 2.69 2.85 2.75 2.81 2.82 2.89 2.83 2.65 2.60
RMSE 14.21
6.09 5.29 5.16 5.15 5.53 5.27 5.57 5.53 5.62 5.45 5.08 5.02
MAPE 16.72%
8.14% 7.10% 6.86% 6.99% 7.63% 7.12% 7.40% 7.75% 7.66% 7.77% 6.73% 6.70%
Step 6
30 minMAE 6.80
3.77 3.15 3.12 3.08 3.20 3.14 3.18 3.19 3.25 3.20 3.04 2.95
RMSE 14.21
7.69 6.35 6.27 6.20 6.52 6.33 6.59 6.57 6.61 6.46 6.18 6.06
MAPE 16.72%
10.71% 8.62% 8.42% 8.47% 9.00% 8.62% 8.47% 9.39% 9.22% 9.19% 8.22% 8.11%
Step 12
60 minMAE 6.80
4.88 3.60 3.54 3.51 3.59 3.59 3.57 3.55 3.68 3.62 3.51 3.37
RMSE 14.21
9.75 7.43 7.47 7.28 7.45 7.44 7.51 7.55 7.59 7.47 7.39 7.22
MAPE 16.71%
14.91% 10.35% 10.32% 9.96% 10.47% 10.25% 10.24% 10.95% 10.78% 10.91% 10.01% 9.79%PEMSBA
YStep
3
15 minMAE 3.05
1.44 1.36 1.31 1.30 1.35 1.37 1.33 1.31 1.37 1.32 1.28 1.27
RMSE 7.03
3.15 2.88 2.76 2.73 2.88 2.92 2.82 2.79 2.88 2.83 2.71 2.68
MAPE 6.85%
3.01% 2.86% 2.73% 2.71% 2.91% 2.85% 2.76% 2.78% 2.86% 2.78% 2.67% 2.64%
Step 6
30 minMAE 3.05
1.97 1.70 1.65 1.63 1.67 1.72 1.65 1.64 1.70 1.64 1.60 1.57
RMSE 7.03
4.60 3.84 3.75 3.73 3.82 3.86 3.77 3.73 3.81 3.79 3.69 3.60
MAPE 6.84%
4.45% 3.79% 3.71% 3.73% 3.81% 3.88% 3.66% 3.73% 3.81% 3.71% 3.60% 3.52%
Step 12
60 minMAE 3.05
2.70 2.02 1.97 1.99 1.94 2.06 1.92 1.91 2.00 1.91 1.90 1.84
RMSE 7.01
6.28 4.63 4.60 4.60 4.50 4.60 4.45 4.42 4.52 4.43 4.49 4.32
MAPE 6.83%
6.72% 4.72% 4.68% 4.71% 4.55% 4.88% 4.46% 4.55% 4.63% 4.51% 4.53% 4.33%PEMS04A
verageMAE 42.35
25.55 19.57 19.63 18.53 19.38 20.96 18.96 18.38 19.06 18.36 18.72 18.14
RMSE 61.66
39.71 31.38 31.26 29.92 31.25 32.95 30.98 29.95 31.02 30.03 30.53 29.88
MAPE 29.92%
17.35% 13.44% 13.59% 12.89% 13.40% 14.66% 12.69% 12.04% 12.52% 12.00% 12.77% 12.00%PEMS07A
verageMAE 49.29
26.74 21.74 21.16 20.47 20.57 22.15 20.50 19.61 20.74 19.97 19.83 19.21
RMSE 71.34
42.78 35.27 34.14 33.47 34.40 35.10 34.66 32.79 34.05 32.95 32.91 32.75
MAPE 22.75%
11.58% 9.24% 9.02% 8.61% 8.74% 9.38% 8.75% 8.30% 8.77% 8.55% 8.36% 8.03%PEMS08A
verageMAE 34.66
19.36 16.08 15.22 14.40 15.32 16.49 15.41 14.21 15.41 13.58 14.75 13.57
RMSE 50.45
31.20 25.39 24.17 23.39 24.41 26.08 24.77 23.28 24.62 23.41 23.73 23.22
MAPE 21.63%
12.43% 10.60% 10.21% 9.21% 10.03% 10.54% 9.76% 9.27% 9.94% 9.05% 9.48% 8.98%
granularity of 5 minutes between consecutive timesteps. Therefore,
ğ‘ğ‘‘=288 andğ‘ğ‘¤=7 in these benchmarks. Additional details regard-
ing the five benchmarks can be found in Table 2. As part of data
preprocessing, we perform Z-score normalization on the raw inputs
to rescale the data to have zero mean and unit variance. To ensure a
fair comparison to previous methods, we adopt the commonly used
dataset divisions from prior works. For the METRLA and PEMSBAY
datasets, we use 70% of the data for training, 10% for validation, and
the remaining 20% for testing [ 36,60]. In the case of the PEMS04,
PEMS07, and PEMS08 datasets, we divide them into 60%, 20%, and
20%, respectively [ 19,38]. In this study, we ensured the ethical
useof these five publicly available datasets.
Settings. We performed hyper-parameter search on each dataset.
The encoder and decoder each contain one layer ğ¿=1 with a hid-
den dimension â„=64 for the first four datasets, and â„=96 for the
PEMS08 dataset. For the embeddings, the dimensions of the tempo-
ral embedding ğ‘‘ğ‘¡, the spatial embedding ğ‘‘ğ‘ , and the spatiotemporal
embedding ğ‘‘ğ‘ ğ‘¡are all set to 16 for the first four datasets. And for
PEMS08, the dimensions are 12, 14, and 10, respectively. The input
and prediction horizons are both set to 1 hour ( ğ‘‡=ğ‘‡â€²=12 timesteps).
We use the Adam optimizer with an initial learning rate of 0.001
and decay over training, and the batch size ğµis 16. The maximum
training epochs is 200, with an early stop patience set to 20. For
the loss function, we employ MAE loss for METRLA and PEMS-
BAY. For PEMS04, PEMS07, and PEMS08, we further employ Huberloss [ 22], a variant of MAE that is more robust. Model performance
is evaluated using MAE, Root Mean Square Error (RMSE), and Mean
Absolute Percentage Error (MAPE). All following experiments are
conducted on NVIDIA GeForce RTX 3090 GPUs.
Baselines. We compare our HimNet to several widely adopted
baselines. HI [ 9] is a standard statistical technique. GRU [ 7] is a
widely adopted univariate time series forecasting method. As for
spatiotemporal forecasting, we select several typical models includ-
ing STGCN [ 64], DCRNN [ 36], Graph WaveNet [ 60], AGCRN [ 2],
GTS [ 49], STNorm [ 11], STID [ 51], ST-WA [ 8], PDFormer [ 26] and
MegaCRN [ 28]. Among them, DCRNN, GTS, and MegaCRN also
apply GCRU-based architectures. AGCRN and ST-WA are meta-
parameter learning methods closely related to our approach.
5.2 Performance Evaluation
The comparison results for the performance of spatiotemporal
forecasting are given in Table 3. For the METRLA and PEMSBAY
datasets, we report performance on 3, 6, and 12 timesteps (15, 30,
and 60 minutes). Consistent with previous works, we report the av-
erage performance over all 12 predicted timesteps for the PEMS04,
PEMS07, and PEMS08 datasets. From the results, we obtain the
following observations: (1) Our proposed model, HimNet, signif-
icantly outperforms all baselines across all metrics and datasets,
demonstrating the effectiveness of our method. By learning meta-
parameters based on spatiotemporal heterogeneity, our approach is
 
636Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
better able to adapt to diverse spatiotemporal contexts for accurate
forecasting. (2) Compared with other GCRU-based models DCRNN,
GTS, and MegaCRN, HimNet achieves better performance, high-
lighting the superiority of our meta-parameter learning scheme.
(3) While AGCRN learns parameters only for each spatial node,
our method further considers the temporal dimension and utilizes
spatiotemporal heterogeneity. ST-WA learns specific parameters for
each location and timestep based only on the knowledge learned
from the current input. However, our method fully captures and
leverages the global spatiotemporal heterogeneity, providing im-
portant guidance for more precise local parameter learning. As a
result, our HimNet demonstrates better forecasting performance
than others.
5.3 Ablation Study
In this section, we conduct ablation studies to validate key com-
ponents of our proposed approach. We designed the following
variants: (1) w/o ğ¸ğ‘¡: replaces the temporal embedding with a static
matrix of all ones, thus removing temporal heterogeneity modeling.
(2) w/oğ¸ğ‘ : replaces the spatial embedding with a static matrix of
all ones, thus removing spatial heterogeneity modeling. (3) w/o ğ¸ğ‘ ğ‘¡:
replaces the spatiotemporal embedding with a static matrix of all
ones. (4) w/o TMP: removes temporal meta-parameters by down-
grading to randomly initialized parameters. (5) w/o SMP: removes
spatial meta-parameters in the same way. (6) w/o STMP: removes
ST-mixed meta-parameters in the same way. As shown in Table 4,
using identical queries decreases forecasting performance, demon-
strating the effectiveness of our heterogeneity-informed approach.
Moreover, removing meta-parameters leads to more substantial
performance degradation, highlighting the value of applying the
spatiotemporal meta-parameters modulated by our method. All
these validate that HimNet is complete and indivisible to achieve
superior spatiotemporal forecasting performance.
Table 4: Average MAE of the ablated variants of HimNet.
Model METRLA PEMSBAY PEMS04 PEMS07 PEMS08
w/oğ¸ğ‘¡ 2.94 1.53 18.35 22.00 14.14
w/oğ¸ğ‘  3.49 1.74 21.30 19.26 14.79
w/oğ¸ğ‘ ğ‘¡ 3.07 1.55 18.55 19.77 13.61
w/o TMP 2.94 1.54 18.65 19.58 14.44
w/o SMP 3.53 1.75 21.41 22.26 14.07
w/o STMP 3.01 1.57 18.65 19.86 13.72
HimNet 2.92 1.51 18.14 19.21 13.57
5.4 Hyper-Parameter Study
We conduct experiments on our modelâ€™s sensitivity to key hyper-
parameters, including the dimension of the temporal embedding ğ‘‘ğ‘¡,
the spatial embedding ğ‘‘ğ‘ , and the spatiotemporal embedding ğ‘‘ğ‘ ğ‘¡.
Figure 3 plots the average RMSE of predictions on the METRLA
dataset when varying each embedding dimension from 4 to 24. In-
terestingly, we find that our model is fairly robust to changes in
these hyper-parameters. Across the ranges tested, the RMSE differ-
ence between the best and worst-performing configurations is only
approximately 0.1. An embedding size of 16 generally provides goodresults with a tractable parameter size. Still, some trends emerge
where decreasing a dimension too much begins to exhibit signs of
under-fitting, while larger dimensions will substantially increase
parameter size but without notable improvement in performance.
Figure 3: RMSE w.r.t. embedding dimensions on METRLA.
5.5 Efficiency Study
We evaluate efficiency by comparing HimNet with 10 spatiotem-
poral baseline models. Table 5 reports the number of parameters,
per-batch runtime, per-epoch runtime, and GPU memory usage on
METRLA (batch size ğµ=16).
As shown in the table, models using TCNs, such as STGCN,
GWNet and STNorm, perform relatively well because of the high
computational efficiency of convolution. In comparison, the next
four models with GCRU architecture show a notable efficiency gap
due to the iterative nature of RNNs that intrinsically presents a
disadvantage. Additionally, Transformer-based models like ST-WA
and PDFormer are even worse due to their quadratic computational
complexity. Among these, STID performs best as its backbone is
simply linear layers. We also observe that HimNetâ€™s memory usage
is relatively high, because our design of the meta-parameter pools
will enlarge its parameter size. In summary, our HimNet achieves
decent efficiency compared with other GCRU-based models, and
outperforms the Transformer-based baselines.
We further analyze a variant HimNet- Î˜â€²that removes meta-
parameter pools and directly optimizes the three enlarged param-
eter spaces. HimNet- Î˜â€²contains over 10.9 billion parameters, un-
available on any GPUs we can get. This is consistent with our
Table 5: Efficiency comparison on METRLA dataset.
Mo
del #Params Time / Batch Time / Epoch Mem Usage
STID
[51] 118K 8ms 12s 1420MB
ST
GCN [64] 246K 23ms 34s 1650MB
GWNet [60] 309K 40ms 60s 1994MB
STNorm [11] 224K 39ms 59s 1818MB
DCRNN
[36] 372K 189ms 284s 2134MB
AGCRN [2] 752K 54ms 82s 2492MB
GTS [49] 38.5M 114ms 171s 4096MB
MegaCRN [28] 389K 89ms 134s 1962MB
ST
-WA [8] 375K 135ms 203s 2668MB
PDFormer [26] 531K 173ms 260s 6938MB
HimNet-Î˜â€²10.9B
N/A N/A N/A
HimNet 1251K 97ms 144s 6056MB
 
637KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zheng Dong et al.
Figure 4: t-SNE visualization of the temporal and spatial meta-parameters. (a): Visualization for each day in a week where
every cluster contains 24 points representing each hourâ€™s temporal meta-parameter. (b): The cosine similarity matrix of the
meta-parameters across 24 hours in a day. (c): Visualization for each sensorâ€™s spatial meta-parameter in METRLA dataset. (d):
Sensor points in (c) plotted on map, with only one lane of points shown for dual carriageways. (e): Clusters 3 and 13 are located
on opposite lanes of the same road. (e): The one-week time series of clusters 3, 10, and 13.
Figure 5: The evolving ST-mixed meta-parameters. (a): Dis-
tribution of clusters 3 and 6 around a crossroad area. (b): The
traffic speed time series of the two clusters at peak hour and
off-peak hour. (c): A closer view of 10 sensors at the crossroad.
(d): The cosine similarity of their ST-mixed meta-parameters
across three time periods.
analysis that directly optimizing the enlarged parameter spaces
leads to intractable memory costs, which validates the necessity
of our meta-parameter pool design to maintain efficiency while
enabling flexible parameterization.
5.6 Case Study
In this section, we analyze the learned meta-parameters through
visualizations to gain better interpretability and demonstrate how
they reflect meaningful circumstances in the real world.
Temporal Meta-Parameters. Figure 4 provides visualizations on
temporal and spatial meta-parameters based on t-SNE [ 54] dimen-
sionality reduction. We extract the temporal meta-parameters for
each hour across the days of a week. With one day sampled at an
hourly granularity, this results in a total of 168 data points embed-
ded in the 2D latent space. As shown in Figure 4(a), distinct tight
clusters form for each individual day, demonstrating our methodâ€™s
ability to differentiate days of the week. Notably, weekdays and
weekends are widely separated, underscoring our effectiveness in
capturing the temporal heterogeneity between these day types.
Figure 6: The evolving ST-mixed meta-parameters of clusters
0 and 8. As the time series diverge, they evolve accordingly.
Figure 4(b) shows the cosine similarity between the temporal meta-
parameters of each hourly interval within a day. Higher similarity is
observed between adjacent hours, decreasing with increased tempo-
ral distance as expected. Interestingly, the time periods when people
travel frequently exhibit high similarity. For example, the periods
between 6-9am and 4-7pm show increased similarity, reflecting
real-world commuting behaviors. These examples demonstrate that
our proposed method successfully identifies and distinguishes the
diverse temporal contexts associated with different time periods.
Spatial Meta-Parameters. For the spatial meta-parameters, we
analyze the METRLA dataset as a case study. It contains 207 sensor
locations whose spatial meta-parameters are embedded in 2D latent
space as shown in Figure 4(c). The 207 points naturally form into
15 clusters marked with different colors. To interpret these clus-
ters, we draw them onto a real map of Los Angeles in Figure 4(d).
Remarkably, each cluster precisely aligns with road segments, un-
derscoring our modelâ€™s strong power to capture real-world urban
structural characteristics, even without the help of graph topology.
Moreover, opposite lanes of the same road contain points from
different clusters. Figure 4(e) shows clusters 3 and 13 as an example.
For visual clarity, we manually select one lane per road to draw
Figure 4(d). This exact matching between learned clusters and real
road topology demonstrates our methodâ€™s ability to explicitly dis-
tinguish spatial heterogeneity. Furthermore, we analyze the time
 
638Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
series of clusters 3, 10, and 13 specifically. Cluster 10 corresponds to
a major downtown road prone to severe congestion. Clusters 3 and
13 are located on the opposite lanes of a minor road. As expected,
their traffic speed time series in Figure 4(f) show that clusters 3
and 13 are closely matched while significantly differing from 10,
consistent with our observations on the map and the latent space in
Figure 4(c). This validates that the spatial meta-parameters encode
meaningful contexts reflecting spatial heterogeneity.
ST-Mixed Meta-Parameters. Figure 5 provides a case study of
the evolving ST-mixed meta-parameters. Clusters 3 and 6 from
the crossroad area in Figure 5(a) are analyzed. Their traffic speed
time series are shown in Figure 5(b) with three periods highlighted:
midnight off-peak, morning peak, and noon off-peak. Ten nearby
sensors (5 per cluster) on the crossroad are visualized in Figure 5(c).
For their ST-mixed parameters, we draw cosine similarity heatmaps
across the three time periods in Figure 5(d). As expected, within-
cluster similarity is generally higher than cross-cluster similarity.
In detail, during midnight off-peak hours, both clusters exhibit sim-
ilar traffic speed patterns, reflected by high cross-cluster similarity.
However, during the morning peak, cluster 3 shows a sharp drop
in speed while cluster 6 maintains a higher level. Their time series
diverge, resulting in lower cross-cluster similarity. At noon, the
speed patterns in both clusters become closer, leading to an increase
in cross-cluster similarity again. Moreover, in Figure 6, we conduct
another case study on clusters 0 and 8. Figure 6(a) shows five sen-
sors each selected from the two clusters. As shown in Figure 6(b),
their time series diverge substantially over time. Correspondingly,
as illustrated in Figure 6(c), the cross-cluster similarity between
the ST-mixed meta-parameters associated with each sensor de-
creases accordingly over the three selected time periods. These
case studies intuitively explain how our ST-mixed meta-parameters
evolve spatially and temporally. Thus, the proposed approach has
strong adaptability to various spatiotemporal contexts, with meta-
parameters accurately capturing changes between different time
periods for each location. Furthermore, this also confirms the power
of our method to jointly model spatiotemporal heterogeneity.
6 CONCLUSION
In this study, we proposed a novel Heterogeneity-Informed
Meta-Parameter Learning scheme along with the state-of-the-art
Heterogeneity-Informed Spatiotemporal Meta-Network (HimNet)
for spatiotemporal forecasting. In detail, we captured spatiotem-
poral heterogeneity by learning spatial and temporal embeddings.
A novel meta-parameter learning paradigm was proposed to learn
spatiotemporal-specific parameters from meta-parameter pools.
Critically, our proposed approach can fully leverage the captured
spatiotemporal heterogeneity to inform meta-parameter learning.
Extensive experiments on five benchmarks demonstrated Him-
Netâ€™s superior performance. Further visualization analyses on meta-
parameters revealed its strong interpretability.
ACKNOWLEDGMENT
This work was partially supported by the grants of National Key
Research and Development Project (2021YFB1714400) of China,
Jilin Provincial International Cooperation Key Laboratory for Super
Smart City and Zhujiang Project (2019QN01S744).REFERENCES
[1]Rafal A Angryk, Petrus C Martens, Berkay Aydin, Dustin Kempton, Sushant S
Mahajan, Sunitha Basodi, Azim Ahmadzadeh, Xumin Cai, Soukaina Fi-
lali Boubrahimi, Shah Muhammad Hamdi, et al .2020. Multivariate time series
dataset for space weather data analytics. Scientific data 7, 1 (2020), 227.
[2]Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive graph
convolutional recurrent network for traffic forecasting. Advances in neural
information processing systems 33 (2020), 17804â€“17815.
[3]Luca Bertinetto, JoÃ£o F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi.
2016. Learning feed-forward one-shot learners. Advances in neural information
processing systems 29 (2016).
[4]Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang,
Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, et al .2020. Spectral temporal graph
neural network for multivariate time-series forecasting. Advances in neural
information processing systems 33 (2020), 17766â€“17778.
[5]Junkun Chen, Xipeng Qiu, Pengfei Liu, and Xuanjing Huang. 2018. Meta multi-
task learning for sequence modeling. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 32.
[6]Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong
Wen, Bin Yang, and Chenjuan Guo. 2024. Multi-scale Transformers with Adaptive
Pathways for Time Series Forecasting. In International Conference on Learning
Representations.
[7]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[8]Razvan-Gabriel Cirstea, Bin Yang, Chenjuan Guo, Tung Kieu, and Shirui Pan.
2022. Towards Spatio-Temporal Aware Traffic Time Series Forecastingâ€“Full
Version. arXiv preprint arXiv:2203.15737 (2022).
[9]Yue Cui, Jiandong Xie, and Kai Zheng. 2021. Historical inertia: A neglected but
powerful baseline for long sequence time-series forecasting. In Proceedings of
the 30th ACM International Conference on Information & Knowledge Management.
2965â€“2969.
[10] Gary A Davis and Nancy L Nihan. 1991. Nonparametric regression and short-
term freeway traffic forecasting. Journal of Transportation Engineering 117, 2
(1991), 178â€“188.
[11] Jinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. 2021. St-
norm: Spatial and temporal normalization for multi-variate time series forecasting.
InProceedings of the 27th ACM SIGKDD conference on knowledge discovery & data
mining. 269â€“278.
[12] Jinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. 2022. A
multi-view multi-task learning framework for multi-variate time series forecast-
ing. IEEE Transactions on Knowledge and Data Engineering (2022).
[13] Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor W
Tsang. 2024. Disentangling Structured Components: Towards Adaptive, Inter-
pretable and Scalable Time Series Forecasting. IEEE Transactions on Knowledge
and Data Engineering (2024).
[14] Jiewen Deng, Jinliang Deng, Renhe Jiang, and Xuan Song. 2023. Learning Gauss-
ian mixture representations for tensor time series forecasting. In Proceedings of
the Thirty-Second International Joint Conference on Artificial Intelligence.
[15] Jiewen Deng, Renhe Jiang, Jiaqi Zhang, and Xuan Song. 2024. Multi-Modality
Spatio-Temporal Forecasting via Self-Supervised Learning. arXiv preprint
arXiv:2405.03255 (2024).
[16] Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, Bingbing Xu, Liang Zeng,
and Chenxing Wang. 2023. When spatio-temporal meet wavelets: Disentangled
traffic forecasting via efficient spectral graph attention networks. In 2023 IEEE
39th International Conference on Data Engineering (ICDE). IEEE, 517â€“529.
[17] Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie. 2021. Spatial-
temporal graph ode networks for traffic flow forecasting. In Proceedings of the
27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 364â€“373.
[18] Haotian Gao, Renhe Jiang, Zheng Dong, Jinliang Deng, and Xuan Song. 2023.
Spatio-Temporal-Decoupled Masked Pre-training for Traffic Forecasting. arXiv
preprint arXiv:2312.00516 (2023).
[19] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.
Attention based spatial-temporal graph convolutional networks for traffic flow
forecasting. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.
922â€“929.
[20] David Ha, Andrew M Dai, and Quoc V Le. 2016. HyperNetworks. In International
Conference on Learning Representations.
[21] Liangzhe Han, Bowen Du, Leilei Sun, Yanjie Fu, Yisheng Lv, and Hui Xiong. 2021.
Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic Speed
Forecasting. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 547â€“555.
[22] Peter J Huber. 1964. Robust Estimation of a Location Parameter. The Annals of
Mathematical Statistics 35, 1 (1964), 73â€“101.
[23] Lee Hyunwook and Ko Sungahn. 2024. TESTAM: A Time-Enhanced Spatio-
Temporal Attention Model with Mixture of Experts. In The Twelfth International
Conference on Learning Representations.
 
639KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Zheng Dong et al.
[24] Jiahao Ji, Jingyuan Wang, Chao Huang, Junjie Wu, Boren Xu, Zhenhe Wu, Junbo
Zhang, and Yu Zheng. 2023. Spatio-temporal self-supervised learning for traffic
flow prediction. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 37. 4356â€“4364.
[25] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. 2016. Dynamic
filter networks. Advances in neural information processing systems 29 (2016).
[26] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.
PDFormer: Propagation Delay-aware Dynamic Long-range Transformer for Traf-
fic Flow Prediction. In AAAI. AAAI Press.
[27] Renhe Jiang, Zhaonan Wang, Yudong Tao, Chuang Yang, Xuan Song, Ryosuke
Shibasaki, Shu-Ching Chen, and Mei-Ling Shyu. 2023. Learning Social Meta-
knowledge for Nowcasting Human Mobility in Disaster. In Proceedings of the
ACM Web Conference 2023. 2655â€“2665.
[28] Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Ya-
sumasa Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura.
2023. Spatio-temporal meta-graph learning for traffic forecasting. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 37. 8078â€“8086.
[29] Renhe Jiang, Du Yin, Zhaonan Wang, Yizhuo Wang, Jiewen Deng, Hangchen Liu,
Zekun Cai, Jinliang Deng, Xuan Song, and Ryosuke Shibasaki. 2021. Dl-traff:
Survey and benchmark of deep learning models for urban traffic prediction. In
Proceedings of the 30th ACM international conference on information & knowledge
management. 4515â€“4525.
[30] Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geof-
frey I Webb, Irwin King, and Shirui Pan. 2023. A survey on graph neural networks
for time series: Forecasting, classification, imputation, and anomaly detection.
arXiv preprint arXiv:2307.03759 (2023).
[31] Yilun Jin, Kai Chen, and Qiang Yang. 2023. Transferable Graph Structure Learning
for Graph-based Traffic Forecasting Across Cities. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 1032â€“1043.
[32] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling
long-and short-term temporal patterns with deep neural networks. In The 41st
international ACM SIGIR conference on research & development in information
retrieval. 95â€“104.
[33] Shiyong Lan, Yitong Ma, Weikang Huang, Wenwu Wang, Hongyu Yang, and
Pyang Li. 2022. Dstagnn: Dynamic spatial-temporal aware graph neural network
for traffic flow forecasting. In International conference on machine learning. PMLR,
11906â€“11917.
[34] Hyunwook Lee, Seungmin Jin, Hyeshin Chu, Hongkyu Lim, and Sungahn Ko.
2022. Learning to Remember Patterns: Pattern Matching Memory Networks for
Traffic Forecasting. In International Conference on Learning Representations.
[35] Mengzhang Li and Zhanxing Zhu. 2021. Spatial-Temporal Fusion Graph Neural
Networks for Traffic Flow Forecasting. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 35. 4189â€“4196.
[36] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion Convolutional
Recurrent Neural Network: Data-Driven Traffic Forecasting. In International
Conference on Learning Representations.
[37] Fan Liu, Weijia Zhang, and Hao Liu. 2023. Robust Spatiotemporal Traffic Fore-
casting with Reinforced Dynamic Adversarial Training. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
[38] Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quan-
jun Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makes
vanilla transformer sota for traffic forecasting. In Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management. 4125â€“4129.
[39] Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, Chao Huang,
Zhenguang Liu, Bryan Hooi, and Roger Zimmermann. 2023. LargeST: A Bench-
mark Dataset for Large-Scale Traffic Forecasting. arXiv preprint arXiv:2306.08259
(2023).
[40] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and
Mingsheng Long. 2023. iTransformer: Inverted Transformers Are Effective for
Time Series Forecasting. arXiv preprint arXiv:2310.06625 (2023).
[41] Bin Lu, Xiaoying Gan, Weinan Zhang, Huaxiu Yao, Luoyi Fu, and Xinbing Wang.
2022. Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge
Transfer. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining. 1162â€“1172.
[42] Zhongjian Lv, Jiajie Xu, Kai Zheng, Hongzhi Yin, Pengpeng Zhao, and Xiaofang
Zhou. 2018. Lc-rnn: A deep learning model for traffic speed prediction.. In IJCAI,
Vol. 2018. 27th.
[43] Xiaolei Ma, Zhimin Tao, Yinhai Wang, Haiyang Yu, and Yunpeng Wang. 2015.
Long short-term memory neural network for traffic speed prediction using remote
microwave sensor data. Transportation Research Part C: Emerging Technologies
54 (2015).
[44] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499
(2016).
[45] Bei Pan, Ugur Demiryurek, and Cyrus Shahabi. 2012. Utilizing real-world trans-
portation data for accurate traffic prediction. In 2012 ieee 12th international
conference on data mining. IEEE, 595â€“604.[46] Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang.
2019. Urban traffic prediction from spatio-temporal data using deep meta learning.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 1720â€“1730.
[47] Zheyi Pan, Wentao Zhang, Yuxuan Liang, Weinan Zhang, Yong Yu, Junbo Zhang,
and Yu Zheng. 2020. Spatio-temporal meta learning for urban traffic prediction.
IEEE Transactions on Knowledge and Data Engineering 34, 3 (2020), 1462â€“1476.
[48] JÃ¼rgen Schmidhuber. 1992. Learning to control fast-weight memories: An alter-
native to dynamic recurrent networks. Neural Computation 4, 1 (1992), 131â€“139.
[49] Chao Shang, Jie Chen, and Jinbo Bi. 2021. Discrete Graph Structure Learning
for Forecasting Multiple Time Series. In International Conference on Learning
Representations.
[50] Zezhi Shao, Fei Wang, Yongjun Xu, Wei Wei, Chengqing Yu, Zhao Zhang, Di Yao,
Guangyin Jin, Xin Cao, Gao Cong, et al .2023. Exploring Progress in Multivari-
ate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity
Analysis. arXiv preprint arXiv:2310.06119 (2023).
[51] Zezhi Shao, Zhao Zhang, Fei Wang, Wei Wei, and Yongjun Xu. 2022. Spatial-
temporal identity: A simple yet effective baseline for multivariate time series
forecasting. In Proceedings of the 31st ACM International Conference on Information
& Knowledge Management. 4454â€“4458.
[52] Chao Song, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2020. Spatial-
temporal synchronous graph convolutional networks: A new framework for
spatial-temporal network data forecasting. In Proceedings of the AAAI conference
on artificial intelligence, Vol. 34. 914â€“921.
[53] James H Stock and Mark W Watson. 2001. Vector autoregressions. Journal of
Economic perspectives 15, 4 (2001), 101â€“115.
[54] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[55] Lelitha Vanajakshi and Laurence R Rilett. 2004. A comparison of the performance
of artificial neural networks and support vector machines for the prediction of
traffic speed. In IEEE Intelligent Vehicles Symposium, 2004. IEEE, 194â€“199.
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[57] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,
and Liang Sun. 2023. Transformers in time series: A survey. In International Joint
Conference on Artificial Intelligence(IJCAI).
[58] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-
composition transformers with auto-correlation for long-term series forecasting.
Advances in Neural Information Processing Systems 34 (2021), 22419â€“22430.
[59] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi
Zhang. 2020. Connecting the dots: Multivariate time series forecasting with graph
neural networks. In Proceedings of the 26th ACM SIGKDD international conference
on knowledge discovery & data mining. 753â€“763.
[60] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.
Graph wavenet for deep spatial-temporal graph modeling. In Proceedings of the
28th International Joint Conference on Artificial Intelligence. 1907â€“1913.
[61] Yutong Xia, Yuxuan Liang, Haomin Wen, Xu Liu, Kun Wang, Zhengyang Zhou,
and Roger Zimmermann. 2023. Deciphering Spatio-Temporal Graph Forecasting:
A Causal Lens and Treatment. In Thirty-seventh Conference on Neural Information
Processing Systems.
[62] Congxi Xiao, Jingbo Zhou, Jizhou Huang, Tong Xu, and Hui Xiong. 2023. Spatial
Heterophily Aware Graph Neural Networks. arXiv preprint arXiv:2306.12139
(2023).
[63] Junchen Ye, Leilei Sun, Bowen Du, Yanjie Fu, and Hui Xiong. 2021. Coupled Layer-
wise Graph Convolution for Transportation Demand Prediction. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 35. 4617â€“4625.
[64] Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-temporal graph convolu-
tional networks: a deep learning framework for traffic forecasting. In Proceedings
of the 27th International Joint Conference on Artificial Intelligence. 3634â€“3640.
[65] Yuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, and Yong Li. 2023. Spatio-
temporal Diffusion Point Processes. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining.
[66] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: A
graph multi-attention network for traffic prediction. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 34. 1234â€“1241.
[67] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-
quence time-series forecasting. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 35. 11106â€“11115.
[68] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
2022. Fedformer: Frequency enhanced decomposed transformer for long-term
series forecasting. In International Conference on Machine Learning. PMLR, 27268â€“
27286.
[69] Zhengyang Zhou, Qihe Huang, Gengyu Lin, Kuo Yang, Lei Bai, and Yang Wang.
2022. Greto: remedying dynamic graph topology-task discordance via target
homophily. In The eleventh international conference on learning representations.
 
640Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[70] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Chaochao Zhu.
2020. Foresee urban sparse traffic accidents: A spatiotemporal multi-granularity
perspective. IEEE Transactions on Knowledge and Data Engineering (2020).
[71] Zhengyang Zhou, Kuo Yang, Yuxuan Liang, Binwu Wang, Hongyang Chen,
and Yang Wang. 2023. Predicting collective human mobility via countering
spatiotemporal heterogeneity. IEEE Transactions on Mobile Computing (2023).
A APPENDIX
A.1 Notation Table
For reference, Table 6 summarizes the key notations and their de-
scriptions in this paper.
Table 6: Notations and their descriptions.
Notation Description
ğµ Batch size.
ğ‘ Number of spatial locations.
ğ‘‡,T Number of time steps.
ğ‘‹,ğ‘‹ğ‘/Ë†ğ‘‹ Input/output time series.
ğ¸ğ‘¡,ğ¸ğ‘ ,ğ¸ğ‘ ğ‘¡ Embedding matrices.
ğ‘‘ğ‘¡ğ‘œğ‘‘,ğ‘‘ğ‘‘ğ‘œğ‘¤,ğ‘‘ğ‘¡,ğ‘‘ğ‘ ,ğ‘‘ğ‘ ğ‘¡ Embedding dimensions.
Î˜,Î˜â€²,Î˜ğ‘¡,Î˜ğ‘ ,Î˜ğ‘ ğ‘¡ Parameter spaces.
ğ‘ƒ,ğ‘ƒğ‘¡,ğ‘ƒğ‘ ,ğ‘ƒğ‘ ğ‘¡ Meta-parameter pools.
ğœ—,ğœ—ğ‘¡,ğœ—ğ‘ ,ğœ—ğ‘ ğ‘¡ Meta-parameters.
ğ‘† Parameter size.
ğ»,â„ GCRU hidden encoding and its size.
Ëœğ´,Ëœğ´ğ‘ ğ‘¡ Adaptive graph adjacency matrices.
A.2 Training Settings
For reproducibility, we provide details of our training settings in
Table 7. Different values used for METRLA, PEMSBAY, PEMS04,
PEMS07, and PEMS08 are separated by slashes. This could also be
found in the configuration file of our public code.Table 7: Detailed training settings.
Configuration METRLA/PEMSBAY/PEMS04/PEMS07/PEMS08
Batch size (ğµ) 16
Optimizer Adam
Weight decay 0.0005/0.0001/0.0001/0.0001/0
Epsilon 0.001
Learning rate (LR) 0.001
Scheduler milestones [30, 40]/[25, 35]/[30, 50]/[40, 60]/[40, 60, 80]
Scheduler LR decay 0.1
Early stop patience 20
Max number of epochs 200
Gradient clip 5
A.3 Efficiency Report
All the experiments are performed on an Intel(R) Xeon(R) Silver
4310 CPU @ 2.10GHz, 256G RAM computing server, equipped with
NVIDIA GeForce RTX 3090 graphics cards. We report the total
training time and memory cost in Figure 7. Even for the largest
dataset PEMS07 (883 sensors), our memory usage is still under
24GB, allowing running all benchmarks on a single RTX 3090 GPU.
Figure 7: Summary of training time and memory cost.
 
641