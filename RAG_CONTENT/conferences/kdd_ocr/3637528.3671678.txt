Privacy-Preserved Neural Graph Databases
Qi Hu
Department of CSE, Hong Kong
University of Science and Technology
Hong Kong, China
qhuaf@connect.ust.hkHaoran Li
Department of CSE, Hong Kong
University of Science and Technology
Hong Kong, China
hlibt@connect.ust.hkJiaxin Bai
Department of CSE, Hong Kong
University of Science and Technology
Hong Kong, China
jbai@connect.ust.hk
Zihao Wang
Department of CSE, Hong Kong
University of Science and Technology
Hong Kong, China
zwanggc@cse.ust.hkYangqiu Song
Department of CSE, Hong Kong
University of Science and Technology
Hong Kong, China
yqsong@cse.ust.hk
Abstract
In the era of large language models (LLMs), efficient and accurate
data retrieval has become increasingly crucial for the use of domain-
specific or private data in the retrieval augmented generation (RAG).
Neural graph databases (NGDBs) have emerged as a powerful para-
digm that combines the strengths of graph databases (GDBs) and
neural networks to enable efficient storage, retrieval, and analysis of
graph-structured data which can be adaptively trained with LLMs.
The usage of neural embedding storage and Complex neural logical
Query Answering (CQA) provides NGDBs with generalization abil-
ity. When the graph is incomplete, by extracting latent patterns and
representations, neural graph databases can fill gaps in the graph
structure, revealing hidden relationships and enabling accurate
query answering. Nevertheless, this capability comes with inherent
trade-offs, as it introduces additional privacy risks to the domain-
specific or private databases. Malicious attackers can infer more
sensitive information in the database using well-designed queries
such as from the answer sets of where Turing Award winners born
before 1950 and after 1940 lived, the living places of Turing Award
winner Hinton are probably exposed, although the living places
may have been deleted in the training stage due to the privacy con-
cerns. In this work, we propose a privacy-preserved neural graph
database (P-NGDB) framework to alleviate the risks of privacy leak-
age in NGDBs. We introduce adversarial training techniques in the
training stage to enforce the NGDBs to generate indistinguishable
answers when queried with private information, enhancing the
difficulty of inferring sensitive information through combinations
of multiple innocuous queries. Extensive experimental results on
three datasets show that our framework can effectively protect
private information in the graph database while delivering high-
quality public answers responses to queries. The code is available
at https://github.com/HKUST-KnowComp/PrivateNGDB.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671678CCS Concepts
â€¢Security and privacy â†’Privacy protections; â€¢Information
systemsâ†’Data mining.
Keywords
Privacy preserving, Knowledge graphs (KGs), Complex query an-
swering (CQA), Neural graph databases (NGDBs)
ACM Reference Format:
Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, and Yangqiu Song. 2024. Privacy-
Preserved Neural Graph Databases. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671678
1 Introduction
Graph DataBases (GDBs) play a crucial role in storing, organiz-
ing, and retrieving structured relational data and support many
traditional data-intensive applications, such as recommender sys-
tems [ 15], fraud detection [ 47], etc. In the era of Large Language
Models (LLMs), the role of GDBs gets increasingly important be-
cause of the Retrieval Augmented Generation (RAG) paradigm,123
where the LLM agents can be significantly enhanced by external
GDBs such as Knowledge Graphs (KGs) [ 18,20,43]. Such interaction
of LLMs and GDBs enables new possibilities by creating interactive
natural language interfaces over structural data for domain-specific
applications.
The RAG paradigm of LLMs also highlights new challenges of
traditional GDB. The natural language interaction enlarges the pos-
sible ways of utilizing the knowledge managed in GDBs, requiring
simultaneous knowledge discovery and query answering. Knowl-
edge discovery is particularly important for domain-specific data
and knowledge graphs, as the graph database may not capture all
the necessary relationships and connections between entities by
traversing [9, 19].
NGDBs have been proposed to address these challenges by ex-
tending the concept of graph DBs [ 6,49]. They combine the flex-
ibility of graph data models with the computational capabilities
of neural networks, enabling effective and efficient representation,
1https://openai.com/research/emergent-tool-use
2https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html
3https://www.langchain.com/use-case/agents
 
1108
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, & Yangqiu Song
Bengio LeCunKnuth
196419471938UofTToronto
Stanford
MontrealNew YorkHintonTuring Award
queryQuery
Find where the Turing Award winner who was born in 1938 lived. Interpretation
Complex Queries
Answer Query
Privacy risk query 
detection
Montreal, Toronto â€¦
Montreal, Toronto â€¦
Toronto , Stanfordâ€¦
Privacy Risk QueriesPrivate edgePublic edge
Neural Graph DatabasesEmbedding 
Storage
Query 
EngineEmbedding
Figure 1: Privacy risks of NGDBs facing malicious queries. To illustrate the issue, consider an example where an attacker
attempts to infer private information about Hintonâ€™s living place in the NGDBs. Direct querying private information can be
easily detected by privacy risk detection, however, attackers can leverage well-designed queries to retrieve desired privacy.
In this example, the privacy attacker can query â€œwhere Turing Award winners born after 1940 lived,â€ â€œwhere ones before
1950 lived,â€ and â€œwhere LeCunâ€™s collaborator lived,â€ etc. And the query can be more complex as shown in privacy risk queries.
The return answer denoted in red may leak private information in NGDBs. The intersection of these queries still presents a
significant likelihood of exposing the living place of Turing Award winner Hinton.
storage, and analysis of interconnected data. As shown in Figure 1,
NGDBs are designed with two modules: neural graph storage pro-
viding unified storage for diverse entries in an embedding space and
neural query engine searching answers to input complex queries
from the unified storage [ 49]. By utilizing graph-based neural net-
work techniques, these databases empower expressive and intelli-
gent querying, inference, and knowledge discovery. This capability,
known as complex query answering (CQA) [ 1], aims to identify
answers that satisfy given logical expressions [ 23]. Query encod-
ing methods are commonly used in CQA. These methods involve
parameterizing entities, relations, and logical operators, encoding
both queries and entities into the same embedding space simultane-
ously, and retrieving answers according to the similarity between
the query (or set) embeddings and answer embeddings. For exam-
ple, GQE [ 23], Q2B [ 50], and Q2P [ 3] encode queries as vectors, a
hyper-rectangle, and a set of particles, respectively. Ideally, CQA
can be jointly trained with RAG so that the language model agents
can better adapt to domain-specific data [22, 34].
While NGDBs and their CQA ability have demonstrated signifi-
cant achievements in various domains, they do face unique privacy
challenges in comparison to traditional graph DBs [ 67,68]. One
notable risk arises from their generalization ability. Although gen-
eralization efficiently handles incomplete knowledge and enriches
the retrieved information, it also enables attackers to infer sensitive
information from NGDBs by leveraging the composition of multi-
ple complex queries. Figure 1 demonstrates an example of privacy
attacks via malicious complex queries: the living place information
of an individual. In this case, Hinton as an example, is considered to
be private. Even though Hintonâ€™s residence is omitted during the
construction of the knowledge graph, or if direct privacy queries are
restricted, a malicious attacker can still infer this sensitive informa-
tion without directly querying Hintonâ€™s living place but conductingwell-designed related queries. The answers denoted in red may leak
sensitive information by accident.
Graph privacy has consistently been a significant concern in the
field of graph research. Multiple studies demonstrate the vulnerabil-
ity of graph neural networks to various privacy leakage issues. For
instance, graph embeddings are vulnerable to attribute inference
attacks [ 16,21,70] and link prediction attacks [ 26,63], and various
protection methods are proposed [ 28,29,58]. However, these works
focus on graph representation learning and have the assumption
that the graph embeddings can be reached by the attackers which
are not typical scenarios in NGDBs. NGDBs often provide CQA
service and do not publish the learned embeddings, and attackers
can only infer private information using thoughtfully designed
queries. Furthermore, these works solely study node classification
and link prediction problems, leaving the query answering privacy
in NGDBs unexplored, resulting in the potential privacy leakage
problem in LLMs [35].
In this paper, we expose the potential privacy risks of NGDBs
with formal definition and evaluation. We introduce privacy pro-
tection objectives to the existing NGDBs, categorizing the answers
of knowledge graph complex queries into private and public do-
mains. To safeguard private information, NGDBs should strive to
maintain high-quality retrieval of non-private answers for queries,
while intentionally obfuscating the private-threatening answers to
innocuous yet private queries. As shown in Figure 1, the answer
sets of queries proposed by privacy attackers are obfuscated, and
attackers will face greater challenges in predicting private infor-
mation, thereby effectively safeguarding the privacy of NGDBs.
Meanwhile, we also create the corresponding benchmark on three
datasets (Freebase [ 8], YAGO [ 55], and DBpedia [ 7]) to evaluate the
performance of CQA on public query answers and the protection
of private information.
 
1109Privacy-Preserved Neural Graph Databases KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
To alleviate the privacy leakage problem in NGDBs, we pro-
pose Privacy-preserved Neural Graph Databases (P-NGDBs) as a
solution. P-NGDBs divide the information in graph databases into
private and public parts and categorize complex queriesâ€™ answers to
various privacy risks based on whether or not they involve sensitive
information. P-NGDBs can provide answers with different levels
of precision in response to queries with varying privacy risks. We
introduce adversarial techniques in the training stage of P-NGDBs
to generate indistinguishable answers when queried with private
information, enhancing the difficulty of inferring privacy through
complex private queries. We summarize our major contributions as
follows:
â€¢To the best of our knowledge, our work represents the pi-
oneering effort in investigating privacy leakage issues of
complex queries in NGDBs and providing formal definitions
for privacy protection in this domain.
â€¢Based on three public datasets, we propose a benchmark
for the simultaneous evaluation of CQA ability and privacy
protection for NGDBs.
â€¢We introduce P-NGDB, a privacy protection framework for
safeguarding privacy in NGDBs. Extensive experiments con-
ducted on three datasets demonstrate its effectiveness in
preserving retrieval performance and protecting privacy.
The rest of the paper is organized as follows. Section 2 systemati-
cally reviews the related work. Section 3 introduces the preliminary
and definition of the privacy problem in NGDBs. Section 4 and 5
introduce the innocuous yet private queries and the framework
of P-NGDBs in detail, respectively. Section 6 evaluates the perfor-
mance of P-NGDBs on the benchmark based on three real-world
datasets. Finally, we conclude our work in Section 7.
2 Related Work
2.1 Complex Query Answering
One essential function of neural graph databases is to answer com-
plex structured logical queries according to the data stored in the
NGDBs, namely complex query answering (CQA) [ 49,62]. Query
encoding is one of the prevalent methods for complex query an-
swering because of its effectiveness and efficiency. These methods
for query embedding utilize diverse structures to represent com-
plex logical queries and effectively model queries across various
scopes. Some methods encode queries as various geometric struc-
tures: such as vectors [ 3,23], boxes [ 42,50], cones [ 71] and cones
in hyperbolic spaces [ 40]. Other methods leverage the probability
distributions such as Beta [ 51], Gaussian [ 12], and Gamma [ 65]
distributions to encode logic knowledge graph queries. Meanwhile,
some approaches motivate the embedding space from the fuzzy
logic theory [ 11,59]. Advanced neural structures are also utilized
to encode complex queries: such as transformers and sequential en-
coders [ 4,33], graph neural networks [ 73]. Pretrained shallow neu-
ral link predictors are also adapted into search algorithms [ 1,5,66],
and learning-to-search frameworks [ 60,64]. Notably, NRN [ 2] is
proposed to handle CQA with numerical values, which is of partic-
ular interest in privacy protection.
While complex query answering has been widely studied in past
works, the privacy problem in NGDBs is overlooked. With the devel-
opment of the representation ability in NGDBs, the issue of privacyleakage has become increasingly critical. There are some works
showing that graph representation learning is vulnerable to vari-
ous privacy attacks [ 28]. [45,53] show that membership inference
attacks can be applied to identify the existence of training exam-
ples. Model extraction attacks [ 56] and link stealing attacks [ 26,69]
try to infer information about the graph representation model and
original graph link, respectively. However, these works assume
that attackers have complete access to the graph representation
models. In NGDBs, however, sensitive information can be leaked
during the query stage, presenting a different privacy challenge.
Our research explores the issue of privacy leakage in NGDBs and
presents a benchmark for comprehensive evaluation.
2.2 Graph Privacy
Various privacy protection methods have been proposed to pre-
serve privacy in graph representation models. Anonymization tech-
niques [ 27,41,52,72] are applied in graphs to reduce the proba-
bility of individual and link privacy leakage. Graph summariza-
tion [ 24] aims to publish a set of anonymous graphs with pri-
vate information removed. Learning-based methods are proposed
with the development of graph representation learning, [ 37,38,58]
regard graph representation learning as the combination of two
sub-tasks: primary objective learning and privacy preservation,
and use adversarial training to remove the sensitive information
while maintaining performance in the original tasks. Meanwhile,
some methods [ 29,39,44] disentangle the sensitive information
from the primary learning objectives. Additionally, differential
privacy [ 14,31,54] introduces noise into representation models
and provides privacy guarantees for the individual privacy of the
datasets [ 13]. Though noise introduced by differential privacy pro-
tects models from privacy leakage, it can also impact the perfor-
mance of the original tasks significantly. To empirically evaluate
the privacy leakage problem in representations, some works are
proposed to construct benchmarks [ 36]. Federated learning utilizes
differential privacy and encryption to prevent the transmission of
participantsâ€™ raw data and is widely applied in distributed graph
representation learning [ 25,30,46]. However, federated learning
cannot be applied to protect intrinsic private information in the
representation models.
While there are various graph privacy preservation methods,
they only focus on simple node-level or edge-level privacy pro-
tection. However, the privacy risks associated with neural graph
databases during complex query answering tasks have not received
sufficient research attention. The development of CQA models in-
troduces additional risks of privacy leakage in NGDBs that attackers
can infer sensitive information with multiple compositional queries.
Our proposed P-NGDB can effectively reduce the privacy leakage
risks of malicious queries.
3 Preliminary and Problem Formulation
3.1 Preliminary
We denote a knowledge graph as G=(V,R,A), whereVdenotes
the set of vertices representing entities and attribute values in the
knowledge graph.Rdenotes the set of relations in the knowledge
graph.Adenotes the set of attributes that can be split into private
attributes and public attributes: A=AprivateâˆªA public , where
 
1110KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, & Yangqiu Song
ğ‘(ğ‘¢,ğ‘¥)âˆˆA is a triplet where ğ‘¢,ğ‘¥âˆˆV, describing the attribute
of entities.ğ‘(ğ‘¢,ğ‘¥)denotes that the entity ğ‘¢has the attribute ğ‘of
valueğ‘¥âˆˆV. To prevent privacy leakage, the private attribute of
entities, we denote the Aprivate cannot be exposed and should be
protected from inferences.
The complex query is the key task of NGDBs and can be defined
in existential positive first-order logic form, consisting of various
types of logic expressions like existential quantifiers âˆƒ, logic con-
junctionsâˆ§, and disjunctionsâˆ¨. In the logical expression, there is
a unique variable ğ‘‰?in each logic query that denotes our query
target. Variables ğ‘‰1,Â·Â·Â·,ğ‘‰ğ‘˜denote existentially quantified entities
in a complex query. Besides, there are anchor entities ğ‘‰ğ‘and values
ğ‘‹ğ‘with the given content in a query. The complex query aims to
identify the target entity ğ‘‰?, such that there are ğ‘‰1,Â·Â·Â·,ğ‘‰ğ‘˜âˆˆV in
the knowledge graph that can satisfy the given logical expressions.
We denote complex query expression in the disjunctive normal
form (DNF) in the following:
ğ‘[ğ‘‰?]=ğ‘‰?.ğ‘‰1,...,ğ‘‰ğ‘˜:ğ‘1âˆ¨ğ‘2âˆ¨...âˆ¨ğ‘ğ‘›
ğ‘ğ‘–=ğ‘’ğ‘–,1âˆ§ğ‘’ğ‘–,2âˆ§...âˆ§ğ‘’ğ‘–,ğ‘š,(1)
whereğ‘’ğ‘–,ğ‘—is the atomic logic expression, which can be ğ‘Ÿ(ğ‘‰,ğ‘‰â€²)
denotes relation ğ‘Ÿbetween entities ğ‘‰andğ‘‰â€², andğ‘(ğ‘‰,ğ‘‰â€²)denotes
attributeğ‘of entitiesğ‘‰with value ğ‘‰â€².ğ‘ğ‘–is the conjunction of
several atomic logic expressions ğ‘’ğ‘–,ğ‘—.ğ‘‰,ğ‘‰â€²are either anchor entities
or attributes, or existentially quantified variables.
3.2 Problem Formulation
Suppose that there is a graph G=(V,R,A), where part of enti-
tiesâ€™ attributes are regarded as private information A=Aprivateâˆª
Apublic , whereğ‘ğ‘(ğ‘¢,ğ‘¥)âˆˆA private denotes that the entity ğ‘¢has
the sensitive attribute ğ‘ğ‘with value ğ‘¥. Due to the variation in
privacy requirements, an attribute can be handled differently for
various entities. NGDBs store the graph in embedding space and
can be queried with arbitrary complex logical queries. Due to the
generalization ability of NGDBs, attackers can easily infer sensi-
tive attributesAprivate utilizing complex queries. To protect pri-
vate information, NGDBs should retrieve obfuscated answers when
queried with privacy risks while preserving high accuracy in query-
ing public information.
4 Innocuous yet Private Query
As there is sensitive information in the neural graph databases,
some specified complex queries can be utilized by attackers to infer
privacy, which is defined as Innocuous yet Private (IP) queries. Some
answers to these queries can only be inferred under the involvement
of private information, which is a risk to privacy and should not be
retrieved by the NGDBs.
4.1 Computational Graph
The queryğ‘can be parsed into a computational graph as a directed
acyclic graph, which consists of various types of directed edges
that represent operators over sets of entities and attributes.
â€¢Projection : Projection has various types for attribute- and
value-sensitive queries [ 2]. Given a set of entities ğ‘†, a relation
between entities ğ‘ŸâˆˆR, an attribute type ğ‘, a set of values
Turing AwardLeCunCollab
WinLiveIn
KnuthLeCun
BengioHinton
MontrealToronto(A)
(B)Figure 2: An example of query demonstrating the retrieved
privacy-threatening query answers. The orange node denotes
privacy risks. (A) The logic knowledge graph query involves
privacy information. (B) An example of a complex query
in the knowledge graph. Toronto is regarded as a privacy-
threatening answer as it has to be inferred by sensitive infor-
mation.
ğ‘†âŠ‚V , projection describes the entities or values can be
achieved under specified relation and attribute types. For
example, attribute projection is denoted as ğ‘ƒğ‘(ğ‘†)={ğ‘¥âˆˆ
V|ğ‘£âˆˆğ‘†,ğ‘(ğ‘£,ğ‘¥) âˆˆ A} describing the values that can be
achieved under the attribute type ğ‘by any of the entities in
ğ‘†; Furthermore, we have relational projection ğ‘ƒğ‘Ÿ(ğ‘†),reverse
attribute projection ğ‘ƒâˆ’1ğ‘(ğ‘†). We denote all these projections
asğ‘ƒ(ğ‘†);
â€¢Intersection : Given a set of entity or value sets ğ‘†1,ğ‘†2,...,ğ‘†ğ‘›,
this logic operator computes the intersection of those sets
asâˆ©ğ‘›
ğ‘–=1ğ‘†ğ‘–;
â€¢Union : Given a set of entity or value sets ğ‘†1,ğ‘†2,...,ğ‘†ğ‘›, the
operator computes the union of âˆªğ‘›
ğ‘–=1ğ‘†ğ‘–.
4.2 Privacy Threatening Query Answers
After a complex query is parsed into the computational graph ğº,
andğ‘€=ğº(ğ‘†)denotes the answer set of entities or values retrieved
by the computational graph. We divide the ğ‘€retrieved by the
complex queries as private answer set ğ‘€private and non-private
answer setğ‘€public based on whether the answer has to be inferred
under the involvement of private information. As shown in Figure 2,
query in (A) retrieves answers from the knowledge graph, Toronto
is regarded as a privacy-threatening answer because the answer
Toronto denotes the private information ğ¿ğ‘–ğ‘£ğ‘’ğ¼ğ‘›(ğ»ğ‘–ğ‘›ğ‘¡ğ‘œğ‘›,ğ‘‡ğ‘œğ‘Ÿğ‘œğ‘›ğ‘¡ğ‘œ).
To analyze the risks, we define private answer sets for different
operators as follows. We denote the input set of operators as ğ‘†=
ğ‘†privateâˆªğ‘†public , whereğ‘†private ,ğ‘†public denote whether the elements
in the set involve private information.
Projection : A projection operatorâ€™s output will be regarded as
private if the operation is applied to infer private attributes. Assume
thatğ‘€=ğ‘ƒ(ğ‘†)and inputğ‘†=ğ‘†privateâˆªğ‘†public .ğ‘ƒ(ğ‘†)=ğ‘ƒ(ğ‘†private)âˆª
ğ‘ƒ(ğ‘†public), we useğ‘denote the corresponding attribute of projection
ğ‘ƒ. Without loss of generalization, we discuss the formal definition
of projection private answer set ğ‘€private in two scenarios. For the
 
1111Privacy-Preserved Neural Graph Databases KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(B) Intersection (C) Union (A) Projectionprojection
projection
Figure 3: Example of privacy-threatening answer sets computation in projection, intersection, and union. Green nodes denote
non-private answers, orange nodes denote privacy-threatening answers, and green-orange nodes denote different privacy
risks in subsets. Red dashed arrows denote privacy projection. The range demarcated by the red dashed lines denotes privacy-
threatening answer sets. The answers circled in red dashed line are at risk of leaking privacy.
projection on public inputs:
ğ‘€private ={ğ‘š|âˆƒğ‘£âˆˆğ‘†:ğ‘šâˆˆğ‘€âˆ§ğ‘(ğ‘£,ğ‘š)âˆˆA privateâˆ§
ğ‘(ğ‘¢,ğ‘š)â‰ ğ‘(ğ‘£,ğ‘š),âˆ€ğ‘¢âˆˆğ‘†/{ğ‘£},ğ‘(ğ‘¢,ğ‘š)âˆ‰Aprivate}.
If an answer can only be inferred from private information, it will
be regarded as a privacy threatening answer. For the projection on
private inputs,
ğ‘€private =ğ‘ƒ(ğ‘†private).
The answers of projection on private inputs are privacy threatening
answers as they can expose the existence of private inputs. As
shown in Figure 3(A), all the orange nodes within the answer sets
can be solely deduced from private attribute links and are classified
as query answers posing privacy risks. On the other hand, nodes
such as the green-orange node, while accessible through private
projection, can also be inferred from public components, thereby
carrying lower privacy risks.
Intersection : An intersection operatorâ€™s output will be regarded
as private if the answer belongs to any of the private answer sets.
Given a set of answer sets ğ‘€1,ğ‘€2,...,ğ‘€ğ‘›, where each answer set
ğ‘€ğ‘–=ğ‘€ğ‘–
privateâˆªğ‘€ğ‘–
public, after intersection operator, ğ‘€private =
âˆ©ğ‘›
ğ‘–=1ğ‘€ğ‘–âˆ’âˆ©ğ‘›
ğ‘–=1ğ‘€ğ‘–
public. As shown in Figure 3(B), the green-orange
node is categorized as a privacy-threatening answer because it
denotes the existence in the answer subsets.
Union : A union operatorâ€™s output will be regarded as private if
the answer is the element that belongs to the private answer set of
computational subgraphs while not belonging to public answer sets.
Given a set of answer sets ğ‘€1,ğ‘€2,...,ğ‘€ğ‘›,ğ‘€private =âˆªğ‘›
ğ‘–=1ğ‘€ğ‘–
privateâˆ’
âˆªğ‘›
ğ‘–=1ğ‘€ğ‘–
public. As shown in Figure 3(C), all the orange nodes withinthe answer sets can only be inferred from private attribute links
and are classified as privacy-threatening answers.
5 Privacy-Preserved Neural Graph Database
In this section, we mainly focus on the query encoding methods in
NGDBs and present the privacy-preserved neural graph databases
for protecting sensitive information in knowledge graphs while pre-
serving high-quality complex query answering performance. We set
two optimization goals for P-NGDBs: preserve the accuracy of re-
trieving non-private answers and obfuscate the privacy-threatening
answers.
5.1 Encoding Representation
The query encoding methods can be decomposed into two steps:
encode queries to embedding space ğ‘âˆˆRğ‘‘and compare the simi-
larity with the entities or attributes for answer retrieval. Given a
query, we iteratively compute the queries based on the sub-query
embeddings and logical operators. Assume the encoding process
in stepğ‘–, the sub-query embedding is ğ‘ğ‘–. We can denote the logical
operators projection, intersection, and union as:
ğ‘ğ‘–+1=ğ‘“ğ‘ƒ(ğ‘ğ‘–,ğ‘Ÿ), ğ‘ŸâˆˆRâˆªA, (2)
ğ‘ğ‘–+1=ğ‘“ğ¼(ğ‘1
ğ‘–,...,ğ‘ğ‘›
ğ‘–), (3)
ğ‘ğ‘–+1=ğ‘“ğ‘ˆ(ğ‘1
ğ‘–,...,ğ‘ğ‘›
ğ‘–), (4)
whereğ‘“ğ‘ƒ,ğ‘“ğ¼, andğ‘“ğ‘ˆdenote the parameterized projection, inter-
section, and union operators, respectively. Here, the ğ‘“ğ‘ƒ,ğ‘“ğ¼, and
ğ‘“ğ‘ˆcan be instantiated with various types of parameterization of
query encoding functions. We list all the instantiations used in our
 
1112KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, & Yangqiu Song
experiments in the Appendix A. After query encoding, we compute
the score for every candidate based on the query encoding and
entity (attribute) embeddings. Finally, we calculate the normalized
probability using the Softmax function:
ğ‘(ğ‘,ğ‘£)=ğ‘’ğ‘ (ğ‘,ğ‘’ğ‘£)/âˆ‘ï¸
ğ‘¢âˆˆCğ‘’ğ‘ (ğ‘,ğ‘’ğ‘¢), (5)
whereğ‘ is the scoring function, like similarity, distance, etc., and C
is the candidate set.
5.2 Learning Objective
In privacy-preserved NGDBs, there are two learning objectives:
given a query, P-NGDBs should accurately retrieve non-private
answers and obfuscate the private answers. For public retrieval,
given query embedding ğ‘, the loss function can be expressed as:
ğ¿ğ‘¢=âˆ’1
ğ‘âˆ‘ï¸
ğ‘£âˆˆMğ‘
publiclogğ‘(ğ‘,ğ‘£), (6)
whereMğ‘
publicis the public answer set for the given query.
While for privacy protection, the learning objective is antitheti-
cal. Instead of retrieving correct answers, the objective is to provide
obfuscated answers to address privacy concerns. Therefore for
private answers, given query embedding ğ‘, the objective can be
expressed as:
Ë†ğœƒğ‘”=arg min
ğœƒğ‘”âˆ‘ï¸
ğ‘£âˆˆMğ‘
privatelogğ‘(ğ‘,ğ‘£), (7)
whereMğ‘
privateis the private risk answer set for given query.
As discussed in Section 4.2, the query can be decomposed into
sub-queries and logical operators. For intersection and union, the
logical operators do not generate new privacy invasive answers,
while for projection, the answer can be private if the projection is
involvement of private information. Therefore, we can directly op-
timize the projection operators to safeguard sensitive information.
The privacy protection learning objective can be expressed as:
ğ¿ğ‘=1
|Aprivate|âˆ‘ï¸
ğ‘Ÿ(ğ‘¢,ğ‘£)âˆˆA privatelogğ‘(ğ‘“ğ‘(ğ‘’ğ‘£,ğ‘Ÿ),ğ‘¢). (8)
We aim to reach the two objectives simultaneously and the final
learning objective function can be expressed as:
ğ¿=ğ¿ğ‘¢+ğ›½ğ¿ğ‘, (9)
whereğ›½is the privacy coefficient controlling the protection strength
in P-NGDBs, larger ğ›½denotes stronger protection.
6 Experiments
In this section, to evaluate the privacy leakage problem in NGDBs
and the protection ability of our proposed Privacy-preserved Neural
Graph Database (P-NGDBs), we construct a benchmark on three
real-world datasets and evaluate P-NGDBâ€™s performance based on
that.
1p 2p 2u up
2i ip 3i piFigure 4: Eight general query types. Black, blue, and orange
arrows denote projection, intersection, and union operators
respectively.
Table 1: The statistics of the three knowledge graphs and
sampled private attributes.
Graphs Data Split #Nodes #Edges #Pri. Edges
FB15k-NTraining 22,964 1,037,480
8,000 Validation 24,021 1,087,296
Testing 27,144 1,144,506
DB15k-NTraining 27,639 340,936
6,000 Validation 29,859 381,090
Testing 36,358 452,348
YAGO15k-NTraining 30,351 383,772
1,600 Validation 31,543 417,356
Testing 33,610 453,688
6.1 Datasets
There are various knowledge graph datasets used in complex query
answering tasks [ 8,10]. Without loss of generality, we consider
the privacy problem in the three commonly used multi-relational
knowledge graph with numerical attributes: FB15K-N, DB15K-N,
and YAGO15K-N [ 32] for several reasons: First, attribute value pro-
jections can be the same as traditional relation projection if the
values themselves are entities, e.g., locations; Second, attributes
and their values are more aligned with real-world privacy consid-
erations; Third, attribute values are vulnerable to be attacked as
we can use group queries to attack individualâ€™s information, which
has been widely used as an illustration in differential privacy [ 17].
We create our benchmarks based on the numerical complex query
datasets proposed in [ 2]. In each knowledge graph, there are vertices
describing entities and attributes and edges describing entity rela-
tions, entity attributes, and numerical relations. We first randomly
select a set of edges denoting entitiesâ€™ attributes as privacy infor-
mation. Then We divide the remaining edges with a ratio of 8:1:1
to construct training, validation, and testing edge sets, respectively.
Then the training graph Gğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› , validation graphGğ‘£ğ‘ğ‘™, and testing
graphGğ‘¡ğ‘’ğ‘ ğ‘¡are constructed on training edges, training+validation
edges and training+validation+testing edges, respectively. The de-
tailed statistics of the three knowledge graphs are shown in Table
1, #Nodes represents the number of entities and attributes. #Edeges
represents the number of relation triples and attribute triples. #Pri.
Edges represent the number of attribute triples that are considered
private.
 
1113Privacy-Preserved Neural Graph Databases KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: The statistics of the testing queriesâ€™ answers sampled from the three knowledge graphs.
Graphs #Test Ans. 1p 2p 2i 3i pi ip 2u up All
FB15k-NPublic 42,952 768,012 138,446 68,677 1,120,065 307,546 1,247,120 1,844,682 5,537,500
Private 524 65,212 1,637 2,576 322,928 30,927 779 239,176 663,099
DB15k-NPublic 16,776 764,899 26,836 20,426 1,050,883 105,096 1,296,453 1,959,063 5,240,432
Private 211 25,236 677 763 148,769 6,067 266 146,218 328,207
YAGO15k-NPublic 14,880 913,082 38,546 23,066 1,553,590 164,430 1,111,663 1,920,157 5,739,414
Private 136 2,319 499 1742 3,656 1,483 175 40,267 50,277
6.2 Benchmark Construction
Following previous work [ 2,23,61], we evaluate the complex query
answering performance on the following eight general query types
with abbreviations of 1ğ‘,2ğ‘,2ğ‘–,3ğ‘–,ğ‘–ğ‘,ğ‘ğ‘–, 2ğ‘¢, andğ‘¢ğ‘, as shown in
Figure 4. For each general query type, each edge represents either
a projection or a logical operator, and each node represents either
a set of entities or numerical values. We use the sampling method
proposed in [ 2] to randomly sample complex queries from knowl-
edge graphs. We randomly sample training, validation, and testing
queries from the formerly constructed graphs respectively. For the
training queries, we search for corresponding training answers on
the training graph. For the validation queries, we use those queries
that have different answers on the validation graph from answers
on the training graph to evaluate the generalization ability. For the
testing queries, we conduct a graph search on the testing graph
with private edges for testing answers and we split the answers
into private set and public set according to the definition of privacy
risk query answers discussed in Section 4. We conduct a statistical
analysis of the number of privacy risk answers for different types
of complex queries of three knowledge graphs and the statistics are
shown in Table 2.
6.3 Experimental Setup
6.3.1 Baselines. Our proposed P-NGDBs can be applied to various
complex query encoding methods to provide privacy protections.
We select three commonly used complex query encoding methods
and compare the performance with and without P-NGDBâ€™s protec-
tion. To better encode the numerical attribute values, we also use
NRN [ 2] in the query encoding process to handle number distri-
butions. The detailed equations of the implementations of these
backbones are given in Appendix A.
â€¢GQE [ 23]: the graph query encoding model encodes a com-
plex query into a vector in embedding space;
â€¢Q2B [ 50]: the graph query encoding model encodes a com-
plex query into a hyper-rectangle embedding space.
â€¢Q2P [ 3]: the graph query encoding model encodes a complex
query into an embedding space with multiple vectors.
To the best of our knowledge, there are no privacy protection meth-
ods in NGDBs. Therefore, we compare our methods with noise
disturbance which is similar to differential privacy [ 31,48], a com-
monly employed technique in database queries that introduces
randomness to answers through the addition of noise.6.3.2 Evaluation Metrics. The evaluation consists of two distinct
parts: reasoning performance evaluation and privacy protection
evaluation. Following the previous work [ 2], given a testing query
ğ‘, the training, validation, and public testing answers are denoted
asğ‘€ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ,ğ‘€ğ‘£ğ‘ğ‘™, andğ‘€ğ‘¡ğ‘’ğ‘ ğ‘¡, respectively. We evaluate the quality
of retrieved answers using Hit ratio (HR) and Mean reciprocal
rank (MRR). HR@K metric evaluates the accuracy of retrieval by
measuring the percentage of correct hits among the top K retrieved
items. The MRR metric evaluates the performance of a ranking
model by computing the average reciprocal rank of the first relevant
item in a ranked list of results. We evaluate the generalization
capability of models by calculating the rankings of answers that
cannot be directly retrieved from an observed knowledge graph,
which isğ‘€ğ‘¡ğ‘’ğ‘ ğ‘¡/ğ‘€ğ‘£ğ‘ğ‘™. For reasoning performance evaluation, higher
metric values denote better retrieval quality. For privacy protection
evaluation, we compute the metric of privacy-threatening answers
as these answers cannot be inferred from the observed graphs.
Because we want to obfuscate those answers, lower values denote
stronger protection. We train all the models by using the training
queries and private attributes and tune hyper-parameters using
the validation queries. The evaluation is then finally conducted on
the testing queries, including the evaluation of public answers for
performance assessment and the evaluation of private answers for
privacy assessment. The experiment results are reported on the
testing public and private queries respectively.
6.3.3 Parameter Setting. We first tune hyper-parameters on the
validation queries for the base query encoding methods. We use
the same parameters for NGDBs for a fair comparison. We tune
the privacy penalty coefficient ğ›½for three datasets respectively
for utility privacy tradeoff for convenient illustration. For noise
disturbance privacy protection, we adjust the noise strength to make
the results comparable to P-NGDBsâ€™ results (similar performance
on public query answers).
6.4 Performance Evaluation
We apply our P-NGDB on three different query encoding methods
GQE [ 23], Q2B [ 50], and Q2P [ 3] and compare the query answering
performance with and without P-NGDBâ€™s protection. Experiment
results are summarized in Table 3 and Table 4. Table 3 reports the
averaged results in HR@3 and MRR. A higher value on public an-
swer sets denotes better reasoning ability, while a smaller value on
private answer sets denotes stronger protection. The experiment
results show that without protection, the base encoding methods all
 
1114KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, & Yangqiu Song
Table 3: The main experiment results of public answers re-
trieval and private answers protection. NGDB can provide ef-
fective protection while sacrificing acceptable performance.
Dataset Encoding ModelPublic Private
HR@3 MRR HR@3 MRR
FB15k-NGQEBaseline 21.99 20.26 28.99 27.82
Noise 15.89 14.67 21.54 21.37
P-NGDB 15.92 14.73 10.77 10.21
Q2BBaseline 18.70 16.88 30.28 28.98
Noise 12.34 12.19 20.01 19.71
P-NGDB 12.28 11.18 10.17 9.38
Q2PBaseline 26.45 24.48 29.08 31.85
Noise 20.13 19.77 22.35 23.17
P-NGDB 19.48 18.19 14.15 14.93
DB15k-NGQEBaseline 24.16 22.37 39.26 37.25
Noise 18.01 16.35 28.59 28.37
P-NGDB 17.58 16.29 10.52 10.79
Q2BBaseline 15.94 14.98 42.19 39.78
Noise 10.76 10.28 26.49 25.93
P-NGDB 10.19 9.49 8.92 7.99
Q2PBaseline 25.72 24.12 46.18 43.48
Noise 19.89 19.32 33.56 33.17
P-NGDB 20.26 19.00 19.38 18.45
YAGO15k-NGQEBaseline 26.06 24.37 43.55 40.81
Noise 20.32 20.27 38.52 38.29
P-NGDB 19.58 19.82 7.56 7.33
Q2BBaseline 23.39 22.53 42.73 40.55
Noise 16.85 15.37 28.23 28.54
P-NGDB 17.07 16.03 6.26 5.79
Q2PBaseline 29.41 27.87 42.56 45.79
Noise 22.85 21.21 34.26 33.68
P-NGDB 23.27 22.59 7.34 7.17
suffer from privacy leakage problems if there is sensitive informa-
tion existing in the knowledge graph. Our proposed P-NGDB can
effectively protect private information with a slight loss of complex
query answering ability. For example, in FB15K-N, GQE retrieves
the public answers with 21.99 HR@3 and the private answers with
28.99 HR@3 without privacy protection. With P-NGDBâ€™s protec-
tion, GQE retrieves public answers with 15.92 HR@3 and private
answers with 10.77 HR@3. The private answers are protected with
a 62.9% decrease in HR@3 sacrificing a 27.4% decrease in HR@3
for public answers. Compared to the noise disturbance protection,
our method can accurately protect sensitive information, so that
the loss of performance is much lower than the noise disturbance.
In Table 4, we show P-NGDBsâ€™ MRR results on various query
types and the percentages within parentheses represent the perfor-
mance of the P-NGDB protected model relative to the performance
of the encoding model without protection. The evaluation is con-
ducted on public and private answers respectively to assess privacy
protection. From the comparison between MRR variation on pub-
lic and private answers, we can know that P-NGDB can provide
protection on all the query types. Besides, comparing the average
change on these query types, we can know from the results that
the P-NGDB have different protection ability on various query
types. For projection and union, the MRR of protected models on
private answers is largely degraded, denoting a stronger protection.
While for the intersection operator, the preservation provided by
P-NGDBs is as effective as protections for other operators.
00.20.40.60.811.2
0 0.01 0.05 0.1 0.5 1
MRR (Ratio to GQE)
Privacy Coefficient Î²public
private0.974
0.681Figure 5: The evaluation results of GQE with various privacy
coefficients ğ›½on FB15K-N.
6.5 Sensitivity Study
The privacy protection needs can be different in various knowledge
graphs. We evaluate the impact of the privacy penalty coefficient
ğ›½, A largerğ›½can provide stronger privacy protection. We select
ğ›½from{0.01,0.05,0.1,0.5,1}, and evaluate the retrieval accuracy
of public answers and private answers, respectively. We evaluate
GQEâ€™s retrieval performance on the FB15K-N dataset and depict
the MRRâ€™s change under each ğ›½compared to the unprotected GQE
model. The results are shown in Figure 5. We know that the penalty
can effectively control the level of privacy protection. For example,
whenğ›½=0.01, the GQE model can achieve 97.4% MRR on public
answers while only having 68.1% MRR on private answers. The flex-
ible adjustment of the privacy coefficient can make P-NGDBs adapt
to various scenarios with different privacy requirements. However,
to meet higher privacy needs, P-NGDBs have to undertake much
utility loss.
7 Conclusion
In this work, we proposed the privacy problem in neural graph
databases and showed that sensitive information can be inferred
by attackers with specified queries. We denote that some query
answers can leak privacy information named privacy risk answer.
To systematically evaluate the problem, we constructed a bench-
mark dataset based on FB15k-N, YAGO15k-N, and DB15k-N. Finally,
we proposed a new framework that can protect the privacy of the
knowledge graph from attackersâ€™ malicious queries. Experimental
results on the benchmark shows that the proposed model (P-NGDB)
can effectively protect privacy while sacrificing slight reasoning
performance. In the future, we will take logical operators into con-
sideration and make improvements in NGDB privacy protection.
ACKNOWLEDGMENTS
The authors of this paper were supported by the NSFC Fund (U20B2053)
from the NSFC of China, the RIF (R6020-19 and R6021-20) and the
GRF (16211520 and 16205322) from RGC of Hong Kong. We also
thank the support from UGC Research Matching Grants (RMGS20EG01-
D, RMGS20CR11, RMGS20CR12, RMGS20EG19, RMGS20EG21,
RMGS23CR05, RMGS23EG08).
 
1115Privacy-Preserved Neural Graph Databases KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 4: The mean reciprocal ranking (MRR) variation for P-NGDBs on different types of queries. The performance ratio to the
corresponding unprotected models is reported in parentheses.
Dataset Enc. Test. 1p 2p 2i 3i pi ip 2u up
FB15k-NGQEPublic 10.51(70.4%) 3.89(55.3%) 23.49(89.1%) 44.14(92.2%) 14.57(83.4%) 6.43(73.5%) 4.15(59.2%) 2.95(70.9%)
Private 0.18(4.6%) 1.80(13.6%) 11.77(45.7%) 32.71(56.2%) 14.07(46.6%) 6.37(36.7%) 0.70(6.1%) 1.03(13.6%)
Q2BPublic 12.88(69.8%) 3.27(53.6%) 20.13(85.3%) 36.52(83.4%) 12.18(69.7%) 4.57(59.1%) 5.41(57.8%) 3.04(62.5%)
Private 0.41(11.6%) 3.02(23.4%) 9.45(31.5%) 27.42(50.3%) 15.21(47.5%) 9.49(47.1%) 0.23(5.0%) 1.48(19.1%)
Q2PPublic 16.36(73.5%) 6.22(59.8%) 23.34(82.5%) 43.29(93.6%) 15.26(77.3%) 7.44(63.8%) 6.79(60.8%) 3.64(58.2%)
Private 0.31(11.3%) 3.24(31.3%) 15.82(46.7%) 39.49(46.7%) 18.12(52.7%) 9.60(45.1%) 0.38(8.1%) 2.55(23.3%)
DB15k-NGQEPublic 2.35(58.1%) 0.98(56.8%) 22.30(83.1%) 49.54(85.4%) 10.06(67.0%) 3.48(53.0%) 0.73(51.2%) 0.94(57.2%)
Private 0.22(9.3%) 1.76(17.4%) 12.33(17.7%) 31.88(44.9%) 23.26(57.9%) 4.83(47.9%) 0.76(12.1%) 0.94(19.2%)
Q2BPublic 2.38(53.0%) 1.54(65.8%) 16.16(83.5%) 28.44(85.1%) 9.23(63.9%) 2.68(52.3%) 0.81(57.2%) 0.98(55.4%)
Private 0.18(7.6%) 2.72(26.8%) 9.34(13.4%) 24.89(34.7%) 19.10(53.3%) 6.06(53.0%) 0.74(12.5%) 0.67(16.0%)
Q2PPublic 4.75(60.3%) 2.59(61.8%) 24.67(80.2%) 52.48(94.8%) 13.41(70.2%) 4.62(48.1%) 2.20(54.3%) 1.58(46.8%)
Private 0.23(8.7%) 2.92(28.4%) 28.37(39.5%) 35.58(47.0%) 24.55(57.4%) 9.05(57.6%) 1.62(23.9%) 1.87(22.5%)
YAGO15k-NGQEPublic 5.28(86.5%) 1.78(57.9%) 24.35(66.9%) 43.2(65.0%) 13.98(70.3%) 4.32(60.7%) 3.56(89.2%) 1.03(66.0%)
Private 0.12(6.3%) 3.21(15.0%) 12.56(37.2%) 27.65(35.6%) 14.09(45.2%) 12.87(53.7%) 1.08(7.9%) 1.20(9.3%)
Q2BPublic 6.06(74.3%) 1.98(51.5%) 24.03(72.2%) 44.52(81.7%) 13.22(63.3%) 4.33(52.4%) 3.84(61.9%) 1.11(56.1%)
Private 0.11(5.6%) 3.45(16.6%) 16.99(50.4%) 20.62(32.4%) 13.51(39.1%) 14.42(48.3%) 1.32(10.1%) 1.14(12.3%)
Q2PPublic 9.96(83.7%) 3.78(67.7%) 30.96(77.1%) 37.87(68.9%) 15.23(64.3%) 6.59(64.9%) 9.63(71.4%) 2.18(66.1%)
Private 0.12(9.0%) 3.17(18.1%) 14.29(35.0%) 34.29(42.6%) 16.59(45.2%) 13.95(47.2%) 1.96(9.9%) 2.68(13.6%)
Avg. ChangePublicâ†“30.0%â†“41.1%â†“20.0%â†“16.7%â†“30.1%â†“41.4%â†“37.4%â†“40.1%
Privateâ†“91.8%â†“78.8%â†“59.9%â†“56.7%â†“50.6%â†“51.5%â†“67.1%â†“83.5%
References
[1]Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. 2021.
Complex Query Answering with Neural Link Predictors. In International Confer-
ence on Learning Representations.
[2]Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Bing Yin, and Yangqiu Song. 2023.
Knowledge Graph Reasoning over Entities and Numerical Values. In KDD. ACM,
57â€“68.
[3]Jiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. 2022.
Query2Particles: Knowledge Graph Reasoning with Particle Embeddings. Find-
ings of the Association for Computational Linguistics: NAACL 2022-Findings (2022).
[4]Jiaxin Bai, Tianshi Zheng, and Yangqiu Song. 2023. Sequential query encoding for
complex query answering on knowledge graphs. arXiv preprint arXiv:2302.13114
(2023).
[5]Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou. 2023. Answering complex logical queries
on knowledge graphs via query computation tree optimization. In International
Conference on Machine Learning. PMLR, 1472â€“1491.
[6]Maciej Besta, Patrick Iff, Florian Scheidl, Kazuki Osawa, Nikoli Dryden, Michal
Podstawski, Tiancheng Chen, and Torsten Hoefler. 2022. Neural graph databases.
InLearning on Graphs Conference. PMLR, 31â€“1.
[7]Christian Bizer, Jens Lehmann, Georgi Kobilarov, SÃ¶ren Auer, Christian Becker,
Richard Cyganiak, and Sebastian Hellmann. 2009. Dbpedia-a crystallization point
for the web of data. Journal of web semantics 7, 3 (2009), 154â€“165.
[8]Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on
Management of data. 1247â€“1250.
[9]Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. Advances in neural information processing systems 26 (2013).
[10] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam Hruschka,
and Tom Mitchell. 2010. Toward an architecture for never-ending language
learning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 24.
1306â€“1313.
[11] Xuelu Chen, Ziniu Hu, and Yizhou Sun. 2022. Fuzzy logic based logical query an-
swering on knowledge graphs. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 36. 3939â€“3948.
[12] Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chan-
dan Reddy. 2021. Probabilistic entity representation model for reasoning over
knowledge graphs. Advances in Neural Information Processing Systems 34 (2021),
23440â€“23451.[13] Ameya Daigavane, Gagan Madan, Aditya Sinha, Abhradeep Guha Thakurta,
Gaurav Aggarwal, and Prateek Jain. 2021. Node-level differentially private graph
neural networks. arXiv preprint arXiv:2111.15521 (2021).
[14] Wei-Yen Day, Ninghui Li, and Min Lyu. 2016. Publishing graph degree distribution
with node differential privacy. In Proceedings of the 2016 International Conference
on Management of Data. 123â€“138.
[15] Xin Luna Dong. 2018. Challenges and innovations in building a product knowl-
edge graph. In Proceedings of the 24th ACM SIGKDD International conference on
knowledge discovery & data mining. 2869â€“2869.
[16] Vasisht Duddu, Antoine Boutet, and Virat Shejwalkar. 2020. Quantifying privacy
leakage in graph embedding. In MobiQuitous 2020-17th EAI International Con-
ference on Mobile and Ubiquitous Systems: Computing, Networking and Services.
76â€“85.
[17] Cynthia Dwork. 2008. Differential privacy: A survey of results. In International
conference on theory and applications of models of computation. Springer, 1â€“19.
[18] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva
Mody, Steven Truitt, and Jonathan Larson. 2024. From Local to Global: A Graph
RAG Approach to Query-Focused Summarization. arXiv preprint arXiv:2404.16130
(2024).
[19] Luis Antonio GalÃ¡rraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek.
2013. AMIE: association rule mining under incomplete evidence in ontological
knowledge bases. In Proceedings of the 22nd international conference on World
Wide Web. 413â€“422.
[20] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,
Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large
language models: A survey. arXiv preprint arXiv:2312.10997 (2023).
[21] Neil Zhenqiang Gong and Bin Liu. 2018. Attribute inference attacks in online
social networks. ACM Transactions on Privacy and Security (TOPS) 21, 1 (2018),
1â€“30.
[22] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
2020. Retrieval Augmented Language Model Pre-Training. In ICML (Proceedings
of Machine Learning Research, Vol. 119). PMLR, 3929â€“3938.
[23] Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec.
2018. Embedding logical queries on knowledge graphs. Advances in neural
information processing systems 31 (2018).
[24] Michael Hay, Gerome Miklau, David Jensen, Don Towsley, and Philipp Weis. 2008.
Resisting structural re-identification in anonymized social networks. Proceedings
of the VLDB Endowment 1, 1 (2008), 102â€“114.
[25] Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie, Lichao
Sun, Lifang He, Liangwei Yang, Philip S Yu, Yu Rong, et al .2021. Fedgraphnn:
A federated learning system and benchmark for graph neural networks. arXiv
preprint arXiv:2104.07145 (2021).
 
1116KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, & Yangqiu Song
[26] Xinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong, and Yang Zhang.
2021. Stealing links from graph neural networks. In 30th USENIX Security Sym-
posium (USENIX Security 21). 2669â€“2686.
[27] Anh-Tu Hoang, Barbara Carminati, and Elena Ferrari. 2020. Cluster-based
anonymization of knowledge graphs. In Applied Cryptography and Network
Security: 18th International Conference, ACNS 2020, Rome, Italy, October 19â€“22,
2020, Proceedings, Part II 18. Springer, 104â€“123.
[28] Hui Hu, Lu Cheng, Jayden Parker Vap, and Mike Borowczak. 2022. Learning
privacy-preserving graph convolutional network with partially observed sensi-
tive attributes. In Proceedings of the ACM Web Conference 2022. 3552â€“3561.
[29] Qi Hu and Yangqiu Song. 2023. Independent Distribution Regularization for
Private Graph Embedding. In Proceedings of the 32nd ACM International Confer-
ence on Information and Knowledge Management (Birmingham, United Kingdom)
(CIKM â€™23). Association for Computing Machinery, New York, NY, USA, 823â€“832.
https://doi.org/10.1145/3583780.3614933
[30] Qi Hu and Yangqiu Song. 2023. User Consented Federated Recommender System
Against Personalized Attribute Inference Attack. arXiv preprint arXiv:2312.16203
(2023).
[31] Shiva Prasad Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam D
Smith. 2013. Analyzing Graphs with Node Differential Privacy.. In TCC, Vol. 13.
Springer, 457â€“476.
[32] Bhushan Kotnis and Alberto GarcÃ­a-DurÃ¡n. 2018. Learning numerical attributes
in knowledge bases. In Automated Knowledge Base Construction (AKBC).
[33] Bhushan Kotnis, Carolin Lawrence, and Mathias Niepert. 2021. Answering
complex queries in knowledge graphs with bidirectional sequence encoders. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4968â€“4977.
[34] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for
Weakly Supervised Open Domain Question Answering. In ACL (1). Association
for Computational Linguistics, 6086â€“6096.
[35] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit
Chan, and Yangqiu Song. 2023. Privacy in large language models: Attacks,
defenses and future directions. arXiv preprint arXiv:2310.10383 (2023).
[36] Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan,
Duanyi Yao, and Yangqiu Song. 2023. P-Bench: A Multi-level Privacy Evaluation
Benchmark for Language Models. arXiv preprint arXiv:2311.04044 (2023).
[37] Kaiyang Li, Guangchun Luo, Yang Ye, Wei Li, Shihao Ji, and Zhipeng Cai. 2020.
Adversarial privacy-preserving graph embedding against inference attack. IEEE
Internet of Things Journal 8, 8 (2020), 6904â€“6915.
[38] Peiyuan Liao, Han Zhao, Keyulu Xu, Tommi S Jaakkola, Geoff Gordon, Stefanie
Jegelka, and Ruslan Salakhutdinov. 2020. Graph adversarial networks: Protecting
information against adversarial attacks. (2020).
[39] Ji Liu, Zenan Li, Yuan Yao, Feng Xu, Xiaoxing Ma, Miao Xu, and Hanghang Tong.
2022. Fair representation learning: An alternative to mutual information. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 1088â€“1097.
[40] Junnan Liu, Qianren Mao, Jianxin Li, Xingcheng Fu, and Zheng Wang. 2023.
POINE 2: Improving PoincarÃ© Embeddings for Hierarchy-Aware Complex Query
Reasoning over Knowledge Graphs. In ECAI 2023. IOS Press, 1521â€“1528.
[41] Kun Liu and Evimaria Terzi. 2008. Towards identity anonymization on graphs.
InProceedings of the 2008 ACM SIGMOD international conference on Management
of data. 93â€“106.
[42] Lihui Liu, Boxin Du, Heng Ji, ChengXiang Zhai, and Hanghang Tong. 2021.
Neural-answering logical queries on knowledge graphs. In Proceedings of the
27th ACM SIGKDD conference on knowledge discovery & data mining. 1087â€“1097.
[43] Costas Mavromatis and George Karypis. 2024. GNN-RAG: Graph Neural Retrieval
for Large Language Model Reasoning. arXiv preprint arXiv:2405.20139 (2024).
[44] Changdae Oh, Heeji Won, Junhyuk So, Taero Kim, Yewon Kim, Hosik Choi,
and Kyungwoo Song. 2022. Learning Fair Representation via Distributional
Contrastive Disentanglement. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 1295â€“1305.
[45] Iyiola E Olatunji, Wolfgang Nejdl, and Megha Khosla. 2021. Membership inference
attack on graph neural networks. In 2021 Third IEEE International Conference on
Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA). IEEE,
11â€“20.
[46] Hao Peng, Haoran Li, Yangqiu Song, Vincent Zheng, and Jianxin Li. 2021. Dif-
ferentially private federated knowledge graphs embedding. In Proceedings of the
30th ACM International Conference on Information & Knowledge Management.
1416â€“1425.
[47] Tahereh Pourhabibi, Kok-Leong Ong, Booi H Kam, and Yee Ling Boo. 2020. Fraud
detection: A systematic literature review of graph-based anomaly detection
approaches. Decision Support Systems 133 (2020), 113303.
[48] Zhan Qin, Ting Yu, Yin Yang, Issa Khalil, Xiaokui Xiao, and Kui Ren. 2017.
Generating synthetic decentralized social graphs with local differential privacy. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
Security. 425â€“438.
[49] Hongyu Ren, Mikhail Galkin, Michael Cochez, Zhaocheng Zhu, and Jure Leskovec.
2023. Neural graph reasoning: Complex logical query answering meets graph
databases. arXiv preprint arXiv:2303.14617 (2023).[50] Hongyu Ren, Weihua Hu, and Jure Leskovec. 2020. Query2box: Reasoning
over knowledge graphs in vector space using box embeddings. arXiv preprint
arXiv:2002.05969 (2020).
[51] Hongyu Ren and Jure Leskovec. 2020. Beta embeddings for multi-hop logical
reasoning in knowledge graphs. Advances in Neural Information Processing
Systems 33 (2020), 19716â€“19726.
[52] Luca Rossi, Mirco Musolesi, and Andrea Torsello. 2015. On the k-anonymization
of time-varying and multi-layer social graphs. In Proceedings of the international
AAAI conference on web and social media, Vol. 9. 377â€“386.
[53] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2018. Ml-leaks: Model and data independent membership
inference attacks and defenses on machine learning models. arXiv preprint
arXiv:1806.01246 (2018).
[54] Entong Shen and Ting Yu. 2013. Mining frequent graph patterns with differential
privacy. In Proceedings of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining. 545â€“553.
[55] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of
semantic knowledge. In Proceedings of the 16th international conference on World
Wide Web. 697â€“706.
[56] Florian TramÃ¨r, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via prediction {APIs}. In25th USENIX
security symposium (USENIX Security 16). 601â€“618.
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[58] Binghui Wang, Jiayi Guo, Ang Li, Yiran Chen, and Hai Li. 2021. Privacy-
preserving representation learning on graphs: A mutual information perspective.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining. 1667â€“1676.
[59] Zihao Wang, Weizhi Fei, Hang Yin, Yangqiu Song, Ginny Wong, and Simon See.
2023. Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local
Comparison and Global Transport. In Findings of the Association for Compu-
tational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki
Okazaki (Eds.). Association for Computational Linguistics, 13679â€“13696.
[60] Zihao Wang, Yangqiu Song, Ginny Wong, and Simon See. 2023. Logical Message
Passing Networks with One-hop Inference on Atomic Formulas. In The Eleventh
International Conference on Learning Representations.
[61] Zihao Wang, Hang Yin, and Yangqiu Song. 2021. Benchmarking the Combinatorial
Generalizability of Complex Query Answering on Knowledge Graphs. In NeurIPS
Datasets and Benchmarks Track. https://openreview.net/forum?id=pX4x8f6Km5T
[62] Zihao Wang, Hang Yin, and Yangqiu Song. 2022. Logical Queries on Knowl-
edge Graphs: Emerging Interface of Incomplete Relational Data. Bulletin of the
Technical Committee on Data Engineering 45, 4 (2022), 3â€“18.
[63] Fan Wu, Yunhui Long, Ce Zhang, and Bo Li. 2022. Linkteller: Recovering private
edges from graph neural networks via influence analysis. In 2022 IEEE Symposium
on Security and Privacy (SP). IEEE, 2005â€“2024.
[64] Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, and Jun Zhao. 2023.
Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries
over Knowledge Graphs. arXiv preprint arXiv:2310.11246 (2023).
[65] Dong Yang, Peijun Qing, Yang Li, Haonan Lu, and Xiaodong Lin. 2022. GammaE:
Gamma Embeddings for Logical Queries on Knowledge Graphs. In Proceedings
of the 2022 Conference on Empirical Methods in Natural Language Processing.
745â€“760.
[66] Hang Yin, Zihao Wang, and Yangqiu Song. 2024. Rethinking Complex Queries
on Knowledge Graphs with Neural Link Predictors. In The Twelfth International
Conference on Learning Representations (ICLR 2024).
[67] Sepanta Zeighami, Raghav Seshadri, and Cyrus Shahabi. 2023. A neural database
for answering aggregate queries on incomplete relational data. IEEE Transactions
on Knowledge and Data Engineering (2023).
[68] Sepanta Zeighami and Cyrus Shahabi. 2023. NeuroDB: Efficient, Privacy-
Preserving and Robust Query Answering with Neural Networks. In NeurIPS
2023 Second Table Representation Learning Workshop.
[69] Kainan Zhang, Zhi Tian, Zhipeng Cai, and Daehee Seo. 2021. Link-privacy
preserving graph embedding data publication with adversarial learning. Tsinghua
Science and Technology 27, 2 (2021), 244â€“256.
[70] Zhikun Zhang, Min Chen, Michael Backes, Yun Shen, and Yang Zhang. 2022. Infer-
ence attacks against graph neural networks. In 31st USENIX Security Symposium
(USENIX Security 22). 4543â€“4560.
[71] Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. 2021. Cone:
Cone embeddings for multi-hop reasoning over knowledge graphs. Advances in
Neural Information Processing Systems 34 (2021), 19172â€“19183.
[72] Elena Zheleva and Lise Getoor. 2007. Preserving the privacy of sensitive rela-
tionships in graph data. In International workshop on privacy, security, and trust
in KDD. Springer, 153â€“171.
[73] Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. 2022. Neural-
symbolic models for logical queries on knowledge graphs. In International Con-
ference on Machine Learning. PMLR, 27454â€“27478.
 
1117Privacy-Preserved Neural Graph Databases KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A Instantiations of Query Encoders
In this section, we present detailed formulas for parameterizing
various logical operations utilized in this paper. Additionally, we
employ the following three query encoders: GQE, Q2B, and Q2P,
as the foundational components of our approaches.
A.1 Graph Query Encoding (GQE)
In the GQE method [ 23], every query is represented as a singu-
lar vector. We denote the query embedding of each sub-query as
ğ‘ğ‘–âˆˆRğ‘˜. In logical operations that involve multiple inputs, if there
are multiple sub-queries, we denote them as ğ‘1
ğ‘–,ğ‘2
ğ‘–,...,ğ‘ğ‘›
ğ‘–. Consis-
tent with the paper, we employ the TransE variant of GQE for the
instantiation of the relational projection in GQE. The specifics are
outlined below. The equation for updating the query embedding in
the GQE method is given by:
ğ‘ğ‘–+1=ğ‘“ğ‘ƒ(ğ‘ğ‘–,ğ‘Ÿ)=ğ‘ğ‘–+ğ‘’ğ‘Ÿ, (10)
whereğ‘’ğ‘Ÿrepresents the relation embedding of ğ‘Ÿ.
Additionally, for modeling the intersection operation, we uti-
lize feed-forward functions and average pooling to implement a
permutation-invariant neural network.
ğ‘ğ‘–+1=ğ‘“ğ¼(ğ‘1
ğ‘–,...,ğ‘ğ‘›
ğ‘–)=ğ‘Šğ¼(ğœ™(FFN(ğ‘ğ‘˜
ğ‘–),âˆ€ğ‘˜âˆˆ{1,2,...,ğ‘›})),(11)
where FFNis a feed-forward function, ğœ™is an average pooling, and
ğ‘Šğ¼is parameter matrix. Following the setting in Q2B paper [ 50],
we express the queries in disjunction normal form so that we can
avoid the parameterization of union operations.
A.2 Box Embeddings (Q2B)
For the Q2B model, we implement the operations following the
original paper. The box embedding can be separated into two parts,
denoting the center and offsets, namely ğ‘ğ‘–=[ğ‘ğ‘–,ğ‘œğ‘–]. Meanwhile,
for each relation ğ‘Ÿ, there is a corresponding box relation embedding
ğ‘’ğ‘Ÿ=[ğ‘ğ‘Ÿ,ğ‘œğ‘Ÿ]. The relation projection ğ‘ğ‘–+1=ğ‘“ğ‘ƒ(ğ‘ğ‘–,ğ‘Ÿ)is computed
as follows
ğ‘ğ‘–+1=ğ‘ğ‘–+ğ‘’ğ‘Ÿ=[ğ‘ğ‘–+ğ‘ğ‘Ÿ,ğ‘œğ‘–+ğ‘œğ‘Ÿ] (12)
The intersection operations ğ‘ğ‘–+1=ğ‘“ğ¼(ğ‘1
ğ‘–,ğ‘1
ğ‘–,...,ğ‘ğ‘›
ğ‘–)is computed as
follows
ğ‘ğ‘–+1=âˆ‘ï¸
ğ‘˜ğ‘ğ‘˜Â·ğ‘ğ‘˜
ğ‘–, ğ‘ğ‘˜=exp(MLP(ğ‘ğ‘˜))Ã
ğ‘—exp(MLP(ğ‘ğ‘—))(13)
ğ‘œğ‘–+1=Min({ğ‘œ1
ğ‘–,ğ‘œ2
ğ‘–,...,ğ‘œğ‘›
ğ‘–})Â·ğœ(DeepSets({ğ‘1
ğ‘–,ğ‘2
ğ‘–,...,ğ‘ğ‘›
ğ‘–})),(14)
then the query embedding of ğ‘–+1step isğ‘ğ‘–+1=[ğ‘ğ‘–+1,ğ‘ğ‘–+1]. In
this equation,Â·is the dimension-wise product, MLP is a multi-
layer perceptron, and ğœis the sigmoid function. DeepSets(Â·)is the
permutation-invariant deep architecture, which treats all the input
sub-queries equally. The DeepSets({ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘})is implemented
asMLP(1
ğ‘Ã
ğ‘–(MLP(ğ‘¥ğ‘–))). Following the original paper [ 50], we usethe disjunction normal form to avoid the direct parameterization
of the union operation.
A.3 Particle Embeddings (Q2P)
In the Q2P embedding method [ 3], query embeddings are repre-
sented as a set of particle embeddings. Specifically, the query em-
bedding can be represented as ğ‘ğ‘–=[ğ‘1
ğ‘–,ğ‘2
ğ‘–,...,ğ‘ğ‘˜
ğ‘–], whereğ‘˜denotes
the number of particles in the embedding.
The relation projection ğ‘“ğ‘ƒis defined as follows: ğ‘ğ‘–+1=ğ‘“ğ‘ƒ(ğ‘ğ‘–,ğ‘’ğ‘™),
whereğ‘ğ‘–andğ‘ğ‘–+1represent the input and output particle embed-
dings, respectively.
Unlike the approach in [ 9], where the same relation embedding
ğ‘’ğ‘™is directly added to all particles in ğ‘ğ‘–to model the relation projec-
tion, Q2P introduces multiple neuralized gates. These gates enable
individual adjustment of the relation transition for each particle in
ğ‘ğ‘–. The formulation is as follows:
ğ‘=ğœ(ğ‘Šğ‘ƒ
ğ‘§ğ‘’ğ‘™+ğ‘ˆğ‘§ğ‘ğ‘–+ğ‘ğ‘§), (15)
ğ‘…=ğœ(ğ‘Šğ‘ƒ
ğ‘Ÿğ‘’ğ‘™+ğ‘ˆğ‘Ÿğ‘ğ‘–+ğ‘ğ‘Ÿ), (16)
ğ‘‡=ğœ™(ğ‘Šğ‘ƒ
â„ğ‘’ğ‘™+ğ‘ˆâ„(ğ‘…âŠ™ğ‘ğ‘–)+ğ‘â„), (17)
ğ´ğ‘–=(1âˆ’ğ‘)âŠ™ğ‘ğ‘–+ğ‘âŠ™ğ‘‡. (18)
Here,ğœandğœ™are the sigmoid and hyperbolic tangent functions,
andâŠ™is the Hadamard product. To allow information exchange
among different particles, a scaled dot-product self-attention [ 57]
module is also incorporated,
ğ‘ğ‘–+1=Attn(ğ‘Šğ‘ƒ
ğ‘ğ´ğ‘‡
ğ‘–,ğ‘Šğ‘ƒ
ğ‘˜ğ´ğ‘‡
ğ‘–,ğ‘Šğ‘ƒ
ğ‘£ğ´ğ‘‡
ğ‘–)ğ‘‡. (19)
The parameters ğ‘Šğ‘ƒğ‘,ğ‘Šğ‘ƒ
ğ‘˜,ğ‘Šğ‘ƒğ‘£âˆˆRğ‘‘Ã—ğ‘‘are utilized to model the in-
put Query, Key, and Value for the self-attention module called Attn .
The intersection operation ğ‘“ğ¼is defined on multiple sets of particle
embeddings{ğ‘(ğ‘›)
ğ‘–}ğ‘
ğ‘›=1. It outputs a single set of particle embed-
dingsğ‘ğ‘–+1=ğ‘“ğ¼({ğ‘(ğ‘›)
ğ‘–}ğ‘
ğ‘›=1). The particles from the {ğ‘(ğ‘›)
ğ‘–}ğ‘
ğ‘›=1are
first merged into a new matrix ğ‘ğ‘–=[ğ‘(1)
ğ‘–,ğ‘(2)
ğ‘–,...,ğ‘(ğ‘)
ğ‘–]âˆˆğ‘…ğ‘‘Ã—ğ‘ğ¾,
and this matrix ğ‘ğ‘–serves as the input of the intersection operation.
The operation updates the position of each particle based on the po-
sitions of other input particles. This process is modeled using scaled
dot-product self-attention, followed by a multi-layer perceptron
(MLP) layer. The equations for this process are as follows:
ğ´ğ‘–=Attn(ğ‘Šğ¼
ğ‘ğ‘ğ‘‡
ğ‘–,ğ‘Šğ¼
ğ‘˜ğ‘ğ‘‡
ğ‘–,ğ‘Šğ¼
ğ‘£ğ‘ğ‘‡
ğ‘–)ğ‘‡, (20)
ğ‘ğ‘–+1=MLP(ğ´ğ‘–). (21)
Here,ğ‘Šğ¼ğ‘,ğ‘Šğ¼
ğ‘˜,ğ‘Šğ¼ğ‘£âˆˆRğ‘‘Ã—ğ‘‘are the parameters for the self-attention
layer. The MLPrepresents a multi-layer perceptron layer with ReLU
activation. It is important to note that the parameters in the MLP
layers of different operations are not shared.
 
1118