Graph Cross Supervised Learning via Generalized Knowledge
Xiangchi Yuan
Brandeis University
Waltham, MA, USA
Georgia Institute of Technology
Atlanta, GA, USA
xiangchiyuan@brandeis.eduYijun Tian
University of Notre Dame
South Bend, IN, USA
yijun.tian@nd.eduChunhui Zhang
Dartmouth College
Hanover, NH, USA
chunhui.zhang.gr@dartmouth.edu
Yanfang Ye
University of Notre Dame
South Bend, IN, USA
yye7@nd.eduNitesh V. Chawla
University of Notre Dame
South Bend, IN, USA
nchawla@nd.eduChuxu Zhang
Brandeis University
Waltham, MA, USA
chuxuzhang@brandeis.edu
ABSTRACT
The success of GNNs highly relies on the accurate labeling of
data. Existing methods of ensuring accurate labels, such as weakly-
supervised learning, mainly focus on the existing nodes in the
graphs. However, in reality, new nodes always continuously emerge
on dynamic graphs, with different categories and even label noises.
To this end, we formulate a new problem, Graph Cross-Supervised
Learning, or Graph Weak-Shot Learning, that describes the chal-
lenges of modeling new nodes with novel classes and potential
label noises. To solve this problem, we propose Lipshitz-regularized
Mixture-of-Experts similarity network (LIME), a novel framework
to encode new nodes and handle label noises. Specifically, we first
design a node similarity network to capture the knowledge from
the original classes, aiming to obtain insights for the emerging
novel classes. Then, to enhance the similarity networkâ€™s generaliza-
tion to new nodes that could have a distribution shift, we employ
the Mixture-of-Experts technique to increase the generalization
of knowledge learned by the similarity network. To further avoid
losing generalization ability during training, we introduce the Lips-
chitz bound to stabilize model output and alleviate the distribution
shift issue. Empirical experiments validate LIMEâ€™s effectiveness:
we observe a substantial enhancement of up to 11.34% in node
classification accuracy compared to the backbone model when
subjected to the challenges of label noise on novel classes across
five benchmark datasets. The code can be accessed through https:
//github.com/xiangchi-yuan/Graph-Cross-Supervised-Learning.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks; â€¢Informa-
tion systemsâ†’Social networks.
KEYWORDS
Graph Cross-Supervised Learning; Graph Weak-Shot Learning;
Mixture-of-Experts; Lipschitz Constant; Label Noise
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671830ACM Reference Format:
Xiangchi Yuan, Yijun Tian, Chunhui Zhang, Yanfang Ye, Nitesh V. Chawla,
and Chuxu Zhang. 2024. Graph Cross Supervised Learning via Generalized
Knowledge. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671830
1 INTRODUCTION
Graph Neural Networks (GNNs) employ message-passing mech-
anisms to iteratively update node representations, allowing them
to aggregate information from neighboring nodes. This capabil-
ity makes GNNs particularly potent for modeling and learning
from graph-structured data. Consequently, GNNs have consistently
demonstrated state-of-the-art performance across various graph-
related tasks, such as node classification [ 9,16,34] and graph clas-
sification [ 40,45,46]. Furthermore, GNNs have proven to be no-
tably effective in real-world applications, including social network
analysis [ 23,55], fraud detection [ 56], natural language process-
ing [6, 33, 47], and recommendation systems [5, 12, 48].
However, the successes of GNNs rely heavily on the precise
annotation of training data. If nodes are labeled with noise, the
model can be fooled, leading to inaccurate representations with
nodes belonging to different categories cross together. Considering
there are always new nodes with noisy or sparse labels added to the
graphs, encoding nodes on real-world web graphs such as social,
citation, and sales networks can be challenging. Therefore, many
graph weakly supervised learning methods are proposed to handle
label noise [ 4,24,31]. For example, NRGNN [ 4] and RTGNN [ 27]
are proposed to predict accurate pseudo-labels and distinguish
noisy labels from clean labels to provide supervision for the new
nodes with label noise. Nevertheless, these methods are limited to
handling new nodes that have pre-existing classes in the training
set. When there is a need to model the nodes with novel classes
and potential label noises, these methods can not fully utilize the
knowledge contained in the original graph and fail to perform well.
Although predicting new nodes that have existing classes can
be straightforward, it is challenging to learn and predict the new
nodes with classes that are distinct from existing node classes,
especially when these nodes have label noise. In light of this, we
introduce a new problem, i.e. Graph Cross-Supervised (Weak-
Shot) Learning, which corresponds to learning from existing nodes
4083
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xiangchi Yuan et al.
New Class Nodes 
w/ Label NoiseBase Class Nodes
w/o Label Noise
New Class Nodes 
w/o Label Noise
New Nodes Injection
Figure 1: Graph Cross-Supervised Learning: in the original
graph, base class nodes with accurate labels; then new novel
class nodes with label noise emerge on the original graph.
with correct labels to deduce the true category of new nodes, while
noises can present for these new nodes.
This problem is realistic and important (detailed discussion on
this bitter lesson is provided Appendix D): experts can annotate
nodes on small-scale graphs with high quality at the start, while it
is unrealistic to continually annotate new nodes with novel labels
accurately when tons of these nodes continuously emerge, consid-
ering limited annotation resources. Therefore, it is common and
natural to annotate nodes through cheap and easy-maintaining
approaches like crowdsourcing and pseudo labels by clustering,
which bring label noise. To solve this problem, we propose a novel
framework called LIME (Lipshitz-regularized Mixture-of-Experts
similarity network). Specifically, we first obtain the position fea-
tures and concatenate them to node features. Then the concatenated
features of the original graph are used to train a similarity network,
which encodes the base class node without label noise and gen-
erates similarity scores of node embedding pairs. After that, the
similarity network inferences on new class nodes with potential
label noises. For each node, the similarity scores between it and the
other nodes with the same label are averaged as its loss weight dur-
ing subsequent model training for node classification. To generalize
the similarity knowledge from training node pairs to inference node
pairs, we utilize the Mixture-of-Experts technique to increase the
generalization of the similarity network and introduce the Lipshitz
constant as loss regularization with theoretical analysis. The major
contributions of this paper are summarized as follows:
â€¢To the best of our knowledge, this work first proposes a new re-
alistic graph learning scenario called Graph Cross-Supervised
Learning. According to our empirical studies in the experimental
section, most current works are ineffective in this setting because
they do not fully utilize existing labels of the base class nodes.
â€¢To solve the problem, we propose LIME model that utilizes a
similarity network to extract implicit knowledge for weighing
the training loss of new class nodes with label noises. To further
generalize this process, the Mixture-of-Experts module and the
corresponding Lipshitz bound are proposed.
â€¢We build benchmark datasets and related dataset synthetic tools
for the new problem. Extensive experiments on the new datasets
demonstrate the effectiveness of our method. Results show that
LIME outperforms popular baselines and handles the graph cross-
supervised learning scenario well against label noises.
2 RELATED WORK
Graph Neural Networks (GNNs). GNNs have gained widespread
attention due to their ability to effectively learn non-Euclideandata and achieve outstanding performance in various graph mining
tasks [ 1,3,9,32,44,53]. Early works of GNNs such as graph con-
volutional networks (GCN) are proposed to apply convolutional
operations on graph data [ 7,16,42]. Graph attention networks were
introduced to improve GCNs by incorporating attention mecha-
nisms to weigh the importance of neighboring nodes during mes-
sage passing [ 34,39]. In addition, graph recurrent neural networks
have been proposed to address the limitations of GNNs in handling
long-distance message passing on large graphs by incorporating
gating mechanisms inspired by recurrent neural networks [ 28].
To overcome the problem of oversmoothing, deeper GNNs were
constructed using skip connections to learn more comprehensive
representations [ 18â€“20]. While prior works have primarily focused
on improving standard accuracy, our method addresses the growing
concerns of label noise and novel node classes.
Graph Learning with Label Noise. Previous research [ 24,54] has
shown that deep graph models are vulnerable to label noises, which
urges the need to design robust graph learning methods against
label noise. Among current methods that handle node label noise,
D-GNN [ 24] employs backward loss correction [ 26] to improve
performance. NRGNN [ 4] learns robust node representations with
noisy and sparse labels by connecting unlabeled nodes with labeled
nodes and further predicting accurate pseudo-labels to provide
supervision. Different from NRGNN which explicitly governs noisy
labels, RTGNN [ 27] distinguishes noisy labels from clean labels
and provides label correction to reduce the impact of label noises.
However, when there is the need to model new nodes with novel
classes and potential label noises, these methods do not utilize
knowledge contained by strongly labeled base class nodes in the
original graph to handle label noise with novel classes.
Cross-Supervised (Weak-Shot) Learning. Cross-Supervised
learning refers to learning novel categories with cheap weak labels,
with knowledge from a set of base categories that are already accu-
rately labeled. Machine learning and computer vision researchers
have explored similar settings (also named mixed-supervised learn-
ing or weak-shot learning) in different tasks, such as object detec-
tion [ 10,21,58], fine-grained classification [ 2], semantic segmen-
tation [ 58], and instance segmentation [ 11,17]. To the best of our
knowledge, we are the first to attempt to define cross-supervised
learning on graphs.
3 PRELIMINARY
Problem Definition. Consider an undirected attributed graph
G=(V,E,X,Y), whereVis the set of ğ‘nodes,Eis the set
of edges,X âˆˆRğ‘Ã—ğ·denotes the set of node features with ğ·
dimensions, andYdenotes node labels with Kclasses. The label
for these nodes with base classes isclean. After new nodes with
novel classes emerge on original graph, the graph becomes to
Gâ€²=(Vâ€²,Eâ€²,Xâ€²,Yâ€²). Note thatYâ€²hasKâ€²classes with|Kâ€²âˆ’K|
new classes annotated with noise, and the ground-truth set is
denoted asYğ‘‡. The objective is to learn a model ğ‘“ğœƒto maximum
prediction accuracy in terms of the ground-truth label. The Graph
Cross-Supervised Learning objective can be formulated as:
min
ğœƒL(ğ‘“ğœƒ(Gâ€²),Yğ‘‡). (1)
Sparse Mixture of Experts [ 29].The Mixture-of-Experts (MoE)
4084Graph Cross Supervised Learning via Generalized Knowledge KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ensembles multiple expert models to make predictions or perform
other tasks. The basic idea behind MoE is to divide the input space
into numerous partitions and assign different experts to different
partitions. Each expert is a specialized model that is trained to
perform well on a specific subset of the input space. The final
output is obtained by assembling the predictions of all the experts,
typically using a gating mechanism that determines the weight of
each expert based on the input. Formally, let â„be the input space
to the MoE, and E={ğ¸ğ‘–(Â·)}ğ‘
ğ‘–=1denotes that an MoE layer consists
ofğ‘experts. The output of the MoE is given by:
ğ‘¦=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–(â„)ğ¸ğ‘–(â„), (2)
whereğ¸ğ‘–is theğ‘–-th expert model, ğ‘ğ‘–(â„)is the weight assigned to
theğ‘–-th expert for the input space â„. The weights are typically
obtained from a gating network, which is trained to assign higher
weights to experts that are more likely to make accurate predictions
for a given input space. A popular approach to design the gating
mechanism is to use a softmax function to calculate the weights
for gating, and to use neural networks as the expert models by
ğ‘ğ‘–(â„)=exp(ğ‘¡(â„)ğ‘–)Ãğ‘
ğ‘—=1exp(ğ‘¡(â„)ğ‘—), whereğ‘¡(â„)is a linear transformation to
compute the logits of the experts given the input space â„,ğ‘¡(â„)ğ‘–is
theğ‘–-th value of the obtained logits, which is the weight for the
ğ‘–-th expert in the current layer. Previous graph MoE models are
designed to improve fairness [ 22], robustness [ 41,49,50,52], and
their practical use to real-world [ 25,51]. Different from them, we
utilize MoE to improve the generalization of node pairs in this work.
Lipschitz Constant [ 14].A functionğ‘“:Rğ‘›â†’Rğ‘šis established
to be Lipschitz continuous on an input set XâŠ†Rğ‘›if there exists a
constantğ¾â‰¥0such that for all ğ‘¥,ğ‘¦âˆˆX,ğ‘“satisfies the following
inequality:
||ğ‘“(ğ‘¥)âˆ’ğ‘“(ğ‘¦)||â‰¤ğ¾||ğ‘¥âˆ’ğ‘¦||,âˆ€ğ‘¥,ğ‘¦âˆˆX. (3)
The smallest possible ğ¾in Equation (3) is the Lipschitz constant of
ğ‘“, denoted as Lip(ğ‘“):
Lip(ğ‘“)= sup
ğ‘¥,ğ‘¦âˆˆX,ğ‘¥â‰ ğ‘¦||ğ‘“(ğ‘¥)âˆ’ğ‘“(ğ‘¦)||
||ğ‘¥âˆ’ğ‘¦||, (4)
and we say that ğ‘“is ağ¾-Lipschitz function. Previous works such
as [35] propose to use Lipschitz bound as spectral norm regular-
ization to make the model more stable and [ 13] improve model
fairness by using Lipschitz regularizer.
4 METHOD
This section presents our proposed LIME model to increase the
GNN performance when novel category data has label noise. Fig-
ure 2 illustrates the overall design of the model. LIME consists of
two training processes. First, a Mixture-of-Experts Similarity Net-
work (SimMoE) is introduced and trained on the original graph
to predict whether two nodes are similar or not in terms of their
labels. SimMoE consists of two key modules: Mixture-of-Experts
GNN encoder (GNNMoE) and Mixture-of-Experts MLP similarity
predictor (MLPMoE). The pipeline of SimMoE can be divided into
three steps: concatenating positional embedding, encoding node
features by GNNMoE, and predicting similarity scores by MLPMoE.
In addition, we further introduce the Lipshitz bound to regularizethe model to generalize learned knowledge and avoid overfitting.
Next, we utilize the trained SimMoE to infer the graph with new
nodes and calculate accumulated similarity scores between nodes
that have the same label. Since outlier nodes with noise labels are
dissimilar to the nodes with accurate labels, they have lower accu-
mulated similarity scores. Finally, the similarity scores are taken
as the weights of node losses to enforce model discard knowledge
from nodes with wrong labels during the training process. The
details of these steps are described in the following section.
4.1 Training the Mixture-of-Experts Similarity
Network on the Original Graph
SimMoE which contains GNNMoE and MLPMoE is used to predict
whether two nodes are similar or not. The illustration of SimMoE
is shown in part (a) of Figure 2. Specifically, input node features
of the original graph Gareğ‘¿âˆˆRğ‘Ã—ğ·, whereğ‘is the number
of nodes and D is the feature dimension. ğ‘¿are concatenated to
the positional features to obtain final node features ğ‘¿ğ¹âˆˆRğ‘Ã—ğ·â€²,
whereğ·â€²is the final node feature dimension. Second, the GNNMoE
encodes features to embeddings ğ’âˆˆRğ‘Ã—ğ», whereğ»is the hidden
dimension. After that, we pair the node embeddings to obtain node
pair similarity features ğ‘¬âˆˆRğ‘ğ‘¡Ã—2ğ», whereğ‘ğ‘¡is the number of
similarity features. Finally, the node similarity features are used
to train the MLPMoE with binary similarity labels, i.e., â€œsimilar"
(two nodes have the same label) and â€œdissimilar" (two nodes have
different labels).
Concatenating Positional Embeddings. To comprehensively
learn the positional information of the subgraph, we propose to
extend node features by obtaining positional embeddings from
unsupervised positional learning methods such as Node2Vec [ 8].
Specifically, the Node2Vec model learns the positional information
from the adjacency matrix ğ‘¨, and the learned embeddings ğ‘¿ğ‘ƒof
nodes is concatenated to original node features ğ‘¿to obtain final
node features ğ‘¿ğ¹. This process can be formulated as:
ğ‘¿ğ‘ƒ=Node2Vec(ğ‘¨),ğ‘¿ğ¹=CONCAT(ğ‘¿ğ‘ƒ,ğ‘¿). (5)
Encoding Node Features by GNNMoE. When we employ the
SimMoE to infer the new graph with injected new nodes, there
exists a distribution shift since the model is trained on the original
graph with base classes. To improve the generalization of knowl-
edge learned by SimMoE under this condition, we incorporate the
MoE techniques in SimMoE to learn diverse representations. Sim-
MoE learns rich representations from diverse experts and during
the inference, the gate network distributes each input feature to
their most suitable expert with the best generalization to the cor-
responding input. Specifically, we divide FC-layer Linear(Â·)into
multiple expert networks to process the target representation â„.
The process is formulated as follows:
LinMoE(â„)=âˆ‘ï¸
ğ‘–âˆˆTğ‘ğ‘–(â„)Â·Linearğ‘–(â„), (6)
whereTrepresents the set of activated top- ğ‘˜expert indices. LinMoE(Â·)
combines the output of multiple expert networks. In particular, the
top-ğ‘˜activated expert indices are determined by the gate-value
4085KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xiangchi Yuan et al.
1 0 1
0 1 0
1 0 0
xÎ£Loss of
each nodes
Total 
 lossOriginal graph
New graph  w. new nodesAdjacency matrix Node features
Positional featuresLearned
embeddingsSelect node pairs to
construct similarity 
feature s
Gating
Expert 1
Expert 2
Expert Mâ€¦x
xÎ£N layers
Similarity
scores
Adjacency matrix Node features
10 1 0
01 0 1
10 0 0
01 0 1GNN
SimMoE
inferencePositional features
Similarity
scores 
matrixAccumulated
similarity
scores for
each node New class nodes w/o label noiseBase class nodes w/o label noise
New class nodes w/ label noise
Lipschitz bound
regularization(a) Train SimMoE
(b) Train GNN MLPMoE
Gating
Expert 1
Expert 2
Expert Mâ€¦x
xÎ£N layers
GNNMoEx
Similarity
scores
Figure 2: Our framework. (a) First, SimMoE which contains GNNMoE and MLPMoE is trained on the original graph. SimMoE
is trained to predict whether two nodes are similar or not in terms of their labels. The pipeline of SimMoE can be divided
into three steps: concatenating position embedding, encoding node features by GNNMoE, and predicting similarity scores by
MLPMoE. (b) Second, GNN is trained for classification with the support of weighted loss. We utilize the trained SimMoE to
infer the graph injected by new nodes and calculate accumulated similarity scores for each node, where lower scores indicate
wrong labels. Finally, the similarity scores are taken as the weight of node losses to enforce model discard knowledge from
nodes with wrong labels during the training process.
ğ‘ğ‘–(â„ğ‘£), which can be obtained using a softmax function:
ğ‘ğ‘–(â„)=exp(ğ‘¡(â„)ğ‘–+ğœ€ğ‘–)
Ãğ‘ğ¸
ğ‘˜=1exp(ğ‘¡(â„)ğ‘˜+ğœ€ğ‘˜), (7)
whereğ‘¡(Â·)is a linear transformation, and ğ‘ğ¸is the number of all
experts. The activated expert indices in the LinMoE module are
determined by a gate-value ğ‘ğ‘–(â„). To illustrate, ğ‘ğ‘–(â„)takes the
logits of all experts, obtained through a linear transformation of
the input feature vector â„, and computes the activation probability
of each expert. The logits are weighted by the ğ‘–-th valueğ‘¡(â„)ğ‘–of
the linear transformation, and a random noise term ğœ€ğ‘–is added to
ensure randomness in the expert activating procedure. ğœ€ğ‘–is typically
chosen to be a sample from a Gaussian distribution. Each layer of
the GNNMoE encoder first transforms the features of the target
node and its neighboring nodes using the LinMoE(ğ‘™)(Â·)and then
aggregates the transformed features of the neighboring nodes using
AGG(Â·), which can be formulated as follows:
â„(ğ‘™)
ğ‘£=COMB(ğ‘™)
LinMoE(ğ‘™)(â„(ğ‘™âˆ’1)
ğ‘£),
AGGn
LinMoE(ğ‘™)(â„(ğ‘™âˆ’1)
ğ‘¢),âˆ€ğ‘¢âˆˆğ‘ğ‘£o
,(8)where AGG(Â·)andCOMB(Â·)represent the neighbor aggregation
and combination functions, respectively. ğ‘ğ‘£denotes the set of
neighboring nodes of node ğ‘£, andâ„(ğ‘™âˆ’1)
ğ‘£ is the node representation
at theğ‘™-th layer.
Predicting Similarity Scores by MLPMoE. After obtaining node
embeddings ğ’(the output of the last layer of GNNMoE), we pair
node embeddings to form node similarity features ğ‘¬. To decrease
the complexity, ğ‘¬are constructed by randomly concatenating two
node embeddings, which is a random sampling process to sample
ğ‘ğ‘¡node pairs from total ğ‘2node pairs. We also discuss different
sampling strategies in Appendix ??. MLPMoE takes ğ‘¬as the input
to output similarity scores for each node pair. Same with GNNMoE,
we need to improve the generalization of the similarity predictor
when we employ the SimMoE to infer the new graph. Therefore,
we incorporate LinMoE to formulate MLPMoE, which updates the
node pair similarity representation ğ‘¬(ğ‘™)in layerğ‘™as follows:
ğ‘¬(ğ‘™)=LinMoE(ğ‘¬(ğ‘™âˆ’1)). (9)
Although the MLPMoE improves the generalization, as the training
process iterates, the model still gradually overfits the training data
while losing generalization. To solve this problem, we further intro-
duce the Lipschitz bound to gain better generalization by stabilizing
4086Graph Cross Supervised Learning via Generalized Knowledge KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
the output of the MLPMoE when the distribution shift happens
during model inference. Here we first analyze MLPMoE and give
its theoretical Lipschitz bound, then utilize it to regularize the train-
ing process. Specifically, consider MLPMoE to be represented as
ğ‘“:ğ‘¬âˆˆRğ‘ğ‘¡Ã—ğ¹inâ†’ğ’€âˆˆRğ‘ğ‘¡Ã—ğ¹out, where ğ‘¬is the input feature
matrix, ğ’€is the output matrix, and ğ‘ğ‘¡is the number of similarity
features. To analyze the stability of the model output, we examine
the Lipschitz bound of the Jacobian matrix of MLPMoE by intro-
ducing the following lemma and proposition. The detailed proofs
are provided in Appendix A.
Lemma 4.1. Letğ‘”be a Lipschitz continuous function. Denote ğ‘”ğ‘–
to be theğ‘–-th layer ofğ‘”,ğ‘–=1,Â·Â·Â·,ğ‘š. Then the Lipschitz constant of
functionğ‘”satisfies:
Lip(ğ‘”)â‰¤[Lip(ğ‘”ğ‘–)]ğ‘š
ğ‘–=1, (10)
where[Lip(ğ‘”ğ‘–)]ğ‘š
ğ‘–=1denotes theğ‘š-dimensional vector whose ğ‘–-th com-
ponent is Lip(ğ‘”ğ‘–).
Lemma 4.1 establishes the connection between the Lipschitz con-
stant of the entire model and those of its constituent subnetworks
in a cascaded arrangement. Based on Lemma 4.1, we present the
following proposition:
Proposition 4.2. Letğ’€be the output of an ğ¿-layer MLPMoE
(represented in ğ‘“(Â·)) with ğ‘¬as the input. Assuming the activation
function (represented in ğœŒ(Â·)) is ReLU with a Lipschitz constant of
Lip(ğœŒ)=1, then the global Lipschitz constant of the Mixture-of-
Experts network, denoted as Lip(ğ‘“), satisfies the following inequality:
Lip(ğ‘“)â©½max
ğ‘—ğ¿Ã–
ğ‘™=1ğ¹ğ‘™hÃğ¾ğ‘™
ğ‘˜=1ğ‘ğ‘˜
ğ‘™Jğ‘˜(â„ğ‘™)i
ğ‘—âˆ, (11)
whereğ¹ğ‘™represents the output dimension of the ğ‘™-th Mixture-of-
Experts layer which contains ğ¾ğ‘™experts;ğ‘—is the index of the node;
ğ‘˜is the index of the expert in the ğ‘™-th layer;ğ‘ğ‘˜
ğ‘™is the gate value for
ğ‘˜-th expert in the ğ‘™-th layer and the vectorh
Jğ‘˜(â„ğ‘™)i
is as follows:
h
Jğ‘˜(â„ğ‘™)i
=hğ‘±ğ‘˜
1(â„ğ‘™),ğ‘±ğ‘˜
2(â„ğ‘™),Â·Â·Â·,ğ‘±ğ‘˜
ğ¹ğ‘™(â„ğ‘™)i
. (12)
Notably, ğ‘±ğ‘˜
ğ‘–(â„ğ‘™)denotes the input and output of the ğ‘–-th row of the
Jacobian matrix of the ğ‘˜-th expert in the ğ‘™-th layer.
Proposition 4.2 provides a method to estimate the Lipschitz con-
stants Lip(ğ‘“)of similarity network ğ‘“(Â·)by estimating Lipschitz
constants of subnetworks. However, it is challenging to consider
the potential cumulative effect in the similarity network. There-
fore, we consider MLPMoE as a unitary model and directly de-
rive the Lipschitz bound from the input and output of MLPMoE.
To achieve this, we define the Jacobian matrix of ğ‘–-th nodes pair
similarity feature as [ğ‘±ğ‘–]ğ¹outÃ—ğ¹in=h
ğ‘±âŠ¤
ğ‘–1,ğ‘±âŠ¤
ğ‘–2,Â·Â·Â·,ğ‘±âŠ¤
ğ‘–ğ¹outiâŠ¤
, where
ğ‘±ğ‘–ğ‘—=hğœ•ğ’€ğ‘–ğ‘—
ğœ•ğ‘¬ğ‘–1,ğœ•ğ’€ğ‘–ğ‘—
ğœ•ğ‘¬ğ‘–2,Â·Â·Â·,ğœ•ğ’€ğ‘–ğ‘—
ğœ•ğ‘¬ğ‘–ğ¹iniâŠ¤
. Then we defineJas follows:
J=
JâŠ¤
1,JâŠ¤
2,Â·Â·Â·,JâŠ¤
ğ‘âŠ¤, (13)
whereJğ‘–=[âˆ¥ğ‘±ğ‘–1âˆ¥,âˆ¥ğ‘±ğ‘–2âˆ¥,Â·Â·Â·,âˆ¥ğ‘±ğ‘–ğ¹outâˆ¥]âŠ¤. According to the defini-
tion ofJ, we take the ğ‘™2-ğ‘›ğ‘œğ‘Ÿğ‘š for each row ofJand then takethe infinite norm for the entire Jto obtain the Lipschitz bound of
the whole MLPMoE, which is formulated as follows:
Lip(ğ‘“)=âˆ¥Jâˆ¥âˆ,2. (14)
During the training, the Lipschitz bound for the model output
is computed using the gradients and norms of the input node pair
features. We utilize this Lipschitz bound as a regularization term
in the loss function, ensuring that the modelâ€™s output stays within
the defined constraints when we generalize the MLPMoE for in-
ference. Equation (14) provides a strict Lipschitz bound from a
theoretical perspective, and in our empirical implementation, we
multiply a factorğ¿ğ‘–ğ‘(ğ¸ğ‘šğ‘–ğ‘›)
1
ğ‘Ãğ‘
1ğ¿ğ‘–ğ‘(ğ¸ğ‘–)to this Lipschitz bound to "tighten"
it, whereğ¿ğ‘–ğ‘(ğ¸ğ‘šğ‘–ğ‘›)denotes the minimal expertâ€™s Lipschitz bound,
andğ¿ğ‘–ğ‘(ğ¸ğ‘–)denotes Lipschitz bound for ğ‘–-th expert. In MoE, ex-
perts are different from each other because of different initialization
and dispatched nodes, but the overall distributions of dispatched
nodes to each expert are similar. Therefore, in this factor, we greed-
ily chase the possible smallest Lipschitz bound ( ğ¿ğ‘–ğ‘(ğ¸ğ‘šğ‘–ğ‘›)part),
while tracing the overall Lipschitz bound (1
ğ‘Ãğ‘
1ğ¿ğ‘–ğ‘(ğ¸ğ‘–)part) of
MLPMoE. The final loss for training SimMoE can be formulated as:
Lğ‘ ğ‘–ğ‘š=Lğ‘ +ğ¿ğ‘–ğ‘(ğ¸ğ‘šğ‘–ğ‘›)
1
ğ‘Ãğ‘
1ğ¿ğ‘–ğ‘(ğ¸ğ‘–)Lip(ğ‘“), (15)
whereLğ‘ is the training loss supervised by labels for similarity
features (The label of the similarity feature is similar/dissimilar if
two nodes have the same/different labels).
4.2 Training GNN on the New Graph with
Weighted Node Classification Loss
After SimMoE is trained, we employ the SimMoE to infer the graph
Gâ€²=(Vâ€²,Eâ€²,Xâ€²)which is associated with new nodes to obtain
similarity scores. Note that during inference, the process of forming
similarity features is different from training. Specifically, for nodes
Vâ€²with|C|class, we randomly sample ğ‘ğ‘–nodes from each class
to form the nodes set Vâ€²ğ‘ , where|Vâ€²ğ‘ |=ğ‘ğ‘–|C|. Then, we pair each
node fromVâ€²to each node fromVâ€²ğ‘ to construct similarity features.
Finally, we take the output of similarity scores and construct a
similarity scores matrix ğ‘ºâˆˆRğ‘Ã—ğ‘ğ‘–. The element ğ‘ºğ‘–,ğ‘—represents
the similarity score between the node ğ‘£â€²âˆˆVâ€²and nodeğ‘£â€²ğ‘ âˆˆVâ€²ğ‘ 
that has the same label with ğ‘£â€²
ğ‘–. Finally, we calculate the accumulated
similarity score ğ‘¤ğ‘£â€²=1
ğ‘Ãğ‘
ğ‘—=1ğ‘ºğ‘£â€²,ğ‘£â€²ğ‘ for the node ğ‘£â€², whereğ‘¤ğ‘£â€²is
the weight for downstream GNN classification training loss for
nodeğ‘£â€²andğ‘—representsğ‘—-th sampled node with the same labels as
nodeğ‘£â€². Formally, this process can be formulated as follows:
L=1
|Vâ€²|âˆ‘ï¸
ğ‘£â€²âˆˆVâ€²âˆ’ğ‘¤ğ‘–ğ‘¦â€²
ğ‘£â€²log[ğ‘“(ğ‘¥ğ‘£â€²)], (16)
whereğ‘¦â€²
ğ‘£â€²is the label for node ğ‘£â€². Since we calculate the accumu-
lated similarity scores between the nodes with the same label, the
outlier and minority nodes that do not belong to their labeled class
have lower accumulated similarity scores (weights). Lower weights
indicate the wrong annotations, and accordingly, these nodes con-
tribute less to learning the GNN classifier. This helps GNN avoid
being fooled by noise labels during the training process.
4087KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xiangchi Yuan et al.
Table 1: Node classification accuracy performance comparison of different methods. N.R. denotes label noise rate on novel
classes and w.o. represents no label noise. The best result is bolded and the runner-up is underlined.
Dataset
N.R. GCN [16] CP-GNN [54] NRGNN [4] RTGNN [27] GPPT [30] JacGCN [43] SimTrans [2] LIME
w
.o. 86.83 Â±0.38 86.72 Â±0.31 87.64 Â±0.40
85.61 Â±0.31 86.72 Â±0.33 85.20 Â±0.38 86.79 Â±0.22 88.34 Â±0.32
0.1
82.47 Â±0.55 83.47 Â±0.72 84.13 Â±0.48 84.94 Â±0.62
83.32 Â±0.36 83.32 Â±0.45 84.24 Â±0.43 86.94 Â±0.24 Cora
0.3
77.86 Â±0.35 78.04 Â±0.69 79.23 Â±0.48 82.95 Â±0.57
81.70 Â±0.51 79.00 Â±0.38 81.85 Â±0.40 85.37 Â±0.52
w
.o. 73.80 Â±0.32
70.92 Â±0.40 71.70 Â±0.15 72.66 Â±0.65 73.56 Â±0.42 72.75 Â±0.36 72.54 Â±0.28 75.82 Â±0.33
0.1
67.58 Â±0.76 70.68 Â±0.71 71.40 Â±0.26 72.00 Â±0.33
69.59 Â±0.56 67.73 Â±0.61 69.92 Â±0.32 75.28 Â±0.70 CiteSeer
0.3
62.98 Â±0.57 64.66 Â±0.39 63.64 Â±0.64 67.91 Â±0.57
66.38 Â±0.61 65.83 Â±0.77 67.43 Â±1.10 74.32 Â±0.32
w
.o. 94.89 Â±0.08 93.70 Â±0.88 93.91 Â±0.19 92.84 Â±0.19 92.54 Â±0.34 91.33 Â±0.46 93.07 Â±0.19 94.51 Â±0.18
0.1
91.29 Â±1.36 92.82 Â±0.20
92.67 Â±1.06 90.95 Â±0.46 92.75 Â±0.26 92.25 Â±3.22 92.58 Â±0.77 93.25 Â±0.24 A-Photo
0.3
87.99 Â±2.40 87.87 Â±4.77 92.60 Â±1.27
91.96 Â±0.74 91.03 Â±0.47 90.39 Â±3.53 90.48 Â±0.23 93.79 Â±0.28
w
.o. 89.83 Â±3.33
88.95 Â±0.40 87.84 Â±1.08 88.20 Â±1.30 90.33 Â±0.62 88.21 Â±1.04 87.61 Â±0.71 90.28 Â±1.08
0.1
85.91 Â±3.92 88.62 Â±2.14
86.51 Â±0.66 86.04 Â±3.26 88.00 Â±2.94 87.27 Â±2.36 85.91 Â±3.92 89.45 Â±0.21 A-Computer
0.3
80.44 Â±7.31 79.13 Â±5.54 83.64 Â±1.04 83.46 Â±2.81 85.83 Â±6.37
84.92 Â±5.85 83.37 Â±5.00 89.13 Â±1.22
w
.o. 26.09 Â±0.73 26.67 Â±0.51 26.87 Â±0.41 26.95 Â±0.63 26.95 Â±0.63 26.97 Â±0.34 27.33 Â±0.44 29.17 Â±0.51
0.1
25.45 Â±0.46 26.37 Â±0.52 26.12 Â±0.31 25.25 Â±0.37 27.13 Â±0.30
27.12 Â±0.68 26.88 Â±0.14 29.68 Â±0.34 Actor
0.3
25.22 Â±0.65 25.86 Â±0.26 25.70 Â±0.35 25.96 Â±0.46 26.75 Â±0.47
25.80 Â±0.77 26.71 Â±0.59 29.68 Â±0.24
w
.o. 47.20 Â±2.40
43.60 Â±1.96 40.40 Â±1.96 44.80 Â±2.04 44.40 Â±2.65 43.20 Â±2.04 46.40 Â±2.65 48.40 Â±2.65
0.1
34.00 Â±1.26 37.60 Â±1.50 36.00 Â±2.19 38.80 Â±0.98
35.20 Â±1.60 36.40 Â±1.96 37.60 Â±1.50 44.00 Â±2.83 Wisconsin
0.3
27.60 Â±1.96 33.60 Â±1.96 27.20 Â±1.60 35.60 Â±3.67
35.60 Â±3.44 26.40 Â±4.08 31.20 Â±2.40 38.80 Â±2.40
5 EXPERIMENTS
This section presents comprehensive experiments to evaluate the
effectiveness of our method when training on nodes with novel
and noise labels. The experiments aim to address the following
key research questions: RQ-1 : Can our method outperform other
state-of-the-art (SOTA) methods in terms of graph cross-supervised
learning? RQ-2 : What is the contribution of each component w.r.t.
robustness against noise labels? RQ-3 : How does our method gen-
eralize similarity when data is out-of-distribution? RQ-4 : How do
similarity scores enhance representations learning on the graph?
5.1 Setup
Datasets. We use six widely used PyG benchmark datasets with
different graph types, i.e., Cora ,CiteSeer ,A-Computers ,A-Photo ,
Actor , and Wisconsin , to evaluate our method. In the experiments,
we randomly split datasets into 80% for training, 10% for validation,
and 10% for testing. To evaluate the performance of LIME on large
graphs, we use Flickr ,Reddit , and AMiner datasets with default
splits from GRB [ 57] benchmark datasets. The statistics of datasets
used in the experiments are listed in Appendix B.
Baselines. We compare our method with five types of baselines.
First, we compare the general GNN model GCN [ 16]. Second, we
compare popular graph weakly supervised learning methods, in-
cluding CP-GNN [ 54], NRGNN [ 4], RTGNN [ 27]. Third, we consider
the graph prompt learning method GPPT [ 30]. Fourth, we com-
pare popularly graph few-shot learning methods GLITTER [ 36],
TENT [ 37], and COSMIC [ 38]. Note that graph few-shot learning
methods have worse performances and scalability issues compared
with other baselines, we present their results in Appendix B.4. Fi-
nally, we adapt the cross-supervised learning method SimTrans[2] for computer vision tasks to graphs. We also adapt Jaccard-
GCN [ 43] to our setting by calculating Jaccard similarity scores to
replace end-to-end similarity calculation in LIME.
Evaluation Setting. To adapt these datasets to our graph cross-
supervised learning setting, we construct two graphs from each
original graph by categories. We split the original category set Cto
half (âŒˆ|C|âŒ‰
2) base category setCğ‘and half (âŒŠ|C|âŒ‹
2) novel category set
Cğ‘›, whereCğ‘âˆªCğ‘›=CandCğ‘âˆ©Cğ‘›=âˆ…. After that, we construct
the graph with label noise rate ğ‘Ÿâˆˆ{0,0.1,0.3}for training. The
rateğ‘Ÿis formally defined as the probability of a new training node ğ‘£
having another label among novel categories. Finally, we train our
method and baselines on the constructed graph with label noises.
We evaluate node classification accuracy according to the test set
with the ground-truth labels.
Implementation. The implementation details of the LIME model
in PyTorch-style pseudocode are displayed in Appendix B.2.
5.2 Performance Comparison
To answer the first research question RQ-1, we evaluate the graph
cross-supervised performance of LIME and compare it with other
SOTA baselines. The results are reported in Table 1. According to
the table, we can observe that LIME comprehensively outperforms
other baselines under 3 different label noise rates (w.o. noise, noise
rate 0.1, and noise rate 0.3) across six graph datasets. Specifically,
when nodes with novel class have label noise, LIME demonstrates
impressive robust accuracy under two different label noise rates
across all the datasets, while other baseline methods experience a
severe decrease in accuracy as the label noise rate increases. For
instance, on the Cora and CiteSeer datasets, LIME outperforms the
most competitive baseline RTGNN by 2.00% and 3.28% when the
label noise rate is 0.1, respectively. LIME outperforms RTGNN by
4088Graph Cross Supervised Learning via Generalized Knowledge KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Ablation studies for our method on three graph datasets of varying label noise rate. N.R. denotes label noise rate on
novel classes and w.o. represents no label noise. The best result is bolded and the runner-up is underlined.
Dataset Cora CiteSeer A-Photo
N.R. w
.o. 0.1 0.3 w
.o. 0.1 0.3 w
.o. 0.1 0.3
GCN 86.83 Â±0.38
82.47 Â±0.55 77.86 Â±0.35 73.80 Â±0.32
67.58 Â±0.76 62.98 Â±0.57 94.89 Â±0.08
91.29 Â±1.36 87.99 Â±2.40
w.o. Position 87.79 Â±0.14 86.72 Â±0.35 85.12 Â±0.72 74.44 Â±0.59
74.59 Â±0.53 70.14 Â±0.43 94.59 Â±0.13 92.96 Â±0.76 93.75 Â±2.40
w
.o. MLPMoE 87.38 Â±0.25
86.53 Â±0.26 84.58 Â±0.36 76.33 Â±0.40 73.62 Â±0.28 73.17 Â±0.36 94.59 Â±0.77
92.45 Â±0.50 91.65 Â±0.80
w.o. GNNMoE 87.12 Â±0.35
86.63 Â±0.13 84.71 Â±0.39 76.93 Â±0.76 74.22 Â±0.56
73.07 Â±0.27 94.89 Â±0.44 92.68 Â±0.32
91.88 Â±1.07
w.o. Lipshitz 88.34 Â±0.27
84.54 Â±0.49 78.81 Â±0.68 75.64 Â±0.55
74.53 Â±0.42 64.72 Â±0.92 93.19 Â±0.60
91.47 Â±0.36 89.43 Â±0.80
LIME 88.34 Â±0.32 86.94 Â±0.24 85.37 Â±0.52 75.82 Â±0.33 75.28 Â±0.70 74.32 Â±0.32 94.51 Â±0.18 93.25 Â±0.24 93.79 Â±0.28
Table 3: The performances of LIME and scalable baselines on
three large-scale graphs. N.R. denotes label noise rate and
w.o. represents no label noise. The best result is bolded.
Dataset
N.R. GCN GPPT SimTrans LIME
Flickrw
.o. 52.19Â±0.45 53.34Â±0.33 51.65Â±0.26 53.96Â±0.20
0.1
48.57Â±0.90 51.03Â±1.01 50.26Â±1.57 52.82Â±0.78
0.3
41.98Â±1.25 43.27Â±0.56 45.98Â±0.48 51.39Â±2.01
Redditw
.o. 93.35Â±0.24 93.45Â±0.19 92.89Â±0.13 93.30Â±0.40
0.1
90.33Â±0.20 90.86Â±0.21 90.12Â±0.34 92.03Â±0.13
0.3
80.76Â±1.57 83.03Â±0.75 83.87Â±53 89.74Â±2.38
AMinerw
.o. 63.41Â±0.23 65.03Â±0.55 64.33Â±0.12 64.46Â±0.23
0.1
58.36Â±0.21 61.35Â±0.24 62.35Â±0.66 63.55Â±0.78
0.3
50.30Â±0.32 52.54Â±1.18 56.40Â±2.52 62.45Â±0.95
2.42% on Cora and 6.41% on CiteSeer when the label noise rate
increases to 0.3. On the Amazon dataset, compared to the most
competitive baselines, LIME outperforms the runner-up baselines
by 3.53% when the label noise rate is 0.1 and by 0.43% when the label
noise rate increases to 0.3 on the Photo dataset. On the Computer
dataset, our method outperforms the runner-up baselines by 0.83%
when the label noise rate is 0.1 and by 3.30% when the label noise
rate increases to 0.3. On the Actor dataset, LIME outperforms the
most competitive baseline GPPT by 2.55% when the label noise rate
is 0.1 and by 2.93% when the label noise rate increases 0.3. On the
Wisconsin dataset, our method outperforms the runner-up baseline
RTGNN by 5.20% when the label noise rate is 0.1 and by 3.30%
when the label noise rate increases 0.3. In addition, our LIME also
demonstrates strong representation ability on clean graphs without
label noise and outperforms other baselines. This observation indi-
cates that our model discriminates many outlier nodes and assigns
them lower loss weight during training, which helps the model
concentrate on learning representations from nodes with better
quality. In summary, the above results showcase the effectiveness of
LIME against label noise against different noise rates on the graph
datasets, facilitated by our Lipschitz bounded Mixture-of-Experts
similarity network.
5.3 Scale to Large Graphs
In the realm of graph learning, handling large-scale graphs effi-
ciently remains a formidable challenge due to the complexity and
size of real-world networks. While Section 1 reviews several graph
learning baselines designed to mitigate label noise, prominent meth-
ods such as CP-GNN [ 54], NRGNN [ 4], and RTGNN [ 27] encounterscalability issues, primarily due to Out-of-Memory (OOM) prob-
lems. This limitation underscores the need for scalable solutions
capable of accommodating the vast node sets characteristic of large
graphs. Fortunately, Our LIME method has time and memory lin-
ear complexities and is able to scale to large graphs. Letâ€™s denote
ğ‘as the number of nodes on a graph, After removing the other
constants, the time and memory complexities for training and in-
ference are both ğ‘‚(ğ‘), which is linear to the number of nodes
ğ‘. The full analysis and computational overhead can be found in
Appendix C. The linear complexity predicts the scalability of LIME,
and the practical scalability and efficacy of LIME are empirically
validated through comparative experiments on three large-scale
graph datasets. As reported in Table 3, LIME not only demonstrates
superior scalability but also maintains robust performance in sce-
narios with prevalent label noise, thereby affirming its potential as a
scalable and effective solution for graph cross-supervised problems.
5.4 Ablation Study
LIME integrates the positional embedding, the network with Mixture-
of-Experts, and the corresponding Lipshitz bound to improve the
generalization of similarity network. Thus, to answer the second
research question RQ-2, we remove each of these components and
conduct ablation experiments. The results are shown in Table 2.
In particular, we ablate the model as (a) without positional embed-
ding (w.o. Position.), (b) without Mixture-of-Experts in MLPMoE
(w.o. MLPMoE), (c) without Mixture-of-Experts in GNNMoE (w.o.
GNNMoE), (d) without the Lipshitz bound (w.o. Lipshitz), and (e)
using the vanilla GCN instead of LIME. The results show that re-
moving any component of the LIME decreases its performance
when novel classes contain noises. This highlights the critical role
each component plays in generalizing the similarity knowledge
and its ability to provide robustness against label noises. For (a)
w.o. Struc., removing the positional embedding of the similarity
network decreases LIMEâ€™s ability to learn position similarity knowl-
edge. For example, on the CiteSeer dataset, the model experiences a
0.69% loss in accuracy when the label noise rate is 0.1, and a 4.18%
loss in accuracy when the label noise rate increases to 0.3. For (b)
w.o. MLPGNN, removing the MoE in MLPMoE prevents the model
from learning diverse similarity representations, indicating lower
model capacity limits the modelâ€™s generalization to unseen data
distribution. On the Cora dataset, the removal of this component
results in a 0.41% loss in accuracy when the label noise rate is 0.1,
and a 0.79% loss in accuracy when the noise rate increases to 0.3.
For (c) w.o. GNNMoE, removing MoE in GNNMoE prevents the
4089KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xiangchi Yuan et al.
Figure 3: The classification accuracy with different numbers
of experts (left) and the inference loss on novel classes when
training on base classes on the Cora dataset (right).
model from learning diverse node representations. On the Photo
dataset, the removal of this component results in a 0.57% loss in
accuracy when the label noise rate is 0.1, and a 1.81% loss in ac-
curacy when the noise rate increases to 0.3. For (d) w.o. Lipshitz,
removing the Lipshitz bound for the Mixture-of-Experts network
decreases the modelâ€™s performance due to the loss of generalization
for similarity. On the Cora dataset, it results in a 2.40% decrease in
accuracy when the noise rate is 0.1, and a 6.56% loss in accuracy
when the noise rate decreases to 0.3. For (e) Vanilla GCN, remov-
ing all components degenerates LIME into a vanilla GCN model.
This results in a 7.51% decrease in accuracy on the Cora dataset,
an 11.34% loss in accuracy on the CiteSeer dataset, and an 8.69%
loss on the Computer dataset when the label noise rate is 0.3. This
ablation study demonstrates the contribution of each component
in the proposed LIME model and shows the effectiveness of LIME
in improving the GNNsâ€™ performance against noise labels.
5.5 What Wins Better Performance Against
Label Noise?
To address the third research question RQ-3, we investigate the
main generalizing contribution of our model provided by the Mixture-
of-Experts as well as corresponding Lipshitz bounds and analyze
how they generalize similarity knowledge.
Mixture-of-Experts. To generalize the model to node pair features
with distribution shift, we introduce the Mixture-of-Experts to the
similarity network. SimMoE model contains multiple experts, thus
it obtains diverse representations and improves the generalization.
As shown in Figure 3, models with more experts bring better per-
formance of LIME. Although too many experts decrease the model
performance because each expert is dispatched with too little data,
i.e., the data-hungry problem, it can be solved by sampling more
nodes to form more similar features in SimMoE during training.
Lipshitz Bound. To study the effectiveness of Lipshitz bound dur-
ing generalizing similarity inference when a data distribution shift
happens, we plot the loss curve in Figure 3. The results show that
as the training epoch increases, thanks to the generalization ability
learned by the Lipshitz bounded network, the loss of the similarity
network equipped with LIME continues to decrease. On the other
hand, the similarity network without the Lipshitz bound gradually
overfits the training data and loses generalization. This demon-
strates how the Lipshitz bound generalizes similarity knowledge.
Figure 4: The similarity scores of nodes with indexes on the
Cora dataset. Blue nodes denote nodes without label noise
and red nodes denote nodes with label noise. Nodes with label
noise will be recognized and separated from nodes without
label noise in terms of similarity scores by LIME. The smaller
similarity scores indicate label noise. The higher label noise
rate makes it harder to recognize nodes with label noise.
Figure 5: T-SNE visualization depicting the node representa-
tions obtained by vanilla GCN and LIME on the Cora dataset.
GCN is fooled by label noise and wrongly clusters node rep-
resentations even if these nodes have different ground-truth
labels (in the red dashed circle). However, our LIME model
still performs well on the clustering of these nodes with label
noise (in the blue dashed circle).
5.6 How Similarity Scores Enhance
Cross-Supervised Learning against Noise
To address the fourth research question RQ-4, we explore how ob-
tained similarity scores enhance the graph representation learning
when the noise is assigned to the labels. As Figure 4 shows, the
similarity network outputs the scores for each node. Generally, the
similarity scores for nodes without label noise (blue) are higher
than the scores for nodes with label noise (red). When the label
noise rate is low, i.e. 0.1, nodes with label noise will be easily recog-
nized and separated from nodes without label noise. When the label
noise rate increases to 0.3, the scores for nodes with and without
label noise are relatively closer, which indicates that more label
noise hinders the right calculation of similarity scores. However,
we observe that the average similarity scores for nodes with label
4090Graph Cross Supervised Learning via Generalized Knowledge KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
noise are still lower than that for nodes without label noise. After
obtaining the similarity scores, we take them as the weight of each
nodeâ€™s loss to minimize the negative effect of nodes with label noise.
5.7 Embedding Visualization
Noise labels mistakenly guide the model to map node embeddings
close even if they have different ground-truth labels. As Figure 5
shows, vanilla GCN is fooled by label noise. It wrongly clusters
node representations even if these nodes have different true labels.
This phenomenon becomes even worse when the label noise rate
increases from 0.1 to 0.3. In contrast, LIME performs well on the
clustering of these nodes with label noises. Moreover, LIME also
maps some node representations to the cluster (class) that has a
different label and no label noises, while having better performance
under w.o. noise setting in Table 1. We explain this phenomenon
as LIME can assign lower loss weight to some outlier or wrongly
labeled training nodes and learn better representations.
6 CONCLUSION
In this paper, we are the first to formulate one practical problem,
Graph Cross-Supervised (Weak-Shot) Learning, from the real world,
which describes the need to model new nodes with novel classes
and potential label noises. To solve this problem, we propose LIME,
a novel model to encode new nodes and handle label noises. In par-
ticular, LIME trains the similarity network to capture the knowledge
from the original classes, aiming to obtain insights for the emerging
novel classes. We also employ the MoE techniques and Lipschitz
bound to increase the generalization of the similarity network. By
utilizing LIME to calculate loss weight for nodes with potential
label noise, the GNN classifier avoids the negative effect of new
nodes with noise labels. Our experimental results show the effec-
tiveness of LIME in encoding new nodes with novel categories and
the superiority in handling label noises for graph cross-supervised
learning problems and real-world applications.
7 ACKNOWLEDGMENTS
Thanks to Prof. Yang Liu at UC Santa Cruz, who supported this
work by providing many precious insights. Thanks to Yifei Wang
at Brandeis University for supporting intuitive figures in the experi-
ment section. This work was partially supported by Dartmouth, the
NSF under grants IIS-2321504, IIS-2334193, IIS2203262, IIS-2217239,
CNS-2203261, IIS-2340346, and CMMI-2146076. Any opinions, find-
ings, conclusions, or recommendations expressed in this material
are those of the authors and do not necessarily reflect the views of
the sponsors.
REFERENCES
[1]Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam
Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and
graph networks. arXiv preprint arXiv:1806.01261, 2018.
[2]Junjie Chen, Li Niu, Liu Liu, and Liqing Zhang. Weak-shot fine-grained classifi-
cation via similarity transfer. In NeurIPS, 2021.
[3]Ming Cheng, Ziyi Zhou, Bowen Zhang, Ziyu Wang, Jiaqi Gan, Ziang Ren, Weiqi
Feng, Yi Lyu, Hefan Zhang, and Xingjian Diao. Efflex: Efficient and flexible
pipeline for spatio-temporal trajectory graph modeling and representation learn-
ing. arXiv preprint arXiv:2404.12400, 2024.[4]Enyan Dai, Charu Aggarwal, and Suhang Wang. Nrgnn: Learning a label noise
resistant graph neural network on sparsely and noisily labeled graphs. In KDD,
2021.
[5]Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
Graph neural networks for social recommendation. In WWW, 2019.
[6]Zichu Fei, Qi Zhang, and Yaqian Zhou. Iterative gnn-based decoder for question
generation. In EMNLP, 2021.
[7]Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph
convolutional networks. In KDD, 2018.
[8]Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for net-
works. In KDD, 2016.
[9]William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation
learning on large graphs. In NeurIPS, 2017.
[10] Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue,
Ross Girshick, Trevor Darrell, and Kate Saenko. Lsda: Large scale detection
through adaptation. In NeurIPS, 2014.
[11] Ronghang Hu, Piotr DollÃ¡r, Kaiming He, Trevor Darrell, and Ross Girshick.
Learning to segment every thing. In CVPR, 2018.
[12] Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu
Wang, and Jie Tang. Mixgcf: An improved training method for graph neural
network-based recommender systems. In KDD, 2021.
[13] Yaning Jia, Chunhui Zhang, and Soroush Vosoughi. Aligning relational learning
with lipschitz fairness. In ICLR, 2024.
[14] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of
self-attention. In International Conference on Machine Learning, 2021.
[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
InICLR, 2015.
[16] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph
convolutional networks. In ICLR, 2017.
[17] Weicheng Kuo, Anelia Angelova, Jitendra Malik, and Tsung-Yi Lin. Shapemask:
Learning to segment novel objects by refining shape priors. In ICCV, 2019.
[18] Guohao Li, Matthias MÃ¼ller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can
gcns go as deep as cnns? In ICCV, 2019.
[19] Guohao Li, Matthias MÃ¼ller, Guocheng Qian, Itzel Carolina Delgadillo Perez,
Abdulellah Abualshour, Ali Kassem Thabet, and Bernard Ghanem. Deepgcns:
Making gcns go as deep as cnns. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2021.
[20] Guohao Li, Matthias MÃ¼ller, Bernard Ghanem, and Vladlen Koltun. Training
graph neural networks with 1000 layers. In ICML, 2021.
[21] Yan Liu, Zhijie Zhang, Li Niu, Junjie Chen, and Liqing Zhang. Mixed supervised
object detection by transferring mask prior and semantic similarity. In NeurIPS,
2021.
[22] Zheyuan Liu, Chunhui Zhang, Yijun Tian, Erchi Zhang, Chao Huang, Yanfang
Ye, and Chuxu Zhang. Fair graph representation learning via diverse mixture-of-
experts. WWW, 2023.
[23] Seth A Myers, Aneesh Sharma, Pankaj Gupta, and Jimmy Lin. Information
network or social network? the structure of the twitter follow graph. In WWW,
2014.
[24] Hoang NT, Choong Jun Jin, and Tsuyoshi Murata. Learning graph neural net-
works with noisy labels. arXiv preprint arXiv:1905.01591, 2019.
[25] Zhongyu Ouyang, Chunhui Zhang, Shifu Hou, Chuxu Zhang, and Yanfang Ye.
How to improve representation alignment and uniformity in graph-based col-
laborative filtering? In International AAAI Conference on Web and Social Media,
2024.
[26] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and
Lizhen Qu. Making deep neural networks robust to label noise: A loss correction
approach. In CVPR, 2017.
[27] Siyi Qian, Haochao Ying, Renjun Hu, Jingbo Zhou, Jintai Chen, Danny Z Chen,
and Jian Wu. Robust training of graph neural networks via noise governance. In
WSDM, 2023.
[28] Luana Ruiz, Fernando Gama, and Alejandro Ribeiro. Gated graph recurrent
neural networks. IEEE Transactions on Signal Processing, 2020.
[29] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Ge-
offrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-
gated mixture-of-experts layer. In ICLR, 2017.
[30] Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gppt: Graph
pre-training and prompt tuning to generalize graph neural networks. In KDD,
2022.
[31] Yijun Tian, Kaiwen Dong, Chunhui Zhang, Chuxu Zhang, and Nitesh V Chawla.
Heterogeneous graph masked autoencoders. In AAAI, 2023.
[32] Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh V Chawla.
Learning mlps on graphs: A unified view of effectiveness, robustness, and effi-
ciency. In ICLR, 2023.
[33] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang,
Nitesh V Chawla, and Panpan Xu. Graph neural prompting with large language
models. In AAAI, 2024.
[34] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. Graph attention networks. In ICLR, 2018.
4091KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xiangchi Yuan et al.
[35] Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks:
analysis and efficient estimation. In NeurIPS, 2018.
[36] Song Wang, Chen Chen, and Jundong Li. Graph few-shot learning with task-
specific structures. NeurIPS, 2022.
[37] Song Wang, Kaize Ding, Chuxu Zhang, Chen Chen, and Jundong Li. Task-adaptive
few-shot node classification. In KDD, 2022.
[38] Song Wang, Zhen Tan, Huan Liu, and Jundong Li. Contrastive meta-learning for
few-shot node classification. In KDD, 2023.
[39] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat:
Knowledge graph attention network for recommendation. In KDD, 2019.
[40] Yifei Wang, Shiyang Chen, Guobin Chen, Ethan Shurberg, Hang Liu, and Pengyu
Hong. Motif-based graph representation learning with application to chemical
molecules. In Informatics. MDPI, 2023.
[41] Qianlong Wen, Zhongyu Ouyang, Chunhui Zhang, Yiyue Qian, Chuxu Zhang,
and Yanfang Ye. GCVR: Reconstruction from cross-view enable sufficient and
robust graph contrastive learning. In The 40th Conference on Uncertainty in
Artificial Intelligence, 2024.
[42] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. Simplifying graph convolutional networks. In ICML, 2019.
[43] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. Adversarial examples on graph data: Deep insights into attack and defense.
InIJCAI, 2019.
[44] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. A comprehensive survey on graph neural networks. IEEE Transactions
on Neural Networks and Learning Systems, 2020.
[45] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and
Jure Leskovec. Hierarchical graph representation learning with differentiable
pooling. In NeurIPS, 2018.
[46] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. Graph contrastive learning with augmentations. In NeurIPS, 2020.
[47] Donghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. Jaket: Joint
pre-training of knowledge graph and language understanding. In AAAI, 2022.
[48] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung
Nguyen. Are graph augmentations necessary? simple graph contrastive learning
for recommendation. In SIGIR, 2022.
[49] Xiangchi Yuan, Chunhui Zhang, Yijun Tian, and Chuxu Zhang. Navigating graph
robust learning against all-intensity attacks. In The Second Workshop on New
Frontiers in Adversarial Machine Learning, 2023.
[50] Xiangchi Yuan, Chunhui Zhang, Yijun Tian, Yanfang Ye, and Chuxu Zhang.
Mitigating emergent robustness degradation while scaling graph learning. In
ICLR, 2024.
[51] Chunhui Zhang, Chao Huang, Yijun Tian, Qianlong Wen, Zhongyu Ouyang,
Youhuan Li, Yanfang Ye, and Chuxu Zhang. When sparsity meets contrastive
models: Less graph data can bring better class-balanced representations. In ICML,
2023.
[52] Chunhui Zhang, Yijun Tian, Mingxuan Ju, Zheyuan Liu, Yanfang Ye, Nitesh
Chawla, and Chuxu Zhang. Chasing all-round graph representation robustness:
Model, training, and optimization. In ICLR, 2023.
[53] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V
Chawla. Heterogeneous graph neural network. In KDD, 2019.
[54] Mengmei Zhang, Linmei Hu, Chuan Shi, and Xiao Wang. Adversarial label-
flipping attack and defense for graph neural networks. In 2020 IEEE International
Conference on Data Mining (ICDM), 2020.
[55] Yanfu Zhang, Shangqian Gao, Jian Pei, and Heng Huang. Improving social
network embedding via new second-order continuous graph neural networks.
InKDD, 2022.
[56] Da Zheng, Minjie Wang, Quan Gan, Zheng Zhang, and George Karypis. Learning
graph neural networks with deep graph library. In WWW, 2020.
[57] Qinkai Zheng, Xu Zou, Yuxiao Dong, Yukuo Cen, Da Yin, Jiarong Xu, Yang
Yang, and Jie Tang. Graph robustness benchmark: Benchmarking the adversarial
robustness of graph machine learning. In NeurIPS, 2021.
[58] Yuanyi Zhong, Jianfeng Wang, Jian Peng, and Lei Zhang. Boosting weakly
supervised object detection with progressive knowledge transfer. In ECCV, 2020.
A PROOFS
The proof of Lemma 4.1 is adapted from Lemma 1 in the research
[13], aligning the notations and the scope of this paper.
A.1 Proof of Lemma 4.1 in Section 4
Proof. We observe that
âˆ¥ğ‘”(ğ’™)âˆ’ğ‘”(ğ’š)âˆ¥
âˆ¥ğ’™âˆ’ğ’šâˆ¥=[ğ‘”ğ‘–(ğ’™)âˆ’ğ‘”ğ‘–(ğ’š)]ğ‘›
ğ‘–=1
âˆ¥ğ’™âˆ’ğ’šâˆ¥
=|ğ‘”ğ‘–(ğ’™)âˆ’ğ‘”ğ‘–(ğ’š)|
âˆ¥ğ’™âˆ’ğ’šâˆ¥ğ‘›
ğ‘–=1.(17)Furthermore, for each ğ‘–âˆˆ[ğ‘›],|ğ‘”ğ‘–(ğ’™)âˆ’ğ‘”ğ‘–(ğ’š)|
âˆ¥ğ’™âˆ’ğ’šâˆ¥â‰¤Lip(ğ‘”ğ‘–). Therefore,
we can write
Lip(ğ‘”)=sup
ğ’™â‰ ğ’šâˆ¥ğ‘”(ğ’™)âˆ’ğ‘”(ğ’š)âˆ¥
âˆ¥ğ’™âˆ’ğ’šâˆ¥
=sup
ğ’™â‰ ğ’š|ğ‘”ğ‘–(ğ’™)âˆ’ğ‘”ğ‘–(ğ’š)|
âˆ¥ğ’™âˆ’ğ’šâˆ¥ğ‘›
ğ‘–=1
â‰¤sup
ğ’™â‰ ğ’š[Lip(ğ‘”ğ‘–)]ğ‘›
ğ‘–=1=[Lip(ğ‘”ğ‘–)]ğ‘›
ğ‘–=1,(18)
â–¡
A.2 Proof of Proposition 4.2 in Section 4
Proof. Letğ’€denotes the output of an ğ¿-layer Mixture-of-Experts
network with input ğ‘¿. Assuming the commonly used ReLU activa-
tion function as the non-linear layer ğœŒ(Â·), we have Lip(ğœŒ)=1. First,
we consider the Lipschitz constant between the hidden states of
two node feature pairs output by any layer â„(Â·)inğ‘“(Â·). Letâ„(ğ‘¥1)
andâ„(ğ‘¥2)represent the hidden states of input feature ğ‘¥1andğ‘¥2,
respectively. Since two inputs have the same activated experts, we
assume each expert has a very close gate value, i.e. ğ‘ğ‘˜
ğ‘™, from a sta-
tistical perspective. By applying the triangle inequality, Lemma 4.1,
we obtain:
âˆ¥â„(ğ‘¥1)âˆ’â„(ğ‘¥2)âˆ¥
âˆ¥ğ‘¥1âˆ’ğ‘¥2âˆ¥=Ãğ¾
ğ‘˜=1ğ‘ğ‘˜[â„(ğ‘¥1)âˆ’â„(ğ‘¥2)]
âˆ¥ğ‘¥1âˆ’ğ‘¥2âˆ¥
â©½Ãğ¹â€²
ğ‘–=1Ãğ‘š
ğ‘˜=1ğ‘ğ‘˜h
â„ğ‘˜(ğ‘¥1)ğ‘–âˆ’â„ğ‘˜(ğ‘¥2)ğ‘–i
âˆ¥ğ‘¥1âˆ’ğ‘¥2âˆ¥
â©½ğ¹â€²Ã—max
ğ‘–Ãğ‘š
ğ‘˜=1ğ‘ğ‘˜[â„ğ‘˜(ğ‘¥1)ğ‘–âˆ’â„ğ‘˜(ğ‘¥2)ğ‘–]
âˆ¥ğ‘¥1âˆ’ğ‘¥2âˆ¥,
(19)
letğ‘“(ğ‘¥)ğ‘—denotes the ğ‘—-th column of the matrix ğ‘“(ğ‘¥). We denote
â„ğ‘˜
ğ‘™(Â·)as theğ‘˜-th expert of the ğ‘™-th layer inğ‘“(Â·), then by applying the
conclusion from Lemma 4.1 and leveraging the Lipschitz property
of the ReLU activation function, we have:
ğ‘“(ğ‘¥1)ğ‘—,1âˆ’ğ‘“(ğ‘¥2)ğ‘—,2
ğ‘¥ğ‘—,1âˆ’ğ‘¥ğ‘—,2â©½ğ¿Ã–
ğ‘™=1ğ¹ğ‘™â€²ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ¾ğ‘™âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜
ğ‘™Jğ‘˜(â„ğ‘™)ï£¹ï£ºï£ºï£ºï£ºï£»ğ‘—âˆ,(20)
whereğ‘¥ğ‘—,1andğ‘¥ğ‘—,2denoteğ‘—-th nodes pair similarity features in ğ‘¥1
andğ‘¥2, respectively, andh
J(â„ğ‘™)i
ğ‘—represents the ğ‘—-th node pair
similarity featureâ€™s the Jacobian matrix of the ğ‘™-th layer. Therefore,
the Lipschitz constant for the MLPMoE can be expressed as:
Lip(ğ‘“)=max
ğ‘—ğ¿Ã–
ğ‘™=1ğ¹ğ‘™â€²ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ¾ğ‘™âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜
ğ‘™ğ‘ğ‘˜
ğ‘™Jğ‘˜(â„ğ‘™)ï£¹ï£ºï£ºï£ºï£ºï£»ğ‘—âˆ. (21)
In summary, we have shown that for any two input samples ğ‘¥1
andğ‘¥2, the Lipschitz constant of the MLPMoE, denoted as Lip(ğ‘“),
satisfies:
âˆ¥ğ’€1âˆ’ğ’€2âˆ¥â©½Lip(ğ‘“)âˆ¥ğ‘¿1âˆ’ğ‘¿2âˆ¥, (22)
4092Graph Cross Supervised Learning via Generalized Knowledge KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1: LIME: A simplified PyTorch-style Pseudocode of LIME.
for epoch in range(1, M):
# Eq. (5), input: G=(A, X)
X_P = Node2Vec(A), X_F = CONCAT(X_P,X)
# Eq. (6) Encode node features
Z_n = GNNMoE (A, X_F)
# Pair node embeddings to obtain the similarity features
Z = Pair(Z_n)
# Eq.(9) Obtain similarity scores
Scores_Prediction = MLPMoE(Z)
# Eq. (14) Calculate the Lipschitz bound Lip(f)
Lipschitz_bound = Lip(MLPMoE)
# Eq. (15) Obtain final loss.
Loss_sim = L_s + Lipschitz_bound
Loss_sim.backward()
optimizer.step()
# After new node arriving, G=(A, X) becomes to G'=(A', X')
# Utilizing trained SimMoE to infer the training nodes and
obtain the similarity matrix S.
S = SimMoE_model(A',X')
# Sum each row of the similarity matrix to obtain the final
similarity score W for each node.
W = Sum (S_i)
for epoch in range(1, N):
# input: G'=(A', X')
Cls_Prediction = GNN(A',X')
# Eq. (16)Train GNN on downstream tasks with weighted loss.
Loss = W*L_s
Loss.backward()
optimizer.step()
Table 4: Statistics of datasets used in the experiments.
Name
#nodes #edges #features #classes
Cora 2,708
10,556 1,433 7
CiteSeer 3,327 9,104 3,703 6
Computers 13,752 491,722 767 10
Photo 7,650 238,162 745 8
Actor 7,600 30,019 932 5
Wisconsin 251 515 1,703 5
Flickr 89,250
449,878 500 7
Reddit 232,965 11,606,919 602 41
AMiner 659,574 2,878,577 100 18
where ğ’€denotes the output of the MLPMoE for inputs ğ‘¿. This
inequality implies that the Lipschitz constant Lip(ğ‘“)controls the
magnitude of changes in the output based on input distribution
shift. Therefore, we have established the following result:
âˆ¥ğ’€1âˆ’ğ’€2âˆ¥â©½ğ¿Ã–
ğ‘™=1ğ¹ğ‘™â€²ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ¾ğ‘™âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜
ğ‘™Jğ‘˜(â„ğ‘™)ï£¹ï£ºï£ºï£ºï£ºï£»ğ‘—âˆâˆ¥ğ‘¿1âˆ’ğ‘¿2âˆ¥.(23)
This inequality demonstrates that the Lipschitz constant of the
MLPMoE, Lip(ğ‘“), controls the magnitude of the difference in the
output ğ’€based on the difference in the input ğ‘¿. It allows us to
analyze the stability of the modelâ€™s output with respect to input
distribution shift. â–¡B IMPLEMENTATION DETAILS
B.1 Implementation Details of LIME
We report the mean and standard deviation of ten independent
runs with the same data splits and random seeds. We use two layers
for the GCN encoder and the MLP in the similarity network. Four
experts are utilized for each layer. In addition, we set the learning
rate to 0.01 and the noisy gate rate to 0.01. The epoch number for
training similarity network is 100 and for node classification is 500.
We use Adam [ 15] to optimize the model. Both the SimMoE and
the GNN classifier are implemented in PyTorch and trained on the
NVIDIA V100 GPU. The hidden sizes for GNNMoE and MLPMoE
are 32 and 16 respectively. For classification GCN, the hidden sizes
are {16, 16, 64, 64, 32, 32, 64, 64, 64} for {Cora, CiteSeer, Photo,
Computer, Actor, Wisconsin, Flickr, Reddit, AMiner} datasets. The
statistics of different datasets are listed in Table 4.
B.2 Pseudo Code
Algorithm 1 displays a PyTorch-style pseudocode of LIME model
in detail.
B.3 More Details about Baselines
Here we include more details about baselines in the experiment
part. We keep the parameters and configurations of baselines the
same as in the original paper.
â€¢GCN [ 16]:Graph Convolution Network uses a localized first-
order approximation of spectral graph convolutions as its convo-
lutional architecture.
â€¢CP-GNN [ 54]:This method proposes a defense framework that
introduces a community-preserving self-supervised task as regu-
larization to avoid overfitting on nodes with label noise.
â€¢NRGNN [ 4]:NRGNN proposes to link the unlabeled nodes
with labeled nodes of high feature similarity and utilize accurate
pseudo labels to reduce the effects of label noise.
â€¢GPPT [ 30]:For GPPT framework, pre-trained GNNs could be
applied without tedious fine-tuning to evaluate the linking prob-
ability of token pair, and produce the node classification decision.
We find this method has a strong performance when there exists
label noise.
â€¢SimTrans [ 2]:SimTrans transfers pairwise semantic similarity
from base categories to novel categories with additional adversar-
ial loss. SimTrans is originally designed for fine-grained image
classification and we adapt it to graph data.
â€¢Jaccard-GCN [ 43]:Jaccard-GCN is originally designed to em-
power the GCN model with strong robustness against graph
structure perturbation. We adapt Jaccard-GCN to our setting
by calculating Jaccard similarity scores to replace end-to-end
similarity calculation in LIME.
B.4 More Baselines
We further experiment on 3 popular graph-based few-shot learning
methods GLITTER [ 36], TENT [ 37], and COSMIC [ 38]. From experi-
ment results in Table 5, we find graph-based few-shot learning meth-
ods perform better than Vallina GCN under the label noise. However,
these methods are outperformed by the most recent graph weakly
supervised baseline RTGNN and our method. This experimental
observation aligns with previous computer vision research [ 2] on
4093KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xiangchi Yuan et al.
Table 5: The performances of graph few-shot learning base-
lines and our LIME method on the Cora dataset.
Noise
rate GCN RTGNN GLITTER TENT COSMIC LIME
0.0
86.83 85.61 84.03 85.54 86.96 88.34
0.1
82.47 84.94 83.21 82.85 84.84 86.94
0.3
77.86 82.95 80.34 80.32 82.47 85.37
cross-supervised learning, which shows that weakly supervised
learning methods are more powerful in cross-supervised learning
problems compared with few-shot learning methods.
C COMPLEXITY AND COMPUTATIONAL
OVERHEAD
Complexity. Here we analyze the time and memory complexities
of our method. Let ğ‘be the number of nodes, ğ‘‘be the size of
the hidden channels (we assume it is of the same order as the
size of the input features), ğ‘™1be the number of encoder layers
in the GNNMoE, ğ‘™2be the number of MLPMoE layers, ğ‘™3be the
number of classification GNN layers, ğ‘˜be the number of experts
for MoEs,ğ‘›be sampled nodes from each class during constructing
inference similarity matrix, our comprehensive complexity analysis
is structured as follows:
Complexities for training. For each MoE layer in SimMoE, ğ‘/ğ‘˜nodes
are distributed into each expert, resulting in ğ‘‚(ğ‘‘2ğ‘/ğ‘˜)time and
ğ‘‚(ğ‘‘2+ğ‘‘ğ‘/ğ‘˜)memory complexity. For all ğ‘˜experts, the total time
complexity is ğ‘‚(ğ‘‘2ğ‘)and memory complexity is ğ‘‚(ğ‘˜ğ‘‘2+ğ‘‘ğ‘).
The gating network is trained with ğ‘‚(ğ‘‘ğ‘˜ğ‘)time complexity and
constantğ‘‚(ğ‘‘ğ‘˜)memory complexity for gating in one layer. For
Lipschitz bound calculation, the time and memory complexities are
2ğ‘‘ğ‘. For constructing similarity features, the time complexity is
ğ‘and the memory complexity is 2ğ‘‘ğ‘Overall time complexity of
SimMoE module is ğ‘‚((ğ‘™1+ğ‘™2)ğ‘‘ğ‘(ğ‘‘+ğ‘˜)+(2ğ‘‘+1)ğ‘)and memory
complexity is ğ‘‚((ğ‘™1+ğ‘™2)ğ‘‘(ğ‘+ğ‘˜ğ‘‘+ğ‘˜)+4ğ‘‘ğ‘). For GNN classifier, the
time complexity is ğ‘‚(ğ‘™3ğ‘‘2ğ‘)and the memory complexity ğ‘‚(ğ‘™3ğ‘‘ğ‘+
ğ‘™3ğ‘‘2).
Complexities for inference. The complexity analysis of inference is
similar to the training process but no Lipschitz bound calculation.
For constructing the similarity matrix and obtaining similarity
scores, the time complexity is 2ğ‘›ğ‘and the memory complexity is
(2ğ‘‘+1)ğ‘›ğ‘. Overall time complexity of SimMoE module is ğ‘‚((ğ‘™1+
ğ‘™2ğ‘›)ğ‘‘ğ‘(ğ‘‘+ğ‘˜)+2ğ‘›ğ‘)and memory complexity is ğ‘‚((ğ‘™1+ğ‘™2ğ‘›)ğ‘‘(ğ‘+
ğ‘˜ğ‘‘+ğ‘˜)+(2ğ‘‘+1)ğ‘›ğ‘). For GNN classifier, the time complexity is
ğ‘‚(ğ‘™3ğ‘‘2ğ‘)and the memory complexity ğ‘‚(ğ‘™3ğ‘‘ğ‘+ğ‘™3ğ‘‘2).
In conlusion, For training, the overall time complexity is ğ‘‚((ğ‘™1+
ğ‘™2)ğ‘‘ğ‘(ğ‘‘+ğ‘˜)+ğ‘™3ğ‘‘2ğ‘+(2ğ‘‘+1)ğ‘)and the memory complexity is
ğ‘‚((ğ‘™1+ğ‘™2)ğ‘‘(ğ‘+ğ‘˜ğ‘‘+ğ‘˜)+ğ‘™3ğ‘‘ğ‘+ğ‘™3ğ‘‘2+4ğ‘‘ğ‘);For inference, the
overall time complexity is ğ‘‚((ğ‘™1+ğ‘™2ğ‘›)ğ‘‘ğ‘(ğ‘‘+ğ‘˜)+2ğ‘›ğ‘+ğ‘™3ğ‘‘2ğ‘)
and the memory complexity is ğ‘‚((ğ‘™1+ğ‘™2ğ‘›)ğ‘‘(ğ‘+ğ‘˜ğ‘‘+ğ‘˜)+(2ğ‘‘+
1)ğ‘›ğ‘+ğ‘™3ğ‘‘ğ‘+ğ‘™3ğ‘‘2). After removing the constants, the time and
memory complexity for training and inference is ğ‘‚(ğ‘), which islinear to the number of nodes ğ‘. The linear complexities predict
the potential scalability and efficiency of LIME. The scalability of
LIME has been verified by Section 5.
D A BITTER LESSON FROM THE REAL
WORLD: GRAPH CROSS-SUPERVISED
LEARNING PROBLEM IS IMPORTANT
We would like to emphasize the importance and reality of the Graph
Cross-Supervised Learning problem proposed by our paper here.
Precedents in general machine learning. The cross-supervised
learning problem (also named mixed-supervised learning or weak-
shot learning) is a well-defined problem in the general machine
learning and computer vision domain. Specifically, previous works
have explored similar settings in a wide range of tasks, such as
object detection [ 10,21,58], fine-grained classification [ 2], semantic
segmentation [ 58], and instance segmentation [ 11,17]. Similarly,
learning on graphs also faces a similar problem but is challenged
by the uniqueness of graph data. Therefore, we first propose the
graph cross-supervised learning problem and give a corresponding
solution, i.e. LIME model.
Real-world graph data face graph cross-supervised learning
problems. Real-world graph web applications like social networks
and recommendation systems face the graph mixed-supervised
learning problem with two realistic intuitive pieces of evidence.
The first evidence is realistic graphs are growing rapidly (more new
nodes with more new classes arriving): the real-world report1have
shown social network BeRealâ€™s 1,200% increase in Gen Z usage As
of April 2023; the report2shows that the number of Boomers whoâ€™d
used TikTok grew by 164%. The second piece of evidence is realistic
graphs are facing more noise like bots compared to before (label
noise rate would increase): the real-world report3have shown that
bots present on social graphs like Twitter at high intensities (20%);
the report4cited Carnegie Mellon research indicating that 30 to 49
percent of accounts tweeting about the protests were bots. These
two pieces of evidence indicate that, with real-world graphs increas-
ing rapidly and more disinformation/noise spread, itâ€™s impossible
to maintain accurate, high-quality, low-efficient annotations on
new nodes with new classes compared to the beginning of anno-
tation, especially for realistic graph applications, that maintained
graph data with new node classes are labeled through cheap and
easy-maintaining approaches like crowdsourcing, pseudo labels by
clustering, and annotators without too much expertise, which leads
to the label noise. Therefore, there exist imbalanced label noise rates
between base class nodes (nearly no noise) and new class nodes
(w. noise). We abstract this problem to the graph cross-supervised
learning problem: base class nodes are labeled accurately while new
class nodes are labeled with noises.
1Top Social Media Statistics And Trends Of 2023, Forbes, May 18, 2023.
2The Fastest Growing Social Media Platforms of 2023, Hubspot, April 10, 2023.
3Twitter Bots Poised to Spread Disinformation Before Election, The New York Times,
October 29, 2020.
4Whoâ€™s a Bot? Whoâ€™s Not? The New York Times, June 16, 2020.
4094