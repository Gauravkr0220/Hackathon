Enhancing Contrastive Learning on Graphs with Node Similarity
Hongliang Chi
chih3@rpi.edu
Rensselaer Polytechnic Institute
Troy, New York, USAYao Ma
may13@rpi.edu
Rensselaer Polytechnic Institute
Troy, NewYork, USA
Abstract
Graph Neural Networks (GNN) have proven successful for graph-
related tasks. However, many GNNs methods require labeled data,
which is challenging to obtain. To tackle this, graph contrastive
learning (GCL) have gained attention. GCL learns by contrasting
similar nodes (positives) and dissimilar nodes (negatives). Current
GCL methods, using data augmentation for positive samples and
random selection for negative samples, can be sub-optimal due to
limited positive samples and the possibility of false-negative sam-
ples. In this study, we propose an enhanced objective addressing
these issues. We first introduce an ideal objective with all positive
and no false-negative samples, then transform it probabilistically
based on sampling distributions. We next model these distributions
with node similarity and derive an enhanced objective. Compre-
hensive experiments have shown the effectiveness of the proposed
enhanced objective for a broad set of GCL models1.
CCS Concepts
â€¢Computing methodologies â†’Machine learning; â€¢Machine
learningâ†’Machine learning approaches; Neural networks.
Keywords
Graph Neural Networks, Self-Supervised Learning, Graph Con-
trastive Learning
ACM Reference Format:
Hongliang Chi and Yao Ma. 2024. Enhancing Contrastive Learning on
Graphs with Node Similarity. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671898
1 Introduction
Graphs are regarded as a type of essential data structure to represent
many real-world data, such as social networks [ 9,11] and trans-
portation networks [ 34] etc. Graph neural networks (GNNs) [ 21,37,
39], which generalize deep neural networks to graphs, have demon-
strated their great power in graph representation learning, thus
facilitating many graph-related tasks from various fields including
1Code is available at https://github.com/frankhlchi/SimEnhancedGCL
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671898recommendations [ 8,43], drug discovery [ 25,31], and computer
vision [ 10,30]. Most GNN models are trained in a supervised set-
ting, which receives guidance from labeled data. However, in real-
world applications, labeled data are often difficult to obtain while
unlabeled data are abundantly available [ 18]. Hence, to promote
GNNsâ€™ adoption in broader real-world applications, it is of great
significance to develop graph representation learning techniques
that do not require labels. More recently, contrastive learning tech-
niques [ 5,13,15], which are able to effectively leverage the unla-
beled data, have been introduced for learning node representations
with no labels available [41].
Graph contrastive learning (GCL) aims to map nodes into an
embedding space where nodes with similar semantic meanings
are embedded closely together while those with different semantic
meanings are pushed far apart. More specifically, to achieve this
goal, each node in the graph is treated as an anchor node. Then,
nodes with similar semantics to this anchor are identified as pos-
itive samples while those with different semantic meanings are
regarded as negative samples. The commonly used objective for
GCL, based on InfoNCE, treats a single node as the positive sample,
typically generated through data augmentation that alters the orig-
inal graph, while negative samples are uniformly selected from the
graph. The goal is to bring the positive sample closer to the anchor
node and distance the negative samples from the anchor. However,
this objective has two main shortcomings: (1) The set of negative
samples often includes nodes that are semantically similar to the
anchor (false-negative samples). Minimizing the contrastive objec-
tive can undesirably push these nodes away, negatively affecting
the quality of the embeddings. Hence, removing false-negative sam-
ples has the potential to improve the performance of contrastive
learning, which is also demonstrated in [ 6]; (2) The contrastive
objective includes only a single positive sample derived from data
augmentation, limiting its ability to group similar nodes effectively.
Preferably, including more positive samples in the numerator bene-
fits the contrastive learning process, which is verified in [20].
An ideal contrastive objective would include all positive samples
and exclude any false-negative samples (see details in Section 3.1).
However, this is unattainable without ground truth labels. In our
study, we introduce an enhanced objective that approximates the
ideal objective. In particular, we first transfer the ideal objective
into a probabilistic form by modeling the anchor-aware distribu-
tions for sampling positive and negative samples. Intuitively, nodes
with higher semantic similarity to the anchor node should have a
higher probability to be selected as positive samples. Hence, we
estimate these anchor-aware distributions by theoretically relating
them with node similarity. Measuring node similarity is challenging
since it involves both graph structure and node features, which
interact with each other in a complicated way. Correspondingly,
we propose a novel strategy to model the pairwise node similarity
 
456
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hongliang Chi and Yao Ma
by effectively utilizing both graph structure and feature informa-
tion. With these estimated distributions, the probabilistic objective
is then empirically estimated with samples, which makes the en-
hanced objective applicable. Our key contributions are summarised
as follows:
â€¢Introduction of an Ideal GCL Objective: We introduce an
ideal contrastive objective for GCL that effectively incorporates
all positive samples and eliminates false negatives.
â€¢Derivation of an Enhanced GCL Objective: We probabilisti-
cally approximate the ideal objective, resulting in an enhanced
objective that requires fewer samples for practical estimation.
This enhancement is achieved through rigorous asymptotic anal-
ysis and by leveraging both graph and features information to
accurately model anchor-aware distributions.
â€¢Comprehensive Experimental Validation: Extensive experi-
ments validate our enhanced objectiveâ€™s effectiveness and confirm
the importance of our enhanced objectiveâ€™s two key components,
positive & negative weights, and also the necessity of dual graph
& feature information used in modelling anchor-aware distribu-
tions.
â€¢Application Beyond GCL: We extend our methodology to en-
hance the neighborhood contrastive loss in the semi-supervised
Graph-MLP model, further validating the effectiveness of our ap-
proach.
2 Preliminary
This section introduces basic notations and key concepts founda-
tional to our discussions on GCL. Let G={V,E}denote a graph
withVandEdenoting its set of nodes and edges, respectively. The
edges describe the connections between the nodes, which can be
summarized in an adjacency matrix Aâˆˆ{0,1}|V|Ã—|V|with|V|
denoting the number of nodes. The ð‘–,ð‘—-th element of the adjacency
matrix is denoted as A[ð‘–,ð‘—]. It equals 1only when the nodes ð‘–and
ð‘—connect to each other, otherwise 0. Each node ð‘–âˆˆV is associated
with a feature vector xð‘–.
GCL aims to learn high-quality node representations by contrast-
ing semantically similar and dissimilar node pairs. More specifically,
given an anchor node ð‘£âˆˆV, those nodes with similar semantics
asð‘£are considered positive samples, while those with dissimilar
semantics are treated as negative samples. The goal of GCL is to pull
the representations of those semantically similar nodes close and
push the semantically dissimilar ones apart. From the perspective
of a single anchor node ð‘£, the goal can be achieved by minimizing
the following objective L(ð‘£).
âˆ’logð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+Ã
ð‘£ð‘ âˆˆN(ð‘£)ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£ð‘ )/ðœ, (1)
whereðœis the temperature hyper-parameter, ð‘£â€²is the positive sam-
ple, which is typically generated by data augmentation, ð‘“(Â·)is a
function that maps a node ð‘£to its low-dimensional representation,
andN(ð‘£)denotes the negative samples corresponding to the anchor
ð‘£. The overall objective for all nodes is a summation of L(ð‘£)over
all nodes inV. Next, we briefly introduce the positive sample, the
ð‘“(Â·)function, and the set of negative samples as follows. To create
positive samples, graph augmentations like topology and feature
transformations are employed. For example, GRACE [50] uses edgeremoval and feature masking for augmentation. Advanced methods,
such as GCA[51], focus on adaptive augmentations prioritizing less
important features and edges.
Many GCL frameworks utilize two augmented graphs of the
original graphGas two views. Nodes from these views serve as
anchor nodes, with the corresponding node in the other view as
its positive sample. The negative sample set consists of nodes from
both views.
3 Methodology
The objective in Eq. (1), despite its widespread use and strong per-
formance, has inherent limitations. It only employs one positive
sample for each anchor node, potentially restricting the quality
of the learned representations. The uniform negative sampling
can also introduce â€œfalse-negativeâ€ samples, undermining repre-
sentation quality. To address these shortcomings, we introduce
an enhanced objective. We first describe an ideal objective in Sec-
tion 3.1, which incorporates more positive samples and eliminates
â€œfalse-negativeâ€ samples. We then detail a strategy for practically
estimating this ideal objective, emphasizing the need for modeling
anchor-aware distributions for both positive and negative sampling.
Using pairwise node similarity, we effectively model these distribu-
tions, as detailed in Section 3.3. Our proposed enhanced objective
is discussed further in Section 3.4.
3.1 The Ideal Objective for GCL
To address the limitations of the conventional objective in Eq. (1),
an ideal objective that enjoys the capability of learning high-quality
representations would include all positive nodes in the numerator
while only including true negative samples in the denominator.
More specifically, such an ideal objective Lð‘–ð‘‘ð‘’ð‘Žð‘™(ð‘£)for an anchor
nodeð‘£could be formulated as follows.
âˆ’logÃ
ð‘£ð‘—âˆˆV 1[ð‘¦=ð‘¦ð‘—]ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£ð‘—)/ðœ
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+Ã
ð‘£ð‘—âˆˆV 1[ð‘¦â‰ ð‘¦ð‘—]ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£ð‘—)/ðœ, (2)
whereð‘¦denotes the ground truth label for node ð‘£andð‘¦ð‘—is the
label ofð‘£ð‘—, and 1[ð‘Ž]is an indicator function, which outputs 1if and
only if the argument ð‘Žholds true, otherwise 0.
Nevertheless, the objective function in Eq. (2)is not achievable,
as it is impossible to know the semantic classes of the downstream
tasks in the contrastive training process, let alone the ground-truth
labels. Hence, to make the objective more practical, in this pa-
per, following the assumptions in [ 2,6,32], we assume there are
a set of discrete latent classes Cstanding for the true semantics
of each node. We use â„Ž:V â†’C to denote the function map-
ping a given node to its latent class. For a node ð‘£âˆˆV ,â„Ž(ð‘£)de-
notes its latent class. Then, we introduce two types of anchor-
aware sampling distributions over the entire node set V. Specifi-
cally, for an anchor node ð‘£, we denote the probability of observing
any nodeð‘¢sharing the same latent class (i.e., ð‘¢is a positive sam-
ple corresponding to ð‘£) asð‘+ð‘£(ð‘¢)=ð‘(ð‘¢|â„Ž(ð‘¢)=â„Ž(ð‘£)). Similarly,
ð‘âˆ’ð‘£(ð‘¢)=ð‘(ð‘¢|â„Ž(ð‘¢)â‰ â„Ž(ð‘£))denotes the probability of observing
ð‘¢as a negative sample corresponding to ð‘£. Note that the subscript
inð‘+ð‘£andð‘âˆ’ð‘£indicates that they are specific to the anchor node ð‘£.
With these distributions, we estimate the objective in Eq. (2)with
positive and negative nodes sampled from the two distributions.
 
457Enhancing Contrastive Learning on Graphs with Node Similarity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Specifically, we estimate the objective Lð‘’ð‘ ð‘¡(ð‘£)as follows.
E{ð‘£+
ð‘—}ð‘š
ð‘—=1âˆ¼ð‘+
ð‘£,
{ð‘£âˆ’
ð‘˜}ð‘›
ð‘˜=1âˆ¼ð‘âˆ’
ð‘£ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âˆ’logð‘™
ð‘šÃð‘š
ð‘—=1ð‘’ð‘“(ð‘£)âŠ¤ð‘“
ð‘£+
ð‘—
/ðœ
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+ð‘ž
ð‘›Ãð‘›
ð‘˜ð‘’ð‘“(ð‘£)âŠ¤ð‘“
ð‘£âˆ’
ð‘˜
/ðœï£¹ï£ºï£ºï£ºï£ºï£ºï£»,(3)
where{ð‘£+
ð‘—}ð‘š
ð‘—=1and{ð‘£âˆ’
ð‘˜}ð‘›
ð‘˜=1denote the set of â€œpositive nodesâ€ and
â€œnegative nodesâ€ sampled following ð‘+ð‘£andð‘âˆ’ð‘£, respectively; and ð‘š
andð‘›denotes the number positive and negative samples, respec-
tively. As similar to [ 6], for the purpose of asymptotic analysis, we
introduce two weight parameters ð‘™andð‘ž. Whenð‘šandð‘›are finite,
we setð‘™=ð‘›andð‘ž=ð‘š, which ensures that Eq. (3)follows the same
form as Eq. (2).
Though Eq. (3) is more practical than Eq. (2), its applicability is
hindered by two main challenges:
â€¢Lack of Access to Anchor-Aware Distributions: We do
not have access to the two anchor-aware distributions ð‘+ð‘£
andð‘âˆ’ð‘£
â€¢High Sampling Complexity for Accurate Estimation:
Even if we were to know these distributions, accurately es-
timating the expectation in the enhanced objective would
require a significant number of samples.
To effectively address the identified challenges in estimating the
ideal objective our method incorporates the following strategies:
â€¢Dual Information for Modeling Anchor-Aware Distri-
butions: To address the first issue, we propose leveraging
both graph structure and feature information. Since directly
modeling the distribution over all nodes in the graph is ex-
tremely difficult, we propose to connect the probabilities
ð‘+ð‘£(ð‘¢)andð‘âˆ’ð‘£(ð‘¢)of a specific node ð‘¢with node similarity
between node ð‘¢and the anchor node ð‘£. This similarity is
modeled using dual graph and feature information. More
details on modeling anchor-aware distributions will be dis-
cussed in Section 3.3.
â€¢From Asymptotic Analysis to Practical Estimation: For
the second challenge, we adopt an asymptotic analysis, which
leads to a new objective requiring fewer samples for esti-
mation. The specifics of this analysis and the new objective
are discussed in Section 3.2. Next, we first discuss how we
address the second challenge assuming we are given the two
sets of anchor-aware distributions ð‘+ð‘£andð‘âˆ’ð‘£in Section 3.2
and then discuss how we model the anchor-ware distribu-
tionsð‘+ð‘£andð‘âˆ’ð‘£in Section 3.3. The enhanced objective will
be discussed in Section 3.4.
3.2 Efficient Estimation of the Objective
To allow a more efficient estimation of Eq. (3), we consider its
asymptotic form by analyzing the case where ð‘šandð‘›go to infinity,
which is summarized in the following theorem.Theorem 3.1. For fixedð‘™andð‘ž, asð‘š,ð‘›â†’âˆž, it holds that:
E{ð‘£+
ð‘—}ð‘š
ð‘—=1âˆ¼ð‘+
ð‘£
{ð‘£âˆ’
ð‘˜}ð‘›
ð‘˜=1âˆ¼ð‘âˆ’
ð‘£
âˆ’logð‘™
ð‘šÃð‘š
ð‘—=1ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+
ð‘—)/ðœ
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+ð‘ž
ð‘›Ãð‘›
ð‘˜=1ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£âˆ’
ð‘˜)/ðœ
â†’âˆ’ logð‘™Eð‘£+âˆ¼ð‘+ð‘£(ð‘£+)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+)/ðœ]
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+ð‘žEð‘£âˆ’âˆ¼ð‘âˆ’ð‘£(ð‘£âˆ’)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£âˆ’)/ðœ].
(4)
Proof. Asðœis a nonzero scalar, the contrastive objective is
bounded. Thus, we could apply the Dominated Convergence Theo-
rem to prove the theorem above as follows:
limð‘šâ†’âˆžlimð‘›â†’âˆžEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âˆ’logð‘™
ð‘šÃð‘š
ð‘—=1ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+
ð‘—)/ðœ
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+ð‘ž
ð‘›Ãð‘›
ð‘˜=1ð‘’ð‘“(ð‘£)âŠ¤ð‘“
ð‘£âˆ’
ð‘˜
/ðœï£¹ï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°limð‘šâ†’âˆžlimð‘›â†’âˆžâˆ’logð‘™
ð‘šÃð‘š
ð‘—=1ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+
ð‘—)/ðœ
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+ð‘ž
ð‘›Ãð‘›
ð‘˜=1ð‘’ð‘“(ð‘£)âŠ¤ð‘“
ð‘£âˆ’
ð‘˜
/ðœï£¹ï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£°âˆ’logð‘™Eð‘£+âˆ¼ð‘+ð‘£(ð‘£+)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+)/ðœ]
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+ð‘žEð‘£âˆ’âˆ¼ð‘âˆ’ð‘£(ð‘£âˆ’)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£âˆ’)/ðœ]ï£¹ï£ºï£ºï£ºï£ºï£»
=âˆ’logð‘™Eð‘£+âˆ¼ð‘+ð‘£(ð‘£+)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+)/ðœ]
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+ð‘žEð‘£âˆ’âˆ¼ð‘âˆ’ð‘£(ð‘£âˆ’)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£âˆ’)/ðœ].
â–¡
As demonstrated in Theorem 1, the objective of Eq. (4)is an as-
ymptotic form of Eq. (3). In this work, we aim to empirically estimate
Eq.(4)instead of Eq. (3). Specifically, Eq. (4)contains two expecta-
tions to be estimated. Compared to Eq. (3), the sampling complexity
is significantly reduced, as we disentangled the joint distribution in
Eq.(3), and only need to estimate these two expectations indepen-
dently. More specifically, to estimate Eð‘£+âˆ¼ð‘+ð‘£(ð‘£+)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+)/ðœ], a
straightforward way is to randomly draw samples from ð‘+ð‘£and cal-
culate its empirical mean. However, it is typically inefficient and in-
convenient to obtain samples directly from ð‘+ð‘£, sinceð‘+ð‘£itself needs
to be estimated (this will be discussed in Section 3.3) and we cannot
obtain a simple analytical form to perform the sampling. The same
reason applies to the estimation of Eð‘£âˆ’âˆ¼ð‘âˆ’ð‘£(ð‘£âˆ’)[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£âˆ’)/ðœ]. There-
fore, in this work, we adopt the importance sampling strategy [ 12]
to estimate the two expectations using samples from the uniform
distribution ð‘as follows.
Eð‘£+âˆ¼ð‘+ð‘£[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+)/ðœ]=Eð‘£+âˆ¼ð‘ð‘+ð‘£(ð‘£+)
ð‘(ð‘£+)ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£+)/ðœ
â‰ˆ1
ð‘€âˆ‘ï¸
ð‘£ð‘—âˆˆVð‘€ð‘+ð‘£(ð‘£ð‘—)
ð‘(ð‘£ð‘—)ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£ð‘—)/ðœ
;(5)
Eð‘£âˆ’âˆ¼ð‘âˆ’ð‘£[ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£âˆ’)/ðœ]=Eð‘£âˆ’âˆ¼ð‘ð‘âˆ’ð‘£(ð‘£âˆ’)
ð‘(ð‘£âˆ’)ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£âˆ’)/ðœ
â‰ˆ1
ð‘âˆ‘ï¸
ð‘£ð‘—âˆˆVð‘ð‘âˆ’ð‘£(ð‘£ð‘—)
ð‘(ð‘£ð‘—)ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£ð‘—)/ðœ
,(6)
whereVð‘€={ð‘£ð‘—}ð‘€
ð‘—=1âˆ¼ð‘containsð‘€nodes sampled from ð‘and
Vð‘={ð‘£ð‘—}ð‘
ð‘—=1âˆ¼ð‘containsð‘nodes sampled from ð‘, which are
utilized for estimation. To obtain the final empirical form of Eq. (4),
 
458KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hongliang Chi and Yao Ma
the two sets of anchor-aware distributions ð‘+ð‘£andð‘âˆ’ð‘£remain to be
estimated, which is discussed in the next section.
3.3 Modeling and Estimating Anchor-Aware
Distributions
Here, we discuss the modeling details of the anchor-aware distribu-
tionsð‘+ð‘£andð‘âˆ’ð‘£. As discussed earlier in Section 3.1, for an anchor
ð‘£, the positive sample distribution is a conditional distribution re-
lying on the agreement of the latent classes of ð‘£and any other
sampleð‘¢, which can be formulated as ð‘+ð‘£(ð‘¢)=ð‘ð‘£(ð‘¢|â„Ž(ð‘£)=â„Ž(ð‘¢)).
Direct modeling this distribution is impossible, since we do not
have access to the latent semantic class. Here, we propose to model
ð‘+ð‘£(ð‘¢)with the node similarity between the anchor node ð‘£and a
given sample ð‘¢(Section 3.3 and Section 3.3.2). We then discuss the
process to evaluate node similarity with both graph structure and
node feature information in Section 3.3.3.
3.3.1 Modeling Anchor-Aware Distributions with Node Similarity.
Based on Bayesâ€™ theorem, we have
ð‘+
ð‘£(ð‘¢)âˆð‘ð‘£(â„Ž(ð‘£)=â„Ž(ð‘¢)|ð‘¢)ð‘(ð‘¢), (7)
whereð‘is a uniform distribution over all nodes, and ð‘ð‘£(â„Ž(ð‘£)=
â„Ž(ð‘¢))is the probability that ð‘¢shares the same latent semantic class
ofð‘£. Therefore, to obtain ð‘+ð‘£(ð‘¢), it is essential to model ð‘ð‘£(â„Ž(ð‘£)=
â„Ž(ð‘¢)|ð‘¢)asð‘is already known. Intuitively, if ð‘£andð‘¢are more
â€œsimilarâ€ to each other, they are more likely to share the same
semantic class. Assuming that we are given a function sim(Â·,Â·)
that measures the pair-wise similarity of any two nodes, then we
further assume that the probability ð‘ð‘£(â„Ž(ð‘£)=â„Ž(ð‘¢)|ð‘¢)is positively
correlated with sim (ð‘£,ð‘¢), which can be formulated as
ð‘ð‘£(â„Ž(ð‘£)=â„Ž(ð‘¢)|ð‘¢)âˆT( sim(ð‘£,ð‘¢)), (8)
whereTis a monotonic increasing transformation. We will discuss
the details of the transformation and the similarity function in
Section 3.3.2 and Section 3.3.3, respectively. Together with Eq. (7),
we have
ð‘+
ð‘£(ð‘¢)âˆT( sim(ð‘£,ð‘¢))ð‘(ð‘¢), (9)
which intuitively expresses that those samples that are more similar
toð‘£are more likely to be sampled as positive samples. We then
formulate the probability ð‘+ð‘£(ð‘¢)with sim(ð‘£,ð‘¢)as follows.
ð‘+
ð‘£(ð‘¢)=T(sim(ð‘£,ð‘¢))ð‘(ð‘¢)âˆ«
T(sim(ð‘£,ð‘£ð‘ ))ð‘(ð‘£ð‘ )ð‘‘ð‘£ð‘ =T(sim(ð‘£,ð‘¢))ð‘(ð‘¢)
Eð‘£ð‘ âˆ¼ð‘[T(sim(ð‘£,ð‘£ð‘ ))].(10)
Note that, in practice, Eð‘£ð‘ âˆ¼ð‘[T(sim(ð‘£,ð‘£ð‘ ))]can be empirically es-
timated using the set of samples Vð‘€in Eq. (5) as follows.
Eð‘£ð‘ âˆ¼ð‘[T(sim(ð‘£,ð‘£ð‘ ))]â‰ˆ1
ð‘€âˆ‘ï¸
ð‘£ð‘—âˆˆVð‘€T(sim(ð‘£,ð‘£ð‘—)). (11)
Then, we can estimate ð‘+(ð‘¢)as follows
Ë†ð‘+
ð‘£(ð‘¢)=T(sim(ð‘£,ð‘¢))ð‘(ð‘¢)
1
ð‘€Ã
ð‘£ð‘—âˆˆVð‘€T(sim(ð‘£,ð‘£ð‘—)), (12)where Ë†ð‘+ð‘£(ð‘¢)is the empirical estimate of ð‘+ð‘£(ð‘¢). Intuitively, given
Ë†ð‘+ð‘£(ð‘¢), we can directly estimate Ë†ð‘âˆ’ð‘£(ð‘¢)as1âˆ’Ë†ð‘+ð‘£(ð‘¢). However, this
is typically not optimal for the purpose of contrastive learning for
several reasons: 1) first, the samples in Vð‘€( in Eq. (5)) andVð‘(in
Eq.(6)) are likely different, which makes it infeasible to directly
modelð‘âˆ’ð‘£using 1âˆ’ð‘+ð‘£for all selected nodes; 2) second, we prefer
different properties of the estimations for the two distributions ð‘+ð‘£
andð‘âˆ’ð‘£for the purpose of contrastive learning. Specifically, we
prefer a relatively conservative estimation for ð‘+ð‘£to reduce the
impact of â€œfalse positivesâ€ (i.e, avoid assigning high ð‘+ð‘£for real
negative samples). In contrast, a more aggressive estimation of ð‘âˆ’ð‘£
is acceptable. Modeling a conservative ð‘+ð‘£and aggressive ð‘âˆ’ð‘£at the
same time cannot be achieved if we constrain ð‘+ð‘£(ð‘¢)+ð‘âˆ’ð‘£(ð‘¢)=1.
Due to the above reasons, in this work, we relax this constraint and
modelð‘âˆ’ð‘£flexibly using node similarity sim (Â·,Â·)as follows.
ð‘âˆ’
ð‘£(ð‘¢)=D(sim(ð‘£,ð‘¢))ð‘(ð‘¢)
Eð‘£ð‘ âˆ¼ð‘[D(sim(ð‘£,ð‘£ð‘ ))], (13)
whereDis a monotonic decreasing function, indicating that ð‘âˆ’ð‘£is
negatively correlated with the similarity. Similar to Eq. (12),ð‘âˆ’ð‘£(ð‘¢)
can be empirically estimated with ð‘samples inVð‘(described in
Eq. (6)) as follows.
Ë†ð‘âˆ’
ð‘£(ð‘¢)=D(sim(ð‘£,ð‘¢))ð‘(ð‘¢)
1
ð‘Ã
ð‘£ð‘—âˆˆND(sim(ð‘£,ð‘£ð‘—)). (14)
Next, we first discuss the details of the monotonic increasing trans-
formation function Tand the monotonic decreasing transformation
Din Section 3.3.2. We then discuss the similarity function sim(ð‘£,ð‘¢)
in Section 3.3.3.
3.3.2 Transformations. To flexibly adjust the two estimated anchor-
aware distributions Ë†ð‘+ð‘£and Ë†ð‘âˆ’ð‘£between conservative estimation to
aggressive estimation, we utilize exponential function with temper-
ature [1] to model the transformation functions as follows
T(sim(ð‘£ð‘–,ð‘£ð‘—))=exp(sim(ð‘£ð‘–,ð‘£ð‘—)/ðœð‘)âˆ’1; (15)
D(sim(ð‘£ð‘–,ð‘£ð‘—))=exp(âˆ’sim(ð‘£ð‘–,ð‘£ð‘—)/ðœð‘›), (16)
whereðœð‘andðœð‘›are two temperature parameters. We could adjust
the estimation of the two distributions Ë†ð‘+ð‘£in Eq. (12)and Ë†ð‘âˆ’ð‘£in
Eq.(14)by varyingðœð‘andðœð‘›, respectively. More specifically, for Ë†ð‘+ð‘£,
we could make the distribution more conservative by decreasing ðœð‘,
which increases the probability mass for those samples with high
similarity. In the extreme case, when ðœð‘goes to 0, the probability
mass concentrates in the sample with the largest similarity. On the
other hand, when ðœð‘reaches infinity, Ë†ð‘+ð‘£converges to a distribution
proportional to similarity. Note that without the â€œ âˆ’1â€ in Eq. (15),
Ë†ð‘+ð‘£converges to a uniform distribution as ðœð‘goes to infinity, which
leads to model collapse as all samples in Eq. (5)will be treated
equally (all treated as positive samples). Thus, we include â€œ âˆ’1â€ in
Eq.(15)to avoid such cases. Similarly, Ë†ð‘âˆ’ð‘£can be adjusted from a
uniform distribution to a distribution with mass concentrated on
the sample with the smallest similarity by varying ðœð‘›. Specifically,
whenðœð‘›goes to 0, the estimated Ë†ð‘âˆ’ð‘£converges to the uniform
distribution and the estimation in Eq. (6)reduces to the same result
as the convectional negative sampling strategy.
 
459Enhancing Contrastive Learning on Graphs with Node Similarity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
3.3.3 Modeling Node Similarity. Here, we delve into modeling node
similarity, considering both graph structure and node features. We
first outline methods for capturing each type of similarity and then
detail their integration for a comprehensive similarity function.
Graph Structure Similarity Personalized Page Rank (PPR) is a
widely adopted tool for measuring the relevance between nodes in
graph mining [ 24,28,29]. More recently, it has also been adopted
to improve graph representation learning [ 22,23]. In this work,
we utilize the PPR score to model the structural node similarity.
Specifically, the personalized PageRank matrix is defined as P=
ð›¼(Iâˆ’(1âˆ’ð›¼)Ë†A)âˆ’1, where Ë†A=Dâˆ’1/2ADâˆ’1/2,Dis the degree matrix,
andð›¼âˆˆ (0,1)is a hyper-parameter. The ð‘–,ð‘—-th element of the
PPR matrix Pdenoting as P[ð‘–,ð‘—]measures the structural similarity
between node ð‘£ð‘–and nodeð‘£ð‘—. However, calculating the matrix P
is computationally expensive, especially for large-scale graphs, as
it involves a matrix inverse. In this work, we adopt the iterative
approximation of the PPR matrix for measuring the node similarity
asË†P=(1âˆ’ð›¼)ð¾Ë†Að¾+ð¾âˆ’1Ã
ð‘˜=0ð›¼(1âˆ’ð›¼)ð‘˜Ë†Að‘˜,whereð¾is the number of
iterations. Note that, Ë†Pconverges to Pasð¾goes infinity [22].
Feature Similarity. To better mine the pairwise node similar-
ity from features, we adopt the classic cosine similarity. Specifi-
cally, feature similarity between nodes ð‘£ð‘–andð‘£ð‘—is evaluated by
simð¹(ð‘£ð‘–,ð‘£ð‘—)=cos(xð‘–,xð‘—), where xð‘–,xð‘—are the original input fea-
tures of node ð‘£ð‘–andð‘£ð‘—, respectively.
Fusing Graph and Feature Similarity. Given the structure simi-
larity simðº(ð‘£ð‘–,ð‘£ð‘—)and feature similarity simð¹(ð‘£ð‘–,ð‘£ð‘—), it is vital to
define an adaptive function sim(Â·,Â·)to fuse them and output a com-
bined similarity score capturing information from both sources.
Specifically, we propose to combine the two similarities to form
the overall similarity as sim(ð‘£ð‘–,ð‘£ð‘—)=ð›½Â·simð¹(ð‘£ð‘–,ð‘£ð‘—)Â·ð›¾+(1âˆ’
ð›½)Â·simðº(ð‘£ð‘–,ð‘£ð‘—),whereð›¾is the scaling factor to control the rel-
ative scale between the two similarity scores, and ð›½is a hyper-
parameter balancing the two types of similarity. In general, ð›¾could
also be treated as a hyper-parameter. In this work, we fix ð›¾=Ãð‘ ð‘–ð‘šðº(ð‘£ð‘–,ð‘£ð‘—)/Ãð‘ ð‘–ð‘šð¹(ð‘£ð‘–,ð‘£ð‘—)such that the two types of similarity
are at the same scale.
3.4 The Proposed Enhanced Objective
With the estimation of Ë†ð‘+ð‘£in Eq. (12)and Ë†ð‘âˆ’ð‘£in Eq. (14), we propose
an enhanced objective Lð¸ð‘(ð‘£)as follows.
âˆ’logÃ
ð‘£ð‘—âˆˆVð‘€h
ð‘¤+ð‘£(ð‘£ð‘—)ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£ð‘—)/ðœi
ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£â€²)/ðœ+Ã
ð‘£ð‘—âˆˆVð‘h
ð‘¤âˆ’ð‘£(ð‘£ð‘—)ð‘’ð‘“(ð‘£)âŠ¤ð‘“(ð‘£ð‘—)/ðœi, (17)
whereð‘¤+ð‘£(ð‘£ð‘—)andð‘¤âˆ’ð‘£(ð‘£ð‘—)are defined as follows.
ð‘¤+
ð‘£(ð‘£ð‘—)=T(sim(ð‘£,ð‘£ð‘—))
1
ð‘€Ã
ð‘£ð‘—âˆˆVð‘€T(sim(ð‘£,ð‘£ð‘˜));
ð‘¤âˆ’
ð‘£(ð‘£ð‘—)=D(sim(ð‘£,ð‘£ð‘—))
1
ð‘Ã
ð‘£ð‘—âˆˆVð‘D(sim(ð‘£,ð‘£ð‘˜)).(18)Vð‘€andVð‘are the two sets of nodes introduced in Eq. (5)and
Eq.(6). If we setVð‘€=Vð‘=V, we can make a direct comparison
between the enhanced objective in Eq. (17)and the ideal objective
in Eq. (2). Specifically, the enhanced objective can be considered
as a soft version of the ideal objective, where the weights ð‘¤+ð‘£(ð‘£ð‘—)
andð‘¤âˆ’ð‘£(ð‘£ð‘—)in Eq. (17)reflect the likelihood of ð‘£ð‘—being a positive
sample or a negative sample, respectively.
4 Beyond Graph Contrastive Learning
The philosophy of contrastive learning has inspired other frame-
works for graph representation learning. In Graph-MLP [16], an
auxiliary neighborhood contrastive loss is proposed to enhance the
performance of MLP on the semi-supervised node classification
task. As indicated by its name, the key idea of the neighborhood
contrastive loss is to treat the â€œneighboring nodesâ€ as positive sam-
ples and contrast them with their corresponding anchor nodes.
Since the neighbors are defined through graph structure, such a
loss helps incorporate the graph information into the represen-
tation learning process of MLP. It has been shown that the MLP
model equipped with the neighborhood contrastive loss is capable
of achieving performance comparable to or even stronger than
graph neural network models. In this section, we briefly describe
theGraph-MLP model with neighborhood contrastive loss and dis-
cuss how the proposed techniques discussed in Section 3 can be
utilized to further enhance this loss.
4.1 Neighborhood Contrastive Loss and
Graph-MLP
For an anchor node ð‘£ð‘–âˆˆV, the neighborhood contrastive loss is
defined as follows:
Lð‘ð¶(ð‘£ð‘–)=âˆ’logÃ
ð‘£ð‘—âˆˆVð‘1[ð‘£ð‘—â‰ ð‘£ð‘–]Ë†Að‘Ÿ[ð‘–,ð‘—]ð‘’ð‘“(ð‘£ð‘–)âŠ¤ð‘“(ð‘£ð‘—)/ðœ
Ã
ð‘£ð‘˜âˆˆVð‘1[ð‘£ð‘˜â‰ ð‘£ð‘–]ð‘’ð‘“(ð‘£ð‘–)âŠ¤ð‘“(ð‘£ð‘˜)/ðœ,(19)
whereVð‘is a set of nodes uniformly sampled from V,Ë†Að‘Ÿis theð‘Ÿ-
th power of the normalized adjacency matrix Ë†A. Theð‘–,ð‘—-th element
Ë†Að‘Ÿ[ð‘–,ð‘—]is only non-zero when node ð‘£ð‘—is within the ð‘Ÿ-hop neigh-
borhood of node ð‘£ð‘–, otherwise Ë†Að‘Ÿ[ð‘–,ð‘—]=0. Hence, in the numerator
of Eq. (19), only theð‘Ÿ-hop neighbors are treated as positive samples.
The denominator is similar to that in contrastive learning. Overall,
the neighborhood contrastive loss for all nodes in the graph can be
formulated as follows.
Lð‘ð¶=âˆ‘ï¸
ð‘–âˆˆVLð‘ð¶(ð‘£ð‘–). (20)
InGraph-MLP , the neighborhood contrastive loss is combined
with the cross-entropy loss for conventional semi-supervised node
classification asLð‘¡ð‘Ÿð‘Žð‘–ð‘› =Lð¶ð¸+ð›¼Lð‘ð¶,whereð›¼>0is a hyper-
parameter that balances the cross-entropy loss Lð¶ð¸and the neigh-
borhood contrastive loss Lð‘ð¶. When the graph is large, the neigh-
borhood contrastive loss can be calculated in a batch-wise way,
whereLð‘ð¶can be calculated over a batch of nodes as Lð‘ð¶=
 
460KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hongliang Chi and Yao Ma
Ã
ð‘£ð‘–âˆˆBLð‘ð¶(ð‘£ð‘–)withBdenoting a specific sampled batch. Corre-
spondingly, in this scenario, Vð‘in Eq. (19) can be replaced by B.
4.2 Enhanced Objective for Graph-MLP
Following the same philosophy as in Section 3, we propose the
following enhanced neighborhood contrastive loss.
Lð¸ð‘âˆ’ð‘ð¶(ð‘£ð‘–)=âˆ’logÃ
ð‘£ð‘—âˆˆVð‘1[ð‘£ð‘—â‰ ð‘£ð‘–]ð‘¤+ð‘£ð‘–(ð‘£ð‘—)ð‘’ð‘“(ð‘£ð‘–)âŠ¤ð‘“(ð‘£ð‘—)/ðœ
Ã
ð‘£ð‘˜âˆˆVð‘1[ð‘£ð‘˜â‰ ð‘£ð‘–]ð‘¤âˆ’ð‘£ð‘–(ð‘£ð‘˜)ð‘’ð‘“(ð‘£ð‘–)âŠ¤ð‘“(ð‘£ð‘˜)/ðœ,(21)
whereð‘¤+ð‘£ð‘–(ð‘£ð‘—)andð‘¤âˆ’ð‘£ð‘–(ð‘£ð‘˜)are the positive weight between nodes
ð‘£ð‘–,ð‘£ð‘—and negative weight between nodes ð‘£ð‘–,ð‘£ð‘˜as defined in Eq. (18).
We can replaceLð‘ð¶(ð‘£ð‘–)in Eq. (20)withLð¸ð‘âˆ’ð‘ð¶(ð‘£ð‘–)to form an
enhanced training framework for MLP models. We name such a
framework as Graph-MLP+ . Its superiority is empirically verified in
the experiments section (Section 5.3).
5 Experiment
In this section, we conduct experiments to verify the effectiveness
of the enhanced objectives. We also perform an ablation study to
provide a deep understanding of the proposed objectives. Specifi-
cally, we first introduce the datasets we adopt for experiments in
Section 5.1. Then, we present the significant enhanced results for
GRACE ,GCA, and Graph-MLP with discussions in Section 5.2 and Sec-
tion 5.3, respectively. The ablation study is presented in Section 5.4.
5.1 Datasets
Here, we introduce the datasets we adopt for the experiments. Fol-
lowing previous papers [50, 51, 51], we adopt 8datasets including
Cora [33],Citeseer [33],Pubmed [33],DBLP [44],A-Photo [35],
A-Computers [35],Co-CS [35], and Wiki-CS [26] for evaluating the
performance. The details of the dataset statistics are shown in the
Table 1.
Table 1: Summary of Datasets.
Dataset
Nodes Edges Features Classes
Cora 2,708
5,429 1,433 7
Citeseer 3,327 4,732 3,703 6
Pubmed 19,717 44,338 500 3
DBLP 17,716 105,734 1,639 4
A-Computers 13,752 245,861 767 10
A-Photo 7,650 119,081 745 8
Wiki-CS 11,701 216,123 300 10
Co-CS 18,333 81,894 6,805 15
5.2 Performance of Enhanced GCL models
The enhanced objective proposed in Eq. (17)is quite flexible and can
be utilized to improve the performance of various frameworks that
adopt the conventional graph contrastive learning objective. In this
work, we adopt GRACE [50], a recently proposed representative GCL
framework and its updated version, GCA[51], as base models (check
Section 2 for a brief description of GRACE andGCA). We denote theGRACE framework with the enhanced objective as GRACE+ and use
GCA+ to represent the enhanced GCA. Next, we first present results
forGCAand then describe results for GCA.
5.2.1 GRACE .Following [ 50], we conduct the experiment with
GRACE andGRACE+ on the first 4citation datasets as introduced
in Section 5.1. To evaluate the effectiveness of GRACE+ , we adopt
the same linear evaluation scheme as in [38, 50].
Here, the experiments are conducted in two stages. In the first
stage, we learn node representations with the graph contrastive
learning frameworks ( GRACE andGRACE+ ) in a self-supervised fash-
ion. Then, in the second stage, we evaluate the quality of the learned
node representations through the node classification task. Specifi-
cally, a logistic regression model with the obtained node represen-
tations as input is trained and tested. To comprehensively evalu-
ate the quality of the representations, we adopt different training-
validation-test splits. Specifically, we first randomly split the node
sets into three parts: 80%for testing, 10%for validation, and the
rest of 10%is utilized to further build the training sets. With the re-
maining 10%of nodes, we build 5different training sets that consist
of2%,4%,6%,8%,10%of nodes in the entire graph. The training set
is randomly sampled from the 10%of data for building the training
subset. In each setting, following the official published code of [ 50],
the logistic regression model is trained for 3runs with different
random initializations. Furthermore, we repeat the entire exper-
iment including both stages for 30times and report the average
performance of 90runs with standard deviation.
The results of GRACE andGRACE+ are summarized in Figure 1.
From these figures, it is clear that GRACE+ consistently outperforms
GRACE on all datasets under various training ratios. These results
validate the effectiveness of the proposed enhanced objective.
Table 2: Node classification results (%) of GCA(original model),
ProGCL (advanced baseline method) and GCA+.
Mo
del A-Computers A-Photo Wiki-CS Co-CS
GCA 87.49Â±0.39
92.03Â±0.39 76.46Â±1.30 92.73Â±0.21
ProGCL 87.58Â±0.55 92.36Â±0.33 76.40Â±0.73 93.00Â±0.21
GCA+ 88.15Â±0.40 92.52Â±0.45 78.64Â±0.18 92.82Â±0.29
5.2.2 GCA.Following [ 51], we evaluate GCAandGCA+ on4datasets
including A-Photo ,A-Computers ,Co-CS , and Wiki-CS . In addition,
we also compare GCA+ with ProGCL [42], which is a recent solid
GCL baseline aiming to enhance the loss of GCAby addressing the
issue of â€œfalse negativeâ€ samples. We adopt the hyper-parameters
provided in [42] for running ProGCL.
In our experimental setup, we randomly split the nodes into
three parts: 80%for testing, 10%for validation, and 10%for training.
We train the logistic regression classifier for 20runs. Furthermore,
we repeat the experiment including both stages for 10 times with
different random seeds. We report the average performance of the
200runs together with the standard deviation in the performance
table in the main body of the paper. For GCA, we adopt the GCA-DE
variant since it achieves the best performance overall among the
three variants proposed in [ 51]. The results of GCAare produced
using the official code and exact parameter settings provided in [ 51].
 
461Enhancing Contrastive Learning on Graphs with Node Similarity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
2 4 6 8 10
Training Ratio (%)747678808284Accuracy (%)
GRACE
GRACE+
(a)Cora
2 4 6 8 10
Training Ratio (%)6466687072Accuracy (%)
GRACE
GRACE+ (b)Citeseer
2 4 6 8 10
Training Ratio (%)84.585.085.586.086.587.0Accuracy (%)
GRACE
GRACE+ (c)Pubmed
2 4 6 8 10
Training Ratio (%)82.583.083.584.084.585.0Accuracy (%)
GRACE
GRACE+ (d)DBLP
Figure 1: Node classification results of GRACE and GRACE+ with various training ratios.
1 3 5 7 9
# Samples6065707580Accuracy (%)Graph-MLP
Graph-MLP+
(a) Cora
1 3 5 7 9
# Samples5055606570Accuracy (%)Graph-MLP
Graph-MLP+ (b) Citeseer
1 3 5 7 9
# Samples6065707580Accuracy (%)Graph-MLP
Graph-MLP+ (c) Pubmed
Figure 2: Node classification results of Graph-MLP and Graph-MLP+ with limited training samples.
ForProGCL, we adopt the ProGCL-reweight variant. The results of
ProGCL are produced using the official code and exact parameter
settings provided in [ 42]. The results of GCAandProGCL are differ-
ent from those reported in [ 42,51] as the experiment settings are
different. In particular, we repeat the entire process of experiments
for10times resulting a total of 200runs, while, in [ 51], the entire
process was executed once with 20runs.
The performances of GCA,GCA+ , and ProGCL are reported in
Table 2. As demonstrated in the Table, GCA+ surpasses GCAon all
datasets, which further illustrates the effectiveness of the proposed
enhanced objective. Also, it suggests that the enhanced objective
is general and can be utilized to advance various GCL methods.
Furthermore, GCA+ outperforms the advanced ProGCL on3out of 4
datasets, which further illustrates the effectiveness of the proposed
enhanced objective.
5.3 Graph-MLP
Here, we investigate how the enhanced objective helps improve
the performance of Graph-MLP by comparing its performance with
Graph-MLP+ . A brief introduction of Graph-MLP andGraph-MLP+
can be found in Section 4.
Table 3: Node classification results of Graph-MLP and Graph-MLP+.
Model Cora Citeseer Pubmed
Graph-MLP 79.7Â± 1.15 72.99Â±0.54 79.62Â±0.67
Graph-MLP+ 80.51Â±0.69 74.03Â±0.51 81.42Â±0.92
Following [ 16], we adopt three datasets including Cora ,Citeseer ,
andPubmed for comparing Graph-MLP+ with Graph-MLP . As de-
scribed in Section 4, Graph-MLP runs in a semi-supervised setting.The classification model is trained in an end-to-end way. We adopt
the conventional public splits of the datasets [ 21] to perform the
experiments. The experiments are repeated for 30times with differ-
ent random initialized parameters, and the average performance is
reported in Table 3. As shown in Table 3, Graph-MLP+ outperforms
Graph-MLP by a large margin on all three datasets. Graph-MLP+
even outperforms message-passing methods such as GCN by a
large margin, especially on Citeseer andPubmed , which indicates
that the proposed objective can effectively incorporate the graph
structure information and improve the performance of MLPs. Note
that, in the Graph-MLP+ framework, only the MLP model is utilized
for inference after training, which is more efficient than message-
passing methods in terms of both time and complexity.
We further compare Graph-MLP+ with Graph-MLP under the set-
ting where labeled nodes are extremely limited. Specifically, we
keep the test and validation set fixed, and only use ð‘ samples per
class for training, where ð‘ is set to 1,3,5,7, and 9. When creating
these various training sets, the ð‘ samples per class are sampled
from the training set in the public split setting. The results are pre-
sented in Figure 2. Graph-MLP+ achieves stronger performance than
Graph-MLP under all settings over all three datasets. Furthermore,
Graph-MLP+ performs extremely well when labels are limited. This
indicates that the proposed enhanced objective can more effectively
utilize the graph structure and feature information, which leads to
high-quality node representations even when labels are scarce.
5.4 Ablation Study
We assess the impact of key elements in our enhanced objective
through ablation studies. We first evaluate the contributions of the
positive weights ð‘¤+ð‘£(ð‘£ð‘—)and negative weights ð‘¤âˆ’ð‘£(ð‘£ð‘—). Then, we
analyze the influence of both graph and feature similarities on the
 
462KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hongliang Chi and Yao Ma
modelâ€™s performance. We only conduct ablation studies based on
GRACE andGraph-MLP asGCAis a variant of GRACE . For GRACE , we
adopt the 10%/10%/80%training, validation, and testing split. For
Graph-MLP, we use the conventional public splits.
5.4.1 Positive and Negative Weights in the Enhanced Objective. In
Section 3.4, two weights ð‘¤+ð‘£(ð‘£ð‘—)andð‘¤âˆ’ð‘£(ð‘£ð‘—)are introduced to in-
crease the variety of positive samples and alleviate the effect of
false negative samples, respectively. Here, we aim to investigate
how these two kinds of weights contribute to the model perfor-
mance. For this investigation, we introduce two variants of the
proposed objective that only incorporate the positive weights or
negative weights. The results for GRACE+ andGraph-MLP+ and their
corresponding variants are summarized in Table 4 and Table 5, re-
spectively. Specifically, in Table 4, we denote the variant of GRACE+
with onlyð‘¤+ð‘£(ð‘£ð‘—)asGRACE+(P) and the one with only ð‘¤âˆ’ð‘£(ð‘£ð‘—)is
denoted as GRACE+(N) . Likewise, the two variants for Graph-MLP+
are denoted as Graph-MLP+(P) andGraph-MLP+(N) in Table 5. In
Table 4, both GRACE+(P) andGRACE+(N) consistently outperform
GRACE on all datasets. Similarly, in Table 5, both Graph-MLP+(P) and
Graph-MLP+(N) consistently outperform Graph-MLP on all datasets.
These results clearly illustrate that both positive weights and nega-
tive weights are important for improving the enhanced objectives.
Furthermore, these two types of weights contribute to the objec-
tives in a complementary way since Graph-MLP+ outperforms all
variants on most datasets.
Table 4: Node classification results (%) of GRACE+ and its variants (â†‘:
increaseâ‰¤0.5%;â†‘â†‘: increase >0.5%).
Cora
Citeseer Pubmed DBLP
GRACE 82.56Â±1.21
71.23Â±0.86 86.12Â±0.23 84.43Â±0.25
GRACE+(P) 83.59Â±0.98â†‘
â†‘ 71.53Â±0.97â†‘ 86.41Â±0.29â†‘ 84.76Â±0.23â†‘
GRACE+(N) 83.20Â±1.26â†‘â†‘ 72.19Â±0.78â†‘â†‘ 86.31Â±0.22â†‘ 84.89Â±0.26â†‘
GRACE+(G) 83.19Â±1.31â†‘
â†‘ 71.40Â±1.03â†‘ 86.34Â±0.23â†‘ 84.55Â±0.26â†‘
GRACE+(F) 82.71Â±1.29â†‘ 71.90Â±0.87â†‘â†‘ 86.11Â±0.26 84.65Â±0.26 â†‘
GRACE+ 83.62Â±1.13â†‘
â†‘72.26Â±0.82â†‘â†‘86.45Â±0.29â†‘84.77Â±0.27â†‘
Table 5: Node classification results (%) of Graph-MLP+ and its variants
(â†‘: increaseâ‰¤0.5%;â†‘â†‘: increase >0.5%).
Cora
Citeseer Pubmed
Graph-MLP 79.7Â±1.15
72.99Â±0.54 79.62Â±0.67
Graph-MLP+(P) 80.33Â±0.96â†‘
â†‘73.69Â±0.45â†‘â†‘ 79.79Â±0.89â†‘
Graph-MLP+(N) 79.80Â±0.86â†‘ 73.41Â±0.44â†‘ 79.76Â±0.82â†‘
Graph-MLP+(G) 80.32Â±0.73â†‘
â†‘73.4Â±0.36â†‘â†‘ 79.96Â±0.94â†‘
Graph-MLP+(F) 61.80Â±0.61 64.04Â±0.73 75.92Â±0.90
Graph-MLP+ 80.51Â±0.69â†‘
â†‘74.03Â±0.51â†‘â†‘ 81.42Â±0.92â†‘â†‘
5.4.2 Similarity Measure. We examine the impact of graph and
feature similarities detailed in Section 3.3.3. Two enhanced objec-
tive variants are introduced: one using only graph similarity and
the other, only feature similarity. Results for GRACE+ ,Graph-MLP+and their variants are presented in Tables 4 and 5. In these ta-
bles, GRACE+(G) represents the GRACE+ variant using only graph
similarity, and GRACE+(F) , the one using only feature similarity.
ForGraph-MLP+ , the variants are denoted as Graph-MLP+(G) and
Graph-MLP+(F). Our observations are as follows:
â€¢In Table 4, both GRACE+(G) and GRACE+(F) outperforms
GRACE on most datasets, which demonstrates that both graph
and feature similarity contain important information about
node similarity and they can be utilized for effectively model-
ing the anchor-aware distributions. GRACE+ outperforms the
two variants and the base model GRACE on all datasets, which
indicates that the graph similarity and feature similarity are
complementary to each other, and properly combining them
results in better similarity estimation leading to strong per-
formance.
â€¢In Table 5, Graph-MLP+(G) significantly outperforms Graph-MLP
while Graph-MLP+(F) does not perform well. This is poten-
tially due to the lack of graph information in MLP mod-
els. Different from GRACE+(F) which incorporates graph
information in the encoder, Graph-MLP+(F) only utilizes fea-
ture information, which leads to inferior performance. On
the other hand, the strong performance of Graph-MLP+ sug-
gests that the enhanced objective effectively incorporates the
graph structure information. However, this does not mean
the feature similarity is not important. Graph-MLP+ outper-
forms Graph-MLP+(G) on all three datasets, which suggests
that the feature similarity brings additional information than
graph similarity, and properly combining them is important.
6 Related Work
Contrastive Learning. Contrastive learning (CL) aims to learn
latent representations by discriminating positive from negative sam-
ples. The instance discrimination loss is introduced in [ 41] without
data augmentation. In [ 3], it is proposed to generate multiple views
by data augmentation and learn representations by maximizing
mutual information. Momentum Contrast (MoCo) [ 15] maintains a
memory bank of negative samples, which significantly increases
the number of negatives used in the contrastive loss. In [ 5], it is dis-
covered that the composition of data augmentations plays a critical
role in CL. BYOL and Barlow Twins [ 45] [13] achieves strong CL
performance without using negative samples. Recently, a series of
tricks such as debiased negative sampling [ 6], and hard negative
mining [ 19,32,40], positive mining [ 7] have been proved effective.
Efforts have also been made to extend CL to supervised setting [ 20].
Graph Contrastive Learning. Deep Graph Infomax (DGI) [ 38]
takes a local-global comparison mode by maximizing the mutual
information between patch representations and high-level sum-
maries of graphs. MVGRL [ 14] contrasts multiple structural views
of graphs generated by graph diffusion. GRACE [ 50] utilizes edge
removing and feature masking to generate two views for node-level
contrastive learning. Based upon GRACE, GCA [ 51] adopts adap-
tive augmentations by considering the topological and semantic
aspects of graphs. Several works investigate the bias in the negative
sampling [ 42,49]. MERIT [ 17] leverages Siamese GNNs to learn
high-quality node representations. Most of these contrastive learn-
ing frameworks utilize negative samples in their training. BGRL [ 36]
 
463Enhancing Contrastive Learning on Graphs with Node Similarity KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
and Graph Barlow Twins [ 4] are GCL methods without requiring
negative samples. CCA-SSG [ 46] leverages Canonical Correlation
Analysis to optimize correlation between two views. COSTA [ 47]
tackles biases inherent in graph augmentation through the imple-
mentation of feature augmentation. SUGRL [ 27] is designed to
enhance differences between classes while minimizing differences
within the same class. Recently, SFA [ 48] leverages spectral feature
augmentation as its augmentation scheme.
7 Conclusion
In this paper, we propose an effective enhanced contrastive ob-
jective to approximate the ideal contrastive objective for graph
contrastive learning. The proposed objective leverages node simi-
larity to model the anchor-aware distributions for sampling positive
and negative samples. Also, the objective is designed to be flexible
and general, which could be adopted for any graph contrastive
learning framework that utilizes the traditional InfoNCE-based
objective. Furthermore, the proposed enhancing philosophy gener-
ally applies to other contrasting-based models such as Graph-MLP
which includes an auxiliary contrastive loss. Extensive experiments
have demonstrated the significant improvement on various GCL
models with the application of the enhanced objectives.
8 ACKNOWLEDGEMENT
This research is supported by the National Science Foundation
(NSF) under grant numbers NSF-2406647 and NSF-2406648.
References
[1]J. S. Rowlinson *. 2005. The Maxwellâ€“Boltzmann distribution. Molecular Physics
(2005). https://doi.org/10.1080/002068970500044749
[2]Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis,
and Nikunj Saunshi. 2019. A theoretical analysis of contrastive unsupervised
representation learning. arXiv preprint arXiv:1902.09229 (2019).
[3]Philip Bachman, R Devon Hjelm, and William Buchwalter. 2019. Learning repre-
sentations by maximizing mutual information across views. NeurIPS (2019).
[4]Piotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. 2022. Graph bar-
low twins: A self-supervised representation learning framework for graphs.
Knowledge-Based Systems 256 (2022), 109631.
[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In ICML.
[6]Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Ste-
fanie Jegelka. 2020. Debiased contrastive learning. NeurIPS 33 (2020).
[7]Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew
Zisserman. 2021. With a little help from my friends: Nearest-neighbor contrastive
learning of visual representations. In ICCV.
[8]Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In WWW.
[9]Wenqi Fan, Yao Ma, Dawei Yin, Jianping Wang, Jiliang Tang, and Qing Li. 2019.
Deep social collaborative filtering. In Proceedings of the 13th ACM RecSys.
[10] Victor Garcia and Joan Bruna. 2017. Few-shot learning with graph neural net-
works. arXiv preprint arXiv:1711.04043 (2017).
[11] Dmitri Goldenberg. 2021. Social network analysis: From graph theory to applica-
tions with python. arXiv preprint arXiv:2102.10014 (2021).
[12] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT
Press. http://www.deeplearningbook.org.
[13] Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
Guo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own latent-a new
approach to self-supervised learning. NeurIPS 33 (2020).
[14] Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-view
representation learning on graphs. In ICML.
[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-
mentum contrast for unsupervised visual representation learning. In CVPR.
[16] Yang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, and Yue
Gao. 2021. Graph-MLP: Node Classification without Message Passing in Graph.
https://doi.org/10.48550/ARXIV.2106.04051[17] Ming Jin, Yizhen Zheng, Yuan-Fang Li, Chen Gong, Chuan Zhou, and Shirui
Pan. 2021. Multi-scale contrastive siamese networks for self-supervised graph
representation learning. arXiv preprint arXiv:2105.05682 (2021).
[18] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang
Tang. 2020. Self-supervised learning on graphs: Deep insights and new direction.
arXiv preprint arXiv:2006.10141 (2020).
[19] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and
Diane Larlus. 2020. Hard negative mixing for contrastive learning. NeurIPS 33
(2020), 21798â€“21809.
[20] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive
learning. NeurIPS 33 (2020).
[21] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[22] Johannes Klicpera, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2018. Pre-
dict then propagate: Graph neural networks meet personalized pagerank. arXiv
preprint arXiv:1810.05997 (2018).
[23] Johannes Klicpera, Stefan WeiÃŸenberger, and Stephan GÃ¼nnemann. 2019. Diffu-
sion improves graph learning. arXiv preprint arXiv:1911.05485 (2019).
[24] Andre Lamurias, Pedro Ruas, and Francisco M Couto. 2019. PPR-SSM: per-
sonalized PageRank and semantic similarity measures for entity linking. BMC
bioinformatics 20, 1 (2019).
[25] Zhen Li, Mingjian Jiang, Shuang Wang, and Shugang Zhang. 2022. Deep learning
methods for molecular representation and property prediction. Drug Discovery
Today 27, 12 (2022), 103373.
[26] PÃ©ter Mernyei and CÄƒtÄƒlina Cangea. 2020. Wiki-CS: A Wikipedia-Based Bench-
mark for Graph Neural Networks. arXiv preprint arXiv:2007.02901 (2020).
[27] Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. 2022. Simple un-
supervised graph representation learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 36. 7797â€“7805.
[28] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The
PageRank citation ranking: Bringing order to the web. Technical Report. Stanford.
[29] Sungchan Park, Wonseok Lee, Byeongseo Choe, and Sang-Goo Lee. 2019. A
survey on personalized PageRank computation algorithms. Access (2019).
[30] Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait. 2019. Rethinking table
recognition using graph neural networks. In 2019 International Conference on
Document Analysis and Recognition (ICDAR). IEEE, 142â€“147.
[31] Prakash Chandra Rathi, R Frederick Ludlow, and Marcel L Verdonk. 2019. Practical
high-quality electrostatic potential surfaces for drug discovery using a graph-
convolutional deep neural network. Journal of medicinal chemistry (2019).
[32] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. 2020.
Contrastive learning with hard negative samples. arXiv preprint arXiv:2010.04592
(2020).
[33] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93â€“93.
[34] Behrooz Shahsavari and Pieter Abbeel. 2015. Short-term traffic forecasting:
Modeling and learning spatio-temporal relations in transportation networks
using graph neural networks. University of California at Berkeley, Technical
Report No. UCB/EECS-2015-243 (2015).
[35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
GÃ¼nnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[36] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, RÃ©mi Munos,
Petar VeliÄkoviÄ‡, and Michal Valko. 2021. Bootstrapped representation learning
on graphs. In ICLR 2021 Workshop on GTRL.
[37] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[38] Petar Velickovic, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio,
and R Devon Hjelm. 2019. Deep Graph Infomax. ICLR (Poster) 2, 3 (2019), 4.
[39] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In ICML. PMLR.
[40] Mike Wu, Milan Mosse, Chengxu Zhuang, Daniel Yamins, and Noah Goodman.
2020. Conditional negative sampling for contrastive learning of visual represen-
tations. arXiv preprint arXiv:2010.02037 (2020).
[41] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. 2018. Unsupervised
feature learning via non-parametric instance-level discrimination. arXiv preprint
arXiv:1805.01978 (2018).
[42] Jun Xia, Lirong Wu, Ge Wang, and Stan Z. Li. 2022. ProGCL: Rethinking Hard
Negative Mining in Graph Contrastive Learning. In International conference on
machine learning. PMLR.
[43] Yishi Xu, Yingxue Zhang, Wei Guo, Huifeng Guo, Ruiming Tang, and Mark Coates.
2020. Graphsail: Graph structure aware incremental learning for recommender
systems. In Proceedings of the 29th ACM International CIKM.
[44] Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating network commu-
nities based on ground-truth. Knowledge and Information Systems (2015).
[45] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and StÃ©phane Deny. 2021. Barlow
twins: Self-supervised learning via redundancy reduction. In ICML. PMLR.
 
464KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hongliang Chi and Yao Ma
[46] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. 2021. From
canonical correlation analysis to self-supervised graph neural networks. Advances
in Neural Information Processing Systems 34 (2021), 76â€“89.
[47] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. 2022. COSTA:
covariance-preserving feature augmentation for graph contrastive learning. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2524â€“2534.
[48] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. 2023. Spectral
feature augmentation for graph contrastive learning and beyond. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 37. 11289â€“11297.[49] Han Zhao, Xu Yang, Zhenru Wang, Erkun Yang, and Cheng Deng. 2021. Graph
debiased contrastive learning with joint representation clustering. In Proc. IJCAI.
3434â€“3440.
[50] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.
Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131
(2020).
[51] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021.
Graph contrastive learning with adaptive augmentation. In Proceedings of the
Web Conference 2021.
 
465