Team up GBDTs and DNNs: Advancing Efficient and Effective
Tabular Prediction with Tree-hybrid MLPs
Jiahuan Yan
Zhejiang University
Hangzhou, China
jyansir@zju.edu.cnJintai Chenâˆ—
University of Illinois at
Urbana-Champaign
Urbana, IL, USA
jtchen721@gmail.comQianxing Wang
Zhejiang University
Hangzhou, China
w.qianxing@zju.edu.cn
Danny Z. Chen
University of Notre Dame
Notre Dame, IN, USA
dchen@nd.eduJian Wu
Zhejiang University
Hangzhou, China
wujian2000@zju.edu.cn
ABSTRACT
Tabular datasets play a crucial role in various applications. Thus,
developing efficient, effective, and widely compatible prediction
algorithms for tabular data is important. Currently, two prominent
model types, Gradient Boosted Decision Trees (GBDTs) and Deep
Neural Networks (DNNs), have demonstrated performance advan-
tages on distinct tabular prediction tasks. However, selecting an
effective model for a specific tabular dataset is challenging, often de-
manding time-consuming hyperparameter tuning. To address this
model selection dilemma, this paper proposes a new framework that
amalgamates the advantages of both GBDTs and DNNs, resulting in
a DNN algorithm that is as efficient as GBDTs and is competitively
effective regardless of dataset preferences for GBDTs or DNNs. Our
idea is rooted in an observation that deep learning (DL) offers a
larger parameter space that can represent a well-performing GBDT
model, yet the current back-propagation optimizer struggles to
efficiently discover such optimal functionality. On the other hand,
during GBDT development, hard tree pruning, entropy-driven fea-
ture gate, and model ensemble have proved to be more adaptable
to tabular data. By combining these key components, we present a
Tree-hybrid simple MLP (T-MLP). In our framework, a tensorized,
rapidly trained GBDT feature gate, a DNN architecture pruning
approach, as well as a vanilla back-propagation optimizer collabora-
tively train a randomly initialized MLP model. Comprehensive ex-
periments show that T-MLP is competitive with extensively tuned
DNNs and GBDTs in their dominating tabular benchmarks (88
datasets) respectively, all achieved with compact model storage
and significantly reduced training duration. The codes and full
experiment results are available at https://github.com/jyansir/tmlp.
âˆ—The corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671964CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning ; Supervised
learning; Neural networks.
KEYWORDS
classification and regression, tabular data, green AI, AutoML
ACM Reference Format:
Jiahuan Yan, Jintai Chenâˆ—, Qianxing Wang, Danny Z. Chen, and Jian Wu.
2024. Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular
Prediction with Tree-hybrid MLPs. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671964
1 INTRODUCTION
Tabular data are a ubiquitous and dominating data structure in var-
ious machine learning applications (e.g., click-through rate (CTR)
prediction [ 17] and financial risk detection [ 3]). Current prevalent
tabular prediction (i.e., classification and regression) models can be
generally categorized into two main types: (1) Gradient Boosted
Decision Trees (GBDTs) [ 16,18,31,43], a kind of classical non-deep-
learning approach that has been extensively verified as test-of-time
solutions [ 7,23,55]; (2) Deep Neural Networks (DNNs), on which
continuous endeavors apply deep learning (DL) techniques from
computer vision (CV) and natural language processing (NLP) to
develop tabular learning methods such as meticulous architecture
engineering [ 2,12,22,42,62] and pre-training [ 48,58,69]. With
recent developments of bespoke tabular DNNs, increasing stud-
ies reported their better comparability [ 13,62] and even superior-
ity [14,48] to GBDTs, especially in complex data scenarios [ 45,58],
while classical thinking believes that GBDTs still completely sur-
pass DNNs in typical tabular tasks [ 7,23], both evaluated with
different benchmarks and baselines, implying respective tabular
data proficiency of these two model types.
For DNNs, their inherent high-dimensional feature spaces and
smooth back-propagation optimization gain tremendous success on
unstructured data [ 10,44] and capability of mining subtle feature
interactions [ 46,49,57,62]. Besides, leveraging DNNâ€™s transfer-
ability, recent popular tabular Transformers can be further im-
proved by costly pre-training [ 48,58,69], like their counterparts in
3679
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu
NLP [ 10,32,68]. However, compared to the simple multi-layer per-
ceptron (MLP) and GBDTs, Transformer architectures are more com-
plicated and are prone to be over-parameterized, data-hungry, and
increase processing latency, especially those recent language-model-
based architectures [ 8,67]. Thus, they typically under-perform on
tabular datasets that are potentially small-sized [23].
Regarding GBDTs, they thrive on greedy feature selection, tree
pruning, and efficient ensemble, yielding remarkable performances
and efficiency on the majority of tabular prediction applications [ 47,
55]. Yet, they are usually hyperparameter-sensitive [ 43,63] and not
well-suited in extreme tabular scenarios, such as large-scale tables
with intricate feature interactions [ 45]. Also, their inference latency
increases markedly as the data scale grows [7].
Besides, both the GBDT and DNN frameworks achieve respective
state-of-the-art results with expensive training costs, since heavy
hyperparameter search is required to achieve considerable perfor-
mance. But, this is carbon-unfriendly and is not compatible in
computation-limited or real-time applications, while not enough
proactive efforts on economical tabular prediction have been made.
To address the model selection dilemma, we comprehensively
combine the advantages of both GBDTs and DNNs, and propose a
newTree-hybrid simple MLP (T-MLP), which is high-performing,
efficient, and lightweight. Specifically, a single T-MLP is equipped
with a GBDT feature gate to perform sample-specific feature selec-
tion in a greedy fashion, and GBDT-inspired pruned MLP architec-
tures to process the selected salient features. The whole framework
is optimized using back-propagation with these GBDTsâ€™ properties,
and all the components make the system compact, overfit-resistant,
and generalizable. Furthermore, model ensemble can be efficiently
achieved by training multiple sparse MLPs (we uniformly use 3
MLPs here) in parallel with a shared gate and predicting in a bag-
ging manner. Overall, T-MLP has the following appealing features.
(1)Generalized data adaptability: Different from existing tabular
prediction methods that suffer from the model selection dilemma,
T-MLP is flexible enough to handle all datasets regardless of the
framework preference (see Sec. 4.2 and Sec. 4.4). (2) Hyperparam-
eter tuning free: T-MLP is able to produce competitive results
with all the configurations pre-fixed, which is significantly
time-saving, user-friendly, environmentally friendly and widely
practical in broader applications. (3) Lightweight storage: In T-
MLP, the DNN part is purely composed of simple and highly sparse
MLP architectures, yet is still able to be state-of-the-art competitive
even with one-block MLP. Table 1 presents the economical cost-
performance trade-off of T-MLP compared to common DNNs; such
cost-effectiveness becomes more profound as the data scale grows.
In summary, our main contributions are as follows:
â€¢We propose a new GBDT-DNN hybrid framework, T-MLP,
which is a one-stop and economical solution for effective
tabular data prediction regardless of framework preferences
of specific datasets, offering a novel optimization paradigm
for tabular model architectures.
â€¢Multi-facet analysis on feature selection strategy, parameter
sparsity, and decision boundary pattern is given for in-depth
understanding of the T-MLP efficiency and superiority.
â€¢Comprehensive experiments on 88 datasets from 4 bench-
marks, covering DNN- and GBDT-favored ones, show that aTable 1: Comparison of model cost-effectiveness on small and
large datasets across popular tabular DNNs. ğ¹andğ‘denote
the amounts of features and samples, ğ‘ƒis the parameter
number, and ğ‘‡denotes the overhead of total training time
against the proposed T-MLP. We reuse performances and
parameter sizes of the best model configurations in the FT-
Transformer benchmark. ğ‘‡is evaluated on an NVIDIA A100
PCIe 40GB (see Sec. 4.1). Based on the fixed architecture and
training configurations, T-MLP achieves stable model size
and cheap training duration cost regardless of the data scale.
Dataset: Adult (ğ¹ =14,ğ‘=49K) Year (ğ¹ =90,ğ‘=515K)
ğ‘ƒ(M)ğ‘‡ ACCâ†‘ğ‘ƒ(M)ğ‘‡ RMSEâ†“
MLP 0.77 7.7Ã— 0.852 1.16 15.9Ã— 8.853
NODE 20.83 120.4Ã— 0.858 7.55 206.0Ã— 8.784
AutoInt 0.01 25.0Ã— 0.859 0.08 101.9Ã— 8.882
DCNv2 1.18 8.0Ã— 0.853 11.32 29.9Ã— 8.890
FT-T 3.82 19.6Ã— 0.859 1.25 116.3Ã— 8.855
T-MLP 0.73 1.0Ã— 0.864 0.75 1.0Ã— 8.768
single T-MLP is competitive with advanced or pre-trained
DNNs, and T-MLP ensemble can even consistently outper-
form them and is competitive with extensively tuned state-
of-the-art GBDTs, all achieved with a compact model size
and significantly reduced training duration.
â€¢We develop an open-source Python package with APIs of
benchmark loading, uniform baseline invocation (DNNs, GB-
DTs, T-MLP), DNN pruning, and other advanced functions
as a developmental tool for the tabular learning community.
2 RELATED WORK
2.1 Model Frameworks for Tabular Prediction
In the past two decades, classical non-deep-learning methods [ 25,35,
65,66] have been prevalent for tabular prediction applications, espe-
cially GBDTs [ 16,18,31,43] due to their efficiency and robustness in
typical tabular tasks [ 23]. Because of the universal success of DNNs
on unstructured data and the development of computation devices,
there is an increasing effort in applying DNNs to such tasks. The
early tabular DNNs aimed to be comparable with GBDTs by emulat-
ing the ensemble tree frameworks (e.g., NODE [ 42], Net-DNF [ 30],
and TabNet [ 2]), but they neglected the advantages of DNNs for au-
tomatic feature fusion and interaction. Hence, more recent attempts
leveraged DNNsâ€™ superiority, as they transferred successful neural
architectures (e.g., AutoInt [ 49], FT-Transformer [ 22]), proposed
bespoke designs (e.g., T2G-Former [ 62]), or adopted pre-training
(e.g., SAINT [ 48], TransTab [ 58]), reporting competitive or even
surpassing results compared to conventionally dominating GB-
DTs in specific data scenarios [ 14,48]. Contemporary surveys [ 7]
demonstrated that GBDTs and DNNs are two prevailing types of
frameworks in current tabular learning research.
2.2 Lightweight DNNs
Lightweight DNNs are an evergreen research topic in CV and NLP,
which aim to maintain effective performance while promoting DNN
compactness and efficiency. A recent trend is to substitute domi-
nating backbones with pure simple MLPs, such as MLP-Mixer [ 53],
3680Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) GBDTs(b) DNNs
MLPTransformer
pre-trainedmodels...-Incompatible with non-typical tables- Heavy hyperparameter tuning (HPT)T-MLPÃ—3
SharedGBDTsFeature GateSimplifiedSparse MLPs
Features# IDğ‘¥!101ğ‘¥"ğ‘¥#
EnsemblePrediction-One framework with bothproperties for alldatasets-Fixed andsimple architecture, stablemodel size- No HPT & pre-training
...featureselection(c) T-MLP (Tree-hybrid MLPs)
pre-pruning
efficientensemble
smoothoptimization-Complicated architecture engineering- Heavy HPTorcostly pre-training- Inferior to GBDTs on typical tables
largemodelcapacity
Figure 1: Our proposed T-MLP vs. existing tabular prediction approaches: GBDTs and DNNs. (a) GBDTs are classical non-
deep-learning models for tabular prediction. (b) DNNs are emerging promising methods especially for large-scale, complex,
cross-table scenarios. (c) T-MLP is a hybrid framework that integrates the strengths of both GBDTs and DNNs, accomplished via
GBDT feature gate tensorization, MLP framework pruning, simple block ensemble, and end-to-end back-propagation. It yields
competitive results on both DNN- and GBDT-favored datasets, with a rapid development process and compact model size.
gMLP [ 36], MAXIM [ 54], and other vision MLPs [ 11,24,51], achiev-
ing comparable or even superior results to their CNN or Trans-
former counterparts with reduced capacity or FLOPs. This pure-
MLP trend is also arising in NLP [ 19] and other real-world appli-
cations [ 15]. Another lightweight scheme is model compression,
where pruning is a predominant approach used to trim down large
language models [ 56,68] from various granularity [ 38,50,61]. In
the tabular prediction field, there are a few pure-MLP studies, but
all focusing on regularization [ 29] or numerical embedding [ 21]
rather than the DNN architecture itself. Besides, model compression
of tabular DNNs has not yet been explored. We introduce related
techniques to make our T-MLP more compact and effective.
3 TREE-HYBRID SIMPLE MLP
We first review some preliminaries of typical GBDTsâ€™ inference
process and feature encoding techniques in current Transformer-
based tabular DNNs. Next, we elaborate on the detailed designs of
several key components of T-MLP, including the GBDT feature gate
for sample-specific feature selection, the pure-MLP basic block, and
GBDT-inspired fine-grained pruning for sparse MLPs. Finally, we
provide a discussion of the T-MLP workflow.
3.1 Preliminaries
Problem Statement. Given a tabular dataset with input features
ğ‘‹âˆˆRğ‘Ã—ğ¹and targets ğ‘¦âˆˆRğ‘, the tabular prediction task is to
find an optimal solution ğ‘“:âˆˆRğ‘Ã—ğ¹â†’Rğ‘that minimizes the
empirical difference between the predictions Ë†ğ‘¦and the targets ğ‘¦.
Here in current practice, the common choice of ğ‘“is either tradi-
tional GBDTs (e.g., XGBoost [ 16], CatBoost [ 43], LightGBM [ 31]) or
tabular DNNs (e.g., TabNet [ 2], FT-Transformer [ 22], SAINT [ 48],
T2G-Former [ 62]). A typical difference metric is accuracy or AUC
score for classification tasks, and is the root of mean squared error
(RMSE) for regression.
Definition 3.1: GBDT Feature Frequency. Given a GBDT model
withğ‘‡decision trees (e.g., CART [ 35]), the GBDT feature frequency
of a sample denotes the number of times each feature is accessed
by this GBDT on the sample. Specifically, the process of GBDTinference on a sample ğ‘¥âˆˆRğ¹providesğ‘‡times a single decision tree
prediction Ë†ğ‘¦(ğ‘˜)=CART(ğ‘˜)(ğ‘¥),ğ‘˜âˆˆ{1,2,...,ğ‘‡}. For each decision
tree prediction, there exists a sample-specific decision path from
its root to one of the leaf nodes, forming a used feature list that
includes features involved in this prediction action. We denote this
accessed feature list of the ğ‘˜-th decision tree as a binary vector
ğ›¼(ğ‘˜)âˆˆ{0,1}ğ¹, in which 0 indicates that the corresponding feature
of this sample is not used by the ğ‘˜-th decision, and 1 indicates that
it is accessed. Consequently, we can represent the GBDT feature
frequency of the sample with the sum of the ğ‘˜decision treesâ€™ binary
vectors, as:
ğ›¼=âˆ‘ï¸
ğ‘˜ğ›¼(ğ‘˜),
whereğ›¼represents the exploitation level of each feature in the
GBDT, suggesting the feature preference of the GBDT model on
this sample.
Feature Tokenizer. Inspired by the classical language models
(e.g., BERT [ 32]), recent dominating Transformer-based tabular
models [ 22,48,62] adopted distributed feature representation [ 39]
by embedding tabular values into vector spaces and treating the
values as â€œunorderedâ€ word vectors. Such Transformer models
usefeature tokenizer [22] to process tabular features as follows:
Each tabular scalar value is mapped to a vector ğ‘’âˆˆRğ‘‘with
a feature-specific linear projection, where ğ‘‘is the feature hid-
den dimension. For numerical (continuous) values, the projection
weights are multiplied with the value magnitudes. Given ğ¹1nu-
merical features and ğ¹2categorical features, the feature tokenizer
outputs feature embedding ğ¸âˆˆR(1+ğ¹ 1+ğ¹2)Ã—ğ‘‘by stacking pro-
jected features (and an extra [CLS]token embedding), i.e., ğ¸=
stackh
ğ‘’CLS,ğ‘’(1)
num,...,ğ‘’(ğ¹1)
num,ğ‘’(1)
cat,...,ğ‘’(ğ¹2)
cati
.
3.2 GBDT Feature Gate
Early attempts of tabular DNNs tried to emulate behavioral patterns
of GBDTs by ensembling neural networks to build differential tree
models, such as representative models NODE [ 42] and TabNet [ 2].
However, even realizing decision-tree-like hard feature selection or
resorting to complicated Transformer architectures, they were still
3681KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu
rapidly submerged in subsequent DNN studies that mainly focused
on promotion from deep learning perspectives [ 13,22,62]. We seek
to rethink this line of work and observe that they achieve hard
feature selection with learnable continuous feature masks through
DNNsâ€™ smooth back-propagation, which may be incompatible with
the discrete nature of GBDTs, and hence restrict their potential.
To resolve this issue, we propose GBDT Feature Gate (GFG), a
GBDT-based feature selector tensorized with GBDT weights to
faithfully replicate its feature selection behavior. Specifically, given
a GFG initialized by a ğ‘‡-tree GBDT, the feature selection process
on anğ¹-feature sample ğ‘¥(Ë†ğ¸=GFG(ğ‘¥)âˆˆRğ¹Ã—ğ‘‘) is formulated as:
ğ¸=FeatureTokenizer(ğ‘¥)âˆˆRğ¹Ã—ğ‘‘, (1)
ğ›¼=GBDTFeatureFrequency (ğ‘¥)âˆˆRğ¹, (2)
Ë†ğ›¼=ğ›¼/ğ‘‡âˆˆRğ¹,Â¯ğ›¼=BinarySampler(Ë†ğ›¼)âˆˆ{0,1}ğ¹, (3)
Ë†ğ¸:,ğ‘–=(
Â¯ğ›¼âŠ™ğ¸:,ğ‘–iftraining
Ë†ğ›¼âŠ™ğ¸:,ğ‘–ifinference,ğ‘–âˆˆ{1,2,...,ğ‘‘}. (4)
The extra[CLS]embedding is omitted in this subsection for
notation brevity; in implementation, it is directly concatenated to
the head of the gated Ë†ğ¸. In Eq. (3), Ë†ğ›¼is the normalized GBDT feature
frequency that represents the access probabilities of each feature in
theğ‘‡-tree GBDT, and Â¯ğ›¼is a binary feature mask sampled with the
probabilities Ë†ğ›¼. To incorporate the GBDTâ€™s feature preference into
the DNN framework, in Eq. (4), we use sparse feature masks from
real GBDT feature access probabilities to perform hard feature selec-
tion during training, and use the soft probabilities during inference
for deterministic prediction. GFG assists in filtering out unneces-
sary features according to the GBDTâ€™s feature preference, ensuring
an oracle selection behavior compared to previous differential tree
models in learning feature masks with neural networks.
Since the original GBDT library (we uniformly use XGBoost
in this work) has no APIs for efficiently fetching sample-specific
GBDT feature frequency in Eq. (2) and the used backend is incom-
patible with common DL libraries (e.g., PyTorch), to integrate the
GFG module into the parallel DNN framework, we tensorize the
behavior of Eq. (2). Technically, we are inspired by the principle of
the Microsoft Hummingbird compiling tools1and extract routing
matrices, a series of parameter matrices that contain information
of each decision treeâ€™s node adjacency and threshold values, from
the XGBoost model. Based on the extracted routing matrices, fea-
ture access frequency can be simply acquired through alternating
tensor multiplication and comparison on input features ğ‘¥, and the
submodule of Eq. (2) is initialized with these parameter matrices.
In the actual implementation, we just rapidly train an XGBoost
with uniform default hyperparameters provided in [ 22] (regardless
of its performance) to initialize and freeze the submodule of Eq. (2)
during the T-MLP initialization step. Other trainable parameters
are randomly initialized. Since there are a large number of deci-
sion trees to vote the feature preference in a GBDT model, slight
hyperparameter modification will not change the overall feature
preference trend, and a lightly-trained default XGBoost is always
usable enough to guide greedy feature selection. To further speed
up the processes in Eqs. (2)-(3), we cache the normalized feature
1https://github.com/microsoft/hummingbirdfrequency Ë†ğ›¼for each sample during the first-epoch computation,
and reuse the cache in the subsequent model training or inference.
3.3 Pure MLP Basic Block
To explore the capability of pure-MLP architecture and keep our
tabular model compact, we take inspiration from vision MLPs.
We observe that a key factor of their success is the attention-like
interaction realized by linear projection and soft gating on fea-
tures [ 11,24,53]. Thus, we employ the spatial gating unit (SGU)
proposed in [36], and formulate a simplified pure-MLP block, as:
Ë†ğ¸(ğ‘™+1)=SGU(GELU(LayerNorm(Ë†ğ¸(ğ‘™))ğ‘Š1))ğ‘Š2+Ë†ğ¸(ğ‘™),(5)
SGU(ğ‘‹)=ğ‘Š3LayerNorm(ğ‘‹:,:ğ‘‘â€²)âŠ™ğ‘‹:,ğ‘‘â€²:. (6)
The block is similar to a single feed-forward neural network (FFN)
in the Transformer with an extra SGU (Eq. (6)) for feature-level in-
teraction. The main parameters are located in two transformations,
i.e.,ğ‘Š1âˆˆRğ‘‘Ã—2ğ‘‘â€²andğ‘Š2âˆˆRğ‘‘â€²Ã—ğ‘‘in Eq. (5), where ğ‘‘â€²corresponds
to the FFN intermediate dimension size. In Eq. (6), ğ‘‹âˆˆRğ¹Ã—2ğ‘‘â€²
denotes the input features of SGU, and ğ‘Š3âˆˆRğ¹Ã—ğ¹is a feature-level
transformation to emulate attention operation. Since ğ‘‘â‰ˆğ‘‘â€²â‰«ğ¹
in most cases, the model size is determined by ğ‘Š1andğ‘Š2, and is
comparable to the FFN size. All the bias vectors are omitted for
notation brevity.
Analogous to vision data, we treat tabular features and feature
embeddings as image pixels and channels. But completely differ-
ent from vision MLPs, T-MLP is a hybrid framework tailored for
economical tabular prediction that performs competitively against
tabular Transformers and GBDTs with significantly reduced run-
time costs. On most uncomplicated datasets, using only one basic
block in T-MLP is enough. In comparison, previous vision MLP
studies emphasized architecture engineering and often demanded
dozens of blocks in order to be comparable to vision Transformers.
3.4 Sparsity with User-controllable Pruning
Inspired by the pre-pruning of GBDTs that controls model complex-
ity and promotes generalization with user-defined hyperparameters
(e.g., maximum tree depth, minimum samples per leaf), we design
a similar mechanism for T-MLP by leveraging the predominant
model compression approach, i.e., DNN pruning [26,38,50], which
is widely used in NLP research to trim down over-parameterized
language models while maintaining the original reliability [61].
Specifically, we introduce two fine-grained variables ğ‘§hâˆˆ{0,1}ğ‘‘
andğ‘§inâˆˆ{0,1}ğ‘‘â€²to mask parameters from hidden dimension and
intermediate dimension, respectively. As the previous FFN prun-
ing in language models [ 59], the T-MLP pruning operation can be
attained by simply applying the mask variables to the weight matri-
ces, i.e., substituting ğ‘Š1andğ‘Š2with diag(ğ‘§h)ğ‘Š1and diag(ğ‘§in)ğ‘Š2
in Eq. (5). We use the classical ğ‘™0regularization reparametrized with
hard concrete distributions [ 37], and adopt a Lagrangian multiplier
objective to achieve the controllable sparsity as in [61].
Although early attempts of tabular DNNs have considered sparse
structures, for example, TabNet [ 2] and NODE [ 42] built learn-
able sparse feature masks, and more recently TabCaps [ 12] and
T2G-Former [ 62] designed sparse feature interaction, there are two
essential differences: (1) existing tabular DNNs only considered
sparsity on the feature dimension, while T-MLP introduces sparsity
3682Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
on the input features (Sec. 3.2) and the hidden dimension (this sub-
section), which was ignored in previous tabular DNN prediction
studies and widely recognized as an over-parameterized facet in
NLP practice [ 59,61]; (2) learnable sparsity in existing tabular DNNs
is completely coupled and determined by prediction loss functions,
while our introduced DNN pruning techniques determine the spar-
sity based on the user-defined sparsity rate (objective-independent),
with the same controllable nature of GBDTs pre-pruning.
In the main experiments (Sec. 4.2), we uniformly fix the target
sparsity at 0.33 for T-MLP, i.e., only around 33% of DNN parameters
are retained after training. We further explore the relationship
between model sparsity and performance in Sec. 4.3, and obtain
performance boost with suitable parameter pruning, even on T-MLP
with one basic block, This implies pervasive over-parameterization
in previous tabular DNN designs.
3.5 Overall Workflow and Efficient Ensemble
The overall T-MLP workflow is as follows: During the training stage,
the input tabular features are embedded with the feature tokenizer
and discretely selected by the sampled feature mask Â¯ğ›¼in Eq. (3);
then, they are processed by a single pruned basic block in Eq. (5),
and the pruning parameter masks ğ‘§handğ‘§inare sampled with
reparameterization on the ğ‘™0regularization; the final prediction is
made with the[CLS]token feature using a normal prediction head
as in other tabular Transformers, as:
Ë†ğ‘¦=FC(ReLU(LayerNorm(Ë†ğ¸(ğ‘™)
[CLS],:))),
where FC denotes a fully connected layer. We use the cross entropy
loss for classification and the mean squared error loss for regression
as in previous tabular DNNs. The whole framework is optimized
with back-propagation. After training, the parameter masks are
directly applied to ğ‘Š1andğ‘Š2by accordingly dropping the pruned
hidden and intermediate dimensions. In the inference stage, the
input features are softly selected by the normalized GBDT feature
frequency Ë†ğ›¼in Eq. (3), and processed by the simplified basic block.
Since the T-MLP architecture is compact and computation-friendly
with low runtime cost, we further provide an efficient ensemble ver-
sion by simultaneously training three branches with the shared
GBDT feature gate from the same initialization point with three
fixed learning rates. This produces three different sparse MLPs,
inspired by the model soups ensemble method [ 60]. The final en-
semble prediction is the average result of the three branches as in
a bagging ensemble model. Since the ensemble learning process
can be implemented by simultaneous training and inference with
multi-processing programming (e.g., RandomForest [ 9]), the train-
ing duration is not tripled but determined by the slowest converging
branch.
4 EXPERIMENTS
In this section, we first compare our T-MLP with advanced DNNs
and classical GBDTs on their dominating benchmarks (including 88
datasets for different task types) and analyze from the perspective
of cost-effectiveness. Next, we conduct ablation and comparison
experiments with multi-facet analysis to evaluate the key designs
that make T-MLP effective. Besides, we compare the optimizedpatterns of common DNNs, GBDTs, and T-MLP by visualizing their
decision boundaries to further examine the superiority of T-MLP.
4.1 Experimental Setup
Datasets. We use four recent high-quality tabular benchmarks
(FT-Transformer2(FT-T, 11 datasets) [ 22], T2G-Former3(T2G, 12
datasets) [ 62], SAINT4(26 datasets) [ 48], and Tabular Benchmark5
(TabBen, 39 datasets) [ 23]), considering their elaborated results on
extensive baselines and datasets. The FT-T and T2G benchmarks
are representative of large-scale tabular datasets, whose sizes vary
from 10K to 1,000K and include various DNN baselines. The SAINT
benchmark is gathered from the OpenML repository6, and is dom-
inated by the pre-trained DNN SAINT, containing balanced task
types and diverse GBDTs. TabBen is based on â€œtypical tabular dataâ€
settings that constrain dataset properties, e.g., the data scale (a
maximum data volume of 10K) and the feature number (not high-
dimension) [ 23], and the datasets are categorized into several types
with combinations of task types and feature characteristics. No-
tably, on TabBen, GBDTs achieve overwhelming victory, surpassing
commonly-used DNNs. Each benchmark represents a specific frame-
work preference. Since several benchmarks have adjusted dataset
arrangements in their current repositories (e.g., some datasets were
removed and some were added), to faithfully follow and reuse the
results, we only retain the datasets reported in the published origi-
nal papers. We provide detailed benchmark statistical information
in Table 2 and discuss benchmark characteristics in Appendix A.
T2GSAINTTabBen
Figure 2: The winning rates of GBDTs and DNNs on three
benchmarks, which represent the proportion of each frame-
work achieving the best performance in the benchmarks. It
exhibits varying framework preferences among the datasets
used in different tabular prediction works.
Implementation Details. We implement our T-MLP model and
Python package using PyTorch on Python 3.10. Since the reported
baseline training durations on the original benchmarks are esti-
mated under different runtime environments and using different
evaluation codes, and do not consider hyperparameter tuning (HPT)
budgets, for uniform comparison of training costs, we encapsulate
the experimental baselines with the same sklearn-style APIs as
T-MLP in our built package, and conduct all the experiments on
2https://github.com/yandex-research/rtdl-revisiting-models/tree/main
3https://github.com/jyansir/t2g-former/tree/master
4https://github.com/somepago/saint/tree/main
5https://github.com/LeoGrin/tabular-benchmark/tree/main
6https://www.openml.org
3683KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu
Table 2: Dataset statistics on four experimental benchmarks. â€œ# bin., # mul., and # reg.â€ are the amounts of binary classification,
multi-class classification, and regression datasets. â€œ# small, # middle, # large, and # ex. largeâ€ represent the amounts of small
(ğ‘â‰¤3K), middle (3K <ğ‘â‰¤10K), large (10K <ğ‘â‰¤100K), and extremely large ( ğ‘>100K) datasets, where ğ‘denotes the
training data size. â€œ# wide and # ex. wideâ€ are the amounts of wide (32 <ğ¹â‰¤64) and extremely wide ( ğ¹>64) datasets, where ğ¹
is the feature amount. â€œbin. metric, mul. metric, and reg. metricâ€ represent the evaluation metrics used for each task type in
the benchmarks. â€œR-Squaredâ€ score is the coefficient of determination.
# bin. # mul. # reg. # small # middle # large # ex. large # wide # ex. wide bin. metric mul. metric reg. metric
FT-T [22] 3 4 4 0 0 6 5 2 5 ACC ACC RMSE
T2G [62] 3 5 4 0 3 7 2 2 2 ACC ACC RMSE
SAINT [48] 9 7 10 10 3 12 1 6 9 AUC ACC RMSE
TabBen [23] 15 0 24 2 37 0 0 5 2 ACC N/A R-Squared
Table 3: Cost-effectiveness comparison on the FT-T benchmark. Classification datasets and regression datasets are evaluated
using the accuracy and RMSE metrics, respectively. â€œRankâ€ denotes the average values (standard deviations) of all the methods
across the datasets. â€œ ğ‘‡â€ represents the average overhead of the used training time against T-MLP, and â€œ ğ‘‡âˆ—â€ compares only the
duration before achieving the best validation scores. All the training durations are estimated with the original hyperparameter
search settings. â€œ ğ‘ƒâ€ denotes the average parameter number of the best model configuration provided by the FT-T repository.
TabNet is not compared considering its different backend (Tensorflow) in the evaluation. The top performances are marked in
bold, and the second best ones are underlined (similar marks are used in the subsequent tables).
CAâ†“ADâ†‘HEâ†‘JAâ†‘HIâ†‘ALâ†‘EPâ†‘ YEâ†“COâ†‘YAâ†“MIâ†“ Rankğ‘‡ ğ‘‡âˆ—ğ‘ƒ(M)
TabNet 0.510 0.850 0.378 0.723 0.719 0.954 0.8896 8.909 0.957 0.823 0.751 9.0 (1.5) N/A N/A N/A
SNN 0.493 0.854 0.373 0.719 0.722 0.954 0.8975 8.895 0.961 0.761 0.751 7.8 (1.1)Ã—42.76Ã—24.87 1.12
AutoInt 0.474 0.859 0.372 0.721 0.725 0.945 0.8949 8.882 0.934 0.768 0.750 7.4 (2.1)Ã—121.68Ã—112.31 1.14
GrowNet 0.487 0.857 N/A N/A 0.722 N/A 0.8970 8.827 N/A 0.765 0.751 N/A N/A N/A N/A
MLP 0.499 0.852 0.383 0.719 0.723 0.954 0.8977 8.853 0.962 0.757 0.747 6.5 (1.7)Ã—27.41Ã—28.46 0.55
DCNv2 0.484 0.853 0.385 0.716 0.723 0.955 0.8977 8.890 0.965 0.757 0.749 6.4 (1.8)Ã—31.15Ã—40.65 4.17
NODE 0.464 0.858 0.359 0.727 0.726 0.918 0.8958 8.784 0.958 0.753 0.745 5.4 (3.2)Ã—386.54Ã—353.38 16.59
ResNet 0.486 0.854 0.396 0.728 0.727 0.963 0.8969 8.846 0.964 0.757 0.748 4.5 (2.2)Ã—56.20Ã—58.46 6.16
FT-T 0.459 0.859 0.391 0.732 0.720 0.960 0.8982 8.855 0.970 0.756 0.746 3.3 (2.4)Ã—117.35Ã—97.49 2.12
T-MLP 0.447 0.864 0.386 0.728 0.729 0.956 0.8977 8.768 0.968 0.756 0.747 3.1 (0.9)Ã—1.00Ã—1.00 0.79
T-MLP(3) 0.438 0.867 0.386 0.732 0.730 0.960 0.8978 8.732 0.969 0.755 0.745 1.7 (0.8)Ã—1.05Ã—1.08 2.37
Table 4: Cost-effectiveness comparison on the T2G benchmark with similar notations as in Table 3. The baseline performances
and configurations are also reused from the T2G repository. According to the T2G paper, for the extremely large dataset Year,
FT-T and T2G use 50-iteration hyperparameter tuning (HPT), DANet-28 follows its default hyperparameters, and the other
baseline results are acquired with 100-iteration HPT.
GEâ†‘CHâ†‘EYâ†‘CAâ†“HOâ†“ADâ†‘OTâ†‘HEâ†‘JAâ†‘HIâ†‘FBâ†“YEâ†“ Rankğ‘‡ ğ‘‡âˆ—ğ‘ƒ(M)
XGBoost 0.684 0.859 0.725 0.436 3.169 0.873 0.825 0.375 0.719 0.724 5.359 8.850 4.3 (3.1)Ã—32.78Ã—42.88 N/A
MLP 0.586 0.858 0.611 0.499 3.173 0.854 0.810 0.384 0.720 0.720 5.943 8.849 8.3 (1.9)Ã—13.73Ã—11.45 0.64
SNN 0.647 0.857 0.616 0.498 3.207 0.854 0.812 0.372 0.719 0.722 5.892 8.901 8.3 (1.5)Ã—22.74Ã—12.54 0.82
TabNet 0.600 0.850 0.621 0.513 3.252 0.848 0.791 0.379 0.723 0.720 6.559 8.916 10.2 (2.4) N/A N/A N/A
DANet-28 0.616 0.851 0.605 0.524 3.236 0.850 0.810 0.355 0.707 0.715 6.167 8.914 10.6 (2.0) N/A N/A N/A
NODE 0.539 0.859 0.655 0.463 3.216 0.858 0.804 0.353 0.728 0.725 5.698 8.777 7.0 (3.0)Ã—329.79Ã—288.21 16.95
AutoInt 0.583 0.855 0.611 0.472 3.147 0.857 0.801 0.373 0.721 0.725 5.852 8.862 8.1 (2.0)Ã—68.30Ã—55.52 0.06
DCNv2 0.557 0.857 0.614 0.489 3.172 0.855 0.802 0.386 0.716 0.722 5.847 8.882 8.4 (2.0)Ã—24.40Ã—21.63 2.30
FT-T 0.613 0.861 0.708 0.460 3.124 0.857 0.813 0.391 0.732 0.731 6.079 8.852 4.7 (2.6)Ã—64.68Ã—50.90 2.22
T2G 0.656 0.863 0.782 0.455 3.138 0.860 0.819 0.391 0.737 0.734 5.701 8.851 3.1 (1.7)Ã—88.93Ã—87.04 1.19
T-MLP 0.706 0.862 0.717 0.449 3.125 0.864 0.814 0.386 0.728 0.729 5.667 8.768 3.3 (0.9)Ã—1.00Ã—1.00 0.72
T-MLP(3) 0.714 0.866 0.747 0.438 3.063 0.867 0.823 0.386 0.732 0.730 5.629 8.732 1.9 (0.8)Ã—1.09Ã—1.11 2.16
3684Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
NVIDIA A100 PCIe 40GB. All the hyperparameter spaces and iter-
ation numbers of the baselines follow the settings in the original
papers to emulate the tuning process of each baseline. For T-MLP,
we use fixed hyperparameters as the model is trained only once.
The XGBoost used for T-MLPâ€™s GBDT Feature Gate is in default
configuration as in [ 22]. In experiments, each single T-MLP uses one
basic block for most datasets if without special specification. We
uniformly use a learning rate of 1e-4 for a single T-MLP and learn-
ing rates of 1e-4, 5e-4, and 1e-3 for the three branches in the T-MLP
ensemble (group â€œT-MLP(3)â€). We reuse the same data splits as in
the original benchmarks. The baseline performances are inherited
from the reported benchmark results, and the baseline capacities
are calculated based on the best model configurations provided in
the corresponding paper repositories. Detailed information of the
runtime environment and hyperparameters is given in Appendix C.
Compared Methods. On the four benchmarks, we compare our
T-MLP (the single-model and 3-model-ensemble versions) with: (1)
known non-pre-trained DNNs: MLP, ResNet, SNN [ 33], GrowNet [ 4],
TabNet [ 2], NODE [ 42], AutoInt [ 49], DCNv2 [ 57], TabTransformer [ 28],
DANets [ 13], FT-Transformer (FT-T) [ 22], and T2G-Former (T2G) [ 62];
(2) pre-trained DNN: SAINT [ 48]; (3) GBDT models: XGBoost [ 16],
CatBoost [ 43], LightGBM [ 31], GradientBoostingTree (GBT), Hist-
GradientBoostingTree (HistGBT), and other traditional non-deep
machine learning methods like RandomForest (RF) [ 9]. For other
unmentioned baselines, please refer to Appendix B. In the experi-
ment tables below, â€œT-MLPâ€ denotes a single T-MLP and â€œT-MLP(3)â€
denotes the ensemble version with three branches.
4.2 Main Results and Analysis
In Table 3 to Table 6, the baseline results are based on heavy HPT,
and are obtained from respectively reported benchmarks. All the
T-MLP results are based on default hyperparameters.
Comparison with Advanced DNNs. Tables 3 and 4 report de-
tailed performances and runtime costs on the FT-T and T2G bench-
marks for comparison of our T-MLP versions and bespoke tabular
DNNs [ 22,62]. The baseline results in these tables are based on 50
(for complicated models on large datasets, e.g., FT-Transformer on
the Year dataset) or 100 (the other cases) iterations of HPT except
special models (default NODE for the datasets with large class num-
bers and default DANets for all datasets). An overall trend that one
may observe is that the single T-MLP is able to achieve competitive
results as the state-of-the-art DNNs on each benchmark, and a sim-
ple ensemble of three T-MLPs (i.e., â€œT-MLP(3)â€) exhibits even better
performances with significantly reduced training costs. Specifically,
benefiting from fixed hyperparameters and simple structures, the
single T-MLP achieves obvious speedup and reduces training du-
rations by orders of magnitude compared to the powerful DNNs,
and is also more training-friendly than XGBoost, a representative
GBDT that highly relies on heavy HPT. Besides, we observe only
about 10% training duration increase in T-MLP ensemble since we
adopt multiprocessing programming to simultaneously train the
three T-MLPs (see Sec. 3.5) and thus the training time depends
on the slowest converging sub-model. In the implementation de-
tails (Sec. 4.1), the single T-MLP uses the smallest learning rate in
the three sub-models, and hence the convergence time of T-MLPTable 5: The average values (standard deviations) of all the
method ranks on the SAINT benchmark of three task types.
|ğ·|is the dataset number in each group. Notably, all the base-
line results are based on HPT, and SAINT variants need fur-
ther training budgets on pre-training and data augmentation.
More detailed results are given in the Appendix.
Task Type:Binclass
(|ğ·|=9)Multiclass
(|ğ·|=7)Regression
(|ğ·|=10)
RF 7.8 (3.3) 7.3 (2.2) 9.1 (4.2)
ExtraTrees 7.8 (3.8) 9.6 (1.9) 8.6 (3.5)
KNeighborsDist 13.7 (0.7) 11.6 (3.5) 12.9 (1.8)
KNeighborsUnif 14.4 (0.5) 12.4 (3.4) 14.0 (1.0)
LightGBM 5.7 (3.3) 3.9 (2.8) 6.5 (3.2)
XGBoost 4.2 (2.8) 6.7 (3.5) 7.3 (2.9)
CatBoost 3.9 (2.8) 7.2 (2.4) 5.6 (2.7)
MLP 10.7 (1.8) 10.1 (3.9) N/A
NeuralNetFastAI N/A N/A 11.9 (2.2)
TabNet 13.2 (2.0) 13.5 (1.1) 10.2 (4.5)
TabTransformer 10.8 (1.4) 10.0 (3.6) 10.0 (2.9)
SAINT-s 7.8 (2.4) 7.9 (6.1) 4.7 (3.8)
SAINT-i 7.2 (2.6) 7.1 (2.7) 5.9 (3.5)
SAINT 4.2 (2.7) 5.2 (2.2) 4.2 (2.3)
T-MLP 4.6 (2.8) 4.6 (3.0) 4.6 (3.3)
T-MLP(3) 3.9 (1.9) 2.9 (2.5) 5.0 (2.9)
ensemble often approximates that of the single T-MLP. From the
perspective of model storage, as expected, the size of the single
T-MLP is comparable to the average level of naive MLPs across the
datasets and its size variation is stable (see Table 1), since the block
number, hidden dimension size, and sparsity rate are all fixed. In
Sec. 4.3, we will further analyze the impact of model sparsity and
theoretical complexity of the model parameters.
Comparison with Pre-trained DNNs. Table 5 reports the means
and standard deviations of model ranks on the SAINT benchmark [ 48].
Surprisingly, we find that the simple pure MLP-based T-MLP outper-
forms Transformer-based SAINT variants (SAINT-s and SAINT-i)
and is comparable with SAINT on all the three task types. It is
worth noting that SAINT and its variants adopt complicated inter-
sample attention and self-supervised pre-training along with HPT
on parameters of the training process. Moreover, T-MLP ensemble
even achieves stable results that are competitive to tuned GBDTs
(i.e., XGBoost, CatBoost, LightGBM) and surpasses the pre-trained
SAINT on classification tasks. Since the detailed HPT conditions
(i.e., iteration times, HPT methods, parameter sampling distribu-
tions) are not reported, we do not estimate specific training costs.
Comparison with Extensively Tuned GBDTs. Table 6 compares
T-MLP on the typically GPDTs-dominating benchmark TabBen [ 23],
on which GBDT frameworks completely outperform various DNNs
across all types of datasets. Results of each baseline on TabBen are
obtained with around 400 iterations of heavy HPT, almost repre-
senting the ultimate performances with unlimited computation re-
sources and budgets. As expected, when extensively tuned XGBoost
is available, the single T-MLP is eclipsed, but it is still competitive to
the other ensemble tree models (i.e., RF, GBT, HistGBT) and superior
to the compared DNNs. Further, we find that T-MLP ensemble is
3685KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu
Table 6: The average values (standard deviations) of all the
method ranks on TabBen (four dataset types). â€œNum.â€ and
â€œCat.â€ denote numerical datasets (all features are numeri-
cal) and categorical datasets (some features are categorical),
respectively. â€œClassif.â€ and â€œReg.â€ denote classification and
regression tasks. â€œNum. Reg.â€ group includes only results of
regression on numerical datasets (similar notations are for
the others).|ğ·|is the dataset number in each group. Baseline
test results are obtained based on the best validation results
duringâˆ¼400 iterations of HPT (according to the TabBen paper
and repository). Detailed results are given in the Appendix.
Dataset Type:Num. Classif.
(|ğ·|=9)Num. Reg.
(|ğ·|=14)Cat. Classif.
(|ğ·|=6)Cat. Reg.
(|ğ·|=10)
MLP 8.4 (0.8) N/A N/A N/A
ResNet 6.9 (1.9) 6.5 (1.9) 7.8 (1.0) 7.7 (0.5)
FT-T 5.7 (1.9) 5.5 (2.3) 5.5 (2.2) 6.7 (1.1)
SAINT 6.9 (1.4) 5.5 (2.2) 8.0 (1.1) N/A
GBT 4.7 (2.0) 4.3 (1.7) 5.2 (2.3) 4.3 (1.1)
HistGBT N/A N/A 5.2 (2.3) 4.3 (1.3)
RF 4.6 (2.1) 4.8 (2.2) 4.0 (3.2) 5.8 (1.9)
XGBoost 2.6 (1.4) 2.4 (1.5) 2.8 (1.5) 2.1 (1.0)
T-MLP 3.2 (1.6) 4.3 (1.9) 3.5 (2.3) 3.6 (1.4)
T-MLP(3) 2.1 (1.4) 2.7 (1.5) 3.0 (1.3) 1.8 (0.7)
able to be comparable to the ultimate XGBoost in all the four dataset
types with similar rank stability, serving as a candidate for a tuned
XGBoost alternative. More significantly, in the experiments, each
T-MLP (or T-MLP ensemble) employs a tensorized XGBoost trained
in default configuration (see implementation details in Sec. 4.1),
and all the other hyperparameters are fixed; thus T-MLP and its
ensemble have potential capability of a higher performance ceiling
by HPT or selecting other GBDTs as the feature gate.
In summary, we empirically show the strong potential of our
hybrid framework to achieve flexible and generalized data adapt-
ability with various tabular preferences (tabular data preferring
advanced DNNs, pre-training, or GBDTs). Based on the impressive
economical performance-cost trade-off and friendly training pro-
cess, T-MLP can serve as a promising tabular model framework
for real-world applications, especially under limited computation
budgets.
4.3 What Makes T-MLP Cost-effective?
Table 7 reports ablation and comparison experimental results of T-
MLP on several classification and regression datasets (i.e., California
Housing (CA) [ 40], Adult (AD) [ 34], Higgs (HI) [ 5], and Year (YE) [ 6])
in various data scales (given in parentheses).
Main Ablations. The top four rows in Table 7 report the impact of
two key designs in a single T-MLP. An overall observation is that
both the structure sparsity and GBDT feature gate (FG) contribute to
performance enhancement of T-MLP. From the perspective of data
processing, GBDT FG brings local sparsity through sample-specific
feature selection, and the sparse MLP structure offers global sparsity
shared by all samples. Interestingly, we find that the impact of GBDT
FG is more profound on the CA dataset. A possible explanation
is that the feature amount of CA (8 features) is relatively smallTable 7: Main ablation and comparison on classical tables in
various task types and data scales. The top 4 rows: ablations
on key designs in the T-MLP framework. The bottom 2 rows:
results of T-MLP with neural network feature gate (NN FG).
Dataset: CA (21K)â†“AD (49K)â†‘HI (98K)â†‘YE (515K)â†“
T-MLP 0.4471 0.864 0.729 8.768
w/o sparsity 0.4503 0.857 0.726 8.887
w/o GBDT FG 0.4539 0.859 0.728 8.799
w/o both 0.4602 0.856 0.724 8.896
T-MLP (NN FG) 0.4559 0.852 0.718 8.925
w/o sparsity 0.4557 0.840 0.713 8.936
compared to the others (14, 28, and 90 features in AD, HI, and YE,
respectively) and the average feature importance may be relatively
large; thus, the CA results are more likely to be affected by feature
selection. For the datasets with larger feature amounts, selecting
effective features is likely to be more difficult.
Greedy Feature Selection. We notice a recent attempt on sample-
specific sparsity for biomedical tables using a gating network; it
was originally designed for low-sample-size tabular settings and
helped prediction interpretability in the biomedical domain [ 64].
We use its code and build a T-MLP version by substituting GBDT
FG with the neural network feature gate (NN FG) for comparison.
The bottom two rows of Table 7 report the results. As expected, on
the smallest dataset CA, NN FG can boost performance by learning
to select informative features, but such a feature gating strategy
consistently hurts the performance as data scales increase. This may
be due to (1) large datasets demand more complicated structures
to learn the meticulous feature selection, (2) the discrete nature of
the selection behavior is incompatible with smooth optimization
patterns of neural networks, and (3) DNNsâ€™ confirmation bias [ 52]
may mislead the learning process, i.e., NN FG will be ill-informed
once the subsequent neural network captures wrong patterns. In
contrast, GBDT FG always selects features greedily as real GBDTs,
which is conservative and generally reasonable. Besides, the com-
plicated sub-tree structures are more complete for the selection
action.
Sparsity Promotes Tabular DNNs. Fig. 3 plots performance vari-
ations on two classification/regression tasks with respect to T-MLP
sparsity. Different from the pruning techniques in NLP that aim to
trim down model sizes while maintaining the ability of the original
models, we find that suitable model sparsity often promotes tabu-
lar prediction, but both excessive and insufficient sparsity cannot
achieve the best results. The results empirically indicate that, com-
pared to DNN pruning in large pre-trained models for unstructured
data, in the tabular data domain, the pruning has the capability
to promote non-large tabular DNNs as GBDTsâ€™ beneficial sparse
structures achieved by tree pre-pruning, and the hidden dimension
in tabular DNNs is commonly over-parameterized.
4.4 Superiority Interpretability of T-MLP
In Fig. 4, we visualize decision boundaries of FT-Transformer, XG-
Boost, and the single T-MLP to inspect data patterns captured by
3686Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 3: Performance variation plots on the Adult and Year
datasets with respect to variations of T-MLP sparsity. All the
best results are achieved with suitable sparsity.
FT-T (82%)XGB (84%)T-MLP (85%)
FT-T (77%)XGB (76%)T-MLP (80%)Credit-g Bioresponse
Figure 4: Decision boundary visualization of FT-Transformer
(FT-T), XGBoost, and a single-block T-MLP on the Biore-
sponse and Credit-g datasets, using two most important fea-
tures. Different colors represent distinct categories, while the
varying shades of colors indicate the predicted probabilities.
these three methods. The two most important features are selected
by mutual information (estimated with the Scikit Learn package).
Different from common DNNs and GBDTs, T-MLP exhibits a novel
intermediate pattern that combines characteristics from both DNNs
and GBDTs. Compared to DNNs, T-MLP yields grid-like boundaries
whose edges are often orthogonal to feature surfaces as GBDTs,
and the complexity is essentially simplified with pruned sparse
architectures. Besides, T-MLP is able to capture tree-model-like sub-
patterns (see T-MLP on Credit-g), while DNNs manage only main
patterns. Hence, DNNs are overfit-sensitive due to their relatively
irregular boundaries and neglecting of fine-grained sub-patterns.
Compared to GBDTs with jagged boundaries and excessively split
sub-patterns, T-MLP holds very smooth vertices at the intersection
of boundaries (see T-MLP on Credit-g). Notably, T-MLP can decide
conditional split points like GBDT feature splitting (orthogonal
edges at feature surfaces) through a smooth process (see T-MLP
boundary edges on Bioresponse, from top to bottom, in which
the split point on the horizontal feature is conditionally changed
with respect to the vertical feature in a smooth manner, while XG-
Boost is hard to attain such dynamical split points). Overall, T-MLPpossesses both the advantages to be overfit-resistant, which helps
provide its superiority on both GBDT- and DNN-favored datasets.
5 CONCLUSIONS
In this paper, we proposed T-MLP, a novel hybrid framework attain-
ing the advantages of both GBDTs and DNNs to address the model
selection dilemma in tabular prediction tasks. We combined a ten-
sorized GBDT feature gate, DNN pruning techniques, and a vanilla
back-propagation optimizer to develop a simple yet efficient and
widely effective MLP model. Experiments on diverse benchmarks
showed that, with significantly reduced runtime costs, T-MLP has
the generalized adaptability to achieve considerably competitive
results regardless of dataset-specific framework preferences. We
expect that our T-MLP will serve as a practical method for econom-
ical tabular prediction as well as in broad applications, and help
advance research on hybrid tabular models.
ACKNOWLEDGMENTS
This research was partially supported by National Natural Sci-
ence Foundation of China under grants No. 62176231, Zhejiang
Key R&D Program of China under grant No. 2023C03053 and No.
2024SSYS0026.
REFERENCES
[1]Naomi S Altman. 1992. An introduction to kernel and nearest-neighbor nonpara-
metric regression. The American Statistician 46, 3 (1992), 175â€“185.
[2]Sercan Ã– Arik and Tomas Pfister. 2021. TabNet: Attentive interpretable tabular
learning. In AAAI. 6679â€“6687.
[3]Saqib Aziz, Michael Dowling, Helmi Hammami, and Anke Piepenbrink. 2022.
Machine learning in finance: A topic modeling approach. European Financial
Management (2022).
[4]Sarkhan Badirli, Xuanqing Liu, Zhengming Xing, Avradeep Bhowmik, Khoa
Doan, and Sathiya S Keerthi. 2020. Gradient boosting neural networks: GrowNet.
arXiv preprint arXiv:2002.07971 (2020).
[5]Pierre Baldi, Peter Sadowski, et al .2014. Searching for exotic particles in high-
energy physics with deep learning. Nature Communications 5, 1 (2014), 4308.
[6]Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. 2011.
The million song dataset. In ISMIR.
[7]Vadim Borisov, Tobias Leemann, Kathrin SeÃŸler, Johannes Haug, Martin Pawel-
czyk, and Gjergji Kasneci. 2022. Deep neural networks and tabular data: A survey.
IEEE Transactions on Neural Networks and Learning Systems (2022).
[8]Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji
Kasneci. 2022. Language Models are Realistic Tabular Data Generators. In ICLR.
[9] Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5â€“32.
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. In NeurIPS, Vol. 33.
1877â€“1901.
[11] Guiping Cao, Shengda Luo, Wenjian Huang, Xiangyuan Lan, Dongmei Jiang,
Yaowei Wang, and Jianguo Zhang. 2023. Strip-MLP: Efficient Token Interaction
for Vision MLP. In ICCV. 1494â€“1504.
[12] Jintai Chen, KuanLun Liao, Yanwen Fang, Danny Chen, and Jian Wu. 2022.
TabCaps: A Capsule Neural Network for Tabular Data Classification with BoW
Routing. In ICLR.
[13] Jintai Chen, Kuanlun Liao, Yao Wan, Danny Z Chen, and Jian Wu. 2022. DANets:
Deep abstract networks for tabular data classification and regression. In AAAI.
[14] Jintai Chen, Jiahuan Yan, Danny Ziyi Chen, and Jian Wu. 2023. ExcelFormer:
A Neural Network Surpassing GBDTs on Tabular Data. arXiv preprint
arXiv:2301.02819 (2023).
[15] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O Arik, and Tomas Pfister. 2023.
TSMixer: An All-MLP architecture for time series forecasting. arXiv preprint
arXiv:2303.06053 (2023).
[16] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting system.
InProceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. 785â€“794.
[17] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for YouTube recommendations. In Proceedings of the 10th ACM Conference on
Recommender Systems.
3687KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu
[18] Jerome H Friedman. 2001. Greedy function approximation: A gradient boosting
machine. Annals of Statistics (2001).
[19] Francesco Fusco, Damian Pascual, Peter Staar, and Diego Antognini. 2023. pNLP-
Mixer: An Efficient all-MLP Architecture for Language. In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics. Association for
Computational Linguistics, 53â€“60.
[20] Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized
trees. Machine learning 63 (2006), 3â€“42.
[21] Yury Gorishniy, Ivan Rubachev, and Artem Babenko. 2022. On embeddings for
numerical features in tabular deep learning. In NeurIPS. 24991â€“25004.
[22] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021.
Revisiting deep learning models for tabular data. In NeurIPS. 18932â€“18943.
[23] LÃ©o Grinsztajn, Edouard Oyallon, and GaÃ«l Varoquaux. 2022. Why do tree-based
models still outperform deep learning on typical tabular data?. In NeurIPS.
[24] Jianyuan Guo, Yehui Tang, et al .2022. Hire-MLP: Vision MLP via hierarchical
rearrangement. In CVPR. 826â€“836.
[25] Xinran He, Junfeng Pan, et al .2014. Practical lessons from predicting clicks on
ads at Facebook. In Proceedings of the International Workshop on Data Mining for
Online Advertising.
[26] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. 2020.
DynaBERT: Dynamic BERT with adaptive width and depth. In NeurIPS, Vol. 33.
9782â€“9793.
[27] Jeremy Howard and Sylvain Gugger. 2020. Fastai: A layered API for deep learning.
Information 11, 2 (2020), 108.
[28] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. 2020. TabTrans-
former: Tabular data modeling using contextual embeddings. arXiv preprint
arXiv:2012.06678 (2020).
[29] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021. Well-
tuned simple nets excel on tabular datasets. In NeurIPS. 23928â€“23941.
[30] Liran Katzir, Gal Elidan, and Ran El-Yaniv. 2020. Net-DNF: Effective deep modeling
of tabular data. In ICLR.
[31] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting
decision tree. In NeurIPS.
[32] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL-HLT. 4171â€“4186.
[33] GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.
2017. Self-normalizing neural networks. In NeurIPS, Vol. 30.
[34] Ron Kohavi et al .1996. Scaling up the accuracy of Naive-Bayes classifiers: A
decision-tree hybrid. In KDD, Vol. 96. 202â€“207.
[35] Bin Li, J Friedman, R Olshen, and C Stone. 1984. Classification and regression
trees (CART). Biometrics (1984).
[36] Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. 2021. Pay attention to MLPs.
InNeurIPS. 9204â€“9215.
[37] Christos Louizos, Max Welling, and Diederik P Kingma. 2018. Learning Sparse
Neural Networks through L_0 Regularization. In ICLR.
[38] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the Struc-
tural Pruning of Large Language Models. In NeurIPS.
[39] Tomas Mikolov, Kai Chen, et al .2013. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781 (2013).
[40] R Kelley Pace and Ronald Barry. 1997. Sparse spatial autoregressions. Statistics
& Probability Letters 33, 3 (1997), 291â€“297.
[41] F. Pedregosa, G. Varoquaux, et al .2011. Scikit-learn: Machine Learning in Python.
Journal of Machine Learning Research 12 (2011), 2825â€“2830.
[42] Sergei Popov, Stanislav Morozov, and Artem Babenko. 2019. Neural Oblivious
Decision Ensembles for Deep Learning on Tabular Data. In ICLR.
[43] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Doro-
gush, and Andrey Gulin. 2018. CatBoost: Unbiased boosting with categorical
features. In NeurIPS.
[44] Alec Radford, Jong Wook Kim, et al .2021. Learning transferable visual models
from natural language supervision. In ICML. 8748â€“8763.
[45] Camilo Ruiz, Hongyu Ren, Kexin Huang, and Jure Leskovec. 2023. Enabling
tabular deep learning when ğ‘‘â‰«ğ‘›with an auxiliary knowledge graph. arXiv
preprint arXiv:2306.04766 (2023).
[46] Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu. 2017. Interpretable convo-
lutional neural networks with dual local and global attention for review rating
prediction. In Proceedings of the 11th ACM Conference on Recommender Systems.
297â€“305.
[47] Ravid Shwartz-Ziv and Amitai Armon. 2022. Tabular data: Deep learning is not
all you need. Information Fusion 81 (2022), 84â€“90.
[48] Gowthami Somepalli, Avi Schwarzschild, Micah Goldblum, C Bayan Bruss, and
Tom Goldstein. 2022. SAINT: Improved Neural Networks for Tabular Data
via Row Attention and Contrastive Pre-Training. In NeurIPS 2022 First Table
Representation Workshop.
[49] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. AutoInt: Automatic feature interaction learning via self-
attentive neural networks. In CIKM. 1161â€“1170.[50] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023. A Simple
and Effective Pruning Approach for Large Language Models. arXiv preprint
arXiv:2306.11695 (2023).
[51] Chuanxin Tang, Yucheng Zhao, et al .2022. Sparse MLP for image recognition: Is
self-attention really necessary?. In AAAI. 2344â€“2351.
[52] Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models:
Weight-averaged consistency targets improve semi-supervised deep learning
results. In NeurIPS, Vol. 30.
[53] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob
Uszkoreit, et al .2021. MLP-Mixer: An all-MLP architecture for vision. In NeurIPS.
24261â€“24272.
[54] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan
Bovik, and Yinxiao Li. 2022. MAXIM: Multi-Axis MLP for image processing. In
CVPR. 5769â€“5780.
[55] Shahadat Uddin, Arif Khan, Md Ekramul Hossain, and Mohammad Ali Moni.
2019. Comparing different supervised machine learning algorithms for disease
prediction. BMC Medical Informatics and Decision Making (2019), 1â€“16.
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS.
[57] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. DCN V2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In WWW. 1785â€“1797.
[58] Zifeng Wang and Jimeng Sun. 2022. TransTab: Learning transferable tabular
Transformers across tables. In NeurIPS, Vol. 35. 2902â€“2915.
[59] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020. Structured Pruning of Large
Language Models. In EMNLP. 6151â€“6162.
[60] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael
Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,
Simon Kornblith, et al .2022. Model soups: Averaging weights of multiple fine-
tuned models improves accuracy without increasing inference time. In ICML.
23965â€“23998.
[61] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured Pruning Learns
Compact and Accurate Models. In ACL.
[62] Jiahuan Yan, Jintai Chen, Yixuan Wu, Danny Z Chen, and Jian Wu. 2023. T2G-
Former: Organizing tabular features into relation graphs promotes heterogeneous
feature interaction. In AAAI.
[63] Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun,
Jian Wu, and Jintai Chen. 2024. Making Pre-trained Language Models Great on
Tabular Prediction. In ICLR.
[64] Junchen Yang, Ofir Lindenbaum, and Yuval Kluger. 2022. Locally sparse neural
networks for tabular biomedical data. In ICML. PMLR, 25123â€“25153.
[65] Jun Zhang and Vasant Honavar. 2003. Learning from attribute value taxonomies
and partially specified instances. In ICML.
[66] Jun Zhang, D-K Kang, et al .2006. Learning accurate and concise NaÃ¯ve Bayes
classifiers from attribute value taxonomies and data. Knowledge and Information
Systems (2006).
[67] Tianping Zhang, Shaowen Wang, Shuicheng Yan, Jian Li, and Qian Liu. 2023.
Generative Table Pre-training Empowers Models for Tabular Prediction. arXiv
preprint arXiv:2305.09696 (2023).
[68] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey
of large language models. arXiv preprint arXiv:2303.18223 (2023).
[69] Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa
Shoaran. 2023. XTab: Cross-table Pretraining for Tabular Transformers. In ICML.
A BENCHMARK CHARACTERISTICS
We provide detailed dataset statistical information of each bench-
mark in Table 2. These benchmarks exhibit broad data diversity in
data scales and task types. From the FT-T benchmark to TabBen, the
overall data volume is gradually reduced. We additionally visualize
the respective winning rates of GBDT and DNN frameworks in
Fig. 2, indicating varying framework preferences among the dataset
collections used in different tabular prediction tasks. FT-T does not
include GBDT baselines in its main benchmark, but has the most
extremely large datasets. Overall, the FT-T benchmark is the ex-
tremely large-scale data collection (in both data volume and feature
width), the T2G benchmark is a large one, the SAINT benchmark
contains diverse data scales, and TabBen focuses on middle-size
typical tables.
3688Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
B BASELINE INFORMATION
We list all the compared baselines in this section.
â€¢MLP: Vanilla multi-layer perception with no feature interac-
tion.
â€¢ResNet: A popular DNN backbone in vision applications.
â€¢SNN [33]: An MLP-like architecture with SELU activation.
â€¢GrowNet [4]: MLPs built in a gradient boosted manner.
â€¢NODE [42]: Generalized oblivious decision tree ensembles.
â€¢TabNet [ 2]: A Transformer-based recurrent architecture em-
ulating tree-based learning process.
â€¢AutoInt [49]: Attention-based feature embeddings.
â€¢DCNv2 [ 57]: An MLP-based architecture with the feature-
crossing module.
â€¢TabTransformer [ 28]: A Transformer model concatenating
numerical features and encoded categorical features.
â€¢DANets [ 13]: An MLP-based architecture with neural-guided
feature selection and abstraction in each block.
â€¢FT-Transformer [ 22]: A popular tabular Transformer encod-
ing both numerical and categorical features.
â€¢T2G-Former [ 62]: A tabular Transformer with automatic
relation graph estimation for selective feature interaction.
â€¢SAINT [ 48]: A Transformer-like architecture performing
row-level and column-level attention, and contrastively pre-
training to minimize the differences between data points and
their augmented views.
â€¢XGBoost [16]: A predominant GBDT implementation.
â€¢CatBoost [ 43]: A GBDT approach with oblivious decision
trees.
â€¢LightGBM [31]: An efficient GBDT implementation.
â€¢RandomForest [ 9]: A popular bagging ensemble algorithm
of decision trees.â€¢ExtraTrees [20]: A classical tree bagging implementation.
â€¢k-NN [ 1]: Traditional supervised machine learning algo-
rithms; two KNeighbors models are used (KNeighborsDist,
KNeighborsUnif).
â€¢NeuralNetFastAI [ 27]: FastAI neural network models that
operate on tabular data.
â€¢sklearn-GBDT [ 41]: Two traditional GBDT implementations
(GradientBoostingTree and HistGradientBoostingTrees) pro-
vided in the Scikit Learn package.
C RUNTIME ENVIRONMENT AND
HYPERPARAMETERS
C.1 Runtime Environment
All the experiments are conducted with PyTorch version 1.11.0,
CUDA version 11.3, and Scikit Learn version 1.1.0, with each trial
using an NVIDIA A100 PCIe 40GB and an Intel Xeon Processor
40C.
C.2 Hyperparameters of T-MLP
In the main experiments, we uniformly set the hidden size ğ‘‘to
1024, the intermediate size ğ‘‘â€²to 676 (2/3 of the hidden size), the
sparsity rate to 0.33, and the residual dropout rate to 0.1, with three
basic blocks for multi-class classification or extremely large binary
classification datasets, and one block for the others. The learning
rate of the single T-MLP is 1e-4, and the learning rates of the three
branches in T-MLP ensemble are 1e-4, 5e-4, and 1e-3, respectively.
C.3 Hyperparameters of Baselines
For all the baselines on the FT-T and T2G benchmarks, we follow the
given hyperparameter spaces and iteration times from the original
benchmark papers to estimate the training costs.
3689