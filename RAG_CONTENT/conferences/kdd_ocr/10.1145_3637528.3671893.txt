Can a Deep Learning Model be a Sure Bet for Tabular Prediction?
Jintai Chenâˆ—
jtchen721@gmail.com
Univ. of Illinois Urbana-Champaign
Urbana, IL, USAJiahuan Yanâˆ—
jyansir@zju.edu.cn
Zhejiang University
Hangzhou, ChinaQiyuan Chen
chenqiyuan1012@gmail.com
Zhejiang University
Hangzhou, China
Danny Z. Chen
dchen@nd.edu
University of Notre Dame
South Bend, IN, USAJian Wu
wujian2000@zju.edu.cn
Zhejiang University
Hangzhou, ChinaJimeng Sun
jimeng@illinois.edu
Univ. of Illinois Urbana-Champaign
Urbana, IL, USA
ABSTRACT
Data organized in tabular format is ubiquitous in real-world applica-
tions, and users often craft tables with biased feature definitions and
flexibly set prediction targets of their interests. Thus, a rapid devel-
opment of a robust, effective, dataset-versatile, user-friendly tabular
prediction approach is highly desired. While Gradient Boosting De-
cision Trees (GBDTs) and existing deep neural networks (DNNs)
have been extensively utilized by professional users, they present
several challenges for casual users, particularly: (i) the dilemma
of model selection due to their different dataset preferences, and
(ii) the need for heavy hyperparameter searching, failing which
their performances are deemed inadequate. In this paper, we delve
into this question: Can we develop a deep learning model that
serves as a sure bet solution for a wide range of tabular prediction
tasks, while also being user-friendly for casual users? We delve
into three key drawbacks of deep tabular models, encompassing:
(P1) lack of rotational variance property, (P2) large data demand,
and (P3) over-smooth solution. We propose ExcelFormer, address-
ing these challenges through a semi-permeable attention module
that effectively constrains the influence of less informative features
to break the DNNsâ€™ rotational invariance property (for P1), data
augmentation approaches tailored for tabular data (for P2), and
attentive feedforward network to boost the model fitting capability
(for P3). These designs collectively make ExcelFormer a sure bet
solution for diverse tabular datasets. Extensive and stratified ex-
periments conducted on real-world datasets demonstrate that our
model outperforms previous approaches across diverse tabular data
prediction tasks, and this framework can be friendly to casual users,
offering ease of use without the heavy hyperparameter tuning. The
codes are available at https://github.com/whatashot/excelformer.
CCS CONCEPTS
â€¢Computing methodologies â†’Artificial intelligence.
*: Co-first Authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/ 08
https://doi.org/10.1145/3637528.3671893KEYWORDS
Tabular data prediction, Mixup
ACM Reference Format:
Jintai Chenâˆ—, Jiahuan Yanâˆ—, Qiyuan Chen, Danny Z. Chen, Jian Wu, and Ji-
meng Sun. 2024. Can a Deep Learning Model be a Sure Bet for Tabular Pre-
diction?. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671893
1 INTRODUCTION
Tabular data is ubiquitous and plays a critical role in real-world
applications, spanning diverse domains, such as medical predic-
tion [ 46,48], market prediction [ 41], and financial risk forecast-
ing [22]. However, unlike fields such as image and natural language
processing, where data from disparate datasets frequently exhibit
similar spatial or sequential feature relations and aligned seman-
tics, tabular data often lack such â€œcommonâ€ and â€œstandardâ€ data
structures. Tables are typically created by casual users for diverse
purposes. The features and targets can be defined subjectively, and
table columns (features) are added or removed arbitrarily, even
sometimes resulting in missing information or adding noise. There-
fore, while some bespoke frameworks following specific inductive
biases have thrived in the domains of image and textual data, achiev-
ing comparable success on tabular data is notably challenging.
Therefore, users are compelled to undergo computationally inten-
sive hyperparameter searching in model development for specific
tabular datasets, and there is currently no universally recognized
method for selecting a model and determining a set of hyperpa-
rameters without comprehensive testing on the target datasets. In
this paper, we endeavor to design a DNN framework that serves
as a sure bet solution for diverse tabular datasets, by solving key
drawbacks of existing DNNs. Inspired by [ 13], we summarize three
key drawbacks of current deep tabular models, including:
(P1) lack of rotational variance property. As each table col-
umn holds distinct semantics and tabular data lack rotational in-
variance [ 26], rotational variant algorithms like decision trees are
more efficient on tabular datasets. However, DNNs are a kind of
rotationally invariant algorithms that has a worst-case sample com-
plexity growing at least linearly in the number of uninformative
features. As mentioned above, tables are created by casual users
who frequently include uninformative features, underscoring the
importance of rotational variance property.
(P2) large data demand. DNNs typically possess larger hy-
pothesis spaces, necessitating more training data to obtain robust
 
288
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jintai Chen et al.
Figure 1: Performance variation with different percentages of training samples on three large-scale tabular datasets (total
training sample count in parentheses). DNNs often exhibit better performance with larger training sample sizes, whereas
GBDTs perform better when data is scarce.
performances. Thus, it is widely noted that DNNs frequently ex-
hibit competitive, and at times, superior performance compared to
GBDTs on large-scale tabular datasets. Yet, their performances tend
to be subpar on smaller datasets.
(P3) over-smooth solution. Observations suggest that DNNs
tend to produce overly smooth solutions, a factor pinpointed by [ 13]
as a contributor to suboptimal performance on datasets featured by
irregular decision boundaries (as illustrated in Fig. 3). In contrast,
decision trees partition the feature space based on thresholds along
each axis, resulting in sharp boundaries that are demonstrated to
be more suitable for a majority of datasets.
Among these, the large data demand (P2) should be the primary
obstacle to current deep tabular models: Intuitively, rotationally-
invariant DNNs are data-inefficient [ 13], but this drawback can be
mitigated if there are sufficient training data [ 26]. Moreover, modern
DNNs have been demonstrated to be able to fit any functions [ 10].
Thus, if a plethora of discrete data points adequately fulfill the
feature space to accurately represent the underlying feature-target
functions, DNNs can definitely fit such functions rather than ob-
tain overly smooth solutions. We empirically observed that even
though DNNs outperform GBDTs on large tabular datasets, their
performance significantly declines and falls short of GBDTs when
fewer training samples are used. Three examples are presented in
Fig. 1. Thus, boosting the effectiveness of DNNs on small datasets
is the key to achieve a sure bet model.
In this paper, we delve into addressing the limitations of existing
DNNs and present a robust model, ExcelFormer, for diverse tabular
data prediction tasks. To address (P1), we design a novel attention
module named the semi-permeable attention (SPA), which selectively
permits more informative features to gather information from less
informative ones. This results in a noticeably reduced influence of
less informative features. A special interaction-attenuation initial-
ization approach is devised to boost this module. This initialization
approach sets SPAâ€™s parameters with minimal values, effectively at-
tenuating tabular data feature interactions during the initial stages
of training. Consequently, SPA undertakes more cautious feature
interactions at the beginning, learning key interactions and aiding
ExcelFormer to break the rotational invariance.
To address (P2), we introduce two interpolation-based data
augmentation approaches for tabular data: Feat-Mix andHid-
Mix. Interpolation-based data augmentation approaches, such as
Mixup [ 51] and its variants [ 37], have demonstrated their effec-
tiveness in computer vision tasks. However, as the feature-targetfunctions are often irregular [ 13], simple interpolation methods,
like Mixup, tend to regularize DNNs to behave linearly in-between
training examples [ 51], which potentially conflicts with the irreg-
ular functions. Therefore, they often fall short of improving and
even degrading the performance of DNNs. We propose two data
augmentation approaches, Feat-Mix andHid-Mix, to avoid such
conflicts and respectively encourage DNNs to learn independent
feature transformations and to conduct sparse feature interactions.
To address (P3), we employ an attentive feedforward network
to replace the vanilla multilayer perceptron-based feedforward
network in the Transformer. Both consisting of two fully connected
layers, this substitution integrates an attentive module to enhance
the modelâ€™s ability to effectively fit irregular boundaries. Following
previous work, we utilize Gated Linear Units (GLU) [ 31] and do not
introduce any new modules. Experimental results confirm that this
simple substitution effectively improves model performances.
Notably, replacing the vanilla self-attention and feedforward net-
work with the SPA and the attentive feedforward network does not
incur any additional computational burden, and the data augmenta-
tion approaches come at negligible cost. That means, the proposed
ExcelFormer maintains a comparable size to that of cutting-edge
tabular Transformers (e.g., FT-Transformer [ 29]). Comprehensive
and stratified experiments have demonstrated the superiority of
our designed ExcelFormer:
(1)Our model outperforms existing GBDTs and DNNs not only
on small tabular datasets where existing DNNs typically
underperform GBDTs, but also on large-scale datasets where
traditional DNNs have shown preference (Sec. 6.2).
(2)Across a spectrum of stratified experiments in terms of fea-
ture quantity, dataset scale, and task types, our ExcelFormer
consistently outperforms both GBDTs and DNNs. Addition-
ally, we observe that besides our ExcelFormer, existing ap-
proaches excel in handling different types of datasets. This
underscores its status as a reliable solution for non-expert
users, mitigating the need for intricate model selection (Sec. 6.3
and Sec. 6.4).
(3)Notably, while most existing approaches necessitate inten-
sive hyperparameter tuning through repeated model runs
(typically 50-100 times), our model achieves superior perfor-
mance with pre-fixed parameters, offering a time-efficient
and user-friendly solution (see Appendix).
 
289Can a Deep Learning Model be a Sure Bet for Tabular Prediction? KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ğ’™ğŸğ’™ğŸğ’™ğŸ‘ğ’™ğŸ’0.31.54Yes0.43Importance(ğ¼)Input tableÃ—Lsemi-permeableAttentionLinearLinearLinearQKVğ’™âˆˆâ„ğ’‡ğ’›âˆˆâ„ğ’‡Ã—ğ’…
ğ’›â€²âˆˆâ„ğ’‡Ã—ğ’…GLULinearLinearLayerNorm
LayerNormElement-wise productElement-wise addTanhEmbedding Layer
Prediction Head1324MaskedMatrix productğ’‘
Figure 2: An illustration of our proposed ExcelFormer model. Diverse feature types are pre-processed procedure as in [ 29],
followed by quantile transformation and embedding layer to convert them into numerical embeddings. Each feature embedding
ğ‘§ğ‘–âˆˆRğ‘‘serves as a token in ExcelFormer.
2 RELATED WORK
2.1 Supervised Tabular Data Prediction
While deep neural networks (DNNs) have proven to be effective in
computer vision [ 21] and natural language processing [ 38], GBDT
approaches like XGBoost continue to be the preferred choice for
tabular data prediction tasks [ 13,20], particularly on smaller-scale
datasets, due to their consistently superior performance. To enhance
the performance of DNNs, recent studies have focused on develop-
ing sophisticated neural modules for (i) handling heterogeneous
feature interactions [ 5,6,12,47], (ii) seeking for decision paths by
emulating decision-tree-like approaches [ 2,20,27], or (iii) resorting
to conventional approaches [ 8,15] and regularizations [ 18]. In ad-
dition to model designs, various feature representation approaches,
such as feature embedding [ 5,11], discretization of continuous fea-
tures [ 14,45], and Boolean algebra based methods [ 44], were well ex-
plored. All these efforts suggested the potentials of DNNs, but they
have not yet surpassed GBDTs in performance, especially on small-
scale datasets. Moreover, there were several attempts [ 2,43,49,52]
to apply self-supervision learning on tabular datasets. However,
many of these approaches are dataset- or domain-specific, and
transferring these models to distant domains remains challenging
due to the heterogeneity across tabular datasets. While pretrained
on a substantial dataset corpus, XTab [ 52] offered only a modest
performance improvement due to the limited shared knowledge
across datasets. TapPFN [ 17] concentrated on solving classification
problems for small-scale tabular datasets and achieved commend-
able results. However, its efficiency waned when applied to larger
datasets and regression tasks. In summary, compared to decision
tree-based GBDTs, DNNs still fall short on tabular data, especially
on small-scale ones, which remains an open challenge.
2.2 Mixup and other Data Augmentations
The vanilla Mixup [ 51] generates a new data through convex in-
terpolations of two existing data, which was proved beneficial on
computer vision tasks [ 34,35]. However, we have observed that
vanilla Mixup may conflict with irregular target patterns (please
refer to Fig. 3) and typically achieves inferior performance. For in-
stance, in the context of predicting therapy feasibility, a 70-year-old
man (elderly individual) and a 10-year-old boy (young individual)may not meet the criteria for a particular therapy, but an individual
with an interpolated feature value (aged 40) would benefit from it.
Namely, the vanilla Mixup can lead to over smooth solution, which
is considered to be unsuitable [ 13]. ManifoldMix [ 39] applied simi-
lar interpolations in the hidden states, which did not fundamentally
alter the data synthesis approach of Mixup and exhibited similar
characteristics to the vanilla Mixup. The follow-up variants Cut-
Mix [ 50], AttentiveMix [ 40], SaliencyMix [ 37], and PuzzleMix [ 23]
spliced image pieces spatially, preserving local image patterns but
being not directly applicable to tabular data. CutMix is used in
SAINT [ 32] for tabular data prediction, but it is highly impacted
by uninformative features, as shown in Table ??. Kadra et al . [19]
investigated various data augmentation techniques aimed at en-
hancing the performance of MLPs on tabular data. However, these
methods were found to be effective only on a limited number of tab-
ular datasets, requiring time-consuming enumeration and testing
of these options. In contrast, this paper introduced two novel data
augmentation approaches for tabular data, Hid-Mix andFeat-Mix,
which avoid the conflicts encountered with Mixup and contribute
to ExcelFormer achieving superior performance.
3 EXCELFORMER
The workflow of ExcelFormer is illustrated in Fig. 2. ExcelFormer
processes data by the following components: 1) After pre-processing
like Gorishniy et al . [12] , the embedding layer featurizes and em-
beds tabular features to token-level embeddings; 2) token-level
embeddings were alternately processed by the newly proposed semi-
permeable attention module (SPA) and gated linear units (GLUs).
3) Finally, a prediction head yields the final target. In the follow-
ing, we will introduce the novel semi-permeable attention with the
interaction attenuated initialization and the GLU based attentive
feedforward network first and then the rest parts of ExcelFormer.
3.1 Solving (P1) with Semi-Permeable Attention
As stated in [ 26], less informative features make minor contribu-
tions on target prediction but still necessitate at least a linear in-
crease in the requirement for training samples to learn how to
â€œignoreâ€ them. DNNs are rotationally invariant algorithms, which
 
290KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jintai Chen et al.
:sampleğ‘¥!:sampleğ‘¥"
Figure 3:ğ‘˜NN (ğ‘˜=8) decision boundaries with 2 key features
of a zoomed-in part of the Higgs dataset. Convex combina-
tions by vanilla Mixup (points on the black line) of 2 samples
ğ‘¥1andğ‘¥2may conflict with irregular category boundaries.
are data-inefficient with a worst-case sample complexity increasing
at least linearly with the number of uninformative features [13].
Our idea is to incorporate an inductive bias into the self-attention
mechanism, which selectively restricts the impacts of a feature to
only those that are less informative, thereby reducing the over-
all impact of uninformative features on prediction outcomes. We
propose a semi-permeable attention module (SPA), as:
ğ‘§â€²=softmax((ğ‘§ğ‘Šğ‘)(ğ‘§ğ‘Šğ‘˜)ğ‘‡âŠ•ğ‘€
âˆš
ğ‘‘)(ğ‘§ğ‘Šğ‘£), (1)
whereğ‘§âˆˆRğ‘“Ã—ğ‘‘is the input embeddings and ğ‘§â€²the output em-
beddings,ğ‘Šğ‘,ğ‘Šğ‘˜,ğ‘Šğ‘£âˆˆRğ‘‘Ã—ğ‘‘are all learnable matrices, and âŠ•is
element-wise addition. ğ‘€âˆˆRğ‘“Ã—ğ‘“is an unoptimizable mask, where
the element at the ğ‘–-th row and ğ‘—-th column is defined by:
ğ‘€[ğ‘–,ğ‘—]=(
âˆ’âˆğ¼(fğ‘–)>ğ¼(fğ‘—)
0ğ¼(fğ‘–)â‰¤ğ¼(fğ‘—)(2)
The function ğ¼(Â·)represents a measure of feature importance, and
we use the â€œmutual informationâ€ metric in this paper. If a feature fğ‘–
is more informative compared to fğ‘—,ğ‘€[ğ‘–,ğ‘—]is setâˆ’âˆ(we useâˆ’105
in implementation) and thus the (ğ‘–,ğ‘—)grid on the attention map is
masked. It prevents the transfer of the embedding of the feature fğ‘—
to the fğ‘–â€™s embedding.
In this way, only more informative features are permitted to prop-
agate information to the less informative ones, and the reverse is
not allowed. By doing so, SPA still maintains interaction pathways
between any two features while constraining the impacts of less in-
formative ones. Intuitively, when training samples are insufficient,
some feature interactions conducted by the model may be sub-
optimal, as vanilla self-attention was proved data-inefficient [ 35].
When using SPA, it can avoid the excessive impacts of a noisy
feature on prediction outcomes in case some associated interac-
tion pathways are ill-suited. Furthermore, the SPA inhibits certain
feature transfer pathways, thereby obviating the need for the Ex-
celFormer to learn partial rotational directions. The rotational prop-
erties of the ExcelFormer lie between those of the rotational invari-
ant counterpart with vanilla self-attention (e.g., FT-Transformer)
and a fully rotational variant model (e.g., feedforward network con-
ducting no feature interactions). Namely, our SPA partially disrupts
DNNsâ€™ rotational invariance property. In practice, SPA is extended
to a multi-head self-attention version, with 32 heads by default.
synthesizedğ‘¥!sampleğ‘¥!sampleğ‘¥"(b)AnexampleofFEAT-MIX(oninputdata),ğœ†"=#$featuresfeaturesemb.sampleğ‘§!synthesizedğ‘§!sampleğ‘§"(a)AnexampleofHID-MIX(onfeatureembedding),ğœ†%=#$ğ‘¦&ğ‘¦#ğ‘¦!=#$ğ‘¦&+&$ğ‘¦#ğ‘¦&ğ‘¦#ğ‘¦!=!.#$!.#$!.#$!.%&ğ‘¦#+!.'$!.#&ğ‘¦'FeatureImportance(ğ¼):0.10.10.10.10.40.2Figure 4: Examples for the Hid-Mix andFeat-Mix , where
â€œemb.â€ means â€œembedingâ€ dimension.
Interaction Attenuated Initialization. Similar to how the
SPA disrupts rotational invariance by diminishing feature interac-
tions, we present a specialized initialization approach for SPA to
ensure that ExcelFormer starts as a largely non-rotationally invari-
ant model. Notably, removing all self-attention operations from
a Transformer model, features are processed individually, which
makes the Transformer model nearly non-rotationally invariant (if
we set aside the full connection layers that fuse features for target
prediction). Concurrently, prior researches have evidenced the in-
dispensable role of feature interactions (e.g., through self-attention)
in Transformer-based models on tabular data [ 12,47]. By integrat-
ing these insights, our proposed interaction attenuated initialization
scheme initially dampens the impact of SPA during the early stages
of training, allowing essential feature interactions progressively
grow under the driving force of the data.
Our interaction attenuated initialization scheme is built upon the
commonly used Heâ€™s initialization [ 16] orXavier initialization [ 9],
by rescaling the variance of an initialized weight ğ‘¤withğ›¾(ğ›¾â†’0+)
while keeping the expectation at 0:
Var(ğ‘¤)=ğ›¾Varprev(ğ‘¤), (3)
where Varprev(ğ‘¤)denotes the weight variance used in the Heâ€™s
initialization and Xavier initialization. In this work, we set ğ›¾=10âˆ’4.
To reduce the impacts of SPA, we apply Eq. (3) to all the parameters
in the SPA module. Thus, ExcelFormer works like a non-rotationally
invariant model initially.
Actually, for a module with an additive identity shortcut like
ğ‘¦=F(ğ‘¥)+ğ‘¥, our initialization approach attenuates the sub-network
F(ğ‘¥)and satisfies the property of dynamical isometry [30] for better
trainability. Some previous work [ 3,36] suggested to rescale the
F(ğ‘¥)path asğ‘¦=ğœ‚F(ğ‘¥)+ğ‘¥, whereğœ‚is a learnable scalar initialized
as 0 or a learnable diagonal matrix whose elements are of very small
values. Different from these methods, our attenuated initialization
approach directly assigns minuscule values to the weights during
initialization. Our approach is better suited for the flexible learning
of whether each feature interaction pathway should be activated
or not, thereby achieving sparse attention.
3.2 Solving (P3) with GLU layer
The irregularities (Fig. 3 shows an example) present in the tabular
feature-target relationship make it particularly advantageous for
decision trees utilizing multiple thresholds to split the feature space.
On the contrary, existing Transformer employs a two-layer MLP as
 
291Can a Deep Learning Model be a Sure Bet for Tabular Prediction? KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
its feedforward network (FFN), which possesses a lesser degree of
non-linear fitting capability. Therefore, we replace the vanilla FFN
by a Gated Linear Unit (GLU) layer. Diverging from the standard
GLU architecture, we employ the â€œtanhâ€ activation in lieu of the
â€œsigmoidâ€ activation for better optimization properties [24], as:
ğ‘§â€²=tanh(Linear 1(ğ‘§))âŠ™ Linear 2(ğ‘§), (4)
where Linear 1andLinear 2are applied onto the embedding di-
mensionğ‘‘ofğ‘§,âŠ™denotes element-wise product. Please note that
both the vanilla FFN and GLU employ two fully connection layers
(FFN is defined by ğ‘§â€²=Linear 1(ReLU(Linear 2(ğ‘§)))), resulting in
similar computational costs. The SPA and GLU modules are alter-
nately stacked to form the core structure of the ExcelFormer model,
as shown in Fig. 2.
Existing tabular Transformers [ 12,47] use linear embedding
layer to independently deal with each feature fğ‘–âˆˆRinto an embed-
ding zğ‘–âˆˆRğ‘‘, byğ‘§ğ‘–=fğ‘–ğ‘Šğ‘–,1+ğ‘ğ‘–,1. Here we also use GLU to replace it
byzğ‘–=tanh(fğ‘–ğ‘Šğ‘–,1+ğ‘ğ‘–,1)âŠ™(fğ‘–ğ‘Šğ‘–,2+ğ‘ğ‘–,2), whereğ‘Šğ‘–,1,ğ‘Šğ‘–,2âˆˆR1Ã—ğ‘‘
andğ‘ğ‘–,1,ğ‘ğ‘–,2âˆˆRğ‘‘are learnable parameters. Then, the initial feature
embedding ğ‘§(0)are obtained by stacking all zğ‘–(ğ‘–=1,2,...,ğ‘“), as
ğ‘§(0)=[z1,z2,z3,..., zğ‘“]ğ‘‡âˆˆRğ‘“Ã—ğ‘‘like previous work.
3.3 The rest part of ExcelFormer
Feature Pre-processing. Feature values are pre-processed be-
fore feeding into ExcelFormer. The numerical features are normal-
ized by quantile transformation and the categorical features are
converted into numerical ones using the CatBoost Encoder imple-
mented with the Sklearn Python package1. This step performs
similar to previous works (e.g., FT-Transformer [12]).
Prediction Head. The prediction head is directly applied to the
output of the topmost transformer block, which contains two fully
connection layers to separately compress the information along
the token embeddings and fuse the information from features, by:
ğ‘=ğœ™(Linearğ‘‘(P-ReLU(Linearğ‘“(ğ‘§(ğ¿))))), (5)
whereğ‘§(ğ¿)is the input, ğ‘Šğ‘“âˆˆRğ‘“Ã—ğ¶andğ‘ğ‘“âˆˆRğ¶. For multi-
classification task, ğ¶is the amount of target categories and ğœ™in-
dicates â€œsoftmaxâ€. For regression and binary classification tasks,
thenğ¶=1andğœ™issigmoid. The fully connection layer Linearğ‘“
andLinearğ‘‘are applied along and the feature dimension and the
embedding dimension of ğ‘§(ğ¿), respectively.
4SOLVING (P2) WITH DATA AUGMENTATION
A straightforward approach to tackle data insufficiency is to create
training data. While Mixup [ 51] regularizes DNNs to favor linear
behaviors between samples and stands as one of the most effective
data augmentation methods in computer vision, empirical evidence
suggests that it does not perform optimally on tabular datasets (e.g.,
see Table 5). This discrepancy may be due to the conflict between
the modelâ€™s linear behavior and the irregularity of target functions,
as intuitively illustrated in Fig. 3. To address this challenge, we intro-
duce two Mixup variants, Hid-Mix andFeat-Mix, which mitigate
the conflicts in creating samples.
1https://contrib.scikit-learn.org/category_encoders/catboost.htmlHid-Mix. OurHid-Mix is applied to the token-level embeddings
after the input samples have been processed by the embedding
layer, along with their corresponding labels. It randomly exchanges
some embedding â€œdimensionsâ€ between two samples (please refer
to Fig. 4(a)). Let ğ‘§(0)
1,ğ‘§(0)
2âˆˆRğ‘“Ã—ğ‘‘be the token-level embeddings
of two randomly selected samples, with ğ‘¦1andğ‘¦2denoting their
respective labels. A new sample represented as a token-label pair
(ğ‘§(0)
m,ğ‘¦m)is synthesized by:
(
ğ‘§(0)
m=ğ‘†ğ»âŠ™ğ‘§(0)
1+( 1ğ»âˆ’ğ‘†ğ»)âŠ™ğ‘§(0)
2,
ğ‘¦m=ğœ†ğ»ğ‘¦1+(1âˆ’ğœ†ğ»)ğ‘¦2,(6)
where the matrix ğ‘†ğ»is of sizeğ‘“Ã—ğ‘‘and is formed by stacking ğ‘“iden-
ticalğ‘‘-dimensional binary vectors denoted as ğ‘ â„:ğ‘†ğ»=[ğ‘ â„,ğ‘ â„,...,ğ‘ â„]ğ‘‡.
ğ‘ â„consists ofâŒŠğœ†ğ»Ã—ğ‘‘âŒ‹randomly selected elements set to 1 and
the rest elements set to 0. The scalar coefficient ğœ†ğ»for labels is
sampled from theBğ‘’ğ‘¡ğ‘(ğ›¼ğ»,ğ›¼ğ»)distribution, where ğ›¼ğ»is a hyper-
parameter. 1ğ»is an all-one matrix with dimensions ğ‘“Ã—ğ‘‘. In practice,
ğœ†ğ»is first sampled from given Bğ‘’ğ‘¡ğ‘(ğ›¼ğ»,ğ›¼ğ»)distribution. Subse-
quently, we randomly select âŒŠğœ†ğ»Ã—ğ‘‘âŒ‹elements to construct the
vectorğ‘ â„and the matrix ğ‘†ğ».
Since the embedding â€œdimensionsâ€ from different samples may
be randomly combined in training, ExcelFormer is encouraged to
independently and equally handle various embedding dimensions.
Considering each embedding dimension as a distinct â€œprofileâ€ ver-
sion of input data (as each embedding element is projected from a
scalar feature value), Hid-Mix regularizes ExcelFormer to behave
like a bagging predictor [ 4]. Therefore, Hid-Mix may also help
mitigate the effects of data noise and perturbations, in addition to
increasing the amount of training data.
Feat-Mix. Our idea of Feat-Mix is visualized as in Fig. 4. Unlike
Hid-Mix that operates on the embedding dimension, our Feat-Mix
synthesizes new sample (ğ‘¥ğ‘š,ğ‘¦ğ‘š)by swapping parts of features
between two randomly selected samples ğ‘¥1,ğ‘¥2âˆˆRğ‘“, and blending
their labelsğ‘¦1andğ‘¦2guided by feature importance, by:
ğ‘¥m=sğ¹âŠ™ğ‘¥1+( 1ğ¹âˆ’sğ¹)âŠ™ğ‘¥2,
ğ‘¦m=Î›ğ‘¦1+(1âˆ’Î›)ğ‘¦2,(7)
where the vector sğ¹and the all-one vector 1ğ¹are of sizeğ‘“,sğ¹con-
tainsâŒŠğœ†ğ¹Ã—ğ‘“âŒ‹randomly chosen elements set to 1 and the remaining
elements set to 0. ğœ†ğ¹âˆ¼Bğ‘’ğ‘¡ğ‘(ğ›¼ğ¹,ğ›¼ğ¹). The coefficient value, Î›, is
determined based on the contribution of ğ‘¥1andğ‘¥2, taking into
account feature importance, by:
Î›=Ã
s(ğ‘–)
ğ¹=1ğ¼(fğ‘–)
Ãğ‘“
ğ‘–=1ğ¼(fğ‘–), (8)
where s(ğ‘–)
ğ¹represents the ğ‘–-th element of sğ¹, andğ¼(Â·)returns the
feature importance using mutual information. When disregarding
feature importance, Î›=ğœ†ğ¹(assumingâŒŠğœ†ğ¹Ã—ğ‘“âŒ‹=ğœ†ğ¹Ã—ğ‘“), making
Feat-Mix degenerate into a form similar to cutmix [ 50]. However,
due to the presence of uninformative features in tabular datasets,
Feat-Mix emerges as a more robust scheme.
As features from two distinct samples are randomly combined
to create new samples, Feat-Mix promotes a solution with fewer
feature interaction. This aligns with the functionality similar to
our Interaction Attenuated Initialization (see Sec. 3.1). We argue
 
292KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jintai Chen et al.
thatFeat-Mix not only supplements the training dataset as a data
augmentation method, but also encourages ExcelFormer to predom-
inantly exhibit like a non-rotationally invariant algorithm.
5 TRAINING AND LOSS FUNCTIONS
ExcelFormer can handle both classification and regression tasks
on tabular datasets in supervised learning. In training, our two
proposed data augmentation schemes can be applied successively
byHid-Mix(Embedding Layer(Feat-Mix(ğ‘¥,ğ‘¦)))or used indepen-
dently. But, our tests suggest that the effect of ExcelFormer on a
certain dataset could be better by using only Feat-Mix orHid-
Mix. Thus, we use only one scheme in dealing with certain tabular
datasets. The cross-entropy loss is used for classification tasks, and
the mean square error loss is for regression tasks.
6 EXPERIMENTS
6.1 Experimental Setups
Implementation Details. We configure the number of SPA and
GRU modules as ğ¿=3, set the feature embedding size to ğ‘‘=256,
and apply a dropout rate of 0.3 to the attention map. AdamW opti-
mizer [25] is used with default settings. The learning rate is set to
10âˆ’4without weight decay, and ğ›¼ğ»andğ›¼ğ¹forBğ‘’ğ‘¡ğ‘distributions
are both set to 0.5. These settings are the default hyperparameters
for our ExcelFormer. In the hyperparameter fine-tuning process,
we utilized the Optuna library [ 1] for all approaches. Consistent
with [ 12], we randomly select 80% of the data as training samples
and the remaining 20% as test samples. During training, we reserve
20% training samples for validation. To fine-tune our ExcelFormer,
we designate two tuning configurations: â€œMix Tunedâ€ and â€œFully
Tunedâ€. â€œMix Tunedâ€ refers to only fine-tune hyperparameters
of data augmentation (for Feat-Mix andHid-Mix), while â€œFully
Tunedâ€ optimizes all hyperparameters, including those related to
data augmentation and model architecture. A comprehensive de-
scription of all settings can be found in Appendix. We applied early
stopping with a patience of 32 for ExcelFormer.
Datasets. A total of 96 small datasets sourced from the Taptap
dataset benchmark2were utilized. The criterion for classifying
datasets as small is based on having a sample size of less than
10,000. Besides, 21 larger public tabular datasets, ranging in scale
from over 10,000 to 581,835 samples were also used. The detailed
dataset descriptions are provided in Appendix.
Compared Models. We compare our new ExcelFormer with two
prominent GBDT approaches XGboost [ 7] and Catboost [ 28] and
several representative DNNs: FT-Transformer (FTT) [ 12], SAINT [ 32],
Multilayer Perceptron (MLP), DCN v2 [ 42], AutoInt [ 33], and TapPFN
[17]. We also include two pre-trained DNNs: TransTab [ 43] and
XTab [ 52] for reference. The implementations of XGboost and Cat-
boost mainly follow [ 12]. Since we aim to extensively tune XGboost
and Catboost for their best performances, we increase the maximum
number of estimators/ iterations (i.e., the number of decision trees)
from 2000 to 4096 and the number of tuning iterations from 100 to
500, which give a more stringent setting and better performances.
The settings for XGboost and Catboost are given in Appendix. We
use the default hyperparameters of pretrained models, TransTab
2https://huggingface.co/datasets/ztphs980/taptap_datasetsand XTab, and fine-tune them on each dataset. They are not hy-
perparameter tuned, since their hyperparameter tuning spaces are
not given. For large-scale datasets, FT-Transformer, SAINT, and
TapFPN were fine-tuned based on the hyperparameters outlined
in their respective papers. The architectures and hyperparameter
tuning settings of the remaining DNNs follows the paper [ 12]. On
small datasets, we tuned 50 iterations for each datasets.
Evaluation metrics. We use the area under the ROC Curve
(AUC) and accuracy (ACC) for binary classification tasks and multi-
class classification tasks. In regression tasks, we employ the neg-
ative root mean square error (nRMSE), where the negative sign
is introduced to RMSE, aligning its direction with AUC and ACC,
such that higher values across all these metrics indicate superior
performance. Due to the high diversity among tabular datasets,
performance ranks are used as a comprehensive metric, and the
detailed results are given in Appendix.
6.2 Model Performances and Discussions
Performances on Small Datasets. DNNs are typically data-
inefficient, thus we initially investigate whether the proposed Ex-
celFormer can effectively perform on small datasets. See Table 1,
our ExcelFormer consistently outperforms other models that un-
dergo dataset-adaptive hyperparameter tuning, regardless of
whether the hyperparameters of the ExcelFormer are tuned or not,
which underscores the superiority of our proposed ExcelFormer. We
observe that ExcelFormer with Hid-Mix slightly outperforms that
with Feat-Mix; and if we tune hyperparameters of ExcelFormer, its
performance achieves further improvement. Notably, hyperparam-
eter fine-tuning reduces the standard deviations of performance
ranks, indicating that applying hyperparameter fine-tuning onto
ExcelFormer can yield more consistently superior results. Interest-
ingly, while fine-tuning all the hyperparameters (â€œFully Tunedâ€)
should result in better performance ideally, it shows that, under the
same fine-tuning iterations, â€œMix Tunedâ€ configuration performs
better. This might be attributed to the higher efficiency of finely
tuning data augmentation setting. To assess the effectiveness of our
ExcelFormerâ€™s architecture, we conducted experiments by exclud-
ing all data augmentation (Feat-Mix andHid-Mix) and compare it
with existing models. The results show that even without the use
ofFeat-Mix andHid-Mix, ExcelFormer still outperforms previous
approaches, underscoring the superiority of our architecture.
Performances on Larger Datasets. We further conduct a com-
parison between our model and three previous state-of-the-art
models: XGboost, Catboost, and FTT. We excluded other models
from the comparison due to their relatively inferior performances
and the significant computational load when dealing with large
datasets. Each model undergoes evaluation with two settings: using
default hyperparameters and dataset-adaptive fine-tuning hyper-
parameters. As depicted in Table 2, our model also outperforms
the previous models under both settings. Additionally, it is worth
noting that our ExcelFormer with Hid-Mix still achieves compa-
rable performance to prior models that undergo hyperparameter
tuning, consistent with the findings on small datasets. Different
from the conclusion on small datasets, the Fully Tuned ExcelFormer
outperforms the Mix Tuned version on large datasets.
 
293Can a Deep Learning Model be a Sure Bet for Tabular Prediction? KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Performance ranking ( â†“) across 96 small tabular datasets containing fewer than 10k samples. Each model underwent 5
independent trials, with the modelâ€™s average rank ( Â±std) reported. The best ranks are highlighted in bold while the runners-up
areunderlined . Our ExcelFormer consistently outperforms prior methods that undergo hyperparameter fine-tuning, regardless
of whether ExcelFormer uses fine-tuned or default hyperparameters. â€œdâ€: using default hyper-parameters; â€œtâ€: using tuned
hyperparameters; â€œNo DAâ€: neither Feat-Mix norHid-Mix is used.
ExcelFormer setting: No DA (t) Feat-Mix (d) Hid-Mix (d) Mix Tuned (t) Fully Tuned (t)
XGboost (t) 4.20Â±2.76 4.21Â±2.70 4.29Â±2.73 4.34Â±2.73 4.28Â±2.77
Catboost (t) 4.61Â±2.73 4.57Â±2.69 4.63Â±2.68 4.66Â±2.61 4.64Â±2.68
FTT (t) 4.32Â±2.36 4.35Â±2.35 4.41Â±2.25 4.44Â±2.32 4.39Â±2.37
MLP (t) 5.23Â±2.31 5.27Â±2.34 5.26Â±2.32 5.30Â±2.37 5.32Â±2.33
DCN v2 (t) 6.01Â±2.78 5.96Â±2.75 5.99Â±2.27 6.03Â±2.74 6.02Â±2.73
AutoInt (t) 5.70Â±2.61 5.78Â±2.51 5.77Â±2.56 5.88Â±2.53 5.80Â±2.55
SAINT (t) 5.48Â±2.59 5.48Â±2.55 5.56Â±2.56 5.61Â±2.55 5.56Â±2.58
TransTab (d) 6.78Â±2.52 6.80Â±2.59 6.82Â±2.57 6.86Â±2.59 6.87Â±2.55
XTab (d) 8.56Â±2.20 8.68Â±2.19 8.67Â±2.19 8.67Â±2.19 8.71Â±2.14
ExcelFormer (ours) 4.11Â±2.68 3.91Â±2.60 3.62Â±2.59 3.20Â±2.10 3.41Â±2.12
Table 2: Performance evaluation across 21 larger-scale
datasets, each containing more than 10,000 samples, is con-
ducted. Average ranks with standard deviations are reported
based on the results of 5 runs with different random seeds.
Excel defines ExcelFormer. The best and second best perfor-
mances are bold and underlined .
Setting Model Rank (meanÂ±std)
defaultXGboost 8.52Â±1.86
hyperparametersCatboost 7.52Â±2.44
FTT 6.71Â±1.74
Excel w/ Feat-Mix 6.62Â±2.44
Excel w/ Hid-Mix 4.76Â±1.95
hyperparameterXGboost 4.29Â±2.59
Catboost 6.24Â±2.39
fine-tunedFT-T 5.19Â±2.60
Excel (Mix Tuned) 2.38Â±1.53
Excel (Fully Tuned) 2.05Â±1.40
Takeaway. We discovered that (1)our model performs well
on GBDT-favored smaller datasets and DNN-favored larger ones.
This suggests that our design addresses the existing drawbacks of
DNNs in tabular prediction. (2)Even with default hyperparame-
ters, ExcelFormer consistently outperforms hyperparameter-tuned
competitors on small datasets and performs competitive with them
on larger ones. This implies that for users who are not experts in
hyperparameter tuning, using our model can still obtain a strong so-
lution. Moreover, even for professional users, our model also stands
out as a top choice since the hyperparameter-tuned ExcelFormer
performs excellent on various tabular prediction tasks.
6.3 Can ExcelFormer be a sure bet solution
across various types of datasets?
We still have to further rigorously examine whether our model per-
forms poorly on certain tabular dataset types to ensure that we have
achieved our goal of building a sure bet solution. We divide datasets
into various subgroups according to the task type, dataset size, and
the number of features, and examine ExcelFormer performancewithin each subgroup. We adopt two configurations, Hid-Mix (de-
fault) and Mix Tuned, for ExcelFormer, while all of the existing
models undergo hyperparameter fine-tuning. As shown in Ta-
ble 3, ExcelFormer with Hid-Mix (default) exhibits the best perfor-
mance in all subgroups except for regression tasks, where it slightly
lags behind hyperparameter-tuned XGboost. The Mix Tuned Ex-
celFormer outperforms other models in all subgroups, indicating
that ExcelFormer does not exhibit overt dataset type preferences.
Takeaway. What changes does ExcelFormer bring to the field of
tabular data prediction? Refer to Table 3, besides our model, runner-
up positions are held by TapFPN, FTT, XGBoost, and CatBoost in
different subgroups. Notably, TapFPN is solely applicable to classi-
fication tasks, CatBoost performs well on datasets with numerous
features, and FTT excels on datasets with fewer than 16 features.
However, our proposed model demonstrates strong performance
across all dataset types, which further proves its status as a sure be
solution for tabular datasets.
6.4 Ablation Analysis
Here we investigate the effects of architectural design (SPA, GLU)
and data augmentation approaches (Hid-Mix andFeat-Mix), with
the results presented in Table 4 and Table 5, respectively.
In Table 4, the baseline model employs an ExcelFormer with the
vanilla self-attention module initialized using typical Kaiming ini-
tialization [ 16], along with a vanilla MLP-based FFN. Subsequently,
we evaluate how the designed approaches enhance this baseline.
It is observed that SPA and IAI individually improve baseline per-
formances, and their joint usage achieves even better results. Addi-
tionally, GLU can also significantly enhances the baseline. These
findings suggest that our architectural designs, SPA with IAI and
GLU, are all well-suited for tabular data predictions. In the last row,
where all these components are utilized (ExcelFormer), we demon-
strate that their combined utilization leads to the best results. The
rotational invariance property brought by SPA and IAI are carefully
demonstrated in Appendix.
See Table 5, where we report the comparison results among var-
ious data augmentation techniques on both the FTT backbone [ 29]
 
294KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jintai Chen et al.
Table 3: Performance evaluation within several dataset subgroups. Performance rank ( â†“) within the datasets are reported. The
best scores are in bold and the runners-up are underlined . â€œ(d)â€: default hyperparameters; â€œ(t)â€: finely tuned hyperparameters.
Mo
del ExcelFormer FTT (t) XGb (t) Cat (t) MLP (t) DCNv2 (t) AutoInt (t) SAINT (t) TransTab (d) XTab (d) TabPFN (t)
Characteristics:
Task Type Classification
Pr
oportion 51%
Setting: Hid
-Mix (d) 3.88 4.88
5.97 5.77 6.61 6.38 6.63 6.07 6.31 9.50 4.01
Setting:
Mix Tuned 3.78 4.91
5.95 5.79 6.60 6.39 6.71 6.10 6.37 9.46 3.95
Characteristics:
Task Type Regr
ession
Pr
oportion 49%
Setting: Hid
-Mix (d) 3.81 4.45 3.43 4.26
4.64 6.26 5.53 5.64 8.21 8.79 /
Setting: Mix Tuned 3.17 4.49
3.53 4.28
4.74 6.32 5.68 5.72 8.23 8.83 /
Characteristics:
#. Sample â‰¥500
Pr
oportion 43%
Setting: Hid
-Mix (d) 3.85 4.50
4.38 5.17
5.57 5.91 5.59 5.24 6.44 8.34 /
Setting: Mix Tuned 3.52 4.50
4.39 5.15
5.60 5.99 5.71 5.33 6.48 8.34 /
Characteristics:
#. Sample <
500
Pr
oportion 57%
Setting: Hid
-Mix (d) 3.45 4.34
4.22 4.23
5.02 6.05 5.90 5.79 7.10 8.92 /
Setting: Mix Tuned 3.18 4.38
4.28 4.29
5.05 6.05 5.97 5.79 7.13 8.88 /
Characteristics:
#. Feature #.
Feature < 8
Pr
oportion 32%
Setting: Hid
-Mix (d) 3.45 3.84 3.98
5.08 4.23 6.32 6.16 5.32 7.52 9.10 /
Setting: Mix Tuned 3.27 3.84 4.03
5.06 4.34 6.26 6.21 5.35 7.50 9.13 /
Characteristics:
#. Feature 8â‰¤#.
Feature < 16
Pr
oportion 38%
Setting: Hid
-Mix (d) 3.76 4.26 4.44
4.61 6.31 5.75 5.39 5.69 6.61 8.17 /
Setting: Mix Tuned 3.17 4.33 4.49
4.74 6.33 5.81 5.58 5.78 6.64 8.14 /
Characteristics:
#. Feature #.
Featureâ‰¥16
Pr
oportion 30%
Setting: Hid
-Mix (d) 3.62 5.19
4.41 4.17 5.05
5.93 5.81 5.64 6.33 8.84 /
Setting: Mix Tuned 3.17 5.22
4.48 4.14 5.05
6.05 5.90 5.69 6.45 8.84 /
Table 4: Additive study for the semi-permeable attention (SPA)
andinteraction-attenuated initialization (IAI), and the GLU
based attentive module. No data augmentation.
baseline SPA IAI GLU rank (Â± std)
âœ“ 4.31Â±0.94
âœ“ âœ“ 3.87Â±1.58
âœ“ âœ“ 3.73Â±2.04
âœ“ âœ“ âœ“ 2.45Â±1.60
âœ“ âœ“ 3.71Â±1.52
âœ“ âœ“ âœ“ âœ“ 2.31Â±1.46
Table 5: Comparison among several data augmentation ap-
proaches on FT-Transformer and ExcelFormer. Ranks are
computed separately for different backbones.
Backbone Data Augmentation rank (Â± std)
FT-TransformerN/A 3.28Â±1.66
Mixup 3.80Â±1.39
CutMix 2.91Â±1.37
Feat-Mix 2.50Â±1.03
Hid-Mix 2.24Â±1.00
ExcelFormerN/A 3.68Â±1.43
Mixup 3.46Â±1.63
CutMix 2.88Â±1.21
Feat-Mix 2.38Â±1.25
Hid-Mix 2.36Â±1.03
and our ExcelFormer. It is crucial to recognize that the performance
rankings are computed independently for different backbones, mak-
ing direct comparisons of ranks unfeasible. It is evident that Mixupdemonstrates minimal to no effect and sometimes even exhibits a
detrimental impact. This could be attributed to Mixupâ€™s interpo-
lation potentially introducing â€œerrorâ€ cases or steering the model
towards an overly smooth solution. In contrast, CutMix consistently
outperforms Mixup, approaching the performance level of Feat-
Mix, albeit with slight inferiority. As discussed in Sec. 4, without
considering feature importance, Feat-Mix may regress to CutMix;
however, feature importance computation is crucial to mitigate the
impacts of uninformative features, a common occurrence in tabular
datasets. Further experiments are detailed in Appendix. It is evident
that our proposed Feat-Mix andHid-Mix consistently enhance
DNN model performance and prove more effective compared to
other Mixup variants.
7 CONCLUSIONS
This paper introduces a novel approach aimed at addressing three
key limitations of DNNs when applied to tabular data prediction.
We present a novel semi-permeable attention module incorporated
with an interaction attenuated initialization approach, a GLU based
FFN, as well as two data augmentation approaches: Hid-Mix and
Feat-Mix. Through the integration of these designs, we present
ExcelFormer, a model that maintains the same size as previous
tabular Transformers but significantly outperforms existing GB-
DTs and DNNs in terms of performance, without hyperparameter
tuning. Extensive and stratified experiments demonstrate that the
ExcelFormer stands out as a sure bet solution for tabular predic-
tion. We believe the proposed framework is highly accessible and
user-friendly for even novices working with tabular data.
ACKNOWLEDGEMENTS. This work was mainly supported by
NSF award SCH-2205289, SCH-2014438, and IIS-2034479.
 
295Can a Deep Learning Model be a Sure Bet for Tabular Prediction? KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
REFERENCES
[1]Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori
Koyama. 2019. Optuna: A next-generation hyperparameter optimization frame-
work. In The ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining.
[2]Sercan Ã– Arik and Tomas Pfister. 2021. TabNet: Attentive interpretable tabular
learning. In The AAAI Conference on Artificial Intelligence.
[3]Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell,
and Julian McAuley. 2021. Rezero is all you need: Fast convergence at large depth.
InUncertainty in Artificial Intelligence.
[4] Leo Breiman. 1996. Bagging predictors. Machine Learning (1996).
[5]Jintai Chen, Kuanlun Liao, Yanwen Fang, Danny Z Chen, and Jian Wu. 2023.
TabCaps: A Capsule Neural Network for Tabular Data Classification with BoW
Routing. In International Conference on Learning Representations.
[6]Jintai Chen, Kuanlun Liao, Yao Wan, Danny Z Chen, and Jian Wu. 2022. DANets:
Deep abstract networks for tabular data classification and regression. In The
AAAI Conference on Artificial Intelligence.
[7]Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting
system. In ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining.
[8]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, et al .2016. Wide & deep learn-
ing for recommender systems. In Workshop on Deep Learning for Recommender
Systems.
[9]Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of train-
ing deep feedforward neural networks. In International Conference on Artificial
Intelligence and Statistics.
[10] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT
Press.
[11] Yura Gorishniy, Ivan Rubachev, and Artem Babenko. 2022. On Embeddings for
Numerical Features in Tabular Deep Learning. In Advances in Neural Information
Processing Systems.
[12] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021. Re-
visiting deep learning models for tabular data. In Advances in Neural Information
Processing Systems.
[13] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. 2022. Why do tree-based
models still outperform deep learning on typical tabular data?. In Advances in
Neural Information Processing Systems.
[14] Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang
He. 2021. An embedding learning framework for numerical features in CTR
prediction. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
[15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: A factorization-machine based neural network for CTR prediction. In
International Joint Conference on Artificial Intelligence.
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep
into rectifiers: Surpassing human-level performance on ImageNet classification.
InInternational Conference on Computer Vision.
[17] Noah Hollmann, Samuel MÃ¼ller, Katharina Eggensperger, and Frank Hutter. 2022.
TabPFN: A Transformer That Solves Small Tabular Classification Problems in a
Second. In International Conference on Learning Representations.
[18] Alan Jeffares, Tennison Liu, Jonathan CrabbÃ©, Fergus Imrie, and Mihaela van der
Schaar. 2023. TANGOS: Regularizing tabular neural networks through gradient
orthogonalization and specialization. arXiv preprint arXiv:2303.05506 (2023).
[19] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021. Well
tuned simple nets excel on tabular datasets. Advances in Neural Information
Processing Systems (2021).
[20] Liran Katzir, Gal Elidan, and Ran El-Yaniv. 2020. Net-DNF: Effective deep modeling
of tabular data. In International Conference on Learning Representations.
[21] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fa-
had Shahbaz Khan, and Mubarak Shah. 2022. Transformers in vision: A survey.
Comput. Surveys (2022).
[22] Alisa Kim, Y Yang, Stefan Lessmann, Tiejun Ma, M-C Sung, and Johnnie EV
Johnson. 2020. Can deep learning predict risky retail investors? A case study
in financial risk behavior forecasting. European Journal of Operational Research
(2020).
[23] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. 2020. Puzzle Mix: Exploiting
saliency and local statistics for optimal mixup. In International Conference on
Machine Learning.
[24] Yann LeCun, LÃ©on Bottou, Genevieve B Orr, and Klaus-Robert MÃ¼ller. 2002.
Efficient backprop. Neural networks: Tricks of the Trade (2002).
[25] Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization.
InInternational Conference on Learning Representations.
[26] Andrew Y Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational
invariance. In International Conference on Machine Learning.
[27] Sergei Popov, Stanislav Morozov, and Artem Babenko. 2019. Neural Oblivious De-
cision Ensembles for Deep Learning on Tabular Data. In International Conference
on Learning Representations.[28] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Doro-
gush, and Andrey Gulin. 2018. CatBoost: Unbiased boosting with categorical
features. Advances in Neural Information Processing Systems (2018).
[29] Ivan Rubachev, Artem Alekberov, Yury Gorishniy, and Artem Babenko. 2022.
Revisiting pretraining objectives for tabular deep learning. arXiv preprint
arXiv:2207.03208 (2022).
[30] Andrew M Saxe, James L McClelland, and Surya Ganguli. 2014. Exact solu-
tions to the nonlinear dynamics of learning in deep linear neural networks. In
International Conference on Learning Representations.
[31] Noam Shazeer. 2020. GLU variants improve Transformer. arXiv preprint
arXiv:2002.05202 (2020).
[32] Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and
Tom Goldstein. 2021. SAINT: Improved neural networks for tabular data via row
attention and contrastive pre-training. arXiv preprint arXiv:2106.01342 (2021).
[33] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. AutoInt: Automatic feature interaction learning via self-
attentive neural networks. In ACM International Conference on Information and
Knowledge Management.
[34] Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey N Chiang, Zhihao Wu, and
Xiaowei Ding. 2020. Embracing imperfect datasets: A review of deep learning
solutions for medical image segmentation. Medical Image Analysis (2020).
[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
Sablayrolles, and HervÃ© JÃ©gou. 2021. Training data-efficient image Transformers
& distillation through attention. In International Conference on Machine Learning.
[36] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
HervÃ© JÃ©gou. 2021. Going deeper with image Transformers. In IEEE/CVF Interna-
tional Conference on Computer Vision.
[37] AFM Shahab Uddin, Mst Sirazam Monira, Wheemyung Shin, TaeChoong Chung,
and Sung-Ho Bae. 2020. SaliencyMix: A Saliency Guided Data Augmentation
Strategy for Better Regularization. In International Conference on Learning Repre-
sentations.
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in Neural Information Processing Systems (2017).
[39] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,
David Lopez-Paz, and Yoshua Bengio. 2019. Manifold Mixup: Better represen-
tations by interpolating hidden states. In International Conference on Machine
Learning.
[40] Devesh Walawalkar, Zhiqiang Shen, Zechun Liu, and Marios Savvides. 2020.
Attentive Cutmix: An Enhanced Data Augmentation Approach for Deep Learning
Based Image Classification. In International Conference on Acoustics, Speech and
Signal Processing.
[41] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In ADKDD.
[42] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. DCN v2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In The ACM Web Conference.
[43] Zifeng Wang and Jimeng Sun. 2022. TransTab: Learning Transferable Tabu-
lar Transformers Across Tables. In Advances in Neural Information Processing
Systems.
[44] Zhuo Wang, Wei Zhang, Ning Liu, and Jianyong Wang. 2021. Scalable rule-
based representation learning for interpretable classification. Advances in Neural
Information Processing Systems (2021).
[45] Zhuo Wang, Wei Zhang, LIU Ning, and Jianyong Wang. 2020. Transparent
classification with multilayer logical perceptrons and random binarization. In
The AAAI Conference on Artificial Intelligence.
[46] Kevin Wu, Eric Wu, Michael DAndrea, Nandini Chitale, Melody Lim, Marek
Dabrowski, Klaudia Kantor, Hanoor Rangi, Ruishan Liu, Marius Garmhausen,
et al.2022. Machine learning prediction of clinical trial operational efficiency.
The AAPS Journal (2022).
[47] Jiahuan Yan, Jintai Chen, Yixuan Wu, Danny Z Chen, and Jian Wu. 2023. T2G-
Former: Organizing Tabular Features into Relation Graphs Promotes Heteroge-
neous Feature Interaction. The AAAI Conference on Artificial Intelligence (2023).
[48] Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun,
Jian Wu, and Jintai Chen. 2024. Making Pre-trained Language Models Great on
Tabular Prediction. In ICLR.
[49] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. 2020. VIME:
Extending the success of self-and semi-supervised learning to tabular domain.
Advances in Neural Information Processing Systems (2020).
[50] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and
Youngjoon Yoo. 2019. CutMix: Regularization strategy to train strong classifiers
with localizable features. In International Conference on Computer Vision.
[51] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018.
Mixup: Beyond Empirical Risk Minimization. In International Conference On
Learning Representations.
[52] Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa
Shoaran. 2023. XTab: Cross-table Pretraining for Tabular Transformers. In ICML.
 
296