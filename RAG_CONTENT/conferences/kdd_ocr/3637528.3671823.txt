Urban-Focused Multi-Task Offline Reinforcement Learning with
Contrastive Data Sharing
Xinbo Zhao
Binghamton University
Binghamton, New York, USA
xzhao9@binghamton.eduYingxue Zhang
Binghamton University
Binghamton, New York, USA
yzhang42@binghamton.eduXin Zhang
San Diego State University
San Diego, California, USA
xzhang19@sdsu.edu
Yu Yang
Lehigh University
Bethlehem, Pennsylvania, USA
yuyang@lehigh.eduYiqun Xie
University of Maryland, College Park
College Park, Maryland, USA
xie@umd.eduYanhua Li
Worcester Polytechnic Institute
Worcester, Massachusetts, USA
yli15@wpi.edu
Jun Luo
Logistics and Supply Chain MultiTech
R&D Centre
Hong Kong, China
jluo@lscm.hk
ABSTRACT
Enhancing diverse human decision-making processes in an urban
environment is a critical issue across various applications, includ-
ing ride-sharing vehicle dispatching, public transportation man-
agement, and autonomous driving. Offline reinforcement learning
(RL) is a promising approach to learn and optimize human ur-
ban strategies (or policies) from pre-collected human-generated
spatial-temporal urban data. However, standard offline RL faces
two significant challenges: (1) data scarcity and data heterogeneity,
and (2) distributional shift. In this paper, we introduce MODA â€” a
Multi-Task Offline Reinforcement Learning with Contrastive Data
ShAring approach. MODA addresses the challenges of data scarcity
and heterogeneity in a multi-task urban setting through Contrastive
Data Sharing among tasks. This technique involves extracting latent
representations of human behaviors by contrasting positive and
negative data pairs. It then shares data presenting similar represen-
tations with the target task, facilitating data augmentation for each
task. Moreover, MODA develops a novel model-based multi-task
offline RL algorithm. This algorithm constructs a robust Markov
Decision Process (MDP) by integrating a dynamics model with a
Generative Adversarial Network (GAN). Once the robust MDP is
established, any online RL or planning algorithm can be applied.
Extensive experiments conducted in a real-world multi-task urban
setting validate the effectiveness of MODA. The results demon-
strate that MODA exhibits significant improvements compared to
state-of-the-art baselines, showcasing its capability in advancing
urban decision-making processes. We also made our code available
to the research community.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671823CCS CONCEPTS
â€¢Computing methodologies â†’Intelligent agents ;Neural
networks.
KEYWORDS
Data Sharing, Offline Reinforcement Learning, Contrastive Learn-
ing
ACM Reference Format:
Xinbo Zhao, Yingxue Zhang, Xin Zhang, Yu Yang, Yiqun Xie, Yanhua
Li, and Jun Luo. 2024. Urban-Focused Multi-Task Offline Reinforcement
Learning with Contrastive Data Sharing. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671823
1 INTRODUCTION
Problem and Goal. In urban scenarios, individuals, referred to
as human agents, engage in strategic planning to optimize their
daily activities. These strategies devised to achieve specific objec-
tives govern the human actions and decisions. For example, taxi
drivers strategize to maximize earnings and minimize travel time
by selecting optimal pickup locations, adjusting working hours,
and planning efficient routes. Many urban strategies, such as those
related to passenger seeking, public transit commuting and travel
route selection, are individually crafted based on personal prefer-
ences. Consequently, they may not represent the most effective or
efficient approaches and often require enhancement. In addition,
these urban strategies are generally implicit to observers or even
the human agents themselves, making their learning and improve-
ment a complex task. Therefore, in this paper, we aim to address
this essential problem: How can we learn and enhance diverse human
decisions in urban environments?
Prior works and limitations. Different human strategies and
decisions employed in various tasks lead to diverse human behav-
iors which are collected as human-generated spatial-temporal data
4512
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinbo Zhao et al.
Figure 1: Example of data scarcity and heterogeneity, and
distributional shift.
(HSTD). Examples of HSTD include GPS trajectories from taxis
and personal vehicles, route choices from bus and train passengers,
etc. Leveraging HSTD, data-driven techniques including imitation
learning and offline reinforcement learning (RL) stand out as reli-
able tools for understanding human decision-making processes. For
instance, imitation learning and its variants [ 15,30,47â€“49] aim at
understanding human decision-making strategies and mimicking
human behaviors across diverse urban scenarios using deep neural
networks (DNNs). However, these approaches primarily focus on
replicating human behaviors presented in the dataset, which is
bounded by the true behavior policies that produce the dataset. In
this sense, the learned policies can hardly outperform true behavior
policies, which renders them less suitable for enhancing human ur-
ban strategies with HSTD. In contrast, offline RL [ 25] leverages the
inherent optimization mechanisms of RL to not only learn but also
improve and optimize policies given HSTD. Many works have ex-
plored offline RL to solve urban problems. For example, some offline
RL approaches are designed for traffic signal control [ 1,5,23,26]
and autonomous driving [ 35,36]. However, these methods are gen-
erally limited to single-task scenarios within urban settings, as they
can only address an individual task using a specific dataset, lack-
ing the capability to learn from a composite dataset encompassing
multiple tasks. This limitation restricts their applicability in more
complex, multi-task urban environments. Some multi-task offline
RL approaches [ 8,44,45] learn diverse skills for multiple tasks with
various data-sharing techniques. However, these solutions are not
transferrable to the urban domain as they rely on strong presump-
tions about the availability of explicit reward functions, which are
inaccessible in most urban scenarios.
The present work. To address the above limitations and learn and
enhance diverse human strategies, we introduce MODA â€” a novel
Multi-task Offline RL with Contrastive Data sh Aring framework,
which addresses the following two challenges:
â€¢Data scarcity and data heterogeneity. Learning from observations
requires extensive amount of data from a task. However, in prac-
tice, it is difficult to collect a large amount of mobility data (i.e.,
HSTD) from each human agent. This leads to data scarcity. More
commonly, the collected data originates from a variety of hu-
man agents, each employing a distinct urban strategy generating
unique trajectories. This leads to data heterogeneity. Both data
scarcity and heterogeneity problems complicate the process of
learning and improving strategies for diverse human agents.
â€¢Distributional shift. Without online data collection during the
learning process, offline RL often encounters failures attributed to
substantial extrapolation errors when evaluating the ğ‘„-function
on out-of-distribution actions [ 25]. This issue worsens over thelearning process as the learned policies increasingly deviate from
the behavior policy. Moreover, the distributional shift problem is
exacerbated in multi-task urban scenarios when learning urban
strategies from diverse human agents, which leads to a huge
divergence of the learned policies and poor performance.
As shown in Fig. 2, MODA integrates a novel Contrastive Data
Sharing method and an innovative model-based multi-task offline
RL algorithm. MODA first addresses the data scarcity and hetero-
geneity issues through Contrastive Data Sharing across all tasks. It
then develops a novel model-based multi-task offline RL algorithm,
which constructs a robust Markov Decision Process (MDP) by com-
bining a dynamics model with a Generative Adversarial Network
(GAN) [ 12]. This design can effectively mitigate the distributional
shift challenge inherent in offline RL. The primary contributions of
this paper can be summarized as follows:
â€¢We make the first attempt to optimize diverse human strategies
by a novel multi-task offline RL framework named MODA. A key
component of MODA is an innovative Contrastive Data Sharing
strategy, which learns representations of human behaviors by
contrasting positive and negative data pairs. It then selectively
shares data from other tasks that display patterns akin to those
of the target task, ensuring a more effective sharing process to
tackle the data scarcity and data heterogeneity problems.
â€¢MODA also incorporates a novel model-based multi-task offline
RL algorithm which can successfully address the distributional
shift issue. It creates a robust MDP by learning a dynamics model
and a GAN model from HSTD. While the dynamics model gen-
erated transitions are not universally accurate â€” given that the
offline dataset after Contrastive Data Sharing still does not cover
the entire state space â€” the GANâ€™s generator has a natural ability
to generalize the offline dataset and learn the real MDPâ€™s data
distribution. This, in turn, improves the discriminatorâ€™s ability
to tell correct transitions from erroneous ones, making it more
capable and generalizable. Once the learned dynamics model is
integrated with the discriminator, which acts as a detector to
distinguish reliable and out-of-distribution transitions, the ac-
curacy of the dynamics model can be improved. Any online RL
or planning algorithm can be applied once the robust MDP is
constructed.
â€¢Extensive experiments are conducted under a real-world multi-
task urban setting with HSTD to validate the effectiveness of our
MODA framework. The results demonstrate that MODA exhibits
significant improvements compared to state-of-the-art baselines
when learning different human urban strategies. We also made
our code available to the research community1.
2 OVERVIEW
In this section, we formally define the urban-focused multi-task
offline RL problem. For ease of understanding and reference, all the
notations used are listed in Table. 1.
2.1 Preliminaries
The standard RL framework treats sequential human decisions as a
Markov Decision Process (MDP), often captured by HSTD in urban
areas. HSTD encompasses the movements of diverse human agents,
1https://github.com/anony11sdf/MODA
4513Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Figure 2: MODA overview.
not just individual ones. In this paper, we treat the urban strategies
of different agents as distinct tasks, and formalize the MDP as
a 5-tupleM=(S,A,ğ‘ƒ,ğ›¾,{ğ‘Ÿğ‘–}ğ‘
ğ‘–=1), whereSandAdenote the
spaces of states and actions, respectively, ğ‘ƒ(ğ‘ â€²|ğ‘ ,ğ‘)is the dynamics
function,ğ›¾âˆˆ [0,1)is the discount factor, and ğ‘Ÿğ‘–represents the
reward function for each task ğ‘–among a total of ğ‘tasks, detailed
further below.
Definition 1 (Grid cells). A city is divided into ğ¼Ã—ğ½grid cells,
with each cell having equal side-length in latitude and longitude.
We denote the collection of grid cells in a city as G={ğ‘”ğ‘– ğ‘—}, where
the indicesğ‘–andğ‘—satisfy 1â‰¤ğ‘–â‰¤ğ¼and1â‰¤ğ‘—â‰¤ğ½, respectively.
Definition 2 (A state ğ’”)represents a spatial-temporal location
which is defined as a tensor ğ’”âˆˆRğ‘›Ã—ğ‘™Ã—ğ‘™. The state ğ’”ğ‘¡containsğ‘›
different feature maps ğ‘‘âˆˆRğ‘™Ã—ğ‘™at timeğ‘¡. Since the human agents
usually consider the status of the surrounding area when making
urban decisions, each feature map characterizes a specific urban
feature (e.g., traffic speed, travel demand, etc.) of both the target
grid cell and its neighboring ğ‘™Ã—ğ‘™grid cells.
Definition 3 (An action ğ‘)is a decision made by a human agent
at state ğ’”. Following an action ğ‘, the human agent moves from a
state ğ’”to the next state ğ’”â€². We denote the set of actions as A={ğ‘}.
Definition 4 (A dynamics ğ‘ƒ)(or transition probability) is decided
by the environment and is defined as ğ‘ƒ:SÃ—AÃ—Sâ†¦â†’[ 0,1].
ğ‘ƒ(ğ‘ â€²|ğ‘ ;ğ‘)characterizes the probability of transiting to state ğ’”â€²at
state ğ’”by following action ğ‘.
Definition 5 (A reward function ğ‘Ÿğ‘–)is a mapping as ğ‘Ÿğ‘–:SÃ—Aâ†¦â†’
R, which provides a numerical score based on a state ğ’”and an action
ğ‘, and incentivizes a human agent ğ‘–to accomplish a task. Note that
we treat distinct agents as separate tasks, using â€œdifferent agentsâ€
and â€œdifferent tasksâ€ synonymously.
Definition 4 (A policy ğœ‹(ğ‘|ğ‘ ,ğ‘–))is a probability distribution
defined asğœ‹:SÃ—Aâ†¦â†’[ 0,1]indicating the probability of choosing
an actions given the state ğ’”. In this work, we follow Yu et al . [45]
and target on data sharing strategies to learn a conditional policy
ğœ‹(ğ‘|ğ’”,ğ‘–)which indicates a policy for a target task ğ‘–.
Definition 5 (A trajectory ğœ)is a sequence of states and actions
that a human agent traverses when completing a task in a geo-
graphic region, i.e.,ğœ=(ğ’”0,ğ‘0,Â·Â·Â·,ğ’”ğ‘‡,ğ‘ğ‘‡)of lengthğ‘‡. The set ofTable 1: Notations
Notations Descriptions
M=(S;A;ğ‘ƒ;ğ›¾;{ğ‘Ÿğ‘–,ğ‘–}ğ‘
ğ‘–=1)Multi-task Markov decision process.
ğ‘ƒ(ğ‘ â€²|ğ’”,ğ‘) Dynamics function.
ğ‘Ÿğ‘–(ğ’”,ğ‘) Reward function for task ğ‘–.
ğœ‹ğ‘–(ğ‘|ğ’”)orğœ‹(ğ‘|ğ’”,ğ‘–) Policy function for task ğ‘–.
ğœ‹ğ›½(ğ‘|ğ’”) Behavior policy.
Deff
ğ‘–Effective dataset for task ğ‘–.
ğ‘¥ğ‘– Sub-trajectory.
ğ‘“(ğ‘¥ğ‘–) Latent representations of ğ‘¥ğ‘–.
trajectories is denoted as T={ğœ}. When generating these trajecto-
ries, the policy each human agent ğ‘–employs is named the behavior
policyğœ‹ğ›½(ğ‘|ğ’”,ğ‘–).
2.2 Urban-focused Multi-Task Offline RL
Problem
This paper aims to develop a new multi-task offline RL framework
that can effectively optimize diverse human urban strategies while
minimizing distributional shift and deal with the data scarcity and
heterogeneity problems at the same time. The formal definition of
the urban-focused multi-task offline RL problem is as follows:
Problem Definition. Given a set of human mobility trajectories
Tcollected from ğ‘different human agents, assume the data from
each individual ğ‘–is generated by a specific behavior policy ğœ‹ğ›½(ğ‘|ğ’”,ğ‘–)
to implement a certain task ğ‘–, we aim to learn the urban decision-
making policy ğœ‹(ğ‘|ğ’”,ğ‘–)for every agent so that they can produce
higher expected cumulative rewards compared to the corresponding
behavior policy ğœ‹ğ›½(ğ‘|ğ’”,ğ‘–).
3 METHODOLOGIES
In this section, we solve the urban-focused multi-task offline RL
problem by introducing an innovative multi-task offline RL with
contrastive data sharing approach, in short, MODA, as shown in
Fig. 2. MODA has two components to address the above challenges:
(1)Contrastive Data Sharing. To tackle the challenge of data scarcity
and heterogeneity, we introduce a Contrastive Data Sharing mech-
anism. It enables data sharing across tasks by learning data repre-
sentations and strategically sharing data from other tasks with the
target task in a contrastive manner. The shared data is selected based
on its similarity to the data of the target task, ensuring that they
reflect comparable behaviors, preferences, and decision-making
logic (See Section 3.1).
(2)Model-based multi-task offline RL with Robust MDP. To tackle the
challenge of distributional shift, we propose a novel model-based
offline RL approach. We employ a GAN model [ 12] to enhance
the reliability of the learned dynamics model, where the generator
is trained to understand the data distribution of the actual envi-
ronment and produce reliable transitions, while the discriminator
distinguishes between reliable and out-of-distribution transitions.
A robust MDP is formed through the integration of the dynamics
model and the discriminator. Any online RL or planning algorithm
can be utilized once a robust MDP is established (See Section 3.2).
4514KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinbo Zhao et al.
3.1 Contrastive Data Sharing
Contrastive Data Sharing is an important component in our MODA
framework, which is specifically designed to address the data scarcity
and heterogeneity challenge and thus enable effective data sharing
among tasks. Standard multi-task offline RL usually requires large
amounts of data for each task to ensure decent performance. How-
ever, HSTD is generally sourced from a variety of human agents
or tasks, with each individual contributing a limited dataset. Thus,
when attempting to learn different urban decision-making pro-
cesses in a multi-task offline RL setting, we face data scarcity and
heterogeneity problems.
3.1.1 Limitations of state-of-the-art works. Previous multi-task of-
fline RL works [ 8,18,45] have indicated the benefits of sharing
data across multiple tasks to assist a target task ğ‘–. However, these
multi-task methods often presume direct access to the functional
form of the reward ğ‘Ÿğ‘–for the target task ğ‘–. In urban scenarios, re-
ward functions and human preferences tend to be implicit, not only
to external observers but also to the agents themselves, resulting in
inaccessible reward functions. This lack of accessibility makes the
prior multi-task offline RL techniques with data sharing ineffective
for urban applications.
3.1.2 Contrastive Data Sharing Objective. We introduce a novel
data sharing method that employs contrastive learning to effec-
tively augment the dataset for a target task by incorporating sim-
ilar data from other tasks. Consider a static multi-task dataset
D=Ãğ‘
ğ‘–=1Dğ‘–, whereğ‘represents the number of tasks, for a target
taskğ‘–, the data (typically a set of transitions {(ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)}) shared
from taskğ‘—to taskğ‘–is denoted asDğ‘—â†’ğ‘–. Consequently, the aug-
mented dataset for task ğ‘–, termed as the effective dataset, is defined
as:Deff
ğ‘–:=Dğ‘–ÃÃ
ğ‘—â‰ ğ‘–Dğ‘—â†’ğ‘–
. The challenge then lies in identify-
ing and sharing relevant data from other tasks to the target task ğ‘–
effectively. Merely sharing all transitions indiscriminately among
all tasks, a method we term Sharing All, has been shown to yield
suboptimal results in previous studies [ 44,45]. A more reasonable
approach would be first learning the meaningful latent representa-
tions of trajectories collected from all tasks and understanding the
their similarities. Subsequently, only those trajectories from other
tasks that exhibit behaviors similar to those of the target task ğ‘–are
incorporated into the effective dataset Deff
ğ‘–.
Thus, to discern the similarities among the latent representa-
tions of all trajectoriesÃğ‘
ğ‘–=1{(ğ’”0,ğ‘0,Â·Â·Â·,ğ’”ğ‘‡,ğ‘ğ‘‡)}ğ‘–, we design a Con-
trastive Data Sharing method. This method chooses a more nuanced
approach for learning representations and data sharing by focusing
on sub-trajectories rather than entire trajectories when sharing data
from other tasks to the target task ğ‘–. This is because the behaviors
and decision-making patterns of human agents can significantly
align in specific locations or time slots with certain segments of
the target agentâ€™s trajectories. However, for a whole trajectory, be-
haviors and decision-making logic may vary considerably, leading
to huge differences between complete trajectories. Ignoring the
commonalities within sub-trajectories would result in an inefficient
data sharing process, as only a few trajectories from other tasks or
agents would be considered relevant to the target task. By lever-
aging sub-trajectories, our method ensures a more effective datasharing process, which captures the nuanced similarities between
agentsâ€™ behaviors.
In Contrastive Data Sharing, a trajectory ğœ=(ğ’”0,ğ‘0,Â·Â·Â·,ğ’”ğ‘‡,ğ‘ğ‘‡)
is partitioned into multiple sub-trajectories ğœ={ğ‘¥ğ‘˜}, where each ğ‘¥ğ‘˜
represents a distinct sub-trajectory composed of ğ‘¤consecutive tran-
sitions, e.g.,ğ‘¥1={(ğ’”0,ğ‘0,ğ’”1,ğ‘Ÿ1),Â·Â·Â·,(ğ’”ğ‘¤âˆ’1,ğ‘ğ‘¤âˆ’1,ğ’”ğ‘¤,ğ‘Ÿğ‘¤)}. The
goal is to learn latent embeddings and discern similarities across all
trajectoriesÃğ‘
ğ‘–=1{(ğ’”0,ğ‘0,Â·Â·Â·,ğ’”ğ‘‡,ğ‘ğ‘‡)}ğ‘–by examining the relation-
ships between their sub-trajectories in a contrastive manner. In this
process, we construct positive samples as pairs of sub-trajectories
from the same target task(/agent) exhibiting strong similarities,
and negative samples as pairs of sub-trajectories from the target
agent and other agents showing clear dissimilarities, as illustrated
in Figure 3(a). As decision-making processes in adjacent locations
and time are often alike [ 49], sub-trajectories in close proximity typ-
ically exhibit similar behavior patterns. Given a trajectory from the
target task, once we set a sub-trajectory as an anchor, we construct
positive pairs composed of the anchor and another sub-trajectory
within a specified positive range ğœ†. Conversely, sub-trajectories
from different tasks at the same spatial-temporal states are consid-
ered negative samples due to behavioral differences.
The embedding of a sub-trajectory ğ‘¥is represented by ğ‘“ğœƒ(ğ‘¥)âˆˆ
Rğ‘‘, whereğ‘“ğœƒis the contrastive neural network parameterized by
ğœƒ. To learn latent representations of sub-trajectories, Contrastive
Data Sharing uses multi-view metric learning via a triplet loss [ 34],
This loss ensures that a pair of sub-trajectories ğ‘¥ğ‘
ğ‘–(anchor) and
ğ‘¥ğ‘
ğ‘–(positive) are closer to each other in the latent space than any
sub-trajectory ğ‘¥ğ‘›
ğ‘—(negative). Thus, we aim to learn a contrastive
networkğ‘“ğœƒsuch that:
ğ‘“ğœƒ(ğ‘¥ğ‘
ğ‘–)âˆ’ğ‘“ğœƒ(ğ‘¥ğ‘
ğ‘–)2
2+ğ›¼<ğ‘“ğœƒ(ğ‘¥ğ‘
ğ‘–)âˆ’ğ‘“ğœƒ(ğ‘¥ğ‘›
ğ‘—)2
2, (1)
âˆ€(ğ‘¥ğ‘
ğ‘–,ğ‘¥ğ‘
ğ‘–)âˆˆD ğ‘–,ğ‘¥ğ‘›
ğ‘—âˆˆD\D ğ‘–,
whereğ›¼is a margin that is enforced between positive and negative
pairs, andDğ‘–is the dataset for target task ğ‘–. The core idea is that two
sub-trajectories (anchor and positive) coming from the target and
showing a lot in common are pulled together, while a sub-trajectory
from different tasks is pushed apart.
After learning the contastive network, the similarities between
two sub-trajectories can be evaluated by the Euclidean distance of
their corresponding embeddings. The final data sharing strategy is
as below:
Deff
ğ‘–=Dğ‘–âˆª{ğ‘¥ğ‘—|ifğœ‡âˆ’ğ‘“ğœƒ(ğ‘¥ğ‘—)2
2<ğœ2,whereğœ‡=1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1ğ‘“ğœƒ(ğ‘¥ğ‘
ğ‘–),
ğœ2=1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1(ğ‘“ğœƒ(ğ‘¥ğ‘
ğ‘–)âˆ’ğœ‡)2,âˆ€ğ‘¥ğ‘
ğ‘–âˆˆD ğ‘–,ğ‘¥ğ‘—âˆˆD\D ğ‘–}.
(2)
Here,ğ‘šis the number of all possible anchors, ğœ2is the variance
of the embeddings of all anchors from the target task ğ‘–,ğœ‡is the
average embedding of all anchors. Based on the sharing rule shown
in Eq. (2), if the similarities are smaller than the variance of the
anchor, sub-trajectories {ğ‘¥ğ‘—}from other tasks can be added to
the effective dataset Deff
ğ‘–for target task ğ‘–. Note that once we get
4515Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Figure 3: MODA structure. Figure 2(a) illustrates the detailed structure of Contrastive Data Sharing, which consists of a
contrastive network working on positive and negative pairs of sub-trajectories from different tasks, a final embedding space is
produced by following the contrastive loss. Figure 2(b) indicates the structure of the model-based multi-task offline RL with
robust MDP, which is constructed by the combination of a dynamics model and the discriminator of a GAN.
the final effective dataset Deff
ğ‘–, we break every sub-trajectory into
transitions for future model-based multi-task offline RL.
3.1.3 Structure & Algorithm. The contrastive network ğ‘“ğœƒis com-
posed of multiple convolutional layers activated by ReLU. The
structure is illustrated in Figure 3(a). The training process of the
Contrastive Data Sharing is shown in Algorithm 1.
Algorithm 1 Contrastive Data Sharing Across Tasks
Input: Datasets{Dğ‘–}and initialized ğ‘“ğœƒ.
Output: Trainedğ‘“ğœƒand the effective datasets {Deff
ğ‘–}for all tasks.
1:fortaskğ‘–=1,2,3,Â·Â·Â·do
2: foriterationğ‘˜=1,2,3,Â·Â·Â·do
3: Sample a batch of contrastive triples {(ğ‘¥ğ‘
ğ‘–,ğ‘¥ğ‘
ğ‘–,ğ‘¥ğ‘›
ğ‘–)}.
4: Get a batch of{(ğ‘“(ğ‘¥ğ‘
ğ‘–),ğ‘“(ğ‘¥ğ‘
ğ‘–),ğ‘“(ğ‘¥ğ‘›
ğ‘–))}withğ‘“ğœƒ.
5: Updateğ‘“ğœƒwith Eq. (1).
6: end for
7: Perform data sharing with Eq. (2) and get Deff
ğ‘–.
8:end for
3.2 Model-based Multi-task offline RL with
Robust MDP
After Contrastive Data Sharing, a novel model-based multi-task
offline RL approach is designed to learn better policies compared
to agentsâ€™ behavior policies. The proposed model-based offline RL
approach aims to construct a robust MDP by learning a dynamics
model and a generative adversarial network (GAN). Once the dy-
namics model and the GANâ€™s discriminator are integrated, a robust
MDP can be established. Any state-of-the-art online RL or planning
algorithm, such as Soft Actor-Critic (SAC) [ 13], can be employedwithin this robust MDP to finally acquire the optimized policies for
the target tasks.
3.2.1 Robust MDP. To successfully construct a robust MDP, we fol-
low the steps including (i) learning a dynamics model, (ii) learning
a GAN, and (iii) Robust MDP construction.
Learning a dynamic model. In our proposed model-based of-
fline RL algorithm, the first step involves using the offline dataset
(i.e., the final effective dataset Deff
ğ‘–for a target task ğ‘–) to learn an
approximate dynamics model with rewards Ë†ğ‘ƒğœ“(ğ’”â€²,ğ‘Ÿ|ğ’”,ğ‘,ğ‘–), where
Ë†ğ‘ƒğœ“is a deep neural network parameterized by ğœ“. Note we cannot
use the whole dataset Dto learn a Ë†ğ‘ƒğœ“(ğ’”â€²,ğ‘Ÿ|ğ’”,ğ‘)applicable to all
agents since different agents have different preferences and rewards,
and correspond to different Ë†ğ‘ƒğœ“(ğ’”â€²,ğ‘Ÿ|ğ’”,ğ‘,ğ‘–)2. This can be achieved
through maximum likelihood estimation or other techniques from
generative and dynamics modeling [ 3,40,41]. However, urban en-
vironments are inherently complex and multifaceted. Even with
the dataset obtained after Contrastive Data Sharing, it is impossible
to cover the entire state space. Consequently, it is challenging to
model every potential transition within the environment using the
learned dynamics model. Thus, simply learning a dynamics model
with rewards from the data Deff
ğ‘–is far from enough to correctly
reflect the real-world dynamics.
Learning a GAN. Given that the transitions generated by the
learned dynamics model are not universally reliable, relying solely
on this model could lead to a highly sub-optimal policy. This is pri-
marily due to the modelâ€™s potential to produce incorrect transitions,
2In practice,Dis much larger than Deff
ğ‘–, if we learn a unified dynamics model
Ë†ğ‘ƒ(ğ’”â€²|ğ’”, ğ‘)usingDand individual reward function ğ‘Ÿğ‘–(ğ’”, ğ‘)usingDeff
ğ‘–,ğ‘Ÿğ‘–(ğ’”, ğ‘)will
not be compatible with Ë†ğ‘ƒ(ğ’”â€²|ğ’”, ğ‘), and cannot produce reliable rewards for transitions
not inDeff
ğ‘–, leading to a sub-optimal policy.
4516KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinbo Zhao et al.
particularly in unexplored parts of the environment. Consequently,
we utilize the discriminator of a GAN model to discern between
reliable and unreliable transitions. Additionally, GANâ€™s generator
can generate more data based on the offline dataset.
By training a GAN using the dataset Deff
ğ‘–, the generator ğºtries
to learn the data distribution of Deff
ğ‘–. The goal of ğºis to generate
transitions that closely mimic the real transitions presented in the
real MDPM. The goal of discriminator ğ·is to differentiate be-
tween these generated transitions and the actual transitions from
the dataset. When the generator tries to create transitions indistin-
guishable from real ones to effectively fooling the discriminator, the
discriminator aims to enhance its ability to identify the generated
transitions, thereby not being deceived. This adversarial process
ensures continuous improvement of both components.
One significant advantage of incorporating a GAN in our model-
based multi-task offline RL is the generatorâ€™s inherent ability to
generalize beyond the offline dataset. The generator will learn the
data distribution of the real MDP and thus the generated transitions
are more generalized and not only limited to Deff
ğ‘–. This broader
scope of generated transitions will benefit the discriminator as
well, enhancing its capability in accurately distinguishing correct
transitions from out-of-distribution ones, even for transitions that
are not inDeff
ğ‘–. The objective of training the GAN model is listed
as below:
min
ğœ™maxğœ’E(ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘[logğ·ğœ’(ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)]
+Eğ‘§âˆ¼ğ‘ğ‘§(ğ‘§)[log(1âˆ’ğ·ğœ’(ğºğœ™(ğ‘§)))], (3)
where(ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)is a transition, ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ is the data distribution of the
real MDP,ğ‘§is a random noise sampled from a Gaussian. In this way,
we get a discriminator working as the transition detector which
can tell whether a transition is from the real data distribution or
not. Bothğºandğ·are deep neural nets parameterized by ğœ™andğœ’.
Robust MDP construction. Once we get the learned dynamics
model and the discriminator, we combine them to construct a robust
MDP. For every transition produced by the dynamics model, we
use the discriminator to tell whether it is a reliable one. The final
robust MDP is as below:
Ë†ğ‘ƒrobust ğ’”â€²,ğ‘Ÿ|ğ’”,ğ‘,ğ‘–=ğ›¿(ğ’”â€²=Terminate)ifğ·(ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)â‰¥ğ›¾,
Ë†ğ‘ƒ(ğ’”â€²,ğ‘Ÿ|ğ’”,ğ‘,ğ‘–) if o.w,
(4)
where Ë†ğ‘ƒrobust is the final robust MDP, ğ›¿(ğ’”â€²=Terminate)is the
Dirac delta function, which forces the MDP to transit to the ter-
mination state, ğ›¾is a threshold for the discriminator output (i.e .,
a score determining the reliability of a given transition), a score
higher than ğ›¾indicates a transition is reliable, while a score below
ğ›¾suggests otherwise.
3.2.2 Policy Optimization. After the construction of the robust
MDP, we treat each individual agent ğ‘–as a task under its robust
MDP. Therefore, for each task ğ‘–, we can apply any online RL or
planning algorithms to optimize the generalized policy ğœ‹(ğ‘|ğ’”,ğ‘–).
In this paper, we directly apply Soft-Actor-Critic (SAC) [ 13] for
policy optimization. The detailed algorithm for our model-based
offline RL is shown in Algorithm 2.
The structure of our model-based offline RL is shown in Fig-
ure 3(b). The dynamics model Ë†ğ‘ƒğœ“contains several fully-connectedlayers followed by ReLU. Ë†ğ‘ƒğœ“uses a state ğ’”and an action ğ‘as the in-
put and tries to predict the next state ğ’”â€²and rewardğ‘Ÿ. The generator
ğºğœ™eats a random noise and generates a transition (ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ), the
discriminator ğ·ğœ’uses a real or generated transition (ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)as
input and outputs a score. Both ğºğœ™andğ·ğœ’contains fully-connected
layers followed by ReLU and Sigmoid, respectively.
Algorithm 2 Model-Based Offline RL with Robust MDP
Input: Effective dataset {Deff
ğ‘–}, initialized dynamics model
Ë†ğ‘ƒğœ“(ğ’”â€²,ğ‘Ÿ|ğ‘ ,ğ‘,ğ‘–), policyğœ‹ğœ”(ğ‘|ğ’”,ğ‘–), generator ğºğœ™and discrimi-
natorğ·ğœ’.
Output: Robust MDP Ë†ğ‘ƒrobust(ğ’”â€²,ğ‘Ÿ|ğ‘ ,ğ‘,ğ‘–)and well-learned ğœ‹ğœ”(ğ‘|
ğ’”,ğ‘–),ğºğœ™andğ·ğœ’.
1:fortaskğ‘–=1,2,3,...do
2: foriteration =1,2,3,...do
3: Sample a batch of transitions {(ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)}fromDeff
ğ‘–.
4: Update Ë†ğ‘ƒğœ“with maximum likelihood estimation [46].
5: end for
6: foriteration =1,2,3,...do
7: Sample a batch of transitions {(ğ’”,ğ‘,ğ’”â€²,ğ‘Ÿ)}fromDeff
ğ‘–.
8: Updateğºğœ™andğ·ğœ’with Eq.3.
9: end for
10: Create Ë†ğ‘ƒrobust using Ë†ğ‘ƒğœ“andğ·ğœ’following Eq. (4).
11: Get the policy ğœ‹ğœ”by running SAC [13] in Ë†ğ‘ƒrobust .
12:end for
4 EXPERIMENTS
In our experiments, we aim to evaluate whether MODA can achieve
satisfactory performance in the real-world multi-task urban setting.
We will answer the following questions with extensive experiments:
(1) Can our MODA learn good policies for different tasks compared
to state-of-the-art baseline methods in the multi-task urban setting?
(2) Can the Contrastive Data Sharing method in MODA effectively
share data across tasks compared to other data sharing methods?
(3) How do different features (including the amount of shared data
and other hyperparameters) affect the performance?
4.1 Dataset and Experiment Descriptions
Data description. We evaluate our MODA using the taxi trajec-
tory dataset representing diverse taxi driversâ€™ passenger-seeking
strategies. Different drivers employ different strategies for seeking
passengers which can be viewed as different tasks. The passenger-
seeking trajectories are collected from 17,877 taxis in Shenzhen,
China from July 1 to Sep 31, 2016. Each passenger-seeking trajec-
tory is formed by multiple consecutive GPS records of a certain
taxi. A GPS record includes five attributes including the taxi plate
ID, longitude, latitude, time stamp and passenger indicator.
In this dataset, the whole Shenzhen City is first divided into 40Ã—
50equal-sized grid cells with a side-length ğ‘™1=0.0084â—¦in latitude
andğ‘™2=0.0126â—¦in longitude. And the time of a day is divided
into five-minute time slots. A spatial-temporal state is defined as a
multi-dimensional tensor which is composed of different feature
maps of its neighboring 5 Ã—5 grid cells in a specific time slot, here
the features includes traffic volume, travel demand, traffic speed,
4517Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 2: Results on the taxi driversâ€™ passenger-seeking environment. All the learned policies are evaluated using 20 rollouts in
the simulated environments. The values are averaged over three random seeds.
MODA CQL BCQ BEAR MOReL MODAâˆ’
Driver 1 (expert) 135.54Â±9.75 119.41Â±9.10 110.94Â±12.62 120.47Â±10.63 109.28Â±7.23 100.24Â±5.34
Driver 2 (medium) 134.93Â±9.95 117.40Â±10.26 109.89Â±16.72 116.08Â±12.53 100.15Â±6.38 96.75Â±9.32
Driver 3 (random) 131.71Â±6.23 122.70Â±8.31 119.08Â±9.25 121.82Â±8.31 107.24Â±9.66 100.96Â±13.57
Average 134.06Â±8.81 117.33Â±11.99 115.81Â±10.80 119.46Â±10.63 105.56Â±7.88 99.32Â±9.99
Figure 4: Performance of Offline RL algorithms with differ-
ent data sharing approaches. All results are averaged over 20
rollouts, utilizing three random seeds.
waiting time, and the distance to the selected Points of Interests
(PoIs). When a taxi is in a specific state, the taxi driver has 10 actions
to choose from, including going to 8 neighboring grid cells, staying
at the current grid cell, and terminating the trip.
Passenger-seeking simulation: We created a simulated passenger-
seeking environment based on the above taxi trajectory data for
the comparison of approaches requiring environmental interac-
tions. We have opened source this simulation environment. More
details about data and the simulated environment can be found in
Appendix.
4.2 Baselines
To evaluate our proposed model-based offline RL with robust MDP,
we evaluate multiple state-of-the-art offline RL models as baselines
including model-free algorithms CQL [22],BCQ [11],BEAR [21],
and a model-based algorithm MOReL [19]. All algorithms are run-
ning on the same effective datasets after applying data sharing for
target tasks. Besides, a baseline called MODAâˆ’is also compared
which removes the GAN from our MODA. Furthermore, to vali-
date whether our Contrastive Data Sharing method is effective, we
compare it with other data sharing strategies without requiring
reward functions including No Sharing [45],Sharing All [45], and
UDS [44]. No Sharing doesnâ€™t perform any data sharing, Sharing
All naively shares all data across tasks, UDS relabels data from other
tasks with zero reward and then share with the target.
4.3 Experimental Settings
Contrastive Data Sharing. We train the Contrastive learning
models by randomly sampling triples containing Anchor, positive,
and negative from the dataset. We use 4 convolutional layers with
Figure 5: Impact of different number of transitions shared
by Contrastive Data Sharing. MODA exhibits varying per-
formance across drivers of different expertise levels when a
distinct number of transitions are shared with the target dri-
ver. All results are averaged over 20 rollouts, utilizing three
random seeds.
kernel size 3. The batch size is set to 32. Adam optimizer is applied.
All models are trained for 2000 epochs.
Model-based Multi-task offline RL with Robust MDP. We use
transitions from effective dataset Deff
ğ‘–to train GAN and the dynam-
ics model. Then we use the discriminator in GAN and dynamics
model to build a Robust MDP. More details about these two models
and Robust MDP can be found in the Appendix.
4.4 Empirical Results
Results of Question (1). To evaluate whether our proposed model-
based offline RL with robust MDP in MODA can produce good
policies for different tasks, we compare MODA with baselines in-
cluding CQL, BCQ, BEAR, MOReL and MODA âˆ’for 3 different
drivers. All baselines are built upon the same effective dataset for a
specific driver after contrastive data sharing. As shown in Table. 2,
we pick 3 drivers from our dataset including an expert driver, a
medium-skilled driver and a random driver, whose behavior poli-
cies are optimal, medium and random policies, respectively. Each
value in Table. 2 is the average expected cumulative reward for 20
rollouts in the simulated environment over three random seeds.
We also present the average results of all drivers for different mod-
els. The findings, as detailed in Table 2, demonstrate that MODA
significantly surpasses the baselines, substantiating the effective-
ness of our robust MDP construction and our approachâ€™s ability
to address distributional shifts. Notably, MODA âˆ’, which excludes
the GAN component from MODA, exhibits inferior performance,
underscoring the critical role of GAN in achieving a robust MDP
4518KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinbo Zhao et al.
Figure 6: Impact of hyper-parameters on MODA.
within MODA. Furthermore, MODA can successfully enhance pol-
icy performance across drivers of varying skill levels, with even
medium and random drivers whose behavior policies are highly
suboptimal achieving outcomes comparable to those of the expert
driver. This underscores MODAâ€™s capacity to markedly improve
policy effectiveness across diverse tasks.
Results of Question (2). To assess the efficacy of the Contrastive
Data Sharing (DS) method in MODA, we conducted a comparative
analysis against alternative data sharing strategies, including No
Sharing, Sharing All, and UDS. As shown in Figure 4, for a certain
driver, we run different offline RL models including MODA, CQL,
BCQ and BEAR on the effective datasets constructed by Contrastive
DS, No Sharing, Sharing All and UDS. We find No Sharing leads to
poor performance mainly due to the lack of data for the target driver.
Naively Sharing All will introduce a lot of bias to the target driver
and result in low rewards. UDS relabels the rewards before sharing
data which degrades the RL performance. In contrast, all offline RL
models achieve the highest rewards on the effective dataset built
by Contrastive Data Sharing, which indicates our Contrastive Data
Sharing can successfully share similar data from other tasks with
the target task by learning effective data representations, ensuring
that the shared data reflect comparable behaviors, preferences, and
decision-making logic.
Results of Question (3). We also evaluate how different amounts
of data shared with the target task by Contrastive Data Sharing
affect the MODA performance. As shown in Figure 5, we find MODA
exhibits varying performance across drivers of different expertise
levels when a distinct number of transitions are shared with the
target driver. For example, the expert driver needs the least number
of shared data and achieves the best performance, and the random
driver needs the most number of shared data to achieve a better
performance. This results align with the real-world cases where itâ€™s
much easier for an expert driver to get high rewards if the shared
data is effective, and for random drivers, they usually need more
effective data to improve their policies.
Besides, we also evaluate how different hyperparameters includ-
ing the number of transitions in a sub-trajectory, positive range
ğœ†, the margin ğ›¼and the training epochs affect our MODA perfor-
mance. As shown in Figure 6(a), we find the MODA performance
decreases if a sub-trajectory contains more transitions, since a long
sub-trtajectory will contain different spatial-temporal states and
actions and thus introduce more bias to the model. As shown in
Figure 6(b), we find large positive range ğœ†will degrade the perfor-
mance since there is no guarantee that positive pairs will exhibitsimilar behaviors. As shown in Figure 6(c), if margin is too large,
we cannot guarantee an effective contrastive loss. As shown in
Figure 6(d), more training epochs will improve the quality of the
dynamics model.
5 RELATED WORK
Offline RL. Offline RL [ 7,24,25] is a data-driven RL framework that
learns better policies from previously collected datasets without
further environment interaction. It shows promising performance
across various domains, including robotic control [ 17,33], NLP [ 16],
and healthcare [ 42]. However, offline RL faces the challenge of distri-
butional shift, where the learned policy diverges from the behavior
policy, leading to erroneous value backups [ 11,25]. To address this
problem, some offline RL algorithms introduce constraints on the
policy [11, 16, 21, 31, 37, 43] or on the learned Q-function [20, 22],
to ensure the learned policy does not stray excessively from the be-
havior policy. Additionally, model-based offline RL strategies have
been explored to mitigate distributional shift, such as constructing
a conservative MDP [ 19] or implementing reward penalties [ 46].
However, all these works are not applicable in our multi-task urban
settings since they did not consider the practical data issues (e.g.,
data scarcity and heterogeneity) and the spatial-temporal nature of
urban decisions.
Multi-Task RL. Multi-task RL learns multiple tasks simultaneously
to improve generalization and learning efficiency in RL settings.
The prior work[ 39] utilize the transfer learning concept, which
shares a "distilled" policy across tasks to encourage positive transfer
while maintaining task-specific policies. More recent approaches
have focused on leveraging meta-learning for multi-task RL. Finn
et al. [ 10] introduced Model-Agnostic Meta-Learning (MAML), a
versatile framework applicable to multi-task RL, demonstrating
substantial improvements in learning efficiency. Similarly, Hessel
et al. [ 14] proposed the PopArt normalization technique to stabilize
value estimates across varying reward scales in multi-task settings.
Besides, the works [ 2,9,44,45] related to multi-task offline RL
mainly focus on goal-conditioned RL. In contrast, in this paper, we
focus on the multi-task urban setting.
RL with data sharing. Sharing data across tasks has been demon-
strated as highly beneficial in both multi-task RL [ 8,18,45] and
meta-RL [ 6,29,32]. Prior works have explored various data-sharing
approaches, such as utilizing learned Q-values [ 8,45], domain
knowledge [ 18], or distances to goals in goal-conditioned RL set-
tings [ 4,27,28,38]. Another approach UDS [ 44] treats all unlabeled
data as having zero reward to facilitate data sharing. Moreover,
4519Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
most of these works rely on access to the reward function of the
target task and exhibit limitations in urban scenarios. In contrast,
our work does not require any reward function and tries to share
data based on latent similarities.
6 CONCLUSION
In this paper, we aim to enhance diverse human decision-making
processes in an urban environment using offline RL which can
learn policies from a static dataset pre-collected by a certain be-
havior policy. However, standard offline RL faces two significant
challenges: (i) data scarcity and heterogeneity, and (ii) distributional
shift. To address both challenges in a multi-task offline RL setting
where learning the policy for each human agent can be viewed
as a task, we introduce MODA â€” a Multi-Task Offline Reinforce-
ment Learning with Contrastive Data Sharing approach. MODA
includes a new contrastive data sharing method which can extract
latent representations of human behaviors by contrasting positive
and negative data pairs. Moreover, MODA develops a novel model-
based offline RL algorithm. This algorithm constructs a robust MDP
by integrating a dynamics model with a Generative Adversarial
Network (GAN). Once the robust MDP is established, any online
RL or planning algorithm can be applied. Extensive experiments
conducted in a real-world multi-task urban setting has validated
the effectiveness of MODA.
ACKNOWLEDGMENTS
This work was supported in part by NSF grants IIS-1942680 (CA-
REER), CNS-1952085 and DGE-2021871. Jun Luo is supported by
The Innovation and Technology Fund (Ref. ITP/069/23LP).
Disclaimer: Any opinions, findings, conclusions or recommenda-
tions expressed in this material/event (or by members of the project
team) do not reflect the views of the Government of the Hong
Kong Special Administrative Region, the Innovation and Technol-
ogy Commission or the Innovation and Technology Fund Research
Projects Assessment Panel.
REFERENCES
[1]2024. MOTSC: Model-based Offline Traffic Signal Control. https://openreview.
net/forum?id=K6BXvqWWmq
[2]Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter
Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017.
Hindsight Experience Replay. CoRR abs/1707.01495 (2017). arXiv:1707.01495
http://arxiv.org/abs/1707.01495
[3]Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Sched-
uled Sampling for Sequence Prediction with Recurrent Neural Networks.
arXiv:1506.03099 [cs.LG]
[4]Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake
Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, and Sergey
Levine. 2021. Actionable Models: Unsupervised Offline Reinforcement Learning
of Robotic Skills. CoRR abs/2104.07749 (2021). arXiv:2104.07749 https://arxiv.
org/abs/2104.07749
[5]Xingyuan Dai, Chen Zhao, Xiaoshuang Li, Xiao Wang, and Fei-Yue Wang. 2021.
Traffic Signal Control Using Offline Reinforcement Learning. In 2021 China
Automation Congress (CAC). 8090â€“8095. https://doi.org/10.1109/CAC53003.2021.
9728551
[6]Ron Dorfman, Idan Shenfeld, and Aviv Tamar. 2021. Offline Meta Reinforcement
Learning â€“ Identifiability Challenges and Effective Data Collection Strategies. In
Advances in Neural Information Processing Systems, M. Ranzato, A. Beygelzimer,
Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Asso-
ciates, Inc., 4607â€“4618. https://proceedings.neurips.cc/paper_files/paper/2021/
file/248024541dbda1d3fd75fe49d1a4df4d-Paper.pdf
[7]Damien Ernst, Pierre Geurts, and Louis Wehenkel. 2005. Tree-Based Batch Mode
Reinforcement Learning. J. Mach. Learn. Res. 6 (dec 2005), 503â€“556.[8] Benjamin Eysenbach, Xinyang Geng, Sergey Levine, and Ruslan Salakhutdinov.
2020. Rewriting History with Inverse RL: Hindsight Inference for Policy Improve-
ment. CoRR abs/2002.11089 (2020). arXiv:2002.11089 https://arxiv.org/abs/2002.
11089
[9] Benjamin Eysenbach, Xinyang Geng, Sergey Levine, and Ruslan Salakhutdinov.
2020. Rewriting History with Inverse RL: Hindsight Inference for Policy Improve-
ment. CoRR abs/2002.11089 (2020). arXiv:2002.11089 https://arxiv.org/abs/2002.
11089
[10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-
Learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th Inter-
national Conference on Machine Learning - Volume 70. 1126â€“1135.
[11] Scott Fujimoto, David Meger, and Doina Precup. 2018. Off-Policy Deep
Reinforcement Learning without Exploration. CoRR abs/1812.02900 (2018).
arXiv:1812.02900 http://arxiv.org/abs/1812.02900
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial
Nets. In NeurIPS.
[13] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a
Stochastic Actor. arXiv:1801.01290 [cs.LG]
[14] Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostro-
vski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Sil-
ver. 2019. Multi-task deep reinforcement learning with popart. arXiv preprint
arXiv:1901.04465 (2019).
[15] Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.
Advances in neural information processing systems 29 (2016).
[16] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Ã€gata
Lapedriza, Noah Jones, Shixiang Gu, and Rosalind W. Picard. 2019. Way Off-Policy
Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog.
CoRR abs/1907.00456 (2019). arXiv:1907.00456 http://arxiv.org/abs/1907.00456
[17] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric
Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and
Sergey Levine. 2018. QT-Opt: Scalable Deep Reinforcement Learning for Vision-
Based Robotic Manipulation. CoRR abs/1806.10293 (2018). arXiv:1806.10293
http://arxiv.org/abs/1806.10293
[18] Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico
Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. 2021. MT-
Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale. CoRR
abs/2104.08212 (2021). arXiv:2104.08212 https://arxiv.org/abs/2104.08212
[19] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
2020. MOReL: Model-Based Offline Reinforcement Learning. In Proceedings
of the 34th International Conference on Neural Information Processing Systems
(Vancouver, BC, Canada) (NIPSâ€™20). Curran Associates Inc., Red Hook, NY, USA,
Article 1830, 14 pages.
[20] Ilya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum. 2021. Offline
Reinforcement Learning with Fisher Divergence Critic Regularization. CoRR
abs/2103.08050 (2021). arXiv:2103.08050 https://arxiv.org/abs/2103.08050
[21] Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. 2019. Stabilizing
Off-Policy Q-Learning via Bootstrapping Error Reduction. CoRR abs/1906.00949
(2019). arXiv:1906.00949 http://arxiv.org/abs/1906.00949
[22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conserva-
tive Q-Learning for Offline Reinforcement Learning. In Proceedings of the 34th
International Conference on Neural Information Processing Systems (Vancouver,
BC, Canada) (NIPSâ€™20). Curran Associates Inc., Red Hook, NY, USA, Article 100,
13 pages.
[23] Mayuresh Kunjir and Sanjay Chawla. 2022. Offline Reinforcement Learning
for Road Traffic Control. CoRR abs/2201.02381 (2022). arXiv:2201.02381 https:
//arxiv.org/abs/2201.02381
[24] Sascha Lange, Thomas Gabel, and Martin Riedmiller. 2012. Batch Reinforcement
Learning. Springer Berlin Heidelberg, Berlin, Heidelberg, 45â€“73. https://doi.org/
10.1007/978-3-642-27645-3_2
[25] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline Rein-
forcement Learning: Tutorial, Review, and Perspectives on Open Problems. CoRR
abs/2005.01643 (2020). arXiv:2005.01643 https://arxiv.org/abs/2005.01643
[26] Jianxiong Li, Shichao Lin, Tianyu Shi, Chujie Tian, Yu Mei, Jian Song, Xianyuan
Zhan, and Ruimin Li. 2023. A Fully Data-Driven Approach for Realistic Traffic
Signal Control Using Offline Reinforcement Learning. arXiv:2311.15920 [cs.AI]
[27] Xingyu Lin, Harjatin Singh Baweja, and David Held. 2019. Reinforcement Learn-
ing without Ground-Truth State. CoRR abs/1905.07866 (2019). arXiv:1905.07866
http://arxiv.org/abs/1905.07866
[28] Hao Liu, Alexander Trott, Richard Socher, and Caiming Xiong. 2019. Competitive
Experience Replay. CoRR abs/1902.00528 (2019). arXiv:1902.00528 http://arxiv.
org/abs/1902.00528
[29] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn.
2020. Offline Meta-Reinforcement Learning with Advantage Weighting. CoRR
abs/2008.06043 (2020). arXiv:2008.06043 https://arxiv.org/abs/2008.06043
[30] Menghai Pan, Weixiao Huang, Yanhua Li, Xun Zhou, and Jun Luo. 2020. XGAIL:
Explainable Generative Adversarial Imitation Learning for Explainable Human
Decision Analysis. In KDD, 2020.
4520KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinbo Zhao et al.
[31] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. 2019. Advantage-
Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning.
CoRR abs/1910.00177 (2019). arXiv:1910.00177 http://arxiv.org/abs/1910.00177
[32] Vitchyr H. Pong, Ashvin Nair, Laura M. Smith, Catherine Huang, and Sergey
Levine. 2021. Offline Meta-Reinforcement Learning with Online Self-Supervision.
CoRR abs/2107.03974 (2021). arXiv:2107.03974 https://arxiv.org/abs/2107.03974
[33] Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. 2020. Of-
fline Reinforcement Learning from Images with Latent Space Models. CoRR
abs/2012.11547 (2020). arXiv:2012.11547 https://arxiv.org/abs/2012.11547
[34] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: A
unified embedding for face recognition and clustering. In 2015 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.
1109/cvpr.2015.7298682
[35] Dhruv Shah, Arjun Bhorkar, Hrishit Leen, Ilya Kostrikov, Nicholas Rhinehart,
and Sergey Levine. 2022. Offline Reinforcement Learning for Visual Navigation.
In6th Annual Conference on Robot Learning. https://openreview.net/forum?id=
uhIfIEIiWm_
[36] Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. 2021. Offline Reinforcement
Learning for Autonomous Driving with Safety and Exploration Enhancement.
CoRR abs/2110.07067 (2021). arXiv:2110.07067 https://arxiv.org/abs/2110.07067
[37] Noah Y. Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin A.
Riedmiller. 2020. Keep Doing What Worked: Behavioral Modelling Priors for
Offline Reinforcement Learning. CoRR abs/2002.08396 (2020). arXiv:2002.08396
https://arxiv.org/abs/2002.08396
[38] Hao Sun, Zhizhong Li, Xiaotong Liu, Dahua Lin, and Bolei Zhou. 2019. Policy
Continuation with Hindsight Inverse Dynamics. CoRR abs/1910.14055 (2019).
arXiv:1910.14055 http://arxiv.org/abs/1910.14055
[39] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor.
2017. Deep multi-task learning with low level tasks supervised at lower layers.
arXiv preprint arXiv:1704.05098 (2017).
[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. CoRR (2017).
[41] Arun Venkatraman, Martial Hebert, and J.. Bagnell. 2015. Improving Multi-Step
Prediction of Learned Time Series Models. Proceedings of the AAAI Conference
on Artificial Intelligence 29, 1 (Feb. 2015). https://doi.org/10.1609/aaai.v29i1.9590
[42] Lu Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha. 2018. Supervised
Reinforcement Learning with Recurrent Neural Network for Dynamic Treat-
ment Recommendation. CoRR abs/1807.01473 (2018). arXiv:1807.01473 http:
//arxiv.org/abs/1807.01473
[43] Yifan Wu, George Tucker, and Ofir Nachum. 2019. Behavior Regularized Offline
Reinforcement Learning. CoRR abs/1911.11361 (2019). arXiv:1911.11361 http:
//arxiv.org/abs/1911.11361
[44] Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and
Sergey Levine. 2022. How to Leverage Unlabeled Data in Offline Reinforcement
Learning. CoRR abs/2202.01741 (2022). arXiv:2202.01741 https://arxiv.org/abs/
2202.01741
[45] Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine,
and Chelsea Finn. 2021. Conservative Data Sharing for Multi-Task Offline Re-
inforcement Learning. CoRR abs/2109.08128 (2021). arXiv:2109.08128 https:
//arxiv.org/abs/2109.08128
[46] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey
Levine, Chelsea Finn, and Tengyu Ma. 2020. MOPO: Model-based Offline Policy
Optimization. In Advances in Neural Information Processing Systems, Vol. 33.
Curran Associates, Inc., 14129â€“14142.
[47] Xin Zhang, Yanhua Li, Xun Zhou, and Jun Luo. 2020. cGAIL: Conditional Gener-
ative Adversarial Imitation Learningâ€”An Application in Taxi Driversâ€™ Strategy
Learning. IEEE TBD (2020).
[48] Xin Zhang, Yanhua Li, Xun Zhou, Ziming Zhang, and Jun Luo. 2020. TrajGAIL:
Trajectory Generative Adversarial Imitation Learning for Long-Term Decision
Analysis. In ICDMâ€™20.
[49] Yingxue Zhang, Yanhua Li, Xun Zhou, Ziming Zhang, and Jun Luo. 2023. STM-
GAIL: Spatial-Temporal Meta-GAIL for Learning Diverse Human Driving Strategies.
A APPENDIX
To support the reproducibility, our code and data are released on-
line3.
3https://github.com/anony11sdf/MODA
A.1 Detailed Settings of Contrastive Data
Sharing
A.1.1 Contrastive network. Our dataset contains a large number
of taxi drivers seeking passenger trajectories. We train the con-
trastive network by sampling triplets randomly from the dataset.Specifically, we first select the target driver, set the size of the sub-
trajectory, and the Positive range, and then select the Anchor, and
the Positive sub-trajectory in the manner described in the main
text. At this point, we iterate through all the trajectories of all the
other drivers, and find the the negative sub-trajectory, thus forming
the triples. In the training process, for a triple, we forward prop-
agate each of the three sub-trajectories to get embeddings, and
then compute the contrastive loss. In the training process, for a
triple, we forward propagate each of the three sub-trajectories to
get embedding, and then compute the contrastive loss.
It is worth noting that the number of triples composed in this
way will be very large, so in order to train the model more efficiently,
we will randomly filter the composition of triples to decide whether
to use it to train the model or not. Usually, we filter 10% to 20%, if
the sub-trajectory contains more transitions, it means that we need
to filter fewer triples because the GPU memory is limited.
A.1.2 Sharing data. Despite we have Contrastive models, sharing
data can be done in many ways. We choose Eq. (2)for data-sharing
to improve efficiency. According to the Contrastive loss, the most
intuitive way to share data is to collect data as in the training process
(without random filtering), and then form triples. then calculate
the Contrastive loss, and then set a threshold to decide whether to
share the current negative sub-trajectory that gets this Contrastive
loss or not. However, in the process of implementation, we found
that this way of sharing data consumes a lot of arithmetic power,
and at the same time, due to the huge number of triples, it is almost
impossible to check all negatives, so it is inefficient. Thatâ€™s why we
use the Eq. (2)description for data analysis. This approach improves
the efficiency of the whole data sharing by hundreds of times, and
at the same time, it also reduces a hyperparameter (Contrastive loss
sharing threshold). Also, In the end, the data-sharing approach we
adopted works well.
A.2 Detailed Settings of Model-based Multi-task
offline RL with Robust MDP
We use transitions from effective dataset Deff
ğ‘–to train GAN models
and dynamics models.
A.2.1 GAN. In GAN, both the Generator and Discriminator utilize
fully connected neural networks with ReLU activation functions. It
is configured with a batch size of 32 and trained over 2000 epochs.
The learning rates for the generator and discriminator are set to
0.0005 and 0.0002, respectively.
A.2.2 Dynamics model. Dynamics model Ë†ğ‘ƒğœ“encompasses two
models tasked with predicting rewards and subsequent states, re-
spectively. The learning rate is set to 0.02, with a batch size of 32,
and the model is trained for 2000 epochs.
For state Sub-Model: This model predicts the next state given
the current state and action. It has two fully connected layers with
ReLU activation functions. The input size is the sum of the state
size and action size, and the output size is the state size.
For reward Sub-Model: This model predicts the reward given
the current state and action. It has two fully connected layers with
4521Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
ReLU activation functions. The input size is the sum of the state
size and action size, and the output size is the reward size.
A.2.3 Robust MDP construction. After completing the training of
the GAN and dynamics models, we need to build a Robust MDP by
combining the GANâ€™s discriminators with the dynamics models. It
works like this: first, we randomize an initial state that is randomly
drawn from the entire dataset in a transition Then the policy ob-
tained by the subsequent algorithm generates an action based on
this state, and this state and action are fed to the dynamics models,
which predicts the forward and next states and forms a transition.
this transition is then this transition will be input to the discrimina-
tor, which will output 0 or 1 to determine whether the transition is
real or not. If the discriminator outputs 0, it means that the current
transition is false, and the environment will be automatically set to
the halt state and return to done, which represents the end of the
current episode. This halt state will set the reward to a very small
value as a penalty.
About why the initial state is randomly selected instead of ran-
domly generated. Because the environment we simulate is a con-
tinuous environment with very high dimensionality, the randomly
generated state has a high probability of being unrealistic and also
has a high probability of being different from the distribution of
the data that the model has learned, which will cause the Robust
MDP to end the current episodes immediately.
A.2.4 Soft-Actor-Critic (SAC). This algorithm contains 2 models,
SACActor and SACCritic. The SACActor model is responsible for
predicting action logits given the state, while the SACCritic model
estimates Q-values given the state-action pair. Additionally, a Re-
playBuffer class is implemented to store and sample experiences
for training. The hyperparameters used in training include the total
number of episodes (20000), batch size (64), discount factor (gamma
= 0.99), soft update coefficient (tau = 0.005), and entropy regular-
ization coefficient (alpha = 0.2). The replay buffer capacity is set to
300000.
A.3 Passenger-seeking simulation and data
selection details
Our dataset comprises trajectories collected from 17,877 drivers,
however, the data quality of all these drivers varies, many of them
suffer from incomplete trajectories. And in our evaluation, we use
many transitionsâ€™ data from whole dataset for environment simula-
tion, and we use 20 drivers (i.e., 20 tasks) for training all offline RL
models (including MODA and all baselines), these drivers provide
the most complete and good quality datasets. Briefly speaking, we
are solving a 20-task offline RL problem in our experiment instead
of a 17,877-task problem. To simulate the real environment of the
city aiming to test the policies learned with our MODA and all
baselines, We created a simulated passenger-seeking environment
based on the above taxi trajectory data. Specifically this model is an
ensemble model, which contains ensemble size dynamics models,
each of which has the same structure as previously mentioned. In
our experiments, we set the ensemble size to 5. Most importantly,
the data we use to train the ensemble model contains all the transi-
tions in the dataset, which is several times more than the amountof data used to train MODA and other baselines, so this model can
better simulate the passer- seeking environment.
A.4 Baselines
For CQL, BCQ and BEAR, since they are all offline reinforcement
learning algorithms, they are trained directly on the dataset and do
not need to be deployed in a certain environment like SAC.
â€¢CQL. This model consists of three fully connected layers, each
followed by a rectified linear unit (ReLU) activation function. The
input dimension of the network corresponds to the state space,
and the output dimension corresponds to the action space. The
network is trained using the Adam optimizer with a learning
rate of 0.001. Additionally, the model sets alpha to 0.1, and trains
1000 epochs with batch size 64.
â€¢BCQ This model includes two neural networks: a Q-network
and an action generator. Both networks consist of three fully
connected layers with ReLU activation functions. The Q-network
takes the state as input and outputs Q-values for each action,
while the action generator takes the state as input and outputs
a probability distribution over actions using the softmax func-
tion. The model is trained using behavior cloning from expert
demonstrations, with data loaded from the provided buffer path.
The training process runs for 1000 epochs with a learning rate of
0.0001.
â€¢BEAR. This model consists of two neural networks: a Q-network
and a policy network. The Q-network has three fully connected
layers with ReLU activation functions, taking both state and
action as input. It outputs Q-values estimating the expected cu-
mulative future rewards for each action. The policy network also
has three fully connected layers with ReLU activation functions,
taking only the state as input and outputting a probability distri-
bution over actions using the softmax function. Both networks
are optimized using the Adam optimizer with a learning rate
of 0.001. Additionally, the model employs a discount factor of
0.99 for future rewards and sets the MMD loss weight to 0.1. The
training loop iterates for 1000 epochs with a batch size of 64.
â€¢MOReL. We trained ensemble models containing multiple dy-
namic models in a single driverâ€™s data constructed the pessimistic
MDP and then deployed the SAC algorithm on that environment.
A.5 Detailed Settings of experiments on Impact
of different number of transitions shared by
contrastive data sharing
After getting effective dataset Deff
ğ‘–again, the number of transitions
it contains is fixed, so for comparison experiments, we labeled
the transitions that were shared in, and then randomly chose the
specified number of transitions to be used to construct the dataset
used as the comparison test. If the specified number is larger than
the total number of shared transitions, we select the remaining
number of unshared transitions.
A.6 Discussion on scalability and applicability
Our framework has good scalability and applicability to various
other tasks and applications. On a theoretical level, itâ€™s a general
framework that can be used in a wide range of applications. Besides,
as an example, our experiments are an urban application of the
4522KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Xinbo Zhao et al.
real world. Therefore, any real-world urban tasks (e.g., public trans-
portation, bike-sharing, ride-sharing), that have offline datasets,
can leverage our framework. From the experimental or application
level, based on the dataset of real-world urban tasks, we can abstractmany trajectories of many agents active in the same environment
based on, for example, GPS records and map information, and then
apply our framework on the dataset of these agents.
4523