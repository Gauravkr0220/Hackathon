Advancing Molecule Invariant Representation via
Privileged Substructure Identification
Ruijia Wang
wangruijia@bupt.edu.cn
Beijing University of Posts and
Telecommunications
China Telecom Cloud Computing
Research InstituteHaoran Dai
stkm_dhr@bupt.edu.cn
Beijing University of Posts and
Telecommunications
Beijing, ChinaCheng Yang
yangcheng@bupt.edu.cn
Beijing University of Posts and
Telecommunications
Beijing, China
Le Song
songle@biomap.com
BioMap Research
Mohamed bin Zayed University of
Artificial IntelligenceChuan Shiâˆ—
shichuan@bupt.edu.cn
Beijing University of Posts and
Telecommunications
Beijing, China
ABSTRACT
Graph neural networks (GNNs) have revolutionized molecule rep-
resentation learning by modeling molecules as graphs, with atoms
represented as nodes and chemical bonds as edges. Despite their
progress, they struggle with out-of-distribution scenarios, such
as changes in size or scaffold of molecules with identical proper-
ties. Some studies attempt to mitigate this issue through graph
invariant learning, which penalizes prediction variance across en-
vironments to learn invariant representations. But in the realm of
molecules, core functional groups forming privileged substructures
dominate molecular properties and remain invariant across distri-
bution shifts. This highlights the need for integrating this prior
knowledge and ensuring the environment split compatible with
molecule invariant learning. To bridge this gap, we propose a novel
framework named MILI. Specifically, we first formalize molecule
invariant learning based on privileged substructure identification
and introduce substructure invariance constraint. Building on this
foundation, we theoretically establish two criteria for environment
splits conducive to molecule invariant learning. Inspired by these
criteria, we develop a dual-head graph neural network. A shared
identifier identifies privileged substructures, while environment
and task heads generate predictions based on variant and privileged
substructures. Through the interaction of two heads, the environ-
ments are split and optimized to meet our criteria. The unified MILI
guarantees that molecule invariant learning and environment split
achieve mutual enhancement from theoretical analysis and network
design. Extensive experiments across eight benchmarks validate
the effectiveness of MILI compared to state-of-the-art baselines.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671886CCS CONCEPTS
â€¢Applied computing â†’Computational biology.
KEYWORDS
Molecule Representation Learning, Molecule Invariant Learning,
Privileged Substructure Identification, Environment Split
ACM Reference Format:
Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, and Chuan Shi. 2024. Ad-
vancing Molecule Invariant Representation via Privileged Substructure Iden-
tification. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671886
1 INTRODUCTION
Molecules, the quintessential components of matter, hold a piv-
otal role in scientific exploration [ 2] and drug discovery [ 9,53],
where deciphering their properties can drive substantial innovation.
Within this realm, Molecule Representation Learning (MRL) [ 4,12,
18] becomes a vital field of study, which embeds complex molecules
into computationally manageable vector representations. Recently,
Graph Neural Networks (GNNs) [ 38,43,52,54,59,61] have revolu-
tionized MRL by leveraging molecule graphs to learn these represen-
tations, achieving state-of-the-art results in predicting molecular
properties [17] and identifying potential drug candidates [69].
Despite their considerable achievements, they often rely on the
fundamental assumption that molecules are independently and
identically sampled from a consistent environment. In reality, the
ever-changing landscape of real-world scenarios results in environ-
mental changes and distribution shifts [ 25,27,58]. For example, in
drug repurposing, molecules initially screened under certain condi-
tions often require reassessment against entirely new diseases or bi-
ological targets. However, current GNN-based MRL methods exhibit
notable performance degradation [ 22] in these out-of-distribution
(OOD) scenarios, underscoring the pressing demand to enhance
their generalization capabilities.
Recent research addressing the OOD challenge of GNNs mainly
concentrates on graph invariant learning (GIL) [ 7,14,28,46], assum-
ing that the causal subgraph is invariant across environments while
3188
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, and Chuan Shi
environment subgraph varies. By penalizing prediction variance
across environments, models capture causal factors rather than spu-
rious correlations. Here, an open problem is effectively determining
the environment split. Existing methods have explored various
strategies, including predefined splits [ 3], graph augmentation [ 57],
and additional models specifically for environment split [29].
Nevertheless, applying GIL to GNN-based MRL involves three
key considerations: (1) Integration of domain knowledge. Privi-
leged substructures [ 26,36] are core functional groups determining
molecular activity. For instance, the analgesic properties of As-
pirin can be attributed to its ester functional group âˆ’ğ¶ğ‘‚ğ‘‚âˆ’. This
suggests that the invariant subgraph in MRL should be these chem-
ically privileged substructures. But most current methods [ 35,46]
learn arbitrary subgraphs, overlooking this a priori knowledge. (2)
Theoretical guidance for environment split facilitating GIL. Meth-
ods [ 31,70] using graph augmentation might result in nonsensical
molecules, losing the potential to provide insights to domain ex-
perts. On the other hand, predefined and learned splits from exist-
ing methods [ 29,57] are independent of downstream GIL and do
not theoretically ensure compatibility with GIL. (3) Unified model
for environment split and downstream prediction. Current meth-
ods [ 62] often treat environment split and downstream prediction
as a two-stage process, leading to a lack of mutual awareness and
suboptimal performance.
To tackle the outlined key points, we propose a novel frame-
work named MILI to advance Molecule Invariant Learning via
privileged substructure Identification. To integrate domain knowl-
edge, we formulate molecule invariant learning based on privileged
substructure identification and introduce Substructure Invariance
Constraint (SIC). We then theoretically establish two criteria for en-
vironment split to guarantee the enhancement of molecule invariant
learning: the environments should be split based on the agreement
between ground truth and downstream predictions from variant
structures, aiming at (1) maximally violating SIC and (2) maintain-
ing class distribution fairness. To fulfill these criteria, we design
a dual-head graph neural network. A shared identifier identifies
privileged substructures, followed by task and environment heads
that make downstream predictions using privileged substructures
and variant structures. In line with our criteria, the environments
are split and optimized to violate SIC by maximizing invariant risk
while enhance class distribution fairness by reweighting empir-
ical risks. Ultimately, this unified framework allows for mutual
reinforcement between environment split and molecule invariant
learning. Extensive experiments across diverse datasets demon-
strate the effectiveness of the proposed MILI.
In summary, our contributions are three-fold:
â€¢We formalize molecule invariant learning based on privi-
leged substructure identification and introduce substructure
invariance constraint. Building upon this foundation, we pro-
pose criteria for environment split, theoretically ensuring
their benefit to molecule invariant learning.
â€¢To meet the criteria, we design a novel dual-head graph neu-
ral network with a shared identifier to identify privileged
substructures. Subsequently, the interaction between envi-
ronment and task heads mutually enhances the environment
split and molecule invariant learning.â€¢Comprehensive experiments demonstrate the effectiveness
of our MILI. Additionally, case studies of identified privi-
leged substructures reflect its effective utilization of domain
knowledge, offering valuable insights for drug design.
2 MOLECULE INVARIANT LEARNING
In this section, we define OOD generalization on molecules and
then expand the invariant learning framework based on privileged
substructure identification.
OOD Generalization on Molecules. The random variable of a mol-
ecule graph is denoted as G, where nodes correspond to atoms and
edges represent chemical bonds. Let Gbe the molecule graph space
andYthe label space. We consider a dataset ğ·={(ğºğ‘–,ğ‘Œğ‘–)}ğ‘
ğ‘–=1,
whereğºğ‘–âˆˆGandğ‘Œğ‘–âˆˆY. In real-world applications, the dataset is
often sourced from multiple environments ğ·={ğ·ğ‘’}ğ‘’âˆˆEğ‘¡ğ‘Ÿ. Here,
ğ·ğ‘’={(ğºğ‘’
ğ‘–,ğ‘Œğ‘’
ğ‘–)}ğ‘ğ‘’
ğ‘–=1represents the dataset from environment ğ‘’, and
Eğ‘¡ğ‘Ÿdenotes the environment space in the training data.
Definition 1. LetErepresents the space of all possible environ-
ments andHdenotes the molecule representation space. Suppose the
predictorğ‘“can be decomposed into ğ‘“=ğœ”â—¦Î¦, where Î¦:Gâ†’H is
an encoder mapping molecules into representations, and ğœ”:Hâ†’Y
is a classifier that maps representations to the logit space of Yvia a
linear map. The goal of OOD generalization on molecules is to find
an optimal predictor ğ‘“âˆ—that performs well across all environments
ğ‘“âˆ—(Â·)=arg min
ğ‘“sup
ğ‘’âˆˆERğ‘’(ğ‘“). (1)
Here,Rğ‘’(ğ‘“)=Eğ‘ƒ(G,Y|ğ‘’)[â„“(ğ‘“(G),Y)]represents the empirical risk
on environment ğ‘’, andâ„“(Â·,Â·):YÃ—Yâ†’ Ris a loss funcion.
The joint distribution of the molecule graph and its correspond-
ing label is denoted as ğ‘ƒ(G,Y)=ğ‘ƒ(G,Y|ğ‘’)âˆ€ğ‘’âˆˆE. Distribution
shifts refer to the scenario where the joint distribution in the train-
ing datağ‘ƒğ‘¡ğ‘Ÿ(G,Y)=ğ‘ƒ(G,Y|ğ‘’)âˆ€ğ‘’âˆˆEğ‘¡ğ‘Ÿdiffers from that in the test
datağ‘ƒğ‘¡ğ‘’ğ‘ ğ‘¡(G,Y)=ğ‘ƒ(G,Y|ğ‘’)âˆ€ğ‘’âˆˆE\E ğ‘¡ğ‘Ÿ.
Molecule Invariant Learning. Molecule graph Gis characterized
by privileged substructures {Gğ‘}determining its properties. This
indicates that the relationship between these privileged substruc-
tures and the corresponding label is invariant across all environ-
ments. The complement of {Gğ‘}is denoted as Gğ‘£, representing
the structure that varies with environments. Following the invari-
ant learning literature [ 8], we define the Substructure Invariance
Constraint (SIC) for molecule invariant learning.
Definition 2. (Substructure Invariance Constraint). Sup-
pose the optimal identifier ğ›¿âˆ—is learned to identify privileged sub-
structures within a molecule graph G. Then, the molecule invariant
representation Î¦âˆ—(ğ›¿âˆ—(G))needs to satisfy the following constraint
ğ‘ƒ(Y|Î¦âˆ—(ğ›¿âˆ—(G)),ğ‘’1)=ğ‘ƒ(Y|Î¦âˆ—(ğ›¿âˆ—(G)),ğ‘’2),âˆ€ğ‘’1,ğ‘’2âˆˆE.(2)
To avoid trivial representations, this constraint is integrated as
a regularization term in the training objective. Similar to Invariant
Risk Minimization (IRM) [ 3], setğœ”as a constant scalar multiplier of
1.0 for each output dimension. The objective function for molecule
invariant learning can be written as follows
min
Î¦,ğ›¿âˆ‘ï¸
ğ‘’âˆˆEğ‘¡ğ‘ŸRğ‘’(ğ‘“â—¦ğ›¿)+ğœ†âˆ‡ğœ”Rğ‘’(ğœ”â—¦Î¦â—¦ğ›¿). (3)
3189Advancing Molecule Invariant Representation via
Privileged Substructure Identification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Obviously, Eq. (3) necessitates predefined environments, the ac-
quisition of which poses a practical challenge. Furthermore, the
availability of environment labels does not imply their suitability for
molecule invariant learning, leading to no guarantee of benefit [ 22].
3 CRITERIA FOR ENVIRONMENT SPLIT IN
MOLECULE INVARIANT LEARNING
With the above formulation, we aim to derive criteria for environ-
ment split that facilitate molecule invariant learning. Intuitively,
these environments should shed light on the variations of variant
features [ 32,48]. Therefore, if the environment split is based solely
on the variant structure, it allows for an exposure of any variance.
Here, we use identifier Â¯ğ›¿(G)=Gğ‘£to represent the complement of
the privileged substructure identification.
Suppose an alternative environment predictor ğ‘“ğ‘’â—¦Â¯ğ›¿predicts the
label using the variant structure Gğ‘£. Contrasting with the molecule
invariant representation that adheres to SIC, the results from the
environment predictor ğ‘“ğ‘’(Â¯ğ›¿(G))intentionally violate SIC.
Theorem 1. For the optimal environment predictor ğ‘“ğ‘’âˆ—(Â¯ğ›¿âˆ—(G))
that relies solely on the variant structure, denote the prediction as Ë†Yğ‘’
and the ground truth as Y. If the environments are split by
e=I(Ë†Yğ‘’=Y), (4)
where the function Idetermines the equality of two random variables,
the substructure invariance constraint will be maximally violated.
Unfortunately, this ideal scenario requires that the environment
predictorğ‘“ğ‘’â—¦Â¯ğ›¿exclusively utilizes the variant structure, a require-
ment complicated by no prior knowledge of its accurate extraction.
In practical implementation, the learned environment split should
maximally violate SIC to the fullest degree.
Criterion 1. The environments eare split according to the agree-
ment between the ground truth Yand the predictions Ë†Yğ‘’from a
learnable environment predictor ğ‘“ğ‘’(Â¯ğ›¿(G)), i.e.,e=I(Ë†Yğ‘’=Y). This
environment split should be optimized to violate the substructure
invariant constraint maximally.
Furthermore, if the environment split eis determined by the
optimal environment predictor ğ‘“ğ‘’âˆ—â—¦Â¯ğ›¿âˆ—and the ground truth Y, we
have molecule invariant representation Î¦âˆ—(ğ›¿âˆ—(G))âŠ¥e|Y. Thus, a
theorem can be deduced as follows.
Theorem 2. For the environment split edetermined by the optimal
environment predictor ğ‘“ğ‘’âˆ—â—¦Â¯ğ›¿âˆ—and the ground truth Y, the following
equation
ğ‘ƒ(Y=ğ‘¦1|ğ‘’1)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’1)=ğ‘ƒ(Y=ğ‘¦1|ğ‘’2)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’2)(5)
holds for any ğ‘¦1,ğ‘¦2âˆˆY and anyğ‘’1,ğ‘’2âˆˆE.
This theorem establishes the relationship between the environ-
ment split and class distribution, indicating that the class distribu-
tion is fair to the environment split. Intending to reach the optimal
scenario, we introduce the second optimization criterion for the
environment split.
Criterion 2. The environments eare split based on the ground
truth Yand the predictions Ë†Yğ‘’from a learnable environment predictor
ğ‘“ğ‘’â—¦Â¯ğ›¿. This environment split should be optimized to ensure the fairness
of class distribution across diverse environments.We direct the readers to Appendix A for the proofs of all the
above theorems.
4 MILI METHODOLOGY
Guided by the established criteria, we propose MILI, a molecule
invariant learning model via privileged substructure identification.
In this section, we first present the details of its neural network
architecture. Following this, we illustrate the training procedure.
4.1 Dual-head Graph Neural Network
Revisiting the molecule invariant learning framework proposed in
Eq. (3), the molecular property predictor consists of two parts ğ‘“â—¦ğ›¿.
The identifier ğ›¿is designated to identify privileged substructures,
whileğ‘“predicts molecular properties. In Sect. 3, the environment
split is reliant on the environment predictor ğ‘“ğ‘’â—¦Â¯ğ›¿, where Â¯ğ›¿acts as
the complement of privileged substructure identification, and ğ‘“ğ‘’
predicts the label from the resultant variant structure. This gives rise
to a dual-head graph neural network. Specifically, it utilizes a shared
backbone as the identifier ğ›¿for privileged substructures, with ğ‘“and
ğ‘“ğ‘’serving as the task and environment heads, respectively. The
overall framework is depicted in Fig. 1, and the implementation of
each module is introduced as follows.
Molecule Fragmentation. Initially, we fragmentize the molecule
G, provided in the SMILES format, into a collection of chemical
substructures{Gğ‘
ğ‘–}ğ‘ğ‘ 
ğ‘–=1. This fragmentation is executed by Breaking
Retrosynthetically Interesting Chemical Substructures (BRICS) [ 10],
recognized for effectively isolating essential substructures from
complex molecules.
Privileged Substructure Identifier. We use the representation of
the complete molecule as a query, and substructure representations
as keys, identifying privileged substructures based on the attention
mechanism [ 49]. Specifically, a GIN encoder [ 60] is adopted to learn
the representation Â®â„of the molecule G
Â®â„=GIN(G). (6)
We apply another GIN subencoder to chemical substructures {Gğ‘
ğ‘–}ğ‘ğ‘ 
ğ‘–=1
to obtain their representations {Â®â„ğ‘
ğ‘–}ğ‘ğ‘ 
ğ‘–=1,
Â®â„ğ‘
ğ‘–=GIN(Gğ‘
ğ‘–). (7)
Treating the molecule representation Â®â„as a query and substructure
representations{Â®â„ğ‘
ğ‘–}ğ‘ğ‘ 
ğ‘–=1as keys, the attention coefficients Â®ğ›¼can be
calculated as
ğ›¼ğ‘–=Â®â„ğ‘Šğ‘(Â®â„ğ‘
ğ‘–ğ‘Šğ‘˜)âŠ¤
âˆš
ğ‘‘, (8)
where matrices{ğ‘Šğ‘,ğ‘Šğ‘˜}are learnable linear transformations used
to enhance expressive power, and ğ‘‘is the representation dimension.
Considering that the task head ğ‘“requires privileged substructures
{Gğ‘}as input, we can directly feed the representation Â®â„ğ‘of them
Â®â„ğ‘=softmax(Â®ğ›¼)ğ»ğ‘, (9)
whereğ»ğ‘represents a matrix stacked with substructure represen-
tations{Â®â„ğ‘
ğ‘–}ğ‘ğ‘ 
ğ‘–=1. On the other hand, the environment head ğ‘“ğ‘’neces-
sitates the variant structure Â¯ğ›¿(G)- the complement of privileged
3190KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, and Chuan Shi
OriginalMolecule!Fragmentizeâ€¦ChemicalSubstructures{!#}SharedBackbone%(')GINEncoder
GINSubencoderâ€¦AttentionVariant!)
{!*}EnvironmentHead+,(')
TaskHead+(')NoBP
â€¦
â€¦
â€¦â€¦Pred.-./â€¦â€¦Pred.-.PrivilegedEnv.,Inv. Riskâ„â„12âƒ—4âˆ’âƒ—4â„12
âƒ—4â„12querykeykeykey
Figure 1: Overall framework of the proposed MILI. Fragmentized chemical substructures {Gğ‘}are fed into a dual-head graph
neural network. (1) The shared backbone serves as identifier ğ›¿to identify privileged substructures. (2) The task head ğ‘“and
environment head ğ‘“ğ‘’respectively utilize privileged substructures {Gğ‘}and variant structures Gğ‘£for molecular property
prediction. Aligning with environment split criteria, the environment head ğ‘“ğ‘’assigns environments ebased on the agreement
between predictions Ë†Yğ‘’and ground truth Y. Meanwhile, the task head ğ‘“considers class distribution fairness across environments
and calculates invariant risk Rğ‘–ğ‘›ğ‘£to refine the environment head. In each training iteration, the molecular property predictor
ğ‘“â—¦ğ›¿and environment head ğ‘“ğ‘’are optimized with awareness of each other.
substructures. To accomplish this, we utilize reverse attention
Â®â„ğ‘£=softmax(âˆ’Â®ğ›¼)ğ»ğ‘. (10)
Here,Â®â„ğ‘£denotes the representation of the variant structure, serving
as the input for the subsequent environment head.
Dual Heads. The task and environment heads employ the repre-
sentations of privileged substructures and the variant structure for
classification, respectively. Particularly, the task head ğ‘“utilizes a
multi-layer perceptron (MLP) to generate predictions Ë†Y,
Ë†Y=softmax(MLP(Â®â„ğ‘)). (11)
Likewise, the environment head ğ‘“ğ‘’obtains both a soft prediction Â®ğ‘ 
and the final prediction Ë†Yğ‘’,
Â®ğ‘ =softmax(MLP(Â®â„ğ‘£)), (12)
Ë†Yğ‘’=argmax(Â®ğ‘ ). (13)
According to Theorem 1, the principle of splitting environment eis
the agreement between the ground truth Yand the predictions Ë†Yğ‘’.
Therefore,ğ‘’1=I(Y=Ë†Yğ‘’)andğ‘’2=I(Yâ‰ Ë†Yğ‘’).
Optimization Objective. Given the environment split, we can
define per-environment risk Rğ‘’,
Rğ‘’(ğ‘“â—¦ğ›¿)=Eğ‘ƒ(G,Y|ğ‘’)â„“
Ë†Y,Y
, (14)
whereâ„“is the cross-entropy for classification. To fulfill Criterion 1
that the environment split maximally violates the substructure
invariant constraint, we fix the molecular property predictor ğ‘“â—¦ğ›¿and optimize the environment head ğ‘“ğ‘’by maximizing the invariant
risk based on soft assignment
ËœRğ‘’(ğ‘“â—¦ğ›¿,Â®ğ‘ )=1Ã
ğ‘–â€²I(ğ‘’ğ‘–â€²=ğ‘’)âˆ‘ï¸
ğ‘–I(ğ‘’ğ‘–=ğ‘’)Â®ğ‘ ğ‘–[Ë†ğ‘Œğ‘’
ğ‘–]â„“
Ë†ğ‘Œğ‘–,ğ‘Œğ‘–
,(15)
L(ğ‘“ğ‘’)=âˆ’âˆ‡ğœ”ËœRğ‘’(ğœ”â—¦Î¦â—¦ğ›¿,Â®ğ‘ ). (16)
Here,Â®ğ‘ ğ‘–[Ë†ğ‘Œğ‘’
ğ‘–]represents the element corresponding to the dimension
ofË†ğ‘Œğ‘’
ğ‘–. Please note that Eq. (16) does not include empirical risk. To
prevent trivial solutions, we stop the backpropagation between the
environment head ğ‘“ğ‘’and the identifier ğ›¿, refiningğ‘“ğ‘’to concentrate
on variant information.
To ensure the fairness of class distribution stated in Criterion 2,
the per-environment risk is reweighted based on the class propor-
tion within environments
Â¥Rğ‘’(ğ‘“â—¦ğ›¿)=Eğ‘ƒ(G,Y|ğ‘’)ğ‘ƒ(Y=ğ‘¦)
ğ‘ƒ(Y=ğ‘¦|ğ‘’)â„“
Ë†Y=ğ‘¦,Y=ğ‘¦
. (17)
Aligned with Theorem 2, the ideal value of ğ‘ƒ(Y=ğ‘¦)/ğ‘ƒ(Y=ğ‘¦|ğ‘’)is
equal to 1. Practically, it serves to balance class distribution across
environments dynamically. By applying increased penalization to
larger values, the class distribution is refined through enhanced
extraction of privileged substructures. Thus following Eq. (3), the
loss function for the molecular property predictor ğ‘“â—¦ğ›¿is
L(ğ‘“,ğ›¿)=âˆ‘ï¸
ğ‘’âˆˆEğ‘¡ğ‘ŸÂ¥Rğ‘’(ğ‘“â—¦ğ›¿)+ğœ†âˆ‡ğœ”Rğ‘’(ğœ”â—¦Î¦â—¦ğ›¿), (18)
3191Advancing Molecule Invariant Representation via
Privileged Substructure Identification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Quantitative OOD generalization performance measured by ROC-AUC (% Â±ğœ). The best is marked with boldface and the
second best is with underline . (em dash: cannot run without environment labels)
Metho
dsOGB IC50 EC50
BA
CE BBBP Assay Scaffold Size Assay Scaffold Size
ERM [41]
78.10Â±1.30 68.77Â±0.85 70.87Â±0.99 68.96Â±0.26 68.03Â±1.96 67.87Â±2.22
66.03Â±1.18 62.49Â±1.17
IRM [3] â€” â€” 71.14Â±0.85 65.56Â±0.47 57.74Â±0.73 69.23Â±1.63 61.38Â±0.53 56.84Â±2.11
GroupDRO [40] â€” â€” 69.65Â±0.67 67.67Â±0.86 57.93Â±1.27 71.07Â±4.24 65.67Â±1.94 60.82Â±1.21
Mixup [66] â€” â€” 71.75Â±1.24 68.96Â±0.62
66.98Â±0.38 68.70Â±1.47 66.48Â±1.73 63.26Â±0.51
DIR [57]
76.49Â±2.80 66.52Â±1.33 67.16Â±2.00 64.45Â±1.39 59.03Â±1.67 67.07Â±2.22 63.14Â±1.64 59.64Â±1.20
GREA [31] 81.66Â±0.98 70.76Â±1.39 71.27Â±1.04
67.96Â±0.62 67.10Â±1.08 73.01Â±1.09 64.64Â±1.36
61.42Â±1.11
GSAT [35] 75.35Â±1.80 68.38Â±0.64 70.04Â±1.15 67.78Â±0.45 66.37Â±0.48 71.73Â±1.76 65.19Â±0.93 60.22Â±1.69
CAL [46] 77.29Â±1.60 68.33Â±1.27 69.42Â±1.64 64.64Â±0.80 64.44Â±1.51 70.54Â±2.30 64.96Â±0.83 60.56Â±1.26
CIGA [7] 76.29Â±2.50 68.06Â±1.37 70.80Â±1.13 68.37Â±1.88 66.25Â±0.47 69.37Â±1.81 67.53Â±1.07 65.54Â±0.67
MoleOOD [62]
77.61Â±4.90 64.77Â±2.44 71.60Â±1.00 67.68Â±1.12 66.47Â±1.67 70.77Â±1.93 65.71Â±1.45 64.21Â±1.02
iMoLD [70] 79.11Â±0.90 68.50Â±1.33 70.74Â±1.21 69.22Â±1.65 67.01Â±1.37
71.38Â±1.54 66.50Â±0.73 65.22Â±1.25
MILI
85.16Â±1.65 72.56Â±0.65 72.67Â±0.52 69.58Â±1.01 68.40Â±0.57 77.11Â±1.37 68.07Â±1.27 65.97Â±0.96
Algorithm 1 Model Training for MILI
Input: Datasetğ·={(ğºğ‘–,ğ‘Œğ‘–)}ğ‘
ğ‘–=1; Number of training epochs for
environment head ğ‘‡1; Number of training epochs for molecule
property predictor ğ‘‡2; Number of training iterations ğ‘‡
Output: Trained molecule property predictor ğ‘“â—¦ğ›¿
1:Pretrain the molecule property predictor ğ‘“â—¦ğ›¿using the cross-
entropy empirical risk;
2:Initialize the environment head ğ‘“ğ‘’â†ğ‘“;
3:forğ‘–â†1toğ‘‡do
4: Fix the molecule property predictor ğ‘“â—¦ğ›¿;
5: Obtain the prediction Ë†Y;
6:forğ‘—â†1toğ‘‡1do
7: Compute the soft prediction Â®ğ‘ and the final prediction Ë†Yğ‘’
of environment head ğ‘“ğ‘’according to Eqs. (12) and (13);
8: Compute the environment split e=I(Ë†Yğ‘’=Y);
9: Optimize the environment head ğ‘“ğ‘’according to Eq. (16);
10: end for
11: Fix the environment head ğ‘“ğ‘’;
12: Obtain the prediction Ë†Yğ‘’of environment head ğ‘“ğ‘’;
13: Obtain the environment split e=I(Ë†Yğ‘’=Y);
14: forğ‘—â†1toğ‘‡2do
15: Compute the prediction Ë†Y;
16: Compute the reweighted risk according to Eq. (17);
17: Optimize the molecule property predictor ğ‘“â—¦ğ›¿according
to Eq. (18);
18: end for
19:end for
20:Output the molecule property predictor ğ‘“â—¦ğ›¿;
where the first term represents the reweighted empirical risk and
the second term is the invariant risk, with ğœ†denoting the trade-off
hyperparameter.4.2 Training Procedure
We adopt an iterative training strategy between the molecule prop-
erty predictor ğ‘“â—¦ğ›¿and the environment head ğ‘“ğ‘’, allowing mutual
benefit and enhancement. Alg. 1 presents the pseudocode.
The computational complexity of MILI primarily stems from
iterative optimization. Each iteration mainly involves the updating
of the environment head ğ‘“ğ‘’and the molecule property predictor
ğ‘“â—¦ğ›¿. The complexity for the environment head ğ‘“ğ‘’isO(ğ‘‡1ğ‘‘2),
whileO(ğ‘‡2(|E|ğ·ğ‘‘+ğ‘‘2))is for molecule property predictor ğ‘“â—¦ğ›¿.
Here,|E|represents the number of edges in the molecule graph,
andğ·denotes the feature dimension. Consequently, the overall
complexity of ğ‘‡-iteration MILI is around ğ‘‡(ğ‘‡1ğ‘‘2+ğ‘‡2|E|ğ·ğ‘‘). Please
note that the values of {ğ‘‡1,ğ‘‡2}are significantly smaller than those
in traditional Empirical Risk Minimization (ERM) training. More-
over, the number of iterations ğ‘‡is relatively modest. Therefore, the
added computational overhead remains acceptable.
5 EXPERIMENTS
In this section, we assess the effectiveness of MILI via extensive ex-
periments. Firstly, we compare MILI with state-of-the-art methods
in OOD generalization for molecule representation learning. Fol-
lowing this, we present that the identified privileged substructures
have substantial chemical significance. Subsequently, we analyze
the mechanisms of MILI, validating the contributions of its modules.
Lastly, we investigate the hyper-parameter sensitivity.
5.1 Experimental Setup
Datasets. We evaluated the proposed MILI on eight benchmark
datasets. BACE and BBBP, from Open Graph Benchmark (OGB) [ 20],
focus on binding affinity against human beta-secretase 1 and brain-
blood barrier penetration, respectively. Their train-validation-test
splits are determined by scaffold differences. The other six datasets
are provided by DrugOOD [ 22], offering binary classification for
3192KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, and Chuan Shi
Figure 2: Visualization of three test cases from the BBBP dataset. The identified privileged substructures are highlighted using
a color-coded scheme based on their learned weights. Functional group nodes are distinctly marked with the most significant
group in red, followed by the second in blue, and the third in purple.
drug target binding affinity prediction. DrugOOD utilizes three
split strategies (assay, scaffold, size) across IC50 and EC50 measure-
ments, thus creating six datasets IC50/EC50-assay/scaffold/size. For
all datasets, we adhere to the standard train-validation-test split. It
is important to note that only the six DrugOOD datasets include
manually specified environment labels. As the above property pre-
diction tasks all relate to classification, we report the ROC-AUC
score, consistent with previous studies [ 51,62,70]. The statistics of
these datasets are summarized in Appendix B.
Baselines. We thoroughly compare our MILI against ERM [ 41]
and three categories of OOD baselines. Specifically, (1) three general
OOD methods for Euclidean data, including the invariant learn-
ing method IRM [ 3], the group distributionally robust optimiza-
tion method GroupDRO [ 40], and the data augmentation method
Mixup [ 66]. Notably, these methods necessitate manual specifica-
tion of environments, we limited this comparison to DrugOOD
datasets. (2) Two interpretable graph learning methods, DIR [ 57]
and GREA [ 31]. (3) Five graph OOD methods, namely GSAT [ 35],
CAL [ 46], CIGA [ 7], MoleOOD [ 62], and iMoLD [ 70]. All meth-
ods use GIN [ 60] backbones and are configured using parameters
reported in the original papers or selected via grid search.
Implementation. We implement MILI using the PyTorch deep
learning library1. For the encoder and subencoder in the identi-
fier, we adopt GIN implementations from the Open Graph Bench-
mark [ 20]. As outlined in Alg. 1, during the pretraining of the
molecule property predictor using cross-entropy empirical risk, we
save the model parameters that exhibit the largest performance
gap between the training and validation sets. For hyper-parameter
tuning, we employ grid search on the validation set, adjusting the
learning rate from {1ğ‘’âˆ’2,5ğ‘’âˆ’3,1ğ‘’âˆ’3,5ğ‘’âˆ’4,1ğ‘’âˆ’4,5ğ‘’âˆ’5,1ğ‘’âˆ’
5}, the number of GIN layers from 2 to 6, the dropout rate from
{0.1,0.3,0.5,0.7}, and the trade-off parameter ğœ†in Eq. (18) from
{0.1,1,10,50,100,150,200,250}. The Adam optimizer [ 23] is used
for efficient gradient-based optimization.
1https://pytorch.org/5.2 Main Results
Performance Comparison. In Table 1, we report the mean and
standard deviation results over 5 independent trials with different
random seeds.
From these results, we draw several conclusions: (1) The pro-
posed MILI consistently outperforms all baselines on the datasets,
demonstrating that our unified molecule invariant learning frame-
work substantially enhances the generalizability of molecule rep-
resentation learning. (2) The performance advantage of MILI over
MoleOOD can be attributed to the environment split criteria and
the inspired dual-head graph neural network architecture, which
ensures that the environment split and molecule invariant learning
reinforce each other. (3) The improvements of OOD baselines over
ERM highlight the importance of considering the OOD scenario
in molecule representation learning. Without specialized design,
neural networks are prone to adopting spurious correlations. Fur-
thermore, the superior performance of graph OOD baselines relative
to those for Euclidean data underscores the inherent suitability of
graph-based learning for capturing intricate patterns in molecules.
(4) The distribution shifts of IC50/EC50-scaffold/size datasets have
a relatively small impact on ERM models, resulting in limited ad-
vantages for OOD methods on these datasets. A large portion of
OOD methods fail to surpass ERM, whereas the proposed MILI still
achieves the best performance in this scenario.
Privileged Substructure Identification. To enhance the understand-
ing of MILI, we detail three cases of identified privileged substruc-
tures on the BBBP dataset, as shown in Fig. 2. These substructures
are ranked by the learned weights, highlighted in red, blue, and
purple, respectively.
Focusing on the brain-blood barrier (BBB) penetration, the BBBP
dataset classifies molecules based on their ability to permeate the
brain cell membrane. Notably, the most significant functional group
ğ‘1ğ¶ğ¶ğ‘(ğ¶ğ¶1)(piperazine), marked in red, is pivotal in aiding
molecular penetration through the BBB. Piperazineâ€™s presence en-
hances BBB crossing, beneficial for central nervous system en-
try [45]. Additionally, the blue-highlighted ğ¶ğ¶(=ğ‘‚)ğ‘(acetamide)
moderately supports BBB penetration, leveraging its amide group
3193Advancing Molecule Invariant Representation via
Privileged Substructure Identification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 15 30 45 60
Epoch0.20.40.60.81.0Invariant Risk1e2
(a) Loss changes of environment head
0.0 0.2 0.4 0.6 0.8 1.0
Invariant Risk1e2
74.076.078.080.082.084.0ROC-AUC (%)
 (b) Impact of invariant risk on model
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018/uni00000014/uni00000011/uni00000019/uni00000014/uni00000011/uni0000001a/uni00000014/uni00000011/uni0000001b/uni00000014/uni00000011/uni0000001c/uni00000015/uni00000011/uni00000013/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000048/uni00000014
/uni00000048/uni00000015
 (c) Changes of class ratio
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000035/uni00000048/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000003/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000035/uni0000004c/uni00000056/uni0000004e/uni00000048/uni00000014
/uni00000048/uni00000015
 (d) Changes of empirical risk
Figure 3: Optimization analysis on BACE dataset.
/uni00000030/uni0000002c/uni0000002f/uni0000002c /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000028/uni00000032 /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000035/uni0000003a /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002c/uni00000032/uni0000001b/uni00000015/uni00000011/uni00000013/uni0000001b/uni00000016/uni00000011/uni00000013/uni0000001b/uni00000017/uni00000011/uni00000013/uni0000001b/uni00000018/uni00000011/uni00000013/uni0000001b/uni00000019/uni00000011/uni00000013/uni0000001b/uni0000001a/uni00000011/uni00000013/uni0000001b/uni0000001b/uni00000011/uni00000013/uni0000001b/uni0000001c/uni00000011/uni00000013/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000001b/uni0000001a/uni00000011/uni0000001c
/uni0000001b/uni00000017/uni00000011/uni00000017/uni00000015/uni0000001b/uni00000017/uni00000011/uni00000014/uni0000001c
/uni0000001b/uni00000016/uni00000011/uni00000018/uni00000018/uni00000025/uni00000024/uni00000026/uni00000028
(a) BACE
/uni00000030/uni0000002c/uni0000002f/uni0000002c /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000028/uni00000032 /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000035/uni0000003a /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002c/uni00000032/uni0000001a/uni00000014/uni00000011/uni00000013/uni0000001a/uni00000014/uni00000011/uni00000018/uni0000001a/uni00000015/uni00000011/uni00000013/uni0000001a/uni00000015/uni00000011/uni00000018/uni0000001a/uni00000016/uni00000011/uni00000013/uni0000001a/uni00000016/uni00000011/uni00000018/uni0000001a/uni00000017/uni00000011/uni00000013/uni0000001a/uni00000017/uni00000011/uni00000018/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000001a/uni00000016/uni00000011/uni00000018/uni00000015
/uni0000001a/uni00000015/uni00000011/uni00000015/uni00000014/uni0000001a/uni00000015/uni00000011/uni0000001b/uni0000001a
/uni0000001a/uni00000015/uni00000011/uni00000016/uni00000014/uni00000025/uni00000025/uni00000025/uni00000033 (b) BBBP
Figure 4: Impact of environment optimization, empirical risk
reweighting, and iterative optimization on (a) BACE and (b)
BBBP datasets.
for hydrogen bonding and polarity. Despite this, piperazine struc-
tures outperform acetamide in BBB penetration due to their nitrogen-
rich ring structure, which receives higher importance in the ranking.
These findings underscore the proficiency of MILI in identifying
and analyzing substructures crucial for drug efficacy and safety.
Meanwhile, its capability to adapt to distribution shifts ensures that
the designed drugs are robust and reliable, making it a useful tool
in modern pharmaceutical research and development.
5.3 Model Analysis
Ablation Studies. Recall that MILI optimizes the environment
head by maximizing invariant risk to satisfy Criterion 1, addresses
Criterion 2 by reweighting empirical risks according to class pro-
portions, and establishes mutual reinforcement between molecule
invariant learning and environment split via iterative optimization.
To ascertain the effectiveness of these modules, we conduct ablation
studies. Specifically, we compare MILI with three variants: w/o EO
(MILI without environment optimization), w/o RW (with empirical
risk reweighting), and w/o IO (without iterative optimization). The
results on the BACE and BBBP datasets are depicted in Fig. 4.
From the plots, we observe that MILI consistently outperforms
the other variants. Such a phenomenon is not surprising and under-
scores that the integration of environment optimization, empirical
risk reweighting, and iterative optimization is critical for the robust
performance of MILI. Each module contributes to the effectiveness
of the overall framework.
Optimization Analysis. To elucidate the learning process of MILI,
We present invariant risk changes of the environment head and their
impact on model performance, the class ratio and the reweightedempirical risk changes within environments on the BACE dataset,
as illustrated in Fig. 3.
Fig. 3 (a) reveals an upward trend in the invariant risk as the
optimization progresses, which aligns with the optimization objec-
tive in Eq. (16). Here, the environment head maximizes invariant
risk to ensure that the environment split maximally violates SIC
and exploits variant information to the utmost. Furthermore, it can
be observed that the loss gradually converges, which is consistent
with theoretical findings in related work [ 3,39] demonstrating the
existence of an upper bound on invariant risk. To investigate how
the loss of the environment head impacts the modelâ€™s effectiveness,
we vary the number of training epochs for the environment head
on the BACE dataset while setting the iteration number to one. We
plot the invariant risk and ROC-AUC as a scatter plot in Fig. 3 (b).
It shows that the invariant risk and the final model performance
are generally positively correlated, indicating that optimizing the
environment head more effectively benefits downstream molecule
invariant learning.
According to Theorem 2, class ratios between different environ-
ments should ideally equalize. Fig. 3 (c) shows that initial disparities
in class ratios ğ‘ƒ(ğ‘¦1|e)/ğ‘ƒ(ğ‘¦2|e)between environments ğ‘’1andğ‘’2
gradually narrow. This indicates that the reweighting in Eq. (17) ef-
fectively refines the shared identifier, moving the class ratios closer
to the ideal state. Fig. 3 (d) depicts a consistent decline in empirical
risks across different environments during optimization, suggesting
that the proposed molecule invariant learning framework indeed
facilitates downstream tasks.
5.4 Hyper-parameter Sensitivity
In this subsection, we investigate the hyper-parameter sensitivity
of MILI, focusing on GIN layers, the number of iterations, and the
trade-off hyperparameter ğœ†. Specifically, we adjust the number of
GIN layers in the encoder ( ğ‘™1) and subencoder ( ğ‘™2) from 2 to 6. As for
the number of iterations, the value ranges from 1 to 6. The trade-off
hyperparameter ğœ†is explored in{0.1,1,10,50,100,150,200,250}.
We report the results on the BACE dataset in Fig. 5.
Effect of GIN Layers. As observed in Fig. 5 (a), both the encoder
and subencoder exhibit relatively poor performance when the num-
ber of GIN layers is too low or too high. We infer that fewer layers
may limit the representation capacity to encapsulate the intricate
patterns in molecule structures, whereas too many layers risk over-
parameterization.
3194KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, and Chuan Shi
(a) GIN layers
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000001b/uni00000013/uni0000001b/uni00000014/uni0000001b/uni00000015/uni0000001b/uni00000016/uni0000001b/uni00000017/uni0000001b/uni00000018/uni0000001b/uni00000019/uni0000001b/uni0000001a/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
 (b) Number of iterations
/uni00000013/uni00000011/uni00000014 /uni00000014 /uni00000014/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
 (c) Trade-off hyperparameter ğœ†
Figure 5: The hyper-parameter sensitivity on BACE dataset.
Effect of Iterations. Improvements in performance with additional
iterations indicate that MILI benefits from iterative optimization.
Each iteration might enable environment split and molecule invari-
ant learning to promote each other. Nevertheless, more iterations
escalate time complexity. Thus, we should balance the trade-off
between performance and complexity.
Effect of Trade-off Hyperparameter. Considering the relatively
small value of invariant risk, too small ğœ†causes the optimization to
neglect the associated penalty for invariant risk. As depicted in Fig. 5
(c), this may lead the model to learn more spurious correlations,
subsequently degrading its performance on the OOD test set. On
the other hand, setting ğœ†too high may result in the inadequate
optimization of empirical risk, affecting its predictive performance.
6 RELATED WORK
In line with the focus of our work, we briefly review the most related
work on molecule representation learning and OOD generalization.
6.1 Molecule Representation Learning
Existing molecule representation learning methods can be clas-
sified into three categories. The first is fingerprint-based meth-
ods [ 4,13,21], which utilize handcrafted representations [ 12,37]
to encode topological substructures. While effectively capturing
substructural presence, these methods suffer from bit collisions and
vector sparsity, limiting their representation capacity. The second
is sequence-based methods [ 18,21] that leverage SMILES (Simpli-
fied Molecular Input Line Entry System) [ 55] strings to represent
molecules. These methods employ sequence-based models such as
recurrent neural networks [ 65] and Transformer [ 49] to learn mole-
cule representations. However, their main challenge lies in compre-
hending SMILES syntax. For example, spatially distant atoms may
appear adjacent in the sequence. The final category is graph-based
methods [ 38,43,59], which model molecules by treating each atom
as a node and each chemical bond as an edge. Many works have
showcased the profound potential of graph neural networks [ 17,44]
in analyzing and predicting molecular behavior, significantly ad-
vancing the field of molecule representation learning.Despite their remarkable achievements, these methods predom-
inantly assume that training and testing molecules are indepen-
dently sampled from an identical environment. However, this as-
sumption often falls short in real-world scenarios, underscoring
the urgency for OOD generalization.
6.2 OOD Generalization
The vulnerability of deep neural networks to significant perfor-
mance drops under distribution shifts has spurred extensive re-
search in OOD generalization [ 42]. Three lines of methods have
emerged for OOD generalization in Euclidean data: group distribu-
tionally robust optimization [ 27,67], domain adaptation [ 15,30],
and invariant learning [ 1,3,6]. Group distributionally robust op-
timization considers groups of distributions and optimizes across
all groups. Domain adaptation [ 11,34,47] strives to align data dis-
tributions with some additional assumptions [ 68]. Invariant learn-
ing [ 5,8] seeks an invariant predictor that upholds invariant rela-
tionships across all environments. It does this by learning invariant
representations that meet the invariant principle: sufficiency for
predictive accuracy and invariance to environmental changes.
However, most existing methods require explicitly multiple envi-
ronments within the training dataset. This requirement for detailed
annotation is not only exceedingly expensive for non-Euclidean
data [ 33,39] but also inherently problematic due to potential inac-
curacies in the predefined split. Furthermore, some studies have
indicated that the direct application of these methods to complex
molecule graphs [ 7] frequently fails to yield promising results [ 22].
6.3 OOD Generalization on Graphs
Recently, there has been a surge of interest regarding OOD gener-
alization on non-Euclidean graphs. Some methods [ 7,14,28,46]
adopt the "first-separation-then-encoding" paradigm to identify
invariant substructures in the explicit structural space. Moreover,
MoleOOD [ 62] and GIL [ 29] utilize inferred environmental labels
to learn invariant representations based on the invariant principle;
GREA [ 31] and iMoLD [ 70] learn disentangled invariant repre-
sentations within the latent space. OOD generalization on graphs
can also be enhanced by related works [ 35,57] in the explainabil-
ity [63,64] of graph neural networks (GNNs) [ 16,19,24,50,56],
3195Advancing Molecule Invariant Representation via
Privileged Substructure Identification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
which aim to pinpoint a subgraph as the rationale behind a GNN
prediction. Although some methods incorporate causality to justify
the generated explanations, their primary focus remains on the
explainability of GNN predictions rather than OOD generalization.
Methods learning invariant representations in the latent space
lack interpretability, while those in the explicit structural space
usually use arbitrary subgraphs as basic units. In molecule repre-
sentation learning, the properties of molecules are frequently deter-
mined by chemical substructures [ 26,36]. Injecting this prior knowl-
edge is crucial for identifying invariant substructures in molecules
and providing new insights to experts. Notably, the most relevant
work [ 62] incorporates this knowledge, whose core idea of envi-
ronment split is to use variational inference to approximate the
posteriorğ‘ğœ(ğ‘’|ğº,ğ‘¦). Specifically, two GINs are employed to model
ğ‘ğœ…(ğ‘’|ğº,ğ‘¦)andğ‘ğœ(ğ‘¦|ğº,ğ‘’), and environment split is achieved by
maximizing the ELBO. The separation between environment split
and molecule representation learning as independent models leads
to a lack of awareness that cannot guarantee mutual benefits.
7 CONCLUSION
In this paper, we formalize molecule invariant learning based on
privileged substructure identification and propose substructure in-
variance constraint. On this foundation, we theoretically derive
criteria for environment split and implement them through a dual-
head graph neural network. Therefore, our framework ensures
mutual enhancement between environment split and molecule in-
variant learning from theoretical and network design perspectives.
Limitations and Broader Impact. One limitation of MILI is its
current focus on classification tasks, presenting an opportunity for
future work to broaden its application across more diverse down-
stream tasks. Our work delves into the OOD problems in molecule
representation learning, a prevalent and inevitable scenario in real-
world applications. Applying machine learning to molecules still
faces numerous practical challenges, such as accurately predict-
ing chemical reactivity. We expect our work will inspire further
research combining domain knowledge with machine learning tech-
niques, contributing to the realization of AI4Science.
ACKNOWLEDGMENTS
This work is partly supported by the National Natural Science
Foundation of China (No. U20B2045, 61772082, 62192784, 62172052,
U1936104) and Young Elite Scientists Sponsorship Program (No.
2023QNRC001) by CAST.
REFERENCES
[1]Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet,
Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. 2021. Invariance principle
meets information bottleneck for out-of-distribution generalization. In NeurIPS,
Vol. 34. 3438â€“3450.
[2] Bruce Alberts. 2017. Molecular biology of the cell. Garland science.
[3]Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[4]AdriÃ  Cereto-MassaguÃ©, MarÃ­a JosÃ© Ojeda, Cristina Valls, Miquel Mulero, Santiago
Garcia-VallvÃ©, and Gerard Pujadas. 2015. Molecular fingerprint similarity search
in virtual screening. Methods 71 (2015), 58â€“63.
[5]Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. 2020. Invariant rational-
ization. In ICML. 1448â€“1458.
[6]Yimeng Chen, Ruibin Xiong, Zhi-Ming Ma, and Yanyan Lan. 2022. When Does
Group Invariant Learning Survive Spurious Correlations?. In NeurIPS, Vol. 35.
7038â€“7051.[7]Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui
Xie, Tongliang Liu, Bo Han, and James Cheng. 2022. Learning causally invariant
representations for out-of-distribution generalization on graphs. In NeurIPS,
Vol. 35. 22131â€“22148.
[8]Elliot Creager, JÃ¶rn-Henrik Jacobsen, and Richard Zemel. 2021. Environment
inference for invariant learning. In ICML. 2189â€“2200.
[9]Laurianne David, Amol Thakkar, RocÃ­o Mercado, and Ola Engkvist. 2020. Molec-
ular representations in AI-driven drug discovery: a review and practical guide.
Journal of Cheminformatics 12, 1 (2020), 1â€“22.
[10] JÃ¶rg Degen, Christof Wegscheid-Gerlach, Andrea Zaliani, and Matthias Rarey.
2008. On the Art of Compiling and Usingâ€™Drug-Likeâ€™Chemical Fragment Spaces.
ChemMedChem: Chemistry Enabling Drug Discovery 3, 10 (2008), 1503â€“1507.
[11] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker.
2019. Domain generalization via model-agnostic learning of semantic features.
InNeurIPS, Vol. 32.
[12] Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. 2002.
Reoptimization of MDL keys for use in drug discovery. Journal of chemical
information and computer sciences 42, 6 (2002), 1273â€“1280.
[13] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell,
Timothy Hirzel, AlÃ¡n Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional
networks on graphs for learning molecular fingerprints. In NeurIPS, Vol. 28.
[14] Shaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. 2022. Debiasing
graph neural networks via learning disentangled causal substructure. In NeurIPS,
Vol. 35. 24934â€“24946.
[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, FranÃ§ois Laviolette, Mario March, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. Journal of machine learning
research 17, 59 (2016), 1â€“35.
[16] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2018.
Predict then Propagate: Graph Neural Networks meet Personalized PageRank. In
ICLR.
[17] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In ICML. 1263â€“1272.
[18] Garrett B Goh, Nathan O Hodas, Charles Siegel, and Abhinav Vishnu. 2017.
Smiles2vec: An interpretable general-purpose deep neural network for predicting
chemical properties. arXiv preprint arXiv:1712.02034 (2017).
[19] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In NeurIPS, Vol. 30.
[20] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets
for machine learning on graphs. In NeurIPS, Vol. 33. 22118â€“22133.
[21] Kexin Huang, Tianfan Fu, Lucas M Glass, Marinka Zitnik, Cao Xiao, and Jimeng
Sun. 2020. DeepPurpose: a deep learning library for drugâ€“target interaction
prediction. Bioinformatics 36, 22-23 (2020), 5545â€“5547.
[22] Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Lanqing Li, Long-Kai Huang,
Tingyang Xu, Yu Rong, Jie Ren, Ding Xue, et al .2023. Drugood: Out-of-
distribution dataset curator and benchmark for ai-aided drug discoveryâ€“a focus
on affinity prediction problems with noise annotations. In AAAI, Vol. 37. 8023â€“
8031.
[23] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[24] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[25] Douglas B Kitchen, HÃ©lÃ¨ne Decornez, John R Furr, and JÃ¼rgen Bajorath. 2004.
Docking and scoring in virtual screening for drug discovery: methods and appli-
cations. Nature reviews Drug discovery 3, 11 (2004), 935â€“949.
[26] Justin Klekota and Frederick P Roth. 2008. Chemical substructures that enrich
for biological activity. Bioinformatics 24, 21 (2008), 2518â€“2525.
[27] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021. Out-of-
distribution generalization via risk extrapolation (rex). In ICML. 5815â€“5826.
[28] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022. Ood-gnn: Out-of-
distribution generalized graph neural network. IEEE Transactions on Knowledge
and Data Engineering (2022).
[29] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. 2022. Learning invariant
graph representations for out-of-distribution generalization. In NeurIPS, Vol. 35.
11828â€“11841.
[30] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang,
and Dacheng Tao. 2018. Deep domain generalization via conditional invariant
adversarial networks. In ECCV. 624â€“639.
[31] Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. 2022. Graph
rationalization with environment-based augmentations. In KDD. 1069â€“1078.
[32] Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. 2021. Heterogeneous
risk minimization. In ICML. 6804â€“6814.
[33] Divyat Mahajan, Shruti Tople, and Amit Sharma. 2021. Domain generalization
using causal matching. In ICML. 7313â€“7324.
[34] Toshihiko Matsuura and Tatsuya Harada. 2020. Domain generalization using a
mixture of multiple latent domains. In AAAI, Vol. 34. 11749â€“11756.
3196KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, and Chuan Shi
[35] Siqi Miao, Mia Liu, and Pan Li. 2022. Interpretable and generalizable graph
learning via stochastic attention mechanism. In ICML. 15524â€“15543.
[36] Chuleeporn Phanus-Umporn, Watshara Shoombuatong, Veda Prachayasittikul,
Nuttapat Anuwongcharoen, and Chanin Nantasenamat. 2018. Privileged sub-
structures for anti-sickling activity via cheminformatic analysis. RSC advances 8,
11 (2018), 5920â€“5935.
[37] David Rogers and Mathew Hahn. 2010. Extended-connectivity fingerprints.
Journal of chemical information and modeling 50, 5 (2010), 742â€“754.
[38] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang,
and Junzhou Huang. 2020. Self-supervised graph transformer on large-scale
molecular data. In NeurIPS, Vol. 33. 12559â€“12571.
[39] Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. 2020. The Risks
of Invariant Risk Minimization. In ICLR.
[40] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. 2019.
Distributionally Robust Neural Networks. In ICLR.
[41] Stephan R Sain. 1996. The nature of statistical learning theory.
[42] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and
Peng Cui. 2021. Towards out-of-distribution generalization: A survey. arXiv
preprint arXiv:2108.13624 (2021).
[43] Hiroyuki Shindo and Yuji Matsumoto. 2019. Gated graph recursive neural net-
works for molecular property prediction. arXiv preprint arXiv:1909.00259 (2019).
[44] Zeren Shui and George Karypis. 2020. Heterogeneous molecular graph neural
networks for predicting molecule properties. In ICDM. 492â€“500.
[45] Manvi Singh, Reshmi Divakaran, Leela Sarath Kumar Konda, and Rajendra Kris-
tam. 2020. A classification model for blood brain barrier penetration. Journal of
Molecular Graphics and Modelling 96 (2020), 107516.
[46] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua.
2022. Causal attention for interpretable and generalizable graph classification. In
KDD. 1696â€“1705.
[47] Baochen Sun and Kate Saenko. 2016. Deep coral: Correlation alignment for deep
domain adaptation. In ECCV Workshops. 443â€“450.
[48] Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. 2021. Unshuffling
data for improved generalization in visual question answering. In ICCV. 1417â€“
1427.
[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS, Vol. 30.
[50] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.
[51] Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han,
and Martin D Burke. 2021. Chemical-Reaction-Aware Molecule Representation
Learning. In ICLR.
[52] Ruijia Wang, Shuai Mou, Xiao Wang, Wanpeng Xiao, Qi Ju, Chuan Shi, and Xing
Xie. 2021. Graph structure estimation neural networks. In WWW. 342â€“353.
[53] Ruijia Wang, Yiwu Sun, Yujie Luo, Shaochuan Li, Cheng Yang, Xingyi Cheng,
Hui Li, Chuan Shi, and Le Song. 2024. Injecting Multimodal Information into
Rigid Protein Docking via Bi-level Optimization. NeurIPS 36.[54] Xiao Wang, Ruijia Wang, Chuan Shi, Guojie Song, and Qingyong Li. 2020. Multi-
component graph convolutional collaborative filtering. In AAAI, Vol. 34. 6267â€“
6274.
[55] David Weininger. 1988. SMILES, a chemical language and information system. 1.
Introduction to methodology and encoding rules. Journal of chemical information
and computer sciences 28, 1 (1988), 31â€“36.
[56] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In ICML. 6861â€“
6871.
[57] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. 2021.
Discovering Invariant Rationales for Graph Neural Networks. In ICLR.
[58] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Ge-
niesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet: a
benchmark for molecular machine learning. Chemical science 9, 2 (2018), 513â€“530.
[59] Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan,
Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al .2019.
Pushing the boundaries of molecular representation for drug discovery with
the graph attention mechanism. Journal of medicinal chemistry 63, 16 (2019),
8749â€“8760.
[60] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How Powerful
are Graph Neural Networks?. In ICLR.
[61] Cheng Yang, Maosong Sun, Wayne Xin Zhao, Zhiyuan Liu, and Edward Y Chang.
2017. A neural network approach to jointly modeling social networks and mobile
trajectories. ACM Transactions on Information Systems (TOIS) 35, 4 (2017), 1â€“28.
[62] Nianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, and Junchi Yan. 2022. Learn-
ing substructure invariance for out-of-distribution molecular representations. In
NeurIPS, Vol. 35. 12964â€“12978.
[63] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. Gnnexplainer: Generating explanations for graph neural networks. In
NeurIPS, Vol. 32.
[64] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. 2022. Explainability in
graph neural networks: A taxonomic survey. IEEE transactions on pattern analysis
and machine intelligence 45, 5 (2022), 5782â€“5799.
[65] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural
network regularization. arXiv preprint arXiv:1409.2329 (2014).
[66] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018.
mixup: Beyond Empirical Risk Minimization. In ICLR.
[67] Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christo-
pher Re. 2022. Correct-N-Contrast: a Contrastive Approach for Improving Ro-
bustness to Spurious Correlations. In ICML. 26484â€“26516.
[68] Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. 2019. On
learning invariant representations for domain adaptation. In ICML. 7523â€“7532.
[69] Tianyi Zhao, Yang Hu, Linda R Valsdottir, Tianyi Zang, and Jiajie Peng. 2021.
Identifying drugâ€“target interactions based on graph convolutional network and
deep neural network. Briefings in bioinformatics 22, 2 (2021), 2141â€“2150.
[70] Xiang Zhuang, Qiang Zhang, Keyan Ding, Yatao Bian, Xiao Wang, Jingsong
Lv, Hongyang Chen, and Huajun Chen. 2023. Learning Invariant Molecular
Representation in Latent Discrete Space. In NeurIPS.
3197Advancing Molecule Invariant Representation via
Privileged Substructure Identification KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Statistics of datasets.
Dataset #T
rain #Validation #Test #Total Split SchemeOGBBA
CE 1,210 151 152 1,513 Scaffold
BBBP 1,631 204 204 2,039 ScaffoldDrugOODIC50-assay 34,179
19,028 19,028 72,235 Assay
IC50-scaffold 21,519 19,041 19,048 59,608 Scaffold
IC50-size 36,597 17,660 16,415 70,672 Size
EC50-assay 4,540 2,572 2,490 9,602 Assay
EC50-scaffold 2,570 2,532 2,533 7,635 Scaffold
EC50-size 4,684 2,313 2,398 9,395 Size
A PROOFS
Theorem. For the optimal environment predictor ğ‘“ğ‘’âˆ—(Â¯ğ›¿âˆ—(G))that
relies solely on the variant structure, denote the prediction as Ë†Yğ‘’and
the ground truth as Y. If the environments are split by
e=I(Ë†Yğ‘’=Y), (19)
where the function Idetermines the equality of two random variables,
the substructure invariance constraint will be maximally violated.
Proof. To measure how well the substructure invariant con-
straint (SIC) is maintained, we introduce an environment sufficiency
gap metric
Î”(ğ‘“ğ‘’â—¦Â¯ğ›¿,e)=EE
(Y|ğ‘“ğ‘’(Â¯ğ›¿(G)), ğ‘’1)
âˆ’E
(Y|ğ‘“ğ‘’(Â¯ğ›¿(G)), ğ‘’2)
.(20)
By substituting the definition of environment split e=I(Ë†Yğ‘’=Y),
we obtain
Î”(ğ‘“ğ‘’â—¦Â¯ğ›¿,e)=EE
(Y|Ë†Yğ‘’,I(Ë†Yğ‘’=Y))
âˆ’E
(Y|Ë†Yğ‘’,I(Ë†Yğ‘’â‰ Y))
.(21)
Given that the prediction Ë†Yğ‘’relies solely on the variant structure,
it follows that YâŠ¥Ë†Yğ‘’. Consequently,
Î”(ğ‘“ğ‘’â—¦Â¯ğ›¿,e)=EE
(Y|I(Ë†Yğ‘’=Y))
âˆ’E
(Y|I(Ë†Yğ‘’â‰ Y))
. (22)
Considering the downstream binary classification, we have Ë†Yğ‘’âˆˆ
{0,1}. When Ë†Yğ‘’=0,
E
(Y|I(Ë†Yğ‘’=Y))
âˆ’E
(Y|I(Ë†Yğ‘’â‰ Y))=|0âˆ’1|=1. (23)
Similarly, when Ë†Yğ‘’=1,
E
(Y|I(Ë†Yğ‘’=Y))
âˆ’E
(Y|I(Ë†Yğ‘’â‰ Y))=|1âˆ’0|=1. (24)
Therefore, for each instance, the absolute difference is 1. This leads
to an overall environment sufficiency gap Î”(ğ‘“ğ‘’â—¦Â¯ğ›¿,e)equating to
the maximum value 1. â–¡
Theorem. For the environment split edetermined by the optimal
environment predictor ğ‘“ğ‘’âˆ—â—¦Â¯ğ›¿âˆ—and the ground truth Y, the following
equation
ğ‘ƒ(Y=ğ‘¦1|ğ‘’1)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’1)=ğ‘ƒ(Y=ğ‘¦1|ğ‘’2)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’2)(25)
holds for any ğ‘¦1,ğ‘¦2âˆˆY and anyğ‘’1,ğ‘’2âˆˆE.Proof. Considering that the molecule invariant representation
Î¦âˆ—(ğ›¿âˆ—(G))satisfies the substructure invariant constraint (SIC),
ğ‘ƒ(Y=ğ‘¦1|Î¦âˆ—(ğ›¿âˆ—(G)), ğ‘’1)=ğ‘ƒ(Y=ğ‘¦1|Î¦âˆ—(ğ›¿âˆ—(G)), ğ‘’2),âˆ€ğ‘’1, ğ‘’2âˆˆE (26)
holds for any ğ‘¦1âˆˆY. Since the environment split eis determined
by the optimal environment predictor ğ‘“ğ‘’âˆ—â—¦Â¯ğ›¿âˆ—and the ground truth
Y, we have Î¦âˆ—(ğ›¿âˆ—(G))âŠ¥ e|Y. Therefore,
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’1,Y=ğ‘¦1)=ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’2,Y=ğ‘¦1),âˆ€ğ‘’1, ğ‘’2âˆˆE.
(27)
Combining Eq. (26), we can obtain
ğ‘ƒ(Y=ğ‘¦1|Î¦âˆ—(ğ›¿âˆ—(G)), ğ‘’1)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’1,Y=ğ‘¦1)=ğ‘ƒ(Y=ğ‘¦1|Î¦âˆ—(ğ›¿âˆ—(G)), ğ‘’2)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’2,Y=ğ‘¦1)â‡â‡’
ğ‘ƒ(Y=ğ‘¦1, ğ‘’1)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G)), ğ‘’1)=ğ‘ƒ(Y=ğ‘¦1, ğ‘’2)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G)), ğ‘’2)â‡â‡’
ğ‘ƒ(Y=ğ‘¦1|ğ‘’1)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’1)=ğ‘ƒ(Y=ğ‘¦1|ğ‘’2)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’2)â‡â‡’
ğ‘ƒ(Y=ğ‘¦1|ğ‘’1)
ğ‘ƒ(Y=ğ‘¦1|ğ‘’2)=ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’1)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’2).(28)
For anyğ‘¦2âˆˆY, a similar conclusion can be drawn
ğ‘ƒ(Y=ğ‘¦2|ğ‘’1)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’2)=ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’1)
ğ‘ƒ(Î¦âˆ—(ğ›¿âˆ—(G))|ğ‘’2). (29)
Noting that the RHS of Eq. (28) and Eq. (29) are equal, thus
ğ‘ƒ(Y=ğ‘¦1|ğ‘’1)
ğ‘ƒ(Y=ğ‘¦1|ğ‘’2)=ğ‘ƒ(Y=ğ‘¦2|ğ‘’1)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’2)â‡â‡’
ğ‘ƒ(Y=ğ‘¦1|ğ‘’1)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’1)=ğ‘ƒ(Y=ğ‘¦1|ğ‘’2)
ğ‘ƒ(Y=ğ‘¦2|ğ‘’2).(30)
â–¡
B DETAILS OF DATASETS
In this work, we leverage eight public benchmarks to evaluate our
model. Specifically,
â€¢Open Graph Benchmark (OGB) [ 20] provides two notable
datasets: BACE and BBBP. BACE focuses on binding affin-
ity to human beta-secretase 1, with each molecule labeled
according to its binding interaction with this enzyme, a cru-
cial target in Alzheimerâ€™s disease research. BBBP assesses
Brain-Blood Barrier Penetration, an essential factor for the
effectiveness of neuroactive drugs. The labels indicate the
ability of molecules to permeate the brain cell membrane
and enter the central nervous system.
3198KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, and Chuan Shi
â€¢The remaining six datasets originate from DrugOOD [ 22]:
IC50-assay, IC50-scaffold, IC50-size, EC50-assay, EC50-scaffold,
and EC50-size. The suffixes appended to these dataset names
delineate the methodology for their respective train-validation-
test splits. These six datasets are primarily concerned withligand-based affinity prediction, a critical measure in phar-
macology, where each molecule is labeled as either active or
inactive based on bioassay results.
The statistics of datasets are summarized in Table 2.
3199