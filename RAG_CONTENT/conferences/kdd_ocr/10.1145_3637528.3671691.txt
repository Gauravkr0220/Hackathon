Meta Clustering of Neural Bandits
Yikun Banâˆ—
yikunb2@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USAYunzhe Qiâˆ—
yunzheq2@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USATianxin Wei
twei10@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USA
Lihui Liu
lihuil2@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USAJingrui He
jingrui@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USA
Abstract
The contextual bandit has been identified as a powerful frame-
work to formulate the recommendation process as a sequential
decision-making process, where each item is regarded as an arm
and the objective is to minimize the regret of ğ‘‡rounds. In this
paper, we study a new problem, Clustering of Neural Bandits, by
extending previous work to the arbitrary reward function, to strike
a balance between user heterogeneity and user correlations in the
recommender system. To solve this problem, we propose a novel
algorithm called M-CNB, which utilizes a meta-learner to represent
and rapidly adapt to dynamic clusters, along with an informative
Upper Confidence Bound (UCB)-based exploration strategy. We
provide an instance-dependent performance guarantee for the pro-
posed algorithm that withstands the adversarial context, and we
further prove the guarantee is at least as good as state-of-the-art
(SOTA) approaches under the same assumptions. In extensive exper-
iments conducted in both recommendation and online classification
scenarios, M-CNB outperforms SOTA baselines. This shows the
effectiveness of the proposed approach in improving online recom-
mendation and online classification performance.
CCS Concepts
â€¢Theory of computation â†’Online learning algorithms; â€¢
Information systems â†’Personalization.
Keywords
Neural Contextual Bandits; Recommendation; User Modeling; Meta
Learning
ACM Reference Format:
Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He. 2024. Meta
Clustering of Neural Bandits. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
âˆ—Both authors contributed equally to this paper.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.367169125â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671691
1 Introduction
Recommender systems play an integral role in various online busi-
nesses, including e-commerce platforms and online streaming ser-
vices. They leverage user correlations to assist the perception of user
preferences, a field of study spanning several decades. In the past,
considerable effort has been directed toward supervised-learning-
based collaborative filtering methods within relatively static envi-
ronments [ 26,52]. However, the ideal recommender systems should
adapt over time to consistently meet user interests. Consequently,
it is natural to formulate the recommendation process as a sequen-
tial decision-making process. In this paradigm, the recommender
engages with users, observes their online feedback (i.e., rewards),
and optimizes the user experience for long-term benefits, rather
than fitting a model on the collected static data based on super-
vised learning [ 13,22,60]. Based on this idea, this paper focuses on
the formulation of contextual bandits, where each item is treated
as an arm (context) in a recommendation round, and the primary
objective is to minimize the cumulative regret over ğ‘‡rounds and
tackle the dilemma of exploitation and exploration in the sequential
decision-making process [1, 3, 4, 8, 23, 24, 39, 39, 40, 43, 46, 47].
Linear contextual bandits model a userâ€™s preference through a
linear reward function based on arm contexts [ 1,16,38]. However,
given the substantial growth of users in recommender systems, it
can be overly ambitious to represent all user preferences with a
single reward function, and it may overlook the user correlations if
each user is modeled as a single bandit. To address this challenge, a
series of methods known as clustering of linear bandits [ 4,23,24,39,
40] have emerged, which represent each cluster of users as a reward
function, achieving a balance between user heterogeneity and user
correlations. Note that the cluster information is unknown in this
problem setting. In essence, with each user being treated as a linear
contextual bandit, these methods adopt graph-based techniques to
dynamically cluster users, and leverage user correlations for making
arm recommendations. However, it is crucial to acknowledge the
limitations of this line of works: they all rely on linear reward
functions, and user clusters are represented as linear combinations
of individual bandit parameters. The assumptions of linearity in
reward functions and the linear representation of clusters may not
hold up well in real-world applications [53, 65].
 
95
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
In relaxation of the assumption on reward mapping functions,
inspired by recent advances in the single neural bandit [ 64,65]
where a neural network is assigned to learn anunknown reward
function, we study the new problem of Clustering of Neural Bandits
(CNB) in this paper. Different from the single neural bandit [ 64,65]
and clustering of linear bandits [ 4,23,24,39,40], CNB introduces
the bandit clusters built upon the arbitrary reward functions, which
can be either linear or non-linear. Meanwhile, we note that the
underlying clusters are usually not static over specific arm contexts
[39]. For example, in the personalized recommendation task, two
users (bandits) may both like "country music", but can have differ-
ent opinions on "rock music". Therefore, adapting to arm-specific
"relative clusters" in a dynamic environment is one of the main
challenges in this problem. We propose a novel algorithm, Meta
Clustering of Neural Bandits ( M-CNB ), to solve the CNB problem.
Next, we will summarize our key ideas and contributions.
Methodology. To address the CNB problem, we must confront
three key challenges: (1) Efficiently determining a userâ€™s relative
group: Our approach involves employing a neural network, named
the "user learner," to estimate each userâ€™s preferences. By group-
ing users with similar preferences, we efficiently create clusters
with a process taking O(ğ‘›)time, where ğ‘›is the number of bandits
(users). (2) Effective parametric representation of dynamic clusters:
Inspired by advancements in meta-learning [ 21,62], we introduce
a meta-learner capable of representing and swiftly adapting to
evolving clusters. In each round ğ‘¡, the meta-learner leverages its
perceived knowledge from prior rounds {1,...,ğ‘¡âˆ’1}to rapidly
adapt to new clusters via a few samples. This enables the rapid
acquisition of nonlinear cluster representations, marking our first
main contribution. (3) Balancing exploitation and exploration with
relative bandit clusters: Our second main contribution is proposing
an informative UCB-type exploration strategy, which takes into
account both user-side and meta-side information for balancing the
exploration and exploitation. By addressing these three main chal-
lenges, our approach manages to solve the CNB problem effectively
and efficiently.
Theoretical analysis. To obtain a regret upper bound for the
proposed algorithm, we need to tackle the following three chal-
lenges: (1) Analyzing neural meta-learner in bandit framework:
To finish the analysis, we must build a confidence ellipsoid for the
meta-learner approximation, which is one of the main research gaps.
To deal with this gap, we bridge the meta-learner and user-learner
via the Neural Tangent Kernel (NTK) regression and build the con-
fidence ellipsoid upon the user-learner, which allows us to achieve
a more comprehensive understanding of the meta-learnerâ€™s behav-
ior. (2) Reducing the naive eO(âˆš
ğ‘›ğ‘‡)regret upper bound: eO(âˆš
ğ‘‡)is
roughly the regret effort to learn a single neural bandit, and thus
eO(âˆš
ğ‘›ğ‘‡)are the regret efforts to learn ğ‘›neural bandits for ğ‘›users.
We reduce the eO(âˆš
ğ‘›ğ‘‡)efforts to eO(âˆšï¸
ğ‘ğ‘‡), whereğ‘is the expected
number of clusters. This also indicates the proposed algorithm
can leverage the collaborative effects among users. (3) Adversarial
attack on contexts: In most neural bandit works, a common as-
sumption is that the NTK matrix is non-singular, requiring that
no two observed contexts (items) are identical or parallel [ 64,65].
This vulnerability makes their regret analysis susceptible to adver-
sarial attacks and less practical in real-world scenarios. In face ofthis challenge, we provide an instance-dependent regret analysis
that withstands the context attack, and allows the contexts to be
repeatedly observed. Furthermore, under the same assumptions as
in existing works, we demonstrate that our regret upper bound is at
least as good as SOTA approaches. The above efforts to address the
challenges in the theoretical analysis is our third main contribution.
Evaluations. We evaluate the proposed algorithm in two scenar-
ios: Online recommendation and Online classification with bandit
feedback. For the first scenario, which naturally lends itself to CNB,
we assess the algorithmâ€™s performance on four recommendation
datasets. Since online classification has been widely used to evaluate
neural bandits [ 6,64,65], we evaluate the algorithms on eight clas-
sification datasets where each class can be considered as a bandit
(user), and correlations among classes are expected to be exploited.
We compare the proposed algorithm with 8 strong baselines and
show the superior performance of the proposed algorithm. Addi-
tionally, we offer the empirical analysis of the algorithmâ€™s time
complexity, and conduct extensive sensitivity studies to investi-
gate the impact of critical hyperparameters. The above empirical
evaluation is our fourth main contribution.
Next, detailed discussion regarding related works is placed in
Section 2. After introducing the problem definition in Section 3, we
present the proposed algorithm, M-CNB , in Section 4 together with
theoretical analysis in Section 5. Then, we provide the experimental
results in Section 6 and conclude the paper in Section 7.
2 Related Work
In this section, we briefly review the related works, including clus-
tering of bandits and neural bandits.
Clustering of bandits. CLUB [ 23] first studies collaborative
effects among users in contextual bandits where each user hosts
an unknown vector to represent the behavior based on the linear
reward function. CLUB formulates user similarity on an evolving
graph and selects an arm leveraging the clustered groups. Then,
Gentile et al. [24], Li et al. [39] propose to cluster users based on
specific contents and select arms leveraging the aggregated infor-
mation of conditioned groups. Li et al. [40] improves the clustering
procedure by allowing groups to split and merge. Ban and He [4]
uses seed-based local clustering to find overlapping groups, differ-
ent from global clustering on graphs. Korda et al. [33], Wu et al.
[57], Yang et al. [61] also study clustering of bandits with various
settings in recommender systems. However, all these works are
based on the linear reward assumption, which may fail in many
real-world applications.
Neural bandits . Lipton et al. [41], Riquelme et al. [49] adapt the
Thompson Sampling (TS) to the last layer of deep neural networks
to select an action. However, these approaches do not provide re-
gret analysis. Zhou et al. [65] and Zhang et al. [64] first provide the
regret analysis of UCB-based and TS-based neural bandits, where
they apply ridge regression on the space of gradients. Ban et al.
[5]studies a multi-facet bandit problem with a UCB-based explo-
ration. Jia et al. [30]perturbs the training samples for incorporating
both exploitation and exploration. EE-Net [ 6,9] proposes to use
another neural network for exploration with applications on active
learning [ 7,10] and meta-learning [ 48]. [59] combines the last-
layer neural network embedding with linear UCB to improve the
 
96Meta Clustering of Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
computation efficiency. Dutta et al. [20] uses an off-the-shelf meta-
learning approach to solve the contextual bandit problem in which
the expected reward is formulated as Q-function. Santana et al. [50]
proposes a Hierarchical Reinforcement Learning framework for
recommendation in the dynamic experiments, where a meta-bandit
is used for the selected independent recommender system. Kassraie
and Krause [31] revisit Neural-UCB type algorithms and shows
theeO(âˆš
ğ‘‡)regret bound without the restrictive assumptions on
the context. Hong et al. [27], Maillard and Mannor [42] study the
latent bandit problem where the reward distribution of arms are
conditioned on some unknown discrete latent state and prove the
eO(âˆš
ğ‘‡)regret bound for their algorithm as well. Federated bandits
[15] consider dealing with multiple bandits (agents) while preserv-
ing the privacy of each bandit. Deb et al. [17] reduce the contextual
bandits to neural online regression for tighter regret upper bound.
Qi et al. [47] propose to use graph to formulate user correlations
with the adoption of graph neural networks. However, the above
works either focus on the different problem settings or overlook
the clustering of bandits.
Other related works. [ 35,51] study meta-learning in Thomp-
son sampling and Hong et al. [28], Wan et al. [54] aims to exploit
the hierarchical knowledge among hierarchical Bayesian bandits.
However, they focus on the Bayesian or non-contextual bandits.
3 Problem: Clustering of Neural Bandits
In this section, we introduce the CNB problem, motivated by learn-
ing correlations among bandits with arbitrary reward functions.
Next, we will use the scenarios of personalized recommendation to
state the problem setting.
Suppose there are ğ‘›users (bandits), ğ‘={1,...,ğ‘›}, to serve on
a platform. In the ğ‘¡thround, the platform receives a user ğ‘¢ğ‘¡âˆˆğ‘
(unique ID for this user) and prepares the corresponding ğ¾candi-
date arms Xğ‘¡={xğ‘¡,1,xğ‘¡,2,..., xğ‘¡,ğ¾}. Each arm is represented by
itsğ‘‘-dimensional feature vector xğ‘¡,ğ‘–âˆˆRğ‘‘,ğ‘–âˆˆ[ğ¾]={1,...,ğ¾},
which will encode the information from both the user side and
the arm side [ 38]. Then, the learner is expected to select an arm
xğ‘¡âˆˆXğ‘¡and recommend it to ğ‘¢ğ‘¡, whereğ‘¢ğ‘¡refers to the target or
served user. In response to this action, ğ‘¢ğ‘¡will provide the platform
with a corresponding reward (feedback) ğ‘Ÿğ‘¡. Here, since different
users may generate different rewards towards the same arm, we
useğ‘Ÿğ‘¡,ğ‘–|ğ‘¢ğ‘¡to represent the reward produced by ğ‘¢ğ‘¡given xğ‘¡,ğ‘–. The
formal definition of arm reward is below.
Givenğ‘¢ğ‘¡âˆˆğ‘, the reward ğ‘Ÿğ‘¡,ğ‘–for each candidate arm xğ‘¡,ğ‘–âˆˆXğ‘¡
is assumed to be governed by an unknown function by
ğ‘Ÿğ‘¡,ğ‘–|ğ‘¢ğ‘¡=â„ğ‘¢ğ‘¡(xğ‘¡,ğ‘–)+ğœğ‘¡,ğ‘–, (1)
whereâ„ğ‘¢ğ‘¡is an unknown reward function associated with ğ‘¢ğ‘¡, and
it can be either linear or non-linear. ğœğ‘¡,ğ‘–is a noise term with zero
expectation E[ğœğ‘¡,ğ‘–]=0. We also assume the reward ğ‘Ÿğ‘¡,ğ‘–âˆˆ[0,1]is
bounded, as in many existing works [ 4,23,24]. Note that previous
works on clustering of linear bandits all assume â„ğ‘¢ğ‘¡is a linear
function with respect to arm xğ‘¡,ğ‘–[4, 23, 24, 39, 40].
Meanwhile, users may exhibit clustering behavior. Inspired by
[24,39], we consider the cluster behavior to be item-varying, i.e.,
the users who have the same preference on a certain item may have
different opinions on another item. Therefore, we formulate a set ofusers with the same opinions on a certain item as a relative cluster,
with the following definition.
Definition 3.1 (Relative Cluster). In roundğ‘¡, given an arm xğ‘¡,ğ‘–âˆˆ
Xğ‘¡, a relative clusterN(xğ‘¡,ğ‘–)âŠ†ğ‘with respect to xğ‘¡,ğ‘–satisfies
(1)âˆ€ğ‘¢,ğ‘¢â€²âˆˆN( xğ‘¡,ğ‘–),E[ğ‘Ÿğ‘¡,ğ‘–|ğ‘¢]=E[ğ‘Ÿğ‘¡,ğ‘–|ğ‘¢â€²]
(2)ÂšNâ€²âŠ†ğ‘,s.t.Nâ€²satisfies(1)andN(xğ‘¡,ğ‘–)âŠ‚Nâ€².
The condition(2)is to guarantee that no other clusters contains
N(xğ‘¡,ğ‘–). This cluster definition allows users to agree on certain
items while disagree on others, which is consistent with the real-
world scenario. Since the users from different clusters are expected
to have distinct behavior with respect to xğ‘¡,ğ‘–, we provide the fol-
lowing constraint among relative clusters.
Definition 3.2 (ğ›¾-gap). Given two different cluster N(xğ‘¡,ğ‘–),Nâ€²(xğ‘¡,ğ‘–),
there exists a constant ğ›¾>0, such that
âˆ€ğ‘¢âˆˆN( xğ‘¡,ğ‘–),ğ‘¢â€²âˆˆNâ€²(xğ‘¡,ğ‘–),|E[ğ‘Ÿğ‘¡,ğ‘–|ğ‘¢]âˆ’E[ğ‘Ÿğ‘¡,ğ‘–|ğ‘¢â€²]|â‰¥ğ›¾.
For any two clusters in ğ‘, we assume that they satisfy the ğ›¾-
gap constraint. Note that such an assumption is standard in the
literature of online clustering of bandit to differentiate clusters
[4,23,24,39,40]. As a result, given an arm xğ‘¡,ğ‘–, the bandit pool ğ‘
can be divided into ğ‘ğ‘¡,ğ‘–non-overlapping clusters:N1(xğ‘¡,ğ‘–),N2(xğ‘¡,ğ‘–),
...,Nğ‘ğ‘¡,ğ‘–(xğ‘¡,ğ‘–), whereğ‘ğ‘¡,ğ‘–â‰ªğ‘›. Note that the cluster information
isunknown in the platform.
For the CNB problem, the goal of the learner is to minimize the
pseudo regret of ğ‘‡rounds:
Rğ‘‡=ğ‘‡âˆ‘ï¸
ğ‘¡=1E[ğ‘Ÿâˆ—
ğ‘¡âˆ’ğ‘Ÿğ‘¡|ğ‘¢ğ‘¡,Xğ‘¡], (2)
whereğ‘Ÿğ‘¡is the reward received in round ğ‘¡, and E[ğ‘Ÿâˆ—
ğ‘¡|ğ‘¢ğ‘¡,Xğ‘¡]=
max xğ‘¡,ğ‘–âˆˆXğ‘¡â„ğ‘¢ğ‘¡(xğ‘¡,ğ‘–).
Notations. Let xğ‘¡be the arm selected in round ğ‘¡, andğ‘Ÿğ‘¡be
the corresponding reward received in round ğ‘¡. We useâˆ¥xğ‘¡âˆ¥2to
represent the Euclidean norm. For each user ğ‘¢âˆˆğ‘, letğœ‡ğ‘¢
ğ‘¡be the
number of rounds that user ğ‘¢â€™ learner has been served up to round
ğ‘¡, andTğ‘¢
ğ‘¡be all ofğ‘¢â€™s historical data up to round ğ‘¡.ğ‘šis the width
of neural network and ğ¿is depth of neural network in the proposed
approach. Given a group N, all its data up to round ğ‘¡can be denoted
by{Tğ‘¢
ğ‘¡}ğ‘¢âˆˆN={Tğ‘¢
ğ‘¡|ğ‘¢âˆˆN} . We use standardOandÎ©notation
to hide constants.
4 Proposed Algorithm
In this section, we present our proposed algorithm, denoted as
M-CNB , to address the formulated CNB problem. M-CNB lever-
ages the potential correlations among bandits, and aims to rapidly
acquire a representation for dynamic relative clusters.
ForM-CNB , we utilize a meta-learner, denoted as Î˜, to rapidly
adapt to clusters, as well as represent the behavior of a cluster. Addi-
tionally, there are ğ‘›user-learners, denoted by {ğœƒğ‘¢}ğ‘¢âˆˆğ‘, responsible
for learning the preference â„ğ‘¢(Â·)for each user ğ‘¢âˆˆğ‘. In terms of
the workflow, the primary role of the meta-learner is to determine
recommended arms, while the user-learners are primarily utilized
for clustering purposes. The meta-learner and user-learners share
the same neural network structure, denoted as ğ‘“. And the workflow
ofM-CNB is divided into three main components: User clustering,
 
97KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
Î˜!"#Î˜!âˆ‡ğ¿!!âˆ‡ğ¿!"âˆ‡ğ¿!#ğ‘¢!(1) Clustering(2) Mata Adaptation
Figure 1: Clustering and Meta Adaptation: Given ğ‘¢ğ‘¡and an
arm xğ‘¡,ğ‘–, (1)M-CNB identifies cluster bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–), and then (2)
meta-learner Î˜ğ‘¡âˆ’1rapidly adapt to this cluster, proceeding
to (3) the UCB exploration.
Meta adaptation, and UCB-based selection. Then, we proceed to
elaborate their details.
User clustering. Recall that in Section 3, each user ğ‘¢âˆˆğ‘
is governed by an unknown function â„ğ‘¢. In this case, we use a
neural network ğ‘“(Â·;ğœƒğ‘¢), to estimate â„ğ‘¢. In roundğ‘¡âˆˆ[ğ‘‡], letğ‘¢ğ‘¡
be the user to serve. Given ğ‘¢ğ‘¡â€™s past data up to round ğ‘¡âˆ’1, i.e.,
Tğ‘¢ğ‘¡
ğ‘¡âˆ’1, we can train parameters ğœƒğ‘¢ğ‘¡by minimizing the following loss:
L(ğœƒğ‘¢ğ‘¡)=Ã
(x,ğ‘Ÿ)âˆˆTğ‘¢ğ‘¡
ğ‘¡âˆ’1(ğ‘“(x;ğœƒğ‘¢ğ‘¡)âˆ’ğ‘Ÿ)2/2.Letğœƒğ‘¢ğ‘¡
ğ‘¡âˆ’1representğœƒğ‘¢ğ‘¡
trained onTğ‘¢ğ‘¡
ğ‘¡âˆ’1in roundğ‘¡âˆ’1by stochastic gradient descent (SGD).
Therefore, for each ğ‘¢âˆˆğ‘, we can obtain the trained parameters
ğœƒğ‘¢
ğ‘¡âˆ’1. Then, given ğ‘¢ğ‘¡and an arm xğ‘¡,ğ‘–, we return ğ‘¢ğ‘¡â€™s estimated
cluster with respect to arm xğ‘¡,ğ‘–by
bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)=
ğ‘¢âˆˆğ‘|ğ‘“(xğ‘¡,ğ‘–;ğœƒğ‘¢
ğ‘¡âˆ’1)âˆ’ğ‘“(xğ‘¡,ğ‘–;ğœƒğ‘¢ğ‘¡
ğ‘¡âˆ’1)|â‰¤ğœˆâˆ’1
ğœˆğ›¾	
.
(3)
whereğ›¾âˆˆ(0,1)represents the assumed ğ›¾-gap andğœˆ>1is a tuning
parameter to for the exploration of cluster members.
Meta adaptation. We employ one meta-learner Î˜to represent
and adapt to the behavior of dynamic clusters. In meta-learning,
the meta-learner is trained based on a number of different tasks
and can quickly adapt to new tasks with a small amount of new
data [ 21]. Here, we consider a cluster Nğ‘¢ğ‘¡(xğ‘¡,ğ‘–)as a task and its
collected data as the task distribution. As a result, M-CNB has two
adaptation phases: meta adaptation, and user adaptation.
Meta adaptation. In the ğ‘¡thround, given a cluster bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–), we
have the available "task distributions" {Tğ‘¢
ğ‘¡âˆ’1}ğ‘¢âˆˆbNğ‘¢ğ‘¡(xğ‘¡,ğ‘–). The goal
of the meta-learner is to quickly adapt to the bandit cluster. Thus, we
randomly draw a few samples from {Tğ‘¢
ğ‘¡âˆ’1}ğ‘¢âˆˆbNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)and update
Î˜in roundğ‘¡using SGD, denoted by Î˜ğ‘¡,ğ‘–, based on Î˜ğ‘¡âˆ’1that is
continuously trained on the collected interactions to incorporate
the knowledge of past ğ‘¡âˆ’1rounds. The workflow is described in
Figure 1 and Algorithm 2.
User adaptation. In the ğ‘¡thround, given ğ‘¢ğ‘¡, after receiving the
rewardğ‘Ÿğ‘¡, we have available data (xğ‘¡,ğ‘Ÿğ‘¡). Then, the user leaner
ğœƒğ‘¢ğ‘¡is updated in round ğ‘¡to have a refined clustering capability,
denoted byğœƒğ‘¢ğ‘¡
ğ‘¡. As the users in a cluster share the same or similar
preferences on a certain item, we update all the user learners in
this cluster, described in Algorithm 1 Lines 14-18.
Note that for the clustering of linear bandits works [ 4,23,24,39,
40], they represent the cluster behavior Î˜by the linear combination
of bandit-learners, e.g., Î˜=1
|bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)|Ã
ğ‘¢âˆˆbNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)ğœƒğ‘¢
ğ‘¡. This canAlgorithm 1 M-CNB
1:Input:ğ‘‡(number of rounds), ğ›¾,ğœˆ(cluster exploration parame-
ter),ğ‘†(norm parameter), ğ›¿(confidence level) , ğœ‚1,ğœ‚2(learning
rate),ğ‘š(width of neural network).
2:Initialize Î˜0;ğœƒğ‘¢
0=Î˜0,ğœ‡ğ‘¢
0=0,Tğ‘¢
0=âˆ…,âˆ€ğ‘¢âˆˆğ‘
3:Observe one data for each ğ‘¢âˆˆğ‘
4:forğ‘¡=1,2,...,ğ‘‡ do
5: Receive a target user ğ‘¢ğ‘¡âˆˆğ‘and observe ğ‘˜arms Xğ‘¡=
{xğ‘¡,1,..., xğ‘¡,ğ‘˜}
6:forğ‘–âˆˆ[ğ‘˜]do
7: Determine bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)={ğ‘¢âˆˆğ‘| |ğ‘“(xğ‘¡,ğ‘–;ğœƒğ‘¢
ğ‘¡âˆ’1) âˆ’
ğ‘“(xğ‘¡,ğ‘–;ğœƒğ‘¢ğ‘¡
ğ‘¡âˆ’1)|â‰¤ğœˆâˆ’1
ğœˆğ›¾}.
8: Î˜ğ‘¡,ğ‘–=SGD_Meta
bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–),Î˜ğ‘¡âˆ’1
9: Uğ‘¡,ğ‘–=ğ‘“(xğ‘¡,ğ‘–;Î˜ğ‘¡,ğ‘–) +âˆ¥âˆ‡Î˜ğ‘“(xğ‘¡,ğ‘–;Î˜ğ‘¡,ğ‘–)âˆ’âˆ‡ ğœƒğ‘“(xğ‘¡,ğ‘–;ğœƒğ‘¢ğ‘¡
0)âˆ¥2
ğ‘š1/4+
âˆšï¸ƒ
ğ‘†+1
2ğœ‡ğ‘¢
ğ‘¡+âˆšï¸‚
2 log(1/ğ›¿)
ğœ‡ğ‘¢
ğ‘¡
10: end for
11:bğ‘–=argğ‘–âˆˆ[ğ‘˜]maxUğ‘¡,ğ‘–
12: Play xğ‘¡,bğ‘–and observe reward ğ‘Ÿğ‘¡,bğ‘–
13: xğ‘¡=xğ‘¡,bğ‘–, ğ‘Ÿğ‘¡=ğ‘Ÿğ‘¡,bğ‘–,Î˜ğ‘¡=Î˜ğ‘¡,bğ‘–
14: forğ‘¢âˆˆbNğ‘¢ğ‘¡(xğ‘¡)do
15:Lğ‘¡ ğœƒğ‘¢
ğ‘¡=(ğ‘“(xğ‘¡;ğœƒğ‘¢
ğ‘¡)âˆ’ğ‘Ÿğ‘¡)2/2
16:ğœƒğ‘¢
ğ‘¡=ğœƒğ‘¢
ğ‘¡âˆ’ğœ‚1â–½ğœƒğ‘¢
ğ‘¡Lğ‘¡ ğœƒğ‘¢
ğ‘¡# User Adaptation
17:ğœ‡ğ‘¢
ğ‘¡=ğœ‡ğ‘¢
ğ‘¡âˆ’1+1,Tğ‘¢
ğ‘¡=Tğ‘¢
ğ‘¡âˆ’1âˆª{(xğ‘¡,ğ‘Ÿğ‘¡)}
18: end for
19: forğ‘¢âˆ‰bNğ‘¢ğ‘¡(xğ‘¡)do
20:ğœƒğ‘¢
ğ‘¡=ğœƒğ‘¢
ğ‘¡âˆ’1,ğœ‡ğ‘¢
ğ‘¡=ğœ‡ğ‘¢
ğ‘¡âˆ’1,Tğ‘¢
ğ‘¡=Tğ‘¢
ğ‘¡âˆ’1
21: end for
22:end for
Algorithm 2 SGD_Meta ( bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–),Î˜ğ‘¡âˆ’1)
bN=bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)
forğ‘¢âˆˆbNdo
Randomly draw(xğ‘¢,ğ‘Ÿğ‘¢)fromTğ‘¢
ğ‘¡âˆ’1
Lğ‘¢(Î˜ğ‘¡âˆ’1)=(ğ‘“(xğ‘¢;Î˜ğ‘¡âˆ’1)âˆ’ğ‘Ÿğ‘¢)2/2
end for
Lğ‘¡âˆ’1(bN)=1
|bN|Ã
ğ‘¢âˆˆbNLğ‘¢(Î˜ğ‘¡âˆ’1)
Î˜ğ‘¡,ğ‘–=Î˜ğ‘¡âˆ’1âˆ’ğœ‚2âˆ‡Î˜ğ‘¡âˆ’1Lğ‘¡âˆ’1(bN) # Meta Adaptation
Return: Î˜ğ‘¡,ğ‘–
lead to limited representation power of the cluster learner, and
their linear reward assumptions may not necessarily hold for real
world settings [ 65]. Instead, we use the meta adaptation to update
the meta-learner Î˜ğ‘¡âˆ’1according to bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–), which can represent
non-linear combinations of user-learners [21, 55].
UCB-based Exploration. To balance the trade-off between
the exploitation of the currently available information and the
exploration of new matches, we introduce the following UCB-based
selection criterion. Based on Lemma A.12, the cumulative error
induced by meta-learner is controlled by
 
98Meta Clustering of Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ğ‘‡âˆ‘ï¸
ğ‘¡=1E
ğ‘Ÿğ‘¡|xğ‘¡
|ğ‘“(xğ‘¡;Î˜ğ‘¡)âˆ’ğ‘Ÿğ‘¡|ğ‘¢ğ‘¡
â‰¤ğ‘‡âˆ‘ï¸
ğ‘¡=1O(âˆ¥âˆ‡ Î˜ğ‘“(xğ‘¡;Î˜ğ‘¡)âˆ’âˆ‡ğœƒğ‘“(xğ‘¡;ğœƒğ‘¢ğ‘¡
0)âˆ¥2)
ğ‘š1/4
|                                           {z                                           }
Meta-side info
+âˆ‘ï¸
ğ‘¢âˆˆğ‘ğœ‡ğ‘¢
ğ‘‡"
O âˆšï¸„
ğ‘†+1
2ğœ‡ğ‘¢
ğ‘‡!
+âˆšï¸„
2 log(1/ğ›¿)
ğœ‡ğ‘¢
ğ‘‡|                               {z                               }
User-side info#
,
whereâˆ‡Î˜ğ‘“(xğ‘¡;Î˜ğ‘¡)incorporates the discriminative information
of meta-learner acquired from the correlations within the relative
cluster bNğ‘¢ğ‘¡(xğ‘¡)andO(1âˆšğœ‡ğ‘¢
ğ‘‡)shows the shrinking confidence in-
terval of user-learner to a specific user ğ‘¢. Then, we select an arm
according to: xğ‘¡=argxğ‘¡,ğ‘–âˆˆXğ‘¡maxUğ‘¡,ğ‘–( where Uğ‘¡,ğ‘–is calculated in
Line 9).
In summary, Algorithm 1 depicts the workflow of M-CNB . In
each roundğ‘¡, given a target user and a pool of candidate arms, we
compute the meta-learner and its bound for each relative cluster
(Line 6-10). Then, we choose the arm according to the UCB-type
strategy (Line 11). After receiving the reward, we update the user-
learners. Note that the meta-learner has been updated in Line 8.
Then, we discuss the time complexity of Algorithm 1. Here, with
ğ‘›being the number of users, M-CNB will takeO(ğ‘›)to find the
cluster for the served user. Given the detected cluster bN, it takes
O(|bN|) to update the meta-learner by SGD. Suppose E[|bN|]=
ğ‘›/Ë†ğ‘andğ‘›/Ë†ğ‘â‰ªğ‘›. Therefore, the overall test time complexity of
Algorithm 1 isO(ğ¾(ğ‘›+ğ‘›/Ë†ğ‘)). To scale M-CNB for deployment
in large recommender systems, we can rely on the assistance of
pre-processing tools: Pre-clustering of users and Pre-selection of
items. On the one hand, we can perform pre-clustering of users
based on the user features or other information. Then, let a pre-
cluster (instead of a single user) hold a neural network, which will
significantly reduce ğ‘›. On the other hand, we can conduct the pre-
selection of items based on item and user features, to reduce ğ¾
substantially. For instance, we only consider the restaurants that
are near the serving user for the restaurant recommendation task.
Furthermore, we can also control the magnitude of ğ‘›/Ë†ğ‘by tuning
the hyperparameter ğœˆbased on the actual application scenario.
Consequently, M-CNB can effectively serve as a core component
of large-scale recommender systems.
5 Regret Analysis
In this section, we provide the performance guarantee of M-CNB ,
which is built in the over-parameterized neural networks regime.
As the standard setting in contextual bandits, all arms are nor-
malized to the unit length. Given an arm xğ‘¡,ğ‘–âˆˆRğ‘‘withâˆ¥xğ‘¡,ğ‘–âˆ¥2=1,
ğ‘¡âˆˆ[ğ‘‡],ğ‘–âˆˆ[ğ¾], without loss of generality, we define ğ‘“as a fully-
connected network with depth ğ¿â‰¥2and widthğ‘š:
ğ‘“(xğ‘¡,ğ‘–;ğœƒorÎ˜)=Wğ¿ğœ(Wğ¿âˆ’1ğœ(Wğ¿âˆ’2...ğœ(W1xğ‘¡,ğ‘–))) (4)
whereğœis the ReLU activation function, W1âˆˆRğ‘šÃ—ğ‘‘,Wğ‘™âˆˆRğ‘šÃ—ğ‘š,
for2â‰¤ğ‘™â‰¤ğ¿âˆ’1,Wğ¿âˆˆR1Ã—ğ‘š, and
ğœƒ,Î˜=[vec(W1)âŠ¤,vec(W2)âŠ¤,..., vec(Wğ¿)âŠ¤]âŠ¤âˆˆRğ‘.Note that our analysis results can also be readily generalized to other
neural architectures such as CNNs and ResNet [ 2,18]. Then, we
employ the following initialization [ 12] forğœƒandÎ˜: Forğ‘™âˆˆ[ğ¿âˆ’1],
each entry of Wğ‘™is drawn from the normal distribution N(0,2/ğ‘š);
Each entry of Wğ¿is drawn from the normal distribution N(0,1/ğ‘š).
Here, given ğ‘…>0, we define the following function class:
ğµ(ğœƒ0,ğ‘…)={ğœƒâˆˆRğ‘:âˆ¥ğœƒâˆ’ğœƒ0âˆ¥2â‰¤ğ‘…/ğ‘š1/4}. (5)
The termğµ(ğœƒ0,ğ‘…)defines a function class ball centered at the ran-
dom initialization point ğœƒ0and with a radius of ğ‘…. This defini-
tion was originally introduced in the context of analyzing over-
parameterized neural networks, and it can be found in the works
of [12] and [ 2]. Recall that ğ‘ğ‘¡,ğ‘–represents the number of clusters
given xğ‘¡,ğ‘–. For the simplicity of analysis, we assume E[ğ‘ğ‘¡,ğ‘–]=ğ‘,ğ‘¡âˆˆ
[ğ‘‡],ğ‘–âˆˆ[ğ¾]. Let{(xğ‘¡,ğ‘Ÿğ‘¡)}ğ‘‡ğ¾
ğ‘¡=1represent all the data in ğ‘‡rounds
and define the squared loss Lğ‘¡(ğœƒ)=(ğ‘“(xğ‘¡;ğœƒ)âˆ’ğ‘Ÿğ‘¡)2/2. Then, we
provide the instance-dependent regret upper bound for M-CNB
with the following theorem.
Theorem 5.1. Given the number of rounds ğ‘‡andğ›¾, for anyğ›¿âˆˆ
(0,1),ğ‘…>0, supposeğ‘šâ‰¥eÎ©(poly(ğ‘‡,ğ¿,ğ‘…)Â·ğ¾ğ‘›log(1/ğ›¿)),ğœ‚1=ğœ‚2=
ğ‘…2âˆšğ‘š, andE[|Nğ‘¢ğ‘¡(xğ‘¡)|]=ğ‘›
ğ‘,ğ‘¡âˆˆ[ğ‘‡]. Then, with probability at least
1âˆ’ğ›¿over the initialization, Algorithm 1 achieves the following regret
upper bound:
Rğ‘‡â‰¤âˆšï¸ƒ
ğ‘ğ‘‡Â·ğ‘†âˆ—
ğ‘‡ğ¾+O( 1)+O(âˆšï¸
2ğ‘ğ‘‡log(O(1)/ğ›¿)).
whereğ‘†âˆ—
ğ‘‡ğ¾= inf
ğœƒâˆˆğµ(ğœƒ0,ğ‘…)Ãğ‘‡ğ¾
ğ‘¡=1Lğ‘¡(ğœƒ).
Theorem 5.1 provides a regret bound for M-CNB , which consists
of two main terms. The first term is instance-dependent and relates
to the squared error achieved by the function class ğµ(ğœƒ0,ğ‘…)on the
data. The second term is a standard large-deviation error term.
There are some noteworthy properties regarding Theorem 5.1.
One important aspect is that it depends on the parameter ğ‘, which
represents the expected number of clusters, rather than the number
of usersğ‘›. Specifically, eO(âˆš
ğ‘‡)corresponds to the regret effort for
learning a single bandit, and thus eO(âˆš
ğ‘›ğ‘‡)is an estimate of the
regret effort for learning ğ‘›bandits. However, Theorem 5.1 refines
this naive bound to eO(âˆšï¸
ğ‘ğ‘‡), linking the regret effort to the actual
underlying clusters among users.
Another advantage of Theorem 5.1 is that it makes no assump-
tions about the contexts {xğ‘¡}ğ‘‡ğ¾
ğ‘¡=1used in the problem. This makes
Theorem 5.1 robust against adversarial attacks on the contexts and
allows the observed contexts to contain repeated items. In contrast,
existing neural bandit algorithms like [ 31,64,65] rely on Assump-
tion 5.1 for the contexts, and their regret upper bounds can be
disrupted by straightforward adversarial attacks, e.g., creating two
identical contexts with different rewards.
The termğ‘†âˆ—
ğ‘‡ğ¾reflects the "regression difficulty" of fitting all the
data using a given function class, while the radius ğ‘…controls the
richness or complexity of that function class. Itâ€™s important to note
that the choice of ğ‘…is flexible, although itâ€™s not without constraints:
specifically, the value of ğ‘šmust be larger than a polynomial of
ğ‘…. Whenğ‘…is set to a larger value, it expands the function class
ğµ(ğœƒ0,ğ‘…), which means it can potentially fit a wider range of data.
Consequently, this tends to make ğ‘†âˆ—
ğ‘‡ğ¾smaller. Recent advances in
 
99KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
the convergence of neural networks, as demonstrated by [ 2] and
[18], have shown that there is an optimal region around the initial-
ization point in over-parameterized neural networks. This suggests
that, with the proper choice of ğ‘…, termğ‘†âˆ—
ğ‘‡ğ¾can be constrained to a
small constant value.
Next, we show the common assumption made on existing neural
bandits, and prove that Theorem 5.1 is no worse than their regret
bounds under the same assumption. The analysis is associated with
the Neural Tangent Kernel (NTK) matrix as follows:
Definition 5.2 (NTK [ 29,56]).LetNdenote the normal distribu-
tion. Given the data instances {xğ‘¡}ğ‘‡
ğ‘¡=1, for allğ‘–,ğ‘—âˆˆ[ğ‘‡], define
H0
ğ‘–,ğ‘—=Î£0
ğ‘–,ğ‘—=âŸ¨xğ‘–,xğ‘—âŸ©,Ağ‘™
ğ‘–,ğ‘—= 
Î£ğ‘™
ğ‘–,ğ‘–Î£ğ‘™
ğ‘–,ğ‘—
Î£ğ‘™
ğ‘—,ğ‘–Î£ğ‘™
ğ‘—,ğ‘—!
Î£ğ‘™
ğ‘–,ğ‘—=2Eğ‘,ğ‘âˆ¼N( 0,Ağ‘™âˆ’1
ğ‘–,ğ‘—)[ğœ(ğ‘)ğœ(ğ‘)],
Hğ‘™
ğ‘–,ğ‘—=2Hğ‘™âˆ’1
ğ‘–,ğ‘—Eğ‘,ğ‘âˆ¼N( 0,Ağ‘™âˆ’1
ğ‘–,ğ‘—)[ğœâ€²(ğ‘)ğœâ€²(ğ‘)]+Î£ğ‘™
ğ‘–,ğ‘—.
Then, the NTK matrix is defined as H=(Hğ¿+Î£ğ¿)/2.
Assumption 5.1. There exists ğœ†0>0, such that Hâª°ğœ†0I
The assumption 5.1 is generally held in the literature of neural
bandits [ 5,6,15,30,59,64,65] to ensure the existence of a solution
for NTK regression. This assumption holds true when any two
contexts in{xğ‘¡}ğ‘‡ğ¾
ğ‘¡=1are not linearly dependent or parallel. Then,
the SOTA regret upper bound for a single neural bandit ( ğ‘›=1)
[5, 15, 64, 65] is as follows:
eO(âˆšï¸
eğ‘‘ğ‘‡(ğ‘†+âˆšï¸
eğ‘‘)). (6)
There are two complexity terms in the regret bounds [ 6,65]. The
first complexity term is ğ‘†=âˆš
hâŠ¤Hâˆ’1h, where
h=[â„ğ‘¢1(x1),â„ğ‘¢1(x2),...,â„ğ‘¢ğ‘‡(xğ‘‡ğ¾)]âŠ¤âˆˆRğ‘‡ğ¾.
The purpose of the term ğ‘†is to provide an upper bound on the
optimal parameters in the context of NTK regression. However,
itâ€™s important to note that the value of ğ‘†becomes unbounded (i.e.,
âˆ) when the matrix Hbecomes singular. This singularity can be
induced by an adversary who creates two identical or parallel con-
texts, causing problems in their analysis.
The second complexity term is the effective dimension Ëœğ‘‘, defined
aseğ‘‘=log det(I+H)
log(1+ğ‘‡ğ¾), which describes the actual underlying dimen-
sion in the RKHS space spanned by NTK. The following lemma is
to show an upper bound of ğ‘†âˆ—
ğ‘‡ğ¾under the same assumption.
Lemma 5.3. Suppose Assumption 5.1 and conditions in Theorem 5.1
holds where ğ‘šâ‰¥eÎ©(poly(ğ‘‡,ğ¿)Â·ğ¾ğ‘›ğœ†âˆ’1
0log(1/ğ›¿)). With probability
at least 1âˆ’ğ›¿over the initialization, there exists ğœƒâ€²âˆˆğµ(ğœƒ0,eÎ©(ğ‘‡3/2)),
such that
E[ğ‘†âˆ—
ğ‘‡ğ¾]â‰¤E[ğ‘‡ğ¾âˆ‘ï¸
ğ‘¡=1Lğ‘¡(ğœƒâ€²)]â‰¤eOâˆšï¸
eğ‘‘+ğ‘†2
Â·eğ‘‘.
Lemma 5.3 provides an upper bound for ğ‘†âˆ—
ğ‘‡ğ¾by settingğ‘…=
eÎ©(ğ‘‡3/2). Subsequently, by applying the Hoeffding-Azuma inequal-
ity overğ‘†âˆ—
ğ‘‡ğ¾and replacing ğ‘†âˆ—
ğ‘‡ğ¾with this upper bound, Theorem 5.1
can be reformulated as eO(âˆšï¸
eğ‘‘ğ‘‡(ğ‘†+âˆšï¸
eğ‘‘))for a single neural banditoreO(âˆšï¸ƒ
ğ‘eğ‘‘ğ‘‡(ğ‘†+âˆšï¸
eğ‘‘))forğ‘›users (CNB problem). This transforma-
tion implies that Theorem 5.1 is at least as good as the SOTA upper
bounds represented by Eq. (6).
6 Experiments
In this section, we evaluate M-CNB â€™s empirical performance on
both online recommendation and classification scenarios. Our source
code are anonymously available at https://anonymous.4open.science/
r/Mn-C35C/ .
Recommendation datasets. We use four public datasets, Ama-
zon [ 45], Facebook [ 37], Movielens [ 25], and Yelp1, to evaluate
M-CNB â€™s ability in discovering and exploiting user clusters to im-
prove the recommendation performance. Amazon is an E-commerce
recommendation dataset consisting of 883636 review ratings. Face-
book is a social recommendation dataset with 88234 links. Movie-
Lens is a movie recommendation dataset consisting of 25million
reviews between 1.6Ã—105users and 6Ã—104movies. Yelp is a shop
recommendation dataset released in the Yelp dataset challenge,
composed of 4.7 million review entries made by 1.18million users
towards 1.57Ã—105merchants. For these four datasets, we extract
ratings in the reviews and build the rating matrix by selecting the
top10000 users and top 10000 items (friends, movies, shops) with
the most rating records. Then, we use the singular-value decompo-
sition (SVD) to extract a normalized 10-dimensional feature vector
for each user and item. The goal of this problem is to select the item
with good ratings. Given an item and a specific user, we generate
the reward by using the userâ€™s rating stars for this item. If the userâ€™s
rating is more than 4 stars (5 stars total), its reward is 1; Otherwise,
its reward is 0. Here, we use pre-clustering (K-means) to form the
user pool with 50 users (pre-clusters). Then, in each round, a user
ğ‘¢ğ‘¡is randomly drawn from the user pool. For the arm pool, we
randomly choose one restaurant (movie) rated by ğ‘¢ğ‘¡with reward
1and randomly pick the other 9restaurants (movies) rated by ğ‘¢ğ‘¡
with 0reward. With each restaurant or movie corresponding to an
arm, the goal for the learner is to pick the arm with the highest
reward.
Classification datasets. In our online classification with bandit
feedback experiments, we utilized a range of well-known classifi-
cation datasets, including Mnist [ 36], Notmnist [ 11], Cifar10 [ 34],
Emnist (Letter) [ 14], Fashion [ 58], as well as the Shuttle, Mush-
room, and MagicTelescope (MT) datasets [ 19]. Here, we provide
some preliminaries for this setup. In the round ğ‘¡âˆˆ [ğ‘‡], given
an instance xğ‘¡âˆˆRğ‘‘drawn from some distribution, we aim to
classify xğ‘¡amongğ¾classes. xğ‘¡is first transformed into ğ¾long
vectors: xğ‘¡,1=(xâŠ¤,0,..., 0)âŠ¤,xğ‘¡,2=(0,xâŠ¤,..., 0)âŠ¤,..., xğ‘¡,ğ¾=
(0,0,..., xâŠ¤)âŠ¤âˆˆRğ‘‘ğ¾, matchingğ¾classes respectively. The index
of the arm that the learner selects is the class predicted by the
learner. Then, the reward is defined as 1ifxğ‘¡belongs to this class;
otherwise, the reward is 0. In other words, each arm represents a
specific class. For example, xğ‘¡,1is only presented to Class 1; xğ‘¡,2
is only presented to Class 2. This problem has been studied in al-
most all the neural bandit works [ 6,31,64,65]. Compared to these
works, we aim to learn the correlations among classes to improve
performance. Thus, we formulate one class as a user (bandit) (i.e.,
1https://www.yelp.com/dataset
 
100Meta Clustering of Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 2: Regret comparison on recommendation datasets.
a user in the recommendation scenario) and all the samples be-
longing to this class are deemed as the data of this user. This set
of experiments aims to evaluate M-CNB â€™s ability to learn various
non-linear reward functions, as well as the ability of discovering
and exploiting the correlations among classes. Additionally, we
extended the evaluation by combining the Mnist and Notmnist
datasets to simulate a more challenging application scenario, given
that both datasets involve 10-class classification problems.
Baselines. We compare M-CNB with SOTA baselines as follows:
(1) CLUB [ 23] clusters users based on the connected components
in the user graph and refines the groups incrementally; (2) COFIBA
[39] clusters on both the user and arm sides based on the evolving
graph, and chooses arms using a UCB-based exploration strategy;
(3) SCLUB [ 40] improves the algorithm CLUB by allowing groups to
merge and split, to enhance the group representation; (4) LOCB [ 4]
uses the seed-based clustering and allows groups to be overlapped.
Then, it chooses the best group candidates for arm selection; (5)
NeuUCB-ONE [ 65] uses one neural network to formulate all users,
and selects arms via a UCB-based recommendation; (6) NeuUCB-
IND [ 65] uses one neural network to formulate one user separately
(totallyğ‘›networks) and applies the same strategy to choose arms.
(7) NeuA+U: we concatenate the arm features and user features
together and treat them as the input for the neural network. Note
that the user features are only available on Movielens and Yelp
datasets. Thus, we only report the results on these two datasets for
NeuA+U. (8) NeuralLinear: following the existing work [ 44,63]. A
shared neural network is built for all users to get an embedding for
each arm. which is fed into the linear bandit with the clustering pro-
cedure. Since LinUCB [ 38] and KernalUCB [ 53] are outperformed
by the above baselines, we will not include them for comparison.
Configurations. We run all experiments on a server with the
NVIDIA Tesla V100 SXM2 GPU. For all the baselines, they all have
two parameters: ğœ†that is to tune the regularization at initialization
andğ›¼which is to adjust the UCB value. To find their best perfor-
mance, we conduct the grid search for ğœ†andğ›¼over(0.01,0.1,1)
and(0.0001,0.001,0.01,0.1)respectively. For LOCB, the number
of random seeds is set as 20following their default setting. For
M-CNB , we setğœˆas5andğ›¾as0.4to tune the cluster, and ğ‘†is set
to1. To ensure fair comparison, for all neural methods, we use the
same simple neural network with 2fully-connected layers, and the
Figure 3: Regret comparison on Mnist and Notmnist, Cifar10,
EMNIST(Letter), and Shuttle.
widthğ‘šis set as 100. To save the running time, we train the neural
networks every 10 rounds in the first 1000 rounds and train the neu-
ral networks every 100 rounds afterwards. In our implementation,
we use Adam [ 32] for SGD. In the end, we choose the best results
for the comparison and report the mean and standard deviation
(shadows in figures) of 10runs for all methods.
Results. Figure 2-4 reports the average regrets of all the methods
on the recommendation and classification datasets. Figure 2 displays
the regret curves for all the methods evaluated on the MovieLens
and Yelp datasets. In these experiments, M-CNB consistently out-
performs all the baseline methods, showcasing its effectiveness.
Specifically, M-CNB improves performance by 5.8% on Amazon,
7.7 % on Facebook, 8.1 % on MovieLens, and 2.0 % on Yelp, com-
pared to the best-performing baseline. These superior results can
be attributed to two specific advantages that M-CNB offers over
the two types of baseline methods. In contrast to conventional lin-
ear clustering of bandits (CLUB, COFIBA, SCLUB, LOCB), M-CNB
has the capability to learn non-linear reward functions. This flexi-
bility allows M-CNB to excel in scenarios where user preferences
exhibit non-linearity in terms of arm contexts. In comparison to neu-
ral bandits (NeuUCB-ONE, NeuUCB-IND, NeuA+U, NeuralLinear),
M-CNB takes advantage of user clustering and leverages the corre-
lations within these clusters, as captured by the meta-learner. This
exploitation of inter-user correlations enables M-CNB to enhance
recommendation performance. By combining these advantages,
M-CNB achieves substantial improvements over the MovieLens
and Yelp datasets, demonstrating its prowess in addressing collab-
orative neural bandit problems and enhancing recommendation
systems. Note M-CNB â€™s regret rate decreases on these four datasets,
even though the "linear-like" behavior in Figure 2.
Figures 3 and 4 show the regret comparison on ML datasets,
where M-CNB outperforms all the baselines. Here, each class can be
thought of as a user in these datasets. The ML datasets exhibit non-
linear reward functions concerning the arms, making them challeng-
ing for conventional clustering of linear bandits (CLUB, COFIBA,
SCLUB, LOCB). These methods may struggle to capture the non-
linearity of the reward functions, resulting in sub-optimal perfor-
mance. Among the neural baselines, NeuUCB-ONE benefits from
the representation power of neural networks. However, it treats all
users (classes) as a single cluster, overlooking the variations and
 
101KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
Figure 4: Regret comparison on Mnist, Fashion-Mnist, Mush-
room, and MagicTelescope.
correlations among them. On the other hand, NeuUCB-IND deals
with users individually, neglecting the potential benefits of leverag-
ing collaborative knowledge among users. NeuralLinear uses one
shared embedding (neural network) for all users, which may not be
the optimal solution given the user heterogeneity. M-CNB â€™s advan-
tage lies in its ability to exploit shared knowledge within clusters
of classes that exhibit strong correlations. It leverages this common
knowledge to improve its performances across different tasks, as it
can efficiently adapt its meta-learner based on past clusters. More
discussions with baselines are placed in Appendix B.1.
1000 2000 3000 4000 5000
Running Time(s)45005000550060006500700075008000RegretMovieLens
'CLUB'
'COFIBA'
'SLUCB'
'LOCB'
'NeuUCB-IND'
'NeuUCB-ONE'
'NeuA+U'
'NeuralLinear'
'M-CNB'
2000 4000 6000 8000 10000
Running Time(s)300040005000600070008000RegretMnist
'CLUB'
'COFIBA'
'SLUCB'
'LOCB'
'NeuUCB-IND'
'NeuUCB-ONE'
'NeuralLinear'
'M-CNB'
Figure 5: Running time vs. Performance for all methods.
Running time analysis. Figure 5 demonstrates the trade-off
between running time and cumulative regret on both the Movie-
lens and Mnist datasets, where the unit of the x-axis is seconds. As
M-CNB is under the framework of neural bandits, we use NeuUCB-
ONE as the baseline (1.0). The results indicate that M-CNB takes
comparable computation costs (1 .6Ã—on Movielens and 2.9Ã—on
Mnist) to NeuUCB-ONE while substantially improving performance.
This suggests that M-CNB can be deployed to significantly enhance
performance when the user correlation is a crucial factor (e.g.,
recommendation tasks), with only a moderate increase in computa-
tional overhead.
Now, let us delve into the analysis of the running time for M-
CNB. Specifically, we can break down the computational cost of
M-CNB into three main components: (1) Clustering : to form the
user cluster (Line 7 in Algorithm 1); (2) Meta adaptation : to train
a meta-model (Algorithm 2); (3) User-learner training : to train the
user-learners (Lines 14-18 in Algorithm 1).
Table 1 provides the breakdown of the time cost for the three
main components of M-CNB . Clustering: This partâ€™s time cost
grows linearly with the number of users ğ‘›because it has a timeTable 1: Breakdown time cost for M-CNB in a round (seconds)
with different number of users on MovieLens.
n
=500 n
= 5000 n
= 10000 n
= 20000
Clustering 0.006 0.057 0.113 0.228
Meta
adaptation 0.003 0.002 0.003 0.003
User-learner
training 0.067 0.068 0.096 0.078
complexity of ğ‘‚(ğ‘›)for clustering. As discussed previously, lever-
aging pre-clustering techniques can significantly reduce this cost.
It is also important to note that all clustering methods inherently
have this time cost, and it is challenging to further reduce it. Meta
adaptation: Due to the benefits of meta-learning, this part requires
only a few steps of gradient descent to train a model with good
performances. Consequently, the time cost for meta-adaptation is
relatively trivial. User-learner training: While this part may require
more SGD steps to converge, it is important to recognize that it is
primarily used for clustering purposes. Therefore, the frequency of
training user-learners can be reduced to decrease the cost. In sum-
mary, M-CNB aims to achieve the clustering of neural bandits and
can manage to strike a good balance between the computational
cost and the model performance.
0 2000 4000 6000 8000 10000
Rounds010002000300040005000RegretAblation Study on MovieLens
 = 1.1
 = 1.5
 = 3
 = 5
 = 10
0 2000 4000 6000 8000 10000
Rounds010002000300040005000RegretAblation Study on MovieLens
 = 0.1
 = 0.2
 = 0.4
 = 0.8
Figure 6: Sensitivity study for ğœˆandğ›¾on MovieLens Dataset.
Study forğœˆandğ›¾. Figure 6 illustrates the performance variation
ofM-CNB concerning the parameters ğœˆandğ›¾. For the sake of
discussion, we will focus on ğœˆbut note that ğ›¾plays a similar role in
terms of controlling clustering. When ğœˆis set to a value like 1.1, the
exploration range of clusters becomes very narrow. In this case, the
inferred cluster size in each round, |bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)|, tends to be small.
This means that the inferred cluster bNğ‘¢ğ‘¡(xğ‘¡,ğ‘–)is more likely to
consist of true members of ğ‘¢ğ‘¡â€™s relative cluster. However, there is a
drawback regarding this narrow exploration range: it might result
in missing out on potential cluster members in the initial phases of
learning. On the other hand, setting ğœˆto a larger value, like ğœˆ=5,
widens the exploration range of clusters. This means that there
are more opportunities to include a larger number of members in
the inferred cluster. However, continuously increasing ğœˆdoes not
necessarily lead to improved performances, because excessively
large values of ğœˆmight result in inferred clusters that include non-
collaborative users and clustering noise. Therefore, in practice, we
recommend to set ğœˆto a relatively large number (e.g., ğœˆ=5) that
strikes a balance between the exploration and exploitation.
Study forğ‘†. Figure 7 provides insight into the sensitivity of
M-CNB concerning the parameter ğ‘†in Algorithm 1. It is evident
that M-CNB exhibits robust performance across a range of values
forğ‘†. This robustness can be attributed to the strong discriminabil-
ity of the meta-learner and the derived upper bound. Even with
varyingğ‘†values, the relative order of arms ranked by M-CNB ex-
periences only slightly changes. This consistency in arm rankings
 
102Meta Clustering of Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 2000 4000 6000 8000 10000
Rounds0500100015002000250030003500RegretAblation Study for S
S = 0.1
S = 1
S = 10
S = 100
Figure 7: Sensitivity study for ğ‘†on Mnist Dataset.
demonstrates that M-CNB is capable of maintaining the robust
performance, which in turn reduces the need for extensive hyper-
parameter tuning.
7 Conclusion
In this paper, we study the Cluster of Neural Bandits problem to
incorporate correlation in bandits with generic reward assumptions.
Then, we propose a novel algorithm, M-CNB , to solve this problem,
where a meta-learner is assigned to represent and rapidly adapt to
dynamic clusters, along with an informative UCB-type exploration
strategy. Moreover, we provide the instance-dependent regret anal-
ysis for M-CNB . In the end, to demonstrate the effectiveness of
M-CNB , we conduct extensive experiments to evaluate its empiri-
cal performance against strong baselines on recommendation and
classification datasets.
Acknowledgement
This work is supported by National Science Foundation under
Award No. IIS-2002540, and Agriculture and Food Research Initia-
tive (AFRI) grant no. 2020-67021-32799/project accession no.1024178
from the USDA National Institute of Food and Agriculture. The
views and conclusions are those of the authors and should not
be interpreted as representing the official policies of the funding
agencies or the government.
References
[1]Y. Abbasi-Yadkori, D. PÃ¡l, and C. SzepesvÃ¡ri. Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems, pages
2312â€“2320, 2011.
[2]Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning, pages
242â€“252. PMLR, 2019.
[3]Y. Ban and J. He. Generic outlier detection in multi-armed bandit. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pages 913â€“923, 2020.
[4]Y. Ban and J. He. Local clustering in contextual multi-armed bandits. In Proceed-
ings of the Web Conference 2021, pages 2335â€“2346, 2021.
[5]Y. Ban, J. He, and C. B. Cook. Multi-facet contextual bandits: A neural network
perspective. In The 27th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 35â€“45, 2021.
[6]Y. Ban, Y. Yan, A. Banerjee, and J. He. Ee-net: Exploitation-exploration neural
networks in contextual bandits. arXiv preprint arXiv:2110.03177, 2021.
[7]Y. Ban, Y. Zhang, H. Tong, A. Banerjee, and J. He. Improved algorithms for
neural active learning. Advances in Neural Information Processing Systems, 35:
27497â€“27509, 2022.
[8]Y. Ban, Y. Qi, and J. He. Neural contextual bandits for personalized recommenda-
tion. arXiv preprint arXiv:2312.14037, 2023.
[9]Y. Ban, Y. Yan, A. Banerjee, and J. He. Neural exploitation and exploration of
contextual bandits. arXiv preprint arXiv:2305.03784, 2023.
[10] Y. Ban, I. Agarwal, Z. Wu, Y. Zhu, K. Weldemariam, H. Tong, and J. He. Neural
active learning beyond bandits. arXiv preprint arXiv:2404.12522, 2024.
[11] Y. Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available:
http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2, 2011.[12] Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide
and deep neural networks. Advances in Neural Information Processing Systems,
32:10836â€“10846, 2019.
[13] M. Chen, C. Xu, V. Gatto, D. Jain, A. Kumar, and E. Chi. Off-policy actor-critic
for recommender systems. In Proceedings of the 16th ACM Conference on Recom-
mender Systems, pages 338â€“349, 2022.
[14] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. Emnist: Extending mnist to
handwritten letters. In 2017 international joint conference on neural networks
(IJCNN), pages 2921â€“2926. IEEE, 2017.
[15] Z. Dai, Y. Shu, A. Verma, F. X. Fan, B. K. H. Low, and P. Jaillet. Federated neural
bandit. arXiv preprint arXiv:2205.14309, 2022.
[16] V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under
bandit feedback. 2008.
[17] R. Deb, Y. Ban, S. Zuo, J. He, and A. Banerjee. Contextual bandits with online
neural regression. arXiv preprint arXiv:2312.07145, 2023.
[18] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima
of deep neural networks. In International Conference on Machine Learning, pages
1675â€“1685. PMLR, 2019.
[19] D. Dua and C. Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
[20] P. Dutta, M. Kit, J. S. Kim, M. Mascaro, et al. Automl for contextual bandits. arXiv
preprint arXiv:1909.03212, 2019.
[21] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pages 1126â€“
1135. PMLR, 2017.
[22] C. Gao, K. Huang, J. Chen, Y. Zhang, B. Li, P. Jiang, S. Wang, Z. Zhang, and
X. He. Alleviating matthew effect of offline reinforcement learning in interactive
recommendation. arXiv preprint arXiv:2307.04571, 2023.
[23] C. Gentile, S. Li, and G. Zappella. Online clustering of bandits. In International
Conference on Machine Learning, pages 757â€“765, 2014.
[24] C. Gentile, S. Li, P. Kar, A. Karatzoglou, G. Zappella, and E. Etrue. On context-
dependent clustering of bandits. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 1253â€“1262. JMLR. org, 2017.
[25] F. M. Harper and J. A. Konstan. The movielens datasets: History and context.
Acm transactions on interactive intelligent systems (tiis), 5(4):1â€“19, 2015.
[26] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua. Neural collaborative
filtering. In Proceedings of the 26th international conference on world wide web,
pages 173â€“182, 2017.
[27] J. Hong, B. Kveton, M. Zaheer, Y. Chow, A. Ahmed, and C. Boutilier. Latent bandits
revisited. Advances in Neural Information Processing Systems, 33:13423â€“13433,
2020.
[28] J. Hong, B. Kveton, M. Zaheer, and M. Ghavamzadeh. Hierarchical bayesian
bandits. In International Conference on Artificial Intelligence and Statistics , pages
7724â€“7741. PMLR, 2022.
[29] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing
systems, pages 8571â€“8580, 2018.
[30] Y. Jia, W. Zhang, D. Zhou, Q. Gu, and H. Wang. Learning neural contextual
bandits through perturbed rewards. In International Conference on Learning
Representations, 2022.
[31] P. Kassraie and A. Krause. Neural contextual bandits without regret. In Interna-
tional Conference on Artificial Intelligence and Statistics, pages 240â€“278. PMLR,
2022.
[32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
[33] N. Korda, B. SzÃ¶rÃ©nyi, and L. Shuai. Distributed clustering of linear bandits
in peer to peer networks. In Journal of machine learning research workshop
and conference proceedings, volume 48, pages 1301â€“1309. International Machine
Learning Societ, 2016.
[34] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny
images. 2009.
[35] B. Kveton, M. Konobeev, M. Zaheer, C.-w. Hsu, M. Mladenov, C. Boutilier, and
C. Szepesvari. Meta-thompson sampling. In International Conference on Machine
Learning, pages 5884â€“5893. PMLR, 2021.
[36] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, 1998.
[37] J. Leskovec and J. Mcauley. Learning to discover social circles in ego networks.
Advances in neural information processing systems, 25, 2012.
[38] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international
conference on World wide web, pages 661â€“670, 2010.
[39] S. Li, A. Karatzoglou, and C. Gentile. Collaborative filtering bandits. In Proceedings
of the 39th International ACM SIGIR conference on Research and Development in
Information Retrieval, pages 539â€“548, 2016.
[40] S. Li, W. Chen, S. Li, and K.-S. Leung. Improved algorithm on online clustering
of bandits. In Proceedings of the 28th International Joint Conference on Artificial
Intelligence, pages 2923â€“2929. AAAI Press, 2019.
 
103KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
[41] Z. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng. Bbq-networks: Efficient
exploration in deep reinforcement learning for task-oriented dialogue systems.
InProceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[42] O.-A. Maillard and S. Mannor. Latent bandits. In International Conference on
Machine Learning, pages 136â€“144. PMLR, 2014.
[43] T. M. McDonald, L. Maystre, M. Lalmas, D. Russo, and K. Ciosek. Impatient bandits:
Optimizing recommendations for the long-term without delay. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,
pages 1687â€“1697, 2023.
[44] O. Nabati, T. Zahavy, and S. Mannor. Online limited memory neural-linear bandits
with likelihood matching. In International Conference on Machine Learning, pages
7905â€“7915. PMLR, 2021.
[45] J. Ni, J. Li, and J. McAuley. Justifying recommendations using distantly-labeled
reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical
methods in natural language processing and the 9th international joint conference
on natural language processing (EMNLP-IJCNLP), pages 188â€“197, 2019.
[46] Y. Qi, Y. Ban, and J. He. Neural bandit with arm group graph. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,
pages 1379â€“1389, 2022.
[47] Y. Qi, Y. Ban, and J. He. Graph neural bandits. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1920â€“1931,
2023.
[48] Y. Qi, Y. Ban, T. Wei, J. Zou, H. Yao, and J. He. Meta-learning with neural bandit
scheduler. Advances in Neural Information Processing Systems, 36, 2023.
[49] C. Riquelme, G. Tucker, and J. Snoek. Deep bayesian bandits showdown: An
empirical comparison of bayesian deep networks for thompson sampling. arXiv
preprint arXiv:1802.09127, 2018.
[50] M. R. Santana, L. C. Melo, F. H. Camargo, B. BrandÃ£o, A. Soares, R. M. Oliveira,
and S. Caetano. Contextual meta-bandit for recommender systems selection. In
Fourteenth ACM Conference on Recommender Systems, pages 444â€“449, 2020.
[51] M. Simchowitz, C. Tosh, A. Krishnamurthy, D. J. Hsu, T. Lykouris, M. Dudik,
and R. E. Schapire. Bayesian decision-making under misspecified priors with
applications to meta-learning. Advances in Neural Information Processing Systems,
34:26382â€“26394, 2021.
[52] X. Su and T. M. Khoshgoftaar. A survey of collaborative filtering techniques.
Advances in artificial intelligence, 2009, 2009.
[53] M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis
of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.
[54] R. Wan, L. Ge, and R. Song. Metadata-based multi-task bandits with bayesian
hierarchical models. Advances in Neural Information Processing Systems, 34:
29655â€“29668, 2021.
[55] L. Wang, Q. Cai, Z. Yang, and Z. Wang. On the global optimality of model-agnostic
meta-learning. In International Conference on Machine Learning, pages 9837â€“9846.
PMLR, 2020.
[56] Z. Wang, P. Awasthi, C. Dann, A. Sekhari, and C. Gentile. Neural active learning
with performance guarantees. Advances in Neural Information Processing Systems,
34:7510â€“7521, 2021.
[57] J. Wu, C. Zhao, T. Yu, J. Li, and S. Li. Clustering of conversational bandits for user
preference learning and elicitation. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management, pages 2129â€“2139, 2021.
[58] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747,
2017.
[59] P. Xu, Z. Wen, H. Zhao, and Q. Gu. Neural contextual bandits with deep repre-
sentation and shallow exploration. arXiv preprint arXiv:2012.01780, 2020.
[60] W. Xue, Q. Cai, R. Zhan, D. Zheng, P. Jiang, and B. An. Resact: Reinforcing
long-term engagement in sequential recommendation with residual actor. arXiv
preprint arXiv:2206.02620, 2022.
[61] L. Yang, B. Liu, L. Lin, F. Xia, K. Chen, and Q. Yang. Exploring clustering of
bandits for online recommendation system. In Fourteenth ACM Conference on
Recommender Systems, pages 120â€“129, 2020.
[62] H. Yao, Y. Wei, J. Huang, and Z. Li. Hierarchically structured meta-learning. In
International Conference on Machine Learning, pages 7045â€“7054. PMLR, 2019.
[63] T. Zahavy and S. Mannor. Deep neural linear bandits: Overcoming catastrophic
forgetting through likelihood matching. arXiv preprint arXiv:1901.08612, 2019.
[64] W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural thompson sampling. In International
Conference on Learning Representations, 2021.
[65] D. Zhou, L. Li, and Q. Gu. Neural contextual bandits with ucb-based exploration.
InInternational Conference on Machine Learning, pages 11492â€“11502. PMLR, 2020.
A Proof Details of Theorem 5.1
Our proof technique is different from related works. [ 4,23,24,39,
40] are built on the classic linear bandit framework and [ 31,64,65]
utilize the kernel-based analysis in the NTK regime. In contrast,
we use the generalization bound of user-learner to bound the errorincurred in each round and bridge meta-learner with user-learner
by bounding their distance, which leads to our final regret bound.
Specifically, we decompose the regret of ğ‘‡rounds into three key
terms, where the first term is the error induced by user learner
ğœƒğ‘¢, the second term is the distance between user learner and meta
learner, and the third term is the error induced by the meta learner
Î˜. Then, Lemma A.10 provides an upper bound for the first term.
Lemma A.10 is an extension of Lemma A.7, which is the key to
removing the input dimension. Lemma A.7 has two terms with
the complexityO(âˆš
ğ‘‡), where the first term is the training error
induced by a class of functions around initialization, the second term
is the deviation induced by concentration inequality for ğ‘“(Â·;ğœƒğ‘¢).
Lemma A.11 bounds the distance between user-learner and meta-
learner. Lemma A.12 bounds the error induced by the meta learner
using triangle inequality bridged by the user learner. Bounding
these three terms completes the proof.
A.1 Analysis for user-learner
Following [ 2,12], given an instance xâˆˆEğ‘‘withâˆ¥xâˆ¥2=1, we
define the outputs of hidden layers of the neural network (Eq. (4)):
h0=x,hğ‘™=ğœ(Wğ‘™hğ‘™âˆ’1),ğ‘™âˆˆ[ğ¿âˆ’1].
Then, we define the binary diagonal matrix functioning as ReLU:
Dğ‘™=diag( 1{(Wğ‘™hğ‘™âˆ’1)1},..., 1{(Wğ‘™hğ‘™âˆ’1)ğ‘š}),ğ‘™âˆˆ[ğ¿âˆ’1].
Accordingly, given an input x, the neural network (Eq. (4)) is repre-
sented by
ğ‘“(xğ‘¡;ğœƒorÎ˜)=Wğ¿(ğ¿âˆ’1Ã–
ğ‘™=1Dğ‘™Wğ‘™)x,
and
âˆ‡Wğ‘™ğ‘“=(
[hğ‘™âˆ’1Wğ¿(Ãğ¿âˆ’1
ğœ=ğ‘™+1DğœWğœ)]âŠ¤,ğ‘™âˆˆ[ğ¿âˆ’1]
hâŠ¤
ğ¿âˆ’1,ğ‘™=ğ¿.
Given a reward ğ‘Ÿâˆˆ[0,1], defineL(ğœƒ)=(ğ‘“(x;ğœƒ)âˆ’ğ‘Ÿ)2/2. Then,
we have the following auxiliary lemmas.
Lemma A.1. Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the conditions in Theorem
5.1. With probability at least 1âˆ’O(ğ‘‡ğ¾ğ¿)Â·exp(âˆ’Î©(ğ‘šğœ”2/3ğ¿))over
the random initialization, for all ğ‘¡âˆˆ[ğ‘‡],ğ‘–âˆˆ[ğ‘˜],ğœƒ(orÎ˜) satisfying
âˆ¥ğœƒâˆ’ğœƒ0âˆ¥2â‰¤ğœ”withğœ”â‰¤O(ğ¿âˆ’9/2[logğ‘š]âˆ’3), it holds uniformly that
(1),|ğ‘“(x;ğœƒ)|â‰¤O( 1).
(2),âˆ¥âˆ‡ğœƒğ‘“(x;ğœƒ)âˆ¥2â‰¤O(âˆš
ğ¿).
(3),âˆ¥âˆ‡ğœƒL(ğœƒ)âˆ¥2â‰¤O(âˆš
ğ¿)
Lemma A.2. Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the conditions in Theorem 5.1.
With probability at least 1âˆ’O(ğ‘‡ğ¾ğ¿)Â·exp(âˆ’Î©(ğ‘šğœ”2/3ğ¿)), for all
ğ‘¡âˆˆ[ğ‘‡],ğ‘–âˆˆ[ğ‘˜],ğœƒ,ğœƒâ€²(orÎ˜,Î˜â€²) satisfyingâˆ¥ğœƒâˆ’ğœƒ0âˆ¥2,âˆ¥ğœƒâ€²âˆ’ğœƒ0âˆ¥2â‰¤ğœ”
withğœ”â‰¤O(ğ¿âˆ’9/2[logğ‘š]âˆ’3), it holds uniformly that
|ğ‘“(x;ğœƒ)âˆ’ğ‘“(x;ğœƒâ€²)âˆ’âŸ¨â–½ğœƒâ€²ğ‘“(x;ğœƒâ€²),ğœƒâˆ’ğœƒâ€²âŸ©|â‰¤O(ğœ”1/3ğ¿2âˆšï¸
logğ‘š)âˆ¥ğœƒâˆ’ğœƒâ€²âˆ¥2.
Lemma A.3. Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the conditions in Theorem
5.1. With probability at least 1âˆ’O(ğ‘‡ğ¾ğ¿)Â·exp(âˆ’Î©(ğ‘šğœ”2/3ğ¿)), for
allğ‘¡âˆˆ[ğ‘‡],ğ‘–âˆˆ[ğ‘˜],ğœƒ,ğœƒâ€²satisfyingâˆ¥ğœƒâˆ’ğœƒ0âˆ¥2,âˆ¥ğœƒâ€²âˆ’ğœƒ0âˆ¥2â‰¤ğœ”with
ğœ”â‰¤O(ğ¿âˆ’9/2[logğ‘š]âˆ’3), it holds uniformly that
(1) |ğ‘“(x;ğœƒ)âˆ’ğ‘“(x;ğœƒâ€²)|â‰¤O(ğœ”âˆš
ğ¿)+O(ğœ”4/3ğ¿2âˆšï¸
logğ‘š)
(7)
 
104Meta Clustering of Neural Bandits KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Lemma A.4 (Almost Convexity). LetLğ‘¡(ğœƒ)=(ğ‘“(xğ‘¡;ğœƒ)âˆ’ğ‘Ÿğ‘¡)2/2.
Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the conditions in Theorem 5.1. For any ğœ–>
0, with probability at least 1âˆ’O(ğ‘‡ğ¾ğ¿2)exp[âˆ’Î©(ğ‘šğœ”2/3ğ¿)]over
randomness of ğœƒ1, for allğ‘¡âˆˆ[ğ‘‡], andğœƒ,ğœƒâ€²satisfyingâˆ¥ğœƒâˆ’ğœƒ0âˆ¥2â‰¤ğœ”
andâˆ¥ğœƒâ€²âˆ’ğœƒ0âˆ¥2â‰¤ğœ”withğœ”â‰¤ O(ğ¿âˆ’6[logğ‘š]âˆ’3/2ğœ–3/4), it holds
uniformly that
Lğ‘¡(ğœƒâ€²)â‰¥Lğ‘¡(ğœƒ)+âŸ¨âˆ‡ğœƒLğ‘¡(ğœƒ),ğœƒâ€²âˆ’ğœƒâŸ©âˆ’ğœ–.
Lemma A.5 (User Trajectory Ball). Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the
conditions in Theorem 5.1. With probability at least 1âˆ’O(ğ‘‡ğ¾ğ¿2)
exp[âˆ’Î©(ğ‘šğœ”2/3ğ¿)]over randomness of ğœƒ0, for anyğ‘…>0, it holds
uniformly that
âˆ¥ğœƒğ‘¡âˆ’ğœƒ0âˆ¥2â‰¤O(ğ‘…/ğ‘š1/4)â‰¤O(ğ¿âˆ’6[logğ‘š]âˆ’3/2ğ‘‡âˆ’3/4),ğ‘¡âˆˆ[ğ‘‡].
Lemma A.6 (Instance-dependent Loss Bound). LetLğ‘¡(ğœƒ)=(ğ‘“(xğ‘¡;ğœƒ)âˆ’
ğ‘Ÿğ‘¡)2/2. Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the conditions in Theorem 5.1. With
probability at least 1âˆ’O(ğ‘‡ğ¾ğ¿2)exp[âˆ’Î©(ğ‘šğœ”2/3ğ¿)]over randomness
ofğœƒ0, given anyğ‘…>0it holds that
ğ‘‡âˆ‘ï¸
ğ‘¡=1Lğ‘¡(ğœƒğ‘¡)â‰¤ğ‘‡âˆ‘ï¸
ğ‘¡=1Lğ‘¡(ğœƒâˆ—)+O( 1)+ğ‘‡ğ¿ğ‘…2
âˆšğ‘š. (8)
whereğœƒâˆ—=arg infğœƒâ€²âˆˆğµ(ğœƒ0,ğ‘…)Ãğ‘‡
ğ‘¡=1Lğ‘¡(ğœƒâ€²).
Lemma A.7. For anyğ›¿âˆˆ(0,1),ğ‘…>0, andğ‘š,ğœ‚ 1,ğœ‚2satisfy the
conditions in Theorem 5.1. In a round ğœwhereğ‘¢âˆˆğ‘is serving
user, letğ‘¥ğœbe the selected arm and ğ‘Ÿğœis the corresponding received
reward. Then, with probability at least 1âˆ’ğ›¿over the randomness of
initialization, the cumulative regret induced by ğ‘¢up to round ğ‘‡is
upper bounded by:
1
ğœ‡ğ‘¢
ğ‘‡âˆ‘ï¸
(xğœ,ğ‘Ÿğœ)âˆˆTğ‘¢
ğ‘‡E
ğ‘Ÿğœ|xğœ[|ğ‘“(xğœ;ğœƒğ‘¢
ğœâˆ’1)âˆ’ğ‘Ÿğœ|]
â‰¤âˆšï¸„
ğ‘†âˆ—
ğ‘‡(ğ‘¢)+O( 1)
ğœ‡ğ‘¢
ğ‘‡+O âˆšï¸„
2 log(O(1)/ğ›¿)
ğœ‡ğ‘¢
ğ‘‡!
.
whereğ‘†âˆ—
ğ‘‡(ğ‘¢)= inf
ğœƒâˆˆğµ(ğœƒ0,ğ‘…)Ã
(xğœ,ğ‘Ÿğœ)âˆˆTğ‘¢
ğ‘‡ğ¿ğœ(ğœƒ).
Lemma A.8. For anyğ›¿âˆˆ(0,1),ğ‘…>0, andğ‘š,ğœ‚ 1,ğœ‚2satisfy the
conditions in Theorem 5.1. Suppose bNğ‘¢ğ‘¡(xğ‘¡)=Nğ‘¢ğ‘¡(xğ‘¡),âˆ€ğ‘¡âˆˆ[ğ‘‡].
Afterğ‘‡rounds, with probability 1âˆ’ğ›¿over the random initialization,
the cumulative error induced by the bandit-learners is upper bounded
by
ğ‘‡âˆ‘ï¸
ğ‘¡=11
|Nğ‘¢ğ‘¡(xğ‘¡)|âˆ‘ï¸
ğ‘¢ğ‘¡,ğ‘–âˆˆNğ‘¢ğ‘¡(xğ‘¡)E
ğ‘Ÿğ‘¡|xğ‘¡[|ğ‘“(xğ‘¡;ğœƒğ‘¢ğ‘¡,ğ‘–
ğ‘¡âˆ’1)âˆ’ğ‘Ÿğ‘¡|]
â‰¤âˆšï¸ƒ
ğ‘ğ‘‡Â·ğ‘†âˆ—
ğ‘‡ğ‘˜log(O(ğ›¿âˆ’1))+O( 1)+O(âˆšï¸ƒ
2ğ‘ğ‘‡log(O(ğ›¿âˆ’1))),
whereğ‘†âˆ—
ğ‘‡ğ‘˜= inf
ğœƒâˆˆğµ(ğœƒ0,ğ‘…)Ãğ‘‡ğ‘˜
ğ‘¡=1Lğ‘¡(ğœƒ).
Corollary A.9. For anyğ›¿âˆˆ(0,1),ğ‘…>0, andğ‘š,ğœ‚ 1,ğœ‚2satisfy the
conditions in Theorem 5.1. In a round ğœwhereğ‘¢âˆˆğ‘is the serving
user, letğ‘¥âˆ—ğœbe the arm selected according to Bayes-optimal policy ğœ‹âˆ—:
ğ‘¥âˆ—
ğœ=arg max
xğœ,ğ‘–,ğ‘–âˆˆ[ğ‘˜]â„ğ‘¢(xğœ,ğ‘–),
andğ‘Ÿâˆ—ğœis the corresponding reward. Then, with probability at least
1âˆ’ğ›¿over the randomness of initialization, after ğ‘¡âˆˆ[ğ‘‡]rounds, thecumulative regret induced by ğ‘¢with policyğœ‹âˆ—is upper bounded by:
1
ğœ‡ğ‘¢
ğ‘¡âˆ‘ï¸
(xâˆ—ğœ,ğ‘Ÿâˆ—ğœ)âˆˆTğ‘¢,âˆ—
ğ‘¡E
ğ‘Ÿâˆ—ğœ|xâˆ—ğœ[|ğ‘“(xâˆ—
ğœ;ğœƒğ‘¢,âˆ—
ğœâˆ’1)âˆ’ğ‘Ÿâˆ—
ğœ||ğœ‹âˆ—]
â‰¤âˆšï¸„
ğ‘†âˆ—
ğ‘‡(ğ‘¢)+O( 1)
ğœ‡ğ‘¢
ğ‘¡+O âˆšï¸„
2 log(O(1)/ğ›¿)
ğœ‡ğ‘¢
ğ‘¡!
.
whereğ‘†âˆ—
ğ‘‡(ğ‘¢)= inf
ğœƒâˆˆğµ(ğœƒ0,ğ‘…)Ã
(xâˆ—ğœ,ğ‘Ÿâˆ—ğœ)âˆˆTğ‘¢,âˆ—
ğ‘¡ğ¿ğœ(ğœƒ), andTğ‘¢,âˆ—
ğ‘¡are stored
Bayes-optimal pairs up to round ğ‘¡forğ‘¢, andğœƒğ‘¢,âˆ—
ğœâˆ’1are the parameters
trained onTğ‘¢,âˆ—
ğœâˆ’1according to SGD_User in round ğœâˆ’1.
Corollary A.10. For anyğ›¿âˆˆ(0,1),ğ‘…>0, andğ‘š,ğœ‚ 1,ğœ‚2satisfy the
conditions in Theorem 5.1. In round ğ‘¡âˆˆ[ğ‘‡], givenğ‘¢âˆˆğ‘, let
ğ‘¥âˆ—
ğ‘¡=arg max
xğ‘¡,ğ‘–,ğ‘–âˆˆ[ğ‘˜]â„ğ‘¢(xğ‘¡,ğ‘–)
the Bayes-optimal arm for ğ‘¢andğ‘Ÿâˆ—
ğ‘¡is the corresponding reward. Then,
with probability at least 1âˆ’ğ›¿over the random initialization, after
ğ‘‡rounds, with probability 1âˆ’ğ›¿over the random initialization, the
cumulative error induced by the bandit-learners is upper bounded by:
ğ‘‡âˆ‘ï¸
ğ‘¡=1E
ğ‘Ÿâˆ—
ğ‘¡|xâˆ—
ğ‘¡[|ğ‘“(xâˆ—
ğ‘¡;ğœƒğ‘¢ğ‘¡,âˆ—
ğ‘¡âˆ’1)âˆ’ğ‘Ÿâˆ—
ğ‘¡|]
â‰¤âˆšï¸ƒ
ğ‘ğ‘‡Â·ğ‘†âˆ—
ğ‘‡ğ‘˜+O( 1)+O(âˆšï¸
2ğ‘ğ‘‡log(O(1)/ğ›¿)).
where the expectation is taken over ğ‘Ÿâˆ—ğœconditioned on xâˆ—ğœandğœƒğ‘¢ğ‘¡,âˆ—
ğ‘¡âˆ’1
are the parameters trained on Tğ‘¢ğ‘¡,âˆ—
ğ‘¡âˆ’1according to SGD in round ğ‘¡âˆ’1.
A.2 Bridge Meta-learner and User-learner
For brevity, we use ğ‘”(xğ‘¡;ğœƒğ‘¢
ğ‘¡âˆ’1)to represent the gradient âˆ‡ğœƒğ‘“(xğ‘¡;ğœƒğ‘¢
ğ‘¡âˆ’1).
Lemma A.11. Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the conditions in Theorem
5.1. With probability at least 1âˆ’O(ğ‘›ğ‘‡ğ¾ğ¿2)exp[âˆ’Î©(ğ‘šğœ”2/3ğ¿)]over
randomness of Î˜0, for allğ‘¡âˆˆ[ğ‘‡], anyğ‘¢âˆˆğ‘andÎ˜ğ‘¡returned by
Algorithm 2, for any âˆ¥xğ‘¡âˆ¥2=1, it holds uniformly for Algorithms
1-2 that
|ğ‘“(xğ‘¡;ğœƒğ‘¢
ğ‘¡âˆ’1)âˆ’ğ‘“(xğ‘¡;Î˜ğ‘¡)|â‰¤ğ‘…âˆ¥âˆ‡Î˜ğ‘“(xğ‘¡;Î˜ğ‘¡)âˆ’âˆ‡ğœƒğ‘“(xğ‘¡;ğœƒğ‘¢
0)âˆ¥2
ğ‘š1/4+ğ‘,
(9)
where
ğ‘=O(ğ‘…ğ¿2âˆšï¸
logğ‘š)
ğ‘š1/3+O(ğ¿7/3ğ‘…2âˆšï¸
logğ‘š)
ğ‘š1/2+O(2ğ‘…âˆš
ğ¿)
ğ‘š1/4.
Lemma A.12. Supposeğ‘š,ğœ‚ 1,ğœ‚2satisfy the conditions in Theorem
5.1. Then, with probability at least 1âˆ’ğ›¿over the random initialization,
for anyğ›¿âˆˆ(0,1),ğ‘…>0, afterğ‘¡rounds, the error induced by meta-
learner is upper bounded by:
ğ‘‡âˆ‘ï¸
ğ‘¡=1E
ğ‘Ÿğ‘¡|xğ‘¡[|ğ‘“(xğ‘¡;Î˜ğ‘¡)âˆ’ğ‘Ÿğ‘¡||ğ‘¢ğ‘¡]
â‰¤ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘…âˆ¥ğ‘”(xğ‘¡;Î˜ğ‘¡)âˆ’ğ‘”(xğ‘¡;ğœƒğ‘¢ğ‘¡
0)âˆ¥2
ğ‘š1/4+âˆ‘ï¸
ğ‘¢âˆˆğ‘ğœ‡ğ‘¢
ğ‘‡"
OÂ©Â­Â­
Â«âˆšï¸ƒ
ğ‘†âˆ—
ğ‘‡ğ‘˜+O( 1)
âˆšï¸ƒ
2ğœ‡ğ‘¢
ğ‘‡ÂªÂ®Â®
Â¬
+âˆšï¸„
2 log(O(1)/ğ›¿)
ğœ‡ğ‘¢
ğ‘‡#
.
(10)
where the expectation is taken over ğ‘Ÿğ‘¡conditioned on ğ‘¥ğ‘¡.
 
105KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
NeuUCB-ONE NeuUCB-IND M-CNB
2 layers 6964.1 6911.7 6773.4
4 layers 6944.7 6942.5 6803.2
8 layers 7012.8 6932.2 6878.3
10 layers 6992.4 6987.4 6854.9
Table 4: The cumulative regret of 10000 rounds on Yelp with
the different number of layers.
B Connections with Neural Tangent Kernel
Lemma B.1 (Lemma 5.3 Restated). Supposeğ‘šsatisfies the condi-
tions in Theorem 5.1. With probability at least 1âˆ’ğ›¿over the initial-
ization, there exists ğœƒâ€²âˆˆğµ(ğœƒ0,eÎ©(ğ‘‡3/2)), such that
E[ğ‘†âˆ—
ğ‘‡ğ‘˜]â‰¤ğ‘‡ğ¾âˆ‘ï¸
ğ‘¡=1E[(ğ‘Ÿğ‘¡âˆ’ğ‘“(xğ‘¡;ğœƒâ€²))2/2]
â‰¤Oâˆšï¸ƒ
eğ‘‘log(1+ğ‘‡ğ¾)âˆ’2 logğ›¿+ğ‘†+12
Â·eğ‘‘log(1+ğ‘‡ğ¾).
Lemma B.2. Supposeğ‘šsatisfies the conditions in Theorem 5.1.
With probability at least 1âˆ’ğ›¿over the initialization, there exists
ğœƒâ€²âˆˆğµ(ğœƒ0,eÎ©(ğ‘‡3/2))for allğ‘¡âˆˆ[ğ‘‡], such that
|â„(xğ‘¡)âˆ’ğ‘“(xğ‘¡;ğœƒâ€²)|
â‰¤OÂ©Â­
Â«âˆšï¸„
logdet(Ağ‘¡)
det(I)
âˆ’2 logğ›¿+ğ‘†+1ÂªÂ®
Â¬âˆ¥ğ‘”(xğ‘¡;ğœƒ0)âˆ¥Aâˆ’1
ğ‘¡
+O 
ğ‘‡2ğ¿3âˆšï¸
logğ‘š
ğ‘š1/3!
B.1 Additional Results
Convergence rate on MovieLens and Yelp. Table 2 illustrates the aver-
age cumulative regrets of M-CNB for different time step intervals
of 2000 rounds on the MovieLens and Yelp datasets. Itâ€™s noticeable
that the average regret per round of M-CNB decreases as more
time steps are considered. One plausible explanation for the "linear-
like" curves in the cumulative regrets is that both the MovieLensand Yelp datasets used for recommendation tasks contain inher-
ent noise. This noise can make it challenging for algorithms to
accurately learn the underlying reward mapping function. Con-
sequently, achieving substantial experimental improvements on
these datasets can be quite challenging. In essence, the presence
of noise in the data can lead to fluctuations in the regret curves,
making it appear as if progress is linear rather than exponential
or logarithmic. Despite these challenges, the algorithm, M-CNB ,
continues to make progress in minimizing regret over time.
Rounds 2000 4000 6000 8000 10000
MovieLens 0.4717 0.4555 0.4475 0.4452 0.4442
Yelp 0.7532 0.7395 0.7358 0.7306 0.7269
Table 2: Convergence rate of M-CNB on MovieLens and Yelp
NeuUCB-ONE NeuUCB-IND M-CNB
2 layers 4839.6 7491.0 4447.1
4 layers 5017.2 7503.8 4498.3
8 layers 5033.5 7764.5 4696.1
10 layers 4808.3 7697.4 4624.7
Table 3: The cumulative regret of 10000 rounds on MovieLens
with the different number of layers.
Ablation Study for Network Layers. We run the experiments on
MovieLens and Yelp datasets with the different number of layers
of neural networks and report the results in Table 3 and 4. M-CNB
achieves the best performance in most cases. In this paper, we try to
propose a generic framework to combine meta-learning and bandits
with the neural network approximation. Since the UCB in M-CNB
only depends on the gradient, the neural network can be easily
replaced by other different structures.
 
106