Towards Adaptive Neighborhood for Advancing Temporal
Interaction Graph Modeling
Siwei Zhang
swzhang22@m.fudan.edu.cn
Shanghai Key Laboratory of Data
Science, School of Computer Science,
Fudan University
Shanghai, ChinaXi Chen
Yun Xiongâˆ—
x_chen21@m.fudan.edu.cn
yunx@fudan.edu.cn
Shanghai Key Laboratory of Data
Science, School of Computer Science,
Fudan University
Shanghai, ChinaXixi Wu
21210240043@m.fudan.edu.cn
Shanghai Key Laboratory of Data
Science, School of Computer Science,
Fudan University
Shanghai, China
Yao Zhang
Yongrui Fu
yaozhang@fudan.edu.cn
23210240154@m.fudan.edu.cn
Shanghai Key Laboratory of Data
Science, School of Computer Science,
Fudan University
Shanghai, ChinaYinglong Zhao
zhaoyinglong.zyl@antgroup.com
Ant Group
Shanghai, ChinaJiawei Zhang
jiawei@ifmlab.org
IFM Lab, Department of Computer
Science, University of California,
Davis
CA, USA
Abstract
Temporal Graph Networks (TGNs) have demonstrated their remark-
able performance in modeling temporal interaction graphs. These
works can generate temporal node representations by encoding
the surrounding neighborhoods for the target node. However, an
inherent limitation of existing TGNs is their reliance on fixed, hand-
crafted rules for neighborhood encoding, overlooking the necessity
for an adaptive and learnable neighborhood that can accommo-
date both personalization and temporal evolution across different
timestamps. In this paper, we aim to enhance existing TGNs by
introducing an adaptive neighborhood encoding mechanism. We
present SEAN (Selective Encoding for Adaptive Neighborhood),
a flexible plug-and-play model that can be seamlessly integrated
with existing TGNs, effectively boosting their performance. To
achieve this, we decompose the adaptive neighborhood encoding
process into two phases: (i) representative neighbor selection, and
(ii) temporal-aware neighborhood information aggregation. Specifi-
cally, we propose the Representative Neighbor Selector component,
which automatically pinpoints the most important neighbors for the
target node. It offers a tailored understanding of each nodeâ€™s unique
surrounding context, facilitating personalization. Subsequently, we
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671877propose a Temporal-aware Aggregator, which synthesizes neigh-
borhood aggregation by selectively determining the utilization of
aggregation routes and decaying the outdated information, allowing
our model to adaptively leverage both the contextually significant
and current information during aggregation. We conduct exten-
sive experiments by integrating SEAN into three representative
TGNs, evaluating their performance on four public datasets and
one financial benchmark dataset introduced in this paper. The re-
sults demonstrate that SEAN consistently leads to performance
improvements across all models, achieving SOTA performance and
exceptional robustness.
CCS Concepts
â€¢Information systems â†’Data mining; â€¢Computing method-
ologiesâ†’Learning latent representations; Neural networks .
Keywords
Temporal Graph Networks; Representation Learning; Data Mining
ACM Reference Format:
Siwei Zhang, Xi Chen, Yun Xiong, Xixi Wu, Yao Zhang, Yongrui Fu, Ying-
long Zhao, and Jiawei Zhang. 2024. Towards Adaptive Neighborhood for
Advancing Temporal Interaction Graph Modeling. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671877
1 Introduction
Temporal Interaction Graphs (TIGs) can model the dynamic graph-
structured data in many real-world application scenarios, where
objects are depicted as nodes and timestamped interactions be-
tween them are represented as edges [ 44]. Unlike static graphs,
TIGs exhibit dynamic changes over time. To effectively capture
the dynamic nature of TIGs and facilitate representation learning,
4290
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siwei Zhang, et al.
#!!,!"!",!#!"!$!%!#!&,!'!(!(!$,!&!#!(#!!,!"!",!#!"!$!%!#!&,!'!(!(!$,!&!#!(
(a) Existing TGNs encode fixed neighborhoods.(b) Our model encodes adaptive neighborhoods.At time"!$At time"!$
Figure 1: Comparison between fixed and adaptive neigh-
borhoods during the encoding process. (a) Existing TGNs
[19,37,44] always adopt fixed rules for neighborhood en-
coding, e.g., encode 2-hop neighborhoods. (b) We propose
adaptive neighborhood encoding to facilitate both the per-
sonalized and temporal understanding of a target node.
extensive research has been conducted on developing Temporal
Graph Networks (TGNs) [ 19,24,33,44]. These TGNs can gener-
ate temporal node representations by encoding the neighborhoods
around the target node, thus enabling downstream predictions.
Despite the remarkable success of existing TGNs, a fundamental
weakness inherent in their designs is the reliance on the fixed,
hand-crafted rules for neighborhood encoding, failing to account
for the necessary personalization and the temporal evolution across
different timestamps. For example, when generating the representa-
tion for a target node at a specific timestamp, as shown in Figure 1a,
existing TGNs, like TIGE [ 44], indiscriminately encode the current
2-hop neighborhood. Different from such existing works, we em-
phasize that an adaptive neighborhood encoding mechanism is
crucial for generating more expressive representations.
Personalization. Personalization is essential for TIG represen-
tation learning at both graph-scale and node-scale levels. Differ-
ent TIGs exhibit distinct characteristics [ 18], and a universal, pre-
defined rule for neighborhood encoding is inflexible [ 29]. For ex-
ample, while the ğ‘˜-hop assumption may work well for low-density
TIGs, it could lead to indistinguishable representations in denser
ones [ 30]. Additionally, even within a single TIG, the suitable neigh-
borhoods for nodes can differ significantly. This hypothesis is rea-
sonable due to the diverse neighborhood connectivity patterns and
potential noise. For instance, in financial networks, transaction
patterns of banks arise from different factors [ 17,45], and often
contain noise irrelevant to their primary financial interests [ 32,40].
Temporal evolution. A suitable neighborhood for a target node
should adapt to different timestamps to align with the temporal
evolution of TIGs, necessitating a temporal-aware design for neigh-
borhood encoding. For example, investorsâ€™ preferences may change
according to economic cycles, such as preferring technology stocks
during technological booms while consumer staples during eco-
nomic downturns. Fixed rules for neighborhood encoding cannot
accommodate to such preference shifts in the temporal dimension,
leading to suboptimal representations for effectively capturing these
changing preferences.Based on the aforementioned motivations, as depicted in Fig-
ure 1b, it becomes critical to allow for a more flexible, scalable, and
robust algorithm that performs adaptive neighborhood encoding
obeying both personalization and temporal awareness.
In this paper, we aim to enhance existing TGNs by adaptively
encoding personalized and temporal-aware neighborhoods through
the introduction of a convenient plug-and-play model, referred to
asSEAN (Selective Encoding for Adaptive Neighborhood). Our
SEAN can significantly boost existing TGNs, and it comprises two
main components: (i) Representative Neighbor Selector. To effec-
tively select personalized neighborhoods for nodes within TIGs, we
introduce Representative Neighbor Selector. It refines the neigh-
borhood by choosing representative neighbors who are important
to the target node. However, an overemphasis on these neighbors
can result in a homogeneous neighborhood that lacks the neces-
sary diversity [ 6]. Therefore, we incorporate a penalty mechanism
that penalizes the over-concentration of neighbors, maintaining a
balanced and personalized neighborhood for the target node. (ii)
Temporal-aware Aggregator. To achieve temporal-aware neighbor-
hood encoding, we further propose the Temporal-aware Aggregator
that automatically aggregates neighborhoods with the temporal un-
derstanding of the target node. Specifically, it takes charge of neigh-
borhood aggregation by explicitly determining the utilization of
each aggregation route, facilitating the adaptive route aggregation
or pruning as needed. Meanwhile, our outdated-decay mechanism
strategically de-emphasizes those outdated routes, allowing our
model to concentrate on more fresh and up-to-date information.
In summary, our main contributions are:
â€¢We focus on adaptive neighborhood encoding for temporal in-
teraction graph (TIG) modeling and propose SEAN. SEAN is the
first model proposed to automate neighborhood encoding in TIG
modeling, and it can be easily adopted to boost existing TGNs.
â€¢We develop a Representative Neighbor Selector, which enables
the model to effectively choose representative neighbors for the
target node, achieving personalized neighborhood encoding.
â€¢We propose a Temporal-aware Aggregator, which aggregates
neighborhoods by explicitly determining the utilization of aggre-
gation routes and decaying the outdated information, facilitating
temporal-aware neighborhood encoding.
â€¢We conduct extensive experiments on several TIG benchmark
datasets to validate SEANâ€™s effectiveness. Furthermore, we in-
troduce TemFin, a new financial transaction TIG benchmark
dataset in this paper. This benchmark represents a more challeng-
ing environment due to its complexity in the financial domain.
2 Related Work
2.1 Temporal Graph Networks (TGNs)
Temporal Graph Networks (TGNs) are designed to generate tempo-
ral node representations of TIGs by encoding the neighborhoods
for the target node at any given timestamp [ 3,40,41,43]. They
typically encode their neighborhoods based on a fixed, pre-defined
rule. According to how these neighborhoods are pre-defined, exist-
ing TGNs can be categorized into two types: Breadth-First Search
TGNs (BFS-TGNs) and Depth-First Search TGNs (DFS-TGNs).
4291Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
BFS-TGNs [ 4,11,13,19,24,33,40,41,44] prioritize selecting
neighborhoods close to the target node. They then encode these
neighborhoods via various aggregation functions, generating repre-
sentations for the target node. Different from BFS-TGNs, DFS-TGNs
[20,27] utilize some temporal walks starting from the target node as
their neighborhoods, and encode these walks through the frequency
of node occurrences. Currently, DyGFormer [ 37], one of BFS-TGNs,
encodes the first-order neighborhoods through Transformer blocks,
achieving SOTA results among existing TGNs.
Different from existing TGNs, our model goes beyond fixed rules
for neighborhood encoding. It adaptively encodes the neighbor-
hoods that contribute most to the representations, thus enhancing
the modelâ€™s overall effectiveness.
2.2 Adaptive Neighborhood for Graph Learning
Graph learning with adaptive neighborhoods aims to encode unde-
tected or enhanced neighborhoods for better representation learn-
ing, which has achieved remarkable performance on static graphs.
Early works [ 7,14,39,46] encode adaptive neighborhoods by
utilizing Reinforcement Learning (RL). However, these works face
significant limitations due to the immense search space and the non-
differentiability of models, resulting in substantial computational
overhead. To address this issue, GeniePath [ 15] employs a differen-
tiable module for adaptive neighborhood encoding, reducing the
model complexity and achieving better performance. Subsequently,
an increasing number of adaptive neighborhood encoding meth-
ods [ 10,16,22,28,29,31,35,47] have been proposed, significantly
empowering the representation learning on static graphs.
These existing methods cannot be directly adopted in TIGs, pri-
marily because TIGs exhibit significant temporal differences com-
pared to static graphs. For instance, TIGs are consistently char-
acterized by a sequence of timestamped interactions, rather than
by adjacency matrices that are typical in static graphs, due to the
dynamic nature of TIGs.
3 Preliminaries
3.1 Problem Formulation
Definition 3.1. Temporal Interaction Graph. Given1a node set
V={1,...,|V|}, a temporal interaction graph can be represented
as a sequence of edge set E={(ğ‘–,ğ‘—,ğ‘¡)}, whereğ‘–,ğ‘—âˆˆV andğ‘¡>0.
Each edge(ğ‘–,ğ‘—,ğ‘¡)denotes an interaction between node ğ‘–and node
ğ‘—that happened at time ğ‘¡with the feature eğ‘–ğ‘—(ğ‘¡). The edge feature
vector can depict various information about the interactions among
nodes, e.g., interaction type. Any edge (ğ‘–,ğ‘—,ğ‘¡)âˆˆE only has access
to its historical data before time ğ‘¡,i.e.,{(ğ‘–,ğ‘—,ğœ)âˆˆE|ğœ<ğ‘¡}.
Definition 3.2. Temporal Interaction Graph Modeling. Given
an edge(ğ‘–,ğ‘—,ğ‘¡)âˆˆE and its historical data {(ğ‘–,ğ‘—,ğœ)âˆˆE|ğœ<ğ‘¡}, tem-
poral interaction graph modeling aims to learn a mapping function
ğ‘“:(ğ‘–,ğ‘—,ğ‘¡)â†¦â†’ zğ‘–(ğ‘¡),zğ‘—(ğ‘¡), where zğ‘–(ğ‘¡),zğ‘—(ğ‘¡)âˆˆ Rğ‘‘represent the
representations of nodes ğ‘–andğ‘—at timeğ‘¡, respectively, and ğ‘‘is the
representation vector dimension.
1For simplicity, in this paper, we can just represent a node by its index, i.e., an integer.3.2 Temporal Embedding Module
As a crucial component within most existing TGNs, temporal em-
bedding module [ 44] is responsible for generating temporal repre-
sentations for any given node ğ‘–at timeğ‘¡,zğ‘–(ğ‘¡), which involves a
ğ¾-layer temporal attention network to encode ğ‘–â€™s current neighbor-
hood. In this section, we generalize various temporal embedding
modules employed in different TGNs into a cohesive framework.
This unification allows us to provide a comprehensive overview of
the neighborhood encoding process within TGNs.
For the target node ğ‘–at timeğ‘¡of theğ‘˜-th layer,ğ‘˜âˆˆ{1,...,ğ¾}, tem-
poral embedding module aggregates ğ‘–â€™s neighborhood information
eh(ğ‘˜)
ğ‘–(ğ‘¡)andğ‘–â€™s representation from the previous layer h(ğ‘˜âˆ’1)
ğ‘–(ğ‘¡)
using a Multi-Layer Perceptron (MLP) as the aggregator:
h(ğ‘˜)
ğ‘–(ğ‘¡)=MLP(ğ‘˜)
h(ğ‘˜âˆ’1)
ğ‘–(ğ‘¡)âˆ¥eh(ğ‘˜)
ğ‘–(ğ‘¡)
, (1)
whereâˆ¥denotes the concatenation operation. The neighborhood
information eh(ğ‘˜)
ğ‘–is obtained through an attention mechanism [ 33],
which involves assigning attention scores to ğ‘–â€™s neighborhood:
eh(ğ‘˜)
ğ‘–(ğ‘¡)=Softmax
a(ğ‘˜)
ğ‘–(ğ‘¡)
Â·V(ğ‘˜)
ğ‘–(ğ‘¡), (2)
where a(ğ‘˜)
ğ‘–(ğ‘¡)=[ğ‘(ğ‘˜)
ğ‘–ğ‘—(ğ‘¡)]ğ‘—âˆˆN ğ‘–(ğ‘¡)represents the attention vector
of nodeğ‘–, and the matrix V(ğ‘˜)
ğ‘–(ğ‘¡)=[v(ğ‘˜)
ğ‘–ğ‘—(ğ‘¡)]ğ‘—âˆˆN ğ‘–(ğ‘¡)encapsulates
the messages from ğ‘–â€™s neighborhood. Each element ğ‘(ğ‘˜)
ğ‘–ğ‘—(ğ‘¡)is the
attention score from node ğ‘–to its neighboring node ğ‘—âˆˆ Nğ‘–(ğ‘¡),
which is computed as follows:
ğ‘(ğ‘˜)
ğ‘–ğ‘—(ğ‘¡)=ğ‘“q
h(ğ‘˜âˆ’1)
ğ‘–(ğ‘¡)
ğ‘“k
h(ğ‘˜âˆ’1)
ğ‘—(ğ‘¡)ğ‘‡
âˆš
ğ‘‘(ğ‘˜), (3)
and each row v(ğ‘˜)
ğ‘–ğ‘—(ğ‘¡)is the message carried from neighbor ğ‘—, which
is determined by v(ğ‘˜)
ğ‘–ğ‘—(ğ‘¡)=ğ‘“v(h(ğ‘˜âˆ’1)
ğ‘—(ğ‘¡)). In the above equations,
ğ‘“âˆ—(Â·)(âˆ—âˆˆ{ q, k, v})represents the encoding functions for queries,
keys, and values, respectively [ 25,36]. These functions may have
different specific representations for different TGNs [ 19,33,44].
For simplicity, we consider a single-head attention formula in this
paper. By incorporating personalized and temporal-aware designs
into the neighborhood attention mechanism (Equation 3) and the
aggregation process (Equation 1), we can elevate this module to
achieve adaptive neighborhood encoding. This innovative design
enables our model to serve as a seamlessly integratable plug-and-
play enhancement for various existing TGNs, broadening their
flexibility and effectiveness.
4 Methodology
As we have mentioned, adaptive neighborhood encoding requires
both personalized and temporal considerations. Therefore, we de-
compose this process into two phases, i.e., a Representative Neigh-
bor Selector first assigns distinctive attention to select important
neighbors for personalization, and our Temporal-aware Aggregator
then aggregates the neighborhood information to ensure temporal
relevance. We will introduce these components in the following
subsections.
4292KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siwei Zhang, et al.
!"dc%!!,!"!#!$!%Representative Neighbor Selector%"cdoccurrence encodingoriginalneighbors%"cd
representativeneighbors%"cdTemporal-awareAggregator%"cdoutdated-decayadaptivepruning
!!"!$
d!#
!!!!!%!$!!,!"!#
LSTMAggregation!#(#)Prediction LayerNeighbor Diversity Penaltyâ„’!"#â„’$%&'Training"-thLayer
occurrence-awareattentionmechanismAt time!routes
Figure 2: Framework of the proposed plug-and-play model. Our Representative Neighbor Selector can empower the model
to pinpoint the important neighbors, who then act as the personalized representatives for the target node. Meanwhile, we
propose the neighbor diversity penalty to penalize the over-concentration of these neighbors, thus maintaining a more balanced
neighborhood. Furthermore, we conduct our Temporal-aware Aggregator by LSTM aggregation, where we propose an adaptive
pruning module that explicitly determines whether to aggregate information from the given route or to prune it as needed,
and an outdated-decay mechanism that de-emphasizes the outdated information. Finally, we generate the temporal node
representations for downstream tasks by extracting the encoded neighborhood information from the ğ¾-th layer of SEAN.
4.1 Representative Neighbor Selector
We propose the occurrence-aware attention and neighbor diversity
penalty mechanisms to select personalized representative neigh-
borhoods for the target node.
4.1.1 Occurrence-aware Attention Mechanism. In Equation 3, the
attention score is calculated based on the semantic correlation be-
tween the target node and its neighboring nodes. However, this
semantics-based attention computation may be suboptimal to se-
lect representative neighborhoods due to its lack of personalization
and adaptability [ 33,36]. Here, we propose the occurrence-aware
attention mechanism to enhance the traditional semantics-based
attention mechanism for selecting important neighbors as personal-
ized representatives. Intuitively, neighbor occurrence, which quan-
tifies how frequently a neighbor occurs, is a significant signal in
identifying the importance of each neighbor for a given target node.
Frequently occurring neighbors are more likely to be important
and possess more relevant information, thus making them more
effective and convincing representatives.
Formally, for node ğ‘–at timeğ‘¡and its historical neighbors Nğ‘–(ğ‘¡),
we first count how often each neighbor has occurred historically
and derive it to a one-dimensional occurrence frequency feature,
which is represented by fğ‘–(ğ‘¡) âˆˆR|Nğ‘–(ğ‘¡)|Ã—1. To balance the raw
counts, we then normalize this feature by dividing its temporal
node degree dğ‘–(ğ‘¡)âˆˆR|Nğ‘–(ğ‘¡)|Ã—1, which is denoted as:
Ë†fğ‘–(ğ‘¡)=fğ‘–(ğ‘¡)âŠ™dâˆ’1
ğ‘–(ğ‘¡)âˆˆR|Nğ‘–(ğ‘¡)|Ã—1, (4)
whereâŠ™denotes the element-wise product operation. The nor-
malized occurrence frequency feature vector both considers the
interaction frequency and mitigates the unexpected adverse im-
pacts from the hub nodes (i.e ., those with large degrees), which areknown as â€œdegree-related biasesâ€ [ 23,42]. To enhance the expres-
siveness of the occurrence information, we apply a function ğ‘“e(Â·)
to encode our normalized occurrence frequency feature Ë†fğ‘–(ğ‘¡)into a
high-dimensional occurrence encoding as follows:
Rğ‘–(ğ‘¡)=ğ‘“e
Ë†fğ‘–(ğ‘¡)
âˆˆR|Nğ‘–(ğ‘¡)|Ã—ğ‘‘. (5)
Similar to DyGFormer [ 37], we implement ğ‘“e(Â·)by a two-layer MLP
whose input and output dimensions are 1 and ğ‘‘, respectively.
To further enrich the occurrence-aware attention computation,
we incorporate both edge attributes and temporal information. For
each neighbor node ğ‘—âˆˆNğ‘–(ğ‘¡), we retrieve its occurrence encoding
asrğ‘–ğ‘—(ğ‘¡)=Rğ‘–,ğ‘—:(ğ‘¡)âˆˆRğ‘‘. Our occurrence-aware attention score
from nodeğ‘–to nodeğ‘—at timeğ‘¡,Ëœğ‘ğ‘–ğ‘—(ğ‘¡), can be represented by:
Ëœğ‘ğ‘–ğ‘—(ğ‘¡)=tanh wğ‘–Â·[rğ‘–ğ‘—(ğ‘¡)âˆ¥eğ‘–ğ‘—(ğ‘¡)âˆ¥ğœ™(ğ‘¡âˆ’ğ‘¡ğ‘—)], (6)
whereğœ™(Â·)is the commonly used time encoding function in TGNs
[33,44], and wğ‘–âˆˆR1Ã—3ğ‘‘represents the trainable reshaping vector.
Finally, we update the traditional attention score ğ‘ğ‘–ğ‘—(ğ‘¡)with our
new occurrence-aware attention score Ëœğ‘ğ‘–ğ‘—(ğ‘¡)to obtain the final
attention score as Ë†ğ‘ğ‘–ğ‘—(ğ‘¡)=ğ‘ğ‘–ğ‘—(ğ‘¡)+Ëœğ‘ğ‘–ğ‘—(ğ‘¡). This combined score inte-
grates both semantic relevance and occurrence frequency, enabling
our model to select important neighbors as representatives.
4.1.2 Neighbor Diversity Penalty. Although our occurrence-aware
attention greatly helps the model in identifying the most important
neighbors as the representatives for the target node, an overempha-
sis on these neighbors can result in a homogeneous neighborhood
that lacks the necessary diversity. For example, in the world of
finance, an abundance of homogeneous investment choices may
lead to the undesirable phenomenon known as the â€œFinancial Echo
Chamberâ€ [ 8], analogous to the â€œInformation Cocoonâ€ in Recom-
mender Systems [ 21,34]. To address this issue, we propose an
4293Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
additional neighbor diversity penalty mechanism to encourage the
selection of more diverse neighbors. By leveraging this mechanism,
our approach aims to strike a balance between prioritizing impor-
tant neighbors and ensuring their diversity, thereby enhancing the
overall effectiveness of our model.
LetI(Â·)be the indication function [ 9], and Ë†ğ‘(ğ‘˜)
ğ‘–ğ‘—represents the
occurrence-aware attention score from node ğ‘–to its neighbor ğ‘—âˆˆ
Nğ‘–(ğ‘¡)in theğ‘˜-th layer2. To ensure that our model penalizes the over-
important neighbors with excessively high attention scores, we first
employ a score-filtering technique in the attention scores based on
a threshold ğœâˆˆ[0,1], beyond which the scores are preserved. We
have:
Ë†ğ‘(ğ‘˜)
ğ‘–ğ‘—=I
Ë†ğ‘(ğ‘˜)
ğ‘–ğ‘—â‰¥ğœ
Â·Ë†ğ‘(ğ‘˜)
ğ‘–ğ‘—. (7)
Subsequently, we calculate the average score within ğ‘–â€™s neighbors
byÂ¯ğ‘(ğ‘˜)
ğ‘–:=1
ğ‘Ã
ğ‘—Ë†ğ‘(ğ‘˜)
ğ‘–ğ‘—whereğ‘=|Nğ‘–(ğ‘¡)|is the total number of
neighbors. Then, we compute the product similarity between ğ‘–â€™s
neighborsâ€™ attention scores and their corresponding average score
byğ‘ (ğ‘˜)
ğ‘–=1
ğ‘Ã
ğ‘—Ë†ğ‘(ğ‘˜)
ğ‘–ğ‘—Â·Â¯ğ‘(ğ‘˜)
ğ‘–. Finally, we minimize the dissimilarity
across nodes and layers, promoting alignment and coherence within
our attention mechanism as follows:
LNDP=âˆ’1
ğ¾ğ¾âˆ‘ï¸
ğ‘˜=1L(ğ‘˜)
NDP,L(ğ‘˜)
NDP=1
|V|âˆ‘ï¸
ğ‘–âˆˆVğ‘ (ğ‘˜)
ğ‘–. (8)
LNDP will serve as a controllable auxiliary loss in our model. This
penalty method helps our model to prevent the over-concentration
of just a few neighbors, ensuring a balanced neighborhood.
4.2 Temporal-aware Aggregator
To incorporate temporal understanding, we introduce a Temporal-
aware Aggregator to replace the initial aggregator MLP(ğ‘˜)(Â·)in
Equation 1. Specifically, we implement this module using LSTM cells
[38], chosen for its well-known strengths in temporal awareness:
h(ğ‘˜)=AGG(ğ‘˜)
h(ğ‘˜âˆ’1)
=LSTM(ğ‘˜)
h(ğ‘˜âˆ’1),eh(ğ‘˜),c(ğ‘˜)
.(9)
Here, we fetch the hidden state from the ğ‘˜-th LSTM cell h(ğ‘˜)as our
output representation of the ğ‘˜-th aggregation layer AGG(ğ‘˜)(Â·). The
inputs are three folds: the output representation (i.e ., hidden state)
from the previous layer h(ğ‘˜âˆ’1), the refined neighborhood informa-
tioneh(ğ‘˜)that incorporates our proposed attention mechanism, and
the cell state from the ğ‘˜-th LSTM c(ğ‘˜). To fully achieve temporal-
aware encoding, we enhance our LSTM aggregator through an
adaptive pruning module to facilitate selective aggregates of h(ğ‘˜),
and an outdated-decay module to enhance the temporal sensitivity
of the cell state c(ğ‘˜). Note that we simplify our notation by omitting
the subscript term ğ‘–and the time ğ‘¡. In practice, for node ğ‘–at timeğ‘¡,
we determine its temporal representation by extracting the output
representation from the last layer, i.e.,zğ‘–(ğ‘¡)=h(ğ¾)
ğ‘–(ğ‘¡).
4.2.1 Adaptive Pruning Module. This module not only encourages
our model to focus on incorporating more crucial information for
performance improvement (Section 5.7) but also enhances the model
interpretability, which explicitly specifies the modelâ€™s decision-
making process in seeking adaptive neighborhoods (Section 5.6).
2For clarity, we omit the timestamp term ğ‘¡in the remaining part of our paper, unless
specified otherwise.Formally, we introduce a layer state Ëœğ‘¢(ğ‘˜)âˆˆ[0,1]for eachğ‘˜-th
layer, which represents the probability that the current route will
either be aggregated or pruned. We have:
ğ‘¢(ğ‘˜)=ğ‘“round
Ëœğ‘¢(ğ‘˜)
, (10)
whereğ‘“round(Â·)denotes the binarization function derived from
the rounding operation and ğ‘¢(ğ‘˜)âˆˆ{0,1}. Then,ğ‘¢(ğ‘˜)is utilized to
determine whether the current route information will be aggregated
(ğ‘¢(ğ‘˜)=1) or copied from the previous layer ( ğ‘¢(ğ‘˜)=0). We have:
h(ğ‘˜)=ğ‘¢(ğ‘˜)Â·h(ğ‘˜)+
1âˆ’ğ‘¢(ğ‘˜)
Â·h(ğ‘˜âˆ’1), (11)
Afterward, we update the following layer state Ëœğ‘¢(ğ‘˜+1)using the
aggregated/copied information from the ğ‘˜-th layer h(ğ‘˜)as follows:
Î”Ëœğ‘¢(ğ‘˜)=ğœ
Wğ‘Â·h(ğ‘˜)+bğ‘
, (12)
Ëœğ‘¢(ğ‘˜+1)=ğ‘¢(ğ‘˜)Â·Î”Ëœğ‘¢(ğ‘˜)+
1âˆ’ğ‘¢(ğ‘˜)
Â·
Ëœğ‘¢(ğ‘˜)+max
Î”Ëœğ‘¢(ğ‘˜),1âˆ’Ëœğ‘¢(ğ‘˜)
,
(13)
where Wğ‘âˆˆRğ‘‘Ã—ğ‘‘is the learnable matrix, bğ‘is the learnable vector,
andğœ(Â·)is the sigmoid function. During aggregation, the aggre-
gated neighborhood progressively becomes closer to the central
node as the number of aggregation layers increases. It means that
higher aggregation layers aggregate more central neighborhoods
that have been proven to hold more vital information [ 37]. This
formulation encodes the observation that the likelihood of aggre-
gation increases with the number of aggregation layers. Whenever
theğ‘˜-th layer aggregation is omitted, the pre-activation of the layer
state for the following layer Ëœğ‘¢(ğ‘˜+1)is incremented by Î”Ëœğ‘¢(ğ‘˜). Con-
versely, if the ğ‘˜-th layer aggregation is performed, the accumulated
value will be reset and Ëœğ‘¢(ğ‘˜+1)=Î”Ëœğ‘¢(ğ‘˜). In this way, our model can
explicitly determine the route aggregation or pruning as needed.
4.2.2 Outdated-decay Module. Outdated-decay module aims to
de-emphasize those outdated aggregation routes, ensuring the in-
corporation of more fresh and up-to-date information.
We believe that the long-term cell state retains important and
more enduring information, while the short-term cell state is uti-
lized for processing immediate information. Therefore, we first
decompose the ğ‘˜-th layer cell sate c(ğ‘˜)into two elements, i.e., the
short-term cell state c(ğ‘˜)
ğ‘ and the long-term cell state c(ğ‘˜)
ğ‘™. We have:
c(ğ‘˜)
ğ‘ =tanh
Wğ‘‘Â·c(ğ‘˜)+bğ‘‘
, (14)
c(ğ‘˜)
ğ‘™=c(ğ‘˜)âˆ’c(ğ‘˜)
ğ‘ , (15)
where matrix Wğ‘‘âˆˆRğ‘‘Ã—ğ‘‘and vector bğ‘‘are learnable parameters.
The short-term cell state c(ğ‘˜)
ğ‘ is initially produced by a neural
network with the activation function. Subsequently, the long-term
cell state c(ğ‘˜)
ğ‘™can be distinguished from the original cell state c(ğ‘˜).
To prevent from forgetting too rapidly, we keep our long-term
cell state c(ğ‘˜)
ğ‘™untouched and forget the short-term cell state c(ğ‘˜)
ğ‘ 
according to the time interval Î”ğ‘¡=ğ‘¡âˆ’ğ‘¡âˆ’between the current time
ğ‘¡and the last interaction time of the neighbor in the corresponding
routeğ‘¡âˆ’. We apply a decay mechanism as follows:
Ë†c(ğ‘˜)
ğ‘ =c(ğ‘˜)
ğ‘ Â·ğ‘”(Î”ğ‘¡). (16)
4294KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siwei Zhang, et al.
Algorithm 1: Traning SEAN (one epoch).
input : Temporal interaction graph edge set E;
Aggregation layer ğ¾; NDP controlling parameter ğœ†.
1Initialize all model parameters;
2foreach batchâŠ†Edo
3 foreachğ‘˜=1,2,...,ğ¾ do
4 Retrieve neighborhood for target node ğ‘–;
5 Compute occurrence-aware attention score by
Equation 6;
6 Compute the final score Ë†a(ğ‘˜)
ğ‘–and refine
neighborhood information ehğ‘–;
7 Apply outdated-decay by Equation 14-17;
8 Round layer state Ë†ğ‘¢(ğ‘˜)toğ‘¢(ğ‘˜)as Equation 10;
9 Aggregate neighborhood by Equation 11;
10 Update Ë†ğ‘¢(ğ‘˜+1)by Equation 13;
11 end
12 Compute NDP loss LNDP by Equation 8;
13 Compute lossLwithğœ†by Equation 19 and backward;
14end
The decay function ğ‘”(Î”ğ‘¡)is defined by exp{âˆ’2Â·Î”ğ‘¡/ğ‘¡ğ‘šğ‘ğ‘¥}, where
ğ‘¡ğ‘šğ‘ğ‘¥ represents the maximum value among all time intervals. Fi-
nally, the forgotten short-term cell state Ë†c(ğ‘˜)
ğ‘ and the untouched
long-term cell state c(ğ‘˜)
ğ‘™are combined to generate the final decayed
cell state c(ğ‘˜)
âˆ—by:
c(ğ‘˜)
âˆ—=c(ğ‘˜)
ğ‘™+Ë†c(ğ‘˜)
ğ‘ , (17)
which is the adjusted input cell state to the ğ‘˜-th LSTM cell, gener-
ating the output representation for the ğ‘˜-th aggregation layer.
4.3 Training
4.3.1 Error Gradients. The entire model is differentiable except
for the round process ğ‘“round(Â·)in Equation 10. In this paper, we
employ the straight-through estimator [ 1,2] to allow all parameters
to be trained efficiently without the need for any extra supervision
signals. This involves approximating the step process with identity
during gradient computation in the backward pass:ğœ•ğ‘“round(ğ‘¥)
ğœ•ğ‘¥=1.
4.3.2 Loss Function. We employ temporal link prediction as our
self-supervised task for training. Specifically, for each link (ğ‘–,ğ‘—,ğ‘¡),
we calculate its occurrence probability Ë†ğ‘ğ‘–ğ‘—(ğ‘¡)with the concatenated
temporal representations of interaction nodes, i.e.,zğ‘–(ğ‘¡)andzğ‘—(ğ‘¡),
through a two-layer MLP [ 44]. Then, we compute the loss function
by the cross entropy of the link prediction task as follows:
Llink=âˆ’âˆ‘ï¸
(ğ‘–,ğ‘—,ğ‘¡)âˆˆE
logË†ğ‘ğ‘–ğ‘—(ğ‘¡)+log(1âˆ’Ë†ğ‘ğ‘–ğ‘˜(ğ‘¡))
, (18)
whereğ‘˜is the negative destination node by random sampling. Fi-
nally, we consider our NDP loss LNDPin Equation 8 for regularizing
the learned attention mechanisms in our final training loss:
L=Llink+ğœ†LNDP, (19)
whereğœ†âˆˆR+is a controlling hyper-parameter for our penalty
method.4.4 Complexity Analysis
We provide a detailed complexity analysis of SEAN where we show
the complexity of both components. The Representative Neigh-
bor Selector module assigns distinct attention scores to nodes and
also provides the neighbor diversity penalty loss, leading to the
computational complexity of O(ğ¾|V|ğ‘‘), whereğ¾,|V|,ğ‘‘refer to
the total number of stacking layers, nodes, and the embedding
dimension, respectively. The Temporal-aware Aggregator is imple-
mented based on the LSTM module, therefore leading to a com-
plexity ofO(ğ¾|V|ğ‘‘2). Therefore, the overall complexity of SEAN
isO(ğ¾|V|ğ‘‘+ğ¾|V|ğ‘‘2), maintaining efficiency as it scales linearly
with the number of nodes.
We want to emphasize that the temporal embedding module
mentioned in Section 3.2 in most existing TGNs also incorporates
an explicit attention assignment stage. In this case, integrating
SEAN only leads to an additional overall complexity of O(ğ¾|V|ğ‘‘2).
5 Experiments
5.1 Experimental Settings
5.1.1 Datasets. We conduct experiments with five datasets, includ-
ing four public datasets [ 13] and one dataset introduced in our
paper. The four public datasets - Wikipedia, Reddit, MOOC, and
LastFM - are widely used in temporal interaction graph modeling.
Meanwhile, recognizing that these available datasets are social or
event networks, we release TemFin, a new TIG benchmark dataset
from the complicated financial domain. TemFin consists of half
a month of transactions sampled from a private financial trans-
fer transaction network in the Ant Finance Group.3Details of all
datasets are described in Table 5 in the Appendix due to the page
limitations. All datasets are sequentially split according to the edge
timestamp order by 70%, 15%, and 15%for training, validation, and
testing, respectively [44].
5.1.2 Baselines. For comparison, we choose nine existing TGNs
as our baselines, and summarize the detailed description of our
baselines as follows:
â€¢DyRep [24]: It is a notable implementation of the temporal point
process in its neighborhoods and introduces a projection layer
that estimates user representation in the future.
â€¢JODIE [13]: This method considers the dynamic representation
of nodes changing over time and utilizes two RNNs for temporal
interaction graph modeling.
â€¢TGAT [33]: TGAT aggregates its neighborhoods by applying a
temporal embedding module based on the temporal attention
mechanism.
â€¢TGN [19]: TGN summarizes the former models and proposes
a memory module to record the historical behaviors of nodes,
significantly improving the performance of temporal interaction
graph modeling.
â€¢TIGE [44]: TIGE enhances the TGN framework by incorporat-
ing a dual memory module, effectively addressing the issue of
staleness and building upon TGNâ€™s foundational concepts.
â€¢GraphMixer [5]: This approach employs a simple MLP-Mixer
for neighborhood encoding and has demonstrated impressive
3The dataset is sampled properly only for experiment purposes and does not imply
any commercial information. All personal identity information (PII) has been removed.
4295Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Average Precision (%) results on Temporal Link Prediction task under both transductive and inductive settings. Results
with the pink background represent the performance and corresponding improvements achieved by integrating our model
into three backbones. The best results are highlighted in bold, and * denotes the benchmark dataset introduced in this paper.
Transductive setting Inductive setting
Wikipedia Reddit MOOC LastFM TemFin* Wikipedia Reddit MOOC LastFM TemFin*
JODIE 94.62 Â±0.5 91.11Â±0.3 76.50Â±1.8 68.77Â±3.0 88.02Â±0.2 93.11Â±0.4 94.36Â±1.1 77.83Â±2.1 82.55Â±1.9 75.35Â±1.6
TGAT 95.34 Â±0.1 98.12Â±0.2 60.97Â±0.3 53.36Â±0.1 87.70Â±0.1 93.99Â±0.3 96.62Â±0.3 63.50Â±0.7 55.65Â±0.2 79.37Â±0.2
DyRep 94.59 Â±0.2 97.98Â±0.1 75.37Â±1.7 68.77Â±2.1 87.99Â±0.1 92.05Â±0.3 95.68Â±0.2 78.55Â±1.1 81.33Â±2.1 76.53Â±0.1
PINT 98.78Â±0.1 99.03Â±.01 85.14Â±1.2 88.06Â±0.7 90.79Â±0.1 98.38Â±.04 98.25Â±.04 85.39Â±1.0 91.76Â±0.7 81.18Â±0.2
iLoRE 98.98 Â±0.3 99.11Â±0.4 90.44Â±1.0 91.39Â±0.1 90.90Â±0.1 98.60Â±0.3 98.65Â±0.3 89.75Â±0.8 93.29Â±0.8 84.33Â±0.2
GraphMixer 97.95 Â±.03 97.31Â±.01 82.78Â±0.2 67.27Â±2.1 86.85Â±0.1 96.65Â±.02 95.26Â±.02 81.41Â±0.2 82.11Â±0.4 77.47Â±0.2
TGN 98.46Â±0.1 98.70Â±0.1 85.88Â±3.0 80.69Â±0.2 90.65Â±0.1 97.81Â±0.1 97.55Â±0.1 85.55Â±2.9 84.66Â±0.1 80.67Â±0.2
TGN+ 98.79Â±0.1 98.73Â±.02 89.91Â±1.2 84.15Â±2.5 91.04Â±0.1 98.18Â±0.1 97.69Â±0.1 88.91Â±1.6 88.60Â±1.2 81.30Â±0.6
(imprv.) (+0.33 ) (+0.03 ) (+4.03 ) (+3.46 ) (+0.39 ) (+0.37 ) (+0.14 ) (+3.36 ) (+3.94 ) (+0.63 )
TIGE 98.38Â±0.1 99.04Â±0.1 89.64Â±0.9 87.85Â±0.9 90.81Â±.02 98.45Â±0.1 98.39Â±0.1 89.51Â±0.7 90.14Â±1.0 83.22Â±0.2
TIGE+ 98.95Â±.04 99.13Â±.02 92.26Â±0.8 91.42Â±0.7 90.92Â±.01 98.61Â±0.1 98.50Â±.03 91.53Â±0.8 93.14Â±0.6 83.79Â±0.1
(imprv.) (+0.12 ) (+0.09 ) (+2.62 ) (+3.79 ) (+0.11) (+0.16 ) (+0.11) (+2.02 ) (+3.08 ) (+0.57 )
DyGFormer 99.03 Â±.02 99.22Â±.01 87.52Â±0.5 93.00Â±0.1 93.34Â±0.2 98.59Â±.03 98.84Â±.02 86.96Â±0.4 94.23Â±.09 86.29Â±0.4
DyGFormer+ 99.27Â±.02 99.59Â±.01 88.29Â±0.5 94.73Â±0.1 95.81Â±0.3 98.63Â±.04 98.91Â±0.1 87.20Â±0.5 94.89Â±0.1 86.91Â±.04
(imprv.) (+0.24) (+0.37 ) (+0.77 ) (+1.73) (+2.47 ) (+0.04) (+0.07 ) (+0.24 ) (+0.66 ) (+0.62 )
Table 2: AUROC (%) results on Evolving Node Classification task. Due to table restrictions, we omit some baseline results that
have been shown inferior to our three backbones [19, 37, 44].
DyRep PINT GraghMixer TGN TGN+ (imprv.) TIGE TIGE+ (imprv.) DyGFormer DyGFormer+ (imprv.)
Wikipedia 84.59Â±2.2 87.59Â±0.6 86.80Â±.01 87.81Â±0.387.91Â±0.8(+0.10 )86.92Â±0.787.16Â±1.5 (+0.24 )87.07Â±0.8 87.49Â±0.6 (+0.42 )
Reddit 62.91Â±2.4 67.31Â±0.2 64.22Â±.03 67.06Â±0.968.26Â±0.8 (+1.20 )69.41Â±1.370.24Â±2.2 (+0.83) 68.30Â±1.5 69.06Â±2.5 (+0.76 )
MOOC 67.76Â±0.5 68.77Â±1.1 67.21Â±.02 69.54Â±1.072.96Â±1.8 (+2.04 )72.35Â±2.373.85Â±2.0 (+1.50 )77.89Â±0.5 78.88Â±3.2 (+0.99 )
TemFin* 78.56Â±2.9 82.22Â±1.2 81.17Â±0.7 80.93Â±2.282.39Â±2.5 (+1.46 )80.52Â±3.083.17Â±2.2(+2.65 )72.50Â±0.4 73.05Â±2.1 (+0.55 )
performance in the Temporal Link Prediction task under the
transductive setting.
â€¢PINT [20]: PINT utilizes injective temporal message passing
on neighborhoods and leverages relative positional features to
improve model performance.
â€¢iLoRE [41]: Focusing on instant long-term modeling and re-
occurrence preservation, iLoRE provides a nuanced approach to
TIG modeling.
â€¢DyGFormer [37]: This model introduces a novel architecture
that performs Transformer blocks through the patch technique,
achieving SOTA results in TIG modeling.
We select three representative baselines to serve as the backbones
for our SEAN: TGN [ 19], TIGE [ 44], and DyGFormer [ 37]. We
then integrate our model into these three backbones and evaluate
their performance in two downstream tasks, i.e., Temporal Link
Prediction and Evolving Node Classification.
5.2 Temporal Link Prediction
We start our experiments with the Temporal Link Prediction task,
which aims to predict the probability of a link occurring between
two specific nodes at a certain time. We utilize two settings: the
transductive setting, which performs link prediction on nodes that
have appeared during training, and the inductive setting, whichpredicts links between unseen nodes. We randomly sample an equal
number of negative nodes as detailed in Equation 18, and report
the average precision (AP) performance with all three backbones,
i.e., TGN [19], TIGE [44], and DyGFormer [37].
The results are shown in Table 1. Clearly, we can observe that
all three backbone models show improved performance across all
datasets in both transductive and inductive settings after integrat-
ing our model. Meanwhile, among the improved models, at least
one has achieved SOTA results, surpassing all baseline comparisons.
This observation proves the effectiveness of the adaptive neighbor-
hood encoding mechanism in TGNs. Moreover, we find the most
significant improvement between the backbones and their corre-
sponding improved models specifically on MOOC and lastFM. This
may be owing to their high neighborhood complexity, constraining
the backbonesâ€™ effectiveness. In contrast, our model introduces an
adaptive neighborhood encoding mechanism that effectively em-
powers these backbones to better capture and utilize the critical
information in such challenging environments, demonstrating our
modelâ€™s effectiveness and robustness in handling complex TIGs.
5.3 Evolving Node Classification
We also conduct the Evolving Node Classification task as a down-
stream task to further validate the effectiveness of our learned
temporal node representations. Specifically, we input the learned
4296KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siwei Zhang, et al.
01020304050
Perturbation Rate in Wikipedia (%)859095100Transductive AP (%)
01020304050
Perturbation Rate in Reddit (%)859095100Transductive AP (%)
01020304050
Perturbation Rate in Wikipedia (%)859095100Inductive AP (%)
01020304050
Perturbation Rate in Reddit (%)80859095100Inductive AP (%)
JODIE TGAT DyRep TGN DyGFormer TIGE GraphMixer PINT TGN+ TIGE+ DyGFormer+
Figure 3: Robustness to noisy neighborhoods on Wikipedia and Reddit in different perturbation rates.
TGN TGN+ TIGE TIGE+ DyGFormer DyGFormer+0.00.20.40.60.8Score value
Figure 4: Attention scores assigned from randomly selected
nodes to their perturbed neighbors on noisy Wikipedia.
12345
Layer90.091.593.094.5 Transductive AP
TGN
TGN+
0.01.53.04.5Performance Improvement
12345
Layer90.091.593.0 Inductive AP
TGN
TGN+
0.01.53.04.5Performance Improvement
(a) TGN model.
12345
Layer90.491.292.092.893.6 Transductive AP
TIGE
TIGE+
0.00.81.62.43.2Performance Improvement
12345
Layer91.092.093.094.0 Inductive AP
TIGE
TIGE+
0.00.81.62.43.2Performance Improvement (b) TIGE model.
Figure 5: Robustness to expanded neighborhoods on MOOC.
representation of node ğ‘–at timeğ‘¡,zğ‘–(ğ‘¡), into a two-layer MLP, which
maps the representations to the dynamic labels. We carry out the
experiments on Wikipedia, Reddit, MOOC, and TemFin. LastFM is
excluded due to its lack of node labels. We employ AUROC as our
evaluation metric due to the label imbalance issue in these datasets.
We report the results in Table 2. All three improved models
demonstrate better performance than the backbones across all
datasets, and at least one of them has achieved the SOTA results.
This indicates that the learned representations from our model can
be more effectively applied to downstream tasks, proving its supe-
riority once again. Notably, the most significant improvement is
observed on MOOC and TemFin. This could be due to their rela-
tively less severe label imbalance issue compared to others, thereby
facilitating the performance enhancement more readily.
5.4 Robustness to Noisy Neighborhoods
We have mentioned that the suitable neighborhoods for nodes
should vary due to the different connectivity patterns and potential
noise. To simulate this scenario, we complicate and pollute thenodesâ€™ neighborhoods by introducing random noise into TIGs [ 40].
We then evaluate the model performance (both w/o and w/ SEAN)
under these noisy conditions. Specifically, we replace the original
links with our randomly sampled noisy links at a certain pertur-
bation rateğ‘âˆˆ{10%,20%,30%,40%,50%}. Meanwhile, at ğ‘=50%,
we collect the attention scores assigned from a subset of randomly
selected nodes to their perturbed neighbors, visualizing the modelâ€™s
ability to handle noise with box plots. We emphasize that these
comparisons are controlled and fair because the noise issue dose
exist in TIGs and the introduced noise is prevalent [26, 40].
The results are detailed in Figure 3 and 4. The improved models
exhibit exceptional robustness, even when encountering significant
noise. This demonstrates the superiority of SEAN in encoding
suitable neighborhoods, thus weakening potential noise attacks. As
for the perturbed neighborsâ€™ scores, we find an obvious reduction
and greater focus in the scores when comparing improved models to
their corresponding backbones. It suggests that SEAN successfully
empowers these backbones to evaluate and mitigate these same-
level random noises introduced in nodesâ€™ neighborhoods.
5.5 Robustness to Expanded Neighborhoods
We compare the performance across expanded neighborhoods be-
tween the backbones and improved models. Specifically, we vary
the aggregation layer ğ¾âˆˆ{1,2,3,4,5}in models and carry out
the experiments using TGN [ 19] and TIGE [ 44] as the backbones.
DyGFormer [ 37] is not included in this group of experiments due to
its limitation to the first-hop neighborhoods in the original design.
In Section 1, we keep the neighbor sampling size of TGN [ 19] at 10
to ensure a fair comparison, which is its default hyper-parameter.
However, during these experiments, maintaining the sampling size
at 10 will lead to GPU out-of-memory issues rapidly. Therefore, we
reduce the size to 5 in practice.
The results are depicted in Figure 5. The improved models also ex-
hibit significant robustness with expanded neighborhoods, and the
performance improvement compared to their corresponding back-
bones progressively widens as the neighborhood expands. It high-
lights SEANâ€™s capability to adaptively encode the suitable neigh-
borhoods, thus avoiding the over-smoothing issue [ 12] caused by
the expanded neighborhoods. Moreover, fine-tuning the accessible
neighborhoods (i.e ., the layer number ğ¾) is essential for optimizing
the backbone performance. In contrast, our improved models reach
their peak performance by employing the largest possible accessible
neighborhoods, as long as the computational resources permit.
4297Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
NodeID: 16NodeID: 1648time
prunedroute(",".%](".%,".'](".',%]
0.190.28
prediction probability withneighborhoods.
0.610.77
0.420.83
0.380.91
0.730.85
0.790.90
0.750.95
0.410.98
fixedadaptive
Figure 6: Case study on Wikipedia with TGN model. The in-
troduction of SEAN can lead to both adaptive neighborhood
consideration and improved prediction performance.
5.6 Case Study
We conduct the case study to interpret the choice-making process
and model effectiveness when SEAN seeks the neighborhoods. As
shown in Figure 6, each grid displays the encoded neighborhood
for a certain node at a particular time, and dashed lines indicate
the aggregation routes pruned by SEAN. The values within those
dashed boxes are the Evolving Node Classification prediction prob-
abilities for the backbone and the improved model, respectively.
Here, higher values signify better performance. For clarity, we limit
the depiction only to 2-hop accessible neighborhoods for each node.
In our observations from Node 16 in Figure 6, the backbone does
not consistently show improved performance over time. Conversely,
the improved model performance continues to increase, indicating
SEANâ€™s ability to manage the increasing neighborhood complexity
and progressively stronger potential noise. Another notable aspect
is that, even within the same node such as Node 1648 in Figure 6,
the improved model shows significant neighborhood differences
at distinct timestamps. It suggests that the improved model can
effectively encode the most suitable neighborhoods across different
timestamps to achieve optimal performance.
5.7 Ablation Study
We conduct the ablation study on the main components of our
SEAN, including Representative Neighbor Selector (RNS) in Sec-
tion 4.1, Temporal-aware Aggregator (TA) in Section 4.2, Neighbor
Diversity Penalty (NDP) in Section 4.1.2, Adaptive Pruning Module
(APM) in Section 4.2.1, and Outdated-decay Module (OM) in Section
4.2.2. To assess the impact of each component, we remove them
individually, resulting in five variants: w/o RNS, w/o TA, w/o NDP,
w/o APM, and w/o OM.
We report the performance of the Temporal Link Prediction task
using both the improved TGN model and its variants as represented
in Figure 7. Note that the green line represents the performance
of the TGN backbone. The model performs best when utilizing all
components, and the removal of any single component leads to a
decrease in performance, highlighting the necessity of each design.
Moreover, the performance of all variants surpasses the backbone,
validating the effectiveness of our components. Additionally, our
Temporal-aware Aggregator component has the most significant
contribution to model performance, reaffirming the importance of
the temporal-aware design for neighborhood encoding.
MOOC85.587.088.590.0Transductive AP (%)
MOOC85.587.088.590.0Inductive AP (%)
LastFM81.082.584.0Transductive AP (%)
LastFM84.085.587.088.590.0Inductive AP (%)      TGN
(Backbone)w/o TA w/o RNS w/o NDP w/o APM w/o OM TGN+Figure 7: Ablation study on MOOC and LastFM with the im-
proved TGN model. TGN backbone is shown for reference.
10âˆ’310âˆ’210âˆ’1100101
Value of Î»858687888990AP
w/o NDP (Î»=0)
TGN+
Transductive
Inductive
10âˆ’310âˆ’210âˆ’1100101
Value of Î»90.491.292.092.8AP
w/o NDP (Î»=0)
TIGE+
Transductive
Inductive
Figure 8: Parameter study on MOOC with improved TGN (left)
and TIGE (right) models. w/o NDP is shown for reference.
5.8 Parameter Study
Now, we study how the controlling hyper-parameter of the Neigh-
bor Diversity Penalty ( ğœ†in Equation 19) affects the performance.
We plot the APs of the improved models with varying values of
ğœ†on MOOC in Figure 8. When the penalty method is too strong
(ğœ†=10), the model may struggle to learn information from those
important neighbors, thus leading to a decrease in model perfor-
mance. Additionally, different models have distinct sensitivities to
ğœ†changes. For example, TIGE requires a larger ğœ†for optimal results
compared to TGN, suggesting a more severe over-concentration
issue within the TIGE model.
6 Conclusion and future work
In this paper, we enhance temporal interaction graph modeling via
adaptive neighborhood encoding. We propose SEAN, a plug-and-
play model designed to boost existing TGNs. By introducing the
Representative Neighbor Selector, we can select the personalized
representatives for each target node. The Temporal-aware Aggrega-
tor then aggregates neighborhoods in a temporal-aware manner. As
for future work, we will consider detecting the hidden routes that
do not originally exist during the aggregation process. Additionally,
we can also consider other implements for our Temporal-aware
Aggregator, such as Transformer blocks.
Acknowledgments
This work is funded in part by the Shanghai Science and Technology
Development Fund No.22dz1200704, the National Natural Science
Foundation of China Projects No. U1936213, NSF through grants
IIS-1763365 and IIS-2106972, and also supported by Ant Group. The
first author, Dr. Zhang, also wants to thank Yifeng Wang for his
efforts and support in this work.
4298KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siwei Zhang, et al.
References
[1]Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. 2013. Estimating or
propagating gradients through stochastic neurons for conditional computation.
arXiv preprint arXiv:1308.3432 (2013).
[2]VÃ­ctor Campos, Brendan Jou, Xavier GirÃ³-i Nieto, Jordi Torres, and Shih-Fu Chang.
2017. Skip rnn: Learning to skip state updates in recurrent neural networks.
arXiv preprint arXiv:1708.06834 (2017).
[3]Xi Chen, Yongxiang Liao, Yun Xiong, Yao Zhang, Siwei Zhang, Jiawei Zhang,
and Yiheng Sun. 2023. SPEED: Streaming Partition and Parallel Acceleration for
Temporal Interaction Graph Embedding. arXiv preprint arXiv:2308.14129 (2023).
[4]Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun, Yao
Zhang, Yinglong Zhao, and Yulin Kang. 2024. Prompt Learning on Temporal
Interaction Graphs. arXiv preprint arXiv:2402.06326 (2024).
[5]Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang
Tong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated Model
Architectures For Temporal Networks? arXiv preprint arXiv:2302.11636 (2023).
[6]Kaituo Feng, Changsheng Li, Xiaolu Zhang, and Jun Zhou. 2023. Towards Open
Temporal Graph Neural Networks. arXiv preprint arXiv:2303.15015 (2023).
[7]Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. 2021. Graph
neural architecture search. In International joint conference on artificial intelligence.
International Joint Conference on Artificial Intelligence.
[8]Ilaria Gianstefani, Luigi Longo, and Massimo Riccaboni. 2022. The echo chamber
effect resounds on financial markets: A social media alert system for meme stocks.
arXiv preprint arXiv:2203.13790 (2022).
[9]Yong Guo, David Stutz, and Bernt Schiele. 2023. Robustifying token attention for
vision transformers. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 17557â€“17568.
[10] ZHAO Huan, YAO Quanming, and TU Weiwei. 2021. Search to aggregate neigh-
borhood for graph neural network. In 2021 IEEE 37th International Conference on
Data Engineering (ICDE). IEEE, 552â€“563.
[11] Zian Jia, Yun Xiong, Yuhong Nan, Yao Zhang, Jinjing Zhao, and Mi Wen. 2023.
MAGIC: Detecting Advanced Persistent Threats via Masked Graph Representa-
tion Learning. arXiv preprint arXiv:2310.09831 (2023).
[12] Nicolas Keriven. 2022. Not too little, not too much: a theoretical analysis of graph
(over) smoothing. Advances in Neural Information Processing Systems 35 (2022),
2268â€“2281.
[13] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-
bedding trajectory in temporal interaction networks. In Proceedings of the 25th
ACM SIGKDD international conference on knowledge discovery & data mining.
1269â€“1278.
[14] Kwei-Herng Lai, Daochen Zha, Kaixiong Zhou, and Xia Hu. 2020. Policy-gnn:
Aggregation optimization for graph neural networks. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
461â€“471.
[15] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan
Qi. 2019. Geniepath: Graph neural networks with adaptive receptive paths. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 4424â€“4431.
[16] Yang Luo, Zehao Gu, Shiyang Zhou, Yun Xiong, and Xiaofeng Gao. 2023.
Meteorology-Assisted Spatio-Temporal Graph Network for Uncivilized Urban
Event Prediction. In 2023 IEEE International Conference on Data Mining (ICDM).
IEEE, 468â€“477.
[17] Dan McGinn, David Birch, David Akroyd, Miguel Molina-Solana, Yike Guo, and
William J Knottenbelt. 2016. Visualizing dynamic bitcoin transaction patterns.
Big data 4, 2 (2016), 109â€“119.
[18] Farimah Poursafaei, Shenyang Huang, Kellin Pelrine, and Reihaneh Rabbany.
2022. Towards better evaluation for dynamic link prediction. Advances in Neural
Information Processing Systems 35 (2022), 32928â€“32941.
[19] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal graph networks for deep learning
on dynamic graphs. arXiv preprint arXiv:2006.10637 (2020).
[20] Amauri Souza, Diego Mesquita, Samuel Kaski, and Vikas Garg. 2022. Provably
expressive temporal graph networks. Advances in Neural Information Processing
Systems 35 (2022), 32257â€“32269.
[21] Cass R Sunstein. 2006. Infotopia: How many minds produce knowledge. Oxford
University Press.
[22] Qiaoyu Tan, Xin Zhang, Ninghao Liu, Daochen Zha, Li Li, Rui Chen, Soo-Hyun
Choi, and Xia Hu. 2023. Bring your own view: Graph neural networks for link
prediction with personalized subgraph selection. In Proceedings of the Sixteenth
ACM International Conference on Web Search and Data Mining. 625â€“633.
[23] Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal,
Prasenjit Mitra, and Suhang Wang. 2020. Investigating and mitigating degree-
related biases in graph convoltuional networks. In Proceedings of the 29th ACM
International Conference on Information & Knowledge Management. 1435â€“1444.
[24] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019.
Dyrep: Learning representations over dynamic graphs. In International conference
on learning representations.[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[26] Yiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, Siddharth
Bhatia, and Bryan Hooi. 2021. Adaptive data augmentation on temporal graphs.
Advances in Neural Information Processing Systems 34 (2021), 1440â€“1452.
[27] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. 2021.
Inductive representation learning in temporal networks via causal anonymous
walks. arXiv preprint arXiv:2101.05974 (2021).
[28] Zhili Wang, Shimin Di, and Lei Chen. 2021. Autogel: An automated graph neural
network with explicit link information. Advances in Neural Information Processing
Systems 34 (2021), 24509â€“24522.
[29] Zhili Wang, Shimin Di, and Lei Chen. 2023. A Message Passing Neural Network
Space for Better Capturing Data-dependent Receptive Fields. In Proceedings of
the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
2489â€“2501.
[30] Zhili Wang, Shimin Di, and Lei Chen. 2023. A Message Passing Neural Network
Space for Better Capturing Data-dependent Receptive Fields. In Proceedings of
the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
2489â€“2501.
[31] Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. 2021. Pooling archi-
tecture search for graph classification. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management. 2091â€“2100.
[32] Sheng Xiang, Mingzhi Zhu, Dawei Cheng, Enxia Li, Ruihui Zhao, Yi Ouyang,
Ling Chen, and Yefeng Zheng. 2023. Semi-supervised credit card fraud detection
via attribute-driven graph representation. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 37. 14557â€“14565.
[33] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
2020. Inductive representation learning on temporal graphs. arXiv preprint
arXiv:2002.07962 (2020).
[34] Huimin Xu, Zhicong Chen, Ruiqi Li, and Cheng-Jun Wang. 2020. The geometry of
information cocoon: Analyzing the cultural space with word embedding models.
arXiv preprint arXiv:2007.10083 (2020).
[35] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In International conference on machine learn-
ing. PMLR, 5453â€“5462.
[36] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,
Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly
for graph representation? Advances in Neural Information Processing Systems 34
(2021), 28877â€“28888.
[37] Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. 2023. Towards Better Dy-
namic Graph Learning: New Architecture and Unified Library. arXiv preprint
arXiv:2303.13047 (2023).
[38] Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. 2019. A review
of recurrent neural networks: LSTM cells and network architectures. Neural
computation 31, 7 (2019), 1235â€“1270.
[39] Chris Zhang, Mengye Ren, and Raquel Urtasun. 2018. Graph hypernetworks for
neural architecture search. arXiv preprint arXiv:1810.05749 (2018).
[40] Siwei Zhang, Yun Xiong, Yao Zhang, Yiheng Sun, Xi Chen, Yizhu Jiao, and
Yangyong Zhu. 2023. RDGSL: Dynamic Graph Representation Learning with
Structure Learning. In Proceedings of the 32nd ACM International Conference on
Information and Knowledge Management. 3174â€“3183.
[41] Siwei Zhang, Yun Xiong, Yao Zhang, Xixi Wu, Yiheng Sun, and Jiawei Zhang.
2023. iLoRE: Dynamic Graph Representation with Instant Long-term Modeling
and Re-occurrence Preservation. In Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management. 3216â€“3225.
[42] Wei Zhang, Xiaogang Wang, Deli Zhao, and Xiaoou Tang. 2012. Graph degree
linkage: Agglomerative clustering on a directed graph. In Computer Visionâ€“ECCV
2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13,
2012, Proceedings, Part I 12. Springer, 428â€“441.
[43] Yao Zhang, Yun Xiong, Dongsheng Li, Caihua Shan, Kan Ren, and Yangyong Zhu.
2021. CoPE: Modeling Continuous Propagation and Evolution on Interaction
Graph. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management. 2627â€“2636.
[44] Yao Zhang, Yun Xiong, Yongxiang Liao, Yiheng Sun, Yucheng Jin, Xuehao Zheng,
and Yangyong Zhu. 2023. TIGER: Temporal Interaction Graph Embedding with
Restarts. In ACM Web Conference.
[45] Fujin Zhou, Thijs Endendijk, and WJ Wouter Botzen. 2023. A review of the
financial sector impacts of risks associated with climate change. Annual Review
of Resource Economics 15 (2023), 233â€“256.
[46] Kaixiong Zhou, Xiao Huang, Qingquan Song, Rui Chen, and Xia Hu. 2022. Auto-
gnn: Neural architecture search of graph neural networks. Frontiers in big Data 5
(2022), 1029307.
[47] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
Koutra. 2020. Beyond homophily in graph neural networks: Current limitations
and effective designs. Advances in neural information processing systems 33 (2020),
7793â€“7804.
4299Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A appendix
A.1 Notations
We summarize relevant notations and their descriptions in this
paper as shown in Table 3.
Table 3: Important notations in this paper.
Notations Description or definition
fğ‘–(ğ‘¡) Occurrence frequency feature of node ğ‘–â€™s neighbors at ğ‘¡
Rğ‘–(ğ‘¡) Occurrence encoding of node ğ‘–â€™s neighbors at ğ‘¡
Ë†ğ‘(ğ‘˜)
ğ‘– ğ‘—Enhanced attention from nodes ğ‘–toğ‘—in theğ‘˜-th layer
ğ‘¢(ğ‘˜)The state of the ğ‘˜-th layer
c(ğ‘˜)
âˆ— Outdated-decay cell state in the ğ‘˜-th layer
LNDP Loss function of neighbor diversity penalty
Llink Loss function of temporal link prediction
A.2 Datasets
We employ five datasets, including four public datasets and one
dataset introduced in our paper.
The four public datasets4used in our paper are provided by
JODIE [ 13]. (i) Wikipedia is a temporal interaction graph capturing
edits made by users on Wikipedia pages. (ii) Reddit is a temporal
interaction graph recording user posts in different subreddits. In
both Wikipedia and Reddit, the edge feature dimension is 172, and
user nodes are dynamically labeled to indicate whether they have
been banned. (iii) MOOC is a temporal interaction graph that tracks
interactions between students and online courses, with dynamic
labels on nodes indicating whether students drop out of a course.
(iv) LastFM is a temporal interaction graph that logs events between
users and songs, but it does not include dynamic labels.
Most existing temporal interaction graph datasets are primarily
focused on social or event networks. However, interaction data
in the financial domain differs significantly. It typically involves
a considerably larger number of participants, resulting in sparser
networks. Consequently, learning tasks on such networks tend
to be more challenging compared to those on existing available
networks. This increased difficulty provides a more rigorous testbed
for validating the capabilities of various TGNs, making a more
effective benchmark to assess their performance and robustness.
In this paper, we release TemFin, a new temporal interaction
graph benchmark dataset that records the financial transfer trans-
actions between bank accounts. Within TemFin, bank accounts are
represented as nodes, and the financial transactions with times-
tamp information occurring between two accounts are modeled as
timestamped interactions. Additionally, the information about the
transactions, e.g., amount and fund channel, is converted into a 154-
dimensional vector through one-hot encoding, which then serves
as the edge feature for each transaction. Furthermore, the source
account nodes are assigned dynamic labels, indicating whether the
corresponding account is implicated in money laundering activity.
In the TemFin dataset, the Temporal Link Prediction task is designed
to forecast whether one account will transfer funds to another at
4https://snap.stanford.edu/jodie/a given future time. Meanwhile, the Evolving Node Classification
task concentrates on determining whether a source account in a
specific transaction is implicated in money laundering activity. The
detailed statistics of all datasets are summarized in Table 5.
A.3 Implementation Details
For baselines, all baselines employ the same experimental settings
as TIGE [ 44], thus ensuring comparability in our evaluation pro-
cess. The detailed default hyper-parameters can be found in its
publication.
For our SEAN, we integrate it into three representative TGNs,
i.e., TGN [ 19], TIGE [ 44], and DyGFormer [ 37], chosen for their
superior performance and widespread recognition. To guarantee
fairness in our evaluation, all hyper-parameters in these three back-
bones remain consistent with their original implementations. We
implement SEAN in PyTorch based on their corresponding offi-
cial implementations. Unless otherwise stated, we use the default
hyper-parameters of SEAN summarized in Table 4.
Experiments on all datasets are conducted on a single server
with 72 cores, 32GB memory, and one Nvidia Tesla V100 GPU.
Table 4: Default hyper-parameters of SEAN.
Hyper-parameter Value
Batch size 200
Learning rate 0.0001
Optimizer Adam
Number of layers 1
Number of attention heads 2
Thresholdğœ 0.1
A.4 Limitations
We consider two main limitations for our SEAN:
â€¢While SEAN enhances model performance, it inevitably intro-
duces some complexity, which may be unavoidable.
â€¢Our proposed adaptive pruning module serves as a residual link
in the aggregation process and improves the overall model per-
formance. However, when the neighborhood is extremely sparse,
the pruning mechanism might lose some useful information.
We will address these limitations in our future work by explor-
ing more efficient adaptive neighborhood encoding methods in
temporal interaction graph modeling.
A.5 Supplementary Results
Due to space constraints in the main body of our paper, we present
the following supplementary results: the robustness study to noisy
neighborhoods is depicted in Figure 9 and 10, the robustness study
to expanded neighborhoods is shown in Figure 11, additional case
study with more other selected nodes are illustrated in Figure 12,
and the ablation study is detailed in Figure 13.
4300KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Siwei Zhang, et al.
Table 5: Detailed statistics of datasets.
Datasets Domain # Nodes # Edges # Edge Feature Duration Label % Positive Labels
Wikipedia social 9,227 157,474 172 1 month editing ban 0.14%
Reddit social 10,984 672,447 172 1 month posting ban 0.05%
MOOC event 7,144 411,749 0 17 month course dropout 0.98%
LastFM event 1,980 1,293,103 0 1 month â€” â€”
TemFin (new ) finance 33,245 709,774 154 half a month money laundering 0.71%
01020304050
Perturbation Rate in MOOC (%)60708090Transductive AP (%)
01020304050
Perturbation Rate in LastFM (%)5060708090Transductive AP (%)
01020304050
Perturbation Rate in MOOC (%)60708090Inductive AP (%)
01020304050
Perturbation Rate in LastFM (%)708090Inductive AP (%)
JODIE TGAT DyRep TGN DyGFormer TIGE GraphMixer PINT TGN+ TIGE+ DyGFormer+
Figure 9: Robustness to noisy neighborhoods on MOOC and LastFM in different perturbation rates.
(Supplementary results for Section 5.4)
TGN TGN+ TIGE TIGE+ DyGFormer DyGFormer+0.00.20.40.60.8Score value
Figure 10: Attention scores assigned from randomly selected
nodes to their perturbed neighbors on noisy Reddit.
(Supplementary results for Section 5.4)
12345
Layer98.598.798.8 Transductive AP
TGN
TGN+
0.00.10.20.30.4Performance Improvement
12345
Layer97.697.898.098.2 Inductive AP
TGN
TGN+
0.00.20.30.50.6Performance Improvement
(a) TGN model.
12345
Layer98.698.798.898.9 Transductive AP
TIGE
TIGE+
0.00.10.20.20.3Performance Improvement
12345
Layer98.198.298.498.598.7 Inductive AP
TIGE
TIGE+
0.00.20.30.50.6Performance Improvement (b) TIGE model.
Figure 11: Robustness to expanded neighborhoods on
Wikipedia with TGN and TIGE models.
(Supplementary results for Section 5.5)
(i) Robustness Study to Noisy Neighborhoods. Supplementary
results for noisy MOOC and LastFM are provided. Additionally, we
report the box plot of the attention scores for perturbed neighbors
on noisy Reddit. (ii) Robustness Study to Expanded Neighborhoods.
Supplementary results for Wikipedia are presented. (iii) Case Study.
NodeID: 778NodeID: 990time
0.340.59
0.330.78
0.390.89
0.720.99
0.880.90
0.900.98
0.690.99
0.440.99prunedroute(",".%](".%,".'](".',%]prediction probability withneighborhoods.
fixedadaptiveFigure 12: Case study on Wikipedia with TGN model.
(Supplementary results for Section 5.6)
MOOC88.590.091.593.0Transductive AP (%)
MOOC89.690.491.292.0Inductive AP (%)
LastFM87.088.590.091.5Transductive AP (%)
LastFM90.091.593.0Inductive AP (%)      TIGE
(Backbone)w/o TA w/o RNS w/o NDP w/o APM w/o OM TIGE+
Figure 13: Ablation study on MOOC and LastFM with the
improved TIGE model. TIGE backbone is shown for
reference.
(Supplementary results for Section 5.7)
We present the additional case study derived from other sampled
nodes. (iv) Ablation Study. Supplementary ablation results for the
improved TIGE model and its variants.
4301