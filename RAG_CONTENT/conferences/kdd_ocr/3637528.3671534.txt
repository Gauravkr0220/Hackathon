SEFraud: Graph-based Self-Explainable Fraud Detection via
Interpretative Mask Learning
Kaidi Liâˆ—
Huawei Inc
Shenzhen, China
likaidi@huawei.comTianmeng Yangâˆ—
Peking University
Beijing, China
youngtimmy@pku.edu.cnMin Zhouâ€ 
Huawei Inc
Shenzhen, China
zhoumin27@huawei.com
Jiahao Meng
Peking University
Beijing, China
mengjiahao@stu.pku.edu.cnShendi Wang
Huawei Inc
Shenzhen, China
wangshendi@huawei.comYihui Wu
Huawei Inc
Shenzhen, China
ngngaifai@huawei.com
Boshuai Tan
ICBC Limited
Shanghai, China
tanbs@sdc.icbc.com.cnHu Song
ICBC Limited
Shanghai, China
songhu@sdc.icbc.com.cnLujia Pan
Huawei Inc
Shenzhen, China
panlujia@huawei.com
Fan Yu
Huawei Inc
Shenzhen, China
fan.yu@huawei.comZhenli Sheng
Huawei Inc
Shenzhen, China
shengzhenli@huawei.comYunhai Tongâ€ 
Peking University
Beijing, China
yhtong@pku.edu.cn
ABSTRACT
Graph-based fraud detection has widespread application in modern
industry scenarios, such as spam review and malicious account de-
tection. While considerable efforts have been devoted to designing
adequate fraud detectors, the interpretability of their results has
often been overlooked. Previous works have attempted to generate
explanations for specific instances using post-hoc explaining meth-
ods such as a GNNExplainer. However, post-hoc explanations can
not facilitate the model predictions and the computational cost of
these methods cannot meet practical requirements, thus limiting
their application in real-world scenarios. To address these issues,
we propose SEFraud, a novel graph-based self-explainable fraud de-
tection framework that simultaneously tackles fraud detection and
result in interpretability. Concretely, SEFraud first leverages cus-
tomized heterogeneous graph transformer networks with learnable
feature masks and edge masks to learn expressive representations
from the informative heterogeneously typed transactions. A new
triplet loss is further designed to enhance the performance of mask
learning. Empirical results on various datasets demonstrate the
effectiveness of SEFraud as it shows considerable advantages in
âˆ—Both authors contributed equally to this research and are listed alphabetically.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671534both the fraud detection performance and interpretability of pre-
diction results. Specifically, SEFraud achieves the most significant
improvement with 8.6% on AUC and 8.5% on Recall over the sec-
ond best on fraud detection, as well as an average of 10x speed-up
regarding the inference time. Last but not least, SEFraud has been
deployed and offers explainable fraud detection service for the
largest bank in China, Industrial and Commercial Bank of China
Limited (ICBC). Results collected from the production environment
of ICBC show that SEFraud can provide accurate detection results
and comprehensive explanations that align with the expert busi-
ness understanding, confirming its efficiency and applicability in
large-scale online services.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks; â€¢Theory of
computationâ†’Models of learning.
KEYWORDS
fraud detection, interpretability, graph neural networks
ACM Reference Format:
Kaidi Li, Tianmeng Yang, Min Zhou, Jiahao Meng, Shendi Wang, Yihui
Wu, Boshuai Tan, Hu Song, Lujia Pan, Fan Yu, Zhenli Sheng, and Yunhai
Tong. 2024. SEFraud: Graph-based Self-Explainable Fraud Detection via
Interpretative Mask Learning. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671534
1 INTRODUCTION
Recent years have witnessed the advances of technology and the
prosperity of various internet industry online services [ 28,29,37].
5329
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Kaidi Li et al.
Example
t1Fund nexus
Guarantor 
Equity Nexuss1
s2
s3SEFraudPrediction : t1 is fraudster 
Explanation for prediction
From edge:
Fund nexus
Equity Nexuss2
s3s1
t10.8
0.1 0.1
Guarantor From node feature:
Feature Score
Capital balance:10K 0.4
Subprime loans: Yes 0.3
Education background: Junior 
High School Education0.2
â€¦ â€¦
Figure 1: As a self-explainable model, SEFraud accepts the subgraph which contains both nodes features and edges as the input
and provides prediction results and the consistent explanations from edge weight and node feature. For the subgraph of â€™t1â€™ in
the example, the prediction rendered by SEFraud is: â€™t1â€™ is a fraudster. The explanation for this prediction is twofold: from
edge perspective, it is attributed to the fund nexus with user â€™s1â€™; from the node feature perspective, the three most important
features contributing to the prediction of â€˜t1â€™ as a fraudster are the capital balance, subprime loans, and education background.
The connected world brings convenience for society but also fos-
ters potential fraudulent activities [ 9,17,39]. Though data mining
algorithms and machine learning methods for detecting fraudsters
in collections of multidimensional points have been developed over
the years, graph machine learning has lately received much atten-
tion in the fraud detection community, owing to the success of
its applications in various graph-structured data [ 16,36,38,41].
In most fraud detection scenarios, users and fraudsters have rich
behavioral interactions when they are engaged in financial activi-
ties or post reviews, and such interactions can be represented as
graph-like data, which provides practical multifaceted information
for identifying fraud behaviors. Aligning with the development
of graph machine learning, various graph-based fraud detection
methods have been proposed[ 24,39] and applied to various fraud de-
tection scenarios such as malicious account detection [ 2,18,19,46],
anti-money laundry [15, 35], and spam review detection [3, 4, 45].
Prior research efforts in the realm of graph-based fraud detection
have primarily focused on incorporating the development of graph
neural networks (GNNs) to identify suspicious fraudsters by captur-
ing relations and connectivity patterns within networks [ 4,17,18].
For example, GeniePath [ 18] adopts an adaptive path layer for
exploring the importance of different-sized neighborhoods and
aggregating information from neighbors of different hops. CARE-
GNN [ 4] introduces two types of camouflage behaviors of fraud-
sters and further enhances the GNNs aggregation process with
reinforcement learning techniques. Flagging a user or transaction
as fraudulent is a complex task. False positives can lead to issues for
clients and have a detrimental impact on the platformâ€™s credibility.
In the realm of fraud detection, there is often an imbalance between
positive and negative samples, making it challenging to identify
the latter. To remedy the class imbalance problem and improve
the identification ability for fraudsters, a Pick and Choose Graph
Neural Network (PC-GNN ) [ 17] is proposed. The label-balanced
sampler is designed to pick nodes and edges for the sub-graph
training process, with a neighborhood sampler to pick neighbors.
However, these approaches are trained end-to-end in a black-box
manner, thus lacking transparency and interpretability for the de-
tection results and limiting their practical applications where high
confidence and explanations are required, such as financial frauddetection. Moreover, examining and interpreting identified nega-
tive samples requires substantial human resources. The methods
mentioned above face practical challenges due to the limited trans-
parency and interpretability of the predicted outcomes. xFraud [ 25]
partly addressed this problem and combined a detector with an
explainer, such as GNNExplainer [ 42], to provide explanations for
detection results. Nevertheless, GNNExplainer needs to be retrained
for every single instance; thus, it is time consuming and needs to
improve efficiency when explaining large-scale graph nodes. While
PGExplainer [ 21] improves the efficiency and accuracy compared
to the GNNExplainer, it primarily focuses on the edge perspective,
thereby overlooking the explanations for node features. These node
features are crucial in the prediction process of fraud detection
models. In addition, both GNNExplainer [ 42] and PGExplainer [ 21]
can only provide post-hoc explanations, which can not facilitate
the model predictions.
To facilitate detector models with recognizing important node
features and topology edges for higher fraud detection accuracy,
as well as providing explanations more efficiently, we propose a
new graph-based Self-Explainable Fraud detection method termed
as SEFraud, as depicted in Figure 1. Concretely, we leverage a cus-
tomized heterogeneous graph transformer with a feature attention
net and edge attention net, and train the model in a unified manner.
Besides, we hope the learned masks can maximize the identification
of essential edges and node features; if the weights are reversed, the
results will also be drastically interfered. To this end, a supervised
contrastive triplet loss is designed to enhance the mask learning.
Empirical studies on various datasets demonstrate that SEFraud
significantly improves both the fraud detection performance and
interpretability of prediction results. Ablation and case studies are
also provided to evaluate the proposed methods thoroughly.
In summary, the advantages of SEFraud are three folds:
â€¢Higher fraud detection performance. Through the learn-
able masking module, SEFraud re-weights the initial node
features and imposes constraints on the influence of edges
during the forward process, which enables SEFraud to more
effectively capture essential node features and fundamental
relations, thereby enhancing the performance of the fraud
detection model.
5330SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
â€¢Milliseconds response efficiency. Rather than retraining
the explanation network for each new instance, the learned
feature mask and edge mask are self-explanatory, which
allows SEFraud to generate precise and comprehensive ex-
planations within milliseconds.
â€¢Excellent application effect. SEFraud has been deployed
and offers explainable detection service for the largest bank
in China, Industrial and Commercial Bank of China Limited
(ICBC). Results on the real production environment with
real graphs from ICBC prove that SEFraud can provide both
accurate detection results and explanations that align with
their business understanding, showing its efficiency and
applicability in practice.
2 PRELIMINARIES AND RELATED WORK
Graph-based fraud detection can be applied to a wide range of sce-
narios, such as financial transactions [ 22], human behaviours [ 1],
and social media interactions [ 33], to uncover patterns and anom-
alies that indicate fraudulent activity. In this section, we introduce
the basic graph-based fraud detection task definition and review the
recent developments on GNNs and their heterogeneous variants.
Then, we briefly introduce works about GNNs and interpretability
and their applications in the fraud detection scenario.
2.1 Problem Formulation
Given a graphG=(V,E,X,Y)with node setVand edge setE;
ğ‘¥ğ‘–âˆˆX represents a ğ‘‘-dimension feature vector of node ğ‘£ğ‘–and
ğ‘¥ğ‘–âˆˆğ‘…ğ‘‘;Yis the set of labels for each node in V. Generally, graph-
based fraud detection can be viewed as a binary node classification
problem onGto classify the nodes into benign ( ğ‘¦ğ‘£=0) or fraudu-
lent (ğ‘¦ğ‘£=1) groups.
It is noted that if there are one type of node and various types of
relations inG, a multi-relation graph can be defined as E={ğ¸ğ‘Ÿ}|ğ‘…ğ‘Ÿ
withğ‘…different types of relations. Considering a heterogeneous
scenario that may contain different types of nodes and different
types of relations simultaneously, we further define a heteroge-
neous graph to be associated with a node type mapping function
ğœ:Vâ†’A and an edge type mapping function ğœ™:Eâ†’R .Aand
Rdenote the sets of predefined node types and relation types, and
|A|+|R| >2.
2.2 GNNs for Fraud Detection
GNNs are widely used for fraud detection by analyzing the connec-
tions and relationships between entities in a network [ 23]. Most
existing GNNs [ 8,14,31] adopt message-passing framework [ 7],
which applies local aggregation to learn node representations. At
each propagation step ğ‘¡, the hidden representation of node ğ‘£is
derived by:
â„ğ‘¡
ğ‘£=ğ‘“(â„ğ‘¡âˆ’1
ğ‘£,aggr(â„ğ‘¡âˆ’1
ğ‘¢|ğ‘¢âˆˆğ‘(ğ‘£))), (1)
whereâ„0ğ‘£=ğ‘¥ğ‘£,aggr(Â·)denotes a differentiable, permutation in-
variant function to aggregate neighbor information, ğ‘“(Â·)is a trans-
formation function between two propagation steps, ğ‘(ğ‘£)is the
connected neighbors of ğ‘£.
Many previous works conduct semi-supervised node classifica-
tion tasks on single-relation graphs [ 18] or define multi-relationgraph [ 4,17,20,32] to tackle fraud detection tasks. The common
insight is to design different massage passing mechanisms for ag-
gregating neighborhood information. For example, GeniePath [ 18]
learns convolutional layers and neighbor weights using LSTM [ 10]
and the attention mechanism [ 31]. SemiGNN [ 32] applies a GNN-
based hierarchical attention mechanism to detect fraudsters on Ali-
pay. GraphConsis [ 20] and CARE-GNN [ 4] filter dissimilar neigh-
bors before aggregation to discover camouflage fraudsters. PC-
GNN [ 17] identifies and solves the label imbalance issue by node
resampling. Heterogeneous GNNs [ 6,11,34,40] are capable of
learning embeddings for heterogeneous graphs but still remain
underexplored for fraud detection scenarios.
2.3 Explainability of GNNs
Despite the notable achievements of graph-based methods, their
lack of transparency hinders easy comprehension of the predictions.
Nevertheless, enhancing explanations and providing the model
credibility which offers valuable guidance to business professionals
in terms of prevention and control holds significant importance,
especially for applications such as fraud detection. Recently, several
models have been proposed to explain the predictions of GNNs,
such as XGNN [ 43], GNNExpaliner [ 42], and PGExplainer [ 21]. GN-
NExplainer [ 42] learns soft masks for edges and node features to
explain the predictions via mask optimization. PGExplainer [ 21]
learns approximated discrete masks for edges to explain the predic-
tions via the reparameterization trick. XGNN [ 43] trains a graph
generator to provide model-level explanations and is only used for
graph classification models. Few works pay efforts to provide expla-
nations for graph-based fraud detection. xFraud [ 25] combines the
fraud detector with an explainer using GNNExplainer and provides
explanations to facilitate further processes. However, these meth-
ods can only provide post-hoc explanations. Besides, GNNExpaliner
needs to re-train the explanation network for each new instance
and thus is criticized for its low efficiency. While PGExplainer [ 21]
enhances both the accuracy and efficiency of explanations com-
pared to GNNExplainer [ 42], it still has limitations. Specifically,
PGExplainerâ€™s explanations are solely from the perspective of edge
aspects, leading to a certain degree of incompleteness. This becomes
particularly problematic in scenarios that necessitate a comprehen-
sive explanation encompassing both node features and relations
simultaneously. By incorporating a self-explanatory mask learning
module into the GNN model, we aim to improve representation
learning of the GNN detector to enhance the modelâ€™s prediction
ability, as well as generate high-quality and comprehensive expla-
nations with milliseconds response efficiency.
3 THE PROPOSED METHOD
In this section, we present the proposed Self-Explainable Fraud
framework. As sketched in Figure 2, SEFraud first utilizes cus-
tomized heterogeneous graph transformer networks to extract
meaningful representations from diverse transaction types, which
are then incorporated by the learnable feature masks and edge
masks, generating the reweighted graph. A novel triplet loss is
specifically designed to enhance the mask learning. We detail the
information aggregation and interpretative mask learning in Sec-
tion 3.1 and 3.2, respectively. Then we explain how to aggregate
5331KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Kaidi Li et al.
ts1s2s3t

 



		s1s2s3FNet1FNet2FNet3FNet2 





	


		e1e2e3!"1"2"3&1&2&3Hetero-grapHeterogeneous Convolution Layerblocks
	


	

	


	
concatenateFeature  Mask Learning
ENet	
0.10.30.6


dot product!'"1'"2'"3'&3'Weighted  Hetero-grapGNN<)*+,)*-, -*>Edge  Mask LearningContrasting Triplet Interpretative Mask LearningHeterogeneous Convolution Layer
+x++
+&1'&2'ShareParam"3'"1'!'"2'
Figure 2: The architecture of SEFraud. A heterogeneous convolution layer is utilized to aggregate the hetero-graph information
and generate the feature embedding for each node. These feature embeddings, raw features, and node type encodings for each
node are then concatenated to form the input for FNet. An edge embedding consists of the node embeddings at its two ends,
and concatenates with the edge type encodings to form the the input for ENet. The learned feature masks and edge masks are
further leveraged to reconstruct a weighted hetero-graph, which serves as the input for the GNN/Detection model. A contrastive
triplet loss is then constructed based on the output of the model for the training process.
information from the reweighted graph and construct the contrast-
ing triplet in Section 3.3.
3.1 Heterogeneous Convolution Layer
Considering various node types and edge types in real-world in-
dustry scenarios, we utilize a Heterogeneous Graph Transformer
(HGT) [ 11] to construct our convolution layer, and aggregate het-
erogeneous neighborhood information from source nodes to get
a contextualized representation for target node. The process can
be decomposed into three components: Heterogeneous Mutual
Attention, Heterogeneous Message Passing, and Target-Specific
Aggregation. Denoting the output of the ğ‘™âˆ’ğ‘¡â„layer isğ»ğ‘™, we can
stack multiple layers to learn the node representations as follows:
ğ»ğ‘™+1=ğ´ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘’
âˆ€ğ‘ âˆˆğ‘(ğ‘¡),âˆ€ğ‘’âˆˆğ¸(ğ‘ ,ğ‘¡)(ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ ,ğ‘¡)Â·ğ‘€ğ‘’ğ‘ ğ‘ ğ‘ğ‘”ğ‘’(ğ‘ )),(2)
whereğ‘(ğ‘¡)is the neighborhood nodes of target node ğ‘¡,ğ¸(ğ‘ ,ğ‘¡)is
the edges connecting source node ğ‘ and target node ğ‘¡.
3.1.1 Heterogeneous Mutual Attention. Similar to the vanilla Trans-
former [ 30], HGT maps target node ğ‘¡into a Query vector, source
nodeğ‘ into a Key vector, and calculate their dot product as attention.Considering â„attention heads, the calculation is be formulated as:
ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ ,ğ‘’,ğ‘¡)=ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥
ğ‘ âˆˆğ‘(ğ‘¡)(âˆ¥
ğ‘–âˆˆ[1,â„]ğ´ğ‘‡ğ‘‡ -ğ»ğ‘’ğ‘ğ‘‘ğ‘–(ğ‘ ,ğ‘’,ğ‘¡)),(3)
ğ´ğ‘‡ğ‘‡ -ğ»ğ‘’ğ‘ğ‘‘ğ‘–(ğ‘ ,ğ‘’,ğ‘¡)=ğ¾ğ‘–(ğ‘ )ğ‘Šğ´ğ‘‡ğ‘‡
ğœ™(ğ‘’)ğ‘„ğ‘–(ğ‘¡), (4)
ğ¾ğ‘–(ğ‘ )=ğ¾-ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘–
ğœ(ğ‘ )(ğ»ğ‘™âˆ’1[ğ‘ ]), (5)
ğ‘„ğ‘–(ğ‘¡)=ğ‘„-ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘–
ğœ(ğ‘¡)(ğ»ğ‘™âˆ’1[ğ‘¡]), (6)
whereğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ means linear projection and ğœ(Â·)represent the node
types.
3.1.2 Heterogeneous Message Passing. To incorporate the meta
relations of different types, the multi-head ğ‘€ğ‘’ğ‘ ğ‘ ğ‘ğ‘”ğ‘’ is further cal-
culated by applying different edge type projections as:
ğ‘€ğ‘’ğ‘ ğ‘ ğ‘ğ‘”ğ‘’(ğ‘ ,ğ‘’,ğ‘¡)=âˆ¥
ğ‘–âˆˆ[1,â„]ğ‘€ğ‘†ğº -ğ»ğ‘’ğ‘ğ‘‘ğ‘–(ğ‘ ,ğ‘’,ğ‘¡), (7)
ğ‘€ğ‘†ğº -ğ»ğ‘’ğ‘ğ‘‘ğ‘–(ğ‘ ,ğ‘’,ğ‘¡)=ğ‘€-ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘–
ğœ(ğ‘ )(ğ»ğ‘™âˆ’1[ğ‘ ])ğ‘Šğ‘€ğ‘†ğº
ğœ™(ğ‘’). (8)
3.1.3 Target-Specific Aggregation. In this step, the attention vector
and corresponding message from source node ğ‘ are combined to
5332SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
get the neighborhood information as:
Ë†ğ»ğ‘™[ğ‘¡]=âˆ‘ï¸
ğ‘ âˆˆğ‘(ğ‘¡)ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ ,ğ‘’,ğ‘¡)Â·ğ‘€ğ‘’ğ‘ ğ‘ ğ‘ğ‘”ğ‘’(ğ‘ ,ğ‘’,ğ‘¡). (9)
Finally, target-specific linear projection are applied to Ë†ğ»ğ‘™[ğ‘¡]and
get the updated vector ğ»ğ‘™[ğ‘¡]with a residual connection:
ğ»ğ‘™[ğ‘¡]=ğ‘‡-ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğœ™(ğ‘¡)(ğœ(Ë†ğ»ğ‘™[ğ‘¡]))+ğ»ğ‘™âˆ’1[ğ‘¡], (10)
whereğœis the activation function.
3.2 Interpretative Mask Learning
Interpretability in graph-based fraud detection tasks includes two
aspects. Firstly, influential edges offer topology explanations for
correlations between nodes. Secondly, certain critical features pro-
vide the properties of nodes to be flagged as fraudsters. By stacking
ğ¿layers, we can obtain the node representations of the entire graph,
denoted asğ»(ğ¿). Unlike the attention weights in HGT that vary
across different layers, we aim to learn a unified, consistent feature
mask and edge mask among all aggregating layers.
In order to achieve this, we integrate a feature attention net-
work (FNet) and an edge attention network (ENet) following the
heterogeneous convolution layers. Given the heterogeneity of node
types and edge types, we also implement type encoding for each
node type and edge type. Specifically, for the feature mask, we
concatenate the initial feature ğ»0, the aggregated feature ğ»ğ¿, and
the node type encoding of each node. We then utilize a ğ¹ğ‘ğ‘’ğ‘¡ for
different node types to learn the node mask. For the edge mask, we
concatenate the source node and target node features along with
their edge type encodings. We then employ an ğ¸ğ‘ğ‘’ğ‘¡ to learn the
edge mask. Formally, the learned feature mask ğ‘†ğ¸-ğ‘€ğ‘ğ‘ ğ‘˜ğ‘›and edge
maskğ‘†ğ¸-ğ‘€ğ‘ğ‘ ğ‘˜ğ‘’are acquired by:
ğ‘†ğ¸-ğ‘€ğ‘ğ‘ ğ‘˜ğ‘›=ğ¹ğ‘ğ‘’ğ‘¡
âˆ€ğ‘£âˆˆğ‘(ğ»0âˆ¥ğ»ğ¿âˆ¥ğ‘ğ‘‡ğ¸(ğœ(ğ‘£)), (11)
ğ‘†ğ¸-ğ‘€ğ‘ğ‘ ğ‘˜ğ‘’=ğ¸ğ‘ğ‘’ğ‘¡
âˆ€ğ‘’âˆˆ(ğ‘ ,ğ‘’,ğ‘¡))(ğ»ğ¿[ğ‘ ]âˆ¥ğ»ğ¿[ğ‘¡]âˆ¥ğ¸ğ‘‡ğ¸(ğœ™(ğ‘’)), (12)
whereğ¹ğ‘ğ‘’ğ‘¡(Â·)andğ¸ğ‘ğ‘’ğ‘¡(Â·)are Multi-Layer Perceptrons, ğ‘ğ‘‡ğ¸(Â·)
andğ¸ğ‘‡ğ¸(Â·)are node type encoding function and edge type encoding
function, respectively.
3.3 Contrastive Triplet Loss
With the learned feature mask and edge mask, the initial node fea-
tures are re-weighted, and the influence of edges is constrained
during the forward process of the model. Intuitively, the modelâ€™s
prediction results with learned masks should align closely with
the ground truth. Conversely, if we assign negative weights to the
masks, the modelâ€™s prediction results should diverge significantly.
Denoting the positive prediction results of node ğ‘–asğ‘ğ‘–+, the nega-
tive results as ğ‘ğ‘–âˆ’, and the ground truth as ğ‘¦ğ‘–, we have designed
a contrastive triplet loss function, denoted as Lğ‘¡ğ‘Ÿ, that operates
on each triplet pair <ğ‘¦ğ‘–,ğ‘ğ‘–+,ğ‘ğ‘–âˆ’>. This function penalizes the
distance between the positive pair and the negative pair with a
marginğ›¼:
Lğ‘¡ğ‘Ÿ=âˆ‘ï¸
ğ‘–âˆˆğ‘‰ğ‘™ğ‘šğ‘ğ‘¥(0,ğ‘‘ğ‘–ğ‘ (ğ‘¦ğ‘–,ğ‘ğ‘–+)âˆ’ğ‘‘ğ‘–ğ‘ (ğ‘¦ğ‘–,ğ‘ğ‘–âˆ’)+ğ›¼), (13)
where the cross-entropy are utilized as the ğ‘‘ğ‘–ğ‘ (Â·)metric andğ‘‰ğ‘™is
the training set.3.4 SEFraud for Detection and Explanation
Finally, we construct the SEFraud framework for fraud detection
tasks, incorporating the aforementioned designs. Besides, the frame-
work provides explanations through the learned feature mask and
edge mask. The accuracy of fraudster classification is guaranteed
with a cross-entropy loss, denoted as Lğ‘ğ‘’:
Lğ‘ğ‘’=âˆ‘ï¸
ğ‘–âˆˆğ‘‰ğ‘™ğ‘¦ğ‘–logğ‘ğ‘–+, (14)
To enhance both the effect of classification and mask learning,
we train the model with a combination of the cross-entropy classifi-
cation lossLğ‘ğ‘’and the contrasting triplet loss Lğ‘¡ğ‘Ÿ. The combined
loss function is defined as follows:
L=(1âˆ’ğœ†)Â·Lğ‘ğ‘’+ğœ†Â·Lğ‘¡ğ‘Ÿ, (15)
where theğœ†is a hyperparameter to balance fraudster classification
and interpretive mask learning.
4 EXPERIMENTS
For a comprehensive evaluation of SEFraud, we conduct experi-
ments for both fraud detection and interpretation tasks, and com-
pare it with various baselines. An ablation study is also performed
to show the effectiveness of our designs. Case studies, time effi-
ciency comparison, and deployment effect are further provided to
demonstrate the advantages of our method in industry application
scenarios.
4.1 Datasets
The datasets include three fraud detection datasets and four widely
used explanation datasets, with the statistics summarized in Table 1
and Table 2.
4.1.1 Fraud detection datasets.
â€¢Yelp andAmazon[ 4]. The Yelp dataset includes hotel and
restaurant reviews filtered (spam) and recommended (legiti-
mate) by Yelp. The nodes in the graph are reviews with three
relations: the reviews posted by the same user, the reviews
under the same product with the same star rating, and the
reviews under the same product posted in the same month.
The Amazon dataset includes product reviews under the
Musical Instruments category. The nodes in the graph of
the Amazon dataset are users with 100-dimension features
and also contain three relations: users reviewing at least one
same product, users having at least one same star rating
within one week, and users with top-5% mutual review TF-
IDF similarities. The users with more than 80% helpful votes
are benign entities and users with less than 20 helpful votes
are fraudulent entities. Handcrafted features are extracted
from prior works [ 26,44] as the raw node features for the
datasets, respectively.
â€¢ICBC. The financial fraud detection dataset, termed ICBC, is
generated by TabSim1, a tool based on the statistical charac-
teristics of debts and customers provided by ICBC (Industrial
and Commercial Bank of China), one of the largest banks
in China. The nodes of ICBC data represent customers and
1TabSim is open-source and can be found at the following link:
https://gitee.com/mindspore/xai/wikis/TabSim
5333KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Kaidi Li et al.
Table 1: Statistics of the fraud detection datasets.
Fraud Detection Yelp Amazon ICBC
#Nodes 45954 11944 100000
#Features 32 25 24
#Edges 3846979 4398392 102472
#Edge Types 3 3 5
#Classes 2 2 2
Table 2: Statistics of the interpretation datasets.
Interpretation BA-2motifs BA-Shapes Tree-Cycles Tree-Grids
#Graphs 1000 1 1 1
#Nodes 25000 700 871 1231
#Edges 51392 4110 1950 3410
#Labels 2 4 2 2
their debts, each containing 24 features, including profession,
education background, and various asset metrics. The edges
depict relations between nodes, encompassing five distinct
types: equity association, card holder, against pledge, staff,
and others.
4.1.2 Interpretation datasets.
â€¢BA-Shapes, Tree-Cycles andTree-Grids. These datasets
are synthetic graphs for node-level explanation. BA-Shapes
is a single graph consisting of a base Barabasi-Albert (BA)
graph with 300 nodes and 80 â€œhouseâ€-structured motifs.
These motifs are attached to randomly selected nodes from
the BA graph. After that, random edges are added to perturb
the graph. Nodes in the base graph are labeled with 0; the
ones locating at the top/middle/bottom of the â€œhouseâ€ are
labeled with 1,2,3, respectively. For the Tree-Cycles dataset,
an 8-level balanced binary tree is adopted as the base graph.
An 80 six-node cycle motifs are attached to randomly se-
lected nodes from the base graph. Tree-Grid is constructed
similarly to Tree-Cycles, except that 3-by-3 grid motifs are
used to replace the cycle motifs.
â€¢BA-2motifs. For graph level explanation, BA-2motifs adopts
the BA graphs as base graphs. Half of the graphs are attached
with â€œhouseâ€ motifs, and the rest are attached with five-
node cycle motifs. Graphs are assigned to one of 2 classes
according to the type of attached motifs.
4.2 Baselines
To verify the ability of SEFraud in graph-based fraud detection
tasks, we compare it with various GNN baselines under the semi-
supervised learning setting, including:
â€¢Four representative general GNN models for node classifica-
tion: GCN [14], GAT [31], GraphSAGE [8] and RGCN [27].
â€¢Two strong graph-based fraud detection methods without
explainability: GeniePath [18] and CARE-GNN [4].
â€¢An explainable fraud prediction system xFraud [ 25] that
consists of a heterogeneous GNN detector and a post-hoc
explainer.Among those baselines, GCN, GAT, GraphSAGE, and GeniePath
are built on homogeneous graphs. Thus, all relations are merged to-
gether. RGCN and CARE-GNN run on multi-relation graphs, while
xFraud and SEFraud can handle information from different relations
and node types based on the heterogeneous graph convolution.
4.3 Experimental Settings
To make a fair comparison, we closely follow the experimental
procedure with [ 4] on fraud detection and [ 21,42] on explanation.
We use the same feature vectors, labels, and train/val/test splits for
all baselines. For SEFraud, the layers of heterogenous convolution
ğ¿is 2, number of heads â„is 4, margin of contrastive triplet loss
ğ›¼is 0.1, trade off parameter of combined loss function ğœ†is 0.3.
Other settings are tuned from: hidden size âˆˆ{8,16,32,64}, learning
rateâˆˆ {5ğ‘’-3,1ğ‘’-2}, and L2 regularization weight âˆˆ {1ğ‘’-3,5ğ‘’-4}.
We implement our methods using Pytorch Geometric [ 5] with the
Adam optimizer [13].
4.4 Fraud Detection Performance
In most fraud detection scenarios, the nodes are predominantly be-
nign, with the proportion of fraudulent entities being significantly
low. From the application perspective, the task of fraud detection is
supposed to pay more attention to recognizing potential fraudsters.
Thus, we report the ROC-AUC (AUC) and Recall metrics follow-
ing [ 4] to evaluate the overall performance of all methods. AUC is
computed based on the relative ranking of prediction probabilities
of all instances, which could eliminate the influence of imbalanced
classes. Recall measures how the model correctly identifies posi-
tive instances (true positives) from all actual positive samples, thus
representing the ability to flag potential fraudsters.
Table 3 summarizes the prediction results of fraud detection
among three datasets. Most GNNs built on homogeneous graphs
such as GCN and GeniePath, or simply aggregate information from
different relations such as RGCN, show inferior performance com-
pared with methods that can handle complex multi-relation graphs
(CARE-GNN) or apply heterogeneous graph convolution (xFraud
and SEFraud) on Yelp and Amazon. In particular, xFraud achieves
remarkable improvements on these two datasets. It is worth men-
tioning that GeniePath outperforms other baselines on the ICBC
dataset, which may be because ICBC is more sparse, and it is critical
to explore essential neighborhoods. In a sparse and noisy scenario,
GeniePath and CARE-GNN, which are capable of adaptively filter-
ing neighborhood information, can even overpass xFraud.
On the one hand, SEFraud tackles heterogeneity compared with
general GNNs. On the other hand, compared with xFraud, SEFraud
can enhance the heterogeneous convolution process by learning
feature mask and edge mask, thus consistently outperforming all
baselines on both AUC and Recall metrics. Specifically, SEFraud
achieves an 8.6% improvement on AUC and an 8.5% improvement
on Recall over xFraud on the Yelp dataset.
We further conducted ablation studies to evaluate the effective-
ness of the each component in the proposed SEFraud. Specifically,
we denoted models without a specific component using w/oand
re-evaluate the performance. The results presented in the bottom
part of Table 3 demonstrate that removing any of the components
leads to performance degradation, confirming the efficacy of each
5334SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 3: Fraud detection performance comparasion. The best result on per benchmark is highlighted.
Dataset Yelp Amazon ICBC
Metrics AUC Recall AUC Recall AUC Recall
GCN 52.47 50.81 74.34 67.45 50.72 50.53
GAT 56.24 54.52 75.16 65.51 87.70 79.22
GraphSAGE 54.00 52.86 75.27 70.16 93.93 93.02
RGCN 53.38 50.43 74.68 67.68 92.42 91.29
GeniePath 55.91 50.94 72.65 65.41 99.27 94.48
CARE-GNN 75.70 71.92 89.73 88.48 98.96 97.18
XFraud 79.88 72.46 92.49 88.26 95.68 95.42
SEFraud(ours) 86.77 78.64 93.23 88.67 99.69 99.38
SEFraud(w/o triplet loss ) 83.51 74.23 92.47 86.23 96.68 96.37
SEFraud(w/o node mask ) 86.21 78.29 93.16 87.16 96.97 96.36
SEFraud(w/o edge mask ) 84.06 75.91 92.64 85.47 99.37 98.17
Table 4: Interpretation Performance (AUC).
BA-2motifs BA-Shapes Tree-Cycles Tree-Grids
GRAD 71.7 88.2 90.5 61.2
ATT 67.4 81.5 82.4 66.7
Gradient 77.3 - - -
GNNExplainer 74.2 92.5 94.8 87.5
PGExplainer 92.6Â±2.1 96.3Â±1.1 98.7Â±0.7 90.7Â±1.4
SE-Mask 97.8Â±1.6 99.8Â±0.1 96.5Â±1.3 95.8Â±2.7
module in the proposal. Notably, for denser graphs like Yelp and
Amazon, the edge masks has a more significant impact than the
feature masks as considerable degradation on the performance is
observed if the corresponding module is dropped. Conversely, in
the case of ICBC where edges are sparse, the node mask plays a
critical role. Additionally, the triplet loss is essential for learning
high-quality masks and exhibits a noticeable decrease on perfor-
mance when removed.
4.5 Interpretability
Given that the FNet and ENet are trained alongside the detection
model, a higher level of model prediction accuracy demonstrates
the validity of the learned masks applied to the original graphs.
The learned masks reflect the significance of the node features and
edges. Therefore, they are interpretative and can directly be utilized
to elucidate the model predictions.
4.5.1 Quantitative evaluation. To quantitatively verify the inter-
pretability of the learned masks, we follow the setting in GNNEx-
plainer [ 42] and PGExplainer [ 21] to construct experiments on
the synthetic datasets on node classification and graph classifica-
tion tasks. Concretely, we incorporate our proposed mask learning
paradigm (termed as SE-Mask) into a GCN backbone used in GN-
NExplainer and PGExplainer, and leverage the learned edge masks
as interpretation when GCN training is finished.
The explanation problem is formalized as a binary classification
task, where edges in the ground-truth explanation are treated as
labels, and importance weights given by the explanation method
BA-2MOTIFS BA-SHAPES TREE- CYCLES TREE- GRIDS
Explanations 
by 
PGExplainer
Explanations 
by 
SE-MaskExplanations 
by 
GNNExplainerMotifsBase
Figure 3: Qualitative explanation examples comparison.
Node labels are represented by their colors. Explanations
of instance in each dataset are highlighted by bold black
edges ranked by their importance weights.
are viewed as prediction scores. A suitable explanation method
assigns higher weights to edges in the ground truth motifs than
outside ones. Thus, AUC is adopted as the metric for quantitative
evaluation. Results in Table 4 show that our proposed SE-Mask
achieves remarkable interpretation performances compared with
baseline explanation methods in three datasets. For Tree-Cycles,
SE-Mask performs inferior to PGExplainer but still overpasses GN-
NExpalainer. The average lifts on the AUC of SE-Mask over GN-
NExpalainer and PGExplainer are 12.8% and 3.2%, demonstrating
its superior interpretability.
4.5.2 Qualitative evaluation. We also choose an instance for each
dataset and visualize its explanations given by SE-Mask and base-
lines in Figure 3. We first compute the edge weights for each in-
stance and highlight the top-K edges where K is the number of
5335KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Kaidi Li et al.
Eg1:Eg2:
t1
s4s5s2 s3
Equity Nexus
Social Connection s
Behavioral Score ï¼š65
Accumulated Released Amount ï¼š50,000RMB
Overdue principal balance ï¼š10,000RMB
â€¦..s2s1
Guarantor Fund nexus
Credit guarantee method ï¼šmortgage
Subprime Loan ï¼šyes
Contract Duration Days:  3 years
â€¦..Trade Nexust2Guarantor 
Fund nexuss1
Figure 4: Explanation examples from ICBC dataset.
edges in each explanation motif. For both graph classification and
node classification tasks, SE-Mask can accurately identify edges
inside the explanation motifs (i.e., the "house," "cycle," and "3-by-3
grid"), while GNNExplainer and PGExplainer have confused some
essential edges. These visualizations demonstrate that SE-Mask
possesses a more robust ability for explanation.
To further validate the credibility of our explanations on finan-
cial fraud detection, we randomly selected 100 nodes that were
predicted to be fraudsters, which were further manually checked
and confirmed by business experts of ICBC. The experts corrobo-
rated that the explanations provided by our system, SEFraud, are
not only logical but also congruent with their extensive business
acumen. This validation underscores the credibility of our system
in the realm of financial fraud detection. Two detailed examples are
sketched in Figure 4.
For node prediction â€˜t1â€™, from the perspective of edge analysis,
the most pivotal edge contributing to its classification as a fraud-
ulent entity is its equity nexus with another identified fraudster,
â€˜s2â€™. When considering node features, the three most significant at-
tributes include â€˜Behavioral Score, â€™ â€˜Accumulated Released Amountâ€™,
and â€˜Overdue principal balanceâ€™. According to the expert analysis
from ICBC, a behavioral score of 65 is considered relatively low,
indicating a high risk. Furthermore, examining the other two fea-
tures reveals that â€˜t1â€™ has already received a substantial accumu-
lated released amount. However, the overdue balance generated
constitutes a significant proportion compared to the accumulated
released amount. This combination of factors further substantiates
the classification of â€˜t1â€™ as fraudulent. In the second example, the
classification of â€˜t2â€™ as a fraudulent entity is primarily predicated
on the â€˜Guarantorâ€™ edge from â€˜S2â€™, which is already identified as a
high-risk fraudster. Regarding the node features, â€˜t2â€™ has provided
a mortgage as the method of credit guarantee associated with a
relatively extended contract duration. However, it is noteworthy
that â€˜t2â€™ also possesses a subprime loan. The combination of these
factors suggests that â€˜t2â€™ may lack the financial capacity to repay
the debt. This amalgamation of circumstances further substantiates
the classification of â€˜t2â€™ as a fraudulent entity. These two examples
demonstrate how the explanations provide insights into the rea-
sons behind the fraudsterâ€™s predictions, aiding in understanding
and interpreting the modelâ€™s decisions.Table 5: Analysis with different GNN encoders.
Dataset ICBC
Metrics AUC Recall Epoch Time
GCN based 63.41Â±3.17 58.76Â±2.62 0.11s
GAT based 94.23Â±2.31 93.15Â±1.64 0.17s
HAN based 98.41Â±1.55 97.26Â±1.72 0.23s
HGT based(SEFraud) 99.69Â±1.27 99.38Â±2.31 0.19s
Table 6: Hyper-parameters analysis on ICBC dataset.
Differentğœ† 0 0.1 0.2 0.3 0.4 0.5
AUC 96.68 99.10 99.37 99.69 98.43 98.18
Recall 96.37 98.79 99.05 99.38 96.98 96.38
Differentğ›¼ 0 0.1 0.2 0.3 0.4 0.5
AUC 99.09 99.69 99.68 99.39 98.78 98.76
Recall 98.19 99.38 98.79 98.19 97.59 97.59
4.6 Model Analysis
Different GNN encoders. While it is possible to use alternative
GNNs like GCN as feature encoders, ignoring the heterogeneity
may result in performance degradation. In general, the data in
ICBC contains heterogeneous types of information related to vari-
ous nodes and relations thus we opt to construct our model using a
customized heterogeneous graph transformer , which has demon-
strated its superiority in our baseline XFraud [ 25]. To provide a
thorough analysis of model design, we also conduct experiment
to evaluate different kinds of GNN based encoders. As shown in
Table 5, heterogenous GNNs (i.e. HAN [ 34] and HGT [ 11]) signif-
icantly outperform homogenous GNNs including GCN and GAT.
HGT has achieved a higher performances as well as less time con-
sumption than HAN.
Hyper-parameters analysis. We also conducted a hyper-parameter
analysis on the ICBC dataset. One of the key hyperparameters is ğœ†,
which balances the classification loss and contrastive tripplet loss.
As shown in the Tabel 6 , the performance initially improves and
then gradually declines when ğœ†ranges from 0 to 0.5. This behavior
can be attributed to the increasing weight of the triplet loss, which
enhances instance discrimination but may weaken classification
performance beyond a certain threshold. A similar trend is observed
for another hyperparamete ğ›¼, which acts as a margin in the triplet
loss. A larger margin encourages the model to distinguish important
edges and node features more confidently. However, excessively
large margins can make it challenging to reduce the triplet loss to
zero, potentially harming the overall model performance.
4.7 Time Comparison
Efficiency plays a critical role in industry applications, especially for
financial systems requiring milliseconds response speed. For each
instance, GNNExplainer needs to re-train the explanation network
before generating a new explanation and thus is criticized for its
low efficiency. The explanation network in PGExplainer is shared
across the population of instances and can be utilized to explain
5336SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 7: Inference Time (ms).
BA-2motifs BA-Shapes Tree-Cycles Tree-Grids
GNNExplainer 934.72ms 650.60ms 690.13ms 713.40ms
#speed-up 1x 1x 1x 1x
PGExplainer 80.13ms 10.92ms 6.36ms 6.72ms
#speed-up 12x 59x 108x 106x
SE-Mask 10.55ms 0.96ms 0.54ms 0.60ms
#speed-up 86x 678x 1278x 1189x
new instances in the inductive setting. However, it also needs to
calculate the weights of the subgraph in each explaining process.
In contrast, explanation weights in SE-Mask are trained with the
backbone GNN model and can be directly extracted for explaining,
which only takes milliseconds. As shown in Tabel 7, we compared
the inference time of generating a new explanation instance with
the three kinds of methods. SE-Mask achieves over 1000x speed-up
on explain Tree-Cycles and Tree-Grids datasets, having a signifi-
cant advantage on time efficiency. In an industry financial fraud
detection system, GNNExplainer requires approximately 7 seconds
to generate a reasonable explanation for a flagged fraudster. In
contrast, our proposed SE-Mask accomplishes the same task in just
0.4 milliseconds.
4.8 Deployment and application
In the studyâ€™s second phase, the proposed SEFraud is further de-
ployed in ICBCâ€™s production environment with the Mindspore ser-
vice [ 12]. The modelâ€™s performance is measured based on ICBCâ€™s
real data, which is divided into four parts corresponding to four
months. Each month contains approximately 86,000 nodes and
18,000 edges. The model is trained using one monthâ€™s data and
tested its predictive performances and explanation effects using
the following monthâ€™s data. The feedback received from ICBC is
highly positive and inspiring. During the real business practice, The
average prediction AUC of SEFraud is 97%, indicating a high level
of accuracy, with a recall rate of 0.98. Additionally, the inference
time taken for predicting and providing explanations for a single
node averaged just at 0.4ms. Moreover, the generated explanations
for the predicted fraudsters are well-received by the experts from
ICBC, demonstrating the effectiveness of our approach.
5 CONCLUSION
Current practical implementations of graph fraud detection appli-
cation are limited for scenarios that require both high confidence
(i.e., explainable detection) and efficiency. In this paper, we intro-
duce the SEFraud, a highly efficient self-explainable fraud detection
framework. The interpretive feature mask and edge mask are inte-
grated into customized heterogeneous graph transformer networks
to enhance the detection modelâ€™s prediction ability and provide ex-
planations. And a specific contrastive triplet loss is further designed
to augment the mask learning. Experimental results demonstrate
the advantages of SEFraud in various fraud detection tasks and it is
able to provide reasonable explanations within milliseconds. The
deployment and verification of SEFraud in ICBC, one of the largestbanks in China, further verify its efficiency and applicability in real
industry environment.
6 ACKNOWLEDGMENT
This work was supported by National Key R&D program of China
under Grant No.2023YFC3807603, No.2021ZD0110400. We also ap-
preciate the help of cooperation partners in this project, and all the
thoughtful and insightful suggestions from reviewers.
REFERENCES
[1]L Karl Branting, Flo Reeder, Jeffrey Gold, and Timothy Champney. 2016. Graph
analytics for healthcare fraud risk estimation. In 2016 IEEE/ACM International
Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE,
845â€“851.
[2]Adam Breuer, Roee Eilat, and Udi Weinsberg. 2020. Friend or faux: Graph-based
early detection of fake accounts on social networks. In Proceedings of The Web
Conference 2020. 1287â€“1297.
[3]Leyan Deng, Chenwang Wu, Defu Lian, Yongji Wu, and Enhong Chen. 2022.
Markov-driven graph convolutional networksfor social spammer detection. IEEE
Transactions on Knowledge and Data Engineering (2022).
[4]Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. 2020.
Enhancing graph neural network-based fraud detectors against camouflaged
fraudsters. In Proceedings of the 29th ACM international conference on information
& knowledge management. 315â€“324.
[5]Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with
PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and
Manifolds.
[6]Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020. Magnn: Metap-
ath aggregated graph neural network for heterogeneous graph embedding. In
Proceedings of The Web Conference 2020. 2331â€“2341.
[7]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
conference on machine learning. PMLR, 1263â€“1272.
[8]William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In Proceedings of the 31st International Conference on
Neural Information Processing Systems. 1025â€“1035.
[9]Waleed Hilal, S Andrew Gadsden, and John Yawney. 2022. Financial fraud: a
review of anomaly detection techniques and recent advances. Expert systems
With applications 193 (2022), 116429.
[10] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735â€“1780.
[11] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous
graph transformer. In Proceedings of the web conference 2020. 2704â€“2710.
[12] Ltd. Huawei Technologies Co. 2022. Huawei MindSpore AI Development Frame-
work. In Artificial Intelligence Technology. Springer, 137â€“162.
[13] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[14] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph
convolutional networks. International Conference on Learning Representations
(ICLR) (2017).
[15] Xiangfeng Li, Shenghua Liu, Zifeng Li, Xiaotian Han, Chuan Shi, Bryan Hooi, He
Huang, and Xueqi Cheng. 2020. Flowscope: Spotting money laundering based
on graphs. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34.
4731â€“4738.
[16] Junfeng Liu, Min Zhou, Shuai Ma, and Lujia Pan. 2023. MATA*: Combining
Learnable Node Matching with A* Algorithm for Approximate Graph Edit Dis-
tance Computation. In Proceedings of the 32nd ACM International Conference on
Information and Knowledge Management. 1503â€“1512.
[17] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing
He. 2021. Pick and choose: a GNN-based imbalanced learning approach for fraud
detection. In Proceedings of the web conference 2021. 3168â€“3177.
[18] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan
Qi. 2019. Geniepath: Graph neural networks with adaptive receptive paths. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 4424â€“4431.
[19] Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li, and Le Song.
2018. Heterogeneous graph neural networks for malicious account detection. In
Proceedings of the 27th ACM international conference on information and knowledge
management. 2077â€“2085.
[20] Zhiwei Liu, Yingtong Dou, Philip S Yu, Yutong Deng, and Hao Peng. 2020. Al-
leviating the inconsistency problem of applying graph neural network to fraud
detection. In Proceedings of the 43rd international ACM SIGIR conference on re-
search and development in information retrieval. 1569â€“1572.
[21] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng
Chen, and Xiang Zhang. 2020. Parameterized explainer for graph neural network.
5337KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Kaidi Li et al.
Advances in neural information processing systems 33 (2020), 19620â€“19631.
[22] Jun Ma, Danqing Zhang, Yun Wang, Yan Zhang, and Alexey Pozdnoukhov. 2018.
GraphRAD: a graph-based risky account detection system. In Proceedings of ACM
SIGKDD conference, London, UK, Vol. 9.
[23] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z. Sheng, Hui
Xiong, and Leman Akoglu. 2021. A Comprehensive Survey on Graph Anom-
aly Detection with Deep Learning. IEEE Transactions on Knowledge and Data
Engineering (2021), 1â€“1. https://doi.org/10.1109/TKDE.2021.3118815
[24] Tahereh Pourhabibi, Kok-Leong Ong, Booi H Kam, and Yee Ling Boo. 2020. Fraud
detection: A systematic literature review of graph-based anomaly detection
approaches. Decision Support Systems 133 (2020), 113303.
[25] Susie Xi Rao, Shuai Zhang, Zhichao Han, Zitao Zhang, Wei Min, Zhiyao Chen,
Yinan Shan, Yang Zhao, and Ce Zhang. 2020. xFraud: explainable fraud transaction
detection. arXiv preprint arXiv:2011.12193 (2020).
[26] Shebuti Rayana and Leman Akoglu. 2015. Collective opinion spam detection:
Bridging review networks and metadata. In Proceedings of the 21th acm sigkdd
international conference on knowledge discovery and data mining. 985â€“994.
[27] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan
Titov, and Max Welling. 2018. Modeling relational data with graph convolu-
tional networks. In The Semantic Web: 15th International Conference, ESWC 2018,
Heraklion, Crete, Greece, June 3â€“7, 2018, Proceedings 15. Springer, 593â€“607.
[28] Xuemeng Song, Chun Wang, Changchang Sun, Shanshan Feng, Min Zhou, and
Liqiang Nie. 2023. MM-FRec: Multi-Modal Enhanced Fashion Item Recommenda-
tion. IEEE Transactions on Knowledge and Data Engineering (2023).
[29] Ghada Taher. 2021. E-commerce: advantages and limitations. International
Journal of Academic Research in Accounting Finance and Management Sciences 11,
1 (2021), 153â€“165.
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[31] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. International Conference
on Learning Representations (ICLR) (2017).
[32] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang,
Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. 2019. A semi-supervised graph
attentive network for financial fraud detection. In 2019 IEEE International Confer-
ence on Data Mining (ICDM). IEEE, 598â€“607.
[33] Jianyu Wang, Rui Wen, Chunming Wu, Yu Huang, and Jian Xiong. 2019. Fdgars:
Fraudster detection via graph convolutional networks in online app review
system. In Companion proceedings of the 2019 World Wide Web conference. 310â€“
316.
[34] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu.
2019. Heterogeneous graph attention network. In The world wide web conference.2022â€“2032.
[35] Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I Weidele, Claudio
Bellei, Tom Robinson, and Charles E Leiserson. 2019. Anti-money laundering in
bitcoin: Experimenting with graph convolutional networks for financial forensics.
arXiv preprint arXiv:1908.02591 (2019).
[36] Menglin Yang, Min Zhou, Marcus Kalander, Zengfeng Huang, and Irwin King.
2021. Discrete-time temporal network embedding via implicit hierarchical learn-
ing in hyperbolic space. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining. 1975â€“1985.
[37] Menglin Yang, Min Zhou, Jiahong Liu, Defu Lian, and Irwin King. 2022. HRCF:
Enhancing Collaborative Filtering via Hyperbolic Geometric Regularization. In
Proceedings of the Web Conference.
[38] Tianmeng Yang, Yujing Wang, Zhihan Yue, Yaming Yang, Yunhai Tong, and Jing
Bai. 2022. Graph pointer neural networks. In Proceedings of the AAAI conference
on artificial intelligence, Vol. 36. 8832â€“8839.
[39] Tianmeng Yang, Min Zhou, Yujing Wang, Zhengjie Lin, Lujia Pan, Bin Cui, and
Yunhai Tong. 2023. Mitigating Semantic Confusion from Hostile Neighborhood
for Graph Active Learning. arXiv preprint arXiv:2308.08823 (2023).
[40] Xiaocheng Yang, Mingyu Yan, Shirui Pan, Xiaochun Ye, and Dongrui Fan. 2023.
Simple and efficient heterogeneous graph neural network. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 37. 10816â€“10824.
[41] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 974â€“983.
[42] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. Gnnexplainer: Generating explanations for graph neural networks. Ad-
vances in neural information processing systems 32 (2019).
[43] Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. 2020. Xgnn: Towards model-
level explanations of graph neural networks. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining . 430â€“
438.
[44] Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and
Lizhen Cui. 2020. Gcn-based user representation learning for unifying robust
recommendation and fraudster detection. In Proceedings of the 43rd international
ACM SIGIR conference on research and development in information retrieval. 689â€“
698.
[45] Haizhong Zheng, Minhui Xue, Hao Lu, Shuang Hao, Haojin Zhu, Xiaohui Liang,
and Keith Ross. 2017. Smoke screener or straight shooter: Detecting elite sybil
attacks in user-review social networks. arXiv preprint arXiv:1709.06916 (2017).
[46] Qiwei Zhong, Yang Liu, Xiang Ao, Binbin Hu, Jinghua Feng, Jiayu Tang, and
Qing He. 2020. Financial defaulter detection on online credit payment via multi-
view attributed heterogeneous information network. In Proceedings of The Web
Conference 2020. 785â€“795.
5338