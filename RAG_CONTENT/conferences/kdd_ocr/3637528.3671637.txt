Transportation Marketplace Rate Forecast Using Signature
Transform
Haotian Gu
University of California, Berkeley
Berkeley, CA, USA
haotian_gu@berkeley.eduXin Guo
Amazon.com Inc.
Worldwide Operations Research
Science
Santa Clara, CA, USA
University of California, Berkeley
Berkeley, CA, USA
xinguo@berkeley.eduTimothy L. Jacobs
Amazon.com Inc.
Worldwide Operations Research
Science
Bellevue, WA, USA
timojaco@amazon.com
Philip Kaminsky
Amazon.com Inc.
Worldwide Operations Research
Science
Santa Clara, CA, USA
University of California, Berkeley
Berkeley, CA, USA
philipka@amazon.comXinyu Liâˆ—
University of California, Berkeley
Berkeley, CA, USA
xinyu_li@berkeley.edu
Abstract
Freight transportation marketplace rates are typically challenging
to forecast accurately. In this work, we have developed a novel
statistical technique based on signature transforms and have built
a predictive and adaptive model to forecast these marketplace rates.
Our technique is based on two key elements of the signature trans-
form: one being its universal nonlinearity property, which linearizes
the feature space and hence translates the forecasting problem into
linear regression, and the other being the signature kernel, which
allows for comparing computationally efficiently similarities be-
tween time series data. Combined, it allows for efficient feature
generation and precise identification of seasonality and regime
switching in the forecasting process.
An algorithm based on our technique has been deployed by
Amazon trucking operations, with far superior forecast accuracy
and better interpretability versus commercially available industry
models, even during the COVID-19 pandemic and the Ukraine con-
flict. Furthermore, our technique is able to capture the influence of
business cycles and the heterogeneity of the marketplace, improv-
ing prediction accuracy by more than fivefold, with an estimated
annualized saving of $50 million.
CCS Concepts
â€¢Applied computing â†’Forecasting; â€¢Computing method-
ologiesâ†’Regularization .
âˆ—Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671637Keywords
Time series forecast; signature transform; two-step LASSO; adaptive
regression
ACM Reference Format:
Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li.
2024. Transportation Marketplace Rate Forecast Using Signature Transform.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671637
1 Introduction
Overview. Linehaul freight transportation costs make up a sig-
nificant portion of overall Amazon transportation costs. To manage
these costs, Amazon has developed a variety of tools to manage
linehaul capacity mix and procurement. One key input to all of
these models is a forecast of transportation freight marketplace
rates, which however are notoriously difficult to forecast â€“ they
are driven by a number of factors: the ever-changing network of
tens of thousands of drivers, shippers of all sizes with a mix of
occasional, seasonal, and regular demands, a huge set of brokers,
traditional and digital exchanges, and local, regional, national, and
international economic factors of all kinds. In addition, the trans-
portation marketplace frequently goes through fundamental shifts
â€“ either because of wars, pandemics, fuel prices, or due to shifting
international trade patterns.
Although Amazon has purchased externally created forecasts for
some time, these forecasts are neither explainable nor sufficiently
accurate to meet specific Amazon needs. To address this challenge,
we have built a forecasting model based on time series data to pre-
dict weekly freight marketplace rates for the North America market,
at both the national and the regional levels. Our approach incorpo-
rates an innovative signature-based statistical technique capable
4997
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
of efficiently capturing significant fluctuations in transportation
marketplace rates.
The key challenges in time series forecasting. Time series data
consists of sequential observations recorded over time and is ubiq-
uitous: finance, economics, transportation, weather, and energy
prices. Given time series data, forecasting additional data points is
critical for informed decision-making and process optimization in
almost every organization and industry.
Time series prediction models such as Autoregressive Integrated
Moving Average (ARIMA) [ 26] and Exponential Smoothing [ 13]
assume that the time series are stationary, which is not the case
for freight marketplace rates. Moreover, ARIMA has limited ability
to capture seasonality and long-term trends [ 20], and Exponential
Smoothing may be insufficient for abrupt changes or outliers and
produce unstable forecasts [ 29]. Furthermore, these methods rely
solely on historical data from the time series, which is inadequate
in capturing the causal relation between the economic factors and
the marketplace rates. Meanwhile, machine learning algorithms
such as Long Short-Term Memory Neural Networks [ 33] and Gated
Recurrent Units [ 10], though capable of capturing nonlinear re-
lationship and complex patterns in time series data, will require
substantially more training data which is not available in our case.
Indeed, one of the main challenges in analyzing time series data
is their ever-changing statistical properties, due to factors includ-
ing changes in business and economic cycles, shifts in policy, or
changes in market conditions. In our case, the market itself has re-
cently experienced shifts in regimes andseasonality [16], in terms of
volatility, trends, and cyclical patterns, partly due to the COVID-19
pandemic and the Ukraine conflict.
Machine learning models and signature transform. Much of sta-
tistical learning theory relies on finding a feature map that em-
beds the data (for instance, samples of time series) into a high-
dimensional feature space. Two requirements for an ideal feature
map are universality, meaning that non-linear functions of the data
are approximated by linear functionals in the feature space; and
characteristicness, meaning that the expected value of the feature
map characterizes the law of the random variable. It is shown that
with the technique of the signature transform these two proper-
ties are in duality and therefore often equivalent [ 27]. This is the
primary inspiration for our proposed signature-based forecasting
technique for our forecast models.
Originally introduced and studied in algebraic topology [ 5,6],
thesignature transform, sometimes referred to as the path signature
or simply signature, has been further developed in rough path
theory [ 12,24], introduced for financial applications [ 1,17,22,23]
and machine learning [ 4,18,21,31,32], and most recently to time
series data analysis [ 9,11,25]. Given any continuous or discrete
time series, their signature transform produces a vector of real-
valued features that extract information such as order and area,
and explicitly considers combinations of different channels. The
signature of time series uniquely determines the time series, and
does so in a computationally efficient way. Most importantly, every
continuous function of a time series data may be asymptotically
approximated by a linear functional of its signature. In other words,
signature transform linearizes the otherwise complicated featurespace, and thus is a powerful tool for feature generation and pattern
identification in machine learning.
Our work. We propose a novel signature-based statistical tech-
nique for the time series forecasting problem. This is based on two
key elements of the signature transform. The first is the universal
nonlinearity property of the signature transform, which linearizes
the feature space of the time series data and hence translates the
forecasting problem into a linear regression. The second is the signa-
ture kernel which allows for computationally efficient comparison
of similarities between time series data. Technically, this is to iden-
tify different â€œsignature feature mapsâ€, the statistical counterpart
of identifying different distributions for a given time series data,
albeit in the linearized feature space from the signature transform.
Our approach starts by collecting data including a hundred of
market supply and demand factors, and runs a correlation test be-
tween the marketplace rate and the factors to remove non-significant
factors and to identify factors that may be colinear. We then exploit
the universal nonlinearity property of signature transform to con-
struct signature features as suitable candidates for the â€œinternalâ€
features. To avoid issues of overfitting and co-linearity between
the signature feature and the external factors, and to improve the
forecast accuracy, we adopt the two-step LASSO for the regres-
sion analysis [ 2]. Finally, this two-step LASSO is enhanced with
adaptive weight using a signature kernel, which enables captur-
ing changes in regimes or seasonality. Combined, this leads to our
signature-based adaptive two-step LASSO approach. This novel
signature-transform-based technique for data analysis allows for
efficient feature generation and more precise identification of sea-
sonality and regime switching embedded in the data.
Implementation and real-time performance. This signature-based
adaptive two-step LASSO algorithm has been implemented for
the trucking operations in Amazon since November 2022. Perfor-
mance analysis shows that our forecast model presents superior
performance than commercially available forecast models, while
providing significantly better interpretability. Despite the onset of
COVID-19 and the Ukraine conflict, it captures the influence of
business cycles and heterogeneity of the marketplace, improves
prediction accuracy by more than fivefold, and has an estimated
annualized saving of approximately $50million.
2 Technical Background
2.1 Signatures of Continuous Paths
We begin with the definition of signatures of continuous piecewise
smooth paths.
Notation. LetRğ‘‘1âŠ—Rğ‘‘2âŠ—Â·Â·Â·âŠ— Rğ‘‘ğ‘›denote the space of all real
tensors with shape ğ‘‘1Ã—ğ‘‘2Ã—Â·Â·Â·Ã—ğ‘‘ğ‘›. Define a binary operation
called tensor product, denoted by âŠ—, which maps a tensor of shape
(ğ‘‘1,...,ğ‘‘ ğ‘›)and a tensor of shape (ğ‘’1,...,ğ‘’ ğ‘š)to a tensor of shape
(ğ‘‘1,...,ğ‘‘ ğ‘›,ğ‘’1,...,ğ‘’ ğ‘š)via ğ´ğ‘–1,...,ğ‘–ğ‘›,ğµğ‘—1,...,ğ‘— ğ‘šâ†¦â†’ğ´ğ‘–1,...,ğ‘–ğ‘›ğµğ‘—1,...,ğ‘— ğ‘š.
When applied to two vectors, it reduces to the outer product. Let
Rğ‘‘âŠ—ğ‘˜
=Rğ‘‘âŠ—Â·Â·Â·âŠ— Rğ‘‘, andğ‘£âŠ—ğ‘˜=ğ‘£âŠ—Â·Â·Â·âŠ—ğ‘£forğ‘£âˆˆRğ‘‘, in each
case withğ‘˜âˆ’1manyâŠ—.
4998Transportation Marketplace Rate Forecast Using Signature Transform KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Definition 1. Letğ‘<ğ‘âˆˆR, andğ‘‹=
ğ‘‹1,...,ğ‘‹ğ‘‘
:[ğ‘,ğ‘]â†’
Rğ‘‘be a continuous piecewise smooth path. The signature of ğ‘‹is then
defined as the collection of iterated integrals
Sig(ğ‘‹)=âˆ«
ğ‘<ğ‘¡1<Â·Â·Â·<ğ‘¡ğ‘˜<ğ‘dğ‘‹ğ‘¡1âŠ—Â·Â·Â·âŠ— dğ‘‹ğ‘¡ğ‘˜
ğ‘˜â‰¥0
= âˆ«
ğ‘<ğ‘¡1<Â·Â·Â·<ğ‘¡ğ‘˜<ğ‘dğ‘‹ğ‘–1
ğ‘¡1Â·Â·Â·dğ‘‹ğ‘–ğ‘˜
ğ‘¡ğ‘˜
1â‰¤ğ‘–1,...,ğ‘–ğ‘˜â‰¤ğ‘‘!
ğ‘˜â‰¥0,
whereâŠ—denotes the tensor product, dğ‘‹ğ‘¡=dğ‘‹ğ‘¡
dğ‘¡dğ‘¡, and theğ‘˜=0
term is taken to be 1âˆˆR. The truncated signature of depth ğ‘ofğ‘‹is
defined as
Sigğ‘(ğ‘‹)=âˆ«
ğ‘<ğ‘¡1<Â·Â·Â·<ğ‘¡ğ‘˜<ğ‘dğ‘‹ğ‘¡1âŠ—Â·Â·Â·âŠ— dğ‘‹ğ‘¡ğ‘˜
0â‰¤ğ‘˜â‰¤ğ‘.(1)
Remark 1. The signature can be defined more generally on paths
of bounded variation [ 11], but the above definition suffices for our
purposes.
Example 1. Supposeğ‘‹:[ğ‘,ğ‘]â†’Rğ‘‘is the linear interpolation of
two pointsğ‘¥,ğ‘¦âˆˆRğ‘‘, so thatğ‘‹ğ‘¡=ğ‘¥+ğ‘¡âˆ’ğ‘
ğ‘âˆ’ğ‘(ğ‘¦âˆ’ğ‘¥). Then its signature
is the collection of tensor products of its total increment:
Sig(ğ‘‹)=
1,ğ‘¦âˆ’ğ‘¥,1
2(ğ‘¦âˆ’ğ‘¥)âŠ—2,1
6(ğ‘¦âˆ’ğ‘¥)âŠ—3,...,1
ğ‘˜!(ğ‘¦âˆ’ğ‘¥)âŠ—ğ‘˜,...
,
which is independent of ğ‘,ğ‘.
Example 2. Supposeğ‘‹:[ğ‘,ğ‘]â†’Ris a one-dimensional smooth
path. Then its signature is the collection of powers of its total incre-
ment:
Sig(ğ‘‹)=
1,ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘),1
2(ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘))2,1
6(ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘))3,
...,1
ğ‘˜!(ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘))ğ‘˜,...
,
which is independent of ğ‘‹(ğ‘¡),ğ‘¡âˆˆ(ğ‘,ğ‘). Furthermore, when ğ‘‹(ğ‘¡)is a
random process, the expected signature
E[Sig(ğ‘‹)]=
1,E[ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘)],1
2E
(ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘))2
,
...,1
ğ‘˜!Eh
(ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘))ğ‘˜i
,...
,
whenever it exists, describes precisely the moments of ğ‘‹(ğ‘)âˆ’ğ‘‹(ğ‘).
Thus, for a high-dimensional stochastic process ğ‘‹(ğ‘¡), the expected
signature naturally forms the generalization of the moments of the
process. In other words, for a stochastic process ğ‘‹(ğ‘¡), its expected
signature characterizes the law of ğ‘‹(ğ‘¡)up to tree-like equivalence, as
proved in [8].
Example 2 shows that the signature for a one-dimensional path
only depends on its total increment. In general, it implies that
the signature of a path itself may not carry sufficient information
to fully characterize the path. Nevertheless, this problem may be
resolved by considering the time-augmented version of the original
path (see Definition 4, Theorem 1 and 2 below).2.2 Signature of Discrete Data
To define and compute signatures of discrete data streams, one can
simply do linear interpolations and then apply signature transforms.
Definition 2. The space of streams of data is defined as
S
Rğ‘‘
=n
ğ‘¥ğ‘¥ğ‘¥=(ğ‘¥ğ‘¥ğ‘¥1,...,ğ‘¥ğ‘¥ğ‘¥ğ‘›):ğ‘¥ğ‘¥ğ‘¥ğ‘–âˆˆRğ‘‘,ğ‘›âˆˆNo
.
Givenğ‘¥ğ‘¥ğ‘¥=(ğ‘¥ğ‘¥ğ‘¥1,...,ğ‘¥ğ‘¥ğ‘¥ğ‘›)âˆˆS
Rğ‘‘
, the integer ğ‘›is called the length
ofğ‘¥ğ‘¥ğ‘¥.Furthermore for ğ‘,ğ‘âˆˆRsuch thatğ‘<ğ‘, fix
ğ‘=ğ‘¢1<ğ‘¢2<Â·Â·Â·<ğ‘¢ğ‘›âˆ’1<ğ‘¢ğ‘›=ğ‘. (2)
Letğ‘‹=
ğ‘‹1,...,ğ‘‹ğ‘‘
:[ğ‘,ğ‘]â†’Rğ‘‘be continuous such that ğ‘‹ğ‘¢ğ‘–=
ğ‘¥ğ‘¥ğ‘¥ğ‘–for allğ‘–, and linear on the intervals in between. Then ğ‘‹is called a
linear interpolation of ğ‘¥ğ‘¥ğ‘¥.
Definition 3. Letğ‘¥ğ‘¥ğ‘¥=(ğ‘¥ğ‘¥ğ‘¥1,...,ğ‘¥ğ‘¥ğ‘¥ğ‘›)âˆˆS
Rğ‘‘
be a stream of data.
Letğ‘‹be a linear interpolation of ğ‘¥ğ‘¥ğ‘¥. Then the signature of ğ‘¥ğ‘¥ğ‘¥is defined
asSig(ğ‘¥ğ‘¥ğ‘¥)=Sig(ğ‘‹),and the truncated signature of depth ğ‘ofğ‘¥ğ‘¥ğ‘¥is
defined as Sigğ‘(ğ‘¥ğ‘¥ğ‘¥)=Sigğ‘(ğ‘‹).
Definition 4. Given a path ğ‘‹:[ğ‘,ğ‘]â†’Rğ‘‘, define the corre-
sponding time-augmented path by bğ‘‹ğ‘¡=(ğ‘¡,ğ‘‹ğ‘¡), a path in Rğ‘‘+1.
2.3 Key Properties of Signature
Theorem 1 (Uniqeness [ 15]).Letğ‘‹:[ğ‘,ğ‘]â†’Rğ‘‘be a contin-
uous piecewise smooth path. Then Sig(bğ‘‹)uniquely determines ğ‘‹up
to translation.
In fact, the signature not only determines a path uniquely up
to translation, but also linearizes any continuous functions of the
path, as stated in the next theorem.
Theorem 2 (Universal nonlinearity [ 1]).Letğ¹be a real-
valued continuous function on continuous piecewise smooth paths in
Rğ‘‘and letKbe a compact set of such paths. Furthermore assume
thatğ‘‹0=0for allğ‘‹âˆˆK. (To remove the translation invariance.) Let
ğœ€>0. Then there exists a linear functional ğ¿such that for all ğ‘‹âˆˆK,
|ğ¹(ğ‘‹)âˆ’ğ¿(Sig(bğ‘‹))|<ğœ€.
This universal nonlinearity is the key property of the signature
transform and is important for our model, and in general for appli-
cations in feature augmentations. See [ 23], [21], [25] for examples.
Note that the signature by definition is an infinite-dimensional
tensor. In practice, one can only compute the truncated signature
Sigğ‘in(1)up to some depth ğ‘. The next result guarantees that
reminder terms in the truncation decay factorially.
Theorem 3 (Factorial decay [ 24]).Letğ‘‹:[ğ‘,ğ‘]â†’Rğ‘‘be a
continuous piecewise smooth path and let âˆ¥Â·âˆ¥ be a tensor norm on
Rğ‘‘âŠ—ğ‘˜
. Then
âˆ«
ğ‘<ğ‘¡1<Â·Â·Â·<ğ‘¡ğ‘˜<ğ‘dğ‘‹ğ‘¡1âŠ—Â·Â·Â·âŠ— dğ‘‹ğ‘¡ğ‘˜â‰¤ğ¶(ğ‘‹)ğ‘˜
ğ‘˜!,
whereğ¶(ğ‘‹)is a constant depending on ğ‘‹.
The next property about signatures, Theorem 4, is the invariance
to time reparameterizations. It implies that the signature encodes the
data by its arrival order and independently of its arrival time. This
4999KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
is a desired property in many applications such as hand-writing
recognition [ 32], [31]. Meanwhile, there is an interesting interplay
between Theorem 2 and Theorem 4: in a problem where time param-
eterizations are irrelevant, it suffices to compute the signature of ğ‘‹
by Theorem 4; However, if time parameterization is important, then
according to Theorem 2, applying the signature transform to the
time-augmented path bğ‘‹ensures that parameterization-dependent
features are still learned.
Theorem 4 (Invariance to time reparameterizations [ 24]).
Letğ‘‹:[0,1] â†’Rğ‘‘be a continuous piecewise smooth path. Let
ğœ“:[0,1]â†’[ 0,1]be continuously differentiable, increasing, and
surjective. Then Sig(ğ‘‹)=Sig(ğ‘‹â—¦ğœ“).
Note that by Theorem 4, the signature of a stream of data is
independent of the choice of ğ‘¢ğ‘–in a linear interpolation in (2).
Meanwhile, by Theorem 2, in order to learn parameterization-
dependent features, one can apply the signature transform to the
time-augmented data stream bğ‘¥ğ‘¥ğ‘¥= bğ‘¥ğ‘¥ğ‘¥1,...,bğ‘¥ğ‘¥ğ‘¥ğ‘›, where bğ‘¥ğ‘¥ğ‘¥ğ‘–=(ğ‘¡ğ‘–,ğ‘¥ğ‘¥ğ‘¥ğ‘–)âˆˆ
Rğ‘‘+1, andğ‘¡ğ‘–is the time when the data point ğ‘¥ğ‘¥ğ‘¥ğ‘–arrives.
Letğ‘¥ğ‘¥ğ‘¥=(ğ‘¥ğ‘¥ğ‘¥1,...,ğ‘¥ğ‘¥ğ‘¥ğ‘›)âˆˆS
Rğ‘‘
be a data stream of length ğ‘›in
Rğ‘‘.Then Sigğ‘(ğ‘¥ğ‘¥ğ‘¥)has
ğ‘€(ğ‘‘,ğ‘):=ğ‘âˆ‘ï¸
ğ‘˜=0ğ‘‘ğ‘˜=ğ‘‘ğ‘+1âˆ’1
ğ‘‘âˆ’1(3)
components. In particular, the number of components does not
depend on the length of the data stream ğ‘›. The truncated signature
maps the infinite-dimensional space of streams of data S
Rğ‘‘
into
a finite-dimensional space of dimension
ğ‘‘ğ‘+1âˆ’1
/(ğ‘‘âˆ’1). Thus
the signature is an efficient way to tackle long streams of data, or
streams of variable length.
2.4 Computation of Signature Transform
The signature transform of a data stream can be computed in an
efficient and tractable way, with the help of Chenâ€™s identity [ 4,24].
It starts by introducing the following âŠ operation: with ğ´0=ğµ0=1,
define âŠ by
âŠ : ğ‘Ã–
ğ‘˜=1
Rğ‘‘âŠ—ğ‘˜!
Ã— ğ‘Ã–
ğ‘˜=1
Rğ‘‘âŠ—ğ‘˜!
â†’ğ‘Ã–
ğ‘˜=1
Rğ‘‘âŠ—ğ‘˜
,
(ğ´1,...ğ´ ğ‘)âŠ (ğµ1,...,ğµ ğ‘)â†¦â†’Â©Â­
Â«ğ‘˜âˆ‘ï¸
ğ‘—=0ğ´ğ‘—âŠ—ğµğ‘˜âˆ’ğ‘—ÂªÂ®
Â¬1â‰¤ğ‘˜â‰¤ğ‘.(4)
Chenâ€™s identity [ 12] states that the image of the signature transform
forms a group structure with respect to âŠ . That is, given a sequence
of data(ğ‘¥1,...,ğ‘¥ ğ¿)âˆˆS
Rğ‘‘
and someğ‘—âˆˆ{2,...,ğ¿âˆ’1},
Sigğ‘((ğ‘¥1,...,ğ‘¥ ğ¿))=Sigğ‘  ğ‘¥1,...,ğ‘¥ ğ‘—âŠ Sigğ‘  ğ‘¥ğ‘—,...,ğ‘¥ ğ¿.
Furthermore, from Example 1, the signature of a sequence of length
two can be computed explicitly from the definition. Letting
exp :Rğ‘‘â†’ğ‘Ã–
ğ‘˜=1
Rğ‘‘âŠ—ğ‘˜
,exp :ğ‘£â†’
ğ‘£,ğ‘£âŠ—2
2!,ğ‘£âŠ—3
3!,...,ğ‘£âŠ—ğ‘
ğ‘!
,
(5)then
Sigğ‘((ğ‘¥1,ğ‘¥2))=exp(ğ‘¥2âˆ’ğ‘¥1).
Chenâ€™s identity further implies that the signature transform can be
computed by
Sigğ‘((ğ‘¥1,...,ğ‘¥ ğ¿))=exp(ğ‘¥2âˆ’ğ‘¥1)âŠ exp(ğ‘¥3âˆ’ğ‘¥2)
âŠ Â·Â·Â·âŠ exp(ğ‘¥ğ¿âˆ’ğ‘¥ğ¿âˆ’1).(6)
(6)implies that computing the signature of an incoming stream of
data is efficient and scalable. Indeed, suppose one has obtained a
stream of data and computed its signature. Then after the arrival
of some more data, in order to compute the signature of the entire
signal, one only needs to compute the signature of the new piece of
information, which is then computed via the tensor product with
the previously-computed signature.
Improving computational efficiency. Recall from (6)that the signa-
ture may be computed by evaluating several âŠ in(4)andexpin(5).
We begin by noticing that the key component in the computation
is to evaluate ğ‘Ã–
ğ‘˜=1
Rğ‘‘âŠ—ğ‘˜!
Ã—Rğ‘‘â†’ğ‘Ã–
ğ‘˜=1
Rğ‘‘âŠ—ğ‘˜
, ğ´,ğ‘§â†¦â†’ğ´âŠ exp(ğ‘§).
Instead of computing ğ´âŠ exp(ğ‘§)conventionally through the com-
position of expandâŠ , [18] suggests to speed up the computation
by Hornerâ€™s method. More specifically, it is to expand
ğ´âŠ—exp(ğ‘§)= ğ‘˜âˆ‘ï¸
ğ‘–=0ğ´ğ‘–âŠ—ğ‘§âŠ—(ğ‘˜âˆ’ğ‘–)
(ğ‘˜âˆ’ğ‘–)!!
1â‰¤ğ‘˜â‰¤ğ‘,
so that theğ‘˜-th term can be computed by
ğ‘˜âˆ‘ï¸
ğ‘–=0ğ´ğ‘–âŠ—ğ‘§âŠ—(ğ‘˜âˆ’ğ‘–)
(ğ‘˜âˆ’ğ‘–)!=  
Â·Â·Â·ğ‘§
ğ‘˜+ğ´1
âŠ—ğ‘§
ğ‘˜âˆ’1+ğ´2
âŠ—ğ‘§
ğ‘˜âˆ’2+Â·Â·Â·!
âŠ—ğ‘§
2+ğ´ğ‘˜âˆ’1!
âŠ—ğ‘§+ğ´ğ‘˜.
As proved in [ 18], this method has uniformly (over ğ‘‘,ğ‘) fewer
scalar multiplications than the conventional approach, and reduces
the asymptotic complexity of this operation from O
ğ‘ğ‘‘ğ‘
to
O
ğ‘‘ğ‘
. Furthermore, this rate is asymptotically optimal, since
the size of the result (an element ofÃğ‘
ğ‘˜=1
Rğ‘‘âŠ—ğ‘˜
), is itself of size
O
ğ‘‘ğ‘
.
3 Forecasting Problem and Our Approach
3.1 Forecasting Problem
The freight marketplace rate forecast problem involves two time se-
ries{ğ‘¥ğ‘¥ğ‘¥ğœ}ğœâˆˆN+and{ğ‘¦ğœ}ğœâˆˆN+. Here,ğ‘¥ğ‘¥ğ‘¥ğœâˆˆXâŠ† Rğ‘‘0is ağ‘‘0-dimensional
vector representing values of the key economic factors that drive
the supply and demand in the freight marketplace at time ğœ. Factors
from the market supply side include information regarding the sup-
ply of drivers and trucks and fuel/oil prices. Market demand factors
include imports, agriculture information, manufacturing activities,
housing indexes, and railway transport. Additionally, ğ‘¦ğœâˆˆYâŠ† R
is the freight marketplace rate at time ğœ. Previously, Amazon relied
5000Transportation Marketplace Rate Forecast Using Signature Transform KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
on a commercial service to obtain forecasts for future marketplace
rates. However, those forecasts lacked accuracy and transparency.
To address these, we consider the following forecast problem.
The general goal of our forecast problem is to find models ğ‘“âˆ—
Î”ğ‘¡âˆˆ
F âŠ† {ğ‘“|ğ‘“:X â†’ Y} such thatğ‘“âˆ—
Î”ğ‘¡(ğ‘¥ğ‘¥ğ‘¥ğœ) â‰ˆğ‘¦ğœ+Î”ğ‘¡, for Î”ğ‘¡=
1,2,3,Â·Â·Â·,Î”ğ‘‡,where Î”ğ‘‡âˆˆN+denotes the longest forecast horizon
andFis the class of all admissible models. More precisely, given
the data up to time ğ‘¡:{(ğ‘¥ğ‘¥ğ‘¥ğœ,ğ‘¦ğœ)}ğœ=1,2,Â·Â·Â·,ğ‘¡, to make prediction for
ğ‘¦ğ‘¡+Î”ğ‘¡, one standard approach to find ğ‘“âˆ—
Î”ğ‘¡âˆˆF is by solving the
following optimization problem:
ğ‘“âˆ—
Î”ğ‘¡âˆˆarg min
ğ‘“âˆˆF(
1
ğ‘¡âˆ’Î”ğ‘¡ğ‘¡âˆ’Î”ğ‘¡âˆ‘ï¸
ğœ=1ğ¿(ğ‘“(ğ‘¥ğ‘¥ğ‘¥ğœ),ğ‘¦ğœ+Î”ğ‘¡))
, (7)
whereğ¿:YÃ—Yâ†’ Ris a loss function measuring the difference
between the model prediction ğ‘“(ğ‘¥ğ‘¥ğ‘¥ğœ)and the actual ğ‘¦ğœ+Î”ğ‘¡. Onceğ‘“âˆ—
Î”ğ‘¡
is obtained, the prediction of ğ‘¦ğ‘¡+Î”ğ‘¡is given by bğ‘¦ğ‘¡+Î”ğ‘¡:=ğ‘“âˆ—
Î”ğ‘¡(ğ‘¥ğ‘¥ğ‘¥ğ‘¡).
3.2 Our Approach
We will present a signature-based adaptive two-step LASSO ap-
proach that we have developed and implemented in Amazon, which
has demonstrated excellent performance in solving this problem.
Data. Our approach and experiment start by collecting data
involving over a hundred of national and regional market supply
and demand factors, downloaded from the governmental public
websites, including Federal Reserve Bank and Bureau of Labor
Statistics, as well as industrial databases such as Logistic Manager.
The time range for the data is from 2018 to 2022.
External factor preprocessing. We first run a correlation test be-
tween the marketplace rate and the factors to remove non-significant
factors, with further correlation analysis to identify factors that
may be colinear. After this round of elimination, over forty factors
remain, including consumer price index, housing index, oil and
gas drilling, logistic managersâ€™ index, employment information,
weather, and other market benchmarks.
Internal features via signature transform. Besides the â€œexternalâ€
factorsğ‘¥ğ‘¥ğ‘¥ğœ, most time series forecasting approaches, such as ARIMA,
also construct â€œinternalâ€ features from the history of ğ‘¦ğœ. Those â€œin-
ternalâ€ features may help to characterize the trend, momentum,
and stationarity of ğ‘¦ğœ. We, instead, exploit the universal nonlin-
earity property of signature transform (Theorem 2), and construct
signature features as suitable candidates for the â€œinternalâ€ features.
More specifically, for any time step ğœâˆˆN+and time window size
ğ‘™âˆˆN, denoteğ‘¦ğœâˆ’ğ‘™:ğœ:=(ğ‘¦ğœâˆ’ğ‘™,Â·Â·Â·,ğ‘¦ğœ)as the slice of the time series
{ğ‘¦ğ‘¡}ğ‘¡âˆˆN+from timeğœâˆ’ğ‘™toğœ. The feature vector for predicting
ğ‘¦ğœ+Î”ğ‘¡consists of both the economic factors ğ‘¥ğ‘¥ğ‘¥ğœand the depth- ğ‘
signature features Sigğ‘(ğ‘¦ğœâˆ’ğ‘™:ğœ). We denote the concatenation of
those two sets of features as
ğ’™ğœ,Sigğ‘(ğ‘¦ğœâˆ’ğ‘™:ğœ)
, whose dimension
is denoted by ğ‘‘.
Two-step LASSO. The universal nonlinearity property of the sig-
nature transform linearizes the feature space, hence translating
the forecasting problem into a linear regression analysis. Since the
dimensionğ‘‘of the feature vector may be relatively large compared
to the number of historical samples, especially when the time step ğ‘¡
Figure 1: The Data Flow of the Adaptive Two-step LASSO via
Signature Kernel (Algorithm 2)
is small, we adopt the approach of two-step LASSO to avoid overfit-
ting and the issue of co-linearity especially between the signature
feature and the external factors. The first step is to select the factors
by solving the standard LASSO regression [ 30], [34], [35]. This is to
add anğ¿1-regularization to model coefficients in the ordinary least
square objective. This ğ¿1-regularization will encourage the sparsity
of model coefficients, and prevent the over-fitting problem. In the
second step, an OLS with only the selected factors is applied. This
two-step LASSO estimation procedure has been shown to produce
a smaller bias than standard LASSO for a range of models [2], [7].
More precisely, recall that the LASSO regression is to solve the
following optimization problem:
bğœ½ğœ†
LASSO ,Î”ğ‘¡âˆˆarg min
ğœ½âˆˆRğ‘‘(
1
ğ‘¡âˆ’Î”ğ‘¡ğ‘¡âˆ’Î”ğ‘¡âˆ‘ï¸
ğœ=1
ğ‘¦ğœ+Î”ğ‘¡
âˆ’h
ğ’™ğœ,Sigğ‘(ğ‘¦ğœâˆ’ğ‘™:ğœ)i
Â·ğœ½2
+ğœ†âˆ¥ğœ½âˆ¥1)
.(8)
Here the constant ğœ†, called the regularization parameter, controls
the sparsity of coefficients: a higher value of ğœ†leads to a smaller
number of nonzero coefficients in bğœ½ğœ†
LASSO ,Î”ğ‘¡. In the two-step LASSO,
the first step is to select the factors by solving the LASSO regression
in(8), and get bğœ½ğœ†
LASSO ,Î”ğ‘¡. In the second step, the subsequent OLS
refitting is to find ğœ½ğœ†
LASSO ,Î”ğ‘¡such that
ğœ½ğœ†
LASSO ,Î”ğ‘¡âˆˆ arg min
supp[ğœ½]=supph
bğœ½ğœ†
LASSO ,Î”ğ‘¡i(9)
(
1
ğ‘¡âˆ’Î”ğ‘¡ğ‘¡âˆ’Î”ğ‘¡âˆ‘ï¸
ğœ=1
ğ‘¦ğœ+Î”ğ‘¡âˆ’h
ğ’™ğœ,Sigğ‘(ğ‘¦ğœâˆ’ğ‘™:ğœ)i
Â·ğœ½2)
.
5001KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
Adaptive weight via signature kernel. In the classical approach of
two-step LASSO, each historical sample is given equal weight in
the optimization problem to obtain the model at time ğ‘¡. However,
this equal-weight scheme may fail to account for changes of regime
or seasonality. Instead, a more effective approach would be to dy-
namically assign weights based on their similarity to the current
period. This is precisely what we propose, as elaborated below.
First, recall the signature feature map (Theorem 2),
Î¦:ğ‘‹â†¦â†’Sig(bğ‘‹) (10)
is a universal feature map from the path space to the linear space of
signatures [ 9]. To avoid computation over a large space of functions,
we kernelize the signature feature map Î¦in(10), and define the
signature kernel ğ‘˜(ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘):=âŸ¨Î¦(ğ‘ğ‘ğ‘),Î¦(ğ‘ğ‘ğ‘)âŸ©, for any discrete time series
ğ‘ğ‘ğ‘andğ‘ğ‘ğ‘,as suggested in [ 9]. HereâŸ¨Â·,Â·âŸ©is the inner product on the
linear space of signatures.
Next, to measure the similarity between two discrete time series
ğ‘ğ‘ğ‘andğ‘ğ‘ğ‘, consider the distance induced by the signature kernel
[3, 9, 14, 19, 28],
ğ‘‘Sig(ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘)=ğ‘˜(ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘)âˆ’2ğ‘˜(ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘)+ğ‘˜(ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘). (11)
Smallğ‘‘Sig(ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘)implies a higher similarity between patterns in ğ‘ğ‘ğ‘
andğ‘ğ‘ğ‘, which suggests that ğ‘ğ‘ğ‘andğ‘ğ‘ğ‘come from the same regime
and share the similar seasonality. In practice, one may truncate
the signature to depth ğ‘when computing (11), and we denote the
distance computed from the depth- ğ‘truncated signature by ğ‘‘Sig,ğ‘.
Finally, we adapt the weights in the two-step LASSO according
to the signature kernel, this is called AdaWeight.Sig. It takes five
hyper-parameters: a forecast horizon Î”ğ‘¡, a window size ğ‘™âˆˆN,
a signature depth ğ‘âˆˆN+, a temperature parameter ğ›¾â‰¥0, and
the distance metric ğ‘‘Sig,ğ‘. More precisely, define ğ‘§ğ‘§ğ‘§ğœ:=(ğ‘¥ğ‘¥ğ‘¥ğœ,ğ‘¦ğœ+Î”ğ‘¡)
for anyğœâˆˆN+; for anyğœ1,ğœ2âˆˆN+andğœ1<ğœ2, denoteğ‘§ğ‘§ğ‘§ğœ1:ğœ2:=
(ğ‘§ğ‘§ğ‘§ğœ1,Â·Â·Â·,ğ‘§ğ‘§ğ‘§ğœ2)as a slice of the time series {ğ‘§ğ‘§ğ‘§ğ‘¡}ğ‘¡âˆˆN+from timeğœ1
toğœ2. At each time ğ‘¡,AdaWeight.Sigtakes the historical samples
{(ğ‘¥ğ‘¥ğ‘¥ğœ,ğ‘¦ğœ+Î”ğ‘¡)}1â‰¤ğœâ‰¤ğ‘¡âˆ’Î”ğ‘¡as input, and outputs an adaptive weight
vector
ğ‘Šğ‘Šğ‘Š(Î”ğ‘¡):=(ğ‘¤(Î”ğ‘¡)
1,ğ‘¤(Î”ğ‘¡)
2,Â·Â·Â·,ğ‘¤(Î”ğ‘¡)
ğ‘¡âˆ’Î”ğ‘¡)âˆˆRğ‘¡âˆ’Î”ğ‘¡
â‰¥0, (12)
withÃğ‘¡âˆ’Î”ğ‘¡
ğœ=1ğ‘¤(Î”ğ‘¡)
ğœ=1. That is, AdaWeight.Sigwill assign a higher
weightğ‘¤(Î”ğ‘¡)
ğœ to the sample(ğ‘¥ğ‘¥ğ‘¥ğœ,ğ‘¦ğœ+Î”ğ‘¡)if the seasonality pattern
near timeğœis more similar to the current seasonality pattern embed-
ded in the sample(ğ‘¥ğ‘¥ğ‘¥ğ‘¡âˆ’Î”ğ‘¡,ğ‘¦ğ‘¡). Thus, when a new data sample arrives,
the weight vector ğ‘Šğ‘Šğ‘Š(Î”ğ‘¡)will be recomputed by the AdaWeight.Sig
module, to adapt to changes in the recent data samples.
Adaptive two-step LASSO via signature kernel. Measuring simi-
larity via signature kernel leads to a novel LASSO-based approach,
in which we propose to adapt weights according to the similari-
ties between time series data to capture seasonality and regime
switching embedded in the data. In the case of forecasting models
with signature transforms, comparing similarities of data trans-
lates to identifying â€œsignature feature mapsâ€. This is the statistical
equivalence of identifying different distributions for a given set
of data, albeit in the linearized features space from the signature
transform. Algorithm 2 summarizes this approach of adaptive two-
step LASSO via signature kernel, by integrating AdaWeight.Sig
outlined in Algorithm 1 with the LASSO approach.Algorithm 1 Adaptive Weight via Signature Kernel
(AdaWeight .Sig)
1:Input: forecast horizon Î”ğ‘¡, window size ğ‘™, signature depth ğ‘,
temperature parameter ğ›¾, kernel-based distance metric ğ‘‘Sig,ğ‘
(11), data set ğ·={ğ‘§ğ‘§ğ‘§ğœ=(ğ‘¥ğ‘¥ğ‘¥ğœ,ğ‘¦ğœ+Î”ğ‘¡)}1â‰¤ğœâ‰¤ğ‘¡âˆ’Î”ğ‘¡.
2:forğœ=1,2,Â·Â·Â·,ğ‘¡âˆ’Î”ğ‘¡do
3: Compute the distance ğ›¿ğœbetween the truncated depth- ğ‘
signatures of ğ‘§ğ‘§ğ‘§ğœâˆ’ğ‘™:ğœandğ‘§ğ‘§ğ‘§ğ‘¡âˆ’Î”ğ‘¡âˆ’ğ‘™:ğ‘¡âˆ’Î”ğ‘¡:
ğ›¿ğœ:=ğ‘‘Sig,ğ‘(ğ‘§ğ‘§ğ‘§ğœâˆ’ğ‘™:ğœ,ğ‘§ğ‘§ğ‘§ğ‘¡âˆ’Î”ğ‘¡âˆ’ğ‘™:ğ‘¡âˆ’Î”ğ‘¡). (13)
4:end for
5:forğœ=1,2,Â·Â·Â·,ğ‘¡âˆ’Î”ğ‘¡do
6: Compute the weight ğ‘¤(Î”ğ‘¡)
ğœ according to
ğ‘¤(Î”ğ‘¡)
ğœ :=exp(âˆ’ğ›¾Â·ğ›¿ğœ)
Ãğ‘¡âˆ’Î”ğ‘¡
ğœ=1exp(âˆ’ğ›¾Â·ğ›¿ğœ). (14)
7:end for
8:Output: the weight vector ğ‘Šğ‘Šğ‘Š(Î”ğ‘¡)=(ğ‘¤(Î”ğ‘¡)
1,Â·Â·Â·,ğ‘¤(Î”ğ‘¡)
ğ‘¡âˆ’Î”ğ‘¡).
Algorithm 2 Adaptive two-step LASSO with Signature Kernel
1:Input: regularization parameter ğœ†, window size ğ‘™, signature
depthğ‘, temperature parameter ğ›¾, kernel-based distance met-
ricğ‘‘Sig,ğ‘, data setğ·={ğ‘§ğ‘§ğ‘§ğœ=(ğ‘¥ğ‘¥ğ‘¥ğœ,ğ‘¦ğœ+Î”ğ‘¡)}1â‰¤ğœâ‰¤ğ‘¡âˆ’Î”ğ‘¡.
2:forÎ”ğ‘¡=1,2,Â·Â·Â·,Î”ğ‘‡do
3: Regime identification: Run AdaWeight.Sig(Algorithm 1)
withÎ”ğ‘¡,ğ‘™,ğ‘,ğ›¾,ğ‘‘Sig,ğ‘, andğ·. Outputğ‘Šğ‘Šğ‘Š(Î”ğ‘¡)in (12).
4: 2-step LASSO: Apply the 2-step LASSO method on the data
setğ·, with adaptive weight ğ‘Šğ‘Šğ‘Š(Î”ğ‘¡)and regularization param-
eterğœ†:
bğœ½ğœ†
LASSO ,Î”ğ‘¡âˆˆarg min
ğœ½âˆˆRğ‘‘(ğ‘¡âˆ’Î”ğ‘¡âˆ‘ï¸
ğœ=1ğ‘¤(Î”ğ‘¡)
ğœÂ·
ğ‘¦ğœ+Î”ğ‘¡
âˆ’h
ğ’™ğœ,Sigğ‘(ğ‘¦ğœâˆ’ğ‘™:ğœ)i
Â·ğœ½2
+ğœ†âˆ¥ğœ½âˆ¥1)
.(15)
ğœ½ğœ†
LASSO ,Î”ğ‘¡âˆˆ arg min
supp[ğœ½]=supph
bğœ½ğœ†
LASSO ,Î”ğ‘¡i(ğ‘¡âˆ’Î”ğ‘¡âˆ‘ï¸
ğœ=1ğ‘¤(Î”ğ‘¡)
ğœÂ·

ğ‘¦ğœ+Î”ğ‘¡âˆ’h
ğ’™ğœ,Sigğ‘(ğ‘¦ğœâˆ’ğ‘™:ğœ)i
Â·ğœ½2)
. (16)
5: Givenğ‘¥ğ‘¥ğ‘¥ğ‘¡,ğ‘¦ğ‘¡, make prediction on ğ‘¦ğ‘¡+Î”ğ‘¡:
bğ‘¦ğ‘¡+Î”ğ‘¡=h
ğ’™ğ‘¡,Sigğ‘(ğ‘¦ğ‘¡âˆ’ğ‘™:ğ‘¡)i
Â·ğœ½ğœ†
LASSO ,Î”ğ‘¡.
6:end for
7:Output: the forecast(bğ‘¦ğ‘¡+1,Â·Â·Â·,bğ‘¦ğ‘¡+Î”ğ‘‡).
4 Real-time performance
Our signature-based adaptive two-step LASSO algorithm has been
implemented for the trucking operations in Amazon since No-
vember 2022. Performance analysis shows that our forecast model
presents superior performance than commercially available fore-
cast models, with prediction accuracy improvement by more than
fivefold, and has an estimated annualized savings of $50 million.
5002Transportation Marketplace Rate Forecast Using Signature Transform KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Comparing national-level 3-month-ahead industry predictions with our model predictions
Month
in 2021Actual Industrial
prediction% Error
Ind. predictionOur prediction % Error
Our prediction% Accuracy im-
provement
Apr $2.44 $1.89 23% $2.39 2% 1050 %
May $2.51 $1.82 27% $2.45 2% 1250%
Jun $2.53 N/A N/A $2.48 2% N/A
Jul $2.57 $2.18 15% $2.53 2% 650%
Aug $2.61 $2.21 15% $2.58 1% 1400%
Sep $2.71 $2.23 18% $2.70 1% 1700%
Oct $2.72 $2.36 13% $2.69 1% 1200%
Nov $2.72 $2.38 13% $2.71 1% 1200%
Table 2: 12-week ahead model predictions across different regions.
# of week
ahead
predictionN.A. A B C D E
1 1.06% 1.88% 1.53% 1.38% 1.26% 1.53%
2 1.53% 1.93% 2.76% 1.92% 2.08% 2.06%
3 1.55% 2.95% 2.18% 1.19% 2.21% 2.07%
4 1.61% 3.83% 1.87% 1.50% 2.67% 2.34%
5 1.33% 2.57% 2.73% 1.05% 2.99% 1.32%
6 1.44% 2.86% 2.96% 1.85% 2.78% 1.20%
7 1.64% 2.33% 5.69% 1.73% 2.77% 2.51%
8 1.86% 1.71% 5.25% 3.64% 2.55% 2.64%
9 1.88% 2.99% 4.86% 2.55% 3.69% 2.15%
10 2.30% 3.80% 3.31% 4.10% 2.13% 2.53%
11 2.60% 4.71% 5.54% 4.33% 2.06% 2.85%
12 2.38% 5.65% 5.25% 4.76% 2.74% 5.33%
Table 3: Comparison between predictions with and without adaptive signature kernel
Week Actual Prediction
without
signature
kernel% Error with-
out signature
kernelPrediction
with signa-
ture kernel% Error with
signature
kernel
10/31/21 3.37 3.30 2.31% 3.32 1.53%
11/7/21 3.45 3.27 5.19% 3.34 3.22%
11/14/21 3.46 3.25 5.88% 3.38 2.15%
11/21/21 3.52 3.22 8.65% 3.37 4.25%
11/28/21 3.48 3.16 9.28% 3.35 3.66%
12/5/21 3.49 3.15 9.85% 3.31 5.15%
Below we will present the real-time performance using data
from April 2021 to July 2022. While this timeframe precedes the
modelâ€™s actual implementation due to confidentiality restrictions,
it represents a particularly challenging period marked by both
the COVID-19 pandemic and the Ukraine conflict, and allows us to
demonstrate the modelâ€™s effectiveness in volatile market conditions.
We will showcase the national-level prediction in North America
(N.A.) along with five representative regions within North America,designated A, B, C, D, and E to protect proprietary information.
We apply the relative error to measure model performances where
relative error B|actual rateâˆ’forecast rate |
|actual rate|.
Predictions comparison between industry models and our model.
We compare the performance of our model at the national-level
predictions with the standard industry predictions for the April
2021 - November 2021 time period. Both our model predictions and
5003KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
Table 4: Our model predictions posted on June 25 2022
Region Week Actual Prediction % Error
N.A.7/3/2022 1.95 1.93 1.34%
7/10/2022 1.95 1.93 1.12%
7/17/2022 1.94 1.95 0.37%
7/24/2022 1.93 1.96 1.25%
7/31/2022 1.94 1.95 0.90%
A7/3/2022 1.8 1.78 0.68%
7/10/2022 1.8 1.76 2.06%
7/17/2022 1.76 1.76 0.04%
7/24/2022 1.72 1.74 1.46%
7/31/2022 1.66 1.73 4.03%
B7/3/2022 1.26 1.26 0.39%
7/10/2022 1.21 1.27 4.68%
7/17/2022 1.21 1.25 3.44%
7/24/2022 1.21 1.25 3.28%
7/31/2022 1.24 1.25 1.25%
C7/3/2022 1.1 1.1 0.40%
7/10/2022 1.08 1.09 0.70%
7/17/2022 1.02 1.08 5.32%
7/24/2022 1.02 1.04 2.01%
7/31/2022 1.02 1.01 0.60%
D7/3/2022 1.92 1.95 1.86%
7/10/2022 1.94 1.96 0.99%
7/17/2022 1.98 1.98 0.28%
7/24/2022 2.01 2.03 1.06%
7/31/2022 2.01 2.07 2.86%
E7/3/2022 1.79 1.76 1.70%
7/10/2022 1.76 1.75 0.75%
7/17/2022 1.76 1.74 0.94%
7/24/2022 1.76 1.73 1.89%
7/31/2022 1.75 1.73 1.01%
industry predictions are made three months (twelve weeks) ahead
of time, with monthly predictions obtained by aggregating weekly
predictions. The detailed results are listed in Table 1. In particular,
our predictions (with a relative error of around 2%) are far superior
to standard industry predictions (with a relative error of around
20%). The prediction accuracy is improved by more than fivefold.
Relative prediction errors up to twelve weeks. Table 2 reports the
relative prediction error of our model for the national level and
five regional predictions (A, B, C, D, and E), up to a twelve-week
horizon. The prediction error moderately increases from around
1%for the one-week prediction to approximately 5%for the twelve-
week prediction, remaining significantly lower than the industry
standard of 15%.
Necessity of adaptive signature kernel. To demonstrate the neces-
sity of the adaptive signature kernel, the key and novel component
of our model, we compare the predictions from Algorithm 2 with
and without the signature kernel in (9). The results are reported in
Table 3. The predictions presented here are for one representativeregion in North America on October 24, 2021. Evidently from Table
3, the errors without the signature kernel are larger ( >8%) even for
short-term predictions. In contrast, the signature kernel method
captures better the seasonality, and obtains a smaller relative error
(<5%) for short-term predictions. This table shows the effectiveness
of incorporating the signature kernel in the forecast model.
Short-term prediction error. Table 4 reports the relative prediction
error of our model for the national level and five regional predictions
(A, B, C, D, and E), for a five-week horizon, with model predictions
made on June 25, 2022. Most of the prediction errors are shown to
be less than 2%.
5 Conclusion
This work presents a novel, highly accurate signature-based adap-
tive two-step LASSO forecasting model for transportation market-
place rates. Deployed at Amazon since November 2022, it delivers
more than fivefold forecast accuracy improvements compared to
5004Transportation Marketplace Rate Forecast Using Signature Transform KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
industry models even amidst major market disruptions, with sig-
nificant cost savings.
References
[1]Imanol Perez Arribas. 2018. Derivatives pricing using signature payoffs. arXiv
preprint arXiv:1809.09466 (2018).
[2]Alexandre Belloni and Victor Chernozhukov. 2013. Least squares after model
selection in high-dimensional sparse models. Bernoulli 19, 2 (2013), 521â€“547.
[3]Alain Berlinet and Christine Thomas-Agnan. 2011. Reproducing Kernel Hilbert
Spaces in Probability and Statistics. Springer Science & Business Media.
[4]Patric Bonnier, Patrick Kidger, Imanol Perez Arribas, Cristopher Salvi, and Terry
Lyons. 2019. Deep signature transforms. In Proceedings of the 33rd International
Conference on Neural Information Processing Systems. 3105â€“3115.
[5]Kuo-Tsai Chen. 1954. Iterated Integrals and Exponential Homomorphism. Pro-
ceedings of the London Mathematical Society s3-4, 1 (1954), 502â€“512.
[6]Kuo-Tsai Chen. 1957. Integration of Paths, Geometric Invariants and a General-
ized Baker-Hausdorff Formula. Annals of Mathematics 65, 1 (1957), 163â€“178.
[7]Didier ChÃ©telat, Johannes Lederer, and Joseph Salmon. 2017. Optimal two-step
prediction in regression. Electronic Journal of Statistics 11, 1 (2017), 2519â€“2546.
[8]Ilya Chevyrev and Terry Lyons. 2016. Characteristic functions of measures on
geometric rough paths. Annals of Probability 44, 6 (2016), 4049â€“4082.
[9]Ilya Chevyrev and Harald Oberhauser. 2022. Signature moments to characterize
laws of stochastic processes. Journal of Machine Learning Research 23, 176 (2022),
1â€“42.
[10] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[11] Peter K Friz and Martin Hairer. 2020. A course on rough paths. Springer.
[12] Peter K Friz and Nicolas B Victoir. 2010. Multidimensional stochastic processes as
rough paths: theory and applications. Vol. 120. Cambridge University Press.
[13] Everette S Gardner Jr. 2006. Exponential smoothing: The state of the artâ€”Part II.
International journal of forecasting 22, 4 (2006), 637â€“666.
[14] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÃ¶lkopf, and
Alexander Smola. 2012. A kernel two-sample test. Journal of Machine Learning
Research 13, 1 (2012), 723â€“773.
[15] Ben Hambly and Terry Lyons. 2010. Uniqueness for the signature of a path of
bounded variation and the reduced path group. Annals of Mathematics (2010),
109â€“167.
[16] James D Hamilton. 1989. A new approach to the economic analysis of nonstation-
ary time series and the business cycle. Econometrica: Journal of the econometric
society (1989), 357â€“384.
[17] Jasdeep Kalsi, Terry Lyons, and Imanol Perez Arribas. 2020. Optimal execution
with rough path signatures. SIAM Journal on Financial Mathematics 11, 2 (2020),
470â€“493.
[18] Patrick Kidger and Terry Lyons. 2020. Signatory: differentiable computations of
the signature and logsignature transforms, on both CPU and GPU. In InternationalConference on Learning Representations.
[19] Franz J KirÃ¡ly and Harald Oberhauser. 2019. Kernels for sequentially ordered
data. Journal of Machine Learning Research 20, 31 (2019), 1â€“45.
[20] S Vasantha Kumar and Lelitha Vanajakshi. 2015. Short-term traffic flow predic-
tion using seasonal ARIMA model with limited input data. European Transport
Research Review 7, 3 (2015), 1â€“9.
[21] Chenyang Li, Xin Zhang, and Lianwen Jin. 2017. LPSNet: a novel log path
signature feature based hand gesture recognition framework. In Proceedings of
the IEEE International Conference on Computer Vision Workshops. 631â€“639.
[22] Terry Lyons, Sina Nejad, and Imanol Perez Arribas. 2019. Numerical method
for model-free pricing of exotic derivatives in discrete time using rough path
signatures. Applied Mathematical Finance 26, 6 (2019), 583â€“597.
[23] Terry Lyons, Hao Ni, and Harald Oberhauser. 2014. A feature set for streams and
an application to high-frequency financial tick data. In Proceedings of the 2014
International Conference on Big Data Science and Computing. 1â€“8.
[24] Terry J Lyons, Michael Caruana, and Thierry LÃ©vy. 2007. Differential equations
driven by rough paths. Springer.
[25] James Morrill, Adeline Fermanian, Patrick Kidger, and Terry Lyons. 2020. A
generalised signature method for multivariate time series feature extraction.
arXiv preprint arXiv:2006.00873 (2020).
[26] Robert H Shumway, David S Stoffer, Robert H Shumway, and David S Stoffer.
2017. ARIMA models. Time series analysis and its applications: with R examples
(2017), 75â€“163.
[27] Carl-Johann Simon-Gabriel and Bernhard SchÃ¶lkopf. 2018. Kernel distribution
embeddings: Universal kernels, characteristic kernels and kernel metrics on
distributions. Journal of Machine Learning Research 19, 1 (2018), 1708â€“1736.
[28] Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard SchÃ¶lkopf,
and Gert RG Lanckriet. 2010. Hilbert space embeddings and metrics on probability
measures. The Journal of Machine Learning Research 11 (2010), 1517â€“1561.
[29] James W Taylor. 2004. Smooth transition exponential smoothing. Journal of
Forecasting 23, 6 (2004), 385â€“404.
[30] Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal
of the Royal Statistical Society: Series B (Methodological) 58, 1 (1996), 267â€“288.
[31] Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, and Terry Lyons. 2017. Learning
spatial-semantic context with fully convolutional recurrent network for online
handwritten chinese text recognition. IEEE transactions on pattern analysis and
machine intelligence 40, 8 (2017), 1903â€“1917.
[32] Weixin Yang, Lianwen Jin, and Manfei Liu. 2015. Chinese character-level writer
identification using path signature feature, DropStroke and deep CNN. In 2015
13th International Conference on Document Analysis and Recognition (ICDAR).
IEEE, 546â€“550.
[33] Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. 2019. A review
of recurrent neural networks: LSTM cells and network architectures. Neural
computation 31, 7 (2019), 1235â€“1270.
[34] Peng Zhao and Bin Yu. 2006. On model selection consistency of Lasso. The
Journal of Machine Learning Research 7 (2006), 2541â€“2563.
[35] Hui Zou. 2006. The adaptive lasso and its oracle properties. Journal of the
American statistical association 101, 476 (2006), 1418â€“1429.
5005