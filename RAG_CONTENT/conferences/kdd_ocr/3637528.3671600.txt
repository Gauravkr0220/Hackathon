EEG2Rep: Enhancing Self-supervised EEG Representation
Through Informative Masked Inputs
Navid Mohammadi Foumaniâˆ—
Monash University
Melbourne, Australia
navid.foumani@monash.eduGeoffrey Mackellar
Emotiv Research
Sydney, Australia
geoff@emotiv.comSoheila Ghane
Emotiv Research
Melbourne, Australia
soheila@emotiv.com
Saad Irtza
Emotiv Research
Sydney, Australia
saad@emotiv.comNam Nguyen
Emotiv Research
Sydney, Australia
namnguyen@emotiv.comMahsa Salehi
Monash University
Melbourne, Australia
mahsa.salehi@monash.edu
ABSTRACT
Self-supervised approaches for electroencephalography (EEG) rep-
resentation learning face three specific challenges inherent to EEG
data: (1) The low signal-to-noise ratio which challenges the quality
of the representation learned, (2) The wide range of amplitudes
from very small to relatively large due to factors such as the inter-
subject variability, risks the models to be dominated by higher
amplitude ranges, and (3) The absence of explicit segmentation in
the continuous-valued sequences which can result in less infor-
mative representations. To address these challenges, we introduce
EEG2Rep, a self-prediction approach for self-supervised representa-
tion learning from EEG. Two core novel components of EEG2Rep
are as follows: 1) Instead of learning to predict the masked in-
put from raw EEG, EEG2Rep learns to predict masked input in
latent representation space, and 2) Instead of conventional masking
methods, EEG2Rep uses a new semantic subsequence preserving
(SSP) method which provides informative masked inputs to guide
EEG2Rep to generate rich semantic representations. In experiments
on 6 diverse EEG tasks with subject variability, EEG2Rep signif-
icantly outperforms state-of-the-art methods. We show that our
semantic subsequence preserving improves the existing masking
methods in self-prediction literature and find that preserving 50% of
EEG recordings will result in the most accurate results on all 6 tasks
on average. Finally, we show that EEG2Rep is robust to noise ad-
dressing a significant challenge that exists in EEG data. Models and
code are available at:https://github.com/Navidfoumani/EEG2Rep
CCS CONCEPTS
â€¢Applied computing â†’Health care information systems; â€¢
Computing methodologies â†’Machine learning.
KEYWORDS
EEG Representation Learning, EEG self-supervised Learning, EEG
Masking, EEG Classification
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671600ACM Reference Format:
Navid Mohammadi Foumani, Geoffrey Mackellar, Soheila Ghane, Saad Irtza,
Nam Nguyen, and Mahsa Salehi. 2024. EEG2Rep: Enhancing Self-supervised
EEG Representation Through Informative Masked Inputs. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671600
1 INTRODUCTION
An electroencephalogram (EEG) is a noninvasive method that cap-
tures brain data by placing electrodes on the patientâ€™s scalp surface,
enabling the recording of electrical activity within the brain [ 1].
This specialized and complex biological electrical signal serves as a
reflection of the brainâ€™s functional state, providing insights into the
individualâ€™s mental condition [ 2]. From this data, valuable informa-
tion can be extracted, including vital signs that facilitate continuous
monitoring of the patientâ€™s health [ 3]. Additionally, EEG plays a
crucial role in diagnosing and identifying various brain conditions,
finding applications in diverse healthcare domains such as sleep
medicine, neurological disorders, cardiovascular disease detection,
and activity monitoring [4â€“6].
In the past decade, the integration of deep learning into biomed-
ical research has experienced significant growth, demonstrating
its ability to frequently outperform conventional machine learning
methods across various tasks [ 4,6â€“9]. However, the training of deep
learning models for biomedical applications requires substantial
amounts of data, annotated by experts, whose collection is often
time and cost-prohibitive. Furthermore, deeper neural networks are
susceptible to overfitting the EEG data, particularly in the presence
of inter-subject variability [ 10,11]. Self-supervised learning (SSL)
has emerged as a prominent solution for such problems, as it al-
lows learning powerful representations from vast unlabeled data by
producing supervisory signals directly from the data [ 9,10,12,13].
In EEG analysis, two commonly used self-supervised learning
approaches are invariance-based methods and self-prediction meth-
ods [ 9,14]. In invariance-based methods, the objective is to optimize
an encoder to generate similar embeddings for two or more views
of the same EEG time series [ 14]. These different views of the time
series are usually crafted using a set of manual data augmentations,
including techniques like jittering, permutation, and scaling [ 15,16].
The main idea behind self-prediction methods is to remove or cor-
rupt parts of the input and train the model to predict or reconstruct
5544
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Navid Mohammadi Foumani et al.
Eyes close
Eyes open
(a) MAEEG (Acc:75.21%)
Eyes Close
Eyes Open (b) EEG2Rep (Acc: 81.66%)
Figure 1: Comparison of 2D t-SNE plots for representation
learned by (a) MAEEG and (b) EEG2Rep on the Crowdsourced
EEG dataset.
the altered content [ 14]. For example, approaches like Masked Au-
toEncoders (MAE) [ 17] learn representations by reconstructing
randomly masked patches from an input [10, 17, 18].
Invariance-based pretraining methods can construct representa-
tions of high-level semantics by capturing essential features con-
sistent across various data views. This strategy enables the model
to identify and prioritize features important for understanding
the underlying semantics of the data, as these features remain un-
changed despite variations in the input. However, we believe that
invariance-based methods perform well in computer vision and
natural language processing due to the strong constraints present
in image and text data. For example, the success with images arises
from tasks related to object interpretation, where transformations
such as scaling, blurring, and rotation assume that the resulting
images will be similar to those generated in the original scenario
with changes in camera zoom, stability, focus, or angle. However,
there do not appear to be equivalent transformations that can be
applied to EEG data. Augmentation methods applied to EEG data
can inadvertently modify the semantic meaning, underlying distri-
bution, and class representation of the signals [ 19,20]. For example,
augmentation may introduce synthetic patterns or artifacts that do
not align with genuine brain activity.
Moreover, invariance-based methods may introduce significant
biases, potentially hindering specific downstream tasks or even
pretraining tasks with diverse data distributions. The generalization
of these biases for tasks requiring varying levels of abstraction often
remains unclear. Distinct data augmentation strategies may result in
misinterpretation or misclassification by the model. For example, in
scenarios such as sleep stage classification (involving low-frequency
bands) and emotion recognition (involving high-frequency bands),
identical augmentations may not be suitable for both cases.
In contrast to invariance-based methods, self-prediction pretrain-
ing tasks demand less prior knowledge and demonstrate ease of
generalization across diverse downstream tasks [ 21,22]. However,
self-prediction pretraining faces unique challenges when applied
to EEG data [ 10,12] which makes it ineffective. Fig. 1a depicts
the visualization of EEG representations by a state-of-the-art self-
prediction EEG pretraining model, namely MAEEG [ 12], on Crowd-
sourced dataset [ 23] with two classes. The two classes are not easily
separable in the learned representation space, resulting in low clas-
sification accuracy. Here, we outline the three main challenges that
exist in EEG data:â€¢Challenge 1: The recorded EEG is invariably contaminated
with noise, impacting the reconstruction loss function and
potentially introducing significant errors. Even accurate pre-
dictions may yield high errors due to the pronounced impact
of noise on the loss function.
â€¢Challenge 2: EEG data has a wide range of amplitude values,
which can be due to the variability between different sub-
jects or the variability of electrode placement. The substan-
tial ranges make it particularly challenging to reconstruct
accurate values during the reconstruction process.
â€¢Challenge 3: EEG signals differ from text and images in that
there is no explicit segmentation for EEG data, as they are
continuous-valued sequences. Hence, the implementation
of a masking strategy becomes essential for a model to have
sufficient enough information for the reconstruction of the
masked EEG.
To address the challenges mentioned above, we introduce EEG2Rep
to enhance the semantic quality of EEG representations without
relying on prior knowledge about downstream tasks. In contrast to
the existing self-prediction methods that learn EEG representations
by reconstructing the raw EEG data space [ 10,12], EEG2Rep is
trained to reconstruct more abstract features of EEG data in the
latent space. Such approaches have shown to be effective in image
and text representation learning [ 22,24,25]. Our motivation is that
the existing noise in EEG is less likely to remain in the abstract
features of EEG, and by learning to reconstruct the abstract features
we potentially eliminate the unnecessary noise that exists in raw
EEG data (addressing challenge 1). Additionally, as the abstract
features are normalized within the representation space, the recon-
struction of these features becomes more straightforward compared
to reconstructing the potentially high amplitude value range of raw
EEG data (addressing challenge 2). Finally, EEG2Rep leverages our
novel Semantic Subsequence Preserving method to ensure that the
context has sufficiently meaningful and rich semantic information
(addressing challenge 3).
Fig. 1b displays the visualization of EEG representations learned
by EEG2Rep. The two classes in this figure are easily separable
which highlights the effectiveness of EEG2Rep in enhancing EEG
representations and, consequently, improving classification accu-
racy. Another core component of EEG2Rep is its efficient multi-
masking design. Specifically, we reuse the same target represen-
tation for various masked versions of each sample. Additionally,
we predict the representation of various target blocks for a single
masked input to improve efficiency further.
This work follows from the project with Emotiv Research1, a
bioinformatics research company based in Australia, and Emotiv, a
global technology company specializing in the development and
manufacturing of wearable EEG products. In our prior work, we
looked at detecting distraction episodes in drivers by analyzing their
brain EEG as a case study. One significant challenge we encountered
was the presence of noise in the recorded EEG datasets and the
inability of current supervised detection models to learn patterns of
distraction within the presence of noise. This paper addresses this
issue, along with the two additional challenges mentioned above.
1www.emotiv.com/research
5545EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
2 METHODOLOGY OF EEG2REP
2.1 Problem Definition
Our goal is to address the problem of learning a nonlinear em-
bedding function that can effectively map each EEG sample ğ‘‹ğ‘–=
{ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ¿}from a given dataset ğ·into a concise and mean-
ingful representation ğ‘…ğ‘–âˆˆRğ‘‘ğ‘’, whereğ‘‘ğ‘’indicates the desired
representation dimension. The EEG dataset ğ·consists ofğ‘›sam-
ples, denoted as ğ·={ğ‘‹1,ğ‘‹2,...,ğ‘‹ğ‘›}, with eachğ‘‹ğ‘–representing a
ğ¶-channel EEG sequence of length ğ¿. To evaluate the quality of
our learned representation R={ğ‘…1,ğ‘…2,...,ğ‘…ğ‘›}, we examine two
scenarios based on the availability of labeled data: i) Linear Probing :
We first pre-train a model without labels through a self-supervised
pretext task. Upon completing the pre-training phase, we freeze
the encoder and add a linear classifier on top of the pre-trained
modelâ€™s output or intermediate representations. The linear classi-
fier is then trained on a downstream task, typically a classification
task, utilizing the pre-trained representations as inputs, and ii) Fine-
Tuning : Initially, we pre-train a model without labels through a
self-supervised pretext task. Next, we perform fine-tuning by train-
ing the entire model for a few epochs using a labeled dataset in a
fully supervised manner.
2.2 Model architecture
Illustrated in Fig. 2, the EEG2Rep model is introduced with the
main goal of predicting the representation of a given EEG sample
based on a masked view of the same EEG input. We now explain
each component of this architecture separately in the following
subsections.
Input Embedding. Building upon established works [ 11,15,20,
26â€“28], we adopt a 3-layer convolutional neural network as input
embedding to convert the raw EEG data into patches. Specifically,
we feed EEG sample ğ‘‹ğ‘–to the first layer that incorporates a depth-
wise convolutional layer, specifically designed to capture the spatial
correlations between channels [ 11,27]. This is succeeded by a linear
spatial filter to amplify the signal-to-noise ratio. The spatial filters
leverage the fact that neural signals exhibit specific spatial patterns
across the scalp, while noise sources may manifest more random
spatial patterns [26].
Following the spatial filter, we integrate max pooling and spatial
padding to ensure translation equivalence and effectively address
edge effects. The output of this network is a set of EEG patches Ë†ğ‘†x=
{Ë†ğ‘†ğ‘¥1,...,Ë†ğ‘†ğ‘¥ğ‘™}where Ë†ğ‘†ğ‘¥ğ‘–âˆˆRğ‘‘ğ‘¥, andğ‘‘ğ‘¥is the embedding dimension,
andğ‘™is the number of patches. Lastly, for every patch Ë†ğ‘†ğ‘¥ğ‘–, the
positional embedding feature of the ğ‘–ğ‘¡â„position is added to it,
resulting in the EEG patches ğ‘†x={ğ‘†ğ‘¥1,...,ğ‘†ğ‘¥ğ‘™}(shown in blue in
Fig. 2). Please note for each EEG sequence ğ‘‹ğ‘–, we will have a set
of EEG patches ğ‘†ğ‘–xas the output of the input embedding network.
However, for simplicity, we will drop the superscript ğ‘–fromğ‘†ğ‘–xand
useğ‘†xas an input to the following components in the EEG2Rep
architecture.
Context-driven target prediction. Previous self-prediction meth-
ods for EEG data have primarily followed the approach of Masked
Autoencoders (MAE) [ 17], reconstructing local windows of the
raw input EEG [ 12], or have adopted a BERT-like method [ 29],
predicting discrete representations [ 10]. However, the resultingrepresentations often demonstrate lower semantic quality than
invariance-based methods during off-the-shelf evaluations, such
as linear probing, due to the intrinsic characteristics of EEG data
mentioned earlier. Low signal-to-noise ratio in EEG challenges the
reconstruction task and a wide range of amplitudes in EEG further
complicates the optimization process.
To enhance the semantic depth of self-supervised representation
learning, we introduce the concept of context-driven target predic-
tionfor EEG data. In this approach, the model is trained to predict
the representations of the original unmasked training data based
on an encoding derived from the masked sample in abstract repre-
sentation space. Compared to self-prediction methods that predict
in raw space, EEG2Rep uses abstract prediction targets for which
unnecessary raw-level details or noise are potentially eliminated,
thereby leading the model to learn richer semantic features.
EEG2Rep comprises three main components: Target network
uses the complete (unmasked) input embedding ğ‘†xto generate
semantically rich representations, allowing each patch to encode
knowledge of all others through its self-attention architecture. The
resulting output serves as the target for our learning task. Context
network shares the architecture with the target network, differing
only in its use of the masked version of the input embedding to gen-
erate representations for visible patches. Our innovative semantic
subsequence preserving method ensures that the context networkâ€™s
output contains sufficient semantic information for reconstruction
purposes. We use a standard transformer [ 29,30] for both target
and context networks. Predictor network leverages the output of the
context network and randomly chosen masked tokens to regress
the targets, i.e., the output of the target network. We enhance con-
textualized information integration for the predictor network by
incorporating cross-attention-based transformers [ 29]. Below, we
elaborate on the process of creating the target, context, and predic-
tor networks for the training task, providing clarification on how
these networks are parameterized.
Target Network
Given an EEG embedding ğ‘†x={ğ‘†ğ‘¥1,...,ğ‘†ğ‘¥ğ‘™}, we feed it through
the target-encoder ğ‘“Â¯ğœƒto obtain a corresponding patch-level repre-
sentation y={ğ‘¦1,...,ğ‘¦ğ‘™}:
y=ğ‘“Â¯ğœƒ(ğ‘†x) (1)
whereğ‘¦ğ‘–âˆˆRğ‘‘ğ‘’is the representation associated with the ğ‘†ğ‘¥ğ‘–patch
andğ‘‘ğ‘’is the transformerâ€™s embedding dimension (shown in green
in Fig. 2). We apply normalization to these representations, prevent-
ing the model from collapsing into a constant representation for
all time steps and ensuring that values with high amplitude do not
dominate the target features.
To obtain the targets for our reconstruction loss, we randomly
sampleğ‘€blocks from the target representation y.ğµğ‘–is theğ‘–ğ‘¡â„
masked block in ywhereğ‘–âˆˆ{1,2,...,ğ‘€}andy(ğ‘–)={ğ‘¦ğ‘—}ğ‘—âˆˆğµğ‘–
(e.g.,ğµ1andğµ2is annotated in red in Fig. 2 and y(1) contains all
{ğ‘¦ğ‘—}ğ‘—âˆˆğµ1). Note that the target blocks are chosen from the output
of the target encoder, not the input of the target encoder. This
distinction is crucial to ensure each target representation retains
a high semantic level as it encodes the knowledge of all input
patches through the self-attention mechanism in Transformers
architecture [21, 22, 24].
5546KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Navid Mohammadi Foumani et al.
Input	EmbeddingL!Lossğ‘”"â€¦â€¦â€¦Target	Encoder
Context	Encoderğ‘“&'ğ‘“'PredictorEMAStop	Grad
Context:	ğ‘ƒ!â€¦ Context:	ğ‘ƒ"â€¦â€¦â€¦ AF3F3O2
Raw	EEGâ€¦ â€¦ ğµ!ğµ"
Sğ±:EEG	Patchğ²	:Target	Representationğ«	:Context	Representationğ¦	:Mask	Token>ğ’š	:Target	Prediction
Figure 2: Architecture of EEG2Rep
Context Network
To obtain the context in EEG2Rep, we initially sample ğ‘contexts
from the EEG patches ğ‘†x(refer to Sec.2.3 for details on our novel
context selection). Only visible context patches are processed via
context-encoder model ğ‘“ğœƒto obtain context representations:
r(ğ‘)=ğ‘“ğœƒ(ğ‘ƒğ‘)=ğ‘“ğœƒ({ğ‘†ğ‘¥ğ‘¡}ğ‘¡âˆˆğ‘ƒğ‘) (2)
Whereğ‘ƒğ‘correspond to the ğ‘ğ‘¡â„context which contains a subset of
EEG patches from ğ‘†xandr(ğ‘)={ğ‘Ÿğ‘¡}ğ‘¡âˆˆğ‘ƒğ‘is its patch level repre-
sentation (shown in yellow in Fig. 2). Since the target blocks are
sampled independently from the contexts, there may be significant
overlap. We exclude any overlapping regions from the target block
in the loss calculation to ensure a non-trivial prediction task. Exam-
ples of various contexts ( ğ‘ƒ1andğ‘ƒğ‘) and target blocks ( ğµ1andğµ2)
are illustrated in Fig. 2. The remaining patches are called masked
tokens (shown in grey in Fig. 2).
Predictor Network
For the Predictor network, we use a 4-layer cross-attention trans-
former to enhance the effective correlation among the context
representation and masked tokens. The â€œcross-attention" aspect
of the transformer is capable of blending two distinct embedding
sequences, which may vary in length and originate from different
sources [ 30]. In this setup, the masked token serves as a query,
while the key and values are sourced from the context encoder.
Given the output of the context encoder r(ğ‘), our goal is to
predictğ‘€target block representations {y(1),...,y(ğ‘€)}. For each
target block y(ğ‘–), the predictor ğ‘”ğœ™takes as input the output of the
context encoder r(ğ‘)and a mask token ğ‘š(ğ‘–)for each patch we wish
to predict and outputs a patch-level prediction:
Ë†y(ğ‘–)=ğ‘”ğœ™(r(ğ‘),ğ‘š(ğ‘–)) (3)
whereğ‘š(ğ‘–)={ğ‘šğ‘—}ğ‘—âˆˆğµğ‘–. The mask tokens are parameterized
by a shared learnable vector with an added positional embedding.
Since we wish to make predictions for ğ‘€target blocks, we apply
our predictor ğ‘€times, each time conditioning on the mask tokens
corresponding to the target block locations we wish to predict.loss: Given context-driven training targets y(ğ‘–)and the predicted
patch-level representation Ë†y(ğ‘–), we use the L2 loss:
ğ¿ğ‘œğ‘ ğ‘ ğ‘Ÿğ‘’ğ‘=1
ğ‘€ğ‘€âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğ‘—âˆˆğµğ‘–||ğ‘¦ğ‘—âˆ’Ë†ğ‘¦ğ‘—||2
2(4)
EEG2Rep Weights
The predictor parameters ğœ™and the context network parameters ğœƒ
are optimized through gradient-based methods. However, the target
networkâ€™s parameters Â¯ğœƒundergoes updates using an exponentially
moving average (EMA) [ 21,31] of the context network parameters:
Â¯ğœƒ=ğœÂ¯ğœƒ+(1âˆ’ğœ)ğœƒ (5)
We implement a schedule for the hyperparameter ğœthat linearly
increases from an initial value ğœ0to the target value ğœğ‘’during the
firstğœğ‘›updates. After this initial phase, the value remains constant
for the rest of the training. This approach ensures that the target
network is updated more frequently in the early stages of training
when the context network is random, and less frequently in later
stages when more robust parameters have been learned.
2.3 Semantic Subsequence Preserving (SSP)
Random masking has proven successful for Masked Autoencoders
[17], where random patches are sampled without replacement, fol-
lowing a uniform distribution, resulting in significant efficiency
improvements. Fig. 3 illustrates various masking strategies applied
to EEG samples, with the top subplot showcasing the original EEG
sample from Crowdsourced datasets [ 23] recorded for 0.5 seconds
at 128 Hz. The subsequent subplots demonstrate different masking
techniques, each with a 50%masking ratio. The second subplot ex-
hibits random masking, following the Masked Autoencoders style.
While the MAE approach has demonstrated success in computer
vision tasks, it may pose challenges in building semantic representa-
tions for EEG data due to the lack of structure in the created masks
(e.g., random binary masking). Notably, most of the masked regions
can be reconstructed trivially, for example, through interpolation.
On the other hand, block masking [ 21,24], involves masking entire
5547EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Time
Figure 3: Different masking strategies applied to EEG samples
from Crowdsourced datasets [ 23] with a 50% masking ratio.
From top to bottom: (1) Original EEG sample, (2) Random
masking, (3) Block masking, and (4) Semantic Subsequence
Preserving (SSP ).
blocks of time-steps or patches. However, this approach does not
guarantee that large continuous portions of the training sample
remain unmasked. As shown in Fig. 3, block masking primarily
focuses on the structure of the mask rather than context. Our ob-
jective is to enable the context encoder to construct semantically
rich representations over local regions of the sample.
To address this, we propose Semantic Subsequence Preserving
(SSP). Instead of determining which time steps to mask, this ap-
proach decides which time steps to preserve in a block-wise manner.
The process begins by randomly selecting ğ›½starting points and
symmetrically expanding them until the block reaches a width
given by:
Block Size =âŒˆ(1âˆ’ğœŒ)Ã—ğ‘™
ğ›½âŒ‰ (6)
Here,ğ‘™represents the total number of time steps (patches) in a
training sample, ğ›½is the desired number of blocks, and ğœŒis the
mask ratio. We allow visible blocks to overlap, leading to oversized
subsequences preserving and some variability in the number of
visible time steps for each EEG sample. As shown in Fig. 3, the
first preserved subsequence arises from overlapping two blocks. As
we encode only visible time steps, we adopt a simple strategy to
maintain uniformity in the number of masked time steps across all
samples in a batch. Following the semantic subsequence preserving
step, we randomly preserve individual time steps until we reach the
targeted number of preserved time steps, calculated as (1âˆ’ğœŒ)Ã—ğ‘™.
2.4 Efficiency: Multiple masking
As discussed in earlier sections, our model comprises three main
components: the target, context, and predictor networks. The pro-
cessing of each sample through these networks can be computation-
ally intensive. Additionally, computing activations for the target
network is less efficient compared to the others, as it requires pro-
cessing the entire unmasked EEG input. Hence, to mitigate the
computation cost, we implement a two-stage approach with multi-
ple masking and multiple predictions.Multiple Masking We exploreğ‘different masked versions of the
training sample and compute the loss with respect to the same target
representation. This is feasible because target representations are
based on the full unmasked version of the sample. As ğ‘increases,
the computational overhead of computing target representations
becomes negligible.
Multiple Prediction Subsequently, having obtained representa-
tions of various masked versions of the same input, we predict
the representation of various target blocks for a single masked in-
put to further enhance efficiency. Now, the reconstruction loss in
Equation 4 is updated to:
ğ¿ğ‘œğ‘ ğ‘ ğ‘Ÿğ‘’ğ‘âˆ—=1
ğ‘€Ã—ğ‘ğ‘âˆ‘ï¸
ğ‘=1ğ‘€âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğ‘—âˆˆğµğ‘–||ğ‘¦ğ‘—âˆ’Ë†ğ‘¦ğ‘—||2
2(7)
2.5 Loss Regularisation
A common challenge faced by algorithms generating and predict-
ing their own targets is representation collapse, where the model
produces similar representations for all masked segments, making
the problem trivial to solve [ 32,33]. Various strategies have been
proposed to address this issue such as contrastive learning[ 34] for
invariance-based methods and stop gradient[ 21,31,35] and addi-
tional clustering [ 33] for self-prediction methods. As we mentioned
earlier, we use the exponentially moving average and stop gradi-
ent methods to prevent representation collapse similar to other
models in CV and NLP like BYOL [ 35], Data2vec [ 21], IBOT [ 31].
However, in our experiment, we found that preventing collapse
does not guarantee that the model learns high-quality representa-
tions. To further enhance the representation learning process, we
add Variance-Invariance-Covariance (VICReg) [ 36] regularization
to our current reconstruction loss. VICReg encourages more vari-
ety among the data in the batch by using a hinge loss that limits
how much the standard deviation can change. It also involves a
covariance loss, which penalizes the off-diagonal elements of the co-
variance matrix of the representation, encouraging less correlation
between features. The final loss of our model is updated to:
Total Loss =ğœ†ğ¿ğ‘œğ‘ ğ‘ ğ‘Ÿğ‘’ğ‘âˆ—+ğœ‡ğ‘£(ğ‘…)+ğ›¾ğ‘(ğ‘…) (8)
Where Total Loss is minimized over batches of samples and ğ‘£(ğ‘…)
andğ‘(ğ‘…)denotes the variance, and covariance losses on the repre-
sentations, respectively. ğœ†,ğœ‡,ğ›¾ are hyperparameters controlling the
balance between these loss components.
3 RELATED WORK
Traditional approaches to extract useful features from EEG are
surveyed in [ 37]. In recent years, advanced self-supervised learn-
ing methods have been proposed aiming to derive representations
from sparsely labeled EEG data. Here, we discuss two primary self-
supervised approaches for EEG representation learning: invariance-
based and self-prediction, and refer interested readers to [ 9,14] for
more details.
3.1 Invariance-based self-supervised learning
In the field of EEG, a series of studies have drawn inspiration from
the SimCLR [ 38] framework in computer vision and ALBERT [ 39]
in NLP, aiming to generate consistent embeddings for various yet
compatible views of the same input [ 9,14â€“16,40]. This involves cap-
turing consistent characteristics across different views, where EEG
5548KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Navid Mohammadi Foumani et al.
views are typically formed using a set of carefully designed data
augmentations. For instance, SeqCLR [ 16] applies diverse strate-
gies like amplitude scaling, temporal shifting, DC shifting, and
band-pass filtering to create different views. Similarly, Time-series
Temporal and Contextual Contrasting (TS-TCC) [ 15] use weak and
strong augmentations to transform input series into two views,
utilizing a temporal contrasting module to learn robust temporal
representations. The contrasting contextual module is then built
upon the contexts from the temporal contrasting module and aims
to maximize similarity among contexts of the same sample while
minimizing similarity among contexts of different samples.
Time-Frequency Consistency (TF-C) [ 40] leverages the frequency
domain to achieve better representation. It proposes that the time-
based and frequency-based representations, learned from the same
time series sample, should be more similar to each other in the time-
frequency space compared to representations of different time series
samples. Kan et al. [ 41] introduced a new augmentation method
called â€˜meiosisâ€™ that involves randomly exchanging data segments
between samples to create positive correlations, using group-level
contrastive learning to distinguish emotional states. Furthermore,
Shen et al. [ 42] implemented cross-subject contrastive learning.
They compared negative pairs from different individuals with posi-
tive pairs from the same subjects during similar events. This helps
the model generalize EEG representations while reducing variations
due to individual differences. While invariance-based methods can
generate informative representations, they introduce strong biases
that may lead to model misinterpretation or misclassification. Addi-
tionally, defining generalizable augmentations is more challenging
in EEG data compared to other types of data.
3.2 Self-Prediction based self-supervised
learning
The primary goal of self-prediction-based models is to reconstruct
corrupted or masked input. Following the success of models like
BERT [ 29], BErt-inspired Neural Data Representations (BENDR)
[10] utilizes a mask-autoencoder approach to generate representa-
tions and minimize reconstruction loss between masked and recon-
structed features. BENDR encodes multi-channel EEG signals into
temporal embeddings using a 1D convolution block, inspired by the
wave2vec approach [ 43]. It then employs a transformer encoder to
process locally masked EEG signals for feature and representation
extraction. Pre-trained on the extensive TUH EEG dataset [ 44,45],
the model shows improved performance across tasks like motor
imagery, sleep stage classification, and event recognition. Similarly,
MAEEG [ 12] refines this approach by adding two layers to convert
the transformer outputs back to the original EEG dimensions, recon-
structing the EEG signal from contextual features and minimizing
the loss between original and reconstructed signals. Li et al. [ 46]
introduce a multi-view mask autoencoder that reconstructs masked
EEG content across spectral, spatial, and temporal dimensions to
derive emotion-related EEG features.
In contrast to invariance-based methods, self-prediction pretrain-
ing tasks demand less prior knowledge and demonstrate ease of
generalization across diverse downstream tasks [ 21]. However, the
resulting representations are typically of a lower semantic level and
may underperform invariance-based pretraining in off-the-shelf
evaluations like linear probing and pretraining, mainly due to theintrinsic nature of EEG data. In this study, we explore ways to
enhance the semantic level of self-supervised representations with-
out relying on additional prior knowledge encoded through data
augmentation. To achieve this, we introduce EEG2Rep to improve
self-supervised EEG representations through informative masked
inputs.
Table 1: Overview of EEG Datasets
Datasets Rate Dim Len #Samples TaskEmotivDREAMER
[47] 128Hz 14 2s 77,910 Emotion Detection
STEW [48] 128Hz 14 2s 26,136 Mental workload Classification
Crowdsourced [23] 128Hz 14 2s 12,296 Eyes open/close Detection
Driver Distraction 128Hz 14 2s 66,197 Driver Distraction DetectionT
empleT
UAB [44] 256Hz 16 10s 409,455 Abnormal EEG Classification
TUEV [45] 256Hz 16 5s 112,464 Event Detection
4 EXPERIMENTAL RESULTS
4.1 Datasets
We employed two distinct dataset types, totaling six datasets, to
evaluate the effectiveness of our EEG2Rep model: i) Four datasets
were obtained using different types of Emotiv headsets(14-channel
wireless headsets capable of data collection in real-world scenarios).
Three of these datasets are publicly available. The fourth dataset,
named â€œDriver Distractionâ€ is a private dataset provided by Emotiv,
and collected using an older version of the headset2. Reporting
results on this private dataset along with other public ones allows
us to assess our modelâ€™s effectiveness across different Emotiv head-
set types, each producing specific types of noise. ii) We expanded
our evaluation to another dataset type, the Temple University Hos-
pital (TUH) Corpus [ 44,45], collected in a different setting (in a
laboratory setting using 24-36 channel braincaps). TUH is one of
the largest open EEG data repositories, featuring diverse devices
with varying channel numbers, all collected in clinical settings.
An overview of the datasets is available in Table 1, and additional
descriptions, including details on data preprocessing, are provided
in Appendix A.
To assess our modelâ€™s performance, we partitioned the Emo-
tiv datasets into subject-wise train/validation/test sets. This setup
poses a challenge for models to learn generalized patterns, given the
inter-subject variability. As for TUAB and TUEV, the training and
test separation is inherent in the dataset. Additionally, we further
split TUEV and TUAB training sets into 20% validation and 80%
training subject-wise.
4.2 Competitors and Implementation Setup
We conducted extensive comparisons against five state-of-the-art
methods for EEG representation learning. These methods include
invariance-based approaches TS-TCC [ 15], TF-C [ 40], and BIOT [ 13]
as well as self-prediction methods BENDR [ 10] and MAEEG [ 12].
To ensure a fair evaluation, we used publicly available code for the
baseline methods. All experiments utilized the PyTorch framework
on a system featuring a single Nvidia A5000 GPU (24GB). Model
and hyperparameter selection relied on the validation set. Table 2
and Table 3 present five sets of results with varied random seeds,
2https://www.emotiv.com/epoc/
5549EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Performance of EEG2Rep in comparison to the competitors in a Linear Probing setting.
Models DREAMER Crowdsourced STEW DriverDistraction TUAB TUEV
Acc AUROC Acc AUROC Acc AUROC Acc AUROC Acc AUROC B-Acc W-F1
BENDR [10] 51.48Â±1.87 51.68Â±1.98 70.46Â±4.14 70.56Â±4.32 63.03Â±1.07 63.03Â±1.07 68.40Â±3.08 55.21Â±3.17 72.78Â±4.17 79.87Â±4.11 37.38Â±3.11 61.31Â±3.18
MAEEG [12] 54.24Â±2.04 53.98Â±2.68 75.21Â±2.11 75.01Â±2.01 67.99Â±1.86 68.58Â±1.88 68.37Â±2.60 55.29Â±2.20 72.62Â±3.99 79.75Â±4.07 37.23Â±2.99 61.38Â±3.08
TS-TCC [15] 53.60Â±2.68 53.87Â±2.28 77.75Â±2.94 77.83Â±2.11 64.54Â±1.62 64.64Â±1.73 76.36Â±3.48 56.27Â±2.88 74.39Â±3.06 81.02Â±2.97 35.98Â±2.85 60.67Â±2.98
TF-C [40] 52.52Â±1.88 52.26Â±2.08 64.82Â±7.23 65.32Â±6.12 58.84Â±2.36 58.69Â±2.43 64.85Â±3.89 53.87Â±4.02 69.33Â±5.78 75.75Â±3.75 30.12Â±4.06 56.23Â±4.05
BIOT [13] 53.35Â±2.41 53.42Â±1.91 76.23Â±4.56 76.33Â±4.12 67.54Â±2.08 67.69Â±3.60 63.93Â±1.28 63.33Â±1.25 75.11Â±2.79 82.92Â±2.01 40.02Â±1.87 65.98Â±2.01
EEG2Rep 58.45Â±1.82 55.19Â±1.95 81.66Â±2.93 81.67Â±2.65 69.04Â±1.04 69.10Â±1.23 76.88Â±2.55 65.59Â±2.43 76.55Â±3.33 83.24Â±3.25 43.25Â±3.12 69.95Â±3.21
reporting mean and standard deviation values. We adhere to the
original VICReg [ 36] for setting the hyperparameters ğœ†,ğœ‡, andğ›¾in
the total loss. For further details on the evaluation metrics, refer to
the Appendix B.
4.3 Linear Probing
Table 2 presents the average performance of EEG2Rep along with
other state-of-the-art methods over five runs. For each dataset, the
number in bold indicates the highest accuracy achieved, while the
number underlined represents the second best (This formatting is
consistent across all tables presented in this paper). The results
presented in Table 2 indicate that our model, EEG2Rep, achieves
the highest average performance on all EEG tasks.
The DREAMER and STEW datasets exhibit high inter-subject
variance, stemming partly from the limited number of patients
in the recordings and partly due to the complexity of the EEG
tasks. As observed in the results, self-prediction-based methods like
MAEEG and EEG2Rep tend to capture more general patterns that
can be applied across different subjects. In contrast, tasks such as
eyes open/eyes close in Crowdsourced dataset are easier to gen-
eralize among subjects due to the nature of the EEG task. TUAB
also experiences low inter-subject variance as it encompasses a sub-
stantial number of subjects (more than 1000 patients). The results
highlight that invariance-based methods like TS-TCC and BIOT
outperform BENDER and MAEEG in Crowdsourced and TUAB.
However, EEG2Rep while being a self-prediction technique, man-
ages to improve the semantic level of representations resulting in
the best average accuracy among all competitors.
4.4 Fine Tuning
Table 3 presents the average classification accuracy results across
different datasets, comparing the performance of EEG2Rep with
other pre-trained models that were initialized using their respective
pretext tasks. These results are consistent with the ones presented in
Table 2, and our EEG2Rep achieves the highest average performance
on all EEG tasks.
The comparison also includes another version of the EEG2Rep
model, initialized randomly and denoted as EEG2Rep (Random).
This is equivalent to supervised training. The table shows that using
pre-trained EEG2Rep leads to an average accuracy improvement
of 6% on average compared to EEG2Rep (Random). Significant en-
hancements are evident in specific datasets, particularly DREAMER,DriverDistraction, and TUEV. In DREAMER and DriverDistraction,
EEG2Rep demonstrates AUROC improvement of 5.81% and 2.26%,
respectively, when compared to random initialization. Similarly,
for TUEV, there is a 6.85% improvement in weighted F1, validat-
ing the effectiveness of incorporating informative context input in
self-supervised methods for enhanced learning and improved EEG
classification. Itâ€™s worth noting that in datasets with pronounced
subject invariance issues, such as DREAMER, STEW, Driverdis-
traction, and TUAB, the performance of the supervised models is
notably subpar. In some cases, these models even exhibit lower
performance compared to models pre-trained self-supervised and
utilized for classification through a linear layer. For instance, in
DREAMER, linear probed EEG2Rep achieves a 3.84% higher accu-
racy than its supervised counterpart.
4.5 Cross-Domain
We evaluated the performance of our EEG2Rep model in a cross-
domain setting, where the model is trained on one dataset and tested
on another. This approach assesses the modelâ€™s ability to generalize
across different types of EEG domains and tasks. We utilized two
Emotiv datasets for the target tasks: STEW and Crowdsourced, both
characterized by limited training samples, making cross-domain
evaluation particularly relevant.
As depicted in Table 4, pre-training the model with datasets
like DREAMER led to performance improvements for the STEW
dataset, where the task is mental workload classification. This im-
provement can be attributed to the contextual alignment between
activities recorded in the DREAMER and STEW datasets. Inter-
estingly, even though the DriverDistraction dataset is not directly
related to mental workload classification, the experimental envi-
ronment and certain distraction classes are similar to those in the
STEW dataset. This similarity allowed the EEG2Rep model to bene-
fit from pre-training on the DriverDistraction dataset, as reflected
in the performance.
For the Crowdsourced dataset, where the task is eye open/closed
classification, the model achieved robust performance when pre-
trained on all available Emotiv datasets. However, this performance
did not surpass that achieved through in-domain pre-training. This
outcome suggests that the alignment of brain activities across
datasets is critical for optimal performance and indicates the po-
tential need to utilize a larger number of pre-training datasets to
cover various subjects and tasks comprehensively.
5550KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Navid Mohammadi Foumani et al.
Table 3: Performance of EEG2Rep in comparison to the competitors in a Fine Tuning setting.
Models DREAMER Crowdsourced STEW DriverDistraction TUAB TUEV
Acc AUROC Acc AUROC Acc AUROC Acc AUROC Acc AUROC B-Acc W-F1
BENDR [10] 54.45Â±2.11 53.02Â±3.11 83.78Â±2.35 83.8Â±2.63 69.74Â±2.11 69.77Â±2.03 74.31Â±2.38 59.86Â±2.6 76.96Â±3.98 83.97Â±3.44 41.17Â±2.89 67.31Â±2.96
MAEEG [12] 53.63Â±2.61 52.08Â±2.36 86.75Â±3.50 86.21Â±3.41 72.46Â±3.67 72.5Â±3.22 74.58Â±2.16 60.79Â±2.72 77.56Â±3.56 86.56Â±3.33 41.23Â±3.65 67.38Â±3.69
TS-TCC [15] 58.16Â±2.11 55.05Â±1.79 89.22Â±1.22 89.22Â±1.22 71.00Â±2.98 71.03Â±3.02 74.21Â±2.68 60.33Â±2.66 79.66Â±2.99 87.02Â±2.68 40.98Â±2.55 68.67Â±2.89
TF-C [40] 52.82Â±1.66 52.86Â±1.85 82.93Â±4.02 82.90Â±4.32 68.65Â±1.12 68.70Â±1.75 65.39Â±4.12 58.75Â±3.98 72.33Â±5.64 78.48Â±3.85 40.12Â±3.66 66.23Â±3.85
BIOT [13] 53.45Â±2.01 53.53Â±2.13 87.95Â±3.52 87.78Â±3.09 69.88Â±2.15 70.11Â±2.57 74.34Â±3.57 61.21Â±4.36 79.21Â±2.15 87.42Â±2.01 46.02Â±1.68 69.98Â±1.99
EEG2Rep (Random) 54.61Â±2.22 53.61Â±2.09 91.19Â±1.18 91.22Â±1.23 70.26Â±1.59 70.49Â±1.86 72.95Â±2.95 59.5Â±3.17 77.85Â±3.14 84.91Â±3.07 44.25Â±3.01 68.95Â±2.89
EEG2Rep (Pre-Trained) 60.37Â±1.52 59.42Â±1.45 94.13Â±2.11 94.13Â±2.17 73.60Â±1.47 74.40Â±1.50 80.07Â±2.63 66.14Â±2.44 80.52Â±2.22 88.43Â±3.09 52.95Â±1.58 75.08Â±1.21
Table 4: EEG2Rep model performance in cross-domain set-
tings on STEW and Crowdsourced datasets.
ModelsSTEW Crowdsourced
ACC AUROC ACC AUROC
Random initialization 70.26Â±1.59 70.49Â±1.86 91.19Â±1.18 91.22Â±1.23
In-domain pre-trained 73.60Â±1.47 74.40Â±1.50 94.13Â±2.11 94.13Â±2.17
Pre-trained on DREAMER 73.75Â±1.95 74.95Â±2.77 93.91Â±1.78 93.86Â±1.80
Pre-trained on DriverDistraction 73.68Â±2.17 74.67Â±3.06 94.09Â±1.95 94.11Â±2.02
Pre-trained on All Emotiv 74.11Â±2.34 77.38Â±2.77 94.05Â±1.68 94.07Â±1.75
Table 5: Ablation Study of EEG2Rep Components
EEG2Rep Average Accuracy: 67.64
Masking Strategy
Block Masking 66.45 (â†“1.19)
Random Masking 60.19 (â†“7.45)
Targets
Input-Space Prediction 54.52 ( â†“13.12)
Loss Component
W/O Variance-Covariance 65.63 (â†“2)
4.6 Ablation Study
Masking Strategy. We compare our semantic subsequence pre-
serving (SSP) strategy with other random and block masking strate-
gies. For random masking, we adopt an approach similar to masking
autoencoders [ 17], where patches are shuffled randomly, and the
initial 50%of these patches are selected. In the case of block mask-
ing, we follow the MAEEG [ 12] approach, emphasizing the masking
of a continuous chunk. The results in Table 5 highlight the effective-
ness of semantic subsequence preservation in guiding our model
towards learning meaningful representations. The subpar perfor-
mance in the random masking strategy could be attributed to the
model primarily attempting to interpolate the masked values rather
than focusing on learning a semantic representation. With block
masking, the visible subsequence may have a shorter length than
the natural time scale of the brain, posing a significant challenge to
the reconstruction process.Masking Ratio. The masking ratio during self-supervised pre-
training is crucial as it determines both the difficulty of the self-
prediction task and the quality of the learned representations. There-
fore, identifying an optimal masking ratio is essential for effective
representation learning from EEG data. Following the recent study
on the effects of masking on representation learning [ 49], we pre-
train our model using five different mask rates [10%, 25%, 50%, 75%,
90%] with our SSP method and random masking to evaluate the
quality of representation learning. As shown in Table 6, several
interesting findings emerged from the results. We observed that
conservative masking (10%) leads to low performance, regardless of
the masking strategy. However, as the masking ratio increases, the
performance of the model improves, indicating that higher masking
can result in more high-level representation learning. Nevertheless,
performance degradation occurs with overly aggressive masking
(90%), suggesting that representation does not become monotoni-
cally more high-level with increasing masking aggressiveness. In
other words, overly aggressive masking also leads to low-level
representations, similar to conservative masking. Moderate and
high masking ratios yield the best results for both random and SSP
masking strategies.
As shown in Table 6, our SSP masking strategy consistently
achieves higher performance than random masking across various
masking ratios. A key distinction between our proposed masking
strategy in EEG2Rep and random masking lies in the arrangement
of visible patches. In random masking, there is a possibility that
visible patches are not consecutive, whereas in EEG2Rep, visible
patches (referred to as preserved patches) are ensured to remain con-
tiguous. Additionally, during the reconstruction of masked patches
near the boundary between masked and unmasked regions, the
model uses nearby visible patches to interpolate, thereby capturing
low-level information, which resonates with the empirical observa-
tions in [ 17,49]. Our proposed masking strategy is less susceptible
to such phenomena compared to random masking, as it ensures
that masked tokens remain contiguous. The high number of bound-
aries in random masking may result in less contextualized learning.
Our empirical results further confirm that our proposed method
yields higher performance compared to random masking, even with
aggressive and conservative masking.
Masking Blocks. In EEG2Rep, our focus is on preserving sub-
sequences to ensure information is retained in the masked input.
Figure 4 depicts the average accuracy of EEG2Rep relative to the
5551EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 6: Performance comparison between random masking and SSP across different datasets and masking ratios.
Mask RatioDREAMER Crowdsourced STEW DriverDistraction TUAB TUEV
Random SSP Random SSP Random SSP Random SSP Random SSP Random SSP
10% 52.11 53.02 72.13 76.69 63.13 68.14 64.02 70.41 64.50 72.36 33.01 40.37
25% 53.23 54.73 75.05 77.28 65.12 70.47 65.40 72.94 64.75 73.73 33.14 42.53
50% 52.63 59.89 76.69 81.24 64.97 69.52 64.12 76.12 68.12 76.55 34.59 44.23
75% 51.77 52.03 72.10 76.24 63.27 64.69 63.12 65.03 66.34 70.81 35.28 42.19
90% 51.51 51.95 70.77 73.44 62.05 63.70 62.06 64.91 66.34 70.12 34.14 38.72
Average 52.25 54.32 73.35 76.98 63.71 67.30 63.74 69.88 66.01 72.71 34.03 41.61
25% 50% 75%
Preserving Percentage (1 )
52.555.057.560.062.565.067.5Average Accuracy
Number of Blocks
 = 1
 = 3
 = 5
Figure 4: Effect of preserving percentage (1 âˆ’ğœŒ) on average
accuracy across all EEG datasets: A comparison of accuracy
variation across different numbers of blocks ( ğ›½), with error
bars indicating standard deviation.
number of masked blocks and the ratio of preserved subsequences
(inverse of masking). Optimal results are observed when preserving
50% of the input in 3 blocks and masking the other half. When
preserving only 75% of the input, it is more effective to mask a
single chunk rather than multiple chunks, as the model tends to
interpolate instead of learning semantic patterns in EEG data.
Input-Space Prediction. Table 5 shows a comparison of linear
probing performance when the loss is computed in input-space
versus representation space. We hypothesize that a key component
of EEG2Rep lies in computing the loss entirely in the representation
space, enabling the target encoder to generate abstract prediction
targets that eliminate irrelevant raw details. As evident from Table
5, predicting in input-space leads to a 13.12%degradation in linear
probing performance, highlighting the impact of noise and wide
range of amplitudes inherent to EEG data.
Loss Regularization. As discussed in Section 2.5, the self-supervised
loss function comprises two components: reconstruction and reg-
ularization. The main role of the reconstruction term is to ensure
similarity between the representations of masked and unmasked
versions of the same sample. In contrast, the regularization term
not only aids in preventing representation collapse but also en-
hances the representation learning task. As shown in Table 5, the
regularization term contributes to a 2% accuracy improvement on
average across all EEG tasks.
BENDR MAEEG TS-TCC TF-C BIOT EEG2REP01020304050607080Accuracy8.11
62.4513.43
61.587.14
70.694.87
60.4511.21
65.123.21
78.46Figure 5: Model Robustness Comparison: Assessing model
performance by introducing Gaussian noise, DC-shift, and
amplitude changes to Crowdsourced data.
Robustness to Noise. In this experiment, our goal is to assess the
robustness of our model to noise, recognizing the high signal-to-
noise ratio as a significant challenge in learning representations
from EEG data. To do so we introduce Gaussian noise, time shift, DC-
shift, and amplitude scaling, as recommended by neurologists [ 16]
to the Crowdsourced dataset. Such transformation do not change
the interpretation of the EEG according to [ 16]. The details of noise
types are described in appendix B.2. As depicted in Figure 5, our
model demonstrates superior robustness to these noises in the
data, while raw input-based models like BENDER and MAEEG
exhibit the most degradation in accuracy. TS-TCC and TF-C show
commendable robustness to these noises, possibly due to their
utilization of similar augmentations for their invariance losses.
5 CONCLUSION
EEG2Rep emerges as a pioneering self-supervised approach tailored
to address the inherent challenges of EEG data representation learn-
ing, including low signal-to-noise ratio. By innovatively predicting
masked inputs in the latent representation space and employing a
novel semantic subsequence preserving method, EEG2Rep facili-
tates the generation of rich semantic representations. Our exten-
sive experiments across six diverse EEG tasks demonstrate that
EEG2Rep not only significantly surpasses state-of-the-art methods
but also highlights remarkable robustness to noise. We found that
preserving 50% of EEG recordings optimizes accuracy across all
tasks. In the future, we will explore the alignment of this finding
with the natural time scale of the brain on different tasks.
5552KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Navid Mohammadi Foumani et al.
REFERENCES
[1]M. Teplan et al., â€œFundamentals of eeg measurement,â€ Measurement science review,
vol. 2, no. 2, pp. 1â€“11, 2002.
[2]E. Niedermeyer and F. L. da Silva, Electroencephalography: basic principles, clinical
applications, and related fields. Lippincott Williams & Wilkins, 2005.
[3]F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rakotomamonjy,
and F. Yger, â€œA review of classification algorithms for eeg-based brainâ€“computer
interfaces: a 10 year update, â€ Journal of neural engineering , vol. 15, no. 3, p. 031005,
2018.
[4]Y. Roy, H. Banville, I. Albuquerque, A. Gramfort, T. H. Falk, and J. Faubert, â€œDeep
learning-based electroencephalography analysis: a systematic review, â€ Journal of
neural engineering, vol. 16, no. 5, p. 051001, 2019.
[5]J. Chen, Y. Yang, T. Yu, Y. Fan, X. Mo, and C. Yang, â€œBrainnet: Epileptic wave
detection from seeg with hierarchical graph diffusion learning,â€ in Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,
2022, pp. 2741â€“2751.
[6]M. RakoviÄ‡, Y. Li, N. M. Foumani, M. Salehi, L. Kuhlmann, G. Mackellar,
R. Martinez-Maldonado, G. Haffari, Z. Swiecki, X. Li et al., â€œMeasuring affective
and motivational states as conditions for cognitive and metacognitive process-
ing in self-regulated learning,â€ in Proceedings of the 14th Learning Analytics and
Knowledge Conference, 2024, pp. 701â€“712.
[7]A. Craik, Y. He, and J. L. Contreras-Vidal, â€œDeep learning for electroencephalo-
gram (eeg) classification tasks: a review,â€ Journal of neural engineering, vol. 16,
no. 3, p. 031001, 2019.
[8]M.-P. Hosseini, A. Hosseini, and K. Ahi, â€œA review on machine learning for
eeg signal processing in bioengineering,â€ IEEE reviews in biomedical engineering,
vol. 14, pp. 204â€“218, 2020.
[9]W. Weng, Y. Gu, S. Guo, Y. Ma, Z. Yang, Y. Liu, and Y. Chen, â€œSelf-
supervised learning for electroencephalogram: A systematic survey,â€ arXiv
preprint arXiv:2401.05446, 2024.
[10] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, â€œBendr: Using transformers and a
contrastive self-supervised learning task to learn from massive amounts of eeg
data,â€ Frontiers in Human Neuroscience, vol. 15, 2021.
[11] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,
K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball, â€œDeep
learning with convolutional neural networks for eeg decoding and visualization,â€
Human brain mapping, vol. 38, no. 11, pp. 5391â€“5420, 2017.
[12] H.-Y. S. Chien, H. Goh, C. M. Sandino, and J. Y. Cheng, â€œMaeeg: Masked auto-
encoder for eeg representation learning,â€ in NeurIPS Workshop, 2022.
[13] C. Yang, M. B. Westover, and J. Sun, â€œBiot: Biosignal transformer for cross-data
learning in the wild,â€ in Thirty-seventh Conference on Neural Information Process-
ing Systems, 2023.
[14] N. Mohammadi Foumani, L. Miller, C. W. Tan, G. I. Webb, G. Forestier, and
M. Salehi, â€œDeep learning for time series classification and extrinsic regression:
A current survey,â€ ACM Computing Surveys, vol. 56, no. 9, pp. 1â€“45, 2024.
[15] E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, and C. Guan, â€œTime-series
representation learning via temporal and contextual contrasting,â€ in Proceedings
of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21,
2021, pp. 2352â€“2359.
[16] P. M. Mostafa Neo Mohsenvand, Mohammad Rasool Izadi, â€œContrastive repre-
sentation learning for electroencephalogram classification,â€ in Machine Learning
for Health Workshop, ML4H@NeurIPS 2020, Virtual Event, 11 December 2020, ser.
Proceedings of Machine Learning Research, vol. 136, 2020, pp. 238â€“253.
[17] K. He, X. Chen, S. Xie, Y. Li, P. DollÃ¡r, and R. Girshick, â€œMasked autoencoders are
scalable vision learners,â€ in Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 2022, pp. 16 000â€“16 009.
[18] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, â€œA
transformer-based framework for multivariate time series representation learn-
ing,â€ in 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,
2021, pp. 2114â€“2124.
[19] H. Ling, Y. Luyuan, L. Xinxin, and D. Bingliang, â€œStaging study of single-channel
sleep eeg signals based on data augmentation,â€ Frontiers in Public Health, vol. 10,
p. 1038742, 2022.
[20] N. M. Foumani, C. W. Tan, G. I. Webb, and M. Salehi, â€œSeries2vec: Similarity-
based self-supervised representation learning for time series classification,â€ arXiv
preprint arXiv:2312.03998, 2023.
[21] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, â€œData2vec: A gen-
eral framework for self-supervised learning in speech, vision and language,â€ in
International Conference on Machine Learning. PMLR, 2022, pp. 1298â€“1312.
[22] A. Baevski, A. Babu, W.-N. Hsu, and M. Auli, â€œEfficient self-supervised learning
with contextualized target representations for vision, speech and language,â€ in
International Conference on Machine Learning. PMLR, 2023, pp. 1416â€“1429.
[23] N. S. Williams, W. King, G. Mackellar, R. Randeniya, A. McCormick, and N. A.
Badcock, â€œCrowdsourced eeg experiments: A proof of concept for remote eeg
acquisition using emotivpro builder and emotivlabs,â€ Heliyon, vol. 9, no. 8, 2023.
[24] M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun,
and N. Ballas, â€œSelf-supervised learning from images with a joint-embeddingpredictive architecture,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 15 619â€“15 629.
[25] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, â€œImage BERT
pre-training with online tokenizer,â€ in International Conference on Learning Rep-
resentations, 2022.
[26] S. Chambon, M. N. Galtier, P. J. Arnal, G. Wainrib, and A. Gramfort, â€œA deep
learning architecture for temporal sleep stage classification using multivariate and
multimodal time series,â€ IEEE Transactions on Neural Systems and Rehabilitation
Engineering, vol. 26, no. 4, pp. 758â€“769, 2018.
[27] S. N. M. Foumani, C. W. Tan, and M. Salehi, â€œDisjoint-cnn for multivariate time
series classification,â€ in 2021 International Conference on Data Mining Workshops
(ICDMW). IEEE, 2021, pp. 760â€“769.
[28] N. M. Foumani, C. W. Tan, G. I. Webb, and M. Salehi, â€œImproving position encoding
of transformers for multivariate time series classification,â€ Data Mining and
Knowledge Discovery, vol. 38, no. 1, pp. 22â€“48, 2024.
[29] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre-training of deep bidi-
rectional transformers for language understanding,â€ in Proceedings of NAACL-HLT
2019, vol. 1. Stroudsburg, PA, USA: Association for Computational Linguistics,
2019, pp. 4171â€“4186.
[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser,
and I. Polosukhin, â€œAttention is all you need,â€ Advances in neural information
processing systems, vol. 30, 2017.
[31] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, â€œImage bert
pre-training with online tokenizer,â€ in International Conference on Learning Rep-
resentations, 2021.
[32] L. Jing, P. Vincent, Y. LeCun, and Y. Tian, â€œUnderstanding dimensional collapse
in contrastive self-supervised learning,â€ in International Conference on Learning
Representations, 2021.
[33] I. Ben-Shaul, R. Shwartz-Ziv, T. Galanti, S. Dekel, and Y. LeCun, â€œReverse engi-
neering self-supervised learning,â€ arXiv preprint arXiv:2305.15614, 2023.
[34] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, â€œA simple framework for
contrastive learning of visual representations,â€ in International conference on
machine learning. PMLR, 2020, pp. 1597â€“1607.
[35] J.-B. Grill, F. Strub, F. AltchÃ©, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch,
B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al., â€œBootstrap your own latent-
a new approach to self-supervised learning,â€ Advances in neural information
processing systems, vol. 33, pp. 21 271â€“21 284, 2020.
[36] A. Bardes, J. Ponce, and Y. Lecun, â€œVicreg: Variance-invariance-covariance regu-
larization for self-supervised learning,â€ in ICLR 2022-International Conference on
Learning Representations, 2022.
[37] D. P. Subha, P. K. Joseph, R. Acharya U, and C. M. Lim, â€œEeg signal analysis: a
survey,â€ Journal of medical systems, vol. 34, pp. 195â€“212, 2010.
[38] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, â€œA simple framework for
contrastive learning of visual representations,â€ in International conference on
machine learning. PMLR, 2020, pp. 1597â€“1607.
[39] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, â€œAlbert: A
lite bert for self-supervised learning of language representations,â€ arXiv preprint
arXiv:1909.11942, 2019.
[40] X. Zhang, Z. Zhao, T. Tsiligkaridis, and M. Zitnik, â€œSelf-supervised contrastive
pre-training for time series via time-frequency consistency,â€ in Proceedings of
Neural Information Processing Systems, NeurIPS, 2022.
[41] H. Kan, J. Yu, J. Huang, Z. Liu, H. Wang, and H. Zhou, â€œSelf-supervised group meio-
sis contrastive learning for eeg-based emotion recognition,â€ Applied Intelligence,
vol. 53, no. 22, p. 27207â€“27225, sep 2023.
[42] X. Shen, X. Liu, X. Hu, D. Zhang, and S. Song, â€œContrastive learning of subject-
invariant eeg representations for cross-subject emotion recognition,â€ IEEE Trans-
actions on Affective Computing, vol. 14, no. 3, pp. 2496â€“2511, 2023.
[43] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, â€œwav2vec 2.0: A framework
for self-supervised learning of speech representations,â€ in Advances in Neural
Information Processing Systems, vol. 33, 2020, pp. 12 449â€“12 460.
[44] S. Lopez, G. Suarez, D. Jungreis, I. Obeid, and J. Picone, â€œAutomated identification
of abnormal adult eegs,â€ in 2015 IEEE signal processing in medicine and biology
symposium (SPMB). IEEE, 2015, pp. 1â€“5.
[45] A. Harati, M. Golmohammadi, S. Lopez, I. Obeid, and J. Picone, â€œImproved eeg
event classification using differential energy,â€ in 2015 IEEE Signal Processing in
Medicine and Biology Symposium (SPMB). IEEE, 2015, pp. 1â€“4.
[46] R. Li, Y. Wang, W.-L. Zheng, and B.-L. Lu, â€œA multi-view spectral-spatial-temporal
masked autoencoder for decoding emotions with self-supervised learning,â€ in
Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp.
6â€“14.
[47] S. Katsigiannis and N. Ramzan, â€œDreamer: A database for emotion recognition
through eeg and ecg signals from wireless low-cost off-the-shelf devices,â€ IEEE
journal of biomedical and health informatics, vol. 22, no. 1, pp. 98â€“107, 2017.
[48] W. Lim, O. Sourina, and L. P. Wang, â€œStew: Simultaneous task eeg workload data
set,â€ IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 26,
no. 11, pp. 2106â€“2114, 2018.
[49] L. Kong, M. Q. Ma, G. Chen, E. P. Xing, Y. Chi, L.-P. Morency, and K. Zhang,
â€œUnderstanding masked autoencoders via hierarchical latent variable models,â€ in
5553EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2023, pp. 7918â€“7928.
[50] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€ arXiv
preprint arXiv:1412.6980, 2014.
A DATASET OVERVIEW AND PROCESSING
A.1 Emotiv
We applied a bandpass filter to all Emotiv datasets and windowed
them into segments consisting of 256 time steps, each equivalent
to 2 seconds of recording.
DREAMER
DREAMER is a multimodal database featuring electroencephalo-
gram (EEG) and electrocardiogram (ECG) signals recorded during
affect elicitation using audio-visual stimuli [ 47] using a 14-channel
Emotiv EPOC headset. The dataset includes signals from 23 par-
ticipants, accompanied by their self-assessments of affective states
after each stimulus in terms of valence, arousal, and dominance.
For our classification task, we specifically utilize the arousal labels.
The DREAMER dataset can be obtained from here3, and we employ
the Torcheeg toolkit for preprocessing, which involves cropping
and applying low-pass and high-pass filters4. It is important to note
that, for our analysis, we solely focus on EEG data, and the ECG
signals are excluded from consideration.
Crowdsourced
Crowdsourced EEG data was collected while participants were
engaged in a resting state task, involving periods with eyes open
and eyes closed, each lasting 2 minutes. Out of 60 participants, only
13 successfully completed both conditions using 14-channel EPOC+,
EPOC X, and EPOC devices. The data was initially recorded at 2048
Hz and later downsampled to 128 Hz. Raw EEG data for these 13
participants, along with preprocessing, analysis, and visualization
scripts, are openly available on the Open Science Framework (OSF)5.
Simultaneous Task EEG Workload (STEW)
STEW dataset comprises raw EEG recordings from 48 participants
using a 14-channel Emotiv EPOC headset involved in a multi-
tasking workload experiment utilizing the SIMKAP multitasking
test [ 48]. Additionally, the subjectsâ€™ baseline brain activity at rest
was recorded before the test. The data was captured using the
Emotiv Epoc device with a sampling frequency of 128Hz and 14
channels, resulting in 2.5 minutes of EEG recording for each case.
Participants were instructed to assess their perceived mental work-
load after each stage using a rating scale ranging from 1 to 9, and
these ratings are available in a separate file. Moreover, this dataset
includes binary class labels, considering a workload rating of more
than 4 as high and otherwise as low. We utilize these labels for our
specific problem. STEW can be accessed upon request through the
IEEE DataPort6.
DriverDistraction
3https://zenodo.org/records/546113
4https://torcheeg.readthedocs.io/en/v1.1.0/torcheeg.datasets.html
5https://osf.io/9bvgh
6https://ieee-dataport.org/open-access/stew-simultaneous-task-eeg-workload-
datasetThe data were gathered by recording the EEG brain signals of 17
participants, each using a driving simulator for approximately 40
minutes. The participants performed various distraction activities
while they were driving. These can be grouped into the following
high-level activities: 1. Talking to a passenger 2. Using a phone
(texting and calling) 3. Problem solving. The EEG data were sam-
pled at a frequency of 128Hz, through 14 channels on the Emotiv
Epoc EEG headset. The sampling result is a multivariate (14 input
variables) time-series dataset containing approximately 5.5 million
records. The data were then manually annotated with the activity
being performed at each time point.
A.2 Temple University Hospital (TUH) EEG
Corpus
TUH Abnormal EEG Corpus (TUAB)
The TUH Abnormal EEG Corpus (TUAB) is a subset of the Temple
University Hospital (TUH) EEG Corpus, which is one of the largest
publicly available collections of clinical EEG data. The TUAB specif-
ically focuses on EEG recordings labeled as abnormal, making it
a valuable resource for studies on neurological disorders, brain
function anomalies, and the development of diagnostic tools [44].
TUH EEG Events (TUEV)
The TUH EEG Events Corpus (TUEV) contains annotations of EEG
segments classified into six different categories: spike and sharp
wave, generalized periodic epileptiform discharges, periodic later-
alized epileptiform discharges, eye movement, artifact, and back-
ground [45].
Acquisition and Preprocessing
The TUH Abnormal EEG Corpus (TUAB) [ 44] and TUH EEG Events
(TUEV) [ 45] can be accessed upon request through the Temple Uni-
versity Electroencephalography (EEG) Resources7. We processed
both datasets to adhere to the 16 EEG montages [ 13], following the
10-20 international system, are as follows: "FP1-F7", "F7-T7", "T7-
P7", "P7-O1", "FP2-F8", "F8-T8", "T8-P8", "P8-O2", "FP1-F3", "F3-C3",
"C3-P3", "P3-O1", "FP2-F4", "F4-C4", "C4-P4", and "P4-O2".
B DETAILS OF EXPERIMENTAL SETTINGS
B.1 Parameter Setting
In our experiment, the EEG2Rep model employed one depth-wise
and two spatial-wise convolution layers for input embedding, each
with 16 filters. During training, a batch size of 256 was used, and
we utilized the Adam optimization algorithm [ 50]. To prevent over-
fitting, we implemented an early stopping method based on the
validation loss. The model was pre-trained for 500 epochs, after
which logistic regression was applied to the representations for
linear probing. Similar to the transformer-based model for multivari-
ate time series classification (TST) [ 18] and the default transformer
block [ 30], in our experiments, we employed eight attention heads
in both the context and target encoder to capture diverse features
from the EEG. The transformer encoding dimension was set to
ğ‘‘ğ‘’=16, and the feed-forward network (FFN) in the transformer
block expanded the input size by a factor of 4 before projecting it
7https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml
5554KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Navid Mohammadi Foumani et al.
back to its original size. For the learning rate, we start with 1Ã—10âˆ’3
and use a cosine learning rate scheduler to adjust it over time [ 21].
B.2 Noise Types for Model Robustness
Table 7 provides the details of the noise types we added to the
Crowdsourced dataset to test the robustness of EEG2Rep and bench-
mark models to noise.
Table 7: Noise Types Details
Transformation Min Max
Amplitude Scale 0.5 2
Time Shift -50 50
DC Shift -10 10
Additive Gaussian Noise 0 0.2B.3 Evaluation Metrics
(Balanced) Accuracy is defined as the average recall obtained for
each class. We use the term ACC for binary classification and B-ACC
for multi-class classification. AUROC represents the area under
the ROC curve, condensing the ROC curve into a single number
that measures the modelâ€™s performance across multiple thresholds.
It is employed for binary classification. Weighted F1 is utilized for
multi-class classification in this paper. It is a weighted average of
individual F1 scores for each class, with each score weighted by the
number of samples in the corresponding class.
5555