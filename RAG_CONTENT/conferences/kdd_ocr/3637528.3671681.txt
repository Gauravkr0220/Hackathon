ITPNet: Towards Instantaneous Trajectory Prediction for
Autonomous Driving
Rongqing Li
Beijing Institute of Technology
Beijing, China
lirongqing99@gmail.comChangsheng Liâˆ—
Beijing Institute of Technology
Beijing, China
lcs@bit.edu.cnYuhang Li
Beijing Institute of Technology
Beijing, China
596983629@qq.com
Hanjie Li
Beijing Institute of Technology
Beijing, China
lihanjieyouxiang@sina.comYi Chen
Beijing Institute of Technology
Beijing, China
2812232328@qq.comYe Yuan
Beijing Institute of Technology
Beijing, China
yuan-ye@bit.edu.cn
Guoren Wang
Beijing Institute of Technology
Beijing, China
wanggrbit@126.com
Abstract
Trajectory prediction of moving traffic agents is crucial for the
safety of autonomous vehicles, whereas previous approaches usu-
ally rely on sufficiently long-observed trajectory (e.g., 2 seconds)
to predict the future trajectory of the agents. However, in many
real-world scenarios, it is not realistic to collect adequate observed
locations for moving agents, leading to the collapse of most predic-
tion models. For instance, when a moving car suddenly appears and
is very close to an autonomous vehicle because of the obstruction,
it is quite necessary for the autonomous vehicle to quickly and
accurately predict the future trajectories of the car with limited
observed trajectory locations. In light of this, we focus on inves-
tigating the task of instantaneous trajectory prediction, i.e., two
observed locations are available during inference. To this end, we
put forward a general and plug-and-play instantaneous trajectory
prediction approach, called ITPNet. Specifically, we propose a back-
ward forecasting mechanism to reversely predict the latent feature
representations of unobserved historical trajectories of the agent
based on its two observed locations and then leverage them as
complementary information for future trajectory prediction. Mean-
while, due to the inevitable existence of noise and redundancy in
the predicted latent feature representations, we further devise a
Noise Redundancy Reduction Former (NRRFormer) module, which
aims to filter out noise and redundancy from unobserved trajecto-
ries and integrate the filtered features and observed features into a
compact query representation for future trajectory predictions. In
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671681essence, ITPNet can be naturally compatible with existing trajec-
tory prediction models, enabling them to gracefully handle the case
of instantaneous trajectory prediction. Extensive experiments on
the Argoverse and nuScenes datasets demonstrate ITPNet outper-
forms the baselines by a large margin and shows its efficacy with
different trajectory prediction models.
CCS Concepts
â€¢Applied computing â†’Forecasting; â€¢Computing method-
ologiesâ†’Machine learning; Spatial and physical reasoning.
Keywords
Instantaneous Trajectory Prediction;Backward Forecasting;Noise
and Redundancy Reduction
ACM Reference Format:
Rongqing Li, Changsheng Li, Yuhang Li, Hanjie Li, Yi Chen, Ye Yuan,
and Guoren Wang. 2024. ITPNet: Towards Instantaneous Trajectory Pre-
diction for Autonomous Driving. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671681
1 Introduction
Predicting the future trajectories of dynamic traffic agents is a
critical task for autonomous driving, which can be beneficial to the
downstream planning module of autonomous vehicles. In recent
years, many trajectory prediction methods have been proposed in
computer vision and machine learning communities [ 11,19,34,39,
45,47,48,52,54]. Among these methods, they usually need to collect
sufficiently long observed trajectories (typically, 2 to 3 seconds)
of an agent, in order to accurately predict its future trajectories.
Recent advances have shown promising performance in trajectory
prediction by learning from these adequate observations.
However,in real-world self-driving scenarios, it is often difficult
to accurately predict trajectories due to the limited availability of
observed locations. For instance, due to the obstruction, a moving
 
1643
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rongqing Li et al.
train : 20
 test : 20train : 2
 test : 2train : 20
 test : 20.00.51.01.52.02.53.03.54.0
minADE@6
minFDE@6
(a)
ErrorRedundancyUnobservedPredictionUnobservedGTObservedFutureGTFuturePrediction (b)
Figure 1: (a) Results of HiVT [ 53] in terms of minADE@6 and minFDE@6 on the validation set of Argoverse [ 10] with different
observed locations as inputs during training and testing. The value in the horizontal axis denotes the number of observed
locations. (b) Future predictions (shown in green) when utilizing different lengths of predicted unobserved trajectory locations.
The observed trajectories are shown in orange, the predicted unobserved trajectories are shown in brown, the ground-truth
unobserved trajectories are shown in blue, and the ground-truth future trajectories are shown in red.
car might suddenly appear and be very close to the autonomous
vehicle. At this moment, the autonomous vehicle does not have
enough time to collect adequate observed locations of the car to
accurately predict the vehicleâ€™s future trajectories. Such a case will
cause the collapse of the aforementioned prediction models due to
the lack of information. To verify this point, we perform a typical
trajectory prediction method, HiVT [ 53], with different settings
on the Argoverse dataset [ 10]. As shown in the left part of Figure
1(a), if we use 20 observed locations as the inputs of the prediction
model during both training and test phases as in [ 53], the prediction
results are 0.698 and 1.053 in terms of minADE@6 and minFDE@6,
respectively. However, if we set only 2 observed locations as the
inputs of the model during testing, the model will degrade sharply,
no matter if the number of observed locations is 2 or 20 during the
training phase. Thus, it is essential to study the trajectory prediction
task, when observed locations are very limited.
In light of this, we focus on studying the task of instantaneously
predicting future trajectories of moving agents, under the assump-
tion of only 2 trajectory locations available. Recently, The work in
[43] proposes a trajectory prediction method based on momentary
observations. However, this method mainly focuses on the trajec-
tory prediction of pedestrians, which has not been explored for
other moving agents. In addition, the input of their model is the
RGB image which usually contains abundant context and seman-
tic information. Thus, it is much easier for the model to predict
future trajectories using RGB images, compared to only several
discrete trajectory locations. Moreover, [ 36] design a knowledge
distillation mechanism based on limited observed locations and
achieves promising results. Since the method needs to pre-train
a teacher model, and learns a student model distilling knowledge
from the teacher model, which largely increases the computational
complexities.
To this end, we propose a general and principled approach, called
ITPNet, for instantaneous trajectory prediction by only two ob-
served trajectory locations. Specifically, ITPNet aims to train a
predictor to backwardly predict the latent feature representations
of unobserved historical trajectory locations of the agent based onits two observed trajectory locations. The additional information
contained in the predicted unobserved trajectory features assists
observed trajectory features in better predicting future trajectories.
Nevertheless, we find that as we increase the number of backwardly
predicted unobserved trajectory locations, the modelâ€™s performance
initially improves but subsequently deteriorates (This is verified
in Table 3 of Experiment section). We analyze two primary factors
that impede the utilization of more unobserved trajectory features:
One is the noise brought by inaccurate prediction of the unobserved
trajectory features. The other is a negative impact on the trajectory
prediction due to the intrinsic redundant information. Letâ€™s con-
sider a scenario where a vehicle travels straightly for a while and
then suddenly executes a turn. In such a case, a longer historical
trajectory may erroneously boost the modelâ€™s confidence in the
vehicle continuing straight in the future, as depicted in the upper
portion of Figure 1(b). Conversely, a shorter unobserved histori-
cal trajectory with less redundancy tends to yield more accurate
predictions because it maintains lower confidence in the vehicleâ€™s
persistence in a straight trajectory and, instead, maintains higher
confidence in the vehicleâ€™s persistence in a turning trajectory, as
shown in the lower portion of Figure 1(b). Thus, how to remove
noisy and redundant information from the predicted features of
the unobserved trajectories becomes the key to success in instanta-
neous trajectory prediction.
In view of this, we devise a Noise Redundancy Reduction For-
mer (NRRFormer) module and integrate it into our framework.
NRRFormer can filter out noise and redundancy from a sequence
of predicted unobserved latent features, and effectively fuse the
filtered unobserved latent features with the observed features by a
compact query embedding to acquire the most useful information
for future trajectory prediction. It is worth noting that our ITPNet
is actually plug-and-play, and is compatible with existing trajectory
prediction models, making them the kinds that can gracefully deal
with the instantaneous trajectory prediction problem.
Our main contributions are summarized as: 1) We design a back-
ward forecasting mechanism to reconstruct unobserved historical
 
1644ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
trajectory information for instantaneous trajectory prediction, mit-
igating the issue of lack of information due to only two observed
locations. 2) We devise a Noise Redundancy Reduction Former (NR-
RFormer), which can remove noise and redundancy among the
predicted unobserved features to further improve the prediction
performance. 3) We perform extensive experiments on two widely
used benchmark datasets, and demonstrate ITPNet can outperform
baselines in a large margin. Moreover, we show the efficacy of
ITPNet, combined with different trajectory prediction models.
2 Related Works
2.1 Trajectory Prediction with Sufficient
Observation
In recent years, many trajectory prediction approaches have been
proposed [ 3,4,8,12,13,15,16,31,44]. In the early stage of tra-
jectory prediction, studies such as [ 1,20] usually rely solely on
observation points and adopt simple social pooling methods to cap-
ture interactions between agents. To capture the map information,
including occupancy or semantic information, [ 6,35,40] propose
to use Convolutional Neural Networks (CNN) to encode map im-
ages. In addition, [ 14,28] incorporate the information of lanes and
traffic lights on the map in the form of vectors. Recently, numer-
ous methods have been proposed to fully exploit the interaction
information between nearby agents, including implicit modeling
by graph neural networks [ 8,9,27,42] and attention mechanisms
[26,30,37,38], and explicit modeling [ 44]. To handle the uncer-
tainty of road agents, researchers propose to generate multi-modal
trajectories using various approaches, including GAN-based meth-
ods [ 20,23,41], VAE-based methods [ 24,25], flow-based methods
[29,50], and diffusion models [ 19,22,33]. Among them, one typi-
cal approach is to establish a mapping between future trajectories
and latent variables, producing multiple plausible trajectories by
sampling the latent variable. In addition, goal-based methods have
become popular recently [ 2,18,32,46,49,51], which first gener-
ates multi-modal goals by sampling [ 51] or learning [ 46], and then
predict future trajectories conditioned on the goals.
Although these methods have shown promising performance in
trajectory prediction, they usually learn depending on sufficiently
long-observed locations. As aforementioned, these methods de-
grade severely or even collapse when the number of observed loca-
tions is limited. Different from these works, we attempt to address
the task of instantaneously predicting the future trajectories of mov-
ing agents, under the condition that only two trajectory locations
are observable.
2.2 Trajectory Prediction with Instantaneous
Observation
Predicting the future trajectories of a moving agent by its limited
observed locations remains a challenging problem. Recently, [ 43]
proposes an approach to integrate the velocity of agents, social and
scene contexts information, and designs a momentary observation
feature Extractor (MOE) for pedestrian trajectory prediction. The
input of MOE contains image frames from videos which usually
contain abundant semantic information. Thus, it is much easier
to predict future trajectories using image frames than that usingseveral discrete trajectory points. Moreover, since this method is
mainly designed for predicting trajectories for pedestrians, what
is the performance on other moving agents, e.g., cars, is worth
to be further verified. [ 36] proposes a knowledge distillation ap-
proach using few observations as input, with the goal of lowering
the influence of noise introduced by the machine perception side
(i.e., incorrect detection and tracking). As we know, knowledge
distillation-based approaches generally need to pre-train a teacher
model, and then distill knowledge from the teacher model to help
the student model learn, which makes this kind of method compu-
tationally expensive. [ 5] designed a unified framework for tackling
multiple tasks including instantaneous pedestrian trajectory predic-
tion, where they constructed a universal singular space to share the
information between long and short trajectory points. Differently,
our focus lies on the task of instantaneous trajectory prediction,
achieved by predicting unobserved historical trajectories, without
using any information of long trajectory points.
3 Proposed Method
3.1 Problem Definition
We denote a sequence of observed trajectory locations for a tar-
get vehicle as Xğ‘œğ‘ğ‘ ={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘‡}, whereğ‘¥ğ‘–âˆˆR2is theğ‘–-th
location of the agent, and ğ‘‡represents the number of observed
historical locations. We set ğ‘‡=2for the most extreme case, align-
ing with the setting of MOE [ 43], which is the first work of in-
stantaneous trajectory prediction. Moreover, we also denote the
sequence of previous unobserved trajectory locations of the agent
asXğ‘¢ğ‘›ğ‘œğ‘ğ‘ ={ğ‘¥âˆ’ğ‘+1,ğ‘¥âˆ’ğ‘+2,Â·Â·Â·,ğ‘¥0}, whereğ‘is the total number
of unobserved trajectory locations. The ground-truth future trajec-
tories are denoted as Xğ‘”ğ‘¡={ğ‘¥3,ğ‘¥4,...,ğ‘¥ 2+ğ‘€}, whereğ‘€is the length
of ground-truth future trajectory. Our goal is to predict ğ¾plausi-
ble trajectories{bXğ‘˜}ğ‘˜âˆˆ[0,ğ¾âˆ’1]={(Ë†ğ‘¥ğ‘˜
3,Ë†ğ‘¥ğ‘˜
4,...,Ë†ğ‘¥ğ‘˜
2+ğ‘€)}ğ‘˜âˆˆ[0,ğ¾âˆ’1], as
in multi-model trajectory prediction methods [ 20,23,24]. In con-
trast to previous methods utilizing sufficient observed trajectory
locations (typically, 20 observed trajecotory locations on the Argo-
verse dataset [ 10]), we attempt to leverage merely ğ‘‡=2observed
locations Xğ‘œğ‘ğ‘ for instantaneous trajectory prediction. It is notewor-
thy that, in principle, our proposed method is capable of accepting
observed trajectory locations of arbitrary length ğ‘‡as input.
3.2 Overall Framework of ITPNet
Figure 2 illustrates an overview of our proposed framework. We
first feed the instantaneous observed trajectory locations Xğ‘œğ‘ğ‘ into
a backbone (e.g., HiVT [ 53]) to obtain the latent feature representa-
tions Vğ‘œğ‘ğ‘ ={ğ‘£1,ğ‘£2}. Based on this representation Vğ‘œğ‘ğ‘ , we then
attempt to backwardly predict the latent feature representations
bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ ={Ë†ğ‘£âˆ’ğ‘+1,Ë†ğ‘£âˆ’ğ‘+2,...,Ë†ğ‘£0}of unobserved historical trajec-
tories Xğ‘¢ğ‘›ğ‘œğ‘ğ‘ . Considering that the predicted unobserved feature
representations bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ inevitably contain redundant and noisy in-
formation as mentioned above, we design a Noise Redundancy
Reduction Former (NRRFormer) module to filter out this informa-
tion from a predicted feature sequence. Subsequently, the filtered
features are combined with the observed features to generate a
compact query embedding Q. The query embedding Qis then sent
to the decoder for future trajectory predictions. Since the backbone
 
1645KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rongqing Li et al.
ğ’™ğŸ
ğ’™ğŸğ’™#ğ‘µ%ğŸğ’™ğŸ
Decoderğ’™#ğŸ"ğ’—ğŸ"ğ’—#ğ‘µ%ğŸğ“›ğ’“ğ’†ğ’„
Observationinput
Unobservedground-truth
Unobservedprediction
{#ğ—'}'âˆˆ[*,,-.]ğ“›ğ’„ğ’•ğ’”
...ğ’—ğŸğ’—#ğ‘µ%ğŸğ’—#ğŸ
...
..."ğ’—#ğŸ
HD-map&Nearbyagents
Q
KVBackboneKVKV
OutKVKVğ—ğ’ğ’ƒğ’”
ğ—ğ’–ğ’ğ’ğ’ƒğ’”ğ•ğ’–ğ’ğ’ğ’ƒğ’”#ğ•ğ’–ğ’ğ’ğ’ƒğ’”ğ’—ğŸğ’—ğŸ
ğ‘¸
...NoiseRedundancyReductionFormer(NRRFormer)......NRRBlockNRRBlockNRRBlock
Queryembeddingğ‘½ğ’ğ’ƒğ’”BackwardForecast
Figure 2: Overview of our ITPNet framework. ITPNet mainly consists of two modules: 1) We propose a backward forecasting
mechanism that attempts to reconstruct the latent feature representations Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ of previous unobserved trajectory locations
Xğ‘¢ğ‘›ğ‘œğ‘ğ‘ by the two observed trajectories locations Xğ‘œğ‘ğ‘ . 2) We devise a Noise Redundancy Reduction Former to filter out
noise and redundancy in the predicted latent feature representations Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ , and both the resulting filtered features and the
observation features Vğ‘œğ‘ğ‘ are integrated into a compact query embedding Q. Finally, the query embedding is sent to the decoder
to instantaneously predict future trajectories {Ë†Xğ‘˜}.
in our framework is arbitrary, our method is plug-and-play, and
is compatible with existing trajectory prediction models, enabling
them to gracefully adapt to the scenario of instantaneous trajectory
prediction. In the following section, we will delve into a detailed
introduction of backward forecasting and the NRRFormer.
3.3 Backward Forecasting
When given only 2 observed locations Xğ‘œğ‘ğ‘ , one major issue we
face is the lack of information, making existing trajectory predic-
tion approaches degraded sharply. To alleviate this problem, we
propose to backwardly predict the latent feature representations of
previous unobserved trajectory locations, and then leverage them
as additional information for future trajectory prediction.
First, we can obtain the latent feature representations Vğ‘œğ‘ğ‘ of
the observed locations Xğ‘œğ‘ğ‘ via the backbone Î¦:
Vğ‘œğ‘ğ‘ ={ğ‘£1,ğ‘£2}=Î¦(Xğ‘œğ‘ğ‘ ;ğœ™), (1)
whereğ‘£ğ‘–âˆˆRğ‘‘is the latent feature representation of the ğ‘–-th lo-
cation of the agent, and ğ‘‘is the dimension of the feature. The
backbone Î¦is parameterized by ğœ™, and can be an arbitrary trajec-
tory prediction model, e.g., HiVT [ 53] and LaneGCN [ 28] used in
this paper.
After that, we attempt to backwardly predict the latent feature
representations bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ on the basis of Vğ‘œğ‘ğ‘ , addressing the issue
of the lack of information. To this end, we introduce two self-
supervised tasks: the first one is the reconstruction of the latent
feature representations, and the loss function is designed as:
Lğ‘Ÿğ‘’ğ‘=J(Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ ;bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ ), (2)where Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ =Î¦(Xğ‘¢ğ‘›ğ‘œğ‘ğ‘ ;ğœ™)is the ground-truth latent feature
representations of previous unobserved trajectory locations, and
can be taken as a self-supervised signal, and Jis a function to
measure the distance between Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ andbVğ‘¢ğ‘›ğ‘œğ‘ğ‘ .bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ are the
predicted features, obtained by:
bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ =Î¨(Vğ‘œğ‘ğ‘ ;ğœ“), (3)
where Î¨is a network parameterized by ğœ“. In this paper, we make
use of a LSTM [21] to predict the bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ on the basis of Vğ‘œğ‘ğ‘ ,
Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
0=Î¨(Mean(Vğ‘œğ‘ğ‘ ),ğœ“),
Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–âˆ’1=Î¨(Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–;ğœ“),ğ‘–=0,...,âˆ’ğ‘+2, (4)
where Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–is theğ‘–ğ‘¡â„predicted unobserved latent feature represen-
tations of bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ , and Mean(Â·)represents averaging the trajectory
features along the length dimension. In order to reconstruct the
latent feature representations, we use the smooth ğ¿1loss [ 17] to
optimize theLğ‘Ÿğ‘’ğ‘as:
Lğ‘Ÿğ‘’ğ‘=0âˆ‘ï¸
ğ‘–=âˆ’ğ‘+1ğ›¿(ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–âˆ’Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–), (5)
whereğ›¿is defined as:
ğ›¿(ğ‘£)=0.5ğ‘£2ğ‘–ğ‘“||ğ‘£||<1
||ğ‘£||âˆ’0.5ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’,(6)
where||ğ‘£||denotes the ğ‘™1norm ofğ‘£.
To further enhance the representation ability of the unobserved
latent feature representations, we devise another self-supervised
 
1646ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
SelfAttentionSelfAttentionFeedForwardQllQ
lQ
Figure 3: Structure of Noise Redundancy Reduction Block.
task. Specifically, we regard the feature pair {ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–,Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–}as the
positive sample pair, ğ‘–=âˆ’ğ‘+1,Â·Â·Â·,0, and take{ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–,Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘—}
as the negative sample pair, ğ‘–â‰ ğ‘—. After that, we present another
self-supervised loss:
Lğ‘ğ‘¡ğ‘ =0âˆ‘ï¸
ğ‘–=âˆ’ğ‘+1âˆ‘ï¸
ğ‘—â‰ ğ‘–max(0,ğ›¿(ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–âˆ’Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–)âˆ’ğ›¿(ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘–âˆ’Ë†ğ‘£ğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘—)+Î”),
(7)
where Î”is a margin. It is worth noting that the first loss Lğ‘Ÿğ‘’ğ‘in
(5) targets at reconstructing the latent feature representation ğ‘£ğ‘–as
accurately as possible, while the second loss Lğ‘ğ‘¡ğ‘ in (7) aims to min-
imize the discrepancy between the predicted unobserved feature
representations and the corresponding ground-truth feature repre-
sentations at each timestep, while it enlarges a margin Î”between
the predicted unobserved and non-corresponding ground-truth fea-
ture representations. This can further assist in better reconstructing
unobserved trajectory locations.
3.4 Noise Redundancy Reduction Former
Our Noise Redundancy Reduction Former (NRRFormer) module
that is parameterized by Î˜containsğ¿Noise Redundancy Reduction
Blocks (NRRBlocks). Each NRRBlock attempts to filter out noise
and redundancy in the predicted latent feature representations
Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™, and integrate the resulting filtered feature representations
and observed feature representations Vğ‘œğ‘ğ‘ into a query embedding
Qğ‘™+1,ğ‘™=0,1,Â·Â·Â·,ğ¿âˆ’1.
As shown in the Figure 3, The ğ‘™ğ‘¡â„layer of NRRBlock takes as
input the query embedding Qğ‘™and the unobserved feature repre-
sentations Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™through a self-attention mechanism:
Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™,Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™+1=SelfAtt(Qğ‘™||Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™;ğœƒğ‘™,1), (8)
where||denotes the concatenation operation, the self-attention
module is parameterized by ğœƒğ‘™,1.Q0is a random initialized tensor,
Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
0=Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ , and the Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™represents the output query
embedding. It is worth noting that the length of the query, denoted
asğ¶, is smaller than the length of Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™, denoted as ğ‘, so that
information in Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™is forced to condense and collate into the
compact query embedding Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™, thereby filtering out redundancy
and noise to extract the meaningful information. After that, we
utilize another self-attention module to integrate information ofVğ‘œğ‘ğ‘ into the query embedding:
Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ ,ğ‘œğ‘ğ‘ 
ğ‘™,Vğ‘œğ‘ğ‘ âˆ—=SelfAtt(Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™||Vğ‘œğ‘ğ‘ ;ğœƒğ‘™,2), (9)
where the self-attention module is parameterized by ğœƒğ‘™,2,Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ ,ğ‘œğ‘ğ‘ 
ğ‘™
represents the query embedding after integrating both the filtered
unobserved trajectory features and the observed trajectory features.
Through this self-attention operation, the information of ğ‘‰ğ‘œğ‘ğ‘ can
be effectively distilled into Q, while enabling it to fuse with ğ‘‰ğ‘¢ğ‘›ğ‘œğ‘ğ‘ ,
thereby facilitating the exchange of complementary information
between them. Note that we assume the observed trajectory features
Vğ‘œğ‘ğ‘ do not contain noise or redundancy, because the features
are obtained by encoding Xğ‘œğ‘ğ‘ . Therefore, the Equation (9) only
integrates the information of Vğ‘œğ‘ğ‘ into the query Qthrough self-
attention, but not input the Vğ‘œğ‘ğ‘ âˆ—into the next NRRBlock. At the
end of the NRRBlock, we employ a feed forward layer to produce
the query representation for the next layer,
Qğ‘™+1=FeedForward(Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ ,ğ‘œğ‘ğ‘ 
ğ‘™;ğœƒğ‘™,3), (10)
where the feed forward layer is parameterized by ğœƒğ‘™,3. We utilizeğ¿
NRRBlocks to denoise and reduce redundancy in the unobserved
trajectory features while effectively fusing the observed trajectory
features. Finally we utilize Qğ¿for future trajectory prediction:
{bXğ‘˜}ğ‘˜âˆˆ[0,ğ¾âˆ’1]=Î©(Qğ¿;ğœ”), (11)
where Î©represents the decoder module parameterized by ğœ”. The
decoder module can be the same structure as in previous trajectory
prediction models [ 28,53], enabling our method to be generalizable.
3.5 Optimization and Inference
We adopt the commonly used winner-takes-all strategy [ 51] on
the obtained ğ¾multi-modal trajectories {bXğ‘˜}ğ‘˜âˆˆ[0,ğ¾âˆ’1], which re-
gresses the trajectory closest to the ground truth, denoted as Lğ‘Ÿğ‘’ğ‘”.
In order to help the downstream planner make better decisions, a
classification lossLğ‘ğ‘™ğ‘ is also adopted to score each trajectory. Here,
we adopt the same Lğ‘Ÿğ‘’ğ‘”andLğ‘ğ‘™ğ‘ as those in the corresponding
backbones (see Appendix A.5 for details of Lğ‘Ÿğ‘’ğ‘”andLğ‘ğ‘™ğ‘ ). Finally,
the total loss function can be expressed as:
L=Lğ‘Ÿğ‘’ğ‘”+Lğ‘ğ‘™ğ‘ +ğ›¼Lğ‘Ÿğ‘’ğ‘+ğ›½Lğ‘ğ‘¡ğ‘ , (12)
whereğ›¼andğ›½are three trade-off hyper-parameters. We provide
the pseudo-code of our training procedure in the Algorithm 1.
For inference, when only 2 observed trajectory locations of a
target vehicle are collected, we first extract the latent feature repre-
sentations based on the backbone Î¦, and then apply our backward
forecasting mechanism to predict the latent feature representations
of previous ğ‘unobserved locations of the target agent by the net-
works Î¨. After that, the NRRFormer Î˜={ğœƒğ‘™,1,ğœƒğ‘™,2,ğœƒğ‘™,3}ğ¿
ğ‘™=1filters
out the noise and redundancy in the unobserved latent feature rep-
resentations and integrates the filtered features and observed latent
feature representations into query embedding. Finally, the query
embedding are fed into the decoder network Î©for instantaneous
trajectory prediction.
 
1647KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rongqing Li et al.
Algorithm 1: Training Procedure of ITPNet
Input: input trajectory X={Xğ‘œğ‘ğ‘ ,Xğ‘¢ğ‘›ğ‘œğ‘ğ‘ }, ground-truth
trajectory Xğ‘”ğ‘¡, query embedding Q, layersğ¿of
NRRFormer, trade-off hyper-parameters: ğ›¼, andğ›½.
Output: Network parameters: ğœ™,ğœ“,{ğœƒğ‘™,1,ğœƒğ‘™,2,ğœƒğ‘™,3}ğ¿
ğ‘™=1, and
ğœ”.
Initialize: Randomly initialize ğœ™,ğœ“,{ğœƒğ‘™,1,ğœƒğ‘™,2,ğœƒğ‘™,3}ğ¿
ğ‘™=1,ğœ”,
andQ.
while not converges do
Compute latent feature representations
Vğ‘œğ‘ğ‘ =Î¦(Xğ‘œğ‘ğ‘ ;ğœ™)andVğ‘¢ğ‘›ğ‘œğ‘ğ‘ =Î¦(Xğ‘¢ğ‘›ğ‘œğ‘ğ‘ ;ğœ™);
Backward forecast bVğ‘¢ğ‘›ğ‘œğ‘ğ‘ bybVğ‘¢ğ‘›ğ‘œğ‘ğ‘ =Î¨(Vğ‘œğ‘ğ‘ ;ğœ“);
ComputeLğ‘Ÿğ‘’ğ‘,Lğ‘ğ‘¡ğ‘ by Eq. (5) and (7), respectively;
Employ NRRFormer to filter out redundancy and noise
in predicted unobserved latent feature representations
and integrate the resulting filtered feature
representations and observed feature representations
intoğ‘„, by
Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
0=Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ ;
forğ‘™=0...ğ¿âˆ’1do
Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™,Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™+1=SelfAtt(Qğ‘™||Ë†Vğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™;ğœƒğ‘™,1);
Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ ,ğ‘œğ‘ğ‘ 
ğ‘™,Vğ‘œğ‘ğ‘ âˆ—=SelfAtt(Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ 
ğ‘™||Vğ‘œğ‘ğ‘ ;ğœƒğ‘™,2);
Qğ‘™+1=FeedForward(Qğ‘¢ğ‘›ğ‘œğ‘ğ‘ ,ğ‘œğ‘ğ‘ 
ğ‘™;ğœƒğ‘™,3);
end
Predict trajectory{bXğ‘˜}ğ‘˜âˆˆ[0,ğ¾âˆ’1]=Î©(Qğ¿;ğœ”);
ComputeLğ‘Ÿğ‘’ğ‘”,Lğ‘ğ‘™ğ‘ through{bXğ‘˜}ğ‘˜âˆˆ[0,ğ¾âˆ’1];
Calculate the total loss LbyL=Lğ‘Ÿğ‘’ğ‘”+Lğ‘ğ‘™ğ‘ +ğ›¼Lğ‘Ÿğ‘’ğ‘+
ğ›½Lğ‘ğ‘¡ğ‘ ;
Update model parameters ğœ™,ğœ“,{ğœƒğ‘™,1,ğœƒğ‘™,2,ğœƒğ‘™,3}ğ¿âˆ’1
ğ‘™=0,ğœ”
and query embedding Qby minimizingL.
end
4 Experiments
4.1 Datasets
We evaluate our method for the instantaneous trajectory prediction
tasks on two widely used benchmark datasets, Argoverse [ 10] and
NuScene [7].
Argoverse Datasets: This dataset contains a total of 324,557 scenes,
which are split into 205,492 training scenes, 39,472 validation scenes,
and 78,143 testing scenes. The observation duration for both the
training and validation sets is 5 seconds with a sampling frequency
of 10Hz. In contrast to previous approaches taking the first 2 seconds
(i.e., 20 locations) as the observed trajectory locations and the last 3
seconds as the future ground-truth trajectory, we only utilize ğ‘‡=2
observed trajectory locations, and predict the future trajectory of
the last 3 seconds in our experiments.
NuScene Datasets The dataset consists of 32,186 training, 8,560
validation, and 9,041 test samples. Each sample is a sequence of x-y
coordinates with a duration of 8 seconds and a sample frequency
of 2Hz. Previous approaches usually take the first 2 seconds (i.e., 5
locations) as the observed trajectory locations and the last 6 seconds
as the future ground-truth trajectory. However, we leverage onlyğ‘‡=2observed trajectory locations to predict the future trajectory
of the last 6 seconds in the experiments.
4.2 Implementation Details
We perform the experiments using two different backbone models,
HiVT [ 53] and LaneGCN [ 28]. Specifically, we utilize the temporal
encoder in HiVT and the ActorNet in LaneGCN to extract the latent
feature representations, respectively. We set the feature dimensions
ğ‘‘to 64 and 128 when using HiVT and LaneGCN as the backbone,
respectively. The hidden size of the LSTM for predicting unob-
served latent feature representations is set to ğ‘‘. The NRRFormer
consists of three NRRBlocks. In our experiments, the predicted un-
observed length ğ‘is set to 10 for the Argoverse dataset and 4 for
the nuScenes dataset. We set the query embedding length to ğ¶=4
for the Argoverse dataset and ğ¶=2for the nuScenes dataset. In
addition, we set the trade-off hyper-parameters ğ›¼andğ›½to 0.1 and
0.1.
4.3 Baselines and Evaluation Metrics
We first compare with two most related works: MOE [ 43] and Distill
[36]. Since we use HiVT [ 53] and LaneGCN [ 28] as our backbone,
respectively, we also compare our method with them. When using
HiVT as the backbone, we denote our method as ITPNet+HiVT.
When using LaneGCN as the backbone, we denote our method as
ITPNet+LaneGCN.
To evaluate these methods, we employ three popular evaluation
metrics [ 18,46,51], minADE@ ğ¾, minFDE@ ğ¾, and minMR@ ğ¾,
whereğ¾represents the number of the generated trajectories. we
setğ¾to 1 and 6 in our experiments.
4.4 Results and Analysis
Performance on Instantaneous Trajectory Prediction . To demon-
strate the effectiveness of our method for instantaneous trajectory
prediction, we compare our method with the state-of-the-art base-
lines. The results listed in Table 1, shows that ITPNet+LaneGCN
and ITPNet+HiVT significantly outperforms LaneGCN and HiVT,
respectively. This illustrates that current state-of-the-art trajectory
prediction approaches struggle to effectively handle cases involving
instantaneous observed trajectory inputs. However, when plugging
our framework into these two backbone models, respectively, the
performance is significantly improved. This shows our method is
effective for instantaneous trajectory prediction, and is compatible
with different trajectory prediction models. Moreover, our methods
achieve better performance than MOE and Distill, which indicates
the effectiveness of our methods once more.
Ablation Study. We conduct ablation studies on the Argoverse
dataset, and we employ HiVT [ 53] as the backbone. Table 2 shows
the results. When the Lğ‘Ÿğ‘’ğ‘is applied to the loss function, our
method significantly improves the performance. This indicates the
effectiveness of our proposed backward forecast mechanism for pre-
dicting the latent feature representations of previous unobserved
trajectory locations. The loss Lğ‘ğ‘¡ğ‘ further boosts the performance
of the model, demonstrating the self-supervised task is meaning-
ful. Moreover, our method can further improve the performance
when integrating our NRRFormer, underscoring the effectiveness
 
1648ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: minADE@ ğ¾, minFDE@ ğ¾, and MR@ ğ¾of different methods on Argoverse and nuScenes, respectively.
Dataset MethodsK=1 K=6
minADE minFDE minMR minADE minFDE minMR
ArgoverseHiVT [53] 4.158 8.368 0.846 1.085 1.712 0.249
MOE+HiVT [43] 3.312 6.840 0.794 0.939 1.413 0.177
Distill+HiVT [36] 3.251 6.638 0.771 0.968 1.502 0.185
ITPNet+HiVT 2.631 5.703 0.757 0.819 1.218 0.141
LaneGCN [28] 4.204 8.647 0.861 1.126 1.821 0.278
MOE+LaneGCN [43] 3.958 8.264 0.842 1.089 1.734 0.265
Distill+LaneGCN [36] 3.768 7.926 0.817 1.077 1.687 0.252
ITPNet+LaneGCN 2.922 5.627 0.765 0.894 1.425 0.173
nuScenesHiVT [53] 6.564 13.745 0.914 1.772 2.836 0.505
MOE+HiVT [43] 5.705 12.619 0.913 1.712 2.813 0.494
Distill+HiVT [36] 5.950 12.606 0.911 1.759 2.861 0.483
ITPNet+HiVT 5.514 12.584 0.909 1.503 2.628 0.483
LaneGCN [28] 6.125 14.300 0.935 1.878 3.497 0.630
MOE+LaneGCN [43] 6.071 13.994 0.931 1.778 3.372 0.613
Distill+LaneGCN [36] 5.968 13.807 0.926 1.737 3.278 0.604
ITPNet+LaneGCN 5.739 13.555 0.919 1.679 3.146 0.580
Table 2: Ablation study of our method for Lğ‘Ÿğ‘ğ‘›ğ‘˜,Lğ‘Ÿğ‘’ğ‘andLğ‘ğ‘¡ğ‘ on the Argoverse dataset.
Lğ‘Ÿğ‘’ğ‘Lğ‘ğ‘¡ğ‘  NRRFormerK=1 K=6
minADE minFDE minMR minADE minFDE minMR
4.158 8.368 0.846 1.085 1.712 0.249
âœ“ 2.646 5.790 0.763 0.841 1.285 0.154
âœ“ âœ“ 2.615 5.733 0.761 0.832 1.262 0.149
âœ“ âœ“ âœ“ 2.631 5.703 0.757 0.819 1.218 0.141
Table 3: Analysis of backward forecasting with different ğ‘and effectiveness of NRRFormer on Argoverse.
NNRR
FormerK=6 NRR
FormerK=6
minADE minFDE minMR minADE minFDE minMR
0 âœ— 1.068 1.678 0.241 - - - -
1 âœ— 0.969 1.494 0.193 âœ“ 0.964 1.498 0.194
2 âœ— 0.872 1.329 0.160 âœ“ 0.868 1.323 0.158
3 âœ— 0.832 1.262 0.149 âœ“ 0.828 1.254 0.147
4 âœ— 0.845 1.291 0.154 âœ“ 0.824 1.240 0.146
5 âœ— 0.859 1.312 0.156 âœ“ 0.822 1.232 0.145
6 âœ— 0.867 1.302 0.161 âœ“ 0.823 1.231 0.145
7 âœ— 0.881 1.375 0.173 âœ“ 0.820 1.222 0.143
8 âœ— 0.903 1.410 0.181 âœ“ 0.821 1.222 0.142
9 âœ— 0.933 1.453 0.187 âœ“ 0.819 1.220 0.142
10 âœ— 0.967 1.522 0.196 âœ“ 0.819 1.218 0.141
of our NRRFormer in filtering out noise and redundancy from the
predicted unobserved latent features.
Analysis of Different Lengths of Unobserved Trajectories.
We investigate the influence of different lengths ğ‘of unobserved
trajectory locations on instantaneous trajectory prediction. We use
HiVT as the backbone on the Argoverse dataset. The results are
listed in Table 3. Note that when NRRFormer is not used, we di-
rectly concatenate the predicted unobserved features with observedfeatures for future trajectory prediction. As ğ‘increases, the per-
formance of the model is gradually improved. This reveals that
predicting more latent feature representations can introduce more
useful information, and thus be beneficial to trajectory prediction.
However, when ğ‘exceeds a certain value ( ğ‘>3), the perfor-
mance deteriorates. This is attributed to the introduction of noise
and redundancy when predicting a longer feature sequence. When
 
1649KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rongqing Li et al.
Table 4: Comparison with baselines on Argoverse dataset using different ğ‘‡
Dataset T MethodK=1 K=6
minADE minFDE minMR minADE minFDE minMR
Argoverse2HiVT 4.158 8.368 0.846 1.085 1.712 0.249
MOE+HiVT 3.312 6.840 0.794 0.939 1.413 0.177
Distill+HiVT 3.251 6.638 0.771 0.968 1.502 0.185
ITPNet+HiVT 2.631 5.703 0.757 0.819 1.218 0.141
5HiVT 2.510 5.523 0.747 0.809 1.203 0.137
MOE+HiVT 2.562 5.607 0.776 0.784 1.221 0.134
Distill+HiVT 2.465 5.452 0.756 0.796 1.248 0.139
ITPNet+HiVT 2.410 5.257 0.738 0.748 1.132 0.122
10HiVT 2.441 5.238 0.741 0.736 1.121 0.121
MOE+HiVT 2.357 5.141 0.733 0.726 1.101 0.117
Distill+HiVT 2.224 5.039 0.726 0.731 1.118 0.119
ITPNet+HiVT 2.190 4.792 0.716 0.718 1.088 0.113
(
a) HiVT
 (
b) MOE+HiVT
 (
c) Distill+HiVT
 (
d) ITPNet+HiVT
Figure 4: Qualitative results of a) HiVT, b) MOE+HiVT, c) Distill+HiVT, d) ITPNet+HiVT on Argoverse. The observed historical
trajectories are shown in red, the ground-truth future trajectories are shown in black, and the predicted multi-modal future
trajectories are shown in green.
NRRFormer is enabled, the performance of our method is consis-
tently improved as ğ‘increases. This illustrates our NRRFormer
module can indeed filter out redundant and noisy information,
demonstrating its effectiveness.
Results of Varied Lengths of Observed Trajectories. To fur-
ther demonstrate the effectiveness of our method, we utilize HiVT
as the backbone and conduct experiments with varied lengths ofobserved trajectories on Argoverse dataset. As depicted in Table 4,
our approach consistently exhibits superior performance across a
range ofğ‘‡when compared to the baseline methods. One interesting
point is that our ITPNet+HiVT with ğ‘‡=2achieves comparable
performance to HiVT with ğ‘‡=5. This means that our method can
averagely save 1.5 seconds for trajectory prediction, compared to
 
1650ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
HiVT. If a car has a driving speed of 70km/s, our method can save
around 30 meters to observe the agent for trajectory prediction.
Qualitative Results. We perform a visualization of the predicted
multi-modal trajectories generated by MOE, Distill, HiVT, and our
proposed method ITPNet+HiVT respectively on Argoverse dataset
with only 2 observed locations. The results are shown in Figure 4.
We observe that our method exhibits diversity and more accurate
trajectory prediction than other baselines in the scenario of turn-
ing and going straight. This suggests that our method can handle
different driving scenarios and can achieve improved predictions
with only 2 observed locations.
5 Conclusion
In this paper, we investigated a challenging problem of instanta-
neous trajectory prediction with very few observed locations. We
proposed a plug-and-play approach that backwardly predicted the
latent feature representations of unobserved locations, to mitigate
the issue of the lack of information. Considering the noise and
redundancy in unobserved feature representations, we designed
the NRRFormer to remove them and integrate the resulting filtered
features and observed trajectory features into a compact query
embedding for future trajectory prediction. Extensive experimental
results demonstrated that the proposed method can be effective for
instantaneous trajectory prediction, and can be compatible with
different trajectory prediction models.
Acknowledgments
This work was supported by the National Natural Science Founda-
tion of China (NSFC) under Grants 62122013, U2001211.
References
[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li
Fei-Fei, and Silvio Savarese. 2016. Social lstm: Human trajectory prediction in
crowded spaces. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). 961â€“971.
[2]GÃ¶rkay Aydemir, Adil Kaan Akan, and Fatma GÃ¼ney. 2023. ADAPT: Effi-
cient Multi-Agent Trajectory Prediction with Adaptation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision.
[3]Inhwan Bae and Hae-Gon Jeon. 2023. A Set of Control Points Conditioned
Pedestrian Trajectory Prediction. Proceedings of the AAAI Conference on Artificial
Intelligence (2023).
[4]Inhwan Bae, Jean Oh, and Hae-Gon Jeon. 2023. EigenTrajectory: Low-Rank
Descriptors for Multi-Modal Trajectory Forecasting. Proceedings of the IEEE/CVF
International Conference on Computer Vision (2023).
[5]Inhwan Bae, Young-Jae Park, and Hae-Gon Jeon. 2024. SingularTrajectory: Uni-
versal Trajectory Predictor Using Diffusion Model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 17890â€“17901.
[6]Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. 2018. Chauffeurnet: Learn-
ing to drive by imitating the best and synthesizing the worst. arXiv preprint
arXiv:1812.03079 (2018).
[7]Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. 2020.
nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 11621â€“
11631.
[8]Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urtasun. 2020. Spagnn:
Spatially-aware graph neural networks for relational behavior forecasting from
sensor data. In 2020 IEEE International Conference on Robotics and Automation
(ICRA). IEEE, 9491â€“9497.
[9]Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie Liao, and Raquel Urtasun.
2020. Implicit latent variable model for scene-consistent motion forecasting. In
Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August
23â€“28, 2020, Proceedings, Part XXIII 16. Springer, 624â€“641.
[10] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir
Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and
James Hays. 2019. Argoverse: 3D Tracking and Forecasting with Rich Maps.
http://arxiv.org/abs/1911.02620 arXiv:1911.02620 [cs].[11] Guangyi Chen, Junlong Li, Jiwen Lu, and Jie Zhou. 2021. Human trajectory
prediction via counterfactual analysis. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (CVPR). 9824â€“9833.
[12] Jie Cheng, Xiaodong Mei, and Ming Liu. 2023. Forecast-MAE: Self-supervised
Pre-training for Motion Forecasting with Masked Autoencoders. Proceedings of
the IEEE/CVF International Conference on Computer Vision (2023).
[13] Sehwan Choi, Jungho Kim, Junyong Yun, and Jun Won Choi. 2023. R-Pred: Two-
Stage Motion Prediction Via Tube-Query Attention-Based Trajectory Refinement.
Proceedings of the IEEE/CVF International Conference on Computer Vision (2023).
[14] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li,
and Cordelia Schmid. 2020. Vectornet: Encoding hd maps and agent dynamics
from vectorized representation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 11525â€“11533.
[15] Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and
Fabien Moutarde. 2022. THOMAS: Trajectory Heatmap Output with learned
Multi-Agent Sampling. In International Conference on Learning Representations.
[16] Roger Girgis, Florian Golemo, Felipe Codevilla, Martin Weiss, Jim Aldon Dâ€™Souza,
Samira Ebrahimi Kahou, Felix Heide, and Christopher Pal. 2021. Latent Vari-
able Sequential Set Transformers for Joint Multi-Agent Motion Prediction. In
International Conference on Learning Representations.
[17] Ross Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV). 1440â€“1448.
[18] Junru Gu, Chen Sun, and Hang Zhao. 2021. Densetnt: End-to-end trajectory
prediction from dense goal sets. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (CVPR). 15303â€“15312.
[19] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou,
and Jiwen Lu. 2022. Stochastic trajectory prediction via motion indeterminacy
diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR). 17113â€“17122.
[20] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi.
2018. Social gan: Socially acceptable trajectories with generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). 2255â€“2264.
[21] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735â€“1780.
[22] Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir
Anguelov, et al .2023. MotionDiffuser: Controllable Multi-Agent Motion Pre-
diction using Diffusion. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 9644â€“9653.
[23] Vineet Kosaraju, Amir Sadeghian, Roberto MartÃ­n-MartÃ­n, Ian Reid, Hamid
Rezatofighi, and Silvio Savarese. 2019. Social-bigat: Multimodal trajectory fore-
casting using bicycle-gan and graph attention networks. Advances in Neural
Information Processing Systems 32 (2019).
[24] Mihee Lee, Samuel S Sohn, Seonghyeon Moon, Sejong Yoon, Mubbasir Kapadia,
and Vladimir Pavlovic. 2022. Muse-VAE: multi-scale VAE for environment-aware
long term trajectory prediction. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 2221â€“2230.
[25] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr,
and Manmohan Chandraker. 2017. Desire: Distant future prediction in dynamic
scenes with interacting agents. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). 336â€“345.
[26] Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean Segal,
and Raquel Urtasun. 2020. End-to-end contextual perception and prediction with
interaction transformer. In 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS). IEEE, 5784â€“5791.
[27] Xin Li, Xiaowen Ying, and Mooi Choo Chuah. 2019. Grip++: Enhanced graph-
based interaction-aware trajectory prediction for autonomous driving. arXiv
preprint arXiv:1907.07792 (2019).
[28] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel
Urtasun. 2020. Learning lane graph representations for motion forecasting. In
Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August
23â€“28, 2020, Proceedings, Part II 16. Springer, 541â€“556.
[29] Rongqin Liang, Yuanman Li, Jiantao Zhou, and Xia Li. 2022. STGlow: A Flow-
based Generative Framework with Dual Graphormer for Pedestrian Trajectory
Prediction. arXiv preprint arXiv:2211.11220 (2022).
[30] Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, and Bolei Zhou. 2021.
Multimodal motion prediction with stacked transformers. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 7577â€“
7586.
[31] Osama Makansi, Julius Von KÃ¼gelgen, Francesco Locatello, Peter Vincent Gehler,
Dominik Janzing, Thomas Brox, and Bernhard SchÃ¶lkopf. 2021. You Mostly Walk
Alone: Analyzing Feature Attribution in Trajectory Prediction. In International
Conference on Learning Representations.
[32] Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. 2021. From
Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting. In 2021
IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Montreal,
QC, Canada, 15213â€“15222. https://doi.org/10.1109/ICCV48922.2021.01495
 
1651KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rongqing Li et al.
[33] Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, and Yanfeng Wang. 2023. Leapfrog
Diffusion Model for Stochastic Trajectory Prediction. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5517â€“5526.
[34] Mancheng Meng, Ziyan Wu, Terrence Chen, Xiran Cai, Xiang Sean Zhou, Fan
Yang, and Dinggang Shen. 2022. Forecasting Human Trajectory from Scene
History. arXiv preprint arXiv:2210.08732 (2022).
[35] Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and Christian Claudel.
2020. Social-stgcnn: A social spatio-temporal graph convolutional neural network
for human trajectory prediction. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 14424â€“14432.
[36] Alessio Monti, Angelo Porrello, Simone Calderara, Pasquale Coscia, Lamberto
Ballan, and Rita Cucchiara. 2022. How many observations are enough? knowledge
distillation for trajectory forecasting. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR). 6553â€“6562.
[37] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat,
and Benjamin Sapp. 2022. Wayformer: Motion forecasting via simple & efficient
attention networks. arXiv preprint arXiv:2207.05844 (2022).
[38] Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zhengdong Zhang, Hao-
Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu,
Ashish Venugopal, et al .2022. Scene transformer: A unified architecture for
predicting future trajectories of multiple agents. In International Conference on
Learning Representations.
[39] Daehee Park, Hobin Ryu, Yunseo Yang, Jegyeong Cho, Jiwon Kim, and Kuk-Jin
Yoon. 2023. Leveraging Future Relationship Reasoning for Vehicle Trajectory
Prediction. arXiv preprint arXiv:2305.14715 (2023).
[40] Tung Phan-Minh, Elena Corina Grigore, Freddy A. Boulton, Oscar Beijbom, and
Eric M. Wolff. 2020. CoverNet: Multimodal Behavior Prediction Using Trajectory
Sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR).
[41] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid
Rezatofighi, and Silvio Savarese. 2019. Sophie: An attentive gan for predicting
paths compliant to social and physical constraints. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). 1349â€“1358.
[42] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. 2020.
Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous
data. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK,
August 23â€“28, 2020, Proceedings, Part XVIII 16. Springer, 683â€“700.
[43] Jianhua Sun, Yuxuan Li, Liang Chai, Hao-Shu Fang, Yong-Lu Li, and Cewu Lu.
2022. Human trajectory prediction with momentary observation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).6467â€“6476.
[44] Qiao Sun, Xin Huang, Junru Gu, Brian C Williams, and Hang Zhao. 2022. M2I:
From factored marginal trajectory prediction to interactive prediction. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 6543â€“6552.
[45] Jingke Wang, Tengju Ye, Ziqing Gu, and Junbo Chen. 2022. Ltp: Lane-based
trajectory prediction for autonomous driving. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). 17134â€“17142.
[46] Mingkun Wang, Xinge Zhu, Changqian Yu, Wei Li, Yuexin Ma, Ruochun Jin,
Xiaoguang Ren, Dongchun Ren, Mingxu Wang, and Wenjing Yang. 2022. Ganet:
Goal area network for motion forecasting. arXiv preprint arXiv:2209.09723 (2022).
[47] Xishun Wang, Tong Su, Fang Da, and Xiaodong Yang. 2023. ProphNet: Efficient
agent-centric motion forecasting with anchor-informed proposals. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 21995â€“
22003.
[48] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. 2022. Group-
Net: Multiscale Hypergraph Neural Networks for Trajectory Prediction With
Relational Reasoning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). 6498â€“6507.
[49] Wenyuan Zeng, Ming Liang, Renjie Liao, and Raquel Urtasun. 2021. Lanercnn:
Distributed representations for graph-centric motion forecasting. In 2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE, 532â€“539.
[50] Bo Zhang, Tao Wang, Changdong Zhou, Nicola Conci, and Hongbo Liu. 2022.
Human trajectory forecasting using a flow-based generative model. Engineering
Applications of Artificial Intelligence 115 (2022), 105236.
[51] Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp, Balakrishnan Varadarajan,
Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, et al .2021. Tnt: Target-driven
trajectory prediction. In Conference on Robot Learning (CoRL). PMLR, 895â€“904.
[52] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai Huang. 2023. Query-
Centric Trajectory Prediction. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 17863â€“17873.
[53] Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Kejie Lu. 2022. Hivt:
Hierarchical vector transformer for multi-agent motion prediction. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
8823â€“8833.
[54] Dekai Zhu, Guangyao Zhai, Yan Di, Fabian Manhardt, Hendrik Berkemeyer,
Tuan Tran, Nassir Navab, Federico Tombari, and Benjamin Busam. 2023. IPCC-
TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent
Trajectory Prediction. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 5507â€“5516.
 
1652ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A Appendix
A.1 Implmentations of Baselines
MOE+HiVT and MOE+LaneGCN. We integrate MOE [ 43] to
HiVT and LaneGCN backbones by employing the soft-pretraining
with masked trajectory complement and context restoreation tasks.
Distill+HiVT and Distill+LaneGCN. We distill knowledge from
the output of the encoder (output of Global Interaction in HiVT,
and output of FusionNet in LaneGCN) and decoder (output of last
hidden layer).
A.2 Failure cases of ITPNet
We provide failure cases of ITPNet+HiVT on Argoverse dataset, as
shown in Figure 5. The model fails (1) when the future intention
of the agents suddenly changes (a, d); (2) the future behavior is
complex and hard to perceive from observed trajectories, such as
overtaking; (3) the agent does not follow the traffic rules, such as
turning left from the lane for right turns (c).
(a)
 (b)
(c)
 (d)
Figure 5: Failure case of ITPNet+HiVT on Argoverse. The
observed trajectories are shown in red, the ground-truth tra-
jectories are shown in black, and the predicted multi-modal
trajectories are shown in green.
A.3 Results with noisy observed historical
trajectories
We take into account the impact of noise in observed historical
trajectories arising from perception device errors. We add GaussianNoiseN(0,ğœ)to observed trajectories. The experimental results,
as listed in Table 5, reveal that our proposed ITPNet maintains its
superiority over both MOE and Distill methods, even in scenarios
where the observed locations exhibit noise.
A.4 Convergence Analysis
We study the convergence of our method on Argoverse and nuScenes.
The curves of the total loss of our method are shown in Figure 6.
we can see the loss decreases as the training steps, and it finally
levels off.
0 20k 40k 60k 80k 100k
Steps0.81.01.21.4 Final loss
05k10k 15k 20k 25k 30k 35k
Steps1.01.52.02.53.03.5Final loss
Figure 6: Convergence analysis of our method. Left for Argo-
verse and right for nuScenes.
A.5 Details about Lğ‘Ÿğ‘’ğ‘”andLğ‘ğ‘™ğ‘ of backbones
HiVT parameterizes the distribution of future trajectories as a mix-
ture model where each mixture component is a Laplace distribution.
The regression loss Lğ‘Ÿğ‘’ğ‘”is defined as:
Lğ‘Ÿğ‘’ğ‘”=ğ‘€+2âˆ‘ï¸
ğ‘–=3log1
2ğ‘e
xp(âˆ’|Ë†ğ‘¥ğ‘–âˆ’ğœ‡ğ‘–|
ğ‘), (13)
whereğ‘is a learnable scale parameter of Laplace distribution, Ë†ğ‘¥ğ‘–is
the predicted future trajectory closest to the ground-truth future
trajectory and ğœ‡ğ‘–is the ground-truth future trajectory.
TheLğ‘ğ‘™ğ‘ is defined as cross-entropy loss to optimize the mixing
coefficients,
Lğ‘ğ‘™ğ‘ =âˆ’ğ¾âˆ‘ï¸
ğ‘˜=1ğœ‹ğ‘˜logË†ğœ‹ğ‘˜, (14)
whereğœ‹ğ‘˜and Ë†ğœ‹ğ‘˜are the probability of the ğ‘˜ğ‘¡â„trajectory to be se-
lected, andğœ‹ğ‘˜=1if and only if Ë†Xğ‘˜is the predicted future trajectory
closest to the ground-truth future trajectory.
LaneGCN employ smooth ğ¿1loss asLğ‘Ÿğ‘’ğ‘”, which is defined as,
Lğ‘Ÿğ‘’ğ‘”=ğ‘€+2âˆ‘ï¸
ğ‘–=3ğ›¿(Ë†ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘–), (15)
where the definition of ğ›¿is same as Equation (6).
The classification loss Lğ‘ğ‘™ğ‘ is defined as,
Lğ‘ğ‘™ğ‘ =1
ğ¾âˆ’1âˆ‘ï¸
ğ‘˜â‰ ğ‘˜â€²max(0,
ğœ‹ğ‘˜+ğœ–âˆ’ğœ‹ğ‘˜â€²), (16)
where theğ‘˜ğ‘¡â„predicted future trajectory is the closest one to the
ground-truth future trajectory. This loss pushes the closest one
away from others at least by a margin ğœ–.
 
1653KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Rongqing Li et al.
Table 5: Performance of instantaneous trajectory prediction with noised observed trajectory locations. ğœrepresents the standard
deviation of Gaussian Noise.
Dataset MethodsK=1 K=6
minADE minFDE minMR minADE minFDE minMR
ArgoverseMOE+HiVT 3.312 6.840 0.794 0.939 1.413 0.177
Distill+HiVT 3.251 6.638 0.771 0.968 1.502 0.185
ITPNet+HiVT 2.631 5.703 0.757 0.819 1.218 0.141
MOE+HiVT ( ğœ=0.1m) 3.426 7.114 0.836 1.002 1.549 0.202
Distill+HiVT ( ğœ=0.1m) 3.374 6.982 0.822 1.046 1.616 0.213
ITPNet+HiVT( ğœ=0.1m) 2.938 6.424 0.792 0.909 1.347 0.165
 
1654