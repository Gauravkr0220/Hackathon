User Welfare Optimization in Recommender Systems with
Competing Content Creators
Fan Yao
University of Virginia
Charlottesville, USA
fy4bc@virginia.eduYiming Liao
Meta Platforms, Inc.
New York, USA
yimingliao@meta.comMingzhe Wu
University of Southern California
Los Angeles, USA
mingzhew@usc.edu
Chuanhao Li
Yale University
New Haven, USA
chuanhao.li.cl2637@yale.eduYan Zhu
Google
Mountain View, USA
yanzhuyz@google.comJames Yang
Meta Platforms, Inc.
Menlo Park, USA
jamesjy@meta.com
Jingzhou Liu
Meta Platforms, Inc.
Menlo Park, USA
jingzhol@meta.comQifan Wang
Meta Platforms, Inc.
Menlo Park, USA
wqfcr@meta.comHaifeng Xu
University of Chicago
Chicago, USA
haifengxu@uchicago.edu
Hongning Wang
University of Virginia
Charlottesville, USA
hw5x@virginia.edu
ABSTRACT
Driven by the new economic opportunities created by the creator
economy, an increasing number of content creators rely on and
compete for revenue generated from online content recommenda-
tion platforms. This burgeoning competition reshapes the dynamics
of content distribution and profoundly impacts long-term user wel-
fare on the platform. However, the absence of a comprehensive
picture of global user preference distribution often traps the compe-
tition, especially the creators, in states that yield sub-optimal user
welfare. To encourage creators to best serve a broad user popula-
tion with relevant content, it becomes the platformâ€™s responsibility
to leverage its information advantage regarding user preference
distribution to accurately signal creators.
In this study, we perform system-side user welfare optimiza-
tion under a competitive game setting among content creators. We
propose an algorithmic solution for the platform, which dynami-
cally computes a sequence of weights for each user based on their
satisfaction of the recommended content. These weights are then
utilized to design mechanisms that adjust the recommendation
policy or the post-recommendation rewards, thereby influencing
creatorsâ€™ content production strategies. To validate the effective-
ness of our proposed method, we report our findings from a series
of experiments, including: 1. a proof-of-concept negative example
illustrating how creatorsâ€™ strategies converge towards sub-optimal
This work is licensed under a Creative Commons Attribution-
ShareAlike International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3672021states without platform intervention; 2. offline experiments em-
ploying our proposed intervention mechanisms on diverse datasets;
and 3. results from a three-week online experiment conducted on
Instagram Reels short-video recommendation platform.
CCS CONCEPTS
â€¢Information systems â†’Personalization; â€¢Theory of com-
putationâ†’Algorithmic mechanism design.
KEYWORDS
Recommender System, Mechanism Design, Welfare Optimization
ACM Reference Format:
Fan Yao, Yiming Liao, Mingzhe Wu, Chuanhao Li, Yan Zhu, James Yang,
Jingzhou Liu, Qifan Wang, Haifeng Xu, and Hongning Wang. 2024. User
Welfare Optimization in Recommender Systems with Competing Content
Creators. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, Barcelona, Spain, 12 pages. https://doi.org/10.1145/3637528.3672021
1 INTRODUCTION
Online content recommendation platforms have evolved into an in-
dispensable component of our daily lives [ 6]. These platforms play
a pivotal role in assisting their users in navigating the vast ocean of
content generated by revenue-seeking creators, including various
social media platforms (e.g., Facebook, Instagram), streaming ser-
vices (e.g., YouTube, TikTok), and many more. One of the primary
functions of these recommendation platforms is to advance user
welfare, defined as the overall volume and quality of interactions
between users and content. This metric is widely regarded as a
fundamental indicator of the well-being of an online ecosystem and
is also closely tied to the platformâ€™s revenue.
3874
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Yao et al.
After decades of effort in relevance-driven matching between
users and content, industry practitioners and researchers have
reached the consensus that user welfare optimization cannot be
achieved through myopic approaches that merely target at eliciting
and predicting user preferences [ 5,7,10,23,27,30,31,35]. One
primary reason is because any matching strategy has a profound
impact on content creatorsâ€™ beliefs about the usersâ€™ demand and
consequently their reactions, i.e., what to produce next, leading to
a shift in the distribution of content available for recommendation.
This influence pathway is unfortunately overlooked in existing
recommendation algorithm design; and therefore, there is a great
need for a robust recommendation strategy that operates with
respect to creatorsâ€™ strategic responses and the resultant content
dynamics. It is imperative for the platform to encourage creators
in generating content that continuously contributes to the overall
health of the ecosystem.
Typically, creatorsâ€™ well-being is intricately linked to the expo-
sure of their content and the economic incentives they accrue from
the platform, compelling them to continuously strive for maximized
benefits [ 13,15]. This dynamic creates a competitive environment
that leads to intriguing phenomena in terms of welfare guarantees
at equilibrium [ 12,20,36]. For instance, Yao et al . [32] introduced a
game theoretical framework to investigate competition dynamics
among content creators. Their research revealed that social welfare
loss can be attributed to factors such as the degree of exploration
in usersâ€™ decision making and the span of recommendation slots.
As indicated by many previous studies, the platform suffers from
sub-optimal social welfare and thus undermines long-term rev-
enue when content distribution lacks necessary diversity to cater
to various usersâ€™ preferences. This issue is also observed in em-
pirical studies, where content creators often exhibit a tendency to
chase trends [16, 24]. In essence, creators tend to produce content
that arouses the interests of the majority user group, owing to the
groupâ€™s high visibility and the creatorsâ€™ myopic creation strategies
[20,32]. However, it is our contention that the platform should not
simply blame creators for their perceived selfishness and myopia.
This is because creators do not possess a holistic view of the de-
mand distribution, i.e., user preferences. Instead, it is the platformâ€™s
responsibility to disseminate knowledge about user demand to cre-
ators. By doing so, creators can make better informed decisions
that mutually benefit their own interest and enhance user welfare
(and hence platformâ€™s revenue).
In this study, we extend the Content Creator Competition ( ğ¶3)
framework introduced by Yao et al . [32,34], to model the dynamics
of competition among content creators. We relax the behavioral
assumptions about creatorsâ€™ updating strategies in the original
framework and explore how the platform can design mechanisms
to optimize user welfare accordingly. Our key idea is to direct
creatorsâ€™ attention towards currently under-served users, by manip-
ulating creatorsâ€™ received utilities with respect to the cumulative
user satisfaction about the recommended content. We present a se-
ries of approaches to implement the interventions with theoretical
justifications.
To validate the effectiveness of our approach, we conducted of-
fline experiments using both synthetic data and the MovieLens
dataset, and demonstrated how our mechanism improves user wel-
fare over time under a creator response simulator. Additionally,we deployed an online experiment on Instagram Reels, a leading
short-video recommendation platform, over a span of three weeks
and observed statistically significant and positive result in terms
of the overall user engagement and content diversity. Our model
and online experiments offer valuable insights into the design of
incentive-aware recommender platforms. To summarize, our con-
tributions can be listed as follows:
(1)We formalize the user welfare optimization problem in a com-
petitive content creation environment and identify the pri-
mary cause for potential sub-optimal outcomes: the information
asymmetry between content creators and the platform.
(2)We propose a dynamic user importance reweighting approach
with theoretical justifications for optimizing user welfare and
three implementation schemes which can be applied to various
practical scenarios.
(3)We demonstrate the effectiveness of our solution with both
offline simulations and online testing on real traffic.
2 RELATED WORK
The characterization and optimization of long-term dynamics on
content platforms involving strategic content creators has garnered
increasing attention from both theoretical [ 3,4,9,17â€“19,19,20,
29,32â€“34,36] and empirical [ 23,26] fields. Seminal works from
Ben-Porat and Tennenholtz [3,4]introduced a game theoretical
setting to model interactions between content creators and users,
and proposed the Shapley mediator to ensure the existence of a
pure Nash Equilibrium [25].
Recently, Yao et al . [32] demonstrated that due to creatorsâ€™ com-
petition, the user welfare loss under a top- ğ¾recommender systems
can be upper-bounded by ğ‘‚(1
logğ¾). This finding suggests that the
platform can improve user welfare by providing more recommenda-
tions. Building on this, the authors further proposed a category of
mechanisms for the platform to ensure a stable equilibrium and de-
veloped a computational solution to identify the optimal mechanism
for social welfare optimization [ 34]. Additionally, Zhu et al . [36]
introduced an online learning method to jointly optimize recom-
mendation policy and payment contracts for creators to maximize
accumulated utility. Hu et al . [18] designed a learning algorithm
to incentivize the creation of high-quality content. However, all
these studies rely on strong behavioral assumptions about content
creators, e.g., they can perform no-regret learning [ 32], or have
oracle access to their utility functions [ 3,4,34], so that the Nash
equilibium is achievable. Our work bridges this gap by developing
a system-side solution to optimize user welfare that even when
creators are not able to achieve Nash equilibria.
On the empirical side, Mladenov et al . [23] explored a scenario
where content creators may leave the platform if their user engage-
ment falls below a threshold. The study optimized social welfare
by solving a constrained matching problem. In a similar spirit,
Prasad et al . [26] introduced a sequential prompting policy aimed
at optimizing user welfare in equilibrium. The optimal policy was
determined through mixed integer programming. The solutions
were reported to be effective under specific behavioral assumptions
or environmental contexts, e.g., the platform can send prompts to
creators as additional signals. However, the platforms are often
constrained in their ability to influence the ecosystem. They may
3875User Welfare Optimization in Recommender Systems with Competing Content Creators KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
primarily rely on monetary incentives to motivate creators and have
limited flexibility to manipulate factors beyond matching strategies
and post-matching rewards. Our solution addresses this broader
range of scenarios, making it applicable, for example, when creators
are highly responsive to monetary incentives, and the platformâ€™s
influence is primarily exerted through adjustments to matching
probabilities and post-matching rewards.
3 THE MODELING OF CONTENT CREATION
COMPETITION
In this section, we formulate the competition among content cre-
ators (i.e., players) as a strategic game, which will serve as an envi-
ronment for the subsequent mechanism design problem. At a high
level, each creatorâ€™s utility is determined by the platformâ€™s matching
strategy and the post-matching reward function. Creators adhere to
simple, local update principles to sequentially alter their strategies,
resulting in a dynamic content distribution on the platform. The
primary objective of the platform is to optimize the cumulative
user welfare by designing its matching strategy and post-matching
reward function. Our strategic game setup builds upon and extends
the framework of Content Creator Competition ( ğ¶3) game intro-
duced in [ 32,34]. For the sake of simplicity in nomenclature, we
retain the name of ğ¶3and refer to our game as ğ¶3
ext, i.e., an exten-
sion of theğ¶3. Formally, a ğ¶3
extinstance is defined by the following
tuple: X,{Sğ‘–}ğ‘›
ğ‘–=1,ğœ,ğ›½,ğ¾,ğ‘…(Â·), which we explain in details below.
(1)Basic setups: a user distribution Xwith finite support {ğ’™ğ‘—âˆˆ
Rğ‘‘}ğ‘š
ğ‘—=1, and a set of content creators denoted by [ğ‘›]={1,Â·Â·Â·,ğ‘›}.
Each creator ğ‘–can take an action ğ’”ğ‘–, is often referred to as a
pure strategy in game-theoretic literature, from an action set
Sğ‘–âŠ‚Rğ‘‘.ğ’”ğ‘–can be understood as the embedding of content that
creatorğ‘–will produce. Without loss of generality, we assume
theğ¿2norms of any ğ’™andğ’”ğ‘–are upper bounded by 1.
(2)Relevance function: the relevance function ğœ(ğ’”,ğ’™):Rğ‘‘Ã—
Rğ‘‘â†’Râ‰¥0measures the relevance score between a user ğ’™âˆ¼X
and content ğ’”. Without loss of generality, we normalize ğœto
[0,1], where 1suggests perfect matching. We focus on modeling
the strategic behavior of creators and thus abstract away the
estimation of ğœ1. For simplicity, we use ğœğ‘–,ğ’™to denoteğœ(ğ’”ğ‘–,ğ’™)
when the joint strategy profile ğ’”=(ğ’”1,Â·Â·Â·,ğ’”ğ‘›)âˆˆS and user
profile ğ’™are clear in the context of our discussion.
(3)Matching function: Given any user ğ’™âˆˆX and when each
creator commits to a strategy ğ’”ğ‘–, the platform retrieves the top-
ğ¾ranked content in terms of the relevance scores {ğœğ‘–,ğ’™}ğ‘›
ğ‘–=1
and match one of them to ğ’™. Specifically, let{ğœğ‘™(1),ğ’™â‰¥Â·Â·Â·â‰¥
ğœğ‘™(ğ‘›),ğ’™}be a permutation of {ğœğ‘–,ğ’™}ğ‘›
ğ‘–=1, we assume that the plat-
form would pick ğ’”ğ’™âˆˆğ¿ğ’™(ğ¾;ğ’”)â‰œ{ğœğ‘™(ğ‘–),ğ’™}ğ¾
ğ‘–=1using a softmax
distribution with temperature ğ›½â‰¥02, i.e.,
ğ‘ƒğ‘–(ğ’”,ğ’™)â‰œğ‘ƒğ‘Ÿğ‘œğ‘[ğ’”ğ’™=ğ’”ğ‘™(ğ‘–)]âˆexp[ğ›½âˆ’1ğœğ‘™(ğ‘–),ğ’™],1â‰¤ğ‘–â‰¤ğ¾. (1)
1We assumeğœis learned from the offline data and ğœ(ğ’”ğ‘–,ğ’™)is an unbiased estimation
of user ğ’™â€™s satisfaction when exposed to ğ’”ğ‘–.
2The formulation in [ 32] also assumes the platform retrieve top- ğ¾content for each user,
but let the user to choose one according to the Random Utility model. The resulting
matching probability shares the same form as in Eq. (1), but differs in the sense that the
ğ›½in our setting is a parameter controlled by the platform while it is the user decision
noise in [32].A smallğ›½makes the matching strategy more deterministic, and
ğ›½â†’âˆ corresponds to random matching.
(4)User utility and welfare: When user ğ’™is matched with ğ’”,
the userâ€™s perceived utility is given by a function ğœ‹(ğ’”,ğ’™). The
user welfare ğ‘Š(ğ’”)is thus defined as the total expected utility
resulted from the matching,
ğ‘Š(ğ’”)=Eğ’™âˆ¼X[ğœ‹(ğ’”ğ’™,ğ’™)]. (2)
To simplify the technical discussions, we assume the learned rel-
evance function ğœis an unbiased estimation of ğœ‹, and therefore
ğ‘Š(ğ’”)can be simplified to
ğ‘Š(ğ’”)=Eğ’™âˆ¼X[ğœ(ğ’”ğ’™,ğ’™)]. (3)
However, our proposed solution works for general welfare
function defined in Eq (2).
(5)Creator utility: For creatorğ‘–, her utility is given by
ğ‘¢ğ‘–(ğ’”)=Eğ’™âˆˆX[ğ‘…(ğ’”ğ‘–,ğ’™)Â·ğ‘ƒğ‘–(ğ’”,ğ’™)], (4)
whereğ‘…(ğ’”ğ‘–,ğ’™)is the system-provided reward for this matching.
Natural choices of ğ‘…includeğ‘…(ğ’”ğ‘–,ğ’™)being proportional to the
userâ€™s perceived utility, or simply setting ğ‘…(ğ’”ğ‘–,ğ’™)=1(i.e.,
reward creators by the amount of traffic). Therefore, we have
ğ‘¢ğ‘–(ğ’”)=Eğ’™âˆˆX[ğœ(ğ’”ğ‘–,ğ’™)Â·ğ‘ƒğ‘–(ğ’”,ğ’™)], (5)
ğ‘¢ğ‘–(ğ’”)=Eğ’™âˆˆX[ğ‘ƒğ‘–(ğ’”,ğ’™)], (6)
Throughout the paper we adopt Eq (5)as the platformâ€™s default
choice, as it is demonstrated in [ 32] that rewarding creators by
user utility enjoys a better welfare guarantee than rewarding
them by traffic.
The most well established concept for characterizing a gameâ€™s
outcome is pure Nash equilibrium (PNE) [ 25]. At a PNE, any possible
deviation from a playerâ€™s current strategy would not increase her
utility conditioned on other playersâ€™ strategies. Under some mild
assumptions, we can prove that the PNE of our ğ¶3
extgame exists
and is unique as stated in the following theorem.
Theorem 3.1. Anyğ¶3
extgame withğ¾=ğ‘›has a unique pure Nash
equilibrium (PNE) under the utility function (6)ifğœ(Â·)is sufficiently
smooth and concave and each creator has a convex strategy set.
Theorem 3.1 guarantees the existence of a unique PNE and thus
theoretically allows the platform to establish a stable outcome.
However, in practical scenarios, we find it uninteresting to either
generalize this result or delve further into its properties for two
reasons. First, it is rare for ğ¾=ğ‘›to hold in practice because no sys-
tem will present the entire collection of content to each user. When
ğ¾<ğ‘›, the existence of a PNE becomes challenging to establish,
due to the discontinuity of the utility functions caused by the top- ğ¾
ranking operator during the matching process. Second, even when
a PNE does exist, it does not suggest that creators can consistently
reach it through sequential updates. Furthermore, the existence of
a PNE does not necessarily imply it is easily achievable in practice,
nor does it suggest an improved user welfare. In fact, as we will
demonstrate in Section 4.1, even in a simple environment with a
unique PNE, a natural updating dynamics among creators fails to
converge to the PNE and results in sub-optimal user welfare.
Therefore, we focus on a more practical solution concept called
Local Nash equilibria (LNE). While a PNE requires that all players
3876KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Yao et al.
do not want to deviate to any other strategy in the entire space, an
LNE merely stipulates players are satisfied with their strategies in
a local region. Its formal definition is given as follows.
Definition 3.2. A profile of creator strategies {ğ’”âˆ—
ğ‘–}ğ‘›
ğ‘–=1forms a local
Nash equilibrium (LNE), if for every creator ğ‘–, there exists an open set
S0
ğ‘–âˆˆSğ‘–such that ğ’”âˆ—
ğ‘–is a best response strategy within S0
ğ‘–; formally,
ğ‘¢ğ‘–(ğ’”âˆ—
ğ‘–,ğ’”âˆ—
âˆ’ğ‘–)â‰¥ğ‘¢ğ‘–(ğ’”ğ‘–,ğ’”âˆ—
âˆ’ğ‘–)for every ğ’”ğ‘–âˆˆS0
ğ‘–. (7)
We argue that LNE offers a more intuitive and practical solution
concept for consideration due to two observations. First, the strate-
gic evolution of content creation is often deeply intertwined with
creatorsâ€™ historical decisions [ 21]. This correlation stems from con-
tent generation being anchored in domain-specific expertise and
accumulated experiences, which are inherently stable attributes.
As a result, the produced content usually demonstrates path depen-
dency, posing significant challenges for creators in implementing
drastic modifications. Second, creators are typically constrained by
a lack of comprehensive insights into their utility functions due to a
limited understanding of the user demographic and the distribution
of user preferences. Given these constraints, creators are likely to
resort to incremental adjustments for strategy update.
Hence, we focus on the setting where creators engage in a re-
peated play of ğ¶3
extand employ a local searching rule termed local
better response (LBR) update for improving their strategies. The
details of LBR is presented in Algorithm 2 in Appendix A. LBR
characterizes two fundamental properties of content creation: 1. it
relies solely on point estimations of the utility function; and 2. it
only incurs local changes at each update. At each step, a creator
who decides to update her strategy would first generate an explo-
ration direction ğ’ˆğ‘–and then she would evaluate whether adjusting
her strategy in this direction results in a higher utility. If so, she
proceeds to update her strategy along ğ’ˆğ‘–in a pace of ğœ‚; otherwise,
she maintains her current strategy. This procedure closely emulates
real-world scenarios where creators strive to optimize their utilities
while having merely black-box access to the utility functions. In
practice, finding a clear direction that guarantees improved utility
can be a challenging and, at times, unrealistic task. Consequently,
we model their strategy evolution as an iterative process of trial and
error. By definition, when LBR converges in ğ¶3
ext, it must converge
to an LNE. Our primary interest lies in understanding how the
platform can devise a dynamic rewarding or matching principle
that maximizes cumulative user welfare within a given time period.
4 INTERVENTION MECHANISM DESIGN
In this section, we introduce the new intervention mechanism de-
signed to optimize user welfare. These mechanisms are intended for
the platform to influence creatorsâ€™ perceived utilities, thereby guid-
ing the evolution of their strategies toward more desirable outcomes.
We will first establish the need for platform-driven mechanism de-
sign by illustrating how suboptimal results can arise in a simplified
example without any intervention. Subsequently, we will delve into
the specifics of our proposed methods.
4.1 The Necessity of Intervention
We start with a simple illustrative example to show how the com-
petition among creators could result in quite inferior user welfare
-1.0 0.0 1.01.0
0.0
-1.0Creator Utility Landscape
0.0
0.0
0.0
0.00.20.50.8
0.80.8
0.8
0.8creator-1
creator-2
creator-3
creator-4
creator-5
0.00.20.40.60.81.0
-1.0 0.0 1.01.0
0.0
-1.0Creator Utility Landscape
0.0
0.0
0.0
0.00.20.50.8
0.80.8
0.8
0.8creator-1
creator-2
creator-3
creator-4
creator-5
0.00.20.40.60.81.0Figure 1: Visualization of creatorsâ€™ evolving strategies. Left:
no intervention, right: platform decreases the weight of the
center user by half. Creatorsâ€™ strategies are marked with
different colors, and the arrows start from initial strategies
and point to the last-iterate strategies.
inğ¶3
extwhen creators employ local update dynamics specified in
Algorithm 2. This example exhibits a stark contrast to the sound
welfare guarantee for no-regret learning [ 2] equipped creators in
[32]. Consider a ğ¶3
extinstance(X,{Sğ‘–}ğ‘›
ğ‘–=1,ğœ,ğ›½,ğ¾,ğ‘…(Â·))described
below. The user population Xis evenly distributed over the finite
set{ğ’™ğ‘—}5
ğ‘—=1={(0,0),(1,0),(0,1),(âˆ’1,0),(0,âˆ’1)}and there are
ğ‘›=5content creators, each with action set Sğ‘–=R2. The reward
function is defined as ğ‘…(ğ’”ğ‘–,ğ’™)=ğœ(ğ’”ğ‘–,ğ’™)=max{2âˆ’âˆ¥ğ’”ğ‘–âˆ’ğ’™âˆ¥2,0}
andğ›½=10,ğ¾=3. It is evident that the user welfare defined in
Eq(3)is maximized when each creator precisely targets a single
user, i.e., ğ’”ğ‘–=ğ’™ğ‘–,1â‰¤ğ‘–â‰¤5, which also represents the PNE of this
game. However, as we will illustrate through simulations, creatorsâ€™
strategies do not converge to the PNE nor optimize the user welfare
under the LBR dynamics when the platform does not intervene.
First, letâ€™s examine what happens when the platform takes no
action to guide the creators. The left panel of Figure 1 visualizes the
trajectories of strategy evolution in our constructed environment.
Initially, creatorsâ€™ strategies are randomly distributed in the region
between ğ’™1andğ’™2. Over time, ğ’™2andğ’™3are exclusively occupied
by one creator each, while ğ’™1has two creators competing for it.
The remaining creator chooses not to target either ğ’™4orğ’™5and
hovers around the region between ğ’™4andğ’™5, leaving both ğ’™4and
ğ’™5unsatisfied.
From the observed strategy evolution paths, we can deduce how
this sub-optimal situation arises. Initially, creators move in different
directions: two creators quickly converge to ğ’™2andğ’™3, while the
remaining three compete for the attention of the central user ğ’™1.
However, after this point, no creator has a strong incentive to move
closer to ğ’™4orğ’™5, as the marginal utility gained from getting closer
toğ’™4orğ’™5does not compensate for the loss incurred by moving
away from ğ’™1. Consequently, two creators decide to remain around
ğ’™1and one creator settles in a region between ğ’™4andğ’™5.
The above observations highlight the pivotal role played by the
central user ğ’™1in the occurrence of sub-optimal results. Since ğ’™1is
close to other users in the embedding space, targeting ğ’™1becomes a
popular and safe choice for creators. It secures a fraction of attention
from ğ’™1without completely sacrificing the utility gained from
other user groups. Thus, users like ğ’™1act as â€œpopular statesâ€ when
creators dynamically adjust their strategies. Whenever a creator is
located near ğ’™1, they are likely to be trapped and reluctant to explore
3877User Welfare Optimization in Recommender Systems with Competing Content Creators KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
potentially better strategies. Consequently, such â€œpopularâ€ users
end up attracting more creators, leaving other users unattended.
One immediate solution for the platform is to identify and re-
duce the impact of these â€œpopular" users. For instance, the platform
can halve the utility gained from the central user ğ’™1for each cre-
ator. This simple mechanism works effectively in this example, as
illustrated in the right panel of Figure 1. Initially, there are still
three creators converging to ğ’™1. However, due to the reduced re-
ward from ğ’™1, two creators find it less profitable to stay, driving
them to deviate towards ğ’™4andğ’™5. By assigning different impor-
tance weight for each user, the platform can reshape each creator
utility landscape and therefore influence their local search based
dynamical behaviors.
4.2 Platformâ€™s Intervention Mechanisms
The observations above motivate our design of intervention mech-
anisms that can be employed by the platform to influence creatorsâ€™
perceived utilities. These mechanisms lay the foundation for the
adaptive optimization methods we will delve into later. As a re-
minder, as defined in Eq (4), a creatorâ€™s expected utility from a
specific user ğ’™is influenced by two key factors: the probability of
creatorğ‘–being matched with user ğ’™denoted asğ‘ƒğ‘–(ğ’”,ğ’™), and the
post-matching reward assigned by the platform, denoted as ğ‘…(ğ’”ğ‘–,ğ’™).
The default choice of the platform is to set the reward function
ğ‘…(ğ’”ğ‘–,ğ’™)=ğœ(ğ’”ğ‘–,ğ’™)as in Eq (5)and the matching probability func-
tionğ‘ƒğ‘–(ğ’”,ğ’™)as the softmax over the top- ğ¾ranked content ğ’”ğ‘–as
demonstrated in Eq (1).
In the example provided in Section 4.1, the primary factors
leading to sub-optimal welfare is the presence of popular user
groups that attract excessive creator attention, making minority
user groups unnoticed by creators. To enhance overall user welfare,
it is crucial for the platform to guide creatorsâ€™ attention toward these
overlooked user groups by re-emphasizing their significance. In this
way, creators who were previously unaware of these user groups
or found them less lucrative may consider adjusting their strategies
to align more closely with those usersâ€™ preferences. To achieve this
objective, we introduce and study three different approaches for
modifying the schemes of ğ‘…(ğ’”ğ‘–,ğ’™)andğ‘ƒğ‘–(ğ’”,ğ’™), namely User Im-
portance Reweighting (UIR), Soft Matching Truncation (SMT), and
Hard Matching Truncation (HMT). These three mechanisms share
a common underlying principle, but they are designed to operate
under different scenarios, taking into account potential constraints
faced by a platform.
User Importance Reweighting (UIR) The most straightforward
approach is UIR,
ğ‘¢ğ‘–(ğ’”ğ‘–,ğ’”âˆ’ğ‘–)=Eğ’™âˆˆX[ğ‘¤(ğ’™)Â·ğ‘…(ğ’”ğ‘–,ğ’™)Â·ğ‘ƒğ‘–(ğ’”,ğ’™)], (8)
where the platform simply adjusts the post-matching rewards for
creators based on the measured importance of each user. Specifically,
if the platform believes a user has been under-served under the
current content distribution, it raises the reward for creators whose
content is consumed by such a user. Intuitively, this sends a message
to creators that â€œif you shift your content towards such users, you
will get a higher marginal reward compared to sticking to your
current content.â€ As a result, the platform can carefully design the
user weights such that a reasonable number of creators can be
successfully incentivized to serve the targeted users.Soft Matching Truncation (SMT) and Hard Matching Trunca-
tion (HMT) Both SMT and HMT function in a similar manner as
UIR but focus on manipulating the matching probability rather than
the post-matching reward by utilizing the weight ğ‘¤(ğ’™). Recall that
the probabilistic matching function ğ‘ƒis characterized by two pa-
rameters: the truncation number ğ¾(which, in practice, corresponds
to the total number of recommendation candidates retrieved for
ranking) and the temperature ğ›½(which can be viewed as a measure
of the exploration strength in the ranking model). When the plat-
form needs to signal the importance of a specific user ğ’™, it enhances
ğ’™â€™s visibility among creators, increasing the chance that creators
who were previously unaware of ğ’™start realizing the potential ben-
efits of catering to ğ’™. This can be achieved by either increasing ğ›½or
ğ¾: increasing ğ›½flattens the distribution of ğ’™â€™s matches among the
top-ğ¾candidates, while increasing ğ¾enlarges the pool of creators
exposed to ğ’™. Therefore, both of them augment the expected num-
ber of creators exposed to ğ’™. Sinceğ¾imposes a rigid threshold on
the number of creators exposed to ğ’™, whileğ›½offers a more flexible
threshold, we refer to them as Hard Matching Truncation (HMT)
and Soft Matching Truncation (SMT), respectively:
ğ‘¢ğ‘–(ğ’”ğ‘–,ğ’”âˆ’ğ‘–)=Eğ’™âˆˆX[ğ‘…(ğ’”ğ‘–,ğ’™)Â·ğ‘ƒğ‘–(ğ’”,ğ’™;ğ›½(ğ‘¤(ğ’™)),ğ¾)], (9)
ğ‘¢ğ‘–(ğ’”ğ‘–,ğ’”âˆ’ğ‘–)=Eğ’™âˆˆX[ğ‘…(ğ’”ğ‘–,ğ’™)Â·ğ‘ƒğ‘–(ğ’”,ğ’™;ğ›½,ğ¾(ğ‘¤(ğ’™)))]. (10)
We remark that UIR is more suitable when the platform possesses
the flexibility to design payment incentives for creators. However,
if the platform has limited control over payment, such as budget
constraints or other factors, SMT or HMT can be employed, as
they only require minor adjustments to the matching function. The
specific choices of increasing functions ğ›½(Â·),ğ¾(Â·)are flexible and
we leave it to the experiments.
4.3 Welfare Optimization through Adaptive
Reweighing
To implement our proposed intervention mechanisms, we need
to compute the corresponding user-specific weighting functions,
namelyğ‘¤(Â·),ğ›½(Â·), andğ¾(Â·). In this section we will use UIR as an
example to illustrate our method and let the user distribution Xbe
a uniform distribution over its support {ğ’™1,Â·Â·Â·,ğ’™ğ‘š}so thatğ‘¤(Â·)
can be parameterized by a vector ğ’˜âˆˆRğ‘š
â‰¥0. When the platform
commits to an intervention mechanism ğ’˜, the content creatorsâ€™
strategic updates according to LBR (i.e., algorithm 2) will lead their
joint strategy to an LNE ğ’”âˆ—, which determines the content distri-
bution and the total user welfare ğ‘Š. Therefore, the task of finding
the optimal ğ’˜maximizing ğ‘Šunderğ¶3
extcan be formulated as the
following bi-level optimization problem:
max
ğ’˜âˆˆRğ‘š
â‰¥0ğ‘Š(ğ’”âˆ—(ğ’˜)) (11)
s.t., ğ’”âˆ—(ğ’˜)is an LNE of ğ¶3
ext. (12)
We adopt the formulation in Eq (11)simply for presentation
purpose, as the constraint in Eq (12)is not well-defined due to the
non-uniqueness of LNE of ğ¶3
extin general. When we takcle problem
in Eq (11), we employ either LBR for simulating an ğ’”âˆ—(ğ’˜)in offline
experiments, or we directly observe ğ’”âˆ—(ğ’˜)based on the creatorsâ€™
actual responses over a period of time for online experiments. An
straightforward approach to solve Eq (11)is to use an iterative
3878KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Yao et al.
method to dynamically adjust ğ’˜, and the main challenge is to pin
down an improving direction of ğ’˜. Ideally, we can apply first-order
optimization if an estimation of the gradientğ‘‘ğ‘Š
ğ‘‘ğ’˜is available. How-
ever, the interplay between ğ’˜and ğ’”âˆ—(ğ’˜)is generally intractable
to analyze and we have to resort to heuristic methods. To get an
intuitive idea about an improving direction of ğ’˜, we consider a
stylized setting where the user population is perfectly separated
and the relevance function is given by dot-product ğœ(ğ’”,ğ’™)=ğ’”âŠ¤ğ’™.
In such a structured environment, the following theorem reveals a
useful principle for finding an improving direction of ğ’˜.
Theorem 4.1. When the number of creators ğ‘›is large enough and
the user population Xis a uniform distribution over an orthogonal
basis in Rğ‘‘, updating ğ’˜with the following formula guarantees an
improvement in ğ‘Šdefined in Eq (3):
ğ‘¤â€²
ğ‘—=ğ‘¤ğ‘—Â·ğ‘’âˆ’ğœ‚Â¯ğœ‹(ğ’™ğ‘—),âˆ€ğ‘—âˆˆ[ğ‘š], (13)
whereğœ‚is a small scalar denoting the learning rate, and Â¯ğœ‹(ğ’™ğ‘—)is the
expected utility of user ğ’™ğ‘—atğ’”âˆ—(ğ’˜).
By the definition in Eq (4), rescaling each ğ‘¤ğ‘—by a constant does
not alter the nature of problem in Eq (11). Therefore, the insight
conveyed by Eq (13)is clear: if a user enjoys a high expected utility
under the current content distribution, the platform should reduce
her weight when rewarding creators. Conversely, if a userâ€™s ex-
pected utility is relatively low, the platform needs to highlight her
significance for motivating a larger set of creators to develop con-
tent that caters to the needs of this user. Despite the fact that Eq
(13)is derived from a significantly simplified user distribution, we
will leverage it as a foundational element in the development of
our adaptive reweighing algorithm and demonstrate in our exper-
iments that this simple heuristic works pretty well for real user
distributions.
Next, we formally introduce our proposed adaptive reweighting
algorithm for optimizing the intervention mechanism ğ’˜. Each user
ğ’™is initially assigned a unit weight ğ’˜(0)(ğ’™)=1. During subsequent
iterations, the platform continuously monitors the average utility
of user ğ’™, denoted as Â¯ğœ‹(ğ’™), within a specified time window, and
updates ğ’˜according to the following (14), whereğ›¼>0is a tunable
parameter. This adjustment process employs the meta-algorithm
structure of multiplicative weight update method [1].
ğ‘¤(ğ‘–+1)(ğ’™)âˆğ‘¤(ğ‘–)(ğ’™)Â·exp(âˆ’ğ›¼Â¯ğœ‹(ğ’™)). (14)
In practice, we can choose the user utility function ğœ‹(ğ’™)as the
metric used for defining the user welfare function Eq (2). Up to
this point, our discussion has primarily focused on the assumption
thatğœ‹(ğ’™;ğ’”)âˆğœ(ğ’”ğ‘–,ğ’™). However, it is important to highlight that
ğœ‹in Eq (14)can also take alternative forms to optimize empirical
performance. For instance, it can be a function of any numerical
measurement related to user satisfaction (e.g., click-through rate).
To reduce the dimension of the user weight vector and enhance the
robustness of weight updates, we recommend that algorithm de-
signers pre-cluster users into ğ¿groups based on their static features
so that users within the same group maintain identical weights.
The platformâ€™s intervention strategy is thus parameterized by an
ğ¿-dimensional vector, ğ’˜=(ğ‘¤1,Â·Â·Â·,ğ‘¤ğ¿), with each entry denoting
the weight assigned to the corresponding user group.For a fixed time horizon ğ‘‡in which the platform plans to per-
form intervention, the platform divides the horizon into ğ¸epochs,
each with an equal length of ğ‘€(i.e.,ğ‘‡=ğ¸ğ‘€). At the start of each
epochğ‘’, the platform commits to a weight vector ğ’˜(ğ‘’)and deploy
it to one of the intervention mechanisms UIR, SMT or HMT. After
that, the platform observes and records the sequence of creatorsâ€™
strategic responses, denoted as {ğ’”(ğ‘’,ğ‘–)}ğ‘€
ğ‘–=1from the online envi-
ronment. Subsequently, the algorithm estimates the average user
welfare Â¯ğœ‹ğ‘™for each group ğ‘™. It then employs values in {Â¯ğœ‹ğ‘™}ğ¿
ğ‘™=1to
update the weights at the beginning of the (ğ‘’+1)-th epoch using
Eq(14). To prevent ğ’˜from growing or declining excessively, after
each update we first normalize and then clip its values within a pre-
determined interval [ğ‘¤min,ğ‘¤max]. The formal description of this
process is presented in Algorithm 1. The implementation details
about the deployment of UIR, SMT and HMT in Line 4 are deferred
to Appendix B.
Algorithm 1 Adaptive Reweighting
1:Input: Number of epochs ğ¸, Epoch length ğ‘€, Initial strategy
profile ğ’”(0), learning rate ğœ‚, temperature parameter ğ›¼, user
groups(ğº1,Â·Â·Â·,ğºğ¿), clipping constant ğ‘¤min,ğ‘¤max.
2:Initialization: Initial weight ğ’˜(0)=(ğ‘¤(0)
1,Â·Â·Â·,ğ‘¤(0)
ğ¿).
3:forğ‘’=0toğ¸do
4: Deploy the weight ğ’˜ğ‘’using UIR (Eq (8)), SMT (Eq (9)) or
HMT (Eq (10)).
5: Observe creatorsâ€™ strategy sequence {ğ’”(ğ‘’,ğ‘–)}ğ‘€
ğ‘–=1.
6: Compute the average user utility for each group
Â¯ğœ‹ğ‘™=1
ğ‘€|ğºğ‘™|âˆ‘ï¸
ğ’™âˆˆğºğ‘™ğ‘€âˆ‘ï¸
ğ‘–=1ğœ‹(ğ’™;ğ’”(ğ‘’,ğ‘–)).
7: Updateğ‘¤(ğ‘’+1
3)
ğ‘™=ğ‘¤(ğ‘’)
ğ‘™Â·exp(âˆ’ğ›¼Â¯ğœ‹ğ‘™),ğ‘™âˆˆ[ğ¿].
8: Normalizeğ‘¤(ğ‘’+2
3)
ğ‘™=ğ¿Â·ğ‘¤(ğ‘’+1
3)
ğ‘™/Ãğ¿
ğ‘—=1ğ‘¤(ğ‘’+1
3)
ğ‘—,ğ‘™âˆˆ[ğ¿].
9: Clip ğ’˜(ğ‘’+1)=Clip(ğ’˜(ğ‘’+2
3),ğ‘¤min,ğ‘¤max).
10: Setğ’”(ğ‘’+1)=ğ’”(ğ‘’,ğ‘€).
5 EXPERIMENTS
In this section, we evaluate our proposed intervention mechanisms
on both offline datasets and an online environment on a leading
short-video recommendation platform in the industry.
5.1 Experiments on Offline Data
We conduct simulations on ğ¶3
extgame instances constructed from
synthetic data and MovieLens-1m dataset [ 14]. In the following, we
first introduce the specification of these two simulation environ-
ments and then report the results.
5.1.1 Synthetic environment. For the synthetic environment, we
first construct the user population as follows: we fix an embed-
ding dimension ğ‘‘=5and independently sample 10cluster cen-
ters, denoted as{c1,Â·Â·Â·,c10}, from the unit sphere Sğ‘‘âˆ’1. For each
center cğ‘–, we generate users belonging to cluster- ğ‘–by indepen-
dently sampling from a Gaussian distribution N(cğ‘–,0.52ğ¼ğ‘‘). The
sizes of the 10user clusters are denoted by a vector ğ’›=10Ã—
3879User Welfare Optimization in Recommender Systems with Competing Content Creators KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(100,50,20,10,10,5,2,1,1,1). In this manner, we generate a popula-
tionXof sizeğ‘š=2000. The number of creators is set to ğ‘›=200,
and each action set Sğ‘–is set to the unit ball in Rğ‘‘. The user util-
ity and relevance score function are set to ğœ‹(ğ’”,ğ’™)=ğœ(ğ’”,ğ’™)=
max{1âˆ’âˆ¥ğ’”âˆ’ğ’™âˆ¥/3,0}. We set(ğ›½,ğ¾)to(0.1,20)by default. Such
synthetic datasets characterize a class of clustered user preference
distributions (e.g., majority vs., minority user groups).
On the creatorsâ€™ side, we let their initial strategies to be close
the center of the largest user group. This environment models a
situation where creators tend to chase popular trends by exclusively
producing content tailored to the taste of the largest user group.
We aim to investigate whether our proposed mechanisms can assist
the platform to escape from such sub-optimal states.
5.1.2 Environment constructed from MovieLens-1m. We use deep
matrix factorization [ 11] to train user and movie embeddings (with
dimension set to 32) by fitting the observed ratings in the range of 1
to 5. To ensure the quality of the trained embeddings, we performed
a 5-fold cross-validation and obtained an averaged RMSE =0.739
on the test sets. Then with the same hyper-parameter settings, we
train the user/item embeddings with the complete dataset.
We select active users with more than 200 ratings, resulting in
a populationXcomprising 1578 users. We set the number of cre-
ators to 20, with each creatorâ€™s action set Sğ‘–consisting of 1000
different movies. All {Sğ‘–}share a common part â€“ the most pop-
ular 700 movies based on the number of ratings they received,
and eachSğ‘–also has a private part â€“ a randomly sampled 300
movies. Our choice of the user utility and matching score functions
isğœ‹(ğ’”,ğ’™)=ğœ(ğ’”,ğ’™)=ğ’”âŠ¤ğ’™, and then normalized to the region [0,1].
Additionally, we set (ğ›½,ğ¾)=(0.1,20)and initialize creatorsâ€™ strate-
gies to the most preferred movie among all users (i.e., the movie
that enjoys the highest average rating among X).
5.1.3 Configurations of adaptive reweighting algorithm and inter-
vention mechanisms. For the adaptive reweighting algorithm, we set
the epoch length ğ‘€=5and the simulation time horizon ğ‘‡=3000
for both environments. During each time step within an epoch,
we simulate creatorsâ€™ responses by letting each of them update
her strategy once using Algorithm 2 in a random order. Creatorsâ€™
learning rate is set to ğœ‚=0.2. On the platform side, we use ğ¾-means
clustering to determine user groups and set the number of clusters
to20for synthetic environment and 15for MovieLens environment,
respectively. We should note as in practice, even the system does
not have the exact knowledge about user distribution, we do not
use the ground-truth clustering of users set in the simulation. In
addition, we set the temperature parameter ğ›¼=0.5for the first
half of the time period and reduce it to 0.1for the remaining period.
The clipping constants are set to (ğ‘¤min,ğ‘¤max)=(0.2,5.0)and the
mapping used in SMT and HMT are set to ğ›½(ğ’™)=ğ›½Â·ğ‘¤(ğ’™)and
ğ¾(ğ’™)=âŒˆğ¾Â·ğ‘¤(ğ’™)âŒ‰.
5.1.4 Results. Figure 2a illustrates the user welfare resulted from
creatorsâ€™ evolving strategies under the three intervention mecha-
nisms: UIR, SMT, and HMT, compared to the baseline (no platform
intervention). Over time, all three mechanisms consistently out-
perform the baseline. In the baseline (shown in blue), the welfare
plateaus quickly and remains stagnant. Conversely, the welfarecurves under the other mechanisms exhibit â€œdouble-ascent" pat-
terns. Initially, they also plateau, but eventually, they begin to rise
again and surpass the baseline. This is because, without platformâ€™s
intervention, creators tend to remain in sub-optimal equilibria
as illustrated in Section 4.1. However, our proposed mechanisms
gradually accumulate user group weights, which, when significant
enough, encourage creators to explore unattended user groups,
leading to increased welfare. Among the three mechanisms, HMT
demonstrates the most substantial gain with the least variance.
UIR, while showing a lower marginal gain, maintains stability with
minimal variance. SMT, which achieves a moderate gain, exhibits
higher variance, suggesting that directly manipulating the matching
temperature may be overly aggressive.
Figure 2b shows the learned group weights at the last iteration
of simulation. As it demonstrates, all three mechanisms emphasize
on small groups over larger ones. This outcome aligns with our
expectation: on one hand, larger user groups are more likely to
â€œtrapâ€ unnecessarily many creators and thus should be deprioritized;
on the other hand, increasing weights of niche user groups also
improve their chances of being discovered by more creators.
Figure 2c breaks down the average utilities across user groups.
The blue dashed line (i.e., the no-intervention baseline) exhibits
a positive correlation between averaged group utility and group
size, mirroring real-world observations. The orange bars show that
UIR strikes a balance by improving the utility of niche groups
while slightly trading off utility in larger groups. HMT achieves a
remarkable Pareto improvement across all groups, as indicated by
the red bars. However, SMTâ€™s gains come at the cost of even greater
skewness in the average utility distribution across groups.
To summarize, all three mechanisms show promising improve-
ments in overall user welfare, but their nature of gains differs,
introducing considerations for the platform. When condition al-
lows, HMT is the top choice due to its strong performance, stability,
and fairness. For platforms that prioritize fairness and stability, UIR
is also a viable option. However, SMT, despite improving overall
welfare, may suffer from potential drawbacks such as instability
and fairness issues. In-depth analysis of the merits and limitations
of these mechanisms remains a topic for future research.
The results in the MovieLens environment align with the in-
sights from the synthetic environment (refer to Figure 3). However,
itâ€™s worth noting that the trends in learned group weights and re-
alized group utilities do not always align with group size, which
is expected in real-world data where unattended user groups may
not necessarily have small sizes. Nevertheless, our proposed mech-
anisms continue to improve overall welfare by identifying and
prioritizing these groups.
5.2 Online Experiments
We conducted online evaluations on Instagram Reels (IG), one of
the worldâ€™s leading short-video content creation and recommenda-
tion platforms (referred to as IG hereafter), spanning over 3 weeks.
We observe that the platformâ€™s intervention can indeed influence
creator behavior because, on average, there is a positive correlation
between the delivery volume and content creation volume for each
topic. (The Pearson correlation is 0.2, and there are hundreds of
3880KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Yao et al.
0 0.5k 1k 1.5k 2k 2.5k 3k
Time Step t0.40.50.60.70.8User Welfare W(t)
User Welfare Curve
Base
UIR
SMT
HMT
(a) User welfare evolving curve.
21181615131312111098877655332
AVG. Group Size012345Avg. Group WeightComputed Weight for Each User Group
Base
UIR
SMT
HMT (b) Avg. group weight at last iteration.
21181615131312111098877655332
AVG. Group Size0.00.20.40.60.81.0Avg. Group User UtilityGroup-wise User Utility
Base
UIR
SMT
HMT (c) Avg. group-wise user utility.
Figure 2: Performance of UIR, SMT and HMT on synthetic dataset against the no-intervention baseline. Results are averaged
over 10 independently sampled synthetic environments including one-sigma error bars. ğ‘¥-axis: group sizes divided by 10.
0 0.5k 1k 1.5k 2k 2.5k 3k
Time Step t0.540.550.560.570.580.590.600.61User Welfare W(t)
User Welfare Curve
Base
UIR
SMT
HMT
(a) User welfare evolving curve.
1551351351321281061059997878683818168
AVG. Group Size012345Avg. Group WeightComputed Weight for Each User Group
Base
UIR
SMT
HMT (b) Avg. group weight at last iteration.
1551351351321281061059997878683818168
AVG. Group Size0.00.10.20.30.40.50.60.70.80.9Avg. Group User UtilityGroup-wise User Utility
Base
UIR
SMT
HMT (c) Avg. group-wise user utility.
Figure 3: Performance of UIR, SMT and HMT on MovieLens-1m dataset against the no-intervention baseline. Results are
averaged over 10 independent simulations including 0.2-sigma error bars.
topics in total). In this experiment, we employed the â€œlike-through-
rate" (LTR) as the user utility function. LTR is calculated as the ratio
of total likes to the number of impressions of a specific short video.
We opted for LTR as the chosen metric because it not only serves
as a reflection of user satisfaction but also offers a straightforward
and easily interpretable signal for content creators to assess their
contentâ€™s perceived quality. The selection of the HMT mechanism
for testing was deliberate, driven not only by its strong perfor-
mance against the baseline and other mechanisms in our offline
experiments, but also due to its ease of integration into production:
HMT solely requires changing the number of candidate content
retrieved for different users within the deployed relevance-based
ranking model.
5.2.1 Experiment Setups. We list the experiment setups below.
User clustering: We utilized explicit user characteristics such
as demographics including country and gender and their level of
activeness including video consumption volume and watch time.
This approach led to the creation of over 10,000 user groups and
we retained groups that had a sufficient number of users, resulting
in hundreds of user groups.
Cluster weight update: We implemented a daily weight updating
cadence. Each day, we assessed the satisfaction of every user group
by calculating the relative change of LTR over its average in the
previous two days. Subsequently, we recalculated the user weights
in accordance with the method outlined in Algorithm 1.
A/B test configurations: To evaluate changes in both user and
creator behavior, we employed a symmetric A/B test setup on IG.
This symmetric A/B test consisted of an experiment arm and a con-
trol arm to measure performance. At the beginning, we randomlypair 3% creators with 3% users from the entire platform for each
arm. Under this setup, users within each arm exclusively received
content created by creators within the same arm, and content cre-
ated by these creators was exclusively exposed to users within the
same arm throughout the testing period. This stringent separation
prevents any cross-group treatment leakage and maintains a closed
feedback loop within each arm. In our online experiment, we ran
these two arms for a duration of 3 weeks: a control arm adhering
to the existing production setup and a test arm where we applied
our proposed mechanism, HMT.
HMT specifics: We implemented HMT during the cold start con-
tent retrieval phase, which pertains to content created within a few
days and has not yet garnered a predefined number of impressions.
Specifically, within the IGâ€™s production pipeline, we integrated an
audience matching stage to retrieve cold start content. During this
stage, content is exclusively delivered to the most suitable user
candidates based on relevance scores generated by a pre-trained
model. In the existing production setup, a fixed relevance score
percentile of 99% is uniformly applied to all users. This means that
every user is only matched with the top 1% of cold-start content
in terms of relevance scores to ensure a high level of personaliza-
tion. When tuning the percentile, we typically observe a trade-off
between overall user satisfaction and the volume of cold-start con-
tent. In our experiment, we leveraged HMT to intelligently adjust
this threshold for different user groups, anticipating improvements
in both of thees metrics. Consequently, user groups with higher
weights were granted a higher chance to be selected by content
creators, while those with lower weights were deprioritized. The
mapping from the group weight ğ‘¤to the percentile of retrieved
3881User Welfare Optimization in Recommender Systems with Competing Content Creators KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Mapping ğ‘”in HMT
Weight <1.0<1.19<1.79<2.13<2.36<2.68â‰¥2.68
Percentile 0.99 0.95 0.90 0.85 0.75 0.7 0.1
Table 2: Gains per User Group
User Groups 1-5 6-20 21-74 75+ TOTAL
LTR +0.43% +1.40% +0.75% +1.36% +1.13%
Impression +2.64% +0.62% +1.42% +0.11% +0.76%
cold start content proportion was designed as a piece-wise constant
function, with details specified in Table 1.
5.2.2 Results. Positive results were obtained in three key aspects.
User-side engagement: The core utility metric LTR increased
by1.13% and the total impression number of cold-start content
increased by 0.76%, leading to a 3.7%increase in impressions for
fresh content created within 2 hours. These improvements are sta-
tistically significant and demonstrate increased user welfare while
enhancing the freshness and diversity of content. The gains in both
user satisfaction and the volume of cold-start content indicate that
HMT influenced many creators to produce more targeted content
that benefits niche user groups. Table 2 provides a breakdown of
performance improvement per user group. We indexed all groups in
descending order by their sizes and divided them into four columns,
with each column constituting approximately 25% of the total traffic
during the experiment period. As shown, smaller groups enjoyed
a higher gain in terms of LTR, which echoes the observations in
offline results. The gain in cold-start content impression volume
shows an opposite trend. This is because the absolute number of
cold-start impressions for larger user groups was smaller as the
distribution of relevance scores in this group was more skewed,
resulting in a larger relative gain in this metric.
Content diversity: The average number of consumed topics per
user during the experimental period increased by 0.71%, and this
increase is also statistically significant.
Creator-side engagement: For popular creators (those with more
than 1000 followers), the number of daily active users (Creator DAU)
increased by an average of 0.17%, while for the remaining creators,
the gain is 0.06%. Additionally, there is a promising increasing trend
in Creator DAU for popular creators over the three weeks of the
experiment: the increases over the first, second, and third weeks are
-0.2%, 0.24%, and 0.48%. This suggests that the three-week duration
of the experiment may have been too short to influence the majority
of creators to respond accordingly, and more time may be needed
to fully observe the positive feedback from creators.
6 CONCLUSION
In this study, we tackle the user welfare optimization challenge
faced by online content recommendation platforms through the
lens of mechanism design. We identified myopic strategy updates
among creators caused by their limited information access as the
culprit of sub-optimal welfare and introduced platform interven-
tions to address this issue. Our three proposed mechanisms, basedon adaptive user importance reweighting, enable platforms to con-
vey global user preference information, reshape creatorsâ€™ perceived
utilities, and influence their behaviors. Empirical experiments in
both offline and online environments demonstrated the effective-
ness of our approach, highlighting its potential for practical impact.
For future work, there remains an intriguing need for a compre-
hensive understanding of the merits and limitations of UIR, SMT,
and HMT to aid practitioners in selecting the most suitable mech-
anism for real-world applications. It is also important to address
practical constraints when applying the developed mechanisms.
For instance, can we find ways to jointly optimize user welfare and
platform costs? Can the mechanism explicitly ensure fairness on the
user side and producer side? Deeper insights into these questions
hold the potential to greatly impact the rapidly evolving online
content landscape and industry practices.
ACKNOWLEDGEMENT
This work is supported in part by the AI2050 program at Schmidt
Sciences (Grant G-24-66104), Army Research Office Award W911NF-
23-1-0030, ONR Award N00014-23-1-2802 and NSF Award CCF-
2303372.
REFERENCES
[1]Sanjeev Arora, Elad Hazan, and Satyen Kale. 2012. The multiplicative weights
update method: a meta-algorithm and applications. Theory of computing 8, 1
(2012), 121â€“164.
[2]E Veronica Belmega, Panayotis Mertikopoulos, Romain Negrel, and Luca San-
guinetti. 2018. Online convex optimization and no-regret learning: Algorithms,
guarantees and applications. arXiv preprint arXiv:1804.04529 (2018).
[3]Omer Ben-Porat and Moshe Tennenholtz. 2017. Shapley facility location games.
InInternational Conference on Web and Internet Economics. Springer, 58â€“73.
[4]Omer Ben-Porat and Moshe Tennenholtz. 2018. A game-theoretic approach to
recommendation systems with strategic content providers. Advances in Neural
Information Processing Systems 31 (2018).
[5]Erdem Biyik, Fan Yao, Yinlam Chow, Alex Haig, Chih-wei Hsu, Mohammad
Ghavamzadeh, and Craig Boutilier. 2023. Preference Elicitation with Soft At-
tributes in Interactive Recommendation. arXiv preprint arXiv:2311.02085 (2023).
[6]JesÃºs Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham GutiÃ©rrez.
2013. Recommender systems survey. Knowledge-based systems 46 (2013), 109â€“
132.
[7]Craig Boutilier, Martin Mladenov, and Guy Tennenholtz. 2023. Modeling Rec-
ommender Ecosystems: Research Challenges at the Intersection of Mecha-
nism Design, Reinforcement Learning and Generative Models. arXiv preprint
arXiv:2309.06375 (2023).
[8]Mario Bravo, David Leslie, and Panayotis Mertikopoulos. 2018. Bandit learning
in concave N-person games. Advances in Neural Information Processing Systems
31 (2018).
[9]Sarah Dean, Evan Dong, Meena Jagadeesan, and Liu Leqi. 2024. Recommender
Systems as Dynamical Systems: Interactions with Viewers and Creators. In
Workshop on Recommendation Ecosystems: Modeling, Optimization and Incentive
Design.
[10] Sarah Dean and Jamie Morgenstern. 2022. Preference dynamics under personal-
ized recommendations. In Proceedings of the 23rd ACM Conference on Economics
and Computation. 795â€“816.
[11] Jicong Fan and Jieyu Cheng. 2018. Matrix completion by deep matrix factorization.
Neural Networks 98 (2018), 34â€“41.
[12] Daniel Fleder and Kartik Hosanagar. 2009. Blockbuster cultureâ€™s next rise or fall:
The impact of recommender systems on sales diversity. Management science 55,
5 (2009), 697â€“712.
[13] Angela Glotfelter. 2019. Algorithmic circulation: how content creators navigate
the effects of algorithms on their work. Computers and composition 54 (2019),
102521.
[14] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History
and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015),
1â€“19.
[15] Thomas Hodgson. 2021. Spotify and the democratisation of music. Popular Music
40, 1 (2021), 1â€“17.
[16] Mattias Holmbom. 2015. The YouTuber: A qualitative study of popular content
creators.
3882KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Yao et al.
[17] Jiri Hron, Karl Krauth, Michael I Jordan, Niki Kilbertus, and Sarah Dean. 2022.
Modeling content creator incentives on algorithm-curated platforms. arXiv
preprint arXiv:2206.13102 (2022).
[18] Xinyan Hu, Meena Jagadeesan, Michael I Jordan, and Jacob Steinhard. 2023.
Incentivizing High-Quality Content in Online Recommender Systems. arXiv
preprint arXiv:2306.07479 (2023).
[19] Nicole Immorlica, Meena Jagadeesan, and Brendan Lucier. 2024. Clickbait vs.
Quality: How Engagement-Based Optimization Shapes the Content Landscape
in Online Platforms. In Proceedings of the ACM Web Conference 2024.
[20] Meena Jagadeesan, Nikhil Garg, and Jacob Steinhardt. 2022. Supply-Side Equilib-
ria in Recommender Systems. arXiv preprint arXiv:2206.13489 (2022).
[21] Hanna Kajander. 2019. Challenges of a Content Creator in the Era of Digital
Marketing. (2019).
[22] Steven George Krantz and Harold R Parks. 2002. The implicit function theorem:
history, theory, and applications. Springer Science & Business Media.
[23] Martin Mladenov, Elliot Creager, Omer Ben-Porat, Kevin Swersky, Richard Zemel,
and Craig Boutilier. 2020. Optimizing long-term social welfare in recommender
systems: A constrained matching approach. In International Conference on Ma-
chine Learning. PMLR, 6987â€“6998.
[24] Vaibhavi Nandagiri and Leena Philip. 2018. Impact of influencers from Insta-
gram and YouTube on their followers. International Journal of Multidisciplinary
Research and Modern Education 4, 1 (2018), 61â€“65.
[25] John F Nash Jr. 1950. Equilibrium points in n-person games. Proceedings of the
national academy of sciences 36, 1 (1950), 48â€“49.
[26] Siddharth Prasad, Martin Mladenov, and Craig Boutilier. 2023. Content Prompting:
Modeling Content Provider Dynamics to Improve User Welfare in Recommender
Ecosystems. arXiv preprint arXiv:2309.00940 (2023).
[27] Kun Qian and Sanjay Jain. 2022. Digital Content Creation: An Analysis of the
Impact of Recommendation Systems. Available at SSRN 4311562 (2022).
[28] J Ben Rosen. 1965. Existence and uniqueness of equilibrium points for concave
n-person games. Econometrica: Journal of the Econometric Society (1965), 520â€“534.
[29] Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, and Peng Cui. 2024. PPA-
Game: Characterizing and Learning Competitive Dynamics Among Online Con-
tent Creators. arXiv preprint arXiv:2403.15524 (2024).
[30] Fan Yao, Chuanhao Li, Denis Nekipelov, Hongning Wang, and Haifeng Xu. 2022.
Learning from a learning user for optimal recommendations. In International
Conference on Machine Learning. PMLR, 25382â€“25406.
[31] Fan Yao, Chuanhao Li, Denis Nekipelov, Hongning Wang, and Haifeng Xu. 2022.
Learning the optimal recommendation from explorative users. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 36. 9457â€“9465.
[32] Fan Yao, Chuanhao Li, Denis Nekipelov, Hongning Wang, and Haifeng Xu. 2023.
How Bad is Top- ğ¾Recommendation under Competing Content Creators?. In
International Conference on Machine Learning. PMLR.
[33] Fan Yao, Chuanhao Li, Denis Nekipelov, Hongning Wang, and Haifeng Xu. 2024.
Human vs. Generative AI in Content Creation Competition: Symbiosis or Con-
flict? arXiv preprint arXiv:2402.15467 (2024).
[34] Fan Yao, Chuanhao Li, Karthik Abinav Sankararaman, Yiming Liao, Yan Zhu,
Qifan Wang, Hongning Wang, and Haifeng Xu. 2023. Rethinking Incentives
in Recommender Systems: Are Monotone Rewards Always Beneficial? arXiv
preprint arXiv:2306.07893 (2023).
[35] Ruohan Zhan, Konstantina Christakopoulou, Ya Le, Jayden Ooi, Martin Mladenov,
Alex Beutel, Craig Boutilier, Ed Chi, and Minmin Chen. 2021. Towards content
provider aware recommender systems: A simulation study on the interplay
between user and provider utilities. In Proceedings of the Web Conference 2021.
3872â€“3883.
[36] Banghua Zhu, Sai Praneeth Karimireddy, Jiantao Jiao, and Michael I Jordan. 2023.
Online learning in a creator economy. arXiv preprint arXiv:2305.11381 (2023).
A DETAILS OF CONTENT CREATORSâ€™
STRATEGY UPDATE DYNAMICS
LBR 2 captures the evolution of creatorsâ€™ strategies in a snapshot,
and characterizes two fundamental properties of content creation:
1. it relies solely on point estimations of the utility function (Line
3); and 2. it only incurs local changes at each update (Line 4). At
each step, a creator who decides to update her strategy would
first generate an exploration direction ğ’ˆğ‘–(Line 2); then she would
evaluate whether adjusting her strategy in this direction results in
a higher utility. If so, she proceeds to update her strategy along ğ’ˆğ‘–
in a pace of ğœ‚; otherwise, she maintains her current strategy.
Algorithm 2 closely emulates real-world scenarios where cre-
ators strive to optimize their utilities while having merely black-boxaccess to the utility functions. In practice, finding a clear direction
that guarantees improved utility can be a challenging and, at times,
unrealistic task. Consequently, we model their strategy evolution
as an iterative process of trial and error. By definition, when LBR
converges in ğ¶3
ext, it must converge to an LNE. Our primary inter-
est lies in understanding how the platform can devise a dynamic
rewarding or matching principle that maximizes cumulative user
welfare within a given time period.
Algorithm 2 (LBR) Local Better Response update at time step ğ‘¡
1:Input: Learning rate ğœ‚, anğ¶3
extinstance including utility func-
tions and strategy sets (ğ‘¢ğ‘–(ğ’”),Sğ‘–)of creatorğ‘–, the joint strategy
profile ğ’”(ğ‘¡)=(ğ’”(ğ‘¡)
1,Â·Â·Â·,ğ’”(ğ‘¡)
ğ‘›)at the current step ğ‘¡.
2:Generate a random direction ğ’ˆğ‘–âˆˆSğ‘‘.
3:ifğ‘¢ğ‘–(ğ’”(ğ‘¡)
ğ‘–+ğœ‚ğ’ˆğ‘–,ğ’”(ğ‘¡)
âˆ’ğ‘–)â‰¥ğ‘¢ğ‘–(ğ’”(ğ‘¡))then
4: ğ’”(ğ‘¡+1
2)
ğ‘–=ğ’”(ğ‘¡)
ğ‘–+ğœ‚ğ’ˆğ‘–.
5: Find ğ’”(ğ‘¡+1)
ğ‘–as the projection of ğ’”(ğ‘¡+1
2)
ğ‘–inSğ‘–.
6:else
7: ğ’”(ğ‘¡+1)
ğ‘–=ğ’”(ğ‘¡)
ğ‘–
B IMPLEMENTATION DETAILS OF UIR, SMT
AND HMT MECHANISMS
The following sub-routine, denoted as Algorithm 3, outlines how
the platform deploys the weights obtained from Line 8, Algorithm 1
as an intervention mechanism in Line 4. In Algorithm 3, the weight
vector ğ’˜is directly employed to modify the reward or payment
associated with each creator-user interaction.
Algorithm 3 UIR Intervention
Input: Default recall capacity ğ¾, matching temperature ğ›½.
foreach user request ğ’™do
Compute the relevance scores {ğœ(ğ’”ğ‘–,ğ’™)}ğ‘›
ğ‘–=1.
Retrieve the top- ğ¾ranked content{ğ‘ ğ‘™(1),Â·Â·Â·,ğ‘ ğ‘™(ğ¾)}list based
on relevance scores and randomly sample one element accord-
ing to Softmax({ğ›½âˆ’1ğœ(ğ’”ğ‘™(ğ‘–),ğ’™)}ğ¾
ğ‘–=1).
For the userâ€™s choice ğ’”ğ‘–, adjust creator- ğ‘–â€™s default reward (pay-
ment) from ğ‘…(ğ’”ğ‘–,ğ’™)toğ‘¤(ğ’™)ğ‘…(ğ’”ğ‘–,ğ’™).
In the case of SMT or HMT intervention types, the platform
requires a function to map ğ‘¤(ğ’™)toğ›½(ğ’™)orğ¾(ğ’™). This mapping can
be implemented as a piecewise constant function and determined
empirically. The specifics of this process are elucidated in Algorithm
4 and 5.
C PROOF OF THEOREM 3.1
We restate Theorem 3.1 as the following with more rigorous char-
acterizations, and then provide its detailed proof.
Theorem C.1. Anyğ¶3
extgame withğ¾=ğ‘›has a unique pure
Nash equilibrium (PNE) if each creatorâ€™s srtategy set Sğ‘–is convex and
ğœ(Â·,ğ’™)is twice-differentiable and satisfies
Eğ’™âˆ¼X"
ğœ•2ğœ
ğœ•ğ’”2
ğ‘–+ğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âŠ¤#
âª¯0,âˆ€ğ‘–âˆˆ[ğ‘›]. (15)
3883User Welfare Optimization in Recommender Systems with Competing Content Creators KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 4 SMT Intervention
Input: Default recall capacity ğ¾, matching temperature ğ›½,ğ‘“:
R+â†’R+.
foreach user request ğ’™do
Compute the relevance scores {ğœ(ğ’”ğ‘–,ğ’™)}ğ‘›
ğ‘–=1.
Retrieve the top- ğ¾ranked content{ğ‘ ğ‘™(1),Â·Â·Â·,ğ‘ ğ‘™(ğ¾)}list based
on relevance scores and randomly sample one element ac-
cording to Softmax({ğ›½(ğ’™)âˆ’1ğœ(ğ’”ğ‘™(ğ‘–),ğ’™)}ğ¾
ğ‘–=1), whereğ›½(ğ’™)=
ğ‘“(ğ‘¤(ğ’™)).
Algorithm 5 HMT Intervention
Input: Default recall capacity ğ¾, matching temperature ğ›½,ğ‘”:
R+â†’N+.
foreach user request ğ’™do
Compute the relevance scores {ğœ(ğ’”ğ‘–,ğ’™)}ğ‘›
ğ‘–=1.
Retrieve the top- ğ¾(ğ’™)ranked content{ğ‘ ğ‘™(1),Â·Â·Â·,ğ‘ ğ‘™(ğ¾(ğ’™))}list
based on relevance scores and randomly sample one element
according to Softmax({ğ›½âˆ’1ğœ(ğ’”ğ‘™(ğ‘–),ğ’™)}ğ¾
ğ‘–=1(ğ’™)), whereğ¾(ğ’™)=
ğ‘”(ğ‘¤(ğ’™)).
Proof. We prove that under the proposed conditions, the ğ¶3
ext
is a strictly monotone game [ 28] and thus possesses a unique PNE.
According to Appendix A in [ 8], a sufficient condition that estab-
lishes strictly monotonicity for any ğ‘›-person gameGis convex
action sets and a negative definite Hessian [ğ»G
ğ‘–ğ‘—]ofG, which is
defined as
ğ»ğ‘–ğ‘—(ğ’”)=1
2âˆ‡ğ‘—âˆ‡ğ‘–ğ‘¢ğ‘–(ğ’”)+1
2âˆ‡ğ‘–âˆ‡ğ‘—ğ‘¢ğ‘—(ğ’”)âŠ¤.
Forğ¶3
extgame, the convexity of strategy sets are satisfied. Next
we prove the property of the gameâ€™s Hessian matrix with associated
utility function
ğ‘¢ğ‘–(ğ’”)=Eğ’™âˆ¼X"
exp(ğœ(ğ’”ğ‘–,ğ’™))Ãğ‘›
ğ‘™=1exp(ğœ(ğ’”ğ‘™,ğ’™))#
. (16)
Without loss of generality, let ğ›½=1. Denoteğ´ğ‘–=exp(ğœ(ğ’”ğ‘–,ğ’™)),ğ‘€=
ğ´1+Â·Â·Â·+ğ´ğ‘›, we have
ğ»ğ‘–ğ‘–=âˆ’Eğ’™âˆ¼Xnh
âˆ’ğœ•2ğœ
ğœ•ğ’”2
ğ‘–âˆ’ğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âŠ¤i
ğ´ğ‘–(ğ‘€âˆ’ğ´ğ‘–)Â·1
ğ‘€2
+2
ğ‘€ğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âŠ¤
ğ´2
ğ‘–(ğ‘€âˆ’ğ´ğ‘–)Â·1
ğ‘€2o
=âˆ’Eğ’™âˆ¼Xnh
âˆ’ğœ•2ğœ
ğœ•ğ’”2
ğ‘–âˆ’ğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âŠ¤
(1âˆ’ğ´ğ‘–
ğ‘€)i
Â·ğ´ğ‘–(ğ‘€âˆ’ğ´ğ‘–)1
ğ‘€2o
âˆ’Eğ’™âˆ¼Xnğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âŠ¤
ğ´2
ğ‘–(ğ‘€âˆ’ğ´ğ‘–)Â·1
ğ‘€3o
â‰œâˆ’Eğ’™âˆ¼Xh
ğ»(0)
ğ‘–ğ‘–(ğ’”,ğ’™)i
âˆ’Eğ’™âˆ¼X
ğ»(1)
ğ‘–ğ‘–(ğ’”,ğ’™)1
ğ‘€3
.
ğ»ğ‘–ğ‘—=âˆ’Eğ’™âˆ¼Xnğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘—âŠ¤
ğ´ğ‘–ğ´ğ‘—(ğ‘€âˆ’ğ´ğ‘–âˆ’ğ´ğ‘—)Â·1
ğ‘€3o
â‰œâˆ’Eğ’™âˆ¼X
ğ»(1)
ğ‘–ğ‘—(ğ’”,ğ’™)1
ğ‘€3
.Next we show that for any ğ’™andğ’”, the block matrix[ğ»(1)
ğ‘–ğ‘—]is
always positive semi-definite (PSD). For simplicity, let
ğ’šğ‘–=ğ´ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âˆˆRğ‘‘Ã—1,ğ’š=[ğ’š1;...;ğ’šğ‘›]âˆˆRğ‘‘ğ‘›Ã—1,
ğ’›=[ğ´1ğ’š1;...;ğ´ğ‘›ğ’šğ‘›]âˆˆRğ‘‘ğ‘›Ã—1,
we obtain
[ğ»(1)
ğ‘–ğ‘—]=
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ’š1ğ’šâŠ¤
1(ğ‘€âˆ’ğ´1) ğ’š1ğ’šâŠ¤
2(ğ‘€âˆ’ğ´1âˆ’ğ´2) Â·Â·Â· ğ’š1ğ’šâŠ¤ğ‘›(ğ‘€âˆ’ğ´1âˆ’ğ´ğ‘›)
ğ’š2ğ’šâŠ¤
1(ğ‘€âˆ’ğ´2âˆ’ğ´1) ğ’š2ğ’šâŠ¤
2(ğ‘€âˆ’ğ´2) Â·Â·Â· ğ’š2ğ’šâŠ¤ğ‘›(ğ‘€âˆ’ğ´2âˆ’ğ´ğ‘›)
............
ğ’šğ‘›ğ’šâŠ¤
1(ğ‘€âˆ’ğ´ğ‘›âˆ’ğ´1)ğ’šğ‘›ğ’šâŠ¤
2(ğ‘€âˆ’ğ´ğ‘›âˆ’ğ´2) Â·Â·Â· ğ’šğ‘›ğ’šâŠ¤ğ‘›(ğ‘€âˆ’ğ´ğ‘›)ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
=ğ‘€ğ’šğ’šâŠ¤âˆ’ğ’šğ’›âŠ¤âˆ’ğ’›ğ’šâŠ¤+diag(ğ´1ğ’š1ğ’šâŠ¤
1,...,ğ´ ğ‘›ğ’šğ‘›ğ’šâŠ¤
ğ‘›)
=1
ğ‘€Â·(ğ‘€ğ’šâˆ’ğ’›)(ğ‘€ğ’šâˆ’ğ’›)âŠ¤+diag(ğ´1ğ’š1ğ’šâŠ¤
1,...,ğ´ ğ‘›ğ’šğ‘›ğ’šâŠ¤
ğ‘›)âˆ’1
ğ‘€ğ’›ğ’›âŠ¤
â‰»diag(ğ´1ğ’š1ğ’šâŠ¤
1,...,ğ´ ğ‘›ğ’šğ‘›ğ’šâŠ¤
ğ‘›)âˆ’1
ğ‘€ğ’›ğ’›âŠ¤.
Therefore, it suffices to prove that the matrix
Ëœğ»=ğ‘€diag(ğ´1ğ’š1ğ’šâŠ¤
1,...,ğ´ğ‘›ğ’šğ‘›ğ’šâŠ¤
ğ‘›)âˆ’ğ’›ğ’›âŠ¤
is PSD. For any ğ’•=[ğ’•1;Â·Â·Â·;ğ’•ğ‘›]âˆˆRğ‘‘ğ‘›Ã—1where ğ’•ğ‘–âˆˆRğ‘‘, we can
verify that
ğ’•âŠ¤Ëœğ»ğ’•=ğ‘€ğ‘›âˆ‘ï¸
ğ‘–=1ğ´ğ‘–ğ’•âŠ¤
ğ‘–ğ’šğ‘–ğ’šâŠ¤
ğ‘–ğ’•ğ‘–âˆ’ğ’•âŠ¤ğ’›ğ’›âŠ¤ğ’•
=ğ‘›âˆ‘ï¸
ğ‘–=1ğ´ğ‘–ğ‘›âˆ‘ï¸
ğ‘–=1ğ´ğ‘–ğ’•âŠ¤
ğ‘–ğ’šğ‘–ğ’šâŠ¤
ğ‘–ğ’•ğ‘–âˆ’ğ’•âŠ¤ğ’›ğ’›âŠ¤ğ’•
=âˆ‘ï¸
1â‰¤ğ‘–<ğ‘—â‰¤ğ‘›ğ´ğ‘–ğ´ğ‘—(ğ’šâŠ¤
ğ‘–ğ’•ğ‘–âˆ’ğ’šâŠ¤
ğ‘—ğ’•ğ‘—)2â‰¥0.
Therefore, the block matrix [ğ»(1)
ğ‘–ğ‘—]is always PSD for any ğ’™andğ’”.
A sufficient condition for [ğ»G
ğ‘–ğ‘—]to be negative definite is thus ğ»(0)
ğ‘–ğ‘–
being positive definite (PD), i.e., ğ»(0)
ğ‘–ğ‘–(ğ’”,ğ’™)â‰»0,âˆ€ğ’”,ğ’™. It remains to
show that
Eğ’™âˆ¼X"h
âˆ’ğœ•2ğœ
ğœ•ğ’”2
ğ‘–âˆ’ğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âŠ¤
(1âˆ’ğ´ğ‘–
ğ‘€)i
Â·ğ´ğ‘–(ğ‘€âˆ’ğ´ğ‘–)1
ğ‘€2#
â‰»0.
(17)
And a sufficient condition for Eq (17) to hold is
Eğ’™âˆ¼X"
âˆ’ğœ•2ğœ
ğœ•ğ’”2
ğ‘–âˆ’ğœ•ğœ
ğœ•ğ’”ğ‘–ğœ•ğœ
ğœ•ğ’”ğ‘–âŠ¤#
âª°0, (18)
which completes the proof.
â–¡
D PROOF OF THEOREM 4.1
Proof. Since the utility functions of ğ¶3
extare twice differentiable,
any LNE ğ’”ofğ¶3
extsatisfies the following definition
ğ’”ğ‘–=arg max
ğ’›ğ‘–âˆˆğµ(ğ’”ğ‘–,ğ›¿)ğ‘¢ğ‘–(ğ’›ğ‘–,ğ’”âˆ’ğ‘–;ğ’˜) (19)
3884KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fan Yao et al.
must also satisfy the first-order conditionğœ•ğ‘¢ğ‘–
ğœ•ğ’”ğ‘–ğ’”=(ğ’”ğ‘–,ğ’”âˆ’ğ‘–)=0. If we
let
ğ¹(ğ’”,ğ’˜)=ğœ•ğ‘¢1(ğ’”;ğ’˜)
ğœ•ğ’”1,Â·Â·Â·,ğœ•ğ‘¢ğ‘›(ğ’”;ğ’˜)
ğœ•ğ’”ğ‘›
:Rğ‘‘ğ‘›Ã—Rğ‘š
â‰¥0â†’Rğ‘‘ğ‘›(20)
be a vector-valued function, the constraint (12)can be rewritten
into
ğ¹(ğ’”âˆ—(ğ’˜),ğ’˜)=0. (21)
From the implicit function theorem [ 22], the derivative of ğ’”âˆ—
w.r.t. ğ’˜can be written as
ğ‘‘ğ’”
ğ‘‘ğ’˜=âˆ’ğœ•ğ¹
ğœ•ğ’”âˆ’1
Â·ğœ•ğ¹
ğœ•ğ’˜,
whereğœ•ğ¹
ğœ•ğ’”
ğ‘›ğ‘‘Ã—ğ‘›ğ‘‘,ğœ•ğ¹
ğœ•ğ’˜
ğ‘›ğ‘‘Ã—ğ‘šare the Jacobian matrices, and
ğ‘‘ğ‘Š
ğ‘‘ğ’˜=ğ‘‘ğ‘Š
ğ‘‘ğ’”Â·ğ‘‘ğ’”
ğ‘‘ğ’˜=âˆ’ğ‘‘ğ‘Š
ğ‘‘ğ’”Â·ğœ•ğ¹
ğœ•ğ’”âˆ’1
Â·ğœ•ğ¹
ğœ•ğ’˜, (22)
where
ğ‘‘ğ‘Š
ğ‘‘ğ’”
1Ã—ğ‘›ğ‘‘is the partial derivative of ğ‘Šw.r.t. ğ’”.
Sinceğ‘¤ğ‘—â‰¥0, we apply a change of variable and denote each
ğ‘¤ğ‘—asğ‘’ğ‘¤ğ‘—instead. Next we calculate each term of the RHS of (22)
to obtain an estimation of the gradient of our objective welfare
functionğ‘Što the user weight vector ğ’˜. Without loss of generality
we let the user distribution Xbe a uniform distribution on unit
basis{e1,Â·Â·Â·,eğ‘‘}andğ‘š=ğ‘‘. The utility functions given in Eq (6)
and the user welfare function read
ğ‘Š(ğ’”)=1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘—=1ğ‘›âˆ‘ï¸
ğ‘–=1ğ’”âŠ¤
ğ‘–ğ’™ğ‘—Â·exp[ğ›½âˆ’1ğ’”âŠ¤
ğ‘–ğ’™ğ‘—]
Ãğ‘›
ğ‘˜=1exp[ğ›½âˆ’1ğ’”âŠ¤
ğ‘˜ğ’™ğ‘—]. (23)
ğ‘¢ğ‘–(ğ’”ğ‘–,ğ’”âˆ’ğ‘–)=1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘—=1ğ‘’ğ‘¤ğ‘—Â·exp[ğ›½âˆ’1ğ’”âŠ¤
ğ‘–ğ’™ğ‘—]
Ãğ‘›
ğ‘˜=1exp[ğ›½âˆ’1ğ’”âŠ¤
ğ‘˜ğ’™ğ‘—],ğ‘–âˆˆ[ğ¾].(24)
If we denote ğ´ğ‘–ğ‘—=exp[ğ›½âˆ’1ğ’”âŠ¤
ğ‘–ğ’™ğ‘—],ğ‘€ğ‘—=Ãğ‘›
ğ‘˜=1exp[ğ›½âˆ’1ğ’”âŠ¤
ğ‘˜ğ’™ğ‘—],
thenğ´ğ‘–ğ‘—
ğ‘€ğ‘—=ğ‘ƒğ‘–(ğ’”,ğ’™ğ‘—)is exactly the probability of matching content
ğ’”ğ‘–toğ’™ğ‘—. Given the assumption that ğ‘›is sufficiently large, we have
ğ´ğ‘–ğ‘—
ğ‘€ğ‘—=ğ‘œ(1)is sufficiently small for any ğ‘–and therefore we ignore the
high-order infiintesimal terms such asğ´2
ğ‘–ğ‘—
ğ‘€2
ğ‘—,ğ´ğ‘˜ğ‘—ğ´ğ‘–ğ‘—
ğ‘€2
ğ‘—in the following
derivation.
ğ‘‘ğ‘Š
ğ‘‘ğ’”ğ‘–=1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘—=1ğ’™ğ‘—"
ğ´ğ‘–ğ‘—
ğ‘€ğ‘—+ğ›½âˆ’1ğ’”âŠ¤
ğ‘–ğ’™ğ‘— 
ğ´ğ‘–ğ‘—
ğ‘€ğ‘—âˆ’ğ´2
ğ‘–ğ‘—
ğ‘€2
ğ‘—!#
âˆ’1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘—=1ğ’™ğ‘—"âˆ‘ï¸
ğ‘˜â‰ ğ‘–ğ›½âˆ’1ğ’”âŠ¤
ğ‘˜ğ’™ğ‘—ğ´ğ‘˜ğ‘—ğ´ğ‘–ğ‘—
ğ‘€2
ğ‘—#
â‰ˆ1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘—=1ğ’™ğ‘—ğ´ğ‘–ğ‘—
ğ‘€ğ‘—
1+ğ›½âˆ’1ğ’”âŠ¤
ğ‘–ğ’™ğ‘—
,ğ‘–âˆˆ[ğ‘›], (25)
where Â¯ğœ‹(ğ’™ğ‘—)â‰œÃğ‘›
ğ‘˜=1ğ’”âŠ¤
ğ‘˜ğ’™ğ‘—ğ´ğ‘˜ğ‘—
ğ‘€ğ‘—.
Next we calculate each term in the RHS of Eq (22). Theğ‘–-th block
ofğ¹(ğ’”,ğ’˜)is ağ‘‘-dimensional vector given byğ¹(ğ’”,ğ’˜)ğ‘–=1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘—=1ğ’™ğ‘—"
ğ›½âˆ’1ğ‘’ğ‘¤ğ‘— 
ğ´ğ‘–ğ‘—
ğ‘€ğ‘—âˆ’ğ´2
ğ‘–ğ‘—
ğ‘€2
ğ‘—!#
â‰ˆ1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘—=1ğ’™ğ‘—ğ´ğ‘–ğ‘—
ğ‘€ğ‘—ğ›½âˆ’1ğ‘’ğ‘¤ğ‘—,ğ‘–âˆˆ[ğ‘›], (26)
the(ğ‘–,ğ‘—)-th block ofğœ•ğ¹
ğœ•ğ’˜is ağ‘‘-dimensional vector given by
ğœ•ğ¹
ğœ•ğ’˜
ğ‘–ğ‘—=1
ğ‘šğ’™ğ‘—ğ›½âˆ’1ğ‘’ğ‘¤ğ‘— 
ğ´ğ‘–ğ‘—
ğ‘€ğ‘—âˆ’ğ´2
ğ‘–ğ‘—
ğ‘€2
ğ‘—!
â‰ˆ1
ğ‘šğ’™ğ‘—ğ›½âˆ’1ğ´ğ‘–ğ‘—
ğ‘€ğ‘—ğ‘’ğ‘¤ğ‘—,ğ‘–âˆˆ[ğ‘›],ğ‘—âˆˆ[ğ‘‘]. (27)
Since{ğ’™ğ‘–}ğ‘›
ğ‘–=1are orthogonal basis, the non-diagonal blocks of ma-
trixğœ•ğ¹
ğœ•ğ’”are all zero matrices and the ğ‘–-th diagonal block of matrix
ğœ•ğ¹
ğœ•ğ’”is given by
ğœ•ğ¹
ğœ•ğ’”
ğ‘–ğ‘–=1
ğ‘šğ›½2ğ‘‘âˆ‘ï¸
ğ‘—=1ğ‘’ğ‘¤ğ‘—"
ğ’™ğ‘—ğ’™âŠ¤
ğ‘— 
ğ´ğ‘–ğ‘—
ğ‘€ğ‘—âˆ’3ğ´2
ğ‘–ğ‘—
ğ‘€2
ğ‘—+2ğ´3
ğ‘–ğ‘—
ğ‘€3
ğ‘—!#
(28)
â‰ˆ1
ğ‘šğ›½2ğ‘‘âˆ‘ï¸
ğ‘—=1ğ‘’ğ‘¤ğ‘—
ğ’™ğ‘—ğ’™âŠ¤
ğ‘—ğ´ğ‘–ğ‘—
ğ‘€ğ‘—
,ğ‘–âˆˆ[ğ‘›].
Therefore, we can derive a approximation ofğ‘‘ğ‘Š
ğ‘‘ğ’˜as below:
ğ‘‘ğ‘Š
ğ‘‘ğ‘¤ğ‘—=âˆ’ğ‘‘ğ‘Š
ğ‘‘ğ’”Â·ğœ•ğ¹
ğœ•ğ’”âˆ’1
Â·ğœ•ğ¹
ğœ•ğ’˜
ğ‘—
â‰ˆâˆ’ğ‘›âˆ‘ï¸
ğ‘–=1(
1
ğ‘šğ‘‘âˆ‘ï¸
ğ‘˜=1ğ’™âŠ¤
ğ‘˜ğ´ğ‘–ğ‘˜
ğ‘€ğ‘˜
1+ğ›½âˆ’1ğ’”âŠ¤
ğ‘–ğ’™ğ‘˜
Â·ğ‘šğ›½2diagâˆ’1(ğ‘’ğ‘¤1ğ´ğ‘–1/ğ‘€1,Â·Â·Â·,ğ‘’ğ‘¤ğ¿ğ´ğ‘–ğ‘‘/ğ‘€ğ‘‘)Â·1
ğ‘šğ’™ğ‘—ğ›½âˆ’1ğ´ğ‘–ğ‘—
ğ‘€ğ‘—ğ‘’ğ‘¤ğ‘—)
=âˆ’ğ›½2
ğ‘šğ‘›âˆ‘ï¸
ğ‘–=1ğ‘’âˆ’ğ‘¤ğ‘—
1+ğ›½âˆ’1ğ’”âŠ¤
ğ‘–ğ’™ğ‘—
ğ›½âˆ’1ğ´ğ‘–ğ‘—
ğ‘€ğ‘—ğ‘’ğ‘¤ğ‘—
â‰ˆâˆ’1
ğ‘šğ‘›âˆ‘ï¸
ğ‘–=1ğ’”âŠ¤
ğ‘–ğ’™ğ‘—ğ´ğ‘–ğ‘—
ğ‘€ğ‘—(29)
=âˆ’1
ğ‘šğ‘›âˆ‘ï¸
ğ‘–=1ğœ‹(ğ’”ğ‘–,ğ’™ğ‘—)ğ‘ƒğ‘–(ğ’”,ğ’™ğ‘—)
=âˆ’1
ğ‘šEğ’”[ğœ‹(ğ’™ğ‘—)], (30)
where (29) holds because ğ›½âˆ’1>>1.
Therefore, Eq (30) suggests that the following update rule
ğ‘’ğ‘¤â€²
ğ‘—=ğ‘’ğ‘¤ğ‘—Â·ğ‘’âˆ’ğœ‚Â¯ğœ‹(ğ’™ğ‘—)(31)
aligns with the gradient direction of ğ‘Š(ğ’˜), which yields Eq (13).
â–¡
3885