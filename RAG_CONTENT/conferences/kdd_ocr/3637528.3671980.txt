Dynamic Neural Dowker Network: Approximating Persistent
Homology in Dynamic Directed Graphs
Hao Li
Wuhan University
Electronic Information School
Wuhan, Hubei, China
whulh@whu.edu.cnHao Jiangâˆ—
Wuhan University
Electronic Information School
Wuhan, Hubei, China
jh@whu.edu.cnFan Jiajun
Wuhan University
Electronic Information School
Wuhan, Hubei, China
2017301200224@whu.edu.cn
Dongsheng Ye
Wuhan University
Electronic Information School
Wuhan, Hubei, China
YesDong@whu.edu.cnLiang Du
Wuhan University
Electronic Information School
Wuhan, Hubei, China
duliang@whu.edu.cn
ABSTRACT
Persistent homology, a fundamental technique within Topological
Data Analysis (TDA), captures structural and shape characteristics
of graphs, yet encounters computational difficulties when applied
to dynamic directed graphs. This paper introduces the Dynamic
Neural Dowker Network (DNDN), a novel framework specifically
designed to approximate the results of dynamic Dowker filtration,
aiming to capture the high-order topological features of dynamic
directed graphs. Our approach creatively uses line graph transfor-
mations to produce both source and sink line graphs, highlighting
the shared neighbor structures that Dowker complexes focus on.
The DNDN incorporates a Source-Sink Line Graph Neural Network
(SSLGNN) layer to effectively capture the neighborhood relation-
ships among dynamic edges. Additionally, we introduce an innova-
tive duality edge fusion mechanism, ensuring that the results for
both the sink and source line graphs adhere to the duality principle
intrinsic to Dowker complexes. Our approach is validated through
comprehensive experiments on real-world datasets, demonstrating
DNDNâ€™s capability not only to effectively approximate dynamic
Dowker filtration results but also to perform exceptionally in dy-
namic graph classification tasks.
CCS CONCEPTS
â€¢Theory of computation â†’Dynamic graph algorithms; â€¢
Computing methodologies â†’Machine learning algorithms.
KEYWORDS
Topological Machine Learning, Dynamic Graphs, Graph Neural
Networks, Persistent Homology, Dowker Filtration, Line Graphs
âˆ—Corresponds Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $15.00
https://doi.org/10.1145/3637528.3671980ACM Reference Format:
Hao Li, Hao Jiang, Fan Jiajun, Dongsheng Ye, and Liang Du. 2024. Dy-
namic Neural Dowker Network: Approximating Persistent Homology in
Dynamic Directed Graphs. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671980
1 INTRODUCTION
In recent years, an increasing number of researchers are focusing on
integrating high-order topological features with graph learning for
downstream tasks such as node classification, link prediction, and
graph classification [ 6,14,27]. As a method under the framework of
TDA, persistent homology captures multi-scale features of graphs
to describe their structural and shape characteristics [ 11,14,31].
Nevertheless, when the subject of study is dynamic directed graphs
commonly found in the real world, existing methods tailored for
undirected static graphs are inadequate in capturing the topological
information of graphs.
Dynamic Dowker filtration is an effective persistent homology
method for capturing the structural and shape characteristics of
dynamic directed graphs [ 7,29,30]. As demonstrated in fig. 1a,
Dowker complexes focus on capturing the shared neighbor struc-
ture of graphs. This approach is particularly sensitive to the direc-
tion and weight of edges, a feature illustrated with a simple example
in fig. 1b. Such sensitivity makes Dowker complexes adept at an-
alyzing complex, continuously evolving dynamic directed graphs
in the real world. Their ability to discern nuanced relationships
based on edge directionality and weights allows for a more detailed
understanding of these graphsâ€™ topological characteristics.
Similar to other persistent homology methods, Dowker com-
plexes face computational challenges, struggling to efficiently han-
dle dynamic graphs. The work in [ 19] explores the capability of
neural networks to learn persistent homology features in digital
images and filtered cubical complexes. Inspired by graph neural
executors [ 24,25], we aim to develop a learning-based method to
approximate the complex results of Dowker computations. Graph
neural executors are a type of neural network designed to approx-
imate the execution of algorithms on graphs. The literature [ 28]
 
1554
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Li et al.
ğ‘£ğ‘£1
ğ‘£ğ‘£2 ğ‘£ğ‘£3ğ‘£ğ‘£4ğ‘£ğ‘£1
ğ‘£ğ‘£2ğ‘£ğ‘£4Dowker  Source Complex
Dowker  Source ComplexAdd Edge1-dim Complex
2-dim Complexğ‘¡ğ‘¡0ğ‘¡ğ‘¡1ğ‘¡ğ‘¡2
ğ‘¡ğ‘¡3ğ‘¡ğ‘¡4
ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5ğºğºğ‘ğ‘
0-dim PD of Dowker  Source Filtration
0-dim PD of Vietoris â€“Rips Filtrationğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5ğºğºğ‘ğ‘ğ‘¡ğ‘¡2ğ‘¡ğ‘¡1 ğ‘¡ğ‘¡0
ğ‘¡ğ‘¡3ğ‘¡ğ‘¡4
ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5
ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5ğºğºğ‘ğ‘ğ‘¡ğ‘¡0ğ‘¡ğ‘¡1ğ‘¡ğ‘¡2
ğ‘¡ğ‘¡3ğ‘¡ğ‘¡4
(a)
ğ‘£ğ‘£1
ğ‘£ğ‘£2 ğ‘£ğ‘£3ğ‘£ğ‘£4ğ‘£ğ‘£1
ğ‘£ğ‘£2ğ‘£ğ‘£4Dowker  Source Complex
Dowker  Source ComplexAdd Edge1-dim Complex
2-dim Complexğ‘¡ğ‘¡0ğ‘¡ğ‘¡1ğ‘¡ğ‘¡2
ğ‘¡ğ‘¡3ğ‘¡ğ‘¡4
ğºğºğ‘ğ‘
0-dim persistent  barcodes  of Dowker  source filtration
0-dim persistent barcodes  of Vietoris â€“Rips filtrationğºğºğ‘ğ‘ğ‘¡ğ‘¡2ğ‘¡ğ‘¡1 ğ‘¡ğ‘¡0
ğ‘¡ğ‘¡3ğ‘¡ğ‘¡4
ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5 ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5 ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5
ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5 ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5 ğ‘ğ‘0ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘3ğ‘ğ‘4ğ‘ğ‘5ğºğºğ‘ğ‘ğ‘¡ğ‘¡0ğ‘¡ğ‘¡1ğ‘¡ğ‘¡2
ğ‘¡ğ‘¡3ğ‘¡ğ‘¡4 (b)
Figure 1: (a) A simple example of a Dowker source complex. The existence of a shared neighbor (in this case, ğ‘£4) betweenğ‘£1and
ğ‘£2creates a higher-order relationship, which the Dowker source complex captures. (b) Illustration of Dowker filtration sensitive
to edge weights and directions in graphs: ğºğ‘represents a common type of subgraph in social media diffusion graphs. Swapping
the timestamps of two edges ( ğºğ‘) or changing the direction of an edge ( ğºğ‘) leads to different diffusion. Dowker source filtration
can effectively distinguish these three types of graphs, whereas Vietoris-Rips (VP) filtration generates the same persistent
barcode for all three cases.
has utilized graph neural networks to approximate extended per-
sistence diagrams (EPDs), demonstrating that persistent homology
results can be effectively predicted through embeddings of edges.
However, applying graph neural executors to Dowker dynamic
filtration presents two key challenges. (1) How to approximate
with a neural network the structural features captured by
Dowker complexes. Traditional graph neural networks, relying
on information transfer between a node and its neighbors, are not
adept at directly representing the computational results of Dowker
complexes. (2) How to preserve the inherent duality charac-
teristic of Dowker complexes. Dowker complexes, designed for
directed graphs, can be divided into source Dowker complexes and
sink Dowker complexes [ 1]. According to the principle of duality,
under the same filtration, the Persistence Diagrams (PDs) corre-
sponding to both types of complexes should be identical [ 8]. It is
imperative for the neural executor to ensure consistency between
these two forms of complexes. This involves designing a neural
network architecture that can simultaneously capture and reconcile
the distinct yet complementary information presented in the source
and sink Dowker complexes, reflecting their dual nature in the PDs.
To address the aforementioned challenges, we focus on line
graphs [ 2,3,18]. A line graph ğ¿(ğº)transforms the edges of a graph
ğºinto nodes of a new graph and connects edges that have a com-
mon node in ğº. As depicted in fig. 2, the line graph establishes
a critical linkage between Dowker complexes and Graph Neural
Networks (GNNs), facilitating the direct computation of edge em-
beddings essential for predicting persistent homology. Further, this
paper expands on the definition of directed line graphs. Based on
the direction of the edges, a directed graph is transformed into
ğ‘£ğ‘£1
ğ‘£ğ‘£2 ğ‘£ğ‘£3ğ‘£ğ‘£4
âƒ‘ğ‘¥ğ‘¥3(ğ‘¡ğ‘¡)âƒ‘ğ‘¥ğ‘¥1(ğ‘¡ğ‘¡)
âƒ‘ğ‘¥ğ‘¥4(ğ‘¡ğ‘¡)
âƒ‘ğ‘¥ğ‘¥2(ğ‘¡ğ‘¡)ğ‘ˆğ‘ˆ(âƒ‘ğ‘¥ğ‘¥1(ğ‘¡ğ‘¡),ğ‘€ğ‘€(âƒ‘ğ‘¥ğ‘¥1ğ‘¡ğ‘¡,âƒ‘ğ‘¥ğ‘¥4ğ‘¡ğ‘¡))âƒ‘ğ‘¥ğ‘¥1(ğ‘¡ğ‘¡+1)
Dowker  Source Complex Node- based MPNN
ğ‘’ğ‘’43 ğ‘’ğ‘’42ğ‘’ğ‘’41
ğ‘’ğ‘’42 ğ‘’ğ‘’43ğ‘’ğ‘’41âƒ‘ğ‘’ğ‘’41(ğ‘¡ğ‘¡)
âƒ‘ğ‘’ğ‘’42(ğ‘¡ğ‘¡)âƒ‘ğ‘’ğ‘’43(ğ‘¡ğ‘¡)âƒ‘ğ‘’ğ‘’41(ğ‘¡ğ‘¡+1)
ğ‘ˆğ‘ˆ(âƒ‘ğ‘’ğ‘’41ğ‘¡ğ‘¡,ğ‘€ğ‘€(âƒ‘ğ‘’ğ‘’41ğ‘¡ğ‘¡,âƒ‘ğ‘’ğ‘’42ğ‘¡ğ‘¡,âƒ‘ğ‘’ğ‘’43(ğ‘¡ğ‘¡)))
Edge- based MPNN Source Line GraphFigure 2: An example demonstrating the relationship be-
tween the source Dowker complex and the source line graph
for a directed graph. In the source line graph, the presence of
an edge between nodes representing ğ‘’41andğ‘’42reflects the
shared neighbor ( ğ‘£4) betweenğ‘£1andğ‘£2in the original graph.
This edge-based perspective is particularly allows the neu-
ral network to focus on the interactions and relationships
between edges.
source and sink line graphs, corresponding to Dowker complexes.
This approach aligns well with the duality requirement of Dowker
complexes.
Specifically, we develop the Dynamic Neural Dowker Network
(DNDN), designed to approximate the computational results of
dynamic Dowker filtration and further validated its effectiveness
 
1555Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
in dynamic graph classification tasks. Initially, we employ a line
graph transformation method to convert the original directed graph
into both source and sink line graphs, capturing the intricate edge
transformation processes in dynamic directed graphs. Subsequently,
we utilize a Source-Sink Line Graph Neural Network (SSLGNN) to
capture the neighbor structures of dynamic edges, aiming to align
with the computational outcomes of dynamic Dowker filtration
and adapt to the evolving nature of dynamic graphs. Within the
SSLGNN framework, we introduce a duality edge fusion mecha-
nism, ensuring that the neural execution results for both the sink
and source line graphs meet the duality requirements of Dowker
complexes. In essence, our main contributions can be summarized
as follows:
â€¢We proposed an innovative line graph transformation method,
which creates sink and source line graphs to directly repre-
sent the shared neighbor structures of interest in Dowker
complexes.
â€¢We developed the Dynamic Neural Dowker Network (DNDN),
leveraging Source-Sink Line Graph Neural Network to an-
alyze complex high-order structures in dynamic directed
graphs. Additionally, we designed a duality edge fusion
mechanism to align with the unique properties of Dowker
complexes.
â€¢We conducted experiments on real-world datasets and found
that DNDN performs well in approximating the computa-
tional results of dynamic Dowker filtration and in graph
classification tasks. These results demonstrate the effective-
ness of our approach in practical applications.
2 RELATED WORKS
In this section, we discuss the relevant research on dynamic graph
neural networks and persistent homology with graph learning.
2.1 Dynamic graph neural networks
Recent studies on dynamic graphs mainly rely on temporal granu-
larity and are divided into two types: discrete-time dynamic graphs
using time snapshots and continuous-time dynamic graphs with
edge timestamps [ 22]. Existing research on dynamic graph neural
networks focuses on extending traditional static GNNs to dynamic
graphs. An intuitive approach involves using time-aware encoders
to capture the dynamic evolution of nodes between discrete snap-
shots. For example, EvolveGCN [ 20] dynamically updates the GNN
model weights using LSTM and GRU. DySAT [ 21] and DHGAT [ 17]
employ attention mechanisms to capture node embeddings through
structural and temporal evolution. Recent dynamic GNNs based on
meta-learning aim to model temporal factors, with meta-learning
adeptly adapting to new temporal data. The Roland framework [ 32]
generates new node embeddings through meta real-time updates.
WinGNN [ 34] utilizes a meta-learning strategy to model associa-
tions in sliding windows of adjacent and consecutive snapshots,
without the need for specific time-aware encoders. However, these
dynamic network studies, based on neighborhood aggregation GNN
architectures, struggle to capture the global topological character-
istics of dynamic graphs from a holistic perspective. Our approach
employs graph neural networks to approximate the results of dy-
namic Dowker filtration, obtaining global topological features ofdynamic graphs and providing a new perspective on understanding
the evolution of dynamic graphs.
2.2 Persistent Homology with Graph Learning
As a significant component in topological machine learning, the in-
tegration of persistent homology with graph learning has garnered
widespread attention. Given the superiority of persistent homology
in capturing the global structural information of graphs, a consid-
erable amount of research has focused on predicting the labels of
entire graphs, that is, graph classification tasks [ 4,6,11â€“13,15,31].
Some methods [ 5,33] utilize the topological features of node neigh-
borhood subgraphs as representations for node classification tasks.
Similarly, [ 27] constructs neighborhood graphs for each pair of
target nodes and calculates their topological features, applying the
outcomes to link prediction tasks. TOGL [ 14] developed a universal
layer that integrates topological information into the hidden repre-
sentations of nodes, capable of computing topological features at
all scales, and demonstrated the efficacy of this approach in both
graph classification and node classification tasks.
Due to the high computational complexity of traditional persis-
tent homology calculations, some methods have focused on using
neural networks to approximate the results of persistent homology,
especially for large-scale graphs. Inspired by neural executors [ 24],
the work in [ 28] reinterprets the computation of extended persis-
tent homology as a prediction problem of paired edges, which can
be resolved using a union-find algorithm. Subsequently, a graph
neural network is designed to learn this union-find algorithm, and
the effectiveness of the resulting Edge-based Persistence Diagrams
(EPDs) in downstream tasks has been validated.
These studies successfully integrate the topological features of
graphs with graph learning methods. However, most of these ap-
proaches focus predominantly on static, undirected graphs, thereby
overlooking the rich and complex information present in real-world
directed dynamic graphs. Addressing this gap, our paper introduces
the Dynamic Neural Dowker Network, which focuses on machine
learning methods for computing persistent homology, an area less
explored in existing work. We combine edge-based Dowker com-
plexes with neural networks to execute persistence diagram (PD)
computations and apply this approach to dynamic graph classifica-
tion tasks. This innovation extends the application of topological
data analysis to more complex and dynamic network structures.
3 PRELIMINARY AND BACKGROUND
In this section, we first provide a brief overview of the concepts that
are relevant to persistent homology. We then describe the Dowker
filtration on graphs, and finally, we discuss the concepts related to
line graphs.
3.1 Persistent Homology
Persistent homology is a key technique in topological data analysis,
which studies the shape and structure of data. The central idea is to
analyze datasets by constructing a series of topological spaces and
observing the persistence of features across changes in a parameter.
When focusing on persistent homology in the context of graphs, the
approach typically involves examining how graph-based structures
evolve.
 
1556KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Li et al.
Given a graphG=(V,E), whereVis the set of nodes and E
is the set of edges, with ğ‘’ğ‘šğ‘›âˆˆE representing an edge between
nodesğ‘£ğ‘š,ğ‘£ğ‘›âˆˆV. The key step in computing persistent homology
involves creating a nested sequence of subgraphs G1âŠ†G 2âŠ†...âŠ†
Gğ‘˜=Gbased on a filter function ğ‘“, letCğ‘–be the simplicial complex
induced byGğ‘–. This nested sequence of simplicial complexes C1âŠ†
C2âŠ†...âŠ†Cğ‘˜is referred to as a filtration of G.
Using the additional information provided by the filtration, we
can obtain the persistent homology groups ğ»and their ranks ğ›½.
Theğ›½ğ‘–,ğ‘—
ğ‘˜captures the topological features of the graph at various
dimensions ğ‘˜, such as connected components for ğ‘˜=0and loops
forğ‘˜=1,ğ‘–andğ‘—represent the birth and death of these topological
features. The computed persistent homology can be encoded as a
multi-point set in ğ‘…2, known as the persistence diagram (PD), where
theğ‘¥andğ‘¦coordinates represent the birth and death of topological
features, respectively. This approach to analyzing graphs captures
their underlying topological characteristics, providing valuable in-
sights into their structure and dynamics beyond what is observable
through traditional graph metrics.
3.2 Dowker Filtration
The Dowker Complex is a type of simplicial complex specifically
designed for directed graphs. Given a weighted directed graph Gğ‘‘=
(Vğ‘‘,Eğ‘‘,Wğ‘‘), whereVğ‘‘is the set of nodes,Eğ‘‘is the set of directed
edges, andğ‘’ğ‘šğ‘›âˆˆEğ‘‘represents a directed edge from source node
ğ‘£ğ‘što target node ğ‘£ğ‘›, withWğ‘‘(ğ‘’ğ‘šğ‘›)being the weight of the edge
ğ‘’ğ‘šğ‘›. The Dowker ğ›¿-sink complex is defined as a simplicial complex
as follows:
ğ”‡ğ‘ ğ‘–
ğ›¿,G:={ğœ=[ğ‘£0,...,ğ‘£ ğ‘›]:there exists ğ‘£ğ‘âˆˆV
such thatW(ğ‘’ğ‘ğ‘)â‰¤ğ›¿for eachğ‘£ğ‘âˆˆ{ğ‘£ 0,...,ğ‘£ ğ‘›}}.(1)
The nodeğ‘£ğ‘is defined as a ğ›¿-sink of a simplex ğœif, for all nodes
ğ‘£ğ‘inğœ, there is a directed edge ğ‘’ğ‘ğ‘fromğ‘£ğ‘toğ‘£ğ‘with a weight less
than or equal to ğ›¿.
Similarly, the Dowker ğ›¿-source complex is symmetrically defined
with the roles of source and target reversed:
ğ”‡ğ‘ ğ‘œ
ğ›¿,G:={ğœ=[ğ‘£0,...,ğ‘£ ğ‘›]:there exists ğ‘£ğ‘âˆˆV
such thatW(ğ‘’ğ‘ğ‘)â‰¤ğ›¿for eachğ‘£ğ‘âˆˆ{ğ‘£ 0,...,ğ‘£ ğ‘›}}.(2)
With increasing ğ›¿, we naturally obtain a sequence of Dowker
complexes, known as the Dowker ğ›¿-sink filtration or Dowker ğ›¿-
source filtration. This filtration process allows for the examination
of the evolving topological structure of the graph as the parameter
ğ›¿changes, providing insights into the hierarchical and directional
properties of the graph.

ğ”‡ğ‘ ğ‘–
ğ›¿â†©â†’ğ”‡ğ‘ ğ‘–
ğ›¿â€²	
0â‰¤ğ›¿â‰¤ğ›¿â€², (3)

ğ”‡ğ‘ ğ‘œ
ğ›¿â†©â†’ğ”‡ğ‘ ğ‘œ
ğ›¿â€²	
0â‰¤ğ›¿â‰¤ğ›¿â€². (4)
According to [ 30], by converting the temporal information car-
ried by edges into weights, we can achieve a stable dynamic
Dowker filtration. Given the sensitivity of Dowker filtration to
edge directionality and weights, dynamic Dowker filtration is capa-
ble of deeply analyzing the high-order interaction relationships in
dynamic graphs as they evolve over time. This approach leveragesthe inherent temporal dynamics of edges, allowing for a more nu-
anced understanding of how graphs change and how these changes
impact the topological features of interest.
Dowker Duality. For a given directed graph Gğ‘‘, any threshold
valueğ›¿âˆˆR, and dimension ğ‘˜â‰¥0, the persistent modules induced
by the Dowker sink filtration and the Dowker source filtration
are isomorphic. This statement reflects a fundamental property
of Dowker complexes in topological data analysis. The duality of
Dowker complexes necessitates that, in designing neural networks,
we must consider both the distinctions and the eventual consistency
between source Dowker complexes and sink Dowker complexes.
3.3 Line Graph
Given an undirected graph G=(V,E)withğ¸â‰ âˆ…, the correspond-
ing line graph ğ¿(G)=(Vğ¿(G),Eğ¿(G))is defined as follows:
Vğ¿(G)={ğ‘£ğ‘’|ğ‘’âˆˆE}, (5)
Eğ¿(G)={{ğ‘£ğ‘’,ğ‘£ğ‘“}|ğ‘’â‰ ğ‘“andğ‘’âˆ©ğ‘“â‰ âˆ…}, (6)
The threshold ğ›¿in Dowker filtration is defined on the edges,
which contrasts with the node-centric approach common in tra-
ditional graph neural networks. To address this, our paper ex-
tends the concept of line graphs, introducing sink line graphs and
source line graphs, corresponding to Dowker complexes. Specifi-
cally, given a weighted directed graph Gğ‘‘=(V,E), its correspond-
ing source line graph ğ¿(Gğ‘‘)ğ‘ ğ‘œ=(Vğ‘ ğ‘œ
ğ¿(Gğ‘‘),Eğ‘ ğ‘œ
ğ¿(Gğ‘‘))and sink line
graphğ¿(Gğ‘‘)ğ‘ ğ‘–=(Vğ‘ ğ‘–
ğ¿(Gğ‘‘),Eğ‘ ğ‘–
ğ¿(Gğ‘‘))are defined as follows:
Vğ‘ ğ‘œ
ğ¿(Gğ‘‘)=Vğ‘ ğ‘–
ğ¿(Gğ‘‘)={ğ‘£ğ‘’|ğ‘’âˆˆE}, (7)
Eğ‘ ğ‘œ
ğ¿(Gğ‘‘)={(ğ‘£ğ‘,ğ‘£ğ‘)|ğ‘â‰ ğ‘,andğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’(ğ‘)=ğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’(ğ‘)},(8)
Eğ‘ ğ‘–
ğ¿(Gğ‘‘)={(ğ‘£ğ‘,ğ‘£ğ‘)|ğ‘â‰ ğ‘,andğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡(ğ‘)=ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡(ğ‘)},(9)
whereğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’(ğ‘)andğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡(ğ‘)respectively refer to the source
and target nodes of an edge ğ‘,ğ¿(Gğ‘‘)ğ‘ ğ‘œandğ¿(Gğ‘‘)ğ‘ ğ‘–, are defined as
undirected graphs.This adaptation allows the integration of edge-
centric filtration methods into graph neural network frameworks,
suitable for analyzing dynamic directed graphs.
4 METHOD
Given a dynamic directed graph Gğ‘¡=(Vğ‘¡,Eğ‘¡), our goal is to learn
a functionğ¹:Eğ‘¡â†’R2to approximate the 0-dimensional and
1-dimensional PDs under dynamic Dowker filtration. To predict
the high-order topological features of dynamic graphs, we propose
the overall framework of the Dynamic Neural Dowker Network
(DNDN) as shown in fig. 3. It specifically includes the following
modules: (1) Line Graph Embedding Module. The DNDN obtains
embeddings for each edge in the dynamic graph through a source-
sink line graph neural network combined with an edge fusion layer.
(2) Joint Prediction Module. The DNDN divides the prediction task
into two joint tasks: PD prediction and graph label prediction, and
designs the loss function incorporating the Wasserstein distance.
 
1557Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Source Edge 
EmbeddingSink Line GraphSSLGNN
SSLGNNInput  ğºğºğ‘¯ğ‘¯ğŸğŸ=ğ‘“ğ‘“(ğ‘’ğ‘’)Filtration ğ‘“ğ‘“
Source Line GraphSink Edge Embedding
Edge FusionLayer 1
Edge 
Embedding
ğ‘¯ğ‘¯ğ’ğ’SSLGNN
SSLGNNEdge FusionLayer N â€¦â€¦
0-PDs 
1-PDs 
Graph
Labels
Figure 3: The framework of DNDN
4.1 Line Graph Embedding Module
To obtain edge embeddings for predicting persistent homology un-
der dynamic Dowker filtration, this paper utilizes line graph trans-
formations to derive the source and sink line graphs corresponding
to the dynamic directed graphs. Two distinct line graph neural
networks are employed to calculate embeddings for the source and
sink line graphs, respectively. Finally, an edge fusion mechanism is
implemented to ensure the duality of Dowker complexes.
Line graph transformation. Initially, for the given input dy-
namic directed graph Gğ‘¡=(Vğ‘¡,Eğ‘¡), we transform it into source
and sink line graphs according to eq. (1) and eq. (2). The temporal at-
tributes ofEğ‘¡are transferred to the nodes in the line graphs Vğ‘ ğ‘–
ğ¿(Gğ‘¡)
andVğ‘ ğ‘œ
ğ¿(Gğ‘¡), facilitating subsequent processing by the source-sink
line graph neural network.
Source-sink line graph neural network. Next, we introduce
the backbone neural network layer used for generating edge embed-
dings in line graphs. After the transformation, the original directed
graph is divided into source line graphs and sink line graphs. We
have designed a Source-Sink Line Graph Neural Network (SSLGNN)
to generate their edge embeddings. The SSLGNN consists of two
distinct line graph neural networks and an edge fusion module.
Specifically, the computation process of the ğ‘š-th layer of SSLGNN
is as follows:
â„ğ‘š
ğ‘¢ğ‘ ğ‘œ=ğ´ğºğºğ‘šn
ğ‘€ğ‘†ğºğ‘š
â„ğ‘šâˆ’1
ğ‘£ğ‘’ğ‘“
,ğ‘£ğ‘ ğ‘œâˆˆN(ğ‘¢ğ‘ ğ‘œ)o
,â„ğ‘šâˆ’1
ğ‘¢ğ‘’ğ‘“
(10)
â„ğ‘š
ğ‘¢ğ‘ ğ‘–=ğ´ğºğºğ‘šn
ğ‘€ğ‘†ğºğ‘š
â„ğ‘šâˆ’1
ğ‘£ğ‘’ğ‘“
,ğ‘£ğ‘ ğ‘–âˆˆN(ğ‘¢ğ‘ ğ‘–)o
,â„ğ‘šâˆ’1
ğ‘¢ğ‘’ğ‘“
(11)
â„ğ‘š
ğ‘¢ğ‘’ğ‘“=ğ¸ğ‘‘ğ‘”ğ‘’ğ¹ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›
â„ğ‘š
ğ‘¢ğ‘ ğ‘œ,â„ğ‘š
ğ‘¢ğ‘ ğ‘–
(12)
whereâ„ğ‘šğ‘¢ğ‘ ğ‘œandâ„ğ‘šğ‘¢ğ‘ ğ‘–represent the features of edge ğ‘¢at theğ‘š-th
layer in the source line graph and sink line graph, respectively, while
â„ğ‘šğ‘¢ğ‘’ğ‘“denotes the features after edge fusion. N(ğ‘¢ğ‘ ğ‘œ)andN(ğ‘¢ğ‘ ğ‘–)
respectively denote the neighborhoods of edge ğ‘¢in the source line
graph and sink line graph. ğ´ğºğº represents the aggregation function,
andğ‘€ğ‘†ğº denotes the message-passing function. The configurations
ofğ´ğºğº andğ‘€ğ‘†ğº are consistent with those described in [28].The initial features of an edge â„0ğ‘¢ğ‘ ğ‘œ=â„0ğ‘¢ğ‘ ğ‘–are computed using a
dynamic Dowker filtration. We have developed a simple yet efficient
dynamic Dowker filtration W(Â·) to meet both the requirements of
dynamic Dowker duality and dynamic Dowker structural stability
[30]. The initial feature calculation is defined as follows:
â„0
ğ‘¢ğ‘ ğ‘œ=â„0
ğ‘¢ğ‘ ğ‘–=W(ğ‘¢)=ğ‘¡ğ‘¢âˆ’ğ‘¡ğ‘šğ‘–ğ‘›
ğ‘¡ğ‘šğ‘ğ‘¥âˆ’ğ‘¡ğ‘šğ‘–ğ‘›, (13)
whereğ‘¡ğ‘¢represents the time at which the edge ğ‘¢appears, and
ğ‘¡ğ‘šğ‘–ğ‘›andğ‘¡ğ‘šğ‘ğ‘¥ are respectively the minimum and maximum values
among all edge appearance times Tin the dynamic graph.
4.2 Joint Prediction Module
The topological features at different dimensions of a graph represent
various structural aspects, such as connected components for ğ‘˜=0
and loops for ğ‘˜=1. However, the quantity of Dowker PDs in a
graph does not directly correlate with the number of nodes or edges.
To better characterize topological features across dimensions, we
have designed a prediction module for simultaneously predicting
the 0-dimensional and 1-dimensional Persistence Diagrams (0-PDs
and 1-PDs) of dynamic graphs. Simultaneously, inspired by the
concept of joint learning, we have designed a graph label prediction
module to accommodate the needs of downstream tasks.
0-PD Prediction. We extend the concept from [ 31] and classify
the elements of a graphâ€™s 0-PD into three elements:
(1)Paired points(ğ‘,ğ‘), representing a connected component
born atğ‘and dying at ğ‘(merging into a higher-dimensional
topological structure).
(2)Unpaired points(ğ‘,+âˆ), representing a connected compo-
nent that is born but does not die.
(3)Disappearing points (ğ‘,ğ‘), indicating that the corresponding
connected component quickly dies after formation.
As shown in fig. 4, in graph ğºğ‘œ, withğ‘¡0<ğ‘¡1<ğ‘¡2<ğ‘¡3<ğ‘¡4,
(ğ‘¡0,+âˆ),(ğ‘¡1,+âˆ),(ğ‘¡3,+âˆ) correspond to the unpaired points for
ğ‘’ğ‘¡0,ğ‘’ğ‘¡1,ğ‘’ğ‘¡3respectively,(ğ‘¡4,ğ‘¡5)corresponds to the paired point for
ğ‘’ğ‘¡4, and the edges ğ‘’ğ‘¡2,ğ‘’ğ‘¡5forming higher-dimensional complexes
correspond to the disappearing points (ğ‘¡2,ğ‘¡2),(ğ‘¡5,ğ‘¡5). This approach
 
1558KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Li et al.
ğ‘’ğ‘’ğ‘¡ğ‘¡0
ğºğºğ‘œğ‘œğ‘’ğ‘’ğ‘¡ğ‘¡1ğ‘’ğ‘’ğ‘¡ğ‘¡2ğ‘’ğ‘’ğ‘¡ğ‘¡3ğ‘’ğ‘’ğ‘¡ğ‘¡5ğ‘’ğ‘’ğ‘¡ğ‘¡41-dim Complex
1-dim Complexğ‘£ğ‘£1
ğ‘£ğ‘£2
ğºğºğ‘šğ‘š
ğºğºğ‘›ğ‘›ğ‘£ğ‘£1
ğ‘£ğ‘£2ğ‘£ğ‘£3ğ‘’ğ‘’31
ğ‘’ğ‘’32ğ‘£ğ‘£3
0-dim Complex
Vietoris -Rips Complex
Dowker Source ComplexVietoris -Rips Complex
Dowker Source Complex
0-dim Complexğ‘£ğ‘£4
ğ‘£ğ‘£5ğ‘’ğ‘’45ğ‘’ğ‘’12
Figure 4: An example demonstrating the 0-PD of Dowker
complexes.
directly correlates edges with 0-PDs without altering the true com-
putational results. Furthermore, we can utilize an MLP layer to
directly predict the PD corresponding to an edge.
ğ‘ƒğ·0=ğ‘€ğ¿ğ‘ƒ(Hn), (14)
whereğ‘ƒğ·0represents the predicted results for 0-dimensional Per-
sistence Diagrams (0-PD), and ğ»ğ‘›denotes the edge embeddings
output by the Source-Sink Line Graph Neural Network (SSLGNN).
1-PD Prediction. The interpretation of the birth and death
of 1-dimensional Dowker PDs from a geometric perspective is
challenging, as each PD point is associated with multiple edges.
Therefore, we propose a neighborhood-based aggregation method
that utilizes dynamic Dowker filtration values W(Â·) to weight the
aggregation of each edgeâ€™s neighborhood, resulting in a subgraph
structural representation. Subsequently, a Multilayer Perceptron
(MLP) layer is employed to calculate the 1-PD.
â„ğ‘›
agg=âˆ‘ï¸
ğ‘£âˆˆN(ğ‘¢ğ‘ ğ‘–)ÃN(ğ‘¢ğ‘ ğ‘œ)W(ğ‘£)Â·â„ğ‘›
ğ‘£, (15)
ğ‘ƒğ·1=ğ‘€ğ¿ğ‘ƒ(Hğ‘›
agg), (16)
in this formulation, â„ğ‘›ğ‘£belongs to Hğ‘›and the neighborhood of edge
ğ‘¢, denoted asN(ğ‘¢ğ‘ ğ‘–)âˆªN(ğ‘¢ğ‘ ğ‘œ), includes its neighborhoods in both
the source and sink line graphs.
Graph Label Prediction. We propose a graph classification
module based on the idea of joint learning, which predicts the
labels of graphs after pooling the edge embeddings.
Ë†ğ‘¦=ğ‘“(Pooling(Hğ‘›)) (17)
where Ë†ğ‘¦represents the predicted label of the graph, and Pooling
is the pooling operation, for which we opt for max pooling in our
method.
For the PD prediction task, we employ the 2-Wasserstein distance
between the predicted results and the ground truth as the loss func-
tion. Simultaneously, for the graph classification task, we choose
cross-entropy as the loss function to train the model. This dual
approach ensures that our model is not only capable of accurately
approximating PDs but also effectively classifies graphs, leveraging
the topological features captured by the PDs for enhanced perfor-
mance in classification tasks.5 EXPERIMENTS
In this section, we provide a thorough evaluation of the proposed
Dynamic Neural Dowker Network from three perspectives. Sec-
tion 5.1 details the datasets and baselines employed in our experi-
ments. In Section 5.2, we assess the modelâ€™s proficiency in approxi-
mating true persistent homology outcomes. Section 5.3 evaluates
the modelâ€™s transferability and efficiency. Building upon the in-
sights gained from the initial experiments, Section 5.4 is dedicated
to validating the modelâ€™s performance in graph classification tasks.
The results demonstrate that our algorithm successfully approxi-
mates the computational results of Dowker complexes and can be
effectively applied to downstream graph classification tasks, with
notable efficiency and transferability to larger graphs.1
5.1 Datasets and baselines
The datasets used in the experiments are categorized into two
types: static and dynamic, with both large and small graph datasets
constructed for each category. The static datasets include REDDIT-
BINARY, REDDIT-MULTI-5K and REDDIT-MULTI-12K, where Red-
dit serves as an online forum, with nodes representing users and
edges representing discussion threads. The dynamic datasets en-
compass four categories: citation graphs, Bitcoin graphs, Q&A
graphs and social graphs. The Bitcoin graphs consist of who-trusts-
whom graphs from two Bitcoin platforms: Bitcoin OTC and Bitcoin
Alpha. The citation graphs include two distinct domains of paper
citation graphs: HEP-PH (high energy physics phenomenology) and
HEP-TH (high energy physics theory). The Q&A graphs compile
records from various websites, including StackOverflow, MathOver-
flow, SuperUser, and AskUbuntu. The social graph datasets consist
of Hashtag diffusion graphs from the Weibo platform, categorized
into two types: entertainment and current affairs topics. Table 1
displays the details of these datasets.
Table 1: Statistics of the datasets
DatasetsSmail Graphs Large Graphs
Classes GraphsAvg. Avg.GraphsAvg. Avg.
Nodes Edges Nodes Edges
REDDIT-B 2 1600 233 274 400 1212 1392
REDDIT-5K 5 4000 375 433 1000 1043 1246
REDDIT-12K 11 9507 258 277 2390 924 1066
Citation 2 400 812 898 100 2886 4288
Bitcoin 2 160 412 977 40 880 2996
Q&A 4 800 918 1397 200 4295 5795
Social 2 800 492 458 200 2713 2410
All datasets are divided into 80% small graphs and 20% large
graphs. For the two static network datasets, they are sorted in
ascending order of node count, with the top 80% categorized as
small graphs and the bottom 20% as large graphs, which are then
shuffled. The four dynamic graph datasets are sampled based on
a set target number of edges, where small and large networks are
sampled differently, ensuring no overlap between the two sizes.
1The source code of DNDN is available at https://github.com/Lihaogx/DNDN
 
1559Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
For the persistent diagram approximation experiments, the base-
lines used include PDGNN [ 28], TOGL [ 14], and RePHINE [ 15]. For
the dynamic graph classification experiments, we utilized static
graph embedding methods such as GCN [ 16], GAT [ 23], GraphSage
[10], and GIN [ 26], as well as dynamic graph embedding methods
like DHGAT [ 17], DySAT [ 21], Roland [ 32], EGCNO, and EGCNH
[20].
â€¢PDGNN [28]: A method designed to approximate the ex-
tended persistent homology of graphs based on the Vietoris-
Rips (VP) complexes.
â€¢TOGL [14]: A novel layer that integrates the persistent ho-
mology into Graph Neural Networks (GNNs) based on the
Vietoris-Rips (VP) complexes.
â€¢RePHINE [15]: A method that combines vertex- and edge-
level PH to create a more expressive topological descriptor,
which can be incorporated into GNN layers, enhancing their
ability to learn topological features.
â€¢DySAT [21]: A Euclidean dynamic graph embedding ap-
proach that employs self-attention mechanisms across both
structural and temporal layers.
â€¢DHGAT [17]: A dynamic hyperbolic graph attention net-
work that utilizes a spatiotemporal self-attention mechanism
based on hyperbolic distances.
â€¢EvolveGCN [20]: A Euclidean dynamic graph embedding
model that uses GCNs to capture structural information of
nodes and RNNs to update the GCN parameters directly.
This category includes two architectures: EGCNâˆ’Oand
EGCNâˆ’H.
â€¢Roland [32]: A Euclidean dynamic graph embedding model
that adapts static GNNs for dynamic graphs by treating node
embeddings at different GNN layers as hierarchical states
and updating them over time. It also approaches the training
process as a meta-learning problem for quick adaptation.
5.2 Approximating PD
In this subsection, we evaluate the approximation error between
the PDs predicted by different methods and the ground truth.
Experimental Setup. Consistent with the setup in [ 28], we use
the following two metrics to assess the quality of the approxima-
tion: (1) The 2-Wasserstein distance between the predicted PD and
the ground truth. (2) The total squared distance between the persis-
tence images (PI) converted from the predicted PD and the ground
truth, denoted as PIE. Our DNDN method uses the 2-Wasserstein
distance(WD) as the loss function, and the suffix _ğ‘ƒğ¼indicates that
the methodâ€™s loss function is designed based on PIE.
Experiments were conducted on small graph datasets, with the
dataset being randomly split into an 80%/20%distribution for the
training/testing set. For our Dynamic Neural Dowker Network
(DNDN) method, edge features are directly input into the graph
as initial features. On static datasets, the edge filter function is
designed based on the degree of nodes connected by edges. Con-
currently, the line graph transformation generates identical source
and sink line graphs. For models that are designed based on graph
neural networks, node features result from the aggregation of their
edgesâ€™ features.Results. As seen in table 2 and table 2, our method exhibits a
significant advantage in approximating Dowker persistent homol-
ogy results, with the best results highlighted in bold. The methods
GIN_PI and GAT_PI, which use PI as the loss function, do not
produce PD outputs, hence the 2-Wasserstein distance cannot be
calculated for them. PDGNN, designed to capture EPD, performs
suboptimally and lacks support for dynamic directed graph data,
leading to poorer performance on some dynamic datasets. Across
both static and dynamic datasets, our method significantly out-
performs the node-based GNN baselines, demonstrating that our
edge-based line graph neural network effectively captures the topo-
logical features corresponding to Dowker complexes.
5.3 Transferability and Efficiency
In this subsection, we designed experiments to investigate the trans-
ferability and efficiency of the DNDN algorithm, focusing primarily
on its adaptability to real-world large graphs that are computation-
ally expensive to analyze.
Experimental Setup. For transferability, we employed pre-
trained models obtained from training on small graph datasets
within the same category and tested them on large graph datasets.
This approach was used to verify DNDNâ€™s capability to learn topo-
logical features from small graphs of the same category and apply
this knowledge to larger graphs. Regarding efficiency, we compared
the time taken by DNDN and the GUDHI[ 9] method to compute
Dowker PDs.
Results. In table 4, "Standard" refers to the results obtained by
training directly on large graph datasets, "Pre_train" denotes the re-
sults of testing pretrained models (trained on small graph datasets)
on large graph datasets, and "Fine_tune" represents the outcomes
after fine-tuning the pretrained models for 10 epoches. Across all
datasets, the performance of models after fine-tuning surpasses
that of models trained directly from scratch. This improvement is
likely attributed to the fine-tuned models having learned more data
information. On dynamic datasets, although "Pre_train" does not
surpass Standard, "fine_tune" generally yields better results than di-
rect training. These experiments validate our modelâ€™s transferability
to large graphs, which is crucial for addressing the computational
expense of persistent homology methods on large dynamic datasets.
In table 5, we compare the efficiency of algorithms in GUDHI
and DNDN in processing Dowker PDs on large graph datasets. It is
evident that our method significantly outperforms GUDHI on large
graph datasets, demonstrating the efficiency and applicability of
DNDN in handling complex, large-scale topological data analysis
tasks.
5.4 Graph Classification
In this subsection, having validated DNDNâ€™s ability to approximate
Dowker PDs and its transferability, we explore the algorithmâ€™s
accuracy in downstream tasks.
Experimental Setup. To ascertain DNDNâ€™s efficacy in graph
classification tasks, we orchestrated two experiments: (1) Classifica-
tion on small graphs, employing 5-fold cross-validation across each
dataset for final outcome derivation. (2) Transferable classification
experiment, where the model, initially trained on small graphs,
was subjected to graph classification tasks on larger graphs. All
 
1560KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Li et al.
Table 2: Approximation error on static datasets
MethodREDDIT-B REDDIT-5K REDDIT-12K
WD PIE WD PIE WD PIE
GIN_PI - 1.78e-03 Â±7.0e-04 - 2.20e-04 Â±4.3e-04 - 5.18e-04 Â±4.3e-04
GAT_PI - 1.57e-03 Â±3.5e-04 - 5.49e-04 Â±1.7e-04 - 7.82e-04 Â±2.2e-04
GAT 0.910 Â±0.12 7.73e-03Â±1.0e-02 0.731Â±0.01 5.36e-04Â±2.0e-04 0.794Â±0.01 1.48e-03Â±3.8e-04
PDGNN 0.679 Â±0.29 2.91e-03Â±3.0e-03 0.697Â±0.04 5.10e-04Â±2.2e-04 0.744Â±0.03 5.10e-04Â±2.2e-04
TOGL 1.114 Â±0.19 1.82e-03Â±5.0e-04 0.829Â±0.08 1.95e-03Â±7.3e-04 1.021Â±0.04 1.95e-03Â±7.3e-04
RePHINE 0.816 Â±0.01 6.78e-04Â±1.4e-05 0.523Â±0.01 4.12e-04Â±2.5e-04 0.685Â±0.04 4.12e-04Â±2.5e-04
DNDN-EF 0.610 Â±0.05 7.68e-04Â±1.3e-04 0.498Â±0.07 3.78e-04Â±4.2e-05 0.595Â±0.03 3.78e-04Â±4.2e-05
DNDN 0.499Â±0.01 1.56e-04Â±1.5e-05 0.317Â±0.05 5.21e-05Â±1.5e-05 0.389Â±0.05 6.73e-05Â±1.6e-05
Table 3: Approximation error on dynamic datasets
MethodCitation Q&A Bitcoin Social
WD PIE WD PIE WD PIE WD PIE
GIN_PI - 4.71e-04 Â±1.6e-04 - 2.73e-03 Â±9.1e-04 - 5.15e-03 Â±1.8e-03 - 1.44e-03 Â±1.9e-04
GAT_PI - 7.82e-04 Â±2.2e-04 - 1.93e-03 Â±3.6e-04 - 2.80e-03 Â±7.9e-04 - 9.04e-04 Â±1.1e-04
GAT 0.960Â±0.11 1.40e-03Â±6.3e-03 2.508Â±0.11 1.09e-01Â±1.8e-01 3.185Â±1.10 1.44e-01Â±2.4e-01 0.900Â±0.01 9.20e-04Â±3.7e-04
PDGNN 1.313Â±0.44 1.87e-02Â±2.0e-02 2.016Â±0.44 6.81e-02Â±9.4e-02 3.708Â±1.74 4.13e-01Â±4.5e-01 1.010Â±0.16 2.34e-03Â±3.4e-03
TOGL 0.935Â±0.07 2.45e-03Â±2.0e-03 1.622Â±0.07 2.12e-02Â±3.3e-02 2.064Â±0.19 8.73e-03Â±3.2e-02 0.943Â±0.04 1.54e-03Â±1.3e-03
RePHINE 0.775Â±0.02 3.38e-04Â±1.5e-04 1.867Â±0.02 2.50e-02Â±8.8e-03 2.270Â±0.01 4.88e-02Â±2.3e-02 0.703Â±0.01 6.81e-04Â±4.6e-04
DNDN-EF 0.815Â±0.01 3.98e-04Â±4.7e-05 1.364Â±0.01 8.28e-03Â±1.0e-02 1.442Â±0.23 1.22e-02Â±1.1e-02 0.654Â±0.12 3.15e-04Â±3.1e-04
DNDN 0.591Â±0.02 1.29e-04Â±3.1e-05 0.804Â±0.02 1.33e-03Â±7.4e-04 0.908Â±0.04 2.13e-03Â±2.1e-04 0.514Â±0.03 1.01e-04Â±1.37e-05
Table 4: Transferability across graph datasets of varying sizes (WD)
MethodStatic Network Dynamic Network
REDDIT-B REDDIT-5K REDDIT-12K Citation Bitcoin Q & A Social
Standard 0.553Â±0.01 0.488Â±0.04 0.409Â±0.12 0.838Â±0.05 0.849Â±0.04 1.355Â±0.12 0.737Â±0.01
Pre_train 0.438Â±0.01 0.177Â±0.01 0.183Â±0.01 0.850Â±0.05 0.924Â±0.02 1.268Â±0.02 0.739Â±0.01
Fine_tune 0.424Â±0.01 0.173Â±0.01 0.176Â±0.01 0.704Â±0.01 0.831Â±0.01 1.121Â±0.02 0.708Â±0.01
Table 5: Time evaluation on different datasets (seconds)
MethodStatic Network Dynamic Network
REDDIT-B REDDIT-5K REDDIT-12K Citation Q & A Bitcoin Social
GUDHI 16.46Â±0.001 4.64Â±0.002 5.10Â±0.002 1.37Â±0.001 1.33Â±0.002 3.73Â±0.001 4.85Â±0.002
DNDN 1.69Â±0.003 0.66Â±0.003 0.70Â±0.003 0.13Â±0.003 0.14Â±0.003 0.17Â±0.001 0.31Â±0.003
experimental approaches leveraged the mean-pooling method to
derive graph embeddings for label prediction.
Results. As indicated in table 6, â€™Smallâ€™ denotes the classification
experiments conducted directly on small graph datasets, whereas
â€™Largeâ€™ refers to graph classification on larger datasets after training
on small graph datasets. Dynamic network embedding methods
like DySAT, EGCNO, and DHGAT, which focus on capturing in-
dividual nodeâ€™s evolving patterns, tend to lose some evolutionary
information after mean-pooling. Conversely, our method achieved
optimal performance in most scenarios, primarily because it learns
Dowker PDs that represent the graphâ€™s topological features, thus
capturing the graphâ€™s global characteristics. Most baseline methods
are designed around a nodeâ€™s neighborhood, failing to adequatelycapture the graphâ€™s comprehensive information. Particularly in
the transferable classification experiments, DNDN outperformed
across mostly datasets, demonstrating that insights learned from
small graph datasets can be effectively transferred to larger graph
datasets.
6 CONCLUSION
In conclusion, our study introduces the Dynamic Neural Dowker
Network (DNDN), a pioneering framework designed to tackle the
computational complexities associated with applying persistent
homology to dynamic graphs. Through a novel integration of line
graph transformations and a Source-Sink Line Graph Neural Net-
work (SSLGNN), coupled with a duality edge fusion mechanism,
 
1561Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 6: Accuracy on graph classification task
MethodStatic Network Dynamic Network
REDDIT-B REDDIT-5K Citation Q&A Bitcoin Social
Small Large Small Large Small Large Small Large Small Large Small Large
GCN 73.8Â±0.5 50.1Â±0.1 33.0Â±0.3 20.7Â±0.0 50.0Â±0.0 52.5Â±0.7 65.0Â±0.8 49.5Â±0.0 84.4Â±1.1 52.3Â±0.3 84.0Â±0.6 88.2Â±0.2
GAT 79.4Â±1.2 52.1Â±0.0 38.1Â±0.2 20.0Â±0.0 51.2Â±0.1 49.0Â±0.1 41.9Â±0.2 57.0Â±0.4 87.5Â±1.2 50.1Â±0.3 71.3Â±1.2 81.3Â±0.6
GraphSage 79.1Â±0.4 52.3Â±0.2 33.2Â±0.2 21.5Â±0.0 67.5Â±0.3 56.0Â±0.1 76.3Â±0.7 53.0Â±0.4 78.1Â±0.1 62.3Â±0.3 91.8Â±1.5 89.5Â±1.2
GIN 73.8Â±0.2 51.1Â±0.0 20.9Â±0.0 25.5Â±0.1 83.8Â±0.3 51.1Â±0.6 70.0Â±1.2 68.0Â±1.8 68.8Â±1.3 71.2Â±1.2 81.3Â±0.6 78.5Â±0.6
DySAT 78.3Â±0.7 54.5Â±0.0 32.2Â±0.1 22.3Â±0.0 86.9Â±1.3 63.0Â±1.6 78.4Â±2.5 77.3Â±2.7 87.4Â±1.4 72.2Â±0.7 91.2Â±1.5 75.2Â±2.0
DHGAT 76.4Â±0.1 56.2Â±0.1 36.7Â±0.2 21.7Â±0.0 85.8Â±1.2 65.9Â±0.3 76.7Â±0.1 78.4Â±0.2 86.4Â±0.3 65.9Â±0.3 92.3Â±0.4 86.5Â±0.2
EGCN-O 82.2Â±0.4 61.0Â±0.0 34.8Â±0.1 30.1Â±0.1 84.5Â±0.8 63.4Â±0.3 79.2Â±0.4 78.3Â±0.6 89.3Â±0.6 73.3Â±0.2 89.2Â±0.4 81.2Â±0.9
EGCN-H 82.5Â±0.5 59.5Â±0.2 32.5Â±0.0 28.3Â±0.0 86.7Â±0.2 62.4Â±0.5 80.4Â±0.5 80.2Â±0.6 89.6Â±0.4 75.5Â±0.6 88.6Â±1.2 80.5Â±1.4
Roland 84.6Â±0.2 65.2Â±0.2 47.2Â±0.1 40.2Â±0.2 87.5Â±0.2 68.2Â±0.7 78.5Â±0.6 78.6Â±0.4 80.2Â±0.2 72.1Â±0.5 90.5Â±0.2 91.2Â±0.3
DNDN 83.4Â±0.2 73.6Â±1.2 56.7Â±0.4 41.3Â±1.0 85.6Â±0.2 72.4Â±0.4 83.4Â±0.4 81.2Â±0.5 84.5Â±0.5 77.6Â±1.0 94.5Â±0.2 89.2Â±0.1
DNDN effectively captures and approximates the Dowker persistent
homology results of dynamic networks. Our experimental evalu-
ation, spanning both static and dynamic datasets, demonstrates
DNDNâ€™s superior performance in approximating true persistent
homology, highlighting its potential to enhance graph classification
tasks. Notably, the method showcases remarkable transferability,
proving its efficacy in learning topological features from smaller
graphs and applying them to larger counterparts with enhanced
efficiency. In the future, we will focus on two critical directions
to address the current limitations of DNDN: (1) extending the dy-
namic Dowker filtration method to node-level tasks by constructing
dynamic neighborhood subgraphs of nodes to study their higher-
order evolutionary patterns, and (2) building on the approximation
of 0-dimensional and 1-dimensional persistence diagrams (PDs),
exploring methods to approximate higher-dimensional PDs.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science
Foundation of China under Grant U19B2004.
REFERENCES
[1]Mehmet E Aktas, Esra Akbas, and Ahmed El Fatmaoui. 2019. Persistence homol-
ogy of networks: methods and applications. Applied Network Science 4, 1 (2019),
1â€“28.
[2]Lowell W Beineke and Jay S Bagga. 2021. Line graphs and line digraphs. Springer.
[3]Lei Cai, Jundong Li, Jie Wang, and Shuiwang Ji. 2021. Line graph neural networks
for link prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence
44, 9 (2021), 5103â€“5113.
[4]Mathieu CarriÃ¨re, FrÃ©dÃ©ric Chazal, Yuichi Ike, ThÃ©o Lacombe, Martin Royer, and
Yuhei Umeda. 2020. Perslay: A neural network layer for persistence diagrams
and new graph topological signatures. In International Conference on Artificial
Intelligence and Statistics. PMLR, 2786â€“2796.
[5]Yuzhou Chen, Baris Coskunuzer, and Yulia Gel. 2021. Topological relational
learning on graphs. Advances in neural information processing systems 34 (2021),
27029â€“27042.
[6]Yuzhou Chen, Elena Sizikova, and Yulia R Gel. 2022. TopoAttn-Nets: Topological
Attention in Graph Representation Learning. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases. Springer, 309â€“325.
[7]Samir Chowdhury and Facundo MÃ©moli. 2016. Persistent homology of directed
networks. In 2016 50th Asilomar Conference on Signals, Systems and Computers .
IEEE, 77â€“81.
[8]Clifford H Dowker. 1952. Homology groups of relations. Annals of mathematics
(1952), 84â€“95.
[9]GUDHI Editorial Board. 2015. GUDHI User and Reference Manual. The GUDHI
Project. Available at http://gudhi.gforge.inria.fr/doc/latest/.
[10] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30(2017).
[11] Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland
Kwitt. 2020. Graph filtration learning. In International Conference on Machine
Learning. PMLR, 4314â€“4323.
[12] Christoph Hofer, Roland Kwitt, Marc Niethammer, and Andreas Uhl. 2017. Deep
learning with topological signatures. Advances in neural information processing
systems 30 (2017).
[13] Christoph D Hofer, Roland Kwitt, and Marc Niethammer. 2019. Learning repre-
sentations of persistence barcodes. Journal of Machine Learning Research 20, 126
(2019), 1â€“45.
[14] Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, and
Karsten Borgwardt. 2021. Topological graph neural networks. arXiv preprint
arXiv:2102.07835 (2021).
[15] Johanna Immonen, Amauri Souza, and Vikas Garg. 2024. Going beyond persistent
homology using persistent homology. Advances in Neural Information Processing
Systems 36 (2024).
[16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[17] Hao Li, Hao Jiang, Dongsheng Ye, Qiang Wang, Liang Du, Yuanyuan Zeng,
Yingxue Wang, Cheng Chen, et al .2024. DHGAT: Hyperbolic representation
learning on dynamic graphs via attention networks. Neurocomputing 568 (2024),
127038.
[18] Jinbi Liang and Cunlai Pu. 2023. Line Graph Neural Networks for Link Weight
Prediction. arXiv preprint arXiv:2309.15728 (2023).
[19] Guido Montufar, Nina Otter, and Yu Guang Wang. 2020. Can neural networks
learn persistent homology features?. In TDA{\&}Beyond.
[20] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,
Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:
Evolving graph convolutional networks for dynamic graphs. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 34. 5363â€“5370.
[21] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2020.
Dysat: Deep neural representation learning on dynamic graphs via self-attention
networks. In Proceedings of the 13th International Conference on Web Search and
Data Mining. 519â€“527.
[22] Joakim Skarding, Bogdan Gabrys, and Katarzyna Musial. 2021. Foundations and
modeling of dynamic networks using dynamic graph neural networks: A survey.
iEEE Access 9 (2021), 79143â€“79168.
[23] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[24] Petar VeliÄkoviÄ‡, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell.
2019. Neural execution of graph algorithms. arXiv preprint arXiv:1910.10593
(2019).
[25] Louis-Pascal Xhonneux, Andreea-Ioana Deac, Petar VeliÄkoviÄ‡, and Jian Tang.
2021. How to transfer algorithmic reasoning knowledge to learn new algorithms?
Advances in Neural Information Processing Systems 34 (2021), 19500â€“19512.
[26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[27] Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, and Chao Chen. 2021. Link pre-
diction with persistent homology: An interactive view. In International conference
on machine learning. PMLR, 11659â€“11669.
[28] Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, Yusu Wang, and Chao Chen.
2022. Neural approximation of graph topological features. Advances in Neural
Information Processing Systems 35 (2022), 33357â€“33370.
 
1562KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Hao Li et al.
[29] Dongsheng Ye, Hao Jiang, Jiajun Fan, and Qiang Wang. 2024. Low-rank persistent
probability representation for higher-order role discovery. Expert Systems with
Applications 236 (2024), 121381.
[30] Dongsheng Ye, Hao Jiang, Ying Jiang, and Hao Li. 2023. Stable distance of
persistent homology for dynamic graph comparison. Knowledge-Based Systems
278 (2023), 110855.
[31] Xue Ye, Fang Sun, and Shiming Xiang. 2023. TREPH: A Plug-In Topological Layer
for Graph Neural Networks. Entropy 25, 2 (2023), 331.
[32] Jiaxuan You, Tianyu Du, and Jure Leskovec. 2022. ROLAND: graph learning
framework for dynamic graphs. In Proceedings of the 28th ACM SIGKDD conference
on knowledge discovery and data mining. 2358â€“2366.
[33] Qi Zhao, Ze Ye, Chao Chen, and Yusu Wang. 2020. Persistence enhanced graph
neural network. In International Conference on Artificial Intelligence and Statistics.
PMLR, 2896â€“2906.
[34] Yifan Zhu, Fangpeng Cong, Dan Zhang, Wenwen Gong, Qika Lin, Wenzheng Feng,
Yuxiao Dong, and Jie Tang. 2023. WinGNN: dynamic graph neural networks with
random gradient aggregation window. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3650â€“3662.
A DOWKER COMPLEXES
A.1 Diffences between VP complexes and
Dowker complexes
As fig. 5 illustrated, Vietoris-Rips (VP) complexes and Dowker com-
plexes focus on different graph structures. VP complexes directly
concern the neighbor structures on the graph, whereas Dowker
complexes are interested in the shared neighbor structures on the
graph. In graph ğºğ‘š,ğ‘£1andğ‘£2are connected by edge ğ‘’12, corre-
sponding to a one-dimensional complex. In graph ğºğ‘›,ğ‘£1andğ‘£2
correspond to the same one-dimensional complex because ğ‘’31and
ğ‘’32share a common source node ğ‘£3, even though ğ‘£1andğ‘£2may
not be directly connected by an edge. Traditional graph neural
networks aggregate the neighbors of a node, making them suit-
able for VP complexes but not necessarily for Dowker complexes.
However, we find that the relationship between ğ‘’31andğ‘’32can be
transformed into the form of a line graph, thus adapting line graph
neural networks for computation with Dowker complexes.
ğ‘’ğ‘’ğ‘¡ğ‘¡0
ğºğºğ‘œğ‘œğ‘’ğ‘’ğ‘¡ğ‘¡1ğ‘’ğ‘’ğ‘¡ğ‘¡2ğ‘’ğ‘’ğ‘¡ğ‘¡3ğ‘’ğ‘’ğ‘¡ğ‘¡5ğ‘’ğ‘’ğ‘¡ğ‘¡41-dim Complex
1-dim Complexğ‘£ğ‘£1
ğ‘£ğ‘£2
ğºğºğ‘šğ‘š
ğºğºğ‘›ğ‘›ğ‘£ğ‘£1
ğ‘£ğ‘£2ğ‘£ğ‘£3ğ‘’ğ‘’31
ğ‘’ğ‘’32ğ‘£ğ‘£3
0-dim Complex
Vietoris -Rips Complex
Dowker Source ComplexVietoris -Rips Complex
Dowker Source Complex
0-dim Complexğ‘£ğ‘£4
ğ‘£ğ‘£5ğ‘’ğ‘’45ğ‘’ğ‘’12
Figure 5: An example demonstrating the difference Vietoris-
Rips (VP) complexes and Dowker complexes.B PSEUDOCODE
In this section, we introduce the pseudocode for constructing source
line graphs and sink line graphs, referenced as algorithm 1, and the
pseudocode describing the DNDN model, cited as algorithm 2.
Algorithm 1: Algorithm to construct the source and sink
line graphs from a directed graph Gğ‘‘
Input: Directed graphGğ‘‘=(V,E), whereVis the set of
vertices andEis the set of directed edges
Output: Source line graph ğ¿(Gğ‘‘)ğ‘ ğ‘œ=(Vğ‘ ğ‘œ
ğ¿(Gğ‘‘),Eğ‘ ğ‘œ
ğ¿(Gğ‘‘))
and Sink line graph ğ¿(Gğ‘‘)ğ‘ ğ‘–=(Vğ‘ ğ‘–
ğ¿(Gğ‘‘),Eğ‘ ğ‘–
ğ¿(Gğ‘‘))
1Function ConstructLineGraphs( ğº):
2 InitializeVğ‘ ğ‘œ
ğ¿(Gğ‘‘)â†âˆ…;
3 InitializeEğ‘ ğ‘œ
ğ¿(Gğ‘‘)â†âˆ…;
4 InitializeVğ‘ ğ‘–
ğ¿(Gğ‘‘)â†âˆ…;
5 InitializeEğ‘ ğ‘–
ğ¿(Gğ‘‘)â†âˆ…;
/* Constructing the Source Line Graph */
6 foreach edgeğ‘’=(ğ‘¢,ğ‘£)âˆˆE do
7 Addğ‘’toVğ‘ ğ‘œ
ğ¿(Gğ‘‘);
8 end
9 foreach pair of edges ğ‘’1=(ğ‘¢,ğ‘£)andğ‘’2=(ğ‘¢,ğ‘¤)âˆˆE do
10 ifğ‘’1â‰ ğ‘’2then
11 Add edge(ğ‘’1,ğ‘’2)toEğ‘ ğ‘œ
ğ¿(Gğ‘‘);
12 end
13 end
/* Constructing the Sink Line Graph */
14 foreach edgeğ‘’=(ğ‘¢,ğ‘£)âˆˆE do
15 Addğ‘’toVğ‘ ğ‘–
ğ¿(Gğ‘‘);
16 end
17 foreach pair of edges ğ‘’1=(ğ‘¢,ğ‘£)andğ‘’2=(ğ‘¤,ğ‘£)âˆˆE do
18 ifğ‘’1â‰ ğ‘’2then
19 Add edge(ğ‘’1,ğ‘’2)toEğ‘ ğ‘–
ğ¿(Gğ‘‘);
20 end
21 end
22 returnğ¿(Gğ‘‘)ğ‘ ğ‘œ=(Vğ‘ ğ‘œ
ğ¿(Gğ‘‘),Eğ‘ ğ‘œ
ğ¿(Gğ‘‘)),
ğ¿(Gğ‘‘)ğ‘ ğ‘–=(Vğ‘ ğ‘–
ğ¿(Gğ‘‘),Eğ‘ ğ‘–
ğ¿(Gğ‘‘));
 
1563Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Algorithm 2: Dynamic Neural Dowker Network (DNDN)
Forward Propagation Algorithm
Input: Source line graph ğ¿(Gğ‘‘)ğ‘ ğ‘œ=(Vğ‘ ğ‘œ
ğ¿(Gğ‘‘),Eğ‘ ğ‘œ
ğ¿(Gğ‘‘));
sink line graph ğ¿(Gğ‘‘)ğ‘ ğ‘–=(Vğ‘ ğ‘–
ğ¿(Gğ‘‘),Eğ‘ ğ‘–
ğ¿(Gğ‘‘));
dynamic Dowker filtration W;aggregation function
ğ´ğºğº ; message-passing function ğ‘€ğ‘†ğº ; Edge Fusion
functionğ¸ğ‘‘ğ‘”ğ‘’ğ¹ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› ; neighborhood function N;
multilayer perceptron function ğ‘€ğ¿ğ‘ƒ ; total number
of layers in the network ğ‘€
[1]foreachğ‘¢âˆˆVğ‘ ğ‘œ
ğ¿(Gğ‘‘)=Vğ‘ ğ‘–
ğ¿(Gğ‘‘)do
[2]â„0ğ‘¢ğ‘ ğ‘œâ†W(ğ‘¢)
[3]â„0ğ‘¢ğ‘ ğ‘–â†W(ğ‘¢)
[4]end
[5]foreach layerğ‘š=1toğ‘€do
[6] foreachğ‘¢âˆˆVğ‘ ğ‘œ
ğ¿(Gğ‘‘)=Vğ‘ ğ‘–
ğ¿(Gğ‘‘)do
[7] Calculateâ„ğ‘šğ‘¢ğ‘ ğ‘œfor source line graph: â„ğ‘šğ‘¢ğ‘ ğ‘œâ†
ğ´ğºğºğ‘šn
ğ‘€ğ‘†ğºğ‘š
â„ğ‘šâˆ’1ğ‘£ğ‘’ğ‘“
,ğ‘£ğ‘ ğ‘œâˆˆN(ğ‘¢ğ‘ ğ‘œ)o
,â„ğ‘šâˆ’1ğ‘¢ğ‘’ğ‘“
[8] Calculateâ„ğ‘šğ‘¢ğ‘ ğ‘–for sink line graph: â„ğ‘šğ‘¢ğ‘ ğ‘–â†
ğ´ğºğºğ‘šn
ğ‘€ğ‘†ğºğ‘š
â„ğ‘šâˆ’1ğ‘£ğ‘’ğ‘“
,ğ‘£ğ‘ ğ‘–âˆˆN(ğ‘¢ğ‘ ğ‘–)o
,â„ğ‘šâˆ’1ğ‘¢ğ‘’ğ‘“
[9] Perform edge fusion to update features:
â„ğ‘šğ‘¢ğ‘’ğ‘“â†ğ¸ğ‘‘ğ‘”ğ‘’ğ¹ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›
â„ğ‘šğ‘¢ğ‘ ğ‘œ,â„ğ‘šğ‘¢ğ‘ ğ‘–
[10] end
[11]end
[12]Hnâ†n
â„ğ‘›ğ‘¢ğ‘’ğ‘“:ğ‘¢âˆˆVğ‘ ğ‘œ
ğ¿(Gğ‘‘)=Vğ‘ ğ‘–
ğ¿(Gğ‘‘)o
[13]ğ‘ƒğ·0â†ğ‘€ğ¿ğ‘ƒ(Hn)
[14]â„ğ‘›aggâ†Ã
ğ‘£âˆˆN(ğ‘¢ğ‘ ğ‘–)ÃN(ğ‘¢ğ‘ ğ‘œ)W(ğ‘£)Â·â„ğ‘›ğ‘£
[15]Hğ‘›aggâ†n
â„ğ‘›aggo
[16]ğ‘ƒğ·1â†ğ‘€ğ¿ğ‘ƒ(Hğ‘›agg)
 
1564