STONE: A Spatio-temporal OOD Learning Framework Kills Both
Spatial and Temporal Shifts
Binwu Wang
University of Science and Technology
of China
Hefei, China
wbw1995@mail.ustc.edu.cnJiaming Maâˆ—
University of Science and Technology
of China
Hefei, China
jiamingma@mail.ustc.edu.cnPengkun Wang
Suzhou Institute for Advanced
Research, University of Science and
Technology of China
Suzhou, China
pengkun@mail.ustc.edu.cn
Xu Wang
Suzhou Institute for Advanced
Research, University of Science and
Technology of China
Suzhou, China
wx309@mail.ustc.edu.cnYudong Zhang
University of Science and Technology
of China
Hefei, China
zyd2020@mail.ustc.edu.cnZhengyang Zhou
Suzhou Institute for Advanced
Research, University of Science and
Technology of China
Suzhou, China
zzy0929@mail.ustc.edu.cn
Yang Wangâˆ—
University of Science and Technology
of China
Hefei, China
angyan@ustc.edu.cn
ABSTRACT
Traffic prediction is a crucial task in the Intelligent Transportation
System (ITS), receiving significant attention from both industry
and academia. Numerous spatio-temporal graph convolutional net-
works have emerged for traffic prediction and achieved remarkable
success. However, these models have limitations in terms of gen-
eralization and scalability when dealing with Out-of-Distribution
(OOD) graph data with both structural and temporal shifts. To tackle
the challenges of spatio-temporal shift, we propose a framework
called STONE by learning invariable node dependencies, which
achieve stable performance in variable environments. STONE ini-
tially employs gated-transformers to extract spatial and temporal
semantic graphs. These two kinds of graphs represent spatial and
temporal dependencies, respectively. Then we design three tech-
niques to address spatio-temporal shifts. Firstly, we introduce a
FrÃ©chet embedding method that is insensitive to structural shifts,
and this embedding space can integrate loose position dependen-
cies of nodes within the graph. Secondly, we propose a graph in-
tervention mechanism to generate multiple variant environments
by perturbing two kinds of semantic graphs without any data aug-
mentations, and STONE can explore invariant node representation
Yang Wang and Jiaming Ma are corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671680from environments. Finally, we further introduce an explore-to-
extrapolate risk objective to enhance the variety of generated en-
vironments. We conduct experiments on multiple traffic datasets,
and the results demonstrate that our proposed model exhibits com-
petitive performance in terms of generalization and scalability.
CCS CONCEPTS
â€¢Computing methodologies â†’Temporal reasoning ;â€¢Informa-
tion systemsâ†’Data mining .
KEYWORDS
Spatio-temporal data mining, Out-of-distribution generalization,
Traffic prediction, Causal graph learning
ACM Reference Format:
Binwu Wang, Jiaming Maâˆ—, Pengkun Wang, Xu Wang, Yudong Zhang,
Zhengyang Zhou, and Yang Wangâˆ—. 2024. STONE: A Spatio-temporal OOD
Learning Framework Kills Both Spatial and Temporal Shifts. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671680
1 INTRODUCTION
With the increasing prevalence of GPS-enabled mobile devices and
sensors, a significant amount of spatio-temporal data is being gath-
ered in various fields such as urban transportation and atmospheric
conditions. This data has become a valuable asset, fueling progress
in the realm of urban computing [ 7,22,38,41,42,54,57,63]. Among
the many applications, predicting traffic flow is a standout task in
urban computing, providing dependable future road insights and
enhancing traffic management systems [20, 26, 27, 56, 65].
 
2948
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Binwu Wang et al.
Current prevailing traffic flow prediction method processes data
into graph-structured data incorporating relationship induction,
namely spatio-temporal graphs (STG), and then spatio-temporal
graph convolutional networks are employed as engines to learn
spatio-temporal features and make prediction [ 25,39,43,59]. Given
a graph Gand training data Xdenoted as the training environment
ğ‘’:{X,G}, the goal of STG prediction is to learn a function Fwhich
can predict target label ğ‘¦given associated input ğ‘¥:
min
FE(ğ‘¥,ğ‘¦)âˆ¼P(ğ‘¥,ğ‘¦|ğ‘’)[L(F( x),ğ‘¦)|ğ‘’]. (1)
While existing models have been highly successful, they heavily
depend on the IID assumption, which states that testing and train-
ing data are independently sourced from the same environment.
Unfortunately, many cutting-edge models like D2STGNN [ 31] and
PDFormer [ 9] are coupled with the training STG. However, this
assumption does not always hold in real-world settings where spa-
tial and temporal features of STG may change over time, leading to
varying testing environments and posing challenges related to Out-
of-Distribution (OOD) scenarios. A few recent studies [ 8,48,64]
have started exploring methods for OOD spatio-temporal learning.
Yet, these methods primarily focus on temporal shifts and overlook
significant spatial changes. In this paper, we first comprehensively
define spatio-temporal shift from two concepts: temporal shift
andspatial shift, as shown in Figure. 1.
Train Val Test
Shift
Â·Â·Â·Â·Â·Â·
Train /Val Test 
New node Removed node(a) Temporal shift 
(b)Spatial shiftEvolve
Figure 1: Spatio-temporal shift is interpreted into the tempo-
ral and spatial shifts. Temporal shift refers to the changes
in the distribution feature (such as mean and variance) of
traffic data over time. Spatial shift refers to the evolution
of the graph structure, typically involving changes in the
graphâ€™s size.
(1) Temporal shift refers to the change in the temporal data distri-
bution (e.g., mean and variance), meaning that P(Xte)is not equal
toP(Xtr). For example, Figure 1(a) shows the traffic flow of a node
(sensor) in the PeMS system over a long period and reveals its
shifted flow distribution.
(2) Spatial shift refers to the evolution of underlying graph struc-
tures, as depicted in Figure 1(b). In a modern transportation system,
the road network tends to gradually expand over time. New nodesmay emerge due to increasing sensors, while existing nodes may
disappear due to engineering renovations or equipment failures.
Further, we formally formalize the spatio-temporal OOD chal-
lenge as,ğ‘’tr:{Xtr,Gtr}â‰ ğ‘’te:{Xte,Gte}with P(Xte)â‰ P(Xtr)
andGteâ‰ Gtr. When dealing with the OOD challenge, we argue
that existing models have two limitations: inflexible scalability and
unreliable generalization. One major reason behind these limitations
is that they use GCN to learn node representations on pre-defined
training graphs, and this learned knowledge is tied to these specific
graphs, which cannot accurately represent unseen graphs. In par-
ticular, when dependencies between nodes change dynamically due
to spatial-temporal shifts, such as the addition or removal of nodes,
the representations aggregated through the original dependency
paths fail to respond to these shifts, substantially impeding their
generalization ability in various environments. Moreover, GCN
cannot effectively generalize the learned knowledge to emerging
nodes that are unknown to the model during the training phase.
thus, the scalability of models in variable environments also poses
a significant challenge. Traffic management personnel might be
particularly interested in the traffic conditions of these nodes to
devise updated scheduling plans.
Causal learning has garnered considerable attention in dealing
with OOD problems within the image and NLP fields [ 30,62]. Cur-
rent approaches in these domains primarily focus on extracting
stable knowledge that can consistently perform well across diverse
data distributions. However, spatio-temporal OOD tasks present
two major challenges: (1) How to leverage GCN to learn reliable rep-
resentations that are resilient to spatio-temporal shifts. (2) How to
devise intervention mechanisms to explicitly enhance environment
environmental modeling for robust generalization.
In this paper, we propose a novel causal graph learning frame-
work for spatio-temporal OOD learning. The key idea is to ex-
tract invariant spatio-temporal dependencies among nodes that
can consistently represent the relationship of nodes across vari-
ous STG distributions, enabling a reliable aggregation path that
empowers GCN to learn generalizable representations irrespective
of specific graphs. To generate a range of distributions, we model
spatio-temporal shift by perturbing learned node dependencies
instead of manipulating the spatio-temporal graph data.
Specifically, we propose Spatio-Temporal OOD Graph Learning
Networks with FrÃ©chet Embedding (STONE) for spatio-temporal
OOD learning. STONE comprises two main components: a semantic
graph learning component and a graph intervention mechanism.
The first component of STONE utilizes a transformer with a gate
to effectively capture spatio-temporal heterogeneity and generate
two kinds of semantic graphs that represent dependencies among
nodes in the dimensions of temporal and spatial, respectively. To
extract stable dependencies, we first employ a FrÃ©chet embedding
method to encode the topology information of the graphs. The
embedding space serves as a loose mapping of the global position
of nodes within the graphs and exhibits flexibility for spatial shifts.
Secondly, we design a spatio-temporal graph intervention mecha-
nism that involves perturbing two generated semantic graphs. This
perturbation process effectively simulates spatio-temporal shifts,
thereby creating variable environments. By extracting invariant
spatio-temporal dependencies from these environments, the model
 
2949STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
is guided by these dependencies as aggregated paths to learn a ro-
bust representation. Finally, we introduce an explore-to-extrapolate
risk term to enhance the variety of generated environments, en-
abling the model to explore and extrapolate beyond the observed
data, thereby improving its ability to handle unseen distributions.
Experiment results on various OOD traffic datasets demonstrate
that our model achieves competitive performance in generalization
performance and scalability. The main contribution of this paper
can be summarized as follows:
â€¢To the best of our knowledge, we are the first to comprehen-
sively investigate spatio-temporal OOD learning, consider-
ing both temporal shift and structural shift.
â€¢We propose a novel framework called STONE, which aims
to learn invariant node dependencies between nodes. This
allows the model to maintain consistent prediction perfor-
mance in variable environments.
â€¢This framework incorporates several innovative components,
including a novel FrÃ©chet embedding, a graph intervention
mechanism, and an intervention loss term. These compo-
nents are designed to enhance the generalization and scala-
bility of the model.
â€¢Experimental results on multiple real-world traffic datasets
demonstrate that our model achieves competitive general-
ization and scalability across various OOD scenarios.
2 RELATED WORK
2.0.1 Spatiotemporal graph prediction. Traffic prediction plays
a vital role in the intelligent transportation system domain [ 10â€“
12,18,40]. Currently, the prevalent approach transforms traffic
data into spatio-temporal graphs and employs cutting-edge spatio-
temporal graph convolutional networks to handle intricate spatio-
temporal dynamics [ 14,17,34â€“37,49,55]. Notably, D2STGNN [ 31]
combines diffusion graph convolutional networks with RNNs to
capture temporal patterns effectively. STGCN [ 53] utilizes TCN
to efficiently model time dependencies. HGC-RNN [ 52] leverages
optimization techniques based on hypergraph convolution, while
STSGCN [ 33] employs local graph convolution to address large-
scale graph scenarios. Nevertheless, existing models may exhibit
suboptimal performance when dealing with OOD challenges. This
limitation stems from their fundamental assumption that the test
and training environments are drawn from the same distribution.
2.0.2 OOD graph learning. Several methods have been proposed
in the field of graph representation learning to enhance gener-
alization performance for OOD problems [ 28,46]. For instance,
GAUG [ 58] improves downstream training and inference processes
by modifying the input graph using an edge prediction module. Dis-
enGCN [ 24] focuses on learning representations that disentangle
distinct and informative factors within the graph data, assigning
these factors to different parts of the factorized vector representa-
tions. OOD-GNN [ 16] introduces a nonlinear graph representation
decorrelation approach utilizing random Fourier features to elimi-
nate statistical dependence between causal and noncausal graph
representations generated by the graph encoder.2.0.3 OOD learning in the time domain. There are some OOD
learning models in the time domain to address shifts of time se-
ries data. For example, AdaRNN [ 5] clusters historical time se-
quences into different classes and dynamically matches input data
to these classes to identify contextual information. Other invariance
learning models for sequential data are commonly used to learn
disentangled seasonal-trend representations [ 44] or environment-
specific representations [ 50]. DIVERSIFY [ 23] attempts to exploit
subdomains within a whole dataset to counteract issues induced
by non-stationary generalized representation learning. However,
these models fail to model spatial dependencies.
2.0.4 Spatio-temporal OOD learning with temporal shift. Influenced
by the advancements in OOD graph learning within the recommen-
dation domain, researchers have recently moved their attention
towards exploring the problem of OOD with temporal shifts. For
example, CauSTG [ 64] presents a causal framework that is capable
of transferring local and global spatio-temporal invariant relations
to out-of-distribution scenarios. CaST [ 48] utilizes a causal model
(SCM) to interpret the data generation process of spatio-temporal
graphs. It employs back-door adjustment to separate the invariant
components from the temporal environment. STEVE [ 6] encodes
traffic data into two disentangled representations and utilizes spatio-
temporal environments as self-supervised signals to incorporate
contextual information into these representations. This enhances
the generalization ability of the learned context-oriented representa-
tions, thereby improving OOD generalization. However, the current
research primarily focuses on investigating the effects of temporal
drift and fails to consider the evolution of the graph structure.
3 PROBLEM PRELIMINARIES
3.0.1 Spatio-temporal graph. We use a graph structure to represent
spatio-temporal data denoted as G=(ğ‘‰,ğ¸,A), whereğ‘‰means the
node set with ğ‘nodes andğ¸means the set of edges, AâˆˆRğ‘Ã—ğ‘is
the adjacency matrix. We use ğ‘¥ğ‘¡âˆˆRğ‘Ã—ğ‘‘to represent the historical
graph data of ğ‘nodes atğ‘¡-th time step, where ğ‘‘means the number
of features.
3.0.2 Spatio-temporal graph prediction. Given a training environ-
mentğ‘’comprising two parts: the graph Gand the training data
X, this task aims to learn a prediction function Fthat takes the
observed data of the past ğ‘‡time steps as input, xâˆˆRğ‘‡Ã—ğ‘Ã—ğ‘‘=
{ğ‘¥1,Â·Â·Â·,ğ‘¥ğ‘‡}sampled from X, to predict the future data in ğ‘‡ğ‘ƒtime
steps. The parameters of Fare optimized by calculating the loss
between the predicted value and the ground truth ğ‘¦as follows:
min
FE(x,ğ‘¦)âˆ¼P(ğ‘¥,ğ‘¦|ğ‘’)[L(F( x),ğ‘¦)]. (2)
3.0.3 Spatio-temporal OOD learning. The goal of spatio-temporal
OOD learning is to find a function Fthat effectively makes predic-
tions for given input graph data from any of support environments
E.
min
Fmax
ğ‘’âˆ—âˆˆEE(x,ğ‘¦)âˆ¼P(x,ğ‘¦|ğ‘’âˆ—)[L(F( x),ğ‘¦)]. (3)
Definition: Self-attention mechanism SA(Â·).Self-attention mech-
anism can learn the global correlation between different positions
in the input sequence, which is widely applied in the field of time
series analysis [ 61]. Given an input HâˆˆRğ‘Ã—ğ‘‘â„, the self-attention
 
2950KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Binwu Wang et al.
Dilation
TCN Dilation
TCNSigmoidX
Overview of STONE
Adjacency matrix Traffic Data
Spatial  FrÃ© chet 
Embedding 
Spatial
Semantic Graph 
ST ModuleGraph 
Intervention
Extrapolate Risk 
MinimizationJoint
Tuning
Temporal 
Embedding
Temporal
Semantic Graph Training Data
Day Type
Holiday
Temporal Embedding
ğ‘½ğŸ
ğ‘½ğŸ ğ‘½ğŸ‘
Anchor set ğ‘½ğŸ:
Anchor set ğ‘½ğŸ:
Anchor set ğ‘½ğŸ‘:FrÃ© chet Embedding
ğ‘¿ğ’Šğ’
Spatial 
Semantic Graph  
Temporal 
Semantic Graph  
Intervention
Temporal GCN
 Spatial GCNTesting Data
Spatio -temporal Shift
 ScaledNormalization
X
MLPğœ+
Local Gated Residualğ‘ºğ’Šğ’MLP MLP MLP
Sample Sample
Figure 2: The details of the proposed model. Our framework first models spatio-temporal heterogeneity and extracts two
semantic graphs, and then we perturb semantic graphs to create variable environments, enabling the model to learn invariant
dependencies from these environments.
mechanism SA(Â·)computes the attention coefficient by the dot
product operation:
SA(H)=Softmax" Hğ‘Šğ‘+ğ‘ğ‘(Hğ‘Šğ‘˜+ğ‘ğ‘˜)âŠ¤
âˆšğ‘‘ğ‘¥#
. (4)
whereğ‘Šğ‘,ğ‘Šğ‘˜âˆˆRğ‘‘â„Ã—ğ‘‘ğ‘¥,ğ‘ğ‘, andğ‘ğ‘˜are learnable parameters. We
can obtain an attention matrix with size ğ‘Ã—ğ‘.
4 METHOD
In this section, we present the details of STONE, as illustrated in Fig-
ure 2 and Algorithm 25. We provide an overview of the framework
and subsequently explain each component of the model individu-
ally.
4.1 Overview of STONE
STONE consists of three main modules that contribute to its func-
tionality: a semantic graph learning module for acquiring semantic
graphs from input data and prior knowledge, a spatio-temporal
graph convolution module for consolidating information across se-
mantic graphs to make predictions, and a graph intervention mech-
anism that aims to create diverse spatio-temporal environments
where the first two modules can extract invariable knowledge.
Spatio-temporal graph learning module. The input data xâˆˆ
Rğ‘‡Ã—ğ‘Ã—ğ‘‘and the adjacency matrix AâˆˆRğ‘Ã—ğ‘of the graph are
passed through embedding layers, which integrates static prior
information into the model. Subsequently, gated transformers are
employed to effectively model spatio-temporal heterogeneity. This
process generates two types of semantic graphs: a spatial semantic
graph denoted as ğ·ğ‘ , and a temporal semantic graph denoted as
ğ·ğ‘¡.ğ·ğ‘¡captures the similarity of time-varying features between
nodes, providing insights into their temporal dependencies. On the
other hand, ğ·ğ‘ encodes graph topology information, describing the
affinity between nodes based on their positions within the graph.We use spatiotemporal graph convolutional networks to aggregate
temporal and spatial information from these two semantic graphs
separately, and the generated node representations are denoted as
ğ‘‹ğ‘œandğ‘†ğ‘œ. Finally, a gated decoder is employed to decode ğ‘‹ğ‘œand
ğ‘†ğ‘œfor making predictions.
Graph intervention mechanism. We design a graph interference
mechanism for spatio-temporal OOD learning, and this mechanism
perturbs two generated semantic graphs, ğ·ğ‘ andğ·ğ‘¡by randomly
masking edges within them. In fact, This process simulates spatial
and temporal shifts, resulting in the creation of diverse intervention
environments. By extracting invariant spatio-temporal dependen-
cies from these environments, spatio-temporal graph convolutional
networks can generate generalizable node representations. Further,
we introduce an explore-to-extrapolate risk term to enhance the
variety of intervention environments which can prompt the model
to explore and extrapolate beyond the observed data.
4.2 Spatial Semantic Graph Learning
4.2.1 Node embedding. Node embedding technique has been proven
to enhance prediction performance by incorporating graph topol-
ogy information [ 60]. However, traditional node embedding meth-
ods struggle with spatial shifts. For instance, Random-walk based
methods, such as node2vec, sample local substructure, and this is
sensitive to shifts.
To solve this problem, we develop a novel FrÃ©chet embedding.
This method can preserve graph local and global context infor-
mation by encoding the distance affinity between the nodes and
the selected anchor points, which can well response to the dy-
namic nature of the graph. Specifically, given the adjacency matrix
Aof a graph, this method can return a low-dimensional vector
ğ‘†ğ‘–ğ‘›âˆˆRğ‘Ã—ğ‘‘ğ‘’.
Definition 1: FrÃ©chet Embedding. For a metric space(ğ‘‰,ğ‘‘ğ‘‰), we
define an function ğ‘“:ğ‘‰â†’â„“ğ‘‘ğ‘’ğ‘asFrÃ©chet embedding, if each of the
 
2951STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
coordinates ğ‘“ğ‘–is proportional to the distance with anchor sets V,
that is,
ğ‘“ğ‘–(ğ‘¢)=ğ›¼ğ‘–Â·ğ‘‘ğ‘‰(ğ‘¢,Vğ‘–)=ğ›¼ğ‘–Â·min
ğ‘£âˆˆğ‘‰ğ‘–ğ‘‘ğ‘‰(ğ‘¢,ğ‘£),âˆ€ğ‘¢âˆˆğ‘‰,âˆ€ğ‘–=1,...,ğ‘‘ğ‘’.
(5)
whereVğ‘–meansğ‘–-th anchor set with ğ‘‘ğ‘’anchors, and ğ‘ğ‘–is a numeric
factor.ğ‘‰is the node set, and a metric function ğ‘‘ğ‘‰(Â·)on the setğ‘‰
is called an â„“ğ‘metric if there exists a natural number ğ‘‘ğ‘’and an
embedding of(ğ‘‰,ğ‘‘)into the space â„“ğ‘‘ğ‘’ğ‘. Forğ‘= 2,â„“2would be an
Euclidean metric. A pseudometric â„“ğ‘is defined similarly.
Proposition 1: Spatio-temporal graph FrÃ©chet embedding. To
define spatio-temporal graph FrÃ©chet embedding, we select K-order
Manhattan distance [ 29] as the metric ğ‘‘ğ‘‰with nodes set ğ‘‰ofğº
to form the metric space (ğ‘‰,ğ‘‘ğ‘‰). Each dimension of the FrÃ©chet
embedding represents the minimum of the metric between nodes
concerning a fixed subset Vof the set of nodes ğ‘‰, and we call
such a fixed subset the anchor sets. We obtain âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰anchor sets
for each resampling, and a total of ğ‘…rounds of resampling are
performed to obtain ğ‘…Ã—âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰anchor sets{V}ğ‘…Ã—âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰
1. In each
roundğ‘–âˆˆ{1,...,ğ‘…}, we sample ğ‘âˆ—2âˆ’ğ‘—nodes into anchor set Vğ‘–ğ‘—1
forğ‘—âˆˆ{1,2,...,âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰}, following a specific rule: for ğ‘—-th anchor
setVğ‘–ğ‘—inğ‘–-th resampling round, any node ğ‘¢inğ‘‰is selected as
anchor node inVğ‘–ğ‘—with probability 2âˆ’ğ‘—. Given the metric space
(ğ‘‰,ğ‘‘ğ‘‰)and{V}ğ‘…Ã—âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰
1, we define spatio-temporal graph FrÃ©chet
embedding as ğ‘“:ğ‘‰â†’â„“ğ‘…Ã—âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰
ğ‘ , whereğ‘“ğ‘–ğ‘—(ğ‘¢)=ğ›¼ğ‘–ğ‘—ğ‘‘(ğ‘˜)
ğ‘‰ ğ‘¢,Vğ‘–ğ‘—,
whereğ›¼ğ‘–ğ‘—is a learnable parameter withÃğ‘…
ğ‘–=1ÃâŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰
ğ‘—=1ğ›¼ğ‘
ğ‘–ğ‘—=1 for
given metric space â„“ğ‘.
Property. The FrÃ©chet embedding can preserve the structural in-
formation of the original metric space by the relative positions
between the perceptual nodes and the selected anchor set. This
means that closely connected nodes within the graph also have
close embeddings, thus the embedding space has good generaliza-
tion and scalability. On the one hand, spatial shifts do not lead
to significant deviations in this embedding space, hence the node
dependencies are elastic; on the other hand, new nodes can easily
obtain good initial embedding in this space by calculating their
distances to the anchors, thereby enhancing the modelâ€™s scalability.
We carefully illustrate this through ablation experiments in 5.4 and
visual studies in 5.5.
The FrÃ©chet embedding, in reality, is not isometric; it undergoes
slight changes in distances while maintaining the graph structure
to a certain extent. These alterations in the distances reflect the
deviation between the embedding space and the original space. Sub-
sequently, we will demonstrate that our embedding is characterized
by a low distortion upper limit of ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘).
Definition 2: Distortion. Given a metric space (ğ‘‰,ğ‘‘ğ‘‰)and em-
bedding metric space (ğ‘Œ,ğ‘‘ğ‘Œ), an injective mapping ğ‘“:ğ‘‹â†’ğ‘Œis
called ağ·-embedding, where ğ·â‰¥1is a real number if there exists
a constantğ‘Ÿ>0,ğ‘â‰¥1:
ğ‘ŸÂ·ğ‘‘ğ‘‰(ğ‘¢,ğ‘£)â‰¤ğ‘‘ğ‘Œ(ğ‘“(ğ‘¢),ğ‘“(ğ‘£))â‰¤ğ·ğ‘ŸÂ·ğ‘‘ğ‘‰(ğ‘¢,ğ‘£),âˆ€ğ‘¢,ğ‘£âˆˆğ‘‰.(6)
The infimum of the numbers ğ·such thatğ‘“is ağ·-embedding is
called the distortion of ğ‘“.
1The valueâŒˆlogğ‘âŒ‰can guarantee that the expected number of anchor nodes per set
is greater than 1 and achieves less distortion.Theorem 1: Bourgain theorem. This tells us that the mapping
functionğ‘“in spatio-temporal graph FrÃ©chet embedding constructed
by the random sample algorithm satisfies:
1
ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘)ğ‘‘(ğ‘˜)
ğ‘šâ„(ğ‘¢,ğ‘£)â‰¤Eğ‘“âˆ¥ğ‘“ğ‘¢âˆ’ğ‘“ğ‘£âˆ¥ğ‘™ğ‘â‰¤ğ‘‘(ğ‘˜)
ğ‘‰(ğ‘¢,ğ‘£),âˆ€ğ‘¢,ğ‘£âˆˆğ‘‰.
(7)
Thus, the distortion of the embedding is ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘).
4.2.2 Spatial gated transformer for graph leanring. Given the out-
put from the spatial FrÃ©chet embedding layer ğ‘†ğ‘–ğ‘›âˆˆRğ‘Ã—ğ‘‘ğ‘’where
ğ‘‘ğ‘’=âŒˆğ‘™ğ‘œğ‘”ğ‘âŒ‰Ã—ğ‘…, we further propose a spatial gated transformer to
extract deep embedding. Then, we use the self-attention mechanism
to generate a spatial semantic graph.
Specifically, we use two MLP layers to map ğ‘†ğ‘–ğ‘›into another
dimensional feature space as follows:
ğ‘†(1)=ğ‘…ğ‘’ğ¿ğ‘ˆ
ğ‘†ğ‘–ğ‘›ğ‘Š(0)
1+ğ‘(0)
1
ğ‘Š(0)
2+ğ‘(0)
2. (8)
whereğ‘Š(0)
1,ğ‘(0)
1,ğ‘Š(0)
2,andğ‘(0)
2are learnable parameters. Then a
spatial gated transformer uses the self-attention function SA(Â·)to
calculate the similarity between nodes: ğ›¼(ğ‘™)
ğ‘†=SA
ğ‘†(ğ‘™)
âˆˆRğ‘Ã—ğ‘,
whereğ‘†(ğ‘™)âˆˆRğ‘Ã—ğ‘‘(ğ‘™)
ğ‘ means the input of ğ‘™-th layer. The output
vectors are then fused to obtain spatial representations with a gate:
ğ‘†(ğ‘™+1)=ğœ(ğ‘™)
ğ‘†âŠ™ReLUh
ğ›¼(ğ‘™)
ğ‘†(ğ‘™)ğ‘Š(ğ‘™)
ğ‘£+ğ‘(ğ‘™)
ğ‘£i
+
1âˆ’ğœ(ğ‘™)
ğ‘†
âŠ™ğ‘†(ğ‘™),
ğœ(ğ‘™)
ğ‘†=Sigmoidh
ğ‘†(ğ‘™)ğ‘Š(ğ‘™)
ğœ+ğ‘(ğ‘™)
ğœi
âˆˆRğ‘Ã—ğ‘‘(ğ‘™+1)
ğ‘ .
(9)
whereğ‘Š(ğ‘™)
ğ‘£,ğ‘(ğ‘™)
ğ‘£,ğ‘Š(ğ‘™)
ğœ, andğ‘Š(ğ‘™)
ğœare learnable parameters. ğ‘†(ğ‘™+1)âˆˆ
Rğ‘Ã—ğ‘‘(ğ‘™+1)
ğ‘’is the output spatial representation. ğœ(ğ‘™)
ğ‘†is a gate to filter
out redundant information. After two spatial gated transformer
layers, the output embedding vector is denoted as ğ‘†ğ‘ . Then we use
the attention mechanism to generate spatial semantic graphs ğ·ğ‘ :
ğ·ğ‘ =SA(ğ‘†ğ‘ )âˆˆRğ‘Ã—ğ‘. (10)
where this spatial semantic graph ğ·ğ‘ encodes relative position
relationships of nodes.
4.3 Temporal semantic graph learning
4.3.1 Temporal position embedding. This module encodes temporal
prior information such as the date type and holiday type of the
input sequence into xâˆˆRğ‘‡Ã—ğ‘Ã—ğ‘‘, this information can help the
model better analyze temporal trends. The output of this layer is
denoted asğ‘‹0
ğ‘‡âˆˆRğ‘‡Ã—ğ‘Ã—ğ‘‘ğ‘¡.
4.3.2 Temporal gated convolution for graph learning. We use the
TCN architecture to extract temporal trends from spatio-temporal
graph data of each node. TCN is configured with ğ¿ğ‘¡causal con-
volution networks with different receptive fields to model long-
and short-term dependencies. We also introduce a gate to improve
performance. Specifically, given the input ğ‘‹(ğ‘™)âˆˆRğ‘Ã—ğ‘‡ğ‘™Ã—ğ‘‘ğ‘¡in the
ğ‘™-the layer, where ğ‘‡ğ‘™means the time-step length of input time series,
the forward process is shown as follows:
ğ‘‹(ğ‘™+1)
ğ‘‡=ğœğ‘™
ğ‘¡âŠ™
ğœƒğ‘˜ğ‘¡âˆ—ğ‘¡ğ‘‹(ğ‘™)
ğ‘‡
âˆˆRğ‘Ã—ğ‘‡ğ‘™+1Ã—ğ‘‘ğ‘¡,
ğœğ‘™
ğ‘‡=ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘h
ğœƒğ‘˜ğ‘‘âˆ—ğ‘¡ğ‘‹(ğ‘™)
ğ‘‡i
âˆˆRğ‘Ã—ğ‘‡ğ‘™+1Ã—ğ‘‘ğ‘¡.(11)
 
2952KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Binwu Wang et al.
whereğœƒğ‘˜ğ‘¡andğœƒğ‘˜ğ‘‘are learnable parameters, ğ‘˜ğ‘¡andğ‘˜ğ‘‘are kernel
sizes of causal convolution networks. ğœğ‘™
ğ‘‡is a gated and the residual
connection technique is used for smooth learning. Finally, we splice
the output of each layer to and merge information of all time-steps:
ğ‘‹ğ‘¡=ğ‘…ğ‘’ğ¿ğ‘ˆh
ğ‘‹(0:ğ¿ğ‘¡)
ğ‘‡Ã—2ğ‘Š1
ğ‘¡i
Ã—2ğ‘Š2
ğ‘¡âˆˆRğ‘Ã—1Ã—ğ¶ğ‘¡,
ğ‘‹(0:ğ¿ğ‘¡)
ğ‘‡=ğ‘‹(0)
ğ‘‡âŠ•ğ‘‹(1)
ğ‘‡âŠ•...âŠ•ğ‘‹(ğ¿ğ‘¡)
ğ‘‡âˆˆRğ‘Ã—Ãğ¿ğ‘¡
ğ‘™=0ğ‘‡ğ‘™
Ã—ğ¶ğ‘¡.(12)
whereğ‘Š1
ğ‘¡âˆˆRÃğ¿ğ‘¡
ğ‘™=0ğ‘‡ğ‘™
Ã—ğ‘‘ğ‘¡
ğ‘¤andğ‘Š2
ğ‘¡âˆˆRğ‘‘ğ‘¡
ğ‘¤Ã—1are learnable parame-
ters,Ã—2means tensor multiplication in the second dimension and
âŠ•means the concatenation of tensors. Finally, we also use the
attention mechanism to calculate the similarity between nodes:
ğ·ğ‘¡=SA(ğ‘‹ğ‘¡)âˆˆRğ‘Ã—ğ‘. (13)
where the similarity matrix ğ·ğ‘¡âˆˆRğ‘Ã—ğ‘means the correlation of
the flow distribution between nodes.
4.4 Spatio-temporal Graph Convolutional
Network
GCN has been shown to be effective in processing graph data in
multiple tasks [ 2?â€“4]. Given learned temporal feature vector ğ‘‹ğ‘¡
with temporal semantic matrix ğ·ğ‘¡and spatial feature vector ğ‘†ğ‘–with
the spatial semantic matrix ğ·ğ‘ , we use diffusion graph convolution
withğ‘diffusion steps as spatio-temporal graph convolutional net-
works to aggregate the information of two dimensions separately.
We exchange the two matrices to fuse spatio-temporal information:
ğ‘‹ğ‘œ=ğ‘âˆ‘ï¸
ğ‘–=1ğœƒğ‘ âˆ—ğ‘ (ğ¼ğ‘ +ğ·ğ‘ )ğ‘–ğ‘‹ğ‘¡âˆˆRğ‘Ã—ğ‘‘ğ‘œ,
ğ‘†ğ‘œ=ğ‘âˆ‘ï¸
ğ‘–=1ğœƒğ‘¡âˆ—ğ‘ (ğ¼ğ‘¡+ğ·ğ‘¡)ğ‘–ğ‘†ğ‘ âˆˆRğ‘Ã—ğ‘‘ğ‘œ.(14)
whereğœƒğ‘ andğœƒğ‘¡are learnable kernel parameters. ğ¼ğ‘ andğ¼ğ‘¡denote
the identity matrices of ğ·ğ‘¡andğ·ğ‘ , respectively.
4.5 Spatio-temporal OOD learning
The traditional spatio-temporal learning process only allows the
model to adapt to the specific training environment, which limits
its ability to handle out-of-distribution data with spatio-temporal
shifts. To overcome this limitation, it is necessary to expose the
model to diverse training environments, enabling it to learn invari-
ant node representations. However, directly generating variable
out-of-distribution spatio-temporal data is computationally com-
plex. To tackle this challenge, we propose a novel graph intervention
mechanism that perturbs generated semantic graphs, thereby sim-
ulating training data from variable environments. Additionally, we
introduce a loss term that encourages the model to explore and
extrapolate beyond the observed data, enhancing its capability to
handle unseen scenarios.
4.5.1 Noise disturbance. We randomly add noise ğ›¾âˆ¼ğœ‹(0,1), which
is drawn from the standard normal distribution, into the output
vectorğ‘†ğ‘ from the FrÃ©chet embedding layer. This process changes
the spatial dependencies of nodes and simulates spatial shifts, which
can help the model explore a more robust embedding space.4.5.2 Spatio-temporal graph intervention mechanism. To illustrate
the graph intervention mechanism, letâ€™s consider the spatial se-
manticğ·ğ‘ as an example. We create an intervention matrix ğ‘€ğ‘ âˆˆ
[0,1]ğ‘Ã—ğ‘, where each row ğ‘¢follows a binomial distribution B(1,ğ‘(ğ‘¢)).
The probability ğ‘(ğ‘¢)is calculated using the softmax function with
learnable parameters ğœ‹ğ‘¢. Thus, itsğ‘–-th row and ğ‘—-th columnğ‘€ğ‘†[ğ‘–,ğ‘—]
is a binary to indicate whether ğ·ğ‘ [ğ‘–,ğ‘—]is masked. The setting that
each row in ğ‘€ğ‘†is sampled from the same learnable binomial distri-
bution is to improve computational efficiency. Then, By performing
the dot product between ğ‘€ğ‘†andğ·ğ‘ , we can generate a new adja-
cency matrix Ë†ğ·ğ‘ , which would be used in the graph convolution
operation in Equ.14. This matrix can be viewed as a changed spatial
dependence caused by spatial shifts.
In an ideal scenario, the training environment would encom-
pass all possible data distributions. However, achieving such an
exhaustive coverage is not impractical. To move towards this ob-
jective, our aim is to enrich the training environment, enabling the
model to learn from a broader spectrum of data distributions. This
enhancement facilitates better adaptation to unforeseen circum-
stances. Specifically, we create ğ¾ğ‘€intervention matrices denoted
asM=n
ğ‘€1ğ‘ ,Â·Â·Â·,ğ‘€ğ¾ğ‘€ğ‘ o
, whereğ¾ğ‘€is a hyper-parameter. Similar
to the spatial semantic graph ğ·ğ‘ , we also perform a comparable
intervention strategy on the temporal semantic graph ğ·ğ‘¡.
4.6 Decoder and optimization loss
We use a gate unit with MLP layers as a decoder to predict future
graph data:
ğœğ‘œğ‘¢ğ‘¡=Sigmoid
ğ‘†ğ‘œğ‘Šğ‘ 
ğ‘œğ‘¢ğ‘¡+ğ‘‹ğ‘œğ‘Šğ‘–
ğ‘œğ‘¢ğ‘¡
âˆˆRğ¾ğ‘€Ã—ğ‘Ã—ğ‘‘ğ‘œğ‘¢ğ‘¡,
Ë†ğ‘Œ=ğœğ‘œğ‘¢ğ‘¡âŠ™ ğ‘‹ğ‘œğ‘Šğ‘œ
ğ‘œğ‘¢ğ‘¡+ğ‘ğ‘œ
ğ‘œğ‘¢ğ‘¡âˆˆRğ¾ğ‘€Ã—ğ‘Ã—ğ‘‡ğ‘.(15)
whereğ‘Šğ‘ 
ğ‘œğ‘¢ğ‘¡,ğ‘Šğ‘–
ğ‘œğ‘¢ğ‘¡,ğ‘Šğ‘œ
ğ‘œğ‘¢ğ‘¡, andğ‘ğ‘œ
ğ‘œğ‘¢ğ‘¡are learnable parameters. ğ‘‡ğ‘
means the prediction window length. The prediction loss between
predicted values Ë†ğ‘Œand ground-truth values ğ‘¦âˆˆRğ‘Ã—ğ‘‡ğ‘can be
computed as follows:
L(ğ‘¦|M,Î˜)=1
ğ¾ğ‘€ğ¾ğ‘€âˆ‘ï¸
ğ‘š=1LÎ˜
Ë†ğ‘Œ[ğ‘š],ğ‘¦
=1
ğ¾ğ‘€ğ¾ğ‘€âˆ‘ï¸
ğ‘š=1Ë†ğ‘Œ[ğ‘š]âˆ’ğ‘¦ğ‘™1.
(16)
where Ë†ğ‘Œ[ğ‘š]âˆˆRğ‘Ã—ğ‘‡ğ‘means theğ‘š-th row of Ë†ğ‘ŒandÎ˜is the
parameter set of prediction function F.
To enhance the extraction of consistent representations from
various simulated environments, we propose the Invariant Risk
Minimization (IRM) objective [ 1]. Furthermore, the presence of di-
verse environments can enhance the modelâ€™s capacity to generalize
to unfamiliar distributions. Therefore, we incorporate an Explore-
to-Extrapolate risk [ 45] to increase the variance of the intervention
matrix, enabling thorough exploration of environments and pro-
moting robust learning of the models. Hence, the optimization loss
function we employ is:
min
Î˜Var L ğ‘¦|Mâˆ—,Î˜+ğ›½L ğ‘¦|Mâˆ—,Î˜,
ğ‘ .ğ‘¡.Mâˆ—=n
ğ‘€1
âˆ—,...,ğ‘€ğ¾ğ‘€âˆ—o
=argmax
ğ‘šâˆˆ{1,Â·Â·Â·,ğ¾ğ‘€}Var
L ğ‘¦|ğ‘€ğ‘š,Î˜	
.
(17)
 
2953STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
where Var(Â·) means the loss variance, ğ›½is a trade-off to balance two
loss terms.
5 EXPERIMENT
In this section, we conduct a comprehensive evaluation of the gen-
eralization performance (Section 5.2) and scalability performance
(Section 5.3) of STONE2. We then perform ablation experiments
in Section 5.4 to verify the validity of each component. Addition-
ally, we visualize the FrÃ©chet embedding to study its properties in
Section 5.5 and analyze the learned semantic graph in Section 5.6.
5.1 Datasets and setting
5.1.1 Original dataset. In our experiments, we utilize two datasets,
namely the SD and GBA datasets, to evaluate the effectiveness of
STONE. These datasets are subsets of the LargeST dataset [ 21],
which records the traffic flow data from thousands of sensors span-
ning the period from 2017 to 2021 in the Caltrans Performance
Measurement System (PeMS). SD and GBA datasets collected traffic
information from 716 and 2352 sensors, respectively, where their
spatio-temporal graphs are constructed based on the travel distance
between sensors.
5.1.2 Data processing. We use two processing methods to process
SD and GBA to imitate both spatial and temporal shifts for testing.
Temporal shift. We choose the data from 1/2019-8/2019 with 21010
timestamps for training, 9/2019-10/2019 with 7003 timestamps as
validation data, and 11/2020-12/2020 with 7022 timestamps as test-
ing data. The ratio of these three subsets is about 6:2:2.
Spatio-temporal shift. With the temporal shift, we select a certain
number of nodes for training, which amounts to 550 in the SD
dataset and 1809 in the GBA dataset. For validation sets, We select
10% of the number of training nodes from the remaining nodes
to add to training graphs and then mask 10% of them. For testing
sets, we consider three different ratios of new nodes compared to
training graphs: 10%, 15%, and 20%. Additionally, we also randomly
mask 10% of nodes in testing sets to simulate node disappearance.
Two spatio-temporal shift datasets with ğ‘Ÿ%new nodes are denoted
as STSD-r and STGBA-r, where ğ‘Ÿâˆˆ{10,15,20}. The details of these
datasets are presented in Table 1.
Table 1: Details of STSD and STGBA datasets.
Dataset STSD
STGBA
T
rainingTime
span 1/2019-8/2019
1/2019-8/2019
No
des 550
1809
V
alTime
span 9/2019-10/2019
9/2019-10/2019
Remo
ved Nodes 55
180
Ne
w Nodes 55
180
T
estTime
span 11/2020-12/2020
11/2020-12/2020
Remo
ved Nodes 55
180
Ne
w Nodes 55/82/110
180/270/360
2The code is available at https://github.com/PoorOtterBob/STONE-KDD-2024, where
also provides the pseudocode for STONE.5.1.3 Model setting. We set the batch size to 64 and use the Adam
optimizer [ 15] with a learning rate of 1ğ‘’âˆ’3. The trade-off parameter,
ğ›½, of the loss function is set to 1. We stacked two gated transformer
layers to generate the semantic graphs. In the intervention mech-
anism, we create two intervention matrices, i.e., ğ¾ğ‘š=2. In the
FrÃ©chet embedding, we perform 10 sampling rounds in the STSD
dataset and 30 sampling rounds in the STGBA dataset. We evaluate
the performance of the models using three widely used metrics:
MAE, RMSE, and MAPE at 3, 6, and 9 horizon. Models are directly
tested on testing sets after training without further fine-tuning.
5.1.4 Baselines. We compare our proposed method, STONE, with
SOTA traffic prediction models and spatio-temporal OOD learning
methods. However, it is important to note that some SOTA mod-
els cannot be run under the spatio-temporal shift setting, as their
core component parameters are coupled to the scale of the graph,
such as D2STGNN [ 31] and PDFormer [ 9]. The traffic prediction
models include Historical Average (HL), GWNet [ 47], STGCN [ 53],
and STNN [ 51]. For spatio-temporal OOD learning methods, we
compare against CauSTG [64] and CaST [48].
5.2 Generalization performance of models
The prediction performance of all nodes on the STSD and STGBA
datasets with varying rates of new nodes is presented in Table 2.
In the realm of various OOD spatio-temporal datasets, GWNet
achieved relatively lower errors, likely attributed to its usage of
diffusion graph convolutional networks. This enables bidirectional
modeling of spatio-temporal dependencies, thereby improving its
capacity to adapt to shifts in space and time. Conversely, STGCN
exhibited higher prediction errors by relying solely on information
aggregation according to a predetermined graph structure, making
it vulnerable to spatial changes. Consequently, its predictive perfor-
mance becomes increasingly limited with the increase in new nodes.
In contrast, STNN outperforms STGCN in prediction accuracy by
incorporating attentional mechanisms to capture general spatio-
temporal correlations independent of a specific training graph. Nev-
ertheless, its prediction accuracy is somewhat compromised due to
the assumption of independently and identically distributed data.
On the other hand, CaST is designed to address temporal shifts
specifically but encounters challenges in effectively accommodating
spatial shifts. These models demonstrate superior performance in
STGBA datasets with larger graphs, offering more comprehensive
spatio-temporal insights.
Our model achieved competitive prediction performance in vari-
ous OOD scenarios, with a maximum improvement of up to 18.13%
in terms of MAPE. This improvement can be attributed to our ap-
proach of creating a range of training environments by perturbing
the learned semantic graphs. By training on these distributions,
STONE can effectively learn generalizable knowledge, which keeps
consistent performance across OOD scenarios.
5.3 Scalability performance of models
We evaluated the scalability of our model by reporting the predic-
tion performance of new nodes in the testing datasets. The experi-
ment results are shown in Table 3. We observe that GCN-based mod-
els can perform neighborhood aggregation mechanisms to generate
representations for new nodes. However, models like STNN and
 
2954KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Binwu Wang et al.
Table 2: Generalization performance of each model in OOD traffic datasets with spatio-temporal shifts. The best results are
marked in bold and the second best results are underlined.
STSD
dataset with ratio of new nodes: (10%/15%/20%)
Mo
del HL
STGCN [53] GWNet [47] STNN [51] CaST [48] CauSTG [64] Ours
3
horizonMAE 29.66/29.78/29.69
26.62/24.88/23.52 18.86 /21.58 /19.12 39.82/40.23/35.58
24.23/24.05/23.89 26.42/25.31/26.17 18.16/19.36/19.32
RMSE 44.55/44.66/44.52
36.49/34.55/34.84 29.40/30.25/34.27 58.02/58.66/54.59
38.42/38.06/37.75 40.01/40.17/39.89 29.67 /31.42 /31.40
MAPE 21.43/21.45/21.41
48.51/40.54/30.76 18.90 /18.62 /18.44 39.53/39.27/20.76
20.73/20.79/30.76 23.04/21.38/22.16 15.50/16.69/16.87
6
horizonMAE 52.05/52.31/52.19
34.35/33.32/32.05 27.15 /27.01 /27.62 39.78/40.20/35.82
35.96/35.71/35.49 40.01/41.76/40.93 26.32/26.23/26.24
RMSE 75.30/75.51/75.34 40.38/42.82/45.97
42.89/41.70 /41.35 57.77/58.43/54.91
55.47/55.00/54.63 60.34/66.21/66.31 41.76 /40.79/40.91
MAPE 39.43/39.41/39.36
55.77/49.54/43.31 30.88 /30.36 /30.12 40.40/40.13/44.21
36.35/36.47/36.39 41.46/41.82/42.17 22.08/26.16/26.48
12
horizonMAE 94.13/94.61/94.44
45.28/47.07/46.71 39.31/39.14 /40.32 43.02/43.42/41.51
61.63/61.21/60.86 64.13/66.00/65.34 41.05 /36.17/36.19
RMSE 128.11/128.59/128.36
64.04/64.67/65.64 58.09 /57.63 /59.60 62.31/62.89/63.39
90.39/89.74/89.19 89.42/90.16/91.02 55.26/53.79/54.10
MAPE 82.08/81.94/81.85
56.49/57.87/68.57 45.47/44.89/44.62 43.36 /43.10 /48.88
60.40/60.72/60.52 65.37/65.31/65.04 35.55/38.57/38.94
ST
GBA dataset with ratio of new nodes: (10%/15%/20%)
Mo
del HL
STGCN [53] GWNet [47] STNN [51] CaST [48] CauSTG [64] Ours
3
horizonMAE 27.09/26.94/26.97
19.36/25.19/35.65 19.23 /17.56/18.53 37.48/37.33/37.55
26.86/26.78/26.82 30.14/30.41/31.13 17.67/18.73 /18.83
RMSE 40.37/40.16/40.15
29.13 /34.78/36.27
30.01/29.34 /32.62 54.31/54.18/54.43
37.05/36.95/36.99 41.03/42.86/43.02 27.84/29.98/30.13
MAPE 18.90/18.86/18.83
15.82/26.85/30.33 13.71 /14.77 /12.86 31.78/31.94/31.96
34.83/35.20/37.94 36.13/37.40/36.93 12.82/12.84/12.91
6
horizonMAE 47.04/46.80/46.85
25.68 /33.73/35.86
28.10/25.24 /26.71 37.07/36.93/37.15
36.89/36.78/36.85 40.35/41.14/41.66 25.35/25.16/25.37
RMSE 67.46/67.14/67.15
43.77/46.36/48.49 42.78 /37.81/40.28
53.85/53.71/53.97 51.37/51.23/51.29 55.39/55.49/55.43 37.80/38.55 /38.88
MAPE 34.53/34.45/34.41
21.38/34.40/39.09 20.88 /20.41 /19.09 31.55/31.69/31.73
43.37/43.80/43.50 46.15/46.18/46.24 18.17/18.21/18.39
12
horizonMAE 84.85/84.45/84.50 34.50/48.59/50.90
39.91/38.94/39.24 41.16/41.03/41.23 58.87/58.67/58.79 63.15/64.28/64.05 36.35 /36.04/36.51
RMSE 114.83/114.34/114.36
59.81/66.16/68.18 57.91 /56.40 /57.33 59.67/59.55/59.77
81.25/80.99/81.12 88.31/89.35/89.02 53.20/52.80/53.55
MAPE 70.53/70.73/70.25
33.18/46.94/51.19 30.53 /32.80 /28.83 34.16/34.31/34.36
65.61/66.31/65.79 70.14/70.64/70.64 28.38/28.47/28.74
STGCN exhibited larger prediction errors, potentially because their
parameters are coupled with the road network structure, resulting
in insufficient information to generate accurate representations for
emerging nodes. GWNet achieved better prediction performance by
utilizing diffusion graph convolutional networks, which aggregate
bidirectional information to provide more comprehensive insights.
In contrast, our proposed model achieved optimal scalability.
This is because our model learns a robust spatio-temporal semantic
graph structure, enabling accurate predictions by perceiving the
semantic neighborhood information of unseen nodes.
5.4 Ablation experiment
To evaluate the effectiveness of each contribution in the model,
we conducted ablation experiments on the STSD-10% dataset. We
created three variations: (1) W/O IL, where we removed the inter-
vention loss term. (2) W/O Emb, where we removed the FrÃ©chet
embedding. (3) W/O Noi, where we no longer added random noise
to the node embeddings after the FrÃ©chet embedding layer.
The results of the 3-horizon prediction are shown in Table 4. W/O
IL achieved worse errors because the intervention loss term helps
improve the diversity of the variable environment, thereby enhanc-
ing the modelâ€™s generalization performance. W/O Emb had higher
errors because the GCN, which is sensitive to structural shifts, failed
to generate accurate representations for the evolved graph. On the
other hand, the FrÃ©chet embedding space loosely preserves the
graphâ€™s structural information, making semantic neighbors more
robust. This property also improves the scalability performance for
new nodes by perceiving their semantic neighbors. In summary,
each variant performed inferior to the proposed model, demonstrat-
ing the effectiveness of each component.
Figure 3: Node position and FrÃ©chet embedding visualization.
5.5 FrÃ©chet embedding study
We selected several nodes from the STSD-10% dataset and extracted
their embedding vectors from ğ‘†ğ‘–ğ‘›, which are the output of the
FrÃ©chet embedding module. Then, we applied the t-SNE technique
to reduce the dimensions of these vectors. Figure 3 visualizes the
positions and embedding vectors of these nodes. It is evident that
the FrÃ©chet embedding effectively preserves the structural infor-
mation of the graph. Nodes that are close in the graph also have
close embeddings in space, indicating the preservation of proximity
relationships. Additionally, the FrÃ©chet embedding space is elastic
to the shift of the graph structure, as the addition or removal of
nodes does not cause significant changes in the embedding space.
5.6 Semantic graph visualization study
We extracted optimized spatial and temporal semantic graphs from
STONE trained in STSD-10%. Figure 4 displays the edge weights
of 10 nodes across these two graphs along with the predefined
graph based on geographic location. In the predefined graph, the
 
2955STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Scalability performance of each model in OOD traffic datasets with spatio-temporal shifts. The best results are marked
in bold and the second best results are underlined.
ST
-SD dataset with ratio of new nodes: (10%/15%/20%)
Mo
del HL
STGCN [53] GWNet [47] STNN [51] CaST [48] CauSTG [64] Ours
3
HorzionMAE 29.64/30.57/29.85
36.19/34.46/29.59 19.58 /19.58 /19.58 42.56/44.52/36.37
23.37/22.24/21.75 42.56/44.52/39.37 17.22/18.92/18.51
RMSE 43.26/44.51/43.69
48.46/42.91/41.29 30.11 /30.05/29.76 61.74/65.06/55.62
36.11/34.11/33.27 61.74/65.06/59.62 26.85/30.43 /29.83
MAPE 21.48/21.56/21.32
48.74/39.05/29.69 19.02 /19.35 /18.38 37.23/36.00/25.38
19.40/20.30/20.19 37.23/36.00/25.38 14.93/18.67/18.60
6
HorzionMAE 51.56/53.70/52.63
51.81/45.34/43.07 28.48 /28.33 /28.55 42.72/44.68/36.63
35.05/33.36/32.69 42.72/44.68/36.63 24.91/26.31/25.71
RMSE 73.30/75.84/74.71
71.92/62.80/60.62 42.82 /41.85 /41.59 61.88/65.06/55.95
53.09/50.34/49.28 61.88/65.06/55.95 38.60/40.71/40.01
MAPE 40.63/39.79/39.39
61.74/53.30/42.74 30.42/31.71/29.10 38.11/36.70/25.19 32.96/34.98/34.88
38.11/36.70/25.19 22.03/30.77/30.43
12
HorzionMAE 93.11/97.12/95.46
76.75/68.71/66.13 41.98 /41.40 /42.12 45.01/47.31/42.13
59.80/57.15/56.06 45.01/47.31/42.13 39.53/37.78/36.74
RMSE 125.86/130.04/128.33
109.81/95.18/92.64 63.23 /60.96 /61.35 65.25/68.45/64.07
86.98/83.12/81.46 65.21/68.42/64.03 59.37/56.51/55.37
MAPE 85.42/83.31/82.39
74.37/75.54/60.52 46.56/44.25/43.58 40.51 /43.05/42.82 52.78/87.73/57.30
41.33/43.02 /44.11 34.71/36.95/34.27
ST
-GBA dataset with ratio of new nodes: (10%/15%/20%)
Mo
del HL
STGCN [53] GWNet [47] STNN [51] CaST [48] CauSTG [64] Ours
3
HorzionMAE 26.67/25.65/26.16
26.14/32.97/35.14 21.64 /18.00/21.39 36.98/36.41/37.70
25.88/25.60/26.17 30.15/27.21/29.87 17.64/18.25 /19.59
RMSE 39.20/37.91/38.44
35.91/46.41/46.11 32.79 /28.59/32.98 52.98/52.94/54.37
35.19/34.98/35.72 40.04/39.15/40.61 27.03/29.73 /30.16
MAPE 18.26/18.13/18.19
21.99/30.60/35.59 14.62 /14.87 /16.48 29.24/31.54/31.58
30.26/34.56/33.17 34.13/35.16/36.03 12.81/13.02/13.15
6
HorzionMAE 46.54/44.86/45.62
37.28/47.88/52.03 33.01 /25.89 /33.46 36.51/35.94/37.23
35.62/35.19/35.98 38.75/39.01/38.40 25.73/24.87/25.46
RMSE 65.93/63.96/64.76
50.74/66.20/68.08 48.41 /39.40 /51.00
52.40/52.32/53.78 49.12/48.78/49.74 55.46/56.14/55.03 37.55/38.82/39.54
MAPE 33.55/33.29/33.31
32.07/42.58/51.97 22.99 /20.55 /27.13 29.01/31.27/31.34
37.31/42.68/41.14 40.14/47.43/43.27 18.61/18.69/19.08
12
HorzionMAE 84.24/81.34/82.46
57.31/74.33/80.60 49.10/39.92/53.09 39.89 /39.46 /40.86 56.85/55.97/57.40
61.24/64.39/65.15 38.24/36.50/37.60
RMSE 112.84/109.71/110.98
77.88/101.28/105.41 69.42/58.67/79.15 67.16 /57.30 /58.88 78.24/77.21/78.94
84.32/86.42/85.22 56.56/54.26/55.81
MAPE 68.55/70.37/67.85
47.28/63.89/74.58 35.51/32.68 /43.06
31.04 /33.31/33.67 54.58/63.70/61.18
59.10/68.91/64.32 29.86/29.84/30.23
Table 4: Ablation experiment on STSD-10% dataset.
ModelGeneralization (All nodes) Scalability (New nodes)
MAE RMSE MAPE MAE RMSE MAPE
Ours 18.17 29.67 15.51 17.23 26.85 14.93
W/O Emb 21.91 32.24 26.79 21.64 31.84 25.30
W/O IL 19.66 31.29 17.83 20.49 32.23 20.49
W/O Noi 19.36 31.43 16.69 18.92 30.43 18.67
correlations between nodes are scattered, which means that a spatio-
temporal shift can disrupt the aggregation of neighborhood infor-
mation and propagate through the entire graph via message passing
mechanisms. In learned semantic graphs, nodes primarily establish
connections with a small number of crucial nodes. As a result, the
addition or removal of nodes has minimal impact. Even if some cru-
cial nodes are removed, the model can still aggregate information
from the remaining nodes in both temporal and spatial dimensions,
leading to the generation of accurate representations.
Figure 4: Edge connectivity of 10 nodes in three graphs.6 CONCLUSION
In this paper, we introduce a new framework STONE for spatio-
temporal OOD learning. STONE integrates a semantic graph learn-
ing module to capture spatial heterogeneity and generate semantic
graphs in both temporal and spatial dimensions. Then we pro-
pose a graph intervention mechanism to perturb the generated
semantic graph to create diverse training environments. With an
Explore-to-Extrapolate loss term, STONE can extract stable spatio-
temporal aggregation information paths, thereby generating in-
variant spatio-temporal representations, which can generalize well
to unknown environments. We conduct extensive experiments to
evaluate the effectiveness of STONE. The results demonstrate that
STONE achieves competitive performance in terms of both gener-
alization and scalability.
ACKNOWLEDGMENTS
This paper is supported by the National Natural Science Foundation
of China (No.62072427, No.12227901), the Project of Stable Support
for Youth Teamin Basic Research Field, CAS (No.YSBR-005), and
Academic Leaders Cultivation Program, USTC.
REFERENCES
[1]Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant risk
minimization. arXiv preprint arXiv:1907.02893 (2019).
[2]Chen, H., Xu, Y., Huang, F., Deng, Z., Huang, W., Wang, S., He, P., and
Li, Z. Label-aware graph convolutional networks. In Proceedings of the 29th
ACM International Conference on Information & Knowledge Management (2020),
p. 1977â€“1980.
[3]Du, W., Chen, L., Wang, H., Shan, Z., Zhou, Z., Li, W., and Wang, Y. Deciphering
urban traffic impacts on air quality by deep learning and emission inventory.
journal of environmental sciences 124 (2023), 745â€“757.
[4]Du, W., Yang, X., Wu, D., Ma, F., Zhang, B., Bao, C., Huo, Y., Jiang, J., Chen, X.,
 
2956KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Binwu Wang et al.
and Wang, Y. Fusing 2d and 3d molecular graphs as unambiguous molecular de-
scriptors for conformational and chiral stereoisomers. Briefings in Bioinformatics
24, 1 (2023), bbac560.
[5]Du, Y., Wang, J., Feng, W., Pan, S., Qin, T., Xu, R., and Wang, C. Adarnn:
Adaptive learning and forecasting of time series. In Proceedings of the 30th
ACM international conference on information & knowledge management (2021),
pp. 402â€“411.
[6]Hu, J., Liang, Y., Fan, Z., Chen, H., Zheng, Y., and Zimmermann, R. Graph neural
processes for spatio-temporal extrapolation. arXiv preprint arXiv:2305.18719
(2023).
[7]Huang, Q., Shen, L., Zhang, R., Cheng, J., Ding, S., Zhou, Z., and Wang, Y.
Hdmixer: Hierarchical dependency with extendable patch for multivariate time
series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence
(2024), vol. 38, pp. 12608â€“12616.
[8]Ji, J., Zhang, W., Wang, J., He, Y., and Huang, C. Self-supervised decon-
founding against spatio-temporal shifts: Theory and modeling. arXiv preprint
arXiv:2311.12472 (2023).
[9]Jiang, J., Han, C., Zhao, W. X., and Wang, J. Pdformer: Propagation delay-
aware dynamic long-range transformer for traffic flow prediction. arXiv preprint
arXiv:2301.07945 (2023).
[10] Jin, G., Li, F., Zhang, J., Wang, M., and Huang, J. Automated dilated spatio-
temporal synchronous graph modeling for traffic prediction. IEEE Transactions
on Intelligent Transportation Systems (2022).
[11] Jin, G., Liang, Y., Fang, Y., Shao, Z., Huang, J., Zhang, J., and Zheng, Y. Spatio-
temporal graph neural networks for predictive learning in urban computing: A
survey. IEEE Transactions on Knowledge and Data Engineering (2023).
[12] Jin, G., Xi, Z., Sha, H., Feng, Y., and Huang, J. Deep multi-view graph-based
network for citywide ride-hailing demand prediction. Neurocomputing 510 (2022),
79â€“94.
[13] Jin, W., Ma, Y., Liu, X., Tang, X., Wang, S., and Tang, J. Graph structure
learning for robust graph neural networks. In Proceedings of the 26th ACM
SIGKDD international conference on knowledge discovery & data mining (2020),
pp. 66â€“74.
[14] Jin, Y., Chen, K., and Yang, Q. Transferable graph structure learning for graph-
based traffic forecasting across cities. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (2023), pp. 1032â€“1043.
[15] Kingma, D. P., and Ba, J. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 (2014).
[16] Li, H., Wang, X., Zhang, Z., and Zhu, W. Ood-gnn: Out-of-distribution general-
ized graph neural network. IEEE Transactions on Knowledge and Data Engineering
(2022).
[17] Li, H., Zhao, Y., Mao, Z., Qin, Y., Xiao, Z., Feng, J., Gu, Y., Ju, W., Luo, X., and
Zhang, M. A survey on graph neural networks in intelligent transportation
systems. arXiv preprint arXiv:2401.00713 (2024).
[18] Li, Z., Xia, L., Tang, J., Xu, Y., Shi, L., Xia, L., Yin, D., and Huang, C. Urbangpt:
Spatio-temporal large language models. arXiv preprint arXiv:2403.00813 (2024).
[19] Liang, Y., Ouyang, K., Wang, Y., Liu, Y., Zhang, J., Zheng, Y., and Rosenblum,
D. S. Revisiting convolutional neural networks for citywide crowd flow analytics.
InMachine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2020, Ghent, Belgium, September 14â€“18, 2020, Proceedings, Part I
(2021), Springer, pp. 578â€“594.
[20] Liu, C., Yang, S., Xu, Q., Li, Z., Long, C., Li, Z., and Zhao, R. Spatial-temporal
large language model for traffic prediction. arXiv preprint arXiv:2401.10134 (2024).
[21] Liu, X., Xia, Y., Liang, Y., Hu, J., Wang, Y., Bai, L., Huang, C., Liu, Z., Hooi,
B., and Zimmermann, R. Largest: A benchmark dataset for large-scale traffic
forecasting. arXiv preprint arXiv:2306.08259 (2023).
[22] Liu, Z., Miao, H., Zhao, Y., Liu, C., Zheng, K., and Li, H. Lighttr: A lightweight
framework for federated trajectory recovery. arXiv preprint arXiv:2405.03409
(2024).
[23] Lu, W., Wang, J., Sun, X., Chen, Y., Ji, X., Yang, Q., and Xie, X. Diversify: A gen-
eral framework for time series out-of-distribution detection and generalization.
IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).
[24] Ma, J., Cui, P., Kuang, K., Wang, X., and Zhu, W. Disentangled graph convolu-
tional networks. In International conference on machine learning (2019), PMLR,
pp. 4212â€“4221.
[25] Miao, H., Fei, Y., Wang, S., Wang, F., and Wen, D. Deep learning based origin-
destination prediction via contextual information fusion. Multimedia Tools and
Applications (2022), 1â€“17.
[26] Miao, H., Shen, J., Cao, J., Xia, J., and Wang, S. Mba-stnet: Bayes-enhanced
discriminative multi-task learning for flow prediction. TKDE (2022).
[27] Miao, H., Zhao, Y., Guo, C., Yang, B., Kai, Z., Huang, F., Xie, J., and Jensen,
C. S. A unified replay-based continuous learning framework for spatio-temporal
prediction on streaming data. In ICDE (2024).
[28] Park, H., Lee, S., Kim, S., Park, J., Jeong, J., Kim, K.-M., Ha, J.-W., and Kim, H. J.
Metropolis-hastings data augmentation for graph neural networks. Advances in
Neural Information Processing Systems 34 (2021), 19010â€“19020.
[29] Peiravi, A., and Kheibari, H. T. A fast algorithm for connectivity graph ap-
proximation using modified manhattan distance in dynamic networks. Appliedmathematics and computation 201, 1-2 (2008), 319â€“332.
[30] SchÃ¶lkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal,
A., and Bengio, Y. Toward causal representation learning. Proceedings of the
IEEE 109, 5 (2021), 612â€“634.
[31] Shao, Z., Zhang, Z., Wei, W., Wang, F., Xu, Y., Cao, X., and Jensen, C. S.
Decoupled dynamic spatial-temporal graph neural network for traffic forecasting.
arXiv preprint arXiv:2206.09112 (2022).
[32] Shuman, D. I., Narang, S. K., Frossard, P., Ortega, A., and Vandergheynst, P.
The emerging field of signal processing on graphs: Extending high-dimensional
data analysis to networks and other irregular domains. IEEE signal processing
magazine 30, 3 (2013), 83â€“98.
[33] Song, C., Lin, Y., Guo, S., and Wan, H. Spatial-temporal synchronous graph
convolutional networks: A new framework for spatial-temporal network data
forecasting. In Proceedings of the AAAI conference on artificial intelligence (2020),
vol. 34, pp. 914â€“921.
[34] Wang, B., Wang, P., Zhang, Y., Wang, X., Zhou, Z., Bai, L., and Wang, Y.
Towards dynamic spatial-temporal graph learning: A decoupled perspective.
InProceedings of the AAAI Conference on Artificial Intelligence (2024), vol. 38,
pp. 9089â€“9097.
[35] Wang, B., Wang, P., Zhang, Y., Wang, X., Zhou, Z., and Wang, Y. Condition-
guided urban traffic co-prediction with multiple sparse surveillance data. IEEE
Transactions on Vehicular Technology (2024).
[36] Wang, B., Zhang, Y., Wang, P., Wang, X., Bai, L., and Wang, Y. A knowledge-
driven memory system for traffic flow prediction. In International Conference on
Database Systems for Advanced Applications (2023), Springer, pp. 192â€“207.
[37] Wang, B., Zhang, Y., Wang, X., Wang, P., Zhou, Z., Bai, L., and Wang, Y. Pattern
expansion and consolidation on evolving graphs for continual traffic prediction.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (2023), pp. 2223â€“2232.
[38] Wang, J., Jiang, J., Jiang, W., Han, C., and Zhao, W. X. Towards efficient
and comprehensive urban spatial-temporal prediction: A unified library and
performance benchmark. arXiv preprint arXiv:2304.14343 (2023).
[39] Wang, L., Guo, D., Wu, H., Li, K., and Yu, W. Tc-gcn: Triple cross-attention and
graph convolutional network for traffic forecasting. Information Fusion (2024),
102229.
[40] Wang, S., Cao, J., Chen, H., Peng, H., and Huang, Z. Seqst-gan: Seq2seq
generative adversarial nets for multi-step urban crowd flow prediction. ACM
Transactions on Spatial Algorithms and Systems (TSAS) 6, 4 (2020), 1â€“24.
[41] Wang, S., Miao, H., Chen, H., and Huang, Z. Multi-task adversarial spatial-
temporal networks for crowd flow prediction. In CIKM (2020), pp. 1555â€“1564.
[42] Wang, S., Miao, H., Li, J., and Cao, J. Spatio-temporal knowledge transfer for
urban crowd flow prediction via deep attentive adaptation networks. TITS 23, 5
(2021), 4695â€“4705.
[43] Wang, X., Wang, P., Wang, B., Zhang, Y., Zhou, Z., Bai, L., and Wang, Y.
Latent gaussian processes based graph learning for urban traffic prediction. IEEE
Transactions on Vehicular Technology (2023).
[44] Woo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. Cost: Contrastive learning
of disentangled seasonal-trend representations for time series forecasting. arXiv
preprint arXiv:2202.01575 (2022).
[45] Wu, Q., Zhang, H., Yan, J., and Wipf, D. Handling distribution shifts on graphs:
An invariance perspective. arXiv preprint arXiv:2202.02466 (2022).
[46] Wu, Y.-X., Wang, X., Zhang, A., He, X., and Chua, T.-S. Discovering invariant
rationales for graph neural networks. arXiv preprint arXiv:2201.12872 (2022).
[47] Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep
spatial-temporal graph modeling. arXiv preprint arXiv:1906.00121 (2019).
[48] Xia, Y., Liang, Y., Wen, H., Liu, X., Wang, K., Zhou, Z., and Zimmermann,
R.Deciphering spatio-temporal graph forecasting: A causal lens and treatment.
arXiv preprint arXiv:2309.13378 (2023).
[49] Yan, H., and Li, Y. A survey of generative ai for intelligent transportation systems.
arXiv preprint arXiv:2312.08248 (2023).
[50] Yang, C., Wu, Q., Wen, Q., Zhou, Z., Sun, L., and Yan, J. Towards out-of-
distribution sequential event prediction: A causal treatment. Advances in neural
information processing systems 35 (2022), 22656â€“22670.
[51] Yang, S., Liu, J., and Zhao, K. Space meets time: Local spacetime neural network
for traffic flow forecasting. In 2021 IEEE International Conference on Data Mining
(ICDM) (2021), IEEE, pp. 817â€“826.
[52] Yi, J., and Park, J. Hypergraph convolutional recurrent neural network. In Pro-
ceedings of the 26th ACM SIGKDD international conference on knowledge discovery
& data mining (2020), pp. 3366â€“3376.
[53] Yu, B., Yin, H., and Zhu, Z. Spatio-temporal graph convolutional networks: A
deep learning framework for traffic forecasting. arXiv preprint arXiv:1709.04875
(2017).
[54] Zhang, G., Yi, J., Yuan, J., Li, Y., and Jin, D. Das: Efficient street view image
sampling for urban prediction. ACM Transactions on Intelligent Systems and
Technology 14, 2 (2023), 1â€“20.
[55] Zhang, J., Zheng, Y., Qi, D., Li, R., and Yi, X. Dnn-based prediction model for
spatio-temporal data. In Proceedings of the 24th ACM SIGSPATIAL international
conference on advances in geographic information systems (2016), pp. 1â€“4.
 
2957STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[56] Zhang, W., Zhang, L., Han, J., Liu, H., Zhou, J., Mei, Y., and Xiong, H. Irregular
traffic time series forecasting based on asynchronous spatio-temporal graph
convolutional network. arXiv preprint arXiv:2308.16818 (2023).
[57] Zhang, Y., Wang, P., Wang, B., Wang, X., Zhao, Z., Zhou, Z., Bai, L., and
Wang, Y. Adaptive and interactive multi-level spatio-temporal network for
traffic forecasting. IEEE Transactions on Intelligent Transportation Systems (2024).
[58] Zhao, T., Liu, Y., Neves, L., Woodford, O., Jiang, M., and Shah, N. Data
augmentation for graph neural networks. In Proceedings of the aaai conference
on artificial intelligence (2021), vol. 35, pp. 11015â€“11023.
[59] Zhao, Z., Shen, G., Wang, L., and Kong, X. Graph spatial-temporal transformer
network for traffic prediction. Big Data Research (2024), 100427.
[60] Zheng, C., Fan, X., Wang, C., and Qi, J. Gman: A graph multi-attention network
for traffic prediction. In Proceedings of the AAAI conference on artificial intelligence
(2020), vol. 34, pp. 1234â€“1241.
[61] Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W.
Informer: Beyond efficient transformer for long sequence time-series forecasting.
InProceedings of the AAAI conference on artificial intelligence (2021), vol. 35,
pp. 11106â€“11115.
[62] Zhou, K., Liu, Z., Qiao, Y., Xiang, T., and Loy, C. C. Domain generalization:
A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 4
(2022), 4396â€“4415.
[63] Zhou, Z., Huang, Q., Wang, B., Hou, J., Yang, K., Liang, Y., and Wang, Y.
Coms2t: A complementary spatiotemporal learning system for data-adaptive
model evolution. arXiv preprint arXiv:2403.01738 (2024).
[64] Zhou, Z., Huang, Q., Yang, K., Wang, K., Wang, X., Zhang, Y., Liang, Y., and
Wang, Y. Maintaining the status quo: Capturing invariant relations for ood
spatiotemporal learning.
[65] Zhou, Z., Yang, K., Liang, Y., Wang, B., Chen, H., and Wang, Y. Predicting
collective human mobility via countering spatiotemporal heterogeneity. IEEE
Transactions on Mobile Computing (2023).
A PSEUDO-CODE OF STONE
We provide the pseudo-code of STONE for spatio-temporal ODD
prediction in Algorithm 1.
B EXPERIMENT
B.1 Setting
To define the graph topology, we utilized the common practice to
construct the normalized geographic adjacency matrix N(A)âˆˆ
Rğ‘Ã—ğ‘for spatio-temporal graph FrÃ©chet embedding computing
via the Gaussian kernel [32] with threshold 0.1, whose entries are
Ağ‘¢ğ‘£=(
exp
âˆ’ğ‘‘ğ‘¢ğ‘£
ğœ2
,ifexp
âˆ’ğ‘‘ğ‘¢ğ‘£
ğœ2
>0.1,
0, otherwise.(18)
N(A)=Ağ‘¢ğ‘£Ã
ğ‘£âˆˆğ‘‰Ağ‘¢ğ‘£(19)
Here,ğ‘‘ğ‘¢ğ‘£denotes the road network distance from sensors ğ‘¢toğ‘£,
andğœis the standard deviation of all distances.
To optimize performance, we set the batch size to 64 and used
the Adam [ 15] optimizer with a learning rate of 1ğ‘’âˆ’3and weight
decay of 1ğ‘’âˆ’4. Additionally, we implemented a learning rate decay
strategy where the learning rate is reduced by a factor of 0.95 every
10 training steps, and a gradient clipping strategy with a threshold
of 5. The loss functionâ€™s trade-off parameter, ğ›½, is set to 1. The
attention network dimension for the adaptive semantic graphs is
20. The TCN comprises 5 hidden layers with a dimension of 128.
There are 2 layers of gated transformer, with a hidden dimension
of 64 for the SD dataset and 128 for the GBA dataset. The decoder
network has a hidden dimension of 128.
B.2 Baseline setting
â€¢HL[19] selects the data from the last observation as the
predicted value for all future time points.Algorithm 1: STONE for spatio-temporal ODD prediction
Input: FrÃ©chet embedding ğ‘†ğ‘–ğ‘›âˆˆRğ‘Ã—ğ‘‘ğ‘’, observed sampled
traffic flow data xâˆˆRğ‘‡Ã—ğ‘Ã—ğ‘‘, ST-module of STONE
Fwith parameters Î˜, masking operators
MâˆˆRğ¾ğ‘€Ã—ğ‘Ã—ğ‘
Output: Predicted traffic flow Ë†ğ‘Œ
1forğ‘™=1,2,...,ğ¿ğ‘ do
2 ifğ‘™==1then
3ğ‘†(1)â†ğ‘†ğ‘–ğ‘›inEq.8; // Shallow encoding
4 else
5ğ‘†(ğ‘™)â†ğ‘†(ğ‘™âˆ’1)inEq.9; // Gated transformer
6 end
7end
8ğ·ğ‘¡â†ğ‘†(ğ¿ğ‘ );
9ğ·ğ‘ â†ğ‘†ğ‘ inEq.10; // Spatial semantic graph
10forğ‘™=0,1,2,...,ğ¿ğ‘¡do
11 ifğ‘™==0then
12ğ‘‹(0)
ğ‘‡â†pos-emb.(x); // Shallow encoding
13 else
14ğ‘‹(ğ‘™)
ğ‘‡â†ğ‘‹(ğ‘™âˆ’1)
ğ‘‡inEq.11; // Dilation TCN
15 end
16end
17ğ‘‹ğ‘¡â†ğ‘‹(0)
ğ‘‡,ğ‘‹(1)
ğ‘‡,...,ğ‘‹(ğ¿ğ‘¡)
ğ‘‡inEq.12; // Residual
concatenation
18ğ·ğ‘¡â†ğ‘‹ğ‘¡inEq.13; // Temporal semantic graph
19ifTraining phase then
20ğ·ğ‘ â†MâŠ™ğ·ğ‘ ; // Masking temporal semantic
graph
21ğ·ğ‘¡â†MâŠ™ğ·ğ‘¡;// Masking spatial semantic graph
22end
23ğ‘‹ğ‘œâ†(ğ·ğ‘ ,ğ‘‹ğ‘¡)inEq.14;// Temporal graph diffusion
24ğ‘†ğ‘œâ†(ğ·ğ‘¡,ğ‘†ğ‘ )inEq.14; // Spatial graph diffusion
25Ë†ğ‘Œâ†(ğ‘‹ğ‘œ,ğ‘†ğ‘œ)inEq.15; // Gate decoder
â€¢GWNet [47] has removed the adaptive adjacency matrix due
to its lack of scalability. The dimensions for the initialization,
skip connections, and output are set to 32, 256, and 512
respectively.
â€¢STGCN [53] consists of 2 ST-Blocks. Each ST-Block has two
layers of TCN with a dimension of 64 and a kernel size of 3,
and a ChebGCN layer with a dimension of 16. The dimension
of the output block is 128.
â€¢STNN [51] has subgraph-conv with hidden layer dimension
32 and 64.
â€¢CaST [48] has removed the random embedding features
of nodes, and set the hidden layer dimension to 64, with a
granularity of 20 for the environment representation.
â€¢CauSTG [64] has removed the node representation. The 4-
layer TCNs have (5, 5, 6, 6) kernels with dimension of (12, 6,
3). We set the sub-environment partition number and model
number of the sub-environment with 6 and 4 respectively.
 
2958KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Binwu Wang et al.
B.3 Effect analysis for temporal shift
As shown in Table 5, we report the performance of each model on
STSD with only temporal shift. GWNet achieved relatively lower er-
rors, potentially because it inherited diffusion graph convolutional
networks, enabling bidirectional modeling of spatio-temporal de-
pendencies and enhancing generalization to spatio-temporal shifts.
CaST had better performance than STGCN, because it specifically
introduces causal learning to learn invariant patterns for temporal
shifts. STONE remained competitive in dealing with temporal shifts.
This is because that the proposed graph intervention mechanism
can enable the model to learn the invariable pattern efficiently.
Table 5: Prediction performance of each model on datasets
with only temporal shift.
ST
-SD dataset with only temporal shift
Mo
del3 horizon 6 horizon 12 horizon
MAE
RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
HL
29.42 44.26 21.48 51.71 74.81 39.55 93.65 127.43 82.48
STGCN [53] 24.94 38.23 36.93 31.95 47.36 44.09 38.70 55.49 51.25
GWNET [47] 18.38 29.46 15.70 25.84 40.21 25.31 34.62 53.10 35.70
STNN
[51] 39.59 57.73 39.69 39.51 57.42 40.56 42.83 62.04 43.54
CaST [48] 22.83 35.37 27.09 35.45 53.86 41.65 61.00 88.11 70.58
CauSTG [64] 24.97 39.48 43.34 40.04 56.98 47.15 66.83 91.27 75.13
Ours 18.26 31.24 14.95
25.06 40.93 21.84
34.46 52.82 33.35B.4 Discussion
In this section, we discuss the limitations and the future works:
â€¢When there are many random mask operators, the cost of
training increases exponentially, and the time it takes to
achieve convergence also increases, making it more challeng-
ing. This highlights the need for better theories of stochastic
optimization. However, this topic is beyond the scope of our
work.
â€¢We use gated-Transformers to learn the spatio-temporal se-
mantic graphs. In the future, inspired by the progress of
graph structure learning [ 13], we will design more effective
spatio-temporal semantic graph learning models.
â€¢STONE is only experimented on the transportation dataset,
and in the future, STONE will be deployed to other spatio-
temporal computing domains, such as the atmospheric do-
main.
 
2959