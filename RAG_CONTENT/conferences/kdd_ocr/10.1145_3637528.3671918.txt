Profiling Urban Streets: A Semi-Supervised Prediction Model
Based on Street View Imagery and Spatial Topology
Meng Chen
School of Software
Shandong University
Jinan, Shandong, China
mchen@sdu.edu.cnZechen Li
School of Software
Shandong University
Jinan, Shandong, China
lizc@mail.sdu.edu.cnWeiming Huang
School of Computer Science and
Engineering
Nanyang Technological University
Singapore
weiming.huang@ntu.edu.sg
Yongshun Gong
School of Software
Shandong University
Jinan, Shandong, China
yongshun2512@hotmail.comYilong Yinâˆ—
School of Software
Shandong University
Jinan, Shandong, China
ylyin@sdu.edu.cn
ABSTRACT
With the expansion and growth of cities, profiling urban areas with
the advent of multi-modal urban datasets (e.g., points-of-interest
and street view imagery) has become increasingly important in ur-
ban planing and management. Particularly, street view images have
gained popularity for understanding the characteristics of urban ar-
eas due to its abundant visual information and inherent correlations
with human activities. In this study, we define a street segment rep-
resented by multiple street view images as the minimum spatial unit
for analysis and predict its functional and socioeconomic indicators,
which presents several challenges in modeling spatial distributions
of images on a street and the spatial topology (adjacency) of streets.
Meanwhile, Large Language Models are capable of understanding
imagery data based on its extraordinary knowledge base and unveil
a remarkable opportunity for profiling streets with images. In view
of the challenges and opportunity, we present a semi-supervised
Urban Street Profiling Model (USPM) based on street view imagery
and spatial adjacency of urban streets. Specifically, given a street
with multiple images, we first employ a newly designed spatial
context-based contrastive learning method to generate feature vec-
tors of images and then apply the LSTM-based fusion method to
encode multiple images on a street to yield the street visual repre-
sentation; we then create the descriptions of street scenes for street
view images based on the SPHINX (a large language model) and
produce the street textual representation; finally, we build an urban
street graph based on spatial topology (adjacency) and employ a
semi-supervised graph learning algorithm to further encode the
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671918street representations for prediction. We conduct thorough experi-
ments with real-world datasets to assess the proposed USPM. The
experimental results demonstrate that USPM considerably outper-
forms baseline methods in two urban prediction tasks.
CCS CONCEPTS
â€¢Information systems â†’Spatial-temporal systems; Data
mining.
KEYWORDS
Urban Street Profiling; Street View Imagery; Street Representation
Learning; Large Language Model
ACM Reference Format:
Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, and Yilong Yin.
2024. Profiling Urban Streets: A Semi-Supervised Prediction Model Based on
Street View Imagery and Spatial Topology. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3637528.3671918
1 INTRODUCTION
Profiling urban areas is crucial for urban planning and manage-
ment, such as in the analysis of economic development, population
dynamics, and air quality [ 2,12,31,42]. Among the various spatial
elements in our cities, urban streets are the veins connecting a
variety of locations, and a major carrier of socioeconomic activities.
They not only provide spatial and functional convenience but also
serve as the proper spatial units for the high-resolution analysis of
urban socioeconomic indicators [ 21]. Traditionally, urban profiling
is managed by authorities through extracting information from
multiple urban datasets, as well as conducting field or airborne
surveys. Such traditional methods are largely labor-intensive and
time-consuming, and may even be financially unfeasible for some
local authorities.
Thanks to physical and social sensing networks, a wealth of
urban datasets (e.g., POI data, human mobility data, and street view
imagery) are generated, leading to significant changes in the study
of urban profiling [ 14,23,24,35,36,39]. Among these urban data,
319
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, and Yilong Yin
street view images, which offer high-resolution views of urban areas
from a human perspective, have gained popularity due to their detail
and frequency of updates. Using street view images to profile urban
streets offers three main advantages. Firstly, street view images
can support comprehensive measurements as they cover almost
the entire area of a city. Secondly, street view image data is widely
available on both commercial and crowd-sourced platforms (e.g.
Google Maps, Tencent Maps, and Mapillary), enabling accurate and
timely reflection of urban changes. Lastly, street view images can
provide socioeconomic information, furthering our understanding
of human-space interactions due to their inherent connections to
human perception and the socioeconomic aspects of cities.
In this study, we choose the street with multiple street view
images as the minimum spatial unit and aim to predict its functional
and socioeconomic indicators (i.e., urban street profiling), which
presents the following challenges.
â€¢There are numerous street view images on a street, showing
diverse spatial distributions. Existing methods primarily con-
centrate on learning the visual representation of an image by
creating contrastive samples based on augmented images or
additional information (e.g., POIs) [ 21,33,34], overlooking
the spatial correlation between a street and the images on
the street. Consequently, modeling the spatial distribution
of street view images on a street to produce the street visual
representation continues to be a challenge.
â€¢According to the First Law of Geography [ 30], nearby streets
typically share similar functional and socioeconomic indica-
tors. Moreover, only a small number of streets are labeled
due to the time and effort required for labeling. Therefore,
addressing the urban street profiling task for label scarcity
and modeling the spatial adjacency of streets present another
challenge.
Meanwhile, the recent prosperity of Large Language Models
(LLMs) unveils a remarkable opportunity for profiling streets with
images â€“ LLMs have been trained with Internet-scale of multi-modal
data, including enormous amount of images and corresponding tex-
tual descriptions. In this way, LLMs have acquired an extraordinary
knowledge base that makes them highly capable of understand-
ing imagery data, and transforming images to textual descriptions
through prompting. This sheds light on our application, i.e., we
can leverage LLMs to generate highly relevant text descriptions
tailored to our applications like function inference, as embodiment
of the LLMâ€™s knowledge base, to enrich the information that can
be utilized for representing urban streets.
In view of the aforementioned challenges and opportunity, we
propose a semi-supervised Urban Street Profiling Model (USPM)
based on street view imagery and spatial topology (adjacency) of
urban streets. USPM mainly consists of two components: street rep-
resentation learning and urban street profiling. Given a street with
multiple street view images, we propose a spatial context-based
contrastive learning method to generate feature embeddings for
each image. Subsequently, we use an LSTM-based fusion method
to encode these images, producing a comprehensive visual repre-
sentation for the street. Further, we create the descriptions of street
scenes via image captioning based on the SPHINX-V2 [ 25] and
utilize a pre-trained Bert [ 4] to produce the textual representationfor the street. Finally, we combine the visual and textual represen-
tations of a street using an attention method to generate the street
representation. In the component of urban street profiling, we first
construct an urban street network based on the spatial adjacency
of streets, with streets serving as nodes and street representations
serving as node features. Then we employ a semi-supervised graph
learning algorithm to capture the spatial relationships of streets for
prediction.
To summarize, our contributions are as follows:
â€¢We present a semi-supervised Urban Street Profiling Method
that mainly models the street view images and spatial adja-
cency of streets, using limited labels of streets. Powered by
LLMs, we create scene captions for street view images and
incorporate textual knowledge into street representations.
We show that the textual modality is a valuable supplement
for understanding urban streets, enabling the city to be not
only watched but also read.
â€¢We propose a spatial context-based contrastive learning
method to yield the visual representations of street view
images, which considers the spatial relationships between
streets and images to generate contrastive samples. Addi-
tionally, we model the spatial distribution of street view
images on a street using a LSTM network to yield the street
representation.
â€¢We conduct extensive experiments to evaluate the proposed
USPM with real-world datasets. Experimental results show
that the significant performance improvements are achieved
compared to various baseline methods. Data and source
codes are available at: https://github.com/lizc-sdu/USPM.
2 PROBLEM FORMULATION
We formulate the urban street profiling problem with the following
definitions.
Definition 1 (Urban Street). We adopt the OpenStreetMap
data to extract the urban streets, represented as S={ğ‘ 1,ğ‘ 2,Â·Â·Â·,ğ‘ ğ‘›},
whereğ‘ ğ‘–denotes theğ‘–th street segment within the study city and ğ‘›is
the total number of streets.
Definition 2 (Street View Image). Street view images are cap-
tured alongside the road network in urban areas, providing a high-
resolution of urban city from a human perspective.
These images are typically captured by specialized vehicles at
specific sampling points from four different directions, in order to
obtain a comprehensive coverage. In our research, we associate
each street inSwith a set of street view images, captured from
various sampling points along the street. For a given street ğ‘ ğ‘–âˆˆS,
its corresponding images are denoted as SVğ‘–={ğ‘†ğ‘‰ğ‘–,1,ğ‘†ğ‘‰ğ‘–,2,Â·Â·Â·}.
It is important to note that the number of images for each street
(|SV| ) is different.
Definition 3 (Street Indicators). Based on surveys and ur-
ban datasets from multiple sources, certain streets are labeled with
functional and socioeconomic indicators (e.g., urban function labels,
population, and resident consumption).
320Profiling Urban Streets: A Semi-Supervised Prediction Model Based on Street View Imagery and Spatial Topology KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
constructing street scene descriptionsimages on a street
The street view shows a gated entrance to a 
building, with a fence and a lamp post nearby. 
There are no visible people or vehicles on the 
street, and the buildings appear to be a mix of 
residential and commercial structuresâ€¦ Text
EncoderSpatial 
Context -based
Image EncoderLSTM -based
FusionStreet Visual
Representation
Street Textual
RepresentationStreet
Representation
Street Representation Learning Process (Step 1) Urban Street Profiling Process (Step 2)
street view sampling points Street View 
CaptioningCross Entropy 
Loss
(labeled streets)
Prediction
(unlabeled streets)street spatial adjacency
labeled streets
unlabeled streetsSemi -Supervised Graph Learning a street
Figure 1: The framework of USPM.
Definition 4 (Urban Street Profiling). Based on both urban
streets and street view images on the streets, we aim to predict func-
tional labels and socioeconomic indicators for all streets in the study
area, utilizing limited pre-labeled streets.
3 METHOD
3.1 Overview
We present an overview of our proposed framework in Figure 1,
which is comprised of two primary steps: the street representation
learning process and the urban street profiling process.
Step 1: Given a street containing multiple street view images, we
first employ a newly designed spatial context-based image encoder
to produce the feature embedding for each image and utilize the
LSTM-based fusion method to generate the visual representation
for the street. Further, we construct the street scene descriptions via
street view image captioning and employ a pre-trained text encoder
to generate the textual representation for the street. We finally fuse
the visual and textual representations through an attention method
to yield the street representation.
Step 2: We initially construct a network based on the spatial
adjacency of urban streets, with each street serving as a node. The
adjacent relationship between streets is established using K-nearest
neighbors. The street representation serves as the node feature and
the limited number of streets with labels serve as the labeled nodes.
We then employ the graph learning method to predict urban street
indicators. During the training stage, we optimize the model using
cross-entropy loss with the limited labeled streets, and during the
testing stage, we predict the indicators for the unlabeled streets.
3.2 Street Visual Representation Learning
3.2.1 Spatial Context-based Contrastive Learning. To capture the
spatial correlation among street view images, we propose the spatial
context-based contrastive learning method to learn the representa-
tions of street view images (cf. Figure 2). Different from traditional
methods that minimize the feature distance between two urban
images that are geographically adjacent, we directly maximize the
similarity between a street view image and its spatial context, i.e.,
the street it belongs to. This particular choice is motivated by the
Image Encoder
street view sampling points a street
a streetanchorpositive
negative
average
poolingaverage
pooling
anchor representationmaximize 
similarity
minimize 
similaritySpatial Context -based Contrastive Learning
a streetFigure 2: The structure of the spatial context-based image
encoder.
expectation that street view images sampled at the same street
exhibit stronger correlations.
Our proposed method allows various choices of network archi-
tectures for visual encoder design. For simplicity, we adopt the
commonly used ResNet [ 13] to obtain initial visual representations
of street view images. Formally, for street ğ‘ ğ‘–âˆˆSwith street view
imagesSVğ‘–={ğ‘†ğ‘‰ğ‘–,1,ğ‘†ğ‘‰ğ‘–,2,Â·Â·Â·} , the visual representation for ğ‘†ğ‘‰ğ‘–,ğ‘—
can be calculated as follows,
hğ‘–,ğ‘—=Resnet(ğ‘†ğ‘‰ğ‘–,ğ‘—). (1)
Its spatial context (i.e., the representation of a street) is calculated
as the average of all the images on the street,
cğ‘–=1
|SVğ‘–||SV ğ‘–|âˆ‘ï¸
ğ‘—=1hğ‘–,ğ‘—, (2)
where|SVğ‘–|represents the number of images on street ğ‘ ğ‘–.
For an anchor image, we take its spatial context as the positive
sample and the representations of streets in the mini-batch as neg-
ative samples. We employ the InfoNCE [ 28] loss to optimize the
321KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, and Yilong Yin
spatial context-based image encoder,
Lğ‘–ğ‘šğ‘ğ‘”ğ‘’=âˆ’1
ğµğµâˆ‘ï¸
ğ‘–|SV ğ‘–|âˆ‘ï¸
ğ‘—=1logÂ©Â­Â­Â­Â­
Â«exp
hâŠ¤
ğ‘–,ğ‘—cğ‘–
ğœ
Ãğµ
ğ‘˜exp
hâŠ¤
ğ‘–,ğ‘—cğ‘˜
ğœÂªÂ®Â®Â®Â®
Â¬, (3)
whereğµis the size of mini-batch, ğœis a temperature parameter.
Here, we minimize Lğ‘–ğ‘šğ‘ğ‘”ğ‘’to train the spatial context-based image
encoder for generating image feature representations.
3.2.2 LSTM-based Fusion. Using the spatial context-based image
encoder, each image is represented as a feature vector. As the num-
ber of street view images on a street is different, a naive method
to generate the street representation is simply averaging the fea-
ture vectors of images belonging to the street. However, such an
approach ignores the location information of these images and pre-
vents capturing the spatial relationships between images. To better
capture the physical environment observed from the perspective
of pedestrians, we sequence street view images in a specific order
and model them through the LSTM method to generate the visual
representation eğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™
ğ‘–of streetğ‘ ğ‘–,
eğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™
ğ‘–=LSTM({hğ‘–,1,hğ‘–,2,Â·Â·Â·,hğ‘–,|SV ğ‘–|}). (4)
Here, the LSTM model allows the fusion of varying numbers of
street view images, leveraging their spatial connection.
3.3 Street Textual Representation Learning
3.3.1 Street View Captioning. In order to enhance the prediction of
street indicators based on street view images, we propose to extract
compressed and useful semantic descriptions from these images.
Such descriptions are designed to highlight key features that are
indicative of urban street functions, rather than other less relevant
visual details.
Inspired by the success of Large Language Models (LLMs), we
employ the SPHINX-V2, an image-to-text foundation model, to
generate the descriptions for street view images. This model takes
street view images and our elaborately designed instruction as input
and generates a concise yet detailed description for each image, as
illustrated in Figure 1. Our prompt is: Analyze the street view image
and provide a concise description focusing on observable elements
indicative of the streetâ€™s functions. Using our designed instruction,
we aim to guide the LLM focusing on crucial spatial information
within the images, such as buildings and infrastructure, thereby
enriching the functional analysis of urban streets.
3.3.2 Street Textual Representation. Based on the generated im-
age descriptions, we treat the street as a â€œdocumentâ€ containing
multiple â€œsentencesâ€ (image captions). We encode the â€œdocumentâ€
using the pre-trained Bidirectional Encoder Representations from
Transformers (BERT) model to generate the textual representation
eğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™
ğ‘–of streetğ‘ ğ‘–,
eğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™
ğ‘–=BERT(ğ·ğ‘–,1,ğ·ğ‘–,2,Â·Â·Â·,ğ·ğ‘–,|SV ğ‘–|), (5)
whereğ·ğ‘–,ğ‘—denotes the textual descriptions of the ğ‘—th image on
streetğ‘ ğ‘–.3.4 Attention-based Fusion
We propose merging the visual and textual representations of a
street to create a more informative final representation. The two
types of representations come from different modalities: the vi-
sual representation captures the raw visual features in street view
images, while the textual representation emphasizes semantic in-
formation related to urban functions. We specifically utilize the
soft-attention mechanism to combine the two representations and
automatically determine the weight for each type of representation.
Foreğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™
ğ‘–(street visual representation) and eğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™
ğ‘–(street tex-
tual representation) of a given street ğ‘ ğ‘–, we compute its final repre-
sentation eğ‘–as follows,
ğ›¼ğ‘š
ğ‘–=ağ‘‡Â·Tanh Veğ‘š
ğ‘–+b, ğ‘šâˆˆ{ğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™,ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™},
ğ›½ğ‘š
ğ‘–=exp
ğ›¼ğ‘š
ğ‘–
Ã
ğ‘šexp
ğ›¼ğ‘š
ğ‘–,
eğ‘–=âˆ‘ï¸
ğ‘šğ›½ğ‘š
ğ‘–Â·eğ‘š
ğ‘–,(6)
where a,V, and bare learnable parameters.
3.5 Urban Street Profiling
Following the First Law of Geography, the functional and socioe-
conomic indicators of streets have certain co-location patterns or
spatial dependencies. To this end, we construct a street network
based on spatial topology and adopt the graph neural network
(GNN) to capture the spatial relationships of streets for prediction.
3.5.1 Graph Construction Based on Spatial Topology. Using spatial
adjacency of streets, we construct an ğ‘›Ã—ğ‘›matrix Ato represent the
neighboring connections between streets, where ğ‘›represents the
total number of streets. For each street, we identify its K-nearest
streets and designate them as neighboring connections, as outlined
in [40]. In general, we establish a graph G(V ;A), whereV=
{ğ‘ ğ‘–}ğ‘›
ğ‘–=1represents the set of ğ‘›streets serving as nodes and AâˆˆRğ‘›âˆ—ğ‘›
signifies the neighboring relationships among each node. If there
exists a neighboring connection between streets ğ‘ ğ‘–andğ‘ ğ‘—, then Ağ‘–,ğ‘—
will be set to 1; otherwise, Ağ‘–,ğ‘—will be set to 0.
3.5.2 Semi-supervised Graph Learning. Based on the constructed
graph, we take street representations as node features and urban
street profiling as our task. As labeling streets is time-consuming
and labor-intensive, only limited labels are obtained, and our street
profiling task is a typical label-scarcity task. Fortunately, GNNs are
able to learn features of the nodes based on their local neighborhood,
which allows it to work with only a small subset of the labeled nodes,
or even with completely unlabeled graphs. In this context, we use
the graph attention network (GAT) [ 32] to make semi-supervised
graph learning for prediction. We would like to point out that other
GNNs, such as graph convolutional network [ 18] and GraphSAGE
[10], can also be used here.
322Profiling Urban Streets: A Semi-Supervised Prediction Model Based on Street View Imagery and Spatial Topology KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Based on the GAT, the new node feature representation eâ€²
ğ‘–is
computed as
eâ€²
ğ‘–=ğœ(âˆ‘ï¸
ğ‘ ğ‘—âˆˆNğ‘–ğœ‡ğ‘–ğ‘—Weğ‘—),
ğœ‡ğ‘–ğ‘—=exp(ğœ‚ğ‘–ğ‘—)Ã
ğ‘ ğ‘˜âˆˆNğ‘–exp(ğœ‚ğ‘–ğ‘˜),
ğœ‚ğ‘–ğ‘—=ReLU
uğ‘‡Â·
Weğ‘–âˆ¥Weğ‘—
,(7)
where Wanduare learnable parameters, ||is the concatenation
operation, andNğ‘–represents the set of neighbors of ğ‘ ğ‘–.
3.5.3 Training and Prediction. Once the new node feature represen-
tation eâ€²
ğ‘–is acquired, it is fed into the softmax function to generate
the predicted output Ë†yi,
Ë†yğ‘–=softmax eâ€²
ğ‘–. (8)
which denotes the probability that ğ‘ ğ‘–is associated with each candi-
date label.
During the training stage, we use the classification objective and
minimize the cross-entropy loss function,
L=âˆ’1
|Sğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›||Sğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›|âˆ‘ï¸
ğ‘–=1|F|âˆ‘ï¸
ğ‘—=1ğ‘¦ğ‘–ğ‘—log Ë†yğ‘–ğ‘—, (9)
whereğ‘¦ğ‘–ğ‘—represents whether street ğ‘ ğ‘–is labeled with the ğ‘—-th label,
Ë†yğ‘–ğ‘—represents the predicted probability that ğ‘ ğ‘–is labeled with the
ğ‘—-th label,|F|is the number of labels, and |Sğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›|is the number
of training instances. During the prediction stage, we choose the
element with the maximum value in Ë†yias the predicted label of ğ‘ ğ‘–.
4 EXPERIMENT
In this session, we evaluate our model on two urban prediction
tasks with real-world datasets and compare the performance with
various baselines. We also conduct extensive ablation studies to
verify the effectiveness of each component within the proposed
USPM. At last, we assess the robustness of USPM on different sizes
of training data.
4.1 Experimental Settings
4.1.1 Datasets. Our study focuses on the area within the third ring
of Wuhan, China. The urban street network in our study consists of
5,458 streets, collected from OpenStreetMap, as shown in Figure 3.
We collect 57,396 street view images from Tencent Maps in 2018,
covering over 10,000 sampling points distributed across the area,
with images taken from four different directions at each location.
These streets are labeled with one of ten functional labels (including
residential, business office, commercial service, industrial, trans-
portation stations, administrative, educational, medical, sport and
cultural, and park and greenspace) based on the EULUC-China
dataset produced by [ 8], following the process in [ 40]. The train-
ing set consists of 295 labeled streets, with an overall annotation
rate of 5.4%. For the test set (all other streets), labels are set as the
nearest regionâ€™s true label following [ 40]. Except street functional
labels, we also collect the POI data (containing 336,454 POIs in total)
from AMap in 2018, and take the number of POIs on a street as the
street-level socioeconomic indicator following [21].
 
 
 
 
 
 
 
 
  Residential 
Business office
Commercial service  
Industrial
Transportation stations
Administrative
Educational
Medical
Sport and cultural 
Park and greenspaceFigure 3: Urban streets in our study area, where different
functional streets are marked in different colors.
4.1.2 Model Parameters. In the module of street visual representa-
tion learning, we use the ResNet18 pre-trained on the Places 365
database [ 43] (a database designed for scene recognition) as the
backbone model, and set its output dimension as 768. For the LSTM
model that fuses image representations on a street, we adopt one-
layer LSTM and set the output dimension at 256. In the module
of street textual representation learning, we initialize the weight
and tokenizers of our text encoder from BERT1, and set the output
dimension at 768. In the module of graph construction, we set the
number of neighbors for each street (the parameter ğ¾) at 5 when
constructing graphs. In the module of semi-supervised graph learn-
ing, we adopt two-layers GAT with dropout rate being 0.6. When
training the spatial context-based image encoder, we utilize the
Adam optimizer and set the learning rate at 1ğ‘’âˆ’3. Then we set the
learning rate at 5ğ‘’âˆ’4in the attention-based fusion module and 4ğ‘’âˆ’3
in the semi-supervised graph learning module.
4.1.3 Baselines. We compare the performance of USPM with sev-
eral state-of-the-art methods.
â€¢ResNet[ 13]: It is a well-established deep learning model
pre-trained on ImageNet to generate visual representations
of images.
â€¢READ[ 11]: It is a semi-supervised model that leverages
limited labeled data and transfer learning methods on a
partially-labeled dataset to extract robust and lightweight
image representations, utilizing a teacher-student network
with pre-trained models.
â€¢Urban2Vec[ 33]: It is an unsupervised model that utilizes
a triplet loss to learn the visual representations for street
view images, aiming to maximize the similarity of proximate
image pairs, while minimize the dissimilarity of distant pairs.
â€¢GeoCLR[ 21]: It generates contrastive samples for augmented
images and for geographically neighboring images, and trains
a contrastive learning model to generate visual representa-
tions of street view images.
1https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens
323KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, and Yilong Yin
â€¢SceneParse[ 20]: It calculates the coverage ratio of every
object in the street view images to obtain the visual repre-
sentations and employs a graph-based approach to capture
the relationships between images.
â€¢KT-BERT[ 40]: It first captures the key urban entities and
their spatial correlation in street view images and gener-
ates image descriptions based on the vision-language model.
Then it employs BERT to encode these descriptions to gen-
erate the image representations.
â€¢UrbanCLIP[ 37]: It utilizes LLM to produce textual descrip-
tions for satellite images. Then it incorporates contrastive
learning alongside language modeling losses as supervis-
ing for learning the image encoder to generate the visual
representations of satellite images.
We use these models to encode street view images, generating
feature vectors and obtaining street representations by averaging
the features of images on a street. These street representations are
then input into our urban street profiling module.
4.2 Street Function Prediction
Street function prediction aims to predict the functional labels of
streets, which is a multi-class classification problem. We employ the
following metrics to assess the performance of various methods.
1)Accuracy (Acc): It computes the overall correctness of the
modelâ€™s prediction. We utilize ğ´ğ‘ğ‘@ğ¾metric, where ğ¾âˆˆ{1,3,5}.
A score of 1 is assigned if the top ğ¾predicted labels encompass the
ground truth, and 0 otherwise.
2)Weighted-F1: We also use the well-known metric Weighted-
F1 to evaluate model performance, considering the label imbalance
problem in our dataset.
3)Mean reciprocal rank (MRR): Additionally, using Ë†yğ‘–, we can
create a ranking list of predicted labels. Therefore, we also utilize
the metric MRR, which considers the position of real labels in the
ranking lists,
MRR =1
|Sğ‘¡ğ‘’ğ‘ ğ‘¡||Sğ‘¡ğ‘’ğ‘ ğ‘¡|âˆ‘ï¸
ğ‘–=11
ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘–, (10)
whereğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘–is the rank of the real label in the predicted list for
streetğ‘ ğ‘–and|Sğ‘¡ğ‘’ğ‘ ğ‘¡|is the number of test instances.
Table 1 reports the comparative results of different methods for
street function prediction in terms of ACC, Weighted-F1, and MRR,
where each method is run five times to obtain the average results.
From Table 1 we can make the following observations.
â€¢ResNet, READ, and Urban2Vec obtain poor performance, as
they all directly utilize the image learning methods to ex-
tract the visual features from street view images, without
deeply considering the spatial relationship among images.
GeoCLR obtains better performance, as it maximizes the sim-
ilarity of geographically close pairs via contrastive learning,
achieving improvement over the backbone (ResNet) network.
SceneParse outperforms GeoCLR, as it recognizes the impor-
tant objects in the street view images and models the spatial
relationships among images.
â€¢KT-BERT and UrbanCLIP obtain relatively better perfor-
mance than those methods merely capturing visual features
from images, as they generate the descriptions of street viewimages via image captioning and successfully capture the
key entities in street view images. This result suggests that
textual features learned from images can better express se-
mantic knowledge for identifying street functions than visual
features.
â€¢The proposed USPM exhibits the best performance, because
we model the spatial relationships between images and streets
and take the textual modality as a supplement for understand-
ing street functions. For instance, compared with the best
results among baseline methods, USPM achieves an improve-
ment of 5.67% in terms of MRR. Moreover, the results of the
superiority paired t-test confirm that USPMâ€™s improvement
over these baselines is statistically significant, with a ğ‘-value
less than 0.01.
4.3 Socioeconomic Indicator Prediction
Street-level socioeconomic indicator prediction aims to predict the
street-level indicator (i.e., the number of POIs on a street in this
study) based on the street representations. Following [ 21], we split
the indicator into 4 classes evenly and use Accuracy, Macro-F1, and
MRR as the evaluation metrics for the classification problem. To
follow the semi-supervised setting, we randomly select 310 labeled
streets (with an annotation rate of 5.7%) to train our model.
We report the comparative results for socioeconomic indicator
prediction in terms of ACC, Macro-F1, and MRR in Table 1. Com-
pared with the methods (including ResNet, READ, Urban2Vec, Geo-
CLR, and SceneParse) that capture visual features from street view
images, KT-BERT and UrbanCLIP obtain better performance, which
further validates that textual knowledge extracted from image cap-
tions contributes more to predict the socioeconomic indicator. Our
proposed USPM considers not only the spatial adjacency among
streetscapes but also the textual data from images, and obtains
the best classification performance. Compared to KT-BERT, USPM
achieves improvements of 7.98%, 21.53%, and 3.98% in terms of ACC,
Macro-F1, and MRR, respectively.
4.4 Ablation Study
Next, we conduct ablation studies to investigate the effectiveness
of different modules in USPM, including the effectiveness of the
textual modality, the spatial context-based image encoder, the LLM-
based image captioning, the LSTM-based fusion method, and the
semi-supervised graph learning method.
4.4.1 Effectiveness of Textual Modality. USPM introduces the tex-
tual modality as a supplement to complement street view imagery.
Thus, it is natural to investigate the effectiveness of incorporating
textual information into street representations. To this end, we
compare USPM with a standard visual model, named USPM-Visual,
which directly feeds the street visual representations into the street
profiling module. We also design a variant named USPM-Textual,
which directly feeds the street textual representations into the street
profiling module.
The performance comparison of USPM-Visual, USPM-Textual,
and USPM is depicted in Figure 4. It is evident that USPM-Visual
underperforms USPM, indicating that the lack of supplementary
textual information leads to a decline in performance. This under-
scores the importance of incorporating textual modality to achieve a
324Profiling Urban Streets: A Semi-Supervised Prediction Model Based on Street View Imagery and Spatial Topology KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 1: Performance comparison on two downstream tasks, where the performance improvements of USPM are compared
with the best results among these baseline methods, marked by the asterisk.
MethodStreet Function Prediction Socioeconomic Prediction
ACC@1 ACC@3 ACC@5 Weighted-F1 MRR ACC Macro-F1 MRR
ResNet 36.46 66.43 82.34 36.93 55.62 37.92 35.58 61.49
READ 37.86 65.19 79.85 33.02 56.02 29.05 21.44 55.11
Urban2Vec 35.93 67.36 83.44 33.92 55.51 37.79 34.02 62.07
GeoCLR 39.76 69.99 85.57 39.00 58.40 37.48 35.79 61.61
SceneParse 43.16* 72.80* 84.39 41.52 60.97* 39.41 33.44 63.18
KT-BERT 43.03 71.49 85.18 40.64 60.69 40.46* 33.77 63.49*
UrbanCLIP 42.95 72.45 85.85* 42.02* 60.86 39.96 37.76* 63.22
USPM 47.01 77.24 88.72 44.04 64.43 43.69 41.04 66.02
Improvement(%) 8.92 6.10 3.34 4.81 5.67 7.98 8.69 3.98
/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000026/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni0000002f/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000038/uni00000036/uni00000033/uni00000030/uni00000013/uni00000011/uni00000016/uni0000001b/uni00000013/uni00000011/uni00000017/uni00000016/uni00000013/uni00000011/uni00000017/uni0000001b /uni00000024/uni00000026/uni00000026
/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000026/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni0000002f/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000038/uni00000036/uni00000033/uni00000030/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000011/uni00000017/uni00000013/uni00000013/uni00000011/uni00000017/uni00000018 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000010/uni00000029/uni00000014
/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000026/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni0000002f/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000038/uni00000036/uni00000033/uni00000030/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018 /uni00000030/uni00000035/uni00000035
(a) Street Function Prediction
/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000026/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni0000002f/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000038/uni00000036/uni00000033/uni00000030/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000011/uni00000017/uni00000013/uni00000013/uni00000011/uni00000017/uni00000018 /uni00000024/uni00000026/uni00000026
/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000026/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni0000002f/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000038/uni00000036/uni00000033/uni00000030/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000011/uni00000017/uni00000013/uni00000013/uni00000011/uni00000017/uni00000018 /uni00000030/uni00000044/uni00000046/uni00000055/uni00000052/uni00000010/uni00000029/uni00000014
/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000044/uni0000004f /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000026/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni0000002f/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030 /uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000038/uni00000036/uni00000033/uni00000030/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013 /uni00000030/uni00000035/uni00000035
(b) Socioeconomic Indicator Prediction
Figure 4: Performance comparison of different variants.
comprehensive street representation. When compared with USPM-
Visual, USPM-Textual yields superior results, further affirming that
textual features from LLM-generated image descriptions encompass
more semantic knowledge than visual features. USPM demonstrates
the best performance, validating the importance of integrating vi-
sual and textual features to generate street representations. Fur-
thermore, it is noteworthy that our model variant USPM-Visual still
surpasses the best results in baselines.
4.4.2 Effectiveness of Spatial Context-based Image Encoder. We
propose a new spatial context-based image encoder to capture the
spatial relationships between street view images and streets for
generating the visual representations of images. To validate theefficacy of modeling the spatial context of images, we design a
variant named USPM w/o SC which directly uses the ResNet to
encode the street view images. Figure 4 shows the performance
comparison between USPM and USPM w/o SC. The substantial
performance gaps observed between these two models suggest that
modeling the spatial context information of street view images
can promote the visual representation learning for street function
(socioeconomic indicator) prediction.
4.4.3 Effectiveness of LLM-based Image Captioning. In the pro-
posed USPM, we employ SPHINX-V2 with our designed prompt to
generate detailed textual descriptions for street view images. To val-
idate the quality of the text generated by LLM, we design a variant
325KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, and Yilong Yin
named USPM w/o LLM which uses a vision-language model [ 40]
trained through the Bottom-Up and Top-Down Attention to obtain
image captions for comparison. Figure 4 clearly shows that USPM
consistently outperforms this variant across all indicators. Such
results validate the ability of LLM to extract the textual semantics
of street view images.
4.4.4 Effectiveness of LSTM-based Fusion for Generating Street Rep-
resentations. To validate the effectiveness of USPM in fusing visual
representations of images on a street via a LSTM architecture to
generate street representations, we design the variant USPM w/o
LSTM, which utilizes average pooling to obtain the overall rep-
resentation of the street. As shown in Figure 4, USPM performs
better than USPM w/o LSTM obviously, suggesting that LSTM can
capture inherent spatial correlations among images on a street and
generate a more effective street visual representation.
4.4.5 Effectiveness of Semi-Supervised Graph Learning. In the pro-
posed USPM, we build an urban street network based on the spatial
adjacency among streets and employ the semi-supervised graph
learning method for prediction. To validate the effectiveness of
the semi-supervised graph learning, we design a variant named
USPM w/o Graph which directly feeds the street representations
into a fully-connected layer to make prediction. Figure 4 shows
that USPM performs better than USPM w/o Graph on both tasks,
validating the effectiveness of our proposed method.
4.5 Robustness Testing
We explore the robustness of our proposed USPM with different
training data sizes. We progressively reduce the number of training
instances from 300 to 100 with a step of 50 to explore how the
prediction performance of our method changes on both tasks. It
is important to note that we should ensure that all labels (i.e., 10
classes or 4 classes) are present in the labelled streets in the street
function (socioeconomic indicator) prediction tasks. The prediction
results with regard to different training data sizes are shown in
Figure 5. We can observe that our method shows very little decline
in predicting the socioeconomic indicator across different training
data sizes, indicating model insensitivity to the size of training
data. While for the street function prediction task, the performance
decreases when the data size is smaller than 200, which may be due
to the fact that street function identification is more challenging
and requires more comprehensive training instances.
5 RELATED WORK
5.1 Street View Imagery Mining
Street view images are obtained by map service providers along
the urban road network, offering additional clues about human
activities. These images usually cover most city areas, serving as a
valuable data source for analyzing urban physical environments.
Rcent studies have utilized street view images to identify the scene
elements in physical urban spaces. For example, Doersch et al.
[5] utilize visual features like windows and balconies in street
view images to automatically distinguish Paris from other cities. Li
and Carlo [ 22] collect street view images from worldwide cities to
extract the vegetation percentage and compare the greenery across
urban areas. Furthermore, street view images can be utilized for
/uni00000016/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a
/uni00000024/uni00000026/uni00000026
/uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000010/uni00000029/uni00000014
/uni00000030/uni00000035/uni00000035(a) Street Function Prediction
/uni00000016/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a
/uni00000024/uni00000026/uni00000026
/uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000010/uni00000029/uni00000014
/uni00000030/uni00000035/uni00000035 (b) Socioeconomic Indicator Prediction
Figure 5: Performance comparison with different number of
training instances.
the estimation of a buildingâ€™s height and number of floors, enabling
the generation of its 3D model [19].
In addition to offering visible physical details of urban spaces,
street view imagery can also be employed in analyzing regional so-
cioeconomic characteristics. For example, Kang et al. [ 16] combine
street-level imagery, house photos and surrounding environmental
conditions to predict housing prices. Li et al. [ 21] integrate satellite
and street view imagery with city structural data, to predict socioe-
conomic indicators at street and neighborhood levels. Noorian et
al. [27] utilize street images to identify the types of POIs (stores)
located in buildings. They extract the text storefronts to categorize
the POIs into multiple types, such as bookstores and pharmacies.
Moreover, due to the development of multiple urban data collec-
tion and data mining techniques, more researchers now adopt deep
learning approaches to identify urban functions based on street
view imagery. For example, Srivastava et al. [ 29] conduct a multi-
label function classification at the building level using street view
images acquired at multiple zoom levels and corresponding govern-
ment census data. Ye et al. [ 38] work to identify urban function and
spatial quality from social media data, and use street view imagery
to enhance the performance. Zhang et al. [ 40] present to solely
rely on imagery data without other data to predict the functions of
streets. They utilize image captioning tools to generate the descrip-
tions of street images, and employ pretrained language encoder to
obtain the street semantic representations. Yan et al. [ 37] leverage
the power of Large Language Models and the knowledge of textual
modality to train an unsupervised image encoder for generating the
visual representations of satellite images. However, most current
studies solely focus on capturing the visual features of individ-
ual images, instead of integrating the visual and textual features
generated from street view imagery to yield street representations.
5.2 Large Language Model
Over the past years, LLMs have gained significant attention, show-
ing remarkable potential across various fields [ 6,7]. These models
are characterized by their extensive pre-training on large-scale
datasets, which imbues them with unparalleled emergent abili-
ties, including contextual reasoning, complex problem solving, and
zero-shot adaptability across diverse tasks [ 3,17]. Recent studies
suggest that LLMs have encoded a wealth of urban factual and
spatio-temporal knowledge from their training data. For instance,
326Profiling Urban Streets: A Semi-Supervised Prediction Model Based on Street View Imagery and Spatial Topology KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Gurnee [ 9] demonstrates that LLMs learn representations of space
and time, which are resilient to changes of prompts and coherent
across different urban areas. Ji et al. [ 15] conduct various experi-
ments to indicate that LLMs can represent the geometric proper-
ties and relationships of spatial objects. Furthermore, Prabin et al.
[1] explore various LLMsâ€™ geospatial skills, including geospatial
knowledge, awareness, and reasoning. By leveraging appropriate
prompts, LLMs can utilize the encoded geospatial knowledge to
produce the desired outcomes. To this end, several studies have
applied LLMs to a range of urban perception tasks. Manvi et al. [ 26]
extract geospatial knowledge from LLMs by using prompts aug-
mented with auxiliary map data, achieving superior performance in
real-world geospatial tasks. Zheng et al. [ 41] employ LLMs in traffic
safety applications through prompt information, including accident
reports automation, traffic data augmentation, and multisensory
data analysis. In this study, we utilize our pre-defined prompt to
steer LLMs to generate the descriptions of street view imagery,
which inherently contain the geospatial knowledge.
6 CONCLUSION
Profiling urban streets in terms of functional and socioeconomic
labels is crucial for urban planning. In this paper, we present a semi-
supervised Urban Street Profiling Model (USPM) that learns street
representations from street view imagery by integrating visual and
textual modality and predicts the functional and socioeconomic
labels of streets using a semi-supervised graph learning framework.
We first model multiple street view images on a street to learn
the street visual representation based on the spatial context-based
contrastive learning method. Powered by LLMs, we then create
the descriptions of street scenes and produce the street textual
representation. We finally combine the visual and textual repre-
sentations of a street and feed it into the semi-supervised graph
learning framework for urban street profiling. The experimental re-
sults with real-world datasets demonstrate that USPM outperforms
state-of-the-art baseline methods on two urban prediction tasks.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science
Foundation of China under Grant (No. 61906107 and 62202270),
the Young Scholars Program of Shandong University. W.H. was
supported by the Knut and Alice Wallenberg Foundation.
REFERENCES
[1]Prabin Bhandari, Antonios Anastasopoulos, and Dieter Pfoser. 2023. Are large
language models geospatially knowledgeable?. In Proceedings of the 31st ACM
International Conference on Advances in Geographic Information Systems. 1â€“4.
[2]Junxiang Bing, Meng Chen, Min Yang, Weiming Huang, Yongshun Gong, and
Liqiang Nie. 2023. Pre-Trained semantic embeddings for POI categories based on
multiple contexts. IEEE Transactions on Knowledge and Data Engineering 35, 09
(2023), 8893â€“8904.
[3]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al .2021. On the opportunities and risks of foundation models. arXiv
preprint arXiv:2108.07258 (2021).
[4]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[5]Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei Efros. 2012.
What makes paris look like paris? ACM Transactions on Graphics 31, 4 (2012).
[6]Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu.
2023. Towards next-generation intelligent assistants leveraging llm techniques.InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 5792â€“5793.
[7]Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Yao, Jingshuai Zhang, Hengshu Zhu,
Fuzhen Zhuang, and Hui Xiong. 2023. Recruitpro: A pretrained language model
with skill-aware prompt learning for intelligent recruitment. In Proceedings of
the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
3991â€“4002.
[8]Peng Gong, Bin Chen, Xuecao Li, Han Liu, Jie Wang, Yuqi Bai, Jingming Chen,
Xi Chen, Lei Fang, Shuailong Feng, et al .2020. Mapping essential urban land
use categories in China (EULUC-China): Preliminary results for 2018. Science
Bulletin 65, 3 (2020), 182â€“187.
[9]Wes Gurnee and Max Tegmark. 2023. Language models represent space and time.
arXiv preprint arXiv:2310.02207 (2023).
[10] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in Neural Information Processing Systems 30
(2017).
[11] Sungwon Han, Donghyun Ahn, Hyunji Cha, Jeasurk Yang, Sungwon Park, and
Meeyoung Cha. 2020. Lightweight and robust representation of economic scales
from satellite imagery. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, Vol. 34. 428â€“436.
[12] Sungwon Han, Donghyun Ahn, Sungwon Park, Jeasurk Yang, Susang Lee, Jihee
Kim, Hyunjoo Yang, Sangyoon Park, and Meeyoung Cha. 2020. Learning to
score economic development from satellite imagery. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
2970â€“2979.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 770â€“778.
[14] Weiming Huang, Lizhen Cui, Meng Chen, Daokun Zhang, and Yao Yao. 2022.
Estimating urban functional distributions with semantics preserved POI embed-
ding. International Journal of Geographical Information Science 36, 10 (2022),
1905â€“1930.
[15] Yuhan Ji and Song Gao. 2023. Evaluating the effectiveness of large language
models in representing textual descriptions of geometry and spatial relations.
arXiv preprint arXiv:2307.03678 (2023).
[16] Yuhao Kang, Fan Zhang, Wenzhe Peng, Song Gao, Jinmeng Rao, Fabio Duarte,
and Carlo Ratti. 2021. Understanding house price appreciation using multi-source
big geo-data and machine learning. Land Use Policy 111 (2021), 104919.
[17] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
[18] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[19] Nicolas J Kraff, Michael Wurm, and Hannes TaubenbÃ¶ck. 2020. The dynamics of
poor urban areas-analyzing morphologic transformations across the globe using
Earth observation data. Cities 107 (2020), 102905.
[20] Jihyeon Lee, Dylan Grosz, Burak Uzkent, Sicheng Zeng, Marshall Burke, David Lo-
bell, and Stefano Ermon. 2021. Predicting livelihood indicators from community-
generated street-level imagery. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 268â€“276.
[21] Tong Li, Shiduo Xin, Yanxin Xi, Sasu Tarkoma, Pan Hui, and Yong Li. 2022.
Predicting multi-level socioeconomic indicators from structural urban imagery. In
Proceedings of the 31st ACM International Conference on Information & Knowledge
Management. 3282â€“3291.
[22] Xiaojiang Li and Carlo Ratti. 2018. Mapping the spatial distribution of shade
provision of street trees in Boston using Google Street View panoramas. Urban
Forestry & Urban Greening 31 (2018), 109â€“119.
[23] Yi Li, Weiming Huang, Gao Cong, Hao Wang, and Zheng Wang. 2023. Urban
region representation learning with OpenStreetMap building footprints. In Pro-
ceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 1363â€“1373.
[24] Zechen Li, Weiming Huang, Kai Zhao, Min Yang, Yongshun Gong, and Meng
Chen. 2024. Urban region embedding via multi-view contrastive prediction. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 8724â€“8732.
[25] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu,
Chen Lin, Wenqi Shao, Keqin Chen, et al .2023. Sphinx: The joint mixing of
weights, tasks, and visual embeddings for multi-modal large language models.
arXiv preprint arXiv:2311.07575 (2023).
[26] Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell,
and Stefano Ermon. 2023. Geollm: Extracting geospatial knowledge from large
language models. arXiv preprint arXiv:2310.06213 (2023).
[27] Shahin Sharifi Noorian, Achilleas Psyllidis, and Alessandro Bozzon. 2019. ST-
Sem: A multimodal method for points-of-interest classification using street-level
imagery. In Web Engineering: 19th International Conference, ICWE 2019, Daejeon,
South Korea, June 11â€“14, 2019, Proceedings 19. Springer, 32â€“46.
[28] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
327KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, and Yilong Yin
[29] Shivangi Srivastava, John E Vargas-MuÃ±oz, David Swinkels, and Devis Tuia. 2018.
Multilabel building functions classification from ground pictures using convolu-
tional neural networks. In Proceedings of the 2nd ACM SIGSPATIAL International
Workshop on AI for Geographic Knowledge Discovery. 43â€“46.
[30] Waldo R Tobler. 1970. A computer movie simulating urban growth in the Detroit
region. Economic Geography 46, sup1 (1970), 234â€“240.
[31] Wei Tu, Jinzhou Cao, Yang Yue, Shih-Lung Shaw, Meng Zhou, Zhensheng Wang,
Xiaomeng Chang, Yang Xu, and Qingquan Li. 2017. Coupling mobile phone and
social media data: A new approach to understanding urban functions and diurnal
patterns. International Journal of Geographical Information Science 31, 12 (2017),
2331â€“2358.
[32] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph attention networks. In International Confer-
ence on Learning Representations.
[33] Zhecheng Wang, Haoyuan Li, and Ram Rajagopal. 2020. Urban2vec: Incorporating
street view imagery and pois for multi-modal urban neighborhood embedding. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1013â€“1020.
[34] Yanxin Xi, Tong Li, Huandong Wang, Yong Li, Sasu Tarkoma, and Pan Hui. 2022.
Beyond the first law of geography: Learning representations of satellite imagery
by leveraging point-of-interests. In Proceedings of the ACM Web Conference 2022.
3308â€“3316.
[35] Ronghui Xu, Meng Chen, Yongshun Gong, Yang Liu, Xiaohui Yu, and Liqiang
Nie. 2023. Tme: Tree-guided multi-task embedding learning towards semantic
venue annotation. ACM Transactions on Information Systems 41, 4 (2023), 1â€“24.
[36] Ronghui Xu, Weiming Huang, Jun Zhao, Meng Chen, and Liqiang Nie. 2023. A
spatial and adversarial representation learning approach for land use classifica-
tion with POIs. ACM Transactions on Intelligent Systems and Technology 14, 6(2023), 1â€“25.
[37] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen,
Roger Zimmermann, and Yuxuan Liang. 2023. When urban region profiling
meets large language models. arXiv preprint arXiv:2310.18340 (2023).
[38] Chao Ye, Fan Zhang, Lan Mu, Yong Gao, and Yu Liu. 2021. Urban function
recognition by integrating social media and street-level imagery. Environment
and Planning B: Urban Analytics and City Science 48, 6 (2021), 1430â€“1444.
[39] Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, and Jun Luo. 2020. Curb-
gan: Conditional urban traffic estimation through spatio-temporal generative
adversarial networks. In Proceedings of the 26th ACM SIGKDD International Con-
ference on Knowledge Discovery & Data Mining. 842â€“852.
[40] Yan Zhang, Pengyuan Liu, and Filip Biljecki. 2023. Knowledge and topology: A
two layer spatially dependent graph neural networks to identify urban functions
with time-series street view image. ISPRS Journal of Photogrammetry and Remote
Sensing 198 (2023), 153â€“168.
[41] Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Zijin Wang, and Shengxuan
Ding. 2023. ChatGPT is on the horizon: Could a large language model be all we
need for Intelligent Transportation? arXiv preprint arXiv:2303.05382 (2023).
[42] Yu Zheng, Furui Liu, and Hsun-Ping Hsieh. 2013. U-air: When urban air quality
inference meets big data. In Proceedings of the 19th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. 1436â€“1444.
[43] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
2017. Places: A 10 million image database for scene recognition. IEEE Transactions
on Pattern Analysis and Machine Intelligence 40, 6 (2017), 1452â€“1464.
328