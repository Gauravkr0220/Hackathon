RC-Mixup: A Data Augmentation Strategy against Noisy Data
for Regression Tasks
Seong-Hyeon Hwang
sh.hwang@kaist.ac.kr
KAIST
Daejeon, Republic of KoreaMinsu Kim
ms716@kaist.ac.kr
KAIST
Daejeon, Republic of KoreaSteven Euijong Whangâˆ—
swhang@kaist.ac.kr
KAIST
Daejeon, Republic of Korea
ABSTRACT
We study the problem of robust data augmentation for regression
tasks in the presence of noisy data. Data augmentation is essential
for generalizing deep learning models, but most of the techniques
like the popular Mixup are primarily designed for classification
tasks on image data. Recently, there are also Mixup techniques that
are specialized to regression tasks like C-Mixup. In comparison to
Mixup, which takes linear interpolations of pairs of samples, C-
Mixup is more selective in which samples to mix based on their label
distances for better regression performance. However, C-Mixup
does not distinguish noisy versus clean samples, which can be prob-
lematic when mixing and lead to suboptimal model performance.
At the same time, robust training has been heavily studied where
the goal is to train accurate models against noisy data through
multiple rounds of model training. We thus propose our data aug-
mentation strategy RC-Mixup, which tightly integrates C-Mixup
with multi-round robust training methods for a synergistic effect. In
particular, C-Mixup improves robust training in identifying clean
data, while robust training provides cleaner data to C-Mixup for it
to perform better. A key advantage of RC-Mixup is that it is data-
centric where the robust model training algorithm itself does not
need to be modified, but can simply benefit from data mixing. We
show in our experiments that RC-Mixup significantly outperforms
C-Mixup and robust training baselines on noisy data benchmarks
and can be integrated with various robust training methods.
CCS CONCEPTS
â€¢Computing methodologies â†’Regularization.
KEYWORDS
Regression, Data augmentation, Robust training
ACM Reference Format:
Seong-Hyeon Hwang, Minsu Kim, and Steven Euijong Whang. 2024. RC-
Mixup: A Data Augmentation Strategy against Noisy Data for Regression
Tasks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671993
âˆ—Corresponding author
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36719931 INTRODUCTION
Deep learning is widely used in applications that perform regres-
sion tasks including manufacturing, climate prediction, and finance.
However, one of the challenges is a lack of enough training data,
and data augmentation techniques have been proposed as a solution
for better generalizing the trained models. A representative tech-
nique is Mixup [ 24,25,52,53], which mixes two samples by linear
interpolation to estimate the label of any sample in between. How-
ever, Mixup is primarily designed for classification tasks mainly
on image data along with most of the other data augmentation
techniques [ 14,27] and does not readily perform well on regression
tasks where the goal is to predict real numbers.
Recently, there is an increasing literature on data augmentation
designed for regression tasks, and C-Mixup [ 50] is the state-of-the-
art Mixup-based method. In order to avoid arbitrarily-incorrect
labels, each sample is mixed with a neighboring sample in terms of
label distance where the selection follows the proposed sampling
probability calculated using a Gaussian kernel that is configured
using a bandwidth parameter. The larger the parameter the wider
the distribution, which means that mixing can be done with distant
neighbors in terms of label distance. In a sense, the bandwidth
indicates whether Mixup should be performed, and to what degree.
C-Mixup has theoretical guarantees for generalization and handling
correlation shifts in the data.
At the same time, robustness against noise is becoming increas-
ingly important for regression tasks. For example, in semiconductor
manufacturing, the layer thickness in a 3D semiconductor needs to
be predicted for defect detection using a model. In this case, labels
(e.g., layer thickness) can be noisy due to erroneous or malfunc-
tioning measurement equipment, leading to a decline in prediction
model performance and thus revenue. Hence, global semiconduc-
tor companies make great efforts to ensure that their models are
robust against noise. To address this challenge, there is a recent
line of multi-round robust training [ 43,44] where noisy samples
are removed or fixed based on their loss values through multiple
model trainings.
We thus contend that integrating C-Mixup with robust training
methods is desirable, but this is not trivial. C-Mixup is not explicitly
designed to be robust against noisy data and may be prone to incor-
rect mixing because any out-of-distribution samples are mixed just
the same way as in-distribution samples. C-Mixup is robust against
correlation shifts, which assume the same data distribution, but un-
fortunately cannot cope with noise in general. More fundamentally,
data augmentation is to add data, while robust training is to clean
(select or refurbish) data, so the two seem to even be contradictory
operations. A naÃ¯ve approach is to run the two methods in sequence,
e.g., run C-Mixup and then robust training or in the other ordering.
 
1155
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Seong-Hyeon Hwang et al.
d Noisy 
samples Step 2: C-Mixup and Tuning 
Current 
model Step 3: Model Updating Step 1: Cleaning (Selecting) 
Mixed 
samples Clean 
samples c c e 
c b 
a Hypothetical 
correct model 
b 
a b 
a 
Figure 1: RC-Mixup tightly integrates C-Mixup with multi-round robust training techniques for a synergistic effect: C-Mixup
improves robust training in identifying clean data, while robust training provides (intermediate) clean data for C-Mixup.
Suppose the x-axis is the only feature, and the y-axis is the label. Also, there are two clean samples ğ‘andğ‘and three noisy
samplesğ‘,ğ‘‘, andğ‘’. In Step 1, suppose that cleaning removes ğ‘‘andğ‘’(the exact outcome depends on the robust training technique).
In Step 2, we perform C-Mixup possibly with bandwidth tuning to generate mixed samples. Here we mix the sample pairs ( ğ‘,ğ‘)
and (ğ‘,ğ‘) to generate the mixed samples denoted as star shapes. Notice that C-Mixup selectively mixes samples that have closer
labels, so in this example ( ğ‘,ğ‘) are not mixed. In Step 3, the augmented samples can be used to train an improved regression
model, which can then be used for better cleaning in the next round.
However, either C-Mixup will end up running on noisy data or
robust training would not benefit from augmented data.
We propose the novel data augmentation strategy of tightly inte-
grating C-Mixup and multi-round robust training for a synergistic
effect (see Figure 1). We call our framework RC-Mixup to emphasize
the robustness of C-Mixup. Each robust training round typically
consists of cleaning (Step 1) and model updating (Step 3) steps. Be-
tween these two steps, we run C-Mixup (Step 2) so that it benefits
from the intermediate clean data that is identified by the cleaning.
In addition, the model updating step now benefits from the aug-
mented data produced by C-Mixup and produces a more accurate
regression model that can clean data better. Another benefit of this
integration is that it is data-centric where the robust training algo-
rithm itself does not need to be modified because RC-Mixup is only
augmenting the data. Hence, RC-Mixup is compatible with any ex-
isting multi-round robust training algorithm like Iterative Trimmed
Loss Minimization (ITLM) [ 43], O2U-Net [ 18], and SELFIE [ 44]. A
technical challenge is efficiency where we would like to keep C-
Mixupâ€™s bandwidth up-to-date during the multiple rounds in robust
training. We propose to periodically update the bandwidth, but
doing so reliably by evaluating candidate bandwidth values for sev-
eral robust training rounds before choosing the one to use. We can
optionally speedup this process by simply updating the bandwidth
in one direction with some tradeoff in performance as well.
We perform extensive experiments of RC-Mixup on various
regression benchmarks and show how it significantly outperforms
C-Mixup in terms of robustness against noise. While RC-Mixup
utilizes a small validation set, we show it is sufficient to use the
robust training to generate one from the training set. In addition,
RC-Mixup also outperforms existing robust training techniques
that do not augment their data.
Summary of Contributions: (1) We propose RC-Mixup, the
first selective mixing framework for regression tasks that is alsorobust against noisy data. (2) We tightly integrate the state-of-the-
art C-Mixup with multi-round robust training techniques where
C-Mixup utilizes intermediate clean data and has a synergistic
effect with robust training. (3) We perform extensive experiments
and show how RC-Mixup significantly outperforms baselines by
utilizing this synergy on noisy real and synthetic datasets.
2 BACKGROUND
Notations. LetD={(ğ‘¥,ğ‘¦)}âˆ¼ğ‘ƒbe the training set where ğ‘¥âˆˆğ‘‹
is ağ‘‘-dimensional input sample, and ğ‘¦âˆˆğ‘Œis anğ‘’-dimensional
label. LetDğ‘£={(ğ‘¥ğ‘£,ğ‘¦ğ‘£)}âˆ¼ğ‘ƒğ‘¡be the validation set, where ğ‘ƒğ‘¡is
the distribution of the test set. Let ğœƒmodel parameters, ğ‘“ğœƒbe a re-
gression model, and â„“ğœƒthe loss function that returns a performance
score comparing ğ‘“ğœƒ(ğ‘¥)with the true label ğ‘¦using Mean Squared
Error (MSE) loss.
Mixup. Mixup [ 53] takes a linear interpolation between any pair
of samples ğ‘¥ğ‘–andğ‘¥ğ‘—with the labels ğ‘¦ğ‘–andğ‘¦ğ‘—to produce the
new sample ğœ†ğ‘¥ğ‘–+(1âˆ’ğœ†)ğ‘¥ğ‘—with the label ğœ†ğ‘¦ğ‘–+(1âˆ’ğœ†)ğ‘¦ğ‘—where
ğœ†âˆ¼ğµğ‘’ğ‘¡ğ‘(ğ›¼,ğ›¼). According to Zhang et al . [53] , mixing all samples
outperforms Empirical Risk Minimization on many classification
datasets. This strategy works well for classification where the labels
are one-hot encoded where many samples may have the same label.
In regression, however, such linear interpolations are not suitable
as labels are in a continuous space instead of a discrete space where
one-hot encodings cannot be used. Here two distant samples may
have labels that are arbitrarily different, and simply mixing them
could result in intermediate labels that are very different than the
actual label.
C-Mixup. C-Mixup [50] overcomes the limitation of Mixup and
is the state-of-the-art approach for regression tasks on clean data. C-
Mixup proposes a sampling probability distribution for each sample
based on the label distance between a neighbor using a symmetric
 
1156A Data Augmentation Strategy against Noisy Data for Regression KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Gaussian kernel. C-Mixup selects a sample to mix following a sam-
pling probability and generates a new sample and label pair similar
to Mixup. The sampling probability introduced by C-Mixup is:
ğ‘ƒ((ğ‘¥ğ‘—,ğ‘¦ğ‘—)|(ğ‘¥ğ‘–,ğ‘¦ğ‘–))âˆ exp(âˆ’ğ‘‘(ğ‘¦ğ‘–,ğ‘¦ğ‘—)
ğ‘2) (1)
whereğ‘‘(ğ‘¦ğ‘–,ğ‘¦ğ‘—)is the label distance and ğ‘is the bandwidth of a
kernel function. Since the values of a probability mass function
sum to one, C-Mixup normalizes the calculated values. The band-
width is the key parameter to tune. As the bandwidth increases, the
probability distribution becomes more uniform, thereby making
C-Mixup similar to Mixup. As the bandwidth decreases, samples are
only mixed with their nearest neighbors, and C-Mixup eventually
becomes identical to Empirical Risk Minimization (ERM).
Robust Training. There is an existing literature [ 7,16,20,41,43,
44] on robust training where the goal is to perform accurate model
training against noisy data. While there are many approaches, we
focus on multi-round robust training where noisy data is either
removed or fixed progressively during the model training iterations.
As a default, we use Iterative Trimmed Loss Minimization (ITLM)
[43] as a representative clean sample selection method, although
we can use other methods as we demonstrate in Section 6.5. ITLM
repeatedly removes a certain percentage of the training data where
the intermediate modelâ€™s predictions differ from the labels and
solves the following optimization problem:
min
ğ‘†:|ğ‘†|=âŒŠğœğ‘›âŒ‹âˆ‘ï¸
ğ‘ ğ‘–âˆˆğ‘†â„“ğœƒ(ğ‘ ğ‘–)
whereğ‘†âŠ†D is the current clean data, ğ‘ ğ‘–is a sample in ğ‘†,â„“ğœƒis a
loss function, and ğœis a parameter that indicates the ratio of clean
samples in the training data. ITLM is widely used because of its
simplicity and scalability.
However, any other multi-round robust training technique [ 6,18,
44] can be used as well. We demonstrate this point in Section 6.5
where we replace ITLM with O2U-Net [ 18], which is a different
clean sample selection method, and SELFIE [ 44], which refurbishes
labels of unclean samples.
3C-MIXUP VULNERABILITY TO NOISY DATA
C-Mixup assumes that all samples are clean data and performs
Mixup using a fixed bandwidth. Although C-Mixup is known to be
robust against covariate shifts in the data where the distribution
ğ‘ƒ(ğ‘‹)may change, but not ğ‘ƒ(ğ‘Œ|ğ‘‹), the data is still assumed to be
clean. In comparison, we consider noisy data where ğ‘ƒ(ğ‘Œ|ğ‘‹)may
change as well.
We demonstrate how C-Mixup is vulnerable to such general
noise in Figure 2a. We experiment on the Spectrum dataset for
manufacturing and add random Gaussian noise to the labels (see
more details in Section 6). We evaluate C-Mixup with the tuned
bandwidth on noisy data. This model is then evaluated on clean
test data. We compare C-Mixup with using ERM only and compare
the Root Mean Squared Error (RMSE, see definition in Section 6)
results where a lower value is better. As the noise ratio increases,
the RMSE of C-Mixup trained on noisy data increases. In addition,
regardless of the noise ratio, the RMSE of C-Mixup increases as
much as ERMâ€™s RMSE does, showing the vulnerability of C-Mixup.
0 10 20 30 40
Noise ratio (%)01020Test RMSE
ERM
C-Mixup(a)
0 10 20 30 40
Noise ratio (%)51525Optimal Bandwidth
 (b)
Figure 2: We evaluate C-Mixup on noisy data where we add
label noise to the Spectrum dataset. A lower RMSE means
better model performance. (a) As the noise ratio increases,
both ERM and C-Mixup gradually perform worse where their
performance gap does not change much. (b) In addition, the
optimal bandwidth may actually increase where mixing di-
lutes out-of-distribution data from negatively impacting the
model performance.
To better understand how C-Mixup is affected by noise, we
also determine the optimal bandwidth, which results in the best
model performance across the different noise ratios as shown in
Figure 2b using the same experimental setup. As a result, as the
noise ratio increases, the optimal bandwidth also increases. This
result is counter-intuitive at first glance because it seems like noisy
data should be mixed less. However, we suspect that mixing also
has a dilution effect where out-of-distribution samples have less
impact on the model training if they are mixed with other clean
samples. We do not claim that this trend always holds, and the
point is that mixing has a non-trivial effect on noisy data. Thus,
it becomes difficult to find a single bandwidth that works best for
both clean and noisy data.
The C-Mixup paper [ 50] also performs a robust training experi-
ment against label noise, but it assumes a fixed noise ratio and does
not necessarily show the entire story. The more extensive results
here suggest that C-Mixup is not designed to handle noise and can
benefit from robust training methods. We thus propose a natural
extension by integrating C-Mixup with robust training.
4 PROBLEM DEFINITION
We formulate our problem as solving the bilevel optimization prob-
lem of cleaning and augmenting samples during model training:
min
ğœƒâˆ‘ï¸
ğ‘ ğ‘–âˆˆCâˆ’Mixup(ğ‘†ğ‘,ğ‘,ğ›¼)â„“ğœƒ(ğ‘ ğ‘–) (2)
ğ‘†ğ‘=arg min
ğ‘†={ğ‘ ğ‘–|ğ‘ğ‘–=1}ğ‘›âˆ‘ï¸
ğ‘–=1â„“ğœƒ(ğ‘ ğ‘–)ğ‘ğ‘–
s.t.ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–=ğœğ‘›
ğ‘ğ‘–âˆˆ{0,1},ğ‘–=1,...,ğ‘›(3)
whereğ‘ğ‘–indicates whether the data sample ğ‘ ğ‘–is selected or not, ğœ
is the ratio of clean samples in the training data, ğ‘†is the selected
samples, Câˆ’Mixup(ğ‘†)is an augmented result of ğ‘†by C-Mixup, ğ‘
is the bandwidth, and ğ›¼is the Mixup parameter.
 
1157KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Seong-Hyeon Hwang et al.
0 1000 2000
Round51015Test RMSEC-Mixup + ITLM
C-Mixup
(a)
0 1000 2000
Round60708090100Noise detection Acc. (%)C-Mixup + ITLM
ITLM (b)
Figure 3: Robust training benefits C-Mixup and vice versa.
(a) As robust training iteratively cleans the data, C-Mixupâ€™s
model performance improves. (b) Using C-Mixup within ro-
bust training helps it remove noisy data better compared to
when not using C-Mixup.
5 RC-MIXUP
We explain how we interleave data augmentation (C-Mixup) with
robust training, empirically analyze how the two techniques have
a synergistic effect, propose bandwidth tuning techniques, and
present the entire RC-Mixup algorithm.
5.1 C-Mixup and Robust Training Integration
We solve the bilevel optimization problem by interleaving C-Mixup
with the robust training rounds. This approach is common in other
bilevel optimization works [ 31,40]. Recall that robust training
iteratively cleans data and then updates its regression model until
the data is clean enough. For each round, we can perform C-Mixup
between the data cleaning and model updating steps as in Figure 1.
We explain how this sequence is beneficial to both methods.
Data Cleaning benefits C-Mixup. We provide a simple empirical
analysis of how data cleaning during robust training benefits C-
Mixup in Figure 3a. We experiment on the Spectrum dataset used
above and add random Gaussian noise to the labels, independently
per label. We start with a 10% noise ratio and run ITLM, which
progressively cleans the data. For each round, we evaluate C-Mixup
by training a model on the mixed data. As a result, when combining
C-Mixup with robust training, the modelâ€™s RMSE on the validation
set clearly improves with each round, unlike when using C-Mixup
only. In addition, while using C-Mixup only has a slight perfor-
mance decrease after convergence due to overfitting to the noisy
data, C-Mixup with robust training does not have this problem.
C-Mixup benefits Data Cleaning Performance. We also empiri-
cally analyze how C-Mixup improves robust training by making
it identify clean data better in Figure 3b. We use the same experi-
mental setup as above and evaluate the noise detection accuracy
of ITLM, which is the percent of noisy samples that are correctly
identified by robust training. We compare ITLM with when it is
combined with C-Mixup. As a result, C-Mixup improves ITLMâ€™s
noise detection accuracy by up to 5%.
5.2 Dynamic Bandwidth Tuning
As we analyzed in Figure 2b, the optimal bandwidth of C-Mixup
may vary as the data is progressively cleaned via robust training, so
we would like to dynamically adjust this value both accurately andefficiently. Recall that the bandwidth adjusts the level of mixing
where a larger bandwidth means that samples are mixed more with
neighbors. Following the original C-Mixup setup [ 50], we consider
a fixed set of bandwidth candidates ğµ={ğ‘1,ğ‘2,...,ğ‘|ğµ|}and aim
to select the best one.
Our strategy is to update the best bandwidth after every ğ¿>0
rounds of robust training. In the initial warm-up phase, we opt for
a higher bandwidth to accommodate the relatively greater noise
ratio. Subsequently, for each update, we evaluate bandwidth candi-
dates within ğµon the next ğ‘>0rounds and then choose the one
that results in the best model performance. Analytically predicting
model performance after multiple rounds is challenging because
the cleaned data itself keeps on changing. While this strategy has an
overhead, we later show in Section 6.3 that only one or two initial
updates are needed to achieve most of the performance benefits.
One way to further reduce the overhead is to avoid the bandwidth
searching altogether and instead simply decay the bandwidth by a
certain ratio (say by 10%) every ğ¿epochs based on the observation
that the optimal bandwidth tends to decrease for lower noise ratios.
Naturally there is be a tradeoff between runtime and performance,
which we show in Section 6.1. In addition, we now need to figure
out the right decay rate. Nonetheless, if lowering runtime is critical,
the decaying strategy can be a viable option.
5.3 Overall Algorithm
Algorithm 1 shows the overall RC-Mixup algorithm when using
ITLM as the robust training method. We first initialize model pa-
rameters using C-Mixup (Steps 2â€“4). The initial bandwidth is tuned
on the validation set. This bootstrapping is important because the
later steps are designed to gradually update its value as the data is
cleaned progressively, so we need to start from a bandwidth value
that is optimal on the noisy data first. Next, for each round, robust
training cleans the data, we run C-Mixup and update the model
on the cleaned data (Steps 18â€“20). After every ğ¿rounds, we also
update the bandwidth of C-Mixup by evaluating the possible band-
widths onğ‘rounds and choosing the one that results in the lowest
validation set RMSE (Steps 8â€“16). We repeat the entire process until
the model converges. Algorithm 2 shows the clean sample selection
algorithm in ITLM, and Algorithm 3 shows the C-Mixup algorithm.
Overhead on Robust Training. In comparison to robust training,
RC-Mixup adds the overhead of mixing samples via C-Mixup during
each round. For every ğ¿rounds, there is also the overhead of tuning
the bandwidth using ğ‘rounds of training. In our experiments, we
observe that the tuning overhead is small because the bandwidth
only needs to be updated once or twice.
6 EXPERIMENTS
We provide experimental results for RC-Mixup. We evaluate the
regression models trained on the augmented training sets on sepa-
rate test sets. We use Root Mean Squared Error (RMSE) and Mean
Absolute Percentage Error (MAPE) for measuring model perfor-
mance where lower values are better. We report the mean and the
standard deviation ( Â±in tables) for results of five random seeds.
We use PyTorch [ 35], and all experiments are performed using Intel
Xeon Silver 4210R CPUs and NVIDIA Quadro RTX 8000 GPUs.
 
1158A Data Augmentation Strategy against Noisy Data for Regression KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1: The RC-Mixup algorithm.
Input: Training dataD, validation setDğ‘£, bandwidth
update interval ğ¿, number of bandwidth update
roundsğ‘, possible bandwidths ğµ={ğ‘1,...,ğ‘|ğµ|},
clean ratioğœ, Mixup parameter ğ›¼, initial bandwidth
ğ‘, initial model parameters ğœƒ
// Warm-up phase
1forround =1 towarm-up rounds do
2ğ‘†ğ‘šğ‘–ğ‘¥=C-Mixup(D,ğ‘,ğ›¼)
3 Update model parameters ğœƒonğ‘†ğ‘šğ‘–ğ‘¥
4ğ‘–â†0
// Clean & Update phase
5while not converge do
6 ifğ‘–%ğ¿== 0 then
// Update bandwidth
7 forğ‘ğ‘—inğµdo
8 ğœƒğ‘ğ‘—â†ğœƒ
9 forround =1 toğ‘do
10 ğ¶=CleanSelection(D,ğœƒğ‘ğ‘—,ğœ)
11 ğ‘†ğ‘šğ‘–ğ‘¥=C-Mixup(ğ¶,ğ‘ğ‘—,ğ›¼)
12 Update model parameters ğœƒğ‘ğ‘—onğ‘†ğ‘šğ‘–ğ‘¥
13(ğ‘âˆ—,ğœƒğ‘âˆ—)= Bandwidth and corresponding model
parameters with the lowest RMSE
14ğœƒâ†ğœƒğ‘âˆ—
15ğ‘â†ğ‘âˆ—
16 else
17ğ¶=CleanSelection(D,ğœƒ,ğœ)
18ğ‘†ğ‘šğ‘–ğ‘¥=C-Mixup(ğ¶,ğ‘,ğ›¼)
19 Update model parameters ğœƒonğ‘†ğ‘šğ‘–ğ‘¥
20ğ‘–=ğ‘–+1
Output:ğœƒ
Algorithm 2: The clean sample selection algorithm.
Input: Datasetğ·, model parameters ğœƒ, clean ratio ğœ
1sortIdx = argSort(â„“ğœƒ(ğ·))(ascending order)
2ğ¶â†firstğœ|ğ·|elements of ğ·[sortIdx]
Output:ğ¶
Metrics. RMSE is defined asâˆšï¸ƒ
1
ğ‘›Ãğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’Ë†ğ‘¦ğ‘–)2. MAPE is defined
as1
ğ‘›Ãğ‘›
ğ‘–=1ğ‘¦ğ‘–âˆ’Ë†ğ‘¦ğ‘–
ğ‘¦ğ‘–Ã—100and is a measure of prediction performance
of forecasting methods.
Experimental environment. We conducted all experiments using
Intel(R) Xeon(R) Silver 4210R CPUs @ 2.40GHz, and eight NVIDIA
Quadro RTX 8000 GPUs equipped with 48GB VRAM on Linux
Ubuntu 18.04.5. We employed Python 3.8.13, Scikit-learn version
0.24.2, and PyTorch version 1.7.1 for our evaluations.
Datasets. We use one synthetic and three real datasets. The Spec-
trum dataset [ 10] is synthetic and contains spectrum data generatedAlgorithm 3: The C-Mixup algorithm.
Input: Datasetğ·, bandwidth ğ‘, Mixup parameter ğ›¼
1ğ‘†â†[]
2for(ğ‘¥ğ‘–,ğ‘¦ğ‘–)inğ·do
3 Sample(ğ‘¥ğ‘—,ğ‘¦ğ‘—)using Equation 1 and ğ‘
4 Sampleğœ†âˆ¼ğµğ‘’ğ‘¡ğ‘(ğ›¼,ğ›¼)
5 Ëœğ‘¥=ğœ†ğ‘¥ğ‘–+(1âˆ’ğœ†)ğ‘¥ğ‘—
6 Ëœğ‘¦=ğœ†ğ‘¦ğ‘–+(1âˆ’ğœ†)ğ‘¦ğ‘—
7ğ‘†â†ğ‘†âˆª{( Ëœğ‘¥,Ëœğ‘¦)}
Output:ğ‘†
Table 1: Settings for the four datasets.
Dataset Data dim. Label dim. |D| |Dğ‘£|
Spectrum 226 4 2,000 500
NO2 7 1 200 200
Airfoil 5 1 1,000 400
Exchange-Rate 168Ã—8 8 4,373 1,518
by applying light waves on 4-layer 3D semiconductors and measur-
ing the returning wavelengths. This data is used to predict layer
thickness without touching the semiconductor itself. The NO2 emis-
sions dataset [ 1] contains traffic and meteorological information
around roads and is used to predict NO2 concentration. The Airfoil
dataset [ 11] contains aerodynamic and acoustic test results for air-
foil blade sections in a wind tunnel. Finally, the Exchange-Rate [29]
is a time-series dataset and contains daily exchange rates from 8
countries. The three real datasets are from [ 50] for a fair comparison.
Table 1 shows the dimension and size information of the datasets.
Noise Injection. We inject noise to labels using two methods: (1)
Gaussian noise, which adds to each label a value sampled from the
Gaussian distribution ğ‘(0,ğ‘š2ğœ2), whereğ‘šis the noise magnitude
(see default values in the appendix), and ğœis the standard deviation
of labels of training set and (2) labeling flipping noise, which sub-
tracts a maximum label value by each label as in classification [ 37].
We use a noise rate of 10â€“40% where the default is 30%. We do not
set the noise ratio to be larger than 50% to prevent the noise from
dominating the data.
Baselines. We consider two types of baselines:
â€¢Individual methods: performing ITLM [ 43] only (â€œRob. train-
ingâ€) and performing C-Mixup [50] only (â€œC-Mixupâ€).
â€¢Simple combinations: performing robust training first and
C-Mixup in sequence (â€œR â†’Câ€), performing C-Mixup and
then robust training in sequence (â€œC â†’Râ€), and performing
RC-Mixup without dynamic bandwidth tuning (â€œC â†’R+Câ€).
Parameters. For robust training, we assume that the clean ratio
ğœis known for each dataset. If ğœis unknown, it can be inferred
with cross validation [ 32,51]. For the C-Mixup, C â†’R, and Câ†’R+C
baselines, we use a single bandwidth value and tune it using a
grid search. We choose other parameters using a grid search on
the validation set or following C-Mixupâ€™s setup. More details on
parameters are in the appendix.
 
1159KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Seong-Hyeon Hwang et al.
0 1000 2000
Round05101520Test RMSERC-Mixup
C-Mixup
ITLM
(a)Spectrum
0 100 200
Round0.40.60.8Test RMSERC-Mixup
C-Mixup
ITLM (b)NO2
0 200 400
Round1357Test RMSERC-Mixup
C-Mixup
ITLM (c)Airfoil
0 100 200
Round0.000.020.04Test RMSERC-Mixup
C-Mixup
ITLM (d)Exchange-Rate
0 1000 2000
Round0102030Bandwidth RC-Mixup
C-Mixup
(e)Spectrum
0 100 200
Round10âˆ’210âˆ’1100101Bandwidth RC-Mixup
C-Mixup (f)NO2
0 200 400
Round10âˆ’310âˆ’210âˆ’1100101102Bandwidth RC-Mixup
C-Mixup (g)Airfoil
0 100 200
Round10âˆ’210âˆ’1100101Bandwidth RC-Mixup
C-Mixup (h)Exchange-Rate
0 250 500
Val. set size7.07.58.08.5Test RMSE
(i)Spectrum
0 100 200
Val. set size0.500.550.600.65Test RMSE
 (j)NO2
0 200 400
Val. set size1.52.53.5Test RMSE
 (k)Airfoil
0 760 1518
Val. set size0.0100.0150.020Test RMSE
 (l)Exchange-Rate
Figure 4: (a)-(d) RC-Mixup model training convergence results. (e)-(h) RC-Mixup dynamic bandwidth tuning results. The tuned
bandwidth values vary slightly depending on the random seed, and we show the bandwidth averaged across five random seeds
(dotted red lines). (i)-(l) RC-Mixup performance results while varying the validation set size.
6.1 Model Performance and Runtime Results
We compare the overall performance RC-Mixup with the two types
of baselines on all the datasets as shown in Table 2. For the individ-
ual method baselines, we observe that C-Mixup is indeed vulnerable
to noise, and that robust training is less affected by the noise, but
still needs improvement. The simple combination baselines C â†’R
and Câ†’R+C increasingly outperform the individual methods as
they start to take advantage of both methods. However, RC-Mixup
performs the best by also dynamically tuning the bandwidth. We
also note that our results are near-optimal. As a reference, the
(RMSE, MAPE) results in an optimal setting where we only use
clean data are: (6.961, 6.000) for Spectrum and (0.535, 13.209) for
NO2. The RC-Mixup results are similar to these results and cannot
be further improved.
We also show how RC-Mixupâ€™s model training converges and
how its bandwidth is tuned in Figures 4aâ€“4d. For all four datasets,
RC-Mixup clearly converges to lower RMSE values for the same
number of rounds compared to C-Mixup and robust training. RC-
Mixupâ€™s RMSE values drastically decrease around the middle of each
figure because robust training and bandwidth tuning are applied
at that point. Once robust training is applied, the model training
is now done on cleaned data, so the model performance improves
significantly afterwards. We then show how RC-Mixup dynamically
adjusts its bandwidth for the four datasets with five random seeds
in Figures 4eâ€“4h. For these datasets, the bandwidth values varyslightly depending on the random seed, and we show the bandwidth
averaged across five random seeds (dotted red lines). The decreasing
trends of the tuned bandwidths are consistent with Figure 2b, where
as the data is cleaned, a smaller bandwidth is more effective. We do
not claim this trend always holds, and the point is that RC-Mixup
is able to find the right bandwidth in any situation.
Runtime. The computational cost of RC-Mixup varies depend-
ing on the number of bandwidth candidates or bandwidth update
frequency. For the Spectrum dataset, the runtimes of all methods
are as follows: robust training (ITLM) only: 52s, C-Mixup only:
244s, Câˆ’ â†’R: 161s, Câˆ’ â†’C+R: 250s, and RC-Mixup: 578s. Although
RC-Mixup requires roughly twice the runtime of C-Mixup, the
substantial performance improvement of RC-Mixup justifies its
overhead. In general, while the efficiency decreases as the search
space grows, we can obtain performance improvements with only
4â€“7 bandwidth candidates in practice. As a reference, C-Mixup
also searches for the optimal bandwidth among 6â€“10 bandwidth
candidates using a grid search.
Bandwidth Decaying Results. We also show RC-Mixupâ€™s perfor-
mance and runtime when using bandwidth decaying in Table 3.
Here we decrease the bandwidth by 10% every ğ¿epochs. As a result,
there is a tradeoff between runtime and performance where the
decaying strategy is 1.4-2.6x faster than the original RC-Mixup, but
 
1160A Data Augmentation Strategy against Noisy Data for Regression KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: RC-Mixup performance compared to the five base-
lines on the real and synthetic datasets.
Dataset Method RMSE MAPE
SpectrumC-Mixup 12.125Â±0.200 10.840Â±0.246
Rob. training 9.756Â±0.541 7.228Â±0.221
Râ†’C 9.055Â±0.445 6.806Â±0.335
Câ†’R 8.024Â±0.321 6.468Â±0.211
Câ†’R+C 8.755Â±0.187 7.031Â±0.067
RC-Mixup 7.471Â±0.220 5.930Â±0.165
NO2C-Mixup 0.586Â±0.033 14.604Â±1.283
Rob. training 0.574Â±0.028 14.418Â±1.551
Râ†’C 0.573Â±0.023 14.340Â±1.258
Câ†’R 0.562Â±0.038 14.020Â±1.562
Câ†’R+C 0.566Â±0.033 14.203Â±1.549
RC-Mixup 0.557Â±0.034 13.816Â±1.440
AirfoilC-Mixup 3.438Â±0.218 2.093Â±0.185
Rob. training 2.760Â±0.329 1.529Â±0.069
Râ†’C 3.226Â±0.260 1.833Â±0.136
Câ†’R 2.721Â±0.463 1.492Â±0.235
Câ†’R+C 2.699Â±0.381 1.501Â±0.167
RC-Mixup 2.530Â±0.357 1.398Â±0.123
Exchange-
RateC-Mixup 0.0216Â±0.0018 2.2931Â±0.2153
Rob. training 0.0180Â±0.0008 1.7715Â±0.1213
Râ†’C 0.0165Â±0.0015 1.5825Â±0.1821
Câ†’R 0.0162Â±0.0011 1.5553Â±0.1428
Câ†’R+C 0.0162Â±0.0011 1.5495Â±0.1214
RC-Mixup 0.0156Â±0.0007 1.4692Â±0.0811
Table 3: RC-Mixup with bandwidth decaying (â€œWith BDâ€)
compared to the RC-Mixup on the real and synthetic datasets.
Dataset Method RMSE MAPE Runtime(s)
SpectrumRC-Mixup 7.471Â±0.220 5.930Â±0.165 578
With BD 7.903Â±0.144 6.284Â±0.126 247
NO2RC-Mixup 0.557Â±0.034 13.816Â±1.440 14
With BD 0.556Â±0.029 13.974Â±1.318 7
AirfoilRC-Mixup 2.530Â±0.357 1.398Â±0.123 188
With BD 2.582Â±0.244 1.446Â±0.111 73
Exchange-
RateRC-Mixup 0.0156Â±0.0007 1.4692Â±0.0811 512
With BD 0.0158Â±0.0010 1.5093Â±0.1116 371
has slightly worse RMSE and MAPE results that are still better than
those of the baselines.
6.2 Varying Noise
We evaluate how robust RC-Mixup is against different types and
levels of noise on the Spectrum dataset in Table 4. We sample
different levels of Gaussian noise by varying the noise magnitudeTable 4: RC-Mixup robustness against different types and
levels of noise on the Spectrum dataset.
Noise Type Magnitude ( ğ‘švalue) Method RMSE MAPE
GaussianLow (1)C-Mixup 10.044 9.031
RC-Mixup 7.442 6.029
Medium (2)C-Mixup 12.125 10.840
RC-Mixup 7.471 5.930
High (3)C-Mixup 13.074 11.660
RC-Mixup 7.941 6.240
Labeln/aC-Mixup 18.259 17.520
Flipping RC-Mixup 7.893 6.124
Table 5: RC-Mixup robustness against different Gaussian
noise rates on the Spectrum dataset.
Noise Rate Method RMSE MAPE
10%C-Mixup 9.291Â±0.129 8.210Â±0.066
RC-Mixup 6.100Â±0.118 5.090Â±0.119
20%C-Mixup 10.721Â±0.202 9.583Â±0.232
RC-Mixup 6.729Â±0.155 5.493Â±0.095
40%C-Mixup 13.376Â±0.369 12.257Â±0.325
RC-Mixup 8.524Â±0.342 6.784Â±0.293
ğ‘šand also use label flipping. As a result, RC-Mixup consistently
performs better than C-Mixup as it is less affected by noise with its
bandwidth tuning. We also evaluate RC-Mixupâ€™s robustness while
varying the noise rate on the Spectrum dataset from 10% to 40% in
Table 5 and observe results that are consistent with when using the
default noise rate of 30%.
6.3 Parameter Analysis
We vary the bandwidth tuning parameters ğ¿andğ‘to see how
RC-Mixup performs in different scenarios on the Spectrum dataset.
The results on the other datasets are similar. As we decrease ğ¿,
we update the bandwidth more frequently, which means that it is
more likely to be up-to-date. Figure 5a indeed shows that the RMSE
decreases against the number of updates, but only for the first one or
two updates. Increasing ğ‘means that we evaluate the bandwidths
using more rounds before selecting one of them. Figure 5b shows
that a higher ğ‘leads to lower RMSE, but with diminishing returns.
Hence, RC-Mixup achieves sufficient performance improvements
even when both ğ¿andğ‘are small, which means that the additional
overhead for bandwidth tuning of RC-Mixup is not large.
6.4 Validation Set Construction and Size
We evaluate RC-Mixup by varying the validation set size on all
the four datasets in Figures 4iâ€“4l. As a result, RC-Mixup works
reasonably well even for smaller validation set sizes. If there is no
clean validation set readily available, we can utilize the given robust
training method to construct a validation set ourselves. In Table 6,
we compare RC-Mixupâ€™s RMSE on two datasets when using (1) a
 
1161KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Seong-Hyeon Hwang et al.
0 1 2 3 4
Update frequency678910Test RMSE
(a) Varying ğ¿
0 500 1000
N7.07.68.2Test RMSE
 (b) Varying ğ‘
Figure 5: RC-Mixup performance when varying ğ¿andğ‘used
for bandwidth tuning on the Spectrum dataset.
Table 6: RC-Mixup performance when using a clean valida-
tion set, a noisy validation set that is cleaned with robust
training (RT), and a noisy validation set.
Dataset Validation set RMSE MAPE
SpectrumClean 7.471Â±0.220 5.930Â±0.165
Cleaned with RT 7.739Â±0.262 6.209Â±0.225
Noisy 8.176Â±0.544 6.625Â±0.659
AirfoilClean 2.530Â±0.357 1.398Â±0.123
Cleaned with RT 2.684Â±0.453 1.471Â±0.136
Noisy 2.692Â±0.244 1.547Â±0.167
clean validation set; (2) a noisy validation set that is cleaned with
robust training first; and (3) a noisy validation set that is a subset
of the training set. We compare with (3) just as a reference. As a
result, RC-Mixup using (2) has slightly lower, but a comparable
performance as when using (1), which means that RC-Mixup can
still perform well without a given clean validation set with an
additional overhead of cleaning the noisy validation set.
6.5 Other Robust Training Methods
To show the generality of RC-Mixup, we integrate C-Mixup with the
two other robust training methods O2U-Net [ 18] and SELFIE [ 44]
and provide more evaluation results. RC-Mixup can be plugged into
any other multi-round robust training method as well.
O2U-Net Integration. O2U-Net consists of two phases for select-
ing clean samples: a pre-training phase and a cyclical training phase
between underfitting and overfitting. During the pre-training phase,
we train the model using C-Mixup. In the cyclical training phase,
we further train this pre-trained model by adjusting the learning
rate cyclically to obtain the clean samples. We then return the pre-
trained C-Mixup model trained on the final clean samples with
bandwidth tuning.
SELFIE Integration. SELFIE refurbishes the labels of unclean sam-
ples with low predictive uncertainty by assigning them the most
frequent class among the previous ğ‘predictions. SELFIE was de-
signed for classification, so we extend it to work for regression.
While SELFIE calculates uncertainty using the entropy of the
predictive categorical distribution, this approach cannot be directly
applied in a regression setting due to the absence of the categorical
distribution. Instead of using entropy, we quantify uncertainty byTable 7: RC-Mixup performance using the robust training
methods O2U-Net [ 18] and SELFIE [ 44] on the four datasets.
Dataset Method RMSE MAPE
SpectrumC-Mixup only 12.125Â±0.200 10.840Â±0.246
O2U-Net only 9.386Â±0.360 7.867Â±0.321
RC-Mixup w/ O2U-Net 8.372Â±0.316 6.702Â±0.144
SELFIE only 9.040Â±0.225 7.000Â±0.178
RC-Mixup w/ SELFIE 7.701Â±0.138 6.234Â±0.186
NO2C-Mixup only 0.586Â±0.033 14.604Â±1.283
O2U-Net only 0.564Â±0.033 14.052Â±1.617
RC-Mixup w/ O2U-Net 0.554Â±0.022 13.782Â±1.285
SELFIE only 0.559Â±0.043 14.002Â±1.580
RC-Mixup w/ SELFIE 0.550Â±0.030 13.562Â±1.247
AirfoilC-Mixup only 3.438Â±0.218 2.093Â±0.185
O2U-Net only 3.026Â±0.199 1.783Â±0.073
RC-Mixup w/ O2U-Net 2.795Â±0.379 1.574Â±0.145
SELFIE only 2.713Â±0.426 1.508Â±0.174
RC-Mixup w/ SELFIE 2.569Â±0.358 1.408Â±0.165
Exchange-
RateC-Mixup only 0.0216Â±0.0018 2.2931Â±0.2153
O2U-Net only 0.0203Â±0.0015 2.0909Â±0.1676
RC-Mixup w/ O2U-Net 0.0162Â±0.0011 1.5523Â±0.1407
SELFIE only 0.0165Â±0.0004 1.5741Â±0.0666
RC-Mixup w/ SELFIE 0.0154Â±0.0009 1.4535Â±0.1137
assessing the variance of predictive labels over the past ğ‘predic-
tions. This approach is consistent with other common regression
techniques [ 12,30], where the uncertainty corresponds to the vari-
ation in predictions across multiple instances or models.
When refurbishing a label of an unclean sample, we average the
previous predictions instead of finding the most frequent label. The
reason is that in regression, labels have continuous values, so it
makes less sense to find the most frequent labels as in classification.
Finally, since SELFIE refurbishes the labels for every mini-batch,
the label distances between two labels change, which means the
sampling probabilities for C-Mixup may change as well. However,
updating the distances between label pairs requires significant
amounts of computation, and we thus choose not to update the sam-
pling probabilities once they are initially computed. This approach
is reasonable because only a fraction of labels that have low uncer-
tainties are actually refurbished. Even if we do update the sampling
probabilities, our experiments show that they have a minor impact
on the modelâ€™s performance. Nonetheless, incrementally updating
the sampling probabilities efficiently is an interesting future work.
Evaluation. We now evaluate RC-Mixup using O2U-Net and
SELFIE on the four datasets in Table 7. As a result, RC-Mixup
improves both C-Mixup and robust training as in Table 7 demon-
strating the synergy between the two for all four datasets.
 
1162A Data Augmentation Strategy against Noisy Data for Regression KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
7 RELATED WORK
There are largely two branches of work for data augmentation in
regression. One is semi-supervised regression [ 23,26,28,54] where
the goal is to utilize unlabeled data for training. Another branch is
data augmentation when there is no unlabeled data, which is our
research focus. Most data augmentation techniques are tailored to
classification and more recently a few are designed for regression.
Data Augmentation for Classification. There are largely three
approaches: generative models, policies, and Mixup techniques.
Generative models including GANs [ 14] and VAEs [ 27] are popular
in classification where the idea is to generate realistic data that
cannot be distinguished from the real data by a discriminator. An-
other approach is to use policies [ 9], which specify fixing rules for
transforming the data. However, a major assumption is that the
labels of these generated samples are the same, which does not
necessarily hold in a regression setting where most samples may
have different labels. Mixup [ 24,25,47,52,53] takes the alternative
approach of generating both data and labels together by mixing
existing samples with different labels assuming linearity between
training samples [5, 48].
Data Augmentation for Regression. There is a new and increas-
ing literature on data augmentation for regression using Mixup
techniques. Although the original Mixup paper mentions that its
techniques can easily be extended to regression, the linearity of a
regression model is limited where mixing samples with all others
may not be beneficial and even detrimental to model performance.
Hence, the key is to figure out how to limit the mixing. RegMix [ 19]
learns for each sample how many nearest neighbors in terms of data
distance it should be mixed with for the best model performance
using a validation set. The state-of-the-art C-Mixup [ 50] uses a
Gaussian kernel to generate a sampling probability distribution
for each sample based on label distances using a global bandwidth
and selects a sample to mix based on the distribution. Anchor Data
Augmentation [ 42] takes a more domain-specific approach where
it clusters data points and modifies original points either towards
or away from the cluster centroids for the augmentation. Finally, R-
Mixup [ 22] specializes in improving model performance on biologi-
cal networks. In comparison, RC-Mixup uses the domain-agnostic
C-Mixup and makes it robust against noise.
Robust Training. The goal of robust training is to train accurate
models against noisy or adversarial data, and we cover the represen-
tative works. While data can be problematic in various places [ 49],
most techniques assume that the labels are noisy [ 45] and mitigate
with the following strategies: (1) developing model architectures
that are less affected by the noise [ 3,8,15,21], (2) applying loss
regularization techniques to reduce overfitting [ 13,17,34,39,46],
(3) correcting the loss function to account for the noise [ 2,4,33,36],
and (4) proposing sample selection techniques [ 18,43,44] for se-
lecting clean samples from noisy data. In comparison, RC-Mixup
is mainly designed to work with the sample selection techniques,
but can also complement other approaches as long as it can access
intermediate clean data within rounds.8 CONCLUSION
We proposed RC-Mixup, an effective and practical data augmenta-
tion strategy against noisy data for regression. The state-of-the-art
data augmentation technique C-Mixup is not designed to handle
noise, and RC-Mixup is the first to tightly integrates it with robust
training for a synergistic effect. C-Mixup benefits from the inter-
mediate clean data information identified by robust training and
performs different mixing depending on whether noisy data is be-
ing used, while robust training cleans its data better with C-Mixupâ€™s
data augmentation. We also proposed dynamic tuning techniques
for C-Mixupâ€™s bandwidth. Our extensive experiments showed how
RC-Mixup significantly outperforms C-Mixup and robust training
baselines on noisy data benchmarks and is compatible with various
robust training methods.
ACKNOWLEDGEMENT
This work was supported by Samsung Electronics Co., Ltd., by the
Institute of Information & Communications Technology Planning
& Evaluation(IITP) grant funded by the Korea government(MSIT)
(No. 2022-0-00157, Robust, Fair, Extensible Data-Centric Contin-
ual Learning), and by the National Research Foundation of Ko-
rea(NRF) grant funded by the Korea government(MSIT) (No. NRF-
2022R1A2C2004382)
REFERENCES
[1]Magne Aldrin. 2004. CMU StatLib Dataset. http://lib.stat.cmu.edu/datasets/.
Accessed: 2022-08-15.
[2]Eric Arazo, Diego Ortego, Paul Albert, Noel Oâ€™Connor, and Kevin McGuinness.
2019. Unsupervised label noise modeling and loss correction. In ICML. 312â€“321.
[3]Alan Joseph Bekker and Jacob Goldberger. 2016. Training deep neural-networks
based on unreliable labels. In IEEE ICASSP. 2682â€“2686.
[4]Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. 2017. Active
Bias: Training More Accurate Neural Networks by Emphasizing High Variance
Samples. In NeurIPS.
[5]Olivier Chapelle, Jason Weston, LÃ©on Bottou, and Vladimir Vapnik. 2000. Vicinal
Risk Minimization. In NIPS. 416â€“422.
[6]Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. 2019. Un-
derstanding and Utilizing Deep Neural Networks Trained with Noisy Labels. In
ICML.
[7]Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. 2019. Under-
standing and utilizing deep neural networks trained with noisy labels. In ICML.
1062â€“1070.
[8]Xinlei Chen and Abhinav Gupta. 2015. Webly supervised learning of convolu-
tional networks. In ICCV. 1431â€“1439.
[9]Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V.
Le. 2019. AutoAugment: Learning Augmentation Strategies From Data. In CVPR.
113â€“123.
[10] DACON Co., Ltd. 2020. Challenge on Semiconductor Thin Film Thickness Anal-
ysis. https://dacon.io/competitions/official/235554/data. Accessed: 2023-07-12.
[11] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http:
//archive.ics.uci.edu/ml
[12] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. In ICML. PMLR, 1050â€“1059.
[13] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In ICLR.
[14] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets. In NIPS. 2672â€“2680.
[15] Bo Han, Jiangchao Yao, Niu Gang, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and
Masashi Sugiyama. 2018. Masking: A new perspective of noisy supervision. In
NeurIPS. 5839â€“5849.
[16] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang,
and Masashi Sugiyama. 2018. Co-teaching: Robust training of deep neural net-
works with extremely noisy labels. In NeurIPS. 8535â€“8545.
[17] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. 2019. Using pre-training can
improve model robustness and uncertainty. In ICML. 2712â€“2721.
[18] Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao. 2019. O2U-Net: A Simple
Noisy Label Detection Approach for Deep Neural Networks. In ICCV. 3325â€“3333.
 
1163KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Seong-Hyeon Hwang et al.
[19] Seong-Hyeon Hwang and Steven Euijong Whang. 2022. RegMix: Data Mixing
Augmentation for Regression. arXiv:2106.03374 [cs.LG]
[20] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. 2018. Mentor-
net: Learning data-driven curriculum for very deep neural networks on corrupted
labels. In ICML. 2304â€“2313.
[21] Ishan Jindal, Matthew Nokleby, and Xuewen Chen. 2016. Learning deep networks
from noisy labels with dropout regularization. In IEEE ICDM. 967â€“972.
[22] Xuan Kan, Zimu Li, Hejie Cui, Yue Yu, Ran Xu, Shaojun Yu, Zilong Zhang, Ying
Guo, and Carl Yang. 2023. R-Mixup: Riemannian Mixup for Biological Networks.
InKDD.
[23] Pilsung Kang, Dongil Kim, and Sungzoon Cho. 2016. Semi-supervised support
vector regression based on self-training with label uncertainty: An application to
virtual metrology in semiconductor manufacturing. Expert Syst. Appl. 51 (2016),
85â€“106.
[24] JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. 2021. Co-Mixup:
Saliency Guided Joint Mixup with Supermodular Diversity. In ICLR.
[25] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. 2020. Puzzle mix: Exploiting
saliency and local statistics for optimal mixup. In ICML. PMLR, 5275â€“5285.
[26] Sung Wook Kim, Young Gon Lee, Bayu Adhi Tama, and Seungchul Lee. 2020.
Reliability-Enhanced Camera Lens Module Classification Using Semi-Supervised
Regression Method. Applied Sciences 10, 11 (2020).
[27] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In
ICLR.
[28] Georgios Kostopoulos, Stamatis Karlos, Sotiris Kotsiantis, and Omiros Ragos.
2018. Semi-supervised regression: A recent review. J. Intell. Fuzzy Syst. 35, 2
(2018), 1483â€“1500.
[29] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling
long-and short-term temporal patterns with deep neural networks. In SIGIR.
95â€“104.
[30] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple
and scalable predictive uncertainty estimation using deep ensembles. NeurIPS 30
(2017).
[31] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable
Architecture Search. ICLR (2019).
[32] Tongliang Liu and Dacheng Tao. 2015. Classification with noisy labels by impor-
tance reweighting. IEEE TPAMI 38, 3 (2015), 447â€“461.
[33] Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia,
Sudanthi Wijewickrema, and James Bailey. 2018. Dimensionality-driven learning
with noisy labels. In ICML. 3355â€“3364.
[34] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar.
2020. Can gradient clipping mitigate label noise?. In ICLR.
[35] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic Differentiation in PyTorch. In NIPS Autodiff Workshop.
[36] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and
Lizhen Qu. 2017. Making deep neural networks robust to label noise: A losscorrection approach. In CVPR. 1944â€“1952.
[37] Andrea Paudice, Luis MuÃ±oz-GonzÃ¡lez, and Emil C. Lupu. 2018. Label Sanitization
Against Label Flipping Poisoning Attacks. In ECML PKDD. 5â€“15.
[38] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. 2021. Deep
learning on a data diet: Finding important examples early in training. NeurIPS 34
(2021), 20596â€“20607.
[39] Gabriel Pereyra, George Tucker, Jan Chorowski, Åukasz Kaiser, and Geoffrey
Hinton. 2017. Regularizing neural networks by penalizing confident output
distributions. (2017).
[40] Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. 2021. Sample selec-
tion for fair and robust training. NeurIPS 34 (2021), 815â€“827.
[41] Peter J. Rousseeuw. 1984. Least Median of Squares Regression. J. Amer. Statist.
Assoc. 79, 388 (1984), 871â€“880.
[42] Nora Schneider, Shirin Goshtasbpour, and Fernando Perez-Cruz. 2023. Anchor
Data Augmentation. In NeurIPS.
[43] Yanyao Shen and Sujay Sanghavi. 2019. Learning with Bad Training Data via
Iterative Trimmed Loss Minimization. In ICML, Vol. 97. PMLR, 5739â€“5748.
[44] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. 2019. SELFIE: Refurbishing
Unclean Samples for Robust Deep Learning. In ICML, Vol. 97. PMLR, 5907â€“5915.
[45] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. 2022.
Learning From Noisy Labels With Deep Neural Networks: A Survey. IEEE
Transactions on Neural Networks and Learning Systems (2022), 1â€“19.
[46] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander,
and Nathan Silberman. 2019. Learning from noisy labels by regularized estimation
of annotator confusion. In CVPR. 11244â€“11253.
[47] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,
David Lopez-Paz, and Yoshua Bengio. 2019. Manifold mixup: Better representa-
tions by interpolating hidden states. In ICML.
[48] Sen Wu, Hongyang R. Zhang, Gregory Valiant, and Christopher RÃ©. 2020. On
the Generalization Effects of Linear Transformations in Data Augmentation. In
ICML. 10410â€“10420.
[49] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. 2015. Learning
from massive noisy labeled data for image classification. In CVPR. 2691â€“2699.
[50] Huaxiu Yao, Yiping Wang, Linjun Zhang, James Y. Zou, and Chelsea Finn. 2022.
C-Mixup: Improving Generalization in Regression. In NeurIPS.
[51] Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng
Tao. 2018. An efficient and provable approach for mixture proportion estimation
using linear independence assumption. In CVPR. 4480â€“4489.
[52] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon
Yoo, and Junsuk Choe. 2019. CutMix: Regularization Strategy to Train Strong
Classifiers With Localizable Features. In ICCV. 6022â€“6031.
[53] Hongyi Zhang, Moustapha CissÃ©, Yann N. Dauphin, and David Lopez-Paz. 2018.
mixup: Beyond Empirical Risk Minimization. In ICLR.
[54] Zhi-Hua Zhou and Ming Li. 2005. Semi-Supervised Regression with Co-Training..
InIJCAI. 908â€“913.
 
1164A Data Augmentation Strategy against Noisy Data for Regression KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A APPENDIX â€“ EXPERIMENTS
A.1 Other Experimental Settings
We explain more detailed experimental setups.
Hyperparameters. We summarize the hyperparameters for all the datasets in Table 8. For each dataset, we either use the traditional notion
of Mixup or the more recent Manifold Mixup [ 47] (ManiMix), whichever performs better. We use a 3-layer fully connected neural network
with one hidden layer with 128 nodes (FCN3) for the Spectrum, NO2, and Airfoil datasets. For the Exchange-Rate dataset, we build an
LST-Attn model [29] with a horizon value of 12.
Table 8: Detailed hyperparameters for each dataset used in the experiments.
Dataset Spectrum NO2 Airfoil Exchange-Rate
Mixup type Mixup Mixup ManiMix Mixup
Batch size 128 32 16 128
Learning rate 1e-2 1e-2 1e-2 1e-3
Maximum epochs 2,000 200 400 200
ğµ {5, 10, 15, 20} {1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1} {1e-3, 1e-2, 1e-1, 1, 10} {1e-3, 1e-2, 5e-2, 1e-1}
Default bandwidth 20 2 10 2
ğ¿ 500 100 200 50
ğ‘ 500 100 200 50
ğ›¼for Mixup 2.0 2.0 0.5 2.0
Noise magnitude ğ‘š 2 4 2 5
Model architecture FCN3 FCN3 FCN3 LST-Attn
Optimizer Adam Adam Adam Adam
A.2 Noisy Versus Hard-to-train Data
While noisy data tends to have high loss values, so does hard-to-train data. However, noisy data typically has higher losses as shown in Paul
et al. [ 38]. Here hard data is identified using the EL2N score, which is similar to loss. Paul et al. [ 38] analyze the challenge of distinguishing
hard data from noisy data and shows that data with noisy labels tends to have higher EL2N scores. If there is a label noise, data with large
loss values are commonly considered as noise [18, 43, 44].
 
1165