Shopping Trajectory Representation Learning with Pre-training
for E-commerce Customer Understanding and Recommendation
Yankai Chenâˆ—
The Chinese University of Hong Kong
Hong Kong SAR, China
ykchen@cse.cuhk.edu.hkQuoc-Tuan Truong
Amazon
Seattle, USA
truquoc@amazon.comXin Shen
Amazon
Seattle, USA
xinshen@amazon.com
Jin Li
Amazon
Seattle, USA
jincli@amazon.comIrwin King
The Chinese University of Hong Kong
Hong Kong SAR, China
king@cse.cuhk.edu.hk
ABSTRACT
Understanding customer behavior is crucial for improving service
quality in large-scale E-commerce. This paper proposes C-STAR, a
new framework that learns compact representations from customer
shopping journeys, with good versatility to fuel multiple down-
stream customer-centric tasks. We define the notion of shopping
trajectory that encompasses customer interactions at the level of
product categories, capturing the overall flow of their browsing
and purchase activities. C-STAR excels at modeling both inter-
trajectory distribution similarity â€“the structural similarities between
different trajectories, and intra-trajectory semantic correlation â€“the
semantic relationships within individual ones. This coarse-to-fine
approach ensures informative trajectory embeddings for represent-
ing customers. To enhance embedding quality, we introduce a pre-
training strategy that captures two intrinsic properties within the
pre-training data. Extensive evaluation on large-scale industrial and
public datasets demonstrates the effectiveness of C-STAR across
three diverse customer-centric tasks. These tasks empower cus-
tomer profiling and recommendation services for enhancing per-
sonalized shopping experiences on our E-commerce platform.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems ;â€¢Comput-
ing methodologies â†’Learning latent representations.
KEYWORDS
Representation Learning; Shopping Trajectory; Customer Under-
standing; Shopping Intention; E-commerce Recommendation
ACM Reference Format:
Yankai Chen, Quoc-Tuan Truong, Xin Shen, Jin Li, and Irwin King. 2024.
Shopping Trajectory Representation Learning with Pre-training for E-commerce
Customer Understanding and Recommendation. In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
âˆ—This work is done during internship at Amazon.
This work is licensed under a Creative Commons Attribution-
NonCommercial International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671747
Task 1: Recall@20Task 1: NDCG@20
Task 2: Recall@20Task 2: NDCG@20Task 3: HitRTask 3: AUC6789
678916171819101112134344454681828384
: C-STAR: Graph-based20: Language-based: DNN-based: Sequential RecSysShopping Trajectories
EuclideanDistanceTrajectory Representations
TrajectorySimilarity
Downstream Tasks(A)C-STAR(B)Figure 1: (A) C-STAR framework illustration; (B) perfor-
mance comparison (%) with selected competitive methods.
â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671747
1 INTRODUCTION
The ever-growing volume of products bombards online shoppers,
making it difficult to identify items of interest. Recommender sys-
tems address this challenge by providing personalized suggestions
throughout the customer shopping journey, from browsing to check-
out [ 22,62]. In pursuit of personalization for various scenarios, the
key prong of delivering high-quality services is rooted in reliable
and comprehensive customer understanding. This motivates us to
effectively unveil customersâ€™ insights via mining information from
their variety of historical shopping engagements.
One approach to achieving this is through customer representa-
tion learning, based on analyzing and encoding their engagement
contents. However, existing methods are often tailored to one spe-
cific task, such as customer next-item recommendation [ 5,24,27,39,
67,73,78,79]. In the context of web-scale E-commerce platforms,
our goal is to develop a versatile framework capable of addressing
multiple downstream tasks related to customer profiling and ser-
vices. Beyond the requisite capability for recommending items, this
framework is expected to be able to effectively segment and group
similar customers who are alike in their shopping behaviors. This
facilitates a broader understanding of their common interests and
market affinities [ 9,16,53]. Moreover, when contemplating per-
sonalization, it is essential for the framework to discern customer
shopping intents, thereby enabling a more focused individual anal-
ysis [ 7,18,80]. In summary, this framework should offer efficacy
and convenience by covering multiple downstream tasks, making
it well-suited for real-world E-commerce scenarios.
 
385
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yankai Chen, Quoc-Tuan Truong, Xin Shen, Jin L, & Irwin King
Challenges. Achieving balanced model performance across multi-
ple tasks poses a non-trivial challenge, as the customer represen-
tations in different scenarios own inherent diversities in learning
objectives. For instance, when segmenting similar customers, the
focus is primarily on capturing customer-customer similarity. Con-
versely, in tasks like item recommendation or customer shopping
intent identification, customer embeddings pivot around learning
customer-item relations or analyzing customer-self behaviors, re-
spectively. Thus, we identify the critical requirement lies in jointly
embedding (1) accurate similarity measurement between customers
and(2) informative content summarization in each customer engage-
ment. Consequently, the technical challenges are twofold:
â€¢Customer engagements exhibit both quantitative and substantive
variations. How to thoroughly reflect the similarity in customersâ€™
diverse activities within the fixed-size representations is the key
question that remains to be investigated. This is particularly im-
portant for customer segmentation, where the similarity between
customers is typically determined by their mutual distance in
the embedding space, requiring alignment with real-world mea-
surements. While one approach could be to aggregate all latent
information, such as through concatenation or pooling of feature
embeddings, this simplistic method may fail to provide a reliable
embedding distance measurement with theoretical guarantees.
â€¢Moreover, each customerâ€™s historical engagements reveal unique
preferences and interests. Apart from capturing customer-wise
similarity, it is imperative to preserve the semantic content of
each representation to the greatest extent possible. This preserves
compatibility for tasks like item recommendation or intention
analysis, enabling embedding matching or classification formula-
tion in the embedding space without exhaustive model retraining.
Approach and Contributions. In this work, we investigate the
aforementioned problem and introduce a novel Customer Shopping
TrAjectory Representation Learning framework (C-STAR), which is
designed to be versatile for multiple downstream tasks, as depicted
in Figure 1(A). C-STAR effectively encodes customer variable-size
engagements into a continuous Euclidean space, facilitating effi-
cient utilization for customer understanding and recommendation.
Initially, we introduce PR-Graph, i.e., an internal knowledge base
of product categories and relations that are organized in the graph
format. Product interactions of each customer are then mapped
into PR-Graph, creating his/her unique shopping trajectory. Each
trajectory represents a sub-graph pattern of PR-Graph, allowing us
to learn the customer trajectory representation that incorporates
both structural and semantic information. The proposed model
implements a representation learning paradigm enabling coarse-to-
fine trajectory-wise similarity measurements as well as informative
semantic enrichment in the embedding space. To enhance the em-
bedding quality, we leverage intrinsic properties within the trajec-
tory data structures and devise an effective pre-training strategy. As
illustrated in Figure 1(B), while specialized methods demonstrate
varying performances across different tasks, C-STAR consistently
achieves superior and well-balanced model performances (further
details in Â§ 5). Our primary contributions can be outlined as follows:(1)Inter-Trajectory Distribution Similarity . We propose to base on
the assumption that elements constructing a trajectory are sam-
pled from an underlying probability distribution reflecting cus-
tomer unique preferences. By measuring distribution distance,
we capture trajectory-wise similarity and incorporate this in-
formation into trajectory representations. Grounded in Optimal
Transport theory, our approach offers a consistent distance
measurement between realistic and embedding spaces.
(2)Intra-Trajectory Semantic Correlation. To capture the relational
knowledge among the trajectory elements, we further propose
to learn intra-trajectory semantic correlation from the structure
posed by PR-Graph. This will not merely provide a fine-grained
proximity measurement but also enrich the semantics of trajec-
tory representations, refining its capability for shopping intent
identification and item recommendation.
(3)C-STAR Pre-training Strategy. To improve the embedding qual-
ity, we leverage the intrinsic data properties and design two
pre-training objectives. Furthermore, while the C-STAR model
accommodates variable-size shopping trajectories for online
inference, tensorized pre-training requires fixed-size trajec-
tory batches. We then introduce effective data-driven sampling
strategies that are independent of human prior knowledge.
(4)Extensive Empirical Evaluation. We systematically conduct ex-
periments, including online A/B testing on our platform and
offline evaluation on both large-scale industrial data and four
public benchmarks across three tasks. Not only do we quanti-
tatively assess our model, but we also provide case studies to
broaden the understanding of our C-STAR framework.
2 RELATED WORK
Probability Distribution Distance Learning. To quantify the
distance between probability distributions, one may utilize diver-
gences such as Kullbackâ€“Leibler divergence [ 37], Jensenâ€“Shannon
divergence [ 15], or metrics likeHellinger distance [26]. Among these
measurement tools, Wasserstein metric [29], known for its rigorous
mathematical properties, has garnered attention in the machine
learning community, particularly in generative modeling [ 1,52,58].
Despite its advantages, the classic Wasserstein distance suffers from
high computational costs, particularly for high-dimensional distri-
butions. In contrast to numerical optimization methods [ 12,34,54],
recent studies of Sliced-Wasserstein distance [2,32] has significantly
reduced computational requirements. The general idea is to obtain
adequate linear projections of the original distribution onto multi-
ple one-dimensional distributions, followed by averaging the dis-
tances between these projected counterparts. This is facilitated by
the closed-form solution of one-dimensional Wasserstein distance.
Consequently, Sliced-Wasserstein distance has found application
in various practical tasks [ 3,33,35,42,46], including our proposed
modeling of high-dimensional trajectory distribution similarity.
E-commerce Customer Representation Learning. Represen-
tation learning for customer understanding constitutes a fundamen-
tal aspect of modern E-commerce recommender systems [ 59,61].
Early methods focuse on leveraging customer information such as
profiles [ 11] or social relationships [ 44]. However, privacy concerns
prompt the development of models that prioritize anonymity, lead-
ing to the learning particularly from item engagement sequences
 
386Shopping Trajectory Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[27,78]. Another line of research centers on exploiting the inher-
ent structure in the customer-item graph. In these models, each
customer is represented by a unique and anonymous ID, and collab-
orative filtering signals between customers and their engaged items
are captured [ 67]. These approaches often employ graph convolu-
tional network (GCN) frameworks, known for their flexibility and
adaptability in learning latent graph information [ 6,19,40,45,56,
57,70,77] with substantial advancements in recent years [ 24,71,73].
Different from sequential methods, these graph-based models typi-
cally require the input graph to be fixed in a customer-item adjacent
matrix, which may hinder their ability to generalize to unobserved
customers.
3 PROBLEM FORMULATION
PR-Graph. Our platform utilizes a product relational graph, re-
ferred to as PR-Graph, as a means to consolidate high-level prod-
uct knowledge for various research and application purposes. PR-
Graph is represented in the graph format as G=(T,E,V)1, where
Trepresents all graph nodes, and EâŠ†TÃ—T denotes the edges
connecting these nodes. Each node in Tassociated with a list of
d-dimensional feature embeddings represented by VâˆˆR|T|Ã—ğ‘‘. No-
tation explanations are in Table 1.
Table 1: Notations and meanings.
Notation Explanation
G=(T,E,V) PR-Graph with sets of nodes, edges, and features.
Gğ‘–=(Tğ‘–,Eğ‘–,Vğ‘–) Customer trajectory pattern.
Tğ‘–=[ğ‘¡ğ‘–
ğ‘›]ğ‘ğ‘–
ğ‘›=1Node list ofğ‘ğ‘–trajectory elements.
Vğ‘–=[ğ’—ğ‘–
ğ‘›]ğ‘ğ‘–
ğ‘›=1Feature list associated with ğ‘ğ‘–trajectory elements.
ğ‘“#ğ‘ƒ Pushfoward of distribution ğ‘ƒ.
ğ‘Šğ‘(Â·,Â·),ğ‘†ğ‘Šğ‘(Â·,Â·)ğ‘-Wasserstein distance, Sliced ğ‘-Wasserstein distance.
ğ¹ğ‘ƒ(Â·),ğ¹âˆ’1
ğ‘ƒ(Â·) Cumulative distribution function, quantile function.
R,S Euclidean space and unit hypersphere.
ğœ½ Unit vector in S.
ğ‘”ğœ½(Â·) Linear projection function with parameter vector ğœ½.
ğ‘ƒ0,ğ‘ƒğ‘– Reference distribution and input distribution.
ğ‘ƒğœ½
0,ğ‘ƒğœ½
ğ‘–The slices of ğ‘ƒ0,ğ‘ƒğ‘–derived by ğœ½.
ğ‘“âˆ—(Â·) Optimal transport map between two distributions.
ğ›¿(Â·) Dirac delta function.
ğœ(Â·|Â·) Ascending rank in the sorting of the given list.
Vğœ½
ğ‘–,Vğœ½
0Feature lists associated with distribution slices.
Inter-SE(Â·) Inter-Trajectory Similarity Encoder.
ğ‘¬ğ‘– Embedding ofGğ‘–with inter-trajectory information.
ğ‘ğ‘”â„(Â·) Set of all neighbor nodes of the input.
ğ’—(ğ‘™)
ğ‘ğ‘”â„(ğ‘¡ğ‘–ğ‘›)Neighborhood feature embedding of ğ‘¡ğ‘–
ğ‘›at theğ‘™-th layer.
N(ğ‘™)
ğ‘–=[ğ’—(ğ‘™)
ğ‘ğ‘”â„(ğ‘¡ğ‘–ğ‘›)]ğ‘¡ğ‘–ğ‘›âˆˆTğ‘–Neighborhood feature list at the ğ‘™-th layer.
Intra-CE(Â·) Intra-Trajectory Correlation Encoder.
ğ‘¬â€²
ğ‘–Embedding ofGğ‘–with intra-trajectory information.
ğ‘¬â˜…
ğ‘–Ultimate trajectory representation.
ğ‘ƒğ‘Ÿ(Â·) Sampling probability.
ğ‘Ÿğ‘¡ Local ranking of node ğ‘¡in its belonging trajectory.
ğ‘¡+,ğ‘¡âˆ’,ğ’—ğ‘¡+,ğ’—ğ‘¡âˆ’ Positive and negative nodes and embeddings.
L1,L2,L Margin ranking loss terms and objective function.
Î” Set of all trainable embeddings and variables.
PR-Graph categorizes all products into approximately 15K nodes2
withinTand establishes 417K linkages in Eto encapsulate strongly-
correlated product relations, e.g., co-purchases.
Problem Formulation. AsT=[ğ‘¡ğ‘›]|T|
ğ‘›=1denotes all observed nodes
inG, for each customer ğ‘–, their interactions over nodes in Tcan
1Data statistics are in Â§ 5.2â€œCustomerâ€ and â€œtrajectoryâ€ are interchangeable as each
customer owns a unique trajectory.be snapshotted as Tğ‘–âŠ†T, i.e.,Tğ‘–=[ğ‘¡ğ‘–ğ‘›]ğ‘ğ‘–
ğ‘›=1withğ‘ğ‘–elements. Al-
thoughTğ‘–is 1-dimensional structure, leveraging the topological
information inG, we derive the customerâ€™s shopping trajectory
Tğ‘–as a unique sub-graph Gğ‘–âŠ†G. Thus, our objective is to learn
the trajectory representation from the knowledge contained in
the 1&2-dimensional list-graph data ( Tğ‘–,Gğ‘–) such that the learned
representation satisfies the following criterion simultaneously:
â€¢Inter-Trajectory Similarity Measurement. This offers a macro
view of trajectory representations in the latent space, enhancing
customer understanding. It is particularly beneficial for applica-
tions like customer segmentation, where a holistic measurement
of trajectory-wise (or customer-wise3) similarity is required.
â€¢Intra-Trajectory Content Summarization. This provides a
micro view of trajectory elements to enhance understanding of
trajectory semantics. It facilitates applications such as shopping
intent identification and trajectory completion by capturing the
crucial correlations between elements.
4 C-STAR METHODOLOGY
Figure 2 depicts the workflow of the model. As previously men-
tioned, we address the first criterion, namely inter-trajectory simi-
larity measurement, by quantifying the distance between trajectory
probability distributions. Leveraging Optimal Transport theory,
which offers rigorous mathematical properties [ 29], we begin by
laying out the preliminaries and formally deriving our method.
4.1 Preliminaries: Optimal Transport
Optimal transport (OT) is the general problem of moving one distri-
bution of mass, e.g., ğ‘ƒ, to another, e.g., ğ‘„, as efficiently as possible.
Formally, given a probability distribution ğ‘ƒ, let random variable ğ‘¿âˆ¼
ğ‘ƒ,ğ‘¿âˆˆRğ‘‘. Ifğ‘“:Rğ‘‘â†’R, thenğ‘“#ğ‘ƒis the push-forward of ğ‘ƒ, i.e.,ğ‘“#ğ‘ƒ(ğ’€)
=ğ‘ƒ({ğ’™:ğ‘“(ğ’™)âˆˆğ’€})=ğ‘ƒ(ğ‘“âˆ’1(ğ’€)). The Wasserstein distance be-
tweenğ‘ƒandğ‘„, as the derived cost of their optimal transport plan,
is defined with ğ¿ğ‘transport cost [66]:
ğ‘Šğ‘(ğ‘ƒ
,ğ‘„)=
infğ‘“âˆˆğ‘‡ğ‘ƒ(ğ‘ƒ,ğ‘„)âˆ«
âˆ¥ğ’™âˆ’ğ‘“(ğ’™)âˆ¥ğ‘ğ‘‘ğ‘ƒ(ğ’™)1
ğ‘,
ğ‘â‰¥1,(1)
where the infimum is over all possible transport plans. If a minimizer
exists, denoted by ğ‘“âˆ—, it is thus the solution to the OT problem.
Forone-dimensional distributions, there is a close-form solution
to compute ğ‘“âˆ—, asğ‘“âˆ—(ğ‘¥):=ğ¹âˆ’1
ğ‘ƒ ğ¹ğ‘„(ğ‘¥), whereğ¹is the cumulative
distribution function (CDF) associated with ğ‘ƒ.ğ¹âˆ’1
ğ‘ƒ(ğ‘¥)is the quantile
function of ğ‘ƒ. For the high-dimensional case, due to its numerical
intractability issue [ 32], the metric of sliced-Wasserstein distance
has been recently investigated [2, 13, 50] and defined as:
ğ‘†ğ‘Šğ‘(ğ‘ƒ,ğ‘„)=âˆ«
Sğ‘‘âˆ’1 ğ‘Šğ‘(ğ‘”ğœ½#ğ‘ƒ,ğ‘”ğœ½#ğ‘„)ğ‘ğ‘‘ğœ½1
ğ‘. (2)
Hereğ‘”ğœ½(ğ’™)=ğœ½Tğ’™andğœ½âˆˆSğ‘‘âˆ’1is a unit vector in Rğ‘‘, and Sğ‘‘âˆ’1is the
unitğ‘‘-dimensional hypersphere. ğ‘”ğœ½#ğ‘ƒis the push-forward of ğ‘ƒwith
ğ‘”ğœ½. This metric satisfies positive-definiteness ,symmetry , and
triangle inequality [32,35], qualified for distance measurement
in our proposed model to capture trajectory-wise similarity.
4.2 Inter-Trajectory Distribution Similarity
Following [ 46], we consider a list of probability measures [ğ‘ƒğ‘–]ğ‘€
ğ‘–=1
defined in Rğ‘‘forğ‘€observed trajectories. For each trajectory, there
 
387KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yankai Chen, Quoc-Tuan Truong, Xin Shen, Jin L, & Irwin King
Shopping TrajectoryIsolated Interaction Data
I. Trajectory Construction.PR-Graph II. Inter-Trajectory Distribution Similarity.
VI. Intra-Trajectory Semantic Correlation.Trajectory Element InputIII. Implementation of             for Each Distribution Slice.fâ‡¤(Â·)<latexit sha1_base64="JuVIKdygEH6PyyCpAhs2dhvQcrQ=">AAAB83icbVBNS8NAEN3Ur1q/qh69BFuheihJUfRY9OKxgv2AJpbNZtMu3eyG3YlQQv+GFw+KePXPePPfuG1z0OqDgcd7M8zMCxLONDjOl1VYWV1b3yhulra2d3b3yvsHHS1TRWibSC5VL8CaciZoGxhw2ksUxXHAaTcY38z87iNVmklxD5OE+jEeChYxgsFIXjV6OKt5JJRwWh2UK07dmcP+S9ycVFCO1qD86YWSpDEVQDjWuu86CfgZVsAIp9OSl2qaYDLGQ9o3VOCYaj+b3zy1T4wS2pFUpgTYc/XnRIZjrSdxYDpjDCO97M3E/7x+CtGVnzGRpEAFWSyKUm6DtGcB2CFTlACfGIKJYuZWm4ywwgRMTCUTgrv88l/SadTd8/rFXaPSvM7jKKIjdIxqyEWXqIluUQu1EUEJekIv6NVKrWfrzXpftBasfOYQ/YL18Q0965CF</latexit>
Inter-trajectory Representations Inter-SE ModuleFeature Distribution
Intra-CEModuleGraph-based Trajectory InputIntra-trajectory Semantics 
Å
Neighborhood Feature PropagationDistribution SlicingComputing the Optimal Transport Map from      to      Pâœ“i<latexit sha1_base64="b6WOJU0q1wzVYhEdvSMR2Pe/tuU=">AAAB83icbVDLSgNBEJyNrxhfUY9eBhPBU9gNih6DXjxGMA/IrmF20psMmX0w0yuEJb/hxYMiXv0Zb/6Nk2QPmljQUFR1093lJ1JotO1vq7C2vrG5Vdwu7ezu7R+UD4/aOk4VhxaPZay6PtMgRQQtFCihmyhgoS+h449vZ37nCZQWcfSAkwS8kA0jEQjO0EhutdkXjy6OAFm1X67YNXsOukqcnFRIjma//OUOYp6GECGXTOueYyfoZUyh4BKmJTfVkDA+ZkPoGRqxELSXzW+e0jOjDGgQK1MR0rn6eyJjodaT0DedIcORXvZm4n9eL8Xg2stElKQIEV8sClJJMaazAOhAKOAoJ4YwroS5lfIRU4yjialkQnCWX14l7XrNuahd3tcrjZs8jiI5IafknDjkijTIHWmSFuEkIc/klbxZqfVivVsfi9aClc8ckz+wPn8AOCSRKA==</latexit>
Pâœ“0<latexit sha1_base64="mKWqzZbsCeob8wdU/qCEOZnNUng=">AAAB83icbVDLSgNBEJyNrxhfUY9eBhPBU9gNih6DXjxGMA/IrmF20psMmX0w0yuEJb/hxYMiXv0Zb/6Nk2QPmljQUFR1093lJ1JotO1vq7C2vrG5Vdwu7ezu7R+UD4/aOk4VhxaPZay6PtMgRQQtFCihmyhgoS+h449vZ37nCZQWcfSAkwS8kA0jEQjO0Ehutdm3H10cAbJqv1yxa/YcdJU4OamQHM1++csdxDwNIUIumdY9x07Qy5hCwSVMS26qIWF8zIbQMzRiIWgvm988pWdGGdAgVqYipHP190TGQq0noW86Q4YjvezNxP+8XorBtZeJKEkRIr5YFKSSYkxnAdCBUMBRTgxhXAlzK+UjphhHE1PJhOAsv7xK2vWac1G7vK9XGjd5HEVyQk7JOXHIFWmQO9IkLcJJQp7JK3mzUuvFerc+Fq0FK585Jn9gff4A3+mQ7w==</latexit>
Pâœ“0<latexit sha1_base64="mKWqzZbsCeob8wdU/qCEOZnNUng=">AAAB83icbVDLSgNBEJyNrxhfUY9eBhPBU9gNih6DXjxGMA/IrmF20psMmX0w0yuEJb/hxYMiXv0Zb/6Nk2QPmljQUFR1093lJ1JotO1vq7C2vrG5Vdwu7ezu7R+UD4/aOk4VhxaPZay6PtMgRQQtFCihmyhgoS+h449vZ37nCZQWcfSAkwS8kA0jEQjO0Ehutdm3H10cAbJqv1yxa/YcdJU4OamQHM1++csdxDwNIUIumdY9x07Qy5hCwSVMS26qIWF8zIbQMzRiIWgvm988pWdGGdAgVqYipHP190TGQq0noW86Q4YjvezNxP+8XorBtZeJKEkRIr5YFKSSYkxnAdCBUMBRTgxhXAlzK+UjphhHE1PJhOAsv7xK2vWac1G7vK9XGjd5HEVyQk7JOXHIFWmQO9IkLcJJQp7JK3mzUuvFerc+Fq0FK585Jn9gff4A3+mQ7w==</latexit>
Pâœ“i<latexit sha1_base64="b6WOJU0q1wzVYhEdvSMR2Pe/tuU=">AAAB83icbVDLSgNBEJyNrxhfUY9eBhPBU9gNih6DXjxGMA/IrmF20psMmX0w0yuEJb/hxYMiXv0Zb/6Nk2QPmljQUFR1093lJ1JotO1vq7C2vrG5Vdwu7ezu7R+UD4/aOk4VhxaPZay6PtMgRQQtFCihmyhgoS+h449vZ37nCZQWcfSAkwS8kA0jEQjO0EhutdkXjy6OAFm1X67YNXsOukqcnFRIjma//OUOYp6GECGXTOueYyfoZUyh4BKmJTfVkDA+ZkPoGRqxELSXzW+e0jOjDGgQK1MR0rn6eyJjodaT0DedIcORXvZm4n9eL8Xg2stElKQIEV8sClJJMaazAOhAKOAoJ4YwroS5lfIRU4yjialkQnCWX14l7XrNuahd3tcrjZs8jiI5IafknDjkijTIHWmSFuEkIc/klbxZqfVivVsfi9aClc8ckz+wPn8AOCSRKA==</latexit>
Input Reference Pi<latexit sha1_base64="LzKMpSTsk4CHDrJQKHnz2sSusq0=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IURY9FLx4rmFpoQ9lsN+3SzSbsToRS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O4W19Y3NreJ2aWd3b/+gfHjUMkmmGfdZIhPdDqnhUijuo0DJ26nmNA4lfwxHtzP/8YlrIxL1gOOUBzEdKBEJRtFKfrXZE9VeueLW3DnIKvFyUoEczV75q9tPWBZzhUxSYzqem2IwoRoFk3xa6maGp5SN6IB3LFU05iaYzI+dkjOr9EmUaFsKyVz9PTGhsTHjOLSdMcWhWfZm4n9eJ8PoOpgIlWbIFVssijJJMCGzz0lfaM5Qji2hTAt7K2FDqilDm0/JhuAtv7xKWvWad1G7vK9XGjd5HEU4gVM4Bw+uoAF30AQfGAh4hld4c5Tz4rw7H4vWgpPPHMMfOJ8/3jKOEg==</latexit>
P0<latexit sha1_base64="aUgMs05r0TpGJa/AEj2Pg6eZoaw=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IURY9FLx4rmFpoQ9lsN+3SzSbsToRS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O4W19Y3NreJ2aWd3b/+gfHjUMkmmGfdZIhPdDqnhUijuo0DJ26nmNA4lfwxHtzP/8YlrIxL1gOOUBzEdKBEJRtFKfrXZc6u9csWtuXOQVeLlpAI5mr3yV7efsCzmCpmkxnQ8N8VgQjUKJvm01M0MTykb0QHvWKpozE0wmR87JWdW6ZMo0bYUkrn6e2JCY2PGcWg7Y4pDs+zNxP+8TobRdTARKs2QK7ZYFGWSYEJmn5O+0JyhHFtCmRb2VsKGVFOGNp+SDcFbfnmVtOo176J2eV+vNG7yOIpwAqdwDh5cQQPuoAk+MBDwDK/w5ijnxXl3PhatBSefOYY/cD5/AIeVjdk=</latexit>
gâœ“(Â·):Rd!R<latexit sha1_base64="TbQdtFC6z0RDuqD4+9XJEHvu/Aw=">AAACI3icbVDLTgIxFO34RHyhLt00ggluyAzRaFgR3bhEI4+EQdLpFGjoTCftHQ2Z8C9u/BU3LjTEjQv/xfJYIHiSJqfn3Jt77/EiwTXY9re1srq2vrGZ2kpv7+zu7WcODmtaxoqyKpVCqoZHNBM8ZFXgIFgjUowEnmB1r38z9utPTGkuwwcYRKwVkG7IO5wSMFI7U8p12y70GJC8S30JZ6Vczg0I9DwvuR8++uaneLcHRCn5PG/lcDuTtQv2BHiZODOSRTNU2pmR60saBywEKojWTceOoJUQBZwKNky7sWYRoX3SZU1DQxIw3UomNw7xqVF83JHKvBDwRJ3vSEig9SDwTOV4R73ojcX/vGYMnatWwsMoBhbS6aBOLDBIPA4M+1wxCmJgCKGKm10x7RFFKJhY0yYEZ/HkZVIrFpzzwsVdMVu+nsWRQsfoBOWRgy5RGd2iCqoiil7QG/pAn9ar9W6NrK9p6Yo16zlCf2D9/AJzSKON</latexit>
Norm Distance ApproximationkEi Ejkpâ‡¡SWp(Pi,Pj)<latexit sha1_base64="aGrVnOyLac+qCIlfITa3H6ecomg=">AAACQHicbVBJSwMxGM241rpVPXoJtkIFLTNF0WNRBI8V7QKdMmTSTBubmYQkI5axP82LP8GbZy8eFPHqyXQRtPWDkMdbSL7nC0aVtu1na2Z2bn5hMbWUXl5ZXVvPbGxWFY8lJhXMGZd1HynCaEQqmmpG6kISFPqM1Pzu2UCv3RKpKI+udU+QZojaEQ0oRtpQXqaWcxkJtHvv+py1VC80V3Le9+jBBHHjStruGKMnoIuEkPwOXsGaJ4b5fNmj+7D849rLeZmsXbCHA6eBMwZZMJ6yl3lyWxzHIYk0ZkiphmML3UyQ1BQz0k+7sSIC4S5qk4aBEQqJaibDAvpw1zAtGHBpTqThkP2dSFCoBqsYZ4h0R01qA/I/rRHr4KSZ0EjEmkR49FAQM6g5HLQJW1QSrFnPAIQlNX+FuIMkwtp0njYlOJMrT4NqseAcFo4ui9nS6biOFNgGOyAPHHAMSuAClEEFYPAAXsAbeLcerVfrw/ocWWescWYL/Bnr6xuVDbDb</latexit>
kE0kp=0<latexit sha1_base64="0Z7CtGo+WEXSoZJr0EpvdG0cy04=">AAACEHicbVDLSsNAFJ34rPUVdelmsBVdlaQouhGKIrisYB/QhDCZTNqhk0yYmQgl9hPc+CtuXCji1qU7/8ZJm4W2HhjmcM693HuPnzAqlWV9GwuLS8srq6W18vrG5ta2ubPbljwVmLQwZ1x0fSQJozFpKaoY6SaCoMhnpOMPr3K/c0+EpDy+U6OEuBHqxzSkGCkteeZR1WEkVM6D43MWyFGkv+x67FmOoP2B1r0EXkCr6pkVq2ZNAOeJXZAKKND0zC8n4DiNSKwwQ1L2bCtRboaEopiRcdlJJUkQHqI+6Wkao4hIN5scNIaHWglgyIV+sYIT9XdHhiKZ76orI6QGctbLxf+8XqrCczejcZIqEuPpoDBlUHGYpwMDKghWbKQJwoLqXSEeIIGw0hmWdQj27MnzpF2v2Se109t6pXFZxFEC++AAHAMbnIEGuAFN0AIYPIJn8ArejCfjxXg3PqalC0bRswf+wPj8AQWBnIs=</latexit>
Embedding OutputkEikpâ‡¡SWp(Pi,P0)<latexit sha1_base64="eoB9JnBjs1OZ7KSoBjRSqcT39Sc=">AAACL3icbVBLSwMxGMzWV62vVY9egq1QQcpuUfRYFMVjRfuAblmyabYNzW5CkhXL2n/kxb/Si4giXv0Xpo+DVj8IGWbmI5kJBKNKO86rlVlYXFpeya7m1tY3Nrfs7Z264onEpIY547IZIEUYjUlNU81IU0iCooCRRtC/GOuNeyIV5fGdHgjSjlA3piHFSBvKt68KHiOh9h69gLOOGkTmSi+HPvUk7fYM7wvoISEkf4C3sOGLib1Y9ekRrPrO1HVY8O28U3ImA/8CdwbyYDZV3x55HY6TiMQaM6RUy3WEbqdIaooZGea8RBGBcB91ScvAGEVEtdNJ3iE8MEwHhlyaE2s4YX9upChS4yjGGSHdU/PamPxPayU6PGunNBaJJjGePhQmDGoOx+XBDpUEazYwAGFJzV8h7iGJsDYV50wJ7nzkv6BeLrnHpZObcr5yPqsjC/bAPigCF5yCCrgGVVADGDyBEXgD79az9WJ9WJ9Ta8aa7eyCX2N9fQNn4qkx</latexit>
Figure 2: Illustration of C-STAR for coarse-to-fine trajectory representation learning (best view in color).
is a unique associated feature list Vğ‘–=[ğ’—ğ‘¡ğ‘–ğ‘›âˆˆRğ‘‘]ğ‘ğ‘–
ğ‘›=1withğ‘ğ‘–ele-
ments. We assume that these feature elements are sampled from
the underlying distribution ğ‘ƒğ‘–, and what we have snapshot is the
empirical (discrete) distribution bğ‘ƒğ‘–with its empirical CDF as:
ğ¹bğ‘ƒğ‘–(ğ’™)=1
ğ‘ğ‘–Ã•ğ‘ğ‘–
ğ‘›=1ğ›¿(ğ’™â‰¥ğ’—ğ‘¡ğ‘–ğ‘›), (3)
whereğ›¿(Â·)returns 1 if the input is zero and 0 otherwise4. Generally,
we believe empirical distributions are representative, i.e., bğ‘ƒğ‘–â‰ˆğ‘ƒğ‘–;
thus we refer ğ‘ƒğ‘–tobğ‘ƒğ‘–hereafter to avoid notation abuse.
To explicitly measure the trajectory-wise similarity, we propose
to compare the input trajectory distribution with a certain trainable
reference that functions as the â€œoriginâ€ in the trajectory embed-
ding space. Specifically, we introduce a reference distribution ğ‘ƒ0
with the embedding list V0=[ğ’—ğ‘¡0ğ‘›âˆˆRğ‘‘]ğ‘
ğ‘›=1, elements in which are
the trainable embeddings. Then our target is: to get the distance
between the distribution pair ( ğ‘ƒ0,ğ‘ƒğ‘–) to guide the learning of asso-
ciated trajectory representations ( ğ‘¬0,ğ‘¬ğ‘–) with a matched distance
measurement back in the embedding space.
4.2.1 Implementing the Optimal Transport Plan ğ‘“âˆ—.Notic-
ing thatğ‘“âˆ—is crucial to determine the (shortest) distribution dis-
tance, thus, we formally introduce how we algorithmically im-
plement it. This lays the foundation to construct the proposed
trajectory representation encoder afterwards.
As explained in Â§ 4.1, directly solving the high-dimensional opti-
mal transport is extremely difficult; thus, we would first conduct
the distribution slicing for one-dimensional Wasserstein distance
computing. Let ğ‘”ğœ½(ğ’™)denote the linear projection function. For
notation simplicity, we use ğ‘ƒğœ½
ğ‘–:=ğ‘”ğœ½#ğ‘ƒğ‘–to denote the slice of ğ‘ƒğ‘–w.r.t.
ğ‘”ğœ½(i.e.,ğ‘ƒğœ½
ğ‘–is the push-forwarded one-dimensional distribution in
R); similarly ğ‘ƒğœ½
0:=ğ‘”ğœ½#ğ‘ƒ0. With proofs attached in Appendix A, we
firstly introduce the following proposition:
Proposition 1. ğ‘“âˆ—from reference slice ğ‘ƒğœ½
0to the input distribution
sliceğ‘ƒğœ½
ğ‘–is formulated as:
ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–):=ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘– ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½), ğ‘¥ğœ½âˆˆVğœ½
0. (4)
To differentiate the high-dimensional input of Eqn. (3), ğ‘¥ğœ½de-
notes the projected input of Eqn. (4) that lives in R. For each sliced
empirical distribution ğ‘ƒğœ½
ğ‘–, the corresponding features are Vğœ½
ğ‘–=
4âˆ«
ğ›¿(ğ‘¥)ğ‘‘ğ‘¥=1for continuous inputs.[ğœ½Tğ’—ğ‘¡ğ‘–ğ‘›]ğ‘ğ‘–
ğ‘›=1. Similarly, the sliced reference list is Vğœ½
0=[ğœ½Tğ’—ğ‘¡0ğ‘›]ğ‘
ğ‘›=1.
Notice that their empirical CDFs, e.g., ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½)=1
ğ‘Ãğ‘
ğ‘›=1ğ›¿(ğ‘¥ğœ½âˆ’ğœ½TÂ·
ğ’—ğ‘¡0ğ‘›), is monotonically increasing. This implies that, if we know the
ranking of each input ğ‘¥ğœ½in the ascending sorting ofVğœ½
0, denoted by
ğœ(ğ‘¥ğœ½|Vğœ½
0), andğ‘=ğ‘ğ‘–, the optimal transport plan ğ‘“âˆ—can be more
quantitatively interpreted as follows:
Proposition 2. Ifğ‘=ğ‘ğ‘–,âˆ€ğ‘¥ğœ½âˆˆVğœ½
0, the optimal plan in Eqn. (4)
functions as the mapping between Vğœ½
0andVğœ½
ğ‘–:
ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–)=argminğ‘¥â€²âˆˆVğœ½
ğ‘–
ğœ(ğ‘¥â€²|Vğœ½
ğ‘–)=ğœ(ğ‘¥ğœ½|Vğœ½
0)
.(5)
Notice that, indicator ğœ(Â·)can be actually pre-processed via
â€œargsort â€ toVğœ½
ğ‘–and â€œ sort â€ toVğœ½
0. However, the major inade-
quacy is that Eqn. (5) requires |ğ‘ğ‘–|=ğ‘, which may not always be
the case as the trajectory size varies in practice. To align the feature
list cardinalities (i.e., for the case of ğ‘ğ‘–â‰ |ğ‘|) but not demolish their
original semantics, one neat yet effective solution is to conduct
linear interpolation, as it is essentially a process for data continuing.
Formally, we summarize our algorithmic implementation as:
For different cardinalities of feature lists Vğœ½
ğ‘–andVğœ½
0, we imple-
mentğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–)=ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘– ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½)between(Vğœ½
ğ‘–,Vğœ½
0)with the
following mapping process:
ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–)=argminğ‘¥â€²âˆˆVğœ½
ğ‘–
ğœ(ğ‘¥â€²|Vğœ½
ğ‘–)â‰¥ğ‘ğ‘–
ğ‘Â·ğœ(ğ‘¥ğœ½|Vğœ½
0)
.(6)
4.2.2 Inter- TrajectorySimilarity Enco ding. Foreach pair of
distribution slices, e.g.,(ğ‘ƒğœ½
0,ğ‘ƒğœ½
ğ‘–),their optimal transp ortplan pro-
duces theshortest one-dimensional distance ,i.e.,ğ‘Šğ‘(ğ‘ƒğœ½
0,ğ‘ƒğœ½
ğ‘–).Ac-
cording tothetheoryshowninEqn. (2),thenextstep istotraverse
allğœ½âˆˆSğ‘‘âˆ’1fortheultimate integral betweenoriginal distributions
(ğ‘ƒ0,ğ‘ƒğ‘–).However,thismay beinfeasible inpractice todraw an
infinite numb erofprojections; therefore,inthiswork,with ğœ½ğ‘ de-
noting theğ‘ -thprojection parameter uniformly sample dfromSğ‘‘âˆ’1,
weapproach thistarget with theMonte-Carlo approximation. Con-
sequently ,thisleads toacumulativ esliced-Wasserstein distance
fortheoriginal trajectorydistributions:
ğ‘†ğ‘Šğ‘(ğ‘ƒ0,ğ‘ƒğ‘–)â‰ˆ1
ğ‘†Ã•ğ‘†
ğ‘ =1ğ‘Šğ‘(ğ‘ƒğœ½ğ‘ 
0,ğ‘ƒğœ½ğ‘ 
ğ‘–)ğ‘1
ğ‘. (7)
Basedonthealgorithmi cimplementation showninEqn. (6)with
theassociate ddista nceregularization, weproceedtoenco dethe
 
388Shopping Trajectory Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
trajectory representation accordingly. Let Î˜={ğœ½ğ‘ }ğ‘†
ğ‘ =1denote the
set of sampled projection parameters. Firstly, we encode the vector
ğ‘¶âˆˆRğ‘Â·ğ‘†from the embedding reference V0=[ğ’—ğ‘¡0ğ‘›]ğ‘
ğ‘›=1ofğ‘ƒ0as:
ğ‘¶:=1
ğ‘†ğ‘ğ‘†
ğ‘ =1ğ‘
ğ‘›=1ğœ½ğ‘ Tğ’—ğ‘¡0ğ‘›. (8)
âˆ¥denotes the concatenation operation along the innermost di-
mension. Given the input feature list Vğ‘–, our Inter-Trajectory
Similarity Encoder (Inter-SE) is formally defined as follows:
Inter-SE(Vğ‘–|Î˜):=1
ğ‘†ğ‘ğ‘†
ğ‘ =1ğ‘
ğ‘›=1ğ‘“âˆ—(ğœ½T
ğ‘ ğ’—ğ‘¡0ğ‘›|Vğœ½ğ‘ 
ğ‘–)âˆ’ğ‘¶,(9)
Letğ‘¬ğ‘–âˆˆRğ‘Â·ğ‘†denote the encoded representation. we have the
following theorem with proof in Appendix A.
Theorem 1 ( ğ¿ğ‘Norm Distance Approximation ).For any
two input trajectories with corresponding distributions ğ‘ƒğ‘–andğ‘ƒğ‘—,
their encoded representations ğ‘¬ğ‘–andğ‘¬ğ‘—hold that: (1)âˆ¥ğ‘¬ğ‘–âˆ’ğ‘¬ğ‘—âˆ¥ğ‘â‰ˆ
ğ‘†ğ‘Šğ‘(ğ‘ƒğ‘–,ğ‘ƒğ‘—)and (2)âˆ¥ğ‘¬ğ‘–âˆ¥ğ‘â‰ˆğ‘†ğ‘Šğ‘(ğ‘ƒğ‘–,ğ‘ƒ0).
By settingğ‘=2,âˆ¥ğ‘¬ğ‘–âˆ’ğ‘¬ğ‘—âˆ¥2is exactly the Euclidean distance form
that is more favorable to scenarios for recalling vectorized objects.
4.3 Intra-Trajectory Semantic Correlation
Due to the discreteness of empirical distributions, solely encoding
the inter-trajectory similarity is coarse-grained, as it essentially
captures the appearance disparity of trajectory features. However,
the trajectory elements are further semantically correlated. To fuse
such knowledge and enrich trajectory representations, we propose
to learn the fine-grained trajectory-element semantic correlation.
We leverage PR-Graph to have the graph-based trajectories and
then employ the graph convolutional paradigm, mainly because
of its powerful ability to learn high-order graph information [ 69].
However, vanilla designs [ 19,31,65] usually aim to encapsulate
graph information into condensed outputs, which may lead to lim-
ited expressivity to measure the correlation level. To tackle this
issue, we notice that if nodes are more correlated via the edges in
PR-Graph, their local neighborhood structures tend to be more iso-
morphic. Based on this intuition, we introduce our Intra-Trajectory
Correlation Encoding as follows.
4.3.1 Intra-Trajectory Correlation Encoding. For each node
in trajectoryGğ‘–, its neighbor node features can be collected as Nğ‘–
=[ğ’—ğ‘¡ğ‘–ğ›¼]ğ‘¡ğ‘–ğ›¼âˆˆğ‘ğ‘”â„(Tğ‘–), whereğ‘ğ‘”â„(Tğ‘–)returns all neighbors of Tğ‘–â€™s el-
ements in PR-Graph. As shown in Figure 3(A), highly-correlated
nodes should have the similar neighborhood. To explicitly embed
such information, a naive solution will be to find the optimal trans-
port map directly for Nğ‘–, providing the correlation measurement
w.r.t. neighbor node distributions. However, the major concern is
that the size ofNğ‘–exponentially increases in the higher-order graph
structure, leading to an intractable computational process.
To circumvent this issue, we develop approximation of high-
order neighborhood features in layer-wise PR-Graph knowledge
propagation. For each node ğ‘¡ğ‘–ğ‘›ofTğ‘–=[ğ‘¡ğ‘–ğ‘›]ğ‘ğ‘–
ğ‘›=1, we first summarize
its neighborhood feature embedding at the ğ‘™-th iteration ( ğ‘™>0):
ğ’—(ğ‘™)
ğ‘ğ‘”â„(ğ‘¡ğ‘–ğ‘›)=Ã•
ğ‘¡âˆˆğ‘ğ‘”â„(ğ‘¡ğ‘–ğ‘›)ğ‘¤(ğ‘™âˆ’1)
ğ‘¡,ğ‘¡ğ‘–ğ‘›qğ‘ğ‘”â„(ğ‘¡)+1qğ‘ğ‘”â„(ğ‘¡ğ‘–ğ‘›)+1ğ’—(ğ‘™âˆ’1)
ğ‘¡ğ‘–ğ‘›.
(10)
OverlappedNeighborsSubgragh PatternsShopping Trajectories on PR-Graph(A)(B)Figure 3: (A) Illustration of correlated neighborhood.
(B) Probability curves of sampling strategies, where (1)
sampling size =32, (2)ğœŒ=0.5for GDS, and (3) ğœ†=10for PGDS.
ğ‘¤(ğ‘™âˆ’1)
ğ‘¡,ğ‘¡ğ‘–ğ‘›âˆˆRandğ’—(0)
ğ‘¡ğ‘–ğ‘›is initialized by ğ’—ğ‘¡ğ‘–ğ‘›inVğ‘–. We then re-define Nğ‘–
as the layer-wise neighborhood feature list as follows:
N(ğ‘™)
ğ‘–=h
ğ’—(ğ‘™)
ğ‘ğ‘”â„(ğ‘¡ğ‘–ğ‘›)i
ğ‘¡ğ‘–ğ‘›âˆˆTğ‘–. (11)
Based on these propagated neighborhood features, we have our
Intra-Trajectory Correlation Encoder (Intra-CE) as:
Intra-CE(Vğ‘–|Î˜):=Ã•ğ¿
ğ‘™=1ğ›¼ğ‘™1
ğ‘†ğ‘ğ‘†
ğ‘ =1ğ‘
ğ‘›=1ğ‘“âˆ—(ğœ½T
ğ‘ ğ’—ğ‘¡0ğ‘›|N(ğ‘™)
ğ‘–)âˆ’ğ‘¶
.
(12)
ğ›¼ğ‘™is theğ‘™-th coefficient and, for brevity, we set it as ğ›¼ğ‘™=1/ğ¿. No-
tice that node embeddings are usually updated with neighborhood
information iteratively, i.e., ğ’—(ğ‘™)
ğ‘¡ğ‘–ğ‘›=AGG(ğ’—(ğ‘™)
ğ‘ğ‘”â„(ğ‘¡ğ‘–ğ‘›),ğ’—(ğ‘™âˆ’1)
ğ‘¡ğ‘–ğ‘›). AGG(Â·)ab-
stracts different aggregators [ 19,31,65,69] and we directly follow
classic GCN [ 31]. With ğ‘¬ğ‘–andğ‘¬â€²
ğ‘–denoting outputs from Inter-SE
andIntra-CE , we complete the trajectory representation as: ğ‘¬â˜…
ğ‘–=
[ğ‘¬ğ‘–,ğ‘¬â€²
ğ‘–]âˆˆR2ğ‘ğ‘†. As shown in Â§ 5.3, both modules make pragmatic
contributions to model performance over all evaluation tasks.
4.4 C-STAR Pre-training Strategy
4.4.1 Pre-training Objectives .Pre-training is an effective strat-
egy to enhance model performance [ 41,55]. We design following
pre-training objectives by capturing two data properties:
â€¢Inter-trajectory Element Overlaps. Given two trajectories, the more
overlapping trajectory elements they share, the more similar they
tend to be with common historical preferences. Therefore, for
each trajectory, e.g., Tğ‘–, we explicitly rank its similar counterparts
based on the number of overlapping elements and use it as the
supervision signal. Specifically, let Î©ğ‘–denote such ranking list
associated withTğ‘–; we retrieve the trajectory pair ( Tğ‘—,Tğ‘˜)âˆˆÎ©ğ‘–
such thatTğ‘–shares more overlapping elements with Tğ‘—thanTğ‘˜,
i.e.,|Tğ‘–âˆ©Tğ‘—|>|Tğ‘–âˆ©Tğ‘˜|. The loss term is defined as follows:
L1=ğ‘€Ã•
ğ‘–=1ğ»Ã•
(
Gğ‘—,Gğ‘˜)âˆˆÎ©ğ‘–max 0,âˆ¥ğ‘¬â˜…
ğ‘–âˆ’ğ‘¬â˜…
ğ‘—âˆ¥2âˆ’âˆ¥ğ‘¬â˜…
ğ‘–âˆ’ğ‘¬â˜…
ğ‘˜âˆ¥2+ğ‘šğ‘ğ‘Ÿğ‘”ğ‘–ğ‘›,
(13)
wher
eğ‘¬â˜…
ğ‘—,ğ‘¬â˜…
ğ‘˜are the encoded trajectory embeddings of Tğ‘—,Tğ‘˜.
â€¢Intra-trajectory Contextual Relations. Another aspect to improve
the trajectory embedding quality is to capture the trajectory-
element relationship. We randomly pick the positive nodes, i.e.,
appeared elements in the given trajectory, and then maximize the
matching scores between trajectory embedding and the chosen
element embeddings. This aims to ensure that these matching
scores surpass the scores computed from other negative nodes
 
389KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yankai Chen, Quoc-Tuan Truong, Xin Shen, Jin L, & Irwin King
that not appear in the trajectory. Our second loss term is:
L2=ğ‘€Ã•
ğ‘–=1ğ»Ã•
ğ‘¡+âˆˆTğ‘–,ğ‘¡âˆ’âˆ‰Tğ‘–max 0,ğ‘¬â˜…
ğ‘–Â·ğ’—ğ‘¡+âˆ’ğ‘¬â˜…
ğ‘–Â·ğ’—ğ‘¡âˆ’+ğ‘šğ‘ğ‘Ÿğ‘”ğ‘–ğ‘›,(14)
where ğ’—ğ‘¡+,ğ’—ğ‘¡âˆ’denotes the embeddings of nodes ğ‘¡+andğ‘¡âˆ’that
are relevant and irrelevant to Tğ‘–, respectively.
Our complete pre-training objective function is formally defined:
L=L1+ğœ‡L2+ğœ‡â€²âˆ¥Î”âˆ¥2
2. (15)
ğœ‡,ğœ‡â€²are hyper-parameters and âˆ¥Î”âˆ¥2
2L2-regularizes all trainable
embeddings and variables to avoid over-fitting.
4.4.2 Sampling Strategy for Efficiency .Although C-STAR takes
variable-size trajectory inputs for representation encoding, it is
however more efficient and common to use fixed-size tensors, i.e.,
batches of trajectory feature lists, for pre-training. As the basic
requirement is: sampled trajectories should be representative and
informative, in this paper, we implement the importance-indicator
by using frequency to rank all observed trajectory elements. Then,
for an element ğ‘¡âˆˆT, based onğ‘¡â€²ğ‘ global frequency rank, its local
rankğ‘Ÿğ‘¡in its belonging trajectory can be subsequently obtained.
Letğ‘ƒğ‘Ÿ(ğ‘¡)denote the derived sampling probability, we provide the
following sampling strategies with illustrative curves in Figure 3(B):
(1)Uniform Sampling (US). The most common manner is to conduct
uniform sampling without any probability bias.
(2)Inversely Proportional Sampling (IPS). This is based on the as-
sumption that frequently-appeared elements should be assigned
with higher probability. We implement it via an inversely pro-
portional function with normalization:
ğ‘ƒğ‘Ÿ(ğ‘¡)=exp(ğ‘Ÿâˆ’1
ğ‘¡)
Ã
ğ‘¡â€²âˆˆTexp(ğ‘Ÿâˆ’1
ğ‘¡â€²). (16)
(3)Geometric Distributed Sampling (GDS). Given a hyper-parameter
ğœŒ, the probability can be assigned with:
ğ‘ƒğ‘Ÿ(ğ‘¡)=exp(ğœŒ(1âˆ’ğœŒ)ğ‘Ÿğ‘¡)Ã
ğ‘¡â€²âˆˆTexp(ğœŒ(1âˆ’ğœŒ)ğ‘Ÿğ‘¡â€²)andğœŒâˆˆ(0,1). (17)
(4)Parametrized Geometric Distributed Sampling (PGDS). A further
modification is to control the curve sharpness:
ğ‘ƒğ‘Ÿ(ğ‘¡)=exp(ğ‘’âˆ’ğ‘Ÿğ‘¡/ğœ†)Ã
ğ‘¡â€²âˆˆTexp(ğ‘’âˆ’ğ‘Ÿğ‘¡â€²/ğœ†)andğœ†âˆˆR+. (18)
With the balanced performance over evaluation tasks shown in
Â§ 5.5, we finally adopt the geometric distributed sampling strategy.
So far, we have explained all the technical parts of C-STAR. We
attach the algorithm pseudo-codes in Algorithm 1.
4.5 Complexity Analysis
We provide complexity analysis in Table 2. (1) ğ‘€andğ‘ğ‘–are the
trajectory number and the trajectory length after the sampling for
tensorization. ğµandğ¸are the batch size and epoch number. In
each epoch, it takes ğ‘‚(ğ‘ğ‘–logğ‘ğ‘–)to implement ğ‘“âˆ—for each sliced
distribution pair. Thus our Inter-SE takesğ‘‚(ğ‘†ğ¸ğ‘€ğ‘ğ‘–logğ‘ğ‘–
ğµ). (2) The
graph normalization for weighting in Eqn. (10) can be pre-processed
withinğ‘‚(2|E|), where|E|is the edge number of PR-Graph. (3)
For the graph convolutions to extract PR-Graph knowledge into
Eqn. (11), it takes ğ‘‚(2ğ¿ğ¸|E|2
ğµ)complexity in total. (4) Based on aggre-
gated embeddings, our Intra-CE module thus takes ğ‘‚(ğ‘†ğ¿ğ¸ğ‘€ğ‘ğ‘–logğ‘ğ‘–
ğµ)Algorithm 1: C-STAR Learning Algorithm.
Input: Trajectories{Gğ‘–}ğ‘€
ğ‘–=1with corresponding feature lists {Vğ‘–}ğ‘€
ğ‘–=1;
variables Î˜,Î”,ğ‘†,ğ‘,ğœ‡,ğœ‡â€²ğ¿,ğœŒ,Â·Â·Â·
1while not converge do
2 foreach trajectoryGğ‘–âˆˆ{Gğ‘–}ğ‘€
ğ‘–=1.do
3Gğ‘–â†Sampled trajectory of Gğ‘–;
4Vğ‘–â†Updated feature list associated with Gğ‘–;
5 ğ‘¬ğ‘–â†Inter-SE(Vğ‘–|Î˜)forğ‘™âˆˆ{1,2,Â·Â·Â·,ğ¿}do
6{N(ğ‘™)
ğ‘–}â†ğ‘™-th neighborhood feature list
7 ğ‘¬â€²
ğ‘–â†Encode with Intra-CE(Vğ‘–|Î˜)and{N(ğ‘™)
ğ‘–}ğ¿
ğ‘™=1
8 ğ‘¬â˜…
ğ‘–â†[ğ‘¬ğ‘–,ğ‘¬â€²
ğ‘–];
9 (Tğ‘—,Tğ‘˜)â†Retrieve trajectories from Î©ğ‘–;
10 ğ‘¬â˜…
ğ‘—,ğ‘¬â˜…
ğ‘˜â†Trajectory representations of Tğ‘—andTğ‘˜;
11ğ‘¡+,ğ‘¡âˆ’â†Retrieve relevant and irrelevant nodes;
12 ğ’—ğ‘¡+,ğ’—ğ‘¡âˆ’â†Get embeddings for ğ‘¡+,ğ‘¡âˆ’;
13L1,L2â†compute the loss terms;
14Lâ† Optimize C-STAR with regularization;
15return Pre-trained model C-STAR.
Table 2: Training time complexity.
Inter-SE Graph Norm. Graph Conv. Intra-CE Pre-training
ğ‘‚(ğ‘†ğ¸ğ‘€ğ‘ğ‘–logğ‘ğ‘–
ğµ)ğ‘‚(2|E|)ğ‘‚(2ğ¿ğ¸|E|2
ğµ)ğ‘‚(ğ‘†ğ¿ğ¸ğ‘€ğ‘ğ‘–logğ‘ğ‘–
ğµ)ğ‘‚(4ğ¸ğ‘€ğ»)
complexity. (5) Lastly, the loss computation is ğ‘‚(4ğ¸ğ‘€ğ»)whereğ»
is a small constant in Eqnâ€™s. (13) and (14) that ğ»â‰ªğ¸andğ‘€.
5 EXPERIMENTAL EVALUATION
In this section, we discuss our empirical model evaluation on large-
scale industrial data with the following research questions. Due to
the page limit, we report the experimental results of four public
benchmarks in Appendix.
â€¢RQ1 : How does our proposed C-STAR perform in both online
and offline evaluation?
â€¢RQ2 : How does different proposed modules contribute to C-
STAR performance?
â€¢RQ3 : Can we provide empirical studies to broaden the under-
standing of C-STAR?
â€¢RQ4 : How do different settings influence the model performance?
5.1 A/B Testing Results (RQ1.A)
We have conducted an A/B testing with randomized live customer
sessions, in both desktop and mobile platforms. For the control
group, we show recommendations from previously deployed sys-
tem; while for the treatment group, we show product recommenda-
tions based on C-STAR. The experiments had been running for two
weeks, where we observed positive results with +1.24% improve-
ment in product sales, +1.03% improvement in profit gain, and all
the results are statistical significance with p-value â‰¤0.05.
5.2 Offline Evaluation (RQ1.B)
5.2.1 Downstream Tasks. We first introduce three important
customer-centric tasks for our E-commerce platform.
â€¢Task 1: Customer Segmentation. The fundamental property re-
quired by customer segmentation is customer-wise similarity
measurement. Given query customers, the model seeks to retrieve
their most similar customers, based on their learned trajectory
embeddings.
 
390Shopping Trajectory Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: The statistics of three datasets.
Task 1 Task 2 Task 3
# Pre-training trajectories 100,000
# Avg. trajectory length 27.52
# PR-Graph nodes 14,695
# PR-Graph edges 416,610
# PR-Graph density 0.0386
# Fine-tuning trajectories 30,000 30,000 30,000
# Avg. trajectory length 28.45 26.96 27.47
# Evaluation trajectories 1,000,000 5,000,000 5,000,000
#Avg. trajectory length 29.22 26.52 27.52
â€¢Task 2: Shopping Trajectory Completion. Assuming customersâ€™
shopping journeys have not yet finished, this task aims to com-
plete customersâ€™ trajectories by recommending relevant yet un-
explored elements.
â€¢Task 3: Shopping Intent Identification. Predicting customersâ€™ shop-
ping intentions gives rise to several scenarios such as comple-
mentary recommendation. For example, â€œankle braces â€ are the
complements of â€œbasketball shoes â€, as they share the same intent,
i.e., â€œplaying basketballâ€. This task is formulated as matching to
labels that are the list of pre-defined shopping intents.
5.2.2 Datasets. We use customer engagements for the period of
28 days whereby the data are fully anonymized. To prevent data
leakage risk, we process them separately for different tasks with
their statistics reported in Table 3. For three downstream tasks,
we collect the datasets for fine-tuning and evaluation as follows.
For Task 1 of Customer Segmentation, about 1M data are collected
following rule-based semantic similarity. Specifically, for each given
customer trajectory, its similar trajectories are ranked by their
common associated shopping intents, which are independently
provided by our internal labeling mechanism. We then use these
ranking lists for model fine-tuning and further evaluation. For Task
2 of Shopping Trajectory Completion, we collect around 5M new
shopping trajectories and randomly hide 20% percent of trajectory
elements. For Task 3 of Shopping Intent Identification, different
from the 100K pre-training trajectories that are collected from the
click data, we merge 5M additional trajectories from the purchase
data. This is based on the intuition that, customers normally need
to gather enough information before making a desirable deal. We
then match the learned trajectory embeddings with the labeled
intents for identification.
5.2.3 Metrics. For Task 1, we treat it as the Top-K ranking task
towards candidates of similar/relevant customers. We directly use
Eqn.(13) for fine-tuning via replacing Î©ğ‘–by the ground-truth rank-
ing list. For Tasks 2 and 3, similarly, we replace the matching scores
in Eqn. (14), i.e., ğ‘¬â˜…
ğ‘–Â·ğ’—ğ‘¡+, by the scores between encoded trajectory
embedding and unexplored product categories or shopping intents.
Thus, we adopt Recall@K and NDCG@K as the evaluation metrics
for Task 2. For Task 3, we use hit ratio (denoted as HitR) and AUC
to evaluate the model classification capability.
5.2.4 Baselines. We include (1) shallow neural models (TPooling
and MLP); (2) conventional GCN models (GCN+[31], GAT+[65],
GraphSage+[19]); (3) language-based models (LSTM [ 28], Trans-
former [ 64], Graph Transformer [ 14]); (4) neural recommender mod-
els (DIN [ 79], DIEN [ 78], SURGE [ 4]) and (5) deep learning models
(DeepSets [ 74], Set Transformer [ 38], PSWE [ 46]). We abbreviatemodel names for brevity, e.g., GS+for GraphSage+. Detailed model
descriptions are introduced in Appendix B.1.
We exclude early collaborative-filtering-based methods [ 25,36,
51] and recent GCN-based recommender models[ 24,67]. The rea-
son is that these methods are transductive only for observed cus-
tomers. Note that for the large-scale setting at E-commerce plat-
forms, we therefore require a method with a good capability of
conducting inductive inference.
5.2.5 Runtime Environment Settings. All codes are based on
Python 3.8 and PyTorch 1.14.0. The experiments are run on a Linux
machine with 4 NVIDIA 3090 GPUs. For all the baselines, we follow
the officially reported hyper-parameter settings or apply a grid
search in case lack recommended settings. The embedding dimen-
sion is searched in {32 ,64,128,256}. The learning rate is tuned within
{10âˆ’4,10âˆ’3,10âˆ’2}. Coefficients ğœ‡andğœ‡â€²are tuned among{0.1,0.5,1}
and{10âˆ’5,10âˆ’4,10âˆ’3}, respectively. We initialize and optimize all
models with default normal initializer and Adam optimizer [30].
5.2.6 Overall Performance. We report the average results based
on a five-fold evaluation in Table 4, where the bold and the under-
lined represent the best- and second-best performing cases. We
color cells when Wilcoxon signed-rank tests indicate the improve-
ments are statistically significant with at least 95% confidence level.
â€¢Task 1: Customer Segmentation. As reported in Table 4, (1) assem-
bling with TPooling or MLP, graph-based implementations (i.e.,
GCN+, GAT+, and GraphSage+) perform better than these two
vanilla baselines, indicating that graph convolutional operations
can effectively extract knowledge from PR-Graph to boost model
performance. (2) Language models and neural recommender
models generally underperform state-of-the-art deep learning
models (i.e., DeepSets, Set Transformer, and PSWE) for the cus-
tomer segmentation task. One explanation is that these deep
methods organize the trajectory into a set structure, which can
well capture their collective information and thus improve their
trajectory-wise similarity measurement. (3) Our C-STAR con-
sistently outperforms the second-best model by 4.74% âˆ¼8.14%
and 2.53%âˆ¼7.88% w.r.t. Recall@K and NDCG@K. Moreover, the
Wilcoxon significance tests indicate these improvements are all
statistically significant with over 95% confidence level.
â€¢Task 2: Shopping Trajectory Completion. From Table 4, we observe:
(1) different from the under-performing situation in Task 1, lan-
guage models generally work better in Task 2, compared to some
recent deep learning models. The main reason is that, they can
well capture the semantic relations between a â€œtrajectoryâ€ and
its â€œelementsâ€, similar to the case between a â€œsentenceâ€ and its
â€œwordsâ€. (2) Specialized recommender models present the best per-
formance among all baselines; meanwhile, our proposed model
C-STAR further achieves at least 0.62% and 0.85% of statistically
significant improvements over Recall and NDCG metrics.
â€¢Task 3: Shopping Intent Identification. For this task, we pre-train
the model with browse data and use purchase data for fine-tuning
and evaluation. Our C-STAR model presents competitive perfor-
mance with positive improvement over two metrics. The fact that
these two parts of data are independently collected from different
data sources, basically substantiates our intuition in which cus-
tomer shopping intentions are correlated during browsing and
 
391KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yankai Chen, Quoc-Tuan Truong, Xin Shen, Jin L, & Irwin King
Table 4: Evaluation results of three downstream tasks.
Task 1 Task 2 Task 3
Top-5 (%) Top-20 (%) Top-50 (%) Top-100 (%) Top-5 (%) Top-20 (%) Top-50 (%) Top-100 (%) (%)
Metric Recall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG HitR AUC
TPooling 0.74 0.93 2.82 1.67 6.10 2.56 10.74 3.29 5.97 7.31 13.71 9.62 22.43 12.36 31.37 14.70 40.26 82.85
MLP 0.65 0.78 2.57 1.45 5.56 2.27 9.42 3.02 5.84 7.11 13.56 9.37 22.28 12.14 30.96 14.44 40.62 82.90
GCN+2.21 4.49 7.48 6.38 12.36 7.83 15.29 9.29 6.74 8.35 15.25 10.79 25.27 13.59 35.69 15.98 43.39 83.81
GAT+2.44 4.72 7.57 6.71 12.91 8.31 16.22 9.78 6.99 8.40 16.97 11.10 26.16 14.23 36.21 16.76 43.84 83.96
GS+2.30 4.56 7.51 6.52 12.68 7.96 15.75 9.74 6.73 8.48 16.48 11.20 26.11 14.22 36.16 16.74 42.81 83.43
LSTM 2.87 4.95 7.53 6.98 13.21 9.67 18.28 11.53 6.85 8.99 17.24 11.78 27.74 15.06 37.79 17.81 40.28 83.43
TSFM 3.08 6.51 7.72 7.52 13.50 9.98 20.25 12.49 7.26 9.11 17.47 11.85 27.66 14.98 37.97 18.15 39.66 82.77
G-TSFM 3.10 6.39 7.89 7.64 13.56 10.04 20.19 12.53 6.81 8.69 16.53 11.18 26.52 14.28 36.84 17.11 43.78 84.29
DIN 2.92 6.62 7.93 7.90 13.67 10.31 20.62 12.68 7.52 9.13 19.05 12.77 28.84 15.83 38.08 18.21 43.45 83.52
DIEN 2.85 6.31 7.87 7.82 13.84 10.40 20.76 12.85 7.88 9.21 19.18 12.95 29.04 16.24 38.73 18.60 43.36 83.41
SURGE 2.96 6.57 8.13 7.77 13.96 10.43 20.74 12.96 7.93 9.23 19.29 13.08 29.13 16.31 38.67 18.72 44.47 83.76
DeepSets 3.13 6.62 7.83 7.59 13.95 10.27 20.67 12.74 6.29 7.60 15.01 10.21 24.96 13.32 35.12 15.97 35.13 82.24
S-TSFM 2.98 6.54 7.84 7.63 14.11 10.34 20.81 12.96 6.42 7.84 14.39 10.20 23.92 12.86 33.17 15.33 38.86 83.28
PSWE 3.18 6.85 8.35 8.13 14.59 10.83 21.73 13.44 7.87 9.27 17.87 12.26 28.09 15.23 38.05 18.13 44.25 84.17
C-STAR 3.41 7.39 9.03 8.42 15.63 11.20 22.76 13.78 8.28 9.64 19.78 13.67 29.61 16.79 38.97 18.88 45.28 84.32
Gain 7.23% 7.88% 8.14% 3.57% 7.13% 3.42% 4.74% 2.53% 4.41% 3.99% 2.54% 4.51% 1.65% 2.94% 0.62% 0.85% 1.82% 0.04%
Table 5: Results of ablation study.
Task 1 Task 2 Task 3
Recall NDCG Recall NDCG HitR AUC
w/o PT 14.94(-34.4%) 9.56(-30.6%) 28.45(-27.0%) 13.34(-29.3%) 38.43(-15.1%) 79.30(-6.0%)
w/o KE 21.21( -6.8%) 12.73(-7.6%) 33.14(-15.0%) 16.24(-14.0%) 43.58(-3.8%) 83.84(-0.6%)
w/oInter-SE 17.53(-23.0%) 10.45(-24.2%) 33.85(-13.1%) 16.45(-12.9%) 42.94(-5.2%) 83.20(-1.3%)
w/oIntra-CE 20.42(-10.3%) 12.04(-12.6%) 35.63(-8.6%) 17.23(-8.7%) 43.69(-3.5%) 83.76(-0.7%)
Best 22.76 13.78 38.97 18.88 45.28 84.32
purchasing; more importantly, this provides the functionality of
knowledge transfer from rich click data to the other, as purchase
data is usually sparse and hard to directly mine knowledge from.
5.3 Ablation Study (RQ2)
We systematically study the effectiveness of each proposed module:
(1)Trajectory Representation Learning with Pre-training. Let the
variant w/o PT skips the pre-training stage and starts from ini-
tializing trainable embeddings with normal initializer. As shown
in Table 5, w/o PT exhibits a conspicuous performance decay
across three tasks. This demonstrates the effect of our designs to
improve the embedding quality by explicitly capturing the tra-
jectory overlapping elements and trajectory-element relations
as the supervision signals within the pre-training data.
(2)PR-Graph Necessity for Knowledge Extraction. Variant w/o KE
omits the graph convolutions in PR-Graph by removing ğ‘¬â€²
ğ‘–in
ğ‘¬â˜…
ğ‘–=[ğ‘¬ğ‘–,ğ‘¬â€²
ğ‘–]and expanding the dimensionality of ğ‘¬ğ‘–to2ğ‘ğ‘†
for fair comparison. As shown in Table 5, the performance
decay of w/o KE not only indicates the informativeness of PR-
Graph in organizing multiple product-to-product relations at
the category-level, but also the usefulness of graph convolutions
for knowledge extraction.
(3)Impact of Inter-SE Module. Variant w/oInter-SE replaces the
algorithmic implementation of Eqn. (9) by a two-layer of MLP
to encode trajectory representations. The performance gaps
of three tasks between w/oInter-SE and C-STAR prove the
effectiveness of our proposed solution, in which we convert the
measurement of trajectory-wise similarity to the distribution
distance with the Optimal Transport methodology.
(4)Impact of Intra-CE Module. We directly disable the Intra-CE
module in Eqn. (12) and average the aggregated neighborhood
Candidate customersCandidate customers
Candidate customersCandidate customers

(1) make laundry easier;(2) make brunch;(3) remove the nut;(4) replace the PCV valve;(5) remove nails;(6) wear to bowling alleys;(7) rope lights;(8) use as subwoofer kits;(9) prevent cavities;(10) function as jewelry pliers. 
Embedding visualizationShopping intentsFigure 4: Illustration of case studies (best view in color).
Table 6: Trajectories with the same elements highlighted.
Customer ID Shopping Trajectory (with anonymized index)
Query Customer 10041, 10048, 9706, 9881, 9965, 10133, 6487
Customer 1 6600, 9706, 9965, 10133, 9887, 13402
Customer 2 3443, 5259, 9965, 10635, 2365, 7609
embeddings in Eqn. (11) for ğ‘¬â€²
ğ‘–inğ‘¬â˜…
ğ‘–=[ğ‘¬ğ‘–,ğ‘¬â€²
ğ‘–]. The notable per-
formance degradation validates our early assumption that the
conventional graph convolutions do not have a certain mecha-
nism to maintain both the similarity measurement and semantic
enrichment, which may lead to the disruption of learning bal-
anced representations, especially for Tasks 1 and 2.
5.4 Case Study (RQ3)
(1)Embedding Distance for Similarity Measurement. C-STAR uses
the Euclidean distance for ranking similar customers. To visual-
ize this process, we randomly select 10 trajectories and present
their mutual distances from their fine-tuned embeddings. As
shown in the left-most heatmap of Figure 4, with lighter-colored
cells representing smaller distances after fine-tuning, the green
block indicates these customers are likely similar in real-world
space. In addition, given a query customer, we retrieve his/her
two most similar customers (ranked by their distances) in Ta-
ble 6. We observe that, similarity measurement using Euclidean
distances with fine-tuned embeddings presents reasonable align-
ment in terms of overlapping trajectory structures, providing
the explainability for our pre-training design.
(2)Embedding Visualization with Shopping Intents. We randomly
pick 10 intents and use them to retrieve 10,000 customer tra-
jectories that predict these 10 intents as their respective major
 
392Shopping Trajectory Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 7: Results of different module designs.
Task 1 Task 2 Task 3
Type Recall NDCG Recall NDCG AUC HitR
US 22.68 13.73 38.43 18.35 44.95 83.95
IPS 22.57 13.64 38.14 17.95 45.40 84.12
GDS 22.76 13.78 38.97 18.88 45.28 84.32
PGDS 22.95 13.83 38.59 18.35 45.12 84.19
Non-attention-based22.76 13.78 38.97 18.88 45.28 84.32
Average pre-training time cost/Epoch: 233 s
Attention-based23.13 13.94 39.69 19.20 45.22 84.28
Average pre-training time cost/Epoch: 815 s
intents (i.e., with the highest prediction score from Task 3). We
project the learned embeddings into a 2D-space with t-SNE [ 63].
Each projected vector is discriminatively colored with his/her
major shopping intent. As shown in the right-hand-side two
figures of Figure 4, these clustered trajectories generally indi-
cate the same major intents, demonstrating that the trajectory
embeddings are indeed effective for intent identification, as they
are geometrically close in the embedding space.
5.5 Analysis on Model Settings (RQ4)
Sampling Strategy for Effective Training. We report the Top-
100 results of all sampling strategies in Table 7. As we can observe,
although these strategies produce different final performances over
different tasks, the geometric distributed sampling strategy gener-
ally presents a balanced performance across all tasks.
Selection of Graph Convolution Paradigm. We experiment with
attention-based graph convolutions [ 65] on PR-Graph. The results
are generally better than the non-attention-based version; however,
we notice both the computation time cost and memory footprint
of attention-based version are much larger, e.g., nearly four times
slower in pre-training. Therefore, in C-STAR, we employ the non-
attention-based implementation. We leave the several advanced
efficiency optimizations [ 8,20,47â€“49,72,76] for future exploration.
Settings of ğ‘andğ‘†.As demonstrated in Figure 5 for Task 1,
Recall of altering SNDCG of altering SRecall of altering NNDCG of altering N(N,S):(N,S)
Figure 5: Altering (N, S).compared to ğ‘, al-
teringğ‘†is more
influential to the
performance, as in-
creasingğ‘†produces
a more accurate
cumulative approx-
imation. However,
this also increases
the computational
cost. Thus,(32,64)
for(ğ‘,ğ‘†)setting is the balanced spot with positive momentum that
presents a practical trade-off between the model performance and
resource consumption.
6 CONCLUSION AND FUTURE WORK
We present C-STAR, a novel framework for learning informative
representations of customer shopping trajectories. The proposed
methodology jointly models two critical aspects: inter-trajectory
distribution similarity and intra-trajectory semantic correlation.
This coarse-to-fine learning paradigm enriches trajectory represen-
tations by leveraging both global feature distribution knowledgeand local product relationships within individual trajectories. Fur-
thermore, we specifically design two pre-training objectives to
enhance the quality of the learned embeddings. The empirical re-
sults on both industrial and public datasets not only illustrate the
usefulness of learned embeddings over multiple customer-centric
tasks, but also justify the effectiveness of all proposed modules.
As for future work, we plan to investigate two major directions.
Firstly, we aim to enhance our model by incorporating multimodal
information [39,43,60] to enrich the object features, where the
difficulty is to propose effective knowlegde fusion methodology.
Secondly, considering that trajectory data undergoes continuous
evolution, it is worth exploring streaming methods through Contin-
ual Learning [21,75] instead of re-training the model. This approach
enables us to efficiently capture emerging patterns while retain-
ing the knowledge acquired by the initial model, which proves
particularly efficacious in large-scale settings.
7 ACKNOWLEDGMENTS
The research presented in this paper was partially supported by the
Research Grants Council of the Hong Kong Special Administrative
Region, China (CUHK 14222922, RGC GRF 2151185).
REFERENCES
[1]Martin Arjovsky, Soumith Chintala, and LÃ©on Bottou. 2017. Wasserstein genera-
tive adversarial networks. In ICML. PMLR, 214â€“223.
[2]Nicolas Bonneel, Julien Rabin, Gabriel PeyrÃ©, and Hanspeter Pfister. 2015. Sliced
and radon wasserstein barycenters of measures. JMIV 51, 1 (2015), 22â€“45.
[3]Mathieu Carriere, Marco Cuturi, and Steve Oudot. 2017. Sliced Wasserstein
kernel for persistence diagrams. In ICML. PMLR, 664â€“673.
[4]Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng
Jin, and Yong Li. 2021. Sequential Recommendation with Graph Neural Networks.
InSIGIR. 378â€“387.
[5]Xiong-Hui Chen, Bowei He, Yang Yu, Qingyang Li, Zhiwei Qin, Wenjie Shang,
Jieping Ye, and Chen Ma. 2023. Sim2rec: A simulator-based decision-making
approach to optimize real-world long-term user engagement in sequential rec-
ommender systems. In ICDE. IEEE, 3389â€“3402.
[6]Yankai Chen, Yixiang Fang, Qiongyan Wang, Xin Cao, and Irwin King. 2024. Deep
Structural Knowledge Exploitation and Synergy for Estimating Node Importance
Value on Heterogeneous Information Networks. In AAAI. 8302â€“8310.
[7]Yankai Chen, Tuan Truong, Xin Shen, Ming Wang, Jin Li, Jim Chan, and Ir-
win King. 2023. Topological representation learning for e-commerce shopping
behaviors. (2023).
[8]Yankai Chen, Yifei Zhang, Huifeng Guo, Ruiming Tang, and Irwin King. 2022.
An Effective Post-training Embedding Binarization Approach for Fast Online
Top-K Passage Matching. In AACL. 102â€“108.
[9]Yankai Chen, Yifei Zhang, Menglin Yang, Zixing Song, Chen Ma, and Irwin
King. 2023. WSFE: Wasserstein Sub-graph Feature Encoder for Effective User
Segmentation in Collaborative Filtering. In SIGIR. 2521â€“2525.
[10] Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility:
user movement in location-based social networks. In SIGKDD. 1082â€“1090.
[11] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for
YouTube Recommendations. In RecSys. 191â€“198.
[12] Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation of optimal
transport. NeurIPS 26 (2013).
[13] Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi
Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. 2019. Max-
sliced wasserstein distance and its use for gans. In CVPR. 10648â€“10656.
[14] Vijay Prakash Dwivedi and Xavier Bresson. 2020. A generalization of transformer
networks to graphs. arXiv preprint arXiv:2012.09699 (2020).
[15] Dominik Maria Endres and Johannes E Schindelin. 2003. A new metric for
probability distributions. IEEE TIT 49, 7 (2003), 1858â€“1860.
[16] Farzad Eskandanian, Bamshad Mobasher, and Robin D Burke. 2016. User Seg-
mentation for Controlling Recommendation Diversity.. In RecSys.
[17] Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. 2015. Learning
image and user features for recommendation in social networks. In ICCV.
[18] Miguel Alves Gomes, Richard Meyes, Philipp Meisen, and Tobias Meisen. 2022.
Will This Online Shopping Session Succeed? Predicting Customerâ€™s Purchase
Intention Using Embeddings. In CIKM. 2873â€“2882.
 
393KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yankai Chen, Quoc-Tuan Truong, Xin Shen, Jin L, & Irwin King
[19] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In NeurIPS. 1025â€“1035.
[20] Bowei He, Xu He, Renrui Zhang, Yingxue Zhang, Ruiming Tang, and Chen Ma.
2023. Dynamic Embedding Size Search with Minimum Regret for Streaming
Recommender System. In CIKM. 741â€“750.
[21] Bowei He, Xu He, Yingxue Zhang, Ruiming Tang, and Chen Ma. 2023. Dy-
namically expandable graph convolution for streaming recommendation. In
Proceedings oftheACM Web Conference 2023. 1457â€“1467.
[22] Bowei He, Yunpeng Weng, Xing Tang, Ziqiang Cui, Zexu Sun, Liang Chen,
Xiuqiang He, and Chen Ma. 2024. Rankability-enhanced Revenue Uplift Modeling
Framework for Online Marketing. arXiv preprint arXiv:2405.15301 (2024).
[23] Ruining He and Julian McAuley. 2016. Modeling the visual evolution of fashion
trends with one-class collaborative filtering. In WWW. 507â€“517.
[24] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In SIGIR. 639â€“648.
[25] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In WWW. 173â€“182.
[26] Ernst Hellinger. 1909. Neue begrÃ¼ndung der theorie quadratischer formen von un-
endlichvielen verÃ¤nderlichen. Journal fÃ¼rdiereine undangewandte Mathematik
1909, 136 (1909), 210â€“271.
[27] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2016. Session-based recommendations with recurrent neural networks. In ICLR .
[28] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735â€“1780.
[29] Leonid V Kantorovich. 1960. Mathematical methods of organizing and planning
production. Management science 6, 4 (1960), 366â€“422.
[30] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-
mization. In ICLR.
[31] Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[32] Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo
Rohde. 2019. Generalized sliced wasserstein distances. NeurIPS 32 (2019).
[33] Soheil Kolouri, Phillip E Pope, Charles E Martin, and Gustavo K Rohde. 2018.
Sliced Wasserstein auto-encoders. In ICLR.
[34] Soheil Kolouri, Akif B Tosun, John A Ozolek, and Gustavo K Rohde. 2016. A
continuous linear optimal transport approach for pattern analysis in image
datasets. Pattern recognition 51 (2016), 453â€“462.
[35] Soheil Kolouri, Yang Zou, and Gustavo K Rohde. 2016. Sliced Wasserstein kernels
for probability distributions. In CVPR. 5258â€“5267.
[36] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer 42, 8 (2009), 30â€“37.
[37] Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency.
Theannals ofmathematical statistics 22, 1 (1951), 79â€“86.
[38] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and
Yee Whye Teh. 2019. Set transformer: A framework for attention-based
permutation-invariant neural networks. In ICML. PMLR, 3744â€“3753.
[39] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian
McAuley. 2023. Text is all you need: Learning language representations for
sequential recommendation. In SIGKDD. 1258â€“1267.
[40] Langzhang Liang, Xiangjing Hu, Zenglin Xu, Zixing Song, and Irwin King. 2023.
Predicting Global Label Relationship Matrix for Graph Neural Networks under
Heterophily. In NeurIPS.
[41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv:1907.11692 (2019).
[42] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-
Robert StÃ¶ter. 2019. Sliced-Wasserstein flows: Nonparametric generative model-
ing via optimal transport and diffusions. In ICML. PMLR, 4104â€“4113.
[43] Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou, Zongpeng Li,
Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2023. RecRanker: Instruction
Tuning Large Language Model as Ranker for Top-k Recommendation. arXiv
preprint arXiv:2312.16018 (2023).
[44] Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu, and Irwin King. 2011.
Recommender systems with social regularization. In WSDM. 287â€“296.
[45] Ziqiao Meng, Peilin Zhao, Yang Yu, and Irwin King. 2023. Doubly stochastic
graph-based non-autoregressive reaction prediction. In IJCAI. 4064â€“4072.
[46] Navid Naderializadeh, Joseph F Comer, Reed Andrews, Heiko Hoffmann, and
Soheil Kolouri. 2021. Pooling by sliced-Wasserstein embedding. NeurIPS 34
(2021), 3389â€“3400.
[47] Zexuan Qiu, Jiahong Liu, Yankai Chen, and Irwin King. 2024. HiHPQ: Hierarchical
Hyperbolic Product Quantization for Unsupervised Image Retrieval. In AAAI .
4614â€“4622.
[48] Zexuan Qiu, Qinliang Su, Zijing Ou, Jianxing Yu, and Changyou Chen. 2021.
Unsupervised Hashing with Contrastive Information Bottleneck. In IJCAI.
[49] Zexuan Qiu, Qinliang Su, Jianxing Yu, and Shijing Si. 2022. Efficient Docu-
ment Retrieval by End-to-End Refining and Quantizing BERT Embedding with
Contrastive Product Quantization. In EMNLP. 853â€“863.[50] Julien Rabin, Gabriel PeyrÃ©, Julie Delon, and Marc Bernot. 2011. Wasserstein
barycenter and its application to texture mixing. In SSVM. Springer, 435â€“446.
[51] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv (2012).
[52] Xin Shen, Kyungdon Joo, and Jean Oh. 2023. FishRecGAN: An End to End GAN
Based Network for Fisheye Rectification and Calibration. Adv. Artif. Intell. Mach.
Learn. 3, 2 (2023), 1180â€“1197.
[53] Xin Shen, Yan Zhao, Sujan Perera, Yujia Liu, Jinyun Yan, and Mitchell Goodman.
2023. Learning Personalized Page Content Ranking Using Customer Representa-
tion. arXiv preprint arXiv:2305.05267 (2023).
[54] Justin Solomon, Fernando De Goes, Gabriel PeyrÃ©, Marco Cuturi, Adrian Butscher,
Andy Nguyen, Tao Du, and Leonidas Guibas. 2015. Convolutional wasser-
stein distances: Efficient optimal transportation on geometric domains. ACM
Transactions onGraphics (ToG) 34, 4 (2015), 1â€“11.
[55] Zixing Song, Ziqiao Meng, and Irwin King. 2024. A Diffusion-Based Pre-training
Framework for Crystal Property Prediction. In AAAI, Vol. 38. 8993â€“9001.
[56] Zixing Song, Yifei Zhang, and Irwin King. 2023. Optimal Block-wise Asymmetric
Graph Construction for Graph-based Semi-supervised Learning. In NeurPS.
[57] Zixing Song, Yuji Zhang, and Irwin King. 2023. Towards fair financial services for
all: A temporal GNN approach for individual fairness on transaction networks.
InCIKM. 2331â€“2341.
[58] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. 2018.
Wasserstein auto-encoders. ICLR (2018).
[59] Quoc-Tuan Truong and Hady Lauw. 2019. Multimodal review generation for
recommender systems. In TheWorld Wide Web Conference. 1864â€“1874.
[60] Quoc-Tuan Truong, Aghiles Salah, and Hady Lauw. 2021. Multi-modal recom-
mender systems: Hands-on exploration. In RecSys. 834â€“837.
[61] Quoc-Tuan Truong, Aghiles Salah, and Hady W Lauw. 2021. Bilateral variational
autoencoder for collaborative filtering. In WSDM. 292â€“300.
[62] Quoc-Tuan Truong, Tong Zhao, Changhe Yuan, Jin Li, Jim Chan, Soo-Min Pantel,
and Hady W Lauw. 2022. AmpSum: Adaptive Multiple-Product Summarization
towards Improving Recommendation Captions. In WebConf. 2978â€“2988.
[63] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
JMLR 9, 11 (2008).
[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. NeurIPS 30 (2017).
[65] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. ICLR.
[66] CÃ©dric Villani. 2009. Optimal transport: oldandnew. Vol. 338. Springer.
[67] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In SIGIR. 165â€“174.
[68] Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua.
2020. Disentangled graph collaborative filtering. In SIGIR. 1001â€“1010.
[69] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful
are graph neural networks? ICLR (2019).
[70] Menglin Yang, Zhihao Li, Min Zhou, Jiahong Liu, and Irwin King. 2022. Hicf:
Hyperbolic informative collaborative filtering. In SIGKDD. 2212â€“2221.
[71] Menglin Yang, Min Zhou, Jiahong Liu, Defu Lian, and Irwin King. 2022. HRCF:
Enhancing collaborative filtering via hyperbolic geometric regularization. In
Proceedings oftheACM Web Conference 2022. 2462â€“2471.
[72] Menglin Yang, Min Zhou, Rex Ying, Yankai Chen, and Irwin King. 2023. Hyper-
bolic Representation Learning: Revisiting and Advancing. ICML (2023).
[73] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In SIGKDD. 974â€“983.
[74] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R
Salakhutdinov, and Alexander J Smola. 2017. Deep sets. NeurIPS 30 (2017).
[75] Xinni Zhang, Yankai Chen, Chenhao Ma, Yixiang Fang, and Irwin King. 2024.
Influential Exemplar Replay for Incremental Learning in Recommender Systems.
InAAAI, Vol. 38. 9368â€“9376.
[76] Yifei Zhang, Yankai Chen, Zixing Song, and Irwin King. 2023. Contrastive Cross-
scale Graph Knowledge Synergy. In SIGKDD.
[77] Yifei Zhang, Hao Zhu, Yankai Chen, Zixing Song, Piotr Koniusz, Irwin King,
et al.2023. Mitigating the popularity bias of graph collaborative filtering: A
dimensional collapse perspective. NeurIPS 36 (2023), 67533â€“67550.
[78] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction. In AAAI. 5941â€“5948.
[79] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In KDD. 1059â€“1068.
[80] Yuchen Zhuang, Xin Shen, Yan Zhao, Chaosheng Dong, Ming Wang, Jin Li, and
Chao Zhang. 2023. G-STO: Sequential main shopping intention detection via
graph-regularized stochastic transformer. In CIKM. 3677â€“3687.
[81] Cai-Nicolas Ziegler, Sean M McNee, Joseph A Konstan, and Georg Lausen. 2005.
Improving recommendation lists through topic diversification. In WWW . 22â€“32.
 
394Shopping Trajectory Representation Learning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A THEORETICAL PROOFS
Proposition 1. ğ‘“âˆ—from reference slice ğ‘ƒğœ½
0to the input distribution
sliceğ‘ƒğœ½
ğ‘–is formulated as:
ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–):=ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘– ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½), ğ‘¥ğœ½âˆˆVğœ½
0. (1)
Proof. The transport cost, i.e., its derived distance, as one can-
didate transport map for Eqn. (1), can be computed as:
Distance =âˆ«
Râˆ¥ğ‘¥ğœƒâˆ’ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–)âˆ¥ğ‘ğ‘‘ğ‘ƒğœ½
0(ğ‘¥ğœ½)1
ğ‘
=âˆ«
Râˆ¥ğ‘¥ğœ½âˆ’ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘– ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½)âˆ¥ğ‘ğ‘‘ğ‘ƒğœ½
0(ğ‘¥ğœ½)1
ğ‘
=âˆ«1
0âˆ¥ğ¹âˆ’1
ğ‘ƒğœ½
0(ğ‘Ÿ)âˆ’ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘–(ğ‘Ÿ)âˆ¥ğ‘ğ‘‘ğ‘Ÿ1
ğ‘=ğ‘Šğ‘(ğ‘ƒğœ½
0,ğ‘ƒğœ½
ğ‘–).(2)
This proves to be the optimal distance for these two slices. â–¡
Proposition 2. Ifğ‘=ğ‘ğ‘–,âˆ€ğ‘¥ğœ½âˆˆVğœ½
0, the optimal plan in Eqn. (4)
functions as the mapping between Vğœ½
0andVğœ½
ğ‘–:
ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–)=argminğ‘¥â€²âˆˆVğœ½
ğ‘–
ğœ(ğ‘¥â€²|Vğœ½
ğ‘–)=ğœ(ğ‘¥ğœ½|Vğœ½
0)
.(3)
Proof. Based on their empirical distributions:
ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½)=1
ğ‘Ã•ğ‘
ğ‘›=1ğ›¿(ğ‘¥ğœ½âˆ’ğœ½TÂ·ğ’—ğ‘¡0ğ‘›)
ğ¹ğ‘ƒğœ½
ğ‘–(ğ‘¥ğœ½)=1
ğ‘ğ‘–Ã•ğ‘ğ‘–
ğ‘›=1ğ›¿(ğ‘¥ğœ½âˆ’ğœ½TÂ·ğ’—ğ‘¡ğ‘–ğ‘›),(4)
It is straightforward to find that they are monotonically increasing.
Ifğ‘=ğ‘ğ‘–, we can firstly modify the original form of the optimal
transport map ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘– ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½)to:
ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–)=argminğ‘¥â€²âˆˆVğœ½
ğ‘– ğ¹ğ‘ƒğœ½
ğ‘–(ğ‘¥â€²)=ğ‘Ÿwhereğ‘Ÿ=ğ¹ğ‘ƒğœ½
0(ğ‘¥ğœ½).
(5)
Letğœ(ğ‘¥ğœ½|Vğœ½
0)denote the ranking of each input ğ‘¥ğœ½in the ascending
sorting ofVğœ½
0, and then we can quantitatively replace the term
ğ¹ğ‘ƒğœ½
ğ‘–(Â·)and completes the proof.
ğ‘“âˆ—(ğ‘¥ğœ½|Vğœ½
ğ‘–)=argminğ‘¥â€²âˆˆVğœ½
ğ‘–
ğœ(ğ‘¥â€²|Vğœ½
ğ‘–)=ğœ(ğ‘¥ğœ½|Vğœ½
0)
ifğ‘=ğ‘ğ‘–.
(6)
â–¡
Theorem 1 ( ğ¿ğ‘Norm Distance Approximation ).For any
two input trajectories with corresponding distributions ğ‘ƒğ‘–andğ‘ƒğ‘—,
their encoded representations ğ‘¬ğ‘–andğ‘¬ğ‘—hold that: (1)âˆ¥ğ‘¬ğ‘–âˆ’ğ‘¬ğ‘—âˆ¥ğ‘â‰ˆ
ğ‘†ğ‘Šğ‘(ğ‘ƒğ‘–,ğ‘ƒğ‘—)and (2)âˆ¥ğ‘¬ğ‘–âˆ¥ğ‘â‰ˆğ‘†ğ‘Šğ‘(ğ‘ƒğ‘–,ğ‘ƒ0).
Proof. (1) Forâˆ¥ğ‘¬ğ‘–âˆ’ğ‘¬ğ‘—âˆ¥ğ‘, we have:
âˆ¥ğ‘¬ğ‘–âˆ’ğ‘¬ğ‘—âˆ¥ğ‘
=ğ‘†
ğ‘ =1(ğ‘¬ğœ½ğ‘ 
ğ‘–âˆ’ğ‘¬ğœ½ğ‘ 
ğ‘—)ğ‘
=Ã•ğ‘†
ğ‘ =1(ğ‘¬ğœ½ğ‘ 
ğ‘–âˆ’ğ‘¬ğœ½ğ‘ 
ğ‘—)ğ‘ğ‘
=1
ğ‘†Ã•ğ‘†
ğ‘ =11
ğ‘Ã•ğ‘
ğ‘›=1
ğ‘“âˆ—(ğœ½T
ğ‘ ğ’—0
ğ‘›|Vğœ½ğ‘ 
ğ‘–)âˆ’ğ‘“âˆ—(ğœ½T
ğ‘ ğ’—0
ğ‘›|Vğœ½ğ‘ 
ğ‘—)ğ‘ğ‘
(7)where its continuous form is:
â‰ˆâˆ«
Sğ‘‘âˆ’1âˆ«
R
ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘– ğ¹ğ‘ƒğœ½
0(ğ‘¡)âˆ’ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘— ğ¹ğ‘ƒğœ½
0(ğ‘¡)ğ‘
ğ‘‘ğ‘ƒğœ½
0(ğ‘¡)ğ‘‘ğœ½ğ‘
=âˆ«
Sğ‘‘âˆ’1âˆ«1
0 ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘–(ğ‘Ÿ)âˆ’ğ¹âˆ’1
ğ‘ƒğœ½
ğ‘—(ğ‘Ÿ)ğ‘ğ‘‘ğ‘Ÿğ‘‘ğœ½ğ‘
=âˆ«
Sğ‘‘âˆ’1ğ‘Šğ‘(ğ‘ƒğœ½
ğ‘–,ğ‘ƒğœ½
ğ‘—)ğ‘ğ‘‘ğœ½ğ‘
=ğ‘†ğ‘Šğ‘(ğ‘ƒğ‘–,ğ‘ƒğ‘—).(8)
(2) For the reference ğ‘ƒ0, its encoded representation is:
ğ‘¬0:=1
ğ‘†ğ‘ğ‘†
ğ‘ =1ğ‘
ğ‘›=1
ğ‘“âˆ—(ğœ½T
ğ‘ ğ’—0
ğ‘›|Vğœ½ğ‘ 
0)
âˆ’ğ‘¶
=1
ğ‘†ğ‘ğ‘†
ğ‘ =1ğ‘
ğ‘›=1 ğœ½T
ğ‘ ğ’—0
ğ‘›âˆ’ğ‘¶=0.(9)
Thus we have:
âˆ¥ğ‘¬ğ‘–âˆ¥ğ‘=âˆ¥ğ‘¬ğ‘–âˆ’ğ‘¬0âˆ¥ğ‘â‰ˆğ‘†ğ‘Šğ‘(ğ‘ƒğ‘–,ğ‘ƒ0), (10)
which completes the proof. â–¡
B EXPERIMENTAL SETUPS
B.1 Baseline Descriptions
â€¢TPooling is a straightforward implementation that aggregates
all element embeddings of each customer trajectory. The pool-
ing strategy could be mean, max, or min. We report the best
performance of these strategies and denote it as TPooling, if no
confusion is caused.
â€¢MLP is a fundamental neural network that first concatenates
trajectory element embeddings as input and passes them through
one hidden layer and finally arrives at the output layer.
â€¢GCN [31] is one of the classic graph convolutional networks.
We implement it on PR-Graph to gather information and fur-
ther aggregate trajectory-level embeddings via TPooling (i.e.,
GCN+TPooling ) or MLP (i.e., GCN+MLP ). We use the notation
GCN+to denote the one with better metrics (e.g., on Recall@K
and NDCG@K in ranking tasks).
â€¢GAT [65] is the representative graph-based model with the at-
tention mechanism. Similarly, we implement it for PR-Graph in-
formation propagation and summarize trajectory embeddings
with two variant models (i.e., GAT+TPooling andGAT+MLP ).
Similarly, GAT+denotes the better variant.
â€¢GraphSage [19] is the graph convolutional network with the in-
ductive learning setting. Similarly, we have two implementations
with TPooling and MLP, and GraphSage+is the better one.
â€¢LSTM [28] is a competitive baseline that directly takes trajectory
data as sequential input to learn the holistic target representation.
â€¢Transformer [64] is another strong baseline with the self-attention
mechanism. In our implementation, we input each customer tra-
jectory as a language sentence to learn its embedding.
â€¢Graph Transformer [14] is one of state-of-the-art Transformer-
based model that deploys on the graph data. We implement it on
PR-Graphand trajectory data to jointly learn the representations.
â€¢DIN [79] is a classical recommender model that attentively learns
customer embeddings by aggregating the history interaction
within the trajectories.
 
395KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yankai Chen, Quoc-Tuan Truong, Xin Shen, Jin L, & Irwin King
Figure 1: Recall@K and NDCG@K are respectively reported in the first and second row (best view in color).
Table 1: Public dataset statistics.
BCrossing Gowalla Pinterest AMZ-Book
#User trajectories 16,411 29,858 55,186 52,643
# Items 36,143 40,919 9,855 91,576
#Avg. trajectory length 35.711 49.272 26.516 56.686
â€¢DIEN [78] is a representative recommender model that encodes
customer representations by a two-layer GRU for sequential
recommendation.
â€¢SURGE [4] is a state-of-the-art recommender model that utilizes
GCN learning ability to capture item-item relations for next-item
sequential recommendation.
â€¢DeepSets [74] is an exemplary deep learning model that is orig-
inally proposed to learn representations for â€œ compound objects â€
such as point clouds. In our experiments, it takes all trajectory
units as a set and learns the unified representation while main-
taining the intrinsic semantics of trajectory elements.
â€¢Set Transformer [38] is a state-of-the-art model that integrates
self-attention mechanism [ 64] while lowering computation com-
plexity. We adapt it to encode the trajectory embeddings.
â€¢PSWE [46] is the latest state-of-the-art method that subsumes
the learning process under the Wasserstein metric framework.
In this work, we reproduce it to learn the trajectory embeddings.
C SUPPLEMENTARY EXPERIMENTS
C.1 Evaluation on Public Benchmarks
Public Datasets. We collect four public datasets that are widely
evaluated for E-commerce recommendation [ 10,17,24,67,68,81].
For these datasets, we synthesize their own â€œPR-Graphâ€ by creating
edges if items are co-purchased by over 20 different customers. Dataset
statistics are reported in Table 1 with descriptions as follows.
â€¢BCrossing5[81] is a public dataset of book ratings in Book-
Crossing Community. To guarantee the dataset quality, we filter
out readers and books with less than five interactions and then
merge each readerâ€™s rated books into a unique trajectory.
â€¢Gowalla6[67] is the check-in dataset [ 10] from Gowalla, where
users share their locations by check-in. We directly use the
dataset split by [ 67] where users and items are selected to have
at least then interactions. We integrate each customerâ€™s check-in
locations into his/her trajectory.
5https://www.kaggle.com/datasets/ruchi798/bookcrossing-dataset
6https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/gowallaâ€¢Pinterest7[17] is an implicit feedback dataset for image rec-
ommendation [ 17]. Each user is associated with his/her own
trajectory towards 9,855 different images.
â€¢AMZ-Book8is the book review dataset between readers and
book trajectories, organized from the book collection of Amazon-
review [ 23]. We directly use the existing data split from [ 67],
where each reader and book have at least ten interactions.
Evaluation Results. For public datasets, we proceed to the tasks of
customer segmentation (CS) and shopping trajectory completion (TC)
for illustration. We select language models (Transfomer and Graph
Transformer), specialized recommender models (DIEN and SURGE),
and deep learning models (DeepSet, Set Transformer, and PSWE)
that show good performance in previous sections as competing
methods. We have two major observations as follows:
â€¢For the task of customer segmentation, due to their absence of
ground-truth similar customers, we construct the similar cus-
tomer data based on the number of overlapping trajectory ele-
ments and thus skip the pre-training phase. We split the data with
8:2 ratio for training and evaluation. As shown in Figure 1(a)-(d),
C-STAR presents consistent performance superiority on these
public datasets. Compared to other methods, this indicates the
effectiveness of our proposed mechanism in capturing trajectory
structural similarity.
â€¢For the second task, similar to the previous one, we directly train
the model with 80% of all trajectories without pre-training. From
Figure 1(e)-(h), C-STAR performs competitively among all com-
parative models, except on Gowalla dataset where it is the second
best. It reassures the effectiveness of our C-STAR framework for
the task of shopping trajectory completion on public datasets.
7https://sites.google.com/site/xueatalphabeta/dataset-1/pinterest_iccv
8https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/amazon-book
 
396