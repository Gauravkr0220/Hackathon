Efficient Decision Rule List Learning via Unified Sequence
Submodular Optimization
Linxiao Yang
linxiao.ylx@alibaba-inc.com
DAMO Academy, Alibaba Group
Hangzhou, Zhejiang, ChinaJingbang Yang
jingbang.yjb@alibaba-inc.com
DAMO Academy, Alibaba Group
Hangzhou, Zhejiang, ChinaLiang Sun
liang.sun@alibaba-inc.com
DAMO Academy, Alibaba Group
Hangzhou, Zhejiang, China
ABSTRACT
Interpretable models are crucial in many high-stakes decision-
making applications. In this paper, we focus on learning a decision
rule list for binary and multi-class classification. Different from
rule set learning problems, learning an optimal rule list involves
not only learning a set of rules, but also their orders. In addition,
many existing algorithms rely on rule pre-mining to handle large-
scale high-dimensional data, which leads to suboptimal rule list
model and degrades its generalization accuracy and interpretablity.
In this paper, we learn a rule list from the sequence submodular
perspective. We consider the rule list as a sequence and define the
cover set for each rule. Then we formulate a sequence function
which combines both model complexity and classification accuracy.
Based on its appealing sequence submodular property, we propose
a general distorted greedy insert algorithm under Minorization-
Maximization (MM) framework, which gradually inserts rules with
highest inserting gain to the rule list. The rule generation process
is treated as a subproblem, allowing our method to learn the rule
list through a unified framework which avoids rule pre-mining.
We further provide a theoretical lower bound of our greedy insert
algorithm in rule list learning. Experimental results show that our
algorithm achieves better accuracy and interpretability than the
state-of-the-art rule learning methods, and in particular it scales
well on large-scale datasets, especially on high-dimensional data.
CCS CONCEPTS
â€¢Computing methodologies â†’Rule learning.
KEYWORDS
Rule list, interpretability, sequence submodular optimization
ACM Reference Format:
Linxiao Yang, Jingbang Yang, and Liang Sun. 2024. Efficient Decision Rule
List Learning via Unified Sequence Submodular Optimization. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671827
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718271 INTRODUCTION
With the proliferation of enterprise artificial intelligence (AI) ap-
plications, model interpretablity has received more attention, es-
pecially in the high-risk decision-making scenarios, such as health
care [ 6,20] and societal problems [ 25,34]. Despite high classifica-
tion accuracy of the black box models, generally they are too com-
plicated for human to understand and difficult to troubleshoot [ 44],
and even be biased [ 17]. In contrast to black box models, inter-
pretable models, such as decision trees and logistic regression, are
able to provide not only predictions but also the underlying process
of how the prediction is derived. Moreover, interpretable models
bring insights into the tasks, such as how high-order feature inter-
actions drive the target variables [ 8], and how samples are breaking
down into subgroups [ 7]. These insights are beneficial for under-
standing the underlying mechanisms of the systems and further
decision making.
In this paper, we focus on rule list, which is composed of a se-
quence of ordered if-then rules. For a rule list, a sample is classified
by sequentially checking whether a rule is satisfied according to
a particular order. The prediction label of the first rule that the
sample satisfies is treated as the output of the rule list. Although
sharing many similarities with decision tree, rule list considers
multiple features together in each rule, while in decision tree, each
internal node only considers a single feature. When comparing
with rule set [ 15], rule list introduces priority for each rule, while
in a rule set, all rules are treated with equal priority. Thus, rule list
is more predictive than rule set and decision tree [ 43], and tends to
produce more compact models with comparable classification accu-
racy. Meanwhile, rule list is interpretable and user-friendly as its
classification process is analogous to the decision process of human
beings. As a result, its superior predictive and interpretable per-
formance makes it applicable and attractive in many classification
scenarios, such as treatment estimation [ 31,37,54] and bank loan
classification [ 16]. For example, rule list can be applied as a symp-
tom checking flowchart to help doctors diagnose and determine
the treatment in healthcare.
Although rule list learning has attracted many attentions in re-
cent years [ 5,41,48,52,53], it remains a challenging task. Firstly,
it involves optimizing not only a set of rules but also their or-
ders, and essentially is an NP-hard problem [ 45]. To alleviate this
difficulty, most existing methods learn a rule list by sequentially
appending rules, starting from an empty list, and thus reduce the
problem to a rule generation and selection problem. Nevertheless,
for appending-based methods, a rule is appended to the list with-
out considering the rules following it. As no further refinement
or modification can be performed once a rule is appended, sub-
optimal solutions are returned which degrade the generalization
3758
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Linxiao Yang, Jingbang Yang, & Liang Sun
accuracy and interpretablity of the rule list. To address this issue, a
few Monte Carlo simulation based works were proposed [ 48,52].
These stochastic methods, however, have slow convergence rate
and unstable outputs. Methods based on integer programming are
also proposed [ 13,45], while most of them suffer from high com-
putational complexity and fail to deal with large-scale dataset in
practice. The second challenge is the lack of the unified rule list
learning framework especially for high-dimensional data. Most
existing methods, including the recent ones [ 13,45,48,52], typi-
cally rely on a two-stage process which first pre-mines candidate
rules based on predefined metrics and then generates the final rule
list based on candidate rules. This approach inherently leads to
suboptimal rule list model due to the two-stage process. As the
metrics used usually measure the goodness of candidate rules on
the whole dataset, but fail to characterize how they work altogether
in a rule list. In addition, tuning the global threshold for pre-mining
is quite difficult in practice and inappropriate thresholding may
select redundant and unused candidate rules or miss some critical
candidates. Therefore, it is desirable to develop a global rule list
learning algorithm that seamlessly integrates rule list construction
and rule generation in a unified framework.
In this paper, we introduce a novel and general algorithm for de-
cision rule list learning from the submodular perspective, formally
SSRL (Sequence Submodular optimization based Rule List learning).
Firstly, we redefine the rule list learning problem by treating the rule
list as a sequence and defining a cover set for each rule. We then for-
mulate an objective function that blends the complexity of the rule
list with the classification accuracy of both prefix and default rules.
A key innovation of our method is its use of sequence submodular-
ity [3,28] to prioritize rules, which allows for a greedy optimization
approach. To the best of our knowledge, it is the first application
of sequence submodularity in rule list learning. Secondly, we de-
velop a distorted greedy insert algorithm for maximizing a general
regularized sequence submodular function. By incorporating the
concept of backward ğœ‚-monotonicity, we establish a performance
lower bound for this method. When applied to the rule list learning
problem within the Minorization-Maximization (MM) framework,
our greedy insert method ensures forward sequence submodular-
ity and possesses a local backward 1/(ğ¾âˆ’1)-monotone property,
whereğ¾is the number of classes. These properties enable us to de-
rive a performance lower bound for the algorithm in the context of
rule list learning. Thirdly, we conceptualize the problem of finding
the best rule to insert as a subproblem, which admits a difference
of submodular function decomposition and can thus be solved by
the Mod-Mod algorithm [ 27]. This unified optimization framework,
which does not separate rule insertion from rule generation, en-
dows our method with superior performance relative to two-stage
methods. In addition, our methodâ€™s exclusive use of set operations
makes it adept at handling high-dimensional data efficiently with-
out the need for rule pre-mining. We substantiate our contributions
through extensive experiments, demonstrating that our method
surpasses state-of-the-art algorithms in terms of classification ac-
curacy, interpretability, and particularly speed when dealing with
high-dimensional data.Table 1: Comparison of some important properties between
proposed method (SSRL) and popular rule-based classifiers,
including whether optimization based, support for multi-
class classification, reliance on rule pre-mining, and support
for handling more than 1k features.
MethodOptimization
basedMulti-classNo
pre-miningâ‰¥1k
features
SSRLâˆšâˆšâˆšâˆš
SBRLâˆšâˆš âˆš
CORELSâˆš
CLASSYâˆšâˆš âˆš
LIBREâˆš
IDRSâˆš âˆšâˆš
RIPPERâˆšâˆšâˆš
2 RELATED WORK
In this section we give a brief overview of the so-called sparse
logical models [ 44], which consist of logical statements involving
â€œif-then" and other clauses using sparse features. Specifically, we
will review rule list, rule set and decision tree. Moreover, as our
work is related to submodular maximization, we review the use of
submodular maximization for rule learning.
Rule List Learning. Learning rule lists requires establishing both
rules and their hierarchical order. Existing algorithms typically
employ a sequential selection process based on certain criteria to
determine rule priority. Early approaches like FOIL [ 42] and LI-
BRE [ 36] utilize a divide-and-conquer strategy, selecting rules to
classify most data and then removing those data to repeat the pro-
cess. However, this could lead to data fragmentation. More recent
methods optimize a unified objective function, such as Certifiable
optimal rules (CORELS) [ 5], which uses branch and bound with
bounds to narrow the search space but struggles with computa-
tional complexity and multi-class problems. Heuristic modifications
can improve efficiency yet compromise optimality. Bayesian meth-
ods like SBRL [ 52] optimize rule lists directly but suffer from slow
convergence due to reliance on Monte Carlo sampling. Minimum
description length (MDL) principle-based methods avoid hyperpa-
rameters, with CLASSY [ 41] being a notable MDL algorithm for
parameter-free multi-class rule list learning. In particular, we would
like to point out that all aforementioned models such as CORELS,
SBRL and MDL-based models rely on rule pre-mining using item
miners. The separation of rule pre-mining steps and rule generation
and selection steps leads to sub-optimal solutions and hence influ-
ence the model performance. Several mathematical programming
methods [ 13,45] are also proposed to find optimal rule list. How-
ever, they are limited by the large number of variables required,
making them impractical for large datasets despite advancements
in solvers. Logic-based methods [ 23,35] convert rule list learning
into Boolean satisfiability problems; however, they face scalabil-
ity issues. An incremental approach proposed by [ 24] addresses
scalability but at the cost of solution optimality and classification
accuracy.
Rule Set Learning. Unlike rule list models, rule set models have
no requirement for rule orders. Generally, if a sample satisfies
at least one rule in a rule set, it will be classified as a specific
3759Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Comparison of submodular maximization methods.
Method ğœ™(ğ‘…) ğœ‹(ğ‘…)Theoretical
guarantee
[39]S-submodular,
mono, NN- ğœ™(ğ‘…)â‰¥( 1âˆ’1
ğ‘’)ğœ™(ğ‘…âˆ—)
[12]S-submodular,
NN- ğœ™(ğ‘…)â‰¥1
2ğœ™(ğ‘…âˆ—)
[26]S-submodular,
mono, NNmodularğœ™(ğ‘…)âˆ’ğœ‹(ğ‘…)
â‰¥(1âˆ’1
ğ‘’)ğœ™(ğ‘…âˆ—)âˆ’ğœ‹(ğ‘…âˆ—)
[40]S-submodular,
mono, NNS-submodular,
mono, NNğœ™(ğ‘…)âˆ’ğœ‹(ğ‘…)
â‰¥(1âˆ’ğ‘ğœ‹
ğ‘’)ğœ™(ğ‘…âˆ—)âˆ’ğœ‹(ğ‘…âˆ—)
[9, 10]F-submodular,
B-mono, NN- ğœ™(ğ‘…)â‰¥( 1âˆ’1
ğ‘’)ğœ™(ğ‘…âˆ—)
class. Rule set models are popular in binary classification problems
rather than multi-class classification. One of the most classic rule
set learning algorithms is RIPPER [ 14], which is a data-covering
method and uses the MDL principle for pruning. Similar to rule
list learning, many rule set learning algorithms rely on rule pre-
mining [ 30,49,50], where candidate rules are extracted first using
frequent-item miners and specific loss functions are then optimized
to learn the rule set. As rule pre-mining is separated from the rule
selection and generation in the later stage, it gains efficiency but
might instead return the sub-optimal solution. To address this issue
caused by pre-mining, some recent research propose to jointly
construct and select rule sets without auxiliary rule miners [ 15,19,
51]. For example, [ 15] formulates an integer programming to train
an optimal rule set model without pre-mined rules; [ 51] proposes
IDRS which builds rule sets by minimizing the misclassification
loss using the submodular optimization approach.
In summary, we compare the key properties of popular rule list
and rule set learning algorithms in Table 1, which shows that our
proposed SSRL algorithm is the only method which not only adopts
a global unified optimization framework and not rely on rule pre-
mining, but also can successfully and efficiently handle multi-class
classification and large-scale data with more than 1k features.
Optimal Decision Tree. Our work is also related to decision tree,
as a decision tree can be considered as a simplified rule list in
which each rule only considers a single feature. Note that greedy
algorithms such as CART cannot generate the optimal decision
tree. Recently, several optimal decision tree algorithms [ 1,2,11,33,
38,56] have been proposed. Most of them solve the optimal tree
problem via mixed integer programming. Nevertheless, due to the
nonlinear structure of the problem, these methods are less scalable
with respect to the size of the problem, making them unpractical
on large-scale datasets.
Submodular Optimization in Rule Learning. Rule learning in-
volves searching for an optimal rule in a potential discrete search
space, making it a discrete optimization problem. Submodular opti-
mization, a classic approach for such problems, has been applied in
the rule learning domain. Specifically, [ 30] proposes a method to
learn a rule set by maximizing a non-negative, non-monotone sub-
modular set function by utilizing smooth local search [ 21], achiev-
ing a 2/5approximation ratio. Additionally, [ 51] developes an ap-
proach to learn a rule set by maximizing a non-negative, monotone
submodular function minus a modular function, with theoreticalguarantees also established. In general, the task of learning an op-
timal rule set/list ğ‘…can formulated as the following optimization
problem:
max
ğ‘…ğœ™(ğ‘…)âˆ’ğœ‹(ğ‘…), (1)
where functions ğœ™(ğ‘…)andğœ‹(ğ‘…)are defined differently and may sat-
isfy different properties in different applications. Correspondingly,
different submodular maximization algorithms can be applied to
solve the problem in the form of Eq. (1).
We delay the detailed introduction to sequence submodular in
Section 3. We emphasize that compared to set submodular optimiza-
tion, sequence submodular methods often require more stringent
properties of the functions, limiting their applicability in rule learn-
ing. We list several submodular maximization methods that have
been proposed to solve the problem in the form of Eq. (1), along with
their theoretical guarantees, in Table 2. Here, â€œS-submodularâ€, â€œF-
submodularâ€, â€œmonoâ€, â€œB-monoâ€, and â€œNNâ€ are abbreviations for set
submodular, forward sequence submodular, monotonic, backward
monotonic, and nonnegativity, respectively.
3 BASICS OF SEQUENCE SUBMODULARITY
We first provide a brief review on sequence submodularity [ 3,9,10],
which is also known as string submodularity [ 55]. LetÎ©denote the
ground set of all possible states. A sequence ğ‘…of lengthğ‘˜is defined
as a string formed by ğ‘˜states{ğ‘Ÿğ‘–âˆˆÎ©}ğ‘˜
ğ‘–=1, i.e.,(ğ‘Ÿ1,...,ğ‘Ÿğ‘˜), and
the number of elements contained in the sequence is equivalently
defined as its length. Appending a sequence ğ‘…2=(ğ‘Ÿ2
1,ğ‘Ÿ2
2,...,ğ‘Ÿ2
|ğ‘…2|)
after another sequence ğ‘…1=(ğ‘Ÿ1
1,ğ‘Ÿ1
2,...,ğ‘Ÿ1
|ğ‘…1|)is denoted by ğ‘…1âŠ•ğ‘…2
and defined as
ğ‘…1âŠ•ğ‘…2=(ğ‘Ÿ1
1,ğ‘Ÿ1
2,...,ğ‘Ÿ1
|ğ‘…1|,ğ‘Ÿ2
1,ğ‘Ÿ2
2,...,ğ‘Ÿ2
|ğ‘…2|). (2)
Similarly, appending a state ğ‘Ÿâ€²after a sequence ğ‘…=(ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿ|ğ‘…|)
is denoted by ğ‘…âŠ•ğ‘Ÿâ€²and defined as
ğ‘…âŠ•ğ‘Ÿâ€²=(ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿ|ğ‘…|,ğ‘Ÿâ€²). (3)
The operation of inserting a state ğ‘Ÿâ€²after theğ‘–-th position of a
sequenceğ‘…=(ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿğ‘–,ğ‘Ÿğ‘–+1,...,ğ‘Ÿ|ğ‘…|)is written asI(ğ‘…,ğ‘–,ğ‘Ÿâ€²)
and defined as
I(ğ‘…,ğ‘–,ğ‘Ÿâ€²)=(ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿğ‘–,ğ‘Ÿâ€²,ğ‘Ÿğ‘–+1,...,ğ‘Ÿ|ğ‘…|). (4)
A sequence ğ‘…2is a subsequence of ğ‘…1, denoted by ğ‘…1â‰½ğ‘…2, ifğ‘…2
can be derived by eliminating some states of ğ‘…1.
A function ğ‘“(ğ‘…)from sequences to real numbers is said to be
backward monotonic if
ğ‘“(ğ‘…1âŠ•ğ‘…2)â‰¥ğ‘“(ğ‘…2) âˆ€ğ‘…1,ğ‘…2 (5)
and forward monotonic if
ğ‘“(ğ‘…1âŠ•ğ‘…2)â‰¥ğ‘“(ğ‘…1) âˆ€ğ‘…1,ğ‘…2 (6)
A function is sequence submodular if it has diminishing-return
property along some order. Specifically, we say a function ğ‘“(ğ‘…)is
forward sequence submodular if for any ğ‘Ÿ1,ğ‘Ÿ2âˆˆÎ©,ğ‘…,
ğ‘“(ğ‘…âŠ•ğ‘Ÿ1âŠ•ğ‘Ÿ2)âˆ’ğ‘“(ğ‘…âŠ•ğ‘Ÿ1)â‰¤ğ‘“(ğ‘…âŠ•ğ‘Ÿ2)âˆ’ğ‘“(ğ‘…). (7)
Apart from forward sequence submodularity, a more general se-
quence submodularity based on the definition of subsequence can
3760KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Linxiao Yang, Jingbang Yang, & Liang Sun
Table 3: The cover set, label set, and number of samples cor-
rectly classified of each rule in an example rule list.
Rule Rule ListCover
setLabel
setNumber of samples
correctly classified
ğ‘Ÿ1 IFğ‘¥1>5THENğ‘¦=0C1L1|C1âˆ©L 1|
ğ‘Ÿ2ELIFğ‘¥2>1ANDğ‘¥1>3
THENğ‘¦=1C2L2|C1âˆ©C 2âˆ©L 2|
ğ‘Ÿ3 ELIFğ‘¥1<1THENğ‘¦=2C3L3|C<3âˆ©C 3âˆ©L 3|
ğ‘Ÿğ‘‘ ELSEğ‘¦=0 Lğ‘‘|C<4âˆ©Lğ‘‘|
be defined for function ğ‘“if it satisfies
ğ‘“(ğ‘…1âŠ•ğ‘Ÿ)âˆ’ğ‘“(ğ‘…1)â‰¤ğ‘“(ğ‘…2âŠ•ğ‘Ÿ)âˆ’ğ‘“(ğ‘…2) âˆ€ğ‘ŸâˆˆÎ©,ğ‘… 1â‰½ğ‘…2.(8)
Different from sequence submodularity, a set function defined
onÎ©is called set submodular [29] if for any ğ´âŠ†ğµâŠ†Î©,ğ‘ŸâˆˆÎ©,
ğ‘“(ğ´âˆª{ğ‘Ÿ})âˆ’ğ‘“(ğ´)â‰¥ğ‘“(ğµâˆª{ğ‘Ÿ})âˆ’ğ‘“(ğµ). (9)
ğ‘“(ğ‘‹)is called modular function if the inequality in Eq. (9)becomes
equality. We now introduce two tight upper modular bounds and
one tight lower modular bound for set submodular functions that
widely used in submodular optimization. As shown in [ 27], given
a submodular function ğ‘“(ğ‘‹), two tight upper modular bounds at
âˆ€ğ‘ŒâŠ†Î©are given as
ğ‘“1(ğ‘‹;ğ‘Œ)Bğ‘“(ğ‘Œ)âˆ’âˆ‘ï¸
ğ‘—âˆˆğ‘Œ\ğ‘‹ğ‘“(ğ‘—|ğ‘Œ\{ğ‘—})+âˆ‘ï¸
ğ‘—âˆˆğ‘‹\ğ‘Œğ‘“(ğ‘—|âˆ…),(10)
ğ‘“2(ğ‘‹;ğ‘Œ)Bğ‘“(ğ‘Œ)âˆ’âˆ‘ï¸
ğ‘—âˆˆğ‘Œ\ğ‘‹ğ‘“(ğ‘—|Î©\{ğ‘—})+âˆ‘ï¸
ğ‘—âˆˆğ‘‹\ğ‘Œğ‘“(ğ‘—|ğ‘Œ),(11)
whereğ‘“(ğ‘—|ğ‘‹)is short for ğ‘“(ğ‘‹âˆª{ğ‘—})âˆ’ğ‘“(ğ‘‹). Again from [ 27], a
tight lower bound of ğ‘“(ğ‘‹)atâˆ€ğ‘ŒâŠ†Î©is given as
ğ‘“ğœ…(ğ‘‹;ğ‘Œ)=âˆ‘ï¸
ğ‘–âˆˆğ‘‹ğ‘“ğœ…(ğ‘–;ğ‘Œ) (12)
where
ğ‘“ğœ…(ğ‘–;ğ‘Œ)=ğ‘“(ğ‘†ğœ…
ğœ…âˆ’1(ğ‘–))âˆ’ğ‘“(ğ‘†ğœ…
ğœ…âˆ’1(ğ‘–)âˆ’1), (13)
ğœ…is a permutation of the ground set Î©, andğ‘†ğœ…
ğ‘–is a set with ğ‘†ğœ…
0=âˆ…,
ğ‘†ğœ…
ğ‘—={ğœ…(1),...,ğœ…(ğ‘—)}, andğ‘†ğœ…
|ğ‘Œ|=ğ‘Œ.
4 PROBLEM FORMULATION
We focus on the ğ¾-class classification problem and specifically
consider the scenario where all features of samples are binary.
Noted that although only binary features are considered here, it is
easy to extend our model to non-binary categorical and numerical
features using one-hot encoding and discretization techniques, re-
spectively. Let{(ğ’™ğ‘–,ğ‘¦ğ‘–)}ğ‘›
ğ‘–=1denote a set of training samples, where
ğ’™ğ‘–=[ğ‘¥ğ‘–,1,ğ‘¥ğ‘–,2,...,ğ‘¥ğ‘–,ğ·]ğ‘‡is ağ·-dimensional binary vector indi-
cating the presence of the ğ·features, and ğ‘¦ğ‘–âˆˆ{1,...,ğ¾}is the
corresponding label. Define Î“as a collection of the ğ·features, we
say that the ğ‘–-th sample has the ğ‘—-th feature in Î“ifğ‘¥ğ‘–ğ‘—=1. We aim
to learn a rule list which is a classifier of the form â€œIF ğ‘1THENğ‘¦=ğ‘™1
ELIFğ‘2THENğ‘¦=ğ‘™2...ELIFğ‘ğ‘šTHENğ‘¦=ğ‘™ğ‘šELSEğ‘¦=ğ‘™ğ‘‘â€, where
{ğ‘ğ‘–}ğ‘š
ğ‘–=1denote the conditions by which each samples are tested, and
{ğ‘™ğ‘–}ğ‘š
ğ‘–=1âˆª{ğ‘™ğ‘‘}denote the corresponding labels. Here the condition
ğ‘ğ‘–is a set of features. A sample is said to satisfy a condition if and
only if it has all the features in the condition. Obviously, a rule list
is a sequence of â€œIF ... THEN ... â€ clauses with order, and end with anâ€œELSE... â€ clause. Mathematically, we write â€œIF ğ‘THENğ‘¦=ğ‘™â€ clauses
as a ruleğ‘Ÿ=(ğ‘,ğ‘™)and â€œELSEâ€ clause as ğ‘Ÿğ‘‘=(ğ‘ğ‘‘,ğ‘™ğ‘‘). Then a rule
list of length ğ‘š+1can be written as a sequence (ğ‘Ÿ1,ğ‘Ÿ2,Â·Â·Â·,ğ‘Ÿğ‘š,ğ‘Ÿğ‘‘).
We call the sequence ğ‘…=(ğ‘Ÿ1,ğ‘Ÿ2,Â·Â·Â·,ğ‘Ÿğ‘š)a prefix, defined as a rule
list without the final "ELSE" clause. The last rule ğ‘Ÿğ‘‘in a rule list is
called the default rule, whose condition is empty. Given a rule (ğ‘,ğ‘™),
we define its cover set Cas a collection of samples that satisfies
conditionğ‘. We also define cover set Lfor labelğ‘™comprised of
samples whose label is ğ‘™. Letğ‘Ÿğ‘–=(ğ‘ğ‘–,ğ‘™ğ‘–)be theğ‘–th rule in a rule
list, a sample is correctly classified by ğ‘Ÿğ‘–if and only if the sample
does not satisfy conditions of any rules in front of ğ‘Ÿğ‘–, i.e.,{ğ‘ğ‘—}ğ‘—<ğ‘–,
meanwhile satisfies the condition of ğ‘Ÿğ‘–, i.e.,ğ‘ğ‘–, and has label ğ‘™ğ‘–.
Hence, forğ‘Ÿğ‘–, the number of correctly classified samples can be
computed asC<ğ‘–âˆ©Cğ‘–âˆ©Lğ‘–, whereCğ‘–andLğ‘–are the cover set of
conditionğ‘ğ‘–and labelğ‘™ğ‘–, respectively,C<ğ‘–BÃğ‘–âˆ’1
ğ‘—=1Cğ‘—denotes the
set of samples that covered by the first ğ‘–âˆ’1rules in the rule list, and
Cdenotes the complementary set of C. As a result, for a rule list
(ğ‘Ÿ1,ğ‘Ÿ2,Â·Â·Â·,ğ‘Ÿğ‘š,ğ‘Ÿğ‘‘), the number of correctly classified samples are
given asÃğ‘š
ğ‘–=1C<ğ‘–âˆ©Cğ‘–âˆ©Lğ‘–+Câ‰¤ğ‘šâˆ©Lğ‘‘, where the first term
denotes the number of samples correctly classified by rules {ğ‘Ÿğ‘–}ğ‘š
ğ‘–=1,
and the second term denotes the number of samples correctly classi-
fied by the default rule ğ‘Ÿğ‘‘. Table 3 provides an example rule list and
its cover set, label set, and number of samples correctly classified
of each rule.
Our objective is to learn a rule list that can classify correctly as
many samples as possible, while still maintaining low complexity.
Formally, given a default rule ğ‘Ÿğ‘‘, our objective is to find a prefix ğ‘…
whose length no larger than ğœˆand maximizes
ğ‘“(ğ‘…)=|ğ‘…|âˆ‘ï¸
ğ‘–=1C<ğ‘–âˆ©Cğ‘–âˆ©Lğ‘–+Câ‰¤|ğ‘…|âˆ©Lğ‘‘âˆ’ğœ†ğ‘¢(ğ‘…)
=ğ‘(ğ‘…)âˆ’ğ‘‘(ğ‘…)âˆ’ğœ†ğ‘¢(ğ‘…)+ğ‘›, (14)
whereğ‘(ğ‘…)=Ã|ğ‘…|
ğ‘–=1C<ğ‘–âˆ©Cğ‘–âˆ©Lğ‘–, and it measures the number of
samples correctly classified by prefix ğ‘…; andğ‘‘(ğ‘…)represents the size
of the complement of the set of samples that are correctly classified
by the default rule, i.e., ğ‘‘(ğ‘…)=Câ‰¤|ğ‘…|âˆ©Lğ‘‘=Câ‰¤|ğ‘…|âˆªLğ‘‘. In
other words, ğ‘‘(ğ‘…)measures how the rule prefix ğ‘…reduces the
number of samples that would otherwise be correctly classified by
the default rule. Here ğœ†is a parameter that controls the trade-off
between the classification accuracy and the complexity of the rule
list, andğ‘¢(ğ‘…)measures the complexity of ğ‘…, counting the number
of featuresğ‘…used, i.e.,ğ‘¢(ğ‘…)=Ã
ğ‘Ÿâˆˆğ‘…|ğ‘Ÿ|, where|ğ‘Ÿ|is defined as the
number of features in the condition of ğ‘Ÿ. Obviously, ğ‘“(ğ‘…),ğ‘(ğ‘…)and
ğ‘‘(ğ‘…)are sequence functions [ 4,9,55] from sequences to real value,
and we summarize their properties in Lemma 1.
Lemma 1. Withğ‘(ğ‘…)andğ‘‘(ğ‘…)defined above, we have: 1) ğ‘(ğ‘…)
is a forward sequence submodular function; 2) ğ‘(ğ‘…)is a forward-
monotone function; 3) ğ‘(ğ‘…)is not a backward-monotone function; 4)
ğ‘‘(ğ‘…)is a submodular function; 5) ğ‘¢(ğ‘…)is a modular function.
The above lemma implies that the objective function can be
viewed as the difference between a sequence submodular function
ğ‘(ğ‘…)and a set submodular function ğ‘‘(ğ‘…)+ğœ†ğ‘¢(ğ‘…). Fig. 1 illustrates
the forward sequence submodularity of a sample prefix. Given
3761Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 1: Illustration of forward sequence submodularity of prefix of the rule list. ğ‘(ğ‘…)denotes the number of samples correctly
classified by prefix ğ‘….
the review of current submodular maximization algorithms in the
literature in Table 2, we highlight two difficulties arising from
maximizing ğ‘“(ğ‘…)in Eq. (14). On the one hand, maximizing the dif-
ference between two submodular functions is non-trivial. Although
several works have been proposed to maximize the difference of
two set submodular functions, few works are able to deal with
the difference between a sequence submodular function and a set
submodular function. On the other hand, the lack of backward-
monotonic property of ğ‘(ğ‘…)makes the problem more challenging.
The requirements of both forward sequence submodularity and
backward monotonicity by most existing sequence subumodular
maximization methods make them inapplicable to our problem.
5 ALGORITHMS
In this section, we propose a greedy method to learn a rule list
efficiently. As discussed above, one difficulty of maximizing the
objective function arises from the difference of a sequence submod-
ular function and a set submodular function. To overcome this, we
resort to the Minorization-Maximization (MM) approach, where
we iteratively maximize a simple surrogate function minorizing the
original objective function. To this end, the submodular function
ğ‘‘(ğ‘…)is first approximated using its modular upper bounds based on
current estimation. As shown in [ 27], given a submodular function
ğ‘“(ğ‘‹), two tight upper modular bounds at âˆ€ğ‘ŒâŠ†Î©are given as
ğ‘“1(ğ‘‹;ğ‘Œ)Bğ‘“(ğ‘Œ)âˆ’âˆ‘ï¸
ğ‘—âˆˆğ‘Œ\ğ‘‹ğ‘“(ğ‘—|ğ‘Œ\{ğ‘—})+âˆ‘ï¸
ğ‘—âˆˆğ‘‹\ğ‘Œğ‘“(ğ‘—|âˆ…),(15)
ğ‘“2(ğ‘‹;ğ‘Œ)Bğ‘“(ğ‘Œ)âˆ’âˆ‘ï¸
ğ‘—âˆˆğ‘Œ\ğ‘‹ğ‘“(ğ‘—|Î©\{ğ‘—})+âˆ‘ï¸
ğ‘—âˆˆğ‘‹\ğ‘Œğ‘“(ğ‘—|ğ‘Œ),(16)
whereğ‘“(ğ‘—|ğ‘‹)is short forğ‘“(ğ‘‹âˆª{ğ‘—})âˆ’ğ‘“(ğ‘‹). Substituting two upper
bounds ofğ‘‘(ğ‘…), denoted by ğ‘‘1(ğ‘…;ğ‘…(ğ‘¡))andğ‘‘2(ğ‘…;ğ‘…(ğ‘¡)), to Eq. (14),
we arrive at two surrogate functions of ğ‘“(ğ‘…)as
ğ‘“1(ğ‘…;ğ‘…(ğ‘¡))=ğ‘(ğ‘…)âˆ’ğ‘‘1(ğ‘…;ğ‘…(ğ‘¡))âˆ’ğœ†ğ‘¢(ğ‘…), (17)
ğ‘“2(ğ‘…;ğ‘…(ğ‘¡))=ğ‘(ğ‘…)âˆ’ğ‘‘2(ğ‘…;ğ‘…(ğ‘¡))âˆ’ğœ†ğ‘¢(ğ‘…), (18)
whereğ‘…(ğ‘¡)denotes the estimate of ğ‘…at theğ‘¡-th MM iteration, and
we ignore the terms independent of ğ‘….
5.1 Proposed Greedy Insert Method
In this subsection, we present a greedy insertion method to optimize
the objectives formulated in Eqs. (17, 18). For streamlined problem-
solving, we consolidate the maximization challenge of Eqs. (17, 18)
into the subsequent unified expression:
max
|ğ‘…|â‰¤ğœˆËœğ‘“(ğ‘…)=ğœ“(ğ‘…)âˆ’ğœ‹(ğ‘…), (19)
whereğœ“(ğ‘…)=ğ‘(ğ‘…), andğœˆis a predefined cardinality constraint on
the number of rules in ğ‘…. The termğœ‹(ğ‘…)is specified as ğ‘‘1(ğ‘…;ğ‘…(ğ‘¡))+
ğœ†ğ‘¢(ğ‘…)for the optimization of Eq. (17), and asğ‘‘2(ğ‘…;ğ‘…(ğ‘¡))+ğœ†ğ‘¢(ğ‘…)
when targeting Eq. (18). Importantly, ğœ“(ğ‘…)quantifies the count ofAlgorithm 1: Distorted Greedy Insert (DGI)
Input : Ground set Î©, cardinality constraint ğœˆ, sequence
submodular function ğœ“, and modular function ğœ‹.
Output: Sequenceğ‘….
1Initializeğ‘…as empty;
2forğ‘–=1,...,ğœˆ do
3ğ›¼ğ‘–â†(1âˆ’1
ğœˆ)(ğœˆâˆ’ğ‘–);
4ğœ“(ğ‘Ÿ,ğ‘˜|ğ‘…)â†ğœ“(I(ğ‘…,ğ‘˜,ğ‘Ÿ))âˆ’ğœ“(ğ‘…);
5(ğ‘Ÿâˆ—,ğ‘˜âˆ—)â† arg max(ğ‘Ÿ,ğ‘˜)âˆˆÎ©Ã—{ğ‘˜}|ğ‘…|
ğ‘˜=0ğ›¼ğ‘–ğœ“(ğ‘Ÿ,ğ‘˜|ğ‘…)âˆ’ğœ‹(ğ‘Ÿ);
6 ifğ›¼ğ‘–ğœ“(ğ‘Ÿâˆ—,ğ‘˜âˆ—|ğ‘…)âˆ’ğœ‹(ğ‘Ÿ)â‰¥0then
7 Insert element ğ‘Ÿinto theğ‘˜th position of ğ‘…
8Return R
samples accurately classified by the prefix ğ‘…alone, while ğœ‹(ğ‘…)mea-
sures the extent to which prefix ğ‘…reduces the number of samples
correctly classified by the default rule, in addition to the quantity
of features employed by prefix ğ‘….
Our method harnesses computational efficiency through the
use of a greedy algorithm. In each iteration, for a chosen inser-
tion position ğ‘˜, we select an optimal rule ğ‘Ÿbased on its insertion
gain, which affects both ğœ“(ğ‘…)andğœ‹(ğ‘…). The gain is computed as
ğœ“(ğ‘Ÿ,ğ‘˜|ğ‘…)=ğœ“(I(ğ‘…,ğ‘˜,ğ‘Ÿ))âˆ’ğœ“(ğ‘…)andğœ‹(ğ‘Ÿ,ğ‘˜|ğ‘…)=ğœ‹(ğ‘Ÿ).
The ideal outcome is that all samples sharing the label of the
default rule are correctly classified by it, which would naturally
result in shorter and more efficient rule lists. To guide the algorithm
towards this ideal goal, we adjust the importance of ğœ“(ğ‘Ÿ,ğ‘˜|ğ‘…)and
ğœ‹(ğ‘Ÿ)dynamically. In the ğ‘–th iteration, we seek a rule ğ‘Ÿand a position
ğ‘˜by maximizing the following:
ğ›¼ğ‘–ğœ“(ğ‘Ÿ,ğ‘˜|ğ‘…)âˆ’ğœ‹(ğ‘Ÿ). (20)
Initially, the weight ğ›¼1is set to a small value, (1âˆ’1/ğœˆ)(ğœˆâˆ’1),
to prevent the prefix from incorrectly classifying samples that
should be correctly classified by the default rule. To ensure op-
timal performance, we incrementally increase the weight with each
iteration by a factor of (1âˆ’1/ğœˆ), yielding an iterative weight of
ğ›¼ğ‘–=(1âˆ’1/ğœˆ)(ğœˆâˆ’ğ‘–). Our method, named Distorted Greedy Insert,
is summarized in Algorithm 1.
Discussion: As discussed earlier, ğœ“(ğ‘…)is a nonnegative forward
sequence submodular function, and ğœ‹(ğ‘…)is a nonnegative modular
function. Thus the problem in Eq. (19)is maximizing a nonnegative
sequence submodular function with modular cost regularization
under cardinality constraints. Although efficient algorithms [ 9,46]
have been proposed to maximize sequence submodular functions
under modular constraints, their extension to maximize the se-
quence submodular function with regularization, i.e., problem in
Eq.(19), is not trivial. The reason is due to the presence of the modu-
lar regularization ğœ‹(ğ‘…)that breaks the non-negativity as well as the
backward monotone of Ëœğ‘“(ğ‘…). Another difficulty comes from the re-
quirement of the backward monotonic. Most sequence submodular
3762KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Linxiao Yang, Jingbang Yang, & Liang Sun
Figure 2: Illustration of theoretical guarantee of respective
methods as well as the properties of the function required by
these methods when dealing with binary class classification
problem.
maximization methods requires the presence of forward sequence
submodular and backward monotone. Clearly, Ëœğ‘“(ğ‘…)does not ex-
hibits backward monotone. As we will show below, our proposed
method is the first method that does not require the monotonicity
of the function while remain exists performance guarantee.
5.1.1 Theoretical analysis. Asğœ“(ğ‘…)is non-monotonic, we intro-
duce a notation, named monotonic ratio, to measure how close
the function is to a monotone function. Specifically, we define the
backward monotone ratio for ğœ“(ğ‘…)as
ğœ‚â‰œmin
ğ‘…1,ğ‘…2â‰ âˆ…ğœ“(ğ‘…1âŠ•ğ‘…2)
ğœ“(ğ‘…2). (21)
We callğœ“(ğ‘…)backwardğœ‚-monotone. Obviously, if ğœ‚â‰¥1, we can
conclude that ğœ“(ğ‘…)is backward monotone, and if ğœ‚<1, we seeğœ“(ğ‘…)
is weak monotone. The following result provides a lower bound
on performance of the proposed method for forward sequence
submodular and backward ğœ‚-monotoneğœ“(ğ‘…)and modular ğœ‹(ğ‘…).
Theorem 2. Assumeğœ“(ğ‘…)is forward sequence submodular and
backwardğœ‚-monotone,ğœ‹(ğ‘…)is modular. Let Ë†ğ‘…denote the output of
Algorithm 1, ğ‘…âˆ—be the optimal solution maximizing Ëœğ‘“(ğ‘…), then
ğœ“(Ë†ğ‘…)âˆ’ğœ‹(Ë†ğ‘…)â‰¥ğœ‚(1âˆ’1
ğ‘’)ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(ğ‘…âˆ—). (22)
The above theorem requires ğœ“(ğ‘…)to be global monotonic or
globalğœ‚âˆ’monotonic, and the following corollary provides an algo-
rithmic property based guarantee for performance which relaxes
the requirement of global monotonicity.
Corollary 1. Assumeğœ“(ğ‘…)is forward sequence submodular and
ğœ‹(ğ‘…)is modular. Ë†ğ‘…(ğ‘–)is the sequence generated by Algorithm 1 at the
ğ‘–-th iteration. If ğœ“(ğ‘…)is backward ğœ‚-monotone on the set {Ë†ğ‘…(ğ‘–)}ğœˆ
ğ‘–=1âˆª
{ğ‘…âˆ—}, whereğ‘…âˆ—denotes the optimal solution of problem in Eq. (19),
then Eq. (22)still holds with Ë†ğ‘…being the output of Algorithm 1.
Corollary 1 shows that if ğœ“exhibits the local backward weak
monotonicity on set {Ë†ğ‘…(ğ‘–)}ğœˆ
ğ‘–=1âˆª{ğ‘…âˆ—}, the performance guarantee
shown in Theorem 2 is still valid.
We have introduced a theoretical guarantee for our proposed
method with notation ğœ‚. To determine the value of ğœ‚for global
backwardğœ‚-monotone is difficult. Fortunately, if we restrict the
labels of the rules in ğ‘…to be different to ğ‘™ğ‘‘, the following lemma
shows thatğœ“(ğ‘…)exhibits backward 1/(ğ¾âˆ’1)-monotonicity locally.Lemma 3. LetË†ğ‘…(ğ‘–)be the generated prefix by Algorithm 1 in
theğ‘–-th iteration while optimizing Eq. (19) over all possible pre-
fixes. If{Ë†ğ‘…(ğ‘–)}ğœˆ
ğ‘–=1are not all empty, then ğœ“(ğ‘…)is at least backward
(1/(ğ¾âˆ’1))-monotone on the set {Ë†ğ‘…(ğ‘–)}ğœˆ
ğ‘–=1âˆª{ğ‘…âˆ—}, whereğ‘…âˆ—denotes
the maximizer of Eq. (19)under the cardinality constraint |ğ‘…|â‰¤ğœˆ.
As an immediate result of Corollary 1 and Lemma 3, we arrive
at the performance guarantee of Algorithm 1 when optimizing
Eq. (19).
Theorem 4. LetË†ğ‘…be the output of Algorithm 1 for optimizing
Eq.(19)over all the possible prefixes, and ğ‘…âˆ—be the optimal prefix
that maximizing Eq. (19)under constrain|ğ‘…|â‰¤ğœˆ. IfË†ğ‘…is not empty
then
ğœ“(Ë†ğ‘…)âˆ’ğœ‹(Ë†ğ‘…)â‰¥1
ğ¾âˆ’1
1âˆ’1
ğ‘’
ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(ğ‘…âˆ—). (23)
The above theorem shows that although ğœ“(ğ‘…)may not exhibit
global monotonicity, we can still optimize Eq. (19)using Algorithm 1
and achieve certain performance guarantee.
Discussion: We observe that our approach bears a relation to
the work presented in [ 26], which introduces a distorted greedy
algorithm for maximizing a set submodular function. The method
in [26] not only depends on the set submodularity property but also
necessitates global monotonicity of the functions. Consequently,
adapting the strategy proposed in [ 26] to sequence submodular
functionsâ€”which represent a more general notion than set submod-
ularityâ€”poses a non-trivial challenge. In contrast, our method is
adept at handling sequence submodular functions without requir-
ing function monotonicity. Surprisingly, when addressing binary
classification problems, i.e., ğ¾=2, our model achieves theoreti-
cal guarantees that are on par with those of [ 26], despite the less
stringent conditions imposed by our approach. Fig. 2 illustrates the
distinctions between our method and the one in [ 26] under binary
classification problem.
Based on the distorted greedy insert algorithm with performance
guarantee, we propose our rule list learning algorithm SSRL (Se-
quence Submodular optimization based Rule List learning), as sum-
marized in Algorithm 2. In Algorithm 2, we iteratively find two
tight upper bounds of ğ‘“(ğ‘…), and optimize them using Algorithm
1. The learned rule list can be further refined using local search
operations, such as exchange and replacement.
5.2 Sub-Problem Solving
The proposed Algorithm 1 involves finding a pair of rule and posi-
tion with maximal insertion gain, which is a combinatorial search
problem with the size of search space exponential to the number of
binary features. In this subsection, we propose an efficient method
to find the optimal rule to insert.
We first discuss the case of finding the optimal condition ğ‘âˆ—when
fixing the position ğ‘˜and labelğ‘™, such that inserting the rule (ğ‘âˆ—,ğ‘™)
to theğ‘˜th position of ğ‘…achieves maximal insertion gain. Without
loss of generality, we only discuss the scenario that ğœ“(R) andğœ‹(ğ‘…)
in Algorithm 1 are set to ğ‘(ğ‘…)andğ‘‘1(ğ‘…;ğ‘…(ğ‘¡))+ğœ†ğ‘¢(ğ‘…), respectively.
Letğº(ğ‘Ÿ,ğ‘˜)denote the distorted gain of inserting a rule ğ‘Ÿ=(ğ‘,ğ‘™)to
theğ‘˜th position of ğ‘…as presented as the 4-th line in Algorithm 1,
3763Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 4: Binary-class Performance: Predicting accuracy (%, standard deviation in parentheses).
Dataset #
samples #
features SSRL SBRL CORELSâˆ—CLASSY IDRS RIPPER
FICO-binar
y 10459 34 72.03(0.7) 71.21(1.0) 70.70(0.8) 71.78(1.5) 71.2(1.1) 60.1(1.2)
FICO 10459 312 71.90(1.5) 70.37(1.2) 69.86(1.1) 70.83(1.5) 70.4(1.4) 69.1(1.9)
WDBC 569 540 95.79(3.6) 92.98(4.1) 94.39(4.1) 94.73(3.3) 94.0(4.8) 94.7(1.6)
gas 13910 2304 98.63(0.2) 95.26(0.6) 87.72(1.4) 99.01(0.3) 98.20.4 99.0(0.4)
magic 19020 180 86.28(0.6) 81.66(0.9) 81.27(0.8) 85.79(0.5) 84.6(0.8) 82.2(1.3)
adult 48842 262 84.31(0.7) 83.76(0.5) 82.94(0.6) 83.97(0.6) 84.4(0.6) 83.3(0.9)
COMP
AS-binary 6907 24 67.12(1.4) 66.21(1.4) 66.60(1.3) 66.72(1.5) 67.0(1.5) 56.0(0.6)
then
we have
ğº(ğ‘Ÿ,ğ‘˜)=ğ›¼ğ‘(ğ‘Ÿ,ğ‘˜|ğ‘…)âˆ’ğ‘‘1(ğ‘Ÿ;ğ‘…(ğ‘¡))
âˆ’ğœ†|ğ‘|, (24)
whereğ‘‘1(ğ‘Ÿ;ğ‘…(ğ‘¡))=ğ‘‘(ğ‘Ÿ|ğ‘…(ğ‘¡)\{ğ‘Ÿ})forğ‘Ÿâˆˆğ‘…(ğ‘¡),ğ‘‘1(ğ‘Ÿ;ğ‘…(ğ‘¡))=
ğ‘‘(ğ‘Ÿ|âˆ…)forğ‘ŸâˆˆÎ©\ğ‘…(ğ‘¡)andğ‘(ğ‘Ÿ,ğ‘˜|ğ‘…)=ğ‘(I(ğ‘…,ğ‘˜,ğ‘Ÿ))âˆ’ğ‘(ğ‘…). We
further define
ğº1(ğ‘Ÿ,ğ‘˜)=ğ›¼ğ‘(ğ‘Ÿ,ğ‘˜|ğ‘…)âˆ’ğ‘‘(ğ‘Ÿ|ğ‘…(ğ‘¡)\{ğ‘Ÿ})âˆ’ğœ†|ğ‘|, (25)
ğº2(ğ‘Ÿ,ğ‘˜)=ğ›¼ğ‘(ğ‘Ÿ,ğ‘˜|ğ‘…)âˆ’ğ‘‘(ğ‘Ÿ|âˆ…)âˆ’ğœ†|ğ‘|. (26)
Sinceğº(ğ‘Ÿ,ğ‘˜)has different evaluations for rules from ğ‘…(ğ‘¡)orÎ©\ğ‘…(ğ‘¡),
a straightforward strategy is to separately compute the optimal
rules inğ‘…(ğ‘¡)andÎ©\ğ‘…(ğ‘¡), and choose the one with larger gain as
the solution. Nevertheless, the presence of the constraint Î©\ğ‘…(ğ‘¡)
makes the optimization of ğº2(ğ‘Ÿ,ğ‘˜)difficult. Fortunately, due to the
submodularity of ğ‘‘(ğ‘…), the following lemma justifies the removal
of this constraint.
Lemma 5. Letğ‘Ÿ1âˆˆarg maxğ‘Ÿâˆˆğ‘…(ğ‘¡)ğº1(ğ‘Ÿ,ğ‘˜),ğ‘Ÿ2âˆˆarg maxğ‘ŸâˆˆÎ©ğº2(ğ‘Ÿ,ğ‘˜),
andğ‘Ÿâˆˆarg maxğ‘Ÿâˆˆ{ğ‘Ÿ1,ğ‘Ÿ2}ğº(ğ‘Ÿ,ğ‘˜). Then we have ğ‘Ÿâˆˆarg maxğ‘ŸâˆˆÎ©ğº(ğ‘Ÿ,ğ‘˜).
As the size of ğ‘…(ğ‘¡)is smaller than a predefined parameter ğœˆ, the
maximizer of ğº1(ğ‘Ÿ,ğ‘˜)can be found by testing all the rules in ğ‘…(ğ‘¡).
Figure 3: Illustration of insert ğ‘Ÿ=(ğ‘,ğ‘™)into the position ğ‘˜,
whereCandLdenote the cover set of ğ‘andğ‘™, respectively.
S>ğ‘˜is defined in Lemma 6.
We next discuss the optimization of ğº2(ğ‘Ÿ,ğ‘˜), which quantifies the
benefit of inserting a rule ğ‘Ÿat positionğ‘˜. As depicted in Fig. 3, the
advantages conferred by the inclusion of rule ğ‘Ÿcan be segregated
into three distinct aspects. Firstly, the insertion of ğ‘Ÿenables correct
classification of certain samples that were previously misclassified.
Secondly, the introduction of ğ‘Ÿaffects the subsequent rules in the
list,{ğ‘Ÿğ‘–}|ğ‘…|
ğ‘–=ğ‘˜+1, by reducing their sample processing load, which
consequently may decrease the number of samples they correctly
classify. The third aspect pertains to the impact of ğ‘Ÿon the default
ruleâ€™s performance. The following lemma encapsulates these three
facets of the influence exerted by the insertion of rule ğ‘Ÿat position
ğ‘˜. It establishes that ğº2(ğ‘Ÿ,ğ‘˜)can be formulated as the difference
between two submodular functions.
Lemma 6. LetS>ğ‘˜=Ã|ğ‘…|
ğ‘–=ğ‘˜+1
C<ğ‘–âˆ©
Cğ‘–âˆ©Lğ‘–
, we haveğº2(ğ‘Ÿ,ğ‘˜)âˆ
ğœ”(ğ‘)âˆ’ğ‘£(ğ‘)where
ğœ”(ğ‘)=ğ›¼S>ğ‘˜âˆ©
âˆªğ‘’âˆˆğ‘E+
âˆªğ‘’âˆˆğ‘E
âˆ©
Lğ‘‘,
ğ‘£(ğ‘)=ğ›¼Câ‰¤ğ‘˜âˆ©
âˆªğ‘’âˆˆğ‘E
âˆ©
L+ğœ†|ğ‘|.
HereEdenotes the set of samples containing feature ğ‘’,C=âˆ©ğ‘’âˆˆğ‘E,
andC=âˆªğ‘’âˆˆğ‘E.
Due to the property of union operation, we can conclude that
bothğœ”(ğ‘)andğ‘£(ğ‘)are nonnegative monotone submodular func-
tions andğº2(ğ‘Ÿ,ğ‘˜)is the difference of two submodular functions.
Several works have been proposed to maximize the difference be-
tween two set submodular functions in the literature. In this paper,
we utilize the ModMod procedure proposed in [ 27] due to its effi-
ciency. Based on the MM algorithm [ 47], in each iteration ModMod
finds a lower modular bound for ğœ”(ğ‘)and two upper modular
bounds forğ‘£(ğ‘). Bringing them together, two tight modular lower
3764KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Linxiao Yang, Jingbang Yang, & Liang Sun
Table 5: Multi-class Performance: Predicting accuracy (%, standard deviation in parentheses).
Dataset(#classes) # samples # features SSRL SBRL CLASSY RIPPER
iris(3) 150 41 98.0(3.1) 98.0(3.2) 98.0(3.2) 98.0(3.2)
cmc(3) 1473 59 55.26(3.2) 52.75(3.1) 53.29(4.0) 51.73(4.0)
cardio(3) 2126 177 90.40(2.0) 89.27(2.5) 90.12(1.9) 89.70(1.8)
page-blocks(5) 5473 174 96.86(0.6) 95.69(0.8) 96.58(0.7) 96.51(0.7)
dry-bean(7) 13611 288 90.98(0.6) 89.32(0.4) 88.87(0.8) 91.04(0.9)
pendigits(10) 10992 268 96.23(0.5) 89.42(0.7) 95.49(0.8) 96.47(0.5)
Table 6: Interpretability for Binary-class datasets: Number of rules and literals (standard deviation in parentheses).
DatasetSSRL SBRL CORELSâˆ—CLASSY IDRS RIPPER
#
rules #
literals #
rules #
literals #
rules #
literals #
rules #
literals #
rules #
literals #
rules #
literals
FICO-binar
y 7.3(1.7) 18.4(4.2) 14.4(0.8) 23.2(1.8) 4.5(0.7) 14.9(2.4) 19.4(1.7) 65.3(5.1) 21.6(3.3) 134.0(23.8) 16.7(2.1) 106.8(14.3)
FICO 11.6(4.2) 28.2(5.8) 11.7(1.4) 20.3(3.5) 3.9(0.6) 12.7(2.2) 18.2(1.2) 64.9(3.6) 16.0(5.5) 118.6(39.3) 16.0(5.5) 118.6(39.3)
WDBC 5.9(0.7) 10.7(0.9) 3.1(0.9) 5.2(1.5) 6.3(1.4) 18.2(6.4) 4.9(0.3) 7.2(0.6) 8.0(1.1) 27.7(3.0) 5.0(1.1) 10.6(3.0)
gas 10.9(0.8) 27.1(1.8) 42.9(3.6) 85.4(7.3) 7.3(1.2) 25.2(4.4) 27.9(2.1) 71.3(5.8) 13.1(1.0) 74.2(4.2) 24.2(1.8) 106.8(11.6)
magic 33.0(2.8) 90.0(8.0) 27.9(1.9) 51.2(3.4) 6.7(0.5) 21.1(1.8) 72.0(2.5) 226.0(0.9) 19.1(6.6) 136.1(49.2) 52.3(10.6) 391.3(75.0)
adult 10.6(2.2) 45.0(6.0) 39.0(2.9) 73.5(6.3) 4.4(0.7) 14.7(3.1) 98.4(3.0) 368.3(8.7) 9.1(3.1) 83.4(30.7) 42.7(15.2) 337.0(128.9)
COMP
AS-binary 7.7(1.6) 14.6(3.3) 7.8(0.9) 12.7(1.3) 3.3(0.5) 12.3(2.2) 11.7(0.8) 22.6(1.9) 11.4(1.3) 42.2(6.3) 12.0(1.7) 48.1(8.8)
Table 7: Interpretability for Multi-class Datasets: Number of rules and literals (standard deviation in parentheses).
DatasetSSRL SBRL CLASSY RIPPER
# rules # literals # rules # literals # rules # literals # rules # literals
iris 3.0(0.) 3.0(0.) 3.0(0.) 4.0(0.) 3.0(0.) 3.0(0.) 3.1(0.3) 4.8(1.2)
cmc 6.2(1.5) 6.5(1.5) 3.9(0.9) 6.6(1.5) 6.4(1.1) 11.5(1.9) 3.9(0.9) 10.7(3.2)
cardio 8.9(1.2) 19.1(3.0) 9.7(1.1) 18.7(2.2) 13.6(0.9) 39.1(3.6) 14.9(3.0) 55.5(12.6)
page-blocks 9.0(0.89) 18.3(1.27) 11.0(0.8) 21.7(1.7) 19.5(1.2) 56.6(4.3) 16.6(2.2) 60.2(7.5)
dry-bean 16.8(1.2) 46.5(3.1) 27.2(1.4) 54.3(2.9) 42.1(3.1) 115.0(10.5) 51.6(6.2) 223.2(31.6)
pendigits 33.5(1.36) 126.3(4.28) 35.8(2.1) 71.5(4.3) 56.1(3.9) 187.9(10.7) 82.1(2.3) 333.9(15.3)
bounds forğº2(ğ‘Ÿ,ğ‘˜)are obtained. By maximizing it, ModMod can
gradually improve the value of ğº2(ğ‘Ÿ,ğ‘˜). As the global maximizer of
a modular function can be easily derived by collecting all the entries
with positive function values, ModMod can find a local optimum
efficiently.
So far we have discussed how to find an optimal condition given
a positionğ‘˜and a label ğ‘™. By enumerating all possible positions
and labels, and finding the corresponding optimal conditions, the
optimal rule can be selected with highest insertion gain. The method
of finding the optimal solution for maximizing ğº(ğ‘Ÿ,ğ‘˜)is presented
as Algorithm 3. As shown in Algorithm 3, we maintain a set ğ‘ƒ
which contains the rule and position pair candidates. Given an
insert position ğ‘˜, we first select the best rule from set ğ‘…(ğ‘¡)and add
it toğ‘ƒas stated in the 3th and 4th lines in Algorithm 3. Then for
each possible label, we run the ModMod method to learn optimal
condition. We finally get the optimal rule ğ‘Ÿand position ğ‘˜by testing
all the pairs of rule and position in ğ‘ƒ. We note that as inserting
rules to different positions of ğ‘…is independent with each other,
Algorithm 3 can run in a parallel manner to speedup.
Next, we present a sketch for ModMod method. In the ğ‘–th it-
eration of ModMod method, we first generate a modular lower
bound forğœ”(ğ‘)according to Eq. (12), denoted by ğœ”ğœ…(ğ‘–;ğ‘(ğ‘–)), and
two upper bounds for ğ‘£(ğ‘), denoted by Â¯ğ‘£ğœ…
1(ğ‘–;ğ‘(ğ‘–))and Â¯ğ‘£ğœ…
2(ğ‘–;ğ‘(ğ‘–)),
according to Eq. (10)and Eq. (11), respectively. Combining these
bounds together, we achieve two modular bounds of the original
function, and maximizing these bounds can gradually improve the
value of the original function. We note that the maximizer of a
modular function can be easily obtained by collecting all the ele-
ments with positive values. The pseudocode of ModMod method is
present at Algorithm 4.Algorithm 4: ModMod
Input : Submodular function ğ‘¤(ğ‘)andğ‘£(ğ‘)
Output: Best condition ğ‘
1Initializeğ‘(0);
2forj=0,. . . do
3 Choose a permutation ğœ…of[|Î“|]and formğ‘†ğœ…;
4ğ‘1â†{ğ‘–|ğœ”ğœ…(ğ‘–;ğ‘(ğ‘—))âˆ’ğ‘£1(ğ‘–;ğ‘(ğ‘—))â‰¥0};
5ğ‘2â†{ğ‘–|ğœ”ğœ…(ğ‘–;ğ‘(ğ‘—))âˆ’ğ‘£2(ğ‘–;ğ‘(ğ‘—))â‰¥0};
6ğ‘(ğ‘¡+1)â†arg maxğ‘âˆˆ{ğ‘1,ğ‘2}ğº2((ğ‘,ğ‘™),ğ‘˜);
7 ifğ‘(ğ‘—+1)=ğ‘(ğ‘—)then break;
8Return c
6 EXPERIMENTS
In this section, we conduct experiments to demonstrate advan-
tages of the proposed sequence submodular optimization based
rule list learning method, which is referred to as SSRL. We report
the performance of our algorithm in terms of predicting accuracy,
interpretability and scalability.
6.1 Experimental Setup
Datasets. Most datasets in our experiments are from the UCI repos-
itory [ 18] and a few are the variants of the ProPublic recidivism
dataset [ 32] and the Fair Issac credit risk dataset [ 22]. Furthermore,
7 binary datasets and 6 multi-class datasets are included in the ex-
periments, where the number of classes of the multi-class datasets
are3,3,3,5,7,10, respectively. For all models compared in this paper,
we pre-process the features in the same way such that categorical
features are one-hot encoded as ğ‘ğ‘–=ğ‘¡andğ‘ğ‘–â‰ ğ‘¡and numerical
3765Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
features are divided using the sample deciles as thresholds with
binarized features created like ğ‘ğ‘–<ğ‘¡andğ‘ğ‘–â‰¥ğ‘¡.
Baselines. Considering that our proposed algorithm is in the
field of rule list learning, we compare with 3 recent rule list mod-
els as baseline models, including SBRL [ 52], CORELS [ 5], and
CLASSY [ 41]. Both SBRL and CLASSY naturally support multi-
class classifications and all three methods resort to item miners
for candidate rules creation. Notice that, since the performance of
CORELS depends on the maximal number of nodes to search, the
process can be time-consuming when the number of features is
greater than 100, resulting in the average training time of CORELS
larger than an hour to get a decent rule list. To accelerate the train-
ing process of CORELS, we use Random Forest to select the top
30 important features as the input for CORELS. This modification
is denoted as CORELSâˆ—. In addition to these 3 rule list models, 2
rule set models, IDRS [ 51] and RIPPER [ 14], are also included in
our baseline models.
Parameters Tuning. We evaluate the performance of mod-
els using 10-fold stratified cross validation, where the parameters
for all models are tuned. For SBRL, we choose the maximal num-
ber of features from {2,3}avoiding training time longer than our
limit. The minimum support ratio for pre-mining is chosen from
{0.001,0.0001}. For CORELS, the search policy for traversing the
tree is set to objective. The maximal cardinality and the minimal
support when mining the rules are set to 4and0.001, respectively.
To ensure that a sound rule can be achieved, the maximum trie
(cache) size is set to 5Ã—105for all datasets. As for the Random
Forest model used before CORELS training, we fix the parameter
max_depth as 10, and other parameters as default. For CLASSY,
the default parameters setting is used, where the maximal num-
ber of features included in a rule is 5 and the model keeps finding
rules until no more compression can be achieved. For IDRS and
RIPPER, we tune the parameters following the same scheme in [ 51].
For the proposed method, we select the major class of the dataset
as the default class. The trade-off parameter ğœ†is selected from
{0.5,1,5,7,10}using cross validation. The maximal rules allowed ğœˆ
are chosen from{10,20,30,40}.
6.2 Classification Accuracy
The classification accuracy for binary and multi-class datasets are
presented in Table 4 and Table 5, respectively. As shown in Table 4,
the proposed SSRL achieves the highest classification accuracy on
most binary datasets. Rule list based methods SSRL and CLASSY
outperform rule set based methods IDRS and RIPPER on most
datasets, which is consistent with the claim that rule lists are more
expressive than rule sets. The proposed SSRL outperforms SBRL
and CORELS by a big margin. As the search space is restricted
to make it applicable to large-scale dataset, CORELS works the
worst among the rule list learning methods on all the large dataset
(number of samples greater than 10000). From Table 5, it is shown
that the proposed SSRL achieves the highest classification accuracy
on all multi-class datasets. All algorithms work well on the small
dataset iris, while performance of SBRL degrades significantly on
the large dataset â€œpendigitsâ€.6.3 Interpretability
To compare the interpretability of each algorithm, we report the
average length of the rule lists learned by respective methods, i.e.,
|ğ‘…|, as well as the literals used in the rule list, i.e.,Ã
ğ‘Ÿâˆˆğ‘…|ğ‘Ÿ|, presented
in Table 6 and Table reftab:main-result-rule2. As shown in Table
6, except dataset WDBC, the proposed SSRL generates much less
literals than rule set based models IDRS and RIPPER, which is again
due to the better expressiveness of rule lists. Since the proposed
method employs a greedy insert method which is more flexible
than appending, the rules learned by the proposed SSRL are more
compact as compared with CLASSY, i.e., less rules and less literals
involved.
6.4 Scalability
To demonstrate the scalability of the proposed method, we report
the running time of respective rule list learning algorithms on
datasets gasandWDBC under different numbers of features. Specif-
ically, we create new datasets by randomly selecting a fraction of
features from the binarized datasets, and then train the correspond-
ing rule list methods on it and also record their training times. The
process is repeated for 5 times and the average training time of
SSRL, SBRL and CLASSY under different numbers of features are
shown in Fig. 4a. From Fig. 4a, we observe that the proposed SSRL
requires much less training time than SBRL and CLASSY when the
number of features is large. CLASSY slightly outperforms SBRL
when the number of features are larger than 620. We do not in-
clude the comparison of running time with CORELS, as it fails to
learn models with comparable classification accuracy within sev-
eral hours. As shown in Fig. 4b, SSRL achieves a roughly linear
relationship between the training time and the data dimension,
which verifies the sound scalability of our algorithm further.
500 550 600 650 700 750
#features500600700800900100011001200Training time(s)
gas: training time vs number of features
SSRL
CLASSY
SBRL
(a) Training time on gas.
100 200 300 400 500
#features468101214161820time (sec)WDBC: training time vs number of features
SSRL (b) Training time on WDBC.
Figure 4: Scalability Study.
7 CONCLUSION
In this paper, we propose an efficient rule list learning method
based on sequence submodular optimization for both binary and
multi-class classification. By formulating the classification accuracy
as a sequence function which is shown to exhibit forward sequence
submodularity and local backward 1/(ğ¾âˆ’1)-monotonicity, we
propose a unified optimization framework to learn the rule list. A
MM-based method is proposed where two tight lower bounds of
the original sequence function are iteratively maximized using the
proposed greedy insert procedure. Performance lower bound of the
proposed greedy insert method is also provided. Extensive experi-
mental results are provided to show the superiority of our method
in terms of both model classification ability and interpretability.
3766KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Linxiao Yang, Jingbang Yang, & Liang Sun
REFERENCES
[1]Sina Aghaei, Andres Gomez, and Phebe Vayanos. 2020. Learning optimal clas-
sification trees: Strong max-flow formulations. arXiv preprint arXiv:2002.09142
(2020).
[2]GaÃ«l Aglin, Siegfried Nijssen, and Pierre Schaus. 2020. Learning optimal decision
trees using caching branch-and-bound search. In Proceedings of the AAAI, Vol. 34.
3146â€“3153.
[3]Saeed Alaei, Ali Makhdoumi, and Azarakhsh Malekian. 2021. Maximizing
sequence-submodular functions and its application to online advertising. Man-
agement Science 67, 10 (2021), 6030â€“6054.
[4]Saeed Alaei and Azarakhsh Malekian. 2010. Maximizing sequence-submodular
functions. arXiv preprint arXiv:1009.4153 (2010).
[5]Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia
Rudin. 2017. Learning Certifiably Optimal Rule Lists. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
35â€“44.
[6]Onur Asan, Alparslan Emrah Bayrak, and Avishek Choudhury. 2020. Artificial
intelligence and human trust in healthcare: focus on clinicians. Journal of medical
Internet research 22, 6 (2020), e15154.
[7]Martin Atzmueller. 2015. Subgroup Discovery. Wiley Interdisciplinary Reviews:
Data Mining and Knowledge Discovery 5, 1 (2015), 35â€“49.
[8]Sumanta Basu, Karl Kumbier, James B Brown, and Bin Yu. 2018. Iterative random
forests to discover predictive and stable high-order interactions. Proceedings of
the National Academy of Sciences 115, 8 (2018), 1943â€“1948.
[9]Sara Bernardini, Fabio Fagnani, and Chiara Piacentini. 2020. Through the lens
of sequence submodularity. In Proceedings of the International Conference on
Automated Planning and Scheduling, Vol. 30. 38â€“47.
[10] Sara Bernardini, Fabio Fagnani, and Chiara Piacentini. 2021. A unifying look at
sequence submodularity. Artificial Intelligence 297 (2021), 103486.
[11] Dimitris Bertsimas and Jack Dunn. 2017. Optimal classification trees. Machine
Learning 106, 7 (2017), 1039â€“1082.
[12] Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. 2014. Submod-
ular maximization with cardinality constraints. In Proceedings of the twenty-fifth
annual ACM-SIAM symposium on Discrete algorithms. SIAM, 1433â€“1452.
[13] Allison Chang, Dimitris Bertsimas, and Cynthia Rudin. 2012. An Integer Optimiza-
tion Approach to Associative Classification. In Advances in Neural Information
Processing Systems. 269â€“277.
[14] William W Cohen. 1995. Fast Effective Rule Induction. In Proceedings of the 12th
International Conference on Machine Learning. 115â€“123.
[15] Sanjeeb Dash, Oktay Gunluk, and Dennis Wei. 2018. Boolean Decision Rules
via Column Generation. In Advances in Neural Information Processing Systems.
4655â€“4665.
[16] J Richard Dietrich and Robert S Kaplan. 1982. Empirical analysis of the commercial
loan classification decision. Accounting Review (1982), 18â€“38.
[17] Julia Dressel and Hany Farid. 2018. The accuracy, fairness, and limits of predicting
recidivism. Science advances 4, 1 (2018), eaao5580.
[18] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http:
//archive.ics.uci.edu/ml
[19] Jonathan Eckstein, Noam Goldberg, and Ai Kagawa. 2017. Rule-enhanced penal-
ized regression by column generation using rectangular maximum agreement.
InICML. PMLR, 1059â€“1067.
[20] Radwa ElShawi, Youssef Sherif, Mouaz Al-Mallah, and Sherif Sakr. 2020. Inter-
pretability in healthcare: A comparative study of local machine learning inter-
pretability techniques. Computational Intelligence (2020).
[21] Uriel Feige, Vahab S Mirrokni, and Jan VondrÃ¡k. 2011. Maximizing non-monotone
submodular functions. SIAM J. Comput. 40, 4 (2011), 1133â€“1153.
[22] FICO, Google, Imperial College London, MIT, University of Oxford, UC Irvine,
and UC Berkeley. 2018. Explainable Machine Learning Challenge. https:
//community.fico.com/s/explainable-machine-learning-challenge
[23] Bishwamittra Ghosh, Dmitry Malioutov, and Kuldeep S. Meel. 2020. Classification
Rules in Relaxed Logical Form. In Proc. of ECAI.
[24] Bishwamittra Ghosh and Kuldeep S. Meel. 2019. IMLI: An Incremental Framework
for MaxSAT-Based Learning of Interpretable Classification Rules. In Proc. of AIES.
[25] Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and
Lalana Kagal. 2018. Explaining explanations: An overview of interpretability of
machine learning. In 2018 IEEE 5th International Conference on data science and
advanced analytics (DSAA). IEEE, 80â€“89.
[26] Chris Harshaw, Moran Feldman, Justin Ward, and Amin Karbasi. 2019. Sub-
modular maximization beyond non-negativity: Guarantees, fast algorithms, and
applications. In ICML. PMLR, 2634â€“2643.
[27] Rishabh Iyer, Stefanie Jegelka, and Jeff Bilmes. 2013. Fast semidifferential-based
submodular function optimization. In ICML. PMLR, 855â€“863.
[28] Jon Kleinberg, Emily Ryu, and Ã‰va Tardos. 2022. Ordered submodularity and its
applications to diversifying recommendations. arXiv preprint arXiv:2203.00233
(2022).
[29] Andreas Krause and Daniel Golovin. 2014. Submodular function maximization.
Tractability 3 (2014), 71â€“104.[30] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable
decision sets: A joint framework for description and prediction. In Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining. 1675â€“1684.
[31] Himabindu Lakkaraju and Cynthia Rudin. 2017. Learning cost-effective and
interpretable treatment regimes. In Artificial intelligence and statistics. PMLR,
166â€“175.
[32] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. How We
Analyzed the COMPAS Recidivism Algorithm. ProPublica (2016).
[33] Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. 2020.
Generalized and scalable optimal sparse decision trees. In ICML. PMLR, 6150â€“
6160.
[34] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. 2021. Ex-
plainable AI: A Review of Machine Learning Interpretability Methods. Entropy
23, 1 (2021), 18.
[35] Dmitry Malioutov and Kuldeep S. Meel. 2018. MLIC: A MaxSAT-Based framework
for learning interpretable classification rules. In Proceedings of International
Conference on Constraint Programming (CP).
[36] Graziano Mita, Paolo Papotti, Maurizio Filippone, and Pietro Michiardi. 2020.
LIBRE: Learning interpretable boolean rule ensembles. In ICAIS. PMLR, 245â€“255.
[37] Susan A Murphy. 2003. Optimal dynamic treatment regimes. Journal of the Royal
Statistical Society: Series B (Statistical Methodology) 65, 2 (2003), 331â€“355.
[38] Nina Narodytska, Alexey Ignatiev, Filipe Pereira, Joao Marques-Silva, and IS RAS.
2018. Learning Optimal Decision Trees with SAT.. In Ijcai. 1362â€“1368.
[39] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. 1978. An analysis
of approximations for maximizing submodular set functionsâ€”I. Mathematical
programming 14 (1978), 265â€“294.
[40] Pierre Perrault, Jennifer Healey, Zheng Wen, and Michal Valko. 2021. On the
approximation relationship between optimizing ratio of submodular (rs) and
difference of submodular (ds) functions. arXiv preprint arXiv:2101.01631 (2021).
[41] Hugo M ProenÃ§a and Matthijs van Leeuwen. 2020. Interpretable multiclass
classification by MDL-based rule lists. Information Sciences 512 (2020), 1372â€“
1393.
[42] J Ross Quinlan and R Mike Cameron-Jones. 1993. FOIL: A midterm report. In
ECML. Springer, 1â€“20.
[43] Ronald L Rivest. 1987. Learning decision lists. Machine learning 2, 3 (1987),
229â€“246.
[44] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and
Chudi Zhong. 2021. Interpretable Machine Learning: Fundamental Principles
and 10 Grand Challenges. arXiv:2103.11251 [cs.LG]
[45] Cynthia Rudin and Åeyda Ertekin. 2018. Learning customized and optimized
lists of rules with mathematical programming. Mathematical Programming
Computation 10, 4 (2018), 659â€“702.
[46] Matthew Streeter and Daniel Golovin. 2008. An online algorithm for maximizing
submodular functions. NeurIPS 21 (2008).
[47] Ying Sun, Prabhu Babu, and Daniel P. Palomar. 2017. Majorization-Minimization
Algorithms in Signal Processing, Communications, and Machine Learning. IEEE
Transactions on Signal Processing 65, 3 (2017), 794â€“816. https://doi.org/10.1109/
TSP.2016.2601299
[48] Fulton Wang and Cynthia Rudin. 2015. Falling Rule Lists. In Proceedings of
Artificial Intelligence and Statistics (AISTATS).
[49] Tong Wang and Cynthia Rudin. 2015. Learning Optimized Orâ€™s of Andâ€™s.
arXiv:1511.02210 [cs.AI]
[50] Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and
Perry MacNeille. 2017. A bayesian framework for learning rule sets for inter-
pretable classification. The Journal of Machine Learning Research 18, 1 (2017),
2357â€“2393.
[51] Fan Yang, Kai He, Linxiao Yang, Hongxia Du, Jingbang Yang, Bo Yang, and Liang
Sun. 2021. Learning Interpretable Decision Rule Sets: A Submodular Optimization
Approach. NeurIPS 34 (2021).
[52] Hongyu Yang, Cynthia Rudin, and Margo Seltzer. 2017. Scalable Bayesian Rule
Lists. In Proceedings of the 34th International Conference on Machine Learning.
3921â€“3930.
[53] Jinqiang Yu, Alexey Ignatiev, Pierre Le Bodic, and Peter J Stuckey. 2020. Optimal
decision lists using SAT. arXiv preprint arXiv:2010.09919 (2020).
[54] Yichi Zhang, Eric B Laber, Anastasios Tsiatis, and Marie Davidian. 2015. Using
decision lists to construct interpretable and parsimonious treatment regimes.
Biometrics 71, 4 (2015), 895â€“904.
[55] Zhenliang Zhang, Edwin KP Chong, Ali Pezeshki, and William Moran. 2015.
String submodular functions with curvature constraints. IEEE Trans. Automat.
Control 61, 3 (2015), 601â€“616.
[56] Haoran Zhu, Pavankumar Murali, Dzung Phan, Lam Nguyen, and Jayant
Kalagnanam. 2020. A scalable MIP-based method for learning optimal mul-
tivariate decision trees. NeurlIPS 33 (2020), 1771â€“1781.
3767Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A APPENDIX
A.1 Proof of Lemmas
A.1.1 Proof of Lemma 1.
Proof. (1). We prove the first statement of Lemma 1. Let ğ‘…be
a prefix and ğ‘Ÿ1=(ğ‘1,ğ‘™1)andğ‘Ÿ2=(ğ‘2,ğ‘™2)be two rules,Cğ‘…andSğ‘…
as the set of samples that covered and correctly classified by ğ‘…,
respectively. We have
ğ‘(ğ‘…âŠ•ğ‘Ÿ1)=|Sğ‘…|+|Cğ‘…âˆ©C1âˆ©L 1| (27)
ğ‘(ğ‘…âŠ•ğ‘Ÿ1âŠ•ğ‘Ÿ2)=|Sğ‘…|+|Cğ‘…âˆ©C1âˆ©L 1|+|Cğ‘…âˆ©C1âˆ©C2âˆ©L 2|
which brings us
ğ‘(ğ‘…âŠ•ğ‘Ÿ1âŠ•ğ‘Ÿ2)âˆ’ğ‘(ğ‘…âŠ•ğ‘Ÿ1)=|Cğ‘…âˆ©C1âˆ©C2âˆ©L 2|
â‰¤|Cğ‘…âˆ©C2âˆ©L 2|
=|Sğ‘…|+|Cğ‘…âˆ©C2âˆ©L 2|âˆ’|Sğ‘…|
=ğ‘(ğ‘…âŠ•ğ‘Ÿ2)âˆ’ğ‘(ğ‘…). (28)
Then we conclude that ğ‘(ğ‘…)is forward sequence submodular func-
tion.
(2). Next we prove that ğ‘(ğ‘…)is forward monotone function. Let
Sğ‘…1andSğ‘…2be the set of samples that correctly classified by ğ‘…1
andğ‘…2, respectively. LetCğ‘…1denote the set of samples covered by
ğ‘…1, then the statement can be easily proven by
ğ‘(ğ‘…1âŠ•ğ‘…2)âˆ’ğ‘(ğ‘…1)=|Sğ‘…1|+|Cğ‘…1âˆ©Sğ‘…2|âˆ’|Sğ‘…1|
=|Cğ‘…1âˆ©Sğ‘…2|â‰¥0. (29)
(3). The third statement is proven by contradiction. As
ğ‘(ğ‘…1âŠ•ğ‘…2)âˆ’ğ‘(ğ‘…2)=|Sğ‘…1|+|Cğ‘…1âˆ©Sğ‘…2|âˆ’|Sğ‘…2|
=|Sğ‘…1|âˆ’|Cğ‘…1âˆ©Sğ‘…2|. (30)
Letğ‘…1andğ‘…2be the rule lists that cover all the samples, then
anyğ‘…2with higher classification accuracy than ğ‘…1will leads to
|Sğ‘…1|âˆ’|Cğ‘…1âˆ©Sğ‘…2|=|Sğ‘…1|âˆ’|Sğ‘…2|<0.
(4). The submodularity of ğ‘‘(ğ‘…)comes from the property of union
operation.
(5). Asğ‘¢(ğ‘…)is a summation over all the gains of elements in ğ‘…,
it is a modular function. â–¡
A.1.2 Proofs of Theorem 2 and Corollary 1. We only prove Corol-
lary 1, as Theorem 2 holds naturally if Corollary 1 holds.
Proof. LetË†ğ‘…(ğ‘–)and Ë†ğ‘…(ğ‘–+1)denote the sequences generated by
Algorithm 1 at the ğ‘–-th and(ğ‘–+1)-th iteration, respectively. Assume
Ë†ğ‘…(ğ‘–+1)is obtained by inserting element ğ‘Ÿâ€²to theğ‘—-th position of Ë†ğ‘…(ğ‘–).
Letğ‘…âˆ—=(ğ‘Ÿâˆ—
1,...,ğ‘Ÿâˆ—
|ğ‘…âˆ—|)be the optimal sequence that maximizes
Ëœğ‘“(ğ‘…). Then we have
ğ›¼ğ‘–+1(ğœ“(Ë†ğ‘…(ğ‘–)âŠ•ğ‘…âˆ—)âˆ’ğœ“(Ë†ğ‘…(ğ‘–)))âˆ’ğœ‹(ğ‘…âˆ—)
=|ğ‘…âˆ—|âˆ‘ï¸
ğ‘¡=1ğ›¼ğ‘–+1(ğœ“(ğ‘‚ğ‘¡)âˆ’ğœ“(ğ‘‚ğ‘¡âˆ’1))âˆ’ğœ‹(ğ‘Ÿâˆ—
ğ‘¡)=|ğ‘…âˆ—|âˆ‘ï¸
ğ‘¡=1ğ›½ğ‘¡, (31)
whereğ›½ğ‘¡=ğ›¼ğ‘–+1(ğœ“(ğ‘‚ğ‘¡)âˆ’ğœ“(ğ‘‚ğ‘¡âˆ’1))âˆ’ğœ‹(ğ‘Ÿâˆ—
ğ‘¡),ğ‘‚ğ‘¡denotes the sequence
obtained by appending (ğ‘Ÿâˆ—
1,...,ğ‘Ÿâˆ—
ğ‘¡)toË†ğ‘…(ğ‘–), withğ‘‚0=Ë†ğ‘…(ğ‘–)andğ‘‚|ğ‘…âˆ—|=Ë†ğ‘…(ğ‘–)âŠ•ğ‘…âˆ—. Letğ›½ğœ‰be the maximal of set {ğ›½ğ‘¡}|ğ‘…âˆ—|
ğ‘¡=1, then we
have
ğ›¼ğ‘–+1(ğœ“(Ë†ğ‘…(ğ‘–)âŠ•ğ‘…âˆ—)âˆ’ğœ“(Ë†ğ‘…(ğ‘–)))âˆ’ğœ‹(ğ‘…âˆ—)
=|ğ‘…âˆ—|âˆ‘ï¸
ğ‘¡=1ğ›¼ğ‘–+1(ğœ“(ğ‘‚ğ‘¡)âˆ’ğœ“(ğ‘‚ğ‘¡âˆ’1))âˆ’ğœ‹(ğ‘Ÿâˆ—
ğ‘¡)
(ğ‘)
â‰¤|ğ‘…âˆ—|(ğ›¼ğ‘–+1(ğœ“(ğ‘‚ğœ‰)âˆ’ğœ“(ğ‘‚ğœ‰âˆ’1))âˆ’ğœ‹(ğ‘Ÿâˆ—
ğœ‰))
(ğ‘)
â‰¤ğœˆ(ğ›¼ğ‘–+1(ğœ“(ğ‘‚ğœ‰)âˆ’ğœ“(ğ‘‚ğœ‰âˆ’1))âˆ’ğœ‹(ğ‘Ÿâˆ—
ğœ‰))
(ğ‘)
â‰¤ğœˆ(ğ›¼ğ‘–+1(ğœ“(Ë†ğ‘…(ğ‘–)âŠ•ğ‘Ÿâˆ—
ğœ‰))âˆ’ğœ“(Ë†ğ‘…(ğ‘–))âˆ’ğœ‹(ğ‘Ÿâˆ—
ğœ‰))
=ğœˆ(ğ›¼ğ‘–+1ğœ“(ğ‘Ÿâˆ—
ğœ‰,ğ‘–+1|Ë†ğ‘…(ğ‘–))âˆ’ğœ‹(ğ‘Ÿâˆ—
ğœ‰))
(ğ‘‘)
â‰¤ğœˆ(ğ›¼ğ‘–+1ğœ“(ğ‘Ÿâ€²,ğ‘—|Ë†ğ‘…(ğ‘–))âˆ’ğœ‹(ğ‘Ÿâ€²))
=ğœˆ(ğ›¼ğ‘–+1(ğœ“(Ë†ğ‘…(ğ‘–+1))âˆ’ğœ“(Ë†ğ‘…(ğ‘–)))âˆ’ğœ‹(Ë†ğ‘…(ğ‘–+1))+ğœ‹(Ë†ğ‘…(ğ‘–)))),(32)
where(ğ‘)comes from the assumption that ğ›½ğœ‰is the maximal of
{ğ›½ğ‘¡}|ğ‘…âˆ—|
ğ‘¡=1,(ğ‘)comes from|ğ‘…âˆ—| â‰¤ğœˆ,(ğ‘)comes from the forward
submodularity of ğœ“(ğ‘…), and(ğ‘‘)comes from the optimality of (ğ‘Ÿâ€²,ğ‘—).
Recall the assumption that ğœ“(ğ‘…)is backward ğœ‚-monotone on the
set{Ë†ğ‘…(ğ‘–)}ğœˆ
ğ‘–=1âˆª{ğ‘…âˆ—}, then we have ğœ“(Ë†ğ‘…(ğ‘–)âŠ•ğ‘…âˆ—)â‰¥ğœ‚ğœ“(ğ‘…âˆ—), which
bring us
ğ›¼ğ‘–+1(ğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ“(Ë†ğ‘…(ğ‘–)))âˆ’ğœ‹(ğ‘…âˆ—)
â‰¤ğ›¼ğ‘–+1(ğœ“(Ë†ğ‘…(ğ‘–)âŠ•ğ‘…âˆ—)âˆ’ğœ“(Ë†ğ‘…(ğ‘–)))âˆ’ğœ‹(ğ‘…âˆ—)
â‰¤ğœˆ(ğ›¼ğ‘–+1(ğœ“(Ë†ğ‘…(ğ‘–+1))âˆ’ğœ“(Ë†ğ‘…(ğ‘–))âˆ’ğœ‹(Ë†ğ‘…(ğ‘–+1))+ğœ‹(Ë†ğ‘…(ğ‘–)))). (33)
By rearranging the inequality, we have
ğ›¼ğ‘–+1ğœ“(Ë†ğ‘…(ğ‘–+1))âˆ’ğ›¼ğ‘–ğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(Ë†ğ‘…(ğ‘–+1))
â‰¥(1âˆ’1
ğœˆ)ğ›¼ğ‘–+1ğœ“(Ë†ğ‘…(ğ‘–))âˆ’( 1âˆ’1
ğœˆ)ğ›¼ğ‘–+1ğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(Ë†ğ‘…(ğ‘–))âˆ’1
ğœˆğœ‹(ğ‘…âˆ—)
=ğ›¼ğ‘–ğœ“(Ë†ğ‘…(ğ‘–))âˆ’ğ›¼ğ‘–ğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(Ë†ğ‘…(ğ‘–))âˆ’1
ğœˆğœ‹(ğ‘…âˆ—). (34)
Then we have
ğ›¼ğœˆğœ“(Ë†ğ‘…(ğœˆ))âˆ’ğ›¼ğœˆğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(Ë†ğ‘…(ğœˆ))
â‰¥ğ›¼0ğœ“(Ë†ğ‘…(0))âˆ’ğ›¼0ğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(Ë†ğ‘…(0))âˆ’ğœ‹(ğ‘…âˆ—)
(ğ‘)=âˆ’(1âˆ’1
ğœˆ)ğœˆğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(ğ‘…âˆ—)
â‰¥âˆ’1
ğ‘’ğœ‚ğ‘(ğ‘…âˆ—)âˆ’ğœ‹(ğ‘…âˆ—), (35)
where(ğ‘)comes from the fact that ğœ“(âˆ…)=0andğœ‹(âˆ…)=0. As
ğ›¼ğœˆ=1, we have
ğœ“(Ë†ğ‘…(ğœˆ))âˆ’ğœ‹(Ë†ğ‘…(ğœˆ))=ğ›¼ğœˆğœ“(Ë†ğ‘…(ğœˆ))âˆ’ğœ‹(Ë†ğ‘…(ğœˆ))
â‰¥(1âˆ’1
ğ‘’)ğœ‚ğœ“(ğ‘…âˆ—)âˆ’ğœ‹(ğ‘…âˆ—), (36)
which completes our proof. â–¡
A.1.3 Proof of Lemma 3. We prove Lemma 3 by first showing that
ğ‘(ğ‘…)is generally backward ğœ‚-monotone on a set Rğœ‚, and then prove
that all the prefixes generated by Algorithm 1 are lying in R1/(ğ¾âˆ’1).
We first have following result.
3768KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Linxiao Yang, Jingbang Yang, & Liang Sun
Lemma 7. LetRğœ‚be the set formed by all the prefixes with accuracy
on the samples whose label different to ğ‘™ğ‘‘no less thanğœ‚. Thenğ‘(ğ‘…)is
backwardğœ‚-monotone onRğœ‚.
Proof. LetCğ‘…be the set of samples covered by ğ‘…,Sğ‘…andSğ‘…â€²de-
note the set of samples correctly classified by ğ‘…andğ‘…â€², respectively.
Asğ‘(ğ‘…)counts the number of samples that correctly classified by
ğ‘…, thenğ‘(ğ‘…âŠ•ğ‘…â€²)=|Sğ‘…|+|Cğ‘…âˆ©Sğ‘…â€²|andğ‘(ğ‘…â€²)=|Sğ‘…â€²|. Recall
thatğœ‚=|Sğ‘…|/|Cğ‘…|, then we have
ğ‘(ğ‘…âŠ•ğ‘…â€²)=|Sğ‘…|+|Cğ‘…âˆ©Sğ‘…â€²|=|Sğ‘…âˆ©Lğ‘‘|+|Cğ‘…âˆ©Sğ‘…â€²âˆ©Lğ‘‘|
=ğœ‚|Cğ‘…âˆ©Lğ‘‘|+|Cğ‘…âˆ©Sğ‘…â€²âˆ©Lğ‘‘|
â‰¥ğœ‚|Cğ‘…âˆ©Lğ‘‘|+ğœ‚|Cğ‘…âˆ©Sğ‘…â€²âˆ©Lğ‘‘|
â‰¥ğœ‚|Cğ‘…âˆ©Sğ‘…â€²âˆ©Lğ‘‘|+ğœ‚|Cğ‘…âˆ©Sğ‘…â€²âˆ©Lğ‘‘|
=ğœ‚|Sğ‘…â€²âˆ©Lğ‘‘|=ğœ‚ğ‘(ğ‘…â€²), (37)
which complete our proof. â–¡
We then show that all the prefixes generated by Algorithm 1 are
lying onR1/(ğ¾âˆ’1).
Lemma 8. LetË†ğ‘…(ğ‘–)be the prefix generated by Algorithm 1 at the
ğ‘–-th iteration, if Ë†ğ‘…(ğ‘–)is not empty sequence, then the classification
accuracy of Ë†ğ‘…(ğ‘–)on samples whose label different to ğ‘™ğ‘‘is no less than
1/(ğ¾âˆ’1), whereğ¾is the number of classes.
Proof. Our proof is based on induction. Obviously, Lemma 8
holds for Ë†ğ‘…(1), as one can always set the label of the rule in Ë†ğ‘…(1)
to the majority class it covers, in which case, its accuracy is no
less than 1/(ğ¾âˆ’1). Assuming the accuracy of Ë†ğ‘…(ğ‘–)is no less than
1/(ğ¾âˆ’1), we now prove that the accuracy of Ë†ğ‘…(ğ‘–+1)is also greater
than 1/(ğ¾âˆ’1). Suppose Ë†ğ‘…(ğ‘–+1)is obtained by inserting a rule ğ‘Ÿ
into theğ‘—th position of Ë†ğ‘…(ğ‘–). We discuss the accuracy of Ë†ğ‘…(ğ‘¡)âŠ•ğ‘Ÿâ€²,
whereğ‘Ÿâ€²is the rule obtained by changing the label of ğ‘Ÿsuch that the
accuracy of Ë†ğ‘…(ğ‘¡)âŠ•ğ‘Ÿâ€²is greater than 1/ğ¾. Noted that Ë†ğ‘…(ğ‘¡)âŠ•ğ‘Ÿâ€²and
Ë†ğ‘…(ğ‘¡+1)share the same values for functions ğ‘‘1(ğ‘…;Ë†ğ‘…(ğ‘–)),ğ‘‘2(ğ‘…;Ë†ğ‘…(ğ‘–)),
andğ‘¢(ğ‘…). Hence, the optimality of (ğ‘Ÿ,ğ‘—)concludes that the accuracy
ofË†ğ‘…(ğ‘–+1)is no less than Ë†ğ‘…(ğ‘–)âŠ•ğ‘Ÿâ€²and consequently no less than
1/(ğ¾âˆ’1). â–¡
Lemma 8 shows that the prefixes generated by Algorithm 1 at
each iteration have accuracy no less than 1/(ğ¾âˆ’1), if they are not
empty. Obviously, the accuracy of optimal prefix ğ‘…âˆ—is no less than
the prefixes we learned. Then we can conclude that all of them are
lying onR1/(ğ¾âˆ’1). Recall the result in Lemma 7, we arrive at the
claim in Lemma 3, which completes our proof.
A.1.4 Proof of Lemma 5.
Proof. We note that as it is trivial to proof the lemma if ğ‘Ÿ2âˆˆ
Î©\ğ‘…(ğ‘¡), then we only discuss the scenario ğ‘Ÿ2âˆˆğ‘…(ğ‘¡). Letğ‘Ÿâ€²
2=
arg maxğ‘ŸâˆˆÎ©\ğ‘…(ğ‘¡)ğº2(ğ‘Ÿ,ğ‘˜). Due to the optimality of ğ‘Ÿ2, we haveğº2(ğ‘Ÿ2,ğ‘˜)â‰¥
ğº2(ğ‘Ÿâ€²
2,ğ‘˜). Asğ‘‘is submodular, then ğ‘‘(ğ‘Ÿ2|âˆ…) â‰¥ğ‘‘(ğ‘Ÿ2|ğ‘…(ğ‘¡)\{ğ‘Ÿ2}),
which consequently brings ğº2(ğ‘Ÿ2,ğ‘˜)â‰¤ğº1(ğ‘Ÿ2,ğ‘˜)â‰¤ğº1(ğ‘Ÿ1,ğ‘˜), where
the last inequality comes from the definition of ğ‘Ÿ1. Then it is clear
that ifğ‘Ÿ2âˆˆğ‘…(ğ‘¡), one can certainly claim that ğ‘Ÿ1is the maximizer of
ğº(ğ‘Ÿ,ğ‘˜), which completes the proof. â–¡Table 8: Learned rule list by SSRL on COMPAS-binary dataset.
conditions label
IFjuvenile-crimes:!=0 AND
juvenile-misdemeanors:=01
EISE IF â€™age:>26 AND priors:2-3â€™ 0
EISE IF â€™age:<21 AND sex: not Femaleâ€™ 1
EISE IF â€™priors:!=0 AND priors:!=1â€™ 1
EISE 0
Table 9: Learned rule list by SSRL on Iris dataset.
conditions label
IF â€™petal length <= 2.63â€™ Iris-setosa
EISE IF â€™petal length <= 4.9 AND petal width <= 1.6â€™ Iris-versicolor
EISE Iris-virginica
A.1.5 Proof of Lemma 6.
Proof. Given(C<ğ‘–âˆ©Cğ‘–âˆ©Lğ‘–)âˆ©(C<ğ‘—âˆ©Cğ‘—âˆ©Lğ‘—)=âˆ…,âˆ€ğ‘–â‰ ğ‘—,
we rewriteğ‘(ğ‘…)=|Sâ‰¤ğ‘˜|+|S >ğ‘˜|, where
Sâ‰¤ğ‘˜=ğ‘˜Ã˜
ğ‘–=1
C<ğ‘–âˆ©Cğ‘–âˆ©Lğ‘–
,S>ğ‘˜=|ğ‘…|Ã˜
ğ‘–=ğ‘˜+1
C<ğ‘–âˆ©Cğ‘–âˆ©Lğ‘–
.(38)
Then we have the number of correctly classified samples after the
insertion
ğ‘(I(ğ‘…,ğ‘˜,ğ‘Ÿ))=|Sâ‰¤ğ‘˜|+|Câ‰¤ğ‘˜âˆ©Câˆ©L|+|S >ğ‘˜âˆ©C| (39)
and the marginal gain
ğ‘(ğ‘Ÿ,ğ‘˜|ğ‘…)=|Câ‰¤ğ‘˜âˆ©Câˆ©L|âˆ’|S >ğ‘˜âˆ©C|. (40)
Consequently,
ğº2(ğ‘Ÿ,ğ‘˜)=ğ›¼ğ‘(ğ‘Ÿ,ğ‘˜|ğ‘…)âˆ’ğ‘‘(ğ‘Ÿ|âˆ…)âˆ’ğœ†|ğ‘|
=ğ›¼|Câ‰¤ğ‘˜âˆ©Câˆ©L|âˆ’ ğ›¼|S>ğ‘˜âˆ©C|âˆ’|Câˆ©L ğ‘‘|âˆ’ğœ†|ğ‘|
(ğ‘)âˆâˆ’ğ›¼|Câ‰¤ğ‘˜âˆ©Câˆ©L|+ğ›¼|S>ğ‘˜âˆ©C|+|Câˆ©Lğ‘‘|âˆ’ğœ†|ğ‘|,
(41)
where(ğ‘)comes from that|Aâˆ©C| =|A|âˆ’|Aâˆ©C|âˆâˆ’|Aâˆ©C|.
We define
ğœ”(ğ‘)=ğ›¼|S>ğ‘˜âˆ©C|+|Câˆ©Lğ‘‘|
=ğ›¼S>ğ‘˜âˆ©
âˆªğ‘’âˆˆğ‘E+
âˆªğ‘’âˆˆğ‘E
âˆ©Lğ‘‘, (42)
ğ‘£(ğ‘)=ğ›¼|Câ‰¤ğ‘˜âˆ©Câˆ©L|+ğœ†|ğ‘|
=ğ›¼Câ‰¤ğ‘˜âˆ©
âˆªğ‘’âˆˆğ‘E
âˆ©L+ğœ†|ğ‘|, (43)
whereEdenotes the set of samples containing feature ğ‘’,C=âˆ©ğ‘’âˆˆğ‘E,
andC=âˆªğ‘’âˆˆğ‘E, we complete our proof at
ğº2(ğ‘Ÿ,ğ‘˜)âˆğœ”(ğ‘)âˆ’ğ‘£(ğ‘). (44)
â–¡
A.2 Example rule lists generated by proposed
SSRL
To further illustrate the interpretability of the proposed method,
we present the rule list generated by our method SSRL on datasets
COMPAS-binary and Iris, as summarized in Table 8 and Table 9,
respectively.
3769