Resurrecting Label Propagation for Graphs with Heterophily and
Label Noise
Yao Cheng
East China Normal University
Shanghai, China
yaocheng_623@stu.ecnu.edu.cnCaihua Shan
Microsoft Research Asia
Shanghai, China
caihuashan@microsoft.comYifei Shen
Microsoft Research Asia
Shanghai, China
yshenaw@connect.ust.hk
Xiang Liâˆ—
East China Normal University
Shanghai, China
xiangli@dase.ecnu.edu.cnSiqiang Luo
Nanyang Technological University
Singapore, Singapore
siqiang.luo@ntu.edu.sgDongsheng Li
Microsoft Research Asia
Shanghai, China
dongsheng.li@microsoft.com
ABSTRACT
Label noise is a common challenge in large datasets, as it can signif-
icantly degrade the generalization ability of deep neural networks.
Most existing studies focus on noisy labels in computer vision;
however, graph models encompass both node features and graph
topology as input, and become more susceptible to label noise
through message-passing mechanisms. Recently, only a few works
have been proposed to tackle the label noise on graphs. One sig-
nificant limitation is that they operate under the assumption that
the graph exhibits homophily and that the labels are distributed
smoothly. However, real-world graphs can exhibit varying degrees
of heterophily, or even be dominated by heterophily, which results
in the inadequacy of the current methods.
In this paper, we study graph label noise in the context of ar-
bitrary heterophily, with the aim of rectifying noisy labels and
assigning labels to previously unlabeled nodes. We begin by con-
ducting two empirical analyses to explore the impact of graph
homophily on graph label noise. Following observations, we pro-
pose a efficient algorithm, denoted as ğ‘…2LP. Specifically, ğ‘…2LP is
an iterative algorithm with three steps: (1) reconstruct the graph
to recover the homophily property, (2) utilize label propagation
to rectify the noisy labels, (3) select high-confidence labels to re-
tain for the next iteration. By iterating these steps, we obtain a
set of â€œcorrectâ€ labels, ultimately achieving high accuracy in the
node classification task. The theoretical analysis is also provided to
demonstrate its remarkable denoising effect. Finally, we perform
experiments on ten benchmark datasets with different levels of
graph heterophily and various types of noise. In these experiments,
we compare the performance of ğ‘…2LP against ten typical baseline
methods. Our results illustrate the superior performance of the
proposedğ‘…2LP. The code and data of this paper can be accessed at:
https://github.com/cy623/R2LP.git.
âˆ—Xiang Li is the corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671774CCS CONCEPTS
â€¢Computing methodologies â†’Artificial intelligence; Neural
networks.
KEYWORDS
Graphs with Heterophily; Graph Label Noise; Graph Neural Net-
works
ACM Reference Format:
Yao Cheng, Caihua Shan, Yifei Shen, Xiang Li, Siqiang Luo, and Dongsheng
Li. 2024. Resurrecting Label Propagation for Graphs with Heterophily and
Label Noise. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671774
1 INTRODUCTION
Graphs are ubiquitous in the real world, such as social networks [ 14],
biomolecular structures [ 37] and knowledge graphs [ 9]. In graphs,
nodes and edges represent entities and their relationships respec-
tively. To enrich the information of graphs, each node is usually
associated with a descriptive label. For example, each user in Face-
book has interest as its label. Recently, to learn from graphs, graph
neural networks (GNNs) have achieved significant success, which
can seamlessly integrate node features with graph structure to learn
node representations. GNNs generally adopt the message passing
strategy to aggregate information from nodesâ€™ neighbors and then
update node representations. After that, the learned representations
are fed into various downstream tasks, such as node classification.
Despite their great performance, GNNs still encounter challenges
from label noise, which is a pervasive issue in real-world datasets.
For example, non-expert sources like Amazonâ€™s Mechanical Turk
and web pages can be used to collect labels, which could uninten-
tionally lead to unreliable labels and noise. It is well known that
deep neural networks are very sensitive to noise, thereby degrading
their generalization ability [ 33]. To address the problem, there have
been a majority of neural network models [ 10,18,20,31] proposed
in the field of computer vision (CV) to tackle label noise in images.
Unfortunately, we cannot directly apply these approaches to graph-
structured data as graphs are non-Euclidean data types. Therefore,
exploring the issue of graph label noise is highly necessary.
Recently, very few works have been proposed to deal with graph
label noise [ 4,29]. These models still attenuate graph label noise
 
433
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yao Cheng et al.
(a) Cornell(b) Actor0.40.60.81.0
0%20%40%60%AccuracyNoise RatioGCNGloGNNH2GCNNRGNNLPM0.20.30.40.5
0%20%40%60%AccuracyNoise RatioGCNGloGNNH2GCNNRGNNLPM
Figure 1: The classification accuracy with flip label noise
ranging from 0% to 60% on Cornell and Actor datasets.
based on an explicit homophily assumption that linked nodes in a
graph are more likely to be with the same label. However, ho-
mophily is not a universal principle in the real world. In fact,
graph datasets often exhibit varying degrees of homophily, and
may even be heterophily-dominated. Methods based on the ho-
mophily assumption evidently cannot handle these heterophilous
graphs. As shown in Figure 1, we compare the performance of GCN,
H2GCN [ 35] and GloGNN [ 16], which are specially designed for
homophilous and heterophilous graphs, against two state-of-the-art
methods for graph label noise, LPM [ 29] and NRGNN [ 4], on the
heterophilous benchmark dataset Cornell andActor with flip noise1
ranging from 0% to 60%. The results indicate that on heterophilous
graphs, current methods for label noise (LPM and NRGNN), are still
underperformed by GNN models designed for graph heterophily
(H2GCN and GloGNN). The problem of graph label noise deserves
further investigation.
In this paper, we study graph label noise in the context of ar-
bitrary heterophily, with the aim of rectifying noisy labels and
assigning labels to previously unlabeled nodes. We first explore the
impact of graph homophily on graph label noise by conducting a
series of experiments on benchmark heterophilous datasets. Specif-
ically, we manipulate graph homophily by graph reconstruction
to see if existing methods could improve their classification per-
formance against label noise. The observations demonstrate that a
high graph homophily rate can indeed mitigate the effect of graph
label noise on GNN performance. In addition, we find that label
propagation (LP) [ 36] based methods achieve great performance as
graph homophily increases.
These inspirations led us to propose a effective algorithm, ğ‘…2LP,
incorporating graph Reconstruction for homophily and noisy label
Rectification by Label Propagation. We further refine this basic
solution in several aspects. In terms of effectiveness, we implement
a multi-round selection process as opposed to correcting noisy
labels all at once. Specifically, we select high-confidence noisy labels
to augment the correct label set, and then repeat the algorithm,
achieving higher accuracy. With respect to efficiency, we reduce
the time complexity to linear time by unifying the computation of
graph construction and label propagation. Ultimately, we provide
a theoretical proof of ğ‘…2LP, illustrating its impressive denoising
effect in addressing graph label noise.
To summarize, our main contributions are:
1Flip noise and uniform noise are explained in Section 6.1.â€¢Empirical Analysis: We first investigate the influence of graph
homophily on label noise through the manipulation of homophily
levels and noise ratios within real-world datasets. Moreover, we
incorporate three common homophilous graph reconstruction mod-
ules into existing approaches, to evaluate the performance improve-
ment of these approaches in heterophilous graphs.
â€¢Algorithm Design and Theoretical Proof: Based on empirical
observations, we propose a effective algorithm ğ‘…2LP to deal with
the issue of graph label noise. Specifically, ğ‘…2LP is an iterative
algorithm with three steps: (1) reconstruct the graph to recover
the homophily property, (2) utilize label propagation to correct
the noisy labels, (3) select high-confidence corrected labels and
add them to the clean label set to retain for the next iteration. By
repeating these steps, we obtain a set of â€œcorrectâ€ labels to achieve
accurate node classification. We also provide a theoretical analysis
of the denoising effect of ğ‘…2LP.
â€¢Extensive Experiments: We conduct extensive experiments on
two homophilous datasets and eight heterophilous datasets with
varying noise types and ratios. Our proposed ğ‘…2LP outperforms
baselines in the majority of cases, demonstrating the superior per-
formance of ğ‘…2LP. The ablation studies are also done to verify the
necessity of each component of ğ‘…2LP.
2 PRELIMINARIES
Problem Definition. We consider the semi-supervised node classi-
fication task in an undirected graph G=(V,E)whereV={ğ‘£ğ‘–}ğ‘›
ğ‘–=1
is a set of nodes and EâŠ†VÃ—V is a set of edges. Let the label set
asYwithğ‘classes, andV=CâˆªNâˆªU , whereCis set of labeled
nodes with clean labels ğ‘ŒC,Nis a set of labeled nodes with noise
labelsğ‘ŒNandUis set of unlabeled nodes. In the paper, ğ‘ŒCdenotes
the set of known clean labels, while ğ‘ŒNrepresents the collection
containing a certain proportion of noisy labels. Both of them are
given as inputs. It is unknown which labels in ğ‘ŒNare erroneous.
Letğ´be the adjacency matrix of Gsuch thatğ´ğ‘–ğ‘—=1if there exists
an edge between nodes ğ‘£ğ‘–andğ‘£ğ‘—; 0, otherwise. For the initial node
feature matrix, we denote it as ğ‘‹. Our goal is to design an effective
algorithm to predict the class label for noisy and unlabeled nodes.
GNN and Label Propagation Basics. Most graph neural networks
follow the message-passing mechanism, which consists of two main
steps: aggregation and update. GNNs first aggregate information
from neighboring nodes Ë†â„(ğ‘™)
ğ‘–=AGGREGATE(â„(ğ‘™âˆ’1)
ğ‘—,âˆ€ğ‘£ğ‘—âˆˆNğ‘–),
and then update each node embeddings â„(ğ‘™)
ğ‘–=UPDATE(â„(ğ‘™âˆ’1)
ğ‘–,Ë†â„(ğ‘™)
ğ‘–).
Afterğ¿aggregations, the final node embedding ğ»(ğ¿)is obtained
and then fed into a softmax layer to generate probability logits for
label prediction. Different from GNNs, label propagation directly
aggregates the labels from neighboring nodes through the formula
ğ‘Œ(ğ‘™)
ğ‘–=(1âˆ’ğ›¼)Â·ğ‘Œ(ğ‘™âˆ’1)
ğ‘–+ğ›¼Â·AGGREGATE(ğ‘Œ(ğ‘™âˆ’1)
ğ‘—,âˆ€ğ‘£ğ‘—âˆˆNğ‘–). It con-
verges when labels do not change significantly between iterations.
Depending on the various utilization of labels, the robustness of
GNNs and LP to graph label noise is also different.
3 HOW DOES HOMOPHILY IMPACT GRAPH
LABEL NOISE?
In this section, we investigate the potential of mitigating the effect
of graph label noise by decreasing the graph heterophily rate. To do
 
434Resurrecting Label Propagation for Graphs with Heterophily and Label Noise KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
GCN(a) Chameleon â€“Uniform noise0.20.40.60.81.00.230.330.430.530.63AccuracyEdge Hom. Noise Ratio = 0%Noise Ratio = 20%Noise Ratio = 40%Noise Ratio = 60%
0.20.40.60.81.0
0.230.330.430.530.63AccuracyEdge Hom. 0.20.40.60.81.0
0.230.330.430.530.63AccuracyEdge Hom. 0.20.40.60.81.0
0.230.330.430.530.63AccuracyEdge Hom. 0.20.40.60.81.0
0.230.330.430.530.63AccuracyEdge Hom. NRGNNLP
0.00.20.40.60.8
0.220.320.420.520.62AccuracyEdge Hom. 0.00.20.40.60.8
0.220.320.420.520.62AccuracyEdge Hom. 0.00.20.40.60.8
0.220.320.420.520.62AccuracyEdge Hom. 0.00.20.40.60.8
0.220.320.420.520.62AccuracyEdge Hom. GCNLPM(a)Chameleon (b) Actor LPGCNNRGNNLPM
Figure 2: The impact of edge homophily on graph label noise across various methods (flip noise)
this, we first outline existing approaches for graph label noise. Next,
we introduce three graph reconstruction modules to transform the
original graphs into homophilous ones. Ultimately, we modify the
graph structure in heterophilous datasets through two manipula-
tions: (1) converting the heterophilous edges into homophilous
ones based on the ground-truth labels and (2) reconstructing the
homophilous graph. We then apply the existing approaches to the
modified datasets and several critical observations are derived from
these experiments.
3.1 Existing Approaches
Here we introduce two state-of-the-art methods for graph label
noise, LPM [29] and NRGNN [4].
â€¢LPM is proposed to tackle graph label noise by combining label
propagation and meta-learning. It consists of three modules: a GNN
for feature extraction, LP for pseudo-label generation and an aggre-
gation network for merging labels. Initially, the similarity matrix
ğ´â€²
ğ‘–ğ‘—=ğ´ğ‘–ğ‘—
ğ‘‘(â„ğ‘–,â„ğ‘—)+ğœis calculated based on the adjacency matrix ğ´and
the node embeddings â„ğ‘–andâ„ğ‘—. These embeddings are learned
by the GNN feature extraction. The function ğ‘‘(Â·,Â·)represents a
distance measure, and ğœis an infinitesimal constant. Then LP is
performed on ğ´â€²to generate pseudo-labels. Lastly, the aggregation
network is used to integrate the original labels and pseudo-labels,
resulting in the final labels through meta-learning.
â€¢NRGNN is also designed to deal with graph label noise and label
sparsity, including three modules: an edge predictor, a pseudo-label
predictor, and a GNN classifier. The edge predictor first generates
extra edges to connect similar labeled and unlabeled nodes by the
link prediction task. As a result, the newly generated adjacency ma-
trixğ´â€²becomes denser. Then the pseudo-label predictor estimates
pseudo-labels ğ‘Œâ€²for unlabeled nodes. The GNN classifier outputs
the final predictions based on the updated ğ´â€²andğ‘Œâ€².Formally, the overall process of these two methods can be for-
mulated into three steps:
ğ´â€²=ğ‘“reconstruct(ğ´,ğ‘‹),
ğ‘Œâ€²=ğ‘“pseudo-label(ğ‘Œ,ğ´â€²),
ğ‘Œfinal=ğ‘“predict(ğ‘‹,ğ´â€²,ğ‘Œ,ğ‘Œâ€²).(1)
Note that both LPM and NRGNN depend on the graph homophily
assumption. Further, they mainly concentrate on pseudo-label gen-
eration, while overlooking the correction of noisy labels.
3.2 Graph Reconstruction with Homophily
While both LPM and NRGNN reconstruct the original graph, their
methods have clear limitations. For LPM, it only adjusts the weights
of raw edges in the graph, which cannot add new edges. Further,
NRGNN employs link prediction as objective to predict new edges,
which takes existing edges in the graph as positive samples. This
restricts its wide applicability in graphs with heterophily. Therefore,
we next introduce three common methods to convert heterophilous
graphs into homophilous ones.
First, the primary goal is to directly construct a similarity matrix
that serves as the homophilous graph. Specifically, we train a GNN
ğ‘“(Â·)to aggregate node features and topological structure, and ob-
tain the hidden representation â„ğ‘–for nodeğ‘£ğ‘–. We then proceed to
calculate the distance between nodes by utilizing the cosine simi-
larity metric. The existence of edge ğ‘ğ‘–ğ‘—between nodes ğ‘£ğ‘–andğ‘£ğ‘—is
decided by a fixed threshold ğœ–:
ğ‘ğ‘–ğ‘—=1if distance(â„ğ‘–,â„ğ‘—)<ğœ–; 0,otherwise (2)
This method can be considered as a direct extension to the one used
by LPM. On the one hand, both methods compute the similarity
between nodes based on node embddings learned by GNN. On the
other hand, our method can further add new edges into the graph,
while LPM cannot.
 
435KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yao Cheng et al.
(a)Chameleon (b) Actor 0.30.40.5
0%20%40%60%AccuracyUniform Noise RatioNRGNNNRGNN-R1NRGNN-R2NRGNN-R30.30.40.5
0%20%40%60%AccuracyFlip Noise RatioNRGNNNRGNN-R1NRGNN-R2NRGNN-R30.550.600.650.70
0%20%40%60%AccuracyUniform Noise RatioLPMLPM-R1LPM-R2LPM-R30.500.550.600.650.70
0%20%40%60%AccuracyFlip Noise RatioLPMLPM-R1LPM-R2LPM-R3
0.250.300.35
0%20%40%60%AccuracyUniform Noise RatioNRGNNNRGNN-R1NRGNN-R2NRGNN-R30.20.30.4
0%20%40%60%AccuracyFlip Noise RatioNRGNNNRGNN-R1NRGNN-R2NRGNN-R30.250.300.350.40
0%20%40%60%AccuracyUniform Noise RatioLPMLPM-R1LPM-R2LPM-R30.20.30.4
0%20%40%60%AccuracyFlip Noise RatioLPMLPM-R1LPM-R2LPM-R3
Figure 3: The performance of LPM and NRGNN incorporated with three graph reconstruction modules
The second option is to learn a new graph adjacency matrix. The
representative method is GloGNN [ 16], which learns the hidden
representation ğ»for nodes and the new homophilous graph ğ‘
together. Specifically, in the ğ‘™-th layer, it aims to obtain a coefficient
matrixğ‘(ğ‘™)âˆˆRğ‘›Ã—ğ‘›and node embeddings ğ»(ğ‘™)âˆˆRğ‘›Ã—ğ‘‘satisfying
ğ»(ğ‘™)â‰ˆğ‘(ğ‘™)ğ»(ğ‘™). Each element ğ‘(ğ‘™)
ğ‘–ğ‘—represents how well node ğ‘£ğ‘—
characterizes node ğ‘£ğ‘–. The objective is formulated as:
min
ğ‘(ğ‘™)âˆ¥ğ»(ğ‘™)âˆ’(1âˆ’ğ›¾)ğ‘(ğ‘™)ğ»(ğ‘™)âˆ’ğ›¾ğ»(0)âˆ¥2
ğ¹+ğ›½1âˆ¥ğ‘(ğ‘™)âˆ¥2
ğ¹+ğ›½2âˆ¥ğ‘(ğ‘™)âˆ’ğ¾âˆ‘ï¸
ğ‘˜=1ğœ†ğ‘˜Ë†ğ´ğ‘˜âˆ¥2
ğ¹,
(3)
whereğ›½1andğ›½2are two hyper-parameters to control term im-
portance.ğ»(0)could be a simple transformation of initial node
featuresğ‘‹and adjacency matrix ğ´(e.g., MLP(Concat(ğ‘‹,ğ´))), and
ğ›¾is to balance the importance of initial and current embeddings.
Ë†ğ´ğ‘˜denotes the ğ‘˜-hop sub-graph, and ğ¾is the pre-set maximum
hop count. Equation 3 has a closed-form solution:
ğ‘(ğ‘™)âˆ—="
(1âˆ’ğ›¾)ğ»(ğ‘™)(ğ»(ğ‘™))T+ğ›½2ğ¾âˆ‘ï¸
ğ‘˜=1ğœ†ğ‘˜Ë†ğ´ğ‘˜âˆ’ğ›¾(1âˆ’ğ›¾)ğ»(0)(ğ»(ğ‘™))T#
Â·h
(1âˆ’ğ›¾)2ğ»(ğ‘™)(ğ»(ğ‘™))T+(ğ›½1+ğ›½2)ğ¼ğ‘›iâˆ’1
.
(4)
Here,ğ‘(ğ‘™)âˆ—characterizes the connections between nodes. After ğ¿
layers, we use ğ‘(ğ¿)âˆ—to denote homophilous graph structure.
In the third option, ğ‘(ğ¿)âˆ—can be further adjusted to be symmetric
and non-negative. Formally, we add the following formula after
the calculation of Equation 4, and still optimize the objective in
Equation 3:
Ë†ğ‘(ğ¿)=ReLU(1
2(ğ‘(ğ¿)âˆ—+(ğ‘(ğ¿)âˆ—)T). (5)
3.3 Empirical Analysis
We next investigate the relationship between graph homophily
and graph label noise using two types of empirical experiments. In
the first experiment, we directly enhance the homophily level by
converting heterophilous edges into homophilous ones based on
the ground-truth labels, and then run existing methods to compare
their performance. In the second experiment, we integrate thethree homophilous graph reconstruction modules with existing
approaches and evaluate them in heterophilous datasets.
3.3.1 Graph Homophily Mitigates the Effect of Graph Label Noise.
We modified the edge homophily level and label noise level on two
real-world heterophilous datasets, Chameleon andActor. In order
to maintain the same sparsity of the dataset, we randomly replace
a heterophilous edge in the original graph with a homophilous
one. We conducted experiments on four methods, including two
classical methods: LP and GCN; and two SOTA methods for graph
label noise: NRGNN and LPM. Figure 2 shows the performance of
all the methods under different levels of flip label noise.
From the figures, we observe that: (1) For all the methods and
noise rates, the classification accuracy consistently improves with
the increase of graph homophily on both Chameleon and Actor
datasets. Surprisingly, when graph homophily gets larger, even
under a high noise rate (e.g., 0.6 for LP, NRGNN and LPM, 0.4 for
GCN), these methods can still achieve better performance than their
corresponding results in the original heterophilous graph without
label noise. This indicates that graph homophily can effectively
mitigate the negative impact of label noise. (2) When the noise
rate is 0.6, GCN performs poorly, while LP can still achieve the
best results among all the methods in some cases. This shows that
although both GCN and LP apply the smoothing operation on
node features and labels, respectively, GCN is adversely affected by
the coupling of node features and learnable transformation matrix,
which is also pointed out in [ 13]. This insight also demonstrates that
LP excels at correcting noisy labels through label smoothing under
various conditions of graph homophily. We will give theoretical
proofs in Sec. 4.4 later. To sum up, all the results empirically show
that graph homophily plays a significant role in denoising labels.
3.3.2 Integration of Homophilous Graph Reconstruction and Exist-
ing Approaches. In the previous section, we show the importance
of graph homophily. However, the experiments use ground-truth
labels to change the level of graph homophily, which is not practical.
Instead, we adapt three kinds of reconstruction modules (denoted
asR1,R2andR3) as introduced in Sec. 3.2 to LPM and NRGNN,
and evaluate their performance in two heterophilous datasets under
 
436Resurrecting Label Propagation for Graphs with Heterophily and Label Noise KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
different noise ratios. In detail, we directly replace the computation
ofğ´â€², and keepğ‘Œâ€²andğ‘Œfinalunchanged for LPM and NRGNN in
Equation 1. For different reconstruction modules (R1, R2andR3),
ğ´â€²is replaced by ğ‘in Equation 2, ğ‘(ğ¿)âˆ—in Equation 4, or Ë†ğ‘(ğ¿)in
Equation 5. Figure 3 summarizes the results of all the variants.
From the figures, we see that (1) The replacement of homophilous
graph reconstruction modules can generally improve the perfor-
mance of LPM and NRGNN in heterophilous datasets. (2) The re-
construction module R1, which is based on cosine similarity, only
considers node representations learned by the GNN model, and
employs a hard threshold to determine the existence of edges. It
heavily relies on the selected GNN and the pre-set threshold. Con-
sequently, it cannot provide consistent improvement for both LPM
and NRGNN. (3) The reconstruction modules R2andR3can clearly
enhance the performance of LPM and NRGNN. This is because they
directly learn the graph structure. Interestingly, the overall perfor-
mance of R2surpasses that of R3, suggesting that ğ‘derived in a
closed-form solution can better capture the true relations between
nodes. To summarize, incorporating graph reconstruction with
homophily can effectively enhance existing approaches for graph
noise labels. Among the tested modules, the second reconstruc-
tion module computed by Equation 4 demonstrates the best overall
performance. We will employ it in our following experiments.
4ğ‘…2LP: GRAPH RECONSTRUCTION AND
NOISY LABEL RECTIFICATION BY LABEL
PROPAGATION
Based on the above empirical analysis, we have identified two
key findings. First, graph homophily can mitigate the effect of
graph label noise. Second, label propagation is a simple yet effective
approach to rectify incorrect labels when graph homophily is high.
These findings inspire us to utilize homophily graph reconstruction
and label propagation, and propose a new approach ğ‘…2LP against
arbitrary heterophily and label noise.
Algorithm 1: ğ‘…2LP
Input: GraphG=(V,ğ´,ğ‘‹),V=CâˆªNâˆªU ,ğ‘ŒC,ğ‘ŒN,
multi-round epoch ğ‘‡, selection ratio ğœ€.
Output: The label of unlabeled nodes ğ‘ŒU.
1forğ‘¡=0,1,...,ğ‘‡âˆ’1do
2 Generateğ‘(ğ¿)âˆ—onCby Eq. 4
3 Train a GNN on ğ‘(ğ¿)âˆ—andCand calculate the predicted labels
ğ‘ŒPfor all the nodes
4 Utilize the label propagation to rectify noisy labels by Eq. 11
5 selectğœ€|N|nodes fromNbased on the confidence of ğ¹âˆ—, and
then transfer them from NtoC
6Train a GNN on the final clean set C, and obtain the predicted
labelsğ‘ŒU
7returnğ‘ŒU
4.1 Basic version
Assume we have a graph Gwith arbitrary heterophily, and a set
of clean labels ğ‘ŒCand noisy labels ğ‘ŒN. Our goal is to obtain the
correct labels for both noisy and unlabeled nodes. As previously
designed, we first reconstruct the graph structure explicitly andcompute the matrix ğ‘(ğ¿)âˆ—by Equation 4. Simultaneously, the node
representation ğ»(ğ¿)is obtained. The training of the reconstruction
module relies on the clean labels ğ‘ŒC, which allows us to establish
a mapping from ğ»(ğ¿)toğ‘Œto generate the predicted labels ğ‘ŒPfor
all unlabeled nodes. While ğ‘ŒPis noisy, it still contains rich correct
labels that are useful in label propagation.
Label propagation [ 34,36] is a graph-based method to propagate
label information across connected nodes. The fundamental assump-
tion of LP is label smoothness, which posits that two connected
nodes tend to share the same label. Thus, propagating existing
labeled data (including clean/noisy/predicted labels) throughout
the graph can effectively help us rectify noisy labels and assign
appropriate labels to previously unlabeled nodes.
Here we set the similarity graph ğ‘†=ğ‘(ğ¿)âˆ—, andğ¹âˆˆRğ‘›Ã—ğ‘as the
soft label matrix for nodes. When ğ‘¡=0, we establish the initial label
matrixğ¹(0)=ğ›¼2ğ‘ŒC+ğ›¼3ğ‘ŒN+ğ›¼4ğ‘ŒP. Note that in addition to the
clean labels ğ‘ŒC, bothğ‘ŒNandğ‘ŒPcould be noise-corrupted. Hence,
we introduce three hyper-parameters ğ›¼2,ğ›¼3andğ›¼4to control the
reliability of these initial labels. After that, in the ğ‘¡-th iteration, LP
can be formulated as:
ğ¹(ğ‘¡+1)=ğ›¼1ğ‘†ğ¹(ğ‘¡)+ğ›¼2ğ‘ŒC+ğ›¼3ğ‘ŒN+ğ›¼4ğ‘ŒP, (6)
whereÃ4
ğ‘—=1ğ›¼ğ‘—=1and eachğ›¼ğ‘—âˆˆ[0,1]. With iterations in Equa-
tion 6, we derive:
ğ¹(ğ‘¡)=(ğ›¼1ğ‘†)ğ‘¡âˆ’1ğ¹(0)+ğ›¼2ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=0(ğ›¼1ğ‘†)ğ‘–ğ‘ŒC+ğ›¼3ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=0(ğ›¼1ğ‘†)ğ‘–ğ‘ŒN+ğ›¼4ğ‘¡âˆ’1âˆ‘ï¸
ğ‘–=0(ğ›¼1ğ‘†)ğ‘–ğ‘ŒP.
(7)
Hence, we can also have a closed-form solution:
ğ¹âˆ—=limğ‘¡â†’âˆğ¹(ğ‘¡)=(ğ¼âˆ’ğ›¼1ğ‘†)âˆ’1(ğ›¼2ğ‘ŒC+ğ›¼3ğ‘ŒN+ğ›¼4ğ‘ŒP).(8)
4.2 Efficiency Improvement
Directly calculating ğ¹âˆ—by Equation 8 is computationally infeasible
due to the cubic time complexity involved in computing ğ‘(ğ¿)âˆ—
in Equation 4 and the inverse term (ğ¼âˆ’ğ›¼1ğ‘†)âˆ’1. However, even
quadratic time complexity is still high for large graph datasets.
Therefore, we aim to accelerate the whole computation to achieve
linear time complexity.
First of all, we utilize the Woodbury formula [ 25] to replace the
inverse and transform Equation 4 into:
ğ‘(ğ‘™)âˆ—="
(1âˆ’ğ›¾)ğ»(ğ‘™)(ğ»(ğ‘™))T+ğ›½2ğ¾âˆ‘ï¸
ğ‘˜=1ğœ†ğ‘˜Ë†ğ´ğ‘˜âˆ’ğ›¾(1âˆ’ğ›¾)ğ»(0)(ğ»(ğ‘™))T#
Â·
"
1
ğ›½1+ğ›½2ğ¼ğ‘›âˆ’1
(ğ›½1+ğ›½2)2ğ»(ğ‘™)1
(1âˆ’ğ›¾)2ğ¼ğ‘‘+1
ğ›½1+ğ›½2(ğ»(ğ‘™))Tğ»(ğ‘™)âˆ’1
(ğ»(ğ‘™))T#
(9)
After that, we use first-order Taylor expansion and derive
ğ¹âˆ—=(ğ¼âˆ’ğ›¼1ğ‘†)âˆ’1ğ¹(0)â‰ˆ(ğ¼+ğ›¼1ğ‘†)ğ¹(0). (10)
The inverse term in Equation 9 is computed on a matrix in Rğ‘‘Ã—ğ‘‘,
whose time complexity is only ğ‘‚(ğ‘‘3). In this way, the overall time
complexity of computing ğ‘(ğ¿)âˆ—isğ‘‚(ğ‘›2), leading to a quadratic
time complexity of calculating ğ¹âˆ—in Equation 10.
Next, we combine Equations 9 and 10:
ğ¹âˆ—=ğ¹(0)+ğ›¼1"
(1âˆ’ğ›¾)ğ»(ğ¿)(ğ»(ğ¿))T+ğ›½2ğ¾âˆ‘ï¸
ğ‘˜=1ğœ†ğ‘˜Ë†ğ´ğ‘˜âˆ’ğ›¾(1âˆ’ğ›¾)ğ»(0)(ğ»(ğ¿))T#
ğ‘„,(11)
 
437KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yao Cheng et al.
where
ğ‘„=1
ğ›½1+ğ›½2ğ¹(0)âˆ’1
(ğ›½1+ğ›½2)2ğ»(ğ¿)
Â·[1
(1âˆ’ğ›¾)2ğ¼ğ‘‘+1
ğ›½1+ğ›½2(ğ»(ğ¿))Tğ»(ğ¿)]âˆ’1(ğ»(ğ¿))Tğ¹(0).
(12)
In this way, we avoid computing ğ‘(ğ‘™)âˆ—and(ğ¼+ğ›¼1ğ‘†)explicitly,
and it can be further accelerated by matrix multiplication reorder-
ing. We first calculate ğ‘„in a right-to-left manner. Due to the time
complexityğ‘‚(ğ‘‘3)of computing the inverse term, the overall time
complexity of calculating ğ‘„isğ‘‚(ğ‘›ğ‘ğ‘‘+ğ‘‘3), whereğ‘ğ‘‘â‰ªğ‘›in the
large datasets. After that, we calculate ğ¹âˆ—in Equation 11 in a simi-
lar right-to-left manner. For example, for the term ğ»(0)(ğ»(ğ¿))Tğ‘„,
we first compute(ğ»(ğ¿))Tğ‘„. Then we left-multiply the result with
ğ»(0), leading to a time complexity of ğ‘‚(ğ‘›ğ‘ğ‘‘). When computing the
termÃğ¾
ğ‘˜âˆ’1ğœ†ğ‘˜Ë†ğ´ğ‘˜ğ‘„, we first calculate Ë†ğ´ğ‘„. Due to the sparsity of Ë†ğ´,
the time complexity is ğ‘‚(ğ‘ğ‘ğ‘›), whereğ‘is the average number of
non-zero entries in each row of Ë†ğ´. The total time complexity forÃğ¾
ğ‘˜âˆ’1ğœ†ğ‘˜Ë†ğ´ğ‘˜ğ‘„isğ‘‚(ğ¾ğ‘ğ‘ğ‘›), whereğ¾ğ‘ğ‘ is a coefficient. Finally, we
approximate the optimal label propagation results in a linear time.
4.3 Effectiveness Enhancement
To further enhance the effectiveness of ğ‘…2LP, we employ a multi-
round selection strategy. Specifically, ğ¹âˆ—is obtained after the com-
putation of graph reconstruction and label propagation. We then
calculate the confidence based on ğ¹âˆ—for noisy nodes, select the
rectified nodes with the highest confidence and add them to the
clean label set ğ‘ŒC. In each time, ğœ€âˆ—|N| nodes are chosen, where ğœ€
is the pre-set ratio. The updated clean label set can be leveraged in
the next round to reconstruct the graph and refine the remaining
nodes. Finally, we obtain an augmented clean label set, upon which
we train a GNN classifier to obtain the node classification result.
The pseudo-code of ğ‘…2LP is provided in Algorithm 1.
The input is a graph with the topology ğ´and node features
ğ‘‹, and we divide the nodes into three sets C,NandU. Here,C
represents clean labeled nodes, Ndenotes noisy labeled nodes, and
Urepresents unlabeled nodes. We have the labels ğ‘ŒCandğ‘ŒN, and
we aim to predict ğ‘ŒU. The whole process is a multi-round training
to select the noisy nodes with corrected labels into the clean node
setC. In each round, we first reconstruct the graph with homophily
based on the current clean labels, and train a GNN to obtain the
predicted labels for all the nodes as extra information. Next, we
employ label propagation to rectify noisy labels. Given the selection
ratioğœ€, we select a portion of the current noisy nodes based on the
confidence of ğ¹âˆ—. Lastly, we transfer the selected noisy nodes with
their corrected labels from NtoCand initiate a new round. ğ‘…2LP
typically runs for 10-20 iterations. After we end all the rounds, we
obtain the final clean set Cto train a GNN and predict the labels
for unlabeled nodes.
4.4 Theoretical Verification
In this section, we theoretically analyze the denoising effect of
label propagation. For clarity, we first assume the clean label is
corrupted by a noise transition matrix ğ‘»âˆˆ[0,1]ğ‘Ã—ğ‘, where ğ‘»ğ‘–ğ‘—=
P(Ëœğ‘Œ=ğ¶ğ‘—|ğ‘Œ=ğ¶ğ‘–)is the probability of the label ğ¶ğ‘–being corrupted
into the label ğ¶ğ‘—. For simplicity and following [ 24], we assumethe classification is binary and the noise transition matrix is ğ‘»=1âˆ’ğ‘’ ğ‘’
ğ‘’ 1âˆ’ğ‘’
. After label propagation, the propagated label of the
nodeğ‘£ğ‘–becomes a random variable
Ë†ğ‘Œğ‘–=(1âˆ’ğ›¼)Ëœğ‘Œğ‘–+ğ›¼
ğ‘‘âˆ‘ï¸
ğ‘£ğ‘—âˆˆNğ‘–Ëœğ‘Œğ‘—. (13)
which is decided by its own noisy label Ëœğ‘Œğ‘–and itsğ‘‘neighborsâ€™ noisy
labels Ëœğ‘Œğ‘—. We establish the following theorem for the propagated
label Ë†ğ‘Œğ‘–, and the detailed proof can be found in our full version2.
Theorem 1. (Label Propagation and Denoising) Suppose the label
noise is generated by ğ‘»and the label propagation follows Equation 13.
For a specific node ğ‘–, we further assume the node has ğ‘‘neighbors, and
its neighbor nodes have the probability ğ‘to have the same true label
with nodeğ‘–, i.e.,P[ğ‘Œğ‘–=ğ‘Œğ‘—]=ğ‘. After one-round label propagation,
the gap between the propagated label and the ground-true label is:
E(ğ‘Œâˆ’Ë†ğ‘Œ)2=P(ğ‘Œ=0)E(Ë†ğ‘Œ|ğ‘Œ=0)2
+P(ğ‘Œ=1)
E(Ë†ğ‘Œ|ğ‘Œ=1)âˆ’12
+Var(Ë†ğ‘Œ),(14)
where we have
E(Ë†ğ‘Œ|ğ‘Œ=0)=(1âˆ’ğ›¼)ğ‘’+ğ›¼[ğ‘ğ‘’+(1âˆ’ğ‘)(1âˆ’ğ‘’)], (15a)
E(Ë†ğ‘Œ|ğ‘Œ=1)=(1âˆ’ğ›¼)(1âˆ’ğ‘’)+ğ›¼[ğ‘(1âˆ’ğ‘’)+(1âˆ’ğ‘)ğ‘’], (15b)
Var(Ë†ğ‘Œ)=(1âˆ’ğ›¼)2ğ‘’(1âˆ’ğ‘’) (15c)
+ğ›¼2
ğ‘‘[ğ‘ğ‘’+(1âˆ’ğ‘)(1âˆ’ğ‘’)][1âˆ’ğ‘ğ‘’âˆ’(1âˆ’ğ‘)(1âˆ’ğ‘’)].
(15d)
(1)Heterophily and the impact of ğ‘:In Equation 14, ğ‘plays
the central role as a larger ğ‘can reduce all three terms in the bound.
The value of ğ‘is related to the heterophily of a specific node and a
more heterophilous node tends to have a smaller ğ‘. This is our major
motivation to construct ğ‘(ğ¿)âˆ—in the algorithm, which attempts to
connect the nodes from the same class.
(2)Label denoising effect: We consider two special cases to
see the label denoising by propagation. When we set ğ›¼=0and do
not propagate the labels, E(ğ‘Œâˆ’Ë†ğ‘Œ)2=E(ğ‘Œâˆ’Ëœğ‘Œ)2=ğ‘’. When we
setğ‘â†’1where all the neighbors have the same true labels, we
haveE(ğ‘Œâˆ’Ë†ğ‘Œ)2â‰ˆğ‘’2+1
ğ‘‘ğ‘’(1âˆ’ğ‘’), which is strictly less than ğ‘’. It
demonstrates the importance of neighbors. Besides, when we have
more neighbors, the distance becomes smaller. It is the reason that
ğ‘(ğ¿)âˆ—is dense so each node could have more neighbors.
(3)The impact of clean and predicted labels: The bound in
Equation 14 monotonically decreases with ğ‘’and therefore having
clean nodes can substantially reduce the bound. When the pre-
dicted labels are present, we have a larger number of neighbors for
propagation, which also increases the value ğ‘‘.
In the following theorem, we analyze the generalization error of
the final GNN classifier Ë†ğ‘“.
Theorem 2. (Generalization Error and Oracle Inequality) Denote the
node feature ğ‘‹sampled from the distribution ğ·, the graph topology
asğ´, the set of training nodes as ğ‘†ğ‘›, the graph neural network as
ğ‘“(Â·,Â·), and the learned GNN classifier Ë†ğ‘“=infğ‘“Ã
ğ‘‹âˆˆğ‘†ğ‘›(Ë†ğ‘Œâˆ’ğ‘“(ğ‘‹,ğ´)).
Suppose that the propagated label concentrated on its mean, i.e., with
2https://arxiv.org/abs/2310.16560
 
438Resurrecting Label Propagation for Graphs with Heterophily and Label Noise KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: The classification accuracy (%) over the methods on 10 datasets with flip noise ranging from 0% to 60%. We highlight
the best score on each dataset in bold. OOM denotes the out-of-memory error.
T
ypes Metho
ds Noise Cora
Citeseer Chameleon Cornell Wisconsin Texas Actor Penn94 arXiv-year snap-patents
GNNsGCN0% 87.75
76.81 56.95 61.89 63.92 62.16 31.27 80.76 44.67 53.59
20% 86.08
75.48 56.27 60.00 62.94 61.89 30.63 78.99 40.93 49.49
40% 81.30
70.99 53.81 60.27 61.17 61.35 26.38 75.84 40.20 41.84
60% 53.85
42.94 52.98 49.18 36.47 48.64 21.51 72.70 38.73 30.55
GloGNN0% 84.57
76.09 68.83 83.51 88.43 84.05 38.55 85.66 52.74 62.33
20% 80.84
74.09 67.76 76.48 81.76 71.62 37.55 76.97 47.67 54.72
40% 77.55
63.54 63.00 70.27 74.31 61.08 33.64 62.55 36.93 38.74
60% 75.10
40.17 55.08 52.43 56.86 53.78 30.31 51.79 25.45 26.37
H2GCN0% 87.87
77.11 60.01 82.59 85.65 86.26 35.70 79.02 48.72 57.21
20% 83.51
74.80 54.10 74.45 71.96 70.78 32.49 74.08 43.20 52.06
40% 82.00
71.42 47.82 65.67 70.60 67.83 29.73 67.42 36.57 46.23
60% 81.36
71.23 34.51 55.27 64.90 60.81 30.21 54.47 29.81 38.57
Metho
ds
for
Label
NoiseCo-teaching+0% 85.76
76.10 72.52 70.54 72.49 67.91 34.28 86.90 OOM OOM
20% 68.79
70.19 66.09 66.09 61.92 64.70 30.16 77.68 OOM OOM
40% 55.34
47.87 41.03 41.03 57.88 57.08 24.98 61.82 OOM OOM
60% 35.72
36.11 28.57 28.57 39.60 38.56 21.16 50.18 OOM OOM
Backwar
d0% 84.88
77.06 71.00 72.70 81.37 84.31 23.90 87.00 46.51 58.54
20% 81.04
72.43 67.69 69.72 75.88 74.72 21.15 80.84 40.29 56.30
40% 71.28
62.03 61.73 68.91 63.13 60.78 20.01 70.00 32.24 43.61
60% 56.70
48.37 53.15 51.35 45.49 53.51 20.09 51.57 26.21 31.57
Metho
ds
for
Graph
Label
NoiseNRGNN0% 82.87
72.52 38.50 69.91 69.17 72.64 28.82 68.31 OOM OOM
20% 82.30
72.47 37.83 61.70 67.37 70.32 28.75 66.80 OOM OOM
40% 79.13
67.42 33.09 54.59 61.64 57.02 25.40 63.59 OOM OOM
60% 75.40
60.00 30.35 42.70 58.23 54.59 21.69 53.14 OOM OOM
LPM0% 89.74
78.77 60.72 63.87 73.72 69.46 31.43 76.10 44.32 56.76
20% 86.55
75.92 60.31 62.21 72.35 68.10 29.11 75.35 42.03 53.84
40% 83.97
72.62 58.77 61.35 70.78 67.02 28.52 71.45 38.46 48.36
60% 80.47
68.72 54.75 58.37 64.11 62.16 27.51 63.14 32.11 40.21
RT
GNN0% 86.08
76.27 45.88 62.83 67.64 71.43 29.55 72.65 OOM OOM
20% 85.24
75.45 39.03 57.43 60.78 67.56 28.25 70.67 OOM OOM
40% 80.21
69.85 31.08 47.97 56.86 60.13 25.67 60.48 OOM OOM
60% 73.25
61.29 25.58 31.75 47.54 50.21 18.96 53.98 OOM OOM
ERASE0% 87.32
77.14 50.15 62.62 65.52 72.91 30.16 75.36 43.27 56.50
20% 86.51
75.86 42.53 58.67 58.46 70.04 29.85 72.61 40.51 52.65
40% 82.19
70.15 35.16 50.11 56.03 65.55 27.14 64.67 35.62 46.33
60% 75.63
61.22 30.88 33.84 45.32 57.47 20.58 58.26 30.17 38.04
ğ‘…2LP0% 87.69
78.09 73.05 87.84 88.82 87.30 38.59 86.78 53.80 63.04
20% 86.85
76.30 68.38 84.32 86.86 83.24 37.37 82.69 50.11 58.59
40% 85.00
74.41 61.23 77.84 83.14 77.02 35.86 79.05 46.19 55.90
60% 82.53
72.73 57.46 71.35 75.68 69.46 35.08 77.09 46.11 52.71
probability at leastğ›¿
2,||Ë†ğ‘Œâˆ’ğ‘Œ|âˆ’E|Ë†ğ‘Œâˆ’ğ‘Œ||â‰¤ğœ–1. We further assume
the generalization error is bounded with respect to the propagated
labels, i.e., with probability at leastğ›¿
2,
Eğ‘‹âˆ¼ğ·|Ë†ğ‘Œâˆ’Ë†ğ‘“(ğ‘‹,ğ´)|âˆ’inf
ğ‘“Eğ‘‹âˆ¼ğ·|Ë†ğ‘Œâˆ’ğ‘“(ğ‘‹,ğ´)|â‰¤ğœ–2.
Then we obtain the generalization error bound for training with noisy
labels and test on the clean labels, i.e., with probability at least ğ›¿, the
generalization error trained with propagated labels is given by
Eğ‘‹âˆ¼ğ·|ğ‘Œâˆ’Ë†ğ‘“(ğ‘‹,ğ´)|â‰¤ inf
ğ‘“Eğ‘‹âˆ¼ğ·|ğ‘Œâˆ’ğ‘“(ğ‘‹,ğ´)|+ğœ–2+2(E|Ë†ğ‘Œâˆ’ğ‘Œ|+ğœ–1).The generalization error mainly depends on ğœ–2andE|Ë†ğ‘Œâˆ’ğ‘Œ|.
Our algorithm iteratively rectifies the noisy labels and selects high-
confidence labels into the clean set, thereby reducing E|Ë†ğ‘Œâˆ’ğ‘Œ|. In
addition, as shown in [ 6], using the predicted labels generated by
the same neural network is able to decrease ğœ–2. This is the reason
for the inclusion of predicted labels within our algorithm.
5 RELATED WORK
5.1 Deep Learning with Noisy Labels
Dealing with noisy labels can be approached through two main-
stream directions: loss adjustment and sample selection [ 21]. Partic-
ularly, loss adjustment aims to mitigate the negative impact of noise
 
439KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yao Cheng et al.
by adjusting the loss value or the noisy labels. For example, [ 18] ex-
plicitly estimates the noise transition matrix to correct the forward
and backward loss. Other effective techniques, such as [ 10,20],
refurbish the noisy labels by a convex combination of noisy and
predicted labels. Sample selection, on the other hand, involves
selecting true-labeled examples from a noisy training set via multi-
network or multi-round learning. For instance, Co-teaching [ 8]
and Co-teaching+ [ 31] use multi-network training, where two deep
neural networks are maintained and each network selects a set
of small-loss examples and feeds them to its peer network. Multi-
round learning methods [ 20,28] iteratively refine the selected set
of clean samples by repeating the training rounds. As a result, the
performance improves as the number of clean samples increases.
5.2 Learning GNNs with Noisy Labels
Graph neural networks (GNNs) [ 7,12,13,23,26,30] have emerged
as revolutionary technologies for graph-structured data, which can
effectively capture complex patterns based on both node features
and structure information, and then infer the accurate node labels.
It is also common that real-world graphs are heterophilous which
node is more likely to be connected with neighbors that have dis-
similar features or different labels. Many studies are proposed to
design GNNs for graphs with heterophily [1, 3, 16, 35].
The robustness of GNNs is well-studied, but most of them focus
on the perturbation of graph structure and node features [ 5,22,27,
38] while few works study the label noise. NRGNN [ 4] and LPM [ 29]
are the first two to deal with the label noise in GNNs explicitly. In
detail, NRGNN generates pseudo labels and assigns more edges
between unlabeled nodes and (pseudo) labeled nodes against label
noise. LPM utilizes label propagation to aggregate original labels
and pseudo labels to correct the noisy labels. However, they are only
suitable for homophilous graphs, and perform badly on graphs with
heterophily. Our proposed approach, ğ‘…2LP relaxes the homophily
assumption and is more resilient to various types of noise.
6 EXPERIMENTS
6.1 Experimental Settings
[Label noise]. For each dataset, we randomly split nodes into 60%,
20%, and 20% for training, validation and testing, and measure the
performance of all models on the test set. We set 10% nodes with
clean labels, and corrupt the remaining training nodes with noise
in training set. Following [ 18], We consider two types of noise:
uniform noise means the true label ğ‘ğ‘–have a probability ğ‘’to be
uniformly flipped to other classes, i.e. ğ‘»ğ‘–ğ‘—=ğ‘’/(ğ‘âˆ’1)forğ‘–â‰ ğ‘—and
ğ‘»ğ‘–ğ‘–=1âˆ’ğ‘’;flip noise means the true label ğ¶ğ‘–have a probability ğ‘’to
be flipped to only a similar class ğ¶ğ‘—, i.e.ğ‘»ğ‘–ğ‘—=ğ‘’andğ‘»ğ‘–ğ‘–=1âˆ’ğ‘’. We
vary the value of ğ‘’from 0% to 60%.
[Baselines]. We compare ğ‘…2LP with 10 baselines, which can be
categorized into three types. The first type involves (1) GCN [12]
(2)GloGNN [16] and (3) H2GCN [35], three popular GNNs for
homophilous and heterophilous graphs. They are directly trained
on both clean and noisy training samples. The second type in-
cludes baselines that handle label noise: (4) Co-teaching+ [31] is a
multi-network training method to select clean samples. (5) Back-
ward [18] is to revise predictions and obtain unbiased loss on noisy
training samples. For fairness, we use GloGNN as a backbone GNNmodel for Co-teaching+ and Backward. The third type includes
five GNNs dealing with label noise: (6) NRGNN [4], (7) LPM [29],
(8)RTGNN [19], (9) ERASE [2], and (10) CGNN [32]. Since the
absence of publicly available code for CGNN, we validate ğ‘…2LP
under the same settings in their paper and report the results of
CGNN directly from the paper. The results are presented in the
appendix C.1 due to the space limitation.
Due to the space limitation, we move details on baselines (Sec. A),
experimental setup (Sec. B), and other experimental results (Sec. C)
to Appendix.
6.2 Performance Results
Table 2: The classification accuracy (%) without a clean label
set. We highlight the best score on each dataset in bold and
underline the runner-upâ€™s.
Datasets Metho
dsUniform
Noise F
lip Noise
20%
40% 60% 20%
40% 60%
CoraGloGNN 82.53
79.92 75.70 80.84
71.06 41.00
H2GCN 85.34 80.62 75.52 82.25 71.20 45.42
NRGNN 79.43
72.10 63.92 76.42
70.19 60.32
Co-teaching+ 80.00
69.59 55.78 75.20
59.25 33.15
Backwar
d 83.35
80.18 76.16 81.04
69.65 42.03
ğ‘…2LP 85.45
84.32 78.87 84.95
76.21 46.62
Chamele
onGloGNN 63.94
60.83 58.46 62.19
52.71 33.35
H2GCN 55.30
51.79 46.60 54.10
47.83 34.36
NRGNN 37.01
34.56 31.85 35.37
31.86 29.32
Co-teaching+ 66.60
61.90 50.61 64.40
40.19 25.94
Backwar
d 69.34 65.46
61.31 67.54 58.33 42.94
ğ‘…2LP 69.45 64.16 58.13 67.23 59.01 42.87
Table 1 presents the performance results of all the methods on
10 benchmark datasets under different flip noise levels. We attach
Table 7 for various uniform noise levels in Appendix. Every method
was repeated 10 times for small-scale datasets and 5 times for large-
scale datasets over different random splits, and we report the mean
classification accuracy on the test set. From the tables, we make
the following observations: (1) ğ‘…2LP consistently outperforms the
competitive methods across various noise settings in the majority
of cases. (2) The GNN models GCN, GloGNN and H 2GCN exhibit
good performance on homophilous graphs. However, their accuracy
drops dramatically as the noise ratio increases on heterophilous
graphs. (3) LPM, NRGNN, RTGNN and ERASE aim to address the
label noise in graphs, but their accuracy is limited on heterophilous
graphs because they rely on the assumption of graph homophily
and label smoothness. (4) Co-teaching+ and Backward are two
mainstream methods to address the label noise through sample
selection and loss correction. However, their accuracy significantly
declines when facing a high noise ratio.
Overall,ğ‘…2LP iteratively learns graph homophily to correct noise
labels, ensures high-quality labels for training the GNN model, and
achieves high and stable accuracy under different noises.
6.3 Ablation Study
The ablation study is done to understand the main components of
ğ‘…2LP. We first remove the whole label propagation step and call this
variantğ‘…2LP-nlp (no label propagation step). Instead, we use self-
training [ 15] that just picks the most confident noisy labels in the
 
440Resurrecting Label Propagation for Graphs with Heterophily and Label Noise KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0.40.60.81.0
CoraUniform-20%CoraUniform-40%CoraFlip-20%CoraFlip-40%ChameleonUniform-20%ChameleonUniform-40%ChameleonFlip-20%ChameleonFlip-40%Accuracyğ‘…Â²LPğ‘…Â²LP-nlpğ‘…Â²LP-nzğ‘…Â²LP-nplğ‘…Â²LP-nnl
Figure 4: Ablation study on the main components of ğ‘…2LP.
Table 3: The classification results and the homophily of the
graph with iterations.
Types Noise Results 5 rounds 10 rounds 15 rounds 20 rounds
Uniform20%Acc (%) 66.67 69.08 70.83 71.59
Edge Homo. 0.54 0.57 0.58 0.58
40%Acc 65.79 65.57 67.59 68.23
Edge Homo. 0.53 0.55 0.57 0.57
Flip20%Acc (%) 66.23 70.61 71.27 70.83
Edge Homo. 0.54 0.57 0.58 0.57
40%Acc (%) 54.17 61.18 62.28 61.09
Edge Homo. 0.51 0.54 0.56 0.56
GNN and put them into the clean set to improve the performance.
Secondly, we replace the reconstructed graph with the similarity
matrix and call this variant ğ‘…2LP-nz (no ğ’(ğ’)âˆ—). Further, we do
not propagate the noisy labels or the predicted labels in the label
propagation, respectively, and call these two variants ğ‘…2LP-nnl (no
noisy labels) andğ‘…2LP-npl (no predicted labels). We compare ğ‘…2LP
with these four variants, and the results are presented in Figure 4.
Our findings show that ğ‘…2LP outperforms all the variants on the
two datasets. Furthermore, the performance gap between ğ‘…2LP and
ğ‘…2LP-nlp highlights the importance of label propagation for label
correction. The noise labels significantly enhance the performance
of label propagation, especially on heterophilous graphs.
6.4 Performance without Clean Set
For fairness, we remove the clean label set and further study the
model performance without clean set. The results on Cora and
Chameleon are given in Table 2. For each dataset, we compare
three noise rates from low to high under uniform/flip noise, which
leads to a total of 12 comparisons. From the table, we see that our
methodğ‘…2LP achieves the best or the runner-up results 11 times,
which shows that ğ‘…2LP are still effective even without clean set.
For other competitors, although they perform well in some cases,
they cannot provide consistently superior results. For example,
under 40%uniform noise, Backward achieves the highest accuracy
of 65.46 on Chameleon, but its accuracy is only 80.18 on Cora (the
winnerâ€™s is 84.32). All these results demonstrate the robustness of
ğ‘…2LP against label noise.
6.5 The Homophily of Graphs with Iterations
We further study classification accuracy and the homophily of
the reconstructed graph with iterations. Specifically, we reported
the results at five-round intervals in Table 3 using the Chameleondataset. The initial homophily of Chameleon is 0.23. We observe
that: (1) Notably, ğ‘…2ğ¿ğ‘ƒsignificantly enhances the homophily of the
reconstructed graph with iterations under different situations. (2)
As the number of learning rounds increases, both the reconstructed
graphâ€™s homophily and accuracy gradually improve and converge.
Table 4: Average running time (s).
Methods Cora Actor arXiv-year
LPM 41.96 158.32 512.37
NRGNN 46.81 137.39 OOM
RTGNN 143.16 283.76 OOM
ERASE 132.50 180.51 455.13
ğ‘…2LP 26.98 84.15 119.46
6.6 Runtime Comparison
We compare the running time using the Cora, Actor and arXiv-year
datasets in Table 4, where all processes are executed on a single
Tesla A100 GPU (80GB memory). From the table, we see that ğ‘…2LP
is much faster than other baselines.
7 CONCLUSION
In this paper, we address the problem of graph label noise under
graph heterophily. We begin by empirically exploring the relation-
ship between graph homophily and label noise, leading to two key
observations: (1) graph homophily can indeed mitigate the effect of
graph label noise on GNN performance, and (2) LP-based methods
achieve great performance as graph homophily increases. These
findings inspire us to combine LP with homophilous graph recon-
struction to propose a effective algorithm ğ‘…2LP. It is a multi-round
process that performs graph reconstruction with homophily, label
propagation for noisy label refinement and high-confidence sample
selection iteratively. Finally, we conducted extensive experiments
and demonstrated the superior performance of ğ‘…2LP compared to
other 10 state-of-the-art competitors on 10 benchmark datasets. In
the future, we plan to explore and improve ğ‘…2LP in more challeng-
ing scenarios, where the clean label set is unavailable, or input data,
including node features and graph topology, also exhibits noise.
ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation of
China No. 62202172, Shanghai Science and Technology Committee
General Program No. 22ZR1419900 and Singapore MOE AcRF Tier-2
Grant (T2EP20122-0003).
 
441KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yao Cheng et al.
REFERENCES
[1]Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond Low-frequency
Information in Graph Convolutional Networks. In AAAI. AAAI Press.
[2]Ling-Hao Chen, Yuanshuo Zhang, Taohua Huang, Liangcai Su, Zeyi Lin, Xi Xiao,
Xiaobo Xia, and Tongliang Liu. 2023. ERASE: Error-Resilient Representation
Learning on Graphs for Label Noise Tolerance. arXiv preprint arXiv:2312.08852
(2023).
[3]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2021. Adaptive Universal
Generalized PageRank Graph Neural Network. In International Conference on
Learning Representations. https://openreview.net/forum?id=n6jl7fLxrP
[4]Enyan Dai, Charu Aggarwal, and Suhang Wang. 2021. Nrgnn: Learning a label
noise resistant graph neural network on sparsely and noisily labeled graphs. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining. 227â€“236.
[5]Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song.
2018. Adversarial attack on graph structured data. In International conference on
machine learning. PMLR, 1115â€“1124.
[6]Bin Dong, Jikai Hou, Yiping Lu, and Zhihua Zhang. 2019. Distillation ğ‘ğ‘ğ‘ğ‘Ÿğ‘œğ‘¥
Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information
Retrieval For Overparameterized Neural Network. arXiv preprint arXiv:1910.01255
(2019).
[7]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. 1024â€“1034.
[8]Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang,
and Masashi Sugiyama. 2018. Co-teaching: Robust training of deep neural net-
works with extremely noisy labels. Advances in neural information processing
systems 31 (2018).
[9]Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia dâ€™Amato, Gerard De Melo,
Claudio Gutierrez, Sabrina Kirrane, JosÃ© Emilio Labra Gayo, Roberto Navigli,
Sebastian Neumaier, et al .2021. Knowledge graphs. ACM Computing Surveys
(Csur) 54, 4 (2021), 1â€“37.
[10] Lang Huang, Chao Zhang, and Hongyang Zhang. 2020. Self-adaptive training:
beyond empirical risk minimization. Advances in neural information processing
systems 33 (2020), 19365â€“19376.
[11] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[12] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph
convolutional networks.
[13] Johannes Klicpera, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2019. Pre-
dict then Propagate: Graph Neural Networks meet Personalized PageRank.
[14] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010. Predicting pos-
itive and negative links in online social networks. In Proceedings of the 19th
international conference on World wide web. 641â€“650.
[15] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph
convolutional networks for semi-supervised learning. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[16] Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and
Weining Qian. 2022. Finding global homophily in graph neural networks when
meeting heterophily. In International Conference on Machine Learning. PMLR,
13242â€“13256.
[17] Deep Patel and PS Sastry. 2023. Adaptive sample selection for robust learning
under label noise. In Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision. 3932â€“3942.
[18] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and
Lizhen Qu. 2017. Making deep neural networks robust to label noise: A loss
correction approach. In Proceedings of the IEEE conference on computer vision andpattern recognition. 1944â€“1952.
[19] Siyi Qian, Haochao Ying, Renjun Hu, Jingbo Zhou, Jintai Chen, Danny Z Chen,
and Jian Wu. 2023. Robust Training of Graph Neural Networks via Noise Gov-
ernance. In Proceedings of the Sixteenth ACM International Conference on Web
Search and Data Mining. 607â€“615.
[20] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. 2019. Selfie: Refurbishing unclean
samples for robust deep learning. In International Conference on Machine Learning.
PMLR, 5907â€“5915.
[21] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. 2022.
Learning from noisy labels with deep neural networks: A survey. IEEE Transac-
tions on Neural Networks and Learning Systems (2022).
[22] Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar.
2020. Adversarial attacks on graph neural networks via node injections: A hier-
archical reinforcement learning approach. In Proceedings of the Web Conference
2020. 673â€“683.
[23] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2018. Graph attention networks.
[24] Jiaheng Wei, Hangyu Liu, Tongliang Liu, Gang Niu, Masashi Sugiyama, and Yang
Liu. 2021. To smooth or not? when label smoothing meets noisy labels. Learning
1, 1 (2021), e1.
[25] M. Woodbury and M. Woodbury. 1950. Inverting modified matrices. (1950).
[26] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. PMLR, 6861â€“6871.
[27] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. 2019. Adversarial examples on graph data: Deep insights into attack and
defense. arXiv preprint arXiv:1903.01610 (2019).
[28] Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao
Chen. 2020. A topological filter for learning with label noise. Advances in neural
information processing systems 33 (2020), 21382â€“21393.
[29] Jun Xia, Haitao Lin, Yongjie Xu, Lirong Wu, Zhangyang Gao, Siyuan Li, and
Stan Z Li. 2021. Towards robust graph neural networks against label noise. (2021).
[30] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks?
[31] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama.
2019. How does disagreement help generalization against label corruption?. In
International Conference on Machine Learning. PMLR, 7164â€“7173.
[32] Jingyang Yuan, Xiao Luo, Yifang Qin, Yusheng Zhao, Wei Ju, and Ming Zhang.
2023. Learning on Graphs under Label Noise. In ICASSP 2023-2023 IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
1â€“5.
[33] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
2021. Understanding deep learning (still) requires rethinking generalization.
Commun. ACM 64, 3 (2021), 107â€“115.
[34] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard
SchÃ¶lkopf. 2003. Learning with local and global consistency. Advances in neural
information processing systems 16 (2003).
[35] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
Koutra. 2020. Beyond homophily in graph neural networks: Current limitations
and effective designs. Advances in Neural Information Processing Systems 33
(2020), 7793â€“7804.
[36] Xiaojin Zhu. 2005. Semi-supervised learning with graphs. Carnegie Mellon
University.
[37] Marinka Zitnik and Jure Leskovec. 2017. Predicting multicellular function through
multi-layer tissue networks. Bioinformatics 33, 14 (2017), i190â€“i198.
[38] Daniel ZÃ¼gner, Amir Akbarnejad, and Stephan GÃ¼nnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining. 2847â€“2856.
 
442Resurrecting Label Propagation for Graphs with Heterophily and Label Noise KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A BASELINES
We compare ğ‘…2LP with 10 baselines, which can be categorized into
three types. The first type involves (1) GCN [12] (2)GloGNN [16]
and (3) H2GCN [35], three popular GNNs for homophilous and
heterophilous graphs. They are directly trained on both clean and
noisy training samples. In the second type, we modify two typi-
cal loss correction and sample selection methods and make them
suitable for graph data. (4) Co-teaching+ [31] is a multi-network
training method to select clean samples. (4) Backward [18] is to
revise predictions and obtain unbiased loss on noisy training sam-
ples. For fairness, we use GloGNN as a backbone GNN model for
Co-teaching+ and Backward. In the third type, we compare with
five GNNs dealing with label noise, including (6) NRGNN [4], (7)
LPM [29], (8) RTGNN [19], (9) ERASE [2], (10) CGNN [32]. In
detail, NRGNN assigns more edges between unlabeled nodes and
(pseudo) labeled nodes against noise. LPM addresses the graph label
noise by label propagation and meta-learning. RTGNN introduce
self-reinforcement and consistency regularization as supplemental
supervision to explicitly govern label noise. ERASE introduce a
decoupled label propagation method to learn representations with
error tolerance by maximizing coding rate reduction. CGNN em-
ploy graph contrastive learning as a regularization term, which
promotes two views of augmented nodes to have consistent repre-
sentations which enhance the robustness of node representations to
label noise. Since the absence of publicly available code for CGNN,
we validated ğ‘…2LP under the settings described in their paper and
compared them with it. The results are presented in the appen-
dix C.1.
B EXPERIMENTAL SETUP
We implemented ğ‘…2LP by PyTorch and conducted the experiments
on 10 datasets with one A100 GPU. The model is optimized by
Adam [ 11]. Considering that some methods are designed for sce-
narios without clean samples, we finetune these baselines on the
initial clean sets for fairness. To fine-tune hyperparameters, we
performed a grid search based on the result of the validation set.
Table 5 summarizes the search space of hyper-parameters in ğ‘…2LP.
Table 5: Grid search space.
Hyper-parameter Search space
lr{0.005,0.01,0.02,0.03}
dropout [0.0,0.9]
weight decay {1e-7, 5e-6, 1e-6, 5e-5, 1e-5, 5e-4}
ğ›½1 {0,1,10}
ğ›½2 {0.1,1,10,102,103}
ğ›¾ [0.0,0.9]
ğ›¼1 [0.0,1.0]
ğ›¼2 [0.0,1.0]
ğ›¼3 [0.0,1.0]
ğœ€ [0.1,0.9]
C ADDITIONAL EXPERIMENTAL RESULT
C.1 Performance Comparison with CGNN
We provide the mean accuracy of 5 runs for each baseline. In our
experiments, the training label rate is set to 0.01 and the percent ofnoisy labels is fixed at 20%. Table 6 presents the result on Coauthor
CS, Amazon Photo and Amazon Computers. From the tables, we
can seeğ‘…2LP consistently outperforms the competitive methods
across the uniform and flip noise settings in the majority of cases.
Table 6: The classification accuracy (%) compared with CGNN.
Methods Noise Coauthor CS Amazon PhotoAmazon
Computers
CGNNUniform-20% 84.1 85.3 81.8
Flip-20% 81.0 85.1 81.6
ğ‘…2LPUniform-20% 85.3 85.8 83.6
Flip-20% 83.1 84.6 82.7
60.070.080.090.0
0.10.20.30.40.50.60.70.80.9AccuracyCora-20%Cora-40%Chameleon-20%Chameleon-40%
ğ›¼!ğ›¼"ğ›¼#Îµ0.60.81.0
0.10.30.50.70.9Accuracy0.60.81.0
0.10.30.50.70.9Accuracy
0.40.60.81.0
0.10.30.50.70.9Accuracy0.60.81.0
0.10.30.50.70.9Accuracy
Figure 5: Hyper-parameter sensitivity analysis
C.2 Hyper-parameter Sensitivity Analysis
Here we study the sensitivity of four hyper-parameters: the weight
ğ›¼1,ğ›¼2,ğ›¼3and the sample selection ratio ğœ€. The results are shown in
Figure 5. For all the hyper-parameters, ğ‘…2LP consistently performs
well across a wide range of values, which demonstrates that these
hyper-parameters do not significantly impact the performance of
ğ‘…2LP.
C.3 Analysis of Selection Strategies
We compare the performance of ğ‘…2LP by using different selection
functions: (1) Threshold [31] chooses the noisy labels whose con-
fidence is greater than a certain threshold; (2) Absolute number
selects an absolute number of noisy labels with high confidence
based on the current number of samples; (3) Statistical selection [17]
uses statistics to derive the threshold for sample selection; (4) ğ‘…2LP
selects samples with high confidence based on a certain proportion
relative to the current noisy nodes. Figure 6 shows the results on
Cora and Chameleon datasets. From the figures, we observe that the
accuracy drops dramatically as the noise ratio increases when us-
ingThreshold andAbsolute number. The reason is that they choose
many noisy labels into the clean set. Statistical selection accurately
selects the node labels, but too stringent selection conditions limit
the number of selected labels. Compared with these functions, ğ‘…2LP
selects the labels with respect to the suitable quantity and high
quality of remaining samples.
 
443KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yao Cheng et al.
80.00085.000
0%20%40%60%AccuracyThresholdAbsolute numbersStatistical selection!Â²LP
(a) Cora -Uniform noise  (b) Cora -Flip noise(c) Chameleon -Uniform noise(d) Chameleon -Flip noise0.800.850.90
0%20%40%60%Accuracy0.800.850.90
0%20%40%60%Accuracy0.500.600.700.80
0%20%40%60%Accuracy0.500.600.700.80
0%20%40%60%AccuracyNoise RatioNoise RatioNoise RatioNoise Ratio
Figure 6: The effect of different selection strategies.
Table 7: The classification accuracy (%) over the methods on 10 datasets with uniform noise ranging from 0% to 60%. We highlight
the best score on each dataset in bold. OOM denotes the out-of-memory error.
T
ypes Metho
ds Noise Cora
Citeseer Chameleon Cornell Wisconsin Texas Actor Penn94 arXiv-year snap-patents
GNNsGCN0% 87.75
76.81 56.95 61.89 63.92 62.16 31.27 80.76 44.67 53.59
20% 86.54
75.66 54.29 61.08 63.52 62.43 30.27 80.29 39.59 50.89
40% 85.74
75.05 48.90 61.08 60.19 62.43 30.08 78.21 38.17 48.39
60% 83.67
73.44 33.13 60.54 60.78 61.62 29.79 76.74 36.88 46.81
GloGNN0% 84.58
76.09 69.83 83.51 88.43 84.05 38.55 85.66 52.74 62.33
20% 82.53
74.64 69.38 80.81 84.31 80.00 37.86 81.85 50.47 58.96
40% 79.92
72.62 65.78 74.86 74.50 68.37 37.33 77.44 47.77 54.97
60% 75.70
69.90 61.55 69.45 66.27 61.89 36.06 71.59 44.82 50.12
H2GCN0% 87.87
77.11 60.00 82.59 85.65 86.26 35.70 79.02 48.72 57.21
20% 85.34
74.51 55.31 74.27 75.09 77.27 31.25 76.22 45.60 54.68
40% 82.46
73.17 51.79 67.02 74.50 69.45 31.95 73.73 40.13 50.18
60% 80.94
70.87 46.60 57.56 69.01 62.16 31.25 71.01 33.65 41.36
Metho
ds
for
Label
NoiseCo-teaching+0% 85.76
76.32 72.52 70.54 72.68 67.21 34.56 86.90 OOM OOM
20% 80.46
73.32 67.30 69.91 60.19 64.56 33.62 82.73 OOM OOM
40% 70.00
63.00 58.81 54.05 54.05 60.70 30.27 73.86 OOM OOM
60% 59.17
52.29 51.90 49.18 43.72 43.24 27.06 52.03 OOM OOM
Backwar
d0% 84.88
77.06 71.00 72.70 81.37 84.31 23.90 87.00 46.51 58.54
20% 83.35
75.52 69.34 68.91 75.68 76.07 22.13 84.34 43.85 56.41
40% 80.18
69.89 65.76 68.64 71.72 70.78 22.98 80.90 33.16 56.37
60% 76.16
67.20 61.93 58.37 66.66 67.19 21.28 76.71 27.80 52.25
Metho
ds
for
Graph
Label
NoiseNRGNN0% 84.14
73.93 42.64 69.91 70.80 72.64 28.95 68.31 OOM OOM
20% 82.73
72.94 41.17 68.84 68.62 71.05 28.24 67.02 OOM OOM
40% 80.66
63.88 35.26 61.37 58.17 61.75 28.15 65.89 OOM OOM
60% 73.67
62.95 31.97 58.19 50.39 57.56 27.24 53.60 OOM OOM
LPM0% 89.74
78.77 60.72 63.87 73.72 69.46 31.43 76.10 44.32 56.76
20% 87.06 78.24 60.57
62.70 72.15 69.19 30.95 75.62 42.15 54.13
40% 84.81
74.50 59.38 62.43 69.21 68.11 29.62 73.50 39.44 50.27
60% 81.42
70.97 57.10 62.16 67.84 63.78 25.71 69.98 35.41 44.36
RT
GNN0% 86.08
76.27 45.88 62.83 67.64 71.43 29.55 72.65 OOM OOM
20% 85.49
75.03 44.39 59.45 63.75 68.91 28.94 72.01 OOM OOM
40% 84.33
73.27 39.03 57.43 61.27 62.16 28.70 65.04 OOM OOM
60% 83.33
71.65 32.23 55.40 57.84 59.45 28.88 62.32 OOM OOM
ERASE0% 87.32
77.14 50.15 62.62 65.52 72.91 30.16 75.36 43.27 56.50
20% 86.83
75.43 45.76 58.26 64.26 70.23 30.01 72.89 42.71 55.08
40% 84.92
74.16 40.21 57.19 63.51 67.50 29.88 66.53 38.76 48.57
60% 83.67
72.07 33.51 53.58 59.06 62.11 28.43 63.63 35.22 40.29
ğ‘…2LP0% 87.69
78.09 73.05 87.84 88.82 87.30 38.59 86.78 53.80 63.04
20% 87.23 76.54 70.28
87.03 88.43 86.49 38.46 84.72 52.18 59.86
40% 85.90
75.16 66.23 80.00 83.92 78.37 37.51 83.03 50.42 57.33
60% 84.37
73.91 61.40 74.32 81.18 73.24 36.82 81.48 48.01 55.29
 
444