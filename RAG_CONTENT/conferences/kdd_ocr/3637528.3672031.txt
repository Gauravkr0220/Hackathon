Scalable Differentiable Causal Discovery in the Presence of
Latent Confounders with Skeleton Posterior
Pingchuan Ma
pmaab@cse.ust.hk
HKUST
Hong Kong SARRui Dingâˆ—
juding@microsoft.com
Microsoft Research
Beijing, ChinaQiang Fu
qifu@microsoft.com
Microsoft Research
Beijing, ChinaJiaru Zhang
jiaruzhang@sjtu.edu.cn
Shanghai Jiao Tong
University
Shanghai, China
Shuai Wang
shuaiw@cse.ust.hk
HKUST
Hong Kong SARShi Han
shihan@microsoft.com
Microsoft Research
Beijing, ChinaDongmei Zhang
dongmeiz@microsoft.com
Microsoft Research
Beijing, China
ABSTRACT
Differentiable causal discovery has made significant advancements
in the learning of directed acyclic graphs. However, its application to
real-world datasets remains restricted due to the ubiquity of latent
confounders and the requirement to learn maximal ancestral graphs
(MAGs). To date, existing differentiable MAG learning algorithms
have been limited to small datasets and failed to scale to larger ones
(e.g., with more than 50 variables).
The key insight in this paper is that the causal skeleton, which
is the undirected version of the causal graph, has potential for im-
proving accuracy and reducing the search space of the optimization
procedure, thereby enhancing the performance of differentiable
causal discovery. Therefore, we seek to address a two-fold challenge
to harness the potential of the causal skeleton for differentiable
causal discovery in the presence of latent confounders: (1) scalable
and accurate estimation of skeleton and (2) universal integration of
skeleton estimation with differentiable causal discovery.
To this end, we propose SPOT (Skeleton Posterior-guided OpTi-
mization), a two-phase framework that harnesses skeleton posterior
for differentiable causal discovery in the presence of latent con-
founders. On the contrary to a â€œpoint-estimationâ€, SPOT seeks to
estimate the posterior distribution of skeletons given the dataset.
It first formulates the posterior inference as an instance of amor-
tized inference problem and concretizes it with a supervised causal
learning (SCL)-enabled solution to estimate the skeleton posterior.
To incorporate the skeleton posterior with differentiable causal dis-
covery, SPOT then features a skeleton posterior-guided stochastic
optimization procedure to guide the optimization of MAGs.
Extensive experiments on various datasets show that SPOT sub-
stantially outperforms SOTA methods for MAG learning. SPOT also
demonstrates its effectiveness in the accuracy of skeleton posterior
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $15.00
https://doi.org/10.1145/3637528.3672031estimation in comparison with non-parametric bootstrap-based,
or more recently, variational inference-based methods. Finally, we
observe that the adoption of skeleton posterior exhibits strong
promise in various causal discovery tasks.
CCS CONCEPTS
â€¢Mathematics of computing â†’Bayesian networks; Causal
networks; â€¢Computing methodologies â†’Causal reasoning and
diagnostics .
KEYWORDS
causal discovery, Bayesian network
ACM Reference Format:
Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han,
and Dongmei Zhang. 2024. Scalable Differentiable Causal Discovery in the
Presence of Latent Confounders with Skeleton Posterior. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3672031
1 INTRODUCTION
Causal discovery in the presence of latent confounders is a long-
standing problem [ 35,43]. Under this setting, the causal relations
are typically represented by a maximal ancestral graph (MAG) [ 35],
a special class of acyclic directed mixed graphs (ADMGs). MAG
learning has historically been conducted by either constraint-based
methods, such as FCI [ 35,43], RFCI [ 10] and ICD [ 32], or by score-
based methods, such as M3HC [ 37], AGIP [ 7] and GPS [ 9]. In recent
years, differentiable causal discovery has emerged as a promising
approach to enhance the accuracy and efficiency of existing meth-
ods [ 39,45]. By recasting the combinatorial constraint of graph
structure into a differentiable form, continuous optimization tech-
niques can be applied in an â€œout-of-the-boxâ€ manner.
The goal of differentiable methods is to identify the ancestral
ADMG, and apply maximal ancestral projection, a standard process,
to generate the corresponding MAG. Despite the encouraging re-
sults achieved thus far, they struggle with large-scale causal graphs,
particularly those containing more than 20 variables. One widely-
used method, ABIC [ 5], acknowledges its limitations with datasets
of only 10â€“15 variables (i.e., a causal graph with 10-15 nodes), which
restricts its broader applicability. Likewise, N-ADMG [ 3], another
 
2148
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pingchuan Ma et al.
Table 1: Boosting differentiable causal discovery with skele-
ton information. (avg. on ten datasets with 50-100 variables.)
Metho
d Skeleton
F1 Arr
owhead F1 T
ail F1
ABIC 0.84 0.76 0.67
w/
True Skeleton 1.00 +19% 0.96 +26% 0.96 +33%
w/
FCI 0.87 +4% 0.84 +11% 0.71 +6%
SPO
T 0.91 +8% 0.86 +13% 0.78 +16%
differentiable method for ADMG learning, suffers from similar scal-
ability issues. Upon closer examination, we find that these methods
are both inefficient and inaccurate when working with large-scale
causal graphs. For example, ABIC may take days to converge and
sometimes produces cyclic graphs when dealing with 50 nodes. This
is likely due to the inherent challenges of learning from causally
insufficient data, as ABIC must manage a more complicated ob-
jective function, additional optimization variables, and complex
constraints. Consequently, the search space of large datasets poses
significant challenges for standard optimization techniques, such
as L-BFGS [15] or gradient descent.
On the other hand, it has been demonstrated that adjacency in-
formation (i.e., the skeleton) can be utilized as a pre/post-processing
step to facilitate differentiable methods for learning DAGs (directed
acyclic graphs). For instance, CDRL [ 46] uses the CAM [ 6] to
rule out spurious edges while ML4S [ 26] investigates using skele-
ton in a pre-processing step to preclude superfluous variables for
NOTEARS [ 45]. These approaches show the potential for boosting
differentiable DAG learning by leveraging skeleton information.
Key Observation. These promising results on DAG learning mo-
tivate us to investigate whether MAG learning can benefit from a
high-quality estimation of the skeleton. To this end, we implement
two ABIC variants, one using the ground-truth skeleton as prior
knowledge and the other using the skeleton learnt by FCI, to vali-
date our hypotheses. In both variants, we black out the edges that
are not in the skeleton and then run ABIC to only optimize the
remaining edges. In this way, the optimization procedure is only
applied on a subset of variables rather than the entire adjacency
matrix. Then, we evaluate them on ten synthetic datasets with
50-100 variables and 1000 samples (see Table 1). In particular, we
find that ABIC is impressively accurate when the true skeleton is
known (3rd row in Table 1). This result, to a considerable extent,
validates our hypothesis that the skeleton can be used to reduce
the search space of the optimization procedure and therefore boost
the performance of differentiable MAG learning.
On the contrary, we also find that when the ground-truth skele-
ton is unknown and the used skeleton is learned by FCI (4th row
in Table 1), the improvement becomes modest because many erro-
neous/missing edges induced by FCI propagate to the subsequent
optimization procedure. This indicates the practical hurdle of using
the skeleton to facilitate differentiable causal discovery. Indeed, due
to finite samples, learning a high-quality skeleton is challenging.
Simply putting a â€œpoint estimation â€ of the skeleton as a hard con-
straint on the optimization procedure can result in considerable
missing edges (i.e., false negatives) and finally impair performance.
The above preliminary results shed light on a possible solution
that synergistically combines differentiable causal discovery with
skeleton information, while alleviating error propagations. Recently,much research has promoted the value of posterior distribution
of DAGs [ 11,24,25]. Enlightened by these works, we advocate
estimating a posterior distribution (i.e.,ğ‘(ğ‘†| D) whereğ‘†is the
skeleton andDis the dataset) over all skeletons to replace the
conventional â€œpoint estimation.â€ (i.e., arg maxğ‘†ğ‘(ğ‘†| D) ) The
posterior effectively quantifies epistemic uncertainty and the degree
of confidence to any skeletons w.r.t. the given dataset. Nonetheless,
unlike DAGs, skeletons over causally insufficient variables do not
have an explicit form of likelihood function (i.e., ğ‘(D,ğ‘†)).
To address these challenges, we propose SPOT (Skeleton Posterior-
guided OpTimization) as a two-phase framework for facilitating
differentiable causal discovery in the presence of latent confounders.
SPOT first performs amortized variational inference to estimate
skeleton posterior, SPOT then employs a novel optimization proce-
dure for boosting subsequent differentiable causal discovery. Specif-
ically, SPOT leverages a recent advancement to amortize the varia-
tional inference into the joint distribution of data and corresponding
skeletonğ‘(D,ğ‘†)and alleviates the need of an explicit likelihood
function. Then, it concretizes the amortized inference with a su-
pervised model to estimate the skeleton posterior. To effectively
facilitate the optimization procedure, SPOT employs the skeleton
posterior to stochastically update variables in each optimization
step, instead of deterministically updating optimization variables
with gradients.
As shown in the last row of Table 1, SPOT improves the perfor-
mance of differentiable causal discovery methods by a notable mar-
gin. In Sec. 4, we conduct extensive experiments on various large-
scale datasets and show that SPOT delivers 8% improvement on
skeleton F1 score and 13% and 16% improvement on arrowhead and
tail F1 scores, respectively, which are representative metrics for eval-
uating the accuracy of the learned ADMGs. We also demonstrate
that the skeleton posterior estimated by SPOT is highly accurate
compared to other variational inference-based and non-parametric
bootstrap-based solutions. Finally, we explore the versatile applica-
tions of SPOT in MAG learning for non-linear causally insufficient
datasets and also in DAG learning methods. Our empirical results
indicate the strong potential of SPOT in such scenarios.
In summary, we make the following contributions:
(1)Conceptually, we advocate a novel focus of using the skele-
ton posterior to facilitate differentiable causal discovery in the
presence of latent confounders.
(2)Technically, we formulate the problem of skeleton posterior
estimation under amortized inference framework and propose
a supervised learning-based solution to estimate the skeleton
posterior from observational data.
(3)On the basis of the skeleton posterior, we propose SPOT, a novel
stochastic optimization procedure, to facilitate differentiable
causal discovery in the presence of latent confounders, which
incorporates the skeleton posterior in a stochastic manner.
(4)Empirically, SPOT demonstrates superior performance on nearly
all evaluation metrics and various datasets, substantially im-
proving upon its counterpart. We also explore the extension of
SPOT to other causal discovery settings, including non-linear
ADMG and DAG. Our results show that SPOT consistently
improves state-of-the-art methods in these settings.
 
2149Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Open-source and Extended Version. Our code will be made pub-
licly available at [ 31]. We also maintain an extended version of this
paper on arXiv [ 27] with additional experiments and discussions.
2 PRELIMINARY
As in many previous works [ 5,7,9,37], we focus on discovering
MAG from a linear Gaussian structural causal model (SCM) in the
presence of latent confounders and assume the absence of selection
bias (no undirected edges in ADMGs). In this section, we introduce
preliminary knowledge of linear Gaussian SCM with latent con-
founders, differentiable causal discovery and amortized inference.
2.1 Linear Gaussian SCM with Latent
Confounders
We first start with the definition of a linear Gaussian SCM without
latent confounders. Consider a linear SCM with ğ‘‘observable vari-
ables parameterized by a coefficient matrix ğ›¿âˆˆRğ‘‘Ã—ğ‘‘. The SCM
can be written as
ğ‘‰ğ‘–â†âˆ‘ï¸
ğ‘‰ğ‘—âˆˆPAğ‘–ğ›¿ğ‘—ğ‘–ğ‘‰ğ‘—+ğœ–ğ‘– (1)
where PAğ‘–are the parents of ğ‘‰ğ‘–andğœ–ğ‘–is a noise term that is mutu-
ally independent of all other noise terms.
The noise term ğœ–ğ‘–is mutually independent of others if and only
ifğ‘‰ğ‘–has no latent confounder. Otherwise, ğœ–ğ‘–is correlated with ğœ–ğ‘—if
ğ‘‰ğ‘—shares a latent confounder with ğ‘‰ğ‘–. When the noise terms are
Gaussian, the correlation can be expressed by a covariance matrix
ğ›½=E[ğœ–ğœ–ğ‘‡]and the joint distribution marginalized over observable
variables ğ‘½Oforms a zero-mean multivariate Gaussian distribution
with covariance matrix as Î£=(ğ¼âˆ’ğ›¿)âˆ’ğ‘‡ğ›½(ğ¼âˆ’ğ›¿)âˆ’1. The induced
graphğºis an ADMG and contains two types of edges, including
directed edges (â†’) and bidirected edges ( â†”), which implies two
adjacency matrices, namely ğ·andğµ. In the special case where
there is no latent confounder, the ADMG is equivalent to a DAG
and the adjacency matrix of bidirected edges ğµis all zeros.ğ‘‰ğ‘–â†’ğ‘‰ğ‘—
exists inğºandğ·ğ‘–ğ‘—=1if and only if ğ›¿ğ‘–ğ‘—â‰ 0.ğ‘‰ğ‘–â†”ğ‘‰ğ‘—exists inğº
andğµğ‘–ğ‘—=1if and only if ğ›½ğ‘–ğ‘—â‰ 0. It is commonly assumed that
ğ›¿ğ‘–ğ‘—=ğ›¿ğ‘—ğ‘–=ğ›½ğ‘–ğ‘—=ğ›½ğ‘—ğ‘–=0if and only if ğ·ğ‘–ğ‘—=ğ·ğ‘—ğ‘–=ğµğ‘–ğ‘—=ğµğ‘—ğ‘–=0.
Remark. In the context of ADMGs, the noise term ğœ–does not exclu-
sively represent the exogenous variables. In particular, ğœ–ğ‘–in Eqn. 1 in-
cludes both the exogenous variable and the confounding effect from ğ‘‰ğ‘¢
(the latent confounder). Similarly, the noise term on another variable
ğ‘‰ğ‘—,ğœ–ğ‘—, also includes its own exogenous variable and the confounding
effect fromğ‘‰ğ‘¢. Therefore,ğœ–ğ‘–andğœ–ğ‘—are jointly influenced by the con-
founding effect from ğ‘‰ğ‘¢, which makes them dependent. We provide a
more detailed explanation below.
Consider the causal graph ğ‘‰ğ‘¢ğ‘–â†’ğ‘‰ğ‘–â†ğ‘‰ğ‘¢â†’ğ‘‰ğ‘—â†ğ‘‰ğ‘¢ğ‘—. Without
loss of generality on the linear setting, we have ğ‘‰ğ‘–=ğ‘‰ğ‘¢ğ‘–+ğ‘ğ‘‰ğ‘¢and
ğ‘‰ğ‘—=ğ‘‰ğ‘¢ğ‘—+ğ‘ğ‘‰ğ‘¢, whereğ‘,ğ‘are non-zero coefficients. Given that only
ğ‘‰ğ‘–andğ‘‰ğ‘—are observed, the resulting ADMG should be ğ‘‰ğ‘–â†”ğ‘‰ğ‘—, based
on Sec. 2.2 of [ 43]. In Eqn. 1,ğ‘‰ğ‘–=Ã
ğ‘‰ğ‘˜âˆˆğ‘ƒğ‘(ğ‘‰ğ‘–)ğ›¿ğ‘˜ğ‘–ğ‘‰ğ‘˜+ğœ–ğ‘–whereğ‘ƒğ‘(ğ‘‰ğ‘–)
are the parents of ğ‘‰ğ‘–in the ADMG. Since there is a bidirectional edge
ğ‘‰ğ‘–â†”ğ‘‰ğ‘—, neitherğ‘‰ğ‘–is the parent of ğ‘‰ğ‘—norğ‘‰ğ‘—the parent of ğ‘‰ğ‘–. Hence,
ğ‘ƒğ‘(ğ‘‰ğ‘–)=ğ‘ƒğ‘(ğ‘‰ğ‘—)=âˆ…. Thus, we have ğ‘‰ğ‘–=ğœ–ğ‘–andğ‘‰ğ‘—=ğœ–ğ‘—. However,
sinceğ‘‰ğ‘–andğ‘‰ğ‘—are dependent, it follows that ğœ–ğ‘–andğœ–ğ‘—must also be
dependent.According to the causal Markov and faithfulness assumption,
ancestral ADMG, MAG and skeleton can be defined as follows.
Definition 1 (Ancestral Acyclic Directed Mixed Graph). ğºis an
ancestral ADMG if it is a mixed graph with directed edges ( â†’)
and bidirected edges ( â†”) and contains no directed cycles or almost
directed cycles [43].
Definition 2 (Maximal Ancestral Graph). An ancestral ADMG
ğºis a Maximal Ancestral Graph if for each pair of non-adjacent
nodes, there exists a set of nodes that make them m-separated [ 43].
Definition 3 (Skeleton). An undirected graph ğ‘†is a skeleton of an
MAGğºifğ‘†is obtained from ğºby replacing all directed edges with
undirected edges.
Therefore, two nodes ğ‘‰ğ‘–,ğ‘‰ğ‘—are adjacent in the skeleton if and
only ifâˆ€ğ’âŠ†ğ‘½O\{ğ‘‰ğ‘–,ğ‘‰ğ‘—},ğ‘‰ğ‘–Ì¸âŠ¥ğ‘‰ğ‘—|ğ’, whereâŠ¥andÌ¸âŠ¥denote
conditional independence and dependence, respectively. Hence, it
is clear that given a dataset whether two nodes are adjacent is not
influenced by the adjacencies of other nodes.
Parameter Estimation of Linear Gaussian SCM. For DAGs,
one easily can estimate the parameters of the SCM by solving a
simple least squares regression problem. However, for ADMGs, the
estimation of the parameters is more challenging due to the pres-
ence of latent confounders. Drton et al . [14] proposed an iterative
algorithm to estimate the parameters of the SCM called Residual
Iterative Conditional Fitting (RICF). RICF generally works for bow-
free ADMGs and ancestral ADMGs, which are our focus in this
paper, are special cases of bow-free ADMGs [ 5]. This algorithm
iteratively fits the SCM to the data and updates the covariance ma-
trix. The algorithm is guaranteed to converge to a local minimum
when the corresponding ADMGs are ancestral.
2.2 Differentiable Causal Discovery
Score-based methods aim to maximize the score (e.g., log-likelihood)
of the graph on the given data, which can be written in the following
form:
arg max
ğºğ‘“(ğº)s.t.ğºis acyclic (2)
whereğ‘“(Â·)is the score function. Given the acyclicity constraint,
the optimization process is combinatorial. Recently, differentiable
methods reformulate this combinatorial constraint into a constraint
â„DAG(ğ‘Š)=tr(ğ‘’ğ‘Šâ—¦ğ‘Š)âˆ’ğ‘‘such that
â„DAG(ğ‘Š)=0â‡â‡’ğºis acyclic (3)
whereğ‘‘=|ğ‘½O|andğ‘Šis a weighted adjacency matrix of ğº. For
linear SCM, an element in ğ‘Šrepresents the linear coefficient. As a
continuous optimization with equality constraints, the augmented
Lagrangian method (ALM) is commonly used to convert the con-
strained optimization problem into several unconstrained subprob-
lems and use standard optimizers to solve them separately [45].
Despite the success of differentiable methods for DAG learning,
the algebraic characterization in Eqn. 3 cannot be directly applied
to ADMGs (and MAGs). As aforementioned, ADMGs requires two
adjacency matrices, ğ·andğµ, to represent directed edges and bidi-
rected edges, respectively. To extend the algebraic characterization
to ADMGs, ABIC [5] modified it as
â„ADMG(ğ·,ğµ)=tr(ğ‘’ğ·)âˆ’ğ‘‘+sum(ğ‘’ğ·â—¦ğµ) (4)
 
2150KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pingchuan Ma et al.
Algorithm 1: ABIC [5]
Input: DatasetD
Output: Adjacency Matrix ğ·,ğµ
1Initializeğ›¿(1,1),ğ›½(1,1);
2Defineâ„(ğ›¿,ğ›½)according to Eqn. 4;
3foreachğ‘¡1=1,Â·Â·Â·,ğ‘‡ALMdo
4 foreachğ‘¡2=1,Â·Â·Â·,ğ‘‡RICFdo
5 Update pseudovariables by ğ›¿(ğ‘¡1,ğ‘¡2),ğ›½(ğ‘¡1,ğ‘¡2);
6 Constituteğ‘“(ğ›¿,ğ›½);
7ğ›¿(ğ‘¡1,ğ‘¡2+1),ğ›½(ğ‘¡1,ğ‘¡2+1)â†arg minğ‘“(ğ›¿,ğ›½);
8 end
9 Updateğ›¼(ğ‘¡1),ğœŒ(ğ‘¡1),ğœ†(ğ‘¡1);
10end
11ğ·â†1(|ğ›¿(ğ‘‡ALM,ğ‘‡RICF)>ğœ”|);
12ğµâ†1(|ğ›½(ğ‘‡ALM,ğ‘‡RICF)>ğœ”|);
13returnğ·,ğµ;
whereâ—¦denotes the Hadamard product, i.e., element-wise multi-
plication, and sum(Â·)is the sum of all elements in a matrix. It has
been proved that â„ADMG(ğ·,ğµ)=0â‡â‡’ğºis ancestral ADMG .
In a nutshell, tr(ğ‘’ğ·)âˆ’ğ‘‘implies the standard directed acyclicity
constraint and the last term sum(ğ‘’ğ·â—¦ğµ)is a term to ensure that
the bidirected edges does not introduce â€œalmost directed cyclesâ€. In
the following, we use â„to denoteâ„ADMG for simplicity.
Now, we introduce ABIC [ 5] in Alg. 1. To ease presentation, we
omit technical details (e.g., calculations of ğ›¼,ğœŒ,ğœ† , pseudovariables,
and stopping criteria) and refer readers to the original paper for the
full version. The ABIC algorithm aims to maximize the likelihood of
ADMG over the observational data D, ensuringâ„ADMG(ğ›¿,ğ›½)=0.
It employs two nested loops: the outer loop applies the augmented
Lagrangian method (ALM) to convert the equality-constrained prob-
lem intoğ‘‡ALM steps of unconstrained ones (lines 3â€“10 in Alg. 1).
After ALM,ğ›¿andğ›½represent the ADMGâ€™s linear coefficients and co-
variance, respectively. The adjacency matrix is derived by applying
a threshold to remove negligible coefficients (lines 22â€“23), yielding
the ancestral ADMG. If a MAG is required, the maximal ancestral
projection, a standard post-processing procedure, is performed.
Inside the inner loop, Alg. 1 iteratively adjust the objective func-
tion (lines 5â€“6) and solve the unconstrained optimization problem
accordingly (line 7). In particular, the objective function ğ‘“(ğ›¿,ğ›½)
(line 6) consists of the least square loss to fit the ADMG to the
data, the constraint â„(ğ›¿,ğ›½), and a regularization term to enforce
sparsity. The form of ğ‘“(ğ›¿,ğ›½)is updated according to the current
pseudovariables (line 5). Then, in the optimization phase (line 7),
optimization algorithms are used in an â€œout-of-the-boxâ€ manner
to find a minimum of ğ‘“(ğ›¿,ğ›½). However, due to the complexity and
non-convex nature of ğ‘“(ğ›¿,ğ›½), it is challenging for standard opti-
mization techniques to find a plausible solution, especially when
the ADMG is large, as we will show in Sec. 4.
2.3 Amortized Inference for Causal Discovery
In the conventional variational inference framework, the posterior
distribution is approximated by a parametric distribution ğ‘ğœ™(ğº|D)
with parameters ğœ™. Here,ğºis the causal graph and Dis the ob-
served data in the context of causal discovery. The parameters arelearned by minimizing the KL divergence between the true poste-
riorğ‘(ğº|D)and the parametric distribution ğ‘ğœ™(ğº|D). Typically,
variational inference requires the likelihood function ğ‘(D,ğº)to
compute the KL divergence. Recently, amortized inference has been
proposed to alleviate the need of an explicit likelihood function
by introducing a simulator that can yield samples from the joint
distribution ğ‘(ğº,D)[2]. Lorch et al . [25] follows this line of work
and introduces AVICI, which amortizes the inference process by
introducing a sampler that can generate samples from the simulator
and capture the domain-specific inductive biases that would have
been difficult to characterize (e.g., gene regulatory networks or
non-linear functions of random Fourier features).
Recently, SCL (supervised causal learning) has emerged as a
promising paradigm for causal discovery [ 12,22,26]. Compared
to standard machine learning models, SCL models features many
advantages tailored for causal discovery (e.g., invariant to the per-
mutation of variables). It is worth noting that the concept of â€œper-
mutation invarianceâ€ is different from the one in machine learning
setting. One dataset (a ğ‘€Ã—ğ‘matrix) is a sample to the SCL model.
It implies that the model is permutation in- and equivariant with
respect to the observation and variable dimensions of the provided
dataset, respectively [ 22]. SCL models can be trained on samples
from a known simulator. In essence, SCL can be viewed as an in-
stantiation of amortized inference when the simulator is in line
with the underlying causal mechanism of the observational data.
3SPOT
As aforementioned, skeleton information can be leveraged to en-
hance the optimization procedure of differentiable causal discovery
in the presence of latent confounders. However, a â€œpoint estimationâ€
of the skeleton is prone-to-error. In this section, we propose SPOT
(Skeleton Posterior-guided OpTimization) to leverage a probabilistic
skeleton posterior to guide the optimization of ADMGs.
As shown in Fig. 1, SPOT consists of four steps: â€a static simu-
lator first generates an initial set of data/skeleton pairs to obtain an
initial amortized inference model (blue one in Fig. 1). âwhile the ini-
tial amortized inference model already provides a good estimation
of the skeleton posterior, SPOT can optionally use a dynamic simu-
lator to generate more in-domain training instances with respect
to the input dataset. â‚with samples from the dynamic simulator,
SPOT adapts the initial amortized inference model to obtain an
updated model (yellow one in Fig. 1) for skeleton posterior infer-
ence.âƒSPOT uses the inferred skeleton posterior to enhance the
optimization procedure of differentiable causal discovery.
Conceptual Complexity. While our solution provides an ad-
ditional layer of complexity compared to standard differentiable
causal discovery, we argue that the level of complexity is generally
manageable and comparable to other causal discovery algorithms.
First, the two stages in our pipeline are independent and do not
involve joint training. Second, constraint-based causal discovery
algorithms, such as FCI, also involve two similar steps: learning the
skeleton in the first step and orienting the edges in the second step.
3.1 Skeleton Variational Objective
LetD={ğ’™1,Â·Â·Â·,ğ’™ğ‘›}âˆ¼ğ‘(ğ‘‰)be the observational dataset, where
ğ’™ğ‘–is sampled from the joint distribution ğ‘(ğ‘‰)andğ‘›is sample
 
2151Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
skeleton posteriorguide
differentiable causal discoveryInput DataStatic SimulatorData + Skeletontraining instancestrainingğ‘ƒ(ğ‘†âˆ£ğ·)Data + Skeletontraining instancesdomainadaptionamortized inference modelamortized inference modelDynamic Simulatortrainingğ‘ƒ(ğ‘†âˆ£ğ·)bootstrap
Figure 1: SPOT workflow.
size. We aim to approximate the posterior over skeletons ğ‘(ğ‘†|D)
with a variational distribution ğ‘(ğ‘†;ğ‘)whereğ‘†is the (symmetric)
adjacency matrix of the skeleton and ğ‘is the variational parameters.
Thus, the skeleton estimation can be decomposed into a set of edge
estimations (i.e., the probability of adjacency) independently. Then,
the variational family of ğ‘(ğ‘†;ğ‘)is defined as
ğ‘(ğ‘†;ğ‘)=Ã–
ğ‘–<ğ‘—ğ‘(ğ‘†ğ‘–ğ‘—;ğ‘ğ‘–ğ‘—)withğ‘†ğ‘–ğ‘—âˆ¼Bernoulli(ğ‘ğ‘–ğ‘—). (5)
In this regard, we aim to find an inference model ğ‘“ğœ™(D) that pre-
dictsğ‘. This procedure can be attained by minimizing the expected
forward KL divergence from ğ‘(ğ‘†|D):
min
ğœ™Eğ‘(D)
KL ğ‘(ğ‘†|D)||ğ‘(ğ‘†;ğ‘“ğœ™(D))
(6)
Following the principle of amortized inference [ 2], we amortize
the inference and rewrite the objective as
Eğ‘(D)KL ğ‘(ğ‘†|D)||ğ‘(ğ‘†;ğ‘“ğœ™(D))
=Eğ‘(D)Eğ‘(ğ‘†|D)
logğ‘(ğ‘†|D)âˆ’ logğ‘(ğ‘†;ğ‘“ğœ™(D))
=âˆ’Eğ‘(ğ‘†)Eğ‘(D|ğ‘†)
logğ‘(ğ‘†;ğ‘“ğœ™(D))
+const.
=âˆ’Eğ‘(D,ğ‘†)
logğ‘(ğ‘†;ğ‘“ğœ™(D))
+const.(7)
Since the constant does not depend on ğœ™, we can merely minimize
L(ğœ™)Bâˆ’Eğ‘(D,ğ‘†)
logğ‘(ğ‘†;ğ‘“ğœ™(D))to obtainğœ™. In other words,
the problem is recast to train a predictive model ğ‘“ğœ™(Â·):Dâ†¦â†’ğ‘†
over the distribution ğ‘(D,ğ‘†).
Estimateğ‘(D,ğ‘†)via Static Simulator. Naturally, we can esti-
mateğ‘(D,ğ‘†)by using a simulator (e.g., ErdÅ‘s-RÃ©nyi random graph
model) then generate the corresponding dataset with pre-defined
functional forms. The simulator serves a direct sampler of ğ‘(D,ğ‘†)if
the test dataD(i.e., the input dataset on which we need to conduct
causal discovery) is in the same distribution of the static simulator.
In other words, the pair of Dand the skeleton ğ‘†is known to be
drawn from ğ‘a priori.
Estimateğ‘(D,ğ‘†)via Dynamic Simulator. To further feed the
model with more in-domain training instances, we propose to use
nonparametric bootstrap method [ 16] to estimate a MAG Ë†ğºwith
a random subset of observational data. Then, we fit the parame-
ters of the underlying SCM (i.e., ğ›¿,ğ›½; see Sec. 2.1) on the given
test observational data using the RICF algorithm [ 14]. Finally, we
regenerate the new observational data Ë†Dfrom the fitted SCM. By
repeating the above procedure, we obtain a set of data/MAG pairs
{(Ë†D1,Ë†ğº1),Â·Â·Â·} from which we can derive the samples of ğ‘(D,ğ‘†)as{(Ë†D1,Ë†ğº1+Ë†ğºğ‘‡
1),Â·Â·Â·}.1We note that, while the way latent vari-
ables affect the observed variables is implicit, in the linear Gaussian
setting, we can characterize the marginalized distribution over ob-
served variables as a zero-mean multivariate Gaussian distribution
with a covariance matrix defined as Î£=(ğ¼âˆ’ğ›¿)âˆ’ğ‘‡ğ›½(ğ¼âˆ’ğ›¿)âˆ’1, where
ğ›¿andğ›½are parameters fitted by RICF. In this way, we presume
this dynamic (dataset-dependent) simulator would provide more
relevant training instances with respect to the input dataset D.
Remark. In summary, the static simulator features a robust model
that generalizes well to the test data, even with potential domain shift,
as validated in [ 22,25]. On the other hand, the dynamic simulator
provides more in-domain training instances, which may lead to a
better performance at a fairly lightweight cost on the runtime, as will
be shown shortly in Sec. 3.2.
3.2 Skeleton Posterior Inference
ğ·!,ğ‘†!training instances from static simulatorahead-of-time trainingInput Datastatic modelruntime trainingğ·!,ğ‘†!training instances from dynamic simulatordynamic modeldomain adaption
SCL Model
Figure 2: Skeleton Inference Procedure.
In light of Sec. 3.1, the predictive model ğ‘“ğœ™(Â·):Dâ†¦â†’ğ‘†can be
instantiated by any supervised learning model. Enlightened by the
prosperous progress in SCL, we propose to use SCL model to enable
the amortized inference and estimate the skeleton posterior ğ‘(ğ‘†|
D). Below, we elaborate the design considerations of instantiating
the SCL model in SPOT.
â€Model Architecture. Existing SCL methods either use an end-
to-end model (e.g., Transformer) to directly predict the adjacency
matrix [ 22] or use a simple classifier (e.g., xgboost) to predict
the local structure (e.g., the adjacency of two variables or the v-
structure) [ 12,26]. While the design of SPOT is agnostic to the
model architecture, we anticipate to use a simple classifier, as it is
more effective and efficient for estimating ğ‘ğ‘–ğ‘—in practice. Hence,
in our implementation, we adopt a ML4S-like cascade model [ 26]
to predict the adjacency and constitute the skeleton posterior. We
formulate the problem of efficient skeleton posterior inference as
1ğºandğºğ‘‡are the adjacency matrix and its transpose, Ë†ğº1+Ë†ğºğ‘‡
1is the skeleton
adjacency matrix.
 
2152KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pingchuan Ma et al.
predicting a series of adjacencies and construct the skeleton poste-
rior. Specifically, we aim to estimate
ğ‘“ğœ™(D)ğ‘–ğ‘—=Eğ‘(D,ğ‘†)
ğ‘(ğ‘‰ğ‘–âˆ’ğ‘‰ğ‘—âˆˆğ‘†|D)
, (8)
We focus on the probability ğ‘(ğ‘‰ğ‘–âˆ’ğ‘‰ğ‘—|D) for the adjacency of
ğ‘‰ğ‘–,ğ‘‰ğ‘—givenD, which helps constitute the skeleton posterior (we
useğ‘ğ‘–ğ‘—as an abbreviation of ğ‘(ğ‘‰ğ‘–âˆ’ğ‘‰ğ‘—| D) ). Besides, we also
reproduce Ke et al . [22] â€™s Transformer-based model, and the results
show that the cascade model is more effective and efficient.
âObjective Refinement. Instead of standard causal discovery ob-
jectives (e.g., the higher F1 score), we prefer higher recall over pre-
cision to prevent missed edges. This is because the spurious edges
will be offloaded to and eventually removed by differentiable causal
discovery algorithms. Specifically, even though the SCL model may
predict a spurious edge and corresponding parameters ğ›¿ğ‘–ğ‘—,ğ›½ğ‘–ğ‘—tend
to be non-zero, the differentiable causal discovery algorithm will
gradually move the coefficients to zero to minimize the objective
function. In this regard, the spurious edges are eventually removed.
However, the missed edges are hard to recover due to the sparse
nature of the ADMG. To this end, we can either set a conservative
threshold (subject to the specific design of the SCL model) or apply
label smoothing trick to the training instances to prevent the model
from predicting zero probability for the true edges.
â‚Efficient Training. When using the dynamic simulator, the
model has to be trained from scratch with the training instances
sampled from non-parametric bootstrap. However, this would im-
pose a heavy sampling and training overheads in the runtime. To
alleviate this issue, we adopt a widely-used domain adaptation par-
adigm to reduce the training overheads by leveraging the static
model. Specifically, given a dataset, we only sample a few training
instances from the dynamic simulator. Then, we use the output
and intermediate values of the static model to augment the origi-
nal features of the samples from the dynamic simulator and train
an adapted dynamic model efficiently. In this way, the dynamic
model can rapidly adapt to the new dataset with few-shot training
instances and yields a better performance with mild cost.
3.3 Posterior-guided Optimization
Non-adjacency between ğ‘‰ğ‘–,ğ‘‰ğ‘—impliesğ›¿ğ‘–ğ‘—=ğ›¿ğ‘—ğ‘–=ğ›½ğ‘–ğ‘—=ğ›½ğ‘—ğ‘–=0.
When ground-truth skeleton is available, we enforce extra equality
constraints to optimize. However, the true skeleton is unattainable
in practice, necessitating estimating a skeleton from observational
data. We advocate using the skeleton posterior for optimization as
it encodes epistemic uncertainty.
It is unclear how to incorporate a skeleton posterior in the opti-
mization procedure. The estimated skeleton posterior from Sec. 3.2
is a continuous adjacency matrix ğ‘ğ‘–ğ‘—âˆˆ[0,1]. Thus, we cannot de-
rive meaningful constraints for coefficients/covariances. Formally,
ğ‘(ğ‘‰ğ‘–âˆ’ğ‘‰ğ‘—|D)=ğ‘ğ‘–ğ‘—implies the probability of the union of (dis-
joint) events ğ‘ƒ(ğ›¿ğ‘–ğ‘—â‰ 0âˆªğ›¿ğ‘—ğ‘–â‰ 0âˆªğ›½ğ‘–ğ‘—â‰ 0|D) equalsğ‘ğ‘–ğ‘—. Higher
posterior probability indicates higher likelihood of non-zero coeffi-
cients. In this regard, ğ‘ğ‘–ğ‘—cannot provide any additional information
regarding the value of non-zero coefficients. For example, ğ›¿ğ‘–ğ‘—=0.1
andğ›¿ğ‘–ğ‘—=0.9both comply with arbitrary positive posterior ğ‘ğ‘–ğ‘—>0
equally well. To account for the probabilistic nature of the skeleton
posterior, we propose the posterior-guided optimizer for structureAlgorithm 2: Skeleton Posterior-guided Optimizer
Input: Skeleton Posterior ğ‘, Optimization Variables ğ›¿,ğ›½,
Objective Function ğ‘“, ALM Stepğ‘¡, Temperature
Constantğ‘
Output: Optimized Variables ğ›¿,ğ›½
1ğ›¿âˆ—,ğ›½âˆ—â†arg minğ‘“(ğ›¿,ğ›½);
2forallğ›¿ğ‘–ğ‘—andğ›½ğ‘–ğ‘—do
3 ifğ›¿ğ‘–ğ‘—Ã—ğœ•ğ‘“(ğ›¿,ğ›½)
ğœ•ğ›¿ğ‘–ğ‘—>0then
4ğ›¿ğ‘–ğ‘—â†ğ›¿âˆ—
ğ‘–ğ‘—
5 else
6 Updateğ›¿ğ‘–ğ‘—â†ğ›¿âˆ—
ğ‘–ğ‘—with probability(ğ‘ğ‘–ğ‘—+ğ‘)1
ğ‘¡+1;
7 end
8end
9returnğ›¿,ğ›½;
learning as a replacement of the standard optimizer in differentiable
causal discovery (e.g., the gradient descent or the L-BFGS).
Alg. 2 presents the skeleton posterior-guided optimizer. In accor-
dance with many standard differentiable causal discovery methods,
we first optimize the objective function ğ‘“(ğ›¿,ğ›½)with the standard op-
timizer (e.g., gradient descent) to obtain the optimal coefficients and
covariances ğ›¿âˆ—,ğ›½âˆ—(line 1). Then, instead of directly taking ğ›¿âˆ—,ğ›½âˆ—as
the final result, we introduce a stochastic update scheme to update
ğ›¿,ğ›½based on the skeleton posterior ğ‘(lines 2â€“8) as follows:
ğ‘ƒ(ğ›¿ğ‘–ğ‘—â†ğ›¿âˆ—
ğ‘–ğ‘—)=(
1 ğ›¿ğ‘–ğ‘—Ã—ğœ•ğ‘“(ğ›¿,ğ›½)
ğœ•ğ›¿ğ‘–ğ‘—>0
(ğ‘ğ‘–ğ‘—+ğ‘)1
ğ‘¡+1 otherwise(9)
Ifğ›¿ğ‘–ğ‘—Ã—ğœ•ğ‘“(ğ›¿,ğ›½)
ğœ•ğ›¿ğ‘–ğ‘—>0, the update is accepted unconditionally (line
3â€“4); otherwise, it is accepted with a probability of (ğ‘ğ‘–ğ‘—+ğ‘)1
ğ‘¡+1(line
12â€“15; Fig. 3). If rejected, we keep ğ›¿ğ‘–ğ‘—unchanged (line 13). ğ›½is also
optimized in a similar way.
Conceptually, the overall procedure can be viewed as a modified
simulated annealing process [ 23].ğ›¿ğ‘–ğ‘—Ã—ğœ•ğ‘“(ğ›¿,ğ›½)
ğœ•ğ›¿ğ‘–ğ‘—>0implies that the
coefficient is â€œmoving to zeroâ€ (i.e., a weakened edge) in this update.
Given that real-world ADMGs are often sparse, â€œmoving-to-zeroâ€
update is encouraged and therefore accepted unconditionally. This
design consideration is thus an analogy of the unconditional update
in simulated annealing.
0.0 0.2 0.4 0.6 0.8 1.0
pij0.40.60.81.0Update Probabilitystep=1
step=3
step=5
step=10
step=20
Figure 3: The probability of accepting an update for different
ğ‘ğ‘–ğ‘—andğ‘¡withğ‘=0.1.
In contrast, if the coefficient is â€œmoving to non-zeroâ€ (i.e., a
strengthened edge), the update is accepted with a probability com-
puted byğ‘ğ‘–ğ‘—andğ‘¡jointly. Recall that ğ‘ğ‘–ğ‘—is the probability of the
 
2153Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
edge existence. The update is thus more encouraged with higher ğ‘ğ‘–ğ‘—.
1
ğ‘¡is a analogy of the temperature parameter in simulated annealing.
As shown in Fig. 3, an update is more likely to be rejected with
lowerğ‘¡and higherğ‘¡yields(ğ‘ğ‘–ğ‘—+ğ‘)1
ğ‘¡â‰ˆ1. With the increase of ğ‘¡,
the probability of accepting an update is gradually similar to what
would be in the vanilla optimization procedure even with a low
ğ‘ğ‘–ğ‘—. In this way, the optimization procedure is gradually stabilized.
Furthermore, it is worth noting that the scheme is robust to the
accuracy of the skeleton posterior. For spurious edges, even if ğ‘ğ‘–ğ‘—is
high, the optimization procedure of differentiable causal discovery
is still able to move the coefficients to zero given the unconditional
update for â€œedge-weakeningâ€ updates.
Convergence. In the context of differentiable causal discovery, the
concept of convergence is three-fold. First, in accordance with the
ABIC [ 5], asymptotically, convergence to the global optimum of its
objective function implies that the corresponding ADMG is within
the Markov equivalence class of the true ADMG. Second, given the
non-convexity of the objective function, it is not guaranteed that the
optimization procedure converges to the global optimum; instead, it
may result in a local minimum or a saddle point. Finally, we discuss
the impact of the stochastic update scheme on convergence to the
local minima in the following sense: Since the probability of an
update reaches 1 (see Fig. 3), the optimization procedure becomes
equivalent to the vanilla optimizer when ğ‘¡is sufficiently large. In
this regard, the stochastic update scheme does not negatively affect
the convergence to the local minima. In fact, our empirical results
show that our stochastic update scheme yields much better results
than ABIC and also surpasses other SOTA methods.
Probabilistic Update vs. Regularization. A natural question
arises: why not use regularization (e.g., a penalty term) to incorpo-
rate the skeleton posterior? We argue that the penalty term may
face several hurdles in practice. First, the objective function is often
unstable due to the acyclic term (Eqn. 4). Its value can range from
1Ã—10âˆ’5to1Ã—105over different stages of the optimization. Second,
as aforementioned, ğ›¿,ğ›½are not directly comparable with posterior
probability. This necessitates considerable efforts for implementa-
tion, thus we did not adopt it.
4 EVALUATION
In evaluation, we aim to evaluate the performance of SPOT from
three perspectives: âŠend-to-end causal discovery performance; â‹
standalone skeleton posterior inference performance; âŒextension
to other differentiable causal structure learning algorithms. We also
compare SPOT with a wide range of baselines.
Data Generation. We follow the similar setting from previous
works [ 4,5] to generate synthetic datasets. We generates ADMGs
from ErdÅ‘s-RÃ©nyi (ER) model with various nodes whose average
indegree isâˆˆ [1,1.5]. Each ADMG contains 5âˆ’15% bidirected
edges. Then, it is parameterized by the same rules of [ 5]: ifğ‘‰ğ‘–â†’ğ‘‰ğ‘—,
ğ›¿ğ‘–ğ‘—is uniformly sampled from Â±[0.5,2.0]; ifğ‘‰ğ‘–â†”ğ‘‰ğ‘—,ğ›½ğ‘–ğ‘—=ğ›½ğ‘—ğ‘–is
uniformly sampled from Â±[0.4,0.7]; andğ›½ğ‘–ğ‘–is uniformly sampled
fromÂ±[0.7,1.2]and add sum(|ğ›½ğ‘–,âˆ’ğ‘–|)to ensure positive definiteness.
For each ADMG, we generate 1000 samples.
Baselines. We compare our method with a wide range of base-
lines, including constraint-based methods (FCI [ 35], RFCI [ 10] andICD [ 32]), score-based methods (M3HC [ 37] and GPS [ 9]) and also
the differentiable causal discovery methods (ABIC [ 5]). The com-
parison is made on synthetic datasets with 50â€“100 nodes. We ob-
serve that ABIC occasionally outputs cyclic graphs with the default
threshold, which is unwarranted for maximal ancestral projection.
In such cases, we use the minimal threshold that produces an acyclic
graph. We also tentatively tried AGIP [ 7] algorithm and excluded
it from comparison, because its preprocessing step is consider-
ably slow (i.e., taking several days) for large datasets. Addition-
ally, we omit algorithms lacking open-source implementation (e.g.,
GreedySPo [ 4]). Since we focus on linear Gaussian data, we omit the
algorithms that do not comply with this assumption (e.g., [ 34,40])
in the main comparison. In Sec. 4.3, we explore the effectiveness of
SPOT on other settings.
Running Time. Many algorithms can take a long time to run on
large datasets and the convergence condition is not always satisfied.
To make the comparison fair, we set a timeout of 24 CPU hours for
each algorithm. For algorithms that take more than 24 CPU hours,
we halt the algorithm and use the best-so-far graph as their output.
Metrics. Under linear Gaussian SCM [ 35], we can only up to iden-
tify a Markov equivalence class. To make all baselines comparable,
for those algorithms that output an MAG, we convert it to a PAG
(Partial Ancestral Graph) and compare it with the true PAG. Fol-
lowing [ 5], we report the F1 score, TPR (True Positive Rate), and
FDR (False Discovery Rate) on the skeleton, arrowheads, and tails.
4.1 End-to-end Comparison
We report the results in Table 2 on ten datasets with nodes sampled
between[50,100]. We observe that the large datasets pose a con-
siderable challenge to all methods while SPOT accurately identifies
the underlying causal structure. ICD fails on one dataset; M3HC
and GPS frequently terminate with convergence warnings on these
datasets. We also observe that constraint-based methods are usually
more powerful at identifying the correct skeleton while all methods
have notable difficulties to accurately identify the arrowheads and
tails. In contrast, SPOT consistently manifests its superiority across
nearly all criteria and yields more precise and stable estimation of
skeletons, arrowheads, and tails. Over the ten datasets, it signifi-
cantly improves the performance of its counterpart, ABIC (with
p-value of 0.008). We also investigate the criterion (i.e., Skeletonâ€™s
FDR) on which SPOT is sub-optimal while M3HC is the best. Hav-
ing said that, we observe that M3HC also suffers from the lowest
TPR and F1 scores on skeleton. Hence, we conclude that M3HC
may be too strict on confirming an edge while SPOT is more robust
and attains the best F1 scores.
It is worth noting that SPOTâ€™s strong performance can be at-
tributed to its ability to effectively balance precision and recall in
estimating the underlying causal relationships. Additionally, the
more consistent performance of SPOT could potentially make it
more suitable for various real-world applications where accuracy
and robustness are crucial factors.
Comparison by Node Size. In addition to the aggregated results,
we also report the edge-wise F1 scores under different node sizes
in Fig. 4 where the data point of each node size is the average of
three datasets and the error bar is the standard deviation. First, we
observe that SPOT consistently outperforms all baselines across all
 
2154KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pingchuan Ma et al.
Table 2: End-to-end comparison (avg. on ten datasets with nodes âˆˆ[50,100]).
Metho
dSkeleton Arr
owhead T
ail #Faile
d
F1
TPR FDR F1
TPR FDR F1
TPR FDR Datasets
FCI 0.84Â±0.06
0.79Â±0.10 0.10Â±0.05 0.57Â±0.05
0.78Â±0.15 0.54Â±0.04 0.53Â±0.18
0.46Â±0.20 0.31Â±0.15 0/10
RFCI 0.85Â±0.07
0.77Â±0.09 0.05Â±0.03 0.62Â±0.09
0.76Â±0.12 0.45Â±0.12 0.51Â±0.15
0.42Â±0.17 0.27Â±0.14 0/10
ICD 0.82Â±0.09
0.81Â±0.07 0.17Â±0.11 0.56Â±0.10
0.82Â±0.06 0.57Â±0.12 0.48Â±0.15
0.41Â±0.15 0.39Â±0.18 1/10
M3HC 0.73Â±0.09
0.59Â±0.12 0.03Â± 0.03 0.56Â±0.15
0.46Â±0.16 0.27Â±0.11 0.32Â±0.11
0.24Â±0.10 0.48Â±0.13 0/10
GPS 0.73Â±0.10
0.81Â±0.11 0.33Â±0.11 0.62Â±0.09
0.85Â±0.11 0.50Â±0.10 0.43Â±0.12
0.62Â±0.19 0.66Â±0.10 0/10
ABIC 0.84Â±0.04
0.89Â±0.07 0.19Â±0.07 0.76Â±0.07
0.85Â±0.15 0.29Â±0.11 0.67Â±0.07
0.80Â±0.15 0.40Â±0.07 0/10
SPO
T 0.91Â±0.03 0.94Â±0.04
0.11Â±0.04 0.86Â±0.05 0.89Â±0.10 0.16Â±0.06 0.78Â±0.09 0.86Â±0.13 0.27Â±0.13 0/10
10 20 30 40 50 60 70 80 90 100
Number of nodes0.20.40.60.81.0F1
ABIC SPOT
Figure 4: Comparison of ABIC and SPOT on different node
sizes.
node sizes except for 20-node graphs. For 20-node graphs, poten-
tially due to randomness, though ABIC slightly outperforms SPOT,
the difference is not statistically significant ( ğ‘=0.51). In other
settings, especially for large graphs, SPOT outperforms ABIC (the
second-best method) by a large margin (e.g., ğ‘=0.001on 90-node
graphs). Second, the performance of ABIC significantly drops on
100-node graphs where it fails to learn a meaningful causal struc-
ture. In contrast, SPOT, by incorporating the skeleton posterior in
the optimization, manifests stable and strong performance across
all node sizes with a small standard deviation.
4.2 Effectiveness of Skeleton Posterior
Inference
Through this experiment, our goal is to determine if SPOT is a reli-
able skeleton posterior estimator. We assess the posterior quality by
calculating the KL-divergence between the estimated posterior and
ground-truth skeleton. Following earlier research [ 25], we report
AUPRC and AUROC for edge predictions to evaluate the quality of
the estimated posterior.
Using the same datasets from Sec. 4.1, we compare SPOT to
three baselines: AVICI [ 25], FCI* (Nonparametric Bootstrap FCI),
and RFCI* (Nonparametric Bootstrap RFCI). We tried to include
more baselines (e.g., DiBS [ 24] and N-ADMG [ 3]) but they either
frequently crash or encounter out-of-memory issues when process-
ing large graphs used in the experiment. These baselines are not
designed for skeleton posterior inference; however, we sum the
probabilities of all edge types, calibrate to a maximum of 1, and
recast their output as a skeleton posterior for a meaningful com-
parison in Table 3. In addition, we also conduct an ablation study
by replacing the ML4S-like model with an end-to-end Transformer-
based model (denoted as SPOT (w/ Ke et al . [22] )) to justify our
design consideration in Sec. 3.2.Table 3: Evaluation on skeleton posterior inference (avg. on
ten datasets). AUROC and AUPRC: higher is better; KL: lower
is better.
Metric A
VICI FCI* RFCI* SPO
T
AUROC 0.96 0.97 0.95 0.99
AUPRC 0.83 0.92 0.91 0.97
KL 0.05 0.06 0.10 0.03
Comparison with Baselines. Overall, we observe that SPOT con-
sistently outperforms the baselines across all criteria in Table 3.
This finding is interpreted as encouraging and reasonable. First,
SPOT employs a machine learning model to estimate the adjacency
probability while FCI and RFCI apply hard thresholds over p-values
to reject edges. Since the relationship between p-values and adja-
cency probabilities may be intricate, SPOT is more adaptable in
capturing these dependencies. Second, despite having a similar fo-
cus on variational inference, AVICI is not intended for use with
causally insufficient data, and the size of the graph may make it
more challenging to perform whole-graph inference. In contrast,
SPOT efficiently resolves this problem by edge decomposition.
Ablation Study. We also observe that the Transformer-based
model (SPOT (w/ Ke et al . [22] )) yields a lower AUROC (0.95) and
AUPRC (0.77) as well as a higher KL (0.07) than the ML4S-like
cascade model (i.e., worse performance on all criteria). This find-
ing is consistent with our design consideration in Sec. 3.2 that the
cascade model is more suitable for the task of skeleton posterior in-
ference. We presume that the cascade model effectively decompose
the whole graph into edge predictions and reduce the complexity
of the task, which is crucial for large graphs.
Out-of-Distribution Setting. In addition, we also explore the
effectiveness of the domain adaption procedure in SPOT for out-of-
distribution datasets. To do so, we generate ten ADMGs from the
Scale-Free (SF) model as test datasets and evaluate the enhancement
of our domain adaption strategy. On the SF graphs, the original
model trained on ER graphs suffers from a downgrade on KL (from
0.03 to 0.06). When augmented with the lightweight domain adap-
tion step, the KL is further reduced to 0.05 (16.7% enhancement).
4.3 Extension to Other Settings
Though the primary focus of SPOT lies in linear Gaussian data with
latent confounders, we also explore its effectiveness on other types
of data. In particular, we integrate SPOT with neural variational
ADMG learning [ 3] (N-ADMG) which handles non-linear data and
 
2155Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
neural variational DAG learning [ 17] (DECI) and GFlowNets [ 13]
on linear Gaussian data without latent confounders.
Table 4: Integration with neural variational ADMG learning
(avg. on 100 sampled ADMGs).
SHD FDR TPR
N-
ADMG [3] 2.61 0.39 1.0
N-
ADMG + SPOT 2.07 -21% 0.34 -13% 1.0
Neural Variational ADMG Learning. Due to the scalability issue
of N-ADMG, we are unable to apply it to datasets with similar
sizes as in Sec. 4.1. Instead, we use the same dataset in its original
paper [ 3] with five nodes and 2000 samples under the non-linear
Gaussian setting. Here, we use the kernel-based conditional inde-
pendence test [ 44] to compute the required statistics in Sec. 3.2.
However, it is worth noting that SPOT itself is agnostic to the func-
tional form of the SCM as long as the distribution is faithful and the
required statistics can be appropriately computed. Since N-ADMG
yields a distribution over ADMGs, we sample 100 ADMGs from the
posterior and report the average performance in Table 4. Note that,
in this setting, because the ADMG is identifiable, we compare the
generated ADMG with the ground-truth ADMG. We observe that
SPOT significantly improves the performance of N-ADMG on all
criteria. In particular, it reduces the FDR by 13% and the SHD by 21%.
In the dataset, N-ADMG is capable of identifying all true edges (i.e.,
TPR=1.0). However, it also introduces many false positives. SPOT
effectively reduces the number of false positives and improves the
overall performance. In this regard, SPOT offers a principled way
to discourage potentially superfluous edges and thus alleviates the
overfitting issue.
Table 5: Integration with neural variational DAG learning
(avg. on 100 sampled DAGs).
Skeleton
F1 Orientation
F1 SHD
DECI
[17] 0.55 0.44 121.2
DECI
+SPOT 0.58 +5% 0.48 +9% 118.7 -2%
Neural Variational DAG Learning. Since MAG is a generaliza-
tion of DAG, our framework is naturally applicable to infer the
skeleton posterior of DAGs and thus can be integrated with neural
variational DAG learning. To demonstrate this potential extension
ofSPOT, we integrate it with DECI [ 17] and report the result in
Table 5. Here, even though SPOT is originally designed for MAGs,
we can observe a non-trivial performance boost on DAGs. In par-
ticular, SPOT improves the F1 score of skeleton and orientation by
5% and 9%, respectively, and reduces the SHD by 2%. This finding
indicates a potential avenue for future research to further improve
the performance of differentiable DAG learning algorithms with
skeleton posterior. In addition to DECI, we also tried to integrate
SPOT with GFlowNets [ 13] which estimates the posterior of DAGs
using generative flow networks. With the same setting, we observe
thatSPOT reduces the expected SHD of the posterior from 339.8 to
224.4, indicating a 34% improvement. We present the full results in
the appendix due to the space limit.5 RELATED WORK
Causal Discovery with Latent Confounders. Causal discovery
with latent confounders involves constraint-based methods [ 10,28,
32,35,43] and score-based methods [ 7,9,37]. Constraint-based
methods establish causal graphs based on conditional indepen-
dence [ 30], while score-based methods find maximal likelihood
estimations. Differentiable causal discovery utilizes continuous
optimization techniques [ 5]. With the prosperity of its practical
applications [ 18â€“21,29,40],SPOT advances scalable and accurate
causal discovery with latent confounders for real-world use.
Posterior Inference of Causal Structure. Traditional causal dis-
covery provides a maximum-likelihood point estimation, but reli-
ability is often criticized for small sample sizes [ 11]. Providing a
posterior distribution is more desired using MCMC [ 38], variational
inference [ 11,24,25], reinforcement learning [ 1,41], generative
flow network [ 13], or supervised learning [ 26]. All methods es-
timate DAGsâ€™ posterior distributions and is infeasible for MAGs.
Among these methods, AVICI [ 25] is the most relevant to SPOT as
it also uses amortized variational inference. In a nutshell, it trains
an inference model over samples from a known (static) simulator
and learn the DAGs from data. In the context of ADMG learning,
N-ADMG [ 3] presents a variational inference method to estimate
the posterior of ADMGs on non-linear data. However, it is not
scalable well to large datasets, as shown in Sec. 4. SPOT trains an
inference model on these samples from static/dynamic simulators
and alleviate the additional assumption of the data.
The Role of Skeleton in Causal Discovery. Skeleton learning
is crucial for constraint-based methods [ 42] and affects overall
performance. Score-based DAG learning methods like MMHC use
skeletons to reduce search space [ 36]. Ma et al. improve the per-
formance of NOTEARS via a more precise skeleton learned by
ML4S [ 26].SPOT demonstrates the importance of skeleton learn-
ing in differentiable causal discovery. Taking these evidences into
consideration, it may become clear that skeleton learning can serve
as the backbone for general causal discovery tasks.
6 CONCLUSION
In this paper, we introduce a framework for differentiable causal
discovery in the presence of latent confounders with skeleton poste-
rior. To this end, we propose SPOT, which features a highly efficient
variational inference algorithm to estimate the underlying skele-
ton distribution from given observational data. It also provides a
stochastic optimization procedure to seamlessly incorporate the
skeleton posterior with the differentiable causal discovery pipeline
in an â€œout-of-the-boxâ€ manner. The results of our experiments are
highly encouraging and show that SPOT outperforms all existing
methods to a notable extent.
ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for their
valuable comments. The authors from HKUST were supported in
part by a RGC CRF grant under the contract C6015-23G.
REFERENCES
[1]Raj Agrawal, Caroline Uhler, and Tamara Broderick. 2018. Minimal I-MAP MCMC
for scalable structure discovery in causal DAG models. In International Conference
 
2156KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pingchuan Ma et al.
on Machine Learning. PMLR, 89â€“98.
[2]Luca Ambrogioni, Umut GÃ¼Ã§lÃ¼, Julia Berezutskaya, Eva Borne, Ya Ë‡gmur GÃ¼Ã§lÃ¼tÃ¼rk,
Max Hinne, Eric Maris, and Marcel Gerven. 2019. Forward amortized inference for
likelihood-free variational marginalization. In The 22nd International Conference
on Artificial Intelligence and Statistics. PMLR, 777â€“786.
[3]Matthew Ashman, Chao Ma, Agrin Hilmkil, Joel Jennings, and Cheng Zhang.
2022. Causal Reasoning in the Presence of Latent Confounders via Neural ADMG
Learning. In The Eleventh International Conference on Learning Representations.
[4]Daniel Bernstein, Basil Saeed, Chandler Squires, and Caroline Uhler. 2020.
Ordering-based causal structure learning in the presence of latent variables. In
International Conference on Artificial Intelligence and Statistics. PMLR, 4098â€“4108.
[5]Rohit Bhattacharya, Tushar Nagarajan, Daniel Malinsky, and Ilya Shpitser. 2021.
Differentiable causal discovery under unmeasured confounding. In International
Conference on Artificial Intelligence and Statistics. PMLR, 2314â€“2322.
[6]Peter BÃ¼hlmann, Jonas Peters, and Jan Ernest. 2014. CAM: Causal additive models,
high-dimensional order search and penalized regression. Annals of Stats. (2014).
[7]Rui Chen, Sanjeeb Dash, and Tian Gao. 2021. Integer programming for causal
structure learning in the presence of latent variables. In International Conference
on Machine Learning. PMLR, 1550â€“1560.
[8]Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785â€“794.
[9]Tom Claassen and Ioan G Bucur. 2022. Greedy equivalence search in the presence
of latent confounders. In Uncertainty in Artificial Intelligence. PMLR, 443â€“452.
[10] Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson.
2012. Learning high-dimensional directed acyclic graphs with latent and selection
variables. Annals of Stats. (2012).
[11] Chris Cundy, Aditya Grover, and Stefano Ermon. 2021. Bcd nets: Scalable varia-
tional approaches for bayesian causal discovery. Advances in Neural Information
Processing Systems 34 (2021), 7095â€“7110.
[12] Haoyue Dai, Rui Ding, Yuanyuan Jiang, Shi Han, and Dongmei Zhang. 2023.
Ml4c: Seeing causality through latent vicinity. In Proceedings of the 2023 SIAM
International Conference on Data Mining (SDM). SIAM, 226â€“234.
[13] Tristan Deleu, AntÃ³nio GÃ³is, Chris Emezue, Mansi Rankawat, Simon Lacoste-
Julien, Stefan Bauer, and Yoshua Bengio. 2022. Bayesian structure learning with
generative flow networks. In Uncertainty in Artificial Intelligence. PMLR, 518â€“528.
[14] Mathias Drton, Michael Eichler, and Thomas S Richardson. 2009. Computing
Maximum Likelihood Estimates in Recursive Linear Models with Correlated
Errors. Journal of Machine Learning Research 10, 10 (2009).
[15] Roger Fletcher. 1987. Practical methods of optimization. A Wiley Interscience
Publication (1987).
[16] Nir Friedman, Moises Goldszmidt, and Abraham Wyner. 1999. Data analysis
with bayesian networks: a bootstrap approach. In Proceedings of the Fifteenth
conference on Uncertainty in artificial intelligence. 196â€“205.
[17] Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre
Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Agrin Hilmkil, et al .2022.
Deep End-to-end Causal Inference. In NeurIPS 2022 Workshop on Causality for
Real-world Impact.
[18] Zhenlan Ji, Pingchuan Ma, Zongjie Li, and Shuai Wang. 2023. Benchmarking and
Explaining Large Language Model-based Code Generation: A Causality-Centric
Approach. arXiv preprint arXiv:2310.06680 (2023).
[19] Zhenlan Ji, Pingchuan Ma, and Shuai Wang. 2023. Perfce: Performance debugging
on databases with chaos engineering-enhanced causality analysis. In 2023 38th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 1454â€“1466.
[20] Zhenlan Ji, Pingchuan Ma, Shuai Wang, and Yanhui Li. 2023. Causality-aided
trade-off analysis for machine learning fairness. In 2023 38th IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE). IEEE, 371â€“383.
[21] Zhenlan Ji, Pingchuan Ma, Yuanyuan Yuan, and Shuai Wang. 2023. Cc: Causality-
aware coverage criterion for deep neural networks. In 2023 IEEE/ACM 45th
International Conference on Software Engineering (ICSE). IEEE, 1788â€“1800.
[22] Nan Rosemary Ke, Silvia Chiappa, Jane X Wang, Jorg Bornschein, Anirudh
Goyal, Melanie Rey, Theophane Weber, Matthew Botvinick, Michael Curtis Mozer,
and Danilo Jimenez Rezende. 2022. Learning to Induce Causal Structure. In
International Conference on Learning Representations.
[23] Scott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vecchi. 1983. Optimization by
simulated annealing. Science (1983).
[24] Lars Lorch, Jonas Rothfuss, Bernhard SchÃ¶lkopf, and Andreas Krause. 2021.
Dibs: Differentiable bayesian structure learning. Advances in Neural Information
Processing Systems 34 (2021), 24111â€“24123.
[25] Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard
SchÃ¶lkopf. 2022. Amortized Inference for Causal Structure Learning.
arXiv:2205.12934 (2022).
[26] Pingchuan Ma, Rui Ding, Haoyue Dai, Yuanyuan Jiang, Shuai Wang, Shi Han,
and Dongmei Zhang. 2022. Ml4s: Learning causal skeleton from vicinal graphs.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 1213â€“1223.[27] Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han, and
Dongmei Zhang. 2024. Scalable Differentiable Causal Discovery in the Presence of
Latent Confounders with Skeleton Posterior (Extended Version). arXiv:2406.10537
(2024).
[28] Pingchuan Ma, Rui Ding, Shuai Wang, Shi Han, and Dongmei Zhang. 2023.
XInsight: eXplainable Data Analysis Through The Lens of Causality. Proceedings
of the ACM on Management of Data 1, 2 (2023), 1â€“27.
[29] Pingchuan Ma, Zhenlan Ji, Qi Pang, and Shuai Wang. 2022. Noleaks: Differentially
private causal discovery under functional causal model. IEEE Transactions on
Information Forensics and Security 17 (2022), 2324â€“2338.
[30] Pingchuan Ma, Zhenlan Ji, Peisen Yao, Shuai Wang, and Kui Ren. 2024. Enabling
Runtime Verification of Causal Discovery Algorithms with Automated Condi-
tional Independence Reasoning. In Proceedings of the 46th IEEE/ACM International
Conference on Software Engineering. 1â€“13.
[31] Microsoft. 2024. reliableAI. https://github.com/microsoft/reliableAI.
[32] Raanan Y Rohekar, Shami Nisimov, Yaniv Gurwicz, and Gal Novik. 2021. Iterative
causal discovery in the possible presence of latent confounders and selection
bias. Advances in Neural Information Processing Systems 34 (2021), 2454â€“2465.
[33] Karen Sachs, Omar Perez, Dana Peâ€™er, Douglas A Lauffenburger, and Garry P
Nolan. 2005. Causal protein-signaling networks derived from multiparameter
single-cell data. Science (2005).
[34] Saber Salehkaleybar, AmirEmad Ghassami, Negar Kiyavash, and Kun Zhang.
2020. Learning linear non-Gaussian causal models in the presence of latent
variables. The Journal of Machine Learning Research 21, 1 (2020), 1436â€“1459.
[35] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. 2000.
Causation, prediction, and search. MIT press.
[36] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. 2006. The
max-min hill-climbing Bayesian network structure learning algorithm. Machine
learning 65 (2006), 31â€“78.
[37] Konstantinos Tsirlis, Vincenzo Lagani, Sofia Triantafillou, and Ioannis Tsamardi-
nos. 2018. On scoring maximal ancestral graphs with the maxâ€“min hill climbing
algorithm. International Journal of Approximate Reasoning 102 (2018), 74â€“85.
[38] Jussi Viinikka, Antti Hyttinen, Johan Pensar, and Mikko Koivisto. 2020. To-
wards scalable bayesian learning of causal dags. Advances in Neural Information
Processing Systems 33 (2020), 6584â€“6594.
[39] Matthew J Vowels, Necati Cihan Camgoz, and Richard Bowden. 2022. Dâ€™ya like
dags? a survey on structure learning and causal discovery. Comput. Surveys 55, 4
(2022), 1â€“36.
[40] Y Samuel Wang and Mathias Drton. 2023. Causal discovery with unobserved
confounding and non-Gaussian data. Journal of Machine Learning Research 24,
271 (2023), 1â€“61.
[41] Dezhi Yang, Guoxian Yu, Jun Wang, Zhengtian Wu, and Maozu Guo. 2023. Rein-
forcement causal structure learning on order graph. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 10737â€“10744.
[42] Kui Yu, Jiuyong Li, and Lin Liu. 2016. A review on algorithms for constraint-based
causal discovery. arXiv preprint arXiv:1611.03977 (2016).
[43] Jiji Zhang. 2008. On the completeness of orientation rules for causal discovery
in the presence of latent confounders and selection bias. Artificial Intelligence
172, 16-17 (2008), 1873â€“1896.
[44] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard SchÃ¶lkopf. 2011. Kernel-
based conditional independence test and application in causal discovery. In Pro-
ceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence.
804â€“813.
[45] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. 2018. Dags
with no tears: Continuous optimization for structure learning. Advances in neural
information processing systems 31 (2018).
[46] Shengyu Zhu, Ignavier Ng, and Zhitang Chen. 2019. Causal Discovery with
Reinforcement Learning. In International Conference on Learning Representations.
A PRELIMINARY
To keep the paper self-contained, we provide additional definitions
and notations used in the main text.
Definition 4 (Directed Cycle). Given an ADMG ğºwith a set of
nodesğ‘‰and a set of edges ğ¸, a directed cycle exists when there is
a directed path from a node ğ‘‰ğ‘–back to itself.
Definition 5 (Almost Directed Cycle). Given an ADMG ğºwith a
set of nodes ğ‘‰and a set of edges ğ¸, an almost directed cycle exists
when there is a bidirected edge ğ‘‰ğ‘–â†”ğ‘‰ğ‘—such thatğ‘‰ğ‘–âˆˆğ‘¨ğ’ğº(ğ‘‰ğ‘—).
ğ‘¨ğ’ğº(ğ‘‰ğ‘—)denotes the set of ancestors (by directed paths) of ğ‘‰ğ‘—in
ğº.
 
2157Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Definition 6 (Bow). Given an ADMG ğºwith a set of nodes ğ‘‰and
a set of edges ğ¸, a bow exists when there are two edges ğ‘‰ğ‘–â†’ğ‘‰ğ‘—
andğ‘‰ğ‘—â†”ğ‘‰ğ‘—.
Definition 7 (Bow-free ADMG). An ADMG is bow-free if it does
not contain any directed or a bow.
A path(ğ‘‹,ğ‘Š 1,Â·Â·Â·,ğ‘Šğ‘˜,ğ‘Œ)is said to be blocked byğ‘âŠ†ğ‘¿\{ğ‘‹,ğ‘Œ}
if there exists a node ğ‘Šğ‘–âˆˆ{ğ‘Š1,Â·Â·Â·,ğ‘Šğ‘˜}such that a) ğ‘Šğ‘–is not a
collider but a member of ğ‘, or b)ğ‘Šğ‘–is a collider but not an ancestor
of any nodes of ğ‘. We now introduce m-separation.
Definition 8 (m-separation [ 43]).ğ‘‹,ğ‘Œ are m-separated by ğ‘(de-
noted byğ‘‹â««Gğ‘Œ|ğ‘) if all paths between ğ‘‹,ğ‘Œ are blocked by ğ‘.
B IMPLEMENTATION OF SCL MODELS IN
SPOT
In accordance with ML4S [ 26], we implement the supervised causal
learning (SCL) model in SPOT as a series of cascade xgboost [ 8]
classifiers with default hyperparameters. For linear datasets, we
use Fisher-z test to calculate the conditional independence and
constitute the features for the classifier. For non-linear datasets,
we use the kernel-based conditional independence test [ 44] in-
stead. We use the output of the last layer of the cascade xgboost
classifiers as the posterior probability of the presence of an edge.
The implementation of the SCL-enabled SPOT is available at https:
//anonymous.4open.science/r/Spot-F67F.
0 20 40 60 80 100 120
Number of epochs0.250.500.751.00Metric value
AUROC AUPRC KL
Figure 5: Convergence of the model in Ke et al. [22].
Since the model used in Ke et al . [22] is not publicly available, we
made our best effort to replicate the model based on the description
in the paper. The experiments are conducted on a server with
NVIDIA A6000 GPU and 256GB RAM. The model is trained with
128 epochs and a batch size of 1. We use the Adam optimizer with a
learning rate of 0.0003. When trained with 128 epochs, the metrics
is seen to converge, as shown in Fig. 5. On the large-scale causal
graphs with 50â€“100 nodes, each dataset contains 1,000 observational
samples. However, the GPU (with 48GB memory which is the largest
GPU memory available to us) can at most handle 600 samples at a
time. Hence, we drop the last 400 samples in each dataset. This issue,
to a certain extent, indicates the scalability limitation of end-to-end
supervised causal learning models and promotes the necessity of
ML4S-like methods, which are much more scalable and efficient.
C ABLATION STUDY ON SPARSITY PRIOR
In our optimization procedure, we enforce a heuristic in which
sparsifying proposals are always accepted. This design attempts toanalogize a range of the regularization term in score-based causal
discovery algorithms where sparse causal graphs are preferred over
dense ones. To evaluate the impact of this design, we conduct an
ablation study by removing the sparsity prior from the optimization
procedure. The results are shown in Table 6.
Table 6: Ablation study on sparsity prior.
Skeleton
F1 Head
F1 T
ail F1
ABIC 0.84 0.76 0.67
SPO
Tw/o Sparsity 0.90 0.81 0.71
SPO
T 0.91 0.86 0.78
Overall, we observed that the strategy indeed provides a non-
trivial improvement to performance and believe that such sparsity
consideration plays an important role in â€œpruningâ€ dense causal
graphs.
D COMPUTE TIME
We refrain from providing a table regarding compute time because
different algorithms are computed on different architectures, mak-
ing it difficult to draw meaningful conclusions. For instance, FCI
can only use a single core, whereas ABIC/SPOT can utilize up to
32 cores, and N-ADMG primarily relies on GPU. Additionally, on
some large datasets, ABIC may fail to terminate even after run-
ning for one week. These issues complicate direct comparisons.
Following your suggestion and consistent with the main setup in
our paper, we report the compute times for ABIC and SPOT on the
same computing architecture below.
Overall, we believe SPOT enables lower compute time as well as
better performance.
E COMPARISON BY DIFFERENT NUMBER OF
NODES
We report the full results of the comparison of different methods on
different methods, including FCI, RFCI, ABIC and SPOT. Aligned
with our observations in Sec. 4.1, SPOT manifests superior perfor-
mance in all node sizes except on 20-node datasets.
F INTEGRATION WITH GFLOWNETS
We also explore the integration of SPOT with GFlowNets [ 13] to
further improve the performance of differentiable causal discovery.
GFlowNets is a generative flow network that learns the posterior
distribution of DAGs. We use the same experimental setup as in
Sec. 4.3, and the results are shown in Table 8. We observe that SPOT
improves the performance of GFlowNets by 13.6% in F1 score and
Table 7: Average compute time (in seconds) for ABIC and
SPOT.
Algorithm Avg. Time (sec.)
ABIC 3139
SPOT 2883
 
2158KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Pingchuan Ma et al.
10 20 30 40 50 60 70 80 90 100
Number of nodes0.20.40.60.81.0F1
FCI RFCI ABIC SPOT
Figure 6: Comparison of methods on different node sizes.
Table 8: Integration with GFlowNets [13].
F1 Pr
ecision Re
call
GF
lowNets [13] 0.22 0.14 0.45
GF
lowNets + SPOT 0.25 0.18 0.44
28.6% in precision at the cost of a slight decrease in recall. This
result demonstrates the synergistical effect of SPOT in differentiable
causal discovery.G REAL-WORLD DATA
PKC PKA
JNKP38
RafMek
ErkPlcg
AktPIP2PIP3
Figure 7: Application on Sachs dataset.
We also explore the application of our proposed framework on
Sachs [ 33], a real-world dataset with 853 entries on protein expres-
sions involved in human immune system cells. Fig. 7 presents the
promising results of SPOT applied to the Sachs dataset. Since ev-
ery edge endpoint of Sachs is unidentifiable, its PAG corresponds
exactly to the skeleton. Therefore, we evaluate the learned graphâ€™s
quality at the skeleton level. The discovered skeleton in Fig.7 achieves
an F1 score of 0.63. Although there is a decrease in performance
compared to the results in Sec. 4.1, it still significantly outperforms
ABIC (with an F1 score of 0.48) and provides satisfactory results.
The decrease in performance can be partially attributed to the non-
Gaussian nature of real-world data, which affects the fundamental
premise of ABIC, the backbone of our framework. Since SPOT itself
(i.e., skeleton posterior inference phase) is agnostic to Gaussianity,
it delivers an impressive 31.2% improvement over vanilla ABIC. We
believe that the performance of SPOT has the potential for even
further improvement by incorporating differentiable methods that
handle non-Gaussian data in future work.
 
2159