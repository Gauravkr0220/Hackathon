When Box Meets Graph Neural Network in Tag-aware
Recommendation
Fake Lin
fklin@mail.ustc.edu.cn
University of Science and Technology
of China
Hefei, ChinaZiwei Zhao
zzw22222@mail.ustc.edu.cn
University of Science and Technology
of China
Hefei, ChinaXi Zhu
xizhu@mail.ustc.edu.cn
University of Science and Technology
of China
Hefei, China
Da Zhang
zhangda17@mail.ustc.edu.cn
University of Science and Technology
of China
Hefei, ChinaShitian Shen
shitians@gmail.com
Alibaba Group
Hangzhou, ChinaXueying Li
xiaoming.lxy@alibaba-inc.com
Alibaba Group
Hangzhou, China
Tong Xuâˆ—
tongxu@ustc.edu.cn
University of Science and Technology
of China
Hefei, ChinaSuojuan Zhang
suojuanzhang@aeu.edu.cn
Army Engineering University of PLA
Nanjing, ChinaEnhong Chenâˆ—
cheneh@ustc.edu.cn
University of Science and Technology
of China
Hefei, China
ABSTRACT
Last year has witnessed the re-flourishment of tag-aware recom-
mender systems supported by the LLM-enriched tags. Unfortu-
nately, though large efforts have been made, current solutions may
fail to describe the diversity and uncertainty inherent in user prefer-
ences with only tag-driven profiles. Recently, with the development
of geometry-based techniques, e.g., box embeddings, the diver-
sity of user preferences now could be fully modeled as the range
within a box in high dimension space. However, defect still exists
as these approaches are incapable of capturing high-order neighbor
signals, i.e., semantic-rich multi-hop relations within the user-tag-
item tripartite graph, which severely limits the effectiveness of user
modeling. To deal with this challenge, in this paper, we propose
a novel framework, called BoxGNN, to perform message aggrega-
tion via combinations of logical operations, thereby incorporating
high-order signals. Specifically, we first embed users, items, and
tags as hyper-boxes rather than simple points in the representation
space, and define two logical operations, i.e., union and intersec-
tion, to facilitate the subsequent process. Next, we perform the
message aggregation mechanism via the combination of logical
operations, to obtain the corresponding high-order box represen-
tations. Finally, we adopt a volume-based learning objective with
Gumbel smoothing techniques to refine the representation of boxes.
âˆ—Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671973Extensive experiments on two publicly available datasets and one
LLM-enhanced e-commerce dataset have validated the superiority
of BoxGNN compared with various state-of-the-art baselines. The
code is released online1.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems.
KEYWORDS
Tag-aware Recommendation,Box Embeddings,Graph Neural Net-
works,Recommendation System
ACM Reference Format:
Fake Lin, Ziwei Zhao, Xi Zhu, Da Zhang, Shitian Shen, Xueying Li, Tong Xu,
Suojuan Zhang, and Enhong Chen. 2024. When Box Meets Graph Neural
Network in Tag-aware Recommendation. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671973
1 INTRODUCTION
Tag-aware Recommender System (TRS) has long been treated as a
crucial foundation to support intelligent e-commerce platforms, es-
pecially with the support of semantic-rich tags generated by large
language models (LLMs) [ 32]. Along this line, it is necessary to
capture the tag semantics for building user profiles. Traditionally,
prior arts could be roughly divided into two categories. The first
line of literature could be feature-based, which mainly focuses on
encoding tags as multi-hot vectors that can be easily processed by
following-up applications [ 3,16,42,43]. Unfortunately, they may
suffer from sparsity issue, making it difficult to depict preferences
of users who are inactive in the platform. More importantly, they
usually ignore the high-order signals, i.e., semantic-rich multi-hop
1https://github.com/critical88/BoxGNN
 
1770
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fake Lin, et al.
Figure 1: An example of collaborative tag graph that users,
items, and tags are interconnected with each other. It illus-
trates that users may buy items due to their diverse interests.
relations within the user-tag-item tripartite graph, which severely
limits the effectiveness of user profiling. To that end, some other
researchers attempt to exploit the rich semantics within high-order
signals via graph neural network (GNN) techniques [ 4,5,15,37],
which perform the message aggregation mechanism to capture
high-order signals from multi-hop neighbors, thereby improving
recommendation quality. However, these approaches usually repre-
sent users, items, and tags as fixed points within a shared vector
space. In this case, the diversity and uncertainty inherent in user
preferences could not be well addressed. For one thing, users may
exhibit varied preferences in different cases. Taking Figure 1 as an
example, it is evident that everyone has variety of preferences, such
as "Portable", "Black" and "Apple", which drives their purchasing
decision. For another, different users may favor the same item for
various motivations. Specifically, we can observe that the reasons
behind their purchases of the "iPhone" differ: the boy might buy
it for its color in black, whereas the girl might choose it for its
portability. In summary, a more comprehensive solution for the
TRS task is still urgently required.
Recently, with the development of geometric embeddings, e.g.,
box embeddings in high-dimensional space, it is possible to de-
scribe the diversity and uncertainty of user preferences [ 17] via the
intersection of high-dimensional boxes which encodes users/items,
respectively. Moreover, the box embedding mechanism could well
fit the hierarchical structure of tagging system [ 21], like â€œfootballâ€
as a subset of â€œsportâ€, which further enriches the semantic of tag rep-
resentations. Nevertheless, these methods only examine the direct
interactions (e.g., purchase records) among items and users, ne-
glecting the importance of collaborative signals from higher-order
neighbors for revealing user preferences [6, 7, 13, 33, 41].
In this paper, we aim to capture high-order collaborative signals
for solving the TRS task, while preserving the powerful represen-
tation capability of the box embedding mechanism. In detail, the
combination of logical operations on embedded boxes will be uti-
lized to simulate the message aggregation process of graphs. Along
this line, there are two challenges urged to be addressed:
(1)How to aggregate the box embeddings? The learned box
embeddings tend to be anisotropic [ 17], i.e., the length ranges
of different box dimensions vary greatly, which could easily
lead to an empty set when discovering the intersection of two
embedded boxes. Therefore, it is intractable to simply employ
interaction operations in all situations. To make matters even
worse, stacking multiple layers will further exacerbate this
issue. For instance, as depicted in Figure 2, by solely utilizing
Target Nodeğ‘¢0ğ‘¡0 ğ‘–0
ğ‘–1First order
ğ‘–0(1)=ğµğ‘œğ‘¥ğ‘¢0âˆ§ğµğ‘œğ‘¥ğ‘¡0
ğ‘¡0(1)=ğµğ‘œğ‘¥ğ‘¢0âˆ§ğµğ‘œğ‘¥ğ‘–0âˆ§ğµğ‘œğ‘¥ğ‘–1
Second order
ğ‘¢0(2)=ğµğ‘œğ‘¥(ğ‘–0(1))âˆ§ğµğ‘œğ‘¥(ğ‘¡0(1))ğ‘¡0(1)ğ‘¢0ğ‘–0
ğ‘¡0
ğ‘–1ğ‘–0(1)Figure 2: Toy example of message aggregation via intersec-
tion operation, where ğµğ‘œğ‘¥(Â·)represents the box embeddings
of nodes,ğ‘¢,ğ‘–andğ‘¡denotes user, item and tag, respectively.
Notably, we fail to obtain the second-order user box embed-
ding, asğ‘¡(1)
0andğ‘–(1)
0are disjoint.
intersections for message aggregation, we can easily obtain
the first-order aggregated nodes ğ‘–(1)
0andğ‘¡(1)
0[17]. However,
it is impractical to obtain the second-order user box ğ‘¢(2)
0,
because no overlapping parts between ğ‘–(1)
0andğ‘¡(1)
0can be
found in the vector space. In summary, we need to devise
an innovative strategy to aggregate embedded boxes on the
tripartite graph.
(2)How to measure the matching score between user and
item? To the best of our knowledge, conventional approaches
[13,19,25] typically use dot product or cosine similarity
to calculate the matching degree between users and items.
However, these methods could be hardly transferred to box
embedding scenario, as they could only reflect the informa-
tion of the box center, but not the wide range of the whole
box. Recent box-based studies, e.g., [ 8,17] typically employ
a distance formula to compute the scores between users
and items. Nonetheless, it is unwise to use the distance be-
tween two boxes as a measure of their similarity, as this
approach would overlook rich information contained within
their overlapping areas, which reflect the preference uncer-
tainty. Along this line, we propose utilizing the volume of the
intersection areas as the matching score. However, we may
encounter the gradient vanishing issue if there is no overlap
between the boxes. In summary, we are expected to formu-
late a smoothed volume-based methodology to consistently
deliver gradient signals.
To tackle the challenges, we propose a novel tag-aware rec-
ommendation framework, named BoxGNN, aiming to harness
the high-order collaborative signals based on the embedded boxes.
Specifically, we first transform the users, items, and tags into sepa-
rate box embeddings, and then implement two logical operations,
i.e. intersection and union, to facilitate the following network prop-
agation process. Afterwards, upon thoroughly assessing the role of
each node type (a.k.a. user, item, and tag), we tailor three combi-
nations of operations instead of relying solely on the intersection
operation. In this way, we can seamlessly aggregate messages of
graph neighbors and obtain the high-order box embeddings. Fi-
nally, to avoid gradient vanishing issue, we employ a Gumbel-based
technique to smooth the gradient signal and ultimately make our
proposed model trainable. Overall, the technical contributions of
this paper could be summarized as follows:
 
1771When Box Meets Graph Neural Network in Tag-aware Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢We propose a novel solution for the TRS task, which adapts
graph neural network into the box embedding mechanism
to derive high-order signals, while capturing the uncertainty
of user preference simultaneously.
â€¢We apply three strategies with two logical operations to
perform message aggregation based on the type of box (i.e.
user, item, and tag). Furthermore, gumbel-based smoothing
technique is utilized to ensure the differentiability during
the training process.
â€¢We conduct extensive experiments on two public benchmark-
ing datasets and one LLM-enhanced dataset to justify the
effectiveness of our proposed BoxGNN.
2 RELATED WORK
2.1 Tag-Aware Recommendation
With its powerful capability to express user interests, large efforts
have been devoted to tag-aware recommendations. Initially, re-
searchers extended the traditional collaborative approaches to incor-
porate information from abundant tags [ 18,20,27,34]. To alleviate
the redundancy of the tag information, cluster-based schemes [ 26]
were proposed to merge the uninformative tags. With the preva-
lence of deep learning, [ 43] attempted to learn tag-enhanced user
representations via deep autoencoders and employ collaborative
filter methods to arrive at accurate recommendations. Moreover,
AIRec [ 3] developed a hierarchical attention network to capture
multi-faceted user representations and introduced an intersection
module to derive conjunctive features from the overlap between
user and item tags.
Recently, GNN-based approaches have shown their superiority in
the tag-aware recommendation systems [ 4,37]. These models were
built upon unified graphs to express the connectivity among users,
items, and tags. Then they aggregated messages from neighbors to
enrich the ego embeddings, incorporating the tag information into
collaborative filters to facilitate the final recommendation. Specifi-
cally, TGCN [ 4] employed a novel message propagation approach
to model tag information, thereby recognizing user interests at
multiple granularities. LFGCF [ 37] borrowed the idea from the
LightGCN [ 13] to learn the high-order representations of users,
items, and tags, which boosts the recommendation performance.
Although their effectiveness, the uncertainty of user interests and
the hierarchical nature of tags remain unexplored, which results in
suboptimal performance.
2.2 Geometric Embedding
Geometric embedding techniques have garnered widespread atten-
tion due to their ability to preserve the intrinsic geometric structure
of data. These techniques map high-dimensional information into
a lower-dimensional space while retaining the significant relation-
ships and topological features of the original data [ 9,10,17,23].
At the beginning, some researchers simply used geometric embed-
dings to represent the implicit partial order relations [ 29]. Then,
this line of literature explored its strong capability to model com-
plex logical structural information in knowledge graph [ 23]. Next,
more sophisticated models were developed to adapt to a variety of
scenarios and datasets [1, 22, 38].Recently, researchers have extended this technique to the field of
recommendation systems [ 8,17]. Specifically, CubeRec [ 8] consid-
ered the groups to be the hypercubes to resolve the long-standing
issue of data sparsity and employed the self-supervision to learn
the expressive representations of groups. CBox4cr [ 17] tried to
capture the user interests by the intersection of the item sequence
that the user has interacted with. Although their achievement, they
still ignore the high-order signals in the recommendation system,
which is proven effective in various recommendation scenarios
[13, 14, 30, 39, 40].
3 PRELIMINARIES
In this section, we first introduce the formal formulation of the
tag-aware recommendation task, and then proceed to the definition
of collaborative tag graph (CTG) to support the user profiling.
3.1 Task Formulation
A social tagging system encourages users to assign a user-defined
tag to the items of interest. Therefore, tags implicitly serve as the
user interests in the items. Following prior works, suppose there
exists user setU, item setIand tag setT, where their size are
|U|=ğ‘ğ‘¢,|I|=ğ‘ğ‘–and|T|=ğ‘ğ‘¡, respectively. Then a tagging
assignment can be formulated as a triplet, i.e. ğ‘=(ğ‘¢,ğ‘¡,ğ‘–), which
denotes that a user ğ‘¢annotates a tag ğ‘¡to the itemğ‘–. Similar to [4],
the folksonomy is a tuple F=(U,I,T,A), whereAâˆˆUÃ—IÃ—T
is the assignments in typical tag-aware recommendation.
Following [ 4], we consider tags as a complement to the informa-
tion of users and items. Here we target at predicting the interaction
between users and items, which is represented by ğ‘Œâˆˆ{0,1}ğ‘ğ‘¢Ã—ğ‘ğ‘–,
where each entry ğ‘¦ğ‘¢ğ‘–=1indicates that the given user ğ‘¢âˆˆU has
interacted with item ğ‘–âˆˆI, otherwise ğ‘¦ğ‘¢ğ‘–=0. Here comes the
formal definition of the tag-aware recommendation task:
Definition 3.1. Tag-aware Recommendation System (TRS).
Given the user setU, the item setI, and their observed interactions
(annotations) ğ‘Œ, with the complementary information from T, TRS
aims to learn a model that is capable of predicting the top-k item
list that meets the interests for each user ğ‘¢âˆˆU.
3.2 Collaborative Tag Graph
To ease the understanding of overall process of message aggrega-
tion, we build a unified graph that includes three types of nodes,
i.e., user, item and tags. Here is the definition:
Definition 3.2. Collaborative Tag Graph. A collaborative tag
graph is an undirected graph G=(V,E), whereVâˆˆUâˆªIâˆªT
denotes the node set that includes three types of nodes and Erepre-
sents the edges. It is worth noting that an assignment (ğ‘¢,ğ‘¡,ğ‘–)will be
split into three edges, that is (ğ‘¢,ğ‘–),(ğ‘¢,ğ‘¡)and(ğ‘–,ğ‘¡). For simplicity, we
define the relation set as Râˆˆ{ğ‘Ÿ0,ğ‘Ÿ1,ğ‘Ÿ2}, whereğ‘Ÿ0,ğ‘Ÿ1,ğ‘Ÿ2represents
the relation between (ğ‘¢,ğ‘–),(ğ‘¢,ğ‘¡)and(ğ‘–,ğ‘¡), respectively. Finally, the
collaborative tag graph can be formally described as:
G={(ğ‘£,ğ‘Ÿ,ğ‘£â€²)|ğ‘£,ğ‘£â€²âˆˆV,ğ‘ŸâˆˆR}, (1)
4 METHODOLOGY
In this section, we elaborate on the overall process of BoxGNN,
which consists of three steps: (a) We first introduce the formulation
 
1772KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fake Lin, et al.
of box embeddings, and transform all nodes into box representa-
tions. (b) Subsequently, we perform the message aggregation in the
context of box language to obtain the high-order representations. (c)
Eventually, we utilize the Gumbel-based volume of the intersection
between arbitrary two boxes to guide the whole learning process.
The framework of BoxGNN is depicted in Figure 3.
4.1 Box Initialization
In this part, we introduce the approach of representing nodes as
boxes in a multi-dimensional space and formulate the logical oper-
ations that apply to these boxes in subsequent sections.
4.1.1 Box Embeddings. Different from earlier models [ 17] that
only portrayed items as boxes, we extend this modeling to all nodes
within the CTG as boxes. This change provides us with a unified
language for logical operations across the CTG and aligns with our
objectives of capturing user interest uncertainty. Formally, a box is
defined as ğ’‘â‰¡(ğ¶ğ‘’ğ‘›(ğ’‘),ğ‘‚ğ‘“ğ‘“(ğ’‘))âˆˆR2ğ‘‘, then:
ğµğ‘œğ‘¥ğ’‘â‰¡{ğ’—âˆˆRğ‘‘:ğ¶ğ‘’ğ‘›(ğ’‘)âˆ’ğ‘‚ğ‘“ğ‘“(ğ’‘)âª¯ğ’—âª¯ğ¶ğ‘’ğ‘›(ğ’‘)+ğ‘‚ğ‘“ğ‘“(ğ’‘)}
(2)
whereâª¯denotes the element-wise inequality, ğ¶ğ‘’ğ‘›(ğ’‘)andğ‘‚ğ‘“ğ‘“(ğ’‘)
are the center and the offset of the box ğ’‘, respectively. For conve-
nience, we denote the box representation of a user, item, tag, and
general node as ğ’–,ğ’Š,ğ’•andğ’—, respectively.
4.1.2 Logical Operations. After initializing the box representations
for all nodes, it is impractical to apply traditional operators such as
addition and multiplication. Therefore, we need to define another
set of logical operations for boxes. Without loss of generality, we
have two logical operations: intersection and union. Here comes
their specific definition:
Intersection. Given a node set{ğ‘£1,ğ‘£2,...,ğ‘£ğ‘›}âˆˆV , the inter-
section operation can be represented as ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ=Ã‘ğ‘›
ğ‘˜=1ğ’—ğ‘˜, where ğ’—ğ‘˜
is the corresponding box representation. ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ denotes the inter-
sected box, which is defined as ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ=(ğ¶ğ‘’ğ‘›(ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ),ğ‘‚ğ‘“ğ‘“(ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ)).
Theğ¶ğ‘’ğ‘›(ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ)andğ‘‚ğ‘“ğ‘“(ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ)are derived as follows:
ğ¶ğ‘’ğ‘›(ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ)=ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–âŠ™ğ¶ğ‘’ğ‘›(ğ’—ğ‘–),ğ‘ğ‘–=ğ‘’ğ‘¥ğ‘(ğ‘€ğ¿ğ‘ƒ(ğ¶ğ‘’ğ‘›(ğ’—ğ‘–)))Ãğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘(ğ‘€ğ¿ğ‘ƒ(ğ¶ğ‘’ğ‘›(ğ’—ğ‘—))),
(3)
ğ‘‚ğ‘“ğ‘“(ğ’—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ)=ğ‘€ğ‘–ğ‘›({ğ‘‚ğ‘“ğ‘“(ğ’—1),...,ğ‘‚ğ‘“ğ‘“(ğ’—ğ‘›)}) (4)
whereâŠ™is the element-wise product, ğ‘€ğ¿ğ‘ƒ(Â·)is the Multi-Layer
Perceptron, ğ‘€ğ‘–ğ‘›(Â·)andğ‘’ğ‘¥ğ‘(Â·)are used in element-wise manner.
Union. Similar to intersection, given the node set {ğ‘£1,...,ğ‘£ğ‘›}âˆˆ
V, we can generate a new box representation ğ’—ğ‘¢ğ‘›ğ‘–=Ãğ‘›
ğ‘˜=1ğ’—ğ‘˜,
where ğ’—ğ‘¢ğ‘›ğ‘–â‰¡ (ğ¶ğ‘’ğ‘›(ğ’—ğ‘¢ğ‘›ğ‘–),ğ‘‚ğ‘“ğ‘“(ğ’—ğ‘¢ğ‘›ğ‘–)), by performing attentive
sum over the box centers and expanding the box offset:
ğ¶ğ‘’ğ‘›(ğ’—ğ‘¢ğ‘›ğ‘–)=ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–âŠ™ğ¶ğ‘’ğ‘›(ğ’—ğ‘–),ğ‘ğ‘–=ğ‘’ğ‘¥ğ‘(ğ‘€ğ¿ğ‘ƒ(ğ¶ğ‘’ğ‘›(ğ’—ğ‘–)))Ãğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘(ğ‘€ğ¿ğ‘ƒ(ğ¶ğ‘’ğ‘›(ğ’—ğ‘—))),
(5)
ğ‘‚ğ‘“ğ‘“(ğ’—ğ‘¢ğ‘›ğ‘–)=ğ‘€ğ‘ğ‘¥({ğ‘‚ğ‘“ğ‘“(ğ’—1),...,ğ‘‚ğ‘“ğ‘“(ğ’—ğ‘›)}) (6)
Upon acquiring these two operators, it becomes imperative to
delve into their roles within the tag-aware recommendation. Theintersection operator preserves the overlapping areas across multi-
ple boxes, which embodies the shared attributes and characteristics.
For instance, given the item set in the interacted history of a spe-
cific user, the intersection of them reveals their shared traits, which
indicates the implicit interests of the user. Conversely, the union
operation retains all the information from the boxes, allowing the
aggregated box to effectively incorporate broad information across
the boxes. For example, considering an item with abundant tags,
performing union operation on these tags yields a box with com-
prehensive semantic information from the tags.
4.2 Box-based Graph Neural Network
In this subsection, we apply the language of boxes to interpret
the message-passing mechanism to capture high-order signals. In
traditional GNN frameworks [ 13,33], we typically use mean or
weighted sum to aggregate neighboring information. However, in
the context of boxes, only intersection and union operators can
be utilized to aggregate neighboring information. This constraint
requires us to rethink the aggregation process in GNNs, as these
operations are fundamentally different from arithmetic aggrega-
tions and can capture the complex relationships and structures that
may exist among the nodes in CTG. Specifically, different nodes
may require distinct strategies when encountering various types
of neighbors. Therefore, we conduct an in-depth analysis of each
type of node and provide the corresponding aggregation formulas.
User-aware Aggregation. The key to the recommendation sys-
tem lies in modeling user interests. Following previous studies, we
aggregate the neighbors of the user to obtain higher-order repre-
sentations. However, there exist two types of neighbors, i.e., tags
and items, which may exert different influences on the user. Along
this line, we have divided the propagation process into two parts
for separate studies.
For the tag side, users are assigned a large number of tags, mani-
fested as edges between users and tags in CTG. These tags can be
considered as explicit interests of the users, with multiple interests
overlapping, thus forming a comprehensive profile of user interests.
In the context of box language, a user box representation can be
aggregated by the box representations of multiple tags through a
union operation. This process can be formulated as:
ğ’–(ğ‘™+1)
ğ‘¡=|Nğ‘¡
ğ‘¢|Ã˜
ğ‘˜=1ğ’•(ğ‘™)
ğ‘˜, (7)
where ğ’–(ğ‘™+1)
ğ‘¡is the(ğ‘™+1)-th user box representation for aggregating
tag neighbors andNğ‘¡ğ‘¢is the tag neighbor set of the user ğ‘¢.
Compared with tags, the neighbors of items may tell a differ-
ent story to the users. Notably, the neighbors of items reflect the
historical interactions of the user, which indicates that these pur-
chased items provide insight into user preferences. Therefore, it is
rational to extract user profiles according to the purchased items,
which can be interpreted as performing an intersection operation
on these item boxes. Along this line, the conjunctive box expresses
the shared traits within the items, which indicates the userâ€™s implicit
preference:
ğ’–(ğ‘™+1)
ğ‘–=|Nğ‘–
ğ‘¢|Ã™
ğ‘˜=1ğ’Š(ğ‘™)
ğ‘˜, (8)
 
1773When Box Meets Graph Neural Network in Tag-aware Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Intersection UnionAttention -based CentralizationUnion
â€¦IntersectionMin Offset Max OffsetMax Offset
IntersectionAttention -based Centralization
Min Offset Min OffsetMin Offset
Attention -based Centralization
Max Offset Max OffsetMax Offset
Union UnionUnion IntersectionTag-aware 
AggregationItem-aware 
AggregationUser-aware 
Aggregation
User 
Nodes
User 
Box
Offset & 
Centerâ€¦Tag
 Nodes
Tag 
Box
Offset & 
Centerâ€¦Item
 Nodes
Item 
Box
Offset & 
Centerâ€¦â€¦ â€¦
User BoxesGumbel
DistributionGumbel 
Distribution Volume -based SimilarityRecommendation
Tag Boxes Item BoxesBox 
InitializationBox-based 
Graph 
Neural 
NetworksVolume -
based 
Prediction
Figure 3: The overall framework of our proposed BoxGNN framework. Note the Intersection and Union are logical operations
including Min/Max Offset and Centralization, where Min/Max Offset is to obtain the minimum/maximum offset among boxes
in an element-wise manner.
where ğ’–(ğ‘™+1)
ğ‘–is the user box representation aggregated by item
neighbors, andNğ‘–ğ‘¢is the item neighbor set of the user ğ‘¢. To union
the two aspects, the final user box can be represented as:
ğ’–(ğ‘™+1)=ğ’–(ğ‘™+1)
ğ‘¡âˆªğ’–(ğ‘™+1)
ğ‘–,
ğ¶ğ‘’ğ‘›(ğ’–(ğ‘™+1))=|Nğ‘¢|âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜âŠ™ğ¶ğ‘’ğ‘›(ğ’—(ğ‘™)
ğ‘˜),
ğ‘ğ‘˜=ğ‘’ğ‘¥ğ‘(ğ‘€ğ¿ğ‘ƒ(ğ¶ğ‘’ğ‘›(ğ’—(ğ‘™)
ğ‘˜)))
Ã|Nğ‘¢|
ğ‘—=1ğ‘’ğ‘¥ğ‘(ğ‘€ğ¿ğ‘ƒ(ğ¶ğ‘’ğ‘›(ğ’—(ğ‘™)
ğ‘—))),
ğ‘‚ğ‘“ğ‘“(ğ’–(ğ‘™+1))=ğ‘€ğ‘ğ‘¥(ğ‘€ğ‘–ğ‘›({ğ‘‚ğ‘“ğ‘“(ğ’—(ğ‘™)
ğ‘–)}),ğ‘€ğ‘ğ‘¥({ğ‘‚ğ‘“ğ‘“(ğ’—(ğ‘™)
ğ‘¡)}))
(9)
whereNğ‘¢denotes all neighbors of the user ğ‘¢,ğ’—ğ‘˜denotes the tag
or item box representations. In the following formulation, we omit
the derivation of ğ¶ğ‘’ğ‘›(Â·), as they are similar to Equation 9.
Tag-aware Aggregation. Tags serve as additional information
to enrich the user and item embeddings. In our TRS scenario, tags
play a crucial role in connecting two users with similar interests or
items with shared characteristics. Therefore, as the neighbors forthe tag include both users and items, we need to investigate their
distinctive impacts on the tags separately.
For users, a tag can be assigned to multiple users, representing
their common interests or hobbies. However, due to the diversity
of user interests, a specific tag covers only a limited part of their
interests, making it hard to locate precisely. To address this, we
apply intersection operation to extract the shared information from
the intricate representations of the neighboring users, serving as
the high-order representations of the tags. This can be formalized
as follows:
ğ’•(ğ‘™+1)
ğ‘¢ =|Nğ‘¢
ğ‘¡|Ã™
ğ‘˜=1ğ’–(ğ‘™)
ğ‘˜, (10)
where ğ’•(ğ‘™+1)
ğ‘¢ is the(ğ‘™+1)-th tag box representation for aggregating
user neighbors and Nğ‘¢
ğ‘¡is the user neighbor set of the tag ğ‘¡.
Similarly, a tag can be also assigned to multiple items, implying
their attributes or features. In this case, the key to obtaining the high-
order representations of tags still lies in the distillation of the shared
features in the item neighbors. Likewise, we apply intersection to
aggregate the information from item neighbors:
ğ’•(ğ‘™+1)
ğ‘–=|Nğ‘–
ğ‘¡|Ã™
ğ‘˜=1ğ’Š(ğ‘™)
ğ‘˜, (11)
 
1774KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fake Lin, et al.
where ğ’•(ğ‘™+1)
ğ‘–is the tag box representation aggregated by item neigh-
bors, andNğ‘–
ğ‘¡is the item neighbor set of tag ğ‘¡. Finally, by combining
these two parts, we can obtain the final high-order box representa-
tion of the tag as follows:
ğ’•(ğ‘™+1)=ğ’•(ğ‘™+1)
ğ‘¢âˆ©ğ’•(ğ‘™+1)
ğ‘–,
ğ‘‚ğ‘“ğ‘“(ğ’•(ğ‘™+1))=ğ‘€ğ‘–ğ‘›({ğ‘‚ğ‘“ğ‘“(ğ’—(ğ‘™)
1),...,ğ‘‚ğ‘“ğ‘“(ğ’—(ğ‘™)
|Nğ‘¡|)}),(12)
whereNğ‘¡is the neighbors of the tag ğ‘¡,ğ’—ğ‘˜denotes the user or item
box representations. Notably, we use intersection to integrate the
ğ’•(ğ‘™+1)
ğ‘¢ andğ’•(ğ‘™+1)
ğ‘–for extracting the shared traits between users and
items, which can be further recognized as ğ’•(ğ‘™+1).
Item-aware Aggregation. In CTG, there are also two types
of neighbors for items, i.e., tags and users. Tags can also serve as
characteristics of items, with which we can form a specific item.
Consistent with user aggregation, we utilize the union operation
to aggregate the neighboring tag nodes:
ğ’Š(ğ‘™+1)
ğ‘¡=|Nğ‘¡
ğ‘–|Ã˜
ğ‘˜=1ğ’•(ğ‘™)
ğ‘˜, (13)
whereğ‘–(ğ‘™+1)
ğ‘¡is the item box representation from tag neighbors and
Nğ‘¡
ğ‘–is the tag neighbor set of item ğ‘–.
When it comes to users, we continue to employ union operation
for two reasons: (1) To capture the multi-facet features of the items.
Different individuals may interact with the same item driven by
various intents, and applying the union operation to user boxes
effectively represents this diversity. (2) To express the popularity
of items. Specifically, the more users who have purchased the item,
the larger the volume of the item box. Along this line, for any
newly-emerged user, its box is more likely to be encompassed by
the box of potential popular items, indicating a higher probability
of purchasing that item. Thus, we aggregate the neighboring user
nodes as follows:
ğ’Š(ğ‘™+1)
ğ‘¢ =|Nğ‘¢
ğ‘–|Ã˜
ğ‘˜=1ğ’–(ğ‘™)
ğ‘˜, (14)
whereğ‘–(ğ‘™+1)
ğ‘¢ is the item box representation from user neighbors and
Nğ‘¢
ğ‘–is the user neighbor set of item ğ‘–. In summary, the aggregation
formula for the item is as follows:
ğ’Š(ğ‘™+1)=ğ’Š(ğ‘™+1)
ğ‘¡âˆªğ’Š(ğ‘™+1)
ğ‘¢,
ğ‘‚ğ‘“ğ‘“(ğ’Š(ğ‘™+1))=ğ‘€ğ‘ğ‘¥({ğ‘‚ğ‘“ğ‘“(ğ’—(ğ‘™)
1),...,ğ‘‚ğ‘“ğ‘“(ğ’—(ğ‘™)
|Nğ‘–|)}).(15)
whereNğ‘–is the neighbors of the item ğ‘–. After stacking ğ¿layers,
we will obtain the ultimate box representations as ğ’–(ğ¿)andğ’Š(ğ¿),
which are fed into next module to get the predictive results.
4.3 Gumbel-based Volume Objective
After obtaining the higher-order box representations, we emphasize
that the key point turns to the similarity calculation between the
candidate users and items. As mentioned before, it is intractable
to still apply dot products to measure their similarity due to the
complex structure within the box representations. Instead, we needto consider more sophisticated metrics that can capture the geo-
metric relationships among these high-dimensional boxes, which
align with the box properties.
To this end, we utilize the volume of the intersection between
user and item boxes as their similarity. However, we may face a gra-
dient vanishing issue if there is no intersection between two boxes.
Inspired by [ 10], we attempt to regard the acquired high-order box
representations ğ’—(ğ¿)as gumbel box, where minimum corner ğ’›ğ’—(ğ‘³)
and maximum corner ğ’ğ’—(ğ¿)follows Gumbel distribution:
ğ‘“(ğ‘¥;ğœ‡,ğ›½)=1
ğ›½ğ‘’ğ‘¥ğ‘(âˆ’ğ‘¥âˆ’ğœ‡
ğ›½âˆ’ğ‘’âˆ’ğ‘¥âˆ’ğœ‡
ğ›½), (16)
whereğ›½controls the scale of the distribution, and ğœ‡governs the
mean of the distribution. Note it is min/max stable, i.e., the min/max
of two such variables still follows Gumbel distribution [ 2]. For sim-
ple notation, we omit the ğ¿superscript in this subsection. Formally,
we define the ğ’›ğ‘£andğ’ğ‘£as:
ğ’›ğ‘£âˆ¼ğºğ‘¢ğ‘šğ‘ğ‘’ğ‘™(ğğ‘§
ğ‘£,ğ›½),ğğ‘§
ğ‘£=ğ¶ğ‘’ğ‘›(ğ’—)âˆ’ğ‘‚ğ‘“ğ‘“(ğ’—),
ğ’ğ‘£âˆ¼ğºğ‘¢ğ‘šğ‘ğ‘’ğ‘™(ğğ‘
ğ‘£,ğ›½),ğğ‘
ğ‘£=ğ¶ğ‘’ğ‘›(ğ’—)+ğ‘‚ğ‘“ğ‘“(ğ’—).(17)
According to min/max stability [ 2], we then can derive the min-
imum and maximum corners of the intersected box between the
user and item box:
ğ’›ğ‘¢ğ‘–=ğ‘€ğ‘ğ‘¥(ğ’›ğ‘¢,ğ’›ğ‘–)âˆ¼ğºğ‘¢ğ‘šğ‘ğ‘’ğ‘™(ğğ‘§
ğ‘¢ğ‘–,ğ›½),
ğ’ğ‘¢ğ‘–=ğ‘€ğ‘–ğ‘›(ğ’ğ‘¢,ğ’ğ‘–)âˆ¼ğºğ‘¢ğ‘šğ‘ğ‘’ğ‘™(ğğ‘
ğ‘¢ğ‘–,ğ›½),
ğğ‘§
ğ‘¢ğ‘–=ğ›½ğ‘™ğ‘›(ğ‘’ğğ‘§
ğ‘¢/ğ›½+ğ‘’ğğ‘§
ğ‘–/ğ›½),
ğğ‘
ğ‘¢ğ‘–=âˆ’ğ›½ğ‘™ğ‘›(ğ‘’âˆ’ğğ‘
ğ‘¢/ğ›½+ğ‘’âˆ’ğğ‘
ğ‘–/ğ›½),(18)
where ğ’–andğ’Šare the user and item box representations after stack-
ingğ¿layers, respectively. Therefore, ğğ‘§ğ‘¢andğğ‘ğ‘¢,ğğ‘§
ğ‘–andğğ‘
ğ‘–are
separately the two corner embeddings of user and item box. Here,
the derivation of ğğ‘§
ğ‘¢ğ‘–andğğ‘
ğ‘¢ğ‘–can be found in [ 10]. Next, in order to
get the expected volume, we derived the formula for calculating the
expected length for each dimension. Thus we arrive at the expected
volume formulation as follows:
E[ğ‘šğ‘ğ‘¥(ğ’ğ‘¢ğ‘–âˆ’ğ’›ğ‘¢ğ‘–,0)]=ğ‘‘Ã–
ğ‘˜2ğ›½ğ¾0
2ğ‘’âˆ’(ğœ‡ğ‘
ğ‘¢ğ‘–,ğ‘˜âˆ’ğœ‡ğ‘§
ğ‘¢ğ‘–,ğ‘˜)/2ğ›½
,(19)
whereğ‘‘is the dimension of the embedding, ğœ‡ğ‘§
ğ‘¢ğ‘–,ğ‘˜is theğ‘˜-th element
inğğ‘§
ğ‘¢ğ‘–, andğœ‡ğ‘
ğ‘¢ğ‘–,ğ‘˜follows the same principal. ğ¾0is the modified
Bessel function of second kind, order 0. The proof of this statement
is given in [10].
To further analyze the Equation 19, let ğ‘š(ğ‘¥)=2ğ›½ğ¾0
2ğ‘’ğ‘¥/2ğ›½
,
then theğ‘š(ğ‘¥)is essentially exponential as ğ‘¥increases, and the
volume function approaches a hinge function as ğ›½â†’0, which
leads to numerical stability concerns. Here, we use softplus-like
function to replace the ğ‘š(ğ‘¥)
ğ‘š(ğ‘¥)=ğ›½ğ‘™ğ‘œğ‘”
1+ğ‘’ğ‘¥/ğ›½âˆ’2ğ›¾
, (20)
whereğ›¾is Euler-Mascheroni constant. Equipped with this, we have
our final version of volume formulation:
ğ‘‰ğ‘œğ‘™(ğµğ‘œğ‘¥ğ‘¢ğ‘–)=ğ‘‘Ã–
ğ‘˜ğ›½ğ‘™ğ‘œğ‘”
1+ğ‘’âˆ’(ğœ‡ğ‘
ğ‘¢ğ‘–,ğ‘˜âˆ’ğœ‡ğ‘§
ğ‘¢ğ‘–,ğ‘˜)/ğ›½âˆ’2ğ›¾
. (21)
 
1775When Box Meets Graph Neural Network in Tag-aware Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Statistics of three real-world datasets.
Dataset #Users #Items #Tags #Assignment
MovieLens 1,651 5,381 1,586 36,728
LastFm 1,808 12,212 2,305 175,641
E-shop 7,277 26,726 4,146 237,059
4.4 Model Training
Through the above modules, the preference score for a user ğ‘¢toward
an itemğ‘–is defined as the volume of the intersection of two box
representations:
Ë†ğ‘¦ğ‘¢ğ‘–=ğ‘™ğ‘œğ‘”(ğ‘‰ğ‘œğ‘™(ğµğ‘œğ‘¥ğ‘¢ğ‘–)). (22)
Here, we use a log function to prevent gradient vanishing. Then
we employ Bayesian Personalized Ranking (BPR) loss to train the
whole model, which assumes that observed interactions should
receive higher scores than unobserved ones:
L=âˆ‘ï¸
(ğ‘¢,ğ‘–+,ğ‘–âˆ’)âˆˆğ¸âˆ’logğœ(Ë†ğ‘¦ğ‘¢ğ‘–+âˆ’Ë†ğ‘¦ğ‘¢ğ‘–âˆ’)+ğœ†âˆ¥Î˜âˆ¥2, (23)
whereğ¸is the set of training interactions, ğ‘–+andğ‘–âˆ’are separately
positive and negative samples from training data, ğœ†âˆ¥Î˜âˆ¥2is the
regularization term to prevent overfitting.
4.5 Model Analysis
We conducted a comparative analysis with existing models to
demonstrate the rationale of BoxGNN.
4.5.1 Relation with GAT. GAT [ 28] is a well-known scheme that
utilizes an attention mechanism to gauge the significance of neigh-
boring signals to recognize more vital information. Formally, The
graph aggregation formulation in GAT can be defined as:
ğ’‰(ğ‘™)
ğ‘–=ğœÂ©Â­
Â«âˆ‘ï¸
ğ‘—âˆˆNğ‘–ğ›¼ğ‘–ğ‘—ğ‘¾ğ’‰(ğ‘™âˆ’1)
ğ‘—ÂªÂ®
Â¬,ğ›¼ğ‘–ğ‘—=ğ‘’ğ‘¥ğ‘(ğ’‰(ğ‘™âˆ’1)
ğ‘—)
Ã
ğ‘˜âˆˆNğ‘–ğ‘’ğ‘¥ğ‘(ğ’‰(ğ‘™âˆ’1)
ğ‘˜),(24)
where ğ‘¾is the learnable parameter and ğœis the activation function.
Compared with Equation 4, if we set the offset of our boxes to zero,
then our aggregation operation would closely mirrors GAT.
5 EXPERIMENTS
5.1 Experimental Settings
5.1.1 Data Description. To demonstrate the superiority of our pro-
posed BoxGNN, we conduct experiments on two publicly avail-
able benchmarking datasets and one LLM-enhanced e-commercial
dataset, i.e. MovieLens, LastFm, and E-shop. Notably, Movielens
and LastFm are released in HetRec 20112and E-shop is collected
from a real-world e-commercial platform and is publicly available
in our paper.
â€¢MovieLens: This is a movie recommendation collection
released by the GroupLens research group3. Within this
dataset, each user is associated with a list of tag assignments
corresponding to the movies they have interacted with.
2https://grouplens.org/datasets/hetrec-2011.
3http://www.grouplens.org.â€¢LastFm: This is an artist recommendation dataset obtained
from an online music system Last.FM4. In this dataset, each
user has a list of tags assigned to artists.
â€¢E-shop: This is a real-world commercial dataset collected
from an online scenario. By feeding the titles of purchased
items into the LLMs, such GPT-4 turbo, we ask to identify
the underlying interests that led to the purchase behaviour.
Then LLMs generate a list of tags which will be assigned to
the users and items.
Following existing literature [ 4], to ensure the data quality, we
filter out the infrequent tags that are used less than 5 times in three
datasets. The statistics of datasets are summarized in Table 1. We
randomly select 80%, 10%,10% of the assignments as training set,
validation set, and test set, respectively. It is worth noting that we
construct the CTG by the users, items, and tags in the training data.
The reproductive settings are included in Appendix A.2.
5.1.2 Evaluation Metrics. We utilize two representative evaluation
metrics for top-K recommendation, i.e. ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ @ğ¾andğ‘ğ·ğ¶ğº @ğ¾
(Normalized Discounted Cumulative Gain) across all experiments,
whereğ¾=10,20. We adopt an all-ranking strategy in the validation
and test sets. We evaluate on the validation set every 5 epochs,
and we perform early stopping if there is no improvement for 10
consecutive evaluations. The performance of the best model on the
test set will be reported as final results.
5.1.3 Compared baselines. To demonstrate the effectiveness of
proposed BoxGNN, we gather recommendation techniques from
various domains as baselines, involving classic method BPR [ 24],
feature-based (NFM [ 12],IFM [ 36]), GNN-based (LightGCN [ 13],
NGCF [ 30]), KG-based (KGIN [ 31], KGRec [ 35]), and tag-based
(DSPR [ 34], TGCN [ 4], LFGCF [ 37]) models. The details of above
baselines are illustrated in Appendix A.1.
5.2 Experimental Results
5.2.1 Overall Performance. In this part, we conduct experiments
on three datasets to show the superiority of our proposed BoxGNN
compared with various baselines. From the results listed in Table 2,
we have following observations:
â€¢Our proposed BoxGNN outperforms all baselines across all
datasets. This performance boost can be attributed primar-
ily to two factors: On one hand, by modeling nodes as box
embeddings, we can capture user uncertainty and diversity,
which in turn enhances the performance of the recommenda-
tion system. On the other hand, by simulating a multi-layer
message propagation mechanism, node embeddings are en-
riched as they aggregate more high-order information.
â€¢In most cases, GNN-based methods surpass feature-based
methods, as the latter do not take advantage of their strengths
when only tags are used as the features. Besides, GNN-based
approaches can effectively reap benefits from the graph struc-
ture and collaborative signals, resulting in more expressive
representations.
â€¢KG-based methods perform better on the Movielens and
LastFm datasets compared to GNN-based methods, because
4http://www.last.fm.com
 
1776KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fake Lin, et al.
Table 2: The experimental comparison among a wide range of recommendation approaches for three datasets. R@K and N@K
stand for Recall@K and NDCG@K, respectively. BoxGNN w/o tags represents the results of our BoxGNN running on a dataset
from which the tag information has been removed. The best results are in bold and the secondary best results are underlined. *
indicates the statistical significance over the best baseline using t-test with ğ‘<0.05.
ModelMovieLens LastFm E-shop
R@10 R@20 N@10 N@20 R@10 R@20 N@10 N@20 R@10 R@20 N@10 N@20
BPR 0.0453 0.0661 0.0320 0.0380 0.0673 0.0978 0.0563 0.0643 0.3409 0.4491 0.2632 0.3006
NFM 0.0351 0.0608 0.0225 0.0306 0.0762 0.1173 0.0656 0.0768 0.0476 0.1230 0.0285 0.0536
IFM 0.0338 0.0474 0.0191 0.0232 0.0562 0.0898 0.0412 0.0507 0.1413 0.2279 0.0961 0.1255
LightGCN 0.0446 0.0724 0.0313 0.0393 0.1020 0.1555 0.0875 0.1002 0.2749 0.3710 0.2118 0.2446
NGCF 0.0352 0.0592 0.0273 0.0331 0.1149 0.1580 0.0943 0.1041 0.3748 0.4976 0.2852 0.3283
KGIN 0.0633 0.0978 0.0516 0.0607 0.1113 0.1662 0.0924 0.1068 0.2934 0.3900 0.2300 0.2619
KGRec 0.0468 0.0819 0.0380 0.0476 0.1166 0.1665 0.0996 0.1116 0.2681 0.3515 0.2118 0.2394
DSPR 0.0661 0.0929 0.0476 0.0549 0.0505 0.0885 0.0438 0.0541 0.2336 0.3297 0.1686 0.2021
TGCN 0.0527 0.0838 0.0427 0.0510 0.0917 0.1398 0.0747 0.0876 0.2621 0.3592 0.1912 0.2255
LFGCF 0.0812 0.1167 0.0601 0.0705 0.1241 0.1848 0.1058 0.1198 0.3586 0.4803 0.2598 0.3018
BoxGNN w/o tags 0.0533 0.0794 0.0362 0.0445 0.1271 0.1814 0.1023 0.1156 0.4260 0.5432 0.3298 0.3718
BoxGNN 0.0866 0.1226 0.0704* 0.0812* 0.1350* 0.1934* 0.1124 0.1264 0.4505* 0.5744* 0.3447* 0.3881*
KG-based approaches incorporate tag information, which
can assist the model in better capturing item characteristics
and user interests.
â€¢BPR and NGCF demonstrate remarkable performance on the
E-shop dataset compared to other baselines, which actually
suggests that the user-item collaborative signals are adequate
for user modeling. Surprisingly, the additional introduction
of tag information has weakened the performance of many
models, indicating that there exists noise in the tag set within
the E-shop dataset. This is understandable, given that these
tags are produced by LLMs.
â€¢The performance of BoxGNN significantly surpasses that
of BoxGNN w/o tags across three datasets, highlighting the
critical importance of tag information in user-item matching
tasks. Moreover, even after the removal of tags, BoxGNN
retains its superior performance over both LightGCN and
NGCF, which demonstrates the capability of our approach
in uncovering user interests.
5.2.2 Ablation Study. In this part, we conduct the ablation study
to demonstrate the effect of each module on the overall model and
deliver some insight into how they affect the results.
Effect of GNNs. The GNN module is the cornerstone of our
approach, which captures rich semantic signals from high-order
neighbors. To show the effectiveness of this module, we create a
variant by setting ğ‘™=0, which means the removal of the GNN mod-
ule, denoted as w/o GNN. From Table 3, it is evident that there is a
dramatic decrease in performance after removing the GNN module.
This is because the box representations fail to absorb knowledge
from high-order neighbors, resulting in poorer representation per-
formance. This reaffirms the importance of high-order signals and
the effectiveness of the GNN module in capturing these signals.
Effect of Gumbel-based Volume Objective. This objective
is the key component to train the whole process, which has the
ability to consistently deliver gradient signals. To demonstrate its
superiority, we obtain a variant that adopts the direct calculationTable 3: Ablation studies for investigating the effects of each
module.
MovieLens LastFm E-shop
Recall NDCG Recall NDCG Recall NDCG
w/o GNN 0.0343 0.0237 0.0965 0.0840 0.3843 0.2961
w/o Gumbel 0.0652 0.0544 0.1083 0.0905 0.4040 0.3166
Ours 0.0866 0.0704 0.1350 0.1124 0.4490 0.3455
Table 4: Impact of the number of layers.
MovieLens LastFm E-shop
Recall NDCG Recall NDCG Recall NDCG
BoxGNN-1 0.0474 0.0355 0.1175 0.0964 0.4203 0.3251
BoxGNN-2 0.0681 0.0563 0.1254 0.1050 0.4490 0.3455
BoxGNN-3 0.0866 0.0704 0.1350 0.1124 0.4343 0.3301
of the volume for the intersected box, namely, w/o Gumbel. As
shown in Table 3, After removing the Gumbel-based module, our
performance suffered a significant decline. The reason lies in the
emergence of the gradient vanishing problem when there is no
overlap between two boxes.
5.2.3 Impact of Model Depth. Furthermore, we turn to the influ-
ence of the layer number on the overall performance. The layer
number is selected in the range of {1, 2, 3}, which is consistent with
most GNN-based methods. As depicted in Table 4, we draw the
following observations: (1) The performance of BoxGNN-2 always
exceeds that of BoxGNN-1, indicating that modeling high-order
connectivity has a significant positive impact on the overall model
performance. (2) The majority of datasets achieve the highest perfor-
mance in BoxGNN-3, e.g., MovieLens and LastFm, which means that
representations of absorbed third-order signals are more expres-
sive than second-order representations. (3) In the E-shop dataset,
BoxGNN-3 performs worse than BoxGNN-2, which is contrary to
 
1777When Box Meets Graph Neural Network in Tag-aware Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 4: Effect of the parameter ğ›½on all datasets.
0.2 0.4 0.6 0.8 1.00.060.070.08Recall@10
MovieLens
0.0400.0450.0500.0550.0600.0650.070
 Recall@10
NDCG@10
0.2 0.4 0.6 0.8 1.00.100.110.120.13
LastFm
0.0900.0950.1000.1050.110
Recall@10
NDCG@10
0.2 0.4 0.6 0.8 1.00.10.20.30.4
E-shop
0.100.150.200.250.300.35
NDCG@10
Recall@10
NDCG@10
the results observed with the previous two datasets. The primary
reason for this discrepancy is the presence of noise in the tags within
this dataset. Excessive connectivity introduced by BoxGNN-3 can
amplify this noise, leading to a decline in performance.
5.2.4 Parameters Sensitivity. Moreover, we investigate the effect
of theğ›½on the entire system. Note that ğ›½controls the scale of
the Gumbel distribution. Similar to the temperature coefficient in
the contrastive loss, when ğ›½is larger, the Gumbel distribution is
closer to a uniform distribution. Conversely, a smaller ğ›½causes its
probability density function to approach a hinge function, meaning
the random variable will degenerate into a constant. In this paper,
we need to strike a balance between the distinctiveness among the
boxes and the uncertainty within each box.
From Figure 4, we have following findings: (1) All the best values
ofğ›½are around 0.2. This choice ensures uncertainty within each
box while maintaining distinctiveness among the boxes, consistent
with the principle mentioned earlier. (2) With the increase in ğ›½, the
performance of our BoxGNN rapidly declines. This is reasonable
because asğ›½increases, the Gumbel distribution tends towards a
uniform distribution. This causes the differences among boxes to
gradually disappear, making it impossible to accurately model the
users. (3) As ğ›½decreases, the performance of our model will also
be negatively affected to some extent. This is inevitable because as
ğ›½decreases, the uncertainty of the Gumbel distribution gradually
disappears, leading to the reemergence of the vanishing gradient
problem and a consequent drop in model accuracy.
5.2.5 Visualization. We visualized the distribution of the centroid
points of boxes on the MovieLens dataset. For comparison, we also
visualized the distribution of node embeddings from LFGCF. From
Figure 5(a), we can see that before performing GNN, nodes of the
same type in the LFGCF model are close to each other. This indi-
cates that they have a high similarity among nodes of the same
type, lacking differentiation and diversity. Furthermore, from Fig-
ure 5(b), it is apparent that after performing GNN, the clustering
phenomenon becomes more significant. As for our BoxGNN frame-
work, from Figure 5(c), we can observe that before performing the
GNN operations, the distribution of points in BoxGNN is quite uni-
form. In comparison, the results in Figure 5(d) demonstrate that our
method achieves sufficient diversity and differentiation after the
GNN operation. This improvement mainly stems from box model-
ing, aligning with our goal of using boxes to represent the diversity
of user interests. Also, Figure 5(d) shows several blue clusters ofFigure 5: The visualization of centroid points of the boxes
and node embeddings of LFGCF on the MovieLens dataset.
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0(a) lfgcf_before_2d
user
item
tag
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0(b) lfgcf_after_2d
user
item
tag
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0(c) BoxGNN_before_2d
user
item
tag
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0(d) BoxGNN_after_2d
user
item
tag
points (items), which is expected since the purpose of GNN is to
explicitly aggregate nodes with similar representations. Moreover,
the distribution of these clusters remains fairly uniform, rather than
coalescing into a few larger clusters as seen in Figure 5(b).
6 CONCLUSION
In this paper, we proposed a novel box-based graph neural network
for tag-aware recommendation which models the uncertainty and
diversity of user interests and thus boosts the overall performance.
Specifically, we first formulated the box embeddings in the vector
space and initialized all nodes as box embeddings. Next, with two
logical operations, we performed the type-aware message aggrega-
tion to obtain the high-order box representations. Then, to avoid
the gradient vanishing issue, we devised the Gumbel-based volume
objective to refine the representations of boxes. Finally, extensive
experiments on three real-world datasets have demonstrated the
superiority of the BoxGNN compared with competitive baselines.
ACKNOWLEDGMENTS
This work was supported in part by the grants from National Natu-
ral Science Foundation of China (No.62222213, 62202443, 62207031,
62072423).
 
1778KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fake Lin, et al.
REFERENCES
[1]Ralph Abboud, Ä°smail Ä°lkan Ceylan, Thomas Lukasiewicz, and Tommaso Sal-
vatori. 2020. BoxE: A Box Embedding Model for Knowledge Base Completion.
InAdvances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Bal-
can, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/
6dbbe6abe5f14af882ff977fc3f35501-Abstract.html
[2]Michael Boratko, Javier Burroni, Shib Sankar Dasgupta, and Andrew McCallum.
2021. Min/max stability and box distributions. In Proceedings of the Thirty-
Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event,
27-30 July 2021 (Proceedings of Machine Learning Research, Vol. 161), Cassio P.
de Campos, Marloes H. Maathuis, and Erik Quaeghebeur (Eds.). AUAI Press,
2146â€“2155. https://proceedings.mlr.press/v161/boratko21a.html
[3]Bo Chen, Yue Ding, Xin Xin, Yunzhe Li, Yule Wang, and Dong Wang. 2021. AIRec:
Attentive intersection model for tag-aware recommendation. Neurocomputing
421 (2021), 105â€“114. https://doi.org/10.1016/J.NEUCOM.2020.08.018
[4]Bo Chen, Wei Guo, Ruiming Tang, Xin Xin, Yue Ding, Xiuqiang He, and Dong
Wang. 2020. TGCN: Tag Graph Convolutional Network for Tag-Aware Rec-
ommendation. In CIKM â€™20: The 29th ACM International Conference on Infor-
mation and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020,
Mathieu dâ€™Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe
CudrÃ©-Mauroux (Eds.). ACM, 155â€“164. https://doi.org/10.1145/3340531.3411927
[5]Hao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang,
Feiran Huang, Senzhang Wang, and Xiao Huang. 2024. Macro graph neural
networks for online billion-scale recommender systems. In Proceedings of the
ACM on Web Conference 2024. 3598â€“3608.
[6]Hao Chen, Yue Xu, Feiran Huang, Zengde Deng, Wenbing Huang, Senzhang
Wang, Peng He, and Zhoujun Li. 2020. Label-Aware Graph Convolutional Net-
works. In Proceedings of the 29th ACM International Conference on Information &
Knowledge Management. 1977â€“1980.
[7]Liyi Chen, Chuan Qin, Ying Sun, Xin Song, Tong Xu, Hengshu Zhu, and Hui
Xiong. 2024. Collaboration-Aware Hybrid Learning for Knowledge Development
Prediction. In Proceedings of the ACM on Web Conference 2024, WWW 2024,
Singapore, May 13-17, 2024, Tat-Seng Chua, Chong-Wah Ngo, Ravi Kumar, Hady W.
Lauw, and Roy Ka-Wei Lee (Eds.). ACM, 3976â€“3985. https://doi.org/10.1145/
3589334.3645326
[8]Tong Chen, Hongzhi Yin, Jing Long, Quoc Viet Hung Nguyen, Yang Wang, and
Meng Wang. 2022. Thinking inside The Box: Learning Hypercube Representations
for Group Recommendation. In SIGIR â€™22: The 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval, Madrid, Spain,
July 11 - 15, 2022, Enrique AmigÃ³, Pablo Castells, Julio Gonzalo, Ben Carterette,
J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 1664â€“1673. https://doi.org/
10.1145/3477495.3532066
[9]Shib Sankar Dasgupta, Michael Boratko, Siddhartha Mishra, Shriya Atmakuri,
Dhruvesh Patel, Xiang Li, and Andrew McCallum. 2022. Word2Box: Capturing
Set-Theoretic Semantics of Words using Box Embeddings. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan,
Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational
Linguistics, 2263â€“2276. https://doi.org/10.18653/V1/2022.ACL-LONG.161
[10] Shib Sankar Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Li,
and Andrew McCallum. 2020. Improving Local Identifiability in Probabilistic Box
Embeddings. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual, Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/
2020/hash/01c9d2c5b3ff5cbba349ec39a570b5e3-Abstract.html
[11] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna
Resort, Sardinia, Italy, May 13-15, 2010 (JMLR Proceedings, Vol. 9), Yee Whye Teh
and D. Mike Titterington (Eds.). JMLR.org, 249â€“256. http://proceedings.mlr.
press/v9/glorot10a.html
[12] Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for
Sparse Predictive Analytics. In Proceedings of the 40th International ACM SIGIR
Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo,
Japan, August 7-11, 2017, Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li,
Arjen P. de Vries, and Ryen W. White (Eds.). ACM, 355â€“364. https://doi.org/10.
1145/3077136.3080777
[13] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network
for Recommendation. In Proceedings of the 43rd International ACM SIGIR con-
ference on research and development in Information Retrieval, SIGIR 2020, Virtual
Event, China, July 25-30, 2020, Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap
Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 639â€“648.
https://doi.org/10.1145/3397271.3401063[14] Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen.
2023. Aligning Distillation For Cold-Start Item Recommendation. In Proceedings
of the 46th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 1147â€“1157.
[15] Ruoran Huang, Chuanqi Han, and Li Cui. 2021. Tag-aware Attentional Graph
Neural Networks for Personalized Tag Recommendation. In International Joint
Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22, 2021.
IEEE, 1â€“8. https://doi.org/10.1109/IJCNN52387.2021.9533380
[16] Ruoran Huang, Nian Wang, Chuanqi Han, Fang Yu, and Li Cui. 2020. TNAM: A
tag-aware neural attention model for Top-N recommendation. Neurocomputing
385 (2020), 1â€“12. https://doi.org/10.1016/J.NEUCOM.2019.11.095
[17] Tingting Liang, Yuanqing Zhang, Qianhui Di, Congying Xia, Youhuizi Li, and
Yuyu Yin. 2023. Contrastive Box Embedding for Collaborative Reasoning. In
Proceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023,
Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane
Mothe, and Barbara Poblete (Eds.). ACM, 38â€“47. https://doi.org/10.1145/3539618.
3591654
[18] Babak Maleki-Shoja and Nasseh Tabrizi. 2019. Tags-Aware Recommender Sys-
tems: A Systematic Review. In 2019 IEEE International Conference on Big Data,
Cloud Computing, Data Science & Engineering, BCD 2019, Honolulu, HI, USA, May
29-31, 2019 , Motoi Iwashita, Atsushi Shimoda, and Prajak Chertchom (Eds.). IEEE,
11â€“18. https://doi.org/10.1109/BCD.2019.8884850
[19] Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao,
and Xiuqiang He. 2021. SimpleX: A Simple and Strong Baseline for Collaborative
Filtering. In CIKM â€™21: The 30th ACM International Conference on Information and
Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021,
Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang
Tong (Eds.). ACM, 1243â€“1252. https://doi.org/10.1145/3459637.3482297
[20] Leandro Balby Marinho and Lars Schmidt-Thieme. 2007. Collaborative Tag
Recommendations. In Data Analysis, Machine Learning and Applications - Pro-
ceedings of the 31st Annual Conference of the Gesellschaft fÃ¼r Klassifikation
e.V., Albert-Ludwigs-UniversitÃ¤t Freiburg, March 7-9, 2007 (Studies in Classifi-
cation, Data Analysis, and Knowledge Organization), Christine Preisach, Hans
Burkhardt, Lars Schmidt-Thieme, and Reinhold Decker (Eds.). Springer, 533â€“540.
https://doi.org/10.1007/978-3-540-78246-9_63
[21] Yasumasa Onoe, Michael Boratko, Andrew McCallum, and Greg Durrett. 2021.
Modeling Fine-Grained Entity Types with Box Embeddings. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,
(Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia,
Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics,
2051â€“2064. https://doi.org/10.18653/V1/2021.ACL-LONG.160
[22] Dhruvesh Patel, Shib Sankar Dasgupta, Michael Boratko, Xiang Li, Luke Vilnis,
and Andrew McCallum. 2020. Representing Joint Hierarchies with Box Embed-
dings. In Conference on Automated Knowledge Base Construction, AKBC 2020,
Virtual, June 22-24, 2020, Dipanjan Das, Hannaneh Hajishirzi, Andrew McCallum,
and Sameer Singh (Eds.). https://doi.org/10.24432/C5KS37
[23] Hongyu Ren, Weihua Hu, and Jure Leskovec. 2020. Query2box: Reasoning over
Knowledge Graphs in Vector Space Using Box Embeddings. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net. https://openreview.net/forum?id=BJgr4kSFDS
[24] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI 2009,
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,
Montreal, QC, Canada, June 18-21, 2009, Jeff A. Bilmes and Andrew Y. Ng (Eds.).
AUAI Press, 452â€“461. https://www.auai.org/uai2009/papers/UAI2009_0139_
48141db02b9f0b02bc7158819ebfa2c7.pdf
[25] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2012. BPR: Bayesian Personalized Ranking from Implicit Feedback. CoRR
abs/1205.2618 (2012). arXiv:1205.2618 http://arxiv.org/abs/1205.2618
[26] Andriy Shepitsen, Jonathan Gemmell, Bamshad Mobasher, and Robin D. Burke.
2008. Personalized recommendation in social tagging systems using hierarchical
clustering. In Proceedings of the 2008 ACM Conference on Recommender Systems,
RecSys 2008, Lausanne, Switzerland, October 23-25, 2008, Pearl Pu, Derek G. Bridge,
Bamshad Mobasher, and Francesco Ricci (Eds.). ACM, 259â€“266. https://doi.org/
10.1145/1454008.1454048
[27] Karen H. L. Tso-Sutter, Leandro Balby Marinho, and Lars Schmidt-Thieme. 2008.
Tag-aware recommender systems by fusion of collaborative filtering algorithms.
InProceedings of the 2008 ACM Symposium on Applied Computing (SAC), Fortaleza,
Ceara, Brazil, March 16-20, 2008, Roger L. Wainwright and Hisham Haddad (Eds.).
ACM, 1995â€“1999. https://doi.org/10.1145/1363686.1364171
[28] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.
net/forum?id=rJXMpikCZ
 
1779When Box Meets Graph Neural Network in Tag-aware Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[29] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2016. Order-
Embeddings of Images and Language. In 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.
06361
[30] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural Graph Collaborative Filtering. In Proceedings of the 42nd International
ACM SIGIR Conference on Research and Development in Information Retrieval,
SIGIR 2019, Paris, France, July 21-25, 2019, Benjamin Piwowarski, Max Chevalier,
Ã‰ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and Falk Scholer (Eds.). ACM, 165â€“174.
https://doi.org/10.1145/3331184.3331267
[31] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,
Xiangnan He, and Tat-Seng Chua. 2021. Learning Intents behind Interactions
with Knowledge Graph for Recommendation. In WWW â€™21: The Web Conference
2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, Jure Leskovec, Marko
Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.). ACM / IW3C2, 878â€“887.
https://doi.org/10.1145/3442381.3450133
[32] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,
Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al .2023. A Survey on Large
Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023).
[33] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2023. Graph Neural
Networks in Recommender Systems: A Survey. ACM Comput. Surv. 55, 5 (2023),
97:1â€“97:37. https://doi.org/10.1145/3535101
[34] Zhenghua Xu, Thomas Lukasiewicz, Cheng Chen, Yishu Miao, and Xiangwu
Meng. 2017. Tag-Aware Personalized Recommendation Using a Hybrid Deep
Model. In Proceedings of the Twenty-Sixth International Joint Conference on Ar-
tificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, Carles
Sierra (Ed.). ijcai.org, 3196â€“3202. https://doi.org/10.24963/IJCAI.2017/446
[35] Yuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang. 2023. Knowledge
Graph Self-Supervised Rationalization for Recommendation. In Proceedings of
the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD
2023, Long Beach, CA, USA, August 6-10, 2023, Ambuj K. Singh, Yizhou Sun, Leman
Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping
Ye (Eds.). ACM, 3046â€“3056. https://doi.org/10.1145/3580305.3599400
[36] Yantao Yu, Zhen Wang, and Bo Yuan. 2019. An Input-aware Factorization Machine
for Sparse Prediction. In Proceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019,
Sarit Kraus (Ed.). ijcai.org, 1466â€“1472. https://doi.org/10.24963/IJCAI.2019/203
[37] Yin Zhang, Can Xu, XianJun Wu, Yan Zhang, LiGang Dong, and Weigang Wang.
2022. LFGCF: Light Folksonomy Graph Collaborative Filtering for Tag-Aware
Recommendation. CoRR abs/2208.03454 (2022). https://doi.org/10.48550/ARXIV.
2208.03454 arXiv:2208.03454
[38] Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. 2021. ConE:
Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs. In Ad-
vances in Neural Information Processing Systems 34: Annual Conference on Neural
Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,
Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and
Jennifer Wortman Vaughan (Eds.). 19172â€“19183. https://proceedings.neurips.cc/
paper/2021/hash/a0160709701140704575d499c997b6ca-Abstract.html
[39] Ziwei Zhao, Xi Zhu, Tong Xu, Aakas Lizhiyu, Yu Yu, Xueying Li, Zikai Yin, and En-
hong Chen. 2023. Time-interval Aware Share Recommendation via Bi-directional
Continuous Time Dynamic Graphs. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval, SIGIR
2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Edward) Duh,
Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete (Eds.).
ACM, 822â€“831. https://doi.org/10.1145/3539618.3591775
[40] Zhi Zheng, Chao Wang, Tong Xu, Dazhong Shen, Penggang Qin, Baoxing Huai,
Tongzhu Liu, and Enhong Chen. 2021. Drug package recommendation via
interaction-aware graph induction. In Proceedings of the Web Conference 2021.
1284â€“1295.
[41] Zhi Zheng, Chao Wang, Tong Xu, Dazhong Shen, Penggang Qin, Xiangyu Zhao,
Baoxing Huai, Xian Wu, and Enhong Chen. 2023. Interaction-aware drug package
recommendation via policy gradient. ACM Transactions on Information Systems
41, 1 (2023), 1â€“32.
[42] Xi Zhu, Pengfei Luo, Ziwei Zhao, Tong Xu, Aakas Lizhiyu, Yu Yu, Xueying
Li, and Enhong Chen. 2023. Few-Shot Link Prediction for Event-Based Social
Networks via Meta-learning. In Database Systems for Advanced Applications -
28th International Conference, DASFAA 2023, Tianjin, China, April 17-20, 2023,
Proceedings, Part III (Lecture Notes in Computer Science, Vol. 13945), Xin Wang,
Maria Luisa Sapino, Wook-Shin Han, Amr El Abbadi, Gill Dobbie, Zhiyong Feng,
Yingxiao Shao, and Hongzhi Yin (Eds.). Springer, 31â€“41. https://doi.org/10.1007/
978-3-031-30675-4_3
[43] Yi Zuo, Jiulin Zeng, Maoguo Gong, and Licheng Jiao. 2016. Tag-aware recom-
mender systems based on deep neural networks. Neurocomputing 204 (2016),
51â€“60. https://doi.org/10.1016/J.NEUCOM.2015.10.134A APPENDIX
A.1 Baseline Details
â€¢BPR [ 24]:It is a well-known model that employs Bayesian
personalized ranking as the objective function.
â€¢NFM [ 12]:It is the first work to model the interactions
among high-order features via neural network.
â€¢IFM [ 36]:It is a variant of factorization matrix method
to consider the impact of each individual input upon the
representations of features.
â€¢NGCF [ 30]:It first propagates embeddings of users and
items in an explicit manner to capture the expressive high-
order collaborative signals.
â€¢LightGCN [ 13]:It is the state-of-the-art GNN-based ap-
proach with the simplest framework that only preserves the
neighborhood aggregation part.
â€¢KGIN [ 31]:It considers the user-item interaction at the finer
granularity of intents and utilizes the relational path-aware
aggregation mechanism to identify user intents.
â€¢KGRec [ 35]:It is the self-supervised KG-based scheme that
applies mask mechanism to locate the critical information
and discards the noisy and irrelevant nodes in KG.
â€¢DSPR [ 34]:It is the first model to employ MLPs to process
tag-based features to extract user and item representations.
â€¢TGCN [ 4]:It first introduces tag information into GNNs
with attention mechanism which brings rich semantic repre-
sentations of users and items.
â€¢LFGCF [ 37]:It is the latest state-of-the-art method in TRS,
which borrows the idea of LightGCN and separately models
three types of message aggregation.
A.2 Reproductive Settings
We implement our BoxGNN in PyTorch. For a fair comparison,
we fix the embedding size to 64 for all methods. The batch size
is fixed to 1024. We employ the grid search strategy to optimize
the hyper-parameters with Adam optimizer. In detail, the learn-
ing rate is searched amongst {1ğ‘’âˆ’5,1ğ‘’âˆ’4,1ğ‘’âˆ’3}, the regularization
coefficient is tuned in {1ğ‘’âˆ’6,1ğ‘’âˆ’5,1ğ‘’âˆ’4,1ğ‘’âˆ’3}. We leverage node
dropout to avoid over-fitting, where dropout ratio is tuned amongst
{0.1,0.2,...,0.7}. Notably, we use xavier[ 11] to initialize both center
and offset embeddings.
 
1780