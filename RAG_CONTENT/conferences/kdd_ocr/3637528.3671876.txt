Top-Down Bayesian Posterior Sampling for Sum-Product
Networks
Soma Yokoi
The University of Tokyo
Tokyo, Japan
syokoi@g.ecc.u-tokyo.ac.jpIssei Sato
The University of Tokyo
Tokyo, Japan
sato@g.ecc.u-tokyo.ac.jp
Abstract
Sum-product networks (SPNs) are probabilistic models character-
ized by exact and fast evaluation of fundamental probabilistic opera-
tions. Its superior computational tractability has led to applications
in many fields, such as machine learning with time constraints
or accuracy requirements and real-time systems. The structural
constraints of SPNs supporting fast inference, however, lead to in-
creased learning-time complexity and can be an obstacle to building
highly expressive SPNs. This study aimed to develop a Bayesian
learning approach that can be efficiently implemented on large-
scale SPNs. We derived a new full conditional probability of Gibbs
sampling by marginalizing multiple random variables to expedi-
tiously obtain the posterior distribution. The complexity analysis
revealed that our sampling algorithm works efficiently even for the
largest possible SPN. Furthermore, we proposed a hyperparameter
tuning method that balances the diversity of the prior distribution
and optimization efficiency in large-scale SPNs. Our method has im-
proved learning-time complexity and demonstrated computational
speed tens to more than one hundred times faster and superior
predictive performance in numerical experiments on more than 20
datasets.
CCS Concepts
â€¢Computing methodologies â†’Latent variable models; â€¢
Mathematics of computing â†’Gibbs sampling; Metropolis-
Hastings algorithm.
Keywords
Sum-Product Network, Bayesian inference, Gibbs Sampling
ACM Reference Format:
Soma Yokoi and Issei Sato. 2024. Top-Down Bayesian Posterior Sampling for
Sum-Product Networks. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671876
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671876Table 1: Tractability of Probabilistic Models [15, 27]
Factorized Mixture SPN DNN
Sampling âœ“ âœ“ âœ“ âœ“
Density âœ“ âœ“ âœ“ âœ“ /Ã—
Marginal âœ“ âœ“ âœ“ Ã—
Conditional âœ“ âœ“ âœ“ Ã—
Moment âœ“ âœ“ âœ“ Ã—
Deep structure Ã— Ã— âœ“ âœ“
1 Introduction
Probabilistic machine learning can account for uncertainty, but
many important inference tasks often encounter computational
difficulties due to high-dimensional integrals. Simple probabilistic
models, such as factorized and mixture models, can compute funda-
mental probabilistic operations exactly in polynomial time to the
model size, e.g., likelihood, marginalization, conditional distribu-
tion, and moments as shown in Table 1. However, these models
often suffer from insufficient expressive power due to the lack of
scalability to complex structures. Consequently, the primary focus
of current probabilistic modeling is to achieve both expressiveness
and computational tractability.
A sum-product network (SPN) [ 20] has received much attention
in recent years thanks to its tractability by design. SPNs are consid-
ered a deep extension of the factorized and mixture models while
maintaining their tractability. The tractability of SPNs is a notable
characteristic, markedly distinct from models using deep neural
networks (DNNs) such as generative adversarial networks [ 8], vari-
ational autoencoders [ 12], and normalizing flows [ 24], where many
operations are approximated. SPNs have many possible uses for
machine learning tasks that require fast and exact inference, e.g.,
image segmentation [ 23,33], speech processing [ 17], language mod-
eling [ 4], and cosmological simulations [ 14]. SPNs have also been
investigated for real-time systems such as activity recognition [ 1],
robotics [ 21], probabilistic programming [ 25], and hardware de-
sign [26].
Learning an SPN is relatively more complicated and time con-
suming than inference. Conventional approaches like gradient de-
scent [ 6,20] and an expectation-maximization algorithm [ 9,20]
often suffer from overfitting, mode collapse, and instability in miss-
ing data. One way to prevent these problems is Bayesian learning.
Bayesian moment matching [ 10,22] and collapsed variation in-
ference [ 34] were proposed, and in recent years, Gibbs sampling
has been remarkable for generating samples from the posterior
distribution. Vergari et al . [32] obtained the samples from the full
conditional distribution using ancestral sampling that recursively
3977
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Soma Yokoi and Issei Sato
Computational Graph Density p(x) Structural constraints
+
Ã— Ã— Ã—
0.6
0.30.1.062
.07 .06 .02
.35 .2 .4 .15 .1 .2
x1x2x1x2x1x20.6
0.30.1Ã—
x1x2x3
decomposableÃ—
x1x2x2
non-decomposable
+
x1x1x1
complete+
x1x1x2
incomplete
Figure 1: Overview of SPNs. Left: computational graph representing the structure and weights of the SPN. Center: evaluation of
the density of input data ğ’™. Right: structural constraints ensuring tractability.
traces the graph from the leaf nodes toward the root, and Trapp
et al. [28] showed that the model parameters and the network struc-
ture can be learned simultaneously in a theoretically consistent
Bayesian framework. However, although these studies validated
the flexibility of Bayesian learning, little attention has been paid to
the computational complexity.
The structural constraints of sum and product nodes guarantee
the inference-time tractability of SPNs but lead to increased time
complexity during posterior sampling. The scope of a node, typ-
ically feature dimensions included in its children, is constrained
bydecomposability andcompleteness conditions. SPNs still possess
high expressiveness despite these constraints, although the graph
shape is restricted. In particular, the height of SPNs has a strict
upper limit, so the graph tends to be broad. Ko et al . [13] reported
that existing structural learning methods [ 3,7,31,36] are more
likely to generate wide graphs, which can be a performance bottle-
neck for inference. It is also computationally disadvantageous for
existing posterior sampling that relies on bottom-up traversal of
the entire graph. This problem is currently an obstacle to building
highly expressive large-scale SPNs.
This study aims to extend the efficiency of SPNs to Bayesian
learning. To enable Bayesian learning of SPNs in a reasonable time
for large network sizes, we solve the three most critical problems
from theory to practice: (1) A new full conditional probability of
Gibbs sampling is derived by marginalizing multiple random vari-
ables to obtain the posterior distribution. (2) To efficiently sample
from the derived probability, we propose a novel sampling algo-
rithm, top-down sampling algorithm, based on the Metropolisâ€“Hast-
ings method considering the complexity of SPNs. It reduces the
exponent by one in learning-time complexity and achieves tens to
more than one hundred times faster runtime in numerical experi-
ments. (3) For the increasing hyperparameters of large-scale SPNs,
a new tuning method is proposed following an empirical Bayesian
approach.This paper is structured as follows. Section 2 quickly reviews
SPNs. We describe an existing latent variable model and Gibbs sam-
pling and illustrate how they are impaired by large-scale SPNs. Sec-
tion 3 reveals the complexity of SPNs and introduces our marginal-
ized posterior, fast sampling algorithm, and hyperparameter tuning
method and discusses their theoretical computational complexity.
Section 4 demonstrates the computational speed, sample correla-
tion, and predictive performance of the proposed method through
numerical experiments on more than 20datasets. Finally, Section 5
concludes the study.
2 Bayesian Sum-Product Networks
This section quickly reviews SPNs and introduces their basics and
constraints. We also describe an existing latent variable model and
approximate Bayesian learning by Gibbs sampling and illustrate
how they are impaired by large-scale SPNs.
2.1 Sum-Product Network
An SPN is a probabilistic model with a computational graph repre-
sented as a rooted directed acyclic graph (DAG) composed of sum,
product, distribution nodes, and edges. Figure 1 left illustrates a typ-
ical computational graph of an SPN. The product node corresponds
to the factorized model, i.e., a product of probability distributions
ğ‘(ğ’™)=Ã
ğ‘‘ğ‘ğ‘‘(ğ‘¥ğ‘‘), and the sum node is the mixture model, i.e.,
a weighted sum of probability distributions ğ‘(ğ’™)=Ã
ğ‘˜ğ‘¤ğ‘˜ğ‘ğ‘˜(ğ’™).
These nodes exist at the root and intermediate levels, performing
the operations on their childrenâ€™s outputs and producing results.
SPNs can stack factorized and mixture models in a complex manner
by repeating the product and sum nodes alternately. The distribu-
tion node at the leaves consists of a probability distribution and its
parameters for a simple, typically one-dimensional variable.
The modeling probability of SPNs corresponds to the bottom-up
evaluation on the computational graph. Figure 1 center depicts an
example of evaluating density ğ‘(ğ’™)ofğ·=2input data ğ’™=(ğ‘¥1,ğ‘¥2):
(1) The input vector ğ’™is divided into simple variables ğ‘¥1andğ‘¥2and
3978Top-Down Bayesian Posterior Sampling for Sum-Product Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Plate Notation Bottom-up Sampling Top-down Sampling
Î¸d
j xd
nÎ³d
jzs
nwsÎ±s
N Jd
DSz= (2,âˆ—,âˆ—,1,2)
+
Ã— Ã—
+ + + +
x1x1x2x2x1x1x2x2z1= 2
z2= 1 z3= 1 z4= 1 z5= 2/hatwidez= (2,1,1,1,2)
+
Ã— Ã—
+ + + +
x1x2/hatwidez1= 2
/hatwidez4= 1 /hatwidez5= 2
Figure 2: Bayesian SPNs. Left: plate notation showing the conditional dependencies of random variables based on the Bayesian
interpretation of SPNs. Center: entire graph traversal for ancestral sampling in the bottom-up approach. Right: subgraph access
for rejection sampling in the proposed top-down approach.
input to the corresponding distribution nodes. The input value is
evaluated by the probability of the distribution node and outputs a
scalar probability value. (2) The intermediate node that receives the
childrenâ€™s output computes their convex combination or product
and outputs the scalar value. This procedure is repeated bottom-up
toward the root. (3) All leaves and intermediate nodes are computed,
and the final output value of the SPN is computed at the root node.
Two simple constraints on the structure of the graph streamline
the fundamental probabilistic operations during inference. Figure 1
right represents the structural constraints of SPNs. The children
of product nodes must have mutually exclusive variables, called
decomposability. Decomposability simplifies the integration of a
product node into the integration of its children. The children of
sum nodes must include common variables, called completeness.
Completeness ensures efficient model counting and minimization
of cardinality [ 5] and simplifies the integration of a sum node into
the integration of its children. SPNs satisfying both conditions can
compute any marginalization in linear time with respect to the
number of nodes [ 18]. The exact evaluation of density, marginaliza-
tion, conditional probability, and moment [ 35] can be performed
efficiently.
2.2 Latent Variable Model
An SPN can be interpreted within the Bayesian framework by con-
sidering it as a latent variable model. Let us introduce a categorical
latent variable zthat indicates a mixture component of the sum
node. The latent state z=(z1,..., zğ‘†)of the SPN is determined by
specifying the state zğ‘ =ğ‘whereğ‘âˆˆ(1,...,ğ¶ğ‘ )for each sum node
ğ‘ âˆˆ(1,...,ğ‘†).
A multinomial distribution is considered as the underlying prob-
ability distribution for latent variable zğ‘ , and a Dirichlet distribution
is given as the conjugate prior distribution
zğ‘ 
ğ‘›âˆ¼Multi zğ‘ 
ğ‘›|wğ‘ âˆ€ğ‘ âˆ€ğ‘›,wğ‘ âˆ¼Dir wğ‘ |ğ›¼ğ‘ âˆ€ğ‘ . (1)
The probability distribution of the distribution node is given by
a distribution ğ¿ğ‘‘
ğ‘—parameterized with ğœƒğ‘‘
ğ‘—and its correspondingconjugate prior distribution
ğ’™ğ‘›âˆ¼Ã–
ğ¿ğ‘‘
ğ‘—âˆˆğ‘‡(zğ‘›)ğ¿ğ‘‘
ğ‘—
ğ’™ğ‘‘
ğ‘›|ğœ½ğ‘‘
ğ‘—
âˆ€ğ‘›, ğœƒğ‘‘
ğ‘—âˆ¼ğ‘
ğœƒğ‘‘
ğ‘—|ğœ¸ğ‘‘
ğ‘—
âˆ€ğ‘—âˆ€ğ‘‘.(2)
The assumption of conjugacy is inherited from existing studies [ 28,
32], and it is noted that even in non-conjugate cases, one-dimensional
leaf distributions can be easily approximated numerically. The prob-
ability distribution ğ¿ğ‘‘
ğ‘—is typically chosen by the variable type (con-
tinuous or discrete) and its domain (real numbers, positive numbers,
finite interval[0,1], etc.). Alternatively, the model can automatically
choose the appropriate distribution by configuring the distribution
node as a heterogeneous mixture distribution [ 32]. The dependency
between the random variables and hyperparameters is summarized
in Figure 2 left.
As indicated by the bold edges in Figure 2 center and right, a
subgraph from the root to the leaves is obtained by removing the
unselected edges from the graph. This is called an induced tree [36].
We denote the induced tree determined by the state zasğ‘‡(z). The
induced tree always includes one distribution node for each feature
dimension,ğ¿ğ‘‘
ğ‘—âˆˆğ‘‡(z) âˆ€ğ‘‘âˆˆ(1,...,ğ·), due to the decomposability
and completeness.
2.3 Gibbs Sampling
From the plate notation in Figure 2 left, the posterior distribution
of SPNs can be obtained as
ğ‘ Z,W,ğš¯|X,ğœ¶,ğœ¸
âˆğ‘(X|Z,ğš¯)ğ‘(Z|W)ğ‘(W|ğœ¶)ğ‘ ğš¯|ğœ¸.(3)
In Bayesian learning of SPNs, it is necessary to numerically
realize the posterior distribution. Previous studies [ 28,32] have used
Gibbs sampling to generate samples alternately from the conditional
probabilities of the three random variables Z,W, and ğš¯:
ğ‘(zğ‘›|xğ‘›,ğš¯,W)âˆ€ğ‘›, ğ‘(ğš¯|X,Z), ğ‘(W|Z). (4)
Since it is difficult to sample all categorical variables Zat once, they
are divided into zğ‘›âˆ€ğ‘›âˆˆ(1,...,ğ‘).
3979KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Soma Yokoi and Issei Sato
Table 2: Complexity of the SPN
Complete Tree Skewed Tree
Definition Maximum Size Asymptotic Order Maximum Size Asymptotic Order
ğ· input dimension - - - -
ğ¶ğ‘ sum outdegree - - - -
ğ¶ğ‘product outdegree ğ· - ğ· -
- graph height 2 logğ¶ğ‘ğ·+2 - 2ğ·âˆ’1
ğ¶ğ‘âˆ’1+2 -
- graph breadth ğ¿ O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
ğ¿ O
ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ 
ğ‘‰ # all nodes ğ‘†+ğ‘ƒ+ğ¿O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
ğ‘†+ğ‘ƒ+ğ¿ O
ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ 
ğ‘† # sum nodes(ğ¶ğ‘ğ¶ğ‘ )logğ¶ğ‘ğ·+1âˆ’1
ğ¶ğ‘ğ¶ğ‘ âˆ’1O
ğ¶logğ¶ğ‘ğ·
ğ‘ 
ğ¶ğ‘ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ +(1âˆ’ğ¶ğ‘)ğ¶ğ‘ 
ğ¶ğ‘ âˆ’1O
ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1
ğ‘ 
ğ‘ƒ # product nodesğ¶ğ‘ Â·(ğ¶ğ‘ğ¶ğ‘ )logğ¶ğ‘ğ·âˆ’ğ¶ğ‘ 
ğ¶ğ‘ğ¶ğ‘ âˆ’1O
ğ¶logğ¶ğ‘ğ·
ğ‘ 
ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ âˆ’ğ¶ğ‘ 
ğ¶ğ‘ âˆ’1O
ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1
ğ‘ 
ğ¿ # distribution nodes ğ¶ğ‘ Â·(ğ¶ğ‘ğ¶ğ‘ )logğ¶ğ‘ğ·O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
(ğ¶ğ‘ğ¶ğ‘ âˆ’1)ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ +(1âˆ’ğ¶ğ‘)ğ¶2
ğ‘ 
ğ¶ğ‘ âˆ’1O
ğ¶ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ 
- # induced trees ğ¶ğ¶ğ‘ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘  O
ğ¶ğ‘ªğ‘ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ 
ğ¶ğ¶ğ‘ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘  O
ğ¶ğ‘ªğ‘ğ·âˆ’1
ğ¶ğ‘âˆ’1+1
ğ‘ 
Ancestral sampling is employed to generate the ğ‘†-dimensional
latent vector zğ‘›=(z1ğ‘›,..., zğ‘†ğ‘›)from the joint distribution. Starting
at the root node, sample the branch ğ‘âˆˆğ¶â„(ğ‘ )for each sum node ğ‘ 
encountered
ğ‘ğ‘  zğ‘ 
ğ‘›=ğ‘|xğ‘›,ğš¯,Wâˆğ‘¤ğ‘ ,ğ‘ğ‘ğ‘(xğ‘›|ğš¯,W). (5)
Figure 2 center illustrates joint sampling of latent vector zğ‘›using
ancestral sampling. The algorithm runs bottom-up, where the out-
put is determined first [ 20,28,32]. The states excluded from the
induced tree are sampled from the prior [16].
Posterior sampling using this approach is often computationally
challenging for large SPNs. Ancestral sampling requires a traversal
of the entire graph, which is too costly to be executed inside the
most critical loop of Gibbs sampling. The larger SPNs typically
require more iterations to reach a stationary state, making the
problem more serious.
3 Proposed Method
This section reveals the complexity of SPNs and the structures that
highly expressive SPNs tend to form. Then we propose a novel
posterior sampling method for the SPNs and discuss its theoret-
ical advantages. The proposed method is explained from three
perspectives: a new full conditional probability of Gibbs sampling
that marginalizes multiple random variables, a top-down sampling
algorithm that includes new proposal and rejection steps, and hy-
perparameter tuning using an empirical Bayesian approach.
3.1 Complexity of SPNs
Both top-down and bottom-up algorithms are applicable to DAG-
structured SPNs. However, the ensuing discussion on computa-
tional complexity confines SPNs to tree structures. Tree-structuredSPNs are not necessarily compact representations [ 27], but they
are frequently used in previous studies [7, 19, 28, 29, 31, 32].
This study aims to propose a sampling method that efficiently
operates even on SPNs of theoretically maximum size. For compu-
tational considerations, we assume that SPNs fulfill the following
conditions: (1) They satisfy the structural constraints of complete-
ness and decomposability. (2) The outdegrees of sum and product
nodes areğ¶ğ‘ ,ğ¶ğ‘â‰¥2, respectively. (3) The graph exhibits a tree
structure, implying that the nodes do not have overlapping children.
(4) The root is a sum node. These assumptions enable subsequent
algorithmic discussions to cover the worst-case computational com-
plexity under given ğ·,ğ¶ğ‘ , andğ¶ğ‘.
The structural constraints are important for making SPNs tractable
for fundamental probability operations, but they restrict the graph
shape. Table 2 shows the maximum possible size of an SPN. For sim-
plicity, it only presents cases where the SPN is a complete tree, i.e.,
the product nodes evenly distribute children, resulting in logğ¶ğ‘ğ·
being an integer, or conversely, where the children of product nodes
are maximally skewed, withğ·âˆ’1
ğ¶ğ‘âˆ’1being an integer. Other tree struc-
tures fall within the intermediate of these cases. In either case, the
number of all nodes ğ‘‰, distribution nodes ğ¿, induced trees, and
graph breadth are larger than the other variables. Note that the
sizes may differ if one considers SPNs with different conditions. For
example,ğ‘‰andğ‘†are proportional in Zhao et al . [34] , while they
are asymptotically different by a degree of one in our case.
The height and breadth of the SPN should be noted. Increasing
ğ¶ğ‘ can widen the graph, whereas increasing ğ¶ğ‘reduces the height.
Since the product nodes must have children of different feature
dimensions for decomposability, the number of product nodes is
restricted by ğ·. Each time a product node is passed through on
the graph, it consumesğ¶ğ‘dimensions from ğ·. When the SPN is
a complete tree, the product node satisfying decomposability can
3980Top-Down Bayesian Posterior Sampling for Sum-Product Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 3: Example of the SPN for ğ·=2input data. Left: the largest computational graph with ğ¶ğ‘ children for each sum node.
Right: graph breadth and expressivity on non-linearly correlated examples.
only be used up to logğ¶ğ‘ğ·times from a leaf to the root. For the
SPN where sum and product nodes alternate, the graph height
is limited to 2logğ¶ğ‘ğ·+2, including the root and leaves. When
the consumption is minimized, i.e., ğ¶ğ‘=2, the graph height is
maximized to 2log2ğ·+2. In contrast, the outdegree ğ¶ğ‘ of sum nodes
can be increased while satisfying completeness. Accordingly, the
graph breadth can increase as ğ¶ğ‘ Â· ğ¶ğ‘ğ¶ğ‘ logğ¶ğ‘ğ·without limitation.
Figure 3 left shows the possible graph structures of an SPN for
ğ·=2data. While the height is limited to a maximum of 4when
ğ¶ğ‘=2, the breadth can expand at 2ğ¶2ğ‘ by increasing ğ¶ğ‘ . Figure 3
right depicts the density of an SPN modeling a two-dimensional
artificial dataset with a strong correlation. As ğ¶ğ‘ increases and the
SPN becomes broader, it can capture the complex distribution of
spiral data. SPNs are highly expressive even for complicated data by
combining multiple one-dimensional distribution nodes. However,
the breadth of SPNs significantly impacts this. In order for SPNs to
have high representational power under the structural constraints,
ğ¶ğ‘ must be increased.
3.2 Marginalized Posterior Distribution
The existing studies [ 28,32] use Gibbs sampling that updates all
random variables zğ‘›(ğ‘›=1,...,ğ‘),W, and ğš¯alternately in Equa-
tion (4), resulting in strong correlation between consecutive sam-
ples. In such cases, the mixing can be slow due to potential barriers
that cannot be overcome unless multiple random variables are up-
dated simultaneously. We solve this problem by marginalization.
From the dependency between variables shown in Figure 2 left,
we marginalize the two random variables Wandğš¯. The marginal-
ized posterior distribution including only Zis given by
ğ‘ Z|X,ğœ¶,ğœ¸âˆğ‘ X|Z,ğœ¸ğ‘(Z|ğœ¶). (6)
Sampling Wandğš¯can be omitted entirely during learning. It
reduces the number of variables that need to be sampled, thereby
reducing the sample correlation and the number of iterations. The
parameters Wandğš¯are required during inference, so they aresampled immediately before use. Since the learning and inference
processes are usually separated in Gibbs sampling, this delayed
evaluation approach works efficiently.
In the algorithm, the sampling of Wandğš¯is replaced by deter-
ministic computation of sufficient statistics, which is constantly
referenced during sampling Z. The problem here is that marginal-
ization can sometimes result in complex implementation, leading
to decreased performance. We will explain the new sampling algo-
rithm designed from the viewpoint of computational complexity.
3.3 Top-Down Sampling Method
The full conditional distribution of Gibbs sampling to obtain a
sample from the posterior is given by
ğ‘
zğ‘›|X,Z\ğ‘›,ğœ¶,ğœ¸
. (7)
Directly generating zğ‘›from Equation (7) is difficult due to the
dependencies of random variables. We break it down into simple
components. The distribution can be interpreted as a product of
two factors:
ğ‘
zğ‘›=ğ’„|X,Z\ğ‘›,ğœ¶,ğœ¸
âˆğ‘
zğ‘›=ğ’„|Z\ğ‘›,ğœ¶
|                  {z                  }
networkÂ·ğ‘
xğ‘›|X\ğ‘›,zğ‘›=ğ’„,Z\ğ‘›,ğœ¸
|                             {z                             }
leaf.(8)
The network determines the first factor, i.e., the weights of the sum
nodes. The leaves determine the second factor, i.e., the probability
distributions of the distribution nodes. These factors have different
properties and thus require different approaches for efficient com-
putation. We design our algorithm to have calculations related to
ğ‘†andğ·, where the degree of increase is relatively small from the
complexity of SPNs in Table 2.
3.3.1 Network Proposal. By the conjugate prior distribution of
the Multinomial and Dirichlet distributions in Equation (1), the
network factor is Dirichlet posterior predictive distributions after
3981KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Soma Yokoi and Issei Sato
integrating out W
ğ‘
zğ‘›=ğ’„|Z\ğ‘›,ğœ¶
âˆğ‘†Ã–
ğ‘ =1
Nğ‘ ,ğ‘ğ‘ 
\ğ‘›+ğ›¼ğ‘ğ‘ 
(9)
represented by allocation counts Nğ‘ ,ğ‘
\ğ‘›=Ã
ğ‘š=(1,...,ğ‘)\ğ‘›ğ›¿ zğ‘ ğ‘š=ğ‘
and the concentration hyperparameters ğœ¶.
Equation (9) can be evaluated and sampled with O(ğ‘†)time. We
regard Equation (9) as mixtures of allocation counts and concentra-
tion parameters for ğ‘ âˆˆ(1,...,ğ‘†)
bzğ‘ 
ğ‘›âˆ¼(
Nğ‘ ,ğ‘ğ‘ 
\ğ‘›with probabilityâˆÃ
ğ‘Nğ‘ ,ğ‘
\ğ‘›
ğ›¼ğ‘ğ‘  with probabilityâˆÃ
ğ‘ğ›¼ğ‘.(10)
This can be implemented with straightforward memory lookup
and basic random number generation. With probabilities propor-
tional toÃ
ğ‘Nğ‘ ,ğ‘
\ğ‘›, sampling from Nğ‘ ,ğ‘
\ğ‘›forğ‘âˆˆ (1,...,ğ¶ğ‘ )is per-
formed, which can be immediately obtained by uniformly selecting
a single element from previous allocations zğ‘ 
\ğ‘›. With probabilities
proportional toÃ
ğ‘ğ›¼ğ‘, sampling from ğ›¼ğ‘is performed, which can
be obtained by uniform or alias sampling. These steps efficiently
generate candidate bzğ‘›=ğ’„from the network factor.
The sampling time complexity O(ğ‘†)is practically important. The
number of sum nodes ğ‘†is approximatelyO
ğ¶logğ¶ğ‘ğ·
ğ‘ 
, and it does
not increase extremely as ğ·andğ¶ğ‘ increase, unlikeO
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
for the number of all nodes ğ‘‰. Therefore, the above steps can be
executed stably in large SPNs.
To make the bzğ‘›=ğ’„generated by this method follow Equation (7),
the rejection described next is necessary.
3.3.2 Leaf Acceptance. By the conjugate prior of leaf probability
distribution in Equation (2), the leaf factor, after integrating out ğš¯,
is a posterior predictive distribution of the leaves in the induced
treeğ‘‡(ğ’„)given state zğ‘›=ğ’„:
ğ‘
xğ‘›|X\ğ‘›,zğ‘›=ğ’„,Z\ğ‘›,ğœ¸
=Ã–
ğ¿ğ‘‘
ğ‘—âˆˆğ‘‡(ğ’„)âˆ«
ğ¿ğ‘‘
ğ‘—
ğ‘¥ğ‘‘
ğ‘›|ğœƒğ‘‘
ğ‘—
ğ‘
ğœƒğ‘‘
ğ‘—|xğ‘‘
\ğ‘›,Z\ğ‘›,ğ›¾ğ‘‘
ğ‘—
ğ‘‘ğœƒğ‘‘
ğ‘—
Â·Ã–
ğ¿ğ‘‘
ğ‘—âˆ‰ğ‘‡(ğ’„)âˆ«
ğ‘
ğœƒğ‘‘
ğ‘—|xğ‘‘
\ğ‘›,Z\ğ‘›,ğ›¾ğ‘‘
ğ‘—
ğ‘‘ğœƒğ‘‘
ğ‘—
=Ã–
ğ¿ğ‘‘
ğ‘—âˆˆğ‘‡(ğ’„)ğ‘ğ¿ğ‘‘
ğ‘—
xğ‘‘
ğ‘›|xğ‘‘
\ğ‘›,Z\ğ‘›,ğ›¾ğ‘‘
ğ‘—
,(11)
whereğ‘(xğ‘›|zğ‘›=ğ’„,ğš¯)is the output of ğ‘‡(ğ’„), and the posterior
predictive distribution of leaf ğ¿ğ‘‘
ğ‘—is given by
ğ‘ğ¿ğ‘‘
ğ‘—
xğ‘‘
ğ‘›|xğ‘‘
\ğ‘›,Z\ğ‘›,ğ›¾ğ‘‘
ğ‘—
=âˆ«
ğ¿ğ‘‘
ğ‘—
ğ‘¥ğ‘‘
ğ‘›|ğœƒğ‘‘
ğ‘—
ğ‘
ğœƒğ‘‘
ğ‘—|xğ‘‘
\ğ‘›,Z\ğ‘›,ğ›¾ğ‘‘
ğ‘—
ğ‘‘ğœƒğ‘‘
ğ‘—.(12)
The induced tree has one distribution node per feature dimension ğ‘‘
in the complete and decomposable SPN. Referring to only ğ·distri-
bution nodes is sufficient in the leaf factor, which is less costly thancalculating all distribution nodes. The problem is that the combina-
tion{ğ¿ğ‘‘
ğ‘—}ğ·
ğ‘‘=1depends on the graph structure of the SPN. The joint
sampling for{ğ¿ğ‘‘
ğ‘—}ğ·
ğ‘‘=1cannot be reduced to simple per-dimension
calculations. Enumerating the possible patterns of induced trees
requiresO 
ğ¶ğ¶ğ‘ğ·âˆ’1
ğ¶ğ‘âˆ’1
ğ‘ !
time as shown in Table 2, exhibiting a steep
growth asğ·andğ¶ğ‘ increase. Generating candidates bzğ‘›=ğ’„from
Equation (11) is computationally prohibited.
Instead, we use the leaf factor to accept candidates. To make
the candidate bzğ‘›=ğ’„generated by the network factor follow the
marginalized posterior distribution, the Metropolisâ€“Hastings accep-
tance probability of the move from zğ‘›=ğ’ƒtoğ’„ismin(1,ğ´(ğ’ƒâ†’ğ’„))
where
ğ´(ğ’ƒâ†’ğ’„)=ğ‘
zğ‘›=ğ’„|X,Z\ğ‘›,ğœ¶,ğœ¸
Â·ğ‘
zğ‘›=ğ’ƒ|Z\ğ‘›,ğœ¶
ğ‘
zğ‘›=ğ’ƒ|X,Z\ğ‘›,ğœ¶,ğœ¸
Â·ğ‘
zğ‘›=ğ’„|Z\ğ‘›,ğœ¶
=ğ‘
xğ‘›|X\ğ‘›,zğ‘›=ğ’„,Z\ğ‘›,ğœ¸
ğ‘
xğ‘›|X\ğ‘›,zğ‘›=ğ’ƒ,Z\ğ‘›,ğœ¸
=Ã
ğ¿ğ‘‘
ğ‘—âˆˆğ‘‡(ğ’„)ğ‘ğ¿ğ‘‘
ğ‘—
xğ‘‘ğ‘›|xğ‘‘
\ğ‘›,Z\ğ‘›,ğ›¾ğ‘‘
ğ‘—
Ã
ğ¿ğ‘‘
ğ‘—âˆˆğ‘‡(ğ’ƒ)ğ‘ğ¿ğ‘‘
ğ‘—
xğ‘‘ğ‘›|xğ‘‘
\ğ‘›,Z\ğ‘›,ğ›¾ğ‘‘
ğ‘—.(13)
The network factors in the numerator and denominator cancel out
each other and do not need to be evaluated.
By computing the sufficient statistics of the distribution nodes
beforehand, Equation (11) can be evaluated in O(ğ·), independent
of the number of datapoints ğ‘. Sinceğ‘ğ¿ğ‘‘
ğ‘—is a one-dimensional
probability distribution, the sufficient statistics can be easily up-
dated in constant time for each sample. As shown in Table 2, ğ·is
tiny compared to other dimensionalities, so the acceptance prob-
ability can be efficiently obtained. Furthermore, the evaluation of
predictive distributions can be omitted for the dimension ğ‘‘where
the candidate component does not change ( ğ‘ğ‘‘=ğ‘ğ‘‘). It accelerates
Gibbs sampling even when the samples are correlated.
3.3.3 Algorithm and Complexity. The efficient implementation of
this algorithm involves traversing the computational graph top-
down while referring to the candidate state bzğ‘›=ğ’„generated by the
network factor, as shown in Figure 2 right. Ignoring the nodes out-
side the induced tree, only the distribution nodes required in Equa-
tion (13) are evaluated. Compared to the existing method traversing
the entire graph bottom-up in Figure 2 center, the proposed method
can be more efficiently executed by accessing a subgraph consisting
of a limited number of nodes. Intuitively, the top-down method is
more efficient for wider graphs. Table 3 compares the time com-
plexity of the algorithms. The entire algorithm of the top-down
method is shown as Appendix B.
The parameters Wandğš¯marginalized by the proposed method
are not updated during Gibbs sampling. These values need to be
updated to the latest ones in inference time. This pre-processing is
identical to what is done during each iteration of Gibbs sampling in
the bottom-up algorithm (Equation (4)) and is typically performed
at checkpoints during training or upon completion of training. Since
these are not significant in terms of time complexity, the inference
3982Top-Down Bayesian Posterior Sampling for Sum-Product Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Time complexity of the sampling algorithms
Top-down Bottom-up [32]
Gibbs sampling O
ğ¶logğ¶ğ‘ğ·
ğ‘ 
O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
inference with pre-processing O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
time complexity of the proposed method does not change as shown
in Table 3.
3.4 Empirical Bayesian Hyperparameter Tuning
Another challenge in large-scale SPNs is hyperparameter optimiza-
tion, an essential task in Bayesian learning. The hyperparameters
of SPNs include ğ¶ğ‘ ,ğ¶ğ‘,ğœ¶, and ğœ¸. In particular, the appropriate
ğœ¸ğ‘‘
ğ‘—induces diversity in each distribution node ğ¿ğ‘‘
ğ‘—and helps SPNs
choose suitable component distributions. As shown in Table 2,
there are asymptotically O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
distribution nodes and ğœ¸is
proportional to them. The number of hyperparameters can easily
exceed hundreds to thousands in a real-world dataset. Applying
currently common hyperparameter optimization methods such as
tree-structured Parzen estimator (TPE) [ 2] is not advised in terms
of the number of trials. We must consider tuning proxy parameters
instead of directly tuning ğœ¸.
In the empirical Bayesian approach, hyperparameters are ob-
tained by maximizing the marginal likelihood. Although calcu-
lating the marginal likelihood ğ‘ X|ğœ¶,ğœ¸of an SPN is difficult,
maximizing it with respect to a hyperparameter ğœ¸ğ‘‘
ğ‘—can be reduced
to maximizing the mixture of leaf marginal likelihoods ğ‘ğ¿ğ‘‘
ğ‘—
Â·|ğœ¸ğ‘‘
ğ‘—
:
arg max
ğœ¸ğ‘‘
ğ‘—ğ‘ X|ğœ¶,ğœ¸=arg max
ğœ¸ğ‘‘
ğ‘—âˆ‘ï¸
Xğ‘âŠ†Xğœ”ğ‘ğ‘ğ¿ğ‘‘
ğ‘—
xğ‘‘
ğ‘|ğœ¸ğ‘‘
ğ‘—
.(14)
This is derived from the fact that SPNs can be considered a mixture
of induced trees [ 36] and induced trees are factorized leaf models
for each feature dimension (Section 2.2). The summationÃ
Xğ‘âŠ†Xis
taken over all possible subset Xğ‘of dataset X. When there are ğ‘
datapoints in the dataset, the summation size is 2ğ‘. The coefficient
ğœ”ğ‘is complicated and depends on the other hyperparameters ğœ¸\ğ›¾ğ‘‘
ğ‘—.
Whereas the exact evaluation of Equation (14) is computationally
impossible, it gives an essential insight that the marginal likelihood
of distribution nodes over subset data gives the empirical Bayes
estimate.
We consider approximating Equation (14) by a significant term
of the leaf marginal likelihood with specific subset data. Our goal
is not to identify the optimal subset Xğ‘directly but to find the
subsampling ratio ğ‘Ÿğ‘‘âˆˆ (0,1]that Xğ‘should contain from the
dataset Xfor each feature dimension ğ‘‘. The empirical Bayes es-
timate of hyperparameter ğ›¾ğ‘‘
ğ‘—is approximated with subset data
xğ‘‘ğ‘=subsample
xğ‘‘,ğ‘Ÿğ‘‘
by
bğ›¾ğ‘‘
ğ‘—=arg max
ğ›¾ğ‘‘
ğ‘—ğ‘ğ¿ğ‘‘
ğ‘—
xğ‘‘
ğ‘|ğ›¾ğ‘‘
ğ‘—
.(15)By tuningğ‘Ÿğ‘‘using hyperparameter optimization methods, distri-
bution nodes with appropriate hyperparameters are expected to
become the main component of the leaf mixture. When ğ‘Ÿğ‘‘=1, it is
empirical Bayes over the full dataset, and all the distribution nodes
ğ¿ğ‘‘have the same hyperparameters for ğ‘‘. Asğ‘Ÿğ‘‘approaches 0, the
intersection of the subset data becomes smaller, resulting in diverse
distribution nodes. This approach significantly reduces the number
of optimized hyperparameters from the order of distribution nodes
O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
toO(ğ·), while maintaining the flexibility of the
prior distributions.
It is easy to obtain the empirical Bayes solution for leaf hy-
perparameters numerically because distribution nodes have one-
dimensional probability distributions. Closed-form solutions can
also be obtained for some parameters.
3.5 Contrast with Prior Work
Zhao et al . [34] employed collapsed variational inference to approx-
imate the posterior distribution. Their algorithm is optimization-
based and significantly different from our method based on poste-
rior sampling. Also, it integrated out Z, which is a unique approach
different from many other variational inferences. It is also distinct
from our posterior distribution marginalizing Wandğš¯.
The proposed method only discusses Bayesian parameter learn-
ing and does not mention structural learning. In the experiments in
Section 4, we use a heterogeneous leaf mixture similar to Vergari
et al. [32] , and an appropriate probability distribution is selected
by weighting from multiple types of distribution nodes. It is pos-
sible to perform Bayesian structural learning similar to [ 28], but
pre-processing is required for the marginalized parameters ğš¯for a
conditional probability in structural inference. The speed benefits
of our method may be compromised depending on the frequency
of structural changes.
Trapp et al . [28] , Vergari et al . [32] assume that all distribution
nodes have the same hyperparameters for each feature dimension
and distribution type. This simple setting always assumes the same
prior distribution, so it does not induce diversity in distribution
nodes like our empirical Bayesian approach on subset data. When
the hyperparameters are selected to maximize the marginal likeli-
hood, the results are expected to be similar to the proposed method
with subsampling proportion ğ‘Ÿğ‘‘=1âˆ€ğ‘‘.
4 Experiments
In this section, we compare the empirical performance of top-
down and bottom-up sampling methods. We used a total of 24
datasets, where 18datasets were from the UCI repository [ 11]
and OpenML [ 30] and 6datasets were from the previous studies
3983KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Soma Yokoi and Issei Sato
Table 4: Elapsed time [s] per Gibbs iteration (mean Â±std)
ğ¶ğ‘ =2 ğ¶ğ‘ =4
Dataset
Top-down Bottom-up Speedup Top-down Bottom-up Speedup
Abalone 0.076Â±0.004 1.574Â±0.043Ã—21 0.367Â±0.019 17.38Â±0.699Ã—47
Ailer
ons 2.938Â±0.121 96.28Â±2.631Ã—33 181.2Â±28.06 3282Â±105.9Ã—18
Airfoil Self-Noise 0.010Â±0.001 0.303Â±0.024Ã—30 0.021Â±0.001 2.047Â±0.143Ã—97
Breast 0.005Â±0.000 0.170Â±0.016Ã—34 0.014Â±0.001 1.938Â±0.079Ã—138
Computer Hardware 0.003Â±0.000 0.098Â±0.011Ã—33 0.007Â±0.000 0.943Â±0.078Ã—135
cpu_act 0.525Â±0.016 16.80Â±0.771Ã—32 11.72Â±2.503 588.0Â±15.46Ã—50
cpu_small 0.204Â±0.005 5.592Â±0.206Ã—27 2.059Â±0.119 99.17Â±3.382Ã—48
Crx 0.023Â±0.001 0.722Â±0.013Ã—31 0.232Â±0.006 14.21Â±0.458Ã—61
Table 5: Effective sample size per 50 samples
ğ¶ğ‘ =2 ğ¶ğ‘ =4
Dataset
Top-down Bottom-up Top-down Bottom-up
Dermatology 15.6Â±0.320.6Â±0.119.1Â±0.120.1Â±0.0
ele
vators 15.1Â±0.218.0Â±0.3 17 .0Â±0.1 -
Forest Fires 18.9Â±0.520.6Â±0.218.6Â±0.120.5Â±0.0
German 17.3Â±0.320.6Â±0.118.9Â±0.119.4Â±0.0
Housing 17.3Â±1.220.6Â±0.117.4Â±0.220.5Â±0.0
Hybrid Price 20.4Â±0.720.5Â±1.020.5Â±0.420.6Â±0.4
kin8nm 18.2Â±2.120.8Â±0.316.8Â±0.119.5Â±0.2
LPGA2008 20.7Â±0.6 20.5Â±0.220.5Â±0.2 20 .5Â±0.1Table 6: Log-likelihood
ğ¶ğ‘ =2 ğ¶ğ‘ =4
Dataset
Top-down Bottom-up Top-down Bottom-up
LPGA2009âˆ’28.3Â±0.4âˆ’29.6Â±0.2âˆ’27.4Â±0.4âˆ’29.7Â±0.2
P
. motor 43.9Â±0.7 41.9Â±0.3 43.5Â±0.6 42.1Â±0.1
P. total 43.4Â±0.6 41.7Â±0.1 42.8Â±0.8 41.5Â±0.1
Voteâˆ’49.6Â±0.1âˆ’49.8Â±0.0âˆ’49.8Â±0.1âˆ’49.9Â±0.1
W. Redâˆ’3.6Â±0.3âˆ’4.4Â±0.4âˆ’2.7Â±0.3âˆ’3.0Â±0.1
W. Whiteâˆ’3.0Â±0.6âˆ’3.6Â±0.6âˆ’2.6Â±0.1âˆ’2.6Â±0.1
Wineâˆ’19.9Â±0.6âˆ’22.4Â±0.5âˆ’18.4Â±0.4âˆ’20.6Â±0.1
Yacht 2.2Â±1.8 1.0Â±1.9 8.3Â±0.6 5.0Â±0.2
(Abalone, Breast, Crx, Dermatology, German, Wine). The datasets
were divided into 8:1:1 for training, validation, and testing. To pre-
vent irregular influence on the results, ğ‘˜-nearest neighbor based
outlier detection was performed as pre-processing. We fixed ğ¶ğ‘=2
and investigated the effect of different ğ¶ğ‘ . Each configuration was
tested 10times with different random seeds and the mean and stan-
dard deviation were plotted. The leaf distribution node consisted
of a heterogeneous mixture of one-dimensional probability distri-
butions (exponential, Gaussian, Poisson, and multinomial) similar
to Vergari et al . [32] . The hyperparameters of the leaf distributions
were obtained by the empirical Bayes approach described in Sec-
tion 3.4 optimized by conducting 100trials of TPE. The code was
written in Julia and iterated as much as possible within 12 hours
(including a 6-hour burn-in period) on an Intel Xeon Bronze 3204
CPU machine.
4.1 Computational Efficiency
We first measured the elapsed time required for each sampling
method to perform one iteration of Gibbs sampling and confirmed
how much acceleration was achieved. Table 4 shows the elapsed
time required for each sampling method to perform one itera-
tion of Gibbs sampling. The results consistently show that the
top-down method is superior and generally several orders of mag-
nitude shorter in execution time. The bottom-up method calculates
outputs at all nodes and propagates them using the logsumexp al-
gorithm, whereas the top-down method only identifies the memory
addresses of induced leaves. Therefore, the actual computation time
is significantly faster than the theoretical time complexity suggests.In particular, the difference is significant as ğ¶ğ‘ increases. These
results show that the top-down method is tens to more than one
hundred times faster, supporting the efficiency of our method.
4.2 Sample Correlation
Samples generated by Gibbs sampling are not independent but
rather long sequences of random vectors that exhibit correlation
over time. Samples with correlation are redundant and do not pro-
vide information about the underlying distribution efficiently, caus-
ing underestimation of errors in Bayesian inference. In this exper-
iment, we investigate the quality of the samples obtained by the
two sampling methods by comparing the effective sample size.
Table 5 shows the effective sample size of 50thinned subsamples
at equal intervals. Since the bottom-up method could not generate
50samples within the time limit for some datasets, the results are
not shown for the cases. The values are large enough for many
configurations, indicating that both sampling methods provide suf-
ficiently independent samples within a realistic time range. The
bottom-up method is slightly superior in many cases, but the dif-
ference is not extreme overall. Due to the marginalization and
rejection, the result of the top-down method remained within a
comparable range to that of the bottom-up method. The results are
stable against the increase in ğ¶ğ‘ .
4.3 Overall Performance
Finally, we evaluated the overall predictive performance of the top-
down sampling method, which reveals the trade-off between fast
iteration speed and sample correlation.
3984Top-Down Bayesian Posterior Sampling for Sum-Product Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 4: Comparison of the temporal evolution of predictive performance between top-down Gibbs sampling, bottom-up
Gibbs sampling, and collapsed variational Bayesian method. The average test-set log-likelihood over 10trials is shown with the
lines, and the standard deviation is indicated by the shaded regions.
Table 6 compares the log-likelihood on the test set for different
ğ¶ğ‘ , showing the results after the burn-in period. Also, Figure 4
illustrates the temporal evolution of predictive performance, show-
ing the differences between the methods by optimizing ğ¶ğ‘ . For
practical interest, this experiment also compares the results with
those of the collapsed variational Bayes [ 34] based on optimization
by the gradient descent method. The top-down method is almost
consistently superior to the others, achieving the same or higher
likelihood in most configurations. These results suggest that the
algorithm speedup outweighs the impact of sample correlation.
While collapsed VB is memory efficient because it does not need
to store latent variables for each data point, it requires two full
network traversals for output propagation and gradient calculation,
resulting in a time complexity of the same order as the bottom-up
method. For a more comprehensive set of results, please refer to
Appendix C.
From the above experiments, we conclude that (1) the top-down
method is tens to more than one hundred times faster than the
bottom-up method, (2) the sample correlation is sufficiently small
for both methods, and (3) as a result, the top-down method can
achieve higher predictive performance than the bottom-up method
in many cases.
5 Conclusion
Prior work has interpreted SPNs as latent variable models and
introduced Gibbs sampling as a Bayesian learning method. This has
made it possible to automatically discriminate different distribution
types for data and learn network structures in a manner consistentwith the Bayesian framework. However, the bottom-up posterior
sampling approach based on the entire graph evaluation had a
computational difficulty. The shape of the computational graph
was a bottleneck due to the structural constraints.
This study aimed to accomplish a fast Bayesian learning method
for SPNs. First, we investigated the complexity of SPNs when the
outdegrees of the sum and product nodes are given and discussed
the graph shape of SPN with high representational power. We
also derived the new full conditional probability that marginalizes
multiple variables to improve sample mixing. For the complexity
and the marginalized posterior distribution, we proposed the top-
down sampling algorithm based on the carefully designed proposal
and rejection steps of the Metropolisâ€“Hastings. Our optimization
efforts resulted in a time complexity reduction from O
ğ¶logğ¶ğ‘ğ·+1
ğ‘ 
down toO
ğ¶logğ¶ğ‘ğ·
ğ‘ 
. In numerical experiments on more than 20
datasets, we demonstrated a speedup of tens to more than one
hundred times and improved predictive performance.
Acknowledgments
Issei Sato was supported by JSPS KAKENHI Grant Number 20H05703
Japan.
3985KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Soma Yokoi and Issei Sato
References
[1]Mohamed R Amer and Sinisa Todorovic. 2016. Sum Product Networks for Activity
Recognition. IEEE Trans Pattern Anal Mach Intell 38, 4 (2016), 800â€“813.
[2]James Bergstra, RÃ©mi Bardenet, Yoshua Bengio, and BalÃ¡zs KÃ©gl. 2011. Algorithms
for Hyper-Parameter Optimization. In Advances in Neural Information Processing
Systems 24 (NIPS 2011). 2546â€“2554.
[3]Cory J. Butz, Jhonatan S. Oliveira, and Andre E. dos Santos. 2017. On Learn-
ing the Structure of Sum-Product Networks. In 2017 IEEE Symposium Series on
Computational Intelligence (SSCI). 1â€“8.
[4]Wei-Chen Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, and Kian Ming A.
Chai. 2014. Language modeling with sum-product networks. In Fifteenth Annual
Conference of the International Speech Communication Association.
[5]Adnan Darwiche. 2001. On the Tractable Counting of Theory Models and its
Application to Truth Maintenance and Belief Revision. Journal of Applied Non-
Classical Logics 11, 1-2 (2001), 11â€“34.
[6]Robert Gens and Pedro Domingos. 2012. Discriminative learning of sum-product
networks. In Advances in Neural Information Processing Systems 25 (NIPS 2012).
3239â€“3247.
[7]Robert Gens and Domingos Pedro. 2013. Learning the Structure of Sum-Product
Networks. In Proceedings of the 30th International Conference on Machine Learning
(ICML 2013). 873â€“880.
[8]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets. In Advances in Neural Information Processing Systems 27 (NIPS
2014). 2672â€“2680.
[9]Wilson Hsu, Agastya Kalra, and Pascal Poupart. 2017. Online Structure Learn-
ing for Sum-Product Networks with Gaussian Leaves. In The 5th International
Conference on Learning Representations (Workshop) (ICLR 2017).
[10] Priyank Jaini, Abdullah Rashwan, Han Zhao, Yue Liu, Ershad Banijamali, Zhitang
Chen, and Pascal Poupart. 2016. Online Algorithms for Sum-Product Networks
with Continuous Variables. In Proceedings of the Eighth International Conference
on Probabilistic Graphical Models (Proceedings of Machine Learning Research,
Vol. 52). 228â€“239.
[11] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. [n. d.]. The UCI Machine
Learning Repository. https://archive.ics.uci.edu. Accessed: 2023-10-12.
[12] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In
The 2nd International Conference on Learning Representations (ICLR 2014).
[13] Ching-Yun Ko, Cong Chen, Zhuolun He, Yuke Zhang, Kim Batselier, and Ngai
Wong. 2020. Deep Model Compression and Inference Speedup of Sumâ€“Product
Networks on Tensor Trains. IEEE Transactions on Neural Networks and Learning
Systems 31, 7 (2020), 2665â€“2671.
[14] Amit Parag and Vaishak Belle. 2022. Tractable Generative Modelling of Cosmo-
logical Numerical Simulations. (2022). working paper or preprint.
[15] Robert Peharz. 2019. Sum-Product Networks and Deep Learning: A Love Marriage.
https://slideslive.com/38917679. Accessed: 2023-10-12.
[16] Robert Peharz, Robert Gens, Franz Pernkopf, and Pedro Domingos. 2017. On the
Latent Variable Interpretation in Sum-Product Networks. IEEE Transactions on
Pattern Analysis and Machine Intelligence 39, 10 (2017), 2030â€“2044.
[17] Robert Peharz, Georg Kapeller, Pejman Mowlaee, and Franz Pernkopf. 2014. Mod-
eling speech with sum-product networks: Application to bandwidth extension.
In2014 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). 3699â€“3703.
[18] Robert Peharz, Sebastian Tschiatschek, Franz Pernkopf, and Pedro M. Domingos.
2015. On Theoretical Properties of Sum-Product Networks. In International
Conference on Artificial Intelligence and Statistics (AISTATS) (Proceedings of the
Eighteenth International Conference on Artificial Intelligence and Statistics, PMLR
38). 744â€“752.[19] Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao,
Martin Trapp, Kristian Kersting, and Zoubin Ghahramani. 2020. Random Sum-
Product Networks: A Simple and Effective Approach to Probabilistic Deep Learn-
ing. In Proceedings of the 35th Uncertainty in Artificial Intelligence Conference
(Proceedings of Machine Learning Research, Vol. 115). 334â€“344.
[20] Hoifung Poon and Pedro Domingos. 2011. Sum-Product Networks: A New Deep
Architecture. In Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence (UAI 2011). 337â€“346.
[21] Andrzej Pronobis and Rajesh P. N. Rao. 2017. Learning deep generative spatial
models for mobile robots. 2017 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS) (2017).
[22] Abdullah Rashwan, Han Zhao, and Pascal Poupart. 2016. Online and Distributed
Bayesian Moment Matching for Parameter Learning in Sum-Product Networks.
InProceedings of the 19th International Conference on Artificial Intelligence and
Statistics (Proceedings of Machine Learning Research, Vol. 51). 1469â€“1477.
[23] Fabian Rathke, Mattia Desana, and Christoph SchnÃ¶rr. 2017. Locally Adaptive
Probabilistic Models for Global Segmentation of Pathological OCT Scans. In
International Conference on Medical Image Computing and Computer-Assisted
Intervention (Lecture Notes in Computer Science, Vol. 10433). Springer, 177â€“184.
[24] Danilo Rezende and Shakir Mohamed. 2015. Variational Inference with Nor-
malizing Flows. In Proceedings of the 32nd International Conference on Machine
Learning (ICML 2015). 1530â€“1538.
[25] Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka. 2021. SPPL: Proba-
bilistic Programming with Fast Exact Symbolic Inference. In Proceedings of the
42nd ACM SIGPLAN International Conference on Programming Language Design
and Implementation. Association for Computing Machinery, 804â€“819.
[26] Lukas Sommer, Julian Oppermann, Alejandro Molina, Carsten Binnig, Kristian
Kersting, and Andreas Koch. 2018. Automatic Mapping of the Sum-Product Net-
work Inference Problem to FPGA-Based Accelerators. In ICCD. IEEE Computer
Society, 350â€“357.
[27] Martin Trapp. 2020. Sum-Product Networks for Complex Modelling Scenarios. Ph. D.
Dissertation. the Graz University of Technology.
[28] Martin Trapp, Robert Peharz, Hong Ge, Franz Pernkopf, and Zoubin Ghahra-
mani. 2019. Bayesian Learning of Sum-Product Networks. In Advances in Neural
Information Processing Systems 32 (NeurIPS 2019).
[29] Martin Trapp, Robert Peharz, and Franz Pernkopf. 2019. Optimisation of Over-
parametrized Sum-Product Networks. In 3rd Workshop of Tractable Probabilistic
Modeling at the International Conference on Machine Learning.
[30] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. OpenML:
networked science in machine learning. SIGKDD Explorations 15, 2 (2013), 49â€“60.
[31] Antonio Vergari, Nicola Di Mauro, and Floriana Esposito. 2015. Simplifying,
Regularizing and Strengthening Sum-Product Network Structure Learning. In
Proceedings of the 2015th European Conference on Machine Learning and Knowledge
Discovery in Databases - Volume Part II (ECMLPKDDâ€™15). 343â€“358.
[32] Antonio Vergari, Alejandro Molina, Robert Peharz, Zoubin Ghahramani, Kristian
Kersting, and Isabel Valera. 2019. Automatic Bayesian Density Analysis. Proceed-
ings of the AAAI Conference on Artificial Intelligence 33, 01 (2019), 5207â€“5215.
[33] Zehuan Yuan, Hao Wang, Limin Wang, Tong Lu, Shivakumara Palaiahnakote, and
Chew Lim Tan. 2016. Modeling Spatial Layout for Scene Image Understanding
via a Novel Multiscale Sum-Product Network. Expert Syst. Appl. 63, C (2016),
231â€“240.
[34] Han Zhao, Tameem Adel, Geoff Gordon, and Brandon Amos. 2016. Collapsed
Variational Inference for Sum-Product Networks. In Proceedings of the 33rd Inter-
national Conference on Machine Learning (ICML 2016) . 1310â€“1318.
[35] Han Zhao and Geoff Gordon. 2017. Linear Time Computation of Moments in
Sum-Product Networks. In Advances in Neural Information Processing Systems 30
(NIPS 2017). 6897â€“6906.
[36] Han Zhao, Pascal Poupart, and Geoff Gordon. 2017. A Unified Approach for Learn-
ing the Parameters of Sum-Product Networks. In Advances in Neural Information
Processing Systems 30 (NIPS 2017). 433â€“441.
3986Top-Down Bayesian Posterior Sampling for Sum-Product Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A Notation
Table 7: Notation
Symbol Definition Dimension
xğ‘› ğ‘›-th data,ğ‘›âˆˆ(1,...,ğ‘) ğ·
zğ‘› categorical assignments of ğ’™ğ‘› ğ‘†
ğ‘  sum node,ğ‘ âˆˆ(1,...,ğ‘†) 1
wğ‘ weight vector of ğ‘  ğ¶
ğ›¼ hyperparameter of wğ‘ 1
ğ¿ğ‘‘
ğ‘—ğ‘—-th leaf distribution of dimension ğ‘‘ 1
ğœƒğ‘‘
ğ‘—parameters of ğ¿ğ‘‘
ğ‘—1
ğ›¾ğ‘‘
ğ‘—hyperparameters of ğœƒğ‘‘
ğ‘—1
ğ‘‡(z) induced tree by z -
B Algorithm
Algorithm 1 Bayesian learning by top-down sampling
Require: dataset X, hyperparameters ğœ¶andğœ¸, the number of
childrenğ¶ğ‘ andğ¶ğ‘
initialize SPN ğœ¶,ğœ¸,ğ¶ğ‘ ,ğ¶ğ‘and sample store Zs
forGibbs iteration ğ‘–do
fordata indexğ‘›âˆˆ(1,...,ğ‘)do
generate candidate ğ’„âˆ¼Â·| Z\ğ‘›,ğœ¶ {Equation (10)}
ifacceptğ´(ğ’ƒâ†’ğ’„)>ğ‘Ÿğ‘ğ‘›ğ‘‘()then
update assignment zğ‘›â†ğ’„
update assign counts Nğ‘ ,ğ‘ğ‘ 
\ğ‘›
update leaf sufficient statistics ğ‘ğ¿ğ‘‘
ğ‘—
end if
end for
ifğ‘–âˆ‰burn-in then
add sample Zsâ†ZsâˆªZ
end if
end for
return Zs
C All Experimental Results
Table 8: Elapsed time [s] per Gibbs iteration (mean Â±std)
ğ¶ğ‘ =2 ğ¶ğ‘ =4
Dataset
Top-down Bottom-up Speedup Top-down Bottom-up Speedup
Abalone 0.076Â±0.004 1.574Â±0.043Ã—21 0.367Â±0.019 17.38Â±0.699Ã—47
Ailer
ons 2.938Â±0.121 96.28Â±2.631Ã—33 181.2Â±28.06 3282Â±105.9Ã—18
Airfoil Self-Noise 0.010Â±0.001 0.303Â±0.024Ã—30 0.021Â±0.001 2.047Â±0.143Ã—97
Breast 0.005Â±0.000 0.170Â±0.016Ã—34 0.014Â±0.001 1.938Â±0.079Ã—138
Computer Hardware 0.003Â±0.000 0.098Â±0.011Ã—33 0.007Â±0.000 0.943Â±0.078Ã—135
cpu_act 0.525Â±0.016 16.80Â±0.771Ã—32 11.72Â±2.503 588.0Â±15.46Ã—50
cpu_small 0.204Â±0.005 5.592Â±0.206Ã—27 2.059Â±0.119 99.17Â±3.382Ã—48
Crx 0.023Â±0.001 0.722Â±0.013Ã—31 0.232Â±0.006 14.21Â±0.458Ã—61
Dermatology 0.034Â±0.002 1.887Â±0.042Ã—56 1.932Â±0.162 92.19Â±3.150Ã—48
elevators 0.759Â±0.025 24.87Â±0.635Ã—33 17.62Â±3.630 804.7Â±35.45Ã—46
Forest Fires 0.015Â±0.001 0.405Â±0.021Ã—27 0.101Â±0.004 5.809Â±0.191Ã—58
German 0.050Â±0.003 2.095Â±0.064Ã—42 1.298Â±0.099 76.78Â±3.338Ã—59
Housing 0.009Â±0.000 0.445Â±0.038Ã—49 0.114Â±0.005 6.616Â±0.250Ã—58
Hybrid Price 0.001Â±0.000 0.015Â±0.000Ã—15 0.001Â±0.000 0.061Â±0.007Ã—61
kin8nm 0.099Â±0.005 3.216Â±0.326Ã—32 0.696Â±0.034 33.76Â±1.653Ã—49
LPGA2008 0.001Â±0.000 0.046Â±0.002Ã—46 0.002Â±0.000 0.318Â±0.034Ã—159
LPGA2009 0.002Â±0.000 0.118Â±0.014Ã—59 0.011Â±0.001 1.347Â±0.061Ã—123
Parkinsons Telemonitoring (motor) 0.207Â±0.012 6.516Â±0.189Ã—31 3.273Â±0.300 157.8Â±5.869Ã—48
Parkinsons Telemonitoring (total) 0.211Â±0.011 6.631Â±0.226Ã—30 3.207Â±0.332 156.4Â±5.911Ã—49
Vote for Clinton 0.028Â±0.002 1.413Â±0.153Ã—50 0.315Â±0.011 15.43Â±0.510Ã—49
Wine Quality Red 0.024Â±0.002 1.100Â±0.079Ã—46 0.294Â±0.012 15.51Â±0.871Ã—53
Wine Quality White 0.088Â±0.006 3.215Â±0.249Ã—37 0.979Â±0.044 49.60Â±2.455Ã—51
Wine 0.004Â±0.000 0.196Â±0.020Ã—49 0.028Â±0.004 2.576Â±0.088Ã—92
Yacht Hydrodynamics 0.003Â±0.000 0.088Â±0.012Ã—29 0.005Â±0.000 0.603Â±0.064Ã—121Table 9: Effective sample size per 50 samples (mean Â±std)
ğ¶ğ‘ =2 ğ¶ğ‘ =4
Dataset
Top-down Bottom-up Top-down Bottom-up
Abalone 19.75Â±0.45 20.39Â±0.40 17.36Â±0.28 19.77Â±0.17
Ailer
ons 14.11Â±0.26 16.74Â±0.18 17 .92Â±0.07 -
Airfoil Self-Noise 20.22Â±0.28 20.47Â±0.55 17.89Â±0.52 20.49Â±0.15
Breast 20.37Â±0.13 20.56Â±0.15 20 .51Â±0.04 20.40Â±0.04
Computer Hardware 20.32Â±0.43 20.69Â±0.24 19.52Â±0.24 20.57Â±0.03
cpu_act 14.37Â±0.63 20.23Â±0.22 15 .50Â±0.70 -
cpu_small 12.94Â±0.99 19.71Â±0.33 17.79Â±0.16 20.18Â±0.13
Crx 18.67Â±0.64 20.63Â±0.13 18.84Â±0.07 19.93Â±0.03
Dermatology 15.56Â±0.25 20.57Â±0.06 19.07Â±0.06 20.10Â±0.03
elevators 15.05Â±0.22 18.01Â±0.34 16 .99Â±0.06 -
Forest Fires 18.86Â±0.45 20.61Â±0.18 18.60Â±0.11 20.53Â±0.03
German 17.34Â±0.34 20.60Â±0.11 18.94Â±0.07 19.35Â±0.03
Housing 17.31Â±1.22 20.56Â±0.14 17.43Â±0.24 20.50Â±0.02
Hybrid Price 20.38Â±0.66 20.54Â±0.97 20.51Â±0.39 20.57Â±0.36
kin8nm 18.15Â±2.14 20.75Â±0.30 16.83Â±0.13 19.45Â±0.18
LPGA2008 20.67Â±0.60 20.46Â±0.24 20.47Â±0.17 20.50Â±0.11
LPGA2009 20.52Â±0.39 20.48Â±0.31 20.37Â±0.15 20.54Â±0.05
Parkinsons Telemonitoring (motor) 16.43Â±2.41 20.29Â±0.29 17 .48Â±0.36 17.07Â±0.06
Parkinsons Telemonitoring (total) 16.54Â±2.52 20.19Â±0.36 18 .46Â±0.62 16.96Â±0.10
Vote for Clinton 16.97Â±0.93 19.95Â±0.58 17.25Â±0.26 19.41Â±0.10
Wine Quality Red 17.27Â±2.25 20.43Â±0.31 17.04Â±0.16 19.72Â±0.05
Wine Quality White 15.81Â±1.21 20.54Â±0.29 16.70Â±0.14 19.37Â±0.10
Wine 20.51Â±0.35 20.53Â±0.13 18.12Â±0.30 20.55Â±0.03
Yacht Hydrodynamics 20.03Â±1.16 20.69Â±0.29 20.19Â±0.21 20.57Â±0.11
Table 10: Log-likelihood (mean Â±std)
ğ¶ğ‘ =2 ğ¶ğ‘ =4
Dataset
Top-down Bottom-up Top-down Bottom-up
Abalone 4.47Â±0.31 1.10Â±0.15 6.97Â±0.30 2.94Â±0.12
Ailer
ons 110.24Â±0.87 108.22Â±0.36 106.95Â±0.83 106.26Â±0.99
Airfoil Self-Noise âˆ’12.57Â±1.25âˆ’13.06Â±1.39âˆ’6.57Â±0.20âˆ’7.91Â±0.27
Breast âˆ’6.56Â±0.18âˆ’7.29Â±0.32âˆ’6.45Â±0.11âˆ’6.90Â±0.16
Computer Hardware âˆ’49.19Â±4.61âˆ’51.94Â±5.76âˆ’32.35Â±0.71âˆ’34.28Â±0.64
cpu_act âˆ’101 .13Â±1.49âˆ’103.49Â±1.03âˆ’99.91Â±0.13âˆ’97.79Â±0.20
cpu_small âˆ’85.03Â±0.76âˆ’87.06Â±2.03âˆ’82.52Â±0.62âˆ’82.09Â±0.32
Crx âˆ’49.87Â±11.20âˆ’52.13Â±13.01âˆ’30.33Â±2.23âˆ’30.08Â±2.18
Dermatology 21.87Â±2.99 22.26Â±1.57 25.92Â±0.55 46.70Â±0.26
elevators 29.07Â±0.15 29.02Â±0.14 29.34Â±0.16 29.14Â±0.19
Forest Fires âˆ’30.81Â±0.49âˆ’32.24Â±0.60âˆ’27.46Â±0.25âˆ’28.45Â±0.15
German âˆ’17.52Â±0.45âˆ’19.00Â±0.32âˆ’15.33Â±0.60âˆ’14.08Â±0.23
Housing âˆ’24.07Â±1.33âˆ’27.02Â±0.95âˆ’18.40Â±0.94âˆ’20.80Â±0.21
Hybrid Price âˆ’22.58Â±0.42âˆ’22.74Â±0.29âˆ’21.42Â±0.36âˆ’22.61Â±0.19
kin8nm âˆ’10.23Â±0.10âˆ’10.26Â±0.07âˆ’10.10Â±0.04âˆ’10.13Â±0.02
LPGA2008 âˆ’15.68Â±0.42âˆ’15.93Â±0.43âˆ’14.68Â±0.20âˆ’14.84Â±0.09
LPGA2009 âˆ’28.29Â±0.40âˆ’29.63Â±0.21âˆ’27.39Â±0.44âˆ’29.72Â±0.20
Parkinsons Telemonitoring (motor) 43.85Â±0.65 41.94Â±0.25 43.50Â±0.56 42.08Â±0.06
Parkinsons Telemonitoring (total) 43.37Â±0.57 41.68Â±0.13 42.82Â±0.83 41.53Â±0.07
Vote for Clinton âˆ’49.61Â±0.14âˆ’49.84Â±0.03âˆ’49.77Â±0.09âˆ’49.89Â±0.05
Wine Quality Red âˆ’3.62Â±0.30âˆ’4.35Â±0.37âˆ’2.73Â±0.28âˆ’2.95Â±0.11
Wine Quality White âˆ’2.96Â±0.57âˆ’3.59Â±0.56âˆ’2.57Â±0.14âˆ’2.61Â±0.10
Wine âˆ’19.90Â±0.63âˆ’22.44Â±0.46âˆ’18.41Â±0.44âˆ’20.58Â±0.14
Yacht Hydrodynamics 2.22Â±1.83 1.06Â±1.85 8.25Â±0.64 4.98Â±0.15
3987KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Soma Yokoi and Issei Sato
Figure 5: Comparison of the temporal evolution of predictive performance between top-down Gibbs sampling, bottom-up
Gibbs sampling, and collapsed variational Bayesian method. The average test-set log-likelihood over 10trials is shown with the
lines, and the standard deviation is indicated by the shaded regions.
3988