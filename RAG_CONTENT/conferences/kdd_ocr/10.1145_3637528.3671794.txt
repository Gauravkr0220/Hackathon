Improved Active Covering via
Density-Based Space Transformation
MohammadHossein Bateni
Google Research
New York City, New York, USA
bateni@google.comHossein Esfandiari
Google Research
London, UK
esfandiari@google.com
Samira HosseinGhorban
Institute for Research in Fundamental Sciences
School of Computer Science
Tehran, Iran
s.hosseinghorban@ipm.irAlipasha Montaseri
Sharif University of Technology
Tehran, Iran
apmontaseri@ce.sharif.edu
ABSTRACT
In this work, we study active covering, a variant of the active-
learning problem that involves labeling (or identifying) all of the
examples with a positive label. We propose a couple of algorithms,
namely Density-Adjusted Non-Adaptive (DANA) learner andDensity-
Adjusted Adaptive (DAA) learner, that query the labels according to
a distance function that is adjusted by the density function. Under
mild assumptions, we prove that our algorithms discover all of the
positive labels while querying only a sublinear number of examples
from the support of negative labels for constant-dimensional spaces
(see Theorems 5 and 6). Our experiments show that our champion
algorithm DAA consistently improves over the prior work on some
standard benchmark datasets, including those used by the previous
work, as well as a couple of data sets on credit card fraud. For
instance, when measuring performance using AUC, our algorithm
is the best in 25out of 27experiments over 7different datasets.
CCS CONCEPTS
â€¢Computing methodologies â†’Active learning settings; On-
line learning settings ;Batch learning ;â€¢Theory of computation
â†’Sketching and sampling.
KEYWORDS
Active Covering, Active Learning, Crowdsourcing.
ACM Reference Format:
MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban,
and Alipasha Montaseri. 2024. Improved Active Covering via Density-Based
Space Transformation. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671794
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36717941 INTRODUCTION
It is widely recognized that data collection often involves costs. One
example is when we are using crowdsourcing to collect data for
machine learning tasks such as spam or fraud detection. This fact
motivates a class of machine-learning tasks called active learning.
In an active-learning task, we intend to indicate a limited set of
valuable examples (iteratively, or in batches) and probe their labels
in order to learn a model. Active covering is a variant of active
learning, in the context of binary classification, where the set of
examples with the positive label is considered valuable, hence we
intend to probe all such examples. One instance of active covering
is disease testing in a new pandemic such as Covid-19. In this case,
we desire to identify and test all patients with Covid-19, while we
do not like to waste many tests on patients without Covid-19.
Previously, Jiang and Rostamizadeh [ 17] studied the active-
covering problem and proposed and analyzed a couple of algorithms
called offline learner and active learner. Both algorithms are based
on first sampling a set of examples to query and then querying
the unlabeled examples close to the examples with a positive label.
The only distinction between the two algorithms is that the offline
learner exclusively considers the examples with a positive label in
the initial samples and queries nearby examples whereas the active
learner considers all queried examples with a positive label. They
show that under the following four assumptions, their algorithms
label all of the positive examples while querying only a sublinear
number of examples from the support of the negative examples.
First, a fixed lower bound on the density function of the positive
labels in the support of the positive labels is assumed. Second, for
every small ball around a point with a positive label, a constant
fraction of the volume of the ball falls inside the support of the
positive examples. Third, there is an upper bound on the density of
the negative examples. Moreover, in their theorem statements, they
use a parameter ğ¶which actually depends linearly on the surface
area of the support of the positive examples. Hence, by hiding ğ¶in
theğ‘‚(.)notation, they implicitly assume that the surface area of
the support of the positive examples is bounded by a constant.
It is evident that both the offline learner and active learner al-
gorithms are highly sensitive to the density of the distribution of
the examples. For instance, in a scenario where the probability of
observing positive labels divided by the probability of observing
negative labels is uniform across the space, both the offline learner
 
107
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
Figure 1: One feature Figure 2: Two features
Figure 3: A learner that is
not density adjusted.Figure 4: A density ad-
justed learner.
and active learner algorithms tend to query the areas with a higher
density at a substantially higher rate and subsequently move to
areas with a lower density, which is a counter-intuitive behavior.
This is why it is necessary for these algorithms to have a global up-
per bound on the probability distribution of the negative examples
and a global lower bound on the probability distribution of positive
labels in the support of positive labels. Such global constraints on
the probability distributions are not very desirable since in many
instances we observe several colonies of points in some dense areas
and some large and sparse pools of points around them.
Another issue is that the previous assumptions are very sensitive
to feature selection, and adding one unnecessary or mildly relevant
feature can break the assumptions. For example, Figure 1 is a very
well-formed example and respects the assumptions of the previous
work. However, adding one relevant but unnecessary feature turns
it into Figure 2 which has very sparse areas with positive examples
and no longer respects the assumptions of the previous work.
To address the aforementioned issues, we implicitly utilize a
space transformation that expands the regions with high density in
the probability distribution of the examples and shrinks the regions
with low density, with the aim of achieving a more uniform prob-
ability distribution. This transformation allows us to remove the
global upper and lower bounds on the probability distributions and
instead use a local constraint that avoids a sudden change in the
probability distribution in a small ball. This allows the probability
distribution to smoothly change from one place in the dataset to
another and hence accept inputs such as Figure 2. We adopt the
offline learner and active learner in this transformed space. We
call our algorithms Density-Adjusted Non-Adaptive (DANA) learner
andDensity-Adjusted Adaptive (DAA) learner, respectively. We ex-
tend the approach of Jiang and Rostamizadeh to prove that our
algorithms label all of the positive examples while querying only a
sublinear number of examples with negative labels.
We conduct experimental comparisons between our algorithms,
the offline learner, the active learner, and the uniform sampler. Re-
markably, in almost all of our experiments, our proposed algorithm
DAA consistently emerges as the best algorithm in terms of label-
ing cost. For example, when we measure the performance via the
AUC, our algorithm achieves the best performance in 25out of 27
experiments over 7different datasets.Figures 3 and 4 give some intuitions on why our density-adjusted
algorithms work better than the previous Euclidean-based algo-
rithms. Note that, both Figures 3 and 4 represent the same instance.
In this instance, there are two discs of positive points inside a pool
of negative points. However, the density of the left side is higher
than the right side. A Euclidean-based algorithm probes some discs
of the same radius to find all of the positive points, regardless of
the density function (see Figure 3). However, our density-adjusted
algorithms probe the labels more cautiously in the denser areas
via smaller discs (see Figure 4). Hence, a fewer number of negative
points fall in the discs that a density-adjusted algorithm probes.
1.1 Problem Setting
In this section, we define the active-covering problem and our
setting. There exists an unknown probability density function ğ‘“:
Rğ·â†’Rrepresenting examples with binary labels. We use XâŠ†Rğ·
to refer to the support of ğ‘“, i.e., the set of all possible examples. We
refer to the examples with label 1as positive examples and to the
examples with label 0as negative examples. Accordingly, we have
unknown density functions ğ‘“+:Rğ·â†’Rwith supportX+and
ğ‘“âˆ’:Rğ·â†’Rwith supportXâˆ’, corresponding to the positive and
negative examples respectively. We denote by P,P+andPâˆ’the
probability distributions corresponding to ğ‘“,ğ‘“+andğ‘“âˆ’, respectively.
Hence for a set ğ´âŠ†Rğ·, for example we have P(ğ´)=âˆ«
ğ´ğ‘“ ğ‘‘ğ´.
For an arbitrary set ğ´âŠ†X, by the law of total probability [ 27,
Page. 9] the probability of ğ´is equal to
P(ğ´)=P(X+)P(ğ´|X+)+P(Xâˆ’)P(ğ´|Xâˆ’).
Letğ‘:=P(X+), then we haveP(Xâˆ’)=1âˆ’ğ‘. Thus, for any ğ´âŠ†X
we have
P(ğ´)=ğ‘P+(ğ´)+(1âˆ’ğ‘)Pâˆ’(ğ´).
We receive a set of unlabeled examples ğ‘‹âŠ† X , with sizeğ‘›
drawn i.i.d. from probability distribution P. We are allowed to
query the label of the examples in X. The goal is to discover all
positive examples while minimizing the number of queries. We use
ğ‘‹+andğ‘‹âˆ’to refer to the true set of positive and negative examples
inğ‘‹, respectively.
Similar to the prior work [ 17], we compare our algorithms to
the optimal performance achievable by an algorithm that knows
the support ofX+, which requires ğ‘›P(X+)queries in expectations.
Note that, if a point ğ‘¥belongs to the support of X+, even though
we may have some information about the probability that ğ‘¥has
a negative label, it still may have a positive label and hence any
algorithm is forced to query it. Hence, any algorithm requires to
make at least ğ‘›P(X+)queries. We refer to this as ğ‘„ğ‘‚ğ‘ƒğ‘‡.
Definition 1 (Excess Query Cost). We define the excess query
cost of an algorithm ğ´, denoted by ğ¶ğ´, to beğ‘„ğ´âˆ’ğ‘„ğ‘‚ğ‘ƒğ‘‡, whereğ‘„ğ´
is the number of queries made by algorithm ğ´to label all the positive
examples.
It is not difficult to observe that, without any structural assump-
tion on the relationship among the positive examples, querying al-
most all of the examples is necessary to retrieve the outlier positive
examples. Therefore, to avoid having to locate positive examples in
extremely narrow and sparse subspaces, some assumptions on the
distribution of the positive examples are necessary [10, 17, 32].
 
108Improved Active Covering via
Density-Based Space Transformation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Our assumptions here are similar to those of Jiang and Ros-
tamizadeh [ 17] in nature. However, there are some technical dif-
ferences that make our assumptions less restrictive. For example,
instead of global upper or lower bounds on the density function, we
require the change in the density function to be bounded in every
small ball. Another example is that when they refer to a small ball
they refer to a ball with a small radius, while we refer to a ball with
a small probability. However, roughly speaking, since they have a
lower bound on the density as well, a ball with a small probability
will have a small radius as well, but not vice versa.
The following assumption ensures that there are no outlier posi-
tive examples. In other words, wherever a positive example exists, a
nontrivial fraction of the examples surrounding it are also positive.
Assumption 2. The support of positive examples is a compact
subspace and a disjoint union of a finite number of connected com-
ponents1X+,1,...,X+,ğ‘. Moreover, there exist ğœ™0andğ¶+âˆˆ (0,1]
such that for any point ğ‘¥âˆˆX+and positive number ğœ–that satisfy
P(ğµ(ğ‘¥,ğœ–))â‰¤ğœ™0, we have
P+(ğµ(ğ‘¥,ğœ–))â‰¥ğ¶+P(ğµ(ğ‘¥,ğœ–)),
whereğµ(ğ‘¥,ğœ–)is a ball with radius ğœ–and centerğ‘¥with respect to the
Euclidean metric.
The next assumption says that the density function ğ‘“does not
significantly change in a small ball of its domain.
Assumption 3. Letğœ™0be the parameter set in Assumption 2. For a
pointğ‘¥âˆˆX, we defineğœ†ğ‘¥andğœ‡ğ‘¥to respectively be the minimum and
maximum of ğ‘“inğµ(ğ‘¥,ğ‘Ÿ), whereğ‘Ÿis chosen such thatP(ğµ(ğ‘¥,ğ‘Ÿ))=ğœ™0.
We assume thatğœ†ğ‘¥
ğœ‡ğ‘¥â‰¥ğ›¼, for a constant ğ›¼.
The following assumption ensures that the support of X+is not
an excessively narrow subspace scattered throughout the domain
ofXâˆ’.
Assumption 4. Letğœ™0be the parameter set in Assumption 2. There
exists a constant ğ¶X+>0such that for all ğœ™âˆˆ[0,ğœ™0],
Pâˆ’ 
â‹“ğ‘¥âˆˆX+ğµ 
ğ‘¥,ğ·âˆšï¸„
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·!!
â‰¤ğ·âˆšï¸
ğœ™ğ¶X+,
whereğ‘£ğ·is the volume of a unit ball in a ğ·dimensional space.
The previous work requires a similar assumption to Assump-
tion 4, that they do not explicitly mention. Specifically, they require
the surface area of the domain of positive examples to be bounded.
In fact, they use a parameter ğ¶in their excess query cost that "de-
pends onP". The parameter ğ¶actually depends linearly on the
surface area of the domain of positive examples and hence enforces
it to be bounded.
1.2 Our Contributions
As our first result, we bound the excess query cost of our density-
adjusted non-adaptive algorithm (i.e., Algorithm 1). This result is
presented in Section 2.
1A setAâŠ†Rğ·is connected if and only if for each pair of points ğ‘¥,ğ‘¥â€²âˆˆA , there is
a curve from ğ‘¥toğ‘¥â€², which completely lies in A.Theorem 5. Suppose that Assumptions 2, 3, and 4 hold. For some
ğ‘šâˆˆeÎ˜(ğ‘›ğ·/(ğ·+1)), we have
E[ğ¶DANA]â‰¤eğ‘‚
ğ‘›ğ·
ğ·+1
.
Next, we bound the excess query cost of our density-adjusted
adaptive algorithm (i.e., Algorithm 2). This result is presented in
Section 3.
Theorem 6. Suppose that Assumptions 2, 3 and 4 hold. For some
ğ‘šâˆˆËœÎ˜(ğ‘›(ğ·âˆ’1)/ğ·), we have
E[ğ¶DAA]â‰¤ Ëœğ‘‚(ğ‘›ğ·âˆ’1
ğ·).
Remark 7. Both our algorithms assume that we know the density
functionğ‘“(Â·)of our examples. In Section 4 we show how to estimate
the density function for our purpose.
The previous work relies on explicitly knowing the number of
positive examples and clarified that â€œItâ€™s worth noting that we may
not know when all of the positive examples are labeledâ€“ thus, in prac-
tice, we can terminate the algorithm when enough positives are found
depending on the task or when the labeling budget runs outâ€ [17].
We resolve this issue from the theoretical perspective and provide
a simple and asymptotically optimal termination condition that
applies both to our algorithms and the algorithms of the previous
work. This condition is provided in Section 5.
Finally, in section 6 we report our experimental study on datasets
used by the previous work, as well as Mini-Imagenet and a couple
of datasets on credit card fraud. We provide further experimental
details in the appendix. Our experiments show that our DAA al-
gorithm consistently improves over the previous work. Due to the
space limit, we provide some of the proofs in the appendix.
1.3 Preliminaries
Subsequently, we present some definitions and lemmas that we use
throughout the paper.
Definition 8. Letğ‘‘(,)denote the Euclidean distance. Let A,BâŠ†
Rğ·be non-empty sets and ğ‘¥âˆˆRğ·andğœ–>0.
(1)Anğœ–-ball with center ğ‘¥is defined as
ğµ(ğ‘¥,ğœ–)=n
ğ‘¦âˆˆRğ·|ğ‘‘(ğ‘¥,ğ‘¦)â‰¤ğœ–o
.
(2)Anğœ–-tubular neighborhood around the set Ais defined as
ğµ(A,ğœ–)=
ğ‘¥âˆˆRğ·inf
ğ‘¥â€²âˆˆAğ‘‘(ğ‘¥,ğ‘¥â€²)â‰¤ğœ–
.
(3)The distance ğ‘¥fromAis defined as
ğ‘‘(ğ‘¥,A)=min
ğ‘¦âˆˆAğ‘‘(ğ‘¥,ğ‘¦).
Definition 9. LetAâŠ† Rğ·,ğ‘†âŠ‚A andğœ™>0. The setAis
connected in the ğœ™-neighborhood graph of ğ‘†if and only if for each
ğ‘¥,ğ‘¥â€²âˆˆA, there is a path ğ‘¥1=ğ‘¥â†’ğ‘¥2â†’...â†’ğ‘¥â„“=ğ‘¥â€²where for
each 1<ğ‘—<â„“we haveğ‘¥ğ‘—âˆˆğ‘†andğ‘¥ğ‘—âˆˆğµ(ğ‘¥ğ‘—âˆ’1,ğ‘Ÿğ‘¥ğ‘—âˆ’1)whereğ‘Ÿğ‘¥ğ‘—âˆ’1
is selected such that P ğµ(ğ‘¥ğ‘—âˆ’1,ğ‘Ÿğ‘¥ğ‘—âˆ’1))=ğœ™.
We use Vol(ğµ)to refer to the volume of a multi-dimensional ball
ğµin Euclidean space. We use ğ‘ƒğ‘Ÿ(ğ¸)to indicate the probability of
an eventğ¸. We also use the following theorem from [ 6] to establish
an upper bound on the number of our queries.
 
109KDD â€™24, August 25â€“29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
Theorem 10. Letğ‘‹be a set of i.i.d samples with size ğ‘›drawn
from a distributionP. For 0<ğ›¿<1, there exists a universal constant
ğ¶0such that with probability at least 1âˆ’ğ›¿uniformly over all balls
ğµâŠ†Rğ·, we have
P(ğµ)â‰¥ğ¶0ğ·log2
ğ›¿logğ‘›
ğ‘›â‡’|ğµâˆ©ğ‘‹|>0.
1.4 Other Related Work
The early studies on active learning date back to the 1990s [ 8,9].
However, due to its significant role in machine learning tasks, there
is still a lot of interest in developing more practical and effective
active learning mechanisms [ 1,7,21,26,30].Active learning has
been used in several learning tasks such as image processing [ 12,20],
fight against COVID-19 [ 30], text classification [ 31] and speech
recognition [14]
Active covering as a variant of active learning appears in several
machine learning tasks. For example active covering in useful in
credit card fraud detection [ 2],computational drug discovery [ 29],
bank loan applications [ 22], moderate abusive content[ 28], fake
account detection in social network platforms [ 24], and, distances
and uncontrollable situations such as the COVID-19 pandemic [ 34].
Garnett et al. study active search in order to retrieve as many
positive labels as possible given a query budget [ 13]. Jiang et al. [ 18]
provide a more time-efficient algorithm for this problem. Active
search has also been formalized as a bandit problem in a few previ-
ous works [ 16,19]. The active covering problem considers a more
aggressive formulation compared to that of active search and at-
tempts to find all (or practically almost all) of the positive labels.
This is particularly important in sensitive situations such as credit
card fraud and providing tests during a pandemic.
Retrieving the positive examples is also known as learning under
one-sided feedback which was studied by Helmold et al. [ 15]. They
used the standard online model, where the learning algorithm tries
to minimize the failure. It is worth mentioning that active covering
is related to the main-stream research path including the online
learning tasks which are investigated widely [ 4,5,25] and the
classical set-cover problem [ 33]. Our techniques have a connection
to the support estimation literature [ 3,10,11,23,35], even though,
these works do not directly consider the active search problem.
2 DENSITY-ADJUSTED NON-ADAPTIVE
LEARNER
Algorithm 1 Density-adjusted Non-Adaptive Algorithm (DANA)
Input Datasetğ‘‹, initial sample size ğ‘šand density function ğ‘“.
1:Letğ‘‹0beğ‘šexamples sampled uniformly without replacement
fromğ‘‹.
2:Label query ğ‘‹0and letğ‘‹+,0be the positive examples.
3:Label query remaining examples ğ‘¥in ascending order of
ğ‘‘(ğ‘¥,ğ‘‹+,0)Â·ğ·âˆšï¸
ğ‘“(ğ‘¥)until all positive examples are labeled.
Initially, DANA learner uniformly samples a set with ğ‘špoints from
the datasetğ‘‹and queries their labels. Subsequently, it labels the
remaining examples of ğ‘‹in ascending order of their minimum
density-adjusted distance to the initially sampled positive examplesuntil all positive examples are retrieved. Now, we are ready to prove
Theorem 5.
Proof of Theorem 5. To prove, we show that for any 0<ğ›¿<1,
ifğ‘šâ‰¥max(
2 log2
ğ›¿
ğ‘2,2ğ¶0ğ·log2
ğ›¿log(ğ‘šğ‘/2)
ğ‘2ğ¶+ğ›¼ğœ™0)
, we have
E[ğ¶DANA]â‰¤( 1âˆ’ğ‘)ğ‘š+ğ‘›ğ¶log(2/ğ›¿)log(ğ‘ğ‘š/2)
ğ‘š1/ğ·
+ğ›¿ğ‘›,
whereğ¶=2(1âˆ’ğ‘)ğ¶X+ğ¶0ğ·
ğ¶+ğ›¼ğ‘21
ğ·.By settingğ‘š=ËœÎ˜(ğ‘›ğ·/(ğ·+1))and
ğ›¿=1
ğ‘›, we obtain
E[ğ¶DANA]â‰¤eğ‘‚
ğ‘›ğ·/(ğ·+1)
.
Now, to prove the above claim we define binary random variables
ğ‘Œ1,...,ğ‘Œğ‘što represent the labels of the examples that Algorithm 1
queries. Note that the probability of ğ‘Œğ‘–being 1isğ‘, i.e.,ğ‘ƒğ‘Ÿ(ğ‘Œğ‘–=
1)=ğ‘. Thus the expected number of positive examples in the initial
sampleğ‘‹0isE[Ãğ‘š
ğ‘–=1ğ‘Œğ‘–]=ğ‘šğ‘. Sinceğ‘Œ1,...,ğ‘Œğ‘šare independent
random variables, Hoeffding inequality gives
ğ‘ƒğ‘Ÿğ‘šâˆ‘ï¸
ğ‘–=1ğ‘Œğ‘–âˆ’ğ‘šğ‘â‰¤âˆ’ğ‘¡
â‰¤ğ‘’âˆ’2ğ‘¡2
ğ‘š,
for any arbitrary ğ‘¡>0. Equivalently, we have
ğ‘ƒğ‘Ÿğ‘šâˆ‘ï¸
ğ‘–=1ğ‘Œğ‘–âˆ’ğ‘šğ‘>âˆ’ğ‘¡
>1âˆ’ğ‘’âˆ’2ğ‘¡2
ğ‘š.
We setğ‘¡=âˆšï¸ƒ
ğ‘š
2log2
ğ›¿in the above to obtain the following lower
bound on the number of positive labels in ğ‘‹0, with probability at
least 1âˆ’ğ›¿
2:
ğ‘šâˆ‘ï¸
ğ‘–=1ğ‘Œğ‘–â‰¥ğ‘šğ‘âˆ’ğ‘¡=ğ‘š(ğ‘âˆ’ğ‘¡/ğ‘š)=ğ‘š 
ğ‘âˆ’âˆšï¸‚
1
2ğ‘šlog2
ğ›¿!
.
Applyingğ‘šâ‰¥2 log2
ğ›¿
ğ‘2gives usÃğ‘š
ğ‘–=1ğ‘Œğ‘–â‰¥ğ‘šğ‘/2with probabil-
ity1âˆ’ğ›¿
2. Defineğœ™=2ğ¶0ğ·log2
ğ›¿log(ğ‘šğ‘/2)
ğ‘2ğ¶+ğ›¼ğ‘š. Note that since
ğ‘šâ‰¥2ğ¶0ğ·log2
ğ›¿log(ğ‘šğ‘/2)
ğ‘2ğ¶+ğ›¼ğœ™0, we haveğœ™0â‰¥2ğ¶0ğ·log2
ğ›¿log(ğ‘šğ‘/2)
ğ‘2ğ¶+ğ›¼ğ‘š,
hence we have ğœ™â‰¤ğœ™0. Therefore, for each ğ‘¥âˆˆX+, the definition
ofğœ‡ğ‘¥impliesP(ğµ(ğ‘¥,ğ·âˆšï¸ƒ
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·))â‰¤ğœ™0. We claim that with proba-
bility 1âˆ’ğ›¿, for eachğ‘¥âˆˆX+, there exists a positive example in
ğµ(ğ‘¥,ğ·âˆšï¸ƒ
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·)that is queried in ğ‘‹0,+, i.e.,ğµ(ğ‘¥,ğ·âˆšï¸ƒ
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·)âˆ©ğ‘‹0,+â‰ âˆ….
To see this, let us calculate the probability mass of positive examples
in this ball:
ğ‘P+
ğµ(ğ‘¥,ğ·âˆšï¸„
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·)
â‰¥ğ‘ğ¶+P
ğµ(ğ‘¥,ğ·âˆšï¸„
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·)
â‰¥ğ‘ğ¶+ğœ†ğ‘¥Vol(ğµ(ğ‘¥,ğ·âˆšï¸„
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·))â‰¥ğ‘ğ¶+ğœ†ğ‘¥ğ‘£ğ·ğœ™
ğœ‡ğ‘¥ğ‘£ğ·â‰¥ğ‘ğ¶+ğ›¼ğœ™.
 
110Improved Active Covering via
Density-Based Space Transformation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Combined with the lower bound ğ‘šâ‰¥2ğ¶0ğ·log2
ğ›¿log(ğ‘šğ‘/2)
ğ‘2ğ¶+ğ›¼ğœ™, The-
orem (10) guarantees that one example from ğ‘‹0,+falls in this ball.
Hence, all positive examples are retrieved by Algorithm (1). Next,
we upper bound the excess query cost of this algorithm. The initial
sample setğ‘‹0contains(1âˆ’ğ‘)ğ‘šnegative examples in expectation.
Moreover, the number of negative examples in ğ‘‹\ğ‘‹0is
(ğ‘›âˆ’ğ‘š)(1âˆ’ğ‘)Pâˆ’ â‹“ğ‘¥âˆˆXâ€²,+ğµ(ğ‘¥,ğ·âˆšï¸„
ğœ™
ğœ‡ğ‘¥ğ‘£ğ·)
â‰¤(ğ‘›âˆ’ğ‘š)(1âˆ’ğ‘)ğ¶X+ğ·âˆšï¸
ğœ™
where the inequality follows from Assumptions 3 and 4. With prob-
abilityğ›¿our concentration bounds fail and in that case, the excess
query cost is at most ğ‘›otherwise our expected query cost is upper
bounded by
(1âˆ’ğ‘)ğ‘š+(ğ‘›âˆ’ğ‘š)(1âˆ’ğ‘)ğ¶X+ğ·âˆšï¸
ğœ™â‰¤
(1âˆ’ğ‘)ğ‘š+ğ‘›2(1âˆ’ğ‘)ğ¶X+ğ¶0ğ·
ğ¶+ğ›¼ğ‘21
ğ·log 2
ğ›¿log ğ‘šğ‘
2
ğ‘š1
ğ·.
Hence we have
E[ğ¶DANA]â‰¤ğ›¿ğ‘›+(1âˆ’ğ‘)ğ‘š
+ğ‘›2(1âˆ’ğ‘)ğ¶X+ğ¶0ğ·
ğ¶+ğ›¼ğ‘21
ğ·log 2
ğ›¿log ğ‘šğ‘
2
ğ‘š1
ğ·.â–¡
3 DENSITY-ADJUSTED ADAPTIVE LEARNER
Initially, DAA learner uniformly samples a set with ğ‘špoints from
the dataset ğ‘‹and queries their labels as same as DANA. Subse-
quently, it labels the remaining examples of ğ‘‹in ascending order
of their minimum density-adjusted distance to the current sample
points ofğ‘‹whose positive labels are revealed until all positive
examples are retrieved. In summary, the key difference between the
DANA and DAA is in ascending order of the remaining examples
inğ‘‹. In DANA, the ordering is done based on the initial positive
sample while in DAA it is based on the updated ones.
Algorithm 2 Density-adjusted Adaptive Algorithm (DAA)
Input Datasetğ‘‹, initial sample size ğ‘šand density function ğ‘“.
1:Letğ‘‹0beğ‘šexamples sampled uniformly without replacement
fromğ‘‹.
2:Label query ğ‘‹0and letğ‘‹+,0be the positive examples.
3:Initializeğ‘‹ğ‘â†ğ‘‹+,0andğ‘‹ğ‘â†ğ‘‹0.
4:while not all positive examples in ğ‘‹are labeled do
5: Label query ğ‘¥=arg minğ‘¥âˆˆğ‘‹\ğ‘‹ğ‘ğ‘‘(ğ‘¥,ğ‘‹ğ‘)Â·ğ·âˆšï¸
ğ‘“(ğ‘¥)
6: ifğ‘¥has a positive label then
7:ğ‘‹ğ‘â†ğ‘‹ğ‘âˆª{ğ‘¥}
8: end if
9:ğ‘‹ğ‘â†ğ‘‹ğ‘âˆª{ğ‘¥}
10:end while
In the next lemma, first, we derive some conditions on ğ‘što guar-
antee, with high probability, that at least one example is chosen
from each connected component of X+. Next, a bound on ğœ™is pro-
posed such thatX+,ğ‘–âˆ©ğ‘‹+is connected in the ğœ™-neighborhood graphofğ‘‹+w.h.p. This ensures that our algorithm probes all of the posi-
tive examples in ğ‘‹+via some paths through the ğœ™-neighborhood
graph.
Lemma 11. Let Assumptions 2 and 3 hold and 0<ğ›¿<1.
(1)Defineğ‘=min 1â‰¤ğ‘–â‰¤ğ‘P+(X+,ğ‘–). If
ğ‘šâ‰¥maxï£±ï£´ï£´ ï£²
ï£´ï£´ï£³2 log
2ğ‘
ğ›¿
ğ‘log
1
1âˆ’ğ‘,2 log
2
ğ›¿
ğ‘2ï£¼ï£´ï£´ ï£½
ï£´ï£´ï£¾,
then for all 1â‰¤ğ‘–â‰¤ğ‘, we haveX+,ğ‘–âˆ©ğ‘‹0â‰ âˆ…, with probability
at least 1âˆ’ğ›¿.
(2)Let
ğœ™=3ğ·ğ¶0ğ·log2Ã—3ğ·ğ‘›2
ğ›¿ğ›¼4ğœ™logğ‘›
ğ‘ğ¶+ğ›¼4ğ‘›,
whereğ‘›is chosen sufficiently large such that ğœ™â‰¤ğœ™0. Then for
all1â‰¤ğ‘–â‰¤ğ‘,X+,ğ‘–âˆ©ğ‘‹+is connected in the ğœ™-neighborhood
graph ofğ‘‹+, with probability at least 1âˆ’ğ›¿.
Proof. For the first part, we define binary random variables
ğ‘Œ1,...,ğ‘Œğ‘što be the labels of the examples that Algorithm 2 queries.
Note that the probability of ğ‘Œğ‘–being 1isğ‘, i.e.,ğ‘ƒğ‘Ÿ(ğ‘Œğ‘–=1)=ğ‘.
Thus the expected value ofÃğ‘š
ğ‘–=1ğ‘Œğ‘–isğ‘šğ‘which represents the
expected number of positive examples in the initial sample ğ‘‹0of
Algorithm (2). Since ğ‘Œ1,...,ğ‘Œğ‘šare independent random variables,
by Hoeffding inequality, we have
ğ‘ƒğ‘Ÿğ‘šâˆ‘ï¸
ğ‘–=1ğ‘Œğ‘–âˆ’ğ‘šğ‘â‰¤âˆ’ğ‘¡
â‰¤ğ‘’âˆ’2ğ‘¡2
ğ‘š
for any arbitrary ğ‘¡>0. Equivalently, we have
ğ‘ƒğ‘Ÿğ‘šâˆ‘ï¸
ğ‘–=1ğ‘Œğ‘–âˆ’ğ‘šğ‘>âˆ’ğ‘¡
>1âˆ’ğ‘’âˆ’2ğ‘¡2
ğ‘š.
Let us setğ‘¡=âˆšï¸ƒ
ğ‘š
2log2
ğ›¿. By applying this to the above inequality,
with probability at least 1âˆ’ğ›¿
2we can lower bound the number of
positive examples in the initial sample set ğ‘‹0as follows.
ğ‘šâˆ‘ï¸
ğ‘–=1ğ‘Œğ‘–â‰¥ğ‘šğ‘âˆ’ğ‘¡=ğ‘š(ğ‘âˆ’ğ‘¡
ğ‘š)=ğ‘š 
ğ‘âˆ’âˆšï¸‚
1
2ğ‘šlog2
ğ›¿!
.
Note that from the statement of the lemma we have ğ‘šâ‰¥2 log(2
ğ›¿)
ğ‘2.
Applying this to the above inequality gives us |ğ‘‹+|â‰¥ğ‘šğ‘
2with
probability 1âˆ’ğ›¿
2. Moreover, for each 1â‰¤ğ‘–â‰¤ğ‘, the probability that
none of these ğ‘šğ‘/2are inX+,ğ‘–is 1âˆ’P+(X+,ğ‘–)ğ‘šğ‘
2â‰¤(1âˆ’ğ‘)ğ‘šğ‘
2.
Letğ‘šâ‰¥2 log(2ğ‘
ğ›¿)
ğ‘log
1
1âˆ’ğ‘, then we have(1âˆ’ğ‘)ğ‘šğ‘
2â‰¤ğ›¿
2ğ‘.Hence, for each
ğ‘–we haveX+,ğ‘–âˆ©ğ‘‹0â‰ âˆ…with probability at least 1âˆ’ğ›¿/2ğ‘. Applying a
simple union bound for all ğ‘–gives usX+,ğ‘–âˆ©ğ‘‹0â‰ âˆ…with probability
at least 1âˆ’ğ›¿/2. This completes the proof of the first part.
Now, for the second part, pick two arbitrary points ğ‘¥,ğ‘¦âˆˆX+,ğ‘–
for someğ‘–âˆˆ{1,Â·Â·Â·,ğ‘}. SinceX+,ğ‘–is connected, there is a carve
betweenğ‘¥,ğ‘¦inX+,ğ‘–. We consider a sequence of points on this carve
 
111KDD â€™24, August 25â€“29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
namely,ğ‘¥=ğ‘¥1â†’ğ‘¥2â†’Â·Â·Â·â†’ğ‘¥â„“=ğ‘¦such that for every ğ‘¥ğ‘—, we
have
ğ‘¥ğ‘—+1âˆˆğµ
ğ‘¥ğ‘—,ğ‘Ÿğ‘¥ğ‘—ğ·âˆš
ğ›¼3
3
,
where radius ğ‘Ÿğ‘¥ğ‘—is set such thatP ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥ğ‘—)=ğœ™. Next, we use
this to show that, with high probability, there exists a path ğ‘¥=
ğ‘¥1â†’ğ‘¥â€²
1â†’ğ‘¥â€²
2â†’Â·Â·Â·â†’ğ‘¥â€²
â„“â†’ğ‘¥â„“=ğ‘¦inX+,ğ‘–âˆ©ğ‘‹+, where for
everyğ‘¥â€²
ğ‘—, we haveğ‘¥â€²
ğ‘—+1âˆˆğµ(ğ‘¥â€²
ğ‘—,ğ‘Ÿâ€²
ğ‘¥â€²
ğ‘—), where radius ğ‘Ÿâ€²
ğ‘¥â€²
ğ‘—is set such
thatP ğµ(ğ‘¥â€²
ğ‘—,ğ‘Ÿâ€²
ğ‘¥â€²
ğ‘—)=ğœ™. This means that ğ‘¥andğ‘¦are connected in
theğœ™-neighborhood graph of ğ‘‹+as desired. Next, we show that
for everyğ‘—the ballğµ
ğ‘¥ğ‘—,ğ‘Ÿğ‘¥ğ‘—ğ·âˆš
ğ›¼3
3
contains at least one positive
example in data set ğ‘‹, w.h.p. Recall that we have P ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥ğ‘—)=ğœ™
and hence we haveğ·âˆšï¸‚
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥ğ‘—â‰¤ğ‘Ÿğ‘¥ğ‘—. To prove our claim, we calculate
the probability mass of positive examples in
ğµ
ğ‘¥ğ‘—,ğ·âˆš
ğ›¼3
3ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥ğ‘—
.
By Assumption 2 we have
ğ‘P+
ğµ(ğ‘¥ğ‘—,ğ·âˆš
ğ›¼3
3ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥ğ‘—)
â‰¥ğ‘ğ¶+P
ğµ(ğ‘¥ğ‘—,ğ·âˆš
ğ›¼3
3ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥ğ‘—)
â‰¥ğ‘ğ¶+ğœ†ğ‘¥ğ‘—Vol(ğµ(ğ‘¥ğ‘—,ğ·âˆš
ğ›¼3
3ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥ğ‘—))
â‰¥ğ‘ğ¶+ğœ†ğ‘¥ğ‘—ğ›¼3ğœ™
3ğ·ğœ‡ğ‘¥ğ‘—â‰¥ğ‘ğ¶+ğ›¼4ğœ™
3ğ·
â‰¥ğ¶0ğ·log 2ğ‘›2
ğ›¿ğœ™logğ‘›
ğ‘›.
Thus, by Theorem (10), with probability at least 1âˆ’ğ›¿ğ›¼4ğœ™
3ğ·ğ‘›2, there
exists a point ğ‘¥â€²
ğ‘—âˆˆğµ(ğ‘¥ğ‘—,ğ·âˆš
ğ›¼3
3ğ·âˆšï¸‚
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥ğ‘—)âˆ©ğ‘‹+. Note that we have
ğ‘‘(ğ‘¥â€²
ğ‘—,ğ‘¥â€²
ğ‘—+1)â‰¤ğ‘‘(ğ‘¥â€²
ğ‘—,ğ‘¥ğ‘—)+ğ‘‘(ğ‘¥ğ‘—,ğ‘¥ğ‘—+1)+ğ‘‘(ğ‘¥ğ‘—+1,ğ‘¥â€²
ğ‘—+1)
â‰¤ğ·âˆš
ğ›¼3
3 ğ‘Ÿğ‘¥ğ‘—+ğ‘Ÿğ‘¥ğ‘—+ğ‘Ÿğ‘¥ğ‘—+1
â‰¤ğ·âˆš
ğ›¼3
3 ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ†ğ‘¥ğ‘—+ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ†ğ‘¥ğ‘—+ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ†ğ‘¥ğ‘—+1
â‰¤ğ·âˆš
ğ›¼3
3 ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ†ğ‘¥ğ‘—+ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ†ğ‘¥ğ‘—+ğ·âˆšï¸„
ğœ™
ğ›¼ğ‘£ğ·ğœ†ğ‘¥ğ‘—
â‰¤ğ·âˆš
ğ›¼3
3 ğ·âˆšï¸„
ğœ™
ğ›¼ğ‘£ğ·ğœ‡ğ‘¥ğ‘—+ğ·âˆšï¸„
ğœ™
ğ›¼ğ‘£ğ·ğœ‡ğ‘¥ğ‘—+ğ·âˆšï¸„
ğœ™
ğ›¼2ğ‘£ğ·ğœ‡ğ‘¥ğ‘—
â‰¤ğ·âˆš
ğ›¼3
33
ğ·âˆš
ğ›¼2ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥ğ‘—â‰¤ğ·âˆš
ğ›¼3
33
ğ·âˆš
ğ›¼2ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğ›¼ğœ‡ğ‘¥â€²
ğ‘—
â‰¤ğ·âˆš
ğ›¼3
33
ğ·âˆš
ğ›¼3ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥â€²
ğ‘—=ğ·âˆšï¸„
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥â€²
ğ‘—.Note that the probability of a ball with radiusğ·âˆšï¸‚
ğœ™
ğ‘£ğ·ğœ‡ğ‘¥â€²
ğ‘—aroundğ‘¥â€²
ğ‘—
is at mostğœ™as claimed. Therefore, for every ğ‘¥â€²
ğ‘—, we haveğ‘¥â€²
ğ‘—+1âˆˆ
ğµ(ğ‘¥â€²
ğ‘—,ğ‘Ÿâ€²
ğ‘¥â€²
ğ‘—), where radius ğ‘Ÿâ€²
ğ‘¥â€²
ğ‘—is set such thatP ğµ(ğ‘¥â€²
ğ‘—,ğ‘Ÿâ€²
ğ‘¥â€²
ğ‘—)=ğœ™. Now
note that each point in the space belongs to at most two of the
ballsğ‘¥1,ğ‘¥2,Â·Â·Â·,ğ‘¥â„“, and the probability of each of the balls isğ›¼4ğœ™
3ğ·,
hence we have â„“â‰¤2Ã—3ğ·
ğ›¼4ğœ™. By a union bound with probability at
least 1âˆ’2ğ›¿
ğ‘›2, we have sampled at least one positive example from
each of the balls ğ‘¥1,ğ‘¥2,Â·Â·Â·,ğ‘¥â„“. This means that the X+,ğ‘–âˆ©ğ‘‹+is
connected in the ğœ™-neighborhood graph of ğ‘‹+, with probability at
least 1âˆ’2ğ›¿
ğ‘›2. Note that there are at most ğ‘›
2possible choices for ğ‘¥
andğ‘¦. By a union bound all of the points in X+that belong to the
same connected component are connected int the ğœ™-neighborhood
graph, with probability at least 1âˆ’2ğ›¿
ğ‘›2 ğ‘›
2â‰¥1âˆ’ğ›¿as claimed.â–¡
Now, we are ready to prove Theorem 6.
Proof of Theorem 6. Defineğ‘=min 1â‰¤ğ‘–â‰¤ğ‘P+(X+,ğ‘–). Pick an
arbitrary 0<ğ›¿<1. By Lemma 11, all of the positive examples are
retrieved by Algorithm 2 with probability at least 1âˆ’2ğ›¿. If
ğ‘šâ‰¥maxï£±ï£´ï£´ ï£²
ï£´ï£´ï£³2 log
2ğ‘
ğ›¿
ğ‘log
1
1âˆ’ğ‘,2 log
2
ğ›¿
ğ‘2ï£¼ï£´ï£´ ï£½
ï£´ï£´ï£¾,
then the excess query cost of Algorithm 2 is upper bounded by
E[ğ¶DAA]â‰¤ğ‘š+ğ¶
log 2
ğ›¿log(ğ‘›)ğ‘›ğ·âˆ’11
ğ·
+2ğ›¿ğ‘›,
whereğ¶=
(1âˆ’ğ‘)ğ¶X+ğ¶0ğ·
ğ‘ğ¶+ğ›¼ğ‘£2
ğ·1
ğ·
. By setting ğ‘š=ËœÎ˜(ğ‘›(ğ·âˆ’1)/ğ·)and
ğ›¿=1
ğ‘›, we obtain
E[ğ¶DANA]âˆˆeğ‘‚
ğ‘›(ğ·âˆ’1)/ğ·
.
Note that the excepted number of negative examples in ğ‘‹0is
equal to(1âˆ’ğ‘)ğ‘š. Also, the number of negative examples in ğ‘‹\ğ‘‹0
is
(ğ‘›âˆ’ğ‘š)(1âˆ’ğ‘)Pâˆ’ 
â‹“ğ‘¥âˆˆX+ğµ(ğ‘¥,ğ·âˆšï¸„
ğœ™
ğœ‡ğ‘¥)!
.
Thus, by Assumption 4, we have
E[ğ¶DAA]â‰¤( 1âˆ’ğ‘)ğ‘š+(ğ‘›âˆ’ğ‘š)(1âˆ’ğ‘)Pâˆ’ 
â‹“ğ‘¥âˆˆX+ğµ(ğ‘¥,ğ·âˆšï¸„
ğœ™
ğœ‡ğ‘¥)!
â‰¤(1âˆ’ğ‘)ğ‘š+(ğ‘›âˆ’ğ‘š)(1âˆ’ğ‘)ğ¶X+ğ·âˆšï¸„
ğœ™
ğ‘£ğ·
â‰¤(1âˆ’ğ‘)ğ‘š+ğ‘›(1âˆ’ğ‘)ğ¶X+ 
ğ¶0ğ·log2
ğ›¿logğ‘›
ğ‘ğ¶+ğ›¼ğ‘£2
ğ·ğ‘›!1
ğ·
â‰¤(1âˆ’ğ‘)ğ‘š+ğ‘› 
(1âˆ’ğ‘)ğ¶X+ğ¶0ğ·
ğ‘ğ¶+ğ›¼ğ‘£2
ğ·!1
ğ· 
log2
ğ›¿logğ‘›
ğ‘›!1
ğ·
.
 
112Improved Active Covering via
Density-Based Space Transformation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Letğ¶=
(1âˆ’ğ‘)ğ¶X+ğ¶0ğ·
ğ‘ğ¶+ğ›¼ğ‘£2
ğ·1
ğ·
, then we have
E[ğ¶DAA]â‰¤ğ‘š+ğ¶
log 2
ğ›¿log(ğ‘›)ğ‘›ğ·âˆ’11
ğ·
.
With probability 2ğ›¿Lemma 11 fails and in that case, the excess
query cost is at most ğ‘›. Hence we have
E[ğ¶DAA]â‰¤ğ‘š+ğ¶
log 2
ğ›¿log(ğ‘›)ğ‘›ğ·âˆ’11
ğ·
+2ğ›¿ğ‘›,
as claimed. â–¡
4 DENSITY ESTIMATION
The algorithms presented in Section 3 assume that the density func-
tionğ‘“is known. However, such an assumption is often unrealistic,
and in practice this function is unknown. In this section, we use the
ğ‘˜-nearest neighbor ( ğ‘˜-NN) method to estimate the density function
ğ‘“with a function Ë†ğ‘“such that for all ğ‘¥âˆˆX, we haveğ‘“(ğ‘¥)
Ë†ğ‘“(ğ‘¥)âˆˆ[ğ›¼
2,3
2ğ›¼]
with probability at least 1âˆ’ğ›¿for an arbitrary small ğ›¿âˆˆ(0,1]. In
this section, we show that incorporating ğ‘˜-nearest neighbor only
affects the excess query cost by replacing ğ›¼withğ‘‚(ğ›¼2).
Pick an arbitrary ğ‘¥âˆˆğ‘‹. Next, we show how to estimate the
value of the density function at ğ‘¥such thatğ‘“(ğ‘¥)
Ë†ğ‘“(ğ‘¥)âˆˆ[ğ›¼
2,3
2ğ›¼]with
probability at least 1âˆ’ğ›¿
ğ‘›. This together with a simple union bound
gives us our desired approximate function Ë†ğ‘“(ğ‘¥)for allğ‘¥âˆˆğ‘‹with
probability at least 1âˆ’ğ›¿. Letğ‘˜=âˆšï¸ƒ
2ğ‘›log(ğ›¿
2ğ‘›)and letğ‘Ÿğ‘¥be the
distance form ğ‘¥to itsğ‘˜-th nearest neighbor. Note that we have
|ğµ(ğ‘¥,ğ‘Ÿğ‘¥)âˆ©ğ‘‹|=ğ‘˜. We define
Ë†ğ‘“(ğ‘¥)=ğ‘˜
ğ‘›1
ğ‘£ğ·ğ‘Ÿğ·ğ‘¥.
In the rest we upper and lower bound ğ‘Ÿğ‘¥and then make use of this
to upper and lower bound ğ‘“(ğ‘¥)viaË†ğ‘“(ğ‘¥).
We defineğ‘Ÿğ‘¥,ğ‘˜
2=ğ·âˆšï¸ƒ
ğ‘˜
2ğ‘›ğ›¼
ğ‘£ğ·ğ‘“(ğ‘¥). We have
P ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥,ğ‘˜
2)=âˆ«
ğ‘¦âˆˆğµ(ğ‘¥,ğ‘Ÿğ‘¥,ğ‘˜
2)ğ‘“(ğ‘¦)â‰¤âˆ«
ğ‘¦âˆˆğµ(ğ‘¥,ğ‘Ÿğ‘¥,ğ‘˜
2)ğ‘“(ğ‘¦)
ğ›¼
=ğ‘£ğ·ğ‘Ÿğ·
ğ‘¥,ğ‘˜
2ğ‘“(ğ‘¥)
ğ›¼=ğ‘£ğ·ğ‘˜
2ğ‘›ğ›¼
ğ‘£ğ·ğ‘“(ğ‘¥)ğ‘“(ğ‘¥)
ğ›¼=ğ‘˜
2ğ‘›.
Note that the definition of ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥,ğ‘˜
2)is independent of ğ‘‹. More-
over, the expected number of points from ğ‘‹that falls inğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥,ğ‘˜
2)
isğ‘˜
2. Hence by Hoeffding inequality, we have
ğ‘ƒğ‘Ÿ
|ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥,ğ‘˜
2)âˆ©ğ‘‹|â‰¥ğ‘˜
=ğ‘ƒğ‘Ÿ
|ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥,ğ‘˜
2)âˆ©ğ‘‹|âˆ’ğ‘˜
2â‰¥ğ‘˜
2
â‰¤expğ‘˜2
2ğ‘›
=exp 
2ğ‘›log(ğ›¿
2ğ‘›)
2ğ‘›!
=ğ›¿
2ğ‘›.
Similarly, we define ğ‘Ÿğ‘¥,3ğ‘˜
2=ğ·âˆšï¸ƒ
3ğ‘˜
2ğ‘›1
ğ›¼ğ‘£ğ·ğ‘“(ğ‘¥), and we have
P ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥,3ğ‘˜
2)â‰¥3ğ‘˜
2ğ‘›, andğ‘ƒğ‘Ÿ
|ğµ(ğ‘¥ğ‘—,ğ‘Ÿğ‘¥,3ğ‘˜
2)âˆ©ğ‘‹|â‰¤ğ‘˜
â‰¤ğ›¿
2ğ‘›.Therefore with probabilityğ›¿
2we haveğ‘Ÿğ‘¥,ğ‘˜
2â‰¤ğ‘Ÿğ‘¥â‰¤ğ‘Ÿğ‘¥,3ğ‘˜
2. This
means that
ğ·âˆšï¸„
ğ‘˜
2ğ‘›ğ›¼
ğ‘£ğ·ğ‘“(ğ‘¥)â‰¤ğ‘Ÿğ‘¥â‰¤ğ·âˆšï¸„
3ğ‘˜
2ğ‘›1
ğ›¼ğ‘£ğ·ğ‘“(ğ‘¥).
The first inequality gives usğ‘˜
2ğ‘›ğ›¼
ğ‘£ğ·ğ‘Ÿğ·ğ‘¥â‰¤ğ‘“(ğ‘¥)and the second one
gives usğ‘“(ğ‘¥)â‰¤3ğ‘˜
2ğ‘›1
ğ›¼ğ‘£ğ·ğ‘Ÿğ·ğ‘¥. These implyğ‘“(ğ‘¥)
Ë†ğ‘“(ğ‘¥)âˆˆ[ğ›¼
2,3
2ğ›¼]as claimed.
Note that, even though in our algorithms we are adjusting the
distance using ğ‘“(ğ‘¥), we are only using the fact that this adjusting
factor is, by Assumption 3, an estimation (i.e., ğ›¼to1
ğ›¼factor) of the
density of any point within a small ball of ğ‘¥. If we use Ë†ğ‘“(ğ‘¥)to factor
the estimation is withinğ›¼
2Ã—ğ›¼to3
2ğ›¼1
ğ›¼factor, then this change only
injects some constant factors of ğ›¼to the excess query cost.
5 TERMINATION CONDITION
In this subsection, we provide a simple but asymptotically optimal
termination condition for active covering algorithms when we are
not aware of the exact number of positive examples. Let ğ´ğ¿ğº be an
active learning algorithm (without a termination constraint) and
letğ¶ğ´ğ¿ğº be an upper bound on the expected excess query cost
ofğ´ğ¿ğº if it (hypothetically) stops immediately after querying the
last positive example. Note that, Theorems 5 and 6 provide upper
bounds on excess query costs of our algorithms DANA and DAA,
which can be used as ğ¶ğ´ğ¿ğº to plug into the following theorem. The
following theorem provides the termination condition. We provide
the proof of this theorem in appendix B
Theorem 12. Pick an arbitrarily small probability ğ›¿âˆˆ(0,0.33]
and letğ´ğ¿ğºâ€²be an algorithm that runs ğ´ğ¿ğº as defined above and
terminates as soon as it observesğ¶ğ´ğ¿ğº
ğ›¿+log(ğ‘›
ğ›¿ğ¶+)consecutive negative
labels.ğ´ğ¿ğºâ€²queries all of the positive examples and has an excess
query cost of at most ğ‘‚(ğ¶ğ´ğ¿ğº)+Ëœğ‘‚(1), with probability at least 1âˆ’3ğ›¿.
6 EXPERIMENTS
In this section, we compare our density-adjusted methods with
the methods presented in [ 17]. Given the superior performance of
the methods outlined in [ 17] compared to the other baseline ap-
proaches, it is reasonable to compare our algorithms against theirs.
The offline algorithm [ 17] selects an initial sample and queries the
other datapoints in ascending order of their distance to the positive
initial samples. The active algorithm [ 17] does the same but it also
considers positive samples retrieved after the initial queries. Our
density-adjusted algorithms query the remaining datapoints in the
order of their distance to the positive datapoints times their density.
In the experiments we do not have access to the function ğ‘“, so we
approximate the density of a datapoint by calculating the inverse
of its distance to its ğ‘˜â€™th nearest neighbor, where ğ‘˜is a hyperpa-
rameter of the algorithm. It is not hard to see that this converges
toğ·âˆšï¸
ğ‘“(ğ‘¥)as the number of samples grows. Due to the space limit,
the modified algorithms are available in the appendix (3 and 4).
Since calculating the exact ğ‘˜nearest neighbors is computationally
expensive, we use locality-sensitive hashing for approximating the
ğ‘˜nearest neighbors.
The effect of hyperparameter ğ‘˜:The density-adjusted algo-
rithms come with a hyperparameter ğ‘˜. We compare different values
 
113KDD â€™24, August 25â€“29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
ofğ‘˜âˆˆ{10,20,50,100}. The results presented in Figure 7 in the
appendix show that the choice of ğ‘˜does not noticeably affect the
performance of our algorithms.
Validating the assumptions on the datasets: We conduct
some experiments to validate our assumptions on the datasets.
For validating assumption 2, we will calculate the percentage of
positive points around each positive point by considering its 100
nearest points. The results are presented in table 4. For validating
assumption 3, we will compare the density of each point with its
100nearest points. The results are presented in table 3.
6.1 Experiment Setup
For each dataset, we use all the available data and their original
features for evaluating the results. Throughout the experiments,
we set the initial sample size to be a uniformly random sample
of1
60of the datapoints. We then run the experiments using each
class as the positive label and the rest of the classes as negative
labels (note that this turns the problem into a binary classification).
The only hyperparameter in our algorithm is ğ‘˜. In the course of
the experiments, we set ğ‘˜=50, and for visualization, we set the
batch size to1
60of the size of the datasets. Subsequently, we plot
the percentage of positives retrieved against the number of batches.
For each experiment, we run it 5times and average out the results.
The experiments were conducted on a machine equipped with
an Intel(R) Xeon(R) CPU running at 2.20 GHz. Running the exact
experiments on the full datasets requires huge computational power,
therefore we use locality-sensitive hashing as an approximation for
calculating the nearest neighbors.
Time Complexity Analysis. All parts of our algorithms are
linear (treating ğ‘˜as a constant), except for finding the nearest
neighbors where we use locality-sensitive hashing. Therefore our
algorithms have a time complexity of O(ğ‘›+ğ‘™)whereğ‘™is the time
complexity for finding the nearest neighbors of all the datapoints
with locality-sensitive hashing, which is practically linear and sub-
quadratic in theory.
6.2 Datasets
The experiments are tested on the following datasets.
UCI Letters, consisting of 20,000 datapoints with 16numerical
features and 26classes representing each letter. The dataset has been
obtained by generating letters using 20fonts, which were randomly
distorted to generate 20,000 unique images. Sixteen features have
been extracted as a result of pixel count and correlations.
MNIST, consisting of 70,000 28Ã—28grayscale images of handwritten
digits and 10classes each representing a digit. We use the pixel
intensities of the images as the features.
CIFAR10, consisting of 60,000 32Ã—32colorized images, and 10
classes representing an object in the image. Same as MNIST, we use
the pixel intensities of the images as the features.
Fashion MNIST, consisting of 70,000 28Ã—28grayscale images
each associated with a label from 10classes. Same as MNIST, we
use the pixel intensities of the images as the features.
Mini-Imagenet, consisting of 60,000 84Ã—84colorized images, and
100 classes organized using the WordNet hierarchy. Same as MNIST,
we use the pixel intensities of the images as the features.Dataset Label Offline Active DANA DAA
MNIST0 90.47 92.98 91.74 93.40
1 92.64 92.77 87.85 92.74
2 76.11 86.82 91.80 93.26
3 81.05 85.17 90.77 92.90
4 83.84 88.68 90.67 93.29
CIFAR100 65.85 72.76 64.64 78.45
1 46.09 52.02 74.53 75.85
2 66.26 68.29 58.98 70.06
3 50.12 53.36 62.82 66.75
4 70.48 71.53 61.43 72.10
UCIA 91.86 96.23 93.30 96.38
B 86.84 94.99 85.00 95.97
C 87.61 95.45 89.85 96.17
D 86.47 94.85 85.41 95.81
E 84.07 93.72 80.71 95.01
Fash. MNIST0 84.79 87.06 88.75 90.85
1 92.10 92.68 92.05 93.33
2 82.96 84.64 86.58 90.12
3 88.06 89.25 90.58 91.93
4 82.81 85.38 86.37 89.76
Mini-Imagenet0 57.02 64.32 72.33 76.42
1 68.67 71.05 58.48 67.74
2 60.83 65.53 65.60 71.03
3 75.44 81.22 60.22 81.28
4 74.52 84.50 81.71 87.99
C. C. Fraud 2013 + 46.96 71.40 93.98 95.19
C. C. Fraud 2023 + 47.22 85.49 94.74 94.84
Table 1: Area under the curve of each algorithm for 7different
datasets. Each experiment is performed 5times and averaged
across all runs. The highest values are bolded out.
Credit Card Fraud, consisting of transactions made by credit
cards by European card holders. It contains only numerical input
variables which are the result of a PCA transformation.
6.3 Evaluation Metrics
We plot the percentage of positives retrieved against the number
of batches to demonstrate the performance of each of the methods.
We calculate the area under the curve for each of the methods as
an evaluation metric (table 1). Note that as the domain of positive
datapoints is not well defined and the excess cost and the number of
positive datapoints add up to the total number of queried datapoints,
this is a reasonable evaluation metric. The percentage of datapoints
required for reaching 95%and98%of the positive datapoints is also
calculated as an alternative evaluation metric. To get more accurate
results, each experiment is performed 5times and averaged across
all runs. The results for the 95% and 98% metrics are briefly discussed
in the next subsection, however, due to the space limit, the detailed
results for 95% are deferred to the appendix (See Table 2) and the
detailed results for 98% are deferred to the full version.
 
114Improved Active Covering via
Density-Based Space Transformation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 5: The percentage of positive samples retrieved after each batch on image datasets for each of the algorithms is plotted.
Figure 6: The percentage of positive samples retrieved after
each batch on the Credit Card Fraud datasets for each of the
algorithms is plotted.
6.4 Results
In each case, the result is either competitive or the density-adjusted
adaptive algorithm outperforms the other algorithms. The plots are
shown in Figures 5 and 6.
1. MNIST. The density-adjusted adaptive algorithm outperforms
the other algorithms on 4out of 5tasks on all three metrics. It
outperforms 3of them by a large margin.
2. CIFAR10. The density-adjusted adaptive algorithm outperforms
the other algorithms on 4out of 5tasks on all three metrics, out-
performing 2of them by a large margin.
3. UCI. The active density algorithm outperforms the other algo-
rithms on all of the 5tasks for the area under the curve metric, but
the results are closely competitive.4. Fashion MNIST. The density-adjusted adaptive algorithm out-
performs the other algorithms on all of the tasks on all three metrics.
It outperforms 3of them by a large margin.
5. Mini-Imagenet. The density-adjusted adaptive algorithm out-
performs the other algorithms on 3out of 5tasks by a large margin
on all three metrics. It performs competitively on 1of the remaining
tasks and falls off on the last task.
6. Credit Card Fraud. The density-adjusted adaptive and non-
adaptive algorithms outperform the other algorithms on both
datasets by a large margin on all three metrics.
7 CONCLUSION
By considering the density function of the examples and adjusting
the distance function, we present two algorithms DANA and DAA
for active covering. Under some necessary conditions, we prove
that both our algorithms discover all of the positive labels with a
sublinear excess cost for constant-dimensional spaces. Moreover,
our experiments on the same set of datasets used by the previous
work show the superiority of our method.
We admit that our method avoids sudden changes in the density
function (By Assumption 3). It might be possible to avoid this as-
sumption by considering a distance function that takes the average
of the density function over a path between the two endpoints.
However, calculating this distance function may not be trivial in
practice. We leave this as an open problem for future work.
 
115KDD â€™24, August 25â€“29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
REFERENCES
[1]Umang Aggarwal, Adrian Popescu, and CÃ©line Hudelot. 2020. Active learning
for imbalanced datasets. In IEEE/CVF-WACV. 1428â€“1437.
[2]John O Awoyemi, Adebayo O Adetunmbi, and Samuel A Oluwadare. 2017. Credit
card fraud detection using machine learning techniques: A comparative analysis.
InICCNI. IEEE, 1â€“9.
[3]GÃ©rard Biau, BenoÃ®t Cadre, and Bruno Pelletier. 2008. Exact rates in density
support estimation. Journal of Multivariate Analysis 99, 10 (2008), 2185â€“2207.
[4]Avrim Blum. 1990. Learning boolean functions in an infinite attribute space. In
STOC. 64â€“72.
[5]Avrim Blum, Lisa Hellerstein, and Nick Littlestone. 1995. Learning in the presence
of finitely or infinitely many irrelevant attributes. J. Comput. System Sci. 50, 1
(1995), 32â€“40.
[6]Kamalika Chaudhuri and Sanjoy Dasgupta. 2010. Rates of convergence for the
cluster tree. NeurIPS 23 (2010).
[7]Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Ra-
jagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. 2021. Batch active learning
at scale. NeurIPS 34 (2021), 11933â€“11944.
[8]David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with
active learning. Machine learning 15 (1994), 201â€“221.
[9]David A Cohn, Zoubin Ghahramani, and Michael I Jordan. 1996. Active learning
with statistical models. Journal of artificial intelligence research 4 (1996), 129â€“145.
[10] Antonio Cuevas and Ricardo Fraiman. 1997. A plug-in approach to support
estimation. The Annals of Statistics (1997), 2300â€“2312.
[11] Luc Devroye and Gary L Wise. 1980. Detection of abnormal behavior via non-
parametric estimation of the support. SIDMA 38, 3 (1980), 480â€“488.
[12] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep bayesian active
learning with image data. In ICML. PMLR, 1183â€“1192.
[13] Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, and
Richard Mann. 2012. Bayesian optimal active search and surveying. arXiv
preprint arXiv:1206.6406 (2012).
[14] Dilek Hakkani-TÃ¼r, Giuseppe Riccardi, and Allen Gorin. 2002. Active learning
for automatic speech recognition. In ICASSP, Vol. 4. IEEE, IVâ€“3904.
[15] David P Helmbold, Nicholas Littlestone, and Philip M Long. 2000. Apple tasting.
Information and Computation 161, 2 (2000), 85â€“139.
[16] Lalit Jain and Kevin G Jamieson. 2019. A new perspective on pool-based active
classification and false-discovery control. NeurIPS 32 (2019).
[17] Heinrich Jiang and Afshin Rostamizadeh. 2021. Active Covering. In ICML. PMLR,
5013â€“5022.
[18] Shali Jiang, Roman Garnett, and Benjamin Moseley. 2019. Cost effective active
search. NeurIPS 32 (2019).
[19] Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, and Roman
Garnett. 2018. Efficient nonmyopic batch active search. NeurIPS 31 (2018).
[20] Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. 2009. Multi-class
active learning for image classification. In CVPR. IEEE, 2372â€“2379.
[21] Seyed Mehran Kazemi, Anton Tsitsulin, Hossein Esfandiari, MohammadHossein
Bateni, Deepak Ramachandran, Bryan Perozzi, and Vahab Mirrokni. 2022. Tack-
ling Provably Hard Representative Selection via Graph Neural Networks. arXiv
preprint arXiv:2205.10403 (2022).
[22] Amir E Khandani, Adlar J Kim, and Andrew W Lo. 2010. Consumer credit-risk
models via machine-learning algorithms. Journal of Banking & Finance 34, 11
(2010), 2767â€“2787.
[23] Alexander P Korostelev and Aleksandr Borisovich Tsybakov. 1993. Estimation
of the density support and its functionals. Problemy Peredachi Informatsii 29, 1
(1993), 3â€“18.
[24] Kang Li, Zhenyu Zhong, and Lakshmish Ramaswamy. 2008. Privacy-aware
collaborative spam filtering. IEEE Transactions on Parallel and Distributed systems
20, 5 (2008), 725â€“739.
[25] Wolfgang Maass. 1991. On-line learning with an oblivious environment and the
power of randomization. International Computer Science Institute.
[26] Venkata Vamsikrishna Meduri, Lucian Popa, Prithviraj Sen, and Mohamed Sarwat.
2020. A comprehensive benchmark framework for active learning methods in
entity matching. In SIGMOD. 1133â€“1147.
[27] Michael Mitzenmacher and Eli Upfal. 2017. Probability and computing: Random-
ization and probabilistic techniques in algorithms and data analysis. Cambridge
university press.
[28] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang.
2016. Abusive language detection in online user content. In WWW. 145â€“153.
[29] Si-sheng Ou-Yang, Jun-yan Lu, Xiang-qian Kong, Zhong-jie Liang, Cheng Luo,
and Hualiang Jiang. 2012. Computational drug discovery. Acta Pharmacologica
Sinica 33, 9 (2012), 1131â€“1140.
[30] KC Santosh. 2020. AI-driven tools for coronavirus outbreak: need of active
learning and cross-population train/test models on multitudinal/multimodal data.
Journal of medical systems 44 (2020), 1â€“5.
[31] Christopher SchrÃ¶der and Andreas Niekler. 2020. A survey of active learning for
text classification using deep neural networks. arXiv preprint arXiv:2008.07267
(2020).[32] Aarti Singh, Clayton Scott, and Robert Nowak. 2009. Adaptive hausdorff estima-
tion of density level sets. The Annals of Statistics 37, 5B (2009), 2760â€“2782.
[33] Petr SlavÃ­k. 1996. A tight analysis of the greedy algorithm for set cover. In STOC.
435â€“441.
[34] Hui Ru Tan, Wei Heng Chng, Christian Chonardo, Magdeline Tao Tao Ng, and
Fun Man Fung. 2020. How chemists achieve active learning online during the
COVID-19 pandemic: using the Community of Inquiry (CoI) framework to sup-
port remote teaching. Journal of Chemical Education 97, 9 (2020), 2512â€“2518.
[35] Puning Zhao and Lifeng Lai. 2022. Analysis of knn density estimation. IEEE
Transactions on Information Theory 68, 12 (2022), 7971â€“7995.
A ADDITIONAL EXPERIMENT DETAILS
Algorithm 3 Density-adjusted Non-Adaptive Algorithm (DANA)
without access to ğ‘“
Input Datasetğ‘‹, initial sample size ğ‘šand density parameter ğ‘˜.
1:Letğ‘Ÿğ‘¥be the distance of ğ‘¥to itsğ‘˜â€™th nearest neighbor in ğ‘‹.
2:Letğ‘‹0beğ‘šexamples sampled uniformly without replacement
fromğ‘‹.
3:Label query ğ‘‹0and letğ‘‹+,0be the positive examples.
4:Label query remaining examples in ascending order of
ğ‘‘(ğ‘¥,ğ‘‹+,0)
ğ‘Ÿğ‘¥until all positive examples are labeled.
Algorithm 4 Density-adjusted Adaptive Algorithm (DAA) without
access toğ‘“
Input Datasetğ‘‹, initial sample size ğ‘šand density parameter ğ‘˜.
1:Letğ‘Ÿğ‘¥be the distance of ğ‘¥to itsğ‘˜â€™th nearest neighbor in ğ‘‹.
2:Letğ‘‹0beğ‘šexamples sampled uniformly without replacement
fromğ‘‹.
3:Label query ğ‘‹0and letğ‘‹+,0be the positive examples.
4:Initializeğ‘‹ğ‘â†ğ‘‹+,0andğ‘‹ğ‘â†ğ‘‹0
5:while not all positive examples in ğ‘‹are labeled do
6: Label query ğ‘¥=arg minğ‘¥âˆˆğ‘‹\ğ‘‹ğ‘ğ‘‘(ğ‘¥,ğ‘‹ğ‘)
ğ‘Ÿğ‘¥
7: ifğ‘¥has a positive label then
8:ğ‘‹ğ‘â†ğ‘‹ğ‘âˆª{ğ‘¥}
9: end if
10:ğ‘‹ğ‘â†ğ‘‹ğ‘âˆª{ğ‘¥}
11:end while
Dataset 100% 99.9% 99.5% 99% 98% 95%
MINST 0.16 0.28 0.38 0.47 0.57 0.67
CIFAR10 0.30 0.43 0.48 0.50 0.53 0.58
UCI 0.19 0.41 0.53 0.57 0.62 0.69
Fashion MNIST 0.24 0.39 0.47 0.51 0.55 0.63
Mini-Imagenet 0.27 0.42 0.47 0.50 0.53 0.58
Table 3: Verifying Assumption 3: The density ratio between
each point and its 100nearest neighbors is computed. For each
threshold ğ‘¡âˆˆ100%,99.9%,99.5%,99%,98%,95%, the displayed
values represent the minimum ratio observed among at least
ğ‘¡percent of the computed ratios.
 
116Improved Active Covering via
Density-Based Space Transformation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 7: The percentage of positive samples retrieved after each batch, using the DAA algorithm for the first three labels with
different values of ğ‘˜is plotted.
Dataset Lab
elOffline A
ctive D
ANA D
AA
MNIST0 27.62 13.79 17.57 11.30
1 12.33 12.20 30.03 12.49
2 63.51 32.05 17.42 11.50
3 56.46 38.13 20.39 12.88
4 44.50 27.55 22.35 11.74
CIF
AR100 87.38 79.33 88.56 71.03
1 93.08 91.18 69.29 65.87
2 87.10 83.07 90.80 81.19
3 95.07 93.09 83.19 79.44
4 81.80 78.36 86.20 79.22
UCIA 38.82 8.25 23.99 6.55
B 46.12 13.08 51.46 8.90
C 48.49 16.13 38.01 8.54
D 51.06 14.22 51.29 9.35
E 54.52 20.21 63.48 17.32
Fashion0 53.03 40.56 26.18 20.20
1 16.75 15.01 17.18 11.34
2 48.09 43.44 31.54 25.34
3 35.47 32.30 20.80 18.11
4 51.07 41.78 32.71 25.20
Mini-Imagenet0 90.17 88.72 73.74 72.30
1 83.48 84.86 86.65 82.22
2 93.94 89.23 82.76 78.78
3 73.53 67.45 89.31 72.73
4 73.28 61.91 86.10 59.74
C.
C. Fraud 2013 + 99.56 87.84 26.05 20.63
C.
C. Fraud 2023 + 98.92 86.39 21.69 22.26
Table 2: The percentage of datapoints required, to obtain 95%
of the positives labels.Dataset #100% 99.9% 99.5% 99% 98% 95%
MNIST00.01 0.02 0.09 0.14 0.30 0.68
10.01 0.01 0.06 0.16 0.55 0.99
20.01 0.01 0.02 0.03 0.06 0.15
30.01 0.01 0.03 0.05 0.09 0.21
40.01 0.01 0.02 0.05 0.13 0.28
CIF
AR1000.01 0.01 0.02 0.02 0.03 0.05
10.01 0.01 0.01 0.01 0.01 0.01
20.01 0.02 0.03 0.04 0.06 0.08
30.01 0.01 0.01 0.02 0.02 0.03
40.01 0.01 0.02 0.03 0.04 0.06
UCIA0.10 0.10 0.11 0.12 0.14 0.18
B0.10 0.10 0.12 0.14 0.16 0.18
C0.03 0.03 0.07 0.08 0.11 0.17
D0.07 0.07 0.10 0.13 0.15 0.19
E0.06 0.06 0.11 0.13 0.15 0.20
Fash.
MNIST00.01 0.01 0.02 0.03 0.05 0.14
10.01 0.01 0.03 0.04 0.08 0.21
20.01 0.01 0.02 0.03 0.07 0.14
30.01 0.01 0.02 0.03 0.06 0.16
40.01 0.01 0.03 0.05 0.09 0.15
Mini-Imagenet00.01 0.01 0.01 0.01 0.01 0.01
10.01 0.01 0.01 0.01 0.01 0.01
20.01 0.01 0.01 0.01 0.01 0.01
30.01 0.01 0.01 0.01 0.01 0.01
40.01 0.01 0.01 0.01 0.01 0.01
Table 4: Verifying Assumption 2: The percentage of pos-
itive points within the 100nearest neighbors of each
positive point is computed. For each threshold ğ‘¡âˆˆ
{100%,99.9%,99.5%,99%,98%,95%}, the depicted values repre-
sent the proportion of positive points found within at least ğ‘¡
percent of the positive points.
 
117KDD â€™24, August 25â€“29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
B PROOF OF THEOREM 12
Proof. First, using a simple Markov inequality we show that
ğ´ğ¿ğºâ€²has an excess query cost of at most ğ‘‚(ğ¶ğ´ğ¿ğº)+Ëœğ‘‚(1)with
probability at least 1âˆ’ğ›¿, then we show that ğ´ğ¿ğºâ€²queries all of the
positive examples with probability at least 1âˆ’2ğ›¿.
Note that, when ğ´ğ¿ğº queries the last positive example, its the
expected excess query cost is ğ¶ğ´ğ¿ğº. Hence by Markov inequality,
whenğ´ğ¿ğº queries all of the positive examples its excess query cost
is at mostğ¶ğ´ğ¿ğº
ğ›¿with probability at least 1âˆ’ğ›¿. After that, ğ´ğ¿ğº
only queries negative examples. Hence ğ´ğ¿ğºâ€²stops after querying
at mostğ¶ğ´ğ¿ğº
ğ›¿+log(ğ‘›
ğ›¿)extra queries. Hence the excess query cost
ofğ´ğ¿ğºâ€²is at mostğ¶ğ´ğ¿ğº
ğ›¿+ğ¶ğ´ğ¿ğº
ğ›¿+log(ğ‘›
ğ›¿ğ¶+)âˆˆğ‘‚(ğ¶ğ´ğ¿ğº)+Ëœğ‘‚(1)as
claimed.
Next, we show that ğ´ğ¿ğºâ€²queries all of the positive examples
with probability at least 1âˆ’2ğ›¿. In order to show this we show
that the probability of observingğ¶ğ´ğ¿ğº
ğ›¿+log(ğ‘›
ğ›¿ğ¶+)consecutive
negative labels before the last positive example is at most 2ğ›¿. As we
mentioned above the probability that we query more thanğ¶ğ´ğ¿ğº
ğ›¿
examples out of the domain of the positive examples is at most
ğ›¿. Hence, with probability at least 1âˆ’ğ›¿log(ğ‘›
ğ›¿ğ¶+)out ofğ¶ğ´ğ¿ğº
ğ›¿+
log(ğ‘›
ğ›¿ğ¶+)consecutive negative labels are queried from the domain
of positive examples. In the rest, we look at the queries that have
been made from the domain of positive examples and bound the
probability that log(ğ‘›
ğ›¿ğ¶+)consecutive queries from the domain of
positive examples are negative, then we apply a union bound over
all such subsequences to calculate the probability of failure.
Note that each query from the domain of positive examples
is positive with probability at least ğ¶+. Hence the probability that
log(ğ‘›
ğ›¿ğ¶+)consecutive queries from the domain of positive examples
are negative is
(1âˆ’ğ¶+)log(ğ‘›
ğ›¿ğ¶+)=ğ‘’âˆ’log(ğ‘›
ğ›¿)=ğ›¿
ğ‘›.
There are at most ğ‘›such subsequences, and hence the probability
that we observe one such subsequence is at most ğ›¿.â–¡
CEMPIRICAL ESTIMATION OF ğœ†0ANDğœ†1[17]
In this section, we conduct some experiments to estimate the param-
etersğœ†0andğœ†1as defined by Jiang and Rostamizadeh [17] acrossdifferent datasets. The parameter ğœ†0represents the lower bound
ofğ‘“+(ğ‘¥)andğœ†1represents the upper bound of ğ‘“âˆ’(ğ‘¥). For a given
ğ‘˜, we empirically estimate ğ‘“(ğ‘¥)by dividing ğ‘˜by the volume of
the smallest ball containing the ğ‘˜nearest neighbors of ğ‘¥and mul-
tiplying this result by the volume of the smallest enclosing ball
divided by the total number of points in the dataset. Note that the
computed values are scaled by the volume of the smallest enclosing
ball. The results indicate that the parameters and their difference
are significantly large, showing that even scaling alone will not be
effective.
Dataset Label log10ğœ†0log10ğœ†1
MNIST0 127 680
1 90 475
2 105 693
3 110 693
4 92 690
CIFAR0 996 2828
1 1025 2768
2 989 2672
3 886 2851
4 948 2712
UCI0 3 14
1 5 15
2 4 15
3 5 15
4 4 14
Fashion0 126 746
1 80 713
2 182 744
3 195 746
4 152 742
Mini-Imagenet0 394 2341
1 411 2326
2 445 2418
3 765 2375
4 638 2394
Table 5: Approximate values of log10(ğœ†0)and log10(ğœ†1)across
different datasets for ğ‘˜=100.
 
118