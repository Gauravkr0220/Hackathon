Graph Mamba: Towards Learning on Graphs with State Space
Models
Ali Behrouzâˆ—
Cornell University
Ithaca, NY, USA
ab2947@Cornell.eduFarnoosh Hashemiâˆ—
Cornell University
Ithaca, NY, USA
sh2574@cornell.edu
Abstract
Graph Neural Networks (GNNs) have shown promising potential
in graph representation learning. The majority of GNNs define a
local message-passing mechanism, propagating information over
the graph by stacking multiple layers. These methods, however,
are known to suffer from two major limitations: over-squashing
and poor capturing of long-range dependencies. Recently, Graph
Transformers (GTs) emerged as a powerful alternative to Message-
Passing Neural Networks (MPNNs). GTs, however, have quadratic
computational cost, lack inductive biases on graph structures, and
rely on complex Positional Encodings (PE). In this paper, we show
that while Transformers, complex message-passing, and PE are
sufficient for good performance in practice, neither is necessary.
Motivated by the recent success of State Space Models (SSMs), we
present Graph Mamba Networks (GMNs), a framework for a new
class of GNNs based on selective SSMs. We discuss the new chal-
lenges when adapting SSMs to graph-structured data, and present
four required steps to design GMNs, where we choose (1) Neigh-
borhood Tokenization, (2) Token Ordering, (3) Architecture of SSM
Encoder, and (4) Local Encoding. We provide theoretical justifica-
tion for the power of GMNs, and experimentally show that GMNs
attain an outstanding performance in various benchmark datasets.
The code is available in this link.
CCS Concepts
â€¢Computing methodologies â†’Machine learning; â€¢Mathe-
matics of computing â†’Graph theory.
Keywords
Graph Representation Learning, State Space Models, Message Pass-
ing Neural Networks, Graph Transformers, Random Walks
ACM Reference Format:
Ali Behrouz and Farnoosh Hashemi. 2024. Graph Mamba: Towards Learning
on Graphs with State Space Models. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3672044
âˆ—Both authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36720441 Introduction
Recently, graph learning has become an important and popular
area of study due to its impressive results in a wide range of ap-
plications, like neuroscience [ 10], social networks [ 24], molecular
graphs [ 88], etc. In recent years, Message-Passing Neural Networks
(MPNNs), which iteratively aggregate neighborhood information
to learn the node/edge representations, have been the dominant
paradigm in machine learning on graphs [ 32,41,83,93]. They, how-
ever, have some inherent limitations, including over-squashing [ 19],
over-smoothing [ 71], and poor capturing of long-range dependen-
cies [ 22]. With the rise of Transformer architectures [ 82] and their
success in diverse applications such as natural language process-
ing [89] and computer vision [ 54], their graph adaptations, so-called
Graph Transformers (GTs), have gained popularity as the alterna-
tives of MPNNs [40, 67, 99].
Graph transformers have shown promising performance in various
graph tasks, and their variants have achieved top scores in several
graph learning benchmarks [ 22,37]. The superiority of GTs over
MPNNs is often explained by MPNNsâ€™ bias towards encoding local
structures [ 59], while a key underlying principle of GTs is to let
nodes attend to all other nodes through a global attention mecha-
nism [ 40,99], allowing direct modeling of long-range interactions.
Global attention, however, has weak inductive bias and typically
requires incorporating information about nodesâ€™ positions to cap-
ture the graph structure [ 40,67]. To this end, various positional and
structural encoding schemes based on spectral and graph features
have been introduced [40, 43, 48].
Despite the fact that GTs with proper positional encodings (PE)
are universal approximators and provably more powerful than any
Weisfeiler-Lehman isomorphism test (WL test) [ 43], their applicabil-
ity to large-scale graphs is hindered by their poor scalability. That is,
the standard global attention mechanism on a graph with ğ‘›nodes
incurs both time and memory complexity of O(ğ‘›2), quadratic in the
input size, making them infeasible on large graphs. To overcome the
high computational cost, inspired by linear attentions [ 100], sparse
attention mechanisms on graphs attracts attention [ 67,75]. For
example, Exphormer [ 75] suggests using expander graphs, global
connectors, and local neighborhoods as three patterns that can be
incorporated in GTs, resulting in a sparse and efficient attention.
Although sparse attentions partially overcome the memory cost
of global attentions, GTs based on these sparse attentions [ 67,75]
still might suffer from quadratic time complexity. That is, they re-
quire costly PE (e.g., Laplacian eigen-decomposition) and structural
encoding (SE) to achieve their best performance, which can take
O(ğ‘›2)to compute.
Another approach to improve GTsâ€™ high computational cost is to
use subgraph tokenization [ 8,16,35,44,104], where tokens are
 
119
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ali Behrouz and Farnoosh Hashemi
small subgraphs extracted with a pre-defined strategy. Typically,
these methods obtain the initial representations of the subgraph
tokens by passing them through an MPNN. Given ğ‘˜extracted sub-
graphs (tokens), the time complexity of these methods is O(ğ‘˜2),
which is more efficient than typical GTs with node tokenization.
Also, these methods often do not rely on complex PE/SE, as their
tokens (subgraphs) inherently carry inductive bias. These methods,
however, have two major drawbacks: (1) To achieve high expressive
power, given a node, they usually require at least a subgraph per
each remaining node [ 9,101], meaning that ğ‘˜âˆˆO(ğ‘›)and so the
time complexity is O(ğ‘›2). (2) Encoding subgraphs via MPNNs can
transmit all their challenges of over-smoothing and over-squashing,
limiting their applicability to heterophilic and long-range graphs.
Recently, Space State Models (SSMs), as an alternative of attention-
based sequence modeling architectures like Transformers have
gained increasing popularity due to their efficiency [ 63,102]. They,
however, do not achieve competitive performance with Transform-
ers due to their limits in input-dependent context compression in
sequence models, caused by their time-invariant transition mech-
anism. To this end, Gu and Dao [28] present Mamba, a selective
state space model that uses recurrent scans along with a selection
mechanism to control which part of the sequence can flow into
the hidden states. This selection can simply be interpreted as using
data-dependent state transition mechanism (See Â§2.4 for a detailed
discussion). Mambaâ€™s outstanding performance in language mod-
eling, outperforming Transformers of the same size and matching
Transformers twice its size, motivates several recent studies to
adapt its architecture for different data modalities [3, 53, 96, 107].
Mamba architecture is specifically designed for sequence data and
the complex non-causal nature of graphs makes directly applying
Mamba on graphs challenging. Further, natural attempts to re-
place Transformers with Mamba in existing GTs frameworks (e.g.,
GPS [ 67], TokenGT [ 40]) results in suboptimal performance in both
effectiveness and time efficiency (See Â§5 for evaluation and Â§3 for a
detailed discussion). The reason is, contrary to Transformers that
allows each node to interact with all the other nodes, Mamaba, due
to its recurrent nature, only incorporates information about previ-
ous tokens (nodes) in the sequence. This introduces new challenges
compared to GTs: (1) The new paradigm requires token ordering
that allows the model take advantage of the provided positional
information as much as possible. (2) The architecture design need
to be more robust to permutation than a pure sequential encoder
(e.g., Mamba). (3) While the quadratic time complexity of attentions
can dominate the cost of PE/SE in GTs, complex PE/SE (with O(ğ‘›2)
cost) can be a bottleneck for scaling Graph Mamba on large graphs.
Contributions. To address all the abovementioned limitations, we
present Graph Mamba Networks (GMNs), a new class of machine
learning on graphs based on state space models (Figure 1 shows the
schematic of the GMNs). In summary our contributions are:
â€¢Recipe for Graph Mamba Networks. We discuss new chal-
lenges of GMNs compared to GTs in architecture design and
motivate our recipe with four required and one optional steps
to design GMNs. In particular, its steps are (1) Tokenization, (2)
Token Ordering, (3) Local Encoding, (4) Bidirectional Selective
SSM Encoder and dispensable (5) PE and SE.â€¢An Efficient Tokenization for Bridging Frameworks. Lit-
erature lacks a common foundation about what constitutes a
good tokenization. Accordingly, architectures are required to
choose either node- or subgraph-level tokenization, while each
of which has its own (dis)advantages, depending on the data.
We present a graph tokenization process that not only is fast
and efficient, but it also bridges the node- and subgraph-level
tokenization methods using a single parameter. Moreover, the
presented tokenization has implicit order, which is specially
important for sequential encoders like SSMs.
â€¢New Bidirectional SSMs for Graphs. Inspired by Mamba,
we design a SSM architecture that scans the input sequence
in two different directions, making the model more robust to
permutation, which is particularly important when we do not
use implicitly ordered tokenization on graphs.
â€¢Theoretical Justification. We provide theoretical justification
for the power of GMNs and show that they are universal ap-
proximator of any functions on graphs. We further show that
GMNs using proper PE/SE is more expressive than any WL test,
matching GTs in this manner.
â€¢Outstanding Performance and New Insights. Our experi-
mental evaluations demonstrate that GMNs attain an outstand-
ing performance in long-range, small-scale, large-scale, and
heterophilic benchmark datasets, while consuming less GPU
memory. These results show that while Transformers, complex
message-passing, and SE/PE are sufficient for good performance
in practice, neither is necessary. We further perform ablation
study and validate the contribution of each architectural choice.
2 Related Work and Backgrounds
To situate GMNs in a broader context, we discuss four relevant
types of machine learning methods:
2.1 Message-Passing Neural Networks
Message-passing neural networks are a class of GNNs that iter-
atively aggregate local neighborhood information to learn the
node/edge representations [ 41]. MPNNs have been the dominant
paradigm in machine learning on graphs, and attracts much at-
tention, leading to various powerful architectures, e.g., GAT [ 83],
GCN [ 36,41], GatedGCN [ 15], GIN [ 94], etc. Simple MPNNs, how-
ever, are known to suffer from some major limitations including: (1)
limiting their expressivity to the 1-WL isomorphism test [ 94], (2)
over-smoothing [ 71], and (3) over-squashing [ 4,19]. Various meth-
ods have been developed to augment MPNNs and overcome such
issues, including higher-order GNNs [ 57,58], graph rewiring [ 6,32],
adaptive and cooperative GNNs [ 23,25], and using additional fea-
tures [60, 73].
2.2 Graph Transformers
With the rise of Transformer architectures [ 82] and their success
in diverse applications such as natural language processing [ 89]
and computer vision [ 54], their graph adaptations have gained
popularity as the alternatives of MPNNs [ 40,67,99]. Using a full
global attention, GTs consider each pair of nodes connected [ 99]
 
120Graph Mamba: Towards Learning on Graphs with State Space Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
TokenizationRandom Walk  For each node and !ğ‘š=1,â€¦,ğ‘š, we sample ğ‘€ walks with length !ğ‘š and consider their induced subgraph as a token. For each !ğ‘š, we repeat the process ğ‘  times.ğ’	=	ğŸ: Each node is an independent token.
s subgraphs    (!ğ‘š=1)â€¦â€¦â€¦s subgraphs   (!ğ‘š=2)s subgraphs   (!ğ‘š=ğ‘š)PE/SEPE/SE (1) Sum over the rows of non-diagonal elements of the random walk matrix. (2) Eigenvectors of the Laplacian. (3) Anonymous random walk encoding, i.e., counting the number of times a node appears at a certain position.Relative PE/SE  Using pair-wise difference of PE/SE as edge features.  ConcatenationLocal EncodingMPNNs  To vectorize each token one can use message-passing to incorporate local information.RWF  Given the walks that corresponds to a subgraph, one can use local identity relation of nodes to vectorize it.For each Token:Token Orderingğ’â‰¥ğŸ Tokens have implicit order due to hierarchical structure.ğ’=ğŸ  Sort nodes based on PRR/Degree.Implicit orderBidirectional MambaRobust to Permutation We scan the sequence of tokens in two directions.
ğœğœÃ—ğœÃ—+
PNAGated GCNGINEMPNNâŠ•â€¦ğ’=ğŸPRR/Degree/â€¦â€¦â€¦($ğ‘š=1)s
smalllargeâ€¦âŠ•
ğœSum/ConcatenationActivation FunctionLinear Layer1-d ConvolutionSelective SSMRequired StepOptional StepâŠ•Key PointsDomain Knowledge One can use domain knowledge (when adapting GMs to specific domain) or structural properties like Personalized PageRank or degree.Long Sequence Mamba shows performance improvement with longer sequences,  and so we use parameter  s to control the length of the subgraph sequence. Based on the dataset, one can tune ğ‘  to achieve better results.Optional PE/SE  When using subgraph tokens (i.e., !ğ‘šâ‰¥1), PE/SE is optional. That is, tokens have their own inductive bias, and do not need additional information about the graph structure. Features  When  node or edge features are available, one can concatenate them with the PE/SE, before the local encoding step. Allows switching between node and subgraph tokenization using a single parameter ğ‘š, making the choice of tokenization a tunable hyperparameter during training.($ğ‘š=ğ‘š)â€¦s
Figure 1: Schematic of the GMNs with four required and one optional steps: (1) Tokenization: the graph is mapped into a sequence of tokens
(ğ‘šâ‰¥1: subgraph and ğ‘š=0: node tokenization) (2) ( Optional Step ) PE/SE: inductive bias is added to the architecture using information about the
position of nodes and the strucutre of the graph. (3) Local Encoding: local structures around each node are encoded using a subgraph vectorization
mechanism. (4) Token Ordering: the sequence of tokens are ordered based on the context. ( Subgraph tokenization ( ğ‘šâ‰¥1) has implicit order
and does not need this step). (5) (Stack of) Bidirectional Mamba: it scans and selects relevant nodes or subgraphs to flow into the hidden states.
â€ In this figure, the last layer of bidirectional Mamba, which performs as a readout on all nodes, is omitted for simplicity.
and so are expected to overcome the problems of over-squashing
and over-smoothing in MPNNs [ 43]. GTs, however, have weak in-
ductive bias and needs proper positional/structural encoding to
learn the structure of the graph [43, 67]. To this end, various stud-
ies have focused on designing powerful positional and structural
encodings [43, 76, 85, 97]. Although these P
Sparse Attention. While GTs have shown outstanding perfor-
mance in different graph tasks on small-scale datasets (up to 10K
nodes), their quadratic computational cost, caused by their full
global attention, has limited their applicability to large-scale graphs
[67]. Motivated by linear attention mechanisms (e.g., BigBird [ 100]
and Performer [ 17]), which are designed to overcome the same scal-
ability issue of Transformers on long sequences, using sparse Trans-
formers in GT architectures has gained popularity [ 42,50,67,75,92].
The main idea of sparse GTs models is to restrict the attention pat-
tern, i.e., the pairs of nodes that can interact with each other. As an
example, Shirzad et al . [75] present Exphormer, the graph adaption
of BigBird that uses three sparse patterns of (1) expander graph at-
tention, (2) local attention among neighbors, and (3) global attention
by connecting virtual nodes to all non-virtual nodes.
Subgraph Tokenization. Another method to overcome GTsâ€™ high
computational cost is to use subgraph tokenization [ 8,16,35,104],
where tokens are small subgraphs extracted with a pre-defined
strategy. These subgraph tokenization strategies usually are ğ‘˜-hop
neighborhood (given a fixed ğ‘˜) [39,61,65], learnable sample ofneighborhood [ 103], ego-networks [ 104], hierarchical ğ‘˜-hop neigh-
borhoods [ 16], graph motifs [ 68], and graph partitions [ 35]. To
vectorize each token, subgraph-based GT methods typically rely
on MPNNs, making them vulnerable to over-smoothing and over-
squashing. Most of them also use a fixed neighborhood of each
node, missing the hierarchical structure of the graph. The only
exception is NAGphormer [ 16] that uses all ğ‘˜=1,...,ğ¾ -hop neigh-
borhoods of each node as its corresponding tokens. Although this
tokenization lets the model learn the hierarchical structure of the
graph, by increasing the hop of the neighborhood, its tokens become
exponentially larger, limiting its ability to scale to large graphs.
2.3 Alternatives of GTs
Recently, several SSM- or RNN-based graph learning methods are
designed as the effecient alternatives of GTs. GRED [ 20] is a re-
cent work that uses an RNN on the set of neighbors with distance
ğ‘˜=1,...,ğ¾ to a node of interest for the node classification task.
S4G [ 77] uses a similar approach as GRED but uses a structured SSM
as the encoder. Wang et al . [84] , in a paper concurrent to and inde-
pendent of ours, replace the attention block in GPS framework [ 67]
with a Mamba block [ 28]. They further use permutation of nodes
in training to mitigate the permutation variance of sequence en-
coders. GMNs, however, use bidirectional SSMs to mitigate the
causal nature of SSMs. They further use random walks to capture
the neighborhood structure around each node, which is provably
more powerful than the other sampling methods [20, 77].
 
121KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ali Behrouz and Farnoosh Hashemi
2.4 State Space Models
State Space Models (SSMs), a type of sequence models, are usually
known as linear time-invariant systems that map input sequence
ğ‘¥(ğ‘¡)âˆˆRğ¿to response sequence ğ‘¦(ğ‘¡)âˆˆRğ¿[5]. Specifically, SSMs
use a latent state â„(ğ‘¡)âˆˆRğ‘Ã—ğ¿, evolution parameter AâˆˆRğ‘Ã—ğ‘,
and projection parameters BâˆˆRğ‘Ã—1,CâˆˆR1Ã—ğ‘such that:
â„â€²(ğ‘¡)=Aâ„(ğ‘¡)+Bğ‘¥(ğ‘¡),
ğ‘¦(ğ‘¡)=Câ„(ğ‘¡). (1)
Due to the hardness of solving the above differential equation in
deep learning settings, discrete space state models [ 29,102] dis-
cretize the above system using a parameter ğš«:
â„ğ‘¡=Â¯Aâ„ğ‘¡âˆ’1+Â¯Bğ‘¥ğ‘¡,
ğ‘¦ğ‘¡=Câ„ğ‘¡, (2)
where
Â¯A=exp(ğš«A),
Â¯B=(ğš«A)âˆ’1(exp(ğš«Aâˆ’ğ¼)).ğš«B. (3)
Gu et al . [29] shows that discrete-time SSMs are equivalent to the
following convolution:
Â¯K=
Â¯CÂ¯B,Â¯CÂ¯AÂ¯B,..., Â¯CÂ¯Ağ¿âˆ’1Â¯B
,
ğ‘¦=ğ‘¥âˆ—Â¯K, (4)
and accordingly can be computed very efficiently. Structured state
space models (S4), another type of SSMs, are efficient alternatives
of attentions and have improved efficiency and scalability of SSMs
using reparameterization [ 26,30,63]. SSMs show promising per-
formance on timeseries data [ 79,102], healthcare domain [ 31], and
computer vision [ 31,62]. They, however, lack selection mecha-
nism, causing missing the context as discussed by Gu and Dao
[28]. Recently, Gu and Dao [28] introduce an efficient and pow-
erful selective structured state space architecture, called Mamba,
that uses recurrent scans along with a selection mechanism to con-
trol which part of the sequence can flow into the hidden states.
The selection mechanism of Mamba can be interpreted as using
data-dependent state transition mechanisms, i.e., making B,C,and
ğš«as function of input ğ‘¥ğ‘¡. Mambaâ€™s outstanding performance in
language modeling, outperforming Transformers of the same size
and matching Transformers twice its size, motivates several recent
studies to adapt its architecture for different data modalities and
tasks [11, 13, 51, 53, 55, 107].
3 Challenges & Motivations: Transformers vs
Mamba
Mamba architecture is specifically designed for sequence data and
the complex non-causal nature of graphs makes directly applying
Mamba on graphs challenging. Based on the common applicabil-
ity of Mamba and Transformers on tokenized sequential data, a
straightforward approach to adapt Mamba for graphs is to replace
Transformers with Mamba in GTs frameworks, e.g., TokenGT [ 40]
or GPS [ 67]. However, this approach might not fully take advantage
of selective SSMs due to ignoring some of their special traits. In this
section, we discuss new challenges for GMNs compared to GTs.Sequences vs 2-D Data. It is known that the self-attentive archi-
tecture corresponds to a family of permutation equivariant func-
tions [ 46,52]. That is, the attention mechanism in Transformers [ 82]
assumes a connection between each pair of tokens, regardless of
their positions in the sequence, making it permutation equivariant.
Accordingly, Transformers lack inductive bias and so properly po-
sitional encoding is crucial for their performance, whenever the
order of tokens matter [ 52,82]. On the other hand, Mamba is a
sequential encoder and scans tokens in a recurrent manner (po-
tentially less sensitive to positional encoding). Thus, it expects
causal data as an input, making it challenging to be adapted to 2-D
(e.g., images) [ 53] or complex graph-structured data. Accordingly,
while in graph adaption of Transformers mapping the graph into
a sequence of tokens along with a positional/structural encodings
were enough, sequential encoders, like SSMs, and more specifically
Mamba, require an ordering mechanism for tokens.
Although this sensitivity to the order of tokens makes the adaption
of SSMs to graphs challenging, it can be more powerful whenever
the order matters. For example, learning the hierarchical structures
in the neighborhood of each node ( ğ‘˜-hops forğ‘˜=1,...,ğ¾ ), which
is implicitly ordered, is crucial in different domains [ 49,105]. More-
over, it provides the opportunity to use domain knowledge when
the order matters [ 98]. In our proposed framework, we provide the
opportunity for both cases: (1) using domain knowledge or struc-
tural properties (e.g., Personalized PageRank [ 64]) when the order
matters, or (2) using implicitly ordered subgraphs (no ordering is
needed). Furthermore, our bidirectional encoder scans nodes in
two different directions, being capable of learning equivariance
functions on the input, whenever it is needed.
Long-range Sequence Modeling. In graph domain, the sequence
of tokens, either node, edge, or subgraph, can be counted as the con-
text. Unfortunately, Transformer architecture, and more specifically
GTs, are not scalable to long sequence. Furthermore, intuitively,
more context (i.e., longer sequence) should lead to better perfor-
mance; however, recently it has been empirically observed that
many sequence models do not improve with longer context in lan-
guage modeling [ 74]. Mamba, because of its selection mechanism,
can simply filter irrelevant information and also reset its state at any
time. Accordingly, its performance improves monotonically with
sequence length [ 28]. To this end, and to fully take advantage of
Mamba, one can map a graph or a node to long sequences, possibly
bags of various subgraphs. Not only the long sequence of tokens
can provide more context, but it also potentially can improve the
expressive power [14].
Scalability. Due to the complex nature of graph-structured data,
sequential encoders, including Transformers and Mamba, require
proper positional and structural encodings [ 40,67]. These PEs/SEs,
however, often have quadratic computational cost, which can be
computed once before training. Accordingly, due to the quadratic
time complexity of Transformers, computing these one-time PEs/SEs
was dominated and they have not been the bottleneck for training
GTs. GMNs, on the other hand, have linear computational cost (with
respect to both time and memory), and so constructing complex
PEs/SEs can be their bottleneck when training on very large graphs.
This bring a new challenge for GMNs, as they need to either (1) do
not use PEs/SEs, or (2) use their more efficient variants to fully take
 
122Graph Mamba: Towards Learning on Graphs with State Space Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
advantage of SSMs efficiency. Our architecture design make the use
of PE/SE optional and our empirical evaluation shows that GMNs
without PE/SE can achieve competitive performance compared to
methods with complex PEs/SEs.
Node or Subgraph? In addition to the above new challenges, there
is a lack of common foundation about what constitutes a good
tokenization, and what differentiates them, even in GT frameworks.
Existing methods use either node/edge [ 40,67,75], or subgraph
tokenization methods [ 16,35,104]. While methods with node tok-
enization are more capable of capturing long-range dependencies,
methods with subgraph tokens have more ability to learn local
neighborhoods, are less rely on PE/SE [ 16], and are more efficient
in practice. Our architecture design lets switching between node
and subgraph tokenization using a single parameter ğ‘š, making the
choice of tokenization a tunable hyperparameter during training.
4 Graph Mamba Networks
In this section, we provide our five-step recipe for powerful, flexible,
and scalable Graph Mamba Networks. Following the discussion
about the importance of each step, we present our architecture. The
overview of the GMN framework is illustrated in Figure 1.
Throughout this section, we let ğº=(ğ‘‰,ğ¸)be a graph, where
ğ‘‰={ğ‘£1,...,ğ‘£ğ‘›}is the set of nodes and ğ¸âŠ†ğ‘‰Ã—ğ‘‰is the set of edges.
We assume each node ğ‘£âˆˆğ‘‰has a feature vector x(0)
ğ‘£âˆˆX, where
XâˆˆRğ‘›Ã—ğ‘‘is the feature matrix describing the attribute information
of nodes and ğ‘‘is the dimension of feature vectors. Given ğ‘£âˆˆğ‘‰, we
letN(ğ‘£)={ğ‘¢|(ğ‘£,ğ‘¢)âˆˆğ¸}be the set of ğ‘£â€™s neighbors. Given a subset
of nodesğ‘†âŠ†ğ‘‰, we useğº[ğ‘†]to denote the induced subgraph
constructed by nodes in ğ‘†, and Xğ‘†to denote the feature matrix
describing the attribute information of nodes in ğ‘†.
4.1 Tokenization and Encoding
Tokenization, which is the process of mapping the graph into a
sequence of tokens, is an inseparable part of adapting sequen-
tial encoders to graphs. As discussed earlier, existing methods
use either node/edge [ 40,67,75], or subgraph tokenization meth-
ods [ 16,35,104], each of which has its own (dis)advantages. In
this part, we present a new simple but flexible and effective neigh-
borhood sampling for each node and discuss its advantages over
existing subgraph tokenization. The main and high-level idea of
our tokenization is to first, sample some subgraphs for each node
that can represent the nodeâ€™s neighborhood structure as well as its
local, and global positions in the graph. Then we vectorize (encode)
these subgraphs to obtain the node representations.
Neighborhood Sampling. Given a node ğ‘£âˆˆğ‘‰, and two integers
ğ‘š,ğ‘€â‰¥0, for each 0â‰¤Ë†ğ‘šâ‰¤ğ‘š, we sample ğ‘€random walks started
fromğ‘£with length Ë†ğ‘š. Letğ‘‡Ë†ğ‘š,ğ‘–(ğ‘£)forğ‘–=0,...,ğ‘€ be the set of
visited nodes in the ğ‘–-th walk. We define the token corresponds to
all walks with length Ë†ğ‘šas:
ğº[ğ‘‡Ë†ğ‘š(ğ‘£)]=ğº"ğ‘€Ã˜
ğ‘–=0ğ‘‡Ë†ğ‘š,ğ‘–(ğ‘£)#
, (5)which is the union of all walks with length Ë†ğ‘š. One can interpret
ğº[ğ‘‡Ë†ğ‘š(ğ‘£)]as the induced subgraph of a sample of Ë†ğ‘š-hop neigh-
borhood of node ğ‘£. At the end, for each node ğ‘£âˆˆğ‘‰we have the
sequence of ğº[ğ‘‡0(ğ‘£)],...,ğº[ğ‘‡ğ‘š(ğ‘£)]as its corresponding tokens.
Using random walks (with fixed length) or ğ‘˜-hop neighborhood of
a node as its representative tokens has been discussed in several re-
cent studies [ 16,20,103,104]. These methods, however, suffer from
a subset of these limitations: (1) they use a fixed-length random
walk [ 44], which misses the hierarchical structure of the nodeâ€™s
neighborhood. This is particularly important when the long-range
dependencies of nodes are important. (2) they use all nodes in all
ğ‘˜-hop neighborhoods [ 16,20], resulting in a trade-off between long-
range dependencies and over-smoothing or over-squashing prob-
lems. Furthermore, the ğ‘˜-hop neighborhood of a well-connected
node might be the whole graph, resulting in considering the graph
as a token of a node, which is inefficient. Our neighborhood sam-
pling approach addresses all these limitations. It sampled the fixed
number of random walks with different lengths for all nodes, cap-
turing hierarchical structure of the neighborhood while mitigat-
ing both inefficiency, caused by considering the entire graph, and
over-smoothing and over squashing, caused by large neighborhood
aggregation.
The More Subgraphs, The Longer Sequence. Empirical eval-
uations have shown that the performance of selective state space
models improves monotonically with sequence length [ 28]. Further-
more, their linear computational cost allow us to use more tokens,
providing them more context about graphâ€™s structure. Accordingly,
to fully take advantage of selective state space models, given an
integerğ‘ >0, we repeat the above neighborhood sampling process
forğ‘ times. Hence, for each node ğ‘£âˆˆğ‘‰we have a sequence of
ğº[ğ‘‡0(ğ‘£)],ğº[ğ‘‡1
1(ğ‘£)],...,ğº[ğ‘‡ğ‘ 
1(ğ‘£)]
|                         {z                         }
ğ‘ times,...,ğº[ğ‘‡1
ğ‘š(ğ‘£)],...,ğº[ğ‘‡ğ‘ 
ğ‘š(ğ‘£)]
|                          {z                          }
ğ‘ times
as its corresponding sequence of tokens. Here, we can see another
advantage of our proposed neighborhood sampling compared to
Chen et al . [16] , Ding et al . [20] . While in NAGphormer [ 16] the
sequence length of each node is limited by the diameter of the graph,
our method can produce a long sequence of diverse subgraphs.
Theorem 1. With large enough ğ‘€,ğ‘š, andğ‘ >0, GMNsâ€™ sampling is
strictly more expressive than ğ‘˜-hop neighborhood sampling.
Structural/Positional Encoding. To further augment our frame-
work, we consider an optional step, when we inject structural and
positional encodings to the initial features of nodes/edges. PE is
meant to provide information about the position of a given node
within the graph. Accordingly, two close nodes within a graph or
subgraph are supposed to have close PE. SE, on the other hand, is
meant to provide information about the structure of a subgraph.
Following RampÃ¡Å¡ek et al . [67] , we concatenate either eigenvectors
of the graph Laplacian or Random-walk structural encodings to the
nodesâ€™ feature, whenever PE/SE are needed: i.e.,
x(new)
ğ‘£ =xğ‘£||ğ‘ğ‘£, (6)
whereğ‘ğ‘£is theğ‘£â€™s corresponding positional encoding. For the sake
of consistency, we use xğ‘£instead of x(new)
ğ‘£ throughout the paper.
 
123KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ali Behrouz and Farnoosh Hashemi
Neighborhood Encoding. Given a node ğ‘£âˆˆğ‘‰and its sequence
of tokens (subgraphs), we encode the subgraph via encoder ğœ™(.).
That is, we construct x1ğ‘£,x2ğ‘£,...,xğ‘šğ‘ âˆ’1ğ‘£,xğ‘šğ‘ ğ‘£âˆˆRğ‘‘as follows:
x((ğ‘–âˆ’1)ğ‘ +ğ‘—)
ğ‘£ =ğœ™
ğº[ğ‘‡ğ‘—
ğ‘–(ğ‘£)],Xğ‘‡ğ‘—
ğ‘–(ğ‘£)
, (7)
where 1â‰¤ğ‘–â‰¤ğ‘šand1â‰¤ğ‘—â‰¤ğ‘ . In practice, this encoder can be
an MPNN, (e.g., Gated-GCN [ 15]), or RWF [ 81] that encodes nodes
with respect to a sampled set of walks into feature vectors with
four parts: (1) node features, (2) edge features along the walk, and
(3, 4) local structural information.
Token Ordering. By Equation 7, we can calculate the neighbor-
hood embeddings for various sampled neighborhoods of a node
and further construct a sequence to represent its neighborhood in-
formation, i.e., x1ğ‘£,x2ğ‘£,..., xğ‘šğ‘ âˆ’1ğ‘£,xğ‘šğ‘ ğ‘£. As discussed in Â§3, adaption
of sequence models like SSMs to graph-structured data requires
an order on the tokens. To understand what constitutes a good
ordering, we need to recall selection mechanism in Mamba [ 28] (we
will discuss selection mechanism more formally in Â§4.2). Mamba
by making B,C,andğš«as the function of input ğ‘¥ğ‘¡(see Â§2.4 for
notations) lets the model filter irrelevant information and select
important tokens in a causal manner, meaning that each token
is updated based on tokens that come before it in the sequence.
Accordingly, earlier tokens have less information about the context,
while later tokens have information about almost entire sequence.
This leads us to order tokens based on either their needs of knowing
information about other tokens or their importance to our task.
Whenğ‘šâ‰¥1:For the sake of simplicity first let ğ‘ =1. In the case
thatğ‘šâ‰¥1, interestingly, our architecture design provides us with
an implicitly ordered sequence. That is, given ğ‘£âˆˆğ‘‰, theğ‘–-th token
is a samples from ğ‘–-hop neighborhood of node ğ‘£, which is the
subgraph of all ğ‘—-hop neighborhoods, where ğ‘—â‰¥ğ‘–. This means,
given a large enough ğ‘€(number of sampled random walks), our
ğ‘‡ğ‘—(ğ‘£)has enough information about ğ‘‡ğ‘–(ğ‘£), not vice versa. To this
end, we use the reverse of initial order, i.e., xğ‘šğ‘£,xğ‘šâˆ’1ğ‘£,...,x2ğ‘£,x1ğ‘£.
Accordingly, inner subgraphs can also have information about the
global structure. When ğ‘ â‰¥2, we use the same procedure as above,
and reverse the initial order, i.e., xğ‘ ğ‘šğ‘£,xğ‘ ğ‘šâˆ’1ğ‘£,...,x2ğ‘£,x1ğ‘£. We will
discuss the ordering in the case of ğ‘š=0later.
4.2 Bidirectional Mamba
As discussed in Â§3, SSMs are recurrent models and require ordered
input, while graph-structured data does not have any order and
needs permutation equivariant encoders. To this end, inspired by
the success of bidirectional SSMs in NLP [ 86] and computer vi-
sion [ 107], we modify Mamba architecture and use two recurrent
scan modules to scan data in two different directions (i.e., forward
and backward). Accordingly, given two tokens ğ‘¡ğ‘–andğ‘¡ğ‘—, where
ğ‘–>ğ‘—and indices show their initial order, in forward scan ğ‘¡ğ‘–comes
afterğ‘¡ğ‘—and so has the information about ğ‘¡ğ‘—(which can be flown
into the hidden states or filtered by the selection mechanism). In
backward pass ğ‘¡ğ‘—comes after ğ‘¡ğ‘–and so has the information about
ğ‘¡ğ‘–. This architecture is particularly important when ğ‘š=0(node
tokenization), which we will discuss later.
More formally, in forward pass module, let ğš½be the input sequence
(e.g., given ğ‘£,ğš½is a matrix whose rows are xğ‘ ğ‘šğ‘£,xğ‘ ğ‘šâˆ’1ğ‘£,...,x1ğ‘£,
calculated in Equation 7), Abe the relative positional encoding oftokens, we have:
ğš½input=ğœ
Conv
Winput LayerNorm(ğš½)
,
B=WBğš½input,C=WCğš½input,ğš«=Softplus
WÎ”ğš½input
,
Â¯A=Discrete A(A,ğš«), Â¯B=Discrete B(B,ğš«),
ğ’š=SSM Â¯A,Â¯B,C
ğš½input
,
ğ’šforward =Wforward,1 ğ’šâŠ™ğœ Wforward,2LayerNorm(ğš½), (8)
where W,WB,WC,Wğš«,Wforward,1andWforward,2are learnable
parameters, ğœ(.)is nonlinear function (e.g., SiLU ),LayerNorm(.)is
layer normalization [ 7],SSM(.)is the state space model discussed
in Equations 2 and 4, and Discrete(.)is discretization process dis-
cussed in Equation 3. We use the same architecture as above for the
backward pass (with different weights) but instead we use ğš½inverse
as the input, which is a matrix whose rows are x1ğ‘£,x2ğ‘£,..., xğ‘ ğ‘šğ‘£. Let
ğ’šbackward be the output of this backward module, we obtain the
final encodings as the aggregation of ğ’šforward andğ’šbackward . Note
that due to our ordering mechanism, the last state of the output
corresponds to the walk with length Ë†ğ‘š=0, i.e., the node itself.
Accordingly, the last state represents the updated node encoding.
Augmentation with MPNNs. We further use an optional MPNN
module that simultaneously performs message-passing and aug-
ments the output of the bidirectional Mamba via its inductive bias.
Particularly this module is very helpful when there are rich edge
features and so an MPNN can help to take advantage of them. While
in our empirical evaluation we show that this module is not nec-
essary for the success of GMNs in several cases, it can be useful
when we avoid complex PE/SE and strong inductive bias is needed.
How Does Selection Work on Subgraphs? Due to the data-
dependency and descritization in Mamba [ 28], in recurrent scan,
based on the input, the model can filter the irrelevant context.
Therefore, as model scans the sampled subgraphs from the ğ‘–-hop
neighborhoods in descending order of ğ‘–, it filters irrelevant neigh-
borhoods to the context, which is the node encoding.
Last Layer(s) of Bidirectional Mamba. To capture the long-range
dependencies and to flow information across the nodes, we use the
node encodings obtained from the last state of Equation 8 as the
input of the last layer(s) of bidirectional Mamba. Therefore, the
recurrent scan of nodes (in both directions) can flow information
across nodes. This design not only helps capturing long-range
dependencies in the graph, but it also is a key to the flexibility of
our framework to bridge node and subgraph tokenization.
4.3 Tokenization When ğ‘š=0
In this case, for each node ğ‘£âˆˆğ‘‰we only consider ğ‘£itself as its
corresponding sequence of tokens. Based on our architecture, in this
case, the first layers of bidirection Mamba become simple projection
as the length of the sequence is one. However, the last layers, where
we use node encodings as their input, treats nodes as tokens and
become an architecture that use a sequential encoder (e.g., Mamba)
with node tokenization. More specifically, in this special case of
framework, the model is the adaption of GPS [ 67] framework, when
we replace its Transformer with our bidirectional Mamba.
 
124Graph Mamba: Towards Learning on Graphs with State Space Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
This architecture design lets switching between node and subgraph
tokenization using a single parameter ğ‘š, making the choice of tok-
enization a tunable hyperparameter during training. Note that this
flexibility comes more from our architecture rather than the method
of tokenization. That is, in practice one can use only 0-hop neigh-
borhood in NAGphormer [ 16], resulting in only considering the
node itself. However, in this case, the architecture of NAGphormer
becomes a stack of MLPs, resulting in poor performance.
Token Ordering. Whenğ‘š=0:One remaining question is how
one can order nodes when we use node tokenization. As discussed
in Â§4.1, tokens need to be ordered based on either (1) their needs of
knowing information about other tokens or (2) their importance
to our task. When dealing with nodes and specifically when long-
range dependencies matter, (1) becomes a must for all nodes. Our
architecture overcomes this challenge by its bidirectional scan pro-
cess. Therefore, we need to order nodes based on their importance.
There are several metrics to measure the importance of nodes in a
graph. For example, various centrality measures [ 45,70], degree,
ğ‘˜-core [ 34,47], Personalized PageRank or PageRank [ 64], etc. In
our experiments, for the sake of efficiency and simplicity, we sort
nodes based on their degree.
4.4 Theoretical Analysis of GMNs
In this section, we provide theoretical justification for the power of
GMNs. More specifically, we first show that GMNs are universal ap-
proximator of any function on graphs. Next, we discuss that given
proper PE and enough parameters, GMNs are more powerful than
any WL isomorphism test, matching GTs (with the similar assump-
tions). Finally, we evaluate the expressive power of GMNs when
they do not use any PE or MPNN and show that their expressive
power is unbounded (might be incomparable).
Theorem 2 (Universality). Let1â‰¤ğ‘<âˆ, andğœ–>0. For
any continues function ğ‘“:[0,1]ğ‘‘Ã—ğ‘›â†’Rğ‘‘Ã—ğ‘›that is permutation
equivariant, there exists a GMN with positional encoding, ğ‘”ğ‘, such
thatâ„“ğ‘(ğ‘“,ğ‘”)<ğœ–, whereâ„“ğ‘(.)is theğ‘-norm.
Theorem 3 (Expressive Power w/ PE). Given the full set of eigen-
functions and enough parameters, GMNs can distinguish any pair of
non-isomorphic graphs and are more powerful than any WL test.
We prove the above theorems based on the recent work of Wang and
Xue[87], where they prove that SSMs with layer-wise nonlinearity
are universal approximators of sequence-to-sequence functions.
Theorem 4 (Expressive Power w/o PE and MPNN). With enough
parameters, for every ğ‘˜â‰¥1there are graphs that are distinguishable
by GMNs, but not by ğ‘˜-WL test, showing that their expressive power
is not bounded by any WL test.
We prove this theorem directly based on the work of TÃ¶nshoff
et al. [81] , where they show a similar theorem for CRaWl. Notably,
this theorem does not rely on the Mambaâ€™s power, and the expres-
sive power comes from the choice of neighborhood sampling and
encoder.
Inspired by S4G [ 77], next theorem justify the robustness of GMNs
against over-squashing:
Theorem 5 (Sensitivity). GMNs has linear sensitivity. That is,
||ğœ•ğ’šğ‘¡
ğœ•ğ‘¥ğ‘˜||is linear, where ğ’šğ‘¡is the output and ğ‘¥ğ‘˜is the input.Table 1: Dataset Statistics.
Dataset
#Graphs Average #Nodes Average #Edges #ClassSetupMetric
Input
Level Task
Long-range
Graph Benchmark [22]
COCO-SP
123,286 476.9 2693.7 81 Node Classification F1
PascalVOC-SP 11,355 479.4 2710.5 21 Node Classification F1
Peptides-Func 15,535 150.9 307.3 10 Graph Classification AP
Peptides-Struct 15,535 150.9 307.3 - Graph Regression MAE
GNN
Benchmark [21]
MNIST
70,000 70.6 564.5 10 Graph Classification ACC.
CIFAR10 60,000 117.6 941.1 10 Graph Classification ACC.
Pattern 14,000 118.9 3,039.3 2 Node Classification ACC.
MalNet-Tiny 5,000 1,410.3 2,859.9 5 Graph Classification ACC.
Heter
ophilic Benchmark [66]
Roman-empir
e 1 22,662 32,927 18 Node Classification ACC.
Amazon-ratings 1 24,492 93,050 5 Node Classification ACC.
Minesweeper 1 10,000 39,402 2 Node Classification AUC
Tolokers 1 11,758 519,000 2 Node Classification AUC
V
ery Large Dataset [37]
OGBN-
Arxiv 1 169,343 1,166,243 40 Node Classification ACC.
5 Experiments
We evaluate the performance of GMNs in long-range, small-scale,
large-scale, and heterophilic benchmark datasets. We further dis-
cuss its efficiency and perform ablation study to validate the con-
tribution of each architectural choice. The detailed statistics of
datasets and additional experiments are available in the appendix.
5.1 Experimental Setup
Dataset. We use three most commonly used benchmark datasets
with long-range, small-scale, large-scale, and heterophilic proper-
ties. For long-range datasets, we use Longe Range Graph Benchmark
(LRGB) dataset [ 22]. For small and large-scale datasets, we use GNN
benchmark [ 21]. To evaluate the GMNs on heterophilic graphs, we
use four heterophilic datasets from the work of Platonov et al . [66] .
Finally, we use a large dataset from Open Graph Benchmark [ 37].
We evaluate the performance of GMNs on various graph learn-
ing tasks (e.g., graph classification, regression, node classification
and link classification). Also, for each datasets we use the propose
metrics in the original benchmark and report the metric across
multiple runs, ensuring the robustness. The statistics of datasets are
reported in Table 1. For additional details about the datasets, we re-
fer to the Long-range graph benchmark [ 22], GNN Benchmark [ 21],
Heterophilic Benchmark [66], and Open Graph Benchmark [37].
Baselines. We compare our GMNs with different types of graph
neural networks, 1MPNNs: GCN [ 41], GIN [ 94], Gated-GCN [ 15],
GraphSAGE [ 33], GAT [ 83], H2GCN [ 106], GPRGNN [ 56], FSGNN
[56], GGCN [ 95], tGNN [ 38], G2-GNN [ 72], DIR-GNN [ 69], SPN [ 1],
MixHop [ 2], DIGL [ 27], and OrderedGNN [ 78];2Random walk
based methods: CRaWl [ 81];3state-of-the-art and effieicnt GTs:
SAN [ 43], NAGphormer [ 16], Graph ViT [ 35], two variants of
GPS [ 67], GOAT [ 42], NodeFormer [ 91], DIFFormer [ 90] and Ex-
phormer [ 75], and 4our baselines: (i) GPS + Mamba: when we
replace the transformer module in GPS with bidirectional Mamba.
(ii) GMN-: when we do not use PE/SE and global MPNN, but we
use different local encodings.
Setup. We use grid search to tune hyperparameters. Following
previous studies, we use the same split of traning/test/validation
as [67]. We report the results over the 4 random seeds. Also, for
 
125KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ali Behrouz and Farnoosh Hashemi
Table 2: Benchmark on Long-Range Graph Datasets [ 22]. Highlighted
are the top first, second, and third results.
Mo
delCOCO-SP PascalVOC-SP Peptides-Func Peptides-Struct
F1 scoreâ†‘ F1 scoreâ†‘ APâ†‘ MAEâ†“
GCN 0.0841Â±0.0010 0.1268Â±0.0060 0.5930Â±0.0023 0.3496Â±0.0013
GIN 0.1339Â±0.0044 0.1265Â±0.0076 0.5498Â±0.0079 0.3547Â±0.0045
Gate
d-GCN 0.2641Â±0.0045 0.2873Â±0.0219 0.5864Â±0.0077 0.3420Â±0.0013
GAT 0.1296Â±0.0028 0.1753Â±0.0329 0.5308Â±0.0019 0.2731Â±0.0402
MixHop - 0.2506Â±0.0133 0.6843Â±0.0049 0.2614Â±0.0023
DIGL - 0.2921Â±0.0038 0.6830Â±0.0026 0.2616Â±0.0018
SPN - 0.2056Â±0.0338 0.6926Â±0.0247 0.2554Â±0.0035
CRaWl 0.3219Â±0.00106 0.4088Â±0.0079 0.6963Â±0.0079 0.2506Â±0.0022
SAN+LapPE 0.2592Â±0.0158 0.3230Â±0.0039 0.6384Â±0.0121 0.2683Â±0.0043
NA
Gphormer 0.3458Â±0.0070 0.4006Â±0.0061 - -
Graph ViT - - 0.6855Â±0.0049 0.2468Â±0.0015
GPS 0.3774Â±0.0150 0.3689Â±0.0131 0.6575Â±0.0049 0.2510Â±0.0015
GPS (BigBird) 0.2622Â±0.0008 0.2762Â±0.0069 0.5854Â±0.0079 0.2842Â±0.0130
Exphormer 0.3430Â±0.0108 0.3975Â±0.0037 0.6527Â±0.0043 0.2481Â±0.0007
NodeFormer 0.3275Â±0.0241 0.4015Â±0.0082 - -
DIFFormer 0.3620Â±0.0012 0.3988Â±0.0045 - -
GPS
+ Mamba 0.3895Â±0.0125 0.4180Â±0.012 0.6624Â±0.0079 0.2518Â±0.0012
GMN- 0.3618Â±0.0053 0.4019Â±0.094 0.6671Â±0.0024 0.2522Â±0.0035
GMN 0.3909Â±0.0128 0.4192Â±0.0108 0.6972Â±0.0100 0.2477Â±0.0019
the baselinesâ€™ results (in Tables 2, 3, and 4), we have re-used and
reported the benchmark results in the work by Deng et al . [18] ,
Shirzad et al. [75], TÃ¶nshoff et al. [80] and Wang et al. [84].
5.2 Long Range Graph Benchmark
Table 2 reports the results of GMNs and baselines on long-range
graph benchmark. GMNs outperform baselines in 3 out of 4 datasets
that requires long-range dependencies between nodes. The reason
for this superior performance is three folds: (1) GMNs based on our
design use long sequence of tokens to learn node encodings and
then use another selection mechanism to filter irrelevant nodes. The
provided long sequence of tokens enables GMNs to learn long-range
dependencies, without facing scalability or over-squashing issues.
(2) GMNs using their selection mechanism are capable of filtering
the neighborhood around each node. Accordingly, only informative
information flows into hidden states. (3) The random-walk based
neighborhood sampling allow GMNs to have diverse samples of
neighborhoods, while capturing the hierarchical nature of ğ‘˜-hop
neighborhoods. Also, it is notable that GMN consistently outper-
forms our baseline GPS + Mamba, which shows the importance
of paying attention to the new challenges. That is, replacing the
transformer module with Mamba, while improves the performance,
cannot fully take advantage of the Mamba traits. Interestingly,
GMN-, a variant of GMNs without Transformer, MPNN, and PE/SE
that we use to evaluate the importance of these elements in achiev-
ing good performance, can achieve competitive performance with
other complex methods, showing that while Transformers, complex
message-passing, and SE/PE are sufficient for good performance in
practice, neither is necessary.
5.3 Comparison on GNN Benchmark
We further evaluate the performance of GMNs in small and large
datasets from the GNN benchmark. The results of GMNs and base-
line performance are reported in Table 3. GMN and Exphormer
achieve competitive performance, each outperforms the other twoTable 3: Benchmark on GNN Benchmark Datasets [ 21]. Highlighted
are the top first, second, and third results.
Mo
delMNIST CIFAR10 PATTERN MalNet-Tiny
Accuracyâ†‘ Accuracyâ†‘ Accuracyâ†‘ Accuracyâ†‘
GCN 0.9071Â±0.0021 0.5571Â±0.0038 0.7189Â±0.0033 0.8100Â±0.0000
GraphSA
GE 0.9731Â±0.0009 0.6577Â±0.0030 0.5049Â±0.0001 0.8730Â±0.0002
GAT 0.9554Â±0.0021 0.6422Â±0.0046 0.7827Â±0.0019 0.8509Â±0.0025
SPN 0.8331Â±0.0446 0.3722Â±0.0827 0.8657Â±0.0014 0.6407Â±0.0581
GIN 0.9649Â±0.0025 0.5526Â±0.0152 0.8539Â±0.0013 0.8898Â±0.0055
Gated-GCN 0.9734Â±0.0014 0.6731Â±0.0031 0.8557Â±0.0008 0.9223Â±0.0065
CRaWl 0.9794Â±0.050 0.6901Â±0.0259 -
-
NA
Gphormer - - 0.8644Â±0.0003 -
GPS 0.9811Â±0.0011 0.7226Â±0.0031 0.8664Â±0.0011 0.9298Â±0.0047
GPS (BigBird) 0.9817Â±0.0001 0.7048Â±0.0010 0.8600Â±0.0014 0.9234Â±0.0034
Exphormer 0.9855Â±0.0003 0.7469Â±0.0013 0.8670Â±0.0003 0.9402Â±0.0020
NodeFormer - - 0.8639Â±0.0021 -
DIFFormer - - 0.8701Â±0.0018 -
GPS
+ Mamba 0.9821Â±0.0004 0.7341Â±0.0015 0.8660Â±0.0007 0.9311Â±0.0042
GMN- 0.9783Â±0.0020 0.7444Â±0.0009 0.8649Â±0.0019 0.9352Â±0.0036
GMN 0.9839Â±0.0018 0.7456Â±0.0038 0.8709Â±0.0126 0.9415Â±0.0020
Table 4: Benchmark on heterophilic datasets [ 66]. Highlighted are
the top first, second, and third results.
Mo
delRoman-empire Amazon-ratings Minesweeper Tolokers
Accuracyâ†‘ Accuracyâ†‘ ROC AUCâ†‘ ROC AUCâ†‘
GCN 0.7369Â±0.0074 0.4870Â±0.0063 0.8975Â±0.0052 0.8364Â±0.0067
GraphSA
GE 0.8574Â±0.0067 0.5363Â±0.0039 0.9351Â±0.0057 0.8243Â±0.0044
GAT 0.7973Â±0.0039 0.5270Â±0.0062 0.9391Â±0.0035 0.8378Â±0.0043
H2GCN 0.6011Â±0.0052 0.3647Â±0.0023 0.8971Â±0.0031 0.7335Â±0.0101
GPRGNN 0.6485Â±0.0027 0.4488Â±0.0034 0.8624Â±0.0061 0.7294Â±0.0097
FSGNN 0.7992Â±0.0056 0.5274Â±0.0083 0.9008Â±0.0070 0.8276Â±0.0061
GGCN 0.7446Â±0.0054 0.4300Â±0.0032 0.8754Â±0.0122 0.7731Â±0.0114
OrderedGNN 0.7768Â±0.0039 0.4729Â±0.0065 0.8058Â±0.0108 0.7560Â±0.0136
G2-GNN 0.8216Â±0.0078 0.4793Â±0.0058 0.9183Â±0.0056 0.8251Â±0.0080
DIR-GNN 0.9123Â±0.0032 0.4789Â±0.0039 0.8705Â±0.0069 0.8119Â±0.0105
tGNN 0.7995Â±0.0075 0.4821Â±0.0053 0.9193Â±0.0077 0.7084Â±0.0175
Gated-GCN 0.7446Â±0.0054 0.4300Â±0.0032 0.8754Â±0.0122 0.7731Â±0.0114
NA
Gphormer 0.7434Â±0.0077 0.5126Â±0.0072 0.8419Â±0.0066 0.7832Â±0.0095
GPS 0.8200Â±0.0061 0.5310Â±0.0042 0.9063Â±0.0067 0.8371Â±0.0048
Exphormer 0.8903Â±0.0037 0.5351Â±0.0046 0.9074Â±0.0053 0.8377Â±0.0078
NodeFormer 0.6449Â±0.0073 0.4386Â±0.0035 0.8671Â±0.0088 0.7810Â±0.0103
DIFFormer 0.7910Â±0.0032 0.4784Â±0.0065 0.9089Â±0.0058 0.8357Â±0.0068
GOAT 0.7159Â±0.0125 0.4461Â±0.0050 0.8109Â±0.0102 0.8311Â±0.0104
GPS
+ Mamba 0.8310Â±0.0028 0.4513Â±0.0097 0.8993Â±0.0054 0.8370Â±0.0105
GMN- 0.8002Â±0.0023 0.5134Â±0.0038 0.8992Â±0.0063 0.8125Â±0.0101
GMN 0.8588Â±0.0126 0.5369Â±0.0121 0.9101Â±0.0173 0.8399Â±0.0059
times. On the other hand again, GMN consistently outperforms
GPS + Mamba baseline, showing the importance of designing a
new framework for GMNs rather then using existing frameworks
of GTs.
5.4 Heterophilic Datasets
To evaluate the performance of GMNs on the heterophilc data as
well as evaluating their robustness to over-squashing and over-
smoothing, we compare their performance with the state-of-the-art
baselines and report the results in Table 4. Our GMN outperforms
GT baselines in 3 out of 4 datasets and achieve the second best result
in the remaining dataset. These results show that the selection
mechanism in GMN can effectively filter irrelevant information
and also consider long-range dependencies in heterophilic datasets.
 
126Graph Mamba: Towards Learning on Graphs with State Space Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
1246810
M0.40.60.8F1/ACC
Roman-empire
PascalVOC-SP
15102030405060
m0.40.60.8F1/ACC
Roman-empire
PascalVOC-SP
5 10 15
s0.40.60.8F1/ACC
Roman-empire
PascalVOC-SP
Figur
e 2: The effect of (Left) ğ‘€, (Middle)ğ‘š, and (Right) ğ‘ on the performance of GMNs.
Table 5: Ablation study on GMN architecture.
Mo
delRoman-empire Amazon-ratings Minesweeper
Accuracyâ†‘ Accuracyâ†‘ ROC AUCâ†‘
GMN 0.8588Â±0.0126 0.5369Â±0.0121 0.9101Â±0.0173
w/o
bidirectional Mamba 0.8409Â±0.0120 0.5258Â±0.0034 0.8797Â±0.0045
w/o MPNN 0.8475Â±0.0071 0.5301Â±0.0106 0.8902Â±0.0051
PPR ordering 0.8469Â±0.0197 0.5285Â±0.0101 0.8980Â±0.0093
w/o PE and MPNN (GMN-) 0.8002Â±0.0023 0.5134Â±0.0038 0.8992Â±0.0063
Table 6: Efficiency evaluation and accuracy of GMNs and baselines on
OBGN-Arxiv andMalNet-Tiny. Highlighted are the top first, second,
andthird results. OOM: Out of Memory.
Metho
d Gated-GCN GPS NAGphormer Exphormerâ€ GOATOurs
GPS+Mamba
GMN
OGBN-
Arxiv
Time/Ep
och (s) 0.68 OOM 5.06 1.97 13.09 1.18 1.30
Memory (GB) 11.09 OOM 6.24 36.18 8.41 5.02 3.85
Accuracy 0.7141 OOM 0.7013 0.7228 0.7196 0.7239 0.7248
MalNet-
Tiny
Time/Ep
och (s) 10.3 148.99 - 57.24 - 36.07 41.00
Accuracy 0.9223 0.9234 - 0.9224 - 0.9311 0.9415
â€ W
e follow the original paper [75] and use one virtual node in efficiency evaluation.
5.5
Ablation Study
To evaluate the contribution of each component of GMNs in its
performance, we perform ablation study. Table 5 reports the re-
sults. The first row, reports the performance of GMNs with its
full architecture. Then in each row, we modify one the elements
while keeping the other unchanged: Row 2 remove the bidirectional
Mamba and use a simple Mamba. Row 3 remove the MPNN and Row
4 use PPR ordering. Finally the last row remove PE and MPNN (pure
Mamba). Results show that all the elements of GMN contributes to
its performance with most contribution from bidirection Mamba.
5.6 Efficiency
As we discussed earlier, one of the main advantages of our model is
its efficiency and memory usage. We evaluate this claim on OGBN-
Arxiv [ 37] and MalNet-Tiny [ 21] datasets and report the results
in Figure 6. Our variants of GMNs are the most efficient methods
while achieving the best performance.
5.7 Parameter Sensitivity
The effect of ğ‘€. Parameter ğ‘€is the number of walks that we
aggregate to construct a subgraph token. To evaluate its effect on
the performance of the GMN, we use two datasets of Roman-empireand PascalVOC-SP, from two different benchmarks, and vary the
value ofğ‘€from 1 to 10. The results are reported in Figure 2 (Left).
These results show that performance peaks at certain value of
ğ‘€and the exact value varies with the dataset. The main reason
is, parameter ğ‘€determines that how many walks can be a good
representative of the neighborhood of a node, and so depends on
the density, homophily score, and network topology this value can
be different.
The effect of ğ‘š. Similar to the above, we use two datasets of
Roman-empire and PascalVOC-SP, from two different benchmarks,
and vary the value of ğ‘šfrom 1 to 60. The results are reported in
Figure 2 (Middle). The performance is non-decreasing with respect
to the value of ğ‘š. That is, increasing the value of ğ‘š, i.e., consid-
ering far neighbors in the tokenization process, does not damage
the performance (might lead to better results). Intuitively, using
large values of ğ‘šis expected to damage the performance due to
the over-smoothing and over-squashing; however, the selection
mechanism in Bidirectional Mamba can select informative tokens
(i.e., neighborhood), filtering information that causes performance
damage.
The effect of ğ‘ . Similarly, we use two datasets of Roman-empire
and PascalVOC-SP. We vary the value of ğ‘ and then report the
results in Figure 2 (Right). Result show that increasing the value of
ğ‘ can monotonically improve the performance. As discussed earlier,
longer sequences of tokens can provide more context about the
structure of the nodeâ€™s neighborhood for our model and due to the
selection mechanism in Mamba [ 28], GMNs can select informative
subgraphs/nodes and filter irrelevant tokens, resulting in better
results with longer sequences.
6 Conclusion
In this paper, we present Graph Mamba Networks (GMNs) as a new
class of graph learning based on state space models. We discuss
and categorize the new challenges when adapting SSMs to graph-
structured data, and present four required and one optional steps
to design GMNs, where we choose (1) Neighborhood Tokenization,
(2) Token Ordering, (3) Architecture of Bidirectional Selective SSM
Encoder, (4) Local Encoding, and dispensable (5) PE and SE. We
further provide theoretical justification for the power of GMNs
and show that while being more efficient than GTs, their efficiency
does not sacrifice the expressive power. We conduct several experi-
ments on various benchmark datasets to empirically evaluate their
performance.
 
127KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ali Behrouz and Farnoosh Hashemi
References
[1]Ralph Abboud, Radoslav Dimitrov, and Ismail Ilkan Ceylan. 2022. Shortest
path networks for graph property prediction. In Learning on Graphs Conference.
PMLR, 5â€“1.
[2]Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina
Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:
Higher-order graph convolutional architectures via sparsified neighborhood
mixing. In international conference on machine learning. PMLR, 21â€“29.
[3]Md Atik Ahamed and Qiang Cheng. 2024. MambaTab: A Simple Yet Effective
Approach for Handling Tabular Data. arXiv preprint arXiv:2401.08867 (2024).
[4]Uri Alon and Eran Yahav. 2021. On the Bottleneck of Graph Neural Networks and
its Practical Implications. In International Conference on Learning Representations.
https://openreview.net/forum?id=i80OPhOCVH2
[5]Masanao Aoki. 2013. State space modeling of time series. Springer Science &
Business Media.
[6]AdriÃ¡n Arnaiz-RodrÃ­guez, Ahmed Begga, Francisco Escolano, and Nuria M
Oliver. 2022. DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound. In
The First Learning on Graphs Conference. https://openreview.net/forum?id=
IXvfIex0mX6f
[7]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normal-
ization. arXiv:1607.06450 [stat.ML]
[8]Jinheon Baek, Minki Kang, and Sung Ju Hwang. 2021. Accurate Learning of
Graph Representations with Graph Multiset Pooling. In International Conference
on Learning Representations. https://openreview.net/forum?id=JHcqXGaqiGn
[9]Guy Bar-Shalom, Beatrice Bevilacqua, and Haggai Maron. 2023. Subgraphormer:
Subgraph GNNs meet Graph Transformers. In NeurIPS 2023 Workshop: New
Frontiers in Graph Learning. https://openreview.net/forum?id=e8ba9Hu1mM
[10] Ali Behrouz, Parsa Delavari, and Farnoosh Hashemi. 2023. Unsupervised
Representation Learning of Brain Activity via Bridging Voxel Activity and
Functional Connectivity. In NeurIPS 2023 AI for Science Workshop. https:
//openreview.net/forum?id=HSvg7qFFd2
[11] Ali Behrouz and Farnoosh Hashemi. 2024. Brain-Mamba: Encoding Brain
Activity via Selective State Space Models. Conference on Health, Inference, and
Learning (CHIL) (2024).
[12] Ali Behrouz and Farnoosh Hashemi. 2024. Graph Mamba: Towards Learning on
Graphs with State Space Models. arXiv preprint arXiv:2402.08678 (2024).
[13] Ali Behrouz, Michele Santacatterina, and Ramin Zabih. 2024. Chimera: Ef-
fectively Modeling Multivariate Time Series with 2-Dimensional State Space
Models. arXiv preprint arXiv:2406.04320 (2024).
[14] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan,
Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron.
2022. Equivariant Subgraph Aggregation Networks. In International Conference
on Learning Representations . https://openreview.net/forum?id=dFbKQaRk15w
[15] Xavier Bresson and Thomas Laurent. 2017. Residual gated graph convnets.
arXiv preprint arXiv:1711.07553 (2017).
[16] Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. 2023. NAGphormer:
A Tokenized Graph Transformer for Node Classification in Large Graphs. In
The Eleventh International Conference on Learning Representations. https://
openreview.net/forum?id=8KYeilT3Ow
[17] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian
Weller. 2021. Rethinking Attention with Performers. In International Conference
on Learning Representations. https://openreview.net/forum?id=Ua6zuk0WRH
[18] Chenhui Deng, Zichao Yue, and Zhiru Zhang. 2024. Polynormer: Polynomial-
Expressive Graph Transformer in Linear Time. In The Twelfth International
Conference on Learning Representations. https://openreview.net/forum?id=
hmv1LpNfXa
[19] Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro
Lio, and Michael M Bronstein. 2023. On over-squashing in message passing
neural networks: The impact of width, depth, and topology. In International
Conference on Machine Learning. PMLR, 7865â€“7885.
[20] Yuhui Ding, Antonio Orvieto, Bobby He, and Thomas Hofmann. 2023. Recurrent
Distance-Encoding Neural Networks for Graph Representation Learning. arXiv
preprint arXiv:2312.01538 (2023).
[21] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent,
Yoshua Bengio, and Xavier Bresson. 2023. Benchmarking graph neural networks.
Journal of Machine Learning Research 24, 43 (2023), 1â€“48.
[22] Vijay Prakash Dwivedi, Ladislav RampÃ¡Å¡ek, Michael Galkin, Ali
Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. 2022. Long
Range Graph Benchmark. In Advances in Neural Information Pro-
cessing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 22326â€“
22340. https://proceedings.neurips.cc/paper_files/paper/2022/file/
8c3c666820ea055a77726d66fc7d447f-Paper-Datasets_and_Benchmarks.pdf
[23] Federico Errica, Henrik Christiansen, Viktor Zaverkin, Takashi Maruyama,
Mathias Niepert, and Francesco Alesiani. 2023. Adaptive Message Passing: AGeneral Framework to Mitigate Oversmoothing, Oversquashing, and Under-
reaching. arXiv preprint arXiv:2312.16560 (2023).
[24] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference. 417â€“426.
[25] Ben Finkelshtein, Xingyue Huang, Michael Bronstein, and Ä°smail Ä°lkan Ceylan.
2023. Cooperative graph neural networks. arXiv preprint arXiv:2310.01267
(2023).
[26] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and
Christopher Re. 2023. Hungry Hungry Hippos: Towards Language Modeling
with State Space Models. In The Eleventh International Conference on Learning
Representations. https://openreview.net/forum?id=COZDy0WYGg
[27] Johannes Gasteiger, Stefan WeiÃŸenberger, and Stephan GÃ¼nnemann. 2019. Diffu-
sion improves graph learning. Advances in neural information processing systems
32 (2019).
[28] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with
selective state spaces. arXiv preprint arXiv:2312.00752 (2023).
[29] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2020. Hippo:
Recurrent memory with optimal polynomial projections. Advances in neural
information processing systems 33 (2020), 1474â€“1487.
[30] Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently Modeling Long
Sequences with Structured State Spaces. In International Conference on Learning
Representations. https://openreview.net/forum?id=uYLFoz1vlAC
[31] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and
Christopher RÃ©. 2021. Combining recurrent, convolutional, and continuous-time
models with linear state space layers. Advances in neural information processing
systems 34 (2021), 572â€“585.
[32] Benjamin Gutteridge, Xiaowen Dong, Michael M Bronstein, and Francesco
Di Giovanni. 2023. Drew: Dynamically rewired message passing with delay. In
International Conference on Machine Learning. PMLR, 12252â€“12267.
[33] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[34] Farnoosh Hashemi, Ali Behrouz, and Laks V.S. Lakshmanan. 2022. FirmCore
Decomposition of Multilayer Networks. In Proceedings of the ACM Web Confer-
ence 2022 (Virtual Event, Lyon, France) (WWW â€™22). Association for Computing
Machinery, New York, NY, USA, 1589â€“1600. https://doi.org/10.1145/3485447.
3512205
[35] Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier
Bresson. 2023. A generalization of vit/mlp-mixer to graphs. In International
Conference on Machine Learning. PMLR, 12724â€“12745.
[36] Mikael Henaff, Joan Bruna, and Yann LeCun. 2015. Deep convolutional networks
on graph-structured data. arXiv preprint arXiv:1506.05163 (2015).
[37] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets
for machine learning on graphs. Advances in neural information processing
systems 33 (2020), 22118â€“22133.
[38] Chenqing Hua, Guillaume Rabusseau, and Jian Tang. 2022. High-order pooling
for graph neural networks with tensor decomposition. Advances in Neural
Information Processing Systems 35 (2022), 6021â€“6033.
[39] Md Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian.
2022. Global self-attention as a replacement for graph convolution. In Proceed-
ings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 655â€“665.
[40] Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak
Lee, and Seunghoon Hong. 2022. Pure transformers are powerful graph learners.
Advances in Neural Information Processing Systems 35 (2022), 14582â€“14595.
[41] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with
graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[42] Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Renkun Ni, C Bayan Bruss, and
Tom Goldstein. 2023. GOAT: A global transformer on large-scale graphs. In
International Conference on Machine Learning. PMLR, 17375â€“17390.
[43] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent LÃ©tourneau, and
Prudencio Tossou. 2021. Rethinking graph transformers with spectral attention.
Advances in Neural Information Processing Systems 34 (2021), 21618â€“21629.
[44] Weirui Kuang, WANG Zhen, Yaliang Li, Zhewei Wei, and Bolin Ding. 2021.
Coarformer: Transformer for large graph via graph coarsening. (2021).
[45] Vito Latora and Massimo Marchiori. 2007. A measure of centrality based on
network efficiency. New Journal of Physics 9, 6 (2007), 188.
[46] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and
Yee Whye Teh. 2019. Set transformer: A framework for attention-based
permutation-invariant neural networks. In International conference on machine
learning. PMLR, 3744â€“3753.
[47] Don R. Lick and Arthur T. White. 1970. k-Degenerate Graphs. Canadian Journal
of Mathematics 22, 5 (1970), 1082â€“1096. https://doi.org/10.4153/CJM-1970-125-1
 
128Graph Mamba: Towards Learning on Graphs with State Space Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[48] Derek Lim, Joshua David Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra,
Haggai Maron, and Stefanie Jegelka. 2023. Sign and Basis Invariant Net-
works for Spectral Graph Representation Learning. In The Eleventh International
Conference on Learning Representations. https://openreview.net/forum?id=Q-
UHqMorzil
[49] Heejoo Lim, Yoonji Joo, Eunji Ha, Yumi Song, Sujung Yoon, In Kyoon Lyoo,
and Taehoon Shin. 2023. Brain age prediction using multi-hop graph attention
module(MGA) with convolutional neural network. In Medical Imaging with Deep
Learning, short paper track. https://openreview.net/forum?id=brK-VVoDpqo
[50] Chuang Liu, Yibing Zhan, Xueqi Ma, Liang Ding, Dapeng Tao, Jia Wu, and
Wenbin Hu. 2023. Gapformer: Graph transformer with graph pooling for node
classification. In Proceedings of the 32nd International Joint Conference on Artifi-
cial Intelligence (IJCAI-23). 2196â€“2205.
[51] Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong
Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, et al .2024. Swin-
UMamba: Mamba-based UNet with ImageNet-based pretraining. arXiv preprint
arXiv:2402.03302 (2024).
[52] Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. 2020. Learn-
ing to encode position for transformer with continuous dynamical model. In
International conference on machine learning. PMLR, 6327â€“6335.
[53] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang,
Qixiang Ye, and Yunfan Liu. 2024. VMamba: Visual State Space Model. arXiv
preprint arXiv:2401.10166 (2024).
[54] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer
using shifted windows. In Proceedings of the IEEE/CVF international conference
on computer vision. 10012â€“10022.
[55] Jun Ma, Feifei Li, and Bo Wang. 2024. U-mamba: Enhancing long-range de-
pendency for biomedical image segmentation. arXiv preprint arXiv:2401.04722
(2024).
[56] Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. 2022. Simplifying approach
to node classification in graph neural networks. Journal of Computational Science
62 (2022), 101695.
[57] Christopher Morris, Gaurav Rattan, and Petra Mutzel. 2020. Weisfeiler and
Leman go sparse: Towards scalable higher-order graph embeddings. Advances
in Neural Information Processing Systems 33 (2020), 21824â€“21840.
[58] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric
Lenssen, Gaurav Rattan, and Martin Grohe. 2019. Weisfeiler and leman go neural:
Higher-order graph neural networks. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 33. 4602â€“4609.
[59] Luis MÃ¼ller, Mikhail Galkin, Christopher Morris, and Ladislav RampÃ¡Å¡ek. 2023.
Attending to graph transformers. arXiv preprint arXiv:2302.04181 (2023).
[60] Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro.
2019. Relational pooling for graph representations. In International Conference
on Machine Learning. PMLR, 4663â€“4673.
[61] Dai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung. 2022. Universal graph
transformer self-attention networks. In Companion Proceedings of the Web Con-
ference 2022. 193â€“196.
[62] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao,
Stephen Baccus, and Christopher RÃ©. 2022. S4nd: Modeling images and videos
as multidimensional signals with state spaces. Advances in neural information
processing systems 35 (2022), 2846â€“2861.
[63] Eric Nguyen, Michael Poli, Marjan Faizi, Armin W Thomas, Michael Wornow,
Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton M. Rabideau,
Yoshua Bengio, Stefano Ermon, Christopher Re, and Stephen Baccus. 2023.
HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Res-
olution. In Thirty-seventh Conference on Neural Information Processing Systems.
https://openreview.net/forum?id=ubzNoJjOKj
[64] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The
pagerank citation ranking: Bring order to the web. Technical Report. Technical
report, stanford University.
[65] Jinyoung Park, Seongjun Yun, Hyeonjin Park, Jaewoo Kang, Jisu Jeong, Kyung-
Min Kim, Jung-woo Ha, and Hyunwoo J Kim. 2022. Deformable Graph Trans-
former. arXiv preprint arXiv:2206.14337 (2022).
[66] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liud-
mila Prokhorenkova. 2023. A critical look at the evaluation of GNNs under
heterophily: Are we really making progress?. In The Eleventh International Con-
ference on Learning Representations . https://openreview.net/forum?id=tJbbQfw-
5wv
[67] Ladislav RampÃ¡Å¡ek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu,
Guy Wolf, and Dominique Beaini. 2022. Recipe for a general, powerful, scalable
graph transformer. Advances in Neural Information Processing Systems 35 (2022),
14501â€“14515.
[68] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang,
and Junzhou Huang. 2020. Self-supervised graph transformer on large-scale
molecular data. Advances in Neural Information Processing Systems 33 (2020),
12559â€“12571.[69] Emanuele Rossi, Bertrand Charpentier, Francesco Di Giovanni, Fabrizio Frasca,
Stephan GÃ¼nnemann, and Michael M. Bronstein. 2023. Edge Directionality
Improves Learning on Heterophilic Graphs. In The Second Learning on Graphs
Conference. https://openreview.net/forum?id=T4LRbAMWFn
[70] Britta Ruhnau. 2000. Eigenvector-centralityâ€”a node-centrality? Social networks
22, 4 (2000), 357â€“365.
[71] T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. 2023. A sur-
vey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993
(2023).
[72] T Konstantin Rusch, Benjamin Paul Chamberlain, Michael W Mahoney,
Michael M Bronstein, and Siddhartha Mishra. 2022. Gradient gating for deep
multi-rate learning on graphs. In The Eleventh International Conference on Learn-
ing Representations.
[73] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. 2021. Random features
strengthen graph neural networks. In Proceedings of the 2021 SIAM international
conference on data mining (SDM). SIAM, 333â€“341.
[74] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H
Chi, Nathanael SchÃ¤rli, and Denny Zhou. 2023. Large language models can be
easily distracted by irrelevant context. In International Conference on Machine
Learning. PMLR, 31210â€“31227.
[75] Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J Sutherland,
and Ali Kemal Sinop. 2023. Exphormer: Sparse transformers for graphs. arXiv
preprint arXiv:2303.06147 (2023).
[76] Vighnesh Shiv and Chris Quirk. 2019. Novel positional encodings to enable
tree-based transformers. Advances in neural information processing systems 32
(2019).
[77] Yunchong Song, Siyuan Huang, Jiacheng Cai, Xinbing Wang, Chenghu Zhou,
and Zhouhan Lin. 2024. S4G: Breaking the Bottleneck on Graphs with Structured
State Spaces. https://openreview.net/forum?id=0Z6lN4GYrO
[78] Yunchong Song, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin. 2023. Ordered
GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing.
InThe Eleventh International Conference on Learning Representations. https:
//openreview.net/forum?id=wKPmPBHSnT6
[79] Siyi Tang, Jared A Dunnmon, Qu Liangqiong, Khaled K Saab, Tina Baykaner,
Christopher Lee-Messer, and Daniel L Rubin. 2023. Modeling Multivariate
Biosignals With Graph Neural Networks and Structured State Space Models. In
Proceedings of the Conference on Health, Inference, and Learning (Proceedings of
Machine Learning Research, Vol. 209), Bobak J. Mortazavi, Tasmie Sarker, Andrew
Beam, and Joyce C. Ho (Eds.). PMLR, 50â€“71. https://proceedings.mlr.press/
v209/tang23a.html
[80] Jan TÃ¶nshoff, Martin Ritzert, Eran Rosenbluth, and Martin Grohe. 2023. Where
did the gap go? reassessing the long-range graph benchmark. arXiv preprint
arXiv:2309.00367 (2023).
[81] Jan TÃ¶nshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. 2023. Walk-
ing Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Mes-
sage Passing. Transactions on Machine Learning Research (2023). https:
//openreview.net/forum?id=vgXnEyeWVY
[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. Advances in neural information processing systems 30 (2017).
[83] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In International
Conference on Learning Representations. https://openreview.net/forum?id=
rJXMpikCZ
[84] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. 2024. Graph-Mamba: Towards
Long-Range Graph Sequence Modeling with Selective State Spaces. arXiv
preprint arXiv:2402.00789 (2024).
[85] Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. 2022. Equivariant
and Stable Positional Encoding for More Powerful Graph Neural Networks. In
International Conference on Learning Representations. https://openreview.net/
forum?id=e95i1IHcWj
[86] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander Rush. 2023. Pre-
training Without Attention. In Findings of the Association for Computational
Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.).
Association for Computational Linguistics, Singapore, 58â€“69. https://doi.org/
10.18653/v1/2023.findings-emnlp.5
[87] Shida Wang and Beichen Xue. 2023. State-space models with layer-wise
nonlinearity are universal approximators with exponential decaying mem-
ory. In Thirty-seventh Conference on Neural Information Processing Systems.
https://openreview.net/forum?id=i0OmcF14Kf
[88] Yingheng Wang, Yaosen Min, Erzhuo Shao, and Ji Wu. 2021. Molecular graph
contrastive learning with parameterized explainable augmentations. In 2021
IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE,
1558â€“1563.
[89] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-
langue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz,
et al .2020. Transformers: State-of-the-art natural language processing. In
Proceedings of the 2020 conference on empirical methods in natural language
 
129KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ali Behrouz and Farnoosh Hashemi
processing: system demonstrations. 38â€“45.
[90] Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi
Yan. 2023. DIFFormer: Scalable (Graph) Transformers Induced by Energy Con-
strained Diffusion. In The Eleventh International Conference on Learning Repre-
sentations. https://openreview.net/forum?id=j6zUzrapY3L
[91] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-
former: A scalable graph structure learning transformer for node classification.
Advances in Neural Information Processing Systems 35 (2022), 27387â€“27401.
[92] Yi Wu, Yanyang Xu, Wenhao Zhu, Guojie Song, Zhouchen Lin, Liang Wang, and
Shaoguo Liu. 2023. KDLGT: a linear graph transformer framework via kernel
decomposition approach. In Proceedings of the Thirty-Second International Joint
Conference on Artificial Intelligence. 2370â€“2378.
[93] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4â€“24.
[94] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Pow-
erful are Graph Neural Networks?. In International Conference on Learning
Representations. https://openreview.net/forum?id=ryGs6iA5Km
[95] Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.
2022. Two sides of the same coin: Heterophily and oversmoothing in graph
convolutional neural networks. In 2022 IEEE International Conference on Data
Mining (ICDM). IEEE, 1287â€“1292.
[96] Yijun Yang, Zhaohu Xing, and Lei Zhu. 2024. Vivim: a Video Vision Mamba for
Medical Video Object Segmentation. arXiv preprint arXiv:2401.14168 (2024).
[97] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,
Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly for
graph representation? Advances in Neural Information Processing Systems 34
(2021), 28877â€“28888.
[98] Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and Shi Wu. 2020.
Order matters: Semantic-aware neural networks for binary code similarity
detection. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34.
1145â€“1152.
[99] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim.
2019. Graph transformer networks. Advances in neural information processing
systems 32 (2019).
[100] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
et al.2020. Big bird: Transformers for longer sequences. Advances in neural
information processing systems 33 (2020), 17283â€“17297.
[101] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. 2023. A Com-
plete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-
Lehman Tests. In Proceedings of the 40th International Conference on Machine
Learning (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause,
Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and
Jonathan Scarlett (Eds.). PMLR, 41019â€“41077. https://proceedings.mlr.press/
v202/zhang23k.html
[102] Michael Zhang, Khaled Kamal Saab, Michael Poli, Tri Dao, Karan Goel, and
Christopher Re. 2023. Effectively Modeling Time Series with Simple Discrete
State Spaces. In The Eleventh International Conference on Learning Representa-
tions. https://openreview.net/forum?id=2EpjkjzdCAa
[103] Zaixi Zhang, Qi Liu, Qingyong Hu, and Chee-Kong Lee. 2022. Hierarchical
graph transformer with adaptive node sampling. Advances in Neural Information
Processing Systems 35 (2022), 21171â€“21183.
[104] Jianan Zhao, Chaozhuo Li, Qianlong Wen, Yiqi Wang, Yuming Liu, Hao Sun,
Xing Xie, and Yanfang Ye. 2021. Gophormer: Ego-graph transformer for node
classification. arXiv preprint arXiv:2110.13094 (2021).
[105] Wen Zhong, Changxiang He, Chen Xiao, Yuru Liu, Xiaofei Qin, and Zhensheng
Yu. 2022. Long-distance dependency combined multi-hop graph neural networks
for proteinâ€“protein interactions prediction. BMC bioinformatics 23, 1 (2022),
1â€“21.
[106] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
Koutra. 2020. Beyond homophily in graph neural networks: Current limitations
and effective designs. Advances in neural information processing systems 33
(2020), 7793â€“7804.
[107] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and
Xinggang Wang. 2024. Vision mamba: Efficient visual representation learning
with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024).
A Complexity Analysis of GMNs
ğ‘šâ‰¥1. For each node ğ‘£âˆˆğ‘‰, we generate ğ‘€Ã—ğ‘ walks with length
Ë†ğ‘š=1,...,ğ‘š , which requiresO(ğ‘€Ã—ğ‘ Ã—(ğ‘š+1))time. Given ğ¾
tokens, the complexity of bidirectional Mamba is 2Ã—of Mamba [ 28],
which is linear with respect to ğ¾. Accordingly, since we have
O(ğ‘€Ã—ğ‘ Ã—ğ‘š)tokens, the final complexity for a given node ğ‘£âˆˆğ‘‰isO(ğ‘€Ã—ğ‘ Ã—(ğ‘š+1)). Repeating the process for all nodes, the
time complexity is O(ğ‘€Ã—ğ‘ Ã—(ğ‘š+1)Ã—|ğ‘‰|+|ğ¸|), which is lin-
ear in terms of|ğ‘‰|and|ğ¸|(graph size). To compare to the qua-
dratic time complexity of GTs, even for small networks, note that
in practice,ğ‘€Ã—ğ‘ Ã—(ğ‘š+1)â‰ª|ğ‘‰|, and in our experiments usually
ğ‘€Ã—ğ‘ Ã—(ğ‘š+1) â‰¤ 200. Also, note that using MPNN as an op-
tional step cannot affect the time complexity as the MPNN requires
O(|ğ‘‰|+|ğ¸|)time.
ğ‘š=0. In this case, each node is a token and so the GMN requires
O(|ğ‘‰|)time. Using MPNN in the architecture, the time complexity
would beO(|ğ‘‰|+|ğ¸|), dominating by the MPNN time complexity.
As discussed above, based on the properties of Mamba architecture,
longer sequence of tokens (larger value of ğ‘ â‰¥1) can improve the
performance of the method. Based on the abovementioned time
complexity when ğ‘šâ‰¥1, we can see that there is a trade-off between
time complexity and the performance of the model. That is, while
largerğ‘ result in better performance, it results in slower model.
B Theoretical Analysis of GMNs
Theorem 6. With large enough ğ‘€,ğ‘š, andğ‘ >0, GMNsâ€™ sampling is
strictly more expressive than ğ‘˜-hop neighborhood sampling.
Proof. We first show that in this condition, the random walk
neighborhood sampling is as expressive as ğ‘˜-hop neighborhood
sampling. To this end, given an arbitrary small ğœ–>0, we show
that the probability that ğ‘˜-hop neighborhood sampling is more
expressive than random walk neighborhood sampling is less than
ğœ–. Letğ‘š=ğ‘˜,ğ‘ =1, andğ‘ğ‘¢,ğ‘£be the probability that we sample
nodeğ‘£in a walk with length ğ‘š=ğ‘˜starting from node ğ‘¢. This
prbobality is zero if the shortest path of ğ‘¢andğ‘£is more than ğ‘˜.
To construct the subgraph token corresponds to Ë†ğ‘š=ğ‘˜, we useğ‘€
samples and so the probability of not seeing node ğ‘£in these sam-
ples isğ‘ğ‘¢,ğ‘£,ğ‘€ =(1âˆ’ğ‘ğ‘¢,ğ‘£)ğ‘€â‰¤1. Now letğ‘€â†’âˆ andğ‘£âˆˆNğ‘˜(ğ‘¢)
(i.e.,ğ‘ğ‘¢,ğ‘£â‰ 0), we have limğ‘€â†’âˆğ‘ğ‘¢,ğ‘£,ğ‘€ =0. Accordingly, with
large enough ğ‘€, we haveğ‘ğ‘¢,ğ‘£,ğ‘€â‰¤ğœ–. This means that with a large
enoughğ‘€whenğ‘š=ğ‘˜andğ‘ =1, we sample all the nodes within
theğ‘˜-hop neighborhood, meaning that random walk neighborhood
sampling at least provide as much information as ğ‘˜-hop neighbor-
hood sampling with arbitrary large probability.
Next, we provide an example that ğ‘˜-hop neighborhood sampling is
not able to distinguish two non-isomorphism graphs, while random
walk sampling can. Let ğ‘†={ğ‘£1,ğ‘£2,...,ğ‘£â„“}be a set of nodes such
that all nodes have shortest path less than ğ‘˜toğ‘¢. Using hyper-
paramtersğ‘š=ğ‘˜and arbitrary ğ‘€, let the probability that we get
ğº[ğ‘†]as the subgraph token be 1>ğ‘ğ‘†>0. Usingğ‘ samples, the
probability that we do not have ğº[ğ‘†]as one of the subgraph tokens
is(1âˆ’ğ‘ğ‘†)ğ‘ . Now using large ğ‘ â†’âˆ , we have limğ‘ â†’âˆ(1âˆ’ğ‘ğ‘†)ğ‘ =0
and so for any arbitrary ğœ–>0there is a large ğ‘ >0such that we see
all non-empty subgraphs of the ğ‘˜-hop neighborhood with probabil-
ity more than 1âˆ’ğœ–, which is more powerful than the neighborhood.
Note that the above proof does not necessarily guarantee an effi-
cient sampling, but it guarantees the expressive power. â–¡
As discussed earlier, the proof of other theorems can directly be
obtained from the work of Wang and Xue [87]; TÃ¶nshoff et al . [81] ;
and Song et al . [77] . Accordingly, for the sake of space constraint,
the proof of other theorems are in the full version of this paper [ 12].
 
130