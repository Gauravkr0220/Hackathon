Compact Decomposition of Irregular Tensors for Data
Compression: From Sparse to Dense to High-Order Tensors
Taehyung Kwon
KAIST
Seoul, Republic of Korea
taehyung.kwon@kaist.ac.krJihoon Ko
KAIST
Seoul, Republic of Korea
jihoonko@kaist.ac.krJinhong Jung
Soongsil University
Seoul, Republic of Korea
jinhong@ssu.ac.kr
Jun-Gi Jang
UIUC
Champaign, IL, USA
jungi@illinois.eduKijung Shin
KAIST
Seoul, Republic of Korea
kijungs@kaist.ac.kr
ABSTRACT
An irregular tensor is a collection of matrices with different num-
bers of rows. Real-world data from diverse domains, including
medical and stock data, are effectively represented as irregular ten-
sors due to the inherent variations in data length. For their analysis,
various tensor decomposition methods (e.g., PARAFAC2) have been
devised. While they are expected to be effective in compressing
large-scale irregular tensors, akin to regular tensor decomposition
methods, our analysis reveals that their compression performance
is limited due to the larger number of first mode factor matrices.
In this work, we propose accurate and compact decomposition
methods for lossy compression of irregular tensors. First, we pro-
pose Light-IT, which unifies all first mode factor matrices into a
single matrix, dramatically reducing the size of compressed out-
puts. Second, motivated by the success of Tucker decomposition in
regular tensor compression, we extend Light-IT to Light-IT++to
enhance its expressive power and thus reduce compression error.
Finally, we generalize both methods to handle irregular tensors of
any order and leverage the sparsity of tensors for acceleration.
Extensive experiments on 6 real-world datasets demonstrate that
our methods are (a) Compact: their compressed output is up to
37Ã—smaller than that of the most concise baseline, (b) Accurate:
our methods are up to 5 Ã—more accurate, with smaller compressed
output, than the most accurate baseline, and (c) Versatile: our
methods are effective for sparse, dense, and higher-order tensors.
CCS CONCEPTS
â€¢Computing methodologies â†’Factorization methods; â€¢In-
formation systems â†’Data mining.
KEYWORDS
Irregular Tensor, Tensor Decomposition, Data Compression
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671846ACM Reference Format:
Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-Gi Jang, and Kijung Shin.
2024. Compact Decomposition of Irregular Tensors for Data Compression:
From Sparse to Dense to High-Order Tensors. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671846
1 INTRODUCTION
A (regular) tensor is a multi-dimensional array [ 21]. Diverse real-
world datasets, such as image databases [ 35,36], semantic graphs [ 22],
and email histories [ 2], have been represented as tensors due to
their high-dimensional nature. The number of the dimensions of
a tensor is called the order of the tensor. A ğ‘‘-order tensor can be
viewed as a collection of (ğ‘‘âˆ’1)-order tensors of the same size.
Ağ‘‘-order irregular tensor is a collection of(ğ‘‘âˆ’1)-order ten-
sors with potentially varying sizes, specifically in the first mode.
For instance, a 3-order irregular tensor is a collection of matrices
with varying numbers of rows (refer to Fig. 1 for an example from
EHRs [ 28,38]). In many real-world datasets (e.g., EHRs), the lengths
of data (e.g. visits counts) vary depending on objects (e.g. patients),
and thus they have been represented as irregular tensors [11, 16].
Given the immense sizes of real-world irregular tensors, com-
pressing them is often necessary for efficient analysis and storage.
In the case of a dense irregular tensor, storing it, without any com-
pression, requires memory proportional to the number of entries,
and the storage cost grows exponentially to the order of the tensor
since the number of entries in each slice grows exponentially to it.
When saving a spare irregular tensor, only indices and values of
non-zero entries are stored, but still, it can be a memory burden.
For example, the EHR dataset that saves the diagnoses of patients
contains 50 million non-zero entries [29].
For regular tensors (or matrices), a number of compression meth-
ods have been developed based on tensor (or matrix) decomposition.
CUR [ 8] and CMD [ 32] decompositions for matrices decrease the
memory space for the factor matrices when they are applied to
sparse matrices because their factor matrices are sparse and can be
saved in sparse matrix formats such as the COO format. Their com-
pression abilities are better than T-SVD (Truncated Singular Value
Decomposition) [ 13], as empirically shown in [ 24]. NeuKron [ 24]
and TensorCodec [ 25] use neural networks to increase the com-
pression abilities of Kronecker product and Tensor-Train (TT) de-
composition [26], respectively.
 
1451
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-Gi Jang, and Kijung Shin
Electronic Health Records (EHR)VisitDiagnosisA Patient:
VisitDiagnosisB Patient:
VisitDiagnosisC Patient:DiagnosisVisit{Data with
VariousLengths
A 3-order Irregular Tensor
Figure 1: An example of a 3-order irregular tensor.
In contrast, there is no advanced decomposition method special-
ized for the compression of an irregular tensor. PARAFAC2 decom-
position [ 14], known for its utility in phenotype discovery [ 1,29],
fault detection [ 17,37], and tracking usersâ€™ interest [ 33], may be
appealing also for compression purposes. However, our analysis
reveals that it requires a large number of factor matrices for the
first mode, limiting a compression ability. In addition, their design
only targets irregular tensors whose orders are three.
In this paper, we propose Light-IT and Light-IT++, which are
accurate and compact decompositions of irregular tensors for lossy
compression. In Light-IT, multiple factor matrices for the first mode
in PARAFAC2 decomposition are reduced to a single vocabulary
matrix, dramatically decreasing the number of parameters. Note
that how PARAFAC2 decomposition (and Light-IT) approximates
an entry is similar to how CP decomposition [ 15] does. Light-IT++
enhances the expressiveness (and thus reduces the compression
error) of Light-IT by generalizing its approximation scheme to
mimic Tucker decomposition [ 34], renowned for its effectiveness
in regular-tensor compression. Moreover, the designs and imple-
mentations of our methods can handle an irregular tensor whose
order is higher than three. Finally, our methods exploit the sparsity
of a sparse irregular tensor to reduce the time complexity.
We show the following advantages of our methods through
experiments in 6 real-world datasets:
â€¢Compact: Our methods give outputs up to 37 Ã—more concise than
the best baseline with similar fitness (i.e., compression accuracy).
â€¢Accurate: Their fitness is up to 5 times better than the most
accurate baseline with similar compressed output sizes.
â€¢Versatile: Our methods effectively compress various types of
irregular tensors, including sparse, dense, and higher-order ones.
Reproducibility: The code and datasets are available at [23].
We present related works in Section 2 and preliminaries in Sec-
tion 3. Then, we analyze the memory bottleneck of PARAFAC2
decomposition in Section 4. The proposed methods are provided
in Section 5, followed by experiments in Section 6. We finalize our
paper with conclusions in Section 7.
2 RELATED WORK
In this section, we review decomposition-based methods for com-
pressing regular and irregular tensors.
General-purpose regular tensor decompositions: CP (Canoni-
cal Polyadic) [ 15] and Tucker [ 34] decompositions have been widely
used for the factorization of regular tensors, serving general pur-
poses including compression. Those methods decompose a regular
tensor into small factor matrices (including a core tensor in the
case of Tucker), which the original tensor can be approximated
from. However, they cannot be directly applied to irregular tensor
compression due to their designs being focused on regular tensors.
One naive approach is to pad zero-valued rows to transform theirregular tensor into a regular one and then apply the methods.
However, zero-padding introduces incorrect information, and it
can hinder the effective capture of the different semantics of the
first mode indices (e.g., visits in Fig. 1) depending on the final mode
indices (e.g., patients), as empirically verified in Section 6.3.
Regular tensor compressions: For a sparse matrix, CUR [ 8] and
CMD [ 32] compress it into a small dense matrix and tall sparse
factor matrices. These sparse matrices are randomly sampled from
the input, and they are stored in sparse formats further reducing
space costs. Recently, deep-learning-based methods have been in-
troduced to enhance regular tensor compression. NeuKron [ 24]
approximates a sparse and reorderable tensor through the Kro-
necker product of small tensors generated by an RNN, trained with
an ordering optimization to exploit meaningful patterns in the in-
put. TensorCodec [ 25] extends Tensor-Train decomposition [ 26]
by incorporating an RNN. TensorCodec optimizes the RNN while
reordering and folding the input tensor simultaneously. However,
those methods are not suitable for irregular tensor compression,
particularly due to their design limitations in ordering optimization
exclusive for regular tensors and their heavy computations.
General-purpose irregular tensor decompositions: To decom-
pose an irregular tensor, PARAFAC2 decomposition [ 14] has been
introduced, and it factorizes each slice of the irregular tensor into
distinct factor and diagonal matrices depending on the first mode
and a shared factor matrix. Various methods have been proposed
to compute PARAFAC2 decomposition under different settings and
algorithms. PARAFAC2-ALS [ 19] is a representative method ex-
ploiting the alternative least square (ALS) algorithm. SPARTan [ 28]
leverages the sparsity in sparse irregular tensors for better effi-
ciency. To efficiently handle a dense irregular tensor, RD-ALS [ 6]
preprocesses it via a dimension reduction, and DPar2 [ 16] employs
randomized SVD [ 12] with careful ordering of computations. For
interpretability, COPA [ 1] introduces several constraints such as
temporal smoothness, sparsity, and non-negativity in the objective
function of PARAFAC2 decomposition. REPAIR [ 29] is designed
to maintain robustness against missing and erroneous values for
PARAFAC2 decomposition. HyTuck2 [ 4] generalizes Tucker de-
composition for irregular tensors, in a manner similar to how
PARAFAC2 extends CP decomposition. The algorithmic design
of all aforementioned methods is restricted to 3-order irregular ten-
sors, while BTD2 [ 3] is a variant of PARAFAC2 for 4-order irregular
tensors. Although the results of all decompositions above can serve
as irregular tensor compression, it still lacks compactness due to
the dominance of the first mode factor matrices across all slices,
resulting in a substantial space requirement for compressed outputs
(refer to Section 4 for the analysis on PARAFAC2). Furthermore,
our methods can cope with irregular tensors of any order.
3 PRELIMINARIES
3.1 Tensor Notations and Operations
A matrix is denoted by X, and a regular tensor is denoted by X. A
ğ‘‘-order irregular tensor with ğ¾slices is represented as {Xğ‘˜}ğ¾
ğ‘˜=1,
whereXğ‘˜âˆˆRğ‘ğ‘˜Ã—ğ‘€1Ã—Â·Â·Â·Ã—ğ‘€ğ‘‘âˆ’2is theğ‘˜-th slice. The number of
non-zero entries of Xis denoted by ğ‘›ğ‘›ğ‘§(X). Matricization [ 20]
is a process that transforms a tensor into a matrix. The details
of matricization, Kronecker product, and Khatri-Rao product are
 
1452Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Descriptions for notations
Symb
ol Description
XâˆˆRğ‘1Ã—ğ‘2ğ‘1-by-ğ‘2matrix
Xâ€ pseudoinv
erse of X
XâˆˆRğ‘1Ã—Â·Â·Â·Ã—ğ‘ğ‘‘ r
egular tensor
ğ‘›ğ‘›ğ‘§(X) numb
er of non-zero entries in X
X(ğ‘–1,Â·Â·Â·,ğ‘–ğ‘‘)(ğ‘–1,Â·
Â·Â·,ğ‘–ğ‘‘)-th entry ofX
ğ‘£ğ‘’ğ‘(X)âˆˆ Rğ‘1...ğ‘ğ‘‘ v
ectorization ofX
X(ğ‘Ÿ1,Â·Â·Â·,ğ‘Ÿğ¿)mo
de-ğ‘Ÿ1,Â·Â·Â·,ğ‘Ÿğ¿matricization ofX
{Xğ‘˜}ğ¾
ğ‘˜=1irr
egular tensor
Xğ‘˜âˆˆRğ‘ğ‘˜Ã—ğ‘€1Ã—Â·Â·Â·Ã—ğ‘€ğ‘‘âˆ’2slice
tensor
PâˆˆRğ·Ã—ğ‘…v
ocabulary matrix
ğ‘… rank
of our methods
Pğ‘˜ first-mo
de factor matrix constructed from P
Uğ‘˜ temp
orary matrix for Pğ‘˜
Vğ‘–âˆˆRğ‘€ğ‘–Ã—ğ‘…(ğ‘–+1)-th
mode factor matrix (1 â‰¤ğ‘–â‰¤ğ‘‘âˆ’2)
SâˆˆRğ¾Ã—ğ‘…last
mode factor matrix
[ğ‘] set
of integers from 0 to ğ‘âˆ’1
ğœ‹ğ‘˜:[ğ‘ğ‘˜]â†’[ğ·] mappings
for theğ‘˜-th slice
G cor
e tensor
âŠ— Kr
onecker product
âŠ™ Khatri-Rao
product
âˆ¥Â·âˆ¥ğ¹ Fr
obenius norm
available in Appendix A. The Moore-Penrose pseudoivnerse [ 9] of
Xis denoted by Xâ€ . The Frobenius norm of a tensor is defined as
the square root of the square sum of all entries in the tensor. The
frequently used notations are described in Table 1.
3.2 PARAFAC2 Decomposition
Given a 3-order irregular tensor {Xğ‘˜}ğ¾
ğ‘˜=1where Xğ‘˜âˆˆRğ‘ğ‘˜Ã—ğ‘€,
PARAFAC2 decomposition [ 14] compresses Xğ‘˜toUğ‘˜âˆˆRğ‘ğ‘˜Ã—ğ‘…,
SâˆˆRğ¾Ã—ğ‘…, and VâˆˆRğ‘€Ã—ğ‘…whereğ‘…is the rank of the decomposition.
The slice Xğ‘˜is approximated by Uğ‘˜ğ‘‘ğ‘–ğ‘ğ‘”(S(ğ‘˜,:))VâŠºwhere Uğ‘˜is
distinct for each slice index ğ‘˜. Harshman et al. [ 14] introduced an
additional constraint ensuring that UâŠ¤
ğ‘˜Uğ‘˜is the same for all indices
ğ‘˜. This constraint leads to the uniqueness of each Uğ‘˜in the solution
space, and the constraint is satisfied by replacing each Uğ‘˜with
Qğ‘˜Hwhere Qğ‘˜is a column orthonormal matrix with ğ‘…columns.
PARAFAC2 decomposition is formulated as follows:
min
Î˜ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥Xğ‘˜âˆ’Qğ‘˜Hğ‘‘ğ‘–ğ‘ğ‘”(S(ğ‘˜,:))VâŠºâˆ¥2
ğ¹s.t.QâŠ¤
ğ‘˜Qğ‘˜=Iâˆ€ğ‘˜, (1)
where Î˜=
{Qğ‘˜}ğ¾
ğ‘˜=1,S,H,V	
. The representative algorithm for this
optimization problem is PARAFAC2-ALS [ 19], which alternatively
optimizes each term in Î˜while fixing the other parameters.
3.3 Problem Definition
The problem of lossy compression of an irregular tensor, addressed
in this paper, is defined as follows:
Problem 1. (Lossy Compression of an Irregular Tensor)
â€¢Given: an irregular tensor{Xğ‘˜}ğ¾
ğ‘˜=1whereXğ‘˜âˆˆRğ‘ğ‘˜Ã—ğ‘€1Ã—Â·Â·Â·Ã—ğ‘€ğ‘‘âˆ’2,
â€¢Find: the compressed data D
â€¢to Minimize: (1) the size ofD
(2) the approximation errorÃğ¾
ğ‘˜=1âˆ¥Xğ‘˜âˆ’ËœXğ‘˜âˆ¥2
ğ¹,
where ËœXğ‘˜is the reconstruction of the ğ‘˜-th slice.
050100
CMS MIMIC -IIIKorea stock US stock
DatasetsRelativ e Sav ing
Costs (%)1stmode
factor 
matrices
theother 
parameters
90
050100
CMS MIMIC-III Korea stock US stock
DatasetsRelative Saving
Cost (%)Figure 2: The first mode factor matrices of PARAFAC2 decom-
position consume most of the parameter space. Relative sav-
ing cost is defined as 100 Ã—(number of parameters)/(number
of total parameters). The results are the same for all ranks.
For our methods, some components of Dare mappings between
the rows of a vocabulary matrix and the first mode indices of an
irregular tensor. The other components are the vocabulary matrix
and the factor matrices for the modes except for the first mode. In
the case of Light-IT++, a core tensor is also included in D.
4 BOTTLENECK ANALYSIS
In this section, we aim to analyze a limitation of PARAFAC2 decom-
position as a solution of Problem 1. A commonly considered ap-
proach for compressing a 3-order irregular tensor is to use PARAFAC2
decomposition. However, its compression level is very limited due
to the sizeÃğ¾
ğ‘˜=1ğ‘ğ‘–ğ‘…of the first mode factor matrices {Uğ‘˜}ğ¾
ğ‘˜=1
which are much larger than the other factor matrices. Fig. 2 shows
that the number of parameters in {Uğ‘˜}ğ¾
ğ‘˜=1overwhelmingly sur-
passes those in the remaining parameters (S andV) for all datasets.
In light of this result, it is crucial to reduce the size of {Uğ‘˜}ğ¾
ğ‘˜=1to
achieve a higher compression performance. Our proposed Light-
IT and Light-IT++, which are explained in the following sections,
focus on overcoming this limitation without an accuracy loss.
5 PROPOSED METHOD
In this section, we present our proposed methods for Problem 1.
5.1 Overview
The technical challenges in addressing Problem 1 are as follows.
Q1.Compactness. As analyzed in Section 4, the compression
capability of PARAFAC2 decomposition is limited due to the
first mode factor matrices, requiring a substantial compression
size. How can we compress it more compactly?
Q2.Expressiveness. How can we increase the expression power
to fit irregular tensors of more complicated patterns that cannot
be expressed with low-rank PARAFAC2 decomposition?
Q3.Handling large sparse irregular tensors. Sparse irregular
tensors usually have large dimensions with few non-zero en-
tries, requiring much time if treated as dense tensors. How can
we efficiently compress them?
Q4.Handling higher-order irregular tensors. Real-world irreg-
ular tensors often have orders higher than three. How can we
compress such higher-order irregular tensors?
We provide the following solutions to handle the challenges above.
A1.Vocabulary-based compression. We devise a vocabulary-
based compression that maps the rows of the first mode factor
matrices to those of a single compact vocabulary matrix.
A2.Extension with core tensor. We extend the vocabulary-based
compression by incorporating a core tensor, similar to how
Tucker decomposition approximates a target tensor.
 
1453KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-Gi Jang, and Kijung Shin
Vocabulary ğ‘·ğ‘·
Mappings
ğ…ğ…ğŸğŸ,ğ…ğ…ğŸğŸ1storder
factor matrices
: ğ‘·ğ‘·ğŸğŸ,ğ‘·ğ‘·ğŸğŸ3rdorder 
factor 
matrix: ğ‘ºğ‘º2ndorder 
factor 
matrix: ğ‘½ğ‘½Irregular 
tensor: ğ“§ğ“§â‰ˆ
â‰ˆ
Figure 3: Approximation by Light-IT.
A3.Sparse design. We design a sparse version of our methods,
which employs operations involving only non-zero entries.
A4.Higher-order design. We design a higher-order version of
our methods through careful matricizations of irregular tensors
with higher orders.
In the following subsections, we describe our compact and accu-
rate irregular tensor compression methods, Light-IT and Light-IT++.
Light-IT compresses an input irregular tensor with the vocabulary
matrix, thereby reducing the compression size (Section 5.2). To
enhance expressiveness, we extend it to Light-IT++based on the
Tucker decomposition, where the initial parameters of Light-IT++
are set to the outputs of Light-IT (Section 5.3). Efficient designs of
Light-IT and Light-IT++for sparse irregular tensors and extensions
to higher-order irregular tensors are explained in the corresponding
subsections. For simplicity, we initially describe our methods in the
context of a 3-order irregular tensor and subsequently extend the
explanation to higher-order irregular tensors.
5.2 Light-IT: Vocabulary-based Compression
To achieve compact compression, our main idea for Light-IT is to
compress{Uğ‘˜}ğ¾
ğ‘˜=1into a single compact vocabulary matrix Pthat
is partially shared across all slices for the approximation. Specif-
ically, Light-IT learns Uğ‘˜âˆˆRğ‘ğ‘˜Ã—ğ‘…andPâˆˆRğ·Ã—ğ‘…, and mapping
functions{ğœ‹ğ‘˜}ğ¾
ğ‘˜=1are computed from them. Here ğœ‹ğ‘˜:[ğ‘ğ‘˜]â†’[ğ·]
maps each row of Uğ‘˜to a corresponding row in Pso that the ap-
proximation of Uğ‘˜can be constructed from Pandğœ‹ğ‘˜. Note thatğ·
is the number of vocabularies, which is much smaller thanÃ
ğ‘˜ğ‘ğ‘˜.
Suppose Uğ‘˜(ğ‘–,:)is mapped to P(ğœ‹ğ‘˜(ğ‘–),:). Then, we can represent
the approximation of Xğ‘˜(ğ‘–1,ğ‘–2)as follows:
Xğ‘˜(ğ‘–1,ğ‘–2)â‰ˆËœXğ‘˜(ğ‘–1,ğ‘–2)=ğ‘…âˆ‘ï¸
ğ‘Ÿ=1P(ğœ‹ğ‘˜(ğ‘–1),ğ‘Ÿ)S(ğ‘˜,ğ‘Ÿ)V(ğ‘–2,ğ‘Ÿ). (2)
Fig. 3 shows an example of the vocabulary-based approximation
of Light-IT. Thanks to Pand{ğœ‹ğ‘˜}ğ¾
ğ‘˜=1for the first mode, the com-
pressed outputs can be significantly smaller than the outputs of
PARAFAC2 decomposition. The compression result of Light-IT isP,S,V,{ğœ‹ğ‘˜}ğ¾
ğ‘˜=1	where VâˆˆRğ‘€Ã—ğ‘…andSâˆˆRğ¾Ã—ğ‘…are the fac-
tor matrices for the second and final modes, respectively. We use
Huffman encoding to compress the integers in {ğœ‹ğ‘˜}ğ¾
ğ‘˜=1. Refer to
Theorem 1 for the analysis of the size of the compressed result.
Next, we explain the optimization process of Light-IT with its
loss function. The overall training process is described in Algo-
rithm 1. Inspired by the general vocabulary learning technique [ 5],
we design a learning process for the vocabulary-based compression.
Specifically, we co-train the temporary factor matrices {Uğ‘˜}ğ¾
ğ‘˜=1that
aims to guide the training of P, and obtain{ğœ‹ğ‘˜}ğ¾
ğ‘˜=1from{Uğ‘˜}ğ¾
ğ‘˜=1
andPwhenever they are updated. When the training phase is com-
pleted, we discard the temporary matrices {Uğ‘˜}ğ¾
ğ‘˜=1. We define themapping function ğœ‹ğ‘˜as follows:
ğœ‹ğ‘˜(ğ‘–):=argmin
ğ‘—âˆ¥Uğ‘˜(ğ‘–,:)âˆ’P(ğ‘—,:)âˆ¥2
ğ¹. (3)
Derived from Eq. (2), the optimization problem of Light-IT is repre-
sented as follows:
min
Î˜ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜âˆ‘ï¸
ğ‘–=1âˆ¥Xğ‘˜(ğ‘–,:)âˆ’P(ğœ‹ğ‘˜(ğ‘–),:) VâŠ™S(ğ‘˜,:)âŠºâˆ¥2
ğ¹, (4)
where Î˜=
P,{Uğ‘˜}ğ¾
ğ‘˜=1,V,S	
. However, Eq. (4) is not differentiable
because it involves discrete functions such as the indexing operation
inP(ğœ‹ğ‘˜(ğ‘–),:)and the argmin function in Eq. (3). To employ gradient
descents, we use the Pâ€²
ğ‘˜(ğ‘–,:)in place of P(ğœ‹ğ‘˜(ğ‘–),:)in Eq. (4), where
Pâ€²
ğ‘˜(ğ‘–,:)is defined as follows:
Pâ€²
ğ‘˜(ğ‘–,:)=Uğ‘˜(ğ‘–,:)âˆ’ğ‘ ğ‘”(Uğ‘˜(ğ‘–,:)âˆ’P(ğœ‹ğ‘˜(ğ‘–),:)), (5)
whereğ‘ ğ‘”(Â·)is the stop gradient operator which is an identity func-
tion in the forward pass but drops the gradient for the input during
the backward pass. Thus, Pâ€²
ğ‘˜(ğ‘–,:)becomes P(ğœ‹ğ‘˜(ğ‘–),:)in the for-
ward pass but only backpropagates to Uğ‘˜(ğ‘–,:), computing gradi-
ents for Uğ‘˜. Additionally, we add a regularization term denoted by
âˆ¥P(ğ‘ ğ‘”(ğœ‹ğ‘˜(ğ‘–)),:)âˆ’ğ‘ ğ‘”(Uğ‘˜(ğ‘–,:))âˆ¥2
ğ¹into the loss in order to update the
vocabulary matrix P. To sum up, the loss function Lfor Light-IT is
L=ğ¾âˆ‘ï¸
ğ‘˜=1
âˆ¥Xğ‘˜âˆ’Pâ€²
ğ‘˜(VâŠ™S(ğ‘˜,:))âŠºâˆ¥2
ğ¹+âˆ¥Pğ‘˜âˆ’ğ‘ ğ‘”(Uğ‘˜)âˆ¥2
ğ¹
, (6)
where theğ‘–-th row of Pğ‘˜isP(ğ‘ ğ‘”(ğœ‹ğ‘˜(ğ‘–)),:), and Pâ€²
ğ‘˜(VâŠ™S(ğ‘˜,:))âŠº
is the reconstruction result ËœXğ‘˜. Light-IT optimizes its parameters
through gradient descent, minimizing the loss in Eq. (6). Note that
under the vocabulary learning mechanism [ 5], the matrices Uğ‘˜
andPare updated at the same time by a gradient descent, and
the mapping function ğœ‹ğ‘˜is determined by them to minimize the
difference (loss function in Eq. (3)) between Uğ‘˜andPğ‘˜.
Handling sparse irregular tensors: The time required to com-
pute Eq. (6), which is the main bottleneck of Light-IT, is propor-
tional to the number of entries in {Xğ‘˜}ğ¾
ğ‘˜=1becauseÃ
ğ‘˜âˆ¥Xğ‘˜âˆ’ËœXğ‘˜âˆ¥2
ğ¹
measures the approximation error for all entries, where ËœXğ‘˜=
Pâ€²
ğ‘˜(VâŠ™S(ğ‘˜,:))âŠº. However, for a sparse irregular tensor, it can
be time-consuming to compute an error Xğ‘˜(ğ‘–,ğ‘—)âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2for
every zero entry as most of the entries that exist of the sparse
tensor are zero. Thus, we propose a sparse design that efficiently
computes the loss in Eq. (6). For this, we first carefully reorganize
the computations inÃ
ğ‘˜âˆ¥Xğ‘˜âˆ’ËœXğ‘˜âˆ¥2
ğ¹. The approximation error of
each sparse slice Xğ‘˜is as follows:
âˆ¥Xğ‘˜âˆ’ËœXğ‘˜âˆ¥2
ğ¹=ğ‘ğ‘˜âˆ‘ï¸
ğ‘–=1ğ‘€âˆ‘ï¸
ğ‘—=1
Xğ‘˜(ğ‘–,ğ‘—)âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2
=âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆÎ©ğ‘˜ËœXğ‘˜(ğ‘–,ğ‘—)2+âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆÎ©ğ‘
ğ‘˜
Xğ‘˜(ğ‘–,ğ‘—)âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2
=âˆ¥ËœXğ‘˜âˆ¥2
ğ¹+âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆÎ©ğ‘
ğ‘˜ Xğ‘˜(ğ‘–,ğ‘—)âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2
,(7)
where Î©ğ‘˜andÎ©ğ‘
ğ‘˜are the sets of indices of zero and non-zero entries
inXğ‘˜, respectively. Then, based on the decomposed results and
Lemma 1,âˆ¥ËœXğ‘˜âˆ¥2
ğ¹is represented as follows:
âˆ¥ËœXğ‘˜âˆ¥2
ğ¹=ğ‘…âˆ‘ï¸
ğ‘Ÿ1=1ğ‘…âˆ‘ï¸
ğ‘Ÿ2=1S(ğ‘˜,ğ‘Ÿ1)S(ğ‘˜,ğ‘Ÿ2)
Pâ€²
ğ‘˜(:,ğ‘Ÿ1)âŠºPğ‘˜(:,ğ‘Ÿ2)
V(:,ğ‘Ÿ1)âŠºV(:,ğ‘Ÿ2)
,(8)
 
1454Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Lemma 1.âˆ¥Xâ€²
ğ‘˜âˆ¥2
ğ¹is equal to Eq. (8)
Proof. Assuming that the vectorization of Xâ€²
ğ‘˜is denoted by
ğ‘£ğ‘’ğ‘(Xâ€²
ğ‘˜), the following equations hold:
âˆ¥Xâ€²
ğ‘˜âˆ¥2
ğ¹=ğ‘£ğ‘’ğ‘(Xâ€²
ğ‘˜)âŠºğ‘£ğ‘’ğ‘(Xâ€²
ğ‘˜)
=ğ‘…âˆ‘ï¸
ğ‘Ÿ1=1(Pâ€²
ğ‘˜(:,ğ‘Ÿ1)âŠ™V(:,ğ‘Ÿ1))S(ğ‘˜,ğ‘Ÿ1)âŠºğ‘…âˆ‘ï¸
ğ‘Ÿ2=1(Pâ€²
ğ‘˜(:,ğ‘Ÿ2)âŠ™V(:,ğ‘Ÿ2))S(ğ‘˜,ğ‘Ÿ2)
=ğ‘…âˆ‘ï¸
ğ‘Ÿ1=1ğ‘…âˆ‘ï¸
ğ‘Ÿ2=1 
S(ğ‘˜,ğ‘Ÿ1)S(ğ‘˜,ğ‘Ÿ2)
Pâ€²
ğ‘˜(:,ğ‘Ÿ1)âŠºPğ‘˜(:,ğ‘Ÿ2)
V(:,ğ‘Ÿ1)âŠºV(:,ğ‘Ÿ2)!
.
(9)
The final formula in Eq. (9) is the same with Eq. (8). â–¡
Note that computing the second term of Eq. (7)takesğ‘‚(ğ‘›ğ‘›ğ‘§(Xğ‘˜))
time. The first term, âˆ¥ËœXğ‘˜âˆ¥2
ğ¹, takesğ‘‚ (ğ‘ğ‘˜+ğ‘€)ğ‘…2time to compute
by Eq. (8), whereğ‘…is the rank, and ğ‘€is the second mode length of
Xğ‘˜, which is much smaller ğ‘‚(ğ‘ğ‘˜ğ‘€ğ‘…)time for naively reconstruct-
ing all entries in Xğ‘˜. Thus, using Eq. (7)and(8), we can efficiently
calculateÃ
ğ‘˜âˆ¥Xğ‘˜âˆ’ËœXğ‘˜âˆ¥2
ğ¹for a large sparse irregular tensor. Con-
sidering this, we analyze the time complexity of Light-IT for sparse
irregular tensors in Theorem 4 in Section 5.4.
Handling higher-order irregular tensors: To handle an irregu-
lar tensor{Xğ‘˜}ğ¾
ğ‘˜=1with an order ğ‘‘higher than three, we matricize
each sliceXğ‘˜and its approximation ËœXğ‘˜along the first mode into
X(1)
ğ‘˜andËœX(1)
ğ‘˜, respectively. We then use the loss in Eq. (6)with the
mode-1 matricizations to update the parameters. As the order ğ‘‘is
higher, we have more order-related factor matrices {Vğ‘–}ğ‘‘âˆ’2
ğ‘–=1, and
each entry of the slice Xğ‘˜is approximated as follows:
Xğ‘˜(ğ‘–1,Â·Â·Â·,ğ‘–ğ‘‘âˆ’1)â‰ˆğ‘…âˆ‘ï¸
ğ‘Ÿ=1P(ğœ‹ğ‘˜(ğ‘–1),ğ‘Ÿ)V1(ğ‘–2,ğ‘Ÿ)Â·Â·Â· Vğ‘‘âˆ’2(ğ‘–ğ‘‘âˆ’1,ğ‘Ÿ)S(ğ‘˜,ğ‘Ÿ).(10)
Then, we replace the terms in Eq. (6)as follows: Xğ‘˜â†X(1)
ğ‘˜and
ËœXğ‘˜â†ËœX(1)
ğ‘˜=Pâ€²
ğ‘˜ (âŠ™ğ‘‘âˆ’2
ğ‘–=1Vğ‘–)âŠ™S(ğ‘˜,:)âŠº, which enables Light-IT
to handle higher-order irregular tensors. For the sparse design,
we replace V(:,ğ‘Ÿ1)âŠºV(:,ğ‘Ÿ2)withÃğ‘‘âˆ’2
ğ‘–=1Vğ‘–(:,ğ‘Ÿ1)âŠºVğ‘–(:,ğ‘Ÿ2)in Eq. (8).
Refer to the full equations for this in Appendix B.1.
5.3 Light-IT++: Extension with Core Tensor
We propose Light-IT++, an expressive irregular tensor decompo-
sition that achieves higher accuracy than Light-IT with little ad-
ditional space. In Eq. (2), the PARAFAC2-based computation has
limited expressiveness since it fails to capture relationships across
multiple latent dimensions; for example, there is no multiplica-
tion between P(ğœ‹ğ‘˜(ğ‘–1),ğ‘Ÿ1),V(ğ‘–2,ğ‘Ÿ2), and S(ğ‘˜,ğ‘Ÿ 3)(ğ‘Ÿ1â‰ ğ‘Ÿ2â‰ ğ‘Ÿ3). To
increase the expression power, our idea is to incorporate a core
tensorGâˆˆRğ‘…Ã—ğ‘…Ã—ğ‘…into the compressed outputs and employ an
approximation method used in Tucker decomposition. As a result,
the approximation formula becomes as follows:
Xğ‘˜(ğ‘–,ğ‘—)â‰ˆğ‘…âˆ‘ï¸
ğ‘Ÿ1=1ğ‘…âˆ‘ï¸
ğ‘Ÿ2=1ğ‘…âˆ‘ï¸
ğ‘Ÿ3=1G(ğ‘Ÿ1,ğ‘Ÿ2,ğ‘Ÿ3)P(ğœ‹ğ‘˜(ğ‘–1),ğ‘Ÿ1)V(ğ‘—,ğ‘Ÿ2)S(ğ‘˜,ğ‘Ÿ3).(11)
1We use the process of earning Qğ‘˜in PARAFAC2-ALS illustrated in [ 28] for the faster
training of the algorithm. After computing Qğ‘˜, we set Ağ‘˜toQğ‘˜Hwhere His sampled
randomly form [0, 1). It allows AâŠº
ğ‘˜Ağ‘˜to be HâŠºHfor allğ‘˜in[ğ¾], helping Light-IT share
the first mode factor matrices along the slices.
2If the given tensor is dense, we set ğ‘Ÿ=0.1; otherwise, ğ‘Ÿ=0.01.Algorithm 1: Compression process of Light-IT
Input: an irregular tensor {Xğ‘˜}ğ¾
ğ‘˜=1, a numberğ¸of epochs.
Output: mappings{ğœ‹}ğ¾
ğ‘˜=1, factor matrices P,V, and S.
11Initialize Uğ‘˜
22Initialize P,V, and Sto random values in [0,ğ‘Ÿ)
3forğ‘’â†1toğ¸do
//Update the mappings ğœ‹ğ‘˜
4 forğ‘˜â†1toğ¾do
5 forğ‘–â†1toğ‘ğ‘˜do
6 ğœ‹ğ‘˜(ğ‘–)â† argminğ‘—âˆ¥Uğ‘˜(ğ‘–,:)âˆ’P(ğ‘—,:)âˆ¥2
ğ¹
7 Pğ‘˜(ğ‘–,:)â† P(ğœ‹ğ‘˜(ğ‘–),:)
8 Pâ€²
ğ‘˜â†Uğ‘˜âˆ’ğ‘ ğ‘”(Uğ‘˜âˆ’Pğ‘˜)
//Update the matrices P,V, and S
9 if{Xğ‘˜}ğ¾
ğ‘˜=1is dense then
10Lis computed by Eq. (6)
11 else if{Xğ‘˜}ğ¾
ğ‘˜=1is sparse then
12Lâ†Ãğ¾
ğ‘˜=1âˆ¥ËœXğ‘˜âˆ¥2
ğ¹based on Eq. (8)
//Calculate the loss terms for non-zero entries.
13 forğ‘˜â†1toğ¾do
14 for(ğ‘–,ğ‘—)âˆˆ{(ğ‘–â€²,ğ‘—â€²)|Xğ‘˜(ğ‘–â€²,ğ‘—â€²)â‰ 0}do
15Lâ†L+ Xğ‘˜(ğ‘–,ğ‘—)âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2
16 forğ‘˜â†1toğ¾do
17Lâ†L+âˆ¥ Pğ‘˜âˆ’ğ‘ ğ‘”(Uğ‘˜)âˆ¥2
ğ¹
18 BackpropagateLand update P,Uğ‘˜,V, and S
19return{ğœ‹}ğ¾
ğ‘˜=1,P,V, and S
Note that ifGis a diagonal tensor whose diagonal entries are all 1,
Eq. (11) is equivalent to Eq. (2). Thus, all irregular tensors generated
by Light-IT can be generated by Light-IT++of the same rank. Light-
IT++can fit an irregular tensor with a more complex pattern by
learning the relationships between the latent dimensions (columns)
ofPğ‘˜,V, and Sin the form ofG. The approximation process of
Light-IT++is illustrated in Fig. 4.
The optimization process for Light-IT++is described in Algo-
rithm 2. We use the loss function as the sum of the squared errors
between the input values and their approximations in Eq. (11) (see
Appendix B.2 for the detailed equation). In the training phase of
Light-IT++, we initialize the factor matrices P,V, and Sto the cor-
responding outputs of Light-IT. The core tensor Gis initialized
to a diagonal tensor whose diagonal entries are all one, and the
mappings{ğœ‹ğ‘˜}ğ¾
ğ‘˜=1from Light-IT are used without changes. To up-
date the parameters G,P,V, and S, we employ the alternating least
square (ALS) method, which alternatively optimizes each parameter
in turn while fixing the others.3
UpdatingG:In the ALS scheme, the optimization problem for the
core tensorGis represented as follows:
min
Gğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥XâŠº
ğ‘˜âˆ’VG(2) Pğ‘˜âŠ—S(ğ‘˜,:)âŠºâˆ¥2
ğ¹, (12)
whereâŠ—denotes Kronecker Product. Note that Pğ‘˜is a matrix built
from Pandğœ‹ğ‘˜where Pğ‘˜(ğ‘–,:)isP(ğœ‹ğ‘˜(ğ‘–),:). This is a least squares
problem, and the following solution is obtained analytically by
3The update order is motivated by the HOOI algorithm [7].
 
1455KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-Gi Jang, and Kijung Shin
Irregular 
tensor: ğ“§ğ“§
Mappings
ğ…ğ…ğŸğŸ,ğ…ğ…ğŸğŸ
Core tensor: ğ“–ğ“–2ndorder 
factor 
matrix: ğ‘½ğ‘½
1strow of the 3rd
order factor 
matrix: ğ‘ºğ‘º(ğŸğŸ,:)1storder
factor matrices
: ğ‘·ğ‘·ğŸğŸ,ğ‘·ğ‘·ğŸğŸ
â‰ˆ
â‰ˆ
Figure 4: Approximation by Light-IT++
setting its gradient to zero:
G(2)â†(VâŠºV)â€ 
VâŠºğ¾âˆ‘ï¸
ğ‘˜=1XâŠº
ğ‘˜ Pğ‘˜âŠ—S(ğ‘˜,:)ğ¾âˆ‘ï¸
ğ‘˜=1PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:)â€ 
,
(13)whereâ€ denotes the pseudoinverse of a matrix.
Updating P:We approach the problem for each row of P. Assuming
that each element (ğ‘—,ğ‘˜)in a setğ‘‡ğ‘–satisfiesğœ‹ğ‘˜(ğ‘—)=ğ‘–(i.e. theğ‘—-th
row of Xğ‘˜is mapped to the ğ‘–-th row of Pby Light-IT). The problem
for theğ‘–-th row of Pis formulated as follows:
min
P(ğ‘–,:)âˆ‘ï¸
(ğ‘—,ğ‘˜)âˆˆğ‘‡ğ‘–âˆ¥Xğ‘˜(ğ‘—,:)âˆ’P(ğ‘–,:)G(1) VâŠ—S(ğ‘˜,:)âŠºâˆ¥2
ğ¹. (14)
This is also a least square problem, and the update formula for the
ğ‘–-th row of Pis as follows:
P(ğ‘–,:)â†âˆ‘ï¸
(ğ‘—,ğ‘˜)âˆˆğ‘‡ğ‘–Xğ‘˜(ğ‘—,:) VâŠ—S(ğ‘˜,:)G(1)âŠº
Ã—
G(1)
(VâŠºV)âŠ— âˆ‘ï¸
(ğ‘—,ğ‘˜)âˆˆğ‘‡ğ‘–S(ğ‘˜,:)âŠºS(ğ‘˜,:)
G(1)âŠºâ€ 
.(15)
Updating V:We use the objective function in Eq. (12) but minimize
the function with respect to Vinstead ofGwhen updating V. The
solution on Vis represented as follows:
Vâ†ğ¾âˆ‘ï¸
ğ‘˜=1XâŠº
ğ‘˜ Pğ‘˜âŠ—S(ğ‘˜,:)G(2)âŠº
G(2)ğ¾âˆ‘ï¸
ğ‘˜=1PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:)
G(2)âŠºâ€ 
.
(16)
Updating S:The problem for Sis defined in a row-wise manner
after vectorizing the slices and their approximations:
min
S(ğ‘˜,:)âˆ¥ğ‘£ğ‘’ğ‘(Xğ‘˜)âˆ’S(ğ‘˜,:)G(3)(Pğ‘˜âŠ—V)âŠºâˆ¥2
ğ¹. (17)
The optimal solution of the least square problem in Eq. (17)is
represented as follows:
S(ğ‘˜,:)â†
ğ‘£ğ‘’ğ‘(Xğ‘˜)(Pğ‘˜âŠ—V)G(3)âŠº
G(3)(PâŠº
ğ‘˜Pğ‘˜âŠ—VâŠºV)G(3)âŠºâ€ 
.(18)
Handling sparse irregular tensors: In the aforementioned up-
date formulas, one of the most dominant parts is the multipli-
cation of a slice and the result of Kronecker product, including
XâŠº
ğ‘˜(Pğ‘˜âŠ—S(ğ‘˜,:))in Eq. (13) and (16), Xğ‘˜(ğ‘—,:)(VâŠ—S(ğ‘˜,:))in Eq. (15),
andğ‘£ğ‘’ğ‘(Xğ‘˜)(Pğ‘˜âŠ—V)in Eq. (18). Below, we describe a way of effi-
ciently computing the term XâŠº
ğ‘˜(Pğ‘˜âŠ—S(ğ‘˜,:)); the other terms can
be computed efficiently in the same manner.
Computing XâŠº
ğ‘˜(Pğ‘˜âŠ—S(ğ‘˜,:))requires operations whose number is
linear in the number of entries in Xğ‘˜. We reduce the cost to be linear
in the number of non-zeros in Xğ‘˜by conducting computations in
units of a single entry in Xğ‘˜. LetBğ‘˜denote a matrix of size ğ‘€Ã—ğ‘…2,
which is initialized to a zero matrix but will eventually become
XâŠº
ğ‘˜(Pğ‘˜âŠ—S(ğ‘˜,:)). An entry Xğ‘˜(ğ‘–,ğ‘—)contributes to Bğ‘˜as follows:
Bğ‘˜(ğ‘—,:)â† Bğ‘˜(ğ‘—,:)+Xğ‘˜(ğ‘–,ğ‘—) Pğ‘˜(ğ‘–,:)âŠ—S(ğ‘˜,:). (19)Algorithm 2: Compression process of Light-IT++
Input: an irregular tensor {Xğ‘˜}ğ¾
ğ‘˜=1
Output: mappings{ğœ‹}ğ¾
ğ‘˜=1, a vocabulary matrix P,
factor matrices VandS, a core-tensorG
1Initialize{ğœ‹}ğ¾
ğ‘˜=1,P,V, and Swith Algorithm 1
24InitializeG
3while fitness does not converge do
4 UpdateGwith Eq. (13)
5 forğ‘–â†1toğ·do
6 Update P(ğ‘–,:)with Eq. (15)
7 UpdateGwith Eq. (13)
8 Update Vwith Eq. (16)
9 UpdateGwith Eq. (13)
10 forğ‘–â†1toğ¾do
11 Update S(ğ‘–,:)with Eq. (18)
12 fitnessâ†1âˆ’âˆšï¸ƒÃğ¾
ğ‘˜=1âˆ¥Xğ‘˜âˆ’Pğ‘˜G(1)(VâŠ—S(ğ‘˜,:))âˆ¥2
ğ¹âˆšï¸ƒÃğ¾
ğ‘˜=1âˆ¥Xğ‘˜âˆ¥2
ğ¹
13return{ğœ‹}ğ¾
ğ‘˜=1,P,V,S, andG
Note that Bğ‘˜becomes XâŠº
ğ‘˜(Pğ‘˜âŠ—S(ğ‘˜,:))if we perform Eq. (19)for all
entries in Xğ‘˜. Therefore, only non-zero entries of Xğ‘˜are engaged
in the computation, and the time complexity becomes linear in
ğ‘‚(ğ‘›ğ‘›ğ‘§(Xğ‘˜)). Refer to Theorem 6 for the detailed time complexity.
Handling higher-order irregular tensors: For a higher-order ir-
regular tensor, an entry Xğ‘˜(ğ‘–1,...,ğ‘–ğ‘‘âˆ’1)is approximated as follows:
ğ‘…âˆ‘ï¸
ğ‘Ÿ1=1Â·Â·Â·ğ‘…âˆ‘ï¸
ğ‘Ÿğ‘‘=1
G(ğ‘Ÿ1,...,ğ‘Ÿğ‘‘)P(ğœ‹ğ‘˜(ğ‘–1),ğ‘Ÿ1)ğ‘‘âˆ’1Ã–
ğ‘—=2Vğ‘—âˆ’1(ğ‘–ğ‘—,ğ‘Ÿğ‘—)S(ğ‘˜,ğ‘Ÿğ‘‘)
.(20)
Regarding the problem formulations of Light-IT++, we matricize
the sliceXğ‘˜and its approximation ËœXğ‘˜with the decomposed results
of Light-IT++. All problems are least square problems when viewed
from the row of a factor matrix or a factor matrix perspective, and
the analytic solutions for them exist.
For example, when updating Vğ‘–, we matricize the slices and their
approximations along the ğ‘–-th mode, and formulate the problem on
Vğ‘–as follows:
min
Vğ‘–ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥X(ğ‘–)
ğ‘˜âˆ’Vğ‘–G(ğ‘–) Pğ‘˜âŠ—S(ğ‘˜,:)âŠ—(âŠ—ğ‘—â‰ ğ‘–Vğ‘—)âŠºâˆ¥2
ğ¹. (21)
By solving the problem in Eq. (21), the update formula on Vğ‘–is
given as follows:
Vğ‘–â†ğ¾âˆ‘ï¸
ğ‘˜=1X(ğ‘–)
ğ‘˜ Pğ‘˜âŠ—S(ğ‘˜,:)âŠ—(âŠ—ğ‘—â‰ ğ‘–Vğ‘—)G(ğ‘–)âŠº
Ã—
G(ğ‘–) ğ¾âˆ‘ï¸
ğ‘˜=1PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:)âŠ— âŠ—ğ‘—â‰ ğ‘–VâŠº
ğ‘—Vğ‘—
G(ğ‘–)âŠºâ€ 
.
(22)
Due to the lack of space, we provide all details of the problems and
update formulas for G,PandSin Appendix B.3.
5.4 Theoretical Analysis
We analyze the size of compressed outputs and compression time
for our methods. The size of Xğ‘˜in{Xğ‘˜}ğ¾
ğ‘˜=1isğ‘ğ‘˜Ã—ğ‘€1Ã—Â·Â·Â·Ã—ğ‘€ğ‘‘âˆ’2
4The tensorGis set to a diagonal tensor whose diagonal entries are all 1
 
1456Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
LIGHT-IT (Proposed) LIGHT-IT++(Proposed) BTD2   LIGHT-IT (Proposed) LIGHT-IT++(Proposed) SPARTan COPA        PARAFAC2 -ALS        DPar2        RD -ALS        REPAIR        HyTuck2        BTD2   
24.95x 5.00x
0.10.20.30.4
222224226228
Compressed Size (Bytes)Fitness
(
a)CMS
10.27x2.05x
0.080.120.160.200.24
220222224226
Compressed Size (Bytes)Fitness (b)MIMIC-III
36.55x
2.70x
0.20.30.40.5
222224226228
Compressed Size (Bytes)Fitness (c)Korea-stock
28.47x
2.73x
0.20.30.40.5
222224226228
Compressed Size (Bytes)Fitness (d)US-stock
2.03.04.05.0
217218219220
) Bytes( Compressed Size FitnessO.O.M2.03.04.05.0
217218219220
) Bytes( Compressed Size Fitness (e)Enron
005.0010.0015.0020.0025.0
221222223
) Bytes( Compressed Size FitnessO.O.M (f)Delicious
Figure 5: Light-IT and Light-IT++concisely and accurately compress irregular tensors. Notably, the output size of Light-IT++is
up to 37Ã— smaller than that of the most compact baseline, showing a similar fitenss. Light-IT++shows up to 5Ã— higher fitness
than the most accurate baseline while providing smaller compressed outputs. Note that within each plot, data points closer to
the upper left corner indicate better trade-offs between compressed sizes and fitness.
and the maximum values among {ğ‘ğ‘˜}ğ¾
ğ‘˜=1isğ‘ğ‘šğ‘ğ‘¥. We denote the
rank of our method to ğ‘…and the size of the vocabulary to ğ·. The
number of non-zero entries in Xğ‘˜is denoted by ğ‘›ğ‘›ğ‘§(Xğ‘˜). Proofs of
all theorems are available in Section 2 of Online Appendix [23].
Size of compressed outputs: The compressed output sizes of our
methods are described in Theorems 1 and 2. It is worth noting that
the output size ğ‘‚(logğ·Ãğ¾
ğ‘˜=1ğ‘ğ‘˜)of Huffman encoding is empir-
ically much smaller than the size ğ‘‚(Ãğ¾
ğ‘˜=1ğ‘ğ‘˜ğ‘…)of the first mode
factor matrices in PARAFAC2 decomposition.
Theorem 1 (Compressed size of Light-IT). The compressed size
of Light-IT is ğ‘‚(ğ‘…(ğ·+Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–+ğ¾)+logğ·Ãğ¾
ğ‘˜=1ğ‘ğ‘˜).
Theorem 2 (Compressed size of Light-IT++).The compressed
size of Light-IT++isğ‘‚(ğ‘…(ğ·+Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–+ğ¾)+logğ·Ãğ¾
ğ‘˜=1ğ‘ğ‘˜+ğ‘…ğ‘‘).
Compression time: We analyze the time for a single epoch of
Light-IT and Light-IT++. Note that there are terms that include the
number of entries (i.e.Ãğ¾
ğ‘˜=1ğ‘ğ¾Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–) in the complexities of the
methods for dense irregular tensors and the terms that include the
number of non-zero entries (i.e.Ãğ¾
ğ‘˜=1ğ‘›ğ‘›ğ‘§(Xğ‘˜)) in the complexities
of the methods for spares irregular tensors.
Theorem 3 (Compression time of Light-IT). The compression
time of Light-IT is ğ‘‚ (Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–)(Ãğ¾
ğ‘˜=1ğ‘ğ‘˜)ğ‘…+(Ãğ¾
ğ‘˜=1ğ‘ğ‘˜)ğ·ğ‘….
Theorem 4 (Compression time of Light-IT for sparse irregu-
lar tensors). The compression time of the spares version of Light-IT
isğ‘‚ (Ãğ¾
ğ‘˜=1ğ‘ğ‘˜+Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–)ğ‘…2+Ãğ¾
ğ‘˜=1ğ‘›ğ‘›ğ‘§(Xğ‘˜)ğ‘‘ğ‘…+(Ãğ¾
ğ‘˜=1ğ‘ğ‘˜)ğ·ğ‘….
Theorem 5 (Compression time of Light-IT++).The compres-
sion time of Light-IT++isğ‘‚ ğ‘…3(ğ‘‘âˆ’2)+ğ¾ğ‘…4+ğ‘…6+ğ‘…2ğ‘‘âˆ’1ğ·+ğ‘‘(Ãğ¾
ğ‘˜=1ğ‘ğ‘˜)Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–)ğ‘…ğ‘‘âˆ’1+ğ¾ğ‘…2ğ‘‘âˆ’1.
Theorem 6 (Compression time of Light-IT++for spare irreg-
ular tensors). The compression time of the sparse version of Light-
IT++isğ‘‚ Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–ğ‘…4+Ãğ¾
ğ‘˜=1ğ‘›ğ‘›ğ‘§(Xğ‘˜)ğ‘…ğ‘‘ğ‘‘+ğ‘…3(ğ‘‘âˆ’2)+(Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–)ğ‘…ğ‘‘+
ğ¾ğ‘…4+ğ‘…6+ğ‘…2ğ‘‘âˆ’1ğ·+Ãğ‘‘âˆ’2
ğ‘–=1ğ‘€ğ‘–ğ¾ğ‘…ğ‘‘+ğ‘‘(Ãğ¾
ğ‘˜=1ğ‘ğ‘˜)ğ‘…2+ğ¾ğ‘…2ğ‘‘âˆ’1.
6 EXPERIMENTS
We conduct experiments to answer the following questions.
Q1.Compression Performance: How do Light-IT and Light-IT++
compactly and accurately compress irregular tensors compared
to their competitors?Table 2: Statistics of real-world datasets.
Name ğ‘ğ‘š
ğ‘ğ‘¥ğ‘ğ‘
ğ‘£ğ‘”Size
(
except the 1stmode)Or
der Density
CMS 175 35.4 284Ã—91,586 3 5.01Ã—10âˆ’3
MIMIC-III 280 12.3 1,000Ã—37,163 3 7.33Ã—10âˆ’3
Korea-stock 5,270 3696.5 88Ã—1,000 3 9.98Ã—10âˆ’1
US-stock 7,883 3192.6 88Ã—1,000 3 1
Enron 554 80.6 1,000Ã—1,000Ã—939 4 6.93Ã—10âˆ’5
Delicious 312 16.4 1,000Ã—1,000Ã—31,311 4 3.97Ã—10âˆ’6
Q2.Ablation Study: How does each component of our methods
affect the accuracy and time?
Q3.Scalability: Is the compression time of our methods linearly
scalable to the number of (non-zero) entries in the input tensor?
Q4.Total Compression Time: How efficiently do our methods
compress an irreuglar tensor in terms of compression speed?
Q5.Vocabulary Size: What is the optimal size of the vocabulary?
Our answer to Q5 can be found in Section 3 of Online Appendix [ 23].
6.1 Experimental Specifications
Machines: All experiments except the ablation study and scalabil-
ity test were conducted on a workstation having an RTX 3090Ti
GPU and 128GB RAM. Our methods for the ablation study and
scalability test in Sections 6.3 and 6.4 were executed on a machine
with an RTX 2080Ti GPU and 128GB RAM. The competitors that
do not require a GPU were executed on a machine with an i5-9600K
(6 cores) and 64GB RAM. The accuracies and compression ratios of
the methods are independent of the machineâ€™s specifications.
Competitors: We use 8 state-of-the-art methods for irregular ten-
sor decomposition as our competitors. PARAFAC2-ALS [ 19], RD-
ALS [ 6], DPar25[16], and HyTuck26[4] are targeting at 3-order
dense irregular tensors; and COPA7[1], SPARTan8[28], and RE-
PAIR9[29] are tailored for 3-order sparse irregular tensors. BTD2 [ 3]
is a method for 4-order dense irregular tensors. Refer to Section 2
for the explanations of the competitors. Since the official imple-
mentations of PARAFAC2-ALS, RD-ALS, and BTD2 have not been
released, we implemented them ourselves. For the other competi-
tors, we used the open-sourced implementations provided by the
authors. All of them were implemented in MATLAB R2020a.
5https://datalab.snu.ac.kr/dpar2
6https://www.cs.ucr.edu/ epapalex/src/HyTuck2-public-code.zip
7https://github.com/aafshar/COPA
8https://github.com/kperros/SPARTan
9https://github.com/Emory-AIMS/Repair/tree/master
 
1457KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-Gi Jang, and Kijung Shin
LIGHT-IT (Proposed) LIGHT-IT++(Proposed ) LIGHT-IT-L CP        Tucker
0.10.2
MethodsFitness
(a)MIMIC-III
0.150.300.45
MethodsFitness (b)US-stock
0.010.02
MethodsFitness (c)Delicious
0.000.150.30
MethodsFitness
(d)CMS
0.20.4
MethodsFitness (e)Korea-stock
0.150.300.45
MethodsFitness (f)Enron
Figure 6: Our ideas of vocabulary-based compression and
extension with core tensor are effective for compression.
Datasets: We used 6 public real-world datasets whose statistics are
summarized in Table 2. Note that the datasets include sparse, dense,
and higher-order irregular tensors. Refer to Appendix C.3 for the
semantics, sources, and preprocessing steps of the datasets.
Evaluation metrics: We evaluated the accuracy of a compression
result by measuring fitness, defined as follows:
ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  =1âˆ’vut ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥Xğ‘˜âˆ’Â¯Xğ‘˜âˆ¥2
ğ¹!
/ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥Xğ‘˜âˆ¥2
ğ¹,
where Â¯Xğ‘˜is the approximation of a given Xğ‘˜by a compression
method, and it is tailored for measuring the accuracy of irregular
tensor compression, extending from the fitness applied to regular
tensors. The concept of fitness has been widely used for measuring
the accuracy of tensor approximation (or compression) [ 20,27]. It
is at most 1, with higher fitness indicating more accurate compres-
sion. The compressed size was measured by counting the number
of parameters in the compression result in bytes. Recall that we
applied Huffman encoding to compress the integer numbers in the
mappings{ğœ‹ğ‘˜}ğ¾
ğ‘˜=1in our methods. More details of compressed
sizes are in Appendix C.1.
Training details: Training details are provided in Appendix C.2.
6.2 Compression Performance
We evaluated the compression performances of our methods and
competitors by comparing the trade-offs between accuracy and com-
pressed size. To this end, we measured the fitness of each method by
varying the compressed size in bytes. We compared the compressed
sizes (or accuracies) of our methods with those of their competitors
at a similar level of accuracy (or size). We set the size of the vocab-
ulary of our methods to the largest mode length of the first mode
in a given irregular tensor. The values of the hyperparameters of
all methods are provided in Appendx C.4.
As shown in Fig. 5, Light-IT and Light-IT++significantly out-
perform their competitors by providing the best trade-off between
accuracy and size. Specifically, the compressed size of Light-IT++is
36.55Ã—smaller than that of the most compact baseline, showing a
similar approximation accuracy in the Korea-stock dataset. In the
CMSdataset, Light-IT++shows 5.02Ã—higher fitness than the most
LIGHT-IT         L IGHT-IT++LIGHT-IT for sparse tensors (ST)        LIGHT-IT++for ST
LIGHT-IT (Proposed) LIGHT-IT++(Proposed) LIGHT-IT-C CP        Tucker
02040
CMS MIMIC-III
DatasetsTime (in seconds)(a) 3-order irregular tensors
100101102103
Enron Delicious
DatasetsTime (in seconds) (b) 4-order irregular tensors
Figure 7: For sparse irregular tensors, the sparse versions of
our methods are faster than their dense versions.
accurate baseline. Note that the maximum settings of our methods
yield much smaller compressed sizes than the minimum settings of
the other methods. As expected, Light-IT++achieves higher accu-
racy than Light-IT, albeit at the cost of slightly more compressed
size in most datasets. Note that no competitor can be run on the
Enron andDelicious datasets since they form 4-dimensional ir-
regular tensors, respectively. The experiments on REPAIR ran out
of memory in all 3-order tensors, except for MIMIC-III . Out-of-
memory issues occurred when running BTD2 on 4-order tensors.
Thus, in Section 1 of Online Appendix [ 23], we sampled the indices
of the original tensors, made them into small tensors, and compared
the performances of our methods and BTD2.
6.3 Ablation Studies
We compared Light-IT and Light-IT++with the following methods
to check the effectiveness of our ideas:
â€¢Light-IT-L: a variant of Light-IT that directly maps the ğ‘–-th row in
a slice to the ğ‘–-th row of the vocabulary matrix without learning
the mapping functions.
â€¢CP and Tucker: CP and Tucker decompositions of the regular
tensor obtained by zero-padding the input irregular tensor.
â€¢Light-IT and Light-IT++for ST: sparse versions of our methods,
leveraging sparsity for irregular sparse tensors (ST).
We used the same rank for all methods, and refer to Appendix C.5
for more details. Note that out-of-memory issues occurred when
running Tucker decomposition on the CMSdataset.
Vocabulary-based compression: Despite being under the same
vocabulary-based scheme, Light-IT-L performs worse than Light-
IT, as shown in Fig. 6, indicating it is crucial to accurately learn the
mappings, achieved by our approach. In addition, Light-IT (or Light-
IT++) outperforms CP (resp., Tucker), implying the effectiveness
of the vocabulary-based compression over the decompositions of
zero-padded regular tensors.
Extension with core tensor: As Light-IT++outperforms Light-
IT at the same rank, our extension with the core tensor provides
enhanced expressiveness for compression.
Sparse design: As shown in Fig. 7, the compression time required
by both Light-IT and Light-IT++decreases when using the sparse
versions instead of their dense versions. The difference is distinct
in the Enron and Delicious datasets, which are much sparser
than the others, indicating that leveraging the sparsity plays an
important role in the efficient compression of sparse tensors.
 
1458Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
LIGHT-IT       
LIGHT-IT++
211213215
220222224
# of Non-zero EntriesTime (in millisceonds)
(a) 3-order sparse tensors
2729211213
223225227
# of EntriesTime (in millisceonds) (b) 3-order dense tensors
29211213215
217219221
# of Non-zero EntriesTime (in millisceonds)
(c) 4-order sparse tensors
252729211213
222224226228
# of EntriesTime (in millisceonds) (d) 4-order dense tensors
Figure 8: Compression time of our methods increases (sub-
)linearly to the number of non-zero entries or entries.
6.4 Scalability
We investigated the scalability of Light-IT and Light-IT++, specifi-
cally the compression (training) time per epoch w.r.t. the number
of non-zero entries in sparse irregular tensors and the number of
entries in dense irregular tensors, respectively. For sparse irregular
tensors, we fixed their sizes while varying the densities by a factor
of2. For dense irregular tensors, we varied their sizes by a factor of
2. All non-zero entries were randomly sampled from [0,1). More
details on sizes and densities are provided in Appendix C.6.
Our methods exhibit near-linear scalability with the number of
non-zero entries in sparse irregular tensors, as shown in Fig. 8(a)
and 8(c), aligning with the analysis in Theorems 4 and 6 where
it asymptotically dominates the other terms. For dense irregular
tensors, as shown in Fig. 8(b) and 8(d), the compression time of our
methods increases sub-linearly with the number of entries, in line
with the analysis presented in Theorems 3 and 5.
6.5 Total Compression Time
We evaluated the total compression time of all methods on each
dataset under parameter settings that yield similar accuracy (refer
to Appendix C.7 for details). As shown in Fig. 9, our methods require
more time than the competitors for compressing 3-order irregular
tensors, except for MIMIC-III . The main cause is the learning of
mappings, and while it slows down our methods, it is worthwhile
for effectively compressing irregular tensors. Remark that out-of-
memory issues occurred when running BTD2. Another observation
is that Light-IT++spends little more time than Light-IT, but recall
that Light-IT++achieves higher accuracy in return.
7 CONCLUSION
This paper presents Light-IT and Light-IT++, novel decomposition
methods for an irregular tensor, specialized for lossy compression.
In Light-IT, we introduced a vocabulary matrix for reducing the
size of the first mode factor matrices of PARAFAC2 decomposition.
LIGHT-IT (Proposed) LIGHT-IT++(Proposed) SPARTan COPA
PARAFAC2 -ALS      DPar2       RD-ALS       REPAIR         HyTuck2 
100101102103104
CMS MIMIC-III Korea stock US stock Enron Delicious
DatasetsTime (in seconds)Figur
e 9: Total time for compressing irregular tensors.
We also devised Light-IT++, which includes a Tucker operation for
enhancing the accuracy. For the same rank, Light-IT is faster, but
Light-IT++is more accurate with a small increase in the number
of parameters. With these contributions, our methods have the
following advantages:
â€¢Compact: The compressed output of our methods is up to 37 Ã—
smaller than that of the most compact baseline, providing a simi-
lar approximation error.
â€¢Accurate: Our method with a smaller compressed output offers
up to 5Ã— better fitness than the most accurate baseline.
â€¢Versatile: Our methods exploit the sparsity of sparse irregular
tensors for reducing the compression time. Furthermore, they
successfully compress irregular tensors of any order.
Reproducibility: The
code and datasets are available at [23].
Acknowledgements
We thank Prof. Hanghang Tong from University of Illinois Urbana-
Champaign for fruitful discussions. This work was funded by the
Korea Meteorological Administration Research and Development
Program â€œDeveloping Intelligent Assistant Technology and Its Ap-
plication for Weather Forecasting Processâ€ (KMA2021-00123). This
work was partly supported by Institute of Information & Commu-
nications Technology Planning & Evaluation (IITP) grant funded
by the Korea government (MSIT) (No. 2022-0-00157, Robust, Fair,
Extensible Data-Centric Continual Learning) (No. RS-2019-II190075,
Artificial Intelligence Graduate School Program (KAIST)). This
work was partly supported by the National Research Foundation
of Korea (NRF) grant funded by the Korea government (MSIT) (No.
2021R1C1C1008526).
REFERENCES
[1]Ardavan Afshar, Ioakeim Perros, Evangelos E Papalexakis, Elizabeth Searles,
Joyce Ho, and Jimeng Sun. 2018. COPA: Constrained PARAFAC2 for sparse &
large datasets. In CIKM.
[2]Brett W Bader, Michael W Berry, and Murray Browne. 2008. Discussion track-
ing in Enron email using PARAFAC. In Survey of Text Mining II: Clustering,
Classification, and Retrieval. Springer, 147â€“163.
[3]Christos Chatzichristos, Eleftherios Kofidis, and Sergios Theodoridis. 2017.
PARAFAC2 and its block term decomposition analog for blind fMRI source
unmixing. In EUSIPCO.
[4]Jia Chen and Dalia Orozco. 2022. Unsupervised Multiview Embedding of Node
Embeddings. In ACSSC.
[5]Ting Chen, Lala Li, and Yizhou Sun. 2020. Differentiable product quantization
for end-to-end embedding compression. In ICML.
[6]Yao Cheng and Martin Haardt. 2019. Efficient computation of the PARAFAC2
decomposition. In ACSCC.
[7]Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. 2000. On the best
rank-1 and rank-(r 1, r 2,..., rn) approximation of higher-order tensors. SIAM
journal on Matrix Analysis and Applications 21, 4 (2000), 1324â€“1342.
[8]Petros Drineas, Ravi Kannan, and Michael W Mahoney. 2006. Fast Monte Carlo
algorithms for matrices III: Computing a compressed approximate matrix decom-
position. SIAM J. Comput. 36, 1 (2006), 184â€“206.
 
1459KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-Gi Jang, and Kijung Shin
[9]Gene H Gloub and Charles F Van Loan. 1996. Matrix computations. Johns Hopkins
Universtiy Press, 3rd edtion (1996).
[10] Olaf GÃ¶rlitz, Sergej Sizov, and Steffen Staab. 2008. PINTS: peer-to-peer infras-
tructure for tagging systems. In IPTPS.
[11] Ekta Gujral, Georgios Theocharous, and Evangelos E Papalexakis. 2020. Spade: S
treaming pa rafac2 de composition for large datasets. In SDM.
[12] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure
with randomness: Probabilistic algorithms for constructing approximate matrix
decompositions. SIAM review 53, 2 (2011), 217â€“288.
[13] Per Christian Hansen. 1987. The truncated SVD as a method for regularization.
BIT Numerical Mathematics 27 (1987), 534â€“553.
[14] Richard A Harshman et al .1972. PARAFAC2: Mathematical and technical notes.
UCLA working papers in phonetics 22, 3044 (1972), 122215.
[15] Frank L Hitchcock. 1927. The expression of a tensor or a polyadic as a sum of
products. J. Math. Phys 6, 1-4 (1927), 164â€“189.
[16] Jun-Gi Jang and U Kang. 2022. Dpar2: Fast and scalable parafac2 decomposition
for irregular dense tensors. In ICDE.
[17] Jun-Gi Jang, Jeongyoung Lee, Yong-chan Park, and U Kang. 2023. Fast and
Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors-Algorithm and
Application. In KDD.
[18] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng,
Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and
Roger G Mark. 2016. MIMIC-III, a freely accessible critical care database. Scientific
data 3, 1 (2016), 1â€“9.
[19] Henk AL Kiers, Jos MF Ten Berge, and Rasmus Bro. 1999. PARAFAC2â€”Part I. A
direct fitting algorithm for the PARAFAC2 model. Journal of Chemometrics: A
Journal of the Chemometrics Society 13, 3-4 (1999), 275â€“294.
[20] Tamara Gibson Kolda. 2006. Multilinear operators for higher-order decompositions.
Technical Report. Sandia National Laboratories (SNL), Albuquerque, NM, and
Livermore, CA (United States).
[21] Tamara G Kolda and Brett W Bader. 2009. Tensor decompositions and applications.
SIAM review 51, 3 (2009), 455â€“500.
[22] Tamara G Kolda, Brett W Bader, and Joseph P Kenny. 2005. Higher-order web
link analysis using multilinear algebra. In ICDM.
[23] Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-GI Jang, and Kijung Shin. 2024.
Compact Decomposition of Irregular Tensors for Data Compression: From Sparse
to Dense to High-Order Tensors (Code, Datasets, and Online Appendix). https:
//github.com/kbrother/Light-IT
[24] Taehyung Kwon, Jihoon Ko, Jinhong Jung, and Kijung Shin. 2023. NeuKron:
Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors.
InWWW.
[25] Taehyung Kwon, Jihoon Ko, Jinhong Jung, and Kijung Shin. 2023. TensorCodec:
Compact Lossy Compression of Tensors without Strong Data Assumptions. In
ICDM.
[26] Ivan V Oseledets. 2011. Tensor-train decomposition. SIAM Journal on Scientific
Computing 33, 5 (2011), 2295â€“2317.
[27] Ioakeim Perros, Evangelos E Papalexakis, Haesun Park, Richard Vuduc, Xiaowei
Yan, Christopher Defilippi, Walter F Stewart, and Jimeng Sun. 2018. Sustain:
Scalable unsupervised scoring for tensors and its application to phenotyping. In
KDD.
[28] Ioakeim Perros, Evangelos E Papalexakis, Fei Wang, Richard Vuduc, Elizabeth
Searles, Michael Thompson, and Jimeng Sun. 2017. SPARTan: Scalable PARAFAC2
for large & sparse data. In KDD.
[29] Yifei Ren, Jian Lou, Li Xiong, and Joyce C Ho. 2020. Robust irregular tensor
factorization and completion for temporal health data analysis. In CIKM.
[30] Jitesh Shetty and Jafar Adibi. 2004. The Enron email dataset database schema and
brief statistical report. Information sciences institute technical report, University of
Southern California 4 (2004).
[31] Shaden Smith, Jee W. Choi, Jiajia Li, Richard Vuduc, Jongsoo Park, Xing Liu, and
George Karypis. 2017. FROSTT: The Formidable Repository of Open Sparse Tensors
and Tools. http://frostt.io/
[32] Jimeng Sun, Yinglian Xie, Hui Zhang, and Christos Faloutsos. 2007. Less is more:
Compact matrix decomposition for large sparse graphs. In SDM.
[33] Yu Sun, Nicholas Jing Yuan, Yingzi Wang, Xing Xie, Kieran McDonald, and Rui
Zhang. 2016. Contextual intent tracking for personal assistants. In KDD.
[34] Ledyard R Tucker. 1966. Some mathematical notes on three-mode factor analysis.
Psychometrika 31, 3 (1966), 279â€“311.
[35] M Alex O Vasilescu and Demetri Terzopoulos. 2002. Multilinear analysis of image
ensembles: Tensorfaces. In ECCV.
[36] M Alex O Vasilescu and Demetri Terzopoulos. 2003. Multilinear subspace analysis
of image ensembles. In ICPR.
[37] Barry M Wise, Neal B Gallagher, and Elaine B Martin. 2001. Application of
PARAFAC2 to fault detection and diagnosis in semiconductor etch. Journal of
Chemometrics: A Journal of the Chemometrics Society 15, 4 (2001), 285â€“298.
[38] Kejing Yin, Ardavan Afshar, Joyce C Ho, William K Cheung, Chao Zhang, and
Jimeng Sun. 2020. LogPar: Logistic PARAFAC2 factorization for temporal binary
data with missing values. In KDD.A TENSOR NOTATIONS AND OPERATIONS
A.1 Matricization
A matricization of a tensor XâˆˆRğ‘1Ã—ğ‘2Ã—Â·Â·Â·Ã—ğ‘ğ‘‘is defined as follows.
Let the ordered sets R={ğ‘Ÿ1,Â·Â·Â·,ğ‘Ÿğ¿}andC={ğ‘1,Â·Â·Â·,ğ‘ğ‘€}be a par-
titioning of the modes D={1,Â·Â·Â·,ğ‘‘}. The mode- ğ‘Ÿ1,Â·Â·Â·,ğ‘Ÿğ¿mati-
cization ofX,X(ğ‘Ÿ1,Â·Â·Â·,ğ‘Ÿğ¿), is a matrix of sizeÃ
ğ‘ŸâˆˆRğ‘ğ‘ŸÃ—Ã
ğ‘âˆˆğ¶ğ‘ğ‘.
Specifically, each entry X(ğ‘–1,ğ‘–2,Â·Â·Â·,ğ‘–ğ‘‘)ofXis mapped to X(ğ‘Ÿ1,Â·Â·Â·,ğ‘Ÿğ¿)
(ğ½,ğ¾)where
ğ½=ğ¿âˆ‘ï¸
ğ‘™=1
ğ‘ğ‘Ÿğ‘™ğ‘™âˆ’1Ã–
ğ‘™â€²=1ğ‘ğ‘Ÿğ‘™â€²
andğ¾=ğ‘€âˆ‘ï¸
ğ‘š=1
ğ‘ğ‘ğ‘šğ‘šâˆ’1Ã–
ğ‘šâ€²=1ğ‘ğ‘ğ‘šâ€²
.
A.2 Kronecker Product and Khatri-rao Product
Kronecker product: Given two matrices AâˆˆRğ¼Ã—ğ½andBâˆˆRğ¾Ã—ğ¿,
the matrix CâˆˆRğ¼ğ¾Ã—ğ½ğ¿is the result of Kronecker product between
AandB, defined as follows:
C=AâŠ—B=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘11BÂ·Â·Â·ğ‘1ğ½B
.........
ğ‘ğ¼1BÂ·Â·Â·ğ‘ğ¼ğ½Bï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£», (23)
whereğ‘ğ‘–ğ‘—is the(ğ‘–,ğ‘—)-th element of the matrix A.
Khatri-rao product: Khatri-rao product is a column-wise Kro-
necker product. Given two matrices AâˆˆRğ¼Ã—ğ½andBâˆˆRğ¾Ã—ğ½
which have the same column size, the Khatri-rao product produces
the matrix CâˆˆRğ¼ğ¾Ã—ğ½:
C=AâŠ™B=A(:,1)âŠ—B(:,1) Â·Â·Â· A(:,ğ½)âŠ—B(:,ğ½)
, (24)
where A(:,ğ‘—)andB(:,ğ‘—)are theğ‘—-th column vectors of AandB,
respectively.
B SUPPLEMENTS FOR PROPOSED METHODS
B.1 Higher-order Version of Light-IT
The loss function in Eq. (6) becomes Eq. (25) when the order of an
irregular tensor is higher than three.
ğ¾âˆ‘ï¸
ğ‘˜=1
âˆ¥X(1)
ğ‘˜âˆ’Pâ€²
ğ‘˜((âŠ™ğ‘‘âˆ’2
ğ‘–=1V)âŠ™S(ğ‘˜,:))âŠºâˆ¥2
ğ¹+âˆ¥Pğ‘˜âˆ’ğ‘ ğ‘”(Uğ‘˜)âˆ¥2
ğ¹
.(25)
The right hand side of Eq. (8) becomes Eq. (26)
ğ‘…âˆ‘ï¸
ğ‘Ÿ1=1ğ‘…âˆ‘ï¸
ğ‘Ÿ2=1S(ğ‘˜,ğ‘Ÿ1)S(ğ‘˜,ğ‘Ÿ2)
Pâ€²
ğ‘˜(:,ğ‘Ÿ1)âŠºPğ‘˜(:,ğ‘Ÿ2)ğ‘‘âˆ’2Ã–
ğ‘–=1Vğ‘–(:,ğ‘Ÿ1)âŠºVğ‘–(:,ğ‘Ÿ2)
.
(26)
B.2 Computing Loss of Light-IT++
We use the following formula when computing the square errors
of Light-IT++for fitness.
ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥Xğ‘˜âˆ’Pğ‘˜G(1)(VâŠ—S(ğ‘˜,:))âŠºâˆ¥2
ğ¹. (27)
When the input tensor is sparse, we exploit its sparsity to accelerate
computation speed. We reformulate the loss function in Eq. (27) as
follows using a similar approach in Eq. (7):
ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥ËœXğ‘˜âˆ¥2
ğ¹+ğ¾âˆ‘ï¸
ğ‘˜=1âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆÎ©ğ‘
ğ‘˜((Xğ‘˜(ğ‘–,ğ‘—)âˆ’ËœXğ‘˜(ğ‘–,ğ‘—))2âˆ’ËœXğ‘˜(ğ‘–,ğ‘—)2), (28)
where the ËœXğ‘˜isPğ‘˜G(1)(VâŠ—S(ğ‘˜,:))âŠºandÎ©ğ‘
ğ‘˜is the set of non-zero
entries in Xğ‘˜. By Lemma 2,Ãğ¾
ğ‘˜=1âˆ¥ËœXğ‘˜âˆ¥2
ğ¹can be quickly computed.
 
1460Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Lemma 2.Ãğ¾
ğ‘˜=1âˆ¥ËœXğ‘˜âˆ¥2
ğ¹in Eq. (28) is equal to Eq. (29).
ğ‘ ğ‘¢ğ‘š
VG(2)ğ¾âˆ‘ï¸
ğ‘˜=1PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:)
âˆ—(VG(2))
. (29)
Note thatğ‘ ğ‘¢ğ‘š(Â·)denotes the sum of all entries in a matrix, and
âˆ—denotes the elementwise product.
Proof. The following equations hold.
ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥ËœXğ‘˜âˆ¥2
ğ¹=ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¡ğ‘Ÿ(ËœXâŠº
ğ‘˜ËœXğ‘˜)
=ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¡ğ‘Ÿ(VG(2)(Pğ‘˜âŠ—S(ğ‘˜,:))âŠº(Pğ‘˜âŠ—S(ğ‘˜,:))G(2)âŠºVâŠº)
=ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘¡ğ‘Ÿ(VG(2)(PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:))G(2)âŠºVâŠº)
=ğ‘¡ğ‘Ÿ
VG(2)ğ¾âˆ‘ï¸
ğ‘˜=1PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:)
G(2)âŠºVâŠº(30)
The final formula in Eq. (30) is the same as Eq. (29). Note that ËœXâŠº
ğ‘˜
can be also written as VG(2)(Pğ‘˜âŠ—S(ğ‘˜,:))âŠº. â–¡
Computation of Eq. (29) is much faster than naively summing
squares of all entries in {ËœXğ‘˜}ğ¾
ğ‘˜=1. Computation of the second term
in Eq. (28) takes time linear in the number of non-zero entries in
{Xğ‘˜}ğ¾
ğ‘˜=1. When compressing an irregular tensor whose order is
higher than three, Eq. (27) is altered as follows:
ğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥Xğ‘˜âˆ’Pğ‘˜G(1)((âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–)âŠ—S(ğ‘˜,:))âŠºâˆ¥2
ğ¹. (31)
Eq. (29) is also altered as follows:
ğ‘ ğ‘¢ğ‘š
(âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–)G(2)ğ¾âˆ‘ï¸
ğ‘˜=1PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:)
âˆ—((âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–)G(2))
.
(32)
The time complexity for a total calculation during an epoch is in
Theorem 5 and 6.
B.3 Higher Order Version of Light-IT++
UpdateG:Eq. (12) is changed as follows.
min
Gğ¾âˆ‘ï¸
ğ‘˜=1âˆ¥X(2,...,ğ‘‘âˆ’1)
ğ‘˜âˆ’(âŠ—ğ‘‘âˆ’2
ğ‘–=1V)G(2,...,ğ‘‘âˆ’1)(Pğ‘˜âŠ—S(ğ‘˜,:))âŠºâˆ¥2
ğ¹, (33)
Remark that X(2,...,ğ‘‘âˆ’1)
ğ‘˜is the mode-(2,...,ğ‘‘âˆ’1)matricization of
Xğ‘˜(see Appendix A.1). Eq. (13) becomes Eq. (34).
G(2)â†(âŠ—ğ‘‘âˆ’2
ğ‘–=1VâŠº
ğ‘–Vğ‘–)â€ 
(âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–)âŠºğ¾âˆ‘ï¸
ğ‘˜=1X(2,Â·Â·Â·,ğ‘‘âˆ’1)
ğ‘˜(Pğ‘˜âŠ—S(ğ‘˜,:)
Ã—ğ¾âˆ‘ï¸
ğ‘˜=1PâŠº
ğ‘˜Pğ‘˜âŠ—S(ğ‘˜,:)âŠºS(ğ‘˜,:)â€ 
,(34)
Update P:Eq. (14) is changed as follows.
min
P(ğ‘–,:)âˆ‘ï¸
(ğ‘—,ğ‘˜)âˆˆğ‘‡ğ‘–âˆ¥X(1)
ğ‘˜(ğ‘—,:)âˆ’P(ğ‘–,:)G(1) (âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–)âŠ—S(ğ‘˜,:)âŠºâˆ¥2
ğ¹.(35)Eq. (15) becomes Eq. (36).
P(ğ‘–,:)â†âˆ‘ï¸
(ğ‘—,ğ‘˜)âˆˆğ‘‡ğ‘–X(1)
ğ‘˜(ğ‘—,:)((âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–)âŠ—S(ğ‘˜,:))G(1)âŠº
Ã—
G(1)
(âŠ—ğ‘‘âˆ’2
ğ‘–=1VâŠº
ğ‘–Vğ‘–)âŠ— âˆ‘ï¸
(ğ‘—,ğ‘˜)âˆˆğ‘‡ğ‘–S(ğ‘˜,:)âŠºS(ğ‘˜,:)
G(1)âŠºâ€ 
.(36)
Update V:The higher-order version of Eq. (16) is Eq. (22).
Update S:Eq. (17) is changed as follows.
min
S(ğ‘˜,:)âˆ¥ğ‘£ğ‘’ğ‘(Xğ‘˜)âˆ’S(ğ‘˜,:)G(ğ‘‘)(Pğ‘˜âŠ—(âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–))âŠºâˆ¥2
ğ¹. (37)
Eq. (18) becomes Eq. (38)
S(ğ‘˜,:)â†
ğ‘£ğ‘’ğ‘(Xğ‘˜)(Pğ‘˜âŠ—(âŠ—ğ‘‘âˆ’2
ğ‘–=1Vğ‘–))G(ğ‘‘)âŠº
Ã—
G(ğ‘‘)(PâŠº
ğ‘˜Pğ‘˜âŠ—(âŠ—ğ‘‘âˆ’2
ğ‘–=1VâŠº
ğ‘–Vğ‘–))G(ğ‘‘)âŠºâ€ 
.(38)
C SUPPLEMENTS FOR EXPERIMENTS
C.1 Metrics for Compressed Sizes
All methods, except COPA, saved their resulting matrices or tensors
in a dense format. For COPA, we chose either a sparse format (i.e.,
COO) or a dense format, selecting the one with minimal space cost
for saving each of the outputs. For the sparse format, we further
compressed integer indices using Huffman encoding.
C.2 Training Details
When training Light-IT, we used 500 epochs for all datasets. We
measured the fitness of Light-IT every 10 epochs and reported the
highest obtained fitness. We tuned the learning rate for Light-IT
by varying it in {1, 0.1, 0.01, 0.001}. As a result, the learning rate
of Light-IT is set to 10âˆ’2except in US-stock . InUS-stock ,10âˆ’1is
used as the learning rate when the ranks are 4 and 8, and 10âˆ’2is
used when the ranks are 12 and 16. The initial parameters of Light-
IT++were set to the parameters of Light-IT, and the core tensor was
initialized to a diagonal tensor whose diagonal entries were all 1. We
updated the parameters of Light-IT++until the increase of fitness
is smaller than 10âˆ’4. We used the same convergence condition
for PARAFAC2-ALS. When training COPA on sparse tensors, we
imposed non-negative constraints to H,Wand a sparsity constraint
toV, following one of the scenarios outlined in the original paper.
For dense tensors, we gave non-negative constraints to all H,W,
andVsince running with sparsity constraints on Vwas failed. We
set the convergence tolerance to 10âˆ’4and the maximum number
of iterations to 1000 when executing SPARTan, as provided in the
authorsâ€™ code. The convergence tolerance was set to 10âˆ’4when
running COPA, and this was the setting provided in the authorsâ€™
code. We also set the convergence tolerance for PARAFAC2-ALS to
10âˆ’4. We set the maximum number of iterations to 32 for DPar2
and RD-ALS, as the authors of DPar2 did. We set the convergence
tolerance to 10âˆ’3and10âˆ’6for REPAIR and HyTuck2, respectively,
following the authorsâ€™ code.
C.3 Semantics of Data and Preprocessing Steps
CMS:10Centers for Medicare and Medicaid Services (CMS) data
are synthesized data of Medicare beneficiaries and their claims
10https://www.cms.gov/data-research/statistics-trends-and-reports/medicare-
claims-synthetic-public-use-files/cms-2008-2010-data-entrepreneurs-synthetic-
public-use-file-de-synpuf
 
1461KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Taehyung Kwon, Jihoon Ko, Jinhong Jung, Jun-Gi Jang, and Kijung Shin
Table 3: Densities of tensors used in Fig. 8(a) and 8(c)
Or
der Density
310âˆ’2
2Ã—10âˆ’2
4Ã—10âˆ’2
8Ã—10âˆ’2
1.6Ã—10âˆ’1Or
der Density
410âˆ’3
2Ã—10âˆ’3
4Ã—10âˆ’3
8Ã—10âˆ’3
1.6Ã—10âˆ’2
Table 4: Sizes of tensors in Fig. 8(b) and 8(d)
Or
derğ‘ğ‘š
ğ‘ğ‘¥ğ‘ğ‘
ğ‘£ğ‘”Size
(
except the
1st
mode)
3255 136.52 512Ã—256
255 136.52 512Ã—512
510 273.04 512Ã—512
510 261.71 1024Ã—512Or
derğ‘ğ‘š
ğ‘ğ‘¥ğ‘ğ‘
ğ‘£ğ‘”Size
(
except the
1st
mode)
463 31.55 64Ã—64Ã—64
126 63.09 64Ã—64Ã—64
126 63.09 128Ã—64Ã—64
126 63.09 128Ã—128Ã—64
126 63.09 128Ã—128Ã—128
from 2008 to 2010. We built a tensor to count the diagnoses that
each patient has received in the form of (visits, diagnoses, patients;
counts). We categorized the diagnosis codes into Clinical Classifi-
cation Software (CCS)11, and assumed that each claim id indicates
a unique visit. We only considered the patients who visited clinics
more than once.
MIMIC-III [18]:12MIMIC-III is an intensive care unit (ICU) dataset
collected between 2001 and 2012. The tensor we built saves the
medications that each patient has prescribed and is composed of
(dates, drugs, patients; counts). We removed the dates that didnâ€™t
include any medications and removed patients whose lengths of
dates were less than 2. We only used 1000 types of drugs which
were most frequently prescribed.
Korea-stock, US-stock [16]:13Stock datasets are the collections
of stocks on the South Korea and US stock markets. Each dataset
is represented as a tensor of (dates, feature types, stocks; feature
values). We used the tensors preprocessed by the authors of [16].
Enron [30, 31]:14Enron stores the counts of words in emails in
the form of (dates, senders, words, receivers; counts). We only
used senders and receivers whose IDs were smaller than 1001, and
we checked the 1000 most frequently used words. We made the
irregularity in the tensor by removing the dates in each receiver
when no email had arrived.
Delicious [10, 31]:15TheDelicious dataset is a collection of tags
from the Delicious website and consists of (dates, items, tags, users;
binary indicators for tags). We sampled the most frequent items
and tags when building a tensor. We removed dates without any
tag for each user, resulting in the irregularity of the tensor.
C.4 Hyperparameter Settings
The hyperparameters of the methods in Section 6.2 are listed in
the section. The ranks of Light-IT are set to 5, 10, 20, 30 in CMS.
MIMIC-III ,Enron , and Delicious . In the cases of Korea-stock
andUS-stock , 4, 8, 12, 16 are used for the ranks of Light-IT. For
11https://hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp
12https://physionet.org/content/mimiciii/1.4/
13https://datalab.snu.ac.kr/dpar2/
14https://frostt.io/tensors/enron/
15https://frostt.io/tensors/delicious/Table 5: Ranks of the methods in Fig. 9
Dataset Light-I
T Light-IT++SPARTan COPAPARAFADpar2 RD-ALS REPAIR HyTuck2C2-
ALS
CMS 10
10 9 12 9 12 12 O.O.M 9
MIMIC-III 5
5 5 8 5 5 5 11 5
KR-stock 4
4 4 4 4 4 4 O.O.M 4
US-stock 4
4 4 5 4 4 4 O.O.M 4
Enron 5
5only applicable to 3-order irregular tensors
Delicious 5
5
Table 6: Fitness of the methods in Fig. 9
Dataset Light-I
T Light-IT++SPARTan COPAPARAFADpar2 RD-ALS REPAIR HyTuck2C2-
ALS
CMS 0.202
0.209 0.194 0.205 0.194 0.192 0.197 O.O.M 0.19
MIMIC-III 0.103
0.107 0.105 0.113 0.106 0.949 0.967 0.108 0.104
KR-stock 0.362
0.363 0.372 0.351 0.372 0.358 0.358 O.O.M 0.372
US-stock 0.405
0.405 0.412 0.403 0.412 0.405 0.405 O.O.M 0.412
Enron 0.263
0.279only applicable to 3-order irregular tensors
Delicious 0.005
0.006
Light-IT++, we set the ranks to 5, 10, 20, 30 in CMSandMIMIC-III ,
4, 8, 12, 15 in Korea-stock andUS-stock , 5, 9. 12, 15 for Enron ,
and 5, 11, 17, 23 in Delicious.
For all competitors except REPAIR, we set the ranks to 3, 6,
9, 12, 15 in CMS, 5, 8, 11, 14, 17, 20 in MIMIC-III , 1, 2, 4, 6, 8 in
Korea-stock , and 1, 2, 3, 4, 5, 6, 7 in US-stock . We set the ranks
of REPAIR to 5, 8, 11, 14, 17, 20 in MIMIC-III . For REPAIR, we set
the number of bases to 253, following the setting in the paper.
C.5 Supplements for the Ablation Study
For the methods in Fig. 6 of Section 6.3, we set the rank to 30 in
CMSandMIMIC-III , 16 in Korea-stock andUS-stock , 15 in Enron ,
and 23 in Delicious . We tuned the learning rates of Light-IT and
Light-IT-L from{1,10âˆ’1,10âˆ’2,10âˆ’3}. The convergence tolerances
for the CP and Tucker decompositions are set to 10âˆ’4.
When conducting the experiments for Fig. 7 of Section 6.3, we
sampled the first few slices from real-world sparse tensors (1000
forCMSandMIMIC-III , 20 for Enron , and 30 for Delicious ) since
total tensors in dense formats are too large. For all datasets in Fig. 7,
we set the number of epochs of Light-IT to 500, the learning rate
of Light-IT to 10âˆ’2, rank to 10, and the convergence tolerance of
Light-IT++to10âˆ’4.
C.6 Supplments for the Scalability Test
The (minimum, maximum, average) number of rows of the 3-order
sparse irregular tensors in Section 6.4 is (2, 510, 261.71), and the
mode lengths of the remaining modes are all 512. The (minimum,
maximum, average) number of rows of the 4-order sparse irregular
tensors in Section 6.4 is (2, 126, 70.38), and the mode lengths of the
remaining modes are all 128. The densities of the sparse irregular
tensors are in Table 3. The sizes of the dense irregular tensors used
in Section 6.4 are in Table 4.
C.7 Supplements for the Time Experiment
The ranks and fitness of all methods used in Section 6.5 are provided
in Tables 5 and 6.
 
1462