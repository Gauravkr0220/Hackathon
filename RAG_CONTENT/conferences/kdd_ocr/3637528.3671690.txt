Neural Collapse Anchored Prompt Tuning for Generalizable
Vision-Language Models
Didi Zhu
Zhejiang University
Hangzhou, China
didi_zhu@zju.edu.cnZexi Li
Zhejiang University
Hangzhou, China
zexi.li@zju.edu.cnMin Zhang
Zhejiang University
Hangzhou, China
zhangmin.milab@zju.eud.cn
Junkun Yuan
Zhejiang University
Hangzhou, China
yuanjk@zju.edu.cnJiashuo Liu
Tsinghua University
Beijing, China
liujiashuo77@gmail.comKun Kuangâˆ—
Zhejiang University
Hangzhou, China
kunkuang@zju.edu.cn
Chao Wuâˆ—
Zhejiang University
Hangzhou, China
chao.wu@zju.edu.cn
ABSTRACT
Large-scale vision-language (V-L) models have demonstrated re-
markable generalization capabilities for downstream tasks through
prompt tuning. However, the mechanisms behind the learned text
representations are unknown, limiting further generalization gains,
and the limitations are more severe when faced with the prevalent
class imbalances seen in web-sourced datasets. Recent advances
in the neural collapse (NC) phenomenon of vision-only models
suggest that the optimal representation structure is the simplex
ETF, which paves the way to study representations in V-L models.
In this paper, we make the first attempt to use NC for examining the
representations in V-L models via prompt tuning. It is found that
NC optimality of text-to-image representations shows a positive
correlation with downstream generalizability, which is more severe
under class imbalance settings. To improve the representations, we
propose Neural-collapse-anchored Prompt Tuning (NPT), a novel
method that learns prompts with text and image representations
that satisfy the same simplex Equiangular Tight Frame (ETF). NPT
incorporates two regularization terms: language-modality collapse
and multi-modality isomorphism; and it is compatible with other
prompt tuning methods. Extensive experiments show that NPT can
consistently help to improve existing prompt tuning techniques
across 11 datasets for both balanced and imbalanced settings.
âˆ—Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than the 
author(
s) must be honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. 
ACM ISBN 979-8-4007-0490-1/24/08. 
https://doi.org/10.1145/3637528.3671690CCS CONCEPTS
â€¢Computing methodologies â†’Computer vision; Image rep-
resentations.
KEYWORDS
Prompt Tuning, Vision-Language Models, Representation Learning
ACM Reference Format:
Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Kun Kuang,
and Chao Wu. 2024. Neural Collapse Anchored Prompt Tuning for Gener-
alizable Vision-Language Models. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671690
1 INTRODUCTION
Recent advancements in the realm of large-scale vision-language
(V-L) pre-trained models such as CLIP [28], have emerged as a
promising alternative for visual representation learning. Drawing
strength from millions of image-text data sourced from the web,
these models proficiently align textual and visual modalities. Their
prowess, anchored in the diverse and rich content of the web, man-
ifests in robust performance on a variety of downstream tasks,
notably in challenges like zero-shot image recognition [46].
However, in real-world scenarios, especially in web-centric do-
mains like e-commerce, health informatics, or social media analyt-
ics, models often encounter previously unseen classes [2,4,19,38].
Considering deploying a pre-trained model like CLIP in a dynamic
e-commerce website, and it is continuously updated with new prod-
ucts from various web sources. Recent studies have highlighted the
inherent limitations of V-L models when adapting to such rapidly
changing environments [10,45]. Given the impracticality of retrain-
ing the model for each new product, many studies have turned
to soft prompt tuning as an approach to address these limitations
without the need for extensive parameter fine-tuning [10,45]. Soft
prompt tuning enhances the generalization abilities of V-L models
by freezing their core parameters and exclusively refining learnable
4631
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Didi Zhu et al.
(d) MaPLe + NPT (c) MaPLe (b) MaPLe (a) ZSCLIP Balance
Imbalance
Base: 56.48  New: 64.05 Base: 94.07  New: 73.23 Base: 58.53  New: 50.07 Base: 63.17  New: 54.43
Figure 1: Neural collapse degree visualization in CLIP on the EuroSAT dataset. Arrows indicate text representations, points
denote image representations, and colors indicate categories. (a-b) Under balanced conditions, the current SOTA MaPLe method
amplifies CLIPâ€™s generalizability by intensifying the neural collapse degree. (c-d) Under imbalance, the optimal structure in
MaPLe is disrupted, resulting in a performance drop. Our NPT method can refine this structure, improving generalization.
prompts using a limited dataset. Building on CoOp [46]that models
context words in prompts as learnable vectors, recent studies have
introduced various techniques to enhance the generalization by
prompt tuning, including generating instance-specific prompt [45]
and integrating additional vision-modality prompt [10, 36].
While current soft prompt tuning methods have made strides in
enhancing generalization, they often fall short when confronted
with the diverse and imbalanced nature of novel classes in real-
world scenarios. A closer look reveals that while these techniques
make adjustments to text representations, they often overlook the
core alignment mechanisms between text and image modalities,
especially in the presence of diverse new classes. This superficial
alignment, devoid of deeper understanding, can lead to sub-optimal
performance. Therefore, in this paper, we delve into: (1) What truly
characterizes effective text and image representations in V-L models,
especially when faced with novel and imbalance classes? (2) How can
we harness these insights to achieve robust and aligned representations
within the embedding space?
An emerging discovery called neural collapse phenomenon [13,
27,34]in the vision-only models has shed light on the opti-
mal structure of visual feature representations. This phenomenon
demonstrates that the features just before feeding the classifier
(termed â€˜last-layer featuresâ€™) within the same class collapse to their
within-class mean after the model reaches zero training loss on
a sufficient and balanced dataset. Additionally, the within-class
means and their corresponding classifier vectors converge to the
vertices of a simplex equiangular tight frame (ETF). The simplex
ETF depicts the geometric structure of several vectors that have
maximal pairwise angles and equal norms [27].
It inspires us to examine the text representation geometries in
the prompt tuning of vision-language models through the lens
of neural collapse. After tuning prompts on a balanced dataset,
text representations in vision-language models serve the same
role as classifier vectors do in vision-only models, and tend to
approximate the ETF structure, as evident in Fig. 1 (a-b). Intuitively,
the text representations of MaPLe (the current SOTA method) are
closer to the simplex ETF structure (i.e., with evenly maximal angles
and equal norms) and are more aligned with the image represen-
tations than ZSCLIP, and MaPLeâ€™s base-to-novel generalization ismuch better than ZSCLIP. We suppose that: A better neural col-
lapse phenomenon of text-to-image representations indicates better
generalizability of prompt-tuned CLIP.
It is further validated by our empirical analysis in Fig. 2 (a). It is
suggested that two metrics indicative of the ETF structure (further
elaborated in Sec 4.1) exhibit a proportional correlation with CLIPâ€™s
base-to-novel generalization performances across several prompt
tuning methods. Our analysis connects the text-to-image represen-
tations of CLIP with the neural collapse phenomenon and unveils
the mechanism behind the generalizability of prompt tuning.
Building on the findings, we further delve into a more practical
setting where the downstream dataset is class imbalanced [11,18,
20,31,41,42,48,49]. The class imbalance indicates that popular cat-
egories (or head classes) often dominate the dataset with numerous
instances, while niche categories (or tail classes) are sparsely rep-
resented, which is prevalent in real-world scenarios. For instance,
in an e-commerce site, while there might be thousands of listings
for popular electronics, unique artisanal crafts might have only
a handful. This imbalance can skew minor class representations
and disturb the simplex ETF. As demonstrated in Fig. 1 (c), minor
class text representations exhibit a tendency to cluster closely, a
behavior that impairs the modelâ€™s capacity for generalization. This
degradation of performance is further corroborated by the empiri-
cal evidence presented in Fig. 2 (b). To address the challenge, we
propose a novel method called Neural-collapse-anchored Prompt
Tuning (NPT), which aims to optimize prompts so that both text
representations and image representations satisfy the same simplex
ETF structure. Specifically, we introduce two regularization terms
to the existing prompt tuning techniques. NPT considerably en-
hances the generalizability of V-L models in both class-imbalanced
and balanced scenarios.
We summarize the contributions as follows:
â€¢To the best of our knowledge, we are the first to extend the neural
collapse phenomenon to V-L models and provide insights into
the underlying representation mechanism of prompt tuning on
CLIPâ€™s generalization ability.
â€¢We further explore the impact of class imbalance on the gener-
alization performance of V-L models, addressing a critical gap
4632Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a)            ,            and Acc Variations under        
 (b)            ,            and Acc Variations under        
Figure 2: Comparison of Î”MIDandÎ”LCDwith generalization performance under the base-to-novel task. ğœdenotes imbalance
degree:ğœ=1for balanced and ğœ=0.01for high imbalance. (a) In balanced settings, we compare zero-shot CLIP with current
prompt tuning methods, finding a direct correlation between Î”MID&Î”LCDerrors (the smaller values, the greater neural collapse)
and accuracy. (b) This correlation holds in class imbalance scenarios, where the degree of neural collapse and generalization
performance are more impaired, even for the SOTA method MaPLe. Our approach effectively bolsters CLIPâ€™s generalization by
minimizing Î”LCDandÎ”MID.
in existing research. To mitigate this, we introduce NPT to im-
prove CLIPâ€™s generalization ability under both class balance and
imbalance scenarios.
â€¢Through theoretical analysis and extensive empirical experi-
ments, we demonstrate the effectiveness of NPT combined with
existing prompt tuning approaches, showcasing its significant
improvements in generalization.
Our takeaway insights of the neural collapse are below:
â€¢A higher degree of neural collapse in text-to-image representa-
tions is indicative of stronger generalizability. This is pivotal in
explaining how soft prompt tuning enhances CLIPâ€™s generaliza-
tion capability.
â€¢Prompt tuning on an imbalanced dataset will disturb the text-
to-image representations from neural collapse, thus, hurting the
generalization. Neural-collapse-anchored approaches are needed
to improve the representations.
2 RELATED WORK
Vision-Language Models. As research into attention mechanism
and other methodologies advances [21â€“25], large language models
have achieved remarkable progress. The fusion of language supervi-
sion with natural images has garnered significant attention within
the computer vision community. These V-L models, as opposed
to those solely based on image supervision, encode rich language-
driven visual multi-modal representations. Recent V-L models such
as CLIP [28], ALIGN [9], LiT [40], FILIP [37], and Florence [39]have
showcased remarkable performance across a diverse range of tasks,
including few-shot and zero-shot visual recognition. These mod-
els learn joint image-language representations by self-supervised
training on vast amounts of web data.
Prompt Learning. Prompt learning indicates that prompts can be
learned automatically for a downstream task during the fine-tuning
stage. This technique was first used in NLP [12,14,16,43]followed
by the adaptation in V-L [5,45â€“47]. CoOp [46]fine-tunes CLIP
for few-shot transfer learning by optimizing a continuous set of
prompt vectors at its language branch. CoCoOp [45]highlights
the inferior performance of CoOp on novel classes and solves thegeneralization issue by explicitly conditioning prompts on image
instances. UPL [7]optimizes the language prompts in an unsuper-
vised fashion. [17]proposes to optimize multiple sets of prompts by
learning the prompts distribution. [1]perform visual prompt tuning
on CLIP by prompting on the vision branch. MaPLe [26]proposes
multi-modal prompts in both language and vision branches of CLIP.
Neural Collapse. In[27], neural collapse was observed in a lin-
ear classification model trained on a balanced dataset that expe-
riences a phenomenon where the last-layer features collapse into
within-class centers during the final stage of training. Since then,
researchers have endeavored to understand this phenomenon theo-
retically [3,8,30,32,50]. Recent studies have attempted to induce
it in imbalanced learning [29,33,34], incremental learning [35],
semantic segmentation [44], and federated learning [6,15]. How-
ever, these investigations into neural collapse are limited to linear
classifiers. In this work, we explore the presence of such a solution
in V-L models and propose NPT to improve the generalizability of
V-L models through prompt tuning.
3 PRELIMINARIES
3.1 Prompt Tuning in V-L models
CLIP comprises a vision-modality image encoder ğºğ‘£:Rğ‘¤Ã—â„Ã—3â†’
Rğ‘‘and a language-modality text encoder ğºğ‘™:Rğ‘šÃ—ğ‘‘ğ‘’â†’Rğ‘‘. The
image encoder ğºğ‘£converts a 3-channel image xwith shapeğ‘¤Ã—â„
into ağ‘‘-dimensional image representation zâˆˆRğ‘‘. Meanwhile,
the text encoder ğºğ‘™generates a ğ‘‘-dimensional text representation
ğ’ˆğ‘˜âˆˆRğ‘‘fromğ‘š-wordsğ‘‘ğ‘’-dimensional text input tğ‘˜, whereğ‘˜âˆˆ
{1,...,ğ¾}andğ¾is the number of classes. Note that both zandğ’ˆğ‘˜
areâ„“2normalized vectors. Two encoders are jointly trained using a
contrastive loss that maximizes the cosine similarity of matched
pairs and minimizes that of the unmatched pairs.
Prompt engineering adapts the pre-trained CLIP to perform
zero-shot recognition without fine-tuning the model. Denote V=
{classğ‘˜}ğ¾
ğ‘˜=1as a class name set, the text input for CLIP is obtained
by extending [classğ‘˜]using a template like tğ‘˜=â€œa photo of a [classğ‘˜]â€²â€².
The corresponding text representation ğ’ˆğ‘˜is obtained by feeding
4633KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Didi Zhu et al.
the prompt into the text encoder ğºğ‘™, while the image representa-
tionzof the image xis extracted by the image encoder ğºğ‘£. The
prediction task is defined as the classification of an image into one
ofğ¾categories, which are represented by the set ğ‘¦âˆˆ{1,...,ğ¾}.
Soft prompt learning replaces the hand-crafted prompts with
learnable prompts. CoOp [46]and CoCoOp [45]incorporate ğ‘learn-
able tokens{u1,..., uğ‘}for modeling text prompts, whereas MaPLe
[26]extends the approach by introducing an additional set of ğ‘
learnable tokens{v1,..., vğ‘}to model vision prompts. Denote ğ’„ğ‘˜
as the word embedding of the ğ‘˜-th class name, the text prompt of
ğ‘˜-th class is formulated as tğ‘˜={u1,..., uğ‘,ğ’„ğ‘˜}. For an image x, its
visual prompt is represented as f={v1,..., vğ‘,x}. Therefore, the
probability that xbelongs to the ğ‘˜-th class can be expressed as:
ğ‘soft(ğ‘¦=ğ‘˜|x)=exp(âŸ¨ğºğ‘£(f),ğºğ‘™(tğ‘˜)âŸ©/ğœ†)
Ãğ¾
ğ‘–=1exp(âŸ¨ğºğ‘£(f),ğºğ‘™(tğ‘–)âŸ©/ğœ†). (1)
Note that the prediction probability can be represented by replacing
ğºğ‘£(f)withğºğ‘£(x)=zin hand-crafted prompt engineering and
language-only prompt tuning.
3.2 Neural Collapse in Linear Classifier
Neural collapse describes that, at the terminal training phase, the
within-class means together with the linear classifier vectors will
collapse to the vertices of a simplex equiangular tight frame (ETF),
which is defined as follows.
Definition 1 (Simplex Eqiangular Tight Frame). Consider
a set of vectors mğ‘–âˆˆRğ‘‘, withğ‘–=1,Â·Â·Â·,ğ¾, andğ‘‘â‰¥ğ¾âˆ’1. This
collection of vectors forms a simplex equiangular tight frame which
requires:
ğ‘´=âˆšï¸‚
ğ¾
ğ¾âˆ’1u
ğ‘°ğ¾âˆ’1
ğ¾1ğ¾1ğ‘‡
ğ¾
, (2)
where ğ‘´=[m1,Â·Â·Â·,mğ¾]âˆˆRğ‘‘Ã—ğ¾,uâˆˆRğ‘‘Ã—ğ¾allows a rotation and
satisfies uğ‘‡u=ğ‘°ğ¾,ğ‘°ğ¾is the identity matrix, and 1ğ¾is an all-ones
vector. All vectors in a simplex ETF have an equal â„“2norm and the
same pair-wise angle, i.e.,
mğ‘‡
ğ‘–mğ‘—=ğ¾
ğ¾âˆ’1ğ›¿ğ‘–,ğ‘—âˆ’1
ğ¾âˆ’1,âˆ€ğ‘–,ğ‘—âˆˆ[ğ¾], (3)
whereğ›¿ğ‘–,ğ‘—equals to 1 when ğ‘–=ğ‘—and 0 otherwise. The pair-wise angle
âˆ’1
ğ¾âˆ’1is the maximal equiangular separation of ğ¾vectors in Rğ‘‘[27].
Denote zğ‘˜,ğ‘–as the last-layer image representation of the ğ‘–-th
sample in the ğ‘˜-th class, the prototype zğ‘˜=Avgğ‘–{zğ‘˜,ğ‘–}is the mean
value of the features in the ğ‘˜-th class, ğ’˜ğ‘˜is the classifier vector
of theğ‘˜-th class. We define three properties instructed by neural
collapse in vision-only models below:
Feature Collapse: For any ğ‘˜âˆˆ{1,...,ğ¾}, the last-layer features in
theğ‘˜-th class collapse to their prototype, i.e., the ğ‘˜-th within-class
covariance Î£ğ‘˜â†’0, where Î£ğ‘˜:=Avgğ‘˜,ğ‘–n zğ‘˜,ğ‘–âˆ’zğ‘˜  zğ‘˜,ğ‘–âˆ’zğ‘˜ğ‘‡o
andAvg{Â·}indicates the average operation.
Prototype Collapse: For any ğ‘˜âˆˆ{1,...,ğ¾}, the normalized pro-
totypes converge to a simplex ETF, i.e., Ëœzğ‘˜=(zğ‘˜âˆ’zğº)/âˆ¥zğ‘˜âˆ’zğºâˆ¥,
satisfies Eq. 3, where zğºis the global mean of the last-layer features,
i.e.,zğº=Avgğ‘˜,ğ‘–{zğ‘˜,ğ‘–}.Classifier Collapse: For any ğ‘˜âˆˆ{1,...,ğ¾}, the normalized classi-
fier vectors converge to the same simplex ETF as prototypes, i.e.,
Ëœğ’˜ğ‘˜=ğ’˜ğ‘˜/âˆ¥ğ’˜ğ‘˜âˆ¥=zğ‘˜, satisfies Eq. 3.
4 METHOD
4.1 Explore Neural Collapse in CLIP
The intriguing neural collapse (NC) phenomenon has primarily
been observed and examined in vision-only models. In this paper,
we investigate the corresponding structures in the vision-language
model CLIP, a model that performs recognition by evaluating simi-
larities between text and image representations. In consideration of
the multi-modality aspect of the V-L model, we design two evalua-
tion criteria during training to assess the presence of neural collapse
phenomena in the V-L model. The definitions are as follows:
Definition 2 (Language-modality Collapse Degree). Given
a text encoder ğºğ‘™(Â·), the language-modality collapse degree (LCD) of
text representations is given by
Î”LCD=Avgğ‘–â‰ ğ‘—
âŸ¨ğºğ‘™(tğ‘–),ğºğ‘™(tğ‘—)âŸ©âˆ’ğ¸ğ‘ŠÂ·ğœ‡	
,
s.t.âˆ¥ğºğ‘™(tğ‘–)âˆ¥2â‰¤ğ¸ğ‘Š,âˆ€1â‰¤ğ‘–â‰¤ğ¾,
whereğœ‡=âˆ’1
ğ¾âˆ’1,tğ‘–,tğ‘—are text prompts of ğºğ‘™(Â·)andâˆšğ¸ğ‘Šis the
fixed length constraint for text representations.
Definition 3 (Multi-modality Isomorphism Degree). Given
a text encoder ğºğ‘™(Â·)and an image encoder ğºğ‘£(Â·), the multi-modality
isomorphism degree (MID) is given by
Î”MID=Avgğ‘˜,ğ‘–n
âŸ¨ğºğ‘£(fğ‘˜,ğ‘–),ğºğ‘™(tğ‘˜)âŸ©âˆ’âˆšï¸
ğ¸ğ‘ŠÂ·ğ¸ğ»o
,
s.t.âˆ¥ğºğ‘™(tğ‘–)âˆ¥2â‰¤ğ¸ğ‘Š,âˆ€1â‰¤ğ‘–â‰¤ğ¾,
ğºğ‘£(fğ‘˜,ğ‘–)2â‰¤ğ¸ğ»,âˆ€1â‰¤ğ‘˜â‰¤ğ¾,1â‰¤ğ‘–â‰¤ğ‘›ğ‘˜,
where fğ‘˜,ğ‘–is the image input of xğ‘–in theğ‘˜-th class and ğ‘›ğ‘˜is the
number of samples belonging to the ğ‘˜-th class.âˆšğ¸ğ»is the fixed length
constraint for image features.
Remark 1. Î”LCDmeasures the degree of separation among text
representations (equivalent to classification vectors in vision-only mod-
els), reflecting the proximity between the language-modality structure
and the simplex ETF. Î”MIDcharacterizes the discrepancy between
the image representation and its text representation, reflecting the
alignment degree of language- and vision-modality structure. Lower
Î”LCDandÎ”MIDindicate that text and image representations are more
aligned and closer to the same simplex ETF.
As illustrated in Fig. 2, we analyzed the variations in Î”LCDand
Î”MIDvalues across different class distributions on the base-to-novel
task. The reported values represent the average across 11 datasets.
Furthermore, we visualize the neural collapse degree for the novel
classes on the EuroSAT dataset, as depicted in Fig. 1. Under balanced
training data conditions, the current SOTA prompt tuning method
MaPLe exhibits relatively low Î”LCDandÎ”MIDvalues compared
with ZSCLIP and other methods. This suggests a clear correlation
between these values and model accuracy. However, when encoun-
tering imbalanced training data, an observable increase in both
values is detected, leading to a subsequent decline in accuracy. We
can deduce that: (1) a higher degree of neural collapse, i.e., smaller
values of Î”LCDandÎ”MID, leads to stronger generalizability, and
4634Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Image Features
Text Features
Image-Text Similarity
Imbalanced 
Text FeaturesCollapsed 
Text FeaturesCollapsed 
Image Features
Text 
Encoder
Image
Encoder
Text 
Prompts
Text Inputs 
[CLS]
Visual 
Prompts 
(Optional)
Input Images
Text Features
Image Features...
...
Figure 3: Overview of Neural-collapse-anchored Prompt Tuning (NPT). NPT capitalizes on the benefits of two distinct regu-
larizers: LC Regularizer LLC, which controls the increase in Î”LCDand fosters the generation of more discriminative textual
representations; and MI Regularizer LMI, which promotes enhanced multi-modal alignment to address challenges and attain a
reduced Î”MID.
(2) data imbalance during training disrupts the neural collapse de-
gree, subsequently affecting generalization performance. These
observations can be explicitly represented as:
ğœâ†“=â‡’Î”LCDâ†‘,Î”MIDâ†‘=â‡’Accâ†“ (4)
Based on these observations, we propose the Neural-collapse-
anchored Prompt Tuning (NPT) method, which optimizes the prompt
by introducing two regularizers to implicitly address the drop in
these values even under class imbalance. As illustrated in Fig. 1
(d) and Fig. 2 (b), our NPT enables multi-modality structures to ap-
proach the simplex ETF, thereby improving CLIPâ€™s generalization
performance.
4.2 Neural-collapse-anchored Prompt Tuning
In this subsection, we propose two neural-collapse-anchored regu-
larization terms: Language-modality Collapse (LC) Regularizer and
Multi-modality Isomorphism Degree (MI) Regularizer, which are
guided by Î”LCDandÎ”MIDrespectively.
Language-modality Collapse Regularization is introduced to
address the issue in language-modality where certain text repre-
sentations are situated exceptionally close while others remain
distantly apart. Achieving an equitable and maximal separation
among text representations is the key to the generalizability of
CLIP. Hence, the LC regularizer encourages the similarity between
any two text representations from different classes to approach ğœ‡
(specified in Definition 1):
LLC=ğ¾âˆ‘ï¸
ğ‘–=1ğ¾âˆ‘ï¸
ğ‘—=1,ğ‘–â‰ ğ‘— âŸ¨ğºğ‘™(tğ‘–),ğºğ‘™(tğ‘—âŸ©âˆ’ğ¸ğ‘ŠÂ·ğœ‡)2. (5)
whereâˆ¥ğºğ‘™(tğ‘–)âˆ¥2â‰¤ğ¸ğ‘Š,ğ‘–=1,...,ğ¾ .ğ¸ğ‘Šis the class-wise con-
straints for text representation ğºğ‘™(tğ‘–).
Multi-modality Isomorphism Regularization aims to tackle the
issue of multi-modality isomorphism degree (MID) by optimizingthe alignment between language and visual modalities as follows:
LMI=ğ¾âˆ‘ï¸
ğ‘˜=1ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–=1
âŸ¨ğºğ‘£(fğ‘˜,ğ‘–),ğºğ‘™(tğ‘˜)âŸ©âˆ’âˆšï¸
ğ¸ğ‘ŠÂ·ğ¸ğ»2
, (6)
whereâˆ¥ğºğ‘£(fğ‘˜,ğ‘–)âˆ¥2â‰¤ğ¸ğ»,ğ‘˜=1,...,ğ¾ .ğ¸ğ»is the instance-wise con-
straints for image representation ğºğ‘£(fğ‘˜,ğ‘–). By minimizing the MI
loss term, our goal is to foster strong alignment between language
and visual representations, which in turn improves model perfor-
mance when handling diverse and imbalanced data distributions.
Overall Framework. By incorporating our proposed regulariza-
tion terms into the contrastive loss function within the current
prompt learning techniques, we can optimize the prompts effec-
tively using the overall loss:
L=LCLIP+ğ‘¤1LLC+ğ‘¤2LMI (7)
whereLCLIP=âˆ’1
ğ‘Ã
ğ’™âˆˆXÃğ¾
ğ‘˜=1ğ‘¦x,ğ‘˜ğ‘soft(ğ‘¦=ğ‘˜|x),ğ‘¤1andğ‘¤2are
hyperparameters governing the contribution of each component.
As illustrated in Fig. 3, the proposed approach harnesses the
strengths of both regularizers: LC Regularizer for minimizing Î”LCD
value and developing more discriminative text representations, and
MI Regularizer for facilitating better multi-modality alignment to
achieve a lower Î”MID.
4.3 Theoretical Analysis
Gradient w.r.t. text prompt under class imbalance. We first
analyze the gradient of LCLIP w.r.t text representations:
ğœ•LCLIP
ğœ•ğºğ‘™(tğ‘˜)=ğ‘›ğ‘˜âˆ‘ï¸
ğ‘–=1 ğ‘ğ‘˜ zğ‘˜,ğ‘–âˆ’1zğ‘˜,ğ‘–
|                      {z                      }
intra-class cohesion+ğ¾âˆ‘ï¸
ğ‘˜â€²â‰ ğ‘˜ğ‘›ğ‘˜â€²âˆ‘ï¸
ğ‘—=1ğ‘ğ‘˜
zğ‘˜â€²,ğ‘—
zğ‘˜â€²,ğ‘—
|                       {z                       }
inter-class repulsion,
where zğ‘˜,ğ‘–=ğºğ‘£(fğ‘˜,ğ‘–),ğ‘ğ‘˜(zğ‘˜,ğ‘–)=ğ‘soft(ğ‘¦=ğ‘˜|xğ‘˜,ğ‘–)is the
predicted probability that zğ‘˜,ğ‘–belongs to the ğ‘˜-th class.
4635KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Didi Zhu et al.
Table 1: Harmonic mean values (%) on 11 datasets with different imbalance ratios on the base-to-novel task.
Metho
dImbalance
Ratioğœ=1(Balance)
IN.
Cal. OP. SC. Flw. Food. FA. SUN. DTD ES. UCF. Avg
CLIP 70.22
95.40 94.12 68.65 74.83 90.66 31.09 72.23 56.37 60.03 73.85 71.58
CoCoOp 73.10
95.84 96.43 72.01 81.71 90.99 27.74 78.27 64.85 71.21 77.64 75.44
MaPLe 73.47
96.02 96.58 73.47 82.56 91.38 36.50 79.75 68.16 82.35 80.77 78.27
CoCoOp
+ NPT 73.78 97.05 96.88 72.50 83.01 91.20 35.03 78.67 66.88 74.52 78.82 77.12
MaPLe
+ NPT 74.02 96.46 96.54 72.71 82.94 91.75 35.54 79.88 69.20 82.85 81.42 78.48
Metho
dImbalance
Ratioğœ=0.05
IN.
Cal. OP. SC. Flw. Food. FA. SUN. DTD ES. UCF. Avg
CoCoOp 70.34
95.29 95.11 69.48 75.03 89.61 21.78 74.67 57.94 62.33 72.83 71.31
MaPLe 70.93
95.38 94.22 70.25 76.59 88.71 28.62 75.98 55.15 51.10 75.43 71.12
CoCoOp
+ NPT 71.21 95.95 95.97 70.69 75.72 90.13 23.28 75.67 59.69 63.78 74.33 72.40
MaPLe
+ NPT 72.28 95.82 96.31
71.07 78.76 90.47 30.24 76.94 57.35 58.14 76.65 73.09
Metho
dImbalance
Ratioğœ=0.01
IN.
Cal. OP. SC. Flw. Food. FA. SUN. DTD ES. UCF. Avg
CoCoOp 68.88
94.54 94.71 68.45 71.24 88.31 11.48 73.87 52.39 57.45 70.11 68.31
MaPLe 70.10
95.53 94.21 69.29 72.29 89.74 21.89 75.00 47.37 53.97 73.85 69.38
CoCoOp
+ NPT 70.23 95.16 95.66 70.17 73.50 90.08 20.34 74.42 54.97 62.95 71.01 70.77
MaPLe
+ NPT 70.89 95.64 94.56 70.58 74.46 90.20 27.72 75.89 56.98 58.48 74.95 71.85
0 2 4 6 8 10
Absolute improvemnet (%)ES.OP.Flw.FA.SC.UCF .Food.IN.DTDSUN.Cal.Datasets
+10.27+2.7+1.53+1.38+1.27+1.14+1.14+0.73+0.57+0.46+0.23Ours vs. MaPLe in Base Classes with =0.05
0 1 2 3 4
Absolute improvemnet (%)ES.DTDFlw.Food.FA.IN.OP.UCF .SUN.Cal.SC.Datasets
+4.5+3.1+2.7+2.4+1.89+1.87+1.43+1.3+0.7+0.64+0.26Ours vs. MaPLe in Novel Classes with =0.05
0 1 2 3 4
Absolute improvemnet (%)ES.DTDFA.UCF .Flw.SC.Food.SUN.IN.OP.Cal.Datasets
+4.64+4.47+4.2+2.47+1.9+1.21+0.6+0.5+0.33+0.3+0.04Ours vs. MaPLe in Base Classes with =0.01
0 2 4 6 8 10 12
Absolute improvemnet (%)DTDFA.ES.Flw.SC.SUN.IN.UCF .OP.Food.Cal.Datasets
+11.67+5.64+4.36+2.46+1.37+1.3+0.7+0.6+0.4+0.3+0.18Ours vs. MaPLe in Novel Classes with =0.01
Figure 4: Absolute improvement of NPT over MaPLe in the base-to-novel generalization task. Compared with MaPLe, our
method achieves improvement for both base and new classes on all datasets with ğœ=0.05andğœ=0.01.
Remark 2. The gradient equation is composed of two terms. The
"intra-class cohesion" term comprises ğ‘›ğ‘˜elements, directs ğºğ‘™(tğ‘˜)to-
wards its corresponding image representations zğ‘˜,ğ‘–. In contrast, the
"inter-class repulsion" term, with ğ‘âˆ’ğ‘›ğ‘˜elements, pushes ğºğ‘™(tğ‘˜)
against features of other classes. Consequently, gradients of certain
minor classes are overwhelmingly influenced by the repulsion term
due to the small ğ‘›ğ‘˜and largeğ‘âˆ’ğ‘›ğ‘˜.
To tackle this issue, we introduce LLC. The corresponding gra-
dient equation w.r.t. tğ‘–is defined as:
ğœ•LLC
ğœ•tğ‘–=ğ¾âˆ‘ï¸
ğ‘—=1,ğ‘–â‰ ğ‘—2 ğ‘†ğ‘–ğ‘—âˆ’ğ¸ğ‘ŠÂ·ğœ‡ğœ•ğºğ‘™(tğ‘–)
ğœ•tğ‘–,ğºğ‘™ tğ‘—
,
whereğ‘†ğ‘–ğ‘—=
ğºğ‘™(tğ‘–),ğºğ‘™(tğ‘—)
.LLCcan explicitly adjust the prompt
tğ‘–to ensure text representations are maximally angularly separated,
i.e.,ğ‘†ğ‘–ğ‘—âˆ’ğ¸ğ‘ŠÂ·ğœ‡. This regularizer effectively mitigates the adverse
effects of imbalanced gradient updates in LCLIP.Gradient w.r.t. image prompt. The gradient ofLCLIP with respect
to the image representation zğ‘–is:
ğœ•LCLIP
ğœ•ğºğ‘£(fğ‘–)=(ğ‘ğ‘˜(zğ‘˜,ğ‘–)âˆ’1)ğºğ‘™(tğ‘˜)+ğ¾âˆ‘ï¸
ğ‘˜â€²â‰ ğ‘˜ğ‘ğ‘˜â€²(zğ‘˜,ğ‘–)ğºğ‘™(tğ‘˜â€²).
where zğ‘˜,ğ‘–=ğºğ‘£(fğ‘˜,ğ‘–),ğ‘˜is the class label of zğ‘˜,ğ‘–.
Remark 3. This gradient formula is similar toğœ•LCLIP
ğœ•ğºğ‘™(tğ‘˜). It aims
to bring image representations closer to text representations of the
same class and farther away from those of other classes. However,
text representations can be influenced by imbalance, causing the
representations of minor classes to be close together. Optimizing image
representations towards biased text representations can compromise
performance.
Similarly, we analyze the gradient of LMIw.r.t. fğ‘˜,ğ‘–:
ğœ•LMI
ğœ•fğ‘˜,ğ‘–=2
ğ‘†â€²
ğ‘–ğ‘—âˆ’âˆšï¸
ğ¸ğ‘Šğ¸ğ»ğœ•ğºğ‘£ fğ‘˜,ğ‘–
ğœ•fğ‘˜,ğ‘–,ğºğ‘™(tğ‘˜)
.
whereğ‘†â€²
ğ‘–ğ‘˜=
ğºğ‘£(fğ‘˜,ğ‘–),ğºğ‘™(tğ‘˜).LMIensures that image represen-
tations closely align with text representations of the same class,
4636Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0.1 0.2 0.3 0.4 0.5
w16065707580Acc(%)
Base
New
0.1 0.2 0.3 0.4 0.5
w26065707580Acc(%)
Base
New
Figure 5: Sensitivity analysis of ğ‘¤1andğ‘¤2under base-to-
novel task with ğœ=0.01.
uninfluenced by imbalances, as represented by ğ‘†â€²
ğ‘–ğ‘˜âˆ’âˆšğ¸ğ‘Šğ¸ğ». This
bolsters the discriminative power, especially for minor classes.
5 EXPERIMENT
Our approach is evaluated on (1) base-to-novel class identifica-
tion, (2) cross-dataset transfer, and (3) domain generalization, un-
der three class imbalance degrees. All trained data are created
by downsampling each classâ€™s samples to obey an exponential
decay with three imbalance ratios ğœ={1,0.05,0.01}. Hereğœ=
min{ğ‘›ğ‘˜}/max{ğ‘›ğ‘˜}withğ‘›ğ‘˜being the number of samples in the
ğ‘˜-th class and max{ğ‘›ğ‘˜}=16.
Datasets. For the first two settings, i.e., base-to-novel generaliza-
tion and cross-dataset transfer, we use the 11 image recognition
datasets as in [45,46]. For domain generalization experiments, we
use ImageNet as the source dataset and four other variants of Im-
ageNet that contain different types of domain shifts as the target
dataset, following [45].
Implementation Details. Since our method is orthogonal to the
other prompt tuning methods, we combine NPT with the existing
CoCoOp and MaPLe, to demonstrate its flexibility and effectiveness.
We apply prompt tuning on ViT-B/16 CLIP where ğ‘‘=512. All
models are trained for 5 epochs with a batch size of 4 and a learning
rate of 0.0035 via SGD optimizer. Both ğ¸ğ‘Šandğ¸ğ»are set at 1, while
ğ‘¤1andğ‘¤2are assigned as 0.3and0.8, respectively. We follow other
default hyperparameters used in MaPLe [26].
5.1 Base-to-Novel Generalization
Following [10,45], for each dataset, we split the classes equally into
two groups: the base classes and the novel classes. The learnable
modules are trained exclusively on the imbalanced base classes,
while evaluation is carried out separately on both the base and
novel classes to testify to generalization ability. Table 1 presents the
harmonic mean values for 11 datasets on the base-to-novel tasks.
In the first subtable, we showcase the improvement of NPT on bal-
anced datasets under the base-to-novel task. Our NPT can improve
generalization for most of the datasets (9/11), and the maximal
performance gain is up to 1.3%. When trained on imbalanced base
classes, both CoCoOp and MaPLe see performance dips of 4.13%
and 7.15% respectively at ğœ=0.05, and 7.13% and 8.89% at ğœ=0.01,
underscoring the catastrophic impact of class imbalance on general-
ization. Our method exhibits improvements of 1.97% when ğœ=0.05,
and improvements of 2.47% when ğœ=0.01over MaPLe.We further compare the accuracy gains of MaPLE on both novel
and base classes before and after integrating NPT in Fig. 4. Our
method significantly outperforms MaPLe in both base and novel
classes across all 11 datasets. Interestingly, we observe that NPT
offers a more substantial improvement in recognizing novel classes,
highlighting its potential in challenging generalization scenarios.
5.2 Cross-Dataset Transfer
We evaluate the generalization ability of our method on more chal-
lenging cross-dataset tasks. In this setting, we learn prompts on
ImageNet of 1000 classes with three imbalance ratios. The effective-
ness of the learned prompts is then tested on the other 10 datasets.
The results are reported in Table 2. The same performance drop
phenomenon can be observed in cross-dataset tasks due to data im-
balance, and combining NPT with CoCoOp and MaPLe can improve
this drop. MaPLe+NPT achieves the best results at all imbalance
ratios, demonstrating the great transfer ability of our method.
5.3 Domain Generalization
We perform experiments with ImageNet as the source domain and
evaluate its ability to generalize to four distinct ImageNet variants,
which serve as unseen target domains. Table 4 presents the results,
indicating that our method significantly improves the performance
on the four ImageNet variants with three imbalance ratios. This
confirms that our approach enhances the discriminability of the
source dataset while preserving its generalization to the target
domain.
5.4 Ablation Study
Effectiveness of each module. We conduct ablation experiments
on three tasks with ğœ=0.05andğœ=0.01to assess the effectiveness
of theLLCDandLMIDof NPT, as presented in Table 3. The results
demonstrate that each module has a significant positive impact on
the modelâ€™s performance. Specifically, for the base-to-novel task,
the combination of LLCDandLMIDmodules leads to remarkable
improvements of 1.88% at ğœ=0.05and 2.47% at ğœ=0.01. These
findings suggest that both modules play a crucial role in enhancing
the modelâ€™s generalization ability.
Sensitivity Analysis of ğ‘¤1andğ‘¤2.In Fig. 3, we analyze the
impact of the hyperparameters ğ‘¤1andğ‘¤2of Eq. 7. The results show
that our methodâ€™s performance remains relatively stable across a
broad range of ğ‘¤1andğ‘¤2values, indicating the robustness of our
approach to these hyperparameters. This suggests that our method
can be effectively applied to a variety of downstream tasks with
different parameter settings.
5.5 Neural Collapse Visualization
As shown in Figure 6, we visualize the distribution of text represen-
tations and image representations for novel classes on base-to-novel
task in the EuroSAT dataset, considering both balanced and imbal-
anced conditions. The evaluated methods include zero-shot CLIP
(ZSCLIP), current soft prompt tuning methods, and our proposed
approach. Under the balanced condition, ZSCLIP demonstrates a
relatively poor degree of neural collapse. Interestingly, CoOp also
exhibits a significant degree of neural collapse attributed to overfit-
ting on base classes, resulting in limited generalization performance
4637KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Didi Zhu et al.
Table 2: Comparison of NPT with existing approaches on cross-dataset task.
Imbalance
Ratioğœ=1(Balance)
IN.â†’ Cal.
OP. SC. Flw. Food. FA. SUN. DTD ES. UCF. Avg
CoOp 71.51
93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 46.39 66.55 63.88
CoCoOp 71.02
94.43 90.14 65.32 71.88 86.06 22.94 67.36 45.73 45.37 68.21 65.74
MaPLe 70.72
93.53 90.49 65.57 72.23 86.20 24.74 67.01 46.49 48.06 68.69 66.30
CoCoOp
+ NPT 71.22 94.58 91.22 66.91 71.82 86.45 24.34 67.88 45.89 46.71 69.13 66.49
MaPLe
+ NPT 70.70 94.50 90.40 66.73 72.33 86.00 25.23 68.37 47.63 48.17 68.87 66.82
Imbalance
Ratioğœ= 0.05
IN.â†’ Cal.
OP. SC. Flw. Food. FA. SUN. DTD ES. UCF. Avg
CoCoOp 69.93
93.18 89.88 65.09 71.02 85.21 20.02 66.89 44.25 44.34 66.53 64.64
MaPLe 69.20
92.23 90.34 65.17 71.54 85.03 21.39 65.57 45.84 47.83 65.34 65.03
CoCoOp
+ NPT 70.14 93.52 91.56 66.43 71.43 85.98 22.34 67.42 45.33 44.90 67.24 65.62
MaPLe
+ NPT 69.40 93.31 90.64 65.95 72.62 86.19 22.78 66.89 46.05 50.25 67.12 66.18
Imbalance
Ratioğœ= 0.01
IN.â†’ Cal.
OP. SC. Flw. Food. FA. SUN. DTD ES. UCF. Avg
CoCoOp 68.42
92.45 89.21 64.34 70.32 84.13 19.88 65.07 44.09 44.18 64.31 63.80
MaPLe 68.32
92.63 90.07 64.38 70.37 84.77 20.47 64.53 45.17 47.03 64.40 64.38
CoCoOp
+ NPT 68.88 93.03 90.12 65.22 70.88 85.65 20.87 66.23 45.34 44.89 65.91 64.81
MaPLe
+ NPT 68.90 93.60 89.93 65.28 71.77 86.21 21.51 66.07 44.97 49.67 66.20 65.52
MaPLe + NPT MaPLe ZSCLIPBalance ImbalanceCoOp CoCoOp
MaPLe + NPT MaPLe ZSCLIP CoOp CoCoOp
Figure 6: Comparison of NPT with standard prompt learning methods. (a) Existing methods suffer from lower generalization
capabilities in the presence of class imbalance due to the lack of constraints on feature structures. (b) NPT addresses this issue
by ensuring that text features and image features adhere to a common simplex ETF structure.
on novel classes. However, both CoCoOp and MaPLe improve this
generalization aspect by displaying an evident neural collapse. Our
proposed method showcases a similar level of neural collapse as
observed in MaPLe. Nonetheless, when confronted with imbalanced
scenarios, both CoCoOp and MaPLe suffer from a substantial de-
cline in performance due to neural collapse. By integrating our NPT
method into the framework, we mitigate this degradation to some
extent, leading to enhanced overall performance.CONCLUSIONS
In this paper, we explore the text-to-image representation geometry
of CLIP prompt tuning from the perspective of neural collapse. It is
found that class imbalance has detrimental effects on representation,
thus hurting generalization. We propose Neural-collapse-anchored
Prompt Tuning (NPT), a method that optimizes prompts to en-
sure text and image representations share the same simplex ETF
structure. By incorporating language-modality collapse and multi-
modality isomorphism regularization terms, NPT enhances V-L
4638Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Ablation study of each loss module.
Base-to-Novel Cross-Dataset Domain Generalization
Imbalance Ratio ğœ= 0.05
MaPLe 71.21 65.03 59.62
+LLC 71.88 65.48 59.41
+LMI 72.39 65.21 59.88
+ NPT 73.09 66.18 60.06
Imbalance Ratio ğœ= 0.01
MaPLe 69.38 64.38 59.12
+LLC 70.02 64.88 59.45
+LMI 70.43 65.23 59.48
+ NPT 71.85 65.52 59.53
Table 4: Comparison results on domain generalization task.
Imbalance
Ratioğœ=1(Balance)
IN.â†’ IN.-
V2 IN.-S IN.-A IN.-R Avg
CoOp 71.51
64.20 47.99 49.71 75.21 59.28
CoCoOp 71.02
64.07 48.75 50.63 76.18 59.91
MaPLe 70.72 64.07 49.15 49.90
76.98 60.23
MaPLe+NPT 70.70 65.03 48.27 50.70 77.33 60.33
Imbalance
Ratioğœ= 0.05
IN.â†’ IN.-
V2 IN.-S IN.-A IN.-R Avg
CoCoOp 68.43
62.21 46.89 48.56 75.64 60.37
MaPLe 69.20
62.40 48.27 50.98 76.83 59.62
CoCoOp+NPT 69.65 64.32 47.41 48.99 77.89 61.65
MaPLe+NPT 69.40 63.10 48.83 51.10 77.20 60.06
Imbalance
Ratioğœ= 0.01
IN.â†’ IN.-
V2 IN.-S IN.-A IN.-R Avg
CoCoOp 66.21
60.35 46.13 50.25 74.56 57.82
MaPLe 67.32
62.33 47.20 50.47 76.47 59.12
CoCoOp+NPT 67.57 62.04 47.55 50.38 75.88 58.96
MaPLe+NPT 68.70 62.83 48.53 50.07 76.70 59.53
modelsâ€™ generalizability, especially when faced with novel and im-
balanced class scenarios. Experiments show that NPT outperforms
existing techniques across 11 diverse datasets under base-to-novel,
cross-dataset, and domain generalization tasks.
ACKNOWLEDGEMENTS
This work was supported by the National Key Research and Devel-
opment Project of China (2021ZD0110505), the National Natural
Science Foundation of China (62441605, 62376243), the Zhejiang
Provincial Key Research and Development Project (2023C01043),
and the Academy Of Social Governance Zhejiang University.
REFERENCES
[1]Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. 2022.
Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models. arXiv
preprint arXiv:2203.17274 (2022).
[2]Limeng Cui, Xianfeng Tang, Sumeet Katariya, Nikhil Rao, Pallav Agrawal, Karthik
Subbian, and Dongwon Lee. 2022. ALLIE: Active learning on large-scale imbal-
anced graphs. In Proceedings of the ACM Web Conference 2022. 690â€“698.
[3]Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. 2021. Exploring deep neu-
ral networks via layer-peeled model: Minority collapse in imbalanced training.
Proceedings of the National Academy of Sciences 118, 43 (2021), e2103091118.
[4]Xingcheng Fu, Yuecen Wei, Qingyun Sun, Haonan Yuan, Jia Wu, Hao Peng,
and Jianxin Li. 2023. Hyperbolic Geometric Graph Representation Learning for
Hierarchy-imbalance Node Classification. arXiv preprint arXiv:2304.05059 (2023).[5]Tao Guo, Song Guo, and Junxiao Wang. 2023. pFedPrompt: Learning Personalized
Prompt for Vision-Language Models in Federated Learning. In Proceedings of the
ACM Web Conference 2023. 1364â€“1374.
[6]Chenxi Huang, Liang Xie, Yibo Yang, Wenxiao Wang, Binbin Lin, and Deng Cai.
2023. Neural Collapse Inspired Federated Learning with Non-iid Data. arXiv
preprint arXiv:2303.16066 (2023).
[7]Tony Huang, Jack Chu, and Fangyun Wei. 2022. Unsupervised Prompt Learning
for Vision-Language Models. arXiv preprint arXiv:2204.03649 (2022).
[8]Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie J Su. 2021. An
unconstrained layer-peeled perspective on neural collapse. arXiv preprint
arXiv:2110.02796 (2021).
[9]Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-
language representation learning with noisy text supervision. PMLR, 4904â€“4916.
[10] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan,
and Fahad Shahbaz Khan. 2023. Maple: Multi-modal prompt learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
19113â€“19122.
[11] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo
Yu. 2022. Taxocom: Topic taxonomy completion with hierarchical discovery of
novel topic clusters. In Proceedings of the ACM Web Conference 2022. 2819â€“2829.
[12] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for
parameter-efficient prompt tuning.
[13] Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui
Zhu, and Qing Qu. 2022. Principled and Efficient Transfer Learning of Deep
Models via Neural Collapse. arXiv preprint arXiv:2212.12206 (2022).
[14] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous
prompts for generation.
[15] Zexi Li, Xinyi Shang, Rui He, Tao Lin, and Chao Wu. 2023. No Fear of Classifier
Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed
Classifier. arXiv preprint arXiv:2303.10058 (2023).
[16] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021.
P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across
scales and tasks.
[17] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. 2022.
Prompt Distribution Learning. 5206â€“5215.
[18] Shuang Luo, Didi Zhu, Zexi Li, and Chao Wu. 2021. Ensemble federated adver-
sarial training with non-iid data. FTL-IJCAI 2021 (2021).
[19] Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024.
Intelligent model update strategy for sequential recommendation. In Proceedings
of the ACM on Web Conference 2024. 3117â€“3128.
[20] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei
Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al .2023. DUET:
A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework
for Efficient Device Model Generalization. In Proceedings of the ACM Web Confer-
ence 2023. 3077â€“3085.
[21] Weimin Lyu, Xinyu Dong, Rachel Wong, Songzhu Zheng, Kayley Abell-Hart,
Fusheng Wang, and Chao Chen. 2022. A Multimodal Transformer: Fusing Clin-
ical Notes with Structured EHR Data for Interpretable In-Hospital Mortality
Prediction. In AMIA Annual Symposium Proceedings, Vol. 2022. American Medical
Informatics Association, 719.
[22] Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, and
Chao Chen. 2024. Task-Agnostic Detector for Insertion-Based Backdoor Attacks.
arXiv preprint arXiv:2403.17155 (2024).
[23] Weimin Lyu, Songzhu Zheng, Haibin Ling, and Chao Chen. 2023. Backdoor At-
tacks Against Transformers with Attention Enhancement. In ICLR 2023 Workshop
on Backdoor Attacks and Defenses in Machine Learning.
[24] Weimin Lyu, Songzhu Zheng, Tengfei Ma, and Chao Chen. 2022. A Study of the
Attention Abnormality in Trojaned BERTs. In Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. 4727â€“4741.
[25] Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, and Chao Chen. 2023.
Attention-Enhancing Backdoor Attacks Against BERT-based Models. In Findings
of the Association for Computational Linguistics: EMNLP 2023. 10672â€“10690.
[26] Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan,
Rao Muhammad Anwer, and Ming-Hsuan Yang. 2022. Class-agnostic Object
Detection with Multi-modal Transformer. Springer.
[27] Vardan Papyan, XY Han, and David L Donoho. 2020. Prevalence of neural collapse
during the terminal phase of deep learning training. Proceedings of the National
Academy of Sciences 117, 40 (2020), 24652â€“24663.
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.2021. Learning transferable visual models from natural language supervision.
PMLR, 8748â€“8763.
[29] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina
Behnia. 2022. Imbalance trouble: Revisiting neural-collapse geometry. Advances
in Neural Information Processing Systems 35 (2022), 27225â€“27238.
4639KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Didi Zhu et al.
[30] Tom Tirer and Joan Bruna. 2022. Extended unconstrained features model for
exploring deep neural collapse. In International Conference on Machine Learning.
PMLR, 21478â€“21505.
[31] Yidong Wang, Zhuohao Yu, Jindong Wang, Qiang Heng, Hao Chen, Wei Ye, Rui
Xie, Xing Xie, and Shikun Zhang. 2023. Exploring Vision-Language Models for
Imbalanced Learning. arXiv preprint arXiv:2304.01457 (2023).
[32] E Weinan and Stephan Wojtowytsch. 2022. On the emergence of simplex sym-
metry in the final and penultimate layers of neural network classifiers. In Mathe-
matical and Scientific Machine Learning. PMLR, 270â€“290.
[33] Liang Xie, Yibo Yang, Deng Cai, and Xiaofei He. 2023. Neural collapse inspired
attraction-repulsion-balanced loss for imbalanced learning. Neurocomputing
(2023).
[34] Yibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, and Dacheng
Tao. 2022. Do we really need a learnable classifier at the end of deep neural
network? arXiv preprint arXiv:2203.09081 (2022).
[35] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, and Dacheng
Tao. 2023. Neural collapse inspired feature-classifier alignment for few-shot class
incremental learning. arXiv preprint arXiv:2302.03004 (2023).
[36] Hantao Yao, Rui Zhang, and Changsheng Xu. 2023. Visual-language prompt tun-
ing with knowledge-guided context optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 6757â€“6767.
[37] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiao-
dan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. Filip: Fine-grained
interactive language-image pre-training. arXiv preprint arXiv:2111.07783 (2021).
[38] Junkun Yuan, Xu Ma, Defang Chen, Kun Kuang, Fei Wu, and Lanfen Lin. 2022.
Label-Efficient Domain Generalization via Collaborative Exploration and Gener-
alization. In Proceedings of the 30th ACM International Conference on Multimedia.
2361â€“2370.
[39] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng
Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al .2021. Florence:
A new foundation model for computer vision. arXiv preprint arXiv:2111.11432
(2021).[40] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexan-
der Kolesnikov, and Lucas Beyer. 2022. Lit: Zero-shot transfer with locked-image
text tuning. 18123â€“18133.
[41] Min Zhang, Haoxuan Li, Fei Wu, and Kun Kuang. 2024. MetaCoCo: A New
Few-Shot Classification Benchmark with Spurious Correlation. In International
Conference on Learning Representations, ICLR.
[42] Min Zhang, Junkun Yuan, Yue He, Wenbin Li, Zhengyu Chen, and Kun Kuang.
2023. MAP: Towards Balanced Generalization of IID and OOD through Model-
Agnostic Adapters. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, ICCV. 11921â€“11931.
[43] Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing
Xu, Wenting Song, and Huajun Chen. 2023. Structure Pretraining and Prompt
Tuning for Knowledge Graph Transfer. In Proceedings of the ACM Web Conference
2023. 2581â€“2590.
[44] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu
Zhang, and Jiaya Jia. 2023. Understanding Imbalanced Semantic Segmentation
Through Neural Collapse. arXiv preprint arXiv:2301.01100 (2023).
[45] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Conditional
prompt learning for vision-language models. 16816â€“16825.
[46] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning
to prompt for vision-language models. (2022), 1â€“12.
[47] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. 2022. Prompt-
aligned Gradient for Prompt Tuning. arXiv preprint arXiv:2205.14865 (2022).
[48] Didi Zhu, Yinchuan Li, Yunfeng Shao, Jianye Hao, Fei Wu, Kun Kuang, Jun Xiao,
and Chao Wu. 2023. Generalized Universal Domain Adaptation with Generative
Flow Networks. In ACM International Conference on Multimedia (MM) 2023.
[49] Didi Zhu, Yinchuan Li, Junkun Yuan, Zexi Li, Kun Kuang, and Chao Wu. 2023.
Universal domain adaptation via compressive attention matching. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 6974â€“6985.
[50] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam,
and Qing Qu. 2021. A geometric analysis of neural collapse with unconstrained
features. Advances in Neural Information Processing Systems 34 (2021), 29820â€“
29834.
4640