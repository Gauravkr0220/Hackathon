Learn Together Stop Apart:
An Inclusive Approach to Ensemble Pruning
Bulat Ibragimov
ibrbulat@yandex.ru
Moscow Institute of Physics and Technology
Moscow, Russian Federation
Artificial Intelligence Research Institute (AIRI)
Moscow, Russian FederationGleb Gusev
gleb57@gmail.com
Sber AI Lab
Moscow, Russian Federation
ABSTRACT
Gradient Boosting is a leading learning method that builds ensem-
bles and adapts their sizes to particular tasks, consistently delivering
top-tier results across various applications. However, determining
the optimal number of models in the ensemble remains a critical yet
underexplored aspect. Traditional approaches assume a universal
ensemble size effective for all data points, which may not always
hold true due to data heterogeneity.
This paper introduces an adaptive approach to early stopping
in Gradient Boosting, addressing data heterogeneity by assigning
different stop moments to different data regions at inference time
while still training a common ensemble on the entire dataset. We
propose two methods: Direct Supervised Partition (DSP) and Indi-
rect Supervised Partition (ISP). The DSP method uses a decision tree
to partition the data based on learning curves, while ISP leverages
the datasetâ€™s geometric and target distribution characteristics.
An effective validation protocol is developed to determine the
optimal number of early stopping regions or detect when the het-
erogeneity assumption does not hold. Experiments using state-
of-the-art implementations of Gradient Boosting, LightGBM, and
CatBoost, on standard benchmarks demonstrate that our methods
enhance model precision by up to 2%, underscoring the significance
of this research direction. This approach does not increase com-
putational complexity and can be easily integrated into existing
learning pipelines.
CCS CONCEPTS
â€¢Computing methodologies â†’Boosting; Regularization ; Clas-
sification and regression trees.
KEYWORDS
Ensemble, Boosting, Regularization, Early Stopping, Decision Tree
ACM Reference Format:
Bulat Ibragimov and Gleb Gusev. 2024. Learn Together Stop Apart: an
Inclusive Approach to Ensemble Pruning. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3672018
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36720181 INTRODUCTION
Despite the rapid advancement of deep neural networks, classi-
cal machine learning algorithms like Gradient Boosting (GB) [ 18]
continue to outperform in several areas. Particularly, GB allows to
obtain high-quality models on structured (or tabular) data with no
multimedia (e.g., images, audio, videos), with categorical features,
noisy features and labels, and missing data [ 1,27,45]. Also, the
undoubted advantage of the boosting method is the relatively low
computational cost of training and inference [ 15]. For these reasons,
Gradient Boosting is widely used in ranking [ 7], recommender sys-
tems [ 9], automatic machine learning [ 25], and many other tasks
[29, 40, 41].
In recent years, new options and hyperparameters have been
proposed for GB, influencing its performance [ 23,24]. However,
one critical aspect affecting the effectiveness of GB, the choice
of the number of ensemble members (or the number of boosting
iterations), remains largely understudied. Larger models can reveal
complex dependencies in the data but are more time-consuming
and prone to overfitting on noisy datasets [ 19]. Conversely, smaller
models perform better on such data. The standard approach to this
problem, known as â€˜early stoppingâ€™, uses a hold-out validation set
to determine the ensemble size, selecting the modelâ€™s prefix with
the optimal validation score.
The standard early stopping method has a significant yet surpris-
ingly understudied weakness. It assumes a â€˜one-fits-allâ€™ universal
ensemble size equally effective for all data points. However, this
is often not the case, as the learning task can consist of different
subtasks, each corresponding to different regions in the input space
and functional dependencies. Some regions may have complex
surfaces requiring many boosting rounds for convergence, while
others may reveal simple but noisy dependencies where overfitting
occurs much earlier. As a result, the standard early stopping often
compromises simple and complex areas, leading to models, which
mix overfitted and underfitted regions.
Recognizing this limitation, we propose a new approach to early
stopping in GB based on an adaptive choice of the optimal size of
the ensemble. As in the standard version of GB [ 18], we train one
sequence of learners in an ensemble. However, our algorithm adapts
to the heterogeneity of the data in the feature space and assigns
different numbers of models to different data regions at inference
time. Specifically, we build an additional â€˜partition modelâ€™, which
sequentially divides the input space into regions of presumably
homogeneous complexity and representativity of data. Simultane-
ously, it learns an early stopping model, which optimizes the size
of the trained ensemble for each region individually. This adaptive
1166
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
approach allows for a more nuanced and compelling model that
can better handle the complexities of different data regions.
However, building such a model is a tricky task. We demonstrate
in Section 4.2 that straightforward approaches such as direct predic-
tion of the best iteration have fundamental drawbacks and do not
work. Another challenge lies in controlling the trade off between
partition complexity and the generalization ability of the early
stopping model. These are, in some sense, meta-hyperparameters
over hyperparameters since the partition model defines ensemble
sizes. To manage this construction, we have developed a specific
two-level cross-validation scheme.
Despite the apparent complexity, the proposed methods incur
meager computational costs and can be easily incorporated into
any existing learning pipeline and applied to any learning task with
arbitrary loss. We apply the proposed approach to state-of-the-art
open-source GB algorithms, LightGBM and CatBoost, and demon-
strate its ability to outperform consistently on standard publicly
available benchmarks for GB. We show that the described prob-
lem of the universal stopping moment highly affects the quality of
trained models. To our knowledge, this is the first research devoted
to effective adaptive early stopping in GB, and we anticipate that
this paper will stimulate further exploration of the GB algorithm.
The rest of the paper is organized as follows. Sections 3 and 2
briefly introduce notation and concepts of traditional Gradient
Boosting and early stopping algorithms. This is followed by the
core of the paper in Section 4, where we discuss some possible adap-
tive early stopping methods and their limitations and outline our
primary algorithm. Section 5 contains the description of the early
stopping validation protocol, a crucial component of the algorithm
that allows to obtain optimal stopping model parameters. The final
part of the paper is the experimental section, where we demon-
strate that our approach yields stable, significant improvements
in terms of quality on standard benchmarks and contemporary
implementations of Gradient Boosting.
2 RELATED WORK
2.1 Ensemble pruning
Pruning is a term often used to describe various techniques aimed
at compressing models for more efficient storage and inference
complexity. The classic work on this task by Margineantu and
Dietterich (1997) [ 32] compared five different pruning methods
applied to the boosting algorithm. In most cases, pruned models
were able to maintain or even enhance the original quality while
achieving a moderate reduction in size. Contemporary pruning
techniques are primarily predicated on the observation that similar
learners in the ensemble tend to duplicate dataset information,
suggesting they can be eliminated from a model sequence [ 5,26].
There have also been efforts to frame ensemble pruning as an
optimization problem, with solutions sought by applying genetic
algorithms [46] or semi-definite programming [44].
Another approach to prune ensemble is to prune individual es-
timators while enhancing the same (or even better) quality. For
instance, Friedmanâ€™s work on rule ensembles [ 20] introduces a
method for predictive learning that combines the interpretability
of rule-based models with the performance of ensemble methods.Similarly, Liu and Mazumderâ€™s ForestPrune [30] focuses on reduc-
ing the complexity of ensemble models through depth-pruning of
decision trees.
A number of studies [ 12,13] have tackled the issue of adaptive
online pruning within the context of Multiple Classifier Systems. In
these settings, classifiers are learned independently and selected via
meta-learning approaches. Oliveira et al. (2017) [ 34] and Hernandez
et al. (2008) [ 21] propose an instanceâ€“wise pruning methods that
allow for halting some models at inference time, while Soto et
al. (2014) [ 38] investigate both static (training time) and dynamic
(inference time) pruning in AdaBoost.
2.2 Gradient Boosting early stopping
Contrary to some other ensemble methods, including bagging, GB is
prone to overfitting when the ensemble size is large. Therefore, the
control of the number of boosting steps is primarily a regularization
technique [16]. Specifically, the original paper by Friedman (2001)
[18] provides direct guidance on tuning the number of models in
the ensemble (Section 5): "For additive expansions (2), a natural
regularization parameter is the number of components M".
Given that the size of GB is responsible for the expressiveness
of the ensemble, one of the ideas proposed in the literature [ 6,
33] is to penalize model complexity, for instance, through AIC-
based methods by approximating the ensembleâ€™s degrees of freedom.
Some studies employ generalization bounds of the algorithm using
VC-dimension [ 17], Rademacher complexity [ 11], or in the PAC
setting [ 42,43]. While these methods do not necessitate separate
validation control, they are not applicable in real-world tasks in
most cases as the obtained bounds are distribution-agnostic.
The standard approach of early stopping used in all modern GB
implementations utilizes the straightforward "waiting" concept. If
the validation quality does not improve for some "reasonable" num-
ber of iterations, then the training must be stopped (see, e.g., [ 10]).
In this paper, we adopt a standard early stopping scheme described
in Margineantu and Dietterich (1997) [ 32]: we shrink the model to
the firstğ‘€learners that yield the best validation score. Neverthe-
less, unlike all previous works on Gradient Boosting, instead of a
universal constant, we strive to select this number adaptively for
different regions of the input space, considering the distribution of
the training data.
3 BACKGROUND
In this section, we introduce necessary notations and briefly discuss
the fundamental concepts related to Gradient Boosting and cross-
validation for the sake of independent reading.
3.1 Gradient Boosting
LetS={ğ’™ğ‘–,ğ‘¦ğ‘–}ğ‘›
ğ‘–=1be a sample from some fixed but unknown dis-
tributionğ‘ƒ(ğ’™,ğ‘¦), where ğ’™ğ‘–=(ğ‘¥1
ğ‘–,...,ğ‘¥ğ‘š
ğ‘–)âˆˆXis anğ‘š-dimensional
feature representation and ğ‘¦ğ‘–âˆˆYis a target value of the ğ‘–-th obser-
vation. We consider the learning problem that consists in construct-
ing a function ğ¹:Xâ†’Yminimizing the expected target prediction
error, which is calculated using a loss function ğ¿:YÃ—Yâ†’R+:
L(ğ‘ƒ,ğ¹):=E(ğ’™,ğ‘¦)âˆ¼ğ‘ƒ[ğ¿(ğ¹(ğ’™),ğ‘¦)]â†’ min
ğ¹.
1167Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Since the distribution ğ‘ƒis not given, the task reduces to empirical
risk minimization problem:
Ë†L(S,ğ¹)=Ë†E(ğ’™,ğ‘¦)âˆ¼S[ğ¿(ğ¹(ğ’™),ğ‘¦)]=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ¿(ğ¹(ğ’™ğ‘–),ğ‘¦ğ‘–)â†’ min
ğ¹
The ability to achieve the smaller value of the empirical risk is
bounded by the complexity of the set Ffrom which the desired
functionğ¹âˆˆF is selected. Gradient Boosting (GB) increases the
expressiveness of the learned model by building (an ensemble )ğ¹ğµ
of sizeğµas a weighted sum of base functions {ğ‘“1,ğ‘“2,...,ğ‘“ğµ}âŠ‚F :
ğ¹ğµ(ğ’™)=ğµâˆ‘ï¸
ğ‘–=1ğ›¼ğ‘–ğ‘“ğ‘–(ğ’™) (1)
When the set of available base functions Fis closed under scalar
multiplication, multipliers ğ›¼ğ‘–are equal:âˆ€ğ‘– ğ›¼ğ‘–=ğ›¼, whereğ›¼is a
hyperparameter of the GB algorithm called learning rate. Having
the firstğ‘¡âˆ’1terms constructed, the learning algorithm aims to
select the next function ğ‘“ğ‘¡sequentially as a solution of:
Ë†L(S,ğ¹ğ‘¡âˆ’1+ğ‘“ğ‘¡)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1[ğ¿(ğ¹ğ‘¡âˆ’1(ğ’™ğ‘–)+ğ‘“ğ‘¡(ğ’™ğ‘–),ğ‘¦ğ‘–)]â†’ min
ğ‘“ğ‘¡
The approximate solution of the latter equation in GB is usually
constructed as follows. Algorithm calculates first and second order
derivatives of Ë†Lat the point ğ¹ğ‘¡âˆ’1w.r.t. predicted values Ë†ğ‘¦:
ğ‘”ğ‘¡
ğ‘–=ğœ•ğ¿(Ë†ğ‘¦ğ‘–,ğ‘¦ğ‘–)
ğœ•Ë†ğ‘¦ğ‘–Ë†ğ‘¦ğ‘–=ğ¹ğ‘¡âˆ’1(ğ’™ğ‘–);â„ğ‘¡
ğ‘–=ğœ•2ğ¿(Ë†ğ‘¦ğ‘–,ğ‘¦ğ‘–)
ğœ•Ë†ğ‘¦2
ğ‘–Ë†ğ‘¦ğ‘–=ğ¹ğ‘¡âˆ’1(ğ’™ğ‘–),
and trains the following least squares estimator to gradient step
in the functional space:
ğ‘“ğ‘¡=arg min
ğ‘“âˆˆFğ‘âˆ‘ï¸
ğ‘–=1â„ğ‘¡
ğ‘–(Â®ğ‘¥ğ‘–,ğ‘¦ğ‘–) 
ğ‘“(Â®ğ‘¥ğ‘–)âˆ’ 
âˆ’ğ‘”ğ‘¡
ğ‘–(Â®ğ‘¥ğ‘–,ğ‘¦ğ‘–)
â„ğ‘¡
ğ‘–(Â®ğ‘¥ğ‘–,ğ‘¦ğ‘–)!!2
,
see [8] for details.
3.2 Early stopping via cross-validation
Since quality estimation based on the train set used in the learning
process is biased in comparison to unseen data [ 35], it is conven-
tional to use an independent validation set to control the general-
ization ability of the algorithm. The whole dataset Sis split into
two disjoint setsSğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› andSğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ , where the first one is used for
learning and the latter one is used for measuring a quality estimate.
Such quality estimate is often highly dependent on the partic-
ular train-validation split and therefore can be noisy. A common
way to tackle this issue is to use cross-validation [39] method: split
the dataSintoğ‘˜disjoint subsets or folds(S1,S2,...,Sğ‘˜)of ap-
proximately equal size s.t. S=ğ‘˜Ãƒ
ğ‘–=1Sğ‘–and perform ğ‘˜rounds of
trainingâ€“evaluation cycle using Sâˆ’ğ‘–:=S\Sğ‘–as the training set
andSğ‘–as the validation data for each ğ‘–âˆˆ{1,2,...,ğ‘˜}. In this way,
we getğ‘˜differentğµ-sized models(
ğ¹ğ‘—
ğµ(ğ’™)=ğµâˆ‘ï¸
ğ‘–=1ğ›¼ğ‘“ğ‘—
ğ‘–(ğ’™)}ğ‘˜
ğ‘—=1)ğ‘˜
ğ‘—=1
learned byğ‘˜training sets{Sâˆ’ğ‘—}ğ‘˜
ğ‘—=1.
Atğ‘—-th crossâ€“validation step, we apply all the prefixes ğ¹ğ‘—
ğ‘–of
the modelğ¹ğ‘—
ğµto validation setSğ‘—and obtain quality estimators
ğ’ğ‘—=(ğ‘™(1)
ğ‘—,ğ‘™(2)
ğ‘—,...,ğ‘™(ğµ)
ğ‘—), where
ğ‘™(ğ‘)
ğ‘—=1
|Sğ‘—|âˆ‘ï¸
(ğ’™,ğ‘¦)âˆˆS ğ‘—ğ¿
ğ¹ğ‘—
ğ‘(ğ’™),ğ‘¦
.
The aggregated estimator ğ’=1
ğ‘˜Ãğ’ğ‘—is further used to define
Ë†ğµ:=arg min
1â‰¤ğ‘â‰¤ğµğ‘™(ğ‘).
The final model shrinked to the first Ë†ğµiterations provides an
estimator with the test quality close to
ğ¸=min
1â‰¤ğ‘â‰¤ğµE(ğ’™,ğ‘¦)âˆ¼ğ‘ƒ[ğ¿(ğ¹ğ‘(ğ’™),ğ‘¦)].
4 ADAPTIVE EARLY STOPPING
The standard early stopping method described in Section 3.2 presents
a challenge. An ideal goal could be the optimisation of the number
of iterations ğ‘(ğ’™)to each individual ğ’™, what would lead to expected
loss
E(ğ’™,ğ‘¦)âˆ¼ğ‘ƒ min
1â‰¤ğ‘(ğ’™)â‰¤ğµ[ğ¿(ğ¹ğ‘(ğ’™)(ğ’™),ğ‘¦)]
However, the expected loss theoretically achievable with the
standard early stopping approach can be greater because of the
apparent inequality:
ğ¸=min
1â‰¤ğ‘â‰¤ğµE(ğ’™,ğ‘¦)âˆ¼ğ‘ƒ[ğ¿(ğ¹ğ‘(ğ’™),ğ‘¦)]â‰¥
E(ğ’™,ğ‘¦)âˆ¼ğ‘ƒ min
1â‰¤ğ‘(ğ’™)â‰¤ğµ[ğ¿(ğ¹ğ‘(ğ’™)(ğ’™),ğ‘¦)],(2)
This mathematical fact indicates that the existing early stopping
scheme may not be the most effective approach. To address this, we
propose an adaptive early stopping method. This method involves
an adaptive selection of individual ensemble sizes for specific areas.
By doing so, we aim to achieve better quality by eliminating the
theoretical gap given by the aforementioned inequality 2. In the
subsequent sections, we will delve into potential approaches for
adaptive iteration count selection. We will discuss how to man-
age its impact and explore its advantages over the standard early
stopping method.
4.1 Main idea
Consider the scenario where the input space Dis partitioned into
ğ¶distinct regions, represented as (D1,D2,...,Dğ¶).
This partitioning is done such that all samples within a given
regionDğ‘–are in close proximity to each other, either following the
same latent distribution or sharing similar geometric properties.
Itâ€™s important to note that this partitioning is independent of the
division induced by cross-validation.
1168KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
Algorithm 1 Adaptive stopping procedure
Input:S=(ğ‘¿,ğ’š)
ğ‘“ğ‘œğ‘™ğ‘‘ğ‘ â†(S 1,S2,...,Sğ‘˜)â†ğ¶ğ‘£ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡(ğ‘˜,S)
ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘â†ğ¶ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡(ğ‘“ğ‘œğ‘™ğ‘‘ğ‘ )
ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›â†(D 1,D2,...,Dğ¶)â†ğºğ‘’ğ‘¡ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›(S)
ğ‘ğ‘’ğ‘ ğ‘¡ğ¼ğ‘¡ğ‘’ğ‘Ÿâ†ğ¸ğ‘£ğ‘ğ‘™ğµğ‘’ğ‘ ğ‘¡ğ¼ğ‘¡ğ‘’ğ‘Ÿ(ğ‘“ğ‘œğ‘™ğ‘‘ğ‘ ,ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘,ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› )
ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™â†ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘¿,ğ’š,ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›,ğ‘ğ‘’ğ‘ ğ‘¡ğ¼ğ‘¡ğ‘’ğ‘Ÿ )
returnğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™
We assume that(D1,D2,...,Dğ¶)forms a clustering, where data
points within the same cluster Dğ‘–exhibit similar behavior during
ensemble training. This implies that the optimal number of boosting
iterations Ë†ğµğ‘–estimated forDğ‘–could significantly differ from that
estimated forDğ‘—(ğ‘—â‰ ğ‘–). Drawing an analogy with inequality 2,
we infer that selecting the ensemble size based on partition D,
where the size is individually chosen for each cluster Dğ‘–, could
yield better quality compared to a single â€œuniversalâ€ common size.
This is represented by the following inequality:
EDğ‘–âˆ¼D min
1â‰¤ğ‘â‰¤ğµE[ğ¿(ğ¹ğ‘(ğ’™),ğ‘¦)|Dğ‘–]â‰¤ min
1â‰¤ğ‘â‰¤ğµEğ‘ƒ[ğ¿(ğ¹ğ‘(ğ’™),ğ‘¦)](3)
Settingğ¶=ğ‘›could potentially achieve the theoretical lower
bound of the left-hand side of Equation 3. However, the ensemble
size Ë†ğµğ‘–will be optimized based on the empirical estimation of the
loss, and an increase in ğ¶is accompanied by the growth of the vari-
ance of this estimation for each region Dğ‘–. Therefore, the number
of regions should be chosen judiciously, a topic we delve into later
in the text.
The upper-level training algorithm comprises four steps:
(1) Cross-validated training of ğ‘˜models;
(2)Distribution-based partitioning of the sample space into
(D1,D2,...,Dğ¶);
(3)Selection of the optimal number of iterations (Ë†ğµ1,Ë†ğµ2,...,Ë†ğµğ¶)
for each region obtained in step 2;
(4) Training of the final model on the entire training data.
A formal description of this process is presented in Algorithm 1.
4.2 Optimal point regression
One of the most straightforward ideas towards adaptive stopping in-
volves learning a regression model that predicts the optimal number
of terms in the trained ensemble for each example. In the context of
the general adaptive stopping framework (Algorithm 1), the func-
tionğºğ‘’ğ‘¡ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› should assign each data point to an individual
cluster, while ğ¸ğ‘£ğ‘ğ‘™ğµğ‘’ğ‘ ğ‘¡ğ¼ğ‘¡ğ‘’ğ‘Ÿ should train a mapping from data points
to the best number of iterations (derived from ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  ma-
trix). This regression model is further applied to test examples to
estimate the early stopping moments.
However, this seemingly simple idea does not yield satisfactory
results in practice. Training a separate GB regression model of the
same size as the primary model, followed by its application on the
test sample and individual pruning, results in a degradation of the
predictionâ€™s logloss by an average of 1.5% compared to the standard
non-adaptive scheme for selecting the number of trees.
Figure 1: Sample learning curves for Gradient Boosting on
the test set. The horizontal axis represents the number of
boosting iterations, while the vertical axis represents the
logloss value. Each colored line represents a learning curve for
a separate test instance, illustrating how the logloss changes
over iterations for individual data points.
Figure 2: Distribution of relative shift of best iteration predic-
tions. The horizontal axis represents the fraction values, and
the vertical axis represents the frequency of these values.
This is primarily due to the fact that the best iteration is known
to be a highly noisy target, given the complexity and partial random-
ness of the Gradient Boosting training process [ 19]. As visualized in
the learning curves (Figure 1) of test samples demonstrates that loss
histories appear chaotic and resemble realizations of some random
process. Consequently, each curve can inadvertently reach the min-
imum value at an arbitrarily late point. However, practical observa-
tions indicate that the general minimum point (following the trend
line) is bounded with some finite value. This makes the training of
an effective regressor a challenging task. Figure 2 demonstrates the
results of best iteration regression via a separate Gradient Boost-
ing model. Upon dividing the predictions of the trained regression
model by the actual best stopping moment, it is evident that at least
half of the samples are overestimated by approximately two times,
indicating that the selected stops are significantly suboptimal.
1169Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
4.3 Unsupervised partition
As previously mentioned at the beginning of this Section, the parti-
tioning of the data should reflect its internal structure to be sophis-
ticated enough to select an appropriate number of models. Under
the reasonable assumption that observations in close proximity
within the feature space also share similar properties, we can em-
ploy one of clustering algorithms such as KMeans [ 31], EM [ 14], or
agglomerative methods [ 37] to partition the data. This corresponds
to the function ğºğ‘’ğ‘¡ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› in Algorithm 1.
Preserving the initial geometry of the input space is crucial,
given that most modern implementations of Gradient Boosting
utilize Decision Trees [ 3] as base learners. Decision Trees construct
piecewise-constant approximations at each step, making it more
likely for closely located instances to fall into the same leaves
during training and inference, thereby leading to similar fits. The
unsupervised partitioning method provides control over the number
of partition regions and their sizes by setting the desired number
of clusters, and minimum sample count in each cluster.
However, when applied to real data, this method exhibits sev-
eral drawbacks Firstly, clustering does not perform well with data
containing non-numeric categorical features. Numeric encoding
of high-cardinality categorical features results in a sparse input
space, dramatically impacting the capacity of clusterization. Sec-
ondly, unsupervised partitioning does not take into account the
labels of the data points, which could provide valuable information
about the required number of boosting steps. Lastly, some advanced
clustering algorithms require high computational costs, posing a
bottleneck during model training.
To validate the above statements and assumptions, we utilized
large numeric datasets, namely Higgs and HEPMASS, and compared
KMeans clustering with the algorithm proposed in this paper. The
unsupervised approach proved to be a viable choice for the adaptive
pruning strategy, but it was strictly inferior (as detailed in the
experiments Section 6) to the supervised method described in the
subsequent subsection.
4.4 Direct Supervised Partition (DSP)
To circumvent the issues outlined in the previous section, the parti-
tion unit (function ğºğ‘’ğ‘¡ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› in Algorithm 1) should be scalable,
interpretable in terms of the subspaces it creates, and capable of
handling heterogeneous feature input. The Decision Tree model
emerges as a suitable candidate as it fulfills all these properties: its
training algorithm is parallelizable and not memory consuming
[36], the cluster manifolds it constructs are similar to the ones built
by base learners, and it supports efficient methods for handling
categorical features [35].
Our objective is to identify data regions with similar training
properties, and to achieve this, we can utilize the validation learn-
ing curves to partition the dataset. The core idea of our method
is to train a clustering model that segments the feature space into
regions using the learning curves as a target. However, a naive
multi-regression models based on the MSE loss may identify sim-
ilar learning curves, but the clusters chosen may not necessarily
exhibit diversified stopping moments. The true goal is to discover a
partition that will benefit the most from assigning individual stopsto each cluster. Formally speaking, the goal is to find a partition
Pâˆ—=(Dâˆ—
1,Dâˆ—
2,...,Dâˆ—
ğ¶)such that:
Pâˆ—=arg min
Pâˆ‘ï¸
Dğ‘˜âˆˆPmin
1â‰¤ğ‘â‰¤ğµâˆ‘ï¸
ğ’™ğ‘–âˆˆDğ‘˜ğ‘™ğ‘–,ğ‘,
whereğ‘™ğ‘–,ğ‘:=ğ¿(ğ¹ğ‘(ğ’™ğ‘–),ğ‘¦ğ‘–)and(ğ’™ğ‘–,ğ‘¦ğ‘–)âˆˆS .
To achieve this, we train a decision tree as clustering model. At
each node of the tree, we seek for a split (ğ¿,ğ‘…)which maximizes
the following score:
S(ğ¿,ğ‘…):=âˆ’Â©Â­
Â«min
1â‰¤ğ‘â‰¤ğµâˆ‘ï¸
ğ’™ğ‘–âˆˆğ¿ğ‘™ğ‘–,ğ‘+min
1â‰¤ğ‘â‰¤ğµâˆ‘ï¸
ğ’™ğ‘–âˆˆğ‘…ğ‘™ğ‘–,ğ‘ÂªÂ®
Â¬, (4)
Figure 3 provides a visual representation of how the partition-
ing operates for one of the benchmarks. The graphs validate the
hypothesis that there exist clusters where the optimal stopping
points can vary considerably. Interestingly, while some clusters
may continue to train (as depicted in the first three out of the five
plots in the figure), others may have already reached the point of
overfitting. Moreover, this characteristic is generalizable, meaning
it is inherent in the nature of the data. This is evidenced by the sim-
ilarity of the learning curves and breakpoints for both the training
and test sets. In other words, the patterns observed in the training
data are also reflected in the test data, indicating a consistent un-
derlying structure. This consistency is crucial as it suggests that
the modelâ€™s performance on the training data is a reliable indicator
of its performance on unseen data.
It is also worth noting that it is possible to optimize any quality
metric, not just the loss function. In the general case, it is sufficient
to store predictions from prefixes of the ensemble and use them
to calculate a particular metric function at each split. In particular,
storing predictions instead of learning curves can aid in directly
optimizing the ğ‘…ğ‘‚ğ¶âˆ’ğ´ğ‘ˆğ¶ score, an example of a non-summable
metric.
The complexity of calculating the score above is linear on the size
of the entire (unpruned) ensemble, resulting in a full tree growing
complexity of ğ‘‚(ğ‘›ğ‘šğµğ‘‘), which is comparable to the GB training
complexity. Additionally, the method requires storing learning his-
tories of size ğµfor each validation data point. Given that these
costs may be prohibitive in many real-world tasks (e.g., where large
ensembles are required), we propose to use values from selected
iterations, e.g., 1, 2, 4, 7, 11, ... (the step increases by one), which
reduces the complexity by a factor ofâˆš
ğµ. Here we assume that
as the number of iterations increases, predictions change less, so
points from earlier iterations carry more information about the
curves. Furthermore, the proposed scheme significantly reduces
time
ğ‘‚(ğ‘›ğ‘šâˆš
ğµğ‘‘)
and memory costs
ğ‘‚(ğ‘›âˆš
ğµ)
, making them
moderate compared to the GB training algorithm.
4.5 Indirect Supervised Partition (ISP)
Building on the concept from the previous section, letâ€™s consider an
extreme case of the method where the number of iterations used
to construct a clustering tree is set to 1. This results in a method
that relies solely on the characteristics of the dataset. In essence,
the partitioning process is reduced to training a single decision tree
1170KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
Figure 3: Clustering results obtained via a decision tree trained with the objective function from Equation 4. The horizontal
axis represents the number of boosting iterations, and the vertical axis represents the logloss value. Each line shows the logloss
reduction over iterations for a distinct cluster of the same dataset. Bold dots indicate the estimated (val) and actual (test)
stopping points.
on the initial training samples and targets. Each leaf of this tree is
then designated as a separate data cluster, forming the partition.
The tree learning process leverages both the geometry of the
feature space and the target distribution in the leaves to split the
data. This encourages the method to identify regions that are similar
in terms of feature representation and label, effectively utilizing all
available static information about the data.
This partition tree may be trained separately from the primary
boosting model and be the first booster in the ensemble. The latter
means that this step does not affect the training time. Moreover,
given that Gradient Boosting typically involves hundreds or even
thousands of trees, the time cost of using a separate partition model
is negligible.
Practical application of this method has shown that these clusters
are sufficiently effective in identifying regions with diversified
optimal stops, thereby enhancing the final quality of the model.
More details can be found in Section 6.
5 VALIDATION PROTOCOL
The methods proposed in this paper introduce two additional hy-
perparameters that can be adjusted: the count of clusters and the
minimum size of clusters. Both the DSP and ISP algorithms employ
a decision tree, and these hyperparameters are regulated by setting
limits on the maximum number of leaves and the minimum size of
each leaf in the decision tree.
We denoteDğ‘–,ğ‘—=Dğ‘–âˆ©Sğ‘—as the set of observations from the
ğ‘—-th fold that belong to the cluster Dğ‘–andğ‘›ğ‘–,ğ‘—as the number of
observations in this set. A straightforward approach to evaluation
involves applying a cross-validation model which is trained on the
sampleSâˆ’ğ‘—, to the validation set Sğ‘—for anyğ‘—. This yields quality
estimators ğ’ğ‘–,ğ‘—in the following form:
ğ‘™(ğ‘)
ğ‘–,ğ‘—=1
ğ‘›ğ‘–,ğ‘—âˆ‘ï¸
(ğ’™,ğ‘¦)âˆˆD ğ‘–,ğ‘—ğ¿
ğ¹ğ‘—
ğ‘(ğ’™),ğ‘¦
.
The resulting estimator ğ‘³ğ‘–for each cluster ğ‘–is derived from a
weighted sum of the corresponding cluster estimators across all
folds. This is mathematically represented as:ğ¿(ğ‘)
ğ‘–=Ãğ‘˜
ğ‘—=1ğ‘›ğ‘–,ğ‘—Â·ğ‘™(ğ‘)
ğ‘–,ğ‘—
Ãğ‘˜
ğ‘—=1ğ‘›ğ‘–,ğ‘—.
then the optimal number of iterations for the ğ‘–-th cluster is Ë†ğµğ‘–:=
arg min ğ‘³ğ‘–and its crossâ€“validation score equals to minğ‘³ğ‘–. The total
complexity of the described procedure is ğ‘‚(ğ¶(ğµ+ğ‘˜)+ğ‘›ğµ), which
is relatively small compared to the ensemble training complexity
ofğ‘‚(ğ‘›ğ‘šğ‘‘ğµ)(forğ‘šbinary features and trees of depth ğ‘‘) as cited
from Friedman [18].
Algorithm 2 Adaptive Early Stopping Selection
functionğ¸ğ‘£ğ‘ğ‘™ğµğ‘’ğ‘ ğ‘¡ğ¼ğ‘¡ğ‘’ğ‘Ÿ (ğ‘“ğ‘œğ‘™ğ‘‘ğ‘  ,ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘ ,ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› )
foreachDğ‘–inğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› do
ğ‘³ğ‘–â†Â®0{initialize a vector of ğµzeros}
ğ‘›ğ‘–â†0
foreachSğ‘—inğ‘“ğ‘œğ‘™ğ‘‘ğ‘  do
Dğ‘–,ğ‘—â†Dğ‘–âˆ©Sğ‘—
ğ‘›ğ‘–,ğ‘—â†|Dğ‘–,ğ‘—|
Î”ğ‘–,ğ‘—â†ğ¸ğ‘£ğ‘ğ‘™(ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘[Dğ‘–,ğ‘—])Â·ğ‘›ğ‘–,ğ‘—
ğ‘³ğ‘–â†ğ‘³ğ‘–+Î”ğ‘–,ğ‘—{elementwise vector sum}
ğ‘›ğ‘–â†ğ‘›ğ‘–+ğ‘›ğ‘–,ğ‘—
end for
ğ‘³ğ‘–â†ğ‘³ğ‘–/ğ‘›ğ‘–
ğ‘€ğ‘–â†arg min ğ‘³ğ‘–
end for
return{ğ‘€ğ‘–}
The quality assessment obtained in the way described above
tends to be biased and often provides an overly optimistic estimate.
This is particularly problematic when trying to determine an opti-
mal number of clusters, as the quality estimator tends to increase
monotonically with finer clustering.
To mitigate this issue in the DSP case, we introduce an additional
validation step to ensure that the partition will be beneficial. This
involves training the early stopping model and applying it to a
separate hold-out sample. If cross-validation is used during GB
1171Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
tuning, we have a learning curve for each training instance,allowing
us to use the entire sample again. Furthermore, DSP can employ
the minimal gain criterion - if the resulting score after a split does
not significantly improve upon the current score, the growth of
the tree should be halted. This ensures that the model does not
overfit the data and maintains a balance between complexity and
performance.
In the ISP case, when the first booster of the full model is utilized
and retraining the clustering tree is either not possible or deemed
too costly, we suggest using a cross-validation evaluation procedure
(Algorithm 3). This procedure is designed to prevent target leakage
and significantly reduce bias. For each fold Sğ‘, we calculate an
optimal stopping moment for cluster ğ‘–. This is achieved by averaging
evaluation metrics for all observations from cluster ğ‘–that are not
part of foldSğ‘. More formally, we compute ğ‘³ğ‘–,âˆ’ğ‘as follows
ğ¿(ğ‘)
ğ‘–,âˆ’ğ‘=Ã
ğ‘—â‰ ğ‘ğ‘›ğ‘–,ğ‘—Â·ğ‘™(ğ‘)
ğ‘–,ğ‘—Ã
ğ‘—â‰ ğ‘ğ‘›ğ‘–,ğ‘—.
This equation represents the weighted average loss of the ğ‘-th
iteration of the model trained on all folds excluding the ğ‘-th fold
and clusterğ‘–. We apply the function ğ¸ğ‘£ğ‘ğ‘™ğµğ‘’ğ‘ ğ‘¡ğ¼ğ‘¡ğ‘’ğ‘Ÿ to all folds except
theğ‘-th one, effectively ignoring Sğ‘fromğ‘“ğ‘œğ‘™ğ‘‘ğ‘  . After this step, we
obtain a set of optimal stopping moments ( Ë†ğµğ‘
1,...,Ë†ğµğ‘
ğ¶)estimated on
Sâˆ’ğ‘.
We then useSğ‘as a validation set to assess the quality of pre-
dicted optimal stopping moments ( Ë†ğµğ‘
1,...,Ë†ğµğ‘
ğ¶). By averaging the
results obtained over all folds Sğ‘, we derive a more accurate esti-
mation of the quality of clustering.
This estimation is used to select the number and size of clusters
and to estimate the potential profit of applying the adaptive stop-
ping procedure. All of this is achieved with a minor additional time
consumption relative to the training time of the ensemble model.
There is still some bias because fold ğ‘†ğ‘is used both to train
models applied toSâˆ’ğ‘and to estimate the performance of stopping
points. However, the desired property of not using the same set for
both tuning and evaluating Ë†ğµis satisfied and allows us to obtain
useful estimations. Objective of training
Algorithm 3 Evaluation Procedure
functionğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’ (ğ‘“ğ‘œğ‘™ğ‘‘ğ‘  ,ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘ ,ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› )
forSğ‘â†ğ‘“ğ‘œğ‘™ğ‘‘ğ‘  do
{ğ‘€ğ‘
ğ‘–}â†ğ¸ğ‘£ğ‘ğ‘™ğµğ‘’ğ‘ ğ‘¡ğ¼ğ‘¡ğ‘’ğ‘Ÿ(ğ‘“ğ‘œğ‘™ğ‘‘ğ‘ \Sğ‘,ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘,ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›)
ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  ğ‘â†ğ‘ğ‘£ğ‘ƒğ‘Ÿğ‘’ğ‘‘[Sğ‘]
forDğ‘–â†ğ‘do
Shrink(ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  ğ‘[Sğ‘âˆ©Dğ‘–],ğ‘€ğ‘
ğ‘–)
end for
ğ‘³ğ‘=ğ¸ğ‘£ğ‘ğ‘™(ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  ğ‘)
end for
returnğ‘€ğ‘’ğ‘ğ‘›({ğ‘³ğ‘})
6 EXPERIMENTS
In this section, we conduct numerical experiments to evaluate the
effectiveness of our proposed framework [ 22] and corroborate theassertions made in Section 4. Our analysis utilizes two widely-
used open-source Gradient Boosting libraries: LightGBM [ 28] and
CatBoost [4].
Following the methodology outlined in [ 35], which includes us-
ing the same hyperparameter set and tuning process, each model is
fine-tuned using a 5-fold cross-validation scheme over 50 iterations
of Hyperopt [ 2]. The specific parameters and grid used for tuning
can be found in Appendix. To optimize the quality of the pruned
ensemble during hyperparameter tuning, each Hyperopt iteration
is supplemented by a greedy search to identify the optimal number
of trees.
The models are tuned with a maximum of ğµ=5,000boosting
iterations. This number was chosen because strong boosting models
typically require hundreds or thousands of trees in the ensemble.
Itâ€™s important to note that, although the model was tuned to reach
an optimum at ğµiterations, the performance of the model may
continue to improve beyond this point. This is why we increase
the total number of trees to ğµâ€²=2ğµ=10,000for subsequent
experiments. This ensures full convergence of all models and al-
lows for the possibility of further improvement beyond the initially
identified optimum.
To compare the quality of the standard early stopping approach
and those proposed in this paper, we hold out 20% of samples from
each dataset as the test set. A 5-fold stratified cross-validation is
employed to ascertain the optimal stopping moment. We use the
standard pruning algorithm as a baseline and contrast it with the
methods proposed in Section 4.
For the DSP algorithm, we store "sparse" learning histories (as
described in Section 4.4) for each data point. We then perform 5-
fold cross-validation (involving 5 retrainings of the clustering tree,
not the entire ensemble) to fine-tune the depth of the clustering
tree and minimal leaf size. In the case of the ISP, we adhere to the
validation protocol outlined in Section 5.
We include all datasets from [ 35] and [ 23] as benchmarks in our
analysis to facilitate a comprehensive comparison of our method
with state-of-the-art Gradient Boosting implementations. These
datasets are all classification datasets, varying in size in terms of
the number of features and samples, and encompass a range of
properties and feature types. The full list of the datasets, employed
in this study, along with datasetsâ€™ properties, is enumerated in
Table 1.
Does the validation protocol proposed in Section 5 have
good generalization ability? To investigate this, we applied both
the naive validation control and the advanced evaluation procedure,
both described in Section 5, to every dataset. As illustrated in Fig-
ure 4 and Figure 5, the naive validation protocol shows a monotonic
decrease with the increase in the number of clusters. This is an
expected outcome due to the nature of the protocol, which does
not provide insights into the optimal cluster count or potential
improvements compared to the baseline.
In contrast, the advanced approach produces a quality estima-
tion that is highly correlated with the test quality. This correlation
is evident in the repeated quality patterns for both the test and
validation sets. Consequently, this correlation provides an opportu-
nity to make an informed decision regarding the optimal cluster
count and other parameters that influence clustering. This informed
1172KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
Table 1: 0-1 loss / logloss, Average value, mean relative error change w.r.t. baseline. Zero-percent improvements mean, that one
cluster is the best option selected by validation protocol.
LightGBM CatBo
ost
Dataset
(#samples/#features) Unprune
d Baseline DSP ISP Unprune
d Baseline DSP ISP
A
dult (49K/15) .1335
/ .2944 .1269
/ .2753.1253/.2726
âˆ’1.27%/
âˆ’0.99%.1265/.2742
âˆ’0.30%/
âˆ’0.39%.1276
/ .2755 .1266
/ .2725.1253/.2696
âˆ’1.05%/
âˆ’1.07%.1260/.2717
âˆ’0.48%/
âˆ’0.28%
Amazon
(33K/10) .0596
/ .1663 .0529
/ .1631.0525/.1628
âˆ’0.75%/
âˆ’0.21%.0527/.1629
âˆ’0.46%/
âˆ’0.12%.0445
/ .1404 .0448
/ .1395.0440/.1395
âˆ’1.68%/0%.0444/.1391
âˆ’0.87%/
âˆ’0.29%
Click
(400K/12) .1592
/ .4067 .1581
/ .3964.1578/.3964
âˆ’0.18%/0%.1581/.3962
0%/
âˆ’0.04%.1577
/ .3919 .1564
/ .3916.1563/.3914
âˆ’0.06%/
âˆ’0.04%.1564/.3914
+0.01%/
âˆ’0.04%
Default
(30K/23) .2011
/ .4694 .1888
/ .4518.1878/.4504
âˆ’0.51%/
âˆ’0.32%.1862/.4497
âˆ’1.4%/
âˆ’0.47%.1833
/ .4361 .1828
/ .4328.1821/.4324
âˆ’0.36%/
âˆ’0.1%.1798/.4323
âˆ’1.63%/
âˆ’0.12%
HEPMASS
(840K/25) .1337
/ .2843 .1270
/ .2791.1238/.2754
âˆ’2.55%/
âˆ’1.33%.1267/.2780
âˆ’0.25%/
âˆ’0.39%.1309
/ .2803 .1258
/ .2768.1245/.2753
âˆ’1.05%/
âˆ’0.55%.1256/.2764
âˆ’0.17%/
âˆ’0.16%
Higgs
(11KK/28) .2476
/ .5021 .2381
/ .4922.2369/.4897
âˆ’0.5%/
âˆ’0.51%.2375/.4906
âˆ’0.27%/
âˆ’0.33%.2385
/ .4864 .2364
/ .4810.2351/.4794
âˆ’0.57%/
âˆ’0.34%.2361/.4803
âˆ’0.14%/
âˆ’0.14%
KDD
Churn (50K/231) .0725
/ .2342 .0725
/ .2323.0725/.2323
0%/0%.0725/.2323
0%/0%.072
/ .2382 .0718
/ .2326.0715/.2326
âˆ’0.42%/0%.0718/.2326
0%/0%
KDD
Internet (10K/69) .0974
/ .2334 .0989
/ .2202.0980/.2167
âˆ’0.92%/
âˆ’1.58%.0969/.2190
âˆ’1.98%/
âˆ’0.53%.0994
/ .2199 .0984
/ .2167.0969/.2127
âˆ’1.55%/
âˆ’1.85%.0963/.2152
âˆ’2.17%/
âˆ’0.68%
KDD
Upselling (50K/231) .0495
/ .1694 .0495
/ .1669.0492/.1670
âˆ’0.67%/+0.08%.0495/.1669
âˆ’0.04%/0%.0487
/ .1677 .0489
/ .1670.0484/.1665
âˆ’1.01%/
âˆ’0.28%.0488/.1668
âˆ’0.10%/
âˆ’0.13%
Kick
(73K/36) .0987
/ .3073 .0991
/ .2956.0987/.2939
âˆ’0.44%/
âˆ’0.56%.0987/.2949
âˆ’0.37%/
âˆ’0.25%.0953
/ .2864 .0951
/ .2855.0949/.2849
âˆ’0.22%/
âˆ’0.22%.0948/.2850
âˆ’0.36%/
âˆ’0.16%
Marketing
(45K/16) .0941
/ .2268 .0931
/ .2044.0919/.2035
âˆ’1.25%/
âˆ’0.43%.0931/.2040
0%/
âˆ’0.20%.0913
/ .2032 .0911
/ .1938.0911/.1925
0%/
âˆ’0.68%.0895/.1922
âˆ’1.78%/
âˆ’0.80%
A
verage - - -0.82%
/ -0.53% -0.46%
/ -0.25% - - -0.72%
/ -0.47% -0.70%
/ -0.25%
Table 2: Time and memory measurements for the baseline (CV-based early stopping) and the additional costs associated with
our DSP algorithm. We do not report ISP measurements here, since the ISP algorithm introduces negligible time overhead
(<0.01%) and a moderate memory overhead (<5%) across all datasets
Dataset A
dult Amazon Click Default HEPMASS Higgs KDD
Churn KDD
Internet KDD
Upselling Kick Marketing
Time(
s)/Memory(MB) 125.7/606.7 200.1/614.5 665.8/968.4 123.7/602.9 732.9/1962.2 10537.3/8386.7 398.5/895.6 130.8/607.8 664.5/898.8 384.8/711.0 209.3/714.4
DSP
(delta in %) +2.1%/+15.7% +1.1%/+12.8% +0.8%/+35.3% +2.0%/+12.9% +1.8%/+49.6% +2.5%/+87.3% +2.2%/+6.8% +1.8%/+8.2% +1.4%/+7.2% +0.8%/+15.9% +1.2%/+11.7%
decision-making process enhances the generalization ability of the
validation protocol proposed in Section 5.
Does the proposed algorithm help to increase the quality
of boosting models? In this paragraph, we conduct an extensive
search for the optimal partition using two loss metrics (lower is
better): Logloss and 0-1 loss. For each cluster generated by DSP or
ISP algorithm, we identify the optimal iteration count and apply
the corresponding number of trees (boosters) to each test sample,
as outlined in Algorithm 1.
This process is repeated ten times for each dataset, each time with
a different CV split (random seed). We then calculate the average
loss over all experiment runs and the mean relative improvement
(loss decrease). The results of this comparison are presented in
Table 1.
Our findings indicate that the proposed techniques outperform
the classic early stopping approach in most settings. The improve-
ments are significant according to paired (ten pairs "baseline vs
ours" for each dataset) Wilcoxon signed-rank test, with a ğ‘âˆ’
ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’â‰ª0.001, except for the Click and KDD Churn datasets.
Interestingly, the relative magnitude of improvements is compa-
rable to that obtained by hyperparameter tuning (50 cross-validated
iterations of Hyperopt). This suggests that modern Gradient Boost-
ing implementations may not be fully leveraging the power of the
models, as they limit themselves to a shared stopping moment forall examples. Our results demonstrate that personalizing the selec-
tion of this parameter can lead to significant improvements in the
algorithmâ€™s performance.
How do the proposed methods impact computational re-
sources such as time and memory?
While predictive quality is a crucial criterion for evaluating ma-
chine learning models, computational resources such as time and
memory are also important considerations. In this context, it is
essential to demonstrate that our proposed methods not only im-
prove model performance but also do so without requiring excessive
computational resources.
In addition to evaluating the predictive performance of the pro-
posed methods, we also analyzed their computational efficiency
in terms of time and memory consumption. The results are sum-
marized in Table 2, which lists the absolute values in seconds and
megabytes for the baseline method, along with the relative increases
for the DSP method.
For the Direct Supervised Partition (DSP) method, the additional
memory consumption varied from 10% to 50% on average across
all datasets. The additional time required for the DSP method was
approximately 1-2%, indicating a moderate increase in computa-
tional overhead. These results suggest that, despite some additional
computational costs, the DSP method remains resource-efficient
and provides a feasible solution for practical applications.
1173Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 4: Marketing, validation
Figure 5: Kick, validation
In contrast, the Indirect Supervised Partition (ISP) method demon-
strated minimal impact on computational resources. The time con-
sumption for ISP was less than 0.01%, and the memory overhead
was less than 5% for all datasets. Due to these negligible increases,
the detailed results for ISP are not included in Table 2.
These results highlight that our adaptive early stopping methods
provide a balanced approach, offering improved model performance
without imposing significant computational burdens. This balance
between quality and resource efficiency ensures that our methods
can be effectively integrated into existing learning pipelines and
applied to a wide range of tasks.
Does it make sense to use unsupervised clustering instead
of the one proposed in the paper? As discussed in Section 4.3,
constructing a space partition without information about targets,
as well as using algorithms that do not consider the geometry of
learning surfaces, can have certain disadvantages. To empirically
evaluate these points, we conduct a series of experiments similar
to those described in the previous section. The only difference was
that we used KMeans [ 31] as the clustering algorithm instead of
the one proposed in our paper.
We selected the Higgs and HEPMASS datasets for these ex-
periments because they lack categorical features, which makes
them suitable for KMeans. Our results indicate that unsupervised
clustering can indeed be effectively applied to the adaptive stop-
ping problem. We observed mean improvements of 0.1%/0.08%and0.09%/0.1% r espectively, which are significantly better than those 
achieved with the standard approach. However, these improve-
ments are significantly less than those achieved with the method 
proposed in Section 4.5, as confirmed by a  paired Wilcoxon signed-
rank test.
7 CONCLUSION AND FUTURE WORK
In this paper, we identified a  previously unexplored issue related 
to ensemble pruning. We examined the potential challenges that 
the simultaneous stopping rule poses to modern boosting models 
and introduced a cluster-based framework for early stopping. This 
framework can be seamlessly integrated into any Gradient Boosting 
implementation (and potentially other ensemble methods) without 
compromising its quality or affecting its training/inference time.
We also proposed a straightforward and computationally effi-
cient evaluation protocol for our method. This protocol simplifies 
the process of determining whether adaptive stopping is effective 
for a particular dataset. Our experiments, conducted using well-
known boosting implementations, validate the assumptions and 
conclusions presented in this paper. They also highlight the signifi-
cant potential for practical applications and further research.
However, this work also uncovers several areas that warrant 
further investigation. Specifically, the noise introduced by data 
points that are well-trained in early iterations and the challenge 
of constructing optimal clusters are areas that require in-depth  
exploration. This ongoing research will continue to advance our 
understanding and application of ensemble methods in machine 
learning.
REFERENCES
[1]Ismail Babajide Mustapha and Faisal Saeed. 2016. Bioactive molecule prediction
using extreme gradient boosting. Molecules 21, 8 (2016), 983.
[2]James Bergstra, Dan Yamins, David D Cox, et al .2013. Hyperopt: A python
library for optimizing the hyperparameters of machine learning algorithms. In
Proceedings of the 12th Python in science conference, Vol. 13. Citeseer, 20.
[3]Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 2017.
Classification and regression trees. Routledge.
[4] CatBoost. 2017. CatBoost library: https://github.com/catboost/catboost .
[5]George DC Cavalcanti, Luiz S Oliveira, Thiago JM Moura, and Guilherme V
Carvalho. 2016. Combining diversity measures for ensemble pruning. Pattern
Recognition Letters 74 (2016), 38â€“45.
[6]Yuan-Chin Ivan Chang, Yufen Huang, and Yu-Pai Huang. 2010. Early stopping in
L2Boosting. Computational Statistics & Data Analysis 54, 10 (2010), 2203â€“2213.
[7]Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview.
InProceedings of the learning to rank challenge. PMLR, 1â€“24.
[8]Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785â€“794.
[9]Chen Cheng, Fen Xia, Tong Zhang, Irwin King, and Michael R Lyu. 2014. Gradient
boosting factorization machines. In Proceedings of the 8th ACM Conference on
Recommender systems. 265â€“272.
[10] Cliff Click, Michal Malohlava, Arno Candel, Hank Roark, and Viraj Parmar. 2016.
Gradient boosting machine with h2o. H2O. ai 11 (2016), 12.
[11] Corinna Cortes, Mehryar Mohri, and Dmitry Storcheus. 2019. Regularized Gradi-
ent Boosting. In Advances in Neural Information Processing Systems, H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.),
Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
465636eb4a7ff4b267f3b765d07a02da-Paper.pdf
[12] Rafael MO Cruz, Robert Sabourin, and George DC Cavalcanti. 2018. Dynamic
classifier selection: Recent advances and perspectives. Information Fusion 41
(2018), 195â€“216.
[13] R. M. Cruz, R. Sabourin, G. D. Cavalcanti, and T. I. Ren. 2015. META-DES: A
dynamic ensemble selection framework using meta-learning. Pattern Recognition
48 (2015), 1925â€“1935.
1174KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
[14] Arthur P Dempster, Nan M Laird, and Donald B Rubin. 1977. Maximum likelihood
from incomplete data via the EM algorithm. Journal of the Royal Statistical Society:
Series B (Methodological) 39, 1 (1977), 1â€“22.
[15] Lei Deng, Juan Pan, Xiaojie Xu, Wenyi Yang, Chuyao Liu, and Hui Liu. 2018.
PDRLGB: precise DNA-binding residue prediction using a light gradient boosting
machine. BMC bioinformatics 19, 19 (2018), 135â€“145.
[16] Wei Fan, Fang Chu, Haixun Wang, and Philip S Yu. 2002. Pruning and dynamic
scheduling of cost-sensitive ensembles. In AAAI/IAAI. 146â€“151.
[17] Yoav Freund and Robert E Schapire. 1997. A Decision-Theoretic Generalization
of On-Line Learning and an Application to Boosting. J. Comput. System Sci. 55, 1
(1997), 119â€“139. https://doi.org/10.1006/jcss.1997.1504
[18] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting
machine. Annals of statistics (2001), 1189â€“1232.
[19] Jerome H Friedman. 2002. Stochastic gradient boosting. Computational statistics
& data analysis 38, 4 (2002), 367â€“378.
[20] Jerome H Friedman and Bogdan E Popescu. 2008. Predictive learning via rule
ensembles. (2008).
[21] Daniel HernÃ¡ndez-Lobato, Gonzalo Martinez-Munoz, and Alberto SuÃ¡rez. 2008.
Statistical instance-based pruning in ensembles of independent classifiers. IEEE
Transactions on Pattern Analysis and Machine Intelligence 31, 2 (2008), 364â€“369.
[22] Bulat Ibragimov. 2024. DSP and ISP algorithms: https://github.com/ibr11/LTSA.
[23] Bulat Ibragimov and Gleb Gusev. 2019. Minimal variance sampling in stochastic
gradient boosting. In Proceedings of the 33rd International Conference on Neural
Information Processing Systems. 15087â€“15097.
[24] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting
decision tree. Advances in neural information processing systems 30 (2017), 3146â€“
3154.
[25] Erin LeDell and Sebastien Poirier. 2020. H2o automl: Scalable automatic machine
learning. In Proceedings of the AutoML Workshop at ICML, Vol. 2020.
[26] Nan Li, Yang Yu, and Zhi-Hua Zhou. 2012. Diversity regularized ensemble
pruning. In Joint European conference on machine learning and knowledge discovery
in databases. Springer, 330â€“345.
[27] Ping Li, Qiang Wu, and Christopher Burges. 2007. Mcrank: Learning to rank using
multiple classification and gradient boosting. Advances in neural information
processing systems 20 (2007), 897â€“904.
[28] LightGBM. 2017. LightGBM library: https://github.com/microsoft/LightGBM .
[29] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun.
2017. Model ensemble for click prediction in bing search ads. In Proceedings of
the 26th International Conference on World Wide Web Companion. 689â€“698.
[30] Brian Liu and Rahul Mazumder. 2023. Forestprune: Compact depth-pruned tree
ensembles. In International Conference on Artificial Intelligence and Statistics.
PMLR, 9417â€“9428.
[31] S. Lloyd. 1982. Least squares quantization in PCM. IEEE Transactions on Informa-
tion Theory 28, 2 (1982), 129â€“137. https://doi.org/10.1109/TIT.1982.1056489
[32] Dragos D Margineantu and Thomas G Dietterich. 1997. Pruning adaptive boosting.
InICML, Vol. 97. Citeseer, 211â€“218.
[33] Andreas Mayr, Benjamin Hofner, and Matthias Schmid. 2012. The importance of
knowing when to stop. Methods of Information in Medicine 51, 02 (2012), 178â€“186.
[34] Dayvid VR Oliveira, George DC Cavalcanti, and Robert Sabourin. 2017. Online
pruning of base classifiers for dynamic ensemble selection. Pattern Recognition
72 (2017), 44â€“58.
[35] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Doro-
gush, and Andrey Gulin. 2017. CatBoost: unbiased boosting with categorical
features. arXiv preprint arXiv:1706.09516 (2017).
[36] Toby Sharp. 2008. Implementing decision trees and forests on a GPU. In European
conference on computer vision. Springer, 595â€“608.
[37] Robin Sibson. 1973. SLINK: an optimally efficient algorithm for the single-link
cluster method. The computer journal 16, 1 (1973), 30â€“34.
[38] VÃ­ctor Soto, Sergio GarcÃ­a-Moratilla, Gonzalo MartÃ­nez-MuÃ±oz, Daniel
HernÃ¡ndez-Lobato, and Alberto SuÃ¡rez. 2014. A double pruning scheme for
boosting ensembles. IEEE transactions on cybernetics 44, 12 (2014), 2682â€“2695.
[39] Mervyn Stone. 1974. Cross-validatory choice and assessment of statistical predic-
tions. Journal of the royal statistical society: Series B (Methodological) 36, 2 (1974),
111â€“133.
[40] Samir Touzani, Jessica Granderson, and Samuel Fernandes. 2018. Gradient boost-
ing machine for modeling the energy consumption of commercial buildings.
Energy and Buildings 158 (2018), 1533â€“1543.
[41] Ilya Trofimov, Anna Kornetova, and Valery Topinskiy. 2012. Using boosted trees
for click-through rate prediction for sponsored search. In In Proceedings of the
Sixth International Workshop on Data Mining for Online Advertising and Internet
Economy. 1â€“6.
[42] Yuting Wei, Fanny Yang, and Martin J Wainwright. 2017. Early stopping
for kernel boosting algorithms: A general analysis with localized complex-
ities. In Advances in Neural Information Processing Systems, I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/
2017/file/a081cab429ff7a3b96e0a07319f1049e-Paper.pdf[43] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. 2007. On Early Stopping in
Gradient Descent Learning. Constructive Approximation 26 (08 2007), 289â€“315.
https://doi.org/10.1007/s00365-006-0663-2
[44] Yi Zhang, Samuel Burer, W Nick Street, Kristin P Bennett, and Emilio Parrado-
HernÃ¡ndez. 2006. Ensemble Pruning Via Semi-definite Programming. Journal of
machine learning research 7, 7 (2006).
[45] Yanru Zhang and Ali Haghani. 2015. A gradient boosting method to improve
travel time prediction. Transportation Research Part C: Emerging Technologies 58
(2015), 308â€“324.
[46] Zhi-Hua Zhou and Wei Tang. 2003. Selective ensemble of decision trees. In
International workshop on rough sets, fuzzy sets, data mining, and granular-soft
computing. Springer, 476â€“483.
A APPENDIX
A.1 Experimental setup
We set the single train/test split in the ratio of 4:1 and use train
data for training and hyperparameter search. Test data is used at
the evaluation step only. For the initial hyperparameter tuning
(baseline), we perform 50 iterations of Tree Parzen Estimator from
the Hyperopt library using LogLoss as a target metric (lower-better).
We consider the following list of hyperparameters to be tuned.
LightGBM:
â€¢"num_leaves" - max number of terminal nodes. Loguniform
grid from 1 to 105;
â€¢"learning_rate" - the weight of each tree in the ensemble.
Loguniform grid from 10âˆ’7to 1;
â€¢"min_data_in_leaf" - minimal number of data points in a leaf
(node is not considered for splitting). Loguniform grid from
1 to106;
â€¢"min_sum_hessian_in_leaf" - minimal value of sum of hes-
sians in a leaf (node is not considered for splitting). Loguni-
form grid from 0 to 105;
â€¢"lambda_l1" - regularizing term multiplier for predictions in
treesâ€™ leaves. Loguniform grid from 0 to 100;
â€¢"lambda_l2" - regularizing term multiplier for predictions in
treesâ€™ leaves. Loguniform grid from 0 to 100;
â€¢"bagging_fraction" - SGB sampling ratio. Uniform grid from
0.5 to 1;
â€¢"feature_fraction" - feature sampling ratio (random subspace).
Uniform grid from 0.5 to 1;
â€¢"n_estimators" - number of trees in the ensemble. Fixed con-
stant 5000. The best value is selected at the end by greedy
search.
CatBoost:
â€¢"depth" - max depth of each decision tree. Integer grid from
1 to 6;
â€¢"learning_rate" - the weight of each tree in the ensemble.
Loguniform grid from 10âˆ’5to 1;
â€¢"random_strength" - regularizing randomized term in the
split scoring function. Integer grid from 1 to 20;
â€¢"one_hot_max_size" - use one-hot encoding for categorical
features with number of unique values less than given pa-
rameter. Integer grid from 0 to 25;
â€¢"l2_leaf_reg" - regularizing term multiplier for predictions
in treesâ€™ leaves. Loguniform grid from 1 to 10;
â€¢"subsample" - SGB sampling ratio. Uniform grid from 0.5 to
1;
1175Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢"rsm" - feature sampling ratio (random subspace). Uniform
grid from 0.5 to 1;
â€¢"iterations" - number of trees in the ensemble. Fixed constant
5000. The best value is selected at the end by greedy search.
A.2 Clustering results
Here we report the results of the clustering algorithm DSP for the
train set (used to predict optimal stops in each cluster) and the test
set.
Figure 6: Adult dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 7: Amazon dataset. Clustering obtained via DSP, vali-
dation and test results. The number after "test" is test samples
count per cluster.)
Figure 8: Click dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 9: Default dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 10: KDD Churn dataset. Clustering obtained via DSP,
validation and test results. The number after "test" is test
samples count per cluster.)
Figure 11: KDD Internet dataset. Clustering obtained via DSP,
validation and test results. The number after "test" is test
samples count per cluster.)
Figure 12: KDD Upselling dataset. Clustering obtained via
DSP, validation and test results. The number after "test" is
test samples count per cluster.)
Figure 13: Kick dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 14: Marketing dataset. Clustering obtained via DSP,
validation and test results. The number after "test" is test
samples count per cluster.)
1176