Fast Computation of Kemenyâ€™s Constant for Directed Graphs
Haisong Xia
Fudan University
Shanghai, China
hsxia22@m.fudan.edu.cnZhongzhi Zhangâˆ—
Fudan University
Shanghai, China
zhangzz@fudan.edu.cn
Abstract
Kemenyâ€™s constant for random walks on a graph is defined as the
mean hitting time from one node to another selected randomly
according to the stationary distribution. It has found numerous
applications and attracted considerable research interest. However,
exact computation of Kemenyâ€™s constant requires matrix inversion,
which scales poorly for large networks with millions of nodes. Exist-
ing approximation algorithms either leverage properties exclusive
to undirected graphs or involve inefficient simulation, leaving room
for further optimization. To address these limitations for directed
graphs, we propose two novel approximation algorithms for es-
timating Kemenyâ€™s constant on directed graphs with theoretical
error guarantees. Extensive numerical experiments on real-world
networks validate the superiority of our algorithms over baseline
methods in terms of efficiency and accuracy.
CCS Concepts
â€¢Theory of computation â†’Graph algorithms analysis; Ran-
dom walks and Markov chains.
Keywords
Random walk; approximation algorithm; hitting time; Kemenyâ€™s
constant; spectral graph theory.
ACM Reference Format:
Haisong Xia and Zhongzhi Zhang. 2024. Fast Computation of Kemenyâ€™s
Constant for Directed Graphs. In Proceedings of Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24).
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671859
1 Introduction
Random walks on complex networks have emerged as a power-
ful analytical tool with broad applications including recommenda-
tion systems [ 31], representation learning [ 38], privacy amplifica-
tion [ 28], and so on. For a random walk on a graph, a fundamental
quantity is the hitting time ğ»ğ‘– ğ‘—, which is defined as the expected
number of steps for a walker starting from node ğ‘–to visit node ğ‘—for
the first time. As a key quantity, hitting times have been widely uti-
lized across domains to address problems in complex networks, such
as assessing transmission costs in communication networks [ 14],
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671859developing clustering algorithms [ 1,10], and identifying significant
nodes [34].
Stemming from the hitting time, many important quantities for
random walks can be formulated, such as Kemenyâ€™s constant. For
random walks on a graph, Kemenyâ€™s constant is defined as the
expected hitting time from an arbitrary source node to the target
selected randomly according to the stationary distribution of the
random walk. Kemenyâ€™s constant has found various applications in
diverse areas. First, it is one of the widely used connectivity [ 6] or
criticality [ 23] measures for a graph. Second, based on Kemenyâ€™s
constant, an edge centrality [ 3,26] was defined to identify important
edges. Finally, Kemenyâ€™s constant was applied to characterize the
performance of consensus protocols with noise [19].
Despite the utility of Kemenyâ€™s constant across various applica-
tions, directly computing it on large real-world networks remains
prohibitively expensive. As discussed in Section 2.3, calculating
Kemenyâ€™s constant involves matrix inversion, whose complexity
isğ‘‚(ğ‘›3)for anğ‘›-node graph. This cubic scaling renders exact
computation infeasible for networks with millions of nodes.
In order to reduce computational time for Kemenyâ€™s constant,
some approximation algorithms have been developed to estimate
this graph invariant. Xu et al. [37] proposed ApproxKemeny, which
is based on Hutchinsonâ€™s method and the nearly linear-time Lapla-
cian solver [ 22]. However, results in Section 5 indicates that Approx-
Kemeny requires much more memory space than other methods, re-
ducing its scalability for large networks. Very recently, Li et al. [27]
provided DynamicMC, which is based on simulating truncated ran-
dom walks. While its GPU implementation achieves state-of-the-art
performance, DynamicMC still has many opportunities for further
optimization.
On the other hand, most of existing methods for estimating
Kemenyâ€™s constant are restricted to undirected networks, includ-
ingApproxKemeny andDynamicMC. For example, the Laplacian
solver [ 22] leverages some specific properties of undirected graphs,
thus ApproxKemeny fails to support digraphs. Although Dynam-
icMC can handle digraphs, its theoretical guarantees are not readily
extended to directed graphs from the perspective in [ 27]. Nonethe-
less, many real-world networks are inherently directed, such as
citation networks, the World Wide Web, and online social networks.
The lack of an efficient approximation algorithm for estimating Ke-
menyâ€™s constant on directed graphs limits further applications on
these important networks.
Motivated by DynamicMC, we provide an approximation algo-
rithm ImprovedMC for estimating Kemenyâ€™s constant of digraphs
with error guarantee. Apart from simulating truncated random
*Corresponding author. Both authors are with Shanghai Key Laboratory of Intelligent
Information Processing, School of Computer Science, Fudan University, Shanghai,
200433, China. Zhongzhi Zhang is also with Institute of Intelligent Complex Systems,
Fudan University, Shanghai, 200433, China.
3472
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
walks, ImprovedMC also incorporates several optimization tech-
niques. First, ImprovedMC adaptively determines the amount of
simulation initialized from each node, reducing unnecessary sim-
ulation without sacrificing theoretical accuracy. Additionally, Im-
provedMC restricts simulation to a subset of nodes in the network.
By sampling from selected starting nodes, ImprovedMC achieves
sublinear time complexity while still preserving theoretical accu-
racy. Extensive experiments reveal that compared with the state-
of-the-art method DynamicMC, ImprovedMC attains up to 800Ã—
speedup, while achieving comparable accuracy. To further improve
the accuracy, we derive an alternative formula that connects Ke-
menyâ€™s constant with the inverse of a submatrix associated with
the transition matrix. This motivates the development of a new
Monte Carlo algorithm TreeMC based on directed tree sampling.
We experimentally demonstrate the superiority of TreeMC over
the state-of-the-art method in terms of both efficiency and accuracy.
The key contributions of our work are summarized as follows.
â€¢First, we develop an improved Monte Carlo algorithm Im-
provedMC to estimate Kemenyâ€™s constant of digraphs by
simulating truncated random walks. ImprovedMC achieves
sublinear time complexity while still ensuring provable ac-
curacy guarantees.
â€¢Based on a derived alternative formula, we propose another
Monte Carlo algorithm TreeMC that approximates Kemenyâ€™s
constant of digraphs by sampling directed rooted spanning
trees, which is considerably accurate.
â€¢We conduct extensive experiments on real-world networks.
The results indicate that both of our proposed algorithms
outperform the baseline approaches by orders of magnitude
speed-up, while still retaining comparable accuracy.
2 Preliminaries
2.1 Notations
LetRdenote the set of real numbers. We use regular lowercase
letters likeğ‘,ğ‘,ğ‘ for scalars within R. Bold lowercase letters, such as
ğ’‚,ğ’ƒ,ğ’„, represent vectors, while bold uppercase letters, like ğ‘¨,ğ‘©,ğ‘ª,
denote matrices. Specific elements are accessed by using subscripts:
ğ‘ğ‘–for theğ‘–thelement of ğ’‚andğ‘¨ğ‘–,ğ‘—for the entry at position (ğ‘–,ğ‘—).
Subvectors and submatrices are similarly indicated with subscript
numerals. For example, ğ’‚âˆ’ğ‘–denotes the subvector of ğ’‚obtained by
excluding its ğ‘–thelement, while ğ‘¨âˆ’ğ‘–represents the submatrix of
ğ‘¨constructed by removing its ğ‘–throw andğ‘–thcolumn. Crucially,
subscripts take precedence over superscripts in this notation. Conse-
quently, ğ‘¨âˆ’1
âˆ’ğ‘–represents the inverse of ğ‘¨âˆ’ğ‘–, rather than a submatrix
ofğ‘¨âˆ’1. We use 1to denote a vector of specific dimensions with
all elements being 1. Table 1 lists the frequently used notations
throughout this paper.
2.2 Graph and Random Walk
Letğº=(ğ‘‰,ğ¸)be a digraph, where ğ‘‰is the set of nodes, and ğ¸is
the set of edges. The digraph ğºhas a total of ğ‘›=|ğ‘‰|nodes and
ğ‘š=|ğ¸|edges. Throughout this paper, all the digraphs mentioned
are assumed to be strongly connected without explicit qualification.
The adjacency matrix ğ‘¨of graphğºmathematically encodes its
topological properties. Here, the entry ğ‘¨ğ‘–,ğ‘—represents the adjacencyTable 1: Frequently used notations.
Notation Description
ğº=(ğ‘‰,ğ¸)A digraph with node set ğ‘‰and edge set ğ¸.
ğ‘›,ğ‘š The number of nodes and edges in ğº.
ğœ‹ğ‘–The stationary distribution of node ğ‘–.
ğ‘·The transition matrix of random walks on ğº.
ğœ†ğ‘–Theğ‘–thlargest eigenvalue of ğ‘·sorted by modulus.
ğœ†Denoted as|ğœ†2|.
Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘— The returning times to node ğ‘–of theğ‘—thğ‘™-truncated
random walk that starts from ğ‘–.
Ë†ğ‘¡ğ‘–,ğ‘— The returning times to node ğ‘–of theğ‘—thabsorbing
random walk that starts from ğ‘–.
Â¯ğ‘¡(ğ‘™)
ğ‘–,Â¯ğ‘¡ğ‘–The empirical mean of Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘—andË†ğ‘¡ğ‘–,ğ‘—.
relation between nodes ğ‘–andğ‘—.ğ‘¨ğ‘–,ğ‘—=1if there exists a directed
edge pointing from ğ‘–toğ‘—. Conversely, the absence of such an edge
is indicated by ğ‘¨ğ‘–,ğ‘—=0. In a digraph ğº, the out-degree ğ‘‘ğ‘–of nodeğ‘–
is defined as the number of its out-neighbours. If we denote the di-
agonal out-degree matrix of digraph ğºasğ‘«=diag(ğ‘‘1,ğ‘‘2,...,ğ‘‘ ğ‘›),
the Laplacian matrix of ğºis defined as ğ‘³=ğ‘«âˆ’ğ‘¨.
For a digraph ğºwithğ‘›nodes, a random walk on ğºis defined
through its transition matrix ğ‘·âˆˆRğ‘›Ã—ğ‘›. At each step, if the walker
is at nodeğ‘–, it moves to an out-neighbour ğ‘—with equal probabil-
ityğ‘·ğ‘–,ğ‘—. It follows readily that ğ‘·=ğ‘«âˆ’1ğ‘¨. Assuming ğ‘·is finite,
aperiodic, and irreducible, the random walk has an unique station-
ary distribution ğ…=(ğœ‹1,ğœ‹2,Â·Â·Â·,ğœ‹ğ‘›)âŠ¤, satisfying ğ…âŠ¤1=1and
ğ…âŠ¤ğ‘·=ğ‘·. Clearly, ğ…is the left 1-eigenvector of ğ‘·. Letğœ†1,ğœ†2,...,ğœ† ğ‘›
be the eigenvalues of ğ‘·, where 1=|ğœ†1|>|ğœ†2|â‰¥Â·Â·Â·â‰¥|ğœ†ğ‘›|. The
second largest eigenvalue of ğ‘·is crucial to our algorithms, whose
modulus is denoted as ğœ†.
For a random walk on digraph ğº, numerous associated quantities
can be expressed in terms of the fundamental matrix ğ‘­[30]. For a
random walk with transition matrix ğ‘·, the fundamental matrix is
defined as the group inverse of ğ‘°âˆ’ğ‘·:
ğ‘­=(ğ‘°âˆ’ğ‘·)#= ğ‘°âˆ’ğ‘·+1ğ…âŠ¤âˆ’1âˆ’1ğ…âŠ¤.
As the generalized inverse, ğ‘­satisfies(ğ‘°âˆ’ğ‘·)ğ‘­(ğ‘°âˆ’ğ‘·)=ğ‘°âˆ’ğ‘·.
Additionally, we can easily prove that ğ‘­andğ‘°âˆ’ğ‘·share the same
left null space and right null space. Concretely, ğ‘­1=(ğ‘°âˆ’ğ‘·)1=0
andğ…âŠ¤ğ‘­=ğ…âŠ¤(ğ‘°âˆ’ğ‘·)=0âŠ¤.
2.3 Hitting Time and Kemenyâ€™s Constant
A key concept in random walks is hitting time [ 13,29]. The hit-
ting timeğ»ğ‘– ğ‘—is defined as the expected time for a random walker
originating at node ğ‘–to arrive at node ğ‘—for the first time. Several
important quantities can be derived from the hitting time, here we
only consider the absorbing random-walk centrality and Kemenyâ€™s
constant.
For a node ğ‘ in theğ‘›-node digraph ğº=(ğ‘‰,ğ¸), its absorbing
random-walk centrality ğ»ğ‘ is defined as ğ»ğ‘ =Ãğ‘›
ğ‘–=1ğœ‹ğ‘–ğ»ğ‘–ğ‘ . Lower
values ofğ»ğ‘ indicate higher importance for node ğ‘ , which has
been analyzed extensively [ 7,8,32]. For brevity, we refer to ğ»ğ‘ =Ãğ‘›
ğ‘–=1ğœ‹ğ‘–ğ»ğ‘–ğ‘ aswalk centrality henceforth. For an ğ‘›-node digraph
ğº=(ğ‘‰,ğ¸), its Kemenyâ€™s constant ğ¾is defined as the expected steps
3473Fast Computation of Kemenyâ€™s Constant for Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
for a walker starting from node ğ‘¢to nodeğ‘–selected with probability
ğœ‹ğ‘–. Formally,ğ¾=Ãğ‘›
ğ‘–=1ğœ‹ğ‘–ğ»ğ‘¢ğ‘–. The invariance of Kemenyâ€™s constant
ğ¾stems from the fact that its value remains unchanged regardless
of the chosen starting node ğ‘¢.
As discussed in Section 2.2, many quantities associated with
random walks are determined by the fundamental matrix ğ‘­. For
example, the hitting time ğ»ğ‘– ğ‘—satisfiesğ»ğ‘– ğ‘—=ğœ‹âˆ’1
ğ‘— ğ‘­ğ‘—,ğ‘—âˆ’ğ‘­ğ‘–,ğ‘—[30].
Therefore, the walk centrality can be expressed as
ğ»ğ‘ =ğ‘›âˆ‘ï¸
ğ‘–=1ğœ‹ğ‘–ğ»ğ‘–ğ‘ =ğ‘›âˆ‘ï¸
ğ‘–=1ğœ‹ğ‘–
ğœ‹ğ‘  ğ‘­ğ‘ ,ğ‘ âˆ’ğ‘­ğ‘–,ğ‘ =ğ‘­ğ‘ ,ğ‘ 
ğœ‹ğ‘ , (1)
while Kemenyâ€™s constant can be represented as
ğ¾=ğ‘›âˆ‘ï¸
ğ‘–=1ğœ‹ğ‘–ğ»ğ‘¢ğ‘–=ğ‘›âˆ‘ï¸
ğ‘–=1 ğ‘­ğ‘–,ğ‘–âˆ’ğ‘­ğ‘¢,ğ‘–=Tr(ğ‘­). (2)
The Kemeny constant can be exactly computed by using (2).
However, this formula requires all the diagonal elements of a group
inverse. Since the complexity of matrix inversion is ğ‘‚(ğ‘›3), direct
computation of Kemenyâ€™s constant is impractical for large-scale
networks with millions of nodes.
2.4 Existing Methods
2.4.1 Method based on Laplacian solver. For an undirected graph,
its Kemenyâ€™s constant is equal to the trace of Lâ€ , where Ldenotes
the normalized Laplacian matrix. Using Hutchinsonâ€™s method [ 18],
ApproxKemeny by Xu et al. [37] reformulates estimating Tr Lâ€ 
as approximating the quadratic forms of Lâ€ , which is connected
to solving linear equations associated with the Laplacian matrix.
Leveraging a nearly linear-time Laplacian solver [ 22],Approx-
Kemeny attains nearly linear-time complexity in terms of edge
number. However, as shown in Section 5, the high memory usage
of Laplacian solver makes this algorithm impractical for large-scale
networks. Additionally, the Laplacian solver inherently leverages
specific properties of undirected graphs, precluding its application
toApproxKemeny for digraphs.
2.4.2 Method based on truncated random walks. To estimate Ke-
menyâ€™s constant, DynamicMC [27] simulates truncated random
walks starting from each node in the network, and then sums up
the probabilities of returning to the source. DynamicMC also in-
corporates a heuristic strategy, where the simulation terminates
once the change of the sum falls below a given threshold. Under the
parallel GPU implementation, DynamicMC achieves exceptional
performance surpassing previous methods. However, the excep-
tional performance largely stems from GPU implementation, which
is not competitive under fair comparison in Section 5. Additionally,
the performance limitations of DynamicMC are two-fold. First, the
inclusion of this heuristic strategy lacks a theoretical guarantee for
maintaining the accuracy. Second, DynamicMC relies on theoret-
ical analysis for undirected graphs, whose Kemenyâ€™s constant is
expressed as the infinite sum over powers of ğœ†ğ‘–. This expression
cannot be extended to digraphs, since the matrix ğ‘·for a digraph
loses diagonalizability and ğœ†ğ‘–becomes complex-valued. Hence, Dy-
namicMC cannot be directly extended to digraphs. In contrast,
we address this issue by reformulating Kemenyâ€™s constant as thetrace of fundamental matrix, which holds for both directed and
undirected graphs.
3 Theoretical Results
3.1 Approximation for Fundamental Matrix by
Truncated Sum
As shown in (2), the Kemeny constant is intimately related to the
diagonal elements of the fundamental matrix ğ‘­. In this subsection,
we first demonstrate that the trace of ğ‘­can be approximated by
anğ‘™-truncated sum ğ‘­(ğ‘™)with an additive error bound. Furthermore,
we prove that the diagonal elements of ğ‘­can also be approximated
byğ‘­(ğ‘™)with arbitrary accuracy.
Lemma 3.1. Letğº=(ğ‘‰,ğ¸)be a digraph with transition matrix
ğ‘·and stationary distribution ğ…. The fundamental matrix ğ‘­can be
expressed as
ğ‘­=(ğ‘°âˆ’ğ‘·)#=âˆâˆ‘ï¸
ğ‘˜=0 ğ‘·ğ‘˜âˆ’1ğ…âŠ¤.
Proof. According to the Perron-Frobenius theorem [ 9], the 1-
eigenvalue is simple. Recall that the left and right 1-eigenvector
ofğ‘·is, respectively, ğ…âŠ¤and1. Therefore, the spectral radius of
ğ‘·âˆ’1ğ…âŠ¤is lower than 1, and ğ‘­can be represented as
ğ‘­=(ğ‘°âˆ’ğ‘·)#= ğ‘°âˆ’ğ‘·+1ğ…âŠ¤âˆ’1âˆ’1ğ…âŠ¤
=
ğ‘°âˆ’ ğ‘·âˆ’1ğ…âŠ¤âˆ’1âˆ’1ğ…âŠ¤
=âˆ’1ğ…âŠ¤+âˆâˆ‘ï¸
ğ‘˜=0 ğ‘·âˆ’1ğ…âŠ¤ğ‘˜=âˆâˆ‘ï¸
ğ‘˜=0 ğ‘·ğ‘˜âˆ’1ğ…âŠ¤.
where the last equality can be easily obtained through mathematical
induction. â–¡
Lemma 3.1 indicates that ğ‘­can be represented by an infinite
sum. Therefore, we attempt to approximate ğ‘­by anğ‘™-truncated
sum, which is defined as ğ‘­(ğ‘™)=Ãğ‘™
ğ‘˜=0 ğ‘·ğ‘˜âˆ’1ğ…âŠ¤. We begin by
approximating Tr(ğ‘­)byTr ğ‘­(ğ‘™)with a theoretical error bound.
Lemma 3.2. Letğº=(ğ‘‰,ğ¸)be anğ‘›-node digraph with transition
matrix ğ‘·. For anyğœ–>0, ifğ‘™is selected satisfying ğ‘™â‰¥log(ğ‘›âˆ’1(ğœ–âˆ’ğœ–ğœ†))
log(ğœ†),
then we haveTr(ğ‘­)âˆ’Tr ğ‘­(ğ‘™)â‰¤ğœ–.
Proof.
Tr(ğ‘­)âˆ’Tr ğ‘­(ğ‘™)=âˆâˆ‘ï¸
ğ‘˜=ğ‘™+1
Tr ğ‘·ğ‘˜âˆ’1â‰¤âˆâˆ‘ï¸
ğ‘˜=ğ‘™+1ğ‘›âˆ‘ï¸
ğ‘–=2|ğœ†ğ‘–|ğ‘˜
â‰¤ğ‘›âˆâˆ‘ï¸
ğ‘˜=ğ‘™+1ğœ†ğ‘˜=ğ‘›ğœ†ğ‘™+1
1âˆ’ğœ†â‰¤ğœ–.
This finishes the proof. â–¡
After giving a theoretical bound of Tr ğ‘­(ğ‘™), we next approximate
theğ‘–thdiagonal element of ğ‘­with arbitrary accuracy. This poses a
greater challenge, as ğ‘·is not diagonalizable for digraphs. In order
to bound the error introduced by the truncated length ğ‘™, we need
the following lemma.
3474KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
Lemma 3.3. [2] For a digraph ğº=(ğ‘‰,ğ¸)with transition matrix
ğ‘·and stationary distribution ğ…, suppose there exists a probability
measureğœ‡, a real number ğ›¿>0and timeğ‘¡such that ğ‘·ğ‘¡
ğ‘–,ğ‘—â‰¥ğ›¿ğœ‡ğ‘—holds
for any nodes ğ‘–,ğ‘—âˆˆğ‘‰. Then for any node ğ‘–âˆˆğ‘‰and integerğ‘˜â‰¥0, we
haveğ‘·ğ‘˜
ğ‘–,ğ‘–âˆ’ğœ‹ğ‘–â‰¤(1âˆ’ğ›¿)âŒŠğ‘˜/ğ‘¡âŒ‹.
Subsequently, we introduce a theoretical bound of ğ‘­(ğ‘™)
ğ‘–,ğ‘–:
Lemma 3.4. Given anğ‘›-node digraph ğº=(ğ‘‰,ğ¸), letğ‘‘maxbe the
maximum out-degree, and let ğœbe the diameter of ğº, which is the
longest distance between nodes. For any ğœ–>0, ifğ‘™is selected satisfying
ğ‘™â‰¥ğœlog(ğ‘›ğœ–ğœâˆ’1ğ‘‘âˆ’ğœ
max)
log(1âˆ’ğ‘›ğ‘‘âˆ’ğœmax)+ğœâˆ’1, then we haveğ‘­ğ‘–,ğ‘–âˆ’ğ‘­(ğ‘™)
ğ‘–,ğ‘–â‰¤ğœ–.
3.2 Alternative Formula for Kemenyâ€™s Constant
In this subsection, we introduce an alternative formula that asso-
ciates Kemenyâ€™s constant of digraphs with a submatrix of ğ‘°âˆ’ğ‘·. The
enhanced diagonal dominance of this submatrix facilitates more
accurate approximation.
Theorem 3.5. Letğº=(ğ‘‰,ğ¸)be anğ‘›-node digraph with transition
matrix ğ‘·, stationary distribution ğ…and the fundamental matrix ğ‘­.
For any node ğ‘ âˆˆğ‘‰, the Kemeny constant of ğºcan be represented as
ğ¾=Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1âˆ’ğ»ğ‘ . (3)
Proof. As shown in Section 2.2, the left and right null vectors
ofğ‘­may be inequivalent. This prompts us to use the diagonally
scaled fundamental matrix Ëœğ‘­.Ëœğ‘­is defined as Ëœğ‘­=ğš·1/2ğ‘­ğš·âˆ’1/2, where
ğš·=diag(ğœ‹1,ğœ‹2,...,ğœ‹ ğ‘›). Similarly, we define the diagonally scaled
Laplacian as Ëœğ‘³=ğš·1/2(ğ‘°âˆ’ğ‘·)ğš·âˆ’1/2. It is easy to verify that Ëœğ‘­and
Ëœğ‘³share the same left and right null vector ğ…1/2.
Subsequently, we attempt to establish the connection between
Ëœğ‘­âˆ’ğ‘ and Ëœğ‘³âˆ’1âˆ’ğ‘ . After properly adjusting the node labels, Ëœğ‘­and Ëœğ‘³can
be rewritten in block forms as
Ëœğ‘­=Ëœğ‘­âˆ’ğ‘ Ëœğ‘­ğ‘‡,ğ‘ 
Ëœğ‘­ğ‘ ,ğ‘‡Ëœğ‘­ğ‘ ,ğ‘ 
,Ëœğ‘³=Ëœğ‘³âˆ’ğ‘ Ëœğ‘³ğ‘‡,ğ‘ 
Ëœğ‘³ğ‘ ,ğ‘‡Ëœğ‘³ğ‘ ,ğ‘ 
,
whereğ‘‡=ğ‘‰\{ğ‘ }. IfËœğ…is denoted as ğœ‹âˆ’1/2
ğ‘ ğ…1/2
âˆ’ğ‘ âˆˆRğ‘›âˆ’1, then we
prove that ğ‘¿=(ğ‘°âˆ’Ëœğ…)Ëœğ‘­ğ‘°
âˆ’Ëœğ…âŠ¤
equals to Ëœğ‘³âˆ’1âˆ’ğ‘ :
Ëœğ‘³âˆ’ğ‘ ğ‘¿Ëœğ‘³âˆ’ğ‘ =Ëœğ‘³âˆ’ğ‘ (ğ‘°âˆ’Ëœğ…)Ëœğ‘­ğ‘°
âˆ’Ëœğ…âŠ¤
Ëœğ‘³âˆ’ğ‘ 
=
Ëœğ‘³âˆ’ğ‘ Ëœğ‘³ğ‘‡,ğ‘ 
Ëœğ‘­Ëœğ‘³âˆ’ğ‘ 
Ëœğ‘³ğ‘ ,ğ‘‡
,
where the last equality is due to (Ëœğ…1)Ëœğ‘³=0âŠ¤and Ëœğ‘³Ëœğ…
1
=0. Then,
we obtain
Ëœğ‘³âˆ’ğ‘ ğ‘¿Ëœğ‘³âˆ’ğ‘ =
Ëœğ‘³âˆ’ğ‘ Ëœğ‘³ğ‘‡,ğ‘ 
Ëœğ‘­Ëœğ‘³âˆ’ğ‘ 
Ëœğ‘³ğ‘ ,ğ‘‡
=(ğ‘°0)Ëœğ‘³Ëœğ‘­Ëœğ‘³ğ‘°
0âŠ¤
=(ğ‘°0)Ëœğ‘³ğ‘°
0âŠ¤
=Ëœğ‘³âˆ’ğ‘ .Finally, we are able to prove Equation (3) as
Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1=Tr Ëœğ‘³âˆ’1
âˆ’ğ‘ =Tr(ğ‘¿)
=Tr Ëœğ‘­âˆ’ğ‘ âˆ’Ëœğ…âŠ¤Ëœğ‘­ğ‘‡,ğ‘ âˆ’Ëœğ‘­ğ‘ ,ğ‘‡Ëœğ…+Ëœğ‘­ğ‘ ,ğ‘ Ëœğ…âŠ¤Ëœğ…
=Tr Ëœğ‘­âˆ’ğ‘ +Ëœğ‘­ğ‘ ,ğ‘  Ëœğ…âŠ¤Ëœğ…+2=Tr Ëœğ‘­+Ëœğ‘­ğ‘ ,ğ‘ 
ğœ‹ğ‘ 
=Tr(ğ‘­)+ğ‘­ğ‘ ,ğ‘ 
ğœ‹ğ‘ =ğ¾+ğ»ğ‘ .
Here the fourth equality is due to (Ëœğ…1)Ëœğ‘­=0âŠ¤and Ëœğ‘­Ëœğ…
1
=0.â–¡
Theorem 3.5 reveals that for any selected node ğ‘ , the estimation
of Kemenyâ€™s constant boils down to the evaluation of the trace of
(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1and the walk centrality of ğ‘ . This alternative formula
motivates us to design an approximation algorithm that estimates
these two quantities separately.
4 Algorithm Design
4.1 Truncated Random Walk Based Algorithm
Combining (2)with Lemma 3.2, we can approximate Kemenyâ€™s
constant of a digraph with an ğ‘™-truncated sum, which is defined as
ğ¾(ğ‘™)=Tr ğ‘­(ğ‘™)=ğ‘™âˆ‘ï¸
ğ‘˜=0
Tr ğ‘·ğ‘˜âˆ’1
. (4)
We note that this estimator is the same as that in DynamicMC [27],
but is proposed from a different perspective. Equation (4)indicates
thatDynamicMC actually supports digraphs. While the analysis of
DynamicMC is restricted to undirected graphs, the approximation
error ofğ¾(ğ‘™)for digraphs is analyzed in Lemma 3.2. Since the direct
computation of ğ‘·ğ‘˜
ğ‘–,ğ‘–involves time-consuming matrix multiplication,
we resort to a Monte Carlo approach similar to DynamicMC. Con-
cretely, for each node ğ‘–âˆˆğ‘‰, we simulate ğ‘Ÿindependent ğ‘™-truncated
random walks starting from ğ‘–. Let Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘—denote the times of ğ‘—thran-
dom walk that return to ğ‘–, and let Â¯ğ‘¡(ğ‘™)
ğ‘–denote the empirical mean of
Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘—, then we can give an unbiased estimator of ğ¾(ğ‘™)based on Â¯ğ‘¡(ğ‘™)
ğ‘–,
which is defined as Ë†ğ¾(ğ‘™)=ğ‘›âˆ’ğ‘™âˆ’1+Ãğ‘›
ğ‘–=1Â¯ğ‘¡(ğ‘™)
ğ‘–.
Although Ë†ğ¾(ğ‘™)is unbiased, we have to simulate numerous trun-
cated random walks to reduce the approximation error. The loose
theoretical error bound leads to excessive simulation, making the ap-
proximation algorithm inefficient in practice. Therefore, we utilize
several optimization techniques. Recall that the truncated random
walk is simulated through two steps: the iteration over nodes and
the simulation of random walks starting from each node. Below we
make improvements from these two perspectives.
4.1.1 Adaptive simulation from each node. In the analysis of Dy-
namicMC, Hoeffdingâ€™s inequality is utilized to derive a theoretical
bound.
Lemma 4.1 (Hoeffdingâ€™s ineqality). Letğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›beğ‘›
independent random variables such that ğ‘â‰¤ğ‘¥ğ‘–â‰¤ğ‘forğ‘–=1,2,...,ğ‘› .
Letğ‘¥=Ãğ‘›
ğ‘–=1ğ‘¥ğ‘–, then for any ğœ–>0,
Pr(|ğ‘¥âˆ’E[ğ‘¥]|â‰¥ğœ–)â‰¤2 exp
âˆ’2ğœ–2
ğ‘›(ğ‘âˆ’ğ‘)2
.
3475Fast Computation of Kemenyâ€™s Constant for Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
However, as Hoeffdingâ€™s inequality does not consider the vari-
ance of random variables, the provided theoretical bound tends
to be relatively loose. To address this limitation, we leverage the
empirical Bernstein inequality [5]:
Lemma 4.2. Letğ‘‹1,ğ‘‹2,...,ğ‘‹ ğ‘›beğ‘›real-valued i.i.d. random vari-
ables that satisfy 0â‰¤ğ‘‹ğ‘–â‰¤ğ‘‹sup. If we denote Â¯ğ‘‹andğ‘‹varas the
empirical mean and the empirical variance of ğ‘‹ğ‘–, then we have
Pr Â¯ğ‘‹âˆ’E[Â¯ğ‘‹]â‰¥ğ‘“ ğ‘›,ğ‘‹ var,ğ‘‹sup,ğ›¿â‰¤ğ›¿,
where
ğ‘“ ğ‘›,ğ‘‹ var,ğ‘‹sup,ğ›¿=âˆšï¸‚
2ğ‘‹varlog(3/ğ›¿)
ğ‘›+3ğ‘‹suplog(3/ğ›¿)
ğ‘›.
Lemma 4.2 differs from Lemma 4.1 in that it involves the em-
pirical variance of random variables. While the empirical variance
remains unknown a priori, it can be efficiently maintained through-
out the simulation. Therefore, our first improvement implements
the empirical Bernstein inequality, but still retains the Hoeffding
bound to preserve theoretical accuracy. Meanwhile, if the empirical
error of Â¯ğ‘¡(ğ‘™)
ğ‘–provided by Lemma 4.2 falls below the threshold, we
terminate the simulation of random walks starting from ğ‘–. Crucially,
the theoretical accuracy of estimating Kemenyâ€™s constant remains
unaffected by applying this adaptive strategy.
4.1.2 Iteration over node subset. Note that Kemenyâ€™s constant of ğº
is concerned with the sum of Â¯ğ‘¡(ğ‘™)
ğ‘–, we attempt to estimate it by only
summing a small proportion of Â¯ğ‘¡(ğ‘™)
ğ‘–, which significantly reduces the
required number of simulated random walks.
Specifically, for an ğ‘›-node digraph ğº=(ğ‘‰,ğ¸), a node subset
XâŠ†ğ‘‰of capacityğ‘˜â‰ªğ‘›is sampled uniformly at random. To effi-
ciently estimate Kemenyâ€™s constant, the original sum ğ‘†=Ã
ğ‘¢âˆˆğ‘‰Â¯ğ‘¡(ğ‘™)
ğ‘¢
is replaced by the partial sum Ëœğ‘†=ğ‘›/ğ‘˜Ã
ğ‘–âˆˆXÂ¯ğ‘¡(ğ‘™)
ğ‘–. As an unbiased
estimator of ğ‘†,Ëœğ‘†is also suitable for estimating Kemenyâ€™s constant.
Lemma 4.3. Ëœğ‘†is an unbiased estimator for ğ‘†.
Next, we provide a guarantee for the additive error of Ëœğ‘†:
Lemma 4.4. Givenğ‘›positive numbers ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›âˆˆ[0,ğ‘]with
their sumğ‘¥=Ãğ‘›
ğ‘–=1ğ‘¥ğ‘–and an error parameter ğœ–>ğ‘›âˆ’1/2log1/2(2ğ‘›). If
we randomly select ğ‘=ğ‘‚ ğ‘ğœ–âˆ’1ğ‘›1/2log1/2ğ‘›numbers,ğ‘¥ğ‘£1,ğ‘¥ğ‘£2,...,ğ‘¥ ğ‘£ğ‘
by Bernoulli trials with success probability ğ‘=ğ‘ğœ–âˆ’1ğ‘›âˆ’1/2log1/2(2ğ‘›)
satisfying 0<ğ‘<1, and define Ëœğ‘¥=Ãğ‘
ğ‘–=1ğ‘¥ğ‘£ğ‘–/ğ‘, then Ëœğ‘¥is an
approximation for the sum ğ‘¥of the original ğ‘›numbers, satisfying
|ğ‘¥âˆ’Ëœğ‘¥|â‰¤ğ‘›ğœ–.
4.1.3 Improved Monte Carlo algorithm. Using Lemmas 4.2 and 4.4,
we propose an improved MC algorithm for approximating Ke-
menyâ€™s constant of digraphs, which is depicted in Algorithm 1.
According to Lemma 4.4, ImprovedMC randomly selects|X|=
ğ‘‚ ğœ–âˆ’1ğ‘™ğ‘›1/2log1/2ğ‘›nodes fromğ‘‰(Line 2). ImprovedMC then simu-
latesğ‘‚ ğœ–âˆ’2ğ‘™2logğ‘›ğ‘™-truncated random walks from each selected
nodeğ‘–(Lines 3-9). If the empirical error computed by Lemma 4.2
is less than threshold ğœ–/3, the simulation terminates (Line 8). By
summing up Â¯ğ‘¡(ğ‘™)
ğ‘–,ImprovedMC returns Ëœğ¾(ğ‘™)as the approximation
for Kemenyâ€™s constant. The performance of ImprovedMC is char-
acterized in Theorem 4.5.Algorithm 1: ImprovedMC(ğº,ğœ–)
Input :ğº=(ğ‘‰,ğ¸): a digraph with ğ‘›nodes;ğœ–>0: an
error parameter
Output : Ëœğ¾(ğ‘™): approximation of Kemenyâ€™s constant ğ¾inğº
1ğ‘™â†llog(3/(ğœ–âˆ’ğœ–ğœ†))
log(1/ğœ†)m
,ğ‘Ÿâ†
9ğœ–âˆ’2ğ‘™2log(2ğ‘›)/4
2Sample a node setXâŠ†ğ‘‰satisfying
|X|=minnl
3ğœ–âˆ’1ğ‘™ğ‘›1/2log1/2ğ‘›/2m
,ğ‘›o
3foreach nodeğ‘–âˆˆXdo
4 Â¯ğ‘¡(ğ‘™)
ğ‘–â†0
5 forğ‘—=1,2,...,ğ‘Ÿ do
6 Sample the ğ‘—thğ‘™-truncated random walk starting
fromğ‘–, and let Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘—be the times of walk that return
toğ‘–
7 Â¯ğ‘¡(ğ‘™)
ğ‘–â†Â¯ğ‘¡(ğ‘™)
ğ‘–+Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘—
8 ifğ‘“ ğ‘—,Ë†ğ‘¡(ğ‘™)
var,ğ‘™/2,1/ğ‘›â‰¤ğ‘›ğœ–/3then break
9 Â¯ğ‘¡(ğ‘™)
ğ‘–â†Â¯ğ‘¡(ğ‘™)
ğ‘–/ğ‘—
10return Ëœğ¾(ğ‘™)=ğ‘›âˆ’ğ‘™âˆ’1+ğ‘›
|X|Ã
ğ‘–âˆˆXÂ¯ğ‘¡(ğ‘™)
ğ‘–
Theorem 4.5. For anğ‘›-node digraph ğºand an error parameter
ğœ–>0, Algorithm 1 runs in ğ‘‚ ğœ–âˆ’3ğ‘™4ğ‘›1/2log3/2ğ‘›time, and returns
Ëœğ¾(ğ‘™)as the approximation for Kemenyâ€™s constant ğ¾ofğº, which is
guaranteed to satisfyËœğ¾(ğ‘™)âˆ’ğ¾â‰¤2ğœ–ğ¾with high probability.
Although the time complexity of ImprovedMC is sublinear with
respect to the number of nodes, this complexity bound remains
relatively loose due to the inclusion of Lemma 4.2.
4.2 Algorithm Based on Directed Tree Sampling
Although ImprovedMC achieves enhanced efficiency through opti-
mization techniques, its accuracy remains to be improved. Lever-
aging the alternative formula in Theorem 3.5, we propose another
MC algorithm TreeMC, which samples incoming directed rooted
spanning trees. Due to the improved diagonal dominance of the sub-
matrix in (3),TreeMC attains enhanced accuracy. After presenting
TreeMC, we analyze its time complexity and error guarantee.
Recall from Theorem 3.5 that the calculation of Kemenyâ€™s con-
stant is reduced to the evaluation of Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1andğ»ğ‘ . Mean-
while, Lemma 3.4 and (1)indicate that ğ»ğ‘ can be estimated by sim-
ulating truncated random walks. Therefore, the main goal of our
MC algorithm is to approximate Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1.
For random walks in digraph ğº=(ğ‘‰,ğ¸)with absorbing node ğ‘ ,
(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
ğ‘–,ğ‘—denotes the expected passage times over node ğ‘—by a
random walker initialized at node ğ‘–prior to absorption at node ğ‘ [40].
Using this physical meaning, we can estimate Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
by simulating absorbing random walks from each node in ğ‘‰\
{ğ‘ }. However, the expected running time of single simulation isÃğ‘›
ğ‘–=1Ãğ‘›
ğ‘—=1(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
ğ‘–,ğ‘—=1âŠ¤(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’11, which is large due to
the dense property of (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1. To improve the efficiency of
simulation, we introduce the method of sampling incoming directed
rooted spanning trees, which is essentially simulating loop-erased
random walks.
3476KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
Starting from a node, the loop-erased random walk consists of
two phases: simulation of a random walk as well as the erasure of
the loop within walk path. The loop-erased random walk is used
in Wilsonâ€™s algorithm [ 35], where loop-erased paths are iteratively
generated. The aggregate of these loop-erased paths is a directed,
cycle-free subgraph of ğº. In this subgraph, every non-root node
ğ‘–âˆˆğ‘‰\{ğ‘ }has out-degree 1, while the root node ğ‘ has out-degree
0. This subgraph is denoted as an incoming directed spanning tree
rooted atğ‘ . Based on a variant of Wilsonâ€™s algorithm, we prove that
the diagonal elements of (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1can be well approximated.
Lemma 4.6. For a digraph ğº=(ğ‘‰,ğ¸)with transition matrix ğ‘·,
we simulate loop-erased random walks with an absorbing node ğ‘ . Let
ğ‘¡ğ‘–denote the passage times over node ğ‘–, then we have
E[ğ‘¡ğ‘–]=(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
ğ‘–,ğ‘–. (5)
Lemma 4.6 reveals that estimating Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1via a single
sampled spanning tree is equivalent to simulating ğ‘›absorbing ran-
dom walks. However, spanning tree sampling proves substantially
more efficient than absorbing walk simulation. This efficiency gain
arises because loop-erased walks terminate upon revisiting prior
loop-free trajectories. Consequently, we estimate Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
through sampling spanning trees rather than absorbing walk simu-
lations.
Specifically, we sample ğ‘Ÿincoming directed rooted spanning trees
by performing loop-erased random walks. Let Ë†ğ‘¡ğ‘–,ğ‘—denote the passage
times overğ‘–for theğ‘—thsample, and Â¯ğ‘¡ğ‘–denote the empirical mean over
ğ‘—. ThenÃ
ğ‘–âˆˆğ‘‰\{ğ‘ }Â¯ğ‘¡ğ‘–is an unbiased estimator of Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1.
We continue to provide an error bound for the sample size ğ‘Ÿ. To
this end, we bound the passage times Ë†ğ‘¡ğ‘–,ğ‘—with high probability.
Lemma 4.7. Given anğ‘›-node digraph ğº=(ğ‘‰,ğ¸)and an absorbing
nodeğ‘ âˆˆğ‘‰, letğ‘‘maxbe the maximum out-degree, and let ğœbe the
diameter ofğº, which is the longest distance between all pairs of nodes.
Ifğ‘¡is selected satisfying ğ‘¡â‰¥eğœğ‘‘ğœmax
log 4ğ‘›2
/2, then
Pr Ë†ğ‘¡ğ‘–,ğ‘—>ğ‘¡â‰¤1
4ğ‘›2.
Finally, we introduce a theoretical bound for the sample size ğ‘Ÿ.
Lemma 4.8. Given anğ‘›-node digraph ğº=(ğ‘‰,ğ¸)with absorb-
ing nodeğ‘ , if we sample ğ‘Ÿincoming directed rooted spanning trees
satisfyingğ‘Ÿâ‰¥ğœ–âˆ’2e2ğœ2ğ‘‘2ğœmax
log3 4ğ‘›2
, then for any ğœ–>0, we have
PrTr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1âˆ’âˆ‘ï¸
ğ‘–âˆˆğ‘‰\{ğ‘ }Â¯ğ‘¡ğ‘–â‰¥ğ‘›ğœ–
2
â‰¤1
2ğ‘›.
Based on the above analyses, we propose a more accurate MC
algorithm TreeMC for estimating Kemenyâ€™s constant, which is
depicted in Algorithm 2. Given an ğ‘›-node digraph ğºand an error
parameterğœ–, By selecting absorbing node ğ‘ as the node with the
largestğœ‹ğ‘ ,TreeMC first estimates Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1(Lines 8-12).
For this purpose, TreeMC samplesğ‘‚ ğœ–âˆ’2ğœ2ğ‘‘2ğœmaxlog3ğ‘›incoming
directed rooted spanning trees. The sampling procedure consists
of the random walk part (Lines 8-12) and the loop-erasure part
(Lines 13-15). Then, TreeMC estimatesğ»ğ‘ (Lines 17-22). Analogous
to Algorithm 1, TreeMC also simulates ğ‘™-truncated random walks
and introduces Lemma 4.2 for optimization. Combining these two
estimations, TreeMC returns Ë†ğ¾as the approximation for Kemenyâ€™sconstant. The performance of TreeMC is analyzed in Theorem 4.9.
Again, the additive error guarantee of ğ‘›ğœ–can be converted to the rel-
ative error guarantee of 2ğœ–due to the minimum Kemenyâ€™s constant
ofğ‘›-node digraph being (ğ‘›+1)/2[17].
Algorithm 2: TreeMC(ğº,ğœ–)
Input :ğº=(ğ‘‰,ğ¸): a digraph with ğ‘›nodes;ğœ–: an error
parameter
Output : Ë†ğ¾: approximation of Kemenyâ€™s constant ğ¾inğº
1ğ‘ â†arg maxğ‘–âˆˆğ‘‰ğœ‹ğ‘–,ğ‘Ÿâ†
ğœ–âˆ’2e2ğœ2ğ‘‘2ğœmax
log3 4ğ‘›2
2forğ‘—=1,2,...,ğ‘Ÿ do
3 Ë†ğ‘¡ğ‘–,ğ‘—â†0forğ‘–âˆˆğ‘‰\{ğ‘ }
4 next ğ‘–â†arbitrary value for ğ‘–âˆˆğ‘‰\{ğ‘ }
5 InTree ğ‘–â†false forğ‘–âˆˆğ‘‰\{ğ‘ }
6 InTree ğ‘ â†true
7 foreachğ‘¢âˆˆğ‘‰\{ğ‘ }do
8ğ‘–â†ğ‘¢
9 while InTree ğ‘–=false do
10 Ë†ğ‘¡ğ‘–,ğ‘—â†Ë†ğ‘¡ğ‘–,ğ‘—+1
11 next ğ‘–â†a randomly selected out-neighbor of ğ‘–
12 ğ‘–â†next ğ‘–
13ğ‘–â†ğ‘¢
14 while InTree ğ‘–=false do
15 InTree ğ‘–â†true,ğ‘–â†next ğ‘–
16Â¯ğ‘¡ğ‘–â†Ãğ‘Ÿ
ğ‘—=1Ë†ğ‘¡ğ‘–,ğ‘—/ğ‘Ÿforğ‘–âˆˆğ‘‰\{ğ‘ }
17ğ‘™â†lğœlog(ğ‘›ğœ–(2ğœ)âˆ’1ğ‘‘âˆ’ğœ
max)
log(1âˆ’ğ‘›ğ‘‘âˆ’ğœmax)m
+ğœâˆ’1,ğ‘Ÿâ€²â†lğ‘™2logğ‘›
2ğœ–2ğœ‹2ğ‘ ğ‘›2m
,Â¯ğ‘¡(ğ‘™)
ğ‘ â†0
18forğ‘—=1,2,...,ğ‘Ÿâ€²do
19 Sample the ğ‘—thğ‘™-truncated random walk starting from ğ‘ ,
and let Ë†ğ‘¡(ğ‘™)
ğ‘ ,ğ‘—be the times of walk returning to ğ‘ 
20 Â¯ğ‘¡(ğ‘™)
ğ‘ â†Â¯ğ‘¡(ğ‘™)
ğ‘ +Ë†ğ‘¡(ğ‘™)
ğ‘ ,ğ‘—
21 ifğ‘“ ğ‘—,Ë†ğ‘¡(ğ‘™)
var,ğ‘™/2,1/ğ‘›â‰¤âˆšğ‘›ğœ–/2then break
22Â¯ğ‘¡(ğ‘™)
ğ‘ â†Â¯ğ‘¡(ğ‘™)
ğ‘ /ğ‘—
23return Ë†ğ¾=âˆ’ğœ‹âˆ’1ğ‘  Â¯ğ‘¡(ğ‘™)
ğ‘ +1+Ã
ğ‘–âˆˆğ‘‰\{ğ‘ }Â¯ğ‘¡ğ‘–
Theorem 4.9. For anğ‘›-node digraph ğº=(ğ‘‰,ğ¸)with absorbing
nodeğ‘ and transition matrix ğ‘·, letğœdenote the diameter of ğºand
letğ‘‘maxdenote the maximum out-degree of nodes in ğº. If the error
parameterğœ–is determined, then the time complexity of Algorithm 2
isğ‘‚(ğ‘‡), where
ğ‘‡=ğœ–âˆ’2ğœ2ğ‘‘2ğœ
maxlog3ğ‘›Â·Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1+ğ‘™3logğ‘›
2ğœ–2ğœ‹2ğ‘ ğ‘›2.
Algorithm 2 returns Ë†ğ¾as the approximation for Kemenyâ€™s constant
ğ¾ofğº, which satisfiesË†ğ¾âˆ’ğ¾â‰¤2ğœ–ğ¾.
As shown in Theorem 4.9, the expected time for sampling an
incoming directed rooted spanning tree scales as Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1,
equivalent to the sum of diagonal entries in (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1. In contrast,
the expected cost of a single naive absorbing walk simulation entails
summing all entries of the dense matrix (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1. Therefore,
the efficiency gains of TreeMC are significant relative to naive
simulation of absorbing random walks.
3477Fast Computation of Kemenyâ€™s Constant for Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Table 2: The running time (seconds, ğ‘ ) ofApproxKemeny (ApprKem), DynamicMC (DynMC), AblationMC (AblatMC),
ImprovedMC andTreeMC with various ğœ–on realistic networks.
Type Network Node EdgeRunning time (s)
ApprKem DynMC AblatMCImprovedMC TreeMC
ğœ–=0.3ğœ–=0.2ğœ–=0.15 ğœ–=0.3ğœ–=0.2ğœ–=0.15Undir
ectedSister
cities 10,320 17,988 0.714 1.246 0.883 0.252 0.586 1.104 0.011 0.027 0.036
PGP 10,680 24,316 0.767 1.230 0.792 0.222 0.508 0.927 0.008 0.016 0.027
CAIDA 26,475 53,381 1.274 1.507 1.408 0.268 0.610 1.107 0.013 0.025 0.042
Skitter 1,694,616 11,094,209 â€“ 437.0 92.78 0.973 1.534 2.102 0.829 1.795 3.000
Orkut 3,072,441 117,184,899 â€“ 446.3 223.2 1.854 2.873 3.930 3.022 5.849 10.03
soc-LiveJournal 5,189,808 48,687,945 â€“ 2843 363.2 2.351 3.610 4.921 4.802 9.920 16.82Dir
ectedGnutella30
8,490 31,706 â€“ 1.138 0.980 0.292 0.714 1.272 0.007 0.014 0.028
Wikilink-wa 22,528 135,661 â€“ 1.388 1.097 0.238 0.509 1.005 0.015 0.028 0.052
Epinions 32,223 443,506 â€“ 2.760 1.545 0.305 0.611 1.256 0.022 0.046 0.075
EU Inst 34,203 151,132 â€“ 2.636 1.486 0.229 0.594 1.087 0.025 0.041 0.072
Wikilink-la 158,427 3,486,928 â€“ 4.724 2.407 0.264 0.352 0.961 0.109 0.227 0.436
Higgs 360,210 14,102,583 â€“ 14.33 8.100 0.479 0.669 0.894 0.315 0.650 1.076
Youtube 509,245 4,269,142 â€“ 27.41 16.10 0.583 0.925 1.236 0.306 0.686 1.183
Pokec 1,304,537 29,183,655 â€“ 128.4 62.22 0.795 1.229 1.635 0.839 1.836 3.153
Stack Overflow 1,642,130 32,759,694 â€“ 152.5 79.31 0.870 1.379 1.875 0.882 1.936 3.350
Wikilink-fr 2,311,584 113,754,248 â€“ 187.7 99.59 0.939 1.487 2.007 1.739 3.416 5.465
DBpedia 3,796,073 97,783,747 â€“ 344.9 164.7 1.299 2.053 2.809 2.925 5.744 9.781
Wikilink-en 8,026,662 416,705,115 â€“ â€“ 445.4 2.778 3.952 5.360 9.386 18.99 33.23
Meanwhile, Theorem 4.9 also emphasizes the importance of
selecting absorbing node ğ‘ inTreeMC. First, the expected running
time of TreeMC depends on the mean hitting time from nodes in
ğºtoğ‘ . Enhanced reachability of ğ‘ leads to improved efficiency of
TreeMC. Additionally, the accuracy of estimating ğ»ğ‘ is related to
the scaling coefficient ğœ‹âˆ’1ğ‘ . Ifğœ‹âˆ’1ğ‘ is smaller, then the theoretical
accuracy of TreeMC can be reduced. Therefore, we choose ğ‘ as the
node with maximum ğœ‹ğ‘ , which is both reasonable and efficient for
implementation.
5 Numerical Experiments
5.1 Experimental Settings
Datasets. The data of real-world digraphs utilized in our experi-
ments are sourced from the Koblenz Network Collection [ 20] and
SNAP [ 25]. To facilitate fair comparison against previous works, we
also conduct experiments on several undirected networks consid-
ered in [ 27,37]. For those networks that are not originally strongly
connected, we perform our experiments on their largest strongly
connected components (LSCCs). Relevant information about the
LSCC of studied real-world networks is shown in Table 2, where
networks are listed in ascending order by the number of nodes. The
smallest network has 8490 nodes, while the largest one consists of
more than 8 million nodes.
Environment. All experiments are conducted on a Linux server
with 32-core 2.5GHz CPU and 256GB of RAM. We implement all
the algorithms in Julia for a fair comparison. For ApproxKemeny,
we leverage the Laplacian Solver from [ 22]. Since all the algorithms
are pleasingly parallelizable, we force the program to run on 32
threads in every experiment.
Baselines and Parameters. To showcase the superiority of
our proposed algorithms, we implement several existing methods
for comparison. First, we implement the dynamic version of thestate-of-the-art method DynamicMC presented in [ 27] as a base-
line. Moreover, we implement the algorithm ApproxKemeny in [37],
which is on the basis of the Laplacian solver [ 22]. Meanwhile, as
our proposed algorithm ImprovedMC incorporates two optimiza-
tion techniques, we need to ensure that both of these techniques
meet our expectations. For this purpose, we implement an ablation
method called AblationMC, which solely utilizes the adaptive
sampling technique while excluding the subset sampling technique.
ForDynamicMC, we set the threshold parameter ğœ–ğ‘‘=0.0005ğ‘›,
which is the same as [ 27]. For ApproxKemeny, the error parameter
ğœ–is set to be 0.2, which is shown in [ 37] to achieve good perfor-
mance. For AblationMC, the error parameter ğœ–is also set to be 0.2.
It is worth noting that ImprovedMC andTreeMC are dependent
on the second largest modulus of eigenvalues ğœ†ofğ‘·, and TreeMC
additionally depends on the stationary distribution ğ…, the left eigen-
vector associated with eigenvalue 1 for ğ‘·. Therefore, we use the
Implicitly Restarted Arnoldi Method [ 24] to calculate ğœ†andğ…in
advance for each tested network. Our results demonstrate that even
for the largest real-world dataset tested, the pre-computation time
is much less than one minute, which is negligible compared with
the time for calculating Kemenyâ€™s constant. Therefore, we do not
take the pre-computation time into account in our experiments.
5.2 Results on Real-world Networks
5.2.1 Efficiency. We first evaluate the efficiency of our proposed
algorithms on real-world networks. The execution time of our
proposed algorithms and baselines is reported in Table 2. Specif-
ically, we present the results of ImprovedMC andTreeMC for
ğœ–âˆˆ{0.3,0.2,0.15}. Note that due to the limit of memory space,
ApproxKemeny is infeasible for the large undirected graphs, while
DynamicMC is infeasible for the largest digraph due to the running
time being more than one hour.
3478KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
Table 3: The mean relative error ( Ã—10âˆ’3) of ApproxKe-
meny (ApprKem), DynamicMC (DynMC), AblationMC
(AblatMC), ImprovedMC andTreeMC.
NetworkMean relative error ( Ã—10âˆ’3)
ApprKem DynMC AblatMCImprovedMC TreeMC
0.3 0 .2 0 .15 0 .3 0 .2 0 .15
Sister cities 0.276 2.550 1.479 2.768 1.504 0.812 0.839 0.265 0.225
PGP 0.904 3.175 2.387 2.268 0.781 0.454 0.523 0.183 0.170
CAIDA 0.103 1.996 0.367 3.231 0.912 0.772 0.157 0.075 0.054
Gnutella30 â€“ 0.685 0.266 0.506 0.252 0.155 0.453 0.179 0.081
Wikilink-wa â€“ 0.427 0.094 1.703 0.167 0.112 0.239 0.049 0.027
Epinions â€“ 4.607 1.622 2.652 1.926 0.829 0.508 0.177 0.140
EU Inst â€“ 9.261 0.295 2.364 1.739 0.632 0.279 0.215 0.165
Table 2 indicates that for every real-world network, the run-
ning time of ImprovedMC andTreeMC withğœ–=0.2is smaller
than that of baselines. Recall that the theoretical running time of
ImprovedMC andTreeMC is proportional to ğœ–âˆ’3andğœ–âˆ’2, respec-
tively. As shown in Table 2, the larger constant factor ğœ–âˆ’3leads
to the phenomenon that ImprovedMC is slower than TreeMC on
relatively small networks. However, for large-scale networks like
Wikilink-en, the sublinear time complexity of ImprovedMC be-
comes dominant, leading to evident speedup compared to TreeMC.
Additionally, we observe that the running time of DynamicMC in
our experiments is longer than that reported in [ 27]. As mentioned
in Section 2.4, the high efficiency of DynamicMC in [27] is largely
attributed to GPU-based implementation. In our experiments, we
implement all baselines and our proposed approaches by using 32
CPU threads, which also ensures a fair comparison.
The results in Table 2 also reveal that the ablation method Abla-
tionMC outperforms DynamicMC in almost every tested network.
This advantage indicates that the optimization by adaptive sampling
technique is effective. Meanwhile, the speed-up of ImprovedMC
compared to AblationMC is also remarkable, which validates the
high efficiency of the subset sampling technique.
5.2.2 Accuracy. We next evaluate the accuracy of our proposed
algorithms. According to (2), we can compute the exact value of
Kemenyâ€™s constant for small real-world networks. The mean rela-
tive error of approximation algorithms compared with exact values
is reported in Table 3.
Table 3 indicates that for all the evaluated algorithms that exhibit
a theoretical error guarantee, their estimated relative error is signif-
icantly lower than guaranteed value, including ImprovedMC and
TreeMC. For the case of ImprovedMC withğœ–=0.2, its maximum
approximation error is less than 0.2%, which is considerably small.
Furthermore, it is evident that TreeMC withğœ–=0.15consistently
provides the most precise answer, which can be largely attributed
to the alternative formula derived in Theorem 3.5.
Finally, the results in Table 3 also indicate that the mean rela-
tive error of ablation method AblationMC is always lower than
that of DynamicMC. This discrepancy in empirical accuracy arises
from the different selections of algorithm parameters. For Dynam-
icMC, the simulation amount ğ‘Ÿis fixed and the truncated length ğ‘™
is dynamically determined. The error introduced by dynamically
determining ğ‘™is biased, and this bias cannot be compensated by afixed large value of ğ‘Ÿ. In contrast, AblationMC fixesğ‘™and dynam-
ically determines ğ‘Ÿbased on Lemma 4.2, whose incurred error is
unbiased. Recall that AblationMC solely incorporates the adaptive
sampling technique, we can confirm that this technique makes sig-
nificant improvement of efficiency without sacrificing theoretical
accuracy. In summary, our proposed algorithms exhibit comparable
accuracy with remarkable speed-up, and the algorithm TreeMC is
even more accurate than other competitors.
5.3 Influence of Varying Error Parameter
In evaluating the efficiency and accuracy of our algorithms, we ob-
serve that the error parameter ğœ–markedly impacts the performance.
We now examine in detail how ğœ–affects the efficiency and accuracy.
Specifically, we range ğœ–from 0.15 to 0.4 and provide the running
time and mean relative error of each algorithm on several real-
world networks. Notably, for DynamicMC the parameter ğœ–ğ‘‘relates
to threshold rather than error guarantee. We thus fix ğœ–ğ‘‘=0.0005ğ‘›
and present the performance of DynamicMC as a baseline.
5.3.1 Effect on efficiency. We first assess the impact of varying
error parameter on the efficiency of different algorithms. The results
on real-world networks are presented in Figure 1.
0.40 0.35 0. 30 0.25 0.20 0 .15101.0101.5102.0102.5103.0 (a)
0.40 0.35 0. 30 0.25 0.20 0 .15102.5103.0103.5104.0 (b)
0.40 0.35 0. 30 0.25 0.20 0 .15102.5103.0103.5104.0104.5
(c)0.40 0 .35 0 .30 0 .25 0.20 0. 15103.0103.5104.0104.5105.0(d)
0.40 0 .35 0 .30 0 .25 0.20 0. 15103104105(e)
0.40 0 .35 0 .30 0 .25 0.20 0. 15103.0103.5104.0104.5105.0 (f)Running time (milliseconds)
Error parameter ğœ–Dyna micMC ApproxKemen y AblationMC Im proved MC TreeMC
Figure 1: Running time of different approximate algorithms
with varying error parameter ğœ–on real-world networks:
CAIDA (a), Higgs (b), Youtube (c), Pokec (d), Skitter (e) and
Wikilink-fr (f).
As shown in Figure 1, the running time of ApproxKemeny and
TreeMC demonstrates comparable growth trends, which is more
apparent than competitors. This aligned scaling corroborates the
asymptotic complexities proportional to ğœ–âˆ’2. For AblationMC and
ImprovedMC, their theoretical complexity is proportional to ğœ–âˆ’2
andğœ–âˆ’3, respectively. However, such growth patterns are less evi-
dent on large networks like Pokec and Skitter. This situation should
be attributed to the leverage of adaptive sampling technique, which
3479Fast Computation of Kemenyâ€™s Constant for Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
may terminate the unnecessary simulation in advance. This result
further validates the effectiveness of adaptive sampling technique.
0.150.200.250.300.350.4010âˆ’3.510âˆ’3.010âˆ’2.510âˆ’2.0
(a)
0.150.200.250.300.350.4010âˆ’3.510âˆ’3.010âˆ’2.510âˆ’2.0
(b)
0.150.200.250.300.350.4010âˆ’4.010âˆ’3.510âˆ’3.010âˆ’2.5(c)0.150.200.250.30 0.350.4010âˆ’4.510âˆ’4.010âˆ’3.510âˆ’3.010âˆ’2.5(d)
0.150.200.250.30 0.350.4010âˆ’3.510âˆ’3.010âˆ’2.5(e)
0.150.200.250.30 0.350.4010âˆ’3.510âˆ’3.010âˆ’2.510âˆ’2.0
(f)Mean relative  error ğœŒ
Error parameter ğœ–Dynami cMC Approx Kemeny AblationMC ImprovedMC TreeMC
Figure 2: Mean relative error of different approximate algo-
rithms with varying error parameter ğœ–on real-world net-
works: Sister cities (a), PGP (b), CAIDA (c), Wikilink-wa (d),
Epinions (e) and EU Inst (f).
5.3.2 Effect on accuracy. We next analyze the impact of varying
ğœ–on the accuracy across algorithms. The results are presented in
Figure 2. As shown in Figure 2, TreeMC consistently yields the
highest or the second highest accuracy in estimating Kemenyâ€™s con-
stant. Although the error of ImprovedMC with largeğœ–is not ideal,
reducingğœ–to 0.2 or 0.15 significantly decreases its error to levels
comparable with other methods. Notably, the estimation errors of
our proposed algorithms are sensitive as ğœ–changes, confirming that
their accuracy is effectively governed by the error parameter.
6 Related Work
Other Laplacian solver-based methods. In addition to existing
methods mentioned in Section 2.4, Zhang et al. [39] introduced
another approximation algorithm ApproxHK, which also leverages
the nearly linear-time Laplacian solver [ 22]. Unlike ApproxKe-
meny [37] that uses Hutchinsonâ€™s method to avoid direct com-
putation of the pseudoinverse, ApproxHK leverages the Johnson-
Lindenstrauss (JL) lemma. However, the total memory requirements
of JL lemma and Laplacian solver limit ApproxHKâ€™s scalability com-
pared to ApproxKemeny on large graphs. In contrast, our proposed
algorithms mainly use memory to store the network, enabling im-
proved scalability. Meanwhile, the usage of Laplacian solver also
restricts ApproxHK to undirected graphs, while our algorithms
support both directed and undirected graphs.
Other spanning tree-based methods. To approximate ma-
trix inverses, researchers have proposed other spanning tree-based
approaches. Angriman et al. [4] introduced an algorithm for esti-
mating the diagonal elements of the Laplacian pseudoinverse bysampling spanning trees. Specifically, they infused current flows
into the sampled spanning trees and used the average value of
current flows to estimate resistance distance for undirected graphs,
which is a key procedure of their algorithm. However, the defini-
tion of resistance distance is limited to undirected graphs, and the
theoretical foundation of current flows cannot be directly applied
to digraphs. In contrast, our proposed algorithm TreeMC samples
rooted spanning trees, which is a distinct approach from that of [ 4].
Discussion of directed Laplacian solver. As discussed in Sec-
tion 2.4, ApproxKemeny [37] cannot be extended to digraphs due to
restrictions of the nearly linear-time Laplacian solver [ 22]. Though
prior works have proposed nearly linear-time algorithms for solv-
ing directed Laplacian systems with errror guarantee [ 12,21], their
theoretical efficiency has not been translated to practical imple-
mentation. Therefore, it remains infeasible to directly apply these
directed Laplacian solvers for efficient estimation of Kemenyâ€™s con-
stant on digraphs.
Computation of personalized PageRank. Apart from the
Kemeny constant, other random walk-based quantities have also
garnered substantial research attention, such as personalized PageR-
ank (PPR) [ 11,15,16,33,36]. Since the PPR vector satisfies a recur-
sive relationship, several recent studies [ 11,33] utilized a variety
of push-based deterministic approaches for its computation, while
others [ 16,36] combined push-based approaches with Monte Carlo
methods. Given that the PPR stems from the ğ›¼-decaying random
walk, where the walker may stop at each visited node with proba-
bilityğ›¼, the expected running time of push-based methods scales
in proportion to ğ›¼âˆ’1. In contrast, simple random walks related to
Kemenyâ€™s constant can be extremely long. Thus, directly adapting
existing push-based algorithms is impractical for efficient Kemenyâ€™s
constant approximation.
7 Conclusion
We presented two different Monte Carlo algorithms ImprovedMC
andTreeMC for approximating Kemenyâ€™s constant effectively and
efficiently. ImprovedMC is based on the simulation of truncated
random walks, which utilizes an adaptive sampling technique as
well as a technique that allows the simulation from a subset of
nodes. Due to these optimization techniques, ImprovedMC exhibits
sublinear time complexity while retaining theoretical accuracy.
In order to further improve the accuracy of estimating Kemenyâ€™s
constant, we proposed TreeMC with the help of an alternative
formula in terms of the inverse of a submatrix associated with
the transition matrix. Extensive numerical results indicate that
ImprovedMC is extremely faster than the state-of-the-art method
with comparable accuracy, and that TreeMC outperforms the state-
of-the-art method in terms of both efficiency and accuracy, but is
slightly slower than ImprovedMC.
Acknowledgements
This work was supported by the National Natural Science Founda-
tion of China (Nos. 62372112, U20B2051, and 61872093).
References
[1]Ahmad Ali Abin. 2018. A random walk approach to query informative constraints
for clustering. IEEE Transactions on Cybernetics 48, 8 (2018), 2272â€“2283.
3480KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
[2]David Aldous and Jim Fill. 2002. Reversible Markov Chains and Random Walks
on Graphs. (2002).
[3]Diego Altafini, Dario A Bini, Valerio Cutini, Beatrice Meini, and Federico Poloni.
2023. An edge centrality measure based on the Kemeny constant. SIAM J. Matrix
Anal. Appl. 44, 2 (2023), 648â€“669.
[4]Eugenio Angriman, Maria Predari, Alexander van der Grinten, and Henning
Meyerhenke. 2020. Approximation of the diagonal of a Laplacianâ€™s Pseudoinverse
for Complex Network Analysis. In Proceedings of the 28th Annual European
Symposium on Algorithms, Vol. 173. 6:1â€“6:24.
[5]Jean-Yves Audibert, RÃ©mi Munos, and Csaba SzepesvÃ¡ri. 2007. Tuning bandit
algorithms in stochastic environments. In Proceedings of the 2007 International
Conference on Algorithmic Learning Theory. 150â€“165.
[6]Joost Berkhout and Bernd F Heidergott. 2019. Analysis of Markov influence
graphs. Operations Research 67, 3 (2019), 892â€“904.
[7]Andrew Beveridge. 2009. Centers for random walks on trees. SIAM Journal on
Discrete Mathematics 23, 1 (2009), 300â€“318.
[8]Andrew Beveridge. 2016. A hitting time formula for the discrete Greenâ€™s function.
Combinatorics, Probability and Computing 25, 3 (2016), 362â€“379.
[9] Norman Biggs. 1993. Algebraic Graph Theory. Cambridge University Press.
[10] Mo Chen, Jianzhuang Liu, and Xiaoou Tang. 2008. Clustering via random walk
hitting time on directed graphs. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 8. 616â€“621.
[11] Zhen Chen, Xingzhi Guo, Baojian Zhou, Deqing Yang, and Steven Skiena. 2023.
Accelerating personalized PageRank vector computation. In Proceedings of the
29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 262â€“273.
[12] Michael B. Cohen, Jonathan Kelner, Rasmus Kyng, John Peebles, Richard Peng,
Anup B. Rao, and Aaron Sidford. 2018. Solving Directed Laplacian Systems in
Nearly-Linear Time through Sparse LU Factorizations. In Proceedings of the 2018
IEEE 59th Annual Symposium on Foundations of Computer Science. 898â€“909.
[13] S Condamin, O BÃ©nichou, V Tejedor, R Voituriez, and J Klafter. 2007. First-passage
times in complex scale-invariant media. Nature 450, 7166 (2007), 77â€“80.
[14] A. El Gamal, J. Mammen, B. Prabhakar, and D. Shah. 2006. Optimal throughput-
delay scaling in wireless networks - part I: the fluid model. IEEE Transactions on
Information Theory 52, 6 (2006), 2568â€“2592.
[15] Guanhao Hou, Xingguang Chen, Sibo Wang, and Zhewei Wei. 2021. Massively
parallel algorithms for personalized PageRank. Proceedings of the VLDB Endow-
ment 14, 9 (2021), 1668â€“1680.
[16] Guanhao Hou, Qintian Guo, Fangyuan Zhang, Sibo Wang, and Zhewei Wei. 2023.
Personalized PageRank on evolving graphs with an incremental index-update
scheme. Proceedings of the ACM on Management of Data 1, 1, Article 25 (2023),
26 pages.
[17] Jeffrey J. Hunter. 2006. Mixing times with applications to perturbed Markov
chains. Linear Algebra Appl. 417, 1 (2006), 108â€“123.
[18] M.F. Hutchinson. 1989. A stochastic estimator of the trace of the influence matrix
for Laplacian smoothing splines. Communications in Statistics - Simulation and
Computation 18, 3 (1989), 1059â€“1076.
[19] Ali Jadbabaie and Alex Olshevsky. 2019. Scaling laws for consensus protocols
subject to noise. IEEE Trans. Automat. Control 64, 4 (2019), 1389â€“1402.
[20] JÃ©rÃ´me Kunegis. 2013. KONECT: the Koblenz network collection. In Proceedings
of the 22nd International Conference on World Wide Web. 1343â€“1350.
[21] Rasmus Kyng, Simon Meierhans, and Maximilian Probst. 2022. Derandomizing
Directed Random Walks in Almost-Linear Time. In Proceedings of the 2022 IEEE
63rd Annual Symposium on Foundations of Computer Science. 407â€“418.
[22] Rasmus Kyng and Sushant Sachdeva. 2016. Approximate Gaussian elimination
for Laplacians - fast, sparse, and simple. In Proceedings of the 2016 IEEE 57th
Annual Symposium on Foundations of Computer Science. 573â€“582.
[23] Bertrand Lebichot and Marco Saerens. 2018. A bag-of-paths node criticality
measure. Neurocomputing 275 (2018), 224â€“236.
[24] R. B. Lehoucq, D. C. Sorensen, and C. Yang. 1998. ARPACK Usersâ€™ Guide. Society
for Industrial and Applied Mathematics.
[25] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network
Dataset Collection. http://snap.stanford.edu/data.
[26] Hui-Jia Li, Lin Wang, Zhan Bu, Jie Cao, and Yong Shi. 2021. Measuring the net-
work vulnerability based on Markov criticality. ACM Transactions on Knowledge
Discovery from Data 16, 2 (2021), 28:1â€“28:24.
[27] Shiju Li, Xin Huang, and Chul-Ho Lee. 2021. An efficient and scalable algorithm
for estimating Kemenyâ€™s constant of a Markov chain on large graphs. In Proceed-
ings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining .
964â€“974.
[28] Seng Pei Liew, Tsubasa Takahashi, Shun Takagi, Fumiyuki Kato, Yang Cao, and
Masatoshi Yoshikawa. 2022. Network shuffling: Privacy amplification via random
walks. In Proceedings of the 2022 International Conference on Management of Data.
773â€“787.
[29] LÃ¡szlÃ³ LovÃ¡sz. 1993. Random walks on graphs: A survey. Combinatorics, Paul
ErdÃ¶s is eighty 2, 1 (1993), 1â€“46.
[30] Carl D. Meyer, Jr. 1975. The role of the group generalized inverse in the theory
of finite Markov chains. SIAM Rev. 17, 3 (1975), 443â€“464.[31] Bibek Paudel and Abraham Bernstein. 2021. Random walks with erasure: diver-
sifying personalized recommendations on social and information networks. In
Proceedings of the Web Conference. 2046â€“2057.
[32] V Tejedor, O BÃ©nichou, and R Voituriez. 2009. Global mean first-passage times of
random walks on complex networks. Physical Review E 80, 6 (2009), 065104.
[33] Hanzhi Wang, Zhewei Wei, Junhao Gan, Ye Yuan, Xiaoyong Du, and Ji-Rong
Wen. 2022. Edge-based local push for personalized PageRank. Proceedings of the
VLDB Endowment 15, 7 (2022), 1376â€“1389.
[34] Scott White and Padhraic Smyth. 2003. Algorithms for estimating relative impor-
tance in networks. In Proceedings of the 9th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. 266â€“275.
[35] David Bruce Wilson. 1996. Generating random spanning trees more quickly than
the cover time. In Proceedings of the 28th Annual ACM Symposium on Theory of
Computing. 296â€“303.
[36] Hao Wu, Junhao Gan, Zhewei Wei, and Rui Zhang. 2021. Unifying the global and
local approaches: An efficient power iteration with forward push. In Proceedings
of the 2021 International Conference on Management of Data. 1996â€“2008.
[37] Wanyue Xu, Yibin Sheng, Zuobai Zhang, Haibin Kan, and Zhongzhi Zhang. 2020.
Power-law graphs have minimal scaling of Kemeny constant for random walks.
InProceedings of The Web Conference. 46â€“56.
[38] Yiming Zhang and Keith W. Ross. 2021. On-policy deep reinforcement learning for
the average-reward criterion. In Proceedings of the 38th International Conference
on Machine Learning, Vol. 139. 12535â€“12545.
[39] Zuobai Zhang, Wanyue Xu, and Zhongzhi Zhang. 2020. Nearly linear time
algorithm for mean hitting times of random walks on a graph. In Proceedings of
the 13th International Conference on Web Search and Data Mining. 726â€“734.
[40] Zhongzhi Zhang, Yihang Yang, and Yuan Lin. 2012. Random walks in modular
scale-free networks with multiple traps. Physical Review E 85, 1 (2012), 011106.
A Proofs of Lemmas and Theorems
A.1 Proof of Lemma 3.4
Proof. Sinceğœrepresents the longest distance between all pairs
of nodes, the inequality ğ‘·ğœ
ğ‘–,ğ‘—â‰¥ğ‘‘âˆ’ğœmaxholds for any nodes ğ‘–,ğ‘—âˆˆğ‘‰.
Plugging this into Lemma 3.3, we obtain
ğ‘­ğ‘–,ğ‘–âˆ’ğ‘­(ğ‘™)
ğ‘–,ğ‘–=âˆâˆ‘ï¸
ğ‘˜=ğ‘™+1ğ‘·ğ‘˜
ğ‘–,ğ‘–âˆ’ğœ‹ğ‘–â‰¤âˆâˆ‘ï¸
ğ‘˜=ğ‘™+1ğ‘·ğ‘˜
ğ‘–,ğ‘–âˆ’ğœ‹ğ‘–
â‰¤âˆâˆ‘ï¸
ğ‘˜=ğ‘™+1 1âˆ’ğ‘›ğ‘‘âˆ’ğœ
maxâŒŠğ‘˜/ğœâŒ‹
â‰¤ğœ 1âˆ’ğ‘›ğ‘‘âˆ’ğœmaxâŒŠ(ğ‘™+1)/ğœâŒ‹
ğ‘›ğ‘‘âˆ’ğœmaxâ‰¤ğœ–,
where the last inequality is due to ğ‘™â‰¥ğœlog(ğ‘›ğœ–ğœâˆ’1ğ‘‘âˆ’ğœ
max)
log(1âˆ’ğ‘›ğ‘‘âˆ’ğœmax)+ğœâˆ’1.â–¡
A.2 Proof of Lemma 4.3
Proof. For a strongly connected ğ‘›-node digraph ğº=(ğ‘‰,ğ¸), we
denote the set of ğ‘˜-combinations of ğ‘‰asğ‘‰ğ‘˜. Since the node subset
Xis sampled uniformly at random, we can represent the expected
value of Ëœğ‘†as
EËœğ‘†
=âˆ‘ï¸
Xâˆˆğ‘‰ğ‘˜1 ğ‘›
ğ‘˜ğ‘›
ğ‘˜âˆ‘ï¸
ğ‘–âˆˆXÂ¯ğ‘¡(ğ‘™)
ğ‘–
=1 ğ‘›
ğ‘˜ğ‘›
ğ‘˜âˆ‘ï¸
ğ‘¢âˆˆğ‘‰ğ‘›âˆ’1
ğ‘˜âˆ’1
Â¯ğ‘¡(ğ‘™)
ğ‘¢=âˆ‘ï¸
ğ‘¢âˆˆğ‘‰Â¯ğ‘¡(ğ‘™)
ğ‘¢=ğ‘†,
which completes our proof. â–¡
A.3 Proof of Lemma 4.4
Proof. Forğ‘–=1,2,...,ğ‘› , letğ‘¦ğ‘–be Bernoulli random variable
such that Pr(ğ‘¦ğ‘–=1)=ğ‘and Pr(ğ‘¦ğ‘–=0)=1âˆ’ğ‘. Hereğ‘¦ğ‘–=1
indicates that ğ‘¥ğ‘–is selected and ğ‘¦ğ‘–=0otherwise. Let ğ‘§ğ‘–=ğ‘¥ğ‘–ğ‘¦ğ‘–beğ‘›
independent random variables with 0â‰¤ğ‘§ğ‘–â‰¤ğ‘. Denote the sum of
random variables ğ‘¦ğ‘–asğ‘¦, and denote the sum of random variables
3481Fast Computation of Kemenyâ€™s Constant for Directed Graphs KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
ğ‘§ğ‘–asğ‘§. Namely,ğ‘¦=Ãğ‘›
ğ‘–=1ğ‘¦ğ‘–andğ‘§=Ãğ‘›
ğ‘–=1ğ‘§ğ‘–. It is clear that ğ‘¦andğ‘§
represent the number of selected numbers and their sum. Therefore
the expectations of ğ‘¦andğ‘§can be expressed as E[ğ‘¦]=ğ‘›ğ‘and
E[ğ‘§]=ğ‘ğ‘¥. According to Lemma 4.1, we have
Pr(|Ëœğ‘¥âˆ’ğ‘¥|â‰¥ğ‘›ğœ–)=Pr|ğ‘§âˆ’ğ‘ğ‘¥|
ğ‘â‰¥ğ‘›ğœ–
â‰¤2 exp
âˆ’2ğ‘2ğ‘›2ğœ–2
ğ‘›ğ‘2
â‰¤1
ğ‘›,
finishing the proof. â–¡
A.4 Proof of Theorem 4.5
Proof. Deriving the time complexity of ImprovedMC is straight-
forward. Therefore, our main focus is to provide the relative error
guarantee for this algorithm.
Referring back to (4),ğ¾is initally approximated by the truncated
sumğ¾(ğ‘™). The approximation error of ğ¾(ğ‘™)dependent on truncated
lengthğ‘™is analyzed in Lemma 3.2. Furthermore, the estimator Ë†ğ¾(ğ‘™)=
ğ‘›âˆ’ğ‘™âˆ’1+Ãğ‘›
ğ‘–=1Â¯ğ‘¡(ğ‘™)
ğ‘–is leveraged to approximate ğ¾(ğ‘™). We next
provide the connection between ğ‘Ÿand the error of Ë†ğ¾(ğ‘™). According
to Lemma 4.1, we have
PrÂ¯ğ‘¡(ğ‘™)
ğ‘–âˆ’EÂ¯ğ‘¡(ğ‘™)
ğ‘–â‰¥ğœ–
3
=Prğ‘Ÿâˆ‘ï¸
ğ‘—=1Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘—âˆ’Eğ‘Ÿâˆ‘ï¸
ğ‘—=1Ë†ğ‘¡(ğ‘™)
ğ‘–,ğ‘—â‰¥ğ‘Ÿğœ–
3
â‰¤2 exp
âˆ’2ğ‘Ÿ2ğœ–2
9ğ‘Ÿ(ğ‘™/2)2
â‰¤1
2ğ‘›2.
Based on the union bound, it holds that
Ë†ğ¾(ğ‘™)âˆ’ğ¾(ğ‘™)â‰¤ğ‘›âˆ‘ï¸
ğ‘–=1Â¯ğ‘¡(ğ‘™)
ğ‘–âˆ’ğ‘™âˆ‘ï¸
ğ‘˜=1ğ‘·ğ‘˜
[ğ‘–,ğ‘–]â‰¤ğ‘›ğœ–
3(6)
with probability

1âˆ’1
ğ‘›2ğ‘›
â‰¥1âˆ’ğ‘›Â·1
2ğ‘›2=1âˆ’1
2ğ‘›.
As stated in Algorithm 1, applying Lemma 4.2 does not introduce
additional error since the error of Ë†ğ¾(ğ‘™)is lower than ğ‘›ğœ–/3. Therefore,
we utilize Lemma 4.4 to give a theoretical bound for the approxi-
mation error of the partial sum Ëœğ¾(ğ‘™)as
Pr Ëœğ¾(ğ‘™)âˆ’Ë†ğ¾(ğ‘™)â‰¥ğ‘›ğœ–
3â‰¤2 exp
âˆ’2|X|ğœ–2
ğ‘›(ğ‘™/2)2
â‰¤1
2ğ‘›. (7)
Plugging (6)and(7)into Lemma 3.2, we derive the additive error
guarantee of Ëœğ¾(ğ‘™):
Pr Ëœğ¾(ğ‘™)âˆ’ğ¾â‰¤ğ‘›ğœ–â‰¥ 1âˆ’1
2ğ‘›2â‰¥1âˆ’1
ğ‘›.
As shown by [ 17], the minimum Kemenyâ€™s constant across all ğ‘›-
node digraphs is (ğ‘›+1)/2. Leveraging this result, we can translate
the additive error bound for Ëœğ¾(ğ‘™)into a relative error guarantee:
Pr Ëœğ¾(ğ‘™)âˆ’ğ¾â‰¤2ğœ–ğ¾â‰¥Pr Ëœğ¾(ğ‘™)âˆ’ğ¾â‰¤ğ‘›ğœ–â‰¥1âˆ’1
ğ‘›,
which completes our proof. â–¡
A.5 Proof of Lemma 4.6
Proof. In the initial round of the loop-erased random walk that
starts from node ğ‘–, there is only one absorbing node ğ‘ and the ex-
pected passage times over ğ‘–is(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
ğ‘–,ğ‘–. Hence (5)holds true for
the initial starting node. For Wilsonâ€™s algorithm, the distributionof sampled random walk path is independent of the node order-
ing [35]. Therefore, every node in ğ‘‰\{ğ‘ }can be sampled in the first
round, which indicates that (5) holds true for all ğ‘–âˆˆğ‘‰\{ğ‘ }.â–¡
A.6 Proof of Lemma 4.7
Proof. Since Ë†ğ‘¡ğ‘–,ğ‘—denotes the passage times of node ğ‘–for the
ğ‘—thloop-erased random walk, it is easy to verify that Ë†ğ‘¡ğ‘–,ğ‘—â‰¤ğ‘‡ğ‘–ğ‘ /2,
whereğ‘‡ğ‘–ğ‘ denotes the hitting time from ğ‘–toğ‘ for theğ‘—thloop-erased
random walk. Therefore, we turn to bound ğ‘‡ğ‘–ğ‘ with high probability,
which requires us to provide an upper bound for its expected value
ğ»ğ‘–ğ‘ .
Recall thatğ»ğ‘–ğ‘ can be expressed as ğ’†âŠ¤
ğ‘–(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’11, we have
max
ğ‘–âˆˆğ‘‰\{ğ‘ }ğ»ğ‘–ğ‘ =(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’11âˆ=âˆâˆ‘ï¸
ğ‘˜=0ğ‘·ğ‘˜
âˆ’ğ‘ âˆ
â‰¤âˆâˆ‘ï¸
ğ‘˜=0 1âˆ’ğ‘‘âˆ’ğœ
maxâŒŠğ‘˜/ğœâŒ‹=ğœ
1âˆ’(1âˆ’ğ‘‘âˆ’ğœmax)=ğœğ‘‘ğœ
max.(8)
According to [ 2], we can finish our proof by providing the upper
bound forğ‘‡ğ‘–ğ‘ with high probability:
Pr Ë†ğ‘¡ğ‘–,ğ‘—>ğ‘¡=Pr(ğ‘‡ğ‘–ğ‘ >2ğ‘¡)â‰¤exp
âˆ’2ğ‘¡
eğ»ğ‘–ğ‘ 
=exp
âˆ’2ğ‘¡
eğœğ‘‘ğœmax
â‰¤1
4ğ‘›2,
where the last inequality is due to ğ‘¡â‰¥eğœğ‘‘ğœmax
log 4ğ‘›2
/2. â–¡
A.7 Proof of Lemma 4.8
Proof. Lemma 4.7 reveals that Ë†ğ‘¡ğ‘–,ğ‘—exhibits an explicit upper
boundğ‘¡=eğœğ‘‘ğœmax
log 4ğ‘›2
/2with a high probability. Plugging
this into Lemma 4.1, we obtain
Pr
|Â¯ğ‘¡ğ‘–âˆ’E[Â¯ğ‘¡ğ‘–]|â‰¥ğœ–
2
=Prğ‘Ÿâˆ‘ï¸
ğ‘—=1Ë†ğ‘¡ğ‘–,ğ‘—âˆ’Eğ‘Ÿâˆ‘ï¸
ğ‘—=1Ë†ğ‘¡ğ‘–,ğ‘—â‰¥ğ‘Ÿğœ–
2
â‰¤1âˆ’
1âˆ’2 exp
âˆ’2ğ‘Ÿ2ğœ–2
4ğ‘Ÿğ‘¡2 
1âˆ’1
4ğ‘›2
â‰¤1âˆ’
1âˆ’1
4ğ‘›22
â‰¤1âˆ’
1âˆ’2
4ğ‘›2
=1
2ğ‘›2,
where the second inequality follows from ğ‘Ÿâ‰¥4ğœ–âˆ’2ğ‘¡2log 2ğ‘›2.
Based on the union bound, it holds thatTr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1âˆ’âˆ‘ï¸
ğ‘–âˆˆğ‘‰\{ğ‘ }Â¯ğ‘¡ğ‘–
â‰¤âˆ‘ï¸
ğ‘–âˆˆğ‘‰\{ğ‘ }(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
ğ‘–,ğ‘–âˆ’Â¯ğ‘¡ğ‘–â‰¤ğ‘›ğœ–
2
with probability

1âˆ’1
2ğ‘›2ğ‘›
â‰¥1âˆ’ğ‘›Â·1
2ğ‘›2=1âˆ’1
2ğ‘›.
This finishes the proof. â–¡
3482KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
A.8 Proof of Theorem 4.9
Proof. The theoretical proof of the additive error guarantee for
TreeMC is straightforward by plugging Lemma 4.8 and Lemma 3.4
into the error analysis of truncated sum mentioned in Theorem 4.5,
and the additive error guarantee can be analogously converted to
the relative error guarantee by [ 17]. Therefore, our primary goal
is to provide the time complexity of this algorithm, which essen-
tially involves assessing the expected running time of sampling an
incoming directed spanning tree.
As shown in Algorithm 2, the time of sampling an incoming
directed spanning tree is determined by the total number of visits
to nodes that are not yet included in the sampled incoming directed
spanning tree. In the first iteration of the loop-erased random walk
starting from node ğ‘–, the expected number of visits to ğ‘–is equal to
(ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1
ğ‘–,ğ‘–. Recall that for Wilsonâ€™s algorithm, the distribution ofsampled path is independent of the node ordering [ 35]. In other
words, any node can be selected in the initial round while the
distribution of sampled path remains unchanged. Therefore, the
expected running time of sampling an incoming directed spanning
tree is equal to Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1.
According to Lemma 4.8, we can obtain the required sample size
ğ‘Ÿ. Combining Lemma 3.4 with the error analysis of truncated sum
in Theorem 4.5, the necessary amount ğ‘Ÿâ€²and lengthğ‘™of simulat-
ing truncated random walks can be determined. Finally we can
represent the time complexity ğ‘‚(ğ‘‡)ofTreeMC as
ğ‘‡=ğ‘ŸÂ·Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1+ğ‘Ÿâ€²Â·ğ‘™
=ğœ–âˆ’2ğœ2ğ‘‘2ğœ
maxlog3ğ‘›Â·Tr (ğ‘°âˆ’ğ‘·âˆ’ğ‘ )âˆ’1+ğ‘™3logğ‘›
2ğœ–2ğœ‹2ğ‘ ğ‘›2,
finishing the proof. â–¡
3483