Offline Reinforcement Learning for Optimizing Production
Bidding Policies
Dmytro Korenkevych
dkorenkevych@meta.com
AI at Meta
Menlo Park, CA, USAFrank Cheng
frankcheng@meta.com
AI at Meta
Sunnyvale, CA, USAArtsiom Balakir
artsiom@meta.com
AI at Meta
Menlo Park, CA, USA
Alex Nikulkov
alexnik@meta.com
AI at Meta
Bellevue, WA, USALingnan Gao
lgao@meta.com
Meta Platform Inc.
Menlo Park, CA, USAZhihao Cen
zcen@meta.com
AI at Meta
Menlo Park, CA, USA
Zuobing Xu
zuobingxu@meta.com
Meta Platform Inc.
Menlo Park, CA, USAZheqing Zhu
billzhu@meta.com
AI at Meta
Bellevue, WA, USA
ABSTRACT
The online advertising market, with its thousands of auctions run
per second, presents a daunting challenge for advertisers who wish
to optimize their spend under a budget constraint. Thus, advertising
platforms typically provide automated agents to their customers,
which act on their behalf to bid for impression opportunities in
real time at scale. Because these proxy agents are owned by the
platform but use advertiser funds to operate, there is a strong prac-
tical need to balance reliability and explainability of the agent with
optimizing power. We propose a generalizable approach to optimiz-
ing bidding policies in production environments by learning from
real data using offline reinforcement learning. This approach can
be used to optimize any differentiable base policy (practically, a
heuristic policy based on principles which the advertiser can easily
understand), and only requires data generated by the base policy
itself. We use a hybrid agent architecture that combines arbitrary
base policies with deep neural networks, where only the optimized
base policy parameters are eventually deployed, and the neural
network part is discarded after training. We demonstrate that such
an architecture achieves statistically significant performance gains
in both simulated and at-scale production bidding environments
compared to the default production bidding policy. Our approach
does not incur additional infrastructure, safety, or explainability
costs, as it directly optimizes the parameters of existing production
routines without necessarily replacing them with black box-style
models like neural networks.
CCS CONCEPTS
â€¢Computing methodologies â†’Reinforcement learning; â€¢
Information systems â†’Online advertising.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671555KEYWORDS
reinforcement learning, auto-bidding
ACM Reference Format:
Dmytro Korenkevych, Frank Cheng, Artsiom Balakir, Alex Nikulkov, Ling-
nan Gao, Zhihao Cen, Zuobing Xu, and Zheqing Zhu. 2024. Offline Reinforce-
ment Learning for Optimizing Production Bidding Policies. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/3637528.3671555
1 INTRODUCTION
An important problem for advertisers participating in online ads
markets is to maximize their total received value under a budget
constraint [ 29]. There are many flavors of these markets, but the
most common is some kind of auction. For each available impres-
sion, these receive the bids of advertisers (together with features
of the ad and ad slot) and output an allocation, specifying which
advertiser wins the impression, along with a price vector, specifying
how much each advertiser participant must pay. For example, in the
widely adopted second-price auction, the price is the second-highest
bid in the auction [ 9,20]. The sum of prices paid by an individual
advertiser during a given period cannot exceed the advertiserâ€™s
budget for that period; this is the advertiserâ€™s budget constraint.
Modern ad markets operate at a scale of thousands of ads auc-
tions per second. It is impractical for advertisers to carefully eval-
uate all of these opportunities on an individual level using their
own computational and data resources [ 28,29]. To alleviate this
issue, many auto-bidding systems have been introduced (chief
among them from platforms themselves), where the optimal bid
values are evaluated by proxy agents on behalf of the advertis-
ers [ 3,6,11,12,15,16,28,35,38,38,40,43â€“45]. Even for auto-
bidding agents, computing the optimal bid values exactly under
budget constraints in real-time is infeasible due to varied mar-
ket competition and auction volumes [ 6,42]. Hence, heuristic bid-
ding policies are usually employed. These policies come in two
flavors: handcrafted heuristic adaptive strategies, such as feedback-
based control, whose policies can be easily bounded and articulated
5251
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dmytro Korenkevych et al.
[5,25,29,38,39,41,43], and black box strategies which are tuned
by learning from data [6, 15, 28, 38, 43, 45].
Studies have shown that the second approach can significantly
improve on the first [ 6,12,15,28,44], yet it has not been widely
employed in consumer technology companies because advertisers
strongly prefer using agents that are safe and explainable due to
the enormous financial impact of making an error. This preference
is shared by platforms, as they share the significant financial conse-
quences of mistakes and must also work to fix mistakes efficiently.
In this paper, we build agents bridging these gaps using offline
reinforcement learning . Our approach is modular with respect
to abase policy , which is an explainable parameterized heuristic
adaptive strategy that enjoys wide acceptance by advertisers. A
simple PID controller [ 5] is one example of such a base policy. These
base policies are highly restricted in both their action space as well
as their observation space. Our output is an optimized policy that
shares the parameter space of the base policy. Empirically, we show
that our approach improves on a highly tuned base policy in a real
production system. This algorithm is currently deployed to Metaâ€™s
online advertising platform for managing all advertiser bids. This
allows us to adhere to advertisersâ€™ and platformsâ€™ transparency
requirements while delivering more total value to advertisers.
Under a reinforcement learning framework, a bidding problem
can be formulated as an episodic Markov Decision Process (MDP),
where each ad campaign represents one episode, and the whole
market (users and competing ads) comprises the environment. The
bidding policy makes decisions sequentially based on the cam-
paign state, and these decisions affect future states and long-term
outcomes. Given this MDP model, the RL framework allows for
the maximization of long-term outcomes through learning opti-
mized bidding policies from real data. Conventionally, modern RL
algorithms are applied to train parameters of powerful function ap-
proximators, namely deep neural networks [ 2,14,18,19,23,26,27].
In this work, we follow a hybrid approach where we combine an
arbitrarily parameterized base bidding policy, whose output is differ-
entiable with respect to its parameters, with deep neural networks
to optimize the base policy parameters. The optimized base policy
can be deployed in production without incurring additional infras-
tructure or safety costs. We train the agent completely offline by
applying an offline RL learning algorithm based on Conservative
Q-Learning (CQL) [ 22]. The training data is collected in produc-
tion by a default production base policy with a small amount of
exploration noise. The agentâ€™s policy is initialized as the behav-
ior policy at the beginning of training, which helps to mitigate
some of the challenges of offline RL, such as distributional shift
[10, 13, 17, 21, 24].
While other optimization approaches, such as Bayesian opti-
mization or evolutionary algorithms, could be applied to similarly
optimize the parameters of a base policy, we would like to empha-
size a critical advantage of the offline RL approach in that a trained
model needs to be deployed and evaluated only once after the train-
ing is complete. An entire optimization process happens offline. In
contrast, these other alternatives require multiple objective func-
tion evaluations during optimization. In a production environment,
each evaluation requires deploying a potentially suboptimal pol-
icy and running an expensive A/B test, and such an optimization
process may incur significant costs.To demonstrate the merit of the described approach empirically,
we apply it to optimize the parameters of a real-time bidding policy.
We first validate our approach in a high-fidelity bidding simulator
and show that it can consistently improve upon the baseline pol-
icy and produce statistically significant performance gains. Most
importantly, we apply the proposed approach to a real production
system and optimize production bidding policy parameters using
real production data. In a series of large-scale A/B tests, we report
statistically significant gains of an optimized model compared to
the original production baseline.
These results confirm that the described approach of using offline
RL with a hybrid agent architecture can be successfully applied to
optimizing varied control policies in real production systems. We
note that this approach is general with respect to the base policy,
and although we empirically validate it in the auto-bidding setting,
it can be applied to any control policy as long as the appropriate
training data can be collected.
2 BACKGROUND AND RELATED WORK
2.1 Reinforcement Learning
Reinforcement learning (RL) framework models an agent inter-
acting with an environment at discrete time steps [ 33]. At each
stepğ‘¡the environment presents the agent with the environment
stateğ‘†ğ‘¡âˆˆS and a scalar reward signal ğ‘…ğ‘¡âˆˆğ‘…. In response, the
agent chooses an action ğ´ğ‘¡âˆˆA according to a policy defined by a
probability distribution function ğœ‹(ğ‘|ğ‘ )=ğ‘ƒ{ğ´ğ‘¡=ğ‘|ğ‘†ğ‘¡=ğ‘ }. At the
subsequent time step ğ‘¡+1partially due to the agentâ€™s action, the
environment transitions to a new state ğ‘†ğ‘¡+1and produces a new
reward value ğ‘…ğ‘¡+1based on a transition probability distribution
functionğ‘(ğ‘ â€²,ğ‘Ÿ|ğ‘ ,ğ‘)=Pr{ğ‘†ğ‘¡+1=ğ‘ â€²,ğ‘…ğ‘¡+1=ğ‘Ÿ|ğ‘†ğ‘¡=ğ‘ ,ğ´ğ‘¡=ğ‘}. The
objective of the agent is to maximize the expected return defined as
the discounted sum of future rewards ğºğ‘¡=Ãâˆ
ğ‘˜=ğ‘¡ğ›¾ğ‘˜âˆ’ğ‘¡ğ‘…ğ‘˜+1, where
ğ›¾âˆˆ[0,1]is a discount factor. In practice, the agent usually does
not have access to the environment state ğ‘ ğ‘¡but observes it partially
through an observation vector ğ‘‚ğ‘¡.
Given a policy ğœ‹, a value function of the policy is defined as:
ğ‘‰ğœ‹(ğ‘ )=Eğœ‹[ğºğ‘¡|ğ‘†ğ‘¡=ğ‘ ].
Similarly, a state-action value function, also called ğ‘„-function, is
defined as:
ğ‘„ğœ‹(ğ‘ ,ğ‘)=Eğœ‹[ğºğ‘¡|ğ‘†ğ‘¡=ğ‘ ,ğ´ğ‘¡=ğ‘].
2.2 Related Work
Our work is related to literature that models the bidding problem
as an MDP and solves it using learning approaches [ 1,4,6,12,
28,34,36,38]. Many of these approaches, however, formulated
the problem as contextual bandits [ 4,12,36], which does not ac-
count for intra-episode dynamics. Other proposed solutions are
not applicable at large production scales [ 1] or require replacing
the production bidding policy with a particular form of the policy
function, usually a deep neural network, which incurs additional
infrastructure, support, and safety costs [ 1,6,28,34,38,45]. In [ 6]
the authors propose applying a dynamic programming approach
to derive an optimal policy for an individual auction. The policy
computation requires access to the predicted click-through rate
(pCTR) for each individual auction. While the approach is suitable
5252Offline Reinforcement Learning for Optimizing Production Bidding Policies KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
for small-scale problems, it is not applicable to real production
scenarios. In [ 38] the authors applied the DQN algorithm to collect
data and train an approximate Q-function parameterized by a deep
neural network. The bidding policy in this case is an epsilon-greedy
policy derived from the Q-function, which is a black box to the
advertiser. This would incur additional costs and risks to implement
in the production environment.
Another relevant line of work applied RL to tuning simple pa-
rameterized policies, such as PID controllers [ 7,8,30â€“32]. In [ 8]
the authors framed the problem as a contextual bandit problem,
where the actions represent PID controller parameters, and an en-
tire episode outcome is treated as a single reward value. In the
context of bidding where the episodes may contain thousands of
time steps, this approach would be sample inefficient and impacted
by a high variance in returns, as an entire episode provides a single
data point in this case. Similarly, in [ 31] an RL agent manipulated
the parameters of a PID controller with the agentâ€™s actions mapped
to the controller parameters values. In [ 7] the authors applied tabu-
larğ‘„-learning to output the PI controller parameters to control a
mobile robot. In [30, 32] the authors applied online RL algorithms
to train agents that output PID controller parameters in real time.
In real production systems, changing the base policy parameters
dynamically at every time step may require additional infrastruc-
ture investment on one hand, and may increase safety risks and
policy explainability costs on the other. In contrast, we learn from
individual time steps in each episode while at the same time we
treat base policy parameters as learnable parameters that do not
change once the model is deployed.
3 METHODS
3.1 Problem Formulation
In this section, we formulate ad campaign budget control as a se-
quential decision-making problem in the context of reinforcement
learning. The campaign budget control agent is given two con-
straints: duration of budget availability, and the total amount of
budget the agent is not allowed to spend over for the duration. At
every time step, the agent bids a portion of its budget to participate
in a set of available auctions. The agentâ€™s budget is then adjusted ac-
cordingly based on the agentâ€™s action and a set of auctions the agent
has won. If the agent wins an auction, the agent receives the auction
target, such as an impression. The goal of the agent is to maximize
the cumulative value of all auction targets it wins throughout the
duration of budget availability without overspending the budget.
To formally model this process as an MDP, we define interac-
tions between the agent and the auction environment in terms of a
quintupleE=(S,A,R,ğ‘‡,ğœŒ):
(1)State spaceS: at each time step ğ‘¡the state element ğ‘†ğ‘¡âˆˆS
describes the full Markovian state of the environment, which
includes the state of the campaign controlled by the agent,
the set of auctions the agent is eligible to participate in at
the given time step, and the states of all competing ads cam-
paigns in those auctions. In practice, the agent doesnâ€™t have
access to the majority of this information and instead op-
erates based on a limited observation vector ğ‘‚ğ‘¡âˆˆğ‘‚that
mainly contains the information about the campaign con-
trolled by the agent.(2)Action spaceA: The agent executes actions that correspond
to auction bids ğ´ğ‘¡âˆˆR.
(3)Reward spaceR: We define the reward ğ‘…ğ‘¡âˆˆRto be equal to
the aggregated value of outcomes of all auctions the agentâ€™s
campaign participated in at the time step ğ‘¡:ğ‘…ğ‘¡=ğ‘ğ‘¡Ã
ğ‘–=1ğ›¿ğ‘–
ğ‘¡ğ‘Šğ‘–
ğ‘¡,
whereğ‘ğ‘¡is the number of auctions the agent participates in
at the stepğ‘¡,ğ‘Šğ‘–
ğ‘¡âˆˆ{0,1}is the outcome of the ğ‘–-th auction,
i.e. win or loss, ğ›¿ğ‘–
ğ‘¡is the value of winning the ğ‘–-th auction,
e.g. the value of a conversion.
(4)Auction campaign horizon ğ‘‡: given that auction campaign
budgets usually have an expiration date, we set an auction
campaign horizon ğ‘‡for the auction environment. We con-
sider each individual ad campaign as one realization, or one
episode, of the above MDP.
(5)State transition probability ğœŒ: given the state and action
pair(ğ‘†ğ‘¡,ğ´ğ‘¡), the environment transitions to the next state
ğ‘†ğ‘¡+1and produces the next reward ğ‘…ğ‘¡+1according to the
conditional probability distribution ğœŒ(ğ‘†ğ‘¡+1,ğ‘…ğ‘¡+1|ğ‘†ğ‘¡,ğ´ğ‘¡).
While the exact form of ğœŒis complex, essentially it models
auction mechanisms and bidding policies of all competing
ads campaigns at each time step.
3.2 Offline reinforcement learning for tuning
production bidding policies
In this work, we view production bidding heuristics as trainable
RL policies. Let us consider a production bidding function ğ¹ğ‘¤:
ğ‘†âŠ‚ğ‘…ğ‘‘â†’ğ´âŠ‚ğ‘…1parameterized by a vector ğ‘¤, which maps
certain ad campaign state representations ğ‘ âˆˆğ‘†to scalar auction
bids. In the proposed view, ğ‘ is an environment state representation,
ğ‘=ğ¹ğ‘¤(ğ‘ )is an action, and the vector ğ‘¤represents learnable policy
parameters. If the function ğ¹ğ‘¤is differentiable with respect to its
parameters ğ‘¤, we can use reinforcement learning and gradient
descent methods to optimize its parameters from real data.
As an example, consider a simple proportional and integral (PI)
controller [5], defined as
ğ‘ğ‘¡=ğ‘ğ‘¡âˆ’1+ğ¾ğ‘ğ‘’ğ‘¡+ğ¾ğ‘–ğ‘¡âˆ‘ï¸
ğœ=0ğ‘’ğœ,
whereğ‘ğ‘¡is the controller output at time ğ‘¡,ğ‘’ğ‘¡is some error measure
at timeğ‘¡that we aim to minimize (for example, a discrepancy
between the fraction of campaign budget spent and the estimated
consumed fraction of total impression opportunities), ğ¾ğ‘andğ¾ğ‘–
are the proportional and integral coefficients respectively. In this
modeling approach, the vector
ğ‘ ğ‘¡=[ğ‘ğ‘¡âˆ’1,ğ‘’ğ‘¡,ğ‘¡âˆ‘ï¸
ğœ=0ğ‘’ğœ]
is a state representation vector at time ğ‘¡, the vector
ğ‘¤=[ğ¾ğ‘,ğ¾ğ‘–]
is a vector of learnable parameters, and the function
ğ¹ğ‘¤(ğ‘ ğ‘¡)=ğ‘ğ‘¡âˆ’1+ğ¾ğ‘ğ‘’ğ‘¡+ğ¾ğ‘–ğ‘¡âˆ‘ï¸
ğœ=0ğ‘’ğœ
5253KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dmytro Korenkevych et al.
Production
bidding
environmenttraining dataset D
 
data collection learning deploymentProduction
bidding
environment
Figure 1: The flowchart of the proposed approach. Here ğ¹ğ‘¤
denotes the base production bidding policy, parameterized
by the vector ğ‘¤;ğœ‹ğ›½
ğ‘¤denotes the base policy with default
parameters and added exploration noise, defined in equation
(2);ğ¹ğ‘¤âˆ—denotes the base policy after training with optimized
parameters vector ğ‘¤âˆ—.
is a deterministic base policy. Please note that the above PI controller
is merely an example of a base policy and it does not represent the
policy that runs in our production system and that we optimized
in this work.
It is unsafe to deploy an under-trained agent in a production en-
vironment, even on a small fraction of traffic. At certain production
scales, even a slightly sub-optimal policy can incur the company
millions of dollars in losses within hours, in addition to customersâ€™
dissatisfaction with the campaignsâ€™ performance. For this reason,
we opted to train the agent offline following an offline RL approach.
Offline RL attempts to learn a high-performing policy from the data
collected by a separate data generation process (also called behav-
ior policy). In offline RL setting the agent does not interact with
the actual environment until fully trained, which is an important
enabling factor in performance-critical production environments.
Given a production base policy ğ¹ğ‘¤, our approach consists of
the following steps (Figure 1). First, we defined a behavior policy
ğœ‹ğ›½
ğ‘¤as the base policy with default production parameter values
and a small scale Gaussian exploration noise (see Section 3.4 for
details). Next, we deployed the behavior policy ğœ‹ğ›½
ğ‘¤to the production
environment and collected a training dataset ğ·. We designed an
agent (Section 3.3) that combines the base policy ğ¹ğ‘¤with deep
neural networks and applied a state-of-the-art offline reinforcement
learning algorithm to optimize the agentâ€™s policy on the dataset ğ·.
Finally, the base policy with optimized parameters ğ¹ğ‘¤âˆ—is deployed
to production for A/B testing. The next sections describe each of
the components of our approach in detail.
3.3 Agent Architecture
In order to adapt a more conventional RL setting and to be able
to use state-of-the-art reinforcement learning algorithms, we de-
signed our agent based on an actor-critic architecture (see Figure 2).
The actor model represents a stochastic bidding policy and defines
an action distribution ğœ‹(Â·|ğ‘†ğ‘¡)conditioned on the state representa-
tion vector ğ‘†ğ‘¡. The critic model approximates the value function
ğ‘„(ğ‘†ğ‘¡,ğ´ğ‘¡)conditioned on a state-action pair (ğ‘†ğ‘¡,ğ´ğ‘¡). To convert a
deterministic base policy ğ¹ğ‘¤to a stochastic policy, we defined the
512, ReLU512, ReLU
actor state representation512, ReLU512, ReLU
critic state representation,
actionactor criticFigure 2: The proposed actor-critic agent architecture. The
actor consists of two components: a deterministic production
base policy that parameterizes the agentâ€™s policy mean, and
an MLP neural network that parameterizes the policy vari-
ance. The critic is parameterized by a separate MLP neural
network.
agentâ€™s policy as a Gaussian distribution with a mean parameterized
byğ¹ğ‘¤, and a variance parameterized by a neural network:
ğœ‹ğ‘¤,ğœ™(Â·|ğ‘†ğ‘¡)=N(ğ¹ğ‘¤(ğ‘†ğ‘¡),ğœ2
ğœ™(ğ‘†ğ‘¡)), (1)
whereğ‘¤is a vector of parameters of the base policy, and ğœ™is a
vector of weights of the neural network representing the variance.
Once the agent is trained, only the base policy with optimized
parameters ğ¹ğ‘¤âˆ—is deployed to production (Figure 1). This design
allows to avoid additional infrastructure and implementation costs
and mitigates the safety risks of deploying a black box type model
such as a neural network, to a production system. In this design,
the actor is restricted to using the state representation features
that are accepted by the base policy ğ¹ğ‘¤. The critic, however, is not
restricted by production constraints, since it is parameterized by a
neural network and is not deployed to production. Consequently,
the critic can use a larger set of input features compared to the
actor, and we have expanded the state representation features set
for the critic to increase the accuracy of critic predictions. A more
accurate critic model results in more accurate gradient estimates
with respect to the policy parameters. Therefore, we used different
state representations for the actor and the critic models. In practice,
we used two-layer MLP neural networks with 512 neurons in each
hidden layer and ReLU non-linearities to represent both, the policy
varianceğœ2
ğœ™and the critic models (Figure 2).
3.4 Behavior Policy
Reinforcement learning requires exploration in order to be able to
discover new policies. In order to facilitate such exploration, we
added a truncated multiplicative Gaussian noise to the deterministic
base policy ğ¹ğ‘¤(ğ‘†ğ‘¡). Our behavior policy ğœ‹ğ›½
ğ‘¤is therefore defined
according to:
ğœ‹ğ›½
ğ‘¤(ğ‘†ğ‘¡)=ğ¹ğ‘¤(ğ‘†ğ‘¡)Â·(1+clip(ğœ€ğ‘¡,âˆ’0.5,0.5)), (2)
ğœ€ğ‘¡âˆ¼N( 0,ğœ2
ğ›½),
whereğœ2
ğ›½is the distribution variance. The reason for truncating
the noise is to exclude extreme bid values that could have a strong
5254Offline Reinforcement Learning for Optimizing Production Bidding Policies KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
negative impact on a production system. The multiplicative noise
of the form presented above is equivalent to the (truncated) con-
ventional additive Gaussian noise with the variance scaled by the
policy mean ğ¹ğ‘¤(ğ‘†ğ‘¡):
ğœ‹ğ›½
ğ‘¤(ğ‘†ğ‘¡)=N(ğ¹ğ‘¤(ğ‘†ğ‘¡),ğ¹ğ‘¤(ğ‘†ğ‘¡)Â·ğœ2
ğ›½) (3)
The rationale for this parameterization was that the appropriate bid
values, depending on factors like the campaignâ€™s remaining budget
and remaining opportunity size, can be widely different in scale,
varying by several orders of magnitude. Additive exploration noise
with a fixed variance would be either too disruptive in states with
small bids, or too negligible in states with large bids. Scaling the
variance by the value of the bid itself ensures that we explore in a
reasonable region relative to the bid value.
3.5 Learning Algorithm
In this work, we used a modified version of a Conservative Q-
Learning (CQL) algorithm [ 22], which is currently one of the state-
of-the-art offline RL algorithms. The general idea of the algorithm
is to penalize the Q-function values on out-of-distribution (OOD)
actions to negate the over-estimation bias, therefore removing the
incentive for the policy to learn to favor such actions. For contin-
uous control domains, such as in bidding, the CQL algorithm is
based on a Soft Actor-Critic (SAC) algorithm, which is one of the
state-of-the-art in online RL [ 14]. Below we provide brief introduc-
tions to both of these algorithms and describe the changes weâ€™ve
made to adapt CQL to the bidding setting.
Soft Actor-Critic (SAC). The SAC algorithm [ 14] is based on the
idea of entropy-regularized Reinforcement Learning where the stan-
dard RL objective of maximizing cumulative reward is augmented
with an entropy term:
ğœ‹âˆ—=arg maxğœ‹âˆ‘ï¸
ğ‘¡E(ğ‘ ğ‘¡,ğ‘ğ‘¡)âˆ¼ğœŒğœ‹[ğ‘Ÿ(ğ‘ ğ‘¡,ğ‘ğ‘¡)+ğ›¼ğ»(ğœ‹(Â·|ğ‘ ğ‘¡))],(4)
whereğ»(ğœ‹(Â·|ğ‘ ğ‘¡))is an entropy of the policy ğœ‹,ğ›¼is a "temperature"
parameter that controls the relative weight of the entropy term in
the objective, ğœŒğœ‹is a trajectory distribution induced by the policy
ğœ‹. This objective incentivizes the policy to explore more broadly
and is better suited for learning multi-modal distributions where
multiple actions have similar values.
SAC is based on an actor-critic architecture where the actor
model learns the policy and the critic model learns to approximate
the corresponding Q-function ğ‘„ğœ‹(ğ‘ ,ğ‘). More specifically, the critic
model is trained by minimizing the soft Bellman operator error
ğ½ğ‘„(ğœƒ)=1
2E(ğ‘ ,ğ‘)âˆˆğ·
(ğ‘„ğœƒ(ğ‘ ,ğ‘)
âˆ’(ğ‘Ÿ(ğ‘ ,ğ‘)+ğ›¾ğ¸ğ‘ â€²âˆ¼ğ‘ğ‘‰ğœƒ(ğ‘ â€²)))2
,(5)
whereğœƒare the critic parameters, typically the weights in a neural
network,ğ›¾is a discount factor, ğ‘is an environment state transition
probability, ğ‘‰ğœƒ(ğ‘ )is a state-value function, ğ·is a replay buffer.
The actor model is trained by minimizing the KL divergence
between the policy and the exponential of the Q-function, which
reduces to the following objective
ğ½ğœ‹(ğœ™)=Eğ‘ âˆ¼ğ·h
Eğ‘âˆ¼ğœ‹ğœ™[ğ›¼log(ğœ‹ğœ™(ğ‘|ğ‘ ))âˆ’ğ‘„ğœƒ(ğ‘ ,ğ‘)]i
, (6)
whereğœ™are the policy parameters, ğ·is a replay buffer.Conservative Q-learning (CQL). CQL(ğ») variant [ 22] of the CQL
algorithm extends the SAC loss (5) to the offline setting by adding
a penalty term to the Bellman operator error objective:
min
ğœƒğ›¼Eğ‘ âˆ¼ğ·"
logâˆ‘ï¸
ğ‘exp(ğ‘„ğœƒ(ğ‘ ,ğ‘))âˆ’ğ¸ğ‘âˆ¼Ë†ğœ‹ğ›½(ğ‘|ğ‘ )[ğ‘„ğœƒ(ğ‘ ,ğ‘)]#
+1
2E(ğ‘ ,ğ‘)âˆˆğ·
(ğ‘„ğœƒ(ğ‘ ,ğ‘)âˆ’(ğ‘Ÿ(ğ‘ ,ğ‘)+ğ›¾ğ¸ğ‘ â€²âˆ¼ğ‘ğ‘‰ğœƒ(ğ‘ â€²)))2
,(7)
whereğ·is an offline dataset, Ë†ğœ‹ğ›½is a behavior policy that generated
the data. The terms in red highlight the penalty terms introduced
in the CQL algorithm.
The first term in the penalty penalizes the Q-values of all actions
in the action space, while the second term counters the effect of the
penalty on the in-distribution actions sampled from the behavior
policy. The combination of these two terms, therefore, attempts to
penalize the Q-values of out-of-distribution actions. In the original
algorithm, the first penalty term
Eğ‘ âˆ¼ğ·[logâˆ‘ï¸
ğ‘exp(ğ‘„ğœƒ(ğ‘ ,ğ‘))] (8)
is estimated by sampling actions from a uniform distribution over
the action space, and the second term
Eğ‘ âˆ¼ğ·,ğ‘âˆ¼Ë†ğœ‹ğ›½(Â·|ğ‘ )[ğ‘„ğœƒ(ğ‘ ,ğ‘)] (9)
is estimated by using actions recorded in the dataset.
We used Tinashou library for a base implementation of the CQL
algorithm [ 37]. To achieve stable learning performance in the bid-
ding environment, we added several modifications to this base
implementation, which we describe below.
(1)The first modification we made is in estimating the first
penalty term (8). In bidding, the optimal action values in
different states can differ by several orders of magnitude
(since the bids are strongly influenced by campaign budgets),
and in order to perform well, both, behavior and learned
policy follow distributions that by design are quite narrow
around these optimal actions (see e.g. (2) for details). In this
setting, sampling actions uniformly across the entire action
space in each state and using them to evaluate the penalty
is wasteful, as the vast majority of those actions will never
be taken in that state. Instead, we sample from the state-
dependent uniform distribution over the interval
ğ¼ğ›½(ğ‘ )=[(1âˆ’ğœ€)ğ¹ğ‘¤(ğ‘ ),(1+ğœ€)ğ¹ğ‘¤(ğ‘ )], (10)
where, by design, ğ¹ğ‘¤(ğ‘ )is the deterministic mean of the
behavior policy. The parameter ğœ€was chosen to match the
clipping parameter in the multiplicative exploration noise
applied to the behavior policy during the data collection
(2). Sampling from (10) ensures that sampled actions stay
in the relevant interval for each state. The ğ¼ğ›½(ğ‘ )interval
covers all actions produced by the behavior policy at each
state, and hence we can think of it as representing an entire
(state-dependent) action space.
(2)The second modification we made is in estimating the sec-
ond penalty term (9). In our case, we have access to the
behavior policy that generated the data (i.e. the default pro-
duction bidding policy with exploration noise), which often
is not available in offline RL setting. Instead of using actions
5255KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dmytro Korenkevych et al.
0 200000 400000 600000 800000
gradient steps2.0
1.5
1.0
0.5
0.00.51.01.5performance gain, %
CQL alpha = 0.0
CQL alpha = 0.01
CQL alpha = 0.1
CQL alpha = 1.0
Figure 3: CQL agent learning curves in the simulator at different values of CQL penalty weight parameter. Error bars represent
95%confidence intervals computed from 1,000 evaluation episodes at each checkpoint.
recorded in the data, we can leverage the access to the be-
havior policy and sample ğ¾actions at each state ğ‘ directly
fromğœ‹ğ›½(Â·|ğ‘ ). Using these sampled actions to estimate the
expectation in (9) reduces the variance of the estimate, and
also reduces the risk of overfitting from re-using the actions
from the dataset.
(3)We removed the entropy bonus term from the value func-
tion objective. The entropy bonus is useful in the online RL
setting, as it promotes exploration. In our setting, however,
the agent is trained entirely offline from the data collected
by a fixed behavior policy, and maximizing policy entropy
in this case is not meaningful.
(4)Since we initialize the agent to match the base policy used to
collect the data ğœ‹ğ›½
ğ‘¤(Â·|ğ‘ ), we can pre-train the critic network
from the collected data to have critic predictions match the
actor performance at the start of the training process. Specifi-
cally, we pre-train the critic for 300,000 gradient steps before
starting to train the actor. Empirically, we found that the
critic loss largely converges at around that point in training.
The resulting critic model matches better the true policy ğ‘„-
function at the beginning of training and allows to compute
more accurate gradients with respect to policy parameters.
4 EXPERIMENTAL RESULTS
To validate our approach, we performed a series of empirical ex-
periments in both, a simulated bidding environment, and a real
production system. We defined the base policy ğ¹ğ‘¤to be the heuris-
tic bidding policy that is running in our production system. In a
nutshell, it is a piece-wise polynomial function in input featureswith a couple dozen scalar parameters. We initialized our agent
actor parameters to default production values. We used a behavior
policy defined in (3) to collect the training dataset for subsequent
offline learning. We have experimented with various values of the
varianceğœ2
ğ›½and found that the value ğœğ›½=0.05resulted inâˆ¼0.5%
drop in performance compared to the default production policy
when deployed online. This was close to the limit at which it was
still acceptable to collect the data over the course of two weeks
even on a fraction of traffic. A larger variance would incur costs
that wouldnâ€™t be acceptable when collecting a substantial amount
of data.
4.1 Simulation setup
To validate the approach and perform a hyper-parameters search,
we trained our bidding agent in a campaign bidding simulator. The
simulator was developed internally by our product team for testing
new variants of production bidding policies, and therefore it has
a reasonably high fidelity to the real environment. The simulator
allows us to customize campaign configurations, such as budget
size, campaign duration, audience size, opportunities distribution
over the campaign duration, etc.
We designed a set of 100 campaign configurations with ran-
domized opportunity distribution density functions, audience sizes,
and budget sizes. The audience size determines the total number
of opportunities (simulated users) available to a given campaign
throughout its lifetime, and the budget determines the total amount
of money a campaign can spend in one episode. Each campaign
runs for one day of simulated time, and the bidding agent makes
decisions every simulated minute. An episode is terminated when
5256Offline Reinforcement Learning for Optimizing Production Bidding Policies KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
either the campaign spends all its budget, or it runs for a full day
of simulated time. Each episode, therefore, consists of up to 1,440
time steps. At the beginning of each episode, a campaign configu-
ration is selected from the available configurations set at random,
and the environment state is initialized with the corresponding
parameters. A single agent is trained on the data from all available
configurations. At each time step, the campaign participates in sim-
ulated auctions where it competes over impressions against other
simulated campaigns. We have defined the total value of simulated
conversions, received at each time step, as a reward function in this
simulated environment. To train the agent, we have generated an
offline dataset containing âˆ¼400,000episodes (âˆ¼600million time
steps) by running the base policy with Gaussian exploration noise
in the simulator (see Section 3.4 for the details).
A major challenge in applying offline RL to production systems is
that we do not have the means to evaluate the agent apart from run-
ning A/B tests in production, which are costly and time-consuming.
In particular, we do not have the option to pick the best model
checkpoint across the training run, since we do not know which
checkpoint is the best. Consequently, we donâ€™t know for how many
gradient steps we should train the model before committing it to
an A/B test. Hence, the main questions we wanted to answer with
our simulated experiments are:
â€¢Can our approach improve upon the base policy?
â€¢How consistent and stable is the learning process?
â€¢Can we pick reliably a model checkpoint with a positive
performance gain compared to the base policy without eval-
uating multiple checkpoints in expensive A/B tests?
To answer these questions, we trained a set of agents with different
values of CQL penalty weight ğ›¼in the simulator and evaluated
each agent at multiple checkpoints during the training process by
running 10 evaluation episodes per each of 100 campaign config-
urations (1000 episodes in total per checkpoint). The value of the
penalty weight parameter determines how conservative the learn-
ing process is in terms of deviating from the behavior policy, and it
directly affects its stability. Figure 3 shows the resulting learning
curves. At small values of ğ›¼, the penalty term was not sufficient
to mitigate the over-estimation bias on out-of-distribution actions,
and it resulted in unstable learning and abrupt changes in perfor-
mance. In contrast, a large value of ğ›¼=1.0was too restrictive
and resulted in very marginal changes in the policy compared to
the baseline. At the intermediate value of ğ›¼=0.1, however, the
agent produced a consistent stable performance, exhibiting positive
performance gains along the entire learning process. While smaller
values ofğ›¼did result in higher peaks in performance, they also
produced significant performance drops. On the other hand, after
400,000 gradient steps the performance gain of the ğ›¼=0.1agent
consistently exceeded the +0.5%mark, and any model checkpoint
past that point would be a suitable candidate for an A/B test. We
used the value ğ›¼=0.1in our subsequent experiments in both,
simulation and a real production environment.
In our bidding environment, the observation data contains only
highly aggregated high-level information about the environment
states, such as total fraction of budget spent, total fraction of op-
portunities passed, average past bid value, etc. We wanted to verify
0 5 10 15 20 25
campaigns01020304050return values
predicted q-values
empirical returnFigure 4: Learned ğ‘„-function predictions at the start of an
episode against empirical discounted returns across 25 differ-
ent campaign configurations. To estimate empirical returns,
we ran the trained policy in the environment for 30 episodes
for each campaign and report the mean return values. The
error bars represent standard deviations. The campaigns are
ordered left to right based on the campaign budget.
that the neural network-based critic model in our hybrid agent ar-
chitecture is able to predict the returns with a reasonable accuracy
based on this information. For a set of 25 different campaign config-
urations with varied budget sizes, we generated new episodes with
the trained agent and computed critic predictions at the initial state
(ğ‘ 0,ğ‘âˆ¼ğœ‹ğ‘¤,ğœ™(Â·,ğ‘ 0))in each episode. We compared these predictions
with the total empirical discounted returns in those episodes. As
can be seen in Figure 4, although the critic tends to over-estimate
the returns in some cases, particularly on small budget campaigns,
in general, the critic predictions are quite accurate and are able
to capture the difference in performance between campaigns with
different budgets. Note, that since new episodes were generated for
this experiment, the data used to compute results on the figure was
not part of the training dataset.
We also wanted to verify that the critic network has learned to
predict returns accurately at the intermediate states and to cap-
ture the dependence between the return, remaining budget, and
remaining time in the campaign. Figure 5 shows predicted q-values
and empirical returns evaluated at each time step for two different
campaigns with different budgets. The curves are averaged across
10 episodes for each campaign, the solid lines represent the mean
and the shaded areas represent the standard deviations. As can be
seen, the critic model is able to accurately predict future returns at
intermediate states within an episode.
Overall, these results indicate that our approach indeed can
reliably improve the base policy, and, under certain algorithm hyper-
parameter values, results in a stable and consistent learning process.
Having verified these hypotheses in the simulator, we applied our
approach to a real production system. The next section describes
the details of our production setup and the results.
5257KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dmytro Korenkevych et al.
0 200 400 600 800 1000 1200
timestep02468returncampaign budget: 4.28
predicted q-values
empirical return
0 200 400 600 800 1000 1200
timestep01020304050returncampaign budget: 29.13
predicted q-values
empirical return
Figure 5: Learned ğ‘„-function predictions and empirical dis-
counted returns-to-go, evaluated at each time step, averaged
across 10 episodes for each campaign. Solid lines represent
mean values and the shaded areas represent standard devia-
tions for each curve.
4.2 Production system
We logged production data from âˆ¼200,000week-long ad cam-
paigns, which corresponds to âˆ¼1.2billion time steps. One time
step represents one bidding decision (computed every minute for
each campaign). The data was generated by running the behavior
policy defined in equation (2) directly in the production environ-
ment on a small percentage of traffic. For each recorded campaign,
full episodes with all intermediate time steps were logged. The
episodes terminated in one of three conditions: the campaign de-
pleted its budget; the campaign ran for a full duration of one week;
the advertiser decided to end the campaign early.
We trained the CQL agent described in Sections 3.3 and 3.5 on the
collected production data. We conducted a hyper-parameters search
in the simulator and chose hyper-parameter values that resulted in
the most stable learning performance (see Table 1). We deployed
Table 1: Learning algorithm hyper-parameters values.
Parameter Value
number of gradient steps during training 600,000
batch size 20,000
critic learning rate 3e-4
actor learning rate 3e-5
discount factor ğ›¾ 0.9998
behavior policy st.d. ğœğ›½ 0.05
CQL penalty weight ğ›¼ 0.1
exploration noise clipping range (-0.5, 0.5)
critic network hidden sizes (512, 512)
variance network hidden sizes (512, 512)
target network update rate ğœ 0.01
number of samples ğ¾to estimate CQL penalty terms 50
the trained parameters ğ‘¤âˆ—of the base policy part in our hybrid
agent architecture and evaluated them online in a series of A/B
tests. While we were restricted in the number of experiments and
ablations we could conduct by the availability of A/B test slots that
are shared between many teams, we were able to test the final model
in two large-scale A/B tests. Similarly to our results in simulation,
we observed statistically significant positive performance metricTable 2: A/B tests results in a real production system compar-
ing trained RL policy against the base production bidding
policy. The performance metric measured in our system is
a proxy to the aggregated value of impressions described in
Section 3.1.
Test type Perf. metric gain Impressions
pre-test +0.17%, (+0.05%, +0.3%) 95%CI âˆ¼50billion
back-test +0.16%, (+0.03%, +0.27%) 95%CI âˆ¼50billion
gains compared to the control production bidding base policy. Each
A/B test ran for one week and compared the policy learned by RL
agent (test variant) to the original base policy (control variant).
Table 1 shows the results of the two tests, a pre-test (a typical A/B
test where a new model being tested serves as a test version) and
a back-test (performed after a new model launch, where the new
model serves as a control, and an old baseline now serves as a
test version). In both tests, the RL-tuned policy showed statistically
significant gains in the main performance metric and revenue. Based
on these tests, our trained model was deployed globally to our
production bidding platform.
5 CONCLUSIONS
In this work, we proposed an offline reinforcement learning ap-
proach to training parameters of production bidding policies. Auto-
bidding is a natural sequential decision problem and can be for-
mulated as a Markov decision process. Our approach allows for
optimizing existing production policies from real data by directly
maximizing the performance metric of interest. Our approach does
not incur additional infrastructure, safety, or explainability costs,
as it directly optimizes parameters of existing production routines
without replacing them with black box-style models like neural
networks. To validate the approach, we conducted learning experi-
ments in both, simulated and real production bidding environments.
In the simulation, we showed that our approach exhibits a consis-
tent and stable learning performance, reliably improving upon a
base bidding policy. A series of A/B tests in a real production envi-
ronment demonstrated statistically significant performance gains
of the trained policy compared to the base policy. Based on these
results we conclude that the proposed approach is a viable and
practical tool for optimizing control policies under production envi-
ronment constraints. We note that the performance gains reported
in production were obtained compared to a highly tuned and suc-
cessful baseline production policy. In new applications where strong
baselines have not been yet developed, our approach for optimizing
policy parameters can result in higher gains.
REFERENCES
[1]Kareem Amin, Michael Kearns, Peter Key, and Anton Schwaighofer. 2012. Budget
optimization for sponsored search: Censored learning in mdps. arXiv preprint
arXiv:1210.4847 (2012).
[2]OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz,
Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell,
Alex Ray, et al .2020. Learning dexterous in-hand manipulation. The International
Journal of Robotics Research 39, 1 (2020), 3â€“20.
[3]Ashwinkumar Badanidiyuru Varadaraja, Zhe Feng, Tianxi Li, and Haifeng Xu.
2022. Incrementality Bidding via Reinforcement Learning under Mixed and
5258Offline Reinforcement Learning for Optimizing Production Bidding Policies KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Delayed Rewards. Advances in Neural Information Processing Systems 35 (2022),
2142â€“2153.
[4]Santiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni, and
Jon Schneider. 2019. Contextual bandits with cross-learning. Advances in Neural
Information Processing Systems 32 (2019).
[5]Stuart Bennett. 1993. Development of the PID controller. IEEE Control Systems
Magazine 13, 6 (1993), 58â€“62.
[6]Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and
Defeng Guo. 2017. Real-time bidding by reinforcement learning in display adver-
tising. In Proceedings of the tenth ACM international conference on web search and
data mining. 661â€“670.
[7]Ignacio Carlucho, Mariano De Paula, Sebastian A Villar, and Gerardo G Acosta.
2017. Incremental Q-learning strategy for adaptive PID control of mobile robots.
Expert Systems with Applications 80 (2017), 183â€“199.
[8]Oguzhan Dogru, Kirubakaran Velswamy, Fadi Ibrahim, Yuqi Wu, Arun Senthil
Sundaramoorthy, Biao Huang, Shu Xu, Mark Nixon, and Noel Bell. 2022. Rein-
forcement learning approach to autonomous PID tuning. Computers & Chemical
Engineering 161 (2022), 107760.
[9]Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. 2007. Internet
advertising and the generalized second-price auction: Selling billions of dollars
worth of keywords. American economic review 97, 1 (2007), 242â€“259.
[10] Benjamin Eysenbach, Matthieu Geist, Sergey Levine, and Ruslan Salakhutdi-
nov. 2023. A Connection between One-Step RL and Critic Regularization in
Reinforcement Learning. (2023).
[11] Zhe Feng, Swati Padmanabhan, and Di Wang. 2023. Online Bidding Algorithms
for Return-on-Spend Constrained Advertisers. In Proceedings of the ACM Web
Conference 2023. 3550â€“3560.
[12] Zhe Feng, Chara Podimata, and Vasilis Syrgkanis. 2018. Learning to bid without
knowing your value. In Proceedings of the 2018 ACM Conference on Economics
and Computation. 505â€“522.
[13] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep rein-
forcement learning without exploration. In International conference on machine
learning. PMLR, 2052â€“2062.
[14] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon
Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al .2018.
Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905
(2018).
[15] Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, and
Xiaoqiang Zhu. 2021. A unified solution to constrained bidding in online display
advertising. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 2993â€“3001.
[16] Kartik Hosanagar and Vadim Cherepanov. 2008. Optimal bidding in stochastic
budget constrained slot auctions. In Proceedings of the 9th ACM Conference on
Electronic Commerce. 20â€“20.
[17] Sham Kakade and John Langford. 2002. Approximately optimal approximate
reinforcement learning. In Proceedings of the Nineteenth International Conference
on Machine Learning. 267â€“274.
[18] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric
Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al .
2018. Scalable deep reinforcement learning for vision-based robotic manipulation.
InConference on Robot Learning. PMLR, 651â€“673.
[19] Dmytro Korenkevych, A Rupam Mahmood, Gautham Vasan, and James Bergstra.
2019. Autoregressive policies for continuous control deep reinforcement learning.
arXiv preprint arXiv:1903.11524 (2019).
[20] Vijay Krishna. 2009. Auction theory. Academic press.
[21] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. 2019.
Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in
Neural Information Processing Systems 32 (2019).
[22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conserva-
tive q-learning for offline reinforcement learning. Advances in Neural Information
Processing Systems 33 (2020), 1179â€“1191.
[23] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 2016. End-to-end
training of deep visuomotor policies. The Journal of Machine Learning Research
17, 1 (2016), 1334â€“1373.
[24] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline rein-
forcement learning: Tutorial, review, and perspectives on open problems. arXiv
preprint arXiv:2005.01643 (2020).[25] Brendan Lucier, Sarath Pattathil, Aleksandrs Slivkins, and Mengxiao Zhang.
2023. Autobidders with budget and roi constraints: Efficiency, regret, and pacing
dynamics. arXiv preprint arXiv:2301.13306 (2023).
[26] A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and
James Bergstra. 2018. Benchmarking reinforcement learning algorithms on
real-world robots. In Conference on robot learning. PMLR, 561â€“591.
[27] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-
othy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning. In International conference
on machine learning. PMLR, 1928â€“1937.
[28] Zhiyu Mou, Yusen Huo, Rongquan Bai, Mingzhou Xie, Chuan Yu, Jian Xu, and
Bo Zheng. 2022. Sustainable Online Reinforcement Learning for Auto-bidding.
Advances in Neural Information Processing Systems 35 (2022), 2651â€“2663.
[29] Claudia Perlich, Brian Dalessandro, Rod Hook, Ori Stitelman, Troy Raeder, and
Foster Provost. 2012. Bid optimizing and inventory scoring in targeted online
advertising. In Proceedings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining. 804â€“812.
[30] Mostafa Sedighizadeh and Alireza Rezazadeh. 2008. Adaptive PID controller
based on reinforcement learning for wind turbine control. In Proceedings of world
academy of science, engineering and technology, Vol. 27. Citeseer, 257â€“262.
[31] William John Shipman. 2021. Learning to Tune a Class of Controllers with Deep
Reinforcement Learning. Minerals 11, 9 (2021), 989.
[32] Qifeng Sun, Chengze Du, Youxiang Duan, Hui Ren, and Hongqiang Li. 2021. De-
sign and application of adaptive PID controller based on asynchronous advantage
actorâ€“critic learning method. Wireless Networks 27 (2021), 3537â€“3547.
[33] Richard S Sutton, Andrew G Barto, et al .1998. Introduction to reinforcement
learning. Vol. 135. MIT press Cambridge.
[34] Haozhe Wang, Chao Du, Panyan Fang, Li He, Liang Wang, and Bo Zheng.
2023. Adversarial Constrained Bidding via Minimax Regret Optimization with
Causality-Aware Reinforcement Learning. arXiv preprint arXiv:2306.07106 (2023).
[35] Jun Wang and Shuai Yuan. 2015. Real-time bidding: A new frontier of com-
putational advertising research. In Proceedings of the Eighth ACM International
Conference on Web Search and Data Mining. 415â€“416.
[36] Jonathan Weed, Vianney Perchet, and Philippe Rigollet. 2016. Online learning in
repeated auctions. In Conference on Learning Theory. PMLR, 1562â€“1583.
[37] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao
Zhang, Yi Su, Hang Su, and Jun Zhu. 2022. Tianshou: A Highly Modularized
Deep Reinforcement Learning Library. Journal of Machine Learning Research 23,
267 (2022), 1â€“6. http://jmlr.org/papers/v23/21-1127.html
[38] Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu,
and Kun Gai. 2018. Budget constrained bidding by model-free reinforcement
learning in display advertising. In Proceedings of the 27th ACM International
Conference on Information and Knowledge Management. 1443â€“1451.
[39] Xun Yang, Yasong Li, Hao Wang, Di Wu, Qing Tan, Jian Xu, and Kun Gai. 2019.
Bid optimization by multivariable control in display advertising. In Proceedings
of the 25th ACM SIGKDD international conference on knowledge discovery & data
mining. 1966â€“1974.
[40] Shuai Yuan, Jun Wang, and Xiaoxue Zhao. 2013. Real-time bidding for online
advertising: measurement and analysis. In Proceedings of the seventh international
workshop on data mining for online advertising. 1â€“8.
[41] Weinan Zhang, Yifei Rong, Jun Wang, Tianchi Zhu, and Xiaofan Wang. 2016.
Feedback control of real-time display advertising. In Proceedings of the Ninth
ACM International Conference on Web Search and Data Mining. 407â€“416.
[42] Weinan Zhang and Jun Wang. 2015. Statistical arbitrage mining for display
advertising. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. 1465â€“1474.
[43] Weinan Zhang, Shuai Yuan, and Jun Wang. 2014. Optimal real-time bidding
for display advertising. In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. 1077â€“1086.
[44] Jun Zhao, Guang Qiu, Ziyu Guan, Wei Zhao, and Xiaofei He. 2018. Deep rein-
forcement learning for sponsored search real-time bidding. In Proceedings of the
24th ACM SIGKDD international conference on knowledge discovery & data mining .
1021â€“1030.
[45] Haolin Zhou, Chaoqi Yang, Xiaofeng Gao, Qiong Chen, Gongshen Liu, and Guihai
Chen. 2022. Multi-Objective Actor-Critics for Real-Time Bidding in Display
Advertising. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases. Springer, 20â€“37.
5259