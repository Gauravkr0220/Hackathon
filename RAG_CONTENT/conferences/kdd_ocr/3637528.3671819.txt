Approximate Matrix Multiplication over Sliding Windows
Ziqi Yaoâˆ—
East China Normal University
Shanghai, China
51265902073@stu.ecnu.edu.cnLianzhi Liâˆ—
East China Normal University
Shanghai, China
51265902102@stu.ecnu.edu.cnMingsong Chen
East China Normal University
Shanghai, China
mschen@sei.ecnu.edu.cn
Xian Wei
East China Normal University
Shanghai, China
xwei@sei.ecnu.edu.cnCheng Chenâ€ 
East China Normal University
Shanghai, China
chchen@sei.ecnu.edu.cn
ABSTRACT
Large-scale streaming matrix multiplication is very common in
various applications, sparking significant interest in develop effi-
cient algorithms for approximate matrix multiplication (AMM) over
streams. In addition, many practical scenarios require to process
time-sensitive data and aim to compute matrix multiplication for
most recent columns of the data matrices rather than the entire ma-
trices, which motivated us to study efficient AMM algorithms over
sliding windows. In this paper, we present two novel deterministic
algorithms for this problem and provide corresponding error guar-
antees. We further reduce the space and time costs of our methods
for sparse matrices by performing an approximate singular value
decomposition which can utilize the sparsity of matrices. Exten-
sive experimental results on both synthetic and real-world datasets
validate our theoretical analysis and highlight the efficiency of our
methods.
CCS CONCEPTS
â€¢Mathematics of computing â†’Computations on matrices.
KEYWORDS
Streaming data, Sliding window, Approximate Matrix Multiplica-
tion
ACM Reference Format:
Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei, and Cheng Chen. 2024.
Approximate Matrix Multiplication over Sliding Windows. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671819
âˆ—Both authors contributed equally to this research.
â€ Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718191 INTRODUCTION
Matrix multiplication is one of the most fundamental subroutines
in machine learning and data mining. It has been applied to many
machine learning tasks such as regression [ 26,35], clustering [ 8,12],
online learning [ 1,14,21,33] and canonical component analy-
sis [16,36]. The ever-increasing size of data matrices poses great
challenges to the computational efficiency of matrix multiplica-
tion, and reinforces efforts to design efficient approximate matrix
multiplication (AMM) methods.
Many recent works [ 13,20,24,32,35,36] consider AMM in
the streaming model, where columns of the data matrices arrive
sequentially. Among these methods, Co-occurring Directions (COD)
and its variants enjoy smaller approximation errors and higher
robustness. Despite the success of COD in streaming AMM, it does
not fully address the situation where the timeliness of data is very
important, i.e., people are more interested in the recent data rather
than the outdated data. Such setting is very common in many real-
world applications. For example, in social media analysis, each
column of the data matrix corresponds to a document (e.g. the
content of a Twitter post) along with a corresponding timestamp.
Usually, advertisers are more interested in the tweets posted in
the most recent week or month. In user behavior analysis, the
correlation between user searches and ad clicks in the most recent
period are more attractive to researchers.
In these time-sensitive applications, the sliding window model is
more appropriate than the unbounded streaming model [ 3,6,7,23,
27,34]. The sliding window model [ 10] only considers a window
of sizeğ‘, i.e., theğ‘most recent columns of the data matrices.
We call the data in the window active data, in contrast to the so-
called expired data. In this paper, we study AMM algorithms over
sliding windows. Specifically, We have two streaming matrices
XâˆˆRğ‘šğ‘¥Ã—ğ‘›andYâˆˆRğ‘šğ‘¦Ã—ğ‘›. The algorithm sequentially receives
the columns of matrices Xand Yand maintains two low-rank
sketches AâˆˆRğ‘šğ‘¥Ã—â„“andBâˆˆRğ‘šğ‘¦Ã—â„“, where the number of columns
â„“is significantly smaller than ğ‘›. The objective of the algorithm
is to guarantee that ABğ‘‡is a good approximation of XYğ‘‡(i.e.,
ABğ‘‡â‰ˆXYğ‘‡) while minimizing the algorithmâ€™s space and time
cost. Though streaming AMM has been widely studied, few works
explore the AMM over sliding windows. Generally, computing
matrix sketching in the sliding window model is harder that in the
streaming model. Wei et al . [34] showed that if X=Y, maintaining
XYğ‘‡only requires ğ‘‚(ğ‘šğ‘¥ğ‘šğ‘¦)time andğ‘‚(ğ‘šğ‘¥ğ‘šğ‘¦)space but tracking
XYğ‘‡exactly in the sliding window models requires the algorithm
3896
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
to store all columns in the window. Thus developing algorithms for
AMM in the sliding window model requires novel techniques.
In this paper, we provide two algorithms for AMM over sliding
windows, called Exponential Histogram Co-occurring Directions
(EH-COD) and Dyadic Interval Co-occurring Directions (DI-COD).
The EH-COD algorithm, based on the idea of Exponential His-
togram [ 10], stores stream data in blocks and maintains an ap-
proximation of the window content with space much smaller than
the window size through regular merging. The DI-COD algorithm,
based on the dyadic interval structure [ 2], constructs a tree-like
structure from the stream data blocks, and concatenate sketches
from each layer to generate the final approximation matrix. The DI-
COD algorithm has better approximation quality than the EH-COD
algorithm when the maximum column norm of the data matrix is
small. We further improve the time and space costs of our meth-
ods for sparse matrices by leveraging the subspace power method
(SPM) [ 25,35] which can utilize the sparsity of matrices. Our main
contributions can be summarized as follows:
â€¢We develop two efficient algorithms for AMM in the sliding
window models. We provide error analysis for both algo-
rithms as well as their time and space complexities.
â€¢We propose improved algorithm for approximate sparse ma-
trix multiplication in the sliding window models.
â€¢We empirically study the performance of proposed methods
on both synthetic and real-world datasets. The experimental
results validate the effectiveness of the proposed approaches.
Paper Organization. The rest of the paper is organized as follows.
In Section 2, we introduce previous works related to this paper.
Then we define the notation used in this paper and introduce the
Co-occurring directions algorithm in Section 3. In Section 4, we
presents our EH-COD and DI-COD algorithms and show their
approximation error bound and complexity analysis. In Section
5, we describe the improved variants of our methods for sparse
matrices. We present our empirical results in Section 6 and provide
conclusions in Section 7.
2 RELATED WORKS
In this section, we first review prior works on approximate matrix
multiplication. Then we introduce sketching methods in the sliding
window model.
2.1 Approximate Matrix Multiplication
The AMM problem has been widely studied in recent decades. Ex-
isting approaches to the AMM problem can be broadly categorized
into two types: randomized algorithms, exemplified by random
sampling [ 13] and random projection [ 9,22,31], and deterministic
algorithms, represented by FD-AMM [ 36] and COD [ 24]. Random-
ized algorithms exhibit favorable time complexity and come with
theoretical guarantees, yet these methods require larger sketch
size to achieve same approximation error as deterministic methods.
The deterministic algorithms leverage singular value decomposi-
tion (SVD) to generate approximate matrices and usually enjoys
smaller approximation errors and better adaptability to the stream-
ing setting. The FD-AMM [ 36] algorithm employs the Frequent
Directions (FD) [ 15,18] to process the stacked matrix Z=[X;Y]
column by column, where XandYare the input matrices. The CODalgorithm computes the correlation sketch by shrinking the singu-
lar values of XandYin each iteration. Compared with FD-AMM,
the COD algorithm [ 24] usually have better empirical performance.
Recently, Wan and Zhang [32] and Luo et al . [20] proposed sparse
variants of COD algorithm, which utilize the sparsity of the input
matrices to improve the time and space costs. Blalock and Guttag
[5]proposed a learning-augmented methods for AMM problem
and achieves better speed-quality tradeoff. However, their method
requires training data and is difficult to adapt to the streaming
or sliding window model. To the best of our knowledge, none of
previous works adapt well to AMM in the sliding window models.
2.2 Sliding Window Algorithms
The sliding window model [ 11] is designed for analyzing data
within a window of most recent elements in the stream. Many
existing works studied various queries over sliding windows such
as distinct elements, frequency count, top- ğ‘˜and skyline. Datar et al .
[10] proposed an effective sliding window framework based on the
Exponential Histogram, creating a logarithmic hierarchical struc-
ture that can effectively approximate the count, sum, and vector
norms of the elements within the window. Arasu and Manku [2]
addressed the problems of element counting and quantiles within
the window using a structure called dyadic intervals.
Random sampling can generate sketches of the data within the
window by maintaining a collection of random samples from the
window [ 3,19]. Recently, Braverman et al . [7] adapted the random
sampling technique to numerical linear algebra over sliding win-
dows. They achieve near optimal space cost in several linear algebra
tasks such as spectral approximation and low-rank approximation.
Badeau et al . [4] studied SVD algorithm in the sliding window
model, but their method needs to store all data in the window rather
than store a sketch. Recently, Wei et al . [34] studied approximating
covariance matrix over sliding windows, which can be regard as
a special case of AMM, i.e., X=Y. Their proposed Logarithmic
Method and Dyadic Interval framework combined with the FD
algorithm can achieve ğœ€-approximation with logarithmic space
complexity relative to the window size. Their methods are later
generalized to the distributed setting by Zhang et al . [37] . However,
few works studied AMM problem in the sliding window model.
3 PRELIMINARIES
In this section, we first introduce the notation used throughout the
paper. Then, we present our problem definition, followed by the
description of the COD algorithm and its theoretical guarantees.
3.1 Notations
We let Iğ‘›be theğ‘›Ã—ğ‘›identity matrix, and 0mÃ—nbe theğ‘šÃ—ğ‘›matrix of
all zeros. We can denote a ğ‘šÃ—ğ‘›matrix as X=[x1,x2,..., xğ‘›], where
xğ‘–âˆˆRğ‘šis theğ‘–-th column of X. We use[X1,X2]to denote their
concatenation on their column dimensions. For a vector xâˆˆRğ‘‘,
we letâˆ¥xâˆ¥2=âˆšï¸ƒÃğ‘‘
ğ‘–=1ğ‘¥2
ğ‘–be itsâ„“2-norm. For a matrix XâˆˆRğ‘šÃ—ğ‘›,
we letâˆ¥Xâˆ¥=max u:âˆ¥uâˆ¥=1âˆ¥Xuâˆ¥be its spectral norm and âˆ¥Xâˆ¥ğ¹=âˆšï¸ƒÃğ‘›
ğ‘–=1âˆ¥xğ‘–âˆ¥2be its Frobenius norm. The condensed singular value
decomposition (SVD) of matrix X, written as SVD(X), is defined
asUÎ£Vğ‘‡where UâˆˆRğ‘šÃ—ğ‘ŸandVâˆˆRğ‘›Ã—ğ‘Ÿare column orthonormal
3897Approximate Matrix Multiplication over Sliding Windows KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Algorithm 1 Co-occurring Directions(COD)
1:Input: XâˆˆRğ‘šğ‘¥Ã—ğ‘›,YâˆˆRğ‘šğ‘¦Ã—ğ‘›and sketch size â„“.
2:Aâ†0ğ‘šğ‘¥Ã—â„“,Bâ†0ğ‘šğ‘¦Ã—â„“.
3:forğ‘–=1,2,...,ğ‘› do
4: Insert xğ‘–into a zero valued column of A.
5: Insert yğ‘–into a zero valued column of B.
6:ifAorBhas no zero valued columns then
7:[Qğ‘¥,Rğ‘¥]â† QR(A).
8:[Qğ‘¦,Rğ‘¦]â† QR(B).
9:[U,Î£,V]â† SVD(Rğ‘¥Rğ‘‡ğ‘¦).
10:ğ›¿â†ğœâ„“/2(Î£).
11: Ë†Î£â†max(Î£âˆ’ğ›¿Iâ„“,0).
12: Aâ†Qğ‘¥Uâˆšï¸
Ë†Î£,Bâ†Qğ‘¦Vâˆšï¸
Ë†Î£.
13: end if
14:end for
15:Output: AandB.
andÎ£is a diagonal matrix with nonzero singular values ğœ1(X)â‰¥
ğœ2(X)â‰¥Â·Â·Â·â‰¥ğœğ‘Ÿ(X)>0. We use nnz(X)to denote number of
nonzero elements of matrix X.
3.2 Problem Setup
We first provide the definition of correlation sketch as follows:
Definition 3.1 ([ 24]).LetXâˆˆRğ‘šğ‘¥Ã—ğ‘›,YâˆˆRğ‘šğ‘¦Ã—ğ‘›,AâˆˆRğ‘šğ‘¥Ã—â„“
andBâˆˆRğ‘šğ‘¦Ã—â„“whereğ‘›â‰¥max(ğ‘šğ‘¥,ğ‘šğ‘¦)andâ„“â‰¤min(ğ‘šğ‘¥,ğ‘šğ‘¦).
We call the pair(A,B)is anğœ€-correlation sketch of (X,Y)if the
correlation error satisfies
corr-err
XYğ‘‡,ABğ‘‡
â‰œXYğ‘‡âˆ’ABğ‘‡2
âˆ¥Xâˆ¥ğ¹âˆ¥Yâˆ¥ğ¹â‰¤ğœ€.
This paper considers the problem of approximate matrix multi-
plication (AMM) over sliding windows. At time step ğ‘¡, the algorithm
receives column pairs (xğ‘¡,yğ‘¡)from the original matrices XandY.
Assuming the window size is ğ‘, we useğ‘Što denote the sliding
window, which consists of ğ‘most recent columns of XandY. We
use matrices Xğ‘ŠandYğ‘Što denote the submatrices of XandYin
the windows, respectively. The algorithm aims to maintain a pair
(A,B), which is an ğœ€-correlation sketch of (Xğ‘Š,Yğ‘Š). Similar to
[34], we assume that both XandYdo not contain zero columns,
and the square norms of the columns in XandYare normalized to
[1,ğ‘…X]and[1,ğ‘…Y], respectively. We let ğ‘…=max(ğ‘…X,ğ‘…Y).
3.3 Co-occurring Directions
Co-occurring directions (COD) [ 24] is a deterministic algorithm for
correlation sketching. COD continuously receives columns from
XâˆˆRğ‘šğ‘¥Ã—ğ‘›andYâˆˆRğ‘šğ‘¦Ã—ğ‘›in a streaming way and maintains two
sketches AâˆˆRğ‘šğ‘¥Ã—â„“andBâˆˆRğ‘šğ‘¦Ã—â„“to approximate XYğ‘‡with
ABğ‘‡. The details of COD are presented in Algorithm 1. COD has
following theoretical guarantees.
Lemma 1 ([24]).The output of co-occurring directions (Algorithm
1) gives a correlation sketch (A,B)of(X,Y)which satisfies: for a
correlation sketch of length â„“â‰¤min(ğ‘šğ‘¥,ğ‘šğ‘¦), we have:
XYğ‘‡âˆ’ABğ‘‡2â‰¤2
â„“âˆ¥Xâˆ¥ğ¹âˆ¥Yâˆ¥ğ¹.Algorithm 1 runs in ğ‘‚(ğ‘›(ğ‘šğ‘¥+ğ‘šğ‘¦+â„“)â„“)time and requires a space
ofğ‘‚((ğ‘šğ‘¥+ğ‘šğ‘¦+â„“)â„“).
4APPROXIMATE MATRIX MULTIPLICATION
OVER SLIDING WINDOWS
In this section, we propose two algorithms for computing approxi-
mate matrix multiplication over sliding windows.
4.1 The EH-COD Algorithm
We first introduce Exponential Histogram Co-occurring Direc-
tions (EH-COD), which leverages the ideas of Exponential His-
tograms [ 10] and incorporates the COD technique for efficiently
approximating matrix multiplication in the sliding window model.
4.1.1 Algorithm Description.
Main Idea. We show the main idea of the EH-COD algorithm in
Figure 1 and describe the detailed steps in Algorithm 2. In the EH-
COD algorithm, columns within a sliding window are segmented
into blocks with different sizes. The blocks are organized into ğ¿
levels (L[1],L[2],...,L[ğ¿]in Algorithm 2) and each level con-
tains up toğ‘=ğ‘‚(1/ğœ€)blocks. Each block ğ¾stores two matrices Aâ€²
andBâ€²as well as the start time, the end time and the size of ğ¾. We
define the size of a block ğ¾asğ¾.ğ‘ ğ‘–ğ‘§ğ‘’ =âˆ¥Aâ€²âˆ¥ğ¹âˆ¥Bâ€²âˆ¥ğ¹. The sketches
Aâ€²andBâ€²are approximations of columns of the streaming data
matrices XandYfrom the start time to the end time, respectively.
The EH-COD algorithm will guarantee that the size of blocks in
levelğ‘–between 2ğ‘–âˆ’1â„“and2ğ‘–â„“, ensuring a structured and efficient
data hierarchy.
Update Algorithm. The EH-COD algorithm employs a buffer ğµâˆ—
to collect columns xğ‘¡,yğ‘¡from matrices XandY. Once the size
ofğµâˆ—exceedsâ„“, EI-COD will move it to the first level, where
the blocks hold raw data columns without approximation. When
level 1fills up and contains ğ‘blocks, we combine the two oldest
blocksğ¾1andğ¾2into blockğ¾, whereğ¾.Aâ€²=[ğ¾1.Aâ€²,ğ¾2.Aâ€²]and
ğ¾.Bâ€²=[ğ¾1.Bâ€²,ğ¾2.Bâ€²]with updated size and timeframe. If the
number of columns in ğ¾.Aâ€²andğ¾.Bâ€²does not exceeds â„“, we directly
move it to level 2. Otherwise we compress the matrices ğ¾.Aâ€²and
ğ¾.Bâ€²intoâ„“columns by the correlation shrinkage (CS) procedure
shown in Algorithm 3, which is a variant of the COD algorithm.
This process of merging and compressing also continues in
higher levels. The algorithm will remove outdated blocks in the
levelğ¿whose start time is earlier than ğ‘¡âˆ’ğ‘. To guarantee that
the block sizes in level ğ‘–are between 2ğ‘–âˆ’1â„“and2ğ‘–â„“, blocks whose
sizes are larger than 2ğ‘–â„“in the levelğ‘–are directly moved up without
merging. They are merged only when they reach level ğ‘—and their
sizes are not larger than 2ğ‘—â„“.
Query Algorithm. The query algorithm of EH-COD is presented
in Algorithm 4. Notice that the EH-COD algorithm maintains a
sequence of blocks with sketches approximating the Xğ‘ŠandYğ‘Š
matrices. We can generate the approximate matrix multiplication
ofXğ‘ŠandYğ‘Šby merging the sketches from non-expiring blocks
in all levels and then performing the COD algorithm for once.
4.1.2 Approximation Error Analysis. The correlation error of the
EH-COD algorithm originates from three parts: the error from
expiring block, the aggregate errors across all block sketches and
3898KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
Figure 1: The illustration of the EH-COD Algorithm
Algorithm 2 EH-COD
1:Input: XâˆˆRğ‘šğ‘¥Ã—ğ‘›,YâˆˆRğ‘šğ‘¦Ã—ğ‘›,sketch sizeâ„“.
2:forğ‘¡=1,2,...,ğ‘› do
3: Remove expiring blocks in L[ğ¿].
4:ğµâˆ—.insert(xğ‘¡,yğ‘¡).
5:ğµâˆ—.ğ‘’ğ‘›ğ‘‘â†ğ‘¡.
6:ifğµâˆ—.sizeâ‰¥â„“then
7:L[1].append(ğµâˆ—).
8: initialize an empty ğµâˆ—withğµâˆ—.start =ğµâˆ—.end=ğ‘¡.
9:end if
10: forğ‘–=1,2,...,ğ¿ do
11: iflength(L[ğ‘–])â‰¥ğ‘+1then
12: Find the furthest blocks ğ¾1andğ¾2inL[ğ‘–].
13:ğ¾â†MERGE(ğ¾1,ğ¾2).
14:L[ğ‘–+1].append(ğ¾).
15: Removeğ¾1andğ¾2fromL[ğ‘–].
16: end if
17: end for
18:end for
19:procedure MERGE(ğ¾1,ğ¾2)
20:ğ¾.Aâ€²â†[ğ¾1.Aâ€²;ğ¾2.Aâ€²],ğ¾.Bâ€²â†[ğ¾1.Bâ€²;ğ¾2.Bâ€²].
21:ğ¾.sizeâ†âˆ¥ğ¾.Aâ€²âˆ¥ğ¹âˆ¥ğ¾.Bâ€²âˆ¥ğ¹.
22:ğ¾.startâ†ğ¾1.start,ğ¾.endâ†ğ¾2.end.
23: ifcolumns(ğ¾.Aâ€²)>â„“then
24:(ğ¾.Aâ€²,ğ¾.Bâ€²)â† CS(ğ¾.Aâ€²,ğ¾.Bâ€²,â„“).
25: end if
26: returnğ¾.
27:end procedure
Algorithm 3 Correlation Shrinkage (CS)
1:Input: XâˆˆRğ‘šğ‘¥Ã—ğ‘›,YâˆˆRğ‘šğ‘¦Ã—ğ‘›,sketch sizeâ„“.
2:[Qğ‘¥,Rğ‘¥]â† QR(X),[Qğ‘¦,Rğ‘¦]â† QR(Y).
3:[U,Î£,V]â† SVD(Rğ‘¥RâŠ¤ğ‘¦).
4:ğ›¿â†ğœâ„“(Î£),Ë†Î£â†max(Î£âˆ’ğ›¿Iğ‘›,0).
5:Aâ†Qğ‘¥Uâˆšï¸
Ë†Î£,Bâ†Qğ‘¦Vâˆšï¸
Ë†Î£.
6:Output: AandB.
the error from merging sketches. The main idea is to bound all parts
of errors to be ğ‘‚(ğœ€). We present our approximation error bound in
the following theorem and defer the proof to the appendix.Algorithm 4 Query for EH-COD
1:Input: sketch sequenceL, sketch size â„“.
2:Câ†empty, Dâ†empty.
3:forğ‘–=1,2,...,ğ¿ do
4:forğ‘—=1,2,..., length(L[ğ‘–])do
5: Câ†[C;L[ğ‘–][ğ‘—].Aâ€²],Dâ†[D;L[ğ‘–][ğ‘—].Bâ€²].
6:end for
7:end for
8:(A,B)â† COD(C,D,â„“).
9:Output: AandB.
Theorem 1. If we setâ„“=ğ‘=8
ğœ€, the EH-COD can generate sketches
AandBof sizeğ‘‚(â„“), with the correlation error upper bounded by
corr-err
Xğ‘ŠYğ‘‡
ğ‘Š,ABğ‘‡
â‰¤ğœ€.
4.1.3 Complexity Analysis.
Space Complexity. The EH-COD algorithm can guarantee that
each block only contains at most â„“columns. Also, the number of
blocks is at most ğ¿ğ‘. Since we haveâˆ¥xâˆ¥âˆ¥yâˆ¥â‰¥1for any column pair
x,y, we know that blocks in level 1contain no more than â„“columns.
In addition, the CS algorithm guarantee that the blocks at higher lev-
els also contain no more than â„“columns. Since the number of blocks
is at mostğ¿ğ‘, the total columns are bounded by ğ¿ğ‘â„“. According
to Lemma 4 in appendix B, we have ğ¿â‰¤log
2
â„“ğ‘âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹
.
Thus, the total space usage is bounded as
ğ¿ğ‘â„“(ğ‘šğ‘¥+ğ‘šğ‘¦)â‰¤log2
â„“ğ‘âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹8
ğœ€â„“(ğ‘šğ‘¥+ğ‘šğ‘¦)
=ğ‘‚ğ‘šğ‘¥+ğ‘šğ‘¦
ğœ€2log(ğœ€ğ‘ğ‘…)
,
whereğ‘is the window size and ğ‘…is the maximum squared column
norm of XandY.
Time complexity. In the worst case, each column will be com-
pressed for ğ¿times by the CS procedure since we have ğ¿levels.
The time to run a CS procedure is ğ‘‚ (ğ‘šğ‘¥+ğ‘šğ‘¦)â„“2and a CS pro-
cedure can compress â„“new columns. Thus the EH-COD algorithm
takesğ‘‚((ğ‘šğ‘¥+ğ‘šğ‘¦)â„“)to compress a column in average. Therefore,
the amortized time cost for processing each column is bounded by
ğ‘‚((ğ‘šğ‘¥+ğ‘šğ‘¦)â„“ğ¿)=ğ‘‚ğ‘šğ‘¥+ğ‘šğ‘¦
ğœ€log(ğœ€ğ‘ğ‘…)
.
3899Approximate Matrix Multiplication over Sliding Windows KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Algorithm 5 DI-COD
1:Input: XâˆˆRğ‘šğ‘¥Ã—ğ‘›,YâˆˆRğ‘šğ‘¦Ã—ğ‘›, sketch size â„“and the size of
windowğ‘.
2:ğ‘ ğ‘–ğ‘§ğ‘’ Xâ†0,ğ‘ ğ‘–ğ‘§ğ‘’ Yâ†0.
3:forğ‘¡=1,2,...,ğ‘› do
4:forğ‘–=1,2,...,ğ¿ do
5: Remove expiring blocks in L[ğ‘–].
6:ğµâˆ—
ğ‘–.insert(xğ‘¡,yğ‘¡).
7:ğµâˆ—
ğ‘–.ğ‘’ğ‘›ğ‘‘â†ğ‘¡.
8: ifcolumns(ğµâˆ—
ğ‘–.Aâ€²)=2â„“ğ‘–then
9: ğµâˆ—
ğ‘–.Aâ€²,ğµâˆ—
ğ‘–.Bâ€²=CS(ğµâˆ—
ğ‘–.Aâ€²,ğµâˆ—
ğ‘–.Bâ€²,â„“ğ‘–).
10: end if
11: end for
12:ğ‘ ğ‘–ğ‘§ğ‘’ Xâ†ğ‘ ğ‘–ğ‘§ğ‘’ X+âˆ¥xğ‘¡âˆ¥2,ğ‘ ğ‘–ğ‘§ğ‘’ Yâ†ğ‘ ğ‘–ğ‘§ğ‘’ Y+âˆ¥yğ‘¡âˆ¥2.
13: ifğ‘ ğ‘–ğ‘§ğ‘’ Xâ‰¥ğ‘ğ‘…X/2ğ¿orğ‘ ğ‘–ğ‘§ğ‘’ Yâ‰¥ğ‘ğ‘…Y/2ğ¿then
14:ğ‘£â†binary trailing zeros (L[1].ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„)+1.
15: forğ‘–=1,2,...,ğ‘£ do
16: Appendğµâˆ—
ğ‘–toL[ğ‘–].
17: Initialize empty ğµâˆ—
ğ‘–withğµâˆ—
ğ‘–.ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ =ğµâˆ—
ğ‘–.ğ‘’ğ‘›ğ‘‘=ğ‘¡.
18: end for
19: end if
20:end for
4.2 The DI-COD Algorithm
The second proposed algorithm for sliding window AMM is the
Dyadic Interval Co-occurring Directions (DI-COD) algorithm, which
utilizes a tree structure to store data within the window. The DI-
COD algorithm requires less space usage than the EH-COD algo-
rithm when the column norm of the data matrices is small.
4.2.1 Algorithm Description.
Main Idea. We show the main idea of the DI-COD algorithm in
Figure 2 and describe the detailed steps in Algorithm 5. The DI-COD
algorithm uses a hierarchical structure with ğ¿levels, each of which
contains a dynamic number of blocks. Each level contains three
types of blocks: expiring, inactive and buffer. Each block contains
two matrices Aâ€²andBâ€². For inactive blocks in level ğ‘–, we let both
Aâ€²andBâ€²containâ„“ğ‘–columns. This ensures that the approximation
error of level ğ‘–satisfiesğœ€ğ‘–=1
â„“ğ‘–=1
2ğ‘–ğ¿. We define the size of a block
ğ¾asğ¾.ğ‘ ğ‘–ğ‘§ğ‘’ =âˆ¥Aâ€²âˆ¥ğ¹âˆ¥Bâ€²âˆ¥ğ¹. Notice that for DI-COD, the size of
blocks in the same level are the same, thus we let ğ‘ ğ‘–ğ‘§ğ‘’ğ‘–be the size
of each blocks in level ğ‘–. For each level ğ‘–, we maintain a buffer ğµâˆ—
ğ‘–to
receive XandY. When the number of columns of ğµâˆ—
ğ‘–reaches 2â„“ğ‘–, we
perform the CS procedure to compress the matrices in ğµâˆ—
ğ‘–, so that
ğµâˆ—
ğ‘–can continue to receive new columns. When ğµâˆ—.ğ‘ ğ‘–ğ‘§ğ‘’ satisfies
the condition in line 13 (here ğ‘…Xandğ‘…Yare the maximum column
norm of XandY, respectively), we append it to L[ğ‘–]and regard it
as an inactive block.
Query Algorithm. Algorithm 6 presents the query algorithm
for DI-COD. As DI-COD uses the tree structure, we need to select
appropriate blocks from top to bottom in each level and concatenate
them to form the final correlation sketch pair (A,B). We set two
timestamps, ğ‘™=ğ‘¡âˆ’ğ‘andğ‘Ÿ=ğ‘¡, to select suitable blocks in each
level. The algorithm first identifies the highest level ğ‘£containing
inactive blocks and proceed downwards for selection. Within eachAlgorithm 6 Query for DI-COD on window (ğ‘¡âˆ’ğ‘,ğ‘¡)
1:Input: sketch sequenceLandğ‘¡.
2:Aâ†ğ‘’ğ‘šğ‘ğ‘¡ğ‘¦, Bâ†ğ‘’ğ‘šğ‘ğ‘¡ğ‘¦ .
3:ğ‘™â†ğ‘¡âˆ’ğ‘,ğ‘Ÿâ†ğ‘¡.
4:Find the highest level ğ‘£with at least 1inactive block.
5:forğ‘–=ğ‘£,ğ‘£âˆ’1,..., 1do
6: Find the leftmost block L[ğ‘–][ğ‘—].
7:ifL[ğ‘–][ğ‘—].ğ‘’ğ‘›ğ‘‘â‰¤ğ‘Ÿthen
8: Aâ†[L[ğ‘–][ğ‘—].Aâ€²;A],Bâ†[L[ğ‘–][ğ‘—].Bâ€²;B].
9:ğ‘Ÿâ†L[ğ‘–][ğ‘—].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ .
10: end if
11: Find the rightmost block L[ğ‘–][ğ‘˜].
12: ifğ‘˜=ğ‘—then
13:ğ‘™â†L[ğ‘–][ğ‘˜].ğ‘’ğ‘›ğ‘‘.
14: else ifL[ğ‘–][ğ‘˜].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡â‰¥ğ‘™then
15: Aâ†[A;L[ğ‘–][ğ‘—].Aâ€²],Bâ†[B;L[ğ‘–][ğ‘—].Bâ€²].
16:ğ‘™â†L[ğ‘–][ğ‘˜].ğ‘’ğ‘›ğ‘‘.
17: end if
18:end for
19:Output: AandB.
level, we locates the furthest block L[ğ‘–][ğ‘—]. If theğ‘’ğ‘›ğ‘‘timestamp
of this block is less than ğ‘Ÿ, it indicates that this block needs to be
combined on the left, and we update ğ‘Ÿto itsğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ timestamp. Then
we find the most recent block L[ğ‘–][ğ‘˜]in the same level. If it is the
same as the leftmost block, we shift ğ‘™to the right and update it to
itsğ‘’ğ‘›ğ‘‘timestamp. IfL[ğ‘–][ğ‘˜].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ is greater than or equal to ğ‘™, it
signifies that this block needs to be combined on the right, and we
updateğ‘™toL[ğ‘–][ğ‘˜].ğ‘’ğ‘›ğ‘‘. We repeat this process until we reach the
first level. The blocks selected are highlighted in bold in figure 2.
4.2.2 Approximation Error Analysis. The approximation error of
the DI-COD algorithm can be divided into two parts: the error from
expiring block and the aggregate errors across all block sketches.
The main idea is to control both parts of errors to be ğ‘‚(ğœ€). We
present our approximation error bound in the following theorem
and defer the proof to the appendix.
Theorem 2. If we setğ¿=logğ‘…
ğœ€andâ„“ğ‘–=2ğ‘–ğ¿, the DI-COD can
generate sketches AandBof sizeğ‘‚
ğ‘…
ğœ€logğ‘…
ğœ€
, with the correlation
error upper bounded by
corr-err
Xğ‘ŠYğ‘‡
ğ‘Š,ABğ‘‡
â‰¤ğœ€.
4.2.3 Complexity Analysis.
Space Complexity. Since the window size satisfies ğ‘ ğ‘–ğ‘§ğ‘’ğ‘Šâ‰¤ğ‘ğ‘…
andğ‘ ğ‘–ğ‘§ğ‘’ğ‘–â‰¥ğ‘ğ‘…
2ğ¿âˆ’ğ‘–+1, theğ‘–-th level can have at most 2ğ¿âˆ’ğ‘–+1blocks.
Thus the space complexity of DI-COD is
2ğ¿âˆ‘ï¸
ğ‘–=1â„“ğ‘–2ğ¿âˆ’ğ‘–+1=ğ¿âˆ‘ï¸
ğ‘–=12ğ‘–ğ¿2ğ¿âˆ’ğ‘–+2=4ğ‘…
ğœ€log2ğ‘…
ğœ€=ğ‘‚ğ‘…
ğœ€log2ğ‘…
ğœ€
.
Time complexity. The amortized time for each column during
the CS process is ğ‘‚((ğ‘šğ‘¥+ğ‘šğ‘¦)â„“). In the worst case, CS operations
are performed concurrently across all ğ¿levels. Thus, the time com-
plexity amounts to ğ‘‚((ğ‘šğ‘¥+ğ‘šğ‘¦)â„“ğ¿)=ğ‘‚ğ‘šğ‘¥+ğ‘šğ‘¦
ğœ€logğ‘…
ğœ€
.
3900KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
Figure 2: The illustration of the DI-COD Algorithm
Algorithm 7 EH-SCOD
1:Input:XâˆˆRğ‘šğ‘¥Ã—ğ‘›,YâˆˆRğ‘šğ‘¦Ã—ğ‘›,sketch sizeâ„“,ratioğœ….
2:forğ‘¡=1,2,...,ğ‘› do
3: Remove expiring blocks in L[ğ¿].
4:ğµâˆ—.insert(xğ‘¡,yğ‘¡).
5:ifğµâˆ—.sizeâ‰¥ğœ€ğ‘then
6:ğµâˆ—â†SPM(ğµâˆ—,â„“,ğ‘).
7: initialize an empty ğµâˆ—withğµâˆ—.start =ğµâˆ—.end=ğ‘¡.
8:end if
9:forğ‘–=1,2,...,ğ¿ do
10: iflength(L[ğ‘–])â‰¥ğ‘+1then
11: find the furthest blocks ğ¾1andğ¾2inL[ğ‘–].
12:ğ¾â†merge(ğ¾1,ğ¾2).
13:L[ğ‘–+1].append(ğ¾).
14: Removeğ¾1andğ¾2fromL[ğ‘–].
15: end if
16: end for
17:end for
5 IMPROVED ALGORITHMS FOR SPARSE
MATRICES
Many data matrices in real-world applications are sparse. In this sec-
tion we propose variants of EH-COD and DI-COD which enhance
the performance over sparse matrices.
5.1 The EH-SCOD Algorithm
We describe details of Exponential Histogram Sparse Co-occurring
Directions (EH-SCOD) in algorithm 7, which is an improved version
of the EH-COD algorithm. The EH-SCOD algorithm enlarge the
capacity of buffer ğµâˆ—toğ‘‚(ğœ€ğ‘). When the buffer size exceeds ğœ€ğ‘,
EH-SCOD uses the subspace power method (SPM) algorithm [ 25]
(outlined in Algorithm 8) to compress the matrices within the buffer
down toâ„“columns, so as not to exceed the space budget. The com-
pressed block is then placed in level 1, where subsequent merging
and compression operations remain consistent with EH-COD. The
query method of EH-SCOD is the same as that of EH-COD, pro-
ducing a final sketch of size â„“. We have the following theoretical
results for the proposed EH-SCOD algorithm.Algorithm 8 Subspace Power Method (SPM)
1:Input: blockğ¾, target rank â„“and integer ğ‘.
2:Mâ†(ğ¾.Aâ€²)(ğ¾.Bâ€²)ğ‘‡âˆˆRğ‘šğ‘¥Ã—ğ‘šğ‘¦.
3:Generate G=[ğºğ‘–ğ‘—]âˆˆRğ‘šğ‘¦Ã—â„“, whereğºğ‘–ğ‘—âˆ¼N( 0,1).
4:Fâ†(MMğ‘‡)ğ‘MGâˆˆRğ‘šğ‘¥Ã—â„“.
5:Zâ†orthonormal column basis of F.
6:[U,Î£,V]â† SVD(Zğ‘‡(ğ¾.Aâ€²)ğ‘‡(ğ¾.Bâ€²)).
7:ğ¾.Aâ€²â†ZUâˆš
Î£,ğ¾.Bâ€²â†Vâˆš
Î£.
8:Output: blockğ¾.
Theorem 3. If we choose ğ‘=â„“=8
ğœ€andğ‘=Î˜(log(â„“/ğ›¿))for EH-
SCOD and SPM algorithms, then with probability 1âˆ’ğ›¿, EH-SCOD
can generate sketches AandBof sizeğ‘‚(â„“), with the correlation error
upper bounded by
corr-err
Xğ‘ŠYğ‘‡
ğ‘Š,ABğ‘‡
â‰¤ğœ€,
In addition, the EH-SCOD algorithm only requires ğ‘‚ğ‘šğ‘¥+ğ‘šğ‘¦
ğœ€2logğ‘…
space cost and ğ‘‚ğ‘šğ‘¥+ğ‘šğ‘¦
ğœ€logğ‘…
time cost in each round.
Remark 1. Sinceğ‘â‰«â„“=ğ‘‚(1
ğœ€), we have logğ‘…<logğœ€ğ‘ğ‘… , which
means EH-SCOD is more efficient than EH-COD.
5.2 The DI-SCOD Algorithm
We describe details of the proposed DI-SCOD algorithm in Algo-
rithm 9, an enhanced version of the DI-COD algorithm. In DI-SCOD,
data within each buffer ğµâˆ—
ğ‘–at every level is stored sparsely without
undergoing any CS procedure until it reaches its designated size.
Once the buffer hits this size threshold, the SPM is applied to effec-
tively reduce the number of columns in the buffer. Subsequently,
the CS procedure is applied just once per buffer, thereby reducing
the frequency of CS operations required across inactive blocks. The
query steps remain unchanged. We have the following theoretical
results for the proposed DI-SCOD algorithm.
Theorem 4. If we setğ¿=logğ‘…
ğœ€,â„“ğ‘–=2ğ‘–ğ¿andğ‘=Î˜(log(â„“/ğ›¿))
for DI-SCOD and SPM algorithms, then with probability 1âˆ’ğ›¿, DI-
SCOD can generate sketches AandBof sizeğ‘‚
ğ‘…
ğœ€logğ‘…
ğœ€
, with the
3901Approximate Matrix Multiplication over Sliding Windows KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
Dataset ğ‘› ğ‘šğ‘¥ğ‘šğ‘¦ density(X) density(Y) ğ‘…Xğ‘…Y
DENSE 1Ã—1042000 1000 1.0 1.0 370 719
APR 2.32Ã—1042.80Ã—1044.28Ã—1046.31Ã—10âˆ’44.53Ã—10âˆ’4773 618
PAN11 8.90Ã—1045.12Ã—1049.96Ã—1044.38Ã—10âˆ’42.43Ã—10âˆ’45655 6188
EURO0 4.76Ã—1057.25Ã—1048.77Ã—1043.46Ã—10âˆ’43.47Ã—10âˆ’4103370 112506
Table 1: The statistics of datasets, where density(X) = nnz (X)/(ğ‘›ğ‘šğ‘¥)and density(Y) = nnz (Y)/ğ‘›ğ‘šğ‘¦.
Algorithm 9 DI-SCOD
1:Input: XâˆˆRğ‘šğ‘¥Ã—ğ‘›,YâˆˆRğ‘šğ‘¦Ã—ğ‘›, sketch size â„“.
2:ğ‘ ğ‘–ğ‘§ğ‘’ Xâ†0,ğ‘ ğ‘–ğ‘§ğ‘’ Yâ†0.
3:forğ‘¡=1,2,...,ğ‘› do
4:forğ‘–=1,2,...,ğ¿ do
5: Remove expiring blocks in L[ğ¿].
6:ğµâˆ—
ğ‘–.insert(xğ‘¡,yğ‘¡).
7:ğµâˆ—
ğ‘–.ğ‘’ğ‘›ğ‘‘â†ğ‘¡.
8:end for
9:ğ‘ ğ‘–ğ‘§ğ‘’ Xâ†ğ‘ ğ‘–ğ‘§ğ‘’ X+âˆ¥xğ‘¡âˆ¥2,ğ‘ ğ‘–ğ‘§ğ‘’ Yâ†ğ‘ ğ‘–ğ‘§ğ‘’ Y+âˆ¥yğ‘¡âˆ¥2.
10: ifğ‘ ğ‘–ğ‘§ğ‘’ Xâ‰¥ğ‘ğ‘…X/2ğ¿orğ‘ ğ‘–ğ‘§ğ‘’ Yâ‰¥ğ‘ğ‘…Y/2ğ¿then
11:ğ‘£â†binary trailing zeros (L[1].ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„)+1.
12: forğ‘–=1,2,...,ğ‘£ do
13:ğµâˆ—
ğ‘–=SPM(ğµâˆ—
ğ‘–,2â„“ğ‘–,ğ‘).
14: ğµâˆ—
ğ‘–.Aâ€²,ğµâˆ—
ğ‘–.Bâ€²=CS(ğµâˆ—
ğ‘–.Aâ€²,ğµâˆ—
ğ‘–.Bâ€²,â„“ğ‘–).
15: Appendğµâˆ—
ğ‘–toL[ğ‘–].
16: Initialize empty ğµâˆ—
ğ‘–withğµâˆ—
ğ‘–.ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ =ğµâˆ—
ğ‘–.ğ‘’ğ‘›ğ‘‘=ğ‘¡.
17: end for
18: end if
19:end for
correlation error upper bounded by
corr-err
Xğ‘ŠYğ‘‡
ğ‘Š,ABğ‘‡
â‰¤ğœ€.
In addition, in each round DI-SCOD requires ğ‘‚
ğ‘…
ğœ€log2ğ‘…
ğœ€
space cost
andğ‘‚ğ‘›ğ‘›ğ‘§(Xğ‘Š)+ğ‘›ğ‘›ğ‘§(Yğ‘Š)
ğ‘ğœ€+ğ‘šğ‘¥+ğ‘šğ‘¦
ğ‘ğœ€2
logğ‘…
ğœ€
time cost.
Remark 2. We haveğ‘â‰«1
ğœ€since the size of final sketches should
be much less than the window size. Thus we haveğ‘šğ‘¥+ğ‘šğ‘¦
ğ‘ğœ€2â‰ªğ‘šğ‘¥+ğ‘šğ‘¦
ğœ€.
In addition, for the sparse matrices Xğ‘ŠandYğ‘Š, we haveğ‘›ğ‘›ğ‘§(Xğ‘Š)+
ğ‘›ğ‘›ğ‘§(Yğ‘Š)â‰ªğ‘(ğ‘šğ‘¥+ğ‘šğ‘¦). Thus DI-SCOD requires less computational
cost than DI-COD.
6 EXPRIMENTS
In this section, we empirically study performance of proposed al-
gorithms1. We compare proposed algorithms with a baseline al-
gorithm: priority sampling, which randomly select each column
pair(xğ‘¡,yğ‘¡)according to its weight ğœŒ=ğ‘¢1/(âˆ¥ xâˆ¥âˆ¥yâˆ¥)
ğ‘¡. Hereğ‘¢ğ‘¡is
uniformly sampled from (0,1).
Datasets. We employ one dense synthetic dataset named DENSE
along with three sparse cross-language datasets: APR, PAN11, and
EURO0 [ 17,28â€“30] for evaluating our methods. The statistics of
the datasets are provided in Table 1.
1The code is available at: https://github.com/lilianzhi0/AMMSWThe DENSE dataset consists of two synthetic random matrices
where each element is uniformly sampled from [0,1]. The matrices
XandYhave 2,000and1,000rows, respectively. Both matrices
have 10,000columns.
The Amazon Product Reviews (APR) dataset is a publicly avail-
able collection that includes product reviews and related informa-
tion from the Amazon website. The dataset contains millions of
sentences in English and French. We construct it into a review ma-
trix where the Xmatrix has 28,017rows and Yhas42,833rows,
with both matrices sharing 23,235columns.
PANPC-11 (PAN11) is a dataset for text analysis, focusing on
tasks such as plagiarism detection, author identification, and near-
duplicate detection. Its texts are in English and French. The two
data matrices XandYcontain 51,219and99,595rows, respectively.
Both of them have 88,977columns.
The Europarl (EURO) dataset is a widely used multilingual paral-
lel corpus, composed of the proceedings of the European Parliament.
We only utilize parts of its English and French texts to form EURO0.
The matrix XandYcontains 72,470and87,686rows, respectively.
They both have 475,834columns.
Setup. We set the window size to be 4,000for the DENSE dataset
and10,000for the larger sparse datasets APR, PAN11, and EURO0.
We evaluate all algorithms according to their correlation errors
corr-err(Xğ‘ŠYğ‘‡
ğ‘Š,ABğ‘‡), where Xğ‘ŠandYğ‘Šare the original ma-
trices in the window, and AandBare the generated correlation
sketches. We compare both average and maximum errors through
all sampled windows. All algorithms are implemented in MATLAB
(R2021a). The experiments are executed on a Windows server with
32GB memory and a single core of Intel i9-13900K.
Performance. For the time comparison, we present error vs. final
sketch size and error vs. total space usage in Figure 3 and Figure
4, respectively. The final sketch size is the number of columns
of the sketches generated by the algorithms for query, and the
total space usage is the maximum number of columns required by
the algorithms. In these figures, we find that EH-SCOD and DI-
SCOD require much less space than EH-COD and DI-COD when
the data matrices are sparse. In addition, Figure 4 shows that DI-
SCOD outperforms EH-SCOD when the maximum column norm
ğ‘…Xandğ‘…Yare large, and vice versa. This phenomenon matches
our theoretical results that the space cost of DI-SCOD is linear to
the maximum column norm while the space cost of EH-SCOD is
logarithmic to the maximum column norm. Figure 3 compares the
final sketch size of proposed algorithms. We can find that EH-COD
and EH-SCOD outperforms DI-COD and DI-SCOD on all datasets.
For the time comparison, we present time vs. final sketch size
in Figure 5, which shows that EH-SCOD and DI-SCOD are signifi-
cantly faster than EH-COD and DI-COD on the sparse datasets. We
3902KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
0 100 200 300 400 500 600
Final Sketch Size00.050.10.150.2Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(a) DENSE
0 100 200 300 400
Final Sketch Size00.10.20.30.40.50.6Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (b) APR
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.3Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (c) PAN11
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.3Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (d) EURO0
0 100 200 300 400
Final Sketch Size00.050.10.150.20.25Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(e) DENSE
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.30.35Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (f) APR
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.30.35Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (g) PAN11
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.3Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (h) EURO0
Figure 3: error vs. final sketch size
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.2Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(a) DENSE
0 500 1000 1500 2000 2500 3000
Total Space Usage (columns)00.050.10.150.20.25Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (b) APR
0 500 1000 1500 2000
Total Space Usage (columns)00.050.10.150.20.250.30.350.4Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (c) PAN11
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.20.250.3Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (d) EURO0
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.20.25Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(e) DENSE
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.20.25Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (f) APR
0 500 1000 1500 2000 2500 3000
Total Space Usage (columns)00.050.10.150.20.250.30.35Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (g) PAN11
0 500 1000 1500 2000 2500 3000
Total Space Usage (columns)00.050.10.150.20.250.3Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (h) EURO0
Figure 4: error vs. total space usage
0 100 200 300 400
Final Sketch Size00.511.522.533.54Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(a) DENSE
0 100 200 300 400 500 600
Final Sketch Size010203040506070Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (b) APR
0 100 200 300 400 500 600
Final Sketch Size050100150200250300Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (c) PAN11
0 100 200 300 400
Final Sketch Size020406080100120140160Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (d) EURO0
Figure 5: time cost per column vs. final sketch size
3903Approximate Matrix Multiplication over Sliding Windows KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
surprisingly find that EH-SCOD is faster than EH-COD on dense
data while DI-SCOD is faster than DI-COD. We think this is because
EH-SCOD increases the buffer capacity and enables each block to
store more data than EH-COD. Thus EH-SCOD performs fewer CS
procedures than EH-COD and thus requires less amortized running
time. On the other hand, DI-SCOD needs to perform more SPM pro-
cedures than EI-SCOD. Since SPM is expensive on dense matrices,
DI-SCOD is not as efficient as EH-SCOD on dense datasets, and is
even slower than DI-COD.
7 CONCLUSION
In this paper, we introduce two novel algorithms, called EH-COD
and DI-COD, for approximate matrix multiplication over sliding
windows. We also provide their error analysis as well as the com-
putational complexities. In addition, we propose sparse variants of
EH-COD and DI-COD which enjoy better performance when the
data matrices are sparse. The experimental results show that the
proposed algorithms have better performance than baseline. In the
future, we will explore the lower bound of the space complexity
of the AMM problem in the sliding window model and study the
optimality of the proposed algorithms.
ACKNOWLEDGMENTS
Cheng Chen is supported by National Natural Science Foundation
of China (62306116). Mingsong Chen is supported by National
Natural Science Foundation of China (62272170) and â€œDigital Silk
Roadâ€ Shanghai International Joint Lab of Trustworthy Intelligent
Software (22510750100).
REFERENCES
[1]Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril
Zhang, and Yi Zhang. 2019. Efficient full-matrix adaptive regularization. In
International Conference on Machine Learning. PMLR, 102â€“110.
[2]Arvind Arasu and Gurmeet Singh Manku. 2004. Approximate counts and quan-
tiles over sliding windows. In Proceedings of the twenty-third ACM SIGMOD-
SIGACT-SIGART symposium on Principles of database systems. 286â€“296.
[3]Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer
Widom. 2002. Models and issues in data stream systems. In Proceedings of the
twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database
systems. 1â€“16.
[4]Roland Badeau, GaÃ«l Richard, and Bertrand David. 2004. Sliding window adaptive
SVD algorithms. IEEE Transactions on Signal Processing 52, 1 (2004), 1â€“10.
[5]Davis Blalock and John Guttag. 2021. Multiplying matrices without multiplying.
InInternational Conference on Machine Learning. PMLR, 992â€“1004.
[6]Michele Borassi, Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, and
Morteza Zadimoghaddam. 2020. Sliding window algorithms for k-clustering
problems. Advances in Neural Information Processing Systems 33 (2020).
[7]Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj
Upadhyay, David P Woodruff, and Samson Zhou. 2020. Near optimal linear
algebra in the online and sliding window models. In 2020 IEEE 61st Annual
Symposium on Foundations of Computer Science (FOCS). IEEE, 517â€“528.
[8]Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina
Persu. 2015. Dimensionality reduction for k-means clustering and low rank
approximation. In Proceedings of the forty-seventh annual ACM symposium on
Theory of computing. 163â€“172.
[9]Michael B Cohen, Jelani Nelson, and David P Woodruff. 2015. Optimal approx-
imate matrix product in terms of stable rank. arXiv preprint arXiv:1507.02268
(2015).
[10] Mayur Datar, Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 2002. Main-
taining stream statistics over sliding windows. SIAM journal on computing 31, 6
(2002), 1794â€“1813.
[11] Mayur Datar and Rajeev Motwani. 2016. The sliding-window computation model
and results. In Data Stream Management: Processing High-Speed Data Streams.
Springer, 149â€“165.[12] Inderjit S Dhillon. 2001. Co-clustering documents and words using bipartite
spectral graph partitioning. In Proceedings of the 7th ACM SIGKDD international
conference on Knowledge discovery and data mining. 269â€“274.
[13] Petros Drineas, Ravi Kannan, and Michael W Mahoney. 2006. Fast Monte Carlo
algorithms for matrices I: Approximating matrix multiplication. SIAM J. Comput.
36, 1 (2006), 132â€“157.
[14] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods
for online learning and stochastic optimization. The Journal of Machine Learning
Research 12, 7 (2011).
[15] Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P Woodruff. 2016. Fre-
quent directions: Simple and deterministic matrix sketching. SIAM J. Comput. 45,
5 (2016), 1762â€“1792.
[16] Harold Hotelling. 1936. Relations between two sets of variates. Biometrika (1936).
[17] Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine transla-
tion. In Proceedings of machine translation summit x: papers. 79â€“86.
[18] Edo Liberty. 2013. Simple and deterministic matrix sketching. In Proceedings of
the 19th ACM SIGKDD international conference on Knowledge discovery and data
mining. 581â€“588.
[19] Zhang Longbo, Li Zhanhuai, Zhao Yiqiang, Yu Min, and Zhang Yang. 2007.
A priority random sampling algorithm for time-based sliding windows over
weighted streaming data. In Proceedings of the 2007 ACM symposium on Applied
computing. 453â€“456.
[20] Luo Luo, Cheng Chen, Guangzeng Xie, and Haishan Ye. 2021. Revisiting Co-
Occurring Directions: Sharper Analysis and Efficient Algorithm for Sparse Ma-
trices. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.
8793â€“8800.
[21] Luo Luo, Cheng Chen, Zhihua Zhang, Wu-Jun Li, and Tong Zhang. 2019. Robust
frequent directions with application in online learning. The Journal of Machine
Learning Research 20, 1 (2019).
[22] Avner Magen and Anastasios Zouzias. 2011. Low rank matrix-valued Chernoff
bounds and approximate matrix multiplication. In Proceedings of the twenty-
second annual ACM-SIAM symposium on Discrete Algorithms. SIAM, 1422â€“1436.
[23] Gurmeet Singh Manku and Rajeev Motwani. 2002. Approximate frequency counts
over data streams. In VLDBâ€™02: Proceedings of the 28th International Conference
on Very Large Databases. Elsevier, 346â€“357.
[24] Youssef Mroueh, Etienne Marcheret, and Vaibahava Goel. 2017. Co-occurring
directions sketching for approximate matrix multiply. In Artificial Intelligence
and Statistics. PMLR, 567â€“575.
[25] Cameron Musco and Christopher Musco. 2015. Randomized block krylov methods
for stronger and faster approximate singular value decomposition. Advances in
Neural Information Processing Systems 28 (2015).
[26] Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. 2010. Linear
regression for face recognition. IEEE transactions on pattern analysis and machine
intelligence (TPAMI) 32, 11 (2010), 2106â€“2112.
[27] Odysseas Papapetrou, Minos Garofalakis, and Antonios Deligiannakis. 2015.
Sketching distributed sliding-window data streams. The VLDB Journal 24 (2015),
345â€“368.
[28] Martin Potthast, Alberto BarrÃ³n-Cedeno, Benno Stein, and Paolo Rosso. 2011.
Cross-language plagiarism detection. Language Resources and Evaluation 45
(2011), 45â€“62.
[29] Martin Potthast, Benno Stein, Alberto BarrÃ³n-CedeÃ±o, and Paolo Rosso. 2010. An
evaluation framework for plagiarism detection. In Coling 2010: Posters. 997â€“1005.
[30] Peter Prettenhofer and Benno Stein. 2010. Cross-language text classification using
structural correspondence learning. In Proceedings of the 48th annual meeting of
the association for computational linguistics. 1118â€“1127.
[31] Tamas Sarlos. 2006. Improved approximation algorithms for large matrices via
random projections. In 2006 IEEE 47th Annual Symposium on Foundations of
Computer Science (FOCS). IEEE, 143â€“152.
[32] Yuanyu Wan and Lijun Zhang. 2021. Approximate Multiplication of Sparse
Matrices with Limited Space. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 10058â€“10066.
[33] Yuanyu Wan and Lijun Zhang. 2022. Efficient Adaptive Online Learning via Fre-
quent Directions. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI) 44, 10 (2022), 6910â€“6923.
[34] Zhewei Wei, Xuancheng Liu, Feifei Li, Shuo Shang, Xiaoyong Du, and Ji-Rong
Wen. 2016. Matrix sketching over sliding windows. In Proceedings of the 2016
International Conference on Management of Data. 1465â€“1480.
[35] David P Woodruff et al .2014. Sketching as a tool for numerical linear algebra.
Foundations and TrendsÂ® in Theoretical Computer Science 10, 1â€“2 (2014), 1â€“157.
[36] Qiaomin Ye, Luo Luo, and Zhihua Zhang. 2016. Frequent direction algorithms
for approximate matrix multiplication with applications in CCA. Computational
Complexity 1, m3 (2016), 2.
[37] Haida Zhang, Zengfeng Huang, Zhewei Wei, Wenjie Zhang, and Xuemin Lin.
2017. Tracking matrix approximation over distributed sliding windows. In 2017
IEEE 33rd International Conference on Data Engineering (ICDE). IEEE, 833â€“844.
3904KDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
A AUXILIARY LEMMAS FOR THE COD
ALGORITHM
We present some properties of the COD algorithm, which are used
in our proofs.
Lemma 2. Co-occurring directions algorithm is parallelizable. Let
X=[X1;X2]âˆˆRğ‘šğ‘¥Ã—(ğ‘›1+ğ‘›2), and Y=[Y1;Y2]âˆˆRğ‘šğ‘¦Ã—(ğ‘›1+ğ‘›2).
(A1,B1)is the correlation sketch of (X1,Y1), and(A2,B2)is the
correlation sketch of (X2,Y2). Then, the correlation sketch (C,D)of
([A1;A2],[B1;B2])is a correlation sketch of (X,Y), and satisfies
XYğ‘‡âˆ’CDğ‘‡â‰¤2
â„“âˆ¥Xâˆ¥ğ¹âˆ¥Yâˆ¥ğ¹.
Lemma 3. Given matrices XâˆˆRğ‘šğ‘¥Ã—ğ‘›andYâˆˆRğ‘šğ‘¦Ã—ğ‘›, suppose
we decompose them into ğ‘˜sub-matrices X=[X1;...;Xğ‘˜]andY=
[Y1;...;Yğ‘˜], in which Xğ‘—andYğ‘—both haveğ‘›ğ‘—columns. Let(Ağ‘–,Bğ‘–)=
ğ¶ğ‘‚ğ·(Xğ‘–,Yğ‘–,â„“ğ‘–), such thatâˆ¥Xğ‘–Yğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–âˆ¥â‰¤ğœ€ğ‘–âˆ¥Xğ‘–âˆ¥ğ¹âˆ¥Yğ‘–âˆ¥ğ¹. Then
A=[A1;...;Ağ‘˜]andB=[B1;...;Bğ‘˜]is an approximation for Xand
Ywith error bound
ğ‘‹ğ‘Œğ‘‡âˆ’ğ´ğµğ‘‡2â‰¤ğ‘˜âˆ‘ï¸
ğ‘–=1ğœ€ğ‘–âˆ¥Xğ‘–âˆ¥ğ¹âˆ¥Yğ‘–âˆ¥ğ¹.
B THE PROOF OF THEOREM 1
We first bound the number of levels during the algorithmâ€™s opera-
tion in the following lemma.
Lemma 4. In EH-COD, the number of levels can be upper bounded
by
ğ¿â‰¤log2
â„“ğ‘âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹
.
Proof. Consider the level L-1, which is already filled with ğ‘
blocks. All these blocks cover columns that are subsets of the entire
window, and the combined size of these blocks certainly does not ex-
ceedâˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹. Therefore, we have ğ‘2ğ¿âˆ’1â„“â‰¤âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹.
Taking the logarithm of this inequality yields an upper bound for
ğ¿. â–¡
We partition the matrices Xğ‘ŠandYğ‘Šinto two submatrices,
denoted as Xğ‘Š=[X0,ËœX]andYğ‘Š=[Y0,ËœY]. Here, X0andY0
represent the submatrices formed by columns covered by expiring
blocks in the last level, while ËœXand ËœYare the rest parts that have
been effectively approximated. The final correlation sketches are A
andB. Assuming we approximate X0andY0with zero matrices,
we can derive the covariance errorXğ‘ŠYğ‘‡
ğ‘Šâˆ’ABğ‘‡2=
X0Yğ‘‡
0+ËœXËœYğ‘‡
âˆ’
0+ABğ‘‡2
â‰¤X0Yğ‘‡
02+ËœXËœYğ‘‡âˆ’ABğ‘‡2,
whereâˆ¥X0Yğ‘‡
0âˆ¥â‰¤âˆ¥ X0âˆ¥ğ¹âˆ¥Y0âˆ¥ğ¹, this denotes the size of the expiring
block. The maximum size of the expiring block at level L is 2ğ¿â„“. FromLemma 3, we can deduce the error associated with the expiring
blockX0Yğ‘‡
02â‰¤2ğ¿â„“â‰¤2
ğ‘âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹=ğœ€
4âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹.
Then, we consider the approximation error in the partËœXËœYğ‘‡âˆ’ABğ‘‡2.
Recalling the process of obtaining AandB, we approximated ËœX
and ËœYin blocks, maintaining a sequence of correlation sketches
(A1,B1),(A2,B2),...,(Ağ‘˜,Bğ‘˜). During the approximation of matrix
multiplication within the query window, we merged all the sketches
and performed a final COD, resulting in
ğ´,ğµ=ğ¶ğ‘‚ğ·([A1,...,Ağ‘˜],[B1,...,Bğ‘˜],â„“)=ğ¶ğ‘‚ğ·(C,D,â„“).
Precisely calculating the error in this process is complex, but we can
roughly view the approximation error as consisting of two parts:
the sum of covariance errors from the individual submatrices and
the error introduced by the COD after merging. We now explain
why the error can be computed in this manner:ËœXËœYğ‘‡âˆ’ABğ‘‡2=ËœXËœYğ‘‡âˆ’ABğ‘‡+CDğ‘‡âˆ’CDğ‘‡2
â‰¤ËœXËœYğ‘‡âˆ’CDğ‘‡2+CDğ‘‡âˆ’ABğ‘‡2.
Here, the first part can be further expanded and calculated in detailËœXËœYğ‘‡âˆ’CDğ‘‡2=
ËœX1ËœYğ‘‡
1+...+ËœXğ‘˜ËœYğ‘‡
ğ‘˜
âˆ’
A1Bğ‘‡
1+...+Ağ‘˜Bğ‘‡
ğ‘˜2
â‰¤ğ‘˜âˆ‘ï¸
ğ‘—=1ËœXğ‘—ËœYğ‘‡
ğ‘—âˆ’Ağ‘—Bğ‘‡
ğ‘—2.
We assume thatËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–
2represents the covariance error
of a block located at level i. Due to the parallelizability, the error
for each block adheres to the upper bound introduced by COD, that
is,ËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–
2â‰¤ğœ€
8âˆ¥ËœXğ‘–âˆ¥ğ¹âˆ¥ËœYğ‘–âˆ¥ğ¹â‰¤ğœ€
82ğ‘–â„“. Therefore, the first
part of the approximation error can be bounded by
ğ¿âˆ‘ï¸
ğ‘–=1ğ‘ËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–2â‰¤ğ¿âˆ‘ï¸
ğ‘–=1ğ‘ğœ€
82ğ‘–â„“
=ğœ€
4ğ‘â„“(2ğ¿âˆ’1)
â‰¤ğœ€
2âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹âˆ’ğœ€
4ğ‘â„“
â‰¤ğœ€
2âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹.
We can easily derive the second part of the approximation errorCDğ‘‡âˆ’ABğ‘‡2â‰¤ğœ€
8âˆ¥Câˆ¥ğ¹âˆ¥Dâˆ¥ğ¹â‰¤ğœ€
8âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹.
Consequently, there is an established upper bound for the corre-
lation error between the matrix within the window and the final
correlation sketch. Then we prove Theorem 1 as follows:
Xğ‘ŠYğ‘‡
ğ‘Šâˆ’ABğ‘‡2â‰¤X0Yğ‘‡
02+Ëœğ‘‹ËœYğ‘‡âˆ’CDğ‘‡2+CDğ‘‡âˆ’ABğ‘‡2
â‰¤ğœ€
4+ğœ€
2+ğœ€
8
âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹
â‰¤ğœ€âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹.
3905Approximate Matrix Multiplication over Sliding Windows KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.
C PROOF OF THEOREM 2
We divide the entire window ğ‘into two parts: expired blocks X0,Y0
and non-expiring blocks ËœX,ËœY, then Xğ‘Š=
X0;ËœX
,Yğ‘Š=
Y0;ËœY
and ËœX=ËœX1;...;ËœXğ‘™ğ‘ğ‘ ğ‘¡
,ËœY=ËœY1;...;ËœYğ‘™ğ‘ğ‘ ğ‘¡. Let AandBrepre-
sent sketches of ËœXandËœY, respectively. Then, A=[A1;...;Ağ‘™ğ‘ğ‘ ğ‘¡],B=
[B1;...;Bğ‘™ğ‘ğ‘ ğ‘¡].
From lemma 2, we can get
ËœXËœYğ‘‡âˆ’ABğ‘‡â‰¤ğ‘™ğ‘ğ‘ ğ‘¡âˆ‘ï¸
ğ‘–=1ËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–â‰¤ğ‘™ğ‘ğ‘ ğ‘¡âˆ‘ï¸
ğ‘–=1ğœ€ğ‘–ËœXğ‘–ğ¹ËœYğ‘–ğ¹.
The approximation error can be proved as follows:Xğ‘ŠYğ‘‡
ğ‘Šâˆ’ABğ‘‡=X0Yğ‘‡
0+ËœXËœYğ‘‡âˆ’ABğ‘‡
â‰¤X0Yğ‘‡
0+ËœXËœYğ‘‡âˆ’ABğ‘‡
â‰¤X0Yğ‘‡
0+ğ‘™ğ‘ğ‘ ğ‘¡âˆ‘ï¸
ğ‘–=1ğœ€ğ‘–ËœXğ‘–ğ¹ËœYğ‘–ğ¹.
We can easily deduce that covering the entire window requires
selecting at most two blocks in each level. Furthermore, the maxi-
mum number of expired blocks is at most one block from the first
level. Then ğ‘™ğ‘ğ‘ ğ‘¡â‰¤2ğ¿,X0Yğ‘‡
0â‰¤ğ‘ ğ‘–ğ‘§ğ‘’ 1=ğ‘ğ‘…
2ğ¿âˆ’1andËœXğ‘–ğ¹ËœYğ‘–ğ¹=
âˆšğ‘ ğ‘–ğ‘§ğ‘’ğ‘–2=ğ‘ ğ‘–ğ‘§ğ‘’ğ‘–â‰¤ğ‘ğ‘…
2ğ¿âˆ’ğ‘–, thus
X0Yğ‘‡
0+ğ‘™ğ‘ğ‘ ğ‘¡âˆ‘ï¸
ğ‘–=1ğœ€ğ‘–ËœXğ‘–ğ¹ËœYğ‘–ğ¹
â‰¤ğ‘ğ‘…
2ğ¿âˆ’1+ğ‘™ğ‘ğ‘ ğ‘¡âˆ‘ï¸
ğ‘–=11
2ğ‘–ğ¿ğ‘ğ‘…
2ğ¿âˆ’ğ‘–
â‰¤ğ‘ğ‘…
2ğ¿âˆ’1+2ğ¿ğ‘ğ‘…
2ğ¿ğ¿
=ğ‘ğ‘…
2ğ¿
=ğœ€ğ‘,
We constrain that 1â‰¤âˆ¥xâˆ¥2â‰¤ğ‘…and1â‰¤âˆ¥yâˆ¥2â‰¤ğ‘…. Thenğ‘â‰¤
âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹, thusğœ€ğ‘â‰¤ğœ€âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹.
D PROOF OF THEOREM 3
During the move from buffer to Level 1, EH-SCOD handles more
columns without increasing the space budget, and simultaneously
expands the size range of all blocks by a multiple of ğœ…, whereğœ…=
ğ‘‚(ğœ€ğ‘
â„“). For a block in level ğ‘–, its size satisfies 2ğ‘–âˆ’1â„“ğœ…â‰¤ğ‘ ğ‘–ğ‘§ğ‘’â‰¤2ğ‘–â„“ğœ….
Similar to Lemma 2, we obtain ğ¿â‰¤log
2
â„“ğœ…ğ‘âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹
. Since
the maximum number of blocks per level remains ğ‘, EH-SCOD
reduces the total number of blocks that need to be maintained forthe same window size. Assuming any column pair xandymeets
âˆ¥xâˆ¥âˆ¥yâˆ¥ â‰¤ğ‘…, it is evident that âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹â‰¤ğ‘ğ‘…. We thus
derive the new space complexity:
ğ¿ğ‘â„“(ğ‘šğ‘¥+ğ‘šğ‘¦)â‰¤log2
â„“ğœ…ğ‘âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹8
ğœ€â„“(ğ‘šğ‘¥+ğ‘šğ‘¦)
â‰¤8â„“
ğœ€logâˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹
4ğ‘
(ğ‘šğ‘¥+ğ‘šğ‘¦)
=ğ‘‚
(ğ‘šğ‘¥+ğ‘šğ‘¦)â„“2logğ‘…
.
Not only is there a substantial reduction in space overhead, but the
decrease in merge operations also significantly cuts down on time
costs. The amortized time expense for maintaining the sequence of
sketches isğ‘‚((ğ‘šğ‘¥+ğ‘šğ‘¦)â„“ğ¿)=ğ‘‚((ğ‘šğ‘¥+ğ‘šğ‘¦)â„“logğ‘…).
We conduct a brief analysis of the error introduced by EH-SCOD.
LetÂ¯Xğ‘–,Â¯Yğ‘–=ğ‘†ğ‘ƒğ‘€(ËœXğ‘–,ËœYğ‘–), and assume that the error introduced
by the SPM compression can be bounded byËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Â¯Xğ‘–Â¯Yğ‘‡
ğ‘–
2â‰¤
ğœâˆ¥ËœXğ‘–âˆ¥ğ¹âˆ¥ËœYğ‘–âˆ¥ğ¹. The error from expiring block is still defined by its
size: 2ğ¿â„“ğœ…â‰¤ğœ€
4âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹. The upper bound for the approxima-
tion error of blocks in level ğ‘–isËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–
2â‰¤ËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Â¯Xğ‘–Â¯Yğ‘‡
ğ‘–
2+Â¯Xğ‘–Â¯Yğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–
2â‰¤ ğœ+ğœ€
82ğ‘–â„“ğœ…. Therefore, the second part of
the error isÃğ¿
ğ‘–=1ğ‘ ğœ+ğœ€
82ğ‘–â„“ğœ…â‰¤ 4ğœ+ğœ€
2âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹. And
the third part, the sketch merging error remains unchanged at
ğœ€
8âˆ¥Xğ‘Šâˆ¥ğ¹âˆ¥Yğ‘Šâˆ¥ğ¹. By the choice of ğ‘and the property of the SPM
method [25], we have ğœ=ğœ€
32and thus achieve the conclusion.
E PROOF OF THEOREM 4
We conduct a brief analysis of the error introduced by DI-SCOD.
By the property of the SCOD method [ 20], we can still guaranteeËœXğ‘–ËœYğ‘‡
ğ‘–âˆ’Ağ‘–Bğ‘‡
ğ‘–â‰¤ğœ€ğ‘–ËœXğ‘–ğ¹ËœYğ‘–ğ¹. Thus, we also have
Xğ‘ŠYğ‘‡
ğ‘Šâˆ’ABğ‘‡â‰¤X0Yğ‘‡
0+ğ‘™ğ‘ğ‘ ğ‘¡âˆ‘ï¸
ğ‘–=1ğœ€ğ‘–ËœXğ‘–ğ¹ËœYğ‘–ğ¹.
Similar to appendix C, we can getXğ‘ŠYğ‘‡
ğ‘Š,ABğ‘‡2â‰¤ğœ€âˆ¥Xâˆ¥ğ¹âˆ¥Yâˆ¥ğ¹.
For space complexity, DI-SCOD dose not use additional space.
Thus, the DI-SCOD algorithm only requires ğ‘‚(â„“ğ‘…log2(â„“ğ‘…))space
cost.
For time complexity, the amortized time for the SPM and CS
procedures to process a column is
ğ‘‚ğ‘›ğ‘›ğ‘§(Xğ‘Š)+ğ‘›ğ‘›ğ‘§(Yğ‘Š)
ğ‘ğœ€+ğ‘šğ‘¥+ğ‘šğ‘¦
ğ‘ğœ€2
.
In the worst-case scenario, SPM and CS operations are performed
concurrently across all ğ¿levels. Thus, the time complexity amounts
toğ‘‚ğ‘›ğ‘›ğ‘§(Xğ‘Š)+ğ‘›ğ‘›ğ‘§(Yğ‘Š)
ğ‘ğœ€+ğ‘šğ‘¥+ğ‘šğ‘¦
ğ‘ğœ€2
logğ‘…
ğœ€
.
3906