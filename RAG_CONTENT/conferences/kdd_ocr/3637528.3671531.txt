Modeling User Retention through Generative Flow Networks
Ziru Liuâ€ 
City University of Hong Kong
Hong Kong, China
ziruliu2-c@my.cityu.edu.hkShuchang Liuâ€ 
Kuaishou Technology
Beijing, China
liushuchang@kuaishou.comBin Yang
Kuaishou Technology
Beijing, China
yangbin11@kuaishou.com
Zhenghai Xue
Nanyang Technology University
Singapore, Singapore
zhenghai001@e.ntu.edu.sgQingpeng Cai*
Kuaishou Technology
Beijing, China
caiqingpeng@kuaishou.comXiangyu Zhao*
City University of Hong Kong
Hong Kong, China
xianzhao@cityu.edu.hk
Zijian Zhang
City University of Hong Kong
Hong Kong, China
zijian.zhang@my.cityu.edu.hkLantao Hu
Kuaishou Technology
Beijing, China
hulantao@kuaishou.comHan Li
Kuaishou Technology
Beijing, China
lihan08@kuaishou.com
Peng Jiang*
Kuaishou Technology
Beijing, China
jiangpeng@kuaishou.com
ABSTRACT
Recommender systems aim to fulfill the userâ€™s daily demands. While
most existing research focuses on maximizing the userâ€™s engage-
ment with the system, it has recently been pointed out that how
frequently the users come back for the service also reflects the
quality and stability of recommendations. However, optimizing this
user retention behavior is non-trivial and poses several challenges
including the intractable leave-and-return user activities, the sparse
and delayed signal, and the uncertain relations between usersâ€™ re-
tention and their immediate feedback towards each item in the
recommendation list. In this work, we regard the retention signal
as an overall estimation of the userâ€™s end-of-session satisfaction and
propose to estimate this signal through a probabilistic flow. This
flow-based modeling technique can back-propagate the retention
reward towards each recommended item in the user session, and we
show that the flow combined with traditional learning-to-rank ob-
jectives eventually optimizes a non-discounted cumulative reward
for both immediate user feedback and user retention. We verify the
effectiveness of our method through both offline empirical studies
on two public datasets and online A/B tests in an industrial platform.
The source code is accessible to facilitate replication1.
1https://github.com/Applied-Machine-Learning-Lab/GFN4Retention
â€ Co-first authors. * Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671531CCS CONCEPTS
â€¢Information systems â†’Recommender systems.
KEYWORDS
Recommender Systems, Generative Flow Networks, Retention Op-
timization
ACM Reference Format:
Ziru Liuâ€ , Shuchang Liuâ€ , Bin Yang, Zhenghai Xue, Qingpeng Cai*, Xiangyu
Zhao*, Zijian Zhang, Lantao Hu, Han Li, and Peng Jiang*. 2024. Modeling
User Retention through Generative Flow Networks. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671531
1 INTRODUCTION
In the era of information abundance, recommender systems have
become essential tools that guide users to content that resonates
with their personal preferences [ 1]. Traditional metrics used to
evaluate these systems â€” such as clicks, likes, and ratings â€” are
adept at capturing user preferences for each recommended item
[17,65] and are formulated as targets to guide the optimization
of the recommender systems. Despite their effectiveness, they es-
sentially estimate the userâ€™s immediate feedback of items and are
incapable of providing a comprehensive assessment of usersâ€™ long-
term engagement [ 53,57]. For example, when the system finds
that items with compelling features (e.g. addictive content) can
maximize the click rate, it may decide to continuously recommend
such items. However, these features may initially be appealing but
quickly lose the userâ€™s fondness. This discrepancy suggests a gap
between the userâ€™s immediate interest in an item and the sustain-
able interest [ 50] of the system. As a resolution, long-term metrics
are adopted to offer deeper insights into usersâ€™ overall satisfaction.
One typical example is the user retention signal that describes the
5497
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziru Liu et al.
userâ€™s return-to-app behavior. This metric is one of the most critical
performance estimators for many online services since it closely
correlates with the pivotal business indicator, i.e. Daily Active Users
(DAU) [72].
In practice, modeling and optimizing user retention is a chal-
lenging task because of its between-session nature. Specifically,
the retention behavior does not happen until the user leaves the
current session and returns at the beginning of the next session.
And it has no clear relation to any single recommendation step in
previous interactions. Furthermore, the userâ€™s activity between the
two consecutive sessions is assumed unobservable for the service,
introducing extra uncertainty. To circumvent these challenges, ev-
idence has found promising results using reinforcement learning
(RL) to optimize the cumulative reward for the entire user interac-
tion sequence [ 6,14] as a surrogate. Intuitively, the user returns
to the platform because the systemâ€™s overall impression is suffi-
ciently positive and attractive, which can be partially measured
by the sum of (positive feedback related) rewards in the session.
RL-based recommendation solutions solve the optimization of this
cumulative reward by formulating the user interaction sequence as
a Markov Decision Process (MDP) and learning a policy that con-
siders the long-term impact of each recommendation action [ 2,69].
This allows them to dynamically tailor recommendations at each
point of user interaction, adapting to the evolving preferences and
optimizing the cumulative reward of the whole session.
Yet, the relation between the session-level cumulative reward
and user retention is still unclear, so the authors in [ 6] further show
that the RL-based method may directly integrate the cross-session
retention signal into the long-term value estimation. Though effec-
tive, this method is not designed to investigate the influence of each
interaction on the retention signal (denoted as â€œretention attribu-
tionâ€) and it also indirectly optimizes the retention with cumulative
immediate rewards as a surrogate. Besides, all RL-based solutions
may suffer from the exploration and exploitation trade-off [ 16] that
limits their performance on unstable metrics. This instability is
particularly pronounced in scenarios where user retention dynam-
ics are complex, uncertain, and rapidly evolving. In general, we
want to have a stable exploratory solution that can simultaneously
optimize user retention and immediate rewards.
Inspired by the recent development of Generative Flow Networks
(GFNs)[ 3,28,40â€“43,59,60,60], we propose an alternative approach
GFN4Retention that considers the session-level recommendation
as a generation task where the retention signal is directly mod-
eled by the trajectory generation probability. Similar to the general
GFN formulation, a probabilistic generation process will finally
construct a user session (trajectory) where the target retention re-
ward is matched only by the end of the process. Specifically, each
recommendation step is considered as a conditional forward proba-
bilistic flow to the next user state and each user state is associated
with a flow estimator that represents the generation probability
of reaching this state. During optimization, the end-of-session ter-
minal state directly matches the generation probability with the
retention reward. For non-terminal states, a flow matching learning
objective that incorporates an additional backward probabilistic
flow is used to back-propagate the retention reward towards every
step in the sequence. This would implicitly model each recommen-
dation actionâ€™s retention attribution. As discussed in [ 4], GFNs havedemonstrated remarkable strength in generating diverse objects
of high quality, which naturally solves the aforementioned explo-
ration challenge of RL. Nevertheless, incorporating GFNs in the
recommendation task with retention optimization also brings new
challenges. In its design, GFNs may find it difficult to track the nu-
anced changes in user engagement of each recommendation step,
since it originally assumes the absence of intermediate rewards.
As a countermeasure, we derive an integrated reward system that
can balance the immediate reward and the retention attribution
in each recommendation step. The final reward of GFN4Retention
derives a refined detailed balance loss that matches the session-level
generation probability with the product of the retention reward
and the non-discounted cumulative reward of immediate feedback.
Secondly, the recommendation space is combinatorially large and
the list-wise recommendation is generated and represented by a
point in the continuous vector space [ 6,35,55] in practice. We
show that the flow matching property still holds in this continuous
space and illustrate how to design each flow estimation component
accordingly. To this end, we summarize our key contributions in
this paper as follows:
â€¢We emphasize the importance of retention optimization in real-
world RS applications and introduce GFN4Retention learning
framework designed for enhancing user retention while main-
taining good exploration.
â€¢Our proposed solution innovatively derived an integrated reward
design with a refined detailed balance objective that can control
the trade-off between the immediate reward and the retention
attribution in each recommendation step. Besides, we also pro-
pose to optimize the flow-matching objectives in the continuous
action space to accommodate the large recommendation space
of the item list.
â€¢We validate the superiority of GFN4Retention through extensive
experiments compared with state-of-the-art RL-based recommen-
dation models on both offline and live experiments, and discuss
the behaviors of each component with ablation study and param-
eter analysis.
2 PRELIMINARIES
This section provides an overview of Generative Flow Networks
and delves into the specific problem in our study: the session-wise
recommendation for retention and immediate reward optimization.
2.1 Generative Flow Networks
In recent years, Generative Flow Networks (GFNs) have emerged as
an innovative force in generative modeling, introducing a method-
ology for learning and sampling within complex distributions. The
core of GFNsâ€™ design is its ability to develop a generation policy
that directs the probabilistic flow across a state space towards ter-
minal states [ 3,4]. Specifically, the general formulation considers
a generation trajectory S={ğ‘ 1â†’ğ‘ 2â†’Â·Â·Â·â†’ ğ‘ ğ‘‡}and assumes a
forward probabilistic policy ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)that determines each action
during the generation process. Here ğ‘ ğ‘¡represents the state at step
ğ‘¡, and ğ‘‡is the terminal step of the trajectory. The goal of GFNs is
to align the generation probability ğ‘ƒ(S)of each trajectory with the
observed reward by the end:
ğ‘ƒ(S)âˆ ğ‘…(ğ‘ ğ‘‡) (1)
5498Modeling User Retention through Generative Flow Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
User 
State 
Encoder
Non-terminal
Terminal
.....
(a) 
User 
State 
Encoding 
Module
(c) 
Learning 
Objective
(b) 
Flow 
Estimation 
Module
.....
Figure 1: Overview of GFN4Retention Framework
which is essentially an energy-based method since the model si-
multaneously serves as the generator and evaluator.
During optimization, the GFN framework introduces a flow esti-
matorF(ğ‘ ğ‘¡)that assesses the likelihood of traversing through a
specific state ğ‘ ğ‘¡. The learning process of flow in GFNs is metic-
ulously calibrated to align with the target distribution, ensuring
equilibrium between the sums of incoming and outgoing flows:
F(ğ‘ ğ‘¡)ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)â‰ˆF(ğ‘ ğ‘¡+1)ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1) (2)
where ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)is the forward probability from ğ‘ ğ‘¡toğ‘ ğ‘¡+1, and
ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)is the corresponding backward probability that models
the likelihood of the source state ğ‘ ğ‘¡given the outcome state ğ‘ ğ‘¡+1.
The foundational work on GFNs has led to the development of
an optimized variant for this objective: the Detailed Balance (DB)
loss [ 4] that minimizes the difference between the forward view and
backward view of each step-wise joint probability ğ‘ƒ(ğ‘ ğ‘¡, ğ‘ ğ‘¡+1). Then
the objective follows the flow matching property in the generation
process which minimizes the Detailed Balance (DB) loss:
minLDB(ğ‘ ğ‘¡, ğ‘ ğ‘¡+1)=
logF(ğ‘ ğ‘¡)ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)
F(ğ‘ ğ‘¡+1)ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)2
(3)
By minimizing the DB loss, it strives to achieve a flow representation
that is highly indicative of the target distribution.
2.2 Problem Definition
In our research, we focus on the problem of session-wise recom-
mendation, which aims to iteratively suggest items to users and
maximize both retention and immediate feedback of a user session.
The session-wise recommendation for a short video application
scenario is illustrated in Figure 2. Formally, we consider a set of
usersUand a set of itemsC. For each session, at any given step ğ‘¡,
we may receive a recommendation request from user ğ‘¢âˆˆU, which
consists of a user feature set Ağ‘¢, the userâ€™s interaction history Hğ‘¢,ğ‘¡.
In recommender systems, the user request provides the necessary
context information to encode the current user state ğ‘ ğ‘¡. Given the
user request and the encoded state, the recommendation policy gen-
erates an action ğ‘ğ‘¡that corresponds to a list of items selected fromC. Then the user provides feedback of Bbehavior types (e.g. clicks,
likes, and comments) for these items which is used to calculate an
immediate reward ğ‘Ÿğ‘¡:
ğ‘Ÿğ‘¡=âˆ‘ï¸
ğ‘âˆˆBğœ”ğ‘Â·ğ‘¦ğ‘¡,ğ‘ (4)
where ğ‘¦ğ‘¡,ğ‘represents the userâ€™s feedback in behavior ğ‘at step ğ‘¡,
andğœ”ğ‘is the weight for behavior ğ‘. By the end of the session (i.e.
atğ‘ ğ‘‡), we also observe the user retention reward Rdefined as
the user return frequency which is the core metric in this work.
We organize each sample as the tuple (S, ğ‘1, . . . , ğ‘ğ‘‡, ğ‘Ÿ1, . . . , ğ‘Ÿğ‘‡,R)
that consists of the observed states (i.e. user requests), actions,
immediate rewards, and the retention reward of a session. And
we set two goals in our problem setting: 1) find a valid reward
design ğ‘…(S)=ğ‘“(ğ‘Ÿ1, . . . , ğ‘Ÿğ‘‡,R)that combines the retention reward
and the cumulative immediate rewards, and helps boost the overall
recommendation performance; 2) learn a recommendation policy
that explores and achieves a better combined reward of ğ‘…(S).
Recommender 
System
UserRequestVideo
 ListImmediate
ResponseOpen/Return
APPLeave
APPOpen/Return
APP
request t
session i session i+1 return time
Figure 2: Session-wise Recommendation Example
3 THE PROPOSED FRAMEWORK
In this section, we present our GFN4Retention framework. As illus-
trated in Figure 1, the main framework includes a feature extraction
module that encodes user request context into the user state, a flow
estimation module that models the generative flow of user sessions,
and a modified detailed balance learning objective with integrated
reward design.
5499KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziru Liu et al.
Positional 
Embedding
Transformer
Items 
Embedding
User 
Embedding
.
.
DNN
DNN
Concate
Context 
Embedding
Figure 3: Framework of User State Encoder
3.1 User State Encoding
While real-world online platforms heavily depend on navigating
the intricate user dynamics as well as their static features, the
challenge of retention optimization in this scenario is not only the
identification of diverse user patterns but also the agility to adjust
to rapid changes in these patterns. In our design, upon receiving
a recommendation request from a user ğ‘¢âˆˆU, we consider two
major types of input including the userâ€™s feature set Ağ‘¢and the
interaction history Hğ‘¢,ğ‘¡. To better employ the dynamics in the user
history and find patterns in the itemsâ€™ mutual influences, we first
process the history Hğ‘¢,ğ‘¡with a transformer, and the last output
embedding is considered as the history encoding. Then, a user
feature embedding generated from Ağ‘¢is concatenated with the
history encoding and further processed by a neural network to
generate an embedding eğ‘¢that forms the first part of the user state
ğ‘ ğ‘¡. In practice, we found that merely using a transformer to encode
user history may over-amplify the most recent history and may
ignore the feature-level interactions. As a result, we include a DNN-
based context-detecting module that encodes all the contexts in the
user requests and outputs an addition embedding ğœ“ğ‘¢that forms the
second part of the user state. The detailed framework of the feature
encoder and context-detecting module is illustrated in Figure 3,
which details Part (a) of Figure 1.
3.2 Recommendation Policy as Forward Flow
During inference, with the encoded user state ğ‘ ğ‘¡, we can generate
the action for the recommendation. As widely adopted in many rec-
ommender systems, the service has to recommend a list of items for
each user request to meet the latency requirement of usersâ€™ frequent
browsing behaviors. This means that it is impractical to directly
consider the enormous item set Cas the action space. Alternatively,
we choose to consider a vector space for each action ğ‘ğ‘¡, this vector
can represent the output item list through a deterministic top-K
selection module [ 35]. In our design, the policy network (i.e. the
forward flow estimator) will first output the statistics of the Gauss-
ian distribution ğœ‡, ğœ=ğœ™fw(ğ‘ ğ‘¡)and then sample the action vector
asğ‘ğ‘¡âˆ¼N( ğœ‡, ğœ). Note that this setting is mathematically different
from the original design of GFNs where a small discrete action
space is adopted. Fortunately, the flow matching property in the
continuous vector space still holds (explained in Appendix D) and
we can safely apply the flow estimation and the detailed balance
optimization as we will present in the following sections. During
training, the recommendation policy is regarded as the forward
flow function ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)which assumes that the output action ğ‘ğ‘¡
determines the next state ğ‘ ğ‘¡+1.3.3 Retention Flow Estimation
Following the general design of GFNs, we include a flow estimator
F(ğ‘ ğ‘¡)of states and a backward flow function ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)that esti-
mates the posterior. In the session-level viewpoint, each observed
session Sis a probabilistic trajectory generated by the recommenda-
tion policy. Each state ğ‘ ğ‘¡may diverge into different future states ac-
cording to the sampling process in ğ‘ƒğ¹, and it may be reachable from
various previous states. Intuitively, the state flow estimator F(ğ‘ ğ‘¡)
represents how likely a state ğ‘ ğ‘¡is reached. The backward function
ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)=ğœ™bw(ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ‘ ğ‘¡+1)takes the current state-action and
the next state as input and estimates how likely the next state is
generated by the current state. To guarantee the property F(ğ‘ ğ‘¡)â‰¥0
andğ‘ƒğµ(Â·)â‰¥ 0, we use sigmoid activation for the network output.
Then, we can match the trajectory generation likelihood with the
observed retention reward and use the flow matching objective
(i.e. Eq (3)) to back-propagate this end-of-session retention signal
towards each intermediate step:
LDB=ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³
logFğ‘…(ğ‘ ğ‘¡)Â·ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)
Fğ‘…(ğ‘ ğ‘¡+1)ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)2
1â‰¤ğ‘¡â‰¤ğ‘‡âˆ’1

logFğ‘…(ğ‘ ğ‘¡)
R2
ğ‘¡=ğ‘‡(5)
where the forward function ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)depends on the recommen-
dation policy ğœ™fwas described in the previous section and we use
Fğ‘…(ğ‘ ğ‘¡)to represents that the flow estimator in this framework only
considers the retention reward. As proven by [ 4], this design en-
sures the learning goal ğ‘ƒ(S)âˆFğ‘…(ğ‘ ğ‘‡)â‰ˆR . During the learning
process, the generation may initially be random and generate low
retention reward, but the exploration effect of the policy would
gradually discover samples with higher retention and the actions
in those sessions will receive higher generation possibility. Eventu-
ally, this flow estimation learning framework helps the generation
policy provide more diversity while maintaining high quality.
3.4 Refined Detailed Balance Learning with
Reward Integration
Merely optimizing the retention as in the previous section only mod-
els the long-term preferences of users. In contrast, the immediate
rewards in each intermediate step of the session provide valuable
nuanced information that may be ignored by the long-term reten-
tion reward. For example, a session that consists of a relevant item
and an irrelevant item may still receive a good retention reward
since the user may return as long as there is relevant information.
However, we should not consider the two items as equal and the
userâ€™s feedback on each of the items may help us differentiate them.
Intuitively, we consider the retention reward and immediate re-
wards as complementary views of user preferences. In this section,
we illustrate how to integrate these two rewards in the generative
flow estimation and derive a refined DB learning process.
3.4.1 Reward Design: To accommodate the flow estimation frame-
work and guarantee the correctness of the detailed balance objective,
we propose a reward integration through the product:
ğ‘…(S)=RÂ·ğ‘’ğ›¼Â·Ãğ‘‡âˆ’1
ğ‘¡=1ğ‘Ÿğ‘¡ (6)
5500Modeling User Retention through Generative Flow Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
where ğ›¼is the parameter to balance the importance of user imme-
diate rewards and we will conduct further analysis for this term
in Section 4.7 to illustrate the relations. The cumulative reward
ğ‘’Ãğ‘‡âˆ’1
ğ‘¡=1ğ‘Ÿğ‘¡is non-discounted and takes the exponential form as in
analogy in supervised learning (i.e. binary cross-entropy) and RL-
based solutions (i.e. TD3) where each step aims to learn the log
scale policy output with the immediate reward. As we will illus-
trate in the following sections, this design derives a flow matching
objective with a simple immediate reward augmentation. The flow
estimation process is detailed in Part (b) of Figure 1.
3.4.2 Integrated Flow Matching. Following the aforementioned
reward integration, we propose to decompose the flow estimator
of a given state ğ‘ ğ‘¡into two corresponding components:
F(ğ‘ ğ‘¡)=Fğ‘…(ğ‘ ğ‘¡)Â·(Fğ¼(ğ‘ ğ‘¡))ğ›¼(7)
whereFğ‘…(ğ‘ ğ‘¡)matches the flow of retention reward and Fğ¼(ğ‘ ğ‘¡)
matches the accumulated immediate rewards before step ğ‘¡:
Fğ¼(ğ‘ ğ‘¡)=ğ‘’Ãğ‘¡âˆ’1
ğ‘—=1ğ‘Ÿğ‘—(8)
Different fromFğ‘…that needs to be optimized along with the GFN
modules,Fğ¼is a non-parametric function that only depends on the
observed immediate reward. Specifically for the terminal state, we
haveFğ‘…(ğ‘ ğ‘‡)=RandFğ¼(ğ‘ ğ‘¡)=ğ‘’Ãğ‘‡âˆ’1
ğ‘—=1ğ‘Ÿğ‘—. Then, based on the flow
matching objective, we can back-propagate the integrated reward
towards intermediate steps:
F(ğ‘ ğ‘¡)Â·ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)=F(ğ‘ ğ‘¡+1)Â·ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1) (9)
Combining Eq. (7), Eq. (8), and Eq. (9) together, we may derive
the following simplified relation:
Fğ‘…(ğ‘ ğ‘¡)(Fğ¼(ğ‘ ğ‘¡))ğ›¼ğ‘ƒğ¹(ğ‘ğ‘¡|ğ‘ ğ‘¡)=Fğ‘…(ğ‘ ğ‘¡+1)(Fğ¼(ğ‘ ğ‘¡+1))ğ›¼ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)
(ğ‘’Ãğ‘¡âˆ’1
ğ‘—=1ğ‘Ÿğ‘—)ğ›¼Â·Fğ‘…(ğ‘ ğ‘¡)ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)=(ğ‘’Ãğ‘¡
ğ‘—=1ğ‘Ÿğ‘—)ğ›¼Â·Fğ‘…(ğ‘ ğ‘¡+1)ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)
Fğ‘…(ğ‘ ğ‘¡)ğ‘ƒğ¹(ğ‘ ğ‘¡+|ğ‘ ğ‘¡)=ğ‘’ğ›¼ğ‘Ÿğ‘¡Â·Fğ‘…(ğ‘ ğ‘¡+1)ğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)
(10)
The Eq. (10) suggests that within the realm of accurate model pre-
dictions, an increased action probability in ğ‘ƒğ¹correlates with either
an enhanced immediate reward or a higher potential flow for future
retention rewards.
3.4.3 Integrated Detail Balance Objective. Following the flow match-
ing in Eq. (10), we derive the log-scale detailed balance learning
objective as the following:
LDB=ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³(logFğ‘…(ğ‘ ğ‘¡)+logğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)âˆ’logFğ‘…(ğ‘ ğ‘¡+1)
âˆ’logğ‘ƒğµ(ğ‘ ğ‘¡|ğ‘ ğ‘¡+1)âˆ’ğ›¼Â·ğ‘Ÿğ‘¡)21â‰¤ğ‘¡â‰¤ğ‘‡âˆ’1
(logFğ‘…(ğ‘ ğ‘¡)âˆ’logR)2ğ‘¡=ğ‘‡
(11)
where each stepâ€™s immediate reward ğ‘Ÿğ‘¡appears and only appears
in the corresponding step-wise DB loss, and the terminal state at
ğ‘¡=ğ‘‡does not observe an immediate reward and only matches the
retention flow. This learning objective is further illustrated in Part
(c) of Figure 1. Systematically, our design of flow Fğ¼for immediate
rewards is non-parametric and can naturally integrate into the
DB objective with a simple extra term. In contrast, the retention
reward requires the learning of Fğ‘…and the flow back-propagation
with flow matching implicitly achieves the â€˜retention attributionâ€™.The overall learning framework optimizes towards the integrated
goal of ğ‘ƒ(S)âˆF(ğ‘ ğ‘‡)=Fğ‘…(ğ‘ ğ‘‡)(Fğ¼(ğ‘ ğ‘‡))ğ›¼. In general, we regard
retention and immediate rewards as two complementary aspects of
the userâ€™s impression of the recommendation policy. In our solution,
immediate rewards provide direct guidance in each step, while the
retention flow guides the policy with step-wise attribution.
Besides, the values of forward probability ğ‘ƒğ¹(Â·)may approach
zero deviating significantly from the valid region of the flow esti-
mator in the log scale. This discrepancy can introduce high vari-
ance that may influence the stability of the gradient computation.
Therefore we include a bias ğ›½ğ¹as the hyperparameter and the cor-
responding log scale estimation becomes log(ğ‘ƒğ¹(Â·)+ ğ›½ğ‘“). Similarly,
we also include ğ›½ğµto stabilize the backward function learning and
ğ›½ğ‘Ÿto reduce the reward variance.
4 EXPERIMENT
In this section, we present a comprehensive performance evalu-
ation of the GFN4Retention framework through experiments on
a simulated user environment for two real-world datasets. Addi-
tionally, we extend our evaluation to include online A/B testing
conducted on a commercial platform to validate GFN4Retentionâ€™s
effectiveness in a live environment. The implementation details are
provided in the Appendix A.
4.1 Dataset
We carried out our experiment using two real-world datasets.
â€¢Kuairand-Pure2is an unbiased sequential recommendation
dataset characterized by random video exposures.
â€¢MovieLens-1M3, a widely-used benchmark for RSs, boasts a
more extensive scale but with a sparser distribution.
The KuaiRand dataset comprises 12 feedback signals, out of
which we focus on six positive feedback signals: â€˜clickâ€™, â€˜view timeâ€™,
â€˜likeâ€™, â€˜commentâ€™, â€˜followâ€™, and â€˜forwardâ€™. We also consider two
negative feedback signals: â€˜hateâ€™ and â€˜leaveâ€™, due to their relevance.
Feedback signals that occur less frequently are not included in our
study to maintain analytical clarity. Additionally, we extend our
analysis to the ML-1m dataset, a widely recognized benchmark in
the field of Recommender Systems, which contains ratings from
6,014 users for 3,417 movies. For the ML-1m dataset, we classify
movies rated above 3 as positive instances (indicative of a â€˜likeâ€™)
and the others as negative instances (indicative of a â€˜hateâ€™), thus
enabling a nuanced understanding of user preferences. The statistics
of datasets are presented in Table 1.
Table 1: Datasets Statistics
Dataset # Users # Items # Interactions # Density
Kuairand-Pure 27,285 7,551 1,436,609 0.70%
MovieLens-1M 6,400 3,706 1,000,208 4.22%
2https://kuairand.com/
3https://grouplens.org/datasets/movielens/1m/
5501KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziru Liu et al.
4.2 Simulated User Environment
We have chosen the KuaiSim retention simulator [ 62] for our study,
which is designed to emulate long-term user behavior on short
video recommendation platforms. KuaiSim features two key com-
ponents: a leave module, responsible for predicting the likelihood
of a user exiting a session and thereby ending an episode; and a
return module, which estimates the daily probability of a userâ€™s
return to the platform, expressed as a multinomial distribution.
4.3 Baselines
Our model is benchmarked against a diverse array of baselines, in-
cluding several classic reinforcement learning methods and models
specially designed for retention optimization:
â€¢CEM [19]: Often employed as a surrogate in recommendation
tasks, this method is known for its effectiveness and simplicity
in various RL scenarios.
â€¢DIN [71]: This network uniquely features a local activation unit,
enabling dynamic learning of user interest representations from
historical behaviors.
â€¢TD3 [22]: Building on the foundation of DDPG, TD3 enhances
performance through techniques like clipped double-Q learning,
delayed policy updates, and target policy smoothing.
â€¢SAC [24]: An advanced off-policy RL algorithm adopts strategies
including clipped double-Q learning for improved stability.
â€¢RLUR [6]: Specifically designed for enhancing long-term user
engagement, this RL-based algorithm focuses on strategies that
cater to sustained user interactions.
4.4 Evaluation Metrics
We assess the efficacy of GFN4Retention using metrics that focus
on both immediate user interactions and long-term retention:
â€¢Return Time: Measuring the average duration between consec-
utive sessions, serving as a direct indicator of user retention.
â€¢Retention: Capturing the average value of user retention reward
as defined in Subsection 2.2, reflecting long-term engagement.
â€¢Click Rate: Calculating the rate of clicks per session, indicating
immediate user engagement with the content.
â€¢Long View Rate: Determining the likelihood of a sessionâ€™s view
time exceeding 50%, showcasing deeper content engagement.
â€¢Like Rate: Assessing the rates of likes per session, further indi-
cating user preference and satisfaction.
For a robust evaluation, we derive our final results by averaging
the outcomes of the last 1000 episodes during the training phase.
4.5 Overall Performance
To assess the effectiveness of our proposed GFN4Retention model,
we conducted a comparative analysis of its overall performance
against five baseline models on two datasets. The results are de-
tailed in Table 2. Additionally, we present the training curves of
GFN4Retention alongside selected baselines focusing on the return
day metric in Figure 4. From these observations, we note that:
â€¢The TD3 model registers the weakest performance in terms of
retention metrics. It exhibits higher â€˜return timeâ€™ across both
datasets, suggesting increased intervals between user sessions
and, hence, lower retention rates. This model does not excel in
0 2000 4000 6000 8000 10000
Episode (x10)1.51.61.71.81.92.02.1Return DayTraining Curve for Return Day Metric
CEM DIN RLUR GFN4Retention(Ours)Figure 4: Training Curve for Return Day Metric.
any metric, likely due to its inadequate adaptation to shifts in the
environment distribution and a policy weakly linked to specific
user behavior patterns, resulting in suboptimal performance.
â€¢Among all the baseline models, the RLUR model stands out in
retention metrics and shows commendable results in optimiz-
ing immediate user feedback. This algorithmâ€™s design, which
acknowledges the inherent biases of sequential recommenda-
tion tasks, adeptly captures user retention dynamics. Despite its
strengths, the RLUR suffers from training volatility and requires
more iterations to reach convergence.
â€¢The SAC model emerges as the most proficient baseline in op-
timizing immediate user feedback. It boasts competitive perfor-
mance across all metrics and leads in Long View Rate for the
Kuairend-Pure dataset. Its approach to balancing expected re-
turns with policy entropy enables effective modeling of user
engagement.
â€¢Our GFN4Retention model surpasses all other models, including
the best baseline models, on several crucial metrics. It achieves
the lowest â€˜return timeâ€™, indicating more frequent user engage-
ment, and secures the highest scores in Retention and Like Rate,
with statistically significant improvements. By integrating imme-
diate feedback with the final retention signal in a meticulously
structured manner, GFN4Retention boosts user retention while
preserving the quality of immediate user feedback. The modelâ€™s
consistency and robustness are further evidenced by the most
stable training curves among all baselines.
In summary, the GFN4Retention model demonstrates superior per-
formance by effectively balancing immediate engagement with user
retention, as indicated by its leading scores in critical metrics and
significant improvements over the baseline models.
4.6 Ablation Study
To rigorously assess the contribution of individual modules within
our proposed GFN4Retention model, we conducted an ablation
study focusing on the context-detection module and the reward
design. This study involves evaluating variations of the full model
with the omission of particular settings to quantify their respective
impacts. We describe the variant models as follows:
â€¢NCD (No Context Detection): This model functions without
the context-detection module, yet retains all other components,
providing insight into the significance of context awareness in
the user state encoder.
5502Modeling User Retention through Generative Flow Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Overall Performance on two datasets for different models.
Dataset MetricModel
TD3 SAC DIN CEM RLUR GFN4Retention
Kuairend-PureReturn Time 2.382 2.373 1.947 1.889 1.786 1.496*
Retention 0.151 0.150 0.154 0.156 0.159 0.163*
Click Rate 0.800 0.801 0.773 0.762 0.789 0.805
Long View Rate 0.791 0.795 0.764 0.757 0.778 0.794
Like Rate 0.852 0.857 0.812 0.804 0.831 0.862
ML-1MReturn Time 2.258 2.246 1.893 1.814 1.723 1.479*
Retention 0.141 0.142 0.153 0.158 0.160 0.165*
Click Rate 0.461 0.468 0.454 0.448 0.459 0.473
Long View Rate 0.459 0.463 0.455 0.453 0.457 0.464
Like Rate 0.568 0.571 0.541 0.524 0.561 0.574
â€œ*â€: the statistically significant improvements (i.e., two-sided t-test with ğ‘<0.05) over the best baseline.
Underline : the best baseline model. Bold: the best performance among all models.
â€¢NIF (No Immediate Feedback): In this variation, the reward
structure is simplified to include only the retention reward, thereby
isolating the effect of immediate feedback signals.
â€¢SIF (Simplified Immediate Feedback): Here, we utilize a sim-
plified immediate feedback reward approach, wherein session-
wise accumulated rewards are considered solely at the terminal
state, revealing the value of step-by-step reward accumulation.
The results of the ablation study on the GFN4Retention model,
employing the Kuairand-Pure dataset, are depicted in Figure 5. This
investigation yields several critical insights:
â€¢The NCD variant demonstrates commendable performance in
immediate feedback metrics such as â€˜click rateâ€™ and â€˜like rateâ€™.
However, it exhibits an approximate 6.7% increase in â€˜return
timeâ€™ relative to the comprehensive GFN4Retention model. This
outcome can be principally ascribed to the absence of the context-
detection module, which is pivotal in adaptively discerning user
preferences that are integral to retention. These findings emphat-
ically highlight the significance of the context-detection module
within the modelâ€™s architecture.
â€¢In contrast to the complete GFN4Retention model, the NIF variant
shows a marked 45% reduction in â€˜click rateâ€™ and a 5% reduction in
â€˜like rateâ€™. The lack of immediate feedback signals during the mod-
eling process likely precipitates this decline. Additionally, there is
a 4.7% increase in â€˜return timeâ€™ compared to the GFN4Retention
model, suggesting that only focusing on retention rewards is
insufficient for capturing user preferences.
â€¢The SIF variant, which incorporates immediate feedback signals
into the terminal session reward, outperforms the NIF variant,
registering gains in both short-term engagement metrics and
user retention. This enhancement underscores the criticality of
maintaining an equilibrium between immediate and sustained
user engagement.
â€¢The SIF variant exhibits a 40% decrease in â€˜click rateâ€™ and a 4%
decrease in â€˜like rateâ€™, along with a 4% increase in â€˜return timeâ€™,
compared to the GFN4Retention model. These comparative met-
rics clearly demonstrate that our innovative reward structure,
coupled with a refined detailed balance objective, significantly
NCDNIF SIFGFN1.471.491.511.531.551.571.591.61(a) Return Time
NCDNIF SIFGFN0.200.280.360.440.520.600.680.760.84(b) Is_click Rate
NCDNIF SIFGFN0.780.790.800.810.820.830.840.850.86(c) Is_like RateFigure 5: Ablation Study Results.
bolsters the modelâ€™s proficiency in enhancing both immediate
feedback and long-term user retention.
4.7 Parameter Analysis
To calibrate the significance of immediate user rewards within our
model, we introduce the parameter ğ›¼as part of the session-wise
reward formulation, as delineated in Equation (6). This subsection
is devoted to analyzing the impact of this balance parameter on
model performance, particularly how the integration of immediate
feedback signals affects outcomes. We assess the modelâ€™s perfor-
mance across a range of ğ›¼values set at [0.5,0.7,0.9,1.0,1.1,1.3,1.5]
with a focus on the â€˜return timeâ€™ and â€˜click rateâ€™ metrics, the results
of which are presented in Figure 6. The following patterns emerge
from the analysis:
â€¢When ğ›¼is reduced from 1.0 to 0.5, diminishing the weight of
immediate user rewards, there is a marked decline in the â€˜click
rateâ€™. This drop indicates that with less optimization towards im-
mediate feedback, user engagement correspondingly decreases.
Intriguingly, the â€˜return timeâ€™ metric also exhibits a significant
increase, suggesting that the modelâ€™s reduced focus on user en-
gagement might adversely affect its capacity to capture user
retention dynamics.
â€¢Conversely, as ğ›¼is incremented from 1.0 to 1.5, thereby increasing
the emphasis on immediate rewards, there is a slight decrease in
â€˜click rateâ€™, particularly by 2% when ğ›¼reaches 1.5. This nuanced
decline implies that an overly concentrated focus on immediate
rewards could slightly detract from optimizing for engagement.
Moreover, the heightened attention to immediate rewards seems
5503KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziru Liu et al.
0.40.60.81.01.21.41.6
Balance Parameter1.4901.4951.5001.5051.5101.5151.5201.5251.530Return Time
(a) Return Time
0.40.60.81.01.21.41.6
Balance Parameter0.500.540.580.620.660.700.740.780.82Click Rate
(b) Is_click Rate
Figure 6: Balance Parameter Analysis.
to influence the retention optimization process, as evidenced by
an increased â€˜return timeâ€™.
These observations underscore the delicate balance required in tun-
ing the ğ›¼parameter to harmonize the dual objectives of optimizing
the retention signal, thereby affirming the intricacies involved in
reward structure design within our model.
Candidate 
Items
from 
Previous 
Stage
Multiple 
scores 
for 
each 
item
Ensemble 
scores
Selector
Multiple
Scoring 
Models
All 
Items
Candidate 
Retrieval
...
1st 
Ranking
...
2nd 
Ranking
...
ReRanking
...
User 
Request
Recommendation
Selected 
items 
for 
next 
stage
Figure 7: Deployment of GFN4Retention in live experiments.
The bottom plot gives the detail of the two ranking stages
whereâŠ—represents the ensemble function that takes item
scores from multiple ranking models as input and ğ‘ğ‘¡as
weights of the scores.
4.8 Live Experiment
We conduct the live evaluation of our GFN4Retention solution
through an A/B test on a real-world industrial video recommenda-
tion platform. The system serves billions of user requests every day
and the daily candidate pool is around several million. The over-
all recommender system consists of a relevant candidate retrieval
stage and three rank-and-filter stages that gradually scale down
the number of selected items before it is ready for exposure. As
shown in Figure 7, we deploy GFN4Retention in the ranking score
ensemble modules of two of the ranking stages (i.e. the first ranking
stage and the second-ranking stage) with separately learned policy
models. The output action corresponds to the weights that fuse the
ranking scores from multiple score prediction models. The baseline
in the first ranking stage linearly combines the input ranking scores
with empirically fixed parameters, while the baseline in the sec-
ond ranking stage adopts an RL-based solution that automatically
searches the action space. For each experiment, we holdout 10% of
the total online traffic for the GFN4Retention solution and 20% ofthe total online traffic for the baseline. For the retention signal, we
use the reciprocal return time gap as the retention reward of a user
session and the normalized watch time as an immediate reward.
Correspondingly, the evaluation consists of the next-day user re-
turn frequency and the average watch time which are evaluated
on daily basis. Both metrics are better if larger. We summarize the
results in Table 3 which proves that the GFN4Retention method can
significantly improve the userâ€™s retention and the corresponding
immediate reward. When focusing on the target user group with rel-
atively lower activity in the system, the improvement in retention
is more significant. Note that we have applied the proposed method
in the two major ranking scenarios in the system, indicating its
generalization ability and scalability in different stages.
Table 3: Online A/B Test Performance.
1stStage 2ndStage
overall next-day retention +0.015%* +0.002%
target usersâ€™ next-day retention +0.069%* +0.056%*
watch_time +0.558%* +0.224%*
â€œ*â€: the statistically significant improvements over the baseline.
5 RELATED WORK
In this section, we briefly discuss existing research related to RL-
based recommender systems and retention optimization.
5.1 Reinforcement Learning Based RSs
The application of Reinforcement Learning (RL) for recommen-
dations is justified by the underlying Markov Decision Process
framework, which is fundamental to the RL paradigm [ 2,29,45,
47,52,58,61]. RLâ€™s primary benefit in this context is its focus on
maximizing the expected cumulative reward from user interactions
over time, rather than just enhancing immediate recommendations.
In environments with limited recommendation options, methods
that utilize tabular approaches [ 38,39] or value function estima-
tion [ 25,49,68,70] have been employed to assess the long-term
implications of recommendations. Conversely, in situations char-
acterized by vast action spaces, policy gradient [ 11,13,46], and
actor-critic methodologies [ 5,7,18,21,31,32,36,44,48,54,64,66â€“
68] are preferred for their ability to steer the recommendation policy
toward higher-quality outcomes. The complexity of optimizing for
multiple metrics is addressed in the literature on multi-objective
optimization, highlighting the varying behavioral patterns among
users [ 7,15,51]. To bridge the discrepancy between real-world
user interactions and offline assessments, user simulators have be-
come a pivotal tool for researchers [ 26,62,65]. Our methodology
extends this direction by performing offline training in simulated
cross-session environments for user retention optimization.
In parallel, Generative Flow Networks (GFNs) have surfaced
as a groundbreaking approach [ 3], drawing parallels with RL but
pushing the boundaries in terms of generating diverse, high-quality
5504Modeling User Retention through Generative Flow Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
samples from intricate distributions. Notably, GFNs have demon-
strated their efficacy in tackling list-wise recommendation chal-
lenges [ 34], showcasing their utility in recommendation systems.
Our research leverages GFNs with novel flow-matching formu-
lations and tailored objective functions to refine user retention
optimization. We notice that the generative process of GFN may
remind readers about the diffusion-based methods which have also
been studied in sequential recommendations [ 30,33,56]. However,
they are not designed for sparse and delayed return signal modeling
since they require precise and abundant representations for unsu-
pervised learning. Intuitively, we believe our GFN-based solution
fits better to this problem since it is an energy-based model that can
predict the delayed retention signal while following the iterative
recommendation process.
5.2 Retention Optimization for RSs
Shifting away from the traditional focus on immediate feedback,
the research community has started delving into strategies for
enhancing usersâ€™ long-term satisfaction [ 8,9,12,37,53]. Along this
direction, several studies have investigated methods to boost long-
term user engagement by analyzing metrics like dwell time [ 10,27,
72]. However, the domain of user retention-oriented optimization
remains relatively underexplored. Notably, a few pioneering efforts
have aimed to predict user retention through innovative perspective
[6,20,23,63]. , such as the rationale contrastive multi-instance
learning approach, designed to elucidate the factors influencing
user retention and thereby augment its interpretability [20].
A notable advancement includes leveraging decision transformer-
based models to tackle user retention challenges, and ingeniously
reframing the reinforcement learning (RL) problem as an autore-
gression issue [ 63]. Furthermore, some researchers argue that user
retentionâ€”viewed as feedback accumulated over multiple interac-
tions with the systemâ€”presents a complex challenge in attributing
retention rewards to individual items or sequences [ 6]. They pro-
pose conceptualizing this issue using reinforcement learning to
minimize the cumulative time intervals across sessions. Our work
builds upon these insights into retention optimization, uniquely
considering the integration of immediate feedback within the mod-
eling framework to better capture the nuances of user retention
dynamics. Moreover, our approach has demonstrated efficacy across
both offline datasets and large-scale online platforms.
6 CONCLUSION
In this work, we delve into optimizing user retention within rec-
ommender systems, a critical aspect for fostering sustained user
engagement. Recognizing the intricate nature of long-term user
interactions, we conceptualize the retention signal as a holistic
measure of user satisfaction at the sessionâ€™s conclusion. Our ap-
proach models this comprehensive estimation through a generative
flow, ingeniously back-propagating the retention reward to each
userâ€™s immediate feedback within a session. By employing a simpli-
fied flow matching technique alongside a novel DB loss function,
our model optimizes long-term retention while also integrating
immediate feedback signals. The efficacy of our methodology is
rigorously validated through extensive offline empirical evaluations
on publicly available datasets and real-world online A/B testingon a commercial platform, demonstrating its practical applicability
and effectiveness in enhancing user retention.
ACKNOWLEDGEMENTS
This research was partially supported by Kuaishou, Research Im-
pact Fund (No.R1015-23), APRC - CityU New Research Initiatives
(No.9610565, Start-up Grant for New Faculty of CityU), CityU -
HKIDS Early Career Research Grant (No.9360163), Hong Kong
ITC Innovation and Technology Fund Midstream Research Pro-
gramme for Universities Project (No.ITS/034/22MS), Hong Kong
Environmental and Conservation Fund (No.88/2022), and SIRG
- CityU Strategic Interdisciplinary Research Grant (No.7020046,
No.7020074).
REFERENCES
[1]Giuseppe Aceto, Valerio Persico, and Antonio PescapÃ©. 2020. Industry 4.0 and
health: Internet of things, big data, and cloud computing for healthcare 4.0.
Journal of Industrial Information Integration 18 (2020), 100129.
[2]M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2021. Reinforcement learning
based recommender systems: A survey. ACM Computing Surveys (CSUR) (2021).
[3]Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua
Bengio. 2021. Flow network based generative models for non-iterative diverse
candidate generation. Advances in Neural Information Processing Systems 34
(2021), 27381â€“27394.
[4]Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and
Emmanuel Bengio. 2023. Gflownet foundations. Journal of Machine Learning
Research 24, 210 (2023), 1â€“55.
[5]Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton.
2007. Incremental natural actor-critic algorithms. Advances in neural information
processing systems 20 (2007).
[6]Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, Bin
Yang, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Reinforcing User Retention
in a Billion Scale Short Video Recommender System. In Companion Proceedings
of the ACM Web Conference 2023. 421â€“426.
[7]Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, Ruohan
Zhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, et al .2023. Two-
Stage Constrained Actor-Critic for Short Video Recommendation. In Proceedings
of the ACM Web Conference 2023. 865â€“875.
[8]Wanling Cai and Li Chen. 2020. Predicting user intents and satisfaction with
dialogue-based conversational recommendations. In Proceedings of the 28th ACM
Conference on User Modeling, Adaptation and Personalization. 33â€“42.
[9]Lucas Augusto MontalvÃ£o Costa Carvalho and Hendrik Teixeira Macedo. 2013.
Usersâ€™ satisfaction in recommendation systems for groups: an approach based
on noncooperative games. In Proceedings of the 22nd International Conference on
World Wide Web. 951â€“958.
[10] Praveen Chandar, Brian St. Thomas, Lucas Maystre, Vijay Pappu, Roberto Sanchis-
Ojeda, Tiffany Wu, Ben Carterette, Mounia Lalmas, and Tony Jebara. 2022. Using
survival models to estimate user engagement in online experiments. In Proceed-
ings of the ACM Web Conference 2022. 3186â€“3195.
[11] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang,
Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with
tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 33. 3312â€“3320.
[12] Li Chen, Yonghua Yang, Ningxia Wang, Keping Yang, and Quan Yuan. 2019. How
serendipity improves user satisfaction with recommendations? a large-scale user
evaluation. In The world wide web conference. 240â€“250.
[13] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and
Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender
system. In Proceedings of the Twelfth ACM International Conference on Web Search
and Data Mining. 456â€“464.
[14] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang.
2021. A survey of deep reinforcement learning in recommender systems: A
systematic review and future directions. arXiv preprint arXiv:2109.03540 (2021).
[15] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu.
2021. Generative inverse deep reinforcement learning for online recommenda-
tion. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management. 201â€“210.
[16] Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. 2019. Better
exploration with optimistic actor critic. Advances in Neural Information Processing
Systems 32 (2019).
5505KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziru Liu et al.
[17] Abhinandan S Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.
Google news personalization: scalable online collaborative filtering. In Proceed-
ings of the 16th international conference on World Wide Web. 271â€“280.
[18] Thomas Degris, Patrick M Pilarski, and Richard S Sutton. 2012. Model-free
reinforcement learning with continuous action in practice. In 2012 American
Control Conference (ACC). IEEE, 2177â€“2182.
[19] Lih-Yuan Deng. 2006. The cross-entropy method: a unified approach to combina-
torial optimization, Monte-Carlo simulation, and machine learning.
[20] Rui Ding, Ruobing Xie, Xiaobo Hao, Xiaochun Yang, Kaikai Ge, Xu Zhang, Jie
Zhou, and Leyu Lin. 2023. Interpretable User Retention Modeling in Recom-
mendation. In Proceedings of the 17th ACM Conference on Recommender Systems.
702â€“708.
[21] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy
Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and
Ben Coppin. 2015. Deep reinforcement learning in large discrete action spaces.
arXiv preprint arXiv:1512.07679 (2015).
[22] Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function ap-
proximation error in actor-critic methods. In International conference on machine
learning. PMLR, 1587â€“1596.
[23] Harsha Gwalani. 2022. Studying Long-Term User Behaviour Using Dynamic Time
Warping for Customer Retention. In Proceedings of the Fifteenth ACM International
Conference on Web Search and Data Mining. 1643â€“1643.
[24] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. In International conference on machine learning . PMLR, 1861â€“
1870.
[25] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,
Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A tractable
decomposition for reinforcement learning with recommendation sets. (2019).
[26] Eugene Ie, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing
Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable Simulation
Platform for Recommender Systems. (2019). arXiv:1909.04847 [cs.LG]
[27] Komal Kapoor, Mingxuan Sun, Jaideep Srivastava, and Tao Ye. 2014. A hazard
based approach to user return time prediction. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge discovery and data mining. 1719â€“
1728.
[28] Elaine Lau, Stephen Zhewen Lu, Ling Pan, Doina Precup, and Emmanuel Ben-
gio. 2024. QGFN: Controllable Greediness with Action Values. arXiv preprint
arXiv:2402.05234 (2024).
[29] Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze
Wu, and Ruocheng Guo. 2023. Automlp: Automated mlp for sequential recom-
mendations. In Proceedings of the ACM Web Conference 2023. 1190â€“1198.
[30] Zihao Li, Aixin Sun, and Chenliang Li. 2023. DiffuRec: A Diffusion Model for
Sequential Recommendation. ACM Trans. Inf. Syst. 42, 3, Article 66 (dec 2023),
28 pages. https://doi.org/10.1145/3631116
[31] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen,
Huifeng Guo, and Yuzhou Zhang. 2018. Deep reinforcement learning based
recommendation with explicit user-item interactions modeling. arXiv preprint
arXiv:1810.12027 (2018).
[32] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen,
Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020. State representation mod-
eling for deep reinforcement learning based recommendation. Knowledge-Based
Systems 205 (2020), 106170.
[33] Qidong Liu, Fan Yan, Xiangyu Zhao, Zhaocheng Du, Huifeng Guo, Ruiming Tang,
and Feng Tian. 2023. Diffusion augmentation for sequential recommendation.
InProceedings of the 32nd ACM International Conference on Information and
Knowledge Management. 1576â€“1586.
[34] Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian McAuley, Dong
Zheng, Peng Jiang, and Kun Gai. 2023. Generative flow network for listwise rec-
ommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 1524â€“1534.
[35] Shuchang Liu, Qingpeng Cai, Bowen Sun, Yuhao Wang, Ji Jiang, Dong Zheng,
Peng Jiang, Kun Gai, Xiangyu Zhao, and Yongfeng Zhang. 2023. Exploration and
Regularization of the Latent Action Space in Recommendation. In Proceedings of
the ACM Web Conference 2023. 833â€“844.
[36] Ziru Liu, Kecheng Chen, Fengyi Song, Bo Chen, Xiangyu Zhao, Huifeng Guo, and
Ruiming Tang. 2024. AutoAssign+: Automatic Shared Embedding Assignment
in streaming recommendation. Knowledge and Information Systems 66, 1 (2024),
89â€“113.
[37] Ziru Liu, Shuchang Liu, Zijian Zhang, Qingpeng Cai, Xiangyu Zhao, Kesen Zhao,
Lantao Hu, Peng Jiang, and Kun Gai. 2024. Sequential Recommendation for
Optimizing Both Immediate Feedback and Long-term Retention. arXiv preprint
arXiv:2404.03637 (2024).
[38] Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive
recommender systems. In Proceedings of the ninth international conference on
Electronic commerce. 75â€“84.
[39] Omar Moling, Linas Baltrunas, and Francesco Ricci. 2012. Optimal radio channel
recommendations with explicit and implicit feedback. In Proceedings of the sixthACM conference on Recommender systems. 75â€“82.
[40] Ling Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. 2023. Pre-Training and
Fine-Tuning Generative Flow Networks. arXiv preprint arXiv:2310.03419 (2023).
[41] Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. 2023. Better
training of gflownets with local credit and incomplete trajectories. In International
Conference on Machine Learning. PMLR, 26878â€“26890.
[42] Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio.
2022. Generative augmented flow networks. arXiv preprint arXiv:2210.03308
(2022).
[43] Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio.
2023. Stochastic generative flow networks. In Uncertainty in Artificial Intelligence.
PMLR, 1628â€“1638.
[44] Jan Peters and Stefan Schaal. 2008. Natural actor-critic. Neurocomputing 71, 7-9
(2008), 1180â€“1190.
[45] Guy Shani, David Heckerman, Ronen I Brafman, and Craig Boutilier. 2005. An
MDP-based recommender system. Journal of Machine Learning Research 6, 9
(2005).
[46] Yueming Sun and Yi Zhang. 2018. Conversational recommender system. In The
41st international acm sigir conference on research & development in information
retrieval. 235â€“244.
[47] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-
duction. MIT press.
[48] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999.
Policy gradient methods for reinforcement learning with function approximation.
Advances in neural information processing systems 12 (1999).
[49] Nima Taghipour, Ahmad Kardan, and Saeed Shiry Ghidary. 2007. Usage-based
web recommendations: a reinforcement learning approach. In Proceedings of the
2007 ACM conference on Recommender systems. 113â€“120.
[50] Markus Viljanen, Antti Airola, Tapio Pahikkala, and Jukka Heikkonen. 2016. Mod-
elling user retention in mobile games. In 2016 IEEE Conference on Computational
Intelligence and Games (CIG). IEEE, 1â€“8.
[51] Yuhao Wang, Ha Tsz Lam, Yi Wong, Ziru Liu, Xiangyu Zhao, Yichao Wang, Bo
Chen, Huifeng Guo, and Ruiming Tang. 2023. Multi-task deep recommender
systems: A survey. arXiv preprint arXiv:2302.03525 (2023).
[52] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson,
Lisa Chung, Ed H Chi, and Minmin Chen. 2022. Surrogate for Long-Term User
Experience in Recommender Systems. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 4100â€“4109.
[53] Qingyun Wu, Hongning Wang, Liangjie Hong, and Yue Shi. 2017. Returning
is believing: Optimizing long-term user engagement in recommender systems.
InProceedings of the 2017 ACM on Conference on Information and Knowledge
Management. 1927â€“1936.
[54] Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M Jose. 2020.
Self-supervised reinforcement learning for recommender systems. In Proceedings
of the 43rd International ACM SIGIR conference on research and development in
Information Retrieval. 931â€“940.
[55] Wanqi Xue, Qingpeng Cai, Ruohan Zhan, Dong Zheng, Peng Jiang, Kun Gai,
and Bo An. 2022. ResAct: Reinforcing long-term engagement in sequential
recommendation with residual actor. arXiv preprint arXiv:2206.02620 (2022).
[56] Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, and
Xiangnan He. 2024. Generate What You Prefer: Reshaping Sequential Recommen-
dation via Guided Diffusion. Advances in Neural Information Processing Systems
36 (2024).
[57] Xing Yi, Liangjie Hong, Erheng Zhong, Nanthan Nan Liu, and Suju Rajan. 2014.
Beyond clicks: dwell time for personalization. In Proceedings of the 8th ACM
Conference on Recommender systems. 113â€“120.
[58] Chi Zhang, Rui Chen, Xiangyu Zhao, Qilong Han, and Li Li. 2023. Denoising and
prompt-tuning for multi-behavior recommendation. In Proceedings of the ACM
Web Conference 2023. 1355â€“1363.
[59] Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C Courville, Yoshua Bengio,
and Ling Pan. 2024. Let the flows tell: Solving graph combinatorial problems
with GFlowNets. Advances in Neural Information Processing Systems 36 (2024).
[60] Dinghuai Zhang, Ling Pan, Ricky TQ Chen, Aaron Courville, and Yoshua Bengio.
2023. Distributional gflownets with quantile flows. arXiv preprint arXiv:2302.05793
(2023).
[61] Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, Fan
Huang, and Xianfeng Tan. 2022. Multi-Task Fusion via Reinforcement Learning
for Long-Term User Satisfaction in Recommender Systems. In Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
4510â€“4520.
[62] Kesen Zhao, Shuchang Liu, Qingpeng Cai, Xiangyu Zhao, Ziru Liu, Dong Zheng,
Peng Jiang, and Kun Gai. 2023. KuaiSim: A comprehensive simulator for recom-
mender systems. Advances in Neural Information Processing Systems 36 (2023),
44880â€“44897.
[63] Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, and Dawei Yin. 2023. User
Retention-oriented Recommendation with Decision Transformer. In Proceedings
of the ACM Web Conference 2023. 1141â€“1149.
5506Modeling User Retention through Generative Flow Networks KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[64] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing
Liu, Hui Liu, and Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning for
Online Advertising Impression in Recommender Systems. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 35. 750â€“758.
[65] Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. 2019. " Deep reinforce-
ment learning for search, recommendation, and online advertising: a survey"
by Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin with Martin Vesely as
coordinator. ACM sigweb newsletter 2019, Spring (2019), 1â€“15.
[66] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang
Tang. 2018. Deep Reinforcement Learning for Page-wise Recommendations. In
Proceedings of the 12th ACM Recommender Systems Conference. ACM, 95â€“103.
[67] Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and Jiliang Tang. 2020.
Whole-chain recommendations. In Proceedings of the 29th ACM international
conference on information & knowledge management. 1883â€“1891.
[68] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin.
2018. Recommendations with negative feedback via pairwise deep reinforcement
learning. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining. 1040â€“1048.
[69] Yufan Zhao, Donglin Zeng, Mark A Socinski, and Michael R Kosorok. 2011.
Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer.
Biometrics 67, 4 (2011), 1422â€“1433.
[70] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan,
Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework
for news recommendation. In Proceedings of the 2018 world wide web conference.
167â€“176.
[71] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059â€“1068.
[72] Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, and Dawei Yin.
2019. Reinforcement learning to optimize long-term user engagement in recom-
mender systems. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining. 2810â€“2818.
A IMPLEMENTATION DETAILS
In our study, we implement a cross-session environment setup
using the KuaiSim Simulator to assess our models. Here are the
detailed configurations for each dataset:
For the Kuairand-Pure Dataset:
â€¢We utilize a CrossSessionBuffer of size 100,000.
â€¢The policy model is configured with a Transformer architecture,
featuring 4 attention heads and an embedding size of 32.
â€¢Learning rates are set to 0.00002 for the flow estimator and 0.0001
for both the forward and backward estimators.
â€¢All estimators share an embedding hidden dimension of 128.
â€¢Optimization is conducted using the Adam optimizer, with a
batch size of 128.
â€¢A default balance parameter ğ›¼of 1 is applied.
â€¢Smooth offset values are set at 1.0 for both forward ğ›½ğ¹and back-
ward ğ›½ğµprobabilities, with a reward smooth offset ğ›½ğ‘Ÿof 0.6.
For the MovieLens-1M Dataset:
â€¢We utilize a CrossSessionBuffer of size 10,000.
â€¢The policy model is configured with a Transformer architecture,
featuring 4 attention heads and an embedding size of 32.
â€¢Learning rates are set to 0.0003 for the flow estimator and 0.001
for both the forward and backward estimators.
â€¢All estimators share an embedding hidden dimension of 64.
â€¢Optimization is conducted using the Adam optimizer, with a
batch size of 32.
â€¢A default balance parameter ğ›¼of 1 is applied.
â€¢Smooth offset values are set at 0.8 for both forward ğ›½ğ¹and back-
ward ğ›½ğµprobabilities, with a reward smooth offset ğ›½ğ‘Ÿof 0.6.
Both datasets undergo 100,000 training steps to ensure comprehen-
sive model evaluation.B ALGORITHM
In this section, we elucidate the optimization algorithm underpin-
ning our model, presented through explanatory pseudo-code. As de-
lineated in Algorithm 1, the training procedure for GFN4Retention
is methodically straightforward, adhering to a clearly defined se-
quence of operations that ensure the modelâ€™s convergence to the
desired objectives. This structured approach facilitates ease of repli-
cation and verification of the results reported herein.
Algorithm 1 Training Process for GFN4Retention
1:# Apply current policy in running episodes:
2:procedure ONLINE INFERENCE
3:Initialize replay buffer B.
4:while True, in each running episode do do
5: Observe user request Sğ‘¢,0.
6: Initial the item sequence ğ‘ 0â†âˆ…
7:forğ‘¡âˆˆ{1, . . . ,ğ‘‡}dodo
8: Sample item list ğ‘ğ‘¡âˆ¼ğ‘ƒ(Â·|ğ‘¢, ğ‘ ğ‘¡âˆ’1)with current policy.
9:end for
10: Obtain user responses yfrom online environment and cal-
culate the retention reward R(ğ‘¢, ğ‘ ).
11:(ğ‘¢, ğ‘ ,R(ğ‘¢, ğ‘ ),y)â†’B
12:end while
13:end procedure
14:# Simultaneous training on the buffer:
15:procedure TRAINING
16:Initialize all trainable parameters in the policy (e.g., ğœ™,ğœƒ1and
ğœƒ2) in GFN4Retention)
17:Wait untilBhas stored minimum amount of data points.
18:while Not Converged, in each iteration do do
19: Obtain mini-batch sample (ğ‘¢, ğ‘ , ğ‘…ğ‘™(ğ‘¢, ğ‘ ),y)â†’B .
20: Calculate flow estimator F(ğ‘ ğ‘¡;ğœ™), forward estimator
ğ‘ƒğ¹(ğ‘ğ‘¡|ğ‘¢, ğ‘ ğ‘¡âˆ’1;ğœƒ1)and backward estimator ğ‘ƒğµ(ğ‘ ğ‘¡âˆ’1|ğ‘¢, ğ‘ ğ‘¡;ğœƒ2)
for each generation step ğ‘¡.
21: Update the policy through one step of gradient descent using
Adam on the DB loss function Lğ·ğµ.
22:end while
23:end procedure
C HYPER-PARAMETER SELECTION
In our study, we meticulously tuned the hyperparameters of the
GFN4Retention model to reinforce the reliability of our experimen-
tal outcomes. The summarized performance metrics, spanning two
datasets, are documented for detailed examination in Table 4.
D PROBABLISTIC FLOW IN CONTINUOUS
SPACE
For a given state, the flow estimator F(ğ‘ ğ‘¡)represents the gener-
ation likelihood of reaching that particular state. Without loss of
generality, assume each sample consists of the transition informa-
tion between the current state ğ‘ ğ‘¡=ğ‘¥1and the next state ğ‘ ğ‘¡+1=ğ‘¦.
The sampled action is denoted as ğ‘ğ‘¡=ğ‘§. Our forward function is
associated with a Gaussian actor ğœ‡, ğœ=ğœ™fwwhere corresponding
5507KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Ziru Liu et al.
Table 4: Hyperparameter Settings and Choices
Dataset Hyper-parameter Tuning Range Our Choice
Number of heads [2, 4, 6, 8] 4
Embedding size [8, 16, 32, 64, 128] 32
LR for flow estimator [0.00001, 0.00002, 0.0001, 0.0002] 0.00002
KuaiRand-Pure LR for forward estimator [0.00005, 0.0001, 0.0002, 0.001] 0.0001
Batch size [32, 64, 128, 256] 128
Forward offset ğ›½ğ¹ [0.2, 0.4, 0.6, 0.8, 1.0, 1.2] 1
Backward offset ğ›½ğµ [0.2, 0.4, 0.6, 0.8, 1.0, 1.2] 1
Reward offset ğ›½ğ‘Ÿ [0.2, 0.4, 0.6, 0.8, 1.0, 1.2] 0.5
LR for flow estimator [0.0001, 0.0002, 0.0003, 0.001] 0.0003
LR for forward estimator [0.0001, 0.0002, 0.001, 0.002] 0.001
MovieLens-1M Batch size [32,64,128,256] 64
Forward offset ğ›½ğ¹ [0.2, 0.4, 0.6, 0.8, 1.0, 1.2] 0.8
Backward offset ğ›½ğµ [0.2, 0.4, 0.6, 0.8, 1.0, 1.2] 0.8
Reward offset ğ›½ğ‘Ÿ [0.2, 0.4, 0.6, 0.8, 1.0, 1.2] 0.5
action distribution follows ğ‘ğ‘¡âˆ¼N( ğœ‡, ğœ)F(ğ‘ ğ‘¡=ğ‘¥1). Then the for-
ward function for the observed transition is ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)=ğ‘ƒ(ğ‘ğ‘¡=ğ‘§)
The flow matching property holds at this observed point with the
joint likelihood
ğ‘ƒğ¹(ğ‘ ğ‘¡+1=ğ‘¦|ğ‘ ğ‘¡=ğ‘¥1)F(ğ‘ ğ‘¡=ğ‘¥1)
=ğ‘ƒ(ğ‘ ğ‘¡=ğ‘¥1, ğ‘ ğ‘¡+1=ğ‘¦)
=ğ‘ƒğµ(ğ‘ ğ‘¡=ğ‘¥1|ğ‘ ğ‘¡+1=ğ‘¦)F(ğ‘ ğ‘¡+1=ğ‘¦)(12)Additionally, the flow of the next state essentially expresses the
expected reachability in the generation process:
F(ğ‘ ğ‘¡+1)=âˆ«
ğ‘ ğ‘¡ğ‘ƒğ¹(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡)F(ğ‘ ğ‘¡) (13)
which addresses the infinite number of previous states for the given
next state. Note that different from the forward policy that follows
the Gaussian distribution, the distribution of state flow Fand the
backward flow ğ‘ƒğµcould be complicated. So we use neural networks
to approximate these likelihoods in our design.
5508