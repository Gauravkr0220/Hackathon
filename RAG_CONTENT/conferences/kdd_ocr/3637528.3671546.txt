Decomposed Attention Segment Recurrent Neural Network for
Orbit Prediction
SeungWon Jeong
Sejong University
Seoul, Republic of Korea
bbigaa123@sju.ac.krSoyeon Woo
Sejong University
Seoul, Republic of Korea
sallyeon@gmail.comDaewon Chung
National Satellite Operation Center
Korea Aerospace Research Institute
Daejeon, Republic of Korea
dwchung@kari.re.kr
Simon S. Woo‚àó
Computer Science & Engineering
Department
Sungkyunkwan University
Suwon, Republic of Korea
swoo@g.skku.eduYoujin Shin‚àó
The Catholic University of Korea
Bucheon, Republic of Korea
yj.shinn@catholic.ac.kr
ABSTRACT
As the focus of space exploration shifts from national agencies to
private companies, the interest in space industry has been steadily
increasing. With the increasing number of satellites, the risk of
collisions between satellites and space debris has escalated, poten-
tially leading to significant property and human losses. Therefore,
accurately modeling the orbit is critical for satellite operations. In
this work, we propose the Decomposed Attention Segment Recur-
rent Neural Network (DASR) model, adding two key components,
Multi-Head Attention and Tensor Train Decomposition, to SegRNN
for orbit prediction. The DASR model applies Multi-Head Attention
before segmenting at input data and before the input of the GRU
layers. In addition, Tensor Train (TT) Decomposition is applied
to the weight matrices of the Multi-Head Attention in both the
encoder and decoder. For evaluation, we use three real-world satel-
lite datasets from the Korea Aerospace Research Institute (KARI),
which are currently operating: KOMPSAT-3, KOMPSAT-3A, and
KOMPSAT-5 satellites. Our proposed model demonstrates supe-
rior performance compared to other SOTA baseline models. We
demonstrate that our approach is 88.52% higher predictive per-
formance than the second-best model in the KOMPSAT-3 dataset,
89.79% higher in the KOMPSAT-3A dataset, and 53.28% higher in
the KOMPSAT-5 dataset.
CCS CONCEPTS
‚Ä¢Applied computing ‚ÜíAerospace; Forecasting; ‚Ä¢Computing
methodologies‚ÜíMachine learning.
‚àóCorresponding Author
Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or affiliate of a national govern-
ment. As such, the Government retains a nonexclusive, royalty-free right to publish
or reproduce this article, or to allow others to do so, for Government purposes only.
Request permissions from owner/author(s).
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671546KEYWORDS
Tensor-train Decomposition, Time series prediction, Orbit Predic-
tion, Model Compression, Parameter Reduction
ACM Reference Format:
SeungWon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, and Youjin
Shin. 2024. Decomposed Attention Segment Recurrent Neural Network
for Orbit Prediction. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671546
1 INTRODUCTION
As the cost of satellite launches has decreased, there has been a
significant interest in space-related ventures over recent years, lead-
ing to the emergence of private space exploration companies such
as SpaceX [ 10], BlueOrigin [ 4], RocketLab [ 34], OneWeb [ 37], etc.
Such companies predominantly operate Low Earth Orbit (LEO)
satellites to provide diverse services across various industries, in-
cluding earth observation, communication services, navigation and
positioning, and global internet coverage. However, a serious con-
cern has arisen due to the exponential growth in the number of
satellites launched by these private space exploration companies.
As of May 2023, Orbiting Now [ 25], a satellite tracking website,
reports a total of 7,702 active satellites distributed across different
Earth orbits, with 12% in Geostationary Earth Orbit (GEO), 3% in
Medium Earth Orbit (MEO), and the remaining 84% in Low Earth
Orbit (LEO). Moreover, astronomy article [ 2] projected that the
number of satellites orbiting our planet could exceed 100,000 by
2030.
In response to the increasing complexity and congestion in the
space environment, maintaining the designated orbits of satellites
has become imperative to mitigate the risk of collisions. However,
satellites‚Äô orbits can be changed due to various factors, including
gravitational forces, atmospheric drag, solar radiation pressure,
magnetic forces, tidal forces, gravitational perturbations, and orbital
resonance. Crashes with space debris such as inactive satellites or
space junk can also influence and alter the orbit of a satellite. A
minor deviation of the orbit could have substantial consequences,
potentially leading to collisions, given the high velocity of satellites
5172
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain SeungWon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, and Youjin Shin
at 7.8 km/s. As the number of satellites increases, this issue could
become more prevalent. Collisions between satellites, as well as
between satellites and space debris, can lead to substantial financial
losses and even pose a risk of casualties from debris re-entering
Earth‚Äôs atmosphere. Therefore, accurate satellite orbit prediction is
crucial to detect and prevent such hazards.
To address this challenge, orbit prediction algorithms have pre-
dominantly relied on a combination of statistical and physical meth-
ods. These traditional approaches involve utilizing mathematical
models based on known physics, and statistical analyses incorporat-
ing historical orbital data [ 18,29,31]. Recently, machine learning
and deep learning techniques have been adopted for more accu-
rate orbit predictions [ 28,30]. Such techniques leverage patterns
and relationships within large datasets to enhance the precision
of forecasting future orbital paths. However, prior research works
are evaluated with simulated dataset rather than real-world ones.
Subsequent research introduced RNN-based models for predicting
the orbits of satellites or spatial targets [ 32]. In addition, addressing
the temporal dependencies inherent in the orbit dataset, several
studies have tried to utilize the Long Short-Term Memory (LSTM)
model [ 3,8,26,33,39] to effectively reduce orbit prediction errors.
Recently, the Segment Recurrent Neural Network (SegRNN)
model [ 21] has been proposed and demonstrated its high perfor-
mance in predicting time series datasets. Due to the challenging
space environments, even a small margin of orbital changes can
lead to actually significant orbital deviations of several kilometers.
Therefore, a model showing higher predictive performance in or-
bital prediction is needed. In this research, we propose the novel
Decomposed Attention Segment Recurrent Neural Network (DASR)
model to enhances prediction performance. In particular, the DASR
model applies the Multi-Head Attention before the input of a seg-
ment in the SegRNN model, in addition to the inputs of both the
encoder and decoder that constitute the model, in order to increase
the prediction performance. Furthermore, we apply Tensor Train
(TT) Decomposition [ 27] on the weight matrices of the Multi-Head
Attention. It helps not only enhance accuracy but also reduce the pa-
rameters when the hidden unit size is sufficiently large. The DASR
achieves the effectiveness of the model, compared to the State of the
Arts (SOTA) multivariate time series prediction models.The main
contributions of our work are summarized as follows:
‚Ä¢In this work, we propose the novel Decomposed Attention
Segment Recurrent Neural Network (DASR) to accurately
predict the orbits of satellites in the challenging space envi-
ronments.
‚Ä¢The DASR model enhances the prediction performance by
applying Multi-Head Attention to the vanilla SegRNN model.
‚Ä¢we demonstrate the utility of Tensor Train Decomposition,
in terms of the parameter reduction when the hidden unit
size is sufficiently large
‚Ä¢We evaluate our approach with three currently operating
satellite datasets and show our approach outperforms other
SOTA prediction models, clearly demonstrating the real-
world applicability of our system1.
1DASR is planned to be deployed in Jan. 2025 for National Satellite Operation Center in
Korea Aerospace Research Institute (KARI) to support the existing satellite monitoring
system in South Korea.2 RELATED WORK
2.1 Prediction in time-series dataset
As interest in time-series prediction and its importance increase,
various models have been proposed. The field began with research
on time-series prediction based on machine learning [ 40,50,51].
This was followed by the advent of conventional neural network
methodologies such as Recurrent Neural Network (RNN) based
Long Short Term Memory (LSTM) [ 8,36], Gated Recurrent Unit
(GRU) [ 16,35], and Bidirectional Long Short Term Memory (Bi-
LSTM) [ 13]. However, these models have issues with long-term
dependencies and computational complexity. In 2017, the Trans-
former model [ 43] emerged for sequence data processing in parallel
using Multi-Head Attention mechanisms to understand relation-
ships between elements, leading to various studies applying it to
time-series prediction.
Subsequently, due to its long training time and large model size,
extensive research [ 22,47,54,56] has been conducted to address
and mitigate these challenges. Recently, Ailing Zeng et al. [ 49]
questioned the validity of using Transformer-based solutions for
long-term time series forecasting tasks. They highlight that while
Transformer is successful in extracting semantic correlations in
long sequences, it suffers from temporal information loss in time
series modeling, due to the permutation-invariant Multi-Head At-
tention mechanism. Instead, they introduce a set of one-layer linear
models called Long sequence Time-Series Forecasting Linear (LTSF-
Linear), which directly regresses historical time series for future
prediction using a weighted sum operation. Their model yield better
performance than a Transformer-based solution. In 2023, the Patch
Time Series Transformer (PatchTST) [ 24] is efficiently designed for
multivariate time series forecasting and self-supervised represen-
tation learning. It incorporates two components: segmentation of
time series into subseries-level patches, which serve as input tokens
to the Transformer, and channel-independence, where each channel
contains a single univariate time series that shares the same embed-
ding and Transformer weights across all the series. Another notable
recent algorithm is SegRNN [ 21], a combination of segment-wise
iterations and parallel multi-step forecasting (PMF), significantly
reducing the required recurrent iterations, and improving forecast
accuracy.
2.2 Orbit Prediction using Machine Learning or
Deep Learning
Satellite orbit prediction is essential for preventing collisions be-
tween satellites, enabling efficient communication and exploration
operations, and managing the space environment. Recently, as the
need for more sophisticated satellite orbit prediction has grown,
machine learning or deep learning-based prediction models have
emerged, enhancing prediction accuracy. Hao Peng and Xiaoli
Bai [28] demonstrated the enhancement of orbit prediction accuracy
using machine learning by employing the Support Vector Machine
(SVM) model with data obtained through simulations of Resident
Space Objects (RSOs), including satellites. Pedro F. Proen√ßa and
Yang Gao [ 30] developed a visual simulator called URSO based on
Unreal Engine4. They generated images of spacecraft orbiting Earth
and used Deep Neural Network (DNN) to estimate the position of
5173Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
spacecraft in space from these images. Both of these studies have
limitations as they relied on simulated data rather than real data.
Kan Wang et. al [ 45] performed short-term orbit predictions based
on real-time LEO satellite orbits using the Batch Least Squares
(BLS) technique. This method uses all observational data at once to
estimate parameters in a mathematical model to find the optimal
model.
While it offers the advantage of real-time location tracking for
LEO satellites, it is closer to traditional numerical optimization
and statistical techniques. Thus, it may not effectively handle large
datasets or complex patterns. In 2022, Alaa Osama et. al [ 26] aimed
to predict satellite orbits using Two-line Element set (TLE) conver-
sions based on LSTM in 2022. Zonghua Qu and Chunling Wei [ 32]
also predicted the orbit of the Spatial Target using an LSTM-based
model combined with the Particle Swarm Optimization (PSO) algo-
rithm.
2.3 Tensor Decomposition in Neural Network
Tensor Decomposition has been used in various studies to reduce
the size and execution time of Neural Network Models by decreas-
ing the number of parameters. Canonical Polyadic Decomposition
(CP Decomposition) [ 14] is a method that decomposes a tensor into
a sum of outer products of rank-one tensors, where the number
of rank-one tensors is determined by the rank. CP Decomposition
has been researched in the field of Convolution Neural Networks
(CNN) to reduce the size of models, maintaining accuracy while
significantly reducing memory and computational costs [ 1,20,55].
However, CP Decomposition can become computationally com-
plex for large tensors and finding the optimal rank value can be
challenging. Tucker Decomposition [ 42], which represents a ten-
sor as the product of a core tensor obtained by compressing the
original tensor and matrices obtained through mode-n transforma-
tions, addresses the limitation of CP Decomposition in handling
high-dimensional tensors effectively. This decomposition has been
actively researched not only in CNN [ 12,23], but also in Recurrent
Neural Network (RNN) [6, 48] fields.
Despite its advantage in processing high-dimensional tensors,
Tucker Decomposition suffers from high computational complex-
ity with high-dimensional data. Tensor Train Decomposition [ 27]
decomposes an input tensor into a series of smaller core tensors,
connecting each core tensor through the last dimension of the
previous core and the first dimension of the next core. This decom-
position maintains a relatively low computational complexity even
with high-dimensional data. The research [ 44] has applied Tensor
Train Decomposition to 3DCNN models, achieving a 160.7-fold
compression of the model with almost similar accuracy. Further
research has continued to apply Tensor Train Decomposition to
RNN models [ 41] and Transformer models [ 53], reducing the size
of the models while maintaining performance.
3 DATASET
3.1 Dataset Discription
The Korea Aerospace Research Institute (KARI) [ 17], established
in 1989, is South Korea‚Äôs national aerospace research institute. It
was founded to explore, develop, and disseminate new aviation and
aerospace science and technology. Among its projects, the KoreaMulti-Purpose Satellite (KOMPSAT) series was designed to observe
the surface of South Korea in high resolution to enhance national
security and public services. In this study, we utilized a dataset of
orbit data from the KOMPSAT-3 (K3), KOMPSAT-3A (K3A) and
KOMPSAT-5 (K5). It consists of a total of about 1.58 million data
points collected every minute from January 2016 to December 2016
from each satellite.
Determining the position (orbit) of a satellite presents several
challenges. Firstly, directly measuring the position and velocity
of a moving satellite from the ground is difficult. Interferences
arise unexpectedly due to various dynamical forces such as Earth‚Äôs
gravitational potential, the gravitational pull from the sun or moon,
and solar radiation pressure.
To address these challenges, the Two-line Element set (TLE) [ 46]
is used to predict the orbit of moving satellites. It encodes a list of
orbital elements using simplified perturbation models such as SGP,
SGP4 and SDP4. So, it allows for the calculation of the position and
velocity of a space object at a given time. For model training, we
used six orbital elements extracted from the TLE dataset. These six
orbital elements will be detailed in Section 3.2. To understand the
characteristics of the data, we extracted data for each element value
from the 300,000th data point and took 1,000 samples, as shown
in Figure 1. Each element generally exhibits repetitive behavior
within a specific time range.
3.2 Preprocessing
We use the following six orbital elements decoded from the KOMPSAT-
3 satellite‚Äôs Two-line Element set (TLE) dataset as follows:
(1)Semi-major axis is a value that indicates the size of the
orbit, expressed in kilometers, and signifies how far the satel-
lite is from the Earth. A smaller Semi-major axis implies a
closer orbit to Earth, while a larger value indicates a greater
distance.
(2)Eccentricity is a value that characterizes the orbit as an ellip-
tical shape, ranging between 0 and 1. As the value approaches
0, the orbit becomes more circular, and as it approaches 1,
the orbit takes on a flatter, more elliptical shape.
(3)Inclination denotes the angle by which the orbital plane
is tilted concerning the equatorial plane. It illustrates how
much a satellite‚Äôs orbit deviates from the Earth‚Äôs equatorial
plane.
(4)Right Ascension of the Ascending Node (RAAN) quanti-
fies the angle between the vernal equinox and the ascending
node of the satellite‚Äôs orbit. The ascending node is where
the satellite‚Äôs orbital plane intersects the Earth‚Äôs equatorial
plane.
(5)Argument of perigee denotes the angle formed between
the point where the satellite is nearest to the Earth, known
as the perigee, and the satellite‚Äôs ascending node in its orbital
path.
(6)Mean anomaly expresses the angle at a specific point in the
orbit, measured from the perigee. It applies when a satellite
maintains a constant speed while orbiting the Earth from
the perigee point.
5174KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain SeungWon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, and Youjin Shin
Figure 1: Plots for each element‚Äôs values at 300,000 to 301,000
time-stamps. The behavior of the K3 satellite represented
consistent patterns with their own characteristics.
This six orbital elements mentioned above have different scales,
so we map the data using the following equation (1).
ùëç=(ùë•‚àíùëöùëñùëõ(ùë•))
(ùëöùëéùë•(ùë•)‚àíùëöùëñùëõ(ùë•))(1)
3.3 Dataset Similarity
The three datasets used in this study are all from low earth orbit
(LEO) satellites and might share some similarities in their global
trajectory patterns in general. The similarities in trajectory patterns
can be observed in Figure 2. However, none of the satellites have the
same data, as each satellite is different. To show that each dataset
distribution is different, we conducted the two-sample Kolmogorov-
Smirnov (KS) test [19] among the three datasets.
The KS test is a non-parametric test used to compare two proba-
bility distributions. Under the null hypothesis that the two distri-
butions are identical, this test calculates the maximum difference
between their cumulative distribution functions (CDFs) to deter-
mine if they follow the same distribution. The CDFs of the two
distributions can be represented by Equation 2, where ùêºis the indi-
cator function.
ùêπùëõ(ùë•)=1
ùëõùëõ‚àëÔ∏Å
ùëñ=1ùêºùëãùëñ‚â§ùë•
ùê∫ùëö(ùë•)=1
ùëöùëö‚àëÔ∏Å
ùëñ=1ùêºùëåùëñ‚â§ùë•(2)
By applying these CDFs to Equation 3, we obtain the KS Statistic.
If the KS Statistic is small or the p-value is high, we cannot reject the
null hypothesis that the two distributions are the same. Conversely,
if the p-value is small (< 0.005), we can reject the null hypothesis,
indicating that they come from two different distributions.
Figure 2: Plots for the distribution of six orbital elements
contained in the three datasets (K3, K3A, K5). The blue line
depicts the distribution of K3, the orange line shows the
distribution of K3A, and the green line illustrates the distri-
bution of K5.
Table 1: The KS test results for three different comparisons:
K3 & K3A, K3 & K5, and K3A & K5. Each KS test is performed
separately for the six orbital elements. For each test, we ob-
tained the KS Statistic and p-value. The p-values were re-
ported as ‚Äô< 0.005‚Äô if they were less than 0.005, and ‚Äô> 0.005‚Äô if
they were greater than 0.005.
Metho
dK3
& K3A K3
& K5 K3A
& K5
KS
Statistic P-value KS
Statistic P-value KS
Statistic P-Value
Semi-major
axis 1.0000
< 0.005 1.0000
< 0.005 1.0000
< 0.005
Eccentricity 0.0333
< 0.005 0.2571
< 0.005 0.2419
< 0.005
Inclination 1.0000
< 0.005 1.0000
< 0.005 1.0000
< 0.005
RAAN. 0.0011
> 0.005 0.0010
> 0.005 0.0010
> 0.005
Argument
of perigee 0.0174
< 0.005 0.1232
< 0.005 0.1062
< 0.005
Mean
anomaly 0.0201
< 0.005 0.1188
< 0.005 0.0998
< 0.005
ùê∑ùëõ,ùëö=sup
ùë•|ùêπùëõ(ùë•)‚àíùê∫ùëö(ùë•)|(3)
In this study, the KS test was performed on pairs of datasets
(K3, K3A, K5) for the six orbital elements described in Section 3.2.
By obtaining the KS Statistic and p-value, we determined whether
the null hypothesis for the distribution of two datasets could be
rejected. The results of the KS test are shown in Table 1, where the
null hypothesis was rejected for the five orbital elements except for
the Right Ascension of the Ascending Node (RAAN).
This experiment demonstrates that the datasets used in our study
do not have statistical similarities.
5175Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
4 METHOD
4.1 Decomposed Attention Segment Recurrent
Neural Network
In conventional RNN-based models, sequence processing is achieved
through point-wise iteration, where each sequence element is pro-
cessed sequentially. The vanilla SegRNN model introduces a segment-
wise iteration technique, dividing the input sequence into segments
and processing them, thus requiring fewer iterations for improved
performance compared to point-wise iteration. Moreover, in the
forecasting phase, instead of using the Recurrent Multi-step Fore-
casting (RMF) employed by traditional RNN-based models, Parallel
Multi-step Forecasting (PMF) is adopted.
In this work, the Decomposed Attention Segment Recurrent
Neural Network (DASR) model we propose adopts the SegRNN,
while there are several improvement we have made. In particular,
our DASR model applies Multi-Head Attention prior to the input
data segmentation of SegRNN and utilizes Tensor Train Multihead
Attention before passing data through the GRU cells of both the
encoder and decoder. The Tensor Train (TT) Multi-Head Attention
is described in detail in Section 4.3. And, the architecture of the
DASR model is depicted in Figure 3.
In our model, before applying segments to the input ùëã(ùëñ)‚ààRùêøof
the SegRNN model‚Äôs encoder, Multi-Head Attention is applied. The
input is then segmented into a form to ùëã(ùëñ)‚ààRùëõùê∏√óùúîand passed
through a Linear layer and transformed ùëã(ùëñ)‚ààRùëõùê∏√óùëë, whereùêø
denotes the historical look-back window, ùëñis theùëñùë°‚Ñédimension, and
ùëãùëñdenotes the time series data for the ùëñùë°‚Ñédimension,ùúîrepresents
the window length of a segment, ùëëmeans hidden unit‚Äôs count, and
ùëõùê∏signifies the number of segments to be processed by the encoder,
defined byùêø
ùúî.
After this step, TT Multi-Head Attention is applied, and the
encoder output is obtained after passing through GRU cells. In the
decoder phase, our model uses the results obtained by concatenating
relative encoding and channel encoding as positional embeddings
ùëÉùê∏(ùëñ)‚ààRùëõùê∑√óùëë, whereùëÉùê∏ùëñdenotes positional embedding for the
ùëñùë°‚Ñédimension. They are then applied to TT Multi-Head Attention
to obtain the result ùëå(ùëñ)‚ààRùëõùê∑√óùëë. And,ùëõùê∑=ùêª
ùúîrepresents the
number of segments to be processed by the decoder, ùëåùëñdenotes
the future time series values for the ùëñùë°‚Ñédimension, and ùêªis the
forecasting horizon. Along with the encoder output, this result
passed through GRU cells to produce the result ùëå(ùëñ)‚ààRùëõùê∑√óùëë. And,
ùëå(ùëñ)‚ààRùëõùê∑√óùëëpasses through a linear layer, and it is changed into
the form of ùëå(ùëñ)‚ààRùëõùê∑√óùúî. Then, it is reshaped into the form of
ùëå(ùëñ)‚ààRùêªto obtain the final result. In our work, since the dataset is
adjusted with Equation (1), Layer Normalization is not necessarily.
4.2 Tensor Train (TT) Decomposition
Tensor Train Decomposition is a method for decomposing high-
dimensional tensors into a linear network of lower-dimensional
core tensors. For instance, applying TT Decomposition to a high-
dimensional tensor ùê¥‚ààRùêº1√óùêº2√ó¬∑¬∑¬∑√óùêºùëëdecomposes the input into ùëë
core tensors ùê∫1,ùê∫2,¬∑¬∑¬∑,ùê∫ùëë, where each core tensor is connected
in a structure, and the output of one core becomes the input to the
next. Each core tensor ùê∫ùëòhas dimensions of ùëÖùëüùëò‚àí1√óùêºùëò√óùëüùëò, with the
condition that ùëü0=ùëüùëë=1, whereùëüùëòrepresents the rank of the ùëòùë°‚Ñécore tensor. TT Decomposition can be represented as the following
Equation (4):
ùê¥=ùê∫1√óùê∫2√ó¬∑¬∑¬∑√óùê∫ùëë(4)
In addition, Figure 4 (b) illustrates how Tensor Decomposition is
used. In our work, Tensor Decomposition is applied to the weight
matrices for the query, key, and value of Multi-Head Attention, de-
noted asùëäùëÑ‚ààRùê∑√óùê∑,ùëäùêæ‚ààRùê∑√óùê∑, andùëäùëâ‚ààRùê∑√óùê∑, respectively.
And,ùê∑is the dimension of weight matrix, where weight matrices
are stacked to form a 3-dimensional tensor, ùëä‚ààR3√óùê∑√óùê∑, which
can be represented as follows:
ùëä=Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùëäùëÑ
ùëäùêæ
ùëäùëâÔ£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª(5)
Subsequently, using TT Decomposition, the 3-dimensional tensor
was split into three core tensors according to their ranks. In our
experiments, we used ranks of [1, 3, 60, 1], resulting in the core
tensorsùëéùëü,ùëèùëü,ùëêùëü. As a result, each of these core tensors has the
following dimension: ùëéùëü‚ààR1√ó3√ó3,ùëèùëü‚ààR3√óùê∑√ó60,ùëêùëü‚ààR60√óùê∑√ó1,
whereùê∑refers to the number of hidden units, taking the value of
[32,64,128,256].
4.3 Tensor Train (TT) Multi-Head Attention
Tensor Train (TT) Multi-Head Attention is an efficient multi-attention
structure that decomposes the attention weight matrices using the
tensor train decomposition, described in the previous section. This
structure not only improves prediction accuracy, but also reduces
the number of parameters resulting from the addition of attention
mechanisms.
Before explaining our TT Multi-Head Attention, we first explain
the TT Attention. When the input entering TT Attention is denoted
asùëç, the original attention mechanism multiplies ùëçbyùëäùëû,ùëäùëòand
ùëäùë£to obtainùëÑ,ùêæ andùëâas shown in Figure 4 (a). However, in Fig-
ure 4 (b), we stack ùëäùëû,ùëäùëòandùëäùë£into a three-dimensional tensor
to form the weight tensor W, and then apply TT decomposition as
described in Section 4.2, to obtain ùëéùëü,ùëèùëüandùëêùëü. At Figure 4 (c), Ten-
sor Train Attention then multiplies ùëéùëü,ùëèùëüandùëêùëüinstead ofùëäùëû,ùëäùëò
andùëäùë£to deriveùëÑ‚Ä≤,ùêæ‚Ä≤andùëâ‚Ä≤. The(ùëü0,ùëü1,ùëü2,ùëü3)represent the
ranks used during the Tensor Train decomposition. And, ùëÑ‚Ä≤,ùêæ‚Ä≤and
ùëâ‚Ä≤are calculated through matrix multiplication of Z and the result
of TT decomposition, ùëéùëü,ùëèùëüandùëêùëü, respectively. The operations on
these values yield the oucome of TT Attention. The equation for
computingùëÑ‚Ä≤,ùêæ‚Ä≤,ùëâ‚Ä≤is given in Equation (6) and TT Attention in
Equation (7) as follows:
ùëÑ‚Ä≤,ùêæ‚Ä≤,ùëâ‚Ä≤=(ùëç‚äóùëéùëü‚äóùëèùëü‚äóùëêùëü) (6)
TT Attention(ùëç,ùëéùëü,ùëèùëü,ùëêùëü)=softmax 
ùëÑ‚Ä≤ùêæ‚Ä≤
‚àöÔ∏Å
ùëëùëò!
ùëâ‚Ä≤(7)
Extending TT Attention , the process of TT Multihead Atten-
tion is segmented across multiple heads. Hence, in TT Multi-Head
Attention, heads are composed multiplicatively, considering the
batch in actual computations. And, it results in a high-dimensional
configuration. We denote this high-dimensional tensor as ùëç‚àà
R3√ó(ùêµ√óùê∂)√óùëÅ√óùêª. For the operations of TT Multi-Head Attention,
5176KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain SeungWon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, and Youjin Shin
Ïó¨Í∏∞Ïóê  ÏàòÏãùÏùÑ  ÏûÖÎ†•ÌïòÏã≠ÏãúÏò§ .
Multi -Head Attention & AddLinear(w, d) & ReLU‚Ñé0 ‚Ñé1 ‚Ñé2 ‚Ñéùëõ‚àí2‚Ñéùëõ‚àí1 ‚Ä¶
Tensor Train Multi -Head AttentionEncoderRelative Encoding Channel Encoding
Concatenate
Tensor Train Multi -Head AttentionRelative Position Index Channel Position IndexDecoder
‚Ñéùëõ ‚Ñéùëõ ‚Ñéùëõ
Linear(d, w)  & DropoutGRU  Cell Vector
ùëã(ùëñ)‚ààùëÖùêøùëã(ùëñ)‚ààùëÖùêøùëã(ùëñ)‚ààùëÖùëõùê∏√óùëë
ùëã(ùëñ)‚ààùëÖùëõùê∏√óùë§ùëÉùê∏(ùëñ)‚ààùëÖùëõùê∑√óùëë
ùëå(ùëñ)‚ààùëÖùëõùê∑√óùëë
ùëå(ùëñ)‚ààùëÖùëõùê∑√óùë§
ùëå(ùëñ)‚ààùëÖùêªùëå(ùëñ)‚ààùëÖùëõùê∑√óùëë
Figure 3: The model architecture of DASR incorporates Multi-Head Attention before performing segment partitioning on the
data, unlike the Vanilla SegRNN model. Additionally, Tensor Train (TT) Multi-Head Attention is executed before the data
passes through the GRU layers of the encoder and decoder. i is the ùëñùë°‚Ñédimension, and ùëãùëñdenotes the time series data for the
ùëñùë°‚Ñédimension, ùëåùëñdenotes the future time series values for the ùëñùë°‚Ñédimension, ùêøindicates historical look-back window, ùëõùê∏,ùëõùê∑
represents number of segment in encoder and decoder and ùúîmeans segment window length.
Figure 4: The architecture of Attention and Tensor Train Attention. (a) represents conventional Attention. (b) illustrate the
Tensor Train (TT) Tensor Decomposition with ranks (ùëü0,ùëü1,ùëü2,ùëü3)in our experiment. We stack the weight matrices of query,
key, and value used in Multi-Head Attention to form a tensor of shape ùëä‚ààRùëñ1√óùëñ2√óùëñ3, and then applied TT Decomposition. The
decomposed tensors obtained in this method have the shapes ùëéùëü‚ààRùëü0√óùëñ1√óùëü1,ùëèùëü‚ààRùëü1√óùëñ2√óùëü2, andùëêùëü‚ààRùëü2√óùëñ3√óùëü3. (c) represents the
structure of the TT Attention. By performing opeartions on ùëçwithùëéùëü,ùëèùëüandùëêùëüobtained from TT Decomposition, we acquire
ùëÑ‚Ä≤,ùêæ‚Ä≤andùëâ‚Ä≤. These are then calculated to obatin the final outcome of TT Attention.
5177Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
we divide the second dimension of ùëç‚ààR3√ó(ùêµ√óùê∂)√óùëÅ√óùêªproportion-
ally to the number of heads, resulting in a tensor to be restructured
toùëç‚ààR3√óùêµ√óùê∂
‚Ñéùëíùëéùëë√ó‚Ñéùëíùëéùëë√óùëÅ√óùêª, whereùêµrepresents the batch size, ùê∂
denotes the dimension of the input data, ùêªstands for the number
of hidden layer, and ùëÅsignifies the value obtained by dividing the
historical look-back window by the segment‚Äôs window length. The
resultingùëç, along with ùëéùëü,ùëèùëü,ùëêùëü, is calculated using the following
Equation (8):
TT MultiHead(ùëç,ùëéùëü,ùëèùëü,ùëêùëü)=
Concat(head 1,..., headùëñ),(8)
where head ùëñ=TT Attention( ùëçùëñ,ùëéùëü,ùëèùëü,ùëêùëü). The detailed algorithm
for TT Multi-Head Attention is provided in Algorithm 2.
Algorithm 1 Tensor Train Decomposition. After stacking the
weight matrices for query, key, and value into a three-dimensional
structure, Tensor Train Decomposition is performed.
1:Class TTLinear
2:procedure Initialize(ùëñùëõùëùùë¢ùë° _ùëëùëñùëö,ùëüùëéùëõùëò )
3:ùë†ùëíùëôùëì.ùëñùëõùëùùë¢ùë° _ùëëùëñùëö‚Üêùëñùëõùëùùë¢ùë° _ùëëùëñùëö
4:ùë†ùëíùëôùëì.ùëûùë¢ùëíùëüùë¶‚Üêùêøùëñùëõùëíùëéùëüùêøùëéùë¶ùëíùëü(ùëñùëõùëùùë¢ùë° _ùëëùëñùëö,ùëñùëõùëùùë¢ùë° _ùëëùëñùëö)
5:ùë†ùëíùëôùëì.ùëòùëíùë¶‚Üêùêøùëñùëõùëíùëéùëüùêøùëéùë¶ùëíùëü(ùëñùëõùëùùë¢ùë° _ùëëùëñùëö,ùëñùëõùëùùë¢ùë° _ùëëùëñùëö)
6:ùë†ùëíùëôùëì.ùë£ùëéùëôùë¢ùëí‚Üêùêøùëñùëõùëíùëéùëüùêøùëéùë¶ùëíùëü(ùëñùëõùëùùë¢ùë° _ùëëùëñùëö,ùëñùëõùëùùë¢ùë° _ùëëùëñùëö)
7:ùëûùë¢ùëíùëüùë¶ _ùë§ùëíùëñùëî‚Ñéùë°‚Üêùë†ùëíùëôùëì.ùëûùë¢ùëíùëüùë¶.ùë§ùëíùëñùëî‚Ñéùë°
8:ùëòùëíùë¶_ùë§ùëíùëñùëî‚Ñéùë°‚Üêùë†ùëíùëôùëì.ùëòùëíùë¶.ùë§ùëíùëñùëî‚Ñéùë°
9:ùë£ùëéùëôùë¢ùëí _ùë§ùëíùëñùëî‚Ñéùë°‚Üêùë†ùëíùëôùëì.ùë£ùëéùëôùë¢ùëí.ùë§ùëíùëñùëî‚Ñéùë°
10:ùëä‚Üêùë†ùë°ùëéùëêùëò(ùëûùë¢ùëíùëüùë¶ _ùë§ùëíùëñùëî‚Ñéùë°,ùëòùëíùë¶ _ùë§ùëíùëñùëî‚Ñéùë°,ùë£ùëéùëôùë¢ùëí _ùë§ùëíùëñùëî‚Ñéùë°)
11:ùëéùëü,ùëèùëü,ùëêùëü‚Üêùê∂ùëúùëõùëëùë¢ùëêùë°ùëáùëíùëõùë†ùëúùëüùëáùëüùëéùëñùëõùê∑ùëíùëêùëúùëöùëùùëúùë†ùëñùë°ùëñùëúùëõ
12:end procedure
13:function Forward
14: returnùëéùëü,ùëèùëü,ùëêùëü
15:end function
16:End Class TTLinear
5 EXPERIMENT SETUP
To objectively compare the performance of our algorithm, DASR,
we compared it with three different types of baselines: (1) Machine
Learning-based models, (2) Neural Network-based models, and (3)
State of the Arts (SOTA) models. For the neural network baseline
models, the learning rate is set to 0.001, the batch size to 60, the
hidden units to 128, epochs to 50, and dropout rate to 0.1. And,
the Adam optimizer and mean squared error (MSE) are used as the
loss function, and all experiments are repeated three times, and we
average them. These experimental settings are consistent across
comparative baseline models. As described in Section 4, data on
six orbital elements were collected for the year of 2016 from three
different satellites: KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-
5, respectively. We used the data from January to November as
training, and the data from December as testing. Moreover, we
experiment with four different hidden unit 32,64,128,256. Baseline
models we used are provided below:
‚Ä¢Machine Learning-based models: Random Forest Regres-
sor (RFR) [ 5], eXtreme Gradient Boosting Regressor (XGBR) [ 7]Algorithm 2 TT Multi-Head Attention. From the TTLinear class,
we generate a decomposed weight matrix and pass it to the TT
Multi-Head Attention class. Here, "mat" represents matrix multipli-
cation (matmul), and "per" indicates permute
1:Class TT Multi-Head Attention
2:procedure Initialize(ùëñùëõùëùùë¢ùë° _ùëëùëñùëö,ùëüùëéùëõùëò,ùëõ _ùëëùëñùëö,‚Ñéùëíùëéùëëùë† )
3:ùë†ùëíùëôùëì.‚Ñéùëíùëéùëëùë†‚Üê‚Ñéùëíùëéùëëùë†
4:ùë†ùëíùëôùëì.‚Ñéùëíùëéùëë _ùëëùëñùëö‚Üêùëñùëõùëùùë¢ùë° _ùëëùëñùëö/ùëõ_ùëëùëñùëö
5:ùë°ùë°_ùëôùëñùëõùëíùëéùëü‚Üêùëáùëáùêøùëñùëõùëíùëéùëü(ùëñùëõùëùùë¢ùë° _ùëëùëñùëö,ùëüùëéùëõùëò)
6:ùë†ùëíùëôùëì.ùëéùëü,ùë†ùëíùëôùëì.ùëèùëü,ùë†ùëíùëôùëì.ùëêùëü ‚Üêùë°ùë°_ùëôùëñùëõùëíùëéùëü.ùëìùëúùëüùë§ùëéùëüùëë()
7:ùë†ùëíùëôùëì.ùëõ _ùëëùëñùëö‚Üêùëüùëéùëõùëò[2]
8:end procedure
9:function split_heads( ùëëùëì,ùë•,ùë¶ )
10: returnùë•.ùë£ùëñùëíùë§(ùë•.ùë†‚Ñéùëéùëùùëí[0],ùë¶,‚àí1,ùë•.ùë†‚Ñéùëéùëùùëí[2],ùë•.ùë†‚Ñéùëéùëùùëí[3])
11:end function
12:function forward(ùëûùë¢ùëíùëüùë¶,ùëòùëíùë¶,ùë£ùëéùëôùë¢ùëí )
13:ùëëùëì‚Üêùë†ùë°ùëéùëêùëò(ùëûùë¢ùëíùëüùë¶,ùëòùëíùë¶,ùë£ùëéùëôùë¢ùëí)
14:ùë†ùëñùëßùëí‚Üêùëëùëì.ùë†ùëñùëßùëí()
15:ùëëùëì‚Üêùë†ùëùùëôùëñùë° _‚Ñéùëíùëéùëëùë†(ùëëùëì,ùë†ùëñùëßùëí[1]/ùë†ùëíùëôùëì.ùëõ _ùëëùëñùëö)
16:ùë†ùëüùëê‚Üêùëöùëéùë°(ùëëùëì.ùëùùëíùëüùëöùë¢ùë°ùëí(3,1,2,4,0),ùë†ùëíùëôùëì.ùëéùëü)
17:ùëõ‚Üêùëöùëéùë°(ùë†ùëüùëê.ùëùùëíùëü(0,1,4,3,2),ùë†ùëíùëôùëì.ùëèùëü.ùëùùëíùëü(0,2,1))
18:ùëõ‚Üêùëõ/‚àöÔ∏Å
ùë†ùëíùëôùëì.‚Ñéùëíùëéùëë _ùëëùëñùëö
19:ùëõ‚Üêùë†ùëúùëìùë°ùëöùëéùë•(ùëõ,ùëëùëñùëö =‚àí1)
20:ùë†ùëüùëê‚Üêùëöùëéùë°(ùëõ,ùë†ùëíùëôùëì.ùëêùëü.ùëùùëíùëü(2,1,0))
21:ùëúùë¢ùë°‚Üêùë†ùëüùëê.ùëùùëíùëü(2,1,4,0,3).ùëüùëíùë†‚Ñéùëéùëùùëí(ùë†ùëñùëßùëí)
22:ùëúùë¢ùë°‚Üêùëúùë¢ùë°.ùë†ùë¢ùëö(ùëëùëñùëö=0).ùëüùëíùë†‚Ñéùëéùëùùëí(ùëûùë¢ùëíùëüùë¶.ùë†‚Ñéùëéùëùùëí)
23: returnùëúùë¢ùë°
24:end function
25:End Class TT Multi-Head Attention
‚Ä¢Neural Network-based models: Long Short Term Memory
(LSTM) [ 15], Bidirectional Long Short Term Memory (Bi-
LSTM) [ 38], Gated Recurrent Unit (GRU) [ 9], Convolutional
Recurrent Neural Network (CRNN) [52]
‚Ä¢SOTA models: Decomposition Linear (DLinear) [ 49], Time
Series Mixer for Forecasting (TSMixer)
[11], Patch Time Series Transformer (PatchTST) [ 24], Seg-
ment Recurrent Neural Network (SegRNN) [21]
6 RESULTS
6.1 Prediction Performance
The orbital prediction performance for three satellites, KOMPSAT-
3, KOMPSAT-3A, and KOMPSAT-5, with 11 comparative baseline
models is provided in Table 2. Table 2 shows the performance from
2 Machine Learning-based models (RFR, XGBR), 4 Conventional
Neural Network-based models (LSTM, Bi-LSTM, GRU, CRNN), 4
latest SOTA models (DLinear, TSMixer, PatchTST, SegRNN), and
our model, DASR.
As discussed, due to the characteristics of satellite orbits, even a
small margin of error can lead to significant real-world deviations,
with orbits differing by several kilometers. Thus, the performance
of predictions is meticulously quantified using mean squared error
(MSE), and it calculated to the eighth decimal place to be very pre-
cise. The value of the best-performing model is highlighted in bold
with a check mark, while the value of the second-best-performing
5178KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain SeungWon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, and Youjin Shin
Table 2: Orbit prediction performance of 11 different models for KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-5, with prediction
errors calculated as MSE averages from three experiments. Our DASR model shows the best performance in all cases.
Mo
delPr
ediction Performance (MSE)
K
OMPSAT-3 K
OMPSAT-3A K
OMPSAT-5
32
64 128 256 32
64 128 256 32
64 128 256
RFR 0.04651498 0.08581217 0.01825760
X
GBR 0.00378736 0.04503533 0.00583716
LSTM 0.00251912
0.00213945 0.00164690 0.00128193 0.10545561
0.08553699 0.05701103 0.07072379 0.00014196
0.00014680 0.00012791 0.00013452
Bi-LSTM 0.00230655
0.00165754 0.00140196 0.00121383 0.06025440
0.03473916 0.04059353 0.08609690 0.00026163
0.00022403 0.00012228 0.00005914
GRU 0.00951377
0.00684603 0.00526623 0.00495451 0.11035253
0.10354365 0.0736679 0.06681571 0.00033342
0.00031882 0.00022465 0.00016057
CRNN 0.00774537
0.00478547 0.00457154 0.00433512 0.08958013
0.06739727 0.06243696 0.07603665 0.00018648
0.00018286 0.00017021 0.00021430
DLinear 0.00384668 0.00969054 0.00384668
TSMixer 0.11679286
0.20405618 0.09865192 0.14202163 0.51438513
0.23561098 0.41945294 0.21939782 0.01159294
0.01864500 0.01327116 0.01062426
PatchTST 0.00520019
0.00392849 0.00405307 0.00400788 0.04706915
0.03609573 0.02294843 0.01432871 0.00371225
0.00128053 0.00131530 0.00135468
SegRNN 0.00407130
0.00391889 0.00392820 0.00374528 0.01713368
0.01745800 0.01869889 0.01765216 0.00076148
0.00080853 0.00075577 0.00079435
DASR (ours) 0.00018776
0.00017381 0.00015354 ‚úì0.00013940 0.00117961 0.00114086 ‚úì0.00098952 0.00125097 0.00004000
0.00003728 0.00003306 ‚úì0.00002763
Impr
ovement 88.52% 89.79% 53.28%
model is indicated in bold without a check mark. In addition, values
indicated by underlines are the best-performing model among 10
comparative models, excluding our model DASR. In the last row,
‚ÄúImprovement‚Äù means a percentage of the accuracy improvement of
the best-performing model among all models (values in bold with
a check mark), compared to the best-performing model except for
ours (underlined values).
Note that for networks with hidden units, we recorded the pre-
diction error values for each h = [32,64,128,256], whereas for the
RFR, XGBR, and DLinear, which are networks without hidden units,
we reported a single prediction error value.
Performance of KOMPSAT-3. Our model DASR shows the
best performance, consistently outperforming across all hidden
unit sizes, with the best performance at a hidden unit count of 256,
showing an error of 0.00013940. The second-best performing model
is also the DASR with a hidden unit size of 128, showing an error of
0.00015354. Excluding the DASR, the best performance comes from
the Bi-LSTM model at a hidden unit count of 256, with an error
of 0.00121383. Compared to this Bi-LSTM model, the best DASR
model improves the prediction performance by 88.52%
Performance of KOMPSAT-3A. The DASR model consistently
delivers the highest performance regardless of the hidden unit
sizes, with the best performance at 128 hidden units, showing
an error of 0.00098952. The DASR model with 64 hidden units
shows the second-best performance with an error of 0.00114086.
The best model excluding DASR is the DLinear model with an er-
ror of 0.00969054. The best-performing model, DASR, leads to an
improvement of 89.79% when compared with the DLinear model.
Performance of KOMPSAT-5. Moreover, our DASR model
achieves the best performance across different sizes of hidden units.
At 256 hidden units, it shows the best performance with an error of
0.00002763, followed by the second-best performance at 128 hidden
units with an of 0.00003306. Excluding the DASR, the best perfor-
mance is observed with the Bi-LSTM model at 256 hidden units,
showing an error of 0.00005914. The best DASR model achieves an
improvement of 53.28% compared to the Bi-LSTM model.
In summary, these experimental results clearly demonstrate that
our DASR model, which has TT Multi-Head Attention structure,
shows the best performance across all datasets and hidden unit
counts.6.2 Ablation Study for TT Multi-Head Attention
The core structure of DASR consists of two main components: Im-
posing the Multi-Head Attention and applying the Tensor Train
Decomposition. We demonstrate the effectiveness of both com-
ponents through this section. We demonstrate the utility of both
components in this section.
We experimented on three different combinations of networks:
(1) SegRNN, (2) SegRNN with Multi-Head Attention, and (3) Seg-
RNN with Tensor Train Multi-Head Attention (DASR). The exper-
imental settings and methods were the same as described in the
previous sections. For different satellites and hidden unit sizes, the
experimental results are shown in Table 3, denoting (1) SegRNN, (2)
SegRNN with Multi-Head Attention, and (3) SegRNN with Tensor
Train Multi-Head Attention as SegRNN, SegRNN + Att., SegRNN +
Att. + TT (DASR), respectively.
The values in bold with check mark represent the best-performing
model for each dataset. Our DASR model, which applies both the
Multi-Head Attention and the Tensor Train Decomposition, shows
the best performance for all three datasets, showing error values of
0.00013940, 0.00098952, and 0.00002763, respectively. Even when
comparing three models within the same hidden size, the DASR
model shows the best performance in all cases. The underlined
values indicate the best-performing models between SegRNN and
SegRNN + Att. models, except the DASR model. SegRNN + Att.
model shows the best performance, excluding the DASR, archiving
error values of 0.00234810, 0.00907815, and 0.00004286. When com-
paring two models (SegRNN and SegRNN + Att.) within the same
hidden size, the SegRNN + Att. model shows better performance in
all cases but the KOMPSAT-3 (h=256).
In the case of the KOMPSAT-3 (h=256), SegRNN shows slightly
better performance, but the difference between the two models
is very trivial. This experiment demonstrates that imposing the
Multi-Head Attention into the vanilla SegRNN enhances predic-
tion accuracy. Additionally, it demonstrates that applying both
Multi-Head Attention and Tensor Train Decomposition yields even
greater improvements in performance.
6.3 Ablation Study for TT Decomposition
The vanilla SegRNN model, without any additional components,
has the smallest number of parameters across all hidden unit sizes.
5179Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 3: Orbit prediction performance of three different combinations of networks: (1) SegRNN, (2) SegRNN with Multi-Head
Attention, and (3) SegRNN with Tensor Train Multi-Head Attention (DASR). SegRNN with a Multi-Head Attention model
surpasses that of the vanilla SegRNN, and the SegRNN with Tensor Train Multi-Head Attention (DASR) model shows the best
performance.
Mo
delPr
ediction Performance (MSE)
K
OMPSAT-3 K
OMPSAT-3A K
OMPSAT-5
32
64 128 256 32
64 128 256 32
64 128 256
SegRNN 0.00407130
0.00391889 0.00392820 0.00374528 0.01713368
0.01745800 0.01869889 0.01765216 0.00076148
0.00080853 0.00075577 0.00079435
SegRNN + Att. 0.00282000 0.00234810
0.00253905 0.00386698 0.00911346 0.00907815
0.01074785 0.01079840 0.00008382
0.00004435 0.00010163 0.00004286
SegRNN
+ Att. + TT
(DASR (ours))0.00018776
0.00017381 0.00015354 ‚úì0.00013940 0.00117961
0.00114086 ‚úì0.00098952 0.00125097 0.00004000
0.00003728 0.00003306 ‚úì0.00002763
Table 4: Figures in bold represent the model with the fewest
number of parameters, while figures underlined denote the
model with the second fewest number of parameters. ‚Äôh‚Äô
means hidden unit‚Äôs counts. The model with the second
fewest parameters is the DASR, and the model with the fewest
parameters is the SegRNN.
Mo
delNumb
er of Parameters
(
h = 32) (
h = 64) (
h = 128) (
h = 256)
SegRNN 6,901 26,085 101,317 399,237
SegRNN
+ Att. 11,741 43,789 169,325 666,157
SegRNN
+ Att. + TT
(DASR (ours))38,246 57,878 134,006 433,718
When Multi-Head Attention is added, the number of parameters
increases to 11,741 from 6,901 and 43,789 from 26,085 for hidden unit
sizes of 32 and 64, respectively. When both Multi-Head Attention
and TT Decomposition are applied, it further increases to 38,246
and 57,878, respectively.
However, for hidden unit sizes of 128 and 256, the number of
parameters when both components (Multi-Head Attention and Ten-
sor Train Decomposition) are added is less than when only one
component (Tensor Train Decomposition) is added. The parame-
ters decrease from 169,325 to 134,006 and from 666,157 to 433,718,
respectively. This is due to the characteristic of Tensor Train Decom-
position, which decomposes the original data into multiple cores
with small size of ranks. When the original data is small (h=32, 64),
the number of parameters may increase due to the multiple cores.
However, for larger original data (h=128, 256), the small size of
ranks can reduce the number of parameters. Applying Tensor Train
Decomposition does not always result in a reduction of parameters,
but as described in Tables 2 and 3, the best prediction performance
is consistently achieved with larger hidden unit sizes (h=128, 256).
Therefore, if the hidden unit size is sufficiently large, applying
Tensor Train Decomposition not only helps achieve the highest
prediction performance but also aids in reducing the number of
parameters.
7 DEPLOYMENT PLAN & REPRODUCIBILITY
All experiments and tests in this paper were conducted using the
currently operational KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-
5 satellites managed by the National Satellite Operation Center
in Korea Aerospace Research Institute (KARI). As our system is
demonstrated to be very effective for predicting the orbits of a rangeof satellites, it is planned to be deployed in Jan. 2025 for KARI to
support the existing satellite monitoring system in South Korea.
For reproducibility of the results, we share and release our source
code here2. Upon acceptance of the paper, we plan to release the
entire code to foster research in this field.
8 CONCLUSION
In this study, we propose the Decomposed Attention Segment Re-
current Neural Network (DASR) to accurately predict the orbit of
satellites. Our DASR model uses Multi-Head Attention inside the
vanilla SegRNN network and decomposes the weight matrices of
the Multi-Head Attention using Tensor Train (TT) Decomposition.
For the dataset collected from KOMPSAT-3, KOMPSAT-3A, and
KOMPSAT-5 satellites of KARI, we compare our method to 11 dif-
ferent models including the latest SOTA prediction algorithms. Our
DASR model shows the best prediction performance in all cases for
all satellites. In addition, we demonstrate the utility of major two
components, Multi-Head Attention and Tensor Train Decomposi-
tion through additional ablation studies in terms of the prediction
performance and parameter reduction.
9 ACKNOWLEDGMENTS
This work was supported by the Research Fund of The Catholic
University of Korea in 2023, and the National Research Founda-
tion of Korea(NRF) grant funded by the Korea government(MSIT)
(No. RS-2023-00213456). Also, this work was partly supported by
Institute for Information & communication Technology Planning &
evaluation (IITP) grants funded by the Korean government MSIT:
(RS-2022-II221199, Graduate School of Convergence Security at
Sungkyunkwan University), (No. RS-2024-00337703, Development
of satellite security vulnerability detection techniques using AI
and specification-based automation tools), (No. 2022-0-01045, Self-
directed Multi-Modal Intelligence for solving unknown, open do-
main problems), (No. RS-2022-II220688, AI Platform to Fully Adapt
and Reflect Privacy-Policy Changes), (No. RS-2021-II212068, Arti-
ficial Intelligence Innovation Hub), (No. 2019-0-00421, AI Gradu-
ate School Support Program at Sungkyunkwan University), (No.
RS-2023-00230337, Advanced and Proactive AI Platform Research
and Development Against Malicious Deepfakes), and (No. RS-2024-
00356293, AI-Generated Fake Multimedia Detection, Erasing, and
Machine Unlearning Research).
2https://github.com/Jeong-Seung-Won/Decomposition-Segment-Recurrent-Neural-
Network
5180KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain SeungWon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, and Youjin Shin
REFERENCES
[1]Marcella Astrid and Seung-Ik Lee. 2017. CP-decomposition with Tensor Power
Method for Convolutional Neural Networks compression. In 2017 IEEE Inter-
national Conference on Big Data and Smart Computing (BigComp). 115‚Äì118.
https://doi.org/10.1109/BIGCOMP.2017.7881725
[2]Astronomy. 2023. ‚ÄúAstronomy". https://www.astronomy.com/space-exploration/
the-future-of-satellites-lies-in-the-constellations/
[3]Nurulhuda Firdaus Mohd Azmi, Siti Sophiayati Yuhaniz, et al .2021. An Adap-
tation of Deep Learning Technique In Orbit Propagation Model Using Long
Short-Term Memory. In 2021 International Conference on Electrical, Communica-
tion, and Computer Engineering (ICECCE). 1‚Äì6.
[4] LLC Blue Origin. 2007-2023. ‚ÄúBlue Origin". https://www.blueorigin.com/
[5]Leo Breiman. 2001. Random Forests. Machine Learning 45, 1 (2001), 5‚Äì32. https:
//doi.org/10.1023/A:1010933404324
[6]Zachariah Carmichael and Dhireesha Kudithipudi. 2019. Stochastic Tucker-
Decomposed Recurrent Neural Networks for Forecasting. In 2019 IEEE Global
Conference on Signal and Information Processing (GlobalSIP). 1‚Äì5. https://doi.org/
10.1109/GlobalSIP45357.2019.8969554
[7]Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD
‚Äô16). Association for Computing Machinery, New York, NY, USA, 785‚Äì794. https:
//doi.org/10.1145/2939672.2939785
[8]Yuwei Chen and Kaizhi Wang. 2019. Prediction of Satellite Time Series Data Based
on Long Short Term Memory-Autoregressive Integrated Moving Average Model
(LSTM-ARIMA). In 2019 IEEE 4th International Conference on Signal and Image
Processing (ICSIP). 308‚Äì312. https://doi.org/10.1109/SIPROCESS.2019.8868350
[9]Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Ben-
gio. 2014. Learning Phrase Representations using RNN Encoder‚ÄìDecoder for
Statistical Machine Translation. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP). 1724‚Äì1734. https:
//doi.org/10.3115/v1/D14-1179
[10] Space Exploration Technologies Corp. 2002-2023. ‚ÄúSpaceX". https://www.spacex.
com/
[11] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant
Kalagnanam. 2023. TSMixer: Lightweight MLP-Mixer Model for Multivariate
Time Series Forecasting. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (, Long Beach, CA, USA,) (KDD ‚Äô23).
Association for Computing Machinery, New York, NY, USA, 459‚Äì469. https:
//doi.org/10.1145/3580305.3599533
[12] Mateusz Gabor and Rafa≈Ç Zdunek. 2023. Compressing convolutional neural
networks with hierarchical Tucker-2 decomposition. Applied Soft Computing 132
(2023), 109856.
[13] Alex Graves and J√ºrgen Schmidhuber. 2005. Framewise phoneme classification
with bidirectional LSTM and other neural network architectures. Neural Networks
18, 5 (2005), 602‚Äì610.
[14] Richard A. Harshman. 1970. Foundations of the PARAFAC procedure: Models
and conditions for an "explanatory" multi-model factor analysis. 16 (1970), 1‚Äì84.
[15] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Comput. 9, 8 (1997), 1735‚Äì1780.
[16] M.S. Islam and E. Hossain. 2021. Foreign exchange currency rate prediction using
a GRU-LSTM hybrid network. Soft Computing Letters 3 (2021), 100009.
[17] KARI. 1989-2023. Korea Aerospace Research Institute. https://www.kari.re.kr
[18] T. Kelecy and Moriba Jah. 2009. Analysis of orbital prediction accuracy improve-
ments using high fidelity physical solar radiation pressure models for tracking
high area-to-mass ratio objects. European Space Agency, (Special Publication) ESA
SP672 (01 2009).
[19] Andrey N. Kolmogorov. 1933. Sulla determinazione empirica di una legge di
distribuzione. Giornale dell‚ÄôIstituto Italiano degli Attuari 4 (1933), 83‚Äì91.
[20] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S.
Lempitsky. 2015. Speeding-up Convolutional Neural Networks Using Fine-tuned
CP-Decomposition. In 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,
Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6553
[21] Shengsheng Lin, Weiwei Lin, Wentai Wu, Feiyu Zhao, Ruichao Mo, and Haotong
Zhang. 2023. SegRNN: Segment Recurrent Neural Network for Long-Term Time
Series Forecasting. ArXiv abs/2308.11200 (2023). https://api.semanticscholar.org/
CorpusID:261064627
[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and
Schahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for
Long-Range Time Series Modeling and Forecasting. In International Conference on
Learning Representations. https://api.semanticscholar.org/CorpusID:251649164
[23] Ye Liu and Michael K. Ng. 2022. Deep neural network compression by Tucker
decomposition with nonlinear response. Knowledge-Based Systems 241 (2022),
108171.
[24] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. InInternational Conference on Learning Representations.
[25] Orbit.ing-now. 2023. ‚ÄúOrbit.ing-now". https://orbit.ing-now.com/
[26] Alaa Osama, Mourad Raafat, Ashraf Darwish, Sara Abdelghafar, and Aboul Ella
Hassanien. 2022. Satellite Orbit Prediction Based on Recurrent Neural Network
using Two Line Elements. In 2022 5th International Conference on Computing and
Informatics (ICCI). 298‚Äì302. https://doi.org/10.1109/ICCI54321.2022.9756063
[27] I. V. Oseledets. 2011. Tensor-Train Decomposition. SIAM Journal on Scientific
Computing 33, 5 (2011), 2295‚Äì2317.
[28] Hao Peng and Xiaoli Bai. 2018. Exploring Capability of Support Vector Ma-
chine for Improving Satellite Orbit Prediction Accuracy. Journal of Aerospace
Information Systems 15 (04 2018), 1‚Äì16. https://doi.org/10.2514/1.I010616
[29] Hao Peng and Xiaoli Bai. 2019. Gaussian Processes for improving orbit prediction
accuracy. Acta Astronautica 161 (2019), 44‚Äì56.
[30] Pedro F. Proen√ßa and Yang Gao. 2020. Deep Learning for Spacecraft Pose Esti-
mation from Photorealistic Rendering. In 2020 IEEE International Conference on
Robotics and Automation (ICRA). 6007‚Äì6013. https://doi.org/10.1109/ICRA40945.
2020.9197244
[31] Cristina Puente, Maria Ana S√°enz-Nu√±o, Augusto Villa-Monte, and Jos√© Angel
Olivas. 2021. Satellite Orbit Prediction Using Big Data and Soft Computing
Techniques to Avoid Space Collisions. Mathematics 9, 17 (2021).
[32] Zonghua Qu and Chunling Wei. 2023. A PSO-LSTM-based Method for Spatial
Target Orbit Phase Prediction. In 2023 4th International Conference on Computer
Vision, Image and Deep Learning (CVIDL). 358‚Äì362. https://doi.org/10.1109/
CVIDL58838.2023.10166272
[33] Haoli Ren, Xiaolin Chen, Bei Guan, Yongji Wang, Tiantian Liu, and Kongyang
Peng. 2019. Research on Satellite Orbit Prediction Based on Neural Network
Algorithm. In Proceedings of the 2019 3rd High Performance Computing and Cluster
Technologies Conference. 267‚Äì273.
[34] USA RocKet Lab. 2006-2023. ‚ÄúRocket Lab". https://www.https://www.
rocketlabusa.com//
[35] Umesh Saini, Rajesh Kumar, Vipin Jain, and M.U Krishnajith. 2020. Univariant
Time Series forecasting of Agriculture load by using LSTM and GRU RNNs. In
2020 IEEE Students Conference on Engineering & Systems (SCES). 1‚Äì6. https:
//doi.org/10.1109/SCES50439.2020.9236695
[36] Nor‚Äôasnilawati Salleh, Nurulhuda Firdaus Mohd Azmi, and Siti Sophiayati
Yuhaniz. 2021. An Adaptation of Deep Learning Technique In Orbit Propa-
gation Model Using Long Short-Term Memory. In 2021 International Confer-
ence on Electrical, Communication, and Computer Engineering (ICECCE). 1‚Äì6.
https://doi.org/10.1109/ICECCE52056.2021.9514264
[37] Oneweb Satellites. 2012-2022. ‚ÄúOneWeb". https://www.oneweb.net/
[38] M. Schuster and K.K. Paliwal. 1997. Bidirectional recurrent neural networks.
IEEE Transactions on Signal Processing 45, 11 (1997), 2673‚Äì2681.
[39] Youjin Shin, Eun-Ju Park, Simon S. Woo, Okchul Jung, and Daewon Chung. 2022.
Selective Tensorized Multi-layer LSTM for Orbit Prediction. In Proceedings of
the 31st ACM International Conference on Information & Knowledge Management
(Atlanta, GA, USA) (CIKM ‚Äô22). Association for Computing Machinery, New York,
NY, USA, 3495‚Äì3504. https://doi.org/10.1145/3511808.3557138
[40] U Thissen, R van Brakel, A.P de Weijer, W.J Melssen, and L.M.C Buydens. 2003.
Using support vector machines for time series prediction. Chemometrics and
Intelligent Laboratory Systems 69, 1 (2003), 35‚Äì49.
[41] Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. 2018. Tensor Decompo-
sition for Compressing Recurrent Neural Network. In 2018 International Joint
Conference on Neural Networks (IJCNN) . 1‚Äì8. https://doi.org/10.1109/IJCNN.2018.
8489213
[42] Ledyard R. Tucker. 1966. Some mathematical on three-mode factor analysis.
Psychometrika (1966), 279‚Äì311.
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),
Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[44] Dingheng Wang, Guangshe Zhao, Guoqi Li, Lei Deng, and Yang Wu. 2020. Com-
pressing 3DCNNs based on tensor train decomposition. Neural Networks 131
(2020), 215‚Äì230.
[45] Kan Wang, Jiawei Liu, Hang Su, Ahmed El-Mowafy, and Xuhai Yang. 2022. Real-
Time LEO Satellite Orbits Based on Batch Least-Squares Orbit Determination
with Short-Term Orbit Prediction. Remote Sensing 15 (12 2022), 133. https:
//doi.org/10.3390/rs15010133
[46] Wikipedia. 2023. ‚ÄúTwo-line element set". https://en.wikipedia.org/wiki/Two-
line_element_set
[47] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer:
Decomposition Transformers with Auto-Correlation for Long-Term Series Fore-
casting. In Advances in Neural Information Processing Systems.
[48] Miao Yin, Siyu Liao, Xiao-Yang Liu, Xiaodong Wang, and Bo Yuan. 2020. Com-
pressing Recurrent Neural Networks Using Hierarchical Tucker Tensor Decom-
position. ArXiv abs/2005.04366 (2020).
5181Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
[49] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are transformers
effective for time series forecasting? (AAAI‚Äô23/IAAI‚Äô23/EAAI‚Äô23). AAAI Press,
Article 1248, 8 pages. https://doi.org/10.1609/aaai.v37i9.26317
[50] Naiju Zhai, Peifu Yao, and Xiaofeng Zhou. 2020. Multivariate Time Series Fore-
cast in Industrial Process Based on XGBoost and GRU. In 2020 IEEE 9th Joint
International Information Technology and Artificial Intelligence Conference (ITAIC),
Vol. 9. 1397‚Äì1400. https://doi.org/10.1109/ITAIC49862.2020.9338878
[51] Li Zhang, Wei-Da Zhou, Pei-Chann Chang, Ji-Wen Yang, and Fan-Zhang Li. 2013.
Iterated time series prediction with multiple support vector regression models.
Neurocomputing 99 (2013), 411‚Äì422.
[52] Zao Zhang and Yuan Dong. 2020. Temperature Forecasting via Convolutional
Recurrent Neural Networks Based on Time-Series Data. Complexity 2020 (03
2020), 1‚Äì8. https://doi.org/10.1155/2020/3536572
[53] Peining Zhen, Ziyang Gao, Tianshu Hou, Yuan Cheng, and Hai-Bao Chen. 2022.
Deeply Tensor Compressed Transformers for End-to-End Object Detection. Pro-
ceedings of the AAAI Conference on Artificial Intelligence 36, 4 (2022), 4716‚Äì4724.[54] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long
Sequence Time-Series Forecasting. In Thirty-Fifth AAAI Conference on Artificial
Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of
Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances
in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press,
11106‚Äì11115.
[55] Mingyi Zhou, Y. Liu, Zhen Long, Longxi Chen, and Ce Zhu. 2019. Tensor rank
learning in CP decomposition via convolutional neural network. Signal Process.
Image Commun. 73 (2019), 12‚Äì21.
[56] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
2022. FEDformer: Frequency Enhanced Decomposed Transformer for Long-
term Series Forecasting. In Proceedings of the 39th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 162), Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
Sabato (Eds.). PMLR, 27268‚Äì27286. https://proceedings.mlr.press/v162/zhou22g.
html
5182