Harm Mitigation in Recommender Systems under User
Preference Dynamics
Jerry Chee
jerrychee@cs.cornell.edu
Cornell University
Ithaca, NY, USAShankar Kalyanaraman
kshankar@meta.com
Meta
Menlo Park, CA, USASindhu Kiranmai Ernala
sindhuernala@meta.com
Meta
Menlo Park, CA, USA
Udi Weinsberg
udi@meta.com
Meta
Menlo Park, CA, USASarah Dean
sdean@cornell.edu
Cornell University
Ithaca, NY, USAStratis Ioannidis
ioannidis@ece.neu.edu
Northeastern University
Boston, MA, USA
ABSTRACT
We consider a recommender system that takes into account the
interplay between recommendations, the evolution of user interests,
and harmful content. We model the impact of recommendations
on user behavior, particularly the tendency to consume harmful
content. We seek recommendation policies that establish a tradeoff
between maximizing click-through rate (CTR) and mitigating harm.
We establish conditions under which the user profile dynamics
have a stationary point, and propose algorithms for finding an
optimal recommendation policy at stationarity. We experiment on
a semi-synthetic movie recommendation setting initialized with
real data and observe that our policies outperform baselines at
simultaneously maximizing CTR and mitigating harm.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems; â€¢Human-
centered computing â†’Social media .
KEYWORDS
Recommender Systems; Harm Mitigation; Amplification; User Pref-
erence Modeling
ACM Reference Format:
Jerry Chee, Shankar Kalyanaraman, Sindhu Kiranmai Ernala, Udi Weinsberg,
Sarah Dean, and Stratis Ioannidis. 2024. Harm Mitigation in Recommender
Systems under User Preference Dynamics. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671925
1 INTRODUCTION
The algorithm of choice for many recommender systems in produc-
tion today is the classic top- ğ‘˜recommendation algorithm [ 15,19,
35]. In short, a top- ğ‘˜recommender uses a score function to rank
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671925items from a candidate pool and subsequently recommends the
ğ‘˜highest-scoring items. Such recommendations also come with
optimality guarantees: for example, if scores are proportional to
a userâ€™s selection probabilities, top- ğ‘˜recommendations maximize
the click-through rate (CTR) [ 51,52], i.e., the probability that the
user will pick at least one item from the recommended set.
A prominent criticism against top- ğ‘˜recommendations (and,
more generally, maximizing engagement) is centered around the
concept of amplification [34], i.e., a feedback loop that arises be-
tween recommended content and user preferences. Adverse effects
of amplification include decreasing content diversity and amplify-
ing biases [ 30,48], as well as increasing the spread of misinforma-
tion [ 74] or extreme content [ 56,68]. Recent studies also explore the
impact of recommendations from a userâ€™s perspective: recommen-
dations may create pathways that steer users towards radicalization
[20], polarization [ 42,43] but may also aid the migration towards
ever more extreme [56, 57] or even harmful content [29, 44, 62].
Motivated by these concerns, we model a recommender system
that takes into account the interplay between recommendations
and the evolution of user interests. In doing so, our work follows a
long line of research in understanding the interplay between recom-
mender systems and user behavior (see, e.g., [ 18,34,45]). Our goal
is to understand (a) the impact of recommendations on pathways
towards, e.g., more extreme or harmful content, and (b) how this
impact should be accounted for when making recommendations.
In particular, the fundamental question that we try to answer is
how should recommendations depart from top- ğ‘˜/CTR-maximizing
recommendations when user preference dynamics are present, and
mitigating subsequent harm is part of the objective.
Our model gives rise to complicated, interesting phenomena
related to recommendation safety and harm mitigation. For exam-
ple, our results (Thms. 1 and 3) suggest that naÃ¯ve approaches to
mitigating harm may fail catastrophically. On one hand, explicitly
incorporating harm as a penalty in the recommenderâ€™s objective
may have no effect in the optimal recommendation policy, if user
preference dynamics are ignored. On the other hand, designing
optimal recommendations when dynamics are accounted for does
not come easy: surprisingly, we prove that data-mining workhorses
such as alternating optimization lead to arbitrarily suboptimal rec-
ommendations. Overall, our contributions are as follows:
â€¢We introduce a model that incorporates harm mitigation in a
recommenderâ€™s objective. When user preferences are static,
 
255
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jerry Chee et al.
we show that top- ğ‘˜recommendations maximize CTR while
simultaneously minimizing harm. This finding suggests that
to properly mitigate harm, it is necessary to account for user
dynamics in the recommenderâ€™s reasoning.
â€¢To that end, we incorporate the user preference dynamics of
attraction [ 23,38,45,48], which explictly model the afore-
mentioned amplification phenomenon. Finding optimal rec-
ommendations under these dynamics is a non-convex prob-
lem. We prove a negative result: the most natural algorithm,
namely, alternating optimization, leads to recommendation
policies that are in fact arbitrarily suboptimal.
â€¢In light of this, we turn our attention to gradient-based al-
gorithms. We propose a tractable method for computing
the gradients when preference dynamics are incorporated
in the objective. We also evaluate resulting gradient-based
recommendation algorithms on a semi-synthetic movie rec-
ommendation setting initialized with MovieLens data, where
harmful movies are determined by IMDB parental guidelines.
We show that our policies are superior at maximizing CTR
and mitigating harm over baselines, and perform up to 77%
better.
Our model, though simple, is based on the multinomial-logit (MNL)
model of choice [ 49], which is ubiquitous in recommender systems
literature [ 13,17,34,71]. From a technical standpoint, our anal-
ysis requires characterizing the stationary point implied by the
combination of recommendations with attraction dynamics; we
accomplish this by using the Banach fixed point theorem. Having
characterized the fixed-point, implementing gradient-based opti-
mization algorithms in our setting poses a significant challenge, as
our objective is not expressed in closed form. Nevertheless, we pro-
pose a tractable algorithm for computing gradients via the implicit
function theorem.
The remainder of this paper is organized as follows. We discuss
related work in Section 2. We define our model accounting for
harm-mitigation, and determine optimal recommendations in the
absence of preference dynamics in Section 3. We present our analy-
sis under user preference dynamics in Section 4, and our empirical
evaluations in Section 5. We conclude is Section 6.
2 RELATED WORK
Characterizing Harm. The literature observing and empirically
characterizing the harmful impact of algorithmic recommendations
is quite extensive. Past works have experimentally observed the ex-
istence radicalization pathways [ 57], polarization and filter bubbles
[40,43,59], the amplification of extreme content [ 56,68], alignment
with human values [ 25], and loss of physical and mental well-being
[44,62]. Though these studies focus on empirical observations as
opposed to the design of harm-mitigating recommendation poli-
cies, which is our main goal, they directly motivate our attempt to
model the tension between maximizing engagement and reducing
exposure to harm.
Multiple Objectives. Multi-objective recommender systems are
extensively studiedâ€“see, e.g., the surveys by Zheng and Wang [75]
and Jannach [31]. Examples include balancing engagement with
diversity [ 66], fairness [ 70], and multi-stakeholder utility [ 64]. Wuet al. [69] propose alternate optimization and gradient-based ap-
proaches, albeit for maximizing utility when making recommenda-
tions collectively to a group of users. While not directly considering
harm reduction as an objective, Suna and Nasraouia [63] propose
methods to account for user polarization in matrix factorization-
based recommender systems. Closer to us, Singh et al. [61]use RL to
balance an engagement-based reward with a notion of harm termed
the â€œhealth riskâ€. We depart from all these works by explicitly mod-
eling user behavior via attraction, as well as by incorporating the
possibility that users reject the recommendation and opt instead
for organically-selected content. Our model allows for more inter-
pretable decision-making (see, e.g., our discussion around Lemmas 1
and Thms. 1â€“2) as well as a better understanding of how different
parameters impact optimal recommendations and the utility/harm
they incur.
Multinomial-Logit Model. For the majority of our analysis, we
model user preferences via the multinomial-logit (MNL) model [ 49],
itself an instance of the more general Plackett-Luce model [ 46] (see
Sec. 3). Both are used extensively to model user choices: they are
workhorses in the field of econometrics, but have also found numer-
ous applications in computer science, particularly in recommender
systems [ 13,17,21,27,32,34,36,51,71]. A popular theoretical
setting is contextual MNL bandits, [ 1,51,52], in which a static
user profile is learned online while top- ğ‘˜recommendations occur;
recommendation algorithms in this setting aim to minimize regret
w.r.t. the CTR of an algorithm that knows the user profile. Our main
departure from this literature is to model a dynamic user profile
and incorporate harm. We also depart by studying recommendation
policies in â€œsteady-stateâ€; revisiting our results in an online/bandit
setting is an interesting and challenging open problem, particularly
due to the user preference dynamics [ 47,73,76], which are not
considered in the aforementioned MNL bandit works.
Preference Dynamics. Models of user preferences are widely
used and studied in recommendation literature. Historically, the
focus has been on static models (e.g. matrix factorization or topic
models), but an emerging line of work models the dynamics of pref-
erences, also termed individual feedback loops [53]. Such models
of preference dynamics formalize and illuminate phenomena like
political polarization [ 42,59] and echo chambers [ 33,34,54], in
which the long term influence of recommendations plays a crucial
role. Many of these works show how feedback loops lead to unin-
tended consequences of traditional recommendation algorithms,
critiquing the status quo and proposing alternative algorithms.
For example, Carroll et al. [11] and Ashton and Franklin [3]ar-
gue that recommendations which cause user preferences to shift
may be viewed as inappropriate â€œmanipulation, â€ while Carroll et al.
[11] and Dean and Morgenstern [18] propose algorithms to avoid
this. While some authors propose general purpose algorithms in
Markov decision processes [ 11,65], most others investigate specific
dynamics models connected to particular phenomena of interest.
Curmei et al. [16] advocate for a framework mapping between such
preference dynamics models in computer science and known psy-
chological effects. We note that the majority of the aforementioned
works [ 18,33,34,42,53,59,65] study steady-state dynamics of their
proposed models, as we do here; however, they do not directly opti-
mize a recommendation policy towards attaining a certain objective
in steady state.
 
256Harm Mitigation in Recommender Systems under User Preference Dynamics KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Closest to us, Lu et al. [45] propose a model of user preference
dynamics including attraction, aversion, and social influence. Like
us, they study optimal recommendations in steady state. However,
their underlying recommendation model is based on real-valued
feedback in matrix factorization, as opposed to discrete user choice
in the MNL model. From a technical standpoint, their dynamics
are simpler, and the steady-state profiles can be computed in a
closed form (rather than via a fixed point). Moreover, their rec-
ommendation design reduces to a quadratic optimization problem
in their setting, which they solve via a semi-definite relaxation.
Both are technically less challenging than the optimization prob-
lem we study here. Furthermore, their focus is on long term user
satisfaction rather than avoiding harmful content.
Integrity Checks. Algorithms for detecting and filtering content
that is inappropriate or harmful prior to recommendations, i.e. â€œin-
tegrity checks,â€ have been extensively explored by academia and
industry alike [ 2,24,39,41,55,72]. The design of such filtering
mechanisms is orthogonal to the question we study. In particular,
our starting point is the assumption of perfect filtering, as the recom-
mender never suggests harmful content, which only exists outside
the platform: users can find it organically irrespective of recom-
mendations. It is natural to ask if the recommender has a role in
mitigating this type of indirect harm, that it cannot directly control.
Surprisingly, our work answers affirmatively. If user dynamics are
present, then the recommender may cause indirect harm, and its
mitigation is non-trivial: naÃ¯ve ways of modeling harm or simple
algorithms like alternating optimization may fail catastrophically.
Operationalization. There are many ways to conceive of and
operationalize harm. In the context of recommendations, Shelby
et al. [60] and Hosseini et al. [28] consider representational and
allocative harms. Other work considers the meaning of harm from
a causal perspective, including loss of utility [ 5] and counterfactual
outcome frameworks [ 58]. We limit our scope to harm arising
from engagement with problematic content. Our approach relies on
modelling the causal mechanisms by which a recommender system
may increase the likelihood of harm, in order to reduce it.
3 MODELING HARM-MITIGATING
RECOMMENDATIONS
We consider a recommender making sequential suggestions to a
user. The user can accept the recommendation, by selecting an item
in this set, or reject it, by making an â€œorganicâ€ selection of items that
exist off-platform; this may lead to a selection of harmful content;
we would like to take this into account when recommending items.
We describe here a model of this behavior assuming that user
preferences are static, and characterize optimal recommendation
policies; as we will see, these differ drastically from recommenda-
tions when user preference dynamics are taken into account; we
consider these in the next section.
3.1 Recommendation Policies
We consider a universe of ğ‘›items Î©âŠ‚Rğ‘‘, with|Î©|=ğ‘›, where
each item is represented by a ğ‘‘-dimensional item profileğ‘£âˆˆÎ©(e.g.,
explicit features, latent factors). A subset ğ»âŠ‚Î©, with|ğ»|=â„, is
known to be harmful. In general, set Î©includes both on-platformÎ© universe of items; Î©âŠ†Rğ‘‘,|Î©|=ğ‘›
ğ» subset of Î©containing harmful items, |ğ»|=â„
ğ¶ candidatesğ¶âŠ†Î©\ğ»sampled from collection C.
ğ¸ set of items recommended, ğ¸âŠ†ğ¶
ğ‘£ item in Î©
ğ‘¢ vector inRğ‘‘representing user profile
ğœ‹ recommendation policy
CLK user accepts an item suggested by the recommender
ORG user rejects recommended items and chooses an item from
Î©
H user selects an item from ğ»
ğ‘ ğ‘£ score denoting userâ€™s preference for ğ‘£
ğ‘ ğ´ Sum of scoresÃ
ğ‘£âˆˆğ´ğ‘ ğ‘£
ğ‘ scaling param to control prob. of CLK(lowerâ‡’more likely)
for MNL model
ğ‘” mapR+â†’[0,1]; for MNL,ğ‘”(ğ‘ ğ¸)=ğ‘ ğ¸/(ğ‘ ğ¸+ğ‘)
ğ‘¢0 â€œinherentâ€ user preference vector
ğ‘¢(ğ‘¡),ğ‘£(ğ‘¡)current user profile and chosen item at time ğ‘¡(resp.)
ğ›½ weight param for ğ‘¢ğ‘¡â€™s dependence on ğ‘¢0(higher means
more)
ğ›¼ğ‘£(ğ‘¡) weight param for ğ‘¢ğ‘¡â€™s dependence on ğ‘£(ğ‘¡)
ğ‘˜ num. of items shown to user by the policy
ğœ† regularization parameter
Table 1: Notation Summary
items, that a recommender may display to users, as well as off-
platform items. At each timeslot ğ‘¡, a recommender has access to
a subsetğ¶ğ‘¡âŠ†Î©\ğ»of candidate on-platform items for possible
recommendation to users. We assume that these candidate sets are
sampled in an i.i.d. fashion from a collection Cof (possibly overlap-
ping) subsets of Î©\ğ». Having access to this set, the recommender
displays to a user a set ğ¸ğ‘¡âŠ‚ğ¶ğ‘¡; we refer to ğ¸ğ‘¡as the recommended
setor, simply, the recommendation.
Intuitively, the recommender could be any platform that rec-
ommends content, and Î©could be all the content on the internet,
including harmful content. For example, news could be recom-
mended on a social media platform with links out to external news
sites; some â€œfake newsâ€ sites will never be recommended but users
could still find them through other means. Note that, by design, the
recommender never suggests harmful content , and harmful-content
is only found off-platform.
We consider (possibly) randomized recommendation policies :
given a candidate set ğ¶, the recommendation policy is expressed
as a distribution over possible sets ğ¸âŠ†ğ¶to recommend to the
user. Recommendations are constrained. In the bounded cardinal-
itysetting, we assume recommended sets ğ¸have at most ğ‘˜âˆˆN
items. In the independent sampling setting, we assume that (a) items
ğ‘£âˆˆğ¸are selected independently from ğ¶and (b) the expected size
of recommendations is at most ğ‘˜, i.e.,E[|ğ¸||ğ¶]â‰¤ğ‘˜. Independent
sampling constrains the size of ğ¸only in expectation, but poli-
cies can be described with polynomially many parameters (in ğ‘›,ğ‘˜),
compared with Î˜(2ğ‘˜)parameters for bounded cardinality (see also
Appendix A in [14]).
3.2 User Preferences & Selection Behavior
At each timeslot, the user can accept the recommendation ğ¸, by
selecting an item in this set, or reject it, by selecting an arbitrary
 
257KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jerry Chee et al.
item in Î©.We refer to the former event as a CLKevent and the latter
as an ORG(for â€œorganic selectionâ€) event. Only ORGevents can lead
to a harmful engagement.
We assume that user selections are governed by the Plackett-
Luce model [ 46]. In particular, for every ğ‘£âˆˆÎ©, there exists a
non-negative score ğ‘ ğ‘£âˆˆR+quantifying a userâ€™s preference toward
ğ‘£. Moreover, the probability that a user selects an item among a
set of alternatives is proportional to this score. Formally, for a set
of itemsğ´âŠ†Î©, let the total score be ğ‘ ğ´â‰¡Ã
ğ‘£âˆˆğ´ğ‘ ğ‘£. Plackett-Luce
postulates that the probability that a user selects item ğ‘£âˆˆğ´against
alternatives ğ´is given byğ‘ ğ‘£/ğ‘ ğ´. In our setting, given recommen-
dationğ¸, the user is also faced with an additional alternative of
rejecting the recommendation and selecting an item organically.
We assume that
ğ‘CLK|ğ¸â‰¡ğ‘”(ğ‘ ğ¸), and, thus ğ‘ORG|ğ¸=1âˆ’ğ‘”(ğ‘ ğ¸),(1)
whereğ‘”:R+â†’[0,1]is a function of the total score of ğ¸. Moreover,
applied to our setting, the Plackett-Luce model gives the following
conditional probabilities of items ğ‘£âˆˆğ¸,ğ‘£â€²âˆˆÎ©being selected,
respectively:
ğ‘ğ‘£|ğ¸,CLK=ğ‘ ğ‘£
ğ‘ ğ¸and ğ‘ğ‘£â€²|ğ¸,ORG=ğ‘ ğ‘£â€²
ğ‘ Î©. (2)
3.3 Top-ğ‘˜Recommendations
As items are linked to scores, a simple, intuitive policy is the top-ğ‘˜
recommendations policy [ 35] (see also Appendix A in [ 14]). Givenğ¶
and budgetğ‘˜, the policy recommends the ğ‘˜highest-scoring items
ğ‘£âˆˆğ¶, w.r.t., scores ğ‘ ğ‘£. This can be implemented in ğ‘‚((|ğ¶|+ğ‘˜)logğ‘˜)
time by traversing the list of scores ğ‘ ğ‘£,ğ‘£âˆˆğ¶, and maintaining a
sorted list of the highest-scoring items.
3.4 Optimal Recommendations under
Static Preferences
Departing from traditional recommender systems, but assuming
static preferences, we would like to select a policy that maximizes
CTR while simultaneously minimizing harm ; to do so, we wish to
solve:
Maximize :ğ‘“0(ğœ‹)=ğ‘CLK(ğœ‹)âˆ’ğœ†ğ‘H(ğœ‹) (3a)
subj. to :ğœ‹âˆˆP, (3b)
whereğœ†is a trade-off parameter, ğœ‹is the recommendation policy
specifying[ğ‘ğ¸|ğ¶]ğ¶âŠ†Î©\ğ»,ğ¸âŠ†ğ¶, and
ğ‘CLK=Eğ¸,ğ¶[ğ‘”(ğ‘ ğ¸)], ğ‘ H=(1âˆ’ğ‘CLK)ğ‘ ğ»
ğ‘ Î©, (4)
are the probabilities of click ( CLK) and harm ( H) events, respectively.
The setPis the set of valid policies, determined either the bounded
cardinality or independent sampling constraints with capacity ğ‘˜âˆˆ
N(see see Eqs. (19) and (20)â€“(21) in Appendix A of [ 14]). In short,
Prob. (3)aims to find a valid recommendation policy ğœ‹âˆˆP that
establishes an optimal tradeoff between click-through rate ğ‘CLKand
probability of harm ğ‘H, as determined by parameter ğœ†â‰¥0.
Note that, despite the fact that it never recommends harmful
content, the recommender does impact the probability of harm:
this is because it can reduce harm by attracting the user to its(non-harmful) recommendations. This has a rather suprising con-
sequence: the top- ğ‘˜recommendations policy is optimal even if the
recommender objective includes harm mitigation (i.e., for all ğœ†>0):
Theorem 1. Assume that function ğ‘”:R+â†’[0,1]in Eq. (1)is
non-decreasing. Then for all ğœ†â‰¥0the top-ğ‘˜recommendation policy is
an optimal solution to Prob. (3)under the bounded cardinality setting.
If, in addition, function ğ‘”is concave, then top- ğ‘˜recommendation
policy is also optimal under the independent sampling setting.
The proof can be found in Appendix B of [ 14]. It is straightfor-
ward for the bounded cardinality setting; the independent sampling
case is somewhat more involved, and requires a submodularity
argument. Intuitively, since the recommender never recommends
harmful content, maximizing the click-through rate is also harm-
minimizing : recommending content likely to be clicked reduces
the chance of an organic selection and thus of selecting harmful
content. Thm. 1 is in stark contrast to what happens when the user
preferences are affected by recommendations, as we will see in
the following section. Moreover, it tells an important cautionary
tale: if recommenders treat users as static, recommendations that
seemingly account for harm mitigation, by directly including it in
the objective, may end up just maximizing click-through rate. This,
however, can be catastrophic in terms of the actual harm (and the
corresponding objective attained), as we will again see in the next
section.
3.5 Multinomial Logit (MNL) Model
We conclude this section by presenting the standard multinomial
logit (MNL) model, used extensively to model user choices in recom-
mender systems [ 1,27,32,34,51,52]. A special case of the Plackett-
Luce model, it is a natural generalization of matrix factorization
from ratings to choices, and an extension of â€œsoft-maxâ€, allowing
for a no-choice alternative (see also Appendix D in [ 14]). Formally,
for everyğ‘£âˆˆÎ©, the non-negative score ğ‘ ğ‘£âˆˆR+quantifying a
userâ€™s preference toward ğ‘£is
ğ‘ ğ‘£â‰¡ğ‘’ğ‘£âŠ¤ğ‘¢, (5)
i.e., it is parameterized by both the item profile ğ‘£âˆˆRğ‘‘as well as by
auser profileğ‘¢âˆˆRğ‘‘. Moreover, the conditional probability ğ‘CLK|ğ¸
is given by:
ğ‘”(ğ‘ ğ¸)â‰¡ğ‘ ğ¸
ğ‘ ğ¸+ğ‘, (6)
whereğ‘â‰¥0is a non-negative constant. The remaining probabilities
are given again by the Plackett-Luce model (Eq. (2)), with scores
determined by Eq. (5).
It is easy to confirm that ğ‘”:R+â†’[0,1]is both non-decreasing
and concave. Thm. 1 thus has the following immediate corollary:
Corollary 1. If user selections are governed by the MNL model
under a static profile ğ‘¢âˆˆRğ‘‘, then for all ğœ†â‰¥0the top-ğ‘˜recom-
mendation policy is an optimal solution to Prob. (3)under both the
bounded cardinality and independent sampling settings.
 
258Harm Mitigation in Recommender Systems under User Preference Dynamics KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
RecommenderUserCatalog w/out Harmful Items : Î©âˆ–HRec Policy: Ï€Rec Set: EtTotal Catalog: Î©Candidate Items: CtPrev User Proï¬le: u(t)Initial Proï¬le: u0
Selected Item: v(t)New User Proï¬le: u(t+1)t
Figure 1: Our full model, incorporating user dynamics in the presence of
recommender system interactions. A recommender presents a recommenda-
tion setğ¸ğ‘¡to the user, who chooses to either interact with the recommended
content ( CLK), or organically select an item from the entire catalog ( ORG), which
includes harmful content. The user profile is subsequently updated under at
attraction model [ 23,38,45,48], leaning closer to the item ğ‘£(ğ‘¡)selected by the
user.
4 HARM-MITIGATING RECOMMENDATIONS
UNDER DYNAMIC USER PROFILES
We turn our attention to what happens when user preferences adapt
based on the content that users consume. As we will see, this has dras-
tic implications for harm-mitigating recommendations. A graphical
model indicating the evolution of profiles under user dynamics is
shown in Fig 1.
4.1 User Profile Dynamics
For the remainder of our analysis, we adopt MNL as the model
governing user choices. Thus, there exists a user profile ğ‘¢âˆˆRğ‘‘
that, jointly with item profiles ğ‘£âˆˆRğ‘‘, determines item scores
via Eq. (5). So far (e.g., in Cor. 1), we assumed that the user profile is
static; we will now depart from this assumption.
Formally, we denote by ğ‘¢(ğ‘¡)the profile of user at time ğ‘¡âˆˆN.
Letğ‘£(ğ‘¡)âˆˆÎ©be the item selected and consumed by the user at
timeslotğ‘¡. We model the influence of an item with an attraction
model [ 23,38,45,48]. First, the user is characterised by an inherent
profileğ‘¢0âˆˆRğ‘‘, that captures their predisposition towards content.
At timeğ‘¡+1, the profile ğ‘¢(ğ‘¡+1)is generated as a convex combination
of (a) the inherent profile, (b) the select item ğ‘£(ğ‘¡), and (c) the current
profileğ‘¢(ğ‘¡). Formally, starting from some ğ‘¢(0)âˆˆRğ‘‘, we have:
ğ‘¢(ğ‘¡+1)=E
ğ›½ğ‘¢0+ğ›¼ğ‘£(ğ‘¡)ğ‘£(ğ‘¡)+(1âˆ’ğ›¼ğ‘£(ğ‘¡)âˆ’ğ›½)ğ‘¢(ğ‘¡)
, (7)
where the randomness is due to the selected item ğ‘£(ğ‘¡), andğ›½âˆˆ[0,1],
ğ›¼ğ‘£âˆˆ[0,1âˆ’ğ›½],ğ‘£âˆˆÎ©, control the the relative importance of the
inherent profile and the item, respectively. This is a â€œmean-fieldâ€ or
â€œfluidâ€ model [ 6,7,10], in which the evolution of the user profile is
governed by mean dynamics. It is also an attraction model, as at ev-
ery timeslot, the user profile â€œnudgesâ€ towards selected object ğ‘£(ğ‘¡)
with a rateğ›¼ğ‘£(ğ‘¡)âˆˆ[0,1]: by Eq. (5), this increases the probability
that the user will select items similar to the ones it consumed in
the past. Note that ğ‘£(ğ‘¡)is a random variable whose distribution de-
pends on the recommendation policy ğœ‹and the current user profile
ğ‘¢(ğ‘¡)(see Eq.(27) in [ 14]). In general, we allow the â€œattractivenessâ€
to be content dependent. For simplicity of exposition, from here
forward, we assume that ğ›¼ğ‘£=ğ›¼H,ifğ‘£âˆˆğ»,andğ›¼ğ‘£=ğ›¼NH,ifğ‘£âˆ‰ğ».Stationary Profiles and Steady State Behavior. Given a recom-
mendation policy, we are interested in understanding the steady-
state behavior of a user, as described by the limit limğ‘¡â†’âˆğ‘¢(ğ‘¡). We
thus turn our attention to the stationary points of this process,
which, as it turns out, we can compute. We define the mean profile
drift as:
Î”ğ‘¢(ğ‘¡)â‰¡ğ‘¢(ğ‘¡+1)âˆ’ğ‘¢(ğ‘¡)
=Eğœ‹
ğ›¼ğ‘£(ğ‘¡)(ğ‘£(ğ‘¡)âˆ’ğ‘¢(ğ‘¡))
+ğ›½(ğ‘¢0âˆ’ğ‘¢(ğ‘¡)).(8)
A profile Â¯ğ‘¢âˆˆRğ‘‘isstationary when Î”ğ‘¢(ğ‘¡)=0forğ‘¢(ğ‘¡)=Â¯ğ‘¢: that is,
if the system starts from this profile, it will remain there indefinitely.
Note that, if limğ‘¡â†’âˆğ‘¢(ğ‘¡)exists, it must be a stationary profile.
4.2 Recommendation Objective
We are now ready to revisit our recommendation objective. We de-
note byâ‰¯the set of all possible recommendation policies (including
invalid ones, not in P). Given policy ğœ‹âˆˆâ‰¯, letğœ‹â†¦â†’Â¯ğ‘¢(ğœ‹)be the
map fromğœ‹to the stationary profile Â¯ğ‘¢, as defined above (see also
Eq. (10) below). We consider the following optimization problem:
Maximize :ğ‘“(ğœ‹)=ğ‘CLK(ğœ‹,Â¯ğ‘¢(ğœ‹))âˆ’ğœ†ğ‘H(ğœ‹,Â¯ğ‘¢(ğœ‹)) (9a)
subj. to :ğœ‹âˆˆP, (9b)
for some regularization parameter ğœ†â‰¥0. Crucially, both the CTR
and the probability of harm are measured at the stationary profile
Â¯ğ‘¢(ğœ‹), our proxy for the steady-state limit of the user dynamics.
Note that, even though the recommendation policy never suggests
harmful content, recommendations affect user selections, which in
turn affect their predisposition towards harmful content.
Solving Prob. (9)is quite challenging. Beyond the fact that it is not
a convex optimization problem, finding a solution is complicated by
the fact that the stationary profiles depend on the recommendation
policyğœ‹via map Â¯ğ‘¢(ğœ‹), which we cannot express in a closed-form.
Thus, we have no direct way of computing it; it is a priori unclear
even whether such a stationary profile exists and is unique. One of
our contributions is to resolve both of these issues (see Thm. 2).
Computing Â¯ğ‘¢(ğœ‹).We now turn our attention to finding the sta-
tionary user profile. We fist characterize the stationary profile via
a fixed-point equation. Eq. (8) implies the following lemma:
Lemma 1. Stationary user profiles Â¯ğ‘¢âˆˆRğ‘‘satisfy the following
fixed-point equation:
Â¯ğ‘¢=ğ¹(ğœ‹,Â¯ğ‘¢), (10)
where mapğ¹:â‰¯Ã—Rğ‘‘is given by:
ğ¹(ğœ‹,ğ‘¢)=ğ›½ğ‘¢0+ğ›¼HÃ
ğ‘£âˆˆğ»[ğ‘£Â·ğ‘ğ‘£(ğœ‹,ğ‘¢)]+ğ›¼NHÃ
ğ‘£âˆ‰ğ»[ğ‘£Â·ğ‘ğ‘£(ğœ‹,ğ‘¢)]
ğ›½+ğ›¼Hğ‘H(ğœ‹,ğ‘¢)+ğ›¼NHğ‘NH(ğœ‹,ğ‘¢). (11)
whereğ‘H,ğ‘NH, are given by Eq. (4), andğ‘ğ‘£is the probability that the
user selects item ğ‘£âˆˆÎ©.
The proof is in Appendix C in [ 14]. Intuitively, a stationary
profile consists of an interpolation between the inherent profile
ğ‘¢0and the â€œaverageâ€ harmful and non-harmful item profiles. The
weights of this average are determined by the policy.
Next, we turn to the question of finding a stationary profile
which satisfies the fixed point equation. In particular, we establish
conditions under which map ğ¹in Eq. (11) is a contraction:
 
259KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jerry Chee et al.
Theorem 2. Letâˆ¥Â·âˆ¥ denote the Euclidean norm in Rğ‘‘. If
âˆ¥ğ‘£âˆ¥<1
6 âˆšï¸‚
âˆ¥ğ‘¢0âˆ¥2+12ğ›¼NH+ğ›½
5ğ‘›ğ‘‘ğ›¼ğ»âˆ’âˆ¥ğ‘¢0âˆ¥, (12)
for allğ‘£âˆˆÎ©, thenğ¹:â‰¯Ã—Rğ‘‘given by Eq. (10)is a contraction w.r.t. âˆ¥Â·âˆ¥
uniformly on all distributions ğœ‹âˆˆâ‰¯. That is, for all distributions ğœ‹âˆˆ
â‰¯(not necessarily inP), and allğ‘¢,ğ‘¢â€²âˆˆRğ‘‘,âˆ¥ğ¹(ğœ‹,ğ‘¢)âˆ’ğ¹(ğœ‹,ğ‘¢â€²)âˆ¥â‰¤
ğ¿âˆ¥ğ‘¢âˆ’ğ‘¢â€²âˆ¥,for someğ¿<1that does not depend on ğœ‹.
We prove this in Appendix E in [ 14]. Thm. 2 along with the
Banach fixed-point theorem [ 4] imply that a stationary profile exists
and is unique when Eq. (12)holds. Most importantly, it can be found
by the following iterative process: starting from any Â¯ğ‘¢0âˆˆRğ‘‘, the
iterations
Â¯ğ‘¢â„“+1=ğ¹(ğœ‹,Â¯ğ‘¢â„“), â„“âˆˆN, (13)
are guaranteed to converge to the unique fixed-point Â¯ğ‘¢âˆ—of Eq. (10).
Convergence happens exponentially fast (see Appendix E in [14]).
Note that Eq. (12)holds w.l.o.g.: multiplying every ğ‘£âˆˆÎ©with a
constantğœ<1andğ‘¢0with 1/ğœyields exactly the same scores in
Eq.(5). Hence, inherent user profiles and item profiles learned from
data (e.g., user-item clicks), can be rescaled so that Eq. (12) holds.
4.3 Alternating Optimization
Armed with Thm. 2, we turn our attention to algorithms for solving
the (non-convex) Prob. (9)in its general form. A simple approach
is via an alternating optimization/EM-like algorithm. In particular,
one could start from, e.g., the userâ€™s initial profile ğ‘¢0, and iterate as
follows, for â„“âˆˆN:
ğœ‹â„“+1=arg maxğœ‹âˆˆP ğ‘CLK(ğœ‹,ğ‘¢â„“)âˆ’ğœ†ğ‘H(ğœ‹,ğ‘¢â„“), (14a)
ğ‘¢â„“+1=Â¯ğ‘¢(ğœ‹â„“). (14b)
In step (14a) , givenğ‘¢â„“, the optimal ğœ‹is determined by the top- ğ‘˜
policy (by Thm. 1). In step (14b) , givenğœ‹â„“, the map Â¯ğ‘¢is computed
via the iterative algorithm in Eq. (13). The algorithm thus alter-
nates between finding an optimal recommendation policy and the
corresponding stationary profile. Unfortunately, this approach fails:
Theorem 3. The alternating optimization steps in Algorithm (14)
can be arbitrarily suboptimal: for every ğ‘€>0, we can construct an
instance of Prob. (9)with optimal solution ğœ‹âˆ—for which the solution
ğœ‹produced by Algorithm (14)satisfiesğ‘“(ğœ‹âˆ—)âˆ’ğ‘“(ğœ‹)>ğ‘€.
The proof is in Appendix F in [ 14]. On a high level, even though
Alg. (14)produces a different solution than the top- ğ‘˜policy with
profileğ‘¢0, as it takes into account the steady state profile, Thm. 1
implies that this trajectory is independent of ğœ†. This leads to arbi-
trarily suboptimal decisions when ğœ†is high, and minimizing the
probability of harm is paramount.
4.4 Gradient-Based Algorithms
The suboptimality of alternating optimization highlights the impor-
tance of understanding the evolution ğœ‹andÂ¯ğ‘¢(ğœ‹)jointly. This can
indeed be accomplished via a continuous optimization algorithm
like projected gradient ascent (PGA) [ 8]. The constraint set in both
the bounded cardinality and the independent sampling settings
is a convex polytope. Applying first-order methods to solve this
requires computing the gradient of the objective (9a). However, thisposes a challenge exactly because we cannot describe Â¯ğ‘¢(Â·)in closed
form. Nevertheless, we show that âˆ‡Â¯ğ‘¢(Â·)and, subsequently âˆ‡ğ‘“(Â·),
can be computed using the implicit function theorem [ 9,22]. We
briefly review PGA for the bounded cardinality setting below, and
provide implementation details for PGA in both constraint settings
in the supplement. Nevertheless, we stress that these gradients
can be used in other standard solvers; we demonstrate this in our
experiments, where we use the (more powerful) SLSQP solver [ 37]
instead.
Projected Gradient Ascent. PGA starts from a feasible policy
ğœ‹0âˆˆP and proceeds iteratively via:
ğœ‹â„“+1=projP
ğœ‹â„“+ğ›¾â„“âˆ‡ğœ‹ğ‘“(ğœ‹â„“,Â¯ğ‘¢(ğœ‹â„“))
, â„“âˆˆN, (15)
where projP(ğœ‹â€²)=arg minğœ‹âˆˆPâˆ¥ğœ‹âˆ’ğœ‹â€²âˆ¥2
2is the projection to P.
AsPis a union of simplices (see Appendix A in [ 14]),projPhas
efficient implementations (see, e.g., [50]).
Computing the Gradient for the Bounded Cardinality Setting.
Letğ‘šâ‰¡Ã
ğ¶âˆˆC|Dğ¶|=Ã
ğ¶âˆˆC |ğ¶|
ğ‘˜be the number of parameters
inğœ‹âˆˆP. By the chain rule, we have:
âˆ‡ğœ‹ğ‘“(ğœ‹,Â¯ğ‘¢(ğœ‹))=âˆ‡ğœ‹U(ğœ‹,Â¯ğ‘¢)+[âˆ‡ğœ‹Â¯ğ‘¢(ğœ‹)]âŠ¤âˆ‡ğ‘¢U(ğœ‹,ğ‘¢),(16)
whereU(ğœ‹,ğ‘¢) â‰¡ğ‘CLK(ğœ‹,ğ‘¢)âˆ’ğœ†ğ‘H(ğœ‹,ğ‘¢),andâˆ‡ğœ‹U(ğœ‹,Â¯ğ‘¢) âˆˆRğ‘š,
âˆ‡ğ‘¢U(ğœ‹,Â¯ğ‘¢)âˆˆRğ‘‘are the gradients of Uw.r.t. its first and second
arguments, respectively, evaluated at inputs ğœ‹andÂ¯ğ‘¢=Â¯ğ‘¢(ğœ‹). As we
have closed-form, differentiable expressions for ğ‘Handğ‘NH, these
two gradients can be computed via standard means, while Â¯ğ‘¢=Â¯ğ‘¢(ğœ‹)
can be computed via Eq. (13).
We thus turn our attention to computing âˆ‡ğœ‹Â¯ğ‘¢(ğœ‹)âˆˆRğ‘‘Ã—ğ‘š, i.e.,
the Jacobian of map Â¯ğ‘¢(Â·)w.r.t.ğœ‹, which we cannot express in closed
form. Observe that ğœ‹âˆˆ[0,1]ğ‘š, while Â¯ğ‘¢âˆˆRğ‘‘. Eq. (10)implies that
ğ¹(ğœ‹,Â¯ğ‘¢)âˆ’Â¯ğ‘¢=0,whereğ¹is given by (11). This is a system of ğ‘‘
non-linear equations, involving both ğœ‹andÂ¯ğ‘¢as unknowns. Hence,
by the implicit function theorem [ 22], the Jacobianâˆ‡ğœ‹Â¯ğ‘¢(ğœ‹), can be
computed via:
âˆ‡ğœ‹Â¯ğ‘¢(ğœ‹)=âˆ’(âˆ‡ğ‘¢ğ¹(ğœ‹,Â¯ğ‘¢)âˆ’ğ¼ğ‘‘Ã—ğ‘‘)âˆ’1Â·âˆ‡ğœ‹ğ¹(ğœ‹,Â¯ğ‘¢), (17)
whereâˆ‡ğœ‹ğ¹(ğœ‹,Â¯ğ‘¢)âˆˆRğ‘‘Ã—ğ‘š,âˆ‡ğ‘¢ğ¹(ğœ‹,Â¯ğ‘¢)âˆˆRğ‘‘Ã—ğ‘‘are the Jacobians of
mapğ¹w.r.t. its two arguments, evaluated again at ğœ‹andÂ¯ğ‘¢=Â¯ğ‘¢(ğœ‹).
Asğ¹is a closed-form, differentiable function, constituent matrices in
the r.h.s. can be computed by standard techniques, and âˆ‡ğœ‹Â¯ğ‘¢(ğœ‹)can
be obtained by solving a linear system (to avoid matrix inversion).
Additional details, along with ways of computing the gradient for
the independent sampling setting, can be found in Appendix G
in [14].
Computational Complexity and Tractability. For the bounded
cardinality setting, gradient computations via Eqs. (16)â€“(17)involve
(a) linear algebra operations (matrix multiplications, solving linear
systems) and (b) computing the stationary point Â¯ğ‘¢(ğœ‹). The former
can be done in polynomial time in constituent matrix dimensions
ğ‘šandğ‘‘. The latter can be computed via the iterations in Eq. (13):
each iteration is again polynomial in ğ‘šandğ‘‘, and convergence is
exponentially fast (see also the discussion below Thm. 2). Having
computed the gradient, using Michelotâ€™s algorithm [ 50], the pro-
jection in Eq. (15)isğ‘‚(ğ‘šlogğ‘š), while the remaining operations in
PGA steps (15)are linear in ğ‘š. Finally, all of the above statements
 
260Harm Mitigation in Recommender Systems under User Preference Dynamics KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A
ction
Policy Obje
ctiveğ‘“(â†‘)ğ‘CLK(â†‘) ğ‘H(â†“)
Grad -4.694
(Â±1.330) 0.810 (Â±0.037) 0.055 (Â±0.013)
Alt -6.268
(Â±2.117) 0.774 (Â±0.060) 0.070 (Â±0.021)
U0 -6.279
(Â±2.417) 0.778 (Â±0.066) 0.071 (Â±0.024)
Unif -15.681
(Â±0.555) 0.487 (Â±0.011) 0.162 (Â±0.005)
A
dventure
Policy Obje
ctiveğ‘“(â†‘)ğ‘CLK(â†‘) ğ‘H(â†“)
Grad -3.445
(Â±1.162) 0.676 (Â±0.091) 0.041 (Â±0.011)
Alt -3.793
(Â±1.304) 0.666 (Â±0.088) 0.045 (Â±0.012)
U0 -4.160
(Â±1.738) 0.645 (Â±0.119) 0.048 (Â±0.016)
Unif -9.243
(Â±0.222) 0.303 (Â±0.009) 0.095 (Â±0.002)
Come
dy
Policy Obje
ctiveğ‘“(â†‘)ğ‘CLK(â†‘) ğ‘H(â†“)
Grad -3.816 (Â±0.673) 0.717
(Â±0.049) 0.045 (Â±0.006)
Alt -4.725
(Â±1.536) 0.684 (Â±0.084) 0.054 (Â±0.015)
U0 -5.556
(Â±2.032) 0.645 (Â±0.111) 0.062 (Â±0.019)
Unif -11.318
(Â±0.520) 0.306 (Â±0.013) 0.116 (Â±0.005)
Fantasy
Policy Obje
ctiveğ‘“(â†‘)ğ‘CLK(â†‘) ğ‘H(â†“)
Grad -2.754
(Â±0.862) 0.649 (Â±0.081) 0.034 (Â±0.008)
Alt -2.760
(Â±0.849) 0.662 (Â±0.073) 0.034 (Â±0.008)
U0 -3.425
(Â±1.416) 0.603 (Â±0.122) 0.040 (Â±0.013)
Unif -7.957
(Â±0.368) 0.223 (Â±0.009) 0.082 (Â±0.004)
Sci-Fi
Policy Obje
ctiveğ‘“(â†‘)ğ‘CLK(â†‘) ğ‘H(â†“)
Grad -2.925
(Â±1.319) 0.812 (Â±0.056) 0.037 (Â±0.013)
Alt -4.772
(Â±2.339) 0.742 (Â±0.094) 0.055 (Â±0.022)
U0 -4.618
(Â±2.108) 0.749 (Â±0.082) 0.054 (Â±0.020)
Unif -12.485
(Â±0.473) 0.385 (Â±0.012) 0.129 (Â±0.005)
Table 2: Key metrics ( ğ‘“,ğ‘CLK, andğ‘H) for all policies under the 5 genre datasets,
forğ›¼H=0.25,ğ‘NH=0.5,ğ›½=0.15,ğœ†=100, and a dataset-specific ğ‘âˆˆ[1,20], as
reported in Appendix H in [ 14], for the bounded cardinality setting with ğ‘˜=1.
We report means and standard deviations across 100 users. Across all genre
datasets, our gradient-based policy achieves superior recommendation objec-
tive. Note that, even though we report standard deviations, the performance
improvement also occurs on a per-user basis (see also the PDFs of objective
differences, which we report in Fig. 2 and Appendix H in [ 14]). Interestingly,
across all genres, Grad attains both a lower ğ‘Hand a higher ğ‘CLKthan competi-
tors.
extend to the independent sampling setting, with ğ‘šreplaced by
ğ‘šâ€²â‰¡Ã
ğ¶âˆˆC|ğ¶|(see Appendices A and G in [14]).
We stress that bounded cardinality setting operations are efficient
only ifğ‘šis polynomial in ğ‘›; this is the case only when ğ‘˜is fixed
(e.g., whenğ‘˜=3), and the recommender only recommends a few
items to the user. In contrast, as ğ‘šâ€²=Ã
ğ¶âˆˆC|ğ¶|, the independent
sampling setting can be polynomial in ğ‘›for allğ‘˜: for example, if
Cis a partition of Î©\ğ», thenğ‘šâ€²=ğ‘›âˆ’â„, which does not depend
onğ‘˜.
10
 5
 0
Objective:/uni00A0Policy/uni00A0/uni00AD/uni00A0Grad
100101102Num/uni00A0UsersGrad/uni00A0Policy
Alt/uni00A0Policy
U0/uni00A0Policy
Unif/uni00A0Policy(a) Action
5
 0
Objective:/uni00A0Policy/uni00A0/uni00AD/uni00A0Grad
100101102Num/uni00A0UsersGrad/uni00A0Policy
Alt/uni00A0Policy
U0/uni00A0Policy
Unif/uni00A0Policy (b) Adventure
Figure 2: PDF of the per-user difference between the objective attained by
different policies minus the objective by attained by Grad, with experimental
settings as in Table 2. Grad dominates other policies in terms of the objective
on a per user basis. Additional genres, and impact on ğ‘CLK,ğ‘Hare shown in
Appendix H in [14].
1.0 1.2 1.4
v,vNH
1.01.21.4v,vH
NH/uni00A0movie
Grad/uni00A0Rec
Alt/uni00A0Rec
U0/uni00A0Rec
(a) Action
1.0 1.2 1.4 1.6
v,vNH
1.01.21.41.6v,vH
NH/uni00A0movie
Grad/uni00A0Rec
Alt/uni00A0Rec
U0/uni00A0Rec (b) Adventure
Figure 3: Visualization of the movies each policy recommends, with respect to
their inner product with the mean non-harmful and harmful vector, under
the setting reported in Table 2, for a specific user in the Action and Adventure
genres. All non-harmful movies are embedded via a +sign in this plane;
the support of each policy is indicated by additional symbols (we omit Unif
as its support is everything). We observe that the gradient-based policy is
more concentrated towards the right, i.e., on movies which have high inner
product with the average non-harmful vector. Remaining genres are shown
in Appendix H in [14].
5 EXPERIMENTS
5.1 Experimental Setup
Dataset. We use the MovieLens25m dataset to extract movie and
item profiles and the IMDB parental guideline ratings [ 12,26] to
determine our definition of harmful movies. The IMDB parental
guideline ratings comprise 5 categories: (1) Sex & Nudity, (2) Vio-
lence & Gore, (3) Profanity, (4) Alcohol, Drugs, & Smoking, and (5)
Frightening & Intense Scenes. For these experiments, we consider
a movie harmful if it is â€œSevereâ€â€”the worst parental ratingâ€”in any
of these categories. We join the MovieLens25m dataset with the
IMDB data set via the IMDB movie identifiers, and then create 5
sub-datasets by filtering containing movies from 5 categories: â€œAc-
tionâ€, â€œAdventureâ€, â€œComedyâ€, â€œFantasyâ€, and â€œSci-Fiâ€. We restrict
each dataset to the 100 movies with the most ratings and 100 users,
with user and movie profiles of dimension ğ‘‘=12constructed via
matrix factorization. Learned profiles play the role of ğ‘¢0andğ‘£in
our experiments. Full details on the this pre-processing pipeline,
including hyperparameters, can be found in Appendix H in [ 14].
We make our code publicly available.1
Model Parameters. We use MF user profiles as ğ‘¢0(inherent pro-
files), and item profiles ğ‘£as extracted from the above matrix fac-
torization process. We use these values into our model to compute
the steady-state behavior of each of different policies, but also
simulate the evolution of the user profiles under these policies.
1https://github.com/jerry-chee/HarmMitigation
 
261KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jerry Chee et al.
Figure 4: Effect of modifying ğœ†,ğ›½, andğ‘on the objective attained by different policies for the Action genre. Additional genres, and impact on ğ‘CLK,ğ‘H, are shown in
Appendix H in [ 14]. Increasing any parameter decreases the objective attained by every policy. We observe that increasing ğœ†naturally increases the performance gap
of the Grad policy. Increasing ğ›½has the opposite effect, as it limits the ability of all policies to impact a userâ€™s profile. Parameter ğ‘also increases the improvement of
Grad over other policies as, the larger ğ‘is, the less likely the recommendation is to be accepted, and the more important it becomes to succesfully minimize harm.
Unless otherwise noted, we use ğ›¼H=0.25,ğ‘NH=0.5,ğ›½=0.15,
andğœ†=100(though we also explore the impact of these param-
eters). Unless otherwise noted, we use ğ‘˜=1under the bounded
cardinality setting for most experiments, exploring the impact of ğ‘˜
on recommendations separately under the independent sampling
setting. We calibrate ğ‘in function ğ‘”(Eq.(33) in [ 14]) in the range
[1,20]to balance the steady-state mean ğ‘CLKandğ‘H( Eq.(4)) under
a benchmark recommendation policy (the uniform recommenda-
tion policy, described below). The full set of values of ğ‘per dataset,
as well as the corresponding justification for these choices, is given
in Appendix H in [14].
As we use profiles learned from data, rather than user ratings
directly, our experiments are semi-synthetic. Note that going be-
yond semi-synthetic experiments poses several challenges which
we share with the broader recommender system literature (e.g., Oh
and Iyengar [51,52]): assessing the performance of different poli-
cies would require knowing how a user would react to alternatives
they have not seen, which we cannot extract from offline datasets.
Algorithms. We implement the following algorithms: Gradient-
Based Algorithm (Grad ) is the SLSQP implementation of SciPy [ 67],
for both the bounded cardinality and the independent sampling
setting, using our gradient computation approach as described in
Sec. 4 and Appendix G in [ 14];Alternating Optimization (Alt) is
is the (suboptimal) alternating optimization algorithm in Eq. (14);
Static Profile Optimization (U0) is the optimal policy under static
profileğ‘¢0, as determined by Thm. 1; finally, Uniform Recommen-
dations (Unif ) is the policy that selects the recommended set u.a.r.
Additional implementation details, including convergence toler-
ance criteria, are in Appendix H in [ 14]. We stress here that, as
Prob. (9)is non-convex, it is not a priori clear that Grad reaches a
globally optimal policy.
Metrics. We report the recommendation objective (Eq. (9)), as well
as its constituent ğ‘CLKandğ‘H. We also measure âˆ¥limğ‘¡â†’âˆğ‘¢(ğ‘¡)âˆ’
Â¯ğ‘¢âˆ¥(Eq.(7),(10)), the distance of the user profile dynamics as time
evolves under a given policy and the corresponding fixed point.5.2 Results
Recommendation Policy Comparison. Table 2 reports our key
metrics over the 5 movie genre data sets, averaged across the 100
users in the dataset. Across all genre datasets, Grad attains superior
recommendation objective values, while the improvement over
uniform is as much as 77% (for Sci-Fi). Surprisingly, our policy does
not need to sacrifice ğ‘CLKto attain low ğ‘Hin comparison to other
policies; it is superior in both terms. This is because it recommends
an item (a) users become more likely to click, in steady state, and
(b) is distinct enough from harmful content, so that ğ‘Hremains low.
We note that, even though we report standard deviations, Grad
dominates other policies in terms of the objective also on a per user
basis: this is evident in Fig. 2, where we report also the PDFs of
the difference of objectives from the one attained by Grad (addi-
tional genres are in Appendix H in [ 14]). In Fig. 3, we visualize the
which movies each policy recommends for two users in the Action
and Adventure genres. We display each vector ğ‘£with respect to
their inner product with the mean non-harmful and harmful vector.
We observe that the gradient-based policy is more concentrated
towards the right, i.e., on movies which have high inner product
with the average non-harmful vector. Similar observations hold for
remaining genres (see Fig. 7 in Appendix H in [14]).
Understanding the Effect of Model Parameters. We plot the
effect of modifying ğœ†,ğ›½, andğ‘on the objective attained by different
policies for the Action genre in Fig. 4; additional genres, as well
as the effect these parameters have on ğ‘CLKandğ‘H, are shown in
Figs. 11â€“13 in Appendix H in [ 14]. Increasing any parameter de-
creases the objective attained by every policy, which is intuitive. We
also observe that increasing ğœ†naturally increases the performance
gap of the Grad policy. Increasing ğ›½has the opposite effect, as it
limits the ability of all policies to impact a userâ€™s profile. Parameter
ğ‘also increases the improvement of Grad over other policies as,
the largerğ‘is, the less likely the recommendation is to be accepted,
and the more important it becomes to minimize harm.
In Fig. 5, we also plot the impact of the ratio ğ›¼H/ğ›¼NHon the
objective attained by different policies for the Action and Adventure
genres. Increasing the ratio decreases the objective attained by every
policy as well as the gap from Grad, as policies have less leeway in
 
262Harm Mitigation in Recommender Systems under User Preference Dynamics KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) Action
(b) Adventure
Figure 5: Effect of modifying ratio ğ›¼H/ğ›¼NHon the objective attained by different
policies for the Action and Adventure genres. Additional genres, and impact
onğ‘CLK,ğ‘H, are shown in Appendix H in [ 14]. Increasing the ratio decreases
the objective attained by every policy as well as the gap from Grad, as policies
have less leeway in minimizing harm.
âˆ¥limğ‘¢(ğ‘¡)âˆ’Â¯ğ‘¢âˆ¥
Policy Action Adventure Comedy
Grad 6.8e-4 (Â±4.2e-4) 7.3e-4 (Â±4.2e-4) 7.1e-4 (Â±4.7e-4)
Alt 3.6e-4 (Â±2.0e-4) 6.1e-4 (Â±4.1e-4) 7.3e-4 (Â±4.9e-4)
U0 3.5e-4 (Â±1.9e-4) 4.2e-4 (Â±3.3e-4) 5.3e-4 (Â±4.5e-4)
Unif 3.7e-4 (Â±1.1e-4) 3.1e-4 (Â±1.1e-4) 3.4e-4 (Â±1.4e-4)
Policy Fantasy Sci-Fi
Grad 4.7e-5 (Â±7.0e-5) 4.2e-4 (Â±3.1e-4)
Alt 4.7e-5 (Â±7.0e-5) 2.2e-4 (Â±1.4e-4)
U0 1.7e-4 (Â±1.3e-4) 2.5e-4 (Â±1.6e-4)
Unif 4.3e-4 (Â±1.6e-4) 3.6e-4 (Â±1.2e-4)
Table 3: The user dynamics converge to the fixed-point for all policies and all
genres;âˆ¥limğ‘¢(ğ‘¡)âˆ’Â¯ğ‘¢âˆ¥is small and on par with the convergence threshold.
We report the meanÂ±std.
2.5 5.0 7.5 10.0
Rec/uni00A0Size/uni00A0k30
25
20
15
10
ObjectiveGrad/uni00A0Policy
Alt/uni00A0Policy
U0/uni00A0Policy
Unif/uni00A0Policy
(a) Fixedğ‘â‰ˆVaryingğ‘CLK
2.5 5.0 7.5 10.0
Rec/uni00A0Size/uni00A0k20
15
10
Objective
Grad/uni00A0Policy
Alt/uni00A0PolicyU0/uni00A0Policy
Unif/uni00A0Policy (b) Fixedğ‘˜/ğ‘â‰ˆFixedğ‘CLK
Figure 6: Effect of increasing the recommendation set size ğ‘˜on the objective
attained by different policies for the Action genre. Impact on ğ‘CLKandğ‘Hare
show in Appendix H in [ 14] (a) We first increase the number of recommended
itemsğ‘˜. From the MNL model in Eq. (6)we see thatğ‘†ğ¸can increase with larger
ğ‘˜, effectively increasing ğ‘CLK-and thus the objective-for all policies. This trend
is observed when plotting ğ‘CLK(Appendix H in [ 14]). (b): we keep the ratio ğ‘˜/ğ‘
fixed in order to counteract the effect of rising ğ‘CLKupon increasing ğ‘˜. The
gradient-computed policy is superior over a variety of ğ‘˜.
minimizing harm. Similar observations hold for other genres (see
Figs. 14â€“16 in Appendix H in [14]).
Convergence. For all datasets, policies, and users in the experi-
ments reported in Table 2, we also compute the user profile dynam-
ics in Eq. (7). In all experiments, they converge to the corresponding
stationary user profile in Lemma 1, within a 10âˆ’3tolerance; see
Table 3 for per genre tolerances.
Increasing Recommendation Size under the Independent
Sampling Setting. Next, we study the scalability of our algorithms
by increasing the recommendation size ğ‘˜, under the independentsampling setting. In Fig. 6 we plot the impact of ğ‘˜on the objective
for the Action genre, under two different scenarios: in one, we vary
ğ‘along withğ‘˜, while in the other we keep it fixed. Intuitively, as
ğ‘˜increases,ğ‘CLKconverges to one, and ğ‘Hcollapses to zero. Thus,
to better understand the performance of different policies as ğ‘˜in-
creases we make the optimization â€œharderâ€ as we increase ğ‘˜. To
accomplish this, recall from Sec. 3 the total score of a recommendes
setğ¸âŠ†Î©isğ‘†ğ¸â‰¡Ã
ğ‘£âˆˆğ¸ğ‘ ğ‘£, and that the conditional probability
ğ‘CLK|ğ¸=ğ‘”(ğ‘ ğ¸)â‰¡ğ‘ ğ¸/(ğ‘ ğ¸+ğ‘). Assuming ğ‘†ğ¸grows proportionally
withğ‘˜, settingğ‘to also grow proportionally with ğ‘˜keepsğ‘CLK|ğ¸
relatively constant, allowing us to better understand how increas-
ingğ‘˜effects harm. In Fig. 6(a), when ğ‘is fixed, we see that all
policies improve (as expected) w.r.t. the overal objective, with Grad
dominating other policies. In Fig. 6(b), when ğ‘is proportional to
ğ‘˜, the problem becomes harder and the objective decreases, with
Grad still being dominant; this is confirmed when observing the
impact onğ‘CLKandğ‘H, reported in Appendix H in [14].
6 CONCLUSIONS
Overall, our goal is to gain a better understanding of real-world
phenomena through a simplified but insightful model. We study
a model in which the likelihood of consuming harmful content,
though never directly the result of a recommendation, can nonethe-
less be influenced by a recommendation policy through its impact
on user preferences. The notion of harm that we study is sim-
ple: content is known to be either harmful or non-harmful, and
recommenders will never recommend harmful content. Despite
this, considerable complexity arises from the user behavior model
which is influenced on both short (via clicks) and long (via inter-
ests) timescales by the choices of the recommender. We develop
algorithms for designing recommenders which simultaneously max-
imize a CTR objective while minimizing the likelihood of harm and
verify their performance on a setting initialized with real movie
rating data. We present compelling evidence that a user pathway
to harm should be accounted for; this is missing in both literature
and practice. We rigorously substantiate that solely maximizing the
click-through-rate may come at the cost of shifting user preferences
towards harm.
Our focus on the long term impacts of recommendations leads
to conceptions of harm that are both mathematically challenging
and conceptually rich. Many opportunities for future work remain
in the setting that we study, for example: under what conditions
are user profiles guaranteed to converge to a fixed point? Is it pos-
sible to make guarantees about tractable algorithms for optimizing
our non-convex objective? Many open questions arise from relax-
ing our assumptions, like: what if users interests evolve according
to something other than attraction (e.g. repulsion [ 45], biased as-
similation [ 18]). What if user and item profiles are not directly
observed, and must be learned from data? Finally, we hope that
our perspective, centered on the dynamics of users, can contribute
to the rich literature on harms caused by algorithmic systems and
inspire effective methods for mitigation.
ACKNOWLEDGMENTS
This work was partly funded by NSF CCF-2312774, NSF OAC-
2311521, and NSF DGE-1922551.
 
263KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jerry Chee et al.
REFERENCES
[1]S. Agrawal, V. Avadhanula, V. Goyal, and A. Zeevi. Thompson sampling for the
mnl-bandit. In S. Kale and O. Shamir, editors, Proceedings of the 2017 Conference
on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages
76â€“78. PMLR, 07â€“10 Jul 2017. URL https://proceedings.mlr.press/v65/agrawal17a.
html.
[2]A. Arora, P. Nakov, M. Hardalov, S. M. Sarwar, V. Nayak, Y. Dinkov, D. Zlatkova,
K. Dent, A. Bhatawdekar, G. Bouchard, et al. Detecting harmful content on online
platforms: what platforms need vs. where research efforts go. ACM Computing
Surveys, 56(3):1â€“17, 2023.
[3]H. Ashton and M. Franklin. The problem of behaviour and preference manipula-
tion in ai systems. In CEUR Workshop Proceedings, volume 3087. CEUR Workshop
Proceedings, 2022.
[4]S. Banach. Sur les opÃ©rations dans les ensembles abstraits et leur application aux
Ã©quations intÃ©grales. Fundamenta mathematicae, 3(1):133â€“181, 1922.
[5]S. Beckers, H. Chockler, and J. Y. Halpern. Causal analysis of harm. Advances in
Neural Information Processing Systems, 36, 2022.
[6]M. BenaÃ¯m. Dynamics of stochastic approximation algorithms. In Seminaire de
probabilites XXXIII, pages 1â€“68. Springer, 2006.
[7]A. Benveniste, M. MÃ©tivier, and P. Priouret. Adaptive algorithms and stochastic
approximations, volume 22. Springer Science & Business Media, 2012.
[8]D. P. Bertsekas. Nonlinear programming. Journal of the Operational Research
Society, 48(3):334â€“334, 1997.
[9]M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-LÃ³pez, F. Pe-
dregosa, and J.-P. Vert. Efficient and modular implicit differentiation. Advances
in neural information processing systems, 35:5230â€“5242, 2022.
[10] V. S. Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48.
Springer, 2009.
[11] M. Carroll, D. Hadfield-Menell, S. Russell, and A. Dragan. Estimating and penal-
izing preference shift in recommender systems. In Fifteenth ACM Conference on
Recommender Systems, pages 661â€“667, 2021.
[12] I. H. Center. Imdb parental guide, 2023. https://help.imdb.com/article/
contribution/titles/parental-guide/GF4KYKYJA4PKQB32.
[13] B. H. Chaptini. Use of discrete choice models with recommender systems. PhD
thesis, Massachusetts Institute of Technology, 2005.
[14] J. Chee, S. Kalyanaraman, S. K. Ernala, U. Weinsberg, S. Dean, and S. Ioannidis.
Harm mitigation in recommender systems under user preference dynamics, 2024.
Arxiv http://arxiv.org/abs/2406.09882.
[15] P. Cremonesi, Y. Koren, and R. Turrin. Performance of recommender algorithms
on top-n recommendation tasks. In Proceedings of the fourth ACM conference on
Recommender systems, pages 39â€“46, 2010.
[16] M. Curmei, A. A. Haupt, B. Recht, and D. Hadfield-Menell. Towards
psychologically-grounded dynamic preference models. In Proceedings of the
16th ACM Conference on Recommender Systems, pages 35â€“48, 2022.
[17] M. Danaf, F. Becker, X. Song, B. Atasoy, and M. Ben-Akiva. Online discrete
choice models: Applications in personalized recommendations. Decision Support
Systems, 119:35â€“45, 2019.
[18] S. Dean and J. Morgenstern. Preference dynamics under personalized recommen-
dations. arXiv preprint arXiv:2205.13026, 2022.
[19] M. Deshpande and G. Karypis. Item-based top-n recommendation algorithms.
ACM Transactions on Information Systems (TOIS), 22(1):143â€“177, 2004.
[20] F. Fabbri, Y. Wang, F. Bonchi, C. Castillo, and M. Mathioudakis. Rewiring what-to-
watch-next recommendations to reduce radicalization pathways. In Proceedings
of the ACM Web Conference 2022, pages 2719â€“2728, 2022.
[21] D. Fleder and K. Hosanagar. Blockbuster cultureâ€™s next rise or fall: The impact
of recommender systems on sales diversity. Management science, 55(5):697â€“712,
2009.
[22] G. Folland. Avanced Calculus. Pearson, 2001.
[23] Y. Ge, S. Zhao, H. Zhou, C. Pei, F. Sun, W. Ou, and Y. Zhang. Understanding
echo chambers in e-commerce recommender systems. In Proceedings of the 43rd
international ACM SIGIR conference on research and development in information
retrieval, pages 2261â€“2270, 2020.
[24] T. Gillespie. Content moderation, ai, and the question of scale. Big Data & Society,
7(2):2053951720943234, 2020.
[25] R. Gormann and S. Armstrong. The dangers in algorithms learning humansâ€™
values and irrationalities. arXiv preprint arXiv:2202.13985, 2022.
[26] B. Hayworth. Imdb parental guide, 2023. https://www.kaggle.com/datasets/
barryhaworth/imdb-parental-guide.
[27] N. Hazrati and F. Ricci. Recommender systems effect on the evolution of usersâ€™
choices distribution. Information Processing & Management, 59(1):102766, 2022.
[28] S. Hosseini, H. Palangi, and A. H. Awadallah. An empirical study of metrics to
measure representational harms in pre-trained language models. arXiv preprint
arXiv:2301.09211, 2023.
[29] Y. Hou, D. Xiong, T. Jiang, L. Song, and Q. Wang. Social media addiction: Its
impact, mediation, and intervention. Cyberpsychology: Journal of psychosocial
research on cyberspace, 13(1), 2019.
[30] F. HuszÃ¡r, S. I. Ktena, C. Oâ€™Brien, L. Belli, A. Schlaikjer, and M. Hardt. Algorithmic
amplification of politics on twitter. Proceedings of the National Academy of Sciences,119(1):e2025334119, 2022.
[31] D. Jannach. Multi-objective recommender systems: Survey and challenges. arXiv
preprint arXiv:2210.10309, 2022.
[32] H. Jiang, X. Qi, and H. Sun. Choice-based recommender systems: a unified
approach to achieving relevancy and diversity. Operations Research, 62(5):973â€“
993, 2014.
[33] R. Jiang, S. Chiappa, T. Lattimore, A. GyÃ¶rgy, and P. Kohli. Degenerate feedback
loops in recommender systems. In Proceedings of the 2019 AAAI/ACM Conference
on AI, Ethics, and Society, pages 383â€“390, 2019.
[34] D. Kalimeris, S. Bhagat, S. Kalyanaraman, and U. Weinsberg. Preference am-
plification in recommender systems. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining, pages 805â€“815, 2021.
[35] G. Karypis. Evaluation of item-based top-n recommendation algorithms. In
Proceedings of the tenth international conference on Information and knowledge
management, pages 247â€“254, 2001.
[36] D. Koutra, A. Dighe, S. Bhagat, U. Weinsberg, S. Ioannidis, C. Faloutsos, and
J. Bolot. PNP: Fast path ensemble method for movie design. In Proceedings of the
23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 1527â€“1536, 2017.
[37] D. Kraft. A software package for sequential quadratic programming.
Forschungsbericht- Deutsche Forschungs- und Versuchsanstalt fur Luft- und Raum-
fahrt, 1988.
[38] K. Krauth, S. Dean, A. Zhao, W. Guo, M. Curmei, B. Recht, and M. I. Jordan.
Do offline metrics predict online performance in recommender systems? arXiv
preprint arXiv:2011.07931, 2020.
[39] A. Lada, M. Wang, and T. Yan. How machine learning powers Facebookâ€™s News
Feed ranking algorithm, 2021. https://engineering.fb.com/2021/01/26/core-infra/
news-feed-ranking/.
[40] M. Ledwich, A. Zaitsev, and A. Laukemper. Radical bubbles on youtube? revisiting
algorithmic extremism with personalised recommendations. First Monday, 2022.
[41] W. Lee, S. S. Lee, S. Chung, and D. An. Harmful contents classification using
the harmful word filtering and svm. In Computational Scienceâ€“ICCS 2007: 7th
International Conference, Beijing, China, May 27-30, 2007, Proceedings, Part III 7,
pages 18â€“25. Springer, 2007.
[42] N. E. Leonard, K. Lipsitz, A. Bizyaeva, A. Franci, and Y. Lelkes. The nonlinear feed-
back dynamics of asymmetric political polarization. Proceedings of the National
Academy of Sciences, 118(50):e2102149118, 2021.
[43] S. A. Levin, H. V. Milner, and C. Perrings. The dynamics of political polarization,
2021.
[44] L. Y. Lin, J. E. Sidani, A. Shensa, A. Radovic, E. Miller, J. B. Colditz, B. L. Hoffman,
L. M. Giles, and B. A. Primack. Association between social media use and
depression among us young adults. Depression and anxiety, 33(4):323â€“331, 2016.
[45] W. Lu, S. Ioannidis, S. Bhagat, and L. V. Lakshmanan. Optimal recommendations
under attraction, aversion, and social influence. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge discovery and data mining, pages
811â€“820, 2014.
[46] R. D. Luce. Individual choice behavior: A theoretical analysis, 1959.
[47] S. Maniu, S. Ioannidis, and B. Cautis. Bandits under the influence. In 2020 IEEE
International Conference on Data Mining (ICDM), pages 1172â€“1177. IEEE, 2020.
[48] M. Mansoury, H. Abdollahpouri, M. Pechenizkiy, B. Mobasher, and R. Burke.
Feedback loop and bias amplification in recommender systems. In Proceedings of
the 29th ACM international conference on information & knowledge management,
pages 2145â€“2148, 2020.
[49] D. McFadden et al. Conditional logit analysis of qualitative choice behavior. 1973.
[50] C. Michelot. A Finite Algorithm for Finding the Projection of a Point onto the
Canonical Simplex of ğ‘…ğ‘›.Journal of Optimization Theory and Applications, 50(1):
195â€“200, 1986.
[51] M.-h. Oh and G. Iyengar. Thompson sampling for multinomial logit contextual
bandits. Advances in Neural Information Processing Systems, 32, 2019.
[52] M.-h. Oh and G. Iyengar. Multinomial logit contextual bandits: Provable optimal-
ity and practicality. In Proceedings of the AAAI conference on artificial intelligence ,
volume 35, pages 9205â€“9213, 2021.
[53] N. Pagan, J. Baumann, E. Elokda, G. De Pasquale, S. Bolognani, and A. HannÃ¡k. A
classification of feedback loops and their relation to biases in automated decision-
making systems. In Proceedings of the 3rd ACM Conference on Equity and Access
in Algorithms, Mechanisms, and Optimization, pages 1â€“14, 2023.
[54] N. Perra and L. E. Rocha. Modelling opinion dynamics in the age of algorithmic
personalisation. Scientific reports, 9(1):7261, 2019.
[55] F. Pierri, L. Luceri, E. Chen, and E. Ferrara. How does twitter account moderation
work? dynamics of account creation and suspension on twitter during major
geopolitical events. EPJ Data Science, 12(1):43, 2023.
[56] N. J. Restrepo, L. Illari, R. Leahy, R. F. Sear, Y. Lupu, and N. F. Johnson. How social
media machinery pulled mainstream parenting communities closer to extremes
and their misinformation during covid-19. IEEE Access, 2021.
[57] M. H. Ribeiro, R. Ottoni, R. West, V. A. Almeida, and W. Meira Jr. Auditing
radicalization pathways on youtube. In Proceedings of the 2020 conference on
fairness, accountability, and transparency, pages 131â€“141, 2020.
 
264Harm Mitigation in Recommender Systems under User Preference Dynamics KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[58] J. G. Richens, R. Beard, and D. H. Thompson. Counterfactual harm. Advances in
Neural Information Processing Systems, 36, 2022.
[59] W. S. Rossi, J. W. Polderman, and P. Frasca. The closed loop between opinion
formation and personalized recommendations. IEEE Transactions on Control of
Network Systems, 9(3):1092â€“1103, 2021.
[60] R. Shelby, S. Rismani, K. Henne, A. Moon, N. Rostamzadeh, P. Nicholas, N. Yilla,
J. Gallegos, A. Smart, E. Garcia, et al. Sociotechnical harms: Scoping a taxonomy
for harm reduction. arXiv preprint arXiv:2210.05791, 2022.
[61] A. Singh, Y. Halpern, N. Thain, K. Christakopoulou, E. Chi, J. Chen, and A. Beutel.
Building healthy recommendation sequences for everyone: A safe reinforcement
learning approach. In FAccTRec Workshop, 2020.
[62] J. J. Smith, L. Jayne, and R. Burke. Recommender systems and algorithmic hate. In
Proceedings of the 16th ACM Conference on Recommender Systems, pages 592â€“597,
2022.
[63] W. Suna and O. Nasraouia. User polarization aware matrix factorization for
recommendation systems. 2021.
[64] Ã–. SÃ¼rer, R. Burke, and E. C. Malthouse. Multistakeholder recommendation with
provider constraints. In Proceedings of the 12th ACM Conference on Recommender
Systems, pages 54â€“62, 2018.
[65] B. Tabibian, V. Gomez, A. De, B. SchÃ¶lkopf, and M. G. Rodriguez. On the design
of consequential ranking algorithms. In Conference on Uncertainty in Artificial
Intelligence, pages 171â€“180. PMLR, 2020.
[66] S. Vargas and P. Castells. Rank and relevance in novelty and diversity metrics for
recommender systems. In Proceedings of the fifth ACM conference on Recommender
systems, pages 109â€“116, 2011.
[67] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett,
J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson,
C. J. Carey, Ä°. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,
R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H.Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:
Fundamental Algorithms for Scientific Computing in Python. Nature Methods,
17:261â€“272, 2020. doi: 10.1038/s41592-019-0686-2.
[68] J. Whittaker, S. Looney, A. Reed, and F. Votta. Recommender systems and the
amplification of extremist content. Internet Policy Review, 10(2):1â€“29, 2021.
[69] Y. Wu, N. Yang, and H. Luo. Unified group recommendation towards multiple
criteria. In Web and Big Data: Third International Joint Conference, APWeb-WAIM
2019, Chengdu, China, August 1â€“3, 2019, Proceedings, Part II 3, pages 137â€“151.
Springer, 2019.
[70] L. Xiao, Z. Min, Z. Yongfeng, G. Zhaoquan, L. Yiqun, and M. Shaoping. Fairness-
aware group recommendation with pareto-efficiency. In Proceedings of the
eleventh ACM conference on recommender systems, pages 107â€“115, 2017.
[71] S.-H. Yang, B. Long, A. J. Smola, H. Zha, and Z. Zheng. Collaborative competitive
filtering: learning recommender using context of user choice. In Proceedings
of the 34th international ACM SIGIR conference on Research and development in
Information Retrieval, pages 295â€“304, 2011.
[72] M. Zampieri, S. Rosenthal, P. Nakov, A. Dmonte, and T. Ranasinghe. Offenseval
2023: Offensive language identification in the age of large language models.
Natural Language Engineering, 29(6):1416â€“1435, 2023.
[73] L. Zhang, T. Yang, Z.-H. Zhou, et al. Dynamic regret of strongly adaptive methods.
InInternational conference on machine learning, pages 5882â€“5891. PMLR, 2018.
[74] Y. Zhang, F. Chen, and J. Lukito. Network amplification of politicized infor-
mation and misinformation about covid-19 by conservative media and partisan
influencers on twitter. Political Communication, pages 1â€“24, 2022.
[75] Y. Zheng and D. X. Wang. A survey of recommender systems with multi-objective
optimization. Neurocomputing, 474:141â€“153, 2022.
[76] M. Zinkevich. Online convex programming and generalized infinitesimal gradient
ascent. In Proceedings of the 20th international conference on machine learning
(icml-03), pages 928â€“936, 2003.
 
265