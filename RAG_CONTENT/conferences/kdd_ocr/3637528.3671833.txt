Budgeted Multi-Armed Bandits with
Asymmetric Confidence Intervals
Marco Heyden
marco.heyden@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, GermanyVadim Arzamasov
vadim.arzamasov@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, Germany
Edouard FouchÃ©
edouard.fouche@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, GermanyKlemens BÃ¶hm
klemens.boehm@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, Germany
ABSTRACT
We study the stochastic Budgeted Multi-Armed Bandit (MAB) prob-
lem, where a player chooses from ğ¾arms with unknown expected
rewards and costs. The goal is to maximize the total reward un-
der a budget constraint. A player thus seeks to choose the arm
with the highest reward-cost ratio as often as possible. Current ap-
proaches for this problem have several issues, which we illustrate.
To overcome them, we propose a new upper confidence bound
(UCB) sampling policy, ğœ”-UCB, that uses asymmetric confidence
intervals. These intervals scale with the distance between the sam-
ple mean and the bounds of a random variable, yielding a more
accurate and tight estimation of the reward-cost ratio compared to
our competitors. We show that our approach has sublinear instance-
dependent regret in general and logarithmic regret for parameter
ğœŒâ‰¥1, and that it outperforms existing policies consistently in
synthetic and real settings.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning algorithms ;
Online learning settings ;â€¢Information systems â†’Decision sup-
port systems.
KEYWORDS
Budgeted multi-armed bandits, Multi-armed bandits, Decision mak-
ing under uncertainty, Online decision making
ACM Reference Format:
Marco Heyden, Vadim Arzamasov, Edouard FouchÃ©, and Klemens BÃ¶hm.
2024. Budgeted Multi-Armed Bandits with Asymmetric Confidence In-
tervals. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671833
1 INTRODUCTION
In the stochastic Multi-Armed Bandit (MAB) problem, a player re-
peatedly plays one of ğ¾arms and receives a corresponding random
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671833reward. The goal is to maximize the cumulative reward by play-
ing the arm with the highest expected reward as often as possible.
The expected rewards are initially unknown, so the player must
balance trying arms to learn their expected rewards (exploration)
versus using the current information to play arms with known high
expected rewards (exploitation).
In the stochastic Budgeted MAB problem [ 16], a player must
consider not only the potential rewards but also the associated ran-
dom costs for each arm. The player chooses arms until the available
budget is exhausted. Budgeted MABs model real-world situations
such as the selection of a cloud service provider [ 1], energy-efficient
task selection for battery-powered embedded devices [ 17], bid opti-
mization [6], or optimizing advertising on social media.
Example 1 (Social media advertising). Consider a retail company
that wants to advertise products on a social network platform.
The retail company provides to the platform an advertisement
campaign consisting of multiple ads, as well as an advertisement
budget. Each time a user interacts with an ad (an arm), the platform
charges the retail company (a cost). Within the given budget, the
retailer wants to find the ads which maximize the likelihood of a
subsequent purchase (a reward). Both the reward and the cost are
random variables since they depend on the actions of users and the
competition from other advertisers. A Budgeted MAB algorithm
can help find the most promising ads in real time.
A variety of policies has been proposed to address the Budgeted
MAB problem. Section 3 provides a summary. Some policies model
the problem as Bandits with Knapsacks, which are able to take the
budget limit into account [ 4,17]. Others extend ideas from tradi-
tional multi-armed bandit algorithms (in which the costs of arms
are assumed to be constant), including Thompson Sampling [ 23]
and Upper Confidence Bound (UCB) sampling [ 24]. Several studies
indicate that the latter â€“ and in particular UCB-sampling â€“ perform
well in practice [18, 19, 22, 24, 25].
UCB-sampling policies continuously update an upper bound of
the ratio of the expected rewards and costs of each arm, and play
the arm with the highest upper bound. We distinguish between
three types: Some policies [ 8,18,19,24] compute the bound from
the ratio of the sample average reward and average cost plus some
uncertainty-related term (cf. Eq. (1), left). We call this type â€œunitedâ€
(u). Other policies [ 4,24,25] divide the rewardâ€™s upper confidence
bound by the costâ€™s lower confidence bound (LCB) (cf. Eq. (1), right).
 
1073
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard FouchÃ©, and Klemens BÃ¶hm
51015% UCB viol.
Ï‰-UCB (c, ours)m-UCB (c)Budget-UCB (h)c-UCB (h) i-UCB (u) UCB-SC (u) UCB-B2 (u)100101102UCB/expect.Samples
100 1000 10000 100000
Figure 1: Issues of existing work
We refer to this type as â€œcompositeâ€ (c). There also are â€œhybridâ€ (h)
policies [22, 24] that combine the united and composite types.
UCBğ‘¢=average reward
average cost+uncertainty
UCBğ‘=average reward+uncertainty
average costâˆ’uncertainty(1)
However, most of the current policies have at least one of the
following issues: (i1) Over-optimism: The policy often computes
UCB that are too tight. (i2) Over-pessimism: The policy often
computes UCB that are too loose. (i3) Invalid values: Negative or
undefined UCB occur if the costâ€™s lower confidence bound in Eq. (1)
becomes negative or zero. The latter can happen, for instance, when
computing the lower confidence bound of an armâ€™s expected cost
with Hoeffdingâ€™s inequality [22, 24].
To illustrate (i1) and (i2), we randomly parameterized 10 000
Bernoulli reward and cost distributions and sampled from them.
We used these samples to compute 99% confidence intervals of
the reward-cost ratio using several state-of-the-art UCB-sampling
policies. The upper plot of Figure 1 shows the share of cases when
the expected reward-cost ratio exceeds (i.e., violates) its UCB. Values
above 1% indicate overly tight bounds. The lower plot shows the
UCB of the reward-cost ratio divided by its expectation, with higher
values indicating looser bounds. Existing united policies (u) tend
to suffer from issue (i1), hybrid approaches suffer from either of
both issues, and issue (i2) mainly affects composite approaches.
An exception is UCB-B2 [ 8], whoâ€™s UCB is both tight and reliable
(although not as tight as that of ğœ”-UCB).
To address (i1) and (i2), some approaches provide a hyperparam-
eter that allows to adjust the confidence interval manually [ 24].
However, setting such a hyperparameter is difficult since it depends
on the unknown mean and variance of the reward and cost distribu-
tions. To address (i3), current approaches set the UCB of the ratio
to infinity [ 24] or the cost LCB to a small positive value [ 22]. Theseheuristic solutions largely ignore the information already acquired
about the cost distribution and tend to cause either (i1) or (i2).
Contributions. (1) We derive asymmetric confidence intervals for
bounded random variables. These intervals have the same range
as the random variable and scale with the distance between the
sample mean and the boundaries. Our formula generalizes Wilsonâ€™s
score interval for binomial proportions [ 21] to arbitrary bounded
random variables. (2) We introduce a policy called ğœ”-UCB, which
leverages these confidence intervals to address issues (i1)â€“(i3). We
also propose an extension of ğœ”-UCB, called ğœ”âˆ—-UCB, that uses
the observed variances of the armsâ€™ rewards and cost to further
tighten the UCB. (3) We prove that ğœ”-UCB has sublinear regret in
general and logarithmic regret for parameter ğœŒâ‰¥1. (4) We conduct
experiments on typical settings found in the literature and real-
world social network advertising data to compare the performance
ofğœ”-UCB andğœ”âˆ—-UCB against state-of-the-art policies. Our results
demonstrate that both policies have substantially lower regret than
the competitors for both small and large budgets. (5) We share the
code of our experiments.1
2 PROBLEM DEFINITION
We focus on a stochastic setting with ğ¾arms. Each arm ğ‘˜has
continuous or discrete reward and cost distributions with unknown
expected values ğœ‡ğ‘Ÿ
ğ‘˜âˆˆ[0,1)andğœ‡ğ‘
ğ‘˜âˆˆ(0,1], respectively. Assume
without loss of generality that arm ğ‘˜=1has the highest ratio ğœ‡ğ‘Ÿ
ğ‘˜/ğœ‡ğ‘
ğ‘˜
among all arms. At time ğ‘¡a player chooses an arm ğ‘˜ğ‘¡âˆˆ{1,...,ğ¾}
and observes the reward ğ‘Ÿğ‘¡âˆˆ[0,1]and the cost ğ‘ğ‘¡âˆˆ[0,1]. We
assume that the arms are independent and that rewards and costs
observed at different time steps are independent and identically
distributed (iid). This is consistent with previous work [ 18,19,22â€“
24]. We do not make any assumptions about the correlation between
rewards and costs of the same arm. The game ends after ğ‘‡ğµplays
that exhaust the available budget ğµ.
Let1ğ‘˜(ğ‘˜ğ‘¡)be the indicator function: 1ğ‘˜(ğ‘˜ğ‘¡)=1iffğ‘˜ğ‘¡=ğ‘˜,
else1ğ‘˜(ğ‘˜ğ‘¡)=0. The number of plays, and the sample average of
rewards and costs of arm ğ‘˜at timeğ‘‡are:
ğ‘›ğ‘˜(ğ‘‡)=ğ‘‡âˆ‘ï¸
ğ‘¡=11ğ‘˜(ğ‘˜ğ‘¡)
Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘‡)=1
ğ‘›ğ‘˜(ğ‘‡)ğ‘‡âˆ‘ï¸
ğ‘¡=11ğ‘˜(ğ‘˜ğ‘¡)ğ‘Ÿğ‘¡
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘‡)=1
ğ‘›ğ‘˜(ğ‘‡)ğ‘‡âˆ‘ï¸
ğ‘¡=11ğ‘˜(ğ‘˜ğ‘¡)ğ‘ğ‘¡
The goal of the player is to minimize the pseudo-regret compared
to the cumulative reward ğ‘…âˆ—of an optimal policy, given by ğ‘…âˆ—âˆ’
EÃğ‘‡ğµ
ğ‘¡=1ğ‘Ÿğ‘¡. Finding the optimal policy in Budgeted MABs is known
to be np-hard, due to the â€œknapsack problemâ€ [ 16]. However, always
choosing arm 1 leads to a suboptimality of at most 2ğœ‡ğ‘Ÿ
1/ğœ‡ğ‘
1, negligible
for not too small budgets [ 23]. Thus, previous work [ 18,19,22â€“24],
as well as our own approach, aim to minimize regret relative to a
1https://github.com/heymarco/OmegaUCB
 
1074Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
policy that always selects arm 1:
Regret =ğ¾âˆ‘ï¸
ğ‘–=1ğœ‡ğ‘
ğ‘˜Î”ğ‘˜E[ğ‘›ğ‘˜(ğ‘‡ğµ)],where Î”ğ‘˜=ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜
ğœ‡ğ‘
ğ‘˜
3 RELATED WORK
There exists many different MAB-related settings and policies. We
refer to [ 11] for an overview and focus on algorithms developed
for the Budgeted MAB setting in this section.
Tran-Thanh et al. [ 16] introduced the Budgeted MAB problem
and proposed an ğœ€-first policy. Subsequent policies KUBE [ 17] and
PD-BwK [ 4] propose to formulate the problem as Bandits with
Knapsacks, where the size of the knapsack represents the available
budget. Both policies require knowledge of ğµ. This restricts their ap-
plicability when ğµis an unknown quantity. However, we argue that
such a restriction is unnecessary, since the advantage of exploiting
ğµbecomes negligible for sufficiently large budgets (cf. Section 2).
Another approach, UCB-BV1 [ 10], addresses the special case of dis-
crete random costs. Later solutions [ 18,19,22â€“24] adapted concepts
from traditional MABs, such as Upper Confidence Bound (UCB) [ 2]
or Thompson sampling [ 14,15] and can deal with continuous ran-
dom costs and unknown budget. However, the one policy based on
Thompson sampling, BTS [ 23], requires transforming continuous
rewards and costs into Bernoulli-samples. As a result, the policy
disregards information about the variance of rewards and costs,
causing over-pessimism (i2) when the variance of the reward or
cost distribution is small. MRCB [ 22] deals with the challenge of
playing multiple arms in each time step; when playing only one
arm at a time, the policy becomes m-UCB [ 24] that is similar to our
policy. However, m-UCB relies on Hoeffdingâ€™s inequality, which
does not take the distance between a random variableâ€™s sample
mean and boundaries into account. To see why this is problematic,
consider the following example2:
Example 2 (m-UCB). Assume two arms with ğœ‡ğ‘Ÿ
1=0.8,ğœ‡ğ‘
1=0.2,
ğœ‡ğ‘Ÿ
2=0.1,ğœ‡ğ‘
2=0.1. Clearly, arm 1 should be preferred for a reason-
ably large budget. However, m-UCB is biased towards pulling arm 2.
For instance, if ğ‘¡=10000 andğ‘›1=ğ‘›2=1000, using m-UCB with
ğ›¼=1would yield reward-cost UCB values of â‰ˆ2.95for arm 1 and
â‰ˆ48.6for arm 2 due to the high influence of the denominator in
Eq.(1)(rhs). m-UCB would hence pull arm 2. In comparison, ğœ”-UCB
would compute values of 5.5and2.1, and pull arm 1.
More recent algorithms are adaptations of exiting ones to spe-
cific scenarios. For example, Avadhanula et al. [ 3] develop a multi-
platform algorithm for Bandits with Knapsacks, and Das et al. [ 9]
extend BwK to the combinatorial setting in which the algorithm
can pull one or more arms in each round. However, the authors
assume that an armâ€™s cost is a known, fixed quantity, while we
address the challenge of dealing with cost distributions. [ 7] pro-
poses a novel â€˜bandits with interruptionsâ€™ framework in which a
player can interrupt a taken action to limit the cost. [ 8] extends
Budgeted MABs to handle unbounded cost and reward distributions
and presents policies for various settings. Policy UCB-B2 is tailored
for our setting of uncorrelated, bounded rewards and costs, and
2One can construct analogous examples for other symmetric bounds, such as Bern-
steinâ€™s inequality.relies on an empirical version of Bernsteinâ€™s inequality. However,
using this bound does not resolve the bias illustrated in Example 2.
The above policies either have issues (i1)â€“(i3) [ 8,18,19,22â€“24],
are not designed for continuous random costs [10, 17, 23], require
knowledge of ğµ[4,17], and/or have been shown to perform in-
ferior to others [ 10,16,17]. Figure 2 provides a compilation of
existing head-to-head empirical comparisons between various poli-
cies. Upwards pointing triangles indicate that the policy in the
corresponding row outperformed the policy in the corresponding
column in the respective paper, while downward pointing triangles
indicate the opposite. Circles represent cases where both policies
performed similarly, while horizontal lines indicate that the policies
have not been compared. One sees that KUBE [ 17] outperforms
ğœ€-first [ 16], while UCB-BV1 [ 10] and BTS[ 22] outperform KUBE.
UCB-BV1 is inferior to more recent policies [ 18,19,22,23]. BTS [ 23],
b-greedy [24], and {i, c, m}-UCB [24] outperform PD-BwK [4]. We
will compare our policy to the best performing existing policies
(BTS, Budget-UCB, {i, c, m}-UCB, b-greedy, and UCB-SC+) and to
the most recent UCB-B2.
4 OUR POLICY
We now detail our policy ğœ”-UCB and analyze it theoretically.
4.1ğœ”-UCB
Our policyğœ”-UCB, summarized in Alg. 1, starts by playing each
arm once. At each subsequent time step ğ‘¡, the policy chooses the
armğ‘˜ğ‘¡with the highest upper confidence bound of the ratio of the
expected reward ğœ‡ğ‘Ÿ
ğ‘˜to the expected cost ğœ‡ğ‘
ğ‘˜. Letğœ”ğ‘Ÿ
ğ‘˜+(ğ›¼,ğ‘¡)denote the
upper confidence bound of ğœ‡ğ‘Ÿ
ğ‘˜for a confidence level 1âˆ’ğ›¼. Similarly,
ğœ”ğ‘
ğ‘˜âˆ’(ğ›¼,ğ‘¡)is the lower confidence bound of ğœ‡ğ‘
ğ‘˜.ğœ”-UCB chooses ğ‘˜ğ‘¡
according to:
ğ‘˜ğ‘¡=arg max
ğ‘˜âˆˆ[ğ¾]Î©ğ‘˜(ğ›¼,ğ‘¡),where Î©ğ‘˜(ğ›¼,ğ‘¡)=ğœ”ğ‘Ÿ
ğ‘˜+(ğ›¼,ğ‘¡)
ğœ”ğ‘
ğ‘˜âˆ’(ğ›¼,ğ‘¡)(2)
Unlike other policies that rely on the same principle [ 4,8,22,
24,25],ğœ”-UCB calculates asymmetric confidence bounds that are
shifted towards the center of the range of the random variable. This
leads to tighter UCB for the reward-cost ratio especially when an
armâ€™s expected cost or the number of plays is low.
Theorem 1 (Asymmetric confidence interval for bounded random
variables). Letğ‘‹be a random variable bounded in the interval
[ğ‘š,ğ‘€], with unknown expected value ğœ‡âˆˆ[ğ‘š,ğ‘€]and variance ğœ2.
Letğ‘§denote the number of standard deviations required to achieve
1âˆ’ğ›¼confidence in coverage of the standard normal distribution.
LetË†ğœ‡be the sample mean of ğ‘›iid samples of ğ‘‹. Then,
Pr[ğœ‡âˆ‰[ğœ”âˆ’(ğ›¼),ğœ”+(ğ›¼)]]â‰¤ğ›¼,
with
ğœ”Â±(ğ›¼)=ğµ
2ğ´Â±âˆšï¸‚
ğµ2
4ğ´2âˆ’ğ¶
ğ´, (3)
where
ğ´=ğ‘›+ğ‘§2ğœ‚, ğµ=2ğ‘›Ë†ğœ‡+ğ‘§2ğœ‚(ğ‘€+ğ‘š), ğ¶=ğ‘›Ë†ğœ‡2+ğ‘§2ğœ‚ğ‘€ğ‘š
and
ğœ‚=(ğœ2
(ğ‘€âˆ’ğœ‡)(ğœ‡âˆ’ğ‘š)ifğœ‡âˆˆ(ğ‘š,ğ‘€)
1 ifğœ‡âˆˆ{ğ‘š,ğ‘€}.
 
1075KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard FouchÃ©, and Klemens BÃ¶hm
Policy
ğœ€-first
KUBE
UCB-BV1
PD-BwK
Budget-UCB
BTS
MRCB
m-UCB
b-greedy
c-UCB
i-UCB
KL-UCB-SC+
UCB-SC+
UCB-B2
Îµ-/f_irst
KUBE
UCB-BV1
PD-BwK
Budget-UCB
BTS
MRCB
m-UCB
b-greedy
c-UCB
i-UCB
KL-UCB-SC+
UCB-SC+
UCB-B2Year Type Compared
2010 â€“ Ã—
2012 â€“ Ã—
2013 h Ã—
2013 c Ã—
2015 hâœ“
2015 â€“âœ“
2016 c â€“
2017 câœ“
2017 â€“âœ“
2017 hâœ“
2017 uâœ“
2017 u â€“
2018 uâœ“
2020 uâœ“
Figure 2: Empirical performance of different Budgeted MAB policies according to related work
Proof. Using the central limit theorem and Bhatia-Davis in-
equality, we follow similar steps as [ 21]. We first handle the case
whereğœ‡âˆˆ(ğ‘š,ğ‘€). We then address the cases ğœ‡=ğ‘šandğœ‡=ğ‘€.
Caseğœ‡âˆˆ(ğ‘š,ğ‘€).The central limit theorem states that for a large
enough sample size, Ë†ğœ‡approximately follows a normal distribution
with meanğœ‡and variance ğœ2/ğ‘›. I.e.,
Ë†ğœ‡âˆ¼N 
ğœ‡,âˆšï¸‚
ğœ2
ğ‘›!
â‡â‡’Ë†ğœ‡âˆ’ğœ‡âˆšï¸ƒ
ğœ2
ğ‘›âˆ¼N( 0,1).
Therefore, Ë†ğœ‡likely falls into an interval that is centered around
ğœ‡and scaled by ğœ. The valueğ‘§is the number of standard deviations
such that Ë†ğœ‡falls out of the corresponding confidence interval with
a probability of ğ›¼.
Pr
Ë†ğœ‡âˆ‰
ğœ‡âˆ’ğœâˆšğ‘›ğ‘§,ğœ‡+ğœâˆšğ‘›ğ‘§
=ğ›¼ (4)
Next, we apply the Bhatia-Davis inequality [ 5] to express ğœas
a function of ğœ‡. It states that ğœ2â‰¤(ğ‘€âˆ’ğœ‡)(ğœ‡âˆ’ğ‘š). Hence, there
exists a factor ğœ‚âˆˆ[0,1]such that
ğœ2=ğœ‚(ğ‘€âˆ’ğœ‡)(ğœ‡âˆ’ğ‘š). (5)
This gives us an expression for the interval bounds in Eq. (4)
that is quadratic w.r.t. ğœ‡:
(Ë†ğœ‡âˆ’ğœ‡)2=ğœ2
ğ‘›ğ‘§2=ğœ‚(ğ‘€âˆ’ğœ‡)(ğœ‡âˆ’ğ‘š)
ğ‘›ğ‘§2(6)
Solving Eq. (6)forğœ‡yields the endpoints ğœ”âˆ’(ğ›¼)andğœ”+(ğ›¼)of
our confidence interval:
Pr[ğœ‡âˆ‰[ğœ”âˆ’(ğ›¼),ğœ”+(ğ›¼)]]=ğ›¼
with
ğœ”âˆ’(ğ›¼),ğœ”+(ğ›¼)=ğµ
2ğ´Â±âˆšï¸‚
ğµ2
4ğ´2âˆ’ğ¶
ğ´Algorithm 1 ğœ”-UCB
Require:ğ¾ âŠ²Number of arms
Require:ğµ âŠ²Available budget
Require:ğœŒ âŠ²CI scaling parameter, defaults to 1/4
Require:Â®ğœ‚ğ‘ŸâŠ²Reward variance parameters, defaults to [1]ğ¾
Require:Â®ğœ‚ğ‘âŠ²Cost variance parameters, defaults to [1]ğ¾
ğ‘¡â†0
Â®ğ‘›â†[0]ğ¾
whileğµ>0do
ifğ‘¡<ğ‘˜then âŠ²Play each arm once
Play armğ‘¡
Observeğ‘Ÿğ‘¡,ğ‘ğ‘¡and update Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡),Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)
Â®ğ‘›[ğ‘¡]=1
else
ğ‘§ğœŒ(ğ‘¡)=âˆšï¸
2ğœŒlogğ‘¡âŠ²Corresponds to ğ›¼(ğ‘¡)<1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒ
for all armsğ‘˜âˆˆ1...ğ¾ do
Compute Î©ğ‘˜(ğ›¼,ğ‘¡)from Eq. (2)and Eq. (3)withğ‘§ğœŒ(ğ‘¡),
Â®ğœ‚ğ‘Ÿ[ğ‘˜],Â®ğœ‚ğ‘[ğ‘˜],Â®ğ‘›[ğ‘˜],Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡), and Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)
Findğ‘˜ğ‘¡=arg maxğ‘˜Î©ğ‘˜(ğ›¼,ğ‘¡)
Play armğ‘˜ğ‘¡
Observeğ‘Ÿğ‘¡,ğ‘ğ‘¡and update Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡),Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)
Â®ğ‘›[ğ‘˜ğ‘¡]=Â®ğ‘›[ğ‘˜ğ‘¡]+1
ğµ=ğµâˆ’ğ‘ğ‘¡
ğ‘¡=ğ‘¡+1
and
ğ´=ğ‘›+ğ‘§2ğœ‚, ğµ =2ğ‘›Ë†ğœ‡+ğ‘§2ğœ‚(ğ‘€+ğ‘š), ğ¶=ğ‘›Ë†ğœ‡2+ğ‘§2ğœ‚ğ‘€ğ‘š.
Casesğœ‡=ğ‘šandğœ‡=ğ‘€.Forğœ‡=ğ‘š(the case for ğœ‡=ğ‘€is
analogous), the probability that ğœ‡is not in the confidence interval
[ğœ”âˆ’(ğ›¼),ğœ”+(ğ›¼)]is zero, which is less than ğ›¼. Additionally, we have
 
1076Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
ğœ2=(ğ‘€âˆ’ğœ‡)(ğœ‡âˆ’ğ‘š)=0, which implies that Eq. (5)holds for
any choice of ğœ‚. However, since ğœ‡is an unknown quantity, we can
never be certain that ğœ‡=ğ‘šbased on some sample from ğ‘‹. In the
worst-case scenario, ğ‘‹is a random variable that takes only extreme
values, i.e., ğ‘‹âˆˆ{ğ‘š,ğ‘€}, withğœ‡greater than and approximately
equal toğ‘š. In this case, ğœ‚=1by definition. Hence, we define ğœ‚=1
forğœ‡âˆˆğ‘š,ğ‘€ . Combining the special case that ğœ‡âˆˆ{ğ‘š,ğ‘€}with the
result from the previous paragraph gives Theorem 1. â–¡
Illustration of asymmetry. The center of the confidence interval
is a weighted average of the sample mean and the center of the
range of the random variable. This leads to confidence intervals that
are shifted towards the center of the range of the random variable.
To see this, we can compare the distance between Ë†ğœ‡and the interval
centerğµ/2ğ´to half the width of the confidence interval:
Asymmetry =Ë†ğœ‡âˆ’ğµ
2ğ´âˆšï¸ƒ
ğµ2
4ğ´2âˆ’ğ¶
ğ´âˆˆ[0,1]
For Bernoulli random variables, after some derivations and inserting
the definitions of ğ´,ğµ,ğ¶ as specified in Theorem 1, we obtain
Asymmetry =(2Ë†ğœ‡âˆ’1)2ğ‘§2
4ğ‘›Ë†ğœ‡(1âˆ’Ë†ğœ‡)+ğ‘§2(7)
Figure 3 plots the asymmetry measure for different values of ğ‘›
over Ë†ğœ‡andğ‘§=3. (1) For Ë†ğœ‡=1and Ë†ğœ‡=0, the asymmetry takes on
a maximum value of 1, while for Ë†ğœ‡=0.5, asymmetry is 0. (2) For
a given value of Ë†ğœ‡âˆˆ(0,1), asymmetry decreases with increasing
sample size. (3) Related to this, we see that asymmetry is maximal
for a given Ë†ğœ‡forğ‘›=1.
0.00 0.25 0.50 0.75 1.00
Âµ01Asymmetryn
1
10
100
1000
10000
Figure 3: Asymmetry measure from Eq. (7)for Bernoulli re-
wards and costs for different ğ‘›andğœ‡forğ‘§=3.
Discussion. According to the Bhatia-Davis inequality [ 5],0â‰¤
ğœ2â‰¤(ğ‘€âˆ’ğœ‡)(ğœ‡âˆ’ğ‘š), and hence ğœ‚âˆˆ[0,1]. For the special case
of Bernoulli random variables, ğœ‚=1,ğ‘š=0,ğ‘€=1, and Theo-
rem 1 recovers Wilsonâ€™s original confidence interval for Binomial
proportions [ 21]. However, this theorem is more flexible than Wil-
sonâ€™s original method. It enables tighter confidence intervals for
non-Bernoulli costs or rewards, by setting ğœ‚<1when an estimate
or an upper bound of the variance is available. Our experiments
demonstrate that this flexibility leads to a significant performance
improvement.
The following theorem defines an upper confidence bound Î©(ğ›¼)
for the ratio of the expected values of two random variables. Com-
bined with Theorem 1, it facilitates the computation of an armâ€™s
index according to Eq. (2).Theorem 2 (UCB for ratio of expected values of two random vari-
ables). Letğ‘…andğ¶be two bounded random variables with expected
valuesğœ‡ğ‘Ÿâ‰¥0andğœ‡ğ‘>0. Letğœ”ğ‘Ÿ+(ğ›¼)â‰¥0denote the upper confi-
dence bound of ğ‘…andğœ”ğ‘âˆ’(ğ›¼)>0the lower confidence bound of ğ¶
as given in Theorem 1. Let Î©(ğ›¼)=ğœ”ğ‘Ÿ+(ğ›¼)/ğœ”ğ‘âˆ’(ğ›¼). Then,
Prğœ‡ğ‘Ÿ
ğœ‡ğ‘>Î©(ğ›¼)
â‰¤ğ›¼
Proof. Define events ğ¸1=ğœ‡ğ‘Ÿ>ğœ”ğ‘Ÿ+(ğ›¼)andğ¸2=ğœ‡ğ‘<ğœ”ğ‘âˆ’(ğ›¼).
A violation of the UCB of the reward-cost ratio requires that ei-
therğ¸1orğ¸2occurs, or that both events happen simultaneously.
Therefore, by the union bound we have that Pr[ğœ‡ğ‘Ÿ/ğœ‡ğ‘>Î©ğ‘˜]=
Pr
ğœ‡ğ‘Ÿ/ğœ‡ğ‘>ğœ”ğ‘Ÿ+(ğ›¼)/ğœ”ğ‘âˆ’(ğ›¼)
â‰¤Pr[ğ¸1]+Pr[ğ¸2].ğ¸1andğ¸2both oc-
cur with probability â‰¤ğ›¼/2, hence Pr[ğœ‡ğ‘Ÿ/ğœ‡ğ‘>Î©ğ‘˜]â‰¤ğ›¼.â–¡
A UCB-sampling policy that keeps parameter ğ›¼constant leads
to linear regret in the worst case. This is because such a policy
will eventually stop exploring arms that may have high costs and
low rewards in the beginning. To avoid this problem, ğœ”-UCB de-
creases the value of ğ›¼as the time ğ‘¡progresses, similarly to the
UCB1-policy [ 2]. This adaptive approach helps to ensure continued
exploration of arms and guarantees sub-linear regret. Theorem 3
introduces the scaling law and relates it to the confidence level.
Theorem 3 (Time-adaptive confidence interval). For an armğ‘˜, let
ğœ‡ğ‘Ÿ
ğ‘˜be its expected reward, ğœ‡ğ‘
ğ‘˜its expected cost, and Î©ğ‘˜(ğ›¼,ğ‘¡)the
upper confidence bound for ğœ‡ğ‘Ÿ
ğ‘˜/ğœ‡ğ‘
ğ‘˜, as in Eq. (2). ForğœŒ,ğ‘¡>0, and
ğ›¼(ğ‘¡)<1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒit holds that
Pr"
Î©ğ‘˜(ğ›¼,ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜
ğœ‡ğ‘
ğ‘˜#
â‰¥1âˆ’ğ›¼(ğ‘¡),
that is, the upper confidence bound holds asymptotically almost
surely.
Proof. We start from Theorem 1 and equate the confidence level
1âˆ’ğ›¼of the individual reward and cost distributions to the number of
standard deviations ğ‘§. This involves the cumulative density function
(cdf) of the standard normal distribution. We then replace the cdf
with an approximation that has a closed form solution for ğ‘§. Our
choice ofğ‘§cancels out the exponential term in this approximation,
similar to the UCB1-policy [ 2]. Last, we apply Theorem 2 to obtain
the final result.
Step 1. We relate the confidence level 1âˆ’ğ›¼(ğ‘¡)at timeğ‘¡to the
cumulative density function of the standard normal distribution
(erf abbreviates the error function).
1âˆ’ğ›¼(ğ‘¡)
2=1
2
1+erfğ‘§âˆš
2
Solving forğ›¼(ğ‘¡)yields:
ğ›¼(ğ‘¡)=1âˆ’erfğ‘§âˆš
2
Step 2. We now replace the error function erf
ğ‘§âˆš
2
in the equa-
tion above with a series expansion based on BÃ¼rmannâ€™s theorem [ 13,
20]; we summarize all but the first addend in a remainder term
 
1077KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard FouchÃ©, and Klemens BÃ¶hm
ğ›¾
ğ‘§âˆš
2
>0. This term has a maximum of ğ›¾(0.71)â‰ˆ0.0554 and
approaches 0 for larger ğ‘§:
ğ›¼(ğ‘¡)=1âˆ’Â©Â­
Â«âˆšï¸„
1âˆ’exp
âˆ’ğ‘§2
2
+ğ›¾ğ‘§âˆš
2
ÂªÂ®
Â¬
Omitting the ğ›¾-term gives an upper bound for ğ›¼(ğ‘¡):
ğ›¼(ğ‘¡)<1âˆ’âˆšï¸„
1âˆ’exp
âˆ’ğ‘§2
2
(8)
Step 3. Next, we choose ğ‘§as a function of logğ‘¡andğœŒ>0,ğ‘§ğœŒ(ğ‘¡)=âˆšï¸
2ğœŒlogğ‘¡. This results in a time-increasing confidence level 1âˆ’ğ›¼(ğ‘¡):
Pr[ğœ‡âˆ‰[ğœ”âˆ’(ğ›¼(ğ‘¡)),ğœ”+(ğ›¼(ğ‘¡))]]â‰¤ğ›¼(ğ‘¡)withğ›¼(ğ‘¡)<1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒ
Step 4. Applying Theorem 2 gives
Pr"
ğœ‡ğ‘Ÿ
ğ‘˜
ğœ‡ğ‘
ğ‘˜>Î©ğ‘˜(ğ›¼,ğ‘¡)#
â‰¤ğ›¼(ğ‘¡)withğ›¼(ğ‘¡)<1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒ.
The complementary event, Î©ğ‘˜(ğ›¼,ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜/ğœ‡ğ‘
ğ‘˜, holds with a prob-
ability of at least 1âˆ’ğ›¼(ğ‘¡),
Pr"
Î©ğ‘˜(ğ›¼,ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜
ğœ‡ğ‘
ğ‘˜#
â‰¥1âˆ’ğ›¼(ğ‘¡)withğ›¼(ğ‘¡)<1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒ,
which is the result given in Theorem 3.
â–¡
Withğ›¼(ğ‘¡)<1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒ, the confidence level 1âˆ’ğ›¼approaches
1as timeğ‘¡goes to infinity. This encourages exploration of arms that
are played less frequently. Moreover, it establishes a logarithmic
dependence between ğ‘§in Theorem 1 and ğ‘¡, i.e.,ğ‘§ğœŒ(ğ‘¡)=âˆšï¸
2ğœŒlogğ‘¡,
which will be useful in our regret analysis. The next section analyzes
the worst-case regret of ğœ”-UCB. To simplify notation, we abbreviate
Î©(ğ›¼,ğ‘¡)asÎ©(ğ‘¡),ğœ”ğ‘
ğ‘˜âˆ’(ğ›¼,ğ‘¡)asğœ”ğ‘
ğ‘˜âˆ’(ğ‘¡), andğœ”ğ‘Ÿ
ğ‘˜+(ğ›¼,ğ‘¡)asğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)in our
regret analysis.
4.2 Regret Analysis
We first bound the expected number of suboptimal plays E[ğ‘›ğ‘˜(ğœ)]
before some time step ğœ. We use the result to derive the regret
bound ofğœ”-UCB (cf. Theorem 5). Due to space restrictions, we omit
longer proofs here but provide them in Appendix B.
Theorem 4 (Number of suboptimal plays). Forğœ”-UCB, the ex-
pected number of plays of a suboptimal arm ğ‘˜>1before time step
ğœ,E[ğ‘›ğ‘˜(ğœ)], is upper-bounded by
E[ğ‘›ğ‘˜(ğœ)]â‰¤ 1+ğ‘›âˆ—
ğ‘˜(ğœ)+ğœ‰(ğœ,ğœŒ), (9)
where
ğœ‰(ğœ,ğœŒ)=(ğœâˆ’ğ¾)
2âˆ’âˆš
1âˆ’ğœâˆ’ğœŒ
âˆ’ğœâˆ‘ï¸
t=K+1âˆš
1âˆ’ğ‘¡âˆ’ğœŒ,
ğ‘›âˆ—
ğ‘˜(ğœ)=8ğœŒlogğœ
ğ›¿2
ğ‘˜max(
ğœ‚ğ‘Ÿ
ğ‘˜ğœ‡ğ‘Ÿ
ğ‘˜
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜,ğœ‚ğ‘
ğ‘˜(1âˆ’ğœ‡ğ‘
ğ‘˜)
ğœ‡ğ‘
ğ‘˜)
,
ğ›¿ğ‘˜=Î”ğ‘˜/(Î”ğ‘˜+1
ğœ‡ğ‘
ğ‘˜), andğ¾andÎ”ğ‘˜are defined as before, cf. Section 2.The1in Eq. (9)represents the very first pull of arm ğ‘˜. We inter-
pret the term ğ‘›âˆ—
ğ‘˜(ğœ)as a minimum number of plays that leads to a
â€œsufficiently smallâ€ deviation between ğœ‡ğ‘Ÿ
ğ‘˜/ğœ‡ğ‘
ğ‘˜andË†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)/Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡). This
number is logarithmic w.r.t. ğœ. The termğœ‰(ğœ,ğœŒ)represents those
plays that occur despite the armsâ€™ sufficiently small deviation be-
tween the true reward-cost ratio and the ratio of the sample means.
For each suboptimal play, a policy suffers a greater-than-zero
regret in expectation. Hence, the number of suboptimal plays is
closely linked to a policyâ€™s regret. Lemma 4 in [ 24] establishes this
connection for Budgeted MABs. In combination with Theorem 4,
this lemma thus yields the worst-case regret for ğœ”-UCB.
Theorem 5 (Finite-budget instance-dependent regret). Define
Î”ğ‘˜,ğ‘›âˆ—
ğ‘˜(ğœğµ)andğœ‰(ğœğµ,ğœŒ)as before, and ğœğµ=j
2ğµ/minğ‘˜âˆˆ[ğ¾]ğœ‡ğ‘
ğ‘˜k
.
For anyğœŒ>0,ğœ”-UCB suffers instance-dependent regret of
Regretâ‰¤ğ¾âˆ‘ï¸
ğ‘˜=2Î”ğ‘˜
1+ğ‘›âˆ—
ğ‘˜(ğœğµ)+ğœ‰(ğœğµ,ğœŒ)
+X(ğµ)ğ¾âˆ‘ï¸
ğ‘˜=2Î”ğ‘˜+2ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1,(10)
whereX(ğµ)is inO
(ğµ/ğœ‡ğ‘
ğ‘šğ‘–ğ‘›)ğ‘’âˆ’0.5ğµğœ‡ğ‘
ğ‘šğ‘–ğ‘›
.
Proof. Lemma 4 of [ 24] provides a policy-independent regret
expression for Budgeted MAB policies:
Regretâ‰¤ğ¾âˆ‘ï¸
ğ‘˜=2Î”ğ‘˜E[ğ‘›ğ‘˜(ğœğµ)]+X(ğµ)ğ¾âˆ‘ï¸
ğ‘˜=2Î”ğ‘˜+2ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1(11)
where
ğœğµ=$
2ğµ
minğ‘˜âˆˆ[ğ¾]ğœ‡ğ‘
ğ‘˜%
andX(ğµ)is inO
(ğµ/ğœ‡ğ‘
ğ‘šğ‘–ğ‘›)ğ‘’âˆ’0.5ğµğœ‡ğ‘
ğ‘šğ‘–ğ‘›
.
Substituting E[ğ‘›ğ‘˜(ğœğµ)]in Eq. (11)with the result from Theorem 4
completes the proof. â–¡
Theorem 5 and our definition of ğ‘›âˆ—
ğ‘˜(ğœ)show that the regret of
ğœ”-UCB decreases for small ğœ‚ğ‘Ÿ
ğ‘˜andğœ‚ğ‘˜ğ‘. I.e., when the reward and cost
distributions have small variance compared to a Bernoulli variable
with the same expected value.
The termğœ‰(ğœ,ğœŒ)decreases, while ğ‘›âˆ—
ğ‘˜(ğœ)increases with ğœŒ. Fur-
ther derivations show that for increasingly large budgets, ğœ‰(ğœ,ğœŒ)
converges for ğœŒ>1, grows logarithmic for ğœŒ=1, and superloga-
rithmic (on the order of O(ğµ1âˆ’ğœŒ)) forğœŒ<1; see Appendix B.4 for
the details. This results in the following asymptotic behavior:
Theorem 6 (Asymptotic regret). The regret of ğœ”-UCB is in
O
ğµ1âˆ’ğœŒ
for0<ğœŒ<1,and inO(logğµ)forğœŒâ‰¥1.
5 EXPERIMENTAL SETUP
This section presents the experimental setup used to evaluate the
policies. We introduce the MAB settings, followed by the configura-
tions ofğœ”-UCB and its competitors. We conducted the experiments
on an Ubuntu 20.04 server with x86-architecture, using 32 cores,
each running at 2.0 GHz, and 128 GB of RAM.
 
1078Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
5.1 Budgeted MAB Settings
We use MAB settings based on synthetic and real data, which we
describe separately. Each setting comprises a specific combination
of reward and cost distributions, and the number of arms ğ¾. See
Table 1 for a summary.
Table 1: Evaluation settings
Type Distr. Params K Used in Id
Synth.BernoulliU(0,1)10 [23, 24] S-Br-10
50 [24] S-Br-50
100 [22, 23] S-Br-100
General.
BernoulliU(0,1)10 [23, 25] S-GBr-10
50 [25] S-GBr-50
100 [23] S-GBr-100
BetaU(0,5)10 [24, 25] S-Bt-10
50 [24, 25] S-Bt-50
100 [22] S-Bt-100
Face-
bookBernoulli given [2,97] â€“ FB-Br
Beta random [2,97] â€“ FB-Bt
Synthetic Data. Previous studies on Budgeted Multi-Armed Ban-
dits (MABs) use synthetic settings with rewards and costs drawn
from discrete (Bernoulli or Generalized Bernoulli with possible
outcomes{0.0,0.25,0.5,0.75,1.0}) or continuous (Beta) distribu-
tions [ 18,19,22,24]. These studies typically generate parame-
ters randomly within a given range [ 22â€“25] and use 10 to 100
arms [ 19,22â€“25]. We adopt this and set the parameter ranges to
those used in related work.
Social-Media Advertisement Data. We also evaluate our policy
in a social media advertisement scenario described in Example 1.
We use real-world data from [ 12]. It contains information about
different ads based on their target gender (female or male) and age
category (30â€“34, 34â€“39, 40â€“44, 45â€“49), along with the number of
displays and clicks, the total cost, and the number of purchases.
We group the ads by target gender and age category, resulting in
19 â€œadvertisement campaignsâ€ (Budgeted MAB settings). Each cam-
paign has between 2 and 93 ads (arms). We compute the expected
rewardsğœ‡ğ‘Ÿ
ğ‘˜and costsğœ‡ğ‘
ğ‘˜of each ad as the average revenue per click
and average cost per click, respectively. We model both discrete
and continuous rewards and costs. For the discrete case, we sample
rewards and costs from two Bernoulli distributions with expected
values ofğœ‡ğ‘Ÿ
ğ‘˜andğœ‡ğ‘
ğ‘˜, respectively. In the continuous case, we use
a Beta distribution and sample the distribution parameters from a
uniform distribution with a range of (0, 5). We then adjust one of
the parameters to ensure that the expected values of rewards and
costs match ğœ‡ğ‘Ÿ
ğ‘˜andğœ‡ğ‘
ğ‘˜.
5.2 Budgeted MAB Policies
We test the performance of two variants of our policy: ğœ”-UCB
andğœ”âˆ—-UCB. Theğœ”-UCB variant uses a fixed value of ğœ‚=1for
rewards and costs. The ğœ”âˆ—-UCB usesğœ‚ğ‘Ÿ
ğ‘˜=ğœ‚ğ‘
ğ‘˜=1as default but
estimates their values using sample variance and mean once armğ‘˜has been played sufficiently many times ( ğ‘›ğ‘˜(ğ‘‡) â‰¥ 30), Ë†ğœ‚ğ‘˜=
Ë†ğœ2
ğ‘˜/((ğ‘€âˆ’Ë†ğœ‡ğ‘˜)(Ë†ğœ‡ğ‘˜âˆ’ğ‘š)), where bars refer to sample estimates as
before. We experiment with two values of the hyperparameter ğœŒ:
ğœŒ=1andğœŒ=1/4. The former is the minimum value for which we
have proven logarithmic regret. The latter has performed well in
our sensitivity study, as we will demonstrate in Section 6.2.
We compare the performance of our policy to several other state-
of-the-art Budgeted MAB policies, including BTS, Budget-UCB, i-
UCB, c-UCB, m-UCB, b-greedy, UCB-SC+, and UCB-B2. We set the
hyperparameters for each competitor to the values recommended
in their respective papers. Appendix A features details about the
policies and their hyperparameters.
6 RESULTS
To observe the asymptotic behavior of the policies, we set the budget
ğµto1.5Â·105times the minimum expected cost. We run each policy
until the available budget is depleted and report the average results
over 100 independent repetitions with the repetition index as the
seed for the random number generator. Since we draw the expected
costğœ‡ğ‘
ğ‘˜and expected reward ğœ‡ğ‘Ÿ
ğ‘˜randomly in each repetition of the
experiment, we normalize the budget in our graphs. Note that, in
some instances, an approach may not appear in a graph if its regret
exceeded the graphâ€™s upper y-axis limit.
6.1 Performance of Budgeted MAB Policies
6.1.1 Synthetic Bernoulli. Figure 4a shows the regret of the policies
for Bernoulli-distributed rewards and costs with 95% confidence
intervals. We do not present ğœ”âˆ—-UCB in this experiment since its
results are almost identical to ğœ”-UCB.ğœ”-UCB and BTS achieve
logarithmic regret and demonstrate better asymptotic behavior than
other methods. Although {i,c,m}-UCB may outperform ğœ”-UCB with
ğœŒ=1for small budgets, their regret grows rapidly as the budget
increases, indicating poor asymptotic behavior. One surprising
observation is that UCB-B2 is not competitive in our experiments
despite its tight and accurate UCB (cf. Fig. 1). Our assumption is
that this approach decreases the confidence level too fast, which we
found can result in over-exploration. For ğ¾=50andğ¾=100, our
policy has lower regret than BTS. BTS outperforms ğœ”-UCB withğœŒ=
1only on the 10-armed bandit. ğœ”-UCB withğœŒ=1/4outperforms
all other policies on small and large budgets and regardless of ğ¾.
ComparingğœŒ=1/4withğœŒ=1, one can see that the curve for ğœŒ=1
is linear (the x-axis is logarithmic), while for ğœŒ=1/4it is convex.
We conclude that ğœŒ=1/4leads to smaller regret than ğœŒ=1for not
too large budgets but that ğœŒ=1performs better asymptotically.
6.1.2 Synthetic Generalized Bernoulli and Synthetic Beta. Figure 4b
shows the policiesâ€™ regret for rewards and costs drawn from Gen-
eralized Bernoulli distributions and Figure 4c for Beta distributed
rewards and costs. Besides ğœ”-UCB, we also present the results for
ğœ”âˆ—-UCB which estimates ğœ‚ğ‘Ÿ
ğ‘˜andğœ‚ğ‘
ğ‘˜from the sample variance of
rewards and costs. ğœ”-UCB andğœ”âˆ—-UCB withğœŒ=1/4outperform
their competitors, except for ğ¾=100in the Beta bandit where
m-UCB performs comparable. We also notice that BTS is not com-
petitive in this evaluation setting, likely because the policy cannot
account for the (often very small) variance of the sampled Beta
distributions. Our ğœ”âˆ—-UCB policy is advantageous in such cases.
 
1079KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard FouchÃ©, and Klemens BÃ¶hm
10âˆ’1100
Normalized Budget02000Regret
K= 10
10âˆ’1100
Normalized Budget05000
K= 50
10âˆ’1100
Normalized Budget010000
K= 100Ï‰-UCB (Ï= 1)
Ï‰-UCB (Ï=1
4)Ï‰âˆ—-UCB (Ï= 1)
Ï‰âˆ—-UCB (Ï=1
4)UCB-SC+
Budget-UCBUCB-B2
m-UCBc-UCB
i-UCBBTS
b-greedy
(a) Settings S-Br-{10,50,100}
10âˆ’1100
Normalized Budget010002000Regret
K= 10
10âˆ’1100
Normalized Budget020004000
K= 50
10âˆ’1100
Normalized Budget020004000
K= 100
(b) Settings S-GBr- {10,50,100}
10âˆ’1100
Normalized Budget02000Regret
K= 10
10âˆ’1100
Normalized Budget05000
K= 50
10âˆ’1100
Normalized Budget010000
K= 100
(c) Settings S-Bt-{10,50,100}
10âˆ’1100
Normalized Budget01000Regret
(d) Setting FB-Br
10âˆ’1100
Normalized Budget01000Regret
 (e) Setting FB-Bt
0.00 0.25 0.50 0.75 1.00
Âµc
k0.00.51.0
Âµr
k (f) Example kernel density estimate (kde)
1
641
321
161
81
41
212
Ï103104RegretBernoulli
1
641
321
161
81
41
212
ÏBeta
1
641
321
161
81
41
212
ÏGen. Bern.
BTS
Ï‰-UCB
Ï‰âˆ—-UCB
(g) Sensitivity study showing the regret for different choices of ğœŒ
Figure 4: Evaluation results
 
1080Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
6.1.3 Social-Media Advertisement Data. Figure 4d and Figure 4e
show the results of our study on social-media advertisement data3
[12]. Here, the default choice ğœŒ=1/4outperforms all other com-
petitors.ğœŒ=1/4also outperforms ğœŒ=1significantly, although
both choices show good asymptotic behavior. The advertisement
data settings seem to be easier for some competitors (BTS, KL-UCB-
SC+) and harder for others (i,c,m-UCB). The likely cause is that the
distribution of expected rewards and costs between arms is non-
uniform: costs are biased towards 1, and rewards are biased towards
the boundaries of[0,1], as the kernel density estimate (kde) plot
for the MAB with ğ¾=33in Figure 4f illustrates exemplary. Last,
we observe that ğœ”âˆ—-UCB has lower regret than ğœ”-UCB, although
the effect is not as strong as in the synthetic settings.
6.2 Sensitivity Study
We investigate the performance difference between ğœ”-UCB and
ğœ”âˆ—-UCB, as well as the sensitivity of our policy with respect to
the hyperparameter ğœŒ. The results based on our synthetic settings
(cf. Table 1) for ğ¾=50are shown in Figure 4g. We omit the results
forğ¾=10andğ¾=100as they look similar to ğ¾=50.ğœ”-UCB
andğœ”âˆ—-UCB perform best with ğœŒ=1/4when rewards and costs
follow Bernoulli distributions. Both policies achieve comparable
performance in this case. It appears that estimating ğœ‚(which is
known to be 1 in Bernoulli bandits) does not result in a performance
decrease ofğœ”âˆ—-UCB compared to ğœ”-UCB. Also, even though ğœŒ=1/8
works well for ğœ”-UCB when rewards and costs follow a generalized
Bernoulli or Beta distribution, ğœŒ=1/4remains a near-optimal
choice forğœ”âˆ—-UCB. Based on these results, we recommend using
ğœ”âˆ—-UCB withğœŒ=1/4as a default.
7 CONCLUSIONS
We presented a new approach for Budgeted MABs called ğœ”-UCB.
It combines UCB sampling with asymmetric confidence intervals
to address issues of existing approaches. Our interval generalizes
Wilsonâ€™s score interval to arbitrary bounded random variables. An
extension of our approach, ğœ”âˆ—-UCB, tracks the variance of the
reward and cost distributions on the fly to tighten the confidence
intervals. This leads to even better performance when rewards
or costs are continuous. Our analysis shows that ğœ”-UCB achieves
logarithmic regret for ğœŒâ‰¥1, whileğœŒ=1/4performed best in our
experiments. In the future, one could extend our approach to the
successive elimination framework which repeatedly eliminates bad
arms, or derive an instance-independent regret bound. One could
also extend our approach to the non-stationary setting where the
reward and cost distributions change over time. This is particularly
relevant in scenarios like online advertising, where companies want
to promote their products and services continuously.
Acknowledgements. This work was supported by the German Re-
search Foundation (DFG) as part of the Research Training Group
GRK 2153: Energy Status Data â€“ Informatics Methods for its Col-
lection, Analysis, and Exploitation and by the Baden-WÃ¼rttemberg
Foundation via the Elite Program for Postdoctoral Researchers.
3Available at https://www.kaggle.com/datasets/madislemsalu/facebook-ad-campaign
under a PDDL license.REFERENCES
[1]Danilo Ardagna, Barbara Panicucci, and Mauro Passacantando. 2011. A game
theoretic formulation of the service provisioning problem in cloud systems. In
WWW. ACM, 177â€“186.
[2]Peter Auer, NicolÃ² Cesa-Bianchi, and Paul Fischer. 2002. Finite-time Analysis
of the Multiarmed Bandit Problem. Mach. Learn. 47, 2-3 (2002), 235â€“256. https:
//doi.org/10.1023/A:1013689704352
[3]Vashist Avadhanula, Riccardo Colini-Baldeschi, Stefano Leonardi, Karthik Abinav
Sankararaman, and Okke Schrijvers. 2021. Stochastic bandits for multi-platform
budget optimization in online advertising. In WWW â€™21, Jure Leskovec, Marko
Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.). ACM / IW3C2, 2805â€“2817.
https://doi.org/10.1145/3442381.3450074
[4]Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. 2013.
Bandits with Knapsacks. In FOCS. IEEE Computer Society, 207â€“216.
[5]Rajendra Bhatia and Chandler Davis. 2000. A better bound on the variance. Amer.
Math. Monthly 107, 4 (2000), 353â€“357.
[6]Christian Borgs, Jennifer T. Chayes, Nicole Immorlica, Kamal Jain, Omid Ete-
sami, and Mohammad Mahdian. 2007. Dynamics of bid optimization in online
advertisement auctions. In WWW. ACM, 531â€“540.
[7]Semih Cayci, Atilla Eryilmaz, and R. Srikant. 2019. Learning to control renewal
processes with bandit feedback. Proc. ACM Meas. Anal. Comput. Syst. 3, 2 (2019),
43:1â€“43:32. https://doi.org/10.1145/3341617.3326158
[8]Semih Cayci, Atilla Eryilmaz, and R. Srikant. 2020. Budget-constrained bandits
over general cost and reward distributions. In AISTATS (PMLR, Vol. 108), Silvia
Chiappa and Roberto Calandra (Eds.). PMLR, 4388â€“4398. http://proceedings.mlr.
press/v108/cayci20a.html
[9] Debojit Das, Shweta Jain, and Sujit Gujar. 2022. Budgeted Combinatorial Multi-
Armed Bandits. In AAMAS. International Foundation for Autonomous Agents
and Multiagent Systems (IFAAMAS), 345â€“353.
[10] Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. 2013. Multi-Armed
Bandit with Budget Constraint and Variable Costs. In AAAI, Vol. 27. AAAI Press,
232â€“238. https://doi.org/10.1609/aaai.v27i1.8637
[11] Tor Lattimore and Csaba SzepesvÃ¡ri. 2020. Bandit algorithms. Cambridge Univer-
sity Press, Cambridge. https://doi.org/10.1017/9781108571401
[12] Madis Lemsalu. 2017. Facebook ad campaign. https://www.
kaggle.com/madislemsalu/facebook-ad-campaign howpublished: Kaggle
(https://www.kaggle.com/madislemsalu/facebook-ad-campaign).
[13] HM SchÃ¶pf and PH Supancic. 2014. On BÃ¼rmannâ€™s theorem and its application
to problems of linear and nonlinear heat transfer and diffusion. The Mathematica
Journal 16 (2014), 1â€“44.
[14] William R. Thompson. 1933. On the Likelihood that One Unknown Probability
Exceeds Another in View of the Evidence of Two Samples. Biometrika 25, 3/4
(1933), 285â€“294. https://doi.org/10.2307/2332286
[15] William Robin Thompson. 1935. On the Theory of Apportionment. American
Journal of Mathematics 57 (1935), 450.
[16] Long Tran-Thanh, Archie C. Chapman, Enrique Munoz de Cote, Alex Rogers, and
Nicholas R. Jennings. 2010. Epsilon-First Policies for Budget-Limited Multi-Armed
Bandits. In AAAI, Vol. 24. AAAI Press. https://doi.org/10.1609/aaai.v24i1.7758
[17] Long Tran-Thanh, Archie C. Chapman, Alex Rogers, and Nicholas R. Jennings.
2012. Knapsack Based Optimal Policies for Budget-Limited Multi-Armed Bandits.
InAAAI, Vol. 26. AAAI Press, 1134â€“1140. https://doi.org/10.1609/aaai.v26i1.8279
[18] Ryo Watanabe, Junpei Komiyama, Atsuyoshi Nakamura, and Mineichi Kudo.
2017. KL-UCB-Based Policy for Budgeted Multi-Armed Bandits with Stochastic
Action Costs. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 100-A, 11
(2017), 2470â€“2486.
[19] Ryo Watanabe, Junpei Komiyama, Atsuyoshi Nakamura, and Mineichi Kudo.
2018. UCB-SC: A Fast Variant of KL-UCB-SC for Budgeted Multi-Armed Bandit
Problem. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 101-A, 3 (2018),
662â€“667.
[20] E.T. Whittaker and G.N. Watson. 2020. A course of modern analysis; an intro-
duction to the general theory of infinite processes and of analytic functions (4 ed.).
Cambridge University Press, Cambridge. pages: 208 section: 7.3.
[21] Edwin B. Wilson. 1927. Probable Inference, the Law of Succession, and Statistical
Inference. J. Amer. Statist. Assoc. 22, 158 (1927), 209â€“212. https://doi.org/10.2307/
2276774
[22] Yingce Xia, Wenkui Ding, Xu-Dong Zhang, Nenghai Yu, and Tao Qin. 2015.
Budgeted Bandit Problems with Continuous Random Costs. In ACML (JMLR
Workshop and Conference Proceedings, Vol. 45). JMLR.org, 317â€“332.
[23] Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2015. Thompson
Sampling for Budgeted Multi-Armed Bandits. In IJCAI. AAAI Press, 3960â€“3966.
[24] Yingce Xia, Tao Qin, Wenkui Ding, Haifang Li, Xudong Zhang, Nenghai Yu,
and Tie-Yan Liu. 2017. Finite budget analysis of multi-armed bandit problems.
Neurocomputing 258 (2017), 13â€“29. https://doi.org/10.1016/j.neucom.2016.12.079
[25] Yingce Xia, Tao Qin, Weidong Ma, Nenghai Yu, and Tie-Yan Liu. 2016. Budgeted
Multi-Armed Bandits with Multiple Plays. In IJCAI. IJCAI/AAAI Press, 2210â€“
2216.
 
1081KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard FouchÃ©, and Klemens BÃ¶hm
A RELATED UCB APPROACHES
Table 2 summarizes our direct competitors and how they compute
the arm selection indexes Î©ğ‘˜(ğ‘¡).
B PROOFS
B.1 Proof of Theorem 4
The proof starts with a general expression for the number of plays of
a suboptimal arm ğ‘˜>1, where 1{Â·}denotes the indicator function.
ğ‘›ğ‘˜(ğœ)â‰¤1+ğœâˆ‘ï¸
t=K+11
Î©ğ‘˜(ğ‘¡)â‰¥Î©ğ‘—(ğ‘¡),âˆ€ğ‘—â‰ ğ‘–	
â‰¤1+ğœâˆ‘ï¸
t=K+11{Î©ğ‘˜(ğ‘¡)â‰¥Î©1(ğ‘¡)}
This is upper-bounded by
ğ‘›ğ‘˜(ğœ)â‰¤1+ğœâˆ‘ï¸
t=K+1
1
Î©ğ‘˜(ğ‘¡)â‰¥Î©1(ğ‘¡),Î©1(ğ‘¡)<ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
+1
Î©ğ‘˜(ğ‘¡)â‰¥Î©1(ğ‘¡),Î©1(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1 
â‰¤1+ğœâˆ‘ï¸
t=K+1
1
Î©1(ğ‘¡)<ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
+1
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1 
The expected number of plays E[ğ‘›ğ‘˜(ğœ)]is given by the proba-
bilities of the individual events:
E[ğ‘›ğ‘˜(ğœ)]â‰¤ 1+ğœâˆ‘ï¸
t=K+1
Pr
Î©1(ğ‘¡)<ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
|                {z                }
Pr[ğ´]+Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
|                {z                }
Pr[ğµ]
(12)
We now evaluate the sum in the equation above.
Sum of Pr[ğ´].We apply Theorem 2:
ğœâˆ‘ï¸
t=K+1Pr[ğ´]<ğœâˆ‘ï¸
t=K+1h
1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒi
=(ğœâˆ’ğ¾)âˆ’ğœâˆ‘ï¸
t=K+1âˆš
1âˆ’ğ‘¡âˆ’ğœŒ
(13)
Sum of Pr[ğµ].For this step, let us first introduce a helpful lemma:
Lemma 1 bounds Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1/ğœ‡ğ‘
1
after a minimum number of
playsğ‘›âˆ—
ğ‘˜(ğœ), which grows logarithmic with ğœ.
Lemma 1. Defineğ›¿ğ‘˜=Î”ğ‘˜
Î”ğ‘˜+1/ğœ‡ğ‘
ğ‘˜andğ‘›âˆ—
ğ‘˜(ğœ)as follows:
ğ‘›âˆ—
ğ‘˜(ğœ)=8ğœŒlogğœ
ğ›¿2
ğ‘˜max(
ğœ‚ğ‘Ÿ
ğ‘˜ğœ‡ğ‘Ÿ
ğ‘˜
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜,ğœ‚ğ‘
ğ‘˜(1âˆ’ğœ‡ğ‘
ğ‘˜)
ğœ‡ğ‘
ğ‘˜)
The following inequality holds whenever ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—
ğ‘˜(ğœ):
Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
<1âˆ’âˆš
1âˆ’ğœâˆ’ğœŒ
Appendix B.3 contains the proof of Lemma 1. See Appendix B.2
for the derivation of ğ›¿ğ‘˜.The lemma allows to decomposeÃğœ
t=K+1Pr[ğµ]into â€œinitial playsâ€
(ğ‘›ğ‘˜(ğ‘¡)<ğ‘›âˆ—
ğ‘˜(ğœ)) and â€œlater playsâ€ ( ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—
ğ‘˜(ğœ)):
ğœâˆ‘ï¸
t=K+1Pr[ğµ]=ğœâˆ‘ï¸
t=K+1Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
=ğ‘›âˆ—
ğ‘˜(ğœ)+ğœâˆ‘ï¸
t=K+1Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1,ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—
ğ‘˜(ğœ)
We now apply Lemma 1 to evaluate the sum in above equation,
ğœâˆ‘ï¸
t=K+1Pr[ğµ]â‰¤ğ‘›âˆ—
ğ‘˜(ğœ)+ğœâˆ‘ï¸
t=K+1h
1âˆ’âˆš
1âˆ’ğœâˆ’ğœŒi
=ğ‘›âˆ—
ğ‘˜(ğœ)+(ğœâˆ’ğ¾)
1âˆ’âˆš
1âˆ’ğœâˆ’ğœŒ
. (14)
Inserting the results of Eq. (13)and Eq. (14)in Eq. (12)yields a
bound on E[ğ‘›ğ‘˜(ğœ)]:
E[ğ‘›ğ‘˜(ğœ)]â‰¤ 1+ğ‘›âˆ—
ğ‘˜(ğœ)+(ğœâˆ’ğ¾)
2âˆ’âˆš
1âˆ’ğœâˆ’ğœŒ
âˆ’ğœâˆ‘ï¸
t=K+1âˆš
1âˆ’ğ‘¡âˆ’ğœŒ
|                                                 {z                                                 }
ğœ‰(ğœ,ğœŒ)
B.2 Derivation of Proportional ğ›¿-gap
Letğ›¿ğ‘˜be the proportional ğ›¿-gap of arm ğ‘˜. It measures how much
one must increase ğœ‡ğ‘Ÿ
ğ‘˜and decrease ğœ‡ğ‘
ğ‘˜in order to make arm ğ‘˜have
the same reward-cost ratio as arm 1, or in other words, to bridge
the suboptimality gap Î”ğ‘˜between arm ğ‘˜and arm 1. The ğ›¿-gap is
proportional to the possible increase in rewards (decrease in costs)
without violating their range [0,1]. We start our derivation with
Eq. (15), which states this mathematically.
ğœ‡ğ‘Ÿ
ğ‘˜+ğ›¿ğ‘˜(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)
ğœ‡ğ‘
ğ‘˜âˆ’ğ›¿ğ‘˜ğœ‡ğ‘
ğ‘˜=ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1(15)
Rearranging the equation yields
ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜
ğœ‡ğ‘
ğ‘˜=ğ›¿ğ‘˜ 
ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜
ğœ‡ğ‘
ğ‘˜+1
ğœ‡ğ‘
ğ‘˜!
.
Now, recall the definition of an armâ€™s suboptimality, Î”ğ‘˜=ğœ‡ğ‘Ÿ
1/ğœ‡ğ‘
1âˆ’
ğœ‡ğ‘Ÿ
ğ‘˜/ğœ‡ğ‘
ğ‘˜, and solve for ğ›¿ğ‘˜:
ğ›¿ğ‘˜=Î”ğ‘˜
Î”ğ‘˜+1
ğœ‡ğ‘
ğ‘˜
B.3 Proof of Lemma 1
Recall Lemma 1:
Lemma 1. Defineğ›¿ğ‘˜=Î”ğ‘˜
Î”ğ‘˜+1/ğœ‡ğ‘
ğ‘˜andğ‘›âˆ—
ğ‘˜(ğœ)as follows:
ğ‘›âˆ—
ğ‘˜(ğœ)=8ğœŒlogğœ
ğ›¿2
ğ‘˜max(
ğœ‚ğ‘Ÿ
ğ‘˜ğœ‡ğ‘Ÿ
ğ‘˜
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜,ğœ‚ğ‘
ğ‘˜(1âˆ’ğœ‡ğ‘
ğ‘˜)
ğœ‡ğ‘
ğ‘˜)
The following inequality holds whenever ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—
ğ‘˜(ğœ):
Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
<1âˆ’âˆš
1âˆ’ğœâˆ’ğœŒ
 
1082Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Overview of our competitors
Policy Î©ğ‘˜(ğ‘¡) Comment
BTS [23]sample from Beta(ğ›¼ğ‘Ÿ(ğ‘¡),ğ›½ğ‘Ÿ(ğ‘¡))
sample from Beta(ğ›¼ğ‘(ğ‘¡),ğ›½ğ‘(ğ‘¡))ğ›¼ğ‘Ÿ(ğ‘¡)=ğ‘›ğ‘˜(ğ‘¡)Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)+1
ğ›½ğ‘Ÿ(ğ‘¡)=ğ‘›ğ‘˜(ğ‘¡)+2âˆ’ğ›¼ğ‘Ÿ(ğ‘¡)
ğ›¼ğ‘(ğ‘¡)=ğ‘›ğ‘˜(ğ‘¡)Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)+1
ğ›½ğ‘(ğ‘¡)=ğ‘›ğ‘˜(ğ‘¡)+2âˆ’ğ›¼ğ‘(ğ‘¡)
Discretization of con-
tinuous values
m-UCB [24]min{Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)+ğœ€ğ‘˜(ğ‘¡),1}
max{Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)âˆ’ğœ€ğ‘˜(ğ‘¡),0}ğœ€ğ‘˜(ğ‘¡)=ğ›¼âˆšï¸ƒ
log(ğ‘¡âˆ’1)
ğ‘›ğ‘˜(ğ‘¡)
Recommendation: ğ›¼=2âˆ’4
c-UCB [24]Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)+ğœ€ğ‘˜(ğ‘¡)
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)ğœ€ğ‘˜(ğ‘¡)=ğ›¼âˆšï¸ƒ
log(ğ‘¡âˆ’1)
ğ‘›ğ‘˜(ğ‘¡)
Recommendation: ğ›¼=2âˆ’3
i-UCB [24]Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)+ğœ€ğ‘˜(ğ‘¡)ğœ€ğ‘˜(ğ‘¡)=ğ›¼âˆšï¸ƒ
log(ğ‘¡âˆ’1)
ğ‘›ğ‘˜(ğ‘¡)
Recommendation: ğ›¼=2âˆ’2
Budget UCB [22]Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)+ğœ€ğ‘˜(ğ‘¡)
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)
1+min{Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)+ğœ€ğ‘˜(ğ‘¡),1}
max{Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)âˆ’ğœ€ğ‘˜(ğ‘¡),ğœ†}ğœ€ğ‘˜(ğ‘¡)=âˆšï¸ƒ
log(ğ‘¡âˆ’1)
ğ‘›ğ‘˜(ğ‘¡)
ğœ†>0: minimum cost
UCB-SC+ [19]Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)+ğ›¼ğ‘˜(ğ‘¡)Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)âˆ’ğ›¼ğ‘˜(ğ‘¡)Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡),ifË†ğœ‡ğ‘
ğ‘˜(ğ‘¡)2>logğ‘¡
ğ‘›ğ‘˜(ğ‘¡)
2ğ‘›ğ‘˜(ğ‘¡)
âˆ, elseğ›¼ğ‘˜(ğ‘¡)=âˆšï¸„
logğ‘¡
ğ‘›ğ‘˜(ğ‘¡)
2ğœ…ğ‘›ğ‘˜(ğ‘¡)âˆ’logğ‘¡
ğ‘›ğ‘˜(ğ‘¡)
withğœ…=Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)2+Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)2
UCB-B2 [8]1.4ğœ–ğ‘˜,ğ‘¡+Ë†ğ‘Ÿğ‘˜,ğ‘¡
Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)+Ë†ğ‘Ÿğ‘˜,ğ‘¡ if condition 7 in [8] for ğœ†=1.28
âˆ, elseË†ğ‘Ÿğ‘˜,ğ‘¡=max{0,Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)}
max{minimum cost ,Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)}
ğœ–ğ‘˜,ğ‘¡=âˆšï¸‚
2Ë†ğ‘‰ğ‘Ÿ
ğ‘˜,ğ‘¡logğ‘¡ğ›¼
ğ‘›ğ‘˜(ğ‘¡)+3 logğ‘¡ğ›¼
ğ‘›ğ‘˜(ğ‘¡)
ğœ‚ğ‘˜,ğ‘¡=âˆšï¸‚
2Ë†ğ‘‰ğ‘
ğ‘˜,ğ‘¡logğ‘¡ğ›¼
ğ‘›ğ‘˜(ğ‘¡)+3 logğ‘¡ğ›¼
ğ‘›ğ‘˜(ğ‘¡)
Ë†ğ‘‰ğ‘Ÿ
ğ‘˜,ğ‘¡,Ë†ğ‘‰ğ‘
ğ‘˜,ğ‘¡: variance estimates
ğ›¼>2; small choices preferable
The proof is structured in four steps. First, we derive an ex-
pression for the maximum deviation between the observed sample
mean and the unknown expected value of an armâ€™s rewards and
costs that we use later on. Second, we decompose the probability
Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1/ğœ‡ğ‘
1
. Third, we evaluate the decomposed probabili-
ties for cases where ğ‘›ğ‘˜(ğ‘¡)is sufficiently large, that is, ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—
ğ‘˜(ğœ).
Finally, we recombine the decomposed probabilities to obtain the
final result.
Deviation between sample mean and expected value. LetË†ğœ‡ğ‘˜(ğ‘¡)be
the sample mean and ğœ‡ğ‘˜the expected value of arm ğ‘˜â€™s rewards or
costs at time ğ‘¡. To quantify the deviation between mean Ë†ğœ‡ğ‘˜(ğ‘¡)andğœ‡ğ‘˜
we start from Eq. (6)(central limit theorem) and set [ğ‘š,ğ‘€]=[0,1]
andğ‘›=ğ‘›ğ‘˜(ğ‘¡)):
(Ë†ğœ‡ğ‘˜(ğ‘¡)âˆ’ğœ‡ğ‘˜)2â‰¤ğœ‚ğ‘˜ğœ‡ğ‘˜(1âˆ’ğœ‡ğ‘˜)
ğ‘›ğ‘˜(ğ‘¡)ğ‘§2(16)
The above inequality holds with the same probability as our
confidence interval since it is the basis of the confidence interval
derivation. This allows us to bound the deviation between sample
mean and expected value, denoted ğœ€ğ‘˜(ğ‘¡), for our choice of ğ‘§ğœŒ(ğ‘¡)=âˆšï¸
2ğœŒlogğ‘¡:
Pr[|Ë†ğœ‡ğ‘˜(ğ‘¡)âˆ’ğœ‡ğ‘˜|>ğœ€ğ‘˜(ğ‘¡)]â‰¤ğ›¼(ğ‘¡) (17)
with
ğœ€ğ‘˜(ğ‘¡)=âˆšï¸„
2ğœ‚ğ‘˜ğœ‡ğ‘˜(1âˆ’ğœ‡ğ‘˜)ğœŒlogğ‘¡
ğ‘›ğ‘˜(ğ‘¡)andğ›¼(ğ‘¡)<1âˆ’âˆš
1âˆ’ğ‘¡âˆ’ğœŒ
Whenever we refer to ğœ€ğ‘˜(ğ‘¡)w.r.t. rewards or costs we use the
notationsğœ€ğ‘Ÿ
ğ‘˜(ğ‘¡)andğœ€ğ‘
ğ‘˜(ğ‘¡).
Decomposition of Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1/ğœ‡ğ‘
1.Next, we decompose the
probability that Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1/ğœ‡ğ‘
1. The decomposition is analogous
to the one in the proof of Theorem 2 and thus omitted here:
Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
=Pr"
ğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)
ğœ”ğ‘
ğ‘˜âˆ’(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1#
=Pr"
ğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)
ğœ”ğ‘
ğ‘˜âˆ’(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜+(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜
ğœ‡ğ‘
ğ‘˜âˆ’ğœ‡ğ‘
ğ‘˜ğ›¿ğ‘˜#
(18)
â‰¤Pr
ğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜+(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜
+Pr
ğœ”ğ‘
ğ‘˜âˆ’(ğ‘¡)â‰¤ğœ‡ğ‘
ğ‘˜âˆ’ğœ‡ğ‘
ğ‘˜ğ›¿ğ‘˜
(19)
 
1083KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard FouchÃ©, and Klemens BÃ¶hm
For the next step, note that if ğœ‡ğ‘Ÿ
ğ‘˜â‰¤ğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)(andğœ‡ğ‘
ğ‘˜â‰¥ğœ”ğ‘
ğ‘˜âˆ’(ğ‘¡)),
we have that ğœ‡ğ‘Ÿ
ğ‘˜âˆ’Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)â‰¤ğœ€ğ‘Ÿ
ğ‘˜(ğ‘¡)(and Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)âˆ’ğœ‡ğ‘
ğ‘˜â‰¤ğœ€ğ‘
ğ‘˜(ğ‘¡)). This
allows us to rewrite the terms in Eq. (19) as follows:
Pr
ğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜+(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜
=Pr
ğœ‡ğ‘Ÿ
ğ‘˜+(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜âˆ’Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)â‰¤ğœ€ğ‘Ÿ
ğ‘˜(ğ‘¡)
=PrË†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)âˆ’ğœ‡ğ‘Ÿ
ğ‘˜â‰¥(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜âˆ’ğœ€ğ‘Ÿ
ğ‘˜(ğ‘¡)
(20)
Pr
ğœ”ğ‘
ğ‘˜âˆ’(ğ‘¡)â‰¤ğœ‡ğ‘
ğ‘˜âˆ’ğœ‡ğ‘
ğ‘˜ğ›¿ğ‘˜
=PrË†ğœ‡ğ‘
ğ‘˜(ğ‘¡)âˆ’(ğœ‡ğ‘
ğ‘˜âˆ’ğœ‡ğ‘
ğ‘˜ğ›¿ğ‘˜)â‰¤ğœ€ğ‘
ğ‘˜(ğ‘¡)
=Pr
ğœ‡ğ‘
ğ‘˜âˆ’Ë†ğœ‡ğ‘
ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘
ğ‘˜ğ›¿ğ‘˜âˆ’ğœ€ğ‘
ğ‘˜(ğ‘¡)
(21)
In the next two paragraphs, we evaluate Eq. (20)(confidence
bound of rewards) and Eq. (21)(confidence bound of costs). We
combine both results afterwards.
Evaluation of Eq. (20)forğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—,ğ‘Ÿ
ğ‘˜(ğœ).We now consider the
cases in which the number of times arm ğ‘˜was played is at least
logarithmic w.r.t. ğœ, i.e.,ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—,ğ‘Ÿ
ğ‘˜(ğœ)with
ğ‘›âˆ—,ğ‘Ÿ
ğ‘˜(ğœ)=2ğœŒlogğœ
ğ›¿2
ğ‘˜(1âˆ’ğœ…)2ğœ‚ğ‘Ÿ
ğ‘˜ğœ‡ğ‘Ÿ
ğ‘˜
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜,for anyğœ…âˆˆ(0,1).
In those cases, ğœ€ğ‘Ÿ
ğ‘˜(ğ‘¡)â‰¤( 1âˆ’ğœ…)ğ›¿ğ‘˜(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜). One can verify this by
insertingğ‘›âˆ—,ğ‘Ÿ
ğ‘˜(ğœ)in the definition of ğœ€ğ‘Ÿ
ğ‘˜(ğ‘¡), cf. Eq. (17). This gives
the following inequality for the right side of Eq. (20):
Pr
ğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜+(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜
â‰¤PrË†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)âˆ’ğœ‡ğ‘Ÿ
ğ‘˜â‰¥ğœ…(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜
Last, we compute the number of standard deviations ğ‘§âˆ—that
corresponds to a deviation between Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)andğœ‡ğ‘Ÿ
ğ‘˜of at maximum
ğœ…(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜based on Eq. (16). In particular, we solve the right-most
inequality in the expression below:
(Ë†ğœ‡ğ‘Ÿ
ğ‘˜(ğ‘¡)âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)2â‰¤ğœ‚ğ‘Ÿ
ğ‘˜ğœ‡ğ‘Ÿ
ğ‘˜(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)
ğ‘›ğ‘˜(ğ‘¡)ğ‘§âˆ—2
â‰¤ğœ‚ğ‘Ÿ
ğ‘˜ğœ‡ğ‘Ÿ
ğ‘˜(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)
ğ‘›âˆ—,ğ‘Ÿ
ğ‘˜(ğœ)ğ‘§âˆ—2â‰¤(ğœ…(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜)2
With our choice of ğ‘›âˆ—,ğ‘Ÿ
ğ‘˜(ğœ), this yieldsğ‘§âˆ—= 2ğœŒlogğœğœ…2(1âˆ’ğœ…)âˆ’21
2.
Insertingğ‘§âˆ—in Eq. (8)(upper bound for ğ›¼(ğ‘¡)) results in the following
bound:
Pr
ğœ”ğ‘Ÿ
ğ‘˜+(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
ğ‘˜+(1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜)ğ›¿ğ‘˜
<1
2Â©Â­
Â«1âˆ’âˆšï¸‚
1âˆ’ğœâˆ’ğœ…2ğœŒ
(1âˆ’ğœ…)2ÂªÂ®
Â¬(22)
Evaluation of Eq. (21) forğ‘›ğ‘˜(ğ‘¡)>ğ‘›âˆ—,ğ‘
ğ‘˜(ğœ).Again, we consider
the cases in which the number of times arm ğ‘˜was played is at least
logarithmic in ğœ, i.e.,ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—,ğ‘
ğ‘˜(ğœ)with
ğ‘›âˆ—,ğ‘
ğ‘˜(ğœ)=2ğœŒlogğœ
ğ›¿2
ğ‘˜(1âˆ’ğœ…)2ğœ‚ğ‘
ğ‘˜(1âˆ’ğœ‡ğ‘
ğ‘˜)
ğœ‡ğ‘
ğ‘˜, ğœ…âˆˆ(0,1).
In those cases, ğœ€ğ‘
ğ‘˜(ğ‘¡)â‰¤( 1âˆ’ğœ…)ğ›¿ğ‘˜ğœ‡ğ‘
ğ‘˜. Following analogous steps
as in the previous paragraph yields Eq. (23):
Pr
ğœ”ğ‘
ğ‘˜âˆ’(ğ‘¡)â‰¤ğœ‡ğ‘
ğ‘˜âˆ’ğœ‡ğ‘
ğ‘˜ğ›¿ğ‘˜
<1
2 
1âˆ’âˆšï¸ƒ
1âˆ’ğœâˆ’(1âˆ’ğœ…)2ğœŒ
ğœ…2!
(23)Obtaining the final result. With the results in Eq. (22)and Eq. (23)
we can finally evaluate Eq. (18). A choice of ğœ…=0.5and
ğ‘›âˆ—
ğ‘˜(ğœ)=8ğœŒlogğœ
ğ›¿2
ğ‘˜max(
ğœ‚ğ‘Ÿ
ğ‘˜ğœ‡ğ‘Ÿ
ğ‘˜
1âˆ’ğœ‡ğ‘Ÿ
ğ‘˜,ğœ‚ğ‘
ğ‘˜(1âˆ’ğœ‡ğ‘
ğ‘˜)
ğœ‡ğ‘
ğ‘˜)
yields the bound given in Lemma 1:
Pr
Î©ğ‘˜(ğ‘¡)â‰¥ğœ‡ğ‘Ÿ
1
ğœ‡ğ‘
1
<1âˆ’âˆš
1âˆ’ğœâˆ’ğœŒ, ğ‘›ğ‘˜(ğ‘¡)â‰¥ğ‘›âˆ—
ğ‘˜(ğœ)
B.4 Proof of Theorem 6
First note that the latter two terms and ğ‘›âˆ—
ğ‘˜(ğœğµ)in Eq. (10)are in
O(logğµ)[24]. Next we show that ğœ‰(ğœğµ,ğœŒ)is inO(logğµ)forğœŒâ‰¥1
and inO(ğµ1âˆ’ğœŒ)for0<ğœŒ<1.
We exploit two inequalities in our proof; the latter is based on
the integral test for convergence and holds for continuous, positive,
decreasing functions.
âˆš
1âˆ’ğ‘¡âˆ’ğœŒâ‰¥1âˆ’ğ‘¡âˆ’ğœŒ, ğ‘¡â‰¥1,ğœŒ>0 (24)
ğœğµâˆ‘ï¸
ğ‘¡=ğ¾+1ğ‘¡âˆ’ğœŒâ‰¤(ğ¾+1)âˆ’ğœŒ+âˆ«ğœğµ
ğ‘¡=ğ¾+1ğ‘¡âˆ’ğœŒğ‘‘ğ‘¡ (25)
We use Eq. (24)to obtain an integrable expression for the sum in
ğœ‰(ğœğµ,ğœŒ). We replace the sum with an integral-based upper bound
as in Eq. (25):
ğœ‰(ğœğµ,ğœŒ)=(ğœğµâˆ’ğ¾)
2âˆ’âˆšï¸ƒ
1âˆ’ğœâˆ’ğœŒ
ğµ
âˆ’ğœğµâˆ‘ï¸
ğ‘¡=ğ¾+1âˆš
1âˆ’ğ‘¡âˆ’ğœŒ
â‰¤(ğœğµâˆ’ğ¾)
2âˆ’âˆšï¸ƒ
1âˆ’ğœâˆ’ğœŒ
ğµ
âˆ’ğœğµâˆ‘ï¸
ğ‘¡=ğ¾+11âˆ’ğ‘¡âˆ’ğœŒ
=(ğœğµâˆ’ğ¾)
1âˆ’âˆšï¸ƒ
1âˆ’ğœâˆ’ğœŒ
ğµ
+ğœğµâˆ‘ï¸
ğ‘¡=ğ¾+1ğ‘¡âˆ’ğœŒ
â‰¤(ğœğµâˆ’ğ¾)
1âˆ’âˆšï¸ƒ
1âˆ’ğœâˆ’ğœŒ
ğµ
+(ğ¾+1)âˆ’ğœŒ+âˆ«ğœğµ
ğ‘¡=ğ¾+1ğ‘¡âˆ’ğœŒğ‘‘ğ‘¡
Next, we evaluate the integral for the cases ğœŒ=1andğœŒâ‰ 1.
Case 1:ğœŒ=1.
ğœ‰(ğœğµ,ğœŒ)â‰¤(ğœğµâˆ’ğ¾)
1âˆ’âˆšï¸ƒ
1âˆ’ğœâˆ’1
ğµ
+(ğ¾+1)âˆ’1+logğœğµâˆ’log(ğ¾+1)
ForğœŒ=1, the first term in above equation converges, so ğœ‰(ğœğµ,ğœŒ=
1)is inO(logğµ). This implies that the overall regret of ğœ”-UCB is
inO(logğµ)forğœŒ=1.
Case 2:ğœŒâ‰ 1.
ğœ‰(ğœğµ,ğœŒ)â‰¤(ğœğµâˆ’ğ¾)
1âˆ’âˆšï¸ƒ
1âˆ’ğœâˆ’ğœŒ
ğµ
+(ğ¾+1)âˆ’ğœŒ+1
1âˆ’ğœŒ
ğœ1âˆ’ğœŒ
ğµâˆ’(ğ¾+1)1âˆ’ğœŒ
ForğœŒ>1,ğœ‰(ğœğµ,ğœŒ)converges and thus the overall regret is in
O(logğµ). For 0<ğœŒ<1, one can show that ğœ‰(ğœğµ,ğœŒ)is inO(ğµ1âˆ’ğœŒ).
Hence, the regret of ğœ”-UCB is inO(ğµ1âˆ’ğœŒ)for0<ğœŒ<1.
To summarize, the regret of our policy is in O(ğµ1âˆ’ğœŒ)for0<
ğœŒ<1and inO(logğµ)forğœŒâ‰¥1.
 
1084