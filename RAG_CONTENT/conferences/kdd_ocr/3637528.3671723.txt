Low Rank Multi-Dictionary Selection at Scale
Boya Ma
University at Albany, State University of New York
Department of Computer Science
Albany, New York, USA
bma@albany.eduMaxwell McNeil
University at Albany, State University of New York
Department of Computer Science
Albany, New York, USA
mmcneil2@albany.edu
Abram Magner
University at Albany, State University of New York
Department of Computer Science
Albany, New York, USA
amagner@albany.eduPetko Bogdanov
University at Albany, State University of New York
Department of Computer Science
Albany, New York, USA
pbogdanov@albany.edu
Abstract
The sparse dictionary coding framework represents signals as a
linear combination of a few predefined dictionary atoms. It has been
employed for images, time series, graph signals and recently for
2-way (or 2D) spatio-temporal data employing jointly temporal and
spatial dictionaries. Large and over-complete dictionaries enable
high-quality models, but also pose scalability challenges which are
exacerbated in multi-dictionary settings. Hence, an important prob-
lem that we address in this paper is: How to scale multi-dictionary
coding for large dictionaries and datasets?
We propose a multi-dictionary atom selection technique for
low-rank sparse coding named LRMDS. To enable scalability to
large dictionaries and datasets, it progressively selects groups of
row-column atom pairs based on their alignment with the data
and performs convex relaxation coding via the corresponding sub-
dictionaries. We demonstrate both theoretically and experimentally
that when the data has a low-rank encoding with a sparse subset
of the atoms, LRMDS is able to select them with strong guaran-
tees under mild assumptions. Furthermore, we demonstrate the
scalability and quality of LRMDS in both synthetic and real-world
datasets and for a range of coding dictionaries. It achieves 3Ã—to10Ã—
speed-up compared to baselines, while obtaining up to two orders
of magnitude improvement in representation quality on some of
the real world datasets given a fixed target number of atoms.
CCS Concepts
â€¢Information systems â†’Data mining.
Keywords
sparse coding, dictionary selection, low rank methods
ACM Reference Format:
Boya Ma, Maxwell McNeil, Abram Magner, and Petko Bogdanov. 2024. Low
Rank Multi-Dictionary Selection at Scale. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671723August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671723
1 Introduction
Sparse coding methods represent data as a linear combination of a
predefined basis (called atoms) arranged in a dictionary [ 26]. Dictio-
naries are either derived analytically, for example, discrete Fourier
transform, Wavelets, Ramanujan periodic basis [ 28] or learned from
data [ 29]. A key assumption in sparse coding is that real-world sig-
nals are sparse (or compressive) and can be represented via a small
subset of dictionary atoms. Sparse coding has been widely adopted
in signal processing [ 26], machine learning [ 18],time series analy-
sis [15,34], image processing [ 7], data mining [ 20,21] and computer
vision [31] among others.
Existing approaches, depending on their sparsity-promoting
functions, fall in three main categories [ 17]: convex relaxation [ 4],
non-convex algorithms [ 12], and greedy strategies based on match-
ing pursuit [ 23]. Most existing work focuses on 1D (vector) sig-
nals such as time series [ 28] and graph signals [ 6]. More recent
approaches employ sparse coding for multi-way datasets such as im-
ages [ 8,10], spatio-temporal data [ 22] and higher order tensors [ 19].
These 2D and higher-D methods employ separate dictionaries for
the different modes (dimensions) of the data. Convex relaxation
2D approaches are typically efficient in practice, but their runtime
significantly increases with the size of dictionaries and datasets
and they also require careful tuning of hyper-parameters to pre-
cisely control the density of encoding coefficients [ 22]. Greedy ap-
proaches recover a desired number of coefficients (model size), but
re-estimate all coding coefficients as new ones are added, and thus
do not scale to large model sizes [ 8]. Fig. 1(a) demonstrates the 2D
setting in the context of user-product purchase data by employing a
separate user and product graph dictionaries (e.g., graph Fourier dic-
tionaries [ 27]) based on the corresponding friendship/association
graphs.
Some multi-way techniques further model the coding matrix
as low-rank [ 19,22,24] which enables improved performance due
to sharing of atom-specific patterns within factors. This model-
ing assumption is demonstrated in the toy example from Fig. 1(a).
Given the purchase history of users, represented as one-hot en-
coded icons, and association graphs (e.g., user-user friendship and
product similarity graphs), one can define graph-based user Î¨and
product Î¦dictionaries which represent natural communities for
 
2106
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boya Ma, Maxwell McNeil, Abram Magner, & Petko Bogdanov
ğ—ğš¿ğ˜ğ–ğš½Sub-selectDictionaries
(a) 2D low rank coding model for user-product data
101102103
Time101102RMSETGSD(1k)
TGSD(2k)
2D-OMP(1k)
2D-OMP(2k)
SC-TGSD(1k)SC-TGSD(2k)
LRMDS(1k)
LRMDS(2k)
LRMDS-f(1k)
LRMDS-f(2k)Our methods
(b) RMSE v.s. time in road traffic data
Figure 1: (a) A 2D low rank coding example for user-product preference
data. The left Î¨and right Î¦dictionaries are derived from user and product
association graphs and the goal is to encode the data sparsely via sparse and
low-rank coefficient matrices ğ‘Œ,ğ‘Š . Our method LRMDS sub-selects the dic-
tionary atoms on both sides to speed up the coding process. (b) Comparison
of competing techniques on a Road traffic dataset. Variants of LRMDS outper-
form all baselines in both representation quality (RMSE) and running time
(best regime in the lower-left corner).
each of the two data dimensions. If purchase behaviors â€œconformâ€
to user and product communities (e.g., first three users purchase
electronics while the remaining two purchase sports products),
then the purchase data can be described efficiently via a few sparse
factors as demonstrated in the ğ‘Œ,ğ‘Š coding matrices. For example,
the user factors in ğ‘Œrequire only two coefficients to represent the
corresponding user groups, and similarly ğ‘Šrepresent groups of
products via three coefficients.
While (low-rank) 2D sparse coding is advantageous and widely
applicable, existing methods do not scale to large dictionaries and
data. The 2D-OMP approach [ 8] composes atoms as outer products
of left and right atoms and greedily selects the best aligning pairs for
encoding one at a time. Low rank approaches [ 19,22] adopt convex
relaxation based on alternating directions methods of multipliers
(ADMM). Both groups suffer from poor scalability with the size of
the employed dictionaries. 2D-OMP considers a quadratic number
of atom combinations while low-rank approaches rely on inversion
of matrices whose size depend on that of the employed dictionaries.
While this challenge of limited scalability has been addressed in the
1D sparse coding scenario via dictionary screening [ 32] and greedy
dictionary selection [ 9], these methods are not readily applicable to
the 2D scenario. Furthermore, as we demonstrate experimentally,
naive generalizations of dictionary screening algorithms for the 2D
setting result in limited representation accuracy and scalability.
We propose a low-rank multi-dictionary selection and coding ap-
proach for 2D data called LRMDS. Our approach is general, scalable
and theoretically justified. To scale to large dictionary sizes, it itera-
tively performs adaptive joint dictionary sub-selection and efficient
low-rank coding based on convex optimization. LRMDS iteratively
improves the encoding by adding dictionary atoms as needed in
rounds. We prove theoretically and demonstrate empirically that ifthe input data conforms to a low-rank coding model via a sparse
subset of atoms, LRMDS is guaranteed to select these atoms in noisy
regimes. An experimental snapshot showcasing the advantages of
LRMDSâ€™s variants is presented in Fig. 1(b) for a real-world sensor
network dataset. In terms of representation error (vertical axis) and
running time (horizontal axis), variants of our method (LRMDS and
LRMDS-f) occupy the lower left corner which is the optimal regime.
We experimentally demonstrate similar advantageous behavior on 3
other datasets detailed in the evaluation section. While in this work
we focus on evaluating dictionary selection in 2-way (matrix) data,
we believe that our framework can be extended to multi-dictionary
settings (tensor data) , though we leave such evaluation for future
investigation. Our contributions in this paper are as follows:
â€¢Novelty: To the best of our knowledge, LRMDS is the first dictio-
nary selection method for multi-dictionary sparse coding.
â€¢Scalability: Our approach scales better than alternatives on large
real-world datasets and when employing large dictionaries.
â€¢Accuracy: LRMDS consistently produces solutions of lower rep-
resentational error compared to the closest baselines from the liter-
ature for a fixed number of coding coefficients (models size).
â€¢Theoretical guarantees: We prove that LRMDSâ€™s dictionary
selection optimally identifies the necessary atoms for low-rank
encoding of the input data in the presence of noise.
2 Related work
Sparse coding is widely employed in signal processing [ 26,35],
image analysis [ 7] and computer vision [ 31]. Existing methods
can be grouped into three main categories: convex optimization
solutions, non-convex techniques, and greedy algorithms [ 17]. Re-
laxation techniques impose sparsity on the coding coefficients via
L1 regularizers [ 19,22], while greedy algorithms select one atom at
a time [ 5,14,30]. Most existing methods focus on 1D signals while
our focus in this paper is on 2D signals.
2D and multi-way coding methods generalize the one dimen-
sional setting by employing separate dictionaries for each dimen-
sion of the data [ 8,10,19,22,33]. Some methods in this group
place no assumptions on the rank of the encoding matrix [ 8,10,
24,33], while others employ a low-rank model for the encoding
matrix [ 19,22]. Most related to LRMDS among above the above are
2D-OMP [ 8] which also utilizes a greedy projection to select atoms,
and TGSD [ 22] as it also enforces that learned coding coefficients
are low rank for 2D data. We elaborate further on these similarities
in the following section and demonstrate a superior performance
of LRMDS over these baselines in the experimental section.
Dictionary screening and selection. Dictionary screening [ 32]
is a suite of methods/bounds for â€œdiscardingâ€ dictionary atoms of 0
encoding weights at a given sparsity level with the goal of reducing
the running time of the encoding process. These techniques are lim-
ited to 1D data (i.e., only one dictionary), and are not immediately
extendable to the two-dictionary setting. We include naive exten-
sions to 2D by creating composite (pairwise) atoms as baselines
and demonstrate that they do not scale well to large-dictionaries
due to the quadratic space of possible composite atoms. There is
also work on greedy atom selection for the 1D case [ 9] and our
method can be viewed as a generalization of such techniques to the
2D sparse coding setting.
 
2107Low Rank Multi-Dictionary Selection at Scale KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
3 Preliminaries
Before we define our problem of low rank sub-dictionary selection,
we introduce necessary preliminaries and notation. The goal of
1D sparse coding is to represent a signal via a single (column)
dictionary Î¨âˆˆRğ‘Ã—ğ¼optimizing the following objective:
minğ‘¦ğ‘“(ğ‘¦)s.t.ğ‘¥=Î¨ğ‘¦,
whereğ‘¥âˆˆRğ‘represents the given signal, ğ‘¦âˆˆRğ¼is the learned
encoding and ğ‘“(ğ‘¦)is a sparsity promoting function (often the ğ¿1
norm). A popular greedy strategy to solve the problem, particularly
when the dictionary forms an over-complete basis, is the orthogonal
matching pursuit (OMP) [ 23]. The OMP algorithm maintains a
residual of the signal ğ‘Ÿthat is not yet represented, and proceeds
in greedy steps to identify the dictionary atom best aligned to the
residual:ğœ“ğ‘¡=argmaxğœ“ğ‘–(ğ‘Ÿğ‘‡ğœ“ğ‘–), whereğ‘–âˆˆ[1,ğ¼], andğœ“ğ‘–is theğ‘–-th
atom in Î¨. The selected atom ğœ“ğ‘¡at stepğ‘¡is appended to the result
set, the signal is re-encoded and the residual re-computed. The
process continues until a desired number of atoms are employed,
while satisfying the sparsity function ğ‘“(ğ‘¦).
In this paper we consider the 2D setting involving two dictionar-
ies. The input to our problem is a real valued data matrix ğ‘‹âˆˆRğ‘Ã—ğ‘€
which can be represented via two dictionaries: a left (column) dic-
tionary Î¨âˆˆRğ‘Ã—ğ¼and a right (row) dictionary Î¦ğ‘‡âˆˆRğ½Ã—ğ‘€, where
ğ¼is the number of atoms in Î¨andğ½is the number of atoms in
Î¦ğ‘‡. It is important to note that both analytical and data-driven
dictionaries can be employed [ 26]. The 2D problem generalizes that
from the 1D case as follows:
min
ğ‘ğ‘“(ğ‘)s.t.ğ‘‹=Î¨ğ‘Î¦ğ‘‡, (1)
whereğ‘âˆˆRğ¼Ã—ğ½is an encoding matrix, and ğ‘“(ğ‘)is the correspond-
ing sparsity promoting function. Intuitively this decomposition
facilitates a representation which aligns to dictionaries across both
modes (dimensions) instead of just one. An early solution for the
problem in Eq. 1 was motivated by decomposing a 2D image via
copies of the same dictionary, i.e. Î¨=Î¦[8]. It generalizes OMP
to obtain a 2D-OMP algorithm by forming 2D atoms ğµğ‘–,ğ‘—=ğœ“ğ‘‡
ğ‘–ğœ™ğ‘—
as outer products of individual left ğœ“ğ‘–and rightğœ™ğ‘—atoms, and by
selecting 2D atoms based on their alignment with the residual ğ‘…at
every iteration. Importantly, while sparse, this solution might in
general result in high-rank encoding matrix ğ‘and as we demon-
strate experimentally it does not scale to large spatio-temporal
datasets and large dictionaries.
A recent method called TGSD [ 22] employs 2D sparse coding for
general spatio-temporal datasets, and specifically temporal graph
signals, where graph and temporal dictionaries are employed as
Î¨andÎ¦respectively. Another major difference from the 2D-OMP
solution is that in order to enforce a low-rank solution, TGSD
considers a model with two â€œslimâ€ dictionary-specific encoding
matricesğ‘ŒâˆˆRğ¼Ã—ğ‘Ÿandğ‘ŠâˆˆRğ‘ŸÃ—ğ½, where the middle dimension
ğ‘Ÿrestricts the rank of the encoding ğ‘‹=Î¨ğ‘Œğ‘ŠÎ¦ğ‘‡. The resulting
objective is:
argmin
ğ‘Œ,ğ‘Š||ğ‘‹âˆ’Î¨ğ‘Œğ‘ŠÎ¦ğ‘‡||2
ğ¹+ğœ†1||ğ‘Œ||1+ğœ†2||ğ‘Š||1,(2)
where sparsity via an ğ¿1norm is enforced for both ğ‘Œandğ‘Š. Here,
âˆ¥ğ‘€âˆ¥ğ¹denotes the Frobenius norm of the matrix ğ‘€. The solutionadopts an ADMM convex relaxation approach (unlike the greedy so-
lution of 2D-OMP) producing an explainable decomposition model
relating non-zero coefficients to periodic behavior and active spa-
tial/graph domains in the data [ 22]. However, this solution also does
not scale to large coding dictionaries and datasetsâ€”a challenge we
address in our solution.
4 Problem formulation and solutions
4.1 Problem formulation
The existing methods for multi-dictionary sparse coding, TGSD and
2D-OMP, do not scale to large datasets and dictionaries for different
reasons. 2D-OMP selects one atom pair from each dictionary at
a time, re-encodes the data and proceeds with the data residual.
While each step is initially fast, the number of atom pairs grows
quadratically with the size of the dictionaries. In addition, when
the data has a low-rank representation through a subset of atoms,
2D-OMP is not guaranteed to uncover it due to its formulation
employing an unconstrained coding matrix ğ‘of quadratic size in
the dictionary atoms. Different from that, TGSD is a low rank model,
however, its optimization relies on inverting matrices whose sizes
are determined by the dictionaries, hence it also does not scale with
the size of the dictionaries. The scalability limitations are further
exacerbated by the use of over-complete dictionaries which has
been shown to produce accurate and succinct models in various
signal processing and machine learning applications.
Our goal is to enable a (i) scalable, (ii) low-rank, (iii) multi-
dictionary sparse coding, accommodating large over-complete dic-
tionaries without compromising the quality of the learned model by
subs-electing the dictionary atoms. An additional goal is applicabil-
ity to any 2D signals, including spatio-temporal data, graph signals
evolving over time, images and others, by employing appropriate
dictionaries for the corresponding data dimensions. Based on the
above intuition we formalize our problem as follows:
Problem definition: Given a 2D signal ğ‘‹, large potentially over-
complete dictionaries Î¨andÎ¦, and a desired rank ğ‘Ÿ, fit a sparse
low-rank model ğ‘‹â‰ˆÎ¨ğ‘ ğ‘Œğ‘ŠÎ¦ğ‘‡ğ‘ , employing a subset of the dictionary
atoms Î¨ğ‘ ,Î¦ğ‘ and coding matrices of ğ‘Œ,ğ‘Š with inner dimension ğ‘Ÿ.
4.2 LRMDS: iterative atom selection and coding
Both TGSD and 2D-OMP with minor modifications can be adopted
for our problem formulation, however, as we discussed earlier they
have limited scalability. The key idea behind our approach is to
sub-select both dictionaries jointly and fit a low-rank encoding
model through the reduced dictionaries. Hence, a key assumption
in LRMDS is that the data can be represented well by a low-rank
encoding matrix and by employing a subset of atoms from the left and
the right dictionaries . This setting is illustrated in Fig. 1(a) where
only a subset of atoms from Î¨andÎ¦are necessary to represent the
data. There are two main steps to obtain LRMDSâ€™s representation:
(i) identify an appropriate subset of dictionary atoms which align
well with the data and (ii) employ them to perform a low rank dic-
tionary decomposition. We repeatedly perform these steps against
the â€œunexplainedâ€ residual of the data after each iteration. As a
result, our approach can be considered a combination of a greedy
sub-dictionary identification followed by a convex encoding step.
Importantly, we demonstrate that the greedy atom selection step
 
2108KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boya Ma, Maxwell McNeil, Abram Magner, & Petko Bogdanov
recovers the optimal atoms to best encode a dataset with low-rank
and sparse encoding in noisy regimes under mild assumptions.
To jointly sub-select atoms from both dictionaries, we consider
all pairwise 2D atoms of the form ğµğ‘–,ğ‘—=ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—,âˆ€ğ‘–âˆˆ[1,ğ¼],âˆ€ğ‘—âˆˆ[1,ğ½]
and the magnitude of the projection of the data on them. Specifically
we maintain a residual matrix ğ‘…âˆˆRğ‘Ã—ğ‘€initialized as the input
datağ‘‹and subsequently capturing the signal not yet represented
by LRMDS. The alignment scores of atom pairs are computed as:
ğ‘ƒğ‘–,ğ‘—=âŸ¨ğ‘…,ğµğ‘–,ğ‘—âŸ©
||ğµğ‘–,ğ‘—||ğ¹,=â‡’ğ‘ƒ=Ë†Î¨ğ‘‡ğ‘…Ë†Î¦, (3)
where on the left-hand-side âŸ¨ğ‘…,ğµğ‘–,ğ‘—âŸ©â‰œğœ“ğ‘‡
ğ‘–ğ‘…ğœ™ğ‘—is the alignment
of theğ‘–-th left atom and the ğ‘—âˆ’ğ‘¡â„right atom with the residual,
and||ğµğ‘–,ğ‘—||ğ¹â‰œğœ“ğ‘–ğœ™ğ‘‡
ğ‘—is a normalization factor based on the Frobe-
nius norm of the atomsâ€™ outer product. The right-hand-side is the
equivalent to the left for all ğ‘ƒğ‘–,ğ‘—when using per-atom normalized
dictionaries Ë†Î¨and Ë†Î¦(details in the supplement). At each iteration,
our method selects the top ğ‘˜total atoms from a combination of left
or right dictionary atoms with respect to this alignment ğ‘ƒ.
Once atoms are selected we calculate a low-rank decomposition
of the data via encoding matrices ğ‘ŒâˆˆRğ¼ğ‘ Ã—ğ‘Ÿandğ‘ŠâˆˆRğ‘ŸÃ—ğ½ğ‘ where
ğ‘Ÿrepresents the rank of the model, while ğ¼ğ‘ andğ½ğ‘ are the number
of atoms selected from Î¨andÎ¦respectively. It is important to note,
that the sparsity of the representation is ensured thanks to the atom
sub-selection and the low-rank (via two encoding matrices) model,
hence in this step we do not further enforce sparsity on the encoding
coefficients as it is typical to convex relaxation approaches. This
modeling decision enables scalable direct solutions as opposed to
more complicated ADMM optimizers. The encoding problems has
the following form:
argmin
ğ‘Œ,ğ‘Š||ğ‘…âˆ’Î¨ğ‘ ğ‘Œğ‘ŠÎ¦ğ‘‡
ğ‘ ||2
ğ¹,(4)
where Î¨ğ‘ andÎ¦ğ‘ are the subselected dictionaries and ğ‘…is the data
residual originally initialized as ğ‘‹. We propose two alternating
optimization schemes for the two variables ğ‘Œ,ğ‘Š leading to two
variants of LRMDS (LRMDS and LRMDS-f). Both variants iteratively
updatedğ‘Œandğ‘Šuntil convergence, however, LRMDS-f does so
faster but at the potential price of accuracy. Detailed explanation
and derivations are available in the supplement.
The overall LRMDS algorithm. The steps of the complete algo-
rithm (corresponding to both versions of our method) are listed in
Alg. 1. The inputs include the data ğ‘‹, leftÎ¨and right Î¦dictionaries
and parameters for the number of atoms to select per iteration ğ‘˜
and the rank ğ‘Ÿof the encoding, i.e., the inner dimension of the two
output encoding matrices ğ‘Œ,ğ‘Š . In the initialization steps, we first
compute per-atom normalized versions of the dictionaries needed
for the alignment scoring (Steps 4-5) and initialize empty sets of
atom indices for both dictionaries (Step 6). Dictionary sub-selection
takes place in Steps 8-15. In Steps 10-14 we add the top aligned
atomsğ‘–andğ‘—to the set of select atoms ğ¼ğ‘ andğ½ğ‘ so long as they
are not already selected. We repeat until a total of ğ‘˜new atoms
are selected. Finally, we sub-select the relevant atoms from their
dictionaries in Step 15 to create sub-dictionaries Î¨ğ‘ andÎ¦ğ‘ .
Steps 16-30 perform the low-rank coding by estimating ğ‘Œandğ‘Š
based on the sub-dictionaries Î¨ğ‘ andÎ¦ğ‘ . We list the iterative updates
of both versions LRMDS (Steps 18-23) and LRMDS-f (Steps 24-29) ofAlgorithm 1 Low Rank Multi-Dictionary Selection (LRMDS)
1:Input: Datağ‘‹; dictionaries Î¨andÎ¦; atoms per iteration ğ‘˜, decomposition rank ğ‘Ÿ
2: Output: Encoding matrices ğ‘Œ,ğ‘Š
3: Initialize residual ğ‘…=ğ‘‹
4: // Compute normalized dictionaries
5: Compute Ë†Î¨:{||Ë†ğœ“ğ‘–||2=1,âˆ€ğ‘–â‰¤ğ¼},Ë†Î¦:{||Ë†ğœ™ğ‘—||2=1,âˆ€ğ‘—â‰¤ğ½}
6: Initialize sets of selected atoms: ğ¼ğ‘ =âˆ…,ğ½ğ‘ =âˆ…
7:repeat
8: // Dictionary sub-selection
9: Letğ‘ƒ=Ë†Î¨ğ‘‡ğ‘…Ë†Î¦ âŠ²Eq. 3
10:ğ‘ğ‘›ğ‘¡=0
11: forğ‘ƒğ‘–,ğ‘—in descending order and while ğ‘ğ‘›ğ‘¡<ğ‘˜do
12: ifğ‘–âˆ‰ğ¼ğ‘ thenğ¼ğ‘ =ğ¼ğ‘ âˆªğ‘–;ğ‘ğ‘›ğ‘¡=ğ‘ğ‘›ğ‘¡+1
13: ifğ‘—âˆ‰ğ½ğ‘ thenğ½ğ‘ =ğ½ğ‘ âˆªğ‘—;ğ‘ğ‘›ğ‘¡=ğ‘ğ‘›ğ‘¡+1
14: end for
15: Sub-select dictionaries: Î¨ğ‘ =Î¨(ğ¼ğ‘ ),Î¦ğ‘ =Î¦(ğ½ğ‘ )
16: // Encoding based on Î¨ğ‘ ,Î¦ğ‘ 
17: Initialize randomly ğ‘Œ|ğ¼ğ‘ |Ã—ğ‘Ÿandğ‘Šğ‘ŸÃ—|ğ½ğ‘ |
18: ifLRMDS then
19: Pre-compute Î¨(ğ‘–ğ‘›ğ‘£)
ğ‘ =Î¨â€ 
ğ‘ andÎ¦(ğ‘–ğ‘›ğ‘£)
ğ‘ =Î¦â€ 
ğ‘ 
20: repeat
21: ğ‘Œ=Î¨(ğ‘–ğ‘›ğ‘£)
ğ‘ ğ‘‹(ğ‘ŠÎ¦ğ‘ )â€ 
22: ğ‘Š=(Î¨ğ‘ ğ‘Œ)â€ ğ‘‹Î¦(ğ‘–ğ‘›ğ‘£)
ğ‘ 
23: untilğ‘Œ,ğ‘Š converge
24: else if LRMDS-f then
25: Pre-compute ğ¶=Î¨â€ ğ‘‹Î¦â€ 
26: repeat
27: ğ‘Œ=ğ¶ğ‘Šâ€ 
28: ğ‘Š=ğ‘Œâ€ ğ¶
29: untilğ‘Œ,ğ‘Š converge
30: end if
31:ğ‘…=ğ‘‹âˆ’Î¨ğ‘ ğ‘Œğ‘ŠÎ¦ğ‘‡
ğ‘ 
32:until||ğ‘…||ğ¹converges to 0or after a fixed number of iterations
our method. For the former, we pre-compute the pseudo inverses of
the subselected dictionaries Î¨â€ ,Î¦â€ and iterate between close form
updates ofğ‘Œandğ‘Šwhile in the latter we precompute the projection
of the data on the pseudo inverses of the subselected dictionaries
Î¨â€ ğ‘‹Î¦â€ and perform simpler updates for the coefficients in ğ‘Œandğ‘Š.
Further discussion of the difference between the two versions and
derivation details for the updates are available in the supplement.
Finally, in Step 31 we re-calculate the residual matrix ğ‘…. We repeat
all steps until a fixed number of iterations is reached or if the norm
of the residual (||ğ‘…||ğ¹) approaches zero.
The overall complexity of LRMDS is ğ‘‚(ğ‘¡(ğ‘(ğ‘+ğ‘€)ğ‘Ÿ2+ğ‘€ğ½2ğ‘ +
ğ‘ğ¼2ğ‘ ))for LRMDS or ğ‘‚(ğ‘¡(ğ‘(ğ¼ğ‘ +ğ½ğ‘ )ğ‘Ÿ2+ğ‘€ğ½2ğ‘ +ğ‘ğ¼2ğ‘ ))when using
LRMDS-f where ğ‘¡,ğ‘are the total number of iterations of the main
loop andğ‘Œ,ğ‘Š updating. Our framework is designed to flexibly
accommodate any dictionaries Î¦andÎ¨. Discussion of dictionaries
employed for modes of different types (e.g., time series, graphs, etc.)
can be found in the extended version [16].
4.3 Dictionary subselection theoretical analysis
In this section, we give an accuracy guarantee for the quality of
our methodâ€™s selection of top ğ‘˜atoms in each step. when used to
recover a low-rank signal matrix ğ‘…from a data matrix Ë†ğ‘…=ğ‘…+ğ‘„
that includes Gaussian noise. Throughout, we assume without loss
of generality that ğ‘€=ğ‘œ(âˆš
ğ‘)asğ‘â†’âˆ (the argument applies
equally well when the roles of ğ‘€andğ‘are switched). We describe
this denoising problem setting and assumptions for our theoretical
guarantee below.
Noise model: We will consider the recovery of a low-rank, sparse
signal matrix ğ‘…from a noise-perturbed version Ë†ğ‘…:=ğ‘…+ğ‘„, whereğ‘„
 
2109Low Rank Multi-Dictionary Selection at Scale KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Dataset #Nodes#TimeRes.Associated TGSD 2D-OMP SC-TGSD LRMDS LRMDS-f
Steps Graph RMSE Time RMSE Time RMSE Time RMSE Time RMSE Time
Synthetic 1k-4k 1k-8k - SBM 0.06 146 0.02 3196 0.03 61.7 0.009 31.3 0.009 30.1
Road [2] 1923 920 1h Road network 17.8 285 10.1 228 12.1 32 5.4 37 5.8 15
Twitch [25] 78,389 512 1hShared audience 1.38 8,413 1.36 341,294 1.35 5,353 1.23 9655 1.26 4,280
Wiki [1] 999 792 1h Co-clicks 15.7 422 9.7 1390 11.7 41 2.8 52 3.5 37
Covid [13] 3047 678 1d Spatial k-NN 31969 551 23908 2668 21320 267 204 145 228 88
Table 1: Statistics of the datasets used for evaluation (left sub-table) and quality and running times for competing techniques (right sub-table). All datasets have a
temporal and graph mode with corresponding dictionaries. The temporal resolution of each dataset is specified in column Res while the following column lists the
kind of associated graph. RMSE and timing results of all competing methods using the same number of atoms are listed in the remaining columns. The target
number of atoms are as follows: Synthetic with ground truth (GT) atom count (200, GW+RS test in Fig. 2); Road, Twitch, and Covid: 40% of total atoms; Twitch: 20%
of total atoms.
is a matrix inRğ‘Ã—ğ‘€with independent and identically distributed
standard Gaussian entries, appropriately normalized. Specifically,
the assumption that ğ‘â‰«ğ‘€implies that with high probability, each
column of a standard Gaussian matrix has ğ¿2norm approximatelyâˆš
ğ‘. Thus, we define the noise component as:
ğ‘„=ğœâˆš
ğ‘ğ‘€Â·N(0,ğ¼ğ‘Ã—ğ‘€), (5)
for some positive standard deviation ğœ. With this normalization,
âˆ¥ğ‘„âˆ¥ğ¹=Î˜(1), and thus has the same order of growth as âˆ¥ğ‘…âˆ¥ğ¹, with
high probability.
Low rank and sparsity assumptions on ğ‘…:We will assume that
ğ‘…has rankğ‘Ÿ, which implies that ğ‘…has an expansion of the form
ğ‘…=Î¨ğ‘Œğ‘ŠÎ¦ğ‘‡=ğ¼âˆ‘ï¸
ğ‘–=1ğ½âˆ‘ï¸
ğ‘—=1ğ‘ğ‘–,ğ‘—ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—, (6)
whereğ‘ŒâˆˆRğ¼Ã—ğ‘Ÿandğ‘ŠâˆˆRğ‘ŸÃ—ğ½, and where the double-sum atom-
based representation is under the assumption that Î¨andÎ¦ğ‘‡are
not under-complete. Eq. 6 implies the following explicit ğ‘ğ‘–,ğ‘—form:
ğ‘ğ‘–,ğ‘—=(ğ‘Œğ‘–,Â·)(ğ‘ŠÂ·,ğ‘—)ğ‘‡. (7)
Furthermore, we will make the following sparsity assumption: there
exist onlyğ‘ =Î˜(1)dictionary coefficients ğ‘ğ‘–,ğ‘—in the expansion of
ğ‘…that are nonzero, and these are Î˜(1)uniformly in ğ‘andğ‘€.
Assumption on approximate orthogonality of dictionary
atoms: We next formulate an approximate orthogonality condition
for the dictionary atoms. To do so, we first recall the definition of
theğ¿âˆoperator norm of a matrix ğ‘€:
âˆ¥ğ‘€âˆ¥ğ‘œğ‘,âˆ:=sup
âˆ¥ğ‘¥âˆ¥âˆ=1âˆ¥ğ‘€ğ‘¥âˆ¥âˆ. (8)
We will assume that the dictionaries Î¨âˆˆRğ‘Ã—ğ¼,Î¦ğ‘‡âˆˆRğ½Ã—ğ‘€are
such thatğ¼âˆˆ[ğ‘,ğ‘ğ‘œğ‘›ğ‘ ğ‘¡Â·ğ‘],ğ½âˆˆ[ğ‘€,ğ‘ğ‘œğ‘›ğ‘ ğ‘¡Â·ğ‘€]and that a subset of
the atoms for each dictionary constitutes a basis for Rğ‘andRğ‘€,
respectively. We fix an ğ›¼â‰¥0, which may depend on ğ‘andğ‘€. We
also define a matrix Î£âˆˆRğ¼Ã—ğ¼collecting the pairwise inner products
between dictionary elements in Î¨: namely, Î£ğ‘–,ğ‘—:=(ğœ“ğ‘–)ğ‘‡Â·ğœ“ğ‘—.We
will assume thatâˆ¥Î£1/2âˆ¥ğ‘œğ‘,âˆâ‰¤ğ›¼=ğ‘œ(1). We similarly define Î“âˆˆ
Rğ½Ã—ğ½forÎ¦ğ‘‡, with the same bound on âˆ¥Î“1/2âˆ¥ğ‘œğ‘,âˆ. Intuitively, this
operator norm upper bound translates to an upper bound on the
sum of absolute values of inner products between dictionary atoms.
Thus, this constitutes an approximate orthogonality assumption.
Such assumptions are common in analyses of orthogonal matching
pursuit, under the name of mutual incoherence between dictionary
atoms. See, e.g., [ 3]. We will also assume that the columns of Î¨and
the rows of Î¦ğ‘‡are normalized in ğ¿2.Statement of the accuracy guarantee: We finally state our main
theoretical result. We denote by ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ the output of the top- Ë†ğ‘˜
atom selection algorithm with input data matrix Ë†ğ‘…and dictionaries
Î¨,Î¦ğ‘‡. Specifically, by this we mean that ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ is the result of first
choosing the Ë†ğ‘˜atoms (outer products of left and right dictionary
elements) with the highest alignment scores with Ë†ğ‘…, then approxi-
mating Ë†ğ‘…via a linear combination of the chosen atoms obtained by
solving (4).
Theorem 4.1 (Accuracy guarantee for top- ğ‘˜atom selection
denoising). Letğ‘,ğ‘€, Î¨,Î¦ğ‘‡,ğ‘…,ğ‘„, Ë†ğ‘…,ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ be as outlined above.
We then have that if Ë†ğ‘˜â‰¥ğ‘ and Ë†ğ‘˜=Î˜(1), whereğ‘ is the sparsity pa-
rameter of the the signal matrix ğ‘…, thenâˆ¥ğ‘…âˆ’ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡âˆ¥ğ¹=ğ‘œ(âˆ¥ğ‘…âˆ¥ğ¹).
In other words, the relative error in approximating ğ‘…byğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡
isğ‘œ(1)asğ‘â†’âˆ . That is, when the data consists of a Gaussian-
noise perturbation of a low-rank signal matrix that is a sparse
linear combination of dictionary atoms, and when the greedy atom
selection algorithm chooses sufficiently many atoms, the signal
matrix is recovered to within a vanishingly small relative error. We
prove Theorem 4.1 in Appendix A. We also extend the result to
cover the case where Ë†ğ‘˜<ğ‘ but the algorithm is run for sufficiently
many iterations.
Our analysis provides a theoretical justification for the top k atom
selection strategy as a recovery guarantee for a noise-perturbed
low-rank and sparse signal matrix, which forms a subroutine of
our method. We also demonstrate empirically that this recovery
guarantee holds in Sec. 5.5. The scalability of our technique comes
from the fact that we are working with a few atoms as opposed to
the complete dictionary and the theoretical analysis shows that this
running time reduction affects minimally the model quality since
theâ€œtrueâ€ atoms necessary for encoding the noise-free version of
the data are retained.
5 Experimental evaluation
Our experimental design focuses on the running time and the rep-
resentation quality of competing methods with both variants of
LRMDS on synthetic and real-world datasets listed in Tbl. 1. We
also empirically confirm our theoretical results. We compare our
approaches to state-of-the-art baselines for 2D sparse coding. We
measure running time in seconds for execution on a dedicated In-
tel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz and 251 GB memory
server using MATLABâ€™s R2019a 64-bit version. The representation
quality is quantified as the root mean squared error (RMSE) be-
tween the data and the learned representation. It is important to
note, that beyond representation quality, the low-rank sparse 2D
coding model offers advantages in a number of downstream tasks
as reported by baselines employing this or similar models [ 8,22].
 
2110KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boya Ma, Maxwell McNeil, Abram Magner, & Petko Bogdanov
We focus our evaluation of scalability and representation qual-
ity as speed-up via dictionary sub-selection is the main contribu-
tion of our work. An implementation of LRMDS is available at
https://www.cs.albany.edu/~petko/lab/code.html.
5.1 Datasets
Synthetic data generation. Our synthetic data is generated based
on the low-rank encoding model Î¨ğ‘ ğ‘Œğ‘ŠÎ¦ğ‘‡ğ‘ +ğœ–, where Î¨ğ‘ andÎ¦ğ‘ 
are small (ground truth) randomly selected subsets of the overall
dictionaries and the corresponding coding coefficients for those
atoms inğ‘Œandğ‘Šare also randomly sampled with ğœ–-mean random
noise added to the input. Real-world datasets. We employ real-
world datasets with temporal and spatial dimensions to evaluate
competing techniques employing both time and graph dictionaries.
The datasets span a variety of domains: data from content exchange
within Twitch [25], web traffic data Wiki [1], spatio-temporal disease
spread over time in the Covid [13] dataset, and sensor network
data from road traffic Road [2]. We provide further details on data
generation and real-world evaluation datasets in the supplement.
5.2 Experimental setup.
Baselines. We compare the versions of LRMDS to the two avail-
able methods for multi-dictionary coding: (TGSD [22] and 2D-
OMP [8]). These methods have already been discussed in detail in
Sec. 3. As a brief summary, TGSD solves the problem of low-rank
encoding within an ğ¿1sparsity regularized optimization framework.
2D-OMP selects dictionary atom pairs in a greedy manner and
estimates the corresponding coding coefficients one at a time. It
produces a solution which is not guaranteed to be low-rank.
Since a key advantage of our method is its sub-selection of large
dictionaries, we also seek to understand if extending 1D dictionary
screening to the 2D setting results in a scalable 2D approach. To this
end, we generalize a 1D dictionary screening approach [ 32] to work
with multiple dictionaries and combine it with TGSD to facilitate
a more thorough comparison. The resulting method SC-TGSD
screens (removes) the worst dictionary atoms from a dictionary by
calculating alignment scores between atoms and associated data.
To perform the subsequent coding, we employ TGSD with only the
sub-selected dictionaries. Intuitively this baseline can be thought
as a 2-step combination of screening and TGSD coding. Details of
the screening process are available in the extended version [16].
Metrics. We measure the reconstruction error of the learned repre-
sentation using root mean squared error (RMSE =âˆšï¸‚Ã
ğ‘–,ğ‘—(ğ‘‹ğ‘–,ğ‘—âˆ’ğ‘‹â€²
ğ‘–,ğ‘—)2
|ğ‘‹|,
whereğ‘‹is the original signal, ğ‘‹â€²is the reconstruction and |ğ‘‹|de-
notes the number of elements in ğ‘‹) of the learned representationâ€™s
departure from the input data. We measure the running time in
seconds for all competing methods.
Experimental design. The goal of our experiments is to demon-
strate the utility of LRMDS in both synthetic and real world datasets.
In synthetic data tests, we varying different properties of the data
generation process, such as SNR and the dictionary size and type.
We seek to quantify the speed up and quality improvement that
LRMDS enables compared to baselines. Parameter settings can be
found in the supplement.5.3 Evaluation on synthetic data.
In our synthetic data evaluation we compare the effects of dictionary
size and type on the performance of LRMDS and its competitors.
Using the ground truth number of atoms as a target, we compare
the reconstruction error (RMSE) and running time (secs) for all
techniques. We also characterize the effect of varying noise and
show these results in the supplement.
Varying dictionary composition and size. As all competitors
perform a type of dictionary sub-selection (implicitly in the case of
TGSD due to its regularization), a natural question to ask is: How
does the composition and size of the input dictionaries affect the ability
of a method to quickly and accurately represent a data matrix? To
answer this question we first utilize a set of composite dictionaries
to generate the data input. In this case the left composite dictionary
is a stack of a GFT (G) and a graph Wavelet (W) dictionaries and
the right composite dictionary is a stack of Ramanujan (R) and
Spline (S) temporal dictionaries. We use 50 randomly chosen atoms
from each of the four dictionaries (200 in total) to generate the
synthetic input data. We then prepare 3 test settings by varying the
dictionaries compositions available to the competitors. The model
input dictionaries in these 3 settings are as follows (1) Î¨=[ğº],Î¦=
[ğ‘…]denoted G+R; (2) Î¨=[ğº,ğ‘Š],Î¦=[ğ‘…]denoted GW+R; and (3)
Î¨=[ğº,ğ‘Š],Î¦=[ğ‘…,ğ‘†]denoted GW+RS.
We first compare the RMSE and running time for all techniques
when using a fixed number of dictionary atoms and report results
of this experiment in Fig. 2(a), 2(b). The x-axis lists the consecutive
dictionary compositions and the total number of atoms is listed on
top of the figure. We report RMSE and run time of each method
when employing 200atoms, which is also the number of ground
truth (GT) atoms. Note that for SC-TGSD and TGSD, we pick the
point for which the employed number of atoms is closest to the GT
since they have no parameters to directly control the exact number
of atoms in their methods. As larger dictionaries are being used,
the RMSE of all methods improves, however LRMDS achieves the
best reconstruction quality at all points. This is due to the higher
quality atom selection compared to baselines. LRMDS variants are
also the fastest to select these atoms as seen in Fig. 2(b).
In Figs. 2(c), 2(d), we further break down the performance of
each method when utilizing ground truth composite dictionaries
GW+RS. Explicitly, we track the RMSE and run time as a function
of the percentage of selected atoms. We additionally show that the
representation quality improves as a function of the total run time,
and similar results for other choices of dictionary combinations
settings (G+R, GW+R) in the extended version [16].
Both variants of LRMDS obtain the most accurate representa-
tions among competitors while simultaneously taking the least
amount of time. The next best approach, 2D-OMP initially selects
and updates its coefficients quickly, closely trailing LRMDS when
the representation quality for both is poor. However, with further
iterations the representation quality gains by 2D-OMP slow down.
This is due to 2D-OMPâ€™s restriction to only select one coefficient
corresponding to an atom pair per iteration and the need to re-
estimate the coefficients for all previously selected pairs. This is
highly inefficient when many pairs of a small subset of atoms in
the optimal sparse coding are non zero. In such cases, 2D-OMP still
 
2111Low Rank Multi-Dictionary Selection at Scale KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
G+R GW+R GW+RS
Dict type10-210-1RMSE TGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f3020 4020 6020
(a) RMSE
G+R GW+R GW+RS
Dict type101102103104TimeTGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f3020 4020 6020 (b) Time
0% 1% 2% 3%
Selected atoms10-210-1RMSETGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (c)GW+RS: RMSE vs Atom%
0% 1% 2% 3%
Selected atoms100102104Time
TGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (d)GW+RS: Time vs Atom%
Figure 2: Comparison of competing techniques on synthetic data. (a), (b): RMSE and running time for varying dictionaries available to each method (listed under
the x axis). The total number of (left and right dictionary) atoms is specified at the top of each figure. We stack increasing sets of dictionaries on the left and right,
while the ground truth atoms are selected from the full set GW+RS. (c): RMSE as a function of the number of selected atoms when multiple dictionaries are provided.
(d): Run time as a function of the number of selected atoms. GW+RS stands for GFT and Graph Haar wavelets stacked together for the graph dimension and RS
stands for Ramanujan and Spline dictionaries stacked for the temporal dimension (details of the dictionary definitions are available in the extended version [ 16]).
adds pairs one at a time, while our approach allows for coding with
all combinations of already selected left and right atoms.
5.4 Evaluation on real-word datasets.
We next evaluate all techniques on the real-world datasets and re-
port the RMSE and running time at set percentages of total available
atoms selected. Results from all datasets for a fixed percentages of
atoms are listed in Tbl. 1. This high-level comparison demonstrates
that given a fixed number of target atoms, LRMDS produces the
most accurate representations, while LRMDS-f is the most scalable
at the cost of slight deteriorating in RMSE compared to LRMDS.
Note that LRMDS-f is still the most accurate among baselines from
the literature.
More detailed results on real-world datasets are presented in
Fig. 3. We employ a graph Fourier dictionary (GFT) for Î¨and Ra-
manujan periodic dictionary for Î¦for all datasets. The sizes of
these dictionaries are listed in the caption of Fig. 3. The detailed
analysis also demonstrates that variants of LRMDS dominate based
on both accuracy and running time across a wide variety of settings
and datasets. We show the representation error as a function of
the percentage of selected atoms in Figs 3(a)-3(d) and the run time
necessary to obtain said percentages in Figs. 3(e)-3(h). Together
these plots show both the quality of representation and the time
necessary to obtain it for a varying percentage of selected atoms
(additional figures in the extended version [ 16] explicitly show this
relationship). For each dataset, 2D-OMP selects highly represen-
tative atoms at first due to its greedy strategy, however, its trend
is quickly overtaken by those of our methods as more atoms are
allowed for selection. LRMDS matches or outperforms LRMDS-f in
terms of representation quality given the same number (percentage)
of atoms, however, LRMDS-f selects new atoms faster demonstrat-
ing the trade-off between running time and quality between the
two. Both of our methods are as fast or faster than all baselines at
selecting atoms with the exception of the 2D-OMP in its first several
iterations. The only method matching the speed of LRMDS is TGSD-
SC (LRMDS-f is always faster). Although fast, TGSD-CS exhibits
poor representation quality rendering it not useful in settings in
which the representation quality is critical. For the Twitch datasets
when a large percentage of atoms are selected, TGSD is able to
obtain similar running time to LRMDS but at a far worse represen-
tation quality. TGSD is not well suited for dictionary sub-selection
as sparsity is only implicitly encouraged through ğ¿1regularization
over all possible coefficients and it has no direct control on whichatoms are used. Thus, even a single nonzero coefficient correspond-
ing to an otherwise poorly selected atom may cause the atom to be
â€œselectedâ€ by TGSD.
An interesting finding is that there is an abrupt drop in RMSE
in Wiki and Covid data for both variants of LRMDS. This indicates
that the learned representation is initially missing some crucial
atoms that LRMDS is able to eventually detect and incorporate
into the selected dictionaries. Competitors omit these crucial atoms
in their representations leading to poorer RMSE. This drop also
corresponds to a setting where the difference in quality between
LRMDS and competitors is most striking. For example, in Fig. 3(d)
LRMDS obtains a roughly two orders of magnitude reduction in
RMSE when 50%of the available atoms are selected.
5.5 Theoretical guarantees validation (Thm 4.1)
Here, we study the performance of the LRMDS for denoising which
serves as empirical validation of Thm. 4.1. Specifically, we demon-
strate that LRMDS is able to recover the underlying â€œcleanâ€ signal
ğ‘…from a noisy signal Ë†ğ‘…=ğ‘…+ğ‘„(Fig.4(a)). The experimental setup
is as follows: ğ‘,ğ‘€,ğ¼,ğ½ are set to 500,10,1000,20, respectively. The
rankğ‘Ÿof the signal is set to 3. For each dictionary, the first half of its
atoms are almost orthogonal (generated as an orthogonal basis with
Gaussian noise added to the atoms at SNR=20), and the atoms in
the second half of the dictionary are generated as Gaussian N(0,1)
random. The sparsity parameter ğ‘ is set to 10%of the total num-
ber of the almost-orthogonal atoms. The GT atoms for the signal
matrixğ‘…are chosen uniformly at random from the first half of the
atoms (almost-orthogonal), and the atom coefficients are selected
randomly (N(0,1)). This constitutes the clean signal matrix ğ‘…. We
also create a pure independent Gaussian noise matrix ğ‘„by first
calculating the standard deviation ğœofğ‘…, and setğ‘„=N(0,ğœ
20).
Finally, we set Ë†ğ‘…=ğ‘…+ğ‘„.
To demonstrate LRMDSâ€™s ability to denoise input data Ë†ğ‘…, we run
LRMDS on both ğ‘…and Ë†ğ‘…producing two sets of coefficients (ğ‘Œğ‘Š)ğ‘…
and(ğ‘Œğ‘Š)ğ‘…+ğ‘„. We then track the RMSE of the reconstruction for
both variants against the clean data ğ‘…(i.e., RMSE(ğ‘…âˆ’Î¨(ğ‘Œğ‘Š)ğ‘…Î¦ğ‘‡)
and RMSE(ğ‘…âˆ’Î¨(ğ‘Œğ‘Š)ğ‘…+ğ‘„Î¦ğ‘‡)). Results from this analysis are pre-
sented in Fig. 4(a). The curves are nearly identical regardless of
the input, demonstrating that LRMDS successfully extracts the un-
derlying signal while ignoring the noise. To further investigate
this property we compare the three different sets of dictionary
coefficients corresponding to ğ‘…,Ë†ğ‘…, andğ‘„:(ğ‘Œğ‘Š)ğ‘…,(ğ‘Œğ‘Š)ğ‘…+ğ‘„(as
above) and ğ‘ğ‘„.ğ‘ğ‘„contains coefficients computed via 2D-OMP
 
2112KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boya Ma, Maxwell McNeil, Abram Magner, & Petko Bogdanov
0% 5% 10% 15% 20%
Selected atoms1.251.31.35 RMSETGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f
(a) Twitch: RMSE vs Atom%
0% 10% 20% 30% 40%
Selected atoms681012141618RMSETGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (b) Road: RMSE vs Atom%
0% 10% 20% 30% 40% 50%
Selected atoms46810121416RMSETGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (c) Wiki: RMSE vs Atom%
0% 10% 20% 30% 40% 50%
Selected atoms102103104RMSETGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (d) Covid: RMSE vs Atom%
0% 5% 10% 15% 20%
Selected atoms102104106TimeTGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f
(e) Twitch: Time vs Atom%
0% 10% 20% 30% 40%
Selected atoms10-2100102Time
TGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (f) Road: Time vs Atom%
0% 10% 20% 30% 40% 50%
Selected atoms10-2100102104TimeTGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (g) Wiki: Time vs Atom%
0% 10% 20% 30% 40% 50%
Selected atoms10-2100102104Time
TGSD
2D-OMP
SC-TGSD
LRMDS
LRMDS-f (h) Covid: Time vs Atom%
Figure 3: Comparison between competitors of representation quality as a function of the percentage of selected atoms Figs.(a)-(d), and runtime as a function of
the percentage of selected atoms Figs.(e)-(h). All methods use a GFT for Î¨and a Ramanujan periodic dictionary for Î¦. The dimensions of the utilized dictionaries
are as follows: Twitch: Î¨âˆˆR78389Ã—78389,Î¦âˆˆR512Ã—2230; Wiki: Î¨âˆˆR999Ã—999,Î¦âˆˆR792Ã—6000; Road: Î¨âˆˆR1923Ã—1923,Î¦âˆˆR720Ã—3044; Covid: Î¨âˆˆR3047Ã—3047,Î¦âˆˆR678Ã—6000.
Note: 2D-OMPâ€™s trace on the Twitch dataset is truncated early as it does not scale (fails to complete in 72hours) when selecting more than 13%of the atoms.
0 5 10 15 20
Iteration00.010.020.03 RMSEclean: LRMDS
clean + noise: LRMDS
(a)clean vs. noise
 (b)Coef. diff distribution
0% 0.5% 1% 1.5% 2%
Selected atoms (percentage)100101RMSE
GT
atoms
LRMDS
RAND
LRMDS-1D (c)Ablation: RMSE vs Atom%
0% 0.5% 1% 1.5% 2%
Selected atoms10-2100Time
GT
atoms
LRMDS
RAND
LRMDS-1D (d)Ablation: Time vs Atoms%
Figure 4: (a)-(b): Empirical demonstration of the theoretical guarantee on LRMDSâ€™s ability to denoise a signal. (a): â€œclean: LRMDSâ€ operates on the clean matrix ğ‘…
whereas â€œclean + noiseâ€ operates on the noisy signal Ë†ğ‘…=ğ‘…+ğ‘„. The RMSE for both methods is measured with respect to the clean data ğ‘…. (b) The absolute difference
between the learned coefficient matrices for the clean data (ğ‘Œğ‘Š)ğ‘…, noisy data(ğ‘Œğ‘Š)ğ‘…+ğ‘„, and pure noise ğ‘ğ‘„. (c)(d): Ablation study demonstrating the importance of
joint selection of atoms from both dictionaries. We compare LRMDS to variants in which atoms are selected from the left and right dictionaries independently
(LRMDS-1D) or randomly (RAND). We measure RMSE (c) and runtime (d) as a function of the percentage of selected atoms.
of the noise matrix. We utilize this instead of LRMDS as due to
its low rank constraint it is not capable of well representing an
arbitrary noise matrix (as demonstrated above). All methods are
run until they converge for their respective inputs. We then calcu-
late the absolute difference in the learned coefficients. Explicitly,
|(ğ‘Œğ‘Š)ğ‘…âˆ’(ğ‘Œğ‘Š)ğ‘…+ğ‘„|and|(ğ‘Œğ‘Š)ğ‘…âˆ’(ğ‘)ğ‘„|and plot the histograms
of the nonzero difference values in Fig. 4(b). While the noise ğ‘ğ‘„and
clean data(ğ‘Œğ‘Š)ğ‘…differ significantly (3683 non-zero differences
between the two), the fits of the noisy (ğ‘Œğ‘Š)ğ‘…+ğ‘„and clean(ğ‘Œğ‘Š)ğ‘…
data align much better (1236 non-zero differences). The histograms
of these differences also indicate that the addition of noise does not
significantly impact the coefficients learned by LRMDS.
5.6 Ablation study: Is joint selection critical?
LRMDS uses the projection of the residual onto left-right atom pairs
(i.e.,ğ‘ƒ=Ë†Î¨ğ‘‡ğ‘…Ë†Î¦) to select atoms. This opens a natural question on
the necessity of this technique: Can we select atoms from each of
the dictionaries independently employing 1D approaches directly onthe left and right dictionary? In other words, is joint selection based
on the projection we employ critical? To answer these questions, we
implement two variants of LRMDS: i) LRMDS-1D selects atoms
from one dictionary at a time via 1D projection, while ii) RAND
chooses 2D atoms randomly. We then evaluate their performance on
a version of our synthetic dataset with an equal number of ground
truth atoms in Î¨andÎ¦. More details on the implementation and
setting for this experiment are available in the extended version
[16].
In Fig. 4(c) we plot RMSE of the three variants of our method as
a function of the number of selected atoms. LRMDS approaches its
optimal fit (smallest RMSE) when using the ground truth number of
atoms. LRMDS-1D requires more atoms to achieve the same level
of RMSE, demonstrating that the joint atom selection is essential
for identifying good representative atoms from both dictionaries.
The RAND method (random 2D atom selection) is unlikely to select
atoms aligned with the data leading to its poor performance. The
running time of LRMDS and LRMDS-1D are similar with the latter
having a slight advantage due to its cheaper selection mechanism
 
2113Low Rank Multi-Dictionary Selection at Scale KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
and residual re-calculation (Fig. 4(d)). For LRMDS, the projection
requires multiplication of complexity ğ‘‚(ğ‘šğ‘–ğ‘›(ğ¼ğ‘ğ‘€+ğ¼ğ‘€ğ½,ğ¼ğ‘ğ½+
ğ‘ğ‘€ğ½ ), whereas the projection in LRMDS-1D has a complexity of
ğ‘‚(ğ¼ğ‘ğ‘€+ğ‘ğ‘€ğ½ ). Note thatğ‘€<ğ½in this experiment, explaining the
runtime advantage of LRMDS-1D. RAND runs much faster at the
beginning as there is no projection to select atoms, however, when
more atoms are added this advantage shrinks dramatically. This is
because the computational complexity quickly becomes dominated
by the coefficient updates which take similar time regardless of
which atoms are selected.
Another significant weakness of LRMDS-1D and RAND not
highlighted by this experiment is their inability to adaptively select
different number of atoms from the right and left dictionaries. The
user must specify how many atoms should be selected from each
dictionary manually. In contrast, LRMDS can dynamically select
the best atoms from either dictionary in a data-driven manner.
Thus, this experiment represents an ideal scenario where a user has
correctly identified the proportion of atoms need from Î¨andÎ¦.
6 Conclusion
In this paper we introduced LRMDS, a scalable and accurate method
for sparse multi-dictionary coding of 2D datasets. Our approach
sub-selects dictionary atoms and employs convex optimization to
encode the data using the selected atoms. We provided a theoretical
guarantee for the quality of the atom sub-selection for the task of
denoising the data. We also demonstrated the quality and scalability
of LRMDS on several real-world datasets and by employing multiple
analytical dictionaries. It outperformed state-of-the-art 2D sparse
coding baselines by up to 1order of magnitude in terms of running
time and up to 2orders of magnitude in representation quality on
some of the real-world datasets. As a future direction, we plan to
extend our dictionary selection approach to multi-way data (i.e.,
tensors) making the core idea applicable to a wider range of problem
settings.
7 Acknowledgements
This research was funded by the NSF SC&C grant CMMI-1831547.
AM was funded by NSF CCF grants CIF-2212327 and CIF-2338855.
References
[1][n. d.]. Wikipedia Page Views Statistics http://dumps.wikimedia.org/other/
pagecounts-raw/.
[2]Peter Bickel, Chao Chen, Jaimie Kwon, John Rice, and Erik Zwet. 2002. Traffic Flow
on a Freeway Network. (01 2002). https://doi.org/10.1007/978-0-387-21579-2_5
[3]T Tony Cai and Lie Wang. 2011. Orthogonal matching pursuit for sparse signal
recovery with noise. IEEE Transactions on Information theory 57, 7 (2011), 4680â€“
4688.
[4]Scott Shaobing Chen, David L Donoho, and Michael A Saunders. 2001. Atomic
decomposition by basis pursuit. SIAM review 43, 1 (2001), 129â€“159.
[5]Nilson Maciel de Paiva, Elaine Crespo Marques, and Lirida Alves de Bar-
ros Naviner. 2017. Sparsity analysis using a mixed approach with greedy and
LS algorithms on channel estimation. In 2017 3rd International Conference on
Frontiers of Signal Processing (ICFSP). IEEE, 91â€“95.
[6]Xiaowen Dong, Dorina Thanou, Laura Toni, Michael Bronstein, and Pascal
Frossard. 2020. Graph signal processing for machine learning: A review and new
perspectives. IEEE Signal processing magazine 37, 6 (2020), 117â€“127.
[7]Michael Elad and Michal Aharon. 2006. Image denoising via sparse and redundant
representations over learned dictionaries. IEEE Transactions on Image processing
15, 12 (2006), 3736â€“3745.
[8]Yong Fang, JiaJi Wu, and BorMin Huang. 2012. 2D sparse signal recovery via
2D orthogonal matching pursuit. Science China Information Sciences 55 (2012),
889â€“897.[9]Kaito Fujii and Tasuku Soma. 2018. Fast greedy algorithms for dictionary selection
with generalized sparsity constraints. Advances in Neural Information Processing
Systems 31 (2018).
[10] Aboozar Ghaffari, Massoud Babaie-Zadeh, and Christian Jutten. 2009. Sparse
decomposition of two dimensional signals. In 2009 IEEE international conference
on acoustics, speech and signal processing. IEEE, 3157â€“3160.
[11] Thomas Nall Eden Greville. 1966. Note on the generalized inverse of a matrix
product. Siam Review 8, 4 (1966), 518â€“521.
[12] Shihao Ji, Ya Xue, and Lawrence Carin. 2008. Bayesian compressive sensing. IEEE
Transactions on signal processing 56, 6 (2008), 2346â€“2356.
[13] Samue Kemp, Jason W Howel, and Peter C Lu. 2020. Bing COVID-19 Tracker.
www.bing.com/covid
[14] Jaeseok Lee, Jun Won Choi, and Byonghyo Shim. 2016. Sparse signal recovery
via tree search matching pursuit. Journal of Communications and Networks 18, 5
(2016), 699â€“712.
[15] Boya Ma, Maxwell McNeil, and Petko Bogdanov. 2023. GIST: Graph Inference for
Structured Time Series. In Proceedings of the 2023 SIAM International Conference
on Data Mining (SDM). SIAM, 433â€“441.
[16] Boya Ma, Maxwell McNeil, Abram Magner, and Petko Bogdanov. 2024. Low Rank
Multi-Dictionary Selection at Scale. arXiv:2406.06960 [cs.LG]
[17] Elaine Crespo Marques, Nilson Maciel, Lirida Naviner, Hao Cai, and Jun Yang.
2018. A review of sparse recovery algorithms. IEEE access 7 (2018), 1300â€“1322.
[18] Andreas Maurer, Massi Pontil, and Bernardino Romera-Paredes. 2013. Sparse
coding for multitask and transfer learning. In International conference on machine
learning. PMLR, 343â€“351.
[19] Maxwell McNeil and Petko Bogdanov. 2023. Multi-dictionary tensor decom-
position. In 2023 IEEE International Conference on Data Mining (ICDM). IEEE,
1217â€“1222.
[20] Maxwell McNeil, Boya Ma, and Petko Bogdanov. 2022. SAGA: Signal-aware
graph aggregation. In Proceedings of the 2022 SIAM International Conference on
Data Mining (SDM). SIAM, 136â€“144.
[21] Maxwell McNeil, Carolina Mattsson, Frank W Takes, and Petko Bogdanov. 2023.
CADENCE: Community-Aware Detection of Dynamic Network States. In Pro-
ceedings of the 2023 SIAM International Conference on Data Mining (SDM). SIAM,
1â€“9.
[22] Maxwell J McNeil, Lin Zhang, and Petko Bogdanov. 2021. Temporal Graph Signal
Decomposition. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 1191â€“1201.
[23] Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krish-
naprasad. 1993. Orthogonal matching pursuit: Recursive function approximation
with applications to wavelet decomposition. In Proceedings of 27th Asilomar
conference on signals, systems and computers. IEEE, 40â€“44.
[24] Wei Qiu, Jianxiong Zhou, and Qiang Fu. 2019. Jointly using low-rank and sparsity
priors for sparse inverse synthetic aperture radar imaging. IEEE Transactions on
Image Processing 29 (2019), 100â€“115.
[25] JÃ©rÃ©mie Rappaz, Julian McAuley, and Karl Aberer. 2021. Recommendation on
Live-Streaming Platforms: Dynamic Availability and Repeat Consumption. In
Fifteenth ACM Conference on Recommender Systems. 390â€“399.
[26] Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. 2010. Dictionaries for
sparse representation modeling. Proc. IEEE 98, 6 (2010), 1045â€“1057.
[27] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre
Vandergheynst. 2013. The emerging field of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks and other irregular domains.
IEEE signal processing magazine 30, 3 (2013), 83â€“98.
[28] Srikanth V. Tenneti and P. P. Vaidyanathan. 2015. Nested Periodic Matrices and
Dictionaries: New Signal Representations for Period Estimation. IEEE Trans.
Signal Processing 63, 14 (2015), 3736â€“3750. https://doi.org/10.1109/TSP.2015.
2434318
[29] Ivana ToÅ¡iÄ‡ and Pascal Frossard. 2011. Dictionary learning. IEEE Signal Processing
Magazine 28, 2 (2011), 27â€“38.
[30] Jian Wang, Seokbeop Kwon, and Byonghyo Shim. 2012. Generalized orthogonal
matching pursuit. IEEE Transactions on signal processing 60, 12 (2012), 6202â€“6216.
[31] John Wright, Allen Y Yang, Arvind Ganesh, S Shankar Sastry, and Yi Ma. 2008.
Robust face recognition via sparse representation. IEEE transactions on pattern
analysis and machine intelligence 31, 2 (2008), 210â€“227.
[32] Zhen James Xiang, Yun Wang, and Peter J Ramadge. 2016. Screening tests for
lasso problems. IEEE transactions on pattern analysis and machine intelligence 39,
5 (2016), 1008â€“1027.
[33] Dong Zhang, Yongshun Zhang, and Cunqian Feng. 2017. Joint-2D-SL0 Algorithm
for Joint Sparse Matrix Reconstruction. International Journal of Antennas and
Propagation 2017, 1 (2017), 6862852.
[34] Lin Zhang, Wenyu Zhang, Maxwell J McNeil, Nachuan Chengwang, David S
Matteson, and Petko Bogdanov. 2021. AURORA: A Unified fRamework fOR
Anomaly detection on multivariate time series. Data Mining and Knowledge
Discovery 35, 5 (2021), 1882â€“1905.
[35] Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, and David Zhang. 2015. A survey
of sparse representation: algorithms and applications. IEEE access 3 (2015), 490â€“
530.
 
2114KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boya Ma, Maxwell McNeil, Abram Magner, & Petko Bogdanov
Supplemental Material
We next add supplemental content to aid reproducibility and the
understanding of our theoretical analysis including proofs of the
theoretical results, method derivation, dataset preparation and pa-
rameter tuning.
A Proofs of Theorem 4.1
Here we give details for the proof of Theorem 4.1. Intuitively, the
task boils down to showing that the coefficients in any dictionary
expansion of the noise matrix ğ‘„are uniformly ğ‘œ(1), which allows us
to recover the dictionary atoms that contribute to the signal matrix
ğ‘…. As a reminder ğ‘„âˆˆRğ‘Ã—ğ‘€with independent and identically
distributed standard Gaussian entries.
We start with several lemmas. In essence, the first lemma al-
lows us to focus on upper bounding the inner product of the
columns of ğ‘„with those of Î¨in order to upper bound the co-
efficients in any dictionary expansion of ğ‘„. Before stating it, we
note thatRğ‘Ã—ğ‘€is an inner product space with inner product
âŸ¨ğ´,ğµâŸ©ğ‘,ğ‘€ :=Ãğ‘
ğ‘–=1Ãğ‘€
ğ‘—=1ğ´ğ‘–,ğ‘—ğµğ‘–,ğ‘—. It is a matter of simple algebra to
show the following formula for âŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€:
âŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€=ğ‘€âˆ‘ï¸
ğ‘˜=1ğœ™ğ‘—,ğ‘˜Â·âŸ¨ğ‘„Â·,ğ‘˜,ğœ“ğ‘–âŸ©ğ‘,ğ‘€. (9)
This implies the following upper bound, since the rows of Î¦ğ‘‡are
normalized in ğ¿2:
|âŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€|â‰¤âˆš
ğ‘€Â·max
ğ‘˜âˆˆ[ğ‘€]|âŸ¨ğ‘„Â·,ğ‘˜,ğœ“ğ‘–âŸ©ğ‘,ğ‘€|, (10)
using the fact that for any vector ğ‘¥âˆˆRğ‘‘,âˆ¥ğ‘¥âˆ¥1â‰¤âˆš
ğ‘‘âˆ¥ğ‘¥âˆ¥2. In other
words, to upper bound the inner product of ğ‘„with any dictionary
element, it suffices to upper bound the inner product of any column
ofğ‘„with any element of the left-hand dictionary.
Lemma A.1 (Comparison of inner products with dictio-
nary coefficients). Under the boundedness assumptions on dictio-
nary atoms, if an ğ‘‚(1)-norm vector ğ‘„âˆˆRğ‘Ã—ğ‘€has an expansion
ğ‘„=Ãğ¼,ğ½
ğ‘–=1,ğ‘—=1ğ‘ğ‘–,ğ‘—Â·ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—forğ‘ğ‘–,ğ‘—âˆˆR, then we may upper bound
maxğ‘–âˆˆ[ğ¼],ğ‘—âˆˆ[ğ½]|ğ‘ğ‘–,ğ‘—|by upper bounding the inner product
maxğ‘–,ğ‘—âŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€. Specifically, for all (ğ‘–,ğ‘—)âˆˆ[ğ¼]Ã—[ğ½],
|âŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€âˆ’ğ‘ğ‘–,ğ‘—|=ğ‘‚(ğ‘€ğ›¼2)=ğ‘œ(1). (11)
Proof. We note that by linearity of the inner product,
âŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€=ğ‘ğ‘–,ğ‘—+âˆ‘ï¸
(ğ‘˜,â„“)â‰ (ğ‘–,ğ‘—)ğ‘ğ‘˜,â„“âŸ¨ğœ“ğ‘˜ğœ™ğ‘‡
â„“,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€. (12)
The coefficients ğ‘ğ‘˜,â„“are uniformly ğ‘‚(1)by virtue ofğ‘„having norm
ğ‘‚(1), so this simplifies to
âŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€âˆ’ğ‘ğ‘–,ğ‘—=ğ‘‚(1)Â·âˆ‘ï¸
(ğ‘˜,â„“)â‰ (ğ‘–,ğ‘—)âŸ¨ğœ“ğ‘˜ğœ™ğ‘‡
â„“,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€ (13)
=ğ‘‚(1)âˆ‘ï¸
(ğ‘˜,â„“)â‰ (ğ‘–,ğ‘—)âŸ¨ğœ“ğ‘˜,ğœ“ğ‘–âŸ©ğ‘,ğ‘€âŸ¨ğœ™ğ‘‡
â„“,ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€
(14)
â‰¤ğ‘‚(1)âˆ‘ï¸
(ğ‘˜,â„“)â‰ (ğ‘–,ğ‘—)âŸ¨ğœ“ğ‘˜,ğœ“ğ‘–âŸ©ğ‘,ğ‘€ (15)
â‰¤ğ‘‚(ğ‘€ğ›¼2). (16)â–¡
Lemma A.1 implies that we may upper bound the coefficients
of an expansion of a matrix ğ‘„using the inner products of ğ‘„with
dictionary elements. The upper bound (10) allows us to further up-
per boundâŸ¨ğ‘„,ğœ“ğ‘–ğœ™ğ‘‡
ğ‘—âŸ©ğ‘,ğ‘€, reducing our problem to upper bounding
the entries of a multivariate Gaussian random variable. Specif-
ically, if we denote by ğ¾âˆˆ Rğ‘€Ã—ğ¼the matrix ğ¾=ğ‘„ğ‘‡Î¨, then
ğ¾ğ‘–,ğ‘—=âŸ¨ğ‘„Â·,ğ‘–,ğœ“ğ‘—âŸ©ğ‘,ğ‘€. We then have that
max
ğ‘–,ğ‘—|ğ‘ğ‘–,ğ‘—|â‰¤âˆš
ğ‘€max
ğ‘–,ğ‘—|ğ¾ğ‘–,ğ‘—|+ğ‘œ(1). (17)
The next lemma gives us a tool to upper bound maxğ‘–,ğ‘—|ğ¾ğ‘–,ğ‘—|by
using the fact that the covariance of ğ¾ğ‘–,ğ‘—andğ¾ğ‘–,â„“, forâ„“â‰ ğ‘—, is equal
toâŸ¨ğœ“ğ‘—,ğœ“â„“âŸ©ğ‘,ğ‘€/âˆš
ğ‘ğ‘€=Î£ğ‘—,â„“/âˆš
ğ‘ğ‘€. In other words, the vector Ë†ğ¾
obtained by appending the columns of ğ¾ğ‘‡into a column vector of
dimensionğ‘€Â·ğ¼has distributionN(0,Ë†Î£), where Ë†Î£âˆˆRğ‘€ğ¼Ã—ğ‘€ğ¼and
satisfiesâˆ¥Ë†Î£1/2âˆ¥ğ‘œğ‘,âˆ=1âˆš
ğ‘Â·âˆ¥Î£1/2âˆ¥ğ‘œğ‘,âˆ.
Lemma A.2 (Upper bound on the maximum of correlated
Gaussians). Letğ‘‹âˆ¼ N( 0,Î£)be a Gaussian vector in Rğ‘›with
covariance matrix Î£âˆˆRğ‘›Ã—ğ‘›. Then we have that
E[âˆ¥ğ‘‹âˆ¥âˆ]=ğ‘‚(âˆ¥Î£1/2âˆ¥ğ‘œğ‘,âˆÂ·âˆšï¸
logğ‘›). (18)
Proof. This is a consequence of a well-known upper bound on
the maximum of independent and identically distributed standard
normal random variables, along with the fact that, for an isotropic,
mean 0Gaussian vector ğ‘,Î£1/2ğ‘has covariance matrix Î£. â–¡
Lemma A.3 (Upper bound on the maximum inner product
between a noise vector and a dictionary atom). Consider the
matrixğ¾=ğ‘„ğ‘‡Â·Î¨âˆˆRğ‘€Ã—ğ¼whose(ğ‘–,ğ‘—)th entry is the inner product
of theğ‘–th column of ğ‘„with theğ‘—th dictionary element of Î¨. We have
that with high probability,
max
ğ‘–,ğ‘—|ğ¾ğ‘–,ğ‘—|=ğ‘‚(âˆšï¸
log(ğ‘€ğ¼)/âˆš
ğ‘). (19)
Proof. We start with Lemma A.2 applied to Ë†ğ¾. This yields
E[âˆ¥Ë†ğ¾âˆ¥âˆ]=ğ‘‚(âˆ¥Ë†Î£1/2âˆ¥ğ‘œğ‘,âˆÂ·âˆšï¸
log(ğ‘€ğ¼)
âˆš
ğ‘ğ‘€) (20)
=ğ‘‚(âˆšï¸
log(ğ‘€ğ¼)
âˆš
ğ‘Â·âˆ¥Î£1/2âˆ¥ğ‘œğ‘,âˆ). (21)
The proof is finished by applying Markovâ€™s inequality. â–¡
A corollary of Lemma A.3 is that the maximum inner product
betweenğ‘„and the dictionary elements is ğ‘œ(1)with high proba-
bility. This implies, by Lemma A.1, that the coefficients of ğ‘„in
any of its dictionary expansions are uniformly ğ‘œ(1). Because of the
sparsity assumption on the coefficients of ğ‘…, the data matrix Ë†ğ‘…has
ğ‘ coefficients that are Î˜(1), while the rest are ğ‘œ(1). Thus, provided
that the dictionary atom selection procedure selects Ë†ğ‘˜â‰¥ğ‘ atoms
with Ë†ğ‘˜=Î˜(1), those atoms for which ğ‘…has nonzero coefficients
will be among those selected. As a result, the reconstructed matrix
ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ differs from the signal matrix ğ‘…by onlyğ‘œ(âˆ¥ğ‘…âˆ¥ğ¹). This
completes the proof of Theorem 4.1.
We note that only a minor tweak of the above proof is needed to
extend to the case where dictionary selection is iteratively applied
 
2115Low Rank Multi-Dictionary Selection at Scale KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
for a fixed number ğ‘¡â‰¥ğ‘ /Ë†ğ‘˜of steps, each time to the residual of
the previous step. Specifically, in order to formulate this, we need
more notation. Suppose, as before, that ğ‘…is a linear combination of
ğ‘ atoms, each with coefficient uniformly Î˜(1). Suppose that Ë†ğ‘˜<ğ‘ .
Letğ‘…â‰¤Ë†ğ‘˜denote the truncation of ğ‘…to its top Ë†ğ‘˜atoms (i.e., those with
the largest coefficients in absolute value), and let ğ‘…>Ë†ğ‘˜:=ğ‘…âˆ’ğ‘…â‰¤Ë†ğ‘˜.
That is,ğ‘…>Ë†ğ‘˜is the residual of the signal matrix after subtracting
ğ‘…â‰¤Ë†ğ‘˜. Finally, we define ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡,â‰¤Ë†ğ‘˜to be the output of the algorithm
after a single iteration. The proof of our theorem so far showed
thatğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡,â‰¤Ë†ğ‘˜âˆ’ğ‘…â‰¤Ë†ğ‘˜=ğ‘œ(ğ‘…â‰¤Ë†ğ‘˜). This implies the following:
Ë†ğ‘…âˆ’ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡,â‰¤Ë†ğ‘˜=(ğ‘„+ğ‘…â‰¤Ë†ğ‘˜+ğ‘…>Ë†ğ‘˜)âˆ’ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡,â‰¤Ë†ğ‘˜(22)
=(ğ‘…>Ë†ğ‘˜+ğ‘„)+(ğ‘…â‰¤Ë†ğ‘˜âˆ’ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡,â‰¤Ë†ğ‘˜) (23)
=(ğ‘…>Ë†ğ‘˜+ğ‘„)+ğ‘œ(ğ‘…â‰¤Ë†ğ‘˜). (24)
We note that Ë†ğ‘…âˆ’ğ‘…ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡,â‰¤Ë†ğ‘˜is the residual after applying a single
iteration of the top- Ë†ğ‘˜atom selection algorithm. After at least ğ‘¡âˆ’1
applications of the algorithm to the residual matrix of the previous
step, the final residual matrix consists of fewer than Ë†ğ‘˜nonzero
atoms, plus Gaussian noise. This satisfies the hypotheses of our
theorem statement. Since the sparsity parameter ğ‘ is assumed to be
Î˜(1), the total accumulated error over all steps of the algorithm is
ğ‘œ(ğ‘…â‰¤Ë†ğ‘˜), which isğ‘œ(ğ‘…)in the Frobenius norm. Additional lemma-
supporting numerical experiments can be found in the extended
version [16].
B Reproducibility
B.1 LRMDS Solution Details
LRMDS has two key steps: atom selection and encoding. To perform
atom selection we quantify the alignment of each 2D atom with
the current residual via projection. We can compute ğ‘…â€™s projection
as follows:
ğ‘ƒğ‘–,ğ‘—=âŸ¨ğ‘…,ğµğ‘–,ğ‘—âŸ©
||ğµğ‘–,ğ‘—||ğ¹, (25)
whereâŸ¨ğ‘…,ğµğ‘–,ğ‘—âŸ©â‰œğœ“ğ‘‡
ğ‘–ğ‘…ğœ™ğ‘—is the alignment, and ||ğµğ‘–,ğ‘—||ğ¹is a nor-
malization based on the Frobenius norm of the 2D atom product.
Intuitively atoms of good alignment will be advantageous for encod-
ing the data. Instead of utilizing Eq. 25 in the algorithm for LRMDS
we perform the functionally equivalent projection via ğ‘ƒ=Ë†Î¨ğ‘‡ğ‘…Ë†Î¦
employing the normalized dictionaries Ë†Î¨and Ë†Î¦. Due to the normal-
ization, the denominator from Eq. 25 can be omitted since:
||ğµğ‘–,ğ‘—||ğ¹=||Ë†ğœ“ğ‘–||2Â·||Ë†ğœ™ğ‘—||2=1,âˆ€ğ‘–,ğ‘—.
This in turn allows to us utilize simply matrix multiplication Ë†Î¨ğ‘‡ğ‘…Ë†Î¦
to obtain alignment scores for atoms.
We then select the top ğ‘˜total atoms from a combination of left or
right dictionary atoms with respect to this alignment. To illustrate
the methodology, suppose ğ‘˜=3, and the top alignments correspond
toğ‘ƒ2,3,ğ‘ƒ3,3in descending order. We would then add the atoms ğœ“2,
ğœ™3,ğœ“3and in that order to our sub-dictionaries we call Î¨ğ‘ âˆˆRğ‘Ã—ğ¼ğ‘ 
andÎ¦ğ‘ âˆˆRğ‘€Ã—ğ½ğ‘ , whereğ¼ğ‘ ,ğ½ğ‘ are the number of selected atoms
from the left and the right dictionaries. It is important to note that
we only add atoms if they donâ€™t already exists in our selected sub-
dictionary. Importantly this may result in uneven selection from
the dictionaries (i.e ğ¼ğ‘ â‰ ğ½ğ‘ ). This is desirable as there may be signif-
icantly more complexity in one of ğ‘‹â€™s modes, necessitating moreatoms from the corresponding dictionary for good representation.
Intuitively, we let the data guide the selection on both sides. Ties
between atoms are resolved arbitrarily.
Once we have sub-selected the dictionary via these chosen atoms
we need to solve for the encoding coefficients in ğ‘Œandğ‘Šby solving
the following:
argmin
ğ‘Œ,ğ‘Š||ğ‘…âˆ’Î¨ğ‘ ğ‘Œğ‘ŠÎ¦ğ‘‡
ğ‘ ||2
ğ¹,(26)
To achieve this we iteratively alternate through solving for ğ‘Œ
andğ‘Šwhile the other is fixed. The updates in each case can be de-
rived by taking the gradients with respect to the non-fixed variable,
setting them to 0, and solving. This results in the following update
rules:
(1) Givenğ‘Š, the update rule for ğ‘Œis as follows:
Î¨â€ 
ğ‘ Î¨ğ‘ ğ‘Œğ‘ŠÎ¦ğ‘‡
ğ‘ (ğ‘ŠÎ¦ğ‘‡
ğ‘ )â€ =Î¨â€ 
ğ‘ ğ‘…(ğ‘ŠÎ¦ğ‘‡
ğ‘ )â€ 
ğ‘Œ=Î¨â€ 
ğ‘ ğ‘…(ğ‘ŠÎ¦ğ‘‡
ğ‘ )â€ ,(27)
whereâ€ denotes the pseudo-inverse of the corresponding matrix.
(2) Givenğ‘Œ, the update rule for ğ‘Šis:
(Î¨ğ‘ ğ‘Œ)â€ Î¨ğ‘ ğ‘Œğ‘ŠÎ¦ğ‘‡
ğ‘ (Î¦â€ 
ğ‘ )ğ‘‡=(Î¨ğ‘ ğ‘Œ)â€ ğ‘…(Î¦â€ 
ğ‘ )ğ‘‡
ğ‘Š=(Î¨ğ‘ ğ‘Œ)â€ ğ‘…(Î¦â€ 
ğ‘ )ğ‘‡(28)
Note that at every iteration, the update rules for the two variables
require four pseudo-inversions solved via singular value decompo-
sition with per-iteration complexity of ğ‘‚(ğ‘šğ‘–ğ‘›(ğ‘šğ‘›2,ğ‘š2ğ‘›)), where
ğ‘š,ğ‘› are the size of the target matrix. The dictionary inversions
Î¨â€ 
ğ‘ andÎ¦â€ 
ğ‘ can be computed only once per decomposition as they
are fixed with respect to ğ‘Œandğ‘Š. Thus, the overall complexity of
these steps assuming ğ‘>ğ¼ğ‘ , andğ‘€>ğ½ğ‘ isğ‘‚(ğ‘ğ¼2ğ‘ )forÎ¨â€ 
ğ‘ and
ğ‘‚(ğ‘€ğ½2ğ‘ )forÎ¦â€ 
ğ‘ . We need to compute (Î¨ğ‘ ğ‘Œ)â€ and(Î¦ğ‘ ğ‘Š)â€ for every
iteration, thus assuming ğ‘iterations to convergence the total com-
plexity of the coding step is ğ‘‚(ğ‘(ğ‘+ğ‘€)ğ‘Ÿ2+ğ‘€ğ½2ğ‘ +ğ‘ğ¼2ğ‘ )assuming
the selected decomposition rank is lower than the corresponding
data dimensions, i.e., ğ‘>ğ‘Ÿandğ‘€>ğ‘Ÿ.
We can further optimize the run time based on the assumption
that the inversions of both products (ğ‘ŠÎ¦ğ‘‡ğ‘ )â€ and(Î¨ğ‘ ğ‘Œ)â€ involve
matrices of full column (left matrix in the product) and row (right
matrix) rank. Then we can separate the inversions and open more
opportunities for savings by using the following matrix product
inversion rule due to [11]:
(ğ´ğµ)â€ =ğµâ€ ğ´â€ (29)
Updates from Eq. 27 and Eq. 28 can then be rewritten as:
ğ‘Œ=Î¨â€ 
ğ‘ ğ‘‹(Î¦â€ 
ğ‘ )ğ‘‡ğ‘Šâ€ (30)
ğ‘Š=ğ‘Œâ€ Î¨â€ 
ğ‘ ğ‘‹(Î¦â€ 
ğ‘ )ğ‘‡, (31)
where Î¨â€ 
ğ‘ ğ‘‹(Î¦â€ )ğ‘‡is a common term that can be pre-computed
outside of the iterative updates. This enables us to only need to
compute the pseudo-inversion of ğ‘Œandğ‘Šwithin the inner-loop.
ğ‘Œâ€ andğ‘Šâ€ have complexity ğ‘‚(ğ‘Ÿ2ğ¼ğ‘ )andğ‘‚(ğ‘Ÿ2ğ½ğ‘ )respectively. Re-
ducing the overall complexity is to ğ‘‚(ğ‘(ğ¼ğ‘ +ğ½ğ‘ )ğ‘Ÿ2+ğ‘€ğ½2ğ‘ +ğ‘ğ¼2ğ‘ )
which is linear with respect to the size of input matrix. We name
this faster LRMDS variation method LRMDS-f. Note that when
 
2116KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Boya Ma, Maxwell McNeil, Abram Magner, & Petko Bogdanov
Method Parameters Range Synthetic Convergence Ablation Twitch Road Wiki Covid
TGSD ğœ†1,ğœ†2[10âˆ’3,10âˆ’2,Â·Â·Â·,103] Vary Vary Vary Vary Vary Vary Vary
2D-OMP ğ‘‡0 3âˆ’100% #atoms 3.5% 100% NA 13% 40% 50% 50%
SC-TGSDğœ†0
ğœ†0ğ‘šğ‘ğ‘¥,ğœ†1,ğœ†2(0.1 : 0.01 : 0.9);[10âˆ’3,10âˆ’2,Â·Â·Â·,103] Vary NA NA Vary Vary Vary Vary
LRMDS ğ‘˜ [5,6,10,100,500] 5 10 6 500 100 100 100
LRMDS-f ğ‘˜ [5,6,10,100,500] 5 10 6 500 100 100 100
Table 2: Parameters for competing methods where ğœ†1,ğœ†2are sparsity parameters for TGSD; ğ‘‡0is the targeting number of coefficients for 2D-OMP;ğœ†0
ğœ†0ğ‘šğ‘ğ‘¥is the
regularizer for SC; ğ‘˜is the number of atoms selected per iteration for LRMDS and LRMDS-f. Some methods are not included in the convergence and ablation
experiments and the corresponding cells are marked as NA for Not Applicable. Ranges for tested values are listed in the Range column.
the conditions for Eq. 29 are not met, our encoding will be not as
accurate in this variant, but we demonstrate experimentally that
this alternative solver offers a good runtime-quality trade-off.
B.2 Datasets Generation and Pre-processing
Synthetic data generation: Unless otherwise noted in specific
experiments, the variables in our model for synthetic data are set
to the following: Î¨ğ‘ corresponds to 20randomly chosen atoms
from a GFT dictionary which itself is generated from a Stochastic
Block Model (SBM) graph with 3blocks of equal size and 1000 total
nodes and internal and cross-block edge probabilities set to 0.2and
0.02respectively. Î¦ğ‘ contains 20randomly selected atoms from a
Ramanujan periodic dictionary. The entries of ğ‘Œandğ‘Šare set to
uniformly random numbers between 0and1and the rank ğ‘Ÿis set
to3. Finally,ğœ–is Gaussian white noise with magnitude ensuring an
overallğ‘†ğ‘ğ‘…=10for the signal.
Real-world dataset: The original Twitch dataset specifies active
viewers over time and the streams that they are viewing. We create
a graph among viewers, where an edge between a pair of viewers
exists if they viewed the same stream at least 3 times over a period
of512hours (which is the temporal dimension of the dataset). The
largest connected component of this co-viewing graph involve
78,389viewers. Each entry of the data matrix ğ‘‹âˆˆR78389Ã—512from
Twitch represents the number of minutes in any given hour that the
viewer spent viewing streams on the platform. The Wiki dataset
captures hourly number of views of Wikipedia articles for a total of
792hours. We construct a graph among the articles by placing edges
between articles with at least 10pairwise (clicked by the same IPs)
click events within a day. Furthermore, we pick a starting node (the
Wikipedia article on China) and construct a breadth-first-search
(snowball) subgraph of 1000 nodes around it. We removed an article
that was not sufficiently active during the observed period resulting
in999total nodes. The Covid dataset tracks daily confirmed COVID
cases for 3047 counties in the US for 678days. We use a k-nearest
neighbor (ğ‘˜=5) spatial graph connecting counties to their closest
neighbors. The Road dataset consists of 1923 highway speed sensors
in the LA area, we use the hourly average speed for 30days as our
signal matrix, and the graph is based on connected road segments.
B.3 Hyper-parameter Settings
The parameter settings for all competing techniques unless other-
wise specified are as follows. We set the rank for low rank decom-
position methods (LRMDS, LRMDS-f, TGSD, SC-TGSD, LRMDS-1D,
RAND ) to be ğ‘Ÿ=3in synthetic (equal to the ground truth) andğ‘Ÿ=50in real-world datasets. For all real-world datasets, we set
the number of atoms per iteration for LRMDS and LRMDS-f to be
ğ‘˜=100for all experiments except for Twitch in which ğ‘˜=500
since this dataset is large and the input dictionaries have in total
close to 80,000atoms. In both synthetic and real world datasets,
we vary SC-TGSDâ€™s screening parameterğœ†0
ğœ†0ğ‘šğ‘ğ‘¥in the range of 0.1
to0.9with a step size of 0.01to create a set of regimes of selected
sub-dictionaries and associated RMSE/running time.
Controlling the number of selected atoms exactly for all com-
petitors is not trivial as TGSD and SC-TGSD employ sparsity regu-
larizers (ğœ†1,ğœ†2) that do not offer explicit control over the number of
used atoms. In order to facilitate direct but fair comparison we use
the ground truth number of selected atoms in synthetic datasets as
targets for 2D-OMP and LRMDS, and report results for TGSD and
SC-TGSD using sparsity levels resulting in atom â€œselectionâ€ closest
to (but exceeding) the ground truth number.
In our ablation study we set ğ‘˜1,ğ‘˜2=3for both LRMDS-1D and
RAND, and ğ‘˜=6for LRMDS. We do this so that the total number
of atoms selected by each method is 6at each iteration to facilitate
fair comparison. We re-run RAND 10times and report its average
performance in terms of RMSE and running time.
Complete details on how parameters were searched (i.e., ranges)
and set for each dataset can are listed in Tbl. 2.
We also performed a more detailed analysis of the effect of the
number of selected atoms ( ğ‘˜) and determined that it controls a
trade-off between quality and runtime. Given a total target (optimal
but unknown) number of atoms ğ‘˜âˆ—, whenğ‘˜<<ğ‘˜âˆ—our algorithm
requires more iterations to converge (involving multiple sparse
coding fits with increasing dictionaries). On the other hand, a larger
ğ‘˜similar toğ‘˜âˆ—will result in fewer iterations, however, the algorithm
may require more than ğ‘˜âˆ—atoms to achieve the same RMSE. For
example, if two â€œgoodâ€ atoms are similar, then the projections on
them will also be similar and high-valued and they will both be
selected although potentially redundant. We chose a middle-ground
k for our experiments.
CFurther experimental analysis and discussion
Additional material is available in the extended version [ 16] includ-
ing: (i) extra numerical experiments supporting the lemmas; (ii)
baseline solution details; (iii) extra experiments and figures, includ-
ing synthetic data analysis for varying noise level and dictionary
sizes and the corresponding representation quality vs runtime; and
(iv) discussion of generalization to multi-way data.
 
2117