PolyFormer: Scalable Node-wise Filters via Polynomial Graph
Transformer
Jiahong Ma
Renmin University of China
Beijing, China
jiahong_ma@ruc.edu.cnMingguo He
Renmin University of China
Beijing, China
mingguo@ruc.edu.cnZhewei Weiâˆ—
Renmin University of China
Beijing, China
zhewei@ruc.edu.cn
Abstract
Spectral Graph Neural Networks have demonstrated superior per-
formance in graph representation learning. However, many current
methods focus on employing shared polynomial coefficients for all
nodes, i.e., learning node-unified filters, which limits the filtersâ€™ flex-
ibility for node-level tasks. The recent DSF attempts to overcome
this limitation by learning node-wise coefficients based on posi-
tional encoding. However, the initialization and updating process
of the positional encoding are burdensome, hindering scalability on
large-scale graphs. In this work, we propose a scalable node-wise
filter, PolyAttn. Leveraging the attention mechanism, PolyAttn
can directly learn node-wise filters in an efficient manner, offering
powerful representation capabilities. Building on PolyAttn, we in-
troduce the whole model, named PolyFormer. In the lens of Graph
Transformer models, PolyFormer, which calculates attention scores
within nodes, shows great scalability. Moreover, the model captures
spectral information, enhancing expressiveness while maintaining
efficiency. With these advantages, PolyFormer offers a desirable
balance between scalability and expressiveness for node-level tasks.
Extensive experiments demonstrate that our proposed methods
excel at learning arbitrary node-wise filters, showing superior per-
formance on both homophilic and heterophilic graphs, and handling
graphs containing up to 100 million nodes. The code is available at
https://github.com/air029/PolyFormer.
CCS Concepts
â€¢Computing methodologies â†’Artificial intelligence.
Keywords
Graph Filter, Graph Transformer, Graph Neural Network
ACM Reference Format:
Jiahong Ma, Mingguo He, and Zhewei Wei. 2024. PolyFormer: Scalable
Node-wise Filters via Polynomial Graph Transformer. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
âˆ—Zhewei Wei is the corresponding author. The work was partially done at Gaoling
School of Artificial Intelligence, Beijing Key Laboratory of Big Data Management and
Analysis Methods, MOE Key Lab of Data Engineering and Knowledge Engineering,
and Pazhou Laboratory (Huangpu), Guangzhou, Guangdong 510555, China.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671849(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671849
1 Introduction
Graph Neural Networks (GNNs) have emerged as a powerful tool
for addressing a variety of graph-related tasks, including node
classification [ 17,25,41], link prediction [ 49,51], and graph clas-
sification [ 47]. GNNs are generally classified into two categories:
spatial-based and spectral-based [ 18]. Spatial-based GNNs utilize
message passing in the spatial domain for information aggregation,
while spectral-based GNNs employ graph filtering operations in
the spectral domain. Recently, Graph Transformer, a variant of the
Transformer architecture [ 40] adapted for graph data following
its success in natural language processing [ 7,12], computer vision
[10,30], and audio processing [ 8,14], has demonstrated superior
performance in graph representation learning in both spatial and
spectral manners [27, 32, 35, 48].
Spectral GNNs have shown improved performance across vari-
ous graph tasks, especially at the node level [ 4,16,18,19]. These
approaches utilize different polynomial bases to approximate graph
convolution and perform graph filtering on raw node signals. De-
spite their advancements, most models adopt shared polynomial
coefficients for all nodes, leading to node-unified filters. Alterna-
tively, learning specific filters for each node, or node-wise filters,
offers greater flexibility [ 21]. Though efforts of employing differ-
ent filters for different signal channels [ 16,42] have been made,
limitations with node-unified filters persist. A naive approach to
learning node-wise filters involves excessive parameters, such as
different polynomial coefficients for each node, leading to scal-
ability issues. Besides, the learning process lacks generalization
across nodes. To address these challenges, DSF [ 15] has attempted
to employ a shared network on positional encoding, i.e., random
walk positional encoding or Laplacian eigenvectors encoding, to
learn node-wise polynomial coefficients, demonstrating the superi-
ority of node-wise filters over node-unified filters. Nonetheless, the
positional encoding update process is computationally intensive,
with a complexity of ğ‘‚(ğ‘2), and its initialization, like obtaining
Laplacian eigenvectors, also costs expensively with a complexity
ofğ‘‚(ğ‘3), whereğ‘represents the number of nodes. These hinder
DSFâ€™s scalability for large graphs. Moreover, the performance of
DSF heavily depends on the initial position encoding, presenting
another limitation. This raises the question: Is it possible to design a
scalable and efficient node-wise filter?
In this work, we provide a positive answer to this question. We
introduce polynomial attention PolyAttn, an attention-based node-
wise filter. Our approach begins with the formulation of polynomial
node tokens, a concept analogous to tokens in a sentence or patches
 
2118
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahong Ma, Mingguo He, and Zhewei Wei
in an image. Through recursive calculation, these tokens are com-
puted efficiently with complexity ğ‘‚(ğ¾|ğ¸|), whereğ¾signifies the
truncated order of the polynomial basis which is often small in
practice, and|ğ¸|represents the number of edges in the graph. Once
computed, these polynomial tokens can be reused across all train-
ing and inference stages, thereby enhancing efficiency. By applying
attention to the ğ¾+1polynomial tokens associated with each node,
PolyAttn is able to learn node-wise filters in a mini-batch manner
efficiently, resulting in a total complexity of ğ‘‚((ğ¾+1)2ğ‘), which
ensures scalability. Additionally, the utilization of shared attention
networks addresses issues related to excessive parameters and lack
of generalization, while still maintaining the flexibility of node-
wise filters. Through both theoretical and empirical analyses, we
demonstrate that PolyAttn provides superior expressiveness over
node-unified filters. Building upon this foundation of node-wise
filters, we further develop the whole model PolyFormer.
In the lens of Graph Transformers, PolyFormer is quite dif-
ferent from previous methods. PolyFormer implements attention
on the polynomial tokens of each node, while former Graph Trans-
formers generally calculate attention scores on nodes and then use
the derived node representations for downstream tasks like node
classification or graph regression [ 33,39]. However, it is worth
considering whether it is necessary to implement attention on node
for both node-level and graph-level tasks. In natural language pro-
cessing and computer vision, Transformer-based models mainly
consider the interactions among tokens within a sentence or patches
within an image, respectively, rather than implementing attention
mechanisms between sentences or images [ 7,10,40]. In other words,
these attention mechanisms are typically employed on the sub-units
that constitute the target object, rather than the target object itself.
By capturing the information exchanges among these sub-units,
the attention mechanism derives the representation of the target
object. When it comes to Graph Transformers for node-level tasks,
it is intuitive to develop an attention mechanism on tokens of each
node rather than on nodes. In fact, for Graph Transformers, ap-
plying an attention mechanism on nodes in node-level tasks can
potentially lead to several limitations. Firstly, calculating attention
score on nodes leads to quadratic computational complexity with
respect to the number of nodes [ 40], which poses challenges in
terms of efficiency and scalability. Secondly, attending to all nodes
in the graph neglect the sparse connectivity of the graph, which
is a crucial inductive bias for graph structures. This has a detri-
mental impact on the performance of the Graph Transformer [ 11].
Although various works [ 26,28,43,50] have attempted to address
these limitations through strategies such as sampling, coarsening,
or adopting a more efficient attention mechanism, striking a bal-
ance between scalability and performance (e.g., accuracy) remains
a challenge.
By implementing attention-based node-wise filter PolyAttn on
the proposed polynomial token, PolyFormer offers several ad-
vantages over existing graph transformers. Firstly, it processes
attention within nodes rather than between nodes, enhancing scala-
bility while keeping superior performance. By adopting node tokens
and implementing attention mechanisms on ğ¾+1tokens for each
node instead of focusing on all nodes, the proposed model reduces
computational complexity from ğ‘‚(ğ‘2)toğ‘‚((ğ¾+1)2ğ‘), thereby im-
proving scalability. Whatâ€™s more, PolyFormer supports mini-batchtraining, enhancing scalability further. Itâ€™s worth noting that recent
work NAGphormer [ 3] has attempted to represent each node using
information from various hops as units to calculate attention on
these units. However, these units are designed based on the spatial
domain, neglecting the spectral information, which compromises
performance [ 27]. In contrast, with the expressive node-wise filters
PolyAttn, PolyFormer is able to maintain superior performance
on large-scale graphs. Secondly, PolyFormer incorporates spectral
information efficiently and demonstrates exceptional expressive power.
Previous Graph Transformers have highlighted the importance of
spectral information for improving modelsâ€™ performance [ 2,27,35],
but they rely on eigendecomposition of complexity ğ‘‚(ğ‘3), which
is computationally demanding and memory-intensive. Our model,
however, utilizes spectral information through polynomial approxi-
mation, offering both high expressiveness and efficiency. Extensive
experiments have empirically validated PolyFormerâ€™s performance,
efficiency, and scalability advantages.
We summarize the contributions of this paper as follows:
â€¢We introduce a node-wise filter through a tailored attention
mechanism, termed PolyAttn. By leveraging polynomial-
based node tokens derived from the spectral domain, Poly-
Attn achieves both scalability and expressiveness. Utilizing
these node tokens in conjunction with PolyAttn, we pro-
pose PolyFormer, serving as a scalable and expressive Graph
Transformer for node-level tasks.
â€¢Theoretically, we demonstrate that PolyAttn functions as
a node-wise filter with the designed node token. We also
illustrate that multi-head PolyAttn serves as a multi-channel
filter. Moreover, we explore the computational complexity
tied to the proposed node token, PolyAttn and PolyFormer.
â€¢Comprehensive experiments validate that PolyAttn possesses
greater expressive power than node-unified filters. Build-
ing on PolyAttn, PolyFormer achieves a desirable balance
between expressive power and scalability. It demonstrates
superior performance on both homophilic and heterophilic
datasets and is capable of handling graphs with up to 100
million nodes.
2 Background
2.1 Notations
Consider an undirected graph ğº=(ğ‘‰,ğ¸), whereğ‘‰is the set of
nodes andğ¸is the set of edges. The adjacency matrix is denoted
asAâˆˆ{0,1}ğ‘Ã—ğ‘, where Ağ‘–ğ‘—=1signifies the existence of an edge
between nodes ğ‘£ğ‘–andğ‘£ğ‘—, andğ‘is the total number of nodes in ğº.
The degree matrix Dis a diagonal matrix where Dğ‘–ğ‘–=Ã
ğ‘—Ağ‘–ğ‘—.
The normalized Laplacian of the graph is then defined as Ë†L=
Iâˆ’Ë†A=Iâˆ’Dâˆ’1/2ADâˆ’1/2. In these equations, Irepresents the identity
matrix, Ë†Adenotes the normalized adjacency matrix, which is ob-
tained by scaling the adjacency matrix with the degree matrix. It is
well-established that Ë†Lis a symmetric positive semidefinite matrix,
allowing for decomposition as Ë†L=UÎ›UâŠ¤=Udiag(ğœ†0,...,ğœ†ğ‘âˆ’1)UâŠ¤.
Here, Î›is a diagonal matrix composed of real eigenvalues ğœ†ğ‘–âˆˆ[0,2],
whereğ‘–âˆˆ{0,...,ğ‘âˆ’1}, and Uconsists ofğ‘corresponding eigen-
vectors that are orthonormal, i.e., U={ğ’–0,..., ğ’–ğ‘âˆ’1}.
Further, we use ğ’™âˆˆRğ‘to denote the graph signal vector and
useXâˆˆRğ‘Ã—ğ‘‘to denote the graph signal matrix or, equivalently,
 
2119PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
node feature matrix. The term ğ’šâˆˆRğ‘is used to denote the node
label of the graph. Typically, in a homophilic graph, node labels
between neighbors tend to be the same, whereas in a heterophilic
graph, labels between neighbors tend to be different.
2.2 Graph Filter
Graph filter serves as a crucial concept in the field of graph signal
processing [ 21]. In graph filtering, graph signals are transformed
from the spatial domain to the spectral domain, analogous to how
the Fourier transformation converts signals from the time domain
to the frequency domain. This process allows for filtering signals
on the spectral domain, facilitating the extraction of specific signal
components.
Graph Signal Filtering. Formally, given an original graph sig-
nalğ’™âˆˆRğ‘, the filtered signal ğ’›âˆˆRğ‘is obtained through a graph
filtering operation in the spectral domain, expressed as:
ğ’›=Uâ„(Î›)UâŠ¤ğ’™. (1)
In the equation above, graph signals ğ’™are first projected into the
spectral domain via the Graph Fourier Transform :Ë†ğ’™=UâŠ¤ğ’™âˆˆRğ‘,
employing the basis of frequency components U={ğ’–0,..., ğ’–ğ‘âˆ’1}.
Here, Ë†ğ’™denotes the frequency response of the original signal on the
basis of frequency components in the spectral domain. The graph fil-
terâ„(Â·)then modulates the intensity of each frequency component
throughâ„(Î›)UâŠ¤ğ’™. Subsequently, the filtered signal is transformed
back into the spatial domain using the Inverse Fourier Transform,
that is, ğ’›=Uâ„(Î›)UâŠ¤ğ’™. These processes are also applicable to the
node feature matrix XâˆˆRğ‘Ã—ğ‘‘:
Z=Uâ„(Î›)UâŠ¤X. (2)
It is worth noting that learning graph filters on Î›necessitates Lapla-
cian eigendecomposition , a process that decomposes the Laplacian
matrix in the form of L=UÎ›UâŠ¤, which has a time complexity
ofğ‘‚(ğ‘3). This significantly hinders efficiency and scalability on
large-scale graphs.
Node-wise and Channel-wise Filters. Considering node sig-
nal matrix XâˆˆRğ‘Ã—ğ‘‘, graph filters can be expanded into more
flexible and expressive forms: channel-wise andnode-wise. Specifi-
cally,â„(Â·)is considered channel-wise if there exists a correspond-
ingâ„(ğ‘—)(Â·)for each signal channel X:,ğ‘—, whereğ‘—âˆˆ{0,...,ğ‘‘âˆ’1}.
AdaGNN [ 9] learns different filters for each feature channel. Simi-
larly, JacobiConv [ 42] and OptBasisGNN [ 16] also implement multi-
channel filters on their proposed bases. Conversely, graph filters
are node-wise when â„(Â·)is tailored for individual nodes, denoted
asâ„(ğ‘–)(Â·)for nodeğ‘£ğ‘–. DSF [ 15] implements a learnable network on
the positional encoding to derive node-wise polynomial coefficients
and shows enhanced performance of node-wise filters. However,
the high overhead of initializing and updating the positional en-
coding poses a challenge to extending this approach to large-scale
graphs.
Polynomial GNNs. To alleviate the computational burden as-
sociated with eigendecomposition, recent studies have introduced
polynomial GNNs to approximate â„(Î›)based on various polyno-
mial bases. ChebNet [ 6] adopts the Chebyshev basis to approximate
the filtering operation, while GCN [ 25] simplifies the ChebNet by
limiting the Chebyshev basis to the first order. GPRGNN [ 4] learns
filters based on the Monomial basis, while BernNet [ 18] utilizes theBernstein basis, providing enhanced interpretability. Furthermore,
ChebNetII [ 19] revisits the Chebyshev basis and constrains the
coefficients through Chebyshev interpolation, showing minimax
polynomial approximation property of truncated Chebyshev expan-
sions. JacobiConv [ 42] further filters based on the family of Jacobi
polynomial bases, with the specific basis determined by hyperpa-
rameters. [ 16] introduces FavardGNN, which employs a learnable
orthonormal basis for a given graph and signal, and OptBasisGNN,
which utilizes an optimal basis with superior convergence proper-
ties.
Generally, using a specific polynomial basis, the approximated
filtering operation can be represented as:
Z=Uâ„(Î›)UâŠ¤Xâ‰ˆğ¾âˆ‘ï¸
ğ‘˜=0ğ›¼ğ‘˜ğ‘”ğ‘˜(P)X, (3)
whereğ›¼ğ‘˜are the polynomial coefficients for all nodes, ğ‘”ğ‘˜(Â·),ğ‘˜âˆˆ
{0,...,ğ¾}denotes a series polynomial basis of truncated order ğ¾,
andPrefers to either the normalized adjacency matrix Ë†Aor the
normalized Laplacian matrix Ë†L. For example, the filtering operation
of GPRGNN [ 4] isZ=Ãğ¾
ğ‘˜=0ğ›¼ğ‘˜Ë†Ağ‘˜X, which uses the Monomial
basis.
2.3 Transformer
The Transformer architecture [ 40] is a powerful deep learning
model that has shown a significant impact in multiple fields, includ-
ing natural language processing [ 7,12], computer vision [ 10,30,45],
audio applications [8, 14] and even graph learning [33, 39].
Attention Mechanism. The critical component of the Trans-
former is its attention mechanism, which calculates pair-wise in-
teractions between input tokens. Typically, for an input matrix
X=[ğ’™1,..., ğ’™ğ‘›]âŠ¤âˆˆRğ‘›Ã—ğ‘‘withğ‘›tokens, the attention mecha-
nism transforms Xinto Q,K,Vvia learnable projection matrices
Wğ‘„âˆˆRğ‘‘Ã—ğ‘‘â€²,Wğ¾âˆˆRğ‘‘Ã—ğ‘‘â€², and Wğ‘‰âˆˆRğ‘‘Ã—ğ‘‘â€²as:
Q=XWğ‘„,K=XWğ¾,V=XWğ‘‰. (4)
The output of the attention mechanism is then computed as:
O=softmaxQKâŠ¤
âˆš
ğ‘‘
V. (5)
This attention mechanism can be executed multiple times to im-
plement a multi-head attention mechanism. It is important to note
that the complexity of the attention mechanism is quadratic with
respect to the input sequence length, denoted as ğ‘‚(ğ‘›2ğ‘‘â€²).
Graph Transformer. The Transformer architecture, adapted
to graph learning and termed Graph Transformers, has garnered
significant attention in recent years [ 33,39]. When adapting Trans-
formers to graph learning, nodes of the graph are typically treated as
input tokens for the model. However, the computational overhead
of the attention mechanism, which scales quadratically with the
number of nodes, restricts the application of Graph Transformers
on large-scale graphs.
Various solutions have been proposed to address the scalability
limitation of Graph Transformers. (1) Adopting Efficient attention
mechanisms enhances scalability. Nodeformer [ 43] employs exist-
ing efficient attention [ 5] with Gumbel-Softmax [ 23] for large-scale
graphs, while SGFormer [ 44] optimizes scalability by substituting
 
2120KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahong Ma, Mingguo He, and Zhewei Wei
softmax attention with proposed linear attention. (2) Node coars-
ening, as another strategy, involves simplifying graph structures.
Coarformer [ 28] employs graph coarsening algorithms [ 31,36] to
obtain coarsened nodes, thereby enabling attention within and
across coarsened and local nodes. GOAT [ 26] utilizes the expo-
nential moving average strategy and K-Means [ 22] to integrate
global information into codebooks and implement attention on
them. Additionally, ANS-GT [ 50] utilizes coarsened nodes for atten-
tion processes. (3) Sampling strategies, such as those employed by
ANS-GT [ 50] with various sampling heuristics (e.g., 1-hop neigh-
bors, Personalized PageRank), reduce the computational demand
by selecting specific nodes for attention calculations. Despite these
approaches, balancing scalability with performance in Graph Trans-
formers remains an ongoing challenge.
3 PolyFormer
In this section, we introduce our proposed node-wise filter PolyAttn
and the whole model PolyFormer, which serves as a scalable and
expressive Graph Transformer. First, we define the concept of node
tokens based on polynomial bases. Utilizing these node tokens,
we describe our attention-based node-wise filter and provide an
overview of the whole model. Finally, we analyze the computational
complexity of our methods and illustrate its relationship with graph
filters.
3.1 Polynomial Token
Analogous to sentence tokenization in natural language processing,
we introduce polynomial tokens.
Definition 3.1. (Polynomial Token) For any node ğ‘£ğ‘–in a
graphğº=(ğ‘‰,ğ¸), the polynomial token of the node is defined as
ğ’‰(ğ‘–)
ğ‘˜=(ğ‘”ğ‘˜(P)X)ğ‘–,:âˆˆRğ‘‘,ğ‘˜âˆˆ{0,...,ğ¾}, whereğ‘”ğ‘˜(Â·)represents a
polynomial basis of order ğ‘˜,Pis either Ë†AorË†L, and Xrepresents the
node features.
In this work, we employ Monomial, Bernstein, Chebyshev, and
the optimal bases for polynomial tokens. These choices offer ease
of implementation compared to some more complex polynomial
bases such as Jacobi basis. Additionally, these bases provide good
properties. the Monomial basis provides a clear spatial interpre-
tation, with ğ’‰(ğ‘–)
ğ‘˜=(Ë†Ağ‘˜X)ğ‘–,:representing the information of the
ğ‘˜-hop neighborhood from node ğ‘£ğ‘–, while the Bernstein basis co-
efficients are highly correlated with the spectral property of the
filer, providing good interpretability. The Chebyshev basis exhibits
excellent fitting capabilities [ 13], while the optimal basis provides
the best converge property [ 16]. Table 1 illustrates the computing
process of polynomial tokens for all nodes in the graph, where
Hğ‘˜=[ğ’‰(0)
ğ‘˜,..., ğ’‰(ğ‘âˆ’1)
ğ‘˜]âŠ¤âˆˆRğ‘Ã—ğ‘‘denotes the matrix consisting
of polynomial tokens of order ğ‘˜forğ‘˜âˆˆ{0,...,ğ¾}.
The adoption of polynomial tokens presents several distinct
advantages. Firstly, these tokens can be computed efficiently. As
1ğ‘”0(Ë†L)=I,ğ‘”1(Ë†L)=Ë†L
2ğ‘”âˆ’1(Ë†A)=0,ğ‘”0(Ë†A)=I/||X||. According to [ 16], the optimal bases differs on each
channel of X.ğ›¾ğ‘˜andğ›½ğ‘˜here, and thus the ğ‘˜-order output basis, are determined by A
andX:,ğ‘—specifically.Table 1: Computing process of polynomial tokens for differ-
ent bases.
T
ype Basisğ‘”ğ‘˜ Token
Monomial ğ‘”ğ‘˜(Ë†A)=Ë†Ağ‘”ğ‘˜âˆ’1(Ë†A),
ğ‘”0(Ë†A)=I Hğ‘˜=ğ‘”ğ‘˜(Ë†A)X
Bernstein ğ‘”ğ‘˜(Ë†L)=1
2ğ¾ ğ¾
ğ‘˜(2Iâˆ’Ë†L)ğ¾âˆ’ğ‘˜(Ë†L)ğ‘˜Hğ‘˜=ğ‘”ğ‘˜(Ë†L)X
Chebyshe
v ğ‘”ğ‘˜(Ë†L)=2Ë†Lğ‘”ğ‘˜âˆ’1(Ë†L)âˆ’ğ‘”ğ‘˜âˆ’2(Ë†L)1Hğ‘˜=ğ‘”ğ‘˜(Ë†L)X
Optimal ğ›½ğ‘˜ğ‘”ğ‘˜(Ë†A)=(Ë†Aâˆ’ğ›¾ğ‘˜âˆ’1I)ğ‘”ğ‘˜âˆ’1(Ë†A)âˆ’ğ›½ğ‘˜âˆ’1ğ‘”ğ‘˜âˆ’2(Ë†A)2(Hğ‘˜):,ğ‘—=(ğ‘”ğ‘˜(Ë†A))X:,ğ‘—
shown in Table 1, most proposed polynomial tokens can be com-
puted recursively, which is an efficient manner. More detailed com-
plexity analysis is illustrated in 3.3.1. Once computed, they can be
reused across each epoch during both the training and inference
phases, leading to substantial reductions in computational time
and memory usage. Furthermore, by incorporating the normalized
adjacency or Laplacian matrix Pinto the computational process,
graph topology information is integrated into node tokens. This
integration eliminates the necessity for additional positional or
structural encodings, such as Laplacian eigenvectors, thereby en-
hancing the modelâ€™s efficiency. Finally, the node-wise independence
of these polynomial tokens allows for mini-batch training, enabling
our model to scale to graphs with up to 100 million nodes.
3.2 PolyAttn and PolyFormer
Given the polynomial tokens associated with each node, Poly-
Former employs an expressive attention-based node-wise filter
PolyAttn to generate node representations. Firstly, we introduce
the proposed attention mechanism PolyAttn, which is tailored for
polynomial tokens and performs as a node-wise filter. Subsequently,
we detail the comprehensive architecture of PolyFormer.
PolyAttn. In this section, we first detail the process of the pro-
posed PolyAttn for a given node ğ‘£ğ‘–. We define the token matrix
H(ğ‘–)for nodeğ‘£ğ‘–asH(ğ‘–)=[ğ’‰(ğ‘–)
0,..., ğ’‰(ğ‘–)
ğ¾]âŠ¤âˆˆR(ğ¾+1)Ã—ğ‘‘. Initially,
the value matrix Vis initialized with the token matrix H(ğ‘–). Subse-
quently, an order-specific multi-layer perceptron (MLP ğ‘—) maps the
ğ‘—-th order token ğ’‰(ğ‘–)
ğ‘—=H(ğ‘–)
ğ‘—,:into a hidden space. This mapping cap-
tures unique contextual information for each order of polynomial
tokens.
Then the query matrix Qand the key matrix Kare obtained by
projecting H(ğ‘–)through the learnable matrices Wğ‘„andWğ¾, respec-
tively. These matrices compute the attention scores. Notably, our at-
tention mechanism utilizes the hyperbolic tangent function tanh(Â·)
rather than the softmax function commonly used in the vanilla
Transformer [ 40]. This modification addresses the limitations of
the softmax function in expressing the capabilities of PolyAttn as a
node-wise graph filter, as detailed in Proposition 3.2. Additionally,
we introduce a node-shared attention bias ğœ·âˆˆRğ¾+1to balance
node-specific and global patterns. The attention scores Sare then
used to update the value matrix V, yielding the final output repre-
sentations. The pseudocode for PolyAttn is outlined in Algorithm 1.
For enhanced expressive power, multi-head PolyAttn is utilized in
practice, with further details available in Appendix C.
PolyFormer. Building upon the attention mechanism designed
for polynomial tokens, we introduce the whole model PolyFormer.
As illustrated in Figure 1, PolyFormer block is described by the
following equations:
 
2121PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 1: Illustration of the proposed PolyFormer. For a given graph, polynomial tokens for each node are computed. These
tokens are subsequently processed by PolyFormer, which consists of ğ¿blocks. Notably, with the defined polynomial token,
PolyAttn within each block functions as a node-wise filter in the spectral domain, adaptively learning graph filter specific to
each node.
Hâ€²(ğ‘–)=PolyAttn
LN
H(ğ‘–)
+H(ğ‘–), (6)
H(ğ‘–)=FFN
LN
Hâ€²(ğ‘–)
+Hâ€²(ğ‘–). (7)
Here, LN denotes Layer Normalization, which is implemented
before PolyAttn [ 46]. FFN refers to the Feed-Forward Network.
Upon obtaining the token matrix H(ğ‘–)âˆˆR(ğ¾+1)Ã—ğ‘‘for nodeğ‘£ğ‘–
throughğ¿PolyFormer blocks, the final representation Zğ‘–,:âˆˆRğ‘of
nodeğ‘£ğ‘–is computed as:
Zğ‘–,:=ğœ  ğ¾âˆ‘ï¸
ğ‘˜=0H(ğ‘–)
ğ‘˜,:!
W1!
W2, (8)
whereğœdenotes the activation function. The matrices W1âˆˆ
Rğ‘‘Ã—ğ‘‘â€²andW2âˆˆRğ‘‘â€²Ã—ğ‘are learnable, with ğ‘‘,ğ‘‘â€²representing the
hidden dimensions and ğ‘representing the number of node classes.
3.3 Theoretical Analysis
3.3.1 Complexity. Here, we analyze the complexity of computing
polynomial tokens, PolyAttn and PolyFormer.
Computing for Polynomial Tokens. As previously discussed,
the polynomial tokens of Monimial, Chenyshev and the optimal
basis can be calculated recursively. Each iteration for all nodes
involves sparse multiplication with a computational complexity
ofğ‘‚(|ğ¸|). Thus, the overall complexity is ğ‘‚(ğ¾|ğ¸|), whereğ¾is the
truncated order of the polynomial tokens, and |ğ¸|is the number
of edges in the graph. It is worth noting that though Bernstein
polynomial tokens are computed in the complexity of ğ‘‚(ğ¾2|ğ¸|), it is
still efficient as ğ¾is small in practice. Importantly, these polynomial
tokens can be computed once and reused throughout the training
and inference process.Algorithm 1: Pseudocode for PolyAttn
Input: Token matrix for node ğ‘£ğ‘–:
H(ğ‘–)=[ğ’‰(ğ‘–)
0,..., ğ’‰(ğ‘–)
ğ¾]âŠ¤âˆˆR(ğ¾+1)Ã—ğ‘‘
Output: New token matrix for node ğ‘£ğ‘–:Hâ€²(ğ‘–)âˆˆR(ğ¾+1)Ã—ğ‘‘
Learnable Parameters: Projection matrix
Wğ‘„,Wğ¾âˆˆRğ‘‘Ã—ğ‘‘â€²,
order-wise MLP ğ‘—(ğ‘—=0,...,ğ¾),
attention bias ğœ·âˆˆRğ¾+1
1Initialize Vwith H(ğ‘–)
2forj =0toğ¾do
3 H(ğ‘–)
ğ‘—,:â†MLPğ‘—(H(ğ‘–)
ğ‘—,:)
4Qâ†H(ğ‘–)Wğ‘„via projection matrix Wğ‘„;Kâ†H(ğ‘–)Wğ¾via
projection matrix Wğ¾
5Compute attention scores Sâ†tanh(QKâŠ¤)âŠ™B, where
Bğ‘–ğ‘—=ğœ·ğ‘—
6Hâ€²(ğ‘–)â†SV
7return Hâ€²(ğ‘–)# The representation of node ğ‘£ğ‘–after
PolyAttn is ğ’ğ‘–,:=Ãğ¾
ğ‘˜=0Hâ€²(ğ‘–)
ğ‘˜,:âˆˆRğ‘‘.
Complexity of PolyAttn and PolyFormer. Letğ‘‘denote the
hidden dimension of polynomial tokens, and ğ¾represent the trun-
cated order. In the context of one layer of PolyAttn, each node
involves(ğ¾+1)polynomial tokens in attention computation, re-
sulting in a complexity of ğ‘‚((ğ¾+1)2ğ‘‘). Withğ‘nodes in the graph
andğ¿layers of attention mechanisms, the total time complexity
isğ‘‚(ğ¿ğ‘(ğ¾+1)2ğ‘‘). Notably, in practical situations where ğ¾â‰ªğ‘,
this signifies a substantial reduction in computational complexity,
especially when compared to the ğ‘‚(ğ¿ğ‘2ğ‘‘)complexity of vanilla
Transformer models.
 
2122KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahong Ma, Mingguo He, and Zhewei Wei
Table 2: Performance of PolyAttn on synthetic datasets, presented as ğ‘…2score (with values closer to 1 indicating better
performance) and the sum of squared errors (with values closer to 0 indicating higher accuracy).
Mo
del (5k para.) Mixed low-pass Mixed high-pass Mixed band-pass Mixed rejection-pass Low&high-pass Band&rejection-pass
GCN
0.9953/2.0766 0.0186/39.6157 0.1060/14.0738 0.9772/10.9007 0.6315/86.8209 0.8823/128.2312
GAT 0.9954/2.0451 0.0441/38.5851 0.0132/14.0375 0.9775/10.7512 0.7373/61.8909 0.9229/83.9671
GPRGNN 0.9978/0.9784 0.9806/0.7846 0.9088/1.2977 0.9962/1.8374 0.8499/35.3719 0.9876/13.4890
BernNet 0.9976/1.0681 0.9808/0.7744 0.9231/1.0937 0.9968/1.5545 0.8493/35.5144 0.9875/13.6485
ChebNetII 0.9980/0.8991 0.9811/0.7615 0.9492/0.7229 0.9982/0.8610 0.8494/35.4702 0.9870/14.1149
PolyAttn (Mono) 0.9994/0.2550 0.9935/0.2631 0.9030/1.3798 0.9971/1.4025 0.9997/0.0696 0.9992/0.8763
Poly
Attn (Bern) 0.9998/0.0842 0.9972/0.1120 0.9809/0.2719 0.9993/0.3337 0.9852/3.4956 0.9882/12.8274
PolyAttn (Opt) 0.9996/0.1922 0.9997/0.0103 0.9951/0.0701 0.9995/0.2275 0.9978/0.5136
0.9992/0.8929
PolyAttn (Cheb) 0.9997/0.1467 0.9960/0.0148 0.9945/0.0782 0.9996/0.1949
0.9999/0.0118 0.9999/0.0416
3.3.2 Connection to Spectral Filtering. To understand the connec-
tion between PolyAttn and graph filters, we give the following
theorem and propositions. All proofs are in Appendix B. First, we
formally propose that PolyAttn serves as a node-wise filter for
polynomial tokens.
Theorem 3.1. With polynomial tokens as input, PolyAttn oper-
ates as a node-wise filter. Specifically, for the representation Zğ‘–,:=Ãğ¾
ğ‘˜=0Hâ€²(ğ‘–)
ğ‘˜,:of nodeğ‘£ğ‘–after applying PolyAttn:
Zğ‘–,:=ğ¾âˆ‘ï¸
ğ‘˜=0Hâ€²(ğ‘–)
ğ‘˜,:=ğ¾âˆ‘ï¸
ğ‘˜=0ğ›¼(ğ‘–)
ğ‘˜(ğ‘”ğ‘˜(P)X)ğ‘–,:. (9)
Here, the coefficients ğ›¼(ğ‘–)
ğ‘˜depend not only on the polynomial order ğ‘˜
but also on the specific node ğ‘£ğ‘–. In other words, PolyAttn performs
a node-wise polynomial filter on the graph signals.
Theorem 3.1 builds the bridge between the proposed atten-
tion and node-wise filters. Building on this, we further propose
that the multi-head PolyAttn acts as a multi-channel filter.
Proposition 3.1. A multi-head PolyAttn with â„heads can be
interpreted as partitioning the node representation into â„channel
groups with dimension ğ‘‘â„=ğ‘‘
â„and applying filtering to each group
separately. Formally:
Zğ‘–,ğ‘:ğ‘=ğ¾âˆ‘ï¸
ğ‘˜=0ğ›¼(ğ‘–)
(ğ‘,ğ‘)ğ‘˜(ğ‘”ğ‘˜(P)X)ğ‘–,ğ‘:ğ‘. (10)
Here,ğ›¼(ğ‘–)
(ğ‘,ğ‘)ğ‘˜denotes the coefficient for order ğ‘˜on channels ğ‘toğ‘of
nodeğ‘£ğ‘–â€™s representation, where (ğ‘,ğ‘)=(ğ‘—Ã—ğ‘‘â„,(ğ‘—+1)Ã—ğ‘‘â„âˆ’1),ğ‘—âˆˆ
{0,...,â„âˆ’1}.
It is worth noting that our chosen activation function, tanh(Â·),
enables PolyAttn with more powerful expressiveness than the soft-
max function.
Proposition 3.2. For PolyAttn, which operates as a graph filter,
the tanh function endows it with enhanced expressiveness, whereas
the softmax function can limit the expressive capability of PolyAttn.
4 Experiments
In this section, we conduct comprehensive experiments to evaluate
the performance of the proposed PolyAttn and PolyFormer. Specif-
ically, we first evaluate PolyAttnâ€™s ability on node-wise filteringusing both synthetic and real-world datasets. Then, we execute
node classification tasks on both small and large graphs to evaluate
the performance of PolyFormer. We also conduct complexity and
ablation comparison experiments.
4.1 PolyAttn Experiments
4.1.1 Fitting Signals on Synthetic Datasets. In this subsection, we
evaluate the efficacy of PolyAttn as a node-wise filter on synthetic
datasets. This evaluation highlights the enhanced capabilities of
PolyAttn in learning individual filter patterns for each node, without
the need for prior knowledge of predefined filters.
Synthetic Datasets. We use images with a resolution of 100Ã—100
from the Image Processing in Matlab library3. Each image can be
represented as a 2D regular 4-neighborhood grid graph. The pixel
values, ranging from 0to1, serve as node signals. For the ğ‘š-th
image, there exists an adjacency matrix Ağ‘šâˆˆR10000Ã—10000and a
node signal ğ’™ğ‘šâˆˆR10000. Based on the raw signal of each node,
we apply two hybrid predefined filters to each image, as detailed
in Table 3. Models are expected to learn these predefined filtering
patterns. More details can be seen in Appendix D.1.1.
Table 3: Predefined filters on graph signals.
Filters ğ’‰1(ğ€) ğ’‰2(ğ€)
Mixe
d low-pass â„(ğœ†)=ğ‘’âˆ’5ğœ†2â„(ğœ†)=ğ‘’âˆ’20ğœ†2
Mixed high-pass â„(ğœ†)=1âˆ’ğ‘’âˆ’5ğœ†2â„(ğœ†)=1âˆ’ğ‘’âˆ’20ğœ†2
Mixed band-pass â„(ğœ†)=ğ‘’âˆ’5(ğœ†âˆ’1)2â„(ğœ†)=ğ‘’âˆ’20(ğœ†âˆ’1)2
Mixed rejection-pass â„(ğœ†)=1âˆ’ğ‘’âˆ’5(ğœ†âˆ’1)2â„(ğœ†)=1âˆ’ğ‘’âˆ’20(ğœ†âˆ’1)2
Low & high-pass â„(ğœ†)=ğ‘’âˆ’10ğœ†2â„(ğœ†)=1âˆ’ğ‘’âˆ’10ğœ†2
Band & rejection-pass â„(ğœ†)=ğ‘’âˆ’10(ğœ†âˆ’1)2â„(ğœ†)=1âˆ’ğ‘’âˆ’10(ğœ†âˆ’1)2
Setup. We compare PolyAttn with 5 baseline methods, including
GCN [ 25], GAT [ 41], GPRGNN [ 4], BernNet [ 18], and ChebNetII
[19]. To ensure a fair comparison, all models are constrained to one
single layer and have approximately 5k parameters. The learning
rate is uniformly set to 0.001, the training epochs to 50,000, and the
early stopping threshold to 400iterations. We employ two metrics
to evaluate each method: the sum of squared errors and the ğ‘…2
score.
Results. As demonstrated in Table 2, PolyAttn outperforms
all baselines on all datasets. Compared to traditional polynomial
GNNs, which employ unified coefficients for all nodes, PolyAttn
3https://ww2.mathworks.cn/products/image.html
 
2123PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0.0 0.5 1.0 1.5 2.0
0.00.20.40.60.81.0h()
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000014
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000015
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000014
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000015
(a) Mixed low-pass
0.0 0.5 1.0 1.5 2.0
0.00.20.40.60.81.0h()
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000014
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000015
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000014
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000015 (b) Mixed high-pass
0.0 0.5 1.0 1.5 2.0
0.2
0.00.20.40.60.81.01.2h()
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000014
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000015
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000014
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000015 (c) Low-pass & high-pass
0.0 0.5 1.0 1.5 2.0
0.00.20.40.60.81.0h()
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000014
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni00000015
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000014
/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000024/uni00000057/uni00000057/uni00000051/uni0000001d/uni00000003/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000015 (d) Mixed band-pass
Figure 2: Learned filters of PolyAttn (Cheb).
uses tailored attention mechanisms for polynomial tokens to en-
able node-wise filtering. This design choice endows PolyAttn with
greater expressive power. Further evidence of this capability is pro-
vided in Figures 2. In the figure, filters learned on each node are
divided into one of two clusters using the ğ‘˜-means [ 22] algorithm,
and the representative filter (centroid) for each cluster is plotted.
PolyAttn is shown to successfully derive individual filter patterns
without requiring any prior knowledge of predefined filters. This
underscores PolyAttnâ€™s ability to learn graph filters for each node
adaptively.
Table 4: Performance of PolyAttn on real-world datasets.
CS
Pubmed Roman-empire Questions
UniFilter
(Mono) 95.32Â±0.24 89.61Â±0.44 73.44Â±0.80 73.19Â±1.52
PolyAttn (Mono) 95.99Â±0.07 90.85Â±0.31 74.17Â±0.59 76.83Â±0.79
Improvement (%) 0.70 1.38 0.99 4.96
UniFilter
(Bern) 96.03Â±0.12 88.55Â±0.43 73.32Â±0.37 74.30Â±0.80
PolyAttn (Bern) 95.84Â±0.21 90.18Â±0.41 76.33Â±0.30 77.79Â±0.74
Improvement (%) -0.20 1.80 4.11 4.70
UniFilter
(Opt) 95.08Â±0.23 89.61Â±0.32 76.33Â±0.37 75.38Â±0.86
PolyAttn (Opt) 95.48Â±0.13 89.89Â±0.53 74.70Â±0.67 76.79Â±0.75
Improvement (%) 0.42 0.31 -2.10 1.87
UniFilter
(Cheb) 96.17Â±0.10 88.65Â±0.35 72.81Â±0.73 74.55Â±0.78
PolyAttn (Cheb) 96.03Â±0.15 89.85Â±0.46 74.03Â±0.45 75.90Â±0.72
Improvement (%) -0.15 1.35 1.68 1.81
4.1.2 Performance on Real-world Datasets. We evaluate the efficacy
of PolyAttn as a node-wise filter on real-world datasets.
Setup. For homophilic datasets CS and Pubmed, we employ a
random split: 60% for the training set, 20% for the validation set, and
20% for the test set, following the approach of [ 18]. For heterophilic
graphs Roman-empire and Questions, we adhere to the partitioning
scheme provided in [ 34], allocating 50% of the data for training, 25%
for validation, and 25% for testing. We employ node-unified filters,
which learn shared polynomial coefficients for all nodes, denoted
as â€œUniFilterâ€. Correspondingly, we use node-wise filters based on
PolyAttn. All models are configured with a single filtering layer
and the same truncated order to ensure a fair comparison. More
details are listed in Appendix D.1.2.
Results. Table 4 shows the mean accuracies with a 95% confi-
dence interval over 10 runs. We observe that PolyAttn performs
better on both homophilic and heterophilic graphs, with especially
notable improvements on the latter one, which suggests the benefits
0.0 0.5 1.0 1.5 2.0
05101520h()
(a) UniFilter on Pubmed
0.0 0.5 1.0 1.5 2.0
020406080h()
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000014
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000015
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000016
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000017
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000018 (b) PolyAttn on Pubmed
0.0 0.5 1.0 1.5 2.0
01234h()
(c) UniFilter on Questions
0.0 0.5 1.0 1.5 2.0
2.55.07.510.012.515.017.5h()
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000014
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000015
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000016
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000017
/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047/uni00000003/uni00000018 (d) PolyAttn on Questions
Figure 3: Filters learned by UniFilter (left) and PolyAttn
(right) on the homophilic graph Pubmed (top) and the het-
erophilic graph Questions (bottom).
of its node-wise filtering ability. Further insights are illustrated in
Figures 3, which show the learned filters by â€œUniFilter (Cheb)â€ and
â€œPolyAttn (Cheb)â€ on Pubmed and Questions. The node-wise filters
learned by PolyAttn are categorized into one of five clusters using
the k-means algorithm [ 22]. Interestingly, we observe that the fil-
ters learned by PolyAttn bear resemblance to the node-unified filter,
yet display a greater level of sophistication. Given that PolyAttn
achieves a relative improvement of up to 4.96% over UniFilter, this
suggests that node-wise filters are essential for enhancing expres-
siveness.
4.2 PolyFormer Experiments
4.2.1 Node Classification. In this subsection, we evaluate the pro-
posed PolyFormer on real-world datasets, which encompass both
homophilic and heterophilic types, demonstrating the modelâ€™s ex-
ceptional performance.
Setup. We employ datasets including four homophilic datasets
[37,38] and six heterophilic datasets [ 34]. For homophilic datasets
including Citeseer, CS, Pubmed, and Physics, we employ a random
split: 60% for the training set, 20% for the validation set, and 20% for
 
2124KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahong Ma, Mingguo He, and Zhewei Wei
Table 5: Performance of PolyFormer on node classification. â€œOOMâ€ means â€œout of memory,â€ and â€œ*â€ indicates the use of
truncated eigenvalues and eigenvectors as suggested by [2].
Homophilic Heter
ophilic
Datasets Cite
. CS Pubm. Phys. Cham.
Squi. Mine. Tolo. Roman. Ques.
Nodes 3,327 18,333 19,717 34,493 890
2,223 10,000 11,758 22,662 48,921
Edges 9,104 163,788 44,324 495,924 17,708
93,996 39,402 519,000 32,927 153,540
Features 3,703 6,805 500 8,415 2,325
2,089 7 10 300 301
Classes 6 15 3 5 5
5 2 2 18 2
MLP
78.74Â±0.64 95.53Â±0.13 87.06Â±0.35 97.10Â±0.71 41.84Â±1.81 39.19Â±1.81 50.97Â±0.54 74.12Â±0.48 66.64Â±0.32 71.87Â±0.41
GCN
80.16Â±1.09 94.95Â±0.17 87.34Â±0.37 97.74Â±0.35 43.43Â±1.92 41.30Â±0.94 72.23Â±0.56 77.22Â±0.73 53.45Â±0.27 76.28Â±0.64
GA
T 80.67 Â±1.05 93.93Â±0.26 86.55Â±0.36 97.82Â±0.28 40.14Â±1.57 35.09Â±0.70 81.39Â±1.69 77.87Â±1.00 51.51Â±0.86 74.94Â±0.56
GPRGNN
80.61Â±0.75 95.26Â±0.15 91.00Â±0.34 97.74Â±0.35 42.28Â±2.87 41.09Â±1.18 90.10Â±0.34 77.25Â±0.61 74.08Â±0.54 74.36Â±0.67
BernNet
79.63Â±0.78 95.42Â±0.29 90.56Â±0.40 97.64Â±0.38 42.57Â±2.72 39.30Â±1.37 77.93Â±0.59 76.83Â±0.53 72.70Â±0.30 74.25Â±0.73
ChebNetII
80.25Â±0.65 96.33Â±0.12 90.60Â±0.17 97.25Â±0.78 42.67Â±1.43 41.22Â±0.37 83.64Â±0.40 79.23Â±0.43 74.64Â±0.39 74.41Â±0.58
OptBasisGNN
80.58Â±0.82 94.77Â±0.23 90.30Â±0.23 97.64Â±0.48 41.23Â±3.16 42.34Â±2.74 89.74Â±1.03 81.08Â±0.96 76.91Â±0.37 73.82Â±0.83
DSF-GPR-R
78.22Â±0.29 96.25Â±0.12 90.51Â±0.07 98.07Â±0.36 43.82Â±1.51 41.31Â±1.07 89.51Â±0.00 79.74Â±1.19 75.18Â±0.37 74.16Â±1.07
DSF-Bern-R
78.27Â±0.26 96.28Â±0.09 90.52Â±0.10 98.47Â±0.10 44.07Â±2.20 39.69Â±1.56 77.18Â±0.05 75.78Â±0.98 75.39Â±0.30 73.81Â±0.39
T
ransformer 78.70 Â±0.59 OOM 89.10 Â±0.43 OOM 43.27Â±1.65 39.82Â±0.84 50.29Â±1.09 74.24Â±0.58 65.29Â±0.47 OOM
Sp
ecformer 81.69Â±0.78 96.07Â±0.10 89.94Â±0.33 97.70Â±0.60 *42.82Â±2.54 40.20Â±0.53 89.93Â±0.41 80.42Â±0.55 69.94Â±0.34 76.49Â±0.58 *
NA
Gphormer 79.77 Â±0.81 95.89Â±0.13 89.65Â±0.45 97.23Â±0.23 40.36Â±1.77 39.79Â±0.84 88.06Â±0.43 81.57Â±0.44 74.45Â±0.48 75.13Â±0.70
GO
AT 76.40 Â±0.43 95.12Â±0.21 90.63Â±0.26 97.29Â±0.24 41.55Â±1.20 38.71Â±0.56 82.90Â±0.62 83.13Â±1.19 72.30Â±0.48 75.95Â±1.38
No
deFormer 80.35 Â±0.75 95.64Â±0.23 91.20Â±0.36 96.45Â±0.28 43.73Â±3.26 37.07Â±9.16 86.91Â±1.02 78.34Â±0.98 74.29Â±0.75 74.48Â±1.32
SGformer
81.11Â±1.08 94.86Â±0.38 89.57Â±0.90 97.96Â±0.81 44.21Â±3.06 43.74Â±2.51 77.69Â±0.96 82.07Â±1.18 73.91Â±0.79 77.06Â±1.20
PolyFormer
(Opt) 79.95Â±0.61 95.87Â±0.23 90.09Â±0.36 97.66Â±0.14 47.55Â±2.61 43.86Â±1.46 91.93Â±0.37 83.15Â±0.49 77.15Â±0.33 77.69Â±0.92
PolyFormer
(Mono) 82.37Â±0.65 96.49Â±0.09 91.01Â±0.41 98.42Â±0.16 46.86Â±1.61 42.56Â±0.96 90.69Â±0.38 84.00Â±0.45 78.89Â±0.39 77.46Â±0.65
PolyFormer
(Bern) 81.39Â±0.61 96.34Â±0.15 91.31Â±0.35 98.34Â±0.23 46.99Â±2.39 44.86Â±0.98 92.02Â±0.32 84.32Â±0.59 77.64Â±0.33 78.32Â±0.67
PolyFormer
(Cheb) 81.80Â±0.76 96.49Â±0.17 90.68Â±0.31 98.08Â±0.27 45.35Â±2.97 41.83Â±1.18 91.90Â±0.35 83.88Â±0.33 80.27Â±0.39 77.26Â±0.50
the test set, following the approach of [ 18]. For heterophilic graphs,
specifically Minesweeper, Tolokers, Roman-empire, and Questions,
we adhere to the partitioning scheme provided in [ 34], allocating
50% of the data for training, 25% for validation, and 25% for test-
ing. Similarly, for the processed heterophilic datasets Chameleon
and Squirrel, we also utilize the splits specified in [ 34]. As for the
baselines, we select several recent state-of-the-art spectral GNNs
[4,16,18,19], as well as extended node-wise filters [ 15]. Addi-
tionally, our comparison includes competitive Graph Transformer
models [ 2,3,26,43,44]. More details are available in Appendix D.2.1.
Results. As shown in Table 5, our model consistently outper-
forms most baseline models, especially excelling on heterophilic
datasets. Notably, when compared with spectral GNNs such as
ChebNetII, which utilize advanced techniques like Chebyshev In-
terpolation, and node-wise filters like DSF based on additional
positional encoding, our model showcases superior performance.
These results suggest that the introduction of node-wise filters,
PolyAttn, significantly boosts the expressive power of our model.
Furthermore, our model maintains competitive performance against
transformer-based approaches. This observation indicates that fo-
cusing on information within a limited scope, i.e., a truncation of
polynomial basis, provides the necessary expressiveness for achiev-
ing competitive results. Conversely, taking all node pairs into ac-
count may introduce redundant noise that diminishes the modelâ€™s
performance.
4.2.2 Node Classifications on Large-scale Datasets. In this subsec-
tion, we extend our model to large-scale datasets, demonstrating thescalability of our model, facilitated by its efficient node tokenization
methods and scalable node-wise filter.
Setup. We perform node classification tasks on two expansive
citation networks: ogbn-arxiv and ogbn-papers100M [ 20], in ad-
dition to two large-scale heterophilic graphs: Twitch-Gamers and
Pokec, sourced from [ 29] to demonstrating the scalability of our
model. For the citation datasets, experiments are conducted using
the given data splits in [ 20]. For the datasets Twitch-gamers and
Pokec, we utilize the five fixed data splits provided in [ 29]. We select
common GNN models, including [ 4,6,25]. For Graph Transformer
models, we use expressive Specformer [ 2], and three scalable base-
lines NAGphormer [ 3], Nodeformer [ 43] and SGFormer [ 44]. More
details are available in Appendix D.2.2.
Results. Table 6 shows the mean accuracy over multiple runs.
Due to our efficient node tokenization techniques and scalable
node-wise filters, PolyFormer exhibits great scalability up to the
graph ogbn-papers100M, which has over 100 million nodes . In
contrast, models such as NAGphormer [ 3] and Specformer [ 2] rely
on Laplacian eigenvectors or eigenvalues, which constrains their
scalability. Moreover, by leveraging expressive PolyAttn, our model
exhibits superior performance.
4.3 Complexity Comparison
In this subsection, we evaluate PolyFormer in comparison to other
attention-based models concerning accuracy, time, and GPU mem-
ory consumption.
 
2125PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 6: Performance of PolyFormer for node classification
on large-scale datasets. â€œ-â€ means â€œout of memoryâ€ or failing
to complete preprocessing within 24 hours.
Datasets T
witch arxiv Pokec papers100M
Nodes 168,114 169,343 1,632,803 111,059,956
Edges 6,797,557 1,166,243 30,622,564 1,615,685,872
Features 7 128 65 128
Classes 2 40 2 172
MLP
60.92 Â±0.07 55.50Â±0.23 62.37Â±0.02 47.24Â±0.31
GCN 62.18 Â±0.26 71.74Â±0.29 75.45Â±0.17 -
ChebNet 62.31Â±0.37 71.12Â±0.22 - -
GPR-GNN 62.59 Â±0.38 71.78Â±0.18 80.74Â±0.22 65.89Â±0.35
Sp
ecformer 64.22 Â±0.04 72.37Â±0.18 - -
NAGphormer 64.38Â±0.04 71.04Â±0.94 - -
NodeFormer 61.12Â±0.05 59.90Â±0.42 70.32Â±0.45 -
SGFormer 65.26Â±0.26 72.63Â±0.13 73.76Â±0.24 66.01Â±0.37
PolyFormer
64.79 Â±0.10 72.42Â±0.19 82.29Â±0.14
67.11Â±0.20
Results. As shown in Figure 4, the x-axis represents training
time, and the y-axis represents accuracy, while the area of the circle
indicates the relative maximum GPU memory consumption. Poly-
Former achieves the best performance with low training time and
GPU memory overhead. With the optimal accuracy obtained, the
model possesses slightly more parameters, leading to a marginally
longer training time, which is still acceptable in practice. Addi-
tionally, we record the processing times for several methods: ap-
proximately 1110 seconds for Laplacian eigendecomposition in
Specformer, 40 seconds for truncated Laplacian eigendecomposi-
tion in NAGphormer, and merely 0.36 seconds for calculating poly-
nomial tokens in PolyFormer. Given the significant reduction in
preprocessing time, PolyFormer proves highly efficient in practical
applications. Furthermore, PolyFormerâ€™s capability for mini-batch
training can result in even lower GPU consumption than reported,
thus facilitating scalability to graphs of any size.
/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013
/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000050/uni00000056/uni00000012/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni0000000c/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000031/uni00000052/uni00000047/uni00000048/uni00000029/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000031/uni00000024/uni0000002a/uni00000053/uni0000004b/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000036/uni00000053/uni00000048/uni00000046/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000029/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000036/uni0000002a/uni00000029/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni0000002a/uni00000024/uni00000037
Figure 4: Accuracy, training time, and relative maximum
GPU memory consumption comparison on Roman-empire.
4.4 Ablation Comparison
In this subsection, we present the results of the ablation study,
which compares the proposed PolyAttn mechanism that uses theTable 7: Performance comparison of SelfAttn and PolyAttn
in fitting node-wise filters. Lower sum of squared errors in-
dicates better performance.
Mo
del (5k para.) Low-pass High-pass Band-pass Rejection-pass
SelfAttn
(Mono) 0.4089 0.4037 2.1106 1.9176
PolyAttn (Mono) 0.2550 0.2631 1.3798 1.4025
SelfAttn
(Bern) 0.2758 0.2320 0.4295 0.4284
PolyAttn (Bern) 0.0842 0.1120 0.2719 0.3337
SelfAttn
(Opt) 0.2107 0.0249 0.1643 0.3026
PolyAttn (Opt) 0.1922 0.0103 0.0701 0.2275
SelfAttn
(Cheb) 0.1226 0.2241 0.1674 0.3853
PolyAttn (Cheb) 0.1467 0.0148 0.0782 0.1949
tanh activation function with the vanilla attention (self-attention)
mechanism, which uses the softmax activation function. We com-
pare the ability of these two mechanisms to fit the predefined node-
wise filters when given the same polynomial tokens. For example,
SelfAttn (Mono) and PolyAttn (Mono) respectively refer to self-
attention and PolyAttn, with node tokens based on the Monomial
basis. The other experimental settings are consistent with those
described in 4.1.1.
Results. Table 7 presents the results of the ablation experiments,
which show the sum of squared errors in fitting node-wise filters -
where lower values indicate better fitting performance. It is clear
that the proposed PolyAttn that uses the tanh activation function
outperforms the vanilla attention mechanism in accurately fitting
node-wise filters, indicating the validity of our proposed PolyAttn.
5 Conclusion
In this study, we introduce PolyAttn, an attention-based node-wise
filter. PolyAttn utilizes polynomial bases to capture spectral infor-
mation efficiently, outperforming traditional node-unified filters
in expressiveness while maintaining scalability and efficiency. Fur-
thermore, we present PolyFormer, a scalable Graph Transformer
tailored for node-level tasks. PolyFormer strikes a balance between
expressive power and scalability. Extensive empirical evaluations
confirm the superior performance, efficiency, and scalability of
PolyFormer. A promising future direction involves enhancing Poly-
Attn and PolyFormer through the incorporation of more advanced
polynomial approximation and graph spectral techniques.
Acknowledgments
This research was supported in part by National Natural Science
Foundation of China (No. U2241212, No. 61932001), by National
Science and Technology Major Project (2022ZD0114800), by Beijing
Natural Science Foundation (No. 4222028), by Beijing Outstanding
Young Scientist Program No.BJJWZYJH012019100020098, and by
Huawei-Renmin University joint program on Information Retrieval.
We also wish to acknowledge the support provided by the fund for
building world-class universities (disciplines) of Renmin Univer-
sity of China, by Engineering Research Center of Next-Generation
Intelligent Search and Recommendation, Ministry of Education,
Intelligent Social Governance Interdisciplinary Platform, Major
Innovation & Planning Interdisciplinary Platform for the â€œDouble-
First Classâ€ Initiative, Public Policy and Decision-making Research
Lab, and Public Computing Cloud, Renmin University of China.
 
2126KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahong Ma, Mingguo He, and Zhewei Wei
References
[1]Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori
Koyama. 2019. Optuna: A next-generation hyperparameter optimization frame-
work. In SIGKDD. 2623â€“2631.
[2]Deyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. 2023. Specformer: Spectral
Graph Neural Networks Meet Transformers. In ICLR.
[3]Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. 2023. NAGphormer: A
Tokenized Graph Transformer for Node Classification in Large Graphs. In ICLR.
[4]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2021. Adaptive Universal
Generalized PageRank Graph Neural Network. In ICLR.
[5]Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
Song, Andreea Gane, TamÃ¡s SarlÃ³s, Peter Hawkins, Jared Quincy Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian
Weller. 2021. Rethinking Attention with Performers. In ICLR.
[6]MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convo-
lutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In
NeurIPS. 3837â€“3845.
[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL-HLT. 4171â€“4186.
[8]Linhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-transformer: a no-recurrence
sequence-to-sequence model for speech recognition. In ICASSP. 5884â€“5888.
[9]Yushun Dong, Kaize Ding, Brian Jalaian, Shuiwang Ji, and Jundong Li. 2021.
Graph Neural Networks with Adaptive Frequency Response Filter. In CIKM.
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is
Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR.
[11] Vijay Prakash Dwivedi and Xavier Bresson. 2020. A Generalization of Trans-
former Networks to Graphs. CoRR abs/2012.09699 (2020). arXiv:2012.09699
https://arxiv.org/abs/2012.09699
[12] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and
consequences. Minds and Machines 30 (2020), 681â€“694.
[13] Keith O Geddes. 1978. Near-minimax polynomial approximation in an elliptical
region. SIAM J. Numer. Anal. 15, 6 (1978), 1225â€“1233.
[14] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui
Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang.
2020. Conformer: Convolution-augmented Transformer for Speech Recognition.
InINTERSPEECH. ISCA, 5036â€“5040.
[15] Jingwei Guo, Kaizhu Huang, Xinping Yi, and Rui Zhang. 2023. Graph Neural
Networks with Diverse Spectral Filtering. In WWW. 306â€“316.
[16] Yuhe Guo and Zhewei Wei. 2023. Graph Neural Networks with Learnable and
Optimal Polynomial Bases. In ICML, Vol. 202. 12077â€“12097.
[17] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In NeurIPS. 1025â€“1035.
[18] Mingguo He, Zhewei Wei, Zengfeng Huang, and Hongteng Xu. 2021. Bern-
Net: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation. In
NeurIPS. 14239â€“14251.
[19] Mingguo He, Zhewei Wei, and Ji-Rong Wen. 2022. Convolutional Neural Net-
works on Graphs with Chebyshev Approximation, Revisited. In NeurIPS. 7264â€“
7276.
[20] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets
for Machine Learning on Graphs. In NeurIPS. 22118â€“22133.
[21] Elvin Isufi, Fernando Gama, David I. Shuman, and Santiago Segarra. 2022.
Graph Filters for Signal Processing and Machine Learning on Graphs. CoRR
abs/2211.08854 (2022). arXiv:2211.08854 https://doi.org/10.48550/arXiv.2211.
08854
[22] Anil K Jain and Richard C Dubes. 1988. Algorithms for clustering data. Prentice-
Hall, Inc.
[23] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization
with Gumbel-Softmax. In ICLR.
[24] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-
mization. In ICLR.
[25] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[26] Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Renkun Ni, C. Bayan Bruss, and
Tom Goldstein. 2023. GOAT: A Global Transformer on Large-scale Graphs. In
ICML, Vol. 202. 17375â€“17390.
[27] Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent LÃ©tourneau, and
Prudencio Tossou. 2021. Rethinking Graph Transformers with Spectral Attention.
InNeurIPS. 21618â€“21629.
[28] Weirui Kuang, WANG Zhen, Yaliang Li, Zhewei Wei, and Bolin Ding. 2021.
Coarformer: Transformer for large graph via graph coarsening. (2021).
[29] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar
Bhalerao, and Ser Nam Lim. 2021. Large scale learning on non-homophilous
graphs: New benchmarks and strong simple methods. In NeurIPS. 20887â€“20902.[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
and Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer
using Shifted Windows. In ICCV. 9992â€“10002.
[31] Andreas Loukas. 2019. Graph Reduction with Spectral and Cut Guarantees. J.
Mach. Learn. Res. 20, 116 (2019), 1â€“42.
[32] GrÃ©goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. 2021.
GraphiT: Encoding Graph Structure in Transformers. CoRR abs/2106.05667
(2021). arXiv:2106.05667 https://arxiv.org/abs/2106.05667
[33] Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang,
Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. 2022. Transformer
for Graphs: An Overview from Architecture Perspective. CoRR abs/2202.08455
(2022). arXiv:2202.08455 https://arxiv.org/abs/2202.08455
[34] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila
Prokhorenkova. 2023. A critical look at the evaluation of GNNs under heterophily:
Are we really making progress?. In ICLR.
[35] Ladislav RampÃ¡sek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy
Wolf, and Dominique Beaini. 2022. Recipe for a General, Powerful, Scalable
Graph Transformer. In NeurIPS. 14501â€“14515.
[36] Dorit Ron, Ilya Safro, and Achi Brandt. 2011. Relaxation-based coarsening and
multiscale graph organization. Multiscale Modeling & Simulation 9, 1 (2011),
407â€“423.
[37] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93â€“93.
[38] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
GÃ¼nnemann. 2018. Pitfalls of Graph Neural Network Evaluation. CoRR
abs/1811.05868 (2018). arXiv:1811.05868 http://arxiv.org/abs/1811.05868
[39] Chengcheng Sun, Chenhao Li, Xiang Lin, Tianji Zheng, Fanrong Meng, Xiaobin
Rui, and Zhixiao Wang. 2023. Attention-based graph neural networks: a survey.
Artificial Intelligence Review (2023), 1â€“48.
[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In NeurIPS. 5998â€“6008.
[41] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.
[42] Xiyuan Wang and Muhan Zhang. 2022. How Powerful are Spectral Graph Neural
Networks. In ICML, Vol. 162. 23341â€“23362.
[43] Qitian Wu, Wentao Zhao, Zenan Li, David P. Wipf, and Junchi Yan. 2022. Node-
Former: A Scalable Graph Structure Learning Transformer for Node Classification.
InNeurIPS.
[44] Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian
Jiang, Yatao Bian, and Junchi Yan. 2023. SGFormer: Simplifying and Empowering
Transformers for Large-Graph Representations. In Advances in Neural Information
Processing Systems (NeurIPS).
[45] Zizhang Wu, Yuanzhu Gan, Tianhao Xu, and Fan Wang. 2024. Graph-Segmenter:
graph transformer with boundary-aware attention for semantic segmentation.
Frontiers Comput. Sci. 18, 5 (2024), 185327.
[46] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,
Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On Layer
Normalization in the Transformer Architecture. In ICML, Vol. 119. 10524â€“10533.
[47] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful
are graph neural networks?. In ICLR.
[48] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,
Yanming Shen, and Tie-Yan Liu. 2021. Do Transformers Really Perform Badly
for Graph Representation?. In NeurIPS. 28877â€“28888.
[49] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural
Networks. In NeurIPS. 5171â€“5181.
[50] Zaixi Zhang, Qi Liu, Qingyong Hu, and Chee-Kong Lee. 2022. Hierarchical Graph
Transformer with Adaptive Node Sampling. In NeurIPS. 21171â€“21183.
[51] Yanping Zheng, Lu Yi, and Zhewei Wei. 2024. A survey of dynamic graph neural
networks. Frontiers of Computer Science (2024). https://doi.org/10.1007/s11704-
024-3853-2
A Notations
We list the main notations in the paper in Table 8.
B Proof
B.1 Proof of the Theorem
Here we provide the detailed proof for Theorem 3.1.
Proof. For a node ğ‘£ğ‘–in the graph, the corresponding token
matrix is given by H(ğ‘–)=[ğ’‰(ğ‘–)
0,..., ğ’‰(ğ‘–)
ğ¾]âŠ¤âˆˆR(ğ¾+1)Ã—ğ‘‘. When
 
2127PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 8: Summary of notations in this paper.
Notation
Description
ğº=(ğ‘‰
,ğ¸) A graph where ğ‘‰is the set of nodes and ğ¸is the set of edges.
ğ‘ Total number of nodes in the graph.
A(Ë†A) The adjacency matrix of the graph and its normalized version.
Ë†L Normalized Laplacian of the graph.
P Refers to either Ë†AorË†L.
XâˆˆRğ‘Ã—ğ‘‘Original
graph signal matrix or node feature matrix.
ZâˆˆRğ‘Ã—ğ‘‘(Rğ‘Ã—ğ‘) Filtered signal or representation of nodes.
{ğ‘”ğ‘˜(
Â·)}ğ¾
ğ‘˜=0Series polynomial basis of truncated order ğ¾.
{ğ›¼ğ‘˜}ğ¾
ğ‘˜=0Polynomial coefficients for all nodes, i.e.Z â‰ˆÃğ¾
ğ‘˜=0ğ›¼ğ‘˜ğ‘”ğ‘˜(P)X.
{ğ›¼(ğ‘–)
ğ‘˜}ğ¾
ğ‘˜=0Polynomial coefficients of nodes ğ‘£ğ‘–, i.e.Zğ‘–,:â‰ˆÃğ¾
ğ‘˜=0ğ›¼(ğ‘–)
ğ‘˜(ğ‘”ğ‘˜(P)X)ğ‘–,:.
{ğ›¼(ğ‘,ğ‘)ğ‘˜}ğ¾
ğ‘˜=0Coefficients on channel (p,q),i.e.Z :,ğ‘:ğ‘â‰ˆÃğ¾
ğ‘˜=0ğ›¼(ğ‘,ğ‘)ğ‘˜(ğ‘”ğ‘˜(P)X):,ğ‘:ğ‘.
ğ’‰(ğ‘–)
ğ‘˜âˆˆRğ‘‘Polynomial
token of order ğ‘˜for nodeğ‘£ğ‘–.
Hğ‘˜âˆˆRğ‘Ã—ğ‘‘Matrix contains order- ğ‘˜polynomial tokens for all nodes.
H(ğ‘–)âˆˆR(ğ¾+1)Ã—ğ‘‘Token matrix for node ğ‘£ğ‘–.
ğœ·âˆˆR(ğ¾+1)Attention
bias vector shared across all nodes.
BâˆˆR(ğ¾+1)Ã—(ğ¾+1)Attention bias matrix, where each entry Bğ‘–ğ‘—equalsğ›½ğ‘—.
Q,K,VâˆˆR(ğ¾+1)Ã—ğ‘‘The query, key, and value matrices, respectively.
SâˆˆR(ğ¾+1)Ã—(ğ¾+1)The attention score matrix.
processed by the order-wise MLP, each row H(ğ‘–)
ğ‘—,:is updated as
H(ğ‘–)
ğ‘—,:=MLPğ‘—(H(ğ‘–)
ğ‘—,:). Subsequently, the query matrix Qand the
key matrix Kare calculated as Q=H(ğ‘–)Wğ‘„andK=H(ğ‘–)Wğ¾,
respectively. The attention matrix Ağ‘ğ‘¡ğ‘¡ğ‘›âˆˆR(ğ¾+1)Ã—(ğ¾+1)is then
formulated as(Ağ‘ğ‘¡ğ‘¡ğ‘›)ğ‘–ğ‘—=ğ‘ğ‘–ğ‘—=(QKâŠ¤)ğ‘–ğ‘—.
Taking the activation function ğœand the attention bias matrix B
into account, the corresponding attention score matrix S=Ağ‘ğ‘¡ğ‘¡ğ‘›âŠ™
BâˆˆR(ğ¾+1)Ã—(ğ¾+1), where Bğ‘–ğ‘—=ğ›½ğ‘—,ğ‘—âˆˆ{0,...,ğ¾}.
According to Hâ€²(ğ‘–)=SVandV=H(ğ‘–), we have:
Hâ€²(ğ‘–)=SH(ğ‘–)="ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ 0ğ‘˜ğ’‰(ğ‘–)
ğ‘˜,...,ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ ğ¾ğ‘˜ğ’‰(ğ‘–)
ğ‘˜#âŠ¤
, (11)
where Hâ€²(ğ‘–)âˆˆR(ğ¾+1)Ã—ğ‘‘. As representation of node ğ‘£ğ‘–is calculated
byZğ‘–,:=Ãğ¾
ğ‘˜=0Hâ€²(ğ‘–)
ğ‘˜,:, we have:
Zğ‘–,:=ğ¾âˆ‘ï¸
ğ‘˜=0Hâ€²(ğ‘–)
ğ‘˜,:=ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ 0ğ‘˜ğ’‰(ğ‘–)
ğ‘˜+Â·Â·Â·+ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ ğ¾ğ‘˜ğ’‰(ğ‘–)
ğ‘˜
=ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ ğ‘˜0ğ’‰(ğ‘–)
0+Â·Â·Â·+ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ ğ‘˜ğ¾ğ’‰(ğ‘–)
ğ¾
=ğ›¼(ğ‘–)
0ğ’‰(ğ‘–)
0+Â·Â·Â·+ğ›¼(ğ‘–)
ğ¾ğ’‰(ğ‘–)
ğ¾
=ğ¾âˆ‘ï¸
ğ‘˜=0ğ›¼(ğ‘–)
ğ‘˜ğ’‰(ğ‘–)
ğ‘˜=ğ¾âˆ‘ï¸
ğ‘˜=0ğ›¼(ğ‘–)
ğ‘˜(ğ‘”ğ‘˜(P)X)ğ‘–,:.(12)
Here,ğ›¼(ğ‘–)
ğ‘—=Ãğ¾
ğ‘˜=0ğ‘ ğ‘˜ğ‘—,ğ‘—âˆˆ{0,...,ğ¾}andğ›¼(ğ‘–)
ğ‘—is computed based
on the nodeâ€™s token matrix H(ğ‘–). This value serves as a node-
wise weight for the polynomial filter and is determined by both
the node features and the topology information of the node ğ‘£ğ‘–.
Consequently, the described PolyAttn mechanism functions as a
node-wise filter. â–¡
B.2 Proof of the Propositions
In the following, we present a proof for Proposition 3.1.Proof. For nodeğ‘£ğ‘–, the multi-head PolyAttn mechanism em-
ploys the sub-channel of the token matrix H(ğ‘–)
:,ğ‘—ğ‘‘â„:(ğ‘—+1)ğ‘‘â„âˆ’1for head
ğ‘—, whereğ‘—âˆˆ{0,...,â„âˆ’1}.
According to Theorem 3.1, there exists a set of node-wise coeffi-
cients for node ğ‘£ğ‘–, denoted by ğ›¼(ğ‘–)
(ğ‘—ğ‘‘â„,(ğ‘—+1)ğ‘‘â„âˆ’1)ğ‘˜, withğ‘˜âˆˆ{0,...,ğ¾}.
These coefficients are computed based on the corresponding sub-
channel of the token matrix H(ğ‘–)
:,ğ‘—ğ‘‘â„:(ğ‘—+1)ğ‘‘â„âˆ’1. The contribution of
headğ‘—to the node representation Zğ‘–,:can then be formally ex-
pressed as:
Zğ‘–,ğ‘—ğ‘‘â„:(ğ‘—+1)ğ‘‘â„âˆ’1=ğ¾âˆ‘ï¸
ğ‘˜=0ğ›¼(ğ‘–)
(ğ‘—ğ‘‘â„,(ğ‘—+1)ğ‘‘â„âˆ’1)ğ‘˜(ğ‘”ğ‘˜(P)X)ğ‘–,ğ‘—ğ‘‘â„:(ğ‘—+1)ğ‘‘â„âˆ’1.
(13)
By concatenating the contributions from all heads, we obtain
the complete node representation for node ğ‘£ğ‘–. Throughout this
procedure, the multi-head PolyAttn mechanism performs a filtering
operation on each channel group separately. â–¡
Below, we deliver a detailed proof for Proposition 3.2.
Proof. According to Proof B.1, when the PolyAttn functions as
a node-wise filter for node ğ‘£ğ‘–, we have:
Zğ‘–,:=ğ¾âˆ‘ï¸
ğ‘˜=0Hâ€²(ğ‘–)
ğ‘˜,:=ğ¾âˆ‘ï¸
ğ‘˜=0ğ›¼(ğ‘–)
ğ‘˜(ğ‘”ğ‘˜(P)X)ğ‘–,:,
where
ğ›¼(ğ‘–)
ğ‘—=ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ ğ‘˜ğ‘—=ğ¾âˆ‘ï¸
ğ‘˜=0ğœ(ğ‘ğ‘˜ğ‘—)ğ›½ğ‘—,ğ‘—âˆˆ{0,...,ğ¾}.
If the softmax function is employed, then for any node ğ‘£ğ‘–in the
graph, the value ofÃğ¾
ğ‘˜=0ğœ(ğ‘ğ‘˜ğ‘—)remains positive after the softmax
operation. The sign of ğ›¼(ğ‘–)
ğ‘—is thus determined by the bias ğ›½ğ‘—. Since
this bias is not node-specific, it implies that the coefficients of all
nodes are constrained by the bias ğ›½ğ‘—, thereby limiting the expressive
power of PolyAttn when acting as a node-wise filter. For instance,
when all biases ğ›½ğ‘—are positive, then
ğ›¼(ğ‘–)
ğ‘—=ğ¾âˆ‘ï¸
ğ‘˜=0ğ‘ ğ‘˜ğ‘—=ğ¾âˆ‘ï¸
ğ‘˜=0ğœ(ğ‘ğ‘˜ğ‘—)ğ›½ğ‘—>0,
PolyAttn with a Monomial basis can only serve as a low-pass filter
for all nodes [ 4]. In contrast, the activation function tanh(Â·)allows
the coefficient ğ›¼(ğ‘–)
ğ‘—=Ãğ¾
ğ‘˜=0ğ‘ ğ‘˜ğ‘—=Ãğ¾
ğ‘˜=0ğœ(ğ‘ğ‘˜ğ‘—)ğ›½ğ‘—to vary across
nodes, enhancing the expressive power of PolyAttn. â–¡
C Implementation Details
Multi-head PolyAttn. Here we provide pseudocode for the multi-
head PolyAttn mechanism as below.
Attention Bias. In implementation, we imposed constraints
on the bias corresponding to each order of polynomial tokens.
Specifically, for the learnable bias ğœ·, the attention bias matrix Bâˆˆ
R(ğ¾+1)Ã—(ğ¾+1)is defined as Bğ‘–ğ‘—=ğ›½ğ‘—
(ğ‘—+1)ğ‘Ÿ, where hyperparameter ğ‘Ÿ
is the constraint factor.
Order-wise MLP. To enhance the expressive capacity of the
order-wise MLP, we use the hyperparameter ğ‘što increase the
intermediate dimension of the order-wise MLP. Specifically, for an
 
2128KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Jiahong Ma, Mingguo He, and Zhewei Wei
Algorithm 2: Pseudocode for Multi-head PolyAttn
Input: Token matrix for node ğ‘£ğ‘–:
H(ğ‘–)=[ğ’‰(ğ‘–)
0,..., ğ’‰(ğ‘–)
ğ¾]âŠ¤âˆˆR(ğ¾+1)Ã—ğ‘‘
Output: New token matrix for node ğ‘£ğ‘–:Hâ€²(ğ‘–)âˆˆR(ğ¾+1)Ã—ğ‘‘
Learnable Parameters: Projection matrix Wğ‘„,
Wğ¾âˆˆRğ‘‘Ã—(ğ‘‘â„Ã—â„),
token-wise MLP ğ‘—(ğ‘—=0,...,ğ¾),
attention bias BâˆˆR(â„Ã—(ğ¾+1)
1Initialize Vwith H(ğ‘–)
2forj =0toğ¾do
3 H(ğ‘–)
ğ‘—,:â†MLPğ‘—(H(ğ‘–)
ğ‘—,:)
4Qâ†H(ğ‘–)Wğ‘„via projection matrix Wğ‘„;Kâ†H(ğ‘–)Wğ¾via
projection matrix Wğ¾
5Reshape Q,Kintoâ„heads to get
Q(ğ‘š)âˆˆR(ğ¾+1)Ã—ğ‘‘â„,K(ğ‘š)âˆˆR(ğ¾+1)Ã—ğ‘‘â„,ğ‘šâˆˆ{0,...,â„âˆ’1}
6form =0toâ„âˆ’1do
7 S(ğ‘š)â†tanh(Q(ğ‘š)KâŠ¤
(ğ‘š))âŠ™Bğ‘š,ğ‘—
8 Hâ€²(ğ‘–)
(ğ‘š)â†S(ğ‘š)V:ğ‘,ğ‘, where
(ğ‘,ğ‘)=(ğ‘‘â„Ã—ğ‘š,ğ‘‘â„Ã—(ğ‘š+1)âˆ’ 1)
9Hâ€²(ğ‘–)â†[Hâ€²(ğ‘–)
(0)||Â·Â·Â·|| Hâ€²(ğ‘–)
(â„âˆ’1)]âˆˆR(ğ¾+1)Ã—ğ‘‘, where
[Â·||Â·Â·Â·||Â·] means concatenating matrices
10return Hâ€²(ğ‘–)
input dimension ğ‘‘the intermediate dimension of the order-wise
MLP isğ‘šÃ—ğ‘‘.
D Experimental Settings
D.1 PolyAttn Experiments
D.1.1 Fitting Signals in Synthetic Datasets. Based on the raw signal
of each node in a graph, we apply one of two predefined filters.
For example, for nodes with signals ğ’™1<0.5, we define a low-
pass filterâ„1(ğœ†)=exp(âˆ’10ğœ†2), resulting in a filtered signal ğ’›1=
Uâ„1(Î›)UâŠ¤ğ’™1. Conversely, for nodes with signals ğ’™1â‰¥0.5, we
implement a high-pass filter â„2(ğœ†)=1âˆ’exp(âˆ’10ğœ†2), yielding the
corresponding filtered signal ğ’›2=Uâ„2(Î›)UâŠ¤ğ’™2. For eigenvalues
ğœ†âˆˆ[0,2], the predefined filters â„1(ğœ†)andâ„2(ğœ†)are shown in Table
3. Given the original graph signals ğ’™1,ğ’™2and the filtered graph
signals ğ’›1,ğ’›2, Models are expected to learn these filtering patterns.
In this experiment, every model uses a truncated order of ğ¾=10
within one layer. Additionally, We employed one head for PolyAttn.
All models have total parameters of approximately 50,000, achieved
by using an adaptive hidden dimension.
D.1.2 Performance on Real-World Datasets. To ensure a fair com-
parison, the truncated order ğ¾is set to 10for the Monomial, Bern-
stein, and Chebyshev bases, and 5and10for the optimal basis. The
number of layers is set to 1for both the node-unified filter and
PolyAttn. Additionally, the number of heads for PolyAttn is one.
Hyperparameters, including hidden dimensions, learning rates,
and weight-decay rates, are fine-tuned through 200 rounds of Op-
tuna [ 1] hyperparameter search. The best configuration is chosenbased on its performance on the validation set. The final outcomes
are the averages of 10 evaluations on the test set with a 95% confi-
dence interval using the optimal parameters.
The Optuna search space consists of 100 trials, with the searching
space provided below:
â€¢Hidden dimension: {16,32,64,128,256};
â€¢Learning rates:{5e-5, 2e-4, 1e-3, 1e-2 };
â€¢Weight decays:{0.0, 1e-5, 1e-4, 5e-4, 1e-3 };
â€¢Dropout rates:{0.0,0.1,0.2,..., 0.9};
There is one extra hyperparameter for PolyAttn:
â€¢Multiplying factor ğ‘šfor order-wise MLP: {1.0, 2.0, 0.5}.
D.2 PolyFormer Experiments
D.2.1 Node Classifications. We train all models with the Adam
optimizer [ 24]. Early stopping is employed with the patience of 250
epochs out of a total of 2000 epochs. The mean test accuracy, along
with a 95% confidence interval, is reported based on 10 runs.
Hyperparameter selection is carried out on the validation sets.
To expedite the hyperparameter selection process, we utilize Op-
tuna [ 1], performing a maximum of 400 complete trials within the
following hyperparameter ranges:
â€¢Truncated order ğ¾of polynomial tokens: {2,4,6,8,10,12,14};
â€¢Number of layers:{1,2,3,4};
â€¢Number of heads:{1,2,4,8,16};
â€¢Hidden dimension: {16,32,64,128,256};
â€¢Hidden size for FFN: {32,64,128,256,512};
â€¢Learning rates:{0.00005,0.0001,0.0005,0.001,0.005,0.01};
â€¢Weight decays:{0.0, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3 };
â€¢Dropout rates:{0.0,0.1,0.2,..., 0.9};
â€¢Constraint factor ğ‘Ÿ:{1.0,1.2,1.4,1.6,1.8,2.0};
â€¢Multiplying factor ğ‘šfor order-wise MLP : {1.0,2.0,0.5}.
D.2.2 Node Classifications on Large-Scale Datasets. The reported
results for GNNs are sourced from He et al . [19] , whereas some
results for the Graph Transformer are sourced from Wu et al . [44] .
The remaining results are derived from recommended hyperparam-
eters or through hyperparameter searching. The mean test accuracy,
accompanied by a 95% confidence interval, is reported based on
either 5 or 10 runs.
We utilize the Adam optimizer [ 24] to train our models. Early
stopping is implemented with patience at 250 epochs within an
overall training span of 2000 epochs. The hyperparameter space
used for experiments on large-scale datasets is enumerated below:
â€¢Truncated order ğ¾of polynomial tokens: {4,8,10};
â€¢Number of layers:{1,2};
â€¢Number of heads:{1,4,8};
â€¢Hidden dimension: {128,512,1024};
â€¢Hidden size for FFN: {512,1024};
â€¢Learning rates:{0.00005,0.0002,0.01};
â€¢Weight decays:{0.0,0.00005,0.0005,0.001};
â€¢Dropout rates:{0.0,0.25,0.4,0.5};
â€¢Constraint factor ğ‘Ÿ:{1.0,2.0};
â€¢Multiplying factor ğ‘šfor order-wise MLP: {0.5,1.0};
â€¢Batch size:{10000,20000,50000}.
 
2129